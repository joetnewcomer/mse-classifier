,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Isomorphism between the space of linear operator and matrices for finite dimensional spaces,Isomorphism between the space of linear operator and matrices for finite dimensional spaces,,"Prove that $\operatorname{Lin}(U,V)$ is isomorphic to the space of $m$ by $n$ matrices, where $\dim(U)=n$ and $\dim(V)=m$. Thanks so much for your enlightment.","Prove that $\operatorname{Lin}(U,V)$ is isomorphic to the space of $m$ by $n$ matrices, where $\dim(U)=n$ and $\dim(V)=m$. Thanks so much for your enlightment.",,"['linear-algebra', 'matrices', 'vector-spaces', 'abstract-algebra']"
1,"A is symmetric iff A=P-Q, where P,Q are positive definite matrices","A is symmetric iff A=P-Q, where P,Q are positive definite matrices",,"Show that an $n \times n$ real matrix $A$ is symmetric iff $A$ can be written as $$A=P-Q$$ where $P$ and $Q$ are some $n \times n$ positive definite matrices. Can there be anything said  similarly about complex $n \times n$ matrices? Attempt: Clearly if $A=P-Q$, it is symmetric. Conversely, suppose $A$ is symmetric. Then $$A=R^{-1}DR$$ where $D=$diag$\lbrace \lambda_1,...\lambda_n \rbrace, \lambda_i \in \mathrm{R}$ so $$A=R^{-1}D_1R - R^{-1}D_2R$$ where $D_1=$diag$\lbrace \alpha_1,...\alpha_n \rbrace$ and $D_2=$diag$\lbrace \mu_1,...\mu_n\rbrace$ where $$\lambda_i = \alpha_i-\mu_i \quad s.t. \quad \alpha_i,\mu_i >0$$ We know that the last statement is possible, since the eigenvalues $\lambda_i$ of $A$ are real numbers. We cannot conclude the same about complex $n \times n$ matrices. I am not sure if there is more to this than what I proved above.","Show that an $n \times n$ real matrix $A$ is symmetric iff $A$ can be written as $$A=P-Q$$ where $P$ and $Q$ are some $n \times n$ positive definite matrices. Can there be anything said  similarly about complex $n \times n$ matrices? Attempt: Clearly if $A=P-Q$, it is symmetric. Conversely, suppose $A$ is symmetric. Then $$A=R^{-1}DR$$ where $D=$diag$\lbrace \lambda_1,...\lambda_n \rbrace, \lambda_i \in \mathrm{R}$ so $$A=R^{-1}D_1R - R^{-1}D_2R$$ where $D_1=$diag$\lbrace \alpha_1,...\alpha_n \rbrace$ and $D_2=$diag$\lbrace \mu_1,...\mu_n\rbrace$ where $$\lambda_i = \alpha_i-\mu_i \quad s.t. \quad \alpha_i,\mu_i >0$$ We know that the last statement is possible, since the eigenvalues $\lambda_i$ of $A$ are real numbers. We cannot conclude the same about complex $n \times n$ matrices. I am not sure if there is more to this than what I proved above.",,"['linear-algebra', 'matrices']"
2,Inequalities for Differences of Absolute Values of matrices,Inequalities for Differences of Absolute Values of matrices,,"Let $A$ and $B$ be two real symmetric $n\times n$ matrices. Let $A=USU^T$ be the eigen-decomposition $A$ and let $|A|=U|S|U^T$ where $|S|$ just denotes elementwise absolute value of the diagonal matrix $S$. $|B|$ is defined similarly. (Sometimes it is written $|A|=(A^T A)^{1/2}$. Ie $|A|$ is the positive definite matrix with the same eigenvectors and same singular values as $A$). Suppose $\|\cdot\|$ is the standard $L^{2}$ operator norm on $n\times n $ matrices, so $\|A\|=\sup_{x} \frac{\|Ax\|_2}{\|x\|_2}$. Is it true that $\|A-B\| \geq \| |A|-|B|\|$? Note, for vectors and the 2-norm where we take the absolute value element-wise, this is true since it is true of the real numbers.","Let $A$ and $B$ be two real symmetric $n\times n$ matrices. Let $A=USU^T$ be the eigen-decomposition $A$ and let $|A|=U|S|U^T$ where $|S|$ just denotes elementwise absolute value of the diagonal matrix $S$. $|B|$ is defined similarly. (Sometimes it is written $|A|=(A^T A)^{1/2}$. Ie $|A|$ is the positive definite matrix with the same eigenvectors and same singular values as $A$). Suppose $\|\cdot\|$ is the standard $L^{2}$ operator norm on $n\times n $ matrices, so $\|A\|=\sup_{x} \frac{\|Ax\|_2}{\|x\|_2}$. Is it true that $\|A-B\| \geq \| |A|-|B|\|$? Note, for vectors and the 2-norm where we take the absolute value element-wise, this is true since it is true of the real numbers.",,"['linear-algebra', 'matrices', 'inequality']"
3,When does a solution to linear equations satisfy $|x_1|>\sum_{i=2}^n|x_i|$?,When does a solution to linear equations satisfy ?,|x_1|>\sum_{i=2}^n|x_i|,"Let $A\overline{X}=0$, where $A=(a_{i,j})\in M_{L, n}(\mathbb{C})$ is a given non-zero $L\times n$ matrix over the complex numbers $\mathbb{C}$, and $\overline{X}=(x_1, x_2, \cdots, x_n)^T $ is an $n$-column vector. Suppose $L< n$, then we know that the above equations have non-zero solutions.  My question is: What are the sufficient (and necessary) conditions on $A$ such that we can find one solution $\overline{X}$ satisfying: $$(P)\ \ \ \ \ \ \ \  |x_1|>\sum_{i=2}^n|x_i|$$ Here are some observations and remarks: We may assume $L>1$, since for $L=1$, $|a_{1,1}|<|a_{1,i}|$ for some $i>1$ is a sufficient condition. For $a_{i,1}\neq 0$, since $x_1=-\sum_{j=2}^n\frac{a_{i,j}x_j}{a_{i,1}}$, we know that a necessary condition for the property $(P)$ to be hold is that $\exists j>1$, s.t., $|a_{i,j}|>|a_{i,1}|$. My primary motivation to ask this question is that I want to consider equations in the general group algebras $\mathbb{Z}G$ for some countable discrete group $G$; see this problem: Find a special element in group algebra","Let $A\overline{X}=0$, where $A=(a_{i,j})\in M_{L, n}(\mathbb{C})$ is a given non-zero $L\times n$ matrix over the complex numbers $\mathbb{C}$, and $\overline{X}=(x_1, x_2, \cdots, x_n)^T $ is an $n$-column vector. Suppose $L< n$, then we know that the above equations have non-zero solutions.  My question is: What are the sufficient (and necessary) conditions on $A$ such that we can find one solution $\overline{X}$ satisfying: $$(P)\ \ \ \ \ \ \ \  |x_1|>\sum_{i=2}^n|x_i|$$ Here are some observations and remarks: We may assume $L>1$, since for $L=1$, $|a_{1,1}|<|a_{1,i}|$ for some $i>1$ is a sufficient condition. For $a_{i,1}\neq 0$, since $x_1=-\sum_{j=2}^n\frac{a_{i,j}x_j}{a_{i,1}}$, we know that a necessary condition for the property $(P)$ to be hold is that $\exists j>1$, s.t., $|a_{i,j}|>|a_{i,1}|$. My primary motivation to ask this question is that I want to consider equations in the general group algebras $\mathbb{Z}G$ for some countable discrete group $G$; see this problem: Find a special element in group algebra",,"['linear-algebra', 'matrices']"
4,How to find positive solution by using LSQR method?,How to find positive solution by using LSQR method?,,"I am trying to solve the unsymmetric equation $AX=B$. $A$ is an $8\times10$ matrix, $X$ and $B$ are both $10\times1$ matrices. But the solution $x$ is a capacitance; it is a positive value. A negative value is impossible. If I use this matlab function ... It will sometime give me negative value solutions, depending on the initial point I choose. Is there any way that I can change the algorithm make it find positive values only? Or is there another numerical method that can let me get the positive solutions?","I am trying to solve the unsymmetric equation $AX=B$. $A$ is an $8\times10$ matrix, $X$ and $B$ are both $10\times1$ matrices. But the solution $x$ is a capacitance; it is a positive value. A negative value is impossible. If I use this matlab function ... It will sometime give me negative value solutions, depending on the initial point I choose. Is there any way that I can change the algorithm make it find positive values only? Or is there another numerical method that can let me get the positive solutions?",,"['linear-algebra', 'matrices']"
5,How to calculate this $\frac{0}{0}$ limit?,How to calculate this  limit?,\frac{0}{0},"Let $R=\left[\begin{array}{ccc} \frac{1}{2} & \frac{1}{2} & 0\\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\ 0 & 0 & 1 \end{array}\right]$, $P=\left[\begin{array}{ccc} \frac{13}{36} & \frac{-17}{36} & \frac{4}{36}\\ \frac{-17}{36} & \frac{25}{36} & \frac{-8}{36}\\ \frac{4}{36} & \frac{-8}{36} & \frac{4}{36} \end{array}\right]$ , $Q=\left[\begin{array}{ccc} 0 & 0 & 0\\ 0 & \frac{1}{9} & \frac{-1}{9}\\ 0 & \frac{-1}{9} & \frac{1}{9} \end{array}\right]$,  $y(0)=\left[\begin{array}{c} 1 \\ 2 \\ 5 \end{array}\right]$. Define  $$ r(k)=\frac{y^{T}(0)(R^{T})^{k-1}PR^{k-1}y(0)}{y^{T}(0)(R^{T})^{k-1}QR^{k-1}y(0)} $$ The problem I'd like to look at is how to represent the value of $\lim_{k\rightarrow\infty}r(k)$ with the property of matrix $P$ and $Q$ (e.g. eigenvalue). Note that we have $\lim_{k\rightarrow\infty}R^{k}=\left[\begin{array}{ccc} 0 & 0 & 1\\ 0 & 0 & 1\\ 0 & 0 & 1 \end{array}\right]$, the row and column sum of $P$ and $Q$ are $0$ and actually $\lim_{k\rightarrow\infty}r(k)$ exists. Because  $\lim_{k\rightarrow\infty} R^{k}y(0)= \left[\begin{array}{c} 5 \\ 5 \\ 5 \end{array}\right]$, thus both the numerator and denominator converge to $0$ which leads to a $\frac{0}{0}$ indefinite form. It is believed that the value of $\lim_{k\rightarrow\infty}r(k)$ is encoded in matrix pair $(P,Q)$, but I'm unable to come up with a formula or even the connection between $\lim_{k\rightarrow\infty}r(k)$ and $(P,Q)$, could anyone help me on that ?","Let $R=\left[\begin{array}{ccc} \frac{1}{2} & \frac{1}{2} & 0\\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\ 0 & 0 & 1 \end{array}\right]$, $P=\left[\begin{array}{ccc} \frac{13}{36} & \frac{-17}{36} & \frac{4}{36}\\ \frac{-17}{36} & \frac{25}{36} & \frac{-8}{36}\\ \frac{4}{36} & \frac{-8}{36} & \frac{4}{36} \end{array}\right]$ , $Q=\left[\begin{array}{ccc} 0 & 0 & 0\\ 0 & \frac{1}{9} & \frac{-1}{9}\\ 0 & \frac{-1}{9} & \frac{1}{9} \end{array}\right]$,  $y(0)=\left[\begin{array}{c} 1 \\ 2 \\ 5 \end{array}\right]$. Define  $$ r(k)=\frac{y^{T}(0)(R^{T})^{k-1}PR^{k-1}y(0)}{y^{T}(0)(R^{T})^{k-1}QR^{k-1}y(0)} $$ The problem I'd like to look at is how to represent the value of $\lim_{k\rightarrow\infty}r(k)$ with the property of matrix $P$ and $Q$ (e.g. eigenvalue). Note that we have $\lim_{k\rightarrow\infty}R^{k}=\left[\begin{array}{ccc} 0 & 0 & 1\\ 0 & 0 & 1\\ 0 & 0 & 1 \end{array}\right]$, the row and column sum of $P$ and $Q$ are $0$ and actually $\lim_{k\rightarrow\infty}r(k)$ exists. Because  $\lim_{k\rightarrow\infty} R^{k}y(0)= \left[\begin{array}{c} 5 \\ 5 \\ 5 \end{array}\right]$, thus both the numerator and denominator converge to $0$ which leads to a $\frac{0}{0}$ indefinite form. It is believed that the value of $\lim_{k\rightarrow\infty}r(k)$ is encoded in matrix pair $(P,Q)$, but I'm unable to come up with a formula or even the connection between $\lim_{k\rightarrow\infty}r(k)$ and $(P,Q)$, could anyone help me on that ?",,"['calculus', 'matrices']"
6,A linear transformation which maps the unit sphere to itself.,A linear transformation which maps the unit sphere to itself.,,"$A \colon\Bbb  R^3\to\Bbb R^3$ is a linear transformation which maps the unit sphere to itself. Then $A$ is a) symmetric; b) orthogonal; c) positive definite; d) symmetric and positive definite. By the given condition my intuition says $\|Ax\|=\|x\|$  so $A$ will be orthogonal transformation. it may have negative eigenvalue so it will not be positive definite. Its characteristic polynomial will be of degree three and may have a complex root, so in that case it will not be symmetric as symm matricx has eigen value only realss. Thank you for help.","$A \colon\Bbb  R^3\to\Bbb R^3$ is a linear transformation which maps the unit sphere to itself. Then $A$ is a) symmetric; b) orthogonal; c) positive definite; d) symmetric and positive definite. By the given condition my intuition says $\|Ax\|=\|x\|$  so $A$ will be orthogonal transformation. it may have negative eigenvalue so it will not be positive definite. Its characteristic polynomial will be of degree three and may have a complex root, so in that case it will not be symmetric as symm matricx has eigen value only realss. Thank you for help.",,"['linear-algebra', 'matrices']"
7,Square of sum of matrices,Square of sum of matrices,,I'm trying to follow these lecture notes on Linear Discriminant Analysis (LDA) but I can't seem to figure out how the author gets from: $$ \Sigma_{x\epsilon\omega_{i}} (w^{T}x - w^{T}\mu_{i})^2$$ to $$ \Sigma_{x\epsilon\omega_{i}} w^{T}(x-\mu_{i})(x-\mu_{i})^Tw$$,I'm trying to follow these lecture notes on Linear Discriminant Analysis (LDA) but I can't seem to figure out how the author gets from: $$ \Sigma_{x\epsilon\omega_{i}} (w^{T}x - w^{T}\mu_{i})^2$$ to $$ \Sigma_{x\epsilon\omega_{i}} w^{T}(x-\mu_{i})(x-\mu_{i})^Tw$$,,['matrices']
8,Simple/Concise proof of Muir's Identity,Simple/Concise proof of Muir's Identity,,"I am not a Math student and I am having trouble finding some small proof for the Muir's identity. Even a slightly lengthy but easy to understand proof would be helpful. Muir's Identity $$\det(A)= (\operatorname{pf}(A))^2;$$ the identity is given in the first paragraph of the following link http://en.wikipedia.org/wiki/Pfaffian I am expecting a proof which uses minimal advanced mathematics.Any reference to a textbook or link would do. I would be very grateful, if any of you could point me in that direction. P.s- i have done all the googling required and i wasnt satisfied with their results,so dont post any results from google's 1st page Thanks in Advance","I am not a Math student and I am having trouble finding some small proof for the Muir's identity. Even a slightly lengthy but easy to understand proof would be helpful. Muir's Identity $$\det(A)= (\operatorname{pf}(A))^2;$$ the identity is given in the first paragraph of the following link http://en.wikipedia.org/wiki/Pfaffian I am expecting a proof which uses minimal advanced mathematics.Any reference to a textbook or link would do. I would be very grateful, if any of you could point me in that direction. P.s- i have done all the googling required and i wasnt satisfied with their results,so dont post any results from google's 1st page Thanks in Advance",,"['matrices', 'determinant', 'pfaffian']"
9,Solving $Ax = b$ when $A$ is singular,Solving  when  is singular,Ax = b A,"I have a system of equations, expressed as $\mathbf{A} \begin{pmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \end{pmatrix} = \begin{pmatrix} 0 \\ i (\frac{1}{2} + C - a) \\ i(\frac{1}{2} - C - a) \frac{m \cos(\alpha)}{k} \\ -i(\frac{1}{2} - C - a) \frac{\sqrt{m^2 + k^2} + m \sin(\alpha)}{k} \end{pmatrix}$ $\mathbf{A} = \begin{pmatrix} m \sin(\alpha) - \sqrt{m^2 + k^2} & m \cos(\alpha) & k & 0 \\ m \cos(\alpha) & -m\sin(\alpha) - \sqrt{m^2 + k^2} & 0 & k \\ k & 0 &  -m\sin(\alpha) - \sqrt{m^2 + k^2} & -m \cos(\alpha) \\ 0 & k & -m \cos(\alpha) & m \sin(\alpha) - \sqrt{m^2 + k^2} \end{pmatrix}$ where $\mathbf{A}$ is a $4\times 4$ singular matrix and $C, a$ are non-zero constants. I am trying to find $x_n$. If $\mathbf{A}$ is singular, as far as I know, this is only possible if the right-hand side is $0$. I can set the constraint $ \frac{1}{2} -C = a$ so that the RHS becomes $\begin{pmatrix} 0 \\ 2iC \\ 0 \\ 0 \end{pmatrix}$, but that doesn't really get me anywhere. Are there other methods of doing this?","I have a system of equations, expressed as $\mathbf{A} \begin{pmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \end{pmatrix} = \begin{pmatrix} 0 \\ i (\frac{1}{2} + C - a) \\ i(\frac{1}{2} - C - a) \frac{m \cos(\alpha)}{k} \\ -i(\frac{1}{2} - C - a) \frac{\sqrt{m^2 + k^2} + m \sin(\alpha)}{k} \end{pmatrix}$ $\mathbf{A} = \begin{pmatrix} m \sin(\alpha) - \sqrt{m^2 + k^2} & m \cos(\alpha) & k & 0 \\ m \cos(\alpha) & -m\sin(\alpha) - \sqrt{m^2 + k^2} & 0 & k \\ k & 0 &  -m\sin(\alpha) - \sqrt{m^2 + k^2} & -m \cos(\alpha) \\ 0 & k & -m \cos(\alpha) & m \sin(\alpha) - \sqrt{m^2 + k^2} \end{pmatrix}$ where $\mathbf{A}$ is a $4\times 4$ singular matrix and $C, a$ are non-zero constants. I am trying to find $x_n$. If $\mathbf{A}$ is singular, as far as I know, this is only possible if the right-hand side is $0$. I can set the constraint $ \frac{1}{2} -C = a$ so that the RHS becomes $\begin{pmatrix} 0 \\ 2iC \\ 0 \\ 0 \end{pmatrix}$, but that doesn't really get me anywhere. Are there other methods of doing this?",,"['linear-algebra', 'matrices']"
10,$M_n(D)$ has only finitely many right ideals if and only if $n = 1$ or $D$ is finite.,has only finitely many right ideals if and only if  or  is finite.,M_n(D) n = 1 D,"Let $D$ be a division ring. Then prove that $R = M_n(D)$ has only finitely many right ideals if and only if  $n = 1$ or $D$ is finite. I know that the ideals of $M_n(D)$ are of the form $M_n(I)$, where $I$ is an ideal of $D$. So there are only $2$ possible choices which are $0$ or $M_n(D)$ since $D$ has only $0$ and $D$ as ideals. So how come the condition of $n = 1$ or $D$ is finite take place?","Let $D$ be a division ring. Then prove that $R = M_n(D)$ has only finitely many right ideals if and only if  $n = 1$ or $D$ is finite. I know that the ideals of $M_n(D)$ are of the form $M_n(I)$, where $I$ is an ideal of $D$. So there are only $2$ possible choices which are $0$ or $M_n(D)$ since $D$ has only $0$ and $D$ as ideals. So how come the condition of $n = 1$ or $D$ is finite take place?",,"['abstract-algebra', 'matrices']"
11,Java Tetris - Using rotation matrix math to rotate piece,Java Tetris - Using rotation matrix math to rotate piece,,"I'm working on building tetris now in Java and am at the point of rotations... I originally hardcoded all of the rotations, but found that linear algebra (matrix rotations) was the better way to go. I'm trying to use a rotation matrix to rotate my pieces, and found I need a good understanding of trigonometry. I don't understand how $R(90^\circ)$ equals a rotation matrix of $R(-\theta)$ =` $$\begin{bmatrix}cos\theta &sin\theta \\-sin\theta & cos\theta\end{bmatrix}$$, aka $$\begin{bmatrix} 0 & 1 \\ -1& 0 \end{bmatrix}$$ (btw, if you know how to make a matrix on stackoverflow, please let me know). How does that equal a 90 degree rotation? Where do those zeros and ones come from? Moreover, how would that translate into code? I'm not looking for someone to code it for me, I'm looking for a concept with maybe a snippet of pseudocode. I'm trying to visualize it with this drawing by putting the grid and tiles on a graph and drawing out the angles... but still don't understand it. Can anyone help? Thanks! Transposing","I'm working on building tetris now in Java and am at the point of rotations... I originally hardcoded all of the rotations, but found that linear algebra (matrix rotations) was the better way to go. I'm trying to use a rotation matrix to rotate my pieces, and found I need a good understanding of trigonometry. I don't understand how $R(90^\circ)$ equals a rotation matrix of $R(-\theta)$ =` $$\begin{bmatrix}cos\theta &sin\theta \\-sin\theta & cos\theta\end{bmatrix}$$, aka $$\begin{bmatrix} 0 & 1 \\ -1& 0 \end{bmatrix}$$ (btw, if you know how to make a matrix on stackoverflow, please let me know). How does that equal a 90 degree rotation? Where do those zeros and ones come from? Moreover, how would that translate into code? I'm not looking for someone to code it for me, I'm looking for a concept with maybe a snippet of pseudocode. I'm trying to visualize it with this drawing by putting the grid and tiles on a graph and drawing out the angles... but still don't understand it. Can anyone help? Thanks! Transposing",,"['linear-algebra', 'matrices']"
12,A matrix eigenvalue question,A matrix eigenvalue question,,"If $A, B, C$ are positive definite matrices of size $n$, is it true that $\lambda_j(A(B+C)^2A)\ge \lambda_j(AB^2A)$, $j=1, \dots, n$? $\lambda_j$ means the $j$-th largest eigenvalue.","If $A, B, C$ are positive definite matrices of size $n$, is it true that $\lambda_j(A(B+C)^2A)\ge \lambda_j(AB^2A)$, $j=1, \dots, n$? $\lambda_j$ means the $j$-th largest eigenvalue.",,"['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors']"
13,Why does $ (A^T x) · y = x · (A y) $ hold?,Why does  hold?, (A^T x) · y = x · (A y) ,Why does $ (A^T x)· y = x ·(A y) $ hold? The proof has to do with properties of transposes. I did a proof using coordinates (which was correct) but there is an infinitely easier way to do it. A is an n by n matrix.,Why does $ (A^T x)· y = x ·(A y) $ hold? The proof has to do with properties of transposes. I did a proof using coordinates (which was correct) but there is an infinitely easier way to do it. A is an n by n matrix.,,"['linear-algebra', 'matrices']"
14,Condition on trace of product of two matrices to be positive,Condition on trace of product of two matrices to be positive,,"Given: $A$ is positive definite, $B$ is symmetric and $\operatorname{tr}(B)\geqslant 0$, what could be a minimal additional condition, so that $\operatorname{tr}(AB)\geqslant 0$? (""$B$ is positive definite"" is too strong)","Given: $A$ is positive definite, $B$ is symmetric and $\operatorname{tr}(B)\geqslant 0$, what could be a minimal additional condition, so that $\operatorname{tr}(AB)\geqslant 0$? (""$B$ is positive definite"" is too strong)",,"['linear-algebra', 'matrices', 'trace']"
15,Counting symmetric unitary matrices with elements of equal magnitude,Counting symmetric unitary matrices with elements of equal magnitude,,"Let $X$ be an $n\times n$ symmetric unitary matrix with elements of equal magnitude and the elements of the first row (and the first column, of course) are $1/\sqrt{n}$, i.e. $X_{j,k} = e^{i \phi_{j,k}}/\sqrt{n}$ with $\phi_{j,k}=\phi_{k,j} \in \mathbb{R}$ and $\phi_{1,k}=0$. Prove (or disprove) that such matrix is unique up to permutations. The background here is to get understanding of the set of bases in $\mathbb{C}^n$ with coordinates differ by phase factors only. The orthogonality relation $(v_j, v_k) = n^{-1}\sum_l \exp(i(\phi_{j,l}-\phi_{k,l})) = 0$ for $j \ne k$ is invariant with respect to $\phi_{j,k} \to \phi_{j,k}+\omega_j + \omega_k$. This would be great if the set could be obtained from a particular choice $$X_{j,k} = \frac{1}{\sqrt{n}}\exp\left(\frac{2\pi i}{n} (j-1)(k-1)\right)$$ by permutations and the symmetry transformation. The statement seems plausible since the matrix $X$ is determined by $n(n-1)/2$ phases with $n(n-1)/2$ constraints imposed by orthogonality relations. The proof, however, is lacking. I would greatly appreciate help in a form of proof/disproof or a strong hint.","Let $X$ be an $n\times n$ symmetric unitary matrix with elements of equal magnitude and the elements of the first row (and the first column, of course) are $1/\sqrt{n}$, i.e. $X_{j,k} = e^{i \phi_{j,k}}/\sqrt{n}$ with $\phi_{j,k}=\phi_{k,j} \in \mathbb{R}$ and $\phi_{1,k}=0$. Prove (or disprove) that such matrix is unique up to permutations. The background here is to get understanding of the set of bases in $\mathbb{C}^n$ with coordinates differ by phase factors only. The orthogonality relation $(v_j, v_k) = n^{-1}\sum_l \exp(i(\phi_{j,l}-\phi_{k,l})) = 0$ for $j \ne k$ is invariant with respect to $\phi_{j,k} \to \phi_{j,k}+\omega_j + \omega_k$. This would be great if the set could be obtained from a particular choice $$X_{j,k} = \frac{1}{\sqrt{n}}\exp\left(\frac{2\pi i}{n} (j-1)(k-1)\right)$$ by permutations and the symmetry transformation. The statement seems plausible since the matrix $X$ is determined by $n(n-1)/2$ phases with $n(n-1)/2$ constraints imposed by orthogonality relations. The proof, however, is lacking. I would greatly appreciate help in a form of proof/disproof or a strong hint.",,"['linear-algebra', 'matrices']"
16,Construct a matrix transform,Construct a matrix transform,,consider $\frac{dx}{dt} = Ax$ where $A$ is the matrix $$         \begin{bmatrix}         1 & 0 & 1 \\         0 & 0 & -2 \\         0 & 1 & 0 \\         \end{bmatrix} $$ construct a real matrix $P$ such that the change of coordinates $x=Py$ transforms our real equation to $\frac{dy}{dt}=By$ where $B$ is the matrix $$         \begin{bmatrix}         1 & 0 & 1 \\         0 & 0 & -\sqrt{2} \\         0 & \sqrt{2} & 0 \\         \end{bmatrix} $$ then solve explicity for $y$ and evaluate the solution in terms of our original $x=Py$ My Solution so far... I have diagonalized both $A$ and $B$ and got the following matrices respectively $$         \begin{bmatrix}         1 & 0 & 0 \\         0 &\sqrt{2} & 0) \\         0 & 0 & -\sqrt{2} \\         \end{bmatrix} $$ and for $B$ $$         \begin{bmatrix}         1 & 0 & 1 \\         0 & \sqrt{2} & 0 \\         0 & 0 & -\sqrt{2} \\         \end{bmatrix} $$ the eigenvectors for $A$ make the matrix $$         \begin{bmatrix}         1 & 1+\sqrt{2} & -1-\sqrt{2} \\         0 & 1/\sqrt{2} & -1/\sqrt{2} \\         0 & 1 & 1 \\         \end{bmatrix} $$ and the eigenvectors for $B$ make the matrix $$         \begin{bmatrix}         1 & 1/\sqrt{2} & -1/\sqrt{2} \\         0 & 0 & 0 \\         0 & 0 &0 \\         \end{bmatrix} $$ Thus the determinant of the eigenvector matrix for $B$ is $0$...I am now stuck. Please help!,consider $\frac{dx}{dt} = Ax$ where $A$ is the matrix $$         \begin{bmatrix}         1 & 0 & 1 \\         0 & 0 & -2 \\         0 & 1 & 0 \\         \end{bmatrix} $$ construct a real matrix $P$ such that the change of coordinates $x=Py$ transforms our real equation to $\frac{dy}{dt}=By$ where $B$ is the matrix $$         \begin{bmatrix}         1 & 0 & 1 \\         0 & 0 & -\sqrt{2} \\         0 & \sqrt{2} & 0 \\         \end{bmatrix} $$ then solve explicity for $y$ and evaluate the solution in terms of our original $x=Py$ My Solution so far... I have diagonalized both $A$ and $B$ and got the following matrices respectively $$         \begin{bmatrix}         1 & 0 & 0 \\         0 &\sqrt{2} & 0) \\         0 & 0 & -\sqrt{2} \\         \end{bmatrix} $$ and for $B$ $$         \begin{bmatrix}         1 & 0 & 1 \\         0 & \sqrt{2} & 0 \\         0 & 0 & -\sqrt{2} \\         \end{bmatrix} $$ the eigenvectors for $A$ make the matrix $$         \begin{bmatrix}         1 & 1+\sqrt{2} & -1-\sqrt{2} \\         0 & 1/\sqrt{2} & -1/\sqrt{2} \\         0 & 1 & 1 \\         \end{bmatrix} $$ and the eigenvectors for $B$ make the matrix $$         \begin{bmatrix}         1 & 1/\sqrt{2} & -1/\sqrt{2} \\         0 & 0 & 0 \\         0 & 0 &0 \\         \end{bmatrix} $$ Thus the determinant of the eigenvector matrix for $B$ is $0$...I am now stuck. Please help!,,"['linear-algebra', 'matrices']"
17,How to recover a shuffled matrix,How to recover a shuffled matrix,,"Suppose that I have a matrix $A$. $A$ can be a rating matrix. That is, $A(i,j)$ is the rating user $i$ has given to item $j$. Suppose that I shuffle the rows and columns of matrix $A$ and get $A_{\text{shuffle}}$. Now, actually, both $A$ and $A_{\text{shuffle}}$ contains the same information. Because shuffling the columns or rows do not change the ratings the users give to items. Is there an efficient way to show that $A$ and $A_{\text{shuffle}}$ contain the same users and items. I.e., as suggested by polkjh,  given matrices $A$ and $B$, how can I check efficiently if $B$ can be obtained by shuffling rows and columns of $A$? Thanks","Suppose that I have a matrix $A$. $A$ can be a rating matrix. That is, $A(i,j)$ is the rating user $i$ has given to item $j$. Suppose that I shuffle the rows and columns of matrix $A$ and get $A_{\text{shuffle}}$. Now, actually, both $A$ and $A_{\text{shuffle}}$ contains the same information. Because shuffling the columns or rows do not change the ratings the users give to items. Is there an efficient way to show that $A$ and $A_{\text{shuffle}}$ contain the same users and items. I.e., as suggested by polkjh,  given matrices $A$ and $B$, how can I check efficiently if $B$ can be obtained by shuffling rows and columns of $A$? Thanks",,"['matrices', 'computational-complexity']"
18,Eigenvector of a sparse structured matrix corresponding to the eigenvalue 1,Eigenvector of a sparse structured matrix corresponding to the eigenvalue 1,,"I have a matrix with the following sparsity pattern: $M = \begin{bmatrix} \ast &\ast &0 &0 &0 &0 &0 &0\\ 0 & 0 &\ast &\ast &0 &0 &0 &0 \\ 0 &0 &0 &0 &\ast &\ast &0 &0 \\ 0 &0 &0 &0 &0 &0 &\ast &\ast \\ \ast &\ast &0 &0 &0 &0 &0 &0\\ 0 & 0 &\ast &\ast &0 &0 &0 &0 \\ 0 &0 &0 &0 &\ast &\ast &0 &0 \\ 0 &0 &0 &0 &0 &0 &\ast &\ast \end{bmatrix}$ where $\ast$ denotes non-zero entries. It is a right stochastic matrix (a transition matrix for a Markov chain; each row sums to one), so I know it has a left eigenvector with eigenvalue 1, i.e., some $x$ for: $xM = x$ I can use the power-iteration to find the eigenvector $x$. I was wondering if there is a faster way of getting the solution for this special structured matrix. (Also, is there a name for this sparsity pattern?) EDIT: the matrix could be larger: size of $2^n \times 2^n$. EDIT2: Note that $M$ is not full rank.","I have a matrix with the following sparsity pattern: $M = \begin{bmatrix} \ast &\ast &0 &0 &0 &0 &0 &0\\ 0 & 0 &\ast &\ast &0 &0 &0 &0 \\ 0 &0 &0 &0 &\ast &\ast &0 &0 \\ 0 &0 &0 &0 &0 &0 &\ast &\ast \\ \ast &\ast &0 &0 &0 &0 &0 &0\\ 0 & 0 &\ast &\ast &0 &0 &0 &0 \\ 0 &0 &0 &0 &\ast &\ast &0 &0 \\ 0 &0 &0 &0 &0 &0 &\ast &\ast \end{bmatrix}$ where $\ast$ denotes non-zero entries. It is a right stochastic matrix (a transition matrix for a Markov chain; each row sums to one), so I know it has a left eigenvector with eigenvalue 1, i.e., some $x$ for: $xM = x$ I can use the power-iteration to find the eigenvector $x$. I was wondering if there is a faster way of getting the solution for this special structured matrix. (Also, is there a name for this sparsity pattern?) EDIT: the matrix could be larger: size of $2^n \times 2^n$. EDIT2: Note that $M$ is not full rank.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'block-matrices']"
19,Matrix calculation,Matrix calculation,,"The matrix is $ M= \frac{d}{d\theta} e^{A+\theta B} \mid _{\theta = 0} $ where $A$ and $B$ are both $n\times n$ matrices. I was thinking solving it by introducing the equations: $\dot x = (A + \theta B)x,\ x(0) = I$ with solution $x = X(t,\theta) $, where $M = \frac{dX(1,0)}{d\theta}$, I was stuck then, thanks for any suggestions : )","The matrix is $ M= \frac{d}{d\theta} e^{A+\theta B} \mid _{\theta = 0} $ where $A$ and $B$ are both $n\times n$ matrices. I was thinking solving it by introducing the equations: $\dot x = (A + \theta B)x,\ x(0) = I$ with solution $x = X(t,\theta) $, where $M = \frac{dX(1,0)}{d\theta}$, I was stuck then, thanks for any suggestions : )",,"['matrices', 'ordinary-differential-equations']"
20,How to generate a random matrix whose eigenvalues are less than one,How to generate a random matrix whose eigenvalues are less than one,,"I generate a random matrix. It has this general form: $$\mathbf{B}=\left[ \matrix{ \mathbf{A}_1&\mathbf{A}_2&\ldots&\mathbf{A}_{p-1}&\mathbf{A}_p \\  \mathbf{I}_M&\mathbf{I}_M&\ldots&\mathbf{I}_M&\mathbf{O}_M \\  \vdots &\vdots &\ddots& \vdots& \vdots\\  \mathbf{I}_M&\mathbf{I}_M&\ldots&\mathbf{I}_M&\mathbf{O}_M \\ } \right]:(pM \times pM)$$ in which $\mathbf{A}_i$ is $M \times M$ for $i=1,\ldots,p$ and $\mathbf{O}_M$ ($M\times M$) and $\mathbf{I}_M$ ($M\times M$) are zero and identity matrices. The elements of $\mathbf{A}_i$ are random (they are from a multivariate normal distribution). The eigenvalues of $\mathbf{B}$ should be less than one and I don't want to repeat random number generation process until this happens. I want to change some elements of a generated matrix (whose at least one of its eigenvalues is larger than one) so that all eigenvalues become less than one. Is there any way to do so? (I think I should answer this question: If I change the $B(i,j)$, how will eigenvalues of $B$ be affected? But I don't know the answer). edit: I have this idea: What if I generate a set of random eigenvalues and then generate my matrix based on them. I don't know how to proceed. Thanks.","I generate a random matrix. It has this general form: $$\mathbf{B}=\left[ \matrix{ \mathbf{A}_1&\mathbf{A}_2&\ldots&\mathbf{A}_{p-1}&\mathbf{A}_p \\  \mathbf{I}_M&\mathbf{I}_M&\ldots&\mathbf{I}_M&\mathbf{O}_M \\  \vdots &\vdots &\ddots& \vdots& \vdots\\  \mathbf{I}_M&\mathbf{I}_M&\ldots&\mathbf{I}_M&\mathbf{O}_M \\ } \right]:(pM \times pM)$$ in which $\mathbf{A}_i$ is $M \times M$ for $i=1,\ldots,p$ and $\mathbf{O}_M$ ($M\times M$) and $\mathbf{I}_M$ ($M\times M$) are zero and identity matrices. The elements of $\mathbf{A}_i$ are random (they are from a multivariate normal distribution). The eigenvalues of $\mathbf{B}$ should be less than one and I don't want to repeat random number generation process until this happens. I want to change some elements of a generated matrix (whose at least one of its eigenvalues is larger than one) so that all eigenvalues become less than one. Is there any way to do so? (I think I should answer this question: If I change the $B(i,j)$, how will eigenvalues of $B$ be affected? But I don't know the answer). edit: I have this idea: What if I generate a set of random eigenvalues and then generate my matrix based on them. I don't know how to proceed. Thanks.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
21,A particular Generalized Eigenvalue Problem,A particular Generalized Eigenvalue Problem,,"Data I have three $N \times N$ complex hermitian matrices $A=xx^{H}$,$R=rr^{H}$ and a positive-definite matrix $B$. Here $x$ and $r$ are two $N \times 1$ complex vectors. Let $\lambda_{i}, 1\leq i\leq N$ denotes the N eigenvalues of B which are also positive. Clearly $A$ and $R$ are two rank one positive semi-definite matrices. $B$ is invertible. What I need to find What is the largest eigenvalue of the GEVP? \begin{align} (A\otimes R)v=\gamma (B\otimes R)v \end{align} Will the maximum eigenvalue be (seemingly nice) $||r||^{2}x^{H}B^{-1}x$? What I know Consider the generalized Eigenvalue problem (GEVP) \begin{align} Av=\gamma Bv \end{align} Since $B$ is invertible, this is equivalent to find the eigenvalues of $B^{-1}A$, in fact , since $A$ is rank one matrix, there is only one eigenvalue which will be positive, and it will be given by $x^{H}B^{-1}x$ ($A=xx^{H}$). Now I am interested in the matrices, $A \otimes R$ and $B \otimes R$ which are $N^{2} \times N^{2}$ in dimension. Now $A \otimes R$ is a rank one matrix, and its only non-zero eigenvalue is $||x||^{2}||r||^{2}$. $B \otimes R$ is a positive semi-definite matrix with $N$ of its eigenvalues being $\lambda_{i}||r||^{2}, 1\leq i \leq N$ and rest of the $N^{2}-N$ eigenvalues being zero.","Data I have three $N \times N$ complex hermitian matrices $A=xx^{H}$,$R=rr^{H}$ and a positive-definite matrix $B$. Here $x$ and $r$ are two $N \times 1$ complex vectors. Let $\lambda_{i}, 1\leq i\leq N$ denotes the N eigenvalues of B which are also positive. Clearly $A$ and $R$ are two rank one positive semi-definite matrices. $B$ is invertible. What I need to find What is the largest eigenvalue of the GEVP? \begin{align} (A\otimes R)v=\gamma (B\otimes R)v \end{align} Will the maximum eigenvalue be (seemingly nice) $||r||^{2}x^{H}B^{-1}x$? What I know Consider the generalized Eigenvalue problem (GEVP) \begin{align} Av=\gamma Bv \end{align} Since $B$ is invertible, this is equivalent to find the eigenvalues of $B^{-1}A$, in fact , since $A$ is rank one matrix, there is only one eigenvalue which will be positive, and it will be given by $x^{H}B^{-1}x$ ($A=xx^{H}$). Now I am interested in the matrices, $A \otimes R$ and $B \otimes R$ which are $N^{2} \times N^{2}$ in dimension. Now $A \otimes R$ is a rank one matrix, and its only non-zero eigenvalue is $||x||^{2}||r||^{2}$. $B \otimes R$ is a positive semi-definite matrix with $N$ of its eigenvalues being $\lambda_{i}||r||^{2}, 1\leq i \leq N$ and rest of the $N^{2}-N$ eigenvalues being zero.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
22,norm of power of matrix ---recursive formula,norm of power of matrix ---recursive formula,,"I have a question concerning matrix analysis. let $A$ be the following $n \times n$-matrix with non-negative integer entries. $$\begin{pmatrix}0&k_2&k_3&\dots&k_n\\ k_1&0&k_3&\dots&k_n\\ k_1&k_2&0&\dots&k_n\\ \vdots&\vdots&\vdots&\vdots&\vdots\\ k_1&k_2&k_3&\dots&0\end{pmatrix}$$ i.e. the $j$-th row of $A$ is $(k_1,k_2,\dots k_n)-(0,0,...,k_j,0,0)$ How to express the norm of $A^n$ in terms of $k_1, k_2,\dots, k_n$ and the entries of $A^{(n-1)}$???","I have a question concerning matrix analysis. let $A$ be the following $n \times n$-matrix with non-negative integer entries. $$\begin{pmatrix}0&k_2&k_3&\dots&k_n\\ k_1&0&k_3&\dots&k_n\\ k_1&k_2&0&\dots&k_n\\ \vdots&\vdots&\vdots&\vdots&\vdots\\ k_1&k_2&k_3&\dots&0\end{pmatrix}$$ i.e. the $j$-th row of $A$ is $(k_1,k_2,\dots k_n)-(0,0,...,k_j,0,0)$ How to express the norm of $A^n$ in terms of $k_1, k_2,\dots, k_n$ and the entries of $A^{(n-1)}$???",,"['linear-algebra', 'matrices']"
23,Invertible antisymmetric matrix and identities,Invertible antisymmetric matrix and identities,,"A link to the page is available here . The relevant bit is on P. 15 of the book. I would really appreciate it if somebody could help! It is probably something quite obvious, hence left out by the author, but I don't seem to see it! Could someone please explain the following I've read in Solitons, Instantons and Twistors . (I have changed the notation a bit -- I am more used to $""i,j,k""$) $\xi_i$ are coordinates, with $i=1,...,2n$ Suppose $w^{ij}$ is an invertible, antisymmetric matrix Define $$\{f,g\}:=\sum_{i,j=1}^{2n} w^{ij}(\xi){\partial f\over \partial \xi_i}{\partial g\over \partial \xi_j}$$ and it satisfies $$\{a,\{b,c\}\}+\{b,\{c,a\}\}+\{c,\{a,b\}\}=0$$ Let $W_{ij}:=(w^{-1})_{ij}$ Why is it that it follows that $${\partial W_{jk}\over \partial \xi_i}+{\partial W_{ki}\over \partial \xi_j}+{\partial W_{ij}\over \partial \xi_k}=0,\,\,\,\,\,\forall i,j,k=1,...,2n$$? It is also said that $$w^{ij}(\xi)=\{\xi^i,\xi^j\}$$ Thank you. Please help!","A link to the page is available here . The relevant bit is on P. 15 of the book. I would really appreciate it if somebody could help! It is probably something quite obvious, hence left out by the author, but I don't seem to see it! Could someone please explain the following I've read in Solitons, Instantons and Twistors . (I have changed the notation a bit -- I am more used to $""i,j,k""$) $\xi_i$ are coordinates, with $i=1,...,2n$ Suppose $w^{ij}$ is an invertible, antisymmetric matrix Define $$\{f,g\}:=\sum_{i,j=1}^{2n} w^{ij}(\xi){\partial f\over \partial \xi_i}{\partial g\over \partial \xi_j}$$ and it satisfies $$\{a,\{b,c\}\}+\{b,\{c,a\}\}+\{c,\{a,b\}\}=0$$ Let $W_{ij}:=(w^{-1})_{ij}$ Why is it that it follows that $${\partial W_{jk}\over \partial \xi_i}+{\partial W_{ki}\over \partial \xi_j}+{\partial W_{ij}\over \partial \xi_k}=0,\,\,\,\,\,\forall i,j,k=1,...,2n$$? It is also said that $$w^{ij}(\xi)=\{\xi^i,\xi^j\}$$ Thank you. Please help!",,"['calculus', 'linear-algebra', 'matrices']"
24,How to Find a Finite-Difference Matrix,How to Find a Finite-Difference Matrix,,"I have read several websites trying to explain finite-differential equations, but I haven't been able to find one that explains how it's put into the matrix form. $f(x) = -\frac{d^2u}{dx^2}$ where $u(0) = 0$ and $u(1) = 0$ becomes $\begin{bmatrix} 2 & -1 & 0 & 0 & 0 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & -1 \\ 0 & 0 & 0 & -1 & 2 \end{bmatrix}$ multiplied by one column of values between $u_1$ to $u_5$ is equal to $h^2$$\begin{bmatrix} f(h) \\ f(2h) \\ f(3h) \\ f(4h) \\ f(5h) \end{bmatrix}$ Looking at the equation that causes this matrix, it confuses me.  The difference equation is $$-u_{j+1}+2u_j-u_{j-1}=h^2f(jh)$$ The initial term, when $j=1$, should make it so that the equation is: $$u_2+2u_1-u_0$$ Would this not cause the first row to become $(-1, 2, 0, 0, 0)$ because $u_0$ has been defined as 0?  I know that my thinking is wrong, since the book tells me so, but I don't understand how the first and last row is determined. On a related note, how does the matrix equation if the boundaries are changed?  For an example, if $u_0 = 1$ and $u_1 = 2$ on the original equation.  Will the answer become $\begin{bmatrix} 2 & 0 & 0 & 0 & 0 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & -1 \\ 0 & 0 & 0 & -1 & 2 \end{bmatrix}$","I have read several websites trying to explain finite-differential equations, but I haven't been able to find one that explains how it's put into the matrix form. $f(x) = -\frac{d^2u}{dx^2}$ where $u(0) = 0$ and $u(1) = 0$ becomes $\begin{bmatrix} 2 & -1 & 0 & 0 & 0 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & -1 \\ 0 & 0 & 0 & -1 & 2 \end{bmatrix}$ multiplied by one column of values between $u_1$ to $u_5$ is equal to $h^2$$\begin{bmatrix} f(h) \\ f(2h) \\ f(3h) \\ f(4h) \\ f(5h) \end{bmatrix}$ Looking at the equation that causes this matrix, it confuses me.  The difference equation is $$-u_{j+1}+2u_j-u_{j-1}=h^2f(jh)$$ The initial term, when $j=1$, should make it so that the equation is: $$u_2+2u_1-u_0$$ Would this not cause the first row to become $(-1, 2, 0, 0, 0)$ because $u_0$ has been defined as 0?  I know that my thinking is wrong, since the book tells me so, but I don't understand how the first and last row is determined. On a related note, how does the matrix equation if the boundaries are changed?  For an example, if $u_0 = 1$ and $u_1 = 2$ on the original equation.  Will the answer become $\begin{bmatrix} 2 & 0 & 0 & 0 & 0 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & -1 \\ 0 & 0 & 0 & -1 & 2 \end{bmatrix}$",,"['linear-algebra', 'matrices']"
25,inequality on inner product,inequality on inner product,,"Let $x \in \Bbb R^n$ and $Q \in M_{n \times n}(\Bbb R)$, where $Q$ is hermitian and negative definite. Let $(\cdot,\cdot)$ be the usual euclidian inner product. I need to prove the following inequality: $$(x,Qx) \le a(x,x),$$ where $a$ is the maximum eigenvalue of $Q$. Any idea?","Let $x \in \Bbb R^n$ and $Q \in M_{n \times n}(\Bbb R)$, where $Q$ is hermitian and negative definite. Let $(\cdot,\cdot)$ be the usual euclidian inner product. I need to prove the following inequality: $$(x,Qx) \le a(x,x),$$ where $a$ is the maximum eigenvalue of $Q$. Any idea?",,"['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors', 'inner-products']"
26,Antiderivative of $x \mapsto \operatorname{tr}\{(\mathbf{A}x+\mathbf{B})^{-1}\mathbf{C}\}$,Antiderivative of,x \mapsto \operatorname{tr}\{(\mathbf{A}x+\mathbf{B})^{-1}\mathbf{C}\},"I am wondering how to integrate the function $$x \mapsto \operatorname{tr}\bigl\{(\mathbf{A}x+\mathbf{B})^{-1}\mathbf{C}\bigr\}$$ In my case, the matrices $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$ are (strictly) positive definite. If $\mathbf{C} = \mathbf{A}$ it is easy to see that $$\ln\det(\mathbf{A}x+\mathbf{B})$$ is an antiderivative. But what if $\mathbf{C} \neq \mathbf{A}$? Thanks, jens","I am wondering how to integrate the function $$x \mapsto \operatorname{tr}\bigl\{(\mathbf{A}x+\mathbf{B})^{-1}\mathbf{C}\bigr\}$$ In my case, the matrices $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$ are (strictly) positive definite. If $\mathbf{C} = \mathbf{A}$ it is easy to see that $$\ln\det(\mathbf{A}x+\mathbf{B})$$ is an antiderivative. But what if $\mathbf{C} \neq \mathbf{A}$? Thanks, jens",,"['calculus', 'matrices', 'integration', 'improper-integrals']"
27,Nearest matrix in doubly stochastic matrix set,Nearest matrix in doubly stochastic matrix set,,"Suppose $\mathcal{D}_N$ denote an $N\times N$ doubly stochastic matrix , given any element $M\in \mathcal{D}_N$ , the singular value decomposition for $M$ is  $$ M=USV'$$ where $U$ and $V$ are two $N\times N$ orthogonal matrix and $S$ is a $N \times N$ diagonal matrix Let $P$ be the 'closest' orthogonal matrix to $M$, i.e. $P=\arg\min_{X\in\mathcal{O}}||X-M||_F^2$,$\mathcal{O}$ represents the $N\times N$ orthogonal matrix set. Note such $P$ may be not unique. In this case, we choose any of it. On conclusion about $P$ is $P=UV'$, where $U$ and $V$ are defined before(although can be not unique, we just choose any of them) $M_1 \in \mathcal{D}_N$, which is 'closest' to $P$. More specifically $$ M_1 = \arg\min_{X\in\mathcal{D}} ||X - P||_F ^2 $$ Similarly, If $M_1$ is not unique, we choose any of it(This should not happen actually. Since we may image it as a 'ball' approaching a 'polygon', should have only one minimum) My question is : The statement: $M_1=M$ if and only if $M$ is a permutation matrix Does this statement always hold true? Actually, if $M$ is a permutation matrix, $M_1=M$, this is obvious, since $S=I$, and $P=M$. However, does another direction always hold true? If so, how to prove this, otherwise, how to give a counter-example? Thanks for any suggestions!","Suppose $\mathcal{D}_N$ denote an $N\times N$ doubly stochastic matrix , given any element $M\in \mathcal{D}_N$ , the singular value decomposition for $M$ is  $$ M=USV'$$ where $U$ and $V$ are two $N\times N$ orthogonal matrix and $S$ is a $N \times N$ diagonal matrix Let $P$ be the 'closest' orthogonal matrix to $M$, i.e. $P=\arg\min_{X\in\mathcal{O}}||X-M||_F^2$,$\mathcal{O}$ represents the $N\times N$ orthogonal matrix set. Note such $P$ may be not unique. In this case, we choose any of it. On conclusion about $P$ is $P=UV'$, where $U$ and $V$ are defined before(although can be not unique, we just choose any of them) $M_1 \in \mathcal{D}_N$, which is 'closest' to $P$. More specifically $$ M_1 = \arg\min_{X\in\mathcal{D}} ||X - P||_F ^2 $$ Similarly, If $M_1$ is not unique, we choose any of it(This should not happen actually. Since we may image it as a 'ball' approaching a 'polygon', should have only one minimum) My question is : The statement: $M_1=M$ if and only if $M$ is a permutation matrix Does this statement always hold true? Actually, if $M$ is a permutation matrix, $M_1=M$, this is obvious, since $S=I$, and $P=M$. However, does another direction always hold true? If so, how to prove this, otherwise, how to give a counter-example? Thanks for any suggestions!",,"['linear-algebra', 'matrices', 'optimization', 'permutations']"
28,Set of symmetric positive semidefinite matrices is a full dimensional convex cone.,Set of symmetric positive semidefinite matrices is a full dimensional convex cone.,,"If $S_n^+$ is the set of all symmetric positive semidefinite $n \times n$ matrices with entries in $\mathbb{R}$, how does it follow that it is a full dimensional closed convex cone in $\mathbb{R}^{n^2}$? I don't understand the relation between positive semidefiniteness and convexity of a cone.","If $S_n^+$ is the set of all symmetric positive semidefinite $n \times n$ matrices with entries in $\mathbb{R}$, how does it follow that it is a full dimensional closed convex cone in $\mathbb{R}^{n^2}$? I don't understand the relation between positive semidefiniteness and convexity of a cone.",,"['matrices', 'symmetric-matrices', 'positive-semidefinite', 'convex-cone']"
29,Need help to understand a theorem,Need help to understand a theorem,,"I have been reading a theorem related with the existence of the outer generalized inverse of a matrix where i have certain difficulties to understand the theorem. Theorem is as follows. Let $A\in\mathbb{C}^{m\times n}$, rank$(A) = r$, and let $T$ and $S$ be a subspace of $\mathbb{C^n}$ and $\mathbb{C^m}$, respectively, with $\dim (T )= \dim (S^\perp)=t\leq r$. Then, $A$    has a $\{2\}$ - inverse X such that $R(X) = T$ and $N(X) = S$ iff one of the following condition is satisfied (where $R(X)$ and $N(X)$ denots the range and null space of $X$, respectively) $AT\oplus S$ = $C^{m}$ $P_{S}{^\perp} AT = S^{\perp}$ $A^*S^\perp\oplus T^\perp$ = $C^{n}$ $P_T~ A^*S^\perp = T$ $\{2\}$ - inverse  of a matrix $A$ is a $n\times m$ matrix $X$ satisfying matrix equation $XAX = X$. All above conditions are equivalent. $P_{L.M}$ stands for the projection on to the space $L$ parallel to $M$ while $P_{L}$ stands for orthogonal projection onto sub space $L$ parallel to $L^\perp$. Earlier i have posted same theorem where i was not clear about $AT$. Now that is cleared to me by answer given by David mitra . I need a proper interpretation of these terms $P_{S}{^\perp} AT = S^{\perp}$, $P_T~ A^*S^\perp = T$, $A^*S^\perp$. it is given in the theorem that  $AT\oplus S$ = $C^{m}$ that means that there must exist projction operator $P_{AT}{S}$ .E projection onto subspace $AT$ parallel to $S$. Also we have $dim (AT) = \dim S^\perp$. I don't need proof.  It has some connection with direct sum of sub spaces and projection associated with that. I just need their interpretation. I have to use this theorem for my own work. But how can i use if the things are not cleared to me? I really need help so that i can proceed further. Heartily thanks for giving me your precious time.","I have been reading a theorem related with the existence of the outer generalized inverse of a matrix where i have certain difficulties to understand the theorem. Theorem is as follows. Let $A\in\mathbb{C}^{m\times n}$, rank$(A) = r$, and let $T$ and $S$ be a subspace of $\mathbb{C^n}$ and $\mathbb{C^m}$, respectively, with $\dim (T )= \dim (S^\perp)=t\leq r$. Then, $A$    has a $\{2\}$ - inverse X such that $R(X) = T$ and $N(X) = S$ iff one of the following condition is satisfied (where $R(X)$ and $N(X)$ denots the range and null space of $X$, respectively) $AT\oplus S$ = $C^{m}$ $P_{S}{^\perp} AT = S^{\perp}$ $A^*S^\perp\oplus T^\perp$ = $C^{n}$ $P_T~ A^*S^\perp = T$ $\{2\}$ - inverse  of a matrix $A$ is a $n\times m$ matrix $X$ satisfying matrix equation $XAX = X$. All above conditions are equivalent. $P_{L.M}$ stands for the projection on to the space $L$ parallel to $M$ while $P_{L}$ stands for orthogonal projection onto sub space $L$ parallel to $L^\perp$. Earlier i have posted same theorem where i was not clear about $AT$. Now that is cleared to me by answer given by David mitra . I need a proper interpretation of these terms $P_{S}{^\perp} AT = S^{\perp}$, $P_T~ A^*S^\perp = T$, $A^*S^\perp$. it is given in the theorem that  $AT\oplus S$ = $C^{m}$ that means that there must exist projction operator $P_{AT}{S}$ .E projection onto subspace $AT$ parallel to $S$. Also we have $dim (AT) = \dim S^\perp$. I don't need proof.  It has some connection with direct sum of sub spaces and projection associated with that. I just need their interpretation. I have to use this theorem for my own work. But how can i use if the things are not cleared to me? I really need help so that i can proceed further. Heartily thanks for giving me your precious time.",,"['linear-algebra', 'matrices', 'reference-request']"
30,Rewriting automorphism of matrix algebra in terms of automorphisms of the underlying ring?,Rewriting automorphism of matrix algebra in terms of automorphisms of the underlying ring?,,"I've used the following idea as a black box for some time now, but it occurred to me I don't fully understand why it's true. Suppose $A=M_n(R)$ is the algebra of square matrices over some division ring $R$. Then for any $\phi\in\operatorname{Aut}(A)$, we can actually write $\phi$ as the composition of an automorphism induced by an automorphism $\psi$ of $R$ and the conjugation by some unit of $A$.  More explicitly, for $\psi\in\operatorname{Aut}(R)$, this induces an automorphism $\tilde{\psi}$ of $A$ by applying $\psi$ to each of the entries in the matrix, for example, $$ M=\begin{pmatrix} a_{11} & a_{12}\\ a_{21} & a_{22} \end{pmatrix} \mapsto \tilde{\psi}(M)\begin{pmatrix} \psi(a_{11}) & \psi(a_{12})\\ \psi(a_{21}) & \psi(a_{22}) \end{pmatrix} $$ and then we can conjugate by an invertible matrix in $A$, say $N$, to get $N\tilde{\psi}(M)N^{-1}$. I don't think the order of applying $\tilde{\psi}$ or conjugating matters, since if I conjugate first, then I could apply a different $\tilde{\psi}$. So the composition would be something like $\phi=\varphi_N\circ\tilde{\psi}$ where $\varphi_N$ is the conjugation by $N$ map. My question is, why can any automorphism $\phi$ of $A$ actually be decomposed in this way?","I've used the following idea as a black box for some time now, but it occurred to me I don't fully understand why it's true. Suppose $A=M_n(R)$ is the algebra of square matrices over some division ring $R$. Then for any $\phi\in\operatorname{Aut}(A)$, we can actually write $\phi$ as the composition of an automorphism induced by an automorphism $\psi$ of $R$ and the conjugation by some unit of $A$.  More explicitly, for $\psi\in\operatorname{Aut}(R)$, this induces an automorphism $\tilde{\psi}$ of $A$ by applying $\psi$ to each of the entries in the matrix, for example, $$ M=\begin{pmatrix} a_{11} & a_{12}\\ a_{21} & a_{22} \end{pmatrix} \mapsto \tilde{\psi}(M)\begin{pmatrix} \psi(a_{11}) & \psi(a_{12})\\ \psi(a_{21}) & \psi(a_{22}) \end{pmatrix} $$ and then we can conjugate by an invertible matrix in $A$, say $N$, to get $N\tilde{\psi}(M)N^{-1}$. I don't think the order of applying $\tilde{\psi}$ or conjugating matters, since if I conjugate first, then I could apply a different $\tilde{\psi}$. So the composition would be something like $\phi=\varphi_N\circ\tilde{\psi}$ where $\varphi_N$ is the conjugation by $N$ map. My question is, why can any automorphism $\phi$ of $A$ actually be decomposed in this way?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'reference-request', 'modules']"
31,Product of matrices; MAPLE giving a strange answer,Product of matrices; MAPLE giving a strange answer,,"Either my brain is seriously fried up right now or the computer is wrong. If I have a matrix $\begin{bmatrix} 4 & -2\\  2 & -1 \\  0 &  0 \end{bmatrix}$ multiply by its transpose $\begin{bmatrix} 4 &2  &0 \\  -2&-1 &0  \end{bmatrix}$, I should get a $3 \times 3$ $\begin{bmatrix} 20 &10  &0 \\  10 &5  &0 \\   0&0  &0  \end{bmatrix}$ For some reason Maple is giving me a $2 \times 2$ $\begin{bmatrix} 20 &10 \\  10 &5   \\    \end{bmatrix}$ Why did they delete the last row and column of 0s? You can't do that","Either my brain is seriously fried up right now or the computer is wrong. If I have a matrix $\begin{bmatrix} 4 & -2\\  2 & -1 \\  0 &  0 \end{bmatrix}$ multiply by its transpose $\begin{bmatrix} 4 &2  &0 \\  -2&-1 &0  \end{bmatrix}$, I should get a $3 \times 3$ $\begin{bmatrix} 20 &10  &0 \\  10 &5  &0 \\   0&0  &0  \end{bmatrix}$ For some reason Maple is giving me a $2 \times 2$ $\begin{bmatrix} 20 &10 \\  10 &5   \\    \end{bmatrix}$ Why did they delete the last row and column of 0s? You can't do that",,['matrices']
32,Eigenvalues of block matrix with a zero diagonal block,Eigenvalues of block matrix with a zero diagonal block,,"I'm stuck on finding the eigenvalues of $$ \bar{A} =  \begin{bmatrix} 0 & S\\ S^\top & A \end{bmatrix} $$ Both $S$ and $A$ are square matrices of the same dimension and are invertible. $A$ is symmetric positive definite. Any help is appreciated. :-D","I'm stuck on finding the eigenvalues of $$ \bar{A} =  \begin{bmatrix} 0 & S\\ S^\top & A \end{bmatrix} $$ Both $S$ and $A$ are square matrices of the same dimension and are invertible. $A$ is symmetric positive definite. Any help is appreciated. :-D",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
33,About positive-definiteness of a matrix Q depending on matrices H and P,About positive-definiteness of a matrix Q depending on matrices H and P,,"$H + H^T$ is a positive definite matrix and $P$ is also a positive definite matrix. Will $Q = PH + H^TP$ be a positive definite matrix? In my calculations, it is not positive definite. But I read a paper saying that $Q$ should be positive definite. Is it so?","$H + H^T$ is a positive definite matrix and $P$ is also a positive definite matrix. Will $Q = PH + H^TP$ be a positive definite matrix? In my calculations, it is not positive definite. But I read a paper saying that $Q$ should be positive definite. Is it so?",,"['linear-algebra', 'matrices']"
34,Fast PCA: how to compute and use the covariance of x,Fast PCA: how to compute and use the covariance of x,,"I'm trying to understand the paper Fast principal component analysis using fixed-point algorithm by Alok Sharma and Kuldip K. Paliwal (1151–1155) , and especially what is said about $\Sigma_x$, the covariance of x. But before being specific, let me summarize how I understand the algorithm. The PCA finds a linear transformation matrix $\varphi$ of size $d \times h$ which is meant to reduce a set of $n$ d-dimensional feature vectors $x$ ($x \in \mathbb{R}^d$) to a set of $n$ h-dimensional feature vectors $y$ ($y \in \mathbb{R}^h$), with $h < d$. So given one feature vector $x$, we have $\varphi x \rightarrow y$, or: $\pmatrix{     a_{1,1} & a_{1,2} & \cdots & a_{1,h} \\     a_{2,1} & a_{2,2} & \cdots & a_{2,h} \\     \vdots  & \vdots  & \ddots & \vdots  \\     a_{d,1} & a_{d,2} & \cdots & a_{d,h} }\pmatrix{     x_{1}  \\     x_{2}  \\     \vdots \\     x_{d} } \rightarrow \pmatrix{     y_{1}  \\     y_{2}  \\     \vdots \\     y_{h} }$ And we want to define the matrix $\varphi$. Let me quote the algorithm (Table 1): Choose $h$, the number of principal axes or eigenvectors required to estimate. Compute covariance $\Sigma_x$ and set $p \leftarrow 1$ Initialize eigenvector $\varphi_p$ of size $d \times 1$ e.g. randomly Update $\varphi_p$ as $\varphi_p \leftarrow \Sigma_x \varphi_p$ Do the Gram-Schmidt orthogonalization process [...] Normalize $\varphi_p$ by dividing it by its norm: $\varphi_p \leftarrow \varphi_p/||\varphi_p||$ If $\varphi_p$ has not converged, go back to step 3 Increment counter $p \leftarrow p + 1$ and go to step 2 until $p$ equals $h$ So basically, the orthogonalization process and the normalization are pretty straightforward and simple to implement, same for the convergence. Unfortunately, I'm having hard time trying to figure out the first steps: How am I supposed to compute the covariance $\Sigma_x$ given that $x$ is one of the feature vector of the input? (BTW, is the described algorithm actually only explaining the definition of $\Sigma$ for one single features vector of the $n$ input vectors?) I was unable to understand how to apply the definition of the covariance matrix to that case; is it possible to have a simple example of what it takes in input, and what gets out? Now suppose I am able to compute the covariance $\Sigma_x$, what does step 2 and 3 means? $\varphi_p$ is supposed to be one column of the $d \times h$ matrix $\varphi$, so what does the random initialization of $\varphi_p$ means and how to apply the transformation $\varphi_p \leftarrow \Sigma_x\varphi_p$ using the previously computed covariance?","I'm trying to understand the paper Fast principal component analysis using fixed-point algorithm by Alok Sharma and Kuldip K. Paliwal (1151–1155) , and especially what is said about $\Sigma_x$, the covariance of x. But before being specific, let me summarize how I understand the algorithm. The PCA finds a linear transformation matrix $\varphi$ of size $d \times h$ which is meant to reduce a set of $n$ d-dimensional feature vectors $x$ ($x \in \mathbb{R}^d$) to a set of $n$ h-dimensional feature vectors $y$ ($y \in \mathbb{R}^h$), with $h < d$. So given one feature vector $x$, we have $\varphi x \rightarrow y$, or: $\pmatrix{     a_{1,1} & a_{1,2} & \cdots & a_{1,h} \\     a_{2,1} & a_{2,2} & \cdots & a_{2,h} \\     \vdots  & \vdots  & \ddots & \vdots  \\     a_{d,1} & a_{d,2} & \cdots & a_{d,h} }\pmatrix{     x_{1}  \\     x_{2}  \\     \vdots \\     x_{d} } \rightarrow \pmatrix{     y_{1}  \\     y_{2}  \\     \vdots \\     y_{h} }$ And we want to define the matrix $\varphi$. Let me quote the algorithm (Table 1): Choose $h$, the number of principal axes or eigenvectors required to estimate. Compute covariance $\Sigma_x$ and set $p \leftarrow 1$ Initialize eigenvector $\varphi_p$ of size $d \times 1$ e.g. randomly Update $\varphi_p$ as $\varphi_p \leftarrow \Sigma_x \varphi_p$ Do the Gram-Schmidt orthogonalization process [...] Normalize $\varphi_p$ by dividing it by its norm: $\varphi_p \leftarrow \varphi_p/||\varphi_p||$ If $\varphi_p$ has not converged, go back to step 3 Increment counter $p \leftarrow p + 1$ and go to step 2 until $p$ equals $h$ So basically, the orthogonalization process and the normalization are pretty straightforward and simple to implement, same for the convergence. Unfortunately, I'm having hard time trying to figure out the first steps: How am I supposed to compute the covariance $\Sigma_x$ given that $x$ is one of the feature vector of the input? (BTW, is the described algorithm actually only explaining the definition of $\Sigma$ for one single features vector of the $n$ input vectors?) I was unable to understand how to apply the definition of the covariance matrix to that case; is it possible to have a simple example of what it takes in input, and what gets out? Now suppose I am able to compute the covariance $\Sigma_x$, what does step 2 and 3 means? $\varphi_p$ is supposed to be one column of the $d \times h$ matrix $\varphi$, so what does the random initialization of $\varphi_p$ means and how to apply the transformation $\varphi_p \leftarrow \Sigma_x\varphi_p$ using the previously computed covariance?",,"['linear-algebra', 'matrices', 'principal-component-analysis']"
35,Matrix Analysis in $\mathbb{C}$,Matrix Analysis in,\mathbb{C},"1.Let $A \in GL_n(\mathbb{C})$. Show that $\det(I+A)=1+\operatorname{tr}(A)+ \epsilon(A)$ where Modulas of epsilon(A) by norm of A=0 as A tends to 0,for any matrix norm. If I define J(A)= det(A) for A is in GLn(C),then J is differentiable at all such A and that,if H is in Mn(C),then J'(A)(H)=det(A)tr(A*H). where A* is inverse of A. Can any one say how to write latex here?","1.Let $A \in GL_n(\mathbb{C})$. Show that $\det(I+A)=1+\operatorname{tr}(A)+ \epsilon(A)$ where Modulas of epsilon(A) by norm of A=0 as A tends to 0,for any matrix norm. If I define J(A)= det(A) for A is in GLn(C),then J is differentiable at all such A and that,if H is in Mn(C),then J'(A)(H)=det(A)tr(A*H). where A* is inverse of A. Can any one say how to write latex here?",,"['matrices', 'functional-analysis']"
36,Inverting $(A + D_p)$ efficiently for many diagonal $D_p$ (and $A$ SPD),Inverting  efficiently for many diagonal  (and  SPD),(A + D_p) D_p A,"At one step of an algorithm (i=1: something big) I have to draw a vector from the normal distribution $X_p^{(i)}\sim N((A^{(i)} + D^{(i)}_p)^{-1}m, (A^{(i)} + D^{(i)}_p)^{-1})$ for $p=1:P$, with a large $P$ (say 5,000). $A^{(i)}$ is symmetric positive definite and rank $K$, $A^{(i)}$ is $K\times K$ symmetric positive definite with $K$ is much smaller than $P$. $(A^{(i)})^{-1}$ is cheap relative to the rest of the calculation. So I'd like to efficiently get $(A + D^{(i)}_p)^{-1}$ (or a matrix square root). I've tried using the matrix inversion lemma (aka Woodbury) to get a quick update from $A^{-1}$ or $(A+D_p)^{-1}$ to $(A+D_{p+1})^{-1}$, but it takes me in circles because it's a full rank update. It feels like this should have a simple solution since the $D^{(i)}_p$ are all diagonal, but if there is it escapes me...","At one step of an algorithm (i=1: something big) I have to draw a vector from the normal distribution $X_p^{(i)}\sim N((A^{(i)} + D^{(i)}_p)^{-1}m, (A^{(i)} + D^{(i)}_p)^{-1})$ for $p=1:P$, with a large $P$ (say 5,000). $A^{(i)}$ is symmetric positive definite and rank $K$, $A^{(i)}$ is $K\times K$ symmetric positive definite with $K$ is much smaller than $P$. $(A^{(i)})^{-1}$ is cheap relative to the rest of the calculation. So I'd like to efficiently get $(A + D^{(i)}_p)^{-1}$ (or a matrix square root). I've tried using the matrix inversion lemma (aka Woodbury) to get a quick update from $A^{-1}$ or $(A+D_p)^{-1}$ to $(A+D_{p+1})^{-1}$, but it takes me in circles because it's a full rank update. It feels like this should have a simple solution since the $D^{(i)}_p$ are all diagonal, but if there is it escapes me...",,"['linear-algebra', 'matrices', 'statistics']"
37,Eigenvalues of a 8x8 Matrix,Eigenvalues of a 8x8 Matrix,,"This is the 6th problem of the TACA in August, 2023. One have 2 hours to solve 15 problems. I am wondering how to calculate the eigenvalues of the following 8 by 8 matrix by hand. Note that this is NOT a circulant matrix. $A=\begin{pmatrix} 10  &-9  &  &  &  &  &  &-9 \\ -9  &10  &-9  &  &  &  &  & \\   &9  &10  &9  &  &  &  & \\   &  &9  &10  &9  &  &  & \\   &  &  &9  &10  &9  &  & \\   &  &  &  &-9  &10  &-9  & \\   &  &  &  &  &-9  &10  &-9 \\ -9  &  &  &  &  &  &-9  &10 \end{pmatrix}$ (Required in the contest: Let $\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_8$ be the eigenvalues of A, find the value of $\lambda_6$ .I have no idea if this makes it simpler.) Calculated with computers, $\lambda_1=\lambda_2=10-9\sqrt{2},\lambda_3=\lambda_4=\lambda_5=\lambda_6=10,\lambda_7=\lambda_8=10+9\sqrt{2}$ I have no idea how to solve $|\lambda I-A|=O$ in a normal way. Maybe I need new methods? When I participated, I mistook A for the following circulant matrix B. $B=\begin{pmatrix} 10  &-9  &  &  &  &  &  &-9 \\ -9  &10  &-9  &  &  &  &  & \\   &-9  &10  &-9  &  &  &  & \\   &  &-9  &10  &-9  &  &  & \\   &  &  &-9  &10  &-9  &  & \\   &  &  &  &-9  &10  &-9  & \\   &  &  &  &  &-9  &10  &-9 \\ -9  &  &  &  &  &  &-9  &10 \end{pmatrix}$ I took the 8th unit roots to $-9x^7-9x+10$ and got $\lambda_1=10-18,\lambda_2=\lambda_3=10-9\sqrt{2},\lambda_4=\lambda_5=10,\lambda_6=\lambda_7=10+9\sqrt{2},\lambda_8=10+18$ . I don't know if this helps with the original problem either.","This is the 6th problem of the TACA in August, 2023. One have 2 hours to solve 15 problems. I am wondering how to calculate the eigenvalues of the following 8 by 8 matrix by hand. Note that this is NOT a circulant matrix. (Required in the contest: Let be the eigenvalues of A, find the value of .I have no idea if this makes it simpler.) Calculated with computers, I have no idea how to solve in a normal way. Maybe I need new methods? When I participated, I mistook A for the following circulant matrix B. I took the 8th unit roots to and got . I don't know if this helps with the original problem either.","A=\begin{pmatrix}
10  &-9  &  &  &  &  &  &-9 \\
-9  &10  &-9  &  &  &  &  & \\
  &9  &10  &9  &  &  &  & \\
  &  &9  &10  &9  &  &  & \\
  &  &  &9  &10  &9  &  & \\
  &  &  &  &-9  &10  &-9  & \\
  &  &  &  &  &-9  &10  &-9 \\
-9  &  &  &  &  &  &-9  &10
\end{pmatrix} \lambda_1\leq\lambda_2\leq\cdots\leq\lambda_8 \lambda_6 \lambda_1=\lambda_2=10-9\sqrt{2},\lambda_3=\lambda_4=\lambda_5=\lambda_6=10,\lambda_7=\lambda_8=10+9\sqrt{2} |\lambda I-A|=O B=\begin{pmatrix}
10  &-9  &  &  &  &  &  &-9 \\
-9  &10  &-9  &  &  &  &  & \\
  &-9  &10  &-9  &  &  &  & \\
  &  &-9  &10  &-9  &  &  & \\
  &  &  &-9  &10  &-9  &  & \\
  &  &  &  &-9  &10  &-9  & \\
  &  &  &  &  &-9  &10  &-9 \\
-9  &  &  &  &  &  &-9  &10
\end{pmatrix} -9x^7-9x+10 \lambda_1=10-18,\lambda_2=\lambda_3=10-9\sqrt{2},\lambda_4=\lambda_5=10,\lambda_6=\lambda_7=10+9\sqrt{2},\lambda_8=10+18","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'contest-math']"
38,Can't seem to get the correct result by diagonalizing a matrix.,Can't seem to get the correct result by diagonalizing a matrix.,,"I am trying to understand parts of the authors solution given to the following question: The generators of $\mathrm{SO}(3)$ can be chosen as $t^1=\begin{pmatrix}0 & 0 & 0\\ 0 & 0 & -i \\\ 0 & i & 0 \\ \end{pmatrix},\ t^2=\begin{pmatrix}0 & 0 & i\\ 0 & 0 & 0 \\ -i & 0 & 0 \\ \end{pmatrix},\ t^3=   \begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}.$ Diagolizing $t^3$ , find the $\mathrm{SO}(3)$ group element $\mathrm{R}(\theta)=\exp(-i\theta t^3)$ . The author's solution is: To diagonalize $t^3$ , we first have to find its eigenvalues by solving $\det(t^3-\mathbb{I}\lambda)=\det\begin{pmatrix}-\lambda & -i & 0\\ i & -\lambda & 0 \\ 0 & 0 & -\lambda\\\end{pmatrix}=(\lambda^2-1)\times(-\lambda)=0.$ The eigenvalues are therefore $\lambda=0$ and $\lambda=\pm 1$ . The eigenvector corresponding to $\lambda=0$ is clearly $(0,0,1)^T$ and the eigenvectors corresponding to $\lambda=\pm 1$ are found by solving $\begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}a\\ b \\ 0\\ \end{pmatrix}=\begin{pmatrix}-ib\\ ia \\ 0\\ \end{pmatrix}=\pm \begin{pmatrix}a\\ b \\ 0\\ \end{pmatrix}\tag{1}$ which gives $\frac{1}{\sqrt{2}}\left(1,\pm i,0\right)\tag{2}.$ Therefore we can write $$t^3=U^\dagger \hat t U\tag{3}$$ where $U=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i & 0\\ 1 & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix},\tag{4}$ . $\hat t=\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\tag{5}$ I don't need to type any more of the solution as I don't understand eqns. $(2)-(4)$ Should eqn. $(2)$ actually read $\frac{1}{\sqrt{2}}\left(1,\pm i,0\right)^\dagger=\frac{1}{\sqrt{2}}\begin{pmatrix}1\\ \mp i \\ 0\\ \end{pmatrix}, \, \text{for} \ \lambda=\pm 1?$ If true then $t^3$ has eigenvectors $\frac{1}{\sqrt{2}}\begin{pmatrix}1\\ -i \\ 0\\ \end{pmatrix} \text{if}\,\,\lambda = +1\,\quad\ \frac{1}{\sqrt{2}}\begin{pmatrix}1\\ i \\ 0\\ \end{pmatrix} \text{if}\,\, \lambda = -1,\quad\ \frac{1}{\sqrt{2}}\begin{pmatrix}0\\ 0 \\ \sqrt{2}\\ \end{pmatrix} \text{if}\, \lambda = 0\tag{a}$ Then writing the matrix of eigenvectors, $U$ , with the eigenvectors of $(\mathrm{a})$ as columns in the same order as the eigenvalues in $(5)$ should yield $U=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}=\begin{pmatrix}1/\sqrt2 & 1/\sqrt2 & 0\\ -i/\sqrt2 & i/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}$ which is not the same as eqn. $(4)$ . In fact, this is the transpose of eqn. $(4)$ . For eqn. $(3)$ , $\det U = i$ , $U$ is unitary (as the rows are orthonormal), so $U^\dagger=U^{-1}$ . In order to find the inverse of $U$ , I first take its transpose $U^T=\begin{pmatrix}1/\sqrt2 & -i/\sqrt2 & 0\\ 1/\sqrt2 & i/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}\tag{b}$ then replacing each element of $(\mathrm{b})$ with its cofactor with the associated signature $\begin{pmatrix}+ & - & +\\ - & + & - \\\ + & - & + \\ \end{pmatrix}$ .   I find that $U^{-1}=\frac{1}{\det U}\begin{pmatrix}+\begin{vmatrix}i/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & -\begin{vmatrix}1/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & 0\\ -\begin{vmatrix}-i/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & +\begin{vmatrix}1/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & 0 \\ 0 & 0 & +\begin{vmatrix}1/\sqrt2 & -i/\sqrt2\\ 1/\sqrt2 & i/\sqrt2 \\ \end{vmatrix} \\ \end{pmatrix}$ . $=-i\begin{pmatrix}i/\sqrt2 & -1/\sqrt2 & 0\\ i/\sqrt2 & 1/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}=\frac{1}{\sqrt2}\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}$ . Putting this altogether and omitting the calculation details , $U^\dagger \hat t U=\frac12\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}$$$$=\begin{pmatrix}0 & 1 & 0\\ 1 & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix}\ne t^3\tag{c}$ . But using the version for $U$ as given in eqn. $(4)$ of the authors solution and again omitting the calculation details , $U^\dagger \hat t U=\frac12\begin{pmatrix}1 & 1 & 0\\ i & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & -i & 0\\ 1 & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}$$$$=\begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix} = t^3\tag{d}$ Interestingly, if I write (with the omission of the calculation details ), $U \hat t U^\dagger=\frac12\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}$$$$=\begin{pmatrix}0 & i & 0\\ -i & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix}=-t^3\tag{e}$ I've stared at this for sometime now but I just cannot understand why the author's solution, $(\mathrm{d})$ , gives the correct result, but my attempt in eqn. $(\mathrm{c})$ $(\text{or}\, (\mathrm{e}))$ does not. Can someone please explain where I am going wrong here? (Sorry for the lengthy post, I've trying to regain some linear algebra skills so needed to show a lot of working).","I am trying to understand parts of the authors solution given to the following question: The generators of can be chosen as Diagolizing , find the group element . The author's solution is: To diagonalize , we first have to find its eigenvalues by solving The eigenvalues are therefore and . The eigenvector corresponding to is clearly and the eigenvectors corresponding to are found by solving which gives Therefore we can write where . I don't need to type any more of the solution as I don't understand eqns. Should eqn. actually read If true then has eigenvectors Then writing the matrix of eigenvectors, , with the eigenvectors of as columns in the same order as the eigenvalues in should yield which is not the same as eqn. . In fact, this is the transpose of eqn. . For eqn. , , is unitary (as the rows are orthonormal), so . In order to find the inverse of , I first take its transpose then replacing each element of with its cofactor with the associated signature .   I find that . . Putting this altogether and omitting the calculation details , . But using the version for as given in eqn. of the authors solution and again omitting the calculation details , Interestingly, if I write (with the omission of the calculation details ), I've stared at this for sometime now but I just cannot understand why the author's solution, , gives the correct result, but my attempt in eqn. does not. Can someone please explain where I am going wrong here? (Sorry for the lengthy post, I've trying to regain some linear algebra skills so needed to show a lot of working).","\mathrm{SO}(3) t^1=\begin{pmatrix}0 & 0 & 0\\ 0 & 0 & -i \\\ 0 & i & 0 \\ \end{pmatrix},\ t^2=\begin{pmatrix}0 & 0 & i\\ 0 & 0 & 0 \\ -i & 0 & 0 \\ \end{pmatrix},\ t^3=  
\begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}. t^3 \mathrm{SO}(3) \mathrm{R}(\theta)=\exp(-i\theta t^3) t^3 \det(t^3-\mathbb{I}\lambda)=\det\begin{pmatrix}-\lambda & -i & 0\\ i & -\lambda & 0 \\ 0 & 0 & -\lambda\\\end{pmatrix}=(\lambda^2-1)\times(-\lambda)=0. \lambda=0 \lambda=\pm 1 \lambda=0 (0,0,1)^T \lambda=\pm 1 \begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}a\\ b \\ 0\\ \end{pmatrix}=\begin{pmatrix}-ib\\ ia \\ 0\\ \end{pmatrix}=\pm \begin{pmatrix}a\\ b \\ 0\\ \end{pmatrix}\tag{1} \frac{1}{\sqrt{2}}\left(1,\pm i,0\right)\tag{2}. t^3=U^\dagger \hat t U\tag{3} U=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i & 0\\ 1 & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix},\tag{4} \hat t=\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\tag{5} (2)-(4) (2) \frac{1}{\sqrt{2}}\left(1,\pm i,0\right)^\dagger=\frac{1}{\sqrt{2}}\begin{pmatrix}1\\ \mp i \\ 0\\ \end{pmatrix}, \, \text{for} \ \lambda=\pm 1? t^3 \frac{1}{\sqrt{2}}\begin{pmatrix}1\\ -i \\ 0\\ \end{pmatrix} \text{if}\,\,\lambda = +1\,\quad\ \frac{1}{\sqrt{2}}\begin{pmatrix}1\\ i \\ 0\\ \end{pmatrix} \text{if}\,\, \lambda = -1,\quad\ \frac{1}{\sqrt{2}}\begin{pmatrix}0\\ 0 \\ \sqrt{2}\\ \end{pmatrix} \text{if}\, \lambda = 0\tag{a} U (\mathrm{a}) (5) U=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}=\begin{pmatrix}1/\sqrt2 & 1/\sqrt2 & 0\\ -i/\sqrt2 & i/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix} (4) (4) (3) \det U = i U U^\dagger=U^{-1} U U^T=\begin{pmatrix}1/\sqrt2 & -i/\sqrt2 & 0\\ 1/\sqrt2 & i/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}\tag{b} (\mathrm{b}) \begin{pmatrix}+ & - & +\\ - & + & - \\\ + & - & + \\ \end{pmatrix} U^{-1}=\frac{1}{\det U}\begin{pmatrix}+\begin{vmatrix}i/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & -\begin{vmatrix}1/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & 0\\ -\begin{vmatrix}-i/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & +\begin{vmatrix}1/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & 0 \\ 0 & 0 & +\begin{vmatrix}1/\sqrt2 & -i/\sqrt2\\ 1/\sqrt2 & i/\sqrt2 \\ \end{vmatrix} \\ \end{pmatrix} =-i\begin{pmatrix}i/\sqrt2 & -1/\sqrt2 & 0\\ i/\sqrt2 & 1/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}=\frac{1}{\sqrt2}\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix} U^\dagger \hat t U=\frac12\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}=\begin{pmatrix}0 & 1 & 0\\ 1 & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix}\ne t^3\tag{c} U (4) U^\dagger \hat t U=\frac12\begin{pmatrix}1 & 1 & 0\\ i & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & -i & 0\\ 1 & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}=\begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix} = t^3\tag{d} U \hat t U^\dagger=\frac12\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}=\begin{pmatrix}0 & i & 0\\ -i & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix}=-t^3\tag{e} (\mathrm{d}) (\mathrm{c}) (\text{or}\, (\mathrm{e}))","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'proof-explanation', 'diagonalization']"
39,What are the possible dimensions of self-anti-involution matrices?,What are the possible dimensions of self-anti-involution matrices?,,"I am considering $2^n\times 2^n$ complex matrices and a general anti-involution $A^{**}=A$ , $(AB)^*=B^* A^*$ . To my knowledge all anti-involutions are a combination of transpose, field involution and similarity transform (is that correct?). What are the possible dimensions of the space created by all matrices which are it's own anti-involution $$A=A^*$$ I am considering a complex number to be 2 dimensions (i.e. in real vector space). For example for $4\times 4$ matrices and the Hermitian conjugate for the anti-involution, you have 16 independent degrees of freedom (4 reals on diagonal and 6 complex numbers above diagonal). Is calling this a 16-dimensional vector space correct? If you were to choose the transpose as the anti-involution, you would have 20 dimensions (4 complex on diagonal and 6 complex numbers above diagonal). However, if you chose the anti-involution $$A^*=RA^TR^{-1}$$ with $$R= \begin{pmatrix} 0&1&0& 0\\ -1&0&0&0\\ 0&0&0&-1\\ 0&0&1&0 \end{pmatrix}$$ then the matrices with $A=A^*$ would form only a 12-dimensional space (correct?). Is this the smallest dimension that self-anti-involution matrices can have? Which other values are possible? I think, generally, for $2^n\times 2^n$ matrices you can find anti-involutions which would make the space of self-anti-involution matrices $d=2^n(2^n+(-1)^{\lfloor n/2\rfloor})$ dimensional. My main motivation is to understand if these particular anti-involutions are special in some sense. Maybe one would restrict to not using complex conjugation (the field involution) for that.","I am considering complex matrices and a general anti-involution , . To my knowledge all anti-involutions are a combination of transpose, field involution and similarity transform (is that correct?). What are the possible dimensions of the space created by all matrices which are it's own anti-involution I am considering a complex number to be 2 dimensions (i.e. in real vector space). For example for matrices and the Hermitian conjugate for the anti-involution, you have 16 independent degrees of freedom (4 reals on diagonal and 6 complex numbers above diagonal). Is calling this a 16-dimensional vector space correct? If you were to choose the transpose as the anti-involution, you would have 20 dimensions (4 complex on diagonal and 6 complex numbers above diagonal). However, if you chose the anti-involution with then the matrices with would form only a 12-dimensional space (correct?). Is this the smallest dimension that self-anti-involution matrices can have? Which other values are possible? I think, generally, for matrices you can find anti-involutions which would make the space of self-anti-involution matrices dimensional. My main motivation is to understand if these particular anti-involutions are special in some sense. Maybe one would restrict to not using complex conjugation (the field involution) for that.","2^n\times 2^n A^{**}=A (AB)^*=B^* A^* A=A^* 4\times 4 A^*=RA^TR^{-1} R=
\begin{pmatrix}
0&1&0& 0\\
-1&0&0&0\\
0&0&0&-1\\
0&0&1&0
\end{pmatrix} A=A^* 2^n\times 2^n d=2^n(2^n+(-1)^{\lfloor n/2\rfloor})","['linear-algebra', 'abstract-algebra', 'matrices']"
40,How to efficiently find all solutions of equation $(A + \text{sgn}(x)\text{sgn}(x)^T )x= y x$,How to efficiently find all solutions of equation,(A + \text{sgn}(x)\text{sgn}(x)^T )x= y x,"Consider the following equation: $$\left (A + \text{sgn}(x)\text{sgn}(x)^T  \right )x= y \, x  \tag{1} $$ where $A$ is an $n \times n$ symmetric matrix. The variables are $y \in \mathbb R, x \in \mathbb R ^n$ with $\| x \|_2=1$ . Note this can be equivalently written as $$Ax +\|x\|_1 \text{sgn}(x) =y \, x.$$ To avoid trivial or degenerate solutions, we restrict ourselves to the solutions $x \neq 0$ with $\| x \|=1$ ; indeed, any solution $x$ of this equation remains a solution after a linear transformation. Such an equation appears in non-convex optimization, where all of its solutions need to be found so that the problem is fully solved. In fact the original system that needs to be solved is of the form $$\color{blue}{\left [ \left(A + \text{sgn}(x)\text{sgn}(x)^T  \right )x- y \, x \right ] \odot \text{sgn}(x) =0.}, \tag{2} $$ which reduces to the above system if we assume $x_i \neq 0, i=,\dots,n$ . Also, if $A$ is a diagonal matrix, the two systems (1) and (2) become the same. A naïve approach to solve the equation (1) is to fix $\text{sgn}(x)=a$ for any $a \in \{-1,1 \}^n$ , and then solve the system. One can see that if $a$ is considered, $-a$ is not needed to be assessed as both produce the same matrix of $aa^T$ . Hence, $2^{n-1}$ cases should be examined. After fixing $\text{sgn}(x)=a$ , each solution $y$ and $x$ of the resulting system: $$(A + aa^T )x= y x$$ is nothing but an eigenvalue and its normalized eigenvector of matrix $A+aa^T.$ Such a solution is a correct solution for our equation only if $\text{sgn}(x)=a$ or $\text{sgn}(x)=-a.$ When $A=0$ , for each $a$ , $\text{rank}(aa^T)=1$ , and the system has the following $n$ solutions $(x,y)$ : $$(e_1,1),\dots, (e_n,1).$$ I have two related questions: How many solutions can this system have (my guess the number of solutions is of order $O(n)$ though $2^{n-1}$ different possibilities can be obtained by fixing $\text{sgn}(x)$ )? Is there any efficient procedure to find all solutions of the above system (the computational complexity of the naïve approach is of exponential order)?","Consider the following equation: where is an symmetric matrix. The variables are with . Note this can be equivalently written as To avoid trivial or degenerate solutions, we restrict ourselves to the solutions with ; indeed, any solution of this equation remains a solution after a linear transformation. Such an equation appears in non-convex optimization, where all of its solutions need to be found so that the problem is fully solved. In fact the original system that needs to be solved is of the form which reduces to the above system if we assume . Also, if is a diagonal matrix, the two systems (1) and (2) become the same. A naïve approach to solve the equation (1) is to fix for any , and then solve the system. One can see that if is considered, is not needed to be assessed as both produce the same matrix of . Hence, cases should be examined. After fixing , each solution and of the resulting system: is nothing but an eigenvalue and its normalized eigenvector of matrix Such a solution is a correct solution for our equation only if or When , for each , , and the system has the following solutions : I have two related questions: How many solutions can this system have (my guess the number of solutions is of order though different possibilities can be obtained by fixing )? Is there any efficient procedure to find all solutions of the above system (the computational complexity of the naïve approach is of exponential order)?","\left (A + \text{sgn}(x)\text{sgn}(x)^T  \right )x= y \, x  \tag{1}  A n \times n y \in \mathbb R, x \in \mathbb R ^n \| x \|_2=1 Ax +\|x\|_1 \text{sgn}(x) =y \, x. x \neq 0 \| x \|=1 x \color{blue}{\left [ \left(A + \text{sgn}(x)\text{sgn}(x)^T  \right )x- y \, x \right ] \odot \text{sgn}(x) =0.}, \tag{2}  x_i \neq 0, i=,\dots,n A \text{sgn}(x)=a a \in \{-1,1 \}^n a -a aa^T 2^{n-1} \text{sgn}(x)=a y x (A + aa^T )x= y x A+aa^T. \text{sgn}(x)=a \text{sgn}(x)=-a. A=0 a \text{rank}(aa^T)=1 n (x,y) (e_1,1),\dots, (e_n,1). O(n) 2^{n-1} \text{sgn}(x)","['real-analysis', 'calculus', 'linear-algebra', 'matrices', 'systems-of-equations']"
41,"Given a $3\times 3$ $\operatorname{adj} A$, find $A$","Given a  , find",3\times 3 \operatorname{adj} A A,Given $\operatorname{adj}A=\begin{bmatrix} -1 & -2 & 1\\ 3 & 0 & -3 \\ 1 & -4 & 1 \end{bmatrix}$ . Find $A$ . My Attempt We know that $|\operatorname{adj}A|=|A|^{n-1}\Rightarrow |A|=\pm\sqrt{|\operatorname{adj}A|}=\pm2\sqrt3$ . Also it is well known that $A(\operatorname{adj}A)=|A| I\Rightarrow A=|A|(\operatorname{adj} A)^{-1}=\pm\frac{1}{2\sqrt3}\begin{bmatrix} -12 & -2 & 6\\ 6 & -2 & 0 \\ -12 & 6 & 6 \end{bmatrix}$ My doubt is that should there be a unique $A$ or there are two possibilities as shown in working above.,Given . Find . My Attempt We know that . Also it is well known that My doubt is that should there be a unique or there are two possibilities as shown in working above.,\operatorname{adj}A=\begin{bmatrix} -1 & -2 & 1\\ 3 & 0 & -3 \\ 1 & -4 & 1 \end{bmatrix} A |\operatorname{adj}A|=|A|^{n-1}\Rightarrow |A|=\pm\sqrt{|\operatorname{adj}A|}=\pm2\sqrt3 A(\operatorname{adj}A)=|A| I\Rightarrow A=|A|(\operatorname{adj} A)^{-1}=\pm\frac{1}{2\sqrt3}\begin{bmatrix} -12 & -2 & 6\\ 6 & -2 & 0 \\ -12 & 6 & 6 \end{bmatrix} A,"['linear-algebra', 'matrices', 'inverse', 'cayley-hamilton']"
42,Nonnegativity of the determinant of a commuting matrix,Nonnegativity of the determinant of a commuting matrix,,Let $A\in M_n(\mathbb{R})$ such that $A^2=-I_n$ and $AB=BA$ for some $B\in M_n(\mathbb{R})$ . Prove that $\det(B)\geq0$ . All the information I could extract from the relation $A^2=-I_n$ are as follows: $(a)$ $A$ is not diagonalizable. $(b)$ $\det(A)=1$ . $(c)$ $n$ must be even. Now how to conclude that $\det(B)$ is nonnegative using these $3$ informations alongwith $AB=BA$ is not clear to me. Any help is appreciated.,Let such that and for some . Prove that . All the information I could extract from the relation are as follows: is not diagonalizable. . must be even. Now how to conclude that is nonnegative using these informations alongwith is not clear to me. Any help is appreciated.,A\in M_n(\mathbb{R}) A^2=-I_n AB=BA B\in M_n(\mathbb{R}) \det(B)\geq0 A^2=-I_n (a) A (b) \det(A)=1 (c) n \det(B) 3 AB=BA,"['linear-algebra', 'matrices', 'contest-math']"
43,Given $A\in M_{2}(\mathbb Z)$ be such that $|A_{ij}(n)|\leq 50$ . prove $|A_{ij}(n)|\leq 50$ for all $n$.,Given  be such that  . prove  for all .,A\in M_{2}(\mathbb Z) |A_{ij}(n)|\leq 50 |A_{ij}(n)|\leq 50 n,"Let $A\in M_{2}(\mathbb Z)$ be such that $|A_{ij}(n)|\leq 50$ for all $1 \leq n \leq 10^{50}$ . and all $1\leq i ,j\leq 2$ .where $A_{ij}(n)$ denotes the $(i,j)$ -th entry of the $2\times 2$ matrix $A^n$ .then prove that $|A_{ij}(n)|\leq 50$ for all positive integers n. I am looking for some hints on this question.",Let be such that for all . and all .where denotes the -th entry of the matrix .then prove that for all positive integers n. I am looking for some hints on this question.,"A\in M_{2}(\mathbb Z) |A_{ij}(n)|\leq 50 1 \leq n \leq 10^{50} 1\leq i ,j\leq 2 A_{ij}(n) (i,j) 2\times 2 A^n |A_{ij}(n)|\leq 50","['linear-algebra', 'matrices']"
44,Proving the Dimension of the Commutant Space of a $n\times n$ Matrix is Greater Than or Equal to $n$,Proving the Dimension of the Commutant Space of a  Matrix is Greater Than or Equal to,n\times n n,"$M_n(F)$ represents the collection of all square matrices of size $n \times n$ with elements from the field $F$ . Let's take a specific matrix $A$ from $M_n(\mathbb{R})$ , where $\mathbb{R}$ is the set of real numbers. We'll define a set $C(A)$ as the set of matrices $B$ in $M_n(\mathbb{R})$ such that the product $AB$ is equal to $BA$ : $$C(A)=\{B\in M_n(\mathbb{R})\mid AB=BA\}$$ It can be shown relatively easily that the set $C(A)$ forms a vector space over the field of real numbers, $\mathbb{R}$ . We are tasked with demonstrating that the dimension of this vector space is greater than or equal to $n$ : $$\text{dim }C(A)\geq n$$ I've extensively researched various approaches to this problem and came across several methods that involve concepts like the Jordan canonical form, the minimal polynomial, and the characteristic polynomial of matrix A, ideas related to linear transformations. However, I aim to tackle this problem using the knowledge I've acquired in my course. Some of them are: Matrix elementary operations and elementary matrices. Row-reduced echelon form of a matrix. Vector space, including its definition, properties, basis, dimension, kernel, and nullity. Subspaces and their properties, as well as their relationship to the vector spaces they are part of. Understanding linear independence and dependence of vectors. The concept of a coordinate vector relative to a given basis. Recognition of row and column spaces of a matrix, although I haven't yet encountered the rank of a matrix. $\dots$ Because the methods we have for solving the problem are quite basic, I believe it's enough to demonstrate that we have a minimum of $n$ elements in the basis. I suspect that techniques involving diagonal matrices, nullity, and elementary matrices might be useful for this purpose. I would greatly appreciate any assistance or hints to help me solve this problem using the knowledge and concepts I've already been taught in my course. If you'd like to utilize advanced ideas, try to demonstrate them using fundamental concepts or establish their characteristics using basic tools. For instance, if you plan to utilize the Jordan canonical form, begin by explaining what it is and then highlight the specific characteristics of this concept that you will use to address the main problem. However, in this context, simply mentioning the names of the relevant concepts along with their properties or referencing a source that introduces and proves these properties is sufficient and greatly appreciated. I have carefully read all the comments, and I am extremely grateful to those who took the time to comment. However, their responses were beyond the scope of what we can incorporate or apply. EDIT Please check this solution .","represents the collection of all square matrices of size with elements from the field . Let's take a specific matrix from , where is the set of real numbers. We'll define a set as the set of matrices in such that the product is equal to : It can be shown relatively easily that the set forms a vector space over the field of real numbers, . We are tasked with demonstrating that the dimension of this vector space is greater than or equal to : I've extensively researched various approaches to this problem and came across several methods that involve concepts like the Jordan canonical form, the minimal polynomial, and the characteristic polynomial of matrix A, ideas related to linear transformations. However, I aim to tackle this problem using the knowledge I've acquired in my course. Some of them are: Matrix elementary operations and elementary matrices. Row-reduced echelon form of a matrix. Vector space, including its definition, properties, basis, dimension, kernel, and nullity. Subspaces and their properties, as well as their relationship to the vector spaces they are part of. Understanding linear independence and dependence of vectors. The concept of a coordinate vector relative to a given basis. Recognition of row and column spaces of a matrix, although I haven't yet encountered the rank of a matrix. Because the methods we have for solving the problem are quite basic, I believe it's enough to demonstrate that we have a minimum of elements in the basis. I suspect that techniques involving diagonal matrices, nullity, and elementary matrices might be useful for this purpose. I would greatly appreciate any assistance or hints to help me solve this problem using the knowledge and concepts I've already been taught in my course. If you'd like to utilize advanced ideas, try to demonstrate them using fundamental concepts or establish their characteristics using basic tools. For instance, if you plan to utilize the Jordan canonical form, begin by explaining what it is and then highlight the specific characteristics of this concept that you will use to address the main problem. However, in this context, simply mentioning the names of the relevant concepts along with their properties or referencing a source that introduces and proves these properties is sufficient and greatly appreciated. I have carefully read all the comments, and I am extremely grateful to those who took the time to comment. However, their responses were beyond the scope of what we can incorporate or apply. EDIT Please check this solution .",M_n(F) n \times n F A M_n(\mathbb{R}) \mathbb{R} C(A) B M_n(\mathbb{R}) AB BA C(A)=\{B\in M_n(\mathbb{R})\mid AB=BA\} C(A) \mathbb{R} n \text{dim }C(A)\geq n \dots n,"['linear-algebra', 'matrices']"
45,Show there exists such $x\in F^n$ for some $A\in M_{m\times n}(F)$ such $Ax$ has no zero entry,Show there exists such  for some  such  has no zero entry,x\in F^n A\in M_{m\times n}(F) Ax,"$M_{m\times n}(F)$ is the set of all ${m\times n}$ matrices over field $F$ . Now, assume $A\in M_{m\times n}(F)$ and $A$ has no zero-rows (rows that all their elements are zero). Also $F$ is an infinite field. Show that there exists an $x\in F^n$ ( ${n\times1}$ vector) such that none of entries of $Ax$ is zero. I know about these: elementary operations row-reduced echelon forms linear dependence or independence equivalent matrices matrix block multiplication Assume $A=[a_{ij}]$ . We know the conditions of problem are satisfied when: $$\begin{bmatrix} a_{11}&a_{12}&\dots&a_{1n}\\ a_{21}&a_{22}&\dots&a_{2n}\\ \vdots&\vdots&\ddots&\vdots\\ a_{m1}&a_{m2}&\dots&a_{mn}\\ \end{bmatrix} \begin{bmatrix} x_1\\ x_2\\ \vdots\\ x_n \end{bmatrix}=\begin{bmatrix} u_1\\ u_2\\ \vdots\\ u_m \end{bmatrix}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(u_i\neq0\text{ for } 1\leq i\leq m)$$ So we should find a vector $x$ that all equations like this are not zero: $$\sum_{j=1}^n{a_{ij}}x_j\neq 0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, (0\leq i\leq m)$$ I tried assuming that there's no matrix like this then reach a contradiction, but I couldn't. I also tried to construct the vector $x$ and use linear dependency, but this also didn't give me any thing good. Any help is so much appreciated!","is the set of all matrices over field . Now, assume and has no zero-rows (rows that all their elements are zero). Also is an infinite field. Show that there exists an ( vector) such that none of entries of is zero. I know about these: elementary operations row-reduced echelon forms linear dependence or independence equivalent matrices matrix block multiplication Assume . We know the conditions of problem are satisfied when: So we should find a vector that all equations like this are not zero: I tried assuming that there's no matrix like this then reach a contradiction, but I couldn't. I also tried to construct the vector and use linear dependency, but this also didn't give me any thing good. Any help is so much appreciated!","M_{m\times n}(F) {m\times n} F A\in M_{m\times n}(F) A F x\in F^n {n\times1} Ax A=[a_{ij}] \begin{bmatrix}
a_{11}&a_{12}&\dots&a_{1n}\\
a_{21}&a_{22}&\dots&a_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
a_{m1}&a_{m2}&\dots&a_{mn}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n
\end{bmatrix}=\begin{bmatrix}
u_1\\
u_2\\
\vdots\\
u_m
\end{bmatrix}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(u_i\neq0\text{ for } 1\leq i\leq m) x \sum_{j=1}^n{a_{ij}}x_j\neq 0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, (0\leq i\leq m) x","['linear-algebra', 'matrices']"
46,Variance of largest eigenvector of bounded random matrix,Variance of largest eigenvector of bounded random matrix,,"Given a random $n\times n$ bounded positive semidefinite matrix $X$ with $\mathbb{E}[X]=A$ and some constant matrix $B$ , define an estimator $\hat{o} = tr[BX^T]$ with $\mathbb{E}[\hat{o}] = \mu$ and $\mbox{Var}[\hat{o}] = \sigma^2$ . We immediately have that $$ \mu = tr[BA] \quad \mbox{and} \quad \sigma^2 = \mathbb{E}[tr[BX^T]^2] - \mu^2 $$ Now pick up the eigenvalue ordering for the variable $X$ as $1 \geq \lambda_1 \geq \lambda_2 \geq \dots \lambda_n \geq 0$ and define the vector $v$ $(vv^T = 1)$ such that $Xv = \lambda_1 v$ i.e. the first eigenvector of $X$ . What can be said about the variance $\mbox{Var}[tr[Bvv^T]]$ of this eigenvector estimator in terms of $X, \mathbb{E}[X]$ and $\mathbb{E}[tr[BX^T]^2]$ ? I am mostly interested in upper bounds on the eigenvector estimator variance but do not know how to even begin to treat this when I am considering eigenvectors of a random matrix $X$ as a starting point, rather than $v$ as the random variable itself. Since $X$ is promised to be positive definite it seems like taking the eigendecomposition of $X$ might yield an upper bound similar to $ \frac{\sigma^2}{\lambda_1^2} $ but this seems rather weak. Any help as to where to begin would be greatly appreciated.","Given a random bounded positive semidefinite matrix with and some constant matrix , define an estimator with and . We immediately have that Now pick up the eigenvalue ordering for the variable as and define the vector such that i.e. the first eigenvector of . What can be said about the variance of this eigenvector estimator in terms of and ? I am mostly interested in upper bounds on the eigenvector estimator variance but do not know how to even begin to treat this when I am considering eigenvectors of a random matrix as a starting point, rather than as the random variable itself. Since is promised to be positive definite it seems like taking the eigendecomposition of might yield an upper bound similar to but this seems rather weak. Any help as to where to begin would be greatly appreciated.","n\times n X \mathbb{E}[X]=A B \hat{o} = tr[BX^T] \mathbb{E}[\hat{o}] = \mu \mbox{Var}[\hat{o}] = \sigma^2 
\mu = tr[BA] \quad \mbox{and} \quad \sigma^2 = \mathbb{E}[tr[BX^T]^2] - \mu^2
 X 1 \geq \lambda_1 \geq \lambda_2 \geq \dots \lambda_n \geq 0 v (vv^T = 1) Xv = \lambda_1 v X \mbox{Var}[tr[Bvv^T]] X, \mathbb{E}[X] \mathbb{E}[tr[BX^T]^2] X v X X  \frac{\sigma^2}{\lambda_1^2} ","['linear-algebra', 'matrices', 'random-variables']"
47,Over which rings is matrix rank defined?,Over which rings is matrix rank defined?,,"I'm investigating $m \times n$ matrices over $R$ , where $R$ is a finite commutative unitary ring, and more specifically a finite chain ring or a principal ideal ring. Define the row rank as the maximal number of lin. independent rows, the column rank in a similar manner, and the inner rank. My question is, are there necessary and sufficient conditions for $R$ such that the row rank is equal to the column rank, so that the term matrix rank is well-defined? I know that the two ranks coincide, for example, over a PID, and in this case they also coincide with other notions of rank I have encountered, such as the McCoy rank and the determinantal rank, but in general I don't know when I have the right to speak about the rank of a matrix. A reference would be welcome.","I'm investigating matrices over , where is a finite commutative unitary ring, and more specifically a finite chain ring or a principal ideal ring. Define the row rank as the maximal number of lin. independent rows, the column rank in a similar manner, and the inner rank. My question is, are there necessary and sufficient conditions for such that the row rank is equal to the column rank, so that the term matrix rank is well-defined? I know that the two ranks coincide, for example, over a PID, and in this case they also coincide with other notions of rank I have encountered, such as the McCoy rank and the determinantal rank, but in general I don't know when I have the right to speak about the rank of a matrix. A reference would be welcome.",m \times n R R R,"['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory', 'matrix-rank']"
48,A matrix $A \in k^{n \times n}$ of rank $1 \leq r \leq n-1$,A matrix  of rank,A \in k^{n \times n} 1 \leq r \leq n-1,"Let $k$ be a field and let $A$ an $n \times n$ matrix over $k$ , with $n \geq 2$ , $A \in k^{n \times n}$ . Assume that $1 \leq rank(A)=r \leq n-1$ ; this means that $A$ has: $r$ $k$ - linearly idependent rows $r$ $k$ -linearly independent columns invertible sub-matrix $B \in k^{r \times r}$ of maximal order Question: Is it possible to obtain additional properties if we further assume that every possible sub-matrix of order $r$ is invertible? In view of Amateur_Algebraist's comment, I would like to divide this question to two cases: (i) $|k| < \infty$ ; (ii) $|k|=\infty$ . Notice that the additional assumption not always holds, for example: $ A= \begin{pmatrix}  1 & 0 & 0 & 0\\  0 & 1 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}$ , $n=4$ , $r=2$ . Any comments are welcome; thank you!","Let be a field and let an matrix over , with , . Assume that ; this means that has: - linearly idependent rows -linearly independent columns invertible sub-matrix of maximal order Question: Is it possible to obtain additional properties if we further assume that every possible sub-matrix of order is invertible? In view of Amateur_Algebraist's comment, I would like to divide this question to two cases: (i) ; (ii) . Notice that the additional assumption not always holds, for example: , , . Any comments are welcome; thank you!","k A n \times n k n \geq 2 A \in k^{n \times n} 1 \leq rank(A)=r \leq n-1 A r k r k B \in k^{r \times r} r |k| < \infty |k|=\infty  A=
\begin{pmatrix} 
1 & 0 & 0 & 0\\ 
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{pmatrix} n=4 r=2","['linear-algebra', 'matrices', 'ring-theory', 'commutative-algebra']"
49,Prove (or disprove) that two pairs of matrices generating the same subgroup give rise to isomorphic groups,Prove (or disprove) that two pairs of matrices generating the same subgroup give rise to isomorphic groups,,"Let $A,B\in \operatorname{GL}(k,\mathbb{Z})$ be matrices of finite order such that $AB=BA$ and $A\neq \operatorname{I}$ , $B\neq \operatorname{I}$ . Define the group $\Sigma_{A,B}:=\mathbb{Z^2}\ltimes_{A,B} \mathbb{Z}^k$ , that is, the multiplication is given by $$\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right) \cdot \left(s_1,s_2, \left(\begin{array}{c}{t'_1\\ \vdots \\ t'_k}\end{array}\right)\right)=\left(r_1+s_1,r_2+s_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)+A^{r_1} B^{r_2}\left(\begin{array}{c}{t'_1\\ \vdots \\ t'_k}\end{array}\right) \right).$$ Note that this multiplication is well defined since $AB=BA$ . Question: Assume that $C,D\in\operatorname{GL}(k,\mathbb{Z})$ are commuting matrices of finite order such that $\langle A,B\rangle=\langle C,D\rangle$ , i.e. the pairs of matrices $\{A,B\}$ and $\{C,D\}$ generate the same subgroup. Also assume that the group $\langle A,B\rangle$ is not cyclic. Are $\Sigma_{A,B}$ and $\Sigma_{C,D}$ isomorphic? We have the following lemma: Lemma: (1) $\Sigma_{A,B}$ is isomorphic to $\Sigma_{B,A}$ (2) $\Sigma_{A,B}$ is isomorphic to $\Sigma_{A^{-1},B}$ (3) $\Sigma_{A,B}$ is isomorphic to $\Sigma_{A,AB}$ Proof: The isomorphisms are: For (1), $\varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(r_2,r_1, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right) $ For (2), $\varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(-r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)$ For (3), $\varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(r_1-r_2,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)$ I was thinking that if we could go from one generator set $\{A,B\}$ to another one $\{A^p B^q, A^r B^s\}$ applying the 3 operations above then the groups would be isomorphic, but I don't know if this is true, and I also don't know which properties must satisfy $p,q,r,s$ in order for $\{A^p B^q, A^r B^s\}$ to generate the subgroup $\langle A,B\rangle$ . Help would be greatly appreciated. Thanks!","Let be matrices of finite order such that and , . Define the group , that is, the multiplication is given by Note that this multiplication is well defined since . Question: Assume that are commuting matrices of finite order such that , i.e. the pairs of matrices and generate the same subgroup. Also assume that the group is not cyclic. Are and isomorphic? We have the following lemma: Lemma: (1) is isomorphic to (2) is isomorphic to (3) is isomorphic to Proof: The isomorphisms are: For (1), For (2), For (3), I was thinking that if we could go from one generator set to another one applying the 3 operations above then the groups would be isomorphic, but I don't know if this is true, and I also don't know which properties must satisfy in order for to generate the subgroup . Help would be greatly appreciated. Thanks!","A,B\in \operatorname{GL}(k,\mathbb{Z}) AB=BA A\neq \operatorname{I} B\neq \operatorname{I} \Sigma_{A,B}:=\mathbb{Z^2}\ltimes_{A,B} \mathbb{Z}^k \left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right) \cdot \left(s_1,s_2, \left(\begin{array}{c}{t'_1\\ \vdots \\ t'_k}\end{array}\right)\right)=\left(r_1+s_1,r_2+s_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)+A^{r_1} B^{r_2}\left(\begin{array}{c}{t'_1\\ \vdots \\ t'_k}\end{array}\right) \right). AB=BA C,D\in\operatorname{GL}(k,\mathbb{Z}) \langle A,B\rangle=\langle C,D\rangle \{A,B\} \{C,D\} \langle A,B\rangle \Sigma_{A,B} \Sigma_{C,D} \Sigma_{A,B} \Sigma_{B,A} \Sigma_{A,B} \Sigma_{A^{-1},B} \Sigma_{A,B} \Sigma_{A,AB} \varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(r_2,r_1, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)  \varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(-r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right) \varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(r_1-r_2,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right) \{A,B\} \{A^p B^q, A^r B^s\} p,q,r,s \{A^p B^q, A^r B^s\} \langle A,B\rangle","['abstract-algebra', 'matrices', 'group-theory', 'finite-groups']"
50,How to find a sparse lattice basis?,How to find a sparse lattice basis?,,"I am working with lattice codes (see here , or here ) and facing the following problem: I have a set of $k$ vectors $\left\{v_1,\ldots,v_k\right\}$ which I know generate an $n$ -dimensional, full-rank lattice, in the sense that any lattice vector can be written as linear combination of the $v_j$ s with integer coefficients. This set is over-complete ( $k>n$ ) and has the property that the weight, i.e. the number of non-zero elements, of any of the $v_j$ s is low (bounded by a constant). Is there any way of constructing a lattice basis from an over-complete set such that the low-weight property is conserved? If that helps, we can assume that the lattice is obtained through construction $A$ , so thinking of the $v_j$ s as column vectors we can write $$ M = \left(v_1 , v_2, \ldots , v_k \right)  =  \left(v_1 , v_2, \ldots , v_{k-n}\ |\ 2\mathbb{I}_n \right) $$ with $\mathbb{I}_n$ the identity matrix, $v_1 , v_2, \ldots , v_{k-n} $ have entries in $\left\{0,1\right\}$ and our lattice would be $$ \mathcal{L} = \left\{Mz\in \mathbb{R}^n :\ z\in\mathbb{Z}^k \right\} .$$ In general, constructing a basis is reasonably easy using the Hermite normal form (HNF), but then the weight can become as large as $n$ . If we do assume we found a basis through the HNF, for example, then my problem could be stated as $$\text{minimize } f(U) = \left(\max_{j\in\left\{1,\ldots,n\right\}} \left\| (AU)(j) \right\|_0 \right)\ \text{subject to } U \text{ is unimodular} $$ where $(A U)(j)$ denotes the $j$ -th column of $A U$ and $\| v \|_0 = \sum\limits_j | v_j |^0$ is the zero ""norm"". I have spent some time looking into lattice basis reduction methods but it seems that most  algorithms aim at finding ""short"" bases (e.g., LLL), in the sense of Euclidean length, whereas I am specifically concerned with the ""sparsity"" of the basis. Intuitively, I think that this could also be phrased as basis reduction in the zero norm. Any pointer to algorithms to reduce the weight of lattice basis vectors would be much appreciated.","I am working with lattice codes (see here , or here ) and facing the following problem: I have a set of vectors which I know generate an -dimensional, full-rank lattice, in the sense that any lattice vector can be written as linear combination of the s with integer coefficients. This set is over-complete ( ) and has the property that the weight, i.e. the number of non-zero elements, of any of the s is low (bounded by a constant). Is there any way of constructing a lattice basis from an over-complete set such that the low-weight property is conserved? If that helps, we can assume that the lattice is obtained through construction , so thinking of the s as column vectors we can write with the identity matrix, have entries in and our lattice would be In general, constructing a basis is reasonably easy using the Hermite normal form (HNF), but then the weight can become as large as . If we do assume we found a basis through the HNF, for example, then my problem could be stated as where denotes the -th column of and is the zero ""norm"". I have spent some time looking into lattice basis reduction methods but it seems that most  algorithms aim at finding ""short"" bases (e.g., LLL), in the sense of Euclidean length, whereas I am specifically concerned with the ""sparsity"" of the basis. Intuitively, I think that this could also be phrased as basis reduction in the zero norm. Any pointer to algorithms to reduce the weight of lattice basis vectors would be much appreciated.","k \left\{v_1,\ldots,v_k\right\} n v_j k>n v_j A v_j  M = \left(v_1 , v_2, \ldots , v_k \right)  =  \left(v_1 , v_2, \ldots , v_{k-n}\ |\ 2\mathbb{I}_n \right)  \mathbb{I}_n v_1 , v_2, \ldots , v_{k-n}  \left\{0,1\right\}  \mathcal{L} = \left\{Mz\in \mathbb{R}^n :\ z\in\mathbb{Z}^k \right\} . n \text{minimize } f(U) = \left(\max_{j\in\left\{1,\ldots,n\right\}} \left\| (AU)(j) \right\|_0 \right)\ \text{subject to } U \text{ is unimodular}  (A U)(j) j A U \| v \|_0 = \sum\limits_j | v_j |^0","['linear-algebra', 'matrices', 'coding-theory', 'integer-lattices', 'unimodular-matrices']"
51,Trace of an important class of operators,Trace of an important class of operators,,"In Physics , there exists an important class of operators of the form $e^{i A}$ (see equation ( $14$ ) in this article ). It is often of interest to calculate the trace $Tr[B e^{i A}]$ (such as equation ( $18$ ) ). I want to know whether there is some standard way(s) to approach this problem in Mathematics ?","In Physics , there exists an important class of operators of the form (see equation ( ) in this article ). It is often of interest to calculate the trace (such as equation ( ) ). I want to know whether there is some standard way(s) to approach this problem in Mathematics ?",e^{i A} 14 Tr[B e^{i A}] 18,"['linear-algebra', 'matrices', 'operator-theory', 'trace']"
52,Alternative expression for the pseudo-determinant $\operatorname{Det}(A^T \operatorname{diag}(b) A)$?,Alternative expression for the pseudo-determinant ?,\operatorname{Det}(A^T \operatorname{diag}(b) A),"Let $\det(\cdot)$ denote the determinant and let $\operatorname{Det}(\cdot)$ denote the pseudo-determinant, which for any square matrix $A \in \mathbb{R}^{n \times n}$ can be defined as $$ \operatorname{Det}(A) := \lim_{\delta \to 0} \frac{\det(A + \delta I)}{\delta^{n - \operatorname{rank}(A)}}. $$ Now, let $A \in \mathbb{R}^{k \times n}$ , $b \in \mathbb{R}^k$ be a vector with strictly positive entries, and assume that $k \geq n$ . For the usual determinant, we have the identity $$ \det(A^T \operatorname{diag}(b) \, A) = \det(\operatorname{diag}(b)) \det(A A^T). $$ In the case that $\ker(A) \neq \{ 0 \}$ , then both sides of the above reduce to 0. Question: is there an analogous way to express the pseudo-determinant $$ \operatorname{Det}(A^T \operatorname{diag}(b) \,A) $$ as a product of factors involving determinants and/or pseudo-determinants of the matrices involved? It is clear that in general $\operatorname{Det}(A^T \operatorname{diag}(b) \,A) \neq \det(\operatorname{diag}(b)) \operatorname{Det}(A A^T)$ . I have tried manipulating the limit definition of the pseudo-determinant using the SVD of $A$ and the matrix determinant lemma, but I have not managed to get a clean expression out of this. I am beginning to think that there is no simple expression, or at least not one that is cleaner than given here . For example, two exemplars I have computed are: $$ A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}, \quad \operatorname{Det}\left( A^T \operatorname{diag}(b) \, A \right) = 2(b_1 + b_2), \quad \operatorname{Det}\left( A A^T \right) = 4, $$ and $$ A = \begin{bmatrix} 1 & 0 & -1 \\ -1 & 1 & 0 \\ 0 & -1 & 1  \end{bmatrix}, \quad \operatorname{Det}\left( A^T \operatorname{diag}(b) \, A \right) = 3(b_1 b_2 + b_2 b_3 + b_1 b_3), \quad \operatorname{Det}(A A^T) = 9. $$","Let denote the determinant and let denote the pseudo-determinant, which for any square matrix can be defined as Now, let , be a vector with strictly positive entries, and assume that . For the usual determinant, we have the identity In the case that , then both sides of the above reduce to 0. Question: is there an analogous way to express the pseudo-determinant as a product of factors involving determinants and/or pseudo-determinants of the matrices involved? It is clear that in general . I have tried manipulating the limit definition of the pseudo-determinant using the SVD of and the matrix determinant lemma, but I have not managed to get a clean expression out of this. I am beginning to think that there is no simple expression, or at least not one that is cleaner than given here . For example, two exemplars I have computed are: and","\det(\cdot) \operatorname{Det}(\cdot) A \in \mathbb{R}^{n \times n} 
\operatorname{Det}(A) := \lim_{\delta \to 0} \frac{\det(A + \delta I)}{\delta^{n - \operatorname{rank}(A)}}.
 A \in \mathbb{R}^{k \times n} b \in \mathbb{R}^k k \geq n 
\det(A^T \operatorname{diag}(b) \, A) = \det(\operatorname{diag}(b)) \det(A A^T).
 \ker(A) \neq \{ 0 \} 
\operatorname{Det}(A^T \operatorname{diag}(b) \,A)
 \operatorname{Det}(A^T \operatorname{diag}(b) \,A) \neq \det(\operatorname{diag}(b)) \operatorname{Det}(A A^T) A 
A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}, \quad \operatorname{Det}\left( A^T \operatorname{diag}(b) \, A \right) = 2(b_1 + b_2), \quad \operatorname{Det}\left( A A^T \right) = 4,
 
A = \begin{bmatrix} 1 & 0 & -1 \\ -1 & 1 & 0 \\ 0 & -1 & 1  \end{bmatrix}, \quad \operatorname{Det}\left( A^T \operatorname{diag}(b) \, A \right) = 3(b_1 b_2 + b_2 b_3 + b_1 b_3), \quad \operatorname{Det}(A A^T) = 9.
","['linear-algebra', 'matrices', 'determinant']"
53,Uniqueness of the square root for non-symmetric definite matrices,Uniqueness of the square root for non-symmetric definite matrices,,"When $A$ is positive semi-definite, the positive semi-definite square root $B$ , which is $BB=A$ is unique. I have heard that this holds true for a symmetric matrix. What about under weaker conditions? In particular, is it true when $A,\ B$ may not be symmetric but $x^TAx>0,\ x^TBx>0$ ? or may not be symmetric and may not be $x^TAx>0,\ x^TBx>0$ but all eigenvalues are positive? It's hard for me to think of a way to prove. For example, I guess this is true for the normal matrix of real numbers such as $$A=\begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1& 0 & 1\end{pmatrix},\ B=\frac{1}{3}\begin{pmatrix}\sqrt{2}+\sqrt{3} & \sqrt{2} & \sqrt{2}-\sqrt{3}\\ \sqrt{2}-\sqrt{3} & \sqrt{2}+\sqrt{3}& \sqrt{2} \\\sqrt{2}  &\sqrt{2}-\sqrt{3} & \sqrt{2}+\sqrt{3}\end{pmatrix}$$","When is positive semi-definite, the positive semi-definite square root , which is is unique. I have heard that this holds true for a symmetric matrix. What about under weaker conditions? In particular, is it true when may not be symmetric but ? or may not be symmetric and may not be but all eigenvalues are positive? It's hard for me to think of a way to prove. For example, I guess this is true for the normal matrix of real numbers such as","A B BB=A A,\ B x^TAx>0,\ x^TBx>0 x^TAx>0,\ x^TBx>0 A=\begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1& 0 & 1\end{pmatrix},\ B=\frac{1}{3}\begin{pmatrix}\sqrt{2}+\sqrt{3} & \sqrt{2} & \sqrt{2}-\sqrt{3}\\ \sqrt{2}-\sqrt{3} & \sqrt{2}+\sqrt{3}& \sqrt{2} \\\sqrt{2}  &\sqrt{2}-\sqrt{3} & \sqrt{2}+\sqrt{3}\end{pmatrix}","['linear-algebra', 'matrices', 'radicals', 'positive-definite', 'positive-semidefinite']"
54,"Prove that the unit lower diagonal matrices $L, M$, and $D$ are all unique","Prove that the unit lower diagonal matrices , and  are all unique","L, M D","If all the leading principal submatrices of a matrix $A$ (i.e. all square submatrices that share a top left corner with $A$ ) are nonsingular, then prove that there are unique unit lower diagonal matrices $L$ and $M$ , and a unique diagonal matrix $D$ such that $A=LDM^T$ . I've seen a proof that such matrices $L,D,M$ all exist, but I'm not sure how to prove uniqueness. I would start by assuming that there are two such matrices $L_1$ and $L_2$ and then derive a contradiction. I think I could then use a similar argument to show $M$ and $D$ are unique. Here is the proof. Factor $A=LU$ . Define $D= diag(d_1,\cdots, d_n), d_i=u_{ii},1\leq i\leq n.$ All $d_i\neq 0$ as all of $A$ 's leading principal submatrices are nonsingular. So $D^{-1}$ exists. Note that $D^{-1}U$ is unit upper triangular. So $M = (D^{-1}U)^T$ is unit lower triangular, and $A=LDM^T$ .","If all the leading principal submatrices of a matrix (i.e. all square submatrices that share a top left corner with ) are nonsingular, then prove that there are unique unit lower diagonal matrices and , and a unique diagonal matrix such that . I've seen a proof that such matrices all exist, but I'm not sure how to prove uniqueness. I would start by assuming that there are two such matrices and and then derive a contradiction. I think I could then use a similar argument to show and are unique. Here is the proof. Factor . Define All as all of 's leading principal submatrices are nonsingular. So exists. Note that is unit upper triangular. So is unit lower triangular, and .","A A L M D A=LDM^T L,D,M L_1 L_2 M D A=LU D= diag(d_1,\cdots, d_n), d_i=u_{ii},1\leq i\leq n. d_i\neq 0 A D^{-1} D^{-1}U M = (D^{-1}U)^T A=LDM^T","['linear-algebra', 'matrices', 'numerical-linear-algebra']"
55,Eigenvectors of $\text{diag}(x) - xx^T$,Eigenvectors of,\text{diag}(x) - xx^T,"Given a vector $x$ what are the eigenvectors of $\text{diag}(x) - xx^T$ ? My observations so far: If we had only the first term, the matrix would already be diagonal. The second term, on the other hand, has a diagonal form with a single nonzero entry corresponding to the direction parallel to $x$ This matrix acts on the vector $(1,1,\ldots,1)$ to yield a multiple of $x$ . I also see how to invert the matrix using the Woodbury identity.","Given a vector what are the eigenvectors of ? My observations so far: If we had only the first term, the matrix would already be diagonal. The second term, on the other hand, has a diagonal form with a single nonzero entry corresponding to the direction parallel to This matrix acts on the vector to yield a multiple of . I also see how to invert the matrix using the Woodbury identity.","x \text{diag}(x) - xx^T x (1,1,\ldots,1) x","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
56,Can we neglect matrices with smaller eigenvalues in comparison to ones with larger eigenvalues?,Can we neglect matrices with smaller eigenvalues in comparison to ones with larger eigenvalues?,,"Can two matrices be compared as being ""small"" and ""large""? For example, consider  matrix $X=X(t)$ as a function of parameter $t$ (say for time), such that \begin{equation} \frac{d X}{dt} = YX^2 + Z \end{equation} for some constant matrices $Y$ and $Z$ . Under what conditions can one neglect matrix $Z$ in this equation?","Can two matrices be compared as being ""small"" and ""large""? For example, consider  matrix as a function of parameter (say for time), such that for some constant matrices and . Under what conditions can one neglect matrix in this equation?","X=X(t) t \begin{equation}
\frac{d X}{dt} = YX^2 + Z
\end{equation} Y Z Z","['matrices', 'matrix-equations']"
57,A system of three `eigenequations' (sort of),A system of three `eigenequations' (sort of),,"In my research I have stumbled upon the following problem: Find all pairwise orthonormal triples of vectors $\mathbf{x},\mathbf{y},\mathbf{z} \in \mathbb{C}^n$ ( $n \geq 4$ ) satisfying the three `eigenequations' $(a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{x}=(c-b)bc \,\mathbf{x} \\ (a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{y}=(c-a)ca \,\mathbf{y} \\ (a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{z}=(a+b)ab \,\mathbf{z}$ where - and this is crucial - the $n\times n$ matrices $\mathbf{A},\mathbf{B},\mathbf{C}$ themselves depend on $\mathbf{x},\mathbf{y},\mathbf{z}$ , respectively, being defined as $A_{ij} = \delta_{ij}\sum_k D_{kj}|x_k|^2 - x_i D_{ij} \bar{x}_j \\ B_{ij} = \delta_{ij}\sum_k D_{kj}|y_k|^2 - y_i D_{ij} \bar{y}_j \\ C_{ij} = \delta_{ij}\sum_k D_{kj}|z_k|^2 - z_i D_{ij} \bar{z_j}$ where $\delta_{ij}$ is the Kronecker symbol and $D_{ij}$ ( $i,j =1,\ldots,n$ ) are certain given coefficients satisfying $D_{ii} = 0$ , $D_{ij} = D_{ji} > 0$ for $i \neq j$ . Notice that the matrices are positive semi-definite and satisfy $\mathbf{A} \mathbf{x} = \mathbf{B} \mathbf{y} = \mathbf{C} \mathbf{z} = 0$ (I think their rank is actually $n-1$ ) as well as $\mathbf{y}^+\mathbf{C}\mathbf{y} = \mathbf{z}^+\mathbf{B}\mathbf{z}, \quad \mathbf{z}^+\mathbf{A}\mathbf{z} = \mathbf{x}^+\mathbf{C}\mathbf{x}, \quad \mathbf{x}^+\mathbf{B}\mathbf{x} = \mathbf{y}^+\mathbf{A}\mathbf{y}$ . Using orthogonality, one can easily show from the equations that all the `mixed terms' vanish, i.e. $\mathbf{y}^+\mathbf{A}\mathbf{z} = 0$ and so on. As for the scalars $a,b,c$ , they are assumed positive and can be shown to be equal to, $a = \sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}} = \sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}, \quad b = \sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}} = \sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}, \quad c = \sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}} = \sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}}$ . I've been able to notice that any triple of distinct canonical basis vectors (with arbitrary phase factors) solves the above problem. In other words, taking $x_j = e^{i\phi_1}\delta_{i_1 j}, \quad y_j = e^{i\phi_2}\delta_{i_2 j}, \quad z_j = e^{i\phi_3}\delta_{i_3 j}$ for any distinct $i_1,i_2,i_3 \in \{1,\ldots,n\}$ and any angles $\phi_1,\phi_2,\phi_3$ does the trick. Sometimes there exist other solutions, e.g. if $D_{ij} = 1 - \delta_{ij}$ , then any orthonormal triple $\mathbf{x},\mathbf{y},\mathbf{z}$ will do. And so my question is: Assuming all $D_{ij}$ 's are distinct (other than $D_{ii} = 0$ and $D_{ij} = D_{ji}$ , that is), is it possible to show that the solutions mentioned above are the only ones? Let me add that with all of the above in mind, one can rewrite the equations in a quite nice form $(\frac{\mathbf{B}}{\sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}}}-\frac{\mathbf{C}}{\sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}})\mathbf{x}=(\sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}}-\sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}) \mathbf{x} \\ (\frac{\mathbf{A}}{\sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}}}-\frac{\mathbf{C}}{\sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}}})\mathbf{y}=(\sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}}-\sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}}) \mathbf{y} \\ (\frac{\mathbf{A}}{\sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}}}+\frac{\mathbf{B}}{\sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}})\mathbf{z}=(\sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}}+\sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}) \mathbf{z}$ But I've not been able to move much further. I tried playing for a while with determinants, but came back empty-handed. Any hint would be much appreciated, even for the real case. EDIT: I've noticed that the original system of three `eigenequations' actually concerns one and the same matrix $a\mathbf{A}+b\mathbf{B}-c\mathbf{C}$ and have now included this fact in the question. Also some other minor errors are now corrected.","In my research I have stumbled upon the following problem: Find all pairwise orthonormal triples of vectors ( ) satisfying the three `eigenequations' where - and this is crucial - the matrices themselves depend on , respectively, being defined as where is the Kronecker symbol and ( ) are certain given coefficients satisfying , for . Notice that the matrices are positive semi-definite and satisfy (I think their rank is actually ) as well as . Using orthogonality, one can easily show from the equations that all the `mixed terms' vanish, i.e. and so on. As for the scalars , they are assumed positive and can be shown to be equal to, . I've been able to notice that any triple of distinct canonical basis vectors (with arbitrary phase factors) solves the above problem. In other words, taking for any distinct and any angles does the trick. Sometimes there exist other solutions, e.g. if , then any orthonormal triple will do. And so my question is: Assuming all 's are distinct (other than and , that is), is it possible to show that the solutions mentioned above are the only ones? Let me add that with all of the above in mind, one can rewrite the equations in a quite nice form But I've not been able to move much further. I tried playing for a while with determinants, but came back empty-handed. Any hint would be much appreciated, even for the real case. EDIT: I've noticed that the original system of three `eigenequations' actually concerns one and the same matrix and have now included this fact in the question. Also some other minor errors are now corrected.","\mathbf{x},\mathbf{y},\mathbf{z} \in \mathbb{C}^n n \geq 4 (a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{x}=(c-b)bc \,\mathbf{x} \\ (a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{y}=(c-a)ca \,\mathbf{y} \\ (a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{z}=(a+b)ab \,\mathbf{z} n\times n \mathbf{A},\mathbf{B},\mathbf{C} \mathbf{x},\mathbf{y},\mathbf{z} A_{ij} = \delta_{ij}\sum_k D_{kj}|x_k|^2 - x_i D_{ij} \bar{x}_j \\ B_{ij} = \delta_{ij}\sum_k D_{kj}|y_k|^2 - y_i D_{ij} \bar{y}_j \\ C_{ij} = \delta_{ij}\sum_k D_{kj}|z_k|^2 - z_i D_{ij} \bar{z_j} \delta_{ij} D_{ij} i,j =1,\ldots,n D_{ii} = 0 D_{ij} = D_{ji} > 0 i \neq j \mathbf{A} \mathbf{x} = \mathbf{B} \mathbf{y} = \mathbf{C} \mathbf{z} = 0 n-1 \mathbf{y}^+\mathbf{C}\mathbf{y} = \mathbf{z}^+\mathbf{B}\mathbf{z}, \quad \mathbf{z}^+\mathbf{A}\mathbf{z} = \mathbf{x}^+\mathbf{C}\mathbf{x}, \quad \mathbf{x}^+\mathbf{B}\mathbf{x} = \mathbf{y}^+\mathbf{A}\mathbf{y} \mathbf{y}^+\mathbf{A}\mathbf{z} = 0 a,b,c a = \sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}} = \sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}, \quad b = \sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}} = \sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}, \quad c = \sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}} = \sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}} x_j = e^{i\phi_1}\delta_{i_1 j}, \quad y_j = e^{i\phi_2}\delta_{i_2 j}, \quad z_j = e^{i\phi_3}\delta_{i_3 j} i_1,i_2,i_3 \in \{1,\ldots,n\} \phi_1,\phi_2,\phi_3 D_{ij} = 1 - \delta_{ij} \mathbf{x},\mathbf{y},\mathbf{z} D_{ij} D_{ii} = 0 D_{ij} = D_{ji} (\frac{\mathbf{B}}{\sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}}}-\frac{\mathbf{C}}{\sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}})\mathbf{x}=(\sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}}-\sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}) \mathbf{x} \\ (\frac{\mathbf{A}}{\sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}}}-\frac{\mathbf{C}}{\sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}}})\mathbf{y}=(\sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}}-\sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}}) \mathbf{y} \\ (\frac{\mathbf{A}}{\sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}}}+\frac{\mathbf{B}}{\sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}})\mathbf{z}=(\sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}}+\sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}) \mathbf{z} a\mathbf{A}+b\mathbf{B}-c\mathbf{C}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'vectors', 'orthonormal']"
58,The determinant of a certain square matrix.,The determinant of a certain square matrix.,,"Let $n > 1$ be an odd number. Let $A$ be an $n \times n$ matrix defined as follows \begin{equation} \label{wams} a_{i, j} = \begin{cases} 1, & \text{for}\ i - j  \equiv \pm 2 \pmod n\\ 2, & \text{for}\ i = j\\ 0, & \text{otherwise}.\end{cases} \end{equation} Calculate the determinant of matrix $A$ . Could someone please give me a hint for this question? I am completely green. I have tried at my best level, and still am not able to come up with a solution.","Let be an odd number. Let be an matrix defined as follows Calculate the determinant of matrix . Could someone please give me a hint for this question? I am completely green. I have tried at my best level, and still am not able to come up with a solution.","n > 1 A n \times n \begin{equation}
\label{wams} a_{i, j} = \begin{cases} 1, & \text{for}\ i - j  \equiv \pm 2 \pmod n\\ 2, & \text{for}\ i = j\\ 0, & \text{otherwise}.\end{cases}
\end{equation} A","['linear-algebra', 'matrices', 'determinant', 'toeplitz-matrices']"
59,Is there a systematic way to generate such matrices?,Is there a systematic way to generate such matrices?,,"Given a positive integer $n > 3$ , is there a way (if any) to generate a matrix $M \in \mathbb{F}_2^{n \times n}$ that satisfies the following three conditions: $M$ is invertible, more precisely, $M \in \text{SL}(n,\mathbb{F}_2)$ $M$ has an almost equal number of $0$ and $1$ entries. That is, $$\| N_M(1) - N_M(0) \| \leq 1$$ where $N_M(1)$ denotes the number of $1$ entries in $M$ . For every $3 \times 3$ -sized blocks (thanks for pointing out this in comments) $M_s$ of $M$ , $$\|N_{M_s}(1) - N_{M_s}(0)\| \leq 1$$ For example, I think, when $n = 4$ , a matrix $$\begin{pmatrix} 1 & 0 & 1 & 0\\ 0 & 1 & 0 & 0\\ 1 & 1 & 0 & 1\\ 0 & 1 & 1 & 0 \end{pmatrix}$$ satisfies all the $3$ requirements listed above. However, I am not sure if such matrices exist for some larger $n > 4$ and I am curious. Motivation I want to use such matrices to encode binary messages. Invertibility ensures one can decode such messages. Up to size $8 \times 8$ is enough for practical uses, I think.","Given a positive integer , is there a way (if any) to generate a matrix that satisfies the following three conditions: is invertible, more precisely, has an almost equal number of and entries. That is, where denotes the number of entries in . For every -sized blocks (thanks for pointing out this in comments) of , For example, I think, when , a matrix satisfies all the requirements listed above. However, I am not sure if such matrices exist for some larger and I am curious. Motivation I want to use such matrices to encode binary messages. Invertibility ensures one can decode such messages. Up to size is enough for practical uses, I think.","n > 3 M \in \mathbb{F}_2^{n \times n} M M \in \text{SL}(n,\mathbb{F}_2) M 0 1 \| N_M(1) - N_M(0) \| \leq 1 N_M(1) 1 M 3 \times 3 M_s M \|N_{M_s}(1) - N_{M_s}(0)\| \leq 1 n = 4 \begin{pmatrix}
1 & 0 & 1 & 0\\
0 & 1 & 0 & 0\\
1 & 1 & 0 & 1\\
0 & 1 & 1 & 0
\end{pmatrix} 3 n > 4 8 \times 8","['matrices', 'finite-fields']"
60,Positive definite matrices - symmetric - diagonal elements,Positive definite matrices - symmetric - diagonal elements,,"a) Show that the tridiagonal matrix $$A=\begin{pmatrix} 2 & 1 & \dots & 0 \\ 1 & 2 & 1 & \vdots \\ \vdots & \vdots & \ddots & \vdots \\ 0 & \ldots & 1 & 2\end{pmatrix}$$ is positive definite. b) Show that a symmetric, positive definite matrix $A\in \mathbb{R}^{n\times n}, \ A=\{a_{k\ell}\}_{k,\ell=1}^n$ has only positive diagonal elements and that it holds that $\displaystyle{\max_{k=1,\ldots, n}| a_{kk}| = \max_{k,\ell=1,\ldots,n}| a_{k\ell}|}$ . I have done the following : a) Let $x\in \mathbb{R}^n\setminus \{\vec{0}\}$ . \begin{align*}x^TAx&=\sum_{i,k=1}^na_{ik}x_ix_k \\ & [\text{ if } i=k : a_{ii}=2 \ , \  \text{ if } k=i-1 : a_{i(i-1)}=1 \ , \  \text{ if } k=i+1 : a_{i(i+1)}=1 ]\\ & =\sum_{i=1}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^n x_ix_{i+1} \\ & =x_1 x_{0}+\sum_{i=2}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} +x_nx_{n+1} \\ & =\sum_{i=2}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} \\ & = \sum_{i=1}^{n-1}x_i x_{i+1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} \\ & = 2\sum_{i=1}^{n-1}x_i x_{i+1}+2\sum_{i=1}^n x_i^2 \\ & = \left (\sum_{i=1}^{n-1} x_i^2+x_n^2\right )+2\sum_{i=1}^{n-1}x_i x_{i+1}+\left (x_1^2+\sum_{i=2}^n x_i^2\right ) \\ & = \sum_{i=1}^{n-1} x_i^2+2\sum_{i=1}^{n-1}x_i x_{i+1}+\sum_{i=2}^n x_i^2+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} x_i^2+2\sum_{i=1}^{n-1}x_i x_{i+1}+\sum_{i=1}^{n-1} x_{i+1}^2+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} \left (x_i^2+2x_i x_{i+1}+ x_{i+1}^2\right )+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} \left (x_i+ x_{i+1}\right )^2+x_1^2+x_n^2 \\ & >0 \end{align*} Since it is a sum of positive (squares) terms. It is equal to zero only if all terms are equal to zero and we get this only when $\vec{x}=\vec{0}$ . Is that correct and complete? $$$$ At b) I need some help. We have that $A$ is symmetric, that means that $\displaystyle{a_{k\ell}=a_{\ell k}}$ , or not? We also have that $A$ is positive definite, that means that $\displaystyle{x^TAx>0 \Rightarrow \sum_{k, \ell=1}^na_{k \ell }x_kx_\ell >0}$ for $x\neq 0$ , right? How do we get that $a_{kk}>0$ ? Do we maybe take the half sum and then the other sum is the same due to symmetric property? $$$$ EDIT : For b) I have done the following : We consider that the contrary is true, i.e $\displaystyle{\max_{k,\ell=1,\ldots,n}|a_{k\ell}|=|a_{ij}|}$ with $i<j$ . Since $A$ is positive definite we have that $x^T A x >0 \quad \forall x\neq 0$ . We consider the vector $x_{ij}$ where the $i$ -th entry is equal to $1$ and the $j$ -th entry is equal to $-1$ and all other entries are equal to $0$ , where $i,j\in \{1, \ldots , n\}$ arbitrary. For this vector we get : \begin{align*}&x_{ij}^T A x_{ij} >0 \\ & \Rightarrow  \begin{pmatrix}0  & \ldots & 0 & 1 & 0 & \ldots & -1 & \ldots & 0\end{pmatrix}\begin{pmatrix}a_{11} & \ldots & a_{1n} \\ \ldots & \ldots & \ldots \\ a_{i1} & \ldots & a_{in} \\ \ldots & \ldots & \ldots \\ a_{j1} & \ldots & a_{jn} \\ \ldots & \ldots & \ldots  \\ a_{n1} & \ldots & a_{nn}\end{pmatrix}\begin{pmatrix}0  \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ -1 \\ \vdots \\ 0\end{pmatrix}>0 \\ & \Rightarrow  \begin{pmatrix}a_{i1}-a_{j1}  & \ldots  & a_{ii}-a_{ji} & \ldots & a_{ij}-a_{jj} & \ldots & a_{in}-a_{jn}\end{pmatrix}\begin{pmatrix}0  \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ -1 \\ \vdots \\ 0\end{pmatrix}>0 \\ & \Rightarrow (a_{ii}-a_{ji})-(a_{ij}-a_{jj})>0\\ & \Rightarrow a_{ii}-a_{ji}-a_{ij}+a_{jj}>0\end{align*} Since $A$ is symmetric we get that $a_{ji}=a_{ij}$ , and so we get \begin{equation*}a_{ii}-a_{ji}-a_{ij}+a_{jj}>0 \Rightarrow a_{ii}-a_{ij}-a_{ij}+a_{jj}>0\Rightarrow a_{ii}+a_{jj}>a_{ij}+a_{ij}\end{equation*} and in that way we get a contradiction. Is that correct and complete?","a) Show that the tridiagonal matrix is positive definite. b) Show that a symmetric, positive definite matrix has only positive diagonal elements and that it holds that . I have done the following : a) Let . Since it is a sum of positive (squares) terms. It is equal to zero only if all terms are equal to zero and we get this only when . Is that correct and complete? At b) I need some help. We have that is symmetric, that means that , or not? We also have that is positive definite, that means that for , right? How do we get that ? Do we maybe take the half sum and then the other sum is the same due to symmetric property? EDIT : For b) I have done the following : We consider that the contrary is true, i.e with . Since is positive definite we have that . We consider the vector where the -th entry is equal to and the -th entry is equal to and all other entries are equal to , where arbitrary. For this vector we get : Since is symmetric we get that , and so we get and in that way we get a contradiction. Is that correct and complete?","A=\begin{pmatrix} 2 & 1 & \dots & 0 \\ 1 & 2 & 1 & \vdots \\ \vdots & \vdots & \ddots & \vdots \\ 0 & \ldots & 1 & 2\end{pmatrix} A\in \mathbb{R}^{n\times n}, \ A=\{a_{k\ell}\}_{k,\ell=1}^n \displaystyle{\max_{k=1,\ldots, n}| a_{kk}| = \max_{k,\ell=1,\ldots,n}| a_{k\ell}|} x\in \mathbb{R}^n\setminus \{\vec{0}\} \begin{align*}x^TAx&=\sum_{i,k=1}^na_{ik}x_ix_k \\ & [\text{ if } i=k : a_{ii}=2 \ , \  \text{ if } k=i-1 : a_{i(i-1)}=1 \ , \  \text{ if } k=i+1 : a_{i(i+1)}=1 ]\\ & =\sum_{i=1}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^n x_ix_{i+1} \\ & =x_1 x_{0}+\sum_{i=2}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} +x_nx_{n+1} \\ & =\sum_{i=2}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} \\ & = \sum_{i=1}^{n-1}x_i x_{i+1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} \\ & = 2\sum_{i=1}^{n-1}x_i x_{i+1}+2\sum_{i=1}^n x_i^2 \\ & = \left (\sum_{i=1}^{n-1} x_i^2+x_n^2\right )+2\sum_{i=1}^{n-1}x_i x_{i+1}+\left (x_1^2+\sum_{i=2}^n x_i^2\right ) \\ & = \sum_{i=1}^{n-1} x_i^2+2\sum_{i=1}^{n-1}x_i x_{i+1}+\sum_{i=2}^n x_i^2+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} x_i^2+2\sum_{i=1}^{n-1}x_i x_{i+1}+\sum_{i=1}^{n-1} x_{i+1}^2+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} \left (x_i^2+2x_i x_{i+1}+ x_{i+1}^2\right )+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} \left (x_i+ x_{i+1}\right )^2+x_1^2+x_n^2 \\ & >0 \end{align*} \vec{x}=\vec{0}  A \displaystyle{a_{k\ell}=a_{\ell k}} A \displaystyle{x^TAx>0 \Rightarrow \sum_{k, \ell=1}^na_{k \ell }x_kx_\ell >0} x\neq 0 a_{kk}>0  \displaystyle{\max_{k,\ell=1,\ldots,n}|a_{k\ell}|=|a_{ij}|} i<j A x^T A x >0 \quad \forall x\neq 0 x_{ij} i 1 j -1 0 i,j\in \{1, \ldots , n\} \begin{align*}&x_{ij}^T A x_{ij} >0 \\ & \Rightarrow  \begin{pmatrix}0  & \ldots & 0 & 1 & 0 & \ldots & -1 & \ldots & 0\end{pmatrix}\begin{pmatrix}a_{11} & \ldots & a_{1n} \\ \ldots & \ldots & \ldots \\ a_{i1} & \ldots & a_{in} \\ \ldots & \ldots & \ldots \\ a_{j1} & \ldots & a_{jn} \\ \ldots & \ldots & \ldots  \\ a_{n1} & \ldots & a_{nn}\end{pmatrix}\begin{pmatrix}0  \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ -1 \\ \vdots \\ 0\end{pmatrix}>0 \\ & \Rightarrow  \begin{pmatrix}a_{i1}-a_{j1}  & \ldots  & a_{ii}-a_{ji} & \ldots & a_{ij}-a_{jj} & \ldots & a_{in}-a_{jn}\end{pmatrix}\begin{pmatrix}0  \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ -1 \\ \vdots \\ 0\end{pmatrix}>0 \\ & \Rightarrow (a_{ii}-a_{ji})-(a_{ij}-a_{jj})>0\\ & \Rightarrow a_{ii}-a_{ji}-a_{ij}+a_{jj}>0\end{align*} A a_{ji}=a_{ij} \begin{equation*}a_{ii}-a_{ji}-a_{ij}+a_{jj}>0 \Rightarrow a_{ii}-a_{ij}-a_{ij}+a_{jj}>0\Rightarrow a_{ii}+a_{jj}>a_{ij}+a_{ij}\end{equation*}","['matrices', 'symmetric-matrices', 'positive-definite', 'tridiagonal-matrices']"
61,"Is it decidable that $A^nx=B^mx$ for some $n,m\geq1$?",Is it decidable that  for some ?,"A^nx=B^mx n,m\geq1","Let $A$ and $B$ be $k\times k$ matrices with integer coefficients and $x \in \mathbb{Z}^k$ be a fixed vector. Is there and algorithm to determine if there exist $n, m \geq 1$ satisfying $A^nx = B^mx$ ? Such an algorithm would be very useful for a problem in which I am working on. Unfortunately, I don't have much intuition about its existence. In my case, $A$ , $B$ and $x$ have positive entries, but I suspect that this is not relevant. Any comment is appreciated.","Let and be matrices with integer coefficients and be a fixed vector. Is there and algorithm to determine if there exist satisfying ? Such an algorithm would be very useful for a problem in which I am working on. Unfortunately, I don't have much intuition about its existence. In my case, , and have positive entries, but I suspect that this is not relevant. Any comment is appreciated.","A B k\times k x \in \mathbb{Z}^k n, m \geq 1 A^nx = B^mx A B x","['linear-algebra', 'matrices', 'algorithms']"
62,Dimension of the fixed space of stochastic matrices (reference request),Dimension of the fixed space of stochastic matrices (reference request),,"Let $A \in \mathbb{R}^{d \times d}$ be row stochastic (i.e., all entries of $A$ are $\ge 0$ , and each row sums up to $1$ . Let $G(A)$ denote the directed graph that is associated to $A$ , i.e., $G(A)$ has vertices $\{1, \dots, n\}$ , and there is an edge from $j$ to $k$ iff $A_{jk} > 0$ . Let us call a non-empty subset $S \subseteq \{1, \dots, n\}$ a sink of $G(A)$ if, from every vertex of $G(A)$ , there is a path to at least one point in $S$ . Theorem. (a) One has $$   \dim \operatorname{Fix}(A)    =    \min \Big\{ \lvert S \rvert : \, S \subseteq \{1, \dots, n\} \text{ is a sink of } G(A) \Big\}, $$ where $\operatorname{Fix}(A) := \ker(\operatorname{id} - A)$ denotes the fixed space of $A$ (in other words: the eigenspace of $A$ for the eigenvalue $1$ ). (b) If $S$ is a sink of $G(A)$ and $A_{jj} > 0$ for all $j \in S$ , then $A$ is aperiodic, i.e., $A$ does not have any eigenvalues in the unit circle except for $1$ . The theorem can, for instance, be proved by combining the classical Perron-Frobenius theorem with a bit of vector lattice theory (but it seems likely that one can find several different proofs). A few colleagues and myself would like to apply this result in an article. It seems very likely that this is known - so we would prefer giving a reference instead of including a proof -, but I couldn't find it in the classical textbooks on (nonnegative) matrices (more specifically, I checked in Gantmacher , Berman and Plemmons , and Minc ). Question: Do you know a reference that contains the theorem? Remark. The theorem also holds if the assumption that $A$ be row stochastic is replaced with the weaker assumption that $A$ has nonnegative entries and a fixed vector whose entries are all strictly positive. A reference for this more general result would be even better - but it can also reduced to the theorem above by a similarity transformation.","Let be row stochastic (i.e., all entries of are , and each row sums up to . Let denote the directed graph that is associated to , i.e., has vertices , and there is an edge from to iff . Let us call a non-empty subset a sink of if, from every vertex of , there is a path to at least one point in . Theorem. (a) One has where denotes the fixed space of (in other words: the eigenspace of for the eigenvalue ). (b) If is a sink of and for all , then is aperiodic, i.e., does not have any eigenvalues in the unit circle except for . The theorem can, for instance, be proved by combining the classical Perron-Frobenius theorem with a bit of vector lattice theory (but it seems likely that one can find several different proofs). A few colleagues and myself would like to apply this result in an article. It seems very likely that this is known - so we would prefer giving a reference instead of including a proof -, but I couldn't find it in the classical textbooks on (nonnegative) matrices (more specifically, I checked in Gantmacher , Berman and Plemmons , and Minc ). Question: Do you know a reference that contains the theorem? Remark. The theorem also holds if the assumption that be row stochastic is replaced with the weaker assumption that has nonnegative entries and a fixed vector whose entries are all strictly positive. A reference for this more general result would be even better - but it can also reduced to the theorem above by a similarity transformation.","A \in \mathbb{R}^{d \times d} A \ge 0 1 G(A) A G(A) \{1, \dots, n\} j k A_{jk} > 0 S \subseteq \{1, \dots, n\} G(A) G(A) S 
  \dim \operatorname{Fix}(A) 
  = 
  \min \Big\{ \lvert S \rvert : \, S \subseteq \{1, \dots, n\} \text{ is a sink of } G(A) \Big\},
 \operatorname{Fix}(A) := \ker(\operatorname{id} - A) A A 1 S G(A) A_{jj} > 0 j \in S A A 1 A A","['matrices', 'graph-theory', 'reference-request', 'eigenvalues-eigenvectors', 'nonnegative-matrices']"
63,Sufficient condition for existence of vectors,Sufficient condition for existence of vectors,,"Suppose that $A\in\mathbb{R}^{m\times m}$ , $B\in\mathbb{R}^{m\times n}$ , $C\in\mathbb{R}^{m\times n}$ and $D\in\mathbb{R}^{m\times m}$ . I'm looking for a sufficient condition on the matrices $A,B,C$ and $D$ such that there exists vectors $x\in\mathbb{R}^{n}$ , $y\in\mathbb{R}^{m}$ and $z\in\mathbb{R}^{m}$ such that \begin{align}  A y &= B x, \\  A z &= C x + Dy,  \end{align} and \begin{align}  y_i &\neq 0 \text{ for each } i = 1,\ldots, m, \\  z_i &\neq 0 \text{ for each } i = 1,\ldots, m, \\  y_i - z_i &\neq 0 \text{ for each } i = 1,\ldots, m. \end{align} Sufficient condition: If $A$ is invertible and if no row of $$M:=\begin{pmatrix}  A^{-1}B\\  C+DA^{-1}B\\  A^{-1}B - C-DA^{-1}B \end{pmatrix}$$ is the null vector. I'm looking for a condition that doesn't assume invertibility. Context: $A,B,C$ and $D$ describe parts of some dynamical system. Exists of such vectors $x,y$ and $z$ guarantees properties of the system.","Suppose that , , and . I'm looking for a sufficient condition on the matrices and such that there exists vectors , and such that and Sufficient condition: If is invertible and if no row of is the null vector. I'm looking for a condition that doesn't assume invertibility. Context: and describe parts of some dynamical system. Exists of such vectors and guarantees properties of the system.","A\in\mathbb{R}^{m\times m} B\in\mathbb{R}^{m\times n} C\in\mathbb{R}^{m\times n} D\in\mathbb{R}^{m\times m} A,B,C D x\in\mathbb{R}^{n} y\in\mathbb{R}^{m} z\in\mathbb{R}^{m} \begin{align}
 A y &= B x, \\
 A z &= C x + Dy, 
\end{align} \begin{align}
 y_i &\neq 0 \text{ for each } i = 1,\ldots, m, \\
 z_i &\neq 0 \text{ for each } i = 1,\ldots, m, \\
 y_i - z_i &\neq 0 \text{ for each } i = 1,\ldots, m.
\end{align} A M:=\begin{pmatrix}  A^{-1}B\\  C+DA^{-1}B\\  A^{-1}B - C-DA^{-1}B \end{pmatrix} A,B,C D x,y z","['linear-algebra', 'matrices']"
64,Permuting subgroups with the same finite index in free abelian group,Permuting subgroups with the same finite index in free abelian group,,"Let $H$ be a subgroup of $\mathbb{Z}^2$ with finite index $m$ . Let $\phi$ be an automorphism on $\mathbb{Z}^2$ . Then $\phi$ corresponds to a matrix in $ \operatorname{GL}(2, \mathbb{Z})$ . Question : What is the bound for the smallest $n \in \mathbb{N} \setminus \{0\}$ , such that $\phi ^n(H) = H$ ? Is it possible to get a linear or even sublinear bound? Thoughts so far : Let $a(m)$ be the number of subgroups in $G$ with index $m$ , then $a(m)$ is bounded by a polynomial but not a linear function in $m$ . It follows that $n$ is bounded by a polynomial in $m$ . I was wondering if this bound can be improved,  it seems unlikely for the automorphism to go through all the subgroups of this index. Any references for this question would be really appreciated, thanks for reading.","Let be a subgroup of with finite index . Let be an automorphism on . Then corresponds to a matrix in . Question : What is the bound for the smallest , such that ? Is it possible to get a linear or even sublinear bound? Thoughts so far : Let be the number of subgroups in with index , then is bounded by a polynomial but not a linear function in . It follows that is bounded by a polynomial in . I was wondering if this bound can be improved,  it seems unlikely for the automorphism to go through all the subgroups of this index. Any references for this question would be really appreciated, thanks for reading.","H \mathbb{Z}^2 m \phi \mathbb{Z}^2 \phi  \operatorname{GL}(2, \mathbb{Z}) n \in \mathbb{N} \setminus \{0\} \phi ^n(H) = H a(m) G m a(m) m n m","['linear-algebra', 'matrices', 'group-theory', 'permutations', 'geometric-group-theory']"
65,Conjugation of integer matrices by $\mathrm{SL}_n \Bbb Z$ and narrowly equivalent ideal classes,Conjugation of integer matrices by  and narrowly equivalent ideal classes,\mathrm{SL}_n \Bbb Z,"I am trying to understand K. Conrad's notes on the Latimer-MacDuffee theorem and related results. Let $\alpha$ be an algebraic integer with minimal polynomial $f$ of degree $n$ . Let $M_f$ be the set of matrices in $M_n \Bbb Z$ with characteristic polynomial $f$ . The Latimer-MacDuffee theorem (Theorem 2.1 in the notes) establishes a bijection between the conjugation classes of matrices in $M_f$ and the ideal class monoid $ICM(\Bbb Z[\alpha])$ of $\Bbb Z[\alpha]$ . Namely, we send a fractional ideal $I$ to the matrix (in some $\Bbb Z$ -basis $B$ ) associated to multiplication by $\alpha$ , viewed as a $\Bbb Z$ -linear map $L_\alpha \colon I \to I$ . In Remark 2.7 of the aforementioned notes, it is claimed that if we instead consider $SL_n \Bbb Z$ conjugacy classes of elements of $M_f$ , then we need to replace the notion of equivalent ideals by narrow equivalence in the following sense: two fractional ideals $I$ and $J$ are narrowly equivalent if there exists $x \in \Bbb Q(\alpha)$ such that $N_{\Bbb Q(\alpha)/\Bbb Q}(x) > 0$ and $I = xJ$ . (This is not in general the relation defining the narrow class group.) I think I have found a counterexample to the claim above, either that or I cannot find the error in my argument. I will first write a concrete counterexample, followed by the more general argument which I would like to proof-check. If $\alpha = i$ and $f = X^2+1$ , the norm of elements in $\Bbb Q(i)$ is always non-negative and $\Bbb Z[i]$ is the maximal order of $\Bbb Q(i)$ . Thus, if the claim were true, $SL_2 \Bbb Z$ -conjugacy classes of integer matrices satisfying $A^2 = -I$ should be in correspondence with $Cl(\Bbb Z[i]) = 0$ . In other words, every two such matrices should be $SL_2 \Bbb Z$ -conjugate. However, if we put $A = \begin{pmatrix}0 & 1\\ -1 & 0\end{pmatrix}, B = \begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix}$ any matrix $C = \begin{pmatrix}a & b \\ c & d\end{pmatrix}$ such that $CA = BC$ must satisfy $a=-d, b = c$ and so $$\det(C) = -(a^2+b^2) \le 0.$$ Now the more general situation: suppose for simplicity that $O := \Bbb Z[\alpha]$ is the maximal order of $K:= \Bbb Q(\alpha)$ . Put $PN(K)$ for the elements of positive norm of $K$ and $PN(O) = PN(K) \cap O$ . Let $NCl(O)$ be the narrow class group in the aforementioned sense and $Cl(O)$ the usual class group. There is an exact sequence $$0 \to O^\times/PN(O) \hookrightarrow K^\times/PN(K) \to NCl(O) \to Cl(O) \to 0,$$ this follows from a similar argument that yields an exact sequence comparing $Cl(O)$ and the usual narrow class group (e.g. as noted here ). Now, $PN(K) = \ker(K^\times \xrightarrow{N} \Bbb Q^\times \xrightarrow{\mathrm{sgn}} \{-1,1\})$ , hence $K^\times/PN(K)$ is isomorphic to a subgroup of $G_2 = \{-1,1\}$ , it will be non-trivial only if there exists an element of negative norm. Likewise $O^\times/PN(O)$ will be either $G_2$ or $1$ depending on the existence of a norm $-1$ integral unit. In particular, $NCl(O) = Cl(O)$ if and only if one of the following holds: there exists $x \in O^\times$ of norm $-1$ . every element in $K^\times$ has positive norm. If the claim of Remark 2.7 were true, then these conditions should characterize when the $SL_n \Bbb Z$ conjugation classes coincide with the $GL_n \Bbb Z$ ones. However, I think the conjugation classes coincide only when (1) holds (hence the counterexample for $\Bbb Q(i)$ ). The relevant direction (for the counterexample) argument is roughly as follows: If $GL_n \Bbb Z$ and $SL_n \Bbb Z$ conjugacy classes coincide for a given matrix $A$ , then $A$ and $DAD^{-1}$ for some matrix such that $\det D= -1$ are $SL_n \Bbb Z$ conjugate. Thus $A = (UD)A(UD)^{-1}$ for some $U$ such that $\det U = 1$ . Hence $A$ commutes with a determinant $-1$ matrix. Now suppose that there exists $A\in M_f$ such that its $GL_n \Bbb Z$ and $SL_n \Bbb Z$ conjugacy classes coincide. By the Latimer-MacDufee theorem,there exists $I$ a fractional ideal and $B$ a $\Bbb Z$ -basis of $I$ such that $A$ is $GL_n \Bbb Z$ conjugate to $X := [L_\alpha \colon I \to I]_B$ . By hypothesis $X$ and $A$ are $SL_n \Bbb Z$ conjugate. Thus the $SL_n \Bbb Z$ and $GL_n \Bbb Z$ conjugacy clases of $X$ also coincide since $$(SL_n \Bbb Z) \cdot X = (SL_n \Bbb Z) \cdot A = (GL_n \Bbb Z) \cdot A = (GL_n \Bbb Z) \cdot X.$$ Thus it must exist $D$ such that $\det D = -1$ commuting with $X$ . In basis $B$ , this defines an $O$ -linear iso $I \to I$ , which must me multiplication by some $x \in K^\times$ . In particular $-1 = \det D$ is the norm of $x$ . Since $xI = I$ , multiplying by $I^{-1}$ it follows that $x \in O^\times$ . I would very much appreciate a sanity check on whether my reasoning is correct and, if so, any comments on how one could compute the $SL_n  \Bbb Z$ conjugacy classes in terms of ideal classes.","I am trying to understand K. Conrad's notes on the Latimer-MacDuffee theorem and related results. Let be an algebraic integer with minimal polynomial of degree . Let be the set of matrices in with characteristic polynomial . The Latimer-MacDuffee theorem (Theorem 2.1 in the notes) establishes a bijection between the conjugation classes of matrices in and the ideal class monoid of . Namely, we send a fractional ideal to the matrix (in some -basis ) associated to multiplication by , viewed as a -linear map . In Remark 2.7 of the aforementioned notes, it is claimed that if we instead consider conjugacy classes of elements of , then we need to replace the notion of equivalent ideals by narrow equivalence in the following sense: two fractional ideals and are narrowly equivalent if there exists such that and . (This is not in general the relation defining the narrow class group.) I think I have found a counterexample to the claim above, either that or I cannot find the error in my argument. I will first write a concrete counterexample, followed by the more general argument which I would like to proof-check. If and , the norm of elements in is always non-negative and is the maximal order of . Thus, if the claim were true, -conjugacy classes of integer matrices satisfying should be in correspondence with . In other words, every two such matrices should be -conjugate. However, if we put any matrix such that must satisfy and so Now the more general situation: suppose for simplicity that is the maximal order of . Put for the elements of positive norm of and . Let be the narrow class group in the aforementioned sense and the usual class group. There is an exact sequence this follows from a similar argument that yields an exact sequence comparing and the usual narrow class group (e.g. as noted here ). Now, , hence is isomorphic to a subgroup of , it will be non-trivial only if there exists an element of negative norm. Likewise will be either or depending on the existence of a norm integral unit. In particular, if and only if one of the following holds: there exists of norm . every element in has positive norm. If the claim of Remark 2.7 were true, then these conditions should characterize when the conjugation classes coincide with the ones. However, I think the conjugation classes coincide only when (1) holds (hence the counterexample for ). The relevant direction (for the counterexample) argument is roughly as follows: If and conjugacy classes coincide for a given matrix , then and for some matrix such that are conjugate. Thus for some such that . Hence commutes with a determinant matrix. Now suppose that there exists such that its and conjugacy classes coincide. By the Latimer-MacDufee theorem,there exists a fractional ideal and a -basis of such that is conjugate to . By hypothesis and are conjugate. Thus the and conjugacy clases of also coincide since Thus it must exist such that commuting with . In basis , this defines an -linear iso , which must me multiplication by some . In particular is the norm of . Since , multiplying by it follows that . I would very much appreciate a sanity check on whether my reasoning is correct and, if so, any comments on how one could compute the conjugacy classes in terms of ideal classes.","\alpha f n M_f M_n \Bbb Z f M_f ICM(\Bbb Z[\alpha]) \Bbb Z[\alpha] I \Bbb Z B \alpha \Bbb Z L_\alpha \colon I \to I SL_n \Bbb Z M_f I J x \in \Bbb Q(\alpha) N_{\Bbb Q(\alpha)/\Bbb Q}(x) > 0 I = xJ \alpha = i f = X^2+1 \Bbb Q(i) \Bbb Z[i] \Bbb Q(i) SL_2 \Bbb Z A^2 = -I Cl(\Bbb Z[i]) = 0 SL_2 \Bbb Z A = \begin{pmatrix}0 & 1\\ -1 & 0\end{pmatrix}, B = \begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix} C = \begin{pmatrix}a & b \\ c & d\end{pmatrix} CA = BC a=-d, b = c \det(C) = -(a^2+b^2) \le 0. O := \Bbb Z[\alpha] K:= \Bbb Q(\alpha) PN(K) K PN(O) = PN(K) \cap O NCl(O) Cl(O) 0 \to O^\times/PN(O) \hookrightarrow K^\times/PN(K) \to NCl(O) \to Cl(O) \to 0, Cl(O) PN(K) = \ker(K^\times \xrightarrow{N} \Bbb Q^\times \xrightarrow{\mathrm{sgn}} \{-1,1\}) K^\times/PN(K) G_2 = \{-1,1\} O^\times/PN(O) G_2 1 -1 NCl(O) = Cl(O) x \in O^\times -1 K^\times SL_n \Bbb Z GL_n \Bbb Z \Bbb Q(i) GL_n \Bbb Z SL_n \Bbb Z A A DAD^{-1} \det D= -1 SL_n \Bbb Z A = (UD)A(UD)^{-1} U \det U = 1 A -1 A\in M_f GL_n \Bbb Z SL_n \Bbb Z I B \Bbb Z I A GL_n \Bbb Z X := [L_\alpha \colon I \to I]_B X A SL_n \Bbb Z SL_n \Bbb Z GL_n \Bbb Z X (SL_n \Bbb Z) \cdot X = (SL_n \Bbb Z) \cdot A = (GL_n \Bbb Z) \cdot A = (GL_n \Bbb Z) \cdot X. D \det D = -1 X B O I \to I x \in K^\times -1 = \det D x xI = I I^{-1} x \in O^\times SL_n  \Bbb Z","['matrices', 'solution-verification', 'algebraic-number-theory']"
66,Image of the map $f : M_n \to M_n$ given by $f(X) = X + \|X-I\|I$.,Image of the map  given by .,f : M_n \to M_n f(X) = X + \|X-I\|I,"Let $M_n = M_n(\Bbb{C})$ be the algebra of $n\times n$ complex matrices and let $\|\cdot\|$ be the operator norm. Consider the map $$f : M_n \to M_n, \qquad f(X) = X + \|X-I\|I.$$ I'm interested in the image of this map. First I noticed that the image seems to be contained in the group of invertible matrices $GL(n)$ . Namely, for all $X \in M_n$ we need to show that $X + \|X-I\|I$ is invertible. By plugging in $X+I$ it is equivalent to show that $X + (1+\|X\|)I$ is invertible for all $X \in M_n$ . But this is true since the spectrum $\sigma(X)$ of $X$ is contained in the closed ball of radius $\|X\|$ around the origin so clearly $-(1+\|X\|) \notin \sigma(X)$ . For $Y \in GL(n)$ we can try to find $X \in M_n$ such that $f(X) = Y$ . Clearly it has to be of the form $X = Y - \alpha I$ for some scalar $\alpha \ge 0$ . Plugging in we have $$Y-\alpha I + \|(Y-\alpha I) - I\| = f(Y-\alpha I) = Y \implies \alpha = \|Y - (\alpha + 1)I\|.$$ The question can be reformulated to: for which $Y \in GL(n)$ exists $\alpha \ge 0$ such that $\alpha = \|Y - (\alpha + 1)I\|$ ? It is easy to see that there is no such $\alpha$ for $Y = \beta I$ when $\beta \in \langle -\infty, 1\rangle$ . So maybe the image is $$GL(n)\setminus \{\beta I : \beta \in \langle -\infty, 1\rangle\}?$$ Any partial information about the image is welcome.","Let be the algebra of complex matrices and let be the operator norm. Consider the map I'm interested in the image of this map. First I noticed that the image seems to be contained in the group of invertible matrices . Namely, for all we need to show that is invertible. By plugging in it is equivalent to show that is invertible for all . But this is true since the spectrum of is contained in the closed ball of radius around the origin so clearly . For we can try to find such that . Clearly it has to be of the form for some scalar . Plugging in we have The question can be reformulated to: for which exists such that ? It is easy to see that there is no such for when . So maybe the image is Any partial information about the image is welcome.","M_n = M_n(\Bbb{C}) n\times n \|\cdot\| f : M_n \to M_n, \qquad f(X) = X + \|X-I\|I. GL(n) X \in M_n X + \|X-I\|I X+I X + (1+\|X\|)I X \in M_n \sigma(X) X \|X\| -(1+\|X\|) \notin \sigma(X) Y \in GL(n) X \in M_n f(X) = Y X = Y - \alpha I \alpha \ge 0 Y-\alpha I + \|(Y-\alpha I) - I\| = f(Y-\alpha I) = Y \implies \alpha = \|Y - (\alpha + 1)I\|. Y \in GL(n) \alpha \ge 0 \alpha = \|Y - (\alpha + 1)I\| \alpha Y = \beta I \beta \in \langle -\infty, 1\rangle GL(n)\setminus \{\beta I : \beta \in \langle -\infty, 1\rangle\}?","['real-analysis', 'linear-algebra', 'matrices', 'functional-analysis', 'matrix-equations']"
67,$\overline{ \{\Delta > 0\} } = \{\Delta\ge 0\}$ for $\Delta$ a certain determinant function,for  a certain determinant function,\overline{ \{\Delta > 0\} } = \{\Delta\ge 0\} \Delta,"Let $A=(a_{ij})$ be a real  matrix. Consider the $(n+1)\times (n+1)$ bordered matrix $\tilde A= (a'_{ij})$ where $\tilde a_{ij} = a_{ij}$ for $1\le i,j\le n$ , $a'_{ij} = 1- \delta_{ij}$ if $\max(i,j) = n+1$ . For instance, if $(a_{ij})$ is $3\times 3$ , then the bordered matrix $\tilde A$ is $$\tilde A=\left(\begin{matrix} a_{11}& a_{12} & a_{13}& 1 \\ a_{21}&a_{22}&a_{23}&1 \\a_{31}&a_{32}& a_{33}& 1\\1&1&1&0\end{matrix} \right)$$ If $A$ is symmetric, then so is $\tilde A$ . Consider the function from real symmetric matrices to real numbers $$\Delta\colon A\mapsto \det \tilde A$$ Note that $\Delta$ is a homogeneous polynomial of degree $n-1$ in the entries of $A$ . I would like to show that if $A$ is a real symmetric matrix -with $\Delta(A)=0$ , then for every $\epsilon > 0$ there exists a symmetric matrix $A'$ , $\|A'- A\|< \epsilon$ , and $\Delta (A')>0$ . Notes: This is related to my previous question . There, the problem was: $\det A= 0$ , find $A'$ close to $A$ , with $\det A'>0$ . @Martin R: showed us how to find a ray $A + \epsilon L$ , $\epsilon>0$ on which the function $\det$ is $>0$ . Moreover, if $A$ is symmetric, then $L$ can be chosen to be symmetric too. Why I am interested  in this problem: Consider $A$ a symmetric matrix that is positive definite on the subspace $\sum x_i = 0$ . Then there exist $\lambda> 0$ such that the matrix $A + \lambda J$ is positive definite ( $J$ the matrix with all $1$ -- see this question ). Consider a principal minor of the matrix $A + \lambda J$ . It will be a polynomial of degree $1$ ( since $J$ is or rank $1$ ) in $\lambda$ . The leading term must be $\ge 0$ . Now the leading term is exactly $- \times$ one of the determinant in the problem. Moreover, this inequality is true for all $A'$ symmetric, close to $A$ ( since $A_{|V}$ positive definite then same is true for $A'$ close to $A$ ). Therefore ( based on our still unproved result), the inequalities must be strict. Thank you for your interest! Any feedback would be appreciated!","Let be a real  matrix. Consider the bordered matrix where for , if . For instance, if is , then the bordered matrix is If is symmetric, then so is . Consider the function from real symmetric matrices to real numbers Note that is a homogeneous polynomial of degree in the entries of . I would like to show that if is a real symmetric matrix -with , then for every there exists a symmetric matrix , , and . Notes: This is related to my previous question . There, the problem was: , find close to , with . @Martin R: showed us how to find a ray , on which the function is . Moreover, if is symmetric, then can be chosen to be symmetric too. Why I am interested  in this problem: Consider a symmetric matrix that is positive definite on the subspace . Then there exist such that the matrix is positive definite ( the matrix with all -- see this question ). Consider a principal minor of the matrix . It will be a polynomial of degree ( since is or rank ) in . The leading term must be . Now the leading term is exactly one of the determinant in the problem. Moreover, this inequality is true for all symmetric, close to ( since positive definite then same is true for close to ). Therefore ( based on our still unproved result), the inequalities must be strict. Thank you for your interest! Any feedback would be appreciated!","A=(a_{ij}) (n+1)\times (n+1) \tilde A= (a'_{ij}) \tilde a_{ij} = a_{ij} 1\le i,j\le n a'_{ij} = 1- \delta_{ij} \max(i,j) = n+1 (a_{ij}) 3\times 3 \tilde A \tilde A=\left(\begin{matrix} a_{11}& a_{12} & a_{13}& 1 \\ a_{21}&a_{22}&a_{23}&1 \\a_{31}&a_{32}& a_{33}& 1\\1&1&1&0\end{matrix} \right) A \tilde A \Delta\colon A\mapsto \det \tilde A \Delta n-1 A A \Delta(A)=0 \epsilon > 0 A' \|A'- A\|< \epsilon \Delta (A')>0 \det A= 0 A' A \det A'>0 A + \epsilon L \epsilon>0 \det >0 A L A \sum x_i = 0 \lambda> 0 A + \lambda J J 1 A + \lambda J 1 J 1 \lambda \ge 0 - \times A' A A_{|V} A' A","['real-analysis', 'linear-algebra', 'matrices', 'quadratic-forms', 'symmetric-matrices']"
68,"Husemoller: homotopy of linear clutching map (proposition $4.5$, pag. $187$)","Husemoller: homotopy of linear clutching map (proposition , pag. )",4.5 187,"Background : I'm currently studying vector bundle through the book of [husemoller,""fibre bundles""] ( https://www.maths.ed.ac.uk/~v1ranick/papers/husemoller ). The following question concerns a detail about a quite specific proof, which I'm going to simplify for the sake of comprehension. However, the pages I'm reffering to are $147-148$ , proposition $4.6$ . Let's define a linear clutching map as map of the form $p(x,z) = a(x)z+b(x)$ , invertible for $|z| \in S^1$ . This a continuos family of automorphism of the fibre over $x$ of a vector bundle $\zeta$ over $X$ (compact connected $CW$ , if needed). Let's also take in account $p_0(x)$ and $p_{\infty}(x)$ two linear maps from $\zeta_x \to \zeta_x$ such that $p_0^2 = p_0$ and $p_{\infty}^2 = p_{\infty}$ (which have an explicit form that I'd like to avoid for the moment if possible). Let's recall that with the ""involution"" property, it holds that the vector space, in this case we may assume $\mathbb{C}^n$ is the direct sum of kernel and image in both cases. Proposition: $p(x,z)p_0(x) = p_{\infty}(x)p(x,z)$ . The proposition give rise to two maps $p_{+} : im p_0 \to im p_{\infty}$ and $p_{-} : ker p_0 \to kerp_{\infty}$ , which are the restrictions of $p(x,z)$ . There is also the followinf proposition: Proposition $(4.5)$ : $p_{+} : im p_0 \to im p_{\infty}$ is an isomorphism for $|z| \geq 1$ while $p_{-} : ker p_0 \to kerp_{\infty}$ is an isomorphism for $|z| \leq 1$ . The background ends here. Problem : Now the problem is the following proposition: Proposition: Let $p_{+},p_{-}$ be defined as above. and let $p^t := > p_{+}^t+p_{-}^t$ where $p_{+}^t := a_{+}z+tb_{+}$ and $p_{-}^t:=  ta_{-}z+b_{-}$ for $0 \leq t \leq 1$ . Then there is a homotopy of linear clutching maps from $a_{+}z+b_{-}$ . Moreover, the bundles $[\zeta,p] \simeq [im p_0,z]\oplus [ker p_0,1]$ The proof given is the following: $\hspace{3.5cm}$ Questions : Why $p_{+}^t,p_{-}^t$ are isomorphism on their images for $0 \leq t\leq 1$ ? I can see an argument of rescaling by $t$ for $p_{-}$ since $|zt| \leq 1$ for $|z| \leq 1$ although I see a problem diving by $t$ in $p_{+}^t$ this rescaling would work for each $t \ne 0$ but in $t=0$ I think I have a problem, or there is a way to prove that $a_{+}$ is invertible apriori? Note that I'm taking $\{\infty\} \in \{|z| \geq 1\}$ so that $\{\infty\} \in \{|z| \geq 1\} \cup \{|z| \leq 1\}$ , perhaps it could help. Why should be true that $a_{+}$ and $b_{-}$ are isomorphism? Why the linear combination i.e the homotopy, which can be re-written as $(1-t)(a_{+}(x)z+b_{-}(x))+tp(x,z)$ is a linear clutching map (i.e invertible) for each $0 \leq t \leq 1?$ For $t = 0$ I don't see how $a_{+}(x)z+b_{-}(x)$ is even defined, since $a_{+}$ and $b_{-}$ are part of the restriction to $a_{+}$ and $b_{-}$ respectively, should take vector in the intersection of the domain, but $ker p_0 \cap im p_0 = \{0\} $ since they are in direct sum. Any help or reference in order to clarify this details would be appreciated, thanks in advance.","Background : I'm currently studying vector bundle through the book of [husemoller,""fibre bundles""] ( https://www.maths.ed.ac.uk/~v1ranick/papers/husemoller ). The following question concerns a detail about a quite specific proof, which I'm going to simplify for the sake of comprehension. However, the pages I'm reffering to are , proposition . Let's define a linear clutching map as map of the form , invertible for . This a continuos family of automorphism of the fibre over of a vector bundle over (compact connected , if needed). Let's also take in account and two linear maps from such that and (which have an explicit form that I'd like to avoid for the moment if possible). Let's recall that with the ""involution"" property, it holds that the vector space, in this case we may assume is the direct sum of kernel and image in both cases. Proposition: . The proposition give rise to two maps and , which are the restrictions of . There is also the followinf proposition: Proposition : is an isomorphism for while is an isomorphism for . The background ends here. Problem : Now the problem is the following proposition: Proposition: Let be defined as above. and let where and for . Then there is a homotopy of linear clutching maps from . Moreover, the bundles The proof given is the following: Questions : Why are isomorphism on their images for ? I can see an argument of rescaling by for since for although I see a problem diving by in this rescaling would work for each but in I think I have a problem, or there is a way to prove that is invertible apriori? Note that I'm taking so that , perhaps it could help. Why should be true that and are isomorphism? Why the linear combination i.e the homotopy, which can be re-written as is a linear clutching map (i.e invertible) for each For I don't see how is even defined, since and are part of the restriction to and respectively, should take vector in the intersection of the domain, but since they are in direct sum. Any help or reference in order to clarify this details would be appreciated, thanks in advance.","147-148 4.6 p(x,z) = a(x)z+b(x) |z| \in S^1 x \zeta X CW p_0(x) p_{\infty}(x) \zeta_x \to \zeta_x p_0^2 = p_0 p_{\infty}^2 = p_{\infty} \mathbb{C}^n p(x,z)p_0(x) = p_{\infty}(x)p(x,z) p_{+} : im p_0 \to im p_{\infty} p_{-} : ker p_0 \to kerp_{\infty} p(x,z) (4.5) p_{+} : im p_0 \to im p_{\infty} |z| \geq 1 p_{-} : ker p_0 \to kerp_{\infty} |z| \leq 1 p_{+},p_{-} p^t :=
> p_{+}^t+p_{-}^t p_{+}^t := a_{+}z+tb_{+} p_{-}^t:=
 ta_{-}z+b_{-} 0 \leq t \leq 1 a_{+}z+b_{-} [\zeta,p] \simeq [im p_0,z]\oplus [ker p_0,1] \hspace{3.5cm} p_{+}^t,p_{-}^t 0 \leq t\leq 1 t p_{-} |zt| \leq 1 |z| \leq 1 t p_{+}^t t \ne 0 t=0 a_{+} \{\infty\} \in \{|z| \geq 1\} \{\infty\} \in \{|z| \geq 1\} \cup \{|z| \leq 1\} a_{+} b_{-} (1-t)(a_{+}(x)z+b_{-}(x))+tp(x,z) 0 \leq t \leq 1? t = 0 a_{+}(x)z+b_{-}(x) a_{+} b_{-} a_{+} b_{-} ker p_0 \cap im p_0 = \{0\} ","['linear-algebra', 'matrices', 'algebraic-topology', 'proof-explanation', 'vector-bundles']"
69,derivative of norm of two matrix.,derivative of norm of two matrix.,,"I am a bit rusty on math. I need to take derivate of this form: $$\frac{d||AW||_2^2}{dW}$$ where $A, W$ are matrices. I recall that $|AW|^2 = (AW)(AW)^T$ . However, I don't know how to solve the derivative in matrix form. Any help? Attempt: Is the solution: $2(W^TWA)$ ?","I am a bit rusty on math. I need to take derivate of this form: where are matrices. I recall that . However, I don't know how to solve the derivative in matrix form. Any help? Attempt: Is the solution: ?","\frac{d||AW||_2^2}{dW} A, W |AW|^2 = (AW)(AW)^T 2(W^TWA)","['linear-algebra', 'matrices', 'derivatives']"
70,How would you encourage an orthogonal matrix to be a permutation matrix?,How would you encourage an orthogonal matrix to be a permutation matrix?,,"We want to solve an optimization problem over the space of permutation matrices (square binary matrices with exactly one entry of $1$ in each row and each column and $0$ s elsewhere) with size $n \times n$ , and we relax the constraint so that we operate optimization for orthogonal matrices ( $P^TP=PP^T=I$ ), how would you encourage the matrices to be permutation matrices? I have tried to add regularization on $\sum_{i,j} P_{ij}^4$ . Would you recommend something else?","We want to solve an optimization problem over the space of permutation matrices (square binary matrices with exactly one entry of in each row and each column and s elsewhere) with size , and we relax the constraint so that we operate optimization for orthogonal matrices ( ), how would you encourage the matrices to be permutation matrices? I have tried to add regularization on . Would you recommend something else?","1 0 n \times n P^TP=PP^T=I \sum_{i,j} P_{ij}^4","['matrices', 'optimization', 'permutations', 'orthogonal-matrices', 'permutation-matrices']"
71,"For $v_1,\dots ,v_n\in\mathbb{C}^n$ define $A=(\langle v_i,v_j\rangle)_{i,j=1}^n$. Prove that $A$ is a non negative linear operator on $\mathbb{C}^n$.",For  define . Prove that  is a non negative linear operator on .,"v_1,\dots ,v_n\in\mathbb{C}^n A=(\langle v_i,v_j\rangle)_{i,j=1}^n A \mathbb{C}^n","Consider $\mathbb C^n$ as an inner product space with the standard inner product $\langle \cdot, \cdot \rangle$ . For $v_1,\dots ,v_n\in \mathbb{C}^n$ define the $n\times n$ matrix $$A=(\langle v_i,v_j \rangle)_{i,j=1}^n.$$ Prove that $A$ is a non negative linear operator on $\mathbb C^n$ . Does the converse hold? Prove or give a counterexample. This link that Vincent pointed out in the comments seems to answer the non-negativity part. I am copying the necessary part of that answer here- Let $x$ be the matrix $$ x=\begin{bmatrix}v_1&v_2&\cdots&v_n\end{bmatrix}. $$ Then $$ x^*x=\begin{bmatrix} v_1^*v_1&v_1^*v_2&\cdots&v_1^*v_n\\ v_2^*v_1&v_2^*v_2&\cdots&v_2^*v_n\\ \vdots & \vdots & \ddots & \vdots \\ v_n^*v_1&v_n^*v_2&\cdots&v_n^*v_n\\  \end{bmatrix} =\begin{bmatrix}  \langle v_1, v_1 \rangle & \langle v_1, v_2\rangle & \cdots &\langle v_1, v_n \rangle \\ \langle v_2, v_1 \rangle & \langle v_2, v_2\rangle & \cdots &\langle v_2, v_n \rangle \\ \vdots & \vdots & \ddots & \vdots \\ \langle v_n, v_1 \rangle & \langle v_n, v_2\rangle & \cdots &\langle v_n, v_n \rangle  \end{bmatrix}. $$ As $x^*x$ is positive-semidefinite, $\det x^*x\geq0$ . Also, the linearity of $A$ trivially follows from the linearity of matrix multiplications. For the converse, I'm not sure how to proceed. I guess, the converse of the given statement is something like ""if $A$ is a non-negative linear operator on $\mathbb C^n$ , then it's entries are of the form $\langle v_i,v_j \rangle$ for $v_1,\dots ,v_n\in \mathbb{C}^n$ "". I feel like it's not true, but I can't produce any counterexamples. Please help me.","Consider as an inner product space with the standard inner product . For define the matrix Prove that is a non negative linear operator on . Does the converse hold? Prove or give a counterexample. This link that Vincent pointed out in the comments seems to answer the non-negativity part. I am copying the necessary part of that answer here- Let be the matrix Then As is positive-semidefinite, . Also, the linearity of trivially follows from the linearity of matrix multiplications. For the converse, I'm not sure how to proceed. I guess, the converse of the given statement is something like ""if is a non-negative linear operator on , then it's entries are of the form for "". I feel like it's not true, but I can't produce any counterexamples. Please help me.","\mathbb C^n \langle \cdot, \cdot \rangle v_1,\dots ,v_n\in \mathbb{C}^n n\times n A=(\langle v_i,v_j \rangle)_{i,j=1}^n. A \mathbb C^n x 
x=\begin{bmatrix}v_1&v_2&\cdots&v_n\end{bmatrix}.
 
x^*x=\begin{bmatrix}
v_1^*v_1&v_1^*v_2&\cdots&v_1^*v_n\\
v_2^*v_1&v_2^*v_2&\cdots&v_2^*v_n\\
\vdots & \vdots & \ddots & \vdots \\
v_n^*v_1&v_n^*v_2&\cdots&v_n^*v_n\\
 \end{bmatrix}
=\begin{bmatrix} 
\langle v_1, v_1 \rangle & \langle v_1, v_2\rangle & \cdots &\langle v_1, v_n \rangle \\
\langle v_2, v_1 \rangle & \langle v_2, v_2\rangle & \cdots &\langle v_2, v_n \rangle \\
\vdots & \vdots & \ddots & \vdots \\
\langle v_n, v_1 \rangle & \langle v_n, v_2\rangle & \cdots &\langle v_n, v_n \rangle
 \end{bmatrix}.
 x^*x \det x^*x\geq0 A A \mathbb C^n \langle v_i,v_j \rangle v_1,\dots ,v_n\in \mathbb{C}^n","['linear-algebra', 'matrices', 'linear-transformations', 'inner-products']"
72,Is it possible to determine the sign of the determinant of a matrix without knowing/using the formula for the determinant?,Is it possible to determine the sign of the determinant of a matrix without knowing/using the formula for the determinant?,,"I'm trying to build intuition for the orientation of a set of vectors independent of the well-known definition of the determinant . My intuition wants to go something like this: any set of vectors can be transformed into the standard basis by a sequence of elementary transformations: shears and positive rescalings should not change the sign of the set of vectors, swaps and negatives rescaling change the sign of the set of vectors. Let $v_1, \ldots, v_N \in \mathbb{R}^N$ . Let $V$ be the matrix whose columns equal $v_i$ . I would like to define a sign function which maps a square matrix $V$ to the set $\{-1, 0, +1\}$ . I'll give two candidates for a sign function which I'll call $S_1(V)$ and $S_2(V)$ . The hope is that $S_{1,2}(V) = \text{sgn}(\text{det}(V))$ , but I would like to establish this without relying on any well-known formulas for $\text{det}(V)$ such as $$ \text{det}(V) = \sum_{\sigma \in S_N} v_{1, \sigma(1)} \ldots v_{N, \sigma(N)} = \sum_{i_1, \ldots, i_N=1}^N \epsilon_{i_1\ldots i_N} v_{1, i_1}\ldots v_{N, i_N}\ $$ I define $S_{1,2}(V)$ as follows. If $V$ is not invertible then $S_{1,2}(V) = 0$ . If $V$ is invertible we proceed as follows. There are three types of elementary matrices. $E^1_i(c)$ is the identity but with $c$ at the $(i,i)$ position instead of 1. $E^2_{ij}(c)$ is the identity but with $c$ at the $(i, j)$ position instead of 0. $E^3_{ij}$ swaps rows $i$ and $j$ . If $V$ is an elementary matrix then if $V=E_i^1(c)$ then let $S_{1,2}(V) = \text{sgn}(c)$ , if $V=E_{ij}^2(c)$ then $S_{1,2}(V)=+1$ , if $V=E_{ij}^3$ then $S_{1,2}(V)=-1$ If $V$ is invertible but not elementary my definitions for $S_1(V)$ and $S_2(V)$ differ. For $S_1(V)$ , if $V$ is invertible but not elementary then $V$ can be uniquely decomposed into elementary row matrices using a well-defined Gauss-Jordan elimination procedure: $$ V = E_1\ldots E_K $$ In this case let $$ S_1(V) = S_1(E_1)\ldots S_1(E_K) $$ For second candidate we define $S_2$ to have the property that $$ S_2(AB) = S_2(A)S_2(B) $$ For $S_1$ it is clearly that $S_1(V)$ is a well-defined function for all matrices, but it is not clear to me how to prove $S_1(AB)=S_1(A)S_1(B)$ . For $S_2$ it is not clear to me how to prove such a function exists. For example, $V$ can be decomposed multiple different ways into elementary matrices. $$ V = E_{1,1}\ldots E_{1,k_1} = E_{2,1}\ldots E_{2, k_2} $$ How to prove $$ S_2(E_{1,1})\ldots S_2(E_{1,k_1}) = S_2(E_{2,1})\ldots S_2(E_{2, k_2})? $$ It is clear that if $S_2$ exists that $S_2(V) = S_1(V)$ and if we relax the constraint that we avoid explicit expersions for $\text{det}(V)$ , then it is clear that all of the above can be proven and $S_1(V)=S_2(V) = \text{sgn}(\text{det}(V))$ , but this is of course what I'm trying to avoid. So my questions are: How to show that $S_1(AB)$ satisfies $S_1(A)S_1(B)$ ? How to show that $S_2(V)$ exists? Is there another way to get at what I am seeking?","I'm trying to build intuition for the orientation of a set of vectors independent of the well-known definition of the determinant . My intuition wants to go something like this: any set of vectors can be transformed into the standard basis by a sequence of elementary transformations: shears and positive rescalings should not change the sign of the set of vectors, swaps and negatives rescaling change the sign of the set of vectors. Let . Let be the matrix whose columns equal . I would like to define a sign function which maps a square matrix to the set . I'll give two candidates for a sign function which I'll call and . The hope is that , but I would like to establish this without relying on any well-known formulas for such as I define as follows. If is not invertible then . If is invertible we proceed as follows. There are three types of elementary matrices. is the identity but with at the position instead of 1. is the identity but with at the position instead of 0. swaps rows and . If is an elementary matrix then if then let , if then , if then If is invertible but not elementary my definitions for and differ. For , if is invertible but not elementary then can be uniquely decomposed into elementary row matrices using a well-defined Gauss-Jordan elimination procedure: In this case let For second candidate we define to have the property that For it is clearly that is a well-defined function for all matrices, but it is not clear to me how to prove . For it is not clear to me how to prove such a function exists. For example, can be decomposed multiple different ways into elementary matrices. How to prove It is clear that if exists that and if we relax the constraint that we avoid explicit expersions for , then it is clear that all of the above can be proven and , but this is of course what I'm trying to avoid. So my questions are: How to show that satisfies ? How to show that exists? Is there another way to get at what I am seeking?","v_1, \ldots, v_N \in \mathbb{R}^N V v_i V \{-1, 0, +1\} S_1(V) S_2(V) S_{1,2}(V) = \text{sgn}(\text{det}(V)) \text{det}(V) 
\text{det}(V) = \sum_{\sigma \in S_N} v_{1, \sigma(1)} \ldots v_{N, \sigma(N)} = \sum_{i_1, \ldots, i_N=1}^N \epsilon_{i_1\ldots i_N} v_{1, i_1}\ldots v_{N, i_N}\
 S_{1,2}(V) V S_{1,2}(V) = 0 V E^1_i(c) c (i,i) E^2_{ij}(c) c (i, j) E^3_{ij} i j V V=E_i^1(c) S_{1,2}(V) = \text{sgn}(c) V=E_{ij}^2(c) S_{1,2}(V)=+1 V=E_{ij}^3 S_{1,2}(V)=-1 V S_1(V) S_2(V) S_1(V) V V 
V = E_1\ldots E_K
 
S_1(V) = S_1(E_1)\ldots S_1(E_K)
 S_2 
S_2(AB) = S_2(A)S_2(B)
 S_1 S_1(V) S_1(AB)=S_1(A)S_1(B) S_2 V 
V = E_{1,1}\ldots E_{1,k_1} = E_{2,1}\ldots E_{2, k_2}
 
S_2(E_{1,1})\ldots S_2(E_{1,k_1}) = S_2(E_{2,1})\ldots S_2(E_{2, k_2})?
 S_2 S_2(V) = S_1(V) \text{det}(V) S_1(V)=S_2(V) = \text{sgn}(\text{det}(V)) S_1(AB) S_1(A)S_1(B) S_2(V)","['linear-algebra', 'matrices', 'determinant', 'signed-measures']"
73,Finding an operator norm in Hilbert space,Finding an operator norm in Hilbert space,,"Let H be Hilbert with an orthonormal basis $(e_n)_n$ . Let $(a_{ij})_{ij}$ , $i,j\geq 1 $ such that $$\sum_{i=1}^\infty \sum_{ j=1}^\infty a_{ij} ^2<\infty.$$ Show there exists a bounded linear operator $T:H\to H$ such that $(Te_j,e_i)=a_{ij}$ and compute the operator norm $\|T\|$ I have defined $$Tx=  \sum_{i=1}^\infty (x, e_i) T e_i=   \sum_{j=1}^\infty  \Big(\sum_{i=1}^\infty (T e_i, e_j) (x, e_i) \Big) e_j=  \sum_{j=1}^\infty  \Big(\sum_{i=1}^\infty a_{ij} (x, e_i) \Big) e_j$$ By Parseval and Cauchy-Schwarz inequality $$\|Tx\|^2=	\sum_{j=1}^\infty  \Big|\sum_{i=1}^\infty a_{ij} (x, e_i)\Big|^2\leq \big(\sum_{i=1}^\infty |(x, e_i)|^2 \big)\big( \sum_{j=1}^\infty \sum_{i=1}^\infty a^2_{ij}\big) = \|x\|^2 \big( \sum_{j=1}^\infty \sum_{i=1}^\infty a^2_{ij}\big). $$ From this it follows that $T$ is bounded and we have $$\|T\|\leq \Big( \sum_{j=1}^\infty \sum_{i=1}^\infty a^2_{ij}\Big)^{1/2}. $$ Is it possible to show that $$\|T\|\geq \Big( \sum_{j=1}^\infty \sum_{i=1}^\infty a^2_{ij}\Big)^{1/2}?$$ Or How to compute $\|T\|$ ?","Let H be Hilbert with an orthonormal basis . Let , such that Show there exists a bounded linear operator such that and compute the operator norm I have defined By Parseval and Cauchy-Schwarz inequality From this it follows that is bounded and we have Is it possible to show that Or How to compute ?","(e_n)_n (a_{ij})_{ij} i,j\geq 1
 \sum_{i=1}^\infty \sum_{ j=1}^\infty a_{ij} ^2<\infty. T:H\to H (Te_j,e_i)=a_{ij} \|T\| Tx=  \sum_{i=1}^\infty (x, e_i) T e_i=   \sum_{j=1}^\infty  \Big(\sum_{i=1}^\infty (T e_i, e_j) (x, e_i) \Big) e_j=  \sum_{j=1}^\infty  \Big(\sum_{i=1}^\infty a_{ij} (x, e_i) \Big) e_j \|Tx\|^2=	\sum_{j=1}^\infty  \Big|\sum_{i=1}^\infty a_{ij} (x, e_i)\Big|^2\leq \big(\sum_{i=1}^\infty |(x, e_i)|^2 \big)\big( \sum_{j=1}^\infty \sum_{i=1}^\infty a^2_{ij}\big) = \|x\|^2 \big( \sum_{j=1}^\infty \sum_{i=1}^\infty a^2_{ij}\big).  T \|T\|\leq \Big( \sum_{j=1}^\infty \sum_{i=1}^\infty a^2_{ij}\Big)^{1/2}.  \|T\|\geq \Big( \sum_{j=1}^\infty \sum_{i=1}^\infty a^2_{ij}\Big)^{1/2}? \|T\|","['matrices', 'functional-analysis', 'operator-theory', 'hilbert-spaces']"
74,Characterize all solutions of the matrix equation $AX=B$ in terms of the SVD $A = U\Sigma V^T$.,Characterize all solutions of the matrix equation  in terms of the SVD .,AX=B A = U\Sigma V^T,"Let $A\in\mathbb R^{m\times n}$ , $B\in\mathbb R^{m\times k}$ and suppose $A$ has an SVD. Assuming $\mathcal R(B) \subseteq \mathcal R(A),$ characterize all solutions of the matrix linear equation $$AX = B$$ in terms of the SVD of $A$ . Attempt: We know that SVD of $A$ and $A^+$ are $A=U\Sigma V^T$ and $A^+ = V\Sigma^+U^T$ , respectively. Thus, all solutions of the matrix equation $$AX=B$$ are of the form $$X = (V\Sigma^+U^T)B +(I- (V\Sigma^+U^T)(U\Sigma V^T))Y.$$ To verify, simply calculate $AX = B$ with our new $X$ . Observe, $$ \begin{equation}\begin{split} AX = (U\Sigma V^T)X &= (U\Sigma V^T)\bigg((V\Sigma^+U^T)B +\big(I- (V\Sigma^+U^T)(U\Sigma V^T)\big)Y\bigg) \\ &= (U\Sigma V^T)V\Sigma^+U^TB +(U\Sigma V^T)(I- V\Sigma^+\Sigma V^T) Y \\ &= U\Sigma \Sigma^+U^TB +(U\Sigma V^T)(\underbrace{I- VIV^T}_{0}) Y \\ &= U\Sigma \Sigma^+U^TB + 0\\ &= B. \end{split}\end{equation} $$ Is this what the question (which seemed trivial) was asking for? Feedback is welcomed! Note that the characterization of all solutions of $AX=B$ not using the SVD is of the form $$X = A^+B +(I- A^+A)Y$$ for $Y\in\mathbb R^{n\times k}$ . NOTE: ' $^+$ ' denotes Moore-Penrose pseudo-inverse.","Let , and suppose has an SVD. Assuming characterize all solutions of the matrix linear equation in terms of the SVD of . Attempt: We know that SVD of and are and , respectively. Thus, all solutions of the matrix equation are of the form To verify, simply calculate with our new . Observe, Is this what the question (which seemed trivial) was asking for? Feedback is welcomed! Note that the characterization of all solutions of not using the SVD is of the form for . NOTE: ' ' denotes Moore-Penrose pseudo-inverse.","A\in\mathbb R^{m\times n} B\in\mathbb R^{m\times k} A \mathcal R(B) \subseteq \mathcal R(A), AX = B A A A^+ A=U\Sigma V^T A^+ = V\Sigma^+U^T AX=B X = (V\Sigma^+U^T)B +(I- (V\Sigma^+U^T)(U\Sigma V^T))Y. AX = B X 
\begin{equation}\begin{split}
AX = (U\Sigma V^T)X &= (U\Sigma V^T)\bigg((V\Sigma^+U^T)B +\big(I- (V\Sigma^+U^T)(U\Sigma V^T)\big)Y\bigg) \\
&= (U\Sigma V^T)V\Sigma^+U^TB +(U\Sigma V^T)(I- V\Sigma^+\Sigma V^T) Y \\
&= U\Sigma \Sigma^+U^TB +(U\Sigma V^T)(\underbrace{I- VIV^T}_{0}) Y \\
&= U\Sigma \Sigma^+U^TB + 0\\
&= B.
\end{split}\end{equation}
 AX=B X = A^+B +(I- A^+A)Y Y\in\mathbb R^{n\times k} ^+","['linear-algebra', 'matrices', 'vector-spaces', 'svd', 'pseudoinverse']"
75,Measurability of random eigenvectors and random eigenvalues,Measurability of random eigenvectors and random eigenvalues,,"Let $A$ be an $n\times n$ real symmetric random matrix. Then $A$ has a spectral decomposition $A=Q\Lambda Q'$ with $Q$ orthogonal and $\Lambda$ diagonal. Am trying to show that we can choose $Q$ and $\Lambda$ to be random matrices, i.e. to have measurable entries. Let $M_n$ denote the space $n\times n$ real matrices, and let $S_n$ denote the subspace of symmetric matrices in $M_n$ . I endow $M_n$ with the Frobenius norm and the corresponding metric topology. I will use the following principle of measurable choice proven here : Theorem. Let $X,Y$ be complete separable metric spaces and $E$ a closed $\sigma$ -compact subset of $X\times Y$ . Then $\pi_1(E)$ is a Borel set in $X$ and there exists a Borel function $\varphi:\pi_1(E)\to Y$ whose graph is contained in $E$ . (Here $\pi_1$ denotes the projection of $X\times Y$ on $X$ ). Let $X=M_n$ , $Y=M_n\times M_n$ and $$E=\bigg\{\big(A,Q,\Lambda\big)\in M_n\times M_n\times M_n: A \text{ is symmetric}, U \text{ is orthogonal},\Lambda \text{ is diagonal}\text{ and } A=Q\Lambda Q'  \bigg\}$$ By identification of $M_n$ and $M_n\times M_n$ with $\mathbb{R}^{n^2}$ and $\mathbb R^{2n^2}$ we see that $X,Y$ are separable complete metric spaces, and by identification of $M_n\times M_n\times M_n$ with $\mathbb R^{3n^2}$ we see that $E$ is $\sigma$ -compact. Using the sequential characterization of a closed set in a metric space and continuity of matrix multiplication we see that $E$ is closed. By the spectral theorem we have $\pi_1(E)=S_n$ . Therefore the above theorem provides us with a Borel function $\varphi:S_n\to M_n\times M_n$ whose graph is contained in $E$ , i.e. such that $\varphi_1(A)$ is orthogonal, $\varphi_2(A)$ is diagonal and $A=\varphi_1(A)\varphi_2(A)\varphi_1(A)'$ for all $A\in S_n$ . Since projections are continuous, $\varphi_1,\varphi_2:S_n\to M_n$ are also Borel measurable. $A$ being a random matrix means that each entry of $A$ is a random variable on some measure space $(\Omega,\mathcal F)$ . By identification of the measure spaces $(M_n,\mathcal B(M_n))$ and $(\mathbb{R}^{n^2},\mathcal B(\mathbb{R}^{n^2}))$ we see that $A:\Omega\to M_n$ is Borel measurable. By assumption $A$ take values in the closed subset $S_n$ of $M_n$ , and so in fact $A:\Omega\to S_n$ is Borel measurable. We conclude that the composition maps $\omega\mapsto Q(\omega):= \varphi_1(A(\omega))$ and $\omega\mapsto \Lambda(\omega):=\varphi_2(A(\omega))$ from $\Omega$ to $M_n$ are both Borel measurable and such that $A(\omega)=Q(\omega)\Lambda(\omega)Q'(\omega)$ for all $\omega\in\Omega$ . Is this proof legitimate? Thank you for your help.","Let be an real symmetric random matrix. Then has a spectral decomposition with orthogonal and diagonal. Am trying to show that we can choose and to be random matrices, i.e. to have measurable entries. Let denote the space real matrices, and let denote the subspace of symmetric matrices in . I endow with the Frobenius norm and the corresponding metric topology. I will use the following principle of measurable choice proven here : Theorem. Let be complete separable metric spaces and a closed -compact subset of . Then is a Borel set in and there exists a Borel function whose graph is contained in . (Here denotes the projection of on ). Let , and By identification of and with and we see that are separable complete metric spaces, and by identification of with we see that is -compact. Using the sequential characterization of a closed set in a metric space and continuity of matrix multiplication we see that is closed. By the spectral theorem we have . Therefore the above theorem provides us with a Borel function whose graph is contained in , i.e. such that is orthogonal, is diagonal and for all . Since projections are continuous, are also Borel measurable. being a random matrix means that each entry of is a random variable on some measure space . By identification of the measure spaces and we see that is Borel measurable. By assumption take values in the closed subset of , and so in fact is Borel measurable. We conclude that the composition maps and from to are both Borel measurable and such that for all . Is this proof legitimate? Thank you for your help.","A n\times n A A=Q\Lambda Q' Q \Lambda Q \Lambda M_n n\times n S_n M_n M_n X,Y E \sigma X\times Y \pi_1(E) X \varphi:\pi_1(E)\to Y E \pi_1 X\times Y X X=M_n Y=M_n\times M_n E=\bigg\{\big(A,Q,\Lambda\big)\in M_n\times M_n\times M_n: A \text{ is symmetric}, U \text{ is orthogonal},\Lambda \text{ is diagonal}\text{ and } A=Q\Lambda Q'  \bigg\} M_n M_n\times M_n \mathbb{R}^{n^2} \mathbb R^{2n^2} X,Y M_n\times M_n\times M_n \mathbb R^{3n^2} E \sigma E \pi_1(E)=S_n \varphi:S_n\to M_n\times M_n E \varphi_1(A) \varphi_2(A) A=\varphi_1(A)\varphi_2(A)\varphi_1(A)' A\in S_n \varphi_1,\varphi_2:S_n\to M_n A A (\Omega,\mathcal F) (M_n,\mathcal B(M_n)) (\mathbb{R}^{n^2},\mathcal B(\mathbb{R}^{n^2})) A:\Omega\to M_n A S_n M_n A:\Omega\to S_n \omega\mapsto Q(\omega):= \varphi_1(A(\omega)) \omega\mapsto \Lambda(\omega):=\varphi_2(A(\omega)) \Omega M_n A(\omega)=Q(\omega)\Lambda(\omega)Q'(\omega) \omega\in\Omega","['matrices', 'measure-theory', 'eigenvalues-eigenvectors', 'solution-verification', 'measurable-functions']"
76,A question on the trace of a matrix,A question on the trace of a matrix,,"Let $f$ be a smooth function on $\mathbb{R}^d$ . We write $\nabla^2f$ for the Hessian matrix of $f$ . For $x \in \mathbb{R}^d$ , we write $\{a_j(x)\}_{j=1}^d$ for the eigenvalue of $\nabla^2 f(x)$ . Then, we have \begin{align*} \Delta f(x)=\text{Trace}[\nabla^2 f(x)]=\sum_{j=1}^d a_j(x). \end{align*} My question Let $\{b_j\}_{j=1}^d$ be $\mathbb{R}$ -valued functions on $\mathbb{R}^d$ . In general, the following formula does not hold: \begin{align*} \text{Trace}[B(x)\nabla^2 f(x)]=\sum_{j=1}^d b_j(x)a_j(x),\quad x \in \mathbb{R}^d. \end{align*} Here, $B(x)$ denotes the $d$ -dimensional diagonal matrix whose $(j,j)$ -th element is $b_j(x)$ . However, is it possible to describe $\sum_{j=1}^d b_j(x)a_j(x)$ using $\nabla^2 f(x)$ and $B(x)$ ?","Let be a smooth function on . We write for the Hessian matrix of . For , we write for the eigenvalue of . Then, we have My question Let be -valued functions on . In general, the following formula does not hold: Here, denotes the -dimensional diagonal matrix whose -th element is . However, is it possible to describe using and ?","f \mathbb{R}^d \nabla^2f f x \in \mathbb{R}^d \{a_j(x)\}_{j=1}^d \nabla^2 f(x) \begin{align*}
\Delta f(x)=\text{Trace}[\nabla^2 f(x)]=\sum_{j=1}^d a_j(x).
\end{align*} \{b_j\}_{j=1}^d \mathbb{R} \mathbb{R}^d \begin{align*}
\text{Trace}[B(x)\nabla^2 f(x)]=\sum_{j=1}^d b_j(x)a_j(x),\quad x \in \mathbb{R}^d.
\end{align*} B(x) d (j,j) b_j(x) \sum_{j=1}^d b_j(x)a_j(x) \nabla^2 f(x) B(x)",['matrices']
77,"Matrix Recurence Relation, $M_n=M_{n-1}M_{n-2}$","Matrix Recurence Relation,",M_n=M_{n-1}M_{n-2},"Let $M_0, M_1\in\mathbb R^{k\times k}$ be matrices, and define $M_n=M_{n-1}M_{n-2}$ . Then $M_n$ is a product of $F_n$ ( $n$ th Fibonacci number) many copies of $M_0$ or $M_1$ . How do I compute the limit $M_n^{1/F_n}$ as $n\to\infty$ ? This is of course easy if $M_0$ and $M_1$ commute, but I'm not sure how to do it in a more general setting. I'm particularly interested in the spectral norm $\|M_n\|^{1/F_n}$ . Can we show, for example, that $\|M_n\|^{1/F_n} \ge \|\frac{M_0+M_1}{2}\|^{1/F_n}$ ?","Let be matrices, and define . Then is a product of ( th Fibonacci number) many copies of or . How do I compute the limit as ? This is of course easy if and commute, but I'm not sure how to do it in a more general setting. I'm particularly interested in the spectral norm . Can we show, for example, that ?","M_0, M_1\in\mathbb R^{k\times k} M_n=M_{n-1}M_{n-2} M_n F_n n M_0 M_1 M_n^{1/F_n} n\to\infty M_0 M_1 \|M_n\|^{1/F_n} \|M_n\|^{1/F_n} \ge \|\frac{M_0+M_1}{2}\|^{1/F_n}","['linear-algebra', 'sequences-and-series', 'matrices', 'matrix-equations']"
78,A question arising from rank of $A^tA$ and rank of $A$,A question arising from rank of  and rank of,A^tA A,"If $A$ is any real matrix, then $A^tA$ and $A$ have same rank. This result is not necessarily true for other fields; one can see various nice comments on this here . The difference comes from following thing: If $(a_1,a_2,\ldots, a_k)$ is a vector with entries in the field $\mathbb{R}$ , then the pointwise (dot) product of this vector with itself is $0$ only if the vector is $\mathbf{0}$ . This is not true for vectors with complex entries, such as $(1,i)$ . I came to the following natural question: Question: Are there non-real fields $F$ , which have property that $$ \left(a_1^2+a_2^2+ \dots + a_k^2 =0\right) \Rightarrow \left(a_1=a_2=\dots = a_k=0\right) $$ for some $k\ge 2$ , or for all $k\ge 2$ .","If is any real matrix, then and have same rank. This result is not necessarily true for other fields; one can see various nice comments on this here . The difference comes from following thing: If is a vector with entries in the field , then the pointwise (dot) product of this vector with itself is only if the vector is . This is not true for vectors with complex entries, such as . I came to the following natural question: Question: Are there non-real fields , which have property that for some , or for all .","A A^tA A (a_1,a_2,\ldots, a_k) \mathbb{R} 0 \mathbf{0} (1,i) F 
\left(a_1^2+a_2^2+ \dots + a_k^2 =0\right) \Rightarrow \left(a_1=a_2=\dots = a_k=0\right)
 k\ge 2 k\ge 2","['linear-algebra', 'abstract-algebra', 'matrices', 'field-theory']"
79,How to show spectral radius of a special matrix is weakly lower than one?,How to show spectral radius of a special matrix is weakly lower than one?,,"Let $A$ be a strictly substochastic matrix, let $x$ be a scalar with $x\in[0,1]$ , and let $$\Lambda(x)\equiv(I-xA^{T})^{-1}.$$ Let $v$ be a vector satisfying $v_{i}\in[0,1],\forall i$ and $\sum_{i}v_{i}=1$ .  Consider the matrix $$J(v,x)=\left(\mathrm{diag}\left(\Lambda(x)v\right)\right)^{-1}\Lambda(x)\left[x\mathrm{diag}\left(v\right)+(1-x)\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-A\iota\right),$$ which can also be written as $$J(v,x)=S(v,x)\left[xI+(1-x)\left(\mathrm{diag}\left(v\right)\right)^{-1}\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-A\iota\right),$$ where $$S(v,x) \equiv \left(\mathrm{diag}\left(\Lambda(x)v\right)\right)^{-1}\Lambda(x)\mathrm{diag}\left(v\right).$$ I want to show that the spectral radius of $J(v,x)$ is weakly lower than one for all $v,x$ . Simulations suggest that this is true and that the maximum is achieved for $v$ at a corner (i.e., $v_{i}=1$ for some $i$ ) and $x=1$ , with the spectral radius at that point equal to $1$ . For the case in which $x = 1$ we have $$J(v,1)=S(v,1) (I-A)^{-1}\mathrm{diag}\left(I-A\iota\right).$$ $S(v,\iota)$ is a stochastic matrix, while $$\left(I-A\right)^{-1}\mathrm{diag}\left(I-A\iota\right)\iota=\left(I-A\right)^{-1}\left(I-A\right)\iota=\iota,$$ so matrix $\left(I-A\right)^{-1}\mathrm{diag}\left(I-A\iota\right)$ is also stochastic, so it follows that $J(v,\iota)$ is substochastic and hence has spectral radius weakly lower than one. However, if $v$ is at a corner with $v_{i}=1$ then $S(v,x)$ is a matrix with ones in column $i$ , implying that $J(v,1)$ is a matrix with all rows equal to row $i$ of matrix $(I-A)^{-1}\mathrm{diag}\left(I-A\iota\right)$ , which adds up to one, and hence has spectral radius equal to one. Thus, the spectral radius of $J(v,1)$ is everywhere weakly lower than one except at corners of $v$ at which it is equal to 1. If $x = 0$ then $J(v,0) = \mathrm{diag}(I-A\iota).$ This is a diagonal matrix with all elements between zero and one and hence the spectral radius is $1-\min_i \sum_j a_{ij}$ , which by assumption is weakly less than one. In the special case in which $A$ is a diagonal matrix then $S(v,x) = I$ and $$ J(v,x) = \left[xI+(1-x) \Lambda(x)\right]\Lambda^{T}(x)\left(I-A\right).$$ Each element of this diagonal matrix is equal to $$ \left( x + \frac{1-x}{1-x a_{ii}} \right) \frac{1-a_{ii}}{1-x a_{ii}},$$ which is lower than one except at $x = 1$ ,  in which case it is equal to one. How can I extend this to show that $\rho(J(v,x))\leq1$ for all $v,x$ ? One idea is to do this in two steps. In step (1) one would show that for any $v,x$ there is a $k$ such that moving to the corner $v$ with $v_k = 1$ (let's call this corner vector by $v^k$ ) increases the spectral radius, $\rho(J(v^k,x))\geq \rho(J(v,x))$ . In step (2) we would then show that $\rho(J(v^k,1 )) \geq \rho(J(v^k,x))$ . Since $ \rho(J(v^k,1))=1$ then these two steps combined would imply that for any $v,x$ we have $\rho(J(v,x)) \leq 1$ . Unfortunately, neither of these two steps seems easy, so this could be a false lead. Something that could be useful is that $$\Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right)\iota=\left(I-xA\right)^{-1}\left(I-xA\right)\iota=\iota,$$ so $\Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right)$ is a stochastic matrix. This implies that $$\Lambda^T(x)\mathrm{diag}\left(I-A\iota\right)\iota \leq \Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right)\iota=\iota,$$ so $\Lambda^T(x)\mathrm{diag}\left(I-A\iota\right)$ is a substochastic matrix. However, we cannot just use the bound $$J(v,x) \leq S(v,x)\left[xI+(1-x)\left(\mathrm{diag}\left(v\right)\right)^{-1}\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-xA\iota\right),$$ because for example in the diagonal case above we would no longer have $\rho(J(v,k)) \leq 1$ (the bound is too lose). Another idea would be to use the fact that $$\rho(J)=\rho(B^{-1}JB)\leq||B^{-1}JB||_{\infty}.$$ The trick here would be to find a matrix $B(v,x)$ such that $$||B^{-1}(v,x)J(v,x)B(v,x)||_{\infty} \leq 1.$$ Since we already have $||J(v^k,1)||_{\infty} = 1$ for any $k$ then presumably one would need that $B(v^k,1) = I$ for any $k$ .","Let be a strictly substochastic matrix, let be a scalar with , and let Let be a vector satisfying and .  Consider the matrix which can also be written as where I want to show that the spectral radius of is weakly lower than one for all . Simulations suggest that this is true and that the maximum is achieved for at a corner (i.e., for some ) and , with the spectral radius at that point equal to . For the case in which we have is a stochastic matrix, while so matrix is also stochastic, so it follows that is substochastic and hence has spectral radius weakly lower than one. However, if is at a corner with then is a matrix with ones in column , implying that is a matrix with all rows equal to row of matrix , which adds up to one, and hence has spectral radius equal to one. Thus, the spectral radius of is everywhere weakly lower than one except at corners of at which it is equal to 1. If then This is a diagonal matrix with all elements between zero and one and hence the spectral radius is , which by assumption is weakly less than one. In the special case in which is a diagonal matrix then and Each element of this diagonal matrix is equal to which is lower than one except at ,  in which case it is equal to one. How can I extend this to show that for all ? One idea is to do this in two steps. In step (1) one would show that for any there is a such that moving to the corner with (let's call this corner vector by ) increases the spectral radius, . In step (2) we would then show that . Since then these two steps combined would imply that for any we have . Unfortunately, neither of these two steps seems easy, so this could be a false lead. Something that could be useful is that so is a stochastic matrix. This implies that so is a substochastic matrix. However, we cannot just use the bound because for example in the diagonal case above we would no longer have (the bound is too lose). Another idea would be to use the fact that The trick here would be to find a matrix such that Since we already have for any then presumably one would need that for any .","A x x\in[0,1] \Lambda(x)\equiv(I-xA^{T})^{-1}. v v_{i}\in[0,1],\forall i \sum_{i}v_{i}=1 J(v,x)=\left(\mathrm{diag}\left(\Lambda(x)v\right)\right)^{-1}\Lambda(x)\left[x\mathrm{diag}\left(v\right)+(1-x)\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-A\iota\right), J(v,x)=S(v,x)\left[xI+(1-x)\left(\mathrm{diag}\left(v\right)\right)^{-1}\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-A\iota\right), S(v,x) \equiv \left(\mathrm{diag}\left(\Lambda(x)v\right)\right)^{-1}\Lambda(x)\mathrm{diag}\left(v\right). J(v,x) v,x v v_{i}=1 i x=1 1 x = 1 J(v,1)=S(v,1) (I-A)^{-1}\mathrm{diag}\left(I-A\iota\right). S(v,\iota) \left(I-A\right)^{-1}\mathrm{diag}\left(I-A\iota\right)\iota=\left(I-A\right)^{-1}\left(I-A\right)\iota=\iota, \left(I-A\right)^{-1}\mathrm{diag}\left(I-A\iota\right) J(v,\iota) v v_{i}=1 S(v,x) i J(v,1) i (I-A)^{-1}\mathrm{diag}\left(I-A\iota\right) J(v,1) v x = 0 J(v,0) = \mathrm{diag}(I-A\iota). 1-\min_i \sum_j a_{ij} A S(v,x) = I  J(v,x) = \left[xI+(1-x) \Lambda(x)\right]\Lambda^{T}(x)\left(I-A\right).  \left( x + \frac{1-x}{1-x a_{ii}} \right) \frac{1-a_{ii}}{1-x a_{ii}}, x = 1 \rho(J(v,x))\leq1 v,x v,x k v v_k = 1 v^k \rho(J(v^k,x))\geq \rho(J(v,x)) \rho(J(v^k,1 )) \geq \rho(J(v^k,x))  \rho(J(v^k,1))=1 v,x \rho(J(v,x)) \leq 1 \Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right)\iota=\left(I-xA\right)^{-1}\left(I-xA\right)\iota=\iota, \Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right) \Lambda^T(x)\mathrm{diag}\left(I-A\iota\right)\iota \leq \Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right)\iota=\iota, \Lambda^T(x)\mathrm{diag}\left(I-A\iota\right) J(v,x) \leq S(v,x)\left[xI+(1-x)\left(\mathrm{diag}\left(v\right)\right)^{-1}\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-xA\iota\right), \rho(J(v,k)) \leq 1 \rho(J)=\rho(B^{-1}JB)\leq||B^{-1}JB||_{\infty}. B(v,x) ||B^{-1}(v,x)J(v,x)B(v,x)||_{\infty} \leq 1. ||J(v^k,1)||_{\infty} = 1 k B(v^k,1) = I k","['linear-algebra', 'matrices', 'spectral-radius']"
80,The magic behind MATLAB's powerful matrix decomposition algorithms [closed],The magic behind MATLAB's powerful matrix decomposition algorithms [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 2 years ago . Improve this question I have always wondered how MATLAB's built-in function for matrix decomposition are crafted and optimized. Examples of MATLAB's built-in function for matrix decompositions are $\mathsf{qr},\mathsf{svd},\mathsf{chol}$ I always wondered how do these functions have their algorithm written with optimized speed and stability performance. Moreover, does there exist algorithms that out performs these built-in functions?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 2 years ago . Improve this question I have always wondered how MATLAB's built-in function for matrix decomposition are crafted and optimized. Examples of MATLAB's built-in function for matrix decompositions are I always wondered how do these functions have their algorithm written with optimized speed and stability performance. Moreover, does there exist algorithms that out performs these built-in functions?","\mathsf{qr},\mathsf{svd},\mathsf{chol}","['matrices', 'matlab', 'numerical-linear-algebra', 'matrix-decomposition']"
81,Alternate proof that principal minors of a singular correlation matrix are non-negative?,Alternate proof that principal minors of a singular correlation matrix are non-negative?,,"What are some elegant proofs to demonstrate that the principal minors of a valid, but singular correlation matrix are all non-negative? A valid correlation matrix is a square symmetric matrix, has ones on its main diagonal and is positive semi-definite. A singular correlation matrix has a matrix determinant of zero, i.e. at least one eigenvalue equal to zero. The proof that I have: It can be shown that any $n\times n$ singular correlation matrix can be written as the product of $n$ idempotent matrices. It can also be shown that the eigenvalues of an idempotent matrix are always either $0$ or $1$ . Therefore, it is not possible that singular correlation matrices have negative eigenvalues. As a consequence, all principal minors of singular correlation matrices have to be non-negative.","What are some elegant proofs to demonstrate that the principal minors of a valid, but singular correlation matrix are all non-negative? A valid correlation matrix is a square symmetric matrix, has ones on its main diagonal and is positive semi-definite. A singular correlation matrix has a matrix determinant of zero, i.e. at least one eigenvalue equal to zero. The proof that I have: It can be shown that any singular correlation matrix can be written as the product of idempotent matrices. It can also be shown that the eigenvalues of an idempotent matrix are always either or . Therefore, it is not possible that singular correlation matrices have negative eigenvalues. As a consequence, all principal minors of singular correlation matrices have to be non-negative.",n\times n n 0 1,"['linear-algebra', 'matrices']"
82,An inequality using column sums of inverse matrices,An inequality using column sums of inverse matrices,,"I want to prove a matrix analogue to inequality $\left(\frac{1-x(1-\alpha)}{\alpha}\right)^{\alpha} x^{1-\alpha}$ for $\alpha \in [0,1)$ and $x \in [0,1]$ , which has a nice proof using GM-AM, as shown in https://math.stackexchange.com/a/4176762/165163 . Let $A=\{a_{ij}\}$ be an $N \times N$ strictly substochastic matrix, $\alpha_i \equiv 1 - \sum_j a_{ij} $ , $x$ be a scalar, and $f(x):\mathbb{R} \rightarrow \mathbb{R}$ be a function defined by $$f(x) = \prod_{i}\left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right)^{\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}}x^{\frac{\bar{w}-N}{\bar{w}}},$$ where $w_{ij}(x)$ are the elements of matrix $W(x) \equiv \left(I-xA\right)^{-1}$ and $\bar{w}\equiv\sum_{i,j}w_{ij}$ . I want to show that $f(x) \leq 1$ for all $x\in[0,1]$ . It is clear that $f(1) = 1$ and that $f(0) = 0.$ Also, note that $\sum_i w_{ji}(1) \alpha_i = 1$ for all $j$ , which follows from $$ \{ \sum_i w_{ji}(1) \alpha_i \} = W(1)\left(I-\mathcal{D}\left\{ A\iota\right\} \right)\iota=W(1)\left(I-A\right)\iota=\left(I-A\right)^{-1}\left(I-A\right)\iota=\iota.$$ This implies that the sum of the powers in the terms above is 1, $$\sum_i \frac{\sum_j w_{ji}(1)\alpha_i}{\bar{w}} + \frac{\bar{w}-N}{\bar{w}} = 1.$$ Thus, we can think of $f(x)$ as a geometric mean. But the arithmetic mean corresponding to this geometric mean, namely $$ \sum_{i} \left(\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}\right) \left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right) + \left(\frac{\bar{w}-N}{\bar{w}} \right) x $$ does not equal one, so the GM-AM approach didn't work immediately in this case, except in the case in which $A$ is diagonal, where the expression just below collapses to $$ \sum_{i} \left(\frac{1}{\bar{w}}\right) \left(\frac{1-x (1-\alpha_i)}{\alpha_i}\right) + \left(\frac{\bar{w}-N}{\bar{w}} \right) x =1, $$ and so the desired inequality follows from the GM-AM. Simulations suggest that the inequality holds -- in fact, they suggest that $f(x)$ is always increasing. I could try to show $f'(x) \geq 0$ but this would limit how much I can then generalize, for example to having $x$ be a vector, in which case $f(x):\mathbb{R}^N \rightarrow \mathbb{R}$ is given by $$f(x)= \prod_{i}\left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right)^{\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}} \prod_i x_i^{\frac{\sum_j w_{ij}-N}{\bar{w}}},$$ where $w_{ij}(x)$ are now the elements of matrix $\left(I-A \mathrm{diag}(x)\right)^{-1}$ Any suggestions would be much appreciated.","I want to prove a matrix analogue to inequality for and , which has a nice proof using GM-AM, as shown in https://math.stackexchange.com/a/4176762/165163 . Let be an strictly substochastic matrix, , be a scalar, and be a function defined by where are the elements of matrix and . I want to show that for all . It is clear that and that Also, note that for all , which follows from This implies that the sum of the powers in the terms above is 1, Thus, we can think of as a geometric mean. But the arithmetic mean corresponding to this geometric mean, namely does not equal one, so the GM-AM approach didn't work immediately in this case, except in the case in which is diagonal, where the expression just below collapses to and so the desired inequality follows from the GM-AM. Simulations suggest that the inequality holds -- in fact, they suggest that is always increasing. I could try to show but this would limit how much I can then generalize, for example to having be a vector, in which case is given by where are now the elements of matrix Any suggestions would be much appreciated.","\left(\frac{1-x(1-\alpha)}{\alpha}\right)^{\alpha} x^{1-\alpha} \alpha \in [0,1) x \in [0,1] A=\{a_{ij}\} N \times N \alpha_i \equiv 1 - \sum_j a_{ij}  x f(x):\mathbb{R} \rightarrow \mathbb{R} f(x) = \prod_{i}\left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right)^{\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}}x^{\frac{\bar{w}-N}{\bar{w}}}, w_{ij}(x) W(x) \equiv \left(I-xA\right)^{-1} \bar{w}\equiv\sum_{i,j}w_{ij} f(x) \leq 1 x\in[0,1] f(1) = 1 f(0) = 0. \sum_i w_{ji}(1) \alpha_i = 1 j  \{ \sum_i w_{ji}(1) \alpha_i \} = W(1)\left(I-\mathcal{D}\left\{ A\iota\right\} \right)\iota=W(1)\left(I-A\right)\iota=\left(I-A\right)^{-1}\left(I-A\right)\iota=\iota. \sum_i \frac{\sum_j w_{ji}(1)\alpha_i}{\bar{w}} + \frac{\bar{w}-N}{\bar{w}} = 1. f(x)  \sum_{i} \left(\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}\right) \left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right) + \left(\frac{\bar{w}-N}{\bar{w}} \right) x  A  \sum_{i} \left(\frac{1}{\bar{w}}\right) \left(\frac{1-x (1-\alpha_i)}{\alpha_i}\right) + \left(\frac{\bar{w}-N}{\bar{w}} \right) x =1,  f(x) f'(x) \geq 0 x f(x):\mathbb{R}^N \rightarrow \mathbb{R} f(x)= \prod_{i}\left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right)^{\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}} \prod_i x_i^{\frac{\sum_j w_{ij}-N}{\bar{w}}}, w_{ij}(x) \left(I-A \mathrm{diag}(x)\right)^{-1}","['matrices', 'algebra-precalculus', 'a.m.-g.m.-inequality']"
83,Using topological tricks to prove algebraic identities with full generality,Using topological tricks to prove algebraic identities with full generality,,"I had a vague idea that algebraic identities proved with such tricks in $\mathbb{R}, \mathbb{C}$ should hold in other rings. In other words, $\mathbb{R}$ or $\mathbb{C}$ does not add new identities, so we can work in those fields without loss of generality. Here are my observations: Let $R$ be a commutative ring with identity. Let's say that we want to prove that $A\operatorname{adj}(A) = \operatorname{adj}(A)A$ for square matrices $A \in M_{n \times n}(R)$ given that $A\operatorname{adj}(A) = \det(A)I$ . First we prove for $R = \mathbb{C}$ . Since the set of singular matrices is Zariski closed in $\mathbb{C}^{n \times n}$ it is nowhere dense in the usual topology, thus we can assume that $A$ is nonsingular. Inverse matrices commute, so we are done. Now since the theory of algebraically closed fields of characteristic zero is complete, the identity(identities) hold in the algebraic closure of the fraction field of $\mathbb{Z}[(x_\alpha)_{\alpha<\kappa}]$ , the polynomial ring in $\kappa$ indeterminates over $\mathbb{Z}$ , where $\kappa$ is an arbitrary cardinal, thus a fortiori in $\mathbb{Z}[(x_\alpha)_{\alpha<\kappa}]$ . Since every commutative ring with identity is a quotient ring of such polynomial rings, we gain the full generality. Furthermore, by elementary arguments, we get $\mathbb{Z}$ has also same identities with $\mathbb{C}$ . So does every commutative ring with characteristic zero. Is this correct?","I had a vague idea that algebraic identities proved with such tricks in should hold in other rings. In other words, or does not add new identities, so we can work in those fields without loss of generality. Here are my observations: Let be a commutative ring with identity. Let's say that we want to prove that for square matrices given that . First we prove for . Since the set of singular matrices is Zariski closed in it is nowhere dense in the usual topology, thus we can assume that is nonsingular. Inverse matrices commute, so we are done. Now since the theory of algebraically closed fields of characteristic zero is complete, the identity(identities) hold in the algebraic closure of the fraction field of , the polynomial ring in indeterminates over , where is an arbitrary cardinal, thus a fortiori in . Since every commutative ring with identity is a quotient ring of such polynomial rings, we gain the full generality. Furthermore, by elementary arguments, we get has also same identities with . So does every commutative ring with characteristic zero. Is this correct?","\mathbb{R}, \mathbb{C} \mathbb{R} \mathbb{C} R A\operatorname{adj}(A) = \operatorname{adj}(A)A A \in M_{n \times n}(R) A\operatorname{adj}(A) = \det(A)I R = \mathbb{C} \mathbb{C}^{n \times n} A \mathbb{Z}[(x_\alpha)_{\alpha<\kappa}] \kappa \mathbb{Z} \kappa \mathbb{Z}[(x_\alpha)_{\alpha<\kappa}] \mathbb{Z} \mathbb{C}","['abstract-algebra', 'matrices', 'logic', 'field-theory']"
84,'elementary' proof that all matrix with zero trace has the form $AB-BA$,'elementary' proof that all matrix with zero trace has the form,AB-BA,"How to prove the fact mentioned in the title without using the theory of lie algebras? For the record, though questions about this fact has indeed appeared in MSE several times, yet up til now, I think no one has given any 'elementary'(only use linear algebra) proof of this fact on MSE. So it's not a duplicate. Any reference or answer would be greatly appreciated. Thanks in advance. P.S. this question is not equivalent to this: If $V_0$ is the subspace of matrices of the form $C=AB-BA$ for some $A,B$ in a vector space $V$ then $V_0=\{A\in V|\operatorname{Trace} (A)=0\}$ or this: Dimensionality of null space when Trace is Zero","How to prove the fact mentioned in the title without using the theory of lie algebras? For the record, though questions about this fact has indeed appeared in MSE several times, yet up til now, I think no one has given any 'elementary'(only use linear algebra) proof of this fact on MSE. So it's not a duplicate. Any reference or answer would be greatly appreciated. Thanks in advance. P.S. this question is not equivalent to this: If $V_0$ is the subspace of matrices of the form $C=AB-BA$ for some $A,B$ in a vector space $V$ then $V_0=\{A\in V|\operatorname{Trace} (A)=0\}$ or this: Dimensionality of null space when Trace is Zero",,"['linear-algebra', 'matrices']"
85,"Given $x= \left ( 6, 2, -3 \right ),$ how to find the coordinate $x$ up to the basis $V$",Given  how to find the coordinate  up to the basis,"x= \left ( 6, 2, -3 \right ), x V","In the vector space $\mathbb{R}^{3},$ given two systems of vectors $$U= \left \{ u_{1}= \left ( 4, 2, 5 \right ), u_{2}= \left ( 2, 1, 3 \right ), u_{3}= \left ( 3, 1, 3 \right ) \right \}$$ $$V= \left \{ v_{1}= \left ( 5, 2, 1 \right ), v_{2}= \left ( 6, 2, 1 \right ), v_{3}= \left ( -1, 7, 4 \right ) \right \}$$ Proved that $U$ and $V$ are two bases of $\mathbb{R}^{3}.$ Source: StackMath/@haidangel_ in.edit In the edited part, I gave a bonus question: Given $x= \left ( 6, 2, -3 \right ).$ How to find the coordinate $x$ up to the basis $V.$ Now I have two approaches but I don't know which one is true ? I need to the help. First approach . Consider the linear combination $$\alpha_{1}v_{1}+ \alpha_{2}v_{2}+ \alpha_{3}v_{3}= x$$ This is equivalent to the matrix equation $$\begin{bmatrix} 5 & 6 & -1\\ 2 & 2 & 7\\ 1 & 1 & 4 \end{bmatrix}\begin{bmatrix} \alpha_{1}\\ \alpha_{2}\\ \alpha_{3} \end{bmatrix}= \begin{bmatrix} 8\\ 2\\ -3 \end{bmatrix}$$ To find the solution, consider the augmented matrix. Applying elementary row operations, we obtain $$\left [ \begin{array}{rrr|r} 5 & 6 & -1 & 8\\ 2 & 2 & 7 & 2\\ 1 & 1 & 4 & -3 \end{array} \right ]\xrightarrow{R_{3}\leftrightarrow R_{2}}\left [ \begin{array}{rrr|r} 5 & 6 & -1 & 8\\ 1 & 1 & 4 & -3\\ 2 & 2 & 7 & 2 \end{array} \right ]\xrightarrow{2R_{2}- R_{3}}\left [ \begin{array}{rrr|r} 5 & 6 & -1 & 8\\ 1 & 1 & 4 & -3\\ 0 & 0 & 1 & -8 \end{array} \right ]$$ $$\left [ \begin{array}{rrr|r} 5 & 6 & -1 & 8\\ 1 & 1 & 4 & -3\\ 0 & 0 & 1 & -8 \end{array} \right ]\xrightarrow{6R_{2}- 25R_{3}- R_{1}}\left [ \begin{array}{rrr|r} 1 & 0 & 0 & 174\\ 1 & 1 & 4 & -3\\ 0 & 0 & 1 & -8 \end{array} \right ]$$ It follows that the solution is $\alpha_{1}= 174, \alpha_{3}= -8, \alpha_{2}= -3- \alpha_{1}- 4\alpha_{3}= -145.$ We obtain $$\left [ x \right ]_{V}= \begin{bmatrix} 174\\ -145\\ -8 \end{bmatrix}$$ Second approach . By the coordinate transformation equation $$\left [ x \right ]_{V}= P_{V\rightarrow E}\cdot\left [ x \right ]_{E}= \left ( P_{E\rightarrow V} \right )^{-1}\cdot\left [ x \right ]_{E}= \begin{bmatrix} 5 & 6 & -1\\ 2 & 2 & 7\\ 1 & 1 & 4 \end{bmatrix}^{-1}\begin{bmatrix} 6\\ 2\\ -3 \end{bmatrix}= \begin{bmatrix} 176\\ -147\\ -8 \end{bmatrix}$$","In the vector space given two systems of vectors Proved that and are two bases of Source: StackMath/@haidangel_ in.edit In the edited part, I gave a bonus question: Given How to find the coordinate up to the basis Now I have two approaches but I don't know which one is true ? I need to the help. First approach . Consider the linear combination This is equivalent to the matrix equation To find the solution, consider the augmented matrix. Applying elementary row operations, we obtain It follows that the solution is We obtain Second approach . By the coordinate transformation equation","\mathbb{R}^{3}, U= \left \{ u_{1}= \left ( 4, 2, 5 \right ), u_{2}= \left ( 2, 1, 3 \right ), u_{3}= \left ( 3, 1, 3 \right ) \right \} V= \left \{ v_{1}= \left ( 5, 2, 1 \right ), v_{2}= \left ( 6, 2, 1 \right ), v_{3}= \left ( -1, 7, 4 \right ) \right \} U V \mathbb{R}^{3}. x= \left ( 6, 2, -3 \right ). x V. \alpha_{1}v_{1}+ \alpha_{2}v_{2}+ \alpha_{3}v_{3}= x \begin{bmatrix} 5 & 6 & -1\\ 2 & 2 & 7\\ 1 & 1 & 4 \end{bmatrix}\begin{bmatrix} \alpha_{1}\\ \alpha_{2}\\ \alpha_{3} \end{bmatrix}= \begin{bmatrix} 8\\ 2\\ -3 \end{bmatrix} \left [ \begin{array}{rrr|r} 5 & 6 & -1 & 8\\ 2 & 2 & 7 & 2\\ 1 & 1 & 4 & -3 \end{array} \right ]\xrightarrow{R_{3}\leftrightarrow R_{2}}\left [ \begin{array}{rrr|r} 5 & 6 & -1 & 8\\ 1 & 1 & 4 & -3\\ 2 & 2 & 7 & 2 \end{array} \right ]\xrightarrow{2R_{2}- R_{3}}\left [ \begin{array}{rrr|r} 5 & 6 & -1 & 8\\ 1 & 1 & 4 & -3\\ 0 & 0 & 1 & -8 \end{array} \right ] \left [ \begin{array}{rrr|r} 5 & 6 & -1 & 8\\ 1 & 1 & 4 & -3\\ 0 & 0 & 1 & -8 \end{array} \right ]\xrightarrow{6R_{2}- 25R_{3}- R_{1}}\left [ \begin{array}{rrr|r} 1 & 0 & 0 & 174\\ 1 & 1 & 4 & -3\\ 0 & 0 & 1 & -8 \end{array} \right ] \alpha_{1}= 174, \alpha_{3}= -8, \alpha_{2}= -3- \alpha_{1}- 4\alpha_{3}= -145. \left [ x \right ]_{V}= \begin{bmatrix} 174\\ -145\\ -8 \end{bmatrix} \left [ x \right ]_{V}= P_{V\rightarrow E}\cdot\left [ x \right ]_{E}= \left ( P_{E\rightarrow V} \right )^{-1}\cdot\left [ x \right ]_{E}= \begin{bmatrix} 5 & 6 & -1\\ 2 & 2 & 7\\ 1 & 1 & 4 \end{bmatrix}^{-1}\begin{bmatrix} 6\\ 2\\ -3 \end{bmatrix}= \begin{bmatrix} 176\\ -147\\ -8 \end{bmatrix}","['linear-algebra', 'matrices']"
86,Measurability of the Moore-Penrose Inverse,Measurability of the Moore-Penrose Inverse,,"Let $A^+$ denote the Moore-Penrose inverse as a function on the space of real $n\times m$ matrices $A\in\mathbb{R}^{n\times m}$ . Is this function $\mathcal B(\mathbb{R}^{n\times m})$ measurable? From here we have the following limit relation: $A^+=\lim_{\delta\downarrow 0} (A'A+\delta I_m)^{-1}A'$ , so in particular we have $A^+=\lim_{k\to \infty} (A'A+\frac{1}{k} I_m)^{-1}A'$ . Matrix transpose and matrix multiplication are continuous on $\mathbb{R}^{n\times m}$ , and matrix inversion on the set of invertible matrices $A\in\mathbb R^{m\times m}$ is also continuous (see here ). Since continuous functions are measurable, we get that $A^+$ is the pointwise limit of a sequence of measurable functions, hence measurable. Is this reasoning ok? Thank you for your help.","Let denote the Moore-Penrose inverse as a function on the space of real matrices . Is this function measurable? From here we have the following limit relation: , so in particular we have . Matrix transpose and matrix multiplication are continuous on , and matrix inversion on the set of invertible matrices is also continuous (see here ). Since continuous functions are measurable, we get that is the pointwise limit of a sequence of measurable functions, hence measurable. Is this reasoning ok? Thank you for your help.",A^+ n\times m A\in\mathbb{R}^{n\times m} \mathcal B(\mathbb{R}^{n\times m}) A^+=\lim_{\delta\downarrow 0} (A'A+\delta I_m)^{-1}A' A^+=\lim_{k\to \infty} (A'A+\frac{1}{k} I_m)^{-1}A' \mathbb{R}^{n\times m} A\in\mathbb R^{m\times m} A^+,"['linear-algebra', 'matrices', 'measure-theory', 'pseudoinverse']"
87,"Solution verification of expressing one variable in terms of other, used in Schweinler-Wigner orthogonalization procedures.","Solution verification of expressing one variable in terms of other, used in Schweinler-Wigner orthogonalization procedures.",,"I don't understand how $$\mathbf{w}_{\kappa}=\sum_{k} p_{\kappa}^{-\frac{1}{2}} u_{k \kappa} \mathbf{v}_{k}\implies\mathbf{v}_{k}=\sum_{\kappa} u_{k \kappa}^{*} p_{\kappa}^{\frac{1}{2}} \mathbf{w}_{k}$$ given $\sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{\ell \kappa}=\delta_{k l}$ . When I tried to solve it, I got an extra summation: We have $\omega_{\kappa}=\sum_{k=1}^{n} p_{\kappa}^{-1 / 2} u_{k \kappa} v_{k}$ so $$u_{k \kappa}^{*} p_{\kappa}^{1 / 2} \omega_{\kappa}=\sum_{k=1}^{n} u_{k \kappa}^{*} p_{\kappa}^{1 / 2} p_{\kappa}^{-1 / 2} u_{k \kappa} v_{k}\implies\sum_{\kappa=1}^{n} u_{k \kappa}^{*} p_{\kappa}^{1 / 2} \omega_{\kappa}=\sum_{k=1}^{n}\left(\sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{k \kappa}\right) v_{k}.$$ Since $\sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{\ell \kappa}=\delta_{k l}$ , it follows that $$\sum_{k=1}^{n} u_{k \kappa}^{*} p^{1/2} \omega_{\kappa}=\sum_{k=1}^{n}\left(\delta_{k k}\right) v_{k}=\sum_{k=1}^{n} v_{k}.$$ For more details on this problem see equations $(3')$ , $(6)$ and $(6')$ of Schweinler (1970) .","I don't understand how given . When I tried to solve it, I got an extra summation: We have so Since , it follows that For more details on this problem see equations , and of Schweinler (1970) .",\mathbf{w}_{\kappa}=\sum_{k} p_{\kappa}^{-\frac{1}{2}} u_{k \kappa} \mathbf{v}_{k}\implies\mathbf{v}_{k}=\sum_{\kappa} u_{k \kappa}^{*} p_{\kappa}^{\frac{1}{2}} \mathbf{w}_{k} \sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{\ell \kappa}=\delta_{k l} \omega_{\kappa}=\sum_{k=1}^{n} p_{\kappa}^{-1 / 2} u_{k \kappa} v_{k} u_{k \kappa}^{*} p_{\kappa}^{1 / 2} \omega_{\kappa}=\sum_{k=1}^{n} u_{k \kappa}^{*} p_{\kappa}^{1 / 2} p_{\kappa}^{-1 / 2} u_{k \kappa} v_{k}\implies\sum_{\kappa=1}^{n} u_{k \kappa}^{*} p_{\kappa}^{1 / 2} \omega_{\kappa}=\sum_{k=1}^{n}\left(\sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{k \kappa}\right) v_{k}. \sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{\ell \kappa}=\delta_{k l} \sum_{k=1}^{n} u_{k \kappa}^{*} p^{1/2} \omega_{\kappa}=\sum_{k=1}^{n}\left(\delta_{k k}\right) v_{k}=\sum_{k=1}^{n} v_{k}. (3') (6) (6'),"['linear-algebra', 'matrices', 'solution-verification', 'orthogonality']"
88,Expected norm of matrix product with random unit vector,Expected norm of matrix product with random unit vector,,Let $S_d = \{x \in \mathbb{R}^d : \|x\| = 1\}$ be the unit sphere in $\mathbb{R}^d$ . Let $A \in \mathbb{R}^{n \times d}$ . Note that \begin{align}     \max_{x \in S_d} \|A x\| &= \max \sigma(A) & \text{(spectral norm)} \\     \min_{x \in S_d} \|A x\| &= \min \sigma(A) \\     \operatorname*{\mathbb{E}}_{x \sim \text{uniform}(S_d)} \|A x\|^2 &= \frac{\|\sigma(A)\|^2}{d} \end{align} where $\sigma$ yields the singular values of a matrix (note that $\|\sigma(\cdot)\| = \|\cdot\|_\text{F}$ is the Frobenius norm). Is there a similar expression for $\operatorname*{\mathbb{E}}_{x \sim \text{uniform}(S_d)} \|A x\|$ ?,Let be the unit sphere in . Let . Note that where yields the singular values of a matrix (note that is the Frobenius norm). Is there a similar expression for ?,"S_d = \{x \in \mathbb{R}^d : \|x\| = 1\} \mathbb{R}^d A \in \mathbb{R}^{n \times d} \begin{align}
    \max_{x \in S_d} \|A x\| &= \max \sigma(A) & \text{(spectral norm)} \\
    \min_{x \in S_d} \|A x\| &= \min \sigma(A) \\
    \operatorname*{\mathbb{E}}_{x \sim \text{uniform}(S_d)} \|A x\|^2 &= \frac{\|\sigma(A)\|^2}{d}
\end{align} \sigma \|\sigma(\cdot)\| = \|\cdot\|_\text{F} \operatorname*{\mathbb{E}}_{x \sim \text{uniform}(S_d)} \|A x\|","['linear-algebra', 'matrices', 'operator-theory', 'normed-spaces', 'singular-values']"
89,Multiplication by an element in semisimple subalgebra of endomorphism,Multiplication by an element in semisimple subalgebra of endomorphism,,"Serge Lang Algebra, Sec. XVII, Exercise 9: Let $E$ be a finite-dimensional vector space over a field $k$ . Let $R$ be a semisimple sub-algebra of $\operatorname{End}_k(E)$ . Let $a, b \in R$ . Assume that $$\ker b_E \supset \ker a_E$$ where $b_E$ is multiplication by $b$ on $E$ and similarly for $a_E$ . Show that there exists an element $s \in R$ such that $sa = b$ . [ Hint : Reduce to $R$ simple. Then $R = \operatorname{End}_D(E_0)$ and $E = E_0^{(n)}$ ). Let $v_1,\dots, v_r$ be a $D$ -basis for $aE$ . Define $s$ by $s(av_j)= bv_j$ , and extend $s$ by $D$ -linearity. Then $sa_E=b_E$ , so $sa=b$ ]. (Simple rings are assumed to be semisimple, and $E^{(n)}$ means the direct sum of $n$ copies of $E$ .) I'm having troubles understanding the hint here. Specifically in the $E=E_0^{(n)}$ part. I think what they meant is that by Artin-Wedderburn's theorem, we may assume $R=M_m(D)$ where $D$ is a division algebra over $k$ . Then $M_m(D) \cong \operatorname{End}_D(V^{(m)})$ where $V$ is a simple left $D$ -module. But I can't see how $E$ relates to $V^{(n)}$ . Perhaps my interpretation is completely off?","Serge Lang Algebra, Sec. XVII, Exercise 9: Let be a finite-dimensional vector space over a field . Let be a semisimple sub-algebra of . Let . Assume that where is multiplication by on and similarly for . Show that there exists an element such that . [ Hint : Reduce to simple. Then and ). Let be a -basis for . Define by , and extend by -linearity. Then , so ]. (Simple rings are assumed to be semisimple, and means the direct sum of copies of .) I'm having troubles understanding the hint here. Specifically in the part. I think what they meant is that by Artin-Wedderburn's theorem, we may assume where is a division algebra over . Then where is a simple left -module. But I can't see how relates to . Perhaps my interpretation is completely off?","E k R \operatorname{End}_k(E) a, b \in R \ker b_E \supset \ker a_E b_E b E a_E s \in R sa = b R R = \operatorname{End}_D(E_0) E = E_0^{(n)} v_1,\dots, v_r D aE s s(av_j)= bv_j s D sa_E=b_E sa=b E^{(n)} n E E=E_0^{(n)} R=M_m(D) D k M_m(D) \cong \operatorname{End}_D(V^{(m)}) V D E V^{(n)}","['abstract-algebra', 'matrices', 'vector-spaces', 'semi-simple-rings']"
90,What might be the (essentially) easiest possible proof of Cayley-Hamilton theorem?,What might be the (essentially) easiest possible proof of Cayley-Hamilton theorem?,,"Today, I learnt that there is a surprisingly easy proof of the Cayley-Hamilton theorem: First we assume $F$ is algebraically closed WLOG, then we treat matrices as points in $F^{n^2}$ where $n$ is the dimension of the vector space $V$ . Now, note that the condition that a matrix is annihilated by its characteristic polynomial is a zariski-closed condition, so we only need to verify this theorem for diagonal matrixes with distinct diagonal entries, which is transparent. I'm deeply impressed by this proof, not only because it's short, but also because this idea seems unbelievably natural and straight-forward once one adopted some of the most basic ideas in Algebraic Geometry. I'm looking forward to some background explanation or similarly easy-and-natural proof of the Cayley-Hamilton theorem.","Today, I learnt that there is a surprisingly easy proof of the Cayley-Hamilton theorem: First we assume is algebraically closed WLOG, then we treat matrices as points in where is the dimension of the vector space . Now, note that the condition that a matrix is annihilated by its characteristic polynomial is a zariski-closed condition, so we only need to verify this theorem for diagonal matrixes with distinct diagonal entries, which is transparent. I'm deeply impressed by this proof, not only because it's short, but also because this idea seems unbelievably natural and straight-forward once one adopted some of the most basic ideas in Algebraic Geometry. I'm looking forward to some background explanation or similarly easy-and-natural proof of the Cayley-Hamilton theorem.",F F^{n^2} n V,"['linear-algebra', 'matrices', 'algebraic-geometry', 'commutative-algebra']"
91,"If $(AB)^n = 0$, must $(BA)^n=0$?","If , must ?",(AB)^n = 0 (BA)^n=0,"Let $A, B$ be two $n\times n$ matrices. If $(AB)^n = 0$ , must $(BA)^n=0$ ? I believe it's true but don't know how to pove it. To clarify, $n$ here is not just any arbitrary integer but the dimension of the matrices.","Let be two matrices. If , must ? I believe it's true but don't know how to pove it. To clarify, here is not just any arbitrary integer but the dimension of the matrices.","A, B n\times n (AB)^n = 0 (BA)^n=0 n","['linear-algebra', 'matrices']"
92,Finding the maximum of the entries of matrix $A B^T$ without computing $A B^T$,Finding the maximum of the entries of matrix  without computing,A B^T A B^T,"Given tall matrices $A, B \in \mathbb{R}^{n \times k}$ , where $n \gg k$ , let $C := AB^T$ . I want to calculate $$ \max C = \max AB^T$$ where $\max: \mathbb{R}^{n\times n} \rightarrow \mathbb{R}$ returns the maximal entry in a matrix. Is there a way to calculate the max element without explicitly calculating $C$ ? This is beneficial in case $n$ is large. Background This problem arises when solving LASSO for large scale data. Basically you choose an active set (i.e., parameters that are not zero and then you set the others to zero) which is relatively small to the number of parameters you want to optimize over. You optimize over the active set using whatever algorithm and then you check the optimality condition for the zero coefficients, which is defined as the maximal element in the multiplication of the residual matrix with the data matrix is less than the regularization parameter. Multiplication the residual (i.e., $A$ ) with the data (i.e., $B$ ) is the computational bottleneck.","Given tall matrices , where , let . I want to calculate where returns the maximal entry in a matrix. Is there a way to calculate the max element without explicitly calculating ? This is beneficial in case is large. Background This problem arises when solving LASSO for large scale data. Basically you choose an active set (i.e., parameters that are not zero and then you set the others to zero) which is relatively small to the number of parameters you want to optimize over. You optimize over the active set using whatever algorithm and then you check the optimality condition for the zero coefficients, which is defined as the maximal element in the multiplication of the residual matrix with the data matrix is less than the regularization parameter. Multiplication the residual (i.e., ) with the data (i.e., ) is the computational bottleneck.","A, B \in \mathbb{R}^{n \times k} n \gg k C := AB^T  \max C = \max AB^T \max: \mathbb{R}^{n\times n} \rightarrow \mathbb{R} C n A B","['matrices', 'optimization', 'algorithms', 'discrete-optimization']"
93,Pseudo determinant of a block matrix with invertible subblock,Pseudo determinant of a block matrix with invertible subblock,,"Consider the following formula for the determinant when $A$ is nonsingular, $$ \det \begin{bmatrix} A & B\\ C & D \end{bmatrix} = \det (A) \det (D - C A^{-1}B). $$ I am thinking if $A$ is non-singular, however, the full matrix is singular, then does the following equation hold? $$ Det \begin{bmatrix} A & B\\ C & D \end{bmatrix} = Det (A) Det (D - C A^{-1}B), $$ where $Det$ denotes the pseudo determinant .","Consider the following formula for the determinant when is nonsingular, I am thinking if is non-singular, however, the full matrix is singular, then does the following equation hold? where denotes the pseudo determinant .","A 
\det \begin{bmatrix}
A & B\\
C & D
\end{bmatrix} = \det (A) \det (D - C A^{-1}B).
 A 
Det \begin{bmatrix}
A & B\\
C & D
\end{bmatrix} = Det (A) Det (D - C A^{-1}B),
 Det","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
94,conditions for which matrix has all eigenvalues inside unit circle or on the unit circle,conditions for which matrix has all eigenvalues inside unit circle or on the unit circle,,"$\mathbf{M}=\begin{bmatrix} \mathbf{y}&0&0&\cdots&0&0&\mathbf{x}\\ \mathbf{I}&0&0&\cdots&0&0&0\\ 0&\mathbf{I}&0&0&0&0&0\\ 0&0&\mathbf{I}&0&0&0&0\\ \vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\ 0&0&0&\cdots&\mathbf{I}&0&0\\ 0&0&0&0&0&\mathbf{I}&0\\ \end{bmatrix}$ Given that $\mathbf{y}=\operatorname{diag}\left(y_i\right)$ and $\mathbf{x}=\operatorname{diag}\left(x_i\right) $ where each $i=1,2,\dots, N$ , $y_i$ are $N\times N$ matrices whose eigenvalues are strictly less inside unit circle. $I$ are $N\times N$ . For each $i$ , $x_i$ are also $N\times N$ . Could anyone help me to find out under what condition(s) this matrix $\mathbf{M}$ has the property that modulus of its all eigenvalues strictly less than $1$ given that each block matrix in $\mathbf{y}$ are given that they all have eigenvalues less than $1$ , for all $i$ ? Thank you.","Given that and where each , are matrices whose eigenvalues are strictly less inside unit circle. are . For each , are also . Could anyone help me to find out under what condition(s) this matrix has the property that modulus of its all eigenvalues strictly less than given that each block matrix in are given that they all have eigenvalues less than , for all ? Thank you.","\mathbf{M}=\begin{bmatrix}
\mathbf{y}&0&0&\cdots&0&0&\mathbf{x}\\
\mathbf{I}&0&0&\cdots&0&0&0\\
0&\mathbf{I}&0&0&0&0&0\\
0&0&\mathbf{I}&0&0&0&0\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
0&0&0&\cdots&\mathbf{I}&0&0\\
0&0&0&0&0&\mathbf{I}&0\\
\end{bmatrix} \mathbf{y}=\operatorname{diag}\left(y_i\right) \mathbf{x}=\operatorname{diag}\left(x_i\right)  i=1,2,\dots, N y_i N\times N I N\times N i x_i N\times N \mathbf{M} 1 \mathbf{y} 1 i","['matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'stability-theory']"
95,Bound on steepest descent,Bound on steepest descent,,I'm learning about the method of steepest descent for approximating the solution of $Ax=b$ where A is an invertible matrix. Here is the part I understand: We do this by minimizing the function $f(x)=\frac{1}{2}\|Ax-b\|_2^2$ . The $\nabla f(x)=A^TAx-A^Tb$ . Let $M=A^TA$ and $c=A^Tb$ . I came across this bound $$f(x^{(i)}) \leq f(x^{(0)})\left(1-\frac{1}{k(A)^2}\right)^i$$ . Where $x^{(i)}$ ist the $i^{th}$ gradient descent step and is defined as follows. $$r^{(i)}=c-Mx^{(i)}\\ \alpha^{(i)}=\frac{(r^{(i)})^T r_n^{(i)}}{(r^{(i)})^T M r^{(i)}}\\ x^{(i+1)}=x^{(i)}+\alpha r^{(i)}$$ and $k(A)$ is the condition number of a matrix. I was able to derive some other bounds such as $$ \frac{f(x^{(i)})-f(x^{(*)})}{f(x^{(0)})-f(x^{(*)})}<\left(\frac{k(M)-1}{k(M)-1}\right)^{2i} $$ but I can't wrap my head around the one mentioned above. Any pointers or suggestions would be great. Thanks!,I'm learning about the method of steepest descent for approximating the solution of where A is an invertible matrix. Here is the part I understand: We do this by minimizing the function . The . Let and . I came across this bound . Where ist the gradient descent step and is defined as follows. and is the condition number of a matrix. I was able to derive some other bounds such as but I can't wrap my head around the one mentioned above. Any pointers or suggestions would be great. Thanks!,"Ax=b f(x)=\frac{1}{2}\|Ax-b\|_2^2 \nabla f(x)=A^TAx-A^Tb M=A^TA c=A^Tb f(x^{(i)}) \leq f(x^{(0)})\left(1-\frac{1}{k(A)^2}\right)^i x^{(i)} i^{th} r^{(i)}=c-Mx^{(i)}\\
\alpha^{(i)}=\frac{(r^{(i)})^T r_n^{(i)}}{(r^{(i)})^T M r^{(i)}}\\
x^{(i+1)}=x^{(i)}+\alpha r^{(i)} k(A) 
\frac{f(x^{(i)})-f(x^{(*)})}{f(x^{(0)})-f(x^{(*)})}<\left(\frac{k(M)-1}{k(M)-1}\right)^{2i}
","['matrices', 'convergence-divergence', 'upper-lower-bounds', 'gradient-descent', 'condition-number']"
96,Is the matrix $\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix}$ significant?,Is the matrix  significant?,\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix},"Is the matrix $\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix}$ significant? The reason I am asking is as follows. As a 'challenge' question at the end of my textbook it asked  to prove by induction that if $$\textbf{A}=\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix}$$ then for $n\in\mathbb N$ $$\textbf{A}^n=\textbf{A}=\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix}$$ I proved this easily, but it struck me that this matrix behaves in exactly the same way as the identity matrices. Of course, this result is equivalent to $$\begin{pmatrix} \cos^2 x & \cos^2 x\\ \sin^2x&\sin^2x\end{pmatrix}^n=\begin{pmatrix} \cos^2 x & \cos^2 x\\ \sin^2x&\sin^2x\end{pmatrix}$$ So, are  these results used in  more advanced linear algebra or university level maths? It just seems too coincidental not to be used in some more advanced results or topics.","Is the matrix significant? The reason I am asking is as follows. As a 'challenge' question at the end of my textbook it asked  to prove by induction that if then for I proved this easily, but it struck me that this matrix behaves in exactly the same way as the identity matrices. Of course, this result is equivalent to So, are  these results used in  more advanced linear algebra or university level maths? It just seems too coincidental not to be used in some more advanced results or topics.",\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix} \textbf{A}=\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix} n\in\mathbb N \textbf{A}^n=\textbf{A}=\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix} \begin{pmatrix} \cos^2 x & \cos^2 x\\ \sin^2x&\sin^2x\end{pmatrix}^n=\begin{pmatrix} \cos^2 x & \cos^2 x\\ \sin^2x&\sin^2x\end{pmatrix},"['linear-algebra', 'matrices', 'algebra-precalculus', 'trigonometry']"
97,Contraction on 3x3 matrices?,Contraction on 3x3 matrices?,,"I would like to prove that the following linear map is a contraction on the $3 \times 3$ complex matrices endowed with the usual operator norm (i.e. the largest singular value norm): $$ \begin{pmatrix}            a_{11} & a_{12} & a_{13} \\   a_{21} & a_{22} & a_{23} \\     a_{31} & a_{32} & a_{33}  \end{pmatrix}  \mapsto {2\over 3}\begin{pmatrix}            a_{11} & a_{12} & a_{13} \\   a_{21} & a_{22} & a_{23} \\     a_{31} & a_{32} & {a_{11} + a_{22} \over 2} \end{pmatrix}. $$ Numerical simulation shows this is likely to be true, but it is absolutely not clear to me how to prove it. Any idea is welcomed.","I would like to prove that the following linear map is a contraction on the complex matrices endowed with the usual operator norm (i.e. the largest singular value norm): Numerical simulation shows this is likely to be true, but it is absolutely not clear to me how to prove it. Any idea is welcomed.","3 \times 3 
\begin{pmatrix}
           a_{11} & a_{12} & a_{13} \\
  a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} 
\end{pmatrix}
 \mapsto {2\over 3}\begin{pmatrix}
           a_{11} & a_{12} & a_{13} \\
  a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & {a_{11} + a_{22} \over 2}
\end{pmatrix}. ","['linear-algebra', 'matrices', 'operator-theory', 'numerical-linear-algebra', 'matrix-norms']"
98,Spectrum of semi-infinite Toeplitz matrices,Spectrum of semi-infinite Toeplitz matrices,,"I am considering a self-adjoint semi-infinite Toeplitz matrix , by which I mean $$M = \left(\begin{array}{ccccc} a_0  & a_1  & a_2  &  a_3 & \cdots \\ a_1^*& a_0  & a_1  &  a_2 & \ddots \\ a_2^* & a_1^*& a_0  & a_1  &  \ddots \\ a_3^*& a_2^* & a_1^*& a_0  & \ddots \\ \vdots & \ddots & \ddots & \ddots & \ddots \end{array}\right) $$ where $\{a_n\}_{n=-\infty}^{+\infty}$ are complex numbers, and the star "" $^*$ "" indicates the complex conjugation. It is assumed that $\sum_n \left|a_n\right|$ is finite. I would like to understand the eigenvalue spectrum of $M$ , especially when compared to its infinite analog $M_0$ . It is well known (and shown e.g. by going to a Fourier-transformed basis) that the eigenvalues of $$M_0 = \left(\begin{array}{cccccc} \ddots & \ddots  & \ddots  & \ddots  &  \ddots & \ddots \\ \ddots & a_0  & a_1  & a_2  &  a_3 & \ddots \\ \ddots & a_1^*& a_0  & a_1  &  a_2 & \ddots \\ \ddots & a_2^* & a_1^*& a_0  & a_1  &  \ddots \\ \ddots & a_3^*& a_2^* & a_1^*& a_0  & \ddots \\ \ddots & \ddots & \ddots & \ddots & \ddots & \ddots \end{array}\right) $$ form a one-dimensional family $$\lambda(\omega) = a_0 + 2\sum_{n=1}^{+\infty} \left[\Re[a_n] \cos(n \omega)+\Im[a_n] \sin(n \omega )\right]$$ where $\omega\in [0,2\pi)$ . It then follows from the finiteness of $\sum_n |a_n|$ that the spectrum of $M_0$ is both compact and connected, namely an interval $$\textrm{spec}(M_0) = [x_1,x_2]$$ where $|x_{1,2}|<\infty$ are some finite numbers. However, the spectrum of the semi-infinite matrix $M$ cannot be found using the same Fourier-transform trick, and I suspect there is no exact solution in this case. The question: Is it true that $\textrm{spec}(M)\subseteq \textrm{spec}(M_0)$ ? My simple numerical tests suggest that this should be true, but I didn't manage to prove this and I also failed to find a suitable reference. I was hoping that someone in the Stack Exchange community either readily knows the answer, or could point me to the relevant references/books. If the answer turns out to be ""no"", then I also have a second question: Is $\textrm{spec}(M)$ always connected (as is the case for $M_0$ ), or is it possible that it exhibits some isolated eigenvalues detached from the continuum part of the spectrum?","I am considering a self-adjoint semi-infinite Toeplitz matrix , by which I mean where are complex numbers, and the star "" "" indicates the complex conjugation. It is assumed that is finite. I would like to understand the eigenvalue spectrum of , especially when compared to its infinite analog . It is well known (and shown e.g. by going to a Fourier-transformed basis) that the eigenvalues of form a one-dimensional family where . It then follows from the finiteness of that the spectrum of is both compact and connected, namely an interval where are some finite numbers. However, the spectrum of the semi-infinite matrix cannot be found using the same Fourier-transform trick, and I suspect there is no exact solution in this case. The question: Is it true that ? My simple numerical tests suggest that this should be true, but I didn't manage to prove this and I also failed to find a suitable reference. I was hoping that someone in the Stack Exchange community either readily knows the answer, or could point me to the relevant references/books. If the answer turns out to be ""no"", then I also have a second question: Is always connected (as is the case for ), or is it possible that it exhibits some isolated eigenvalues detached from the continuum part of the spectrum?","M = \left(\begin{array}{ccccc}
a_0  & a_1  & a_2  &  a_3 & \cdots \\
a_1^*& a_0  & a_1  &  a_2 & \ddots \\
a_2^* & a_1^*& a_0  & a_1  &  \ddots \\
a_3^*& a_2^* & a_1^*& a_0  & \ddots \\
\vdots & \ddots & \ddots & \ddots & \ddots
\end{array}\right)  \{a_n\}_{n=-\infty}^{+\infty} ^* \sum_n \left|a_n\right| M M_0 M_0 = \left(\begin{array}{cccccc}
\ddots & \ddots  & \ddots  & \ddots  &  \ddots & \ddots \\
\ddots & a_0  & a_1  & a_2  &  a_3 & \ddots \\
\ddots & a_1^*& a_0  & a_1  &  a_2 & \ddots \\
\ddots & a_2^* & a_1^*& a_0  & a_1  &  \ddots \\
\ddots & a_3^*& a_2^* & a_1^*& a_0  & \ddots \\
\ddots & \ddots & \ddots & \ddots & \ddots & \ddots
\end{array}\right)  \lambda(\omega) = a_0 + 2\sum_{n=1}^{+\infty} \left[\Re[a_n] \cos(n \omega)+\Im[a_n] \sin(n \omega )\right] \omega\in [0,2\pi) \sum_n |a_n| M_0 \textrm{spec}(M_0) = [x_1,x_2] |x_{1,2}|<\infty M \textrm{spec}(M)\subseteq \textrm{spec}(M_0) \textrm{spec}(M) M_0","['matrices', 'eigenvalues-eigenvectors', 'spectral-theory', 'toeplitz-matrices', 'infinite-matrices']"
99,How will the singular values change after right multiplication with the diagonal matrix?,How will the singular values change after right multiplication with the diagonal matrix?,,"Say that we have an SVD for a matrix $X = U \Sigma V^T$ , with the singular values $\sigma_i$ . What will happens  to the SVD and singular values if we right multiply with a diagonal matrix that simply scales the columns of $X$ ? Is it possible to write the SVD of $XD$ or get the singular values $\sigma_i$ in terms of the diagonal of $D$ and the SVD of $X$ ? And more specifically: would it be possible to get the SVD of the $XD$ in terms of the same $U$ we've obtained from the SVD of the X?","Say that we have an SVD for a matrix , with the singular values . What will happens  to the SVD and singular values if we right multiply with a diagonal matrix that simply scales the columns of ? Is it possible to write the SVD of or get the singular values in terms of the diagonal of and the SVD of ? And more specifically: would it be possible to get the SVD of the in terms of the same we've obtained from the SVD of the X?",X = U \Sigma V^T \sigma_i X XD \sigma_i D X XD U,"['linear-algebra', 'matrices', 'svd', 'singular-values']"
