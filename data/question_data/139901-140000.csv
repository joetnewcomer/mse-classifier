,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"ODEs in the space of distributions $\mathcal{D}'(a,b)$: some general solution technique.",ODEs in the space of distributions : some general solution technique.,"\mathcal{D}'(a,b)","I am studying ordinary differential equations in the space of distribution $\mathcal{D}'$ where $\mathcal{D}$ is the space of bump (aka test) functions. The equations we treat are linear, although not always with constant coefficients. What my instructor does to solve this equations is the following Either resorting directly to the lemma which states that $T'=0\ in\     \mathcal{D}'\Rightarrow T=constant$ , in the simplest cases In less trivial cases, such as, say $u'+a(x)u=0$ with $a(x)\in C^\infty$ , she exploits some kind of 'suggestions' from the classical solution and, given some $\phi\in\mathcal{D}$ she lets the derivative acts on a term $e^{-A(x)}\phi$ ,   instead of on $\phi$ alone, where $A(x)$ is a primitive of $a(x)$ : hence $\phi$ multiplied by the classical solution of the equation. This leads to a convenient rewriting of the equation which allows to exploit the lemma of point 1. I can smell the idea of the tecnique, but since this is done without justification, I would like to know if this is just an euristic or a general approach. We pretty soon moved to partial differential equations, where through the fundamental solution of an operator and convolution one does get a general approach so any help, suggestion or explanation would be great.","I am studying ordinary differential equations in the space of distribution where is the space of bump (aka test) functions. The equations we treat are linear, although not always with constant coefficients. What my instructor does to solve this equations is the following Either resorting directly to the lemma which states that , in the simplest cases In less trivial cases, such as, say with , she exploits some kind of 'suggestions' from the classical solution and, given some she lets the derivative acts on a term ,   instead of on alone, where is a primitive of : hence multiplied by the classical solution of the equation. This leads to a convenient rewriting of the equation which allows to exploit the lemma of point 1. I can smell the idea of the tecnique, but since this is done without justification, I would like to know if this is just an euristic or a general approach. We pretty soon moved to partial differential equations, where through the fundamental solution of an operator and convolution one does get a general approach so any help, suggestion or explanation would be great.","\mathcal{D}' \mathcal{D} T'=0\ in\
    \mathcal{D}'\Rightarrow T=constant u'+a(x)u=0 a(x)\in C^\infty \phi\in\mathcal{D} e^{-A(x)}\phi \phi A(x) a(x) \phi","['functional-analysis', 'ordinary-differential-equations', 'distribution-theory']"
1,Proving that the potential solution to a differential equation converges (or not),Proving that the potential solution to a differential equation converges (or not),,I'm looking at this equation: $$x''(t) - \sin(x(t))\cdot x'(t) + a ^{2}x(t) = 0$$ My question is therefore / how to prove that a solution is not convergent. Numerically we can see it oscillates but I'm not sure how to prove it. So far I've proven it stays at a bounded distance to the solution of: $$x''(t) + a^2x(t) = 0$$ (Gronwall lemma),I'm looking at this equation: My question is therefore / how to prove that a solution is not convergent. Numerically we can see it oscillates but I'm not sure how to prove it. So far I've proven it stays at a bounded distance to the solution of: (Gronwall lemma),x''(t) - \sin(x(t))\cdot x'(t) + a ^{2}x(t) = 0 x''(t) + a^2x(t) = 0,"['ordinary-differential-equations', 'convergence-divergence']"
2,"Tenenbaum and Pollard, Ordinary Differential Equations, Exercise $3.2$(e)","Tenenbaum and Pollard, Ordinary Differential Equations, Exercise (e)",3.2,"I'm working through Exercise $3$ in Tenenbaum and Pollard's ODE book. The question is about determining whether functions are solutions to differential equations, and to state the interval where the differential equation makes sense. The part I'm stuck on is: $$\text{(e) }xy'=2y, \text{  } y=x^2$$ The actual calculation to demonstrate that this is true is simple enough. $$y'=2x\rightarrow xy'=2y\rightarrow 2x^2=2(x^2)\text{    }$$ However, the solution states that the interval where it makes sense is $x\neq 0$ , which is confusing to me. I don't understand why the interval isn't $-\infty<x<\infty$ , as neither the function nor the differential equation have any discontinuties at $x=0$ . Am I missing something, or is this a mistake in the solutions? Any input would be greatly appreciated.","I'm working through Exercise in Tenenbaum and Pollard's ODE book. The question is about determining whether functions are solutions to differential equations, and to state the interval where the differential equation makes sense. The part I'm stuck on is: The actual calculation to demonstrate that this is true is simple enough. However, the solution states that the interval where it makes sense is , which is confusing to me. I don't understand why the interval isn't , as neither the function nor the differential equation have any discontinuties at . Am I missing something, or is this a mistake in the solutions? Any input would be greatly appreciated.","3 \text{(e) }xy'=2y, \text{  } y=x^2 y'=2x\rightarrow xy'=2y\rightarrow 2x^2=2(x^2)\text{    } x\neq 0 -\infty<x<\infty x=0","['ordinary-differential-equations', 'self-learning']"
3,Solving a second-order homogeneous differential equation [closed],Solving a second-order homogeneous differential equation [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I got into this ODE which looks simple but I have a hard time figuring out how to solve: $$a\frac{d^2 y}{dx^2}+\frac{da}{dx}\frac{dy}{dx}-\frac{l(l+1)}{x^2}y(x)=0$$ where $a=(1-k/x)^2$ , $l\geq 0$ , and $k>0$ . Can anyone help me? I just want the general solution. I tried using series solution but I guess there must be a simpler way. Thanks!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I got into this ODE which looks simple but I have a hard time figuring out how to solve: where , , and . Can anyone help me? I just want the general solution. I tried using series solution but I guess there must be a simpler way. Thanks!",a\frac{d^2 y}{dx^2}+\frac{da}{dx}\frac{dy}{dx}-\frac{l(l+1)}{x^2}y(x)=0 a=(1-k/x)^2 l\geq 0 k>0,['ordinary-differential-equations']
4,Criterion of Dahlquist and consistency of a process,Criterion of Dahlquist and consistency of a process,,"We have the linear multi-step process $-y_k-y_{k+1}+y_{k+2}+y_{k+3}=4hf(t_k,y_k)$ for the initial value problem $y'(t)=f(t,y(t)), y(0)=0$ Check the consistency of the process and the criterion of Dahlquist of the characteristical polynomial. Checking the criterion of Dahlquist is easy. The characteristical polynomial of this process is given by: $\rho(x)=x^3+x^2-x-1=(x-1)(x+1)^2$ with the roots $\lambda_1=1$ and $\lambda_{2,3}=-1$ . So $|\lambda_i|\leq 1$ but we have a double root with $|\lambda_2|=1$ so the criterion of Dahlquist does not apply here. I struggle with the consistency of this process. I have to show that for $\tau_h(t,y(t))=\frac1h\sum_{j=0}^m \alpha_j y(t+jh)-\sum_{j=0}^m \beta_j f(t+jh,y(t+jh))$ it is $\lim_{h\to\infty} \tau_h =0$ Correct? Is there an easier way? Thanks in advance.",We have the linear multi-step process for the initial value problem Check the consistency of the process and the criterion of Dahlquist of the characteristical polynomial. Checking the criterion of Dahlquist is easy. The characteristical polynomial of this process is given by: with the roots and . So but we have a double root with so the criterion of Dahlquist does not apply here. I struggle with the consistency of this process. I have to show that for it is Correct? Is there an easier way? Thanks in advance.,"-y_k-y_{k+1}+y_{k+2}+y_{k+3}=4hf(t_k,y_k) y'(t)=f(t,y(t)), y(0)=0 \rho(x)=x^3+x^2-x-1=(x-1)(x+1)^2 \lambda_1=1 \lambda_{2,3}=-1 |\lambda_i|\leq 1 |\lambda_2|=1 \tau_h(t,y(t))=\frac1h\sum_{j=0}^m \alpha_j y(t+jh)-\sum_{j=0}^m \beta_j f(t+jh,y(t+jh)) \lim_{h\to\infty} \tau_h =0","['ordinary-differential-equations', 'numerical-methods']"
5,Find the eigenvalue and eigenfunction successions for S-L BVP,Find the eigenvalue and eigenfunction successions for S-L BVP,,"The problem in question is $(1+x)y''(x)+\frac12y'(x) + \lambda y(x)=0$ subject to $y(0)=y(3)=0$ . Thanks to a suggested change of variable ( $y(x)=u(\sqrt{1+x})$ ), I've managed to find the general solution to the equation: $y(x)=c_1\cos(2\sqrt{\lambda}\sqrt{1+x})+c_ 2\sin(2\sqrt{\lambda}\sqrt{1+x})$ for $c_1, c_2 \in \mathbb{R}$ . However, the boundary values don't help much to find the problem's solution (is it even possible?), so I don't know how to proceed in finding the eigenvalues and eigenfunctions. Does anyone have any suggestion? Edit: this sums up to finding $\lambda$ such that the system $Sc = 0$ below admits solutions other than $c=0$ : $$Sc=0\Leftrightarrow\begin{bmatrix}\cos(2\sqrt{\lambda}) & \sin(2\sqrt{\lambda}) \\ \cos(4\sqrt{\lambda}) & \sin(4\sqrt{\lambda})\end{bmatrix}\begin{bmatrix}c_1 \\ c_2\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}$$ Taking the determinant of $S$ , we get $\cos(2\sqrt{\lambda})\sin(4\sqrt{\lambda}) - \cos(4\sqrt{\lambda})\sin(2\sqrt{\lambda})=\sin(2\sqrt{\lambda})$ . If $Sc=0$ for $c\neq0$ , then $\det(S)=0$ , so we're pretty much done: $$\sin(2\sqrt{\lambda})=0\Leftrightarrow\lambda=\frac{k^2\pi^2}4,\, k\in\mathbb{Z}.$$ After this, we can find that $c_1=0$ and $c_2$ can be anything, so the solution to this problem would be $y(x) = c_2\sin(k\pi\sqrt{1+x})$ .","The problem in question is subject to . Thanks to a suggested change of variable ( ), I've managed to find the general solution to the equation: for . However, the boundary values don't help much to find the problem's solution (is it even possible?), so I don't know how to proceed in finding the eigenvalues and eigenfunctions. Does anyone have any suggestion? Edit: this sums up to finding such that the system below admits solutions other than : Taking the determinant of , we get . If for , then , so we're pretty much done: After this, we can find that and can be anything, so the solution to this problem would be .","(1+x)y''(x)+\frac12y'(x) + \lambda y(x)=0 y(0)=y(3)=0 y(x)=u(\sqrt{1+x}) y(x)=c_1\cos(2\sqrt{\lambda}\sqrt{1+x})+c_ 2\sin(2\sqrt{\lambda}\sqrt{1+x}) c_1, c_2 \in \mathbb{R} \lambda Sc = 0 c=0 Sc=0\Leftrightarrow\begin{bmatrix}\cos(2\sqrt{\lambda}) & \sin(2\sqrt{\lambda}) \\ \cos(4\sqrt{\lambda}) & \sin(4\sqrt{\lambda})\end{bmatrix}\begin{bmatrix}c_1 \\ c_2\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix} S \cos(2\sqrt{\lambda})\sin(4\sqrt{\lambda}) - \cos(4\sqrt{\lambda})\sin(2\sqrt{\lambda})=\sin(2\sqrt{\lambda}) Sc=0 c\neq0 \det(S)=0 \sin(2\sqrt{\lambda})=0\Leftrightarrow\lambda=\frac{k^2\pi^2}4,\, k\in\mathbb{Z}. c_1=0 c_2 y(x) = c_2\sin(k\pi\sqrt{1+x})","['ordinary-differential-equations', 'sturm-liouville']"
6,Explicit solution to nonlinear ODE,Explicit solution to nonlinear ODE,,"I'm trying to find an explicit solution $u(t)$ of $$ \dot{u} = \frac{\beta}{\sqrt{\alpha t}-u},\quad u(0)=0 $$ where $\alpha>0$ and $\beta\in\mathbb{R}$ are given constants. I did not know how to solve it directly, so I tried to solve it numerically, but still I could not find a way to find an explicit solution. Does anybody have idea to solve it. Any explanation would be greatly appreciated.","I'm trying to find an explicit solution of where and are given constants. I did not know how to solve it directly, so I tried to solve it numerically, but still I could not find a way to find an explicit solution. Does anybody have idea to solve it. Any explanation would be greatly appreciated.","u(t) 
\dot{u} = \frac{\beta}{\sqrt{\alpha t}-u},\quad u(0)=0
 \alpha>0 \beta\in\mathbb{R}","['calculus', 'ordinary-differential-equations', 'problem-solving']"
7,Linear first-order systems of ODEs - Proving linear dependence of a set of function vector solutions on an entire interval from a single point,Linear first-order systems of ODEs - Proving linear dependence of a set of function vector solutions on an entire interval from a single point,,"In my Analysis course at university, the professor's textbook describes linear first-order systems of ordinary differential equations, and more precisely, the solutions to the homogeneous equation of the form $$\vec{Y}\,'(t)=A(t)\vec{Y}(t)$$ where $\vec{Y}(t)$ is a vector of $n$ components that are each a function of $t$ . Now, the book states that given $n$ solutions to this equation, $\vec{Y}_1(t) ... \vec{Y}_n(t)$ , one can prove that these solutions are linearly dependent or linearly independent on an entire interval $I$ based on just a single $t_0\in I$ for which the solutions are one of both. I have spent quite some time trying to wrap my head around the proof, and I can't seem to do it, so I was wondering if anyone could reformulate it, or perhaps point me to appropriate search terms so that I can do my own further reading on the internet. The proof uses the case of linear dependence, and is as follows: Suppose $\vec{Y}_1(t) ... \vec{Y}_n(t)$ are solutions to the homogeneous differential equation above, and for a certain $t_0 \in I$ , it holds that $$\sum_{i=1}^nc_i\;\vec{Y}_i(t_0)=\vec0$$ with not all $c_i=0$ . Then, there can be defined a vector function $\vec{Y}_{total}(t)$ that is this linear combination, for which - by linearity - the differential equations above holds. This same $\vec{Y}_{total}(t)$ can then be seen as the solution to the initial value problem $\vec{Y}\,'(t)=A(t)\vec{Y}(t)$ with $\vec{Y}_{total}(t_0)=\vec0$ . Thus, because of the uniqueness of a solution, it must be true that this $\vec{Y}_{total}(t)$ is the trivial solution, proving the linear dependence of $\vec{Y}_1(t) ... \vec{Y}_n(t)$ on the entire interval $I$ . It is this final step in italics I am just not getting. How so does ""uniqueness of a solution"" come into play here? It baffles me every time I read it.","In my Analysis course at university, the professor's textbook describes linear first-order systems of ordinary differential equations, and more precisely, the solutions to the homogeneous equation of the form where is a vector of components that are each a function of . Now, the book states that given solutions to this equation, , one can prove that these solutions are linearly dependent or linearly independent on an entire interval based on just a single for which the solutions are one of both. I have spent quite some time trying to wrap my head around the proof, and I can't seem to do it, so I was wondering if anyone could reformulate it, or perhaps point me to appropriate search terms so that I can do my own further reading on the internet. The proof uses the case of linear dependence, and is as follows: Suppose are solutions to the homogeneous differential equation above, and for a certain , it holds that with not all . Then, there can be defined a vector function that is this linear combination, for which - by linearity - the differential equations above holds. This same can then be seen as the solution to the initial value problem with . Thus, because of the uniqueness of a solution, it must be true that this is the trivial solution, proving the linear dependence of on the entire interval . It is this final step in italics I am just not getting. How so does ""uniqueness of a solution"" come into play here? It baffles me every time I read it.","\vec{Y}\,'(t)=A(t)\vec{Y}(t) \vec{Y}(t) n t n \vec{Y}_1(t) ... \vec{Y}_n(t) I t_0\in I \vec{Y}_1(t) ... \vec{Y}_n(t) t_0 \in I \sum_{i=1}^nc_i\;\vec{Y}_i(t_0)=\vec0 c_i=0 \vec{Y}_{total}(t) \vec{Y}_{total}(t) \vec{Y}\,'(t)=A(t)\vec{Y}(t) \vec{Y}_{total}(t_0)=\vec0 \vec{Y}_{total}(t) \vec{Y}_1(t) ... \vec{Y}_n(t) I","['linear-algebra', 'ordinary-differential-equations', 'systems-of-equations']"
8,"Proving that $\int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq 2 \int_0^1 T^2(t,z)\mathrm{d}z$",Proving that,"\int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq 2 \int_0^1 T^2(t,z)\mathrm{d}z","Exercise : Assume that $T$ satisfies the equation $T_t(t,z) = aT_{zz}(t,z)$ for $t>0, z \in (0,1)$ and $a > 0$ a constant. Moreover, suppose that $T(0,z) = T_0(z)$ for $z \in [0,1]$ , where $T_0 : [0,1] \to \mathbb R$ and that $T(t,0) = T(t,1) = 0$ . Show that : $$\int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq 2 \int_0^1 T^2(t,z)\mathrm{d}z$$ Attempt-thoughts : Since we have the boundary conditions $T(t,0) = T(t,1) = 0$ for the problem and our integrations are over the interval $[0,1]$ , we can ""see"" the given inequality as a norm-2 inequality and use the Wirtinger inequality which gives us a weaker lower bound than the one desired though, as it would be : $$\text{Wirtinger :} \;\|T_z\|_2^2 \geq \pi^2\|T\|_2^2 \Leftrightarrow \int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq \pi^2 \int_0^1 T^2(t,z)\mathrm{d}z$$ So, that intuition falls short. Important : Another thought, since its often carried out in such cases (and also a hint given by our professor) is that the Cauchy-Schwarz inequality shall be used. I cannot see how though. Finally, I have previously proven via substitution from the PDE and integration by parts, that : $$\frac{\mathrm{d}}{\mathrm{d}t} \int_0^1 T^2(t,z)\mathrm{d}z = -2a\int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z $$ I don't know if that can be of any help. Any hints or elaborations will be greatly appreciated.","Exercise : Assume that satisfies the equation for and a constant. Moreover, suppose that for , where and that . Show that : Attempt-thoughts : Since we have the boundary conditions for the problem and our integrations are over the interval , we can ""see"" the given inequality as a norm-2 inequality and use the Wirtinger inequality which gives us a weaker lower bound than the one desired though, as it would be : So, that intuition falls short. Important : Another thought, since its often carried out in such cases (and also a hint given by our professor) is that the Cauchy-Schwarz inequality shall be used. I cannot see how though. Finally, I have previously proven via substitution from the PDE and integration by parts, that : I don't know if that can be of any help. Any hints or elaborations will be greatly appreciated.","T T_t(t,z) = aT_{zz}(t,z) t>0, z \in (0,1) a > 0 T(0,z) = T_0(z) z \in [0,1] T_0 : [0,1] \to \mathbb R T(t,0) = T(t,1) = 0 \int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq 2 \int_0^1 T^2(t,z)\mathrm{d}z T(t,0) = T(t,1) = 0 [0,1] \text{Wirtinger :} \;\|T_z\|_2^2 \geq \pi^2\|T\|_2^2 \Leftrightarrow \int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq \pi^2 \int_0^1 T^2(t,z)\mathrm{d}z \frac{\mathrm{d}}{\mathrm{d}t} \int_0^1 T^2(t,z)\mathrm{d}z = -2a\int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z ","['functional-analysis', 'ordinary-differential-equations', 'inequality', 'integral-inequality', 'cauchy-schwarz-inequality']"
9,What sets can we define continuity and differentiability on?,What sets can we define continuity and differentiability on?,,"I am an undergraduate Physics student (completing my first year shortly) who has had a (first) course on Calculus, and another on Linear Algebra. When working with differential equations (in physical problems), I decided to look with mathematical rigour what I was dealing with. So I decided to start with the beginning. The following questions popped up. Let $\Bbb{F=R}$ or $\Bbb{C}$ . Generalising Intervals What is the concept parallel to an interval in $\Bbb{R}$ , in $\Bbb{C}$ ? (Note: I’m not looking for contours, rather something which generalises the “rectanglular” and “circular” domains.) Can the answer be open or closed subsets of $\Bbb{C}$ ? Can it be generalised to $\Bbb{F}^n$ ? Generalising Continuity and Differentiability Suppose we have a subset $S\subseteq \Bbb{F}$ . What properties must $S$ have so that some form of continuity and differentiability can be defined on some subset $\mathcal{S} \subseteq S$ ? (See this post (later in the post) and this .) (I’d appreciate if your answer is generalisable to $\Bbb{F}^n$ .) Extending Normal Calculus For such a “continuous and differentiable” $\mathcal{S} \subseteq S$ , what theorems of normal calculus will apply? Also, how will the integral, if needed, be redefined? Finally, ODE’s Now turning to ODE’s, I found a very satisfying definition of an ODE here . But unfortunately, it is restricted to intervals of $\Bbb{R}$ . How can you generalise this definition to $\Bbb{C}$ ? (That’s why I was asking for generalisation of interval.) Very importantly, can we have solutions to ODE’s not as “intervals”, but sets like $\mathcal{S}$ above?","I am an undergraduate Physics student (completing my first year shortly) who has had a (first) course on Calculus, and another on Linear Algebra. When working with differential equations (in physical problems), I decided to look with mathematical rigour what I was dealing with. So I decided to start with the beginning. The following questions popped up. Let or . Generalising Intervals What is the concept parallel to an interval in , in ? (Note: I’m not looking for contours, rather something which generalises the “rectanglular” and “circular” domains.) Can the answer be open or closed subsets of ? Can it be generalised to ? Generalising Continuity and Differentiability Suppose we have a subset . What properties must have so that some form of continuity and differentiability can be defined on some subset ? (See this post (later in the post) and this .) (I’d appreciate if your answer is generalisable to .) Extending Normal Calculus For such a “continuous and differentiable” , what theorems of normal calculus will apply? Also, how will the integral, if needed, be redefined? Finally, ODE’s Now turning to ODE’s, I found a very satisfying definition of an ODE here . But unfortunately, it is restricted to intervals of . How can you generalise this definition to ? (That’s why I was asking for generalisation of interval.) Very importantly, can we have solutions to ODE’s not as “intervals”, but sets like above?",\Bbb{F=R} \Bbb{C} \Bbb{R} \Bbb{C} \Bbb{C} \Bbb{F}^n S\subseteq \Bbb{F} S \mathcal{S} \subseteq S \Bbb{F}^n \mathcal{S} \subseteq S \Bbb{R} \Bbb{C} \mathcal{S},"['calculus', 'ordinary-differential-equations', 'continuity']"
10,solve the differential equation $\dot{x} + a\cdot x - b\cdot \sqrt{x} = 0$,solve the differential equation,\dot{x} + a\cdot x - b\cdot \sqrt{x} = 0,Problem I want to know how to solve the differential equation $$ \dot{x} + a\cdot x - b\cdot \sqrt{x} = 0 $$ for $a>0$ and both situations: for $b > 0$ and $b < 0$ . My work One can separate the variables to obtain: $$ \frac{dx}{b\cdot \sqrt{x} - a\cdot x} = dt$$ but I do not know how to proceed ... https://www.wolframalpha.com/input/?i=solve+x%27(t)%2Bax(t)-bsqrt(x(t))+%3D+0 it seems to have an explicit solution ... Context This problem occurs in the following context: $$ \ddot{X} + a \cdot \dot{X} = f(X)$$ then multiplying both sides with $2\dot{X}^T$ one obtains: $$ (\dot{X}^T\dot{X})' + 2a\cdot \dot{X}^T\dot{X} = 2\dot{X}^T f(X)$$ Let $v= \dot{X}^T \dot{X}$ and the above differential equation arises ...,Problem I want to know how to solve the differential equation for and both situations: for and . My work One can separate the variables to obtain: but I do not know how to proceed ... https://www.wolframalpha.com/input/?i=solve+x%27(t)%2Bax(t)-bsqrt(x(t))+%3D+0 it seems to have an explicit solution ... Context This problem occurs in the following context: then multiplying both sides with one obtains: Let and the above differential equation arises ..., \dot{x} + a\cdot x - b\cdot \sqrt{x} = 0  a>0 b > 0 b < 0  \frac{dx}{b\cdot \sqrt{x} - a\cdot x} = dt  \ddot{X} + a \cdot \dot{X} = f(X) 2\dot{X}^T  (\dot{X}^T\dot{X})' + 2a\cdot \dot{X}^T\dot{X} = 2\dot{X}^T f(X) v= \dot{X}^T \dot{X},['ordinary-differential-equations']
11,Solve and find the flaw in this integral equation,Solve and find the flaw in this integral equation,,"The following integral equation often appears in the books and it has once been asked in the prestigious examination called IIT JEE (M) dated 10-04-2016. The question is: $\forall x \in R-\{0\}$ , if $y(x)$ is  differentiable function such that $$ x\int_{1}^{x}~ y(t)~dt =(x+1) \int_{1}^{x} t ~y(t)~dt.$$ Find $y(x)$ . Some four interesting expression of $y(x)$ were given as alternatives in this MCQ type question. Solving this you may find a serious flaw in this question. I would like to thank Ninad Sutrave for expressing a doubt about this question.","The following integral equation often appears in the books and it has once been asked in the prestigious examination called IIT JEE (M) dated 10-04-2016. The question is: , if is  differentiable function such that Find . Some four interesting expression of were given as alternatives in this MCQ type question. Solving this you may find a serious flaw in this question. I would like to thank Ninad Sutrave for expressing a doubt about this question.",\forall x \in R-\{0\} y(x)  x\int_{1}^{x}~ y(t)~dt =(x+1) \int_{1}^{x} t ~y(t)~dt. y(x) y(x),"['integration', 'ordinary-differential-equations', 'integral-equations']"
12,Different methods of solving a linear system of first order DE?,Different methods of solving a linear system of first order DE?,,"$$ x'(t)=\begin{pmatrix} 1&\frac{-2}3\\3&4\end{pmatrix}x(t).$$ This is the method described in my book: finding the eigenvalues of matrix $A  = \begin{pmatrix} 1&\frac{-2}3\\3&4\end{pmatrix}$ : $$ \begin{vmatrix} 1-\lambda&\frac{-2}3 \\3&4-\lambda\end{vmatrix}=0\iff \lambda=3, \lambda=2.$$ eigenvector corresponding to $\lambda=3$ : $\begin{pmatrix} -1/3 \\ 1\end{pmatrix}$ , eigenvector corresponding to $\lambda=2$ : $\begin{pmatrix} -2/3 \\ 1\end{pmatrix}$ . define $C = \begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix}$ , then $C^{-1} = \begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix}$ , and $$ A = C\operatorname{diag}(\lambda_1,\lambda_2)C^{-1} = \begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix} \begin{pmatrix} 2&0\\0&3\end{pmatrix}\begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix}.$$ calculate $e^{tA} $ . Using previous expression for $A$ we get $$ e^{tA}=\begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix} \begin{pmatrix} e^{2t}&0\\0&e^{3t}\end{pmatrix}\begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix} = \begin{pmatrix} 2e^{2t}+\frac13e^{3t} & -2e^{2t}-\frac23e^{3t} \\ -3e^{2t}-e^{3t} & 3e^{2t}+2e^{3t}\end{pmatrix}.$$ the solution of the given system, considering the initial condition $x_0=(x_1,x_2)^t$ , equals to $e^{tA}x_0$ : $$ e^{tA}x_0 = \dots = x_1\begin{pmatrix}2e^{2t}+\frac13e^{3t} \\-3e^{2t}-e^{3t} \end{pmatrix}+x_2\begin{pmatrix} -2e^{2t}-\frac23e^{3t} \\3e^{2t}+2e^{3t}\end{pmatrix}$$ Now, I have looked up some extra exercises online, but these seem to solve such systems in a shorter way: the solution of the system above with be given by $c_1e^{2t}\begin{pmatrix} -2/3 \\ 1 \end{pmatrix} + c_2e^{3t}\begin{pmatrix} -1/3 \\ 1\end{pmatrix}$ . What is the difference between both approaches? The method used by my book seems to have more coefficients (and is therefore maybe a little more detailed/exact?). Are these solution methods equivalent? Which one would you use? Thanks.","This is the method described in my book: finding the eigenvalues of matrix : eigenvector corresponding to : , eigenvector corresponding to : . define , then , and calculate . Using previous expression for we get the solution of the given system, considering the initial condition , equals to : Now, I have looked up some extra exercises online, but these seem to solve such systems in a shorter way: the solution of the system above with be given by . What is the difference between both approaches? The method used by my book seems to have more coefficients (and is therefore maybe a little more detailed/exact?). Are these solution methods equivalent? Which one would you use? Thanks."," x'(t)=\begin{pmatrix} 1&\frac{-2}3\\3&4\end{pmatrix}x(t). A  = \begin{pmatrix} 1&\frac{-2}3\\3&4\end{pmatrix}  \begin{vmatrix} 1-\lambda&\frac{-2}3 \\3&4-\lambda\end{vmatrix}=0\iff \lambda=3, \lambda=2. \lambda=3 \begin{pmatrix} -1/3 \\ 1\end{pmatrix} \lambda=2 \begin{pmatrix} -2/3 \\ 1\end{pmatrix} C = \begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix} C^{-1} = \begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix}  A = C\operatorname{diag}(\lambda_1,\lambda_2)C^{-1} = \begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix} \begin{pmatrix} 2&0\\0&3\end{pmatrix}\begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix}. e^{tA}  A  e^{tA}=\begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix} \begin{pmatrix} e^{2t}&0\\0&e^{3t}\end{pmatrix}\begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix} = \begin{pmatrix} 2e^{2t}+\frac13e^{3t} & -2e^{2t}-\frac23e^{3t} \\ -3e^{2t}-e^{3t} & 3e^{2t}+2e^{3t}\end{pmatrix}. x_0=(x_1,x_2)^t e^{tA}x_0  e^{tA}x_0 = \dots = x_1\begin{pmatrix}2e^{2t}+\frac13e^{3t} \\-3e^{2t}-e^{3t} \end{pmatrix}+x_2\begin{pmatrix} -2e^{2t}-\frac23e^{3t} \\3e^{2t}+2e^{3t}\end{pmatrix} c_1e^{2t}\begin{pmatrix} -2/3 \\ 1 \end{pmatrix} + c_2e^{3t}\begin{pmatrix} -1/3 \\ 1\end{pmatrix}","['ordinary-differential-equations', 'proof-verification', 'systems-of-equations']"
13,Why is the most general solution of a non-homogenous linear ODE the sum of the complementary and particular solutions?,Why is the most general solution of a non-homogenous linear ODE the sum of the complementary and particular solutions?,,"Take a linear, non-homogenous ODE: $$ Ay^{\prime\prime}(x) + By^\prime(x) + Cy(x) = f(x) $$ It is known that one can find the general solution to this equation by taking a solution to the following: $$ Ay_h^{\prime\prime}(x) + By_h^\prime(x) + Cy_h(x) = 0 $$ And a particular solution of the original ODE: $$ Ay_p^{\prime\prime}(x) + By_p^\prime(x) + Cy_p(x) = f(x) $$ By adding the two together, you get the following: $$ A\Big(y_p^{\prime\prime}(x) + y_c^{\prime\prime}(x)\Big) + B\Big(y_p^\prime(x) + y_c^\prime(x)\Big) + C\Big(y_p(x) + y_c(x)\Big) = f(x) $$ Thus, it is proven that a solution to the ODE is given by $y_p(x) + y_c(x)$ . Why, though, is this the most general solution? I'm aware the solution is made more general by the introduction of the general homogenous solution, but how do we know that there isn't a more comprehensive solution?","Take a linear, non-homogenous ODE: It is known that one can find the general solution to this equation by taking a solution to the following: And a particular solution of the original ODE: By adding the two together, you get the following: Thus, it is proven that a solution to the ODE is given by . Why, though, is this the most general solution? I'm aware the solution is made more general by the introduction of the general homogenous solution, but how do we know that there isn't a more comprehensive solution?", Ay^{\prime\prime}(x) + By^\prime(x) + Cy(x) = f(x)   Ay_h^{\prime\prime}(x) + By_h^\prime(x) + Cy_h(x) = 0   Ay_p^{\prime\prime}(x) + By_p^\prime(x) + Cy_p(x) = f(x)   A\Big(y_p^{\prime\prime}(x) + y_c^{\prime\prime}(x)\Big) + B\Big(y_p^\prime(x) + y_c^\prime(x)\Big) + C\Big(y_p(x) + y_c(x)\Big) = f(x)  y_p(x) + y_c(x),['ordinary-differential-equations']
14,"Suppose that $W(x_0) = 0$, and $y_1(x_0) = 0$. Show that $y_1(x)$=0 or $y_2(x) = \frac{y'_2(x_0)}{y'_1(x_0)} y_1(x)$","Suppose that , and . Show that =0 or",W(x_0) = 0 y_1(x_0) = 0 y_1(x) y_2(x) = \frac{y'_2(x_0)}{y'_1(x_0)} y_1(x),"I know that $W(x_0)=y_1(x_0)y'_2(x_0) -y'_1(x_0)y_2(x_0) = 0$ that implies $y'_1(x_0)y_2(x_0) = 0$ , If I supposed that $y_1$ and $y_2$ are solutions of $$y'' + p(x)y' + qy=0$$ I know that $y_1$ and $y_2$ are linearly depedent, then I know that $$y_1(x)y'_2(x) -y'_1(x)y_2(x)=0$$ that implies $$y_1(x)y'_2(x) =y'_1(x)y_2(x)$$ someone could give a hint?","I know that that implies , If I supposed that and are solutions of I know that and are linearly depedent, then I know that that implies someone could give a hint?",W(x_0)=y_1(x_0)y'_2(x_0) -y'_1(x_0)y_2(x_0) = 0 y'_1(x_0)y_2(x_0) = 0 y_1 y_2 y'' + p(x)y' + qy=0 y_1 y_2 y_1(x)y'_2(x) -y'_1(x)y_2(x)=0 y_1(x)y'_2(x) =y'_1(x)y_2(x),['ordinary-differential-equations']
15,Can't solve complicated second order differential equation (Poisson-Boltzmann equation),Can't solve complicated second order differential equation (Poisson-Boltzmann equation),,"Is there anyone able to solve this second order differential equation? It is the Poisson-Boltzmann equation (found in the field of electrostatics) solved on cylindrical coordinates just on the radial direction. $$ (\varphi'+r\cdot\varphi'')=A\cdot e^{−(B\cdotφ+C)} $$ where $\varphi$ is the variable I want to solve the problem for, the derivatives of $\varphi$ are with respect to $r$ , while $A,B,C$ are constants. I have been around it for a long time and can't solve it. What I had tried was to do a variable change by setting a new variable called z for instance which is equal to the exponential term, but I arrive nowhere. All the help is much appreciated :)","Is there anyone able to solve this second order differential equation? It is the Poisson-Boltzmann equation (found in the field of electrostatics) solved on cylindrical coordinates just on the radial direction. where is the variable I want to solve the problem for, the derivatives of are with respect to , while are constants. I have been around it for a long time and can't solve it. What I had tried was to do a variable change by setting a new variable called z for instance which is equal to the exponential term, but I arrive nowhere. All the help is much appreciated :)","
(\varphi'+r\cdot\varphi'')=A\cdot e^{−(B\cdotφ+C)}
 \varphi \varphi r A,B,C","['calculus', 'ordinary-differential-equations']"
16,Finding solution to linear equations using integrating factors,Finding solution to linear equations using integrating factors,,"Here's a couple of problems: $$2xy' + y = 2 \sqrt{x}$$ so according to my book, I should get it into this form: so let's divide by 2x: $$y' + \frac{1}{2x} * y = \frac{2x^{\frac{1}{2}}}{2x} = x^{\frac{-1}{2}}$$ So is the integrating factor = $$I(x) = e^{\int \frac{1}{2x}} = e^{\frac{1}{2} ln(2x)}$$ A bit stuck from here. How does that integral reduce? EDIT going further: $$I(x) = \sqrt{2x}$$ so multiplying both sides: $$\sqrt{2x} \frac{dy}{dx} + \frac{1}{\sqrt{2x}} y = \sqrt{2x}x^{\frac{-1}{2}} = \sqrt{2}$$ $$(\sqrt{2x}y)' = \sqrt{2}$$ $$\sqrt{2x}y = \sqrt{2}x + c$$ $$y = \frac{\sqrt{2}x + c}{\sqrt{2x}} = x^{\frac{1}{2}} + \frac{c}{\sqrt{2x}}$$ When plugging this back in... It's not equal. Did I do something wrong? $$xy' + y = \sqrt{x}$$ divide by x: $$\frac{dy}{dx} + \frac{1}{x}y = x^{\frac{-1}{2}}$$ so the integrating factor is: $e^{\int \frac{1}{x}dx} = x$ $$x * \frac{dy}{dx} + y = x^{\frac{1}{2}}$$ $$(xy)' = x^{\frac{1}{2}}$$ $$xy = \frac{2}{3} x^{\frac{3}{2}}$$ $$y = \frac{2}{3} x^{\frac{1}{2}}$$ Is that right?","Here's a couple of problems: so according to my book, I should get it into this form: so let's divide by 2x: So is the integrating factor = A bit stuck from here. How does that integral reduce? EDIT going further: so multiplying both sides: When plugging this back in... It's not equal. Did I do something wrong? divide by x: so the integrating factor is: Is that right?",2xy' + y = 2 \sqrt{x} y' + \frac{1}{2x} * y = \frac{2x^{\frac{1}{2}}}{2x} = x^{\frac{-1}{2}} I(x) = e^{\int \frac{1}{2x}} = e^{\frac{1}{2} ln(2x)} I(x) = \sqrt{2x} \sqrt{2x} \frac{dy}{dx} + \frac{1}{\sqrt{2x}} y = \sqrt{2x}x^{\frac{-1}{2}} = \sqrt{2} (\sqrt{2x}y)' = \sqrt{2} \sqrt{2x}y = \sqrt{2}x + c y = \frac{\sqrt{2}x + c}{\sqrt{2x}} = x^{\frac{1}{2}} + \frac{c}{\sqrt{2x}} xy' + y = \sqrt{x} \frac{dy}{dx} + \frac{1}{x}y = x^{\frac{-1}{2}} e^{\int \frac{1}{x}dx} = x x * \frac{dy}{dx} + y = x^{\frac{1}{2}} (xy)' = x^{\frac{1}{2}} xy = \frac{2}{3} x^{\frac{3}{2}} y = \frac{2}{3} x^{\frac{1}{2}},['ordinary-differential-equations']
17,How to change this Runge-Kutta method implementation from first order ode solver to system of ODEs solver?,How to change this Runge-Kutta method implementation from first order ode solver to system of ODEs solver?,,"I implemented 4-step Runge-Kutta method (k1..k4) ODE solver for a function $u'(x) = f(x,u(x))$ with initial condition $u(x_0) = u_0$ But it solves just ODEs of the first order. How could I change the code, so that it also takes ODEs of higher order? ${\bf u}'(x)={\bf f}(x,{\bf u}(x))$ with initial condition ${\bf u}(x_0)= {\bf u}_0$ ? All as vectors here not scalars. Here is the code for my solution, which takes first order ODEs: def euler(f, x, y, h):     yn = y + h*f(x,y)     xn = x + h     return xn, yn  def rk4(f, x, y, h):     k1 = h*f(x,y)     k2 = h*f(x+(1/2)*h,y+(1/2)*k1)      k3 = h*f(x+(1/2)*h,y+(1/2)*k2)     k4 = h*f(x+h,y+k3)     yn = y + ((1/6)*(k1+(2*k2)+(2*k3)+k4))     xn = x + h     return xn, yn  def integrate(method, f, t0, y0, tend, h): # Depending on the 'method' (euler or rk4) this method solves the ode f)     T = [t0]     Y = [y0]     t = t0     y = y0      while (t < tend):         h = min(h, tend-t)         t, y = method(f, t, y, h)         T.append(t)         Y.append(y)     return np.array(T), np.array(Y)  #def f1(t, y):   # this is an 1st order ODE     #return (t*y) + t  # Usage example xv, yv = integrate(method=rk4, f=f1, t0=0, y0=2, tend=1, h=0.5) yv[-1] # Output: 6.15203857421875 Is there a solution, to make this work also for a given system of ODEs? (dynamically, without knowing the input ODE if it's 1st or higher order)for example this one: $$u''(x)+u(x)+u(x)^3 =0 ~~{\rm with}~~ u(0)=u'(0)=0 $$ or this one: $$u_1'(x) = 98u_1(x) + 198u_2(x) ~~{\rm and}~~ u_2'(x) = −99u_1(x) − 199u_2(x) \\~~{\rm with}~~ u_1(0) = 1 ~~{\rm and}~~ u_2(0) = 0$$ EDIT: changed the code to this now, to try if the 1st given example 2nd order ODE works: def f(t,y):     return np.array([ y[1], -y[0]-(y[0])**3 ])  Y0 = np.array([0.0,0.0])  xv, yv = integrate(method=rk4, f=f, t0=0.0, y0=Y0, tend=100.0, h=h) print(""[ %20.15f, %20.15f]""%(yv[-1,0], yv[-1,1]) ) print(yv) but I get an output consisting of 0 vectors like this: [    0.000000000000000,    0.000000000000000] [[0. 0.]  [0. 0.]  [0. 0.]  ...  [0. 0.]  [0. 0.]  [0. 0.]]","I implemented 4-step Runge-Kutta method (k1..k4) ODE solver for a function with initial condition But it solves just ODEs of the first order. How could I change the code, so that it also takes ODEs of higher order? with initial condition ? All as vectors here not scalars. Here is the code for my solution, which takes first order ODEs: def euler(f, x, y, h):     yn = y + h*f(x,y)     xn = x + h     return xn, yn  def rk4(f, x, y, h):     k1 = h*f(x,y)     k2 = h*f(x+(1/2)*h,y+(1/2)*k1)      k3 = h*f(x+(1/2)*h,y+(1/2)*k2)     k4 = h*f(x+h,y+k3)     yn = y + ((1/6)*(k1+(2*k2)+(2*k3)+k4))     xn = x + h     return xn, yn  def integrate(method, f, t0, y0, tend, h): # Depending on the 'method' (euler or rk4) this method solves the ode f)     T = [t0]     Y = [y0]     t = t0     y = y0      while (t < tend):         h = min(h, tend-t)         t, y = method(f, t, y, h)         T.append(t)         Y.append(y)     return np.array(T), np.array(Y)  #def f1(t, y):   # this is an 1st order ODE     #return (t*y) + t  # Usage example xv, yv = integrate(method=rk4, f=f1, t0=0, y0=2, tend=1, h=0.5) yv[-1] # Output: 6.15203857421875 Is there a solution, to make this work also for a given system of ODEs? (dynamically, without knowing the input ODE if it's 1st or higher order)for example this one: or this one: EDIT: changed the code to this now, to try if the 1st given example 2nd order ODE works: def f(t,y):     return np.array([ y[1], -y[0]-(y[0])**3 ])  Y0 = np.array([0.0,0.0])  xv, yv = integrate(method=rk4, f=f, t0=0.0, y0=Y0, tend=100.0, h=h) print(""[ %20.15f, %20.15f]""%(yv[-1,0], yv[-1,1]) ) print(yv) but I get an output consisting of 0 vectors like this: [    0.000000000000000,    0.000000000000000] [[0. 0.]  [0. 0.]  [0. 0.]  ...  [0. 0.]  [0. 0.]  [0. 0.]]","u'(x) = f(x,u(x)) u(x_0) = u_0 {\bf u}'(x)={\bf f}(x,{\bf u}(x)) {\bf u}(x_0)= {\bf u}_0 u''(x)+u(x)+u(x)^3 =0 ~~{\rm with}~~ u(0)=u'(0)=0  u_1'(x) = 98u_1(x) + 198u_2(x) ~~{\rm and}~~ u_2'(x) = −99u_1(x) − 199u_2(x) \\~~{\rm with}~~ u_1(0) = 1 ~~{\rm and}~~ u_2(0) = 0","['ordinary-differential-equations', 'numerical-methods', 'python', 'runge-kutta-methods']"
18,How is stability for a numerical solution generalized to a system of ODEs?,How is stability for a numerical solution generalized to a system of ODEs?,,"Consider the system of ODEs $$y' = \begin{bmatrix}-6&4\\4&-6\end{bmatrix}y, \quad t\in[t_0, t_e], \quad y(t_0)=y_0$$ I'm asked for what stepsize the explicit Euler method generates a stable solution. If it would be one ODE I would just find for what stepsize $h$ the stability function $R(z)$ in $$y_{n+1} = R(\lambda h) y_{n}$$ would be less than $1$ , but I get confused when it's a system of ODEs. Do I write $$y_{n+1} = \left(1 + h \begin{bmatrix}-6&4\\4&-6\end{bmatrix}\right) y_n$$ and use the eigenvalues somehow?","Consider the system of ODEs I'm asked for what stepsize the explicit Euler method generates a stable solution. If it would be one ODE I would just find for what stepsize the stability function in would be less than , but I get confused when it's a system of ODEs. Do I write and use the eigenvalues somehow?","y' = \begin{bmatrix}-6&4\\4&-6\end{bmatrix}y, \quad t\in[t_0, t_e], \quad y(t_0)=y_0 h R(z) y_{n+1} = R(\lambda h) y_{n} 1 y_{n+1} = \left(1 + h \begin{bmatrix}-6&4\\4&-6\end{bmatrix}\right) y_n","['ordinary-differential-equations', 'numerical-methods', 'systems-of-equations']"
19,Phase Portrait vs Explicit Solution,Phase Portrait vs Explicit Solution,,Currently revising Ordinary Differential equations and I seem to have come across a contradiction Now my confusion comes with the sign of $x_0$ surely using the phase portraits if $x_0 < k$ when $k <0$ the solution will tend to negative infinity and the same thought for the second case where $k>0$ Thanks for any help.,Currently revising Ordinary Differential equations and I seem to have come across a contradiction Now my confusion comes with the sign of surely using the phase portraits if when the solution will tend to negative infinity and the same thought for the second case where Thanks for any help.,x_0 x_0 < k k <0 k>0,['ordinary-differential-equations']
20,Particular solution for $D^2 y + 4D y + 4y =18 \cosh x$?,Particular solution for ?,D^2 y + 4D y + 4y =18 \cosh x,"Set $$y_p = K \cosh x~, \quad y_p' = K \sinh x~, \quad y_p''=K\cosh x$$ and substitute these functions into the original equation, then: $$K\cosh x+4K\sinh x+4K\cosh x=18\cosh x$$ the coefficient of $\cosh x$ would be $5K=18$ , the coefficient of $\sinh x$ is $4K=0$ . (the contradiction would come out ??) How could I get the particular solution $Y_p$ ?","Set and substitute these functions into the original equation, then: the coefficient of would be , the coefficient of is . (the contradiction would come out ??) How could I get the particular solution ?","y_p = K \cosh x~, \quad y_p' = K \sinh x~, \quad y_p''=K\cosh x K\cosh x+4K\sinh x+4K\cosh x=18\cosh x \cosh x 5K=18 \sinh x 4K=0 Y_p","['ordinary-differential-equations', 'homogeneous-equation']"
21,Solutions of the differential equation $\dot{x}^2-2x=1$,Solutions of the differential equation,\dot{x}^2-2x=1,"I am studying how to solve certain nonlinear differential equations and am currently trying to solve the equation $\dot{x}^2-2x=1$ , where $x$ is a function of $t$ , with initial condition $x(1)=1$ . My method is as follows: Solving for $x$ in this equation gives $x=\frac{1}{2}(\dot x-1)$ . Let $p=\dot x$ , then differentiating with respect to $t$ gives $\frac{d}{dt}x=\dot x=\frac{d}{dt}\frac{1}{2}(\dot x-1)=\frac{1}{2}(2\ddot x\dot x)$ by chain rule, so we get the first-order differential equation $p=pp'$ . If $p\equiv0$ then we get the solution $x(t)\equiv C$ . Otherwise, dividing by $p$ gives $p'=1$ , so $p=t+c_1$ and $x=\frac{t^2}{2}+c_1t+c_2$ . Plugging back into the original equation: $$(t+c_1)^2-2(\frac{t^2}{2}+c_1t+c_2)=1$$ $$t^2+2c_1t+c_1^2-t^2-2c_1t-2c_2=1$$ $$c_1^2-2c_2=1$$ $$c_1^2=1-2c_2$$ $$c_1=\pm\sqrt{1-2c_2}$$ Now applying the initial condition $x(1)=1$ : $$1=\frac{1}{2}\pm\sqrt{1-2c_2}+c_2$$ Solving this equation for $c_2$ I get $c_2=-\left(\sqrt5+\frac 5 2\right)$ and $c_2=\sqrt5-\frac 5 2$ . Therefore, the solution is $$x(t)=\frac{t^2}{2}\pm \sqrt{1+2(\frac 5 2\pm\sqrt5)}-\frac{5}{2}\pm\sqrt5$$ But according to WolframAlpha the solutions are $$x(t)=\frac{1}{2}\big(t^2\pm2(1+\sqrt3)t\mp(2\sqrt3)+3\big)$$ Where am I going wrong in my approach? Did I make an algebraic mistake or is there something more deeply wrong with this method?","I am studying how to solve certain nonlinear differential equations and am currently trying to solve the equation , where is a function of , with initial condition . My method is as follows: Solving for in this equation gives . Let , then differentiating with respect to gives by chain rule, so we get the first-order differential equation . If then we get the solution . Otherwise, dividing by gives , so and . Plugging back into the original equation: Now applying the initial condition : Solving this equation for I get and . Therefore, the solution is But according to WolframAlpha the solutions are Where am I going wrong in my approach? Did I make an algebraic mistake or is there something more deeply wrong with this method?",\dot{x}^2-2x=1 x t x(1)=1 x x=\frac{1}{2}(\dot x-1) p=\dot x t \frac{d}{dt}x=\dot x=\frac{d}{dt}\frac{1}{2}(\dot x-1)=\frac{1}{2}(2\ddot x\dot x) p=pp' p\equiv0 x(t)\equiv C p p'=1 p=t+c_1 x=\frac{t^2}{2}+c_1t+c_2 (t+c_1)^2-2(\frac{t^2}{2}+c_1t+c_2)=1 t^2+2c_1t+c_1^2-t^2-2c_1t-2c_2=1 c_1^2-2c_2=1 c_1^2=1-2c_2 c_1=\pm\sqrt{1-2c_2} x(1)=1 1=\frac{1}{2}\pm\sqrt{1-2c_2}+c_2 c_2 c_2=-\left(\sqrt5+\frac 5 2\right) c_2=\sqrt5-\frac 5 2 x(t)=\frac{t^2}{2}\pm \sqrt{1+2(\frac 5 2\pm\sqrt5)}-\frac{5}{2}\pm\sqrt5 x(t)=\frac{1}{2}\big(t^2\pm2(1+\sqrt3)t\mp(2\sqrt3)+3\big),"['calculus', 'ordinary-differential-equations']"
22,Infinitely many solutions for a first order Cauchy problem.,Infinitely many solutions for a first order Cauchy problem.,,"Is this correct that the following Cauchy problem has infinitely many solutions? ‎ \begin{cases}‎ ‎xu_t+u_x=0 \\‎ ‎u(x,0)=\cos x‎  ‎\end{cases} ‎ Using the method of characteristics it is obvious that it has a local solution around the curve $t=0$ . But I am puzzled why it should have infinitely many solutions. It seems that we can construct many solutions by Laplace transform method but I am not confident.",Is this correct that the following Cauchy problem has infinitely many solutions? ‎ ‎ Using the method of characteristics it is obvious that it has a local solution around the curve . But I am puzzled why it should have infinitely many solutions. It seems that we can construct many solutions by Laplace transform method but I am not confident.,"\begin{cases}‎
‎xu_t+u_x=0 \\‎
‎u(x,0)=\cos x‎ 
‎\end{cases} t=0","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'linear-pde', 'cauchy-problem']"
23,Does this system of equations have attractors and periodic solutions?,Does this system of equations have attractors and periodic solutions?,,"I want to solve the following exercise: Determine the critical points of the system \begin{align} \dot{x} &= x^2- y^3\\ \dot{y} &= 2x(x^2 - y) \end{align} Are there attractors in this system? Determine a first integral. Do periodic solutions exist? What I've tried so far: I think that I've found the critical points of this system. If I inspect \begin{align} x^2 - y^3 = 0\\ 2x(x^2 - y) = 0 \end{align} I find that $x = 0$ or $x = \pm \sqrt{y}$ . Case $x = 0$ : $-y^3 = 0$ so that $y = 0$ . Hence the first critical point that I find is $(0,0)$ . Case $x = \pm\sqrt{y}$ : $y - y^3 = 0$ so $y=1$ or $y = -1$ . If $y = -1$ then $x = \pm i$ . If $y = 1$ then $x = \pm1$ . Hence I find the critical points $(1, 1), (i, -1), (-1,1),$ and $(-i,-1)$ . I think that I've found a first integral by looking at the equation $$ \dfrac{\dfrac{dy}{dt}}{\dfrac{dx}{dt}} = \dfrac{dy}{dx} = \dfrac{2x^3 - 2xy}{x^2 - y^3} $$ Integrating this final equation gives $$ \int \dfrac{2x^3 - 2xy}{x^2 - y^3}dx = y(y^2 - 1)\log(y^3 - x^2) + x^2 + C $$ Hence a first integral of this system is $$ F(x,y) = y(y^2 - 1)\log(y^3 - x^2) + x^2 $$ I tried plotting this function to find out if the system of equations has periodic solutions but I don't get any wiser looking at the plots. I have the following definitions of an attractor: A critical point $x = a$ of the equation $\dot{x} = f(x)$ in $\mathbb{R}^n$ is called a positive attractor if there exists a neighborhood $\Omega_a\subset \mathbb{R}^n$ of $x = a$ such that $x(t_0)\in\Omega_a$ implies $\lim_{t\to\infty} x(t) = a$ . If a critical point $x = a$ has this property for $t \to - \infty$ then $x = a$ is called a negative attractor. I think that in order to determine whether either of the critical points is an attractor I need to know what the solution $x(t)$ looks like. I'm not sure how to solve this system though and if I enter the equations in wolfram then I don't get a solution either (I might be missing something). So therefore I tried to linearize the equations. I have: $$ f(x, y) = \begin{pmatrix}x^2 - y^3\\2x(x^2 - y)\end{pmatrix}, \dfrac{\partial f(x,y)}{\partial(x,y)} = \begin{pmatrix}2x & -3y^2\\6x^2& -2x\end{pmatrix}, \dfrac{\partial^2f(x,y)}{\partial (x,y)^2} = \begin{pmatrix}2 & -6y\\12x & 0\end{pmatrix}, \\\dfrac{\partial^3 f(x,y)}{\partial (x,y)^3} = \begin{pmatrix}0 & -6\\12 & 0\end{pmatrix} $$ Hence, using the Taylor expansion around $(x,y) = (0,0)$ I get \begin{align} \dot{x} &= x^2 - y^3 + \ldots\\ \dot{y} &= 2x^3 + \ldots \end{align} but I'm not really sure how I can use this to learn anything new.. Question: How can I learn more about this system so that I will be able to say more about the existence of attractors and periodic solutions? Is my reasoning thus far correct?","I want to solve the following exercise: Determine the critical points of the system Are there attractors in this system? Determine a first integral. Do periodic solutions exist? What I've tried so far: I think that I've found the critical points of this system. If I inspect I find that or . Case : so that . Hence the first critical point that I find is . Case : so or . If then . If then . Hence I find the critical points and . I think that I've found a first integral by looking at the equation Integrating this final equation gives Hence a first integral of this system is I tried plotting this function to find out if the system of equations has periodic solutions but I don't get any wiser looking at the plots. I have the following definitions of an attractor: A critical point of the equation in is called a positive attractor if there exists a neighborhood of such that implies . If a critical point has this property for then is called a negative attractor. I think that in order to determine whether either of the critical points is an attractor I need to know what the solution looks like. I'm not sure how to solve this system though and if I enter the equations in wolfram then I don't get a solution either (I might be missing something). So therefore I tried to linearize the equations. I have: Hence, using the Taylor expansion around I get but I'm not really sure how I can use this to learn anything new.. Question: How can I learn more about this system so that I will be able to say more about the existence of attractors and periodic solutions? Is my reasoning thus far correct?","\begin{align}
\dot{x} &= x^2- y^3\\
\dot{y} &= 2x(x^2 - y)
\end{align} \begin{align}
x^2 - y^3 = 0\\
2x(x^2 - y) = 0
\end{align} x = 0 x = \pm \sqrt{y} x = 0 -y^3 = 0 y = 0 (0,0) x = \pm\sqrt{y} y - y^3 = 0 y=1 y = -1 y = -1 x = \pm i y = 1 x = \pm1 (1, 1), (i, -1), (-1,1), (-i,-1) 
\dfrac{\dfrac{dy}{dt}}{\dfrac{dx}{dt}} = \dfrac{dy}{dx} = \dfrac{2x^3 - 2xy}{x^2 - y^3}
 
\int \dfrac{2x^3 - 2xy}{x^2 - y^3}dx = y(y^2 - 1)\log(y^3 - x^2) + x^2 + C
 
F(x,y) = y(y^2 - 1)\log(y^3 - x^2) + x^2
 x = a \dot{x} = f(x) \mathbb{R}^n \Omega_a\subset \mathbb{R}^n x = a x(t_0)\in\Omega_a \lim_{t\to\infty} x(t) = a x = a t \to - \infty x = a x(t) 
f(x, y) = \begin{pmatrix}x^2 - y^3\\2x(x^2 - y)\end{pmatrix}, \dfrac{\partial f(x,y)}{\partial(x,y)} = \begin{pmatrix}2x & -3y^2\\6x^2& -2x\end{pmatrix}, \dfrac{\partial^2f(x,y)}{\partial (x,y)^2} = \begin{pmatrix}2 & -6y\\12x & 0\end{pmatrix}, \\\dfrac{\partial^3 f(x,y)}{\partial (x,y)^3} = \begin{pmatrix}0 & -6\\12 & 0\end{pmatrix}
 (x,y) = (0,0) \begin{align}
\dot{x} &= x^2 - y^3 + \ldots\\
\dot{y} &= 2x^3 + \ldots
\end{align}","['ordinary-differential-equations', 'taylor-expansion', 'approximation']"
24,Comparing two Differential equations which have a same solution set.,Comparing two Differential equations which have a same solution set.,,"$$P(x)y''+Q(x)y'+R(x)y=0$$ $$P_1(x)y''+Q_1(x)y'+R_1(x)y=0$$ Suppose above two differential equations have a same solution set, then does it true that it must imply conditions $$P(x)=kP_1(x) , Q(x)=kQ_1(x) , R(x)=kR_1(x) \ \ \ \ \ \ \  \text{for some $k(x,y)$}$$ ?? ( $y''= \frac{d^2y}{dx^2} ,y'= \frac{dy}{dx}$ ) If this is true, then how can i prove this?","Suppose above two differential equations have a same solution set, then does it true that it must imply conditions ?? ( ) If this is true, then how can i prove this?","P(x)y''+Q(x)y'+R(x)y=0 P_1(x)y''+Q_1(x)y'+R_1(x)y=0 P(x)=kP_1(x) , Q(x)=kQ_1(x) , R(x)=kR_1(x) \ \ \ \ \ \ \  \text{for some k(x,y)} y''= \frac{d^2y}{dx^2} ,y'= \frac{dy}{dx}",['ordinary-differential-equations']
25,"$y-xy' =2(x+yy'), y(1) =1 $ is..",is..,"y-xy' =2(x+yy'), y(1) =1 ","Question : $y-xy' =2(x+yy'), y(1) =1 $ is.. Firstly, I do not even know what the question is asking for. Secondly, Why does this question put $y(1)=1$ ? Isn't it obvious or is there any special meaning to it. Surprisingly, the answer to it is also very unique. The answer is: $\tan^{-1} \frac{y}{x} + \log (x^2 + y^2) = \frac{\pi}{4} + \log 2$ . Also, another thing is that this question was solved by trigonometry, which is confusing me further. P.S.: Please do not downvote as I completely do not understand the question even a bit. Thats why I am asking it here.","Question : is.. Firstly, I do not even know what the question is asking for. Secondly, Why does this question put ? Isn't it obvious or is there any special meaning to it. Surprisingly, the answer to it is also very unique. The answer is: . Also, another thing is that this question was solved by trigonometry, which is confusing me further. P.S.: Please do not downvote as I completely do not understand the question even a bit. Thats why I am asking it here.","y-xy' =2(x+yy'), y(1) =1  y(1)=1 \tan^{-1} \frac{y}{x} + \log (x^2 + y^2) = \frac{\pi}{4} + \log 2","['calculus', 'ordinary-differential-equations', 'trigonometry']"
26,AnHarmonic oscillator for pendulum,AnHarmonic oscillator for pendulum,,The ODE for ‘‘small angles’’ of a pendulum is given by $\frac{d^2\theta}{dt^2}+\frac{g}{l}\theta=0$ . Without small angle approximations the right ODE is: $$\frac{d^2\theta}{dt^2}+\frac{g}{l}\sin{\theta}=0 \tag{1}$$ Is there a closed form of $\theta(t)$ for $(1)$ ?,The ODE for ‘‘small angles’’ of a pendulum is given by . Without small angle approximations the right ODE is: Is there a closed form of for ?,\frac{d^2\theta}{dt^2}+\frac{g}{l}\theta=0 \frac{d^2\theta}{dt^2}+\frac{g}{l}\sin{\theta}=0 \tag{1} \theta(t) (1),"['real-analysis', 'integration', 'ordinary-differential-equations', 'analysis']"
27,Solving an optimization problem involving a differential equation?,Solving an optimization problem involving a differential equation?,,"I am trying to solve what I think is a single variable calculus optimization problem.  The actual problem I’m trying to solve is rather hard to describe, but I think it’s isomorphic to this one. Suppose there are two linked containers, container A and container B. Rain is falling on container A for the next hour, at the rate of $1$ liter per hour.  (Rain is not falling on container B.)  And water flows from container A to container B at a rate of $r(t) = \frac{q(t)}{1-t}$ , where $q(t)$ is the amount of water currently in container A.  Now at any time in the next hour, you can dump out all the water currently in container A (after which rain will continue to fall on container A).  But you can only do it once.  So what is the best time to do it if you want to minimize the amount of water that ends up in container B at the end of the hour? I think the basic situation is described by the differential equation $\frac{dq}{dt} = 1 - r = 1 - \frac{q(t)}{1-t}$ where $q(0)=0$ , whose solution according to Wolfram Alpha is given by $q(t)=(t-1)ln(1-t)$ .  And if the water in container A is dumped at time $T$ , then I think the amount of water in container B at the end of the hour is given by $P(T)= \int_0^T r(t) dt + 1 - q(T) = T - 2q(T) +2$ .  And we want to find the time $T$ between $0$ and $1$ which minimizes $P(T)$ . Yet there must be an error somewhere, because if I plug in $q(t)=(t-1)ln(1-t)$ into the expression for $P(T)$ , then for any value of $T$ between $0$ and $1$ , it looks like $P(T)>1$ .  But that makes no sense, because if only $1$ liter of water falls on container A over the course of the hour, there’s no way that there will be more than a liter of water in container B at the end of the hour.  So where am I going wrong?","I am trying to solve what I think is a single variable calculus optimization problem.  The actual problem I’m trying to solve is rather hard to describe, but I think it’s isomorphic to this one. Suppose there are two linked containers, container A and container B. Rain is falling on container A for the next hour, at the rate of liter per hour.  (Rain is not falling on container B.)  And water flows from container A to container B at a rate of , where is the amount of water currently in container A.  Now at any time in the next hour, you can dump out all the water currently in container A (after which rain will continue to fall on container A).  But you can only do it once.  So what is the best time to do it if you want to minimize the amount of water that ends up in container B at the end of the hour? I think the basic situation is described by the differential equation where , whose solution according to Wolfram Alpha is given by .  And if the water in container A is dumped at time , then I think the amount of water in container B at the end of the hour is given by .  And we want to find the time between and which minimizes . Yet there must be an error somewhere, because if I plug in into the expression for , then for any value of between and , it looks like .  But that makes no sense, because if only liter of water falls on container A over the course of the hour, there’s no way that there will be more than a liter of water in container B at the end of the hour.  So where am I going wrong?",1 r(t) = \frac{q(t)}{1-t} q(t) \frac{dq}{dt} = 1 - r = 1 - \frac{q(t)}{1-t} q(0)=0 q(t)=(t-1)ln(1-t) T P(T)= \int_0^T r(t) dt + 1 - q(T) = T - 2q(T) +2 T 0 1 P(T) q(t)=(t-1)ln(1-t) P(T) T 0 1 P(T)>1 1,"['calculus', 'integration', 'ordinary-differential-equations', 'optimization']"
28,Newton law of cooling with variable surrounding temperature.,Newton law of cooling with variable surrounding temperature.,,"Newton law of cooling is a very popular law of nature to study for first differential equation in high school. It says that an object's temperature rate of change (time derivative) is proportional to the difference of temperatures of object and surrounding. $$\frac{\partial{T}}{\partial t} = k(T(t)-T_s)$$ Usually here $T_s$ (Temperature of (s)urrounding) is assumed constant. But what happens if $T_s$ is not constant? For example let us model outdoor temperature as a cosine with minimum at midnight: $$T_s(t) = 20-10\cos\left(\frac{2\pi t}{24}\right)$$ This could be a typical Swedish summer day, temperature between $10$ and $30$ degrees (celcius). Assume at a party someone forgets a beer at pre-party $t=22$ in evening ( $10$ pm ) but finds it again at after-party $t=26$ ( $2$ am ). How can we approach this problem of calculating how much warmer the beer has gotten ( in other words of solving $T(t)$ )?","Newton law of cooling is a very popular law of nature to study for first differential equation in high school. It says that an object's temperature rate of change (time derivative) is proportional to the difference of temperatures of object and surrounding. Usually here (Temperature of (s)urrounding) is assumed constant. But what happens if is not constant? For example let us model outdoor temperature as a cosine with minimum at midnight: This could be a typical Swedish summer day, temperature between and degrees (celcius). Assume at a party someone forgets a beer at pre-party in evening ( pm ) but finds it again at after-party ( am ). How can we approach this problem of calculating how much warmer the beer has gotten ( in other words of solving )?",\frac{\partial{T}}{\partial t} = k(T(t)-T_s) T_s T_s T_s(t) = 20-10\cos\left(\frac{2\pi t}{24}\right) 10 30 t=22 10 t=26 2 T(t),"['calculus', 'ordinary-differential-equations', 'soft-question', 'physics']"
29,Recovering a matrix from a linear ODE given observations,Recovering a matrix from a linear ODE given observations,,"To make this simple, let's say we have $x: \mathbb{R} \rightarrow \mathbb{R}^2$ such that $$\frac{d}{dt}\vec{x}(t) = \begin{pmatrix} x_1'(t) \\ x_2'(t) \end{pmatrix} = A \vec{x}(t)$$ for some constant matrix $A$ . We know that the general solution has the form $$\vec{x}(t) = e^{A t} \begin{pmatrix}x_1(0) \\ x_2(0) \end{pmatrix},$$ where $e^{At}$ denotes the matrix exponential . Suppose we are given $\vec{x}(0)$ and also have the observations $$\hat{x}(t_1), \ldots, \hat{x}(t_k) $$ for some known collection $\{t_i\}_{i=1}^k.$ Right now, we don't worry about whether the observations are noisy; we just assume they are perfect. The goal is to recover the matrix $A$ , rather than $\vec{x}(t)$ . All we know is that $A$ is constant. This is stumping me, because it seems so simple; however, the standard matrix differential equation techniques (e.g. here ) usually assume that $A$ is given. One option numerically is to define the error function $$ err(A) = \sum_{i=1}^k \| \vec{x}(t_i,A) - \hat{x}(t_i)\|^2 $$ for the given $\{t_i\}_{i=1}^k$ and compute $$ \min_{A \in \mathbb{R}^{2 \times 2}} err(A) $$ in $\texttt{Matlab}$ (or some other program). However, I keep getting incorrect/unstable results, which is worrisome since $A$ is only $2 \times 2$ . In general, I am not even sure what it means to minimize $err(A)$ with respect to a matrix , so I also do not understand what $\texttt{Matlab}$ is doing from a theoretical standpoint in this computation. How can one recover the matrix $A$ theoretically or via some brute-force algorithm -- and if the latter is used, why does that algorithm converge to or output the correct $A$ ?","To make this simple, let's say we have such that for some constant matrix . We know that the general solution has the form where denotes the matrix exponential . Suppose we are given and also have the observations for some known collection Right now, we don't worry about whether the observations are noisy; we just assume they are perfect. The goal is to recover the matrix , rather than . All we know is that is constant. This is stumping me, because it seems so simple; however, the standard matrix differential equation techniques (e.g. here ) usually assume that is given. One option numerically is to define the error function for the given and compute in (or some other program). However, I keep getting incorrect/unstable results, which is worrisome since is only . In general, I am not even sure what it means to minimize with respect to a matrix , so I also do not understand what is doing from a theoretical standpoint in this computation. How can one recover the matrix theoretically or via some brute-force algorithm -- and if the latter is used, why does that algorithm converge to or output the correct ?","x: \mathbb{R} \rightarrow \mathbb{R}^2 \frac{d}{dt}\vec{x}(t) = \begin{pmatrix} x_1'(t) \\ x_2'(t) \end{pmatrix} = A \vec{x}(t) A \vec{x}(t) = e^{A t} \begin{pmatrix}x_1(0) \\ x_2(0) \end{pmatrix}, e^{At} \vec{x}(0) \hat{x}(t_1), \ldots, \hat{x}(t_k)  \{t_i\}_{i=1}^k. A \vec{x}(t) A A  err(A) = \sum_{i=1}^k \| \vec{x}(t_i,A) - \hat{x}(t_i)\|^2  \{t_i\}_{i=1}^k  \min_{A \in \mathbb{R}^{2 \times 2}} err(A)  \texttt{Matlab} A 2 \times 2 err(A) \texttt{Matlab} A A","['ordinary-differential-equations', 'numerical-methods', 'numerical-optimization']"
30,Exact Differential Equation Geometry,Exact Differential Equation Geometry,,"In a variety of contexts, I have noticed hints of a strong connection between exact differential equations and machinery from multivariable calculus. From another question, I have gathered that the geometry of finding the general solution to an exact differential equation $M(x,\ y)\ dx + N(x,\ y)\ dy = 0$ includes finding the potential function of $\begin{bmatrix}M(x,\ y) \\ N(x,\ y)\end{bmatrix}$ , that is, the surface in $\mathbb{R^3}$ whose gradient is the vector field $\begin{bmatrix}M(x,\ y) \\ N(x,\ y)\end{bmatrix}$ .  What is missing from this description?  For example, we can write the LHS of a not-necessarily-exact $M(x,\ y)\ dx + N(x,\ y)\ dy = 0$ as $\begin{bmatrix}M(x,\ y) \\ N(x,\ y) \end{bmatrix} \cdot \begin{bmatrix}dx \\ dy\end{bmatrix}$ , which expresses the differential of work done by $\begin{bmatrix}M(x,\ y) \\ N(x,\ y)\end{bmatrix}$ , and thus shows promise for extending the above geometric picture.  In fact, since work is a path function, I suspect the Wikipedia quote In mathematics, an integrating factor is a function that is chosen to   facilitate the solving of a given equation involving differentials. It   is commonly used to solve ordinary differential equations, but is also   used within multivariable calculus when multiplying through by an   integrating factor allows an inexact differential to be made into an   exact differential (which can then be integrated to give a scalar   field). This is especially useful in thermodynamics where temperature   becomes the integrating factor that makes entropy an exact   differential. is misleading, in that the ordinary differential equations and multivariable calculus uses are actually identical, which if correct, might allow geometry from other path functions (such as the referenced thermodynamics example) to be leveraged. A big part of what I'm trying to remedy is that it's not clear to me why finding the potential function of $\begin{bmatrix}M(x,\ y) \\ N(x,\ y)\end{bmatrix}$ should solve the ODE.  I understand the algebraic rationale that the solution technique undoes the multivariable chain rule (divide through by $dx$ then integrate both sides), but is the geometric picture just an afterthought, or does it offer its own independent rationale for the solution technique?","In a variety of contexts, I have noticed hints of a strong connection between exact differential equations and machinery from multivariable calculus. From another question, I have gathered that the geometry of finding the general solution to an exact differential equation includes finding the potential function of , that is, the surface in whose gradient is the vector field .  What is missing from this description?  For example, we can write the LHS of a not-necessarily-exact as , which expresses the differential of work done by , and thus shows promise for extending the above geometric picture.  In fact, since work is a path function, I suspect the Wikipedia quote In mathematics, an integrating factor is a function that is chosen to   facilitate the solving of a given equation involving differentials. It   is commonly used to solve ordinary differential equations, but is also   used within multivariable calculus when multiplying through by an   integrating factor allows an inexact differential to be made into an   exact differential (which can then be integrated to give a scalar   field). This is especially useful in thermodynamics where temperature   becomes the integrating factor that makes entropy an exact   differential. is misleading, in that the ordinary differential equations and multivariable calculus uses are actually identical, which if correct, might allow geometry from other path functions (such as the referenced thermodynamics example) to be leveraged. A big part of what I'm trying to remedy is that it's not clear to me why finding the potential function of should solve the ODE.  I understand the algebraic rationale that the solution technique undoes the multivariable chain rule (divide through by then integrate both sides), but is the geometric picture just an afterthought, or does it offer its own independent rationale for the solution technique?","M(x,\ y)\ dx + N(x,\ y)\ dy = 0 \begin{bmatrix}M(x,\ y) \\ N(x,\ y)\end{bmatrix} \mathbb{R^3} \begin{bmatrix}M(x,\ y) \\ N(x,\ y)\end{bmatrix} M(x,\ y)\ dx + N(x,\ y)\ dy = 0 \begin{bmatrix}M(x,\ y) \\ N(x,\ y) \end{bmatrix} \cdot \begin{bmatrix}dx \\ dy\end{bmatrix} \begin{bmatrix}M(x,\ y) \\ N(x,\ y)\end{bmatrix} \begin{bmatrix}M(x,\ y) \\ N(x,\ y)\end{bmatrix} dx","['geometry', 'ordinary-differential-equations', 'multivariable-calculus', 'vector-fields', 'differential']"
31,Proving that a system of ODEs has a focus at the origin,Proving that a system of ODEs has a focus at the origin,,"Prove that the following system of ODEs has a focus at the origin. $$\begin{aligned} \dot{x} &= -x^3-y^3\\                   \dot{y} &= x^3       \end{aligned}$$ Plotting the vector field confirms this fact. I've tried to convert the system to polar coordinates and I've shown that $\dot{\theta}\neq 0$ except at the origin. I believe it would suffice to show that $$r(\theta+2\pi)-r(\theta) < 0$$ for al $\theta$ when close to the origin. However, this seems to involve expressing $r$ in terms of $\theta$ , which I don't know how to do. Any help? Additionaly, is there an easier method? Thank you in advance. I also had the following idea: according to the fundamental theorem of calculus, $$r(\theta+2\pi)-r(\theta)=\int_\theta^{\theta+2\pi}\frac{dr}{d\theta}d\theta$$ If only I could somehow show this is strictly less than $0$ ...","Prove that the following system of ODEs has a focus at the origin. Plotting the vector field confirms this fact. I've tried to convert the system to polar coordinates and I've shown that except at the origin. I believe it would suffice to show that for al when close to the origin. However, this seems to involve expressing in terms of , which I don't know how to do. Any help? Additionaly, is there an easier method? Thank you in advance. I also had the following idea: according to the fundamental theorem of calculus, If only I could somehow show this is strictly less than ...","\begin{aligned} \dot{x} &= -x^3-y^3\\
                  \dot{y} &= x^3       \end{aligned} \dot{\theta}\neq 0 r(\theta+2\pi)-r(\theta) < 0 \theta r \theta r(\theta+2\pi)-r(\theta)=\int_\theta^{\theta+2\pi}\frac{dr}{d\theta}d\theta 0","['ordinary-differential-equations', 'dynamical-systems', 'polar-coordinates', 'stability-in-odes']"
32,First-order ODE,First-order ODE,,Please look at the equation $$ \frac{d x }{dt } = \frac{c}{x} - x + h(t) $$ where $c \geq 0$ is a constant; the initial condition is given at time $t=0$ (say $x = x_0$ at $t=0$ ); and $h(t)$ is a function defined for all $t\geq 0$ . For $c=0$ the solution is $$ x = x_0e^{-t} + \int_0^t e^{-(t-\tau)} h(\tau) d \tau $$ For $h=0$ the solution is $$ x =  \sqrt{c + (x_0^2 - c) e^{-2 t} } $$ What would the solution be for both $c$ and $h$ are different from zero?,Please look at the equation where is a constant; the initial condition is given at time (say at ); and is a function defined for all . For the solution is For the solution is What would the solution be for both and are different from zero?, \frac{d x }{dt } = \frac{c}{x} - x + h(t)  c \geq 0 t=0 x = x_0 t=0 h(t) t\geq 0 c=0  x = x_0e^{-t} + \int_0^t e^{-(t-\tau)} h(\tau) d \tau  h=0  x =  \sqrt{c + (x_0^2 - c) e^{-2 t} }  c h,"['integration', 'ordinary-differential-equations', 'analysis', 'dynamical-systems']"
33,Solution to an ODE 3,Solution to an ODE 3,,"I want to find the solutions to $$x(t)+\frac{1}{10}\int_0^1 e^{t-s} x(s) ds=1.$$ Differential equations are not my field, so I'm not sure where to start.","I want to find the solutions to Differential equations are not my field, so I'm not sure where to start.",x(t)+\frac{1}{10}\int_0^1 e^{t-s} x(s) ds=1.,['ordinary-differential-equations']
34,confused with using $dx$ [duplicate],confused with using  [duplicate],dx,"This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) Closed 5 years ago . I am studying differential equations right now and I am confused the way $dx$ is being used. When I learnt calculus I thought that $\frac{df}{dx}$ is just symbolic representation of derivative and we can't use it as a fraction. But when it comes to differential equation like $y = \frac{df}{dx}$ they do the following: $$ydx = df$$ $$\int ydx = f$$ And then again I was taught that $dx$ in integral has no meaning but only convenient way to represent integral. I am totally fine with the intuition why they do that, I can imagine $\frac{df}{dx}$ meaning small change in $f$ divided by small change in $x$ and integral meaning sum of recatngles of width $dx$ and height $f$ , but what I lack is rigorous transition from $\frac{df}{dx}$ meaning just a symbolic representation of derivative to the state where we can algebraically manipulate it as a fraction. Could you please help me with that or suggest some reading? Also, could you please recommend me some books on differential equations that teach intuition behind the equations?","This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) Closed 5 years ago . I am studying differential equations right now and I am confused the way is being used. When I learnt calculus I thought that is just symbolic representation of derivative and we can't use it as a fraction. But when it comes to differential equation like they do the following: And then again I was taught that in integral has no meaning but only convenient way to represent integral. I am totally fine with the intuition why they do that, I can imagine meaning small change in divided by small change in and integral meaning sum of recatngles of width and height , but what I lack is rigorous transition from meaning just a symbolic representation of derivative to the state where we can algebraically manipulate it as a fraction. Could you please help me with that or suggest some reading? Also, could you please recommend me some books on differential equations that teach intuition behind the equations?",dx \frac{df}{dx} y = \frac{df}{dx} ydx = df \int ydx = f dx \frac{df}{dx} f x dx f \frac{df}{dx},"['calculus', 'ordinary-differential-equations', 'derivatives', 'infinitesimals']"
35,ODE Interval of Validity,ODE Interval of Validity,,"The problem is stated as such; Solve the I.V.P $$y'=\frac{1+3x^2}{3y^2-6y}; y(0)=1 $$ and determine the interval in which the solution is valid. The hint given is:  To find the interval of definition, look for points where the integral curve has a vertical tangent line. I was able to solve the IVP with the following solution; $$y^3-3y^2-x-x^3+2=0$$ But with the hint, I am under the assumption that the curve has vertical tangent lines when the denominator of the derivative is equal to 0, which would be when $y=0$ and $y=2$ .  But the solution is give as $|x|<1$ .  What am I doing wrong?","The problem is stated as such; Solve the I.V.P and determine the interval in which the solution is valid. The hint given is:  To find the interval of definition, look for points where the integral curve has a vertical tangent line. I was able to solve the IVP with the following solution; But with the hint, I am under the assumption that the curve has vertical tangent lines when the denominator of the derivative is equal to 0, which would be when and .  But the solution is give as .  What am I doing wrong?","y'=\frac{1+3x^2}{3y^2-6y}; y(0)=1
 y^3-3y^2-x-x^3+2=0 y=0 y=2 |x|<1","['calculus', 'ordinary-differential-equations', 'derivatives']"
36,How to find the second solution to an ODE of the form $xy''+y'-xy=0$?,How to find the second solution to an ODE of the form ?,xy''+y'-xy=0,"I am learning about the Frobenius method to solving ordinary differential equations, but I seem to have some difficulty with finding the second linearly independent solution to the above equation (this is a homework problem). I was hoping someone could point out what is going wrong here. To start, I found my first solution by letting $y = x^{r} \sum_{n=0}^{\infty} c_n x^n$ . Using this method and substituting it above, I found the first solution to be: $$ y_1 = \sum_{n=0}^{\infty} \frac{x^{2n}}{2^{2n}(n!)^2}.$$ The characteristic of the original ODE is $r^2 = 0$ , so we have repeated roots. Therefore, the second solution should contain a logarithm and be given by: $$y_2 = y_1 \ln x + x^{r+1} \sum_{n=0}^{\infty} b_n x^n.$$ When I do this, I find that the coefficients $b_{2n} = 0$ , and the recursion relation for the odd coefficients is (with $b_1 = -1/4$ ): $$ b_{2n-1} = \frac{b_{2n-3}}{(2n)^2} - \frac{2}{2^{2n} (n!)^2 (2n) }, \,\, n \ge 2.$$ The issue is that this recursion relation will yield coefficients that are always negative. However, the actual solution yields alternating coefficients, which means something above is wrong. I'm only interested in the first few terms of the solution for $y_2$ , but I still need to get the alternative terms. I know the actual second solution is given by (from the textbook Elementary Differential Equations With Boundary Value Problems , sixth edition, Edwards and Penney, question 9 on page 246): $$y_2 \approx y_1 \left( \ln x - \frac{x^2}{4} + \frac{5x^4}{128} - \frac{23x^6}{3456} \right).$$ In addition to this, I know there is a different way to go about this by using an integrating factor and doing some long division (through the method of reduction of order), and I do indeed get the correct answer. It's just that I would like to confirm why I'm getting the wrong coefficients here. The first coefficient $b_1$ is good, but the others don't match the solution above. Finally, I realize that this equation yields Bessel functions with imaginary arguments, but I'm looking for the actual terms using the above method.","I am learning about the Frobenius method to solving ordinary differential equations, but I seem to have some difficulty with finding the second linearly independent solution to the above equation (this is a homework problem). I was hoping someone could point out what is going wrong here. To start, I found my first solution by letting . Using this method and substituting it above, I found the first solution to be: The characteristic of the original ODE is , so we have repeated roots. Therefore, the second solution should contain a logarithm and be given by: When I do this, I find that the coefficients , and the recursion relation for the odd coefficients is (with ): The issue is that this recursion relation will yield coefficients that are always negative. However, the actual solution yields alternating coefficients, which means something above is wrong. I'm only interested in the first few terms of the solution for , but I still need to get the alternative terms. I know the actual second solution is given by (from the textbook Elementary Differential Equations With Boundary Value Problems , sixth edition, Edwards and Penney, question 9 on page 246): In addition to this, I know there is a different way to go about this by using an integrating factor and doing some long division (through the method of reduction of order), and I do indeed get the correct answer. It's just that I would like to confirm why I'm getting the wrong coefficients here. The first coefficient is good, but the others don't match the solution above. Finally, I realize that this equation yields Bessel functions with imaginary arguments, but I'm looking for the actual terms using the above method.","y = x^{r} \sum_{n=0}^{\infty} c_n x^n  y_1 = \sum_{n=0}^{\infty} \frac{x^{2n}}{2^{2n}(n!)^2}. r^2 = 0 y_2 = y_1 \ln x + x^{r+1} \sum_{n=0}^{\infty} b_n x^n. b_{2n} = 0 b_1 = -1/4  b_{2n-1} = \frac{b_{2n-3}}{(2n)^2} - \frac{2}{2^{2n} (n!)^2 (2n) }, \,\, n \ge 2. y_2 y_2 \approx y_1 \left( \ln x - \frac{x^2}{4} + \frac{5x^4}{128} - \frac{23x^6}{3456} \right). b_1","['ordinary-differential-equations', 'power-series', 'recursion']"
37,Analytical solution for a nonlinear coupled PDE,Analytical solution for a nonlinear coupled PDE,,"I would like to find the solution to this nonlinearly coupled PDE: This is an equation involving quantum mechanics and found in this paper . (Technically these E's are operators, but you can simply treat them as functions of z and t. Also $E^+$ is the conjugate of E so $E^+E = E^2 = E(z, t)^2$ ) According to this paper, with some approximations, this has an analytical solution of the form $$E_{1,2}(z, t) = E_{1,2}(0,t') \exp(i \eta z |E_{2,1}(0, t')|^2)$$ where $t' = t-z/v_g$ If I am reading the paper correctly, the approximations are that $\beta \to 0$ and $F \to 0$ - but I'm not entirely sure. In my attempts to find the solution, I first tried to solve it as an ode (where the time derivatives are zero). $$ E_1'(z) = -k E_1(z) + (i \eta)|E_2|^2 E_1 $$ $$ E_2'(z) = -k E_2(z) + (i \eta)|E_1|^2 E_2 $$ But I'm struggling to even work this out. Any ideas for how I can proceed? EDIT: One of the answers suggests a method at arriving at the solution, and here I'm showing my handwritten attempt at getting a solution using this method. I was successful in finding a solution (maybe with mistakes?) but ended up with an answer that does not match what is described in the text.","I would like to find the solution to this nonlinearly coupled PDE: This is an equation involving quantum mechanics and found in this paper . (Technically these E's are operators, but you can simply treat them as functions of z and t. Also is the conjugate of E so ) According to this paper, with some approximations, this has an analytical solution of the form where If I am reading the paper correctly, the approximations are that and - but I'm not entirely sure. In my attempts to find the solution, I first tried to solve it as an ode (where the time derivatives are zero). But I'm struggling to even work this out. Any ideas for how I can proceed? EDIT: One of the answers suggests a method at arriving at the solution, and here I'm showing my handwritten attempt at getting a solution using this method. I was successful in finding a solution (maybe with mistakes?) but ended up with an answer that does not match what is described in the text.","E^+ E^+E = E^2 = E(z, t)^2 E_{1,2}(z, t) = E_{1,2}(0,t') \exp(i \eta z |E_{2,1}(0, t')|^2) t' = t-z/v_g \beta \to 0 F \to 0  E_1'(z) = -k E_1(z) + (i \eta)|E_2|^2 E_1   E_2'(z) = -k E_2(z) + (i \eta)|E_1|^2 E_2 ","['ordinary-differential-equations', 'partial-differential-equations']"
38,Euler-Lagrange for Anharmonic Oscillator,Euler-Lagrange for Anharmonic Oscillator,,"Suppose we are given some potential: $V(x)=\frac{1}{2}kx^2+\frac{1}{4}\lambda x^4$ where $k$ and $\lambda$ are constants. $\\$ i) I'm trying to find the Lagrangian and use Euler-Lagrange to find the Equation of Motion. So i know the Lagrangian is $$L=T-V$$ which leads to my guess: $$\frac{1}{2}m\dot x^2-(\frac{1}{2}kx^2+\frac{1}{4}\lambda x^4)$$ giving us $$\frac{d}{dt}(m\dot x)-(kx+\lambda x^3)=0$$ and hence $$m\ddot x-kx-\lambda x^3=0$$ and now I'm stuck from this point on, and unsure what to do here.","Suppose we are given some potential: where and are constants. i) I'm trying to find the Lagrangian and use Euler-Lagrange to find the Equation of Motion. So i know the Lagrangian is which leads to my guess: giving us and hence and now I'm stuck from this point on, and unsure what to do here.",V(x)=\frac{1}{2}kx^2+\frac{1}{4}\lambda x^4 k \lambda \\ L=T-V \frac{1}{2}m\dot x^2-(\frac{1}{2}kx^2+\frac{1}{4}\lambda x^4) \frac{d}{dt}(m\dot x)-(kx+\lambda x^3)=0 m\ddot x-kx-\lambda x^3=0,"['ordinary-differential-equations', 'mathematical-physics', 'euler-lagrange-equation']"
39,Solution curves of autonomous ODE,Solution curves of autonomous ODE,,What are some defining characteristics of the solution curves of autonomous ordinary differential equations? I have difficulty figuring out which of the solution curves below correspond to that of an autonomous differential equation system.,What are some defining characteristics of the solution curves of autonomous ordinary differential equations? I have difficulty figuring out which of the solution curves below correspond to that of an autonomous differential equation system.,,['ordinary-differential-equations']
40,"If $f(x,y) = g(r)$, where $r=(x^2+y^2)^{1/2}$, prove $\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = \frac 1r g'(r)+ g''(r).$","If , where , prove","f(x,y) = g(r) r=(x^2+y^2)^{1/2} \frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = \frac 1r g'(r)+ g''(r).","Consider a scalar field defined in $\mathbb{R}^2$ such that $f(x,y)$ depends only on the distance $r$ of $(x,y)$ from the origin, say $f(x,y) = g(r)$ , where $r=(x^2+y^2)^{1/2}$ . a) Prove that for $(x,y) \ne (0,0)$ , we have $$\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = \frac 1r g'(r)+ g''(r).$$ b) Now assume further that $f$ satisfies Laplace's Equation, $$\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = 0. $$ Use part (a) to show that $f(x,y) = a \log(x^2+y^2) +b$ for $(x,y) \ne (0,0),$ where $a$ and $b$ are constants. What I've tried: I think we can express $g'(r)$ as $$\frac{d}{dr}g(r) = \frac{\partial g}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial g}{\partial y} \frac{\partial y}{\partial r},$$ but I'm confused as to how to determine $\frac{\partial x}{\partial r}$ and $\frac{\partial y}{\partial r}$ since $r$ is in terms of both $x$ and $y$ .","Consider a scalar field defined in such that depends only on the distance of from the origin, say , where . a) Prove that for , we have b) Now assume further that satisfies Laplace's Equation, Use part (a) to show that for where and are constants. What I've tried: I think we can express as but I'm confused as to how to determine and since is in terms of both and .","\mathbb{R}^2 f(x,y) r (x,y) f(x,y) = g(r) r=(x^2+y^2)^{1/2} (x,y) \ne (0,0) \frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = \frac 1r g'(r)+ g''(r). f \frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = 0.  f(x,y) = a \log(x^2+y^2) +b (x,y) \ne (0,0), a b g'(r) \frac{d}{dr}g(r) = \frac{\partial g}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial g}{\partial y} \frac{\partial y}{\partial r}, \frac{\partial x}{\partial r} \frac{\partial y}{\partial r} r x y","['ordinary-differential-equations', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
41,General solution to linear system with general form,General solution to linear system with general form,,"I am trying to find the general solution of the system $$X' = \begin{bmatrix}     a & b  \\     c & d   \end{bmatrix}X$$ where $a+d \not= 0 $ and $ad-bc=0$ I find that the eigenvalues are 0 and $a+d$ with corresponding eigenvectors both $[0,0]$ which implies that the general solution is simply $X(t)=0$ but this solution does not seem right to me. Is there something I have done wrong?",I am trying to find the general solution of the system where and I find that the eigenvalues are 0 and with corresponding eigenvectors both which implies that the general solution is simply but this solution does not seem right to me. Is there something I have done wrong?,"X' = \begin{bmatrix}
    a & b  \\
    c & d
  \end{bmatrix}X a+d \not= 0  ad-bc=0 a+d [0,0] X(t)=0",['ordinary-differential-equations']
42,What to multiply by to get correct form ODE,What to multiply by to get correct form ODE,,"Suppose $y'' + f(x)y = 0$ where $M \geq f(x) \geq m > 0$ on some interval $[a,b]$ , then the number zeros $N$ of a non trivial solution is $\lfloor\frac{(b-a)\sqrt{m}}{\pi}\rfloor \leq N \leq \lceil\frac{(b-a)\sqrt{M}}{\pi}\rceil$ Simple. Now suppose I have an equation $y''+4y'+\frac{8x+\sin(x)}{x+1}y = 0$ and I want to estimate the number of zeros of a non trivial solution. I can't use the theorem as is, because the ODE is not in the correct form, to fix this, we can multiply by $e^{2x}$ and get $y''e^{2x}+4y'e^{2x}+\frac{8x+\sin(x)}{x+1}ye^{2x} = 0$ Now if we let $ye^{2x} = z$ we have an ODE $z'' + (\frac{8x+\sin(x)}{x+1}-4)z = 0$ which is in the correct form How did the professor know to multiply by $e^{2x}$ ? Is there a method to this or was this just a lucky guess","Suppose where on some interval , then the number zeros of a non trivial solution is Simple. Now suppose I have an equation and I want to estimate the number of zeros of a non trivial solution. I can't use the theorem as is, because the ODE is not in the correct form, to fix this, we can multiply by and get Now if we let we have an ODE which is in the correct form How did the professor know to multiply by ? Is there a method to this or was this just a lucky guess","y'' + f(x)y = 0 M \geq f(x) \geq m > 0 [a,b] N \lfloor\frac{(b-a)\sqrt{m}}{\pi}\rfloor \leq N \leq \lceil\frac{(b-a)\sqrt{M}}{\pi}\rceil y''+4y'+\frac{8x+\sin(x)}{x+1}y = 0 e^{2x} y''e^{2x}+4y'e^{2x}+\frac{8x+\sin(x)}{x+1}ye^{2x} = 0 ye^{2x} = z z'' + (\frac{8x+\sin(x)}{x+1}-4)z = 0 e^{2x}","['calculus', 'ordinary-differential-equations']"
43,Explanation of differing solutions for an exact ODE,Explanation of differing solutions for an exact ODE,,"I was solving the following ODE $$ y \cos(xy) + x \cos(xy) y' = 0$$ My initial intuition to solving it is to observe that it can be factored as: $$ \cos(xy) (y + xy') = 0$$ So over $x,y$ that don't satisfy $\cos(xy) = 0$ we have $$ y  + xy' = 0$$ And this is an exact eqation which has solutions of the form $$ xy = C$$ Now the video series that the ODE originated from takes a different approach. It notices that $$\frac{\partial}{\partial y} [y \cos(xy)] = \cos(xy)- xy \sin(xy) , \frac{\partial}{\partial x} [x\cos(xy)] = \cos(xy) - xy \sin(xy)  $$ So therefore the solutions are of the form $ \sin(xy) = C$ clearly my approach doesn't agree with the classical approach, but my only difference was dividing out  a term which is 0 in a set non-dense in $\mathbb{R}^2$ so I don't understand why my answer differs so much. And my intuition that $xy$ should be constant for all points where $\cos(xy) \ne 0$ can't possibly be true so I feel i'm deeply missing some intuition here.","I was solving the following ODE My initial intuition to solving it is to observe that it can be factored as: So over that don't satisfy we have And this is an exact eqation which has solutions of the form Now the video series that the ODE originated from takes a different approach. It notices that So therefore the solutions are of the form clearly my approach doesn't agree with the classical approach, but my only difference was dividing out  a term which is 0 in a set non-dense in so I don't understand why my answer differs so much. And my intuition that should be constant for all points where can't possibly be true so I feel i'm deeply missing some intuition here."," y \cos(xy) + x \cos(xy) y' = 0  \cos(xy) (y + xy') = 0 x,y \cos(xy) = 0  y  + xy' = 0  xy = C \frac{\partial}{\partial y} [y \cos(xy)] = \cos(xy)- xy \sin(xy) , \frac{\partial}{\partial x} [x\cos(xy)] = \cos(xy) - xy \sin(xy)    \sin(xy) = C \mathbb{R}^2 xy \cos(xy) \ne 0","['ordinary-differential-equations', 'partial-differential-equations']"
44,Solve ODE using Fourier series.,Solve ODE using Fourier series.,,"I have that $$f(x)=\frac{4}{3}+\frac{2}{\pi^2}\sum_{n\in\mathbb{Z}}\frac{(-1)^n(1+i\pi  n)}{n^2}e^{i\pi n x}, \quad n\neq0\tag1$$ and I want to find a 2-periodic function $y(x)$ that solves the   following diff-equation $$2y''-y'-y=f(x).\tag{2}$$ Since $(1)$ is a linear ODE I know that it's solution is of the form $y(x)=y_h+y_p.$ Finding $y_h$ is trivial and done by solving the characteristic equation. I found it to be $y_h=C_1e^x+C_2e^{-x/2}.$ For $y_p$ , I need to use $f(x)$ somehow, I could not find any good example in the book on how to do this. I only found one video on youtube but it does not really seem to be that similar to my problem. I want to follow the answer in this thread but I can't seem to grasp the mechanics of what he means. Can someone show me how to go about this?","I have that and I want to find a 2-periodic function that solves the   following diff-equation Since is a linear ODE I know that it's solution is of the form Finding is trivial and done by solving the characteristic equation. I found it to be For , I need to use somehow, I could not find any good example in the book on how to do this. I only found one video on youtube but it does not really seem to be that similar to my problem. I want to follow the answer in this thread but I can't seem to grasp the mechanics of what he means. Can someone show me how to go about this?","f(x)=\frac{4}{3}+\frac{2}{\pi^2}\sum_{n\in\mathbb{Z}}\frac{(-1)^n(1+i\pi
 n)}{n^2}e^{i\pi n x}, \quad n\neq0\tag1 y(x) 2y''-y'-y=f(x).\tag{2} (1) y(x)=y_h+y_p. y_h y_h=C_1e^x+C_2e^{-x/2}. y_p f(x)","['ordinary-differential-equations', 'fourier-analysis', 'fourier-series']"
45,Solving $(D^5-D)y = 8\sin x$ using operator methods,Solving  using operator methods,(D^5-D)y = 8\sin x,"This question is similar to this link but here it involves $\sin x$ and it creates a problem. I tried writing $\sin x = \Im (e^{ix})$ but that doesnt help: $$(D^5-D) y = 8\Im (e^{ix})$$ Now $(D^5-D)$ has factor $D=i$ so we need to factor that out and apply it on right side: $$(D-1)(D+i)(D+1)D y=\frac{1}{D-i}(\Im(e^{ix}))\\ \implies y = \frac{8}{(i-1)(i+1)(2i)(i)}\Im(e^{ix}\int e^{ix} e^{-ix} dx)$$ which gives $y_p = 2x\sin x$ . Another method would have been assuming $y_p = x(A\cos x + B\sin x)$ but that too would have been lengthy Is the method in this answer correct, because it is using complex numbers loosely. Is this operator method?","This question is similar to this link but here it involves and it creates a problem. I tried writing but that doesnt help: Now has factor so we need to factor that out and apply it on right side: which gives . Another method would have been assuming but that too would have been lengthy Is the method in this answer correct, because it is using complex numbers loosely. Is this operator method?","\sin x \sin x = \Im (e^{ix}) (D^5-D) y = 8\Im (e^{ix}) (D^5-D) D=i (D-1)(D+i)(D+1)D y=\frac{1}{D-i}(\Im(e^{ix}))\\
\implies y = \frac{8}{(i-1)(i+1)(2i)(i)}\Im(e^{ix}\int e^{ix} e^{-ix} dx) y_p = 2x\sin x y_p = x(A\cos x + B\sin x)","['calculus', 'ordinary-differential-equations']"
46,Solving a difficult differential equation,Solving a difficult differential equation,,"Well, I've to solve the following DE: $$y(t)=x(t)\cdot\text{a}+\text{b}\cdot\ln\left(1+\frac{x(t)}{\text{c}}\right)+\int_0^tx(\tau)\cdot p(t-\tau)\space\text{d}\tau\tag1$$ And I've no idea how to start. Some background information: this DE describes a current in an electric circuit. For the values of $a,b$ and $c$ I know that they are real and positive. For $c$ I know that $10^{-16}\le c\le10^{-4}$ . The function $y(t)=k\cdot\theta\left(t-m\right)+(n-k)\cdot\theta\left(t-v\right)$ where all the values of the constants are ral and positive and $\theta$ is the Heaviside Theta function. The function $p(t-\tau)=\mathcal{L}_\text{s}^{-1}\left\{\frac{1}{\frac{1}{R_2+sl}+\frac{s}{sR_3+\frac{1}{z}}}\right\}_{\left(t-\tau\right)}$ where all the constants are again real and positive.","Well, I've to solve the following DE: And I've no idea how to start. Some background information: this DE describes a current in an electric circuit. For the values of and I know that they are real and positive. For I know that . The function where all the values of the constants are ral and positive and is the Heaviside Theta function. The function where all the constants are again real and positive.","y(t)=x(t)\cdot\text{a}+\text{b}\cdot\ln\left(1+\frac{x(t)}{\text{c}}\right)+\int_0^tx(\tau)\cdot p(t-\tau)\space\text{d}\tau\tag1 a,b c c 10^{-16}\le c\le10^{-4} y(t)=k\cdot\theta\left(t-m\right)+(n-k)\cdot\theta\left(t-v\right) \theta p(t-\tau)=\mathcal{L}_\text{s}^{-1}\left\{\frac{1}{\frac{1}{R_2+sl}+\frac{s}{sR_3+\frac{1}{z}}}\right\}_{\left(t-\tau\right)}","['integration', 'ordinary-differential-equations', 'definite-integrals', 'logarithms']"
47,Possible spectra of singular Sturm-Liouville problems,Possible spectra of singular Sturm-Liouville problems,,"A Sturm–Liouville (SL) eigenvalue problem with separated boundary condition on $[a,b]$ $$(py')'-qy=-\lambda^2wy$$ is regular if $p(x),w(x)>0$ and $p(x),p'(x),q(x),w(x)$ are continuous in the finite interval $[a,b]$ . Otherwise, it becomes singular . Let's just take the homogeneous Dirichlet b.c. if necessary. The regular case exhibits an infinite sequence of discrete eigenvalues and corresponding orthonormal eigenfunctions. But for the singular case, is there any conclusion and condition on the possibilities of the spectrum? By skimming over this wiki page and other materials, I only vaguely know it's possible to have the following only an inifinite sequence of discrete eigenvalues only a continuous spectrum a finite sequence of discrete eigenvalues a discrete/continuous mixture I'm confused by case 3 and case 4. The questions arise from another post that seems to have a finite sequence of discrete eigenvalues. Is only a finite sequence of discrete eigenvalues possible? Or case 3 always implies a complementary continuous spectrum, i.e., case 4? In case 4 with a finite sequence of discrete eigenvalues, can the ranges (I mean the interval between the upper and lower bounds) of discrete and continuous spectra have overlap? Or always exclusive? If exclusive, could there be region(s) not covered by either of the two? how about case 4 with an infinite sequence of discrete eigenvalues? (optional) Examples are also welcome.","A Sturm–Liouville (SL) eigenvalue problem with separated boundary condition on is regular if and are continuous in the finite interval . Otherwise, it becomes singular . Let's just take the homogeneous Dirichlet b.c. if necessary. The regular case exhibits an infinite sequence of discrete eigenvalues and corresponding orthonormal eigenfunctions. But for the singular case, is there any conclusion and condition on the possibilities of the spectrum? By skimming over this wiki page and other materials, I only vaguely know it's possible to have the following only an inifinite sequence of discrete eigenvalues only a continuous spectrum a finite sequence of discrete eigenvalues a discrete/continuous mixture I'm confused by case 3 and case 4. The questions arise from another post that seems to have a finite sequence of discrete eigenvalues. Is only a finite sequence of discrete eigenvalues possible? Or case 3 always implies a complementary continuous spectrum, i.e., case 4? In case 4 with a finite sequence of discrete eigenvalues, can the ranges (I mean the interval between the upper and lower bounds) of discrete and continuous spectra have overlap? Or always exclusive? If exclusive, could there be region(s) not covered by either of the two? how about case 4 with an infinite sequence of discrete eigenvalues? (optional) Examples are also welcome.","[a,b] (py')'-qy=-\lambda^2wy p(x),w(x)>0 p(x),p'(x),q(x),w(x) [a,b]","['functional-analysis', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'spectral-theory', 'sturm-liouville']"
48,Laplace transform for solving differential equations [closed],Laplace transform for solving differential equations [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I am confused about Laplace transform. the only thing that is important in Laplace theorem is the value of $f(t)$ in $t \geq 0$ . laplace transform: $\mathcal{L}\{f(t)\}$ $=$ $\int_0^\infty \! e^{-st}f(t)$ so as we see in the above transform $f(t)$ if $t<0$ is not important at all. so do we find the answer for only $t > 0$ or the answer is correct in $\mathbb{R}$ ?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I am confused about Laplace transform. the only thing that is important in Laplace theorem is the value of in . laplace transform: so as we see in the above transform if is not important at all. so do we find the answer for only or the answer is correct in ?,f(t) t \geq 0 \mathcal{L}\{f(t)\} = \int_0^\infty \! e^{-st}f(t) f(t) t<0 t > 0 \mathbb{R},"['ordinary-differential-equations', 'laplace-transform']"
49,Prove $\frac{dw}{dt} + aw \leq 0$ implies that $w(t)$ is a decreasing function,Prove  implies that  is a decreasing function,\frac{dw}{dt} + aw \leq 0 w(t),"Question: Suppose that $w(t)$ is a non-negative continuous function, and there exists a constant $a \in \Bbb R$ such that \begin{align} \frac{dw}{dt} + aw & \leq 0 \qquad \qquad \text{for }t \geq 0 \\ w(0) & = 0 \end{align} Is it then true that $w(t)$ is a decreasing function (on the interval $[0,\infty)$ )? Attempt: If in addition it is known that $a \geq 0$ , then this would be easy, since we would have $$\frac{dw}{dt} + aw \leq 0 \implies \frac{dw}{dt} \leq -aw \leq 0$$ However, I want to prove this for general constant $a$ . I have a feeling that I can somehow compare this to the exponential function $Ae^{-at}$ as this is the solution of the differential equation $$\frac{dw}{dt} + aw =0$$ Gronwall's inequality also comes to mind, but I am not sure how to use it. Any help would be much appreciated. Thanks! Edit: It seems to me that $w$ must in fact be the zero function. Thinking about it in an intuitive/heuristic way, $$w'(0) + aw(0) \leq 0 \implies w'(0) \leq 0$$ since $w(0)=0$ , i.e. $w$ is decreasing at $t=0$ . However, $w$ is non-negative with $w(0)=0$ , so in fact $w'(0)=0$ . This implies that $w(0 + \delta) = 0$ and we can repeat the above argument.","Question: Suppose that is a non-negative continuous function, and there exists a constant such that Is it then true that is a decreasing function (on the interval )? Attempt: If in addition it is known that , then this would be easy, since we would have However, I want to prove this for general constant . I have a feeling that I can somehow compare this to the exponential function as this is the solution of the differential equation Gronwall's inequality also comes to mind, but I am not sure how to use it. Any help would be much appreciated. Thanks! Edit: It seems to me that must in fact be the zero function. Thinking about it in an intuitive/heuristic way, since , i.e. is decreasing at . However, is non-negative with , so in fact . This implies that and we can repeat the above argument.","w(t) a \in \Bbb R \begin{align}
\frac{dw}{dt} + aw & \leq 0 \qquad \qquad \text{for }t \geq 0 \\
w(0) & = 0
\end{align} w(t) [0,\infty) a \geq 0 \frac{dw}{dt} + aw \leq 0 \implies \frac{dw}{dt} \leq -aw \leq 0 a Ae^{-at} \frac{dw}{dt} + aw =0 w w'(0) + aw(0) \leq 0 \implies w'(0) \leq 0 w(0)=0 w t=0 w w(0)=0 w'(0)=0 w(0 + \delta) = 0","['ordinary-differential-equations', 'inequality']"
50,Integro-differential equation including a convolution of the first derivative.,Integro-differential equation including a convolution of the first derivative.,,"I am having difficulty finding the right approach to solving the following differential equation, $$ y''(t)+\int_t^Tg(s-t)y'(s)\,ds=f(t), $$ with the boundary conditions, $$y(0)=y_0\,,\quad y(T)=0.$$ I considered representing $y(t)$ using a Fourier series, but term-wise differentiation is not guaranteed because the periodic extension of $y(t)$ isn't  differentiable at the boundaries $t=0,T$ . What approach is the smartest to solve this equation? Any help will be appreciated.","I am having difficulty finding the right approach to solving the following differential equation, with the boundary conditions, I considered representing using a Fourier series, but term-wise differentiation is not guaranteed because the periodic extension of isn't  differentiable at the boundaries . What approach is the smartest to solve this equation? Any help will be appreciated.","
y''(t)+\int_t^Tg(s-t)y'(s)\,ds=f(t),
 y(0)=y_0\,,\quad y(T)=0. y(t) y(t) t=0,T","['ordinary-differential-equations', 'convolution', 'boundary-value-problem', 'integro-differential-equations']"
51,Laplace Transform of $y^n$,Laplace Transform of,y^n,"When the Laplace transform of $y$ is denoted $Y(s)$ , we have formulas for the derivatives of $y$ without actually knowing what $y$ is. Is there an explicit formula for $y^2$ ? More generally, $y^n$ ?","When the Laplace transform of is denoted , we have formulas for the derivatives of without actually knowing what is. Is there an explicit formula for ? More generally, ?",y Y(s) y y y^2 y^n,"['ordinary-differential-equations', 'laplace-transform']"
52,Bifurcation points of differential equation (example),Bifurcation points of differential equation (example),,"Assume the differential equation: $$ x'=\lambda^2-8a\lambda x+2x^2, \quad a\in \mathbb{R}. $$ The critical points are the solutions to the equation: $$ x'=0 \iff 2x^2-8a\lambda x +\lambda^2=0\tag{1} $$ which admits solutions: $$ x=\lambda \cdot \frac{8a \pm \sqrt{64a^2-8}}{2} $$ Now, having in mind that by bifurcation point , we mean a point where change of the number of equilibrium points occur, it seems to me that the number of $(1)$ 's solutions depends only on $a$ and not on $\lambda$ . Is this the case or did I get something wrong?","Assume the differential equation: The critical points are the solutions to the equation: which admits solutions: Now, having in mind that by bifurcation point , we mean a point where change of the number of equilibrium points occur, it seems to me that the number of 's solutions depends only on and not on . Is this the case or did I get something wrong?","
x'=\lambda^2-8a\lambda x+2x^2, \quad a\in \mathbb{R}.
 
x'=0 \iff 2x^2-8a\lambda x +\lambda^2=0\tag{1}
 
x=\lambda \cdot \frac{8a \pm \sqrt{64a^2-8}}{2}
 (1) a \lambda","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'bifurcation']"
53,What is the solution to $y*y’’=\sin(x)$? [closed],What is the solution to ? [closed],y*y’’=\sin(x),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I saw this differential equation in a Khan Academy video. I’m still in high school and I have just began to learn a lot about differential equations, so I wanted to see if i could solve this. I don’t really know how to though. Apologies if it’s a stupid question, that’s just me. The differential equation is $$y\frac{d^2y}{dx^2}=\sin(x)$$ . If anybody can tell me the answer to this, and how to get it, if it is solvable, I’d love to know. Link to the video, the uploader mentions it just at the end but doesn’t really say much about it: https://youtu.be/-_POEWfygmU","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I saw this differential equation in a Khan Academy video. I’m still in high school and I have just began to learn a lot about differential equations, so I wanted to see if i could solve this. I don’t really know how to though. Apologies if it’s a stupid question, that’s just me. The differential equation is . If anybody can tell me the answer to this, and how to get it, if it is solvable, I’d love to know. Link to the video, the uploader mentions it just at the end but doesn’t really say much about it: https://youtu.be/-_POEWfygmU",y\frac{d^2y}{dx^2}=\sin(x),['ordinary-differential-equations']
54,Lipschitz continuity of $e^{\sin}$,Lipschitz continuity of,e^{\sin},"We want to use the Picard-Lindelöf-Theorem to show that the ODE $$y'=\mathrm{e}^{\sin(ty)}$$ has a unique solution on $\mathbb{R}$ with the initial value $y(0)=0$ . As far as I know, we have to show that the right hand side is Lipschitz-continuous with respect to $y$ . So we need to proof $$\vert \mathrm{e}^{\sin(tx)}-\mathrm{e}^{\sin(ty)}\vert \leq L \vert x-y\vert$$ for all $t,x,y \in \mathbb{R}$ . I don't get it, to show this. I heard that because the right hand side is differentiable, that is eventually possible to show this with the mean-value theorem or gradient. But I don't know whether this works because I think the derivative is not bounded. What can one do?","We want to use the Picard-Lindelöf-Theorem to show that the ODE has a unique solution on with the initial value . As far as I know, we have to show that the right hand side is Lipschitz-continuous with respect to . So we need to proof for all . I don't get it, to show this. I heard that because the right hand side is differentiable, that is eventually possible to show this with the mean-value theorem or gradient. But I don't know whether this works because I think the derivative is not bounded. What can one do?","y'=\mathrm{e}^{\sin(ty)} \mathbb{R} y(0)=0 y \vert \mathrm{e}^{\sin(tx)}-\mathrm{e}^{\sin(ty)}\vert \leq L \vert x-y\vert t,x,y \in \mathbb{R}","['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'lipschitz-functions']"
55,Verfying partial derivatives for an ODE,Verfying partial derivatives for an ODE,,"Let $\phi(t,x)$ : $[\mathbb{R}$ x $\mathbb{R^N}] \rightarrow \mathbb{R^N}$ have the following properties: for any $t\in\mathbb{R}$ and $x\in\mathbb{R^N}$ , $\phi(t,x)$ is differentiable in both arguments and has a differentiable inverse. For any $s\in\mathbb{R}$ , $\phi(t+s,x)=\phi(t,\phi(s,x))$ . Define T(t,x) = $\dfrac{\partial\phi(t,x)}{\partial{x}}$ and $f(x)=\dfrac{\partial\phi(t,x)}{\partial{t}}\big|_{t=0}$ . Show that $\phi(t,x)$ and $T(t,x)$ satisfy: a. $\dfrac{\partial\phi(t,x)}{\partial{t}} = f(\phi(t,x))$ b. $\dfrac{\partial{T(t,x)}}{\partial{t}} = Df(\phi(t,x))\hspace{0.5mm}T(t,x)$ where $Df$ is the Jacobian of f. Here is my proposed solution. For the first part, I need to show that $\dfrac{\partial\phi(t,x)}{\partial{t}} = f(\phi(t,x))$ . Starting from the LHS, \begin{equation} \begin{split} \dfrac{\partial\phi(t,x)}{\partial{t}}  & = \dfrac{\partial\phi(t+0,x)}{\partial{t}}  \\  & = \dfrac{\partial\phi(t,\phi(0,x))}{\partial{t}} \\  & = \dfrac{\partial\phi(t,\phi(t,x))}{\partial{t}}\big|_{t=0} \\  & = f(\phi(t,x))  \end{split} \end{equation} I think that the second to last equality is justified since $\phi(t,x)$ is evaluated at $t=0$ . Although, I'm not sure if I have overlooked something and made a mistake. For the second part, I need to show that $\dfrac{\partial{T(t,x)}}{\partial{t}} = Df(\phi(t,x))\hspace{0.5mm}T(t,x)$ . Starting from the LHS, \begin{equation} \begin{split} \dfrac{\partial{T(t,x)}}{\partial{t}} & = \dfrac{\partial\phi(t,x)}{\partial{x}\hspace{0.5mm}\partial{t}}  \\  & = \dfrac{f(\phi(t,x))}{\partial{x}} \\  & = D{f(\phi(t,x))}\hspace{0.5mm}\dfrac{\partial\phi(t,x)}{\partial{x}} \\  & = D{f(\phi(t,x))}\hspace{0.5mm}T(t,x) \end{split} \end{equation} As $\phi\in{C^1}$ , the Jacobian is defined by the third equality. Is this approach correct? Please let me know if there are any better alternatives.","Let : x have the following properties: for any and , is differentiable in both arguments and has a differentiable inverse. For any , . Define T(t,x) = and . Show that and satisfy: a. b. where is the Jacobian of f. Here is my proposed solution. For the first part, I need to show that . Starting from the LHS, I think that the second to last equality is justified since is evaluated at . Although, I'm not sure if I have overlooked something and made a mistake. For the second part, I need to show that . Starting from the LHS, As , the Jacobian is defined by the third equality. Is this approach correct? Please let me know if there are any better alternatives.","\phi(t,x) [\mathbb{R} \mathbb{R^N}] \rightarrow \mathbb{R^N} t\in\mathbb{R} x\in\mathbb{R^N} \phi(t,x) s\in\mathbb{R} \phi(t+s,x)=\phi(t,\phi(s,x)) \dfrac{\partial\phi(t,x)}{\partial{x}} f(x)=\dfrac{\partial\phi(t,x)}{\partial{t}}\big|_{t=0} \phi(t,x) T(t,x) \dfrac{\partial\phi(t,x)}{\partial{t}} = f(\phi(t,x)) \dfrac{\partial{T(t,x)}}{\partial{t}} = Df(\phi(t,x))\hspace{0.5mm}T(t,x) Df \dfrac{\partial\phi(t,x)}{\partial{t}} = f(\phi(t,x)) \begin{equation}
\begin{split}
\dfrac{\partial\phi(t,x)}{\partial{t}}  & = \dfrac{\partial\phi(t+0,x)}{\partial{t}}  \\
 & = \dfrac{\partial\phi(t,\phi(0,x))}{\partial{t}} \\
 & = \dfrac{\partial\phi(t,\phi(t,x))}{\partial{t}}\big|_{t=0} \\
 & = f(\phi(t,x)) 
\end{split}
\end{equation} \phi(t,x) t=0 \dfrac{\partial{T(t,x)}}{\partial{t}} = Df(\phi(t,x))\hspace{0.5mm}T(t,x) \begin{equation}
\begin{split}
\dfrac{\partial{T(t,x)}}{\partial{t}} & = \dfrac{\partial\phi(t,x)}{\partial{x}\hspace{0.5mm}\partial{t}}  \\
 & = \dfrac{f(\phi(t,x))}{\partial{x}} \\
 & = D{f(\phi(t,x))}\hspace{0.5mm}\dfrac{\partial\phi(t,x)}{\partial{x}} \\
 & = D{f(\phi(t,x))}\hspace{0.5mm}T(t,x)
\end{split}
\end{equation} \phi\in{C^1}","['ordinary-differential-equations', 'partial-derivative']"
56,Solving an DE involving a logarithm,Solving an DE involving a logarithm,,"I've the following DE, describing a physical phenomenon. And the prupose is to solve that DE: $$x(t)\cdot r+x'(t)\cdot l+a\cdot\ln\left(1+\frac{x(t)}{b}\right)=0\space\Longleftrightarrow\space x(t)=\dots$$ The intial conditon is equal to $x(0)=x_0$ . For the constants (because that is maybe important for an approximation): $r$ can be very large; $l$ can be very large; $a$ is round about $0.02526$ ; $b$ is very small, round about $300\cdot10^{-6}$ ; $x_0$ can be very large I've no idea where to start what so ever. Thanks for any help or ideas","I've the following DE, describing a physical phenomenon. And the prupose is to solve that DE: The intial conditon is equal to . For the constants (because that is maybe important for an approximation): can be very large; can be very large; is round about ; is very small, round about ; can be very large I've no idea where to start what so ever. Thanks for any help or ideas",x(t)\cdot r+x'(t)\cdot l+a\cdot\ln\left(1+\frac{x(t)}{b}\right)=0\space\Longleftrightarrow\space x(t)=\dots x(0)=x_0 r l a 0.02526 b 300\cdot10^{-6} x_0,"['ordinary-differential-equations', 'functions', 'logarithms', 'physics', 'mathematical-physics']"
57,Find the resolvent set of a differential operator,Find the resolvent set of a differential operator,,"Let $X=C[0,1]$ and define: $$Af=f''\ \text{for }\ f \in D(A)=\{f\in C^{2}[0,1]: f(0)=f(1)=0 \}.$$ I want to find $\rho(A)$ that is the resolvent set of operator $A.$ I'm not sure how to proceed with this. From the definition $\rho(A)$ is the set of all $z\in \mathbb{C}$ such that $(A-zT)^{-1}$ is $1-1$ and onto. My take on this problem is following. First I find the solutions to $Af-zf=f''-zf=0$ to get that $$f(t) = c_{1}e^{\sqrt{z}t}+c_{2}e^{-\sqrt{z}t}$$ solves the equation. Now by applying the boundary conditions I get that there is no $z \in \mathbb{C}$ such that the BC are satisfied and thus $\sigma(A)$ is empty and since $\rho(A) =\mathbb{C}\setminus\sigma(A) = \mathbb{C}.$ Is this correct?",Let and define: I want to find that is the resolvent set of operator I'm not sure how to proceed with this. From the definition is the set of all such that is and onto. My take on this problem is following. First I find the solutions to to get that solves the equation. Now by applying the boundary conditions I get that there is no such that the BC are satisfied and thus is empty and since Is this correct?,"X=C[0,1] Af=f''\ \text{for }\ f \in D(A)=\{f\in C^{2}[0,1]: f(0)=f(1)=0 \}. \rho(A) A. \rho(A) z\in \mathbb{C} (A-zT)^{-1} 1-1 Af-zf=f''-zf=0 f(t) = c_{1}e^{\sqrt{z}t}+c_{2}e^{-\sqrt{z}t} z \in \mathbb{C} \sigma(A) \rho(A) =\mathbb{C}\setminus\sigma(A) = \mathbb{C}.","['functional-analysis', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'operator-theory']"
58,Peetre's Inequality - not strict?,Peetre's Inequality - not strict?,,"(Peetre's inequality)       Let $x,y \in \Bbb R^n$ and $s \in \Bbb R$ . Then $$ \frac{(1+|x|^2)^s}{(1+|y|^2)^s} \le 2^{|s|} (1+|x-y|^2)^{|s|}.$$ Proof: By switching roles of $x,y$ we may suppose $s \ge 0$ , and taking $s$ th root may assume $s=1$ . Then the argument i found online: \begin{align*} (1+|x|^2) &=  1 + |x-y|^2 + |y|^2 +2(x-y)y  \\   & \le 1 + |x-y|^2 + |y|^2 + (|x-y|^2+|y|^2) \\  & \le 2(1+|y|^2 + |x-y|^2 +|y|^2|x-y|^2 ) \\  & = 2(1+|y|^2)(1+|x-y|^2). \end{align*} What I don't understand is that on the third inequality, isn't this clearly a strict inequality when $s\not= 0$ ? (at least by 1)","(Peetre's inequality)       Let and . Then Proof: By switching roles of we may suppose , and taking th root may assume . Then the argument i found online: What I don't understand is that on the third inequality, isn't this clearly a strict inequality when ? (at least by 1)","x,y \in \Bbb R^n s \in \Bbb R  \frac{(1+|x|^2)^s}{(1+|y|^2)^s} \le 2^{|s|} (1+|x-y|^2)^{|s|}. x,y s \ge 0 s s=1 \begin{align*}
(1+|x|^2) &=  1 + |x-y|^2 + |y|^2 +2(x-y)y  \\ 
 & \le 1 + |x-y|^2 + |y|^2 + (|x-y|^2+|y|^2) \\ 
& \le 2(1+|y|^2 + |x-y|^2 +|y|^2|x-y|^2 ) \\ 
& = 2(1+|y|^2)(1+|x-y|^2).
\end{align*} s\not= 0","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'inequality']"
59,How to solve linear differential-difference equation?,How to solve linear differential-difference equation?,,"Given a linear differential-difference equation: $$A_{n+2}+\partial A_{n+1}+\partial^2 A_n=0,$$ where $A$ is a function of $n$ and $x$ , and $\partial$ represents the derivative about $x$ . How to solve this equation? The general case is this form $$A_{n+2}+P_1 A_{n+1}+P_2 A_n=0,$$ where $P_1,P_2$ are differential operator depending on function of $x$ . I have tried to set $A_n=B^nA_0$ , where $B$ is a pseudo differential operator and $A_0$ is a function of $x$ , then I obtain $$B^2+P_1B+P_2=0.$$ After solving $B$ , we have the solution of $A_n$ . I don't know whether this is correct and how to do next.","Given a linear differential-difference equation: where is a function of and , and represents the derivative about . How to solve this equation? The general case is this form where are differential operator depending on function of . I have tried to set , where is a pseudo differential operator and is a function of , then I obtain After solving , we have the solution of . I don't know whether this is correct and how to do next.","A_{n+2}+\partial A_{n+1}+\partial^2 A_n=0, A n x \partial x A_{n+2}+P_1 A_{n+1}+P_2 A_n=0, P_1,P_2 x A_n=B^nA_0 B A_0 x B^2+P_1B+P_2=0. B A_n","['ordinary-differential-equations', 'recurrence-relations', 'differential-operators']"
60,How to solve a system of ODE's using laplace transform?,How to solve a system of ODE's using laplace transform?,,"System of ODE's: $$y_1^""=16y_2$$ $$y_2^""=16y_1$$ Initial conditions : $$y_1(0)=2,\space y_1^{'}(0)=12$$ $$y_2(0)=6, \space y_2^{'}(0)=4$$ The equations i get once i do the laplace transform are: $$ s^2 \mathbf Y_1(s)-12s-2=16 \mathbf Y_2(s)$$ $$ s^2 \mathbf Y_2(s)-4s-6=16 \mathbf Y_1(s)$$ Then I put $\mathbf Y_2(s)$ in terms of $\mathbf Y_1(s)$ This gave me $$ \mathbf Y_2(s)=\frac{4s^3+6s^2+192s+32}{s^4-256}$$ I've tried to do partial fractions but keep getting the wrong answer, is there another way?","System of ODE's: Initial conditions : The equations i get once i do the laplace transform are: Then I put in terms of This gave me I've tried to do partial fractions but keep getting the wrong answer, is there another way?","y_1^""=16y_2 y_2^""=16y_1 y_1(0)=2,\space y_1^{'}(0)=12 y_2(0)=6, \space y_2^{'}(0)=4  s^2 \mathbf Y_1(s)-12s-2=16 \mathbf Y_2(s)  s^2 \mathbf Y_2(s)-4s-6=16 \mathbf Y_1(s) \mathbf Y_2(s) \mathbf Y_1(s)  \mathbf Y_2(s)=\frac{4s^3+6s^2+192s+32}{s^4-256}","['ordinary-differential-equations', 'laplace-transform']"
61,Solving a linear system of differential equations,Solving a linear system of differential equations,,"Given that $v_1 = \begin{bmatrix}1&1\end{bmatrix}$ and $v_2 = \begin{bmatrix}2 &1\end{bmatrix}$ are eigenvectors of the matrix $$ \begin{bmatrix}-1&-2\\1&-4\end{bmatrix} $$ which is a $2\times 2$ matrix. Find the solution to the linear system of differential equations \begin{align*} x' &= -x - 2y\\ y' &= x - 4y \end{align*} satisfying the initial conditions $x(0)=7$ and $y(0)=5$ . So I already found the eigenvalues, $-3$ and $-2$ and I know that you need to plug the eigenvalues into the matrix you get from doing $\det(It - A)$ but I'm not sure where to go from there in terms of making it into an equation?","Given that and are eigenvectors of the matrix which is a matrix. Find the solution to the linear system of differential equations satisfying the initial conditions and . So I already found the eigenvalues, and and I know that you need to plug the eigenvalues into the matrix you get from doing but I'm not sure where to go from there in terms of making it into an equation?","v_1 = \begin{bmatrix}1&1\end{bmatrix} v_2 = \begin{bmatrix}2 &1\end{bmatrix} 
\begin{bmatrix}-1&-2\\1&-4\end{bmatrix}
 2\times 2 \begin{align*}
x' &= -x - 2y\\
y' &= x - 4y
\end{align*} x(0)=7 y(0)=5 -3 -2 \det(It - A)","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
62,Global existence of IVP on $\mathbb R$,Global existence of IVP on,\mathbb R,"Suppose $f(t, x)\in C^1(\mathbb R^2)$ and satisfies $|f(t, x) |\leq 1+|x|$ for any $(t, x) \in\mathbb R^2$ . Then prove that the IVP $x'(t) =f(t, x)$ and $x(0)=0$ has a solution defined on whole $\mathbb R$ . I tried to find local solutions by restricting $f$ to rectangles. But it seems like it couldn't be guaranteed that the solution even exists on the whole interval. Another idea is to try to prove it by approximating $f$ uniformly by polynomials. I'm sure how to make it work. I'd like to get hints on how to solve this. Thanks in advance.",Suppose and satisfies for any . Then prove that the IVP and has a solution defined on whole . I tried to find local solutions by restricting to rectangles. But it seems like it couldn't be guaranteed that the solution even exists on the whole interval. Another idea is to try to prove it by approximating uniformly by polynomials. I'm sure how to make it work. I'd like to get hints on how to solve this. Thanks in advance.,"f(t, x)\in C^1(\mathbb R^2) |f(t, x) |\leq 1+|x| (t, x) \in\mathbb R^2 x'(t) =f(t, x) x(0)=0 \mathbb R f f","['ordinary-differential-equations', 'metric-spaces']"
63,solving Differential Equation $y''+x^2 y'+(2x+1)y=0$,solving Differential Equation,y''+x^2 y'+(2x+1)y=0,"I tried to solve this problem with power series method, but it became so complicated. like this: $na_{n-2}+n(n+1)a_{n+1}+a_{n-1}=0 $ for $n>=2$ And I cannot solve this a_{n} How can I get this? $y''+x^2 y'+(2x+1)y=0$","I tried to solve this problem with power series method, but it became so complicated. like this: for And I cannot solve this a_{n} How can I get this?",na_{n-2}+n(n+1)a_{n+1}+a_{n-1}=0  n>=2 y''+x^2 y'+(2x+1)y=0,['ordinary-differential-equations']
64,Is there any known closed-form solutions to $y''yx+y'y-(y')^2x=x^3$?,Is there any known closed-form solutions to ?,y''yx+y'y-(y')^2x=x^3,The title says it all... The equation $y''yx+y'y+(y')^2x=x^3$ belongs to a known DE class (listed on EqWorld). The equation $y''yx+y'y-(y')^2x=x^3$ appears to be way more difficult to solve... Is the analytic solution to this equation known at all?,The title says it all... The equation belongs to a known DE class (listed on EqWorld). The equation appears to be way more difficult to solve... Is the analytic solution to this equation known at all?,y''yx+y'y+(y')^2x=x^3 y''yx+y'y-(y')^2x=x^3,"['ordinary-differential-equations', 'closed-form']"
65,Functional equation for distribution function,Functional equation for distribution function,,"I have next functional equation for some distribution: $$\overline F_\xi^2(T) = \overline F_\xi(2T) \forall T>0$$ If suggest, that it differentiable, we can do something like this: $$2\overline F_\xi(T) \cdot \overline F'_\xi(T) = 2\overline F'_\xi(2T)$$ $$\overline F_\xi(T) \cdot \overline F'_\xi(T) = \overline F'_\xi(2T)$$ But how to solve this differential functional equation? where $$\overline F_\xi = 1 - F_\xi = P(\xi > T)$$ Also, I have $\mathbb E\xi=1.$","I have next functional equation for some distribution: If suggest, that it differentiable, we can do something like this: But how to solve this differential functional equation? where Also, I have",\overline F_\xi^2(T) = \overline F_\xi(2T) \forall T>0 2\overline F_\xi(T) \cdot \overline F'_\xi(T) = 2\overline F'_\xi(2T) \overline F_\xi(T) \cdot \overline F'_\xi(T) = \overline F'_\xi(2T) \overline F_\xi = 1 - F_\xi = P(\xi > T) \mathbb E\xi=1.,"['ordinary-differential-equations', 'probability-theory', 'probability-distributions']"
66,I need help finding the general solution to the differential equation $y''(t)+7y'(t)=-14$,I need help finding the general solution to the differential equation,y''(t)+7y'(t)=-14,"What I've tried: I have the inhomogeneous differential equation: $$y''(t)+7y'(t)=-14$$ I find the particular solution to be on the form $$kt$$ by inserting the particular solution in the equation $$(kt)''+7(kt)'=-14$$ and isolating for k, I get that: $$k=-2$$ and therefore the particular solution is $$y(t)-2t$$ I also need the general solution for the homogenous equation $$y''(t)+7y'(t)=0$$ by finding the roots of the characteristic polynomial $$z^2+7z=z(z+7)=0$$ $$z_1=0$$ $$z_2=-7$$ I get the general solution: $$c_1e^{0t}+c_2e^{-7t}=c_1+c_2e^{-7t}$$ Now, according to my textbook, the general solution of an inhomogeneous differential equation is given by $$y(t)=y_p(t)+y_{hom}(t)$$ Where $y_p(t)$ is the particular solution and $y_{hom}(t)$ is the general solution to the homogenous equation. Therefore I get the general solution to be $$y(t)=c_1+c_2e^{-7t}-2t$$ This is not consistent with Maple's result however Can anyone see where I've gone wrong?","What I've tried: I have the inhomogeneous differential equation: I find the particular solution to be on the form by inserting the particular solution in the equation and isolating for k, I get that: and therefore the particular solution is I also need the general solution for the homogenous equation by finding the roots of the characteristic polynomial I get the general solution: Now, according to my textbook, the general solution of an inhomogeneous differential equation is given by Where is the particular solution and is the general solution to the homogenous equation. Therefore I get the general solution to be This is not consistent with Maple's result however Can anyone see where I've gone wrong?",y''(t)+7y'(t)=-14 kt (kt)''+7(kt)'=-14 k=-2 y(t)-2t y''(t)+7y'(t)=0 z^2+7z=z(z+7)=0 z_1=0 z_2=-7 c_1e^{0t}+c_2e^{-7t}=c_1+c_2e^{-7t} y(t)=y_p(t)+y_{hom}(t) y_p(t) y_{hom}(t) y(t)=c_1+c_2e^{-7t}-2t,['ordinary-differential-equations']
67,Using Parseval's identity to show that $\frac{\pi^2}{8}=1+\frac{1}{3^2}+\frac{1}{5^2}+\frac{1}{7^2}+...$,Using Parseval's identity to show that,\frac{\pi^2}{8}=1+\frac{1}{3^2}+\frac{1}{5^2}+\frac{1}{7^2}+...,"By considering the Fourier sine series on the interval $[0,\pi]$ for $f(x)=1$ , show that $$\frac{\pi^2}{8}=1+\frac{1}{3^2}+\frac{1}{5^2}+\frac{1}{7^2}+...$$ I am having trouble computing the Fourier sine series, which will have the form $$Sf(x)=\frac{a_0}{2}+\sum_{k=1}^{\infty} a_k\cos\left(\frac{k\pi x}{L}\right), \ \ k\geq 1.$$ I have computed that $a_0=1$ and $a_n=0$ , using $$a_0=\frac{1}{L}\int_{-L}^{L} f(x) \ dx,  \ \ a_k=\frac{1}{L}\int_{-L}^{L} f(x)\cos\left(\frac{k\pi x}{L}\right) \ dx,$$ where $L$ denotes half a period. Once the correct Fourier series is calculated, showing the result using $$\Vert{f}\Vert^2_{w}=\sum_{j=1}^{\infty} A^2_j\Vert\phi_j\Vert^2_w,$$ should not be so hard. I have found the Fourier sine series, $$Sf(x)=\frac{4}{\pi}\sum_{j=1}^{\infty} \frac{\sin((2j-1)x)}{(2j-1)}.$$ But I having trouble proving the identity in the title. Using Parseval's identity provided above, I get that $$\Vert f\Vert^2_w=1, \ \ \sum_{j=1}^{\infty} A^2_j\Vert\phi_j\Vert^2_w=\frac{8}{\pi}\sum_{j=1}^{\infty}\frac{1}{(2j-1)^2},$$ which does not show the required result (out by a factor of $\pi$ ). Note: I took the weight function, $w$ , to be $1$ . Please help.","By considering the Fourier sine series on the interval for , show that I am having trouble computing the Fourier sine series, which will have the form I have computed that and , using where denotes half a period. Once the correct Fourier series is calculated, showing the result using should not be so hard. I have found the Fourier sine series, But I having trouble proving the identity in the title. Using Parseval's identity provided above, I get that which does not show the required result (out by a factor of ). Note: I took the weight function, , to be . Please help.","[0,\pi] f(x)=1 \frac{\pi^2}{8}=1+\frac{1}{3^2}+\frac{1}{5^2}+\frac{1}{7^2}+... Sf(x)=\frac{a_0}{2}+\sum_{k=1}^{\infty} a_k\cos\left(\frac{k\pi x}{L}\right), \ \ k\geq 1. a_0=1 a_n=0 a_0=\frac{1}{L}\int_{-L}^{L} f(x) \ dx,  \ \ a_k=\frac{1}{L}\int_{-L}^{L} f(x)\cos\left(\frac{k\pi x}{L}\right) \ dx, L \Vert{f}\Vert^2_{w}=\sum_{j=1}^{\infty} A^2_j\Vert\phi_j\Vert^2_w, Sf(x)=\frac{4}{\pi}\sum_{j=1}^{\infty} \frac{\sin((2j-1)x)}{(2j-1)}. \Vert f\Vert^2_w=1, \ \ \sum_{j=1}^{\infty} A^2_j\Vert\phi_j\Vert^2_w=\frac{8}{\pi}\sum_{j=1}^{\infty}\frac{1}{(2j-1)^2}, \pi w 1","['ordinary-differential-equations', 'proof-verification', 'fourier-analysis']"
68,"Solve $2x''\ln (x') =x' \,\, x(0)=1, x'(0)=e$",Solve,"2x''\ln (x') =x' \,\, x(0)=1, x'(0)=e","Solve $2x''\ln (x') =x'  \,\, x(0)=1, x'(0)=e$ . My attempt $p=x' \implies x''=pp'.$ $2pp'\ln p=p$ . $p(2p' \ln p-1)=0$ . We have that $p=0 \implies x(t)=c$ and also that $2p'\ln(p)=1 \implies \int \ln(p)dp=\frac 12 \int dx$ . $p\ln (p)-p=\frac 12x+c$ . In the picture you can See that I try to write in a different form",Solve . My attempt . . We have that and also that . . In the picture you can See that I try to write in a different form,"2x''\ln (x') =x'  \,\, x(0)=1, x'(0)=e p=x' \implies x''=pp'. 2pp'\ln p=p p(2p' \ln p-1)=0 p=0 \implies x(t)=c 2p'\ln(p)=1 \implies \int \ln(p)dp=\frac 12 \int dx p\ln (p)-p=\frac 12x+c","['ordinary-differential-equations', 'initial-value-problems']"
69,"Geometrical principle used in Fourier's paper ""Theory of Heat""","Geometrical principle used in Fourier's paper ""Theory of Heat""",,"Below follow an extract from Fourier's paper  ""THEORY OF HEAT"" in which he says: Consider the variable state of a solid whose heat is dispersed into air, maintained at the fixed temperature 0. Let $ω$ be an infinitely small part of the external surface, and $μ$ a point of $ω$ , through which a normal to the surface is drawn ; different points of this line have at the same instant different temperatures. Let $v$ be the actual temperature of the point $μ$ , taken at a definite instant, and $w$ the corresponding temperature of a point $ν$ of the solid taken on the normal, and distant from $μ$ , by an infinitely small quantity $α$ . Denote by $x, y, z$ the co-ordinates of the point $μ$ , and those of the point $ν$ by $x + δx, y + δy, z + δz$ ; let $f(x, y, z) =0$ be the known equation to the surface of the solid, and $v = Φ(x,y,z,t)$ ; the general equation which ought to give the value of $v$ as a function of the four variables $x,y,z,t$ . Differentiating the equation $f(x, y, z) = 0$ , we shall have: $$mdx+ndy+pdz=0$$ $m,n,p$ being functions of $x,y,z$ . (...) Now, it follows from the principles of geometry, that the co-ordinates $δx,δy,δz$ which fix the position of the point $ν$ of the normal relative to the point $μ$ satisfy the following conditions: $$pδx=mδz$$ and $$pδy=nδz$$ My question is about the geometrical principle he make use to derive the last expressions? You can find the paper here: page 115-116 https://www3.nd.edu/~powers/ame.20231/fourier1878.pdf","Below follow an extract from Fourier's paper  ""THEORY OF HEAT"" in which he says: Consider the variable state of a solid whose heat is dispersed into air, maintained at the fixed temperature 0. Let be an infinitely small part of the external surface, and a point of , through which a normal to the surface is drawn ; different points of this line have at the same instant different temperatures. Let be the actual temperature of the point , taken at a definite instant, and the corresponding temperature of a point of the solid taken on the normal, and distant from , by an infinitely small quantity . Denote by the co-ordinates of the point , and those of the point by ; let be the known equation to the surface of the solid, and ; the general equation which ought to give the value of as a function of the four variables . Differentiating the equation , we shall have: being functions of . (...) Now, it follows from the principles of geometry, that the co-ordinates which fix the position of the point of the normal relative to the point satisfy the following conditions: and My question is about the geometrical principle he make use to derive the last expressions? You can find the paper here: page 115-116 https://www3.nd.edu/~powers/ame.20231/fourier1878.pdf","ω μ ω v μ w ν μ α x, y, z μ ν x + δx, y + δy, z + δz f(x, y, z) =0 v = Φ(x,y,z,t) v x,y,z,t f(x, y, z) = 0 mdx+ndy+pdz=0 m,n,p x,y,z δx,δy,δz ν μ pδx=mδz pδy=nδz","['geometry', 'ordinary-differential-equations', 'fourier-analysis']"
70,Gradient systems and equilibrium points,Gradient systems and equilibrium points,,"I have been studying the following problem of gradient systems. Given a system: \begin{equation} \overset{\cdot}{x}=f(x), \ \ x=x(t) \in \mathbb{R}^3, \end{equation} where \begin{equation} f=-\nabla g, \ \ g\in C^1(\mathbb{R}^3), \end{equation} let's suppose that $x_0=0$ is an equilibrium point of this system. I have found references stating that when for example $x_0=0$ is also a local minimum of $g$ , then this means that $0$ is a stable point of equilibrium. This is shown by means of the Lyapunov function $V=g(x)-g(0)$ . I have found no reference whatsoever about maxima. If we had to study the same system, but $x_0=0$ is now a local (or global) maximum of $g$ , then is there a conclusion about the stability of $0$ ? I have tried defining the function $V=g(0)-g(x)$ , but for this function we have that $\overset{\cdot}{V} \geqslant 0$ and not $\overset{\cdot}{V} \leqslant  0$ . Is there even a way to define a Lyapunov function? Thanks in advance.","I have been studying the following problem of gradient systems. Given a system: where let's suppose that is an equilibrium point of this system. I have found references stating that when for example is also a local minimum of , then this means that is a stable point of equilibrium. This is shown by means of the Lyapunov function . I have found no reference whatsoever about maxima. If we had to study the same system, but is now a local (or global) maximum of , then is there a conclusion about the stability of ? I have tried defining the function , but for this function we have that and not . Is there even a way to define a Lyapunov function? Thanks in advance.","\begin{equation}
\overset{\cdot}{x}=f(x), \ \ x=x(t) \in \mathbb{R}^3,
\end{equation} \begin{equation}
f=-\nabla g, \ \ g\in C^1(\mathbb{R}^3),
\end{equation} x_0=0 x_0=0 g 0 V=g(x)-g(0) x_0=0 g 0 V=g(0)-g(x) \overset{\cdot}{V} \geqslant 0 \overset{\cdot}{V} \leqslant  0","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
71,Simple autonomous ODE perturbed by a constant coefficient,Simple autonomous ODE perturbed by a constant coefficient,,"Consider the ODEs \begin{equation} y'=f(y), y(0)=y_0 \end{equation} and \begin{equation} y'=a f(y), y(0)=y_0 \end{equation} where $f$ is Lipschitz. Let $y_1(t)$ ( $y_2(t)$ ) be the unique solution of the first (second) ODE. Can I conclude that $y_1(at)=y_2(t)$ ? My attempt . I should verify that \begin{align} \frac{d y_1(at)}{dt} = a f(y_1(at)) \end{align} that is true if and only if \begin{align} \frac{d y_1(at)}{d(at)} = f(y_1(at)) \end{align} that is true if and only if \begin{align} \frac{d y_1(x)}{d(x)} = f(y_1(x)) \end{align} which is true because $y_1$ is a solution of the first ODE. Is this correct?",Consider the ODEs and where is Lipschitz. Let ( ) be the unique solution of the first (second) ODE. Can I conclude that ? My attempt . I should verify that that is true if and only if that is true if and only if which is true because is a solution of the first ODE. Is this correct?,"\begin{equation}
y'=f(y), y(0)=y_0
\end{equation} \begin{equation}
y'=a f(y), y(0)=y_0
\end{equation} f y_1(t) y_2(t) y_1(at)=y_2(t) \begin{align}
\frac{d y_1(at)}{dt} = a f(y_1(at))
\end{align} \begin{align}
\frac{d y_1(at)}{d(at)} = f(y_1(at))
\end{align} \begin{align}
\frac{d y_1(x)}{d(x)} = f(y_1(x))
\end{align} y_1","['real-analysis', 'ordinary-differential-equations']"
72,How to solve a differential equation after integration.,How to solve a differential equation after integration.,,"The question is: Find the general solution of the differential equation $$\frac{dy}{dx}=2y+y^{3}$$ I have integrated by partial fractions to get to the following result: $$\frac{1}{2}ln(y)-\frac{1}{4}ln(y^{2}+2)=x+C_{1}$$ I now need to solve it for y. I have put this question into an online calculator so I know the answer is: $$y=-\frac{{{i\sqrt{2}} e^{{\frac{1}{2}}(C_{1}+4x)}}}{\sqrt{((e^{C_1+4x})-1)}}$$ and $$y=\frac{{{i\sqrt{2}} e^{{\frac{1}{2}}(C_{1}+4x)}}}{\sqrt{((e^{C_1+4x})-1)}}$$ I don't understand how to solve $$\frac{1}{2}ln(y)-\frac{1}{4}ln(y^{2}+2)=x+C_{1}$$ to get those answers... I tried using the quadratic formula and could only get it partially so I am either doing something accidentally wrong, or the quadratic formula is the wrong approach. If anyone could show me the steps of how to solve this it would be wonderful, this is driving me crazy.","The question is: Find the general solution of the differential equation I have integrated by partial fractions to get to the following result: I now need to solve it for y. I have put this question into an online calculator so I know the answer is: and I don't understand how to solve to get those answers... I tried using the quadratic formula and could only get it partially so I am either doing something accidentally wrong, or the quadratic formula is the wrong approach. If anyone could show me the steps of how to solve this it would be wonderful, this is driving me crazy.","\frac{dy}{dx}=2y+y^{3} \frac{1}{2}ln(y)-\frac{1}{4}ln(y^{2}+2)=x+C_{1} y=-\frac{{{i\sqrt{2}}
e^{{\frac{1}{2}}(C_{1}+4x)}}}{\sqrt{((e^{C_1+4x})-1)}} y=\frac{{{i\sqrt{2}}
e^{{\frac{1}{2}}(C_{1}+4x)}}}{\sqrt{((e^{C_1+4x})-1)}} \frac{1}{2}ln(y)-\frac{1}{4}ln(y^{2}+2)=x+C_{1}","['integration', 'ordinary-differential-equations']"
73,"Sine Fourier Series for $\min(x, 1 - x)$.",Sine Fourier Series for .,"\min(x, 1 - x)","I am currently taking a class on differential equations and only Analysis and Linear Algebra I and II were expected. In those subjects we never covered Fourier series. In an exercise I am now expected to solve $$ \sum_{k = 1}^{\infty} a_k \sin(\pi k x) = \min(x, 1 - x) $$ for the coefficients $a_k$ and for $x \in (0,1)$ . I'd be interested if anyone could share an approach, which someone with no previous knowledge of Fourier series could understand. for reference: the given differential equation is $u_t - u_{xx} = 0$ for $t > 0$ and $x \in (0,1)$ with $u(0,t) = u(1,t) = 0$ and $u(x,0) = \min(x, 1 - x)$ . for $x \in (0,1)$ EDIT 4 - Finally Correct \begin{align*}     a_k     & = 2 [1 - (-1)^k] \int_{0}^{\frac{1}{2}} x \sin(k \pi x) dx     = 2 [1 - (-1)^k]  \left[ \frac{\sin(k \pi x)}{(k \pi )^2} - \frac{x}{k \pi } \cos(k \pi x) \right]_{x = 0}^{\frac{1}{2}} \\     & = 2 [1 - (-1)^k] \left( \frac{\sin(\frac{1}{2} k \pi)}{(k \pi )^2} - \frac{1}{2 k \pi } \cos\left(\frac{1}{2} k \pi\right) - \left( \frac{\sin^2(0)}{(k \pi )^2} - \frac{0}{k \pi } \cos(0) \right) \right) \\     & = \frac{[1 - (-1)^k]}{k \pi} \left[ \frac{2 \sin(\frac{1}{2} k \pi)}{k \pi} - \cos\left(\frac{1}{2} k \pi\right) \right] \end{align*} Therefore we have $a_2k = 0$ . For $a_{2k + 1}$ we have \begin{align*}     a_{2k + 1}     & = \frac{1 - (-1)^{2k + 1}}{(2k + 1) \pi} \left( \frac{2 \sin(\frac{1}{2} (2k + 1) \pi)}{(2k + 1) \pi} - \cos\left(\frac{1}{2} (2k + 1) \pi\right) \right) \\     & = \frac{2}{(2k + 1) \pi} \left[ \frac{2 \overbrace{\sin\left(\frac{\pi}{2} + k \pi\right)}^{= \cos(k \pi)}}{(2k + 1) \pi} - \underbrace{\cos\left(\frac{\pi}{2} + k \pi\right)}_{= - \sin(k\pi) = 0.} \right] \\     & = \frac{4 (-1)^k}{\left((2k + 1)\pi \right)^2}. \end{align*}","I am currently taking a class on differential equations and only Analysis and Linear Algebra I and II were expected. In those subjects we never covered Fourier series. In an exercise I am now expected to solve for the coefficients and for . I'd be interested if anyone could share an approach, which someone with no previous knowledge of Fourier series could understand. for reference: the given differential equation is for and with and . for EDIT 4 - Finally Correct Therefore we have . For we have","
\sum_{k = 1}^{\infty} a_k \sin(\pi k x)
= \min(x, 1 - x)
 a_k x \in (0,1) u_t - u_{xx} = 0 t > 0 x \in (0,1) u(0,t) = u(1,t) = 0 u(x,0) = \min(x, 1 - x) x \in (0,1) \begin{align*}
    a_k
    & = 2 [1 - (-1)^k] \int_{0}^{\frac{1}{2}} x \sin(k \pi x) dx
    = 2 [1 - (-1)^k]  \left[ \frac{\sin(k \pi x)}{(k \pi )^2} - \frac{x}{k \pi } \cos(k \pi x) \right]_{x = 0}^{\frac{1}{2}} \\
    & = 2 [1 - (-1)^k] \left( \frac{\sin(\frac{1}{2} k \pi)}{(k \pi )^2} - \frac{1}{2 k \pi } \cos\left(\frac{1}{2} k \pi\right) - \left( \frac{\sin^2(0)}{(k \pi )^2} - \frac{0}{k \pi } \cos(0) \right) \right) \\
    & = \frac{[1 - (-1)^k]}{k \pi} \left[ \frac{2 \sin(\frac{1}{2} k \pi)}{k \pi} - \cos\left(\frac{1}{2} k \pi\right) \right]
\end{align*} a_2k = 0 a_{2k + 1} \begin{align*}
    a_{2k + 1}
    & = \frac{1 - (-1)^{2k + 1}}{(2k + 1) \pi} \left( \frac{2 \sin(\frac{1}{2} (2k + 1) \pi)}{(2k + 1) \pi} - \cos\left(\frac{1}{2} (2k + 1) \pi\right) \right) \\
    & = \frac{2}{(2k + 1) \pi} \left[ \frac{2 \overbrace{\sin\left(\frac{\pi}{2} + k \pi\right)}^{= \cos(k \pi)}}{(2k + 1) \pi} - \underbrace{\cos\left(\frac{\pi}{2} + k \pi\right)}_{= - \sin(k\pi) = 0.} \right] \\
    & = \frac{4 (-1)^k}{\left((2k + 1)\pi \right)^2}.
\end{align*}","['ordinary-differential-equations', 'fourier-analysis', 'fourier-series']"
74,second order differential equation $xy^n + 2\frac{dy}{dx} = 12x^2$,second order differential equation,xy^n + 2\frac{dy}{dx} = 12x^2,$xy^n + 2\frac{dy}{dx} = 12x^2$ Solve the second-order diﬀerential equation by making the substitution $u = \frac{dy}{dx} $ this is the question. I tried to solve it using the integration factor. But since it has a $y^n$ in i can't figure it out. They do say solve the second order differential equation does this mean that since it is second order that $y$ should equeal $2$ ?  Should i use the integration factor or use a different technique? I also dont get how substituting $u$ for $\frac{dy}{dx}$ would do anything.,Solve the second-order diﬀerential equation by making the substitution this is the question. I tried to solve it using the integration factor. But since it has a in i can't figure it out. They do say solve the second order differential equation does this mean that since it is second order that should equeal ?  Should i use the integration factor or use a different technique? I also dont get how substituting for would do anything.,xy^n + 2\frac{dy}{dx} = 12x^2 u = \frac{dy}{dx}  y^n y 2 u \frac{dy}{dx},"['calculus', 'ordinary-differential-equations']"
75,Solve the equation $y' + (xy)^2 = -2/(x^4)$ knowing that $y_1 = 1/(x^3)$ is the particular solution,Solve the equation  knowing that  is the particular solution,y' + (xy)^2 = -2/(x^4) y_1 = 1/(x^3),$$y' + (xy)^2 = -\frac2{x^4}\text{ with }y_1 = \frac1{x^3}$$ I have tried using various differential equations methods to solve but it appears to be very challenging.,I have tried using various differential equations methods to solve but it appears to be very challenging.,y' + (xy)^2 = -\frac2{x^4}\text{ with }y_1 = \frac1{x^3},['ordinary-differential-equations']
76,ODE featuring product of derivatives,ODE featuring product of derivatives,,"I consider an ODE of the form $$\frac{\mathrm{d}U(u)}{\mathrm{d}u} \frac{\mathrm{d}V(v)}{\mathrm{d}v} = f(u, v).  $$ The function $f$ is known, and the goal is to find $U$ and $V$ , assuming that any initial or boundary conditions which turn out to be necessary have been specified. This equation arises when trying to find the Weyl rescaling upon coordinates $u$ and $v$ leading to the 2D line element $$ ds^2 = -f^{-1}(u, v)  du dv.$$ Despite its apparent simplicitly I find I do not know how to solve or even classify this system: it is not a PDE since $U$ and $V$ are respectively independent of $v$ and $u$ ; it does not obviously separate into a pair of coupled ODEs, etc. Anyway, I would like to solve this system, presumably numerically. Can anyone point me in the right direction? Edit: It has been pointed out that the form given implies $f(u,v) = g(u) h(v)$ . Thus the problem separates into that of solving two coupled ODEs: $\frac{dU}{du} = -\lambda g(u)$ and $\frac{dV}{dv} = -\lambda h(v)$ . Therefore, the remaining substance of the question is to find $g$ and $h$ .","I consider an ODE of the form The function is known, and the goal is to find and , assuming that any initial or boundary conditions which turn out to be necessary have been specified. This equation arises when trying to find the Weyl rescaling upon coordinates and leading to the 2D line element Despite its apparent simplicitly I find I do not know how to solve or even classify this system: it is not a PDE since and are respectively independent of and ; it does not obviously separate into a pair of coupled ODEs, etc. Anyway, I would like to solve this system, presumably numerically. Can anyone point me in the right direction? Edit: It has been pointed out that the form given implies . Thus the problem separates into that of solving two coupled ODEs: and . Therefore, the remaining substance of the question is to find and .","\frac{\mathrm{d}U(u)}{\mathrm{d}u} \frac{\mathrm{d}V(v)}{\mathrm{d}v} = f(u, v).   f U V u v  ds^2 = -f^{-1}(u, v)  du dv. U V v u f(u,v) = g(u) h(v) \frac{dU}{du} = -\lambda g(u) \frac{dV}{dv} = -\lambda h(v) g h","['ordinary-differential-equations', 'differential-geometry']"
77,Classify the points at 0 and $\infty$ of $x^7~\frac{d^4y}{dx^4}=y'$,Classify the points at 0 and  of,\infty x^7~\frac{d^4y}{dx^4}=y',"$$\displaystyle x^7~\frac{d^4y}{dx^4}=y'$$ I know that 0 is an irregular singular point. At $\infty$ , I'm using the change of variable $x = \frac{1}{t}$ and I don't understand how to differentiate and do the substitution.","I know that 0 is an irregular singular point. At , I'm using the change of variable and I don't understand how to differentiate and do the substitution.",\displaystyle x^7~\frac{d^4y}{dx^4}=y' \infty x = \frac{1}{t},"['ordinary-differential-equations', 'singularity']"
78,Is there a function whose derivative is proportional to that function squared?,Is there a function whose derivative is proportional to that function squared?,,"I am working on a Physics problem. The problem involves a ball of mass m falling through the air, with the drag force at a time t being equal to $bv^3(t)$ . The problem wants me to find and solve a differential equation for the instantaneous velocity at a given time. I did some research, and I found an article on the Oregon State University website which explains how to solve the equation assuming that drag is directly proportional to velocity. I tried solving it the same way, and I eventually end up at a point where an auxiliary function $p'(t) = -2\frac{b^2}{m^2}p^2(t)$ . This is pretty much where the problem dead-ended for me, though. My two questions are: 1) Is there an elementary function / combination of elementary functions such that $f'(t)\propto f^2(t)$ ? 2) How do I actually solve this equation for $v(t)$ ?","I am working on a Physics problem. The problem involves a ball of mass m falling through the air, with the drag force at a time t being equal to . The problem wants me to find and solve a differential equation for the instantaneous velocity at a given time. I did some research, and I found an article on the Oregon State University website which explains how to solve the equation assuming that drag is directly proportional to velocity. I tried solving it the same way, and I eventually end up at a point where an auxiliary function . This is pretty much where the problem dead-ended for me, though. My two questions are: 1) Is there an elementary function / combination of elementary functions such that ? 2) How do I actually solve this equation for ?",bv^3(t) p'(t) = -2\frac{b^2}{m^2}p^2(t) f'(t)\propto f^2(t) v(t),['ordinary-differential-equations']
79,Phase Portraits in 3D differential equations,Phase Portraits in 3D differential equations,,"We know that when we have two equations and two variables, there are certain rules that make the phase portrait a saddle, node, etc... based on whether eigenvalues are positive or negative. For example, if one is positive and one is negative, the phase portrait is saddle. If both are negative, it is a sink node, etc... I was wondering if there are sets of similar rules with three variables instead of two. This would mean three eigenvalues and I was wondering if there are ways to construct rules with these three eigenvalues. Any input? Thanks.","We know that when we have two equations and two variables, there are certain rules that make the phase portrait a saddle, node, etc... based on whether eigenvalues are positive or negative. For example, if one is positive and one is negative, the phase portrait is saddle. If both are negative, it is a sink node, etc... I was wondering if there are sets of similar rules with three variables instead of two. This would mean three eigenvalues and I was wondering if there are ways to construct rules with these three eigenvalues. Any input? Thanks.",,"['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
80,Poincare type inequalities,Poincare type inequalities,,"I want to prove if following inequality holds: $$\int_0^1(f')^2\ dx\geq f^2(1)-f^2(0)$$ where $f$ is a function in $H^1([0,1])$ satisfying $\int_0^1f \ dx=0$ . It is actually a one dimensional case of following inequality $$\int_{D}|\nabla f|^2\geq \int_{\partial D}f^2$$ where $D$ is the 2-dimensional unit disc and $f$ is a function in sobolev space $H^1(D)$ with mean value zero, i.e., $\int_{D}f=0$ . First I'd like to clarify that the above inequality might be not ture, but I have a feeling it's true and I couldn't find a counterexample. Any example and explanation will be helpful.","I want to prove if following inequality holds: where is a function in satisfying . It is actually a one dimensional case of following inequality where is the 2-dimensional unit disc and is a function in sobolev space with mean value zero, i.e., . First I'd like to clarify that the above inequality might be not ture, but I have a feeling it's true and I couldn't find a counterexample. Any example and explanation will be helpful.","\int_0^1(f')^2\ dx\geq f^2(1)-f^2(0) f H^1([0,1]) \int_0^1f \ dx=0 \int_{D}|\nabla f|^2\geq \int_{\partial D}f^2 D f H^1(D) \int_{D}f=0","['calculus', 'real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'functional-inequalities']"
81,Explain why the hypothesis of Picard's Theorem hold in this case,Explain why the hypothesis of Picard's Theorem hold in this case,,"Using Picard's Theorem, show that $$\frac{dy}{dx}=y^2 \qquad \quad y(0)=1$$ has a unique solution in the rectangle $R=[0,0.5] \times [0,2]$ . Picard's Theorem: The differential equation $$\frac{dy}{dx}=f(x,y) \qquad \quad y(a)=b$$ has a unique solution in the rectangle $R=\{(x,y):|x-a|≤h, \; |y-b|≤k \}$ if: (i) $f$ is continuous in $R$ (ii) $M := \max _{(x,y) \in R} |f(x,y)|$ exists and satisfies $Mh≤k$ ; and (iii) $f$ is Lipshitz in $R$ I have shown that the differential equation satisfies conditions (i) and (iii), and also that $M$ exists. Clearly, $M=2^2=4$ in this case, $h=0.5$ and $k=1$ , but it is not true that $$4 \times 0.5 ≤ 1$$ Have I misunderstood something about Picard's Theorem, or is there something wrong with this question?","Using Picard's Theorem, show that has a unique solution in the rectangle . Picard's Theorem: The differential equation has a unique solution in the rectangle if: (i) is continuous in (ii) exists and satisfies ; and (iii) is Lipshitz in I have shown that the differential equation satisfies conditions (i) and (iii), and also that exists. Clearly, in this case, and , but it is not true that Have I misunderstood something about Picard's Theorem, or is there something wrong with this question?","\frac{dy}{dx}=y^2 \qquad \quad y(0)=1 R=[0,0.5] \times [0,2] \frac{dy}{dx}=f(x,y) \qquad \quad y(a)=b R=\{(x,y):|x-a|≤h, \; |y-b|≤k \} f R M := \max _{(x,y) \in R} |f(x,y)| Mh≤k f R M M=2^2=4 h=0.5 k=1 4 \times 0.5 ≤ 1","['ordinary-differential-equations', 'initial-value-problems']"
82,A way of solving this PDE? (Other than method of characteristics),A way of solving this PDE? (Other than method of characteristics),,"I've been trying to work with this coupled PDE for some time. Here, $P = P(k,t)$ $$\frac{\partial{P}}{\partial t}-\frac{\sin(k)}{t}\frac{\partial{P}}{\partial k}=\left(\cos(k)-1\right){P}$$ To solve this PDE, I looked at the ODE $$\frac{dP}{dt}=\left(\cos(k)-1\right)P$$ We know that $$\frac{dP}{dt}=\frac{\partial P}{\partial t} + \frac{dk}{dt}\frac{\partial P}{\partial k}$$ So we can see that whenever $$\frac{dk}{dt}=\frac{-\sin(k)}{t}$$ the solution to the ODE gives us a solution to the PDE. But is there another way to get a more general solution than this? Thanks for the help :)","I've been trying to work with this coupled PDE for some time. Here, To solve this PDE, I looked at the ODE We know that So we can see that whenever the solution to the ODE gives us a solution to the PDE. But is there another way to get a more general solution than this? Thanks for the help :)","P = P(k,t) \frac{\partial{P}}{\partial t}-\frac{\sin(k)}{t}\frac{\partial{P}}{\partial k}=\left(\cos(k)-1\right){P} \frac{dP}{dt}=\left(\cos(k)-1\right)P \frac{dP}{dt}=\frac{\partial P}{\partial t} + \frac{dk}{dt}\frac{\partial P}{\partial k} \frac{dk}{dt}=\frac{-\sin(k)}{t}","['ordinary-differential-equations', 'partial-differential-equations']"
83,"What is the solution for the differential equation $\frac{dy}{dx} = -\frac{y}{x}, y(-2) = -2$",What is the solution for the differential equation,"\frac{dy}{dx} = -\frac{y}{x}, y(-2) = -2","Given $\dfrac{dy}{dx} = -\dfrac{y}{x}, y(-2) = -2$ I wish to solve for $y(x)$ . Separating the variables, I have, $\dfrac{1}{y} dy = -\dfrac{1}{x} dx$ So $\ln(|y|) = -\ln(|x|) + c$ So $|y| = \exp({-\ln(|x|)+ c)}) = K \exp({\ln(\dfrac{1}{|x|})}) = K\dfrac{1}{|x|}$ At this step, I am unsure how to deal with the absolute value sign. Does anyone have idea as to how to proceed?","Given I wish to solve for . Separating the variables, I have, So So At this step, I am unsure how to deal with the absolute value sign. Does anyone have idea as to how to proceed?","\dfrac{dy}{dx} = -\dfrac{y}{x}, y(-2) = -2 y(x) \dfrac{1}{y} dy = -\dfrac{1}{x} dx \ln(|y|) = -\ln(|x|) + c |y| = \exp({-\ln(|x|)+ c)}) = K \exp({\ln(\dfrac{1}{|x|})}) = K\dfrac{1}{|x|}","['ordinary-differential-equations', 'analysis', 'mathematical-modeling']"
84,Finding Inverse Laplace Transform,Finding Inverse Laplace Transform,,"Calculate the inverse Laplace transform of: $$\frac{1}{s\cdot(\sqrt{s}+1)} \cdot e^{-\sqrt{s} \cdot x}$$ My attempt at the solution was to break down the fraction with partial fraction decomposition as follows: $$\frac{1}{s \cdot (\sqrt{s}+1)} = \frac1s-\frac{1}{\sqrt{s}}+\frac{1}{1+\sqrt{s}}$$ Then the first part can be easily computed from the table or by using some software: $$\mathcal{L}^{-1}( s^{-1} \cdot e^{-\sqrt{s} \cdot x} )= 1-erf(\frac{x}{2\sqrt{t}})$$ However the second part is not at all trivial, as I was unable to find any coherent answer to the problem: $$\mathcal{L}^{-1}( (-\frac{1}{\sqrt{s}}+\frac{1}{1+\sqrt{s}}) \cdot e^{-\sqrt{s} \cdot x} )$$ Could someone, please, guide towards the correct answer? I have tried using computer algebra systems such Mathematica, but nothing seems to work. ***Treat x as a constant with x>0.","Calculate the inverse Laplace transform of: My attempt at the solution was to break down the fraction with partial fraction decomposition as follows: Then the first part can be easily computed from the table or by using some software: However the second part is not at all trivial, as I was unable to find any coherent answer to the problem: Could someone, please, guide towards the correct answer? I have tried using computer algebra systems such Mathematica, but nothing seems to work. ***Treat x as a constant with x>0.",\frac{1}{s\cdot(\sqrt{s}+1)} \cdot e^{-\sqrt{s} \cdot x} \frac{1}{s \cdot (\sqrt{s}+1)} = \frac1s-\frac{1}{\sqrt{s}}+\frac{1}{1+\sqrt{s}} \mathcal{L}^{-1}( s^{-1} \cdot e^{-\sqrt{s} \cdot x} )= 1-erf(\frac{x}{2\sqrt{t}}) \mathcal{L}^{-1}( (-\frac{1}{\sqrt{s}}+\frac{1}{1+\sqrt{s}}) \cdot e^{-\sqrt{s} \cdot x} ),"['ordinary-differential-equations', 'laplace-transform']"
85,Sine/Cosine Subtraction Formulas,Sine/Cosine Subtraction Formulas,,"The differential equation $\frac{d^2 y}{dt^2}+ \omega^2 y=0$ has the general solution $y = A\cos (\omega t)+ B\sin (\omega t)$ . Also given are the initial values: $y(a) = A, y'(a) = B$ . I tried: $$y(a) = A\cos(\omega a) + B\sin(\omega a )= A$$ $$y'(a) = −\omega A\sin(\omega a) + \omega B\cos(\omega a)= B$$ And then substituted $A$ and $B$ into the general solution: $$y = (A\cos(\omega a) + B\sin(\omega a))\cosωt + \omega (−A\sin(\omega a) + B\cos(\omega a))\sin(\omega t)$$ From here I can't get it to work. I want to simplify and express y with the subtraction formulas.  The answer is $y = A\cos(\omega (t−a)) + \frac{B}{\omega}\sin(\omega (t−a))$ I know how the formulas work, But I can't figure out how $\frac{B}{\omega}$ got there. Please help, I'm stuck.","The differential equation has the general solution . Also given are the initial values: . I tried: And then substituted and into the general solution: From here I can't get it to work. I want to simplify and express y with the subtraction formulas.  The answer is I know how the formulas work, But I can't figure out how got there. Please help, I'm stuck.","\frac{d^2 y}{dt^2}+ \omega^2 y=0 y = A\cos (\omega t)+ B\sin (\omega t) y(a) = A, y'(a) = B y(a) = A\cos(\omega a) + B\sin(\omega a )= A y'(a) = −\omega A\sin(\omega a) + \omega B\cos(\omega a)= B A B y = (A\cos(\omega a) + B\sin(\omega a))\cosωt + \omega (−A\sin(\omega a) + B\cos(\omega a))\sin(\omega t) y = A\cos(\omega (t−a)) + \frac{B}{\omega}\sin(\omega (t−a)) \frac{B}{\omega}","['ordinary-differential-equations', 'trigonometry']"
86,what is the distinction between differential equations and differential forms?,what is the distinction between differential equations and differential forms?,,"Say I have these two things: $$ A(x,y)+B(x,y)\frac{dy}{dx}=0 $$ $$A(x,y)dx+B(x,y)dy=0$$ what is the difference? I always thought they were just different ways of expressing the same things, but recently I was told it wasn't quite like that. How come?","Say I have these two things: what is the difference? I always thought they were just different ways of expressing the same things, but recently I was told it wasn't quite like that. How come?","
A(x,y)+B(x,y)\frac{dy}{dx}=0
 A(x,y)dx+B(x,y)dy=0",['ordinary-differential-equations']
87,Taylor Series Method for a DE,Taylor Series Method for a DE,,"I'm going through a practice exam which has solutions and I'm a bit confused about this question: Write down the third order Taylor series method for the differential equation $y'(x)=-y+x+1, \ \ 0\le x\le1, \ \ y(0)=1.$ Compute three steps using $h=0.1$ . The solutions computes derivatives as $ \displaystyle f(x,y)=-y+x+1, \frac{d}{dx}f(x,y)=y-x,\frac{d^2}{dx^2}f(x,y)=-y+x.$ I don't understand how on earth this was achieved. Is there a different type of differentiation at play here? When I did it I obtained $ \displaystyle f(x,y)=-y+x+1, \frac{d}{dx}f(x,y)=-y'+1,\frac{d^2}{dx^2}f(x,y)=-y''$ . What am I missing here?",I'm going through a practice exam which has solutions and I'm a bit confused about this question: Write down the third order Taylor series method for the differential equation Compute three steps using . The solutions computes derivatives as I don't understand how on earth this was achieved. Is there a different type of differentiation at play here? When I did it I obtained . What am I missing here?,"y'(x)=-y+x+1, \ \ 0\le x\le1, \ \ y(0)=1. h=0.1  \displaystyle f(x,y)=-y+x+1, \frac{d}{dx}f(x,y)=y-x,\frac{d^2}{dx^2}f(x,y)=-y+x.  \displaystyle f(x,y)=-y+x+1, \frac{d}{dx}f(x,y)=-y'+1,\frac{d^2}{dx^2}f(x,y)=-y''","['ordinary-differential-equations', 'numerical-methods', 'taylor-expansion']"
88,"Solution to $u_x + y u_y = -u$ with $u(0, y) = \cos y$ [duplicate]",Solution to  with  [duplicate],"u_x + y u_y = -u u(0, y) = \cos y","This question already has answers here : Transport equation $u_t + xu_x + u = 0$ with $u(x_0, 0) = \cos(x_0)$ (3 answers) Closed 5 years ago . I am studying for my PDEs test and I want to make sure I can solve this type of equations. I used the method of characteristics. $$\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{b}{a} = y$$ Integrating I get $$C = ye^{-x}$$ where $C$ is a constant. I can make the substitutions $$\xi = x$$ $$\eta = ye^{-x}$$ $$u(x,y) = w(\xi,\eta)$$ Substituting in the PDE, in terms of $w, \eta, \xi$ it becomes $$\frac{\mathrm{d}w}{\mathrm{d}\xi} + w = 0$$ I used the Integrating factor method to solve this ODE, which gives $$w = f(\eta) e ^{-\xi}$$ If we substitute back to $u, x, y$ this becomes $$w(\xi,\eta)=u(x,y)=f(ye^{-x})e^{-x}$$ The initial conditions give $$u(0,y)=\cos y =f(y)$$ Thus, the final solution is $$u(x,y) = \cos (ye^{-x})e^{-x}$$ Is this correct? Thanks in advance.","This question already has answers here : Transport equation $u_t + xu_x + u = 0$ with $u(x_0, 0) = \cos(x_0)$ (3 answers) Closed 5 years ago . I am studying for my PDEs test and I want to make sure I can solve this type of equations. I used the method of characteristics. Integrating I get where is a constant. I can make the substitutions Substituting in the PDE, in terms of it becomes I used the Integrating factor method to solve this ODE, which gives If we substitute back to this becomes The initial conditions give Thus, the final solution is Is this correct? Thanks in advance.","\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{b}{a} = y C = ye^{-x} C \xi = x \eta = ye^{-x} u(x,y) = w(\xi,\eta) w, \eta, \xi \frac{\mathrm{d}w}{\mathrm{d}\xi} + w = 0 w = f(\eta) e ^{-\xi} u, x, y w(\xi,\eta)=u(x,y)=f(ye^{-x})e^{-x} u(0,y)=\cos y =f(y) u(x,y) = \cos (ye^{-x})e^{-x}","['ordinary-differential-equations', 'partial-differential-equations']"
89,Discussing Whether Solutions to ODE Exist for all $t \in \mathbb{R}$,Discussing Whether Solutions to ODE Exist for all,t \in \mathbb{R},"I am asked whether any solution for $y''+y'+y+y^3 =0$ exists for all $t \in \mathbb{R}$ . Here is what I am thinking: First, introduce a change of variables $z_1 = y$ , $z_2 = y'$ so that the equation given becomes the 1st order system: \begin{equation} \pmatrix{z'_1\\z_2'}=\pmatrix{0&1\\-1&-1}\pmatrix{z_1\\ z_2} +\pmatrix{0\\-z_1^3} \end{equation} After computing the Jacobian, we get that: $\frac{\partial F}{\partial y} = \pmatrix{0&1\\-1-3z_1^2&-1}$ Now, we need to check whether $F(z_1,z_2)=\pmatrix{0&1\\-1&-1}\pmatrix{z_1\\ z_2} +\pmatrix{0\\-z_1^3}$ is Lipschitz. Clearly, $F$ is continuous (component wise). Note then if we compute the operator norm of the Jacobian, we get (using the sup norm): $\left\lVert\frac{\partial F}{\partial y}\right\rVert_\infty = \max$ { $|1|$ , $|1+3z_1^2|+|1|$ } Since $3x^2 +1 \neq 0$ for $x \in \mathbb{R}$ , then the above expression is just \begin{equation} 1 +|3z_1^2 +1| \leq 2 +3|z_1|^2 \end{equation} Because of the $z_1$ term above, then the Jacobian is not bounded. Thus, $F$ cannot be (globally) Lipschitz. If we did assume that $z_1 =  y$ was bounded beforehand, then it might work. However, we're not given any further assumptions. I am aware that there are alternate methods to solving this problem, maybe finding a potential function $V(y)$ and checking if $V'(y) \leq 0$ . However, I don't want to go that route, as we can only use material we've covered so far. We just got to linear systems last lecture. Does this work? I feel there's something I am overlooking.","I am asked whether any solution for exists for all . Here is what I am thinking: First, introduce a change of variables , so that the equation given becomes the 1st order system: After computing the Jacobian, we get that: Now, we need to check whether is Lipschitz. Clearly, is continuous (component wise). Note then if we compute the operator norm of the Jacobian, we get (using the sup norm): { , } Since for , then the above expression is just Because of the term above, then the Jacobian is not bounded. Thus, cannot be (globally) Lipschitz. If we did assume that was bounded beforehand, then it might work. However, we're not given any further assumptions. I am aware that there are alternate methods to solving this problem, maybe finding a potential function and checking if . However, I don't want to go that route, as we can only use material we've covered so far. We just got to linear systems last lecture. Does this work? I feel there's something I am overlooking.","y''+y'+y+y^3 =0 t \in \mathbb{R} z_1 = y z_2 = y' \begin{equation}
\pmatrix{z'_1\\z_2'}=\pmatrix{0&1\\-1&-1}\pmatrix{z_1\\ z_2} +\pmatrix{0\\-z_1^3}
\end{equation} \frac{\partial F}{\partial y} = \pmatrix{0&1\\-1-3z_1^2&-1} F(z_1,z_2)=\pmatrix{0&1\\-1&-1}\pmatrix{z_1\\ z_2} +\pmatrix{0\\-z_1^3} F \left\lVert\frac{\partial F}{\partial y}\right\rVert_\infty = \max |1| |1+3z_1^2|+|1| 3x^2 +1 \neq 0 x \in \mathbb{R} \begin{equation}
1 +|3z_1^2 +1| \leq 2 +3|z_1|^2
\end{equation} z_1 F z_1 =  y V(y) V'(y) \leq 0","['ordinary-differential-equations', 'dynamical-systems', 'lipschitz-functions']"
90,Taylor Expansion of a Differential,Taylor Expansion of a Differential,,"How can the differential of an arbitrary function $f(t,S_t)$, through a second-order Taylor Expansion, become: $df = \frac{∂f}{∂t}​dt+ \frac{∂f}{∂S_t}​dS_t​ + \frac{1}{2}​\frac{∂²f}{∂S^2_t}(dSt_t)^2$ What are the steps to arrive at this equation? Is the function expanded first and differentiated next? Or are ""f,x, and a"" of an ordinary TE replaced before expansion?","How can the differential of an arbitrary function $f(t,S_t)$, through a second-order Taylor Expansion, become: $df = \frac{∂f}{∂t}​dt+ \frac{∂f}{∂S_t}​dS_t​ + \frac{1}{2}​\frac{∂²f}{∂S^2_t}(dSt_t)^2$ What are the steps to arrive at this equation? Is the function expanded first and differentiated next? Or are ""f,x, and a"" of an ordinary TE replaced before expansion?",,"['ordinary-differential-equations', 'taylor-expansion']"
91,Log transforming an ODE,Log transforming an ODE,,"I'm doing some numerical simulations of an exponential growth like system which, for simplicity, has the form: $$ \frac{dx}{dt}= ax + bxy \quad\quad \frac{dy}{dt}= cy + dxy  $$ For some parameter values i get instability in the simulation though I remember reading a paper which used log transformations to prevent this. Any ideas on how I could do this or how to rewrite the equations as: $$ \frac{d log(x)}{dt}= \ldots \quad\quad \frac{d log(y)}{dt}= \ldots $$","I'm doing some numerical simulations of an exponential growth like system which, for simplicity, has the form: $$ \frac{dx}{dt}= ax + bxy \quad\quad \frac{dy}{dt}= cy + dxy  $$ For some parameter values i get instability in the simulation though I remember reading a paper which used log transformations to prevent this. Any ideas on how I could do this or how to rewrite the equations as: $$ \frac{d log(x)}{dt}= \ldots \quad\quad \frac{d log(y)}{dt}= \ldots $$",,"['ordinary-differential-equations', 'numerical-methods', 'systems-of-equations', 'simulation']"
92,Differential equation $(ax+by+c)dx+(ex+fy+g)dy=0$,Differential equation,(ax+by+c)dx+(ex+fy+g)dy=0,"On Wikipedia:Homogeneous differential equation , it is said that a first order differential equation of the form ($a$, $b$, $c$, $e$, $f$, $g$ are all constants) $${\displaystyle (ax+by+c)dx+(ex+fy+g)dy=0\,}$$ where $af \not= be$ can be transformed into a homogeneous type. What if $af = be$? Is there a general method for solving this kind of equations?","On Wikipedia:Homogeneous differential equation , it is said that a first order differential equation of the form ($a$, $b$, $c$, $e$, $f$, $g$ are all constants) $${\displaystyle (ax+by+c)dx+(ex+fy+g)dy=0\,}$$ where $af \not= be$ can be transformed into a homogeneous type. What if $af = be$? Is there a general method for solving this kind of equations?",,['ordinary-differential-equations']
93,Obtaining the polar form of the planar system?,Obtaining the polar form of the planar system?,,"Consider the two variables $x_{1},x_{2}$ which are functions of time $t$, we refer $\dot{x_{1}}$ as the time derivative of $x_{1}$. Let $\alpha$ be a constant parameter, and consider the following system of equations: \begin{align*} \dot{x_{1}} &= \alpha x_{1} - x_{2} - x_{1}(x_{1}^2 + x_{2}^2)\\ \dot{x_{2}} &= x_{1} + \alpha x_{2} - x_{2}(x_{1}^2 + x_{2}^2). \end{align*} Now we transform the above system to polar $(\rho,\theta)$ coordinates. I think after the transformation the above equations lead to the following: \begin{align*} \dot{\rho} &= \rho(\alpha - \rho^2)\\ \dot{\theta} &= 1. \end{align*} Where $x_{1} = \rho \cos(\theta)$ and $x_{2} = \rho \sin(\theta)$. If I substitute then I am thinking about the partial derivatives involved and I think which variable I should differentiate with respect to the time? I can observe that the $x_{1}^2 +x_{2}^2 = 1$ which could simplify a lot but how should I deal with $\dot{x_{1}}, \dot{x_{2}}$ which can help me in getting the polar form of the above system?","Consider the two variables $x_{1},x_{2}$ which are functions of time $t$, we refer $\dot{x_{1}}$ as the time derivative of $x_{1}$. Let $\alpha$ be a constant parameter, and consider the following system of equations: \begin{align*} \dot{x_{1}} &= \alpha x_{1} - x_{2} - x_{1}(x_{1}^2 + x_{2}^2)\\ \dot{x_{2}} &= x_{1} + \alpha x_{2} - x_{2}(x_{1}^2 + x_{2}^2). \end{align*} Now we transform the above system to polar $(\rho,\theta)$ coordinates. I think after the transformation the above equations lead to the following: \begin{align*} \dot{\rho} &= \rho(\alpha - \rho^2)\\ \dot{\theta} &= 1. \end{align*} Where $x_{1} = \rho \cos(\theta)$ and $x_{2} = \rho \sin(\theta)$. If I substitute then I am thinking about the partial derivatives involved and I think which variable I should differentiate with respect to the time? I can observe that the $x_{1}^2 +x_{2}^2 = 1$ which could simplify a lot but how should I deal with $\dot{x_{1}}, \dot{x_{2}}$ which can help me in getting the polar form of the above system?",,"['ordinary-differential-equations', 'dynamical-systems', 'transformation', 'polar-coordinates']"
94,Definition of ODE and flow on manifold.,Definition of ODE and flow on manifold.,,"I have the following confusion in my recent lectures in Riemannian geometry. The idea is to define the notion of Lie derivative using the exponential map. In my lecture notes is the following: Let $ M $ be a smooth manifold. $ X $ be a vector field on $M$. Let $ F: V \rightarrow U \subset M $ be a local coordinate chart of $ M $ where $ V \subseteq \mathbb{R}^{n} $ is an open set in $ \mathbb{R}^{n} $. If $ u = (u_1,...u_n) \in V $ is the coordinate in $V$. Then we consider the following system of ordinary differential equations: $$\begin{cases} du/dt = X(u)\\ \\ u(0)= F^{-1}(p) \end{cases}$$ for $ p \in M $. The first question I have is what does the above equality means. More precisely, we think of tangent vectors in $ T_{p}M $ as elements that are behaves like directional derivatives. Hence $ X $ should admits the expression $ X = \sum_{j=1}^{n} X_{j} \frac{\partial_{}}{\partial{u_j}} $ which takes values in $ M $ and where $ ( \frac{\partial}{\partial{u_j}} )_{p}$ is defined such that  $$ (\frac{\partial }{\partial{u_j}} )_p f = \partial_{e_j}( f \circ F \circ F^{-1} \circ p ) $$ where $f$ is a smooth function on $M$ defined locally near $p$. In the lecture, however, it was just said that we should interpret the above equality as $$ du_j/dt = X_j .$$ My first question is then, is this a consequence of the above discussion? Or simply by definition? After all, $X$ is not meant to take values in $\mathbb{R}^{n}$. We then defined the solution of the above  $$e^{tX}(p) = u(t,p)$$ corresponding to the initial position $F^{-1}(p)$. And somehow this becomes an element of $ M $. But following the above discussion $u(t,p)$ is clearly in $ \mathbb{R}^{n} $! The only reasonable explanation I can think of is that $e^{tX}(p) = F( u(t,p) ) $ would be correct notation. This is my second question. Could somebody explain what the actual definition is? Thanks!","I have the following confusion in my recent lectures in Riemannian geometry. The idea is to define the notion of Lie derivative using the exponential map. In my lecture notes is the following: Let $ M $ be a smooth manifold. $ X $ be a vector field on $M$. Let $ F: V \rightarrow U \subset M $ be a local coordinate chart of $ M $ where $ V \subseteq \mathbb{R}^{n} $ is an open set in $ \mathbb{R}^{n} $. If $ u = (u_1,...u_n) \in V $ is the coordinate in $V$. Then we consider the following system of ordinary differential equations: $$\begin{cases} du/dt = X(u)\\ \\ u(0)= F^{-1}(p) \end{cases}$$ for $ p \in M $. The first question I have is what does the above equality means. More precisely, we think of tangent vectors in $ T_{p}M $ as elements that are behaves like directional derivatives. Hence $ X $ should admits the expression $ X = \sum_{j=1}^{n} X_{j} \frac{\partial_{}}{\partial{u_j}} $ which takes values in $ M $ and where $ ( \frac{\partial}{\partial{u_j}} )_{p}$ is defined such that  $$ (\frac{\partial }{\partial{u_j}} )_p f = \partial_{e_j}( f \circ F \circ F^{-1} \circ p ) $$ where $f$ is a smooth function on $M$ defined locally near $p$. In the lecture, however, it was just said that we should interpret the above equality as $$ du_j/dt = X_j .$$ My first question is then, is this a consequence of the above discussion? Or simply by definition? After all, $X$ is not meant to take values in $\mathbb{R}^{n}$. We then defined the solution of the above  $$e^{tX}(p) = u(t,p)$$ corresponding to the initial position $F^{-1}(p)$. And somehow this becomes an element of $ M $. But following the above discussion $u(t,p)$ is clearly in $ \mathbb{R}^{n} $! The only reasonable explanation I can think of is that $e^{tX}(p) = F( u(t,p) ) $ would be correct notation. This is my second question. Could somebody explain what the actual definition is? Thanks!",,"['ordinary-differential-equations', 'riemannian-geometry', 'lie-derivative']"
95,modelling a differential equation system,modelling a differential equation system,,"I have a system of ODEs with a pathogen population $(P)$ being modelled as a logistic growth as, ${dP\over dt}=rP(1-{P\over k})$, where $r$ is the replication rate and $k$ is the carrying capacity term and both these parameters are constant values. Now I want to introduce antibiotic effect so that the antibiotic will inhibit the growth of pathogen.For this I am planning on introducing a constant $\alpha \in (0,1)$ so that the replication will be reduced as $\alpha r$. But I want this replication to depend on the antibiotic concentration. So if I model the system as, ${dP\over dt}=\alpha A rP(1-{P\over k})\\ {dA\over dt}=-dA$ where $A$ is the antibiotic concentration, will this be right? ($d$ is the antibiotic decay rate) Or, should this be, ${dP\over dt}= \alpha {1\over A} rP(1-{P\over k})\\ {dA\over dt}=-dA$ as, when $A$ is high the replication should be reduced more (because the antibiotic will be more effective at high concentrations), and as $A$ decreases the reduction in the replication will be less? Out of these two systems which one maps this relationship correctly or is there any other way to model this?","I have a system of ODEs with a pathogen population $(P)$ being modelled as a logistic growth as, ${dP\over dt}=rP(1-{P\over k})$, where $r$ is the replication rate and $k$ is the carrying capacity term and both these parameters are constant values. Now I want to introduce antibiotic effect so that the antibiotic will inhibit the growth of pathogen.For this I am planning on introducing a constant $\alpha \in (0,1)$ so that the replication will be reduced as $\alpha r$. But I want this replication to depend on the antibiotic concentration. So if I model the system as, ${dP\over dt}=\alpha A rP(1-{P\over k})\\ {dA\over dt}=-dA$ where $A$ is the antibiotic concentration, will this be right? ($d$ is the antibiotic decay rate) Or, should this be, ${dP\over dt}= \alpha {1\over A} rP(1-{P\over k})\\ {dA\over dt}=-dA$ as, when $A$ is high the replication should be reduced more (because the antibiotic will be more effective at high concentrations), and as $A$ decreases the reduction in the replication will be less? Out of these two systems which one maps this relationship correctly or is there any other way to model this?",,"['ordinary-differential-equations', 'mathematical-modeling', 'biology']"
96,What am I doing wrong?- Differential equations and Integrals,What am I doing wrong?- Differential equations and Integrals,,"I am trying to find an equation for $\int x^x dx = h$ (I've not been told it's impossible... so I tried to... just to for fun) So we know that $f^{g(x)}(x) = h(x)$ where $g(x) = k$ (constant) so: $h'(x) = k \cdot f^{k-1} $ $x^x = h'(x) = k \cdot f^{k-1}(x)$ and thus:  $f(x) = (\dfrac{x^x}{c}) ^ {\dfrac{1}{c-1}}$ and if we substitude back ""f(x)"" we get: $h(x) = ((\dfrac{x^x}{c}) ^ {\dfrac{1}{c-1}})^{c} = (\dfrac{x^x}{c}) ^ {\dfrac{c} {c-1}}$ and we get that: $\int x^x dx = h(x) = (\dfrac{x^x}{c}) ^ {\dfrac{c}{c-1}}$ but it doesn't work out for let's say $c=2$ or $c=3$ ...  any ideas why? I am new to this field of maths.","I am trying to find an equation for $\int x^x dx = h$ (I've not been told it's impossible... so I tried to... just to for fun) So we know that $f^{g(x)}(x) = h(x)$ where $g(x) = k$ (constant) so: $h'(x) = k \cdot f^{k-1} $ $x^x = h'(x) = k \cdot f^{k-1}(x)$ and thus:  $f(x) = (\dfrac{x^x}{c}) ^ {\dfrac{1}{c-1}}$ and if we substitude back ""f(x)"" we get: $h(x) = ((\dfrac{x^x}{c}) ^ {\dfrac{1}{c-1}})^{c} = (\dfrac{x^x}{c}) ^ {\dfrac{c} {c-1}}$ and we get that: $\int x^x dx = h(x) = (\dfrac{x^x}{c}) ^ {\dfrac{c}{c-1}}$ but it doesn't work out for let's say $c=2$ or $c=3$ ...  any ideas why? I am new to this field of maths.",,"['integration', 'ordinary-differential-equations']"
97,Row-sum condition for Runge-Kutta methods,Row-sum condition for Runge-Kutta methods,,"Consider a general RK-method with weights $\vec{b}$ $(s\times 1)$ , nodes $\vec{c}$ $(s\times 1)$ and matrix $\vec{A}$ $(s\times s)$ . In the literature, there is a widely repeated minimum condition for consistency called the row-sum condition: $$c_i = \sum_{j=1}^{s}A_{ij}, $$ see e.g. Wikipedia . However, I have not been able to find any proof of this fact. Any ideas? The odd thing is that there seems to be another, just as fundamental, condition for consistency rarely mentioned, namely that $$\sum_{j=1}^{s}b_{j}=1.$$ Indeed, recall that the RK-method, for the equation $y'=f(t,y)$ , is defined by: $$Y_j' =  f(t_n+c_jh,Y_j), \ j = 1,...,s$$ $$Y_j = y_n+\sum_{k=1}^{s}A_{ik}hY_k', \ j=1,...,s$$ $$y_{n+1} = y_n + \sum_{j=1}^{s}b_jhY_j'$$ Taking exact data at $t_n$ and expanding to lowest order gives: $$Y_j' =  f(t_n+c_jh,Y_j)=f(t_n,y(t_n))+\mathcal{O}(h), \ j = 1,...,s$$ $$Y_j = y(t_n)+\mathcal{O}(h), \ j=1,...,s$$ $$y_{n+1}|_{\mathrm{exact}} = y(t_n) + \sum_{j=1}^{s}b_jhY_j'=y(t_n) + hf(t_n,y(t_n))\sum_{j=1}^{s}b_j + \ \mathcal{O}(h^2)$$ while $$y(t_{n+1}) = y(t_n+h) = y(t_n)+hy'(t_n) + \mathcal{O}(h^2) = y(t_n)+hf(t_n,y(t_n)) + \mathcal{O}(h^2), $$ showing that we must have $$\sum_{j=1}^{s}b_{j}=1$$ for consistency of order 1.","Consider a general RK-method with weights , nodes and matrix . In the literature, there is a widely repeated minimum condition for consistency called the row-sum condition: see e.g. Wikipedia . However, I have not been able to find any proof of this fact. Any ideas? The odd thing is that there seems to be another, just as fundamental, condition for consistency rarely mentioned, namely that Indeed, recall that the RK-method, for the equation , is defined by: Taking exact data at and expanding to lowest order gives: while showing that we must have for consistency of order 1.","\vec{b} (s\times 1) \vec{c} (s\times 1) \vec{A} (s\times s) c_i = \sum_{j=1}^{s}A_{ij},  \sum_{j=1}^{s}b_{j}=1. y'=f(t,y) Y_j' =  f(t_n+c_jh,Y_j), \ j = 1,...,s Y_j = y_n+\sum_{k=1}^{s}A_{ik}hY_k', \ j=1,...,s y_{n+1} = y_n + \sum_{j=1}^{s}b_jhY_j' t_n Y_j' =  f(t_n+c_jh,Y_j)=f(t_n,y(t_n))+\mathcal{O}(h), \ j = 1,...,s Y_j = y(t_n)+\mathcal{O}(h), \ j=1,...,s y_{n+1}|_{\mathrm{exact}} = y(t_n) + \sum_{j=1}^{s}b_jhY_j'=y(t_n) + hf(t_n,y(t_n))\sum_{j=1}^{s}b_j + \ \mathcal{O}(h^2) y(t_{n+1}) = y(t_n+h) = y(t_n)+hy'(t_n) + \mathcal{O}(h^2) = y(t_n)+hf(t_n,y(t_n)) + \mathcal{O}(h^2),  \sum_{j=1}^{s}b_{j}=1","['ordinary-differential-equations', 'numerical-methods']"
98,Using Substitution to Solve a 1st Order Differential Equation,Using Substitution to Solve a 1st Order Differential Equation,,"Solve $\frac{dy}{dx} = \frac{x^2 + y^2}{xy}$ for $y(1) = 2$ I began by simplifying such that: $\frac{dy}{dx} = \frac{x}{y} - \frac {y}{x}$ Then, I set $v = \frac {y}{x}$ Therefore: $y = xv$ and $y'$ = $v$ + $xv'$ Substituting back: $v + xv' = \frac{x}{xv} - \frac{xv}{x}$ or $v + xv' = \frac{1}{v} - v$ To continue, should $x$ be divided from both sides to allow for use of an integrating factor?","Solve $\frac{dy}{dx} = \frac{x^2 + y^2}{xy}$ for $y(1) = 2$ I began by simplifying such that: $\frac{dy}{dx} = \frac{x}{y} - \frac {y}{x}$ Then, I set $v = \frac {y}{x}$ Therefore: $y = xv$ and $y'$ = $v$ + $xv'$ Substituting back: $v + xv' = \frac{x}{xv} - \frac{xv}{x}$ or $v + xv' = \frac{1}{v} - v$ To continue, should $x$ be divided from both sides to allow for use of an integrating factor?",,"['ordinary-differential-equations', 'substitution']"
99,How to solve the Leaky Integrate and Fire ODE?,How to solve the Leaky Integrate and Fire ODE?,,"In the article on neuronal dynamics I read he solves the equation,  $$  \tau_{m}\,{{\text{d}}u\over{\text{d}}t}=-[u(t)-u_{\rm rest}]+R\,I(t)\,. $$ as $$ u(t)=u_{\rm rest}+R\,I_{0}\left[1-\exp\left(-{t\over\tau_{m}}\right)\right]\,. $$ When $I(t) = I_0$, $u_{\text{rest}} = u(0)$, and which starts at $t=0$ and then solving for $u(t)$, however, I have little knowledge of ODE's and I was wondering if someone could show me the steps between these two equations.","In the article on neuronal dynamics I read he solves the equation,  $$  \tau_{m}\,{{\text{d}}u\over{\text{d}}t}=-[u(t)-u_{\rm rest}]+R\,I(t)\,. $$ as $$ u(t)=u_{\rm rest}+R\,I_{0}\left[1-\exp\left(-{t\over\tau_{m}}\right)\right]\,. $$ When $I(t) = I_0$, $u_{\text{rest}} = u(0)$, and which starts at $t=0$ and then solving for $u(t)$, however, I have little knowledge of ODE's and I was wondering if someone could show me the steps between these two equations.",,"['ordinary-differential-equations', 'neural-networks']"
