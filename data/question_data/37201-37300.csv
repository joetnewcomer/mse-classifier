,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Estimated value of $\sum \limits_{i=1}^{50} \frac{1}{2i-1}$? [duplicate],Estimated value of ? [duplicate],\sum \limits_{i=1}^{50} \frac{1}{2i-1},"This question already has answers here : Finite sum of reciprocal odd integers (3 answers) Closed 7 years ago . Anyone has any idea the estimated result(no need to be accurate) of $\sum \limits_{i=1}^{50} \frac{1}{2i-1}$? I think this problem is not hard but I don't have a generalized method to solve! And I also remember this problem related to a very famous probability problem, any one has any idea? Thank you!","This question already has answers here : Finite sum of reciprocal odd integers (3 answers) Closed 7 years ago . Anyone has any idea the estimated result(no need to be accurate) of $\sum \limits_{i=1}^{50} \frac{1}{2i-1}$? I think this problem is not hard but I don't have a generalized method to solve! And I also remember this problem related to a very famous probability problem, any one has any idea? Thank you!",,"['probability', 'sequences-and-series']"
1,"When five dice are rolled, what is the chance to get five 6's if you can roll the dice that do not show a 6 on the first roll once more?","When five dice are rolled, what is the chance to get five 6's if you can roll the dice that do not show a 6 on the first roll once more?",,"We have $5$ normal dice. What is the chance to get five $6$'s if you can roll the dice that do not show a 6 one more time (if you do get a die with a $6$, you can leave it and roll the others one more time. Example: first roll $6$ $5$ $1$ $2$ $3$, we will roll $4$ dice and hope for four $6$s or if we get $6$ $6$ $2$ $3$ $3$ we will roll three dice one more time). I tried to calculate if you get $1$, $2$, $3$, $4$ dice with $6$ but I don't know how to ""sum"" the cases.","We have $5$ normal dice. What is the chance to get five $6$'s if you can roll the dice that do not show a 6 one more time (if you do get a die with a $6$, you can leave it and roll the others one more time. Example: first roll $6$ $5$ $1$ $2$ $3$, we will roll $4$ dice and hope for four $6$s or if we get $6$ $6$ $2$ $3$ $3$ we will roll three dice one more time). I tried to calculate if you get $1$, $2$, $3$, $4$ dice with $6$ but I don't know how to ""sum"" the cases.",,['probability']
2,MSE of uniform distribution,MSE of uniform distribution,,"I would like to know how to calculate the MSE for a Uniform Distribution on $(θ,2θ)$ I know that MSE is the variance of the Method of Moments Estimator (MME). I have found this to be $3θ/2$ . Then I think I should calculate the $Var(2/3X\bar)$ . Assuming that the bias of the MME is $0$ . I think the answer is $3(θ^2) / 48n$ . However I am unsure if this is the correct computation.",I would like to know how to calculate the MSE for a Uniform Distribution on I know that MSE is the variance of the Method of Moments Estimator (MME). I have found this to be . Then I think I should calculate the . Assuming that the bias of the MME is . I think the answer is . However I am unsure if this is the correct computation.,"(θ,2θ) 3θ/2 Var(2/3X\bar) 0 3(θ^2) / 48n","['probability', 'statistics']"
3,"Choosing 26 of 52 cards, reciprocal probability.","Choosing 26 of 52 cards, reciprocal probability.",,"Here we have a deck of $52$ cards numbered $1$ thru $52$ (integers only).  We randomly select $26$ of those $52$ cards (without replacement).  Each card is equiprobable. The task is to find out how many hands have the sum of the reciprocals of the chosen ranks exactly equal to $2$. $52 \choose 26$ is quite large (about $496$ trillion) so a straightforward count might not be possible on some computers in a reasonable amount of time.  $26$ happens to be very convenient though since there are $26$ letters in the alphabet, thus allowing me to run nested loops with single letter names of a,b,c.. .z. I am interested to see what ""pruning"" and rank exclusions can be done to this problem cuz this is a much larger state space than $16$ billion (if choosing only $10$ cards from a deck of $52$).  About $31,000$ times larger. A slight hint/head start:  The maximum possible sum is about $3.85442$ and the minimum possible sum is about $0.6836$.  This is one reason why I chose $2$ as it appears it is possible.  I don't even know yet if there is a single solution but I would suspect there are many, although a very small percentage.","Here we have a deck of $52$ cards numbered $1$ thru $52$ (integers only).  We randomly select $26$ of those $52$ cards (without replacement).  Each card is equiprobable. The task is to find out how many hands have the sum of the reciprocals of the chosen ranks exactly equal to $2$. $52 \choose 26$ is quite large (about $496$ trillion) so a straightforward count might not be possible on some computers in a reasonable amount of time.  $26$ happens to be very convenient though since there are $26$ letters in the alphabet, thus allowing me to run nested loops with single letter names of a,b,c.. .z. I am interested to see what ""pruning"" and rank exclusions can be done to this problem cuz this is a much larger state space than $16$ billion (if choosing only $10$ cards from a deck of $52$).  About $31,000$ times larger. A slight hint/head start:  The maximum possible sum is about $3.85442$ and the minimum possible sum is about $0.6836$.  This is one reason why I chose $2$ as it appears it is possible.  I don't even know yet if there is a single solution but I would suspect there are many, although a very small percentage.",,['probability']
4,Variance of the random sum of a Poisson?,Variance of the random sum of a Poisson?,,"We have that $N$ and $X_1, X_2, \dots$ are all independent. We also have $\operatorname{E} [X_j] = \mu$ and $\operatorname{Var}[X_j] = σ^2$. Then, we introduce an integer–valued random variable, $N$, which is the random sum such that: $$Z = \sum_{j=1}^{N+1}X_j.$$ Assuming that $N$ is distributed $\sim\mathrm{Poisson}(\lambda)$, what is the first moment and what is the variance of $Z$? For a normal Poisson distribution, I know the variance is just $\lambda$, as is the mean. I'm having trouble understanding the implication of having the bounds be poisson distributed. Normally, I would just say ""variance of the sum is the sum of the variance,"" but I don't think that's how it works with random sums. Any hints/guidance appreciated.","We have that $N$ and $X_1, X_2, \dots$ are all independent. We also have $\operatorname{E} [X_j] = \mu$ and $\operatorname{Var}[X_j] = σ^2$. Then, we introduce an integer–valued random variable, $N$, which is the random sum such that: $$Z = \sum_{j=1}^{N+1}X_j.$$ Assuming that $N$ is distributed $\sim\mathrm{Poisson}(\lambda)$, what is the first moment and what is the variance of $Z$? For a normal Poisson distribution, I know the variance is just $\lambda$, as is the mean. I'm having trouble understanding the implication of having the bounds be poisson distributed. Normally, I would just say ""variance of the sum is the sum of the variance,"" but I don't think that's how it works with random sums. Any hints/guidance appreciated.",,"['probability', 'random-variables', 'poisson-distribution']"
5,What are numerical methods of evaluating $P(1 < Z \leq 2)$ for standard normal Z?,What are numerical methods of evaluating  for standard normal Z?,P(1 < Z \leq 2),"Let $Z \sim Norm(0, 1)$ and denote its PDF and CDF by $\phi$ and $\Phi$ respectively. Then, theoretically, $P(1 < Z \leq 2) = \Phi(2) - \Phi(1).$ However $\Phi$ cannot be expressed in closed form, so some sort of computational method is required to obtain a numerical answer. The traditional method has been to consult tables of of $\Phi$ such as the five place table available online from NIST online, or similar tables available in many textbooks on probability and statistics to obtain $P(1 < Z \leq 2) = 0.47725 - 0.34134 = 0.13591.$ Nowadays, many software packages give such answers. For example, using R one obtains. diff(pnorm(c(1,2)))   ##  0.1359051 in which pnorm (with default second and third parameters) is a function for computing $\Phi.$ This R function is based on numerical integration. In the initial answer below, our purpose is to illustrate several computational and Monte Carlo methods that have been used to make CDF tables or to program software for finding intervals associated with normal distributions, and a few other commonly occurring distributions. Mostly, our focus is on widely applicable methods that are also useful for practical problems involving a variety continuous distributions. Additional answers illustrating different practical approaches are welcome.","Let $Z \sim Norm(0, 1)$ and denote its PDF and CDF by $\phi$ and $\Phi$ respectively. Then, theoretically, $P(1 < Z \leq 2) = \Phi(2) - \Phi(1).$ However $\Phi$ cannot be expressed in closed form, so some sort of computational method is required to obtain a numerical answer. The traditional method has been to consult tables of of $\Phi$ such as the five place table available online from NIST online, or similar tables available in many textbooks on probability and statistics to obtain $P(1 < Z \leq 2) = 0.47725 - 0.34134 = 0.13591.$ Nowadays, many software packages give such answers. For example, using R one obtains. diff(pnorm(c(1,2)))   ##  0.1359051 in which pnorm (with default second and third parameters) is a function for computing $\Phi.$ This R function is based on numerical integration. In the initial answer below, our purpose is to illustrate several computational and Monte Carlo methods that have been used to make CDF tables or to program software for finding intervals associated with normal distributions, and a few other commonly occurring distributions. Mostly, our focus is on widely applicable methods that are also useful for practical problems involving a variety continuous distributions. Additional answers illustrating different practical approaches are welcome.",,"['probability', 'statistics', 'simulation']"
6,Why does Binomial distribution fits Normal distribution,Why does Binomial distribution fits Normal distribution,,"I use Statgraphics for working with statistics. I generated 3 Binomial samples: 1) N=100, p=0.5 2) N=100, p=0.01 3) N=100, p=0.99 First sample looks like Normal sample: And it passes Kolmogorov-Smirnov test: So I have theoretical question from probability theory: why first sample acts like Normal and why other two don't?","I use Statgraphics for working with statistics. I generated 3 Binomial samples: 1) N=100, p=0.5 2) N=100, p=0.01 3) N=100, p=0.99 First sample looks like Normal sample: And it passes Kolmogorov-Smirnov test: So I have theoretical question from probability theory: why first sample acts like Normal and why other two don't?",,"['probability', 'probability-theory', 'statistics', 'central-limit-theorem', 'probability-limit-theorems']"
7,The Wald test with Poisson distribution,The Wald test with Poisson distribution,,"Let $X_1,\ldots, X_n\sim \operatorname{Poisson}(\lambda)$. Let $\lambda_w>0$ be given, I am trying to find the size $\alpha$ Wald test for $H_0$: $\lambda=\lambda_w$ vs $H_1$: $\lambda\neq \lambda_w$. I got stuck with how to get start to compute. I think I need to go with computing the power function and use it (I am still trying...). But I was told that Wald test size is computed in a special way (which is faster also). Can any body help me on finding this size $\alpha$ Wald test?","Let $X_1,\ldots, X_n\sim \operatorname{Poisson}(\lambda)$. Let $\lambda_w>0$ be given, I am trying to find the size $\alpha$ Wald test for $H_0$: $\lambda=\lambda_w$ vs $H_1$: $\lambda\neq \lambda_w$. I got stuck with how to get start to compute. I think I need to go with computing the power function and use it (I am still trying...). But I was told that Wald test size is computed in a special way (which is faster also). Can any body help me on finding this size $\alpha$ Wald test?",,"['probability', 'statistics', 'statistical-inference']"
8,Finding the probability that $A$ occurs before $B$,Finding the probability that  occurs before,A B,"Given an experiment has $3$ possible outcomes $A, B$ and $C$ with respective probabilities $p, q$ and $r$ where $p + q + r = 1.$ The experiment is repeated until either outcome $A$ or outcome $B$ occurs. How can I show that $A$ occurs before B with probability $\dfrac{p}{p+q}$ ?",Given an experiment has possible outcomes and with respective probabilities and where The experiment is repeated until either outcome or outcome occurs. How can I show that occurs before B with probability ?,"3 A, B C p, q r p + q + r = 1. A B A \dfrac{p}{p+q}",['probability']
9,Will adding a constant to a random variable change its distribution?,Will adding a constant to a random variable change its distribution?,,"Suppose I have a random variable $X$ and to this, I add a constant $c>0$. Will $X+c$ have a different distribution? From my intuition it seems so, but I am unable to prove it from a measure-theoretic point of view. Does the proof require measure theory? Thanks!","Suppose I have a random variable $X$ and to this, I add a constant $c>0$. Will $X+c$ have a different distribution? From my intuition it seems so, but I am unable to prove it from a measure-theoretic point of view. Does the proof require measure theory? Thanks!",,"['probability', 'measure-theory']"
10,"If I select two numbers $x_1,x_2$ from the interval $(0, 1) $, what is the probability $x_1 <x_2$?","If I select two numbers  from the interval , what is the probability ?","x_1,x_2 (0, 1)  x_1 <x_2",The question is in the description.  One of the answers I am getting is $\int_0^1 {x (1-x) }=1/6$. The other answer I am getting is there are only two possibilities: $x_1>x_2$ and $x_1<x_2$. Hence probability is 1/2 Any help would be great.,The question is in the description.  One of the answers I am getting is $\int_0^1 {x (1-x) }=1/6$. The other answer I am getting is there are only two possibilities: $x_1>x_2$ and $x_1<x_2$. Hence probability is 1/2 Any help would be great.,,[]
11,Sum of independent random variables is also independent,Sum of independent random variables is also independent,,"Given that $X, Y$ and $Z$ are discrete independent random variables, how can one show that $X+Y$ and $Z$ are independent as well? So far, I tried using the definition of independent variables and simplifying (X+Y)'s probability function using discrete convolution. I'm not sure that's the best way though.","Given that $X, Y$ and $Z$ are discrete independent random variables, how can one show that $X+Y$ and $Z$ are independent as well? So far, I tried using the definition of independent variables and simplifying (X+Y)'s probability function using discrete convolution. I'm not sure that's the best way though.",,"['probability', 'random-variables']"
12,How many bags would need to be bought to have all the red noses? [duplicate],How many bags would need to be bought to have all the red noses? [duplicate],,"This question already has answers here : Four balls with different colors in a box, how many times do I need to pick to see all four colors? (4 answers) Closed 9 years ago . There are 9 types of Red Noses, for comic relief this year: Each one is sold in an opaque packet, so it is lucky dip which one you will get. Assuming there is the same amount of each type (${1\over9}$ th ). On average, how many bags would need to be bought for someone to get all 9 of them?","This question already has answers here : Four balls with different colors in a box, how many times do I need to pick to see all four colors? (4 answers) Closed 9 years ago . There are 9 types of Red Noses, for comic relief this year: Each one is sold in an opaque packet, so it is lucky dip which one you will get. Assuming there is the same amount of each type (${1\over9}$ th ). On average, how many bags would need to be bought for someone to get all 9 of them?",,"['probability', 'coupon-collector']"
13,"Indistinguishable particles: obeing ""Bose-Einstein Statistics""","Indistinguishable particles: obeing ""Bose-Einstein Statistics""",,"Let $r$ indistinguishable particles be placed at random into $n$ cells. If the particles obey ""Bose-Einstein Statistics"", prove that the probability that there are exactly $i$ particles in a given cell is  $$\frac{{n+r-i-2 \choose r-i}}{{n+r-1 \choose r}}$$ I don't know Bose-Einstein Statistics principle. This troubles me to understand and solve the problem. I shall be obliged if some one be kind enough to solve me in full in details. I shall be highly benefited them.","Let $r$ indistinguishable particles be placed at random into $n$ cells. If the particles obey ""Bose-Einstein Statistics"", prove that the probability that there are exactly $i$ particles in a given cell is  $$\frac{{n+r-i-2 \choose r-i}}{{n+r-1 \choose r}}$$ I don't know Bose-Einstein Statistics principle. This troubles me to understand and solve the problem. I shall be obliged if some one be kind enough to solve me in full in details. I shall be highly benefited them.",,"['probability', 'combinatorics', 'statistics']"
14,Borel $\sigma$-Algebra on Real Numbers and interval elements [closed],Borel -Algebra on Real Numbers and interval elements [closed],\sigma,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question I am confused when learning about Borel $\sigma$-Algebra , the source of confusion is that the Borel $\sigma$-Algebra is on the Real numbers... Confusion arises from the fact that i began learning about $\sigma$-Algebra's and how it can work with sets like the ""Power Set"", ie discrete cases. So how can I come to understand that [2,6] or (π,9) is a Borel Set ?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question I am confused when learning about Borel $\sigma$-Algebra , the source of confusion is that the Borel $\sigma$-Algebra is on the Real numbers... Confusion arises from the fact that i began learning about $\sigma$-Algebra's and how it can work with sets like the ""Power Set"", ie discrete cases. So how can I come to understand that [2,6] or (π,9) is a Borel Set ?",,"['probability', 'measure-theory', 'probability-theory']"
15,Probability that n people collectively occupy all 365 birthdays,Probability that n people collectively occupy all 365 birthdays,,"The problem is quite simple to formulate. If you have a large group of people (n > 365), and their birthdays are uniformly distributed over the year (365 days), what's the probability that every day of the year is someone's birthday? I am thinking that the problem should be equivalent to finding the number of ways to place n unlabeled balls into k labeled boxes, such that all boxes are non-empty, but C((n-k)+k-1, (n-k))/C(n+k-1, n) (C(n,k) being the binomial coefficient) does not yield the correct answer.","The problem is quite simple to formulate. If you have a large group of people (n > 365), and their birthdays are uniformly distributed over the year (365 days), what's the probability that every day of the year is someone's birthday? I am thinking that the problem should be equivalent to finding the number of ways to place n unlabeled balls into k labeled boxes, such that all boxes are non-empty, but C((n-k)+k-1, (n-k))/C(n+k-1, n) (C(n,k) being the binomial coefficient) does not yield the correct answer.",,"['probability', 'combinatorics', 'discrete-mathematics']"
16,quadratic variations of Brownian motion squared,quadratic variations of Brownian motion squared,,"I'm trying to refresh my memories about stochastic processes. We know that Brownian motion has as quadratic variation equals to t. What is the quadratic variation of the Brownian motion squared ? Usually for this I would just use Ito's formula and pick out whatever is in front of the dWt, except in that case it doesn't work. Is there a straightforward way to compute this ? thanks !","I'm trying to refresh my memories about stochastic processes. We know that Brownian motion has as quadratic variation equals to t. What is the quadratic variation of the Brownian motion squared ? Usually for this I would just use Ito's formula and pick out whatever is in front of the dWt, except in that case it doesn't work. Is there a straightforward way to compute this ? thanks !",,"['probability', 'stochastic-processes', 'brownian-motion', 'quadratic-variation']"
17,Probability of four-of-a-kind flawed logic,Probability of four-of-a-kind flawed logic,,"I know my logic is completely wrong and I want to know why. I'm faced with the question of counting the probability that a hand of ($5$) cards in poker contains four cards of the same kind (four of a kind) in a standard $52$ card deck $$_ _ _ _ _$$ Now: First card: Can be any card so probability is 1 Second card: The second card has to be the same rank as first one, and since there are only 4 cards of that rank in a deck and one has already been used in the first card then there is a probability of $\frac{3}{51}$ of picking a second card same as the first Third card: Similarly the third card has probability $\frac{2}{50}$ of being chosen since there are only two cards of the rank we want left Fourth card: Similarly the fourth card has probability $\frac{1}{49}$ of being chosen since there are is only one card of the rank we want left in those 49 cards left Fifth card: The fifth card can be any of the 48 cards left so $\frac{48}{48}=1$ Now this 5 cards can be ordered in $P(5,5)=120 \text{ ways}$ So the $P(\text{four_of_a_kind})=1\cdot \frac{3}{51} \cdot \frac{2}{50} \cdot \frac{1}{49} \cdot 1 \cdot 120$ Which is wrong. What part I'm I doing wrongly?","I know my logic is completely wrong and I want to know why. I'm faced with the question of counting the probability that a hand of ($5$) cards in poker contains four cards of the same kind (four of a kind) in a standard $52$ card deck $$_ _ _ _ _$$ Now: First card: Can be any card so probability is 1 Second card: The second card has to be the same rank as first one, and since there are only 4 cards of that rank in a deck and one has already been used in the first card then there is a probability of $\frac{3}{51}$ of picking a second card same as the first Third card: Similarly the third card has probability $\frac{2}{50}$ of being chosen since there are only two cards of the rank we want left Fourth card: Similarly the fourth card has probability $\frac{1}{49}$ of being chosen since there are is only one card of the rank we want left in those 49 cards left Fifth card: The fifth card can be any of the 48 cards left so $\frac{48}{48}=1$ Now this 5 cards can be ordered in $P(5,5)=120 \text{ ways}$ So the $P(\text{four_of_a_kind})=1\cdot \frac{3}{51} \cdot \frac{2}{50} \cdot \frac{1}{49} \cdot 1 \cdot 120$ Which is wrong. What part I'm I doing wrongly?",,['probability']
18,"When to use $\mathbf{P}$ , and when to use $\mathbb{P}$ as the symbol for probability?","When to use  , and when to use  as the symbol for probability?",\mathbf{P} \mathbb{P},"I am trying to make myself some notes in preparatino for my FRM examination. Because I did a little mathematics and mathematical statistics at uni, I am getting very impatient with the inconsistent notations usen in FRM prescribed learning material. Having glanced through some many a probability theory books, and financial mathematics books, i have noticed that the mathematicians often use the $\mathbb{P}$ and $\mathbf{P}$ symbols, while most statisticians just stick to using $\mathbf{P}$ , or $\mathrm{P}$ . At first I thought that its just a matter of choice, but then I noticed that in some financial mathematics texts, that both $\mathbb{P}$ and $\mathrm{P}$  are sometimes used in the same expression? Typo, choice, or are the two symbols used for different purposes?","I am trying to make myself some notes in preparatino for my FRM examination. Because I did a little mathematics and mathematical statistics at uni, I am getting very impatient with the inconsistent notations usen in FRM prescribed learning material. Having glanced through some many a probability theory books, and financial mathematics books, i have noticed that the mathematicians often use the $\mathbb{P}$ and $\mathbf{P}$ symbols, while most statisticians just stick to using $\mathbf{P}$ , or $\mathrm{P}$ . At first I thought that its just a matter of choice, but then I noticed that in some financial mathematics texts, that both $\mathbb{P}$ and $\mathrm{P}$  are sometimes used in the same expression? Typo, choice, or are the two symbols used for different purposes?",,"['probability', 'measure-theory']"
19,Is is true that if $E|X_n - X| \to 0$ then $E[X_n] \to E[X] $?,Is is true that if  then ?,E|X_n - X| \to 0 E[X_n] \to E[X] ,"My question is motivated by the following problem: Show that if $|X_n - X| \le Y_n$ and $E[Y_n] \to 0$ then $E[X_n] \to E[X]$. I started off by saying that since $$|X_n - X|\ge 0 $$ then $$E[|X_n - X|]\ge 0 $$ At the same time  $$E[|X_n - X|]\le E[Y_n] $$ and so $$0 \le E[|X_n - X|] \le E[Y_n]$$ By the squeeze theorem then $E[|X_n - X|] \to 0$. I don't know how to proceed from here. I know that if $E[|X_n - X|] = 0$ then I  can set up a contradiction, like so: Suppose that $|X_n - X| = c$, $c \ne 0$ and $E[|X_n - X|] = 0$. Then $$E[|X_n - X|] = E[c] = c \ne 0. $$ But I don't know how to show this would hold in the limit. I am planning to use this to show that $X_n \to X$ and therefore $E[X_n] \to E[X]$","My question is motivated by the following problem: Show that if $|X_n - X| \le Y_n$ and $E[Y_n] \to 0$ then $E[X_n] \to E[X]$. I started off by saying that since $$|X_n - X|\ge 0 $$ then $$E[|X_n - X|]\ge 0 $$ At the same time  $$E[|X_n - X|]\le E[Y_n] $$ and so $$0 \le E[|X_n - X|] \le E[Y_n]$$ By the squeeze theorem then $E[|X_n - X|] \to 0$. I don't know how to proceed from here. I know that if $E[|X_n - X|] = 0$ then I  can set up a contradiction, like so: Suppose that $|X_n - X| = c$, $c \ne 0$ and $E[|X_n - X|] = 0$. Then $$E[|X_n - X|] = E[c] = c \ne 0. $$ But I don't know how to show this would hold in the limit. I am planning to use this to show that $X_n \to X$ and therefore $E[X_n] \to E[X]$",,"['probability', 'measure-theory']"
20,Why is there a difference between a population variance and a sample variance,Why is there a difference between a population variance and a sample variance,,Sorry if this answer is simple but I was wondering why is there a difference between a population variance and a sample variance? I understand The variance is calculated as: $$\text{Var} = \frac{1}{N}(x_i-\mu)^2$$ and the sample variance is computed as $$\text{Var}_s = \frac{1}{N-1}(x_i-\mu)^2$$ In real world data sets would you use the sample variance most of the time? What if the population is also a sample? How would you know? Also does the mean change when you are looking at a population or a sample?,Sorry if this answer is simple but I was wondering why is there a difference between a population variance and a sample variance? I understand The variance is calculated as: $$\text{Var} = \frac{1}{N}(x_i-\mu)^2$$ and the sample variance is computed as $$\text{Var}_s = \frac{1}{N-1}(x_i-\mu)^2$$ In real world data sets would you use the sample variance most of the time? What if the population is also a sample? How would you know? Also does the mean change when you are looking at a population or a sample?,,"['probability', 'statistics', 'statistical-inference', 'sampling']"
21,Probability question-help! P(A)+P(B)>1,Probability question-help! P(A)+P(B)>1,,"Firstly I wanted to ask for this question how can probability of a and b add up to more then one. If this happens then are they saying the two events do not belong to the same sample space because that's the only way that their probabilities can add up to more than 1 I think. Also, if we have two different events from two different experiments is it possible for us to compare them e.g to see if they are independent. E.g let one experiment be rolling a die once and another experiment be tossing a coin. Then if we choose an event from experiment 1 and an event from experiment two is it possible for us to compare the independence of those two events? (Although they would be independent anyways.) Sorry if this seems like a stupid question.","Firstly I wanted to ask for this question how can probability of a and b add up to more then one. If this happens then are they saying the two events do not belong to the same sample space because that's the only way that their probabilities can add up to more than 1 I think. Also, if we have two different events from two different experiments is it possible for us to compare them e.g to see if they are independent. E.g let one experiment be rolling a die once and another experiment be tossing a coin. Then if we choose an event from experiment 1 and an event from experiment two is it possible for us to compare the independence of those two events? (Although they would be independent anyways.) Sorry if this seems like a stupid question.",,['probability']
22,Guessing a number between 1 and 100 in 7 guesses or less.,Guessing a number between 1 and 100 in 7 guesses or less.,,"This question was asked before but I have a lot of trouble understanding the answers given. Why is it that a number selected at random between 1 and 100 can be determined in 7 or less guesses by always guessing the number in the middle of the remaining values, given that you're told whether your previous guess was too high or low? Here's a link to the earlier thread: Guessing number between 1-100 always can always be guessed in 7 guess. Why? I wonder if someone could explain this in greater detail?","This question was asked before but I have a lot of trouble understanding the answers given. Why is it that a number selected at random between 1 and 100 can be determined in 7 or less guesses by always guessing the number in the middle of the remaining values, given that you're told whether your previous guess was too high or low? Here's a link to the earlier thread: Guessing number between 1-100 always can always be guessed in 7 guess. Why? I wonder if someone could explain this in greater detail?",,"['probability', 'statistics']"
23,Coupon collector's problem worst case time?,Coupon collector's problem worst case time?,,"The expected time is $n Hn$. So for 9 coupons we get ~26 trials. But what is the probability that all coupons have been collected after 26 trials? How do we know the number of trials required to collect all with arbitrary certainty? (* Wolfram Mathematica: *)  (*Exact, but slow for n > 200*) trialsToP2[c_,n_,k_] := (n!/((n-c)! n^k)) StirlingS2[k,n]  (*Approximate*) trialsToP[n_,t_] := Block[{u,p},   u = (t-n Log[n])/n;   p = Exp[-Exp[-u]];   p   ]  trialsFromP[n_,p_] := Block[{u,t},   u = Log[1/Log[1/p]];   t = n * Log[n] + n *u;   t   ] The results seem pretty close even for small values of N: Input          | Results N   P          | trialsFromP trialsToP2 trialsToP 9   EulerGamma | 25.16       0.5904     0.5772     0.99       | 61.1764     0.9932     0.99                | 100 EulerGamma | 520.4       0.5790     0.5772     0.99       | 920.532     0.9905     0.99   a = test[9, EulerGamma]  b = test[9, 0.99] c = test[100, EulerGamma] d = test[100, 0.99]  test[n_, p_] := Block[{t,p1,p2},   t  = trialsFromP[n, p];   p1 = trialsToP2[n, n, Round[t]];   p2 = trialsToP[n, t];    {N[t, 4], N[p1, 4], N[p2, 4]} ]","The expected time is $n Hn$. So for 9 coupons we get ~26 trials. But what is the probability that all coupons have been collected after 26 trials? How do we know the number of trials required to collect all with arbitrary certainty? (* Wolfram Mathematica: *)  (*Exact, but slow for n > 200*) trialsToP2[c_,n_,k_] := (n!/((n-c)! n^k)) StirlingS2[k,n]  (*Approximate*) trialsToP[n_,t_] := Block[{u,p},   u = (t-n Log[n])/n;   p = Exp[-Exp[-u]];   p   ]  trialsFromP[n_,p_] := Block[{u,t},   u = Log[1/Log[1/p]];   t = n * Log[n] + n *u;   t   ] The results seem pretty close even for small values of N: Input          | Results N   P          | trialsFromP trialsToP2 trialsToP 9   EulerGamma | 25.16       0.5904     0.5772     0.99       | 61.1764     0.9932     0.99                | 100 EulerGamma | 520.4       0.5790     0.5772     0.99       | 920.532     0.9905     0.99   a = test[9, EulerGamma]  b = test[9, 0.99] c = test[100, EulerGamma] d = test[100, 0.99]  test[n_, p_] := Block[{t,p1,p2},   t  = trialsFromP[n, p];   p1 = trialsToP2[n, n, Round[t]];   p2 = trialsToP[n, t];    {N[t, 4], N[p1, 4], N[p2, 4]} ]",,['probability']
24,Change of measure of conditional expectation,Change of measure of conditional expectation,,"How can I prove that: $E_π [ (dQ_X/dπ)  S (T)| F_t ]= E_{Q_X} [S(T) | F_t]E_π [ dQ_X/dπ  | F_t ]$. Obviously $E_π [(dQ_X/dπ)  S(T) ]= E_{Q_X} [S(T)]$ I know that much, but how to prove when it is conditioned on $F_t$.","How can I prove that: $E_π [ (dQ_X/dπ)  S (T)| F_t ]= E_{Q_X} [S(T) | F_t]E_π [ dQ_X/dπ  | F_t ]$. Obviously $E_π [(dQ_X/dπ)  S(T) ]= E_{Q_X} [S(T)]$ I know that much, but how to prove when it is conditioned on $F_t$.",,"['probability', 'stochastic-calculus', 'conditional-probability']"
25,Expected number of steps till a random walk hits a or -b. [duplicate],Expected number of steps till a random walk hits a or -b. [duplicate],,"This question already has answers here : Symmetric random walk with bounds (3 answers) Closed 8 years ago . On wikipedia I read that the expected number of steps till a 1D simple random walk hits either $a$ or $-b$ is equal to $ab$. (I have seen this result also on other websites.) However, no proof or further reference is given. Could someone please explain how they arrived at this result? Thanks in advance, Claus","This question already has answers here : Symmetric random walk with bounds (3 answers) Closed 8 years ago . On wikipedia I read that the expected number of steps till a 1D simple random walk hits either $a$ or $-b$ is equal to $ab$. (I have seen this result also on other websites.) However, no proof or further reference is given. Could someone please explain how they arrived at this result? Thanks in advance, Claus",,"['probability', 'random-walk']"
26,probability selecting marbles,probability selecting marbles,,"can someone solve this example? An urn contains 2 Red marbles, 3 White marbles and 4 Blue marbles. You reach in and draw out 3 marbles at random (without replacement). What is the probability that you will get one marble of each color? What is the answer if you draw out the marbles one at a time with replacement?","can someone solve this example? An urn contains 2 Red marbles, 3 White marbles and 4 Blue marbles. You reach in and draw out 3 marbles at random (without replacement). What is the probability that you will get one marble of each color? What is the answer if you draw out the marbles one at a time with replacement?",,"['probability', 'combinatorics', 'discrete-mathematics']"
27,Probability each of six different numbers will appear at least once?,Probability each of six different numbers will appear at least once?,,"If seven balanced dice are rolled, what is the probability that each of the six different numbers will appear at least once? My solution is $\frac{6 \cdot 7!}{6^7}$. Is this correct? How would I implement a solution using multinomial coefficients?","If seven balanced dice are rolled, what is the probability that each of the six different numbers will appear at least once? My solution is $\frac{6 \cdot 7!}{6^7}$. Is this correct? How would I implement a solution using multinomial coefficients?",,['probability']
28,Card probabilities,Card probabilities,,Five cards are dealt from a standard deck of 52. What is the probability that the 3rd card is a Queen? What I dont understand here is how to factor in when one or both of the first two cards drawn are also Queens.,Five cards are dealt from a standard deck of 52. What is the probability that the 3rd card is a Queen? What I dont understand here is how to factor in when one or both of the first two cards drawn are also Queens.,,['probability']
29,What is the moment generating function given a density of a continuous random variable?,What is the moment generating function given a density of a continuous random variable?,,"Let $X$ be a continuous random variable with a density $$f_X(x) = \begin{cases}e^{-(x-1)}& \text{for }x > 1\\ 0 & \text{otherwise}\end{cases}$$ How do I compute the moment generating function of X? What is the range on which the moment generating function is defined? How do I used the moment generating function to compute the mean, the second moment and the variance of X?","Let $X$ be a continuous random variable with a density $$f_X(x) = \begin{cases}e^{-(x-1)}& \text{for }x > 1\\ 0 & \text{otherwise}\end{cases}$$ How do I compute the moment generating function of X? What is the range on which the moment generating function is defined? How do I used the moment generating function to compute the mean, the second moment and the variance of X?",,['probability']
30,Flipping Cards Probability,Flipping Cards Probability,,"You have a deck of cards, 26 red, 26 black. These are turned over,   and at any point you may stop and exclaim ""The next card is red."". If   the next card is red you win £10. What's the optimal strategy? Prove this is the optimal strategy. I feel like the optimal strategy is whenever you are in a state that you flip more black than red cards you call out red card, but I do not know how to prove it, nor can I think of why it must be the absolute best strategy. Thanks for any help.","You have a deck of cards, 26 red, 26 black. These are turned over,   and at any point you may stop and exclaim ""The next card is red."". If   the next card is red you win £10. What's the optimal strategy? Prove this is the optimal strategy. I feel like the optimal strategy is whenever you are in a state that you flip more black than red cards you call out red card, but I do not know how to prove it, nor can I think of why it must be the absolute best strategy. Thanks for any help.",,"['probability', 'puzzle', 'recreational-mathematics']"
31,What is the density of the product of $k$ i.i.d. normal random variables?,What is the density of the product of  i.i.d. normal random variables?,k,Say you have $k$ i.i.d. normal random variables with some mean $\mu$ and variance $\sigma^2$ and you multiply them all together. What is the density function of the result?,Say you have $k$ i.i.d. normal random variables with some mean $\mu$ and variance $\sigma^2$ and you multiply them all together. What is the density function of the result?,,"['probability', 'probability-distributions']"
32,Another probability question from my textbook,Another probability question from my textbook,,"Suppose that we have a tennis tournament with 32 players. Players are matched in a completely random fashion, and we assume that each player always has probability 1/2 to win a match. What is the probability that two given players meet each other during the tournament.","Suppose that we have a tennis tournament with 32 players. Players are matched in a completely random fashion, and we assume that each player always has probability 1/2 to win a match. What is the probability that two given players meet each other during the tournament.",,['probability']
33,What is the probability that $X<Y$?,What is the probability that ?,X<Y,"If two dice are rolled repeatedly, and $X$ is the number of tosses until $3$ is rolled, and $Y$ is the number of tosses until a $5$ is rolled, what is the probability$(X < Y)$? Also, if $Z$ is the number of rolls until a $10$ is rolled, what the probability$(X < Y < Z)$? Is $P(X < Y) = \sum_{n=2}^{\infty}\sum_{k=1}^{\infty}((32/36)^{(n-1)}(4/36)-(34/36)^{(k-1)}(2/36)]$?","If two dice are rolled repeatedly, and $X$ is the number of tosses until $3$ is rolled, and $Y$ is the number of tosses until a $5$ is rolled, what is the probability$(X < Y)$? Also, if $Z$ is the number of rolls until a $10$ is rolled, what the probability$(X < Y < Z)$? Is $P(X < Y) = \sum_{n=2}^{\infty}\sum_{k=1}^{\infty}((32/36)^{(n-1)}(4/36)-(34/36)^{(k-1)}(2/36)]$?",,['probability']
34,While calculating: Why do I get $1-P(A^c)\neq P(A) $?,While calculating: Why do I get ?,1-P(A^c)\neq P(A) ,"I have one single dice. And I throw this dice 4 times. What is the probability to get at least once a ""6""? Let $A$ denote the desired event. Sure this is easy, but I got my problems trying to understand why this $$P(A)=1-P(A^c)=1-\left(\frac{5}{6}\right)^4$$ is correct. What I do understand is that $\frac{5}{6}$ is the probability that the dice does not show a ""6"" after being thrown. But why the power of 4? Because the events ""to throw a six (or not)"" are independent? Assuming my guess is correct. I tryed to calculate the probability without considering the complement $A^c$: $$P(A)=\left(\frac{1}{6}\right)^4$$ Clearly this is not the same result. I would be glad if someone could enlighten me. I clearly do miss something. :-)","I have one single dice. And I throw this dice 4 times. What is the probability to get at least once a ""6""? Let $A$ denote the desired event. Sure this is easy, but I got my problems trying to understand why this $$P(A)=1-P(A^c)=1-\left(\frac{5}{6}\right)^4$$ is correct. What I do understand is that $\frac{5}{6}$ is the probability that the dice does not show a ""6"" after being thrown. But why the power of 4? Because the events ""to throw a six (or not)"" are independent? Assuming my guess is correct. I tryed to calculate the probability without considering the complement $A^c$: $$P(A)=\left(\frac{1}{6}\right)^4$$ Clearly this is not the same result. I would be glad if someone could enlighten me. I clearly do miss something. :-)",,['probability']
35,How many fixed points in a permutation,How many fixed points in a permutation,,"I am taking a probability course and we have been of course learning the classic problems of men with hats or variations of that in some other form. The problems being of the type if $n$ men have $n$ hats and mix them all together, whats the probability $0, 1, 2,$ or $k$ men choose their hat. But from what I can gather is that they are all problems of the same type, for any given random permutation of $k$ objects how many fixed points are there. My professor taught us to solve this using conditional probabilities (conditioning on whether the first person matched and then generating a system of equations) and using the inclusion-exclusion principle. But is there any other way to reason this?","I am taking a probability course and we have been of course learning the classic problems of men with hats or variations of that in some other form. The problems being of the type if men have hats and mix them all together, whats the probability or men choose their hat. But from what I can gather is that they are all problems of the same type, for any given random permutation of objects how many fixed points are there. My professor taught us to solve this using conditional probabilities (conditioning on whether the first person matched and then generating a system of equations) and using the inclusion-exclusion principle. But is there any other way to reason this?","n n 0, 1, 2, k k",['probability']
36,How can I compute $\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right)$ for a Brownian motion?,How can I compute  for a Brownian motion?,\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right),"Let $a\in \Bbb{R}_+$ and $B$ be a standart Brownian motion. For $\lambda >0$ I want to compute $$\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right)$$ I am somehow a bit lost where to start. I thought about using the tower property of conditional expectation and get that $$\begin{align}\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right)&=\Bbb{E}\left(\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\big|\mathcal{F}_t\right)\right)\\&=\Bbb{E}\left(1_{B_t\leq -a} \Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}}\big|\mathcal{F}_t\right)\right) \end{align}$$ where I used that $\{B_t\leq -a\}=B_t^{-1}((-\infty, -a])\in \mathcal{F}_t$ . But also here I don't see how to continue since I don't have independence of $1_{B_t\leq -a} $ and $\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}}\big|\mathcal{F}_t\right)$ . Can someone give me a hint how do do this or what to use?",Let and be a standart Brownian motion. For I want to compute I am somehow a bit lost where to start. I thought about using the tower property of conditional expectation and get that where I used that . But also here I don't see how to continue since I don't have independence of and . Can someone give me a hint how do do this or what to use?,"a\in \Bbb{R}_+ B \lambda >0 \Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right) \begin{align}\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right)&=\Bbb{E}\left(\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\big|\mathcal{F}_t\right)\right)\\&=\Bbb{E}\left(1_{B_t\leq -a} \Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}}\big|\mathcal{F}_t\right)\right) \end{align} \{B_t\leq -a\}=B_t^{-1}((-\infty, -a])\in \mathcal{F}_t 1_{B_t\leq -a}  \Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}}\big|\mathcal{F}_t\right)","['probability', 'probability-theory', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
37,Expected value of a sum of geometric dice roll,Expected value of a sum of geometric dice roll,,"We roll a 6 sided fair dice until the number 3 is received. What is the expected value of the sum of the rolls? What I've done: I defined a random variable $X$ to count the rolls amount. $X\sim G\left(\dfrac{1}{6}\right)$ so we have $P(X=k)=\left(\dfrac{5}{6}\right)^{k-1}\cdot\dfrac{1}{6}$ Then I defined another random variable, $S$ , as the rolls sum. Using the law of total expectation we get $ E\left(S \right) =\sum ^{k}_{i=1}E(  S| X= i) \cdot P( X= i) $ Now we define $x_{1},x_{2},\ldots ,x_{k}$ as the result of the $k$ -th roll. We note that $S=x_{1}+x_{2}+\ldots +x_{k}$ , so we get: $$E\left( S\right) =E\left( x_{1}+x_{2}+\ldots +x_{k}\right)$$ $$\Rightarrow E(  S| X= i) =E(  x_{1}+x_{2}+\ldots +x_{i}| X= i)$$ Using the linearity of expectation, it is equal to: $$E(  x_{1}|X=i) + E(  x_{2}|X=i) + ... + E(  x_{i}|X=i)$$ Now for each $1\leq t\leq i$ we have $$ E(  x_{t}|X=i)= \sum_{j=1}^{6} j \cdot \frac{P(x_t = j, X=k)}{P(X=k)} $$ So that would conclude as $$E(  S| X= i) = \sum_{t=1}^{i} \left( \sum_{j=1}^{6} j \cdot \frac{P(x_t = j, X=i)}{P(X=i)} \right) $$ But I'm not quite sure how to calculate the numerator here. Would appriciate a hint. Thanks!","We roll a 6 sided fair dice until the number 3 is received. What is the expected value of the sum of the rolls? What I've done: I defined a random variable to count the rolls amount. so we have Then I defined another random variable, , as the rolls sum. Using the law of total expectation we get Now we define as the result of the -th roll. We note that , so we get: Using the linearity of expectation, it is equal to: Now for each we have So that would conclude as But I'm not quite sure how to calculate the numerator here. Would appriciate a hint. Thanks!","X X\sim G\left(\dfrac{1}{6}\right) P(X=k)=\left(\dfrac{5}{6}\right)^{k-1}\cdot\dfrac{1}{6} S  E\left(S \right) =\sum ^{k}_{i=1}E(  S| X= i) \cdot P( X= i)  x_{1},x_{2},\ldots ,x_{k} k S=x_{1}+x_{2}+\ldots +x_{k} E\left( S\right) =E\left( x_{1}+x_{2}+\ldots +x_{k}\right) \Rightarrow E(  S| X= i) =E(  x_{1}+x_{2}+\ldots +x_{i}| X= i) E(  x_{1}|X=i) + E(  x_{2}|X=i) + ... + E(  x_{i}|X=i) 1\leq t\leq i  E(  x_{t}|X=i)= \sum_{j=1}^{6} j \cdot \frac{P(x_t = j, X=k)}{P(X=k)}  E(  S| X= i) = \sum_{t=1}^{i} \left( \sum_{j=1}^{6} j \cdot \frac{P(x_t = j, X=i)}{P(X=i)} \right) ","['probability', 'expected-value']"
38,Why m + n is 85?,Why m + n is 85?,,"Suppose $4$ balls are placed at random into $4$ boxes. The probability that exactly two boxes remain empty is $\frac mn$ in its simplest form. I divided it into $2$ cases, where $2$ balls are in two boxes each; $3$ in one box and $1$ in another. For the first case, I think there are ${4 \choose 2}$$\times$${4 \choose 1}$$\times$${3 \choose 1}$ = $72$ different ways. For the second case, I think there are ${4 \choose 3}$$\times$${4 \choose 1}$$\times$${3 \choose 1}$ = $48$ different ways. The total number of ways to place the balls is $4^4$ = $256$ . Thus, I calculated that $\frac mn$ = $\frac {15}{32}$ However, the answer said that m+n is $85$ . Where did I do wrong?","Suppose balls are placed at random into boxes. The probability that exactly two boxes remain empty is in its simplest form. I divided it into cases, where balls are in two boxes each; in one box and in another. For the first case, I think there are = different ways. For the second case, I think there are = different ways. The total number of ways to place the balls is = . Thus, I calculated that = However, the answer said that m+n is . Where did I do wrong?",4 4 \frac mn 2 2 3 1 {4 \choose 2}\times{4 \choose 1}\times{3 \choose 1} 72 {4 \choose 3}\times{4 \choose 1}\times{3 \choose 1} 48 4^4 256 \frac mn \frac {15}{32} 85,"['probability', 'statistics']"
39,"If a fair die (with 6 faces) is cast twice, what is the probability that the two numbers obtained differ by 2?","If a fair die (with 6 faces) is cast twice, what is the probability that the two numbers obtained differ by 2?",,"Don't think that this is my homework. Also, I do not have my graduation in mathematics. I was looking at different exams and their sample papers and came across this question... If a fair die (with 6 faces) is cast twice, what is the probability that the two numbers obtained differ by 2? (A) 1/12 (B) 1/6 (C) 2/9 (D) 1/2 I believe the answer is 2/9 and I want to find out if I am correct. For the first time rolling the dice, probability would be 1/6 (as per my knowledge)  there are 8 possible combinations for rolling the dice for the second time and difference between two numbers is equal to 2 1 -> 3  2 -> 4      |-> 1 3 -|    |-> 5     |-> 2 4 -|    |-> 6  5 -> 3  6 -> 4 And thus, final probability as per my knowledge should be (1/6)(8/6) = 2/9 Am I correct? Also I would welcome better approaches that can solve this problem more quicker.","Don't think that this is my homework. Also, I do not have my graduation in mathematics. I was looking at different exams and their sample papers and came across this question... If a fair die (with 6 faces) is cast twice, what is the probability that the two numbers obtained differ by 2? (A) 1/12 (B) 1/6 (C) 2/9 (D) 1/2 I believe the answer is 2/9 and I want to find out if I am correct. For the first time rolling the dice, probability would be 1/6 (as per my knowledge)  there are 8 possible combinations for rolling the dice for the second time and difference between two numbers is equal to 2 1 -> 3  2 -> 4      |-> 1 3 -|    |-> 5     |-> 2 4 -|    |-> 6  5 -> 3  6 -> 4 And thus, final probability as per my knowledge should be (1/6)(8/6) = 2/9 Am I correct? Also I would welcome better approaches that can solve this problem more quicker.",,['probability']
40,Why this test statistics for these distributions,Why this test statistics for these distributions,,"For a random sample $X_1,\ldots,X_n$ from the Poisson distribution I get where the T-statistic comes from because for a Poisson $E(X) = \lambda$ and $V(X)=\lambda$ so it becomes: $$T= \frac{\bar{X}-E(X)}{\sqrt{\frac{V(X)}{n}}} =\frac{\bar{X}-\lambda}{\sqrt{\frac{\lambda}{n}}},$$ where $\bar{X} = (X_1+\ldots,X_n)/n$ . But for the geometric distribution I have $E(X) = \frac{1}{p}$ and $V(X) = \frac{1}{p^2}$ . And if I substitute: $$T = \frac{\bar{X}-E(X)}{\sqrt{\frac{V(X)}{n}}} = \frac{\bar{X}-\frac{1}{p}}{\sqrt{\frac{1}{np^2}}} \neq \frac{\bar{X}-\frac{1}{p}}{\sqrt{\frac{1-p}{np^2}}}$$ The expression on the right is the real expression. Where does that $1-p$ in the numerator come from? How would I proceed with the binomial and exponential?",For a random sample from the Poisson distribution I get where the T-statistic comes from because for a Poisson and so it becomes: where . But for the geometric distribution I have and . And if I substitute: The expression on the right is the real expression. Where does that in the numerator come from? How would I proceed with the binomial and exponential?,"X_1,\ldots,X_n E(X) = \lambda V(X)=\lambda T= \frac{\bar{X}-E(X)}{\sqrt{\frac{V(X)}{n}}} =\frac{\bar{X}-\lambda}{\sqrt{\frac{\lambda}{n}}}, \bar{X} = (X_1+\ldots,X_n)/n E(X) = \frac{1}{p} V(X) = \frac{1}{p^2} T = \frac{\bar{X}-E(X)}{\sqrt{\frac{V(X)}{n}}} = \frac{\bar{X}-\frac{1}{p}}{\sqrt{\frac{1}{np^2}}} \neq \frac{\bar{X}-\frac{1}{p}}{\sqrt{\frac{1-p}{np^2}}} 1-p","['probability', 'probability-theory', 'statistics', 'probability-distributions', 'statistical-inference']"
41,Probability of first increase in ordering of iid random variables,Probability of first increase in ordering of iid random variables,,"What is the probability that the first $n-1$ terms of iid Unif(0,1) random draws are in decreasing order, but the first $n$ terms are not? $$ I know that due to exchangability, $\mathbb{P}(X_{1}<X_{2}<\cdots<X_{n-1})=\frac{1}{n-1!}$ . Why can't a similar exchangability argument hold to show that $\mathbb{P}(X_{1}>X_{2}>\cdots>X_{n-1}<X_{n})=\frac{1}{n!}$ ? My reasoning is that the distribution of $(X_1,X_2,\dots,X_n)$ is symmetric, so no matter which permutation of the random variables exist in the random vector, they will all have the same probability.","What is the probability that the first terms of iid Unif(0,1) random draws are in decreasing order, but the first terms are not? $$ I know that due to exchangability, . Why can't a similar exchangability argument hold to show that ? My reasoning is that the distribution of is symmetric, so no matter which permutation of the random variables exist in the random vector, they will all have the same probability.","n-1 n \mathbb{P}(X_{1}<X_{2}<\cdots<X_{n-1})=\frac{1}{n-1!} \mathbb{P}(X_{1}>X_{2}>\cdots>X_{n-1}<X_{n})=\frac{1}{n!} (X_1,X_2,\dots,X_n)","['probability', 'probability-theory', 'probability-distributions', 'random-variables']"
42,Tail Probabilities of $L^p$ bounded martingale differences,Tail Probabilities of  bounded martingale differences,L^p,"Assume that I have a probability space $(\Omega, \mathcal{A}, \mathbb{P})$ on which we define a sequence of martingale differences $X_1,X_2,\dots$ (w.r.t. to a certain filtration). Further let $p \in (1,2)$ and assume that the $X_i$ are bounded in $L^p$ , i.e. there exists $M>0$ such that $$\Vert X_i \Vert_p \leq M$$ for all $i \in \mathbb{N}$ . Does this already imply that I find a random variable $X \in L^p(\Omega, \mathcal{A}, \mathbb{P})$ and a $C > 0$ such that $$\mathbb{P}(\vert X_i \vert > z) \le C \mathbb{P}(\vert X \vert > z)$$ for all $z > 0$ , $i \in \mathbb{N}$ ?","Assume that I have a probability space on which we define a sequence of martingale differences (w.r.t. to a certain filtration). Further let and assume that the are bounded in , i.e. there exists such that for all . Does this already imply that I find a random variable and a such that for all , ?","(\Omega, \mathcal{A}, \mathbb{P}) X_1,X_2,\dots p \in (1,2) X_i L^p M>0 \Vert X_i \Vert_p \leq M i \in \mathbb{N} X \in L^p(\Omega, \mathcal{A}, \mathbb{P}) C > 0 \mathbb{P}(\vert X_i \vert > z) \le C \mathbb{P}(\vert X \vert > z) z > 0 i \in \mathbb{N}","['probability', 'probability-theory', 'distribution-tails']"
43,"Given a random variable $X$, can any random variable $Y$ be ""decomposed"" as a function of $X$ and $Z$ independent of $X$?","Given a random variable , can any random variable  be ""decomposed"" as a function of  and  independent of ?",X Y X Z X,"I want to know if given a random variable $X$ on some measure space $(\Omega, \mathscr M, P)$ , can any random variable $Y$ on the same measure space be ""decomposed"" as a function of $X$ and an r.v. $Z$ (again on $\Omega$ )  independent of $X$ , i.e. $Y= f(X,Z)$ (for I gues some measurable $f$ )? The reason I ask is because I was again thinking on my old question Conditional expectation $\mathbb E(Y \mid X \in B)$ instead of $\mathbb E(Y \,|\, X = x)$ (generalization of Shorack's PFS Notation 7.4.1) , and came across this comment in Probability, conditional on a zero probability event , which presents a scenario in which conditioning on $X=x$ makes complete sense: for $Y:= Z+X$ for some independent r.v.'s $X,Z$ uniform $(0,1$ ), we have $P(Z+X > 1.5 | X = 0.8) = P(Z+0.8>1.5) = P(Z>0.7)=0.3$ , i.e. we plug in $X=0.8$ to $Z+X$ and then evaluate the probability in a very straightforward way. Thus although it is true in general that conditioning on null sets is ill-defined ( Borel-Kolmogorov paradox ; or in the scenario of my comment in my previous question , there is no way of randomly generating an arbitrary rational number in $[0,1]$ in a ""uniform"" fashion --- cf. StackOverflow --- much like there is no way of generating an arbitrary integer in a ""uniform"" fashion), the reason why conditioning on the null events $X=x$ is fine is because if my conjecture is true, then it just boils down to plugging in an explicit value for $X$ and evaluating the probabilities/expectations as normal. In more detail, if we have $Y = f(Z,X)$ , then fixing $x_0$ in the codomain of $X$ we have $\mathbb E(Y|X=x_0) = \mathbb E(f(Z,x_0))$ (which then can specialize to statements about conditional probabilities).","I want to know if given a random variable on some measure space , can any random variable on the same measure space be ""decomposed"" as a function of and an r.v. (again on )  independent of , i.e. (for I gues some measurable )? The reason I ask is because I was again thinking on my old question Conditional expectation $\mathbb E(Y \mid X \in B)$ instead of $\mathbb E(Y \,|\, X = x)$ (generalization of Shorack's PFS Notation 7.4.1) , and came across this comment in Probability, conditional on a zero probability event , which presents a scenario in which conditioning on makes complete sense: for for some independent r.v.'s uniform ), we have , i.e. we plug in to and then evaluate the probability in a very straightforward way. Thus although it is true in general that conditioning on null sets is ill-defined ( Borel-Kolmogorov paradox ; or in the scenario of my comment in my previous question , there is no way of randomly generating an arbitrary rational number in in a ""uniform"" fashion --- cf. StackOverflow --- much like there is no way of generating an arbitrary integer in a ""uniform"" fashion), the reason why conditioning on the null events is fine is because if my conjecture is true, then it just boils down to plugging in an explicit value for and evaluating the probabilities/expectations as normal. In more detail, if we have , then fixing in the codomain of we have (which then can specialize to statements about conditional probabilities).","X (\Omega, \mathscr M, P) Y X Z \Omega X Y= f(X,Z) f X=x Y:= Z+X X,Z (0,1 P(Z+X > 1.5 | X = 0.8) = P(Z+0.8>1.5) = P(Z>0.7)=0.3 X=0.8 Z+X [0,1] X=x X Y = f(Z,X) x_0 X \mathbb E(Y|X=x_0) = \mathbb E(f(Z,x_0))","['probability', 'probability-theory', 'conditional-expectation', 'independence']"
44,Probability of receiving gifts on time,Probability of receiving gifts on time,,"Alex plans to order a birthday gift for his friend from an online retailer. However, the birthday coincides with the festival season during which there is a huge demand for buying online goods and hence deliveries are often delayed. He estimates that the probability of receiving the gift, in time, from the retailers A, B, C and D would be 0.6, 0.8, 0.9 and 0.5 respectively. Playing safe, he orders from all four retailers simultaneously. What would be the probability that his friend would receive the gift in time? My solution:- If we even get 1 gift on time, the task would be done, so I thought of finding 1-P(No gift on time) P(No gift on time)=P(A)*P(Not on time | A) + P(B)*P(Not on time | B) + P(C)*P(Not on time | C) + P(D)*P(Not on time | D) Choosing any of gifts among A,B,C,D are equally likely therefore P(A)=P(B)=P(C)=P(D)=1/4 P(No gift on time) = 1/4 * (0.4 + 0.2 + 0.1 + 0.5) = 0.3 Therefore P(At least 1 gift on time)= 1-0.3 = 0.7 but the answer is given as 0.996 , what mistake am I making ? Update :- I have understood that P(A)=P(B)=P(C)=P(D)=1 as it is certain that alex has ordered the gifts from all these retailers, but my question now is why is the following notation wrong ? P(No gift on time)=P(A)*P(Not on time | A) + P(B)*P(Not on time | B) + P(C)*P(Not on time | C) + P(D)*P(Not on time | D) A= event of ordering gift from retailer A P(Not on time | A) = Probability of gift not arriving on time given that it was ordered from retailer A","Alex plans to order a birthday gift for his friend from an online retailer. However, the birthday coincides with the festival season during which there is a huge demand for buying online goods and hence deliveries are often delayed. He estimates that the probability of receiving the gift, in time, from the retailers A, B, C and D would be 0.6, 0.8, 0.9 and 0.5 respectively. Playing safe, he orders from all four retailers simultaneously. What would be the probability that his friend would receive the gift in time? My solution:- If we even get 1 gift on time, the task would be done, so I thought of finding 1-P(No gift on time) P(No gift on time)=P(A)*P(Not on time | A) + P(B)*P(Not on time | B) + P(C)*P(Not on time | C) + P(D)*P(Not on time | D) Choosing any of gifts among A,B,C,D are equally likely therefore P(A)=P(B)=P(C)=P(D)=1/4 P(No gift on time) = 1/4 * (0.4 + 0.2 + 0.1 + 0.5) = 0.3 Therefore P(At least 1 gift on time)= 1-0.3 = 0.7 but the answer is given as 0.996 , what mistake am I making ? Update :- I have understood that P(A)=P(B)=P(C)=P(D)=1 as it is certain that alex has ordered the gifts from all these retailers, but my question now is why is the following notation wrong ? P(No gift on time)=P(A)*P(Not on time | A) + P(B)*P(Not on time | B) + P(C)*P(Not on time | C) + P(D)*P(Not on time | D) A= event of ordering gift from retailer A P(Not on time | A) = Probability of gift not arriving on time given that it was ordered from retailer A",,['probability']
45,Logical error in probability problem about postman delivering letters so that every house gets a wrong letter,Logical error in probability problem about postman delivering letters so that every house gets a wrong letter,,"A postman has to deliver four letters to four different houses in a street. Unfortunately the rain has erased the addresses, so he just distributes them randomly, one letter per house. What is the probability that every house gets the right letter? (☆ What is the probability that every house gets a wrong letter?) This was  a problem I stumbled on to while revising combinatorics. For the first part it is pretty straightforward, as for four houses, say $A, B, C$ and $D$ , there are $4$ letters $L_A, L_B, L_C$ and $L_D$ , and only one combination exists such that the correct letter is delivered to the correct house. However, for the starred question, my logic goes as follows, but the answer quite doesn't agree with what the site says. My logic : Let the houses $A, B, C$ and $D$ be side by side in the same order as they appear in the alphabet. $A$ $B$ $C$ $D$ X_______________ X marks the location of the postman. At house $A$ , out of the four letters $L_A, L_B, L_C$ , and $L_D$ , there exist only three possibilities such that house $A$ gets the wrong letter. For house $B$ , two exist and for house $C$ only $1$ possibility, either of the remaining $2$ letters. The last house is guaranteed to have received the wrong letter provided the above conditions hold true. Therefore the possibilities are $3 \cdot 2 \cdot 1 = 6$ . Therefore the probability is $6 / 24 = 1 / 4 = 0.25$ However the site says (quoted exactly from https://mathigon.org/world/Combinatorics#:~:text=To%20find%20the%20probability%20that,called%20the%20Inclusion%20Exclusion%20principle. ) To find the probability that every letter gets delivered to the wrong house is a bit more difficult. It is not simply $1 – 0.0417$ , since there are many cases in which one or two, but not all houses get the right letter. In this simple case, the easiest solution would be to write down all $24$ possibilities. You will find that in $9$ out of the $24$ cases every house gets a wrong letter, which gives a probability of $0.375 = 37.5\%$ . If there more too many houses to write down all possibilities, you can use an idea called the Inclusion Exclusion principle. Can someone explain the loophole in my logic, because the solution on the site asks us to bruteforce which happens to be a very frowned upon technique in exams?","A postman has to deliver four letters to four different houses in a street. Unfortunately the rain has erased the addresses, so he just distributes them randomly, one letter per house. What is the probability that every house gets the right letter? (☆ What is the probability that every house gets a wrong letter?) This was  a problem I stumbled on to while revising combinatorics. For the first part it is pretty straightforward, as for four houses, say and , there are letters and , and only one combination exists such that the correct letter is delivered to the correct house. However, for the starred question, my logic goes as follows, but the answer quite doesn't agree with what the site says. My logic : Let the houses and be side by side in the same order as they appear in the alphabet. X_______________ X marks the location of the postman. At house , out of the four letters , and , there exist only three possibilities such that house gets the wrong letter. For house , two exist and for house only possibility, either of the remaining letters. The last house is guaranteed to have received the wrong letter provided the above conditions hold true. Therefore the possibilities are . Therefore the probability is However the site says (quoted exactly from https://mathigon.org/world/Combinatorics#:~:text=To%20find%20the%20probability%20that,called%20the%20Inclusion%20Exclusion%20principle. ) To find the probability that every letter gets delivered to the wrong house is a bit more difficult. It is not simply , since there are many cases in which one or two, but not all houses get the right letter. In this simple case, the easiest solution would be to write down all possibilities. You will find that in out of the cases every house gets a wrong letter, which gives a probability of . If there more too many houses to write down all possibilities, you can use an idea called the Inclusion Exclusion principle. Can someone explain the loophole in my logic, because the solution on the site asks us to bruteforce which happens to be a very frowned upon technique in exams?","A, B, C D 4 L_A, L_B, L_C L_D A, B, C D A B C D A L_A, L_B, L_C L_D A B C 1 2 3 \cdot 2 \cdot 1 = 6 6 / 24 = 1 / 4 = 0.25 1 – 0.0417 24 9 24 0.375 = 37.5\%","['probability', 'combinatorics', 'permutations', 'inclusion-exclusion']"
46,Showing that a stopping time is finite for a biased random walk.,Showing that a stopping time is finite for a biased random walk.,,"If we consider $X_i$ iid with $\mathbb{P}(X_i=1) = p$ and $\mathbb{P}(X_i=-1)=1-p$ . Where $p \in (1/2,1)$ . The random walk is then given by, $$S_n=\sum_{i=1}^n X_i. $$ We also define the stopping time $\tau = \inf\{k:S_k \in \{-\alpha,\beta\} \}$ with $\alpha,\beta>0$ in the natural numbers. How to prove that $P(\tau<\infty)=1$ ? Intuitively it's clear that for sure at some point $S_n$ will hit $\beta$ (because the random walk has a tendincy to move upwards.). I was thinking about showing that $P(S_n=\infty \text{ i.o.})=1$ . But I'm not really sure how to make a rigorous argument. Could someone help me with this?","If we consider iid with and . Where . The random walk is then given by, We also define the stopping time with in the natural numbers. How to prove that ? Intuitively it's clear that for sure at some point will hit (because the random walk has a tendincy to move upwards.). I was thinking about showing that . But I'm not really sure how to make a rigorous argument. Could someone help me with this?","X_i \mathbb{P}(X_i=1) = p \mathbb{P}(X_i=-1)=1-p p \in (1/2,1) S_n=\sum_{i=1}^n X_i.  \tau = \inf\{k:S_k \in \{-\alpha,\beta\} \} \alpha,\beta>0 P(\tau<\infty)=1 S_n \beta P(S_n=\infty \text{ i.o.})=1","['probability', 'martingales', 'random-walk', 'stopping-times']"
47,Sleepwalking students on a 9x9 checkerboard [duplicate],Sleepwalking students on a 9x9 checkerboard [duplicate],,"This question already has an answer here : Moving counters on a chessboard (1 answer) Closed 2 years ago . Each square of a 9x9 checkerboard is initially slept on by one of 81 students. At noon, each student will wake up and randomly sleepwalk to a valid adjacent square horizontally or vertically (but not diagonally). What is the probability that two or more students end up on the same square?","This question already has an answer here : Moving counters on a chessboard (1 answer) Closed 2 years ago . Each square of a 9x9 checkerboard is initially slept on by one of 81 students. At noon, each student will wake up and randomly sleepwalk to a valid adjacent square horizontally or vertically (but not diagonally). What is the probability that two or more students end up on the same square?",,"['probability', 'combinatorics', 'statistics']"
48,Probability of meeting your friend on a tournament.,Probability of meeting your friend on a tournament.,,"There are $2^n$ players in a knock-out chess tournament. They all have identical skill. This means for any two pairings the probability of either winning is $\frac{1}{2}$ . The table is assigned uniformly randomly. You and your friend are both competing. What is the probability you guys meet? Method one Draw out a knock-out table in a rooted binary tree sense. Without loss of generality put yourself at the bottom left leaf. We can condition the probability you and your friend meet on the friends initial location. Notice we have $2^n-1 $ positions they could be in. One of the positions is adjacent to you in which case the probability you meet is $1$ . Two of these positions are only one game away from in which case you must both win your games to meet, with probability $\frac{1}{4}$ . Four of these positions are two games away from you in which case you meet if you both win two games. $\frac{1}{2^4}$ . And so on. Formally: Probability of meeting = $\sum\limits_{i = 0}^n \frac{2^i}{2^n-1} \cdot (\frac{1}{2^i})^2$ = $\frac{1}{2^n-1}\sum\limits_{i = 0}^n \frac{1}{2^i} = \frac{1}{{2^n}-1} \cdot \frac{\frac{1}{2^{n+1}} -1}{1-\frac{1}{2}} = \frac{1}{2^{n-1}}$ This method is the nuts and bolts. A little bit of fiddle with geometric series but fairly intuitive. Method two This method seems much nicer. I thought it was legitimate and it gets the same answer but I found a big hole in it explained at the end. Let the positions of the contestants be $C_1 , C_2, C_3 , ... , C_{2^n} $ . Let $M_{i,j}$ denote the event that contestant $i$ meets contestant $j$ By full symmetry the probability of $M_{i,j}$ is the same for all $i \not= j \in [1,2^n] $ . We have a total of $2^n - 1$ games in the tournament as each game removes a single player. We have a total of $2^n$ choose $2$ = $\frac{2^n \cdot (2^n - 1)}{2} = 2^{2n-1} -2^{n-1}$ couplings of $M_{i,j}$ . ( $\star$ ) Matches played are like picking $2^n -1$ balls from a bag with $2^{2n-1} -2^{n-1}$ balls in it. So the probability you and your friend meet is $\frac{2^n -1}{2^{2n-1} -2^{n-1}} = \frac{1}{2^{n-1}}$ . Nifty huh, we get the same answer as before but much cleaner. However this method is wrong! Step $(\star)$ is a violation. We cannot for example pick the balls $M_{1,2} , M_{1,3} , M_{1,4} , ..., M_{1,2^n}$ as contestant $1$ cannot play everyone! Can someone help me make the second method work? Im sure its not far off. Or is it just a coincidence that it works?","There are players in a knock-out chess tournament. They all have identical skill. This means for any two pairings the probability of either winning is . The table is assigned uniformly randomly. You and your friend are both competing. What is the probability you guys meet? Method one Draw out a knock-out table in a rooted binary tree sense. Without loss of generality put yourself at the bottom left leaf. We can condition the probability you and your friend meet on the friends initial location. Notice we have positions they could be in. One of the positions is adjacent to you in which case the probability you meet is . Two of these positions are only one game away from in which case you must both win your games to meet, with probability . Four of these positions are two games away from you in which case you meet if you both win two games. . And so on. Formally: Probability of meeting = = This method is the nuts and bolts. A little bit of fiddle with geometric series but fairly intuitive. Method two This method seems much nicer. I thought it was legitimate and it gets the same answer but I found a big hole in it explained at the end. Let the positions of the contestants be . Let denote the event that contestant meets contestant By full symmetry the probability of is the same for all . We have a total of games in the tournament as each game removes a single player. We have a total of choose = couplings of . ( ) Matches played are like picking balls from a bag with balls in it. So the probability you and your friend meet is . Nifty huh, we get the same answer as before but much cleaner. However this method is wrong! Step is a violation. We cannot for example pick the balls as contestant cannot play everyone! Can someone help me make the second method work? Im sure its not far off. Or is it just a coincidence that it works?","2^n \frac{1}{2} 2^n-1  1 \frac{1}{4} \frac{1}{2^4} \sum\limits_{i = 0}^n \frac{2^i}{2^n-1} \cdot (\frac{1}{2^i})^2 \frac{1}{2^n-1}\sum\limits_{i = 0}^n \frac{1}{2^i} = \frac{1}{{2^n}-1} \cdot \frac{\frac{1}{2^{n+1}} -1}{1-\frac{1}{2}} = \frac{1}{2^{n-1}} C_1 , C_2, C_3 , ... , C_{2^n}  M_{i,j} i j M_{i,j} i \not= j \in [1,2^n]  2^n - 1 2^n 2 \frac{2^n \cdot (2^n - 1)}{2} = 2^{2n-1} -2^{n-1} M_{i,j} \star 2^n -1 2^{2n-1} -2^{n-1} \frac{2^n -1}{2^{2n-1} -2^{n-1}} = \frac{1}{2^{n-1}} (\star) M_{1,2} , M_{1,3} , M_{1,4} , ..., M_{1,2^n} 1","['probability', 'statistics']"
49,What is the probability that the quadratic equation $ax^2+x+1=0$ has two real roots? [duplicate],What is the probability that the quadratic equation  has two real roots? [duplicate],ax^2+x+1=0,"This question already has answers here : Probability that a quadratic polynomial with random coefficients has real roots (6 answers) Closed 2 years ago . A number $a$ is chosen at random within the interval $(-1, 1)$ . What is the probability that the quadratic equation $ax^2+x+1=0$ has two real roots? For it to have its real roots, we must guarantee that $1-4a \geq 0$ , or $a\leq \frac{1}{4}$ . It is no longer clear to me what I have to do.","This question already has answers here : Probability that a quadratic polynomial with random coefficients has real roots (6 answers) Closed 2 years ago . A number is chosen at random within the interval . What is the probability that the quadratic equation has two real roots? For it to have its real roots, we must guarantee that , or . It is no longer clear to me what I have to do.","a (-1, 1) ax^2+x+1=0 1-4a \geq 0 a\leq \frac{1}{4}","['probability', 'geometric-probability']"
50,Characteristic functions inequality,Characteristic functions inequality,,"How to show that for any random variables $X,Y$ with characteristic functions $\phi_X, \phi_Y$ we have: $$\sup_{\xi \in \mathbb{R}} |\phi_X(\xi) - \phi_Y(\xi)| \leq 2P(X \neq Y)?$$ My attempt: First, I was considering an easier case, when $X,Y$ have densities $f_X, f_Y$ . In that case, we have: \begin{align} |\phi_X(\xi)  - \phi_Y(\xi)| = \big|  \int_\mathbb{R} e^{i\xi t} (f_X(t)-f_Y(t)) \, dt \big| \leq \int_\mathbb{R} |f_X(t)-f_Y(t)| \, dt \leq\\ \leq \int_\mathbb{R} f_X(t) \, dt + \int_{\mathbb{R}} f_Y(t) \, dt = 2. \end{align} But since $X,Y$ have densities $Z = X-Y$ does as well, so $P(Z=z)=0$ for any $z \in \mathbb{R}$ , so $P(Z \neq 0) = 1$ , so $2 P(X \neq Y) = 2.$ Is that correct? If so, how to generalise it for any random variables? Or maybe is there a completely different solution?","How to show that for any random variables with characteristic functions we have: My attempt: First, I was considering an easier case, when have densities . In that case, we have: But since have densities does as well, so for any , so , so Is that correct? If so, how to generalise it for any random variables? Or maybe is there a completely different solution?","X,Y \phi_X, \phi_Y \sup_{\xi \in \mathbb{R}} |\phi_X(\xi) - \phi_Y(\xi)| \leq 2P(X \neq Y)? X,Y f_X, f_Y \begin{align}
|\phi_X(\xi)  - \phi_Y(\xi)| = \big|  \int_\mathbb{R} e^{i\xi t} (f_X(t)-f_Y(t)) \, dt \big| \leq \int_\mathbb{R} |f_X(t)-f_Y(t)| \, dt \leq\\ \leq \int_\mathbb{R} f_X(t) \, dt + \int_{\mathbb{R}} f_Y(t) \, dt = 2.
\end{align} X,Y Z = X-Y P(Z=z)=0 z \in \mathbb{R} P(Z \neq 0) = 1 2 P(X \neq Y) = 2.","['probability', 'probability-theory', 'random-variables', 'characteristic-functions']"
51,"Given $E[Y]=1$ , $E[Y^2]=2$ and $E[Y^3]=5$, Y non negative integer random variable, find min of $P[Y=0]$","Given  ,  and , Y non negative integer random variable, find min of",E[Y]=1 E[Y^2]=2 E[Y^3]=5 P[Y=0],"If $Y$ is a non negative integer-valued random variable with $E[Y]=1$ , $E[Y^2]=2$ and $E[Y^3]=5$ ,where $E$ is the average value of $Y$ . Find the minimum value of the possibility $P[Y=0]$ . I know that $E[Y]=\sum^{n}_{i=1}y_{i}p(y_{i})$ and $E[Y]=\sum_{y:p(y)>0}yp(y)$ . I know that Markov's inequality is : $P[Y\geq a] \leq\frac{E[Y]}{a}$ I know that  Chebyshev's inequality is : $P(|Y-E[Y]|\geq k)\leq \frac{Var(Y)}{k^2}$ The book I'm studying is the : A first course in probability 8th edition , Sheldon Ross","If is a non negative integer-valued random variable with , and ,where is the average value of . Find the minimum value of the possibility . I know that and . I know that Markov's inequality is : I know that  Chebyshev's inequality is : The book I'm studying is the : A first course in probability 8th edition , Sheldon Ross","Y E[Y]=1 E[Y^2]=2 E[Y^3]=5 E Y P[Y=0] E[Y]=\sum^{n}_{i=1}y_{i}p(y_{i}) E[Y]=\sum_{y:p(y)>0}yp(y) P[Y\geq a]
\leq\frac{E[Y]}{a} P(|Y-E[Y]|\geq k)\leq \frac{Var(Y)}{k^2}",['probability']
52,Conditional probability on a multiple choice test,Conditional probability on a multiple choice test,,"A student is taking a multiple-choice test. Each question on the test has five possible answers and only one choice is correct. The probability this student knows the correct answer is 70%. If the student does not know the answer, they select an answer at random with each answer having an equal probability of being picked. Calculate the conditional probability that the student knew the answer given they answered the question correctly. I started off like this: Let B denote the event that the student knew the answer. Let A denote the event that the student answered the question correctly. I was able to work out that $P(A)=\frac{3}{10}*\frac{1}{5} + \frac{7}{10} = \frac{19}{25}$ And I know the formula $P(B|A)=\frac{P(A \cap B)}{P(A)}$ I am unsure on how to work out $P(A \cap B)$ . Any advice would be greatly apprectiated.","A student is taking a multiple-choice test. Each question on the test has five possible answers and only one choice is correct. The probability this student knows the correct answer is 70%. If the student does not know the answer, they select an answer at random with each answer having an equal probability of being picked. Calculate the conditional probability that the student knew the answer given they answered the question correctly. I started off like this: Let B denote the event that the student knew the answer. Let A denote the event that the student answered the question correctly. I was able to work out that And I know the formula I am unsure on how to work out . Any advice would be greatly apprectiated.",P(A)=\frac{3}{10}*\frac{1}{5} + \frac{7}{10} = \frac{19}{25} P(B|A)=\frac{P(A \cap B)}{P(A)} P(A \cap B),"['probability', 'conditional-probability']"
53,Person X and Y throw a Fair die one after another. Whoever throws 6 first wins. What is the probability that X wins? [duplicate],Person X and Y throw a Fair die one after another. Whoever throws 6 first wins. What is the probability that X wins? [duplicate],,"This question already has answers here : Alternate moves, first one to roll a 6 wins: what's the probability of winning? (3 answers) Closed 10 months ago . Two people take turns rolling a fair die. Person $X$ rolls first, then person $Y$ , then $X$ , and so on. The winner is the first to roll a $6$ . What is the probability that person $X$ wins? Our teacher in class solved this question as follows: Probability of winning in 1st round = Probability that 6 occurs = 1/6 So Probability of $Y$ losing in first round becomes $= 5/6$ Where it is assumed that X starts the game. suppose if X throws and if 6 comes then the game will stop and X will win. but if X loses then Y will throw the die (here Y will have to lose because according to question we want X to win the game) and then X will throw . and so on. it will be like $$X+ \neg X\neg YX+ \neg X\neg Y\neg X\neg YX+\dots$$ HERE $X$ denotes X wins and $\neg X$ means X loses i.e. 6 does not come . similarly $\neg Y$ means Y loses. This will form an infinite geometric progression series. Converting into probabilities we have $$(1/6) + (5/6)(5/6)(1/6) + (5/6)(5/6)(5/6)(5/6)(1/6)+ \dots$$ The sum of this geometric series is the answer. MY question is: how the probability of $X$ in $2nd$ toss is $(5/6)(5/6)(1/6)$ and similarly in the following tosses? Why these terms are in multiplication not addition?","This question already has answers here : Alternate moves, first one to roll a 6 wins: what's the probability of winning? (3 answers) Closed 10 months ago . Two people take turns rolling a fair die. Person rolls first, then person , then , and so on. The winner is the first to roll a . What is the probability that person wins? Our teacher in class solved this question as follows: Probability of winning in 1st round = Probability that 6 occurs = 1/6 So Probability of losing in first round becomes Where it is assumed that X starts the game. suppose if X throws and if 6 comes then the game will stop and X will win. but if X loses then Y will throw the die (here Y will have to lose because according to question we want X to win the game) and then X will throw . and so on. it will be like HERE denotes X wins and means X loses i.e. 6 does not come . similarly means Y loses. This will form an infinite geometric progression series. Converting into probabilities we have The sum of this geometric series is the answer. MY question is: how the probability of in toss is and similarly in the following tosses? Why these terms are in multiplication not addition?",X Y X 6 X Y = 5/6 X+ \neg X\neg YX+ \neg X\neg Y\neg X\neg YX+\dots X \neg X \neg Y (1/6) + (5/6)(5/6)(1/6) + (5/6)(5/6)(5/6)(5/6)(1/6)+ \dots X 2nd (5/6)(5/6)(1/6),"['probability', 'dice']"
54,Calculating expected value of non-empty chambers. What is wrong in my solution?,Calculating expected value of non-empty chambers. What is wrong in my solution?,,"Instering $5$ different balls to $12$ different chambers, all the options are equally probabable. Let $X$ be the number of non-empty chambers, calculate $\mathbb{E}[X]$ I know that $\mathbb{E}(X)=\sum_{t \in R_{X}} t \cdot \mathbb{P}(X=t)$ so I just need to calculate $\mathbb{P}(X=t)$ for every $1\leq t \leq 5$ $$\mathbb{P}(X=1)= \frac{1}{12^4}$$ $$\mathbb{P}(X=2)= \frac{11 \cdot 2^3}{12^4}$$ $$\mathbb{P}(X=3)= \frac{11\cdot 10\cdot 3^2}{12^4}$$ $$\mathbb{P}(X=4)= \frac{11\cdot 10 \cdot 9 \cdot4}{12^4}$$ $$\mathbb{P}(X=5)= \frac{11\cdot 10 \cdot 9 \cdot8}{12^4}$$ now I just substitute this values in the expression $$\mathbb{E}(X)=\sum_{t \in R_{X}} t \cdot \mathbb{P}(X=t)$$ but I am not getting the right answer. I assume I have mistake in develop the expressions above. The options are: A. $\displaystyle \frac{161051}{20736}$ B. $\displaystyle \frac{161051}{248832}$ C. $\displaystyle \frac{87781}{20736}$ D. $\displaystyle \frac{87781}{248832}$","Instering different balls to different chambers, all the options are equally probabable. Let be the number of non-empty chambers, calculate I know that so I just need to calculate for every now I just substitute this values in the expression but I am not getting the right answer. I assume I have mistake in develop the expressions above. The options are: A. B. C. D.",5 12 X \mathbb{E}[X] \mathbb{E}(X)=\sum_{t \in R_{X}} t \cdot \mathbb{P}(X=t) \mathbb{P}(X=t) 1\leq t \leq 5 \mathbb{P}(X=1)= \frac{1}{12^4} \mathbb{P}(X=2)= \frac{11 \cdot 2^3}{12^4} \mathbb{P}(X=3)= \frac{11\cdot 10\cdot 3^2}{12^4} \mathbb{P}(X=4)= \frac{11\cdot 10 \cdot 9 \cdot4}{12^4} \mathbb{P}(X=5)= \frac{11\cdot 10 \cdot 9 \cdot8}{12^4} \mathbb{E}(X)=\sum_{t \in R_{X}} t \cdot \mathbb{P}(X=t) \displaystyle \frac{161051}{20736} \displaystyle \frac{161051}{248832} \displaystyle \frac{87781}{20736} \displaystyle \frac{87781}{248832},"['probability', 'solution-verification', 'expected-value']"
55,Exercise 2.3.5 in Vershynin's HDP book,Exercise 2.3.5 in Vershynin's HDP book,,"Let $X_i$ be independent Bernoulli random variables with parameters $p_i$ . Consider their sum $S_N=\sum_{i=1}^N X_i$ and denote its mean by $\mu=\mathsf{E} S_N$ . Then, for $\delta\in (0,1]$ , prove that $$\mathsf{P}\{|S_N-\mu|\geqslant \delta\mu\}\leqslant 2e^{-c\mu\delta^2}$$ where $c>0$ is an absolute constant. This is the Exercise 2.3.5 in Vershynin's HDP book. My idea and problem are as following: ============================ My Solution ============================ I use the following two inequalities: 1.[Chenoff's inequality] $$\mathsf{P}\{S_N\geqslant t\}\leqslant e^{-\mu}\left(\frac{e\mu}{t}\right)^t,\qquad (t>\mu)$$ 2.[Extension of Chernoff's inequality, see Exercise 2.3.2 in Vershynin's HDP book] $$\mathsf{P}\{S_N\leqslant t\}\leqslant e^{-\mu}\left(\frac{e\mu}{t}\right)^t,\qquad (t<\mu)$$ Then I get $$ \begin{aligned} \mathsf{P}\{|S_N-\mu|\geqslant \delta\mu\}&\leqslant \mathsf{P}\{S_N-\mu\geqslant \delta\mu\} + \mathsf{P}\{\mu-S_N\geqslant \delta\mu\}\\[0.5em] &=\mathsf{P}\{S_N\geqslant (1+\delta)\mu\}+\mathsf{P}\{S_N\leqslant (1-\delta)\mu\}\\[1em] &\leqslant e^{-\mu} \left\{\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu}+\left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right\}.\qquad[\text{use ineq. 1 & 2 above}] \end{aligned} $$ ============================ My Problem ============================ However, I cannot prove that $$ e^{-\mu} \left\{\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu}+\left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right\} \leqslant 2e^{-c\mu\delta^2}.\tag{*} $$ Is the ineq. (*) true?","Let be independent Bernoulli random variables with parameters . Consider their sum and denote its mean by . Then, for , prove that where is an absolute constant. This is the Exercise 2.3.5 in Vershynin's HDP book. My idea and problem are as following: ============================ My Solution ============================ I use the following two inequalities: 1.[Chenoff's inequality] 2.[Extension of Chernoff's inequality, see Exercise 2.3.2 in Vershynin's HDP book] Then I get ============================ My Problem ============================ However, I cannot prove that Is the ineq. (*) true?","X_i p_i S_N=\sum_{i=1}^N X_i \mu=\mathsf{E} S_N \delta\in (0,1] \mathsf{P}\{|S_N-\mu|\geqslant \delta\mu\}\leqslant 2e^{-c\mu\delta^2} c>0 \mathsf{P}\{S_N\geqslant t\}\leqslant e^{-\mu}\left(\frac{e\mu}{t}\right)^t,\qquad (t>\mu) \mathsf{P}\{S_N\leqslant t\}\leqslant e^{-\mu}\left(\frac{e\mu}{t}\right)^t,\qquad (t<\mu) 
\begin{aligned}
\mathsf{P}\{|S_N-\mu|\geqslant \delta\mu\}&\leqslant \mathsf{P}\{S_N-\mu\geqslant \delta\mu\} + \mathsf{P}\{\mu-S_N\geqslant \delta\mu\}\\[0.5em]
&=\mathsf{P}\{S_N\geqslant (1+\delta)\mu\}+\mathsf{P}\{S_N\leqslant (1-\delta)\mu\}\\[1em]
&\leqslant e^{-\mu} \left\{\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu}+\left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right\}.\qquad[\text{use ineq. 1 & 2 above}]
\end{aligned}
 
e^{-\mu} \left\{\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu}+\left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right\} \leqslant 2e^{-c\mu\delta^2}.\tag{*}
","['probability', 'inequality']"
56,Probability of picking buttons from a bag,Probability of picking buttons from a bag,,"A bag contains $30$ buttons that are colored either blue, red or yellow. There are the same number of each color ( $10$ each). A total $4$ buttons are drawn from the bag. Compute the followings: Find $n(\Omega)$ . The probability that at least $3$ of them are red? The probability that there is at least one of each color? This seems like a basic problem but my professor and I cannot agree on an answer. I think the probabilities are $2/21$ and $100/203$ for parts $2$ and $3$ respectively. I used combinations to calculate the probabilities. My professor said $n(A)/n(\Omega)$ is $3/15$ for both so that is the answer for both $2$ and $3$ .","A bag contains buttons that are colored either blue, red or yellow. There are the same number of each color ( each). A total buttons are drawn from the bag. Compute the followings: Find . The probability that at least of them are red? The probability that there is at least one of each color? This seems like a basic problem but my professor and I cannot agree on an answer. I think the probabilities are and for parts and respectively. I used combinations to calculate the probabilities. My professor said is for both so that is the answer for both and .",30 10 4 n(\Omega) 3 2/21 100/203 2 3 n(A)/n(\Omega) 3/15 2 3,"['probability', 'combinations', 'conditional-probability']"
57,Probability of choosing envelopes,Probability of choosing envelopes,,"Suppose that you have 20 different letters and 10 distinctly addressed envelopes. The 20 letters consists of 10 pairs, where each pair belongs inside one of the 10 envelopes. Suppose that you place the 20 letters inside the 10 envelopes, two per envelope, but at random. What is the probability that exactly 3 of the 10 envelopes will contain both of the letters which they should contain? I have seen similar questions to this one but they always assign only one letter to one envelope. Also, the scenario is usually how to choose AT LEAST one right envelope. I am not too clear as to how to adapt to this new scenario.","Suppose that you have 20 different letters and 10 distinctly addressed envelopes. The 20 letters consists of 10 pairs, where each pair belongs inside one of the 10 envelopes. Suppose that you place the 20 letters inside the 10 envelopes, two per envelope, but at random. What is the probability that exactly 3 of the 10 envelopes will contain both of the letters which they should contain? I have seen similar questions to this one but they always assign only one letter to one envelope. Also, the scenario is usually how to choose AT LEAST one right envelope. I am not too clear as to how to adapt to this new scenario.",,['probability']
58,Expected winning of a player with highest lowest and second highest lowest grouping,Expected winning of a player with highest lowest and second highest lowest grouping,,"The following is an interview question. Question:     Given 4 players $A,B,C,D$ and a fair $50$ -sided dice, assume that we do not allow repeated score (i.e. next player cannot get the same score as all previous players. Otherwise, the player roll again). We group players with the highest and lowest scores together and second and third highest together. The winning team is the group that has the larger sum which will win the difference between team score. For example, say $A,B$ and $C,D$ form 2 groups and $A=1,B=7,C=3,D=2,$ then $A,B$ groups wins with $8-5=3$ units. What number should player $A$ hopes to get to maximize his expected winning? I totally have no idea how to start this question at all.","The following is an interview question. Question:     Given 4 players and a fair -sided dice, assume that we do not allow repeated score (i.e. next player cannot get the same score as all previous players. Otherwise, the player roll again). We group players with the highest and lowest scores together and second and third highest together. The winning team is the group that has the larger sum which will win the difference between team score. For example, say and form 2 groups and then groups wins with units. What number should player hopes to get to maximize his expected winning? I totally have no idea how to start this question at all.","A,B,C,D 50 A,B C,D A=1,B=7,C=3,D=2, A,B 8-5=3 A","['probability', 'combinatorics', 'probability-theory', 'discrete-mathematics', 'expected-value']"
59,Probability of Detection,Probability of Detection,,"There exist radars used for detecting aircrafts passing through the state of Iowa.  The state only has three radars, with each having a 25% chance of failing to detect a plane in the state.  Suddenly, a plane has entered the state. What is the probability the plane is detected? Let’s say the plane was detected.  What is the probability that at least two of the radars detected the plane? For the first question I computed $(1-.25)^3$ since there are three aircrafts and it would be a $75%$ percent chance the planes are detected. For the second part I am less sure but would it be $(.25)(.75)^2$ since there are two radars that detect and one that doesn’t?","There exist radars used for detecting aircrafts passing through the state of Iowa.  The state only has three radars, with each having a 25% chance of failing to detect a plane in the state.  Suddenly, a plane has entered the state. What is the probability the plane is detected? Let’s say the plane was detected.  What is the probability that at least two of the radars detected the plane? For the first question I computed since there are three aircrafts and it would be a percent chance the planes are detected. For the second part I am less sure but would it be since there are two radars that detect and one that doesn’t?",(1-.25)^3 75% (.25)(.75)^2,['probability']
60,"Expected number of query pings in a simple ""concentration"" game","Expected number of query pings in a simple ""concentration"" game",,"This question is based on a recently posted clever memory game (that uses $k=16$ ) that can be explained as follows: There are $k$ cards numbered $1 \to k$ face down randomly ordered in a row.  You (the player) make one ""ping"" consisting of choosing any card you like, turning it over, and seeing its number.  If it is $1$ , the card remains face up (remains ""exposed""), but if it is any other number, the card is returned face down.  Then you repeat.  The only card that can become ""exposed"" is the one that has a value $1$ greater than the highest existing ""exposed"" card.  You continue until all $k$ cards have are ""exposed."" In short, the sequence of cards that becomes ""exposed"" must be $1, 2, \ldots, k$ . Your final score is your total number of pings.  The lower the number of pings, the better.  Thus of course you do your best to remember all cards you have turned over (but are not yet ""exposed""). Clearly the optimal score is $k$ , where by great luck you just happen to ping cards in the order $1, 2, \ldots , k$ .  But this is quite rare.  (It occurs with probability $\prod\limits_{i=1}^k \frac{1}{i}$ .) Questions Perfect memory :  If you assume you have perfect memory of any card you have seen, and you play optimally, what is your expected score (as a function of $k$ )? No memory :  If you assume you have no memory of any previous card you have seen, what is your expected score (as a function of $k$ )?  Of course you never ping an ""exposed"" card, but by ""no memory"" I mean you might indeed randomly choose a card you have previously turned over (even on the current ""round""), even if it will not become ""exposed."" No memory but systematic search :  Assume you have no memory but after each successful ping (leading to an ""exposed"" card) you repeat your sequence of pings, always starting at the left of the line of cards and pinging each available non-exposed card to the right until hitting the one card that can be ""exposed.""  (This guarantees the number of pings per exposed card becomes lower and lower as the game progresses.)","This question is based on a recently posted clever memory game (that uses ) that can be explained as follows: There are cards numbered face down randomly ordered in a row.  You (the player) make one ""ping"" consisting of choosing any card you like, turning it over, and seeing its number.  If it is , the card remains face up (remains ""exposed""), but if it is any other number, the card is returned face down.  Then you repeat.  The only card that can become ""exposed"" is the one that has a value greater than the highest existing ""exposed"" card.  You continue until all cards have are ""exposed."" In short, the sequence of cards that becomes ""exposed"" must be . Your final score is your total number of pings.  The lower the number of pings, the better.  Thus of course you do your best to remember all cards you have turned over (but are not yet ""exposed""). Clearly the optimal score is , where by great luck you just happen to ping cards in the order .  But this is quite rare.  (It occurs with probability .) Questions Perfect memory :  If you assume you have perfect memory of any card you have seen, and you play optimally, what is your expected score (as a function of )? No memory :  If you assume you have no memory of any previous card you have seen, what is your expected score (as a function of )?  Of course you never ping an ""exposed"" card, but by ""no memory"" I mean you might indeed randomly choose a card you have previously turned over (even on the current ""round""), even if it will not become ""exposed."" No memory but systematic search :  Assume you have no memory but after each successful ping (leading to an ""exposed"" card) you repeat your sequence of pings, always starting at the left of the line of cards and pinging each available non-exposed card to the right until hitting the one card that can be ""exposed.""  (This guarantees the number of pings per exposed card becomes lower and lower as the game progresses.)","k=16 k 1 \to k 1 1 k 1, 2, \ldots, k k 1, 2, \ldots , k \prod\limits_{i=1}^k \frac{1}{i} k k","['probability', 'combinatorics']"
61,Get statistical significance from the likelihood and so from the posterior (in anisotropy expansion),Get statistical significance from the likelihood and so from the posterior (in anisotropy expansion),,"I try to understand, from a technical point of view, how are computed the statistical significance from a Bayesian study (I guess) in this abstract below  from article "" Evidence for anisotropy of cosmic acceleration "" by Jacques Colin, Roya Mohayaee, Mohamed Rameez, Subir Sarkar: Observations reveal a 'bulk flow' in the local Universe which is    faster and extends to much larger scales than is expected around    a typical observer in  the standard ΛCDM cosmology. This is expected    to result in a scale-dependent dipolar modulation of the acceleration    of the expansion rate inferred from observations of objects within    the bulk flow. From a maximum-likelihood analysis  of the Joint    Lightcurve Analysis (JLA) catalogue of Type Ia supernovae we find    that the deceleration parameter, in addition to a small monopole,    indeed has a much bigger dipole component aligned with the CMB    dipole which falls exponentially  with redshift $z$ : $q_0=q_m+\vec{q}_d\cdot \hat{n}\exp(−z/S)$ .    The best fit to data yields $q_d=−8.03$ and $S=0.0262$ ( $⇒d∼100 \text{Mpc}$ ), rejecting isotropy ( $q_d=0$ ) with $3.9\sigma$ statistical significance,  while $q_m=−0.157$ and    consistent with no acceleration ( $q_m=0$ ) at $1.4\sigma$ .    Thus the cosmic acceleration deduced from supernovae may be an    artefact of our being non-Copernican observers, rather than    evidence for a dominant component  of 'dark energy' in the Universe. Indeed, I have few notions like the relation : $$\text{posterior}=\frac{\text{likelihood}\times\text{prior}}{\text{evidence}}$$ using likelihood or more classically : $$p(\theta|d)=\frac{p(d|\theta)p(\theta)}{p(d)}$$ with $\theta$ are the parameters to estimate and $d$ are the data. I would like to understand how the statistical significance announced (the first one $= 3.9 \sigma$ ) is computed from the Bayesian relations above. I think this is computed from the posterior but how to get this value : they estimate from the likelihood at $d_d = -8.03$ and $S = 0.0262$ : how to compute this $3.9 \sigma$ ? Do they use the MLE (Maximum Likelihood Estimator)  or MAP (Maximum Aposteriori Probability) methods ? I hope you will understand my issue of understanding since I am interested into the necessity to introduce a cosmological constant or not into standard model. Any explanations are welcome. Regards","I try to understand, from a technical point of view, how are computed the statistical significance from a Bayesian study (I guess) in this abstract below  from article "" Evidence for anisotropy of cosmic acceleration "" by Jacques Colin, Roya Mohayaee, Mohamed Rameez, Subir Sarkar: Observations reveal a 'bulk flow' in the local Universe which is    faster and extends to much larger scales than is expected around    a typical observer in  the standard ΛCDM cosmology. This is expected    to result in a scale-dependent dipolar modulation of the acceleration    of the expansion rate inferred from observations of objects within    the bulk flow. From a maximum-likelihood analysis  of the Joint    Lightcurve Analysis (JLA) catalogue of Type Ia supernovae we find    that the deceleration parameter, in addition to a small monopole,    indeed has a much bigger dipole component aligned with the CMB    dipole which falls exponentially  with redshift : .    The best fit to data yields and ( ), rejecting isotropy ( ) with statistical significance,  while and    consistent with no acceleration ( ) at .    Thus the cosmic acceleration deduced from supernovae may be an    artefact of our being non-Copernican observers, rather than    evidence for a dominant component  of 'dark energy' in the Universe. Indeed, I have few notions like the relation : using likelihood or more classically : with are the parameters to estimate and are the data. I would like to understand how the statistical significance announced (the first one ) is computed from the Bayesian relations above. I think this is computed from the posterior but how to get this value : they estimate from the likelihood at and : how to compute this ? Do they use the MLE (Maximum Likelihood Estimator)  or MAP (Maximum Aposteriori Probability) methods ? I hope you will understand my issue of understanding since I am interested into the necessity to introduce a cosmological constant or not into standard model. Any explanations are welcome. Regards",z q_0=q_m+\vec{q}_d\cdot \hat{n}\exp(−z/S) q_d=−8.03 S=0.0262 ⇒d∼100 \text{Mpc} q_d=0 3.9\sigma q_m=−0.157 q_m=0 1.4\sigma \text{posterior}=\frac{\text{likelihood}\times\text{prior}}{\text{evidence}} p(\theta|d)=\frac{p(d|\theta)p(\theta)}{p(d)} \theta d = 3.9 \sigma d_d = -8.03 S = 0.0262 3.9 \sigma,['probability']
62,Probability of sequence of events,Probability of sequence of events,,"Question : Let ${A_n}$ be a sequence of events with P( $A_n$ )=1 $\forall n$ $\geq$ 1. Find P( $\cup A_n$ ) and P( $\cap A_n $ ) What I did :   Since it is given that P( $A_n$ )= $1$ we can easily say that all $A_n$ are sure events for all n $\geq 1$ .So as all are sure events so each $A_n$ contains every other events so basically, we can think $\cup A_n$ as $A_k$ for any k $\in {1,2,....n}$ thus P( $\cup A_n$ )=P( $A_k$ )= $1$ Similarly for ( $\cap A_n $ ) we get it as $ 1$ . Am I doing any mistake ? Any help is appreciated.","Question : Let be a sequence of events with P( )=1 1. Find P( ) and P( ) What I did :   Since it is given that P( )= we can easily say that all are sure events for all n .So as all are sure events so each contains every other events so basically, we can think as for any k thus P( )=P( )= Similarly for ( ) we get it as . Am I doing any mistake ? Any help is appreciated.","{A_n} A_n \forall n \geq \cup A_n \cap A_n  A_n 1 A_n \geq 1 A_n \cup A_n A_k \in {1,2,....n} \cup A_n A_k 1 \cap A_n   1","['probability', 'probability-theory']"
63,"If three cards are drawn without replacement, what is the probability of each subsequent card being larger than the previous card?","If three cards are drawn without replacement, what is the probability of each subsequent card being larger than the previous card?",,"Suppose we have a deck of $500$ cards numbered from 1 to 500. If  the cards are shuffled randomly and you are asked to pick three cards (without replacement), one at a time, what's the probability of each subsequent card being larger than the previous drawn card? My solution: Let $i$ be the second card that is picked, then $i-1$ cards will be less that $i$ and $500 - i$ cards will be greater than $i$ . Thus: P(subsequent card being larger than the previous card) ${\displaystyle =  \sum_{i=1}^{500} \frac {(i -1)(500 - i)}{500 \cdot 499 \cdot 498}}$ I'm not sure if my answer is correct.","Suppose we have a deck of cards numbered from 1 to 500. If  the cards are shuffled randomly and you are asked to pick three cards (without replacement), one at a time, what's the probability of each subsequent card being larger than the previous drawn card? My solution: Let be the second card that is picked, then cards will be less that and cards will be greater than . Thus: P(subsequent card being larger than the previous card) I'm not sure if my answer is correct.",500 i i-1 i 500 - i i {\displaystyle =  \sum_{i=1}^{500} \frac {(i -1)(500 - i)}{500 \cdot 499 \cdot 498}},"['probability', 'probability-theory']"
64,"Quant Trading interview probability question, what's wrong with my reasoning?","Quant Trading interview probability question, what's wrong with my reasoning?",,"I roll a die up to three times. You can decide to stop and choose the number on the die (where the number is your payoff) during each roll. What's your strategy? I am confused about the strategy for the first roll. My reasoning is this: the chance of getting a 5 or 6 from rolls 2 and 3 is: $$1 - 4/6*4/6=1-4/9=5/9$$ So, you have a greater than $50\%$ chance to get a 5 or a 6 on either the 2nd or 3rd dice. Thus, you'd want to select the first dice only when it rolls a 6. However, this answer is incorrect. The solution says the Expected value during the first roll is \$4.25, which I can understand how they computed, and thus the strategy would be to settle with either a 5 or 6 on the first roll, but I can't figure out my logical fallacy.","I roll a die up to three times. You can decide to stop and choose the number on the die (where the number is your payoff) during each roll. What's your strategy? I am confused about the strategy for the first roll. My reasoning is this: the chance of getting a 5 or 6 from rolls 2 and 3 is: So, you have a greater than chance to get a 5 or a 6 on either the 2nd or 3rd dice. Thus, you'd want to select the first dice only when it rolls a 6. However, this answer is incorrect. The solution says the Expected value during the first roll is \$4.25, which I can understand how they computed, and thus the strategy would be to settle with either a 5 or 6 on the first roll, but I can't figure out my logical fallacy.",1 - 4/6*4/6=1-4/9=5/9 50\%,"['probability', 'finance']"
65,Expected Value Problem with 100 side dice and 100 doors,Expected Value Problem with 100 side dice and 100 doors,,"One hundred doors, one dollar behind each door. Roll a  one-hundred dice for one hundred times. You can take the dollar after the door whose number is rolled out but the dollar is not replaced. What's the expectation? What I tried: I tried to write a general formula taking ideas from the Coupon collector problem. I found that the E[X]= (N-n)/N where N is the total number of doors and n is the amount we've opened but I don't think that works. Any advice would be appreciated.","One hundred doors, one dollar behind each door. Roll a  one-hundred dice for one hundred times. You can take the dollar after the door whose number is rolled out but the dollar is not replaced. What's the expectation? What I tried: I tried to write a general formula taking ideas from the Coupon collector problem. I found that the E[X]= (N-n)/N where N is the total number of doors and n is the amount we've opened but I don't think that works. Any advice would be appreciated.",,"['probability', 'expected-value']"
66,Proof that 1-P(B|C)=P(~B|C). Is everything correct?,Proof that 1-P(B|C)=P(~B|C). Is everything correct?,,"Proof: Suppose we have a set of students. Different students study different subjects, with some of them studing several subjects at once. Suppose among many other subjects we have Biology and Chemistry. Then $P(B|C)$ would be probability of selecting a Biology student out of set of Chemistry students. Now we need to ask ourselves: what would be the complement of event ""selecting a Biology student out of set of Chemistry students""?. Remember that we select a Biology student out of set of Chemistry students, NOT from all students. In other words, our sample space shrunk to the size of the set of Chemistry students. The complement event would be selecting a Chemistry student who does NOT study Biology (although the person can study other subjects along with Chemistry). The probability of selecting such person out of set of Chemistry students can be written as $P(\tilde B|C)$ . Thus $1-P(B|C)=P(\tilde B|C)$ because $P(B|C)$ and $P(\tilde B|C)$ are complements of each other.","Proof: Suppose we have a set of students. Different students study different subjects, with some of them studing several subjects at once. Suppose among many other subjects we have Biology and Chemistry. Then would be probability of selecting a Biology student out of set of Chemistry students. Now we need to ask ourselves: what would be the complement of event ""selecting a Biology student out of set of Chemistry students""?. Remember that we select a Biology student out of set of Chemistry students, NOT from all students. In other words, our sample space shrunk to the size of the set of Chemistry students. The complement event would be selecting a Chemistry student who does NOT study Biology (although the person can study other subjects along with Chemistry). The probability of selecting such person out of set of Chemistry students can be written as . Thus because and are complements of each other.",P(B|C) P(\tilde B|C) 1-P(B|C)=P(\tilde B|C) P(B|C) P(\tilde B|C),"['probability', 'proof-verification', 'conditional-probability']"
67,The probability of ending up with red balls,The probability of ending up with red balls,,"In a bag there are $a$ red balls and $b$ blue balls ( $a,b>0$ ). You conduct the following process. Randomly fetch a ball from the bag, record the colour as the current colour . And throw it away. Go to 2. If there's no ball left, break. Otherwise, randomly fetch a ball from the bag. If the colour coincides with current colour, throw it away, and repeat 2; otherwise, put back the ball, and go back to 1. Qs: What's the probability that the last ball fetched is red, in terms of $a,b$ ? (I seem to lack the necessary combinatorial tools to tackle it. Yeah I deem it to be combinatorial. Initially thought of constructing some kind of martingale based on the balls fetched and apply optional stopping, but didn't work out...) Does a closed form solution exist? (I have absolutely no idea other than imagination about a numerical implementation.) Is it generalisable to multicoloured balls (more than two) cases? (This one just for fun.)","In a bag there are red balls and blue balls ( ). You conduct the following process. Randomly fetch a ball from the bag, record the colour as the current colour . And throw it away. Go to 2. If there's no ball left, break. Otherwise, randomly fetch a ball from the bag. If the colour coincides with current colour, throw it away, and repeat 2; otherwise, put back the ball, and go back to 1. Qs: What's the probability that the last ball fetched is red, in terms of ? (I seem to lack the necessary combinatorial tools to tackle it. Yeah I deem it to be combinatorial. Initially thought of constructing some kind of martingale based on the balls fetched and apply optional stopping, but didn't work out...) Does a closed form solution exist? (I have absolutely no idea other than imagination about a numerical implementation.) Is it generalisable to multicoloured balls (more than two) cases? (This one just for fun.)","a b a,b>0 a,b","['probability', 'probability-theory', 'discrete-mathematics']"
68,Probability of Winning Coin Tosses - Variable Number of Games,Probability of Winning Coin Tosses - Variable Number of Games,,"Hillary and Trump play a game of coin toss. The coin is fair such that $\mathrm{Pr}(x=H) = \mathrm{Pr}(x=T) = 0.5$ . If it gets a Head (H), Hillary wins, otherwise, Trump wins. They agree in advance that the first player who has won $3$ rounds will collect the entire prize. Coin flipping, however, is interrupted for some reason after $3$ rounds and they got $1$ Head and $2$ Tails. Suppose that they continue to toss the coin afterwards, what is the probability that Henry Hillary will win the entire prize? I saw this question also on Chegg and I got the correct answer, but I drew a tree of the rest of the remaining outcomes. We get: 3T, 3HT, and 3HH. Hillary winning would be HH, so $\mathrm{Pr}(HH) = 3/9 = 1/3$ . But I want to know the proper way to do this problem. For example, what formula is used? The second the coin changed to be biased, I have no idea what to do. In addition, how would you derive the solution is the coin is biased? Ex: Pr(x=H)=0.75? I believe this is an Expected Value problem...looked through some other questions and I couldn't quite figure out the solution. Any solutions or links to duplicate questions (with solutions) would be great. Thank you in advance! P.S. I saw this question but it didn't quite help me... Fair and Unfair coin Probability","Hillary and Trump play a game of coin toss. The coin is fair such that . If it gets a Head (H), Hillary wins, otherwise, Trump wins. They agree in advance that the first player who has won rounds will collect the entire prize. Coin flipping, however, is interrupted for some reason after rounds and they got Head and Tails. Suppose that they continue to toss the coin afterwards, what is the probability that Henry Hillary will win the entire prize? I saw this question also on Chegg and I got the correct answer, but I drew a tree of the rest of the remaining outcomes. We get: 3T, 3HT, and 3HH. Hillary winning would be HH, so . But I want to know the proper way to do this problem. For example, what formula is used? The second the coin changed to be biased, I have no idea what to do. In addition, how would you derive the solution is the coin is biased? Ex: Pr(x=H)=0.75? I believe this is an Expected Value problem...looked through some other questions and I couldn't quite figure out the solution. Any solutions or links to duplicate questions (with solutions) would be great. Thank you in advance! P.S. I saw this question but it didn't quite help me... Fair and Unfair coin Probability",\mathrm{Pr}(x=H) = \mathrm{Pr}(x=T) = 0.5 3 3 1 2 \mathrm{Pr}(HH) = 3/9 = 1/3,"['probability', 'statistics', 'expected-value']"
69,Permutations vs. Combinatorial vs. Factorials vs. Exponents,Permutations vs. Combinatorial vs. Factorials vs. Exponents,,"I'm currently working on a probability course, and I am constantly having trouble figuring out when to use permutations vs. combinations vs. factorials vs. exponents in order to calculate sample size, or in order to complete calculations. I was wondering if there is some kind of set of rules to go by when deciding when to use each? Thank you very much.","I'm currently working on a probability course, and I am constantly having trouble figuring out when to use permutations vs. combinations vs. factorials vs. exponents in order to calculate sample size, or in order to complete calculations. I was wondering if there is some kind of set of rules to go by when deciding when to use each? Thank you very much.",,"['probability', 'permutations', 'combinations', 'factorial']"
70,Show that there does not exist a unique stationary distribution.,Show that there does not exist a unique stationary distribution.,,"So I was doing some self study and came across a proposition in one of my chemical engineering course's prescribed textbooks. I can't quite get the proof out. It's to do with a particle moving through a medium such that when it makes contact with to either of two plates $L$ units apart (i.e. one at $0$ and one at $L$ ), it remains there. Consider that the movement of a single particle follows a random walk which can be described by a Markov chain with states $[0, L]$ where $P(X_n = -1) = p_{-1}$ , $P(X_n = 0) = p_{0}$ and $P(X_n = 1) = p_{1}$ with $p_{-1} + p_{0} + p_{1} = 1$ . Show that if states $0$ and $L$ are completely absorbing, then there does not exist a stationary distribution. Hint: Start by considering ${\pi} = \pi P$ This makes sense intuitively since we have two recurrent classes $\{0\}$ and $\{L\}$ and one transient class $\{1, 2, ..., L - 2, L - 1\}$ . However, once I try and expand ${\pi} = \pi P$ , I don't know how to proceed next. Ideally I'd like a few more hints rather than an answer.","So I was doing some self study and came across a proposition in one of my chemical engineering course's prescribed textbooks. I can't quite get the proof out. It's to do with a particle moving through a medium such that when it makes contact with to either of two plates units apart (i.e. one at and one at ), it remains there. Consider that the movement of a single particle follows a random walk which can be described by a Markov chain with states where , and with . Show that if states and are completely absorbing, then there does not exist a stationary distribution. Hint: Start by considering This makes sense intuitively since we have two recurrent classes and and one transient class . However, once I try and expand , I don't know how to proceed next. Ideally I'd like a few more hints rather than an answer.","L 0 L [0, L] P(X_n = -1) = p_{-1} P(X_n = 0) = p_{0} P(X_n = 1) = p_{1} p_{-1} + p_{0} + p_{1} = 1 0 L {\pi} = \pi P \{0\} \{L\} \{1, 2, ..., L - 2, L - 1\} {\pi} = \pi P","['probability', 'markov-chains', 'chemistry']"
71,Intuitive method of finding probability of getting an onto function from all possible functions from a set to another.,Intuitive method of finding probability of getting an onto function from all possible functions from a set to another.,,"The question I was dealing with is as follows: Let sets $A$ and $B$ have $7$ and $5$ elements, respectively. If one function is selected from all possible defined functions from $A$ to $B$, what is the probability that it is onto? Now, this is how my thought process was: For a relation to classify as a function only an element of $A$ should possess only one image in the co-domain $B$. Thus, each element of $A$ has $5$ elements to choose as its image. Since it is a function from $A$ to $B$, two elements from $A$ mapping to the same image is allowed. And so the total number of possible functions would be $= 5^7$ Next, to choose the number of onto functions, what I thought was: each element of $B$ must be mapped, but not to the same element. Therefore, the first element of $B$ would have $7$ options to choose from, the next one would have $6$ and so on... After every element of $B$ is mapped, there would be $2$ remaining elements in $A$ and they have to be mapped to any element of $B$ so as to become a function. Therefore, those two elements would have to choose any $2$ out of $5$ elements of $B$. Thus the total number of possible onto functions would be = $$(7 \cdot 6 \cdot 5 \cdot 4 \cdot 3) \cdot (5 \cdot 5) = \frac{7! \cdot 5^2}{ 2!}$$ Thus the probability required = $$\frac{\frac{7! \cdot 5^2}{2!}}{5^7}$$ But, the answer was $$\frac{7! \cdot 2}{3 \cdot 5^6}$$ Where did I go wrong?","The question I was dealing with is as follows: Let sets $A$ and $B$ have $7$ and $5$ elements, respectively. If one function is selected from all possible defined functions from $A$ to $B$, what is the probability that it is onto? Now, this is how my thought process was: For a relation to classify as a function only an element of $A$ should possess only one image in the co-domain $B$. Thus, each element of $A$ has $5$ elements to choose as its image. Since it is a function from $A$ to $B$, two elements from $A$ mapping to the same image is allowed. And so the total number of possible functions would be $= 5^7$ Next, to choose the number of onto functions, what I thought was: each element of $B$ must be mapped, but not to the same element. Therefore, the first element of $B$ would have $7$ options to choose from, the next one would have $6$ and so on... After every element of $B$ is mapped, there would be $2$ remaining elements in $A$ and they have to be mapped to any element of $B$ so as to become a function. Therefore, those two elements would have to choose any $2$ out of $5$ elements of $B$. Thus the total number of possible onto functions would be = $$(7 \cdot 6 \cdot 5 \cdot 4 \cdot 3) \cdot (5 \cdot 5) = \frac{7! \cdot 5^2}{ 2!}$$ Thus the probability required = $$\frac{\frac{7! \cdot 5^2}{2!}}{5^7}$$ But, the answer was $$\frac{7! \cdot 2}{3 \cdot 5^6}$$ Where did I go wrong?",,"['probability', 'combinatorics', 'functions']"
72,"Joint differential entropy of sum of random variables: $h(X,X+Y)=h(X,Y)$?",Joint differential entropy of sum of random variables: ?,"h(X,X+Y)=h(X,Y)","I see the following simplification used frequently in the literature, but I have not been able to verify it. Let $X$ and $Y$ be absolutely continuous (i.e. they have pdfs)   $\mathbb{R}^d$-valued random variables. Assume the joint variable   $(X,X+Y)$ is absolutely continuous on $\mathbb{R}^{2d}$. Then    $$h(X,X+Y)=h(X,Y).$$ Here $h$ signifies differential entropy, defined by  $$h(W)=-\int_{\mathbb{R}^{d_W}}f_W(w)\log(f_W(w))\ dw$$ whenever $W$ is an $\mathbb{R}^{d_W}$-valued random variable with pdf $f_W$. Note1: $X$ and $Y$ are not assumed to be independent. Note2: Examples where the lhs is finite but the rhs is not defined would be accepted as a counterexample. I am also wondering, if the statement can be proved, then is it more generally true that $$h(X,g(X,Y))=h(X,Y)$$ where $g$ is a deterministic function of its arguments? This question is similar, but seems to concern Shannon entropy (i.e. discrete variables). Shannon Entropy and Differential Entropy have different sets of properties as discussed in these links answer1 , answer2 , question1 ,and question2 .","I see the following simplification used frequently in the literature, but I have not been able to verify it. Let $X$ and $Y$ be absolutely continuous (i.e. they have pdfs)   $\mathbb{R}^d$-valued random variables. Assume the joint variable   $(X,X+Y)$ is absolutely continuous on $\mathbb{R}^{2d}$. Then    $$h(X,X+Y)=h(X,Y).$$ Here $h$ signifies differential entropy, defined by  $$h(W)=-\int_{\mathbb{R}^{d_W}}f_W(w)\log(f_W(w))\ dw$$ whenever $W$ is an $\mathbb{R}^{d_W}$-valued random variable with pdf $f_W$. Note1: $X$ and $Y$ are not assumed to be independent. Note2: Examples where the lhs is finite but the rhs is not defined would be accepted as a counterexample. I am also wondering, if the statement can be proved, then is it more generally true that $$h(X,g(X,Y))=h(X,Y)$$ where $g$ is a deterministic function of its arguments? This question is similar, but seems to concern Shannon entropy (i.e. discrete variables). Shannon Entropy and Differential Entropy have different sets of properties as discussed in these links answer1 , answer2 , question1 ,and question2 .",,"['probability', 'statistics', 'probability-distributions', 'random-variables', 'entropy']"
73,How many seats should be available such that the probability.. is less than $0.01$?,How many seats should be available such that the probability.. is less than ?,0.01,"Two railway companies respectively deploy one train (to get from city   X to city Y). In total, $1000$ people randomly choose the train,   respectively with probability $\frac{1}{2}$. How many seats should make one railway company available in the train,   such that the probability, that at least one of their passengers needs   to stand, is less than $0.01$? I think for these problem I need to use theorem of De Moivre Laplace. I call total amount of people $n = 1000$ Probability for choose train is $p = \frac{1}{2}$ But formula for it is strange and I'm not sure how use it correct: $$\lim_{n \rightarrow \infty}\mathbb{P}\left(\frac{X-np}{\sqrt{np(1-p)}}\leq x\right) = \Phi(x)$$ When insert everything in formula we have $$\lim_{n \rightarrow \infty} \left(\frac{1000-0.5n}{\sqrt{0.5n(1-0.5)}}\right) = \lim_{n \rightarrow \infty} \left(\frac{1000-0.5n}{\sqrt{0.25n}}\right) = \lim_{n \rightarrow \infty}\left(\frac{1000}{\sqrt{0.25n}} - \sqrt{n}\right) = 0-\infty = -\infty$$ But I see from solution something went wrong :( It was also hard find the correct formula on internet because in script there is strange thing. How solve this correct because I think similar question can asked in lesson and I want do it correct in test.","Two railway companies respectively deploy one train (to get from city   X to city Y). In total, $1000$ people randomly choose the train,   respectively with probability $\frac{1}{2}$. How many seats should make one railway company available in the train,   such that the probability, that at least one of their passengers needs   to stand, is less than $0.01$? I think for these problem I need to use theorem of De Moivre Laplace. I call total amount of people $n = 1000$ Probability for choose train is $p = \frac{1}{2}$ But formula for it is strange and I'm not sure how use it correct: $$\lim_{n \rightarrow \infty}\mathbb{P}\left(\frac{X-np}{\sqrt{np(1-p)}}\leq x\right) = \Phi(x)$$ When insert everything in formula we have $$\lim_{n \rightarrow \infty} \left(\frac{1000-0.5n}{\sqrt{0.5n(1-0.5)}}\right) = \lim_{n \rightarrow \infty} \left(\frac{1000-0.5n}{\sqrt{0.25n}}\right) = \lim_{n \rightarrow \infty}\left(\frac{1000}{\sqrt{0.25n}} - \sqrt{n}\right) = 0-\infty = -\infty$$ But I see from solution something went wrong :( It was also hard find the correct formula on internet because in script there is strange thing. How solve this correct because I think similar question can asked in lesson and I want do it correct in test.",,"['probability', 'probability-theory', 'statistics', 'discrete-mathematics', 'probability-distributions']"
74,Probability of 6 die rolls,Probability of 6 die rolls,,"If two fair $6$-sided dice are tossed six times, find the probability that   the sixth sum obtained is not a repetition. The solution given to me is not very helpful in explaining the steps, and my attempt at it is far away from the final answer. This question appears to be different from the standard die questions I am used to. Intuition and explanation would be appreciated. The Solution: $2*\frac{1}{36}(\frac{35}{36})^5+2*\frac{1}{18}(\frac{7}{18})^5+2*\frac{1}{12}(\frac{11}{12})^5+2*\frac{1}{9}(\frac{8}{9})^5+2*\frac{5}{36}(\frac{31}{36})^5+\frac{1}{6}(\frac{5}{6})^5\approx0.5614$","If two fair $6$-sided dice are tossed six times, find the probability that   the sixth sum obtained is not a repetition. The solution given to me is not very helpful in explaining the steps, and my attempt at it is far away from the final answer. This question appears to be different from the standard die questions I am used to. Intuition and explanation would be appreciated. The Solution: $2*\frac{1}{36}(\frac{35}{36})^5+2*\frac{1}{18}(\frac{7}{18})^5+2*\frac{1}{12}(\frac{11}{12})^5+2*\frac{1}{9}(\frac{8}{9})^5+2*\frac{5}{36}(\frac{31}{36})^5+\frac{1}{6}(\frac{5}{6})^5\approx0.5614$",,"['probability', 'dice']"
75,What does a semicolon denote in the context of probability and statistics?,What does a semicolon denote in the context of probability and statistics?,,"I sometimes see the notation $;$ being used in a statistical context For example, let $f_X(x)$ be the probability density function associated with random variable $X$, then sometimes I see things like $f_X(x| y; \theta)$, where $\theta$ is a set of the mean and the covariance associated with the distribution. What does these $;$ mean? Any reference helps.","I sometimes see the notation $;$ being used in a statistical context For example, let $f_X(x)$ be the probability density function associated with random variable $X$, then sometimes I see things like $f_X(x| y; \theta)$, where $\theta$ is a set of the mean and the covariance associated with the distribution. What does these $;$ mean? Any reference helps.",,"['probability', 'statistics', 'probability-distributions', 'notation', 'statistical-inference']"
76,Conditional joint probabiltiy of a given pair,Conditional joint probabiltiy of a given pair,,"The bivariate PDF of a random pair $(X, Y)$ is given by: $f_{X,Y}(x,y) = 2e^{-x}e^{-2y}$ , $x\ge0, y\ge0$ What is the probability $Y < 4$ given  $X > 1$? I calculated the conditional probability as $f_{Y\mid X}(y) = \frac{f_{X,Y}(x,y)}{f_X(x)}$ From using the above formula I got $f_{Y\mid X}(y) = 2e^{-2y}$, with $f_X(x) = e^{-x}$ I am confused on how to calculate the probability now that I have the equation $f_{Y, X}$","The bivariate PDF of a random pair $(X, Y)$ is given by: $f_{X,Y}(x,y) = 2e^{-x}e^{-2y}$ , $x\ge0, y\ge0$ What is the probability $Y < 4$ given  $X > 1$? I calculated the conditional probability as $f_{Y\mid X}(y) = \frac{f_{X,Y}(x,y)}{f_X(x)}$ From using the above formula I got $f_{Y\mid X}(y) = 2e^{-2y}$, with $f_X(x) = e^{-x}$ I am confused on how to calculate the probability now that I have the equation $f_{Y, X}$",,"['probability', 'bivariate-distributions']"
77,How to prove the tightness of Markov's bound?,How to prove the tightness of Markov's bound?,,"Show that Markov's inequality is as tight as it possible. Given a positive integer $k$, describe a random variable $X$ that assumes only non-negative values: $$\Pr[X \geq k E[X] ] = 1/k.$$ Using Markov's bound, we can show at most $1/k$. But how to show equality?! My question to be exact what is the idea to prove the tightness of this bound!","Show that Markov's inequality is as tight as it possible. Given a positive integer $k$, describe a random variable $X$ that assumes only non-negative values: $$\Pr[X \geq k E[X] ] = 1/k.$$ Using Markov's bound, we can show at most $1/k$. But how to show equality?! My question to be exact what is the idea to prove the tightness of this bound!",,"['probability', 'probability-theory', 'inequality', 'random-variables']"
78,What is the expected number of tyres that are installed in their original positions?,What is the expected number of tyres that are installed in their original positions?,,"After summer, the winter tyres of a car (with four wheels) are to be put back. However, the owner has forgotten which tyre goes to which wheel, and the tyres are installed `randomly', each of the $4! = 24$ permutations being equally likely. What is the expected number of tyres that are installed in their   original positions? Expected no. of tyres installed in original position = $P(1\ \text{tyre}) + 2P(2\ \text{tyre}) + 3P(3\ \text{tyre}) + 4P(4\ \text{tyre})$ I'm stuck after that. When counting $P(1\ \text{tyre})$ am I allowed to take the combination that all 4 tyres are in their right position ? Because that also includes 1 tyre in its original position... If yes then $P(1\ \text{tyre}) = 13/24$ Or must I find the probability that exactly 1 tyre is in the original position ? If so, then what is the probability that 3 tyres are in their original position ? Because having 3 tyres in the original position would mean the 4th tyre must be in the original position ?","After summer, the winter tyres of a car (with four wheels) are to be put back. However, the owner has forgotten which tyre goes to which wheel, and the tyres are installed `randomly', each of the $4! = 24$ permutations being equally likely. What is the expected number of tyres that are installed in their   original positions? Expected no. of tyres installed in original position = $P(1\ \text{tyre}) + 2P(2\ \text{tyre}) + 3P(3\ \text{tyre}) + 4P(4\ \text{tyre})$ I'm stuck after that. When counting $P(1\ \text{tyre})$ am I allowed to take the combination that all 4 tyres are in their right position ? Because that also includes 1 tyre in its original position... If yes then $P(1\ \text{tyre}) = 13/24$ Or must I find the probability that exactly 1 tyre is in the original position ? If so, then what is the probability that 3 tyres are in their original position ? Because having 3 tyres in the original position would mean the 4th tyre must be in the original position ?",,"['probability', 'combinatorics']"
79,Coin tosses until doubling amount,Coin tosses until doubling amount,,"Two friends are playing the following game: The first player named A has an initial amount of M dollars. By tossing a fair coin once, he bets 1 dollar and if he gets heads, player B gives him 1 dollar, while if he gets tails, A gives his 1 dollar to B. The game ends when A doubles his money or when he loses it all. What is the probability he doubles his money? OK at the first coin toss he starts with M dollars and bets 1. With probability 1/2 he now has M+1 and with probability 1/2 he has M-1. At the second round, he bets another dollar and with probability 1/2 he has M+2 and with probability 1/2 he has M-2 and so on. Since the events are independent, the total probability of winning each time is 1/2*1/2 etc. How do we calculate the total probability? I clarify that each time he bets only 1 dollar.","Two friends are playing the following game: The first player named A has an initial amount of M dollars. By tossing a fair coin once, he bets 1 dollar and if he gets heads, player B gives him 1 dollar, while if he gets tails, A gives his 1 dollar to B. The game ends when A doubles his money or when he loses it all. What is the probability he doubles his money? OK at the first coin toss he starts with M dollars and bets 1. With probability 1/2 he now has M+1 and with probability 1/2 he has M-1. At the second round, he bets another dollar and with probability 1/2 he has M+2 and with probability 1/2 he has M-2 and so on. Since the events are independent, the total probability of winning each time is 1/2*1/2 etc. How do we calculate the total probability? I clarify that each time he bets only 1 dollar.",,['probability']
80,expected value and variance of a binomial variable,expected value and variance of a binomial variable,,"If $X \sim \operatorname{Bin}(10, \theta)$, $p(2)=0.1$ and $p(3)=0.2$, find $\operatorname{E}(X)$ and $\operatorname{V}(X)$ I'm a little lost here so the first thing I did was: $$p(2)={10 \choose 2}\theta^2(1-\theta)^8=0.1$$ then, $$45\theta^2(1-\theta)^8=0.1 \rightarrow \theta^2(1-\theta)^8=\frac{1}{450}$$ and, $$\theta(1-\theta)^4=\sqrt{\frac{1}{450}}$$  I did the same with $p(3)=0.2$, but I'm not very sure that's what I am supposed to do.","If $X \sim \operatorname{Bin}(10, \theta)$, $p(2)=0.1$ and $p(3)=0.2$, find $\operatorname{E}(X)$ and $\operatorname{V}(X)$ I'm a little lost here so the first thing I did was: $$p(2)={10 \choose 2}\theta^2(1-\theta)^8=0.1$$ then, $$45\theta^2(1-\theta)^8=0.1 \rightarrow \theta^2(1-\theta)^8=\frac{1}{450}$$ and, $$\theta(1-\theta)^4=\sqrt{\frac{1}{450}}$$  I did the same with $p(3)=0.2$, but I'm not very sure that's what I am supposed to do.",,"['probability', 'statistical-inference']"
81,how to find probability of two coins,how to find probability of two coins,,"Rudolph flips two fair coins, each with a half probability of getting heads. What is the probability he flips two tails in two tosses? So I was thinking $0.5 + 0.5 = 1$, but I got it wrong. I guess it doesn't make sense, because he could flip heads and then flip tails, or flip tails then flip heads, or flip two heads. How do I find the probability? Do I subtract, or divide? SAT Math.","Rudolph flips two fair coins, each with a half probability of getting heads. What is the probability he flips two tails in two tosses? So I was thinking $0.5 + 0.5 = 1$, but I got it wrong. I guess it doesn't make sense, because he could flip heads and then flip tails, or flip tails then flip heads, or flip two heads. How do I find the probability? Do I subtract, or divide? SAT Math.",,['probability']
82,Intuition behind the normal distribution,Intuition behind the normal distribution,,"The probability density of the normal distribution is :  $$ f(x) = \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$ And for a random variable $X$ such that : $X \sim \mathcal N (0, 1)$ then we have : $$f(x) = \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}x^2}$$ Yet what is the intuition behind this formula ? Why is there the number $\pi$ and $e$ in the formula of the probability density of the normal distribution ? Moreover, in a lot of my exercices on the normal distribution it's always saying at the beginning of the exercice : Let $X$ be a random variable that follow a normal distribution. Yet, how can we know that a random variable follows a normal distribution ? For example let's say we are studying some process, and more spefically the behaviour of a random variable. Then how can we know, and from which properties of this random variable we can say that it follows a normal distribution ?","The probability density of the normal distribution is :  $$ f(x) = \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$ And for a random variable $X$ such that : $X \sim \mathcal N (0, 1)$ then we have : $$f(x) = \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}x^2}$$ Yet what is the intuition behind this formula ? Why is there the number $\pi$ and $e$ in the formula of the probability density of the normal distribution ? Moreover, in a lot of my exercices on the normal distribution it's always saying at the beginning of the exercice : Let $X$ be a random variable that follow a normal distribution. Yet, how can we know that a random variable follows a normal distribution ? For example let's say we are studying some process, and more spefically the behaviour of a random variable. Then how can we know, and from which properties of this random variable we can say that it follows a normal distribution ?",,"['probability', 'probability-theory', 'probability-distributions', 'normal-distribution', 'intuition']"
83,Lognormal random variable times a constant,Lognormal random variable times a constant,,"If $X$ is a random variable lognormally distributed, then can we say that $Y=cX$ for $x\in \mathbb{R}^+$ is lognormal too? I think that if $c$ is large enough the central limit theorem applies and $Y$ will be normal. But I guess for reasonable values of $c$ (for example $c<30$) maybe it is not true. Mostly I'm thinking in the case when we need to change the units of $X$. Does it changing the units of a quantity change also its distribution? Thanks in advance.","If $X$ is a random variable lognormally distributed, then can we say that $Y=cX$ for $x\in \mathbb{R}^+$ is lognormal too? I think that if $c$ is large enough the central limit theorem applies and $Y$ will be normal. But I guess for reasonable values of $c$ (for example $c<30$) maybe it is not true. Mostly I'm thinking in the case when we need to change the units of $X$. Does it changing the units of a quantity change also its distribution? Thanks in advance.",,"['probability', 'statistics']"
84,A bag contains 9 balls 3 of which are blue,A bag contains 9 balls 3 of which are blue,,"A bag contains 9 balls 3 of which are blue. Suppose one draws one ball at a time, until the bag is empty. What is the probability of drawing three blue balls consecutively?","A bag contains 9 balls 3 of which are blue. Suppose one draws one ball at a time, until the bag is empty. What is the probability of drawing three blue balls consecutively?",,['probability']
85,"Expected value of $x^t\Sigma x$ for multivariate normal distribution $N(0,\Sigma)$",Expected value of  for multivariate normal distribution,"x^t\Sigma x N(0,\Sigma)","For standard normal distribution, the expected value of $x^2$ is $1$. A natural question is that in the multivariate case, what is the expected value of $x^t\Sigma x$ for multivariate normal distribution $x \sim N(0,\Sigma)$? I have difficulty to carry out the integral, but would guess the result is related to the norm of $\Sigma$.","For standard normal distribution, the expected value of $x^2$ is $1$. A natural question is that in the multivariate case, what is the expected value of $x^t\Sigma x$ for multivariate normal distribution $x \sim N(0,\Sigma)$? I have difficulty to carry out the integral, but would guess the result is related to the norm of $\Sigma$.",,"['probability', 'statistics', 'normal-distribution', 'gaussian-integral']"
86,Why do some pdfs y axis values greater than 1,Why do some pdfs y axis values greater than 1,,I've been trying to wrap my head around different types of pdf. Do pdfs show relative probabilities? Take this example of the exponential: When lamba=1.5 p(0)=1.4 Obviously the probability of something cannot be 1.4 Is this a relative probabilty? We can estimate that p(1)=0.3 is the event (x=0) 1.4/0.3 = 4.67 times more likely to occur than the event (x=1) ? I suppose I don't really know what a pdf really IS Is the reason you get values higher than 1.0 on the y axis just a consequence of scaling the pdf so the total area is 1?,I've been trying to wrap my head around different types of pdf. Do pdfs show relative probabilities? Take this example of the exponential: When lamba=1.5 p(0)=1.4 Obviously the probability of something cannot be 1.4 Is this a relative probabilty? We can estimate that p(1)=0.3 is the event (x=0) 1.4/0.3 = 4.67 times more likely to occur than the event (x=1) ? I suppose I don't really know what a pdf really IS Is the reason you get values higher than 1.0 on the y axis just a consequence of scaling the pdf so the total area is 1?,,"['probability', 'statistics', 'density-function']"
87,Condition probability distributions: Two people flipping fair coins,Condition probability distributions: Two people flipping fair coins,,"Suppose that two people are playing a game where they each flip a fair coin 100 times. The winner of this game is the person who has flipped the most heads. What is the expected number of heads flipped by the winner? I understand that in general the probability of a given number of heads flipped will be given by the binomial distribution and we can approximate it using a normal distribution. On average, we expect them to both flip around the same number of heads, but conditional on the fact that there will be a winner, we should expect the number of heads of the winner to be slightly above 50. How does one get the distribution of the winning player from the initial distribution?","Suppose that two people are playing a game where they each flip a fair coin 100 times. The winner of this game is the person who has flipped the most heads. What is the expected number of heads flipped by the winner? I understand that in general the probability of a given number of heads flipped will be given by the binomial distribution and we can approximate it using a normal distribution. On average, we expect them to both flip around the same number of heads, but conditional on the fact that there will be a winner, we should expect the number of heads of the winner to be slightly above 50. How does one get the distribution of the winning player from the initial distribution?",,"['probability', 'probability-distributions', 'conditional-expectation']"
88,"A stick is broken into two pieces, at a uniformly random chosen break point. Find the CDF.","A stick is broken into two pieces, at a uniformly random chosen break point. Find the CDF.",,"I'm having trouble understanding how the CDF is found in the solution below: We can assume the units are chosen so that the stick has length $1$. Let $L$ be the length of the longer piece, and let the break point be $U \sim Unif(0,1)$. For any $l \in [1/2,1]$, observe that $L<l$ is equivalent to $U<l,1-U<l$, which can be written as $1-l<U<l$. We can thus obtain $L$'s CDF as $$F_L(l) = P(L<l)=P(1-l<U<l)=2l-1$$ Can someone please explain why $L<l$ is equivalent to $U<l,1-U<l$? Isn't the break point $U$ in between $[1/2,1]$?","I'm having trouble understanding how the CDF is found in the solution below: We can assume the units are chosen so that the stick has length $1$. Let $L$ be the length of the longer piece, and let the break point be $U \sim Unif(0,1)$. For any $l \in [1/2,1]$, observe that $L<l$ is equivalent to $U<l,1-U<l$, which can be written as $1-l<U<l$. We can thus obtain $L$'s CDF as $$F_L(l) = P(L<l)=P(1-l<U<l)=2l-1$$ Can someone please explain why $L<l$ is equivalent to $U<l,1-U<l$? Isn't the break point $U$ in between $[1/2,1]$?",,"['probability', 'uniform-distribution']"
89,Expected value of $g(X)$.,Expected value of .,g(X),"If $\mathrm{E}(X) = \sum_{x\in I} x\,\mathrm{P}(X=x)$, how can I deduce that $E(g(X)) = \sum_{x\in ?} g(x)\,\mathrm{P}(X=x)$? I don't see why it isn't $E(g(X)) = \sum_{g(x)\in ?} g(x)\,\mathrm{P}(X=g(x))$ instead. Are these simply definitions, or is there any logic behind this notation?","If $\mathrm{E}(X) = \sum_{x\in I} x\,\mathrm{P}(X=x)$, how can I deduce that $E(g(X)) = \sum_{x\in ?} g(x)\,\mathrm{P}(X=x)$? I don't see why it isn't $E(g(X)) = \sum_{g(x)\in ?} g(x)\,\mathrm{P}(X=g(x))$ instead. Are these simply definitions, or is there any logic behind this notation?",,"['probability', 'notation']"
90,Two wins in a row in a game involving three players,Two wins in a row in a game involving three players,,"Three players, let's call them $A,B$ and $C$, play a game of chess. The first match is between $A$ and $B$. The winner will go on to play the third player (who is $C$ in the second match). The game continues until a player win $2$ matches in a row, who will be the eventual winner. The chance for each player to win a match is one half. Find the chance of winning the game for $A,B$ and $C$.","Three players, let's call them $A,B$ and $C$, play a game of chess. The first match is between $A$ and $B$. The winner will go on to play the third player (who is $C$ in the second match). The game continues until a player win $2$ matches in a row, who will be the eventual winner. The chance for each player to win a match is one half. Find the chance of winning the game for $A,B$ and $C$.",,['probability']
91,Do the sample space ($\Omega$) and the $\sigma$-algebra ($\mathcal{F}$) of a probability space form a topology space?,Do the sample space () and the -algebra () of a probability space form a topology space?,\Omega \sigma \mathcal{F},"[I realize that this question may be too basic and likely to contain a fundamental misunderstanding, but similar questions such as this one on the site are beyond my math level. So please assume very limited understanding of measure theory. If at all possible an answer in plain English would be ideal.] A probability space is defined as the triple ($\Omega,\mathcal{F},P$). The sample space and the sigma-algebra of events of a probability space -($\Omega,\mathcal{F}$)- would seem to fulfill the conditions of a topological space , ($X,\tau$): There is always a null event ($P(\{\emptyset\} = 0)$) and a certain event ($P(\{\Omega\})=1$). This corresponds to the condition, ""The empty set and $X$ itself belong to $\tau$"" in a topology sapce. Any union of events belongs to $\mathcal{F}$ (closed under countable unions). The counterpart in topology spaces: ""Any (finite or infinite) union of members of $\tau$ still belongs to $\tau$."" The intersection of any number of events belongs to $\mathcal{F}$ (closed under countable intersections). In topology spaces: ""The intersection of any finite number of members of $\tau$ still belongs to $\tau$."" Can we then say that the sample space $\Omega$ and the $\sigma$-algebra, ($\Omega, \mathcal{F}$), form a topology on $\Omega$? I believe that ($\Omega, \mathcal{F}$) also fulfills the conditions of a measurable space, so I may be indirectly asking the difference between a topological and a measurable space . Is the coincidence of criteria an accident among conceptually disparate mathematical objects: probability (or measurable) spaces v. topology spaces? Is it just a matter of the geometric component of topology? Intuitively, I see that when applying a function to these spaces, in the case of topology we may want to relate somehow subsets by proximity; and I can see how in assigning a probability measure in [0,1] to different events, the result will be similar the more related (closer?) these events are... Post-mortem notes: In a $\sigma$-algebra any countable union of elements in  $\mathcal{F}$ have to be contained in  $\mathcal{F}$. On the other hand for a topological space it is any arbritrary union (finite or infinite) of the elments of the topology $\tau$ has to be contained in $\tau$.      $\sim \text{Comment 1 below.}$ By De Morgan's law and closure under complements of probability spaces, $\mathcal{F}$ is similary closed under countable intersections . Conversely, in topology it is the intersection of finite elements of $\tau$ that remains contained in $\tau$.","[I realize that this question may be too basic and likely to contain a fundamental misunderstanding, but similar questions such as this one on the site are beyond my math level. So please assume very limited understanding of measure theory. If at all possible an answer in plain English would be ideal.] A probability space is defined as the triple ($\Omega,\mathcal{F},P$). The sample space and the sigma-algebra of events of a probability space -($\Omega,\mathcal{F}$)- would seem to fulfill the conditions of a topological space , ($X,\tau$): There is always a null event ($P(\{\emptyset\} = 0)$) and a certain event ($P(\{\Omega\})=1$). This corresponds to the condition, ""The empty set and $X$ itself belong to $\tau$"" in a topology sapce. Any union of events belongs to $\mathcal{F}$ (closed under countable unions). The counterpart in topology spaces: ""Any (finite or infinite) union of members of $\tau$ still belongs to $\tau$."" The intersection of any number of events belongs to $\mathcal{F}$ (closed under countable intersections). In topology spaces: ""The intersection of any finite number of members of $\tau$ still belongs to $\tau$."" Can we then say that the sample space $\Omega$ and the $\sigma$-algebra, ($\Omega, \mathcal{F}$), form a topology on $\Omega$? I believe that ($\Omega, \mathcal{F}$) also fulfills the conditions of a measurable space, so I may be indirectly asking the difference between a topological and a measurable space . Is the coincidence of criteria an accident among conceptually disparate mathematical objects: probability (or measurable) spaces v. topology spaces? Is it just a matter of the geometric component of topology? Intuitively, I see that when applying a function to these spaces, in the case of topology we may want to relate somehow subsets by proximity; and I can see how in assigning a probability measure in [0,1] to different events, the result will be similar the more related (closer?) these events are... Post-mortem notes: In a $\sigma$-algebra any countable union of elements in  $\mathcal{F}$ have to be contained in  $\mathcal{F}$. On the other hand for a topological space it is any arbritrary union (finite or infinite) of the elments of the topology $\tau$ has to be contained in $\tau$.      $\sim \text{Comment 1 below.}$ By De Morgan's law and closure under complements of probability spaces, $\mathcal{F}$ is similary closed under countable intersections . Conversely, in topology it is the intersection of finite elements of $\tau$ that remains contained in $\tau$.",,"['probability', 'general-topology', 'measure-theory']"
92,Not understanding division in Birthday Paradox,Not understanding division in Birthday Paradox,,"I am reading Scientific American's explanation for birthday paradox here I understand everything in the article up to Every one of the 253 combinations has the same odds, 99.726027   percent, of not being a match. If you multiply 99.726027 percent by   99.726027 253 times, or calculate (364/365)253, you'll find there's a 49.952 percent chance that all 253 comparisons contain no matches. When I compute $364/365 \times 253$, the answer is $252.306$ Why is the article saying it is $49.952%$?","I am reading Scientific American's explanation for birthday paradox here I understand everything in the article up to Every one of the 253 combinations has the same odds, 99.726027   percent, of not being a match. If you multiply 99.726027 percent by   99.726027 253 times, or calculate (364/365)253, you'll find there's a 49.952 percent chance that all 253 comparisons contain no matches. When I compute $364/365 \times 253$, the answer is $252.306$ Why is the article saying it is $49.952%$?",,"['probability', 'arithmetic']"
93,Combinatorial identity $i{n \choose i } =n {n-1 \choose i - 1}$,Combinatorial identity,i{n \choose i } =n {n-1 \choose i - 1},"In the first course of probability by sheldon ross on page 155 at the bottom. It is the proof for the expectation of Binomial distribution. It uses the identity: $$i{n \choose i } =n {n-1 \choose i - 1}$$ Since learning combinatorics, I have been trying to think of this intuitively. So is there an intuitive proof for this identity, rather than just using brute force?","In the first course of probability by sheldon ross on page 155 at the bottom. It is the proof for the expectation of Binomial distribution. It uses the identity: $$i{n \choose i } =n {n-1 \choose i - 1}$$ Since learning combinatorics, I have been trying to think of this intuitively. So is there an intuitive proof for this identity, rather than just using brute force?",,"['probability', 'combinatorics']"
94,Find probabilities and probability mass function,Find probabilities and probability mass function,,"Suppose an urn contains three balls, once black, one white, and one green. Assume that balls are repeatedly drawn, one at a time, with replacement, and let X denote the number of draws until each colour appears at least once. (a) Find the probabilities $P(X > n)$ for $n = 0, 1, 2,....$ (b) Find the probability mass function (pmf) of X For part (a) we can say $P(X>n) = P(A_n \cup B_n \cup C_n)$ where $A_n$ is the event that no black balls are drawn, $B_n$ is the event that no white balls are drawn and $C_n$ is the event that no green balls are drawn So, $P(A_n \cup B_n \cup C_n) = P(A_n) + P(B_n) + P(C_n) - P(A_n \cap B_n) - P(A_n \cap C_n) - P(B_n \cap C_n) + P(A_n \cap B_n \cap C_n)$ How do I compute these probabilities ?","Suppose an urn contains three balls, once black, one white, and one green. Assume that balls are repeatedly drawn, one at a time, with replacement, and let X denote the number of draws until each colour appears at least once. (a) Find the probabilities $P(X > n)$ for $n = 0, 1, 2,....$ (b) Find the probability mass function (pmf) of X For part (a) we can say $P(X>n) = P(A_n \cup B_n \cup C_n)$ where $A_n$ is the event that no black balls are drawn, $B_n$ is the event that no white balls are drawn and $C_n$ is the event that no green balls are drawn So, $P(A_n \cup B_n \cup C_n) = P(A_n) + P(B_n) + P(C_n) - P(A_n \cap B_n) - P(A_n \cap C_n) - P(B_n \cap C_n) + P(A_n \cap B_n \cap C_n)$ How do I compute these probabilities ?",,"['probability', 'probability-distributions']"
95,You flip a coin until you get a total of n heads. What is the expected number of flips this will take? Expectation and Variance [closed],You flip a coin until you get a total of n heads. What is the expected number of flips this will take? Expectation and Variance [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question You flip a coin until you get a total of n heads. What is the expected number of flips this will take? In terms of Expectation, E[x] and Variance Var(x) . Edit: The heads don't have to be consecutive.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question You flip a coin until you get a total of n heads. What is the expected number of flips this will take? In terms of Expectation, E[x] and Variance Var(x) . Edit: The heads don't have to be consecutive.",,"['probability', 'expectation']"
96,A subset $P$(may be void also) is selected at random from set $A$ and the set $A$ is then reconstructed by replacing the elements of $P$.,A subset (may be void also) is selected at random from set  and the set  is then reconstructed by replacing the elements of .,P A A P,"$A$ is a set containing $n$ elements,a subset $P$(may be void also) is selected at random from set $A$ and the set $A$ is then reconstructed by replacing the elements of $P.$A subset $Q$(may be void also)of $A$ is again chosen at random. $(A)$What is the probability that number of elements in $P$ is equal to the number of elements in $Q?$ $(B)$What is the probability that $P\cap Q=\emptyset?$ I could not solve this question,i have no idea how to even start it.Please help me.","$A$ is a set containing $n$ elements,a subset $P$(may be void also) is selected at random from set $A$ and the set $A$ is then reconstructed by replacing the elements of $P.$A subset $Q$(may be void also)of $A$ is again chosen at random. $(A)$What is the probability that number of elements in $P$ is equal to the number of elements in $Q?$ $(B)$What is the probability that $P\cap Q=\emptyset?$ I could not solve this question,i have no idea how to even start it.Please help me.",,['probability']
97,Why take the logarithm of likelihood function when finding MLE,Why take the logarithm of likelihood function when finding MLE,,"To calculate the MLE, I see that we can easily take the logarithm of the likelihood function like so: https://en.wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood I have the following likelihood function: L(λ) = (λ^3)(e^(-9λ)) I know that I can use the procedure described in the wikipedia article (take the natural log of both sides and find the MLE). Can I also take the derivative and use the product rule?","To calculate the MLE, I see that we can easily take the logarithm of the likelihood function like so: https://en.wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood I have the following likelihood function: L(λ) = (λ^3)(e^(-9λ)) I know that I can use the procedure described in the wikipedia article (take the natural log of both sides and find the MLE). Can I also take the derivative and use the product rule?",,"['probability', 'statistics']"
98,"How do mathematician make sense of ""outcome"" and ""events"" in probability?","How do mathematician make sense of ""outcome"" and ""events"" in probability?",,"One of the biggest challenge for me to understand probability is to make sense of this concept of outcomes and events. To put it plainly, it just doesn't feel like mathematics anymore when we talk about Head or Tail. We are used to deal with sets in all other areas of mathematics e.g. the set of integers, the real line, a vector, a function, a set of sets,...Then you are hit in the face with ""events"" which are set of ""outcomes"" of an ""experiment"". I cannot grasp why I feel more comfortable when people say that ""$1$ is an element of the integers"" than when people say ""head is an element in the space of outcomes which contains head and tail"". Perhaps the latter doesn't contain any numbers? You cannot put it into a computer? Perhaps you can call ""1"" a number whereas head is ...an linguistic variable, a word, a string, a binary number, an ""outcome""...what is head? Perhaps the mapping is ill defined in my mind. You have ""1"", you define a function ""+"" and you take another number ""1"", it cranks out ""2"". Whereas you have ""head"" and you define a random variable which so happens to assign the number ""1/2"" to head and that is somehow a function. Does anyone share my concerns when learning about probability? How can I let myself to see that concepts such as ""events"", ""outcomes"", ""sample space"", ""random variable"" are not so far fetched compared to other branches of mathematics?","One of the biggest challenge for me to understand probability is to make sense of this concept of outcomes and events. To put it plainly, it just doesn't feel like mathematics anymore when we talk about Head or Tail. We are used to deal with sets in all other areas of mathematics e.g. the set of integers, the real line, a vector, a function, a set of sets,...Then you are hit in the face with ""events"" which are set of ""outcomes"" of an ""experiment"". I cannot grasp why I feel more comfortable when people say that ""$1$ is an element of the integers"" than when people say ""head is an element in the space of outcomes which contains head and tail"". Perhaps the latter doesn't contain any numbers? You cannot put it into a computer? Perhaps you can call ""1"" a number whereas head is ...an linguistic variable, a word, a string, a binary number, an ""outcome""...what is head? Perhaps the mapping is ill defined in my mind. You have ""1"", you define a function ""+"" and you take another number ""1"", it cranks out ""2"". Whereas you have ""head"" and you define a random variable which so happens to assign the number ""1/2"" to head and that is somehow a function. Does anyone share my concerns when learning about probability? How can I let myself to see that concepts such as ""events"", ""outcomes"", ""sample space"", ""random variable"" are not so far fetched compared to other branches of mathematics?",,"['probability', 'probability-theory', 'soft-question', 'self-learning', 'intuition']"
99,"What is the probability of getting a 3 or higher on a six sided die, if I reroll after failing the first time?","What is the probability of getting a 3 or higher on a six sided die, if I reroll after failing the first time?",,"Just as the question says... What is the probability of rolling a 3 or higher on a six sided die, if I reroll the die a second time when I fail the first time?","Just as the question says... What is the probability of rolling a 3 or higher on a six sided die, if I reroll the die a second time when I fail the first time?",,['probability']
