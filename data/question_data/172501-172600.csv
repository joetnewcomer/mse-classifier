,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Concentration inequality for Lipschitz Function,Concentration inequality for Lipschitz Function,,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, $(X_n:\Omega\rightarrow \mathbb{R}^m)_n$ be a sequence of i.i.d. random variables and let $L:\mathbb{R}^m\rightarrow [0,\infty)$ be Lipschitz.  Let $\mu_n:=\frac1{n} \sum_{k=1}^n \delta_{X_k}$ .  Are there conditions under which: $$ \mathbb{P}\left(|\mathbb{E}_{X\sim\mu_n}[L(X)]-\mathbb{E}_{X\sim Law(X_1)}[L(X)]|\geq t\right)\leq \exp\left( -t^2 \right), $$ where $c>0$ is some constant?","Let be a probability space, be a sequence of i.i.d. random variables and let be Lipschitz.  Let .  Are there conditions under which: where is some constant?","(\Omega,\mathcal{F},\mathbb{P}) (X_n:\Omega\rightarrow \mathbb{R}^m)_n L:\mathbb{R}^m\rightarrow [0,\infty) \mu_n:=\frac1{n} \sum_{k=1}^n \delta_{X_k} 
\mathbb{P}\left(|\mathbb{E}_{X\sim\mu_n}[L(X)]-\mathbb{E}_{X\sim Law(X_1)}[L(X)]|\geq t\right)\leq \exp\left(
-t^2
\right),
 c>0","['probability-theory', 'statistics', 'concentration-of-measure', 'large-deviation-theory']"
1,Changing the sequential probabilities but reaching the same final states - possible? [closed],Changing the sequential probabilities but reaching the same final states - possible? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . Improve this question I am not very good at math-speak and I hope my explanation is clear below. I will provide an example & hopefully I am not missing something obvious in thinking about this problem. Question: Consider the case where you have the following distribution; $A : P(A)$ $B : P(B)$ $P(A) + P(B) = 1.$ furthermore, in the case of A, events C,D,E and F follow, again with P(C),P(D),P(E) AND P(F) so that you have sum of P(C), ... , P(F) = 1. i.e in all events A - F all scenarios are accounted for. Worth noting here that there does not necessarily have to be 4 different sub-events, although I think that the scenario changes once you have more than 2. Anyway, now we have something like this; $P(A)*P(C) + P(A)*P(B) + P(A)*P(C) + P(A)*P(D) + P(B) = 1.$ my question is this; Is it possible to re-represent this using 4 different events over 4 different combinations of the events, combining them to reach the same end states? i.e $P(X)^4 = P(A)*P(C)$ $P(Y)^4 = P(A)*P(D)$ $P(Z)^4 = P(A)*P(E)$ $P(W)^4 = P(A)*P(F)$ and that P(B) is equal to all ""mismatching"" sequences. i.e $P(B) = 1 - (P(X)^4 + P(Y)^4 + P(Z)^4 + P(W)^4)$ . One way to ask is under which conditions can you take the 4th root of all P(C), P(D), P(E), P(F) and still have the result + P(B) = 1, Furthermore, does this become simpler if the X,Y,Z or W probabilities can differ, i.e $P(X1)*P(X2)*P(X3)*P(X4) = P(A)*P(C), but P(X1) + P(Y1) + P(Z1) + P(W1) = 1$ etc.. In an example - On a game show you have 1 bag with white balls and black balls. If you reach in and grab get a white ball you win prize A with probability A, prize B with probability B, prize C with probability C and prize D with probability D. A black ball gets you nothing. The game show host wants to change the game but not the probability of winning the prizes. He wants to have 4 bags, each bag has a ball with A, B, C or D on it. The player pics on ball from each bag and is only awarded a prize if all of the balls match, otherwise gets nothing. Question - is this possible? Another example for fun... An intruder with a random hand (haha...) changes the switches in your house. You have initially a switch that could go on or off. It is on with probability P(A) and off with probability P(B). When it is on there is a probability P(C) that the light is green, P(D) that the light is red, P(E) that the light is blue and P(F) that the light is yellow. You want to make things easier for your intruder. You rebuild the house and now you have 4 switches, and each switch has 4 states, one for every colour. If all the switches are on the same colour, the light shines with that colour. If there are any mismatches the light is off. Is your intruder able to re-create the probability of the final states of the lights every time that you walk in after he has been there?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . Improve this question I am not very good at math-speak and I hope my explanation is clear below. I will provide an example & hopefully I am not missing something obvious in thinking about this problem. Question: Consider the case where you have the following distribution; furthermore, in the case of A, events C,D,E and F follow, again with P(C),P(D),P(E) AND P(F) so that you have sum of P(C), ... , P(F) = 1. i.e in all events A - F all scenarios are accounted for. Worth noting here that there does not necessarily have to be 4 different sub-events, although I think that the scenario changes once you have more than 2. Anyway, now we have something like this; my question is this; Is it possible to re-represent this using 4 different events over 4 different combinations of the events, combining them to reach the same end states? i.e and that P(B) is equal to all ""mismatching"" sequences. i.e . One way to ask is under which conditions can you take the 4th root of all P(C), P(D), P(E), P(F) and still have the result + P(B) = 1, Furthermore, does this become simpler if the X,Y,Z or W probabilities can differ, i.e etc.. In an example - On a game show you have 1 bag with white balls and black balls. If you reach in and grab get a white ball you win prize A with probability A, prize B with probability B, prize C with probability C and prize D with probability D. A black ball gets you nothing. The game show host wants to change the game but not the probability of winning the prizes. He wants to have 4 bags, each bag has a ball with A, B, C or D on it. The player pics on ball from each bag and is only awarded a prize if all of the balls match, otherwise gets nothing. Question - is this possible? Another example for fun... An intruder with a random hand (haha...) changes the switches in your house. You have initially a switch that could go on or off. It is on with probability P(A) and off with probability P(B). When it is on there is a probability P(C) that the light is green, P(D) that the light is red, P(E) that the light is blue and P(F) that the light is yellow. You want to make things easier for your intruder. You rebuild the house and now you have 4 switches, and each switch has 4 states, one for every colour. If all the switches are on the same colour, the light shines with that colour. If there are any mismatches the light is off. Is your intruder able to re-create the probability of the final states of the lights every time that you walk in after he has been there?","A : P(A) B : P(B) P(A) + P(B) = 1. P(A)*P(C) + P(A)*P(B) + P(A)*P(C) + P(A)*P(D) + P(B) = 1. P(X)^4 = P(A)*P(C) P(Y)^4 = P(A)*P(D) P(Z)^4 = P(A)*P(E) P(W)^4 = P(A)*P(F) P(B) = 1 - (P(X)^4 + P(Y)^4 + P(Z)^4 + P(W)^4) P(X1)*P(X2)*P(X3)*P(X4) = P(A)*P(C), but P(X1) + P(Y1) + P(Z1) + P(W1) = 1","['probability', 'combinatorics', 'statistics', 'discrete-mathematics']"
2,Log Sum Exp trick confusion with probability distributions,Log Sum Exp trick confusion with probability distributions,,"I am struggling to understand how exactly the log-sum-exp trick is applied in mixture models (say Gaussian mixture models) and the EM algorithm. It is well known that parameter estimation for a mixture: $$ f(x)= \sum_{i=1}^K \alpha_i \mathcal{N}(\mu_i,\Sigma_i) = \sum_{i=1}^K \alpha_ie^{-(x-\mu_i)^T\,\, \Sigma_i^{-1} \,\, (x-\mu_i)} $$ can be achieved with the EM algorithm. The E-step corresponds in estimating in which distribution $i$ a given $x$ comes from. This probability is given by: $$ p(x\in \mathcal{N(\mu_i,\Sigma_i)}) = \arg\max_i \frac{\alpha_i e^{-(x-\mu_i)^T\,\, \Sigma_i^{-1} \,\, (x-\mu_i)} }{\sum_{j=1}^{K}\alpha_k e^{-(x-\mu_j)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)}} \quad (1) $$ At this point it is common to take the logarithm: $$ (1) \equiv \arg \max_i \Big\{ \log a_i - \log[(x-\mu_j)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)] - 1  + \log \sum_{i=1}^N [(x-\mu_i)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)]  \Big\} $$ where I used the fact that $\log \sum_{i=1}^{N}a_k=1$ . I don't understand how the log sum trick is used now in order to conclude the computation. My understanding is that arg min will be replaced by min or similar. At this point I cannot figure it out.",I am struggling to understand how exactly the log-sum-exp trick is applied in mixture models (say Gaussian mixture models) and the EM algorithm. It is well known that parameter estimation for a mixture: can be achieved with the EM algorithm. The E-step corresponds in estimating in which distribution a given comes from. This probability is given by: At this point it is common to take the logarithm: where I used the fact that . I don't understand how the log sum trick is used now in order to conclude the computation. My understanding is that arg min will be replaced by min or similar. At this point I cannot figure it out.,"
f(x)= \sum_{i=1}^K \alpha_i \mathcal{N}(\mu_i,\Sigma_i) = \sum_{i=1}^K \alpha_ie^{-(x-\mu_i)^T\,\, \Sigma_i^{-1} \,\, (x-\mu_i)}
 i x 
p(x\in \mathcal{N(\mu_i,\Sigma_i)}) = \arg\max_i \frac{\alpha_i e^{-(x-\mu_i)^T\,\, \Sigma_i^{-1} \,\, (x-\mu_i)} }{\sum_{j=1}^{K}\alpha_k e^{-(x-\mu_j)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)}} \quad (1)
 
(1) \equiv \arg \max_i \Big\{ \log a_i - \log[(x-\mu_j)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)] - 1  + \log \sum_{i=1}^N [(x-\mu_i)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)]  \Big\}
 \log \sum_{i=1}^{N}a_k=1","['statistics', 'probability-distributions', 'bayesian', 'gaussian']"
3,Measurability issues in the symmetrization step in the proof of $\varepsilon$-sample theorem,Measurability issues in the symmetrization step in the proof of -sample theorem,\varepsilon,"Let $(\mathcal{X},d)$ be a metric space and $\mu$ be a Borel probability measure on $(\mathcal{X},d)$ . Let $m \in \mathbb{N}$ and define the two probability product measures $\mu^{m} := \otimes_{k=1}^{m}\mu$ and $\mu^{2m} := \otimes_{k=1}^{2m}\mu$ on the Borel subsets of the product spaces $\mathcal{X}^{m}$ and $\mathcal{X}^{2m}$ , respectively. Let $\mathcal{C}$ be an arbitrary family of Borel subsets of $(\mathcal{X},d)$ (in general not denumerable, but that we may assume to have finite Vapnik-Chervonenkis dimension) and $\varepsilon>0$ . Define $$Q:=\Big\{(x_1,\dots,x_m) \in \mathcal{X}^{m}\ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \Big\}.$$ The $\varepsilon$ -sample theorem proof has as a first step a symmetrization trick. I have a question about the rigorousness of that part of the proof. It is based on the following step that left me baffled: \begin{align*} \mu^{2m}\bigg[ \Big\{(x_1,\dots,x_m,x_1',\dots,x'_m) \in \mathcal{X}^{2m}\ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \land \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k') \Big| \le \frac{\varepsilon}{2}\Big\} \bigg]\\ =\int_{Q} \mu^m \bigg[ \Big\{ (x_1',\dots,x_m') \in \mathcal{X}^m \ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \land \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k') \Big| \le \frac{\varepsilon}{2}   \Big\}\bigg]\operatorname{d}\mu^m(x_1,\dots,x_m) \end{align*} Here, it is clear that the author of the proof wanted to use disintegration via independence. However, the involved sets are not measurable in general: it is not assumed that the family $\mathcal{C}$ is denumerable, so we have no guarantee about the measurability of every set defined through a condition of the form "" $\exists C \in \mathcal{C}$ "". Then, I thought that the above equation could be interpreted in terms of the corresponding (uniquely defined) outer measures. But, at that point, I'm not even sure that what it is written actually makes sense (we are integrating on a set $Q$ that, in general, is not measurable, and it isn't clear that the integrand function is measurable at all), nevermind if disintegration could be performed anymore. Is it possible to make the previous step rigorous without assuming the measurability of the involved sets? That line of the proof, with just slightly different notations, could be found in e.g., Anthony, Bartlett - Neural Network Learning, Theoretical Foundations, Lemma 4.4, pag. 46 and 47, eq. (4.3).","Let be a metric space and be a Borel probability measure on . Let and define the two probability product measures and on the Borel subsets of the product spaces and , respectively. Let be an arbitrary family of Borel subsets of (in general not denumerable, but that we may assume to have finite Vapnik-Chervonenkis dimension) and . Define The -sample theorem proof has as a first step a symmetrization trick. I have a question about the rigorousness of that part of the proof. It is based on the following step that left me baffled: Here, it is clear that the author of the proof wanted to use disintegration via independence. However, the involved sets are not measurable in general: it is not assumed that the family is denumerable, so we have no guarantee about the measurability of every set defined through a condition of the form "" "". Then, I thought that the above equation could be interpreted in terms of the corresponding (uniquely defined) outer measures. But, at that point, I'm not even sure that what it is written actually makes sense (we are integrating on a set that, in general, is not measurable, and it isn't clear that the integrand function is measurable at all), nevermind if disintegration could be performed anymore. Is it possible to make the previous step rigorous without assuming the measurability of the involved sets? That line of the proof, with just slightly different notations, could be found in e.g., Anthony, Bartlett - Neural Network Learning, Theoretical Foundations, Lemma 4.4, pag. 46 and 47, eq. (4.3).","(\mathcal{X},d) \mu (\mathcal{X},d) m \in \mathbb{N} \mu^{m} := \otimes_{k=1}^{m}\mu \mu^{2m} := \otimes_{k=1}^{2m}\mu \mathcal{X}^{m} \mathcal{X}^{2m} \mathcal{C} (\mathcal{X},d) \varepsilon>0 Q:=\Big\{(x_1,\dots,x_m) \in \mathcal{X}^{m}\ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \Big\}. \varepsilon \begin{align*}
\mu^{2m}\bigg[ \Big\{(x_1,\dots,x_m,x_1',\dots,x'_m) \in \mathcal{X}^{2m}\ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \land \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k') \Big| \le \frac{\varepsilon}{2}\Big\} \bigg]\\
=\int_{Q} \mu^m \bigg[ \Big\{ (x_1',\dots,x_m') \in \mathcal{X}^m \ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \land \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k') \Big| \le \frac{\varepsilon}{2}   \Big\}\bigg]\operatorname{d}\mu^m(x_1,\dots,x_m)
\end{align*} \mathcal{C} \exists C \in \mathcal{C} Q","['probability-theory', 'measure-theory', 'statistics', 'proof-explanation', 'machine-learning']"
4,How did Facebook calculate the 3.5 degrees of separation of everyone on their platform? (Bitwise Operators question),How did Facebook calculate the 3.5 degrees of separation of everyone on their platform? (Bitwise Operators question),,"A few years ago the Facebook team calculated that everyone was an average 3.5 degrees of separation away from every other user on the FB platform. https://research.fb.com/three-and-a-half-degrees-of-separation/ They explain the maths later in the article. First they use a statistical approximation of the number of friends you have based on the max number of zeros (n) in a binary hash of your friends ID's. Friends ~= c * 2^n where c is a constant This much I understand. Then they go on to say... By using a bitwise OR operation on the hash, this process can be repeated recursively to estimate the number of unique friends-of-friends, and then friends-of-friends-of-friends. What does this part mean?","A few years ago the Facebook team calculated that everyone was an average 3.5 degrees of separation away from every other user on the FB platform. https://research.fb.com/three-and-a-half-degrees-of-separation/ They explain the maths later in the article. First they use a statistical approximation of the number of friends you have based on the max number of zeros (n) in a binary hash of your friends ID's. Friends ~= c * 2^n where c is a constant This much I understand. Then they go on to say... By using a bitwise OR operation on the hash, this process can be repeated recursively to estimate the number of unique friends-of-friends, and then friends-of-friends-of-friends. What does this part mean?",,"['statistics', 'logic']"
5,Sum of Squares of Binomial Distributions?,Sum of Squares of Binomial Distributions?,,"The $\chi^2$ distribution describes the sum of squares of independent normal random variables. Is there an analogous distribution for the discrete case of sum of squares of independent (identical) binomial random variables? I'm particularly interested in concentration bounds on the resulting sum. I know how to compute the expectation (and I suppose Chernoff bound from there), but is there anything stronger or more explicit to be said? I know that the binomial distribution approximates a normal distribution---does this imply that the sum of squares of binomials also approximates a $\chi^2$ distribution? Thanks in advance.","The distribution describes the sum of squares of independent normal random variables. Is there an analogous distribution for the discrete case of sum of squares of independent (identical) binomial random variables? I'm particularly interested in concentration bounds on the resulting sum. I know how to compute the expectation (and I suppose Chernoff bound from there), but is there anything stronger or more explicit to be said? I know that the binomial distribution approximates a normal distribution---does this imply that the sum of squares of binomials also approximates a distribution? Thanks in advance.",\chi^2 \chi^2,"['statistics', 'probability-distributions', 'binomial-distribution', 'concentration-of-measure']"
6,PCA for retrieval from absolute values,PCA for retrieval from absolute values,,"I have given $(x_i)_{i=1}^n$ $d$ -dimensional iid. random variables with $x_i\sim\mathcal N(0,I_d)$ and $y_i=|\langle x_i,\theta\rangle|$ with $\theta\in\mathbb R^d$ . First I have to assume that $\|\theta\|_2=1$ and define $\hat\theta$ as the maximal eigenvector of the matrix $\frac1n\sum_{i=1}^n y_i^2 x_i x_i^T$ . Now I have to prove that $\hat\theta$ is a consistent estimator of $\theta$ . My ideas: That means I have to show that $\lim\limits_{n\to\infty} P(\|\hat\theta-\theta\|_2\geq\varepsilon)=0\quad \forall\theta\in\mathbb R^d$ . I guess the first thing to do is to find a explicit representation of $\hat\theta$ depending on $n$ . Maybe the min-max theorem could be helpfull. In the second part I have to provide an estimator for arbitrary $\theta\in\mathbb R^d$ based on the $\hat\theta$ from above and show its consistency. For this case there was also a hint to construct a random matrix $Z$ for a pair $(x,y)$ such that $E[Z] =\sqrt\frac2\pi(\theta^*\otimes\theta^*+I_d)$ . The Exercise can similarly be found on page 257 in Wainwright , Exercise 8.7. I hope someone can help me. Greetings","I have given -dimensional iid. random variables with and with . First I have to assume that and define as the maximal eigenvector of the matrix . Now I have to prove that is a consistent estimator of . My ideas: That means I have to show that . I guess the first thing to do is to find a explicit representation of depending on . Maybe the min-max theorem could be helpfull. In the second part I have to provide an estimator for arbitrary based on the from above and show its consistency. For this case there was also a hint to construct a random matrix for a pair such that . The Exercise can similarly be found on page 257 in Wainwright , Exercise 8.7. I hope someone can help me. Greetings","(x_i)_{i=1}^n d x_i\sim\mathcal N(0,I_d) y_i=|\langle x_i,\theta\rangle| \theta\in\mathbb R^d \|\theta\|_2=1 \hat\theta \frac1n\sum_{i=1}^n y_i^2 x_i x_i^T \hat\theta \theta \lim\limits_{n\to\infty} P(\|\hat\theta-\theta\|_2\geq\varepsilon)=0\quad \forall\theta\in\mathbb R^d \hat\theta n \theta\in\mathbb R^d \hat\theta Z (x,y) E[Z] =\sqrt\frac2\pi(\theta^*\otimes\theta^*+I_d)","['statistics', 'random-variables', 'normal-distribution', 'random-matrices', 'parameter-estimation']"
7,Bounding sum of quartic deviations from sample mean,Bounding sum of quartic deviations from sample mean,,"I came across the following statement in The Jackknife and Bootstrap, Shao and Tu , p. 87: $$\sum_i(X_i-\bar X)^4\leq16\sum_i(X_i-\mu)^4,$$ where $\bar X$ and $\mu$ are the sample mean and expected value of the $X_i$ , respectively. Even without the term 16, the statement is well-known (for any $a$ , not only $\mu$ when we replace 4 by 2. My hunch is that the proof uses Pascal's triangle, as there are 16 terms in the l.h.s. when adding and subtracting $\mu$ and multiplying out: $$\sum_i(X_i-\bar X)^4=\sum_i(X_i-\mu)^4+4(X_i-\mu)^3(\mu-\bar X)+4(X_i-\mu)(\mu-\bar X)^3+6(X_i-\mu)^2(\mu-\bar X)^2+(\mu-\bar X)^4$$ My attempts at bounding the terms on the r.h.s. (other than the first one, which is already in the ""right"" format), including Hölder's inequality, however have not led anywhere useful.","I came across the following statement in The Jackknife and Bootstrap, Shao and Tu , p. 87: where and are the sample mean and expected value of the , respectively. Even without the term 16, the statement is well-known (for any , not only when we replace 4 by 2. My hunch is that the proof uses Pascal's triangle, as there are 16 terms in the l.h.s. when adding and subtracting and multiplying out: My attempts at bounding the terms on the r.h.s. (other than the first one, which is already in the ""right"" format), including Hölder's inequality, however have not led anywhere useful.","\sum_i(X_i-\bar X)^4\leq16\sum_i(X_i-\mu)^4, \bar X \mu X_i a \mu \mu \sum_i(X_i-\bar X)^4=\sum_i(X_i-\mu)^4+4(X_i-\mu)^3(\mu-\bar X)+4(X_i-\mu)(\mu-\bar X)^3+6(X_i-\mu)^2(\mu-\bar X)^2+(\mu-\bar X)^4","['probability', 'statistics']"
8,"Product of characteristic functions, counter example","Product of characteristic functions, counter example",,"We know that for independent random variables $X,Y$ we have: $\varphi_{X+Y}(t)=\varphi_X(t)\cdot \varphi_Y(t)$ . Now I'm searching for an example that shows that the reversal of the statement is not true, so I want $X$ and $Y$ which fulfill $\varphi_{X+Y}(t)=\varphi_X(t)\cdot \varphi_Y(t)$ but are dependent. I can't think of an example that shows this.","We know that for independent random variables we have: . Now I'm searching for an example that shows that the reversal of the statement is not true, so I want and which fulfill but are dependent. I can't think of an example that shows this.","X,Y \varphi_{X+Y}(t)=\varphi_X(t)\cdot \varphi_Y(t) X Y \varphi_{X+Y}(t)=\varphi_X(t)\cdot \varphi_Y(t)","['probability-theory', 'statistics', 'characteristic-functions']"
9,"calculating the coefficient of skewness for $f_Y(y) = e^{-y} ,y>0$",calculating the coefficient of skewness for,"f_Y(y) = e^{-y} ,y>0","I'm trying to find the coefficient of skewness for the pdf: $f_Y(y) = e^{-y}; y>0$ After having calculated for $\mu$ and $\sigma^2$ I get the values $(1, 1)$ respectively. Then placing this into the equation for skewness: $$\gamma = \frac{E[(X-\mu)^3]}{1} = \sum_{j=0}^{3} \binom3jE(Y^j)(-1^{3-j}).$$ I'm not sure how to proceed from here, and having a look at the solution to this exercise I find that $E(Y^j) = j!$ why is this the case?","I'm trying to find the coefficient of skewness for the pdf: After having calculated for and I get the values respectively. Then placing this into the equation for skewness: I'm not sure how to proceed from here, and having a look at the solution to this exercise I find that why is this the case?","f_Y(y) = e^{-y}; y>0 \mu \sigma^2 (1, 1) \gamma = \frac{E[(X-\mu)^3]}{1} = \sum_{j=0}^{3} \binom3jE(Y^j)(-1^{3-j}). E(Y^j) = j!","['probability', 'statistics', 'expected-value']"
10,Finding a confidence interval for parameter of normal distribution,Finding a confidence interval for parameter of normal distribution,,"We observe a simple random sample $(X_1,\ldots , X_n$ ), where the distribution of observed property $X$ is given by its density: $f(x)=\frac{1}{x\sigma \sqrt{2\pi}}e^{-\frac{(\ln x-m)^2}{2\sigma^2}}, x>0$ . Find a 80% confidence interval for $\sigma$ , if we know: Mean value of logarithms of the values from the dataset that we get deviates from its expected value no more than $\frac{\sigma}{2}$ , with probability 0.98758; If $(x_1, \ldots ,x_n)$ is our dataset (a realisation of our sample), $\sum_{i=1}^n \ln x_i=3.139797$ and $\sum_{i=1}^n (\ln x_i)^2=35.63178$ . My attempt: It's not hard to see that, if $Y=\ln X$ , then $Y$ has $\mathscr{N}(m,\sigma^2)$ distribution. Now we work with $Y$ . Let $$\overline{Y}_n=\frac{1}{n}\sum_{i=1}^n Y_i$$ $$\overline{S}_n^2=\frac{1}{n}\sum_{i=1}^n (Y_i-\overline{Y}_n)^2$$ . We know that $\frac{n\overline{S}_n^2}{\sigma^2}$ has $\chi_{n-1}^2$ distribution. If we look for confidence interval in the form: $$p\{U_n\leq \sigma \leq V_n\}=0.8$$ , we can actually demand $$p\{\frac{n\overline{S}_n^2}{V_n^2}\leq  \frac{n\overline{S}_n^2}{\sigma^2} \leq \frac{n\overline{S}_n^2}{U_n^2} \}=0.8$$ . We can demand: $$p\{\frac{n\overline{S}_n^2}{V_n^2}>\frac{n\overline{S}_n^2}{\sigma^2}\}=p\{\frac{n\overline{S}_n^2}{U_n^2}<\frac{n\overline{S}_n^2}{\sigma^2}\}=\frac{1-0.8}{2}=0.1$$ . This gives us (from what we know about $\chi_{n-1}^2$ distribution): $$V_n^2=\frac{n\overline{S}_n^2}{\chi_{n-1,0.9}^2}$$ $$U_n^2=\frac{n\overline{S}_n^2}{\chi_{n-1,0.1}^2}$$ . I assume that the info given in this problem means that one confidence interval for $\frac{1}{n}\sum_{i=1}^n \ln x_i-m$ , with confidence level $0.98758$ is $[-\frac{\sigma}{2}, \frac{\sigma}{2}]$ . But I don't know how to move on from here. We also know that for: $$\overline{S}_*^2=\frac{1}{n}\sum_{i=1}^n (Y_i-m)^2$$ $\frac{n\overline{S}_*^2}{\sigma^2}$ has $\chi_n^2$ distribution. I thought maybe we should use this because of this CI that has been given to us, but I don't know how to do that.","We observe a simple random sample ), where the distribution of observed property is given by its density: . Find a 80% confidence interval for , if we know: Mean value of logarithms of the values from the dataset that we get deviates from its expected value no more than , with probability 0.98758; If is our dataset (a realisation of our sample), and . My attempt: It's not hard to see that, if , then has distribution. Now we work with . Let . We know that has distribution. If we look for confidence interval in the form: , we can actually demand . We can demand: . This gives us (from what we know about distribution): . I assume that the info given in this problem means that one confidence interval for , with confidence level is . But I don't know how to move on from here. We also know that for: has distribution. I thought maybe we should use this because of this CI that has been given to us, but I don't know how to do that.","(X_1,\ldots , X_n X f(x)=\frac{1}{x\sigma \sqrt{2\pi}}e^{-\frac{(\ln x-m)^2}{2\sigma^2}}, x>0 \sigma \frac{\sigma}{2} (x_1, \ldots ,x_n) \sum_{i=1}^n \ln x_i=3.139797 \sum_{i=1}^n (\ln x_i)^2=35.63178 Y=\ln X Y \mathscr{N}(m,\sigma^2) Y \overline{Y}_n=\frac{1}{n}\sum_{i=1}^n Y_i \overline{S}_n^2=\frac{1}{n}\sum_{i=1}^n (Y_i-\overline{Y}_n)^2 \frac{n\overline{S}_n^2}{\sigma^2} \chi_{n-1}^2 p\{U_n\leq \sigma \leq V_n\}=0.8 p\{\frac{n\overline{S}_n^2}{V_n^2}\leq  \frac{n\overline{S}_n^2}{\sigma^2} \leq \frac{n\overline{S}_n^2}{U_n^2} \}=0.8 p\{\frac{n\overline{S}_n^2}{V_n^2}>\frac{n\overline{S}_n^2}{\sigma^2}\}=p\{\frac{n\overline{S}_n^2}{U_n^2}<\frac{n\overline{S}_n^2}{\sigma^2}\}=\frac{1-0.8}{2}=0.1 \chi_{n-1}^2 V_n^2=\frac{n\overline{S}_n^2}{\chi_{n-1,0.9}^2} U_n^2=\frac{n\overline{S}_n^2}{\chi_{n-1,0.1}^2} \frac{1}{n}\sum_{i=1}^n \ln x_i-m 0.98758 [-\frac{\sigma}{2}, \frac{\sigma}{2}] \overline{S}_*^2=\frac{1}{n}\sum_{i=1}^n (Y_i-m)^2 \frac{n\overline{S}_*^2}{\sigma^2} \chi_n^2","['probability', 'statistics', 'probability-distributions', 'statistical-inference', 'confidence-interval']"
11,Generative model evaluation metric : Precision & Recall,Generative model evaluation metric : Precision & Recall,,"In this paper , a new metric was proposed to evaluate generative model. The equation (1) decomposes generative distribution and real distribution into two parts w.r.t their intersection of the supports. Then he demonstrated the case that the distribution on the support $S$ is equal with pictures (a)-(d) in Figure 2. My first question: In the picture (a), the distribution $P$ has two modes, and $Q$ has one mode but higher than $P$ . Why they have same distribution over the $S$ ? Then, the author discussed the case when the distribution over $S$ are different and he decomposed the distribution similarly like above. And the define a PRD function in definition 2. My second question is that after my generative model was trained, its distribution was determined. When two distribution were given, shouldn't the alpha and beta be constants? Why in the definition said ""all (alpha,beta) satisfying definition 1""?","In this paper , a new metric was proposed to evaluate generative model. The equation (1) decomposes generative distribution and real distribution into two parts w.r.t their intersection of the supports. Then he demonstrated the case that the distribution on the support is equal with pictures (a)-(d) in Figure 2. My first question: In the picture (a), the distribution has two modes, and has one mode but higher than . Why they have same distribution over the ? Then, the author discussed the case when the distribution over are different and he decomposed the distribution similarly like above. And the define a PRD function in definition 2. My second question is that after my generative model was trained, its distribution was determined. When two distribution were given, shouldn't the alpha and beta be constants? Why in the definition said ""all (alpha,beta) satisfying definition 1""?",S P Q P S S,"['statistics', 'machine-learning']"
12,Order of a non-linear autoregressive exogenous (NARX) model,Order of a non-linear autoregressive exogenous (NARX) model,,"Suppose I have a non-linear autoregressive exogenous (NARX) model of the kind $$y(k+1)=f(y(k), y(k-1), ..., y(k-s), u(k), u(k-1),..., u(k-t)) $$ where $y$ and $u$ represent respectively the output and the input of a discrete non linear system. Is it correct to say that the order of the NARX model is $\max\{s+1, t+1\}$ ? Here the Wikipedia link on info about the NARX model: https://en.wikipedia.org/wiki/Nonlinear_autoregressive_exogenous_model Here is the paper which references the NARX model. See equation (1).",Suppose I have a non-linear autoregressive exogenous (NARX) model of the kind where and represent respectively the output and the input of a discrete non linear system. Is it correct to say that the order of the NARX model is ? Here the Wikipedia link on info about the NARX model: https://en.wikipedia.org/wiki/Nonlinear_autoregressive_exogenous_model Here is the paper which references the NARX model. See equation (1).,"y(k+1)=f(y(k), y(k-1), ..., y(k-s), u(k), u(k-1),..., u(k-t))  y u \max\{s+1, t+1\}","['statistics', 'regression', 'nonlinear-system', 'time-series', 'discrete-time']"
13,Drawing Conclusions From Only Medians,Drawing Conclusions From Only Medians,,"At a small high school, the 12th grade is only 20 students. They all take the same five classes (English, Math, Science, History, and Latin) together. At the end of the year each student is assigned a numerical grade between 0 and 100 for each class. Obviously, to calculate their gpa for the year, a student adds their five numbers and then divides by five. The median grade for each class is known, but nothing else. We cannot even assume that the grade distributions in each class are normal. Can one draw any conclusions about the median or mean gpa? If concrete numbers helps, pretend that the median grades are: English: 83, Math: 84, Science: 85, History: 86, Latin: 87.","At a small high school, the 12th grade is only 20 students. They all take the same five classes (English, Math, Science, History, and Latin) together. At the end of the year each student is assigned a numerical grade between 0 and 100 for each class. Obviously, to calculate their gpa for the year, a student adds their five numbers and then divides by five. The median grade for each class is known, but nothing else. We cannot even assume that the grade distributions in each class are normal. Can one draw any conclusions about the median or mean gpa? If concrete numbers helps, pretend that the median grades are: English: 83, Math: 84, Science: 85, History: 86, Latin: 87.",,['statistics']
14,On the definition of the stochastic $o$ and $O$ symbols,On the definition of the stochastic  and  symbols,o O,"Given two sequence $(X_n),(Y_n)$ of random variables with values in $\mathbb{R}^d$ , consider the following definitions: Write $X_n=O_p(Y_n)$ if, for all $\epsilon>0$ , there exists $M>0$ and $N\geq1$ such that $P\Big[\|X_n\|>M\|Y_n\|\Big]<\epsilon$ whenever $n\geq N$ . Write $X_n=o_p(Y_n)$ if, for all $\epsilon>0$ , $P\Big[\|X_n\|>\epsilon\|Y_n\|\Big]\to 0 \text{ as } n\to \infty.$ In the book Asymptotic Statistics , A.W. van der Vaart defines $X_n=O_p(Y_n)$ if $X_n=Z_nY_n$ with $Z_n=O_p(1)$ , and $X_n=o_p(Y_n)$ if $X_n=Z_nY_n$ with $Z_n=o_p(1)$ . But these definitions are not equivalent to the definitions $1,2$ above. For if we take $Y_n=0$ for all $n$ , then $1,2$ are equivalent to $X_n=0$ with probability approaching $1$ , while A.W. van der Vaart's definitions are equivalent to $X_n=0$ for all $n$ . Which of these definitions are preferable? It seems to me that $1,2$ are better since they reduce to the usual meanings of $o,O$ in the nonstochastic case (see here for example). Am I missing something? Thanks a lot for any help. EDIT. Van der Vaart's definition of $X_n=O_p(1)$ is, for all $\epsilon>0$ , there exists $M>0$ such that $\sup_{n\geq 1} P[\|X_n\|>M] <\epsilon$ . Using the fact that any real random variable is tight (see here ), we see that this definition is equivalent to definition $1$ with $Y_n=1$ for all $n$ . Van der Vaart's definition of $X_n=o_p(1)$ is $X_n\overset{p}\to  0$ as $n\to\infty$ .","Given two sequence of random variables with values in , consider the following definitions: Write if, for all , there exists and such that whenever . Write if, for all , In the book Asymptotic Statistics , A.W. van der Vaart defines if with , and if with . But these definitions are not equivalent to the definitions above. For if we take for all , then are equivalent to with probability approaching , while A.W. van der Vaart's definitions are equivalent to for all . Which of these definitions are preferable? It seems to me that are better since they reduce to the usual meanings of in the nonstochastic case (see here for example). Am I missing something? Thanks a lot for any help. EDIT. Van der Vaart's definition of is, for all , there exists such that . Using the fact that any real random variable is tight (see here ), we see that this definition is equivalent to definition with for all . Van der Vaart's definition of is as .","(X_n),(Y_n) \mathbb{R}^d X_n=O_p(Y_n) \epsilon>0 M>0 N\geq1 P\Big[\|X_n\|>M\|Y_n\|\Big]<\epsilon n\geq N X_n=o_p(Y_n) \epsilon>0 P\Big[\|X_n\|>\epsilon\|Y_n\|\Big]\to 0 \text{ as } n\to \infty. X_n=O_p(Y_n) X_n=Z_nY_n Z_n=O_p(1) X_n=o_p(Y_n) X_n=Z_nY_n Z_n=o_p(1) 1,2 Y_n=0 n 1,2 X_n=0 1 X_n=0 n 1,2 o,O X_n=O_p(1) \epsilon>0 M>0 \sup_{n\geq 1} P[\|X_n\|>M] <\epsilon 1 Y_n=1 n X_n=o_p(1) X_n\overset{p}\to  0 n\to\infty","['probability', 'sequences-and-series', 'probability-theory', 'statistics', 'asymptotics']"
15,"$\log L(\theta \mid \mathbf{x}) =\int \log L(\theta \mid \mathbf{x}) k(\mathbf{z} \mid \theta_{0}, \mathbf{x}) d \mathbf{z} $",,"\log L(\theta \mid \mathbf{x}) =\int \log L(\theta \mid \mathbf{x}) k(\mathbf{z} \mid \theta_{0}, \mathbf{x}) d \mathbf{z} ","I am trying to peace tighter the EM algorithm but I am not able to follow certain steps: What I know: Marginal pdf: $$f_{X_{1}}\left(x_{1}\right)=\int_{-\infty}^{\infty} f_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right) d x_{2}$$ for all $x_1$ element of domain of $X_1$ Conditional Distributions: $$p_{X_{2} \mid X_{1}}\left(x_{2} \mid x_{1}\right)=\frac{p_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right)}{p_{X_{1}}\left(x_{1}\right)}$$ MLE: $$L(\theta ; \mathbf{x})=\prod_{i=1}^{n} f\left(x_{i} ; \theta\right), \quad \theta \in \Omega$$ $$l(\theta)=\log L(\theta)=\sum_{i=1}^{n} \log f\left(x_{i} ; \theta\right), \quad \theta \in \Omega$$ $$ k(\mathbf{z} \mid \theta, \mathbf{x})=\frac{h(\mathbf{x}, \mathbf{z} \mid \theta)}{g(\mathbf{x} \mid \theta)}         (6.6.1) $$ What I am trying to follow: The observed likelihood function is $L(\theta \mid \mathbf{x})=g(\mathbf{x} \mid \theta) .$ The complete likelihood function is defined by $$ L^{c}(\theta \mid \mathbf{x}, \mathbf{z})=h(\mathbf{x}, \mathbf{z} \mid \theta) $$ Our goal is maximize the likelihood function $L(\theta \mid \mathbf{x})$ by using the complete likelihood $L^{c}(\theta \mid \mathbf{x}, \mathbf{z})$ in this process. Using (6.6.1), we derive the following basic identity for an arbitrary but fixed $\theta_{0} \in \Omega:$ $$ \begin{aligned} \log L(\theta \mid \mathbf{x}) &=\int \log L(\theta \mid \mathbf{x}) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\ &=\int \log g(\mathbf{x} \mid \theta) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\ &=\int[\log h(\mathbf{x}, \mathbf{z} \mid \theta)-\log k(\mathbf{z} \mid \theta, \mathbf{x})] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\ &=\int \log [h(\mathbf{x}, \mathbf{z} \mid \theta)] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z}-\int \log [k(\mathbf{z} \mid \theta, \mathbf{x})] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\ &=E_{\theta_{0}}\left[\log L^{c}(\theta \mid \mathbf{x}, \mathbf{Z}) \mid \theta_{0}, \mathbf{x}\right]-E_{\theta_{0}}\left[\log k(\mathbf{Z} \mid \theta, \mathbf{x}) \mid \theta_{0}, \mathbf{x}\right] \end{aligned} $$ where the expectations are taken under the conditional pdf $k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right)$ . Define the first term on the right side of $(6.6 .3)$ to be the function $$ Q\left(\theta \mid \theta_{0}, \mathbf{x}\right)=E_{\theta_{0}}\left[\log L^{c}(\theta \mid \mathbf{x}, \mathbf{Z}) \mid \theta_{0}, \mathbf{x}\right] $$ Q1: from the definition of observed likelihood and complete likelihood above I don't see how this $$\log L(\theta \mid \mathbf{x}) =\int \log L(\theta \mid \mathbf{x}) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\$$ equations has come about. Q2: If I understand Q1 I might be able to understand why Log is only taken on $g(x|omega)$ And if this is correct, then I do given(6.6.1) understand the steps log(a/b)=log(a)-log(b) so I can follow down to line 4. Q3: I don't understand what happening from line 4 to the last line, what do they mean by: where the expectations are taken under the conditional pdf $k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right)$ . and how k(...) term disappear and we get a capital Z and the conditional part of k is now the condition inside the Expectation.","I am trying to peace tighter the EM algorithm but I am not able to follow certain steps: What I know: Marginal pdf: for all element of domain of Conditional Distributions: MLE: What I am trying to follow: The observed likelihood function is The complete likelihood function is defined by Our goal is maximize the likelihood function by using the complete likelihood in this process. Using (6.6.1), we derive the following basic identity for an arbitrary but fixed where the expectations are taken under the conditional pdf . Define the first term on the right side of to be the function Q1: from the definition of observed likelihood and complete likelihood above I don't see how this equations has come about. Q2: If I understand Q1 I might be able to understand why Log is only taken on And if this is correct, then I do given(6.6.1) understand the steps log(a/b)=log(a)-log(b) so I can follow down to line 4. Q3: I don't understand what happening from line 4 to the last line, what do they mean by: where the expectations are taken under the conditional pdf . and how k(...) term disappear and we get a capital Z and the conditional part of k is now the condition inside the Expectation.","f_{X_{1}}\left(x_{1}\right)=\int_{-\infty}^{\infty} f_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right) d x_{2} x_1 X_1 p_{X_{2} \mid X_{1}}\left(x_{2} \mid x_{1}\right)=\frac{p_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right)}{p_{X_{1}}\left(x_{1}\right)} L(\theta ; \mathbf{x})=\prod_{i=1}^{n} f\left(x_{i} ; \theta\right), \quad \theta \in \Omega l(\theta)=\log L(\theta)=\sum_{i=1}^{n} \log f\left(x_{i} ; \theta\right), \quad \theta \in \Omega 
k(\mathbf{z} \mid \theta, \mathbf{x})=\frac{h(\mathbf{x}, \mathbf{z} \mid \theta)}{g(\mathbf{x} \mid \theta)}         (6.6.1)
 L(\theta \mid \mathbf{x})=g(\mathbf{x} \mid \theta) . 
L^{c}(\theta \mid \mathbf{x}, \mathbf{z})=h(\mathbf{x}, \mathbf{z} \mid \theta)
 L(\theta \mid \mathbf{x}) L^{c}(\theta \mid \mathbf{x}, \mathbf{z}) \theta_{0} \in \Omega: 
\begin{aligned}
\log L(\theta \mid \mathbf{x}) &=\int \log L(\theta \mid \mathbf{x}) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\
&=\int \log g(\mathbf{x} \mid \theta) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\
&=\int[\log h(\mathbf{x}, \mathbf{z} \mid \theta)-\log k(\mathbf{z} \mid \theta, \mathbf{x})] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\
&=\int \log [h(\mathbf{x}, \mathbf{z} \mid \theta)] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z}-\int \log [k(\mathbf{z} \mid \theta, \mathbf{x})] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\
&=E_{\theta_{0}}\left[\log L^{c}(\theta \mid \mathbf{x}, \mathbf{Z}) \mid \theta_{0}, \mathbf{x}\right]-E_{\theta_{0}}\left[\log k(\mathbf{Z} \mid \theta, \mathbf{x}) \mid \theta_{0}, \mathbf{x}\right]
\end{aligned}
 k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) (6.6 .3) 
Q\left(\theta \mid \theta_{0}, \mathbf{x}\right)=E_{\theta_{0}}\left[\log L^{c}(\theta \mid \mathbf{x}, \mathbf{Z}) \mid \theta_{0}, \mathbf{x}\right]
 \log L(\theta \mid \mathbf{x}) =\int \log L(\theta \mid \mathbf{x}) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\ g(x|omega) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right)","['probability', 'statistics']"
16,Using Bayesian inference to optimize my food stand - Is this feasible?,Using Bayesian inference to optimize my food stand - Is this feasible?,,"During the last days I was learning about bayesian inference, and am now wondering if this could be of any practical use for a food stand I own. Here is the problem: I own a food stand and sell sausages before football matches. Before every match, I have to decide how many buns and sausages I have to buy / how many sells I expect. This depends mostly on the condition, of how many tickets have been sold for the match. This condition can change quite dynamically from match to match. So although I can make quite an educated guess out of the years of experience, I still end up throwing away many buns and sausages after each match. So the problem I am thinking about can be summarized as: Given the number of sells I've made in the past for number of tickets T, how many buns and sausages shall I buy for todays match, when the sold tickets are T_today. Even if the following approach is of no practical use, I am still curious in terms of it being an academic exercise. So my idea is the following: Describe the guessed number of sells for the next match using a normal distribution, with mean and variance being an educated guess. Measure the number of tickets that have been sold, and the number of sells I have made. Before the next matchday, look at the numbers of tickets that have been sold and update the normal distribution, given this number of tickets and the experience of the last match. Now, its quite straightforward how you update a normal distribution using bayesian inference. You measure a dataset, compute its variance and mean and you are then able to compute the updated normal distributions mean, variance etc. However, in this case, the measurement I do is the number of sells I've made and tickets sold. This I can measure with 100% accuracy, so there is no variance. How can I perform bayesian inference in this case?","During the last days I was learning about bayesian inference, and am now wondering if this could be of any practical use for a food stand I own. Here is the problem: I own a food stand and sell sausages before football matches. Before every match, I have to decide how many buns and sausages I have to buy / how many sells I expect. This depends mostly on the condition, of how many tickets have been sold for the match. This condition can change quite dynamically from match to match. So although I can make quite an educated guess out of the years of experience, I still end up throwing away many buns and sausages after each match. So the problem I am thinking about can be summarized as: Given the number of sells I've made in the past for number of tickets T, how many buns and sausages shall I buy for todays match, when the sold tickets are T_today. Even if the following approach is of no practical use, I am still curious in terms of it being an academic exercise. So my idea is the following: Describe the guessed number of sells for the next match using a normal distribution, with mean and variance being an educated guess. Measure the number of tickets that have been sold, and the number of sells I have made. Before the next matchday, look at the numbers of tickets that have been sold and update the normal distribution, given this number of tickets and the experience of the last match. Now, its quite straightforward how you update a normal distribution using bayesian inference. You measure a dataset, compute its variance and mean and you are then able to compute the updated normal distributions mean, variance etc. However, in this case, the measurement I do is the number of sells I've made and tickets sold. This I can measure with 100% accuracy, so there is no variance. How can I perform bayesian inference in this case?",,"['statistics', 'normal-distribution', 'bayesian', 'bayes-theorem', 'gaussian']"
17,How to derive $SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{(x_0-\bar{x})^2}{(n-1)s^2_x}\bigg]^\frac{1}{2}$,How to derive,SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{(x_0-\bar{x})^2}{(n-1)s^2_x}\bigg]^\frac{1}{2},"I am trying to derive the following formula given by the lecture notes $$SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{(x_0-\bar{x})^2}{(n-1)s^2_x}\bigg]^\frac{1}{2}$$ My attempt $$SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\big[SE^2(\hat{\beta_0})+x_0^2SE^2(\hat{\beta_1})\big]^\frac{1}{2}\\ SE^2(\hat{\beta_0})=\hat{\sigma}^2\bigg[\frac{1}{n}+\frac{\bar{x}^2}{(n-1)s^2_x}\bigg]\\ x^2_0SE^2(\hat{\beta_1})=\hat{\sigma}^2\bigg[\frac{x^2_0 }{(n-1)s^2_x}\bigg]\\ \text{Plug in}\\ SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{\bar{x}^2}{(n-1)s^2_x}+\frac{x^2_0}{(n-1)s^2_x}\bigg]^{\frac{1}{2}}=\hat{\sigma}\bigg[\frac{1}{n}+\frac{\color{red}{x^2_0+\bar{x}^2}}{(n-1)s^2_x}\bigg]^{\frac{1}{2}} $$ In the correct answer, the numerator should be $(x_0-\bar{x})^2$ , I don't see how can I obtained that. Any comment are more than welcome and appreciated it for that.","I am trying to derive the following formula given by the lecture notes My attempt In the correct answer, the numerator should be , I don't see how can I obtained that. Any comment are more than welcome and appreciated it for that.","SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{(x_0-\bar{x})^2}{(n-1)s^2_x}\bigg]^\frac{1}{2} SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\big[SE^2(\hat{\beta_0})+x_0^2SE^2(\hat{\beta_1})\big]^\frac{1}{2}\\
SE^2(\hat{\beta_0})=\hat{\sigma}^2\bigg[\frac{1}{n}+\frac{\bar{x}^2}{(n-1)s^2_x}\bigg]\\
x^2_0SE^2(\hat{\beta_1})=\hat{\sigma}^2\bigg[\frac{x^2_0 }{(n-1)s^2_x}\bigg]\\
\text{Plug in}\\
SE(\hat{\beta_0}+\hat{\beta_1}x_0)=\hat{\sigma}\bigg[\frac{1}{n}+\frac{\bar{x}^2}{(n-1)s^2_x}+\frac{x^2_0}{(n-1)s^2_x}\bigg]^{\frac{1}{2}}=\hat{\sigma}\bigg[\frac{1}{n}+\frac{\color{red}{x^2_0+\bar{x}^2}}{(n-1)s^2_x}\bigg]^{\frac{1}{2}}
 (x_0-\bar{x})^2","['statistics', 'statistical-inference', 'regression', 'standard-error']"
18,Magicians Problem [closed],Magicians Problem [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Consider a competition for magicians. The point of the competition is to prove to the general audience that there is no such thing as magical abilities at all, so the organizers hope that nobody wins simply due to luck. The procedure of the competition is as follows. A fair coin is tossed 11 times and the magician tries to guess the results of these tosses. The organizers of the competition test null hypothesis that claims that probability to guess is 1/2 against an alternative that this probability is larger than 1/2. If null hypothesis is rejected on 5% significance level, then the organizers will claim that the magician has magical abilities and pay him his winning money. Assume that 50 magicians participate in the competition and try to prove that they have magical abilities. All magicians are tested independently. What is the probability that at least one magician wins (and therefore the organizers have to conclude that people with magical abilities exist). Enter answer with 2 digits after decimal point. Ans: W : no. of magicians out of 50 who wins W ~ Binom(n=50, p = 0.5) P(W >= 1) = 1 - P(W=0) 0.92 but this not correct answer `","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Consider a competition for magicians. The point of the competition is to prove to the general audience that there is no such thing as magical abilities at all, so the organizers hope that nobody wins simply due to luck. The procedure of the competition is as follows. A fair coin is tossed 11 times and the magician tries to guess the results of these tosses. The organizers of the competition test null hypothesis that claims that probability to guess is 1/2 against an alternative that this probability is larger than 1/2. If null hypothesis is rejected on 5% significance level, then the organizers will claim that the magician has magical abilities and pay him his winning money. Assume that 50 magicians participate in the competition and try to prove that they have magical abilities. All magicians are tested independently. What is the probability that at least one magician wins (and therefore the organizers have to conclude that people with magical abilities exist). Enter answer with 2 digits after decimal point. Ans: W : no. of magicians out of 50 who wins W ~ Binom(n=50, p = 0.5) P(W >= 1) = 1 - P(W=0) 0.92 but this not correct answer",`,"['probability', 'statistics']"
19,Understanding the $\alpha$-regularity assumption for trees,Understanding the -regularity assumption for trees,\alpha,"In this paper , definition 4 claims that a tree grown by recursive partitioning is $\alpha$ -regular for some $\alpha>0$ if each split leaves at least a fraction $\alpha$ of the available training examples on each side of the split, and moreover, the trees are fully grown to depth $k$ for some $k \in \mathbb{N}$ (i.e. there are between $k$ and $2k-1$ observations in each terminal node of the tree. the first part of the definition makes sense to me, but I am not sure I understand how they conclude that there are between $k$ and $2k-1$ observations in a terminal node. Say you train a tree with $n$ observations, and you choose a depth $k$ , then based on the $\alpha$ -regularity, and assuming $\alpha < 0.5$ the minimum number of observations in a node at depth $k$ is going to be $n\alpha^k$ , and the largest is $n(1-\alpha)^k$ - so I am not sure how they are able to conclude that the size of the node is between $k$ and $2k-1$ . update : my best guess is that they didn't mean a depth of $k$ , but rather they require that the tree is grown fully, and since $\alpha$ is a proportion, it can at most be $0.5$ , and so the number of observations in a terminal leaf will be between $k$ and $2k-1$ for some $k$ . As the number of data points gets larger, this requires that the tree to become deeper since (I assume) $k$ is kept fixed","In this paper , definition 4 claims that a tree grown by recursive partitioning is -regular for some if each split leaves at least a fraction of the available training examples on each side of the split, and moreover, the trees are fully grown to depth for some (i.e. there are between and observations in each terminal node of the tree. the first part of the definition makes sense to me, but I am not sure I understand how they conclude that there are between and observations in a terminal node. Say you train a tree with observations, and you choose a depth , then based on the -regularity, and assuming the minimum number of observations in a node at depth is going to be , and the largest is - so I am not sure how they are able to conclude that the size of the node is between and . update : my best guess is that they didn't mean a depth of , but rather they require that the tree is grown fully, and since is a proportion, it can at most be , and so the number of observations in a terminal leaf will be between and for some . As the number of data points gets larger, this requires that the tree to become deeper since (I assume) is kept fixed",\alpha \alpha>0 \alpha k k \in \mathbb{N} k 2k-1 k 2k-1 n k \alpha \alpha < 0.5 k n\alpha^k n(1-\alpha)^k k 2k-1 k \alpha 0.5 k 2k-1 k k,"['statistics', 'statistical-inference', 'machine-learning', 'trees', 'decision-trees']"
20,Showing that swaps of columns does not affect covariance matrix,Showing that swaps of columns does not affect covariance matrix,,"Assume I have the following vector $X^*=(X_1,...,X_p,\tilde X_1, ...,\tilde X_p)$ , with a data matrix $\mathbf{X^*}$ of dimension $n \times 2p$ . The corresponding covariance matrix (lets define it with $G$ ), is given by $G = \begin{bmatrix} \Sigma & \Sigma -  \operatorname{diag}\{s\} \\ \Sigma -  \operatorname{diag}\{s\} & \Sigma \end{bmatrix},$ with $\Sigma$ being a $p\times p$ symmetric covariance matrix of $(X_1,...,X_p)$ and also the symmetric covariance matrix of $(\tilde X_1, ...,\tilde X_p)$ . The term $\operatorname{diag}\{s\}$ is simply a diagonal matrix with non-negative entries on the diagonals. Hence, $\Sigma -  \operatorname{diag}\{s\}$ defines the covariances between $(X_1,...,X_p)$ and $(\tilde X_1, ...,\tilde X_p)$ . I want to show that for any column-wise permutation of $(X_1,...,X_p,\tilde X_1, ...,\tilde X_p)$ or more specifically the data matrix $\mathbf{X}^*$ encoded by the permutation matrix $P$ , the covariance matrix $G$ does not change. With a column-wise permutation, I mean for example swapping the first and $p+1$ th entry, which would be $(\tilde X_1, X_2 ..., X_p, X_1, \tilde X_2 ,...,\tilde X_p )$ / swapping the first and p+1th column in $\mathbf{X}^*$ . This should have the same covariance matrix $G$ . I know how to encode a column-wise permutation with a multiplication of permutation matrix $P$ from the right $\mathbf{X} P$ but I don't know how to prove such a statement in general.","Assume I have the following vector , with a data matrix of dimension . The corresponding covariance matrix (lets define it with ), is given by with being a symmetric covariance matrix of and also the symmetric covariance matrix of . The term is simply a diagonal matrix with non-negative entries on the diagonals. Hence, defines the covariances between and . I want to show that for any column-wise permutation of or more specifically the data matrix encoded by the permutation matrix , the covariance matrix does not change. With a column-wise permutation, I mean for example swapping the first and th entry, which would be / swapping the first and p+1th column in . This should have the same covariance matrix . I know how to encode a column-wise permutation with a multiplication of permutation matrix from the right but I don't know how to prove such a statement in general.","X^*=(X_1,...,X_p,\tilde X_1, ...,\tilde X_p) \mathbf{X^*} n \times 2p G G = \begin{bmatrix}
\Sigma & \Sigma -  \operatorname{diag}\{s\} \\
\Sigma -  \operatorname{diag}\{s\} & \Sigma
\end{bmatrix}, \Sigma p\times p (X_1,...,X_p) (\tilde X_1, ...,\tilde X_p) \operatorname{diag}\{s\} \Sigma -  \operatorname{diag}\{s\} (X_1,...,X_p) (\tilde X_1, ...,\tilde X_p) (X_1,...,X_p,\tilde X_1, ...,\tilde X_p) \mathbf{X}^* P G p+1 (\tilde X_1, X_2 ..., X_p, X_1, \tilde X_2 ,...,\tilde X_p ) \mathbf{X}^* G P \mathbf{X} P","['linear-algebra', 'statistics', 'permutations', 'covariance']"
21,"Statistical tests of the form ""is my PDF correct?""","Statistical tests of the form ""is my PDF correct?""",,"Let's say we observe some data points $x_i$ and we hypothesize they come from some continuous r.v. X which has PDF $f$ . I know that e.g. the Kolmogorov-Smirnov test can be applied to this kind of problems. Do these tests/problems of the form ""is my PDF correct?"" have a specific name? These problems can be viewed as continuous analogues of problems of the kind ""is my PMF correct?"" Example of the latter is the test if a given e.g. 6-sided die is fair. In that test our $H_0$ hypothesis is that $P(X=i) = 1/6$ which is a PMF and we ask: ""Is my PMF correct?"".  Do these also have some name? This question came to me while watching this last lecture of this MIT course. https://youtu.be/rYefUsYuEp0?t=2167","Let's say we observe some data points and we hypothesize they come from some continuous r.v. X which has PDF . I know that e.g. the Kolmogorov-Smirnov test can be applied to this kind of problems. Do these tests/problems of the form ""is my PDF correct?"" have a specific name? These problems can be viewed as continuous analogues of problems of the kind ""is my PMF correct?"" Example of the latter is the test if a given e.g. 6-sided die is fair. In that test our hypothesis is that which is a PMF and we ask: ""Is my PMF correct?"".  Do these also have some name? This question came to me while watching this last lecture of this MIT course. https://youtu.be/rYefUsYuEp0?t=2167",x_i f H_0 P(X=i) = 1/6,['statistics']
22,Need help understanding spline and basis function,Need help understanding spline and basis function,,"I am reading chapter 5 of The Elements of Statistical Learning by Hastie et al.  I would like to ask: Are the two knots positioned at the two red circles in the top left panel? What is the purpose of $h_1(X) = 1$ ? What is the purpose of $h_2(X) = X$ ? Why do we only have $K$ basis functions?  For instance with 2 knots, dont we need 3 equations (one for each interval)? Can I have an explanation for equation $(5.5)$ ? Thank you so much!","I am reading chapter 5 of The Elements of Statistical Learning by Hastie et al.  I would like to ask: Are the two knots positioned at the two red circles in the top left panel? What is the purpose of ? What is the purpose of ? Why do we only have basis functions?  For instance with 2 knots, dont we need 3 equations (one for each interval)? Can I have an explanation for equation ? Thank you so much!",h_1(X) = 1 h_2(X) = X K (5.5),"['linear-algebra', 'matrices', 'statistics', 'spline']"
23,T-test and F-test in Multiple Linear Regression,T-test and F-test in Multiple Linear Regression,,"In simple linear regression, $$ y = \beta_0 + \beta_1X_1, $$ the T-test for $\hat{\beta_1}$ is $$ H_0: \beta_1 = \beta_1^0 \quad \text{and} \quad H_A: \beta_1 \neq \beta_1^0, $$ where $\beta_1^0 = 0$ , and the F-test is $$ H_0: \beta_1 = 0 \quad \text{and} \quad H_A: \beta_1 \neq 0. $$ We know that the T-statistics is $$ T = \frac{\hat{\beta}_1}{se(\hat{\beta}_1)} \implies t_{n-2} $$ and F-statistics is $$ F = \frac{SSreg}{\frac{RSS}{n-1}} \implies F_{1,n-2}, $$ where $RSS = \sum(y_i-\hat{y}_i)^2$ and $SSreg = \sum(\hat{y}_i - \bar{y})^2$ . We know that $$ T^2 = F $$ Problem: I am wondering whether this property still holds in multiple linear regression where predictors are $x_1,x_2,...,x_p$ . Since now the case is $$ H_0: \beta_j = 0 \quad \text{and} \quad H_A: \beta_j \neq 0.  $$ I don't see why it can work here since I find the degrees of freedom don't match in two tests. So I am wondering how can we show the property still holds in this case analytically?","In simple linear regression, the T-test for is where , and the F-test is We know that the T-statistics is and F-statistics is where and . We know that Problem: I am wondering whether this property still holds in multiple linear regression where predictors are . Since now the case is I don't see why it can work here since I find the degrees of freedom don't match in two tests. So I am wondering how can we show the property still holds in this case analytically?","
y = \beta_0 + \beta_1X_1,
 \hat{\beta_1} 
H_0: \beta_1 = \beta_1^0 \quad \text{and} \quad H_A: \beta_1 \neq \beta_1^0,
 \beta_1^0 = 0 
H_0: \beta_1 = 0 \quad \text{and} \quad H_A: \beta_1 \neq 0.
 
T = \frac{\hat{\beta}_1}{se(\hat{\beta}_1)} \implies t_{n-2}
 
F = \frac{SSreg}{\frac{RSS}{n-1}} \implies F_{1,n-2},
 RSS = \sum(y_i-\hat{y}_i)^2 SSreg = \sum(\hat{y}_i - \bar{y})^2 
T^2 = F
 x_1,x_2,...,x_p 
H_0: \beta_j = 0 \quad \text{and} \quad H_A: \beta_j \neq 0. 
","['statistics', 'hypothesis-testing', 'linear-regression']"
24,Joint probability upper bound,Joint probability upper bound,,"Let $X$ and $Y$ independent random variables distributed as $N(0,1)$ . For any $a\in [-1,1]$ , and any positive $t\in\mathbb{R}$ how to find an upper bound of the joint probability $P(aX+\sqrt{1-a^2}Y>t, X>t)$ ? Here $t$ can be thought of a large number so that we are interested in the upper tail and using standard normal tail bound $P(X>t)\leq e^{-t^2/2}$ . My primary interest is to bound to the quantity $P(aX+\sqrt{1-a^2}Y>t | X>t)$ , which can be written as $\frac{P(aX+\sqrt{1-a^2}Y>t, X>t)}{P(X>t)}$ . For the conditional probability, consider three extreme cases: i) $a=1$ , then $P(X>t | X>t)=1$ ii) $a=0$ , then $P(Y>t|X>t)=P(Y>t)\leq e^{-t^2/2}$ (since $X$ and $Y$ are independent and using tail bound of $Y$ ). iii) $a=-1$ , then $P(-X>t|X>t)=0$ . For any other values of $a$ , how to find an upper bound of the conditional probability as a function of $a$ ?","Let and independent random variables distributed as . For any , and any positive how to find an upper bound of the joint probability ? Here can be thought of a large number so that we are interested in the upper tail and using standard normal tail bound . My primary interest is to bound to the quantity , which can be written as . For the conditional probability, consider three extreme cases: i) , then ii) , then (since and are independent and using tail bound of ). iii) , then . For any other values of , how to find an upper bound of the conditional probability as a function of ?","X Y N(0,1) a\in [-1,1] t\in\mathbb{R} P(aX+\sqrt{1-a^2}Y>t, X>t) t P(X>t)\leq e^{-t^2/2} P(aX+\sqrt{1-a^2}Y>t | X>t) \frac{P(aX+\sqrt{1-a^2}Y>t, X>t)}{P(X>t)} a=1 P(X>t | X>t)=1 a=0 P(Y>t|X>t)=P(Y>t)\leq e^{-t^2/2} X Y Y a=-1 P(-X>t|X>t)=0 a a","['probability', 'probability-theory', 'statistics', 'probability-distributions', 'computational-mathematics']"
25,"Convergence in quadratic mean, addition of random variables","Convergence in quadratic mean, addition of random variables",,"We defined that $X_n\rightarrow X$ , i.e, that $X_n$ converges in quadratic mean to $X$ , if $\mathbb{E}[(X_n-X)^2]\rightarrow0$ as $n\rightarrow\infty$ . I'm trying to prove the following statement: $$X_n\rightarrow X, Y_n\rightarrow Y \Rightarrow X_n+Y_n\rightarrow X+Y$$ I did the following: \begin{split} \mathbb{E}[(X_n+Y_n-(X+Y))^2] &= \mathbb{E}[(X_n-X+Y_n-Y)^2] \\ &= \mathbb{E}[(X_n-X)^2]+2\mathbb{E}[(X_n-X)(Y_n-Y)]+\mathbb{E}[(Y_n-Y)^2] \\ \end{split} I know that the first and third terms go to $0$ as $n\rightarrow \infty$ , but what happens to the term in the middle? How do I show that it goes to $0$ as $n\rightarrow \infty$ ?","We defined that , i.e, that converges in quadratic mean to , if as . I'm trying to prove the following statement: I did the following: I know that the first and third terms go to as , but what happens to the term in the middle? How do I show that it goes to as ?","X_n\rightarrow X X_n X \mathbb{E}[(X_n-X)^2]\rightarrow0 n\rightarrow\infty X_n\rightarrow X, Y_n\rightarrow Y \Rightarrow X_n+Y_n\rightarrow X+Y \begin{split}
\mathbb{E}[(X_n+Y_n-(X+Y))^2] &= \mathbb{E}[(X_n-X+Y_n-Y)^2] \\
&= \mathbb{E}[(X_n-X)^2]+2\mathbb{E}[(X_n-X)(Y_n-Y)]+\mathbb{E}[(Y_n-Y)^2] \\
\end{split} 0 n\rightarrow \infty 0 n\rightarrow \infty","['statistics', 'convergence-divergence', 'order-statistics']"
26,Is there a way to find UMVUE without just guessing?,Is there a way to find UMVUE without just guessing?,,"We have $(X_1,...,X_n)$ random sample of distribution $N(m,1)$ . I need to find UMVUE of $g(m)=e^m$ . The natural guess was that the unbiased estimator which I need to find may be something like $e^T$ , where $T=\frac{1}{n} \sum\limits_{i=1}^n X_i$ . So I calculated $$\mathbb{E}[e^{T}]=exp \left[m+\frac{1}{2n}\right]$$ therefore the unbiased estimator that I was looking for is $g(T)=e^{T-\frac{1}{2n}}.$ My question is: is there a way to get this result in any other way than just guessing? I considered solving: $$\mathbb{E}g(T)=e^m$$ so I would get $$\int\limits_0^\infty g(t) \frac{1}{\sqrt{\frac{2 \pi}{n}}}e^{-\frac{(t-m)^2}{\frac{2}{n}}} dt=e^m$$ but that doesn't seem to get me anywhere. What do you think?","We have random sample of distribution . I need to find UMVUE of . The natural guess was that the unbiased estimator which I need to find may be something like , where . So I calculated therefore the unbiased estimator that I was looking for is My question is: is there a way to get this result in any other way than just guessing? I considered solving: so I would get but that doesn't seem to get me anywhere. What do you think?","(X_1,...,X_n) N(m,1) g(m)=e^m e^T T=\frac{1}{n} \sum\limits_{i=1}^n X_i \mathbb{E}[e^{T}]=exp \left[m+\frac{1}{2n}\right] g(T)=e^{T-\frac{1}{2n}}. \mathbb{E}g(T)=e^m \int\limits_0^\infty g(t) \frac{1}{\sqrt{\frac{2 \pi}{n}}}e^{-\frac{(t-m)^2}{\frac{2}{n}}} dt=e^m","['statistics', 'normal-distribution', 'estimation', 'parameter-estimation']"
27,Finding autocorrelation function,Finding autocorrelation function,,"$\{X(t),t>0\}$ be a random variable and have $Y\sim U(0,\pi )$ distribution. Let $X(t)=e^{2Y}t$ . Find autocorrelation function of $X(t)$ . $Y\sim U(0,\pi )\Rightarrow f_Y(y)=\frac 1 \pi $ where $0\le y\le \pi$ then $\begin{align}R_{X}(t_{1},t_{2})&=\Bbb E\left [ X({t_{1}})X(t_{2}) \right ]\\[1ex]&=\Bbb E\left [ e^{2Y}t_{1}\cdot e^{2Y}t_{2} \right ]\\[1ex]&=\Bbb E\left [ e^{4Y}\cdot t_{1}t_{2} \right ]\\[1ex]&=\int_{0}^{\pi }\frac{1}{\pi }e^{4y}(t_{1}t_{2})dy\\[1ex]&=\frac{t_{1}t_{2}}{4\pi }(e^{4\pi }-1)\end{align}$ Is my solution okay? Any answer will be appreciated.",be a random variable and have distribution. Let . Find autocorrelation function of . where then Is my solution okay? Any answer will be appreciated.,"\{X(t),t>0\} Y\sim U(0,\pi ) X(t)=e^{2Y}t X(t) Y\sim U(0,\pi )\Rightarrow f_Y(y)=\frac 1 \pi  0\le y\le \pi \begin{align}R_{X}(t_{1},t_{2})&=\Bbb E\left [ X({t_{1}})X(t_{2}) \right ]\\[1ex]&=\Bbb E\left [ e^{2Y}t_{1}\cdot e^{2Y}t_{2} \right ]\\[1ex]&=\Bbb E\left [ e^{4Y}\cdot t_{1}t_{2} \right ]\\[1ex]&=\int_{0}^{\pi }\frac{1}{\pi }e^{4y}(t_{1}t_{2})dy\\[1ex]&=\frac{t_{1}t_{2}}{4\pi }(e^{4\pi }-1)\end{align}","['probability', 'statistics', 'probability-distributions', 'expected-value', 'covariance']"
28,EM algorithm for maximum of 2 normal distribution,EM algorithm for maximum of 2 normal distribution,,"Let $X_i \sim N(\mu_1,\sigma^2), Y_i \sim N(\mu_2,\sigma^2)$ $O_i = \max(X_i,Y_i)$ i need to find $\mu_1, \mu_2$ using EM my attempt: first i defined $Z_i = \left\{\begin{matrix} 1, ~ X_i \ge Y_i \\ 0, ~ X_i < Y_i \end{matrix}\right.$ we can assume we know Z there for: $O_i = X_i*Z_i + Y_i * (1-Z_i)$ $L(O,Z) = \Pi f_x^{z_i} f_y^{z_i} p^{z_i} (1-p)^{1-z_i} \Rightarrow l :=log-likelihood = ∑z_i  ln⁡(p)+n-∑z_i  ln⁡(1-p)+nln(2√π  σ)-\frac{(∑(o_i-μ_1 )^2 z_i+ (o_i-μ_2 )^2 (1-z_i)}{2σ}  $ where $ p = P(Z=1) = P(Y-X\le0) =P(N(μ_2-μ_1,2 σ^2)≤0)$ E stage: let $e_i=E(Z_i |O_i=k)$ $Q = E(l│O)=∑e_i  ln⁡(p)+n-∑e_i  ln⁡(1-p)+nln(2√π  σ)-\frac{(∑(o_i-μ_1 )^2 e_i+ (o_i-μ_2 )^2 (1-e_i)}{2σ}$ M stage: $\frac{\partial Q}{\partial \mu_1 } = \frac{\sum(O_i + \mu_1)e_i}{\sigma}  = 0 \iff \mu_1 = \frac{\sum O_i e_i }{\sum e_i}$ $\frac{\partial Q}{\partial \mu_2 } = \frac{\sum(O_i + \mu_2)(1-e_i)}{\sigma}  = 0 \iff \mu_2 = \frac{\sum O_i (1-e_i) }{\sum (1-e_i)}$ i do does stages until Q starts to converge i tried this with some given values but the results were wrong, where did i go wrong in my calculations? who should i solve this?","Let i need to find using EM my attempt: first i defined we can assume we know Z there for: where E stage: let M stage: i do does stages until Q starts to converge i tried this with some given values but the results were wrong, where did i go wrong in my calculations? who should i solve this?","X_i \sim N(\mu_1,\sigma^2), Y_i \sim N(\mu_2,\sigma^2) O_i = \max(X_i,Y_i) \mu_1, \mu_2 Z_i = \left\{\begin{matrix}
1, ~ X_i \ge Y_i
\\
0, ~ X_i < Y_i
\end{matrix}\right. O_i = X_i*Z_i + Y_i * (1-Z_i) L(O,Z) = \Pi f_x^{z_i} f_y^{z_i} p^{z_i} (1-p)^{1-z_i} \Rightarrow l :=log-likelihood = ∑z_i  ln⁡(p)+n-∑z_i  ln⁡(1-p)+nln(2√π  σ)-\frac{(∑(o_i-μ_1 )^2 z_i+ (o_i-μ_2 )^2 (1-z_i)}{2σ}    p = P(Z=1) = P(Y-X\le0) =P(N(μ_2-μ_1,2 σ^2)≤0) e_i=E(Z_i |O_i=k) Q = E(l│O)=∑e_i  ln⁡(p)+n-∑e_i  ln⁡(1-p)+nln(2√π  σ)-\frac{(∑(o_i-μ_1 )^2 e_i+ (o_i-μ_2 )^2 (1-e_i)}{2σ} \frac{\partial Q}{\partial \mu_1 } = \frac{\sum(O_i + \mu_1)e_i}{\sigma}  = 0 \iff \mu_1 = \frac{\sum O_i e_i }{\sum e_i} \frac{\partial Q}{\partial \mu_2 } = \frac{\sum(O_i + \mu_2)(1-e_i)}{\sigma}  = 0 \iff \mu_2 = \frac{\sum O_i (1-e_i) }{\sum (1-e_i)}","['statistics', 'optimization', 'algorithms', 'conditional-expectation', 'machine-learning']"
29,Hitting time for 2-dimensional brownian motion to hit the unit circle,Hitting time for 2-dimensional brownian motion to hit the unit circle,,"Let $\tau=\inf\{t : ||W_t||=1 \}$ for a two dimensional brownian motion $W_t$ .  We want to find the expectation of $\tau$ . My attempt:  So far I know that $X_t=(W_t^1)^2+(W_t^2)^2-2t$ is a martingale and $\tau$ is a stopping time for $X_t$ , so would I just say that $0=E(X_0)=E(X_{\tau})=1-2E(\tau)$ and conclude $E(\tau)=\frac{1}{2}$ ?","Let for a two dimensional brownian motion .  We want to find the expectation of . My attempt:  So far I know that is a martingale and is a stopping time for , so would I just say that and conclude ?",\tau=\inf\{t : ||W_t||=1 \} W_t \tau X_t=(W_t^1)^2+(W_t^2)^2-2t \tau X_t 0=E(X_0)=E(X_{\tau})=1-2E(\tau) E(\tau)=\frac{1}{2},"['probability', 'probability-theory', 'statistics']"
30,The probability of recovering from a disease,The probability of recovering from a disease,,"It is known that the probability of recovering from a disease is $0.23$ . In a community of 6 people with this disease, what is the probability that at least $3$ people will survive? Hello guys, today our teacher had asked this question and I found and answer. I used binomial distribution to solve this problem and my answer is $0.1391$ , in the system the answer $0,89$ . How can this would be possible? I am thinking that answer of teacher is wrong and mine is correct.","It is known that the probability of recovering from a disease is . In a community of 6 people with this disease, what is the probability that at least people will survive? Hello guys, today our teacher had asked this question and I found and answer. I used binomial distribution to solve this problem and my answer is , in the system the answer . How can this would be possible? I am thinking that answer of teacher is wrong and mine is correct.","0.23 3 0.1391 0,89","['probability', 'statistics', 'rational-numbers', 'binomial-distribution', 'word-problem']"
31,To calculate correleation between two large vectors [closed],To calculate correleation between two large vectors [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have some graph with $500$ k nodes. after applying different hierarchical clustering algorithms I want to calculate the cophenetic correlation coefficient. However, since the pairwise distance vectors length is roughly $n^2$ , straight forward correlation calculation is not feasible. How can I split the calcualtion? Or is there any different approach in order to calculate the cophenetic correlation coefficient?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have some graph with k nodes. after applying different hierarchical clustering algorithms I want to calculate the cophenetic correlation coefficient. However, since the pairwise distance vectors length is roughly , straight forward correlation calculation is not feasible. How can I split the calcualtion? Or is there any different approach in order to calculate the cophenetic correlation coefficient?",500 n^2,"['statistics', 'graph-theory']"
32,Find $a_1$ and $a_2$ such that estimator is unbiased,Find  and  such that estimator is unbiased,a_1 a_2,"Let us consider the sample of independent random variables $X_1,\ldots,X_n$ with $E[X_1]=E[X_2]\ldots=E[X_n]=\mu$ . We examine the following estimator of $\mu$ : $$a_1(X_1+X_2+\ldots+X_n)+a_2.$$ Find constants $a_1$ and $a_2$ such that the estimator is unbiased. I proceed as follows: $$E[a_1(X_1+X_2+\ldots+X_n)+a_2]=a_1E[X_1+X_2+\ldots+X_n]+a_2=a_1n\mu+a_2.$$ So estimator is unbiased for $a_1=1/n$ and $a_2=0$ since the above expected value is equal to $\mu$ . I wonder if I should separately consider the case when $a_2\neq 0$ and find $a_1$ such that the expected value is equal to $\mu$ .",Let us consider the sample of independent random variables with . We examine the following estimator of : Find constants and such that the estimator is unbiased. I proceed as follows: So estimator is unbiased for and since the above expected value is equal to . I wonder if I should separately consider the case when and find such that the expected value is equal to .,"X_1,\ldots,X_n E[X_1]=E[X_2]\ldots=E[X_n]=\mu \mu a_1(X_1+X_2+\ldots+X_n)+a_2. a_1 a_2 E[a_1(X_1+X_2+\ldots+X_n)+a_2]=a_1E[X_1+X_2+\ldots+X_n]+a_2=a_1n\mu+a_2. a_1=1/n a_2=0 \mu a_2\neq 0 a_1 \mu","['statistics', 'estimation', 'parameter-estimation']"
33,Difficulty in computing the full conditional for a Probit model (Hoff 6.3),Difficulty in computing the full conditional for a Probit model (Hoff 6.3),,"In a course of Bayesian statistics, I have been given an excercise from Hoff, about the computations of some full conditional for the Probit model. I have not great confidence with conditional probability computation. So I would like to ask if the following resoning is correct. Thanks in advance. Consider the following Probit model: $$Z_i=\beta x_i+\epsilon_i\qquad\epsilon_1,\dots,\epsilon_n\sim_{iid}\mathcal{N}(0,1)$$ $$Y_i=\mathbb{1}_{(c,+\infty)}(Z_i)$$ where: $\beta$ , $c$ unknown parameters, for which we assume priors $\beta\sim\mathcal{N}(0,\sigma_\beta^2)$ , $c\sim\mathcal{N}(0,\sigma_c^2)$ the covariates $x_i$ are known and hence tretated as constants We have to derive $p(\beta\mid z_{1:n}, y_{1:n}, c)$ . Clearly, $c$ and $y_{1:n}$ bring no information on $\beta$ . Then: $$p(\beta\mid z_{1:n},y_{1:n}, c)=p(\beta\mid z_{1:n})\propto p(z_{1:n}\mid\beta)p(\beta)$$ $$\propto\mathtt{exp}(-\frac{1}{2}\{\sum_{i=1}^{n}(z_i-\beta x_i)^2+(\beta/\sigma_\beta)^2\})                 \propto \mathtt{exp}[-\frac{1}{2}(\frac{\beta-\tilde{\mu}_\beta}{\tilde{\sigma}_\beta})^2]$$ where: $$\tilde{\sigma}_\beta^2=\left(\sum_{i=1}^n x_i^2+\frac{1}{\sigma^2_\beta}\right)^{-1}\qquad\tilde{\mu}_\beta=\tilde{\sigma}_\beta\left(\sum_{i=1}^n x_iz_i\right)$$ Hence, we conclude $p(\beta\mid z_{1:n}, y_{1:n}, c)\sim\mathcal{N}(\tilde{\mu}_\beta,\tilde{\sigma}_\beta)$ Question 1 Is this computation correct? We have to derive $p(c\mid z_{1:n}, y_{1:n}, \beta)$ Clearly, $\beta$ brings no information on $c$ . Moreover, observe that $p(c\mid z_{1:n})=p(c)$ , since $z_{1:n}$ alone brings no information on $c$ . Then: $$ p(c\mid\beta,z_{1:n},y_{1:n})=p(c\mid z_{1:n},y_{1:n})\propto p(y_{1:n}\mid c,z_{1:n})p(c\mid z_{1:n})=p(y_{1:n}\mid c,z_{1:n})p(c)$$ Now, observe that $y_i\mid c,z_i$ is not random any more. In particular it is one if $z_i>c$ and it is $0$ if $z_i\leq c$ . Then we may write, observing that $z_{-i}$ brings no information about $y_i$ : $$p(y_i\mid c,z_{1:n})=p(y_i\mid  c,z_i)=\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}$$ Then: $$p(y_{1:n}\mid c,z_{1:n})=\prod_{i=1}^{n}\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}$$ And in turn: $$p(c\mid\beta,z_{1:n},y_{1:n})\propto\left(\prod_{i=1}^{n}\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}\right)\mathcal{N}(0,\sigma_c^2)$$ Question 2 According to a given hint,parenthesis should reduce to the charachterstic function of an interval, so that we have a truncated gaussian. But I cannot see how it can. can someone help? We have to derive $p(z_i\mid z_{-i}, y_{1:n}, \beta, c)$ . Clearly, $z_{-i}$ brings no information on $z_{-i}$ , similarly for $y_{-i}$ . $$p(z_i\mid y_{i},\beta,c)\propto p(y_i\mid z_{i},\beta,c)p(c)                     \propto p(y_i\mid z_{i},c)p(c)                     \propto \mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}\mathcal{N}(0,\sigma_c^2)$$ Question 3 Is this computation correct?","In a course of Bayesian statistics, I have been given an excercise from Hoff, about the computations of some full conditional for the Probit model. I have not great confidence with conditional probability computation. So I would like to ask if the following resoning is correct. Thanks in advance. Consider the following Probit model: where: , unknown parameters, for which we assume priors , the covariates are known and hence tretated as constants We have to derive . Clearly, and bring no information on . Then: where: Hence, we conclude Question 1 Is this computation correct? We have to derive Clearly, brings no information on . Moreover, observe that , since alone brings no information on . Then: Now, observe that is not random any more. In particular it is one if and it is if . Then we may write, observing that brings no information about : Then: And in turn: Question 2 According to a given hint,parenthesis should reduce to the charachterstic function of an interval, so that we have a truncated gaussian. But I cannot see how it can. can someone help? We have to derive . Clearly, brings no information on , similarly for . Question 3 Is this computation correct?","Z_i=\beta x_i+\epsilon_i\qquad\epsilon_1,\dots,\epsilon_n\sim_{iid}\mathcal{N}(0,1) Y_i=\mathbb{1}_{(c,+\infty)}(Z_i) \beta c \beta\sim\mathcal{N}(0,\sigma_\beta^2) c\sim\mathcal{N}(0,\sigma_c^2) x_i p(\beta\mid z_{1:n}, y_{1:n}, c) c y_{1:n} \beta p(\beta\mid z_{1:n},y_{1:n}, c)=p(\beta\mid z_{1:n})\propto p(z_{1:n}\mid\beta)p(\beta) \propto\mathtt{exp}(-\frac{1}{2}\{\sum_{i=1}^{n}(z_i-\beta x_i)^2+(\beta/\sigma_\beta)^2\})
                \propto \mathtt{exp}[-\frac{1}{2}(\frac{\beta-\tilde{\mu}_\beta}{\tilde{\sigma}_\beta})^2] \tilde{\sigma}_\beta^2=\left(\sum_{i=1}^n x_i^2+\frac{1}{\sigma^2_\beta}\right)^{-1}\qquad\tilde{\mu}_\beta=\tilde{\sigma}_\beta\left(\sum_{i=1}^n x_iz_i\right) p(\beta\mid z_{1:n}, y_{1:n}, c)\sim\mathcal{N}(\tilde{\mu}_\beta,\tilde{\sigma}_\beta) p(c\mid z_{1:n}, y_{1:n}, \beta) \beta c p(c\mid z_{1:n})=p(c) z_{1:n} c  p(c\mid\beta,z_{1:n},y_{1:n})=p(c\mid z_{1:n},y_{1:n})\propto p(y_{1:n}\mid c,z_{1:n})p(c\mid z_{1:n})=p(y_{1:n}\mid c,z_{1:n})p(c) y_i\mid c,z_i z_i>c 0 z_i\leq c z_{-i} y_i p(y_i\mid c,z_{1:n})=p(y_i\mid  c,z_i)=\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i} p(y_{1:n}\mid c,z_{1:n})=\prod_{i=1}^{n}\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i} p(c\mid\beta,z_{1:n},y_{1:n})\propto\left(\prod_{i=1}^{n}\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}\right)\mathcal{N}(0,\sigma_c^2) p(z_i\mid z_{-i}, y_{1:n}, \beta, c) z_{-i} z_{-i} y_{-i} p(z_i\mid y_{i},\beta,c)\propto p(y_i\mid z_{i},\beta,c)p(c)
                    \propto p(y_i\mid z_{i},c)p(c)
                    \propto \mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}\mathcal{N}(0,\sigma_c^2)","['probability', 'statistics', 'conditional-probability', 'bayesian']"
34,Proof regarding conditional expectation of sum of minima,Proof regarding conditional expectation of sum of minima,,"Assume $X_1, X_2$ are continuous bounded random variables, where the dependency between them is unknown, where the joint and marginal density functions are well-defined. Let \begin{align} I_1 := \min(X_1, \overset{\sim}{a_1}), \quad I_2 := \min(X_2,\overset{\sim}{a_2}),  \end{align} where $\overset{\sim}{a_1}, \overset{\sim}{a_2}$ are non-negative numbers. Let $\alpha \in (0, 1)$ be fixed, and let $F_{I_1+I_1}^{-1}(\alpha)$ denote the $\alpha$ -quantile of $I_1+I_2$ . Moreover, define $[t]^+ := \max(t, 0)$ . I am trying to show that given $a_1, a_2, \overset{\sim}{a_1}, \overset{\sim}{a_2}$ such that \begin{align} a_1+a_2 = F_{I_1+I_2}^{-1}(\alpha) \end{align} and \begin{align}  \overset{\sim}{a_1}+ \overset{\sim}{a_2} > F_{I_1+I_2}^{-1}(\alpha),  \end{align} with $a_1 < \sup X_1, a_2 < \sup X_2$ , then \begin{align} & E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I_2}^{-1}(\alpha)] + E\left[ [X_1 -\overset{\sim}{a_1}]^+\right] + E\left[ [X_1 -\overset{\sim}{a_2}]^+\right]\\ & \leq a_1 + a_2 + E\left[ [X_1 -a_1]^+\right] + E\left[ [X_1 -a_2^+\right]. \end{align} I have not been able to contradict this using Monte Carlo simulation, but I cannot seem to prove it formally. I have considered \begin{align} E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I _2}^{-1}(\alpha)] &= \frac{1}{1-\alpha} \int_{\mathbb{R}} x f_{I_1+I_2}(x) \chi_{\{x \geq F_{X_1+X_2}^{-1}(\alpha)\}}\ dx\\ &= \frac{1}{1-\alpha} \left( \int_{\mathbb{R}} x f_{I_1+I_2}(x) \chi_{ \{\overset{\sim}{a_1}+\overset{\sim}{a_2} > x \geq F_{I_1+I_2}^{-1}(\alpha)\}}\ dx + (\overset{\sim}{a_1} +\overset{\sim}{a_2})( 1-F_{I_1+I_2}(\overset{\sim}{a_1} +\overset{\sim}{a_2})) \right). \end{align} The integrand is clearly larger than $a_1+a_2$ everywhere, so it evaluates to a larger value than $a_1+a_2$ . At the same time, $E[ [X_1-a_1]^+]$ and $E[ [X_2-a_2]^+]$ are decreasing in $a_1$ and $a_2$ , but as we don't make assumptions on $a_1, a_2$ but on the sum $a_1+a_2$ , the change in these expectations would intuitively depend on the tails of the distributions of $X_1$ , $X_2$ . Is there a way forward with this problem? Or is my setting too general? Do I need to assume independence between $X_1$ and $X_2$ to get anywhere? Edit: Another consideration - The terms \begin{align} E[[X_1-\overset{\sim}{a_1} ]^+] + E[X_2-\overset{\sim}{a_2}]^+] & \geq E[[X_1 + X_2 - (\overset{\sim}{a_1} +\overset{\sim}{a_2})]^+], \end{align} by convexity of the function $[\cdot]^+$ . On the other hand, \begin{align}  & E[[X_1-a_1 ]^+] + E[X_2-a_2]^+] - E[[X_1-\overset{\sim}{a_1} ]^+] + E[X_2-\overset{\sim}{a_2}]^+]\\ &= \int_{a_1}^{\overset{\sim}{a_1}} xf_{X_1}(x) \ dx + \int_{a_2}^{\overset{\sim}{a_2}} xf_{X_2}(x) \ dx,  \end{align} so I need to show that \begin{align}  E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I_2}^{-1}(\alpha)] - F^{-1}_{I_1+I_2}(\alpha) \geq \int_{a_1}^{\overset{\sim}{a_1}} xf_{X_1}(x)\ dx + \int_{a_2}^{\overset{\sim}{a_2}} xf_{X_2}(x) \ dx,  \end{align}","Assume are continuous bounded random variables, where the dependency between them is unknown, where the joint and marginal density functions are well-defined. Let where are non-negative numbers. Let be fixed, and let denote the -quantile of . Moreover, define . I am trying to show that given such that and with , then I have not been able to contradict this using Monte Carlo simulation, but I cannot seem to prove it formally. I have considered The integrand is clearly larger than everywhere, so it evaluates to a larger value than . At the same time, and are decreasing in and , but as we don't make assumptions on but on the sum , the change in these expectations would intuitively depend on the tails of the distributions of , . Is there a way forward with this problem? Or is my setting too general? Do I need to assume independence between and to get anywhere? Edit: Another consideration - The terms by convexity of the function . On the other hand, so I need to show that","X_1, X_2 \begin{align}
I_1 := \min(X_1, \overset{\sim}{a_1}), \quad I_2 := \min(X_2,\overset{\sim}{a_2}), 
\end{align} \overset{\sim}{a_1}, \overset{\sim}{a_2} \alpha \in (0, 1) F_{I_1+I_1}^{-1}(\alpha) \alpha I_1+I_2 [t]^+ := \max(t, 0) a_1, a_2, \overset{\sim}{a_1}, \overset{\sim}{a_2} \begin{align}
a_1+a_2 = F_{I_1+I_2}^{-1}(\alpha)
\end{align} \begin{align}
 \overset{\sim}{a_1}+ \overset{\sim}{a_2} > F_{I_1+I_2}^{-1}(\alpha), 
\end{align} a_1 < \sup X_1, a_2 < \sup X_2 \begin{align}
& E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I_2}^{-1}(\alpha)] + E\left[ [X_1 -\overset{\sim}{a_1}]^+\right] + E\left[ [X_1 -\overset{\sim}{a_2}]^+\right]\\ & \leq a_1 + a_2 + E\left[ [X_1 -a_1]^+\right] + E\left[ [X_1 -a_2^+\right].
\end{align} \begin{align}
E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I
_2}^{-1}(\alpha)] &= \frac{1}{1-\alpha} \int_{\mathbb{R}} x f_{I_1+I_2}(x) \chi_{\{x \geq F_{X_1+X_2}^{-1}(\alpha)\}}\ dx\\
&= \frac{1}{1-\alpha} \left( \int_{\mathbb{R}} x f_{I_1+I_2}(x) \chi_{ \{\overset{\sim}{a_1}+\overset{\sim}{a_2} > x \geq F_{I_1+I_2}^{-1}(\alpha)\}}\ dx + (\overset{\sim}{a_1} +\overset{\sim}{a_2})( 1-F_{I_1+I_2}(\overset{\sim}{a_1} +\overset{\sim}{a_2})) \right).
\end{align} a_1+a_2 a_1+a_2 E[ [X_1-a_1]^+] E[ [X_2-a_2]^+] a_1 a_2 a_1, a_2 a_1+a_2 X_1 X_2 X_1 X_2 \begin{align}
E[[X_1-\overset{\sim}{a_1} ]^+] + E[X_2-\overset{\sim}{a_2}]^+] & \geq E[[X_1 + X_2 - (\overset{\sim}{a_1} +\overset{\sim}{a_2})]^+],
\end{align} [\cdot]^+ \begin{align}
 & E[[X_1-a_1 ]^+] + E[X_2-a_2]^+] - E[[X_1-\overset{\sim}{a_1} ]^+] + E[X_2-\overset{\sim}{a_2}]^+]\\
&= \int_{a_1}^{\overset{\sim}{a_1}} xf_{X_1}(x) \ dx + \int_{a_2}^{\overset{\sim}{a_2}} xf_{X_2}(x) \ dx, 
\end{align} \begin{align}
 E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I_2}^{-1}(\alpha)] - F^{-1}_{I_1+I_2}(\alpha) \geq \int_{a_1}^{\overset{\sim}{a_1}} xf_{X_1}(x)\ dx + \int_{a_2}^{\overset{\sim}{a_2}} xf_{X_2}(x) \ dx, 
\end{align}","['probability', 'integration', 'statistics', 'conditional-expectation', 'stochastic-analysis']"
35,Marginal distribution of a normalized vector of Gaussians,Marginal distribution of a normalized vector of Gaussians,,"Imagine I have a vector $\mathbf{x} = [x_1, \dots, x_N]^\top$ where each of the $x_n \sim \mathcal{N}(0, 1)$ . Its $\ell_2$ -norm is $||\mathbf{x}||_2 = \sqrt{x_1^2 +\dots + x_N^2}$ . I am interested in the distribution of the marginals of the normalized $\mathbf{y} = \Big[\frac{x_1}{||\mathbf{x}||_2}, \dots, \frac{x_N}{||\mathbf{x}||_2}\Big]^\top$ . Is there a way to specify the distribution of the individual $\frac{x_n}{||\mathbf{x}||_2}, n\in[N]$ at all? By simulation, it seems that at least when $N$ grows, the marginals behave like Gaussian random variables with $\mathcal{N}(0, N^{-1})$ . Or is that effect solely due to an approximate independence between numerator and denominator for large $N$ , which makes the marginals act like a student-t random variable? So far, I have found plenty on the distribution of the entire vector, for example here: On the distribution of a normalized Gaussian vector .","Imagine I have a vector where each of the . Its -norm is . I am interested in the distribution of the marginals of the normalized . Is there a way to specify the distribution of the individual at all? By simulation, it seems that at least when grows, the marginals behave like Gaussian random variables with . Or is that effect solely due to an approximate independence between numerator and denominator for large , which makes the marginals act like a student-t random variable? So far, I have found plenty on the distribution of the entire vector, for example here: On the distribution of a normalized Gaussian vector .","\mathbf{x} = [x_1, \dots, x_N]^\top x_n \sim \mathcal{N}(0, 1) \ell_2 ||\mathbf{x}||_2 = \sqrt{x_1^2 +\dots + x_N^2} \mathbf{y} = \Big[\frac{x_1}{||\mathbf{x}||_2}, \dots, \frac{x_N}{||\mathbf{x}||_2}\Big]^\top \frac{x_n}{||\mathbf{x}||_2}, n\in[N] N \mathcal{N}(0, N^{-1}) N","['probability', 'statistics', 'random-variables', 'normal-distribution', 'marginal-distribution']"
36,When is the probability measure in de Finetti's theorem finitely supported?,When is the probability measure in de Finetti's theorem finitely supported?,,"Let $(X_i)_{i=1}^\infty$ be an exchangeable infinite sequence of random variables taking values in $\{0,1\}$ . De Finetti's theorem states that there exists a unique probability measure $\mu$ on $[0,1]$ such that for any fixed sequence $(e_i)_{i=1}^\infty$ taking values in $\{0,1\}$ it holds that $$\Pr(X_1=e_1,\dots,X_n=e_n) = \int_0^1 p^k(1-p)^{n-k} d\mu(p),\tag{1}$$ where $k = \sum_{i=1}^n e_i$ . Thus, for each $n$ , the distribution of $(X_i)_{i=1}^n$ is a (possibly infinite) mixture of independent Bernoulli trials. Question: Under what conditions on $(X_i)_{i=1}^\infty$ does $\mu$ have finite support? That is, when does there exist $m\in \mathbb N$ , $p_1,\dots,p_m\in [0,1]$ and $\alpha_1,\dots,\alpha_m \ge 0$ with $\sum_{j=1}^m \alpha_j = 1$ such that for any fixed sequence $(e_i)_{i=1}^\infty$ taking values in $\{0,1\}$ : $$\Pr(X_1=e_1,\dots,X_n=e_n) = \sum_{j=1}^m p_j^k(1-p_j)^{n-k} \alpha_j,\tag{2}$$ where $k = \sum_{i=1}^n e_i$ ?","Let be an exchangeable infinite sequence of random variables taking values in . De Finetti's theorem states that there exists a unique probability measure on such that for any fixed sequence taking values in it holds that where . Thus, for each , the distribution of is a (possibly infinite) mixture of independent Bernoulli trials. Question: Under what conditions on does have finite support? That is, when does there exist , and with such that for any fixed sequence taking values in : where ?","(X_i)_{i=1}^\infty \{0,1\} \mu [0,1] (e_i)_{i=1}^\infty \{0,1\} \Pr(X_1=e_1,\dots,X_n=e_n) = \int_0^1 p^k(1-p)^{n-k} d\mu(p),\tag{1} k = \sum_{i=1}^n e_i n (X_i)_{i=1}^n (X_i)_{i=1}^\infty \mu m\in \mathbb N p_1,\dots,p_m\in [0,1] \alpha_1,\dots,\alpha_m \ge 0 \sum_{j=1}^m \alpha_j = 1 (e_i)_{i=1}^\infty \{0,1\} \Pr(X_1=e_1,\dots,X_n=e_n) = \sum_{j=1}^m p_j^k(1-p_j)^{n-k} \alpha_j,\tag{2} k = \sum_{i=1}^n e_i","['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'statistics']"
37,Statistical advantages of complex-valued random variables?,Statistical advantages of complex-valued random variables?,,"I am interested in random complex numbers and am trying to understand: why to use complex random variables in statistics? Complex numbers can equally be viewed as a vector in $\mathbb{R}^2$ , but with a defined multiplication (or $2\times 2$ matrices). This thread addresses how to do linear regression with complex data. There are also several discussions around the value of complex numbers in Quantum Mechanics, where you also encounter probabilistic behaviour. I am aware complex-valued data arise in areas such as signal processing through complex numbers as a transform of real information. Alternatively, complex numbers can be constructed to represent a pair of real values and provide compact notation in other areas such as economics. Statistical operations are then run upon such data obeying the complex algebra. A common discussed benefit is compactness of notation, but do we also have calculation or computational benefits by taking advantage of $\mathbb{C}$ and its multiplication? To ask another way, once you have complex-valued data, are there specific statistical (e.g. distribution, modelling, estimation) benefits by continuing to treat it in the complex field, rather than using real bivariate statistical methods? (Note: This question was originally posted on CV , however I am hoping to broaden the audience it reaches given that it originates as a consequence of other fields' use of complex numbers.)","I am interested in random complex numbers and am trying to understand: why to use complex random variables in statistics? Complex numbers can equally be viewed as a vector in , but with a defined multiplication (or matrices). This thread addresses how to do linear regression with complex data. There are also several discussions around the value of complex numbers in Quantum Mechanics, where you also encounter probabilistic behaviour. I am aware complex-valued data arise in areas such as signal processing through complex numbers as a transform of real information. Alternatively, complex numbers can be constructed to represent a pair of real values and provide compact notation in other areas such as economics. Statistical operations are then run upon such data obeying the complex algebra. A common discussed benefit is compactness of notation, but do we also have calculation or computational benefits by taking advantage of and its multiplication? To ask another way, once you have complex-valued data, are there specific statistical (e.g. distribution, modelling, estimation) benefits by continuing to treat it in the complex field, rather than using real bivariate statistical methods? (Note: This question was originally posted on CV , however I am hoping to broaden the audience it reaches given that it originates as a consequence of other fields' use of complex numbers.)",\mathbb{R}^2 2\times 2 \mathbb{C},"['statistics', 'complex-numbers', 'random-variables']"
38,Which tosses should one choose to better estimate $p$?,Which tosses should one choose to better estimate ?,p,"Puzzle $1$ : A coin $C_1$ has probability $p$ of turning up head, while a coin $C_2$ has probability $2p$ of turning up head. All we know is that $0 < p < \frac12$ . Now, $20$ tosses are given. You can choose all tosses from $C_1$ , all tosses from $C_2$ , or some tosses from each (the total is $20$ ). If the objective is to estimate $p$ , what do you do? (Hint: Minimize mean-squared error of the estimator). If we choose all $n = 20$ tosses from $C_1$ , then we get $X_1,X_2,\ldots,X_n \sim \text{Ber}(p)$ . $\overline{X_n}$ is an estimate for $p$ which is unbiased, and hence $$\text{MSE}_{\overline X_n}(p) = \text{var}(\overline X_n) = \frac{np(1-p)}{n^2} = \frac{p(1-p)}{n}$$ since $\text{MSE}_{\overline X_n}(p) = \text{var}(\overline X_n) + (\text{bias}(\overline X_n))^2$ . Similarly, if we choose all tosses from $C_2$ , we should get $$\text{MSE}_{\frac{\overline X_n}{2}}(p) = \text{var}\left(\frac{\overline X_n}{2}\right) = \frac{p(1-2p)}{2n}$$ right? The factor of two in the denominator is due to the fact that we're estimating $p$ , while drawing from $\text{Ber}(2p)$ . What happens if we choose some from $C_1$ and some from $C_2$ ? Answer: Choose all from $C_2$ . Here's another puzzle with a different answer: Puzzle $2$ : A factory produces light bulbs having an exponential distribution with mean $µ$ . Another factory produces light bulbs having an exponential distribution with a mean of $2µ$ . Your goal is to estimate $µ$ . You are allowed to choose a total of $50$ light bulbs (all from the first, all from the second, or some from each factory). What do you do? Answer: It does not matter what factory you choose from. In this case, I saw that the mean-squared error if we choose all bulbs from the first factory is $\mu^2/n$ , and it is the same if we choose all from the second factory as well. Due to the square of $\mu$ in the variance of this exponential distribution, it so happens that all choices make equivalently good estimators of $\mu$ . Follow-up question: Is there a way to probabilistically approach both these puzzles, i.e. without getting into actual MSE calculations to predict the MSE?","Puzzle : A coin has probability of turning up head, while a coin has probability of turning up head. All we know is that . Now, tosses are given. You can choose all tosses from , all tosses from , or some tosses from each (the total is ). If the objective is to estimate , what do you do? (Hint: Minimize mean-squared error of the estimator). If we choose all tosses from , then we get . is an estimate for which is unbiased, and hence since . Similarly, if we choose all tosses from , we should get right? The factor of two in the denominator is due to the fact that we're estimating , while drawing from . What happens if we choose some from and some from ? Answer: Choose all from . Here's another puzzle with a different answer: Puzzle : A factory produces light bulbs having an exponential distribution with mean . Another factory produces light bulbs having an exponential distribution with a mean of . Your goal is to estimate . You are allowed to choose a total of light bulbs (all from the first, all from the second, or some from each factory). What do you do? Answer: It does not matter what factory you choose from. In this case, I saw that the mean-squared error if we choose all bulbs from the first factory is , and it is the same if we choose all from the second factory as well. Due to the square of in the variance of this exponential distribution, it so happens that all choices make equivalently good estimators of . Follow-up question: Is there a way to probabilistically approach both these puzzles, i.e. without getting into actual MSE calculations to predict the MSE?","1 C_1 p C_2 2p 0 < p < \frac12 20 C_1 C_2 20 p n = 20 C_1 X_1,X_2,\ldots,X_n \sim \text{Ber}(p) \overline{X_n} p \text{MSE}_{\overline X_n}(p) = \text{var}(\overline X_n) = \frac{np(1-p)}{n^2} = \frac{p(1-p)}{n} \text{MSE}_{\overline X_n}(p) = \text{var}(\overline X_n) + (\text{bias}(\overline X_n))^2 C_2 \text{MSE}_{\frac{\overline X_n}{2}}(p) = \text{var}\left(\frac{\overline X_n}{2}\right) = \frac{p(1-2p)}{2n} p \text{Ber}(2p) C_1 C_2 C_2 2 µ 2µ µ 50 \mu^2/n \mu \mu","['statistics', 'probability-distributions', 'parameter-estimation']"
39,Transforming sum of random variables and conditional expectation,Transforming sum of random variables and conditional expectation,,"Homework warning I've been having a lot of trouble with questions where we transform a random variable with a sum of other random variables. I'll give an example and what I've tried to do (this is not an actual question from my homework but is tangentially related). Say $$X=\begin{cases}-1 , \text{with probability of } \frac{1}{2} \\  1 , \text{with probability of } \frac{1}{2}\end{cases}$$ and $$Y = X + Z, Z \sim Uniform[-1,1],\text{ and } X\perp \!\!\! \perp Z$$ If I'm given $$Y=y$$ I'm completely lost on how I would find $$p(X \mid y) \text{ and similarly } E(X|y)$$ The method I used in the past would be $$F_{X \mid Y}(x \mid y) = P(X \leq x \mid y) = P(y - z \leq x \mid y) = P(z \leq y - x \mid y)=F_{Z \mid Y}(y-x \mid y)  \\ \Rightarrow p_{X\mid Y}(x \mid y) = \frac{d}{dz}(F_{Z \mid Y}(y-x \mid y))$$ But at that point I get stuck as I don't know if it's correct to assume that $$F_{Z \mid Y}(y-x \mid y)) = F_Z(y-x)$$ or if there's another assumption I should make to transform this into the pdf I want or if that's even the correct method for finding the pdf I want. I know this question seems like it's all over the place and I apologize in advance for that as I'm very lost at the moment in what route I should be taking. If there's anyway I can clarify things please comment and I'll try my best",Homework warning I've been having a lot of trouble with questions where we transform a random variable with a sum of other random variables. I'll give an example and what I've tried to do (this is not an actual question from my homework but is tangentially related). Say and If I'm given I'm completely lost on how I would find The method I used in the past would be But at that point I get stuck as I don't know if it's correct to assume that or if there's another assumption I should make to transform this into the pdf I want or if that's even the correct method for finding the pdf I want. I know this question seems like it's all over the place and I apologize in advance for that as I'm very lost at the moment in what route I should be taking. If there's anyway I can clarify things please comment and I'll try my best,"X=\begin{cases}-1 , \text{with probability of } \frac{1}{2} \\  1 , \text{with probability of } \frac{1}{2}\end{cases} Y = X + Z, Z \sim Uniform[-1,1],\text{ and } X\perp \!\!\! \perp Z Y=y p(X \mid y) \text{ and similarly } E(X|y) F_{X \mid Y}(x \mid y) = P(X \leq x \mid y) = P(y - z \leq x \mid y) = P(z \leq y - x \mid y)=F_{Z \mid Y}(y-x \mid y) 
\\ \Rightarrow p_{X\mid Y}(x \mid y) = \frac{d}{dz}(F_{Z \mid Y}(y-x \mid y)) F_{Z \mid Y}(y-x \mid y)) = F_Z(y-x)",['statistics']
40,"Let $X_1, \dots, X_n$ i.i.d. from the $DE(\theta,b)$ Prove the MLE is asymptotically efficient",Let  i.i.d. from the  Prove the MLE is asymptotically efficient,"X_1, \dots, X_n DE(\theta,b)","Let $X_1, \dots, X_n$ be i.i.d. from the $DE(\theta,b)$ distribution (DE is the double exponential). (a) Show that the MLE $\theta$ is the sample median. (b) Prove that the MLE $\theta$ is asymptotically efficient. For part (a): $$L(\theta) = \frac{1}{(2b)^n}e^{-\sum_{i=1}^{n}\frac{|X_i-\theta|}{b}}$$ Let $g(\theta) = \sum_{i=1}{n}|X_i-\theta|$ and $L(\theta)$ is the largest when $g(\theta)$ is the smallest. Since $g(\theta)$ is continuous function of $\theta$ , so $$g'(\theta) = \sum_{i=1}^{n}[-1\times 1\{X_i>\theta\}+1\{X_i<\theta\}]= \begin{cases} \text{positive if } \theta > \operatorname{median}(X_i) \\\text{negtive if } \theta < \operatorname{median}(X_i) \end{cases}$$ So, MLE $\hat{\theta}$ is the sample median. For part (b): $lnf_\theta(x_1) = -ln2b-\frac{|x_1-\theta|}{b}$ $\frac{\partial}{\partial\theta}lnf_\theta(x_1)=\frac{1}{b}$ $I(\theta) = E[\frac{\partial}{\partial\theta}lnf_\theta(x_1)]^2=\int_{-\infty}^{\infty}\frac{1}{b^2}\frac{1}{2b}e^{-\frac{|x_1-\theta|}{b}}dx=\frac{1}{2b^3}\int_{0}^{\infty}be^{-y}dy=\frac{1}{2b^2}$ where $y=\frac{|x_1-\theta|}{b}$ , $\frac{dy}{dx}=\frac{1}{b}dx$ , $y \in(0,\infty)$ so, $\mathcal{v(\theta_0)}=\frac{1}{I(\theta_0)}=2b^2$ But, in class, the professor showed us that We have proved that for the sample median $M_n$ , $\sqrt{n}(M_n-\theta) \xrightarrow{L} N(0, \frac{1}{4(f_\theta(\theta))^2})$ and $f_\theta(\theta)) = \frac{1}{2b}e^{-\frac{|\theta-\theta|}{b}}=\frac{1}{2b}$ So, $\sqrt{n}(M_n-\theta) \xrightarrow{L} N(0, \frac{1}{4(f_\theta(\theta))^2})=N(0, b^2)$ I suppose to get $I(\theta)=\frac{1}{b^2}$ right? Could you help me out?","Let be i.i.d. from the distribution (DE is the double exponential). (a) Show that the MLE is the sample median. (b) Prove that the MLE is asymptotically efficient. For part (a): Let and is the largest when is the smallest. Since is continuous function of , so So, MLE is the sample median. For part (b): where , , so, But, in class, the professor showed us that We have proved that for the sample median , and So, I suppose to get right? Could you help me out?","X_1, \dots, X_n DE(\theta,b) \theta \theta L(\theta) = \frac{1}{(2b)^n}e^{-\sum_{i=1}^{n}\frac{|X_i-\theta|}{b}} g(\theta) = \sum_{i=1}{n}|X_i-\theta| L(\theta) g(\theta) g(\theta) \theta g'(\theta) = \sum_{i=1}^{n}[-1\times 1\{X_i>\theta\}+1\{X_i<\theta\}]= \begin{cases} \text{positive if } \theta > \operatorname{median}(X_i) \\\text{negtive if } \theta < \operatorname{median}(X_i) \end{cases} \hat{\theta} lnf_\theta(x_1) = -ln2b-\frac{|x_1-\theta|}{b} \frac{\partial}{\partial\theta}lnf_\theta(x_1)=\frac{1}{b} I(\theta) = E[\frac{\partial}{\partial\theta}lnf_\theta(x_1)]^2=\int_{-\infty}^{\infty}\frac{1}{b^2}\frac{1}{2b}e^{-\frac{|x_1-\theta|}{b}}dx=\frac{1}{2b^3}\int_{0}^{\infty}be^{-y}dy=\frac{1}{2b^2} y=\frac{|x_1-\theta|}{b} \frac{dy}{dx}=\frac{1}{b}dx y \in(0,\infty) \mathcal{v(\theta_0)}=\frac{1}{I(\theta_0)}=2b^2 M_n \sqrt{n}(M_n-\theta) \xrightarrow{L} N(0, \frac{1}{4(f_\theta(\theta))^2}) f_\theta(\theta)) = \frac{1}{2b}e^{-\frac{|\theta-\theta|}{b}}=\frac{1}{2b} \sqrt{n}(M_n-\theta) \xrightarrow{L} N(0, \frac{1}{4(f_\theta(\theta))^2})=N(0, b^2) I(\theta)=\frac{1}{b^2}",['statistics']
41,How-many-hedron is round enough?,How-many-hedron is round enough?,,"Let $X$ be a convex polytope that contains the unit sphere $\mathbb{S}^n$ (In $\mathbb{R}^{n+1}$ ). If $$\sup_{\textbf{a}\in \mathbb{S}^n}\sup_{\textbf{x}\in X}\textbf{a}.\textbf{x}\leq1+\varepsilon$$ then how many faces does $X$ AT LEAST have? The condition simply says the polytope is never more than $\varepsilon$ apart from the sphere and is therefore fairly smooth and round. An asymptotic answer suffices. PS: I came up with this problem out of curiosity, not sure if it has any applications.","Let be a convex polytope that contains the unit sphere (In ). If then how many faces does AT LEAST have? The condition simply says the polytope is never more than apart from the sphere and is therefore fairly smooth and round. An asymptotic answer suffices. PS: I came up with this problem out of curiosity, not sure if it has any applications.",X \mathbb{S}^n \mathbb{R}^{n+1} \sup_{\textbf{a}\in \mathbb{S}^n}\sup_{\textbf{x}\in X}\textbf{a}.\textbf{x}\leq1+\varepsilon X \varepsilon,"['geometry', 'statistics', 'euclidean-geometry']"
42,"When running a t-test with a Control and Experimental group, are both groups Samples? Or is the Control group a Population?","When running a t-test with a Control and Experimental group, are both groups Samples? Or is the Control group a Population?",,"When running a t-test with a Control and Experimental group, are both groups Samples? Or is the Control group a Population? I am pretty confused on this, since I've pretty much only ever worked with Samples, but have no idea when to switch to a population","When running a t-test with a Control and Experimental group, are both groups Samples? Or is the Control group a Population? I am pretty confused on this, since I've pretty much only ever worked with Samples, but have no idea when to switch to a population",,['statistics']
43,Mathematical significance of upside deviation bigger than downside deviation,Mathematical significance of upside deviation bigger than downside deviation,,"I have been searching around for an explanation, but could not find any. What is the mathematical significance of a sample set's upside deviation (standard deviation of values larger than the mean) being bigger than the downside deviation? Intuitively I feel that skewness might have to do with it, but I am not so sure.","I have been searching around for an explanation, but could not find any. What is the mathematical significance of a sample set's upside deviation (standard deviation of values larger than the mean) being bigger than the downside deviation? Intuitively I feel that skewness might have to do with it, but I am not so sure.",,"['probability', 'statistics', 'probability-distributions', 'statistical-inference']"
44,When is the standard deviation of the squares of a random number greater than the square of the standard deviation of those numbers?,When is the standard deviation of the squares of a random number greater than the square of the standard deviation of those numbers?,,"I was playing around with the uncertainty of the kinetic energy operator in quantum mechanics, and really desired to have the following inequality be true: $$\Delta K=\frac{1}{2m}\Delta (p^2)\geq\frac{1}{2m}(\Delta p)^2\geq\frac{\hbar^2}{8m(\Delta x)^2},$$ where $\Delta Q$ is the uncertainty (standard deviation) in variable $Q$ and the last step follows from the Heisenberg uncertainty principle. The problem is, $\Delta(Q^2)\geq(\Delta Q)^2$ turns out not to be true in general. I ran some sample distributions in Mathematica, and while this inequality is true most of the time, there are exceptions, though in those cases the difference is generally slight. Is there a condition we can apply (distribution or allowed range of $Q$ ) that will enforce this inequality? If not, ""how often"" does the inequality fail? Expanding in terms of expectation values (after squaring to get a variance on the left and not have square roots) doesn't seem to help: $$\langle Q^4\rangle\geq 2\langle Q^2\rangle(\langle Q^2\rangle-\langle Q\rangle^2)+\langle Q\rangle^4,$$ even though we know $\langle Q^4\rangle\geq \langle Q^2\rangle^2\geq \langle Q\rangle^4$ .","I was playing around with the uncertainty of the kinetic energy operator in quantum mechanics, and really desired to have the following inequality be true: where is the uncertainty (standard deviation) in variable and the last step follows from the Heisenberg uncertainty principle. The problem is, turns out not to be true in general. I ran some sample distributions in Mathematica, and while this inequality is true most of the time, there are exceptions, though in those cases the difference is generally slight. Is there a condition we can apply (distribution or allowed range of ) that will enforce this inequality? If not, ""how often"" does the inequality fail? Expanding in terms of expectation values (after squaring to get a variance on the left and not have square roots) doesn't seem to help: even though we know .","\Delta K=\frac{1}{2m}\Delta (p^2)\geq\frac{1}{2m}(\Delta p)^2\geq\frac{\hbar^2}{8m(\Delta x)^2}, \Delta Q Q \Delta(Q^2)\geq(\Delta Q)^2 Q \langle Q^4\rangle\geq 2\langle Q^2\rangle(\langle Q^2\rangle-\langle Q\rangle^2)+\langle Q\rangle^4, \langle Q^4\rangle\geq \langle Q^2\rangle^2\geq \langle Q\rangle^4","['statistics', 'probability-distributions', 'expected-value', 'variance', 'standard-deviation']"
45,Construct a (conditional) independent strong approximation,Construct a (conditional) independent strong approximation,,"It is well known that for the sum of independent r.v., the strong approximation can be achieved using Yurinskii's coupling: Theorem (Yurinskii, 1978). Let $\xi_{1},\ldots,\xi_{n}$ be independent random variables with $\mathbb{E}\left[\xi_{i}\right]=0$ for each $i$ and $\beta\equiv\sum_{i}^{n}\mathbb{E}\left[\left|\xi_{i}\right|^{3}\right]$ finite. Let $S=\xi_{1}+\ldots+\xi_{n}.$ For each $\delta>0$ there exists a random variable $T$ with a $\mathcal{N}\left(0,\mathrm{Var}\left(S\right)\right)$ distribution such that $$\mathbb{P}\left(\left|S-T\right|>3\delta\right)\leq C_{0}B\left(1+\left|\log\left(1/B\right)\right|\right)\quad \text{where }B=\beta\delta^{-3},$$ for some universal constant $C_{0}$ . In above theorem, $\xi_{i}$ are independent and are assumed in $\left(\Omega,\mathcal{F},\mathbb{P}\right)$ . I wonder if it still holds when the independent condition is replaced by conditional independence. That is, suppose now $\xi_{i}$ are defined in the probability space $\left(\Omega,\mathcal{F},\mathbb{P}\right)$ , where the probability space have the following structure: $\Omega=\Omega^{\left(0\right)}\times\Omega^{\left(1\right)}$ , $\mathcal{F}=\mathcal{F}^{\left(0\right)}\otimes\mathcal{F}^{\left(1\right)}$ . The only difference is that $\xi_{i}$ are not naturally independent, instead, we have $\xi_{i}$ are independent conditional on $\mathcal{F}^{\left(0\right)}$ . Does the above Gaussian coupling variable still exist? If so, will the variable be independent of $\mathcal{F}^{\left(0\right)}$ ? Any comments / counter examples / hints on proof are welcome. Thanks!","It is well known that for the sum of independent r.v., the strong approximation can be achieved using Yurinskii's coupling: Theorem (Yurinskii, 1978). Let be independent random variables with for each and finite. Let For each there exists a random variable with a distribution such that for some universal constant . In above theorem, are independent and are assumed in . I wonder if it still holds when the independent condition is replaced by conditional independence. That is, suppose now are defined in the probability space , where the probability space have the following structure: , . The only difference is that are not naturally independent, instead, we have are independent conditional on . Does the above Gaussian coupling variable still exist? If so, will the variable be independent of ? Any comments / counter examples / hints on proof are welcome. Thanks!","\xi_{1},\ldots,\xi_{n} \mathbb{E}\left[\xi_{i}\right]=0 i \beta\equiv\sum_{i}^{n}\mathbb{E}\left[\left|\xi_{i}\right|^{3}\right] S=\xi_{1}+\ldots+\xi_{n}. \delta>0 T \mathcal{N}\left(0,\mathrm{Var}\left(S\right)\right) \mathbb{P}\left(\left|S-T\right|>3\delta\right)\leq C_{0}B\left(1+\left|\log\left(1/B\right)\right|\right)\quad \text{where }B=\beta\delta^{-3}, C_{0} \xi_{i} \left(\Omega,\mathcal{F},\mathbb{P}\right) \xi_{i} \left(\Omega,\mathcal{F},\mathbb{P}\right) \Omega=\Omega^{\left(0\right)}\times\Omega^{\left(1\right)} \mathcal{F}=\mathcal{F}^{\left(0\right)}\otimes\mathcal{F}^{\left(1\right)} \xi_{i} \xi_{i} \mathcal{F}^{\left(0\right)} \mathcal{F}^{\left(0\right)}","['probability', 'statistics', 'weak-convergence', 'coupling']"
46,SARIMA: Fitted values of the time series [closed],SARIMA: Fitted values of the time series [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I'm trying to forecast future sales via SARIMA model. Since I have multiple time series, I have used a grid search technique to find the best parameters for the SARIMA model. But when looking at the fitted values the fit shows a huge dip at September. Details: Model fitted on $2$ years of weekly sales. SARIMA: $(0, 1, 0) \times (1, 1, 0, 52)$ AIC= $4.000$ Can someone explain why is the SARIMA model fit shows such dips. Ideally it show consider it as an outlier.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I'm trying to forecast future sales via SARIMA model. Since I have multiple time series, I have used a grid search technique to find the best parameters for the SARIMA model. But when looking at the fitted values the fit shows a huge dip at September. Details: Model fitted on years of weekly sales. SARIMA: AIC= Can someone explain why is the SARIMA model fit shows such dips. Ideally it show consider it as an outlier.","2 (0, 1, 0) \times (1, 1, 0, 52) 4.000","['statistics', 'time-series']"
47,Relationship between law of large numbers and regression to the mean,Relationship between law of large numbers and regression to the mean,,"I hear both of these terms often explained independently of one another but to me it seems that the are very related. I know the law of large numbers deals with the cumulative average of successive observations while regression to the mean is about the next observation. But my thinking is as follows: If the proportion of heads in a 100 coin flips is .75, regression to the mean says that it is more likely that the proportion of heads in the next 100 coin flips would be less extreme. Wether that is as extreme in the opposite direction(say .25) or less extreme in the same direction(say .6), both cases ultimately lower the total proportion of heads over the 200 flips. This, at least when repeated many times, leads the the result of the law of large numbers. Or more generally each successive group of an arbitrary amount of flips is more likely to be less extreme then the previous so the cumulative average  of all of the groups, as the number a groups approach’s infinity, approaches the theoretical average. Is this a correct understanding of these two concepts?","I hear both of these terms often explained independently of one another but to me it seems that the are very related. I know the law of large numbers deals with the cumulative average of successive observations while regression to the mean is about the next observation. But my thinking is as follows: If the proportion of heads in a 100 coin flips is .75, regression to the mean says that it is more likely that the proportion of heads in the next 100 coin flips would be less extreme. Wether that is as extreme in the opposite direction(say .25) or less extreme in the same direction(say .6), both cases ultimately lower the total proportion of heads over the 200 flips. This, at least when repeated many times, leads the the result of the law of large numbers. Or more generally each successive group of an arbitrary amount of flips is more likely to be less extreme then the previous so the cumulative average  of all of the groups, as the number a groups approach’s infinity, approaches the theoretical average. Is this a correct understanding of these two concepts?",,"['probability', 'statistics', 'law-of-large-numbers']"
48,Optimal Strategy for a betting game.,Optimal Strategy for a betting game.,,"I shall present my question and then add some of the working I have tried, unfortunately I haven't had much luck so far. The game The game is played between $3$ players. You and $2$ opponents who we shall call $I$ and $R$ (for intelligent and random) You each select an integer from $1-100$ (inclusive) and then money is exchanged as follows: Whoever selected the Largest number is the looser, they must pay the two winners what they each selected. Example: You pick $42$ , $I$ picks $12$ and $R$ picks $51$ then $R$ must pay you $\$42$ and he must pay $I$ $\$12$ In the event of a two or more players selecting the largest number then a looser is selected uniformly between them. $R$ plays uniformly randomly, that is he selects his pick uniformly between the first $100$ integers. $I$ Plays perfectly to maximise their own E.V (all standard game theory assumptions) What should your strategy be to maximise your expected value? My workings so far: For what it is worth in the 2 player scenario where it is just you v.s $R$ then you should pick $33$ as $\mathbb{E}[\delta_x ] = \frac{100-x}{100}(x) - \frac{x-1}{100}(-\frac{x}{2})$ (where $\delta_x$ is the strategy of picking $x$ ) This function is maximised at $33$ from simple calculus. No the answer is not $1$ as picking $2$ performs much better when you know someone is picking $1$ I asked my teacher and they said a mixed strategy but I am struggling with formulating this. My idea was to determine $f(x,y) = \mathbb{E}[\delta_x | \text{other player is picking y } ]$ and then find the maxima of this function but I am struggling. Any help would be great :)","I shall present my question and then add some of the working I have tried, unfortunately I haven't had much luck so far. The game The game is played between players. You and opponents who we shall call and (for intelligent and random) You each select an integer from (inclusive) and then money is exchanged as follows: Whoever selected the Largest number is the looser, they must pay the two winners what they each selected. Example: You pick , picks and picks then must pay you and he must pay In the event of a two or more players selecting the largest number then a looser is selected uniformly between them. plays uniformly randomly, that is he selects his pick uniformly between the first integers. Plays perfectly to maximise their own E.V (all standard game theory assumptions) What should your strategy be to maximise your expected value? My workings so far: For what it is worth in the 2 player scenario where it is just you v.s then you should pick as (where is the strategy of picking ) This function is maximised at from simple calculus. No the answer is not as picking performs much better when you know someone is picking I asked my teacher and they said a mixed strategy but I am struggling with formulating this. My idea was to determine and then find the maxima of this function but I am struggling. Any help would be great :)","3 2 I R 1-100 42 I 12 R 51 R \42 I \12 R 100 I R 33 \mathbb{E}[\delta_x ] = \frac{100-x}{100}(x) - \frac{x-1}{100}(-\frac{x}{2}) \delta_x x 33 1 2 1 f(x,y) = \mathbb{E}[\delta_x | \text{other player is picking y } ]","['probability', 'statistics', 'optimization', 'expected-value', 'game-theory']"
49,"How to summarize the rewards of a ""lottery"" sequence?","How to summarize the rewards of a ""lottery"" sequence?",,"I play a game where we can collect a certain type of resource, let's say it is money. With a certain amount of money we can buy a sort of lottery ticket. The ticket either gives back some money or gives other types of resources with certain chances. So what we have is something like: B[money]      balance  T[function]   ticket    p[money]   the price of the ticket    rm[money]  money reward    cm[%]      chance of winning the money reward    rx[x]      resource x reward    cx[%]      chance of winning the resource x reward    ry[y]      resource y reward    cy[%]      chance of winning the resource y reward    rz[z]      resource z reward    cz[%]      chance of winning the resource z reward    ... We can buy tickets until we run out of money and there are different types of tickets we can choose from. The goal is comparing tickets and choosing the right one for winning the most type ""x"" resources. It really depends on the players needs which resource is more important for them so for others ""y"" might be more useful. I want to compare tickets to be able to decide which one to choose for my needs. For a comparison I need to know how much rewards I can expect from them when I start with a certain amount of money. For now what I did is using a random number generator and buying tickets until I ran out of money. After that I start over, and I do this for example 10k times. I end up with a multi dimensional reward distribution: $$ T_p(B) = \{ 		a_1 x + b_1 y + c_1 z + ..., 		a_2 x + b_2 y + c_2 z + ..., 		... 		a_{10000} x + b_{10000} y + c_{10000} z + ... \} $$ After that I calculate average and standard deviation for each resources and end up with a statistics something like the following with for example 68% confidence: $$S(T_p(B)) = (avg(a_i) \pm stdev(a_i))x + (avg(b_i) \pm stdev(b_i))y + (avg(c_i) \pm stdev(c_i))z + ...$$ I am not sure if standard deviation is valid here. Is it? With the average I can compare my chests so for example if ""x"" is the most important resource, then a comparison is something simple like: $$S(T_p).avg.x > S(T_q).avg.x$$ Which means ticket ""p"" gives more resource ""x"" than ticket ""q"", so I have to choose ticket ""p"" over ticket ""q"". What I want to do is figuring out the formula for the upper results. For certain tickets it is trivial to count the average, because they don't give back any money $r_m = 0, x_m = 0$ . $$S(T(B)).{avg} = B/p \cdot [ (r_x \cdot c_x)x + (r_y \cdot c_y)y + (r_z \cdot c_z)z + ... ]$$ Here the number of drawing lots is $L = B/p$ . It is possible to count this number for a ticket that gives back money too. We have a series there: $$L(B) = B/p + L(r_m \cdot c_m \cdot B/p)$$ $$L(B) = B/p \cdot [1 + (r_m \cdot c_m) / p + ((r_m \cdot c_m)/p)^2 + ((r_m \cdot c_m)/p)^3 + ... ]$$ $$L(B) = B/p \cdot \sum_{i=0}^\infty{((r_m \cdot c_m)/p)^i}$$ For this kind of series we can use the following solution: $$L(B) \cdot [1 - ((r_m \cdot c_m)/p)] = B/p \cdot [1 - ((r_m \cdot c_m)/p)^\infty]$$ $$0 < (r_m \cdot c_m)/p < 1$$ $$L(B) = \frac{B/p}{1 - (r_m \cdot c_m)/p} = \frac{B}{p - r_m \cdot c_m}$$ This works pretty well for the average number of drawing lots, but I am not sure how do I get the average for the rewards. Should I just multiply this with the chances and rewards like I did in the trivial case? $$S(T(B)).{avg} = \frac{B}{p - r_m \cdot c_m} \cdot [ (r_x \cdot c_x)x + (r_y \cdot c_y)y + (r_z \cdot c_z)z + ... ]$$ I guess this is right, so this is the easy question. The hard question is how do I get information about the distribution? So what kind of distribution do we have here and how can I describe it? The $0 < (r_m \cdot c_m)/p < 1$ is always true, but $r_m > p$ is possible in the case of some tickets, so it is possible to win more money than we spend on the ticket. Which means it is possible to buy certain tickets infinite times $L_{max}(B) = \infty$ with a zero chance, while there are tickets which can we win only a finite times $L_{max}(B) \neq \infty$ . The $L_{min}(B) = B/p$ is trivial again. I think I can calculate the probability for these corner cases, but I have no idea about how to calculate the probability for the $L(B)$ values that are not average, min or max. I can get the same L value in many possible ways and for me it is a little confusing to work through all of these permutations. Even if I manage to somehow calculate the probabilities for different $L(B)$ values, I have no idea how I can calculate the chance and reward distributions even for these corner cases. For me knowing the $S(T(B)).{avg}$ is enough and probably the formula I wrote for it is ok, I just want to know if it is possible to say anything about the distributions without doing a simulation and how?","I play a game where we can collect a certain type of resource, let's say it is money. With a certain amount of money we can buy a sort of lottery ticket. The ticket either gives back some money or gives other types of resources with certain chances. So what we have is something like: B[money]      balance  T[function]   ticket    p[money]   the price of the ticket    rm[money]  money reward    cm[%]      chance of winning the money reward    rx[x]      resource x reward    cx[%]      chance of winning the resource x reward    ry[y]      resource y reward    cy[%]      chance of winning the resource y reward    rz[z]      resource z reward    cz[%]      chance of winning the resource z reward    ... We can buy tickets until we run out of money and there are different types of tickets we can choose from. The goal is comparing tickets and choosing the right one for winning the most type ""x"" resources. It really depends on the players needs which resource is more important for them so for others ""y"" might be more useful. I want to compare tickets to be able to decide which one to choose for my needs. For a comparison I need to know how much rewards I can expect from them when I start with a certain amount of money. For now what I did is using a random number generator and buying tickets until I ran out of money. After that I start over, and I do this for example 10k times. I end up with a multi dimensional reward distribution: After that I calculate average and standard deviation for each resources and end up with a statistics something like the following with for example 68% confidence: I am not sure if standard deviation is valid here. Is it? With the average I can compare my chests so for example if ""x"" is the most important resource, then a comparison is something simple like: Which means ticket ""p"" gives more resource ""x"" than ticket ""q"", so I have to choose ticket ""p"" over ticket ""q"". What I want to do is figuring out the formula for the upper results. For certain tickets it is trivial to count the average, because they don't give back any money . Here the number of drawing lots is . It is possible to count this number for a ticket that gives back money too. We have a series there: For this kind of series we can use the following solution: This works pretty well for the average number of drawing lots, but I am not sure how do I get the average for the rewards. Should I just multiply this with the chances and rewards like I did in the trivial case? I guess this is right, so this is the easy question. The hard question is how do I get information about the distribution? So what kind of distribution do we have here and how can I describe it? The is always true, but is possible in the case of some tickets, so it is possible to win more money than we spend on the ticket. Which means it is possible to buy certain tickets infinite times with a zero chance, while there are tickets which can we win only a finite times . The is trivial again. I think I can calculate the probability for these corner cases, but I have no idea about how to calculate the probability for the values that are not average, min or max. I can get the same L value in many possible ways and for me it is a little confusing to work through all of these permutations. Even if I manage to somehow calculate the probabilities for different values, I have no idea how I can calculate the chance and reward distributions even for these corner cases. For me knowing the is enough and probably the formula I wrote for it is ok, I just want to know if it is possible to say anything about the distributions without doing a simulation and how?","
T_p(B) = \{
		a_1 x + b_1 y + c_1 z + ...,
		a_2 x + b_2 y + c_2 z + ...,
		...
		a_{10000} x + b_{10000} y + c_{10000} z + ...
\}
 S(T_p(B)) = (avg(a_i) \pm stdev(a_i))x + (avg(b_i) \pm stdev(b_i))y + (avg(c_i) \pm stdev(c_i))z + ... S(T_p).avg.x > S(T_q).avg.x r_m = 0, x_m = 0 S(T(B)).{avg} = B/p \cdot [ (r_x \cdot c_x)x + (r_y \cdot c_y)y + (r_z \cdot c_z)z + ... ] L = B/p L(B) = B/p + L(r_m \cdot c_m \cdot B/p) L(B) = B/p \cdot [1 + (r_m \cdot c_m) / p + ((r_m \cdot c_m)/p)^2 + ((r_m \cdot c_m)/p)^3 + ... ] L(B) = B/p \cdot \sum_{i=0}^\infty{((r_m \cdot c_m)/p)^i} L(B) \cdot [1 - ((r_m \cdot c_m)/p)] = B/p \cdot [1 - ((r_m \cdot c_m)/p)^\infty] 0 < (r_m \cdot c_m)/p < 1 L(B) = \frac{B/p}{1 - (r_m \cdot c_m)/p} = \frac{B}{p - r_m \cdot c_m} S(T(B)).{avg} = \frac{B}{p - r_m \cdot c_m} \cdot [ (r_x \cdot c_x)x + (r_y \cdot c_y)y + (r_z \cdot c_z)z + ... ] 0 < (r_m \cdot c_m)/p < 1 r_m > p L_{max}(B) = \infty L_{max}(B) \neq \infty L_{min}(B) = B/p L(B) L(B) S(T(B)).{avg}","['sequences-and-series', 'statistics']"
50,"MLE of $(\theta_1,\theta_2)$ in a piecewise PDF",MLE of  in a piecewise PDF,"(\theta_1,\theta_2)","I am trying to find the MLE of $\theta=(\theta_1,\theta_2)$ in a random sample $\{X\}_{i=1}^n$ with the following pdf $$f(x\mid\theta)= \begin{cases} (\theta_1+\theta_2)^{-1}\exp\left(\frac{-x}{\theta_1}\right) &,  x>0\\  (\theta_1+\theta_2)^{-1}\exp\left(\frac{x}{\theta_2}\right) &,  x\le0\\ \end{cases} $$ If I let $\bar{X}_1$ be the average of the $n_1$ values where $X_1>0$ and $\bar{X}_2$ the average of $n_2$ values where $X_i\le 0$ and $n_1+n_2=n$ Then the likelihood function is: $$L(\theta\mid  X)=\left(\frac 1 {\theta_1+\theta_2}\right)^n\exp\left(\frac{-n_1\bar{X}_1}{\theta_1}+\frac{n_2\bar{X}_2}{\theta_2}\right)$$ but I am having trouble maximizing this function.",I am trying to find the MLE of in a random sample with the following pdf If I let be the average of the values where and the average of values where and Then the likelihood function is: but I am having trouble maximizing this function.,"\theta=(\theta_1,\theta_2) \{X\}_{i=1}^n f(x\mid\theta)= \begin{cases}
(\theta_1+\theta_2)^{-1}\exp\left(\frac{-x}{\theta_1}\right) &,  x>0\\ 
(\theta_1+\theta_2)^{-1}\exp\left(\frac{x}{\theta_2}\right) &,  x\le0\\
\end{cases}
 \bar{X}_1 n_1 X_1>0 \bar{X}_2 n_2 X_i\le 0 n_1+n_2=n L(\theta\mid  X)=\left(\frac 1 {\theta_1+\theta_2}\right)^n\exp\left(\frac{-n_1\bar{X}_1}{\theta_1}+\frac{n_2\bar{X}_2}{\theta_2}\right)","['statistics', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
51,Covariance Matrix in PCA,Covariance Matrix in PCA,,"Principal component analysis (PCA) uses one scatter matrix (an estimation of covariance matrix) to find eigen vectors of the original matrix. Suppose we have two scatter $V_1, V_2$ matrices for our observations (assume classical scatter and fourth momentum scatter matrices), some metrics for outliers like Invariant Coordinate selection (ICS) uses two scatter matrices to compute the invariant coordinates or components. The goal of ICS is to find $p\times p$ matrix $B(X_n)$ and diagonal matrix $D(X_n)$ , s.t. $$B(X_n)V_1(X_n)B'\left(X_n \right) = I_p ~~~~~\& B(X_n)V_2(X_n)B'\left(X_n \right)=D(X_n)$$ $D(X_n)$ contains the eigenvalues of $V_1(X_n)^{-1}V_2(X_n)$ in decreasing order while $B(X_n)$ contains the corresponding eigen vectors. Then using any affine equivariant location estimator $m(X_n)$ , the corresponding scores are the invariant coordinates or components: $$Z_n = (z_1,...,z_n)' = (X_n - 1_nm(X_n)')B(X_n)'$$ Question : In PCA, we use one covariance matrix (scatter matrix), what is the purpose of using two scatter matrices in ICS? How does using two scatter matrices affect getting the invariant coordinates? Is it because of the eigenvectors we get as a result of that, which are stored in $B(X_n)$ ?","Principal component analysis (PCA) uses one scatter matrix (an estimation of covariance matrix) to find eigen vectors of the original matrix. Suppose we have two scatter matrices for our observations (assume classical scatter and fourth momentum scatter matrices), some metrics for outliers like Invariant Coordinate selection (ICS) uses two scatter matrices to compute the invariant coordinates or components. The goal of ICS is to find matrix and diagonal matrix , s.t. contains the eigenvalues of in decreasing order while contains the corresponding eigen vectors. Then using any affine equivariant location estimator , the corresponding scores are the invariant coordinates or components: Question : In PCA, we use one covariance matrix (scatter matrix), what is the purpose of using two scatter matrices in ICS? How does using two scatter matrices affect getting the invariant coordinates? Is it because of the eigenvectors we get as a result of that, which are stored in ?","V_1, V_2 p\times p B(X_n) D(X_n) B(X_n)V_1(X_n)B'\left(X_n \right) = I_p ~~~~~\& B(X_n)V_2(X_n)B'\left(X_n \right)=D(X_n) D(X_n) V_1(X_n)^{-1}V_2(X_n) B(X_n) m(X_n) Z_n = (z_1,...,z_n)' = (X_n - 1_nm(X_n)')B(X_n)' B(X_n)",['statistics']
52,Reading through a paper on got confused on this part,Reading through a paper on got confused on this part,,"Look at the following  please :) Assuming an uninformative prior for $N$ then $$ \begin{aligned} \log p(N \mid y) &=\text { const }+\log \mathrm{p}(\mathbf{y} \mid N) \\ & \approx \text { const }-\frac{1}{2} \mathrm{BIC},\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (17) \end{aligned} $$ using (16) and substituting for the BIC. Generally, the BIC would give an overprecise approximation to the posterior $p(N \mid y)$ . I'm kind of lost on how they got to (17) and the Bayesian information criterion. (16) if it may be related $$ \log \mathrm{p}(\mathbf{y} \mid N) \approx \log p(\mathbf{y} \mid \hat{\Theta}, N)-\frac{p_{N}}{2} \log (T),\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ (16) $$","Look at the following  please :) Assuming an uninformative prior for then using (16) and substituting for the BIC. Generally, the BIC would give an overprecise approximation to the posterior . I'm kind of lost on how they got to (17) and the Bayesian information criterion. (16) if it may be related","N 
\begin{aligned}
\log p(N \mid y) &=\text { const }+\log \mathrm{p}(\mathbf{y} \mid N) \\
& \approx \text { const }-\frac{1}{2} \mathrm{BIC},\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (17)
\end{aligned}
 p(N \mid y) 
\log \mathrm{p}(\mathbf{y} \mid N) \approx \log p(\mathbf{y} \mid \hat{\Theta}, N)-\frac{p_{N}}{2} \log (T),\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ (16)
",['statistics']
53,$\mathbb{Q}$ is in the Borel $\sigma$-algebra [duplicate],is in the Borel -algebra [duplicate],\mathbb{Q} \sigma,"This question already has an answer here : Prove that $\mathbb{Q}$ belongs to the Borel $\sigma$-algebra of $\mathbb{R}$ [closed] (1 answer) Closed 3 years ago . Prove that the set of rational numbers $\mathbb{Q}$ belongs to the Borel σ-algebra of $\mathbb{R}$ I was just thinking about the method I would go about proving this I was thinking if we can write $$\mathbb{Q} = \bigcup_{x \in \mathbb{Q}} \left \{ x \right \}$$ And then if we prove the {x} is in the Borel σ-algebra then the union is too? If there's a method to prove this, It would really help thank you","This question already has an answer here : Prove that $\mathbb{Q}$ belongs to the Borel $\sigma$-algebra of $\mathbb{R}$ [closed] (1 answer) Closed 3 years ago . Prove that the set of rational numbers belongs to the Borel σ-algebra of I was just thinking about the method I would go about proving this I was thinking if we can write And then if we prove the {x} is in the Borel σ-algebra then the union is too? If there's a method to prove this, It would really help thank you",\mathbb{Q} \mathbb{R} \mathbb{Q} = \bigcup_{x \in \mathbb{Q}} \left \{ x \right \},['real-analysis']
54,Tree Diagram: What is the probability that a customer purchases the basic or classic car wash if they vacuum their car?,Tree Diagram: What is the probability that a customer purchases the basic or classic car wash if they vacuum their car?,,"This equation comes from Edgenuity's course of Statistics, and I am taking the course as a high school senior. I understand that to find $P(A$ | $B)$ , one divides $P(A$ and $B)$ by $P(B)$ . Also, to find $P(A$ or $B)$ , one adds $P(A)$ and $P(B)$ . I attempt to do this in my solving of the problem below: A car wash has three different types of washes: basic, classic, and ultimate. Based on records, 45% of customers get the basic wash, 35% get the classic wash, and 20% get the ultimate wash. Some customers also vacuum out their cars after the wash. The car wash records show that 10% of customers who get the basic wash, 25% of customers who get the classic wash, and 60% of customers who get the ultimate wash also vacuum their cars. The probabilities are displayed in the tree diagram. What is the probability that a randomly selected customer purchases the basic or classic car wash if they vacuum their car? First , I found the total probability of having any type of wash and having the car vacuumed $(0.2525)$ by adding each probability of having whatever type of wash and the probability of vacuuming after using that type of wash (i.e., $(0.45 * 0.10) + (0.35 * 0.25) + (0.2 * 0.6) = 0.2525$ ). Second , I added $P($ Basic and Vacuum $)$ and $P($ Classic and Vacuum $)$ , which is $0.1325$ , because $P(A$ or $B)$ is $P(A)$ $+$ $P(B)$ . Lastly , I divided $0.1325$ by $0.2525$ to find $P($ Basic or Classic | Vacuum $)$ because $P(A$ | $B)$ $=$ $P(A$ and $B)$ $÷$ $P(B)$ . The answer is approximately $0.52$ . Are my methods and answer correct? If not, please explain why.","This equation comes from Edgenuity's course of Statistics, and I am taking the course as a high school senior. I understand that to find | , one divides and by . Also, to find or , one adds and . I attempt to do this in my solving of the problem below: A car wash has three different types of washes: basic, classic, and ultimate. Based on records, 45% of customers get the basic wash, 35% get the classic wash, and 20% get the ultimate wash. Some customers also vacuum out their cars after the wash. The car wash records show that 10% of customers who get the basic wash, 25% of customers who get the classic wash, and 60% of customers who get the ultimate wash also vacuum their cars. The probabilities are displayed in the tree diagram. What is the probability that a randomly selected customer purchases the basic or classic car wash if they vacuum their car? First , I found the total probability of having any type of wash and having the car vacuumed by adding each probability of having whatever type of wash and the probability of vacuuming after using that type of wash (i.e., ). Second , I added Basic and Vacuum and Classic and Vacuum , which is , because or is . Lastly , I divided by to find Basic or Classic | Vacuum because | and . The answer is approximately . Are my methods and answer correct? If not, please explain why.",P(A B) P(A B) P(B) P(A B) P(A) P(B) (0.2525) (0.45 * 0.10) + (0.35 * 0.25) + (0.2 * 0.6) = 0.2525 P( ) P( ) 0.1325 P(A B) P(A) + P(B) 0.1325 0.2525 P( ) P(A B) = P(A B) ÷ P(B) 0.52,"['probability', 'statistics']"
55,Martingale in Cox Model,Martingale in Cox Model,,"Can someone help me to show that $$ \hat{A}(t, \beta_0) = \sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} dN_i(s) $$ is a martingale. The setup is the Cox proportional hazard model in a semiparametric manner. This is what I got so far: $$ \sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} dN_i(s) = \sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} (dM_i(s|X) + d\Lambda_i(s|X) ) $$ $$ = \sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} (dM_i(s|X) + d \int_0^s Y_i(u) \alpha_0 e^{X_i^T \beta_0} du   ) $$ $$ =\sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} dM_i(s|X) + \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} \sum_{i=1}^{n} Y_i(s) e^{X_i^T \beta} \alpha_0(s)ds $$ $$ =\sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} dM_i(s|X) + \int_0^t \alpha_0(s)ds $$",Can someone help me to show that is a martingale. The setup is the Cox proportional hazard model in a semiparametric manner. This is what I got so far:,"
\hat{A}(t, \beta_0) = \sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} dN_i(s)
 
\sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} dN_i(s) = \sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} (dM_i(s|X) + d\Lambda_i(s|X) )
 
= \sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} (dM_i(s|X) + d \int_0^s Y_i(u) \alpha_0 e^{X_i^T \beta_0} du   )
 
=\sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} dM_i(s|X) + \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} \sum_{i=1}^{n} Y_i(s) e^{X_i^T \beta} \alpha_0(s)ds
 
=\sum_{i=1}^{n} \int_0^t\frac{1}{\sum_{j}^n Y_j(s) e^{X_i^T \beta_0}} dM_i(s|X) + \int_0^t \alpha_0(s)ds
","['probability-theory', 'statistics', 'stochastic-processes', 'martingales']"
56,"Real-world example where two positively correlated pairs (X, Y ) and (Y, Z) of random variables give a negative correlation for (X,Z).","Real-world example where two positively correlated pairs (X, Y ) and (Y, Z) of random variables give a negative correlation for (X,Z).",,"Let X, Y and Z be random variables with finite means and finite variances. Suppose that the correlations ρ(X, Y ) and ρ(Y, Z) have unspecified positive values. Then without additional information on the distributions of X, Y and Z, the correlation ρ(X,Z) could possibly be positive, negative, or zero. In particular, we can find specific random variables X,Y,Z such that ρ(X, Z) is negative. The question is to come up with a creative real-world example where two positively correlated pairs (X, Y ) and (Y, Z) of random variables could still give a negative correlation for (X,Z). For example: In basketball, NBA basketball players could play as many as > 80 games per year, and their points scored over the year are recorded. For a randomly selected NBA basketball player, define the following random variables: • Let X be the number of slam dunks made in a year. • Let Y be the total number of points scored in a year. • Let Z be the number of 3-pointers made in a year. In this case, X,Y and Y,Z are both positively correlated, however X,Z is negatively correlated. I want to think of other creative examples like this (not related to other sports, please)","Let X, Y and Z be random variables with finite means and finite variances. Suppose that the correlations ρ(X, Y ) and ρ(Y, Z) have unspecified positive values. Then without additional information on the distributions of X, Y and Z, the correlation ρ(X,Z) could possibly be positive, negative, or zero. In particular, we can find specific random variables X,Y,Z such that ρ(X, Z) is negative. The question is to come up with a creative real-world example where two positively correlated pairs (X, Y ) and (Y, Z) of random variables could still give a negative correlation for (X,Z). For example: In basketball, NBA basketball players could play as many as > 80 games per year, and their points scored over the year are recorded. For a randomly selected NBA basketball player, define the following random variables: • Let X be the number of slam dunks made in a year. • Let Y be the total number of points scored in a year. • Let Z be the number of 3-pointers made in a year. In this case, X,Y and Y,Z are both positively correlated, however X,Z is negatively correlated. I want to think of other creative examples like this (not related to other sports, please)",,"['probability', 'statistics', 'random-variables', 'correlation']"
57,What sigma algebra are we using when considering the sum of two random variables?,What sigma algebra are we using when considering the sum of two random variables?,,"If we consider random variables $X,Y$ both with the sigma algebra $F$ , is $Z=X+Y$ defined on the sigma algebra $Z \times Z$ or on $Z$ ?  In other words, if $X(x)$ is a function of x and $Y(y)$ is a function of y, is $Z$ a function of x and y or just one variable?  Thanks.","If we consider random variables both with the sigma algebra , is defined on the sigma algebra or on ?  In other words, if is a function of x and is a function of y, is a function of x and y or just one variable?  Thanks.","X,Y F Z=X+Y Z \times Z Z X(x) Y(y) Z","['probability', 'probability-theory', 'statistics', 'probability-distributions']"
58,Measure theory book suggestion for a foundation to Probability theory,Measure theory book suggestion for a foundation to Probability theory,,"I would like your comment (or another suggestions) on the 2 measure theory books for self-study: Introduction to measure theory by Terence Tao Measures, Integrals and Martingales by René L. Schilling My goal is to have only necessary foundations (because I don't want to go too deep in the measure theory) to understand the rigorous foundation of probability (to the point of Brownian motion and Ito's calculus) and furthermore, the theoretical foundation of statistics ( $i.e$ to understand the rigorous logic behind Maximum Likelihood Estimator or hypothesis testing, monte-carlo simulation). Which measure theory book of these 2 books above will serve better for my purpose ? Could you suggest me a book (good for self-study) on Probability/statistics which serve my purposes above ? Thank you very much!","I would like your comment (or another suggestions) on the 2 measure theory books for self-study: Introduction to measure theory by Terence Tao Measures, Integrals and Martingales by René L. Schilling My goal is to have only necessary foundations (because I don't want to go too deep in the measure theory) to understand the rigorous foundation of probability (to the point of Brownian motion and Ito's calculus) and furthermore, the theoretical foundation of statistics ( to understand the rigorous logic behind Maximum Likelihood Estimator or hypothesis testing, monte-carlo simulation). Which measure theory book of these 2 books above will serve better for my purpose ? Could you suggest me a book (good for self-study) on Probability/statistics which serve my purposes above ? Thank you very much!",i.e,"['probability-theory', 'measure-theory', 'statistics', 'stochastic-calculus']"
59,support vs state space,support vs state space,,"In one of my examples, I saw that the state space for a binomial distribution is $({0,1,...,n})$ . I then thought that for $X_1, X_2,...,X_n$ iid RVs, where $f_\theta(x_i)=\theta x_i^{\theta -1}$ for $x \in (0,1)$ , and $0$ otherwise, the state space is $(0,1)^n$ . However, it was said to be $\Bbb R^n$ , meaning other values outside $(0,1)$ are also counted in the state space (which would make sense, since the state space is the set of values assumed by our RV), and that $(0,1)^n$ is rather the support. But, following this logic, how comes the state space for the binomial distribution isn't $\Bbb N$ , where for all $N>n,$ the probability that the RV is equal to that value is zero? Or ss it because the binomial distribution isn't defined for $n+1$ onwards?","In one of my examples, I saw that the state space for a binomial distribution is . I then thought that for iid RVs, where for , and otherwise, the state space is . However, it was said to be , meaning other values outside are also counted in the state space (which would make sense, since the state space is the set of values assumed by our RV), and that is rather the support. But, following this logic, how comes the state space for the binomial distribution isn't , where for all the probability that the RV is equal to that value is zero? Or ss it because the binomial distribution isn't defined for onwards?","({0,1,...,n}) X_1, X_2,...,X_n f_\theta(x_i)=\theta x_i^{\theta -1} x \in (0,1) 0 (0,1)^n \Bbb R^n (0,1) (0,1)^n \Bbb N N>n, n+1","['statistics', 'probability-distributions']"
60,How did Gauss come up with this function?,How did Gauss come up with this function?,,"I have been studying statistics lately, and after having my first brush with normal distributions, I was curious to know more about it. I have researched about the history of normal distributions and found the following information: When gauss published his monograph, Theoria motus corporum coelestium in sectionibus conicis solem ambientium , he discussed among other things the normal distribution (interestingly, the book was on how celestial objects have orbits that are conic sections). According to Wikipedia , he did the following to arrive at the distribution: There is an unknown quantity $V$ . $M, M',M"",\dots$ are the measurements of $V$ . $\varphi$ is the function that governs the law of probability. The aim was to  find $\varphi$ . It says on Wikipedia that the most likely estimator of $V$ would be the one that maximizes: $$\varphi(M-V)\varphi(M'-V)\varphi(M""-V)\dots\;\;\;\;\;\;\;\;\;\;-(1)$$ Gauss guessed that the answer to the $V$ maximizing the above equation must be the mean of the measured values. Then gauss demonstrated that the only function $\varphi$ that gives the mean as the answer was: $$\varphi\Delta=\frac{h}{\sqrt{\pi}}e^{-h^2{\Delta}^2}$$ where, $\Delta$ is the measurement of errors. $h$ is the measure of precision of the observation A number of questions pop up in my head. Why should the most likely estimate be the one that maximizes equation 1? How did Gauss' derive $\varphi$ using the assumed solution to be the mean? What does $\Delta$ actually mean? MY ANSWERS/ATTEMPTS I actually feel that it should not be the maximum. It should rather be the minimum as when the expression is minimized it means that $V$ is closer to each of the values $M,M',M""\dots$ which are the actual measurements. Therefore such a value should be the most likely estimate. From my knowledge of statistical inference, in the procedure of maximum likelihood estimator, we maximize the probability density function of the distribution. The value of random variable at the maxima is the estimate. I do not know the answer to this one. I tried looking up Gauss' original work but it would take me too off topic as Gauss' has treated these ideas with respect to orbits of celestial objects being conic sections. I do not wish to look into the applications right now (But I do understand that reading the book right from the start will be wonderful learning experience). I once again do not fully understand what $\Delta$ means here. According to me, $\varphi$ should have been a function of $V$ and not $\Delta$ . It would be really helpful if someone could answer my questions and also provide an explanation of what actually is being done here as I am getting confused with so many ideas in my head!!","I have been studying statistics lately, and after having my first brush with normal distributions, I was curious to know more about it. I have researched about the history of normal distributions and found the following information: When gauss published his monograph, Theoria motus corporum coelestium in sectionibus conicis solem ambientium , he discussed among other things the normal distribution (interestingly, the book was on how celestial objects have orbits that are conic sections). According to Wikipedia , he did the following to arrive at the distribution: There is an unknown quantity . are the measurements of . is the function that governs the law of probability. The aim was to  find . It says on Wikipedia that the most likely estimator of would be the one that maximizes: Gauss guessed that the answer to the maximizing the above equation must be the mean of the measured values. Then gauss demonstrated that the only function that gives the mean as the answer was: where, is the measurement of errors. is the measure of precision of the observation A number of questions pop up in my head. Why should the most likely estimate be the one that maximizes equation 1? How did Gauss' derive using the assumed solution to be the mean? What does actually mean? MY ANSWERS/ATTEMPTS I actually feel that it should not be the maximum. It should rather be the minimum as when the expression is minimized it means that is closer to each of the values which are the actual measurements. Therefore such a value should be the most likely estimate. From my knowledge of statistical inference, in the procedure of maximum likelihood estimator, we maximize the probability density function of the distribution. The value of random variable at the maxima is the estimate. I do not know the answer to this one. I tried looking up Gauss' original work but it would take me too off topic as Gauss' has treated these ideas with respect to orbits of celestial objects being conic sections. I do not wish to look into the applications right now (But I do understand that reading the book right from the start will be wonderful learning experience). I once again do not fully understand what means here. According to me, should have been a function of and not . It would be really helpful if someone could answer my questions and also provide an explanation of what actually is being done here as I am getting confused with so many ideas in my head!!","V M, M',M"",\dots V \varphi \varphi V \varphi(M-V)\varphi(M'-V)\varphi(M""-V)\dots\;\;\;\;\;\;\;\;\;\;-(1) V \varphi \varphi\Delta=\frac{h}{\sqrt{\pi}}e^{-h^2{\Delta}^2} \Delta h \varphi \Delta V M,M',M""\dots \Delta \varphi V \Delta","['statistics', 'probability-distributions', 'normal-distribution', 'statistical-inference', 'gaussian']"
61,Hypothesis Testing Biased vs Unbiased Coin with One Flip [closed],Hypothesis Testing Biased vs Unbiased Coin with One Flip [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question You're given a coin to flip and you only can flip it once. The null hypothesis is that it is unbiased. The alternate hypothesis is that it is twice as likely to land heads than it is tails. Create a powerful test with 90% confidence and calculate the power. If you can help me with that question, it would be much appreciated. Thank you!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question You're given a coin to flip and you only can flip it once. The null hypothesis is that it is unbiased. The alternate hypothesis is that it is twice as likely to land heads than it is tails. Create a powerful test with 90% confidence and calculate the power. If you can help me with that question, it would be much appreciated. Thank you!",,"['statistics', 'normal-distribution', 'binomial-distribution', 'hypothesis-testing']"
62,Calculation for Standard Deviation Given a Gaussian Distribution,Calculation for Standard Deviation Given a Gaussian Distribution,,"Quick background: I work as a Software Engineer in the Embedded Systems space, and my primary job function has me working with signal processing algorithms. I feel pretty good when it comes to statistics, but this one comment left in a codebase by a former employee has me stumped. The calculation is supposed to be for the standard deviation of a signal to noise difference, and is described as such: sigma = pow(1-normdist(n), m). I haven't been able to place the equation to anything standard deviation related, at least from my knowledge. The 1-normdist(n) part remind me of calculating tail probabilities, but even then, taking a power of a probability doesn't make much sense to me. Any help and advice is welcomed!","Quick background: I work as a Software Engineer in the Embedded Systems space, and my primary job function has me working with signal processing algorithms. I feel pretty good when it comes to statistics, but this one comment left in a codebase by a former employee has me stumped. The calculation is supposed to be for the standard deviation of a signal to noise difference, and is described as such: sigma = pow(1-normdist(n), m). I haven't been able to place the equation to anything standard deviation related, at least from my knowledge. The 1-normdist(n) part remind me of calculating tail probabilities, but even then, taking a power of a probability doesn't make much sense to me. Any help and advice is welcomed!",,"['statistics', 'signal-processing', 'standard-deviation']"
63,"Brussel Sprout Random Variable, Mean, SD, and Probability","Brussel Sprout Random Variable, Mean, SD, and Probability",,"A local farmer's market sells 1 pound bags of Brussels Sprouts. The farmer believes that the mean number of Brussels Sprouts in each bag is 18 with a standard deviation of 3.4. You buy 2 pounds of Brussels Sprouts. a. You want to explore the range of sizes between brussels sprouts. You find the difference between the number of sprouts in your two bags. Define your random variable for this situation, and find its mean and standard deviation. I'm a bit confused with this question. Would the random variable be the # (number) of brussels sprouts in each bag? Wouldn't the mean be 18 and the standard deviation be 3.4? b. IF the number of Brussels Sprouts per bag is normally distributed, what is the probability that the difference in number of Brussels Sprouts between two bags is more than 4? I have no idea how to answer b. Would we make a normal probability distribution and use normalcdf?","A local farmer's market sells 1 pound bags of Brussels Sprouts. The farmer believes that the mean number of Brussels Sprouts in each bag is 18 with a standard deviation of 3.4. You buy 2 pounds of Brussels Sprouts. a. You want to explore the range of sizes between brussels sprouts. You find the difference between the number of sprouts in your two bags. Define your random variable for this situation, and find its mean and standard deviation. I'm a bit confused with this question. Would the random variable be the # (number) of brussels sprouts in each bag? Wouldn't the mean be 18 and the standard deviation be 3.4? b. IF the number of Brussels Sprouts per bag is normally distributed, what is the probability that the difference in number of Brussels Sprouts between two bags is more than 4? I have no idea how to answer b. Would we make a normal probability distribution and use normalcdf?",,"['probability', 'statistics']"
64,Finding minimal sufficient statistics for this family coming from the given probability mass function,Finding minimal sufficient statistics for this family coming from the given probability mass function,,"Let $X_i\big|_{i = 1...n}$ be random sample from the PMF: $P(X_i = 0) = \frac{1-\theta}2;\;P(X_i = 1) = \frac12 ; P(X_i = 2) = \frac\theta2$ where $\theta\in(0,1)$ . Find the minimal sufficient statistics. Let $X = (X_1, \ldots, X_n) $ and $x = (x_1, \ldots, x_n)$ , then $P(X = x, \theta ) \\= \prod_{i=1}^n \big(\frac{1-\theta}2\big)^{I(x_i = 0)} \big(\frac{1}2\big)^{I(x_i = 1)} \big(\frac{\theta}2\big)^{I(x_i = 2)}\\ =  \prod_{i=1}^n \big(\frac{1}2\big)^{I(x_i = 0) +I(x_i = 1) + I(x_i = 2)} (1-\theta)^{I(x_i = 0)} \;\theta^{I(x_i = 2)} \\ = \prod_{i=1}^n \frac12 (1-\theta)^{I(x_i = 0)} \;\theta^{I(x_i = 2)} \text{ because  } I(x_i = 0) +I(x_i = 1) + I(x_i = 2) = 1\\  = \frac{1}{2^n}(1-\theta)^{\sum_{i = 1}^nI(x_i = 0)} \theta^{\sum_{i=1}^nI(x_i = 2)} $ Now, $$ \frac{P(X= x, \theta )}{P(X = y, \theta)} = (1-\theta)^{\sum_{i = 1}^nI(x_i = 0)- \sum_{i = 1}^nI(y_i = 0)} \;\theta^{\sum_{i=1}^nI(x_i = 2) - \sum_{i = 1}^nI(y_i = 2)}$$ is independent of $\theta$ iff $$\sum_{i = 1}^nI(x_i = 0)= \sum_{i = 1}^nI(y_i = 0)$$ and $$\sum_{i=1}^nI(x_i = 2) = \sum_{i = 1}^nI(y_i = 2)$$ Hence, $T(X) = (\sum_{i=1}^nI(X_i = 0), \sum_{i=1}^nI(X_i = 2))$ is minimal sufficient. Is my attempt correct?","Let be random sample from the PMF: where . Find the minimal sufficient statistics. Let and , then Now, is independent of iff and Hence, is minimal sufficient. Is my attempt correct?","X_i\big|_{i = 1...n} P(X_i = 0) = \frac{1-\theta}2;\;P(X_i = 1) = \frac12 ; P(X_i = 2) = \frac\theta2 \theta\in(0,1) X = (X_1, \ldots, X_n)  x = (x_1, \ldots, x_n) P(X = x, \theta ) \\= \prod_{i=1}^n \big(\frac{1-\theta}2\big)^{I(x_i = 0)} \big(\frac{1}2\big)^{I(x_i = 1)} \big(\frac{\theta}2\big)^{I(x_i = 2)}\\ =  \prod_{i=1}^n \big(\frac{1}2\big)^{I(x_i = 0) +I(x_i = 1) + I(x_i = 2)} (1-\theta)^{I(x_i = 0)} \;\theta^{I(x_i = 2)} \\ = \prod_{i=1}^n \frac12 (1-\theta)^{I(x_i = 0)} \;\theta^{I(x_i = 2)} \text{ because  } I(x_i = 0) +I(x_i = 1) + I(x_i = 2) = 1\\ 
= \frac{1}{2^n}(1-\theta)^{\sum_{i = 1}^nI(x_i = 0)} \theta^{\sum_{i=1}^nI(x_i = 2)}   \frac{P(X= x, \theta )}{P(X = y, \theta)} = (1-\theta)^{\sum_{i = 1}^nI(x_i = 0)- \sum_{i = 1}^nI(y_i = 0)} \;\theta^{\sum_{i=1}^nI(x_i = 2) - \sum_{i = 1}^nI(y_i = 2)} \theta \sum_{i = 1}^nI(x_i = 0)= \sum_{i = 1}^nI(y_i = 0) \sum_{i=1}^nI(x_i = 2) = \sum_{i = 1}^nI(y_i = 2) T(X) = (\sum_{i=1}^nI(X_i = 0), \sum_{i=1}^nI(X_i = 2))","['statistics', 'statistical-inference', 'sufficient-statistics']"
65,"If you know why something happened, and why something didn't happen...","If you know why something happened, and why something didn't happen...",,"I'm not a mathematician, so go easy. I dropped out of high school. But I'm a programmer / logician / writer of algorithms. And I'm currently engaged in one of my most complex projects to date, which involves building up something like Markov chains within one another to lead people to find unexpected, complex results out of their own psychological preferences. I'm raising this question not because I need help with my project, but because I stumbled on an unexpected feature of my data, and then had a corresponding dream about it. The feature I stumbled on was that it's quite easy to record all the events and conditions leading up to a happening, but it's quite another thing to record all the ones that didn't lead to it, let alone all the ones that didn't lead to something not happening . And let's not forget all the ones that didn't lead to something not happening which caused a whole chain of other things not to happen. The dream In the dream, K[0] is a set of things that had never been imagined happening. K[1] is a set of things that you could imagine, because they diverged from the chain of events at some point where you could still conceive of their results. But they didn't actually happen. K[2] is the much smaller set of things that actually did happen. From K[2] you can begin to infer K[1]. As soon as you imagine something from K[0], it becomes part of K[1]. For all intents and purposes, K[0] is infinite, even after you imagine parts out of it. K[1] is only limited by your imagination, and K[2] is the reality upon which your imagination is based. So in this dream, K[1] acts as a kind of thicker or thinner membrane around reality, describing the contour of K[0] which most closely match K[2]. Depending on how much of K[0] you can imagine and reference to something happening in K[2]. What I'm interested in is, is what is the relationship between these sets? For example, how can you prove that K[1] doesn't exist until you imagine it, and that K[0] is still unknown even though you can pull an element out randomly, by imagining it, and insert it into K[1]? Apologies if this makes no sense, just a human brain trying to make order out of chaos.","I'm not a mathematician, so go easy. I dropped out of high school. But I'm a programmer / logician / writer of algorithms. And I'm currently engaged in one of my most complex projects to date, which involves building up something like Markov chains within one another to lead people to find unexpected, complex results out of their own psychological preferences. I'm raising this question not because I need help with my project, but because I stumbled on an unexpected feature of my data, and then had a corresponding dream about it. The feature I stumbled on was that it's quite easy to record all the events and conditions leading up to a happening, but it's quite another thing to record all the ones that didn't lead to it, let alone all the ones that didn't lead to something not happening . And let's not forget all the ones that didn't lead to something not happening which caused a whole chain of other things not to happen. The dream In the dream, K[0] is a set of things that had never been imagined happening. K[1] is a set of things that you could imagine, because they diverged from the chain of events at some point where you could still conceive of their results. But they didn't actually happen. K[2] is the much smaller set of things that actually did happen. From K[2] you can begin to infer K[1]. As soon as you imagine something from K[0], it becomes part of K[1]. For all intents and purposes, K[0] is infinite, even after you imagine parts out of it. K[1] is only limited by your imagination, and K[2] is the reality upon which your imagination is based. So in this dream, K[1] acts as a kind of thicker or thinner membrane around reality, describing the contour of K[0] which most closely match K[2]. Depending on how much of K[0] you can imagine and reference to something happening in K[2]. What I'm interested in is, is what is the relationship between these sets? For example, how can you prove that K[1] doesn't exist until you imagine it, and that K[0] is still unknown even though you can pull an element out randomly, by imagining it, and insert it into K[1]? Apologies if this makes no sense, just a human brain trying to make order out of chaos.",,"['statistics', 'chaos-theory']"
66,Confidence interval for likelihood-ratio test,Confidence interval for likelihood-ratio test,,"I've been trying to solve the following problem: Let $X_1,\ldots,X_n$ be a sample of a random variable $X\sim\text{Exp}(\lambda)$ , i.e., the exponential distribution with parameter $\lambda>0$ . Consider the null hypothesis $H_0: \lambda=\lambda_0$ and the alternative hypothesis $H_1: \lambda\neq\lambda_0$ . Construct a confidence interval for the likelihood-ratio test with significance level $\alpha$ . I know that $\sum\limits_{i=1}^nX_i\sim\text{Gamma}(n,\lambda)$ , so $2\lambda n\mathbf{\overline X}\sim\text{Gamma}(n,\frac{1}{2})\sim\chi^2_{2n}$ , and normally I'd use that to construct the confidence interval picking $c_1$ and $c_2$ quantiles of the $\chi^2_{2n}$ distribution such that $P(c_1\le2\lambda n\mathbf{\overline X}\le c_2)=1-\alpha$ and then isolate $\lambda$ (usually I take $c_1$ and $c_2$ such that there is a probability of $\frac{\alpha}{2}$ on both sides of the opposite inequality). However, now I must use the likelihood-ratio test first, and then construct the confidence interval, and that's where I'm stumped. I obtained the likelihood-ratio test statistic: $\lambda(\mathbf{x})=\lambda_0^n\,\mathbf{\overline x}^ne^n\exp(-n\mathbf{\overline x}\lambda_0)$ , so the test would be $\begin{cases}\text{Reject }H_0\text{ if }\lambda(\mathbf{x})\le c\\\text{Not reject }H_0\text{ otherwise}\end{cases}$ with $c$ such that $P_{\lambda_0}\big(\lambda(\mathbf X)\le c\big)=\alpha$ , where the subindex indicates we calculate that probability assuming $\lambda=\lambda_0$ . I can isolate a nicer part of the likelihood-ratio test in the inequality $\lambda(\mathbf x)\le c\Rightarrow\lambda_0\,\mathbf{\overline x}\exp(-\lambda_0\mathbf{\overline x})\le\dfrac{\sqrt[n] c}{e}$ , so naming $k=\dfrac{\sqrt[n] c}{e}$ we get $P_{\lambda_0}\big(\lambda(\mathbf X)\le c\big)=P_{\lambda_0}\Big(\lambda_0\,\mathbf{\overline X}\exp(-\lambda_0\,\mathbf{\overline X})\le k\Big)$ . This is the point of the problem where I need help, since I have no idea how to proceed. Any ideas? Thanks in advance!","I've been trying to solve the following problem: Let be a sample of a random variable , i.e., the exponential distribution with parameter . Consider the null hypothesis and the alternative hypothesis . Construct a confidence interval for the likelihood-ratio test with significance level . I know that , so , and normally I'd use that to construct the confidence interval picking and quantiles of the distribution such that and then isolate (usually I take and such that there is a probability of on both sides of the opposite inequality). However, now I must use the likelihood-ratio test first, and then construct the confidence interval, and that's where I'm stumped. I obtained the likelihood-ratio test statistic: , so the test would be with such that , where the subindex indicates we calculate that probability assuming . I can isolate a nicer part of the likelihood-ratio test in the inequality , so naming we get . This is the point of the problem where I need help, since I have no idea how to proceed. Any ideas? Thanks in advance!","X_1,\ldots,X_n X\sim\text{Exp}(\lambda) \lambda>0 H_0: \lambda=\lambda_0 H_1: \lambda\neq\lambda_0 \alpha \sum\limits_{i=1}^nX_i\sim\text{Gamma}(n,\lambda) 2\lambda n\mathbf{\overline X}\sim\text{Gamma}(n,\frac{1}{2})\sim\chi^2_{2n} c_1 c_2 \chi^2_{2n} P(c_1\le2\lambda n\mathbf{\overline X}\le c_2)=1-\alpha \lambda c_1 c_2 \frac{\alpha}{2} \lambda(\mathbf{x})=\lambda_0^n\,\mathbf{\overline x}^ne^n\exp(-n\mathbf{\overline x}\lambda_0) \begin{cases}\text{Reject }H_0\text{ if }\lambda(\mathbf{x})\le c\\\text{Not reject }H_0\text{ otherwise}\end{cases} c P_{\lambda_0}\big(\lambda(\mathbf X)\le c\big)=\alpha \lambda=\lambda_0 \lambda(\mathbf x)\le c\Rightarrow\lambda_0\,\mathbf{\overline x}\exp(-\lambda_0\mathbf{\overline x})\le\dfrac{\sqrt[n] c}{e} k=\dfrac{\sqrt[n] c}{e} P_{\lambda_0}\big(\lambda(\mathbf X)\le c\big)=P_{\lambda_0}\Big(\lambda_0\,\mathbf{\overline X}\exp(-\lambda_0\,\mathbf{\overline X})\le k\Big)","['statistics', 'maximum-likelihood', 'confidence-interval']"
67,Why the cauchy is t distribution with 1 degree of freedom,Why the cauchy is t distribution with 1 degree of freedom,,"Cauchy: Place a spinner at (0,1) in the plane. Spin it in such a way that all angles are equally likely (uniformly distributed). The number that the spinner points to on the x axis follows the Cauchy distribution. This is the same as the T distribution with 1 degree of freedom. I don't understand this paragraph. I know the Student's t distribution is defined as follows: let $Z \sim N(0,1)$ and $V \sim \chi^2(v)$ . If Z and V are independent, then the distribution of $$T=\frac{Z}{\sqrt{V/v}}$$ has the Student's t distribution with v degrees of freedom. I am wondering if the df is 1, then $$T=\frac{Z}{ \sqrt{\chi^2(1)}}=\frac{Z}{Z}=1$$ Please point me where I am wrong.","Cauchy: Place a spinner at (0,1) in the plane. Spin it in such a way that all angles are equally likely (uniformly distributed). The number that the spinner points to on the x axis follows the Cauchy distribution. This is the same as the T distribution with 1 degree of freedom. I don't understand this paragraph. I know the Student's t distribution is defined as follows: let and . If Z and V are independent, then the distribution of has the Student's t distribution with v degrees of freedom. I am wondering if the df is 1, then Please point me where I am wrong.","Z \sim N(0,1) V \sim \chi^2(v) T=\frac{Z}{\sqrt{V/v}} T=\frac{Z}{ \sqrt{\chi^2(1)}}=\frac{Z}{Z}=1","['probability-theory', 'statistics']"
68,How to derive the kth coefficient standard error?,How to derive the kth coefficient standard error?,,"Given a multiple regression with the usual assumptions satisfied, with $X \in R^{n \times p}$ $$ y = X \beta + e $$ I know that the estimated variance is given by $\sigma^2 (X^TX)^{-1}$ . But what I want to know is the estimated variance for the $k^{th}$ coefficient out of this whole covariance matrix. I've seen from the resource A (cited below) that this value is the $k^{th}$ diagonal term of the $\sigma^2 (X^TX)^{-1}$ . But more interestingly (AND THIS IS WHAT I WANT TO PROVE), this value is: $$ \dfrac{\sigma^2}{(1-R^2_k) \sum_{i=1}^n (x_{ik} - \bar{x_k})^2 } $$ Here, $x_k$ is the $k^{th}$ column of $X$ . $R^2_k$ is the $R^2$ from regressing $x_k$ on $X_{(k)}$ (= $X$ after taking $k^{th}$ column out). Here is my (failed) attempt at this derivation: Using resource B (cited below), the $k^{th}$ diagonal value of $\sigma^2 (X^TX)^{-1}$ is: $$\sigma^2 [x_k^Tx_k - x_k^T X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k ] ^ {-1}$$ Since $X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k$ can be seen as projection of $x_k$ onto column space of $X_{(k)}$ , we can say: $$ X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k = \hat{x_k} $$ , which is the regression prediction after regressing $x_k$ on $X_{(k)}$ . So our diagonal value simplifies to: $$ \sigma^2 [x_k^Tx_k - x_k^T X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k ] ^ {-1}= \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1}$$ Now, since (I will omit subscript $i$ so $x_k = x_{ik}$ ) $$ R^2_k = 1 - \dfrac{ \sum (x_k - \hat{x_k} )^2 }{ \sum (x_k - \bar{x_k} )^2 } $$ $$ \dfrac{\sigma^2}{(1-R^2_k) \sum (x_{k} - \bar{x_k})^2 } $$ will simplify to: $$ \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 } $$ So in summary, we want to show that: $$ \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1} = \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 }$$ But then, $$ \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1} = \dfrac{\sigma^2}{\sum (x_k)^2 - \sum x_k \hat{x_k} } $$ where as, $$ \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 } = \dfrac{\sigma^2}{\sum (x_k)^2 - 2\sum x_k \hat{x_k} + \sum x_k^2 } $$ Could someone please help me find where I am making a mistake, and if this is not the right approach, help me derive it correctly? Resource A: http://people.stern.nyu.edu/wgreene/MathStat/GreeneChapter4.pdf (page 40) Resource B: About the diagonal entries of an inverse matrix","Given a multiple regression with the usual assumptions satisfied, with I know that the estimated variance is given by . But what I want to know is the estimated variance for the coefficient out of this whole covariance matrix. I've seen from the resource A (cited below) that this value is the diagonal term of the . But more interestingly (AND THIS IS WHAT I WANT TO PROVE), this value is: Here, is the column of . is the from regressing on (= after taking column out). Here is my (failed) attempt at this derivation: Using resource B (cited below), the diagonal value of is: Since can be seen as projection of onto column space of , we can say: , which is the regression prediction after regressing on . So our diagonal value simplifies to: Now, since (I will omit subscript so ) will simplify to: So in summary, we want to show that: But then, where as, Could someone please help me find where I am making a mistake, and if this is not the right approach, help me derive it correctly? Resource A: http://people.stern.nyu.edu/wgreene/MathStat/GreeneChapter4.pdf (page 40) Resource B: About the diagonal entries of an inverse matrix",X \in R^{n \times p}  y = X \beta + e  \sigma^2 (X^TX)^{-1} k^{th} k^{th} \sigma^2 (X^TX)^{-1}  \dfrac{\sigma^2}{(1-R^2_k) \sum_{i=1}^n (x_{ik} - \bar{x_k})^2 }  x_k k^{th} X R^2_k R^2 x_k X_{(k)} X k^{th} k^{th} \sigma^2 (X^TX)^{-1} \sigma^2 [x_k^Tx_k - x_k^T X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k ] ^ {-1} X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k x_k X_{(k)}  X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k = \hat{x_k}  x_k X_{(k)}  \sigma^2 [x_k^Tx_k - x_k^T X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k ] ^ {-1}= \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1} i x_k = x_{ik}  R^2_k = 1 - \dfrac{ \sum (x_k - \hat{x_k} )^2 }{ \sum (x_k - \bar{x_k} )^2 }   \dfrac{\sigma^2}{(1-R^2_k) \sum (x_{k} - \bar{x_k})^2 }   \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 }   \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1} = \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 }  \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1} = \dfrac{\sigma^2}{\sum (x_k)^2 - \sum x_k \hat{x_k} }   \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 } = \dfrac{\sigma^2}{\sum (x_k)^2 - 2\sum x_k \hat{x_k} + \sum x_k^2 } ,"['statistics', 'regression', 'standard-error']"
69,"Finding MLE of $\mu_1,\mu_2,\Sigma$",Finding MLE of,"\mu_1,\mu_2,\Sigma","Let $X_1,...,X_{n_1}$ be an i.i.d. sample from $N_p(\mu_1,\Sigma)$ and let $Y_1,...,Y_{n_2}$ be an independent sample from $N_p(\mu_2,\Sigma)$ , for some $\mu_1,\mu_2 \in \mathbb{R}^p$ and some invertible, $p\times p$ positive definite matrix $\Sigma$ . I'd like to find the likelihood function $L(\mu_1,\mu_2,\Sigma)$ of the commbined sample: In my book, the likelihood function of $X_1,...,X_n \sim N_p(\mu,\Sigma)$ is given by $$\frac{1}{(2\pi)^{np/2}\text{det}(\Sigma)^{n/2}}\exp\biggl(-1/2\bigl(\sum^n_{i=1}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)\bigr)\biggr)$$ So, $$L(\mu_1,\mu_2,\Sigma)=$$ $$\frac{1}{(2\pi)^{p(n_1+n_2)/2}\text{det}(\Sigma)^{\frac{n_1+n_2}{2}}}\exp\biggl(-1/2\sum^{n_1}_{i=1}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)-1/2\sum^{n_2}_{i=1}(y_i-\mu_2)^T\Sigma^{-1}(y_i-\mu_2)\biggr)$$ And so now I would like to find the MLE for $\mu_1,\mu_2,\Sigma$ : Set $$ \bar{x}:=\frac{1}{n_1}\sum_{i=1}^{n_1}x_{i} \quad\text{and}\quad S_x:=\sum_{i=1}^{n_1}(x_{i}-\bar{x})(x_{i}-\bar{x})^{\top}, $$ and, similarly, $\bar{y}$ and $S_y$ . Now I take the log of $\mathcal{L}$ : \begin{align} \ln\mathcal{L}(\mu_1,\mu_2,\Sigma)&=-\frac{(n_1+n_2)p}{2}\ln(2\pi)+\frac{(n_1+n_2)}{2}\ln|\Sigma^{-1}| \\ &\quad-\frac{1}{2}\operatorname{tr}(\Sigma^{-1}(S_x+S_y)) \\ &\quad-\frac{n_1}{2}(\bar{x}-\mu_1)^{\top}\Sigma^{-1}(\bar{x}-\mu_1)-\frac{n_2}{2}(\bar{y}-\mu_2)^{\top}\Sigma^{-1}(\bar{y}-\mu_2) \end{align} To find MLE for $\mu_1$ , I take the derivate of $\ln\mathcal{L}(\mu_1,\mu_2,\Sigma)$ w.r.t $\mu_1$ : $$\frac{d}{d\mu_1}\ln\mathcal{L}(\mu_1,\mu_2,\Sigma)=0$$ $$\frac{n_1}{2}(\bar{x}-\mu_1)^T\bigl((\Sigma^{-1})^T+\Sigma^{-1}\bigr)\frac{d}{d\mu_1}(\bar{x}-\mu_1)=0$$ $$-\frac{n_1}{2}(\bar{x}-\mu_1)^T(2\Sigma^{-1})=0$$ $$\iff\bar{x}=\mu_1$$ So MLE for $\mu_1=\bar{x}$ . So, $$-\frac{n_1}{2}(\bar{x}-\mu_1)^T(2\Sigma^{-1})$$ is maximized when $\mu_1=\bar{x}$ since $\Sigma^{-1}$ is positive definite? So, I'm not sure whether I'm doing this correct. Could someone please correct me if possible?","Let be an i.i.d. sample from and let be an independent sample from , for some and some invertible, positive definite matrix . I'd like to find the likelihood function of the commbined sample: In my book, the likelihood function of is given by So, And so now I would like to find the MLE for : Set and, similarly, and . Now I take the log of : To find MLE for , I take the derivate of w.r.t : So MLE for . So, is maximized when since is positive definite? So, I'm not sure whether I'm doing this correct. Could someone please correct me if possible?","X_1,...,X_{n_1} N_p(\mu_1,\Sigma) Y_1,...,Y_{n_2} N_p(\mu_2,\Sigma) \mu_1,\mu_2 \in \mathbb{R}^p p\times p \Sigma L(\mu_1,\mu_2,\Sigma) X_1,...,X_n \sim N_p(\mu,\Sigma) \frac{1}{(2\pi)^{np/2}\text{det}(\Sigma)^{n/2}}\exp\biggl(-1/2\bigl(\sum^n_{i=1}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)\bigr)\biggr) L(\mu_1,\mu_2,\Sigma)= \frac{1}{(2\pi)^{p(n_1+n_2)/2}\text{det}(\Sigma)^{\frac{n_1+n_2}{2}}}\exp\biggl(-1/2\sum^{n_1}_{i=1}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)-1/2\sum^{n_2}_{i=1}(y_i-\mu_2)^T\Sigma^{-1}(y_i-\mu_2)\biggr) \mu_1,\mu_2,\Sigma 
\bar{x}:=\frac{1}{n_1}\sum_{i=1}^{n_1}x_{i} \quad\text{and}\quad S_x:=\sum_{i=1}^{n_1}(x_{i}-\bar{x})(x_{i}-\bar{x})^{\top},
 \bar{y} S_y \mathcal{L} \begin{align}
\ln\mathcal{L}(\mu_1,\mu_2,\Sigma)&=-\frac{(n_1+n_2)p}{2}\ln(2\pi)+\frac{(n_1+n_2)}{2}\ln|\Sigma^{-1}| \\
&\quad-\frac{1}{2}\operatorname{tr}(\Sigma^{-1}(S_x+S_y)) \\
&\quad-\frac{n_1}{2}(\bar{x}-\mu_1)^{\top}\Sigma^{-1}(\bar{x}-\mu_1)-\frac{n_2}{2}(\bar{y}-\mu_2)^{\top}\Sigma^{-1}(\bar{y}-\mu_2)
\end{align} \mu_1 \ln\mathcal{L}(\mu_1,\mu_2,\Sigma) \mu_1 \frac{d}{d\mu_1}\ln\mathcal{L}(\mu_1,\mu_2,\Sigma)=0 \frac{n_1}{2}(\bar{x}-\mu_1)^T\bigl((\Sigma^{-1})^T+\Sigma^{-1}\bigr)\frac{d}{d\mu_1}(\bar{x}-\mu_1)=0 -\frac{n_1}{2}(\bar{x}-\mu_1)^T(2\Sigma^{-1})=0 \iff\bar{x}=\mu_1 \mu_1=\bar{x} -\frac{n_1}{2}(\bar{x}-\mu_1)^T(2\Sigma^{-1}) \mu_1=\bar{x} \Sigma^{-1}",['statistics']
70,Troubles with the Fokker–Planck equation,Troubles with the Fokker–Planck equation,,"I have read in several references that claims the following result: Given a 1-dimensional SDE $$dX_t = a(X_t,t)dt + \sigma(X_t,t) dW_t $$ where $W_t$ is a brownian motion and $\alpha,\sigma:\mathbb R\times \mathbb R\to\mathbb R$ are smooth functions (with $\sigma>0$ ), then the Fokker-Plank equation $$\frac{\partial }{\partial t}p(x,t) = -\frac{\partial}{\partial x}\left(a(x,t)p(x,t)\right)+\frac{1}{2}\frac{\partial}{\partial x^2}\left(\sigma^2 (x,t) p(x,t)\right),$$ correspond to the equation of the density function of the solution of the SDE (for instance, on the Wikipedia page it is possible to see this result). I have seen several ""derivations"" of the Fokker-Plank equation (for instance in the book ""Green functions for second-order parabolic integro-differential problems""- M.G. Garroni and J. L. Menaldi). My problem is that in all references that I have seen, they seem to start assuming that the measure $$\mu(A) = \mathbb P(X_t\in A),$$ is absolutely continuous with respect to the Lebesgue measure. Therefore, my question is the following, is it possible for the inverse path? If $p$ satisfies the Fokker-Plank equation is it possible to show that $p$ is the probability density function o the SDE solution $X_t$ ? A reference would be enough for me. All the best.","I have read in several references that claims the following result: Given a 1-dimensional SDE where is a brownian motion and are smooth functions (with ), then the Fokker-Plank equation correspond to the equation of the density function of the solution of the SDE (for instance, on the Wikipedia page it is possible to see this result). I have seen several ""derivations"" of the Fokker-Plank equation (for instance in the book ""Green functions for second-order parabolic integro-differential problems""- M.G. Garroni and J. L. Menaldi). My problem is that in all references that I have seen, they seem to start assuming that the measure is absolutely continuous with respect to the Lebesgue measure. Therefore, my question is the following, is it possible for the inverse path? If satisfies the Fokker-Plank equation is it possible to show that is the probability density function o the SDE solution ? A reference would be enough for me. All the best.","dX_t = a(X_t,t)dt + \sigma(X_t,t) dW_t  W_t \alpha,\sigma:\mathbb R\times \mathbb R\to\mathbb R \sigma>0 \frac{\partial }{\partial t}p(x,t) = -\frac{\partial}{\partial x}\left(a(x,t)p(x,t)\right)+\frac{1}{2}\frac{\partial}{\partial x^2}\left(\sigma^2 (x,t) p(x,t)\right), \mu(A) = \mathbb P(X_t\in A), p p X_t","['probability-theory', 'measure-theory', 'statistics', 'partial-differential-equations', 'stochastic-differential-equations']"
71,Trinomial distribution conditional mean,Trinomial distribution conditional mean,,"Let $(X_1,X_2)$ have the trinomial distribution with $n=5$ , $p_1=0.2$ , $p_2=0.3$ $f\left(x_1,x_2\right)=\frac{5!}{x_1!x_2!\left(5-x_1-x_2\right)!}\left(0.2\right)^{x_1}\left(0.3\right)^{0.3}\left(0.5\right)^{5-x_1-x_2}$ we want to find the conditional mean $E\left(X_2|X_1>0\right)$ . $\because X_2|X_1=x_1\sim Binomial\left(n-x_1,\ \frac{2}{7}\right)$ $ X_2|X_1=1\sim Binomial\left(4,\ \frac{2}{7}\right)$ $\Rightarrow E\left(X_2|X_1=1\right)=4\times \frac{2}{7}$ $X_2|X_1=2\sim Binomial\left(3,\ \frac{2}{7}\right)$ $\Rightarrow  E\left(X_2|X_1=2\right)=3\times \frac{2}{7} $ $X_2|X_1=3\sim Binomial\left(2,\ \frac{2}{7}\right)$ $\Rightarrow  E\left(X_2|X_1=3\right)=2\times \frac{2}{7} $ $X_2|X_1=4\sim Binomial\left(1,\ \frac{2}{7}\right)$ $\Rightarrow  E\left(X_2|X_1=4\right)=1\times \frac{2}{7} $ $X_2|X_1=5=0 $ $\Rightarrow E(X_1|X_1=5)=0$ So $E\left(X_2|X_1>0\right)=\left(5-n\right)\times \frac{2}{7},\ n=1,2,3,4,5$ , is the answer correct?","Let have the trinomial distribution with , , we want to find the conditional mean . So , is the answer correct?","(X_1,X_2) n=5 p_1=0.2 p_2=0.3 f\left(x_1,x_2\right)=\frac{5!}{x_1!x_2!\left(5-x_1-x_2\right)!}\left(0.2\right)^{x_1}\left(0.3\right)^{0.3}\left(0.5\right)^{5-x_1-x_2} E\left(X_2|X_1>0\right) \because X_2|X_1=x_1\sim Binomial\left(n-x_1,\ \frac{2}{7}\right)  X_2|X_1=1\sim Binomial\left(4,\ \frac{2}{7}\right) \Rightarrow E\left(X_2|X_1=1\right)=4\times \frac{2}{7} X_2|X_1=2\sim Binomial\left(3,\ \frac{2}{7}\right) \Rightarrow 
E\left(X_2|X_1=2\right)=3\times \frac{2}{7}  X_2|X_1=3\sim Binomial\left(2,\ \frac{2}{7}\right) \Rightarrow 
E\left(X_2|X_1=3\right)=2\times \frac{2}{7}  X_2|X_1=4\sim Binomial\left(1,\ \frac{2}{7}\right) \Rightarrow 
E\left(X_2|X_1=4\right)=1\times \frac{2}{7}  X_2|X_1=5=0  \Rightarrow E(X_1|X_1=5)=0 E\left(X_2|X_1>0\right)=\left(5-n\right)\times \frac{2}{7},\ n=1,2,3,4,5","['probability', 'statistics']"
72,Integrate lognormal pdf multiplied by an exponential to find the mean and variance of transmission through a lognormally varying medium,Integrate lognormal pdf multiplied by an exponential to find the mean and variance of transmission through a lognormally varying medium,,"Seeking to find the mean and standard deviation of two-way transmission of light through a slab whose optical depth is a lognormal random variable. The expression for the mean is $$\overline T  = \int_0^\infty  {\frac{{\exp \left( { - 2x} \right)}}{{\sqrt {2\pi } \sigma x}}} \exp \left[ { - \frac{{{{\left( {\ln x - \mu } \right)}^2}}}{{2{\sigma ^2}}}} \right]dx$$ where $T = \exp ( - 2x)$ is the two-way transmission, $x$ is the one-way optical depth, $\mu  = \overline {\ln x} $ , and ${\sigma ^2} = {\mathop{\rm var}} \left( {\ln x} \right)$ .  An algebraic solution is desired, but my attempts to solve have been in vain. An alternative formulation by using $y = \ln x$ is $$\overline T  = \int_{ - \infty }^\infty  {\frac{{\exp \left( { - 2{e^y}} \right)}}{{\sqrt {2\pi } \sigma }}} \exp \left[ { - \frac{{{{\left( {y - \mu } \right)}^2}}}{{2{\sigma ^2}}}} \right]dy$$ The expressions for the second moment of $T$ are the same, except in the integral $ - 2x$ becomes $ - 4x$ , and $ - 2{e^y}$ becomes $ - 4{e^y}$ .","Seeking to find the mean and standard deviation of two-way transmission of light through a slab whose optical depth is a lognormal random variable. The expression for the mean is where is the two-way transmission, is the one-way optical depth, , and .  An algebraic solution is desired, but my attempts to solve have been in vain. An alternative formulation by using is The expressions for the second moment of are the same, except in the integral becomes , and becomes .",\overline T  = \int_0^\infty  {\frac{{\exp \left( { - 2x} \right)}}{{\sqrt {2\pi } \sigma x}}} \exp \left[ { - \frac{{{{\left( {\ln x - \mu } \right)}^2}}}{{2{\sigma ^2}}}} \right]dx T = \exp ( - 2x) x \mu  = \overline {\ln x}  {\sigma ^2} = {\mathop{\rm var}} \left( {\ln x} \right) y = \ln x \overline T  = \int_{ - \infty }^\infty  {\frac{{\exp \left( { - 2{e^y}} \right)}}{{\sqrt {2\pi } \sigma }}} \exp \left[ { - \frac{{{{\left( {y - \mu } \right)}^2}}}{{2{\sigma ^2}}}} \right]dy T  - 2x  - 4x  - 2{e^y}  - 4{e^y},"['calculus', 'probability', 'statistics', 'definite-integrals']"
73,Method of least squares - solution verification,Method of least squares - solution verification,,"I'm homelearning statistics and trying to solve the following problem: We have regression model $\tilde{y}=a \cdot \cos(x)+\varepsilon$ . Therefore $\tilde{y}_k =  a \cdot \cos(x_k) + \varepsilon_k$ , where $\varepsilon_k \approx N(0,  \sigma^2), k= \overline{1,n} $ . Calculate an estimate of $\widehat{a}$ using the method of least squares. What is the expected value of $\tilde{y}_k$ ? What is the dispersion of $\tilde{y}_k$ ? Prove that $\widehat{a}$ is an unbiased estimate and show that the dispersion of $\widehat{a}=\frac{\sigma^2}{\sum_k^n(\cos^2(x_k))}$ . $\varepsilon_k$ is just ""noise"", or a deviation. This is my attempt, can you verify it whether it is correct please? $\hat{a}=\frac{\sum \tilde{y}_i \cdot \cos(x_i)}{\sum \cos^2(x_i)}$ $E\tilde{y}_k=E(a\cos(x_i)+\epsilon_i)=a \cdot \cos(x_i)+E(\epsilon_i)=a \cdot \cos(x_i)$ $D\tilde{y}_i=D(a\cos(x_i)+\epsilon_i)=D(\epsilon_i)=\sigma^2$ $E(\hat{a})=\frac{1}{\sum \cos^2(x)x_i} \cdot E(\sum \tilde{y}_i \cdot \cos(x_i))=\frac{1}{\sum \cos^2(x_i)} \cdot \sum E(\tilde{y}_i \cdot \cos(x_i))=\frac{1}{\sum \cos^2(x_i)} \cdot E(\tilde{y}_i) = \frac{1}{\sum \cos^2(x_i)} \cdot \sum {\cos(x_i)} \cdot a \cdot \cos(x_i)=a \cdot \frac{\sum \cos^2(x_i)}{\sum \cos^2(xi)}=a$ $D(\hat{a})=D\frac{\sum \tilde{a}_i \cdot \cos(x_i)}{\sum \cos^2(x_i)}=\frac{1}{\sum \cos^2(x_i)} \cdot D(\sum \tilde{y_i} \cdot \cos(x_i))=(\frac{1}{\sum \cos^2(x_i)})^2 \cdot \sum D(\tilde{y}_i \cdot \cos(x_i))=(\frac{1}{\sum \cos^2(x_i)})^2 \cdot \sum \cos^2(x_i) \cdot D(\tilde{y}_i)=(\frac{1}{\sum \cos^2(x_1)})^2 \cdot \sum \cos^2(x_i) \cdot \sigma^2=\sigma^2 \cdot \frac{\sum \cos^2(x_i)}{(\sum \cos^2(x_i))^2}=\frac{\sigma^2}{\sum \cos^2 (x_i)}$ Is it correct? Thanks","I'm homelearning statistics and trying to solve the following problem: We have regression model . Therefore , where . Calculate an estimate of using the method of least squares. What is the expected value of ? What is the dispersion of ? Prove that is an unbiased estimate and show that the dispersion of . is just ""noise"", or a deviation. This is my attempt, can you verify it whether it is correct please? Is it correct? Thanks","\tilde{y}=a \cdot \cos(x)+\varepsilon \tilde{y}_k =
 a \cdot \cos(x_k) + \varepsilon_k \varepsilon_k \approx N(0,
 \sigma^2), k= \overline{1,n}  \widehat{a} \tilde{y}_k \tilde{y}_k \widehat{a} \widehat{a}=\frac{\sigma^2}{\sum_k^n(\cos^2(x_k))} \varepsilon_k \hat{a}=\frac{\sum \tilde{y}_i \cdot \cos(x_i)}{\sum \cos^2(x_i)} E\tilde{y}_k=E(a\cos(x_i)+\epsilon_i)=a \cdot \cos(x_i)+E(\epsilon_i)=a \cdot \cos(x_i) D\tilde{y}_i=D(a\cos(x_i)+\epsilon_i)=D(\epsilon_i)=\sigma^2 E(\hat{a})=\frac{1}{\sum \cos^2(x)x_i} \cdot E(\sum \tilde{y}_i \cdot \cos(x_i))=\frac{1}{\sum \cos^2(x_i)} \cdot \sum E(\tilde{y}_i \cdot \cos(x_i))=\frac{1}{\sum \cos^2(x_i)} \cdot E(\tilde{y}_i) = \frac{1}{\sum \cos^2(x_i)} \cdot \sum {\cos(x_i)} \cdot a \cdot \cos(x_i)=a \cdot \frac{\sum \cos^2(x_i)}{\sum \cos^2(xi)}=a D(\hat{a})=D\frac{\sum \tilde{a}_i \cdot \cos(x_i)}{\sum \cos^2(x_i)}=\frac{1}{\sum \cos^2(x_i)} \cdot D(\sum \tilde{y_i} \cdot \cos(x_i))=(\frac{1}{\sum \cos^2(x_i)})^2 \cdot \sum D(\tilde{y}_i \cdot \cos(x_i))=(\frac{1}{\sum \cos^2(x_i)})^2 \cdot \sum \cos^2(x_i) \cdot D(\tilde{y}_i)=(\frac{1}{\sum \cos^2(x_1)})^2 \cdot \sum \cos^2(x_i) \cdot \sigma^2=\sigma^2 \cdot \frac{\sum \cos^2(x_i)}{(\sum \cos^2(x_i))^2}=\frac{\sigma^2}{\sum \cos^2 (x_i)}","['statistics', 'regression', 'least-squares']"
74,consolidate codependent continuous probability distributions into one multivariable distribution: how?,consolidate codependent continuous probability distributions into one multivariable distribution: how?,,"Sorry if this has been answered satisfactorily elsewhere; if it has and eluded me, post the link and I shall close this. This post is motivated by a couple of questions that I’ve asked recently (at inscribed vs. circumscribed randomly generated similar triangles: which one has more ‘excess’ area on average? by how much = triangles,circles ; and A square appears randomly within a square of ten time its area. What is probability that the smaller square contains the larger square's center? = squares), both which hinge on assigning some sort of probability distribution to two or more interrelated factors. Some of the commenters called an impasse at deciding which type of distribution to use, although in those two geometric cases I believe that a uniform (instead of a gaussian, poisson, chi-square, or other) matches its problem statement. In any case, I don’t know how to combine the two or more distributions into single cohesive one without prioritizing preference to one or another caused by order of calculation. What are the appropriate or applicable methods for combining continuous probabilities defined by known relationships with eachother and finite boundaries, in a non-ordered manner, into a single multi-variable probability distribution?","Sorry if this has been answered satisfactorily elsewhere; if it has and eluded me, post the link and I shall close this. This post is motivated by a couple of questions that I’ve asked recently (at inscribed vs. circumscribed randomly generated similar triangles: which one has more ‘excess’ area on average? by how much = triangles,circles ; and A square appears randomly within a square of ten time its area. What is probability that the smaller square contains the larger square's center? = squares), both which hinge on assigning some sort of probability distribution to two or more interrelated factors. Some of the commenters called an impasse at deciding which type of distribution to use, although in those two geometric cases I believe that a uniform (instead of a gaussian, poisson, chi-square, or other) matches its problem statement. In any case, I don’t know how to combine the two or more distributions into single cohesive one without prioritizing preference to one or another caused by order of calculation. What are the appropriate or applicable methods for combining continuous probabilities defined by known relationships with eachother and finite boundaries, in a non-ordered manner, into a single multi-variable probability distribution?",,"['linear-algebra', 'probability-theory']"
75,Is there a Central limit theorem for max,Is there a Central limit theorem for max,,"I have multiple IID (bigger then 0) of unknown distribution and I get the max of them as a result of my calculation. Can I say that the max also have certain gaussian shape distribution? If not what is the distribution of the result? I saw this answer which describe the result of IID product in a log-normal distribution. but as far as I understand this is not the same as max. I tried to run a simulation in MATLAB with 1000*1000 uniform IID in range 1000 and got that the max of each column is not distibuted randomlly but getting close to 1000. here is the code: r = randi(1000,[1000,1000]); mxR = max(r); figure; hist(mxR); After that I tried some other non - caped distribution (normal dist. with \mu = 5000 and \sigma = 500) and I got something similar tolog-normal dist. distribution result of max IID (normal) here is the code: r = random('Normal',5000,500,[10000,10000]); mxR = max(r); figure; hist(mxR); Is there a rule or a theorem regarding this? thanks for your help!! (If you think this is interesting (or not) please let me know)","I have multiple IID (bigger then 0) of unknown distribution and I get the max of them as a result of my calculation. Can I say that the max also have certain gaussian shape distribution? If not what is the distribution of the result? I saw this answer which describe the result of IID product in a log-normal distribution. but as far as I understand this is not the same as max. I tried to run a simulation in MATLAB with 1000*1000 uniform IID in range 1000 and got that the max of each column is not distibuted randomlly but getting close to 1000. here is the code: r = randi(1000,[1000,1000]); mxR = max(r); figure; hist(mxR); After that I tried some other non - caped distribution (normal dist. with \mu = 5000 and \sigma = 500) and I got something similar tolog-normal dist. distribution result of max IID (normal) here is the code: r = random('Normal',5000,500,[10000,10000]); mxR = max(r); figure; hist(mxR); Is there a rule or a theorem regarding this? thanks for your help!! (If you think this is interesting (or not) please let me know)",,['statistics']
76,MAP estimation for discriminative models,MAP estimation for discriminative models,,"I have some problems in understanding the MAP estimation for discriminative models. I will use the notation used in the very first two pages of this paper https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-Valencia-07.pdf As far as I understand, the posterior distribution of a discriminative model is $p(\theta|X, C)$ . Where $X = \{x_1,x_2,\dots,x_n\}$ is the training set while $C=\{c_1,c_2,\dots,c_n\}$ are the corresponding labels. As usual the posterior is split into the prior and likelihood. $$p(\theta|X,C) \overset{?}{=} \frac{p(\theta)L(\theta)}{p(C|X)} = \frac{p(\theta)p(C|X,\theta)}{p(C|X)}$$ The step that I do not understand is the one marked by the ""?"". Moreover, in the same paper, it is also noted that: $p(\theta,C|X)=p(\theta)L(\theta)$ . However, if we go a bit further: \begin{aligned} p(\theta,C|X)=p(\theta)L(\theta) \implies \\ p(\theta,C|X)=p(\theta)p(C|X,\theta) \implies \\ \frac{p(\theta,C,X)}{p(X)} = \frac{p(\theta)p(\theta,C,X)}{p(X,\theta)} \implies \\ p(X)p(\theta) = p(X,\theta) \end{aligned} Therefore, it seems that $X$ and $\theta$ are independent but I fail to see why. Given such independence, it would be fairly simple also proving the step marked with ""?"".","I have some problems in understanding the MAP estimation for discriminative models. I will use the notation used in the very first two pages of this paper https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-Valencia-07.pdf As far as I understand, the posterior distribution of a discriminative model is . Where is the training set while are the corresponding labels. As usual the posterior is split into the prior and likelihood. The step that I do not understand is the one marked by the ""?"". Moreover, in the same paper, it is also noted that: . However, if we go a bit further: Therefore, it seems that and are independent but I fail to see why. Given such independence, it would be fairly simple also proving the step marked with ""?"".","p(\theta|X, C) X = \{x_1,x_2,\dots,x_n\} C=\{c_1,c_2,\dots,c_n\} p(\theta|X,C) \overset{?}{=} \frac{p(\theta)L(\theta)}{p(C|X)} = \frac{p(\theta)p(C|X,\theta)}{p(C|X)} p(\theta,C|X)=p(\theta)L(\theta) \begin{aligned}
p(\theta,C|X)=p(\theta)L(\theta) \implies \\
p(\theta,C|X)=p(\theta)p(C|X,\theta) \implies \\
\frac{p(\theta,C,X)}{p(X)} = \frac{p(\theta)p(\theta,C,X)}{p(X,\theta)} \implies \\
p(X)p(\theta) = p(X,\theta)
\end{aligned} X \theta","['statistics', 'statistical-inference', 'machine-learning']"
77,Energy distance between random variable and random sample,Energy distance between random variable and random sample,,"If $X$ and $Y$ are independent RVs we can expect $\mathbb{E}|X-Y|$ to be a measure of how ""far apart"" the variables are, but it doesn't work like a metric in the sense that its minimum is positive, attained when $Y=X'$ is an iid RV. But we can actually offset the expectation $\mathbb{E}|X-Y|$ accordingly - that is, if we pick $X'$ and $Y'$ to be iid with $X$ and $Y$ then $$ d(X,Y)^2=2\mathbb{E}|X-Y|-\mathbb{E}|X-X'|-\mathbb{E}|Y-Y'| $$ which apparently equals $$ =2\int_{\mathbb{R}} \big(F_X(t)-F_Y(t)\big)^2\,\mathrm{d}t $$ and $d(X,Y)$ is called the energy distance which measures how close $X$ and $Y$ are in distribution. Given a random variable $X$ , we can define an energy function $E(x_1,\cdots,x_n)$ to be (half the squared) energy distance between $X$ and the discrete uniform distribution on the set $\{x_1,\cdots,x_n\}$ . Then  given a random sample, i.e. $n$ RVs $X_1,\cdots,X_n$ which are iid with $X$ , we can define the energy variable $E=E(X_1,\cdots,X_n)$ , which is itself an RV. Question . Do we know anything about how $E$ is distributed? Can we calculate it explicitly, or any nice statistics (like mean, median, etc.) for particular RVs $X$ ? What about good estimates? Presumably as $n\to\infty$ we have $E\to0$ in some sense, can we describe how fast and in what sense $E$ converges to $0$ (in terms of $X$ 's distribution)? If $U$ is the uniform distribution on $[0,1]$ I calculated $$ E(u_1,\cdots,u_n) = \frac{1}{12n^2}+\frac{1}{n}\sum_{k=1}^n\left(u_k-\frac{2k-1}{2n}\right)^2 $$ where $0<u_1<\cdots<u_n<1$ . This tells us $E_{\mathrm{min}}=\frac{1}{12n^2}$ is attained when $u_k=\frac{2k-1}{2n}$ , and possibly $E_{\mathrm{max}}$ is attained when $u_k=1$ , though it seems too difficult to find $F_E(e)$ for the median, although the mean looks calculable, albeit with a pretty large formula. Edit . Intuitively, a point-set being in a narrower range should make $E$ larger, so we should expect $E_\mathrm{min}$ to not occur at a boundary point, so we can find it by solving $\nabla E=0$ , which occurs at the vector $\vec\alpha$ for which $F_X(\alpha_k)=\frac{2k-1}{2n}$ for $k=1,\cdots,n$ . Then we may integrate $\nabla E$ from $\vec\alpha$ to $\vec x$ to obtain a generalized energy formula: $$ E-E_{\mathrm{min}}=\frac{2}{n}\sum_{k=1}^n \int_{\alpha_k}^{x_k}(x_k-u)f_X(u)\,\mathrm{d}u. $$ Plugging order statistics $X_{(k)}$ into this doesn't seem very amenable but it's a nice formula.","If and are independent RVs we can expect to be a measure of how ""far apart"" the variables are, but it doesn't work like a metric in the sense that its minimum is positive, attained when is an iid RV. But we can actually offset the expectation accordingly - that is, if we pick and to be iid with and then which apparently equals and is called the energy distance which measures how close and are in distribution. Given a random variable , we can define an energy function to be (half the squared) energy distance between and the discrete uniform distribution on the set . Then  given a random sample, i.e. RVs which are iid with , we can define the energy variable , which is itself an RV. Question . Do we know anything about how is distributed? Can we calculate it explicitly, or any nice statistics (like mean, median, etc.) for particular RVs ? What about good estimates? Presumably as we have in some sense, can we describe how fast and in what sense converges to (in terms of 's distribution)? If is the uniform distribution on I calculated where . This tells us is attained when , and possibly is attained when , though it seems too difficult to find for the median, although the mean looks calculable, albeit with a pretty large formula. Edit . Intuitively, a point-set being in a narrower range should make larger, so we should expect to not occur at a boundary point, so we can find it by solving , which occurs at the vector for which for . Then we may integrate from to to obtain a generalized energy formula: Plugging order statistics into this doesn't seem very amenable but it's a nice formula.","X Y \mathbb{E}|X-Y| Y=X' \mathbb{E}|X-Y| X' Y' X Y  d(X,Y)^2=2\mathbb{E}|X-Y|-\mathbb{E}|X-X'|-\mathbb{E}|Y-Y'|   =2\int_{\mathbb{R}} \big(F_X(t)-F_Y(t)\big)^2\,\mathrm{d}t  d(X,Y) X Y X E(x_1,\cdots,x_n) X \{x_1,\cdots,x_n\} n X_1,\cdots,X_n X E=E(X_1,\cdots,X_n) E X n\to\infty E\to0 E 0 X U [0,1]  E(u_1,\cdots,u_n) = \frac{1}{12n^2}+\frac{1}{n}\sum_{k=1}^n\left(u_k-\frac{2k-1}{2n}\right)^2  0<u_1<\cdots<u_n<1 E_{\mathrm{min}}=\frac{1}{12n^2} u_k=\frac{2k-1}{2n} E_{\mathrm{max}} u_k=1 F_E(e) E E_\mathrm{min} \nabla E=0 \vec\alpha F_X(\alpha_k)=\frac{2k-1}{2n} k=1,\cdots,n \nabla E \vec\alpha \vec x  E-E_{\mathrm{min}}=\frac{2}{n}\sum_{k=1}^n \int_{\alpha_k}^{x_k}(x_k-u)f_X(u)\,\mathrm{d}u.  X_{(k)}","['probability-theory', 'statistics']"
78,Determining how a Markov transition function move from one state to the next,Determining how a Markov transition function move from one state to the next,,"If $(X, \mathcal{F})$ is a measure space, we may define a Markov transition function $P: X \times \mathcal{F} \to \mathbb{R}$ as a function such that if $x$ is fixed, $P(x, A)$ is a probability measure on $\mathcal{G},$ and if $A$ is fixed, then $P(x,A)$ is a measurable function of $x.$ However, the definition does not say anything about how we move from one state to the next, so I'm trying to figure that out. My 1st guess: If $A$ is fixed, then starting at $x_0$ we move to $P(x_0, A),$ then $P(P(x_0,A), A)$ and so on. This is clearly deterministic and does not explain the case $X \ne \mathbb{R},$ so it's wrong. My 2nd guess: Instead, $P(x_0, A)$ represents the probability $x_0$ moves to a point in $A.$ However, how do we know that just by defining $P,$ the function doesn't contradict itself? If we have $X = \mathbb{R}$ and somehow arrive at $P(0, [-1,0]) = 1/5, P(0, [1,2]) = 2/5, P(0, [-1,0] \cup [1,2]) = 4/5,$ then $P$ is clearly inconsistent. If $P(x_0, \cdot)$ is $\sigma$ -additive as a function $\mathcal{F} \to \mathbb{R},$ then we can guarantee consistency, and the probabilities actually mean something. Luckily, this is implied by the italicized part of the definition. But how about actually calculating where $x_0$ goes? If $X = \mathbb{R},$ we should have some way of calculating a probability distribution. What would the distribution be in terms of $P$ ? Certainly, if $F_n(x)$ is the distribution that is $P(x, [i/n, (i+1)/n])$ on $[i/n, (i+1)/n]$ for all $i \in \mathbb{Z},$ then $F(x) = \lim\limits_n F_n(x),$ is the distribution we seek. But there has to be a simpler way, right? I thought of these issues while solving the problem below. It is easy, but I suspect if someone hands me a more complicated distribution in the future and asks me to calculate a different probability, I would be hard pressed to jump through the hoops I did in my solution. Consider  a  Markov  chain  whose  state  space  is $\mathbb{R}.$ Let $P(x,A), x \in \mathbb{R} ,A \in \mathcal{B}(\mathbb{R}),$ be the following Markov transition function, $$P(x,A) =\lambda([x−1/2,x+ 1/2]\cap A),$$ where $\lambda$ is  the  Lebesgue  measure.  Assuming  that  the  initial  distribution  is concentrated at the origin, find $P(|\omega_2| \le 1/4).$ Solution: $P$ clearly models $x$ moving to a point selected from $[x-\frac{1}{2}, x+\frac{1}{2}]$ uniformly at random. Thus, the distribution of $\omega_1$ is a uniform distribution on $[-1/2, 1/2],$ and the probability $|\omega_2| \le 1/4$ is $\int_{-1/2}^{1/2} P(|w_2| \le 1/4 | w_1 = x) \, dx = \int_{-1/2}^{1/2} \lambda([x-0.5, x+0.5] \cap [-1/4, 1/4]) \, dx = 2(\int_0^{1/4} \frac{1}{2} \, dx + \int_{1/4}^{1/2} \frac{3}{4} - x \, dx) = 2(\frac{1}{8} + \frac{3}{32}) = 7/16.$ My solution relies on recognizing that $P$ is much simpler than it sounds. But what if it's impossible to interpret $P$ and you're just given a bizarre expression? Would there be some brute force way to proceed with the problem and reduce it to mere integration?","If is a measure space, we may define a Markov transition function as a function such that if is fixed, is a probability measure on and if is fixed, then is a measurable function of However, the definition does not say anything about how we move from one state to the next, so I'm trying to figure that out. My 1st guess: If is fixed, then starting at we move to then and so on. This is clearly deterministic and does not explain the case so it's wrong. My 2nd guess: Instead, represents the probability moves to a point in However, how do we know that just by defining the function doesn't contradict itself? If we have and somehow arrive at then is clearly inconsistent. If is -additive as a function then we can guarantee consistency, and the probabilities actually mean something. Luckily, this is implied by the italicized part of the definition. But how about actually calculating where goes? If we should have some way of calculating a probability distribution. What would the distribution be in terms of ? Certainly, if is the distribution that is on for all then is the distribution we seek. But there has to be a simpler way, right? I thought of these issues while solving the problem below. It is easy, but I suspect if someone hands me a more complicated distribution in the future and asks me to calculate a different probability, I would be hard pressed to jump through the hoops I did in my solution. Consider  a  Markov  chain  whose  state  space  is Let be the following Markov transition function, where is  the  Lebesgue  measure.  Assuming  that  the  initial  distribution  is concentrated at the origin, find Solution: clearly models moving to a point selected from uniformly at random. Thus, the distribution of is a uniform distribution on and the probability is My solution relies on recognizing that is much simpler than it sounds. But what if it's impossible to interpret and you're just given a bizarre expression? Would there be some brute force way to proceed with the problem and reduce it to mere integration?","(X, \mathcal{F}) P: X \times \mathcal{F} \to \mathbb{R} x P(x, A) \mathcal{G}, A P(x,A) x. A x_0 P(x_0, A), P(P(x_0,A), A) X \ne \mathbb{R}, P(x_0, A) x_0 A. P, X = \mathbb{R} P(0, [-1,0]) = 1/5, P(0, [1,2]) = 2/5, P(0, [-1,0] \cup [1,2]) = 4/5, P P(x_0, \cdot) \sigma \mathcal{F} \to \mathbb{R}, x_0 X = \mathbb{R}, P F_n(x) P(x, [i/n, (i+1)/n]) [i/n, (i+1)/n] i \in \mathbb{Z}, F(x) = \lim\limits_n F_n(x), \mathbb{R}. P(x,A), x \in \mathbb{R} ,A \in \mathcal{B}(\mathbb{R}), P(x,A) =\lambda([x−1/2,x+ 1/2]\cap A), \lambda P(|\omega_2| \le 1/4). P x [x-\frac{1}{2}, x+\frac{1}{2}] \omega_1 [-1/2, 1/2], |\omega_2| \le 1/4 \int_{-1/2}^{1/2} P(|w_2| \le 1/4 | w_1 = x) \, dx = \int_{-1/2}^{1/2} \lambda([x-0.5, x+0.5] \cap [-1/4, 1/4]) \, dx = 2(\int_0^{1/4} \frac{1}{2} \, dx + \int_{1/4}^{1/2} \frac{3}{4} - x \, dx) = 2(\frac{1}{8} + \frac{3}{32}) = 7/16. P P","['probability-theory', 'statistics', 'markov-chains', 'markov-process']"
79,How many distinctively different necklaces are possible if you use 0 to 3 beads out of 5 different beads given? [closed],How many distinctively different necklaces are possible if you use 0 to 3 beads out of 5 different beads given? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Today in my IB Math II HL/AP Calculus BC class we were going over IB Math I topics and did the given problem. So you would calculate the number of necklaces for a 0, 1, 2, and 3 bead necklace. I used $5C0+5C1+5C2(2!)(1/2)+5C3(3!/3)(1/2)=26 to find my answer. In a two bead necklace it would be multiplied by 2 because of the two different combinations, but then is cancelled out because a necklace is circular. However, my teacher said that a two bead necklace cannot exist, and therefore did not divide by 2, saying that a necklace A-B and B-A are two different necklaces. He said that two points cannot make a circular shape and must be treated as linear. Maybe he is right?? Therefore he gave the answer 36 because he did not divide the 5C2 term by 2. Thank you in advance, I hope I can find an answer to this problem.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Today in my IB Math II HL/AP Calculus BC class we were going over IB Math I topics and did the given problem. So you would calculate the number of necklaces for a 0, 1, 2, and 3 bead necklace. I used $5C0+5C1+5C2(2!)(1/2)+5C3(3!/3)(1/2)=26 to find my answer. In a two bead necklace it would be multiplied by 2 because of the two different combinations, but then is cancelled out because a necklace is circular. However, my teacher said that a two bead necklace cannot exist, and therefore did not divide by 2, saying that a necklace A-B and B-A are two different necklaces. He said that two points cannot make a circular shape and must be treated as linear. Maybe he is right?? Therefore he gave the answer 36 because he did not divide the 5C2 term by 2. Thank you in advance, I hope I can find an answer to this problem.",,"['probability', 'statistics']"
80,How to prove this asymptotic behavior of the expectation of the maximum of the independent standard normal variables?,How to prove this asymptotic behavior of the expectation of the maximum of the independent standard normal variables?,,"How to prove this famous theorem? If $G_1,G_2,...,G_n$ are independent standard normal random variables( $\mathcal{N}(0,1)$ ), then: $$\lim_{n\rightarrow\infty}\frac{\mathbb{E}[\max_{i=1,2,...,n}G_i]}{\sqrt{2\log n}}=1$$ I know the first step: $\Psi_G(\lambda)=\mathbb{E}[e^{\lambda G}]=\frac{\lambda^2}{2}$ then according to the Jensen's inequality, for all $\lambda>0$ \begin{equation*} \begin{split} \exp(\lambda\mathbb{E}[\max_{i=1,2,...,n}G_i]&\leq\mathbb{E}[\exp(\lambda\max_{i=1,2,...,n}G_i)]\\ &=\mathbb{E}[\max_{i=1,2,...,n}\exp(\lambda G_i)]\\ &\leq \sum_{i=1}^n\mathbb{E}[\exp(\lambda G_i)]\leq ne^{\frac{\lambda^2}{2}} \end{split} \end{equation*} then $$\mathbb{E}[\max_{i=1,2,...,n}G_i]\leq\frac{\log n}{\lambda}+\frac{\lambda}{2}$$ the right term is minimised with $\lambda=\sqrt{2\log n}$ we have $$\mathbb{E}[\max_{i=1,2,...,n}G_i]\leq\sqrt{2\log n}$$ What's the next step?","How to prove this famous theorem? If are independent standard normal random variables( ), then: I know the first step: then according to the Jensen's inequality, for all then the right term is minimised with we have What's the next step?","G_1,G_2,...,G_n \mathcal{N}(0,1) \lim_{n\rightarrow\infty}\frac{\mathbb{E}[\max_{i=1,2,...,n}G_i]}{\sqrt{2\log n}}=1 \Psi_G(\lambda)=\mathbb{E}[e^{\lambda G}]=\frac{\lambda^2}{2} \lambda>0 \begin{equation*}
\begin{split}
\exp(\lambda\mathbb{E}[\max_{i=1,2,...,n}G_i]&\leq\mathbb{E}[\exp(\lambda\max_{i=1,2,...,n}G_i)]\\
&=\mathbb{E}[\max_{i=1,2,...,n}\exp(\lambda G_i)]\\
&\leq \sum_{i=1}^n\mathbb{E}[\exp(\lambda G_i)]\leq ne^{\frac{\lambda^2}{2}}
\end{split}
\end{equation*} \mathbb{E}[\max_{i=1,2,...,n}G_i]\leq\frac{\log n}{\lambda}+\frac{\lambda}{2} \lambda=\sqrt{2\log n} \mathbb{E}[\max_{i=1,2,...,n}G_i]\leq\sqrt{2\log n}","['probability', 'statistics', 'probability-distributions', 'integral-inequality']"
81,"If $X$ is binomial, $2X$ isn't binomial","If  is binomial,  isn't binomial",X 2X,"My question is really simple. I Don't understand why if a random variable $X \sim \text{Bin}(n,p)$ , then $2X$ isn't binomial. I know every value of $2X$ is even, but I can't prove $2X$ isn't binomial using this fact.","My question is really simple. I Don't understand why if a random variable , then isn't binomial. I know every value of is even, but I can't prove isn't binomial using this fact.","X \sim \text{Bin}(n,p) 2X 2X 2X","['statistics', 'random-variables', 'binomial-distribution']"
82,One-sided alternative hypothesis,One-sided alternative hypothesis,,"I am a little bit confused about definitions of null and alternative hypotheses. My understanding is that the null and alternative hypotheses are defined based one a partition of the parameter space $\Theta$ . Suppose that $\Theta$ can be partitioned into two disjoint subsets $\Theta_0$ and $\Theta_1$ . Then, the null is such that $\theta \in \Theta_0$ and the alternative is such that $\theta \in \Theta_1$ . However, it is not uncommon to find examples where people test: $$ H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta > \theta_0 $$ or $$ H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta < \theta_0 .$$ For example, here : https://statweb.stanford.edu/~owen/courses/200/lec07.pdf (bottom of page 3) http://www.stat.yale.edu/Courses/1997-98/101/sigtest.htm These hypotheses don't form a partition of the parameter space (assuming $\theta\in\mathbb{R}$ ). Is it still correct to do that? Do people assume (implicitly) that the equality in the null is actually an inequality?","I am a little bit confused about definitions of null and alternative hypotheses. My understanding is that the null and alternative hypotheses are defined based one a partition of the parameter space . Suppose that can be partitioned into two disjoint subsets and . Then, the null is such that and the alternative is such that . However, it is not uncommon to find examples where people test: or For example, here : https://statweb.stanford.edu/~owen/courses/200/lec07.pdf (bottom of page 3) http://www.stat.yale.edu/Courses/1997-98/101/sigtest.htm These hypotheses don't form a partition of the parameter space (assuming ). Is it still correct to do that? Do people assume (implicitly) that the equality in the null is actually an inequality?",\Theta \Theta \Theta_0 \Theta_1 \theta \in \Theta_0 \theta \in \Theta_1  H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta > \theta_0   H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta < \theta_0 . \theta\in\mathbb{R},"['statistics', 'statistical-inference', 'hypothesis-testing']"
83,Bound for Gaussian integral,Bound for Gaussian integral,,"Let $g_1, ..., g_n \stackrel{\text{iid}}{\sim} N(0,1)$ and $T$ be an arbitrary index set for a bounded family of distributions, say bounded by $M$ . Let $Y_i(t), i \in \{1, ..., n\}, t \in T$ be iid from the family and also independent of the Gaussians. I want to show the following: $$E \left(\sup_{t \in T} \left \lvert \sum_{i=1}^n g_i Y_i(t) \right \rvert \right) \leq 2E\left(\sup_{t \in T} \sum_{i=1}^n g_i Y_i(t)  \right) + C\sqrt{n}$$ where $C$ is a fixed constant. I really don't know where to start, or which symmetrization principle I'm supposed to use (if any). I know that for $\epsilon_i$ iid Bernoulli's and independent of everything else that the LHS of the inequality is equal to $$E\left(\sup_{t \in T} \left \lvert \sum_{i=1}^n \epsilon_i |g_i| Y_i(t) \right \rvert \right)$$ but I don't know any tricks on how to leverage this. Since asking, I produced another rather feeble attempt (perhaps it's useful to someone).  Denote $g \cdot Y(t)$ the corresponding dot product of vectors and decompose the absolute value into pieces: $$\begin{aligned} LHS & = E\left(\sup_{t \in T} \bigg( g \cdot Y(t) + 2(-g \cdot Y(t) \mathbb{1}(g \cdot Y < 0) \bigg)  \right)\\ & \leq E\left(\sup_{t \in T} \sum_{i=1}^n g_i Y_i(t)  \right) + 2 E\left(\sup_{t \in T} - g \cdot Y(t) \mathbf{1}(g \cdot Y(t) < 0)  \right)\end{aligned}$$","Let and be an arbitrary index set for a bounded family of distributions, say bounded by . Let be iid from the family and also independent of the Gaussians. I want to show the following: where is a fixed constant. I really don't know where to start, or which symmetrization principle I'm supposed to use (if any). I know that for iid Bernoulli's and independent of everything else that the LHS of the inequality is equal to but I don't know any tricks on how to leverage this. Since asking, I produced another rather feeble attempt (perhaps it's useful to someone).  Denote the corresponding dot product of vectors and decompose the absolute value into pieces:","g_1, ..., g_n \stackrel{\text{iid}}{\sim} N(0,1) T M Y_i(t), i \in \{1, ..., n\}, t \in T E \left(\sup_{t \in T} \left \lvert \sum_{i=1}^n g_i Y_i(t) \right \rvert \right) \leq 2E\left(\sup_{t \in T} \sum_{i=1}^n g_i Y_i(t)  \right) + C\sqrt{n} C \epsilon_i E\left(\sup_{t \in T} \left \lvert \sum_{i=1}^n \epsilon_i |g_i| Y_i(t) \right \rvert \right) g \cdot Y(t) \begin{aligned} LHS & = E\left(\sup_{t \in T} \bigg( g \cdot Y(t) + 2(-g \cdot Y(t) \mathbb{1}(g \cdot Y < 0) \bigg)  \right)\\
& \leq E\left(\sup_{t \in T} \sum_{i=1}^n g_i Y_i(t)  \right) + 2 E\left(\sup_{t \in T} - g \cdot Y(t) \mathbf{1}(g \cdot Y(t) < 0)  \right)\end{aligned}","['statistics', 'inequality', 'normal-distribution', 'gaussian-integral']"
84,Random i.i.d variable series,Random i.i.d variable series,,"I want to prove that if { $X_n$ } $_{n\in N}$ is a sequence of independent and identically distributed random variables, non-degenerated in 0, it means, none of them are almost sure  equal to 0, then $P(\sum_{i=1}^n X_{i}$ converges $) = 0$ Some advice would be very helpful. Thanks.","I want to prove that if { } is a sequence of independent and identically distributed random variables, non-degenerated in 0, it means, none of them are almost sure  equal to 0, then converges Some advice would be very helpful. Thanks.",X_n _{n\in N} P(\sum_{i=1}^n X_{i} ) = 0,"['probability', 'probability-theory', 'statistics', 'probability-limit-theorems']"
85,Derivation of the Entropy of the Johnson-SU distribution,Derivation of the Entropy of the Johnson-SU distribution,,The pdf of the Normal distribution is $$ f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^{2}} $$ The pdf of the Johnson-SU distribution is $$ f(x) = \frac{\delta}{\lambda \sqrt{2 \pi}} \frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}} e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}} $$ The differential entropy of the Normal pdf is derived as \begin{align}\label{equation:hN} h(X) = & -\int_{-\infty}^{\infty} f(x) \ln f(x) \mathrm{d} x\\ =&-\int_{-\infty}^{\infty} \left(2\pi \sigma^2\right)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \ln\left[ \left(2\pi \sigma^2\right)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \right] \mathrm{d} x\\ = &\frac{1}{2} \ln(2\pi \sigma^2) \int_{-\infty}^{\infty} (2\pi \sigma^2)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \mathrm{d} x\\ &+ \frac{1}{2\sigma^2} \left(2\pi \sigma^2\right)^{-\frac{1}{2}} (x-\mu)^2 e^{-(x-\mu)^2 / 2\sigma^2} \mathrm{d} x\\ =& \frac{1}{2} \ln (2\pi\sigma^2) + \frac{1}{2}\\ =& \frac12\ln(2\pi\sigma^2) + \frac12\ln e\\ =& \frac12\left(\ln(2\pi\sigma^2) + \ln e\right)\\ =& \frac{1}{2} \ln (2\pi e \sigma^2) \end{align} What is the derivation of the Johnson-SU pdf's differential entropy ? First Attempt \begin{align} h(X) = &-\int_{-\infty}^{\infty} f(x) \ln f(x) \mathrm{d} x\\ =& \mathbb{E}[ -\ln f(x)] \\ =& \mathbb{E} \left[ -\ln \left( \frac{\delta}{\lambda \sqrt{2 \pi}} \frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}} e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}} \right) \right] \\ =& \mathbb{E} \left[ -\ln \left( \frac{\delta}{\lambda \sqrt{2 \pi}} \right) -\ln \left(\frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}}\right) - \ln \left( e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}} \right) \right] \\ =& \delta \ln \left( \lambda \sqrt{2 \pi} \right) - \mathbb{E} \left[ -\ln \left(\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}\right) -\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}\right] \\ =& \delta \ln \left( \lambda \sqrt{2 \pi} \right) - \mathbb{E} \left[ -\frac{1}{2} \ln \left(1+\left(\frac{x-\xi}{\lambda}\right)^{2}\right) -\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}\right] \\ =& \delta \ln \left( \lambda \sqrt{2 \pi} \right) -\frac{1}{2} \mathbb{E} \left[ - \ln \left(1+\left(\frac{x-\xi}{\lambda}\right)^{2}\right) - \left(\gamma+\delta  \log \left[\left(\frac{x-\xi}{\lambda}\right) + \sqrt{\left(\frac{x-\xi}{\lambda}\right)^2+1}\right] \right)^{2}\right] \\ = & ? \end{align} Note: $\sinh^{-1} z= \log (z + \sqrt{z^2+1})$ . Is the attempt so far correct? How to simplify and complete the derivation?,The pdf of the Normal distribution is The pdf of the Johnson-SU distribution is The differential entropy of the Normal pdf is derived as What is the derivation of the Johnson-SU pdf's differential entropy ? First Attempt Note: . Is the attempt so far correct? How to simplify and complete the derivation?,"
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^{2}}
 
f(x) = \frac{\delta}{\lambda \sqrt{2 \pi}} \frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}} e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}}
 \begin{align}\label{equation:hN}
h(X) = & -\int_{-\infty}^{\infty} f(x) \ln f(x) \mathrm{d} x\\
=&-\int_{-\infty}^{\infty} \left(2\pi \sigma^2\right)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \ln\left[ \left(2\pi \sigma^2\right)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \right] \mathrm{d} x\\
= &\frac{1}{2} \ln(2\pi \sigma^2) \int_{-\infty}^{\infty} (2\pi \sigma^2)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \mathrm{d} x\\
&+ \frac{1}{2\sigma^2} \left(2\pi \sigma^2\right)^{-\frac{1}{2}} (x-\mu)^2 e^{-(x-\mu)^2 / 2\sigma^2} \mathrm{d} x\\
=& \frac{1}{2} \ln (2\pi\sigma^2) + \frac{1}{2}\\
=& \frac12\ln(2\pi\sigma^2) + \frac12\ln e\\
=& \frac12\left(\ln(2\pi\sigma^2) + \ln e\right)\\
=& \frac{1}{2} \ln (2\pi e \sigma^2)
\end{align} \begin{align}
h(X) = &-\int_{-\infty}^{\infty} f(x) \ln f(x) \mathrm{d} x\\
=& \mathbb{E}[ -\ln f(x)] \\
=& \mathbb{E} \left[ -\ln \left( \frac{\delta}{\lambda \sqrt{2 \pi}} \frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}} e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}} \right) \right] \\
=& \mathbb{E} \left[ -\ln \left( \frac{\delta}{\lambda \sqrt{2 \pi}} \right) -\ln \left(\frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}}\right) - \ln \left( e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}} \right) \right] \\
=& \delta \ln \left( \lambda \sqrt{2 \pi} \right) - \mathbb{E} \left[ -\ln \left(\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}\right) -\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}\right] \\
=& \delta \ln \left( \lambda \sqrt{2 \pi} \right) - \mathbb{E} \left[ -\frac{1}{2} \ln \left(1+\left(\frac{x-\xi}{\lambda}\right)^{2}\right) -\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}\right] \\
=& \delta \ln \left( \lambda \sqrt{2 \pi} \right) -\frac{1}{2} \mathbb{E} \left[ - \ln \left(1+\left(\frac{x-\xi}{\lambda}\right)^{2}\right) - \left(\gamma+\delta  \log \left[\left(\frac{x-\xi}{\lambda}\right) + \sqrt{\left(\frac{x-\xi}{\lambda}\right)^2+1}\right] \right)^{2}\right] \\
= & ?
\end{align} \sinh^{-1} z= \log (z + \sqrt{z^2+1})","['statistics', 'probability-distributions', 'information-theory', 'entropy', 'gaussian']"
86,Why this is an empirical relation?,Why this is an empirical relation?,,"As a high school student I am not sure why this following formula which is used as a relation between mean , median and mode found to hold for unimodal distribution , is called empirical : $$ mean-mode=3(mean-median)$$ is there really no mathematical proof? My text book says that this is completely based on symmetry. Can some one help me to clarify this? Is there any existing proof in higher classes? Thanks in advance. EDIT :Later I found this in cross validated but that's not quite accessible to me I will be happy if I get a simpler version or at least an intuitive understanding.","As a high school student I am not sure why this following formula which is used as a relation between mean , median and mode found to hold for unimodal distribution , is called empirical : is there really no mathematical proof? My text book says that this is completely based on symmetry. Can some one help me to clarify this? Is there any existing proof in higher classes? Thanks in advance. EDIT :Later I found this in cross validated but that's not quite accessible to me I will be happy if I get a simpler version or at least an intuitive understanding.", mean-mode=3(mean-median),"['statistics', 'descriptive-statistics']"
87,A problem about a application of the MLE in $Y_{i}\sim \mathbf{Exp}(\beta)$,A problem about a application of the MLE in,Y_{i}\sim \mathbf{Exp}(\beta),"Suppose that $X_{1}, X_{2}, \ldots, X_{n}$ denote a random sample from an exponentially distributed population with mean $\beta$ . Find the MLE of the population variance $\beta^{2}$ . My approach: I know that Method of Maximum Likelihood: Suppose that the likelihood depends on $k$ parameters $\theta_{1},\theta_{2},\ldots,\theta_{k}$ . Choose those values of the parameters that maximize the likelihood $\mathbb{L}[y_{1},y_{2},\ldots,y_{n}|\theta_{1},\theta_{2}\ldots,\theta_{k}]$ . So, in my problem a need to find the value of the parameter $\beta^{2}$ sucht that maximize the $$\mathbb{L}[x_{1},x_{2},\ldots,x_{n}|\beta^{2}]:=\mathbb{L}[\beta^{2}]$$ But I know that if $t(\theta)$ is a one-to-one function of $\theta$ and if $\hat{\theta}$ is the MLE for $\theta$ , so the MLE of $t(\theta)$ is given by $$\hat{t(\theta)}=t(\hat{\theta})$$ This result, sometimes refered to as the invariance property of MLEs . Now, using that result, I think I can calculate first $$\mathbb{L}(y_{1},y_{2},\ldots,y_{n}|\beta)=f(y_{1}|\beta)\times f(y_{2}|\beta)\times \cdots f(y_{n}|\beta)$$ where $f(y_{i}|\beta)$ is the density functio of the $Y_{i}\sim \mathbf{Exp}(\beta)$ . So, we have \begin{eqnarray*} \mathbb{L}(\beta)&=&\left(\frac{1}{\beta}\right)^{n}\mathbf{exp}\left[\frac{-y_{1}-y_{2}-\cdots -y_{n}}{\beta}\right] \end{eqnarray*} Question: How can I write the support of $\mathbb{L}[\beta]$ ? Also, I know  to do $\frac{d\mathbb{L}[\beta]}{d \beta}=0$ , and so we have $$\frac{d}{d\beta}\left(\left(\frac{1}{\beta}\right)^{n}\mathbf{exp}\left[\frac{-y_{1}-y_{2}-\cdots -y_{n}}{\beta}\right] \right)=0$$ since that $\ln(\mathbb{L}[\beta]$ and $\mathbb{L}[\beta]$ reach the maximum in the same $\beta$ , by monotony, then we can calculate $\beta$ in $$\frac{\ln(\mathbb{L}[\beta])}{d\beta}=0 \implies \frac{1}{\beta^{2}}\sum_{1\leq i \leq n}y_{i}=\frac{n}{\beta} \implies \beta=\frac{1}{n}\sum_{1\leq i \leq n}y_{i}=\overline{Y}_n$$ So, we have that $$\hat{\beta}_{MLE}=\overline{Y}_n$$ Now, by invariance property we can see that $$\hat{\beta}^{2}_{MLE}=\overline{Y}_n^{2}$$ Is correct my solution? Any suggestion?","Suppose that denote a random sample from an exponentially distributed population with mean . Find the MLE of the population variance . My approach: I know that Method of Maximum Likelihood: Suppose that the likelihood depends on parameters . Choose those values of the parameters that maximize the likelihood . So, in my problem a need to find the value of the parameter sucht that maximize the But I know that if is a one-to-one function of and if is the MLE for , so the MLE of is given by This result, sometimes refered to as the invariance property of MLEs . Now, using that result, I think I can calculate first where is the density functio of the . So, we have Question: How can I write the support of ? Also, I know  to do , and so we have since that and reach the maximum in the same , by monotony, then we can calculate in So, we have that Now, by invariance property we can see that Is correct my solution? Any suggestion?","X_{1}, X_{2}, \ldots, X_{n} \beta \beta^{2} k \theta_{1},\theta_{2},\ldots,\theta_{k} \mathbb{L}[y_{1},y_{2},\ldots,y_{n}|\theta_{1},\theta_{2}\ldots,\theta_{k}] \beta^{2} \mathbb{L}[x_{1},x_{2},\ldots,x_{n}|\beta^{2}]:=\mathbb{L}[\beta^{2}] t(\theta) \theta \hat{\theta} \theta t(\theta) \hat{t(\theta)}=t(\hat{\theta}) \mathbb{L}(y_{1},y_{2},\ldots,y_{n}|\beta)=f(y_{1}|\beta)\times f(y_{2}|\beta)\times \cdots f(y_{n}|\beta) f(y_{i}|\beta) Y_{i}\sim \mathbf{Exp}(\beta) \begin{eqnarray*}
\mathbb{L}(\beta)&=&\left(\frac{1}{\beta}\right)^{n}\mathbf{exp}\left[\frac{-y_{1}-y_{2}-\cdots -y_{n}}{\beta}\right]
\end{eqnarray*} \mathbb{L}[\beta] \frac{d\mathbb{L}[\beta]}{d \beta}=0 \frac{d}{d\beta}\left(\left(\frac{1}{\beta}\right)^{n}\mathbf{exp}\left[\frac{-y_{1}-y_{2}-\cdots -y_{n}}{\beta}\right] \right)=0 \ln(\mathbb{L}[\beta] \mathbb{L}[\beta] \beta \beta \frac{\ln(\mathbb{L}[\beta])}{d\beta}=0 \implies \frac{1}{\beta^{2}}\sum_{1\leq i \leq n}y_{i}=\frac{n}{\beta} \implies \beta=\frac{1}{n}\sum_{1\leq i \leq n}y_{i}=\overline{Y}_n \hat{\beta}_{MLE}=\overline{Y}_n \hat{\beta}^{2}_{MLE}=\overline{Y}_n^{2}","['statistics', 'probability-distributions']"
88,"given the length of event $x$, in $90$ occurrences how many times will length length $x$ be greater than $3$?","given the length of event , in  occurrences how many times will length length  be greater than ?",x 90 x 3,"the length of event $x$ has approximately an exponential distribution with a mean of 90 minutes. if event $x$ occurs $90$ times, how many times will the event last longer than $3$ hours? My work so far: here for exponential distribution parameter $β = 90$ $P$ (event occur after 3 hours (180 minutes)): $$P(X>180)=1-P(X<180)=1-(1-\exp(-180/90))=	0.1353$$ therefore the number of events that last longer than $3$ hours $$=np=90*0.1353= 12.177 \sim 12$$","the length of event has approximately an exponential distribution with a mean of 90 minutes. if event occurs times, how many times will the event last longer than hours? My work so far: here for exponential distribution parameter (event occur after 3 hours (180 minutes)): therefore the number of events that last longer than hours",x x 90 3 β = 90 P P(X>180)=1-P(X<180)=1-(1-\exp(-180/90))=	0.1353 3 =np=90*0.1353= 12.177 \sim 12,"['probability', 'statistics']"
89,"Mean, variance, .., higher variances?","Mean, variance, .., higher variances?",,"TL; DR Let $e,v$ be two real numbers. The constant distribution $f \equiv e$ is universal in the sense that it has the most entropy among those that have mean being $e$ . We define variance using this fact. The normal distribution $N(e,v)$ is universal in the sense that it has the most entropy among those that have mean being $e$ and variance being $v$ . We define 2-variance using this fact. What's next? What's the name of the study along this vein? Original post The study of a random variable $X$ is the study of its underlying distribution $P_X$ . However, it's too expensive, so we wish to start simple. To make life even simpler, let's assume all variables are from $\mathbb{R}$ to $\mathbb{R}$ . The simplest yet important information of $X$ is its mean $E(X)$ . Once we know its mean, we can ask further questions.. like how much does $X$ differ from the universal distribution among those with mean $E(X)$ ? Here by universal , I mean the distribution(s) possessing the maximal entropy among those that have the same mean. Of course, it's the constant distribution $f \equiv E(X)$ , taking value at $E(X)$ . Moving forward, we wish to see how much $X$ differs from $E(X)$ . We of course take their difference $|X - E(X)|$ , but decide to consider instead of its square $(X-E(X))^2$ .. because why not? It's both easier to analyze and yet equivalent to the difference. Same as when we thought $X$ contains too much information, so does $(X-E(X))^2$ . Hence, we again consider its mean $E((X-E(X))^2)$ , and coin it to be $Var(X)$ . We wish to move forward, and derive higher versions of variances. I'm aware of usual higher moments $E((X - E(X))^n)$ but that's not what I want. What I want is to keep following the same vein of thought, and consider the difference of $X$ with the universal distribution that has the known attributes. Henceforth, we consider the difference $|X - N(E(X),Var(X))|$ , where $N$ denotes the normal distribution, because the normal distribution is the one that has the most entropy among those that have mean $E(X)$ and variance $Var(X)$ . Let's again consider the square with the same reason as above. And so we define the $2$ -variance $$Var_2(X) := E ( (X-N(E(X),Var(X))^2 ),$$ and so on. Questions What is the universal distribution with mean $E(X)$ , variance $Var(X)$ , and 2-variance $Var_2(X)$ ? Again by universal I mean the one that has the most entropy among those having the same known attributes. What's the standard name for $Var_2, Var_3, \cdots$ ?","TL; DR Let be two real numbers. The constant distribution is universal in the sense that it has the most entropy among those that have mean being . We define variance using this fact. The normal distribution is universal in the sense that it has the most entropy among those that have mean being and variance being . We define 2-variance using this fact. What's next? What's the name of the study along this vein? Original post The study of a random variable is the study of its underlying distribution . However, it's too expensive, so we wish to start simple. To make life even simpler, let's assume all variables are from to . The simplest yet important information of is its mean . Once we know its mean, we can ask further questions.. like how much does differ from the universal distribution among those with mean ? Here by universal , I mean the distribution(s) possessing the maximal entropy among those that have the same mean. Of course, it's the constant distribution , taking value at . Moving forward, we wish to see how much differs from . We of course take their difference , but decide to consider instead of its square .. because why not? It's both easier to analyze and yet equivalent to the difference. Same as when we thought contains too much information, so does . Hence, we again consider its mean , and coin it to be . We wish to move forward, and derive higher versions of variances. I'm aware of usual higher moments but that's not what I want. What I want is to keep following the same vein of thought, and consider the difference of with the universal distribution that has the known attributes. Henceforth, we consider the difference , where denotes the normal distribution, because the normal distribution is the one that has the most entropy among those that have mean and variance . Let's again consider the square with the same reason as above. And so we define the -variance and so on. Questions What is the universal distribution with mean , variance , and 2-variance ? Again by universal I mean the one that has the most entropy among those having the same known attributes. What's the standard name for ?","e,v f \equiv e e N(e,v) e v X P_X \mathbb{R} \mathbb{R} X E(X) X E(X) f \equiv E(X) E(X) X E(X) |X - E(X)| (X-E(X))^2 X (X-E(X))^2 E((X-E(X))^2) Var(X) E((X - E(X))^n) X |X - N(E(X),Var(X))| N E(X) Var(X) 2 Var_2(X) := E ( (X-N(E(X),Var(X))^2 ), E(X) Var(X) Var_2(X) Var_2, Var_3, \cdots","['probability', 'functional-analysis', 'statistics', 'random-variables', 'entropy']"
90,How many times do I need to draw $𝑋$ items from a set on $𝑁$ items to get a probability $𝑃$ that each item is selected at least $𝐾$ times?,How many times do I need to draw  items from a set on  items to get a probability  that each item is selected at least  times?,𝑋 𝑁 𝑃 𝐾,"This and this are related (but slightly different) questions. My question is more general. I have a set of $N$ elements. I want to repeatedly draw $X$ items ( $X \le N$ ) with replacement, i.e. I first choose $X$ items without replacement and then I replace them in the original set. I want each element to be drawn at least $K$ times. So how many times do I have to draw $X$ elements such that I have probability $P$ that each element has been drawn at least $K$ times? Let's do an example. I have a deck of $N=10$ cards, labeled $1, 2, ..., 10$ . At each step I draw $X=3$ cards, I look at them and then I put them back in the deck. How many times do I have to perform such step in order to draw each single card at least $K=5$ times with probability $P=0.95$ ? EDIT : I would be happy also with a solution where $X=1$ .","This and this are related (but slightly different) questions. My question is more general. I have a set of elements. I want to repeatedly draw items ( ) with replacement, i.e. I first choose items without replacement and then I replace them in the original set. I want each element to be drawn at least times. So how many times do I have to draw elements such that I have probability that each element has been drawn at least times? Let's do an example. I have a deck of cards, labeled . At each step I draw cards, I look at them and then I put them back in the deck. How many times do I have to perform such step in order to draw each single card at least times with probability ? EDIT : I would be happy also with a solution where .","N X X \le N X K X P K N=10 1, 2, ..., 10 X=3 K=5 P=0.95 X=1","['probability', 'combinatorics', 'statistics']"
91,Using probability of overlap to calculate number of particles,Using probability of overlap to calculate number of particles,,"I have a setup with multiple synchronized cameras recording an event with an unknown number $N$ particles. Only a subset of the particles are detected in any given camera, we assume an average of $n$ particles are detected in each camera. Of the $n$ particles detected in each camera, there is a certain number of particles $\eta$ that overlap in all cameras. Based on the known values of $n$ and $\eta$ , and assuming a uniformly distributed choice of particles $n$ , it is possible to estimate the value of $N$ with \begin{equation} N=\left(\frac{n^c}{\eta}\right)^{\frac{1}{c-1}}, \end{equation} where c is the number of cameras used. I have derived this expression using very elementary probabilities and checked that it is correct with simple numerical experiments. Derivation Let there be $N$ particles that are divided into group $\alpha$ with $n=rN$ particles, where $r=1/2$ is the find rate of each camera. A second group $\beta$ contains the other $n=rN$ particles. There are $c=3$ cameras imaging the particles and each camera will randomly sample $n=rN$ particles. For now let us assume that these will be either exactly group $\alpha$ or group $\beta$ . The probability of all three cameras imaging $\alpha$ is \begin{equation} P(A)=\frac{1}{2}\frac{1}{2}\frac{1}{2}=\frac{1}{8}  \,, \end{equation} and likewise the probability of all three cameras imaging $\beta$ is \begin{equation} P(B)=\frac{1}{2}\frac{1}{2}\frac{1}{2}=\frac{1}{8}  \,. \end{equation} The probability that all three camera image the same group (either all $\alpha$ or all $\beta$ ) is \begin{equation} P(A \cup B)=P(A)+P(B)= \frac{1}{4}  \,. \end{equation} This means that there is a 1 out of 4 chance that all three cameras will image the same group of particles. To generalize the statement, we remove the initial constraint of grouping particles into $\alpha$ and $\beta$ and allow any combination of particles $n$ in each camera. Now instead of having $r=1/2$ particles intersecting $P(A \cup B)= 1/4$ of the time, we can equivalently expect \begin{equation} R=r\,P(A \cup B)=\frac{1}{2}\frac{1}{4}=\frac{1}{8}\,, \end{equation} of the particles intersecting all the time, where $R=\eta/N$ is the expected number of intersections as a ratio of $N$ . By examining this simple example, I can write a relationship for intersecting sets of random variables \begin{equation} \label{432} R=r^c \,, \end{equation} where $\{r:\, 0\leq r\leq1\}$ is the find rate of each camera and $c$ is the number of cameras. Substituting $R=\eta/N$ and $r=n/N$ into the above equation and solving for $N$ gives \begin{equation} N=\left(\frac{n^c}{\eta}\right)^{\frac{1}{c-1}}. \end{equation} Question I am not very satisfied with the way I derived my final equation. I need help coming up with a more mathematical approach to the derivation. I have tried with a Hypergeometric distribution, but could not extend that to beyond $c=2$ . As a next step I would like to derive a similar expression to solve for $N$ for the case where $n$ is no longer uniformly distributed over $N$ , but rather distributed based on a known weighting function. I think if I get a good grasp on how to derive the original expression for $N$ mathematically, extending this to a weighted distribution should not be too difficult.","I have a setup with multiple synchronized cameras recording an event with an unknown number particles. Only a subset of the particles are detected in any given camera, we assume an average of particles are detected in each camera. Of the particles detected in each camera, there is a certain number of particles that overlap in all cameras. Based on the known values of and , and assuming a uniformly distributed choice of particles , it is possible to estimate the value of with where c is the number of cameras used. I have derived this expression using very elementary probabilities and checked that it is correct with simple numerical experiments. Derivation Let there be particles that are divided into group with particles, where is the find rate of each camera. A second group contains the other particles. There are cameras imaging the particles and each camera will randomly sample particles. For now let us assume that these will be either exactly group or group . The probability of all three cameras imaging is and likewise the probability of all three cameras imaging is The probability that all three camera image the same group (either all or all ) is This means that there is a 1 out of 4 chance that all three cameras will image the same group of particles. To generalize the statement, we remove the initial constraint of grouping particles into and and allow any combination of particles in each camera. Now instead of having particles intersecting of the time, we can equivalently expect of the particles intersecting all the time, where is the expected number of intersections as a ratio of . By examining this simple example, I can write a relationship for intersecting sets of random variables where is the find rate of each camera and is the number of cameras. Substituting and into the above equation and solving for gives Question I am not very satisfied with the way I derived my final equation. I need help coming up with a more mathematical approach to the derivation. I have tried with a Hypergeometric distribution, but could not extend that to beyond . As a next step I would like to derive a similar expression to solve for for the case where is no longer uniformly distributed over , but rather distributed based on a known weighting function. I think if I get a good grasp on how to derive the original expression for mathematically, extending this to a weighted distribution should not be too difficult.","N n n \eta n \eta n N \begin{equation}
N=\left(\frac{n^c}{\eta}\right)^{\frac{1}{c-1}},
\end{equation} N \alpha n=rN r=1/2 \beta n=rN c=3 n=rN \alpha \beta \alpha \begin{equation}
P(A)=\frac{1}{2}\frac{1}{2}\frac{1}{2}=\frac{1}{8}  \,,
\end{equation} \beta \begin{equation}
P(B)=\frac{1}{2}\frac{1}{2}\frac{1}{2}=\frac{1}{8}  \,.
\end{equation} \alpha \beta \begin{equation}
P(A \cup B)=P(A)+P(B)= \frac{1}{4}  \,.
\end{equation} \alpha \beta n r=1/2 P(A \cup B)= 1/4 \begin{equation}
R=r\,P(A \cup B)=\frac{1}{2}\frac{1}{4}=\frac{1}{8}\,,
\end{equation} R=\eta/N N \begin{equation}
\label{432}
R=r^c \,,
\end{equation} \{r:\, 0\leq r\leq1\} c R=\eta/N r=n/N N \begin{equation}
N=\left(\frac{n^c}{\eta}\right)^{\frac{1}{c-1}}.
\end{equation} c=2 N n N N","['probability', 'statistics']"
92,A combinatorics puzzle related to Isserlis' theorem,A combinatorics puzzle related to Isserlis' theorem,,"The puzzle is as follows: $$ \newcommand{\empty}{\phantom{1}} \newcommand{\boxes}[2]{\boxed{#1}\hspace{-0.04cm}\boxed{#2}} $$ At iteration (1) you start out with a pair of boxes, one empty and one with a 1: $$ \boxes{1}{\empty} $$ At iteration $i$ do the following: Fill each empty box, creating a new term for each box you fill Then, add new terms corresponding to all of the terms from iteration $i - 1$ , but with a $\boxes{i}{\empty}$ box tacked onto the end. Examples : Iteration (2) would look like: $$ \boxes{1}{2} + \boxes{1}{\empty}\:\boxes{2}{\empty} $$ where we've added one term for the one empty box, and then tacked on $\boxes{2}{\empty}$ to the term from iteration (1). Iteration (3) would look like: $$ \boxes{1}{3} \boxes{2}{\empty} + \boxes{1}{\empty} \boxes{2}{3} + \boxes{1}{2} \boxes{3}{\empty} + \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{\empty} $$ Here the first two terms come from filling the two empty boxes, and the last two terms come from tacking on a $\boxes{3}{\empty}$ to the end of the previous terms. Iteration (4) would look like: \begin{multline} \boxes{1}{3} \boxes{2}{4}  + \boxes{1}{4} \boxes{2}{3}  + \boxes{1}{2} \boxes{3}{4}  + \boxes{1}{4} \boxes{2}{\empty} \boxes{3}{\empty}  + \boxes{1}{\empty} \boxes{2}{4} \boxes{3}{\empty} \\ + \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{4}  +\boxes{1}{3} \boxes{2}{\empty} \boxes{4}{\empty} + \boxes{1}{\empty} \boxes{2}{3} \boxes{4}{\empty} + \boxes{1}{2} \boxes{3}{\empty} \boxes{4}{\empty} + \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{\empty} \boxes{4}{\empty} \end{multline} Once you have done $n$ iterations get rid of any terms which have an empty box. Here, iterations (1) and (3) would just give nothing, while (2) would give $\boxes{1}{2}$ and (4) would give: $$ \boxes{1}{3} \: \boxes{2}{4}  + \boxes{1}{4} \: \boxes{2}{3}  + \boxes{1}{2} \: \boxes{3}{4} $$ How do you describe what you're left with at the end ? I think this should end up being identical to Isserlis' Theorem , but I'm hoping to come up with a slick combinatorial proof given this formulation. I haven't been successful so far, and all of the proofs that I have found use results from statistics that I am not familiar with.","The puzzle is as follows: At iteration (1) you start out with a pair of boxes, one empty and one with a 1: At iteration do the following: Fill each empty box, creating a new term for each box you fill Then, add new terms corresponding to all of the terms from iteration , but with a box tacked onto the end. Examples : Iteration (2) would look like: where we've added one term for the one empty box, and then tacked on to the term from iteration (1). Iteration (3) would look like: Here the first two terms come from filling the two empty boxes, and the last two terms come from tacking on a to the end of the previous terms. Iteration (4) would look like: Once you have done iterations get rid of any terms which have an empty box. Here, iterations (1) and (3) would just give nothing, while (2) would give and (4) would give: How do you describe what you're left with at the end ? I think this should end up being identical to Isserlis' Theorem , but I'm hoping to come up with a slick combinatorial proof given this formulation. I haven't been successful so far, and all of the proofs that I have found use results from statistics that I am not familiar with.","
\newcommand{\empty}{\phantom{1}}
\newcommand{\boxes}[2]{\boxed{#1}\hspace{-0.04cm}\boxed{#2}}
 
\boxes{1}{\empty}
 i i - 1 \boxes{i}{\empty} 
\boxes{1}{2} + \boxes{1}{\empty}\:\boxes{2}{\empty}
 \boxes{2}{\empty} 
\boxes{1}{3} \boxes{2}{\empty} + \boxes{1}{\empty} \boxes{2}{3} + \boxes{1}{2} \boxes{3}{\empty} + \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{\empty}
 \boxes{3}{\empty} \begin{multline}
\boxes{1}{3} \boxes{2}{4} 
+ \boxes{1}{4} \boxes{2}{3} 
+ \boxes{1}{2} \boxes{3}{4} 
+ \boxes{1}{4} \boxes{2}{\empty} \boxes{3}{\empty} 
+ \boxes{1}{\empty} \boxes{2}{4} \boxes{3}{\empty} \\
+ \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{4} 
+\boxes{1}{3} \boxes{2}{\empty} \boxes{4}{\empty}
+ \boxes{1}{\empty} \boxes{2}{3} \boxes{4}{\empty}
+ \boxes{1}{2} \boxes{3}{\empty} \boxes{4}{\empty}
+ \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{\empty} \boxes{4}{\empty}
\end{multline} n \boxes{1}{2} 
\boxes{1}{3} \: \boxes{2}{4} 
+ \boxes{1}{4} \: \boxes{2}{3} 
+ \boxes{1}{2} \: \boxes{3}{4}
","['combinatorics', 'statistics', 'puzzle', 'means', 'gaussian']"
93,Independent vs Overall Percentage of Turns in Game where Next Player is Probabilistically Determined.,Independent vs Overall Percentage of Turns in Game where Next Player is Probabilistically Determined.,,"I'm writing a program in which there are n ""players"". On each player's turn, the next player is randomly determined with a weight. My goal is to have each player have it be their turn a certain percentage of the time (e.g. Player 1 gets 0.20 of all turns). If there were three players, [1, 2, 3] and I would like each of them to have it be their turn [0.20, 0.50, 0.30] of the total number of turns respectively. When I run this simulation several thousand times and count up the number of turns each player takes, however, I get a distribution of [0.258, 0.403, 0.339] of the turns for each player. I am picking the next player by repeatedly sampling the list of players until I get one that is not the current player. I think this is being complicated by the fact that the next player must not be the player whose turn it currently is, but I am unsure why this. How can I sample the next player and achieve some specific proportion of turns in the long run?","I'm writing a program in which there are n ""players"". On each player's turn, the next player is randomly determined with a weight. My goal is to have each player have it be their turn a certain percentage of the time (e.g. Player 1 gets 0.20 of all turns). If there were three players, [1, 2, 3] and I would like each of them to have it be their turn [0.20, 0.50, 0.30] of the total number of turns respectively. When I run this simulation several thousand times and count up the number of turns each player takes, however, I get a distribution of [0.258, 0.403, 0.339] of the turns for each player. I am picking the next player by repeatedly sampling the list of players until I get one that is not the current player. I think this is being complicated by the fact that the next player must not be the player whose turn it currently is, but I am unsure why this. How can I sample the next player and achieve some specific proportion of turns in the long run?",,"['probability', 'statistics', 'probability-distributions', 'sampling', 'programming']"
94,Determining a proper sample size in binomial distribution,Determining a proper sample size in binomial distribution,,"An agency is holding a poll, where each participant can either support or not support some upcoming motion. We model the answers with i.i.d. Bernoulli variables for which $X = 1$ means to support the motion with probability $P(X = 1) = p$ . We'd like that results of a poll are within $a$ percentage points of the true fraction with $b$ probability and we are trying to determine what number of people we need to interview in order for this poll to be reliable. We know that we are dealing with a binomial distribution and thus $E(X) = pn$ . With Chernoff bounds we get $P(X \geq (1 - d)E(X)) \leq e^{-(E(X)d^2)/2}$ and $P(X \geq (1 + d)E(X)) \leq e^{-(E(X)d^2)/(2 + d)}$ So if we set $p = \frac{m}{n}$ , where $n$ denotes the total number of participants and $m$ denotes the number of participants who support the motion, we have that $E(X) = pn = m$ . Therefore to me this question sounds like finding $m$ for which $P((1-d)E(X) \leq X \leq (1+d)E(X)) = 0.99$ . This ends up being equivalent of $0.01 = P(X \geq (1+d)E(X)) + P(X \geq (1-d)E(X))$ . However if I use the Chernoff bounds and expand this equation I end up in a situation where $0.01 = e^{-(E(X)d^2)/2} + e^{-(E(X)d^2)/(2 + d)}$ and I have no idea how to factor out the $E(X)$ . So is my reasoning correct and/or is there a less painful way of solving for $E(X)$ ?","An agency is holding a poll, where each participant can either support or not support some upcoming motion. We model the answers with i.i.d. Bernoulli variables for which means to support the motion with probability . We'd like that results of a poll are within percentage points of the true fraction with probability and we are trying to determine what number of people we need to interview in order for this poll to be reliable. We know that we are dealing with a binomial distribution and thus . With Chernoff bounds we get and So if we set , where denotes the total number of participants and denotes the number of participants who support the motion, we have that . Therefore to me this question sounds like finding for which . This ends up being equivalent of . However if I use the Chernoff bounds and expand this equation I end up in a situation where and I have no idea how to factor out the . So is my reasoning correct and/or is there a less painful way of solving for ?",X = 1 P(X = 1) = p a b E(X) = pn P(X \geq (1 - d)E(X)) \leq e^{-(E(X)d^2)/2} P(X \geq (1 + d)E(X)) \leq e^{-(E(X)d^2)/(2 + d)} p = \frac{m}{n} n m E(X) = pn = m m P((1-d)E(X) \leq X \leq (1+d)E(X)) = 0.99 0.01 = P(X \geq (1+d)E(X)) + P(X \geq (1-d)E(X)) 0.01 = e^{-(E(X)d^2)/2} + e^{-(E(X)d^2)/(2 + d)} E(X) E(X),['statistics']
95,"Cumulative Distribution Function given mean, std dev, skew and kurtosis?","Cumulative Distribution Function given mean, std dev, skew and kurtosis?",,"Is there a function that calculates the cumulative distribution function (CDF) of a Gaussian distribution given the mean, std dev, skew, and kurtosis?  Does anyone know of one written in a c-like language (c, c++, java, c#)? Thanks","Is there a function that calculates the cumulative distribution function (CDF) of a Gaussian distribution given the mean, std dev, skew, and kurtosis?  Does anyone know of one written in a c-like language (c, c++, java, c#)? Thanks",,"['statistics', 'probability-distributions', 'moment-generating-functions', 'programming', 'cumulative-distribution-functions']"
96,What's this distribution?,What's this distribution?,,"Suppose you have a row of $n$ switches with two states each, on ( $1$ ) or off ( $0$ ), as well as a coin that shows heads with probability $p$ and tails with prob. $q=1-p$ . Initially, all switches are off. You flip the coin, and if you see tails, you randomly toggle one of the switches (when it's on, you turn it off, and vice versa) and then flip the coin again. If you see heads, you stop and count the number of switches that are on. Q: What probability distribution does this number $K$ of activated switches follow? I found this is a very easy to simulate but tricky to formalize probability distribution. My thoughts and insights so far: $K$ obviously has support in $\lbrace 0,\dots,n\rbrace$ . The number of total flips $M$ follows a geometric distribution with parameter $p$ . The probability of switch $i$ being toggled $a_i$ times is the probability of drawing $i$ exactly $a_i$ times out of a uniform distribution over $\lbrace 1,\dots,n\rbrace$ in $M$ tries, so $$P(a_i)=\sum_{m=0}^\infty P(M=m)\cdot{m\choose a_i}\left({1\over n}\right)^{a_i}\left({n-1\over n}\right)^{m-a_i}$$ with $P(M=m)=q^{m}\cdot p$ . Now the probability for $K=k$ is the probability that among all $(a_i)_{i\in [n]}$ exactly $k$ are odd. This is where I don't know how to continue, I haven't been able to come up with a closed form solution to this problem. Some insights: For $p\searrow 0$ , $P(K=k)$ approaches a uniform binomial distribution over $\lbrace 0,\dots,n\rbrace$ (EDIT: Thanks to Henry for the correction) For large $n\gg \mathbb{E}[M]={1\over q}$ , the probability $P(a_i)=1$ approaches $q$ , which is by design, i.e. the process simulates flipping all switches independently with probability $q$ . My questions: Do you, by any chance, recognize this as a known distribution? Do you have an idea for how to find a closed form expression for $P(K=k)$ , if it exists at all? Thanks in advance, best regards! - smuecke","Suppose you have a row of switches with two states each, on ( ) or off ( ), as well as a coin that shows heads with probability and tails with prob. . Initially, all switches are off. You flip the coin, and if you see tails, you randomly toggle one of the switches (when it's on, you turn it off, and vice versa) and then flip the coin again. If you see heads, you stop and count the number of switches that are on. Q: What probability distribution does this number of activated switches follow? I found this is a very easy to simulate but tricky to formalize probability distribution. My thoughts and insights so far: obviously has support in . The number of total flips follows a geometric distribution with parameter . The probability of switch being toggled times is the probability of drawing exactly times out of a uniform distribution over in tries, so with . Now the probability for is the probability that among all exactly are odd. This is where I don't know how to continue, I haven't been able to come up with a closed form solution to this problem. Some insights: For , approaches a uniform binomial distribution over (EDIT: Thanks to Henry for the correction) For large , the probability approaches , which is by design, i.e. the process simulates flipping all switches independently with probability . My questions: Do you, by any chance, recognize this as a known distribution? Do you have an idea for how to find a closed form expression for , if it exists at all? Thanks in advance, best regards! - smuecke","n 1 0 p q=1-p K K \lbrace 0,\dots,n\rbrace M p i a_i i a_i \lbrace 1,\dots,n\rbrace M P(a_i)=\sum_{m=0}^\infty P(M=m)\cdot{m\choose a_i}\left({1\over n}\right)^{a_i}\left({n-1\over n}\right)^{m-a_i} P(M=m)=q^{m}\cdot p K=k (a_i)_{i\in [n]} k p\searrow 0 P(K=k) \lbrace 0,\dots,n\rbrace n\gg \mathbb{E}[M]={1\over q} P(a_i)=1 q q P(K=k)","['probability', 'statistics', 'probability-distributions', 'closed-form', 'simulation']"
97,formal book about Generalized linear models,formal book about Generalized linear models,,"I'm searching for a book who treat GLM in a formal way, with a measure theoretic approach. Someone could help me?","I'm searching for a book who treat GLM in a formal way, with a measure theoretic approach. Someone could help me?",,"['probability-theory', 'measure-theory', 'statistics']"
98,Representation for the survival function of the multinomial distribution in terms of the Dirichlet density?,Representation for the survival function of the multinomial distribution in terms of the Dirichlet density?,,"Let $K_p\sim \text{Binomial}(n,p)$ where $n\in \mathbb{N}$ and $0 \leq p \leq 1$ . Simple computations show that, for $1 \leq k \leq n-1$ and $p_0\in (0,1)$ , \begin{align}     \mathbb{P}(K_{p_0} \geq k)     &= \int_0^{p_0} \frac{d}{d p} \mathbb{P}(K_p \geq k) d p \notag \\[1mm]     &= \int_0^{p_0} \left[\frac{d}{d p} \sum_{i=k}^n \binom{n}{i} p^i (1 - p)^{n - i}\right] d p \notag \\[1mm]     &= \int_0^{p_0} \left[\hspace{-1mm}         \begin{array}{l}             n \sum_{i=k}^n \binom{n-1}{i-1} \, p^{i-1} (1 - p)^{(n-1) - (i-1)} \\[1mm]             - n \sum_{i=k}^{n-1} \binom{n-1}{i} \, p^i (1 - p)^{(n-1) - i}         \end{array}         \hspace{-1mm}\right] d p \notag \\[1mm]     &= \int_0^{p_0} \left[\hspace{-1mm}         \begin{array}{l}             n \sum_{i=k}^n \binom{n-1}{i-1} \, p^{i-1} (1 - p)^{(n-1) - (i-1)} \\[1mm]             - n \sum_{j=k+1}^n \binom{n-1}{j-1} \, p^{j-1} (1 - p)^{(n-1) - (j-1)}         \end{array}         \hspace{-1mm}\right] d p \quad (\text{with } j = i + 1) \notag \\     &= \int_0^{p_0} \left[n \binom{n-1}{k-1} \, p^{k-1} (1 - p)^{(n-1) - (k-1)}\right] d p \notag \\     &= \frac{n!}{(k-1)! (n - k)!} \int_0^{p_0} p^{k-1} (1 - p)^{n-k} d p. \end{align} This turns the survival function of the binomial distribution into an integral over the density of the beta distribution. Is it possible to derive an analogous representation for the survival function of the multinomial distribution in terms of an integral over the density function of the Dirichlet distribution ?","Let where and . Simple computations show that, for and , This turns the survival function of the binomial distribution into an integral over the density of the beta distribution. Is it possible to derive an analogous representation for the survival function of the multinomial distribution in terms of an integral over the density function of the Dirichlet distribution ?","K_p\sim \text{Binomial}(n,p) n\in \mathbb{N} 0 \leq p \leq 1 1 \leq k \leq n-1 p_0\in (0,1) \begin{align}
    \mathbb{P}(K_{p_0} \geq k)
    &= \int_0^{p_0} \frac{d}{d p} \mathbb{P}(K_p \geq k) d p \notag \\[1mm]
    &= \int_0^{p_0} \left[\frac{d}{d p} \sum_{i=k}^n \binom{n}{i} p^i (1 - p)^{n - i}\right] d p \notag \\[1mm]
    &= \int_0^{p_0} \left[\hspace{-1mm}
        \begin{array}{l}
            n \sum_{i=k}^n \binom{n-1}{i-1} \, p^{i-1} (1 - p)^{(n-1) - (i-1)} \\[1mm]
            - n \sum_{i=k}^{n-1} \binom{n-1}{i} \, p^i (1 - p)^{(n-1) - i}
        \end{array}
        \hspace{-1mm}\right] d p \notag \\[1mm]
    &= \int_0^{p_0} \left[\hspace{-1mm}
        \begin{array}{l}
            n \sum_{i=k}^n \binom{n-1}{i-1} \, p^{i-1} (1 - p)^{(n-1) - (i-1)} \\[1mm]
            - n \sum_{j=k+1}^n \binom{n-1}{j-1} \, p^{j-1} (1 - p)^{(n-1) - (j-1)}
        \end{array}
        \hspace{-1mm}\right] d p \quad (\text{with } j = i + 1) \notag \\
    &= \int_0^{p_0} \left[n \binom{n-1}{k-1} \, p^{k-1} (1 - p)^{(n-1) - (k-1)}\right] d p \notag \\
    &= \frac{n!}{(k-1)! (n - k)!} \int_0^{p_0} p^{k-1} (1 - p)^{n-k} d p.
\end{align}","['calculus', 'probability', 'statistics', 'multivariable-calculus']"
99,Can you subtract a line from an area?,Can you subtract a line from an area?,,"I'm trying to find the probability of C, and I learned the probability is the area by the uniform law. The thing is I get a line from C. Did I find the wrong area or is there a way to subtract a line from the area?","I'm trying to find the probability of C, and I learned the probability is the area by the uniform law. The thing is I get a line from C. Did I find the wrong area or is there a way to subtract a line from the area?",,['probability']
