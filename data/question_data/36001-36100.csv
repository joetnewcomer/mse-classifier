,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What's the probability of the max segment is equal to k?,What's the probability of the max segment is equal to k?,,"There are $m$ integer points on a line of length $m-1$ (including the end point of the line). We randomly mark $n$ points of the $m$ points ($n\leq m$), dividing the line into several segments, so each segment have an integer length. What is the probability the max segment's length is equal to $k$?","There are $m$ integer points on a line of length $m-1$ (including the end point of the line). We randomly mark $n$ points of the $m$ points ($n\leq m$), dividing the line into several segments, so each segment have an integer length. What is the probability the max segment's length is equal to $k$?",,['probability']
1,Sequence of events probability proof,Sequence of events probability proof,,"So, here's the problem (1.5.12, Probability and Statistics, Degroot and Schervish 3e) : Let $ A_1, A_2, ... $ be an arbitrary infinite sequence of events, and let $ B_1, B_2, ... $ be another infinite sequence of events defined as follows: $B_1 = A_1, B_2 = A^c_1\cap A_2, B_3 = A^c_1 \cap A^c_2 \cap A_3 ... $  Prove that $Pr\left(\bigcup_{i=1}^{n} A_i\right) = \sum_{i=1}^{n} Pr\left(B_i\right)$ for $n = 1, 2, ... $ I'm not stuck solving it, yet; I just want to make sure I am understanding the problem. I can show that $\bigcup A_i = \bigcup B_i$ and that $B_i$ are all disjoint, which allows me to show that $ Pr(\bigcup B_i) = \sum Pr(B_i) $ for finite $n$ by induction and hence $Pr(\bigcup B_i) = Pr(\bigcup A_i)$ follows? Is there anything wrong with my argument? Thanks for any advice, and this is homework so please don't post a full solution :)","So, here's the problem (1.5.12, Probability and Statistics, Degroot and Schervish 3e) : Let $ A_1, A_2, ... $ be an arbitrary infinite sequence of events, and let $ B_1, B_2, ... $ be another infinite sequence of events defined as follows: $B_1 = A_1, B_2 = A^c_1\cap A_2, B_3 = A^c_1 \cap A^c_2 \cap A_3 ... $  Prove that $Pr\left(\bigcup_{i=1}^{n} A_i\right) = \sum_{i=1}^{n} Pr\left(B_i\right)$ for $n = 1, 2, ... $ I'm not stuck solving it, yet; I just want to make sure I am understanding the problem. I can show that $\bigcup A_i = \bigcup B_i$ and that $B_i$ are all disjoint, which allows me to show that $ Pr(\bigcup B_i) = \sum Pr(B_i) $ for finite $n$ by induction and hence $Pr(\bigcup B_i) = Pr(\bigcup A_i)$ follows? Is there anything wrong with my argument? Thanks for any advice, and this is homework so please don't post a full solution :)",,['probability']
2,Expectation of time integral,Expectation of time integral,,"I've 2 questions: let be $W_s$ a standard Brownian motion: using Ito's formula show that $\left( W_t,\int_0^t W_sds \right)$ has a normal distribution; and calculate $ E\left[e^{W_t}e^{\int_0^t W_sds}   \right] .$ For the first part, i know that $W_t$ and $\int_0^t W_sds$  have normal distribution with mean and variance respectively $(0,t)$ and $(0, t^3/3)$, but i need help with Ito's formula. For the second part i've tried to solve $E\left[e^{W_t}e^{\int_0^t W_sds}   \right]= \iint e^{W_t}e^{\int_0^t W_sds} \;\phi \left( W_t,\int_0^t W_sds \right)\: dW_t \int_0^t W_sds$... Is these the only way? P.S. sorry for my poor english","I've 2 questions: let be $W_s$ a standard Brownian motion: using Ito's formula show that $\left( W_t,\int_0^t W_sds \right)$ has a normal distribution; and calculate $ E\left[e^{W_t}e^{\int_0^t W_sds}   \right] .$ For the first part, i know that $W_t$ and $\int_0^t W_sds$  have normal distribution with mean and variance respectively $(0,t)$ and $(0, t^3/3)$, but i need help with Ito's formula. For the second part i've tried to solve $E\left[e^{W_t}e^{\int_0^t W_sds}   \right]= \iint e^{W_t}e^{\int_0^t W_sds} \;\phi \left( W_t,\int_0^t W_sds \right)\: dW_t \int_0^t W_sds$... Is these the only way? P.S. sorry for my poor english",,['calculus']
3,Counting all possible size 'm' sets of 'N' marbles under intersection and membership constraints,Counting all possible size 'm' sets of 'N' marbles under intersection and membership constraints,,"Consider the case where we have a bag of 'N' unique marbles, and we randomly select all 'S' possible sets of size 'm' from the bag (with replacement) under the following constraints: (1) - Any set of 'm' marbles must have all unique elements, i.e. there must be no duplicate copies of a marble in any given set. (2) - The maximum intersection between any two sets, i.e. the number of unique marbles they have in common, can be of at most size 'k'. Provided (1) & (2), what is 'S'?  Equivalently, what is the number of possible sets of size 'm' obeying the aforementioned constraints?","Consider the case where we have a bag of 'N' unique marbles, and we randomly select all 'S' possible sets of size 'm' from the bag (with replacement) under the following constraints: (1) - Any set of 'm' marbles must have all unique elements, i.e. there must be no duplicate copies of a marble in any given set. (2) - The maximum intersection between any two sets, i.e. the number of unique marbles they have in common, can be of at most size 'k'. Provided (1) & (2), what is 'S'?  Equivalently, what is the number of possible sets of size 'm' obeying the aforementioned constraints?",,"['probability', 'combinatorics']"
4,Generating random values from non-normal and correlated distributions,Generating random values from non-normal and correlated distributions,,"I have a random variable X that is a mixture of a binomial and two normals (see what the probability density function would look like (first chart)) and I have another random variable Y of similar shape but with different values for each normally distributed side. X and Y are also correlated, here's an example of data that could be plausible : X     Y 1.  0    -20 2. -5     2 3. -30    6 4.  7    -2 5.  7     2 As you can see, that was simply to represent that my random variables are either a small positive (often) or a large negative (rare) and have a certain covariance. My problem is : I would like to be able to sample correlated and random values from these two distributions. I could use Cholesky decomposition for generating correlated normally distributed random variables, but the random variables we are talking here are not normal but rather a mixture of a binomial and two normals. Many thanks!","I have a random variable X that is a mixture of a binomial and two normals (see what the probability density function would look like (first chart)) and I have another random variable Y of similar shape but with different values for each normally distributed side. X and Y are also correlated, here's an example of data that could be plausible : X     Y 1.  0    -20 2. -5     2 3. -30    6 4.  7    -2 5.  7     2 As you can see, that was simply to represent that my random variables are either a small positive (often) or a large negative (rare) and have a certain covariance. My problem is : I would like to be able to sample correlated and random values from these two distributions. I could use Cholesky decomposition for generating correlated normally distributed random variables, but the random variables we are talking here are not normal but rather a mixture of a binomial and two normals. Many thanks!",,"['probability', 'statistics', 'probability-theory']"
5,simple random walks on undirected graphs,simple random walks on undirected graphs,,"Consider a simple random walk on a undirected, connected graph. This is the random walk which, at every time step, moves to a random neighbor, with all neighbors being equally likely. Lets assume every node has a self-loop to avoid issues associated with periodicity. Then, the following (surprising to me) fact is true: under the stationary distribution, every edge is traversed with the same probability . I'm trying to get some intuition for why this holds, and I am wondering whether someone  can provide an explanation for this phenomenon. Naturally, I know the standard proof which observes that $\pi_i = d_{i}/\sum_k d_k$ satisfies the equations for the stationary distribution, and so $\pi_i p_{ij} = 1/\sum_k d_k$ whenever the edge $(i,j)$ is present in the graph. This proof, however, feels more like a lucky calculation to me than an explanation. Note that the fact in boldface is false for directed graphs. Its also false for the corresponding two-step chain, i.e. the Markov chain with probability transition matrix $P^2$, where $P$ is the transition matrix of the simple random walk.","Consider a simple random walk on a undirected, connected graph. This is the random walk which, at every time step, moves to a random neighbor, with all neighbors being equally likely. Lets assume every node has a self-loop to avoid issues associated with periodicity. Then, the following (surprising to me) fact is true: under the stationary distribution, every edge is traversed with the same probability . I'm trying to get some intuition for why this holds, and I am wondering whether someone  can provide an explanation for this phenomenon. Naturally, I know the standard proof which observes that $\pi_i = d_{i}/\sum_k d_k$ satisfies the equations for the stationary distribution, and so $\pi_i p_{ij} = 1/\sum_k d_k$ whenever the edge $(i,j)$ is present in the graph. This proof, however, feels more like a lucky calculation to me than an explanation. Note that the fact in boldface is false for directed graphs. Its also false for the corresponding two-step chain, i.e. the Markov chain with probability transition matrix $P^2$, where $P$ is the transition matrix of the simple random walk.",,['probability']
6,Birthday Probability,Birthday Probability,,"In my daughter's class of $23$, three students and the teacher all share the same birthday. Of course, there are $365$ days in the year, and the first case of the shared birthday is not counted in the probability. But is the likelihood of this just $$\frac{1}{365}\times 3 = 0.0082?$$ For my curiosity, leap years can be omitted.","In my daughter's class of $23$, three students and the teacher all share the same birthday. Of course, there are $365$ days in the year, and the first case of the shared birthday is not counted in the probability. But is the likelihood of this just $$\frac{1}{365}\times 3 = 0.0082?$$ For my curiosity, leap years can be omitted.",,"['probability', 'birthday']"
7,The limiting case of a discrete probability problem,The limiting case of a discrete probability problem,,"Say there are three jars, $j_1, j_2, j_3$ filled with different binary sequences of length two. The distribution of the binary sequences in each of the jars is given by the $p_i^k(1-p_i)^{n-k}$, where  $p_i = \frac{i}{m + 1}$ where $m$ is the number of jars, $i$ is the jar index, $k $is number of 1$$'s and $n$ is the length of the string. So for three jars we have $p_1 = 0.25, p_2 = 0.5$, and $p_3 = 0.75$ for $j_1, j_2, j_3$ respectively. Here are the sequences and their probabilities for $j_1$ with $p_1 = 0.25$: \begin{align*} P(00) = 9 / 16 \\ P(10) = 3 / 16 \\   P(01) = 3 / 16 \\   P(11) = 1 / 16. \end{align*} If I tell you that I have selected a binary sequence and the first element is $1$ what is the E($p_i$)? Well, this can be calculated by looking at each of the jars and adding up the probability of candidate sequences times the value of $p_i$. Edit: I wasn't normalizing this conditionally space properly. I'm skipping a step which I'll explain, someone wants. \begin{equation*} E(p_i) = (4/24 * 1/4) + (8/24 * 1/2) + (12/24 * 3/4) = 14 / 24 = 0.58. \end{equation*} So the question is ... what is $E(p_i)$ when the numbers of jars goes to infinity (or alternatively, when $p$ can take on values between $0$ and $1$)? Also what happens when the size of the binary strings goes to infinity? Does it have an effect on the outcome? If it does, does the order we take the limits change the answer? And most importantly what is the general case for when I have $s$ 1's and $r$ $0$'s?, with a continuous $p$ from $0$ to $1$ and infinite sequences?","Say there are three jars, $j_1, j_2, j_3$ filled with different binary sequences of length two. The distribution of the binary sequences in each of the jars is given by the $p_i^k(1-p_i)^{n-k}$, where  $p_i = \frac{i}{m + 1}$ where $m$ is the number of jars, $i$ is the jar index, $k $is number of 1$$'s and $n$ is the length of the string. So for three jars we have $p_1 = 0.25, p_2 = 0.5$, and $p_3 = 0.75$ for $j_1, j_2, j_3$ respectively. Here are the sequences and their probabilities for $j_1$ with $p_1 = 0.25$: \begin{align*} P(00) = 9 / 16 \\ P(10) = 3 / 16 \\   P(01) = 3 / 16 \\   P(11) = 1 / 16. \end{align*} If I tell you that I have selected a binary sequence and the first element is $1$ what is the E($p_i$)? Well, this can be calculated by looking at each of the jars and adding up the probability of candidate sequences times the value of $p_i$. Edit: I wasn't normalizing this conditionally space properly. I'm skipping a step which I'll explain, someone wants. \begin{equation*} E(p_i) = (4/24 * 1/4) + (8/24 * 1/2) + (12/24 * 3/4) = 14 / 24 = 0.58. \end{equation*} So the question is ... what is $E(p_i)$ when the numbers of jars goes to infinity (or alternatively, when $p$ can take on values between $0$ and $1$)? Also what happens when the size of the binary strings goes to infinity? Does it have an effect on the outcome? If it does, does the order we take the limits change the answer? And most importantly what is the general case for when I have $s$ 1's and $r$ $0$'s?, with a continuous $p$ from $0$ to $1$ and infinite sequences?",,"['probability-theory', 'probability']"
8,An unintuitive expected value in a coin toss sequence,An unintuitive expected value in a coin toss sequence,,"Originally I was investigating how to calculate the expected number of flips to obtain $n$ consecutive heads and $m$ consecutive tails. I played with $n=3$ , $m=2$ for a fair coin, and arrive at an expected value that seemed unintuitive. From now on let $n=3$ , $m=2$ , and let $X$ be the number of flips to achieve the goal. For clarity also let $M$ (resp. $N$ ) be the number of flips to obtain $m$ consecutive tails (resp. $n$ ) consecutive heads. Also assume the coin lands on its head with probability $1/2$ . Define $H$ (resp. $T$ ) as the event that the first flip is a head (resp. tail). Conditioning on the first flip, we get $$E[X]=1+\frac{1}{2}(E[X|H]+E[X|T]).$$ We can also condition $E[X|H]$ on the first position of a tail in the next $(n-1)$ flips. Since any tail in the next (n-1) flips resets the state to an initial $T$ , we get $$E[X|H]=\frac{1}{2}\cdot(1+E[X|T])+\left(\frac{1}{2}\right)^2\cdot(2+E[X|T])+\left(\frac{1}{2}\right)^2\cdot(2+E[M]).$$ Let me note that the term $\left(\frac{1}{2}\right)^2\cdot(2+E[M])$ is for the next two flips being heads. From there we only need to obtain $m$ consecutive tails. Similarly, we can condition $E[X|T]$ on the position of the first head in the next $(m-1)$ flips. We have $$E[X|T]=\frac{1}{2}\cdot(1+E[X|H])+\frac{1}{2}\cdot(1+E[N]).$$ Now let me use the fact that $E[M]=6$ and $E[N]=14$ to arrive at \begin{equation*} \begin{split} E[X|H] =& 3+\frac{3}{4}E[X|T],\\ E[X|T] =& 8+\frac{1}{2}E[X|H]. \end{split} \end{equation*} We can then solve $E[X|H]=\frac{72}{5}$ and $E[X|T]=\frac{76}{5}$ , resulting in $$E[X] = \frac{79}{5}.$$ It seems to me a bit surprising that $$E[X|H]<E[X|T],$$ where there are less consecutive tails required. I simply want to check that my calculation was  correct.","Originally I was investigating how to calculate the expected number of flips to obtain consecutive heads and consecutive tails. I played with , for a fair coin, and arrive at an expected value that seemed unintuitive. From now on let , , and let be the number of flips to achieve the goal. For clarity also let (resp. ) be the number of flips to obtain consecutive tails (resp. ) consecutive heads. Also assume the coin lands on its head with probability . Define (resp. ) as the event that the first flip is a head (resp. tail). Conditioning on the first flip, we get We can also condition on the first position of a tail in the next flips. Since any tail in the next (n-1) flips resets the state to an initial , we get Let me note that the term is for the next two flips being heads. From there we only need to obtain consecutive tails. Similarly, we can condition on the position of the first head in the next flips. We have Now let me use the fact that and to arrive at We can then solve and , resulting in It seems to me a bit surprising that where there are less consecutive tails required. I simply want to check that my calculation was  correct.","n m n=3 m=2 n=3 m=2 X M N m n 1/2 H T E[X]=1+\frac{1}{2}(E[X|H]+E[X|T]). E[X|H] (n-1) T E[X|H]=\frac{1}{2}\cdot(1+E[X|T])+\left(\frac{1}{2}\right)^2\cdot(2+E[X|T])+\left(\frac{1}{2}\right)^2\cdot(2+E[M]). \left(\frac{1}{2}\right)^2\cdot(2+E[M]) m E[X|T] (m-1) E[X|T]=\frac{1}{2}\cdot(1+E[X|H])+\frac{1}{2}\cdot(1+E[N]). E[M]=6 E[N]=14 \begin{equation*}
\begin{split}
E[X|H] =& 3+\frac{3}{4}E[X|T],\\
E[X|T] =& 8+\frac{1}{2}E[X|H].
\end{split}
\end{equation*} E[X|H]=\frac{72}{5} E[X|T]=\frac{76}{5} E[X] = \frac{79}{5}. E[X|H]<E[X|T],",['probability']
9,Spinner probability question with repetition (HS),Spinner probability question with repetition (HS),,"I found this problem in a 10th grade math text book in a section about using permutations to solve probability problems. The book presents the answer as $$ \frac{1}{110880} $$ Which is 1 over the number of ways to arrange the 12 colors accounting for repetition. This seems undoubtedly incorrect to me. I suggested to a colleague that the solution is simply: $$ \frac{3}{12} \times \frac{5}{12} \times \frac{3}{12} \times \frac{1}{12} \approx 0.002$$ They suggested that that this doesn't account for order because you could rearrange them and get the same answer. I'm still not convinced that this matters. Similarly to how fundamental counting principle would result in the same answer no matter the order you arranged the numbers. Either way I'm curious if there is a way to solve this using permutations. Doesn't seem like it to me, I don't ever recall learning how to account for repetitions when you're restricted, i.e. spinning four times in this case, instead of twelve times. And the book answer is definitely wrong right? Thanks in advance for everyone's help.","I found this problem in a 10th grade math text book in a section about using permutations to solve probability problems. The book presents the answer as Which is 1 over the number of ways to arrange the 12 colors accounting for repetition. This seems undoubtedly incorrect to me. I suggested to a colleague that the solution is simply: They suggested that that this doesn't account for order because you could rearrange them and get the same answer. I'm still not convinced that this matters. Similarly to how fundamental counting principle would result in the same answer no matter the order you arranged the numbers. Either way I'm curious if there is a way to solve this using permutations. Doesn't seem like it to me, I don't ever recall learning how to account for repetitions when you're restricted, i.e. spinning four times in this case, instead of twelve times. And the book answer is definitely wrong right? Thanks in advance for everyone's help.", \frac{1}{110880}   \frac{3}{12} \times \frac{5}{12} \times \frac{3}{12} \times \frac{1}{12} \approx 0.002,"['probability', 'solution-verification', 'permutations', 'problem-solving']"
10,Expressing a continuous local martingale as an integral against a Brownian motion,Expressing a continuous local martingale as an integral against a Brownian motion,,"I'm interested in the following problem. Suppose $X$ , $X_0=0$ is a continuous local martingale with quadratic variation $$ [X]_t = \int_0^t A_s\mathrm{d}s $$ for a non-negative previsible process $(A_t)_{t\ge 0}$ . Show that there exists a Brownian motion $B$ such that $$X_t = \int_0^t A_s^{1/2} \mathrm{d}B_s.$$ The solution I've seen involves defining $$B_t = \int_0^t A_s^{-1/2} 1_{A_s>0} \mathrm{d}X_s+\int_0^t 1_{A_s=0} \mathrm{d}W_s$$ where $W$ is a Brownian motion independent of $X$ . It is not hard to check that $B$ is a Brownian motion by the Levy characterisation, but I'm confused as to how we can conclude $A_t^{1/2} \mathrm{d}B_t = \mathrm{d}X_t$ ? It is immediate that $A_t^{1/2}\mathrm{d}B_t = 1_{A_t>0} \mathrm{d}X_t$ but why is this enough to conclude? Intuitively it may have something to do with continuous local martingales being constant on intervals where their quadratic variation is but I can't see how to justify this rigorously.","I'm interested in the following problem. Suppose , is a continuous local martingale with quadratic variation for a non-negative previsible process . Show that there exists a Brownian motion such that The solution I've seen involves defining where is a Brownian motion independent of . It is not hard to check that is a Brownian motion by the Levy characterisation, but I'm confused as to how we can conclude ? It is immediate that but why is this enough to conclude? Intuitively it may have something to do with continuous local martingales being constant on intervals where their quadratic variation is but I can't see how to justify this rigorously.",X X_0=0  [X]_t = \int_0^t A_s\mathrm{d}s  (A_t)_{t\ge 0} B X_t = \int_0^t A_s^{1/2} \mathrm{d}B_s. B_t = \int_0^t A_s^{-1/2} 1_{A_s>0} \mathrm{d}X_s+\int_0^t 1_{A_s=0} \mathrm{d}W_s W X B A_t^{1/2} \mathrm{d}B_t = \mathrm{d}X_t A_t^{1/2}\mathrm{d}B_t = 1_{A_t>0} \mathrm{d}X_t,"['probability', 'probability-theory', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals']"
11,"Expected value of stopping time of non symmetric random walk$E[\tau_{a,b}]$ is finite",Expected value of stopping time of non symmetric random walk is finite,"E[\tau_{a,b}]","Suppose we have a random walk $S_n$ that increases by $1$ with probability $p \ne \frac{1}{2}$ and decreases by $1$ with probability $1-p$ . And let $a,b \in \mathbb{N}$ . How can I show that the expected value of $\tau_{a,b} := \inf \{n \ge 0: S_n \in \{-a,b\}\}$ ist finite. In lecture it was stated that this is easy to see, however I do not know where to start proving this. Any help would be appreciated.","Suppose we have a random walk that increases by with probability and decreases by with probability . And let . How can I show that the expected value of ist finite. In lecture it was stated that this is easy to see, however I do not know where to start proving this. Any help would be appreciated.","S_n 1 p \ne \frac{1}{2} 1 1-p a,b \in \mathbb{N} \tau_{a,b} := \inf \{n \ge 0: S_n \in \{-a,b\}\}","['probability', 'probability-theory', 'stopping-times']"
12,The need for independence in random sums when using law of total expectation,The need for independence in random sums when using law of total expectation,,"Let $(X_n : n\in \mathbb{N})$ be a sequence of i.i.d. random variables with mean $\mu$ and variance $\sigma^2$ . Let $S_0 = 0$ and $S_n = \sum _{i=1}^{n}X_i$ for $n \geq 1$ . Let $N$ be a non-negative integer-valued random variable. The question I am working on asks to assume further that $N$ is independent of the random variables $X_i$ , and to show that $\mathbb{E}(S_N)=\mu\mathbb{E}(N) $ . I know how to show this in the standard way using total expectation, by conditioning on $N$ : \begin{align*}\mathbb{E}(S_N)&=\sum_{n=0}^\infty \mathbb{E}(S_N|N=n)\mathbb{P}(N=n)\\ &=\sum_{n=0}^\infty \mathbb{E}(S_n)\mathbb{P}(N=n)\\ &=\sum_{n=0}^\infty n\mu \mathbb{P}(N=n) \\ &=  \mu\mathbb{E}(N). \end{align*} My question is the following: Do we need $N$ to be independent of the random variables $X_i$ for the above argument to follow? I do not see where I used the independence of $N$ in the above working. On trying to find an answer to this, I found the following document concerning the proof of Wald's equation. In remark 1.2 on page 4, it talks about the case where the stopping time is independent as a special case, so it seems that independence is indeed important. I would like to understand why independence is needed.","Let be a sequence of i.i.d. random variables with mean and variance . Let and for . Let be a non-negative integer-valued random variable. The question I am working on asks to assume further that is independent of the random variables , and to show that . I know how to show this in the standard way using total expectation, by conditioning on : My question is the following: Do we need to be independent of the random variables for the above argument to follow? I do not see where I used the independence of in the above working. On trying to find an answer to this, I found the following document concerning the proof of Wald's equation. In remark 1.2 on page 4, it talks about the case where the stopping time is independent as a special case, so it seems that independence is indeed important. I would like to understand why independence is needed.","(X_n : n\in \mathbb{N}) \mu \sigma^2 S_0 = 0 S_n = \sum _{i=1}^{n}X_i n \geq 1 N N X_i \mathbb{E}(S_N)=\mu\mathbb{E}(N)  N \begin{align*}\mathbb{E}(S_N)&=\sum_{n=0}^\infty \mathbb{E}(S_N|N=n)\mathbb{P}(N=n)\\
&=\sum_{n=0}^\infty \mathbb{E}(S_n)\mathbb{P}(N=n)\\
&=\sum_{n=0}^\infty n\mu \mathbb{P}(N=n) \\
&=  \mu\mathbb{E}(N). \end{align*} N X_i N","['probability', 'stochastic-processes']"
13,Random walk where Increments have exponential distribution. Probability of never reaching a negative value after $n$ steps.,Random walk where Increments have exponential distribution. Probability of never reaching a negative value after  steps.,n,"Consider the random walk $S_n = \sum_{i=1}^n (X_i-1)$ where $X_i$ are i.i.d. with exponential distribution and mean $1$ , i.e. $P(X_i \leq x) = 1-e^{-x}$ . I am trying to figure out the probability $p_n$ , that the random walk has never reached a negative value after $n$ steps. I have obtained the exact values for the first small $n$ : $p_1 = e^{-1}$ $p_2 = 2e^{-2}$ $p_3 = \frac{9}{2}e^{-3}$ $p_4 = \frac{64}{6}e^{-4}$ I have noticed the pattern, that apparently $p_n = \frac{n^{n-1}}{(n-1)!}e^{-n}$ . I have also done some numeric simulations that seem to confirm that this is in fact the answer. However, I have been unable to prove this. I have tried using the law of total probability, but there I then need to determine the probability that a random walk like above never reaches a negative value if it starts at a specific (positive) value, which I don't know how to compute. If the exact value is too difficult to prove, I would also be content with an upper bound that also converges to 0 for large $n$ . Any insight is greatly appreciated.","Consider the random walk where are i.i.d. with exponential distribution and mean , i.e. . I am trying to figure out the probability , that the random walk has never reached a negative value after steps. I have obtained the exact values for the first small : I have noticed the pattern, that apparently . I have also done some numeric simulations that seem to confirm that this is in fact the answer. However, I have been unable to prove this. I have tried using the law of total probability, but there I then need to determine the probability that a random walk like above never reaches a negative value if it starts at a specific (positive) value, which I don't know how to compute. If the exact value is too difficult to prove, I would also be content with an upper bound that also converges to 0 for large . Any insight is greatly appreciated.",S_n = \sum_{i=1}^n (X_i-1) X_i 1 P(X_i \leq x) = 1-e^{-x} p_n n n p_1 = e^{-1} p_2 = 2e^{-2} p_3 = \frac{9}{2}e^{-3} p_4 = \frac{64}{6}e^{-4} p_n = \frac{n^{n-1}}{(n-1)!}e^{-n} n,"['probability', 'random-walk', 'exponential-distribution']"
14,Maximizing log-likelihood: Determinining whether critical points are maximums,Maximizing log-likelihood: Determinining whether critical points are maximums,,"Consider the following proof of the fact that $\bar{X}$ , the sample mean, is the MLE of parameter $\lambda$ in a Poisson distribution. Let $x_1, \ldots, x_n$ be the observations of $X_1, \ldots, X_n$ with $X_i \sim \mathcal{P}(\lambda)$ . Then we want to solve \begin{align*}     \text{argmax}_{\lambda} \mathcal{L}(\lambda \mid x_1, \ldots, x_n) \end{align*} Observe that \begin{align*}     P(x_1, \ldots, x_n \mid \lambda) &= \prod_{i=1}^{n}  \frac{\lambda^{x_i}     e^{-\lambda}}{x_i!} \end{align*} To maximize this with respect to $\lambda$ , we observe that \begin{align*}     \frac{d}{d\lambda} \ln \left[  \prod_{i=1}^{n}  \frac{\lambda^x_i     e^{-\lambda}}{x_i!}\right] &= \frac{d}{d\lambda} \sum_{i=1}^{n} \left(\ln \frac{\lambda^{x_i}e^{-\lambda}}{x_i !}\right) \\  &= \frac{1}{\lambda} \sum_{i=1}^{n} x_i - n \end{align*} Then, if we let $S := \sum_{i=1}^{n} x_i$ , \begin{align*}     \frac{S}{\lambda} - n &= 0 \iff \lambda = \frac{S}{n} \end{align*} where obviously $S/n = \bar{X}$ . My question regards the claim that setting the derivative to zero finds the maximizing $\lambda$ . That the log-likelihood has a critical point at $\lambda = \bar{X}$ is proven; however, what guarantees that this critical point is not a minimum? I am aware that the second derivative test answers this question, because it is negative at $\lambda = \bar{X}$ . But the book this comes from is entirely on probability theory and I presume that it's using some fact about the Poisson distribution to implicitly conclude that the point is a maximum. In short, can one conclude that $\lambda = S/n$ is a maximizing point instead of a minimizing point from the nature of the distribution at hand?","Consider the following proof of the fact that , the sample mean, is the MLE of parameter in a Poisson distribution. Let be the observations of with . Then we want to solve Observe that To maximize this with respect to , we observe that Then, if we let , where obviously . My question regards the claim that setting the derivative to zero finds the maximizing . That the log-likelihood has a critical point at is proven; however, what guarantees that this critical point is not a minimum? I am aware that the second derivative test answers this question, because it is negative at . But the book this comes from is entirely on probability theory and I presume that it's using some fact about the Poisson distribution to implicitly conclude that the point is a maximum. In short, can one conclude that is a maximizing point instead of a minimizing point from the nature of the distribution at hand?","\bar{X} \lambda x_1, \ldots, x_n X_1, \ldots, X_n X_i \sim
\mathcal{P}(\lambda) \begin{align*}
    \text{argmax}_{\lambda} \mathcal{L}(\lambda \mid x_1, \ldots, x_n)
\end{align*} \begin{align*}
    P(x_1, \ldots, x_n \mid \lambda) &= \prod_{i=1}^{n}  \frac{\lambda^{x_i}
    e^{-\lambda}}{x_i!}
\end{align*} \lambda \begin{align*}
    \frac{d}{d\lambda} \ln \left[  \prod_{i=1}^{n}  \frac{\lambda^x_i
    e^{-\lambda}}{x_i!}\right] &= \frac{d}{d\lambda} \sum_{i=1}^{n} \left(\ln
\frac{\lambda^{x_i}e^{-\lambda}}{x_i !}\right) \\ 
&= \frac{1}{\lambda} \sum_{i=1}^{n} x_i - n
\end{align*} S := \sum_{i=1}^{n} x_i \begin{align*}
    \frac{S}{\lambda} - n &= 0 \iff \lambda = \frac{S}{n}
\end{align*} S/n = \bar{X} \lambda \lambda = \bar{X} \lambda = \bar{X} \lambda = S/n","['probability', 'probability-theory', 'probability-distributions', 'parameter-estimation', 'maximum-likelihood']"
15,Characteristic function of product of two random variables with arbitrary normal distributions,Characteristic function of product of two random variables with arbitrary normal distributions,,"I have $X\sim N(0,5)$ and $Y\sim N(1,1)$ components of a gaussian random vector. The covariance of $X$ and $Y$ is 2. I've already proved that $\frac{X}{2}-Y$ is independent from $Y$ . I have to calculate the characteristic function of $\left(\frac{X}{2}-Y\right)\frac{Y}{2}$ . My attempt is the following. $$\begin{align}\varphi_{\left(\frac{X}{2}-Y\right)\frac{Y}{2}}(\theta)&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} e^{i\theta\left(\frac{x}{2}-y\right)\frac{y}{2}}f_{X,Y}(x,y) dxdy\\&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{i\theta \frac{xy}{4}}e^{-i\theta\frac{y^2}{2}}f_X(x)f_Y(y)dxdy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}f_Y(y)\left(\int_{-\infty}^{+\infty}e^{i\theta \frac{xy}{4}}f_X(x)dx \right)dy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}f_Y(y)\varphi_{X}\left(\frac{\theta y}{4}\right)dy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}e^{-\frac{1}{2}\left(\frac{5y^2}{16}\theta^2\right)}f_Y(y)dy\end{align}$$ Now, I'm stuck. How can I rewrite that last integral? Maybe I can view it as a Gaussian characteristic function...","I have and components of a gaussian random vector. The covariance of and is 2. I've already proved that is independent from . I have to calculate the characteristic function of . My attempt is the following. Now, I'm stuck. How can I rewrite that last integral? Maybe I can view it as a Gaussian characteristic function...","X\sim N(0,5) Y\sim N(1,1) X Y \frac{X}{2}-Y Y \left(\frac{X}{2}-Y\right)\frac{Y}{2} \begin{align}\varphi_{\left(\frac{X}{2}-Y\right)\frac{Y}{2}}(\theta)&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} e^{i\theta\left(\frac{x}{2}-y\right)\frac{y}{2}}f_{X,Y}(x,y) dxdy\\&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{i\theta \frac{xy}{4}}e^{-i\theta\frac{y^2}{2}}f_X(x)f_Y(y)dxdy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}f_Y(y)\left(\int_{-\infty}^{+\infty}e^{i\theta \frac{xy}{4}}f_X(x)dx \right)dy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}f_Y(y)\varphi_{X}\left(\frac{\theta y}{4}\right)dy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}e^{-\frac{1}{2}\left(\frac{5y^2}{16}\theta^2\right)}f_Y(y)dy\end{align}","['probability', 'normal-distribution', 'characteristic-functions']"
16,What is the probability to get a value of 11 or 12 in 10 rolls or less?,What is the probability to get a value of 11 or 12 in 10 rolls or less?,,"Suppose I have a 3-sided die with each side contains different number of 1, 2, and 3. In this problem, each time I roll the die I will add up the number that shows up with previous rolls. The target number I need is either 11 or 12. Once it reaches either of those number I won't roll the die again. What is the probability to get a value of 11 or 12 in 10 rolls or less? I tried to calculate myself but the result is more than 100%. Sum of 11 or 12 in 4 rolls: 0,061728395 Sum of 11 or 12 in 5 rolls: 0,308641975 Sum of 11 or 12 in 6 rolls: 0,366255144 Sum of 11 or 12 in 7 rolls: 0,195244627 Sum of 11 or 12 in 8 rolls: 0,057613169 Sum of 11 or 12 in 9 rolls: 0,010211858 Sum of 11 or 12 in 10 rolls: 0,001100781 Total: 1,000795949","Suppose I have a 3-sided die with each side contains different number of 1, 2, and 3. In this problem, each time I roll the die I will add up the number that shows up with previous rolls. The target number I need is either 11 or 12. Once it reaches either of those number I won't roll the die again. What is the probability to get a value of 11 or 12 in 10 rolls or less? I tried to calculate myself but the result is more than 100%. Sum of 11 or 12 in 4 rolls: 0,061728395 Sum of 11 or 12 in 5 rolls: 0,308641975 Sum of 11 or 12 in 6 rolls: 0,366255144 Sum of 11 or 12 in 7 rolls: 0,195244627 Sum of 11 or 12 in 8 rolls: 0,057613169 Sum of 11 or 12 in 9 rolls: 0,010211858 Sum of 11 or 12 in 10 rolls: 0,001100781 Total: 1,000795949",,"['probability', 'combinatorics', 'dice']"
17,Probability that $n$ white balls are drawn before $m$ black balls,Probability that  white balls are drawn before  black balls,n m,"Question: Balls are randomly withdrawn, one at a time without replacement, from an urn that initially has N white and M black balls. Find the probability that n white balls are drawn before m black balls, $n\leq N ,m \leq M$ . The answer: Now I have a a lot of issues with this answer and I will try to summarize. First: I don't understand what the $m+n-1$ should represent. Second: The answer says $m$ or less; shouldn't it be just less than $m$ ? since if were to have at least $n$ balls then the largest number of black balls we are to have is simply $m-1$ . Third: Overall as you've probably already noticed, I simply do not understand this solution whatsoever, for instance, why did we have to consider at least $n$ balls; shouldn't we just care about getting exactly $n$ balls before $m$ ? Fourth: Following up my third point I propose another solution where the experiment ends once we withdraw our $n^{th}$ ball before having withdrawn our $m^{th}$ ball; this can be done in a number of ways: $n$ white; $0$ black $n$ white; $1$ black; and so on until: $n$ white; $m-1$ black so the solution in this case should be: $\sum\limits_{i=0}^{m-1} \frac{ \binom{N}{n} \binom{M}{i}}{\binom{M+N}{n+i}}$ Now obviously if you were to substitute with values for $n, m, N, M$ ; the results will be different. What I want is to understand the author's answer first; then understand why my answer is incorrect. Thanks in advance!","Question: Balls are randomly withdrawn, one at a time without replacement, from an urn that initially has N white and M black balls. Find the probability that n white balls are drawn before m black balls, . The answer: Now I have a a lot of issues with this answer and I will try to summarize. First: I don't understand what the should represent. Second: The answer says or less; shouldn't it be just less than ? since if were to have at least balls then the largest number of black balls we are to have is simply . Third: Overall as you've probably already noticed, I simply do not understand this solution whatsoever, for instance, why did we have to consider at least balls; shouldn't we just care about getting exactly balls before ? Fourth: Following up my third point I propose another solution where the experiment ends once we withdraw our ball before having withdrawn our ball; this can be done in a number of ways: white; black white; black; and so on until: white; black so the solution in this case should be: Now obviously if you were to substitute with values for ; the results will be different. What I want is to understand the author's answer first; then understand why my answer is incorrect. Thanks in advance!","n\leq N ,m \leq M m+n-1 m m n m-1 n n m n^{th} m^{th} n 0 n 1 n m-1 \sum\limits_{i=0}^{m-1} \frac{ \binom{N}{n} \binom{M}{i}}{\binom{M+N}{n+i}} n, m, N, M","['probability', 'combinatorics']"
18,Independence is preserved by joint weak convergence,Independence is preserved by joint weak convergence,,"Suppose a sequence of random vectors $(X_n,Y_n)$ converges jointly to some $(X,Y)$ in the weak topology. Question: If $X_n$ and $Y_n$ are independent for all $n$ , are also $X$ and $Y$ independent? This question has been answered in 1 under some hypothesis on the state space. However, since weak convergence only concerns the laws, and since independence can be inferred from the joint law, I feel like it should hold without any hypothesis on the state space. The following seems to establish the answer if the random variables take values in separable metric spaces with their Borel $\sigma$ -algebra, which we denote $(\mathcal{S}_1,\mathcal{B}_1)$ and $(\mathcal{S}_2,\mathcal{B}_2)$ . We remark that the Borel $\sigma$ -algebra of $\mathcal{S}_1 \times \mathcal{S}_2$ is generated by the Cartesian product of Borel sets, by separability. Let $\mu_n$ be the law of $(X_n,Y_n)$ and let $\mu_n^1$ , $\mu_n^2$ be the marginals. Denote their weak limits by $\mu$ and $\mu^1$ , $\mu^2$ . Consider $$ \mathcal{C} = \{ A \times B \colon A \in \mathcal{B}_1 , B \in \mathcal{B}_2, \mu^1(\partial A) = 0 = \mu^2(\partial B) \}. $$ Note that $$ \mu(\partial (A \times B)) \leq \mu(\partial A \times B) + \mu(A \times \partial B) \leq \mu^1(\partial A) + \mu^2(\partial B)  = 0 $$ for $A \times B \in \mathcal{C}$ . Thus, sets in $\mathcal{C}$ are sets of continuity for the measures $\mu^1$ , $\mu^2$ and $\mu$ , so by the Portmanteau theorem \begin{equation} \begin{split} \mathbb{P}( X \in A, Y \in B ) &= \mathbb{P}( (X,Y) \in A \times B ) = \lim_n \mathbb{P}( (X_n,Y_n) \in A \times B ) \\ &= \lim_n \mathbb{P}( X_n \in A ) \mathbb{P}( Y_n \in B ) = \mathbb{P}( X \in A ) \mathbb{P}( Y \in B ). \end{split} \end{equation} Thus, the probability measure $\mu$ and $\mu_1 \otimes \mu_2$ coincide on $\mathcal{C}$ and, hence, on the $\sigma$ -algebra generated by $\mathcal{C}$ since $\mathcal{C}$ is closed under finite intersections. Finally, it was shown in 2 that the $\sigma$ -algebra generated by all sets of continuity is the entire Borel $\sigma$ -algebra for separable metric spaces. Is this proof correct? Can it be extended to more general settings or is there another proof that covers more general settings? Cheers","Suppose a sequence of random vectors converges jointly to some in the weak topology. Question: If and are independent for all , are also and independent? This question has been answered in 1 under some hypothesis on the state space. However, since weak convergence only concerns the laws, and since independence can be inferred from the joint law, I feel like it should hold without any hypothesis on the state space. The following seems to establish the answer if the random variables take values in separable metric spaces with their Borel -algebra, which we denote and . We remark that the Borel -algebra of is generated by the Cartesian product of Borel sets, by separability. Let be the law of and let , be the marginals. Denote their weak limits by and , . Consider Note that for . Thus, sets in are sets of continuity for the measures , and , so by the Portmanteau theorem Thus, the probability measure and coincide on and, hence, on the -algebra generated by since is closed under finite intersections. Finally, it was shown in 2 that the -algebra generated by all sets of continuity is the entire Borel -algebra for separable metric spaces. Is this proof correct? Can it be extended to more general settings or is there another proof that covers more general settings? Cheers","(X_n,Y_n) (X,Y) X_n Y_n n X Y \sigma (\mathcal{S}_1,\mathcal{B}_1) (\mathcal{S}_2,\mathcal{B}_2) \sigma \mathcal{S}_1 \times \mathcal{S}_2 \mu_n (X_n,Y_n) \mu_n^1 \mu_n^2 \mu \mu^1 \mu^2 
\mathcal{C} = \{ A \times B \colon A \in \mathcal{B}_1 , B \in \mathcal{B}_2, \mu^1(\partial A) = 0 = \mu^2(\partial B) \}.
 
\mu(\partial (A \times B)) \leq \mu(\partial A \times B) + \mu(A \times \partial B) \leq \mu^1(\partial A) + \mu^2(\partial B)  = 0
 A \times B \in \mathcal{C} \mathcal{C} \mu^1 \mu^2 \mu \begin{equation}
\begin{split}
\mathbb{P}( X \in A, Y \in B )
&= \mathbb{P}( (X,Y) \in A \times B )
= \lim_n \mathbb{P}( (X_n,Y_n) \in A \times B )
\\
&= \lim_n \mathbb{P}( X_n \in A ) \mathbb{P}( Y_n \in B )
= \mathbb{P}( X \in A ) \mathbb{P}( Y \in B ).
\end{split}
\end{equation} \mu \mu_1 \otimes \mu_2 \mathcal{C} \sigma \mathcal{C} \mathcal{C} \sigma \sigma","['probability', 'probability-theory']"
19,A model inspired by Plants vs. Zombies: What is the expected value of damage dealt by a Kernel-Pult when a zombie walks one step?,A model inspired by Plants vs. Zombies: What is the expected value of damage dealt by a Kernel-Pult when a zombie walks one step?,,"Here is an interesting model inspired by the game Plants vs. Zombies. A Kernel-Pult can lob either a kernel (with probability $1 - p$ , dealing $1$ damage) or a butter (with probability $p$ , dealing $2$ damage with stunning effect) to a zombie. (The damage is counted relative to a peashooter.) Suppose that an invincible zombie is walking in front of him (so the zombie will not die prematurely). A zombie will walk one ""step"" during the interval between firing two projectiles if not stunned. If the zombie is hit by a butter, then he will be unable to walk for an interval of $t$ steps (assuming $t$ is a positive integer for simplicity). If he is hit by another butter when he is already stunned, the number of unable-to-walk-steps will be reset to $t$ . The question is: What is the expected value of damage taken by the zombie within one step? I think the possible values of damage taken within one step ranges from $1$ (one kernel) to $\infty$ (consecutive butters as much as possible). However, I can't derive the distribution behind it. (PS: In the real-world PVZ game, $p = 0.25$ , and the time interval between two projectiles is slightly randomized.)","Here is an interesting model inspired by the game Plants vs. Zombies. A Kernel-Pult can lob either a kernel (with probability , dealing damage) or a butter (with probability , dealing damage with stunning effect) to a zombie. (The damage is counted relative to a peashooter.) Suppose that an invincible zombie is walking in front of him (so the zombie will not die prematurely). A zombie will walk one ""step"" during the interval between firing two projectiles if not stunned. If the zombie is hit by a butter, then he will be unable to walk for an interval of steps (assuming is a positive integer for simplicity). If he is hit by another butter when he is already stunned, the number of unable-to-walk-steps will be reset to . The question is: What is the expected value of damage taken by the zombie within one step? I think the possible values of damage taken within one step ranges from (one kernel) to (consecutive butters as much as possible). However, I can't derive the distribution behind it. (PS: In the real-world PVZ game, , and the time interval between two projectiles is slightly randomized.)",1 - p 1 p 2 t t t 1 \infty p = 0.25,['probability']
20,"$\mathbf{P}(S_{n(i)}=i \mid S_1=i),\mathbf{P}(S_{n(i)+1}=i\mid S_1=i),......$ are zeros?",are zeros?,"\mathbf{P}(S_{n(i)}=i \mid S_1=i),\mathbf{P}(S_{n(i)+1}=i\mid S_1=i),......","$\left\{\xi_{n}\right\}_{n\in\ \mathbb{N}_{+}}$ is a sequence of independently and identically distributed random variables, each taking a finite number of integer values. $\mathbf{E}(\xi_1)\ne 0,$ For any $n\in \mathbb{N}_{+},$ define $S_{n}:=\sum_{i=1}^{n}\xi_{i}.$ Show that $\left\{S_{n}\right\}_{n\in\ \mathbb{N}_{+}}$ is a Markov chain, with each state being transient. My question is how to prove whose each state is transient.I attempt to demonstrate $\sum_{n=1}^{\infty}\mathbf{P}(S_{n}=i\mid S_1=i)<\infty $ for each $i$ in the state space $S$ , by Kolmogorov's strong law of large numbers and  Borel–Cantelli lemma. Maybe we can use this equivalence $$\frac{S_n}{n} \xrightarrow[]{a.s.}\mathbf{E}(\xi_1)\Longleftrightarrow \displaystyle \lim_{ n\to \infty}\mathbf{P}\left(\bigcup_{k=n}^{\infty}\left\{\left|\frac{S_k}{k} -\mathbf{E}(\xi_1)\right|\ge\varepsilon\right\}\right)=0, \forall \varepsilon>0.$$ to create a paradox to support that starting from some positive integer $n(i)$ , all subsequent terms in $\sum_{n=1}^{\infty}\mathbf{P}(S_{n}=i\mid S_1=i)$ i.e. $\mathbf{P}(S_{n(i)}=i\mid S_1=i),\mathbf{P}(S_{{n(i)+1}}=i\mid S_1=i),......$ are zeros. $\left\{S_{n}\right\}_{n\in\ \mathbb{N}_{+}}$ is a time-homogeneous Markov chain. Someone have objections, and the reasons are as follows: From Chapman-Kolmogorov equation , $k+m$ -step transition probability \begin{align}    \mathbf{P}(S_{n+k+m}=j\mid S_{n}=j)&=:\mathbf{P}^{(k+m)}(j,j)\\ &=\sum_{r\in S}\mathbf{P}^{(k)}(j,r)\cdot\mathbf{P}^{(m)}(r,j) \\ &\ge\mathbf{P}^{(k)}(j,j)\cdot\mathbf{P}^{(m)}(j,j).\end{align} If both $\mathbf{P}^{(k)}(j,j)>0$ and $\mathbf{P}^{(m)}(j,j)>0$ , then $\mathbf{P}^{(k+m)}(j,j)>0.$ That will lead to $\mathbf{P}(S_{n+k+m}=j)>0,$ further steps will result in infinitely many $\mathbf{P}(S_{n}=j)>0.$ What is your opinion on this? I thought from the definition of time-homogeneous, $\mathbf{P}(S_{n+1}=j|S_{n}=i)=\mathbf{P}(S_{m+1}=j|S_{m}=i),\forall i,j\in S \& \forall n,m\in \mathbb{N},$ both $\mathbf{P}(S_{n})>0$ and $\mathbf{P}(S_{m})>0$ seem to be needed.","is a sequence of independently and identically distributed random variables, each taking a finite number of integer values. For any define Show that is a Markov chain, with each state being transient. My question is how to prove whose each state is transient.I attempt to demonstrate for each in the state space , by Kolmogorov's strong law of large numbers and  Borel–Cantelli lemma. Maybe we can use this equivalence to create a paradox to support that starting from some positive integer , all subsequent terms in i.e. are zeros. is a time-homogeneous Markov chain. Someone have objections, and the reasons are as follows: From Chapman-Kolmogorov equation , -step transition probability If both and , then That will lead to further steps will result in infinitely many What is your opinion on this? I thought from the definition of time-homogeneous, both and seem to be needed.","\left\{\xi_{n}\right\}_{n\in\ \mathbb{N}_{+}} \mathbf{E}(\xi_1)\ne 0, n\in \mathbb{N}_{+}, S_{n}:=\sum_{i=1}^{n}\xi_{i}. \left\{S_{n}\right\}_{n\in\ \mathbb{N}_{+}} \sum_{n=1}^{\infty}\mathbf{P}(S_{n}=i\mid S_1=i)<\infty  i S \frac{S_n}{n} \xrightarrow[]{a.s.}\mathbf{E}(\xi_1)\Longleftrightarrow \displaystyle \lim_{ n\to \infty}\mathbf{P}\left(\bigcup_{k=n}^{\infty}\left\{\left|\frac{S_k}{k} -\mathbf{E}(\xi_1)\right|\ge\varepsilon\right\}\right)=0, \forall \varepsilon>0. n(i) \sum_{n=1}^{\infty}\mathbf{P}(S_{n}=i\mid S_1=i) \mathbf{P}(S_{n(i)}=i\mid S_1=i),\mathbf{P}(S_{{n(i)+1}}=i\mid S_1=i),...... \left\{S_{n}\right\}_{n\in\ \mathbb{N}_{+}} k+m \begin{align}
   \mathbf{P}(S_{n+k+m}=j\mid S_{n}=j)&=:\mathbf{P}^{(k+m)}(j,j)\\ &=\sum_{r\in S}\mathbf{P}^{(k)}(j,r)\cdot\mathbf{P}^{(m)}(r,j) \\
&\ge\mathbf{P}^{(k)}(j,j)\cdot\mathbf{P}^{(m)}(j,j).\end{align} \mathbf{P}^{(k)}(j,j)>0 \mathbf{P}^{(m)}(j,j)>0 \mathbf{P}^{(k+m)}(j,j)>0. \mathbf{P}(S_{n+k+m}=j)>0, \mathbf{P}(S_{n}=j)>0. \mathbf{P}(S_{n+1}=j|S_{n}=i)=\mathbf{P}(S_{m+1}=j|S_{m}=i),\forall i,j\in S \& \forall n,m\in \mathbb{N}, \mathbf{P}(S_{n})>0 \mathbf{P}(S_{m})>0","['probability', 'probability-theory', 'stochastic-processes', 'markov-chains', 'self-learning']"
21,Show that a family is uniformly integrable,Show that a family is uniformly integrable,,"I consider a family of sub sigma algebra $(\mathcal{F}_s)_{s\in S}$ on ( $\Omega,\mathcal{A}, \mathbb{P}$ ) and $X\in L^1(\Omega,\mathcal{A}, \mathbb{P})$ . I want to show that $Y_s =\mathbb{E}(X | \mathcal{F}_s)$ is uniformly integrable. My attempt is the following : I use the caracterization in terms of boundedness of a uniformly integrable family. First we notice that there exists $M\geq 0$ such that $\mathbb{E}(\lvert X\rvert)\leq M$ . Now we notice that for all $s\in S$ we have $$ \lVert Y_s \rVert_{L^1} = \mathbb{E}\left\lvert[\mathbb{E}(X | \mathcal{F}_s)\right\rvert] \leq \mathbb{E}[\mathbb{E}(\lvert X  \rvert | \mathcal{F}_s)] = \mathbb{E}(\lvert X\rvert)\leq M $$ Which shows that the family is bounded in $L^1$ and thus uniformly integrable. Is this seems correct ? Edit : This is false, my memory was totally wrong as my intuition. To solve the problem unfortunately I have not found other solutions than strenghten  hypothesis by considering that $X\in L^p$ in order to use a characterization in terms of epsilon delta of the uniform integrability. First we notice that there exists $M$ such that for all $s\in S$ $\lVert Y_s\rVert_{L^p}\leq M$ Let $\epsilon>0$ . Take $\delta = \frac{\epsilon^q}{M^q}$ . We have $$ \mathbb{E}(\lvert Y_s\rvert 1_{A})\leq \lVert Y_s\rVert_{L^p}(\mathbb{P}(A))^{1/q}\leq M(\mathbb{P}(A))^{1/q}\leq M\frac{\epsilon}{M} $$ I am almost sure the hypothesis I have made is superficial","I consider a family of sub sigma algebra on ( ) and . I want to show that is uniformly integrable. My attempt is the following : I use the caracterization in terms of boundedness of a uniformly integrable family. First we notice that there exists such that . Now we notice that for all we have Which shows that the family is bounded in and thus uniformly integrable. Is this seems correct ? Edit : This is false, my memory was totally wrong as my intuition. To solve the problem unfortunately I have not found other solutions than strenghten  hypothesis by considering that in order to use a characterization in terms of epsilon delta of the uniform integrability. First we notice that there exists such that for all Let . Take . We have I am almost sure the hypothesis I have made is superficial","(\mathcal{F}_s)_{s\in S} \Omega,\mathcal{A}, \mathbb{P} X\in L^1(\Omega,\mathcal{A}, \mathbb{P}) Y_s =\mathbb{E}(X | \mathcal{F}_s) M\geq 0 \mathbb{E}(\lvert X\rvert)\leq M s\in S 
\lVert Y_s \rVert_{L^1} = \mathbb{E}\left\lvert[\mathbb{E}(X | \mathcal{F}_s)\right\rvert] \leq \mathbb{E}[\mathbb{E}(\lvert X 
\rvert | \mathcal{F}_s)] = \mathbb{E}(\lvert X\rvert)\leq M
 L^1 X\in L^p M s\in S \lVert Y_s\rVert_{L^p}\leq M \epsilon>0 \delta = \frac{\epsilon^q}{M^q} 
\mathbb{E}(\lvert Y_s\rvert 1_{A})\leq \lVert Y_s\rVert_{L^p}(\mathbb{P}(A))^{1/q}\leq M(\mathbb{P}(A))^{1/q}\leq M\frac{\epsilon}{M}
","['probability', 'probability-theory', 'measure-theory', 'uniform-integrability']"
22,Computing $E\left[\log\left(1 + \frac{\sum X_i}{n}\right)\right]$ when $X_i$ is iid $Geom\left(e^{-\lambda}\right)$,Computing  when  is iid,E\left[\log\left(1 + \frac{\sum X_i}{n}\right)\right] X_i Geom\left(e^{-\lambda}\right),"Originally, I was attempting to find a Method of Moments estimator for $\lambda$ given $X_i \sim Geom(e^{-\lambda})$ s.t. $f_\lambda(x) = e^{-\lambda}(1 - e^{-\lambda})^x$ . I found it to be $\hat{\lambda} = \log\left(1 + \frac{\sum_{i=1}^n X_i}{n}\right)$ .  I am now attempting to verify if it is unbiased ( $E\left[\hat{\lambda}\right] \stackrel{?}{=} \lambda$ ). This leads to a super nasty summation: $$ E\left[\log\left(1 + \frac{\sum_{i=1}^n X_i}{n}\right)\right] = \sum_{k=0}^\infty \log\left(1 + \frac{k}{n}\right)\cdot  \underbrace{\pmatrix{k + n - 1 \\ k}  \cdot (1 - e^{-\lambda})^k e^{-n\lambda}}_\text{Negative binomial pmf} $$ Note that $\sum X_i \sim NB(n, e^{-\lambda})$ . My thoughts: I am convinced that the sum is either divergent or not at all equal to $\lambda$ . Are there any tools I could use to indirectly state this? Is there a feasible pen/paper way to calculate the exact sum or should this be left for something like Mathematica?","Originally, I was attempting to find a Method of Moments estimator for given s.t. . I found it to be .  I am now attempting to verify if it is unbiased ( ). This leads to a super nasty summation: Note that . My thoughts: I am convinced that the sum is either divergent or not at all equal to . Are there any tools I could use to indirectly state this? Is there a feasible pen/paper way to calculate the exact sum or should this be left for something like Mathematica?","\lambda X_i \sim Geom(e^{-\lambda}) f_\lambda(x) = e^{-\lambda}(1 - e^{-\lambda})^x \hat{\lambda} = \log\left(1 + \frac{\sum_{i=1}^n X_i}{n}\right) E\left[\hat{\lambda}\right] \stackrel{?}{=} \lambda 
E\left[\log\left(1 + \frac{\sum_{i=1}^n X_i}{n}\right)\right] = \sum_{k=0}^\infty \log\left(1 + \frac{k}{n}\right)\cdot 
\underbrace{\pmatrix{k + n - 1 \\ k} 
\cdot (1 - e^{-\lambda})^k e^{-n\lambda}}_\text{Negative binomial pmf}
 \sum X_i \sim NB(n, e^{-\lambda}) \lambda","['probability', 'sequences-and-series', 'statistics', 'expected-value']"
23,Question Regarding Vershynin's Proof of Bernstein's Inequality,Question Regarding Vershynin's Proof of Bernstein's Inequality,,"I have been studying Vershynin's ""High-dimensional Probability,"" and  I have some confusion regarding the proof of Bernstein's inequality (Thm 2.8.2). It concerns the following step: (Perhaps note that $(X_i)_i$ is a finite sequence of independent, mean zero subexponential random variables) By a property of subexponential random variables, we can bound their MGF the following way for $|\lambda| \leq \frac{c}{\max \|X_i\|_{\phi_1}}$ : $$\mathbb{E}[\exp (\lambda X_i)] \leq \exp (C \lambda^2 \|X_i\|_{\phi_1}^2).$$ Thus we get (from a previous step, note $S := \sum X_i$ ) $$ \mathbb{P}(S \geq t) \leq \exp (-\lambda t + C \lambda^2 \sigma^2) \quad (*)$$ where $\sigma^2 = \sum_{i=1}^N \|X_i\|_{\phi_1}^2$ . Now we minimize this expression in $\lambda$ with respect to the constraint and get an optimal choice of $$\lambda = \min \left(\frac{t}{2C\sigma^2}, \frac{c}{\max_i \|X_i\|_{\phi_1}}\right).$$ Thus, with this we obtain $$ \mathbb{P}(S \geq t) \leq \exp \left(-\min \left(\frac{t^2}{4C\sigma^2}, \frac{ct}{2 \max_i \|X_i\|_{\phi_1}}\right)\right).$$ Now, my issue is with this last step. I get how we get the first term in the minimum simply by plugging in the first possible value of $\lambda$ into (*), and I would think to get the second one I just have to plug in the second possible value, but I don't see how I then get the second value... am I missing something? If you want to check the source material, the book is available online for free under https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf","I have been studying Vershynin's ""High-dimensional Probability,"" and  I have some confusion regarding the proof of Bernstein's inequality (Thm 2.8.2). It concerns the following step: (Perhaps note that is a finite sequence of independent, mean zero subexponential random variables) By a property of subexponential random variables, we can bound their MGF the following way for : Thus we get (from a previous step, note ) where . Now we minimize this expression in with respect to the constraint and get an optimal choice of Thus, with this we obtain Now, my issue is with this last step. I get how we get the first term in the minimum simply by plugging in the first possible value of into (*), and I would think to get the second one I just have to plug in the second possible value, but I don't see how I then get the second value... am I missing something? If you want to check the source material, the book is available online for free under https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf","(X_i)_i |\lambda| \leq \frac{c}{\max \|X_i\|_{\phi_1}} \mathbb{E}[\exp (\lambda X_i)] \leq \exp (C \lambda^2 \|X_i\|_{\phi_1}^2). S := \sum X_i  \mathbb{P}(S \geq t) \leq \exp (-\lambda t + C \lambda^2 \sigma^2) \quad (*) \sigma^2 = \sum_{i=1}^N \|X_i\|_{\phi_1}^2 \lambda \lambda = \min \left(\frac{t}{2C\sigma^2}, \frac{c}{\max_i \|X_i\|_{\phi_1}}\right).  \mathbb{P}(S \geq t) \leq \exp \left(-\min \left(\frac{t^2}{4C\sigma^2}, \frac{ct}{2 \max_i \|X_i\|_{\phi_1}}\right)\right). \lambda","['probability', 'probability-theory', 'random-variables', 'concentration-of-measure', 'distribution-tails']"
24,Expanding a Probability Tree,Expanding a Probability Tree,,"Consider the following situation: Take some integer $N$ Each day there is a: $p_1$ probability that $N$ will increase by $n_1$ % of its current value $p_2$ probability that $N$ will decrease by $n_2$ % of its current value $p_3 = 1- p_1+ p_2$ probability that $N$ keep its current value My Question: On any given day, I am trying to find out what possible values $N$ can assume - and what are the probabilities of assuming these values. To solve this question, I first wrote the possible values that $N$ can take on the first day: $$N_{1} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{1}$$ On the second day, we can write: $$N_{2} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{2}$$ $$  N_{2} = N \times [p_3^2 + 2p_3p_1(1+n_1) + 2p_3p_2(1-n_2) + p_1^2(1+n_1)^2 + 2p_1(1+n_1)p_2(1-n_2) + p_2^2(1-n_2)^2]$$ By analyzing the above expression, I can indirectly see that on the second day, $N$ can have 9 possible values: $N$ can happen one possible way with probability $p_3^2$ $N*(1+n_1)$ can happen two possible ways with a total probability $2*p_3p_1$ $N*(1-n_2)$ can happen two possible ways with a total probability $2*p_3p_2$ $N*(1+n_1)^2$ can happen one possible way with probability $p_1^2$ $N*(1+n_1)(1-n_2)$ can happen two possible ways with a total probability $2*p_1p_2$ $N*(1-n_2)^2$ can happen one possible way with probability $p_2^2$ Based on this, I can observe that : $$(p_1 + p_2 + p_3)^2 = p_3^2 + 2p_3p_1 + 2p_3p_2 + p_1^2 + 2p_1p_2 + p_2^2 = 1$$ Thus, it would appear that I could find out all possible values that $N$ can assume on any given day along with the corresponding probabilities using the relationship: $$N_{k} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{k}$$ Is my understanding correct? Can this above logic be used to expand any Probability Function corresponding to any Discrete Random Variable - and thus find out the possible values and corresponding probabilities that a Discrete Random Variable can take in the future? And is there a more compact way (in mathematical notation) to represent this expansion in the general case? Thanks! Note: The following relationship (Multinomial Theorem : https://en.wikipedia.org/wiki/Multinomial_theorem ) can be useful here: $$(x_1 + x_2 + ... + x_m)^n = \sum_{k_1=0}^n \sum_{k_2=0}^n ... \sum_{k_m=0}^n \binom{n}{k_1, k_2, ..., k_m} x_1^{k_1} x_2^{k_2} ... x_m^{k_m} = \sum_{k_1=0}^n \sum_{k_2=0}^n ... \sum_{k_m=0}^n \frac{n!}{k_1! k_2! ... k_m!} x_1^{k_1} x_2^{k_2} ... x_m^{k_m} $$","Consider the following situation: Take some integer Each day there is a: probability that will increase by % of its current value probability that will decrease by % of its current value probability that keep its current value My Question: On any given day, I am trying to find out what possible values can assume - and what are the probabilities of assuming these values. To solve this question, I first wrote the possible values that can take on the first day: On the second day, we can write: By analyzing the above expression, I can indirectly see that on the second day, can have 9 possible values: can happen one possible way with probability can happen two possible ways with a total probability can happen two possible ways with a total probability can happen one possible way with probability can happen two possible ways with a total probability can happen one possible way with probability Based on this, I can observe that : Thus, it would appear that I could find out all possible values that can assume on any given day along with the corresponding probabilities using the relationship: Is my understanding correct? Can this above logic be used to expand any Probability Function corresponding to any Discrete Random Variable - and thus find out the possible values and corresponding probabilities that a Discrete Random Variable can take in the future? And is there a more compact way (in mathematical notation) to represent this expansion in the general case? Thanks! Note: The following relationship (Multinomial Theorem : https://en.wikipedia.org/wiki/Multinomial_theorem ) can be useful here:","N p_1 N n_1 p_2 N n_2 p_3 = 1- p_1+ p_2 N N N N_{1} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{1} N_{2} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{2}   N_{2} = N \times [p_3^2 + 2p_3p_1(1+n_1) + 2p_3p_2(1-n_2) + p_1^2(1+n_1)^2 + 2p_1(1+n_1)p_2(1-n_2) + p_2^2(1-n_2)^2] N N p_3^2 N*(1+n_1) 2*p_3p_1 N*(1-n_2) 2*p_3p_2 N*(1+n_1)^2 p_1^2 N*(1+n_1)(1-n_2) 2*p_1p_2 N*(1-n_2)^2 p_2^2 (p_1 + p_2 + p_3)^2 = p_3^2 + 2p_3p_1 + 2p_3p_2 + p_1^2 + 2p_1p_2 + p_2^2 = 1 N N_{k} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{k} (x_1 + x_2 + ... + x_m)^n = \sum_{k_1=0}^n \sum_{k_2=0}^n ... \sum_{k_m=0}^n \binom{n}{k_1, k_2, ..., k_m} x_1^{k_1} x_2^{k_2} ... x_m^{k_m} = \sum_{k_1=0}^n \sum_{k_2=0}^n ... \sum_{k_m=0}^n \frac{n!}{k_1! k_2! ... k_m!} x_1^{k_1} x_2^{k_2} ... x_m^{k_m}
",['probability']
25,Show $Q(\eta_n) := \mathbb P \{\exists m < \infty : \eta_m = 0 |\eta_0 = \eta_n\}$ is a martingale,Show  is a martingale,Q(\eta_n) := \mathbb P \{\exists m < \infty : \eta_m = 0 |\eta_0 = \eta_n\},"I am trying to solve this problem Let $\eta_n$ be a homogeneous Markov chain on the countable state space $S : =\{ 0,1,2,...\}$ and $\mathcal F_n := \sigma(\eta_j; 0 \le j \le n), n \ge 0$ its natural filtration. For $i \in S$ denote by $Q(i)$ the probability that the Markov chain starting from site $i$ ever reaches the point $0 \in S$ : $Q(i) := \mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = i\}$ Prove that $Z_n := Q(\eta_n)$ is an $(\mathcal F_n)_{n\ge 0}$ -martingale I read the definition of Markov chain, transition probability and homogeneous Markov chain, apart from that I don't know much about Markov chains. Still I am not making much sense of this problem To prove the martngale property, I don't know how to manage $Q$ in $E[Q(\eta_{n+1})|\mathcal F_n)]=E[\mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = \eta_{n+1}\}|\mathcal F_n)]$ How  am I supposed to compute the probability of a probability(That is why I need to compute the expectation I guess) or equivalently the expectation of a probability? $Q(\eta_n)$ does not seem like a random variable Moreover I am not sure how to check the other conditions either. $Z_n$ does not seem adapted as it depends on future events, i.e. on some $m>n+1$ , i.e. $\eta_m$ which is not $\mathcal F_n$ -measurable.  I am not sure how to check $E[|Z_n|]< \infty $ either I am not sure if this makes a lot of sense: $ Q(\eta_{n+1})= \mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = \eta_{n+1}\}$ then $n+1=0 $","I am trying to solve this problem Let be a homogeneous Markov chain on the countable state space and its natural filtration. For denote by the probability that the Markov chain starting from site ever reaches the point : Prove that is an -martingale I read the definition of Markov chain, transition probability and homogeneous Markov chain, apart from that I don't know much about Markov chains. Still I am not making much sense of this problem To prove the martngale property, I don't know how to manage in How  am I supposed to compute the probability of a probability(That is why I need to compute the expectation I guess) or equivalently the expectation of a probability? does not seem like a random variable Moreover I am not sure how to check the other conditions either. does not seem adapted as it depends on future events, i.e. on some , i.e. which is not -measurable.  I am not sure how to check either I am not sure if this makes a lot of sense: then","\eta_n S : =\{ 0,1,2,...\} \mathcal F_n := \sigma(\eta_j; 0 \le j \le n), n \ge 0 i \in S Q(i) i 0 \in S Q(i) := \mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = i\} Z_n := Q(\eta_n) (\mathcal F_n)_{n\ge 0} Q E[Q(\eta_{n+1})|\mathcal F_n)]=E[\mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = \eta_{n+1}\}|\mathcal F_n)] Q(\eta_n) Z_n m>n+1 \eta_m \mathcal F_n E[|Z_n|]< \infty   Q(\eta_{n+1})= \mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = \eta_{n+1}\} n+1=0 ","['probability', 'probability-theory', 'markov-chains', 'martingales']"
26,Showing that $\exp(rN_t-\lambda t(e^r-1))$ is a Martingale,Showing that  is a Martingale,\exp(rN_t-\lambda t(e^r-1)),"Let $(N_t)_t$ be a homogeneous Poisson process with intensity $\lambda>0$ . How can I prove that $Y_t=\exp(rN_t-\lambda t(e^r-1))$ is a Martingal w.r.t. the canonical filtration, $r\in \mathbb{R}$ , $t\geq > 0$ . My attempt $Y_t$ is adapted to $\mathcal{F}_t$ since $N_t$ is adapted to $\mathcal{F}_t$ and $Y_t=f(N_t)$ is a measurable function of $N_t$ . Integrability: $E(|Y_t|)<\infty$ ( I don't really see at first glance why that is the case) For $0\leq s\leq t$ , one has to show: $$E(\exp(rN_t-\lambda t(e^r-1))|\mathcal{F}_s)=\cdots = \exp(rN_s-\lambda s(e^r-1))$$ However, I don't know how to deal with the $\exp$ in $E(\cdot)$ . It might be noteworthy that I know that $N_t-\lambda t$ (i. e. the compensated poisson process) and $(N_t-\lambda t)^2-\lambda t$ are martingales. Can someone help?","Let be a homogeneous Poisson process with intensity . How can I prove that is a Martingal w.r.t. the canonical filtration, , . My attempt is adapted to since is adapted to and is a measurable function of . Integrability: ( I don't really see at first glance why that is the case) For , one has to show: However, I don't know how to deal with the in . It might be noteworthy that I know that (i. e. the compensated poisson process) and are martingales. Can someone help?","(N_t)_t \lambda>0 Y_t=\exp(rN_t-\lambda t(e^r-1)) r\in \mathbb{R} t\geq
> 0 Y_t \mathcal{F}_t N_t \mathcal{F}_t Y_t=f(N_t) N_t E(|Y_t|)<\infty 0\leq s\leq t E(\exp(rN_t-\lambda t(e^r-1))|\mathcal{F}_s)=\cdots = \exp(rN_s-\lambda s(e^r-1)) \exp E(\cdot) N_t-\lambda t (N_t-\lambda t)^2-\lambda t","['probability', 'stochastic-processes', 'martingales', 'poisson-process']"
27,Girsanov theorem for discrete-time stochastic processes,Girsanov theorem for discrete-time stochastic processes,,"I am reading Buehler et al. (2022) ""Learning to Trade II: Deep Hedging"" and the slide on p. 44 states Fun fact: in discrete time, we can change also the volatility of a process by changing measure. I am familiar with continuous-time stochastic processes and the change of measure there, which (through Girsanov's theorem) would only change the drift part but not the diffusion part of the process. Now, according to the quote above, I am assuming that a discrete time version of Girsanov's theorem exists with the additional property of changing the diffusion part in a difference equation. However, I haven't been able to find papers or textbooks describing this. I would be grateful if somebody could point me to such an article (preferably with examples) or could outline how this could be derived.","I am reading Buehler et al. (2022) ""Learning to Trade II: Deep Hedging"" and the slide on p. 44 states Fun fact: in discrete time, we can change also the volatility of a process by changing measure. I am familiar with continuous-time stochastic processes and the change of measure there, which (through Girsanov's theorem) would only change the drift part but not the diffusion part of the process. Now, according to the quote above, I am assuming that a discrete time version of Girsanov's theorem exists with the additional property of changing the diffusion part in a difference equation. However, I haven't been able to find papers or textbooks describing this. I would be grateful if somebody could point me to such an article (preferably with examples) or could outline how this could be derived.",,"['probability', 'stochastic-processes', 'discrete-time']"
28,Showing $\sum_{r \leq k} \sum_{j_1 + \dots + j_r = k} \frac{1}{r!j_1\dots j_r} = 1$ for all $k$,Showing  for all,\sum_{r \leq k} \sum_{j_1 + \dots + j_r = k} \frac{1}{r!j_1\dots j_r} = 1 k,"I wanted to know why is this equality true for all $k$ : $$\sum_{r \leq k} \sum_{j_1 + \dots + j_r = k} \frac{1}{r!j_1\dots j_r} = 1$$ and was told to look at the $z^k$ term in the power series expansion of $$\dfrac{1}{1 - z}= \exp(- \ln(1- z))= \exp \big( \sum_{j \geq 1} \frac{z^j}{j}\big) = \sum_{r\geq 0} \frac{\big( \sum_{j \geq 1} \frac{z^j}{j}\big)^r }{r!}$$ I thought of taking $z=0$ but its not the right way, can someone see how is the equality true ?","I wanted to know why is this equality true for all : and was told to look at the term in the power series expansion of I thought of taking but its not the right way, can someone see how is the equality true ?",k \sum_{r \leq k} \sum_{j_1 + \dots + j_r = k} \frac{1}{r!j_1\dots j_r} = 1 z^k \dfrac{1}{1 - z}= \exp(- \ln(1- z))= \exp \big( \sum_{j \geq 1} \frac{z^j}{j}\big) = \sum_{r\geq 0} \frac{\big( \sum_{j \geq 1} \frac{z^j}{j}\big)^r }{r!} z=0,"['probability', 'combinatorics', 'summation', 'proof-explanation', 'taylor-expansion']"
29,"Deduce a probability inequality via ""standard symmetrization argument""","Deduce a probability inequality via ""standard symmetrization argument""",,"Let $\boldsymbol{A}\in\mathbb{R}^{n_1\times n_2}$ be some fixed matrix, and $\{\boldsymbol{X}_i\}_{i=1}^n$ be independent random matrices for which $\mathbb{E}(\boldsymbol{X}_i)=\boldsymbol{A}$ . I would like to deduce the following probability inequality $$ \mathbb{P}\left\{\left\|\frac{1}{n}\sum_{i=1}^n\boldsymbol{X}_i-\boldsymbol{A}\right\|\ge 3t\right\}\le\max_{\|\boldsymbol{u}\|=\|\boldsymbol{v}\|=1}\mathbb{P}\left\{\left\langle\frac{1}{n}\sum_{i=1}^n\boldsymbol{X}_i-\boldsymbol{A},\boldsymbol{u}\boldsymbol{v}^\top\right\rangle\ge t\right\}+4\mathbb{P}\left\{\left\|\frac{1}{n}\sum_{i=1}^n\varepsilon_i\boldsymbol{X}_i\right\|\ge t\right\}, $$ where the $\|\cdot\|$ is the matrix spectral norm, $\{\varepsilon_i\}_{i=1}^n$ are i.i.d. Rademacher (i.e., symmetric Bernoulli) random variables. The probability inequality above is a simplified version of the first step in Proof of Theorem 2 in (the arxiv version of) Yuan, M., & Zhang, C. H. (2017). Incoherent tensor norms and their applications in higher order tensor completion. IEEE Transactions on Information Theory, 63(10), 6753-6766. see https://arxiv.org/pdf/1606.03504.pdf , which was concerning tensors and their more specialized norms, so as to adapt the problem to a wider audience. The paper said ""the standard symmetrization argument gives"" the above inequality, but I totally have no idea about how to do this standard step. I know how to use the symmetrization technique to derive inequalities regarding expectations, such as $\mathbb{E}\left\|\sum_{i=1}^n \boldsymbol{X}_i\right\| \leq 2 \mathbb{E}\left\|\sum_{i=1}^n \varepsilon_i \boldsymbol{X}_i\right\|$ , but I don't know how to apply this to bound probabilities . The paper also gave the following reference about the used ""standard symmetrization argument"" Giné, E., & Zinn, J. (1984). Some limit theorems for empirical processes. The Annals of Probability, 929-989. see https://www.jstor.org/stable/2243347 , but unfortunately the contents of the paper are too advanced to be understandable to me. I am wondering if someone can give me a hint on how to deduce this standard inequality, or point out for me which part of the latter reference implies the result. Thanks in advance.","Let be some fixed matrix, and be independent random matrices for which . I would like to deduce the following probability inequality where the is the matrix spectral norm, are i.i.d. Rademacher (i.e., symmetric Bernoulli) random variables. The probability inequality above is a simplified version of the first step in Proof of Theorem 2 in (the arxiv version of) Yuan, M., & Zhang, C. H. (2017). Incoherent tensor norms and their applications in higher order tensor completion. IEEE Transactions on Information Theory, 63(10), 6753-6766. see https://arxiv.org/pdf/1606.03504.pdf , which was concerning tensors and their more specialized norms, so as to adapt the problem to a wider audience. The paper said ""the standard symmetrization argument gives"" the above inequality, but I totally have no idea about how to do this standard step. I know how to use the symmetrization technique to derive inequalities regarding expectations, such as , but I don't know how to apply this to bound probabilities . The paper also gave the following reference about the used ""standard symmetrization argument"" Giné, E., & Zinn, J. (1984). Some limit theorems for empirical processes. The Annals of Probability, 929-989. see https://www.jstor.org/stable/2243347 , but unfortunately the contents of the paper are too advanced to be understandable to me. I am wondering if someone can give me a hint on how to deduce this standard inequality, or point out for me which part of the latter reference implies the result. Thanks in advance.","\boldsymbol{A}\in\mathbb{R}^{n_1\times n_2} \{\boldsymbol{X}_i\}_{i=1}^n \mathbb{E}(\boldsymbol{X}_i)=\boldsymbol{A} 
\mathbb{P}\left\{\left\|\frac{1}{n}\sum_{i=1}^n\boldsymbol{X}_i-\boldsymbol{A}\right\|\ge 3t\right\}\le\max_{\|\boldsymbol{u}\|=\|\boldsymbol{v}\|=1}\mathbb{P}\left\{\left\langle\frac{1}{n}\sum_{i=1}^n\boldsymbol{X}_i-\boldsymbol{A},\boldsymbol{u}\boldsymbol{v}^\top\right\rangle\ge t\right\}+4\mathbb{P}\left\{\left\|\frac{1}{n}\sum_{i=1}^n\varepsilon_i\boldsymbol{X}_i\right\|\ge t\right\},
 \|\cdot\| \{\varepsilon_i\}_{i=1}^n \mathbb{E}\left\|\sum_{i=1}^n \boldsymbol{X}_i\right\| \leq 2 \mathbb{E}\left\|\sum_{i=1}^n \varepsilon_i \boldsymbol{X}_i\right\|","['probability', 'probability-theory', 'inequality', 'probability-distributions']"
30,Find the CDF of $X$ being the number of tosses of a fair dice until the numbers in each toss add to more than $6$,Find the CDF of  being the number of tosses of a fair dice until the numbers in each toss add to more than,X 6,"I recently ran into this problem on an assingment. Supose you have a fair dice and let $X$ be the random variable that counts how many tosses happen before the sum of the numbers that showed up in each toss adds up to something greater than $6$ . You're asked to find the CDF of $X$ which has support in $\{2,\dots,7\}$ , but I can't figure out a closed form for it. I've tried calculating the probability function $\mathbb{P}[X=k]$ for each $k$ in the support of $X$ but it's a bit messy. Any ideas?","I recently ran into this problem on an assingment. Supose you have a fair dice and let be the random variable that counts how many tosses happen before the sum of the numbers that showed up in each toss adds up to something greater than . You're asked to find the CDF of which has support in , but I can't figure out a closed form for it. I've tried calculating the probability function for each in the support of but it's a bit messy. Any ideas?","X 6 X \{2,\dots,7\} \mathbb{P}[X=k] k X","['probability', 'conditional-probability']"
31,There exists $j>i\geq 1$ such that $\mu(A\cap f^{-(k_j-k_i)}(A))>0$ when $\mu(A\cap f^{-n_j}(A))>0$ for a probability measure $\mu$ and sequence $n_j$,There exists  such that  when  for a probability measure  and sequence,j>i\geq 1 \mu(A\cap f^{-(k_j-k_i)}(A))>0 \mu(A\cap f^{-n_j}(A))>0 \mu n_j,"This exercise is from the book Foundations of Ergodic Theory by Viana, p. 9, exercise 1.2.5. part a.) Let $(X,\mathcal{F},\mu)$ be a probability space and $f:X\to X$ be a $\mu$ -preserving mapping in the sense that $\forall A\in\mathcal{F}:\mu(A) = \mu(f^{-1}(A))$ . Assume that $A\in\mathcal{F}:\mu(A) > 0$ and that $\left(n_j\right)_{j=1}^\infty$ is the sequence of non-negative integers such that $\mu(A\cap f^{-n_j}(A)) > 0$ for all $j=1,2,\dots$ . The task is the show that for any increasing sequence $k_1<k_2<\cdots$ there exists $j>i\geq 1$ such that $\mu(A\cap f^{-(k_j - k_i)}(A)) > 0$ . A given hint is to first show that for every $N\in\mathbb{N}$ such that $\frac{1}{N} < \mu(A)$ there exists $j\in\mathbb{N}$ such that $0\leq n_j\leq N$ and $\mu(A\cap f^{-n_j}(A)) > 0$ . A minor detail which has occurred to me is that instead of "" any "" sequence $k_1<k_2<\cdots$ we should probably only look at subsequences of $(n_j)_{j\in\mathbb{N}}$ . Since it is not true that if $(a_n)_{n=1}^\infty, (b_n)_{n=1}^\infty$ are any two sequences of non-negative integers, then $\exists n\in\mathbb{N}:\exists j>i: a_n = b_j - b_i$ . But I digress. As of now I have not made any progress in the main task or the given hint. Namely, ( Progress on hint: ) I have not managed to deduce any relationship with connects increasing power of $f$ to decreasing lower bound of $\mu(A\cap f^{-n_j}(A))$ . Poincarés recurrence theorem gives us that $\mu$ almost every point of $A$ visits $A$ for infinitely many powers of $f$ . Therefore $B := \limsup_{n\to\infty} A\cap f^{-n}(A)$ has a non-zero measure and by Borel-Cantelli lemma $\sum_{n=1}^\infty \mu(A\cap f^{-n}(A)) = +\infty$ . ( Progress on the main task: ) While we can add as many pre-images of $f$ to $\mu$ 's argument, $\mu(A) = \mu(f^{-M}(A)),\forall M\geq 0$ , I have not managed to connect this to the difference between $n_j,n_i$ . Something tells me that we would like to use a proof by contradiction by bounding $A\cap f^{-n_j}(A)$ above by some $A\cap f^{-(n_{j_1} - n_{j_2})}(A)$ , but so far this has not lead to anything. The biggest problem that I think I am having is that I have no way of knowing with which indices the points of $A\cap f^{-n_j}(A)$ may visit $A$ again, and hence it is difficult to say anything about $\mu(A\cap f^{-(n_j - n_i)}(A))$ .","This exercise is from the book Foundations of Ergodic Theory by Viana, p. 9, exercise 1.2.5. part a.) Let be a probability space and be a -preserving mapping in the sense that . Assume that and that is the sequence of non-negative integers such that for all . The task is the show that for any increasing sequence there exists such that . A given hint is to first show that for every such that there exists such that and . A minor detail which has occurred to me is that instead of "" any "" sequence we should probably only look at subsequences of . Since it is not true that if are any two sequences of non-negative integers, then . But I digress. As of now I have not made any progress in the main task or the given hint. Namely, ( Progress on hint: ) I have not managed to deduce any relationship with connects increasing power of to decreasing lower bound of . Poincarés recurrence theorem gives us that almost every point of visits for infinitely many powers of . Therefore has a non-zero measure and by Borel-Cantelli lemma . ( Progress on the main task: ) While we can add as many pre-images of to 's argument, , I have not managed to connect this to the difference between . Something tells me that we would like to use a proof by contradiction by bounding above by some , but so far this has not lead to anything. The biggest problem that I think I am having is that I have no way of knowing with which indices the points of may visit again, and hence it is difficult to say anything about .","(X,\mathcal{F},\mu) f:X\to X \mu \forall A\in\mathcal{F}:\mu(A) = \mu(f^{-1}(A)) A\in\mathcal{F}:\mu(A) > 0 \left(n_j\right)_{j=1}^\infty \mu(A\cap f^{-n_j}(A)) > 0 j=1,2,\dots k_1<k_2<\cdots j>i\geq 1 \mu(A\cap f^{-(k_j - k_i)}(A)) > 0 N\in\mathbb{N} \frac{1}{N} < \mu(A) j\in\mathbb{N} 0\leq n_j\leq N \mu(A\cap f^{-n_j}(A)) > 0 k_1<k_2<\cdots (n_j)_{j\in\mathbb{N}} (a_n)_{n=1}^\infty, (b_n)_{n=1}^\infty \exists n\in\mathbb{N}:\exists j>i: a_n = b_j - b_i f \mu(A\cap f^{-n_j}(A)) \mu A A f B := \limsup_{n\to\infty} A\cap f^{-n}(A) \sum_{n=1}^\infty \mu(A\cap f^{-n}(A)) = +\infty f \mu \mu(A) = \mu(f^{-M}(A)),\forall M\geq 0 n_j,n_i A\cap f^{-n_j}(A) A\cap f^{-(n_{j_1} - n_{j_2})}(A) A\cap f^{-n_j}(A) A \mu(A\cap f^{-(n_j - n_i)}(A))","['real-analysis', 'probability', 'measure-theory', 'ergodic-theory']"
32,Find $P(X=k)$ where $X$ is the random variable that represents the number of draws required to obtain a white ball,Find  where  is the random variable that represents the number of draws required to obtain a white ball,P(X=k) X,"Consider a box containing one black ball and one white ball. Every time a black ball is drawn, a die is rolled and a number of black balls equal to the result of the die are added to the box. We want to determine $P(X=k)$ where $X$ is the random variable that represents the number of draws required to obtain a white ball. Examples I found: $$\begin{split} P(X=1)&=\frac 12\\ P(X=2)&=\frac 1{24}\\ P(X=3)&=\dfrac 1{2.6^2}\displaystyle \sum_{k=1}^6\dfrac {k+1}{k+2}\sum_{i=1}^{6}\dfrac 1{i+k+1}= 0.0472490588115\dots$ \end{split}$$ it appears that there is no elegant formula for $P(X=k)$ .","Consider a box containing one black ball and one white ball. Every time a black ball is drawn, a die is rolled and a number of black balls equal to the result of the die are added to the box. We want to determine where is the random variable that represents the number of draws required to obtain a white ball. Examples I found: it appears that there is no elegant formula for .","P(X=k) X \begin{split}
P(X=1)&=\frac 12\\
P(X=2)&=\frac 1{24}\\
P(X=3)&=\dfrac 1{2.6^2}\displaystyle \sum_{k=1}^6\dfrac {k+1}{k+2}\sum_{i=1}^{6}\dfrac 1{i+k+1}= 0.0472490588115\dots
\end{split} P(X=k)","['probability', 'combinatorics', 'conditional-probability']"
33,When does the product of random matrices diverge?,When does the product of random matrices diverge?,,"Suppose $A_i$ are IID samples of a random matrix-valued variable. I'm interested in determining whether the following infinite product is likely to diverge $$A_1 A_2 A_3\cdots$$ Finding necessary+sufficient condition is hard (Theorem 2). There are cheap to compute sufficient conditions for the product to converge. Are there cheap to compute sufficient conditions for the product to diverge? Of particular interest is case of $A_i=(I-x_i x_i^T)$ where $x_i$ are IID Gaussian. The case for isotropic Gaussian is solved here , I'm looking for insight into non-isotropic case","Suppose are IID samples of a random matrix-valued variable. I'm interested in determining whether the following infinite product is likely to diverge Finding necessary+sufficient condition is hard (Theorem 2). There are cheap to compute sufficient conditions for the product to converge. Are there cheap to compute sufficient conditions for the product to diverge? Of particular interest is case of where are IID Gaussian. The case for isotropic Gaussian is solved here , I'm looking for insight into non-isotropic case",A_i A_1 A_2 A_3\cdots A_i=(I-x_i x_i^T) x_i,"['probability', 'stochastic-processes', 'dynamical-systems', 'ergodic-theory', 'stability-theory']"
34,Expectancy and Variance of getting different-colored pairs of marbles from two urns,Expectancy and Variance of getting different-colored pairs of marbles from two urns,,"I have two urns, each containing $N$ marbles. Urn $A$ has $K_A$ blue marbles and the rest are red, while Urn $B$ has $K_B$ blue marbles and the remaining marbles are red. I then sample pairs of marbles, one from each urn, without replacement. If I repeat this $k \leq N$ times, on average how many pairs of marbles will have mismatched colours? What is the variance of this random variable? My first attempt Let $X$ be the random variable denoting the number of pairs of marbles with mismatched marbles. $$E[X] = \sum_{i=1}^{k}i\cdot P(X=i)$$ where $$P(X=i) = \frac{\# \text{ microstates with exactly $i$ mismatches}}{\#  \text{ all possible microstates}}\,.$$ For the denominator, we may enumerate all possible combinations as in the hypergeometric distribution and use Vandermonde's identity to simplify: $$ \sum_{l} \ {K_A \choose l}{N-K_A \choose k-l}\sum_{m} {K_B \choose m}{N-K_B \choose k-m} = {N \choose k}^2\,.$$ Though it remains to find a similar combinatoric expression for the numerator... My second attempt Let $I_i$ be an indicator variable denoting occurrence of a mismatch in the $i^{th}$ pair of marbles. Note that $X = \sum_{i=1}^{k} I_i$ . Invoking linearity of expectation: $$ E[X] = E[\sum_{i=1}^{k} I_i] = \sum_{i=1}^{k} E[I_i] $$ Since $E[I_i] = P(I_i = 1)$ , $$  \begin{align} E[X] &= \sum_{i=1}^{k} P(I_i = 1) = \sum_{i=1}^{k}\sum_{\mathbf{I} \in \{0,1\}^{i-1}}  P(I_i = 1\;|\;\mathbf{I}) \\ &= \sum_{i=1}^{k}\sum_{\mathbf{I} \in \{0,1\}^{i-1}}  P(I_i = 1\;|\;I_{i-1} \land I_{i-2} \dots \land I_{1})P(I_{i-1} \;|\;I_{i-2} \land I_{i-3} \dots \land I_{1})\cdots P(I_1 ) \end{align}$$ This method requires calculating exponentially many conditional probabilities unless there is some recursive relationship (which I'm struggling to derive): $$  \begin{align}P(I_1 = 1) &= \frac{K_A}{N}(1-\frac{K_B}{N}) + \frac{K_B}{N}(1-\frac{K_A}{N}) = \frac{K_A}{N}+\frac{K_B}{N}-2\frac{K_A}{N}\frac{K_B}{N}\\ \\ P(I_1 = 0) &= 1 - P(I_1 = 1)\\ \\ P(I_2 = 1\;|\;I_1 = 0) &=\;?\end{align}$$ CORRECTION It seems that $$E[X] = \sum_{i=1}^{k} E[I_i] = k\cdot E[I_1] = k\cdot P(I_1 = 1)$$ But it is not clear to me why it can be argued from symmetry that $E[I_i] = E[I_1]$ ?","I have two urns, each containing marbles. Urn has blue marbles and the rest are red, while Urn has blue marbles and the remaining marbles are red. I then sample pairs of marbles, one from each urn, without replacement. If I repeat this times, on average how many pairs of marbles will have mismatched colours? What is the variance of this random variable? My first attempt Let be the random variable denoting the number of pairs of marbles with mismatched marbles. where For the denominator, we may enumerate all possible combinations as in the hypergeometric distribution and use Vandermonde's identity to simplify: Though it remains to find a similar combinatoric expression for the numerator... My second attempt Let be an indicator variable denoting occurrence of a mismatch in the pair of marbles. Note that . Invoking linearity of expectation: Since , This method requires calculating exponentially many conditional probabilities unless there is some recursive relationship (which I'm struggling to derive): CORRECTION It seems that But it is not clear to me why it can be argued from symmetry that ?","N A K_A B K_B k \leq N X E[X] = \sum_{i=1}^{k}i\cdot P(X=i) P(X=i) = \frac{\# \text{ microstates with exactly i mismatches}}{\#  \text{ all possible microstates}}\,.  \sum_{l} \ {K_A \choose l}{N-K_A \choose k-l}\sum_{m} {K_B \choose m}{N-K_B \choose k-m} = {N \choose k}^2\,. I_i i^{th} X = \sum_{i=1}^{k} I_i  E[X] = E[\sum_{i=1}^{k} I_i] = \sum_{i=1}^{k} E[I_i]  E[I_i] = P(I_i = 1)  
\begin{align} E[X] &= \sum_{i=1}^{k} P(I_i = 1) = \sum_{i=1}^{k}\sum_{\mathbf{I} \in \{0,1\}^{i-1}}  P(I_i = 1\;|\;\mathbf{I}) \\ &= \sum_{i=1}^{k}\sum_{\mathbf{I} \in \{0,1\}^{i-1}}  P(I_i = 1\;|\;I_{i-1} \land I_{i-2} \dots \land I_{1})P(I_{i-1} \;|\;I_{i-2} \land I_{i-3} \dots \land I_{1})\cdots P(I_1 ) \end{align}  
\begin{align}P(I_1 = 1) &= \frac{K_A}{N}(1-\frac{K_B}{N}) + \frac{K_B}{N}(1-\frac{K_A}{N}) = \frac{K_A}{N}+\frac{K_B}{N}-2\frac{K_A}{N}\frac{K_B}{N}\\
\\
P(I_1 = 0) &= 1 - P(I_1 = 1)\\
\\
P(I_2 = 1\;|\;I_1 = 0) &=\;?\end{align} E[X] = \sum_{i=1}^{k} E[I_i] = k\cdot E[I_1] = k\cdot P(I_1 = 1) E[I_i] = E[I_1]","['probability', 'combinatorics']"
35,Random walks on regular trees,Random walks on regular trees,,"An infinite $d$ -regular tree is a graph where every vertex has degree $d$ and there are no cycles. Suppose we define a standard random walk $X_n$ on such a tree. Is the claim that $X_n$ is transient for $d \ge 3$ true? If so, how can I prove that? I do know for a fact that random walks are transient on $\mathbb{Z}^d$ with $d \ge 3$ . Can we use this somehow to prove the claim?","An infinite -regular tree is a graph where every vertex has degree and there are no cycles. Suppose we define a standard random walk on such a tree. Is the claim that is transient for true? If so, how can I prove that? I do know for a fact that random walks are transient on with . Can we use this somehow to prove the claim?",d d X_n X_n d \ge 3 \mathbb{Z}^d d \ge 3,"['probability', 'graph-theory', 'markov-chains', 'random-walk', 'trees']"
36,"Completeness of $\Lambda^\alpha$ in $D([0,\infty))$",Completeness of  in,"\Lambda^\alpha D([0,\infty))","I was going through this paper in which the weak $L_\alpha$ norm $$\Vert X\Vert_{\alpha,w}^\alpha = \sup_{\lambda>0} \lambda^\alpha \mathbb P\bigg[\sup_{0\leq t\leq T} |X_t|>\lambda\bigg]$$ was defined. The space $$\Lambda^\alpha :=\bigg\lbrace X \text{ cadlag and adapted } : \Vert X\Vert_{\alpha,w}<\infty\bigg\rbrace $$ was introduced. Now it was claimed that $\Lambda^\alpha$ is a complete linear metric space for all $0<\alpha<2$ . It's easy to show that this space is indeed linear, but i have some trouble verifying that it is complete. Here is my attempt to far Let $(X^{(n)})_{n\in\mathbb N}$ be a Cauchy sequence (w.r.t. $\Vert\cdot\Vert_{\alpha,w}$ ) in $\Lambda^\alpha$ , meaning for all $\epsilon>0$ and sufficiently large $M$ we have $$\epsilon> \Vert X^{(n)}-X^{(m)}\Vert_{\alpha,w}^\alpha = \sup_{\lambda>0}\lambda^\alpha \mathbb P\bigg[\sup_{0\leq t\leq T} |X_t^{(n)}-X_t^{(m)}|>\lambda\bigg]\geq \sup_{\lambda>0} \lambda^\alpha \mathbb P[|X_t^{(n)}-X_t^{(m)}|>\lambda]$$ for $n,m>M$ and each $t\in[0,T]$ . This implies that the finite-dimensional projections of $X^{(n)}$ converge weakly. Now i would have to verify some tightness criterion of the sequence $X^{(n)}$ to show that it converges weakly in $D([0,T])$ and show that its limit $X$ satisfies $\Vert X\Vert_{\alpha,w}<\infty$ . For the tightness criterion, one might use the characterization using the modulus of continuity $w'$ , meaning one has to show that $$\lim_{\delta\to 0}\limsup_{n\to\infty} \mathbb P[w'(X^{(n)},0,T,\delta)>\epsilon]=0.$$ Now by the triangle inequality we get $$\mathbb P[w'(X^{(n)},0,T,\delta)>\epsilon]\leq \mathbb P[w'(X^{(n)}-X^{(m)},0,T,\delta)>\epsilon]+\mathbb P[w'(X^{(m)},0,T,\delta)>\epsilon] \hspace{1cm} (*)$$ which i think is sufficient to show that $\lim_{\delta\to 0}\limsup_{n\to\infty} (*)=0$ . Is this proof correct? Is there an easier way to show the completeness?","I was going through this paper in which the weak norm was defined. The space was introduced. Now it was claimed that is a complete linear metric space for all . It's easy to show that this space is indeed linear, but i have some trouble verifying that it is complete. Here is my attempt to far Let be a Cauchy sequence (w.r.t. ) in , meaning for all and sufficiently large we have for and each . This implies that the finite-dimensional projections of converge weakly. Now i would have to verify some tightness criterion of the sequence to show that it converges weakly in and show that its limit satisfies . For the tightness criterion, one might use the characterization using the modulus of continuity , meaning one has to show that Now by the triangle inequality we get which i think is sufficient to show that . Is this proof correct? Is there an easier way to show the completeness?","L_\alpha \Vert X\Vert_{\alpha,w}^\alpha = \sup_{\lambda>0} \lambda^\alpha \mathbb P\bigg[\sup_{0\leq t\leq T} |X_t|>\lambda\bigg] \Lambda^\alpha :=\bigg\lbrace X \text{ cadlag and adapted } : \Vert X\Vert_{\alpha,w}<\infty\bigg\rbrace  \Lambda^\alpha 0<\alpha<2 (X^{(n)})_{n\in\mathbb N} \Vert\cdot\Vert_{\alpha,w} \Lambda^\alpha \epsilon>0 M \epsilon> \Vert X^{(n)}-X^{(m)}\Vert_{\alpha,w}^\alpha = \sup_{\lambda>0}\lambda^\alpha \mathbb P\bigg[\sup_{0\leq t\leq T} |X_t^{(n)}-X_t^{(m)}|>\lambda\bigg]\geq \sup_{\lambda>0} \lambda^\alpha \mathbb P[|X_t^{(n)}-X_t^{(m)}|>\lambda] n,m>M t\in[0,T] X^{(n)} X^{(n)} D([0,T]) X \Vert X\Vert_{\alpha,w}<\infty w' \lim_{\delta\to 0}\limsup_{n\to\infty} \mathbb P[w'(X^{(n)},0,T,\delta)>\epsilon]=0. \mathbb P[w'(X^{(n)},0,T,\delta)>\epsilon]\leq \mathbb P[w'(X^{(n)}-X^{(m)},0,T,\delta)>\epsilon]+\mathbb P[w'(X^{(m)},0,T,\delta)>\epsilon] \hspace{1cm} (*) \lim_{\delta\to 0}\limsup_{n\to\infty} (*)=0","['probability', 'measure-theory', 'stochastic-processes', 'stochastic-calculus', 'skorohod-space']"
37,"Expectation of the max number of times an element is chosen, if repeatedly randomly choosing a subset out of a set","Expectation of the max number of times an element is chosen, if repeatedly randomly choosing a subset out of a set",,"Given a set of $n$ elements. For a total of $s$ times, you randomly choose exactly $m$ elements among them. (i.e. each $m$ -element set has $p=\frac{1}{\binom{n}{m}}$ of being chosen) Here $n\gg m$ and maybe $s\sim\frac{n}{m}$ . If we denote $X_i$ as the number of times the ith element is chosen, I want to know about the maximum among $X_i$ , i.e. $X=\max\{X_1,X_2,\cdots,X_n\}$ . Specifically, I want to know either $E(X)$ when $s=O\left(\frac{n}{m}\right)$ , or an $s$ such that $E(X)=O(1)$ . I don't think there is a pretty answer to this, so I am trying some approximations. I'm not sure if I can safely ignore the covariance between $X_i$ and treat them as iid binomial distributions (i.e. $X_i\sim Binom(s,p)$ where $p=\frac{m}{n}$ ). But even so I fail to figure out the expectation of $X$ . I think there must be a better way to approximate $X_i$ but I do need help on it.","Given a set of elements. For a total of times, you randomly choose exactly elements among them. (i.e. each -element set has of being chosen) Here and maybe . If we denote as the number of times the ith element is chosen, I want to know about the maximum among , i.e. . Specifically, I want to know either when , or an such that . I don't think there is a pretty answer to this, so I am trying some approximations. I'm not sure if I can safely ignore the covariance between and treat them as iid binomial distributions (i.e. where ). But even so I fail to figure out the expectation of . I think there must be a better way to approximate but I do need help on it.","n s m m p=\frac{1}{\binom{n}{m}} n\gg m s\sim\frac{n}{m} X_i X_i X=\max\{X_1,X_2,\cdots,X_n\} E(X) s=O\left(\frac{n}{m}\right) s E(X)=O(1) X_i X_i\sim Binom(s,p) p=\frac{m}{n} X X_i","['probability', 'combinatorics', 'binomial-coefficients', 'expected-value']"
38,How long does it take for a two-site wait activation?,How long does it take for a two-site wait activation?,,"Consider two sites, linked as sketched Initially, both sites are off (red). However, each activates (turns green) at a constant rate $f$ . Once activated, a site remains activated. If one activates, it generates an activation wave (green line) which propagates at speed $v$ (given as the number of sites per time unit), as shown Then, one of two things can happen: either the wave activates the neighboring site, provided it has not been activated yet, or the site activates on its own How do we calculate the expected time it takes for one site to activate? In other words, what is the expected waiting time on one site? If there was only one site, the answer would simply be $1/f$ , but I am struggling to frame the two (and possible $n$ -case ) scenario. Any suggestions? My attempt: Given a time interval $dt$ , the probability $P_f$ that one site is activated, within $dt$ , on its own is simply $fdt$ . However, the probability $P$ that a site is activated within $dt$ is given by $$ \begin{align} P(dt)=&P_1(\text{""site activates autonomously, given the wave didn't reach it within }dt\text{""})\\ &+P_2(\text{""wave activates the site, given it wasn't autonomously activated within }dt\text{""}) \end{align} $$ However, it seems relatively tricky to write these expressions. Intuitively, I would expect something like $$ P_1(t)=1-e^{-ft/v} $$ for example, but I have no particular motivation for this choice. It simply satisfies some expected asymptotics. Any ideas? Perhaps an approach based on first-hitting time analysis?","Consider two sites, linked as sketched Initially, both sites are off (red). However, each activates (turns green) at a constant rate . Once activated, a site remains activated. If one activates, it generates an activation wave (green line) which propagates at speed (given as the number of sites per time unit), as shown Then, one of two things can happen: either the wave activates the neighboring site, provided it has not been activated yet, or the site activates on its own How do we calculate the expected time it takes for one site to activate? In other words, what is the expected waiting time on one site? If there was only one site, the answer would simply be , but I am struggling to frame the two (and possible -case ) scenario. Any suggestions? My attempt: Given a time interval , the probability that one site is activated, within , on its own is simply . However, the probability that a site is activated within is given by However, it seems relatively tricky to write these expressions. Intuitively, I would expect something like for example, but I have no particular motivation for this choice. It simply satisfies some expected asymptotics. Any ideas? Perhaps an approach based on first-hitting time analysis?","f v 1/f n dt P_f dt fdt P dt 
\begin{align}
P(dt)=&P_1(\text{""site activates autonomously, given the wave didn't reach it within }dt\text{""})\\
&+P_2(\text{""wave activates the site, given it wasn't autonomously activated within }dt\text{""})
\end{align}
 
P_1(t)=1-e^{-ft/v}
","['probability', 'statistics', 'stochastic-processes', 'random-variables', 'dynamical-systems']"
39,Lower bound for the Gaussian tail,Lower bound for the Gaussian tail,,"I am asked to prove the following lower bound for the normal tail. Let $X\sim N(0,1)$ $$P(X\geq a)\geq c\exp\left(-a-\frac{a^{2}}{2}\right)$$ for some $c>0$ . To do this , as a hint I am asked to find the density of $N(0,1)$ with respect to $N(a,1)$ which I have computed as $\exp\left(-xa+\frac{a^{2}}{2}\right)$ and then asked to show that $$ \bigg(F(t)-F(0)\bigg)\exp\left(-a(t+a)+\frac{a^{2}}{2}\right)\leq P(X\geq a)$$ for all $a,t>0$ . Where $F$ is the the cdf of a standard Gaussian. So I would have to make $t=1$ . But I am unable to show the above inequality. How do I use the density? For example if $\mu\sim N(a,1)$ then the LHS is $$\displaystyle\int_{0}^{t}\exp\left(-xa+\frac{a^{2}}{2}-a^{2}-at-\frac{a^{2}}{2}\right)\,d\mu(x) = \int_{0}^{t}\exp(-a(x+t))\,d\mu(x)$$ . How do I now go from here to $\int_{a}^{\infty}\exp\left(-xa+\frac{a^{2}}{2}\right)\,d\mu(x)=P(X\geq a)$ ?","I am asked to prove the following lower bound for the normal tail. Let for some . To do this , as a hint I am asked to find the density of with respect to which I have computed as and then asked to show that for all . Where is the the cdf of a standard Gaussian. So I would have to make . But I am unable to show the above inequality. How do I use the density? For example if then the LHS is . How do I now go from here to ?","X\sim N(0,1) P(X\geq a)\geq c\exp\left(-a-\frac{a^{2}}{2}\right) c>0 N(0,1) N(a,1) \exp\left(-xa+\frac{a^{2}}{2}\right)  \bigg(F(t)-F(0)\bigg)\exp\left(-a(t+a)+\frac{a^{2}}{2}\right)\leq P(X\geq a) a,t>0 F t=1 \mu\sim N(a,1) \displaystyle\int_{0}^{t}\exp\left(-xa+\frac{a^{2}}{2}-a^{2}-at-\frac{a^{2}}{2}\right)\,d\mu(x) = \int_{0}^{t}\exp(-a(x+t))\,d\mu(x) \int_{a}^{\infty}\exp\left(-xa+\frac{a^{2}}{2}\right)\,d\mu(x)=P(X\geq a)","['probability', 'probability-theory', 'probability-distributions']"
40,Understanding Central Limit Theorem vs. Law of Large Numbers,Understanding Central Limit Theorem vs. Law of Large Numbers,,"I am trying to clarify these two concepts - and understand the differences between the Central Limit Theorem ( https://en.wikipedia.org/wiki/Central_limit_theorem ) and the Weak Law of Large Numbers ( https://en.wikipedia.org/wiki/Law_of_large_numbers ). As an example, suppose I have a coin and I don't know the true probability of Heads or Tails - I start to flip the coin again and again: The Law of Large Numbers states that if I flip this coin enough times, I will get an estimate of the true probability of getting a Heads The Central Limit Theorem states that as I flip the coin again and again, the distribution  for the probability of getting a Heads will follow a Standard Normal Distribution Is my understanding of this correct? Thanks!","I am trying to clarify these two concepts - and understand the differences between the Central Limit Theorem ( https://en.wikipedia.org/wiki/Central_limit_theorem ) and the Weak Law of Large Numbers ( https://en.wikipedia.org/wiki/Law_of_large_numbers ). As an example, suppose I have a coin and I don't know the true probability of Heads or Tails - I start to flip the coin again and again: The Law of Large Numbers states that if I flip this coin enough times, I will get an estimate of the true probability of getting a Heads The Central Limit Theorem states that as I flip the coin again and again, the distribution  for the probability of getting a Heads will follow a Standard Normal Distribution Is my understanding of this correct? Thanks!",,"['probability', 'statistics', 'central-limit-theorem', 'law-of-large-numbers']"
41,Optimal Resetting Strategy for Maximizing Win Rate,Optimal Resetting Strategy for Maximizing Win Rate,,"I thought of an interesting probability problem but I'm not sure how to approach it. My knowledge of statistics is not terrible, but this is complex enough that I'm not even sure how to start. Any insight would be appreciated (such as if this is similar to a well-known problem) or how to start on solving this. Say Bob really wants to prove to someone that he is a top player in Video Game. In order to do so, his gamer profile needs to have a win rate greater than or equal to X% with at least Y games played. That is to say, if W is the number of wins on Bob's profile and L is the number of losses, Bob wants W/(W+L) ≥ X% and W+L ≥ Y . For example, assume X% = 80% and Y=50 . At any point between games, Bob can choose to reset his profile, setting both W and L to 0 (Bob has no knowledge of the outcome of future games when choosing to do this). Say Bob's true chance of winning any given game is P% , and he knows this number. Bob wants to strategically reset his profile such that he minimizes the expected number of games he needs to play to attain his goal. What is Bob's optimal strategy for when to reset his profile? How many games will he be expected to play before achieving his goal, and how many times will he be expected to reset his profile? This problem is a bit more complicated than just finding the first occurrence of a sequence of 50 games with a win percentage higher than 80% because that sequence might have started out Win-Loss-Loss, for example, and Bob might have chosen to reset his profile after those two consecutive losses without knowing that he would achieve many wins in the next 47 games. I'm not even sure what the optimal resetting strategy is for Bob. Clearly you're supposed to reset if your profile is 0-1, since that's strictly worse than being 0-0. But if Bob's record is currently at 39-11, for example, is it better to continue onwards in hopes that his next five games will be wins (thereby pushing his record up to 80% win rate with only five additional games played), or reset and be guaranteed to play at least 50 more games? Does this problem even have a nice solution?","I thought of an interesting probability problem but I'm not sure how to approach it. My knowledge of statistics is not terrible, but this is complex enough that I'm not even sure how to start. Any insight would be appreciated (such as if this is similar to a well-known problem) or how to start on solving this. Say Bob really wants to prove to someone that he is a top player in Video Game. In order to do so, his gamer profile needs to have a win rate greater than or equal to X% with at least Y games played. That is to say, if W is the number of wins on Bob's profile and L is the number of losses, Bob wants W/(W+L) ≥ X% and W+L ≥ Y . For example, assume X% = 80% and Y=50 . At any point between games, Bob can choose to reset his profile, setting both W and L to 0 (Bob has no knowledge of the outcome of future games when choosing to do this). Say Bob's true chance of winning any given game is P% , and he knows this number. Bob wants to strategically reset his profile such that he minimizes the expected number of games he needs to play to attain his goal. What is Bob's optimal strategy for when to reset his profile? How many games will he be expected to play before achieving his goal, and how many times will he be expected to reset his profile? This problem is a bit more complicated than just finding the first occurrence of a sequence of 50 games with a win percentage higher than 80% because that sequence might have started out Win-Loss-Loss, for example, and Bob might have chosen to reset his profile after those two consecutive losses without knowing that he would achieve many wins in the next 47 games. I'm not even sure what the optimal resetting strategy is for Bob. Clearly you're supposed to reset if your profile is 0-1, since that's strictly worse than being 0-0. But if Bob's record is currently at 39-11, for example, is it better to continue onwards in hopes that his next five games will be wins (thereby pushing his record up to 80% win rate with only five additional games played), or reset and be guaranteed to play at least 50 more games? Does this problem even have a nice solution?",,"['probability', 'expected-value', 'game-theory']"
42,Borel-Cantelli lemma variation,Borel-Cantelli lemma variation,,"Consider $(X_i)$ independent events such that $\sum_{i = 1}^\infty P(X_i) = \infty$ Let $S_n = \sum_{i = 1}^n \mathbb{1}_{X_i}$ , then $\frac{S_n}{\mathbb{E}S_n} \to 1$ almost surely. Do you have any ideas how to prove this? I've been thinkint about applying law of large numbers or usual Borel-Cantelli lemma, but I could not come up with anything.","Consider independent events such that Let , then almost surely. Do you have any ideas how to prove this? I've been thinkint about applying law of large numbers or usual Borel-Cantelli lemma, but I could not come up with anything.",(X_i) \sum_{i = 1}^\infty P(X_i) = \infty S_n = \sum_{i = 1}^n \mathbb{1}_{X_i} \frac{S_n}{\mathbb{E}S_n} \to 1,['probability']
43,Transformation of Variables as Marginalisation of Joint Distribution - Where am I going wrong?,Transformation of Variables as Marginalisation of Joint Distribution - Where am I going wrong?,,"I am trying to derive some equivalent of the transformation of variables formula. That is, given a random variable $Y=g(X)$ , where $g$ is an invertible function, then the pdf of $Y$ , $f_Y(y)$ , is given by, $$ f_Y(y) = f_X(x)\left|\frac{dx}{dy}\right| \tag{1}. $$ This result is readily derived by considering the relevant cdf and applying the chain rule. I am trying to find some alternate derivation via marginalisation. The marginal distribution, $f_Y(y)$ can also be found by, \begin{align} f_Y(y) = &\int_\mathcal{X}f_{YX}(y,x)dx,\tag{2}\\ &=\int_\mathcal{X}f_{Y|X}(y|x)f_X(x)dx.\tag{3} \end{align} Assuming that $g$ is invertible, I would have intuitively thought that $$ f_{Y|X}(y|x)=\delta(x-g^{-1}(y)),\tag{4} $$ where $\delta$ is the Dirac delta function. As such, $$ f_Y(y)=\int_\mathcal{X}\delta(x-g^{-1}(y))f_X(x)dx=f_X(g^{-1}(y)).\tag{5} $$ This is obviously incorrect. I believe I am making a mistake in (2), as the joint distribution is potentially not well defined? For context, I have a little knowledge of measure theory. Any help explaining where I made a mistake and how to correctly proceed would be greatly appreciated!","I am trying to derive some equivalent of the transformation of variables formula. That is, given a random variable , where is an invertible function, then the pdf of , , is given by, This result is readily derived by considering the relevant cdf and applying the chain rule. I am trying to find some alternate derivation via marginalisation. The marginal distribution, can also be found by, Assuming that is invertible, I would have intuitively thought that where is the Dirac delta function. As such, This is obviously incorrect. I believe I am making a mistake in (2), as the joint distribution is potentially not well defined? For context, I have a little knowledge of measure theory. Any help explaining where I made a mistake and how to correctly proceed would be greatly appreciated!","Y=g(X) g Y f_Y(y) 
f_Y(y) = f_X(x)\left|\frac{dx}{dy}\right| \tag{1}.
 f_Y(y) \begin{align}
f_Y(y) = &\int_\mathcal{X}f_{YX}(y,x)dx,\tag{2}\\
&=\int_\mathcal{X}f_{Y|X}(y|x)f_X(x)dx.\tag{3}
\end{align} g 
f_{Y|X}(y|x)=\delta(x-g^{-1}(y)),\tag{4}
 \delta 
f_Y(y)=\int_\mathcal{X}\delta(x-g^{-1}(y))f_X(x)dx=f_X(g^{-1}(y)).\tag{5}
","['probability', 'probability-theory', 'metric-spaces', 'random-variables', 'conditional-probability']"
44,Finding the probability of $P(40.5<Y<48.9|X>=68.6)$,Finding the probability of,P(40.5<Y<48.9|X>=68.6),"Two students go to a pizza place every week. Let $X$ and $Y$ be the weekly spend of each student at the pizza place. Assume that $X$ and $Y$ have a bivariate normal distribution with $\mu_X=60.6, \sigma_X=11.2, \mu_Y=46.8, \sigma_Y=8.4, \rho=0.94$ a) Find $P(40.5<Y<48.9|X\geq  68.6)$ EDIT: I think I have a solution. Can someone please verify? $$f_{XY}(x,y)=\frac{1}{2\pi(11.2)(8.4)\sqrt{1-(0.94)^2}}*\exp{-\frac{1}{2(1-(0.94)^2)}[(\frac{x-60.6}{11.2})^2+(\frac{y-46.8}{8.4})^2]-2(0.94)\frac{(x-60.6)(y-46.8)}{(11.2)(8.4)}}$$ $P(40.5<Y<48.9|X\geq  68.6)=\frac{P(40.5<Y<48.9, X\geq 68.6)}{P(X\geq 68.6)}$ $P(X\geq 68.6)=P(Z\geq 0.7143)=0.23752$ $$\int_{40.5}^{48.9}\int_{68.6}^{\infty}\frac{1}{2\pi(11.2)(8.4)\sqrt{1-(0.94)^2}}*\exp{-\frac{1}{2(1-(0.94)^2)}[(\frac{x-60.6}{11.2})^2+(\frac{y-46.8}{8.4})^2]-2(0.94)\frac{(x-60.6)(y-46.8)}{(11.2)(8.4)}} dx dy = 0.0051345$$ $P(40.5<Y<48.9|X\geq  68.6) = \frac{0.0051345}{0.23752}$ b) Find $P(40.5<Y<48.9)$ $=P(-0.75<Y<0.25)=0.3721$ c) Find $P(40.5<Y<48|X=68.6)$ $E(Y|X)=46.8+(0.94)(\frac{8.4}{11.2})(68.6-60.6)=52.44$ $Var(Y|X)=(8.4)^2-(0.94)^2(8.4)^2=8.21$ $P(-4.17<Z<-1.24)=0.1075$ d) What is the relationship between these probabilities? The relationship is that part (a) is smaller than the other two probabilities since you are dividing out a larger number. Also part(b) is univariate while part (a) and part (c) is bivariate. I am mostly just trying to see whether my solutions for parts (a) and part (d) are correct because those are the ones I am most confused about.",Two students go to a pizza place every week. Let and be the weekly spend of each student at the pizza place. Assume that and have a bivariate normal distribution with a) Find EDIT: I think I have a solution. Can someone please verify? b) Find c) Find d) What is the relationship between these probabilities? The relationship is that part (a) is smaller than the other two probabilities since you are dividing out a larger number. Also part(b) is univariate while part (a) and part (c) is bivariate. I am mostly just trying to see whether my solutions for parts (a) and part (d) are correct because those are the ones I am most confused about.,"X Y X Y \mu_X=60.6, \sigma_X=11.2, \mu_Y=46.8, \sigma_Y=8.4, \rho=0.94 P(40.5<Y<48.9|X\geq  68.6) f_{XY}(x,y)=\frac{1}{2\pi(11.2)(8.4)\sqrt{1-(0.94)^2}}*\exp{-\frac{1}{2(1-(0.94)^2)}[(\frac{x-60.6}{11.2})^2+(\frac{y-46.8}{8.4})^2]-2(0.94)\frac{(x-60.6)(y-46.8)}{(11.2)(8.4)}} P(40.5<Y<48.9|X\geq  68.6)=\frac{P(40.5<Y<48.9, X\geq 68.6)}{P(X\geq 68.6)} P(X\geq 68.6)=P(Z\geq 0.7143)=0.23752 \int_{40.5}^{48.9}\int_{68.6}^{\infty}\frac{1}{2\pi(11.2)(8.4)\sqrt{1-(0.94)^2}}*\exp{-\frac{1}{2(1-(0.94)^2)}[(\frac{x-60.6}{11.2})^2+(\frac{y-46.8}{8.4})^2]-2(0.94)\frac{(x-60.6)(y-46.8)}{(11.2)(8.4)}} dx dy = 0.0051345 P(40.5<Y<48.9|X\geq  68.6) = \frac{0.0051345}{0.23752} P(40.5<Y<48.9) =P(-0.75<Y<0.25)=0.3721 P(40.5<Y<48|X=68.6) E(Y|X)=46.8+(0.94)(\frac{8.4}{11.2})(68.6-60.6)=52.44 Var(Y|X)=(8.4)^2-(0.94)^2(8.4)^2=8.21 P(-4.17<Z<-1.24)=0.1075","['probability', 'conditional-probability']"
45,What is the probability of sharing a birthday if a year has an infinite number of days?,What is the probability of sharing a birthday if a year has an infinite number of days?,,"Here is the problem: Suppose that there are $k$ people. Each of them independently picks a uniformly random number from the set $\{1, 2,...,n\}$ . We say that a collision happens if there exist two people picking the same number. Let $k = \lceil n^\beta\rceil$ , where $\beta$ is  some  constant  that  does  not  change  with $n$ . Prove that there is a constant $\beta_0\in(0,1)$ such that if $\beta>\beta_0$ , then a collision happens with probability $1$ when $n\to\infty$ ; and if $\beta<\beta_0$ , then a collision happens with probability $0$ when $n\to\infty$ . Also find the value of $\beta_0$ . Here is my attempt: The sample space is $\Omega=\{1,2,\ldots,n\}^k$ . Let $A$ be the event that a collision happens. Then, $$     \begin{align}     \mathbf P(A)&=1-\frac{n(n-1)(n-2)\ldots(n-k+1)}{n^k}\\     &=1-\frac{n-1}n\frac{n-2}n\ldots\frac{n-k+1}n\\     &=1-\left(1-\frac1n\right)\left(1-\frac2n\right)\ldots\left(1-\frac{k-1}n\right)\\     &=1-\prod_{i=1}^{k-1}\left(1-\frac in\right)\\     &\ge1-\prod_{i=1}^{k-1}e^{-\frac in}\\     &=1-e^{-\sum_{i=1}^{k-1}\frac in}\\     &=1-e^{-\frac{k(k-1)}{2n}}\\     &=1-e^{-\frac{k^2}{2n}}e^{\frac k{2n}}     \end{align} $$ We used the inequality $1-x\le e^{-x}$ . Thus, we have obtained a lower bound of $\mathbf P(A)$ . Let $k=n^\beta$ . Then, $\mathbf P(A)\ge1-e^{-\frac{n^{2\beta}}{2n}}e^{\frac {n^\beta}{2n}}$ . For any $\beta\in(0,1)$ , we have $\lim_{n\to\infty}e^\frac {n^\beta}{2n}=1$ . If $\beta>\frac12$ , then $\lim_{n\to\infty}e^{-\frac{n^{2\beta}}{2n}}=0$ , so the lower bound goes to $1$ , and $\lim_{n\to\infty}\mathbf P(A)=1$ . If $\beta<\frac12$ , then $\lim_{n\to\infty}e^{-\frac{n^{2\beta}}{2n}}=1$ , so $\lim_{n\to\infty}\mathbf P(A)\ge0$ . I guess that I have to find a upper bound for the probability which goes to zero if $\beta<\frac12$ , but I have been unable to find it. Here are my questions: Does such an upper bound exist? If yes, how can I find it? For simplicity, I used $k=n^\beta$ , which is not valid because $k$ must be a positive integer. What happens if I use $k=\lceil n^\beta\rceil$ ? When taking the limit, do we have to rely on the continuity property of probabilities? We cannot define a sequence of events $A_1,A_2,\ldots,A_n$ on the same probability space (although we could if it were $A_1,A_2,\ldots,A_k$ ).","Here is the problem: Suppose that there are people. Each of them independently picks a uniformly random number from the set . We say that a collision happens if there exist two people picking the same number. Let , where is  some  constant  that  does  not  change  with . Prove that there is a constant such that if , then a collision happens with probability when ; and if , then a collision happens with probability when . Also find the value of . Here is my attempt: The sample space is . Let be the event that a collision happens. Then, We used the inequality . Thus, we have obtained a lower bound of . Let . Then, . For any , we have . If , then , so the lower bound goes to , and . If , then , so . I guess that I have to find a upper bound for the probability which goes to zero if , but I have been unable to find it. Here are my questions: Does such an upper bound exist? If yes, how can I find it? For simplicity, I used , which is not valid because must be a positive integer. What happens if I use ? When taking the limit, do we have to rely on the continuity property of probabilities? We cannot define a sequence of events on the same probability space (although we could if it were ).","k \{1, 2,...,n\} k = \lceil n^\beta\rceil \beta n \beta_0\in(0,1) \beta>\beta_0 1 n\to\infty \beta<\beta_0 0 n\to\infty \beta_0 \Omega=\{1,2,\ldots,n\}^k A 
    \begin{align}
    \mathbf P(A)&=1-\frac{n(n-1)(n-2)\ldots(n-k+1)}{n^k}\\
    &=1-\frac{n-1}n\frac{n-2}n\ldots\frac{n-k+1}n\\
    &=1-\left(1-\frac1n\right)\left(1-\frac2n\right)\ldots\left(1-\frac{k-1}n\right)\\
    &=1-\prod_{i=1}^{k-1}\left(1-\frac in\right)\\
    &\ge1-\prod_{i=1}^{k-1}e^{-\frac in}\\
    &=1-e^{-\sum_{i=1}^{k-1}\frac in}\\
    &=1-e^{-\frac{k(k-1)}{2n}}\\
    &=1-e^{-\frac{k^2}{2n}}e^{\frac k{2n}}
    \end{align}
 1-x\le e^{-x} \mathbf P(A) k=n^\beta \mathbf P(A)\ge1-e^{-\frac{n^{2\beta}}{2n}}e^{\frac {n^\beta}{2n}} \beta\in(0,1) \lim_{n\to\infty}e^\frac {n^\beta}{2n}=1 \beta>\frac12 \lim_{n\to\infty}e^{-\frac{n^{2\beta}}{2n}}=0 1 \lim_{n\to\infty}\mathbf P(A)=1 \beta<\frac12 \lim_{n\to\infty}e^{-\frac{n^{2\beta}}{2n}}=1 \lim_{n\to\infty}\mathbf P(A)\ge0 \beta<\frac12 k=n^\beta k k=\lceil n^\beta\rceil A_1,A_2,\ldots,A_n A_1,A_2,\ldots,A_k","['probability', 'limits', 'birthday']"
46,"$X_n$ are independent exponential variables, if $\sum_{n=1}^\infty λ_n^{-1}=\infty$,show that $\sum_{n=1}^\infty X_n=\infty$ a.e.","are independent exponential variables, if ,show that  a.e.",X_n \sum_{n=1}^\infty λ_n^{-1}=\infty \sum_{n=1}^\infty X_n=\infty,"Let $X_1,X_2,\cdots,X_n$ be independent nonnegative random variables, with $X_n$ having density $λ_n\exp(-λ_n x),x\geq 0,λ_n\geq 0$ ,if $\sum_{n=1}^\infty λ_n^{-1}=\infty$ , show that $\sum_{n=1}^\infty X_n=\infty$ a.e. Attempts: $\sum_{n=1}^\infty λ_n^{-1}=\infty$ implies $\sum_n E(X_n)=E(\sum_n X_n)=\infty$ ,also I get a hint to try to consider $\exp(-\sum_nX_n)$ ,and from $X_n$ are independent, $E(\exp(-\sum_nX_n))=\Pi_n E(\exp(-X_n))$ , but I don't know what to do next? A very similar question: Let $X_1, X_2, \ldots$ be independent r.v.'s with $0 \leq X_n \leq 1$ and $\sum_n E(X_n) = \infty$. Show $\sum_n X_n = \infty$ with probability 1? but differs in $0\leq X_n\leq 1$","Let be independent nonnegative random variables, with having density ,if , show that a.e. Attempts: implies ,also I get a hint to try to consider ,and from are independent, , but I don't know what to do next? A very similar question: Let $X_1, X_2, \ldots$ be independent r.v.'s with $0 \leq X_n \leq 1$ and $\sum_n E(X_n) = \infty$. Show $\sum_n X_n = \infty$ with probability 1? but differs in","X_1,X_2,\cdots,X_n X_n λ_n\exp(-λ_n x),x\geq 0,λ_n\geq 0 \sum_{n=1}^\infty λ_n^{-1}=\infty \sum_{n=1}^\infty X_n=\infty \sum_{n=1}^\infty λ_n^{-1}=\infty \sum_n E(X_n)=E(\sum_n X_n)=\infty \exp(-\sum_nX_n) X_n E(\exp(-\sum_nX_n))=\Pi_n E(\exp(-X_n)) 0\leq X_n\leq 1","['real-analysis', 'probability', 'probability-theory', 'convergence-divergence']"
47,Average Distance of the Sample Mean from the True Mean?,Average Distance of the Sample Mean from the True Mean?,,"This is a question I have been thinking of: Suppose I have a Normal Distribution with a specific mean (e.g. ""a"") and standard deviation (e.g. ""b"") - if I draw ""n"" random numbers from this distribution and take the mean of these ""n"" numbers : on average, how close will this mean be from ""a""? For example, using the R programming language, I tried to run this simulation: set.seed(123) results = list()  for (i in 1:1000)  {  #  n = 100, a = 5, b = 5 sample_i =  rnorm(100, 5, 5) mean_i = mean(sample_i) difference_i = abs(5 - mean_i) results[[i]] = data.frame(i,difference_i) }  final = do.call(rbind.data.frame, results) plot(density(final$difference_i), main = ""Spread of Errors : n = 100, a = 5, b = 5"") I can now show this for n = 1000: results = list()  for (i in 1:1000)  {  #  n = 1000, a = 5, b = 5 sample_i =  rnorm(100, 5, 5) mean_i = mean(sample_i) difference_i = abs(5 - mean_i) results[[i]] = data.frame(i,difference_i) }  final = do.call(rbind.data.frame, results) plot(density(final$difference_i), main = ""Spread of Errors : n = 1000, a = 5, b = 5"") My Question: In general, given a specific probability distribution -  is there some mathematical formula which shows on average, how far the mean from a sample of size ""n"" will deviate from the true mean of this specific probability distribution? Thanks! EDIT - NOTE: As a concrete example : Consider 1000 random draws from a Normal Distribution with Mean=a and Standard_Deviation = b : On average, what will be the expected difference between the mean of these 1000 random draws and the true mean (i.e. ""a"")? Consider 1000 random draws from a Poisson Distribution with the Rate_Parameter = ""lambda: : On average, what will be the expected difference between the Rate Parameter calculated from these 1000 random draws and the true Rate Parameter (i.e. ""lambda"")? In general, for ""n"" random draws from some general probability distribution - how will the mean calculated from these ""n"" random draws differ from the true mean of this distribution (on average)? Is there a mathematical formula that can be used to describe this relationship? (e.g. via Central Limit Theorem)","This is a question I have been thinking of: Suppose I have a Normal Distribution with a specific mean (e.g. ""a"") and standard deviation (e.g. ""b"") - if I draw ""n"" random numbers from this distribution and take the mean of these ""n"" numbers : on average, how close will this mean be from ""a""? For example, using the R programming language, I tried to run this simulation: set.seed(123) results = list()  for (i in 1:1000)  {  #  n = 100, a = 5, b = 5 sample_i =  rnorm(100, 5, 5) mean_i = mean(sample_i) difference_i = abs(5 - mean_i) results[[i]] = data.frame(i,difference_i) }  final = do.call(rbind.data.frame, results) plot(density(final$difference_i), main = ""Spread of Errors : n = 100, a = 5, b = 5"") I can now show this for n = 1000: results = list()  for (i in 1:1000)  {  #  n = 1000, a = 5, b = 5 sample_i =  rnorm(100, 5, 5) mean_i = mean(sample_i) difference_i = abs(5 - mean_i) results[[i]] = data.frame(i,difference_i) }  final = do.call(rbind.data.frame, results) plot(density(final$difference_i), main = ""Spread of Errors : n = 1000, a = 5, b = 5"") My Question: In general, given a specific probability distribution -  is there some mathematical formula which shows on average, how far the mean from a sample of size ""n"" will deviate from the true mean of this specific probability distribution? Thanks! EDIT - NOTE: As a concrete example : Consider 1000 random draws from a Normal Distribution with Mean=a and Standard_Deviation = b : On average, what will be the expected difference between the mean of these 1000 random draws and the true mean (i.e. ""a"")? Consider 1000 random draws from a Poisson Distribution with the Rate_Parameter = ""lambda: : On average, what will be the expected difference between the Rate Parameter calculated from these 1000 random draws and the true Rate Parameter (i.e. ""lambda"")? In general, for ""n"" random draws from some general probability distribution - how will the mean calculated from these ""n"" random draws differ from the true mean of this distribution (on average)? Is there a mathematical formula that can be used to describe this relationship? (e.g. via Central Limit Theorem)",,"['probability', 'statistics']"
48,Probability of this event,Probability of this event,,"Let $S$ be a set of 27 pairwise different numbers. Let $T$ be a set of 9 elements chosen from $S$ uniformly at random (no repetitions). Let $u$ be an element sampled uniformly at random from $T$ . Let $S'= \{s_1, s_2, \dots, s_{27}\}$ be the sequence of elements in $S$ but in increasing order. Given that $u$ is in the middle third of $T$ , what is the probability that the $i$ -th element from $S'$ was selected. This is my attempt. On the one hand, I’m not sure if $\mathbb P(u = s_i) = 1/21$ , where $i = 4, 5, \dots, 24$ , and 0 otherwise. On the other hand, I don’t know if the answer should be $$\mathbb P( u = s_i) = {27\choose 9}^{-1}\times\sum^{8}_{j=0} {{i-1}\choose{9+j}}\cdot{{27-i}\choose{18-j-1}},$$ where $i = 4, \dots, 24$ and 0 otherwise.","Let be a set of 27 pairwise different numbers. Let be a set of 9 elements chosen from uniformly at random (no repetitions). Let be an element sampled uniformly at random from . Let be the sequence of elements in but in increasing order. Given that is in the middle third of , what is the probability that the -th element from was selected. This is my attempt. On the one hand, I’m not sure if , where , and 0 otherwise. On the other hand, I don’t know if the answer should be where and 0 otherwise.","S T S u T S'= \{s_1, s_2, \dots, s_{27}\} S u T i S' \mathbb P(u = s_i) = 1/21 i = 4, 5, \dots, 24 \mathbb P( u = s_i) = {27\choose 9}^{-1}\times\sum^{8}_{j=0} {{i-1}\choose{9+j}}\cdot{{27-i}\choose{18-j-1}}, i = 4, \dots, 24","['probability', 'discrete-mathematics']"
49,Expected number of Same color run in standard 52 deck,Expected number of Same color run in standard 52 deck,,"I encountered this question What is the expected number of runs of same color in a standard deck of cards? , and I understand the answer approach by @George. However, I'm unsure why my answer leads to a different answer. I cannot find my logic error, and I'm looking for some explanations or suggestions. My approach: There are $26$ Black cards, so we assume there are $26+1=27$ slots for the Red cards to be. For each slot i, the probability that at least one Red card is in it is $1- (26/27)^{26}$ . If the slot has at least one Red card, the number of same color runs increases by 2; this holds for all $2\leq i\leq 27.$ For the first slot, if there is at least one Red, the number of run is increased by $1$ , and $0$ otherwise. A illustration is below: ___ B ___ B ___ B...___ B___B___ Then I get the answer to be $(2\cdot 26+1)\cdot(1-(26/27)^{26})=33.133$ What caused the error?","I encountered this question What is the expected number of runs of same color in a standard deck of cards? , and I understand the answer approach by @George. However, I'm unsure why my answer leads to a different answer. I cannot find my logic error, and I'm looking for some explanations or suggestions. My approach: There are Black cards, so we assume there are slots for the Red cards to be. For each slot i, the probability that at least one Red card is in it is . If the slot has at least one Red card, the number of same color runs increases by 2; this holds for all For the first slot, if there is at least one Red, the number of run is increased by , and otherwise. A illustration is below: ___ B ___ B ___ B...___ B___B___ Then I get the answer to be What caused the error?",26 26+1=27 1- (26/27)^{26} 2\leq i\leq 27. 1 0 (2\cdot 26+1)\cdot(1-(26/27)^{26})=33.133,"['probability', 'statistics']"
50,Approximative formula for normal distribution being above threshold,Approximative formula for normal distribution being above threshold,,"Suppose that $X \sim \mathcal{N}(r + \frac{1}{N}, s) $ and $Y \sim \mathcal{N}(r, s)$ for some $r, s \approx 1$ and $N \approx 10^6$ . What are good approximate formulas for the quantity $$\frac{ \mathbb{E}(X 1_{X > n})}{\mathbb{E}(Y1_{Y > n}) + \frac{1}{N}}$$ as a function of $r$ in the limit where $r \approx n$ ? The problem arises as one potential answer here: Assessing the efficiency of a single vote in a multiparty presidential election",Suppose that and for some and . What are good approximate formulas for the quantity as a function of in the limit where ? The problem arises as one potential answer here: Assessing the efficiency of a single vote in a multiparty presidential election,"X \sim \mathcal{N}(r + \frac{1}{N}, s)  Y \sim \mathcal{N}(r, s) r, s \approx 1 N \approx 10^6 \frac{ \mathbb{E}(X 1_{X > n})}{\mathbb{E}(Y1_{Y > n}) + \frac{1}{N}} r r \approx n","['probability', 'approximation', 'applications', 'voting-theory']"
51,What is the distribution of the product of three random variables?,What is the distribution of the product of three random variables?,,"From Wikipedia : If $X$ and $Y$ are two independent, continuous random variables, described by probability density functions $f_{X}$ and $f_{Y}$ then the probability density function of $Z=XY$ is \begin{equation} f_{Z}(z)=\int_{-\infty}^{\infty}f_{X}(x)f_{Y}(z/x)\frac{1}{|x|}dx \end{equation} Suppose now that we have another independent, continuous random variable $W$ , described by probability density function $f_{W}$ , what is the probability density function of $Z=XYW$ ? Is the following expression correct \begin{equation} f_{Z}(z)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X}(x)f_{Y}\left(\frac{y}{x}\right)f_{W}\left(\frac{z}{y}\right)\frac{1}{|xy|}dxdy \end{equation} ?","From Wikipedia : If and are two independent, continuous random variables, described by probability density functions and then the probability density function of is Suppose now that we have another independent, continuous random variable , described by probability density function , what is the probability density function of ? Is the following expression correct ?","X Y f_{X} f_{Y} Z=XY \begin{equation}
f_{Z}(z)=\int_{-\infty}^{\infty}f_{X}(x)f_{Y}(z/x)\frac{1}{|x|}dx
\end{equation} W f_{W} Z=XYW \begin{equation}
f_{Z}(z)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X}(x)f_{Y}\left(\frac{y}{x}\right)f_{W}\left(\frac{z}{y}\right)\frac{1}{|xy|}dxdy
\end{equation}","['probability', 'statistics', 'probability-distributions']"
52,About existence of martingale,About existence of martingale,,"Question: Define $X_t=\exp (B_t-\frac{t}{2})$ to be a martingale, where $(B_t)$ is a brownian motion. Does $\lim_{t \rightarrow \infty} X_t$ exist? I am thinking around this result (long-term behavior of trajectories): Define $\{B_t\}_{t \in [0,\infty)}$ to be a brownian motion, then $\lim_{t \rightarrow \infty} \sup \frac{B_t}{\sqrt{t}}=\infty$ and $\lim_{t \rightarrow \infty} \inf \frac{B_t}{\sqrt{t}}=-\infty$ . To fit in this result, can I rewrite $B_t-\frac{t}{2}$ as $\sqrt{t}(\frac{B_t}{\sqrt{t}}-\frac{\sqrt{t}}{2})$ , then apply the result (long-term behavior of trajectories) to get infinity to get that $\lim_{t \rightarrow \infty} X_t$ exists and equal to 0? Another approach I am thinking about is writing $X_t=\exp (B_t-\frac{t}{2})$ as $X_t=\exp (t[\frac{B_t}{t}-\frac{1}{2}])$ , then apply strong law of large numbers for brownian motion. to get $X_t=\exp (-\infty)=0$ , so the limit exists and is equal to 0? I think the second approach works, but I am not sure if the first approach works. Appreciate any help.","Question: Define to be a martingale, where is a brownian motion. Does exist? I am thinking around this result (long-term behavior of trajectories): Define to be a brownian motion, then and . To fit in this result, can I rewrite as , then apply the result (long-term behavior of trajectories) to get infinity to get that exists and equal to 0? Another approach I am thinking about is writing as , then apply strong law of large numbers for brownian motion. to get , so the limit exists and is equal to 0? I think the second approach works, but I am not sure if the first approach works. Appreciate any help.","X_t=\exp (B_t-\frac{t}{2}) (B_t) \lim_{t \rightarrow \infty} X_t \{B_t\}_{t \in [0,\infty)} \lim_{t \rightarrow \infty} \sup \frac{B_t}{\sqrt{t}}=\infty \lim_{t \rightarrow \infty} \inf \frac{B_t}{\sqrt{t}}=-\infty B_t-\frac{t}{2} \sqrt{t}(\frac{B_t}{\sqrt{t}}-\frac{\sqrt{t}}{2}) \lim_{t \rightarrow \infty} X_t X_t=\exp (B_t-\frac{t}{2}) X_t=\exp (t[\frac{B_t}{t}-\frac{1}{2}]) X_t=\exp (-\infty)=0","['probability', 'stochastic-processes', 'martingales']"
53,Can someone please verify my solutions for this probability question on bayes' theorem?,Can someone please verify my solutions for this probability question on bayes' theorem?,,"Assume a COVID test can identify the presence of COVID, given that the person has COVID, with probability $p_d$ . Assume the test assigns false positives with probability $p_f$ (a test will be positive but the person does not have COVID). Let $p_{\theta}$ be the prior probability a person has COVID. a) Calculate the probability that a test subject has COVID, given a test was positive. Do the same given the test was negative. Calculate the probability the test subject doesn’t have COVID given the test was positive, and then do the same for a negative test. *** $P(COVID|+)=\frac{P(+|COVID)P(COVID)}{P(+)}$ Using law of total probability for $P(+)$ : $P(+)=P(+|COVID)P(COVID)+P(+|NOCOVID)P(NOCOVID)$ $=p_dp_{\theta}+p_f(1-p_{\theta})$ $\implies P(COVID|+)=\frac{p_{d}p_{\theta}}{p_dp_{\theta}+p_f(1-p_{\theta})}$ *** $P(COVID|-)= \frac{P(-|COVID)P(COVID)}{P(-)}$ $P(-)=P(-|COVID)P(COVID)+P(-|NOCOVID)P(NOCOVID)$ $= (1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta})$ $\implies P(COVID|-)=\frac{(1-p_d)p_{\theta}}{(1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta})}$ *** $P(NOCOVID|+)= 1-P(COVID|+) = 1 - \frac{p_{d}p_{\theta}}{p_dp_{\theta}+p_f(1-p_{\theta})}$ *** $P(NOCOVID|-) = 1 - P(COVID|-) = 1 - \frac{(1-p_d)p_{\theta}}{(1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta})}$ b) Assume $p_d=0.8$ and $p_f=0.5$ , and assume that the prior probability any person has COVID is 0.1. Given a person has COVID, how many positive tests in a row do they need to take to be 99% confident they have it? 99.9%? $1-P(COVID|+)^{n} \geq 0.99$ $\implies 1-\Big[ \frac{(0.8)(0.1)}{(0.8)(0.1)+(0.5)(1-0.1)} \Big]^{n}\geq 0.99$ $\implies 1-(0.15)^n\geq 0.99$ $n\geq 2.4$ Need at least three tests. For 99.9%, perform the same calculation but with 0.999 instead of 0.99. We get $1-(0.15)^n \geq 0.999$ $\implies n \geq 3.6$ Need at least 4 tests","Assume a COVID test can identify the presence of COVID, given that the person has COVID, with probability . Assume the test assigns false positives with probability (a test will be positive but the person does not have COVID). Let be the prior probability a person has COVID. a) Calculate the probability that a test subject has COVID, given a test was positive. Do the same given the test was negative. Calculate the probability the test subject doesn’t have COVID given the test was positive, and then do the same for a negative test. *** Using law of total probability for : *** *** *** b) Assume and , and assume that the prior probability any person has COVID is 0.1. Given a person has COVID, how many positive tests in a row do they need to take to be 99% confident they have it? 99.9%? Need at least three tests. For 99.9%, perform the same calculation but with 0.999 instead of 0.99. We get Need at least 4 tests",p_d p_f p_{\theta} P(COVID|+)=\frac{P(+|COVID)P(COVID)}{P(+)} P(+) P(+)=P(+|COVID)P(COVID)+P(+|NOCOVID)P(NOCOVID) =p_dp_{\theta}+p_f(1-p_{\theta}) \implies P(COVID|+)=\frac{p_{d}p_{\theta}}{p_dp_{\theta}+p_f(1-p_{\theta})} P(COVID|-)= \frac{P(-|COVID)P(COVID)}{P(-)} P(-)=P(-|COVID)P(COVID)+P(-|NOCOVID)P(NOCOVID) = (1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta}) \implies P(COVID|-)=\frac{(1-p_d)p_{\theta}}{(1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta})} P(NOCOVID|+)= 1-P(COVID|+) = 1 - \frac{p_{d}p_{\theta}}{p_dp_{\theta}+p_f(1-p_{\theta})} P(NOCOVID|-) = 1 - P(COVID|-) = 1 - \frac{(1-p_d)p_{\theta}}{(1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta})} p_d=0.8 p_f=0.5 1-P(COVID|+)^{n} \geq 0.99 \implies 1-\Big[ \frac{(0.8)(0.1)}{(0.8)(0.1)+(0.5)(1-0.1)} \Big]^{n}\geq 0.99 \implies 1-(0.15)^n\geq 0.99 n\geq 2.4 1-(0.15)^n \geq 0.999 \implies n \geq 3.6,"['probability', 'bayes-theorem']"
54,Sum of non i.i.d. Bernoulli and Le Cam's theorem,Sum of non i.i.d. Bernoulli and Le Cam's theorem,,"Consider a (deterministic) sequence $(p_i)_i$ such that $p_i \in (0, 1)$ and $\sum_{I}^n p_i = T_n \rightarrow T < +\infty$ . Then define a sequence of independent Bernoulli random variables $B_i \sim \text{Be}(p_i)$ . From le Cam's theorem, I know that $S_n = B_1 + \cdots + B_n$ is approximately Poisson distributed with parameter $T_n$ . Is there a result that shows that $S_{\infty} = \lim_{n \rightarrow \infty} S_n$ is Poisson distributed with parameter $T$ ? I know of similar limit results when $T_n$ is constant across $n$ (see Chen, 1974 at https://www.jstor.org/stable/2959300 ), but I'm missing something with this more general setting. EDIT: I have found some places where the result I'm looking for is stated (e.g., https://d-nb.info/1197139125/34 , https://stats.stackexchange.com/questions/391461/limit-behavior-of-weighted-poisson-binomial-distribution ) but no proof is given. Looking at Le Cam's statement, it seems that the upper bound of the total variation distance between the law of $S_n$ and the Poisson distribution could be large if some of the $p_i$ 's do not tend to zero.","Consider a (deterministic) sequence such that and . Then define a sequence of independent Bernoulli random variables . From le Cam's theorem, I know that is approximately Poisson distributed with parameter . Is there a result that shows that is Poisson distributed with parameter ? I know of similar limit results when is constant across (see Chen, 1974 at https://www.jstor.org/stable/2959300 ), but I'm missing something with this more general setting. EDIT: I have found some places where the result I'm looking for is stated (e.g., https://d-nb.info/1197139125/34 , https://stats.stackexchange.com/questions/391461/limit-behavior-of-weighted-poisson-binomial-distribution ) but no proof is given. Looking at Le Cam's statement, it seems that the upper bound of the total variation distance between the law of and the Poisson distribution could be large if some of the 's do not tend to zero.","(p_i)_i p_i \in (0, 1) \sum_{I}^n p_i = T_n \rightarrow T < +\infty B_i \sim \text{Be}(p_i) S_n = B_1 + \cdots + B_n T_n S_{\infty} = \lim_{n \rightarrow \infty} S_n T T_n n S_n p_i","['probability', 'probability-theory', 'poisson-distribution', 'probability-limit-theorems', 'bernoulli-distribution']"
55,Average value from summed uniform rolls with threshold,Average value from summed uniform rolls with threshold,,"Suppose you roll an 6-sided dice and sum the values until a threshold is reached. When that threshold is reached any overflowing amount is discarded. What is the average dice roll value, depending on that threshold value? For a threshold of 1 , the average roll would also be 1 , since any dice roll is capped to a value of 1 . For an infinitely large threshold, the average roll would be 3.5 . What would be the formula to calculate the average value of a roll for a N -sided dice and a T threshold? Simulations I made using python :","Suppose you roll an 6-sided dice and sum the values until a threshold is reached. When that threshold is reached any overflowing amount is discarded. What is the average dice roll value, depending on that threshold value? For a threshold of 1 , the average roll would also be 1 , since any dice roll is capped to a value of 1 . For an infinitely large threshold, the average roll would be 3.5 . What would be the formula to calculate the average value of a roll for a N -sided dice and a T threshold? Simulations I made using python :",,"['probability', 'discrete-mathematics', 'dice', 'stopping-times']"
56,The catch in proving that a singleton is a Borel set?,The catch in proving that a singleton is a Borel set?,,"Let $\Omega= (0,1]$ . Let $\mathcal{C}$ be the collection of all open interval in $(0,1]$ . By open interval in $(0,1]$ I mean any interval of the form $(a,b)$ , where $a\geq 0$ and $b\leq 1$ . Let $\sigma(\mathcal{C})$ be the smallest sigma-algebra that contains $\mathcal{C}$ . We call this sigma-algebra a Borel-sigma algebra. Now, let us we have to prove that the singleton $\{0.7\}$ is in $\sigma(\mathcal{C})$ . In order to prove it, we use the following result: \begin{equation} \{0.7\}=\bigcap_{n=1}^{\infty}\Big[(0.7-1/n, 0.7+1/n)\cap(0,1]\Big]. \end{equation} The above result is based on the fact that \begin{equation} (0.7-1/n, 0.7+1/n)\cap(0,1]\in\mathcal{C}\in\sigma(\mathcal{C}), \mbox{for}\ n=2,3,4... \end{equation} Therefore the intersection of $\bigcap_{n=1}^{\infty}\Big[(0.7-1/n, 0.7+1/n)\cap(0,1]\Big]$ would be contained in $\sigma(\mathcal{C})$ . However, here is the catch, for $n=2$ , we have \begin{equation} (0.7-1/n, 0.7+1/n)\cap(0,1]= (0.2, 1.2)\cap(0,1]=(0.2, 1]\notin \mathcal{C} \end{equation} Therefore the quantity $\bigcap_{n=1}^{\infty}\Big[(0.7-1/n, 0.7+1/n)\cap(0,1]\Big]$ does not have to be contained in $\sigma(\mathcal{C})$ . Now, can anybody help me to figure out how my argument is not valid?","Let . Let be the collection of all open interval in . By open interval in I mean any interval of the form , where and . Let be the smallest sigma-algebra that contains . We call this sigma-algebra a Borel-sigma algebra. Now, let us we have to prove that the singleton is in . In order to prove it, we use the following result: The above result is based on the fact that Therefore the intersection of would be contained in . However, here is the catch, for , we have Therefore the quantity does not have to be contained in . Now, can anybody help me to figure out how my argument is not valid?","\Omega= (0,1] \mathcal{C} (0,1] (0,1] (a,b) a\geq 0 b\leq 1 \sigma(\mathcal{C}) \mathcal{C} \{0.7\} \sigma(\mathcal{C}) \begin{equation}
\{0.7\}=\bigcap_{n=1}^{\infty}\Big[(0.7-1/n, 0.7+1/n)\cap(0,1]\Big].
\end{equation} \begin{equation}
(0.7-1/n, 0.7+1/n)\cap(0,1]\in\mathcal{C}\in\sigma(\mathcal{C}), \mbox{for}\ n=2,3,4...
\end{equation} \bigcap_{n=1}^{\infty}\Big[(0.7-1/n, 0.7+1/n)\cap(0,1]\Big] \sigma(\mathcal{C}) n=2 \begin{equation}
(0.7-1/n, 0.7+1/n)\cap(0,1]= (0.2, 1.2)\cap(0,1]=(0.2, 1]\notin \mathcal{C}
\end{equation} \bigcap_{n=1}^{\infty}\Big[(0.7-1/n, 0.7+1/n)\cap(0,1]\Big] \sigma(\mathcal{C})","['probability', 'measure-theory', 'lebesgue-measure', 'borel-sets', 'borel-measures']"
57,Question on the proof of the Hahn Decomposition,Question on the proof of the Hahn Decomposition,,"I am reading this proof of the Hahn decomposition from Billingsley Probability and Measure. The Hahn Decomposition Theorem 32.1. For any additive set function $\varphi$ , there exist disjoint sets $A^{+}$ and $A^{-}$ such that $A^{+} \cup A^{-}=\Omega, \varphi(E) \geq 0$ for all $E$ in $A^{+}$ , and $\varphi(E) \leq 0$ for all $E$ in $A^{-}$ . A set $A$ is positive if $\varphi(E) \geq 0$ for $E \subset A$ and negative if $\varphi(E) \leq 0$ for $E \subset A$ . The $A^{+}$ and $A^{-}$ in the theorem decompose $\Omega$ into a positive and a negative set. This is the Hahn decomposition. Proof. Let $\alpha=\sup [\varphi(A): A \in \mathscr{F}]$ . Suppose that there exists a set $A^{+}$ satisfying $\varphi\left(A^{+}\right)=\alpha$ (which implies that $\alpha$ is finite). Let $A^{-}=\Omega-A^{+}$ . If $A \subset A^{+}$ and $\varphi(A)<0$ , then $\varphi\left(A^{+}-A\right)>\alpha$ , an impossibility; hence $A^{+}$ is a positive set. If $A \subset A^{-}$ and $\varphi(A)>0$ , then $\varphi\left(A^{+} \cup A\right)>\alpha$ , an impossibility; hence $A^{-}$ is a negative set. It is therefore only necessary to construct a set $A^{+}$ for which $\varphi\left(A^{+}\right)=\alpha$ . Choose sets $A_{n}$ such that $\varphi\left(A_{n}\right) \rightarrow \alpha$ , and let $A=\cup_{n} A_{n}$ . For each $n$ consider the $2^{n}$ sets $B_{n i}$ (some perhaps empty) that are intersections of the $B_{n}=\left[B_{k=1}: 1 \leq i \leq 2^{n}\right]$ of these sets partitions $A$ . Clearly, $A_{k}$ . The collection $\mathscr{B}_{n}=\left[B_{n i}: 1 \leq i \leq 2^{n}\right]$ of these sets partitions $A$ . Clearly, $\mathscr{B}_{n}$ refines $\mathscr{B}_{n-1}$ : Let $B_{n j}$ is contained in exactly one of the $B_{n-1, i}$ . Let $C_{n}$ be the union of those $B_{n i}$ in $\mathscr{B}_{n}$ for which $\varphi\left(B_{n i}\right)>0$ . At this point of the proof, I have a question. Why couldn't I conclude the theorem by constructing the set $A=\bigcup_n C_n$ becuase we have $\alpha =  \lim_{n\to \infty}\varphi(A_n)\leq \lim_{n\to \infty}\varphi(C_n)$ ? The proof goes on to say Since $A_{n}$ is the union of certain of the $B_{n i}$ , it follows that $\varphi\left(A_{n}\right) \leq \varphi\left(C_{n}\right)$ . Since the partitions $\mathscr{B}_{1}, \mathscr{B}_{2}, \ldots$ are successively finer, $m<n$ implies that $\left(C_{m} \cup \cdots \cup\right.$ $\left.C_{n-1} \cup C_{n}\right)-\left(C_{m} \cup \cdots \cup C_{n-1}\right)$ is the union (perhaps empty) of certain of the sets $B_{n i}$ ; the $B_{n i}$ in this union must satisfy $\varphi\left(B_{n i}\right)>0$ because they are contained in $C_{n}$ . Therefore, $\varphi\left(C_{m} \cup \cdots \cup C_{n-1}\right) \leq \varphi\left(C_{m} \cup \cdots \cup C_{n}\right)$ , so that by induction $\varphi\left(A_{m}\right) \leq \varphi\left(C_{m}\right) \leq \varphi\left(C_{m} \cup \cdots \cup C_{n}\right)$ . If $D_{m}=\cup_{n=m}^{\infty} C_{n}$ , then by Lemma 1 (take $\left.E_{v}=C_{m} \cup \cdots \cup C_{m+v}\right) \varphi\left(A_{m}\right) \leq \varphi\left(D_{m}\right)$ . Let $A^{+}=\cap_{m=1}^{\infty} D_{m}$ (note that $\left.A^{+}=\lim \sup _{n} C_{n}\right)$ , so that $D_{m} \downarrow A^{+}$ . By Lemma $1, \alpha=\lim _{m} \varphi\left(A_{m}\right)$ $\leq \lim _{m} \varphi\left(D_{m}\right)=\varphi\left(A^{+}\right)$ . Thus $A^{+}$ does have maximal $\varphi$ -value. but frankly, I am confused as to what the motivation is for going further as they have done. We want intuitively, the biggest possible set with positive signed measure. Why couldn't I just find this by unioning all the little partitions which have positive measure i.e. unioning the $C_n$ together?","I am reading this proof of the Hahn decomposition from Billingsley Probability and Measure. The Hahn Decomposition Theorem 32.1. For any additive set function , there exist disjoint sets and such that for all in , and for all in . A set is positive if for and negative if for . The and in the theorem decompose into a positive and a negative set. This is the Hahn decomposition. Proof. Let . Suppose that there exists a set satisfying (which implies that is finite). Let . If and , then , an impossibility; hence is a positive set. If and , then , an impossibility; hence is a negative set. It is therefore only necessary to construct a set for which . Choose sets such that , and let . For each consider the sets (some perhaps empty) that are intersections of the of these sets partitions . Clearly, . The collection of these sets partitions . Clearly, refines : Let is contained in exactly one of the . Let be the union of those in for which . At this point of the proof, I have a question. Why couldn't I conclude the theorem by constructing the set becuase we have ? The proof goes on to say Since is the union of certain of the , it follows that . Since the partitions are successively finer, implies that is the union (perhaps empty) of certain of the sets ; the in this union must satisfy because they are contained in . Therefore, , so that by induction . If , then by Lemma 1 (take . Let (note that , so that . By Lemma . Thus does have maximal -value. but frankly, I am confused as to what the motivation is for going further as they have done. We want intuitively, the biggest possible set with positive signed measure. Why couldn't I just find this by unioning all the little partitions which have positive measure i.e. unioning the together?","\varphi A^{+} A^{-} A^{+} \cup A^{-}=\Omega, \varphi(E) \geq 0 E A^{+} \varphi(E) \leq 0 E A^{-} A \varphi(E) \geq 0 E \subset A \varphi(E) \leq 0 E \subset A A^{+} A^{-} \Omega \alpha=\sup [\varphi(A): A \in \mathscr{F}] A^{+} \varphi\left(A^{+}\right)=\alpha \alpha A^{-}=\Omega-A^{+} A \subset A^{+} \varphi(A)<0 \varphi\left(A^{+}-A\right)>\alpha A^{+} A \subset A^{-} \varphi(A)>0 \varphi\left(A^{+} \cup A\right)>\alpha A^{-} A^{+} \varphi\left(A^{+}\right)=\alpha A_{n} \varphi\left(A_{n}\right) \rightarrow \alpha A=\cup_{n} A_{n} n 2^{n} B_{n i} B_{n}=\left[B_{k=1}: 1 \leq i \leq 2^{n}\right] A A_{k} \mathscr{B}_{n}=\left[B_{n i}: 1 \leq i \leq 2^{n}\right] A \mathscr{B}_{n} \mathscr{B}_{n-1} B_{n j} B_{n-1, i} C_{n} B_{n i} \mathscr{B}_{n} \varphi\left(B_{n i}\right)>0 A=\bigcup_n C_n \alpha =  \lim_{n\to \infty}\varphi(A_n)\leq \lim_{n\to \infty}\varphi(C_n) A_{n} B_{n i} \varphi\left(A_{n}\right) \leq \varphi\left(C_{n}\right) \mathscr{B}_{1}, \mathscr{B}_{2}, \ldots m<n \left(C_{m} \cup \cdots \cup\right. \left.C_{n-1} \cup C_{n}\right)-\left(C_{m} \cup \cdots \cup C_{n-1}\right) B_{n i} B_{n i} \varphi\left(B_{n i}\right)>0 C_{n} \varphi\left(C_{m} \cup \cdots \cup C_{n-1}\right) \leq \varphi\left(C_{m} \cup \cdots \cup C_{n}\right) \varphi\left(A_{m}\right) \leq \varphi\left(C_{m}\right) \leq \varphi\left(C_{m} \cup \cdots \cup C_{n}\right) D_{m}=\cup_{n=m}^{\infty} C_{n} \left.E_{v}=C_{m} \cup \cdots \cup C_{m+v}\right) \varphi\left(A_{m}\right) \leq \varphi\left(D_{m}\right) A^{+}=\cap_{m=1}^{\infty} D_{m} \left.A^{+}=\lim \sup _{n} C_{n}\right) D_{m} \downarrow A^{+} 1, \alpha=\lim _{m} \varphi\left(A_{m}\right) \leq \lim _{m} \varphi\left(D_{m}\right)=\varphi\left(A^{+}\right) A^{+} \varphi C_n","['probability', 'probability-theory', 'analysis', 'probability-distributions', 'conditional-probability']"
58,Five bags of distinct colour each bag has 5 five balls of corresponding colour. One ball is transferred to other bag from each bag. Find probability.,Five bags of distinct colour each bag has 5 five balls of corresponding colour. One ball is transferred to other bag from each bag. Find probability.,,"There are $5$ bags of colours Blue, Green, Pink, Red and yellow. Each of these bags have $5$ balls of same colour as that of the bag. A ball is drawn from blue bag and is transferred to one of the other bags. Then, a ball is drawn from green bag and transferred to one of the other bags. Similarly, $1$ ball is transferred from the pink bag, then from red bag and then from yellow bag (to one of the other bags). Find the probability that at the end each bag has $5$ balls of same colour, given that there are $5$ balls in each bag at the end. My working: To keep the number of balls same in each bag, none of the bags must receive more than 1 ball i.e., $𝐷_5=44$ cases to do it. All Favourable Cycles: (1) $B\to G\to B; P\to R\to Y\to P$ (2) $B\to P\to B; G\to R\to Y\to G$ (3) $B\to R\to B; G\to P\to Y\to G$ (4) $B\to Y\to B; G\to P\to R\to G$ (5) $B\to G\to P\to B; R\to Y\to R$ (6) $B\to G\to R\to B; P\to Y\to P$ (7) $B\to G\to Y\to B; P\to R\to P$ (8) $B\to P\to R\to B; G\to Y\to G$ (9) $B\to P\to Y\to B; G\to R\to G$ (10) $B\to R\to Y\to B; G\to P\to G$ (11) $B\to G\to P\to R\to Y\to B$ Now, P(Each bags content remain same/Each bag has 5 balls at the end) $=\frac{\left[10\times 1\times \left(\frac16\right)^3\times \left(\frac14\right)^5+1\times\left(\frac16\right)^4\left(\frac14\right)^5\right]}{44\times\left(\frac14\right)^5}=\frac{61\times \left(\frac16\right)^4\times \left(\frac14\right)^5}{44\times\left(\frac14\right)^5}=\frac{61}{57024}$","There are bags of colours Blue, Green, Pink, Red and yellow. Each of these bags have balls of same colour as that of the bag. A ball is drawn from blue bag and is transferred to one of the other bags. Then, a ball is drawn from green bag and transferred to one of the other bags. Similarly, ball is transferred from the pink bag, then from red bag and then from yellow bag (to one of the other bags). Find the probability that at the end each bag has balls of same colour, given that there are balls in each bag at the end. My working: To keep the number of balls same in each bag, none of the bags must receive more than 1 ball i.e., cases to do it. All Favourable Cycles: (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) Now, P(Each bags content remain same/Each bag has 5 balls at the end)",5 5 1 5 5 𝐷_5=44 B\to G\to B; P\to R\to Y\to P B\to P\to B; G\to R\to Y\to G B\to R\to B; G\to P\to Y\to G B\to Y\to B; G\to P\to R\to G B\to G\to P\to B; R\to Y\to R B\to G\to R\to B; P\to Y\to P B\to G\to Y\to B; P\to R\to P B\to P\to R\to B; G\to Y\to G B\to P\to Y\to B; G\to R\to G B\to R\to Y\to B; G\to P\to G B\to G\to P\to R\to Y\to B =\frac{\left[10\times 1\times \left(\frac16\right)^3\times \left(\frac14\right)^5+1\times\left(\frac16\right)^4\left(\frac14\right)^5\right]}{44\times\left(\frac14\right)^5}=\frac{61\times \left(\frac16\right)^4\times \left(\frac14\right)^5}{44\times\left(\frac14\right)^5}=\frac{61}{57024},"['probability', 'conditional-probability']"
59,Professor gets wet,Professor gets wet,,"Problem : A professor has $N$ umbrellas . He walks to the office in the morning and walks home in the evening . If it's raining he likes to carry an umbrella and if it's fine he doesn't . Suppose that it rains on each journey with probability $p$ , independently of past weather . What's the long-run proportion of journeys on which the professor gets wet ? Question : I want to see if my plan below is viable . Or you can share your approach . (The problem's cited directly from exercise 1.10.2 of Markov Chains by James R. Norris .) My interpretation : Suppose now professor leaves his home . If he sees it's raining ,  he will pick up an umbrella , if any , and he'll not get wet , otherwise he will . The umbrella will follow him to office , which he may not bring back .  If he sees it's not raining , he'll just walk out , and he'll not get wet because I assumed it'll not rain in the middle of his journey .  By journey I mean each commute between home & office , so 2 journeys every day . ""The long-run proportion of journeys ..."" means $\frac{\text{ no.journeys he gets wet}}{ \text{total no. of journeys } }$ when total no. of journeys tends to infinity . My plan : Assume $(1-p)p \neq 0 , N<\infty$ . Let $(X_n)_{n\ge 0}$ be the number of umbrellas at his home at night , suppose $X_0 = N$ . The state space is $ \{ 0,...,N \}$ and transition probabilities $$ \left\{ \begin{array}{cc} p_{i,i+1} = p_{i,i-1} = (1-p)p , & p_{ii} = 1 - 2(1-p)p , \; i= 1,..,N-1 \\ p_{01} =  p , &p_{00} = 1 - p   \\ p_{N,N-1} =  (1-p)p , & p_{NN} = 1 - (1-p)p   \end{array}\right. $$ . This time homogeneous markov chain is irreducible and recurrent . Let $$ Y_n = \left\{ \begin{array}{cc} 1 & \text{ if } \{\text{Professor gets wet tomorrow}\}\\ 0 & \text{otherwise}  \end{array}\right. $$ where $$ \{\text{Professor gets wet tomorrow}\} =  (\{ X_n = 0 \} \cap \{ \text{ rains tomorrow morning } \} )\\ \cup   (\{ X_n = N \} \cap \{ \text{ doesn't rain tomorrow morning } \} \cap   \{ \text{ rains tomorrow evening } \} ) $$ Note that he will only get wet on at most 1 journey each day .  Let $Z_n=(X_n,Y_n)$ , the state space is $$  \{(0,1)  , (N,1)\} \cup  \{(0,0) , ... , (N,0) \}  $$ The transition probabilities (please scroll) $$ \left\{ \begin{array}{cccc} p_{(i,0)(i+1,0)} = p_{(i,0)(i-1,0)} = (1-p)p , & p_{(i,0)(i,0)} = 1 - 2(1-p)p , \; i= 2,..,N-2 \\ p_{(1,0)(0,0)} =  (1-p)^2p , & p_{(1,0)(0,1)} =  (1-p)p^2  ,  & p_{(1,0)(2,0)} = (1-p)p ,  &  p_{(1,0)(1,0)} =  1 - 2(1-p)p    \\ p_{(N-1,0)(N,0)} =  (1-p)p(p+(1-p)^2) , & p_{(N-1,0)(N,1)} =  ((1-p)p)^2 , &  p_{(N-1,0)(N-2,0)} = (1-p)p , & p_{(N-1,0)(N-1,0)} =  1 - 2(1-p)p \\ p_{(0,0)(0,1)} = (1-p)p , & p_{(0,0)(1,0)} = p  , &  p_{(0,0)(0,0)} = 1 - (1-p)p - p \\ p_{(N,0)(N,1)} = \left(1 - \frac{(1-p)p}{p+(1-p)^2}\right)(1-p)p , & p_{(N,0)(N-1,0)} = \frac{(1-p)p}{p+(1-p)^2} , &  p_{(N,0)(N,0)} = 1 - \left(1 - \frac{(1-p)p}{p+(1-p)^2}\right)(1-p)p  -  \frac{(1-p)p}{p+(1-p)^2} \\ p_{(0,1)(0,0)} = (1-p)^2 , & p_{(0,1)(1,0)} = p , & p_{(0,1)(0,1)}  = 1 - (1-p)^2 - p \\ p_{(N,1)(N,0)} = p+(1-p)^2 , & p_{(N,1)(N-1,0)} = 0 , & p_{(N,1)(N,1)} = 1 - p-(1-p)^2 \end{array}\right. $$ $(Z_n)_{n\ge 0}$ is also a time homogeneous irreducible recurrent Markov chain . Informally , this Markov chain looks like 2 triangles connected to each end of a bar . My plan is to find the the expected return times $m_{(0,1)} , m_{(N,1)} $ to states $(0,1) , (N,1) $ resp. by solving linear equations using recurrence relation and minimization  , and the desired quantity will be $\frac{1}{m_{(0,1)}} + \frac{1}{m_{(N,1)}}  $ by Theorem 1.10.2  . Theorem 1.10.2  Let $P$ be irreducible and let $\lambda$ be any distribution . If $(X_n)_{n\ge 0}$ is Markov $(\lambda,P)$ then $$ \mathbb{P}\left( \frac{V_i(n)}{n} \to \frac{1}{m_i} \text{ as } n \to \infty  \right) = 1  $$ where $V_i(n) = \sum_{k=0}^{n-1} 1_{\{X_k = i\}}$ and $m_i$ is expected return time to state $i$ . (From Markov Chains by James R. Norris)","Problem : A professor has umbrellas . He walks to the office in the morning and walks home in the evening . If it's raining he likes to carry an umbrella and if it's fine he doesn't . Suppose that it rains on each journey with probability , independently of past weather . What's the long-run proportion of journeys on which the professor gets wet ? Question : I want to see if my plan below is viable . Or you can share your approach . (The problem's cited directly from exercise 1.10.2 of Markov Chains by James R. Norris .) My interpretation : Suppose now professor leaves his home . If he sees it's raining ,  he will pick up an umbrella , if any , and he'll not get wet , otherwise he will . The umbrella will follow him to office , which he may not bring back .  If he sees it's not raining , he'll just walk out , and he'll not get wet because I assumed it'll not rain in the middle of his journey .  By journey I mean each commute between home & office , so 2 journeys every day . ""The long-run proportion of journeys ..."" means when total no. of journeys tends to infinity . My plan : Assume . Let be the number of umbrellas at his home at night , suppose . The state space is and transition probabilities . This time homogeneous markov chain is irreducible and recurrent . Let where Note that he will only get wet on at most 1 journey each day .  Let , the state space is The transition probabilities (please scroll) is also a time homogeneous irreducible recurrent Markov chain . Informally , this Markov chain looks like 2 triangles connected to each end of a bar . My plan is to find the the expected return times to states resp. by solving linear equations using recurrence relation and minimization  , and the desired quantity will be by Theorem 1.10.2  . Theorem 1.10.2  Let be irreducible and let be any distribution . If is Markov then where and is expected return time to state . (From Markov Chains by James R. Norris)","N p \frac{\text{ no.journeys he gets wet}}{ \text{total no. of journeys } } (1-p)p \neq 0 , N<\infty (X_n)_{n\ge 0} X_0 = N  \{ 0,...,N \} 
\left\{
\begin{array}{cc}
p_{i,i+1} = p_{i,i-1} = (1-p)p , & p_{ii} = 1 - 2(1-p)p , \; i= 1,..,N-1 \\
p_{01} =  p , &p_{00} = 1 - p   \\
p_{N,N-1} =  (1-p)p , & p_{NN} = 1 - (1-p)p  
\end{array}\right.
 
Y_n =
\left\{
\begin{array}{cc}
1 & \text{ if } \{\text{Professor gets wet tomorrow}\}\\
0 & \text{otherwise} 
\end{array}\right.
 
\{\text{Professor gets wet tomorrow}\} = 
(\{ X_n = 0 \} \cap \{ \text{ rains tomorrow morning } \} )\\ \cup 
 (\{ X_n = N \} \cap \{ \text{ doesn't rain tomorrow morning } \} \cap   \{ \text{ rains tomorrow evening } \} )
 Z_n=(X_n,Y_n) 
 \{(0,1)  , (N,1)\} \cup  \{(0,0) , ... , (N,0) \} 
 
\left\{
\begin{array}{cccc}
p_{(i,0)(i+1,0)} = p_{(i,0)(i-1,0)} = (1-p)p , & p_{(i,0)(i,0)} = 1 - 2(1-p)p , \; i= 2,..,N-2 \\
p_{(1,0)(0,0)} =  (1-p)^2p , & p_{(1,0)(0,1)} =  (1-p)p^2  , 
& p_{(1,0)(2,0)} = (1-p)p ,  &  p_{(1,0)(1,0)} =  1 - 2(1-p)p    \\
p_{(N-1,0)(N,0)} =  (1-p)p(p+(1-p)^2) , & p_{(N-1,0)(N,1)} =  ((1-p)p)^2 , & 
p_{(N-1,0)(N-2,0)} = (1-p)p , & p_{(N-1,0)(N-1,0)} =  1 - 2(1-p)p \\
p_{(0,0)(0,1)} = (1-p)p , & p_{(0,0)(1,0)} = p  , &  p_{(0,0)(0,0)} = 1 - (1-p)p - p \\
p_{(N,0)(N,1)} = \left(1 - \frac{(1-p)p}{p+(1-p)^2}\right)(1-p)p , & p_{(N,0)(N-1,0)} = \frac{(1-p)p}{p+(1-p)^2} , &  p_{(N,0)(N,0)} = 1 - \left(1 - \frac{(1-p)p}{p+(1-p)^2}\right)(1-p)p  -  \frac{(1-p)p}{p+(1-p)^2} \\
p_{(0,1)(0,0)} = (1-p)^2 , & p_{(0,1)(1,0)} = p , & p_{(0,1)(0,1)}  = 1 - (1-p)^2 - p \\
p_{(N,1)(N,0)} = p+(1-p)^2 , & p_{(N,1)(N-1,0)} = 0 , & p_{(N,1)(N,1)} = 1 - p-(1-p)^2
\end{array}\right.
 (Z_n)_{n\ge 0} m_{(0,1)} , m_{(N,1)}  (0,1) , (N,1)  \frac{1}{m_{(0,1)}} + \frac{1}{m_{(N,1)}}   P \lambda (X_n)_{n\ge 0} (\lambda,P) 
\mathbb{P}\left( \frac{V_i(n)}{n} \to \frac{1}{m_i} \text{ as } n \to \infty  \right) = 1 
 V_i(n) = \sum_{k=0}^{n-1} 1_{\{X_k = i\}} m_i i","['probability', 'discrete-mathematics', 'solution-verification', 'markov-chains', 'ergodic-theory']"
60,Remainder of Taylor expansion of function at random point,Remainder of Taylor expansion of function at random point,,"Given a function $f: \mathbb{R}^n \to \mathbb{R}$ , we can write its Taylor expansion about the point $m$ as $$ f(x) = f(m) + \nabla^T f(m) (x-m) + \frac{1}{2} (x-m)^TH_f(m)(x-m) + o(\|x-m\|^2), $$ where $H_f(m)$ is the Hessian of $f$ evaluated at $m$ . I am interested in the case where $X$ is a Gaussian random vector with mean $\mu$ and covariance $\Sigma$ , then plugging in we get $$ f(X) = f(\mu) + \nabla^T f(\mu) (X-\mu) + \frac{1}{2} (X-\mu)^TH_f(\mu)(X-\mu) + \text{Rem}_f(X, \mu). $$ What is the correct characterization of the remainder term? Clearly it is going to be a random variable, and intuitively it should be upper bounded (in probability) by $$ \text{Rem}_f(X, \mu) \le \|X-\mu\|^3 \sup_y \|D^{(3)}f(y) \|_{\text{op}}, $$ where the supremum is being taken over $y$ which is a point between $\mu$ and $X$ , so the domain is random. I am looking for a more rigorous expression or a reference to a text/notes that cover such issues. Generally speaking, in the statistics literature this kind of thing comes up under 'delta method' but in those cases you are dealing with a statistic (like the sample mean), and so the remainder can be dealt with by saying it vanishes at some rate as the sample increases. Here I am trying to characterise $f(X)$ where $X$ is a single random vector.","Given a function , we can write its Taylor expansion about the point as where is the Hessian of evaluated at . I am interested in the case where is a Gaussian random vector with mean and covariance , then plugging in we get What is the correct characterization of the remainder term? Clearly it is going to be a random variable, and intuitively it should be upper bounded (in probability) by where the supremum is being taken over which is a point between and , so the domain is random. I am looking for a more rigorous expression or a reference to a text/notes that cover such issues. Generally speaking, in the statistics literature this kind of thing comes up under 'delta method' but in those cases you are dealing with a statistic (like the sample mean), and so the remainder can be dealt with by saying it vanishes at some rate as the sample increases. Here I am trying to characterise where is a single random vector.","f: \mathbb{R}^n \to \mathbb{R} m 
f(x) = f(m) + \nabla^T f(m) (x-m) + \frac{1}{2} (x-m)^TH_f(m)(x-m) + o(\|x-m\|^2),
 H_f(m) f m X \mu \Sigma 
f(X) = f(\mu) + \nabla^T f(\mu) (X-\mu) + \frac{1}{2} (X-\mu)^TH_f(\mu)(X-\mu) + \text{Rem}_f(X, \mu).
 
\text{Rem}_f(X, \mu) \le \|X-\mu\|^3 \sup_y \|D^{(3)}f(y) \|_{\text{op}},
 y \mu X f(X) X","['probability', 'probability-theory', 'statistics', 'taylor-expansion']"
61,Show that a random variable uniformly distributed over the unit square is independent,Show that a random variable uniformly distributed over the unit square is independent,,"Let $X=(U,V)$ be a random variable which is uniformly distributed on $[0,1]^2$ . I want to show that $U$ is independent of $V$ . Let $A,B\in B([0,1])$ where $B([0,1])$ denotes the Borel Sigma algebra. We have, $$\begin{aligned}P(U\in A, V\in B) &= P((U,V)\in A\times B) \\&=1_{[0,1]^2}(A\times B) \\ &=1_{[0,1]}(A) \cdot 1_{[0,1]}(B) \\ &=P(U\in A)P(V\in B) \end{aligned}$$ Is my solution correct? What could I have done better?","Let be a random variable which is uniformly distributed on . I want to show that is independent of . Let where denotes the Borel Sigma algebra. We have, Is my solution correct? What could I have done better?","X=(U,V) [0,1]^2 U V A,B\in B([0,1]) B([0,1]) \begin{aligned}P(U\in A, V\in B) &= P((U,V)\in A\times B) \\&=1_{[0,1]^2}(A\times B) \\ &=1_{[0,1]}(A) \cdot 1_{[0,1]}(B) \\ &=P(U\in A)P(V\in B) \end{aligned}","['probability', 'probability-theory', 'solution-verification']"
62,Strong mixing of a function of strong mixing and convergence sequence,Strong mixing of a function of strong mixing and convergence sequence,,"Suppose the process $X = \left\{X_{t}:t\in Z\right\}$ is strong mixing with the coefficient $\alpha(j) \rightarrow 0$ defined as \begin{equation} \alpha(j)=\sup_T\sup_{1\leq k\leq T-j}\sup\left\{\lvert P(A\cap B)-P(A)P(B)\rvert: B\in\mathcal{F}_{-\infty}^{k},A\in\mathcal{F}_{k+j}^{+\infty}\right\}, \end{equation} where $\mathcal{F}_{i}^{k}=\sigma\left(X_{l}:i\leq l\leq k\right)$ . Meanwhile, let $\beta = \{\beta_{t}:t\in Z\}$ be a process that is convergent to the constant $\beta_0$ in probability in the sense that for all $\epsilon>0\,,$ \begin{equation} \lim_{t\to\infty}P(|\beta_t - \beta_0|<\epsilon) = 0 \end{equation} My aim is to show that the process $Y = \{Y_t\}$ where $Y_t = f(X_t,\beta_t)$ is also a strong mixing for a Borel function $f$ . I know that if $X$ and $\beta$ are two independent strong mixing then $Y$ is also a strong mixing. But, here, $\beta$ and $X$ are not independent.","Suppose the process is strong mixing with the coefficient defined as where . Meanwhile, let be a process that is convergent to the constant in probability in the sense that for all My aim is to show that the process where is also a strong mixing for a Borel function . I know that if and are two independent strong mixing then is also a strong mixing. But, here, and are not independent.","X = \left\{X_{t}:t\in Z\right\} \alpha(j) \rightarrow 0 \begin{equation}
\alpha(j)=\sup_T\sup_{1\leq k\leq T-j}\sup\left\{\lvert P(A\cap B)-P(A)P(B)\rvert: B\in\mathcal{F}_{-\infty}^{k},A\in\mathcal{F}_{k+j}^{+\infty}\right\},
\end{equation} \mathcal{F}_{i}^{k}=\sigma\left(X_{l}:i\leq l\leq k\right) \beta = \{\beta_{t}:t\in Z\} \beta_0 \epsilon>0\,, \begin{equation} \lim_{t\to\infty}P(|\beta_t - \beta_0|<\epsilon) = 0 \end{equation} Y = \{Y_t\} Y_t = f(X_t,\beta_t) f X \beta Y \beta X","['probability', 'probability-theory', 'convergence-divergence', 'mixing']"
63,find the probability both planes can park at a gate,find the probability both planes can park at a gate,,"Two airplanes are supposed to park at the same gate of a concourse. The arrival times of the airplanes are independent and randomly distributed throughout the 24 hours of the day. What is the probability both can park at the gate, provided the first to arrive will stay for two hours, while the second can wait behind it for one hour? The question seems a little ambiguous to me, and I think an example would help. Suppose the first plane arrives at hour 0. Then it'll stay for 2 hours, so as long as the second arrives within that time, can't both airplanes park? What's the significance of saying ""The second can wait behind the first one for one hour""? I found the solution below, but I have some questions about it: Can someone elaborate on why the desired region is obtained by removing the points satisfying $x-1 \leq y \leq x+1$ and $x-2\leq y\leq x+2$ ?","Two airplanes are supposed to park at the same gate of a concourse. The arrival times of the airplanes are independent and randomly distributed throughout the 24 hours of the day. What is the probability both can park at the gate, provided the first to arrive will stay for two hours, while the second can wait behind it for one hour? The question seems a little ambiguous to me, and I think an example would help. Suppose the first plane arrives at hour 0. Then it'll stay for 2 hours, so as long as the second arrives within that time, can't both airplanes park? What's the significance of saying ""The second can wait behind the first one for one hour""? I found the solution below, but I have some questions about it: Can someone elaborate on why the desired region is obtained by removing the points satisfying and ?",x-1 \leq y \leq x+1 x-2\leq y\leq x+2,"['probability', 'geometry', 'contest-math']"
64,Finding the expected stopping time of random walk,Finding the expected stopping time of random walk,,"been stuck on this optional stopping theorem quesiton for some time. I feel like I'm heading in the right direction but I am not sure if I am right. Problem Consider a random walk $\left(S_{n}\right)_{n}$ with i.i.d. increments $$ X_{i}= \begin{cases}-1 & 1 / 2 \\ 0 & 1 / 4 \\ 1 & 1 / 4\end{cases} $$ Suppose $S_{0}=0$ . Let $T=\inf \left\{n: S_{n}\text{ is either }100\text{ or }-10\right\}$ . Find $E[T]$ . Find $\mathbb{P}\left(S_{T}=100\right)$ . My attempt $$E[x_i]=(-1) \cdot \frac{1}{2}+(0) \cdot \frac{1}{4}+(1) \cdot \frac{1}{4} = -\frac{1}{4}$$ We need a martingale to use optional stopping theorem. Hence, I used: $$M_{n}=\sum^{n} x_{i}+\frac{n}{4}$$ I showed this a martingale. However I am stuck on how to move forward from here: $$E\left[M_{T}\right]=E\left[\sum^{T} x_{i}+\frac{T}{4}\right]$$ $$E[T]=\frac{- E\left[S_{T}\right]}{4}$$ So would this mean $E[T] = -400$ or $40$ depending if $S_{n} = -100$ or $10$ .","been stuck on this optional stopping theorem quesiton for some time. I feel like I'm heading in the right direction but I am not sure if I am right. Problem Consider a random walk with i.i.d. increments Suppose . Let . Find . Find . My attempt We need a martingale to use optional stopping theorem. Hence, I used: I showed this a martingale. However I am stuck on how to move forward from here: So would this mean or depending if or .","\left(S_{n}\right)_{n} 
X_{i}= \begin{cases}-1 & 1 / 2 \\ 0 & 1 / 4 \\ 1 & 1 / 4\end{cases}
 S_{0}=0 T=\inf \left\{n: S_{n}\text{ is either }100\text{ or }-10\right\} E[T] \mathbb{P}\left(S_{T}=100\right) E[x_i]=(-1) \cdot \frac{1}{2}+(0) \cdot \frac{1}{4}+(1) \cdot \frac{1}{4} = -\frac{1}{4} M_{n}=\sum^{n} x_{i}+\frac{n}{4} E\left[M_{T}\right]=E\left[\sum^{T} x_{i}+\frac{T}{4}\right] E[T]=\frac{- E\left[S_{T}\right]}{4} E[T] = -400 40 S_{n} = -100 10","['probability', 'expected-value', 'random-walk', 'stopping-times']"
65,Understanding why do we need $\delta>0$ in Lyapunov's condition?,Understanding why do we need  in Lyapunov's condition?,\delta>0,"I am showing that Lyapunov's condition $$ \exists\delta>0:\sum_{m=1}^ns_n^{-(2+\delta)}\mathbb{E}[(X_m)^{2+\delta}]\to0 $$ implies the Lindeberg's condition $$ \forall\epsilon>0:\sum_{m=1}^n \mathbb{E}\left[\left(\frac{X_m}{s_n}\right)^2\mathbb{1}_{\{X_m\geq\epsilon s_n\}}\right]\to0. $$ However, I do not see why the $\delta>0$ is needed. If we have $$ \sum_{m=1}^ns_n^{-2}\mathbb{E}[(X_m)^{2}]\to0 $$ then apparently Lindeberg's condition holds since $\left(\frac{X_m}{s_n}\right)^2>\left(\frac{X_m}{s_n}\right)^2\mathbb{1}_{\{X_m\geq\epsilon s_n\}}\geq0,\forall\epsilon>0$ . I cannot identify the problem in the argument above. I am aware of a duplicate of this question ( here ) but I fail to see why the answerer suggests that ""The $\dfrac{1}{c\delta}$ with $\delta>0$ is needed to make this go to 0"" and the counterexample (it doesn't satisfy the Lyapunov's condition in first place). May someone help me identify the errors I made in my arguments and understand how $\delta>0$ is needed? Any input is appreciated.","I am showing that Lyapunov's condition implies the Lindeberg's condition However, I do not see why the is needed. If we have then apparently Lindeberg's condition holds since . I cannot identify the problem in the argument above. I am aware of a duplicate of this question ( here ) but I fail to see why the answerer suggests that ""The with is needed to make this go to 0"" and the counterexample (it doesn't satisfy the Lyapunov's condition in first place). May someone help me identify the errors I made in my arguments and understand how is needed? Any input is appreciated.","
\exists\delta>0:\sum_{m=1}^ns_n^{-(2+\delta)}\mathbb{E}[(X_m)^{2+\delta}]\to0
 
\forall\epsilon>0:\sum_{m=1}^n \mathbb{E}\left[\left(\frac{X_m}{s_n}\right)^2\mathbb{1}_{\{X_m\geq\epsilon s_n\}}\right]\to0.
 \delta>0 
\sum_{m=1}^ns_n^{-2}\mathbb{E}[(X_m)^{2}]\to0
 \left(\frac{X_m}{s_n}\right)^2>\left(\frac{X_m}{s_n}\right)^2\mathbb{1}_{\{X_m\geq\epsilon s_n\}}\geq0,\forall\epsilon>0 \dfrac{1}{c\delta} \delta>0 \delta>0","['probability', 'convergence-divergence', 'central-limit-theorem']"
66,Calculating the Expected value of a function over a random vector,Calculating the Expected value of a function over a random vector,,"let $$(X_1, X_2, \dots, X_n)$$ be the order statistics of $n$ i.i.d. uniform random variables. that satisfies the following condition. $$0 < x_1 < x_2 < \dots < x_n < 1$$ now consider a continuous function $f : [0,1] \rightarrow \mathbb{R}$ we define the random variable $R$ to be: $$ R = \sum_{i = 0}^{n - 1}f(X_{i+1}) \times (X_{i+1} - X_i) \hspace{0.5cm}X_0  = 0 $$ I need to prove the expected value of R equals: $$ E[R] = \int_{0}^{1}f(t)(1-(1-t)^n)dt $$ What I've tried so far is that I tried to find the pdf of each $X_i$ using the following formula: $$ f_{X_k}(x_k) = \frac{n!}{(k-1)!(n-k)!}F_X^{k-1}(x)[1-F_X(x)]^{n-k}f_X(x) $$ where $f_{X}$ is the pdf of our Uniform random variable(not to be mistaken with the f function described above) and F is the CDF of the aforementioned random variable. Using the said pdf and CDF and the linearity of expected value seems to get me nowhere, and I'm stuck.","let be the order statistics of i.i.d. uniform random variables. that satisfies the following condition. now consider a continuous function we define the random variable to be: I need to prove the expected value of R equals: What I've tried so far is that I tried to find the pdf of each using the following formula: where is the pdf of our Uniform random variable(not to be mistaken with the f function described above) and F is the CDF of the aforementioned random variable. Using the said pdf and CDF and the linearity of expected value seems to get me nowhere, and I'm stuck.","(X_1, X_2, \dots, X_n) n 0 < x_1 < x_2 < \dots < x_n < 1 f : [0,1] \rightarrow \mathbb{R} R 
R = \sum_{i = 0}^{n - 1}f(X_{i+1}) \times (X_{i+1} - X_i) \hspace{0.5cm}X_0  = 0
 
E[R] = \int_{0}^{1}f(t)(1-(1-t)^n)dt
 X_i 
f_{X_k}(x_k) = \frac{n!}{(k-1)!(n-k)!}F_X^{k-1}(x)[1-F_X(x)]^{n-k}f_X(x)
 f_{X}","['probability', 'statistics', 'expected-value']"
67,"The order statistics of uniform distribution $U_{n,k_n}\rightarrow p$ a.s., when $\frac{K_n}{n} \rightarrow p$","The order statistics of uniform distribution  a.s., when","U_{n,k_n}\rightarrow p \frac{K_n}{n} \rightarrow p","Let $U_1,U_2,...U_n,$ be iid samples from uniform distribution $U(0,1)$ . And the order Statistic: \begin{equation} U_{n,1}\leq U_{n,2}\leq ...\leq U_{n,n}, \end{equation} Given $p\in(0,1)$ , if $1\leq K_n\leq n$ and $\frac{K_n}{n}\rightarrow p$ . Proof $U_{n,k_n}\rightarrow p$ a.s. My ideas so far: The density function of $U_{n,k_n}$ is \begin{equation} f_n(x)=\frac{n!}{(k_n-1)!(n-k_n)!}x^{k_n-1}(1-x)^{n-k_n}. \end{equation} Then I would like to proof: \begin{equation} \sum_n P(|U_{n,k_n}-p|>\epsilon)<\infty \end{equation} Then by Borel-Cantelli lemma: \begin{equation} P(|U_{n,k_n}-p|>\epsilon \quad\text{i.o.})=0 \end{equation} which means: \begin{equation} U_{n,k_n}\rightarrow p = 0. \quad a.s. \end{equation} However, the integral of $P(|U_{n,k_n}-p|>\epsilon)<\infty$ is hard to calculate. Do I miss something? Does there exist some easy way to proof this? Thanks in advance for any help!","Let be iid samples from uniform distribution . And the order Statistic: Given , if and . Proof a.s. My ideas so far: The density function of is Then I would like to proof: Then by Borel-Cantelli lemma: which means: However, the integral of is hard to calculate. Do I miss something? Does there exist some easy way to proof this? Thanks in advance for any help!","U_1,U_2,...U_n, U(0,1) \begin{equation}
U_{n,1}\leq U_{n,2}\leq ...\leq U_{n,n},
\end{equation} p\in(0,1) 1\leq K_n\leq n \frac{K_n}{n}\rightarrow p U_{n,k_n}\rightarrow p U_{n,k_n} \begin{equation}
f_n(x)=\frac{n!}{(k_n-1)!(n-k_n)!}x^{k_n-1}(1-x)^{n-k_n}.
\end{equation} \begin{equation}
\sum_n P(|U_{n,k_n}-p|>\epsilon)<\infty
\end{equation} \begin{equation}
P(|U_{n,k_n}-p|>\epsilon \quad\text{i.o.})=0
\end{equation} \begin{equation}
U_{n,k_n}\rightarrow p = 0. \quad a.s.
\end{equation} P(|U_{n,k_n}-p|>\epsilon)<\infty","['probability', 'probability-theory', 'probability-distributions', 'uniform-distribution', 'probability-limit-theorems']"
68,Probability that the histogram for the sum of two dice has an expected shape,Probability that the histogram for the sum of two dice has an expected shape,,"Suppose that two 6-sided dice are thrown $n$ times and that the sum after each throw is plotted on a histogram. Let $s_i$ be the frequency of the sum $i \in \{2, 3, \dots, 12 \}$ . As $n \rightarrow \infty$ , the probability that $s_i \le s_{i+1}$ for $i \in \{2, 3, \dots, 6\}$ and $s_{i} \ge s_{i+1}$ for $i \in \{7, 8, \dots, 11\}$ approaches $1$ (because the further the sum is from $7$ , the less likely it is to occur). But after only a finite number of throws, what is the probability that the above expression is true? Can this be expressed as a closed-form function of $n$ ?","Suppose that two 6-sided dice are thrown times and that the sum after each throw is plotted on a histogram. Let be the frequency of the sum . As , the probability that for and for approaches (because the further the sum is from , the less likely it is to occur). But after only a finite number of throws, what is the probability that the above expression is true? Can this be expressed as a closed-form function of ?","n s_i i \in \{2, 3, \dots, 12 \} n \rightarrow \infty s_i \le s_{i+1} i \in \{2, 3, \dots, 6\} s_{i} \ge s_{i+1} i \in \{7, 8, \dots, 11\} 1 7 n","['probability', 'dice']"
69,Order of variables of integration in double integral,Order of variables of integration in double integral,,"This is a question from the actuarial practice set: An insurance company insures a large number of drivers. Let X be the random variable representing the company’s losses under collision insurance, and let Y represent the company’s losses under liability insurance. X $$f(x,y )= \begin{cases}  \frac{2x + 2 + y}{4}&(x,y) \in [0,1] \times [0,2]\\       0\      &\text{otherwise} \end{cases}$$ Calculate the probability that the total company loss is at least 1. The solution suggests to integrate with respect to y first: $$ \int_0^1 \int_{1-x}^2 \frac{2x + 2 + y}{4} dy \, dx   $$ I thought this is more natural, yet it gives the wrong answer: $$ \int_0^2 \int_{1-y}^1 \frac{2x + 2 + y}{4} dx \, dy  $$ Why does the latter not work?","This is a question from the actuarial practice set: An insurance company insures a large number of drivers. Let X be the random variable representing the company’s losses under collision insurance, and let Y represent the company’s losses under liability insurance. X Calculate the probability that the total company loss is at least 1. The solution suggests to integrate with respect to y first: I thought this is more natural, yet it gives the wrong answer: Why does the latter not work?","f(x,y )=
\begin{cases}
 \frac{2x + 2 + y}{4}&(x,y) \in [0,1] \times [0,2]\\
      0\      &\text{otherwise}
\end{cases}  \int_0^1 \int_{1-x}^2 \frac{2x + 2 + y}{4} dy \, dx     \int_0^2 \int_{1-y}^1 \frac{2x + 2 + y}{4} dx \, dy  ","['calculus', 'probability', 'integration', 'definite-integrals']"
70,"Show that $\alpha(\mathcal A,\mathcal B)\leq 1/4$.",Show that .,"\alpha(\mathcal A,\mathcal B)\leq 1/4","Let $(\Omega,\mathcal F,P)$ be a probability space, and let $\mathcal A$ , $\mathcal B$ be sub- $\sigma$ -algebras of $\mathcal F$ . I'm asked to show that the $\alpha$ -mixing coefficient $$\alpha(\mathcal A,\mathcal B):=\sup\bigg\{\big|P(A\cap B)-P(A)P(B)\big|:A\in\mathcal A, B \in \mathcal B\bigg\}$$ between $\mathcal A$ and $\mathcal B$ is bounded above by $1/4$ . My attempt: Let $A\in\mathcal A$ and $B\in\mathcal B$ . Suppose first that $P(A)\geq P(B)$ . Then $$P(A\cap B)-P(A)P(B)\leq P(B)-P(B)^2=P(B)P(B^c)$$ and $$P(A)P(B)-P(A\cap B)=P(A\setminus B)P(B)+P(A\cap B)P(B)-P(A\cap B)$$ $$\leq P(B^c)P(B)+P(A\cap B)(P(B)-1)\leq P(B)P(B^c)$$ Therefore $$|P(A\cap B)-P(A)P(B)|\leq  P(B)P(B^c)\leq 1/4$$ where the last inequality is because the function $x\mapsto x-x^2$ has a unique maximum at $x=1/2$ . The case $P(A)\leq P(B)$ can be proven similarly. As $A\in\mathcal A$ and $B\in\mathcal B$ were arbitrary we conclude that $\alpha(\mathcal A,\mathcal B)\leq 1/4$ . Am I missing something? Thanks for your help.","Let be a probability space, and let , be sub- -algebras of . I'm asked to show that the -mixing coefficient between and is bounded above by . My attempt: Let and . Suppose first that . Then and Therefore where the last inequality is because the function has a unique maximum at . The case can be proven similarly. As and were arbitrary we conclude that . Am I missing something? Thanks for your help.","(\Omega,\mathcal F,P) \mathcal A \mathcal B \sigma \mathcal F \alpha \alpha(\mathcal A,\mathcal B):=\sup\bigg\{\big|P(A\cap B)-P(A)P(B)\big|:A\in\mathcal A, B \in \mathcal B\bigg\} \mathcal A \mathcal B 1/4 A\in\mathcal A B\in\mathcal B P(A)\geq P(B) P(A\cap B)-P(A)P(B)\leq P(B)-P(B)^2=P(B)P(B^c) P(A)P(B)-P(A\cap B)=P(A\setminus B)P(B)+P(A\cap B)P(B)-P(A\cap B) \leq P(B^c)P(B)+P(A\cap B)(P(B)-1)\leq P(B)P(B^c) |P(A\cap B)-P(A)P(B)|\leq  P(B)P(B^c)\leq 1/4 x\mapsto x-x^2 x=1/2 P(A)\leq P(B) A\in\mathcal A B\in\mathcal B \alpha(\mathcal A,\mathcal B)\leq 1/4","['probability', 'probability-theory']"
71,"$\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0,E[X_1^2]<\infty$",,"\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0,E[X_1^2]<\infty","$\{X_n\}$ are independent identical random variables, $S_n=X_1+...+X_n$ . Proof: for each $\epsilon>0,$ \begin{equation}  \sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0,E[X_1^2]<\infty \end{equation} My ideas so far: From Borel 0-1 law: \begin{equation}  \sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow p(\text{limsup}\frac{|Sn|}{n}\geq \epsilon )=0\Leftrightarrow \text{lim} \frac{|S_n|}{n}=0\Leftrightarrow\text{lim} \frac{S_n}{n}=0 \end{equation} Also, $\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow E|S_n|<\infty$ (From the formula $\sum p(|S_n|\geq n)\leq E|S_n|\leq 1+\sum p(|S_n|\geq n)$ ), then $E|X_n|<\infty$ . By strong law of large number, we have: \begin{equation} 0=\text{lim}\frac{Sn}{n}=EX_1 \end{equation} Which means: $\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0$ . I don't known how to prove or use the condition $E[X_1^2]<\infty$ . Are there something wrong with my proof? Thanks in advance for any tips or help in general.","are independent identical random variables, . Proof: for each My ideas so far: From Borel 0-1 law: Also, (From the formula ), then . By strong law of large number, we have: Which means: . I don't known how to prove or use the condition . Are there something wrong with my proof? Thanks in advance for any tips or help in general.","\{X_n\} S_n=X_1+...+X_n \epsilon>0, \begin{equation} 
\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0,E[X_1^2]<\infty
\end{equation} \begin{equation} 
\sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow p(\text{limsup}\frac{|Sn|}{n}\geq \epsilon )=0\Leftrightarrow \text{lim} \frac{|S_n|}{n}=0\Leftrightarrow\text{lim} \frac{S_n}{n}=0
\end{equation} \sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow E|S_n|<\infty \sum p(|S_n|\geq n)\leq E|S_n|\leq 1+\sum p(|S_n|\geq n) E|X_n|<\infty \begin{equation}
0=\text{lim}\frac{Sn}{n}=EX_1
\end{equation} \sum p(|S_n|\geq n\epsilon)<\infty\Leftrightarrow EX_1=0 E[X_1^2]<\infty","['probability', 'sequences-and-series', 'probability-theory']"
72,Compute the derivative of $E[1_{\{X\leq x\}}Y]$ w.r.t. $x$,Compute the derivative of  w.r.t.,E[1_{\{X\leq x\}}Y] x,"Let $X,Y$ be real random variable on the probability space $(\Omega,\mathcal F,P)$ , and suppose that $Y$ is $P$ -integrable and that $X$ has probability density $f$ with respect to the Lebesgue measure $\lambda$ . Am trying to show that $$\frac{d}{dx}E[1_{\{X\leq x\}}Y]=E[Y|X=x]f(x)$$ for $\lambda$ -almost every $x\in\mathbb R$ . Attempt: First lets recall the definition of $E[Y|X=x]$ . The Doob-Dynkin lemma implies the existence of a real-valued Borel measurable map $g:\mathbb R\mathbb \to \mathbb R$ such that $E[Y|\sigma(X)]=g\circ X$ $P$ -almost surely. Such map $g$ is $P_X$ -integrable and unique $P_X$ -almost surely. We let $E[Y|X=x]$ denote any version of $g$ . Now fix $x\in \mathbb R $ and compute using the properties of conditional expectation: $$E[1_{\{X\leq x\}}Y]=E[1_{\{X\leq x\}}(g\circ X)]=E[(1_{(-\infty,x]}g) \circ X]=\int 1_{(-\infty,x]}g \,dP_X=\int 1_{(-\infty,x]}g f \, d\lambda$$ Moreover the same calculation without $1_{\{X\leq x\}}$ shows that $g f$ is $\lambda$ -integrable. Hence we can invoke the Lebesgue differentiation theorem to obtain the result. Am I missing something? Thanks a lot for your help.","Let be real random variable on the probability space , and suppose that is -integrable and that has probability density with respect to the Lebesgue measure . Am trying to show that for -almost every . Attempt: First lets recall the definition of . The Doob-Dynkin lemma implies the existence of a real-valued Borel measurable map such that -almost surely. Such map is -integrable and unique -almost surely. We let denote any version of . Now fix and compute using the properties of conditional expectation: Moreover the same calculation without shows that is -integrable. Hence we can invoke the Lebesgue differentiation theorem to obtain the result. Am I missing something? Thanks a lot for your help.","X,Y (\Omega,\mathcal F,P) Y P X f \lambda \frac{d}{dx}E[1_{\{X\leq x\}}Y]=E[Y|X=x]f(x) \lambda x\in\mathbb R E[Y|X=x] g:\mathbb R\mathbb \to \mathbb R E[Y|\sigma(X)]=g\circ X P g P_X P_X E[Y|X=x] g x\in \mathbb R  E[1_{\{X\leq x\}}Y]=E[1_{\{X\leq x\}}(g\circ X)]=E[(1_{(-\infty,x]}g) \circ X]=\int 1_{(-\infty,x]}g \,dP_X=\int 1_{(-\infty,x]}g f \, d\lambda 1_{\{X\leq x\}} g f \lambda","['probability', 'probability-theory', 'lebesgue-integral', 'lebesgue-measure', 'conditional-expectation']"
73,Finding $\mathbb{E}[h(X)\mid X^2]$,Finding,\mathbb{E}[h(X)\mid X^2],"Suppose that $h: \mathbb{Z}\rightarrow \mathbb{R}$ is such that $\mathbb{E}[|h(Z)|]< \infty$ . I have to prove that $$\mathbb{E}[h(X)\mid X^2]=h(|X|)\dfrac{p(|X|)}{p(|X|)+p(-|X|)}+h(-|X|)\dfrac{p(-|X|)}{p(|X|)+p(-|X|)}.$$ I know that $$\psi(y)=\sum_x h(x)P(X=x\mid X^2=y)=\dfrac{\sum_x h(x)P(X=x \land X^2=y)}{P(X^2=y)}$$ if $y > 0$ , $\sum_x h(x)P(X=x \land X^2=y)$ has two expressions, because $x=\pm\sqrt y$ . How to write the solution correctly?","Suppose that is such that . I have to prove that I know that if , has two expressions, because . How to write the solution correctly?",h: \mathbb{Z}\rightarrow \mathbb{R} \mathbb{E}[|h(Z)|]< \infty \mathbb{E}[h(X)\mid X^2]=h(|X|)\dfrac{p(|X|)}{p(|X|)+p(-|X|)}+h(-|X|)\dfrac{p(-|X|)}{p(|X|)+p(-|X|)}. \psi(y)=\sum_x h(x)P(X=x\mid X^2=y)=\dfrac{\sum_x h(x)P(X=x \land X^2=y)}{P(X^2=y)} y > 0 \sum_x h(x)P(X=x \land X^2=y) x=\pm\sqrt y,"['probability', 'probability-theory', 'conditional-expectation']"
74,Expected value of random expressions,Expected value of random expressions,,"I'm trying to solve this math puzzle: write numbers $1$ to $N$ in a row. Randomly insert $+$ or $\times$ between two adjacent numbers with equal probability. What is the expected value of the expression if the expression is evaluated as an ordinary arithmetic expression? For instance, $1 + 2 \times 3$ will be evaluated as $1 + (2\times3)$ . Initially I thought that it was a simple recursion, but then I realized that $\times$ would change the precedence of the entire expression and then got stuck. It's easy to find a solution for a small enough $N$ with code, but I'm curious how one can solve it with math. Thanks,","I'm trying to solve this math puzzle: write numbers to in a row. Randomly insert or between two adjacent numbers with equal probability. What is the expected value of the expression if the expression is evaluated as an ordinary arithmetic expression? For instance, will be evaluated as . Initially I thought that it was a simple recursion, but then I realized that would change the precedence of the entire expression and then got stuck. It's easy to find a solution for a small enough with code, but I'm curious how one can solve it with math. Thanks,",1 N + \times 1 + 2 \times 3 1 + (2\times3) \times N,"['probability', 'puzzle']"
75,Show that a particular process is white noise,Show that a particular process is white noise,,"Given $0< p < 1$ and $T_t \overset{i.i.d.}{\sim} t _5 $ , Student's-t distribution with 5 degrees of freedom; $B_t \overset{i.i.d.}{\sim} B(1,p)$ , Bernoulli distrution. Define: $$\epsilon_t = B_t T_t, \, \forall\, t $$ I want to show that $\{\epsilon_t\}_{t \in \mathbb{Z}}$ is a white noise process. First, my lecture notes does not suppose any thing about the independence or dependence between $T_t$ and $B_t$ . There is a possibility that my reading notes are considering $T_t$ and $B_t$ independent, but not written. This would be a fault. So, is it possible to show that $\{\epsilon_t\}_{t \in \mathbb{Z}}$ is white noise not assuming the independence of $T_t$ and $B_t$ ? If they are independent, then $E(\epsilon_t) = E(T_t B_t) = E(T_t)E(B_t) = 0$ , because $E(T_t)=0$ . But if $T_t$ and $B_t$ are dependent? How about the other properties? $E(\epsilon_t^2) = \sigma^2 < \infty\,\, \forall t $ ; $E(\epsilon_t \epsilon_s) = 0, \,\,\forall s \neq t.$ Some help?","Given and , Student's-t distribution with 5 degrees of freedom; , Bernoulli distrution. Define: I want to show that is a white noise process. First, my lecture notes does not suppose any thing about the independence or dependence between and . There is a possibility that my reading notes are considering and independent, but not written. This would be a fault. So, is it possible to show that is white noise not assuming the independence of and ? If they are independent, then , because . But if and are dependent? How about the other properties? ; Some help?","0< p < 1 T_t \overset{i.i.d.}{\sim} t _5  B_t \overset{i.i.d.}{\sim} B(1,p) \epsilon_t = B_t T_t, \, \forall\, t  \{\epsilon_t\}_{t \in \mathbb{Z}} T_t B_t T_t B_t \{\epsilon_t\}_{t \in \mathbb{Z}} T_t B_t E(\epsilon_t) = E(T_t B_t) = E(T_t)E(B_t) = 0 E(T_t)=0 T_t B_t E(\epsilon_t^2) = \sigma^2 < \infty\,\, \forall t  E(\epsilon_t \epsilon_s) = 0, \,\,\forall s \neq t.","['probability', 'stochastic-processes']"
76,Markov chain returning to its starting position,Markov chain returning to its starting position,,"Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and let $(X_n)_{n \in \mathbb{N_0}}$ be an $I$ -valued (homogeneous) Markov chain, where $I \subset \mathbb{R}$ is countable. For $i \in I$ , set $$ T_i := \inf\{ n \geq 1 : X_n = i \} \quad (\inf \emptyset = + \infty), $$ which represents the first time $X$ takes the value $i$ starting from time $1$ . Furthermore, set $T_i^{(1)} := T_i$ and $$ T_i^{(k)} := \inf\{ n \geq T^{(k-1)}_i : X_n = i \}, \quad k \geq 2. $$ Now let $i, j \in \mathbb{N}$ be such that $i \neq j$ and $$ \mathbb{P}( T_j < T_i \mid X_0 = i ) = \mathbb{P}( T_i < T_j \mid X_0 = j ). \tag{1} $$ Denote by $\mathbb{P}_i (B) := \mathbb{P} ( B \mid X_0 = i )$ , $\mathbb{P}_j (B) := \mathbb{P} ( B \mid X_0 = j )$ , $B \in \mathscr{B}(\mathbb{R})$ . Task: Given $X_0=i$ , determine the expected number of visits to $j$ before the chain returns to $i$ . Hint: Let $N$ denote the number of visits to $j$ before it returns $i$ . Show that $$ \mathbb{P}_{i} (N \geq k) = \mathbb{P}_i (T_i > T_j) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1}. \tag{2} $$ My approach: One can first observe that $$ N = \sum_{ k = 1 }^{ \infty } \mathbb{1}_{\{ X_k = j \}} \mathbb{1}_{\{ T_i > k \}}, $$ which is well-defined as a countable sum of non-negative random variables. If one is able to show $(2)$ , then due to $(1)$ , it follows that $$ \mathbb{P}_{i} (N \geq k) = \mathbb{P}_i (T_i > T_j) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1} = \mathbb{P}_j (T_j > T_i) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1} = \alpha ( 1 - \alpha)^{k-1}, $$ where $\alpha : =  \mathbb{P}_j (T_j > T_i)$ . Since $N$ is non-negative, we can further use the formula $$ \mathbb{E}_i(N) = \sum_{ k = 1 }^{\infty} \mathbb{P}_i ( N \geq k ) = \sum_{ k = 1 }^{\infty} \alpha ( 1 - \alpha)^{k-1} = \sum_{ k = 0 }^{\infty} \alpha ( 1 - \alpha)^k = \frac{\alpha}{1 - (1 -\alpha )} = 1, $$ if $0< \alpha < 1$ ; otherwise $\mathbb{E}_i(N) = 0$ . But how can one show $(2)$ ? It is possible to observe that for $k \geq 1$ , we have $N \geq k$ if and only if the first time $X$ visits $i$ is strictly greater than the $k$ 'th time $X$ vsits $j$ , i.e., $$ \mathbb{P}_{ i }  ( N \geq k ) = \mathbb{P}_i ( T_i >  T^{(k)}_j ) = \ldots $$ Can this be transformed to $(2)$ ? If the answer includes the application of a certain property of Markov chains, I would kindly ask to state this property explicitly as well.","Let be a probability space and let be an -valued (homogeneous) Markov chain, where is countable. For , set which represents the first time takes the value starting from time . Furthermore, set and Now let be such that and Denote by , , . Task: Given , determine the expected number of visits to before the chain returns to . Hint: Let denote the number of visits to before it returns . Show that My approach: One can first observe that which is well-defined as a countable sum of non-negative random variables. If one is able to show , then due to , it follows that where . Since is non-negative, we can further use the formula if ; otherwise . But how can one show ? It is possible to observe that for , we have if and only if the first time visits is strictly greater than the 'th time vsits , i.e., Can this be transformed to ? If the answer includes the application of a certain property of Markov chains, I would kindly ask to state this property explicitly as well.","(\Omega, \mathcal{F}, \mathbb{P}) (X_n)_{n \in \mathbb{N_0}} I I \subset \mathbb{R} i \in I 
T_i := \inf\{ n \geq 1 : X_n = i \} \quad (\inf \emptyset = + \infty),
 X i 1 T_i^{(1)} := T_i 
T_i^{(k)} := \inf\{ n \geq T^{(k-1)}_i : X_n = i \}, \quad k \geq 2.
 i, j \in \mathbb{N} i \neq j 
\mathbb{P}( T_j < T_i \mid X_0 = i ) = \mathbb{P}( T_i < T_j \mid X_0 = j ). \tag{1}
 \mathbb{P}_i (B) := \mathbb{P} ( B \mid X_0 = i ) \mathbb{P}_j (B) := \mathbb{P} ( B \mid X_0 = j ) B \in \mathscr{B}(\mathbb{R}) X_0=i j i N j i 
\mathbb{P}_{i} (N \geq k) = \mathbb{P}_i (T_i > T_j) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1}. \tag{2}
 
N = \sum_{ k = 1 }^{ \infty } \mathbb{1}_{\{ X_k = j \}} \mathbb{1}_{\{ T_i > k \}},
 (2) (1) 
\mathbb{P}_{i} (N \geq k) = \mathbb{P}_i (T_i > T_j) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1} = \mathbb{P}_j (T_j > T_i) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1} = \alpha ( 1 - \alpha)^{k-1},
 \alpha : =  \mathbb{P}_j (T_j > T_i) N 
\mathbb{E}_i(N) = \sum_{ k = 1 }^{\infty} \mathbb{P}_i ( N \geq k ) = \sum_{ k = 1 }^{\infty} \alpha ( 1 - \alpha)^{k-1} = \sum_{ k = 0 }^{\infty} \alpha ( 1 - \alpha)^k = \frac{\alpha}{1 - (1 -\alpha )} = 1,
 0< \alpha < 1 \mathbb{E}_i(N) = 0 (2) k \geq 1 N \geq k X i k X j 
\mathbb{P}_{ i }  ( N \geq k ) = \mathbb{P}_i ( T_i >  T^{(k)}_j ) = \ldots
 (2)","['probability', 'probability-theory', 'stochastic-processes', 'markov-chains']"
77,"Example of 2 random variables s.t. $(X+Y)$ ~ $U(0,2)$",Example of 2 random variables s.t.  ~,"(X+Y) U(0,2)","In my book I found: Can you give an example of 2 random variables $X,Y$ S.T $(X+Y)$ ~ $U(0,2)$ and $X,Y$ are not independent. Any ideas of how I can find such 2 random variabes? I would prefer if those random variables tell a story so I can relate to the real world, for example selecting number in $[0,1]$ has uniform disturbution of $(0,1)$ .","In my book I found: Can you give an example of 2 random variables S.T ~ and are not independent. Any ideas of how I can find such 2 random variabes? I would prefer if those random variables tell a story so I can relate to the real world, for example selecting number in has uniform disturbution of .","X,Y (X+Y) U(0,2) X,Y [0,1] (0,1)","['probability', 'probability-distributions', 'random-variables', 'independence', 'uniform-distribution']"
78,Converting odds to probabilities,Converting odds to probabilities,,"Gambling odds on sports betting are designated with a number in the set $(-\infty , -100) \cup [+100 , +\infty)$ . If one places a wager of $w$ dollars at $p \in (-\infty , -100) \cup [+100 , +\infty)$ odds and your bet is successful, then your winnings $E(w , p)$ are calculated as follows: $$ E(w , p) : = \begin{cases}  \left( 1 + \dfrac{100}{p} \right)w & p < -100\\ \left(1 + \dfrac{p}{100} \right)w & p \geq +100. \end{cases}  $$ So large positive odds correspond to events that are less likely to happen and thus have greater payouts while large negative odds correspond to events that are more likely to happen and thus have smaller payouts. +100 odds on an event correspond to a 50% chance of that event occurring, but how does one convert odds to the probability that event will occur in general ? Basically, is there a function $L : (-\infty , -100) \cup [+100 , +\infty) \rightarrow [0,1]$ that takes the odds $p$ of an event and maps it to the probability it will occur? I am particularly interested in applying this to betting on who will score the first basket of a basketball game. Betting on a single player scoring the first basket has relatively high (positive) odds and therefore a good payout should the bet be successful. While betting on a single player is risky and not the most likely to happen, betting on multiple players increases the likelihood of winning while good individual odds can offset the losses on the other bets. Take for instance the following odds (which are real, but writing out the names will take too long) on who will score the first basket in tomorrow's Bulls v. Cavaliers game: Player 1: +490 Player 2: +500 Player 3: +550 Player 4: +600 Player 5: +850 Player 6: +850 Player 7: +1000 Player 8: +1100 Player 9: +1300 Player 10: +1500 Is there a function $L$ with the properties I have described above and satisfies $$ 1 = L(490) + L(500) + L(550) + L(600) + L(850) + L(850) + L(1000) + L(1100) + L(1300) + L(1500)? $$ Will I need to adapt the function $L$ if I was to look at a different game with different odds?","Gambling odds on sports betting are designated with a number in the set . If one places a wager of dollars at odds and your bet is successful, then your winnings are calculated as follows: So large positive odds correspond to events that are less likely to happen and thus have greater payouts while large negative odds correspond to events that are more likely to happen and thus have smaller payouts. +100 odds on an event correspond to a 50% chance of that event occurring, but how does one convert odds to the probability that event will occur in general ? Basically, is there a function that takes the odds of an event and maps it to the probability it will occur? I am particularly interested in applying this to betting on who will score the first basket of a basketball game. Betting on a single player scoring the first basket has relatively high (positive) odds and therefore a good payout should the bet be successful. While betting on a single player is risky and not the most likely to happen, betting on multiple players increases the likelihood of winning while good individual odds can offset the losses on the other bets. Take for instance the following odds (which are real, but writing out the names will take too long) on who will score the first basket in tomorrow's Bulls v. Cavaliers game: Player 1: +490 Player 2: +500 Player 3: +550 Player 4: +600 Player 5: +850 Player 6: +850 Player 7: +1000 Player 8: +1100 Player 9: +1300 Player 10: +1500 Is there a function with the properties I have described above and satisfies Will I need to adapt the function if I was to look at a different game with different odds?","(-\infty , -100) \cup [+100 , +\infty) w p \in (-\infty , -100) \cup [+100 , +\infty) E(w , p) 
E(w , p) : = \begin{cases} 
\left( 1 + \dfrac{100}{p} \right)w & p < -100\\
\left(1 + \dfrac{p}{100} \right)w & p \geq +100.
\end{cases} 
 L : (-\infty , -100) \cup [+100 , +\infty) \rightarrow [0,1] p L 
1 = L(490) + L(500) + L(550) + L(600) + L(850) + L(850) + L(1000) + L(1100) + L(1300) + L(1500)?
 L","['probability', 'recreational-mathematics', 'gambling']"
79,Prove/Disprove: $\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y)$ is a probability density function,Prove/Disprove:  is a probability density function,\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y),"Question : Assume that $f_X(x)$ is the probability density function of a random variable $X$ such that $X\in[a,b]$ for $b\gt a$ . Does there exist another random variable $Y\in[a,b]$ having $\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y)$ as its probability density function? Note: There reason I'm asking this question is that I want to treat $\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y)$ as the pdf of a new random variable like $Y$ and somehow change the measure I'm working with (cause dealing with that $Y$ is much more simple for me in some situation). I believe the "" Radon–Nikodym derivative (density) "" should be related to my question. However, I'm not very familiar with the probability measures. My try : I know that the integral of the probability density function over the whole support of the variable should be equal to $1$ . So I tried to prove that: $$ \int_{a}^{b} \dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y) dy =1 $$ However, I do not know how to deal with the fraction $\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]}$ in the integral. We know that $\int_a^b f_X(x)dx=1$ . I cannot see what happens when the fraction is multiplied, and how it affects the whole integral.","Question : Assume that is the probability density function of a random variable such that for . Does there exist another random variable having as its probability density function? Note: There reason I'm asking this question is that I want to treat as the pdf of a new random variable like and somehow change the measure I'm working with (cause dealing with that is much more simple for me in some situation). I believe the "" Radon–Nikodym derivative (density) "" should be related to my question. However, I'm not very familiar with the probability measures. My try : I know that the integral of the probability density function over the whole support of the variable should be equal to . So I tried to prove that: However, I do not know how to deal with the fraction in the integral. We know that . I cannot see what happens when the fraction is multiplied, and how it affects the whole integral.","f_X(x) X X\in[a,b] b\gt a Y\in[a,b] \dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y) \dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y) Y Y 1 
\int_{a}^{b} \dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y) dy =1
 \dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} \int_a^b f_X(x)dx=1","['probability', 'density-function']"
80,A man invited five friends.,A man invited five friends.,,"A man invited five friends. He was born in April as also all the invited friends. What is the probability that none of the friends was born on the same day of the month as the host? The way I approached it was $\frac{(30\times 29^5)}{(30^6)}$ . However, there is yet another equally convincing way i.e. Probability that a friend's birthday is on the same day as the host is $\frac{1}{30}$ . So if this goes for all friends then we have $\big(\frac{1}{30}\big)^5$ . And we want the negation of it so $1-\big(\frac{1}{30}\big)^5$ . Which one is correct?","A man invited five friends. He was born in April as also all the invited friends. What is the probability that none of the friends was born on the same day of the month as the host? The way I approached it was . However, there is yet another equally convincing way i.e. Probability that a friend's birthday is on the same day as the host is . So if this goes for all friends then we have . And we want the negation of it so . Which one is correct?",\frac{(30\times 29^5)}{(30^6)} \frac{1}{30} \big(\frac{1}{30}\big)^5 1-\big(\frac{1}{30}\big)^5,"['probability', 'combinatorics', 'birthday']"
81,"n black balls k white balls in M bins, what is the probability of selecting a black ball from any bin.","n black balls k white balls in M bins, what is the probability of selecting a black ball from any bin.",,"There are $n$ black balls and $k$ white, with $M$ bins. The process for filling the bins is as follows: Consider each of the $N=n+k$ balls, one at a time. For each ball, select 1 of the $M$ bins uniformly at random. Place the ball in the bin. The $N$ balls are allocated into the $M$ bins. Now, from a bin select a single ball, if a bin contains multiple balls a single ball is chosen uniformly at random. I want to work out the probability that a black ball is selected from a particular bin? Here is my approach: Let, $X_r$ be the number of balls of type $r\in \{b,w\}$ in a bin let $y_r=n$ for black balls and $y_r=k$ for white balls. Then the probability it has $s$ balls of type $r$ is given by the binomial distribution $$P(X_r=s)=\binom{y_r}{s}\left(\frac{1}{M}\right)^s\left(1-\frac{1}{M} \right)^{N-s}.$$ By independence of the events the probability that a bin has $i$ white and $j$ black balls in it $(t=i+j)$ , with $X=X_w+X_b$ , is $$P(X=t)=\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t}.$$ Let $P(b_t)$ be the probability of selecting a black ball given $t$ balls in the bin. Since, the selection is uniformly at random it should just be the product of the fraction of black balls and the likelihood of t balls being in the bin $$P(b_{t=i+j})=\frac{j}{i+j}\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t}.$$ Now, the total probability of choosing a black ball is given by $$ \sum_{i=0}^k \sum_{j=1}^n \frac{j}{i+j}\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t} \tag{*}\label{*}$$ where index $j$ starts at 1, since if no black balls are in the bin the probability of selecting one is zero and it ensures we don't get the undefined $\frac{0}{0}$ when $i=0$ . I think that $\eqref{*}$ is correct. My question is what ways can $\eqref{*}$ be simplified? I dont think I can use Vandermonde's Identity as the term $\left(\frac{1}{M}\right)^i$ is not able to be taken out of the sum over $i$ for example. I have put it into mathematica and get the following Sum[(s/(s + t))*Binomial[nh, s]*Binomial[nl, t]*(1/M)^(s + t)*(1 - 1/M)^(nh + nl - s - t),      {s, 1, nh}, {t, 0, nl}]   ProbHireH[M, nh, nl]=(1 - 1/M)^(m + n) (1/((1 - 1/M) M))^n  DifferenceRoot[Function[{\[FormalY], \[FormalN]}, {(-1 + M) (-1 - \[FormalN] +            n) (-\[FormalN] + m + n) \[FormalY][\[FormalN]] + (2 +            5 \[FormalN] + 3 \[FormalN]^2 - 2 m - 2 \[FormalN] m - M -            3 \[FormalN] M - 2 \[FormalN]^2 M + m M + \[FormalN] m M -            3 n - 4 \[FormalN] n + m n + 2 M n + 3 \[FormalN] M n -            m M n + n^2 - M n^2) \[FormalY][          1 + \[FormalN]] - (1 + \[FormalN]) (4 + 3 \[FormalN] - m -            M - \[FormalN] M - 2 n + M n) \[FormalY][          2 + \[FormalN]] + (1 + \[FormalN]) (2 + \[FormalN]) \ \[FormalY][3 + \[FormalN]] == 0, \[FormalY][0] == 0, \[FormalY][1] ==        Hypergeometric2F1[-m, n, 1 + n, -(1/(-1 + M))], \[FormalY][        2] == (1 - 1/M) M n Hypergeometric2F1[-m, -1 + n,           n, -(1/(-1 + M))] +         Hypergeometric2F1[-m, n, 1 + n, -(1/(-1 + M))]}]][n] Now, while this is a simplification of sorts, I am unsure that this is the simplest respresentation. This is because with the independence of everything I think that the probability of choosing a black ball from a bin after the allocation, could be the product of the probability that a bin has at least one ball and the fraction of black balls overall $$ \left(1- \left(1-\frac{1}{M}\right)^{n+k}\right)\frac{n}{n+k}. \tag{**}\label{**}$$ I can veryify (although not exhaustively) that $\eqref{*}$ and $\eqref{**}$ yield the same probabilities, for example SimpleProbHireH[M_, nh_, nl_] := (1 - (1 - 1/M)^(nh + nl))*nh/(nh + nl)  SimpleProbHireH[10, 5, 5]=6513215599/20000000000   ProbHireH[M_, nh_, nl_] :=   Sum[(s/(s + t))*Binomial[nh, s]*Binomial[nl, t]*(1/M)^(s + t)*(1 - 1/M)^(nh + nl - s - t), {s, 1, nh}, {t, 0, nl}]   ProbHireH[10, 5, 5]=6513215599/20000000000 Any help with this would be much appreciated.","There are black balls and white, with bins. The process for filling the bins is as follows: Consider each of the balls, one at a time. For each ball, select 1 of the bins uniformly at random. Place the ball in the bin. The balls are allocated into the bins. Now, from a bin select a single ball, if a bin contains multiple balls a single ball is chosen uniformly at random. I want to work out the probability that a black ball is selected from a particular bin? Here is my approach: Let, be the number of balls of type in a bin let for black balls and for white balls. Then the probability it has balls of type is given by the binomial distribution By independence of the events the probability that a bin has white and black balls in it , with , is Let be the probability of selecting a black ball given balls in the bin. Since, the selection is uniformly at random it should just be the product of the fraction of black balls and the likelihood of t balls being in the bin Now, the total probability of choosing a black ball is given by where index starts at 1, since if no black balls are in the bin the probability of selecting one is zero and it ensures we don't get the undefined when . I think that is correct. My question is what ways can be simplified? I dont think I can use Vandermonde's Identity as the term is not able to be taken out of the sum over for example. I have put it into mathematica and get the following Sum[(s/(s + t))*Binomial[nh, s]*Binomial[nl, t]*(1/M)^(s + t)*(1 - 1/M)^(nh + nl - s - t),      {s, 1, nh}, {t, 0, nl}]   ProbHireH[M, nh, nl]=(1 - 1/M)^(m + n) (1/((1 - 1/M) M))^n  DifferenceRoot[Function[{\[FormalY], \[FormalN]}, {(-1 + M) (-1 - \[FormalN] +            n) (-\[FormalN] + m + n) \[FormalY][\[FormalN]] + (2 +            5 \[FormalN] + 3 \[FormalN]^2 - 2 m - 2 \[FormalN] m - M -            3 \[FormalN] M - 2 \[FormalN]^2 M + m M + \[FormalN] m M -            3 n - 4 \[FormalN] n + m n + 2 M n + 3 \[FormalN] M n -            m M n + n^2 - M n^2) \[FormalY][          1 + \[FormalN]] - (1 + \[FormalN]) (4 + 3 \[FormalN] - m -            M - \[FormalN] M - 2 n + M n) \[FormalY][          2 + \[FormalN]] + (1 + \[FormalN]) (2 + \[FormalN]) \ \[FormalY][3 + \[FormalN]] == 0, \[FormalY][0] == 0, \[FormalY][1] ==        Hypergeometric2F1[-m, n, 1 + n, -(1/(-1 + M))], \[FormalY][        2] == (1 - 1/M) M n Hypergeometric2F1[-m, -1 + n,           n, -(1/(-1 + M))] +         Hypergeometric2F1[-m, n, 1 + n, -(1/(-1 + M))]}]][n] Now, while this is a simplification of sorts, I am unsure that this is the simplest respresentation. This is because with the independence of everything I think that the probability of choosing a black ball from a bin after the allocation, could be the product of the probability that a bin has at least one ball and the fraction of black balls overall I can veryify (although not exhaustively) that and yield the same probabilities, for example SimpleProbHireH[M_, nh_, nl_] := (1 - (1 - 1/M)^(nh + nl))*nh/(nh + nl)  SimpleProbHireH[10, 5, 5]=6513215599/20000000000   ProbHireH[M_, nh_, nl_] :=   Sum[(s/(s + t))*Binomial[nh, s]*Binomial[nl, t]*(1/M)^(s + t)*(1 - 1/M)^(nh + nl - s - t), {s, 1, nh}, {t, 0, nl}]   ProbHireH[10, 5, 5]=6513215599/20000000000 Any help with this would be much appreciated.","n k M N=n+k M N M X_r r\in \{b,w\} y_r=n y_r=k s r P(X_r=s)=\binom{y_r}{s}\left(\frac{1}{M}\right)^s\left(1-\frac{1}{M} \right)^{N-s}. i j (t=i+j) X=X_w+X_b P(X=t)=\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t}. P(b_t) t P(b_{t=i+j})=\frac{j}{i+j}\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t}.  \sum_{i=0}^k \sum_{j=1}^n \frac{j}{i+j}\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t} \tag{*}\label{*} j \frac{0}{0} i=0 \eqref{*} \eqref{*} \left(\frac{1}{M}\right)^i i  \left(1- \left(1-\frac{1}{M}\right)^{n+k}\right)\frac{n}{n+k}. \tag{**}\label{**} \eqref{*} \eqref{**}","['probability', 'combinatorics', 'balls-in-bins', 'polya-urn-model']"
82,"""On the Probability of Sequences in the Genoese Lottery"" by Euler. How did he do it?","""On the Probability of Sequences in the Genoese Lottery"" by Euler. How did he do it?",,"This a problem solved by Leonard Euler. Translated English version is available in Euler archives.[E338] On the Probability of Sequences in the Genoese Lottery Euler . I have difficulty in understanding Corollary 2 on page 29 he derived out of the blue. Can any one explain how did he arrive at  the formula for the number of cases of all species which belong to a particular  value of k. Here's an attached screenshot. I only want explanation of the first factor in those formulas (m-1),(m-1)(m-2)/2....","This a problem solved by Leonard Euler. Translated English version is available in Euler archives.[E338] On the Probability of Sequences in the Genoese Lottery Euler . I have difficulty in understanding Corollary 2 on page 29 he derived out of the blue. Can any one explain how did he arrive at  the formula for the number of cases of all species which belong to a particular  value of k. Here's an attached screenshot. I only want explanation of the first factor in those formulas (m-1),(m-1)(m-2)/2....",,"['probability', 'combinatorics', 'integer-partitions']"
83,Density of a random vector [duplicate],Density of a random vector [duplicate],,"This question already has an answer here : Joint density of uniform distribution and maximum of two uniform distributions. (1 answer) Closed 2 years ago . Let $X,Y \sim U[0,1]$ be two independent uniformly distributed on $[0,1]$ random variables. Let $Z := \max(X,Y)$ . I'm interested in the probability density function of the vector $(X,Z)^T$ . The CDF of $(X,Z)^T$ is \begin{align*} F_{X,Z}(x,z) &= \mathbb{P}(X \leq x, \max(X,Y) \leq z) = \mathbb{P}(X \leq x, X \leq z, Y \leq z) = \\ &= xz I\{0 \leq x \leq z \leq 1\} + z^2 I\{0 \leq z \leq 1, z \leq x\} + x I\{0\leq x \leq 1, 1 < z\} + I\{1 \leq x, 1 \leq z\} \end{align*} Now $p_{X,Z}(x,z) = \frac{\partial^2 F_{X,Z}(x,z)}{\partial x \partial z} = I\{0 \leq x \leq z \leq 1\}$ . But $\int_{\mathbb{R}^2}I\{0 \leq x \leq z \leq 1\} dxdz = \frac{1}{2} \neq 1$ The setting of the problem is very simple but I just don't understand where is the mistake. Thank you in advance.",This question already has an answer here : Joint density of uniform distribution and maximum of two uniform distributions. (1 answer) Closed 2 years ago . Let be two independent uniformly distributed on random variables. Let . I'm interested in the probability density function of the vector . The CDF of is Now . But The setting of the problem is very simple but I just don't understand where is the mistake. Thank you in advance.,"X,Y \sim U[0,1] [0,1] Z := \max(X,Y) (X,Z)^T (X,Z)^T \begin{align*}
F_{X,Z}(x,z) &= \mathbb{P}(X \leq x, \max(X,Y) \leq z) = \mathbb{P}(X \leq x, X \leq z, Y \leq z) = \\
&= xz I\{0 \leq x \leq z \leq 1\} + z^2 I\{0 \leq z \leq 1, z \leq x\} + x I\{0\leq x \leq 1, 1 < z\} + I\{1 \leq x, 1 \leq z\}
\end{align*} p_{X,Z}(x,z) = \frac{\partial^2 F_{X,Z}(x,z)}{\partial x \partial z} = I\{0 \leq x \leq z \leq 1\} \int_{\mathbb{R}^2}I\{0 \leq x \leq z \leq 1\} dxdz = \frac{1}{2} \neq 1","['probability', 'probability-theory', 'probability-distributions']"
84,Modelling the variance of dollar values of the rare/mythic slot in a Magic: the Gathering booster pack,Modelling the variance of dollar values of the rare/mythic slot in a Magic: the Gathering booster pack,,"I'm trying to model the variance of a certain kind of card that is pulled from a Magic: the Gathering trading card pack. My simplified model is this: I open a booster pack containing exactly one card. In this instance, the probability of getting a mythic is $w_M=\frac{1}{8}$ and the probability of a rare is $1-w_M=\frac{7}{8}$ , so: $\frac{7}{8}$ of the time, it is a rare card that is selected from a list of $i$ equally likely rare cards having dollar values $R = (r_1,...,r_i)$ $\frac{1}{8}$ of the time, it is a mythic card that is selected from another list of $j$ equally likely mythic cards having dollar values $M = (m_1,...,m_j)$ A example set of values we can easily work with might be: $R = (0.1, 0.2, 0.5, 3.0, 3.5)$ $M = (0.1, 5, 10)$ Calculating the population variance of each component of $R$ or $M$ would be straightforward: $$\sigma^2_R=\frac{\sum^i_{k=1}{(r_i - \mu_R)^2}}{i} = \frac{(0.1-1.46)^2+(0.2-1.46)^2+(0.5-1.46)^2+(3.0-1.46)^2+(3.5-1.46)^2}{5} = 2.1784$$ $$\sigma^2_M=\frac{\sum^j_{k=1}{(r_i - \mu_M)^2}}{i} = \frac{(0.1-5.033..)^2+(5-5.033..)^2+(10-5.033..)^2}{3} = 16.3356$$ However, I'm not sure how to combine these two values into something that accurately reflects the entire model of the variance of dollar values from getting a rare card $\frac{7}{8}$ of the time and a mythic card $\frac{1}{8}$ of the time. I expect that naively averaging variances like this doesn't work, but I think this incorrect calculation hints at what I'm trying to calculate: $$\sigma^2_{R\cap M}= \frac{7\sigma^2_{R}+\sigma^2_{M}}{8}=\frac{7\times2.1784+16.3356}{8}=3.9481$$ After doing some research of what I've modelled, this feels like trying to calculate the variance of combined weighted discrete independent uniform distributions - that is to say, each of $R$ and $M$ are discrete uniform distributions in themselves, and I'm trying to combine them together in a way that one distribution is seven times more likely to occur than the other. When looking at combining continuous independent uniform distributions, I found the Irwin-Hall distribution , but that seems to be modelling equally likely continuous independent uniform distributions and likely more complex than what I'm looking for. Some ideas I have to work around my inability to combine these distributions would be: Experimentally calculate variance by simulating a large number of pack openings Making some combined uniform discrete distribution $R_7M = R\cup R\cup R\cup R\cup R\cup R \cup R \cup M$ that has seven of each rare value and one of each mythic value (this doesn't work because there isn't typically the same number of mythic and rares) Doing something like above but with tuples of overall probability and dollar values like $RM = {(\frac{7}{7i+j},r_1), ..., (\frac{7}{7i+j},r_i), (\frac{1}{7i+j},m_1), ..., (\frac{1}{7i+j},m_j)}$ and then calculating the variance using these tuples somehow? I'm looking for a way that for arbitrary $P_M, R, M$ I can either transform the problem into something that I can calculate the variance on, or a way to calculate the combined weighed variance of the two distributions. I'm sure that I'm just forgetting some basic concept of statistics and probability - this concept of combined variance feels like something that would have been covered in an introductory statistics class, but I'm just not finding the right words to describe it.","I'm trying to model the variance of a certain kind of card that is pulled from a Magic: the Gathering trading card pack. My simplified model is this: I open a booster pack containing exactly one card. In this instance, the probability of getting a mythic is and the probability of a rare is , so: of the time, it is a rare card that is selected from a list of equally likely rare cards having dollar values of the time, it is a mythic card that is selected from another list of equally likely mythic cards having dollar values A example set of values we can easily work with might be: Calculating the population variance of each component of or would be straightforward: However, I'm not sure how to combine these two values into something that accurately reflects the entire model of the variance of dollar values from getting a rare card of the time and a mythic card of the time. I expect that naively averaging variances like this doesn't work, but I think this incorrect calculation hints at what I'm trying to calculate: After doing some research of what I've modelled, this feels like trying to calculate the variance of combined weighted discrete independent uniform distributions - that is to say, each of and are discrete uniform distributions in themselves, and I'm trying to combine them together in a way that one distribution is seven times more likely to occur than the other. When looking at combining continuous independent uniform distributions, I found the Irwin-Hall distribution , but that seems to be modelling equally likely continuous independent uniform distributions and likely more complex than what I'm looking for. Some ideas I have to work around my inability to combine these distributions would be: Experimentally calculate variance by simulating a large number of pack openings Making some combined uniform discrete distribution that has seven of each rare value and one of each mythic value (this doesn't work because there isn't typically the same number of mythic and rares) Doing something like above but with tuples of overall probability and dollar values like and then calculating the variance using these tuples somehow? I'm looking for a way that for arbitrary I can either transform the problem into something that I can calculate the variance on, or a way to calculate the combined weighed variance of the two distributions. I'm sure that I'm just forgetting some basic concept of statistics and probability - this concept of combined variance feels like something that would have been covered in an introductory statistics class, but I'm just not finding the right words to describe it.","w_M=\frac{1}{8} 1-w_M=\frac{7}{8} \frac{7}{8} i R = (r_1,...,r_i) \frac{1}{8} j M = (m_1,...,m_j) R = (0.1, 0.2, 0.5, 3.0, 3.5) M = (0.1, 5, 10) R M \sigma^2_R=\frac{\sum^i_{k=1}{(r_i - \mu_R)^2}}{i} = \frac{(0.1-1.46)^2+(0.2-1.46)^2+(0.5-1.46)^2+(3.0-1.46)^2+(3.5-1.46)^2}{5} = 2.1784 \sigma^2_M=\frac{\sum^j_{k=1}{(r_i - \mu_M)^2}}{i} = \frac{(0.1-5.033..)^2+(5-5.033..)^2+(10-5.033..)^2}{3} = 16.3356 \frac{7}{8} \frac{1}{8} \sigma^2_{R\cap M}= \frac{7\sigma^2_{R}+\sigma^2_{M}}{8}=\frac{7\times2.1784+16.3356}{8}=3.9481 R M R_7M = R\cup R\cup R\cup R\cup R\cup R \cup R \cup M RM = {(\frac{7}{7i+j},r_1), ..., (\frac{7}{7i+j},r_i), (\frac{1}{7i+j},m_1), ..., (\frac{1}{7i+j},m_j)} P_M, R, M","['probability', 'uniform-distribution', 'variance', 'card-games']"
85,Hard vs Soft accuracy: Bounds in specific probability problem,Hard vs Soft accuracy: Bounds in specific probability problem,,"Let $\mathbf{X}=(X_1,\cdots,X_m)$ a random vector which takes values in $(0,1)^m$ such that $\sum_{i=1}^mX_i=1$ (a.s.) and let $Y$ a random variable which takes values in $\{1,\cdots,m\}$ . Let $s=\mathbb{E}[X_Y]$ and $h=\mathbb{P}\left(Y\in\mathop{\arg\max}_i X_i\right)$ , I need bounds that relate these magnitudes in the general case. MOTIVATION: In Machine Learning context, $s$ and $h$ are the soft  and hard accuracy. Usually we take the hard one, so we assume that $h>s$ . I want to prove it. The tightest bound that I found is: $$s=\int_0^1\mathbf{P}(X_Y>t)dt=\int_0^\frac{1}{2}\mathbf{P}(X_Y>t)dt+\int_\frac{1}{2}^1\mathbf{P}(X_Y>t)dt\leq\frac{1}{2}+\frac{1}{2}\mathbf{P}(X_Y>1/2)\leq \frac{1+h}{2}$$ where I use that if some value is greater than $1/2$ , it is the maximum. Q1) Is there another upper bound of $s$ tighter than this? (suppose $h$ and $s$ are values close to $1$ ). Q2) I would like $s\leq h$ . Can you think of a counterexample? Q3) Can you think extra assumptions to ensure $s\leq h$ ? Thanks","Let a random vector which takes values in such that (a.s.) and let a random variable which takes values in . Let and , I need bounds that relate these magnitudes in the general case. MOTIVATION: In Machine Learning context, and are the soft  and hard accuracy. Usually we take the hard one, so we assume that . I want to prove it. The tightest bound that I found is: where I use that if some value is greater than , it is the maximum. Q1) Is there another upper bound of tighter than this? (suppose and are values close to ). Q2) I would like . Can you think of a counterexample? Q3) Can you think extra assumptions to ensure ? Thanks","\mathbf{X}=(X_1,\cdots,X_m) (0,1)^m \sum_{i=1}^mX_i=1 Y \{1,\cdots,m\} s=\mathbb{E}[X_Y] h=\mathbb{P}\left(Y\in\mathop{\arg\max}_i X_i\right) s h h>s s=\int_0^1\mathbf{P}(X_Y>t)dt=\int_0^\frac{1}{2}\mathbf{P}(X_Y>t)dt+\int_\frac{1}{2}^1\mathbf{P}(X_Y>t)dt\leq\frac{1}{2}+\frac{1}{2}\mathbf{P}(X_Y>1/2)\leq \frac{1+h}{2} 1/2 s h s 1 s\leq h s\leq h","['probability', 'probability-theory', 'machine-learning', 'upper-lower-bounds']"
86,Consider two i.i.d random variables $X$ and $Y$ with same PDF $f(x) = e^{-x}$,Consider two i.i.d random variables  and  with same PDF,X Y f(x) = e^{-x},"Question: Given two iid random variables $X$ and $Y$ with same PDF of $f(x) = e^{-x}$ , $x>0$ , find the PDF of $X+Y$ and $3X+2Y$ . Are these two new PDF's independent? What I've got so far: First I defined $Z = X + Y$ , since $x,y>0$ , $z>0$ . By applying a convolution on $X$ and $Y$ we get: $f_Z(z) = \int_{0}^{z} f_{XY}(x,z-x) \,dx \rightarrow f_Z(z) = \int_{0}^{z} e^{-x + x - z} \,dx \rightarrow f_Z(z) = ze^{-z}$ . Now for $W = 3X+2Y$ , by also applying a convolution, we get: $f_W(w) = \int_{0}^{w/3} f_{XY}(x,\dfrac{w-3x}{2}) \,dx \rightarrow f_W(w) = \int_{0}^{w/3} e^{(x-w)/2} \,dx \rightarrow f_W(w) = 2(e^{-w/3} - e^{-w/2})$ . However, when calculating the joint distribution of $Z$ and $W$ using the jacobian method, we get: $|J| = 1$ , $Y = 3Z-W$ and $X = W-2Z$ , and thus: $f_{WZ}(w,z) = f_{XY}(w-2z,3z-w) \rightarrow f_{WZ}(w,z) = e^{-(w-2z+3z-w)} \rightarrow f_{WZ}(w,z) = e^{-z} $ And so I concluded that the two new PDF's are not independent, because their joint probability is not equal to the product of the marginal probabilities. Are these results correct? Also, how can I get to $f_Z(z)$ from $f_{WZ}(w,z)$ ? Because I can't seem to figure out the right integration bounds in order to get the right marginal probabilities.","Question: Given two iid random variables and with same PDF of , , find the PDF of and . Are these two new PDF's independent? What I've got so far: First I defined , since , . By applying a convolution on and we get: . Now for , by also applying a convolution, we get: . However, when calculating the joint distribution of and using the jacobian method, we get: , and , and thus: And so I concluded that the two new PDF's are not independent, because their joint probability is not equal to the product of the marginal probabilities. Are these results correct? Also, how can I get to from ? Because I can't seem to figure out the right integration bounds in order to get the right marginal probabilities.","X Y f(x) = e^{-x} x>0 X+Y 3X+2Y Z = X + Y x,y>0 z>0 X Y f_Z(z) = \int_{0}^{z} f_{XY}(x,z-x) \,dx \rightarrow f_Z(z) = \int_{0}^{z} e^{-x + x - z} \,dx \rightarrow f_Z(z) = ze^{-z} W = 3X+2Y f_W(w) = \int_{0}^{w/3} f_{XY}(x,\dfrac{w-3x}{2}) \,dx \rightarrow f_W(w) = \int_{0}^{w/3} e^{(x-w)/2} \,dx \rightarrow f_W(w) = 2(e^{-w/3} - e^{-w/2}) Z W |J| = 1 Y = 3Z-W X = W-2Z f_{WZ}(w,z) = f_{XY}(w-2z,3z-w) \rightarrow f_{WZ}(w,z) = e^{-(w-2z+3z-w)} \rightarrow f_{WZ}(w,z) = e^{-z}  f_Z(z) f_{WZ}(w,z)","['calculus', 'probability', 'probability-distributions', 'random-variables', 'convolution']"
87,Circuits probability problem,Circuits probability problem,,"In the circuit shown, each switch is closed with probability $p$ , independently of all other switches. The task is to find the probability that a flow of current is possible between $A$ and $B$ . My approach was to use the inclusion-exclusion principle as follows. Label the circuit $ABCD$ : Then the four possible routes the current can take are $ACB$ , $ADB$ , $ACDB$ and $ADCB$ . Define events $R_1 = \{\text{current can flow along } ACB \}$ , $R_2 = \{\text{current can flow along } ADB \}$ , $R_3 = \{\text{current can flow along } ACDB \}$ , $R_4 = \{\text{current can flow along } ADCB \}$ . Then the required probability is \begin{align} \mathbb{P}(R_1 \cup R_2 \cup R_3 \cup R_4) = \mathbb{P}(R_1) &+ \mathbb{P}(R_2) + \mathbb{P}(R_3) + \mathbb{P}(R_4) \\ &- \mathbb{P}(R_1 \cap R_2) - \mathbb{P}(R_1 \cap R_3) - \mathbb{P}(R_1 \cap R_4) \\ &- \mathbb{P}(R_2 \cap R_3) - \mathbb{P}(R_2 \cap R_4) - \mathbb{P}(R_3 \cap R_4) \\ &+ \mathbb{P}(R_1 \cap R_2 \cap R_3) + \mathbb{P}(R_1 \cap R_2 \cap R_4) + \mathbb{P}(R_1 \cap R_3 \cap R_4) + \mathbb{P}(R_2 \cap R_3 \cap R_4) \\ &- \mathbb{P}(R_1 \cap R_2 \cap R_3 \cap R_4) \end{align} This works out as $\mathbb{P}(R_1 \cup R_2 \cup R_3 \cup R_4) = (2p^2 + 2p^3) - (5p^4 + p^5) + 4p^5 - p^5 = \boxed{2p^2 + 2p^3 - 5p^4 + 2p^5}$ . However, the answer given by the textbook is $\boxed{1 - (1-p)(1-p^2)^2 - p + p[1 - (1-p)^2]^2}$ , which is equivalent, but I can't figure out what the intended method was to get the solution in this form.","In the circuit shown, each switch is closed with probability , independently of all other switches. The task is to find the probability that a flow of current is possible between and . My approach was to use the inclusion-exclusion principle as follows. Label the circuit : Then the four possible routes the current can take are , , and . Define events , , , . Then the required probability is This works out as . However, the answer given by the textbook is , which is equivalent, but I can't figure out what the intended method was to get the solution in this form.","p A B ABCD ACB ADB ACDB ADCB R_1 = \{\text{current can flow along } ACB \} R_2 = \{\text{current can flow along } ADB \} R_3 = \{\text{current can flow along } ACDB \} R_4 = \{\text{current can flow along } ADCB \} \begin{align}
\mathbb{P}(R_1 \cup R_2 \cup R_3 \cup R_4) = \mathbb{P}(R_1) &+ \mathbb{P}(R_2) + \mathbb{P}(R_3) + \mathbb{P}(R_4) \\
&- \mathbb{P}(R_1 \cap R_2) - \mathbb{P}(R_1 \cap R_3) - \mathbb{P}(R_1 \cap R_4) \\
&- \mathbb{P}(R_2 \cap R_3) - \mathbb{P}(R_2 \cap R_4) - \mathbb{P}(R_3 \cap R_4) \\
&+ \mathbb{P}(R_1 \cap R_2 \cap R_3) + \mathbb{P}(R_1 \cap R_2 \cap R_4) + \mathbb{P}(R_1 \cap R_3 \cap R_4) + \mathbb{P}(R_2 \cap R_3 \cap R_4) \\
&- \mathbb{P}(R_1 \cap R_2 \cap R_3 \cap R_4)
\end{align} \mathbb{P}(R_1 \cup R_2 \cup R_3 \cup R_4)
= (2p^2 + 2p^3) - (5p^4 + p^5) + 4p^5 - p^5
= \boxed{2p^2 + 2p^3 - 5p^4 + 2p^5} \boxed{1 - (1-p)(1-p^2)^2 - p + p[1 - (1-p)^2]^2}","['probability', 'conditional-probability', 'inclusion-exclusion']"
88,Intuition of Lindeberg Condition,Intuition of Lindeberg Condition,,"I'm reading Shiryaev's probability, where he discusses the Central Limit Theorem for normalized and centered sums $S_n$ of i.i.d random variables $X_1, \ldots X_n$ , $n\geq 1$ under the classical Lindeberg condition. The condition states that for any $\epsilon>0$ , as $n\to \infty$ , for independent $X_1, X_2 \ldots$ : $$ (L) \ \ \ \ \frac{1}{D^2_n} \sum_{k=1}^{n} \int_{\{|x-m_k|\geq \epsilon D_n\}}  (x-m_k)^2 dF_k(x) \to 0, $$ where $E X_n = m_k$ , $S_n = \sum_{j=1}^{n} X_j$ , $Var X_k = \sigma^2_k < \infty $ and also $D_n^2 = \sum_{k=1}^{n} \sigma^2_k$ . I read the proof of the TLC for the ""triangle array"" which requires this condition. However, I don't seem to truly grasp the intuition behind the Lindeberg's condition. I mean, I understand the proof and I have no doubt that it holds, but I'm not sure what the condition (L) in fact would mean. Could anyone explain me the intuition behind the Lindeberg's condition? Perhaps with a simple example, or more theoretically. My goal is simply to grasp the intuition behind it.","I'm reading Shiryaev's probability, where he discusses the Central Limit Theorem for normalized and centered sums of i.i.d random variables , under the classical Lindeberg condition. The condition states that for any , as , for independent : where , , and also . I read the proof of the TLC for the ""triangle array"" which requires this condition. However, I don't seem to truly grasp the intuition behind the Lindeberg's condition. I mean, I understand the proof and I have no doubt that it holds, but I'm not sure what the condition (L) in fact would mean. Could anyone explain me the intuition behind the Lindeberg's condition? Perhaps with a simple example, or more theoretically. My goal is simply to grasp the intuition behind it.","S_n X_1, \ldots X_n n\geq 1 \epsilon>0 n\to \infty X_1, X_2 \ldots  (L) \ \ \ \ \frac{1}{D^2_n} \sum_{k=1}^{n} \int_{\{|x-m_k|\geq \epsilon D_n\}}  (x-m_k)^2 dF_k(x) \to 0,  E X_n = m_k S_n = \sum_{j=1}^{n} X_j Var X_k = \sigma^2_k < \infty  D_n^2 = \sum_{k=1}^{n} \sigma^2_k","['probability', 'probability-theory', 'measure-theory', 'central-limit-theorem']"
89,Discontinuities in the expectation of a stopping time (Bayesian coin-tossing),Discontinuities in the expectation of a stopping time (Bayesian coin-tossing),,"Suppose a coin is either Unbiased ( $P(\text{Head})=1/2$ ), or Biased with $P(\text{Head})=b\ne{1\over 2}$ , where $b$ is a known value. To decide between Unbiased vs. Biased --assumed equally likely a priori-- we toss the coin until one alternative has a posterior probability at least $9$ times that of the other, deciding in favor of the more probable one. Let $N$ be the number of tosses needed to reach a decision, and consider the expectation $EN(b)$ as a function of $b.$ (We can focus on $b\in(0,{1\over 2})$ , since the function is symmetric about $b={1\over 2}$ .) Here's a plot of Monte Carlo simulation results, with simulation errors less than the width of the ""dots"": The above plot seems unremarkable, but closer inspection suggests that $EN(b)$ is neither monotonic nor continuous on $(0,{1\over 2})$ . In the following plots, the very-light-grey lines give 3-sigma error bounds that vary with sample size from plot-to-plot: These images show, row-wise from the top, apparent ""jump discontinuities"" at $b\approx 0.05054567564, 0.1339745962, 0.1666666666,$ and $0.28867513459.$ (The first is an instance of what seems to be a negative jump, where $EN(b)$ surprisingly decreases .) Numerically, we notice -- and it seems intuitively clear -- that the jumps occur at the ""threshold"" values of polynomials appearing in the definition of $N$ (see the formulation below): $$N:=\inf\left\{n\ge 1:\ 2^nb^{S_n}(1-b)^{n-S_n}\not\in\left({1\over 9},\,9\right)\right\}.$$ Thus, for threshold value $t\in\{{1\over 9},9\}$ , the set of candidate discontinuities is simply $$D_{t}:=\left\{b\in\left(0,{1\over 2}\right): 2^{n}\,b^s\,(1-b)^{n-s}=t,\ n\in\mathbb{Z^+},s\in\{0,\ldots,n\}\right\},$$ and indeed all of the apparent discontinuities that I've so far checked do seem to fall in the set $D_{1\over 9}\cup D_{9};\ $ e.g., the jumps in the top two plots above are at the points in $D_{9}$ with $(s,n)=(1,8)$ and $(0,4)$ , respectively, and the bottom two plots are at the points in $D_{1\over 9}$ with $(s,n)=(2,2)$ and $(4,4)$ , respectively. Question #1 : How to prove that $EN(b)$ is discontinuous at every $b\in D_{1\over 9}\cup D_{9}?$ Can a formula be found to estimate the size and/or direction of the jump? Question #2 : Is it the case that every nonempty open interval $I\subseteq(0,{1\over 2})$ contains an element of $D_{1\over 9}\cup D_{9}?$ Question #3 : Presumably, for any $b\in(0,{1\over 2})$ there is a well-defined proportion of the value $EN(b)$ that's due purely to jump discontinuities at points $b'\in(0,b]$ . What can be said about this quantity as $b\to{1\over 2}?$ (Asymptotically, can a nonzero proportion of the growth be attributed purely to ""jumps""?) Formulation : For $n=1,2,3,\ldots,$ $$\begin{align}(X_1,\ldots X_n)\mid p &\sim \text{i.i.d. Bernoulli($p$),}\\[3mm] p &\sim\text{Uniform$\left(\left\{{1\over 2},\,b\right\}\right)$},\end{align}$$ where $b\ne{1\over 2}.$ Given $(X_1,\ldots,X_n),$ we have the posterior odds for $\ p=b\ $ vs. $\ p={1\over 2}\ $ as follows (because of the uniform prior, this is the same as the Bayes factor, or likelihood ratio): $$R_n:={P(p=b\mid X_1,\ldots,X_n)\over P(p={1\over 2}\mid X_1,\ldots,X_n)}={P(p=b\mid X_1,\ldots,X_n)\over P(p={1\over 2}\mid X_1,\ldots,X_n)}=2^nb^{S_n}(1-b)^{n-S_n},\quad S_n=\sum_{i=1}^nX_i.$$ The number of tosses required to make a decision is then $$N:=\inf\left\{n\ge 1:\ R_n\not\in\left({1\over 9},\,9\right)\right\}.$$ Since the distribution of $R_n$ is invariant under $b\mapsto 1-b$ , the function $EN(b)$ is symmetric about $b={1\over 2}$ ; hence, WLOG we can take $b\lt {1\over 2}.$","Suppose a coin is either Unbiased ( ), or Biased with , where is a known value. To decide between Unbiased vs. Biased --assumed equally likely a priori-- we toss the coin until one alternative has a posterior probability at least times that of the other, deciding in favor of the more probable one. Let be the number of tosses needed to reach a decision, and consider the expectation as a function of (We can focus on , since the function is symmetric about .) Here's a plot of Monte Carlo simulation results, with simulation errors less than the width of the ""dots"": The above plot seems unremarkable, but closer inspection suggests that is neither monotonic nor continuous on . In the following plots, the very-light-grey lines give 3-sigma error bounds that vary with sample size from plot-to-plot: These images show, row-wise from the top, apparent ""jump discontinuities"" at and (The first is an instance of what seems to be a negative jump, where surprisingly decreases .) Numerically, we notice -- and it seems intuitively clear -- that the jumps occur at the ""threshold"" values of polynomials appearing in the definition of (see the formulation below): Thus, for threshold value , the set of candidate discontinuities is simply and indeed all of the apparent discontinuities that I've so far checked do seem to fall in the set e.g., the jumps in the top two plots above are at the points in with and , respectively, and the bottom two plots are at the points in with and , respectively. Question #1 : How to prove that is discontinuous at every Can a formula be found to estimate the size and/or direction of the jump? Question #2 : Is it the case that every nonempty open interval contains an element of Question #3 : Presumably, for any there is a well-defined proportion of the value that's due purely to jump discontinuities at points . What can be said about this quantity as (Asymptotically, can a nonzero proportion of the growth be attributed purely to ""jumps""?) Formulation : For where Given we have the posterior odds for vs. as follows (because of the uniform prior, this is the same as the Bayes factor, or likelihood ratio): The number of tosses required to make a decision is then Since the distribution of is invariant under , the function is symmetric about ; hence, WLOG we can take","P(\text{Head})=1/2 P(\text{Head})=b\ne{1\over 2} b 9 N EN(b) b. b\in(0,{1\over 2}) b={1\over 2} EN(b) (0,{1\over 2}) b\approx 0.05054567564, 0.1339745962, 0.1666666666, 0.28867513459. EN(b) N N:=\inf\left\{n\ge 1:\ 2^nb^{S_n}(1-b)^{n-S_n}\not\in\left({1\over 9},\,9\right)\right\}. t\in\{{1\over 9},9\} D_{t}:=\left\{b\in\left(0,{1\over 2}\right): 2^{n}\,b^s\,(1-b)^{n-s}=t,\ n\in\mathbb{Z^+},s\in\{0,\ldots,n\}\right\}, D_{1\over 9}\cup D_{9};\  D_{9} (s,n)=(1,8) (0,4) D_{1\over 9} (s,n)=(2,2) (4,4) EN(b) b\in D_{1\over 9}\cup D_{9}? I\subseteq(0,{1\over 2}) D_{1\over 9}\cup D_{9}? b\in(0,{1\over 2}) EN(b) b'\in(0,b] b\to{1\over 2}? n=1,2,3,\ldots, \begin{align}(X_1,\ldots X_n)\mid p &\sim \text{i.i.d. Bernoulli(p),}\\[3mm]
p &\sim\text{Uniform\left(\left\{{1\over 2},\,b\right\}\right)},\end{align} b\ne{1\over 2}. (X_1,\ldots,X_n), \ p=b\  \ p={1\over 2}\  R_n:={P(p=b\mid X_1,\ldots,X_n)\over P(p={1\over 2}\mid X_1,\ldots,X_n)}={P(p=b\mid X_1,\ldots,X_n)\over P(p={1\over 2}\mid X_1,\ldots,X_n)}=2^nb^{S_n}(1-b)^{n-S_n},\quad S_n=\sum_{i=1}^nX_i. N:=\inf\left\{n\ge 1:\ R_n\not\in\left({1\over 9},\,9\right)\right\}. R_n b\mapsto 1-b EN(b) b={1\over 2} b\lt {1\over 2}.","['probability', 'polynomials', 'expected-value', 'random-walk', 'stopping-times']"
90,"Find a family of random variables $X$, with density $f$, such that $X$ and $Y=f(X)$ have the same distribution.","Find a family of random variables , with density , such that  and  have the same distribution.",X f X Y=f(X),"I have to demonstrate the following Probability/Statistics result: ""Find a family of random variables $X$ , having pdf $f$ , such that $X$ and $Y=f(X)$ have the same distribution."" I have tried to find a proof to this result and I found out that, in the case of pdf $f$ strictly increasing, the result is true if $f$ is the identity function. I have to find a characterization even in the following 2 cases: $f$ strictly decreasing; $f$ stictly increseasing for $x\leq m$ and stricly decreasing for $x \geq m$ (where $m$ is the mode of the distribution). Any ideas and/or suggestions?","I have to demonstrate the following Probability/Statistics result: ""Find a family of random variables , having pdf , such that and have the same distribution."" I have tried to find a proof to this result and I found out that, in the case of pdf strictly increasing, the result is true if is the identity function. I have to find a characterization even in the following 2 cases: strictly decreasing; stictly increseasing for and stricly decreasing for (where is the mode of the distribution). Any ideas and/or suggestions?",X f X Y=f(X) f f f f x\leq m x \geq m m,"['probability', 'statistics', 'probability-distributions', 'random-variables']"
91,If the rat starts at room 2 what is the probability of eventually ending up in room 5?,If the rat starts at room 2 what is the probability of eventually ending up in room 5?,,I have a following doubt on a question of Markov chain. I have done the parts (a) to (d). I am facing problem with the last part. I am attaching what I have done so far.,I have a following doubt on a question of Markov chain. I have done the parts (a) to (d). I am facing problem with the last part. I am attaching what I have done so far.,,"['calculus', 'probability', 'matrices', 'markov-chains']"
92,Conditional expectation of controlled diffusion process with respect to a trajectory of the control,Conditional expectation of controlled diffusion process with respect to a trajectory of the control,,"Suppose $a_t$ is the solution to an SDE controlled by the process $b_t$ (both processes are defined on the same probability space) \begin{align*}     a_t &= a_0 + \int_0^t f_a(a_s, b_s)ds + \int_0^t \sigma_a(a_s, b_s) dW_s \\     %b_t &=b_0 + \int_0^t f_b(a_s, b_s)ds + \int_0^t \sigma_b dW^b_s \end{align*} Let $b_{0\leq s\leq t}$ denote a trajectory of the process $b$ up to time $t$ and consider the conditional expectation \begin{align*}   \textbf a_t :=  \mathbb E[a_t \mid b_{0\leq s\leq t}] \end{align*} Question: given a trajectory $b_{\leq t}$ does $\textbf a_t$ solve the SDE \begin{align*}    \textbf a_t &= \textbf a_0 + \int_0^t f_a(\textbf a_s, b_s)ds \end{align*} ? What if $f_a$ is linear or when $\sigma_a$ is constant? This sounds plausible by taking the conditional expectation of the initial SDE and using the fact that Ito stochastic integrals are martingales so should vanish in expectation, however, I am not able to prove it rigorously. Maybe with a generalization of Ito's lemma for stochastic processes? Any help or pointers to references (books, papers) that could help me solve this would be really appreciated.","Suppose is the solution to an SDE controlled by the process (both processes are defined on the same probability space) Let denote a trajectory of the process up to time and consider the conditional expectation Question: given a trajectory does solve the SDE ? What if is linear or when is constant? This sounds plausible by taking the conditional expectation of the initial SDE and using the fact that Ito stochastic integrals are martingales so should vanish in expectation, however, I am not able to prove it rigorously. Maybe with a generalization of Ito's lemma for stochastic processes? Any help or pointers to references (books, papers) that could help me solve this would be really appreciated.","a_t b_t \begin{align*}
    a_t &= a_0 + \int_0^t f_a(a_s, b_s)ds + \int_0^t \sigma_a(a_s, b_s) dW_s \\
    %b_t &=b_0 + \int_0^t f_b(a_s, b_s)ds + \int_0^t \sigma_b dW^b_s
\end{align*} b_{0\leq s\leq t} b t \begin{align*}
  \textbf a_t :=  \mathbb E[a_t \mid b_{0\leq s\leq t}]
\end{align*} b_{\leq t} \textbf a_t \begin{align*}
   \textbf a_t &= \textbf a_0 + \int_0^t f_a(\textbf a_s, b_s)ds
\end{align*} f_a \sigma_a","['probability', 'stochastic-processes', 'conditional-expectation', 'control-theory', 'stochastic-differential-equations']"
93,Simple way to sample from a Gaussian distribution on SO(3) rotation group,Simple way to sample from a Gaussian distribution on SO(3) rotation group,,"Given a mean and variance, I want to be able to sample rotations that are ""normally distributed"" according to this mean and variance. I have a metric over rotations, the chordal metric: $\| R_1 - R_2\|$ which tells us how close two rotations are to each other. How do I do this? One way is to consider SO(3) as a sphere $S^3$ in $R^4$ , the space of quaternions, but I don't know how to setup the Gaussian distribution correctly in this case considering a single rotation in SO(3) is an antipodal pair in $S^3$ .","Given a mean and variance, I want to be able to sample rotations that are ""normally distributed"" according to this mean and variance. I have a metric over rotations, the chordal metric: which tells us how close two rotations are to each other. How do I do this? One way is to consider SO(3) as a sphere in , the space of quaternions, but I don't know how to setup the Gaussian distribution correctly in this case considering a single rotation in SO(3) is an antipodal pair in .",\| R_1 - R_2\| S^3 R^4 S^3,"['linear-algebra', 'probability', 'differential-geometry', 'probability-distributions', 'rotations']"
94,"Given a coin that produces the probability $\lambda$, what is a constructive way to produce $f(\lambda)$, where $f$ is continuous non-Hölder?","Given a coin that produces the probability , what is a constructive way to produce , where  is continuous non-Hölder?",\lambda f(\lambda) f,"Given a coin of ""bias"" $\lambda$ , sample the probability $f(\lambda)$ . This is the Bernoulli factory problem , and it can be solved only for certain functions $f$ . (For example, flipping the coin twice and taking heads only if exactly one coin shows heads, we can simulate the probability $\lambda(1-\lambda)$ .) Usually, the Bernoulli factory problem can be solved for $f$ if and only if— $f$ is continuous in $[0, 1]$ , and $f$ is identically $0$ , identically $1$ , or polynomially bounded away from $0$ and $1$ (roughly speaking, the function takes on only values in $ [0, 1]$ and its graph doesn't touch $0$ or $1$ except at the points $0$ and/or $1$ ) (Keane and O'Brien 1994). This question is a continuation of another question of mine , but this time we focus on the case when $f(\lambda)$ is a continuous non-Hölder function , which roughly means a continuous function with an exponentially steep slope (steeper than any $n$ th-root). The question just linked to seeks ways to compute polynomials that converge from above and below to $f$ in a manner that solves the Bernoulli factory problem for $f$ . These polynomials form an approximation scheme for $f$ . See that question for a formal statement of such approximation schemes. There are two algorithms (one by Thomas and Blanchet $2012$ , another by Łatuszyński et al.) to simulate a function $f$ via an approximation scheme. Roughly speaking, the algorithms work as follows: Generate U, a uniform random number in $[0, 1]$ . Flip the input coin (with ""bias"" $\lambda$ ), then build an upper and lower bound for $f(\lambda)$ , based on the outcomes of the flips so far. In this case, these bounds come from two degree- $n$ polynomials that approach $f$ as $n$ gets large, where $n$ is the number of coin flips so far in the algorithm. If U is less than or equal to the lower bound, return 1. If U is greater than the upper bound, return $0$ . Otherwise, go to step $2$ . The result of the algorithm is $1$ with probability exactly equal to $f(\lambda)$ , or $0$ otherwise. In general, the rate at which a function can be simulated by these algorithms depends on the smoothness of $f$ . It roughly corresponds to the rate of convergence of an approximation scheme for $f$ . I list approximation schemes for different kinds of functions $f$ , which work with the two algorithms described above. For example, roughly speaking, a $C^0$ continuous function $f$ can be simulated at the rate $O(n^{\alpha})$ only if $f$ is $\alpha$ -Hölder continuous (Holtz et al. 2011). This seems to imply that a function not meeting a Hölder condition (a non-Hölder function) can achieve, at best, a simulation rate of $O(1)$ , so that the polynomials won't necessarily converge uniformly for all $\lambda \in [0, 1]$ and for all non-Hölder functions, so that the algorithms above won't terminate in all cases. And this suggests to me that we need to add the following extra condition to the Bernoulli factory problem, since it appears that no approximation scheme exists for non-Hölder functions: $f$ is $\alpha$ -Hölder continuous in its domain for some $\alpha > 0$ . Is this true? Is there really no algorithm to sample the probability $f(\lambda)$ for a non-Hölder function $f$ , given sample access to the probability $\lambda$ ? This suggests that a different approach may be needed for Hölder functions. Thus: Is there a constructive method to approximate non-Hölder functions with polynomials for the Bernoulli factory problem (in the form of a formula to compute their coefficients), so that we can sample the probability $f(\lambda)$ for a non-Hölder function $f$ , given sample access to the probability $\lambda$ ? Note: An example of a non-Hölder function is the following, with a ""cusp"" at $0$ : $1/10$ if $\lambda = 0$ , and $-1/(2*\ln(\lambda/2))+1/10$ otherwise. Another example is the following, where the cusp is at $3/10$ : $1/10$ if $\lambda = 3/10$ , and $-1/(2*\ln(\frac{|\lambda-3/10|}{2}))+1/10$ otherwise. A third example is the following monotone function: $3/10$ if $\lambda = 3/10$ , $1/(2*\ln(\frac{3/10-\lambda}{2}))+3/10$ if $\lambda < 3/10$ , and $-1/(2*\ln(\frac{\lambda-3/10}{2}))+3/10$ otherwise. Roughly speaking: A non-Hölder function is continuous but has an exponentially steep slope. If a continuous function has no vertical slope, the function is 1-Hölder continuous. REFERENCES: Thomas, A.C., Blanchet, J., "" A Practical Implementation of the Bernoulli Factory "", arXiv:1106.2508v3 [stat.AP], 2012. Łatuszyński, K., Kosmidis, I., Papaspiliopoulos, O., Roberts, G.O., ""Simulating events of unknown probabilities via reverse time martingales"", arXiv:0907.4018v2 [stat.CO], $2009/2011$ . Keane, M. S., and O'Brien, G. L., ""A Bernoulli factory"", ACM Transactions on Modeling and Computer Simulation 4(2), 1994. Holtz, O., Nazarov, F., Peres, Y., "" New Coins from Old, Smoothly "", Constructive Approximation $33$ ( $2011$ ). Update: I have become aware of so-called Dini-continuous functions , which are a superset of Hölder continuous functions. Essentially, a Dini-continuous function has a slope no ""steeper"" than that of $\phi(\lambda)$ for some increasing non-negative function $\phi$ with a finite integral of $\phi(\lambda)/\lambda$ over $[0, 1]$ . However, I haven't found any paper yet on the rate at which polynomials can approximate a Dini-continuous (but not Hölder-continuous) function (let alone in a manner that solves the Bernoulli factory problem). Moreover, the following function, adapted from this question , is apparently not even Dini continuous: $1/10 + (1/(1+|\ln(\lambda)|))/2$ if $\lambda>0$ , and $1/10$ otherwise.","Given a coin of ""bias"" , sample the probability . This is the Bernoulli factory problem , and it can be solved only for certain functions . (For example, flipping the coin twice and taking heads only if exactly one coin shows heads, we can simulate the probability .) Usually, the Bernoulli factory problem can be solved for if and only if— is continuous in , and is identically , identically , or polynomially bounded away from and (roughly speaking, the function takes on only values in and its graph doesn't touch or except at the points and/or ) (Keane and O'Brien 1994). This question is a continuation of another question of mine , but this time we focus on the case when is a continuous non-Hölder function , which roughly means a continuous function with an exponentially steep slope (steeper than any th-root). The question just linked to seeks ways to compute polynomials that converge from above and below to in a manner that solves the Bernoulli factory problem for . These polynomials form an approximation scheme for . See that question for a formal statement of such approximation schemes. There are two algorithms (one by Thomas and Blanchet , another by Łatuszyński et al.) to simulate a function via an approximation scheme. Roughly speaking, the algorithms work as follows: Generate U, a uniform random number in . Flip the input coin (with ""bias"" ), then build an upper and lower bound for , based on the outcomes of the flips so far. In this case, these bounds come from two degree- polynomials that approach as gets large, where is the number of coin flips so far in the algorithm. If U is less than or equal to the lower bound, return 1. If U is greater than the upper bound, return . Otherwise, go to step . The result of the algorithm is with probability exactly equal to , or otherwise. In general, the rate at which a function can be simulated by these algorithms depends on the smoothness of . It roughly corresponds to the rate of convergence of an approximation scheme for . I list approximation schemes for different kinds of functions , which work with the two algorithms described above. For example, roughly speaking, a continuous function can be simulated at the rate only if is -Hölder continuous (Holtz et al. 2011). This seems to imply that a function not meeting a Hölder condition (a non-Hölder function) can achieve, at best, a simulation rate of , so that the polynomials won't necessarily converge uniformly for all and for all non-Hölder functions, so that the algorithms above won't terminate in all cases. And this suggests to me that we need to add the following extra condition to the Bernoulli factory problem, since it appears that no approximation scheme exists for non-Hölder functions: is -Hölder continuous in its domain for some . Is this true? Is there really no algorithm to sample the probability for a non-Hölder function , given sample access to the probability ? This suggests that a different approach may be needed for Hölder functions. Thus: Is there a constructive method to approximate non-Hölder functions with polynomials for the Bernoulli factory problem (in the form of a formula to compute their coefficients), so that we can sample the probability for a non-Hölder function , given sample access to the probability ? Note: An example of a non-Hölder function is the following, with a ""cusp"" at : if , and otherwise. Another example is the following, where the cusp is at : if , and otherwise. A third example is the following monotone function: if , if , and otherwise. Roughly speaking: A non-Hölder function is continuous but has an exponentially steep slope. If a continuous function has no vertical slope, the function is 1-Hölder continuous. REFERENCES: Thomas, A.C., Blanchet, J., "" A Practical Implementation of the Bernoulli Factory "", arXiv:1106.2508v3 [stat.AP], 2012. Łatuszyński, K., Kosmidis, I., Papaspiliopoulos, O., Roberts, G.O., ""Simulating events of unknown probabilities via reverse time martingales"", arXiv:0907.4018v2 [stat.CO], . Keane, M. S., and O'Brien, G. L., ""A Bernoulli factory"", ACM Transactions on Modeling and Computer Simulation 4(2), 1994. Holtz, O., Nazarov, F., Peres, Y., "" New Coins from Old, Smoothly "", Constructive Approximation ( ). Update: I have become aware of so-called Dini-continuous functions , which are a superset of Hölder continuous functions. Essentially, a Dini-continuous function has a slope no ""steeper"" than that of for some increasing non-negative function with a finite integral of over . However, I haven't found any paper yet on the rate at which polynomials can approximate a Dini-continuous (but not Hölder-continuous) function (let alone in a manner that solves the Bernoulli factory problem). Moreover, the following function, adapted from this question , is apparently not even Dini continuous: if , and otherwise.","\lambda f(\lambda) f \lambda(1-\lambda) f f [0, 1] f 0 1 0 1  [0, 1] 0 1 0 1 f(\lambda) n f f f 2012 f [0, 1] \lambda f(\lambda) n f n n 0 2 1 f(\lambda) 0 f f f C^0 f O(n^{\alpha}) f \alpha O(1) \lambda \in [0, 1] f \alpha \alpha > 0 f(\lambda) f \lambda f(\lambda) f \lambda 0 1/10 \lambda = 0 -1/(2*\ln(\lambda/2))+1/10 3/10 1/10 \lambda = 3/10 -1/(2*\ln(\frac{|\lambda-3/10|}{2}))+1/10 3/10 \lambda = 3/10 1/(2*\ln(\frac{3/10-\lambda}{2}))+3/10 \lambda < 3/10 -1/(2*\ln(\frac{\lambda-3/10}{2}))+3/10 2009/2011 33 2011 \phi(\lambda) \phi \phi(\lambda)/\lambda [0, 1] 1/10 + (1/(1+|\ln(\lambda)|))/2 \lambda>0 1/10","['probability', 'polynomials', 'approximation-theory', 'simulation', 'holder-spaces']"
95,"Probability, dependent Events","Probability, dependent Events",,"I am trying to understand some of the tasks and would be happy if someone could check my solution and possibly point out any mistakes. In advance, I am not asking for a solution, because I would like to work it out by myself. The purpose of this exercise is to check whether events A and B are independent of each other. Three dices are thrown. The events are defined as follows: A: All dice have the same number of dots. B: The total sum of the dices is less than 5. Therefore is $A = \left \{ \left ( 1,1,1 \right ), (2,2,2), (3,3,3), (4,4,4), (5,5,5), (6,6,6) \right \}$ $B = \left \{ \left ( 1,1,1 \right ), (1,1,2), (1,2,1), (2,1,1) \right \}$ Some intermediate results: $P\left ( A \right ) = \frac{6}{216} = \frac{3}{108}$ $P\left ( not A \right ) = 1 - P(A) = 1 - \frac{3}{108} = \frac{35}{36}$ $P(B) = \frac{4}{216} = \frac{1}{54}$ $P(A\cap B) = \frac{1}{216}$ $P(notA\cap B) = \frac{3}{216}$ A and B are independent if $P(B|notA) = P(B|A)$ $ P(B|A) = \frac{P(A\cap B)}{P(A)} = \frac{\frac{1}{216}}{\frac{6}{216}} = \frac{1}{6} $ $ P(B|notA) = \frac{P(notA\cap B)}{P(notA)} = \frac{\frac{3}{216}}{\frac{35}{36}} = \frac{1}{70} $ Thus, the events A and B are dependent. Thank you in advance for your time.","I am trying to understand some of the tasks and would be happy if someone could check my solution and possibly point out any mistakes. In advance, I am not asking for a solution, because I would like to work it out by myself. The purpose of this exercise is to check whether events A and B are independent of each other. Three dices are thrown. The events are defined as follows: A: All dice have the same number of dots. B: The total sum of the dices is less than 5. Therefore is Some intermediate results: A and B are independent if Thus, the events A and B are dependent. Thank you in advance for your time.","A = \left \{ \left ( 1,1,1 \right ), (2,2,2), (3,3,3), (4,4,4), (5,5,5), (6,6,6) \right \} B = \left \{ \left ( 1,1,1 \right ), (1,1,2), (1,2,1), (2,1,1) \right \} P\left ( A \right ) = \frac{6}{216} = \frac{3}{108} P\left ( not A \right ) = 1 - P(A) = 1 - \frac{3}{108} = \frac{35}{36} P(B) = \frac{4}{216} = \frac{1}{54} P(A\cap B) = \frac{1}{216} P(notA\cap B) = \frac{3}{216} P(B|notA) = P(B|A) 
P(B|A) = \frac{P(A\cap B)}{P(A)} = \frac{\frac{1}{216}}{\frac{6}{216}} = \frac{1}{6}
 
P(B|notA) = \frac{P(notA\cap B)}{P(notA)} = \frac{\frac{3}{216}}{\frac{35}{36}} = \frac{1}{70}
",['probability']
96,One sided derivative of the moment generating function,One sided derivative of the moment generating function,,"Let $Z$ be a random variable with one-sided heavy tailed distribution, that it, the moment generating function of $Z$ is infinite for every $t<0$ , but is finite on interval $[0,z)$ for some $z>0$ . In this case, does the moment generating function have one-sided derivative at $0$ ? If yes, does this derivative is equal to (possibly negative infinite) expectation of $Z$ ? If yes, reference to a book where this is rigorously proved would be appreciated.","Let be a random variable with one-sided heavy tailed distribution, that it, the moment generating function of is infinite for every , but is finite on interval for some . In this case, does the moment generating function have one-sided derivative at ? If yes, does this derivative is equal to (possibly negative infinite) expectation of ? If yes, reference to a book where this is rigorously proved would be appreciated.","Z Z t<0 [0,z) z>0 0 Z","['calculus', 'probability', 'probability-theory', 'moment-generating-functions']"
97,Are the ordinary least squares regression coefficients uniformly integrable?,Are the ordinary least squares regression coefficients uniformly integrable?,,"In my previous question I asked whether a set of asymptotically normal random variables $\{X_n\}_{n \ge 1}$ are uniformly integrable. In the accepted answer the poster showed that this does not hold in general. Now what about a specific case. Consider the standard ordinary least squares (OLS) regression coefficients $\hat{\beta}_n = \beta + (X_n^T X_n)^{-1} X_n^T \varepsilon$ where I have used the subscript $n$ to indicate that these are the OLS coefficients for a regression model with $n$ observations. Is $\{\hat{\beta}_n\}_{n \ge 1}$ uniformly integrable? Of course, $\hat{\beta}_n$ is a random vector, so I am asking whether the elements of this vector, which are all asymptotically normal, are uniformly integrable. Note that the $X_n$ above are stochastic regressors, not fixed. Edit: Attempt for the case of single regressor with no intercept Here is an attempt for the case of a single regressor with no intercept, i.e., the regression model is $$ Y = X \beta + \varepsilon, $$ where we have $n$ i.i.d observations $Y = [Y_1,Y_2,\dots,Y_n]^T$ and regressors $X = [X_1,X_2,\dots,X_n]^T$ , and $\varepsilon = [\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n]^T$ . The errors are mutually independent with $E[\varepsilon_i|X] = 0$ and they all have the same finite variance $\sigma^2$ . Let $\hat \beta_n$ denote the OLS estimate for $\beta$ where the subscript $n$ indicates the number of observations in the regression model. Note that because we only have a single regressor that $(X^T X)^{-1}$ reduces from a matix to a scalar: $$ (X^T X)^{-1} = \bigg(\sum_{i=1}^N X_i^2\bigg)^{-1}.  $$ From what I understand, to show that $\{\hat \beta_n\}_{n \ge 1}$ is uniformly integrable, it is enough to show that there exists a $\delta >0$ such that $\sup_n E[|\hat\beta_n|^{1 + \delta}] < \infty$ . First, I think that wlog we can consider $\hat\beta_n - \beta$ instead of $\hat\beta_n$ in the above expectation. Let's take $\delta = 1$ and define $A = (X^TX)^{-1}$ . Then \begin{align} E[|\hat\beta_n - \beta|^2] &=  E[|A\varepsilon|^2] \\ &=  E[A\varepsilon \varepsilon A^T] \\ &=  E[A E[\varepsilon \varepsilon|X]A^T] \\ &=  \sigma^2 E[AA^T] \\ &=  \sigma^2 E[(X^TX)^{-1}]. \end{align} It is commonly assumed that $\text{plim}_{n\to \infty} (X^TX/n) \stackrel{p}{\to} Q$ where $Q$ is a positive constant (usually a positive definite matrix, but since we only have a single regressor its a scalar in our case). And thus $$ \text{plim}_{n\to \infty} (X^TX/n)^{-1} \stackrel{p}{\to} Q^{-1}. $$ Write $(X^TX)^{-1} = n^{-1}(X^TX/n)^{-1}$ and substitute into the expectation above to get \begin{align} E[|\hat\beta_n - \beta|^2] \ & = \sigma^2 E[(X^TX)^{-1}] \\ & = \sigma^2 n^{-1} E[(X^TX/n)^{-1}]. \end{align} Now we know $(X^TX/n)^{-1} \stackrel{p}{\to} Q^{-1}$ and if can convert this convergence in probability into convergence in expectation we will have shown that $E[|\hat\beta_n - \beta|^2] < \infty$ (in fact it will go to zero due to the $n^{-1}$ coefficient), and thus the OLS regression coefficients are uniformly integrable. So it seems the problem of uniform integrability of the OLS regression coefficients reduces to the uniform integrability of $(X^TX/n)^{-1}$ . So here are my questions now: Does my analysis seem correct? Is $(X^TX/n)^{-1}$ uniformly integrable? If $(X^TX/n)^{-1}$ is not uniformly integrable by default is it reasonable to assume it is uniformly integrable? How strong an assumption would this be? P.S. We have $(X^TX/n)^{-1} = \bigg(\frac{1}{n} \sum_{i=1}^n X_i^2\bigg)^{-1}$ . So is the inverse of the second sample moment uniformly integrable?","In my previous question I asked whether a set of asymptotically normal random variables are uniformly integrable. In the accepted answer the poster showed that this does not hold in general. Now what about a specific case. Consider the standard ordinary least squares (OLS) regression coefficients where I have used the subscript to indicate that these are the OLS coefficients for a regression model with observations. Is uniformly integrable? Of course, is a random vector, so I am asking whether the elements of this vector, which are all asymptotically normal, are uniformly integrable. Note that the above are stochastic regressors, not fixed. Edit: Attempt for the case of single regressor with no intercept Here is an attempt for the case of a single regressor with no intercept, i.e., the regression model is where we have i.i.d observations and regressors , and . The errors are mutually independent with and they all have the same finite variance . Let denote the OLS estimate for where the subscript indicates the number of observations in the regression model. Note that because we only have a single regressor that reduces from a matix to a scalar: From what I understand, to show that is uniformly integrable, it is enough to show that there exists a such that . First, I think that wlog we can consider instead of in the above expectation. Let's take and define . Then It is commonly assumed that where is a positive constant (usually a positive definite matrix, but since we only have a single regressor its a scalar in our case). And thus Write and substitute into the expectation above to get Now we know and if can convert this convergence in probability into convergence in expectation we will have shown that (in fact it will go to zero due to the coefficient), and thus the OLS regression coefficients are uniformly integrable. So it seems the problem of uniform integrability of the OLS regression coefficients reduces to the uniform integrability of . So here are my questions now: Does my analysis seem correct? Is uniformly integrable? If is not uniformly integrable by default is it reasonable to assume it is uniformly integrable? How strong an assumption would this be? P.S. We have . So is the inverse of the second sample moment uniformly integrable?","\{X_n\}_{n \ge 1} \hat{\beta}_n = \beta + (X_n^T X_n)^{-1} X_n^T \varepsilon n n \{\hat{\beta}_n\}_{n \ge 1} \hat{\beta}_n X_n 
Y = X \beta + \varepsilon,
 n Y = [Y_1,Y_2,\dots,Y_n]^T X = [X_1,X_2,\dots,X_n]^T \varepsilon = [\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n]^T E[\varepsilon_i|X] = 0 \sigma^2 \hat \beta_n \beta n (X^T X)^{-1} 
(X^T X)^{-1} = \bigg(\sum_{i=1}^N X_i^2\bigg)^{-1}. 
 \{\hat \beta_n\}_{n \ge 1} \delta >0 \sup_n E[|\hat\beta_n|^{1 + \delta}] < \infty \hat\beta_n - \beta \hat\beta_n \delta = 1 A = (X^TX)^{-1} \begin{align}
E[|\hat\beta_n - \beta|^2]
&= 
E[|A\varepsilon|^2] \\
&= 
E[A\varepsilon \varepsilon A^T] \\
&= 
E[A E[\varepsilon \varepsilon|X]A^T] \\
&= 
\sigma^2 E[AA^T] \\
&= 
\sigma^2 E[(X^TX)^{-1}].
\end{align} \text{plim}_{n\to \infty} (X^TX/n) \stackrel{p}{\to} Q Q 
\text{plim}_{n\to \infty} (X^TX/n)^{-1} \stackrel{p}{\to} Q^{-1}.
 (X^TX)^{-1} = n^{-1}(X^TX/n)^{-1} \begin{align}
E[|\hat\beta_n - \beta|^2] \
& = \sigma^2 E[(X^TX)^{-1}] \\
& = \sigma^2 n^{-1} E[(X^TX/n)^{-1}].
\end{align} (X^TX/n)^{-1} \stackrel{p}{\to} Q^{-1} E[|\hat\beta_n - \beta|^2] < \infty n^{-1} (X^TX/n)^{-1} (X^TX/n)^{-1} (X^TX/n)^{-1} (X^TX/n)^{-1} = \bigg(\frac{1}{n} \sum_{i=1}^n X_i^2\bigg)^{-1}","['probability', 'measure-theory', 'random-variables', 'regression', 'uniform-integrability']"
98,Upper bound on expected value of $\frac{1}{(1+X)^2}$ where $X$ is binomial,Upper bound on expected value of  where  is binomial,\frac{1}{(1+X)^2} X,"As observed in Find the expected value of $E[\frac{1}{\left(X+1\right)^2}]$ where X is binomial , there exists a closed form solution to $$\mathbb{E}\left[\frac{1}{(1+X)^2}\right]$$ where $X$ is binomial random variable i.e $X \sim \textrm{Bin}(n,p)$ . I am interested in proving an upper bound which I expect to be $$\mathbb{E}\left[\frac{1}{(1+X)^2}\right] \leq \frac{c(p)}{n^2}$$ for $n>1$ where $c(p)$ is some constant dependent on $p$ . Any ideas on how to approach this? EDIT: After @NN2's answer, I changed the question from $O(1/n^2)$ to $c(p)/n^2$ for $n>1$ .","As observed in Find the expected value of $E[\frac{1}{\left(X+1\right)^2}]$ where X is binomial , there exists a closed form solution to where is binomial random variable i.e . I am interested in proving an upper bound which I expect to be for where is some constant dependent on . Any ideas on how to approach this? EDIT: After @NN2's answer, I changed the question from to for .","\mathbb{E}\left[\frac{1}{(1+X)^2}\right] X X \sim \textrm{Bin}(n,p) \mathbb{E}\left[\frac{1}{(1+X)^2}\right] \leq \frac{c(p)}{n^2} n>1 c(p) p O(1/n^2) c(p)/n^2 n>1","['probability', 'statistics', 'expected-value', 'binomial-distribution']"
99,Probability: Conditioning equations,Probability: Conditioning equations,,"I saw in many different context that any probability result that is true for unconditional probability remains true if everything is conditioned on some event. For instance, consider the following equation: $$P(B\cap C)=P(B)P(C|B).$$ Then by conditioning both sides on $A$ , we get $$P(B\cap C|A)=P(B|A)P(C|B\cap A).$$ However, it is possible to find counterexamples as given here . So I was wondering when this conditioning the both sides of equality operation is valid. I am, in particular, interested   in the following equality: $$P(X_4, X_3, X_2, X_1)=P(X_4 \mid X_3, X_2, X_1)\cdot \mathrm P(X_3 \mid X_2, X_1)\cdot \mathrm P(X_2 \mid X_1)\cdot \mathrm P(X_1).$$ Is it possible to condition both sides on $A$ and get $$P(X_4, X_3, X_2, X_1|A)=P(X_4 \mid X_3, X_2, X_1,A)\cdot \mathrm P(X_3 \mid X_2, X_1,A)\cdot \mathrm P(X_2 \mid X_1,A)\cdot \mathrm P(X_1|A)$$ here? Many thanks in advance.","I saw in many different context that any probability result that is true for unconditional probability remains true if everything is conditioned on some event. For instance, consider the following equation: Then by conditioning both sides on , we get However, it is possible to find counterexamples as given here . So I was wondering when this conditioning the both sides of equality operation is valid. I am, in particular, interested   in the following equality: Is it possible to condition both sides on and get here? Many thanks in advance.","P(B\cap C)=P(B)P(C|B). A P(B\cap C|A)=P(B|A)P(C|B\cap A). P(X_4, X_3, X_2, X_1)=P(X_4 \mid X_3, X_2, X_1)\cdot \mathrm P(X_3 \mid X_2, X_1)\cdot \mathrm P(X_2 \mid X_1)\cdot \mathrm P(X_1). A P(X_4, X_3, X_2, X_1|A)=P(X_4 \mid X_3, X_2, X_1,A)\cdot \mathrm P(X_3 \mid X_2, X_1,A)\cdot \mathrm P(X_2 \mid X_1,A)\cdot \mathrm P(X_1|A)",['probability']
