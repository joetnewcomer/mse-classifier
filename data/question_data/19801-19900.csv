,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Origin and use of an identity of formal power series: $\det(1 - \psi T) = \exp \left(-\sum_{s=1}^{\infty} \text{Tr}(\psi^{s})T^{s}/s\right)$,Origin and use of an identity of formal power series:,\det(1 - \psi T) = \exp \left(-\sum_{s=1}^{\infty} \text{Tr}(\psi^{s})T^{s}/s\right),"The following is a historical question , but first some background : Let $\psi$ be a linear operator from a vector space to itself. The following two expressions, viewed as formal power series, can be shown to be equal (even for an infinite dimensional vector space): $$ \det(1 - \psi T) = \exp \left(-\sum_{s=1}^{\infty} \text{Tr}(\psi^{s})T^{s}/s\right) $$ Here is a simple example using the associated matrix in the finite dimensional case: For more on this topic (and how to ensure the definitions make sense for infinite dimensional vector spaces) see: N. Koblitz, p-adic Numbers, p-adic Analysis, and Zeta-Functions , Springer-Verlag, New York, 1984. In particular, see Koblitz's book for how ($p$-adic versions of) this identity can be used in Bernard Dwork's proof of the rationality of the zeta function (resolving the first of the Weil Conjectures). I would welcome any insight as to how one thinks up, observes, or conjectures such an identity in the first place, but my question is: what is the origin of this identity and where else has it appeared (perhaps in a somewhat modified form)?","The following is a historical question , but first some background : Let $\psi$ be a linear operator from a vector space to itself. The following two expressions, viewed as formal power series, can be shown to be equal (even for an infinite dimensional vector space): $$ \det(1 - \psi T) = \exp \left(-\sum_{s=1}^{\infty} \text{Tr}(\psi^{s})T^{s}/s\right) $$ Here is a simple example using the associated matrix in the finite dimensional case: For more on this topic (and how to ensure the definitions make sense for infinite dimensional vector spaces) see: N. Koblitz, p-adic Numbers, p-adic Analysis, and Zeta-Functions , Springer-Verlag, New York, 1984. In particular, see Koblitz's book for how ($p$-adic versions of) this identity can be used in Bernard Dwork's proof of the rationality of the zeta function (resolving the first of the Weil Conjectures). I would welcome any insight as to how one thinks up, observes, or conjectures such an identity in the first place, but my question is: what is the origin of this identity and where else has it appeared (perhaps in a somewhat modified form)?",,"['linear-algebra', 'number-theory', 'power-series', 'determinant', 'p-adic-number-theory']"
1,How to geometrically interpret $\sum^{p}_{1}\lambda_{i}(A)=\operatorname{tr}(A)$?,How to geometrically interpret ?,\sum^{p}_{1}\lambda_{i}(A)=\operatorname{tr}(A),"$A$ is a $p\times p$ real matrix and $\lambda_{i}$ are its eigenvalues. $\operatorname{tr}(A)$ is the trace of $A$. How to geometrically interpret $\sum^{p}_{1}\lambda_{i}(A)=\operatorname{tr}(A)$? I have learnt linear algebra for two semesters. I knew the basic concepts of trace and eigenvector. The answer in the mathoverflow interprets geometrical meaning of trace.But,how to interpret the trace is equal to the sum of eigenvalue geometrically?","$A$ is a $p\times p$ real matrix and $\lambda_{i}$ are its eigenvalues. $\operatorname{tr}(A)$ is the trace of $A$. How to geometrically interpret $\sum^{p}_{1}\lambda_{i}(A)=\operatorname{tr}(A)$? I have learnt linear algebra for two semesters. I knew the basic concepts of trace and eigenvector. The answer in the mathoverflow interprets geometrical meaning of trace.But,how to interpret the trace is equal to the sum of eigenvalue geometrically?",,['linear-algebra']
2,On linearly independent matrices,On linearly independent matrices,,"I have been reading J.S. Milne's lecture notes on fields and Galois theory and came across the normal basis theorem (Thm 5.18 on page 66 of the notes). Trying to find my own proof, the following problem in linear algebra quickly arose: Question: Let $F$ be a field. Given linearly independent matrices $A_1, \dots, A_n \in \operatorname{GL}_n(F)$, does there necessarily exist some $b\in F^n$ such that $A_1b, \dots, A_nb$ are linearly independent over $F$? This is clearly not true if the matrices are not linearly independent. Also, if they are not invertible, the claim is false in general: e.g. set $n=2$ and consider $$A_1 = \begin{pmatrix} 1 & 0 \\ 0& 0\end{pmatrix},\quad  A_2 = \begin{pmatrix} 0 & 1 \\ 0& 0\end{pmatrix}$$ Going through a case-by-case analysis, I think I can prove the claim for $n=2$, so there seems to be some hope... Any help would be appreciated. Thanks!","I have been reading J.S. Milne's lecture notes on fields and Galois theory and came across the normal basis theorem (Thm 5.18 on page 66 of the notes). Trying to find my own proof, the following problem in linear algebra quickly arose: Question: Let $F$ be a field. Given linearly independent matrices $A_1, \dots, A_n \in \operatorname{GL}_n(F)$, does there necessarily exist some $b\in F^n$ such that $A_1b, \dots, A_nb$ are linearly independent over $F$? This is clearly not true if the matrices are not linearly independent. Also, if they are not invertible, the claim is false in general: e.g. set $n=2$ and consider $$A_1 = \begin{pmatrix} 1 & 0 \\ 0& 0\end{pmatrix},\quad  A_2 = \begin{pmatrix} 0 & 1 \\ 0& 0\end{pmatrix}$$ Going through a case-by-case analysis, I think I can prove the claim for $n=2$, so there seems to be some hope... Any help would be appreciated. Thanks!",,"['linear-algebra', 'abstract-algebra']"
3,"What matrices preserve the $L_1$ norm for positive, unit norm vectors?","What matrices preserve the  norm for positive, unit norm vectors?",L_1,"It's easy to show that orthogonal/unitary matrices preserve the $L_2$ norm of a vector, but if I want a transformation that preserves the $L_1$ norm, what can I deduce about the matrices that do this?  I feel like it should be something like the columns sum to 1, but I can't manage to prove it. EDIT: To be more explicit, I'm looking at stochastic transition matrices that act on vectors that represent probability distributions, i.e. vectors whose elements are positive and sum to 1.  For instance, the matrix $$ M = \left(\begin{array}{ccc}1 & 1/4 & 0 \\0 & 1/2 & 0 \\0 & 1/4 & 1\end{array}\right) $$ acting on $$ x=\left(\begin{array}{c}0 \\1 \\0\end{array}\right) $$ gives $$ M \cdot x = \left(\begin{array}{c}1/4 \\1/2 \\1/4\end{array}\right)\:, $$ a vector whose elements also sum to 1. So I suppose the set of vectors whose isometries I care about is more restricted than the completely general case, which is why I was confused about people saying that permutation matrices were what I was after. Sooo... given the vectors are positive and have entries that sum to 1, can we say anything more exact about the matrices that preserve this property?","It's easy to show that orthogonal/unitary matrices preserve the $L_2$ norm of a vector, but if I want a transformation that preserves the $L_1$ norm, what can I deduce about the matrices that do this?  I feel like it should be something like the columns sum to 1, but I can't manage to prove it. EDIT: To be more explicit, I'm looking at stochastic transition matrices that act on vectors that represent probability distributions, i.e. vectors whose elements are positive and sum to 1.  For instance, the matrix $$ M = \left(\begin{array}{ccc}1 & 1/4 & 0 \\0 & 1/2 & 0 \\0 & 1/4 & 1\end{array}\right) $$ acting on $$ x=\left(\begin{array}{c}0 \\1 \\0\end{array}\right) $$ gives $$ M \cdot x = \left(\begin{array}{c}1/4 \\1/2 \\1/4\end{array}\right)\:, $$ a vector whose elements also sum to 1. So I suppose the set of vectors whose isometries I care about is more restricted than the completely general case, which is why I was confused about people saying that permutation matrices were what I was after. Sooo... given the vectors are positive and have entries that sum to 1, can we say anything more exact about the matrices that preserve this property?",,"['linear-algebra', 'matrices', 'vector-spaces', 'normed-spaces']"
4,A geometric way to reason about Schur complements?,A geometric way to reason about Schur complements?,,"I am trying to understand some theorems whose standard proofs seem to involve extensive matrix manipulations. I'm finding it a bit difficult to see a big picture behind all the matrix algebra, and I'd like to ask two questions which will hopefully help me understand. Question 1: What geometric interpretations can be given to the Schur complement of a matrix? To be precise, I need not just any geometric interpretation, but one which will help me make sense of the proofs I am looking at. So here is something more concrete: a theorem about some properties of the Schur complement. Theorem: Suppose $$EA=AF,$$ where $E$ is lower triangular and $F$ is upper triangular: $$ E = \begin{pmatrix} e & 0 \\ * & E' \end{pmatrix}, ~~~ F = \begin{pmatrix} f & * \\ 0 & F' \end{pmatrix}$$ Further, suppose that the $(1,1)$ entry of $A$ is nonzero, so that we can take the Schur complement of A with respect to it, which we will denote by $S$. Then $$E'S = S F'.$$ This is a special case of Theorem 6 from these lecture notes . Question 2: It feels like this theorem should have both a geometric interpretation and geometric proof. Can someone provide these? Thank you!","I am trying to understand some theorems whose standard proofs seem to involve extensive matrix manipulations. I'm finding it a bit difficult to see a big picture behind all the matrix algebra, and I'd like to ask two questions which will hopefully help me understand. Question 1: What geometric interpretations can be given to the Schur complement of a matrix? To be precise, I need not just any geometric interpretation, but one which will help me make sense of the proofs I am looking at. So here is something more concrete: a theorem about some properties of the Schur complement. Theorem: Suppose $$EA=AF,$$ where $E$ is lower triangular and $F$ is upper triangular: $$ E = \begin{pmatrix} e & 0 \\ * & E' \end{pmatrix}, ~~~ F = \begin{pmatrix} f & * \\ 0 & F' \end{pmatrix}$$ Further, suppose that the $(1,1)$ entry of $A$ is nonzero, so that we can take the Schur complement of A with respect to it, which we will denote by $S$. Then $$E'S = S F'.$$ This is a special case of Theorem 6 from these lecture notes . Question 2: It feels like this theorem should have both a geometric interpretation and geometric proof. Can someone provide these? Thank you!",,"['linear-algebra', 'matrices', 'block-matrices', 'schur-complement']"
5,In-place inversion of large matrices,In-place inversion of large matrices,,"In Solving very large matrices in ""pieces"" there is a way shown to solve matrix inversion in pieces. Is it possible to apply the method in-place? I am refering to the answer in the referenced question that starts with: $\textbf{A}=\begin{pmatrix}\textbf{E}&\textbf{F}\\\\ \textbf{G}&\textbf{H}\end{pmatrix}$ And computes the inverse via: $\textbf{A}^{-1}=\begin{pmatrix}\textbf{E}^{-1}+\textbf{E}^{-1}\textbf{F}\textbf{S}^{-1}\textbf{G}\textbf{E}^{-1}&-\textbf{E}^{-1}\textbf{F}\textbf{S}^{-1}\\\\ -\textbf{S}^{-1}\textbf{G}\textbf{E}^{-1}&\textbf{S}^{-1}\end{pmatrix}$, where $\textbf{S} = \textbf{H}-\textbf{G}\textbf{E}^{-1}\textbf{F}$ Edit 05.10.2017: Terrence Tao makes use of the same in a recent blog post: https://terrytao.wordpress.com/2017/09/16/inverting-the-schur-complement-and-large-dimensional-gelfand-tsetlin-patterns/ , and puts it in relation to a ""Schur complement"".","In Solving very large matrices in ""pieces"" there is a way shown to solve matrix inversion in pieces. Is it possible to apply the method in-place? I am refering to the answer in the referenced question that starts with: $\textbf{A}=\begin{pmatrix}\textbf{E}&\textbf{F}\\\\ \textbf{G}&\textbf{H}\end{pmatrix}$ And computes the inverse via: $\textbf{A}^{-1}=\begin{pmatrix}\textbf{E}^{-1}+\textbf{E}^{-1}\textbf{F}\textbf{S}^{-1}\textbf{G}\textbf{E}^{-1}&-\textbf{E}^{-1}\textbf{F}\textbf{S}^{-1}\\\\ -\textbf{S}^{-1}\textbf{G}\textbf{E}^{-1}&\textbf{S}^{-1}\end{pmatrix}$, where $\textbf{S} = \textbf{H}-\textbf{G}\textbf{E}^{-1}\textbf{F}$ Edit 05.10.2017: Terrence Tao makes use of the same in a recent blog post: https://terrytao.wordpress.com/2017/09/16/inverting-the-schur-complement-and-large-dimensional-gelfand-tsetlin-patterns/ , and puts it in relation to a ""Schur complement"".",,['linear-algebra']
6,What are left and right singular vectors in SVD?,What are left and right singular vectors in SVD?,,"Let $USV^T$ be a singular value decomposition of matrix $A$ . In the textbook ""Linear Algebra and Its Applications"" by D. C. Lay et. al., where SVD is introduced, it says that ""the columns of $U$ in such a decomposition are called left singular vectors of $A$ , and the columns of $V$ are called right singular vectors of $A$ ."" But it does not make any connections with the eigenvectors of $A^T\!A$ . It also says that ""the matrices $U$ and $V$ are not uniquely determined by $A$ . But in this web page it says that ""the eigenvectors of $A^T\!A$ make up the columns of $V$ , the eigenvectors of $AA^T$ make up the columns of $U$ ."" I'm confused about the relationship between the left and right singular vectors (that is columns of $U$ and $V$ ) and the eigenvectors of $A^T\!A$ and $AA^T$ . Any clarification is appreciated.","Let be a singular value decomposition of matrix . In the textbook ""Linear Algebra and Its Applications"" by D. C. Lay et. al., where SVD is introduced, it says that ""the columns of in such a decomposition are called left singular vectors of , and the columns of are called right singular vectors of ."" But it does not make any connections with the eigenvectors of . It also says that ""the matrices and are not uniquely determined by . But in this web page it says that ""the eigenvectors of make up the columns of , the eigenvectors of make up the columns of ."" I'm confused about the relationship between the left and right singular vectors (that is columns of and ) and the eigenvectors of and . Any clarification is appreciated.",USV^T A U A V A A^T\!A U V A A^T\!A V AA^T U U V A^T\!A AA^T,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
7,Determinant of a companion matrix,Determinant of a companion matrix,,I have to find determinant of $$A := \begin{bmatrix}0 & 0 & 0 & ... &0 & a_0 \\ -1 & 0 & 0 & ... &0 & a_1\\ 0 & -1 & 0 & ... &0 & a_2 \\ 0 & 0 & -1 & ... &0 & a_3 \\ \vdots &\vdots &\vdots & \ddots &\vdots&\vdots \\0 & 0 & 0 & ... &-1 & a_{n-1}     \end{bmatrix} + t I_{n \times n}$$ It is not a difficult thing to do. My method is as follows : $$\begin{bmatrix}0 & 0 & 0 & ... &0 & a_0 \\ -1 & 0 & 0 & ... &0 & a_1\\ 0 & -1 & 0 & ... &0 & a_2 \\ 0 & 0 & -1 & ... &0 & a_3 \\ \vdots &\vdots &\vdots & \ddots &\vdots&\vdots \\0 & 0 & 0 & ... &-1 & a_{n-1}     \end{bmatrix} + t I_{n \times n} = \begin{bmatrix}t & 0 & 0 & ... &0 & a_0 \\ -1 & t & 0 & ... &0 & a_1\\ 0 & -1 & t & ... &0 & a_2 \\ 0 & 0 & -1 & ... &0 & a_3 \\ \vdots &\vdots &\vdots & \ddots &\vdots&\vdots \\0 & 0 & 0 & ... &-1 & a_{n-1} + t     \end{bmatrix} $$ Performing the row reduction of type $R_{k+1} \to R_{k+1} + \dfrac{1}{t}R_k$ I get an upper triangular matrix $$\begin{bmatrix}t & 0 & 0 & ... &0 & a_0 \\ 0 & t & 0 & ... &0 & a_1 + \dfrac {a_0} t\\ 0 & 0 & t & ... &0 & a_2 + \dfrac{a_1}{t} + \dfrac {a_0} {t^2} \\ 0 & 0 & 0 & ... &0 & a_3 + \dfrac{a_2}{t} + \dfrac{a_1}{t^2} + \dfrac {a_0} {t^3} \\ \vdots &\vdots &\vdots & \ddots &\vdots&\vdots \\0 & 0 & 0 & ... &0 & a_{n-1} + t   + \sum_{k=0}^{n-2} \dfrac{a_{k}}{t^{(n-1) - k }}  \end{bmatrix} $$ Determinant of which is $t^n + \sum^{n-1}_{k = 1} a_k t^{k}$ . My friend says this is not a rigorous proof and that I have to use induction to prove $$\det A = t^n + \sum^{n-1}_{k = 1} a_k t^{k}$$ She says that I have only found a formula for $\det A$ and I can't be sure if it works for all $n\in \Bbb N$ without a proof. Is she correct?,I have to find determinant of It is not a difficult thing to do. My method is as follows : Performing the row reduction of type I get an upper triangular matrix Determinant of which is . My friend says this is not a rigorous proof and that I have to use induction to prove She says that I have only found a formula for and I can't be sure if it works for all without a proof. Is she correct?,A := \begin{bmatrix}0 & 0 & 0 & ... &0 & a_0 \\ -1 & 0 & 0 & ... &0 & a_1\\ 0 & -1 & 0 & ... &0 & a_2 \\ 0 & 0 & -1 & ... &0 & a_3 \\ \vdots &\vdots &\vdots & \ddots &\vdots&\vdots \\0 & 0 & 0 & ... &-1 & a_{n-1}     \end{bmatrix} + t I_{n \times n} \begin{bmatrix}0 & 0 & 0 & ... &0 & a_0 \\ -1 & 0 & 0 & ... &0 & a_1\\ 0 & -1 & 0 & ... &0 & a_2 \\ 0 & 0 & -1 & ... &0 & a_3 \\ \vdots &\vdots &\vdots & \ddots &\vdots&\vdots \\0 & 0 & 0 & ... &-1 & a_{n-1}     \end{bmatrix} + t I_{n \times n} = \begin{bmatrix}t & 0 & 0 & ... &0 & a_0 \\ -1 & t & 0 & ... &0 & a_1\\ 0 & -1 & t & ... &0 & a_2 \\ 0 & 0 & -1 & ... &0 & a_3 \\ \vdots &\vdots &\vdots & \ddots &\vdots&\vdots \\0 & 0 & 0 & ... &-1 & a_{n-1} + t     \end{bmatrix}  R_{k+1} \to R_{k+1} + \dfrac{1}{t}R_k \begin{bmatrix}t & 0 & 0 & ... &0 & a_0 \\ 0 & t & 0 & ... &0 & a_1 + \dfrac {a_0} t\\ 0 & 0 & t & ... &0 & a_2 + \dfrac{a_1}{t} + \dfrac {a_0} {t^2} \\ 0 & 0 & 0 & ... &0 & a_3 + \dfrac{a_2}{t} + \dfrac{a_1}{t^2} + \dfrac {a_0} {t^3} \\ \vdots &\vdots &\vdots & \ddots &\vdots&\vdots \\0 & 0 & 0 & ... &0 & a_{n-1} + t   + \sum_{k=0}^{n-2} \dfrac{a_{k}}{t^{(n-1) - k }}  \end{bmatrix}  t^n + \sum^{n-1}_{k = 1} a_k t^{k} \det A = t^n + \sum^{n-1}_{k = 1} a_k t^{k} \det A n\in \Bbb N,"['linear-algebra', 'matrices', 'proof-writing', 'determinant', 'companion-matrices']"
8,Positive definiteness of difference of inverse matrices,Positive definiteness of difference of inverse matrices,,"Let $A$ and $B$ be two $n \times n$ symmetric and positive definite matrices. If $A \prec B$, then is it true that $B^{-1} \prec A^{-1}$? Here, $A \prec B$ means that $B-A$ is positive definite.","Let $A$ and $B$ be two $n \times n$ symmetric and positive definite matrices. If $A \prec B$, then is it true that $B^{-1} \prec A^{-1}$? Here, $A \prec B$ means that $B-A$ is positive definite.",,"['linear-algebra', 'matrices', 'inverse', 'symmetric-matrices', 'positive-definite']"
9,"If a commutator has an eigenvalue $0$, do the two operators share an eigenvector?","If a commutator has an eigenvalue , do the two operators share an eigenvector?",0,"Let $A,B$ be two diagonalizable linear operators, such that their commutator $\left[A,B\right] = AB-BA$ has an eigenvalue $\lambda = 0$. Does this mean $A$ and $B$ share an eigenvector? Of course, the vice-versa is correct. If they do share an eigenvector, the commutator necessarily has an eigenvalue of $0$. I'm not sure though if this is a sufficient condition, or only a necessary one.","Let $A,B$ be two diagonalizable linear operators, such that their commutator $\left[A,B\right] = AB-BA$ has an eigenvalue $\lambda = 0$. Does this mean $A$ and $B$ share an eigenvector? Of course, the vice-versa is correct. If they do share an eigenvector, the commutator necessarily has an eigenvalue of $0$. I'm not sure though if this is a sufficient condition, or only a necessary one.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
10,How exactly does the sign of the dot product determine the angle between two vectors?,How exactly does the sign of the dot product determine the angle between two vectors?,,"I am told that $v\cdot w=0$ means that the angle between the vectors $v$ and $w$ is $90$ degrees. Then I am told that the sign of $v\cdot w$ (when it isn't equal to zero) determines whether the angle between vectors $v$ and $w$ is above or below $90$ degrees; The angle is above $90$ degrees when $v\cdot w<0$ and below $90$ degrees when $v\cdot w>0$ I have a picture from the book Introduction to Linear Algebra by Gilbert Strang, but it's quite confusing for me. I don't understand how the above information is reflected in this diagram. I would like an explanation of this for me as it will help me answer a question from the section's accompanying problem set.","I am told that $v\cdot w=0$ means that the angle between the vectors $v$ and $w$ is $90$ degrees. Then I am told that the sign of $v\cdot w$ (when it isn't equal to zero) determines whether the angle between vectors $v$ and $w$ is above or below $90$ degrees; The angle is above $90$ degrees when $v\cdot w<0$ and below $90$ degrees when $v\cdot w>0$ I have a picture from the book Introduction to Linear Algebra by Gilbert Strang, but it's quite confusing for me. I don't understand how the above information is reflected in this diagram. I would like an explanation of this for me as it will help me answer a question from the section's accompanying problem set.",,['linear-algebra']
11,Definition of Trace of Linear Operator,Definition of Trace of Linear Operator,,The trace of a linear operator $f$ can be defined as the trace of the matrix $A$ representing $f$ with respect to some basis $B$.  However the trace does not depend on the basis chosen.  This suggests to me that there is some definition of the trace of $f$ independent of matrices (and thus coordinate-independent).  Any suggestions as to how I could define $\mathrm {tr}(f) $ without defining it as $\mathrm {tr}([f]_B)$?,The trace of a linear operator $f$ can be defined as the trace of the matrix $A$ representing $f$ with respect to some basis $B$.  However the trace does not depend on the basis chosen.  This suggests to me that there is some definition of the trace of $f$ independent of matrices (and thus coordinate-independent).  Any suggestions as to how I could define $\mathrm {tr}(f) $ without defining it as $\mathrm {tr}([f]_B)$?,,[]
12,Matrices $B$ that commute with every matrix commuting with $A$,Matrices  that commute with every matrix commuting with,B A,"There have been many questions in the vein of this one, but I can't find one that answers it specifically. Suppose $A,B\in M_n(\mathbb C)$ are two matrices such that, for any other matrix $C\in M_n(\mathbb C)$ , $$AC=CA\implies BC=CB.$$ Prove that $B=p(A)$ for a polynomial $p\in\mathbb C[t]$ . Nothing is assumed about $A$ being diagonalisable or having distinct eigenvalues or being invertible. Edit: after further work, this might not actually be true. If it's not, a counterexample would be great.","There have been many questions in the vein of this one, but I can't find one that answers it specifically. Suppose are two matrices such that, for any other matrix , Prove that for a polynomial . Nothing is assumed about being diagonalisable or having distinct eigenvalues or being invertible. Edit: after further work, this might not actually be true. If it's not, a counterexample would be great.","A,B\in M_n(\mathbb C) C\in M_n(\mathbb C) AC=CA\implies BC=CB. B=p(A) p\in\mathbb C[t] A","['linear-algebra', 'matrices', 'noncommutative-algebra']"
13,Product of nilpotent matrices.,Product of nilpotent matrices.,,"Let $A$ and $B$ be $n \times n$ complex matrices  and let  $[A,B] = AB - BA$. Question: If  $A , B$ and  $[A,B]$  are all nilpotent matrices, is it necessarily true that  $\operatorname{trace}(AB) = 0$? If,in fact, $[A,B] = 0$, then we can take $A$ and $B$ to be strictly upper triangular matrices so that the answer would be yes in this very special case.","Let $A$ and $B$ be $n \times n$ complex matrices  and let  $[A,B] = AB - BA$. Question: If  $A , B$ and  $[A,B]$  are all nilpotent matrices, is it necessarily true that  $\operatorname{trace}(AB) = 0$? If,in fact, $[A,B] = 0$, then we can take $A$ and $B$ to be strictly upper triangular matrices so that the answer would be yes in this very special case.",,"['linear-algebra', 'matrices']"
14,Tensor Decomposition,Tensor Decomposition,,"Consider a tensor product $$ V^{\otimes n} = \underbrace{V\otimes\cdots\otimes V}_{n} $$ where $V$ is a vector space over $\mathbb R$, $\dim V = m$ , hence $\dim V^{\otimes n} = m^n$ . So every $A \in V^{\otimes n}$ can be represented as $$A = \sum_{i=1}^r a^i_1 \otimes a^i_2 \ldots \otimes a^i_n, \;\;\; a_i \in V $$ in a non-unique way. Taking $R$ to be minimum $r$ among all the possible decompositions of A. $$R = \min \left \{ r : A = \sum_{i=1}^r a^i_1 \otimes a^i_2 \ldots \otimes a^i_n, \;\;\; a_i \in V \right \}$$ How many tensors have certain $R$ ? How many tensors have $R=1$? Or $R = m^n$ ? What is the typical $R$ (mean, median mean, the most probable), what is the distribution? IMPORTANT How should I imagine (picture) tensors for which $R$ is (near) maximum? What hinders them from decomposition? Maybe there are some experimental data. I'm mostly interested in high $m$'s and $n$'s, though every answer is welcome.","Consider a tensor product $$ V^{\otimes n} = \underbrace{V\otimes\cdots\otimes V}_{n} $$ where $V$ is a vector space over $\mathbb R$, $\dim V = m$ , hence $\dim V^{\otimes n} = m^n$ . So every $A \in V^{\otimes n}$ can be represented as $$A = \sum_{i=1}^r a^i_1 \otimes a^i_2 \ldots \otimes a^i_n, \;\;\; a_i \in V $$ in a non-unique way. Taking $R$ to be minimum $r$ among all the possible decompositions of A. $$R = \min \left \{ r : A = \sum_{i=1}^r a^i_1 \otimes a^i_2 \ldots \otimes a^i_n, \;\;\; a_i \in V \right \}$$ How many tensors have certain $R$ ? How many tensors have $R=1$? Or $R = m^n$ ? What is the typical $R$ (mean, median mean, the most probable), what is the distribution? IMPORTANT How should I imagine (picture) tensors for which $R$ is (near) maximum? What hinders them from decomposition? Maybe there are some experimental data. I'm mostly interested in high $m$'s and $n$'s, though every answer is welcome.",,"['linear-algebra', 'tensor-products', 'multilinear-algebra']"
15,Does every $4$-dimensional lattice have a minimal system that's also a lattice basis?,Does every -dimensional lattice have a minimal system that's also a lattice basis?,4,"An full $n$ -dimensional lattice $\Lambda$ is a discrete subgroup of $\mathbb{R}^n$ (equipped with some norm $\lVert \cdot \rVert$ ) containing $n$ linearly independent points. If $\Lambda = \{ A z, z\in \mathbb{Z}^n\}$ for $A \in GL(n,\mathbb{R})$ , we call the $n$ columns of $A$ a basis of $\Lambda$ (every full lattice has a basis). We call $n$ points $l_1,\dots,l_n \in \Lambda$ a minimal system if for $k\in\{1,\dots,n\},$ $$\lVert l_k \rVert = \min \{ \lVert l \rVert: l \in \Lambda \setminus \left< l_1,\dots,l_{k-1}\right>_\mathbb{R}\}.$$ Let's just consider the standard $2$ -norm $\lVert x \rVert = \left< x,x\right>^\frac{1}{2}$ . In two dimensions, every minimal system is also a basis. In five dimensions, this is no longer true, as is stated in the answer to this post . The lattice generated by $$ A = \begin{pmatrix} 1 & 0 & 0 & 0 & \frac{1}{2} \\ 0 & 1 & 0 & 0 & \frac{1}{2} \\ 0 & 0 & 1 & 0 & \frac{1}{2} \\ 0 & 0 & 0 & 1 & \frac{1}{2} \\ 0 & 0 & 0 & 0 & \frac{1}{2} \\ \end{pmatrix} $$ has the five standard unit vectors as a minimal system, but they do not form a basis of the lattice. In four dimensions, the lattice generated by the basis $$ A = \begin{pmatrix} 1 & 0 & 0 & \frac{1}{2} \\ 0 & 1 & 0 & \frac{1}{2} \\ 0 & 0 & 1 & \frac{1}{2} \\ 0 & 0 & 0 & \frac{1}{2} \\ \end{pmatrix} $$ has both $A$ and the standard basis as minimal systems. This is not a stable property: If the last vector is perturbed a little, the minimal system is unique (up to reflections) and a basis again. This has led to my feeling that this is an extremal case, and the situation is as follows: Every four-dimensional lattice has a minimal system that's also a lattice basis. Furthermore, only a zero set of matrices generate lattices that have minimal systems that are not bases. Is that true?","An full -dimensional lattice is a discrete subgroup of (equipped with some norm ) containing linearly independent points. If for , we call the columns of a basis of (every full lattice has a basis). We call points a minimal system if for Let's just consider the standard -norm . In two dimensions, every minimal system is also a basis. In five dimensions, this is no longer true, as is stated in the answer to this post . The lattice generated by has the five standard unit vectors as a minimal system, but they do not form a basis of the lattice. In four dimensions, the lattice generated by the basis has both and the standard basis as minimal systems. This is not a stable property: If the last vector is perturbed a little, the minimal system is unique (up to reflections) and a basis again. This has led to my feeling that this is an extremal case, and the situation is as follows: Every four-dimensional lattice has a minimal system that's also a lattice basis. Furthermore, only a zero set of matrices generate lattices that have minimal systems that are not bases. Is that true?","n \Lambda \mathbb{R}^n \lVert \cdot \rVert n \Lambda = \{ A z, z\in \mathbb{Z}^n\} A \in GL(n,\mathbb{R}) n A \Lambda n l_1,\dots,l_n \in \Lambda k\in\{1,\dots,n\}, \lVert l_k \rVert = \min \{ \lVert l \rVert: l \in \Lambda \setminus \left< l_1,\dots,l_{k-1}\right>_\mathbb{R}\}. 2 \lVert x \rVert = \left< x,x\right>^\frac{1}{2}  A = \begin{pmatrix}
1 & 0 & 0 & 0 & \frac{1}{2} \\
0 & 1 & 0 & 0 & \frac{1}{2} \\
0 & 0 & 1 & 0 & \frac{1}{2} \\
0 & 0 & 0 & 1 & \frac{1}{2} \\
0 & 0 & 0 & 0 & \frac{1}{2} \\
\end{pmatrix}
 
A = \begin{pmatrix}
1 & 0 & 0 & \frac{1}{2} \\
0 & 1 & 0 & \frac{1}{2} \\
0 & 0 & 1 & \frac{1}{2} \\
0 & 0 & 0 & \frac{1}{2} \\
\end{pmatrix}
 A","['linear-algebra', 'integer-lattices', 'diophantine-approximation']"
16,Maximize the trace of a matrix by permuting its rows,Maximize the trace of a matrix by permuting its rows,,"I have been struggling with a combinatorial problem that eventually translates to the following: Given an $n \times n$ nonnegative matrix, find a permutation of the rows that maximizes the trace. I could compute all $n!$ permutations and find the one that maximizes the trace, but that is not computationally efficient. What is the most efficient way of accomplishing this? If it helps, this is the same as finding $n$ entries of the matrix with no two of them in the same row or column, so that the sum of these entries is maximal.","I have been struggling with a combinatorial problem that eventually translates to the following: Given an $n \times n$ nonnegative matrix, find a permutation of the rows that maximizes the trace. I could compute all $n!$ permutations and find the one that maximizes the trace, but that is not computationally efficient. What is the most efficient way of accomplishing this? If it helps, this is the same as finding $n$ entries of the matrix with no two of them in the same row or column, so that the sum of these entries is maximal.",,"['linear-algebra', 'matrices', 'algorithms', 'linear-programming', 'discrete-optimization']"
17,"Meaning cone, ray, fan for polytopes","Meaning cone, ray, fan for polytopes",,"I'm trying to understand some mathematical operations and definitions for my project. Could you explain the meanings? $P_b =\{x∈\mathbb{R}^d : Ax≤b\}$ is a polytope. Let's have a 10x3 matrix $Ax≤b$ $$ \begin{bmatrix} 0 & 1 & 1 \\  1 & 0 &1 \\  1 & 0 & 0\\  0 & 1 & 0\\  0 & 0 & -1 \\  0 &  0& 1\\  0 &  -1& 0\\  0 &  -1& 1\\  -1 &  0& 0\\  -1 &  0& 1 \end{bmatrix} \begin{bmatrix} x_1\\  x_2\\  x_3 \end{bmatrix} \leq \begin{bmatrix} 4\\ 4\\ 3\\  3\\  0\\ 2\\ 0\\ 1\\ 0\\ 1  \end{bmatrix} $$ Let us denote the set of such right hand side vectors $b$ by $U(A)=\{b∈\mathbb{R}^m : P_b \neq ∅\}.$ Switching from the H-representation to its V- representation, the cone U (A) is generated by 9 rays and 3 lines in $Z^{10}$ . The normal cone of a face F of a polytope P in $R^d$ is the set $N(F;P)=\{v∈\mathbb{R}^dd : v^⊤x=h(P,v)~for~all~ x∈F\}.$ The dimension of the normal cone of a $k$ -dimensional face is $(d − k)$ . The normal fan $N(P)$ of $P$ , which is the collection of the normal cones of all faces of $P$ , is a complete fan in $\mathbb{R}^d$ . I have difficulty in understanding of the terms. So, what is the exact meaning of $U(A)$ here? What are the $b_{1,2,...}$ letters and how obtained? How is the 9 rays and 3 lines expression written? Can you explain also meanings of normal fan,cone and complete fan? Article can be found from here. I want to grasp all of the article also I have to, but at least up to there is enough. @Edit: I think it's a kinda Millenium Problem . If not, isn't there any helper mathematician?","I'm trying to understand some mathematical operations and definitions for my project. Could you explain the meanings? is a polytope. Let's have a 10x3 matrix Let us denote the set of such right hand side vectors by Switching from the H-representation to its V- representation, the cone U (A) is generated by 9 rays and 3 lines in . The normal cone of a face F of a polytope P in is the set The dimension of the normal cone of a -dimensional face is . The normal fan of , which is the collection of the normal cones of all faces of , is a complete fan in . I have difficulty in understanding of the terms. So, what is the exact meaning of here? What are the letters and how obtained? How is the 9 rays and 3 lines expression written? Can you explain also meanings of normal fan,cone and complete fan? Article can be found from here. I want to grasp all of the article also I have to, but at least up to there is enough. @Edit: I think it's a kinda Millenium Problem . If not, isn't there any helper mathematician?","P_b =\{x∈\mathbb{R}^d : Ax≤b\} Ax≤b 
\begin{bmatrix}
0 & 1 & 1 \\ 
1 & 0 &1 \\ 
1 & 0 & 0\\ 
0 & 1 & 0\\ 
0 & 0 & -1 \\ 
0 &  0& 1\\ 
0 &  -1& 0\\ 
0 &  -1& 1\\ 
-1 &  0& 0\\ 
-1 &  0& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\ 
x_2\\ 
x_3
\end{bmatrix}
\leq
\begin{bmatrix}
4\\
4\\
3\\ 
3\\ 
0\\
2\\
0\\
1\\
0\\
1 
\end{bmatrix}
 b U(A)=\{b∈\mathbb{R}^m : P_b \neq ∅\}. Z^{10} R^d N(F;P)=\{v∈\mathbb{R}^dd : v^⊤x=h(P,v)~for~all~ x∈F\}. k (d − k) N(P) P P \mathbb{R}^d U(A) b_{1,2,...}","['linear-algebra', 'matrices', 'polytopes', 'convex-geometry', 'convex-hulls']"
18,Definiteness of a general partitioned matrix $\mathbf M=\left[\begin{matrix}\bf A & \bf B\\\bf B^\top & \bf D \\\end{matrix}\right]$,Definiteness of a general partitioned matrix,\mathbf M=\left[\begin{matrix}\bf A & \bf B\\\bf B^\top & \bf D \\\end{matrix}\right],"If $$\mathbf M=\left[\begin{matrix}\bf A & \bf b\\\bf b^\top & \bf d \\\end{matrix}\right]$$ such that $\bf A$ is positive definite, under what conditions is $\bf M$ positive definite, positive semidefinite and indefinite? It is readily seen that $\det(\mathbf M)=\alpha\det(\mathbf A)$ , where $\alpha=\mathbf d-\bf b^\top A^{-1}b$ Now, $\alpha>0\Rightarrow \det(\mathbf M)>0$ $\quad(\det(\mathbf A)>0$ by hypothesis $)$ This is not enough for $\bf M$ to be p.d. as I need all its leading principal minors to be positive as well. But the answers tell me that $\alpha>0$ ensures all of that. $\alpha=0\Rightarrow \bf M$ is singular As it turns out, this condition ensures that $\bf M$ is p.s.d, but as far as I know this is not the case in general. $\alpha<0\Rightarrow \det(\mathbf M)<0$ This condition is apparently sufficient in this case for $\bf M$ to be indefinite. But if $\bf M$ is indefinite, it may be a singular matrix as well (i.e., $\alpha$ might as well have vanished in this case). Moreover, as I note in the above two cases, this isn't a sufficient condition either for a matrix to be indefinite. I would like to clarify my doubts and a possible proof sketch would be helpful too. EDIT. Extending the problem from a bordered matrix to a partitioned matrix: Suppose $\mathbf M=\left[\begin{matrix}\bf A & \bf B\\\bf B^\top & \bf D \\\end{matrix}\right]$ is symmetric and $\bf A$ is square. Then show that $\bf M$ is p.d. iff $\bf A$ and $\mathbf D-\bf B^\top A^{-1}B$ are p.d. What should be the correct approach now? I can calculate the determinant like before but the same problem still bothers me. Thinking in terms of quadratic forms and using the definitions of definiteness do not seem to be the way to go about it.","If such that is positive definite, under what conditions is positive definite, positive semidefinite and indefinite? It is readily seen that , where Now, by hypothesis This is not enough for to be p.d. as I need all its leading principal minors to be positive as well. But the answers tell me that ensures all of that. is singular As it turns out, this condition ensures that is p.s.d, but as far as I know this is not the case in general. This condition is apparently sufficient in this case for to be indefinite. But if is indefinite, it may be a singular matrix as well (i.e., might as well have vanished in this case). Moreover, as I note in the above two cases, this isn't a sufficient condition either for a matrix to be indefinite. I would like to clarify my doubts and a possible proof sketch would be helpful too. EDIT. Extending the problem from a bordered matrix to a partitioned matrix: Suppose is symmetric and is square. Then show that is p.d. iff and are p.d. What should be the correct approach now? I can calculate the determinant like before but the same problem still bothers me. Thinking in terms of quadratic forms and using the definitions of definiteness do not seem to be the way to go about it.",\mathbf M=\left[\begin{matrix}\bf A & \bf b\\\bf b^\top & \bf d \\\end{matrix}\right] \bf A \bf M \det(\mathbf M)=\alpha\det(\mathbf A) \alpha=\mathbf d-\bf b^\top A^{-1}b \alpha>0\Rightarrow \det(\mathbf M)>0 \quad(\det(\mathbf A)>0 ) \bf M \alpha>0 \alpha=0\Rightarrow \bf M \bf M \alpha<0\Rightarrow \det(\mathbf M)<0 \bf M \bf M \alpha \mathbf M=\left[\begin{matrix}\bf A & \bf B\\\bf B^\top & \bf D \\\end{matrix}\right] \bf A \bf M \bf A \mathbf D-\bf B^\top A^{-1}B,"['linear-algebra', 'matrices', 'quadratic-forms', 'positive-definite', 'positive-semidefinite']"
19,Why we need eigenvectors and eigenvalues [duplicate],Why we need eigenvectors and eigenvalues [duplicate],,"This question already has answers here : What is the importance of eigenvalues/eigenvectors? (11 answers) Closed 4 years ago . Given a matrix A, what do eigenvectors and eigenvalues of A imply? I know how to calculate them but I want to understand WHY do we need to find them? In what application this is important Thank you","This question already has answers here : What is the importance of eigenvalues/eigenvectors? (11 answers) Closed 4 years ago . Given a matrix A, what do eigenvectors and eigenvalues of A imply? I know how to calculate them but I want to understand WHY do we need to find them? In what application this is important Thank you",,"['linear-algebra', 'matrices']"
20,Exact same solutions implies same row-reduced echelon form?,Exact same solutions implies same row-reduced echelon form?,,"In Hoffman and Kunze they have two exercises where they ask to show that if two homogeneous linear systems have the exact same solutions then they have the same row-reduced echelon form. They first ask to prove it in the case of $2\times 2$ (Exercise 1.2.6) and then they ask to prove it in the case $2\times 3$ ( Exercise 1.4.10 ).  I am able to prove it in both of these special cases, but as far as I can tell Hoffman and Kunze never tell us whether or not this is true in general. So that's my question, is this true in general?  And if not, can anybody provide a counter-example?  Thank you!","In Hoffman and Kunze they have two exercises where they ask to show that if two homogeneous linear systems have the exact same solutions then they have the same row-reduced echelon form. They first ask to prove it in the case of $2\times 2$ (Exercise 1.2.6) and then they ask to prove it in the case $2\times 3$ ( Exercise 1.4.10 ).  I am able to prove it in both of these special cases, but as far as I can tell Hoffman and Kunze never tell us whether or not this is true in general. So that's my question, is this true in general?  And if not, can anybody provide a counter-example?  Thank you!",,"['linear-algebra', 'matrices', 'linear-transformations']"
21,The SVD Solution to Linear Least Squares / Linear System of Equations,The SVD Solution to Linear Least Squares / Linear System of Equations,,"I'm a little confused about the various explanations for using Singular Value Decomposition (SVD) to solve the Linear Least Squares (LLS) problem. I understand that LLS attempts fit $Ax=b$ by minimizing $\|A\hat{x}-b\|$, then calculating the vector $\hat{x}$ such that $\hat{x}=(A^{\top}A)^{-1}A^{\top}b$ But my question(s) are in relation to the two explanations given at SVD and least squares proof and Why does SVD provide the least squares solution to $Ax=b$? : Why do we need (or care to) to calculate $\hat{x}=V{\Sigma}^{-1}U^{\top}b$ where $SVD(A)=U\Sigma V^{\top}$ when $\hat{x}$ can be calculated vie at the pseudo-inverse mentioned above ($\hat{x}=(A^{\top}A)^{-1}A^{\top}b$) The first post mentioned that we are subject to the constraint that $\|\hat{x}\|=1$? What happens when the least squares solution does not have $\|\hat{x}\|=1$? Does this invalidate using SVD for the solution of $\hat{x}$ or is there a ""back-door"" approach? How do the answers to the questions above (as well as our approach) change when we are minimizing $Ax=0$ versus a generic $Ax=b$? Example:  When the SVD of A is $U$, $\Sigma$, and $V^{\top}$ (that is $A\hat{x}=U\Sigma V^{\top}\hat{x}$), I would think we only care about the smallest singular value $\sigma_i$ in $\Sigma$ when solving $Ax=0$, since using the smallest $\sigma_i$ does not necessarily give the best fit to $u_i \sigma_i v^{\top}_i \hat{x} = b$? Much thanks, Jeff","I'm a little confused about the various explanations for using Singular Value Decomposition (SVD) to solve the Linear Least Squares (LLS) problem. I understand that LLS attempts fit $Ax=b$ by minimizing $\|A\hat{x}-b\|$, then calculating the vector $\hat{x}$ such that $\hat{x}=(A^{\top}A)^{-1}A^{\top}b$ But my question(s) are in relation to the two explanations given at SVD and least squares proof and Why does SVD provide the least squares solution to $Ax=b$? : Why do we need (or care to) to calculate $\hat{x}=V{\Sigma}^{-1}U^{\top}b$ where $SVD(A)=U\Sigma V^{\top}$ when $\hat{x}$ can be calculated vie at the pseudo-inverse mentioned above ($\hat{x}=(A^{\top}A)^{-1}A^{\top}b$) The first post mentioned that we are subject to the constraint that $\|\hat{x}\|=1$? What happens when the least squares solution does not have $\|\hat{x}\|=1$? Does this invalidate using SVD for the solution of $\hat{x}$ or is there a ""back-door"" approach? How do the answers to the questions above (as well as our approach) change when we are minimizing $Ax=0$ versus a generic $Ax=b$? Example:  When the SVD of A is $U$, $\Sigma$, and $V^{\top}$ (that is $A\hat{x}=U\Sigma V^{\top}\hat{x}$), I would think we only care about the smallest singular value $\sigma_i$ in $\Sigma$ when solving $Ax=0$, since using the smallest $\sigma_i$ does not necessarily give the best fit to $u_i \sigma_i v^{\top}_i \hat{x} = b$? Much thanks, Jeff",,"['linear-algebra', 'systems-of-equations', 'numerical-linear-algebra', 'least-squares', 'svd']"
22,Prove or Disprove the Existence of Solutions...,Prove or Disprove the Existence of Solutions...,,"Let $A$ be a $3\times 4$ and $b$ be a $3\times 1$ matrix with integer entries.Suppose that the system $Ax=b$ has a complex solution. Then which of the following are true? (CSIR December 2014) $Ax=b$ has an integer solution. $Ax=b$ has a rational solution. The set of real solutions to $Ax=0$ has a basis consisting of rational solutions. If $b$ is not equal to zero then $A$ has positive rank. Is it possible to say that 1 and 2 are true as $Ax=b$ has a complex solution? If not  what we have to understand from the statement ""the system $Ax=b$ has a complex solution "" ? What about 3 and 4  ?","Let $A$ be a $3\times 4$ and $b$ be a $3\times 1$ matrix with integer entries.Suppose that the system $Ax=b$ has a complex solution. Then which of the following are true? (CSIR December 2014) $Ax=b$ has an integer solution. $Ax=b$ has a rational solution. The set of real solutions to $Ax=0$ has a basis consisting of rational solutions. If $b$ is not equal to zero then $A$ has positive rank. Is it possible to say that 1 and 2 are true as $Ax=b$ has a complex solution? If not  what we have to understand from the statement ""the system $Ax=b$ has a complex solution "" ? What about 3 and 4  ?",,"['linear-algebra', 'systems-of-equations']"
23,Find the probability that a matrix is diagonalizable,Find the probability that a matrix is diagonalizable,,"The original question is as follows, Let $p$ be an odd prime number. The entries of a $2\times2$ matrix $A\in M_2(\mathbb Z_p)$ are selected at random. (a) Find the possibility that $f_A(x)$ splits (b) Find the possibility that $A$ is diagonalizable. So I started by calculating probability with very small $p$. But this did not give me any useful information. I would like to seek for some hint to get me start with the question. Thanks for any help.","The original question is as follows, Let $p$ be an odd prime number. The entries of a $2\times2$ matrix $A\in M_2(\mathbb Z_p)$ are selected at random. (a) Find the possibility that $f_A(x)$ splits (b) Find the possibility that $A$ is diagonalizable. So I started by calculating probability with very small $p$. But this did not give me any useful information. I would like to seek for some hint to get me start with the question. Thanks for any help.",,"['linear-algebra', 'probability', 'abstract-algebra']"
24,Nonsingularity of Euclidean distance matrix,Nonsingularity of Euclidean distance matrix,,"Let $x_1, \dots, x_k \in \mathbb{R}^n$ be distinct points and let $A$ be the matrix defined by $A_{ij} = d(x_i, x_j)$, where $d$ is the Euclidean distance. Is $A$ always nonsingular? I have a feeling this should be well known (or, at least a reference should exists), on the other hand, this fact fails for general metrics (take e.g. path metric on the cycle $C_4$) edit: changed number of points from $n$ to general $k$","Let $x_1, \dots, x_k \in \mathbb{R}^n$ be distinct points and let $A$ be the matrix defined by $A_{ij} = d(x_i, x_j)$, where $d$ is the Euclidean distance. Is $A$ always nonsingular? I have a feeling this should be well known (or, at least a reference should exists), on the other hand, this fact fails for general metrics (take e.g. path metric on the cycle $C_4$) edit: changed number of points from $n$ to general $k$",,"['linear-algebra', 'metric-spaces']"
25,What is the connection between linear algebra and geometry?,What is the connection between linear algebra and geometry?,,"I am currently studying linear algebra. Yet, I found discussions about linear algebra usually explain things in a geometric fashion. I am quite confused on how to link up these two topics. Can anyone kindly recommend some books/readings for me as an introduction to the concept of combining these two topics?","I am currently studying linear algebra. Yet, I found discussions about linear algebra usually explain things in a geometric fashion. I am quite confused on how to link up these two topics. Can anyone kindly recommend some books/readings for me as an introduction to the concept of combining these two topics?",,"['linear-algebra', 'geometry', 'reference-request']"
26,An invertible matrix is orthogonal if and only if the inverse is equal to the transpose on nonzero elements,An invertible matrix is orthogonal if and only if the inverse is equal to the transpose on nonzero elements,,"Let $A$ be an invertible real matrix, and suppose that $(A^{-1})_{i,j} = (A^{T})_{i,j}$ whenever $(A^{T})_{i,j}\ne 0$ . Is it true that $A$ is orthogonal? I found this statement in a paper without proof. The converse is true for obvious reasons. Trying to write $AA^{-1} =  A^{-1}A= I$ one can find that $A$ has all columns and rows of unitary norm, but not much more. This is sadly not enough to conclude that $A$ is orthogonal (there are explicit counterexamples). Can you help? For those who asked, notice that for all $i,j$ , $$(A^{-1})_{i,j}(A)_{j,i} = (A^{-1})_{i,j}(A^{T})_{i,j} = (A^{T})^2_{i,j}$$ so $$1 = (A^{-1}A)_{i,i} = \sum_j (A^{-1})_{i,j}(A)_{j,i} = \sum_j (A^{T})^2_{i,j}$$ meaning that the norm of any column of $A$ is 1. From $AA^{-1}=I$ you get that also the norm of the rows is 1.","Let be an invertible real matrix, and suppose that whenever . Is it true that is orthogonal? I found this statement in a paper without proof. The converse is true for obvious reasons. Trying to write one can find that has all columns and rows of unitary norm, but not much more. This is sadly not enough to conclude that is orthogonal (there are explicit counterexamples). Can you help? For those who asked, notice that for all , so meaning that the norm of any column of is 1. From you get that also the norm of the rows is 1.","A (A^{-1})_{i,j} = (A^{T})_{i,j} (A^{T})_{i,j}\ne 0 A AA^{-1} =  A^{-1}A= I A A i,j (A^{-1})_{i,j}(A)_{j,i} = (A^{-1})_{i,j}(A^{T})_{i,j} = (A^{T})^2_{i,j} 1 = (A^{-1}A)_{i,i} = \sum_j (A^{-1})_{i,j}(A)_{j,i} = \sum_j (A^{T})^2_{i,j} A AA^{-1}=I","['linear-algebra', 'matrices', 'inverse', 'orthogonal-matrices', 'hadamard-product']"
27,Inequality for 0-1-matrices.,Inequality for 0-1-matrices.,,Given a $n \times n$ matrix $A$ with entries 0 or 1 and non-zero determinant. Question 1: Is true that the sum of the entries of the inverse of $A$ is less than or equal to $n$ ? Question 2: Is true that the sum of the entries of the inverse of $A$ is less than or equal to $n$ in case $A$ has determinant $\pm 1$ and only the entries 1 on the diagonal? Is in this case the value $n$ uniquely attained by the identity matrix as this sum? Both questions have a positive answer for $n \leq 4$ . It would also be nice when someone with a good program/computer could gheck it for n=5 or even n=6.,Given a matrix with entries 0 or 1 and non-zero determinant. Question 1: Is true that the sum of the entries of the inverse of is less than or equal to ? Question 2: Is true that the sum of the entries of the inverse of is less than or equal to in case has determinant and only the entries 1 on the diagonal? Is in this case the value uniquely attained by the identity matrix as this sum? Both questions have a positive answer for . It would also be nice when someone with a good program/computer could gheck it for n=5 or even n=6.,n \times n A A n A n A \pm 1 n n \leq 4,"['linear-algebra', 'matrices', 'inequality', 'inverse']"
28,Annihilator of a vector space $V$ is the zero subspace of $V^*$,Annihilator of a vector space  is the zero subspace of,V V^*,"I am reading Hoffman and Kunze's Linear Algebra and in Section 3.5, page 101, they define the annihilator of a subset as follows: Definition. If $V$ is a vector space over the field $F$ and $S$ is a subset of $V$, the annihilator of $S$ is the set $S^0$ of linear functionals $f$ on $V$ such that $f(\alpha) = 0$ for every $\alpha$ in $S$. In the following paragraph, they say that If $S = V$, then $S^0$ is the zero subspace of $V^*$. (This is easy to see when $V$ is finite-dimensional.) My question is regarding the statement within the parentheses. Is there some subtlety when considering infinite-dimensional vector spaces? Below is my proof of the fact that $S^0 = \{ 0 \}$ when $S = V$. Let $S = V$. If $f \in V^*$ is a non-zero functional, then $f(v) \neq 0$ for some $v \in V = S$. So, $f \not\in S^0$. So, $S^0 \subseteq \{ 0 \}$. Also, if $0$ is the zero functional, then it maps every $v \in V = S$ to $0$, and so $\{ 0 \} \subseteq S^0$. Hence, $S^0$ is the zero subspace of $V^*$. I don't see where the dimension of $V$ plays a role in the above proof. Is the statement given within the parantheses redundant, or am I missing something crucial in understanding the case $\dim V = \infty$?","I am reading Hoffman and Kunze's Linear Algebra and in Section 3.5, page 101, they define the annihilator of a subset as follows: Definition. If $V$ is a vector space over the field $F$ and $S$ is a subset of $V$, the annihilator of $S$ is the set $S^0$ of linear functionals $f$ on $V$ such that $f(\alpha) = 0$ for every $\alpha$ in $S$. In the following paragraph, they say that If $S = V$, then $S^0$ is the zero subspace of $V^*$. (This is easy to see when $V$ is finite-dimensional.) My question is regarding the statement within the parentheses. Is there some subtlety when considering infinite-dimensional vector spaces? Below is my proof of the fact that $S^0 = \{ 0 \}$ when $S = V$. Let $S = V$. If $f \in V^*$ is a non-zero functional, then $f(v) \neq 0$ for some $v \in V = S$. So, $f \not\in S^0$. So, $S^0 \subseteq \{ 0 \}$. Also, if $0$ is the zero functional, then it maps every $v \in V = S$ to $0$, and so $\{ 0 \} \subseteq S^0$. Hence, $S^0$ is the zero subspace of $V^*$. I don't see where the dimension of $V$ plays a role in the above proof. Is the statement given within the parantheses redundant, or am I missing something crucial in understanding the case $\dim V = \infty$?",,"['linear-algebra', 'abstract-algebra']"
29,"What does a symmetric matrix transformation do, geometrically?","What does a symmetric matrix transformation do, geometrically?",,"I need some visual intuition behind what exactly a symmetric matrix transformation does. In a $2 \times 2$ and $3 \times 3$ vector space, what are they generally?","I need some visual intuition behind what exactly a symmetric matrix transformation does. In a and vector space, what are they generally?",2 \times 2 3 \times 3,['linear-algebra']
30,Number of singular $2\times2$ matrices with distinct integer entries,Number of singular  matrices with distinct integer entries,2\times2,"Given an integer $n$, what is the total number of singular $2 \times 2$ matrices with distinct elements from $\{0,1,\ldots,n\}$? Example: a) For $n=6$, the numbers to be considered are $\{0,1,2,3,4,5,6\}$. So the valid matrices whose determinants equal $0$ are: 1)$$         \begin{vmatrix}         1 & 2 \\           3 & 6 \\                \end{vmatrix}=0 $$ 2)$$         \begin{vmatrix}         1 & 3 \\           2 & 6 \\                \end{vmatrix}=0 $$ 3)$$         \begin{vmatrix}         2 & 1 \\           6 & 3 \\                \end{vmatrix}=0 $$ 4)$$         \begin{vmatrix}         2 & 4 \\           3 & 6 \\                \end{vmatrix}=0 $$,etc like these there are 16 possible ways for n=6 , So the answer is 16 b)For n=50 there are 5824 ways c) For n=24 , there are 920 ways So I need To know the way of evaluating this problem","Given an integer $n$, what is the total number of singular $2 \times 2$ matrices with distinct elements from $\{0,1,\ldots,n\}$? Example: a) For $n=6$, the numbers to be considered are $\{0,1,2,3,4,5,6\}$. So the valid matrices whose determinants equal $0$ are: 1)$$         \begin{vmatrix}         1 & 2 \\           3 & 6 \\                \end{vmatrix}=0 $$ 2)$$         \begin{vmatrix}         1 & 3 \\           2 & 6 \\                \end{vmatrix}=0 $$ 3)$$         \begin{vmatrix}         2 & 1 \\           6 & 3 \\                \end{vmatrix}=0 $$ 4)$$         \begin{vmatrix}         2 & 4 \\           3 & 6 \\                \end{vmatrix}=0 $$,etc like these there are 16 possible ways for n=6 , So the answer is 16 b)For n=50 there are 5824 ways c) For n=24 , there are 920 ways So I need To know the way of evaluating this problem",,"['linear-algebra', 'combinatorics', 'matrices', 'determinant']"
31,What are some Group representation of the rubik's cube group?,What are some Group representation of the rubik's cube group?,,"The Rubik's cube corresponds to valid sequences of moves of the Rubik's cube. What are some group representations of this group (with respect to finite dimensional vector spaces on finite fields)? Ideally, I am looking for embeddings. I know that you can make a representation of the symmetric group 48 on a 48 dimensional vector space, and then embed the rubik's cube group into that. Can you make a representation based on a lower dimensional vector space? (My group theory and linear algebra are a little rusty. Feel free to edit this to make more sense.)","The Rubik's cube corresponds to valid sequences of moves of the Rubik's cube. What are some group representations of this group (with respect to finite dimensional vector spaces on finite fields)? Ideally, I am looking for embeddings. I know that you can make a representation of the symmetric group 48 on a 48 dimensional vector space, and then embed the rubik's cube group into that. Can you make a representation based on a lower dimensional vector space? (My group theory and linear algebra are a little rusty. Feel free to edit this to make more sense.)",,"['linear-algebra', 'group-theory', 'finite-groups', 'representation-theory', 'rubiks-cube']"
32,Group structure of $\mathrm{O}_n(R)$ over a **ring** $R$,Group structure of  over a **ring**,\mathrm{O}_n(R) R,"I have been looking at the orthogonal matrix group $$ \mathrm{O}_n(\mathbb{R}) := \{ M \in \mathbb{R}^{n \times n} : M^T M = I_n \} $$ This group for $n \geq 2$ is infinite because e.g. $\begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \in \mathrm{O}_2(\mathbb{R})$ can be embedded into higher dimensions. Since there are infinitely many Pythagorean triples $(x, y, z)$ , we also know $\frac{1}{z}\begin{pmatrix} x & -y \\ y & x \end{pmatrix} \in \mathrm{O}_2(\mathbb{Q})$ , so $\mathrm{O}_n(\mathbb{Q})$ is infinite. On the other hand, I know that the group over integers is finite: each column/row must be of norm $1$ , so they must contain a single nonzero element from $\pm 1$ . In fact, $\mathrm{O}_n(\mathbb{Z}) \cong S_n \wr \{-1, 1\}$ . My question is the generalisation of this $\mathrm{O}_n(\mathbb{Q})$ vs $\mathrm{O}_n(\mathbb{Z})$ comparison: For a number field $K = \mathbb{Q}(X)$ with ring of integers $\mathcal{O}_K$ , is the matrix group $\mathrm{O}_n(\mathcal{O}_K)$ finite? In either case, can we describe the group structure or even provide generators, probably depending on $K$ itself? I looked at $\mathrm{O}_2(\mathcal{O}_K)$ , which has the condition $$ \begin{pmatrix} a & b \\ c & d \end{pmatrix} \in \mathrm{O}_2(\mathcal{O}_K) \iff a^2 + c^2 = b^2 + d^2 = 1 \land ab + cd = 0 $$ However, I can't seem to get anything beyond a few special cases. This also doesn't seem to give any insight for $n \geq 2$ . Looking at the ""opposite"" direction, we can show that $\mathrm{O}_n(\mathcal{O}_K) \cong S_n \times \mathcal{O}_K^{\times}$ doesn't hold in general: Fix an element $a \in \mathcal{O}_K$ and consider $L = K(\sqrt{1 - a^2})$ . Then, $\begin{pmatrix} a & -\sqrt{1 - a^2} \\ \sqrt{1 - a^2} & a \end{pmatrix} \in \mathrm{O}_2(L)$ . (This problem came from implementing .is_finite() in Sage) Update: I found a slightly nontrivial result, which is that if $\sqrt{-k} \in R$ for any positive non-square integer $k$ , then $\mathrm{O}_n(\mathcal{O}_K)$ is infinite. The reason is that $$ \begin{pmatrix} a & b \\ -b & a \end{pmatrix} \in \mathrm{O}_n(\mathcal{O}_K) \iff a^2 + b^2 = 1 $$ And the Pell's equation $x^2 - ky^2 = 1$ has infinitely many solution, so $x^2 + (\sqrt{-k}y)^2 = 1$ ! In particular, for $m \geq 3$ and $K = \mathbb{Q}(\zeta_{2^m})$ , we have $\frac{1}{\sqrt{2}}\left(1 + i\right) \in \mathcal{O}_K \implies \sqrt{-2} \in \mathcal{O}_K$ .","I have been looking at the orthogonal matrix group This group for is infinite because e.g. can be embedded into higher dimensions. Since there are infinitely many Pythagorean triples , we also know , so is infinite. On the other hand, I know that the group over integers is finite: each column/row must be of norm , so they must contain a single nonzero element from . In fact, . My question is the generalisation of this vs comparison: For a number field with ring of integers , is the matrix group finite? In either case, can we describe the group structure or even provide generators, probably depending on itself? I looked at , which has the condition However, I can't seem to get anything beyond a few special cases. This also doesn't seem to give any insight for . Looking at the ""opposite"" direction, we can show that doesn't hold in general: Fix an element and consider . Then, . (This problem came from implementing .is_finite() in Sage) Update: I found a slightly nontrivial result, which is that if for any positive non-square integer , then is infinite. The reason is that And the Pell's equation has infinitely many solution, so ! In particular, for and , we have .","
\mathrm{O}_n(\mathbb{R}) := \{ M \in \mathbb{R}^{n \times n} : M^T M = I_n \}
 n \geq 2 \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \in \mathrm{O}_2(\mathbb{R}) (x, y, z) \frac{1}{z}\begin{pmatrix} x & -y \\ y & x \end{pmatrix} \in \mathrm{O}_2(\mathbb{Q}) \mathrm{O}_n(\mathbb{Q}) 1 \pm 1 \mathrm{O}_n(\mathbb{Z}) \cong S_n \wr \{-1, 1\} \mathrm{O}_n(\mathbb{Q}) \mathrm{O}_n(\mathbb{Z}) K = \mathbb{Q}(X) \mathcal{O}_K \mathrm{O}_n(\mathcal{O}_K) K \mathrm{O}_2(\mathcal{O}_K) 
\begin{pmatrix} a & b \\ c & d \end{pmatrix} \in \mathrm{O}_2(\mathcal{O}_K) \iff a^2 + c^2 = b^2 + d^2 = 1 \land ab + cd = 0
 n \geq 2 \mathrm{O}_n(\mathcal{O}_K) \cong S_n \times \mathcal{O}_K^{\times} a \in \mathcal{O}_K L = K(\sqrt{1 - a^2}) \begin{pmatrix} a & -\sqrt{1 - a^2} \\ \sqrt{1 - a^2} & a \end{pmatrix} \in \mathrm{O}_2(L) \sqrt{-k} \in R k \mathrm{O}_n(\mathcal{O}_K) 
\begin{pmatrix} a & b \\ -b & a \end{pmatrix} \in \mathrm{O}_n(\mathcal{O}_K) \iff a^2 + b^2 = 1
 x^2 - ky^2 = 1 x^2 + (\sqrt{-k}y)^2 = 1 m \geq 3 K = \mathbb{Q}(\zeta_{2^m}) \frac{1}{\sqrt{2}}\left(1 + i\right) \in \mathcal{O}_K \implies \sqrt{-2} \in \mathcal{O}_K","['linear-algebra', 'commutative-algebra']"
33,Proving there is an eigenvalue $\lambda$ for which $|\lambda - b_{jj}| < \epsilon \sqrt{n}$,Proving there is an eigenvalue  for which,\lambda |\lambda - b_{jj}| < \epsilon \sqrt{n},"Let $A$ be an $n\times n$ real symmetric matrix. By applying Jacobi's   method, suppose we have generated an orthogonal matrix $R$ and a   symmetric matrix $B$ such that the equality $$B = R^{T}AR $$ holds. Moreover, suppose the inequality $|b_{ij}| < \epsilon$ holds   for all $i \neq j$ . Show that for each $j = 1, 2, \ldots, n$ , there is at least one   eigenvalue $\lambda$ of $A$ such that $|\lambda - b_{jj}| < \epsilon \sqrt{n}$ holds. This is an exercise that I am doing to study for my final exam. So, I've just recently learned Jacobi's method, and I know that the eigenvalues and eigenvectors are related to the matrices $B$ and $R$ ; however, I have no idea how to use those results to prove an inequality. I also have no idea how to get the $\sqrt{n}$ term in there. I would greatly appreciate any help in this exercise. Thanks UPDATE: These are some theorems in my book that might help. Theorem (Gerschgorin’s Theorem): Let $n \geq 2$ and $A \in \mathbb{C}^{n\times n}$ . All eigenvalues of $A$ lie in the region $D = \bigcup_{i=1}^{n} D_{i}$ , where $D_{i}$ are the Gerschgorin discs of $A$ . Definition: (Gerschgorin Disc): Suppose $n \geq 2$ and $A \in \mathbb{C}^{n\times n}$ . The Gerschgorin discs $D_{i}$ of the matrix $A$ are defined by the closed circular regions $$D_{i} = \{z \in \mathbb{C} : |z - a_{ii}| \leq R_{i}\}, $$ where $$ R_{i} = \sum_{j = 1, \\ i \neq j}^{n}  |a_{ij}|$$ is the radius of $D_{i}$ . Theorem (Bauer-Fike): Suppose $A$ and $E$ are real symmetric $n\times n$ matrices and $B = A - E$ . Assume, further, that the eigenvalues of $A$ are denoted by $\lambda_{j}, j = 1, 2, 3, \ldots, n$ and $\mu$ is an eigenvalue of $B$ . Then at least one eigenvalue of $\lambda_{j}$ of $A$ satisfies $|\lambda_{j} - \mu| \leq ||E||_{2}$ , where $|| \cdot ||_{2}$ denotes the $2$ -norm of a matrix. Book link: http://newdoc.nccu.edu.tw/teasyllabus/111648701013/Numerical_Analysis.pdf The problem is from chapter 5. I would appreciate it if the answer does not use too many outside results from the book. I suppose a few are okay though, as long as they aren't really strong results that are hard to understand.","Let be an real symmetric matrix. By applying Jacobi's   method, suppose we have generated an orthogonal matrix and a   symmetric matrix such that the equality holds. Moreover, suppose the inequality holds   for all . Show that for each , there is at least one   eigenvalue of such that holds. This is an exercise that I am doing to study for my final exam. So, I've just recently learned Jacobi's method, and I know that the eigenvalues and eigenvectors are related to the matrices and ; however, I have no idea how to use those results to prove an inequality. I also have no idea how to get the term in there. I would greatly appreciate any help in this exercise. Thanks UPDATE: These are some theorems in my book that might help. Theorem (Gerschgorin’s Theorem): Let and . All eigenvalues of lie in the region , where are the Gerschgorin discs of . Definition: (Gerschgorin Disc): Suppose and . The Gerschgorin discs of the matrix are defined by the closed circular regions where is the radius of . Theorem (Bauer-Fike): Suppose and are real symmetric matrices and . Assume, further, that the eigenvalues of are denoted by and is an eigenvalue of . Then at least one eigenvalue of of satisfies , where denotes the -norm of a matrix. Book link: http://newdoc.nccu.edu.tw/teasyllabus/111648701013/Numerical_Analysis.pdf The problem is from chapter 5. I would appreciate it if the answer does not use too many outside results from the book. I suppose a few are okay though, as long as they aren't really strong results that are hard to understand.","A n\times n R B B = R^{T}AR  |b_{ij}| < \epsilon i \neq j j = 1, 2, \ldots, n \lambda A |\lambda - b_{jj}| < \epsilon \sqrt{n} B R \sqrt{n} n \geq 2 A \in \mathbb{C}^{n\times n} A D = \bigcup_{i=1}^{n} D_{i} D_{i} A n \geq 2 A \in \mathbb{C}^{n\times n} D_{i} A D_{i} = \{z \in \mathbb{C} : |z - a_{ii}| \leq R_{i}\},   R_{i} = \sum_{j = 1, \\ i \neq j}^{n}  |a_{ij}| D_{i} A E n\times n B = A - E A \lambda_{j}, j = 1, 2, 3, \ldots, n \mu B \lambda_{j} A |\lambda_{j} - \mu| \leq ||E||_{2} || \cdot ||_{2} 2","['linear-algebra', 'matrices']"
34,"What is the ""right"" extra information needed to define the trace of a map between two different vector spaces?","What is the ""right"" extra information needed to define the trace of a map between two different vector spaces?",,"Let $V,W$ be real vector spaces of dimension $d$, and let $T \in \text{Hom}(V,W)$. Is there a ""natural piece of additional information"" required in order to give a meaningful interpretation of the trace of $T$? (which is ""less than"" choosing an isomorphism $V \cong W$). Let me explain a bit what do I mean by ""additional information"" through other examples: Suppose we want to define the determinant of $T$. Then sufficient additional information is a choice of preferred volume forms on $V,W$. Assuming we are given such forms, we can then define $\det T$ by requiring  $$T^*(\text{Vol}_W)=\det T \cdot \text{Vol}_V.$$ Perhaps an even more economical option would be to provide an isomorphism $\bigwedge^d V \simeq \bigwedge^d W$. Since $\bigwedge^d T:\bigwedge^d V \to \bigwedge^d W $, by composing it with the given isomorphism we can identify $\bigwedge^d T$ with a map $\bigwedge^d V \to \bigwedge^d V$ which can then be naturally identified with a  scalar. (Choosing a volume form is equivalent to choosing an isomorphism $\bigwedge^d V \cong \mathbb{R}$). Of course, we could be given an isomorphism $V \simeq W$ and consider $T$ as a map $V \to V$, but my point is that this is much more than we really need. In the same spirit, in order to define the singular values of a map, we need to choose inner products on both spaces. (Which is again less information then choosing isomorphisms of $V,W$ to $\mathbb{R}^d$). The question is whether there is an ""analogous"" piece of extra structure which naturally fits into the definition of the trace. Of course, when $V=W$ (or when we are given some isomorphism between $V$ and $W$) we can define the trace of $T$ as a map $V \to V$. (A coordinate-free version would be using the natural identification $V^* \otimes V \cong \text{Hom}(V,V)$, and the contraction map $V^* \otimes V  \to F$. Alternatively we can choose a basis, and use the fact the trace of a matrix is invariant under conjugation). I am aware that it's not entirely clear whether the trace can be given a purely ""geometric meaning"" (in the sense that it's not invariant under isometries for instance). This is different than the situation with the determinant, or the singular values. Edit: In a slight contrast to the last contrast, here is some evidence that the trace is at least ""partially"" a geometric creature: The space of trace-free matrices is the tangent space at the identity to $SL_d$ (the group of volume preserving matrices). Thus, I am guessing that we shall need at least something like an isomorphism $\bigwedge^d V \simeq \bigwedge^d W$ (in order to define a notion of volume preservation for maps). Using this isomorphism, we can now identify $\bigwedge^d T$ as a map $\bigwedge^d V \to \bigwedge^d V$, and define  $$ ""SL""=\{ T \in \text{Hom}(V,W) \, | \, \bigwedge^d T=\text{Id}_{\bigwedge^d V} \}. $$ The problem is that I am not sure if we can ""single out"" which element is the analog of the ""identity map"" among all these elements of our $""SL""$. Thus, I am not sure that a choice of an isomorphism $\bigwedge^d V \simeq \bigwedge^d W$ would suffice to even define what is ""trace=$0$""...","Let $V,W$ be real vector spaces of dimension $d$, and let $T \in \text{Hom}(V,W)$. Is there a ""natural piece of additional information"" required in order to give a meaningful interpretation of the trace of $T$? (which is ""less than"" choosing an isomorphism $V \cong W$). Let me explain a bit what do I mean by ""additional information"" through other examples: Suppose we want to define the determinant of $T$. Then sufficient additional information is a choice of preferred volume forms on $V,W$. Assuming we are given such forms, we can then define $\det T$ by requiring  $$T^*(\text{Vol}_W)=\det T \cdot \text{Vol}_V.$$ Perhaps an even more economical option would be to provide an isomorphism $\bigwedge^d V \simeq \bigwedge^d W$. Since $\bigwedge^d T:\bigwedge^d V \to \bigwedge^d W $, by composing it with the given isomorphism we can identify $\bigwedge^d T$ with a map $\bigwedge^d V \to \bigwedge^d V$ which can then be naturally identified with a  scalar. (Choosing a volume form is equivalent to choosing an isomorphism $\bigwedge^d V \cong \mathbb{R}$). Of course, we could be given an isomorphism $V \simeq W$ and consider $T$ as a map $V \to V$, but my point is that this is much more than we really need. In the same spirit, in order to define the singular values of a map, we need to choose inner products on both spaces. (Which is again less information then choosing isomorphisms of $V,W$ to $\mathbb{R}^d$). The question is whether there is an ""analogous"" piece of extra structure which naturally fits into the definition of the trace. Of course, when $V=W$ (or when we are given some isomorphism between $V$ and $W$) we can define the trace of $T$ as a map $V \to V$. (A coordinate-free version would be using the natural identification $V^* \otimes V \cong \text{Hom}(V,V)$, and the contraction map $V^* \otimes V  \to F$. Alternatively we can choose a basis, and use the fact the trace of a matrix is invariant under conjugation). I am aware that it's not entirely clear whether the trace can be given a purely ""geometric meaning"" (in the sense that it's not invariant under isometries for instance). This is different than the situation with the determinant, or the singular values. Edit: In a slight contrast to the last contrast, here is some evidence that the trace is at least ""partially"" a geometric creature: The space of trace-free matrices is the tangent space at the identity to $SL_d$ (the group of volume preserving matrices). Thus, I am guessing that we shall need at least something like an isomorphism $\bigwedge^d V \simeq \bigwedge^d W$ (in order to define a notion of volume preservation for maps). Using this isomorphism, we can now identify $\bigwedge^d T$ as a map $\bigwedge^d V \to \bigwedge^d V$, and define  $$ ""SL""=\{ T \in \text{Hom}(V,W) \, | \, \bigwedge^d T=\text{Id}_{\bigwedge^d V} \}. $$ The problem is that I am not sure if we can ""single out"" which element is the analog of the ""identity map"" among all these elements of our $""SL""$. Thus, I am not sure that a choice of an isomorphism $\bigwedge^d V \simeq \bigwedge^d W$ would suffice to even define what is ""trace=$0$""...",,"['linear-algebra', 'abstract-algebra', 'soft-question', 'category-theory', 'trace']"
35,"If matrix $A$ has entries $A_{ij}=\sin(\theta_i - \theta_j)$, why does $\|A\|_* = n$ always hold?","If matrix  has entries , why does  always hold?",A A_{ij}=\sin(\theta_i - \theta_j) \|A\|_* = n,"If we let $\theta\in\mathbb{R}^n$ be a vector that contains $n$ arbitrary phases $\theta_i\in[0,2\pi)$ for $i\in[n]$, then we can define a matrix $X\in\mathbb{R}^{n\times n}$, where \begin{align*} X_{ij} = \theta_i - \theta_j. \end{align*} Then the matrices that I consider are the antisymmetric matrix $A=\sin(X)$ and the symmetric matrix $B=\cos(X)$. Through numerical experiments (by randomly sampling the phase vector $\theta$) I find that the nuclear norm of $A$ and $B$ are always $n$, i.e. \begin{align*} \|A\|_* = \|B\|_* = n. \end{align*} Moreover, performing SVD on $A$ yields the largest two singular value $\sigma_1 = \sigma_2 = n/2$ and all the other $\sigma_3 = \ldots = \sigma_n = 0$. Further, if we look at the matrix $A\circ B$, where \begin{align*} (A\circ B)_{ij} = \sin(\theta_i - \theta_j)\cos(\theta_i - \theta_j) = \sin(2(\theta_i - \theta_j))/2, \end{align*} then  \begin{align*} \|A\circ B\|_* = n/2 \end{align*} with $\sigma_1 = \sigma_2 = n/4$ and $\sigma_3 = \ldots = \sigma_n = 0$. Is there any way to see why $A$ and $B$ have these properties?","If we let $\theta\in\mathbb{R}^n$ be a vector that contains $n$ arbitrary phases $\theta_i\in[0,2\pi)$ for $i\in[n]$, then we can define a matrix $X\in\mathbb{R}^{n\times n}$, where \begin{align*} X_{ij} = \theta_i - \theta_j. \end{align*} Then the matrices that I consider are the antisymmetric matrix $A=\sin(X)$ and the symmetric matrix $B=\cos(X)$. Through numerical experiments (by randomly sampling the phase vector $\theta$) I find that the nuclear norm of $A$ and $B$ are always $n$, i.e. \begin{align*} \|A\|_* = \|B\|_* = n. \end{align*} Moreover, performing SVD on $A$ yields the largest two singular value $\sigma_1 = \sigma_2 = n/2$ and all the other $\sigma_3 = \ldots = \sigma_n = 0$. Further, if we look at the matrix $A\circ B$, where \begin{align*} (A\circ B)_{ij} = \sin(\theta_i - \theta_j)\cos(\theta_i - \theta_j) = \sin(2(\theta_i - \theta_j))/2, \end{align*} then  \begin{align*} \|A\circ B\|_* = n/2 \end{align*} with $\sigma_1 = \sigma_2 = n/4$ and $\sigma_3 = \ldots = \sigma_n = 0$. Is there any way to see why $A$ and $B$ have these properties?",,"['linear-algebra', 'trigonometry', 'eigenvalues-eigenvectors', 'spectral-theory', 'nuclear-norm']"
36,"What is the fundamental group of the variety of singular matrices, without $0$?","What is the fundamental group of the variety of singular matrices, without ?",0,"Consider $S = V(det) \subset M_n(\mathbb{C})$. I want to know what the fundamental group of $S \setminus {0}$ is. Equivalently, I want to know the fundamental group of $V(det) \subset \mathbb{P}^{n^2 - 1}$. (Using the homotopy long exact sequence.) In some cases it's easy - if $n = 2$, then the variety is a full rank (smooth) quadric hypersurface in $P^3$, and is thus isomorphic to $P^1 \times P^1$, which is simply connected. If $n= 3$, I already have no idea how to proceed. It is also non-smooth for $n \geq 3$. The result is a variety with a lot of symmetry - conjugation by $GL_n(\mathbb{C})$, left and right multiplication by $GL_n(C) \times GL_n(C)$. There are finitely many orbits for the latter action, classified by rank, and the closure of each orbit is the union of the lower rank orbits. Maybe this is useful? To put something at stake, I guess that it is simply connected for all $n$. What about the higher homotopy groups? (Is the limit with the natural inclusions as $n \to \infty$ contractible?)","Consider $S = V(det) \subset M_n(\mathbb{C})$. I want to know what the fundamental group of $S \setminus {0}$ is. Equivalently, I want to know the fundamental group of $V(det) \subset \mathbb{P}^{n^2 - 1}$. (Using the homotopy long exact sequence.) In some cases it's easy - if $n = 2$, then the variety is a full rank (smooth) quadric hypersurface in $P^3$, and is thus isomorphic to $P^1 \times P^1$, which is simply connected. If $n= 3$, I already have no idea how to proceed. It is also non-smooth for $n \geq 3$. The result is a variety with a lot of symmetry - conjugation by $GL_n(\mathbb{C})$, left and right multiplication by $GL_n(C) \times GL_n(C)$. There are finitely many orbits for the latter action, classified by rank, and the closure of each orbit is the union of the lower rank orbits. Maybe this is useful? To put something at stake, I guess that it is simply connected for all $n$. What about the higher homotopy groups? (Is the limit with the natural inclusions as $n \to \infty$ contractible?)",,"['linear-algebra', 'algebraic-geometry', 'algebraic-topology']"
37,Norms of linear maps,Norms of linear maps,,"Let $M_n(\mathbb{C})$ denote the algebra of $n \times n$ complex matrices, and $H_n(\mathbb{C})$ denote the linear space of $n \times n$ Hermitians. Both spaces are endowed with the usual operator norm. Assume that   $$ \Phi \colon H_n(\mathbb{C}) \rightarrow H_n(\mathbb{C})$$ is a (real) linear map of norm $1.$ Then $\Phi$ has a natural linear extension to $M_n(\mathbb{C})$ by $$\Phi(A) := \Phi\left({A+A^* \over 2}\right) + i\Phi\left({A-A^* \over 2i}\right).$$ Could you give me an example with $\|\Phi\| > 1 ?$ Or an explanation why this might happen?","Let $M_n(\mathbb{C})$ denote the algebra of $n \times n$ complex matrices, and $H_n(\mathbb{C})$ denote the linear space of $n \times n$ Hermitians. Both spaces are endowed with the usual operator norm. Assume that   $$ \Phi \colon H_n(\mathbb{C}) \rightarrow H_n(\mathbb{C})$$ is a (real) linear map of norm $1.$ Then $\Phi$ has a natural linear extension to $M_n(\mathbb{C})$ by $$\Phi(A) := \Phi\left({A+A^* \over 2}\right) + i\Phi\left({A-A^* \over 2i}\right).$$ Could you give me an example with $\|\Phi\| > 1 ?$ Or an explanation why this might happen?",,"['linear-algebra', 'matrices', 'functional-analysis', 'operator-theory', 'operator-algebras']"
38,Prove the n-th power of a matrix is the null matrix,Prove the n-th power of a matrix is the null matrix,,"Let $A,B$ be $n\times n$ matrices with complex elements, $AB=BA$ , $\det(B)\ne0$ , having the following property: $$|\det(A+zB)|=1, \forall z \in \mathbb{C}, |z|=1.$$ Prove $A^n=0_n$ . Does this remain true if $AB \ne BA$ ? So far, no idea. Any help is appreciated. Update I think I made some progress: Let $f(z)=\det(A+zB)=a_nz^n + .. + a_0$ From $|f(z)|=1$ we have $f(z)\overline {f(z)}=1, \forall z, |z|=1$ then, after replacing $\overline z$ with $\frac 1 z$ : $(a_nz^n + .. + a_0)(\overline a_0z^n + .. + \overline a_n)=z^n \tag 1$ Because (1) is true for infinitely many $z$ then (1) must be a polynomial identity. Identifying coefficients I've got $$a_0=a_1=..a_{n-1}=0, a_n\overline a_n=1$$ So $\det(A+zB)=a_nz^n$ where $|a_n|=1$ . It follows that $det(A)=0, det(B)=a_n$ .","Let be matrices with complex elements, , , having the following property: Prove . Does this remain true if ? So far, no idea. Any help is appreciated. Update I think I made some progress: Let From we have then, after replacing with : Because (1) is true for infinitely many then (1) must be a polynomial identity. Identifying coefficients I've got So where . It follows that .","A,B n\times n AB=BA \det(B)\ne0 |\det(A+zB)|=1, \forall z \in \mathbb{C}, |z|=1. A^n=0_n AB \ne BA f(z)=\det(A+zB)=a_nz^n + .. + a_0 |f(z)|=1 f(z)\overline {f(z)}=1, \forall z, |z|=1 \overline z \frac 1 z (a_nz^n + .. + a_0)(\overline a_0z^n + .. + \overline a_n)=z^n \tag 1 z a_0=a_1=..a_{n-1}=0, a_n\overline a_n=1 \det(A+zB)=a_nz^n |a_n|=1 det(A)=0, det(B)=a_n","['linear-algebra', 'matrices']"
39,Name for determinant identity,Name for determinant identity,,"Let $A$ be an $N\times N$ square matrix. There exists a determinant identity $$\operatorname{det}\left(I+A\right)=1+\sum_m A_{mm}+\frac1{2!}\sum_{m,n}\left| \begin{array}{cc} A_{mm} & A_{mn} \\ A_{nm} & A_{nn}\end{array}\right|+ \frac1{3!}\sum_{m,n,l}\left| \begin{array}{ccc} A_{mm} & A_{mn} & A_{ml} \\ A_{nm} & A_{nn} & A_{nl} \\ A_{lm} & A_{ln} & A_{ll}\end{array}\right|+\ldots$$ Could you please recall me how is this relation usually named?","Let $A$ be an $N\times N$ square matrix. There exists a determinant identity $$\operatorname{det}\left(I+A\right)=1+\sum_m A_{mm}+\frac1{2!}\sum_{m,n}\left| \begin{array}{cc} A_{mm} & A_{mn} \\ A_{nm} & A_{nn}\end{array}\right|+ \frac1{3!}\sum_{m,n,l}\left| \begin{array}{ccc} A_{mm} & A_{mn} & A_{ml} \\ A_{nm} & A_{nn} & A_{nl} \\ A_{lm} & A_{ln} & A_{ll}\end{array}\right|+\ldots$$ Could you please recall me how is this relation usually named?",,"['linear-algebra', 'matrices', 'terminology', 'determinant']"
40,What are some Applications of the Permanent of a Matrix?,What are some Applications of the Permanent of a Matrix?,,"I have a decent understanding of the determinant of a matrix in terms of its role in Telling you if a matrix is invertible (zero vs. nonzero) Expressing the product of a matrix's eigenvalues with multiplicities Representing the constant term in a matrix's characteristic polynomial Having geometrical interpretations However, I was curious to learn in the operation known as the permanent has any interesting properties. The permanent is defined in the same way a determinant is, but all entries are added instead of alternating between positive and negative terms. In particular, what are the significant applications of defining the permanent as a notable matrix operation? Also, are there any matrix problems whose solutions directly/indirectly depend on understanding the permanent?","I have a decent understanding of the determinant of a matrix in terms of its role in Telling you if a matrix is invertible (zero vs. nonzero) Expressing the product of a matrix's eigenvalues with multiplicities Representing the constant term in a matrix's characteristic polynomial Having geometrical interpretations However, I was curious to learn in the operation known as the permanent has any interesting properties. The permanent is defined in the same way a determinant is, but all entries are added instead of alternating between positive and negative terms. In particular, what are the significant applications of defining the permanent as a notable matrix operation? Also, are there any matrix problems whose solutions directly/indirectly depend on understanding the permanent?",,"['linear-algebra', 'matrices']"
41,Can any two disjoint nonempty convex sets in a vector space be separated by a hyperplane?,Can any two disjoint nonempty convex sets in a vector space be separated by a hyperplane?,,"Let $V$ be a normed vector space over $\mathbb{R}$, and let $A$ and $B$ be two disjoint nonempty convex subsets of $V$. A geometric form of Hahn-Banach Theorem states that $A$ and $B$ can be separated by a closed hyperplane (i.e. there is $f \in V^\ast$ and $\alpha \in \mathbb{R}$ such that $f(a) \le \alpha, \forall a \in A$ and $\alpha \le f(b), \forall b \in B$) if either $A$ or $B$ is open, or $A$ is closed and $B$ is compact. (This statement is not in the full generality.) There are examples of two disjoint nonempty convex sets which cannot be separated by a closed hyperplane.(These convex sets don't satisfy the condition of the previous statement.) My question is: If the separating hyperplane need not be closed, can any pair of disjoint nonempty convex sets be separated by a hyperplane? More precisely, for any vector space $V$ over $\mathbb{R}$ and two disjoint nonempty convex subsets $A$ and $B$ of $V$, does there exist a linear functional $f:V\to\mathbb{R}$ and a real number $\alpha\in\mathbb{R}$ such that $f(a) \le \alpha, \forall a \in A$ and $\alpha \le f(b), \forall b \in B$? This question doesn't involve any topological concepts. For the finite dimensional case, it is known that the separation is possible. What would happen if the underlying space is infinite dimensional?","Let $V$ be a normed vector space over $\mathbb{R}$, and let $A$ and $B$ be two disjoint nonempty convex subsets of $V$. A geometric form of Hahn-Banach Theorem states that $A$ and $B$ can be separated by a closed hyperplane (i.e. there is $f \in V^\ast$ and $\alpha \in \mathbb{R}$ such that $f(a) \le \alpha, \forall a \in A$ and $\alpha \le f(b), \forall b \in B$) if either $A$ or $B$ is open, or $A$ is closed and $B$ is compact. (This statement is not in the full generality.) There are examples of two disjoint nonempty convex sets which cannot be separated by a closed hyperplane.(These convex sets don't satisfy the condition of the previous statement.) My question is: If the separating hyperplane need not be closed, can any pair of disjoint nonempty convex sets be separated by a hyperplane? More precisely, for any vector space $V$ over $\mathbb{R}$ and two disjoint nonempty convex subsets $A$ and $B$ of $V$, does there exist a linear functional $f:V\to\mathbb{R}$ and a real number $\alpha\in\mathbb{R}$ such that $f(a) \le \alpha, \forall a \in A$ and $\alpha \le f(b), \forall b \in B$? This question doesn't involve any topological concepts. For the finite dimensional case, it is known that the separation is possible. What would happen if the underlying space is infinite dimensional?",,"['linear-algebra', 'functional-analysis', 'convex-geometry']"
42,Properties of 4 by 4 Matrices,Properties of 4 by 4 Matrices,,"Define $     A=\begin{pmatrix} x_1 & x_2 & 0 & 0\\ 0 &1&0&0\\ 0&0&1&0\\ 0&0&0&1 \end{pmatrix},  B=\begin{pmatrix} 1 & 0 & 0 & 0\\ x_3 &x_4&x_5&0\\ 0&0&1&0\\ 0&0&0&1 \end{pmatrix}, C=\begin{pmatrix} 1 & 0 & 0 & 0\\ 0 &1&0&0\\ 0&x_6&x_7&x_8\\ 0&0&0&1 \end{pmatrix},$ $ D=\begin{pmatrix} 1 & 0 & 0 & 0\\ 0 &1&0&0\\ 0&0&1&0\\ 0&0&x_9&x_{10} \end{pmatrix}$. Let  $W$ be a product of above matrices. (For example $ W $ can be $D^2ABDC$ or $CAD$ (any product of above matrices)). Now define reverse of $W$ as $Re(W)$. For example if $W=D^2ABDC$ then $Re(W)=CDBAD^2$, If $W=CAD$ then $Re(W)=DAC$. I want to prove $\det(W+Re(W))-\det(W-Re(W))$ is divisible by $4$. In other words $\det(W+Re(W))-\det(W-Re(W))\equiv 0 \mod 4$. Any comment really appreciated. Note (Grumpy Parsnip): A generalized phenomenon seems to be true here. One can define $n\times n$ matrices of a similar form. E.g. $A_2=\left(\begin{array}{cc} x_1&x_2\\0&1\end{array}\right)$ and $B_2=\left(\begin{array}{cc} 1&0\\x_3&x_4\end{array}\right)$. Then it seems that for $2m\times 2m$ matrices, $$\det(W+Re(W))+(-1)^{m-1}\det(W-Re(W))=0\mod 4.$$ Perhaps the $2\times 2$ case is easier to understand. I'll award a bounty for a solution to the $2\times 2$ case. (Hopefully the OP, who is a friend of mine, will not mind my edits.)","Define $     A=\begin{pmatrix} x_1 & x_2 & 0 & 0\\ 0 &1&0&0\\ 0&0&1&0\\ 0&0&0&1 \end{pmatrix},  B=\begin{pmatrix} 1 & 0 & 0 & 0\\ x_3 &x_4&x_5&0\\ 0&0&1&0\\ 0&0&0&1 \end{pmatrix}, C=\begin{pmatrix} 1 & 0 & 0 & 0\\ 0 &1&0&0\\ 0&x_6&x_7&x_8\\ 0&0&0&1 \end{pmatrix},$ $ D=\begin{pmatrix} 1 & 0 & 0 & 0\\ 0 &1&0&0\\ 0&0&1&0\\ 0&0&x_9&x_{10} \end{pmatrix}$. Let  $W$ be a product of above matrices. (For example $ W $ can be $D^2ABDC$ or $CAD$ (any product of above matrices)). Now define reverse of $W$ as $Re(W)$. For example if $W=D^2ABDC$ then $Re(W)=CDBAD^2$, If $W=CAD$ then $Re(W)=DAC$. I want to prove $\det(W+Re(W))-\det(W-Re(W))$ is divisible by $4$. In other words $\det(W+Re(W))-\det(W-Re(W))\equiv 0 \mod 4$. Any comment really appreciated. Note (Grumpy Parsnip): A generalized phenomenon seems to be true here. One can define $n\times n$ matrices of a similar form. E.g. $A_2=\left(\begin{array}{cc} x_1&x_2\\0&1\end{array}\right)$ and $B_2=\left(\begin{array}{cc} 1&0\\x_3&x_4\end{array}\right)$. Then it seems that for $2m\times 2m$ matrices, $$\det(W+Re(W))+(-1)^{m-1}\det(W-Re(W))=0\mod 4.$$ Perhaps the $2\times 2$ case is easier to understand. I'll award a bounty for a solution to the $2\times 2$ case. (Hopefully the OP, who is a friend of mine, will not mind my edits.)",,"['linear-algebra', 'matrices']"
43,The Gram-Schmidt process is a deformation retraction,The Gram-Schmidt process is a deformation retraction,,"Consider the Gram-Schmidt process $r : GL(n) \rightarrow O(n)$ that sends invertible matrices to orthogonal matrices. I need to show this is a deformation retraction and, by restrictions of $r$, establish each space is a deformation retract of the following: $SO(n) \subset SL(n) \subset GL^{+}(n)$ where $GL^{+}(n)$ is the set of matrices with positive determinant. I'm not very experienced with linear algebra so I'm having some difficulty in this. The idea I had was to define, for vectors $u, v \in R^{n}$ and real $t \in [0, 1]$, the altered projection: $P(u, v, t) = \displaystyle\frac{<u, (1 - t)u + tv> u}{<u, u>}$. Then we'd have the homotopy $H((a_{1}, ..., a_{n}), t) = (w_{1}, ..., w_{n})$ where $w_{1} = a_{1}$ $w_{2} = a_{2} - P(w_{1}, a_{2}, t)$ . . $w_{n} = a_{n} - \displaystyle\sum_{i = 1}^{n - 1} P(w_{i}, a_{n}, t)$. But actually now I think this doesn't work, because I can't really guarantee it's invertible for all $t$.  I have no other ideas.","Consider the Gram-Schmidt process $r : GL(n) \rightarrow O(n)$ that sends invertible matrices to orthogonal matrices. I need to show this is a deformation retraction and, by restrictions of $r$, establish each space is a deformation retract of the following: $SO(n) \subset SL(n) \subset GL^{+}(n)$ where $GL^{+}(n)$ is the set of matrices with positive determinant. I'm not very experienced with linear algebra so I'm having some difficulty in this. The idea I had was to define, for vectors $u, v \in R^{n}$ and real $t \in [0, 1]$, the altered projection: $P(u, v, t) = \displaystyle\frac{<u, (1 - t)u + tv> u}{<u, u>}$. Then we'd have the homotopy $H((a_{1}, ..., a_{n}), t) = (w_{1}, ..., w_{n})$ where $w_{1} = a_{1}$ $w_{2} = a_{2} - P(w_{1}, a_{2}, t)$ . . $w_{n} = a_{n} - \displaystyle\sum_{i = 1}^{n - 1} P(w_{i}, a_{n}, t)$. But actually now I think this doesn't work, because I can't really guarantee it's invertible for all $t$.  I have no other ideas.",,"['linear-algebra', 'algebraic-topology', 'lie-groups']"
44,The Hessian of the Determinant,The Hessian of the Determinant,,"It is well known how to take the derivative of the determinant: let $A(s)$ be a family of square matrices smoothly parametrised by the variable $s$ (in other words, $A:\mathbb{R}\to \mathbb{R}^{N^2}$ is a smooth curve). Then the derivative of the determinant can be given by $$ \frac{d}{ds} \det A(s) = \operatorname{Tr} \left(\tilde{A}(s) A'(s)\right)$$ where $A'(s) = \frac{d}{ds}A(s)$ and $\tilde{A}(s)$ is the adjugate matrix of $A(s)$. The above formula also generalises to the case that $A:\mathbb{R}^d\to \mathbb{R}^{N^2}$ is a smooth, $d$-parameter family of square matrices. On the other hand, I have not come across a nice expression for the second derivative (Hessian) of the determinant of such a family. Just by using Leibniz rule, one term is obvious: $\operatorname{Tr}\left(\tilde{A}(s) A''(s)\right)$. However, I don't know of any nice expression of the derivative of the adjugate. Evaluating the adjugate component wise, we see that it should be linear in $A'(s)$. So the question is: does anyone know of a nice form to present the derivative of the Adjugate matrix, or a nice form to present the Hessian of the determinant? For my purpose, an answer to the following weaker form of the problem would also be satisfactory: Let $A:\mathbb{R}^d\to \mathbb{R}^{N^2}$ be a smooth $d$-parameter family of symmetric matrices. Suppose $\det(A(0)) = 0$. Now, if we know that the kernel of $A(0)$ as dimension at least 2, then by a diagonalisation argument, the adjugate matrix $\tilde{A}(0)$ must vanish identically. And therefore the Hessian is determined by the term with the derivative of the adjugate. Then under the assumptions that $d \geq 2$ and $A(0)$ has nullity at least 2: (a) is it possible for the Hessian to be non-degenerate? (b) If so, what are some sufficient conditions to guarantee nondegeneracy? (In the case $d = 1$ it is clear that the ""Hessian"" can be non-degenerate, just by taking $N = 2$ and $A(s) = s I$.)","It is well known how to take the derivative of the determinant: let $A(s)$ be a family of square matrices smoothly parametrised by the variable $s$ (in other words, $A:\mathbb{R}\to \mathbb{R}^{N^2}$ is a smooth curve). Then the derivative of the determinant can be given by $$ \frac{d}{ds} \det A(s) = \operatorname{Tr} \left(\tilde{A}(s) A'(s)\right)$$ where $A'(s) = \frac{d}{ds}A(s)$ and $\tilde{A}(s)$ is the adjugate matrix of $A(s)$. The above formula also generalises to the case that $A:\mathbb{R}^d\to \mathbb{R}^{N^2}$ is a smooth, $d$-parameter family of square matrices. On the other hand, I have not come across a nice expression for the second derivative (Hessian) of the determinant of such a family. Just by using Leibniz rule, one term is obvious: $\operatorname{Tr}\left(\tilde{A}(s) A''(s)\right)$. However, I don't know of any nice expression of the derivative of the adjugate. Evaluating the adjugate component wise, we see that it should be linear in $A'(s)$. So the question is: does anyone know of a nice form to present the derivative of the Adjugate matrix, or a nice form to present the Hessian of the determinant? For my purpose, an answer to the following weaker form of the problem would also be satisfactory: Let $A:\mathbb{R}^d\to \mathbb{R}^{N^2}$ be a smooth $d$-parameter family of symmetric matrices. Suppose $\det(A(0)) = 0$. Now, if we know that the kernel of $A(0)$ as dimension at least 2, then by a diagonalisation argument, the adjugate matrix $\tilde{A}(0)$ must vanish identically. And therefore the Hessian is determined by the term with the derivative of the adjugate. Then under the assumptions that $d \geq 2$ and $A(0)$ has nullity at least 2: (a) is it possible for the Hessian to be non-degenerate? (b) If so, what are some sufficient conditions to guarantee nondegeneracy? (In the case $d = 1$ it is clear that the ""Hessian"" can be non-degenerate, just by taking $N = 2$ and $A(s) = s I$.)",,"['linear-algebra', 'determinant']"
45,Did I just discover a new way to calculate the signature of a matrix?,Did I just discover a new way to calculate the signature of a matrix?,,"Due to the complains for more clarity down below I've cut my post into segments. Feel free to skip right to Definitions, Algorithm & Conjecture. If this is not clear enough, then I'm afraid I can't help it. Story I'm taking a course on linear algebra and recently we were covering congruence of matrices. By Sylvester's law of inertia any two real symmetric matrices are congruent if they have the same signatures. We were advised to calculate signatures by considering matrices as bilinear forms and finding their orthogonal bases, which from my point of view is extremely tedious and requires painstaking work, both with regard to memorization and computation. So I was looking out for a better way to do this and by Googling I discovered that simultaneous row and column transformations preserve the signature, which turns out to be quite simple to understand once you consider elementary operations as matrices: $$ \boldsymbol{A'} = \boldsymbol{EAE}^T $$ That is way easier! However, as an extremely lazy person I still wasn't satisfied and here's where the fun part begins: I began looking at the elementary operations and how they affect the outcome. Multiplying a row by a negative constant might trivially change the signature (just consider identity matrix). After a while I found an example of how interchanging rows might also affect it. And adding a row multiplied by $-2$ to itself is equivalent to multiplying it by a negative scalar. Thus I was left with adding to the row a different row multiplied by a constant and I couldn't find a counterexample for this one. More than that! Using only this operation I got through my previous assignment and by turning a matrix into a row echelon form I was able to get a correct signature in every exercise. It also helped me spot a mistake in my simultaneous row and column operations on the recent test. By this method I calculated 11 correct signatures - it would be very odd if this was just an accident! I know that chances of me discovering something new in math are infinitesimal but I couldn't resist the clickbaity title. I hope you'll forgive me. But I'm genuinely curious about this one. I tried talking with my professor about it but he seemed uninterested, or maybe I did a poor job explaining it. He just dismissed the entire problem by saying that reduction to a row echelon form does not preserve the signature. Did I stumble upon some already known algorithm? Why then would no one talk about it at uni? I tried thinking about how to prove this but nothing comes to mind. Perhaps I miss some obvious counterexamples? If so, why did it work in all of the previous exercises? Definitions We use this definition of congruence and this definition of signature. Congruence :We say that two squrare matrices A and B over some field are congruent if there exists an invertible matrix P such that: $$\boldsymbol{A} = \boldsymbol{P}^T \boldsymbol{B P}$$ Signature : A real, nondegenerate $n\times n$ symmetric matrix A , and its corresponding symmetric bilinear form $\boldsymbol{G}(v,u) = v^T \boldsymbol{A} u$ , has signature $\boldsymbol{(p,q)}$ (or $\boldsymbol{p-q}$ in a different notation) if there is a nondegenerate matrix C such that $ \boldsymbol{CAC}^T $ is a diagonal matrix with p 1s and q (-1)s. Algorithm Using only this operation - adding a row multiplied by a constant to another row - get a matrix to its upper-triangular form. Let $p$ be a number of positive entries on the diagonal and $q$ be a number of negative ones. Signature of a matrix is equal to $(p,q)$ . Conjecture The aforementioned Algorithm provides a correct signature for all nondegenerate symmetric square matrices. Further questions Why would that be? How to prove it? Any ideas for counterexamples? Does it hold for $3 \times 3$ matrices and below but fails for bigger matrices, as suggested by Ben Grossmann in the comments? Any counterexamples of this sort? In this case - why would it work for n = 3? Examples $$ \boldsymbol{A}= \begin{bmatrix} 8 & 8 & 5\\ 8 & 0 & 4\\ 5 & 4 & 3 \end{bmatrix}  \overset{r_2 \to r_2-r_1}{\longrightarrow} \begin{bmatrix} 8 & 8 & 5\\ 0 & -8 & -1\\ 5 & 4 & 3 \end{bmatrix} \overset{r_3 \to r_3-\frac{5}{8}r_1}{\longrightarrow} \begin{bmatrix} 8 & 8 & 5\\ 0 & -8 & -1\\ 0 & -1 & -\frac{1}{8} \end{bmatrix}  \overset{r_3 \to r_3-\frac{1}{8}r_2}{\longrightarrow} \begin{bmatrix} 8 & 8 & 5\\ 0 & -8 & -1\\ 0 & 0 & 0 \end{bmatrix} $$ And we already see that the signature is (1,1) . Let $x \in \mathbb{R}$ For which values of $x$ the signature of B equals $2$ ? $$ \boldsymbol{B} = \begin{bmatrix} 1 & 0 & 1\\ 0 & 2 & 3\\ 1 & 3 & x \end{bmatrix}  \overset{r_3 \to r_3-r_1}{\longrightarrow} \begin{bmatrix} 1 & 0 & 1\\ 0 & 2 & 3\\ 0 & 3 & x-1 \end{bmatrix}  \overset{r_3 \to r_3-\frac{3}{2}r_2}{\longrightarrow} \begin{bmatrix} 1 & 0 & 1\\ 0 & 2 & 3\\ 0 & 0 & x-\frac{11}{2} \end{bmatrix} $$ And the answer is for $x = \frac{11}{2}$ : $(2,0) = 2$ . Let $t,s \in \mathbb{R} $ $$ \boldsymbol{C}= \begin{bmatrix} 0 & 0 & 0 & 0 & t^2\\ 0 & -1 & 0 & 1 & 0\\ 0 & 0 & 1 & s & 0\\ 0 & 1 & s & s^2-1 & 0\\ t^2 & 0 & 0 & 0 & 0\\ \end{bmatrix} \underset{r_5 \to r_5-r_1}{\overset{r_1 \to r_1+r_5}{\longrightarrow}} \begin{bmatrix} t^2 & 0 & 0 & 0 & t^2\\ 0 & -1 & 0 & 1 & 0\\ 0 & 0 & 1 & s & 0\\ 0 & 1 & s & s^2-1 & 0\\ 0 & 0 & 0 & 0 & -t^2\\ \end{bmatrix} \underset{r_4 \to r_4-sr_3}{\overset{r_4 \to r_4+r_2}{\longrightarrow}} \begin{bmatrix} t^2 & 0 & 0 & 0 & t^2\\ 0 & -1 & 0 & 1 & 0\\ 0 & 0 & 1 & s & 0\\ 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & -t^2\\ \end{bmatrix} $$ Singature is (2,2) for $t\not= 0$ and (1,1) for $t = 0$ . I achieved the same result with simultaneous row and column operations, it took twice as long.","Due to the complains for more clarity down below I've cut my post into segments. Feel free to skip right to Definitions, Algorithm & Conjecture. If this is not clear enough, then I'm afraid I can't help it. Story I'm taking a course on linear algebra and recently we were covering congruence of matrices. By Sylvester's law of inertia any two real symmetric matrices are congruent if they have the same signatures. We were advised to calculate signatures by considering matrices as bilinear forms and finding their orthogonal bases, which from my point of view is extremely tedious and requires painstaking work, both with regard to memorization and computation. So I was looking out for a better way to do this and by Googling I discovered that simultaneous row and column transformations preserve the signature, which turns out to be quite simple to understand once you consider elementary operations as matrices: That is way easier! However, as an extremely lazy person I still wasn't satisfied and here's where the fun part begins: I began looking at the elementary operations and how they affect the outcome. Multiplying a row by a negative constant might trivially change the signature (just consider identity matrix). After a while I found an example of how interchanging rows might also affect it. And adding a row multiplied by to itself is equivalent to multiplying it by a negative scalar. Thus I was left with adding to the row a different row multiplied by a constant and I couldn't find a counterexample for this one. More than that! Using only this operation I got through my previous assignment and by turning a matrix into a row echelon form I was able to get a correct signature in every exercise. It also helped me spot a mistake in my simultaneous row and column operations on the recent test. By this method I calculated 11 correct signatures - it would be very odd if this was just an accident! I know that chances of me discovering something new in math are infinitesimal but I couldn't resist the clickbaity title. I hope you'll forgive me. But I'm genuinely curious about this one. I tried talking with my professor about it but he seemed uninterested, or maybe I did a poor job explaining it. He just dismissed the entire problem by saying that reduction to a row echelon form does not preserve the signature. Did I stumble upon some already known algorithm? Why then would no one talk about it at uni? I tried thinking about how to prove this but nothing comes to mind. Perhaps I miss some obvious counterexamples? If so, why did it work in all of the previous exercises? Definitions We use this definition of congruence and this definition of signature. Congruence :We say that two squrare matrices A and B over some field are congruent if there exists an invertible matrix P such that: Signature : A real, nondegenerate symmetric matrix A , and its corresponding symmetric bilinear form , has signature (or in a different notation) if there is a nondegenerate matrix C such that is a diagonal matrix with p 1s and q (-1)s. Algorithm Using only this operation - adding a row multiplied by a constant to another row - get a matrix to its upper-triangular form. Let be a number of positive entries on the diagonal and be a number of negative ones. Signature of a matrix is equal to . Conjecture The aforementioned Algorithm provides a correct signature for all nondegenerate symmetric square matrices. Further questions Why would that be? How to prove it? Any ideas for counterexamples? Does it hold for matrices and below but fails for bigger matrices, as suggested by Ben Grossmann in the comments? Any counterexamples of this sort? In this case - why would it work for n = 3? Examples And we already see that the signature is (1,1) . Let For which values of the signature of B equals ? And the answer is for : . Let Singature is (2,2) for and (1,1) for . I achieved the same result with simultaneous row and column operations, it took twice as long."," \boldsymbol{A'} = \boldsymbol{EAE}^T  -2 \boldsymbol{A} = \boldsymbol{P}^T \boldsymbol{B P} n\times n \boldsymbol{G}(v,u) = v^T \boldsymbol{A} u \boldsymbol{(p,q)} \boldsymbol{p-q}  \boldsymbol{CAC}^T  p q (p,q) 3 \times 3  \boldsymbol{A}=
\begin{bmatrix}
8 & 8 & 5\\
8 & 0 & 4\\
5 & 4 & 3
\end{bmatrix} 
\overset{r_2 \to r_2-r_1}{\longrightarrow}
\begin{bmatrix}
8 & 8 & 5\\
0 & -8 & -1\\
5 & 4 & 3
\end{bmatrix}
\overset{r_3 \to r_3-\frac{5}{8}r_1}{\longrightarrow}
\begin{bmatrix}
8 & 8 & 5\\
0 & -8 & -1\\
0 & -1 & -\frac{1}{8}
\end{bmatrix} 
\overset{r_3 \to r_3-\frac{1}{8}r_2}{\longrightarrow}
\begin{bmatrix}
8 & 8 & 5\\
0 & -8 & -1\\
0 & 0 & 0
\end{bmatrix}  x \in \mathbb{R} x 2  \boldsymbol{B} = \begin{bmatrix}
1 & 0 & 1\\
0 & 2 & 3\\
1 & 3 & x
\end{bmatrix} 
\overset{r_3 \to r_3-r_1}{\longrightarrow}
\begin{bmatrix}
1 & 0 & 1\\
0 & 2 & 3\\
0 & 3 & x-1
\end{bmatrix} 
\overset{r_3 \to r_3-\frac{3}{2}r_2}{\longrightarrow}
\begin{bmatrix}
1 & 0 & 1\\
0 & 2 & 3\\
0 & 0 & x-\frac{11}{2}
\end{bmatrix}  x = \frac{11}{2} (2,0) = 2 t,s \in \mathbb{R}   \boldsymbol{C}=
\begin{bmatrix}
0 & 0 & 0 & 0 & t^2\\
0 & -1 & 0 & 1 & 0\\
0 & 0 & 1 & s & 0\\
0 & 1 & s & s^2-1 & 0\\
t^2 & 0 & 0 & 0 & 0\\
\end{bmatrix}
\underset{r_5 \to r_5-r_1}{\overset{r_1 \to r_1+r_5}{\longrightarrow}}
\begin{bmatrix}
t^2 & 0 & 0 & 0 & t^2\\
0 & -1 & 0 & 1 & 0\\
0 & 0 & 1 & s & 0\\
0 & 1 & s & s^2-1 & 0\\
0 & 0 & 0 & 0 & -t^2\\
\end{bmatrix}
\underset{r_4 \to r_4-sr_3}{\overset{r_4 \to r_4+r_2}{\longrightarrow}}
\begin{bmatrix}
t^2 & 0 & 0 & 0 & t^2\\
0 & -1 & 0 & 1 & 0\\
0 & 0 & 1 & s & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & -t^2\\
\end{bmatrix}  t\not= 0 t = 0","['linear-algebra', 'matrices', 'algebra-precalculus', 'symmetric-matrices', 'matrix-congruences']"
46,Avoid the planes - the geometry of grassmannians,Avoid the planes - the geometry of grassmannians,,"Suppose we have $n$ planes $H_1, \ldots, H_n$ in $\mathbb{R}^m$ of codimension $q$ , or equivalently of dimension $d=m-q$ . I want to choose a vector which does not belong to the planes in a continuous way. There are two versions of this problem, depending on how we parameterize the planes, and the answer can be actually different. Unframed version . Let $Gr_q(m) $ be the grassmannian, that is the space of codimension q planes in $\mathbb{R}^m$ . Does there exist a function $$c : Gr_q(m) ^n \to \mathbb{R}^m$$ Such that $c(H_1, \ldots, H_n) \not \in H_i$ for all $i$ ? Framed version . Let $V_{d,m}$ be the Stiefel manifold, that is the space of orthonormal systems in $\mathbb{R}^m$ of cardinality $d$ . Does there exist a function $$c : V_{d,m} ^n \to \mathbb{R}^m$$ Such that $c(H_1, \ldots, H_n) \not \in H_i$ for all $i$ ? Note . I slightly changed the notation to agree with Chris one; now $d$ denote the dimension of the planes and $q$ the codimension.","Suppose we have planes in of codimension , or equivalently of dimension . I want to choose a vector which does not belong to the planes in a continuous way. There are two versions of this problem, depending on how we parameterize the planes, and the answer can be actually different. Unframed version . Let be the grassmannian, that is the space of codimension q planes in . Does there exist a function Such that for all ? Framed version . Let be the Stiefel manifold, that is the space of orthonormal systems in of cardinality . Does there exist a function Such that for all ? Note . I slightly changed the notation to agree with Chris one; now denote the dimension of the planes and the codimension.","n H_1, \ldots, H_n \mathbb{R}^m q d=m-q Gr_q(m)  \mathbb{R}^m c : Gr_q(m) ^n \to \mathbb{R}^m c(H_1, \ldots, H_n) \not \in H_i i V_{d,m} \mathbb{R}^m d c : V_{d,m} ^n \to \mathbb{R}^m c(H_1, \ldots, H_n) \not \in H_i i d q","['linear-algebra', 'geometry', 'algebraic-topology', 'continuity', 'grassmannian']"
47,"If $D$ is a diagonal matrix, when is the commutator $DA - AD$ full rank?","If  is a diagonal matrix, when is the commutator  full rank?",D DA - AD,"Suppose $A \in \mathbb{C}^{n \times n}$ is full rank.   I'm looking for a sufficient condition on $A$ such that for some diagonal matrix $D \in \mathbb{C}^{n \times n}$ , the commutator $D A - A D$ is full rank. I've worked out some necessary conditions- $A$ and $D$ cannot share an eigenvector, so no column of $D$ can have $n-1$ zero indices (ie,  no column of $A$ is a scaled column of the identity matrix). For $n=2$ , \begin{align} DA - A D &=  \begin{bmatrix}  0 & (d_1 - d_2) a_{1,2} \\ (d_2 - d_1) a_{2, 1} & 0 \end{bmatrix}\\  \det(DA - AD) &= (d_1-d_2)^2 a_{1,2} a_{2,1}, \end{align} so a $a_{1,2}, a_{2,1} \neq 0$ is both necessary and sufficient. But for $n=3$ , \begin{align} DA - A D &=  \begin{bmatrix}  0 & (d_1 - d_2) a_{1,2} & (d_1 - d_3) a_{1, 3} \\ (d_2 - d_1) a_{2, 1} & 0 & (d_2 - d_3) a_{2, 3} \\ (d_3 - d_1) a_{3, 1} & (d_3 - d_2) a_{3, 2} & 0 \end{bmatrix}\\  \det(DA - AD) &= (d_1-d_2)(d_1-d_3)(d_2-d_3) (a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2}). \end{align} So if $(a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2})=0$ , $DA - AD$ is rank deficient regardless of the choice of $D$ . Q1 : Is there a geometric interpretation for the constraint $(a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2}) \neq 0$ ? Q2 :  I don't get a factored form of the determinant (one term depending on $D$ , one term on $A$ ) for $n > 3$ .  Is $n=3$ a special case? Q3 : Is there a sufficient condition on $A$ such that $DA - AD$ is full rank for $n>3$ ? I'd prefer to not assume $A$ is positive definite. Edit :  If $A$ is symmetric, $DA - AD$ is skew-symmetric and thus rank deficient if $n$ is odd. Q4 : $A$ symmetric is one way to make $(a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2})=0$ , but clearly other choices result in a rank-deficient commutator.  Is there a clean way to express this for $n > 3$ ?","Suppose is full rank.   I'm looking for a sufficient condition on such that for some diagonal matrix , the commutator is full rank. I've worked out some necessary conditions- and cannot share an eigenvector, so no column of can have zero indices (ie,  no column of is a scaled column of the identity matrix). For , so a is both necessary and sufficient. But for , So if , is rank deficient regardless of the choice of . Q1 : Is there a geometric interpretation for the constraint ? Q2 :  I don't get a factored form of the determinant (one term depending on , one term on ) for .  Is a special case? Q3 : Is there a sufficient condition on such that is full rank for ? I'd prefer to not assume is positive definite. Edit :  If is symmetric, is skew-symmetric and thus rank deficient if is odd. Q4 : symmetric is one way to make , but clearly other choices result in a rank-deficient commutator.  Is there a clean way to express this for ?","A \in \mathbb{C}^{n \times n} A D \in \mathbb{C}^{n \times n} D A - A D A D D n-1 A n=2 \begin{align}
DA - A D &= 
\begin{bmatrix} 
0 & (d_1 - d_2) a_{1,2} \\
(d_2 - d_1) a_{2, 1} & 0
\end{bmatrix}\\ 
\det(DA - AD) &= (d_1-d_2)^2 a_{1,2} a_{2,1},
\end{align} a_{1,2}, a_{2,1} \neq 0 n=3 \begin{align}
DA - A D &= 
\begin{bmatrix} 
0 & (d_1 - d_2) a_{1,2} & (d_1 - d_3) a_{1, 3} \\
(d_2 - d_1) a_{2, 1} & 0 & (d_2 - d_3) a_{2, 3} \\
(d_3 - d_1) a_{3, 1} & (d_3 - d_2) a_{3, 2} & 0
\end{bmatrix}\\ 
\det(DA - AD) &= (d_1-d_2)(d_1-d_3)(d_2-d_3) (a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2}).
\end{align} (a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2})=0 DA - AD D (a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2}) \neq 0 D A n > 3 n=3 A DA - AD n>3 A A DA - AD n A (a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2})=0 n > 3","['linear-algebra', 'matrices']"
48,Deciding eigenvalue degeneracy without calculating all eigenvalues and eigenvectors,Deciding eigenvalue degeneracy without calculating all eigenvalues and eigenvectors,,"Given a diagonalizable matrix $M$ (that is, a normal matrix ), can we determine whether the matrix has degenerate eigenvalues without explicitly calculating all the eigenvalues and eigenvectors? An example that came to my mind is that $M$ is square of a skew-Hermitian matrix since a skew-symmetric matrix always has pairs of pure imaginary eigenvalues $\pm i \lambda_i$ . Similarly, a matrix that is square of a matrix that has pairs of eigenvalues with different signs such as $\lambda_1,-\lambda_1,\dots$ is such a case. However, these things require sorts of decomposition of the matrix $M$ , which is another problem! Another way is calculating the characteristic polynomial $\det(M-\lambda I)=0$ and factorize it, then check the degrees of each terms. But this amounts to calculating all the eigenvalues already. Do we have other (simple) criteria or ways to determine the degeneracy of eigenvalues of a matrix $M$ ?","Given a diagonalizable matrix (that is, a normal matrix ), can we determine whether the matrix has degenerate eigenvalues without explicitly calculating all the eigenvalues and eigenvectors? An example that came to my mind is that is square of a skew-Hermitian matrix since a skew-symmetric matrix always has pairs of pure imaginary eigenvalues . Similarly, a matrix that is square of a matrix that has pairs of eigenvalues with different signs such as is such a case. However, these things require sorts of decomposition of the matrix , which is another problem! Another way is calculating the characteristic polynomial and factorize it, then check the degrees of each terms. But this amounts to calculating all the eigenvalues already. Do we have other (simple) criteria or ways to determine the degeneracy of eigenvalues of a matrix ?","M M \pm i \lambda_i \lambda_1,-\lambda_1,\dots M \det(M-\lambda I)=0 M","['linear-algebra', 'matrices', 'functional-analysis', 'eigenvalues-eigenvectors', 'spectral-theory']"
49,Derivative of a bra?,Derivative of a bra?,,"I understand that $$ \frac{\mathrm d}{\mathrm dt} \langle\psi|\psi\rangle =\left[\frac{\mathrm d}{\mathrm dt} \langle\psi|\right]|\psi\rangle + \langle\psi|\left[\frac{\mathrm d}{\mathrm dt}|\psi\rangle\right]$$ and that this can be established by a direct application of the definition of the derivative; but it is not clear to me why $$\frac{\mathrm d}{\mathrm d t} |\psi\rangle \;=\; -i H |\psi\rangle\implies\left[\frac{\mathrm d}{\mathrm dt} \langle\psi|\right]=i\langle\psi|H^\dagger$$ given only a linear vector space and the properties of an inner product. Should I be able to derive the above conclusion from just the properties of a linear vector space with an inner product, or is more needed?","I understand that $$ \frac{\mathrm d}{\mathrm dt} \langle\psi|\psi\rangle =\left[\frac{\mathrm d}{\mathrm dt} \langle\psi|\right]|\psi\rangle + \langle\psi|\left[\frac{\mathrm d}{\mathrm dt}|\psi\rangle\right]$$ and that this can be established by a direct application of the definition of the derivative; but it is not clear to me why $$\frac{\mathrm d}{\mathrm d t} |\psi\rangle \;=\; -i H |\psi\rangle\implies\left[\frac{\mathrm d}{\mathrm dt} \langle\psi|\right]=i\langle\psi|H^\dagger$$ given only a linear vector space and the properties of an inner product. Should I be able to derive the above conclusion from just the properties of a linear vector space with an inner product, or is more needed?",,"['linear-algebra', 'derivatives', 'physics', 'quantum-mechanics']"
50,"Etymology of the word ""isotropic""","Etymology of the word ""isotropic""",,"Given a quadratic form $q : V \rightarrow k$, a nonzero vector $v \in V$ is said to be isotropic if $q(v) = 0$.  Any subspace of $V$ containing such a vector is also said to be isotropic, and the quadratic form itself is also said to be isotropic.  The word ""isotropic"" is Greek for ""the same in every direction.""  What does this have to do with being a zero of a quadratic form?","Given a quadratic form $q : V \rightarrow k$, a nonzero vector $v \in V$ is said to be isotropic if $q(v) = 0$.  Any subspace of $V$ containing such a vector is also said to be isotropic, and the quadratic form itself is also said to be isotropic.  The word ""isotropic"" is Greek for ""the same in every direction.""  What does this have to do with being a zero of a quadratic form?",,"['linear-algebra', 'terminology', 'quadratic-forms']"
51,Calculate the determinant of $a_{ij} = \frac{(1+x)^{i+j-1}-1}{i+j-1}$,Calculate the determinant of,a_{ij} = \frac{(1+x)^{i+j-1}-1}{i+j-1},"There is a question asked by my classmate. Looking forward to some ideas, thanks. Set $A=\{a_{ij}\}_{n\times n}$ , where $$a_{ij}=\frac{(1+x)^{i+j-1}-1}{i+j-1}.$$ Prove that $\det A=cx^{n^2}$ for some $c$ . I have tried to calculate it, but failed. I computed $$\frac{(1+x)^{i+j-1}-1}{i+j-1}=\sum_{k=1}^{i+j-1}\frac{(i+j-2)!}{k!(i+j-1-k)!}x^k,$$ but I have no idea how to continue. I know when $a_{ij}=\frac{1}{i+j-1}$ , it is the Hilbert matrix, and we can get its determinant, but I don’t know how to calculate the above determinant. Are there some hints? Looking forward to your answer. Thanks!","There is a question asked by my classmate. Looking forward to some ideas, thanks. Set , where Prove that for some . I have tried to calculate it, but failed. I computed but I have no idea how to continue. I know when , it is the Hilbert matrix, and we can get its determinant, but I don’t know how to calculate the above determinant. Are there some hints? Looking forward to your answer. Thanks!","A=\{a_{ij}\}_{n\times n} a_{ij}=\frac{(1+x)^{i+j-1}-1}{i+j-1}. \det A=cx^{n^2} c \frac{(1+x)^{i+j-1}-1}{i+j-1}=\sum_{k=1}^{i+j-1}\frac{(i+j-2)!}{k!(i+j-1-k)!}x^k, a_{ij}=\frac{1}{i+j-1}","['linear-algebra', 'matrices', 'determinant']"
52,Sum of determinants of block submatrices,Sum of determinants of block submatrices,,"I have a $2n \times 2n$ matrix, $M$ . I view it a block matrix, of $n^2$ blocks, each of shape $2\times 2$ . Computing the determinant of $M$ is easy by conventional methods. I could also look at diagonal block submatrices : given a set $S \subseteq [n]$ , I take the submatrix of all the rows in $S$ and columns in $S$ . For instance, if $n=3$ , I have 9 blocks (each of 4 entries), $A_{1,1}, A_{1,2} \dots  A_{3,3}$ . $S$ could be ${1,3}$ , in which case I take the 4x4 submatrix $$\begin{bmatrix} A_{1,1} & A_{1,3} \\ A_{3,1} & A_{3,3}\end{bmatrix}$$ There are $2^n$ such submatrices, and each has its own determinant. I would like to compute the sum of those determinants as efficiently as possible. (The trivial case, where $S = \emptyset$ , naturally has a determinant of 1.) This might sound like a bold hope, but consider that if this were not a block matrix, then you can very easily compute the sum of the determinants of the diagonal submatrices: $$ \sum_{S \subseteq [n]} M_{S,S} = \det(M + I) $$ So I'm hoping that there's some way to make a good ""block identity matrix"" that I can carry out operations with to compute this sum. Honestly, I'd be happy with anything in polynomial time. Proofs of hardness would also be interesting. For instance, I could believe that the problem of computing the permanent might somehow be reduced this problem, in which case it would be NP-Hard. I have several fairly different approaches that take $2^n$ time, so anything in even $1.9^n$ would be stimulating.","I have a matrix, . I view it a block matrix, of blocks, each of shape . Computing the determinant of is easy by conventional methods. I could also look at diagonal block submatrices : given a set , I take the submatrix of all the rows in and columns in . For instance, if , I have 9 blocks (each of 4 entries), . could be , in which case I take the 4x4 submatrix There are such submatrices, and each has its own determinant. I would like to compute the sum of those determinants as efficiently as possible. (The trivial case, where , naturally has a determinant of 1.) This might sound like a bold hope, but consider that if this were not a block matrix, then you can very easily compute the sum of the determinants of the diagonal submatrices: So I'm hoping that there's some way to make a good ""block identity matrix"" that I can carry out operations with to compute this sum. Honestly, I'd be happy with anything in polynomial time. Proofs of hardness would also be interesting. For instance, I could believe that the problem of computing the permanent might somehow be reduced this problem, in which case it would be NP-Hard. I have several fairly different approaches that take time, so anything in even would be stimulating.","2n \times 2n M n^2 2\times 2 M S \subseteq [n] S S n=3 A_{1,1}, A_{1,2} \dots  A_{3,3} S {1,3} \begin{bmatrix} A_{1,1} & A_{1,3} \\ A_{3,1} & A_{3,3}\end{bmatrix} 2^n S = \emptyset  \sum_{S \subseteq [n]} M_{S,S} = \det(M + I)  2^n 1.9^n","['linear-algebra', 'block-matrices', 'computational-algebra']"
53,Geometric Intuition about the relation between Clifford Algebra and Exterior Algebra,Geometric Intuition about the relation between Clifford Algebra and Exterior Algebra,,"It is common to see a relation being established between the Clifford Algebra and the Exterior Algebra of a vector space. Recently reading some texts written by Physicists I've seem applications of Clifford Algebras on which it seems that the author is dealing with the usual exterior algebra somehow. It seems, in that case, that this relation is quite often used when appliying Clifford Algebras on Physics and Geometry. Searching for this a little I've found the Clifford Algebra page on Wikipedia, where we can find a section ""Relation to the exterior algebra"". It is then said: Given a vector space $V$ one can construct the exterior algebra $\bigwedge (V)$, whose definition is independent of any quadratic form on $V$. It turns out that if $K$ does not have characteristic $2$ then there is a natural isomorphism between $\bigwedge(V)$ and $\mathcal{Cl}(V,Q)$ considered as vector spaces. This is an algebra isomorphism if and only if $Q = 0$. One can thus consider the Clifford algebra $\mathcal{Cl}(V,Q)$ as an enrichment of the exterior algebra on $V$ with a multiplication that depends on $Q$ (one can still define the exterior product independent of $Q$). Now I want to get some geometrical intuition on this. I know that the exterior algebra is the algebra inside of which all $k$-vectors can be manipulated together for all possible values of $k$. I also know that a $k$-vector has a direct geometrical meaning: it is a piece of oriented $k$-dimensional subspace. Also I know that one quadratic form $Q$ also has one geometric meaning: it might represent some sort of length defined on $V$. Now, what is the idea here? That when we construct $\bigwedge (V)$ we somehow are not accounting for the idea of ""measure"" of those pieces of $k$-dimensional subspace, because no idea of length was defined in $V$ and then $\mathcal{Cl}(V,Q)$ is the result of carrying the idea of length introduced in $V$ by $Q$ to all the $k$ vectors and hence to the algebra where we manipulate them together? I feel this is not quite the right intuition yet. What is the correct geometrical intuition behind the relation between the clifford algebra and the exterior algebra?","It is common to see a relation being established between the Clifford Algebra and the Exterior Algebra of a vector space. Recently reading some texts written by Physicists I've seem applications of Clifford Algebras on which it seems that the author is dealing with the usual exterior algebra somehow. It seems, in that case, that this relation is quite often used when appliying Clifford Algebras on Physics and Geometry. Searching for this a little I've found the Clifford Algebra page on Wikipedia, where we can find a section ""Relation to the exterior algebra"". It is then said: Given a vector space $V$ one can construct the exterior algebra $\bigwedge (V)$, whose definition is independent of any quadratic form on $V$. It turns out that if $K$ does not have characteristic $2$ then there is a natural isomorphism between $\bigwedge(V)$ and $\mathcal{Cl}(V,Q)$ considered as vector spaces. This is an algebra isomorphism if and only if $Q = 0$. One can thus consider the Clifford algebra $\mathcal{Cl}(V,Q)$ as an enrichment of the exterior algebra on $V$ with a multiplication that depends on $Q$ (one can still define the exterior product independent of $Q$). Now I want to get some geometrical intuition on this. I know that the exterior algebra is the algebra inside of which all $k$-vectors can be manipulated together for all possible values of $k$. I also know that a $k$-vector has a direct geometrical meaning: it is a piece of oriented $k$-dimensional subspace. Also I know that one quadratic form $Q$ also has one geometric meaning: it might represent some sort of length defined on $V$. Now, what is the idea here? That when we construct $\bigwedge (V)$ we somehow are not accounting for the idea of ""measure"" of those pieces of $k$-dimensional subspace, because no idea of length was defined in $V$ and then $\mathcal{Cl}(V,Q)$ is the result of carrying the idea of length introduced in $V$ by $Q$ to all the $k$ vectors and hence to the algebra where we manipulate them together? I feel this is not quite the right intuition yet. What is the correct geometrical intuition behind the relation between the clifford algebra and the exterior algebra?",,"['linear-algebra', 'intuition', 'multilinear-algebra', 'exterior-algebra', 'clifford-algebras']"
54,When does $\det e^A=e^{\det A}?$,When does,\det e^A=e^{\det A}?,"Which $2\times 2$ matrices satisfy the equation   $$\det e^A=e^{\det A}?$$ I know that $\det e^A=e^{\operatorname{trace}A}$ so assuming $A$ is real we get $$\operatorname{trace}A=\det A.$$ Then, $$\det(A-\lambda I)=\lambda^2-2\operatorname{trace}(A)\lambda+\det(A)=(\lambda-\det(A))^2$$ so the only eigenvalue is $$\lambda=\det(A)=\operatorname{trace}A.$$ Hence, $$\operatorname{trace}A=2\lambda=2\operatorname{trace}A\implies\operatorname{trace}A=\det A=0.$$ Write $$A=\begin{pmatrix}a&b\\c&-a\end{pmatrix}$$ so that $\operatorname{trace}A=0$ is already taken into account. Then, $$\det A=-a^2-bc=0$$ so $a^2=-bc$. Thus, $$A=\begin{pmatrix}\sqrt{-bc}&b\\c&-\sqrt{-bc}\end{pmatrix}$$ Is that the end of the solution?","Which $2\times 2$ matrices satisfy the equation   $$\det e^A=e^{\det A}?$$ I know that $\det e^A=e^{\operatorname{trace}A}$ so assuming $A$ is real we get $$\operatorname{trace}A=\det A.$$ Then, $$\det(A-\lambda I)=\lambda^2-2\operatorname{trace}(A)\lambda+\det(A)=(\lambda-\det(A))^2$$ so the only eigenvalue is $$\lambda=\det(A)=\operatorname{trace}A.$$ Hence, $$\operatorname{trace}A=2\lambda=2\operatorname{trace}A\implies\operatorname{trace}A=\det A=0.$$ Write $$A=\begin{pmatrix}a&b\\c&-a\end{pmatrix}$$ so that $\operatorname{trace}A=0$ is already taken into account. Then, $$\det A=-a^2-bc=0$$ so $a^2=-bc$. Thus, $$A=\begin{pmatrix}\sqrt{-bc}&b\\c&-\sqrt{-bc}\end{pmatrix}$$ Is that the end of the solution?",,['linear-algebra']
55,What is the interpretation of the eigenvectors of the jacobian matrix?,What is the interpretation of the eigenvectors of the jacobian matrix?,,I'm trying to think about the jacobian matrix as a abstract linear map. What is the interpretation of the eigenvalues and eigenvectors of the jacobian?,I'm trying to think about the jacobian matrix as a abstract linear map. What is the interpretation of the eigenvalues and eigenvectors of the jacobian?,,"['linear-algebra', 'multivariable-calculus', 'eigenvalues-eigenvectors', 'jacobian']"
56,Abelianization of GL_n,Abelianization of GL_n,,"Suppose $k$ is a field. How to prove that the abelianization of $GL_2(k)$ is $GL_1(k)$ ? Ditto for $GL_n(k)$. Can we say the same thing, were we to replace $k$ with $\mathbb Z$ ?","Suppose $k$ is a field. How to prove that the abelianization of $GL_2(k)$ is $GL_1(k)$ ? Ditto for $GL_n(k)$. Can we say the same thing, were we to replace $k$ with $\mathbb Z$ ?",,"['linear-algebra', 'group-theory']"
57,"Prove that matrices of this form have eigenvalues $0,1,\ldots , n-1$",Prove that matrices of this form have eigenvalues,"0,1,\ldots , n-1","Fix arbitrary real numbers $x_1,\ldots ,x_n$ which are pairwise distinct, i.e. so that $x_i \neq x_j$ for any pair $i \neq j$ . Let $A = (a_{ij})$ be the following $n \times n$ matrix: Its diagonal entries are given by the equation, $$a_{ii}=\sum_{j\neq i}\frac{x_i}{x_i-x_j},$$ while its off-diagonal entries given by the equation, $$a_{ij}=\frac{x_i}{x_i-x_j}$$ , for $i\neq j$ . For instance when n=2, the matrix looks like: $$A=\begin{pmatrix} \frac{x_1}{x_1-x_2} & \frac{x_1}{x_1-x_2}\\  \frac{x_2}{x_2-x_1} & \frac{x_2}{x_2-x_1} \end{pmatrix}$$ Prove that the set of eigenvalues for the matrix A is of the form $\left \{0,1,\ldots,n-1\right \}$ . I'm completely lost as to how to continue. I've tried to work on the determinant of $A-\lambda I$ for $2 \times 2$ and $3 \times 3$ matrices $A$ but I haven't managed to find anything helpful towards the proof. Update 1 I'm not entirely sure how to write this formula nicely as a mathematical expression, but as code in python I have that the $k$ th element of $v_0$ is p = product([L[i-1] - L[j-1] for i in [1..n] for j in [i+1..n] if i != k and j != k])         v[k-1] = p if k % 2 == 1 else -p where L refers to the list [x_1,...,x_n] , and I want for $1 \leq i < j \leq n$ and for $i \neq j \neq k$ I also have that $v_i$ is equal to $diag(x_1,...,x_n)^{i}v_0$ Update 2 the eigenvectors for the case where $n=4$ are, $$ v_0 = \begin{pmatrix} 1\\  -\frac{x_1^2 - x_1x_3 - (x_1 - x_3)x_4}{x_2^2 - x_2x_3 - (x_2 - x_3)x_4}\\  \frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_4}{x_2x_3 - x_3^2 - (x_2 - x_3)x_4}\\  -\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3}{x_2x_3 - (x_2 + x_3)x_4 + x_4^2} \end{pmatrix},$$ $$v_1 = \begin{pmatrix} 1\\  -\frac{x_1^2x_2 - x_1x_2x_3 - (x_1x_2 - x_2x_3)*x_4}{x_1x_2^2 - x_1x_2x_3 - (x_1x_2 - x_1x_3)x_4}\\  -\frac{(x_1 - x_2)x_3x_4 - (x_1^2 - x_1x_2)x_3}{x_1x_2x_3 - x_1x_3^2 - (x_1x_2 - x_1x_3)x_4}\\  -\frac{(x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4}{x_1x_2x_3 + x_1x_4^2 - (x_1x_2 + x_1x_3)x_4} \end{pmatrix},$$ $$v_2 = \begin{pmatrix} 1\\  -\frac{x_1^2x_2^2 - x_1x_2^2x_3 - (x_1x_2^2 - x_2^2x_3)x_4}{x_1^2x_2^2 - x_1^2x_2x_3 - (x_1^2x_2 - x_1^2x_3)x_4}\\  -\frac{(x_1 - x_2)x_3^2x_4 - (x_1^2 - x_1x_2)x_3^2}{x_1^2x_2x_3 - x_1^2x_3^2 - (x_1^2x_2 - x_1^2x_3)x_4}\\  -\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4^2}{x_1^2x_2x_3 + x_1^2x_4^2 - (x_1^2x_2 + x_1^2x_3)x_4} \end{pmatrix}, $$ $$v_3 = \begin{pmatrix} 1\\  -\frac{x_1^2x_2^3 - x_1x_2^3x_3 - (x_1x_2^3 - x_2^3x_3)x_4}{x_1^3x_2^2 - x_1^3x_2x_3 - (x_1^3x_2 - x_1^3x_3)x_4}\\  -\frac{(x_1 - x_2)x_3^3x_4 - (x_1^2 - x_1x_2)x_3^3}{x_1^3x_2x_3 - x_1^3x_3^2 - (x_1^3x_2 - x_1^3x_3)x_4}\\  -\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4^3}{x_1^3x_2x_3 + x_1^3x_4^2 - (x_1^3x_2 + x_1^3x_3)x_4} \end{pmatrix}$$ Update 3 I managed to work out a formula for the $j$ th element of the eigenvector with $eigenvalue=\lambda$ and $size=n$ , $$(-1)^{j+1}\frac{x_j^\lambda}{x_1^\lambda} \prod_{k\neq 1,k\neq j}^n \frac{x_1-x_k}{x_j-x_k} $$ I'm just not sure now as to how to use the formula for one entry of an eigenvector to prove the set of eigenvalues","Fix arbitrary real numbers which are pairwise distinct, i.e. so that for any pair . Let be the following matrix: Its diagonal entries are given by the equation, while its off-diagonal entries given by the equation, , for . For instance when n=2, the matrix looks like: Prove that the set of eigenvalues for the matrix A is of the form . I'm completely lost as to how to continue. I've tried to work on the determinant of for and matrices but I haven't managed to find anything helpful towards the proof. Update 1 I'm not entirely sure how to write this formula nicely as a mathematical expression, but as code in python I have that the th element of is p = product([L[i-1] - L[j-1] for i in [1..n] for j in [i+1..n] if i != k and j != k])         v[k-1] = p if k % 2 == 1 else -p where L refers to the list [x_1,...,x_n] , and I want for and for I also have that is equal to Update 2 the eigenvectors for the case where are, Update 3 I managed to work out a formula for the th element of the eigenvector with and , I'm just not sure now as to how to use the formula for one entry of an eigenvector to prove the set of eigenvalues","x_1,\ldots ,x_n x_i \neq x_j i \neq j A = (a_{ij}) n \times n a_{ii}=\sum_{j\neq i}\frac{x_i}{x_i-x_j}, a_{ij}=\frac{x_i}{x_i-x_j} i\neq j A=\begin{pmatrix}
\frac{x_1}{x_1-x_2} & \frac{x_1}{x_1-x_2}\\ 
\frac{x_2}{x_2-x_1} & \frac{x_2}{x_2-x_1}
\end{pmatrix} \left \{0,1,\ldots,n-1\right \} A-\lambda I 2 \times 2 3 \times 3 A k v_0 1 \leq i < j \leq n i \neq j \neq k v_i diag(x_1,...,x_n)^{i}v_0 n=4  v_0 = \begin{pmatrix}
1\\ 
-\frac{x_1^2 - x_1x_3 - (x_1 - x_3)x_4}{x_2^2 - x_2x_3 - (x_2 - x_3)x_4}\\ 
\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_4}{x_2x_3 - x_3^2 - (x_2 - x_3)x_4}\\ 
-\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3}{x_2x_3 - (x_2 + x_3)x_4 + x_4^2}
\end{pmatrix}, v_1 = \begin{pmatrix}
1\\ 
-\frac{x_1^2x_2 - x_1x_2x_3 - (x_1x_2 - x_2x_3)*x_4}{x_1x_2^2 - x_1x_2x_3 - (x_1x_2 - x_1x_3)x_4}\\ 
-\frac{(x_1 - x_2)x_3x_4 - (x_1^2 - x_1x_2)x_3}{x_1x_2x_3 - x_1x_3^2 - (x_1x_2 - x_1x_3)x_4}\\ 
-\frac{(x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4}{x_1x_2x_3 + x_1x_4^2 - (x_1x_2 + x_1x_3)x_4}
\end{pmatrix}, v_2 = \begin{pmatrix}
1\\ 
-\frac{x_1^2x_2^2 - x_1x_2^2x_3 - (x_1x_2^2 - x_2^2x_3)x_4}{x_1^2x_2^2 - x_1^2x_2x_3 - (x_1^2x_2 - x_1^2x_3)x_4}\\ 
-\frac{(x_1 - x_2)x_3^2x_4 - (x_1^2 - x_1x_2)x_3^2}{x_1^2x_2x_3 - x_1^2x_3^2 - (x_1^2x_2 - x_1^2x_3)x_4}\\ 
-\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4^2}{x_1^2x_2x_3 + x_1^2x_4^2 - (x_1^2x_2 + x_1^2x_3)x_4}
\end{pmatrix},
 v_3 = \begin{pmatrix}
1\\ 
-\frac{x_1^2x_2^3 - x_1x_2^3x_3 - (x_1x_2^3 - x_2^3x_3)x_4}{x_1^3x_2^2 - x_1^3x_2x_3 - (x_1^3x_2 - x_1^3x_3)x_4}\\ 
-\frac{(x_1 - x_2)x_3^3x_4 - (x_1^2 - x_1x_2)x_3^3}{x_1^3x_2x_3 - x_1^3x_3^2 - (x_1^3x_2 - x_1^3x_3)x_4}\\ 
-\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4^3}{x_1^3x_2x_3 + x_1^3x_4^2 - (x_1^3x_2 + x_1^3x_3)x_4}
\end{pmatrix} j eigenvalue=\lambda size=n (-1)^{j+1}\frac{x_j^\lambda}{x_1^\lambda} \prod_{k\neq 1,k\neq j}^n \frac{x_1-x_k}{x_j-x_k}
","['linear-algebra', 'abstract-algebra', 'eigenvalues-eigenvectors']"
58,determining if a coincident point in a pair of rotated hexagonal lattices is closest to the origin?,determining if a coincident point in a pair of rotated hexagonal lattices is closest to the origin?,,"A pair of hexagonal lattices with one scaled by the square root of a rational number $r = \sqrt{\frac{m}{n}}$ and then rotated will produce a variety of different hexagonal lattices of coincident points. For the first lattice let $$x, y = i+\frac{1}{2}j, \ \frac{\sqrt{3}}{2}j$$ and for the second $$x, y = r\left(k+\frac{1}{2}l\right), \ r\left(\frac{\sqrt{3}}{2}l\right).$$ Per this and this helpful answer the squares of the distances to unit lattice points are given by Loeschian numbers (A003136) equal to $i^2+ij+j^2$ so in this case a point $i, j$ on the first lattice will coincide with a point $k, l$ on the second lattice once rotated by some amount if $$n(i^2+ij+j^2) = m(k^2+kl+l^2).$$ For example if $m, n = 13, 7$ then both $(i, j) = (5, 6)$ and $(6, 5)$ will coincide with $(k, l) = (5, 3)$ at rotation angles of about 5.2 and 11.2 degrees as given by. $$\theta = \arctan\left( \frac{\frac{\sqrt{3}}{2}l}{k+\frac{1}{2}l} \right) - \arctan\left( \frac{\frac{\sqrt{3}}{2}j}{i+\frac{1}{2}j} \right)$$ However, while the first solution is part of the hexagonal superlattice built on the much closer point $(i, j), (k, l) = (1, 3), (1, 2)$ the second point represents the shortest possible coincident distance and therefore a far lower density coincident lattice . Question: Is there a simple test that can be applied to the pairs (5, 6), (3, 5) and (6, 5), (3, 5) (and knowing m, n) that will indicate immediately that one is based on a superlattice of much smaller period but the other represents the shortest distance in a much more sparse coincident lattice? This answer and this comment below it provide some related tests and might adapted here, but ideally I'm looking for a yes/no test that does not involving testing all points closer. plotting script: https://pastebin.com/6mwvudt6","A pair of hexagonal lattices with one scaled by the square root of a rational number and then rotated will produce a variety of different hexagonal lattices of coincident points. For the first lattice let and for the second Per this and this helpful answer the squares of the distances to unit lattice points are given by Loeschian numbers (A003136) equal to so in this case a point on the first lattice will coincide with a point on the second lattice once rotated by some amount if For example if then both and will coincide with at rotation angles of about 5.2 and 11.2 degrees as given by. However, while the first solution is part of the hexagonal superlattice built on the much closer point the second point represents the shortest possible coincident distance and therefore a far lower density coincident lattice . Question: Is there a simple test that can be applied to the pairs (5, 6), (3, 5) and (6, 5), (3, 5) (and knowing m, n) that will indicate immediately that one is based on a superlattice of much smaller period but the other represents the shortest distance in a much more sparse coincident lattice? This answer and this comment below it provide some related tests and might adapted here, but ideally I'm looking for a yes/no test that does not involving testing all points closer. plotting script: https://pastebin.com/6mwvudt6","r = \sqrt{\frac{m}{n}} x, y = i+\frac{1}{2}j, \ \frac{\sqrt{3}}{2}j x, y = r\left(k+\frac{1}{2}l\right), \ r\left(\frac{\sqrt{3}}{2}l\right). i^2+ij+j^2 i, j k, l n(i^2+ij+j^2) = m(k^2+kl+l^2). m, n = 13, 7 (i, j) = (5, 6) (6, 5) (k, l) = (5, 3) \theta = \arctan\left( \frac{\frac{\sqrt{3}}{2}l}{k+\frac{1}{2}l} \right) - \arctan\left( \frac{\frac{\sqrt{3}}{2}j}{i+\frac{1}{2}j} \right) (i, j), (k, l) = (1, 3), (1, 2)","['linear-algebra', 'geometry']"
59,“Geometric” problems on the Jordan normal form of a particular operator,“Geometric” problems on the Jordan normal form of a particular operator,,"Assume you have a class of students more or less familiar with the notion of the matrix of a linear operator. They have seen and calculated lots of examples in various context: geometric transformations (rotations, reflections, scaling along axes, ...), operators on polynomials (derivation), number-theoretic ($\mathbb{C}^n\to\mathbb{C}^n$, linear over $\mathbb{R}$ but not over $\mathbb{C}$). In the study of the Jordan normal form the basic problem is to find the canonical form and a Jordan basis of an operator. The algorithm one usually gives to the students starts with the line “pick a basis and find the matrix of the given operator with respect to this basis”. But then we give the students a problem of the form “given a matrix , find its canonical form and a Jordan basis”. Now I would very much like to force the students to calculate the Jordan form of an operator , so they would pick a basis themselves, find the corresponding matrix, find the Jordan basis and then express it not as a set of columns of numbers, but as elements of the vector space in question. This needs a couple of examples, here they are: $V=\mathbb{C}^2$, the operator is $A\colon\begin{pmatrix}x\\y\end{pmatrix}\mapsto\begin{pmatrix}\overline{x}-\operatorname{Re}(y)\\(1+i)\cdot\operatorname{Im}(x)-y\end{pmatrix}$. The natural $\mathbb{R}$-basis is $$\begin{pmatrix}1\\0\end{pmatrix},\ \begin{pmatrix}i\\0\end{pmatrix},\ \begin{pmatrix}0\\1\end{pmatrix},\ \begin{pmatrix}0\\i\end{pmatrix},$$ the matrix of $A$ is $$\begin{pmatrix}1&0&-1&0\\0&-1&0&0\\0&1&-1&0\\0&1&0&-1\end{pmatrix},$$ the JNF is $\operatorname{diag}(1,J_2(-1),-1)$, and the Jordan basis is, for example, $$\begin{pmatrix}-1\\0\end{pmatrix},\ \begin{pmatrix}2\\4+4i\end{pmatrix},\ \begin{pmatrix}1+4i\\4i\end{pmatrix},\ \begin{pmatrix}0\\4i\end{pmatrix}.$$ $V=\mathbb{R}[t]_{\leqslant4}$, the space of polynomials of degree at most 4, and the operator if $f\mapsto f'+f(0)+f'(0)$. The Jordan basis in this case is a set of polynomials. The two examples above are not very interesting in terms of the calculating the JNF (few small blocks, distinct eigenvalues), but this can be easily fixed. But I find it pretty hard to invent a problem of this sort which have a geometric origin (transformations in, say 4- or 5-dimensional Euclidean space). Most of the transformations I can describe in simple geometric terms (rotations, reflections, projections) are either diagonalizable, or have imaginary eigenvalues (so it is impossible to get back form the coordinate columns to points in space), or both. Is there a way to construct a “geometric” problem on the computation of the JNF? Since there must be other contexts similar to the three described above, what are the interesting problems on the computation of the JNF of a particular operator? To clarify this second question, I am well-aware of the problems of the sort “one knows the characteristic and minimal polynomials, the rank of the square and the maximal number of linearly independent eigenvectors, find the JNF”. Apart from the use in the class in order for the students to recall the notion of the matrix of an operator, this can also be very useful in an online course with automated assignment check.","Assume you have a class of students more or less familiar with the notion of the matrix of a linear operator. They have seen and calculated lots of examples in various context: geometric transformations (rotations, reflections, scaling along axes, ...), operators on polynomials (derivation), number-theoretic ($\mathbb{C}^n\to\mathbb{C}^n$, linear over $\mathbb{R}$ but not over $\mathbb{C}$). In the study of the Jordan normal form the basic problem is to find the canonical form and a Jordan basis of an operator. The algorithm one usually gives to the students starts with the line “pick a basis and find the matrix of the given operator with respect to this basis”. But then we give the students a problem of the form “given a matrix , find its canonical form and a Jordan basis”. Now I would very much like to force the students to calculate the Jordan form of an operator , so they would pick a basis themselves, find the corresponding matrix, find the Jordan basis and then express it not as a set of columns of numbers, but as elements of the vector space in question. This needs a couple of examples, here they are: $V=\mathbb{C}^2$, the operator is $A\colon\begin{pmatrix}x\\y\end{pmatrix}\mapsto\begin{pmatrix}\overline{x}-\operatorname{Re}(y)\\(1+i)\cdot\operatorname{Im}(x)-y\end{pmatrix}$. The natural $\mathbb{R}$-basis is $$\begin{pmatrix}1\\0\end{pmatrix},\ \begin{pmatrix}i\\0\end{pmatrix},\ \begin{pmatrix}0\\1\end{pmatrix},\ \begin{pmatrix}0\\i\end{pmatrix},$$ the matrix of $A$ is $$\begin{pmatrix}1&0&-1&0\\0&-1&0&0\\0&1&-1&0\\0&1&0&-1\end{pmatrix},$$ the JNF is $\operatorname{diag}(1,J_2(-1),-1)$, and the Jordan basis is, for example, $$\begin{pmatrix}-1\\0\end{pmatrix},\ \begin{pmatrix}2\\4+4i\end{pmatrix},\ \begin{pmatrix}1+4i\\4i\end{pmatrix},\ \begin{pmatrix}0\\4i\end{pmatrix}.$$ $V=\mathbb{R}[t]_{\leqslant4}$, the space of polynomials of degree at most 4, and the operator if $f\mapsto f'+f(0)+f'(0)$. The Jordan basis in this case is a set of polynomials. The two examples above are not very interesting in terms of the calculating the JNF (few small blocks, distinct eigenvalues), but this can be easily fixed. But I find it pretty hard to invent a problem of this sort which have a geometric origin (transformations in, say 4- or 5-dimensional Euclidean space). Most of the transformations I can describe in simple geometric terms (rotations, reflections, projections) are either diagonalizable, or have imaginary eigenvalues (so it is impossible to get back form the coordinate columns to points in space), or both. Is there a way to construct a “geometric” problem on the computation of the JNF? Since there must be other contexts similar to the three described above, what are the interesting problems on the computation of the JNF of a particular operator? To clarify this second question, I am well-aware of the problems of the sort “one knows the characteristic and minimal polynomials, the rank of the square and the maximal number of linearly independent eigenvectors, find the JNF”. Apart from the use in the class in order for the students to recall the notion of the matrix of an operator, this can also be very useful in an online course with automated assignment check.",,"['linear-algebra', 'education', 'jordan-normal-form']"
60,Upper bounding the Frobenius norm of the inverse of a positive-definite symmetric matrix,Upper bounding the Frobenius norm of the inverse of a positive-definite symmetric matrix,,"Let $\Sigma$ be a symmetric positive-definite $n \times n$ matrix.  I want an upper bound on the Frobenius norm of $\Sigma^{-1}$ that does not involve calculating the determinant of $\Sigma$ .  The Frobenius norm for a generic $n\times n$ matrix $A$ with typical element $a_{ij}$ is $\|A\|_F = \sqrt{\sum_{i=1}^n\sum_{j=1}^n a_{ij}^2}\,.$ I am looking for an upper bound on $\|\Sigma^{-1}\|_F$ that is a function of the elements of $\Sigma$ . Here's what I have so far.  Let $\lambda_i$ be the eigenvalues of $\Sigma$ and $\sigma_{ij}$ be a typical element of $\Sigma$ . $\|\Sigma^{-1}\|_{F} = \sqrt{\text{tr}\left\{ \left(\Sigma^{-1}\right)^{\top}\Sigma^{-1}\right\}} = \sqrt{\text{tr}\left\{ \Sigma^{-1} \Sigma^{-1}\right\}} = \sqrt{\sum_{i=1}^{n} \frac{1}{\lambda_{i}^{2}}} \le \sum_{i=1}^{n} \frac{1}{\lambda_{i}} = \text{tr}\left\{\Sigma^{-1}\right\}$ . A few things that would work: A lower bound on the minimal eigenvalue or an upper bound on the trace of the inverse of the matrix. In this case I have found the following bound on the minimal eigenvalue: $\lambda_{\text{min}} \ge \left(\frac{n-1}{\text{tr}\left\{\Sigma\right\}}\right)^{n-1} \times \det \Sigma$ . But it uses the determinant which is going to be tough to handle in my problem.  It'd be great to get an upper bound on $\|\Sigma^{-1}\|_{F}$ that uses the trace of $\Sigma$ , and/or $\|\Sigma\|_{F}$ . I know a lower bound because it's submultiplicative: $\|I\|_{F} \le \left\|\Sigma^{-1}\right\|_{F} \| \Sigma \|_{F}$ .","Let be a symmetric positive-definite matrix.  I want an upper bound on the Frobenius norm of that does not involve calculating the determinant of .  The Frobenius norm for a generic matrix with typical element is I am looking for an upper bound on that is a function of the elements of . Here's what I have so far.  Let be the eigenvalues of and be a typical element of . . A few things that would work: A lower bound on the minimal eigenvalue or an upper bound on the trace of the inverse of the matrix. In this case I have found the following bound on the minimal eigenvalue: . But it uses the determinant which is going to be tough to handle in my problem.  It'd be great to get an upper bound on that uses the trace of , and/or . I know a lower bound because it's submultiplicative: .","\Sigma n \times n \Sigma^{-1} \Sigma n\times n A a_{ij} \|A\|_F = \sqrt{\sum_{i=1}^n\sum_{j=1}^n a_{ij}^2}\,. \|\Sigma^{-1}\|_F \Sigma \lambda_i \Sigma \sigma_{ij} \Sigma \|\Sigma^{-1}\|_{F} = \sqrt{\text{tr}\left\{ \left(\Sigma^{-1}\right)^{\top}\Sigma^{-1}\right\}} = \sqrt{\text{tr}\left\{ \Sigma^{-1} \Sigma^{-1}\right\}} = \sqrt{\sum_{i=1}^{n} \frac{1}{\lambda_{i}^{2}}} \le \sum_{i=1}^{n} \frac{1}{\lambda_{i}} = \text{tr}\left\{\Sigma^{-1}\right\} \lambda_{\text{min}} \ge \left(\frac{n-1}{\text{tr}\left\{\Sigma\right\}}\right)^{n-1} \times \det \Sigma \|\Sigma^{-1}\|_{F} \Sigma \|\Sigma\|_{F} \|I\|_{F} \le \left\|\Sigma^{-1}\right\|_{F} \| \Sigma \|_{F}","['linear-algebra', 'matrices']"
61,Properties shared by similar and unitary similar matrices.,Properties shared by similar and unitary similar matrices.,,"We know that matrices $A$ and $B$ are similar if there exists an invertible matrix $P$ such that $A=PBP^{-1}$ and they are unitarily similar if $P$ is unitary ($PP^*=P^*P=I$). I want to know : What are the properties of the matrix that are preserved by these transformations ? . I'm a little confident of a few properties, but there are some I'm not sure of. What I need is : 1) Answers to those places which I not sure of and corrections for existing answers. 2) Interpretation for the differences in similarity and unitary similarity. 3) Other properties I might have missed out here. Thanks a lot. \begin{array}{ccc}&Property&Similarity & Unitary \ Similarity   \\\hline 1.&Characteristic \ polynomial&Yes&Yes \\2.&Eigenvalues&Yes&Yes \\3.&Geometric \ multiplicity \ of \ eigenvalues&Yes&Yes \\4.&Elementary \ divisors \ and \ Rational \ canonical \ form&Yes&Yes \\5.&Positive\ definiteness&Yes&Yes \\6.& Matrix \ 2-Norm&Yes&Yes \\7.&Symmetry&No&Yes \\8.&SkewSymmetry&No&Yes \\9.&Orthogonality&No&Yes \\10.&Frobenius\ Norm&No&No \\11.&1-norm  \ and  \ \infty- norm&No&No \\12.&Unitary \ Diagnalizability& No& Not\ sure \\13.&Nilpotency&Not \ sure& Not \ sure \\14.&Solution \ to \ Ax=b&Not \ sure& Not \ sure \\15.&Minimal \ polynomial&Not \ sure&Not \ sure \\16.&Diagonalizability&Not \ sure& Not \ sure \\17.&Rank &Not\ sure& Not\ sure \\18.&Singular\ Values&Not \ Sure&Not \ Sure \\19.&Condition\ Number&Not \ Sure&Not \ Sure \end{array}","We know that matrices $A$ and $B$ are similar if there exists an invertible matrix $P$ such that $A=PBP^{-1}$ and they are unitarily similar if $P$ is unitary ($PP^*=P^*P=I$). I want to know : What are the properties of the matrix that are preserved by these transformations ? . I'm a little confident of a few properties, but there are some I'm not sure of. What I need is : 1) Answers to those places which I not sure of and corrections for existing answers. 2) Interpretation for the differences in similarity and unitary similarity. 3) Other properties I might have missed out here. Thanks a lot. \begin{array}{ccc}&Property&Similarity & Unitary \ Similarity   \\\hline 1.&Characteristic \ polynomial&Yes&Yes \\2.&Eigenvalues&Yes&Yes \\3.&Geometric \ multiplicity \ of \ eigenvalues&Yes&Yes \\4.&Elementary \ divisors \ and \ Rational \ canonical \ form&Yes&Yes \\5.&Positive\ definiteness&Yes&Yes \\6.& Matrix \ 2-Norm&Yes&Yes \\7.&Symmetry&No&Yes \\8.&SkewSymmetry&No&Yes \\9.&Orthogonality&No&Yes \\10.&Frobenius\ Norm&No&No \\11.&1-norm  \ and  \ \infty- norm&No&No \\12.&Unitary \ Diagnalizability& No& Not\ sure \\13.&Nilpotency&Not \ sure& Not \ sure \\14.&Solution \ to \ Ax=b&Not \ sure& Not \ sure \\15.&Minimal \ polynomial&Not \ sure&Not \ sure \\16.&Diagonalizability&Not \ sure& Not \ sure \\17.&Rank &Not\ sure& Not\ sure \\18.&Singular\ Values&Not \ Sure&Not \ Sure \\19.&Condition\ Number&Not \ Sure&Not \ Sure \end{array}",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
62,The Hodge $*$-operator and the wedge product,The Hodge -operator and the wedge product,*,"On every Riemannian manifold $M$, we can consider the Hodge $*$-operator, which is characterised by the following formula: $$a \wedge *b = (a,b)\nu.$$ Here $a$ and $b$ are smooth forms on $M$, $(\ ,\ )$ is a metric on $\wedge T^*\!M$ and $\nu$ is the volume form with respect to the Riemannian metric. My question: Is a formula of $*(a \wedge b)$ known? I suspect that we can have a formula like ""$*(a \wedge b)=(*a)\wedge(*b)$"" or ""$*(a \wedge b) = *a \wedge b \pm a \wedge *b$"". Of course these formulae never hold. (Look at the degree.)","On every Riemannian manifold $M$, we can consider the Hodge $*$-operator, which is characterised by the following formula: $$a \wedge *b = (a,b)\nu.$$ Here $a$ and $b$ are smooth forms on $M$, $(\ ,\ )$ is a metric on $\wedge T^*\!M$ and $\nu$ is the volume form with respect to the Riemannian metric. My question: Is a formula of $*(a \wedge b)$ known? I suspect that we can have a formula like ""$*(a \wedge b)=(*a)\wedge(*b)$"" or ""$*(a \wedge b) = *a \wedge b \pm a \wedge *b$"". Of course these formulae never hold. (Look at the degree.)",,"['linear-algebra', 'differential-geometry', 'exterior-algebra']"
63,Determinant game - winning strategy [duplicate],Determinant game - winning strategy [duplicate],,"This question already has answers here : Alice and Bob play the determinant game (6 answers) Closed 5 years ago . I came across this problem while looking at Putnam problems a while ago: Alan and Barbara play a game in which they take turns filling entries of an initially empty $2008 \times 2008$ array. Alan plays first. At each turn, a player chooses a real number and places it in a vacant entry. The game ends when all the entries are filled. Alan wins if the determinant of the resulting matrix is nonzero; Barbara wins if it is zero. Which player has a winning strategy? I figured out the answer to the question they asked, but it made me wonder about a different question, which might not have an easy answer. For the original question, Barbara has a winning strategy. She can force the first two columns of the matrix to be the same, by copying whatever Alan plays in one column into the other column. My question is: can Barbara still win if she goes first instead of Alan? She can't use the same strategy as before, because now Alan can force her to play in the first two columns by filling up the rest of the grid. It might seem likely that Alan would have a winning strategy if Barbara went first, since he gets to control the last entry of the matrix. On the other hand, from sketching out a few games on paper, I concluded that Barbara could win whether she went first or second on both a $2 \times 2$ or a $3 \times 3$ matrix, so maybe that pattern continues to hold for larger matrices. I also sketched out a few games with a $4 \times 4$ matrix, but I wasn't able to come to a conclusion for that one. Edit: Maybe the $2008 \times 2008$ array is too large of a problem. I would be equally happy knowing the answer to the $4 \times 4$ problem: is there a winning strategy if Barbara goes first in a $4 \times 4$ array? Edit 2: It looks like my question was asked before: Alice and Bob matrix problem.","This question already has answers here : Alice and Bob play the determinant game (6 answers) Closed 5 years ago . I came across this problem while looking at Putnam problems a while ago: Alan and Barbara play a game in which they take turns filling entries of an initially empty $2008 \times 2008$ array. Alan plays first. At each turn, a player chooses a real number and places it in a vacant entry. The game ends when all the entries are filled. Alan wins if the determinant of the resulting matrix is nonzero; Barbara wins if it is zero. Which player has a winning strategy? I figured out the answer to the question they asked, but it made me wonder about a different question, which might not have an easy answer. For the original question, Barbara has a winning strategy. She can force the first two columns of the matrix to be the same, by copying whatever Alan plays in one column into the other column. My question is: can Barbara still win if she goes first instead of Alan? She can't use the same strategy as before, because now Alan can force her to play in the first two columns by filling up the rest of the grid. It might seem likely that Alan would have a winning strategy if Barbara went first, since he gets to control the last entry of the matrix. On the other hand, from sketching out a few games on paper, I concluded that Barbara could win whether she went first or second on both a $2 \times 2$ or a $3 \times 3$ matrix, so maybe that pattern continues to hold for larger matrices. I also sketched out a few games with a $4 \times 4$ matrix, but I wasn't able to come to a conclusion for that one. Edit: Maybe the $2008 \times 2008$ array is too large of a problem. I would be equally happy knowing the answer to the $4 \times 4$ problem: is there a winning strategy if Barbara goes first in a $4 \times 4$ array? Edit 2: It looks like my question was asked before: Alice and Bob matrix problem.",,"['linear-algebra', 'matrices', 'determinant', 'combinatorial-game-theory']"
64,Linear algebra and arbitrary fields,Linear algebra and arbitrary fields,,"The linear algebra course that I took was fairly consistent about assuming that the scalar field is either the reals or the complex numbers. The theory about linear maps, basis, their matrices, eigenvalues and eigenvectors, trace and determinant clearly generalize to a general field without any changes. Similarly, the Jordan canonical form only seems to require algebraic closedness of a field. The definition of an inner-product seems to explicitly require either the reals or the complex numbers, but even then one should be able to replace it by a bilinear pairing $V\times V\to \mathbb{F}$, where $\mathbb{F}$ is our field. However, why do we then want conjugate symmetry in the complex case? Do we need something similar for fields which have a ""similar"" automorphism? Is there a precise way to formalize this? My questions is essentially the following: How can we generalize spectral theorems to general fields? What would the results look like and what do we need to assume? What's the right way to generalize inner-product spaces and what can we translate unchanged from the setting of an undergraduate linear algebra class?","The linear algebra course that I took was fairly consistent about assuming that the scalar field is either the reals or the complex numbers. The theory about linear maps, basis, their matrices, eigenvalues and eigenvectors, trace and determinant clearly generalize to a general field without any changes. Similarly, the Jordan canonical form only seems to require algebraic closedness of a field. The definition of an inner-product seems to explicitly require either the reals or the complex numbers, but even then one should be able to replace it by a bilinear pairing $V\times V\to \mathbb{F}$, where $\mathbb{F}$ is our field. However, why do we then want conjugate symmetry in the complex case? Do we need something similar for fields which have a ""similar"" automorphism? Is there a precise way to formalize this? My questions is essentially the following: How can we generalize spectral theorems to general fields? What would the results look like and what do we need to assume? What's the right way to generalize inner-product spaces and what can we translate unchanged from the setting of an undergraduate linear algebra class?",,['linear-algebra']
65,What do adjoints have to do with this problem?,What do adjoints have to do with this problem?,,"Question: Let $V\ $ be the vector space of the polynomials over $\mathbf{R}$ of degree less than or equal to 3, with the inner product $$ (f|g) = \int_0^1 f(t)g(t) dt. $$ If $t$ is a real number, find the polynomial $g_t$ in $V$ such that $(f|g_t) = f(t)$ for all $f$ in $V$. My Attempt: The way I thought to do it was,  let $f(x) = a_0 + a_1x + a_2x^2 + a_3x^3$ and $g_t(x) = b_0 + b_1x + b_2x^2 + b_3x^3$. $$(f|g_t) = \sum_{j, k} \frac{1}{1 + j + k} a_j b_k $$ Since $(f|g_t) = f(t)$, I get $$t^j = \sum_k \frac{1}{1 + j + k}b_k.$$ Let $A$ be the matrix $A_{kj} = \frac{1}{1 + j + k}$, so $$ (b_0, b_1, b_2, b_3)A = (1, t, t^2, t^3) $$ Thus $$(b_0, b_1, b_2, b_3) = (1, t, t^2, t^3)A^{-1}.$$ I can compute $A^{-1}$ and that would give me the answer, I think, but it seems like a lot of work, and I would not be using any of the information from the chapter to solve it. I am assuming there is a lot easier way to do this. The chapter is called ""Linear Functionals and Adjoints"" from Linear Algebra by Hoffman and Kunze. EDIT : I think the way the chapter wanted me to do this was the following. Find an orthonormal basis using Gram Schmidt, say $f_1, f_2, f_3, f_4$. Then let $L_t(f) = f(t)$. We can then let $$g_t = L_t(f_1)f_1 + L_t(f_2)f_2 + L_t(f_3)f_3 + L_t(f_4)f_4.$$ Then say $f = a_1f_1 + a_2f_2 + a_3f_3 + a_4f_4$. $$ \begin{align*} (f| g_t) &= a_1L_t(f_1)(f_1| f_1) + a_2L_t(f_2)(f_2| f_2) + a_3L_t(f_3)(f_3| f_3) + a_4L_t(f_4)(f_4| f_4) \\ &= L_t(a_1f_1 + a_2f_2 + a_3f_3 + a_4f_4) = L_t(f) = f(t). \end{align*}$$ The computation is still more than I want to do, but the ideas are all there. I guess this was more focused on the linear functional part of the chapter, instead of the adjoint part.","Question: Let $V\ $ be the vector space of the polynomials over $\mathbf{R}$ of degree less than or equal to 3, with the inner product $$ (f|g) = \int_0^1 f(t)g(t) dt. $$ If $t$ is a real number, find the polynomial $g_t$ in $V$ such that $(f|g_t) = f(t)$ for all $f$ in $V$. My Attempt: The way I thought to do it was,  let $f(x) = a_0 + a_1x + a_2x^2 + a_3x^3$ and $g_t(x) = b_0 + b_1x + b_2x^2 + b_3x^3$. $$(f|g_t) = \sum_{j, k} \frac{1}{1 + j + k} a_j b_k $$ Since $(f|g_t) = f(t)$, I get $$t^j = \sum_k \frac{1}{1 + j + k}b_k.$$ Let $A$ be the matrix $A_{kj} = \frac{1}{1 + j + k}$, so $$ (b_0, b_1, b_2, b_3)A = (1, t, t^2, t^3) $$ Thus $$(b_0, b_1, b_2, b_3) = (1, t, t^2, t^3)A^{-1}.$$ I can compute $A^{-1}$ and that would give me the answer, I think, but it seems like a lot of work, and I would not be using any of the information from the chapter to solve it. I am assuming there is a lot easier way to do this. The chapter is called ""Linear Functionals and Adjoints"" from Linear Algebra by Hoffman and Kunze. EDIT : I think the way the chapter wanted me to do this was the following. Find an orthonormal basis using Gram Schmidt, say $f_1, f_2, f_3, f_4$. Then let $L_t(f) = f(t)$. We can then let $$g_t = L_t(f_1)f_1 + L_t(f_2)f_2 + L_t(f_3)f_3 + L_t(f_4)f_4.$$ Then say $f = a_1f_1 + a_2f_2 + a_3f_3 + a_4f_4$. $$ \begin{align*} (f| g_t) &= a_1L_t(f_1)(f_1| f_1) + a_2L_t(f_2)(f_2| f_2) + a_3L_t(f_3)(f_3| f_3) + a_4L_t(f_4)(f_4| f_4) \\ &= L_t(a_1f_1 + a_2f_2 + a_3f_3 + a_4f_4) = L_t(f) = f(t). \end{align*}$$ The computation is still more than I want to do, but the ideas are all there. I guess this was more focused on the linear functional part of the chapter, instead of the adjoint part.",,['linear-algebra']
66,Is there a geometric proof that the determinant of a 3x3 matrix is invariant under switching rows and columns?,Is there a geometric proof that the determinant of a 3x3 matrix is invariant under switching rows and columns?,,"A basic fact about $3$ dimensional vectors is that the quantity $\pm\det\left( \begin{array}{ccc} 	 a_{1} & a_{2} & a_{3} \\ b_{1} & b_{2} & b_{3} \\ c_{1} & c_{2} & c_{3} \\  \end{array}  \right)$ is equal to the volume of the parallelepiped determined by the vectors $\vec{a}$, $\vec{b}$ and $\vec{c}$ where  $\vec{a} = \langle a_1, a_2, a_3 \rangle$, $\vec{b} = \langle b_1, b_2, b_3\rangle$, and $\vec{c} = \langle c_1, c_2, c_3\rangle$. From e.g. the explicit formula for the determinant of a matrix in terms of its entries it is evident that this is the same as $\pm\det\left( \begin{array}{ccc} 	 a_{1} & b_{1} & c_{1} \\ a_{2} & b_{2} & c_{2} \\ a_{3} & b_{3} & c_{3} \\  \end{array}  \right)$ so that the volume of of the parallelepiped determined by $\vec{a}$, $\vec{b}$ and $\vec{c}$ is the same as the the volume of parallelepiped determined by $\langle a_1, b_1, c_1\rangle$, $\langle a_2, b_2, c_2\rangle$, and $\langle a_3, b_3, c_3\rangle$. My question is: Is there a geometric proof that the volumes of the two parallelepipeds are the same?","A basic fact about $3$ dimensional vectors is that the quantity $\pm\det\left( \begin{array}{ccc} 	 a_{1} & a_{2} & a_{3} \\ b_{1} & b_{2} & b_{3} \\ c_{1} & c_{2} & c_{3} \\  \end{array}  \right)$ is equal to the volume of the parallelepiped determined by the vectors $\vec{a}$, $\vec{b}$ and $\vec{c}$ where  $\vec{a} = \langle a_1, a_2, a_3 \rangle$, $\vec{b} = \langle b_1, b_2, b_3\rangle$, and $\vec{c} = \langle c_1, c_2, c_3\rangle$. From e.g. the explicit formula for the determinant of a matrix in terms of its entries it is evident that this is the same as $\pm\det\left( \begin{array}{ccc} 	 a_{1} & b_{1} & c_{1} \\ a_{2} & b_{2} & c_{2} \\ a_{3} & b_{3} & c_{3} \\  \end{array}  \right)$ so that the volume of of the parallelepiped determined by $\vec{a}$, $\vec{b}$ and $\vec{c}$ is the same as the the volume of parallelepiped determined by $\langle a_1, b_1, c_1\rangle$, $\langle a_2, b_2, c_2\rangle$, and $\langle a_3, b_3, c_3\rangle$. My question is: Is there a geometric proof that the volumes of the two parallelepipeds are the same?",,['linear-algebra']
67,A (new?) proof of the classification of continuous morphisms from $\mathbb{S}$ to $\text{GL}_n (\mathbb{R})$. Reference request :),A (new?) proof of the classification of continuous morphisms from  to . Reference request :),\mathbb{S} \text{GL}_n (\mathbb{R}),"About the classification of continuous group morphisms from $(\mathbb{S},\times)$ the circle of elements of module $1$ in $\mathbb{C}$ to $\text{GL}_n (\mathbb{R})$ which is a well known problem, I prove it without reference and had a completely different proof than the usual proof using (I believe) classification of morphisms from $\mathbb{R}$ to $\text{GL}_n ( \mathbb{R})$ with exponentials. EDIT: I realized this is a well known result for student that aim the ENS (French very hard to get math school), I have the original reference , but it is in French (sorry guys) here are all the references that I know about S. Francinou, H. Gianella, S. Nicolas, Exercices de mathématiques, Oraux X-ENS, Algèbre 2, 2e édition, Cassini. Exercise 4.29 page 251. A PDF written by a fellow student that does the exact same proof (it follows from the book). However, I did not find any reference that use my proof or something analogous. My question is (since it has very probably been done) do you have one reference? The proof uses various mechanisms and I find it interesting. If my question is off-topic, please tell me, I'll close the subject. Here is the theorem: Theorem : Let $\varphi: \mathbb{S} \longrightarrow \text{GL}_n (\mathbb{R})$ be a continuous group morphism, then there exist $r \leqslant \frac{n}{2}$ , $m_1, \dots m_r \in \mathbb{Z}$ , $P \in \text{GL}_n (\mathbb{R})$ such that: $$\forall \theta \in \mathbb{R}, \qquad \varphi(e^{i\theta}) = P \begin{pmatrix} R(m_1 \theta) \\ & \ddots \\ && R(m_r \theta) \\ &&&I_{n-2r} \end{pmatrix} P^{-1}$$ With $R(\theta)$ the rotation matrix $\begin{pmatrix} \cos(\theta) &\sin(\theta)\\ -\sin(\theta) & \cos (\theta) \end{pmatrix}$ Some intuition about the theorem in itself: $\varphi$ stabilizes planes in $\mathbb{R}^n$ and rotates them as you would rotate $\mathbb{R}^2$ , the more you move on the circle, the more the planes rotate. For the rest of the space, it has to be trivial on it. Here is the proof in multiple steps: Prove that every continuous group endomorphism $\varphi$ of $\mathbb{S}$ is of the form $$z \longmapsto z^m$$ for some $m \in \mathbb{Z}$ . To do so, I apply the lifting path theorem to $t \mapsto \varphi(e^{it})$ and get a continuous endomorphism of $\mathbb{R}$ , $\phi$ . These morphisms are well known and of the form $t \mapsto at$ . A quick study gives $a \in \mathbb{Z}$ which finishes the proof. We get into the deep and fix a goal: use codiagonalisation on the image. For that we already have commutativity since the source group is abelian, we just need to show diagonalisation. Let us take $z \in \mathbb{S}$ of finite order, i.e. $$z \in \bigcup_{n \in \mathbb{N}^*} U_n = U_\infty .$$ Then there exist $n$ for which $z^n = 1$ , so $\varphi(z)^n = I_n$ is idempotent, thus diagonalisable over $\mathbb{C}$ with spectrum in $\mathbb{S}$ . So the image of $U_\infty$ by $\varphi$ is codiagonalisable, which means there exists $ Q \in \text{GL}_n ( \mathbb{C} ) $ such that $$ \varphi_{ | U_\infty } (z) = Q \begin{pmatrix} \tilde{\lambda_1} (z) \\ & \ddots \\ && \tilde{\lambda_n} (z) \end{pmatrix} Q^{-1}. $$ with $\tilde{\lambda_i}$ continuous (hence uniformly continuous) functions from $ U_\infty $ to $ \mathbb{S} $ . Thanks to the theorem of extension for uniformly continuous mappings, we extend each $\lambda$ and $\varphi_{|\mathbb{U}_\infty}$ and by unicity, we obtain: $$\varphi (z) = Q \begin{pmatrix} \lambda_1 (z) \\ & \ddots \\ && \lambda_n (z) \end{pmatrix} Q^{-1}.$$ Obviously, the $\lambda_i$ are continuous endormorphism of the circle, thus there exists $k_1, \dots , k_n \in \mathbb{Z}$ such that $$\varphi (z) = Q \begin{pmatrix} z^{k_1} \\ & \ddots \\ && z^{k_n} \end{pmatrix} Q^{-1}.$$ By a quick study of the characteristic polynomial of $\varphi \left( e^{i \frac{1}{ \max_{k_i \neq 0} k_i}} \right) $ which does not depend on the field extension thus it has real coefficients, we can reorganize the $k_i$ in $(m_1, -m_1, m_2, -m_2,\dots, m_r,-m_r, 0, \dots 0)$ . It is common knowledge that these two matrices are $\mathbb{C}$ -conjuguate $$\begin{pmatrix} e^{i\theta} & 0 \\ 0 & e^{-i \theta} \end{pmatrix} \sim R(\theta)$$ We get then $$ \varphi(e^{i\theta}) = P' \begin{pmatrix} R(m_1 \theta) \\ & \ddots \\ && R(m_r \theta) \\ &&&I_{n-2r} \end{pmatrix} P'^{-1}$$ with $P' \in \text{GL}_n ( \mathbb{C} ) $ , but two real $\mathbb{C}$ -conjuguate matrices are $\mathbb{R}$ -conjuguate, which proves the result.","About the classification of continuous group morphisms from the circle of elements of module in to which is a well known problem, I prove it without reference and had a completely different proof than the usual proof using (I believe) classification of morphisms from to with exponentials. EDIT: I realized this is a well known result for student that aim the ENS (French very hard to get math school), I have the original reference , but it is in French (sorry guys) here are all the references that I know about S. Francinou, H. Gianella, S. Nicolas, Exercices de mathématiques, Oraux X-ENS, Algèbre 2, 2e édition, Cassini. Exercise 4.29 page 251. A PDF written by a fellow student that does the exact same proof (it follows from the book). However, I did not find any reference that use my proof or something analogous. My question is (since it has very probably been done) do you have one reference? The proof uses various mechanisms and I find it interesting. If my question is off-topic, please tell me, I'll close the subject. Here is the theorem: Theorem : Let be a continuous group morphism, then there exist , , such that: With the rotation matrix Some intuition about the theorem in itself: stabilizes planes in and rotates them as you would rotate , the more you move on the circle, the more the planes rotate. For the rest of the space, it has to be trivial on it. Here is the proof in multiple steps: Prove that every continuous group endomorphism of is of the form for some . To do so, I apply the lifting path theorem to and get a continuous endomorphism of , . These morphisms are well known and of the form . A quick study gives which finishes the proof. We get into the deep and fix a goal: use codiagonalisation on the image. For that we already have commutativity since the source group is abelian, we just need to show diagonalisation. Let us take of finite order, i.e. Then there exist for which , so is idempotent, thus diagonalisable over with spectrum in . So the image of by is codiagonalisable, which means there exists such that with continuous (hence uniformly continuous) functions from to . Thanks to the theorem of extension for uniformly continuous mappings, we extend each and and by unicity, we obtain: Obviously, the are continuous endormorphism of the circle, thus there exists such that By a quick study of the characteristic polynomial of which does not depend on the field extension thus it has real coefficients, we can reorganize the in . It is common knowledge that these two matrices are -conjuguate We get then with , but two real -conjuguate matrices are -conjuguate, which proves the result.","(\mathbb{S},\times) 1 \mathbb{C} \text{GL}_n (\mathbb{R}) \mathbb{R} \text{GL}_n ( \mathbb{R}) \varphi: \mathbb{S} \longrightarrow \text{GL}_n (\mathbb{R}) r \leqslant \frac{n}{2} m_1, \dots m_r \in \mathbb{Z} P \in \text{GL}_n (\mathbb{R}) \forall \theta \in \mathbb{R}, \qquad \varphi(e^{i\theta}) =
P
\begin{pmatrix}
R(m_1 \theta) \\
& \ddots \\
&& R(m_r \theta) \\
&&&I_{n-2r} \end{pmatrix}
P^{-1} R(\theta) \begin{pmatrix} \cos(\theta) &\sin(\theta)\\ -\sin(\theta) & \cos (\theta) \end{pmatrix} \varphi \mathbb{R}^n \mathbb{R}^2 \varphi \mathbb{S} z \longmapsto z^m m \in \mathbb{Z} t \mapsto \varphi(e^{it}) \mathbb{R} \phi t \mapsto at a \in \mathbb{Z} z \in \mathbb{S} z \in \bigcup_{n \in \mathbb{N}^*} U_n = U_\infty . n z^n = 1 \varphi(z)^n = I_n \mathbb{C} \mathbb{S} U_\infty \varphi  Q \in \text{GL}_n ( \mathbb{C} )   \varphi_{ | U_\infty } (z) = Q \begin{pmatrix}
\tilde{\lambda_1} (z) \\
& \ddots \\
&& \tilde{\lambda_n} (z)
\end{pmatrix} Q^{-1}.  \tilde{\lambda_i}  U_\infty   \mathbb{S}  \lambda \varphi_{|\mathbb{U}_\infty} \varphi (z) = Q
\begin{pmatrix}
\lambda_1 (z) \\
& \ddots \\
&& \lambda_n (z)
\end{pmatrix} Q^{-1}. \lambda_i k_1, \dots , k_n \in \mathbb{Z} \varphi (z) = Q
\begin{pmatrix}
z^{k_1} \\
& \ddots \\
&& z^{k_n}
\end{pmatrix} Q^{-1}. \varphi \left( e^{i \frac{1}{ \max_{k_i \neq 0} k_i}} \right)  k_i (m_1, -m_1, m_2, -m_2,\dots, m_r,-m_r, 0, \dots 0) \mathbb{C} \begin{pmatrix}
e^{i\theta} & 0 \\
0 & e^{-i \theta}
\end{pmatrix}
\sim
R(\theta)  \varphi(e^{i\theta}) =
P'
\begin{pmatrix}
R(m_1 \theta) \\
& \ddots \\
&& R(m_r \theta) \\
&&&I_{n-2r} \end{pmatrix}
P'^{-1} P' \in \text{GL}_n ( \mathbb{C} )  \mathbb{C} \mathbb{R}","['linear-algebra', 'group-theory', 'algebraic-topology', 'reference-request', 'diagonalization']"
68,Can we recover all matrix minors from some of them?,Can we recover all matrix minors from some of them?,,"Let $k,n$ be natural numbers, $1<k<n$ . Suppose we have an ""unknown"" invertible $n \times n$ matrix $A$ over a field of characteristic zero. (we do not know the entries of $A$ ). Can we recover all the $k$ -minors of $A$ from a fixed, ordered partial list of them? That is, suppose that we are given the values of $r$ of the minors- i.e. we are given an indexed list of $r$ numbers, and we are told which number corresponds to which minor. Can we recover the other minors? Comment: Some non-degeneracy conditions on $A$ are necessary here: We at least need to assume that $\text{rank}(A)>k$ . Otherwise, if $\text{rank}(A)\le k$ , then even if we know all the $k$ -minors of $A$ except one, we cannot recover the last unknown minor. Indeed, take $A=\pmatrix{D&0\\ 0&0}$ where $D$ is any diagonal matrix of size $k$ . We can't recover the $k$ -minor corresponding to the first $k$ rows and columns (which is $\det D$ ) from the other $k$ -minors (which are zeroes). This example was suggested by user1551 .","Let be natural numbers, . Suppose we have an ""unknown"" invertible matrix over a field of characteristic zero. (we do not know the entries of ). Can we recover all the -minors of from a fixed, ordered partial list of them? That is, suppose that we are given the values of of the minors- i.e. we are given an indexed list of numbers, and we are told which number corresponds to which minor. Can we recover the other minors? Comment: Some non-degeneracy conditions on are necessary here: We at least need to assume that . Otherwise, if , then even if we know all the -minors of except one, we cannot recover the last unknown minor. Indeed, take where is any diagonal matrix of size . We can't recover the -minor corresponding to the first rows and columns (which is ) from the other -minors (which are zeroes). This example was suggested by user1551 .","k,n 1<k<n n \times n A A k A r r A \text{rank}(A)>k \text{rank}(A)\le k k A A=\pmatrix{D&0\\ 0&0} D k k k \det D k","['linear-algebra', 'matrices', 'algebraic-geometry', 'determinant', 'exterior-algebra']"
69,Determinant of $5 \times 5$ Boolean matrix,Determinant of  Boolean matrix,5 \times 5,"Consider the set of $5 \times 5$ matrices with $10$ entries equal to $1$ and the other $15$ entries equal to $0$. I would like to know how many such matrices have nonzero determinant. Is there a way to characterize them? I experimented a little with some examples, but could not make much progress.","Consider the set of $5 \times 5$ matrices with $10$ entries equal to $1$ and the other $15$ entries equal to $0$. I would like to know how many such matrices have nonzero determinant. Is there a way to characterize them? I experimented a little with some examples, but could not make much progress.",,"['linear-algebra', 'combinatorics', 'matrices', 'determinant', 'numerical-linear-algebra']"
70,"Find shortest vectors $u_1,v_1,\cdots,u_N,v_N$ such that $\langle u_i,v_j\rangle=1$ if $i\le j$ and $\langle u_i,v_j\rangle=0$ if $i>j$",Find shortest vectors  such that  if  and  if,"u_1,v_1,\cdots,u_N,v_N \langle u_i,v_j\rangle=1 i\le j \langle u_i,v_j\rangle=0 i>j","The following problem has come up in my work. Any help would be appreciated. Problem. Given $N \in \mathbb{N}$ , find vectors $u_1, u_2, \cdots, u_N, v_1, v_2, \cdots, v_N \in \mathbb{R}^d$ which satisfy $$\forall i,j \in \{1, 2, \cdots, N\} ~~~~~~~~~~ \langle u_i, v_j \rangle = \left\{ \begin{array}{cl} 1 & \text{if } i \leq j \\ 0 & \text{if } i > j \end{array} \right.$$ and which minimize $$A := \max_{i,j \in \{1, 2, \cdots, N\}} \|u_i\|_2 \cdot \|v_j\|_2.$$ Let $A_*(N)$ denote the optimal value of $A$ for a given value of $N$ . I'm mainly interested in the asymptotic behavior of $A_*(N)$ as $N \to \infty$ . I don't care what $d$ is. (EDIT: To be clear, $A_*(N)$ is defined to be the infimum -- taken over all $d \in \mathbb{N}$ and all $u_1, \cdots, u_N, v_1, \cdots, v_N \in \mathbb{R}^d$ satisfying the constraint -- of the objective $A$ . That is, I place no constraint on $d$ . Note that $d$ must depend on $N$ ; in particular, $d \geq N$ is necessary for the constraint to be satisfiable. However, without loss of generality, $d = 2N$ : If $d<2N$ , we can add superfluous dimensions. If $d>2N$ , then we can project the solution to the space spanned by $\{u_1, \cdots, u_N, v_1, \cdots, v_N\}$ , which has dimension at most $2N$ .) Question. What is $A_*(N)$ ? In particular, what is $$c_* := \limsup_{N \to \infty} \frac{\log A_*(N)}{\log \log N}~~~?$$ The value $c_*$ is what I really want to know, as it governs the asymptotics. i.e. $A_*(N) \approx (\log N)^{c_*}$ . I can prove $0 \leq c_* \leq 1$ . Any improved bounds (such as showing $c_*>0$ or showing $c_*<1$ ) would be really helpful. By Cauchy-Schwartz, $A_*(N) \geq \|u_1\|_2 \cdot \|v_1\|_2 \geq \langle u_1, v_1 \rangle = 1$ and, hence, $c_* \geq 0$ , but I have no nontrivial lower bound. The obvious upper bound is to let $u_1, \cdots, u_N$ be the standard basis vectors and then let $v_j = \sum_{i=1}^j u_i$ . Unfortunately, this only shows $A_*(N)\leq\sqrt{N}$ ; we can do better: The following inductive construction shows $A_*(N) \leq \lceil \log_2 N \rceil + 1$ and, hence, $c_* \leq 1$ . We will construct a solution for powers of $2$ i.e. $N=2^n$ with $n \in \mathbb{N}$ . The solution we construct will be denoted $u_1^n, \cdots, u_{2^n}^n, v_1^n, \cdots, v_{2^n}^n \in \mathbb{R}^{2^{n+1}-1}$ . The base case is $u_1^0 = v_1^0 = (1)$ . For $n \geq 1$ and $1 \leq i \leq 2^{n-1}$ , define $$u_i^{n} = \left( \begin{array}{c} 1 \\ u_i^{n-1} \\ 0^{2^{n}-1} \end{array} \right), ~~~~~~~~ u_{2^{n-1}+i}^{n} = \left( \begin{array}{c} 0 \\ 0^{2^{n}-1} \\ u_i^{n-1} \end{array} \right), ~~~~~~~~ v_i^{n} = \left( \begin{array}{c} 0 \\ v_i^{n-1} \\ 0^{2^{n}-1} \end{array} \right), ~~~~~~~~ v_{2^{n-1}+i}^{n} = \left( \begin{array}{c} 1 \\ 0^{2^{n}-1} \\ v_i^{n-1} \end{array} \right),$$ where $0^m \in \mathbb{R}^m$ denotes the $m$ -dimensional zero vector. It is easy to verify inductively that this construction satisfies the constraint and achieves $\|u_i^n\|_2 \leq \sqrt{n+1}$ and $\|v_i^n\|_2 \leq \sqrt{n+1}$ for $1 \leq i \leq 2^n$ , as required to show $A_*(2^n)\leq n+1$ . Clearly, $A_*$ is an increasing function. Hence $A_*(N) \leq A_*\left(2^{\lceil \log_2 N \rceil}\right) \leq \lceil \log_2 N \rceil +1$ for all $N \in \mathbb{N}$ , as desired.","The following problem has come up in my work. Any help would be appreciated. Problem. Given , find vectors which satisfy and which minimize Let denote the optimal value of for a given value of . I'm mainly interested in the asymptotic behavior of as . I don't care what is. (EDIT: To be clear, is defined to be the infimum -- taken over all and all satisfying the constraint -- of the objective . That is, I place no constraint on . Note that must depend on ; in particular, is necessary for the constraint to be satisfiable. However, without loss of generality, : If , we can add superfluous dimensions. If , then we can project the solution to the space spanned by , which has dimension at most .) Question. What is ? In particular, what is The value is what I really want to know, as it governs the asymptotics. i.e. . I can prove . Any improved bounds (such as showing or showing ) would be really helpful. By Cauchy-Schwartz, and, hence, , but I have no nontrivial lower bound. The obvious upper bound is to let be the standard basis vectors and then let . Unfortunately, this only shows ; we can do better: The following inductive construction shows and, hence, . We will construct a solution for powers of i.e. with . The solution we construct will be denoted . The base case is . For and , define where denotes the -dimensional zero vector. It is easy to verify inductively that this construction satisfies the constraint and achieves and for , as required to show . Clearly, is an increasing function. Hence for all , as desired.","N \in \mathbb{N} u_1, u_2, \cdots, u_N, v_1, v_2, \cdots, v_N \in \mathbb{R}^d \forall i,j \in \{1, 2, \cdots, N\} ~~~~~~~~~~ \langle u_i, v_j \rangle = \left\{ \begin{array}{cl} 1 & \text{if } i \leq j \\ 0 & \text{if } i > j \end{array} \right. A := \max_{i,j \in \{1, 2, \cdots, N\}} \|u_i\|_2 \cdot \|v_j\|_2. A_*(N) A N A_*(N) N \to \infty d A_*(N) d \in \mathbb{N} u_1, \cdots, u_N, v_1, \cdots, v_N \in \mathbb{R}^d A d d N d \geq N d = 2N d<2N d>2N \{u_1, \cdots, u_N, v_1, \cdots, v_N\} 2N A_*(N) c_* := \limsup_{N \to \infty} \frac{\log A_*(N)}{\log \log N}~~~? c_* A_*(N) \approx (\log N)^{c_*} 0 \leq c_* \leq 1 c_*>0 c_*<1 A_*(N) \geq \|u_1\|_2 \cdot \|v_1\|_2 \geq \langle u_1, v_1 \rangle = 1 c_* \geq 0 u_1, \cdots, u_N v_j = \sum_{i=1}^j u_i A_*(N)\leq\sqrt{N} A_*(N) \leq \lceil \log_2 N \rceil + 1 c_* \leq 1 2 N=2^n n \in \mathbb{N} u_1^n, \cdots, u_{2^n}^n, v_1^n, \cdots, v_{2^n}^n \in \mathbb{R}^{2^{n+1}-1} u_1^0 = v_1^0 = (1) n \geq 1 1 \leq i \leq 2^{n-1} u_i^{n} = \left( \begin{array}{c} 1 \\ u_i^{n-1} \\ 0^{2^{n}-1} \end{array} \right), ~~~~~~~~ u_{2^{n-1}+i}^{n} = \left( \begin{array}{c} 0 \\ 0^{2^{n}-1} \\ u_i^{n-1} \end{array} \right), ~~~~~~~~ v_i^{n} = \left( \begin{array}{c} 0 \\ v_i^{n-1} \\ 0^{2^{n}-1} \end{array} \right), ~~~~~~~~ v_{2^{n-1}+i}^{n} = \left( \begin{array}{c} 1 \\ 0^{2^{n}-1} \\ v_i^{n-1} \end{array} \right), 0^m \in \mathbb{R}^m m \|u_i^n\|_2 \leq \sqrt{n+1} \|v_i^n\|_2 \leq \sqrt{n+1} 1 \leq i \leq 2^n A_*(2^n)\leq n+1 A_* A_*(N) \leq A_*\left(2^{\lceil \log_2 N \rceil}\right) \leq \lceil \log_2 N \rceil +1 N \in \mathbb{N}","['linear-algebra', 'inner-products']"
71,Why do the squared permanents sum to 1?,Why do the squared permanents sum to 1?,,"Consider an  $n^2$ by $n^2$ real orthogonal matrix $M$.  Let $M'$ be the $n^2$  by $n$ matrix created by selecting the first $n$ columns of $M$. Now consider all $n$ by $n$ matrices $A_i$ created by selecting exactly $n$ not  necessarily distinct rows from $M'$ with replacement.  There are ${n^2+n-1 \choose n}$ such  matricex $A_i$. Let $s_1,\dots,s_k$ be the number of times each row is selected and let $\tau = \prod_i s_i!$. As an example, if $n=2$ and $A_i$ is formed by choosing row $1$ twice from $M'$ then $A_i$ will be a $2$ by $2$ matrix with two identical rows.  We have $s_1 = 2$ and $\tau = 2$. Let $\operatorname{perm}$ be the function that computes the permanent of a matrix. Why is the following true? $$\sum \frac{\operatorname{perm}(A_i)^2}{\tau} = 1?$$","Consider an  $n^2$ by $n^2$ real orthogonal matrix $M$.  Let $M'$ be the $n^2$  by $n$ matrix created by selecting the first $n$ columns of $M$. Now consider all $n$ by $n$ matrices $A_i$ created by selecting exactly $n$ not  necessarily distinct rows from $M'$ with replacement.  There are ${n^2+n-1 \choose n}$ such  matricex $A_i$. Let $s_1,\dots,s_k$ be the number of times each row is selected and let $\tau = \prod_i s_i!$. As an example, if $n=2$ and $A_i$ is formed by choosing row $1$ twice from $M'$ then $A_i$ will be a $2$ by $2$ matrix with two identical rows.  We have $s_1 = 2$ and $\tau = 2$. Let $\operatorname{perm}$ be the function that computes the permanent of a matrix. Why is the following true? $$\sum \frac{\operatorname{perm}(A_i)^2}{\tau} = 1?$$",,['linear-algebra']
72,Mathematical properties of two dimensional projection of three dimensional rotated object,Mathematical properties of two dimensional projection of three dimensional rotated object,,"Please be gentle as I do not have any degree in maths. By using a compass/straighedge method to construct Metatron's cube, a regular dodecahedron can be inferred from intersecting points. I'm looking for the ratio between the lengths of the edges ( blue ) of the dodecahedron and the radius of the initial circle ( red ) used for the construction. What I actually want is to have on of the faces of the dodecahedron, to be a regular pentagon ( purple ) on the two dimensional plane on which it's being projected. If you take a horizontal line through the center of the dodecahedron and rotate the object over that line ( green ). 1. How many degrees does it need to be rotated to make the irregular pentagon below, regular? 2. Is it true that after this rotation, the circle is perfectly inscribed inside the pentagon? EDIT: please understand that the purple pentagon only appears after a rotation of the resulting dodecahedron in 3D space Can I then say that if $r = 1$ then $x = 1.45308505601$ by using the formula for calculating the apothem ($DB$) given the length of a side which is: $$y = \frac{s}{2tan\frac{180}{5}} $$ For $y=1$ that gives $1.45308505601$. (ref: http://www.mathopenref.com/apothem.html )","Please be gentle as I do not have any degree in maths. By using a compass/straighedge method to construct Metatron's cube, a regular dodecahedron can be inferred from intersecting points. I'm looking for the ratio between the lengths of the edges ( blue ) of the dodecahedron and the radius of the initial circle ( red ) used for the construction. What I actually want is to have on of the faces of the dodecahedron, to be a regular pentagon ( purple ) on the two dimensional plane on which it's being projected. If you take a horizontal line through the center of the dodecahedron and rotate the object over that line ( green ). 1. How many degrees does it need to be rotated to make the irregular pentagon below, regular? 2. Is it true that after this rotation, the circle is perfectly inscribed inside the pentagon? EDIT: please understand that the purple pentagon only appears after a rotation of the resulting dodecahedron in 3D space Can I then say that if $r = 1$ then $x = 1.45308505601$ by using the formula for calculating the apothem ($DB$) given the length of a side which is: $$y = \frac{s}{2tan\frac{180}{5}} $$ For $y=1$ that gives $1.45308505601$. (ref: http://www.mathopenref.com/apothem.html )",,"['linear-algebra', 'geometry', 'recreational-mathematics', 'analytic-geometry']"
73,Coefficients in expansion of $(\sqrt[3]{2} - 1)^m$,Coefficients in expansion of,(\sqrt[3]{2} - 1)^m,"In trying to solve $a^3 - 2b^3 = 1$ over the integers I came across the need to answer the question: when does $(1+ \sqrt[3]{2} + \sqrt[3]{2}^2)^n$ have no $\sqrt[3]{2}^2$ term in it's expansion (in terms of the basis $\{1,\sqrt[3]{2}, \sqrt[3]{2}^2\}$ of $\mathbb{Z}[\sqrt[3]{2}]$). Clearly this cannot happen for positive $n$ (create a set of recursions that generate the next coeffs from the previous ones and you can observe all three coeffs are always positive). Of course for $n=0$ it does happen and you get the trivial solution $(a,b)=(1,0)$. I am struggling to sort out the negative $n$ case. This is the same as studying positive integer powers of $(\sqrt[3]{2} - 1)$. I guess that for positive $m$ you only ever get a zero coefficient of $\sqrt[3]{2}^2$ in $(\sqrt[3]{2} - 1)^m$ whenever $m=1$ (giving another solution $(a,b)=(-1,-1)$). However I am struggling to prove this using the recursions alone. Have I missed something easy?","In trying to solve $a^3 - 2b^3 = 1$ over the integers I came across the need to answer the question: when does $(1+ \sqrt[3]{2} + \sqrt[3]{2}^2)^n$ have no $\sqrt[3]{2}^2$ term in it's expansion (in terms of the basis $\{1,\sqrt[3]{2}, \sqrt[3]{2}^2\}$ of $\mathbb{Z}[\sqrt[3]{2}]$). Clearly this cannot happen for positive $n$ (create a set of recursions that generate the next coeffs from the previous ones and you can observe all three coeffs are always positive). Of course for $n=0$ it does happen and you get the trivial solution $(a,b)=(1,0)$. I am struggling to sort out the negative $n$ case. This is the same as studying positive integer powers of $(\sqrt[3]{2} - 1)$. I guess that for positive $m$ you only ever get a zero coefficient of $\sqrt[3]{2}^2$ in $(\sqrt[3]{2} - 1)^m$ whenever $m=1$ (giving another solution $(a,b)=(-1,-1)$). However I am struggling to prove this using the recursions alone. Have I missed something easy?",,"['linear-algebra', 'algebraic-number-theory', 'recurrence-relations', 'diophantine-equations']"
74,constructive canonical form of orthogonal matrix,constructive canonical form of orthogonal matrix,,"For every orthogonal matrix $Q$ over the reals there is an orthogonal matrix $P$ and a block diagonal matrix $D$ such that $D=PQP^{t}$. Each block in D is either $(1)$, $(-1)$ or a two dimensional block of the form $\left( \begin{array}{cc} \cos(\alpha) & -\sin(\alpha)  \\ \sin(\alpha) & \cos(\alpha)  \\ \end{array} \right) $. Is there a constructive proof for this fact? Maybe a code in Matlab or sage?","For every orthogonal matrix $Q$ over the reals there is an orthogonal matrix $P$ and a block diagonal matrix $D$ such that $D=PQP^{t}$. Each block in D is either $(1)$, $(-1)$ or a two dimensional block of the form $\left( \begin{array}{cc} \cos(\alpha) & -\sin(\alpha)  \\ \sin(\alpha) & \cos(\alpha)  \\ \end{array} \right) $. Is there a constructive proof for this fact? Maybe a code in Matlab or sage?",,"['linear-algebra', 'matrices', 'numerical-methods', 'orthogonal-matrices', 'constructive-mathematics']"
75,Find conjugation invariant functions without using eigenvalues?,Find conjugation invariant functions without using eigenvalues?,,"Let $M_n(\mathbb C)$ be the algebra of $n\times n$ complex matrices.  The coefficients of the characteristic polynomial $\det(\lambda I-A)=\sum f_i(A)\lambda^i$ are polynomials in the entries of $A\in M_n(\mathbb C)$, and are conjugation invariant.  Moreover, every conjugation invariant polynomial function $F:M_n(\mathbb C)\to \mathbb C$ is of the form $P(f_0,f_1,\ldots, f_{n-1})$ for $P\in \mathbb C[x_0,\ldots,x_{n-1}]$. Here is a proof (hover mouse over to view): Given a group action on a space, any continuous invariant function must be constant on the closure of each orbit.  Because the closure of every conjugacy class contains a diagonal matrix, the entries of which are the eigenvalues of the matrices in the conjugacy class, invariant functions must be polynomials in the eigenvalues.  Given any permutation $\sigma\in S_n$, we have that $\operatorname{diag}(a_1,\ldots, a_n)$ is conjugate to $\operatorname{diag}(a_{\sigma(1)},\ldots, a_{\sigma(n)})$, and hence an invariant function must be a symmetric function in the eigenvalues.  The coefficient $f_i(A)$ is up to a sign the $(n-i)$th elementary symmetric function in the eigenvalues of $A$, and since the elementary symmetric functions generate the ring of all symmetric functions, the result follows. Is there a proof that doesn't require reducing the problem to eigenvalues and symmetric functions?  Perhaps more important, can the result be extended to non-algebraically closed fields or other base rings where this particular proof fails because we don't have diagonalization?  If not, what additional conjugation-invariant polynomial functions are there over $\mathbb R$, $\mathbb Q$, or $\mathbb Z$?","Let $M_n(\mathbb C)$ be the algebra of $n\times n$ complex matrices.  The coefficients of the characteristic polynomial $\det(\lambda I-A)=\sum f_i(A)\lambda^i$ are polynomials in the entries of $A\in M_n(\mathbb C)$, and are conjugation invariant.  Moreover, every conjugation invariant polynomial function $F:M_n(\mathbb C)\to \mathbb C$ is of the form $P(f_0,f_1,\ldots, f_{n-1})$ for $P\in \mathbb C[x_0,\ldots,x_{n-1}]$. Here is a proof (hover mouse over to view): Given a group action on a space, any continuous invariant function must be constant on the closure of each orbit.  Because the closure of every conjugacy class contains a diagonal matrix, the entries of which are the eigenvalues of the matrices in the conjugacy class, invariant functions must be polynomials in the eigenvalues.  Given any permutation $\sigma\in S_n$, we have that $\operatorname{diag}(a_1,\ldots, a_n)$ is conjugate to $\operatorname{diag}(a_{\sigma(1)},\ldots, a_{\sigma(n)})$, and hence an invariant function must be a symmetric function in the eigenvalues.  The coefficient $f_i(A)$ is up to a sign the $(n-i)$th elementary symmetric function in the eigenvalues of $A$, and since the elementary symmetric functions generate the ring of all symmetric functions, the result follows. Is there a proof that doesn't require reducing the problem to eigenvalues and symmetric functions?  Perhaps more important, can the result be extended to non-algebraically closed fields or other base rings where this particular proof fails because we don't have diagonalization?  If not, what additional conjugation-invariant polynomial functions are there over $\mathbb R$, $\mathbb Q$, or $\mathbb Z$?",,"['linear-algebra', 'algebraic-groups']"
76,Size of a linear image of a cube in $\mathbb{Z}^d$,Size of a linear image of a cube in,\mathbb{Z}^d,"Suppose that we have an element $v = (v_1, \dots, v_d) \in \mathbb{Z}^d$ such that $\gcd(v_1, \dots, v_d) = 1$ . Then $v$ is contained in some base of $\mathbb{Z}^d$ (seen as a free-abelian group or a free module over $\mathbb{Z}$ ). In particular, there exists a regular integer matrix $A \in \mathop{GL}_d(\mathbb{Z})$ such that $A(v) = (1, 0, \dots, 0)$ . My question is the following: assuming that $|v_1| + \dots + |v_d| = n$ , what is the smallest $\ell \in \mathbb{N}$ (taken over all possible matrices $A$ ) such that the image $A\left([-1, 1]^d\right)$ is contained in the cube $[-\ell, \ell]^d$ ? I care about asymptotics of $\ell(n)$ up to multiplicative constant.","Suppose that we have an element such that . Then is contained in some base of (seen as a free-abelian group or a free module over ). In particular, there exists a regular integer matrix such that . My question is the following: assuming that , what is the smallest (taken over all possible matrices ) such that the image is contained in the cube ? I care about asymptotics of up to multiplicative constant.","v = (v_1, \dots, v_d) \in \mathbb{Z}^d \gcd(v_1, \dots, v_d) = 1 v \mathbb{Z}^d \mathbb{Z} A \in \mathop{GL}_d(\mathbb{Z}) A(v) = (1, 0, \dots, 0) |v_1| + \dots + |v_d| = n \ell \in \mathbb{N} A A\left([-1, 1]^d\right) [-\ell, \ell]^d \ell(n)","['linear-algebra', 'group-theory', 'modules', 'abelian-groups']"
77,"Let $A$ and $x$ be $n \times n$ and $n \times 1$ matrices, all entries real and strictly positive. Assume that $A^2 x = x$. Show that $A x = x$.","Let  and  be  and  matrices, all entries real and strictly positive. Assume that . Show that .",A x n \times n n \times 1 A^2 x = x A x = x,"I am stuck for weeks with the following problem: Let $A$ and $x$ be $n \times n$ and $n \times 1$ matrices, respectively, with all entries real and strictly positive. Assume that $A^2 x = x$. Show that $A x = x$. This was on the first problem set on a course of linear algebra based on the book written by Hoffman & Kunze. We haven't seen eigenvalues and eigenvectors yet. So, while any solution that uses anything more advanced than the first 3 chapters of that book is welcome (it may incentivize me to study something!), it does not solve the problem. Can anyone help? Thanks! EDIT: My question was marked as a exact duplicate of a question by Igor Caetano Diniz. While that is the exact same question, that post has one wrong answer and one answer that has a theorem that I haven't studied yet. So it doesn't solve my problem.","I am stuck for weeks with the following problem: Let $A$ and $x$ be $n \times n$ and $n \times 1$ matrices, respectively, with all entries real and strictly positive. Assume that $A^2 x = x$. Show that $A x = x$. This was on the first problem set on a course of linear algebra based on the book written by Hoffman & Kunze. We haven't seen eigenvalues and eigenvectors yet. So, while any solution that uses anything more advanced than the first 3 chapters of that book is welcome (it may incentivize me to study something!), it does not solve the problem. Can anyone help? Thanks! EDIT: My question was marked as a exact duplicate of a question by Igor Caetano Diniz. While that is the exact same question, that post has one wrong answer and one answer that has a theorem that I haven't studied yet. So it doesn't solve my problem.",,"['linear-algebra', 'matrices']"
78,"In a real normed linear space if $||x||=||y||$ implies $\lim_{n \to \infty} ||x+ny||-||nx+y||=0$ , then the norm comes from an inner-product space?","In a real normed linear space if  implies  , then the norm comes from an inner-product space?",||x||=||y|| \lim_{n \to \infty} ||x+ny||-||nx+y||=0,"$(V,\|\cdot|)$ be a real  normed linear space such that $\|x\|=\|y\|$ implies $\lim\limits_{n\to\infty} \|x+ny\|-\|nx+y\|=0$, then is it true that the norm comes from an inner-product space ?","$(V,\|\cdot|)$ be a real  normed linear space such that $\|x\|=\|y\|$ implies $\lim\limits_{n\to\infty} \|x+ny\|-\|nx+y\|=0$, then is it true that the norm comes from an inner-product space ?",,"['linear-algebra', 'analysis']"
79,How to prove this determinant is $\pi$?,How to prove this determinant is ?,\pi,"prove or disprove   $$\pi=\begin{vmatrix} 3&1&0&0&0&\cdots\\ -1&6&1&0&0&\cdots\\ 0&-1&\dfrac{6}{3^2}&1&0&\cdots\\ 0&0&-1&\dfrac{3^2\cdot 6}{5^2}&1&\cdots\\ 0&0&0&-1&\dfrac{5^2\cdot 6}{3^2\cdot 7^2}&\cdots\\ 0&0&0&0&-1&\dfrac{3^2\cdot 7^2\cdot 6}{5^2\cdot 9^2}&\cdots\\ 0&0&0&0&0&-1&\dfrac{5^2\cdot 9^2\cdot 6}{3^2\cdot 7^2\cdot 11^2}&\cdots\\ \vdots&\vdots&\vdots&\vdots&\vdots&\ddots&\ddots\\ \end{vmatrix}$$ I found maybe this is true.and is very interesting,(It seems Euler proved it?),because this  follows from the Euler result: $$\pi=3+\dfrac{1^2}{6+\dfrac{3^2}{6+\dfrac{5^2}{6+\dfrac{7^2}{6+\cdots}}}}$$ and can we solve it? Thank you","prove or disprove   $$\pi=\begin{vmatrix} 3&1&0&0&0&\cdots\\ -1&6&1&0&0&\cdots\\ 0&-1&\dfrac{6}{3^2}&1&0&\cdots\\ 0&0&-1&\dfrac{3^2\cdot 6}{5^2}&1&\cdots\\ 0&0&0&-1&\dfrac{5^2\cdot 6}{3^2\cdot 7^2}&\cdots\\ 0&0&0&0&-1&\dfrac{3^2\cdot 7^2\cdot 6}{5^2\cdot 9^2}&\cdots\\ 0&0&0&0&0&-1&\dfrac{5^2\cdot 9^2\cdot 6}{3^2\cdot 7^2\cdot 11^2}&\cdots\\ \vdots&\vdots&\vdots&\vdots&\vdots&\ddots&\ddots\\ \end{vmatrix}$$ I found maybe this is true.and is very interesting,(It seems Euler proved it?),because this  follows from the Euler result: $$\pi=3+\dfrac{1^2}{6+\dfrac{3^2}{6+\dfrac{5^2}{6+\dfrac{7^2}{6+\cdots}}}}$$ and can we solve it? Thank you",,"['linear-algebra', 'number-theory', 'determinant', 'continued-fractions']"
80,"Can the ""inducing"" vector norm be deduced or ""recovered"" from an induced norm?","Can the ""inducing"" vector norm be deduced or ""recovered"" from an induced norm?",,"Can the ""inducing"" vector norm be deduced or ""recovered"" from an induced (operator) norm? This question occurred to me after seeing this question .  I'm hoping that perhaps there exists something like the polarization identity , i.e. some identity one may use to ""recover"" the vector norm the same way the polarization identity ""recovers"" the inner product. I am aware that any induced norm satisfies the inequality $$ \left|\|A^r\|\right|^{1/r} \geq \rho(A) $$ Also, there exists some invertible matrix $S$ such that $$ \left|\|SAS^{-1}\|\right| = \rho(A) $$ I'm wondering if one or both of the above might also be a sufficient condition for $\left|\|\cdot \|\right|$ to be a derived norm, and that they might somehow be used to derive some sort of identity producing the necessary vector norm. Any input is appreciated! Runaway train of thought below: Following HHO's advice (see comment below), here's a neat way to recover the vector norm (assuming that a suitable vector norm does exist): Let $\|\cdot \|_O$ denote the operator norm.  I will define a vector norm $\|\cdot \|$ as follows: arbitrarily, I set $\|e_1\| = 1$ (because $\alpha \|\cdot\|$ is a vector norm for any $\alpha>0$ and any vector norm $\|\cdot\|$ and since both of these result in the same induced norm, we may set $\|e_1\| = \alpha$ for any $\alpha>0$).  From there, we may define $$ \|u\| = \left\| u e_1^* \right\|_O =  \left\|  \pmatrix{|&|&&|\\ u&0&\cdots&0\\ |&|&&|} \right\|_O $$ What remains to be seen is under which conditions this defines a valid vector-norm. In fact, the above must always be a valid vector-norm, by the definition of a matrix norm.  It is necessary to check whether the vector norm produced above induces the operator norm that we started with. By the above, here's a neat criterion for checking whether $\|\cdot\|_O$ is an operator norm: We can state that $\|\cdot\|_O$ is an induced (matrix) norm if and only if for all $A \in \mathbb{F}^{n\times n}$, we have $$ \|A\|_O = \max_{x \neq 0} \frac{\|Ax e_1^*\|_O}{\|xe_1^*\|_O} $$ and, presumably, $e_1$ can be replaced by any convenient $v \in \mathbb{F}^n: v^*v = 1$.  I guess I answered my own question then. Is this a known theorem?  This question is now a reference request.  If anyone has seen something like this, please say so.","Can the ""inducing"" vector norm be deduced or ""recovered"" from an induced (operator) norm? This question occurred to me after seeing this question .  I'm hoping that perhaps there exists something like the polarization identity , i.e. some identity one may use to ""recover"" the vector norm the same way the polarization identity ""recovers"" the inner product. I am aware that any induced norm satisfies the inequality $$ \left|\|A^r\|\right|^{1/r} \geq \rho(A) $$ Also, there exists some invertible matrix $S$ such that $$ \left|\|SAS^{-1}\|\right| = \rho(A) $$ I'm wondering if one or both of the above might also be a sufficient condition for $\left|\|\cdot \|\right|$ to be a derived norm, and that they might somehow be used to derive some sort of identity producing the necessary vector norm. Any input is appreciated! Runaway train of thought below: Following HHO's advice (see comment below), here's a neat way to recover the vector norm (assuming that a suitable vector norm does exist): Let $\|\cdot \|_O$ denote the operator norm.  I will define a vector norm $\|\cdot \|$ as follows: arbitrarily, I set $\|e_1\| = 1$ (because $\alpha \|\cdot\|$ is a vector norm for any $\alpha>0$ and any vector norm $\|\cdot\|$ and since both of these result in the same induced norm, we may set $\|e_1\| = \alpha$ for any $\alpha>0$).  From there, we may define $$ \|u\| = \left\| u e_1^* \right\|_O =  \left\|  \pmatrix{|&|&&|\\ u&0&\cdots&0\\ |&|&&|} \right\|_O $$ What remains to be seen is under which conditions this defines a valid vector-norm. In fact, the above must always be a valid vector-norm, by the definition of a matrix norm.  It is necessary to check whether the vector norm produced above induces the operator norm that we started with. By the above, here's a neat criterion for checking whether $\|\cdot\|_O$ is an operator norm: We can state that $\|\cdot\|_O$ is an induced (matrix) norm if and only if for all $A \in \mathbb{F}^{n\times n}$, we have $$ \|A\|_O = \max_{x \neq 0} \frac{\|Ax e_1^*\|_O}{\|xe_1^*\|_O} $$ and, presumably, $e_1$ can be replaced by any convenient $v \in \mathbb{F}^n: v^*v = 1$.  I guess I answered my own question then. Is this a known theorem?  This question is now a reference request.  If anyone has seen something like this, please say so.",,"['linear-algebra', 'matrices', 'functional-analysis', 'reference-request']"
81,Number in Base b as Dot Product,Number in Base b as Dot Product,,"Question out of curiosity: Any $k$ digit number $a_1 a_2 a_3 ... a_k$ written in base $b$ can be thought of as the dot product of a digits vector $a = \langle a_1, a_2, a_3, ..., a_k\rangle$ and a vector of powers of the base $b=\langle b^{k-1}, b^{k-2}, ..., b^1, b^0\rangle$ ( For instance the number $365=3\cdot10^2+6\cdot10^1+5\cdot10^0$ ). Given the cosine angle formula $$\frac{a \cdot b}{|a| |b|}= \cos(\theta)$$ is there a meaningful interpretation of the angle in the right hand side? For small k, it is easy to draw. Like a cube grid of points sitting on some space https://www.desmos.com/3d/idxu82btpv and the base vector goes along a parametric curve. Edit: Considering 365 vs 653, it seems like it has something to do with the ""decreasingness"" of digits given the sorted nature of b. Wondering what the pattern looks like.","Question out of curiosity: Any digit number written in base can be thought of as the dot product of a digits vector and a vector of powers of the base ( For instance the number ). Given the cosine angle formula is there a meaningful interpretation of the angle in the right hand side? For small k, it is easy to draw. Like a cube grid of points sitting on some space https://www.desmos.com/3d/idxu82btpv and the base vector goes along a parametric curve. Edit: Considering 365 vs 653, it seems like it has something to do with the ""decreasingness"" of digits given the sorted nature of b. Wondering what the pattern looks like.","k a_1 a_2 a_3 ... a_k b a = \langle a_1, a_2, a_3, ..., a_k\rangle b=\langle b^{k-1}, b^{k-2}, ..., b^1, b^0\rangle 365=3\cdot10^2+6\cdot10^1+5\cdot10^0 \frac{a \cdot b}{|a| |b|}= \cos(\theta)","['linear-algebra', 'algebra-precalculus', 'number-systems']"
82,"What axioms are needed to show that every vector space has a ""coordinate system""?","What axioms are needed to show that every vector space has a ""coordinate system""?",,"It is a well-known theorem that the axiom of choice (AC) is equivalent to the statement that every vector space over any field has a basis. In this question I am interested in the following somewhat dual concept to that of a basis: If $V$ is some vector space over a field $K$ , we call a subset $S\subseteq V'$ of the dual space a coordinate system for $V$ if $S$ separates points, i.e. for each $0\neq v\in V$ there is some $s\in S$ with $sv\neq 0$ and $S$ is irredundant in that no proper subset of it also separates points. (If this is known under a different name, please let me know.) This notion is somewhat less well behaved than that of a basis, but still mimics some of its properties. For instance: A subset $B\subseteq V$ is a basis if and only if the natural map $K^{\oplus B}\to V$ is an isomorphism (and also, linear independence and generation correspond to injectivity and surjectivity vice versa). Along these same lines, some subset $S\subseteq V'$ is a coordinate system for $V$ if and only if the natural map $V\to K^S$ is injective and ""uses all the space"" in that following this map with a projection to any $K^{S'}$ for a proper subset $S'$ is never injective. Under ZFC, every vector space has a coordinate system - simply pick a basis $B$ and take all the projections $\pi_b$ that extract the coefficient of $b$ from a linear combination (in other words, the dual basis $B'$ of $B$ ). It is also clear that for finite dimensional $V$ , every coordinate system arises in this way. But for larger spaces, these notions diverge. For instance, this procedure performed on the direct product $K^\mathbb{N}$ gives some uncountable family in its (huge) dual space. But we may also simply take the countably many coordinate projections and obtain a coordinate system that way. Neither family is a basis for $(K^{\mathbb{N}})'$ and we even see that there can be coordinate systems of different cardinality. However, it can be extrapolated from this example that having a coordinate system is a more robust property than having a basis: In ZF, every vector $V$ space that has a basis $B$ , also has a coordinate system (the dual basis). But we can also use this same basis to obtain a coordinate system on $V'$ , simply by taking the family of point evaluations at $b\in B$ (in other words, the image of $B$ under the double dual map $j:V\mapsto V''$ ). Hence, since it is consistent with ZF that not every dual space of a space with a basis itself has a basis (for instance $K^\mathbb{N}$ as the dual space of $K^{\oplus \mathbb{N}}$ ), there can be spaces without a basis that still have a coordinate system. Which finally brings me to the question: What is the set-theoretic strength of the statement that every vector space has a coordinate system? Are there concrete spaces that (consistently) do not have any? Or also, is there maybe some concrete procedure that, from a coordinate system of $V'''$ (say) produces a basis of $V$ ? Thanks for any information on that matter.","It is a well-known theorem that the axiom of choice (AC) is equivalent to the statement that every vector space over any field has a basis. In this question I am interested in the following somewhat dual concept to that of a basis: If is some vector space over a field , we call a subset of the dual space a coordinate system for if separates points, i.e. for each there is some with and is irredundant in that no proper subset of it also separates points. (If this is known under a different name, please let me know.) This notion is somewhat less well behaved than that of a basis, but still mimics some of its properties. For instance: A subset is a basis if and only if the natural map is an isomorphism (and also, linear independence and generation correspond to injectivity and surjectivity vice versa). Along these same lines, some subset is a coordinate system for if and only if the natural map is injective and ""uses all the space"" in that following this map with a projection to any for a proper subset is never injective. Under ZFC, every vector space has a coordinate system - simply pick a basis and take all the projections that extract the coefficient of from a linear combination (in other words, the dual basis of ). It is also clear that for finite dimensional , every coordinate system arises in this way. But for larger spaces, these notions diverge. For instance, this procedure performed on the direct product gives some uncountable family in its (huge) dual space. But we may also simply take the countably many coordinate projections and obtain a coordinate system that way. Neither family is a basis for and we even see that there can be coordinate systems of different cardinality. However, it can be extrapolated from this example that having a coordinate system is a more robust property than having a basis: In ZF, every vector space that has a basis , also has a coordinate system (the dual basis). But we can also use this same basis to obtain a coordinate system on , simply by taking the family of point evaluations at (in other words, the image of under the double dual map ). Hence, since it is consistent with ZF that not every dual space of a space with a basis itself has a basis (for instance as the dual space of ), there can be spaces without a basis that still have a coordinate system. Which finally brings me to the question: What is the set-theoretic strength of the statement that every vector space has a coordinate system? Are there concrete spaces that (consistently) do not have any? Or also, is there maybe some concrete procedure that, from a coordinate system of (say) produces a basis of ? Thanks for any information on that matter.",V K S\subseteq V' V S 0\neq v\in V s\in S sv\neq 0 S B\subseteq V K^{\oplus B}\to V S\subseteq V' V V\to K^S K^{S'} S' B \pi_b b B' B V K^\mathbb{N} (K^{\mathbb{N}})' V B V' b\in B B j:V\mapsto V'' K^\mathbb{N} K^{\oplus \mathbb{N}} V''' V,"['linear-algebra', 'set-theory']"
83,A mistake in Bourbaki's construction of general tensor product?,A mistake in Bourbaki's construction of general tensor product?,,"Fortunately someone seems to have a asked a similar question before here: Bourbaki's construction of generalized tensor product of modules ; to save space and time, I will use the same notation and terminology. I will begin where the other question ended. After defining $\bigotimes^{(c,p,q)}_{\lambda\in L}G_\lambda$ , Bourbaki writes the following: Then we are given details of the associativity properties: The existence of the claimed isomorphism is then proved by induction on $n$ . The proof of the base case is easy. However, I have found that ""permutability hypothesis (P)"" is not enough to allow the proof in the induction step. To highlight this, I will try to show that the case $n=3$ doesn't reduce to a routine application of the result for $n=2$ . So, let all notation be as before and let $n=3$ . That is to say, let $(L_1,L_2,L_3)$ be a partition of $L$ . Then $(L_1\cup L_2,L_3)$ is also a partition of $L$ and we should be able to apply the result for $n=2$ , right? But this is not possible! What do we need to apply $n=2$ result? We need the following: A family $(G_\lambda)_{\lambda\in L}$ of abelian groups; A mapping $c:\Omega\rightarrow L\times L$ ; Two mappings $p\in\prod_{\omega\in\Omega}\text{End}_{\mathbb{Z}}(E_{\rho(\omega)})$ and $q\in\prod_{\omega\in\Omega}\text{End}_{\mathbb{Z}}(E_{\sigma(\omega)})$ satisfying a corresponding form of hypothesis (P). The problem is that (3) is not satisfied. By assumption, we are given a map $c:\Omega\rightarrow L\times L$ and the two mapping $p,q$ . Then we have $$\Omega_1=c^{-1}(L_1\times L_1)\cup c^{-1}(L_1\times L_2)\cup c^{-1}(L_2\times L_1)\cup c^{-1}(L_2\times L_2)\text{ and }\Omega_2=c^{-1}(L_3\times L_3).$$ Now, let us see whether the corresponding hypothesis (P) holds for this partition and mappings. Fix $\omega\in\Omega-(\Omega_1\cup\Omega_2)$ , say, $\omega\in c^{-1}(L_1\times L_3)$ . Let $\xi\in\Omega_1\cup\Omega_2$ , say, $\xi\in c^{-1}(L_1\times L_2)$ . We need, for example, for $$p_\omega\circ p_\xi=p_\xi\circ p_\omega$$ to hold, but we have no reason to deduce that it does. By assumption we only have the above identity for $\xi\in c^{-1}(L_i\times L_i)\cap\rho^{-1}(\{\rho(\omega)\})$ for $i=1,2,3$ for a fixed $\omega$ not in sets of the form $c^{-1}(L_i\times L_i)$ . The same problem exists for the other identities. So, it's clear that hypothesis (P) in the form given by Bourbaki is not enough. Am I correct? Does anyone know how to fix it? EDIT: You can find this in Algebra I p.259-264.","Fortunately someone seems to have a asked a similar question before here: Bourbaki's construction of generalized tensor product of modules ; to save space and time, I will use the same notation and terminology. I will begin where the other question ended. After defining , Bourbaki writes the following: Then we are given details of the associativity properties: The existence of the claimed isomorphism is then proved by induction on . The proof of the base case is easy. However, I have found that ""permutability hypothesis (P)"" is not enough to allow the proof in the induction step. To highlight this, I will try to show that the case doesn't reduce to a routine application of the result for . So, let all notation be as before and let . That is to say, let be a partition of . Then is also a partition of and we should be able to apply the result for , right? But this is not possible! What do we need to apply result? We need the following: A family of abelian groups; A mapping ; Two mappings and satisfying a corresponding form of hypothesis (P). The problem is that (3) is not satisfied. By assumption, we are given a map and the two mapping . Then we have Now, let us see whether the corresponding hypothesis (P) holds for this partition and mappings. Fix , say, . Let , say, . We need, for example, for to hold, but we have no reason to deduce that it does. By assumption we only have the above identity for for for a fixed not in sets of the form . The same problem exists for the other identities. So, it's clear that hypothesis (P) in the form given by Bourbaki is not enough. Am I correct? Does anyone know how to fix it? EDIT: You can find this in Algebra I p.259-264.","\bigotimes^{(c,p,q)}_{\lambda\in L}G_\lambda n n=3 n=2 n=3 (L_1,L_2,L_3) L (L_1\cup L_2,L_3) L n=2 n=2 (G_\lambda)_{\lambda\in L} c:\Omega\rightarrow L\times L p\in\prod_{\omega\in\Omega}\text{End}_{\mathbb{Z}}(E_{\rho(\omega)}) q\in\prod_{\omega\in\Omega}\text{End}_{\mathbb{Z}}(E_{\sigma(\omega)}) c:\Omega\rightarrow L\times L p,q \Omega_1=c^{-1}(L_1\times L_1)\cup c^{-1}(L_1\times L_2)\cup c^{-1}(L_2\times L_1)\cup c^{-1}(L_2\times L_2)\text{ and }\Omega_2=c^{-1}(L_3\times L_3). \omega\in\Omega-(\Omega_1\cup\Omega_2) \omega\in c^{-1}(L_1\times L_3) \xi\in\Omega_1\cup\Omega_2 \xi\in c^{-1}(L_1\times L_2) p_\omega\circ p_\xi=p_\xi\circ p_\omega \xi\in c^{-1}(L_i\times L_i)\cap\rho^{-1}(\{\rho(\omega)\}) i=1,2,3 \omega c^{-1}(L_i\times L_i)","['linear-algebra', 'abstract-algebra', 'tensor-products']"
84,"Determinant of a Pascal Matrix, sort of","Determinant of a Pascal Matrix, sort of",,"Let $A_{n}$ be the $(n+1) \times(n+1)$ matrix with coefficients $$ a_{i j}={i+j \choose i} $$ (binomial coefficients), where the rows and columns are indexed by the numbers from 0 to $n$ are indexed. Now I want to determine the Determinant and with the first 5 matrices i found out that it is $n+1$ if i did not make a mistake. The Matrix looks like this: $$ \left(\begin{matrix} {1+1 \choose 1} & {1+2 \choose 1} & {1+3 \choose 1} & \dots & {1+n+1 \choose 1} \\ {2+1 \choose 2} & {2+2 \choose 2} & {2+3 \choose 2} & \dots & {2+n+1 \choose 2} \\ {3+1 \choose 3} &{3+2 \choose 3} & {3+3 \choose 3} & \dots & {3+n+1 \choose 3} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ {n+1+1 \choose n+1} & {n+1+2 \choose n+1} & {n+1+3 \choose n+1} & \dots & {n+1+n+1 \choose n+1} \end{matrix}\right) $$ The Problem is to bring this Matrix into an upper or lower triangle matrix. If anyone has hints or ideas that can help, please help, thanks in advance. Maybe the approach is not even good. If I make progress at all i will update this question.","Let be the matrix with coefficients (binomial coefficients), where the rows and columns are indexed by the numbers from 0 to are indexed. Now I want to determine the Determinant and with the first 5 matrices i found out that it is if i did not make a mistake. The Matrix looks like this: The Problem is to bring this Matrix into an upper or lower triangle matrix. If anyone has hints or ideas that can help, please help, thanks in advance. Maybe the approach is not even good. If I make progress at all i will update this question.","A_{n} (n+1) \times(n+1) 
a_{i j}={i+j \choose i}
 n n+1 
\left(\begin{matrix}
{1+1 \choose 1} & {1+2 \choose 1} & {1+3 \choose 1} & \dots & {1+n+1 \choose 1} \\
{2+1 \choose 2} & {2+2 \choose 2} & {2+3 \choose 2} & \dots & {2+n+1 \choose 2} \\
{3+1 \choose 3} &{3+2 \choose 3} & {3+3 \choose 3} & \dots & {3+n+1 \choose 3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
{n+1+1 \choose n+1} & {n+1+2 \choose n+1} & {n+1+3 \choose n+1} & \dots & {n+1+n+1 \choose n+1}
\end{matrix}\right)
","['linear-algebra', 'binomial-coefficients', 'determinant']"
85,Can an orthogonal matrix move monotonically toward a signed permutation matrix?,Can an orthogonal matrix move monotonically toward a signed permutation matrix?,,"The question is motivated by this question on Mathematics SE. Let $A \in O(n)$ be an orthogonal matrix that is not a signed permutation matrix, and let $P$ be the nearest signed permutation matrix to $A$ (for 'nearest', use the distance induced by the Frobenius norm, i.e., $d(A,P)=||A-P||_F$ ). Then can $A$ move 'monotonically' to $P$ ? I.e., in every neighborhood of $A$ , does there exist $B \in O(n)$ such that： (1) $|b_{ij}| \geq |a_{ij}|$ at every non-zero entry $(i,j)$ of $P$ (and at least one inequality is strict), and (2) $|b_{ij}| \leq |a_{ij}|$ at every other entry Note that if we remove the condition that $P$ is the nearest signed permutation matrix to $A$ , then the claim is not true and a counterexample is given in the original question. Also note that the claim is true if $A$ is sufficiently close to $P$ , as we can form a path from $A$ to $P$ by using exponential maps, say a path $B_t$ where $B_0 = A$ and $B_1 = P$ . Since the entries of $B_t$ is analytic in $t$ , in a small enough neighborhood the entries of $B_t$ would be monotonic in $t$ , which shows every matrix in the path satisfies our property. I am leaning toward that the claim is correct, but I am not sure. Any thoughts? Edit: Here is a weaker problem: for every $A\in O(n)$ , $\textit{does there always exist}$ a signed permutation matrix $P$ where $A$ could move 'monotonically' to $P$ in the sense described above? A possible approach is that, let $$C_P=\{B\in M_{n\times n}\mid B \text{ has the same sign as }A\text{ at the non-zero entries of }P, B \text{ has different sign from }A\text{ at the zero entries of }P\}.$$ Because the tangent space of $A$ is $n(n-1)/2$ dimensional, we would be done if we can prove that every $n(n-1)/2$ dimensional subspace of $M_{n\times n}$ intersects $\bigcup_{P\text{ is a permutation matrix}}C_P$ untrivially. Edit 2: Maybe the original question is still too strong, so I would like to weaken (2) to be: (2) $|b_{ij}| \leq |a_{ij}|+\epsilon$ at every other entry Then for every $\epsilon>0$ , does such $B$ exists? Since we have some freedom here, maybe Gram-Schmidt would work?","The question is motivated by this question on Mathematics SE. Let be an orthogonal matrix that is not a signed permutation matrix, and let be the nearest signed permutation matrix to (for 'nearest', use the distance induced by the Frobenius norm, i.e., ). Then can move 'monotonically' to ? I.e., in every neighborhood of , does there exist such that： (1) at every non-zero entry of (and at least one inequality is strict), and (2) at every other entry Note that if we remove the condition that is the nearest signed permutation matrix to , then the claim is not true and a counterexample is given in the original question. Also note that the claim is true if is sufficiently close to , as we can form a path from to by using exponential maps, say a path where and . Since the entries of is analytic in , in a small enough neighborhood the entries of would be monotonic in , which shows every matrix in the path satisfies our property. I am leaning toward that the claim is correct, but I am not sure. Any thoughts? Edit: Here is a weaker problem: for every , a signed permutation matrix where could move 'monotonically' to in the sense described above? A possible approach is that, let Because the tangent space of is dimensional, we would be done if we can prove that every dimensional subspace of intersects untrivially. Edit 2: Maybe the original question is still too strong, so I would like to weaken (2) to be: (2) at every other entry Then for every , does such exists? Since we have some freedom here, maybe Gram-Schmidt would work?","A \in O(n) P A d(A,P)=||A-P||_F A P A B \in O(n) |b_{ij}| \geq |a_{ij}| (i,j) P |b_{ij}| \leq |a_{ij}| P A A P A P B_t B_0 = A B_1 = P B_t t B_t t A\in O(n) \textit{does there always exist} P A P C_P=\{B\in M_{n\times n}\mid B \text{ has the same sign as }A\text{ at the non-zero entries of }P, B \text{ has different sign from }A\text{ at the zero entries of }P\}. A n(n-1)/2 n(n-1)/2 M_{n\times n} \bigcup_{P\text{ is a permutation matrix}}C_P |b_{ij}| \leq |a_{ij}|+\epsilon \epsilon>0 B","['linear-algebra', 'matrices', 'topological-groups', 'orthogonal-matrices']"
86,Matrix to power $2012$,Matrix to power,2012,"How to calculate $A^{2012}$? $A = \left[\begin{array}{ccc}3&-1&-2\\2&0&-2\\2&-1&-1\end{array}\right]$ How can one calculate this? It must be tricky or something, cause there was only 1 point for solving this.","How to calculate $A^{2012}$? $A = \left[\begin{array}{ccc}3&-1&-2\\2&0&-2\\2&-1&-1\end{array}\right]$ How can one calculate this? It must be tricky or something, cause there was only 1 point for solving this.",,"['linear-algebra', 'matrices', 'exponentiation']"
87,"Find a $4\times 4$ matrix $A$ where $A\neq I$ and $A^2 \neq I$, but $A^3 = I$.","Find a  matrix  where  and , but .",4\times 4 A A\neq I A^2 \neq I A^3 = I,"Give an example of a $4 \times 4$ matrix where $A \neq I$, $A^2 \neq I$, and $A^3 = I$. I found a $2 \times 2$ matrix where $A \neq I$ and $A^2 = I$, but this problem is more complex and has me completely stumped.","Give an example of a $4 \times 4$ matrix where $A \neq I$, $A^2 \neq I$, and $A^3 = I$. I found a $2 \times 2$ matrix where $A \neq I$ and $A^2 = I$, but this problem is more complex and has me completely stumped.",,['linear-algebra']
88,"If $A$ is a $2\times2$ matrix, what is $\det(4A)$ in terms of $\det(A)$?","If  is a  matrix, what is  in terms of ?",A 2\times2 \det(4A) \det(A),"If $A$ is a $2\times2$ matrix, what is $\det(4A)$ in terms of $\det(A)$? This seems trivial, but I'm not sure exactly what they are asking. I'm guessing I have some matrix $A = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ where I know $\det(A) = ad - bc$. So if they want to know what is $\det(4A)$ wouldn't it just be $4A = 4\begin{bmatrix}a&b\\c&d\end{bmatrix} = \begin{bmatrix}4a&4b\\4c&4d\end{bmatrix} =  \det(A) = 16ad - 16bc$?","If $A$ is a $2\times2$ matrix, what is $\det(4A)$ in terms of $\det(A)$? This seems trivial, but I'm not sure exactly what they are asking. I'm guessing I have some matrix $A = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ where I know $\det(A) = ad - bc$. So if they want to know what is $\det(4A)$ wouldn't it just be $4A = 4\begin{bmatrix}a&b\\c&d\end{bmatrix} = \begin{bmatrix}4a&4b\\4c&4d\end{bmatrix} =  \det(A) = 16ad - 16bc$?",,"['linear-algebra', 'determinant']"
89,Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are square matrices,Eigenvalues of  and  where  and  are square matrices,AB BA A B,"Show that if $A,B \in M_{n \times  n}(K)$ , where $K=\mathbb{R}, \mathbb{C}$ , then the matrices $AB$ and $BA$ have same eigenvalues. I do that like this: let $\lambda$ be the eigenvalue of $B$ and $v\neq 0$ $ABv=A\lambda v=\lambda Av=BAv$ the third equation is valid, because $Av$ is the eigenvector of $B$ . Am I doing it right?","Show that if , where , then the matrices and have same eigenvalues. I do that like this: let be the eigenvalue of and the third equation is valid, because is the eigenvector of . Am I doing it right?","A,B \in M_{n \times  n}(K) K=\mathbb{R}, \mathbb{C} AB BA \lambda B v\neq 0 ABv=A\lambda v=\lambda Av=BAv Av B","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
90,Are column operations legal in matrices also?,Are column operations legal in matrices also?,,"In linear algebra we have been talking a lot about the three elementary row operations. What I don't understand is why we can't multiply by any column by a constant? Since a matrix is really just a grouping of column vectors, shouldn't we be able to multiply a whole column by a constant but maintain the same set of solutions for the original and resulting matrices?","In linear algebra we have been talking a lot about the three elementary row operations. What I don't understand is why we can't multiply by any column by a constant? Since a matrix is really just a grouping of column vectors, shouldn't we be able to multiply a whole column by a constant but maintain the same set of solutions for the original and resulting matrices?",,"['linear-algebra', 'vector-spaces']"
91,Trace of AB = Trace of BA [duplicate],Trace of AB = Trace of BA [duplicate],,"This question already has answers here : How to prove $\operatorname{Tr}(AB) = \operatorname{Tr}(BA)$? (4 answers) Closed 7 years ago . We can define trace if $A =\sum_{i} \langle e_i, Ae_i\rangle$ where $e_i$'s are standard column vectors, and $\langle x, y\rangle =x^t y$ for suitable column vectors $x, y$. With this set up, I want to prove trace of AB and BA are same, so it's enough to prove that $$\sum_{i} \langle e_i, ABe_i\rangle =\sum_{i} \langle e_i, BAe_i\rangle$$ but how to conclude that?","This question already has answers here : How to prove $\operatorname{Tr}(AB) = \operatorname{Tr}(BA)$? (4 answers) Closed 7 years ago . We can define trace if $A =\sum_{i} \langle e_i, Ae_i\rangle$ where $e_i$'s are standard column vectors, and $\langle x, y\rangle =x^t y$ for suitable column vectors $x, y$. With this set up, I want to prove trace of AB and BA are same, so it's enough to prove that $$\sum_{i} \langle e_i, ABe_i\rangle =\sum_{i} \langle e_i, BAe_i\rangle$$ but how to conclude that?",,['linear-algebra']
92,What is the point of subspaces? [duplicate],What is the point of subspaces? [duplicate],,"This question already has answers here : Why a subspace of a vector space is useful (7 answers) Closed 8 years ago . I've been studying for my linear algebra final for the past few days, and just had this thought.  I know what a subspace is; It's a subset of a vector space that contains the zero vector, and preserves addition and scalar multiplication. But what can we do with a set after we've discovered that it's a subspace? The questions in my textbook only go as far as telling us to determine if a set is a subspace or not, we're not asked to use the fact that a set is a subspace to do something else. So my question is, what is the point of a subspace then? Thanks","This question already has answers here : Why a subspace of a vector space is useful (7 answers) Closed 8 years ago . I've been studying for my linear algebra final for the past few days, and just had this thought.  I know what a subspace is; It's a subset of a vector space that contains the zero vector, and preserves addition and scalar multiplication. But what can we do with a set after we've discovered that it's a subspace? The questions in my textbook only go as far as telling us to determine if a set is a subspace or not, we're not asked to use the fact that a set is a subspace to do something else. So my question is, what is the point of a subspace then? Thanks",,"['linear-algebra', 'vector-spaces']"
93,$A\in M_3(\mathbb R)$ orthogonal matrix with $\det (A)=1$.  Prove that $(\mathrm{tr} A)^2- \mathrm{tr}(A^2) = 2 \mathrm{tr} (A)$,orthogonal matrix with .  Prove that,A\in M_3(\mathbb R) \det (A)=1 (\mathrm{tr} A)^2- \mathrm{tr}(A^2) = 2 \mathrm{tr} (A),"$A\in M_3(\mathbb R)$ orthogonal matrix with $\det (A)=1$. I need to prove that $(\mathrm{tr} A)^2-\mathrm{tr}(A^2) = 2 \mathrm{tr} (A)$ ; $\mathrm{tr}$=trace. I know that if $A$ is orthogonal than $A^tA=I$ and that $A$ is diagonalizable and similar to $D=\begin{pmatrix} \lambda_1 &  & \\   & \lambda_2 & \\   &  & \lambda_3 \end{pmatrix}$. We know as well that $\mathrm{tr} A=\mathrm{tr} D= \lambda_1+ \lambda_2+\lambda_3$  that  $\mathrm{tr} A^2=\mathrm{tr} D^2= \lambda_1^2+ \lambda_2^2+\lambda_3^2$ and that  $\lambda_1 \lambda_2 \lambda_3=1$. It's not enough for solving the question. What more should I know or use in order to solve it? Thanks","$A\in M_3(\mathbb R)$ orthogonal matrix with $\det (A)=1$. I need to prove that $(\mathrm{tr} A)^2-\mathrm{tr}(A^2) = 2 \mathrm{tr} (A)$ ; $\mathrm{tr}$=trace. I know that if $A$ is orthogonal than $A^tA=I$ and that $A$ is diagonalizable and similar to $D=\begin{pmatrix} \lambda_1 &  & \\   & \lambda_2 & \\   &  & \lambda_3 \end{pmatrix}$. We know as well that $\mathrm{tr} A=\mathrm{tr} D= \lambda_1+ \lambda_2+\lambda_3$  that  $\mathrm{tr} A^2=\mathrm{tr} D^2= \lambda_1^2+ \lambda_2^2+\lambda_3^2$ and that  $\lambda_1 \lambda_2 \lambda_3=1$. It's not enough for solving the question. What more should I know or use in order to solve it? Thanks",,['linear-algebra']
94,"If $A^2=2A$, then $A$ is diagonalizable.","If , then  is diagonalizable.",A^2=2A A,"I think, I should use a double linear transformation but can't find any proper solution. Let $\mathbb F$ be a field, $\mathscr M_n (\mathbb F)$, the set of $n\times n$ matrices with elements in $\mathbb F$, and $A\in \mathscr M_n (\mathbb F)$ satisfying the equation $$ A^2=2A. $$ Show that $A$ is diagonalizable and find its eigenvalues. Thank you all!","I think, I should use a double linear transformation but can't find any proper solution. Let $\mathbb F$ be a field, $\mathscr M_n (\mathbb F)$, the set of $n\times n$ matrices with elements in $\mathbb F$, and $A\in \mathscr M_n (\mathbb F)$ satisfying the equation $$ A^2=2A. $$ Show that $A$ is diagonalizable and find its eigenvalues. Thank you all!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'matrix-calculus']"
95,Drawing Ellipse from eigenvalue-eigenvector.,Drawing Ellipse from eigenvalue-eigenvector.,,"If I have two eigenvalue $\lambda_1$ and $\lambda_2$ and two associated normalized eigenvector $\mathbf e_1$ and $\mathbf e_2$ respectively, and I want to draw ellipse, How can I know which eigenvalue and eigenvector will construct the major axis and which one will be associated with minor axis ? Edit: The ellipse looks like the following :","If I have two eigenvalue and and two associated normalized eigenvector and respectively, and I want to draw ellipse, How can I know which eigenvalue and eigenvector will construct the major axis and which one will be associated with minor axis ? Edit: The ellipse looks like the following :",\lambda_1 \lambda_2 \mathbf e_1 \mathbf e_2,"['linear-algebra', 'eigenvalues-eigenvectors', 'conic-sections']"
96,Is there an operation on matrices such that the determinant yields a homomorphism with the additive group of the reals?,Is there an operation on matrices such that the determinant yields a homomorphism with the additive group of the reals?,,"It well known that, under standard matrix multiplication $\det(AB) = \det(A)\det(B)$, or in other words, that $\det : \mathbb{R}^{n \times n} \rightarrow \langle\mathbb{R}, * \rangle$ is a monoid homomorphism. In a similar vein, given any two matrices $A,B \in \mathbb{R}^{n \times n}$ , is there an operation $A\star B$ such that $\det(A \star B) = \det(A) + \det(B)$?","It well known that, under standard matrix multiplication $\det(AB) = \det(A)\det(B)$, or in other words, that $\det : \mathbb{R}^{n \times n} \rightarrow \langle\mathbb{R}, * \rangle$ is a monoid homomorphism. In a similar vein, given any two matrices $A,B \in \mathbb{R}^{n \times n}$ , is there an operation $A\star B$ such that $\det(A \star B) = \det(A) + \det(B)$?",,"['linear-algebra', 'matrices']"
97,"How to prove that the set $\{\sin(x),\sin(2x),...,\sin(mx)\}$ is linearly independent? [closed]",How to prove that the set  is linearly independent? [closed],"\{\sin(x),\sin(2x),...,\sin(mx)\}","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Could you help me to show that the functions $\sin(x),\sin(2x),...,\sin(mx)\in V$ are linearly independent, where $V$ is the space of real functions? Thanks.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Could you help me to show that the functions $\sin(x),\sin(2x),...,\sin(mx)\in V$ are linearly independent, where $V$ is the space of real functions? Thanks.",,['linear-algebra']
98,A vector space is finite dimensional if all of its proper subspaces are finite dimensional,A vector space is finite dimensional if all of its proper subspaces are finite dimensional,,"Suppose every proper subspace $(\neq V)$ of a vector space $V$ is finite dimensional. Prove that $V$ is finite dimensional. This question just popped into my head when I was reading about inner product space, so I can't guarantee how legitimate the question is. My try: Assume V is an inner product space. Take any proper subspace $U$ of $V$. Then $V=U\oplus U^{\bot}$. Both $U$ and $U^{\bot}$ are finite dimensional. So they both have a basis of finite dimension and we will be done.  So for the inner product space $V$ the statement holds true. What about other cases? Does the statement hold true for every vector space? Does there exist a characteristic to identify vector spaces with this property?","Suppose every proper subspace $(\neq V)$ of a vector space $V$ is finite dimensional. Prove that $V$ is finite dimensional. This question just popped into my head when I was reading about inner product space, so I can't guarantee how legitimate the question is. My try: Assume V is an inner product space. Take any proper subspace $U$ of $V$. Then $V=U\oplus U^{\bot}$. Both $U$ and $U^{\bot}$ are finite dimensional. So they both have a basis of finite dimension and we will be done.  So for the inner product space $V$ the statement holds true. What about other cases? Does the statement hold true for every vector space? Does there exist a characteristic to identify vector spaces with this property?",,"['linear-algebra', 'vector-spaces']"
99,Is zero a scalar?,Is zero a scalar?,,"Is zero considered a scalar? In other words, is $\begin{bmatrix}0\\0\\\end{bmatrix}$ a scalar multiple of $\begin{bmatrix}a\\b\\\end{bmatrix}$ where $a$ and $b$ are real numbers?","Is zero considered a scalar? In other words, is $\begin{bmatrix}0\\0\\\end{bmatrix}$ a scalar multiple of $\begin{bmatrix}a\\b\\\end{bmatrix}$ where $a$ and $b$ are real numbers?",,"['linear-algebra', 'vector-spaces', 'terminology', 'vectors']"
