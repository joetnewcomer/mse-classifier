,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why is this identity about commutators of Lie derivatives true?,Why is this identity about commutators of Lie derivatives true?,,"I am reading the paper ""On splitting methods for Schrödinger-Poisson and cubic nonlinear Schrödinger equations"" by Lubich. On page 2147 the author claims $$[T,V](\psi) = T'(\psi) V(\psi) - V'(\psi) T(\psi) $$ where $T(\psi) = i\Delta \psi$ and $V(\psi) = -i\tilde{V}[\psi]\psi$ with $\tilde{V}([\psi])= \Delta^{-1} |\psi|^2$ where $\psi$ is a complex-valued $H^2$ -function on $\mathbb{R}^3$ (so that $\Delta\psi$ is well-defined). It is then claimed that $$ \begin{split}  T'(\psi) V(\psi) - V'(\psi) T(\psi) = &\; i\Delta(-i\Delta^{-1}(\psi\bar{\psi}) \psi) + i\Delta^{-1}(-i\Delta \psi \bar{\psi})\psi \\ &\;+ i\Delta^{-1}(\psi  \overline{i\Delta\psi}) + i\Delta^{-1}(\psi\bar{\psi})i\Delta \psi   \end{split}\label{1}\tag{1} $$ So the derivative in $T'(\psi)$ is probably just some sort of formal derivative with respect to $\psi$ but what would $(i\Delta \psi)'$ be then? The first term in $(1)$ is cleary coming from $T'(\psi)V(\psi)$ but it doesn't make sense to me from the definition of $T'(\psi)$ . It would make sense as $T(V(\psi))$ , though. So is $T'(\psi)=T(\psi)$ then? The other terms are confusing as well. Naively, we would have $$(\tilde{V}[\psi]\psi)' = \tilde{V}[\psi]'\psi + \tilde{V}[\psi]$$ which, again, doesn't fit with what we have in \eqref{1}. More formal than above, $T$ and $V$ are vector fields and their and the corresponding differential equation is $$\partial_t \psi = T(\psi) + V(\psi)$$ In that sense the commutator is the Lie bracket of vector fields. Maybe someone can give me some clarification?","I am reading the paper ""On splitting methods for Schrödinger-Poisson and cubic nonlinear Schrödinger equations"" by Lubich. On page 2147 the author claims where and with where is a complex-valued -function on (so that is well-defined). It is then claimed that So the derivative in is probably just some sort of formal derivative with respect to but what would be then? The first term in is cleary coming from but it doesn't make sense to me from the definition of . It would make sense as , though. So is then? The other terms are confusing as well. Naively, we would have which, again, doesn't fit with what we have in \eqref{1}. More formal than above, and are vector fields and their and the corresponding differential equation is In that sense the commutator is the Lie bracket of vector fields. Maybe someone can give me some clarification?","[T,V](\psi) = T'(\psi) V(\psi) - V'(\psi) T(\psi)  T(\psi) = i\Delta \psi V(\psi) = -i\tilde{V}[\psi]\psi \tilde{V}([\psi])= \Delta^{-1} |\psi|^2 \psi H^2 \mathbb{R}^3 \Delta\psi 
\begin{split}
 T'(\psi) V(\psi) - V'(\psi) T(\psi) = &\; i\Delta(-i\Delta^{-1}(\psi\bar{\psi}) \psi) + i\Delta^{-1}(-i\Delta \psi \bar{\psi})\psi \\
&\;+ i\Delta^{-1}(\psi  \overline{i\Delta\psi}) + i\Delta^{-1}(\psi\bar{\psi})i\Delta \psi  
\end{split}\label{1}\tag{1}
 T'(\psi) \psi (i\Delta \psi)' (1) T'(\psi)V(\psi) T'(\psi) T(V(\psi)) T'(\psi)=T(\psi) (\tilde{V}[\psi]\psi)' = \tilde{V}[\psi]'\psi + \tilde{V}[\psi] T V \partial_t \psi = T(\psi) + V(\psi)","['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'lie-derivative']"
1,How does one lift higher-order ODEs (and PDEs) on Manifolds?,How does one lift higher-order ODEs (and PDEs) on Manifolds?,,"In a course on Manifolds its often emphasized that [integral curves of] vector fields are the right way to do ODE theory without coordinates.  I'd say that ODEs and vector fields are morally the same.  My understanding is that integral sub-manifolds of distributions are essentially first-order PDEs. But we also like to study ODEs and PDEs of arbitrary order.  Do these have their own place on a manifold?  There's a couple objects one could write down whose coordinate representations will involve higher derivatives, such as $\nabla_v\dot{x}$ , or $(\nabla_v)^n\dot{x}$ .  Probably if you write down the coordinate equation for an integral curves of a section in $TTTTTT\dots TM$ it will also involve higher derivatives. Nothing stands out as nearly so obvious as in the first-order case.","In a course on Manifolds its often emphasized that [integral curves of] vector fields are the right way to do ODE theory without coordinates.  I'd say that ODEs and vector fields are morally the same.  My understanding is that integral sub-manifolds of distributions are essentially first-order PDEs. But we also like to study ODEs and PDEs of arbitrary order.  Do these have their own place on a manifold?  There's a couple objects one could write down whose coordinate representations will involve higher derivatives, such as , or .  Probably if you write down the coordinate equation for an integral curves of a section in it will also involve higher derivatives. Nothing stands out as nearly so obvious as in the first-order case.",\nabla_v\dot{x} (\nabla_v)^n\dot{x} TTTTTT\dots TM,"['ordinary-differential-equations', 'partial-differential-equations', 'manifolds', 'vector-fields']"
2,domain of flow of an inner vector field is a manifold with corners,domain of flow of an inner vector field is a manifold with corners,,"Let $X$ be a manifold with corners. Let $\vec v$ be an inner vector field on $X$ . The existence and uniqueness theorem for ODE says there's a domain of flow $\mathfrak D(\vec v)\subset \mathbb R\times X$ containing $\left\{ 0 \right\} \times X$ , $[0,\varepsilon_x)\times \left\{ x \right\} $ for each $x\in \partial X$ and $(-a_x,b_x)\times \left\{ x \right\}$ for $x\in X\setminus \partial X$ . Question 1. Is the domain of flow $\mathfrak D(\vec v)\subset \mathbb R\times X$ an embedded submanifold with corners? Question 2. For each $x\in X$ , does $(0,x)\in \mathfrak D(\vec v)$ have a product neighborhood $[0,\varepsilon _x)\times U_x\subset \mathfrak D(\vec v)$ with $U_x\subset X$ open?","Let be a manifold with corners. Let be an inner vector field on . The existence and uniqueness theorem for ODE says there's a domain of flow containing , for each and for . Question 1. Is the domain of flow an embedded submanifold with corners? Question 2. For each , does have a product neighborhood with open?","X \vec v X \mathfrak D(\vec v)\subset \mathbb R\times X \left\{ 0 \right\} \times X [0,\varepsilon_x)\times \left\{ x \right\}  x\in \partial X (-a_x,b_x)\times \left\{ x \right\} x\in X\setminus \partial X \mathfrak D(\vec v)\subset \mathbb R\times X x\in X (0,x)\in \mathfrak D(\vec v) [0,\varepsilon _x)\times U_x\subset \mathfrak D(\vec v) U_x\subset X","['ordinary-differential-equations', 'differential-geometry', 'smooth-manifolds', 'manifolds-with-boundary']"
3,Find Adjoint of $L = p(x) \frac{d^2}{dx^2} + r(x) \frac{d}{dx} + q(x)$,Find Adjoint of,L = p(x) \frac{d^2}{dx^2} + r(x) \frac{d}{dx} + q(x),"Suppose that \begin{align*}   L &= p(x) \frac{d^2}{dx^2} + r(x) \frac{d}{dx} + q(x) \\ \end{align*} Consider \begin{align*}   \int_a^b vL(u) \, dx \\ \end{align*} By repeated integration by parts, determine the adjoint operator $L^*$ such that \begin{align*}   \int_a^b \left[ uL^*(v) - vL(u) \right] &= \left. H(x) \right|_a^b \\ \end{align*} What is $H(x)$ ? Under what conditions does $L=L^*$ , the self-adjoint case? Hint : Show that \begin{align*}   L^* &= p \frac{d^2}{dx^2} + \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\ \end{align*} The given answer that I must find is: \begin{align*}   H(x) &= p \left( u \frac{dv}{dx} - v \frac{du}{dx} \right) + uv \left( \frac{dp}{dx} - r \right) \\ \end{align*} I'm not sure how to calculate the $L^*$ given in the hint, I obviously know integration by parts, but I don't see how that would be used here, I don't see how to calculate $H(x)$ when we have $L^*$ . Once we have $L^*$ , I can identify the self-adjoint conditions where $L = L^*$ : \begin{align*}   p \frac{d^2}{dx^2} + r \frac{d}{dx} + q &= p \frac{d^2}{dx^2} + \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\   r \frac{d}{dx} + q &= \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\ \end{align*} which would mean that \begin{align*}   r(x) &= 2 \frac{dp}{dx} - r(x) \\   \frac{dp}{dx} &= r(x) \\   p(x) &= \int r(x) \, dx \\ \end{align*} and \begin{align*}   q &= \frac{d^2p}{dx^2} - \frac{dr}{dx}(x) + q \\   \frac{d^2p}{dx^2} &= \frac{dr}{dx}(x) \\   \frac{dp}{dx} &= r(x) + c \\   p(x) &= \int \left( r(x) + c \right) \, dx \\ \end{align*} Since both expressions must be true, the constant in the second expression can be eliminated. We can conclude that $L = L^*$ if and only if $p(x) = \int r(x) \, dx$ I try to calculate $u L^*(v) - v L(u)$ but this doesn't seem useful: \begin{align*}   u L^*(v) &= u p \frac{d^2v}{dx^2} + u \left( 2 \frac{dp}{dx} - r \right) \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) v \\   v L(u) &= v p \frac{d^2u}{dx^2} + v r \frac{du}{dx} + v q u \\   u L^*(v) - v L(u) &= p \left( u \frac{d^2v}{dx^2} - v \frac{d^2u}{dx^2} \right) + u \left( 2 \frac{dp}{dx} - r \right) \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} \right) v - v r \frac{du}{dx} \\   &= p \left( u \frac{d^2v}{dx^2} - v \frac{d^2u}{dx^2} \right) + 2 u \frac{dp}{dx} \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} \right) v - r \left( u \frac{dv}{dx} + v \frac{du}{dx} \right) \\ \end{align*}","Suppose that Consider By repeated integration by parts, determine the adjoint operator such that What is ? Under what conditions does , the self-adjoint case? Hint : Show that The given answer that I must find is: I'm not sure how to calculate the given in the hint, I obviously know integration by parts, but I don't see how that would be used here, I don't see how to calculate when we have . Once we have , I can identify the self-adjoint conditions where : which would mean that and Since both expressions must be true, the constant in the second expression can be eliminated. We can conclude that if and only if I try to calculate but this doesn't seem useful:","\begin{align*}
  L &= p(x) \frac{d^2}{dx^2} + r(x) \frac{d}{dx} + q(x) \\
\end{align*} \begin{align*}
  \int_a^b vL(u) \, dx \\
\end{align*} L^* \begin{align*}
  \int_a^b \left[ uL^*(v) - vL(u) \right] &= \left. H(x) \right|_a^b \\
\end{align*} H(x) L=L^* \begin{align*}
  L^* &= p \frac{d^2}{dx^2} + \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\
\end{align*} \begin{align*}
  H(x) &= p \left( u \frac{dv}{dx} - v \frac{du}{dx} \right) + uv \left( \frac{dp}{dx} - r \right) \\
\end{align*} L^* H(x) L^* L^* L = L^* \begin{align*}
  p \frac{d^2}{dx^2} + r \frac{d}{dx} + q &= p \frac{d^2}{dx^2} + \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\
  r \frac{d}{dx} + q &= \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\
\end{align*} \begin{align*}
  r(x) &= 2 \frac{dp}{dx} - r(x) \\
  \frac{dp}{dx} &= r(x) \\
  p(x) &= \int r(x) \, dx \\
\end{align*} \begin{align*}
  q &= \frac{d^2p}{dx^2} - \frac{dr}{dx}(x) + q \\
  \frac{d^2p}{dx^2} &= \frac{dr}{dx}(x) \\
  \frac{dp}{dx} &= r(x) + c \\
  p(x) &= \int \left( r(x) + c \right) \, dx \\
\end{align*} L = L^* p(x) = \int r(x) \, dx u L^*(v) - v L(u) \begin{align*}
  u L^*(v) &= u p \frac{d^2v}{dx^2} + u \left( 2 \frac{dp}{dx} - r \right) \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) v \\
  v L(u) &= v p \frac{d^2u}{dx^2} + v r \frac{du}{dx} + v q u \\
  u L^*(v) - v L(u) &= p \left( u \frac{d^2v}{dx^2} - v \frac{d^2u}{dx^2} \right) + u \left( 2 \frac{dp}{dx} - r \right) \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} \right) v - v r \frac{du}{dx} \\
  &= p \left( u \frac{d^2v}{dx^2} - v \frac{d^2u}{dx^2} \right) + 2 u \frac{dp}{dx} \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} \right) v - r \left( u \frac{dv}{dx} + v \frac{du}{dx} \right) \\
\end{align*}","['ordinary-differential-equations', 'partial-differential-equations', 'adjoint-operators']"
4,Exercise 2.1. (chapter 1) of Hale (ODE),Exercise 2.1. (chapter 1) of Hale (ODE),,"I am sorry for the elementary question. I would like only to confirm if there isn't anything wrong with my solution. In Jack Hale's book on ODEs, pag. 17, exercise 2.1, he asks for the following. ""For $ t, x $ scalars, give na exemple of a function $ f(t,x) $ which is defined  and continuous on an open bounded connected set $ D $ and yet not every noncontinuable solution $ \phi $ of (1.1.) defined on $ (a,b) $ has $ \phi (a+0) $ , $ \phi (b-0 ) $ existing."" Obs.: (1.1) is the usual $ x'(t)=f(x,t) $ . Firstly, I found it hard to understand his notation of $ \phi (a+0) $ and $ \phi (b+0 ) $ . I looked to his previous results, and I still do not fully understand it. So, if you could explain to me, I would appreciate it. Secondly, I think that ""noncontinuable"" means a solution that could not be extended to $ a $ and $ b $ . If this is not the case, please correct my conceptual flaw. Thirdly, it seems to me that the $ f(x, t) $ equals to the derivative of $ \displaystyle\frac{1}{(t-a)(t-b)} $ would suffice. That is to say, $$ f(x,t)=\frac{a+b-2t}{(t-a)^2(t-b)^2} $$ Am I right? Fourthly, even if I am right, is there another interesting exemple. For instance, an example in which it would have a diferente behaviour? Finally, thank you very much for your attention. I like very much this platform. I have already used this platform, but only reading answers and questions of others as an undegrad student. This is my first time asking a question, and the first time I took the initiative of registering.","I am sorry for the elementary question. I would like only to confirm if there isn't anything wrong with my solution. In Jack Hale's book on ODEs, pag. 17, exercise 2.1, he asks for the following. ""For scalars, give na exemple of a function which is defined  and continuous on an open bounded connected set and yet not every noncontinuable solution of (1.1.) defined on has , existing."" Obs.: (1.1) is the usual . Firstly, I found it hard to understand his notation of and . I looked to his previous results, and I still do not fully understand it. So, if you could explain to me, I would appreciate it. Secondly, I think that ""noncontinuable"" means a solution that could not be extended to and . If this is not the case, please correct my conceptual flaw. Thirdly, it seems to me that the equals to the derivative of would suffice. That is to say, Am I right? Fourthly, even if I am right, is there another interesting exemple. For instance, an example in which it would have a diferente behaviour? Finally, thank you very much for your attention. I like very much this platform. I have already used this platform, but only reading answers and questions of others as an undegrad student. This is my first time asking a question, and the first time I took the initiative of registering."," t, x   f(t,x)   D   \phi   (a,b)   \phi (a+0)   \phi (b-0 )   x'(t)=f(x,t)   \phi (a+0)   \phi (b+0 )   a   b   f(x, t)   \displaystyle\frac{1}{(t-a)(t-b)}   f(x,t)=\frac{a+b-2t}{(t-a)^2(t-b)^2} ",['ordinary-differential-equations']
5,Book recommendation: Differential equations with differential geometry,Book recommendation: Differential equations with differential geometry,,"I have been doing some self-study of differential equations and have finished Habermans' elementary text on linear ordinary differential equations and about half of Strogatz's nonlinear differential equations book. The thing that I am noticing is just how much these text avoid engaging the underlying differential geometry/topology of phase spaces. It also feels like the further I got in this differential equations, the more important it is to understand the underlying differential topology--for instance understanding Hamiltonian systems and symplectic manifolds, etc. Indeed, the only text that I have seen that seems to engage the differential topology of phase spaces seems to be Arnold's 1973 book on Ordinary Differential Equations. This seems to be a really good book. The challenge is that Arnold can be a bit terse sometimes, so I was hoping to find a book to supplement Arnold's text. I have enough background in differential topology by watching Fredric Schuller's lectures and then working through some of Renteln and John Lee's books. Hence, I was hoping to find a book that elaborates on the differential topology side of differential equations. So all of these topics about vector fields on a manifold are fair game. Now I looked at Hirsch and Smale 1974, but this did not really get into the differential topology stuff. Perko's book was also pretty terse and did not systematically develop the topology. If anyone has any good recommendations, that would be appreciated. Thanks.","I have been doing some self-study of differential equations and have finished Habermans' elementary text on linear ordinary differential equations and about half of Strogatz's nonlinear differential equations book. The thing that I am noticing is just how much these text avoid engaging the underlying differential geometry/topology of phase spaces. It also feels like the further I got in this differential equations, the more important it is to understand the underlying differential topology--for instance understanding Hamiltonian systems and symplectic manifolds, etc. Indeed, the only text that I have seen that seems to engage the differential topology of phase spaces seems to be Arnold's 1973 book on Ordinary Differential Equations. This seems to be a really good book. The challenge is that Arnold can be a bit terse sometimes, so I was hoping to find a book to supplement Arnold's text. I have enough background in differential topology by watching Fredric Schuller's lectures and then working through some of Renteln and John Lee's books. Hence, I was hoping to find a book that elaborates on the differential topology side of differential equations. So all of these topics about vector fields on a manifold are fair game. Now I looked at Hirsch and Smale 1974, but this did not really get into the differential topology stuff. Perko's book was also pretty terse and did not systematically develop the topology. If anyone has any good recommendations, that would be appreciated. Thanks.",,"['ordinary-differential-equations', 'manifolds', 'differential-topology']"
6,Solve differential equation $u_t = i u_{xx} - x^2 u$,Solve differential equation,u_t = i u_{xx} - x^2 u,"Consider $u_t = i u_{xx} - x^2 u$ with $u_{t = 0} = 1$ . We want to find a solution. My attempt : let's say $u = X(x)T(t)$ , hence we have $\frac{T'(t)}{T(t)} = i \frac{X""(x)}{X(x)} - x^2$ . We may say that $\frac{T'(t)}{T(t)} = \lambda$ and $i \frac{X""(x)}{X(x)} - x^2 = \lambda$ , so $ \frac{X""(x)}{X(x)} =  -i(x^2 + \lambda)$ . But the second equation is give some problem. Maybe there is a better way to solve it? UPD : I actually think that this idea is bad, because if $u = X(x)T(t)$ , when $u(x,0) = X(x) = 1$","Consider with . We want to find a solution. My attempt : let's say , hence we have . We may say that and , so . But the second equation is give some problem. Maybe there is a better way to solve it? UPD : I actually think that this idea is bad, because if , when","u_t = i u_{xx} - x^2 u u_{t = 0} = 1 u = X(x)T(t) \frac{T'(t)}{T(t)} = i \frac{X""(x)}{X(x)} - x^2 \frac{T'(t)}{T(t)} = \lambda i \frac{X""(x)}{X(x)} - x^2 = \lambda  \frac{X""(x)}{X(x)} =  -i(x^2 + \lambda) u = X(x)T(t) u(x,0) = X(x) = 1","['ordinary-differential-equations', 'proof-verification', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
7,"Under Poincaré-Bendixson hypothesis, can a $\omega(p)$-limit set be like this picture?","Under Poincaré-Bendixson hypothesis, can a -limit set be like this picture?",\omega(p),"Last class we were proving Poincaré-Bendixson theorem in $\mathbb R^2$ which states that: Assume that the positive orbit $\mathcal O^+(p)$ is contained in a compact subset $K$ of the planar domain $D$ of the differential equation $x'=X(x)$ . Assume further that $X$ has only finitely many fixed points in $K$ . Then one of the following is satisfied: a) $\omega(p)$ is a periodic orbit; b) $\omega(p)$ is a single fixed point c) $\omega(p)$ consists of a finite number of fixed points, together with a finite set of orbits such that for each orbit its $\alpha$ -limit set is a single fixed point and its $\omega$ -limit set is also a single fixed point. During the proof, he stopped the class and drew the following picture in the blackboard: and asked, can $\omega(p)$ be like that? So I assume that he asked that in the context given by the hypothesis of Poincaré-Bendixson theorem. Also he intended to picture with those dots the singularities of the field. I want to say that the answer is no, because I think that every singularity must be connected by an orbit, which does not happen in this picture. But the problem is: I can't justify that. I've read the poincaré-bendixson demonstration quite a few times, but I can't find this justificative there. Any insight would be very helpful. Thank you","Last class we were proving Poincaré-Bendixson theorem in which states that: Assume that the positive orbit is contained in a compact subset of the planar domain of the differential equation . Assume further that has only finitely many fixed points in . Then one of the following is satisfied: a) is a periodic orbit; b) is a single fixed point c) consists of a finite number of fixed points, together with a finite set of orbits such that for each orbit its -limit set is a single fixed point and its -limit set is also a single fixed point. During the proof, he stopped the class and drew the following picture in the blackboard: and asked, can be like that? So I assume that he asked that in the context given by the hypothesis of Poincaré-Bendixson theorem. Also he intended to picture with those dots the singularities of the field. I want to say that the answer is no, because I think that every singularity must be connected by an orbit, which does not happen in this picture. But the problem is: I can't justify that. I've read the poincaré-bendixson demonstration quite a few times, but I can't find this justificative there. Any insight would be very helpful. Thank you",\mathbb R^2 \mathcal O^+(p) K D x'=X(x) X K \omega(p) \omega(p) \omega(p) \alpha \omega \omega(p),"['ordinary-differential-equations', 'dynamical-systems']"
8,Show that there is no vector field with transitive orbit on $\mathbb{R}^{2}$,Show that there is no vector field with transitive orbit on,\mathbb{R}^{2},"I need to show that there is no vector field with transitive orbit on $\mathbb{R}^{2}$ . Let $X:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2}$ a $C^{k}, k\geq 1$ , vector field in $\mathbb{R}^{2}$ . The integral curves of $X$ are the solutions $\phi_{p}(t)$ of the system $x'=X(x)$ through the point $p\in\mathbb{R}^{2}$ defined on its maximal interval $I_{p}$ The orbits of $X$ are the sets $\gamma_{p}=\{\phi_{p}(t):t\in I_{p}\}$ . A transitive orbit of $X$ is a dense orbit of $X$ , i.e., $\bar{\gamma_{p}}=\mathbb{R}^{2}$ . Is there some theorem that I can use to solve my question? I tried to suppose that $X$ has dense orbit and get a contradiction, but I didn't got anything.","I need to show that there is no vector field with transitive orbit on . Let a , vector field in . The integral curves of are the solutions of the system through the point defined on its maximal interval The orbits of are the sets . A transitive orbit of is a dense orbit of , i.e., . Is there some theorem that I can use to solve my question? I tried to suppose that has dense orbit and get a contradiction, but I didn't got anything.","\mathbb{R}^{2} X:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2} C^{k}, k\geq 1 \mathbb{R}^{2} X \phi_{p}(t) x'=X(x) p\in\mathbb{R}^{2} I_{p} X \gamma_{p}=\{\phi_{p}(t):t\in I_{p}\} X X \bar{\gamma_{p}}=\mathbb{R}^{2} X","['ordinary-differential-equations', 'vector-fields']"
9,Oscillations of an Energy Eigenstate,Oscillations of an Energy Eigenstate,,"Energy eigenstates of a 1-dimensional particle are given by solutions to differential equations of the form $$ \left(-\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + V(x) \right) \psi(x) = E\psi(x) $$ where $V$ is the potential, $\psi$ is the energy eigenstate, and $E$ is the energy eigenvalue. In introductory (and even fairly advanced) texts on quantum mechanics, the following facts are asserted without proof: The set $\{E \in \mathbb{R} : E \text{ is an energy eigenvalue and } E < \sup_{x \in \mathbb{R}} V(x) \}$ is a discrete subset of the reals bounded below by $\inf_{x \in \mathbb{R}} V(x)$ . If the discrete energy eigenvalues above are listed in ascending order as $E_0, \ldots, E_n$ and have corresponding energy eigenstates $\psi_0, \ldots, \psi_n$ , then $\psi_n$ will have $n + 1$ local maxima. The above results are often stated with less precision, and sometimes even conflict with each other. What statements like 1 and 2 above are actually true, and how does one go about proving them?","Energy eigenstates of a 1-dimensional particle are given by solutions to differential equations of the form where is the potential, is the energy eigenstate, and is the energy eigenvalue. In introductory (and even fairly advanced) texts on quantum mechanics, the following facts are asserted without proof: The set is a discrete subset of the reals bounded below by . If the discrete energy eigenvalues above are listed in ascending order as and have corresponding energy eigenstates , then will have local maxima. The above results are often stated with less precision, and sometimes even conflict with each other. What statements like 1 and 2 above are actually true, and how does one go about proving them?","
\left(-\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + V(x) \right) \psi(x) = E\psi(x)
 V \psi E \{E \in \mathbb{R} : E \text{ is an energy eigenvalue and } E < \sup_{x \in \mathbb{R}} V(x) \} \inf_{x \in \mathbb{R}} V(x) E_0, \ldots, E_n \psi_0, \ldots, \psi_n \psi_n n + 1","['ordinary-differential-equations', 'mathematical-physics', 'quantum-mechanics']"
10,The role of eigenvalue multiplicity in bifurcations,The role of eigenvalue multiplicity in bifurcations,,"Consider $\dot x =f(x)$ where $x\in\mathbb{R}^n$ and $f:\mathbb{R}^n\to\mathbb{R}^n$ is smooth enough. I am studying Fold and Hopf bifurcations in such ordinary differential equations. We know that a bifurcation happens when we break the hyperbolicity of the Jacobian matrix $\frac{\partial f}{\partial x}$ . Now, if we break the hyperbolicity via one (or several) zero eigenvalue(s), Fold bifurcation takes place. Moreover, if we we break the hyperbolicity via one (or several) pair(s) of pure imaginary eigenvalue, Hopf takes place. I was wondering if there is any relationship between the algebraic/geometric multiplicty of these eigenvalues and the bifurcations (e.g., their stability, type, etc.)? Any comment or response is greatly appreciated!","Consider where and is smooth enough. I am studying Fold and Hopf bifurcations in such ordinary differential equations. We know that a bifurcation happens when we break the hyperbolicity of the Jacobian matrix . Now, if we break the hyperbolicity via one (or several) zero eigenvalue(s), Fold bifurcation takes place. Moreover, if we we break the hyperbolicity via one (or several) pair(s) of pure imaginary eigenvalue, Hopf takes place. I was wondering if there is any relationship between the algebraic/geometric multiplicty of these eigenvalues and the bifurcations (e.g., their stability, type, etc.)? Any comment or response is greatly appreciated!",\dot x =f(x) x\in\mathbb{R}^n f:\mathbb{R}^n\to\mathbb{R}^n \frac{\partial f}{\partial x},"['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems', 'control-theory', 'bifurcation']"
11,Synchronization in Coupled Nonlinear Oscillators,Synchronization in Coupled Nonlinear Oscillators,,"I hope with this first question I respect the standards of the forum :) I am currently working on an ANN model for Working Memory (Short Term memory in the Brain) and as a first step I am studying a reduced dimensional system. Unfortunately, I am kind of new to the theory of Nonlinear oscillators, therefore I have some doubts about how to approach the problem. I attach here the equations and what I found at the moment about the system and what I think it would be interesting to analyze. I have two 2-dimensional units that interacts with their outputs and synchronize on a limit cycle. $\dot d_1 = - d_1 -e_1 + \epsilon\sigma(d_2) \\ \tau_a \dot e_1 = g_a\sigma(d_1) - e_1 \\$ $\dot d_2 = - d_2 -e_2 + \epsilon\sigma(d_1) \\ \tau_a \dot e_2 = g_a\sigma(d_2) - e_2$ $\sigma(d_i) = 1 - \frac{2}{1+e^d}$ From some simple simulations one can see that the system above synchronize ( $||d_1-d_2||->0$ ) on a stable limit cycle. Limit_Cycle , Synch Now we can go with the questions: My intuition suggested me that to study the limit cycle's properties (with Poincaré-Bendixsson theorem for example) I can study the system on the synchronization manifold where $d_1=d_2$ . On this invariant manifold with with a fairly simple bifurcation analysis, one can study how the system's behaviour changes with changing the parameters that define the dynamics Q1) In order to study the synchronization of the two systems, what could be a good approach? I read several works but most of them are on systems that are diffusively coupled (the coupling depends on a function of the difference of the output/states of the system). Updates One idea could be to study the error dynamics. By defining the quantities $x = d_1 - d_2$ and $y = e_1 - e_2$ we obtain: $\dot x = - x - y + \epsilon(\sigma(d_2)-\sigma(d_1)) \\ \tau_a \dot y = g_a(\sigma(d_1)-\sigma(d_2)) - y \\$ The main issue is to 'get rid of' the quantity $(\sigma(d_2)-\sigma(d_1))$ and replace it with some function $f(x)$ of the only difference. Unfortunately, this is not possible since: $a(d_1, d_2) = \sigma(d_2)-\sigma(d_1) = \frac{e^{d_2}-e^{d_1}}{(1+e^{d_1})(1+e^{d_2})}$ Although the function $a(d_1, d_2)$ can not be expressed only as a function of the difference $x$ it could be interesting to see $a(d_1, d_2)$ as a time-varying function $f(x, t)$ that preserves some properties, for example $x f(x,t)<0, \forall x\neq0$ . At this point if one finds the right tools the GAS of the synchronization manifold could be proven. (Maybe some passivity analysis or similar for time-varying systems) (If somebody is interested in simulating the system by themselves, parameters that could be used are: $\tau_a = 2.7, g_a=97, \epsilon>2(1+1/\tau_a)$ )","I hope with this first question I respect the standards of the forum :) I am currently working on an ANN model for Working Memory (Short Term memory in the Brain) and as a first step I am studying a reduced dimensional system. Unfortunately, I am kind of new to the theory of Nonlinear oscillators, therefore I have some doubts about how to approach the problem. I attach here the equations and what I found at the moment about the system and what I think it would be interesting to analyze. I have two 2-dimensional units that interacts with their outputs and synchronize on a limit cycle. From some simple simulations one can see that the system above synchronize ( ) on a stable limit cycle. Limit_Cycle , Synch Now we can go with the questions: My intuition suggested me that to study the limit cycle's properties (with Poincaré-Bendixsson theorem for example) I can study the system on the synchronization manifold where . On this invariant manifold with with a fairly simple bifurcation analysis, one can study how the system's behaviour changes with changing the parameters that define the dynamics Q1) In order to study the synchronization of the two systems, what could be a good approach? I read several works but most of them are on systems that are diffusively coupled (the coupling depends on a function of the difference of the output/states of the system). Updates One idea could be to study the error dynamics. By defining the quantities and we obtain: The main issue is to 'get rid of' the quantity and replace it with some function of the only difference. Unfortunately, this is not possible since: Although the function can not be expressed only as a function of the difference it could be interesting to see as a time-varying function that preserves some properties, for example . At this point if one finds the right tools the GAS of the synchronization manifold could be proven. (Maybe some passivity analysis or similar for time-varying systems) (If somebody is interested in simulating the system by themselves, parameters that could be used are: )","\dot d_1 = - d_1 -e_1 + \epsilon\sigma(d_2) \\
\tau_a \dot e_1 = g_a\sigma(d_1) - e_1 \\ \dot d_2 = - d_2 -e_2 + \epsilon\sigma(d_1) \\
\tau_a \dot e_2 = g_a\sigma(d_2) - e_2 \sigma(d_i) = 1 - \frac{2}{1+e^d} ||d_1-d_2||->0 d_1=d_2 x = d_1 - d_2 y = e_1 - e_2 \dot x = - x - y + \epsilon(\sigma(d_2)-\sigma(d_1)) \\
\tau_a \dot y = g_a(\sigma(d_1)-\sigma(d_2)) - y \\ (\sigma(d_2)-\sigma(d_1)) f(x) a(d_1, d_2) = \sigma(d_2)-\sigma(d_1) = \frac{e^{d_2}-e^{d_1}}{(1+e^{d_1})(1+e^{d_2})} a(d_1, d_2) x a(d_1, d_2) f(x, t) x f(x,t)<0, \forall x\neq0 \tau_a = 2.7, g_a=97, \epsilon>2(1+1/\tau_a)","['ordinary-differential-equations', 'control-theory', 'nonlinear-system', 'lyapunov-functions', 'synchronization']"
12,Solving Nonlinear differential equation system,Solving Nonlinear differential equation system,,"So, I've been trying to calculate the motion of a Frisbee accounting for drag, lift, and weight, but have reached the nonlinear differential equation system $$ \begin{align} x'&=\frac{-Dx^3}{m\sqrt{x^2+y^2}}\\ y'&=\frac{Ax}{m}+\frac{Dx^2y}{m\sqrt{x^2+y^2}}-g \end{align} $$ where $m,D,A,g$ and constants with initial conditions $y(0)=0$ and $x(0)=x_0$ Any ideas on how to solve it? Thanks!","So, I've been trying to calculate the motion of a Frisbee accounting for drag, lift, and weight, but have reached the nonlinear differential equation system where and constants with initial conditions and Any ideas on how to solve it? Thanks!","
\begin{align}
x'&=\frac{-Dx^3}{m\sqrt{x^2+y^2}}\\
y'&=\frac{Ax}{m}+\frac{Dx^2y}{m\sqrt{x^2+y^2}}-g
\end{align}
 m,D,A,g y(0)=0 x(0)=x_0",['ordinary-differential-equations']
13,Analytical Solution to Nonlinear Second Order ODE,Analytical Solution to Nonlinear Second Order ODE,,"I'm trying to solve the following nonlinear second order ODE where $a$ and $b$ are constants: $$\frac{d^2y}{dx^2}+\frac{1}{x}\frac{dy}{dx}-\frac{y}{ay+b}=0$$ It looks somewhat like the modified Bessel equation , except the third term on the left makes it nonlinear. I've been trying to determine some way to find an analytical solution but haven't been able to come up with anything. It doesn't help much but it can also be written: $$\frac{1}{x}\frac{d}{dx}\left(x\frac{dy}{dx}\right)=\frac{y}{ay+b}$$ Any suggestions would be greatly appreciated, thanks!","I'm trying to solve the following nonlinear second order ODE where and are constants: It looks somewhat like the modified Bessel equation , except the third term on the left makes it nonlinear. I've been trying to determine some way to find an analytical solution but haven't been able to come up with anything. It doesn't help much but it can also be written: Any suggestions would be greatly appreciated, thanks!",a b \frac{d^2y}{dx^2}+\frac{1}{x}\frac{dy}{dx}-\frac{y}{ay+b}=0 \frac{1}{x}\frac{d}{dx}\left(x\frac{dy}{dx}\right)=\frac{y}{ay+b},"['ordinary-differential-equations', 'nonlinear-system']"
14,differential equation of the explicit RMS function,differential equation of the explicit RMS function,,"This is my first time posting on any math forum, let alone stackexchange, so I do hope I'm doing everything correct! Some Background I'm an engineer, and not a mathematician, although I do enjoy maths which is the reason for this question. In my own time I've been looking at RMS - DC converters within the field of electronics, essentially they take an input of an AC signal, and output a DC voltage. The signal should, ideally, have a mean equal to the RMS of the input signal. For example, a 1Vrms sine wave input should produce a DC voltage of 1V. Due to the nature of the conversion, the output is a signal with a small amount of AC content (ripple) and also a DC offset (due to attenuation, resulting from a low pass filter used to implement the moving average). I've been attempting to derive this error mathematically - which is certainly nothing new. However, it seems i'm struggling with the maths and I hope someone here can help! The Problem Determine the error from an RMS-DC converter. There is a paper available on IEEE Xplore (which I have and can upload here if that is allowed?) that details one method of determining this error. There are two methods of determining RMS, implicit and explicit. The explicit is easier for me to understand so I hope someone can help me with the following: $V_{RMS}(t) = \sqrt{AVG(V^2_{in}(t))}$ In electronics, the average is done with an operational amplifier, which gives: $V_{RMS}(t) = \sqrt{-\frac{1}{\tau}\int_{0}^{t}V_{in}(t)^2\delta t}$ The minus sign is due to the practical implementation of the operational amplifier being setup in an inverting operation (input signal is entering the inverting pin (-) on the operational amplifier) Conventionally, $V_{RMS}()$ is used as the signal, representing voltage. However, the paper uses $e_o(t)$ , presumably representing the error, so I will continue with their nomenclature from now on: $V_{RMS}(t) == e_o(t)$ and $V_{in}(t) == e_i(t)$ The paper shows an example circuit: Example circuit To which it goes on to say that ""it is easy to show that the differential equation for $e_o^2(t)$ can be written as..."" (unfortunately, not so easy for me!): $\tau \frac{\delta e_o^2(t)}{\delta t} + e_o^2(t) = e_i^2(t)$ The paper continues to derive the DC error (through, as I understand, solving the differential equation, and then using some series expansion, and taking the DC term). however, I fail at the first hurdle and can't seem to get the differential equation. My Workings So Far From the definition of RMS, I derived what I thought was the differential equation by (I believe it's called) implicit differentiation as follows: $e_{o}(t) = \sqrt{- \frac{1}{\tau}\int^t_0{e_i^2(t)}\delta t}$ Where $t$ is the time-varying quantity, and $\tau$ is the Resistor-Capacitor time constant, introduced by the R and C in the circuit. Squaring both sides: $e^2_{o}(t) = - \frac{1}{\tau}\int^t_0{e_i^2(t)}\delta t$ and then (implicit?) differentiation: $\frac{\delta}{\delta t} e^2_{o}(t) = - \frac{1}{\tau} \frac{\delta}{\delta t}[\int^t_0{e_i^2(t)}\delta t] $ giving: $\frac{\delta}{\delta t} e^2_{o}(t) = - \frac{1}{\tau} {e_i^2(t)} $ $\tau \frac{\delta e^2_{o}(t)}{\delta t} = - e_i^2(t) $ Which, of course is not the same as the author in the paper. Can anyone see where I have gone wrong? I understand this is quite a simple question and probably didn't need so much detail, but I thought I would include just in case it's an assumption that's causing the wrong answer. Thanks in advance!","This is my first time posting on any math forum, let alone stackexchange, so I do hope I'm doing everything correct! Some Background I'm an engineer, and not a mathematician, although I do enjoy maths which is the reason for this question. In my own time I've been looking at RMS - DC converters within the field of electronics, essentially they take an input of an AC signal, and output a DC voltage. The signal should, ideally, have a mean equal to the RMS of the input signal. For example, a 1Vrms sine wave input should produce a DC voltage of 1V. Due to the nature of the conversion, the output is a signal with a small amount of AC content (ripple) and also a DC offset (due to attenuation, resulting from a low pass filter used to implement the moving average). I've been attempting to derive this error mathematically - which is certainly nothing new. However, it seems i'm struggling with the maths and I hope someone here can help! The Problem Determine the error from an RMS-DC converter. There is a paper available on IEEE Xplore (which I have and can upload here if that is allowed?) that details one method of determining this error. There are two methods of determining RMS, implicit and explicit. The explicit is easier for me to understand so I hope someone can help me with the following: In electronics, the average is done with an operational amplifier, which gives: The minus sign is due to the practical implementation of the operational amplifier being setup in an inverting operation (input signal is entering the inverting pin (-) on the operational amplifier) Conventionally, is used as the signal, representing voltage. However, the paper uses , presumably representing the error, so I will continue with their nomenclature from now on: and The paper shows an example circuit: Example circuit To which it goes on to say that ""it is easy to show that the differential equation for can be written as..."" (unfortunately, not so easy for me!): The paper continues to derive the DC error (through, as I understand, solving the differential equation, and then using some series expansion, and taking the DC term). however, I fail at the first hurdle and can't seem to get the differential equation. My Workings So Far From the definition of RMS, I derived what I thought was the differential equation by (I believe it's called) implicit differentiation as follows: Where is the time-varying quantity, and is the Resistor-Capacitor time constant, introduced by the R and C in the circuit. Squaring both sides: and then (implicit?) differentiation: giving: Which, of course is not the same as the author in the paper. Can anyone see where I have gone wrong? I understand this is quite a simple question and probably didn't need so much detail, but I thought I would include just in case it's an assumption that's causing the wrong answer. Thanks in advance!",V_{RMS}(t) = \sqrt{AVG(V^2_{in}(t))} V_{RMS}(t) = \sqrt{-\frac{1}{\tau}\int_{0}^{t}V_{in}(t)^2\delta t} V_{RMS}() e_o(t) V_{RMS}(t) == e_o(t) V_{in}(t) == e_i(t) e_o^2(t) \tau \frac{\delta e_o^2(t)}{\delta t} + e_o^2(t) = e_i^2(t) e_{o}(t) = \sqrt{- \frac{1}{\tau}\int^t_0{e_i^2(t)}\delta t} t \tau e^2_{o}(t) = - \frac{1}{\tau}\int^t_0{e_i^2(t)}\delta t \frac{\delta}{\delta t} e^2_{o}(t) = - \frac{1}{\tau} \frac{\delta}{\delta t}[\int^t_0{e_i^2(t)}\delta t]  \frac{\delta}{\delta t} e^2_{o}(t) = - \frac{1}{\tau} {e_i^2(t)}  \tau \frac{\delta e^2_{o}(t)}{\delta t} = - e_i^2(t) ,"['integration', 'ordinary-differential-equations', 'derivatives', 'definite-integrals']"
15,Is 2nd-order ODE with quadratic coefficients solvable?,Is 2nd-order ODE with quadratic coefficients solvable?,,"Consider an ODE eigensystem $$\begin{bmatrix} 0 & d_1-\mathrm id_2 \\ d_1+\mathrm id_2 & 0  \end{bmatrix}  \begin{bmatrix}  a(y) \\ b(y)  \end{bmatrix} = \lambda  \begin{bmatrix}  a(y) \\ b(y)  \end{bmatrix}, $$ where $$d_1=-\mathrm i(p+qy)\partial_y+ry+s$$ $$d_2=-\mathrm i(u+vy)\partial_y+wy+t,$$ $p,q,r,s,u,v,w,t$ are just real constants, and $\mathrm i$ is the imaginary unit. Is it analytically solvable? I reduce it to a 2nd-order ODE of $b$ with coefficients quadratic in $y$ $$\alpha b''(y) + \beta b'(y) + \gamma b(y)=-\lambda^2 b(y)$$ where $$\alpha=(p+q y)^2+(u+v y)^2$$ $$\beta=p (q+2 i s-i v)+u (v+iq+2 it)+(2 i p r+q^2+2 i q s+2 i t v+2 i u w+v^2)y+2 i (q r+v w)y^2 $$ $$\gamma=-s^2-t^2+(p+i u) (w+i r)+[w (q-2 t+i v)-r (-i q+2 s+v)]y-(r^2+w^2)y^2 $$ When $u,v=0$ or $p,q=0$ , it is solvable, although the coefficients are still quadratic polynomials of $y$ . I was wondering if the more general case could be tackled as well. But I don't know how to proceed.","Consider an ODE eigensystem where are just real constants, and is the imaginary unit. Is it analytically solvable? I reduce it to a 2nd-order ODE of with coefficients quadratic in where When or , it is solvable, although the coefficients are still quadratic polynomials of . I was wondering if the more general case could be tackled as well. But I don't know how to proceed.","\begin{bmatrix}
0 & d_1-\mathrm id_2 \\
d_1+\mathrm id_2 & 0 
\end{bmatrix} 
\begin{bmatrix}  a(y) \\ b(y)  \end{bmatrix} = \lambda  \begin{bmatrix}  a(y) \\ b(y)  \end{bmatrix},
 d_1=-\mathrm i(p+qy)\partial_y+ry+s d_2=-\mathrm i(u+vy)\partial_y+wy+t, p,q,r,s,u,v,w,t \mathrm i b y \alpha b''(y) + \beta b'(y) + \gamma b(y)=-\lambda^2 b(y) \alpha=(p+q y)^2+(u+v y)^2 \beta=p (q+2 i s-i v)+u (v+iq+2 it)+(2 i p r+q^2+2 i q s+2 i t v+2 i u w+v^2)y+2 i (q r+v w)y^2  \gamma=-s^2-t^2+(p+i u) (w+i r)+[w (q-2 t+i v)-r (-i q+2 s+v)]y-(r^2+w^2)y^2  u,v=0 p,q=0 y","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'sturm-liouville']"
16,Explicit Euler method for Fokker-Planck equation,Explicit Euler method for Fokker-Planck equation,,"I'm trying to obtain an approximation of the solution of the following equation: $$ \left\lbrace \begin{array}{l,l} u_t = \alpha u_{xx} + (\beta u)_x,  & u,\alpha ,\beta \in [T_0,T_f]\times [X_0,X_f]\\ u(t=T_0,x) = u_0(x), & \forall x \in [X_0,X_f] \\ u(t,x=X_0) = g_0(t), u(t,x=X_f) = g_f(t), & \forall t \in [T_0,Tf] \end{array}\right. $$ With $u_t = \frac{\partial u}{\partial t}$ , $(\beta u)_x = \frac{\partial (\beta u)}{\partial x}$ and $u_{xx} = \frac{\partial^2 u}{(\partial x)^2}$ . The thing is, there must be something wrong since whenever $\beta$ is negative the approximations end up blowing up several times (so far it goes well for as long as this is avoided). I'll explain what I used for obtaining this approximations in case you can see if there's something I missed or simply is mistaken. I used a finite difference method, by first applying the method of lines leaving the $t$ variable continuous as $x$ is discretized with $N+2$ nodes as follows: $$ \begin{array}{l} \Delta x = \frac{X_f - X_0}{N+1} \\ x_j = X_0 + j\Delta x,\hspace{.2cm} j \in \lbrace0,1,...,N+1\rbrace \\ u_j(t) \simeq u(t,x_j) \end{array} $$ Using central difference for $u_{xx}(x)$ ( $u_{xx}\simeq \frac{u(x+\Delta x) - 2u(x) + u(x-\Delta x)}{(\Delta x)^2}$ ) the equation results as follows: $$ \frac{\partial u_j}{\partial t}(t) = \alpha(t,x_j)\frac{u_{j+1} - 2u_j + u_{j-1}}{(\Delta x)^2}(t) + \beta(t,x_j)\frac{u_{j+1}-u_{j-1}}{2\Delta x}(t) + \beta_x(t,x_j) u_j(t) = \\ = u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right) $$ Naming $a_1(t,x_j) = \frac{\alpha(t,x_j)}{(\Delta x)^2} + \frac{\beta(t,x_j)}{2\Delta x}$ , $a_2(t,x_j) = \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2}$ and $a_3(t,x_j) = \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x}$ the following system of equations is obtained: $$ U_t(t) = A(t)U(t) $$ With: $$ \begin{array}{l}  U_t(t) \simeq (u_t(t,x_1),...,u_t(t,x_N))' \\  U = (u_1(t),...,u_N(t))' \\  g = (a_1(t,x_1)g_0(t),0,0,...,0,a_3(t,x_N)g_f(t))' \end{array} $$ All of them being $N\times 1$ . A is the following $N\times N$ matrix: $$ \left(\begin{array}{c,c,c,c,c,c,c,c} a_2(t,x1) & a_3(t,x_1) & 0 & 0 & ... & ...& ... & 0 \\ a_1(t,x_2) & a_2(t,x_2) & a_3(t,x_2) & 0 & ... & ... & ... & 0 \\ 0 & a_1(t,x_3) & a_2(t,x_3) & a_3(t,x_3) & 0 & ... & ... & 0 \\ 0 & 0 & \ddots & \ddots & \ddots & 0 & ... & 0 \\ \vdots & 0 & 0 & \ddots & \ddots & \ddots & ... & \vdots \\ 0 & ... & ... & 0 & 0 & a_1(t,x_{N-1}) & a_2(t,x_{N-1}) & a_3(t,x_{N-1}) \\ 0 & 0 & ... & ... & ... & 0 & a_1(t,x_N) & a_2(t,x_N) \end{array}\right) $$ Then I discretized time with $M+1$ nodes as follows: $$ \begin{array}{l} \Delta t = \frac{T_f - T_0}{M}\\ t_i = T_0 + i\Delta t, \hspace{.2cm} i \in \lbrace 0, 1, ..., M\rbrace \\ u_j^i \simeq u(t_i,x_j) \end{array} $$ Applying forward Euler ( $\frac{\partial u_j^i}{\partial t} \simeq \frac{u_j^{i+1} - u_j^i}{\Delta t}$ ) to the equation obtained after space discretization: $$ \frac{u_j^{i+1} - u_j^i}{\Delta t} = u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right) $$ Hence, we can state $u_j^{i+1}$ as: $$ u_j^{i+1} = u_j^i + \Delta t \left(u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + \\ u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right)\right) $$ With the following notation: $$ \begin{array}{l} U^i = (u_1^i,u_2^i,...,u_N^i)' \\ h^i = (a_1(t_i,x_ 1)g_0(t_i)\Delta t,0,...,0,a_3(t_i,x_N)g_f(t_i)\Delta t )'  \end{array} $$ $U^i$ and $h^i$ with dimensions $N \times 1$ . Considering $I_N$ the $N \times N$ identity matrix we have: $$ U^{i+1} = \left(I_N + \Delta t A(t_i)\right)U^i + h^i, \hspace{.2cm} 1 \leq i \leq N  $$ I plotted the approximations at some time values and, as I said before, whenever $\beta$ has negative values several blow ups appear. I would like to know if there's something wrong with the method I used or mistakes during the process so I can determine whether the error lies in the method or the code. Thank you in advance! P.S: The discretizations are taken in a way the Courant-Friedrichs-Levy condition is satisfied, that is not the issue.","I'm trying to obtain an approximation of the solution of the following equation: With , and . The thing is, there must be something wrong since whenever is negative the approximations end up blowing up several times (so far it goes well for as long as this is avoided). I'll explain what I used for obtaining this approximations in case you can see if there's something I missed or simply is mistaken. I used a finite difference method, by first applying the method of lines leaving the variable continuous as is discretized with nodes as follows: Using central difference for ( ) the equation results as follows: Naming , and the following system of equations is obtained: With: All of them being . A is the following matrix: Then I discretized time with nodes as follows: Applying forward Euler ( ) to the equation obtained after space discretization: Hence, we can state as: With the following notation: and with dimensions . Considering the identity matrix we have: I plotted the approximations at some time values and, as I said before, whenever has negative values several blow ups appear. I would like to know if there's something wrong with the method I used or mistakes during the process so I can determine whether the error lies in the method or the code. Thank you in advance! P.S: The discretizations are taken in a way the Courant-Friedrichs-Levy condition is satisfied, that is not the issue.","
\left\lbrace \begin{array}{l,l}
u_t = \alpha u_{xx} + (\beta u)_x,  & u,\alpha ,\beta \in [T_0,T_f]\times [X_0,X_f]\\
u(t=T_0,x) = u_0(x), & \forall x \in [X_0,X_f] \\
u(t,x=X_0) = g_0(t), u(t,x=X_f) = g_f(t), & \forall t \in [T_0,Tf]
\end{array}\right.
 u_t = \frac{\partial u}{\partial t} (\beta u)_x = \frac{\partial (\beta u)}{\partial x} u_{xx} = \frac{\partial^2 u}{(\partial x)^2} \beta t x N+2 
\begin{array}{l}
\Delta x = \frac{X_f - X_0}{N+1} \\
x_j = X_0 + j\Delta x,\hspace{.2cm} j \in \lbrace0,1,...,N+1\rbrace \\
u_j(t) \simeq u(t,x_j)
\end{array}
 u_{xx}(x) u_{xx}\simeq \frac{u(x+\Delta x) - 2u(x) + u(x-\Delta x)}{(\Delta x)^2} 
\frac{\partial u_j}{\partial t}(t) = \alpha(t,x_j)\frac{u_{j+1} - 2u_j + u_{j-1}}{(\Delta x)^2}(t) + \beta(t,x_j)\frac{u_{j+1}-u_{j-1}}{2\Delta x}(t) + \beta_x(t,x_j) u_j(t) = \\
= u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right)
 a_1(t,x_j) = \frac{\alpha(t,x_j)}{(\Delta x)^2} + \frac{\beta(t,x_j)}{2\Delta x} a_2(t,x_j) = \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} a_3(t,x_j) = \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} 
U_t(t) = A(t)U(t)
 
\begin{array}{l}
 U_t(t) \simeq (u_t(t,x_1),...,u_t(t,x_N))' \\
 U = (u_1(t),...,u_N(t))' \\
 g = (a_1(t,x_1)g_0(t),0,0,...,0,a_3(t,x_N)g_f(t))'
\end{array}
 N\times 1 N\times N 
\left(\begin{array}{c,c,c,c,c,c,c,c}
a_2(t,x1) & a_3(t,x_1) & 0 & 0 & ... & ...& ... & 0 \\
a_1(t,x_2) & a_2(t,x_2) & a_3(t,x_2) & 0 & ... & ... & ... & 0 \\
0 & a_1(t,x_3) & a_2(t,x_3) & a_3(t,x_3) & 0 & ... & ... & 0 \\
0 & 0 & \ddots & \ddots & \ddots & 0 & ... & 0 \\
\vdots & 0 & 0 & \ddots & \ddots & \ddots & ... & \vdots \\
0 & ... & ... & 0 & 0 & a_1(t,x_{N-1}) & a_2(t,x_{N-1}) & a_3(t,x_{N-1}) \\
0 & 0 & ... & ... & ... & 0 & a_1(t,x_N) & a_2(t,x_N)
\end{array}\right)
 M+1 
\begin{array}{l}
\Delta t = \frac{T_f - T_0}{M}\\
t_i = T_0 + i\Delta t, \hspace{.2cm} i \in \lbrace 0, 1, ..., M\rbrace \\
u_j^i \simeq u(t_i,x_j)
\end{array}
 \frac{\partial u_j^i}{\partial t} \simeq \frac{u_j^{i+1} - u_j^i}{\Delta t} 
\frac{u_j^{i+1} - u_j^i}{\Delta t} = u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right)
 u_j^{i+1} 
u_j^{i+1} = u_j^i + \Delta t \left(u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + \\ u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right)\right)
 
\begin{array}{l}
U^i = (u_1^i,u_2^i,...,u_N^i)' \\
h^i = (a_1(t_i,x_ 1)g_0(t_i)\Delta t,0,...,0,a_3(t_i,x_N)g_f(t_i)\Delta t )' 
\end{array}
 U^i h^i N \times 1 I_N N \times N 
U^{i+1} = \left(I_N + \Delta t A(t_i)\right)U^i + h^i, \hspace{.2cm} 1 \leq i \leq N 
 \beta","['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'finite-differences']"
17,How to find a specific curve if the initial value is not given?,How to find a specific curve if the initial value is not given?,,"Question: Let $y(x)$ be the solution of the differential equation $x\cdot ln(x)\dfrac{dy}{dx}+y=2x\cdot ln(x)$ , $x\ge1$ . Find $y(e)$ . Answer : $y(e) = 2$ Problem: So I understand that this can be converted into a simple linear differential equation and found that the solution is: $y\cdot ln(x)=2(x\cdot ln(x) - x) + C$ This is a family of curves. However for solving the question, I need a specific curve out of all these. What I don't understand is how how do I find that particular curve  as the initial value of the function is not given.","Question: Let be the solution of the differential equation , . Find . Answer : Problem: So I understand that this can be converted into a simple linear differential equation and found that the solution is: This is a family of curves. However for solving the question, I need a specific curve out of all these. What I don't understand is how how do I find that particular curve  as the initial value of the function is not given.",y(x) x\cdot ln(x)\dfrac{dy}{dx}+y=2x\cdot ln(x) x\ge1 y(e) y(e) = 2 y\cdot ln(x)=2(x\cdot ln(x) - x) + C,"['calculus', 'ordinary-differential-equations']"
18,"Transformation from ODE or PDE via operator to ""easier form"" - what is he background?","Transformation from ODE or PDE via operator to ""easier form"" - what is he background?",,"I know a bit about how integral transformations can be applied to differential equations. ODEs can be transformed under some circumstances to algebraic equations and PDEs can be transformed under some circumstances to ordinary differential equations.  My question is very fundamental and so unfortunately not very precise... What is this sort of mathematics which is transforming ""more complicated objects"" to ""easier objects"", especially via those transforms? Is there a ""transform mathematic"" behind it?  Thanks for hints and sorry for strange wording, I am not a mathematician. Best,  Jens","I know a bit about how integral transformations can be applied to differential equations. ODEs can be transformed under some circumstances to algebraic equations and PDEs can be transformed under some circumstances to ordinary differential equations.  My question is very fundamental and so unfortunately not very precise... What is this sort of mathematics which is transforming ""more complicated objects"" to ""easier objects"", especially via those transforms? Is there a ""transform mathematic"" behind it?  Thanks for hints and sorry for strange wording, I am not a mathematician. Best,  Jens",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'laplace-transform']"
19,Differentiability of solution to ODE,Differentiability of solution to ODE,,"Consider the problem $$\frac{d X(t,x)}{dt} = f(t, X(t,x))$$ $$X(0,x) = x$$ where $f:[0,T]\times \mathbb{R}^n \to \mathbb{R}^n$ and $X:[0,T]\times \mathbb{R}^n \to \mathbb{R}^n$ . Assume that $f$ is Holder continuous or Lipschitz continuous. How can I prove and where can I find a reference for the fact that $X$ is differentiable and $$\nabla X$$ is the solution to $$\frac{d}{dt} \nabla X(t,x) = \nabla f(t,X(t,x)) \nabla X(t,x)?$$ Do we need to assume that $f$ is differentiable of does the Lipschitz continuity suffice? Also, does the formula $$\nabla X = e^{\int_0^T\nabla f}$$ hold if at least $f \in L^1((0,T),W^{1,1}(\mathbb{R}^n))$ for example? If not, why?","Consider the problem where and . Assume that is Holder continuous or Lipschitz continuous. How can I prove and where can I find a reference for the fact that is differentiable and is the solution to Do we need to assume that is differentiable of does the Lipschitz continuity suffice? Also, does the formula hold if at least for example? If not, why?","\frac{d X(t,x)}{dt} = f(t, X(t,x)) X(0,x) = x f:[0,T]\times \mathbb{R}^n \to \mathbb{R}^n X:[0,T]\times \mathbb{R}^n \to \mathbb{R}^n f X \nabla X \frac{d}{dt} \nabla X(t,x) = \nabla f(t,X(t,x)) \nabla X(t,x)? f \nabla X = e^{\int_0^T\nabla f} f \in L^1((0,T),W^{1,1}(\mathbb{R}^n))","['real-analysis', 'integration', 'ordinary-differential-equations', 'partial-differential-equations', 'sobolev-spaces']"
20,What kind of flow minimizes resistance?,What kind of flow minimizes resistance?,,"Consider a domain $D$ , where $\sigma(x)$ is the spatially dependent ""conductivity"". On the boundary we have $2$ ""electrodes"" $E_1$ and $E_2$ where matter flows in and out. The rest of the boundary is insulating material $J\cdot\vec n=0$ (Neumann BC). To each divergence-free vector field $J(x)$ on $D$ which flow strictly from $E_1$ to $E_2$ we can assign a ""resistance"" $R_J$ as follows, because we can insert non-conducting walls along the streamlines, we can consider the streamlines from $E_1$ to $E_2$ as resistors of thickness dt, then $R_J$ is defined as the resistance of them all in parallel (so that would be $1/(1/R_1+1/R_2....))$ , in the limit as dt goes to 0. Each of the streamline resistors is defined as a series connection of resistors (So thats $R_a+R_b...$ ) with length dt2, and we take the limit as dt2 goes to zero aswell. The resistance of each resistor of thickness $dt*dt2$ is computed from $\sigma(x)$ Let $K(x)$ be the $J(x)$ with smallest $R_J$ . What differential equation governs $K(x)$ ? (besides $\nabla \cdot K=0) $ Edit: This is probably incorrect because du is not constant, streamlines can come closer along the flow for instance, $R_J=1/(\int_{E_1} 1/(\int_{C_u} (1/\sigma(x))ds))du$ Where the u integral is along the length of the electrode, and $C_u$ is the streamline starting at u.","Consider a domain , where is the spatially dependent ""conductivity"". On the boundary we have ""electrodes"" and where matter flows in and out. The rest of the boundary is insulating material (Neumann BC). To each divergence-free vector field on which flow strictly from to we can assign a ""resistance"" as follows, because we can insert non-conducting walls along the streamlines, we can consider the streamlines from to as resistors of thickness dt, then is defined as the resistance of them all in parallel (so that would be , in the limit as dt goes to 0. Each of the streamline resistors is defined as a series connection of resistors (So thats ) with length dt2, and we take the limit as dt2 goes to zero aswell. The resistance of each resistor of thickness is computed from Let be the with smallest . What differential equation governs ? (besides Edit: This is probably incorrect because du is not constant, streamlines can come closer along the flow for instance, Where the u integral is along the length of the electrode, and is the streamline starting at u.",D \sigma(x) 2 E_1 E_2 J\cdot\vec n=0 J(x) D E_1 E_2 R_J E_1 E_2 R_J 1/(1/R_1+1/R_2....)) R_a+R_b... dt*dt2 \sigma(x) K(x) J(x) R_J K(x) \nabla \cdot K=0)  R_J=1/(\int_{E_1} 1/(\int_{C_u} (1/\sigma(x))ds))du C_u,"['ordinary-differential-equations', 'physics', 'mathematical-physics', 'fluid-dynamics']"
21,When does an algebraic differential equation have algebraic solutions?,When does an algebraic differential equation have algebraic solutions?,,"Suppose $f(x,y)$ is a rational function. Typically the solution curves of the differential equation $$ \frac{dy}{dx}=f(x,y) $$ are not algebraic (for example, if $f(x,y)=y$ , the solutions are the non-algebraic curves $y=ce^x$ ). There are certain $f$ , however, for which all solutions are algebraic (for example, if $f$ is a rational function in just $x$ , having $0$ residue at every pole). Given $f$ , is there a way to check whether all the solutions to $\frac{dy}{dx}=f(x,y)$ are algebraic?","Suppose is a rational function. Typically the solution curves of the differential equation are not algebraic (for example, if , the solutions are the non-algebraic curves ). There are certain , however, for which all solutions are algebraic (for example, if is a rational function in just , having residue at every pole). Given , is there a way to check whether all the solutions to are algebraic?","f(x,y) 
\frac{dy}{dx}=f(x,y)
 f(x,y)=y y=ce^x f f x 0 f \frac{dy}{dx}=f(x,y)","['ordinary-differential-equations', 'algebraic-geometry']"
22,Existence of solution of a 3-dimensional linear PDE.,Existence of solution of a 3-dimensional linear PDE.,,"Maybe this is a canonical result but I'm facing difficulties to find a reference. It is well known the following theorem about Linear Partial Differential Equations: Theorem: Let $\Omega \subset \mathbb{R}^2$ be an open set and $\Gamma \subset \Omega$ a $\mathcal{C}^1$ curve. Let $\gamma (s) = (\alpha(s),\beta(s))$ be a $\mathcal{C}^1$ parametrization of $\Gamma$ , defined in the interval $I \subset \mathbb{R}$ . Suppose that $a,b,c$ $\in$ $\mathcal{C}^1 (\Omega , \mathbb{R})$ and $f\in\mathcal{C}^1 (I,\mathbb{R})$ , such that $a$ and $b$ never simultaneously vanish in $\Omega$ and satisfy $$\text{det} \begin{bmatrix} \alpha'(s) & a(\alpha(s),\beta(s)) \\ \beta'(s) & b (\alpha(s),\beta(s))  \end{bmatrix} \neq 0, \ \forall \  s \in I.  $$ Then, there is a solution of the Cauchy Problem \begin{align*} \left\{\begin{array}{l} a(x,y)u_x + b(x,y) u_y = c(x,y),\quad &\text{if}\ (x,y) \in \Omega, \\ u(\alpha(s),\beta(s)) = f(s)\ &\text{if} \ s \in I.   \end{array}\right. \end{align*} defined in a neighborhood of $\Gamma$ . However, I wasn't able to find a 3-dimensional version of this theorem.   All the books in which I looked for such a theorem only presented the 2-dimensional version. I think that a 3-dimensional version of this theorem would look like the following Possible Theorem: Let $\Omega \subset \mathbb{R}^3$ be an open set and $\Sigma \subset \Omega$ be a $\mathcal{C}^1$ surface. Let $\sigma (u,v) = (\sigma_1(u,v),\sigma_2(u,v),\sigma_3(u,v))$ a $\mathcal{C}^1$ parametrization of $\Sigma$ , defined in the square $I\times I \subset \mathbb{R}^2$ . Suppose that $a,b,c,d$ $\in$ $\mathcal{C}^1 (\Omega , \mathbb{R})$ and $f\in\mathcal{C}^1 (I,\mathbb{R})$ , such that $a$ , $b$ and $c$ never simultaneously vanish in $\Omega$ and satisfy $$\text{det} \begin{bmatrix} \frac{\partial \sigma_1}{\partial u}(u,v) & \frac{\partial \sigma_1}{\partial v}(u,v) & a(\sigma(u,v))\\ \frac{\partial \sigma_2}{\partial u}(u,v) & \frac{\partial \sigma_2}{\partial v}(u,v) &b(\sigma(u,v))\\ \frac{\partial \sigma_3}{\partial u}(u,v) & \frac{\partial \sigma_3}{\partial v}(u,v) & c(\sigma(u,v)) \end{bmatrix} \neq 0, \ \forall \  (u,v) \in I\times I.  $$ Then, there is a solution of the Cauchy Problem \begin{align*} \left\{\begin{array}{l} a(x,y,z)u_x + b(x,y,z) u_y + c(x,y,z)u_z = d(x,y,z) ,\quad& \text{if}\ (x,y,z) \in \Omega, \\ u(\sigma(u,v) ) = f(u,v)\ &\text{if} \ (u,v) \in I\times I. \\ \end{array}\right. \end{align*} defined in a neighborhood of $\Sigma$ . Does anyone know if this theorem is true and could indicate me a reference?","Maybe this is a canonical result but I'm facing difficulties to find a reference. It is well known the following theorem about Linear Partial Differential Equations: Theorem: Let be an open set and a curve. Let be a parametrization of , defined in the interval . Suppose that and , such that and never simultaneously vanish in and satisfy Then, there is a solution of the Cauchy Problem defined in a neighborhood of . However, I wasn't able to find a 3-dimensional version of this theorem.   All the books in which I looked for such a theorem only presented the 2-dimensional version. I think that a 3-dimensional version of this theorem would look like the following Possible Theorem: Let be an open set and be a surface. Let a parametrization of , defined in the square . Suppose that and , such that , and never simultaneously vanish in and satisfy Then, there is a solution of the Cauchy Problem defined in a neighborhood of . Does anyone know if this theorem is true and could indicate me a reference?","\Omega \subset \mathbb{R}^2 \Gamma \subset \Omega \mathcal{C}^1 \gamma (s) = (\alpha(s),\beta(s)) \mathcal{C}^1 \Gamma I \subset \mathbb{R} a,b,c \in \mathcal{C}^1 (\Omega , \mathbb{R}) f\in\mathcal{C}^1 (I,\mathbb{R}) a b \Omega \text{det} \begin{bmatrix}
\alpha'(s) & a(\alpha(s),\beta(s)) \\
\beta'(s) & b (\alpha(s),\beta(s)) 
\end{bmatrix} \neq 0, \ \forall \  s \in I.   \begin{align*}
\left\{\begin{array}{l}
a(x,y)u_x + b(x,y) u_y = c(x,y),\quad &\text{if}\ (x,y) \in \Omega, \\
u(\alpha(s),\beta(s)) = f(s)\ &\text{if} \ s \in I.  
\end{array}\right.
\end{align*} \Gamma \Omega \subset \mathbb{R}^3 \Sigma \subset \Omega \mathcal{C}^1 \sigma (u,v) = (\sigma_1(u,v),\sigma_2(u,v),\sigma_3(u,v)) \mathcal{C}^1 \Sigma I\times I \subset \mathbb{R}^2 a,b,c,d \in \mathcal{C}^1 (\Omega , \mathbb{R}) f\in\mathcal{C}^1 (I,\mathbb{R}) a b c \Omega \text{det} \begin{bmatrix}
\frac{\partial \sigma_1}{\partial u}(u,v) & \frac{\partial \sigma_1}{\partial v}(u,v) & a(\sigma(u,v))\\
\frac{\partial \sigma_2}{\partial u}(u,v) & \frac{\partial \sigma_2}{\partial v}(u,v) &b(\sigma(u,v))\\
\frac{\partial \sigma_3}{\partial u}(u,v) & \frac{\partial \sigma_3}{\partial v}(u,v) & c(\sigma(u,v))
\end{bmatrix} \neq 0, \ \forall \  (u,v) \in I\times I.   \begin{align*}
\left\{\begin{array}{l}
a(x,y,z)u_x + b(x,y,z) u_y + c(x,y,z)u_z = d(x,y,z) ,\quad& \text{if}\ (x,y,z) \in \Omega, \\
u(\sigma(u,v) ) = f(u,v)\ &\text{if} \ (u,v) \in I\times I. \\
\end{array}\right.
\end{align*} \Sigma","['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'differential-topology', 'partial-derivative']"
23,Solving parabolic partial differential equations with polynomial coefficients,Solving parabolic partial differential equations with polynomial coefficients,,"Suppose I have a set of real valued polynomial functions: $p_1(x)=\sum_{i=1}^la_ix^i,\;p_2(x)=\sum_{i=1}^mb_ix^i,\;p_3(x)=\sum_{i=1}^nc_ix^i$ . Are parabolic PDEs of the form $\frac{\partial u}{\partial t}+p_1(x)\frac{\partial u}{\partial x}+p_2(x)\frac{\partial^2 u}{\partial x^2}+p_3(x)=0$ exactly solvable? If so, what is the method? If not, are there efficient numerical methods to solve the above?","Suppose I have a set of real valued polynomial functions: . Are parabolic PDEs of the form exactly solvable? If so, what is the method? If not, are there efficient numerical methods to solve the above?","p_1(x)=\sum_{i=1}^la_ix^i,\;p_2(x)=\sum_{i=1}^mb_ix^i,\;p_3(x)=\sum_{i=1}^nc_ix^i \frac{\partial u}{\partial t}+p_1(x)\frac{\partial u}{\partial x}+p_2(x)\frac{\partial^2 u}{\partial x^2}+p_3(x)=0","['ordinary-differential-equations', 'numerical-methods', 'partial-derivative', 'laplace-transform', 'parabolic-pde']"
24,Solving the diffusion equation with an absorbing boundary,Solving the diffusion equation with an absorbing boundary,,"There is a one-dimensional diffusion process in which particles start running at $t = 0$ and from $x_o > 0$ . When particles reach x = 0 they are removed from the system, thus the total concentration is not conserved anymore. I have to solve the diffusion equation, which is the following partial differential equation: $$\frac{\partial P (R, t)}{\partial t} = D\triangledown^2P(R,t) $$ Where $P(R, t)$ is the probability that the particles  arrive at R at time t. And I am given the initial conditions: $$c(x,0) = N\delta(x - x_o)$$ $$c(0, t) = 0$$ I have been doing some research in how to do so and I came across with a method which is based on a particular Gaussian function: $$G (R, t) = (\frac{1}{4\pi Dt})^{\frac{d}{2}} e^{\frac{(R-R_0)^2}{4Dt}}$$ Where d is the dimensionality of the system. But the issue here is that we are working with an 'absorbing boundary' that makes the condition $c(0, t) = 0$ useless because we work from $x_o > 0$ . Then how could I solve the probability (i.e this differential equation)? I have been suggested the method of images, but not sure about it.","There is a one-dimensional diffusion process in which particles start running at and from . When particles reach x = 0 they are removed from the system, thus the total concentration is not conserved anymore. I have to solve the diffusion equation, which is the following partial differential equation: Where is the probability that the particles  arrive at R at time t. And I am given the initial conditions: I have been doing some research in how to do so and I came across with a method which is based on a particular Gaussian function: Where d is the dimensionality of the system. But the issue here is that we are working with an 'absorbing boundary' that makes the condition useless because we work from . Then how could I solve the probability (i.e this differential equation)? I have been suggested the method of images, but not sure about it.","t = 0 x_o > 0 \frac{\partial P (R, t)}{\partial t} = D\triangledown^2P(R,t)  P(R, t) c(x,0) = N\delta(x - x_o) c(0, t) = 0 G (R, t) = (\frac{1}{4\pi Dt})^{\frac{d}{2}} e^{\frac{(R-R_0)^2}{4Dt}} c(0, t) = 0 x_o > 0","['calculus', 'integration', 'ordinary-differential-equations', 'mathematical-physics', 'statistical-mechanics']"
25,Exact Differential: Integrating Factor in Higher Dimensions,Exact Differential: Integrating Factor in Higher Dimensions,,"In two dimensions, we can turn every inexact differential $f(x,y)dx+g(x,y)dy$ into an exact version by multiplying both functions $f(x,y)$ and $g(x,y)$ by an additional function $I(z)$ , i.e. $I(z) f(x,y) dx + I(z) g(x,y) dy = 0$ , where $z = xy$ . Then, the condition that the cross-derivatives should be the same, i.e. $$f_y(x,y) I(z) + x I'(z) f(x,y) = g_x(x,y) I(z) + y I'(z) g(x,y),$$ lead to a differential equation for $I$ and therewith to the solution. Is this trick of multiplying the inexact differential by an additional function also possible in higher dimension, i.e. for $n$ variables? In other words, is there a possibility to make $\sum_{i=1}^n a_i(x) dx_i$ exact, where $a_i(x)$ are arbitrary $C^1$ -functions? An idea would be $$\sum_{i=1}^n a_i(x) \sum_{k=1}^n I_{ik}(x_i x_k) dx_i$$ to account for the cross-derivatives. Is this solveable for $I_{ik}$ or are there other approaches? Does anybody has experiences therefore? Thanks already.","In two dimensions, we can turn every inexact differential into an exact version by multiplying both functions and by an additional function , i.e. , where . Then, the condition that the cross-derivatives should be the same, i.e. lead to a differential equation for and therewith to the solution. Is this trick of multiplying the inexact differential by an additional function also possible in higher dimension, i.e. for variables? In other words, is there a possibility to make exact, where are arbitrary -functions? An idea would be to account for the cross-derivatives. Is this solveable for or are there other approaches? Does anybody has experiences therefore? Thanks already.","f(x,y)dx+g(x,y)dy f(x,y) g(x,y) I(z) I(z) f(x,y) dx + I(z) g(x,y) dy = 0 z = xy f_y(x,y) I(z) + x I'(z) f(x,y) = g_x(x,y) I(z) + y I'(z) g(x,y), I n \sum_{i=1}^n a_i(x) dx_i a_i(x) C^1 \sum_{i=1}^n a_i(x) \sum_{k=1}^n I_{ik}(x_i x_k) dx_i I_{ik}","['ordinary-differential-equations', 'multivariable-calculus', 'differential', 'integrating-factor']"
26,Gradient estimate for convex function,Gradient estimate for convex function,,"Consider $\varphi:\mathbb{R}^n\rightarrow \mathbb{R}$ a convex, twice differentiable function with the gradient $\nabla \varphi$ Lipschitz-continuous. Suppose the function achieve a minimum in $\mathbb{R}^n$. We can write the following gradient system: \begin{equation} \begin{cases} \dot{x}(t) = -\nabla\varphi(x(t))\\ x(0) = x_0 \in \mathbb{R}^n \end{cases} \end{equation} From a much more general case it is known that the following estimate is true: \begin{equation} \|\nabla \varphi(x(t))\|\leq \frac{C}{t} \end{equation} for a certain constant $C$. I would like to prove this inequality without using the general case, but I haven't found anything yet. How can I prove it?","Consider $\varphi:\mathbb{R}^n\rightarrow \mathbb{R}$ a convex, twice differentiable function with the gradient $\nabla \varphi$ Lipschitz-continuous. Suppose the function achieve a minimum in $\mathbb{R}^n$. We can write the following gradient system: \begin{equation} \begin{cases} \dot{x}(t) = -\nabla\varphi(x(t))\\ x(0) = x_0 \in \mathbb{R}^n \end{cases} \end{equation} From a much more general case it is known that the following estimate is true: \begin{equation} \|\nabla \varphi(x(t))\|\leq \frac{C}{t} \end{equation} for a certain constant $C$. I would like to prove this inequality without using the general case, but I haven't found anything yet. How can I prove it?",,"['real-analysis', 'ordinary-differential-equations', 'derivatives']"
27,On the solution of a linear system of differential equations for the unknown series coefficients,On the solution of a linear system of differential equations for the unknown series coefficients,,"In a mathematical physical problem, I came across the following linear system of differential equations (obtained upon using Fourier series expansion): \begin{align} 		\frac{\mathrm{d} \rho_n}{\mathrm{d}  t} + 		\alpha \sum_{i = 1}^{\infty} \frac{\mathrm{d} \rho_i}{\mathrm{d} t}  		&= - H_n \bigg( H_n \, \rho_n 		+ \frac{\phi_n}{2} \bigg) +  1    \, , \\ 		\frac{\mathrm{d}  \phi_n}{\mathrm{d}  t}  		&= - H_n \, \rho_n - \phi_n \, , \end{align} where  $$ H_n = 2n-1 \, ,  \quad \alpha \in \mathbb{R} \, , \quad \text{and} \,\, n \ge 1\, . $$ The system is subject to the initial conditions $\rho_n(0)=\phi_n(0)=0$. The goal is to determine the general expression of the coefficients $\rho_n$ and $\phi_n$.     When $\alpha=0$, the solution of the problem is easy and straightforward. I have tried to solve the above linear system of differential equations for $\alpha \ne 0$ using the Laplace transform technique but without success. The calculation of the inverse Laplace transform doesn't seem to be possible (since the Laplace-transformed function has an infinite number of singularities and choosing an abscissa of convergence $\sigma>0$ for which the contour is located to the right of all singularities is not within reach.) I was wondering whether someone here could be of help and try to tell how one can solve such a mathematical problem. Your hints and ideas and very welcome. Very much thanks!","In a mathematical physical problem, I came across the following linear system of differential equations (obtained upon using Fourier series expansion): \begin{align} 		\frac{\mathrm{d} \rho_n}{\mathrm{d}  t} + 		\alpha \sum_{i = 1}^{\infty} \frac{\mathrm{d} \rho_i}{\mathrm{d} t}  		&= - H_n \bigg( H_n \, \rho_n 		+ \frac{\phi_n}{2} \bigg) +  1    \, , \\ 		\frac{\mathrm{d}  \phi_n}{\mathrm{d}  t}  		&= - H_n \, \rho_n - \phi_n \, , \end{align} where  $$ H_n = 2n-1 \, ,  \quad \alpha \in \mathbb{R} \, , \quad \text{and} \,\, n \ge 1\, . $$ The system is subject to the initial conditions $\rho_n(0)=\phi_n(0)=0$. The goal is to determine the general expression of the coefficients $\rho_n$ and $\phi_n$.     When $\alpha=0$, the solution of the problem is easy and straightforward. I have tried to solve the above linear system of differential equations for $\alpha \ne 0$ using the Laplace transform technique but without success. The calculation of the inverse Laplace transform doesn't seem to be possible (since the Laplace-transformed function has an infinite number of singularities and choosing an abscissa of convergence $\sigma>0$ for which the contour is located to the right of all singularities is not within reach.) I was wondering whether someone here could be of help and try to tell how one can solve such a mathematical problem. Your hints and ideas and very welcome. Very much thanks!",,"['real-analysis', 'linear-algebra', 'sequences-and-series', 'ordinary-differential-equations', 'laplace-transform']"
28,Solving the wave equation under a Neumann boundary condition on the gradient at one point,Solving the wave equation under a Neumann boundary condition on the gradient at one point,,"This is my first question/contribution even though I have been following hot topics for a while, I have been confronted a to a problem recently and I don't really know how to address it. This comes from acoustics, there is a consequent amount of literature around solving the wave equation under homogeneous Neumann condition on some boundaries. This represent the case of a sound wave propagating in a room with perfectly reflecting walls. One method for addressing this problem is the image source method where we construct a lattice from the source by reflecting though a list of walls. This was done for an arbitrary polyhedral room in 1984 in a paper named ""Extension of the image model to arbitrary polyhedra"" by Jeffrey Borish. Unfortunately he don't give proofs of his argument, just some intuition. I am currently researching on this and I would like to be able to prove his results, but I would like to also generalize the method for non closed rooms So long story short, I end up been stuck solving the following equation for the pressure $$\nabla^2 p({\bf x}) + k^2 p({\bf x}) = 0$$ Subject to $$\nabla p({\bf y}) = {\bf q}$$ Where we know $k$, ${\bf q}$ and ${\bf y}$ (${\bf x}$ and ${\bf y}$ are vector in $\mathbb{R}^n$) Of course as is, there might even not be a solution to this problem, I have been reading through section 6 of the book ""Methods of Theoretical physics"" which gave me some useful insight, in the end of section 6.2 he states the cases when the solution exists and about uniqueness. For my case, Hyperbolic equation with open boundaries, they state that we have insufficient information about the problem. So I may actually have a solution. In the sane book section 6.3, he solve a lot of these kinds of equations but the boundaries condition always seems to be homogeneous. They are using an eigen decomposition of the problem, maybe it is possible to adapt this reasoning to my problem. Finally I went through a substantial amount of literature on the subject but the boundaries conditions where almost always homogeneous. I also could not find a similar question here or somewhere else on the web. So if any one have an idea about this I would be glad to discuss it. I hope that this is clear enough. Thanks in advance. EDIT : I forgot to mention that in I also have boundaries conditions at infinite distance from the origin, the pressure is supposed to be zero. EDIT 2 : I think the solution may actually be unique. The uniqueness should not depend on the choice of ${\bf q}$ or ${\bf y}$ and so we can probably just look at ${\bf y} = {\bf q} = {\bf 0}$. In this case if we use a eigen decomposition argument, we get that our eigen functions would be of the form $\cosh(a_{k,i} x_i)$ for $x_i$ the i'th coordinate of ${\bf x}$ and for any $a_{k,i}$, then we can express our pressure as  $$p({\bf x}) = \sum_k \sum_i A_{k,i} \cosh(a_{k,i} x_i)$$ and $$\nabla^2 p({\bf x}) = \sum_k \sum_i A_{k,i} a_{k,i}^2 \cosh(a_{k,i} x_i)$$ Now the next step is a bit unclear but I think that in order to have $\nabla^2 p({\bf x}) + k^2 p({\bf x}) = 0$, then we need to have $A_{k,i} a_{k,i}^2 + A_{k,i} k^2 =0$ for all $k$ and $i$. This can only be true if for all $k$ and $i$, $A_{k,i}=0$. Does this reasoning makes sense ? And is way to generalize to any ${\bf q}$ ? Then generalizing to any ${\bf y}$ is trivial.","This is my first question/contribution even though I have been following hot topics for a while, I have been confronted a to a problem recently and I don't really know how to address it. This comes from acoustics, there is a consequent amount of literature around solving the wave equation under homogeneous Neumann condition on some boundaries. This represent the case of a sound wave propagating in a room with perfectly reflecting walls. One method for addressing this problem is the image source method where we construct a lattice from the source by reflecting though a list of walls. This was done for an arbitrary polyhedral room in 1984 in a paper named ""Extension of the image model to arbitrary polyhedra"" by Jeffrey Borish. Unfortunately he don't give proofs of his argument, just some intuition. I am currently researching on this and I would like to be able to prove his results, but I would like to also generalize the method for non closed rooms So long story short, I end up been stuck solving the following equation for the pressure $$\nabla^2 p({\bf x}) + k^2 p({\bf x}) = 0$$ Subject to $$\nabla p({\bf y}) = {\bf q}$$ Where we know $k$, ${\bf q}$ and ${\bf y}$ (${\bf x}$ and ${\bf y}$ are vector in $\mathbb{R}^n$) Of course as is, there might even not be a solution to this problem, I have been reading through section 6 of the book ""Methods of Theoretical physics"" which gave me some useful insight, in the end of section 6.2 he states the cases when the solution exists and about uniqueness. For my case, Hyperbolic equation with open boundaries, they state that we have insufficient information about the problem. So I may actually have a solution. In the sane book section 6.3, he solve a lot of these kinds of equations but the boundaries condition always seems to be homogeneous. They are using an eigen decomposition of the problem, maybe it is possible to adapt this reasoning to my problem. Finally I went through a substantial amount of literature on the subject but the boundaries conditions where almost always homogeneous. I also could not find a similar question here or somewhere else on the web. So if any one have an idea about this I would be glad to discuss it. I hope that this is clear enough. Thanks in advance. EDIT : I forgot to mention that in I also have boundaries conditions at infinite distance from the origin, the pressure is supposed to be zero. EDIT 2 : I think the solution may actually be unique. The uniqueness should not depend on the choice of ${\bf q}$ or ${\bf y}$ and so we can probably just look at ${\bf y} = {\bf q} = {\bf 0}$. In this case if we use a eigen decomposition argument, we get that our eigen functions would be of the form $\cosh(a_{k,i} x_i)$ for $x_i$ the i'th coordinate of ${\bf x}$ and for any $a_{k,i}$, then we can express our pressure as  $$p({\bf x}) = \sum_k \sum_i A_{k,i} \cosh(a_{k,i} x_i)$$ and $$\nabla^2 p({\bf x}) = \sum_k \sum_i A_{k,i} a_{k,i}^2 \cosh(a_{k,i} x_i)$$ Now the next step is a bit unclear but I think that in order to have $\nabla^2 p({\bf x}) + k^2 p({\bf x}) = 0$, then we need to have $A_{k,i} a_{k,i}^2 + A_{k,i} k^2 =0$ for all $k$ and $i$. This can only be true if for all $k$ and $i$, $A_{k,i}=0$. Does this reasoning makes sense ? And is way to generalize to any ${\bf q}$ ? Then generalizing to any ${\bf y}$ is trivial.",,"['ordinary-differential-equations', 'wave-equation']"
29,Is the convergence of $\dot{x}=2A(t)x$ faster than that of $\dot{x}=A(t)x$?,Is the convergence of  faster than that of ?,\dot{x}=2A(t)x \dot{x}=A(t)x,"Let $x \in \mathbb{R}^{n}$ and $A(t) \in \mathbb{R}^{n\times n}$. If $\dot{x}=A(t)x$ and $\dot{x}=cA(t)x$ with $c>1$ are exponentially stable. Is the convergence rate of $x$ to zero of $\dot{x}=cA(t)x$ faster than that of $\dot{x}=A(t)x$? Here is my initial thought: For linear time-invariant system, the fact $\dot{x}=A(t)x$ is exponentially stable implies $A(t)=A$ is Hurwitz. It is clear that the $i$th eigenvalue $\lambda_{i}(cA)=c\lambda_{i}(A)$, $i,\ldots,n$, thus, the convergence for $\dot{x}=cAx$ with $c>1$ is faster than that of $\dot{x}=Ax$. For a linear time-varying system. Let us consider the extreme case where $c=0$, $\dot{x}=cA(t)x=0$, which implies it is no longer exponentially stable. For $c>1$, can we have the same conclusion for exponentially stable linear time-varying systems, i.e., can we conclude that the convergence is faster when $c>1$? Update 1: For a scalar time-varying system, i.e., $x\in\mathbb{R}$. We can actually prove this conjecture. In fact, from the uniqueness of the equilibrium point ($x=0$ is the only solution that renders $\dot{x}=0$), the solution of $\dot{x}=ca(t)x$ is a monotone function either strictly increasing or strictly decreasing, depending on its initial condition $x(0)$. Thus, for $\dot{x}=ca(t)x$ where $c>1$, the absolute value of the derivative is larger than that of $\dot{x}=a(t)x$, while for both cases the sign of $\dot{x}(t)$ remains unchanged for all $t\ge0$. Thus, $\dot{x}=ca(t)x$ does converge faster to $x=0$ than $\dot{x}=a(t)x$.","Let $x \in \mathbb{R}^{n}$ and $A(t) \in \mathbb{R}^{n\times n}$. If $\dot{x}=A(t)x$ and $\dot{x}=cA(t)x$ with $c>1$ are exponentially stable. Is the convergence rate of $x$ to zero of $\dot{x}=cA(t)x$ faster than that of $\dot{x}=A(t)x$? Here is my initial thought: For linear time-invariant system, the fact $\dot{x}=A(t)x$ is exponentially stable implies $A(t)=A$ is Hurwitz. It is clear that the $i$th eigenvalue $\lambda_{i}(cA)=c\lambda_{i}(A)$, $i,\ldots,n$, thus, the convergence for $\dot{x}=cAx$ with $c>1$ is faster than that of $\dot{x}=Ax$. For a linear time-varying system. Let us consider the extreme case where $c=0$, $\dot{x}=cA(t)x=0$, which implies it is no longer exponentially stable. For $c>1$, can we have the same conclusion for exponentially stable linear time-varying systems, i.e., can we conclude that the convergence is faster when $c>1$? Update 1: For a scalar time-varying system, i.e., $x\in\mathbb{R}$. We can actually prove this conjecture. In fact, from the uniqueness of the equilibrium point ($x=0$ is the only solution that renders $\dot{x}=0$), the solution of $\dot{x}=ca(t)x$ is a monotone function either strictly increasing or strictly decreasing, depending on its initial condition $x(0)$. Thus, for $\dot{x}=ca(t)x$ where $c>1$, the absolute value of the derivative is larger than that of $\dot{x}=a(t)x$, while for both cases the sign of $\dot{x}(t)$ remains unchanged for all $t\ge0$. Thus, $\dot{x}=ca(t)x$ does converge faster to $x=0$ than $\dot{x}=a(t)x$.",,"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
30,Surface with mean curvature proportional to height $z$,Surface with mean curvature proportional to height,z,"This curve, described with the inverse hyperbolic secant $\text{arsech}(x) = \text{arcosh}(1/x)$ , $$y = \text{arsech}\frac x2 - \sqrt{4-x^2}$$ has its curvature equal to the linear coordinate $x$ : $$\frac{dy}{dx} = \frac{-2}{x\sqrt{4-x^2}} - \frac{-x}{\sqrt{4-x^2}}\quad = \frac{x^2-2}{x\sqrt{4-x^2}}$$ $$\frac{d^2y}{dx^2} = \frac{2x}{x\sqrt{4-x^2}} + \frac{-(x^2-2)}{x^2\sqrt{4-x^2}} + \frac{(x^2-2)x}{x\sqrt{4-x^2\;}^3}\quad = \frac{8}{x^2\sqrt{4-x^2\;}^3}$$ $$k = \frac{\frac{d^2y}{dx^2}}{\sqrt{1+\Big(\frac{dy}{dx}\Big)^2\;}^3} = \frac{\bigg(\frac{8x}{x^3\sqrt{4-x^2\;}^3}\bigg)}{\sqrt{\frac{x^2(4-x^2)+(x^4-4x^2+4)}{x^2(4-x^2)}\;}^3}\quad = x$$ Here's a graph . (This curve can be extended by reflecting it across the $x$ and $y$ axes, and the relation $k = x$ holds, depending on sign conventions.) Is there a surface in 3D with this property? Obviously, the ( generalized ) cylinder based on the above curve would work. What about a surface of revolution? After some calculations in cylindrical coordinates (with $r$ and $z$ functions of $t$ , independent of $\theta$ ), I find the mean curvature: $$2H = \frac{(r'^2+z'^2)z' + (r'z''-r''z')r}{r\sqrt{r'^2+z'^2\;}^3}\quad \overset{?}{=} z$$ If the parameter $t = r$ or $t = z$ , then this simplifies to $$\frac{(1+z'^2)z' + rz''}{r\sqrt{1+z'^2\;}^3} = z$$ ( $z$ is a function of $r$ ) or $$\frac{1+r'^2 - rr''}{r\sqrt{1+r'^2\;}^3} = z$$ ( $r$ is a function of $z$ ). Are these equations solvable? in terms of elementary functions? in terms of elliptic functions?","This curve, described with the inverse hyperbolic secant , has its curvature equal to the linear coordinate : Here's a graph . (This curve can be extended by reflecting it across the and axes, and the relation holds, depending on sign conventions.) Is there a surface in 3D with this property? Obviously, the ( generalized ) cylinder based on the above curve would work. What about a surface of revolution? After some calculations in cylindrical coordinates (with and functions of , independent of ), I find the mean curvature: If the parameter or , then this simplifies to ( is a function of ) or ( is a function of ). Are these equations solvable? in terms of elementary functions? in terms of elliptic functions?",\text{arsech}(x) = \text{arcosh}(1/x) y = \text{arsech}\frac x2 - \sqrt{4-x^2} x \frac{dy}{dx} = \frac{-2}{x\sqrt{4-x^2}} - \frac{-x}{\sqrt{4-x^2}}\quad = \frac{x^2-2}{x\sqrt{4-x^2}} \frac{d^2y}{dx^2} = \frac{2x}{x\sqrt{4-x^2}} + \frac{-(x^2-2)}{x^2\sqrt{4-x^2}} + \frac{(x^2-2)x}{x\sqrt{4-x^2\;}^3}\quad = \frac{8}{x^2\sqrt{4-x^2\;}^3} k = \frac{\frac{d^2y}{dx^2}}{\sqrt{1+\Big(\frac{dy}{dx}\Big)^2\;}^3} = \frac{\bigg(\frac{8x}{x^3\sqrt{4-x^2\;}^3}\bigg)}{\sqrt{\frac{x^2(4-x^2)+(x^4-4x^2+4)}{x^2(4-x^2)}\;}^3}\quad = x x y k = x r z t \theta 2H = \frac{(r'^2+z'^2)z' + (r'z''-r''z')r}{r\sqrt{r'^2+z'^2\;}^3}\quad \overset{?}{=} z t = r t = z \frac{(1+z'^2)z' + rz''}{r\sqrt{1+z'^2\;}^3} = z z r \frac{1+r'^2 - rr''}{r\sqrt{1+r'^2\;}^3} = z r z,"['ordinary-differential-equations', 'differential-geometry', 'surfaces', 'curvature']"
31,Solving this Fokker-Plack Equation,Solving this Fokker-Plack Equation,,"I have a bidimensional Fokker-Planck equation to solve, namely, $$ \frac{\partial p}{\partial t} =  \frac{\sigma_{1}^{2}}{2} \frac{\partial^{2} p}{\partial I^{2}} - \frac{\partial }{\partial S} (b_{1}(S,I) p) - \frac{\partial }{\partial I}(b_{2} (S,I) p) \,\, , $$ where, $$ b_{1} = -aSI \,\, , \\ b_{2} = aSI-\mu I + cI -cSI-cI^2 \,\, . $$ My initial and Neumann conditions are the following, $$ p(S,I,0) = \delta (S-S_{0},I-I_{0})  \,\, , \\ \lim_{S,I \to \pm \infty} p(S,I,t)= \lim_{S,I \to \pm \infty} \partial_{S} p(S,I,t) = \partial_{I} p(S,I,t) = 0 \,\, . $$ How can I solve this FP equation? Can I apply separation of variables?","I have a bidimensional Fokker-Planck equation to solve, namely, $$ \frac{\partial p}{\partial t} =  \frac{\sigma_{1}^{2}}{2} \frac{\partial^{2} p}{\partial I^{2}} - \frac{\partial }{\partial S} (b_{1}(S,I) p) - \frac{\partial }{\partial I}(b_{2} (S,I) p) \,\, , $$ where, $$ b_{1} = -aSI \,\, , \\ b_{2} = aSI-\mu I + cI -cSI-cI^2 \,\, . $$ My initial and Neumann conditions are the following, $$ p(S,I,0) = \delta (S-S_{0},I-I_{0})  \,\, , \\ \lim_{S,I \to \pm \infty} p(S,I,t)= \lim_{S,I \to \pm \infty} \partial_{S} p(S,I,t) = \partial_{I} p(S,I,t) = 0 \,\, . $$ How can I solve this FP equation? Can I apply separation of variables?",,"['ordinary-differential-equations', 'partial-differential-equations', 'linear-pde']"
32,"Solution of the nonlinear ODE $y'' y' =A y' y + B (x-1) y$, with $y(0) = 1$, $y(1) = 0$","Solution of the nonlinear ODE , with ,",y'' y' =A y' y + B (x-1) y y(0) = 1 y(1) = 0,"What analytical techniques are available for finding solutions to the nonlinear ODE $$y'' y' =A y' y + B (x-1) y,$$ with boundary conditions $$y(0) = 1, \quad y(1) = 0,$$ where $A$ and $B$ are positive, real constants? Unfortunately, neither $A$ nor $B$ are necessarily small. Does assuming that $y'(x) \neq 0$ allow further progress?","What analytical techniques are available for finding solutions to the nonlinear ODE $$y'' y' =A y' y + B (x-1) y,$$ with boundary conditions $$y(0) = 1, \quad y(1) = 0,$$ where $A$ and $B$ are positive, real constants? Unfortunately, neither $A$ nor $B$ are necessarily small. Does assuming that $y'(x) \neq 0$ allow further progress?",,['ordinary-differential-equations']
33,How to solve Fokker-Planck PDE for Brownian particle in square potential driven by periodic time-dependent force,How to solve Fokker-Planck PDE for Brownian particle in square potential driven by periodic time-dependent force,,"Problem statement: We have a Brownian particle in harmonic potential with additional time-dependent force.  Langevin equation(mass taken to be unit): $\ddot{x} + \gamma \dot{x} + \omega_0^2 x = \epsilon \cos(\omega t) + \sqrt{2D} \xi(t)$ Corresponding Fokker-Planck equation for $P = P(x,v,t)$, with $v = \dot{x}$: $\partial_t P = - v\partial_x P + \partial_v(\gamma v + \omega_0^2 x - \epsilon \cos(\omega t)) + D\partial_v^2 P$ My attempt at solving: Perform bilateral Laplace transform on $x$ and $v$. This would lead to  a differential equation only involving derivatives in $t$. We would solve that and then preform inverse Laplace transform to get back dependency on $x$ and $v$. The problem is that Laplace transform doesn't get rid entirely of derivatives in $v$ and $x$ since for example $v\partial_x P \Rightarrow  \hat{x}\partial_{\hat{v}}\hat{P}$ Because of that, we would first have to make some sort of variable substitution: $y = y(x,v)$ $z = z(x,v)$ and possibly $F = u(x,v) P (x,v,t)$ which would than lead to PDE without variables as prefactors, sth like: $\partial_t F = c_1 \partial_x F + c_2(t) \partial_v F+ c_3 F + c_4 \partial_v^2 F$ and then continue as described.","Problem statement: We have a Brownian particle in harmonic potential with additional time-dependent force.  Langevin equation(mass taken to be unit): $\ddot{x} + \gamma \dot{x} + \omega_0^2 x = \epsilon \cos(\omega t) + \sqrt{2D} \xi(t)$ Corresponding Fokker-Planck equation for $P = P(x,v,t)$, with $v = \dot{x}$: $\partial_t P = - v\partial_x P + \partial_v(\gamma v + \omega_0^2 x - \epsilon \cos(\omega t)) + D\partial_v^2 P$ My attempt at solving: Perform bilateral Laplace transform on $x$ and $v$. This would lead to  a differential equation only involving derivatives in $t$. We would solve that and then preform inverse Laplace transform to get back dependency on $x$ and $v$. The problem is that Laplace transform doesn't get rid entirely of derivatives in $v$ and $x$ since for example $v\partial_x P \Rightarrow  \hat{x}\partial_{\hat{v}}\hat{P}$ Because of that, we would first have to make some sort of variable substitution: $y = y(x,v)$ $z = z(x,v)$ and possibly $F = u(x,v) P (x,v,t)$ which would than lead to PDE without variables as prefactors, sth like: $\partial_t F = c_1 \partial_x F + c_2(t) \partial_v F+ c_3 F + c_4 \partial_v^2 F$ and then continue as described.",,"['ordinary-differential-equations', 'partial-differential-equations', 'stochastic-processes', 'brownian-motion', 'stochastic-pde']"
34,Integral curves to a non-vanishing vector field on the unit square,Integral curves to a non-vanishing vector field on the unit square,,"Let $X$ be a non-vanishing vector field on the unit square $I^2$ in $\mathbb{R}^2$. I would like to show that every integral curve to $X$ exits the unit square in finite time. This fact is used in a paper I am reading, in which the author says ""(assume there is an integral curve that does not exit the unit square,) it would approach asymptotically some simple closed curve in $I^2$. In the interior of this curve the vector field would have to have a singularity."" This does seem reasonable, since integral curves cannot cross themselves (unless they are simply closed curves, but as the author has noted this would result in a contradiction in the interior of the closed curve) so they should have no place to go except wrapping around. However I cannot make this idea rigorous at all. Any help is appreciated!","Let $X$ be a non-vanishing vector field on the unit square $I^2$ in $\mathbb{R}^2$. I would like to show that every integral curve to $X$ exits the unit square in finite time. This fact is used in a paper I am reading, in which the author says ""(assume there is an integral curve that does not exit the unit square,) it would approach asymptotically some simple closed curve in $I^2$. In the interior of this curve the vector field would have to have a singularity."" This does seem reasonable, since integral curves cannot cross themselves (unless they are simply closed curves, but as the author has noted this would result in a contradiction in the interior of the closed curve) so they should have no place to go except wrapping around. However I cannot make this idea rigorous at all. Any help is appreciated!",,"['ordinary-differential-equations', 'analysis', 'vector-analysis']"
35,Eigenvectors of discrete Laplace matrix for 2D unit square under Neumann boundary condition,Eigenvectors of discrete Laplace matrix for 2D unit square under Neumann boundary condition,,"Eigenvectors of discrete Laplace matrix for 2D unit square with free boundary is simply $$ \phi(x,y)= \cos(\frac{\pi}{n} kx) \cos(\frac{\pi}{m} ly)  $$ It is easy to see that its 2nd order derivative equals itself (scaled). For example, the Laplace matrix of a 4 by 4 grid is The numerically computed eigenvectors are consistent with the expression $\cos(\frac{\pi}{n} kx) \cos(\frac{\pi}{m} ly)$. My question is what is the following matrix: Precisely put, what is the differential equation on a continuous 2D unit square domain corresponds to this discrete operator? The value 4 corresponds to the inner nodes, value 3 for the boundary nodes, value 2 for the corner nodes. Is this the Laplacian under Neumann boundary condiction? Second question: what are the eigenvectors of such matrix? Of course one can compute them numerically, but is there an analytic expression for these  eigenvectors ? I find this related entry and this though they did not resolve my question.","Eigenvectors of discrete Laplace matrix for 2D unit square with free boundary is simply $$ \phi(x,y)= \cos(\frac{\pi}{n} kx) \cos(\frac{\pi}{m} ly)  $$ It is easy to see that its 2nd order derivative equals itself (scaled). For example, the Laplace matrix of a 4 by 4 grid is The numerically computed eigenvectors are consistent with the expression $\cos(\frac{\pi}{n} kx) \cos(\frac{\pi}{m} ly)$. My question is what is the following matrix: Precisely put, what is the differential equation on a continuous 2D unit square domain corresponds to this discrete operator? The value 4 corresponds to the inner nodes, value 3 for the boundary nodes, value 2 for the corner nodes. Is this the Laplacian under Neumann boundary condiction? Second question: what are the eigenvectors of such matrix? Of course one can compute them numerically, but is there an analytic expression for these  eigenvectors ? I find this related entry and this though they did not resolve my question.",,"['linear-algebra', 'ordinary-differential-equations', 'operator-theory', 'eigenfunctions']"
36,Stiffness of a system of differential equations,Stiffness of a system of differential equations,,"For a university assignment I've been given three systems of differential equations: $$\bf{\dot{y}} = \begin{pmatrix}-2 & 1\\ 1 & -2\end{pmatrix} * \bf{y} + \begin{pmatrix}2 * \sin(t)\\ 2*(\cos(t) - \sin(t))\end{pmatrix}$$ $$\bf{\dot{y}} = \begin{pmatrix}-2 & 1\\ 998 & -999\end{pmatrix} * \bf{y} + \begin{pmatrix}2 * \sin(t)\\ 999*(\cos(t) - \sin(t))\end{pmatrix}$$ $$\bf{\dot{y}} = \begin{pmatrix}-2 & 1\\ -1.999 & 0.999\end{pmatrix} * \bf{y} + \begin{pmatrix}2 * \sin(t)\\ -0.999*(\cos(t) - \sin(t))\end{pmatrix}$$ For each system, the eigenvalues and the stiffness ratio $S = \frac{\left|Re(\lambda_{max})\right|}{\left|Re(\lambda_{min})\right|}$ have to be calculated. Then the systems should be solved using ode45 and an appropiate stiff solver in MATLAB. This leads to the following results: System 2 is clearly a stiff problem, right? But why isn't system three showing the same behaviour as the stiffness ratio $S$ is the same for both problems? MATLAB-code for reference: https://pastebin.com/Zaj8Ca1j EDIT: I just noticed i forgott to label the axes, the x-axis is the time $t$ and the y-axis is $y_1$ and $y_2$","For a university assignment I've been given three systems of differential equations: $$\bf{\dot{y}} = \begin{pmatrix}-2 & 1\\ 1 & -2\end{pmatrix} * \bf{y} + \begin{pmatrix}2 * \sin(t)\\ 2*(\cos(t) - \sin(t))\end{pmatrix}$$ $$\bf{\dot{y}} = \begin{pmatrix}-2 & 1\\ 998 & -999\end{pmatrix} * \bf{y} + \begin{pmatrix}2 * \sin(t)\\ 999*(\cos(t) - \sin(t))\end{pmatrix}$$ $$\bf{\dot{y}} = \begin{pmatrix}-2 & 1\\ -1.999 & 0.999\end{pmatrix} * \bf{y} + \begin{pmatrix}2 * \sin(t)\\ -0.999*(\cos(t) - \sin(t))\end{pmatrix}$$ For each system, the eigenvalues and the stiffness ratio $S = \frac{\left|Re(\lambda_{max})\right|}{\left|Re(\lambda_{min})\right|}$ have to be calculated. Then the systems should be solved using ode45 and an appropiate stiff solver in MATLAB. This leads to the following results: System 2 is clearly a stiff problem, right? But why isn't system three showing the same behaviour as the stiffness ratio $S$ is the same for both problems? MATLAB-code for reference: https://pastebin.com/Zaj8Ca1j EDIT: I just noticed i forgott to label the axes, the x-axis is the time $t$ and the y-axis is $y_1$ and $y_2$",,"['calculus', 'ordinary-differential-equations', 'matlab']"
37,Show Lipschitz continuity of ODE solution with respect to initial condition,Show Lipschitz continuity of ODE solution with respect to initial condition,,"I am working on the nonlinear pendulum differential equation given by $$ \theta'' = -g\sin \theta. $$ Let $t \geq 0$ be fix, I define for $\theta_0 \in (-\frac\pi2, \frac\pi2)$ a mapping $S$ by $$ S(\theta_0) = \theta(t; \theta_0), $$ where $\theta(t; \theta_0)$ is the solution of the initial value problem for the ODE defined above with initial conditions $\theta'(0) = 0$ and $ \theta(0) = \theta_0$. I believe the mapping $S$ is Lipschitz continuous, but is it true? If yes, how could I prove it?","I am working on the nonlinear pendulum differential equation given by $$ \theta'' = -g\sin \theta. $$ Let $t \geq 0$ be fix, I define for $\theta_0 \in (-\frac\pi2, \frac\pi2)$ a mapping $S$ by $$ S(\theta_0) = \theta(t; \theta_0), $$ where $\theta(t; \theta_0)$ is the solution of the initial value problem for the ODE defined above with initial conditions $\theta'(0) = 0$ and $ \theta(0) = \theta_0$. I believe the mapping $S$ is Lipschitz continuous, but is it true? If yes, how could I prove it?",,['ordinary-differential-equations']
38,Equivalence between a SDE and an ODE,Equivalence between a SDE and an ODE,,"Let $X$ be a continuous semimartingale. We look at the following SDE $$dY(t)=Y(t)dX(t)$$ with $Y(0)=1$ and $Y>0$. The above notation means $Y(t) = 1 + \int_0^t Y(s) \, dX(s)$, it's just a notation. Wikipedia says that if $X$ is differentiable (meaning it sample paths are differentiable), the equation is equivalent to the ODE $$Y'(t)=Y(t)X'(t)$$ Is there a way to see this without solving the SDE ?","Let $X$ be a continuous semimartingale. We look at the following SDE $$dY(t)=Y(t)dX(t)$$ with $Y(0)=1$ and $Y>0$. The above notation means $Y(t) = 1 + \int_0^t Y(s) \, dX(s)$, it's just a notation. Wikipedia says that if $X$ is differentiable (meaning it sample paths are differentiable), the equation is equivalent to the ODE $$Y'(t)=Y(t)X'(t)$$ Is there a way to see this without solving the SDE ?",,"['real-analysis', 'ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals']"
39,Solving following second order matrix differential equation,Solving following second order matrix differential equation,,"Which methods (analytical or numerical) do you suggest to solve the following matrix diff. equation? And why? (Presumably analytical method) $$ \textbf{M}\ddot{V}+\textbf{C}\dot{V}+\textbf{K}V=\textbf{P},$$ where matrices $\ddot{V}$ , $\dot{V}$ and $V$ with respect to $t$ are as follows: $$ \mathbf{M} \begin{bmatrix} \ddot{V}(1,t) \\ \ddot{V}(2,t) \\ \vdots \\ \ddot{V}(n,t) \end{bmatrix} +\mathbf{C} \begin{bmatrix} \dot{V}(1,t) \\ \dot{V}(2,t) \\ \vdots \\ \dot{V}(n,t) \end{bmatrix} +\mathbf{K} \begin{bmatrix} V(1,t) \\ V(2,t) \\ \vdots \\ V(n,t) \end{bmatrix} = \mathbf{P}.$$ In here, the matrices $\textbf{M}$ , $\textbf{C}$ , $\textbf{K}$ and $\textbf{P}$ are as follows ( $\textbf{M}$ , $\textbf{C}$ and $\textbf{K}$ are $n \times n$ matrices, and $\textbf{V}$ , $\textbf{P}$ , $\textbf{F}$ are $n \times 1$ matrices) $$\mathbf{P} = \frac{P}{\rho A} \begin{bmatrix} \sin\left(\dfrac{\pi vt}{l}\right) \\ \sin\left(\dfrac{2\pi vt}{l}\right) \\ \vdots \\ \sin\left(\dfrac{n\pi vt}{l}\right) \end{bmatrix} +\mathbf{F}(t), \quad\text{where } \mathbf{F}(t) = \begin{bmatrix} f_1(t) \\ f_2(t) \\ \vdots \\ f_n(t) \end{bmatrix}.$$ In here, $l$ , $P$ , $A$ , $\rho$ , $\alpha$ , $v$ , $N$ , $E$ and $I$ are constants. It doesn't matter what are these constants or initial or boundary conditions for me. You can select appropriate values. The matrix $\textbf{F}(t)$ is ungiven. So solution $\textbf{V}$ will depend on $\textbf{F}$ .","Which methods (analytical or numerical) do you suggest to solve the following matrix diff. equation? And why? (Presumably analytical method) where matrices , and with respect to are as follows: In here, the matrices , , and are as follows ( , and are matrices, and , , are matrices) In here, , , , , , , , and are constants. It doesn't matter what are these constants or initial or boundary conditions for me. You can select appropriate values. The matrix is ungiven. So solution will depend on ."," \textbf{M}\ddot{V}+\textbf{C}\dot{V}+\textbf{K}V=\textbf{P}, \ddot{V} \dot{V} V t  \mathbf{M} \begin{bmatrix} \ddot{V}(1,t) \\ \ddot{V}(2,t) \\ \vdots \\ \ddot{V}(n,t) \end{bmatrix} +\mathbf{C} \begin{bmatrix} \dot{V}(1,t) \\ \dot{V}(2,t) \\ \vdots \\ \dot{V}(n,t) \end{bmatrix} +\mathbf{K} \begin{bmatrix} V(1,t) \\ V(2,t) \\ \vdots \\ V(n,t) \end{bmatrix} = \mathbf{P}. \textbf{M} \textbf{C} \textbf{K} \textbf{P} \textbf{M} \textbf{C} \textbf{K} n \times n \textbf{V} \textbf{P} \textbf{F} n \times 1 \mathbf{P} = \frac{P}{\rho A} \begin{bmatrix} \sin\left(\dfrac{\pi vt}{l}\right) \\ \sin\left(\dfrac{2\pi vt}{l}\right) \\ \vdots \\ \sin\left(\dfrac{n\pi vt}{l}\right) \end{bmatrix} +\mathbf{F}(t), \quad\text{where } \mathbf{F}(t) = \begin{bmatrix} f_1(t) \\ f_2(t) \\ \vdots \\ f_n(t) \end{bmatrix}. l P A \rho \alpha v N E I \textbf{F}(t) \textbf{V} \textbf{F}","['ordinary-differential-equations', 'numerical-methods', 'matrix-equations']"
40,Can $ \boldsymbol{u}' - \boldsymbol{A}(t) \boldsymbol{u} = \boldsymbol{f}(t)$ be solved using an matrix exponential integrating factor?,Can  be solved using an matrix exponential integrating factor?, \boldsymbol{u}' - \boldsymbol{A}(t) \boldsymbol{u} = \boldsymbol{f}(t),"Can $$ \boldsymbol{u}' - \boldsymbol{A}(t) \boldsymbol{u} = \boldsymbol{f}(t)$$ be solved using an integration factor method analogous to that used in the non-system case? (i.e. $u'-a(t)u=f(t)$). Giving the following solution: $$ \boldsymbol{u} = e^{-\int^t A(k)dk}\left (\int^t e^{\int^s A(k)dk}\boldsymbol{f}(s) ds + \boldsymbol{c}\right )$$ where the matrix exponential defined in the usual way. If you rearrange the solution and differentiate you find $$ \frac {\text{d}e^{-\int^t A(k)dk}} {\text{d}t} \boldsymbol{u} + e^{-\int^t A(k)dk} \boldsymbol{u}' = e^{-\int^t A(k)dk} \boldsymbol{f} $$ If this is indeed the solution, it would then imply that $$ \frac {\text{d}e^{-\int^t A(k)dk}} {\text{d}t} =  - e^{-\int^t A(k)dk} \boldsymbol{A} $$ However this doesn't seem to be the case from Eq. 2.1 of http://aip.scitation.org/doi/pdf/10.1063/1.1705306 referenced on the Wiki page for matrix exponentials . EDIT: Disproved on page 6 here .","Can $$ \boldsymbol{u}' - \boldsymbol{A}(t) \boldsymbol{u} = \boldsymbol{f}(t)$$ be solved using an integration factor method analogous to that used in the non-system case? (i.e. $u'-a(t)u=f(t)$). Giving the following solution: $$ \boldsymbol{u} = e^{-\int^t A(k)dk}\left (\int^t e^{\int^s A(k)dk}\boldsymbol{f}(s) ds + \boldsymbol{c}\right )$$ where the matrix exponential defined in the usual way. If you rearrange the solution and differentiate you find $$ \frac {\text{d}e^{-\int^t A(k)dk}} {\text{d}t} \boldsymbol{u} + e^{-\int^t A(k)dk} \boldsymbol{u}' = e^{-\int^t A(k)dk} \boldsymbol{f} $$ If this is indeed the solution, it would then imply that $$ \frac {\text{d}e^{-\int^t A(k)dk}} {\text{d}t} =  - e^{-\int^t A(k)dk} \boldsymbol{A} $$ However this doesn't seem to be the case from Eq. 2.1 of http://aip.scitation.org/doi/pdf/10.1063/1.1705306 referenced on the Wiki page for matrix exponentials . EDIT: Disproved on page 6 here .",,"['real-analysis', 'ordinary-differential-equations']"
41,"$(\nabla - \vec{g}) \cdot \vec{f}(r,\theta)=0$",,"(\nabla - \vec{g}) \cdot \vec{f}(r,\theta)=0","Knowing $\vec{g}$, how can above equation be solved for $f$? There is a symmetry in $\phi$ in spherical coordinate, so the equation is a 2D equation. Also, $\vec{g}$ is a complete curl. Could anyone please help me? more information $\vec{f}$ could be written as $\nabla h(r,\theta)$. Boundary condition: $\vec{f}_r=0$ at infinity. $f_r(r=1)= - A$ which $A$ is a constant. If more is needed: $g_\theta(r,\theta) = \sin \theta (1-\frac{\sin \theta }{2 r^2})-\frac{\alpha \sin ^2 \theta \cos \theta}{r^4}$ $g_r(r,\theta) = -\cos \theta (1-\frac{1}{ r^3})+\frac{3\alpha}{2r^2}(\frac{1}{r^2}-1)(\cos^2 \theta-\frac{1}{3})$","Knowing $\vec{g}$, how can above equation be solved for $f$? There is a symmetry in $\phi$ in spherical coordinate, so the equation is a 2D equation. Also, $\vec{g}$ is a complete curl. Could anyone please help me? more information $\vec{f}$ could be written as $\nabla h(r,\theta)$. Boundary condition: $\vec{f}_r=0$ at infinity. $f_r(r=1)= - A$ which $A$ is a constant. If more is needed: $g_\theta(r,\theta) = \sin \theta (1-\frac{\sin \theta }{2 r^2})-\frac{\alpha \sin ^2 \theta \cos \theta}{r^4}$ $g_r(r,\theta) = -\cos \theta (1-\frac{1}{ r^3})+\frac{3\alpha}{2r^2}(\frac{1}{r^2}-1)(\cos^2 \theta-\frac{1}{3})$",,"['ordinary-differential-equations', 'partial-differential-equations']"
42,Generalizing the concept of ODE,Generalizing the concept of ODE,,"An ODE is a relationship such as $F(t,y(t),\dots,y^{(n)}(t))=0$ with $F:\Omega \subset R \times R^n \to R^n$, $\Omega$ open set not empty. What happens if $F$ is similar, but  contains derivative of $y$ arbitrary large? Does anyone know where I can find some material that talks about this theme? For example, the easiest thing that bumps in my mind: $\sum_{i=0}^{\infty} y^{(i)}=0$ Now, lets put initial conditions : $y^{(n)}(0)=n^2$ for all $n\in N$ What can I say about the solutions of this equation? I don't expect to calculate them analitically, but can I get some info? (Existence, unicity, global existence...)","An ODE is a relationship such as $F(t,y(t),\dots,y^{(n)}(t))=0$ with $F:\Omega \subset R \times R^n \to R^n$, $\Omega$ open set not empty. What happens if $F$ is similar, but  contains derivative of $y$ arbitrary large? Does anyone know where I can find some material that talks about this theme? For example, the easiest thing that bumps in my mind: $\sum_{i=0}^{\infty} y^{(i)}=0$ Now, lets put initial conditions : $y^{(n)}(0)=n^2$ for all $n\in N$ What can I say about the solutions of this equation? I don't expect to calculate them analitically, but can I get some info? (Existence, unicity, global existence...)",,['ordinary-differential-equations']
43,Solution to ODE with Dirac Delta satisfies ODE,Solution to ODE with Dirac Delta satisfies ODE,,"I am working on a problem where I have the following ODE. $$m\dot{v}+bv=\delta_I(t)$$ where $$\delta_I(t)=\begin{cases}0, & \text{for}&t\ne0\\ I, & \text{for} &t=0\end{cases}.$$ The solution $v(t)$ was derived using Laplace transforms, the ODE in the Laplace domain is (with $0$ initial conditions)$$(ms+b)V(s)=I$$ giving $$v(t)=\frac{I}{m}e^{-bt/m}.$$ How does this solution satisfy the original ODE though? At $t\ne0$ everything is good,$$-\frac{Ib}{m}e^{-bt/m}+\frac{Ib}{m}e^{-bt/m}=0,$$ while at $t=0$, $$-\frac{Ib}{m}+\frac{Ib}{m}=I$$ the result seems to be saying $0=1$ which is obviously false. What am I missing here?","I am working on a problem where I have the following ODE. $$m\dot{v}+bv=\delta_I(t)$$ where $$\delta_I(t)=\begin{cases}0, & \text{for}&t\ne0\\ I, & \text{for} &t=0\end{cases}.$$ The solution $v(t)$ was derived using Laplace transforms, the ODE in the Laplace domain is (with $0$ initial conditions)$$(ms+b)V(s)=I$$ giving $$v(t)=\frac{I}{m}e^{-bt/m}.$$ How does this solution satisfy the original ODE though? At $t\ne0$ everything is good,$$-\frac{Ib}{m}e^{-bt/m}+\frac{Ib}{m}e^{-bt/m}=0,$$ while at $t=0$, $$-\frac{Ib}{m}+\frac{Ib}{m}=I$$ the result seems to be saying $0=1$ which is obviously false. What am I missing here?",,"['ordinary-differential-equations', 'laplace-transform', 'dirac-delta']"
44,Show the following PDE from the known nonlinear ODE and its solution?,Show the following PDE from the known nonlinear ODE and its solution?,,"Suppose I have the following nonlinear ODE: $$\frac{dy}{dt}=F(y)$$ with $y(0) = z$ . Suppose the solution of this nonlinear ODE is $y(t,z)$ . Show the following: $$\frac{\partial y(t,z)}{\partial t}\Bigr\rvert_{z \text{ fixed }} -F(z)\frac{\partial y(t,z)}{\partial z} \Bigr\rvert_{t \text{ fixed }} =0$$ We can consider the simplest case such as $F(y) = y$ . And can solve $y(t) = ze^t$ and then plug into the partial differential equation. We have $F(z) = z$ . To formally prove this, I am not sure where to start with. I am also confused about the following, $$\frac{\partial y(t,z)}{\partial t}\Bigr\rvert_{z \text{ fixed }} = F(z)??$$ If this is true, then $\frac{\partial y(t,z)}{\partial z} \Bigr\rvert_{t \text{ fixed }} = 1$ , which does not make any sense obviously. The following is my new work on this: $$\int_z^y \frac{dy}{F(y)} = \int_{0}^t dt \Rightarrow G(y) - G(z) = t$$ where $$G(y) = \int \frac{dy}{F(y)}, \text{ which is a indefinite integral}$$ Now we have $$G(y) = t + G(z) \Rightarrow y(t,z) = G^{-1}(t + G(z))$$ where $$G^{-1}(y) = ?$$ I am a bit confused the following: Does the above derivation make sense? What is $G^{-1}$ here? I am also not quite sure the following? $$\frac{\partial G^{-1}(t+G(z))}{\partial t} = \frac{\partial G^{-1}(t+G(z))}{\partial (t+G(z))}\frac{\partial (t+G(z)))}{\partial t} = ? $$","Suppose I have the following nonlinear ODE: with . Suppose the solution of this nonlinear ODE is . Show the following: We can consider the simplest case such as . And can solve and then plug into the partial differential equation. We have . To formally prove this, I am not sure where to start with. I am also confused about the following, If this is true, then , which does not make any sense obviously. The following is my new work on this: where Now we have where I am a bit confused the following: Does the above derivation make sense? What is here? I am also not quite sure the following?","\frac{dy}{dt}=F(y) y(0) = z y(t,z) \frac{\partial y(t,z)}{\partial t}\Bigr\rvert_{z \text{ fixed }} -F(z)\frac{\partial y(t,z)}{\partial z} \Bigr\rvert_{t \text{ fixed }} =0 F(y) = y y(t) = ze^t F(z) = z \frac{\partial y(t,z)}{\partial t}\Bigr\rvert_{z \text{ fixed }} = F(z)?? \frac{\partial y(t,z)}{\partial z} \Bigr\rvert_{t \text{ fixed }} = 1 \int_z^y \frac{dy}{F(y)} = \int_{0}^t dt \Rightarrow G(y) - G(z) = t G(y) = \int \frac{dy}{F(y)}, \text{ which is a indefinite integral} G(y) = t + G(z) \Rightarrow y(t,z) = G^{-1}(t + G(z)) G^{-1}(y) = ? G^{-1} \frac{\partial G^{-1}(t+G(z))}{\partial t} = \frac{\partial G^{-1}(t+G(z))}{\partial (t+G(z))}\frac{\partial (t+G(z)))}{\partial t} = ? ","['ordinary-differential-equations', 'partial-differential-equations', 'nonlinear-system']"
45,How to empirically verify convergence results with stochastic differential equations (with fractional Brownian motions),How to empirically verify convergence results with stochastic differential equations (with fractional Brownian motions),,"Let $ \frac{1}{2} < H < 1$ and let $B^H_t$ be a fractional Brownian motion with Hurst parameter $H$. Then the following stochastic differential equation $$\mathrm{d}Y_t = 5Y_t\mathrm{d}B^H_t, \quad Y_0 = 1$$ has as unique solution $Y_t = \exp(5B^H_t)$ in the sense of the Young (1936) integral. Recently there has been some literature on solving such equations numerically. When considering the Euler-Maruyama scheme: \begin{align*} &Y_0^{(n)} = \xi \\ &Y_{(k+1)/n}^{(n)} = Y_{k/n}^n + 5Y_{k/n}^n\left( B^H_{(k+1)/n} - B^H_{k/n}\right), \quad k \in \{ 0,\dots, n-1\} \end{align*} on the interval $[0,1]$ and $n$ is the number of gridpoints. It has been proven (p. 5) that the following convergence result holds: \begin{equation*} n^{2H - 1} \| Y^{(n)} - Y \|_{\infty} \overset{\text{a.s.}}{\to} \dfrac{1}{2}\left(\sup_{t \in [0, 1]}\left\lvert\int_0^1 D_s Y_t \mathrm{d}s\right\rvert\right)\label{error1} \end{equation*} Where $D_sY_t$ is the Malliavin derivative of $Y_t$. I want to empirically validate this result (and for a different, better approximation scheme). I have implemented the numerical scheme in R and got satisfactory results, but I am now stuck on how to exactly test convergence and how to compare my approximation with the exact result. Is there any literature that details how to do this? More specifically: (1) How can you compare the approximation with the exact result, given that it depends on the sample of the fractional Brownian motion and (2) How do you test the convergence result given above. When dealing with normal ODEs, these questions are easy. But at the moment I am stuck... Edit1: In the end I want to produce graphs similar to the ones given in this presentation on slides 14 through 17.","Let $ \frac{1}{2} < H < 1$ and let $B^H_t$ be a fractional Brownian motion with Hurst parameter $H$. Then the following stochastic differential equation $$\mathrm{d}Y_t = 5Y_t\mathrm{d}B^H_t, \quad Y_0 = 1$$ has as unique solution $Y_t = \exp(5B^H_t)$ in the sense of the Young (1936) integral. Recently there has been some literature on solving such equations numerically. When considering the Euler-Maruyama scheme: \begin{align*} &Y_0^{(n)} = \xi \\ &Y_{(k+1)/n}^{(n)} = Y_{k/n}^n + 5Y_{k/n}^n\left( B^H_{(k+1)/n} - B^H_{k/n}\right), \quad k \in \{ 0,\dots, n-1\} \end{align*} on the interval $[0,1]$ and $n$ is the number of gridpoints. It has been proven (p. 5) that the following convergence result holds: \begin{equation*} n^{2H - 1} \| Y^{(n)} - Y \|_{\infty} \overset{\text{a.s.}}{\to} \dfrac{1}{2}\left(\sup_{t \in [0, 1]}\left\lvert\int_0^1 D_s Y_t \mathrm{d}s\right\rvert\right)\label{error1} \end{equation*} Where $D_sY_t$ is the Malliavin derivative of $Y_t$. I want to empirically validate this result (and for a different, better approximation scheme). I have implemented the numerical scheme in R and got satisfactory results, but I am now stuck on how to exactly test convergence and how to compare my approximation with the exact result. Is there any literature that details how to do this? More specifically: (1) How can you compare the approximation with the exact result, given that it depends on the sample of the fractional Brownian motion and (2) How do you test the convergence result given above. When dealing with normal ODEs, these questions are easy. But at the moment I am stuck... Edit1: In the end I want to produce graphs similar to the ones given in this presentation on slides 14 through 17.",,"['ordinary-differential-equations', 'numerical-methods', 'stochastic-processes', 'brownian-motion', 'malliavin-calculus']"
46,"What's the difference between ""quadrature"" and ""integration""?","What's the difference between ""quadrature"" and ""integration""?",,"What is the difference between the terms ""quadrature"" and ""integration""? From what I gather, ""quadrature"" is the more archaic term for ""integration,"" but I see ""quadrature"" used frequently in the study of differential equations for when one can solve a differential equation by integration (cf. the phrase: ""can be integrated by quadrature""). I've also seen ""quadrature"" used as a shorthand for ""to integrate numerically."" Is that what's meant by ""quadrature"" most frequently today?","What is the difference between the terms ""quadrature"" and ""integration""? From what I gather, ""quadrature"" is the more archaic term for ""integration,"" but I see ""quadrature"" used frequently in the study of differential equations for when one can solve a differential equation by integration (cf. the phrase: ""can be integrated by quadrature""). I've also seen ""quadrature"" used as a shorthand for ""to integrate numerically."" Is that what's meant by ""quadrature"" most frequently today?",,"['integration', 'ordinary-differential-equations', 'terminology']"
47,Total number of roots of $f(x)=g(x)$,Total number of roots of,f(x)=g(x),"Let $f(x)$ be non positive continuous function and $F(x)=\int_0^x f(t) dt,\forall x\geq 0$ and $f(x)\geq cF(x)$ where $c \gt 0$ and let $g:[0,\infty]\to R$ be a function such that $\frac{d}{dx} g(x)< g(x),\forall x>0$ and $ g(0)=0$ The question is to find the total number of roots of $f(x)=g(x)$ Since $f(x)$ is a non positive function it implies that $F(x)$ is a decreasing function also F(0)=0 implies $F(x)\leq 0$.So $f(0)=0$ from $f(0)\geq cF(0)$ and the condition that $f$ is non positive. I couldn't proceed after this.Any ideas?","Let $f(x)$ be non positive continuous function and $F(x)=\int_0^x f(t) dt,\forall x\geq 0$ and $f(x)\geq cF(x)$ where $c \gt 0$ and let $g:[0,\infty]\to R$ be a function such that $\frac{d}{dx} g(x)< g(x),\forall x>0$ and $ g(0)=0$ The question is to find the total number of roots of $f(x)=g(x)$ Since $f(x)$ is a non positive function it implies that $F(x)$ is a decreasing function also F(0)=0 implies $F(x)\leq 0$.So $f(0)=0$ from $f(0)\geq cF(0)$ and the condition that $f$ is non positive. I couldn't proceed after this.Any ideas?",,"['calculus', 'ordinary-differential-equations']"
48,How do I obtain the first fundamental form in terms of the arclength and unit normal of an arbitrary ray in the real plane?,How do I obtain the first fundamental form in terms of the arclength and unit normal of an arbitrary ray in the real plane?,,"My question is based on the 1981 paper ""Computation of wave fields in inhomogenous media"" by Cerveny et.al. (I will add any appropriately needed information to this posted question, so there is no need to obtain the paper). They search for solutions to the wave equation $$\frac{\partial^2u}{\partial{x^2}} + \frac{\partial^2u}{\partial{z^2}} = \frac{1}{V(x,z)^2}\frac{\partial^2u}{\partial{t^2}}$$ where $V(x,z)$ is known. The $\mathbb{R}^2$ Cartesian coordinates $(x,z)$ are used. An arbitrary ray $\Omega \subset \mathbb{R}^2$ is taken, and parameterized by arclength $s$ increasing from an initial point $s = 0$ at $t = 0$. The real plane is parameterized based on $\Omega$ by the arclength $s$ and the unit normal $n$ off the ray (see figure, reproduced from reference). For non-straight rays $\Omega$ there are irregular regions (where different pairs $(s,n)$ that describe the same point in $\mathbb{R}^2$), and the complement in $\mathbb{R}^2$ is called the regularity region. They then obtain the first fundamental form in terms of a small segment $dr$ of arclength as $$dr^2 = h^2 ds^2 + dn^2$$ where $$h = 1 + \frac{n}{v}v_n$$ for $v(s) = V(s,0)$ and $$v_n = \left[\frac{\partial V(s,n)}{\partial n}\right]_{n=0}.$$ My question: How did they obtain the first fundamental form? I know that $$dr^2 = F_{ss} ds^2 + 2F_sF_n ds dn + F_{nn} dn^2$$ where $F(s,n) = (x,z)$ is the coordinate transformation and $F_s$ is the first derivative of $F$ wrt $s$, $F_{ss}$ the second, $F_{sn}$ the mixed derivative. However, they obtain the first fundamental form with arbitrary array $\Omega$. Let me know if you need more information. Reference: Červený, Vlastislav, Mikhail M. Popov, and Ivan Pšenčík. ""Computation of wave fields in inhomogeneous media—Gaussian beam approach."" Geophysical Journal International 70 .1 (1982): 109-128.","My question is based on the 1981 paper ""Computation of wave fields in inhomogenous media"" by Cerveny et.al. (I will add any appropriately needed information to this posted question, so there is no need to obtain the paper). They search for solutions to the wave equation $$\frac{\partial^2u}{\partial{x^2}} + \frac{\partial^2u}{\partial{z^2}} = \frac{1}{V(x,z)^2}\frac{\partial^2u}{\partial{t^2}}$$ where $V(x,z)$ is known. The $\mathbb{R}^2$ Cartesian coordinates $(x,z)$ are used. An arbitrary ray $\Omega \subset \mathbb{R}^2$ is taken, and parameterized by arclength $s$ increasing from an initial point $s = 0$ at $t = 0$. The real plane is parameterized based on $\Omega$ by the arclength $s$ and the unit normal $n$ off the ray (see figure, reproduced from reference). For non-straight rays $\Omega$ there are irregular regions (where different pairs $(s,n)$ that describe the same point in $\mathbb{R}^2$), and the complement in $\mathbb{R}^2$ is called the regularity region. They then obtain the first fundamental form in terms of a small segment $dr$ of arclength as $$dr^2 = h^2 ds^2 + dn^2$$ where $$h = 1 + \frac{n}{v}v_n$$ for $v(s) = V(s,0)$ and $$v_n = \left[\frac{\partial V(s,n)}{\partial n}\right]_{n=0}.$$ My question: How did they obtain the first fundamental form? I know that $$dr^2 = F_{ss} ds^2 + 2F_sF_n ds dn + F_{nn} dn^2$$ where $F(s,n) = (x,z)$ is the coordinate transformation and $F_s$ is the first derivative of $F$ wrt $s$, $F_{ss}$ the second, $F_{sn}$ the mixed derivative. However, they obtain the first fundamental form with arbitrary array $\Omega$. Let me know if you need more information. Reference: Červený, Vlastislav, Mikhail M. Popov, and Ivan Pšenčík. ""Computation of wave fields in inhomogeneous media—Gaussian beam approach."" Geophysical Journal International 70 .1 (1982): 109-128.",,"['ordinary-differential-equations', 'multivariable-calculus', 'differential-geometry', 'coordinate-systems', 'wave-equation']"
49,Poincare map of this system,Poincare map of this system,,"Let $$x'=1+y-x^2-y^2$$ $$y'=1-x-x^2-y^2$$ How do I use the Poincare map to show that this has a not asymptotically stable solution? What I did: I transformed the system to polar coordinates $$r'=(\cos\theta+\sin\theta)(1-r^2)$$ $$\theta'=(\cos\theta-\sin\theta)(1/r-r)-1$$ which has a periodic solution $(r,\theta)=(1,-t)$ or $f(t)=(x,y)=(\cos(t),-\sin(t))$. How do I find the Poincare map? And how do I show that $f(t)$ is not aymptotically stable? Edit: The original question gives the system as above and asks for an explicit periodic solution $f(t)$ (which I'm sure is the one I found above). Show that $f(t)$ is stable (but not asymptotically stable). What can you say about the Poincare map?","Let $$x'=1+y-x^2-y^2$$ $$y'=1-x-x^2-y^2$$ How do I use the Poincare map to show that this has a not asymptotically stable solution? What I did: I transformed the system to polar coordinates $$r'=(\cos\theta+\sin\theta)(1-r^2)$$ $$\theta'=(\cos\theta-\sin\theta)(1/r-r)-1$$ which has a periodic solution $(r,\theta)=(1,-t)$ or $f(t)=(x,y)=(\cos(t),-\sin(t))$. How do I find the Poincare map? And how do I show that $f(t)$ is not aymptotically stable? Edit: The original question gives the system as above and asks for an explicit periodic solution $f(t)$ (which I'm sure is the one I found above). Show that $f(t)$ is stable (but not asymptotically stable). What can you say about the Poincare map?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-theory']"
50,Solution Form to Differential Inequality,Solution Form to Differential Inequality,,I have the following differential inequality: $$2\frac{\partial g(x)}{\partial x}+\left[\frac{\partial g(x)}{\partial x}\right]^2+f(x)\cdot[g(x)]^2 < 0$$ Can you recommend a solution for $g(x)$ in terms of $f(x)$?,I have the following differential inequality: $$2\frac{\partial g(x)}{\partial x}+\left[\frac{\partial g(x)}{\partial x}\right]^2+f(x)\cdot[g(x)]^2 < 0$$ Can you recommend a solution for $g(x)$ in terms of $f(x)$?,,['ordinary-differential-equations']
51,Verifying solution of Heat Equation,Verifying solution of Heat Equation,,"Let $\theta$ be such that $0<\theta<1/2$. Show that $$F(t,x)=\int_{-\infty}^{+\infty}\exp[i\tau t-(i\tau)^{1/2}x-(i\tau)^\theta]d\tau$$ defines a $C^\infty$ function in $\mathbb{R}^2$ which solves the homogeneous heat equation $F_t=F_{xx}$. Also show that $F$ can be written as $$\int_{-a-i\infty}^{a+i\infty}\exp[-zt-z^{1/2}x-z^\theta]dz$$ where complex integration is performed over the vertical straight line $\text{Re} z=-a$ where $a>0$. How to proceed with this question ?","Let $\theta$ be such that $0<\theta<1/2$. Show that $$F(t,x)=\int_{-\infty}^{+\infty}\exp[i\tau t-(i\tau)^{1/2}x-(i\tau)^\theta]d\tau$$ defines a $C^\infty$ function in $\mathbb{R}^2$ which solves the homogeneous heat equation $F_t=F_{xx}$. Also show that $F$ can be written as $$\int_{-a-i\infty}^{a+i\infty}\exp[-zt-z^{1/2}x-z^\theta]dz$$ where complex integration is performed over the vertical straight line $\text{Re} z=-a$ where $a>0$. How to proceed with this question ?",,"['ordinary-differential-equations', 'complex-integration', 'heat-equation', 'linear-pde']"
52,Stationary points and linearisation of non-linear system,Stationary points and linearisation of non-linear system,,"So, the problem is: Find and discuss the behavior of the stationary points of the system : $$ x'=-y+x\cdot (x^2+y^2)\cdot \sin\sqrt{x^2+y^2} =f(x,y)$$ $$ y'=x+y\cdot (x^2+y^2)\cdot \sin\sqrt{x^2+y^2}=g(x,y)$$ So in the beggining I Linearised the non-linear system, using the limits: $$ \lim_{ r\to 0}\frac{f_1(x,y)}{r}=0 $$ and $$\lim_{r \to 0}\frac{g_1(x,y)}{r}=0$$ where $$f_1(x,y)=x\cdot (x^2+y^2)\cdot \sin\sqrt{x^2+y^2}$$ and  $$g_1(x,y)=y\cdot (x^2+y^2)\cdot \sin\sqrt{x^2+y^2}$$ I linearise the non-linear system and reach it to the form: $$x'=-y$$ and $$y'=x$$ but the problem is that i don't know how to calculate the stationary points from this non-linear system so to proceed with the next question of my problem .I also know that when i find the stationary points i have to take the jacobian so to characterise my stationary points.I would really, really appreciate any thorough help or hints/tips. Thanks in advance!","So, the problem is: Find and discuss the behavior of the stationary points of the system : $$ x'=-y+x\cdot (x^2+y^2)\cdot \sin\sqrt{x^2+y^2} =f(x,y)$$ $$ y'=x+y\cdot (x^2+y^2)\cdot \sin\sqrt{x^2+y^2}=g(x,y)$$ So in the beggining I Linearised the non-linear system, using the limits: $$ \lim_{ r\to 0}\frac{f_1(x,y)}{r}=0 $$ and $$\lim_{r \to 0}\frac{g_1(x,y)}{r}=0$$ where $$f_1(x,y)=x\cdot (x^2+y^2)\cdot \sin\sqrt{x^2+y^2}$$ and  $$g_1(x,y)=y\cdot (x^2+y^2)\cdot \sin\sqrt{x^2+y^2}$$ I linearise the non-linear system and reach it to the form: $$x'=-y$$ and $$y'=x$$ but the problem is that i don't know how to calculate the stationary points from this non-linear system so to proceed with the next question of my problem .I also know that when i find the stationary points i have to take the jacobian so to characterise my stationary points.I would really, really appreciate any thorough help or hints/tips. Thanks in advance!",,"['linear-algebra', 'ordinary-differential-equations', 'linear-transformations', 'dynamical-systems', 'linearization']"
53,A pointwise bound for $\frac{\partial}{\partial t}u=\Delta u+ au$,A pointwise bound for,\frac{\partial}{\partial t}u=\Delta u+ au,"In this question I asked whether the Dirichlet PDE:  $$\frac{\partial}{\partial t}u=\Delta u+ au$$ over a bounded smooth open subset $\Omega \subset \mathbb{R}^N$ has a bound in the form $$|u(t)|_{L^2(\Omega)}\leq Me^{\omega t}.$$ I wonder if we have a pointwise version of such an estimate. That is: $$|u(t,x)|\leq M_x e^{\omega_xt}$$ where $M_x$ and $\omega_x$ are constants that may depend on $x$. I thought about using some comparison between $L^2$-norm and $L^\infty$-norm, but we only have  $$|.|_{L^2}\leq C|.|_{\infty}.$$ I don't know but I feel that only finding a closed form for the solution will solve the problem.","In this question I asked whether the Dirichlet PDE:  $$\frac{\partial}{\partial t}u=\Delta u+ au$$ over a bounded smooth open subset $\Omega \subset \mathbb{R}^N$ has a bound in the form $$|u(t)|_{L^2(\Omega)}\leq Me^{\omega t}.$$ I wonder if we have a pointwise version of such an estimate. That is: $$|u(t,x)|\leq M_x e^{\omega_xt}$$ where $M_x$ and $\omega_x$ are constants that may depend on $x$. I thought about using some comparison between $L^2$-norm and $L^\infty$-norm, but we only have  $$|.|_{L^2}\leq C|.|_{\infty}.$$ I don't know but I feel that only finding a closed form for the solution will solve the problem.",,"['ordinary-differential-equations', 'partial-differential-equations', 'lp-spaces']"
54,Wave equation for a contracting circle,Wave equation for a contracting circle,,"I am trying to solve the wave equation for a periodically contracting and expanding circle, in two dimensions. It was so long ago that I did any kind of similar calculation that I have kinda forgotten how to proceed. What I remember is that the wave equation in polar coordinates leads to the Bessel equation. But is that really the way to go about it in this case? Edit: The reason for my confusion stems from the fact that: $$\partial_{\alpha}\partial_{\alpha}X^{\mu}=0$$ where $\alpha=(\tau,\lambda)$ and $X(\tau,\lambda)=(X^1,X^2)$, so for polar coordinates I would have that $r(\tau,\lambda)$ similarly for $\theta$. Therefore I doubt that I would need reparametrize the differential equation to Bassel equation. Am I correct in thinking that it would be unwise to do so?","I am trying to solve the wave equation for a periodically contracting and expanding circle, in two dimensions. It was so long ago that I did any kind of similar calculation that I have kinda forgotten how to proceed. What I remember is that the wave equation in polar coordinates leads to the Bessel equation. But is that really the way to go about it in this case? Edit: The reason for my confusion stems from the fact that: $$\partial_{\alpha}\partial_{\alpha}X^{\mu}=0$$ where $\alpha=(\tau,\lambda)$ and $X(\tau,\lambda)=(X^1,X^2)$, so for polar coordinates I would have that $r(\tau,\lambda)$ similarly for $\theta$. Therefore I doubt that I would need reparametrize the differential equation to Bassel equation. Am I correct in thinking that it would be unwise to do so?",,['ordinary-differential-equations']
55,A question about Sturm Liouville of transforming it into normal form,A question about Sturm Liouville of transforming it into normal form,,"I have a question about Sturm Liouville problem. Given a SL equation, say $\frac{d^2 y}{dx^2}+p(x)\frac{d y}{dx}+(q(x)+\lambda r(x))y=0$, how to transform it to its normal form $\frac{d^2 \eta}{d\xi^2}+(\phi(\xi)+\lambda)\eta=0$? How one can think of the transformation? Note that I need to change both of independent and dependent variables. Indeed, I can find corresponding transformation but I would like to know how to think of the corresponding transformation. What's more, if I do not consider changing independent variables, I can change to normal form as follow: Let $y=u(x)v(x)$, differentiating and let the coefficients involving first order term to be 0, one can solve $u(x)$ and get an equation of $v(x)$ in normal form. However, many authors use the transformation of both independent and dependent variables when I read their papers. The question is what are the benefits of considering transformation of both independent and dependent variables?","I have a question about Sturm Liouville problem. Given a SL equation, say $\frac{d^2 y}{dx^2}+p(x)\frac{d y}{dx}+(q(x)+\lambda r(x))y=0$, how to transform it to its normal form $\frac{d^2 \eta}{d\xi^2}+(\phi(\xi)+\lambda)\eta=0$? How one can think of the transformation? Note that I need to change both of independent and dependent variables. Indeed, I can find corresponding transformation but I would like to know how to think of the corresponding transformation. What's more, if I do not consider changing independent variables, I can change to normal form as follow: Let $y=u(x)v(x)$, differentiating and let the coefficients involving first order term to be 0, one can solve $u(x)$ and get an equation of $v(x)$ in normal form. However, many authors use the transformation of both independent and dependent variables when I read their papers. The question is what are the benefits of considering transformation of both independent and dependent variables?",,"['ordinary-differential-equations', 'proof-verification', 'sturm-liouville']"
56,Supplemental reference request-Graduate level PDE problems and solutions book,Supplemental reference request-Graduate level PDE problems and solutions book,,"I have been able to find these two but I don't know how valuable they are as a reference, ""Problems and examples in differential equations"" By Biler, and, ""Partial Differential Equations through Examples and Exercises"" by Pap. However, I don't know if these are any good, or the ones that I'm looking for, maybe you guys can answer that. Side note: these is my second course on PDEs, my first course covered the usual topics in a standard PDE class for undergraduates, and was less rigorous than what's expected as a graduate student. I also found unpublished handbooks, with problems and their solutions, that are occasionally encountered in graduate level courses in ODE, and PDE, by UCLA Prof. Yanovsky, that deals with more theoretical questions on PDEs that I am looking for. So a textbook along the lines of these notes should suffice. I'm hoping there is more textbooks, or more lecture notes, out there which focuses more on the theoretical side of PDE. Similar to what these General Topology supplemental texts provided, ""Elementary Topology Problem Textbook"" by Viro, and, ""Fundamentals of General Topology: Problems and Exercises"" by Arkhangel'skii. I doubt any theory focused PDE book will contain a solutions manual. I am focused on a textbook that is at the level of ""Partial Differential Equations"" by Fritz.","I have been able to find these two but I don't know how valuable they are as a reference, ""Problems and examples in differential equations"" By Biler, and, ""Partial Differential Equations through Examples and Exercises"" by Pap. However, I don't know if these are any good, or the ones that I'm looking for, maybe you guys can answer that. Side note: these is my second course on PDEs, my first course covered the usual topics in a standard PDE class for undergraduates, and was less rigorous than what's expected as a graduate student. I also found unpublished handbooks, with problems and their solutions, that are occasionally encountered in graduate level courses in ODE, and PDE, by UCLA Prof. Yanovsky, that deals with more theoretical questions on PDEs that I am looking for. So a textbook along the lines of these notes should suffice. I'm hoping there is more textbooks, or more lecture notes, out there which focuses more on the theoretical side of PDE. Similar to what these General Topology supplemental texts provided, ""Elementary Topology Problem Textbook"" by Viro, and, ""Fundamentals of General Topology: Problems and Exercises"" by Arkhangel'skii. I doubt any theory focused PDE book will contain a solutions manual. I am focused on a textbook that is at the level of ""Partial Differential Equations"" by Fritz.",,"['ordinary-differential-equations', 'reference-request', 'soft-question']"
57,Solving a first order differential equation involving $\sin(x/y)$,Solving a first order differential equation involving,\sin(x/y),I am trying to solve the differential equation $$ y'\left(x\right) - \frac{y\left(x\right)}{2x} = x\sin\left(x \over y\left(x\right)\right) $$ I think it is separable variable differential equations. I tried to substitute:  $$z=\frac{x}{y}$$ and $$y'=\frac {z-z'x}{z^2}$$ $$\frac {z-z'x}{z^2}-\frac{1}{2z}=x\sin(z)$$ multiply by $z^2$ $$z-z'x -\frac{z}{2}=z^2x\sin(z)$$ And now I have no idea how to manipulate this.,I am trying to solve the differential equation $$ y'\left(x\right) - \frac{y\left(x\right)}{2x} = x\sin\left(x \over y\left(x\right)\right) $$ I think it is separable variable differential equations. I tried to substitute:  $$z=\frac{x}{y}$$ and $$y'=\frac {z-z'x}{z^2}$$ $$\frac {z-z'x}{z^2}-\frac{1}{2z}=x\sin(z)$$ multiply by $z^2$ $$z-z'x -\frac{z}{2}=z^2x\sin(z)$$ And now I have no idea how to manipulate this.,,['ordinary-differential-equations']
58,"Liouville's equation and Cauchy problem $\dot{x} = f(x(t),t)$ in optimal control",Liouville's equation and Cauchy problem  in optimal control,"\dot{x} = f(x(t),t)","In a guest lecture about optimal control problem, the speaker introduce the following: The Liouville's equation encodes a superposition al all classical solutions soving Cauchy problem: Liouville's equation:  $$\frac{\partial \mu}{\partial t}+ \mbox{div }(f\mu)=\mu_0-\mu_T$$ Cauchy Problem:   $$\dot{x}(t) = f(t,x(t))$$ $\mu$ might be of different meanings in different topics; in this lecture, $\mu$ is a occupation measure, $\mu_0$ is the measure evaluated at the initial time. $f(t,x)$ is a nonlinear system. Note: the nonlinear system can actually include the control, i.e., $\dot{x} = f(x,u,t)$ to capture the optimal control problem: \begin{equation*} \begin{aligned} & {\underset{u}{\text{min}}} & &\int_0^{t_f} l(t,x,u)\\ & \text{s.t.} & & \dot{x}=f(t,x(t),u(t)) \quad \text{a.e. on }[0,t_f]\\              \end{aligned}  \end{equation*} I am very confused about "" Liouville's equation encodes a superposition al all classical solutions of $\dot{x}= f(x,t)$ "". Could anyone please provide the fundamental background or introduction note about this? I even have no idea how to relate both stuffs; they look quite different for me.","In a guest lecture about optimal control problem, the speaker introduce the following: The Liouville's equation encodes a superposition al all classical solutions soving Cauchy problem: Liouville's equation:  $$\frac{\partial \mu}{\partial t}+ \mbox{div }(f\mu)=\mu_0-\mu_T$$ Cauchy Problem:   $$\dot{x}(t) = f(t,x(t))$$ $\mu$ might be of different meanings in different topics; in this lecture, $\mu$ is a occupation measure, $\mu_0$ is the measure evaluated at the initial time. $f(t,x)$ is a nonlinear system. Note: the nonlinear system can actually include the control, i.e., $\dot{x} = f(x,u,t)$ to capture the optimal control problem: \begin{equation*} \begin{aligned} & {\underset{u}{\text{min}}} & &\int_0^{t_f} l(t,x,u)\\ & \text{s.t.} & & \dot{x}=f(t,x(t),u(t)) \quad \text{a.e. on }[0,t_f]\\              \end{aligned}  \end{equation*} I am very confused about "" Liouville's equation encodes a superposition al all classical solutions of $\dot{x}= f(x,t)$ "". Could anyone please provide the fundamental background or introduction note about this? I even have no idea how to relate both stuffs; they look quite different for me.",,"['ordinary-differential-equations', 'partial-differential-equations', 'optimal-control', 'sturm-liouville']"
59,Projectile Motion with Differential Equations,Projectile Motion with Differential Equations,,"I've been trying to write code that will calculate the required intercept angles of a projectile launch to hit a moving target, a person running around a game world. I have no issues programming things, but I need a bit of help with the mathematics. The projectile's motion in a 3-dimensional Cartesian coordinate system is modeled as the solutions to the following initial value problems (taking advantage of the fact that motion along one axis is independent of the others). The coordinate system is rotated so that y points ""upward"" (against the direction of gravity), and x and z point ""horizontally"". It is still right-handed. Sorry about the ASCII equations; I don't have enough reputation to post images. $x(0) = c_1$ $x'(0) = v \cos(θ)\cos(φ)$ $x''(t) = -0.01x'(t)$ $y(0) = c_3$ $y'(0) = v \sin(φ)$ $y''(t) = -0.03 + -0.01y'(t)$ $z(0) = c_2$ $z'(0) = v \sin(θ)\cos(φ)$ $z''(t) = -0.01x'(t)$ θ (horizontal angle), φ (elevation angle), and v are the launch parameters, but I can only control the two angles. I have no problem solving these differential equations to derive the components of the projectile's velocity, but I have a moving target to contend with. Let's say X (t, θ, φ, v) is the projectile's position as a function of time with given launch parameters guaranteed to stay constant with respect to changing t. I also have Y (t) as the target's position as a function of time, which is differentiable. However, this function doesn't have a clean analytical solution because the moving person's acceleration is often not differentiable (as when trying to intercept someone jumping off a cliff -- his acceleration will be about -0.03 m/s^2 until he hits the ground, when it suddenly goes to 0). I know I can remove this discontinuity, but still I'm left with a piecewise function which is really hard to use. This made my second approach by modeling the relative positions, velocities, and accelerations with differential equations fail. I've also tried simplifying the x and z coordinates into just one coordinate, and modeling the projectile as traveling an arc within a plane; this works well since there's no wind, and I can hit any static target in range accurately. But this only calculates based on the target's position and essentially calculates a function Z(φ) that is the altitude of the projectile when it is either directly above or below the target. I then calculate the roots of Z by using Newton's method, getting the angle (sometimes two). I tried adapting the static method by calculating the time-of-flight of the projectile to the target's original position, using this to estimate the target's moved position, repeating for a few iterations, and then firing at the final moved position, but this method resulted in worse accuracy than simply spreading shots out around the static target. I think it diverged. Now I'm stuck. I can't figure out a numerical method to find the required launch parameters given the target's position Y (t) as a function of time and a function X (t, θ, φ, v) and their derivatives. Is there anything that gives a reasonable degree of accuracy (10% or so) without anything a modern Intel processor can't do in about 50ms? Edit I don't care when the time of impact happens; I just need the paths to intersect at a given time. I can calculate the target's position at any point in the near future (or in the past).","I've been trying to write code that will calculate the required intercept angles of a projectile launch to hit a moving target, a person running around a game world. I have no issues programming things, but I need a bit of help with the mathematics. The projectile's motion in a 3-dimensional Cartesian coordinate system is modeled as the solutions to the following initial value problems (taking advantage of the fact that motion along one axis is independent of the others). The coordinate system is rotated so that y points ""upward"" (against the direction of gravity), and x and z point ""horizontally"". It is still right-handed. Sorry about the ASCII equations; I don't have enough reputation to post images. $x(0) = c_1$ $x'(0) = v \cos(θ)\cos(φ)$ $x''(t) = -0.01x'(t)$ $y(0) = c_3$ $y'(0) = v \sin(φ)$ $y''(t) = -0.03 + -0.01y'(t)$ $z(0) = c_2$ $z'(0) = v \sin(θ)\cos(φ)$ $z''(t) = -0.01x'(t)$ θ (horizontal angle), φ (elevation angle), and v are the launch parameters, but I can only control the two angles. I have no problem solving these differential equations to derive the components of the projectile's velocity, but I have a moving target to contend with. Let's say X (t, θ, φ, v) is the projectile's position as a function of time with given launch parameters guaranteed to stay constant with respect to changing t. I also have Y (t) as the target's position as a function of time, which is differentiable. However, this function doesn't have a clean analytical solution because the moving person's acceleration is often not differentiable (as when trying to intercept someone jumping off a cliff -- his acceleration will be about -0.03 m/s^2 until he hits the ground, when it suddenly goes to 0). I know I can remove this discontinuity, but still I'm left with a piecewise function which is really hard to use. This made my second approach by modeling the relative positions, velocities, and accelerations with differential equations fail. I've also tried simplifying the x and z coordinates into just one coordinate, and modeling the projectile as traveling an arc within a plane; this works well since there's no wind, and I can hit any static target in range accurately. But this only calculates based on the target's position and essentially calculates a function Z(φ) that is the altitude of the projectile when it is either directly above or below the target. I then calculate the roots of Z by using Newton's method, getting the angle (sometimes two). I tried adapting the static method by calculating the time-of-flight of the projectile to the target's original position, using this to estimate the target's moved position, repeating for a few iterations, and then firing at the final moved position, but this method resulted in worse accuracy than simply spreading shots out around the static target. I think it diverged. Now I'm stuck. I can't figure out a numerical method to find the required launch parameters given the target's position Y (t) as a function of time and a function X (t, θ, φ, v) and their derivatives. Is there anything that gives a reasonable degree of accuracy (10% or so) without anything a modern Intel processor can't do in about 50ms? Edit I don't care when the time of impact happens; I just need the paths to intersect at a given time. I can calculate the target's position at any point in the near future (or in the past).",,"['ordinary-differential-equations', 'trigonometry', 'numerical-methods', 'physics', 'projectile-motion']"
60,Solve transport equation $\frac{\partial\phi}{\partial t}+\phi\frac{\partial\phi}{\partial x}=0$ using method of characteristics,Solve transport equation  using method of characteristics,\frac{\partial\phi}{\partial t}+\phi\frac{\partial\phi}{\partial x}=0,"I'm trying to solve the following transport equation $$\frac{\partial\phi}{\partial t}+\phi\frac{\partial\phi}{\partial x}=0$$ subject to the initial condition $$\phi(x,0)=f(x)=\left\{   \begin{array}{l l}     1,\quad x<0 \\     0, \quad x\geq0  \end{array} \right. $$ This is my solution: $\frac{dx}{dt}=\phi \Rightarrow x=\phi t+C$. At $t=0$ let $x=s$. Therefore $x=\phi t+s$. So along a characteristic we have $s=x-\phi t$. For $s<0$, $\phi=1$ so $s=x-t$. For $s\geq0$, $\phi=0$ so $s=x$. $\frac{d\phi}{dt}=0 \Rightarrow \phi(x,t)=f(x)$ and given $\phi=f(s)$ at $x=s,t=0$ we find that $$u(x,t)=f(s)=\left\{\begin{array}{l l}     1,\quad s<0 \\     0, \quad s\geq0  \end{array} \right.$$ $$\Rightarrow \phi(x,t)=\left\{\begin{array}{l l}     1,\quad x-t<0 \\     0, \quad x\geq0  \end{array} \right.$$ Apparently the answer is $$ \phi(x,t)=\left\{\begin{array}{l l}     1,\quad t>2x \\     0, \quad t<2x  \end{array} \right.$$ I'm not sure where I went wrong. Can someone please tell me where my working is wrong?","I'm trying to solve the following transport equation $$\frac{\partial\phi}{\partial t}+\phi\frac{\partial\phi}{\partial x}=0$$ subject to the initial condition $$\phi(x,0)=f(x)=\left\{   \begin{array}{l l}     1,\quad x<0 \\     0, \quad x\geq0  \end{array} \right. $$ This is my solution: $\frac{dx}{dt}=\phi \Rightarrow x=\phi t+C$. At $t=0$ let $x=s$. Therefore $x=\phi t+s$. So along a characteristic we have $s=x-\phi t$. For $s<0$, $\phi=1$ so $s=x-t$. For $s\geq0$, $\phi=0$ so $s=x$. $\frac{d\phi}{dt}=0 \Rightarrow \phi(x,t)=f(x)$ and given $\phi=f(s)$ at $x=s,t=0$ we find that $$u(x,t)=f(s)=\left\{\begin{array}{l l}     1,\quad s<0 \\     0, \quad s\geq0  \end{array} \right.$$ $$\Rightarrow \phi(x,t)=\left\{\begin{array}{l l}     1,\quad x-t<0 \\     0, \quad x\geq0  \end{array} \right.$$ Apparently the answer is $$ \phi(x,t)=\left\{\begin{array}{l l}     1,\quad t>2x \\     0, \quad t<2x  \end{array} \right.$$ I'm not sure where I went wrong. Can someone please tell me where my working is wrong?",,['ordinary-differential-equations']
61,Prove that the Green´s Function satisfy the Differential Equation,Prove that the Green´s Function satisfy the Differential Equation,,"Prove that the Green´s Function: $$G(t,t´)=\theta(t-t´) \dfrac{\sin{[(t-t´)(\omega_0^2-\gamma^2)^{1/2}}]}{(\omega_0^2-\gamma^2)^{1/2}}\cdot e^{-\gamma(t-t´)}  \tag{1}$$ satisfy the Differential Equation: $$\left(\dfrac{d^2}{dt^2}+2\gamma\dfrac{d}{dt}+\omega_0^2\right)G(t,t´)=\delta(t-t´) \tag{2}$$ Starting with the equation: $$\ddot{x}(t)+2\gamma\dot{x}(t)+\omega_0^2x(t)=F(t) \tag{3}$$ And using the Fourier transform, we get the given Greens Function: $$G(t,t´)=\theta(t-t´) \dfrac{\sin{[(t-t´)(\omega_0^2-\gamma^2)^{1/2}}]}{(\omega_0^2-\gamma^2)^{1/2}}\cdot e^{-\gamma(t-t´)} \tag{4}$$ But i dont know how i can show that this Green function satisfy the given equation.","Prove that the Green´s Function: $$G(t,t´)=\theta(t-t´) \dfrac{\sin{[(t-t´)(\omega_0^2-\gamma^2)^{1/2}}]}{(\omega_0^2-\gamma^2)^{1/2}}\cdot e^{-\gamma(t-t´)}  \tag{1}$$ satisfy the Differential Equation: $$\left(\dfrac{d^2}{dt^2}+2\gamma\dfrac{d}{dt}+\omega_0^2\right)G(t,t´)=\delta(t-t´) \tag{2}$$ Starting with the equation: $$\ddot{x}(t)+2\gamma\dot{x}(t)+\omega_0^2x(t)=F(t) \tag{3}$$ And using the Fourier transform, we get the given Greens Function: $$G(t,t´)=\theta(t-t´) \dfrac{\sin{[(t-t´)(\omega_0^2-\gamma^2)^{1/2}}]}{(\omega_0^2-\gamma^2)^{1/2}}\cdot e^{-\gamma(t-t´)} \tag{4}$$ But i dont know how i can show that this Green function satisfy the given equation.",,"['ordinary-differential-equations', 'dirac-delta', 'greens-function']"
62,How is the Gastner-Newman equation implemented to create value-by-area cartograms?,How is the Gastner-Newman equation implemented to create value-by-area cartograms?,,"There is a paper called ""Density-equalizing map projections: Diffusion-based algorithm and applications"" by Michael T. Gastner and M. E. J. Newman, which explains their algorithm for generating value-by-area cartograms. While it explains the theoretical side of the mathematics involved with their algorithm, it doesn't explain how they actually implemented it. I tried to piece it together by looking at the source code from cart , but I don't have the programming knowledge (it's written in c, which I don't know) required to understand it. If anyone has at least a decent understanding of it and can explain the steps needed to create a cartogram using their algorithm, that would be greatly appreciated. Otherwise, if you have other helpful resources on the topic, those would be good too.","There is a paper called ""Density-equalizing map projections: Diffusion-based algorithm and applications"" by Michael T. Gastner and M. E. J. Newman, which explains their algorithm for generating value-by-area cartograms. While it explains the theoretical side of the mathematics involved with their algorithm, it doesn't explain how they actually implemented it. I tried to piece it together by looking at the source code from cart , but I don't have the programming knowledge (it's written in c, which I don't know) required to understand it. If anyone has at least a decent understanding of it and can explain the steps needed to create a cartogram using their algorithm, that would be greatly appreciated. Otherwise, if you have other helpful resources on the topic, those would be good too.",,"['linear-algebra', 'ordinary-differential-equations', 'linear-transformations', 'physics', 'cartography']"
63,"If $p_0$ is a strict local maximum of $g$, then it is a center of the Hamiltonian System $p'=H_g(p)$","If  is a strict local maximum of , then it is a center of the Hamiltonian System",p_0 g p'=H_g(p),"This is supposed to be an ordinary differential equations class exercise: Let $g\in \scr{C}^2(\Bbb{R}^2;\Bbb{R})$ and consider   $$H_g(x,y)=\begin{pmatrix}\partial_yg(x,y)\\ -\partial_xg(x,y)\end{pmatrix}.$$ Show that, if $g$ admits a strict local maximum point (non-degenerated) at $p_0=(x_0,y_0)$ then near to $p_0$ there are infinitely many periodic orbits of the o.d.e. $p'(s)=H_g(p(s))$ I know facts like: the level curves of $g$ are integral curves of the o.d.e. $p'(s)=H_g(p(s))$; $\langle \nabla g,H_g\rangle=0$; basic facts about $\nabla g$, etc; basic facts about o.d.e.'s like existence and uniqueness theorem, o.d.e.'s systems, etc. Furthermore, I can geometrically visualize that, if $p_0$ is a point of maximum isolated, then ""around it"", there is infinitely many closed level curves (imagine the surface given by the graph of $g$). But I don't have any idea of how to prove it formally... I'm really stuck!","This is supposed to be an ordinary differential equations class exercise: Let $g\in \scr{C}^2(\Bbb{R}^2;\Bbb{R})$ and consider   $$H_g(x,y)=\begin{pmatrix}\partial_yg(x,y)\\ -\partial_xg(x,y)\end{pmatrix}.$$ Show that, if $g$ admits a strict local maximum point (non-degenerated) at $p_0=(x_0,y_0)$ then near to $p_0$ there are infinitely many periodic orbits of the o.d.e. $p'(s)=H_g(p(s))$ I know facts like: the level curves of $g$ are integral curves of the o.d.e. $p'(s)=H_g(p(s))$; $\langle \nabla g,H_g\rangle=0$; basic facts about $\nabla g$, etc; basic facts about o.d.e.'s like existence and uniqueness theorem, o.d.e.'s systems, etc. Furthermore, I can geometrically visualize that, if $p_0$ is a point of maximum isolated, then ""around it"", there is infinitely many closed level curves (imagine the surface given by the graph of $g$). But I don't have any idea of how to prove it formally... I'm really stuck!",,"['ordinary-differential-equations', 'derivatives', 'gradient-flows', 'hamilton-equations']"
64,Brachistichrone problem with friction,Brachistichrone problem with friction,,"In this Mathworld article for the Brachistichrone problem, it is said that the following: $$\left(1+(y')^2\right)(1+\mu y')+2(y-\mu x)y''=0$$ Implies that: $$\frac{1+(y')^2}{(1+\mu y')^2}=\frac{C}{y-\mu x}$$ I'm not sure how they arrived at this step -- how could a second-order differential equation even be reduced to a first-order one? I thought maybe they had a reason to assume constant curvature, but constant curvature would actually mean: $$\frac{1+(y')^2}{(1+\mu y')^2}=\frac{C}{(y-\mu x)^2}$$ (which doesn't make sense, anyway, because setting $\mu=0$ does not return the differential equation for the standard, frictionless brachistichrone)","In this Mathworld article for the Brachistichrone problem, it is said that the following: $$\left(1+(y')^2\right)(1+\mu y')+2(y-\mu x)y''=0$$ Implies that: $$\frac{1+(y')^2}{(1+\mu y')^2}=\frac{C}{y-\mu x}$$ I'm not sure how they arrived at this step -- how could a second-order differential equation even be reduced to a first-order one? I thought maybe they had a reason to assume constant curvature, but constant curvature would actually mean: $$\frac{1+(y')^2}{(1+\mu y')^2}=\frac{C}{(y-\mu x)^2}$$ (which doesn't make sense, anyway, because setting $\mu=0$ does not return the differential equation for the standard, frictionless brachistichrone)",,[]
65,Series solution for a differential equation.,Series solution for a differential equation.,,"I have the differential equation $(1-x^2)y'' -2xy' +\lambda y = 0$. If I solve it with a power series of the form $\displaystyle \sum_{n=0}^{\infty} a_n (x+1)^n $ centered at $x=-1$ I got a recurrence relation. Should I get the same recurence if I solve my differential equation with a power series of the form $\displaystyle \sum_{n=0}^{\infty} a_n x^n$ centered at $x=0$? Should it give the same coefficients $a_j$? Thanks in advance, Best regards.","I have the differential equation $(1-x^2)y'' -2xy' +\lambda y = 0$. If I solve it with a power series of the form $\displaystyle \sum_{n=0}^{\infty} a_n (x+1)^n $ centered at $x=-1$ I got a recurrence relation. Should I get the same recurence if I solve my differential equation with a power series of the form $\displaystyle \sum_{n=0}^{\infty} a_n x^n$ centered at $x=0$? Should it give the same coefficients $a_j$? Thanks in advance, Best regards.",,"['ordinary-differential-equations', 'recurrence-relations', 'power-series']"
66,Strogatz 3.3.1d: when is adiabatic elimination allowable?,Strogatz 3.3.1d: when is adiabatic elimination allowable?,,"I worked through much of 3.3.1 (Laser Threshold) in Strogatz's Nonlinear Dynamics and Chaos, but I'm struggling to understand the adiabatic elimination he does and when it's allowable. We have a system modeling a laser where $n$ is the number of photons in the laser and $N$ is the number of excited atoms. The equations are: $$\dot n= GnN-kn$$ $$\dot N= -GnN-fN+p$$ where $G$, $k$, $f$, and $p$ are various control parameters. To convert it from a one-dimensional system, we make the 'quasi-static' approximation $\dot N \approx 0$, which Strogatz says represents ""$N$ relaxing more rapidly than $n$"". This approximation is the part I'm confused about: a) If $\dot N\approx0$, do we assume that $N$ is constant? Or are these different assumptions? How can $\dot N\approx0$ be true when $\dot N$ has the constant, non-zero $p$ term? b) In the 4th part of the question, we are asked to find the range of parameters for which this approximation is acceptable. I tried the 'dimensionless' groups approach from earlier in the book, but that led to a dead-end. Is there a good introduction to when adiabatic elimination is allowed that isn't in the context of complex Quantum Mechanics?","I worked through much of 3.3.1 (Laser Threshold) in Strogatz's Nonlinear Dynamics and Chaos, but I'm struggling to understand the adiabatic elimination he does and when it's allowable. We have a system modeling a laser where $n$ is the number of photons in the laser and $N$ is the number of excited atoms. The equations are: $$\dot n= GnN-kn$$ $$\dot N= -GnN-fN+p$$ where $G$, $k$, $f$, and $p$ are various control parameters. To convert it from a one-dimensional system, we make the 'quasi-static' approximation $\dot N \approx 0$, which Strogatz says represents ""$N$ relaxing more rapidly than $n$"". This approximation is the part I'm confused about: a) If $\dot N\approx0$, do we assume that $N$ is constant? Or are these different assumptions? How can $\dot N\approx0$ be true when $\dot N$ has the constant, non-zero $p$ term? b) In the 4th part of the question, we are asked to find the range of parameters for which this approximation is acceptable. I tried the 'dimensionless' groups approach from earlier in the book, but that led to a dead-end. Is there a good introduction to when adiabatic elimination is allowed that isn't in the context of complex Quantum Mechanics?",,"['ordinary-differential-equations', 'nonlinear-system', 'approximation-theory']"
67,Analytically derive the reproduction number of an SI model,Analytically derive the reproduction number of an SI model,,"For an assignment I have to create an SI model with Python and answer some questions about it. The model itself is very simple. I use a Python package to create a graph with an average degree of $\langle k \rangle$. In this graph an infected node, for each timestep, will infect a neighbouring node with a probability of $i$. The problem I am having is understanding one of the questions that has nothing to do with programming, but with mathematics! Specifically we have to analytically derive the reproduction number $R_0$ (which they define as ""the expected number of new infections in the first step of time per infected node""). I have Googled a lot and found out the following: \begin{align*}     \frac{dI}{dT} &= \beta \frac{SI}{N}\\     \frac{dS}{dT} &= -\beta \frac{SI}{N} \end{align*} where $I$ is the total number of infected nodes, $S$ the total number of susceptible nodes and $N$ the total number of nodes ($N = S + I$). In most places I've read that $\beta$ is defined as the infection/contact rate or the probability of infection times the average contact between susceptible and infected nodes. In my case I assume that $\beta = i \cdot \langle k \rangle$, right? How do I get $R_0$ from this, and is this even what they mean by ""analytically derive""?","For an assignment I have to create an SI model with Python and answer some questions about it. The model itself is very simple. I use a Python package to create a graph with an average degree of $\langle k \rangle$. In this graph an infected node, for each timestep, will infect a neighbouring node with a probability of $i$. The problem I am having is understanding one of the questions that has nothing to do with programming, but with mathematics! Specifically we have to analytically derive the reproduction number $R_0$ (which they define as ""the expected number of new infections in the first step of time per infected node""). I have Googled a lot and found out the following: \begin{align*}     \frac{dI}{dT} &= \beta \frac{SI}{N}\\     \frac{dS}{dT} &= -\beta \frac{SI}{N} \end{align*} where $I$ is the total number of infected nodes, $S$ the total number of susceptible nodes and $N$ the total number of nodes ($N = S + I$). In most places I've read that $\beta$ is defined as the infection/contact rate or the probability of infection times the average contact between susceptible and infected nodes. In my case I assume that $\beta = i \cdot \langle k \rangle$, right? How do I get $R_0$ from this, and is this even what they mean by ""analytically derive""?",,"['ordinary-differential-equations', 'graph-theory', 'self-learning', 'mathematical-modeling']"
68,Challenge Problem - Calculus,Challenge Problem - Calculus,,"Suppose $u$ and $v$ are continuous on $[a,b]$ and differentiable on $(a,b)$, and assume that for all $t\in(a,b)$, at least one of $u'(t)$ and $v'(t)$ is nonzero. Let $C$ be the curve given by $(u(t),v(t))$ for $t \in [a,b]$. Let $A = (u(a),v(a))$ and $B = (u(b),v(b))$ be the endpoints of the curve, and assume $A\ne B$. Show that there is some point $c \in (a,b)$ such that the tangent line to $C$ at $(u(c),v(c))$ is parallel to $\overline{AB}$. I had to answer this last week, and I was struggling. I know that by the Mean Value Theorem we have that $\exists c : v'(c) = \frac{v(b) - v(a)}{b - a}$ and $\exists d : u'(d) = \frac{u(b) - u(a)}{b - a}$, which gives us $\frac{v(b) - v(a)}{u(b) - u(a)} = \frac{v'(c)}{u'(d)}$. A friend gave me a hint that ""Let $h(t) = v(t) - v(a) - \frac{v(b) - v(a)}{u(b) - u(a)}(u(t) - u(a))$. Remembering that $v(a)$, $\frac{v(b) - v(a)}{u(b) - u(a)}$, and $u(a)$ are constants, what is $h'(t)$?"" This is no longer for credit, I just want help understanding what I'm missing.","Suppose $u$ and $v$ are continuous on $[a,b]$ and differentiable on $(a,b)$, and assume that for all $t\in(a,b)$, at least one of $u'(t)$ and $v'(t)$ is nonzero. Let $C$ be the curve given by $(u(t),v(t))$ for $t \in [a,b]$. Let $A = (u(a),v(a))$ and $B = (u(b),v(b))$ be the endpoints of the curve, and assume $A\ne B$. Show that there is some point $c \in (a,b)$ such that the tangent line to $C$ at $(u(c),v(c))$ is parallel to $\overline{AB}$. I had to answer this last week, and I was struggling. I know that by the Mean Value Theorem we have that $\exists c : v'(c) = \frac{v(b) - v(a)}{b - a}$ and $\exists d : u'(d) = \frac{u(b) - u(a)}{b - a}$, which gives us $\frac{v(b) - v(a)}{u(b) - u(a)} = \frac{v'(c)}{u'(d)}$. A friend gave me a hint that ""Let $h(t) = v(t) - v(a) - \frac{v(b) - v(a)}{u(b) - u(a)}(u(t) - u(a))$. Remembering that $v(a)$, $\frac{v(b) - v(a)}{u(b) - u(a)}$, and $u(a)$ are constants, what is $h'(t)$?"" This is no longer for credit, I just want help understanding what I'm missing.",,"['calculus', 'ordinary-differential-equations', 'curves', 'tangent-line']"
69,Find that solution satisfying $ϕ(1) = 3ϕ(0)$ for $y' + 5y = 2$.,Find that solution satisfying  for .,ϕ(1) = 3ϕ(0) y' + 5y = 2,"Find that solution satisfying $ϕ(1) = 3ϕ(0)$ for the following second order linear ordinary differential equation: $y' + 5y = 2$ I found the solution to be $ϕ(x) = \frac{2}{5} + ce^{5x}$ . Now how do I find a particular solution satisfying $ϕ(1) = 3ϕ(0)?$ Please help me with this. In the previous part, I was asked to find the solution satisfying $ϕ(1) = 2$ which I found to be $ϕ(x) = \frac{2}{5} + \frac{8}{5}e^5e^{-5x}$ if that helps.","Find that solution satisfying for the following second order linear ordinary differential equation: I found the solution to be . Now how do I find a particular solution satisfying Please help me with this. In the previous part, I was asked to find the solution satisfying which I found to be if that helps.",ϕ(1) = 3ϕ(0) y' + 5y = 2 ϕ(x) = \frac{2}{5} + ce^{5x} ϕ(1) = 3ϕ(0)? ϕ(1) = 2 ϕ(x) = \frac{2}{5} + \frac{8}{5}e^5e^{-5x},"['integration', 'ordinary-differential-equations', 'proof-verification']"
70,Differential Equations Help,Differential Equations Help,,"I need to answer the following question Assume that all the cash flows in this problem occur continuously, rather than only at discrete times. Suppose that your parents deposit money into your bank account at the rate of \$50 a day. You start out with \$3,000 in your account. You also spend at a rate of 5% of your money per day. Your account is a no-interest checking account. Write a differential equation for the amount of money in your account as a function of time, and solve the equation. Also find an equilibrium solution. I'm having trouble writing a differential equation that represents the situation, everything else I can do","I need to answer the following question Assume that all the cash flows in this problem occur continuously, rather than only at discrete times. Suppose that your parents deposit money into your bank account at the rate of \$50 a day. You start out with \$3,000 in your account. You also spend at a rate of 5% of your money per day. Your account is a no-interest checking account. Write a differential equation for the amount of money in your account as a function of time, and solve the equation. Also find an equilibrium solution. I'm having trouble writing a differential equation that represents the situation, everything else I can do",,"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
71,Lemma of Gronwall,Lemma of Gronwall,,"Let $T \in \mathbb R^+\cup \{\infty\}$, $t_o \in [0,T)$, $a,b \in L^\infty(t_0,T)$ and $\lambda \in L^1(T_0,t)$, $\lambda(t) \geq 0$ for almost all $t \in (t_o,T)$. From the inequality $$a(t) \leq b(t) + \int_{t_0}^t\lambda(s)a(s)ds \,\,\,\,\,$$ a.e. in $(t_o,T)$ it follows  $$a(t) \leq b(t) + \int_{t_0}^t e^{\phi(t)-\phi(s)}\lambda(s)b(s) \,ds$$ for almost all $t\in (t_0,T)$ where $\phi(s):=\int_{t_o}^s \lambda(\tau)\,d\tau$. Is there a counterexample for the case that $\lambda$ is negative? And what changes about the implication if $t<t_o$?","Let $T \in \mathbb R^+\cup \{\infty\}$, $t_o \in [0,T)$, $a,b \in L^\infty(t_0,T)$ and $\lambda \in L^1(T_0,t)$, $\lambda(t) \geq 0$ for almost all $t \in (t_o,T)$. From the inequality $$a(t) \leq b(t) + \int_{t_0}^t\lambda(s)a(s)ds \,\,\,\,\,$$ a.e. in $(t_o,T)$ it follows  $$a(t) \leq b(t) + \int_{t_0}^t e^{\phi(t)-\phi(s)}\lambda(s)b(s) \,ds$$ for almost all $t\in (t_0,T)$ where $\phi(s):=\int_{t_o}^s \lambda(\tau)\,d\tau$. Is there a counterexample for the case that $\lambda$ is negative? And what changes about the implication if $t<t_o$?",,"['real-analysis', 'integration', 'ordinary-differential-equations', 'inequality']"
72,Find the general solution to this Differential Equation.,Find the general solution to this Differential Equation.,,"Given that $z=f(x,y)$ and $a\in \mathbb{R}$ is a constant, I have to solve the following differential equation: $$ \frac{z \,dz+y \,dy}{y^2+z^2}=\frac{dx}{\sqrt{(x-a)^2+y^2+z^2}+(x-a)}.$$ I have not seen anything like this before so any ideas/hints would be much appreciated.","Given that $z=f(x,y)$ and $a\in \mathbb{R}$ is a constant, I have to solve the following differential equation: $$ \frac{z \,dz+y \,dy}{y^2+z^2}=\frac{dx}{\sqrt{(x-a)^2+y^2+z^2}+(x-a)}.$$ I have not seen anything like this before so any ideas/hints would be much appreciated.",,['ordinary-differential-equations']
73,"Is there an algebraic solution for the differential equation $y'= x^y$, and if not, can we prove this?","Is there an algebraic solution for the differential equation , and if not, can we prove this?",y'= x^y,"I believe that no algebraic solution exists, however, can we prove this? Below I have plotted the (real part) of the direction field on $[-3 < x < 3]\times [ -3 < y < 3 ]$","I believe that no algebraic solution exists, however, can we prove this? Below I have plotted the (real part) of the direction field on $[-3 < x < 3]\times [ -3 < y < 3 ]$",,['ordinary-differential-equations']
74,Necessary and sufficient condition for a periodic system to have a non-trivial periodic solution.,Necessary and sufficient condition for a periodic system to have a non-trivial periodic solution.,,"Consider the linear periodic system in $\mathbb{R}^n$ \begin{equation} \begin{cases} \dot{x}(t) & = A(t)x(t),\\ x(0) & = x_0, \end{cases} \label{eq:Floquet} \tag{1} \end{equation} where $A(t)$ is a real $n\times n$ matrix function which is smooth in $t$ and periodic of period $T>0$. Floquet theory states that there exists at least 1 non-trivial solution $\chi(t)$ satisfying \begin{equation} \chi(t+T) = \mu\chi(t), \ \ t\in(-\infty, \infty), \label{eq:Floquet2} \tag{2} \end{equation} where $\mu$ is an eigenvalue of the Floquet matrix. $\mu$ is more well-known as a Floquet multiplier of the system. What is the necessary and sufficient conditions so that \eqref{eq:Floquet} has a non-trivial $T$-periodic solution? By non-trivial I meant a periodic solution with minimal period $T$. It seems like according to \eqref{eq:Floquet2}, one would want to impose condition on the Floquet matrix such that it has eigenvalue $\mu=1$, but I know nothing about this, not to mention that this sounds like a very strong condition. Spefically, I am looking for conditions that stem from Floquet theory.","Consider the linear periodic system in $\mathbb{R}^n$ \begin{equation} \begin{cases} \dot{x}(t) & = A(t)x(t),\\ x(0) & = x_0, \end{cases} \label{eq:Floquet} \tag{1} \end{equation} where $A(t)$ is a real $n\times n$ matrix function which is smooth in $t$ and periodic of period $T>0$. Floquet theory states that there exists at least 1 non-trivial solution $\chi(t)$ satisfying \begin{equation} \chi(t+T) = \mu\chi(t), \ \ t\in(-\infty, \infty), \label{eq:Floquet2} \tag{2} \end{equation} where $\mu$ is an eigenvalue of the Floquet matrix. $\mu$ is more well-known as a Floquet multiplier of the system. What is the necessary and sufficient conditions so that \eqref{eq:Floquet} has a non-trivial $T$-periodic solution? By non-trivial I meant a periodic solution with minimal period $T$. It seems like according to \eqref{eq:Floquet2}, one would want to impose condition on the Floquet matrix such that it has eigenvalue $\mu=1$, but I know nothing about this, not to mention that this sounds like a very strong condition. Spefically, I am looking for conditions that stem from Floquet theory.",,"['ordinary-differential-equations', 'dynamical-systems']"
75,Rewriting a second-order nonlinear ode in first-order system - how?,Rewriting a second-order nonlinear ode in first-order system - how?,,"I wanna show that $y''+\alpha \sin(y)=f(x)$  with  $y(a)=y_0$ and $y'(a)=y_1$ has got an unique solution. I wanted to rewrite this ODE in an ODE system of first order and than apply picard-lindelöf on each row. My problem is that I don't know how to rewrite this in an ODE of first order because of $\sin(y)$ and because it's non-homogeneous. I thought of substituting $z:=y'$ than we've got: $z'+\alpha \sin(y) = f(x)$ and if we look at the homogeneous ODE $z'+\alpha \sin(y) = 0$ we get $z(y)= \alpha \cos(y)+c_0$ as a solution. Substituting back we get  $y'(x)= \alpha \cos(y(x))+c_0$ I've read that we can rewrite this equation in $Ax=b$ with $x=\pmatrix{x \\ \dot x}$ and $b=\pmatrix{0 \\f(t)}$ , but I'm not sure about the matrix I guess it's something like $A=\pmatrix{0 & 1 \\ -\alpha \sin & 0}$. Any hints or good books to recommend?","I wanna show that $y''+\alpha \sin(y)=f(x)$  with  $y(a)=y_0$ and $y'(a)=y_1$ has got an unique solution. I wanted to rewrite this ODE in an ODE system of first order and than apply picard-lindelöf on each row. My problem is that I don't know how to rewrite this in an ODE of first order because of $\sin(y)$ and because it's non-homogeneous. I thought of substituting $z:=y'$ than we've got: $z'+\alpha \sin(y) = f(x)$ and if we look at the homogeneous ODE $z'+\alpha \sin(y) = 0$ we get $z(y)= \alpha \cos(y)+c_0$ as a solution. Substituting back we get  $y'(x)= \alpha \cos(y(x))+c_0$ I've read that we can rewrite this equation in $Ax=b$ with $x=\pmatrix{x \\ \dot x}$ and $b=\pmatrix{0 \\f(t)}$ , but I'm not sure about the matrix I guess it's something like $A=\pmatrix{0 & 1 \\ -\alpha \sin & 0}$. Any hints or good books to recommend?",,['ordinary-differential-equations']
76,Eigenvalues and eigenfunctions of differential operator.,Eigenvalues and eigenfunctions of differential operator.,,"I'm trying to find the eigenvalues (atleast the lowest) and eigenvectors of $$\alpha \frac{\partial^2}{\partial r^2} + \beta V(r) $$ with $\alpha$ and $\beta$ constant, $V(r) = \frac{a}{r}$ and for $V(r) = br^2$. In case of the first potential, one solution I have found is of the form $r \exp(-\lambda r)$ but it's only one, and I expect a whole range of possible solutions. On the second potential I'm simply failing to see a solution. Can some one give me the general direction I should move to? Straight up solutions are nice as well.","I'm trying to find the eigenvalues (atleast the lowest) and eigenvectors of $$\alpha \frac{\partial^2}{\partial r^2} + \beta V(r) $$ with $\alpha$ and $\beta$ constant, $V(r) = \frac{a}{r}$ and for $V(r) = br^2$. In case of the first potential, one solution I have found is of the form $r \exp(-\lambda r)$ but it's only one, and I expect a whole range of possible solutions. On the second potential I'm simply failing to see a solution. Can some one give me the general direction I should move to? Straight up solutions are nice as well.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'quantum-mechanics', 'eigenfunctions']"
77,What type of bifurcation occurs from the transition from a saddle to a center,What type of bifurcation occurs from the transition from a saddle to a center,,"If I have a system that takes the form $\dot x =by$ $\leftrightarrow$ $\dot y = y^2 + x$, I get a Jacobian matrix at the origina of the form $$         \begin{matrix}         0 & b & \\         1 & 0 & \\         \end{matrix} $$ We see that $\tau$ = 0 and $\Delta$=-$b$ This means that that eigenvalues $\lambda$ = $\pm$$\sqrt b$. As the value $b$ passes through 0 the behavior around the origin goes from a center to a saddle. What kind of bifurcation is this called?","If I have a system that takes the form $\dot x =by$ $\leftrightarrow$ $\dot y = y^2 + x$, I get a Jacobian matrix at the origina of the form $$         \begin{matrix}         0 & b & \\         1 & 0 & \\         \end{matrix} $$ We see that $\tau$ = 0 and $\Delta$=-$b$ This means that that eigenvalues $\lambda$ = $\pm$$\sqrt b$. As the value $b$ passes through 0 the behavior around the origin goes from a center to a saddle. What kind of bifurcation is this called?",,"['ordinary-differential-equations', 'dynamical-systems', 'parametric', 'nonlinear-system', 'bifurcation']"
78,"Show that $(x_1,x_2)=(0,0)$ is an unstable fixed point for $\dot{x_1} = -x_1 + x_2^6$, $\dot{x_2} =x_2^3 + x_1^6$","Show that  is an unstable fixed point for ,","(x_1,x_2)=(0,0) \dot{x_1} = -x_1 + x_2^6 \dot{x_2} =x_2^3 + x_1^6","Show that $(x_1,x_2)=(0,0)$ is an unstable fixed point for the system $$\dot{x_1} = -x_1 + x_2^6\qquad\dot{x_2} =x_2^3 + x_1^6$$   Hint: Consider the Lyapunov function $V(x_1,x_2) = ax_1^i + bx_2^j$. What I have done: Let $$f(x):={-x_1 + x_2^6 \choose x_2^3 + x_1^6}$$ then $$\dot V = \nabla V\cdot f(x) = (aix_1^{i-1},bjx_2^{j-1})\cdot{-x_1 + x_2^6 \choose x_2^3 + x_1^6} = aix_1^{i-1}(x_2^6 - x_1) + bjx_2^{j-1}(x_1^6+x_2^3).$$ I have a theorem that basically says that if $\dot V(x) > 0$ for all $x\in E \setminus \{(0,0)\}$, where $E$ is an open subset of $\mathbb R^2$ containing $(0,0)$, then $(0,0)$ is unstable. Here's my main question : We were given the hint to consider the given Lyapunov function, but I'm having some difficulty understanding how to use this Lyapunov function to show that basically any nonzero $x\in\mathbb R^2$ gives us that $\dot V(x) > 0$. Must I remain general with the $i$ and $j$, or should I arbitrarily pick $i$ and $j$ in this case? If we don't, then we'd have to break this up into cases, but from what I understand, we just need to find a function that satisfies the criteria for the Lyapunov function, not show it works for every possibility.","Show that $(x_1,x_2)=(0,0)$ is an unstable fixed point for the system $$\dot{x_1} = -x_1 + x_2^6\qquad\dot{x_2} =x_2^3 + x_1^6$$   Hint: Consider the Lyapunov function $V(x_1,x_2) = ax_1^i + bx_2^j$. What I have done: Let $$f(x):={-x_1 + x_2^6 \choose x_2^3 + x_1^6}$$ then $$\dot V = \nabla V\cdot f(x) = (aix_1^{i-1},bjx_2^{j-1})\cdot{-x_1 + x_2^6 \choose x_2^3 + x_1^6} = aix_1^{i-1}(x_2^6 - x_1) + bjx_2^{j-1}(x_1^6+x_2^3).$$ I have a theorem that basically says that if $\dot V(x) > 0$ for all $x\in E \setminus \{(0,0)\}$, where $E$ is an open subset of $\mathbb R^2$ containing $(0,0)$, then $(0,0)$ is unstable. Here's my main question : We were given the hint to consider the given Lyapunov function, but I'm having some difficulty understanding how to use this Lyapunov function to show that basically any nonzero $x\in\mathbb R^2$ gives us that $\dot V(x) > 0$. Must I remain general with the $i$ and $j$, or should I arbitrarily pick $i$ and $j$ in this case? If we don't, then we'd have to break this up into cases, but from what I understand, we just need to find a function that satisfies the criteria for the Lyapunov function, not show it works for every possibility.",,"['ordinary-differential-equations', 'proof-writing', 'dynamical-systems', 'stability-in-odes']"
79,Non-example of a flow,Non-example of a flow,,"Suppose $\begin{cases}x'(t)=f(x(t)) \\ x(0)=x_0\end{cases}$ where $f:\mathbb{R}^n\to\mathbb{R}^n$ is continuous and suppose $\phi(t,x_0)$ is a solution. Can someone provide an example of when $\phi(t+s,x_0)\neq\phi(t,\phi(s,x_0))$? I would think the example would have to violate the uniqueness of the IVP. I was thinking $f:\mathbb{R}\to\mathbb{R}$ being $f(x)=x^{1/3}$, with $\phi(s,x_0)=0$, but I don't think this works.","Suppose $\begin{cases}x'(t)=f(x(t)) \\ x(0)=x_0\end{cases}$ where $f:\mathbb{R}^n\to\mathbb{R}^n$ is continuous and suppose $\phi(t,x_0)$ is a solution. Can someone provide an example of when $\phi(t+s,x_0)\neq\phi(t,\phi(s,x_0))$? I would think the example would have to violate the uniqueness of the IVP. I was thinking $f:\mathbb{R}\to\mathbb{R}$ being $f(x)=x^{1/3}$, with $\phi(s,x_0)=0$, but I don't think this works.",,"['real-analysis', 'ordinary-differential-equations', 'examples-counterexamples']"
80,System of differential equations. Recurrence relation.,System of differential equations. Recurrence relation.,,"Starting from physical problem of calculating the vector potential, I come up with following system of differential equations: $$  \begin{cases} \color{red}{R_{zzz}}(1-\lambda^2+z^2)+\color{red}{R_{zz}}(3z)+\color{red}{R_z}(k^2z^2-k^2\lambda^2)+\color{red}{R}(-k^2z)+\color{blue}{E_z}(-k^2z)=\color{orange}{C_z}(k^2z) \\ \color{blue}{E_{zzz}}(1-\lambda^2+z^2)+\color{blue}{E_{zz}}(3z)+\color{blue}{E_z}(1+k^2-k^2\lambda^2)+\color{red}{R_z}(-k^2z)+\color{red}{R}(k^2)=\color{orange}{C_z}(k^2\lambda^2-k^2)\\ \end{cases} $$ where $\color{orange}{C}=\frac{\omega_pq}{ck}\frac{1}{\sqrt{1+z^2}-\lambda z},\quad k\text{ and }z\text{ are free variables}$ Then substitute $\color{red}{R}\to\frac{\omega_pq}{ck}\color{red}{R},\quad \color{blue}{E}\to\frac{\omega_pq}{ck}\color{blue}{E},\quad\color{orange}{C}\to\frac{\omega_pq}{ck}\color{orange}{C}\quad$ and divide both eq. by $\frac{\omega_pq}{ck}$ New $\color{orange}{C}=\frac{1}{\sqrt{1+z^2}-\lambda z}$ Now I look for solution in form of $\color{red}{R}=\sum_{i=0}^{\infty}\color{red}{R^i}\frac{1}{k^i},\quad\color{blue}{E_z}=\sum_{i=0}^{\infty}\color{blue}{E^i}\frac{1}{k^i},\quad$where in case of $\frac{1}{k^i}-$ $i$ means power, in other cases upper index. It leads to following result: for odd $i$ holds $\color{red}{R^i}=\color{blue}{E^i}=0$, while for even $i$ there is a reccurent system of differential equations: $$ \begin{cases} \color{red}{R^i_{zzz}}(1-\lambda^2+z^2)+\color{red}{R^i_{zz}}(3z)+\color{red}{R^{i+2}_z}(z^2-\lambda^2)+\color{red}{R^{i+2}}(-z)+\color{blue}{E^{i+2}_z}(-z)=0\\ \color{blue}{E^i_{zzz}}(1-\lambda^2+z^2)+\color{blue}{E^i_{zz}}(3z)+\color{blue}{E^i_z}(1)+\color{blue}{E^{i+2}_z}(1-\lambda^2)+\color{red}{R^{i+2}_z}(-z)+\color{red}{R^{i+2}}(1)=0\\ \end{cases} $$ With initial conditions $\color{red}{R^0}=0,\quad\color{blue}{E^0_z}=-\color{orange}{C_z}$ It is possible to solve this system in general case: $$\color{red}{R^{i+2}}=(\frac{1}{\lambda^2})(1+z^2-\lambda^2)^{\frac{1}{2}}\int(1+z^2-\lambda^2)^{-\frac{1}{2}}\left[\color{red}{R^i_{zzz}}(1+z^2-\lambda^2)(-1-z^2+2\lambda^2)+\\+\color{red}{R^i_{zz}}(5z\lambda^2-2z-2z^3)+\color{red}{R^i_{z}}(2z^2+\lambda^2)+\color{red}{R^i}(z)\right]dz+\\+(-\frac{1}{\lambda^2})(1+z^2-\lambda^2)^{\frac{1}{2}}\int(1+z^2-\lambda^2)^{-\frac{1}{2}}\left[\color{red}{R^{i-2}_{zzzzz}}(1+z^2-\lambda^2)^2+\color{red}{R^{i-2}_{zzzz}}(6z)(1+z^2-\lambda^2)+\\+\color{red}{R^{i-2}_{zzz}}(1+10z^2-\lambda^2)+\color{red}{R^{i-2}_{zz}}(3z)\right]dz $$ $\color{blue}{E^i_z}$ is fully defined through $\color{red}{R^i}$: $$ \color{blue}{E^{i+2}_z}=\color{red}{R^{i+2}_z}(z-\frac{\lambda^2}{z})+\color{red}{R^{i+2}}(-1)+\color{red}{R^i_{zzz}}(\frac{1-\lambda^2}{z}+z)+\color{red}{R^i_{zz}}(3) $$ With initial conditions: $\color{red}{R^0}=0,\quad\color{red}{R^2}=(-\frac{1}{\lambda^2})(1+z^2-\lambda^2)^{\frac{1}{2}}\left[\int\color{orange}{C_{zzz}}(z)dz+\int\color{orange}{C_{zz}}(\frac{3z^2}{1+z^2-\lambda^2})dz+\int\color{orange}{C_{z}}(\frac{z}{1+z^2-\lambda^2})dz\right]$ Note: due to physical reasons all constants of integration equal to zero. And that's where I stuck, having no idea how to get closed form of $\color{red}{R}, \color{blue}{E}$ or at least solve reccurence. Really appreciate if someone can point a way.","Starting from physical problem of calculating the vector potential, I come up with following system of differential equations: $$  \begin{cases} \color{red}{R_{zzz}}(1-\lambda^2+z^2)+\color{red}{R_{zz}}(3z)+\color{red}{R_z}(k^2z^2-k^2\lambda^2)+\color{red}{R}(-k^2z)+\color{blue}{E_z}(-k^2z)=\color{orange}{C_z}(k^2z) \\ \color{blue}{E_{zzz}}(1-\lambda^2+z^2)+\color{blue}{E_{zz}}(3z)+\color{blue}{E_z}(1+k^2-k^2\lambda^2)+\color{red}{R_z}(-k^2z)+\color{red}{R}(k^2)=\color{orange}{C_z}(k^2\lambda^2-k^2)\\ \end{cases} $$ where $\color{orange}{C}=\frac{\omega_pq}{ck}\frac{1}{\sqrt{1+z^2}-\lambda z},\quad k\text{ and }z\text{ are free variables}$ Then substitute $\color{red}{R}\to\frac{\omega_pq}{ck}\color{red}{R},\quad \color{blue}{E}\to\frac{\omega_pq}{ck}\color{blue}{E},\quad\color{orange}{C}\to\frac{\omega_pq}{ck}\color{orange}{C}\quad$ and divide both eq. by $\frac{\omega_pq}{ck}$ New $\color{orange}{C}=\frac{1}{\sqrt{1+z^2}-\lambda z}$ Now I look for solution in form of $\color{red}{R}=\sum_{i=0}^{\infty}\color{red}{R^i}\frac{1}{k^i},\quad\color{blue}{E_z}=\sum_{i=0}^{\infty}\color{blue}{E^i}\frac{1}{k^i},\quad$where in case of $\frac{1}{k^i}-$ $i$ means power, in other cases upper index. It leads to following result: for odd $i$ holds $\color{red}{R^i}=\color{blue}{E^i}=0$, while for even $i$ there is a reccurent system of differential equations: $$ \begin{cases} \color{red}{R^i_{zzz}}(1-\lambda^2+z^2)+\color{red}{R^i_{zz}}(3z)+\color{red}{R^{i+2}_z}(z^2-\lambda^2)+\color{red}{R^{i+2}}(-z)+\color{blue}{E^{i+2}_z}(-z)=0\\ \color{blue}{E^i_{zzz}}(1-\lambda^2+z^2)+\color{blue}{E^i_{zz}}(3z)+\color{blue}{E^i_z}(1)+\color{blue}{E^{i+2}_z}(1-\lambda^2)+\color{red}{R^{i+2}_z}(-z)+\color{red}{R^{i+2}}(1)=0\\ \end{cases} $$ With initial conditions $\color{red}{R^0}=0,\quad\color{blue}{E^0_z}=-\color{orange}{C_z}$ It is possible to solve this system in general case: $$\color{red}{R^{i+2}}=(\frac{1}{\lambda^2})(1+z^2-\lambda^2)^{\frac{1}{2}}\int(1+z^2-\lambda^2)^{-\frac{1}{2}}\left[\color{red}{R^i_{zzz}}(1+z^2-\lambda^2)(-1-z^2+2\lambda^2)+\\+\color{red}{R^i_{zz}}(5z\lambda^2-2z-2z^3)+\color{red}{R^i_{z}}(2z^2+\lambda^2)+\color{red}{R^i}(z)\right]dz+\\+(-\frac{1}{\lambda^2})(1+z^2-\lambda^2)^{\frac{1}{2}}\int(1+z^2-\lambda^2)^{-\frac{1}{2}}\left[\color{red}{R^{i-2}_{zzzzz}}(1+z^2-\lambda^2)^2+\color{red}{R^{i-2}_{zzzz}}(6z)(1+z^2-\lambda^2)+\\+\color{red}{R^{i-2}_{zzz}}(1+10z^2-\lambda^2)+\color{red}{R^{i-2}_{zz}}(3z)\right]dz $$ $\color{blue}{E^i_z}$ is fully defined through $\color{red}{R^i}$: $$ \color{blue}{E^{i+2}_z}=\color{red}{R^{i+2}_z}(z-\frac{\lambda^2}{z})+\color{red}{R^{i+2}}(-1)+\color{red}{R^i_{zzz}}(\frac{1-\lambda^2}{z}+z)+\color{red}{R^i_{zz}}(3) $$ With initial conditions: $\color{red}{R^0}=0,\quad\color{red}{R^2}=(-\frac{1}{\lambda^2})(1+z^2-\lambda^2)^{\frac{1}{2}}\left[\int\color{orange}{C_{zzz}}(z)dz+\int\color{orange}{C_{zz}}(\frac{3z^2}{1+z^2-\lambda^2})dz+\int\color{orange}{C_{z}}(\frac{z}{1+z^2-\lambda^2})dz\right]$ Note: due to physical reasons all constants of integration equal to zero. And that's where I stuck, having no idea how to get closed form of $\color{red}{R}, \color{blue}{E}$ or at least solve reccurence. Really appreciate if someone can point a way.",,"['ordinary-differential-equations', 'recurrence-relations', 'systems-of-equations']"
81,Four-Dogs Pursuit [closed],Four-Dogs Pursuit [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Four dogs start at the corners of square $ABCD$ (labelled anti-clockwise). Running anti-clockwise, the dog starting at $A$ pursues the dog starting at $B$ , which pursues the dog starting at $C$ , which pursues the dog starting at $D$ , which pursues the dog starting at $A$ . They run at the constant speed of $7$ meters per second and the sides of the square are $30$ meters long. The pursuit stops when at least one dog has reached the centre of the square. Use a system of ordinary differential equations to model the trajectories of the dogs. Make a plot of the paths followed by the dogs. Determine how long the pursuit lasts.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Four dogs start at the corners of square (labelled anti-clockwise). Running anti-clockwise, the dog starting at pursues the dog starting at , which pursues the dog starting at , which pursues the dog starting at , which pursues the dog starting at . They run at the constant speed of meters per second and the sides of the square are meters long. The pursuit stops when at least one dog has reached the centre of the square. Use a system of ordinary differential equations to model the trajectories of the dogs. Make a plot of the paths followed by the dogs. Determine how long the pursuit lasts.",ABCD A B C D A 7 30,"['ordinary-differential-equations', 'differential-games']"
82,Clarification on asymptotically stability of dynamical systems,Clarification on asymptotically stability of dynamical systems,,"I'm wondering if someone can provide a clarification between 2 seemingly opposing definitions from reputable sources on dynamical systems! My Russian textbook, ""Dynamical Systems I: Ordinary Differential Equations and Smooth Dynamical Systems"" by Anosov, Arnol'd, Aronson, et al., says the following to determine whether a singular point of a dynamical system is asymptotically stable : Theorem 4.2: If all eigenvalues of the linear part of a vector field $v$ at a singular point have negative real part, then the singular point is asymptotically stable. To me, this means for any arbitrary dynamical system, say, $\dot{x} = f(x)$, where $x \in \mathbb{R}^{n}$, one can find where $f(x) = 0$, and solve the corresponding Jacobian for the eigenvalues of to determine stability. Further, if one finds that $\lambda_{i} < 0$, for $i = 1,2,...n$ then, this point is locally stable, by this theorem.  But, is this theorem now suggesting that this is point is now asymptotically stable as well? Almost every single textbook on ODEs that I have checked says to determine whether an equilibrium point is asymptotically stable, some more general method is required like constructing Lyapunov functions, determining limit sets, etc... Why is there such a difference? Is there a difference?","I'm wondering if someone can provide a clarification between 2 seemingly opposing definitions from reputable sources on dynamical systems! My Russian textbook, ""Dynamical Systems I: Ordinary Differential Equations and Smooth Dynamical Systems"" by Anosov, Arnol'd, Aronson, et al., says the following to determine whether a singular point of a dynamical system is asymptotically stable : Theorem 4.2: If all eigenvalues of the linear part of a vector field $v$ at a singular point have negative real part, then the singular point is asymptotically stable. To me, this means for any arbitrary dynamical system, say, $\dot{x} = f(x)$, where $x \in \mathbb{R}^{n}$, one can find where $f(x) = 0$, and solve the corresponding Jacobian for the eigenvalues of to determine stability. Further, if one finds that $\lambda_{i} < 0$, for $i = 1,2,...n$ then, this point is locally stable, by this theorem.  But, is this theorem now suggesting that this is point is now asymptotically stable as well? Almost every single textbook on ODEs that I have checked says to determine whether an equilibrium point is asymptotically stable, some more general method is required like constructing Lyapunov functions, determining limit sets, etc... Why is there such a difference? Is there a difference?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
83,Past exam paper question on Reduction of order.,Past exam paper question on Reduction of order.,,"Verify that $U_1(x)=x$ is a solution to the differential equation and solve $$x^2y''-x(x+2)y'+(x+2)y=0$$ Where $x>0$ . I've done most of the method, i'm at the point where you reduce the problem to a first order system, and then solve by using the separable method. It sounds stupid but i'm not sure what to do with this integral $\int (1/w) {dw} = \int (1) {dx}$ and what the final answer should be. Then use the Wronskien to show that the two solutions gained are linearly independent. (sorry that I couldn't write my entire methodology, i'm not that good with HTML yet).","Verify that is a solution to the differential equation and solve Where . I've done most of the method, i'm at the point where you reduce the problem to a first order system, and then solve by using the separable method. It sounds stupid but i'm not sure what to do with this integral and what the final answer should be. Then use the Wronskien to show that the two solutions gained are linearly independent. (sorry that I couldn't write my entire methodology, i'm not that good with HTML yet).",U_1(x)=x x^2y''-x(x+2)y'+(x+2)y=0 x>0 \int (1/w) {dw} = \int (1) {dx},['ordinary-differential-equations']
84,Sufficient Boundary Condition to a General PDE on a General Domain,Sufficient Boundary Condition to a General PDE on a General Domain,,"We know that for an ODE of $n^{th}$  order we need $n$ different boundary conditions. In PDEs, for example, for Laplace equation $\nabla^2 U=0$ (which is a second order PDE) we need only one B.C. (e.g Dirichlet or Neumann boundary conditions on the boundary $\partial \Omega$.) $\large {\frac{\partial U}{\partial \vec n}|_{\partial \Omega}=0\space}$ or  $\large  {   \space U|_{\partial \Omega}=0}$ I would like to know what is a sufficient boundary condition to a more general equation like $\Large {\frac{\partial^m U}{\partial x^m}+\frac{\partial^n U}{\partial y^n}=0}$ on a boundary $\partial  \Omega$. How do we know if a given boundary condition is sufficient to solve a general PDE? What is the relation between the order of a PDE and the number and type of boundary conditions needed on $\partial \Omega$. Thanks.","We know that for an ODE of $n^{th}$  order we need $n$ different boundary conditions. In PDEs, for example, for Laplace equation $\nabla^2 U=0$ (which is a second order PDE) we need only one B.C. (e.g Dirichlet or Neumann boundary conditions on the boundary $\partial \Omega$.) $\large {\frac{\partial U}{\partial \vec n}|_{\partial \Omega}=0\space}$ or  $\large  {   \space U|_{\partial \Omega}=0}$ I would like to know what is a sufficient boundary condition to a more general equation like $\Large {\frac{\partial^m U}{\partial x^m}+\frac{\partial^n U}{\partial y^n}=0}$ on a boundary $\partial  \Omega$. How do we know if a given boundary condition is sufficient to solve a general PDE? What is the relation between the order of a PDE and the number and type of boundary conditions needed on $\partial \Omega$. Thanks.",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative', 'boundary-value-problem']"
85,What can we move $dx$ around and integrate both sides when solving ODE?,What can we move  around and integrate both sides when solving ODE?,dx,"It is a common practice to move the $dx$ around when solving ODE and we take for granted when we integrate both sides. However, I've been rather uncomfortable with this. From an analysis perspective, $dx$ itself doesn't make sense to me. Instead, we always consider $\frac{df(x)}{dx}$, or the operator $\frac{d}{dx}$ by itself. Now, some people argue that both $dy$ and $dx$ are infinitesimal things that are not equal to zero, and thus we can move them around. But that doesn't convince me neither. Shouldn't we concretely define what $dy$ and $dx$ are before we do anything to them? Is it possible to justify moving $dx$ around by using $\delta-\epsilon$ analysis? As for integrating both sides, what annoys me is that sometimes extra terms (usually the constant $C$) appear. It's tempting to think of ""integrating both sides"" as a kind of operations (maybe not binary?) just like add or subtract, but that doesn't seem right because by apply indefinite integral a function we actually get a bunch of functions. What is the right way to think of this process? Should we think about this process under the framework of algebra, maybe? Hope my questions make sense. Thanks.","It is a common practice to move the $dx$ around when solving ODE and we take for granted when we integrate both sides. However, I've been rather uncomfortable with this. From an analysis perspective, $dx$ itself doesn't make sense to me. Instead, we always consider $\frac{df(x)}{dx}$, or the operator $\frac{d}{dx}$ by itself. Now, some people argue that both $dy$ and $dx$ are infinitesimal things that are not equal to zero, and thus we can move them around. But that doesn't convince me neither. Shouldn't we concretely define what $dy$ and $dx$ are before we do anything to them? Is it possible to justify moving $dx$ around by using $\delta-\epsilon$ analysis? As for integrating both sides, what annoys me is that sometimes extra terms (usually the constant $C$) appear. It's tempting to think of ""integrating both sides"" as a kind of operations (maybe not binary?) just like add or subtract, but that doesn't seem right because by apply indefinite integral a function we actually get a bunch of functions. What is the right way to think of this process? Should we think about this process under the framework of algebra, maybe? Hope my questions make sense. Thanks.",,"['calculus', 'ordinary-differential-equations']"
86,Does $A$ commute with $e^{\int A \: dt}$,Does  commute with,A e^{\int A \: dt},"I have been studying the linear system of the form: $$D_tX = AX + \textbf{b}$$ Where $A$ is not necessarily constant Suppose we aim to find an integrating factor $M$ such that: $$M[D_tX - AX] = D_t(MX)$$ This gives: $$MD_tX - MAX = (D_tM)X + M(D_tX)$$ By equating coefficients we get: $$D_tM = -MA$$ Solving this gives: $$M = e^{-\int A \: dt}$$ But $$D_t(e^{-\int{A} \: dt}) = -Ae^{-\int{A} \: dt} = -AM$$ So can we conclude that these two matrices commute? edit I have proven that $$AM = MA$$ if and only if $$A\left(\int{A} \: dt \right ) = \left (\int{A} \: dt \right ) A$$ edit 2 After looking further into the question, it appears that for non-constant matrices $$D_te^{A(t)} \neq \left ( D_tA(t) \right ) e^{A(t)}$$ more can be found here","I have been studying the linear system of the form: $$D_tX = AX + \textbf{b}$$ Where $A$ is not necessarily constant Suppose we aim to find an integrating factor $M$ such that: $$M[D_tX - AX] = D_t(MX)$$ This gives: $$MD_tX - MAX = (D_tM)X + M(D_tX)$$ By equating coefficients we get: $$D_tM = -MA$$ Solving this gives: $$M = e^{-\int A \: dt}$$ But $$D_t(e^{-\int{A} \: dt}) = -Ae^{-\int{A} \: dt} = -AM$$ So can we conclude that these two matrices commute? edit I have proven that $$AM = MA$$ if and only if $$A\left(\int{A} \: dt \right ) = \left (\int{A} \: dt \right ) A$$ edit 2 After looking further into the question, it appears that for non-constant matrices $$D_te^{A(t)} \neq \left ( D_tA(t) \right ) e^{A(t)}$$ more can be found here",,"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations']"
87,The transformation from Ito integral to Stratnonvich integral,The transformation from Ito integral to Stratnonvich integral,,"In the book Introduction to SDE by Evans, it says that if $\mathbf{X}$ solves the Ito sde $$ \left\{\begin{aligned} d \mathbf{X} &=\mathbf{b}(\mathbf{X}, t) d t+\mathbf{B}(\mathbf{X}, t) d \mathbf{W} \\ \mathbf{X}(0) &=\mathbf{X}_{0} \end{aligned}\right.$$ if and only if $\mathbf{X}$ solves the Stratonovich sde $$ \left\{\begin{aligned} d \mathbf{X} &=\left[\mathbf{b}(\mathbf{X}, t)-\frac{1}{2} \mathbf{c}(\mathbf{X}, t)\right] d t+\mathbf{B}(\mathbf{X}, t) \circ d \mathbf{W} \\ \mathbf{X}(0) &=\mathbf{X}_{0} \end{aligned}\right. $$ where $$  c^{i}(x, t) :=\sum_{k=1}^{m} \sum_{j=1}^{n} b_{x_{j}}^{i k}(x, t) b^{j k}(x, t). $$ However, if I use the conversion formula that is $$ \begin{aligned}&\left[\int_{0}^{T} \mathbf{B}(\mathbf{W}, t) \circ d \mathbf{W} \right]^{i} \\ &=\left[\int_{0}^{T} \mathbf{B}(\mathbf{W}, t) d \mathbf{W}\right]^{i}+\frac{1}{2} \int_{0}^{T} \sum_{j=1}^{n} b_{x_{j}}^{i j}(\mathbf{W}, t) d t \end{aligned}$$ to the ito sde then we have $$ \begin{aligned} d \mathbf{X}&=\mathbf{b}(\mathbf{X}, t) d t+\mathbf{B}(\mathbf{X}, t) d \mathbf{W}\\ &=\mathbf{b}(\mathbf{X}, t) d t+\mathbf{B}(\mathbf{X}, t) \circ d \mathbf{W}-\frac{1}{2} \sum_{j=1}^{n} b_{x_{j}}^{i j}(\mathbf{W}, t) d t \end{aligned} $$ which is not the same one in the Stratonovich sde.","In the book Introduction to SDE by Evans, it says that if solves the Ito sde if and only if solves the Stratonovich sde where However, if I use the conversion formula that is to the ito sde then we have which is not the same one in the Stratonovich sde.","\mathbf{X} 
\left\{\begin{aligned} d \mathbf{X} &=\mathbf{b}(\mathbf{X}, t) d t+\mathbf{B}(\mathbf{X}, t) d \mathbf{W} \\ \mathbf{X}(0) &=\mathbf{X}_{0} \end{aligned}\right. \mathbf{X} 
\left\{\begin{aligned} d \mathbf{X} &=\left[\mathbf{b}(\mathbf{X}, t)-\frac{1}{2} \mathbf{c}(\mathbf{X}, t)\right] d t+\mathbf{B}(\mathbf{X}, t) \circ d \mathbf{W} \\ \mathbf{X}(0) &=\mathbf{X}_{0} \end{aligned}\right.
  
c^{i}(x, t) :=\sum_{k=1}^{m} \sum_{j=1}^{n} b_{x_{j}}^{i k}(x, t) b^{j k}(x, t).
 
\begin{aligned}&\left[\int_{0}^{T} \mathbf{B}(\mathbf{W}, t) \circ d \mathbf{W} \right]^{i} \\ &=\left[\int_{0}^{T} \mathbf{B}(\mathbf{W}, t) d \mathbf{W}\right]^{i}+\frac{1}{2} \int_{0}^{T} \sum_{j=1}^{n} b_{x_{j}}^{i j}(\mathbf{W}, t) d t \end{aligned} 
\begin{aligned}
d \mathbf{X}&=\mathbf{b}(\mathbf{X}, t) d t+\mathbf{B}(\mathbf{X}, t) d \mathbf{W}\\
&=\mathbf{b}(\mathbf{X}, t) d t+\mathbf{B}(\mathbf{X}, t) \circ d \mathbf{W}-\frac{1}{2} \sum_{j=1}^{n} b_{x_{j}}^{i j}(\mathbf{W}, t) d t
\end{aligned}
","['stochastic-processes', 'stochastic-calculus', 'stochastic-integrals']"
88,To find Wronskian of a ODE,To find Wronskian of a ODE,,"Let $y_{1}=\phi(x)$ and $y_{2}=\psi(x)$ be solutions of the ODE $$y''-2xy'+(sinx^2)y=0$$ such that $\phi(0)=1,\phi'(0)=1$ and $\psi(0)=1,\psi'(0)=2$. Then the value of the Wronskian $W(\phi,\psi)$ at $x=1$ is $0$ $1$ $e$ $e^2$ The Wronskian of a set of solutions for a second order ODE has a formula of the form: $W[\phi,\psi](x) = Ce^{f(x)}$, where $C$ is a constant and $f(x)$ is a function. I know that at any point $x_0$ \begin{alignat*}{2} W(x_{0}) &= \left| \begin{matrix} \phi(x_{0}) & \psi(x_{0}) \\ \phi^{\prime}(x_{0}) & \psi^{\prime}(x_{0}) \\ \end{matrix} \right| &= \phi(x_{0})\psi^{\prime}(x_{0}) - \psi(x_{0})\phi^{\prime}(x_{0}) \end{alignat*} but I am unable to find  $C$ and $f$. Please help. Thanks.","Let $y_{1}=\phi(x)$ and $y_{2}=\psi(x)$ be solutions of the ODE $$y''-2xy'+(sinx^2)y=0$$ such that $\phi(0)=1,\phi'(0)=1$ and $\psi(0)=1,\psi'(0)=2$. Then the value of the Wronskian $W(\phi,\psi)$ at $x=1$ is $0$ $1$ $e$ $e^2$ The Wronskian of a set of solutions for a second order ODE has a formula of the form: $W[\phi,\psi](x) = Ce^{f(x)}$, where $C$ is a constant and $f(x)$ is a function. I know that at any point $x_0$ \begin{alignat*}{2} W(x_{0}) &= \left| \begin{matrix} \phi(x_{0}) & \psi(x_{0}) \\ \phi^{\prime}(x_{0}) & \psi^{\prime}(x_{0}) \\ \end{matrix} \right| &= \phi(x_{0})\psi^{\prime}(x_{0}) - \psi(x_{0})\phi^{\prime}(x_{0}) \end{alignat*} but I am unable to find  $C$ and $f$. Please help. Thanks.",,['ordinary-differential-equations']
89,Backgrounds of the p-Laplacian Operator,Backgrounds of the p-Laplacian Operator,,"Motivation I encountered the following partial differential equation (PDE) in a mathematical paper $$\begin{array}{} u_{tt}+\Delta^2u-\nabla\cdot\left(|\nabla u|^{p-2}\nabla u\right)-\Delta u_{t}+\int_{0}^{t}g(t-s)\Delta u(x,s) ds=f(x,u,u_{t}) & \text{in} & \partial \Omega \times (0,T) \\ u=\frac{\partial u}{\partial n}=0 & \text{on} & \partial \Omega \times [0,T) \\ u(x,0)=u_0(x), \qquad u_{t}(x,0)=v_0(x) & \text{in} & \partial \Omega \end{array} $$ where $\Omega \subset \mathbb{R}^n$ is a bounded domain with Lipschitz-continuous boundary $\partial \Omega$. Also, $g \ge 0$ is called memory kernel that decays with a general rate and $f(x,u,u_t)$ is some nonlinear function. $p \ge 2$ is a real constant. The differential operators $\Delta$, $\Delta^2$ and $\nabla$ are the Laplacian, the Biharmonic and gradient operators, respectively. We are interested in the case $n=3$ which has a physical meaning. Now, let us go into some physical insights. Plates are initially flat structural members bounded by two parallel planes, called faces, and a cylindrical surface, called an edge or boundary. The generators of the cylindrical surface are perpendicular to the plane faces. The deck of a ship is an example of a plate. This PDE is describing the lateral displacement of a plate made of a homogeneous isotropic nonelinear viscoelastic material. The function $u(x,t)$ is the lateral displacement of the plate at position $x$ and time $t$. I know that in classical linear elasticity , the following equation $$u_{tt}+\Delta^2u=f(x) \tag{1}$$ describes the lateral displacement of a plate made of a homogeneous isotropic elastic material where $f(x)$ is an external force applied to the plate and $u_{tt}$ describes the inertia or acceleration term. It is also known as the equation for vibration of plates. I found from this paper that if we have structural damping then the term $\Delta u_t$ shows up in $(1)$. Also, an article in wikipedia revealed that the integral term $\int_{0}^{t}g(t-s)\Delta u(x,s) ds$ can show up in $(1)$ when viscoelasticity comes in. What remains unknown is $$\Delta_p u \equiv \nabla\cdot\left(|\nabla u|^{p-2}\nabla u\right)$$ which is called the p-Laplacian operator. For $p=2$, it is the usual Laplacian operator $\Delta$. I really cannot find any background of this operator. Question Can you please shed some light on the physical , mathematical or historical background of the p-Laplacian term? Where does it come from? Notes This question is answered on MathOverFlow . Interested readers can check it out.","Motivation I encountered the following partial differential equation (PDE) in a mathematical paper $$\begin{array}{} u_{tt}+\Delta^2u-\nabla\cdot\left(|\nabla u|^{p-2}\nabla u\right)-\Delta u_{t}+\int_{0}^{t}g(t-s)\Delta u(x,s) ds=f(x,u,u_{t}) & \text{in} & \partial \Omega \times (0,T) \\ u=\frac{\partial u}{\partial n}=0 & \text{on} & \partial \Omega \times [0,T) \\ u(x,0)=u_0(x), \qquad u_{t}(x,0)=v_0(x) & \text{in} & \partial \Omega \end{array} $$ where $\Omega \subset \mathbb{R}^n$ is a bounded domain with Lipschitz-continuous boundary $\partial \Omega$. Also, $g \ge 0$ is called memory kernel that decays with a general rate and $f(x,u,u_t)$ is some nonlinear function. $p \ge 2$ is a real constant. The differential operators $\Delta$, $\Delta^2$ and $\nabla$ are the Laplacian, the Biharmonic and gradient operators, respectively. We are interested in the case $n=3$ which has a physical meaning. Now, let us go into some physical insights. Plates are initially flat structural members bounded by two parallel planes, called faces, and a cylindrical surface, called an edge or boundary. The generators of the cylindrical surface are perpendicular to the plane faces. The deck of a ship is an example of a plate. This PDE is describing the lateral displacement of a plate made of a homogeneous isotropic nonelinear viscoelastic material. The function $u(x,t)$ is the lateral displacement of the plate at position $x$ and time $t$. I know that in classical linear elasticity , the following equation $$u_{tt}+\Delta^2u=f(x) \tag{1}$$ describes the lateral displacement of a plate made of a homogeneous isotropic elastic material where $f(x)$ is an external force applied to the plate and $u_{tt}$ describes the inertia or acceleration term. It is also known as the equation for vibration of plates. I found from this paper that if we have structural damping then the term $\Delta u_t$ shows up in $(1)$. Also, an article in wikipedia revealed that the integral term $\int_{0}^{t}g(t-s)\Delta u(x,s) ds$ can show up in $(1)$ when viscoelasticity comes in. What remains unknown is $$\Delta_p u \equiv \nabla\cdot\left(|\nabla u|^{p-2}\nabla u\right)$$ which is called the p-Laplacian operator. For $p=2$, it is the usual Laplacian operator $\Delta$. I really cannot find any background of this operator. Question Can you please shed some light on the physical , mathematical or historical background of the p-Laplacian term? Where does it come from? Notes This question is answered on MathOverFlow . Interested readers can check it out.",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-physics']"
90,proof that a system has at least one limit cycle,proof that a system has at least one limit cycle,,"The problem is: Assuming that the parameters $a, b$ are real numbers and that $ab \neq0$, by transforming the system using polar coordinates , prove that the system $$x'=y+x(1-a^2x^2-b^2y^2)$$ $$y'=-x+y(1-a^2x^2-b^2y^2)$$ has at least one limit cycle in the phase plane. My proof is: taking $x=r\cos\theta$ and $y=r\sin\theta$, we have $x^2+y^2=r^2$, i.e. $rr'=xx'+yy'$, so: $$r'=r(1-a^2r^2\cos^2\theta-b^2r^2\sin^2\theta)$$ $$\theta'=-1.$$ So, $\theta = \theta(0)-t$ and $$1-a^2r^2\cos^2\theta-b^2r^2\sin^2\theta=0 \implies \frac{x^2}{b^2}+\frac{y^2}{a^2}=\frac{1}{(ab)^2}.$$ That means the system has a periodic solution which is ellipse. My question is how to prove that it is limit cycle? or the system has another solution which is limit cycle? Any hints are welcome! Thank you!","The problem is: Assuming that the parameters $a, b$ are real numbers and that $ab \neq0$, by transforming the system using polar coordinates , prove that the system $$x'=y+x(1-a^2x^2-b^2y^2)$$ $$y'=-x+y(1-a^2x^2-b^2y^2)$$ has at least one limit cycle in the phase plane. My proof is: taking $x=r\cos\theta$ and $y=r\sin\theta$, we have $x^2+y^2=r^2$, i.e. $rr'=xx'+yy'$, so: $$r'=r(1-a^2r^2\cos^2\theta-b^2r^2\sin^2\theta)$$ $$\theta'=-1.$$ So, $\theta = \theta(0)-t$ and $$1-a^2r^2\cos^2\theta-b^2r^2\sin^2\theta=0 \implies \frac{x^2}{b^2}+\frac{y^2}{a^2}=\frac{1}{(ab)^2}.$$ That means the system has a periodic solution which is ellipse. My question is how to prove that it is limit cycle? or the system has another solution which is limit cycle? Any hints are welcome! Thank you!",,['ordinary-differential-equations']
91,Classification of non-hyperbolic equilibrium and its global manifolds,Classification of non-hyperbolic equilibrium and its global manifolds,,"Given  $$ \begin{cases} \dot{x} = x^2\\ \dot{y} = -y\\ \dot{z} = z \end{cases} $$ Classify the type of equilibrium for the point $(0,0,0)$ when $u=0$ as well as its stability. Also, describe the global stable and unstable manifolds of $(0,0,0)$. My attempt: We easily see that $(0,0,0)$ is not a hyperbolic equilibrium, so we cannot apply either Principle of Linearized Stability or Global Stable/Unstable manifold theorem. However, we also see that the explicit solution for this problem is $(x(t), y(t), z(t)) = (\frac{-1}{t+c_1}, c_2e^{-t}, c_3e^t)$. The global stable manifold is defined as the set of $(x(t), y(t), z(t))$ such that $\lim_{t\rightarrow \infty} (x(t), y(t), z(t)) = (0,0,0)$. This means $c_3 = 0$ is the only choice, while $c_1$ and $c_2$ can be anything. Thus, the global stable manifold are the entire $xy$-plane. Similarly, for the global unstable manifold, $\lim_{t\rightarrow -\infty} (x(t), y(t), z(t)) = (0,0,0)$. This occurs only if $c_2 = 0$. Thus, the unstable manifold is the entire $xz$-plane. My question: Is my solution above correct? Also, is the above method the only way to classify the type of equilibrium for NON-HYPERBOLIC equilibrium in a planar system, assume that we can solve explicitly the general formula for $x(t)$ and $y(t)$?","Given  $$ \begin{cases} \dot{x} = x^2\\ \dot{y} = -y\\ \dot{z} = z \end{cases} $$ Classify the type of equilibrium for the point $(0,0,0)$ when $u=0$ as well as its stability. Also, describe the global stable and unstable manifolds of $(0,0,0)$. My attempt: We easily see that $(0,0,0)$ is not a hyperbolic equilibrium, so we cannot apply either Principle of Linearized Stability or Global Stable/Unstable manifold theorem. However, we also see that the explicit solution for this problem is $(x(t), y(t), z(t)) = (\frac{-1}{t+c_1}, c_2e^{-t}, c_3e^t)$. The global stable manifold is defined as the set of $(x(t), y(t), z(t))$ such that $\lim_{t\rightarrow \infty} (x(t), y(t), z(t)) = (0,0,0)$. This means $c_3 = 0$ is the only choice, while $c_1$ and $c_2$ can be anything. Thus, the global stable manifold are the entire $xy$-plane. Similarly, for the global unstable manifold, $\lim_{t\rightarrow -\infty} (x(t), y(t), z(t)) = (0,0,0)$. This occurs only if $c_2 = 0$. Thus, the unstable manifold is the entire $xz$-plane. My question: Is my solution above correct? Also, is the above method the only way to classify the type of equilibrium for NON-HYPERBOLIC equilibrium in a planar system, assume that we can solve explicitly the general formula for $x(t)$ and $y(t)$?",,['ordinary-differential-equations']
92,Numerical method for SDEs,Numerical method for SDEs,,"I'm using a 4th order Adams predictor-corrector method to numerically solve a regular differential equation. Now I would be interested to be able to include a noisy term to the equation -as in the Euler-Maruyama method , the classical and easy way to simulate a Brownian motion via a Wiener process-, but I have almost zero experience with convergence and orders in Monte Carlo methods. Is there a canonical way to do it? I believe the Runge-Kutta method can be adapted to stochastic differential equations but, as before, I know next to nothing about it. As a first guess, I would include a $w_{0,1}\cdot \sigma \cdot \sqrt{h}$ term in the corrector part. Could you give me any reference or describe an example on how to do it?","I'm using a 4th order Adams predictor-corrector method to numerically solve a regular differential equation. Now I would be interested to be able to include a noisy term to the equation -as in the Euler-Maruyama method , the classical and easy way to simulate a Brownian motion via a Wiener process-, but I have almost zero experience with convergence and orders in Monte Carlo methods. Is there a canonical way to do it? I believe the Runge-Kutta method can be adapted to stochastic differential equations but, as before, I know next to nothing about it. As a first guess, I would include a $w_{0,1}\cdot \sigma \cdot \sqrt{h}$ term in the corrector part. Could you give me any reference or describe an example on how to do it?",,"['ordinary-differential-equations', 'reference-request', 'numerical-methods', 'stochastic-calculus']"
93,how to solve fokker-planck equation using space-time laplace transform?,how to solve fokker-planck equation using space-time laplace transform?,,"I was wondering about how to solve a simple linear Fokker-Planck equation using space-time Laplace transform on space interval $[0,+ \ \infty)$ , $$\frac{\partial f(x,t)}{\partial t}= k_1 \frac{\partial f(x,t)}{\partial x} + k_2 \frac{\partial^2 f(x,t)}{\partial x^2}.$$ The usual method is to do Laplace transform in time and then solve the spatial differential equation by transforming it into a Sturm-Liouville problem. But I feel that for a special case of semi-infinite, i.e., $[0, \ \infty)$ , one can solve it easily if used Laplace transform for space-time instead of only time. EDIT: The OP didn't specify boundary conditions, making the problem ill-defined. Among the many possible boundary conditions confining the process to the interval $[0,+\infty)$ , one of the most used ones are the reflecting boundary conditions, $$\left[k_1 f(x,t)+k_2\frac{\partial}{\partial x}f(x,t)\right]_{x=0}=0,\quad \forall t.$$","I was wondering about how to solve a simple linear Fokker-Planck equation using space-time Laplace transform on space interval , The usual method is to do Laplace transform in time and then solve the spatial differential equation by transforming it into a Sturm-Liouville problem. But I feel that for a special case of semi-infinite, i.e., , one can solve it easily if used Laplace transform for space-time instead of only time. EDIT: The OP didn't specify boundary conditions, making the problem ill-defined. Among the many possible boundary conditions confining the process to the interval , one of the most used ones are the reflecting boundary conditions,","[0,+ \ \infty) \frac{\partial f(x,t)}{\partial t}= k_1 \frac{\partial f(x,t)}{\partial x} + k_2 \frac{\partial^2 f(x,t)}{\partial x^2}. [0, \ \infty) [0,+\infty) \left[k_1 f(x,t)+k_2\frac{\partial}{\partial x}f(x,t)\right]_{x=0}=0,\quad \forall t.","['ordinary-differential-equations', 'partial-differential-equations', 'stochastic-processes']"
94,"Referral of a Textbook or Book that teaches Intuition, focusing on Calculus.","Referral of a Textbook or Book that teaches Intuition, focusing on Calculus.",,"I was wondering if there is a book out there that doesn't teach you how to do calculus, but teaches you how to apply it in the physical or social sciences. I know calculus, integration and differentiation and the applications for each, but I struggle to find a book that teaches me how to use calculus to derive my own mathematical models. I think it will be easier to give an example: Imagine a spinning rod with uniform mass with length, $L$. To find the kinetic energy of the rod I know that each part of the rod moves at a different speed, and therefore the entire kinetic energy of the rod is the sum of all the infinitesimals of little sections on the rod. I am not going to solve this problem, because this isn't the question on hand, but the point is is that I am looking for a book that teaches you, and challenges you to use your already knowledge of calculus to solve problems like this? Hope I am making myself clear, and any input would greatly be appreciated.","I was wondering if there is a book out there that doesn't teach you how to do calculus, but teaches you how to apply it in the physical or social sciences. I know calculus, integration and differentiation and the applications for each, but I struggle to find a book that teaches me how to use calculus to derive my own mathematical models. I think it will be easier to give an example: Imagine a spinning rod with uniform mass with length, $L$. To find the kinetic energy of the rod I know that each part of the rod moves at a different speed, and therefore the entire kinetic energy of the rod is the sum of all the infinitesimals of little sections on the rod. I am not going to solve this problem, because this isn't the question on hand, but the point is is that I am looking for a book that teaches you, and challenges you to use your already knowledge of calculus to solve problems like this? Hope I am making myself clear, and any input would greatly be appreciated.",,"['calculus', 'ordinary-differential-equations', 'education']"
95,"In $\mathbb{R}^n$, locally lipschitz on compact set implies lipschitz","In , locally lipschitz on compact set implies lipschitz",\mathbb{R}^n,"I need to prove: Let $A$ be open in $\mathbb{R}^m$ , $g:A \longrightarrow \mathbb{R}^n$ a  locally lipschitz function and $C$ a compact subset of $A$ . Show that $g$ is lipschitz on $C$ . Can anyone help me?","I need to prove: Let be open in , a  locally lipschitz function and a compact subset of . Show that is lipschitz on . Can anyone help me?",A \mathbb{R}^m g:A \longrightarrow \mathbb{R}^n C A g C,['real-analysis']
96,Arbitrary factors for the (modified) Mathieu equation,Arbitrary factors for the (modified) Mathieu equation,,"I am currently confronted with a physical equation that, after a fair amount of reworking, can be recast in the form of the modified Mathieu equation : \begin{equation} y(x)'' - (a - 2q \cosh(2x)) y(x) = 0 \end{equation} In this case, the parameters $a$ and $q$ are somewhat arbitrary real numbers (q can be any positive real number but is bounded from below at a negative value), although they do obey the relation \begin{equation} a = k^2 - 2q,\ k \in \mathbb{N} \end{equation} But from what I can see on the Mathieu equation, those types of situations are very rarely taken into account and are instead generally speaking of the form \begin{equation} y(x)'' - (a_n(q) - 2q \cosh(2x)) y(x) = 0 \end{equation} where only certain values of $a$ are considered, with $n$ generally integer, rational or in very rare cases real, and even then usually restricted to specific regions As far as I can tell, my equation will inevitably cross into the unstable regions for some configurations of the physical system. How can the modified Mathieu equation be dealt with in such circumstances? Does it differ significantly from the Mathieu equation in terms of stability? Is there any exact solution, or if not, can properties still be decided from it?","I am currently confronted with a physical equation that, after a fair amount of reworking, can be recast in the form of the modified Mathieu equation : \begin{equation} y(x)'' - (a - 2q \cosh(2x)) y(x) = 0 \end{equation} In this case, the parameters $a$ and $q$ are somewhat arbitrary real numbers (q can be any positive real number but is bounded from below at a negative value), although they do obey the relation \begin{equation} a = k^2 - 2q,\ k \in \mathbb{N} \end{equation} But from what I can see on the Mathieu equation, those types of situations are very rarely taken into account and are instead generally speaking of the form \begin{equation} y(x)'' - (a_n(q) - 2q \cosh(2x)) y(x) = 0 \end{equation} where only certain values of $a$ are considered, with $n$ generally integer, rational or in very rare cases real, and even then usually restricted to specific regions As far as I can tell, my equation will inevitably cross into the unstable regions for some configurations of the physical system. How can the modified Mathieu equation be dealt with in such circumstances? Does it differ significantly from the Mathieu equation in terms of stability? Is there any exact solution, or if not, can properties still be decided from it?",,"['ordinary-differential-equations', 'physics', 'stability-in-odes']"
97,"Find necessary and sufficient conditions so that $(0,0)$ is stable.",Find necessary and sufficient conditions so that  is stable.,"(0,0)","Suppose we have the system $$ \left(\begin{array}{c} \dot{x} \\ \dot{y} \end{array}\right) = \left(\begin{array}{c} f(x) + y \\ g(x) \end{array}\right). $$ Here $f,g: \mathbb{R} \to \mathbb{R}$ are smooth analytic functions of $x$ such that $$ \lim_{x\to 0} \frac{f(x)}{x^k} $$ and $$ \lim_{x\to 0} \frac{g(x)}{x^l} $$ exists and are non-zero for some $k, l \geq 2$. What are necessary and sufficient conditions on $f$ and $g$ so that $(0, 0)$ is (asymptotically) stable? I have no idea where to start. I thought it has to do something with the signs of $f$ and $g$ near $0$, but the $y$ term causes a disturbance, and I don't know how to handle that, so to say.","Suppose we have the system $$ \left(\begin{array}{c} \dot{x} \\ \dot{y} \end{array}\right) = \left(\begin{array}{c} f(x) + y \\ g(x) \end{array}\right). $$ Here $f,g: \mathbb{R} \to \mathbb{R}$ are smooth analytic functions of $x$ such that $$ \lim_{x\to 0} \frac{f(x)}{x^k} $$ and $$ \lim_{x\to 0} \frac{g(x)}{x^l} $$ exists and are non-zero for some $k, l \geq 2$. What are necessary and sufficient conditions on $f$ and $g$ so that $(0, 0)$ is (asymptotically) stable? I have no idea where to start. I thought it has to do something with the signs of $f$ and $g$ near $0$, but the $y$ term causes a disturbance, and I don't know how to handle that, so to say.",,"['ordinary-differential-equations', 'dynamical-systems']"
98,"PDE (similar to Heat equation) tranformation, how to solve.","PDE (similar to Heat equation) tranformation, how to solve.",,"I suddenly ran into this equation: Let $u:[a,b]\times \mathbb{R} \rightarrow \mathbb{R}$ be a function satisfying: $$\partial_t u = u' + \frac{1}{2}u'', \quad (1)$$ with some boundary conditions that are not relevant for the question. I solved this equation by choosing $u(t,x):= e^{ax+bt}v(t,x)$ where $v$ solves the well-known Heat equation: $\partial_t v = \frac{1}{2} v''$ with corresponding modified boundary conditions. The question is: What about considering the more general version of equation $(1)$? $$\partial_t u = p(x)u' + \frac{1}{2}u''$$ where $p(x) = \sum_{k=0}^n a_k x^k$ is a polynomial of degree $n$? Can one use a similar ""tranformation trick"" like ""$u(t,x)= \mbox{something}\times v(t,x)$"" where $v$ solves the Heat equation or even $(1)$ or any other solvable PDE? If this trick is not possible, are there other tricks, is there a theory on how to solve such PDEs? If this is too demanding , would it be at least possible for the case $p(x)=x$ or $p(x)=x^2$? Thanks a lot! :)","I suddenly ran into this equation: Let $u:[a,b]\times \mathbb{R} \rightarrow \mathbb{R}$ be a function satisfying: $$\partial_t u = u' + \frac{1}{2}u'', \quad (1)$$ with some boundary conditions that are not relevant for the question. I solved this equation by choosing $u(t,x):= e^{ax+bt}v(t,x)$ where $v$ solves the well-known Heat equation: $\partial_t v = \frac{1}{2} v''$ with corresponding modified boundary conditions. The question is: What about considering the more general version of equation $(1)$? $$\partial_t u = p(x)u' + \frac{1}{2}u''$$ where $p(x) = \sum_{k=0}^n a_k x^k$ is a polynomial of degree $n$? Can one use a similar ""tranformation trick"" like ""$u(t,x)= \mbox{something}\times v(t,x)$"" where $v$ solves the Heat equation or even $(1)$ or any other solvable PDE? If this trick is not possible, are there other tricks, is there a theory on how to solve such PDEs? If this is too demanding , would it be at least possible for the case $p(x)=x$ or $p(x)=x^2$? Thanks a lot! :)",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
99,No local optima in quantum control?,No local optima in quantum control?,,"Given a manifold $M$ and a set of smooth functions of one real variable $\mathcal{A}$ and a 'control system' type first order differential equation: $\frac{d x(t)}{dt} = F(x,u)$ one can consider the 'end-point map' $V_T:\mathcal{A}\rightarrow M$ which takes a control and sends it to the associated solution at time $T$. Further given a smooth real valued function $J: M \rightarrow [0,1]$ with a single optimum taking the value $1$ ,is there a general principle for determining if a given $G(w)=J(V_T(w))$ has, assuming that the system is controllable, no local optima in the space of controls? It is clear to me that it can have. One important example is that of quantum control. In this example $M=SU(n)$, $\mathcal{A}$ is some space of smooth functions (typically large or just taken as all smooth, bounded functions) the differential equation is: $\frac{d U_t}{dt}= (a + w(t)b)U_t$ where $a,b$ generate $\mathfrak{su}(n)$. In this case $J(U)=|Tr(U^{\dagger}G)|^2$ for some $G\in SU(n)$. Numerical evidence very strongly suggests that $J(V_T(w))$ never has local optima in these situations for $SU(4)$, but this seems very hard to prove. My attempts have all involved attempting to find the appropriate Hessian and understanding its index, but to no avail. My instinct is that for almost all $a,b$ there will no local optima. Cross-posted from MO: https://mathoverflow.net/q/221383/41654 after no answer.","Given a manifold $M$ and a set of smooth functions of one real variable $\mathcal{A}$ and a 'control system' type first order differential equation: $\frac{d x(t)}{dt} = F(x,u)$ one can consider the 'end-point map' $V_T:\mathcal{A}\rightarrow M$ which takes a control and sends it to the associated solution at time $T$. Further given a smooth real valued function $J: M \rightarrow [0,1]$ with a single optimum taking the value $1$ ,is there a general principle for determining if a given $G(w)=J(V_T(w))$ has, assuming that the system is controllable, no local optima in the space of controls? It is clear to me that it can have. One important example is that of quantum control. In this example $M=SU(n)$, $\mathcal{A}$ is some space of smooth functions (typically large or just taken as all smooth, bounded functions) the differential equation is: $\frac{d U_t}{dt}= (a + w(t)b)U_t$ where $a,b$ generate $\mathfrak{su}(n)$. In this case $J(U)=|Tr(U^{\dagger}G)|^2$ for some $G\in SU(n)$. Numerical evidence very strongly suggests that $J(V_T(w))$ never has local optima in these situations for $SU(4)$, but this seems very hard to prove. My attempts have all involved attempting to find the appropriate Hessian and understanding its index, but to no avail. My instinct is that for almost all $a,b$ there will no local optima. Cross-posted from MO: https://mathoverflow.net/q/221383/41654 after no answer.",,"['ordinary-differential-equations', 'optimization', 'lie-groups', 'lie-algebras', 'control-theory']"
