,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Elementary proof that monotone functions are differentiable somewhere,Elementary proof that monotone functions are differentiable somewhere,,"It is well-known that every monotone function $f : \mathbb{R} \to \mathbb{R}$ is differentiable almost everywhere (with respect to Lebesgue measure). It is also known if $E$ has measure $0$, then there exists a continuous, monotone function that is differentiable at no point of $E$. The proofs of these results, at least those I have seen, are a bit too technical for first-year calculus students to digest. On the other hand, I'm willing to settle for a much weaker result: Every monotone function is differentiable at some point. Is there an elementary way avoiding all measure theory, and preferably also avoiding Baire's theorem or other topological concepts that won't be familar to most calculus students showing this? Edit: to clarify, I'd like to avoid integrals too. See the motivating example further down. If we have the Riemann integral at our disposal, there are much simpler ways to define $x^y$ which gives differentiability with less effort. If you need to assume continuity to simplify the proof, that's ok. (One possible) motivation Let's try to define exponentiation $x^y$ for $x >0$, $y \in \mathbb{R}$.If $y$ is a positive integer, of course $$ x^n = \underbrace{x \cdot x \cdots x}_{n~\text{times}}. $$ Assuming we have dealt with $q$:th roots of real numbers, the extension to rational exponents is also straight-forward: $$ x^{p/q} = \big(\sqrt[q]{x}\big)^p $$ Finally, it's a little tedious, but not too bad to extend first to negative rational numbers $x^{-r} = 1/x^r$ and finally to real exponents by ""continuity"". Doing all this will give (for a fixed $x$) a continuous monotone function $f(y) = x^y$ satisfying the functional equation $$f(y_1+y_2) = f(y_1)f(y_2).$$ How do we prove that this function $f$ is differentiable? (See Show $\lim\limits_{h\to 0} \frac{(a^h-1)}{h}$ exists without l'Hôpital or even referencing $e$ or natural log for an expanded version of this question.) Among the answers is a clever way to do it using convexity, but I'm still curious if it's possible to give an elementary solution by just exploiting monotonicity. If we can show that $f$ is differentiable at a single point, then the functional equation implies diffentiability everwhere.","It is well-known that every monotone function $f : \mathbb{R} \to \mathbb{R}$ is differentiable almost everywhere (with respect to Lebesgue measure). It is also known if $E$ has measure $0$, then there exists a continuous, monotone function that is differentiable at no point of $E$. The proofs of these results, at least those I have seen, are a bit too technical for first-year calculus students to digest. On the other hand, I'm willing to settle for a much weaker result: Every monotone function is differentiable at some point. Is there an elementary way avoiding all measure theory, and preferably also avoiding Baire's theorem or other topological concepts that won't be familar to most calculus students showing this? Edit: to clarify, I'd like to avoid integrals too. See the motivating example further down. If we have the Riemann integral at our disposal, there are much simpler ways to define $x^y$ which gives differentiability with less effort. If you need to assume continuity to simplify the proof, that's ok. (One possible) motivation Let's try to define exponentiation $x^y$ for $x >0$, $y \in \mathbb{R}$.If $y$ is a positive integer, of course $$ x^n = \underbrace{x \cdot x \cdots x}_{n~\text{times}}. $$ Assuming we have dealt with $q$:th roots of real numbers, the extension to rational exponents is also straight-forward: $$ x^{p/q} = \big(\sqrt[q]{x}\big)^p $$ Finally, it's a little tedious, but not too bad to extend first to negative rational numbers $x^{-r} = 1/x^r$ and finally to real exponents by ""continuity"". Doing all this will give (for a fixed $x$) a continuous monotone function $f(y) = x^y$ satisfying the functional equation $$f(y_1+y_2) = f(y_1)f(y_2).$$ How do we prove that this function $f$ is differentiable? (See Show $\lim\limits_{h\to 0} \frac{(a^h-1)}{h}$ exists without l'Hôpital or even referencing $e$ or natural log for an expanded version of this question.) Among the answers is a clever way to do it using convexity, but I'm still curious if it's possible to give an elementary solution by just exploiting monotonicity. If we can show that $f$ is differentiable at a single point, then the functional equation implies diffentiability everwhere.",,['real-analysis']
1,Can fundamental theorem of algebra for real polynomials be proven without using complex numbers?,Can fundamental theorem of algebra for real polynomials be proven without using complex numbers?,,"Update 24 Nov 2015: It is solved. Please refer to this arXiv paper . For polynomials with real coefficients,  I am trying to prove the following version of fundamental theorem of algebra, which avoids using complex numbers in the proof. Existence of complex roots will be a corollary of this theorem,  if proven successfully. Sorry for the long post. This is my own method and I could not make it shorter right now. Theorem: For every polynomial with real coefficients of order greater than 2,  there exists a quadratic polynomial with real coefficients which factorizes it. Why? Every real odd polynomial can be factored using only one variable, which produces a real root. Odd and even numbers interlace, but real even polynomials cannot always be factorized using one variable. Maybe two variables can factorize them always. My question: In the quotient formalism , if we divide a monic polynomial with positive coefficients with divisor $q(x)=x^{2}-ax-b$, the remainder is of the form $P(a,b)x+Q(a,b)$. I can show that $P(a,b)=0$ has one solution for large positive $b=b2$ and $n-1$ roots for large negative $b=-b1$ using Sturm chain . Equivalently, using Sturm's chain I can show that $Q(a,b)=0$ has two asymptotic solutions for $b=0$ on the lower half $a/b$ plane, as well as $n-2$ solutions for $Q(a,b)=0$ for the large negative $b=-b1$. I am certain that for large negative $b=-b1$, the solutions of $P(a,b)=0$ and $Q(a,b)=0$ interlace. Since the zero contours form connected curves, applying Jordan's curve theorem in the rectangle ABCD, A=[-a1 -b1] and C=[a2 b2] for will force at least once intersection of $P(a,b)=0$ and $Q(a,b)=0$ inside the rectangle, thus proving the theorem. **I have not being able to show the interlacing yet. Can someone please help me with relevant ideas or existing previous work regarding the interlacing? Please open the image in new tab for easier viewing Procedure of my proof: Part 1. Given any monic polynomial of order even order $n$,  shifting it  by $s$ gives the polynomial $$f(x+s)=\sum_{k=0}^{n} \frac{d^{k} f(s) }{k! \ d x^{k}}x^{n-k}=\sum_{k=0}^{n} C^{n}_{k} \ (1+\epsilon(k,s)) \ s^{n-k} \ x^{n-k} \ ....(1)$$ $\lim_{s \to \infty} \epsilon(k,s)=0$ $C^{n}_{k}$ is the permutation formula. Now, if any polynomial has a quadratic factor, any shift of origin still retains the factorization. So it suffices to prove the proposed theorem for transformed polynoials of the form $(1)$, which have positive and increasing coeffcients with decreasing power of $x$. Part 2. We can derive the quotient by repeatedly replacing $x^{2}=ax+b$. For any power $m$, if we write the quotient formula as $x^{m}\equiv p_{m}x+q_{m}$  , we can use the recursion formula $p_{m}=ap_{m-1}+q_{m}$ and $q_{m}=b q_{m-1}$. Using this method to write the quotient formula for $f(x+s)$, we get $$P(a,b)=\sum_{k=1}^{n-1} l_{k} \ g(k,b) \ a^{n-k}$$ $$Q(a,b)=\sum_{k=2}^{n} p_{k} \ h(k,b) \ a^{n-k}$$ $g(k,b)$ is monic polynomial with positive coefficients of $b$ with highest power $ \lfloor {\frac{k}{2}} \rfloor $ $h(k,b)$ is monic polynomial with positive coefficients of $b$ with highest power $ 1+\lfloor {\frac{k}{2}} \rfloor $ $l_{k}$ and $p_{k}$ are positive increasing sequences of $k$ (Which can be ensured by using large $s$ for $f(x+s)$). For sufficiently large positive $b$, applying Sturm chain method directly shows that there is only one root for $P(a,b)=0$. $Q(a,b=0)=Constant \ne 0$. So $Q(a,b=0)=0$ does not have any solution for $a$. However, by using the transformation $b=ma$ and using the recusrion formula, we can show that  $$\lim_{m \to 0} [P(a,m)-n \ (1+\epsilon(n-1,s)) s^{n-1}]  = \lim_{m \to 0} [Q(a,m)- \frac{ \ (1+\epsilon(n,s)) \ s^{n}}{m}]$$  With some work, this directly shows that $Q(a,b)=0$ has two asymptotic solutions along $+a$ and $-a$ in the lower half $a/b$ plane. Using Sturm chain, it can be proved that large negative $b$, $P(a,b)=0$ has $n-1$ roots and  $Q(a,b)=0$ has $n-2$ roots. This happens because the coefficients of the polynomial of $a$ increase in magnitude with decreasing power and changes sign after every second power I have a feeling this is the basic structure behind the Fundamental Theorem of Algebra, since sign change over four consecutive powers is what complex numbers are cpable of producing The vertical sides of the rectangle ABCD can be directly estimated by taking a lower and upper limit of the roots of $P(a,-b1<b<b2)=0$, in presence of interlacing. Part 3. What is left We need to show that for large negative $b$, the $n-1$ roots of $P(a,b)=0$ and  $n-2$ roots of $Q(a,b)=0$ interlace. I am certain it can be used by comparing $l_{k}$ and $p_{k}$ and using the recursion formula. If there is an easy way to prove this from the recursion formula, it will be great. Update: Here is the outline of the proof. $$f(x)=\sum_{k=0}^{n} c_{k} \ x^{n-k} $$ $$g_{0}=P(f(x))=P(\sum_{k=0}^{n} c_{k} \ x^{n-k} )= \sum_{k=0}^{n} c_{k} \ P(x^{n-k}) ....(1)$$ $$ \frac{g_{1}}{b} =\frac{Q(f(x))}{b}=P(\sum_{k=0}^{n-1} c_{k} \ x^{n-k-1} )= \sum_{k=0}^{n-1} c_{k} \ P(x^{n-k-1})+ \frac{c_{0}}{b}....(1)$$ Using $g_{0}$ and $g_{1}$ as the first two entries of of Sturm chain, we get  $$ \frac{-1^{i-1} g_{i}}{b^{i}} \approx \sum_{k=0}^{n-i} c_{k} \ P(x^{n-k-i})....(1)$$ for large $b$. Using the recursion formula between $g_{i-1},g_{i}$ and $g_{i+1}$, the intermediate value theorem and the growth properties of real polynomials, if the roots of $g_{i}$ and $g_{i+1}$ interlace,  it can be shown that $g_{i-1}$ and $g_{i}$ have interlacing roots for large negative $b$. The quadratic and linear terms of $g$ have interlacing roots for large negative $b$. This implies that $P(a,b)$ and $Q(a,b)$ have interlacing roots for large negative $b$. The quadratic term of $g$ does not have a root for large positive $b$.Using the same technique as above, it can be shown that $P(a,b)$ has one root and  $Q(a,b)$ has no root  for large positive $b$. I will follow up with a full write up over the weekend.","Update 24 Nov 2015: It is solved. Please refer to this arXiv paper . For polynomials with real coefficients,  I am trying to prove the following version of fundamental theorem of algebra, which avoids using complex numbers in the proof. Existence of complex roots will be a corollary of this theorem,  if proven successfully. Sorry for the long post. This is my own method and I could not make it shorter right now. Theorem: For every polynomial with real coefficients of order greater than 2,  there exists a quadratic polynomial with real coefficients which factorizes it. Why? Every real odd polynomial can be factored using only one variable, which produces a real root. Odd and even numbers interlace, but real even polynomials cannot always be factorized using one variable. Maybe two variables can factorize them always. My question: In the quotient formalism , if we divide a monic polynomial with positive coefficients with divisor $q(x)=x^{2}-ax-b$, the remainder is of the form $P(a,b)x+Q(a,b)$. I can show that $P(a,b)=0$ has one solution for large positive $b=b2$ and $n-1$ roots for large negative $b=-b1$ using Sturm chain . Equivalently, using Sturm's chain I can show that $Q(a,b)=0$ has two asymptotic solutions for $b=0$ on the lower half $a/b$ plane, as well as $n-2$ solutions for $Q(a,b)=0$ for the large negative $b=-b1$. I am certain that for large negative $b=-b1$, the solutions of $P(a,b)=0$ and $Q(a,b)=0$ interlace. Since the zero contours form connected curves, applying Jordan's curve theorem in the rectangle ABCD, A=[-a1 -b1] and C=[a2 b2] for will force at least once intersection of $P(a,b)=0$ and $Q(a,b)=0$ inside the rectangle, thus proving the theorem. **I have not being able to show the interlacing yet. Can someone please help me with relevant ideas or existing previous work regarding the interlacing? Please open the image in new tab for easier viewing Procedure of my proof: Part 1. Given any monic polynomial of order even order $n$,  shifting it  by $s$ gives the polynomial $$f(x+s)=\sum_{k=0}^{n} \frac{d^{k} f(s) }{k! \ d x^{k}}x^{n-k}=\sum_{k=0}^{n} C^{n}_{k} \ (1+\epsilon(k,s)) \ s^{n-k} \ x^{n-k} \ ....(1)$$ $\lim_{s \to \infty} \epsilon(k,s)=0$ $C^{n}_{k}$ is the permutation formula. Now, if any polynomial has a quadratic factor, any shift of origin still retains the factorization. So it suffices to prove the proposed theorem for transformed polynoials of the form $(1)$, which have positive and increasing coeffcients with decreasing power of $x$. Part 2. We can derive the quotient by repeatedly replacing $x^{2}=ax+b$. For any power $m$, if we write the quotient formula as $x^{m}\equiv p_{m}x+q_{m}$  , we can use the recursion formula $p_{m}=ap_{m-1}+q_{m}$ and $q_{m}=b q_{m-1}$. Using this method to write the quotient formula for $f(x+s)$, we get $$P(a,b)=\sum_{k=1}^{n-1} l_{k} \ g(k,b) \ a^{n-k}$$ $$Q(a,b)=\sum_{k=2}^{n} p_{k} \ h(k,b) \ a^{n-k}$$ $g(k,b)$ is monic polynomial with positive coefficients of $b$ with highest power $ \lfloor {\frac{k}{2}} \rfloor $ $h(k,b)$ is monic polynomial with positive coefficients of $b$ with highest power $ 1+\lfloor {\frac{k}{2}} \rfloor $ $l_{k}$ and $p_{k}$ are positive increasing sequences of $k$ (Which can be ensured by using large $s$ for $f(x+s)$). For sufficiently large positive $b$, applying Sturm chain method directly shows that there is only one root for $P(a,b)=0$. $Q(a,b=0)=Constant \ne 0$. So $Q(a,b=0)=0$ does not have any solution for $a$. However, by using the transformation $b=ma$ and using the recusrion formula, we can show that  $$\lim_{m \to 0} [P(a,m)-n \ (1+\epsilon(n-1,s)) s^{n-1}]  = \lim_{m \to 0} [Q(a,m)- \frac{ \ (1+\epsilon(n,s)) \ s^{n}}{m}]$$  With some work, this directly shows that $Q(a,b)=0$ has two asymptotic solutions along $+a$ and $-a$ in the lower half $a/b$ plane. Using Sturm chain, it can be proved that large negative $b$, $P(a,b)=0$ has $n-1$ roots and  $Q(a,b)=0$ has $n-2$ roots. This happens because the coefficients of the polynomial of $a$ increase in magnitude with decreasing power and changes sign after every second power I have a feeling this is the basic structure behind the Fundamental Theorem of Algebra, since sign change over four consecutive powers is what complex numbers are cpable of producing The vertical sides of the rectangle ABCD can be directly estimated by taking a lower and upper limit of the roots of $P(a,-b1<b<b2)=0$, in presence of interlacing. Part 3. What is left We need to show that for large negative $b$, the $n-1$ roots of $P(a,b)=0$ and  $n-2$ roots of $Q(a,b)=0$ interlace. I am certain it can be used by comparing $l_{k}$ and $p_{k}$ and using the recursion formula. If there is an easy way to prove this from the recursion formula, it will be great. Update: Here is the outline of the proof. $$f(x)=\sum_{k=0}^{n} c_{k} \ x^{n-k} $$ $$g_{0}=P(f(x))=P(\sum_{k=0}^{n} c_{k} \ x^{n-k} )= \sum_{k=0}^{n} c_{k} \ P(x^{n-k}) ....(1)$$ $$ \frac{g_{1}}{b} =\frac{Q(f(x))}{b}=P(\sum_{k=0}^{n-1} c_{k} \ x^{n-k-1} )= \sum_{k=0}^{n-1} c_{k} \ P(x^{n-k-1})+ \frac{c_{0}}{b}....(1)$$ Using $g_{0}$ and $g_{1}$ as the first two entries of of Sturm chain, we get  $$ \frac{-1^{i-1} g_{i}}{b^{i}} \approx \sum_{k=0}^{n-i} c_{k} \ P(x^{n-k-i})....(1)$$ for large $b$. Using the recursion formula between $g_{i-1},g_{i}$ and $g_{i+1}$, the intermediate value theorem and the growth properties of real polynomials, if the roots of $g_{i}$ and $g_{i+1}$ interlace,  it can be shown that $g_{i-1}$ and $g_{i}$ have interlacing roots for large negative $b$. The quadratic and linear terms of $g$ have interlacing roots for large negative $b$. This implies that $P(a,b)$ and $Q(a,b)$ have interlacing roots for large negative $b$. The quadratic term of $g$ does not have a root for large positive $b$.Using the same technique as above, it can be shown that $P(a,b)$ has one root and  $Q(a,b)$ has no root  for large positive $b$. I will follow up with a full write up over the weekend.",,"['real-analysis', 'proof-verification', 'complex-numbers', 'elementary-functions']"
2,The Typewriter Sequence,The Typewriter Sequence,,"The typewriter sequence is an example of a sequence which converges to zero in measure but does not converge to zero a.e. Could someone explain why it does not converge to zero a.e.? $f_n(x) = \mathbb 1_{\left[\frac{n-2^k}{2^k}, \frac{n-2^k+1}{2^k}\right]} \text{, where } 2^k \leqslant n < 2^{k+1}.$ Note: the typewriter sequence (Example 7).",The typewriter sequence is an example of a sequence which converges to zero in measure but does not converge to zero a.e. Could someone explain why it does not converge to zero a.e.? Note: the typewriter sequence (Example 7).,"f_n(x) = \mathbb 1_{\left[\frac{n-2^k}{2^k}, \frac{n-2^k+1}{2^k}\right]} \text{, where } 2^k \leqslant n < 2^{k+1}.","['real-analysis', 'measure-theory', 'convergence-divergence']"
3,Limit question - L'Hopital's rule doesn't seem to work,Limit question - L'Hopital's rule doesn't seem to work,,"I have been recently trying to solve this limit problem. First of all, I used L'Hopital's rule but it doesn't seem to work (because I thought that this limit is of form $\frac{\infty}{\infty}$). Am I doing it correctly? I don't seem to understand where am I wrong. $$\lim_{x \to \infty} \left(\frac{x+\sin^3x}{5x+6}\right)$$","I have been recently trying to solve this limit problem. First of all, I used L'Hopital's rule but it doesn't seem to work (because I thought that this limit is of form $\frac{\infty}{\infty}$). Am I doing it correctly? I don't seem to understand where am I wrong. $$\lim_{x \to \infty} \left(\frac{x+\sin^3x}{5x+6}\right)$$",,"['calculus', 'real-analysis', 'limits', 'trigonometry', 'limits-without-lhopital']"
4,Is there any function which grows 'slower' than its derivative?,Is there any function which grows 'slower' than its derivative?,,"Does a function $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $f'(x) > f(x) > 0$ exist? Intuitively, I think it can't exist. I've tried finding the answer using the definition of derivative: I know that if $\lim_{x \rightarrow k} f(x)$ exists and is finite, then $\lim_{x \rightarrow k} f(x) = \lim_{x \rightarrow k^+} f(x) = \lim_{x \rightarrow k^-} f(x)$ Thanks to this property, I can write: $$\begin{align} & f'(x) > f(x) > 0 \\ & \lim_{h \rightarrow 0^+} \frac{f(x + h) - f(x)}h > f(x) > 0 \\ & \lim_{h \rightarrow 0^+} f(x + h) - f(x) > h f(x) > 0 \\ & \lim_{h \rightarrow 0^+} f(x + h) > (h + 1) f(x) > f(x) \\ & \lim_{h \rightarrow 0^+} \frac{f(x + h)}{f(x)} > h + 1 > 1 \end{align}$$ This leads to the result $1 > 1 > 1$ (or $0 > 0 > 0$ if you stop earlier), which is false. However I guess I made serious mistakes with my proof. I think I've used limits the wrong way. What do you think?","Does a function $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $f'(x) > f(x) > 0$ exist? Intuitively, I think it can't exist. I've tried finding the answer using the definition of derivative: I know that if $\lim_{x \rightarrow k} f(x)$ exists and is finite, then $\lim_{x \rightarrow k} f(x) = \lim_{x \rightarrow k^+} f(x) = \lim_{x \rightarrow k^-} f(x)$ Thanks to this property, I can write: $$\begin{align} & f'(x) > f(x) > 0 \\ & \lim_{h \rightarrow 0^+} \frac{f(x + h) - f(x)}h > f(x) > 0 \\ & \lim_{h \rightarrow 0^+} f(x + h) - f(x) > h f(x) > 0 \\ & \lim_{h \rightarrow 0^+} f(x + h) > (h + 1) f(x) > f(x) \\ & \lim_{h \rightarrow 0^+} \frac{f(x + h)}{f(x)} > h + 1 > 1 \end{align}$$ This leads to the result $1 > 1 > 1$ (or $0 > 0 > 0$ if you stop earlier), which is false. However I guess I made serious mistakes with my proof. I think I've used limits the wrong way. What do you think?",,"['real-analysis', 'limits', 'derivatives']"
5,Why do we define the modulus of a complex number as we do?,Why do we define the modulus of a complex number as we do?,,"For a complex number $z = a+bi$ , we say that its modulus is: $$|z|=\sqrt{a^2+b^2}$$ When we draw complex numbers in the Argand diagram, intuitively, this makes sense. But if we used a different projection for the diagram (i.e. a different metric for distance) then it wouldn't necessarily. Of course, complex numbers can also be written as: $$z = re^{i\theta} = r(\cos\theta +i\sin\theta)$$ so an equivalent question could be, if this is what we define, why we define that: $$|e^{i\theta}| = |\cos\theta + i\sin\theta| = 1$$ for all values of $\theta$ , rather than just $\theta = n\pi$ . The answer may simply be that it is convenient to work with this definition. But is there a deeper reason? Are there any problems for which it is convenient to define things differently? And what would be the consequences if we did things differently?","For a complex number , we say that its modulus is: When we draw complex numbers in the Argand diagram, intuitively, this makes sense. But if we used a different projection for the diagram (i.e. a different metric for distance) then it wouldn't necessarily. Of course, complex numbers can also be written as: so an equivalent question could be, if this is what we define, why we define that: for all values of , rather than just . The answer may simply be that it is convenient to work with this definition. But is there a deeper reason? Are there any problems for which it is convenient to define things differently? And what would be the consequences if we did things differently?",z = a+bi |z|=\sqrt{a^2+b^2} z = re^{i\theta} = r(\cos\theta +i\sin\theta) |e^{i\theta}| = |\cos\theta + i\sin\theta| = 1 \theta \theta = n\pi,"['real-analysis', 'complex-numbers']"
6,$\sqrt{x}$ isn't Lipschitz function,isn't Lipschitz function,\sqrt{x},"A function f such that $$ |f(x)-f(y)| \leq C|x-y| $$ for all $x$ and $y$ , where $C$ is a constant independent of $x$ and $y$ , is called a Lipschitz function show that $f(x)=\sqrt{x}\hspace{3mm} \forall x \in \mathbb{R_{+}}$ isn't Lipschitz function Indeed,  there is no such constant C where $$ |\sqrt{x}-\sqrt{y}| \leq C|x-y| \hspace{4mm} \forall x,y \in \mathbb{R_{+}} $$ we have only that inequality $$ |\sqrt{x}-\sqrt{y}|\leq |\sqrt{x}|+|\sqrt{y}| $$ Am i right ? remark for @Vintarel  i plot it i don't know  graphically ""Lipschitz"" mean?  what is the big deal in the graph of the square-root function in wikipedia they said Continuous functions that are not (globally) Lipschitz continuous The function f(x) = $\sqrt{x}$ defined on [0, 1] is not Lipschitz continuous. This function becomes infinitely steep as x approaches 0 since its derivative becomes infinite. However, it is uniformly continuous as well as Hölder continuous of class $C^{0,\alpha}$ , α for $α ≤ 1/2$ . Reference 1] could someone explain to me this by math and not by words, please ?? 2] what does ""Lipschitz"" mean graphically?","A function f such that for all and , where is a constant independent of and , is called a Lipschitz function show that isn't Lipschitz function Indeed,  there is no such constant C where we have only that inequality Am i right ? remark for @Vintarel  i plot it i don't know  graphically ""Lipschitz"" mean?  what is the big deal in the graph of the square-root function in wikipedia they said Continuous functions that are not (globally) Lipschitz continuous The function f(x) = defined on [0, 1] is not Lipschitz continuous. This function becomes infinitely steep as x approaches 0 since its derivative becomes infinite. However, it is uniformly continuous as well as Hölder continuous of class , α for . Reference 1] could someone explain to me this by math and not by words, please ?? 2] what does ""Lipschitz"" mean graphically?","
|f(x)-f(y)| \leq C|x-y|
 x y C x y f(x)=\sqrt{x}\hspace{3mm} \forall x \in \mathbb{R_{+}} 
|\sqrt{x}-\sqrt{y}| \leq C|x-y| \hspace{4mm} \forall x,y \in \mathbb{R_{+}}
 
|\sqrt{x}-\sqrt{y}|\leq |\sqrt{x}|+|\sqrt{y}|
 \sqrt{x} C^{0,\alpha} α ≤ 1/2","['real-analysis', 'analysis', 'continuity', 'holder-spaces', 'lipschitz-functions']"
7,$f(a+b)=f(a)+f(b)$ but $f$ is not linear,but  is not linear,f(a+b)=f(a)+f(b) f,Can you show me a continuous function $f \colon \mathbb{R}^n\to\mathbb{R}^m$ that satisfies $f(a+b)=f(a)+f(b)$ but is not linear? We have that  $$f(0)=f(0+0)=2f(0)\implies f(0)=0\\ f(x-x)=f(0)=f(x)+f(-x)=0\implies f(-x)=-f(x)\\ f(nx)=f(x+x+\dots+x)=f(x)+\dots+f(x)=nf(x)\quad \forall n \in \mathbb{N}$$ But $$ f(-nx)=-f(nx)=-nf(x) $$ So: $$ f(ax)=af(x) \quad \forall a \in \mathbb{Z} $$,Can you show me a continuous function $f \colon \mathbb{R}^n\to\mathbb{R}^m$ that satisfies $f(a+b)=f(a)+f(b)$ but is not linear? We have that  $$f(0)=f(0+0)=2f(0)\implies f(0)=0\\ f(x-x)=f(0)=f(x)+f(-x)=0\implies f(-x)=-f(x)\\ f(nx)=f(x+x+\dots+x)=f(x)+\dots+f(x)=nf(x)\quad \forall n \in \mathbb{N}$$ But $$ f(-nx)=-f(nx)=-nf(x) $$ So: $$ f(ax)=af(x) \quad \forall a \in \mathbb{Z} $$,,"['real-analysis', 'linear-algebra', 'abstract-algebra', 'functional-equations']"
8,Does strict convexity imply differentiability?,Does strict convexity imply differentiability?,,"I know that convexity does not imply differentiability, for example f(x)=|x| is convex but not differentiable. However, |x| is not strictly convex. So I wonder whether strict convexity imply differentiability. I did some search and found out the Wikipedia implicitly gives the negative answer: http://en.wikipedia.org/wiki/Convex_function#Strongly_convex_functions It says that ""a strongly convex function is also strictly convex"" and ""a function doesn't have to be differentiable in order to be strongly convex"". Can anyone provide a concrete example? Thanks in advance.","I know that convexity does not imply differentiability, for example f(x)=|x| is convex but not differentiable. However, |x| is not strictly convex. So I wonder whether strict convexity imply differentiability. I did some search and found out the Wikipedia implicitly gives the negative answer: http://en.wikipedia.org/wiki/Convex_function#Strongly_convex_functions It says that ""a strongly convex function is also strictly convex"" and ""a function doesn't have to be differentiable in order to be strongly convex"". Can anyone provide a concrete example? Thanks in advance.",,['real-analysis']
9,u-substitution shows 0=1,u-substitution shows 0=1,,"This question/observation is inspired by the integral: $$\int_0^{\sqrt{\pi}}x\sin(x^2)\cos(x^2)dx$$ The $u$ -substitution $u=\sin(x^2)$ yields $du=2x\cos(x^2)dx$ and $$\int_0^{\sqrt{\pi}}x\sin(x^2)\cos(x^2)dx=\frac{1}{2}\int_0^0 u du=0,$$ right? Wolframalpha certainly agrees. Great, now consider the much harder integral $$\int_0^1 dx.$$ After banging our heads against the wall for hours we take $u=x^2-x$ so $du=(2x-1)dx=\pm\sqrt{1+4u}*dx$ by the quadratic formula, so $$\int_0^1 dx=\int_0^0\frac{du}{\pm\sqrt{1+4u}}=0$$ The problem is wolframalpha says this integral should be $1$ . Well, I guess it is a unit square. Since $0\neq 1$ there is something fishy going on - I'm just trying to fully nail down the issue here. I think it boils down to hidden division by $0$ like most false proofs. In particular we can't solve the equation $du=(2x-1)dx$ for $dx$ if $x=\frac{1}{2}$ , which happens inside the domain of integration. This isn't a problem for the original integral because even though there is a place where $\frac{du}{dx}=0$ inside the domain of integration $(x=\sqrt{\frac{\pi}{2}})$ there is no issue because we don't have to divide by this expression to make all the $x$ s cancel. Anyway, I'm curious for further explanation and to know if there are any references which carefully explain subtleties such as this for integration by substitution.","This question/observation is inspired by the integral: The -substitution yields and right? Wolframalpha certainly agrees. Great, now consider the much harder integral After banging our heads against the wall for hours we take so by the quadratic formula, so The problem is wolframalpha says this integral should be . Well, I guess it is a unit square. Since there is something fishy going on - I'm just trying to fully nail down the issue here. I think it boils down to hidden division by like most false proofs. In particular we can't solve the equation for if , which happens inside the domain of integration. This isn't a problem for the original integral because even though there is a place where inside the domain of integration there is no issue because we don't have to divide by this expression to make all the s cancel. Anyway, I'm curious for further explanation and to know if there are any references which carefully explain subtleties such as this for integration by substitution.","\int_0^{\sqrt{\pi}}x\sin(x^2)\cos(x^2)dx u u=\sin(x^2) du=2x\cos(x^2)dx \int_0^{\sqrt{\pi}}x\sin(x^2)\cos(x^2)dx=\frac{1}{2}\int_0^0 u du=0, \int_0^1 dx. u=x^2-x du=(2x-1)dx=\pm\sqrt{1+4u}*dx \int_0^1 dx=\int_0^0\frac{du}{\pm\sqrt{1+4u}}=0 1 0\neq 1 0 du=(2x-1)dx dx x=\frac{1}{2} \frac{du}{dx}=0 (x=\sqrt{\frac{\pi}{2}}) x","['real-analysis', 'calculus', 'integration', 'differential-forms', 'fake-proofs']"
10,"A ""non-trivial"" example of a Cauchy sequence that does not converge?","A ""non-trivial"" example of a Cauchy sequence that does not converge?",,"A Cauchy sequence doesn't necessarily converge, e.g. take the sequence $(1/n)$ in the space $(0,1)$. Maybe my intuition is wrong but I tend to think of this as, ""it does converge but what it converges to is not in the space"".  Are there any examples of a Cauchy sequence that does not converge and avoids this type of saying?","A Cauchy sequence doesn't necessarily converge, e.g. take the sequence $(1/n)$ in the space $(0,1)$. Maybe my intuition is wrong but I tend to think of this as, ""it does converge but what it converges to is not in the space"".  Are there any examples of a Cauchy sequence that does not converge and avoids this type of saying?",,"['real-analysis', 'general-topology', 'convergence-divergence', 'cauchy-sequences']"
11,Do the Liouville Numbers form a field?,Do the Liouville Numbers form a field?,,"The Liouville numbers are those which are better-than-polynomially approximated by rationals. More precisely, we say $x\in\mathbb{R}$ is Liouville when for all $n\in\mathbb{N}$ there is a $\tfrac pq\in\mathbb{Q}$ with $$\left|x-\frac{p}{q}\right|<\frac{1}{q^n}.$$ For the purposes of this question, we will take rational numbers to be Liouville. Do the Liouville numbers form a field? It seems to me that if $x\simeq \tfrac pq$ and $x'\simeq \tfrac {p'}{q'}$ then $x+x',x-x',xx'$ and $x/x'$ are approximated by $\tfrac pq+\tfrac {p'}{q'},\tfrac pq-\tfrac {p'}{q'},\tfrac pq\tfrac {p'}{q'}$ and $\tfrac pq/\tfrac {p'}{q'}$, with the approximation only getting quadratically worse in each case. But I've never seen it mentioned anywhere that the Liouville numbers form a field, and you would think that if they were the wikipedia page would at least mention it. So are they or not?","The Liouville numbers are those which are better-than-polynomially approximated by rationals. More precisely, we say $x\in\mathbb{R}$ is Liouville when for all $n\in\mathbb{N}$ there is a $\tfrac pq\in\mathbb{Q}$ with $$\left|x-\frac{p}{q}\right|<\frac{1}{q^n}.$$ For the purposes of this question, we will take rational numbers to be Liouville. Do the Liouville numbers form a field? It seems to me that if $x\simeq \tfrac pq$ and $x'\simeq \tfrac {p'}{q'}$ then $x+x',x-x',xx'$ and $x/x'$ are approximated by $\tfrac pq+\tfrac {p'}{q'},\tfrac pq-\tfrac {p'}{q'},\tfrac pq\tfrac {p'}{q'}$ and $\tfrac pq/\tfrac {p'}{q'}$, with the approximation only getting quadratically worse in each case. But I've never seen it mentioned anywhere that the Liouville numbers form a field, and you would think that if they were the wikipedia page would at least mention it. So are they or not?",,"['real-analysis', 'field-theory', 'transcendental-numbers']"
12,Olympiad calculus problem,Olympiad calculus problem,,"This problem is from a qualifying round in a Colombian math Olympiad, I thought some time about it but didn't make any progress. It is as follows. Given a continuous function $f : [0,1] \to \mathbb{R}$ such that $$\int_0^1{f(x)\, dx} = 0$$ Prove that there exists $c \in (0,1) $ such that $$\int_0^c{xf(x) \, dx} = 0$$ I will appreciate any help with it.","This problem is from a qualifying round in a Colombian math Olympiad, I thought some time about it but didn't make any progress. It is as follows. Given a continuous function $f : [0,1] \to \mathbb{R}$ such that $$\int_0^1{f(x)\, dx} = 0$$ Prove that there exists $c \in (0,1) $ such that $$\int_0^c{xf(x) \, dx} = 0$$ I will appreciate any help with it.",,"['calculus', 'real-analysis', 'integration', 'contest-math']"
13,For each $a \in \mathbb{R}$ evaluate $ \lim\limits_{n \to \infty}\left(\begin{smallmatrix}1&\frac{a}{n}\\\frac{-a}{n}&1\end{smallmatrix}\right)^n$,For each  evaluate,a \in \mathbb{R}  \lim\limits_{n \to \infty}\left(\begin{smallmatrix}1&\frac{a}{n}\\\frac{-a}{n}&1\end{smallmatrix}\right)^n,"If $a \in \mathbb{R}$ , evaluate $$ \lim_{n \to \infty}\left(\begin{matrix} 1&\frac{a}{n}\\\frac{-a}{n}&1\end{matrix}\right)^{n}$$ My attempt: Let $$A = \left(\begin{matrix} 0&a\\-a&0\end{matrix}\right) = -a\left(\begin{matrix} \cos(\frac{\pi}{2})&-\sin(\frac{\pi}{2})\\\sin(\frac{\pi}{2})&\cos(\frac{\pi}{2})\end{matrix}\right)$$ so that $$A^k = (-a)^k \left(\begin{matrix} \cos(\frac{k\pi}{2})&-\sin(\frac{k\pi}{2})\\\sin(\frac{k\pi}{2})&\cos(\frac{k\pi}{2})\end{matrix}\right)$$ Thus, \begin{align}\displaystyle \lim_{n \to \infty}\left(\begin{matrix} 1&\dfrac{a}{n}\\\dfrac{-a}{n}&1\end{matrix}\right)^{n} &=\displaystyle \lim_{n \to \infty} \left(I+\dfrac{A}{n}\right)^n =e^A=\displaystyle \sum_{k=0}^{\infty}\dfrac{A^k}{k!}\\&= \sum_{k=0}^{\infty} \left(\begin{matrix} \dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}&-\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}\\\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}&\dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}\end{matrix}\right) \end{align} and since $\displaystyle \sum_{k=0}^{\infty}\dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}=1+0-\dfrac{a^2}{2!}+0+\dfrac{a^4}{4!}+\cdots= \cos a$ and $\displaystyle \sum_{k=0}^{\infty}\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}=0-a+0+\dfrac{a^3}{3!}+0-\dfrac{a^5}{5!}+\cdots= -\sin a$ therefore the required answer is $\left(\begin{matrix} \cos a&\sin a\\-\sin a&\cos a\end{matrix}\right).$ However the above answer does not match the choices provided which are $I, 0$ and none of the above. So my question is: Is my answer correct?","If , evaluate My attempt: Let so that Thus, and since and therefore the required answer is However the above answer does not match the choices provided which are and none of the above. So my question is: Is my answer correct?","a \in \mathbb{R}  \lim_{n \to \infty}\left(\begin{matrix} 1&\frac{a}{n}\\\frac{-a}{n}&1\end{matrix}\right)^{n} A = \left(\begin{matrix} 0&a\\-a&0\end{matrix}\right) = -a\left(\begin{matrix} \cos(\frac{\pi}{2})&-\sin(\frac{\pi}{2})\\\sin(\frac{\pi}{2})&\cos(\frac{\pi}{2})\end{matrix}\right) A^k = (-a)^k \left(\begin{matrix} \cos(\frac{k\pi}{2})&-\sin(\frac{k\pi}{2})\\\sin(\frac{k\pi}{2})&\cos(\frac{k\pi}{2})\end{matrix}\right) \begin{align}\displaystyle \lim_{n \to \infty}\left(\begin{matrix} 1&\dfrac{a}{n}\\\dfrac{-a}{n}&1\end{matrix}\right)^{n} &=\displaystyle \lim_{n \to \infty} \left(I+\dfrac{A}{n}\right)^n =e^A=\displaystyle \sum_{k=0}^{\infty}\dfrac{A^k}{k!}\\&= \sum_{k=0}^{\infty} \left(\begin{matrix} \dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}&-\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}\\\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}&\dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}\end{matrix}\right) \end{align} \displaystyle \sum_{k=0}^{\infty}\dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}=1+0-\dfrac{a^2}{2!}+0+\dfrac{a^4}{4!}+\cdots= \cos a \displaystyle \sum_{k=0}^{\infty}\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}=0-a+0+\dfrac{a^3}{3!}+0-\dfrac{a^5}{5!}+\cdots= -\sin a \left(\begin{matrix} \cos a&\sin a\\-\sin a&\cos a\end{matrix}\right). I, 0","['calculus', 'real-analysis', 'matrices', 'analysis', 'limits']"
14,An integration of product $(1-x^n)$,An integration of product,(1-x^n),"Prove $$\int_0^1\prod_{n=1}^\infty(1-x^n)dx=\frac{4\pi\sqrt3}{\sqrt{23}}\frac{\sinh\frac{\pi\sqrt{23}}3}{\cosh\frac{\pi\sqrt{23}}2}.$$ In fact, product $(1-x^n)$ is difficult to compute, I hope you can show me some ideas, thank you!","Prove $$\int_0^1\prod_{n=1}^\infty(1-x^n)dx=\frac{4\pi\sqrt3}{\sqrt{23}}\frac{\sinh\frac{\pi\sqrt{23}}3}{\cosh\frac{\pi\sqrt{23}}2}.$$ In fact, product $(1-x^n)$ is difficult to compute, I hope you can show me some ideas, thank you!",,"['calculus', 'real-analysis', 'integration', 'products']"
15,Infinite Product $\prod\limits_{k=1}^\infty\left({1-\frac{x^2}{k^2\pi^2}}\right)$,Infinite Product,\prod\limits_{k=1}^\infty\left({1-\frac{x^2}{k^2\pi^2}}\right),"I've been looking at proofs of Euler's sine expansion, that is  $$\frac{\sin x}{x}=\prod_{k=1}^\infty\left({1-\frac{x^2}{k^2\pi^2}}\right)$$ All the proofs seem to rely on complex analysis and Fourier series. Is there any more elementary proof ?","I've been looking at proofs of Euler's sine expansion, that is  $$\frac{\sin x}{x}=\prod_{k=1}^\infty\left({1-\frac{x^2}{k^2\pi^2}}\right)$$ All the proofs seem to rely on complex analysis and Fourier series. Is there any more elementary proof ?",,"['real-analysis', 'sequences-and-series', 'analysis', 'trigonometry', 'infinite-product']"
16,Sigma algebra generated by the set of all singletons,Sigma algebra generated by the set of all singletons,,"Let $S=\{\{x\}\mid x \in \mathbb{R}\}$. Is $\sigma(S)$ included in the Borel $\sigma$-algebra on $\mathbb{R}$? Is $\sigma(S)$ equivalent to the Borel $\sigma$-algbrea on $\mathbb{R}$? I think the answer to the first question is yes. Because the Borel sets include singletons, the Borel $\sigma$-algebra must contain the smallest sigma-algebra of the set of all singletons. Here's my attempt at the second question: my answer would be no. Countable unions (or intersections) of countable sets are countable, so that any $X \in \sigma(S)$ is either countable or a compliment of a countable set. Therefore, the interval $(0,1) \not \in \sigma (S)$ but it is included in the Borel $\sigma$-algebra. My issue here is that I don't know how to formalize this statement, if it is even correct.","Let $S=\{\{x\}\mid x \in \mathbb{R}\}$. Is $\sigma(S)$ included in the Borel $\sigma$-algebra on $\mathbb{R}$? Is $\sigma(S)$ equivalent to the Borel $\sigma$-algbrea on $\mathbb{R}$? I think the answer to the first question is yes. Because the Borel sets include singletons, the Borel $\sigma$-algebra must contain the smallest sigma-algebra of the set of all singletons. Here's my attempt at the second question: my answer would be no. Countable unions (or intersections) of countable sets are countable, so that any $X \in \sigma(S)$ is either countable or a compliment of a countable set. Therefore, the interval $(0,1) \not \in \sigma (S)$ but it is included in the Borel $\sigma$-algebra. My issue here is that I don't know how to formalize this statement, if it is even correct.",,"['real-analysis', 'measure-theory']"
17,"Evaluation of $\int_0^{\pi/4} \sqrt{\tan x} \sqrt{1-\tan x}\,\,dx$",Evaluation of,"\int_0^{\pi/4} \sqrt{\tan x} \sqrt{1-\tan x}\,\,dx","How to evaluate the following integral $$\int_0^{\pi/4} \sqrt{\tan x} \sqrt{1-\tan x}\,\,dx$$ It looks like beta function but Wolfram Alpha cannot evaluate it. So, I computed the numerical value of integral above to 70 digits using Wolfram Alpha and I used the result to find its closed-form. The possible candidate closed-form from Wolfram Alpha is $$\pi\sqrt{\frac{1+\sqrt{2}}{2}}-\pi$$ Is this true? If so, how to prove it?","How to evaluate the following integral $$\int_0^{\pi/4} \sqrt{\tan x} \sqrt{1-\tan x}\,\,dx$$ It looks like beta function but Wolfram Alpha cannot evaluate it. So, I computed the numerical value of integral above to 70 digits using Wolfram Alpha and I used the result to find its closed-form. The possible candidate closed-form from Wolfram Alpha is $$\pi\sqrt{\frac{1+\sqrt{2}}{2}}-\pi$$ Is this true? If so, how to prove it?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'closed-form']"
18,Show that $\int_0^\pi \log^2\left(\tan\frac{ x}{4}\right)dx=\frac{\pi^3}{4}.$,Show that,\int_0^\pi \log^2\left(\tan\frac{ x}{4}\right)dx=\frac{\pi^3}{4}.,"Hi I am trying to prove the relation $$ I:=\int_0^\pi \log^2\left(\tan\frac{ x}{4}\right)dx=\frac{\pi^3}{4}. $$ I tried expanding the log argument by using $\sin x/ \cos x=\tan x,$ and than used $\log(a/b)=\log a-\log b$, I get $$ I=\int_0^\pi \left( \log \sin \frac{x}{4}-\log\cos \frac{x}{4}\right)^2dx. $$ We can distribute this out  $$ \int_0^\pi \log^2 \sin \frac{x}{4}dx +\int_0^\pi \log^2\cos \frac{x}{4}dx-2\int_0^\pi\log \sin \frac{x}{4}\log \cos \frac{x}{4}dx. $$ Now I am stuck at how to solve these.  Thanks.","Hi I am trying to prove the relation $$ I:=\int_0^\pi \log^2\left(\tan\frac{ x}{4}\right)dx=\frac{\pi^3}{4}. $$ I tried expanding the log argument by using $\sin x/ \cos x=\tan x,$ and than used $\log(a/b)=\log a-\log b$, I get $$ I=\int_0^\pi \left( \log \sin \frac{x}{4}-\log\cos \frac{x}{4}\right)^2dx. $$ We can distribute this out  $$ \int_0^\pi \log^2 \sin \frac{x}{4}dx +\int_0^\pi \log^2\cos \frac{x}{4}dx-2\int_0^\pi\log \sin \frac{x}{4}\log \cos \frac{x}{4}dx. $$ Now I am stuck at how to solve these.  Thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
19,Difference Between Limit Point and Accumulation Point?,Difference Between Limit Point and Accumulation Point?,,"I want to clarify the definition of limit point and accumulation point. According to many of my text books they are synonymous that is $x$ is a limit/accumulation point of set $A$ if open ball $B(x, r)$ contains an an element of $A$ distinct from $x$ . But from one of the problems in Aksoy: A Problem Book in Real Analysis says: Show that if $x \in (M,d)$ is an accumulation point of $A$ , then $x$ is a limit point of $A$ . Is the converse true? So what is the definition?","I want to clarify the definition of limit point and accumulation point. According to many of my text books they are synonymous that is is a limit/accumulation point of set if open ball contains an an element of distinct from . But from one of the problems in Aksoy: A Problem Book in Real Analysis says: Show that if is an accumulation point of , then is a limit point of . Is the converse true? So what is the definition?","x A B(x, r) A x x \in (M,d) A x A",['real-analysis']
20,A smooth function's domain of being non-analytic,A smooth function's domain of being non-analytic,,"I am wondering how much a smooth function may be non-analytic, because in proofs, whilst there non-analytic smooth functions, it would suffice if a smooth function were analytic on only a ""small set"". More exactly: Let $U \in \mathbb R^n$ be open, $C^\infty = C^\infty(U,\mathbb R)$. A smooth function in $C^\infty$ is analytic in $a \in U$, iff there exists $\epsilon > 0$, s.t. the function is equal to its own Taylor series in $B_\epsilon(a)$. There exist smooth functions that are non-analytic, i.e. there exists $f \in C^\infty, b \in U, \epsilon > 0$ s.t. the function is not its taylor series at $x$ in $B_\epsilon (b)$. Let $A$ be the union of all $\epsilon$-Balls in $U$ where $f$ is analytic. By definition, $A$ is open. It's complement $C = A^c$is the closed set of points where $f$ is non-analytic. Does $C$ have an interior?","I am wondering how much a smooth function may be non-analytic, because in proofs, whilst there non-analytic smooth functions, it would suffice if a smooth function were analytic on only a ""small set"". More exactly: Let $U \in \mathbb R^n$ be open, $C^\infty = C^\infty(U,\mathbb R)$. A smooth function in $C^\infty$ is analytic in $a \in U$, iff there exists $\epsilon > 0$, s.t. the function is equal to its own Taylor series in $B_\epsilon(a)$. There exist smooth functions that are non-analytic, i.e. there exists $f \in C^\infty, b \in U, \epsilon > 0$ s.t. the function is not its taylor series at $x$ in $B_\epsilon (b)$. Let $A$ be the union of all $\epsilon$-Balls in $U$ where $f$ is analytic. By definition, $A$ is open. It's complement $C = A^c$is the closed set of points where $f$ is non-analytic. Does $C$ have an interior?",,"['real-analysis', 'analysis']"
21,Compactness of a metric space,Compactness of a metric space,,"If a metric space $(X,d)$ is compact then for every equivalent metric $\sigma$ , $(X,\sigma)$ is complete. This is because, for any Cauchy sequence in $(X,\sigma)$ has a convergent subsequence due to fact $(X,\sigma)$ is a compact metric space, hence original sequence is convergent. My question is, does the converse also hold ? In other words, let $(X,d)$ be a metric space such that for every equivalent metric $\sigma$ on $X$ is complete. Does this imply $(X,d)$ is compact?","If a metric space is compact then for every equivalent metric , is complete. This is because, for any Cauchy sequence in has a convergent subsequence due to fact is a compact metric space, hence original sequence is convergent. My question is, does the converse also hold ? In other words, let be a metric space such that for every equivalent metric on is complete. Does this imply is compact?","(X,d) \sigma (X,\sigma) (X,\sigma) (X,\sigma) (X,d) \sigma X (X,d)","['real-analysis', 'general-topology', 'metric-spaces', 'compactness', 'complete-spaces']"
22,Calculate the Fourier transform of $\log |x| $,Calculate the Fourier transform of,\log |x| ,"How can one prove that the Fourier transform of $\log |x|$ is  $$-\pi \mathrm{pf} \frac{1}{|\xi|} +C \delta,$$ where $\mathrm{pf}\frac{1}{|x|} = D(\mathrm{sign}(x)\log|x|)$ (in the sense of distributions) and how can I compute the constant $C$?","How can one prove that the Fourier transform of $\log |x|$ is  $$-\pi \mathrm{pf} \frac{1}{|\xi|} +C \delta,$$ where $\mathrm{pf}\frac{1}{|x|} = D(\mathrm{sign}(x)\log|x|)$ (in the sense of distributions) and how can I compute the constant $C$?",,"['calculus', 'real-analysis', 'integration', 'functional-analysis', 'fourier-analysis']"
23,Prove that the number of jump discontinuities is countable for any function,Prove that the number of jump discontinuities is countable for any function,,I would like to prove that the number of simple jump discontinuities of any function is countable. Can someone point me some material where the proof is or explain the proof here? Thanks.,I would like to prove that the number of simple jump discontinuities of any function is countable. Can someone point me some material where the proof is or explain the proof here? Thanks.,,['real-analysis']
24,"What are the zero divisors of $C[0,1]$?",What are the zero divisors of ?,"C[0,1]","Suppose you have a ring $(C[0,1],+,\cdot,0,1)$ of continuous real valued functions on $[0,1]$, with addition defined as $(f+g)(x)=f(x)+g(x)$ and multiplication defined as $(fg)(x)=f(x)g(x)$. I'm curious what the zero divisors are. My hunch is that the zero divisors are precisely the functions whose zero set contains an open interval. My thinking is that if $f$ is a function which is at least zero on an open interval $(a,b)$, then there exists some function which is nonzero on $(a,b)$, but zero everywhere else on $[0,1]\setminus(a,b)$. Conversely, if $f$ is not zero on any open interval, then every zero is isolated in a sense. But if $fg=0$ for some $g$, then $g$ is zero everywhere except these isolated points, but continuity would imply that it is also zero at the zeros of $f$, but then $g=0$, so $f$ is not a zero divisor. I have a hard time stating this formally though, since I'm only studying algebra, and not analysis. Is this intuition correct, and if so, how could it be rigorously expressed?","Suppose you have a ring $(C[0,1],+,\cdot,0,1)$ of continuous real valued functions on $[0,1]$, with addition defined as $(f+g)(x)=f(x)+g(x)$ and multiplication defined as $(fg)(x)=f(x)g(x)$. I'm curious what the zero divisors are. My hunch is that the zero divisors are precisely the functions whose zero set contains an open interval. My thinking is that if $f$ is a function which is at least zero on an open interval $(a,b)$, then there exists some function which is nonzero on $(a,b)$, but zero everywhere else on $[0,1]\setminus(a,b)$. Conversely, if $f$ is not zero on any open interval, then every zero is isolated in a sense. But if $fg=0$ for some $g$, then $g$ is zero everywhere except these isolated points, but continuity would imply that it is also zero at the zeros of $f$, but then $g=0$, so $f$ is not a zero divisor. I have a hard time stating this formally though, since I'm only studying algebra, and not analysis. Is this intuition correct, and if so, how could it be rigorously expressed?",,"['real-analysis', 'abstract-algebra', 'ring-theory']"
25,When can the order of limit and integral be exchanged?,When can the order of limit and integral be exchanged?,,"I was wondering for a  real-valued function with two real variables, if there are some theorems/conclusions that can be used to decide  the exchangeability of the order of taking limit wrt one variable and taking integral (Riemann integral, or even more generally Lebesgue integral ) wrt another variable, like $$\lim_{y\rightarrow a} \int_A f(x,y) \, dx = \int_A \lim_{y\rightarrow a} f(x,y) \,dx \text{ ?}$$ If $y$ approaches $a$ as a countable sequence $\{y_n, n\in \mathbb{N}\}$, is the order exchangeable  when $f(x,y_n), n \in \mathbb{N}$ is uniformly convergent in some subset for $x$ and $y$? How shall one tell if the limit and integral can be exchanged in the following examples? If not, how would you compute the values of the integrals: $$\lim_{y\rightarrow 3} \int_1^2 x^y \, dx$$ $$ \lim_{y\rightarrow \infty} \int_1^2 \frac{e^{-xy}}{x} \, dx$$ Thanks and regards!","I was wondering for a  real-valued function with two real variables, if there are some theorems/conclusions that can be used to decide  the exchangeability of the order of taking limit wrt one variable and taking integral (Riemann integral, or even more generally Lebesgue integral ) wrt another variable, like $$\lim_{y\rightarrow a} \int_A f(x,y) \, dx = \int_A \lim_{y\rightarrow a} f(x,y) \,dx \text{ ?}$$ If $y$ approaches $a$ as a countable sequence $\{y_n, n\in \mathbb{N}\}$, is the order exchangeable  when $f(x,y_n), n \in \mathbb{N}$ is uniformly convergent in some subset for $x$ and $y$? How shall one tell if the limit and integral can be exchanged in the following examples? If not, how would you compute the values of the integrals: $$\lim_{y\rightarrow 3} \int_1^2 x^y \, dx$$ $$ \lim_{y\rightarrow \infty} \int_1^2 \frac{e^{-xy}}{x} \, dx$$ Thanks and regards!",,"['real-analysis', 'measure-theory', 'convergence-divergence', 'integration']"
26,Archimedean Property and Real Numbers,Archimedean Property and Real Numbers,,"I have few confusions: a) What exactly is Archimedean Property. What does infinitesimal and infinite numbers do not exist in Archimedian ordered fields mean? Are not 0 and infinity such numbers? b) What are the surreal numbers? Do they have anything to do with extended real numbers? I mean real numbers and positive and negative infinity. Rudin introduces extended real numbers with these two additional numbers. Does it mean in the field of reals, infinite means undefined and in extended, infinite means defined? Does this mean extended real numbers are not Archimedian ? Thank You.","I have few confusions: a) What exactly is Archimedean Property. What does infinitesimal and infinite numbers do not exist in Archimedian ordered fields mean? Are not 0 and infinity such numbers? b) What are the surreal numbers? Do they have anything to do with extended real numbers? I mean real numbers and positive and negative infinity. Rudin introduces extended real numbers with these two additional numbers. Does it mean in the field of reals, infinite means undefined and in extended, infinite means defined? Does this mean extended real numbers are not Archimedian ? Thank You.",,"['real-analysis', 'analysis']"
27,Accumulation points of uncountable sets,Accumulation points of uncountable sets,,"Given any uncountable subset $S$ of the unit interval. Then $S$ clearly has an accumulation point and indeed uncountably many (which might also be a nice exercise). So my question is: Is there an accumulation point, that again lies in $S$?","Given any uncountable subset $S$ of the unit interval. Then $S$ clearly has an accumulation point and indeed uncountably many (which might also be a nice exercise). So my question is: Is there an accumulation point, that again lies in $S$?",,"['real-analysis', 'general-topology']"
28,Extending a function by continuity from a dense subset of a space,Extending a function by continuity from a dense subset of a space,,"I am given two spaces $X$ and $Y$, both Hausdorff. I have defined a uniformly continuous function on a dense set $D$ of $X$ that goes to $Y$. So once you have defined a function on a dense subset, if you would like to extend this function to the whole set in a continuous manner, then there is at most one way to do this. For the extension to be sequentially continuous, all decisions are ""already made"" by determining the value of the function on the dense subset. The value of $f(x)$ simply must be $\lim f(x_{n})$ where $x_{n}$ are in $D$ and converge to $x$. I proved well defined and unique. I am having trouble proving this is continuous, any help is appreciated. Thanks.. And this is embarrassing, but can someone tell me how to accept an answer. Sorry...","I am given two spaces $X$ and $Y$, both Hausdorff. I have defined a uniformly continuous function on a dense set $D$ of $X$ that goes to $Y$. So once you have defined a function on a dense subset, if you would like to extend this function to the whole set in a continuous manner, then there is at most one way to do this. For the extension to be sequentially continuous, all decisions are ""already made"" by determining the value of the function on the dense subset. The value of $f(x)$ simply must be $\lim f(x_{n})$ where $x_{n}$ are in $D$ and converge to $x$. I proved well defined and unique. I am having trouble proving this is continuous, any help is appreciated. Thanks.. And this is embarrassing, but can someone tell me how to accept an answer. Sorry...",,['real-analysis']
29,"If $f'(x) = 0$ for all $x \in \mathbb{Q}$, is $f$ constant? [duplicate]","If  for all , is  constant? [duplicate]",f'(x) = 0 x \in \mathbb{Q} f,"This question already has an answer here : Let $f:\mathbb{R}\longrightarrow \mathbb{R}$ a differentiable function such that $f'(x)=0$ for all $x\in\mathbb{Q}$ [closed] (1 answer) Closed 7 years ago . Let $f$ be a differentiable function on $\mathbb{R}$ such that $$f'(x)=0,\forall x\in \mathbb Q.$$ Prove or disprove that $$f(x)=c$$ for some constant $c$. I've heard this problem is true, but I'm not sure. Can you prove it or provide a counterexample? Auxiliary question: I wonder the converse of it, if $f′=0$ for all irrational, can we say that $f$ is constant?","This question already has an answer here : Let $f:\mathbb{R}\longrightarrow \mathbb{R}$ a differentiable function such that $f'(x)=0$ for all $x\in\mathbb{Q}$ [closed] (1 answer) Closed 7 years ago . Let $f$ be a differentiable function on $\mathbb{R}$ such that $$f'(x)=0,\forall x\in \mathbb Q.$$ Prove or disprove that $$f(x)=c$$ for some constant $c$. I've heard this problem is true, but I'm not sure. Can you prove it or provide a counterexample? Auxiliary question: I wonder the converse of it, if $f′=0$ for all irrational, can we say that $f$ is constant?",,"['calculus', 'real-analysis', 'analysis']"
30,What do the cosets of $\mathbb{R} / \mathbb{Q}$ look like?,What do the cosets of  look like?,\mathbb{R} / \mathbb{Q},"$\newcommand{\R}{\Bbb R}\newcommand{\Q}{\Bbb Q}$ Looking at the group of real numbers under addition $(\R, +)$ it contains the (normal) subgroup of rational numbers $(\Q, +)$. I am wondering how to describe the cosets of $\R / \Q$. I know from looking at the cardinality of the sets that because $\R$ is uncountable and $\Q$ is countable that $\R / \Q$ is uncountable. I am also thinking of $\R / \Q$ containing a ""representative"" of each irrational number. I am also aware that $\Q$ is dense in $\R$, so that each member of $\R$ is the limit of a sequence of numbers in $\Q$. Both $\R$ and $\Q$ are ordered. But is there a natural order on $\R / \Q$? What else can we determine about  $\R / \Q$? Background: I am investigating functions satisfying $f(a + b) = f(a)f(b)$ for all $a,b\in\R$. If $f$ is required to be continuous then $f(x) = \exp(A x)$ but if $f$ is not required to be continuous then I think I can define $f(x) = \exp(A_t x)$ where $x$ in $\Q$ where $t$ in some coset $\R / \Q$ and $t \Q = {t + q \text{ where } q\in \Q}$ and $A_t$ is different for each coset. This makes for quite an interesting function!!","$\newcommand{\R}{\Bbb R}\newcommand{\Q}{\Bbb Q}$ Looking at the group of real numbers under addition $(\R, +)$ it contains the (normal) subgroup of rational numbers $(\Q, +)$. I am wondering how to describe the cosets of $\R / \Q$. I know from looking at the cardinality of the sets that because $\R$ is uncountable and $\Q$ is countable that $\R / \Q$ is uncountable. I am also thinking of $\R / \Q$ containing a ""representative"" of each irrational number. I am also aware that $\Q$ is dense in $\R$, so that each member of $\R$ is the limit of a sequence of numbers in $\Q$. Both $\R$ and $\Q$ are ordered. But is there a natural order on $\R / \Q$? What else can we determine about  $\R / \Q$? Background: I am investigating functions satisfying $f(a + b) = f(a)f(b)$ for all $a,b\in\R$. If $f$ is required to be continuous then $f(x) = \exp(A x)$ but if $f$ is not required to be continuous then I think I can define $f(x) = \exp(A_t x)$ where $x$ in $\Q$ where $t$ in some coset $\R / \Q$ and $t \Q = {t + q \text{ where } q\in \Q}$ and $A_t$ is different for each coset. This makes for quite an interesting function!!",,"['real-analysis', 'group-theory', 'cardinals', 'rational-numbers']"
31,Prove that $\left|30240\int_{0}^{1}x(1-x)f(x)f'(x)dx\right|\le1$.,Prove that .,\left|30240\int_{0}^{1}x(1-x)f(x)f'(x)dx\right|\le1,"Let $f\in C^{3}[0,1]$ such that $f(0)=f'(0)=f(1)=0$ and $\big|f''' (x)\big|\le 1$ .Prove that $$\left|30240\int_{0}^{1}x(1-x)f(x)f'(x)dx\right|\le1 .$$ I couldn't make much progress on this problem. I thought that maybe I should try using polynomial interpolation since I have a bound for $|f'''|$ , but I can't determine the interpolation polynomial and I am quickly stuck (there is also the problem that I am dealing with both $f$ and $f'$ under the integral). Apart from this, I don't think that there is much we can do, the solution probably relies on this technique, but I can't make further progress.","Let such that and .Prove that I couldn't make much progress on this problem. I thought that maybe I should try using polynomial interpolation since I have a bound for , but I can't determine the interpolation polynomial and I am quickly stuck (there is also the problem that I am dealing with both and under the integral). Apart from this, I don't think that there is much we can do, the solution probably relies on this technique, but I can't make further progress.","f\in C^{3}[0,1] f(0)=f'(0)=f(1)=0 \big|f''' (x)\big|\le 1 \left|30240\int_{0}^{1}x(1-x)f(x)f'(x)dx\right|\le1 . |f'''| f f'","['real-analysis', 'calculus', 'inequality', 'definite-integrals', 'integral-inequality']"
32,Integral $\int_{-\infty}^\infty J^3_0(x) e^{i\omega x}\mathrm dx $,Integral,\int_{-\infty}^\infty J^3_0(x) e^{i\omega x}\mathrm dx ,"Hi I am trying to evaluate the integral $$ \mathcal{I}(\omega)=\int_{-\infty}^\infty J^3_0(x) e^{i\omega x}\mathrm dx $$ analytically.  We can also write $$ \mathcal{I}(\omega)=\mathcal{FT}\big(J^3_0(x)\big) $$ which is the Fourier Transform of the cube of Bessel function.  The Bessel function $J_0$ is given by $$ J_0(x)=\frac{1}{2\pi}\int_{-\pi}^\pi e^{-ix\sin t} \mathrm dt. $$ If it helps, we can represent the cube of the Bessel function by $$ J^3_0(x)=-3\int J^2_0(x) J_1(x) \mathrm dx, \ \ \ \ \ J_1(x)=\frac{1}{2\pi}\int_{-\pi}^\pi e^{i(t-x\sin t)} \mathrm dt. $$ In general $$ J_n(x)=\frac{1}{2\pi}\int_{-\pi}^\pi e^{i(nt-x\sin t)}\mathrm  dt. $$ The Fourier Transforms of the Bessel function and its square is given by $$ \mathcal{FT}\big(J_0(x)\big)=\sqrt{\frac{2}{\pi}}\frac{\theta(\omega+1)-\theta(\omega-1)}{\sqrt{1-\omega^2}} $$  and  $$ \mathcal{FT}\big(J^2_0(x)\big)=\frac{\sqrt{2}K\big(1-\frac{\omega^2}{4}\big)\big(\theta(-\omega-2)-1\big)\big(\theta(\omega-2)-1\big)}{\pi^{3/2}}          $$ where K is the elliptic-K function and $\theta$ is the heaviside step function.  However I need the cube...","Hi I am trying to evaluate the integral $$ \mathcal{I}(\omega)=\int_{-\infty}^\infty J^3_0(x) e^{i\omega x}\mathrm dx $$ analytically.  We can also write $$ \mathcal{I}(\omega)=\mathcal{FT}\big(J^3_0(x)\big) $$ which is the Fourier Transform of the cube of Bessel function.  The Bessel function $J_0$ is given by $$ J_0(x)=\frac{1}{2\pi}\int_{-\pi}^\pi e^{-ix\sin t} \mathrm dt. $$ If it helps, we can represent the cube of the Bessel function by $$ J^3_0(x)=-3\int J^2_0(x) J_1(x) \mathrm dx, \ \ \ \ \ J_1(x)=\frac{1}{2\pi}\int_{-\pi}^\pi e^{i(t-x\sin t)} \mathrm dt. $$ In general $$ J_n(x)=\frac{1}{2\pi}\int_{-\pi}^\pi e^{i(nt-x\sin t)}\mathrm  dt. $$ The Fourier Transforms of the Bessel function and its square is given by $$ \mathcal{FT}\big(J_0(x)\big)=\sqrt{\frac{2}{\pi}}\frac{\theta(\omega+1)-\theta(\omega-1)}{\sqrt{1-\omega^2}} $$  and  $$ \mathcal{FT}\big(J^2_0(x)\big)=\frac{\sqrt{2}K\big(1-\frac{\omega^2}{4}\big)\big(\theta(-\omega-2)-1\big)\big(\theta(\omega-2)-1\big)}{\pi^{3/2}}          $$ where K is the elliptic-K function and $\theta$ is the heaviside step function.  However I need the cube...",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'special-functions']"
33,"Quadruple Integral $\int\limits_0^1\!\!\int\limits_0^1\!\!\int\limits_0^1\!\!\int\limits_0^1\frac1{1-xyzw}\,dw\,dz\,dy\,dx$",Quadruple Integral,"\int\limits_0^1\!\!\int\limits_0^1\!\!\int\limits_0^1\!\!\int\limits_0^1\frac1{1-xyzw}\,dw\,dz\,dy\,dx","In page 122 of the book Topics in Number Theory (1956) by William J. LeVeque , there is an exercise for evaluating the following integral in two ways. $$\int_0^1\!\!\!\int_0^1\frac1{1-xy}\,dy\,dx$$ First way is to write the integrand as a geometric series, $$\int_0^1\!\!\!\int_0^1\frac1{1-xy}\,dy\,dx=\int_0^1\!\!\!\int_0^1\left(\sum_{n=1}^\infty(xy)^{n-1}\right)\,dy\,dx=\sum_{n=1}^\infty\frac1{n^2}$$ and the second way by use of a suitable change of variables ( $y:=u-v,x:=u+v$ ) which is also published by Tom M. Apostol in this paper . Hence, the second way together with the result of the first way is a proof for the famous Basel problem , in fact to show that $\sum_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}6$ . Now, the main question is, if there is a suitable change of variables for the following integral $$\int_0^1\!\!\!\int_0^1\!\!\!\int_0^1\!\!\!\int_0^1\frac1{1-xyzw}\,dw\,dz\,dy\,dx~?$$ Unfortunately, I think a similar change of variables like ( $w:=p\pm q\pm r\pm s,\cdots$ ) doesn't work here, while I'm not really sure!","In page 122 of the book Topics in Number Theory (1956) by William J. LeVeque , there is an exercise for evaluating the following integral in two ways. First way is to write the integrand as a geometric series, and the second way by use of a suitable change of variables ( ) which is also published by Tom M. Apostol in this paper . Hence, the second way together with the result of the first way is a proof for the famous Basel problem , in fact to show that . Now, the main question is, if there is a suitable change of variables for the following integral Unfortunately, I think a similar change of variables like ( ) doesn't work here, while I'm not really sure!","\int_0^1\!\!\!\int_0^1\frac1{1-xy}\,dy\,dx \int_0^1\!\!\!\int_0^1\frac1{1-xy}\,dy\,dx=\int_0^1\!\!\!\int_0^1\left(\sum_{n=1}^\infty(xy)^{n-1}\right)\,dy\,dx=\sum_{n=1}^\infty\frac1{n^2} y:=u-v,x:=u+v \sum_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}6 \int_0^1\!\!\!\int_0^1\!\!\!\int_0^1\!\!\!\int_0^1\frac1{1-xyzw}\,dw\,dz\,dy\,dx~? w:=p\pm q\pm r\pm s,\cdots","['real-analysis', 'complex-analysis', 'multivariable-calculus', 'closed-form', 'multiple-integral']"
34,Prove that every Lebesgue measurable function is equal almost everywhere to a Borel measurable function,Prove that every Lebesgue measurable function is equal almost everywhere to a Borel measurable function,,"Suppose $(\mathbb{R},\Sigma(m),m)$ is our measure space, where $m$ is Lebesgue measure.  Also, suppose $f : \mathbb{R} \to [-\infty, \infty]$ is a Lebesgue measurable function. The problem: Prove that $f$ is equal almost everywhere to a Borel measurable function. My attempt: We only have to prove this assertion for a non-negative Lebesgue measurable function $f$ because the result will follow for all Lebesgue measurable functions. Furthermore, since any Lebesgue measurable function can be approximated by a sequence of non-negative, monotonically increasing, Lebesgue measurable simple functions $s_{n}$, we only need to prove the claim for an arbitrary Lebesgue measurable simple function. So, let $s: \mathbb{R} \to [-\infty, \infty] $ be a Lebesgue measurable simple function.  We can write $s$ canonically as $$ s(x) = \sum \limits_{i = 1}^{n} \alpha_{i} \chi_{A_{i}}(x)$$ where $\alpha_{i} \in [-\infty, \infty]$, $\bigcup \limits_{i = 1}^{n} A_{i} = \mathbb{R}$, and $A_{i} \cap A_{j} = \emptyset$ if $i \neq j$. For each $i$, since $A_{i}$ is Lebesgue measurable, we can find a Borel set $B_{i}$ such that $A_{i} \subseteq B_{i}$ and $m(B_{i} \setminus A_{i}) = 0$.  Clearly, this implies that $\bigcup \limits_{i = 1}^{n} B_{i} = \mathbb{R}$.  Now we just need the $B_{i}$'s to be pairwise disjoint, with each $B_{i}$ still retaining $A_{i}$. To make them pairwise disjoint, I constructed the following sets: $\tilde{B_{i}} = [B_{i} \setminus (\bigcup \limits_{j \neq i} B_{j})] \cup A_{i}$.  This construction gives us that the $\tilde{B_{i}}$'s are pairwise disjoint (I think....) and $A_{i} \subseteq \tilde{B_{i}}$.  But I don't know that $\tilde{B_{i}}$ is necessarily still a Borel set. :( :(  Am I approaching this problem all wrong?","Suppose $(\mathbb{R},\Sigma(m),m)$ is our measure space, where $m$ is Lebesgue measure.  Also, suppose $f : \mathbb{R} \to [-\infty, \infty]$ is a Lebesgue measurable function. The problem: Prove that $f$ is equal almost everywhere to a Borel measurable function. My attempt: We only have to prove this assertion for a non-negative Lebesgue measurable function $f$ because the result will follow for all Lebesgue measurable functions. Furthermore, since any Lebesgue measurable function can be approximated by a sequence of non-negative, monotonically increasing, Lebesgue measurable simple functions $s_{n}$, we only need to prove the claim for an arbitrary Lebesgue measurable simple function. So, let $s: \mathbb{R} \to [-\infty, \infty] $ be a Lebesgue measurable simple function.  We can write $s$ canonically as $$ s(x) = \sum \limits_{i = 1}^{n} \alpha_{i} \chi_{A_{i}}(x)$$ where $\alpha_{i} \in [-\infty, \infty]$, $\bigcup \limits_{i = 1}^{n} A_{i} = \mathbb{R}$, and $A_{i} \cap A_{j} = \emptyset$ if $i \neq j$. For each $i$, since $A_{i}$ is Lebesgue measurable, we can find a Borel set $B_{i}$ such that $A_{i} \subseteq B_{i}$ and $m(B_{i} \setminus A_{i}) = 0$.  Clearly, this implies that $\bigcup \limits_{i = 1}^{n} B_{i} = \mathbb{R}$.  Now we just need the $B_{i}$'s to be pairwise disjoint, with each $B_{i}$ still retaining $A_{i}$. To make them pairwise disjoint, I constructed the following sets: $\tilde{B_{i}} = [B_{i} \setminus (\bigcup \limits_{j \neq i} B_{j})] \cup A_{i}$.  This construction gives us that the $\tilde{B_{i}}$'s are pairwise disjoint (I think....) and $A_{i} \subseteq \tilde{B_{i}}$.  But I don't know that $\tilde{B_{i}}$ is necessarily still a Borel set. :( :(  Am I approaching this problem all wrong?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lebesgue-measure']"
35,Smooth functions for which $f(x)$ is rational if and only if $x$ is rational,Smooth functions for which  is rational if and only if  is rational,f(x) x,"A friend of mine introduced me to the following question: Does there exist a smooth function $f: \mathbb{R} \to \mathbb{R}$ ($f \in C^{\infty}$), such that $f$ maps rationals to rationals and irrationals to irrationals and is nonlinear? He has been able to prove that such a polynomial (with degree at least 2) doesn't exist. The problem has been asked before at least at http://www.artofproblemsolving.com .","A friend of mine introduced me to the following question: Does there exist a smooth function $f: \mathbb{R} \to \mathbb{R}$ ($f \in C^{\infty}$), such that $f$ maps rationals to rationals and irrationals to irrationals and is nonlinear? He has been able to prove that such a polynomial (with degree at least 2) doesn't exist. The problem has been asked before at least at http://www.artofproblemsolving.com .",,"['number-theory', 'real-analysis']"
36,Meaning of convolution?,Meaning of convolution?,,"I am currently learning about the concept of convolution  between two functions in my university course. The course notes are vague about what convolution is, so I was wondering if anyone could give me a good explanation. I can't seem to grasp other than the fact that it is just a particular integral of two functions. What is the physical meaning of convolution and why is it useful? Thanks a lot.","I am currently learning about the concept of convolution  between two functions in my university course. The course notes are vague about what convolution is, so I was wondering if anyone could give me a good explanation. I can't seem to grasp other than the fact that it is just a particular integral of two functions. What is the physical meaning of convolution and why is it useful? Thanks a lot.",,"['real-analysis', 'intuition', 'convolution']"
37,What is the value of $\left(\frac{1}{2}\right)^{\left(\frac{1}{3}\right)^{\left(\frac{1}{4}\right)^{\unicode{x22F0}}}}$?,What is the value of ?,\left(\frac{1}{2}\right)^{\left(\frac{1}{3}\right)^{\left(\frac{1}{4}\right)^{\unicode{x22F0}}}},"Motivation I have been thinking about this thing for quite a while now. I have tried many ways but couldn't really figure it out. I am pretty sure that this kind of expressions don't really have a closed form, so I am asking for other representation for example maybe in terms of an infinite sum or product or something. Creating A New Notation Just like there is a notation for a sum $\textstyle\displaystyle{\sum_{n=a}^{b}s_n=s_a+\cdots+s_b}$ and also for a product $\textstyle\displaystyle{\prod_{n=a}^{b}s_n=s_a\cdots s_b}$ , I was quite surprised that there wasn't  any notation for exponentiation. I would agree that there wouldn't be any use for this notation but still, why would no mathematician ever would create such a notation just for the sake of curiosity. That is why I would request readers to give me any references if there are any. I haven't found any, so I am creating my own. Let $$\boxed{\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=a}^{b}s_k=s_a^{\unicode{x22F0}^{s_b}}}}$$ where $b>a$ . If $a>b$ then $\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=a}^{b}s_k=1}$ and $\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=a}^{b}x=^{b-a+1}x}$ . Obviously $a,b\in\mathbb{Z}$ . Unlike product and sum, exponentiation isn't commutative so we have to be careful when using the notation. Maybe we can modify it a little bit to include the ordering. By the way we are defining $\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=a}^{\infty}s_k:= \lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=a}^{n}s_k\right)}.$ Some Natural Questions When written out in the form of this notation, some natural curious questions arrive or at least some arrived in my mind, for example $\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{\infty}\frac{1}{k^s}}$ and $\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{\infty}\frac{1}{n}}$ . My Curiosity My initial curiosity was $H=\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{\infty}\frac{1}{k}=\left(\frac{1}{2}\right)^{\left(\frac{1}{3}\right)^{\left(\frac{1}{4}\right)^{\unicode{x22F0}}}}}$ . As pointed out by Tavish in the comments, this can be written as a recurrence relation given by $$\textstyle\displaystyle{a_{n+1}=-\frac{\ln(a_n)}{\ln(n)}}$$ where $\textstyle\displaystyle{a_n=\left(\frac{1}{n}\right)^{\left(\frac{1}{n+1}\right)^{\unicode{x22F0}}}}$ . Solving this will help us derive $H$ . But as pointed out in the comments and in this question ( Note that this question focuses on the convergence of $H$ , while my question focuses on something different ), $H$ doesn't really make much sense by the definition of infinite power tower above because it seems that $$\textstyle\displaystyle{\lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=2}^{2n}\frac{1}{k}\right)\neq\lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=2}^{2n+1}\frac{1}{k}\right)}.$$ In particular, we have \begin{align}H_O&=\lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=2}^{2n+1}\frac{1}{k}\right)=0.6903471261\cdots\\H_E&=\lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=2}^{2n}\frac{1}{k}\right)=0.6583655992\cdots\end{align} Let $E_n=\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{2n}\frac{1}{k}}$ and $O_n=\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{2n+1}\frac{1}{k}}$ . I tried constructing a recurrence relation for $E_n$ and $O_n$ , but couldn't, except \begin{align}E_{n+1}&=\left(\log_{\frac{1}{2n-1}}(\cdots\log_{\frac{1}{2}}(E_n))\right)^{\left(\frac{1}{2n+1}\right)^{\left(\frac{1}{2n+2}\right)}}\\O_{n+1}&=\left(\log_{\frac{1}{2n}}(\cdots\log_{\frac{1}{2}}(O_n))\right)^{\left(\frac{1}{2n+2}\right)^{\left(\frac{1}{2n+3}\right)}}\end{align} which is not really workable. If there is a way to simplify it then please do tell me or write a partial answer about it, because it would help us a lot in finding the values of $H_E$ and $H_O$ . My Question I am pretty sure that there really isn't closed form of $H_E$ and $H_O$ . So I am asking for a different representation for those constants, maybe as a sum or an integral possibly?","Motivation I have been thinking about this thing for quite a while now. I have tried many ways but couldn't really figure it out. I am pretty sure that this kind of expressions don't really have a closed form, so I am asking for other representation for example maybe in terms of an infinite sum or product or something. Creating A New Notation Just like there is a notation for a sum and also for a product , I was quite surprised that there wasn't  any notation for exponentiation. I would agree that there wouldn't be any use for this notation but still, why would no mathematician ever would create such a notation just for the sake of curiosity. That is why I would request readers to give me any references if there are any. I haven't found any, so I am creating my own. Let where . If then and . Obviously . Unlike product and sum, exponentiation isn't commutative so we have to be careful when using the notation. Maybe we can modify it a little bit to include the ordering. By the way we are defining Some Natural Questions When written out in the form of this notation, some natural curious questions arrive or at least some arrived in my mind, for example and . My Curiosity My initial curiosity was . As pointed out by Tavish in the comments, this can be written as a recurrence relation given by where . Solving this will help us derive . But as pointed out in the comments and in this question ( Note that this question focuses on the convergence of , while my question focuses on something different ), doesn't really make much sense by the definition of infinite power tower above because it seems that In particular, we have Let and . I tried constructing a recurrence relation for and , but couldn't, except which is not really workable. If there is a way to simplify it then please do tell me or write a partial answer about it, because it would help us a lot in finding the values of and . My Question I am pretty sure that there really isn't closed form of and . So I am asking for a different representation for those constants, maybe as a sum or an integral possibly?","\textstyle\displaystyle{\sum_{n=a}^{b}s_n=s_a+\cdots+s_b} \textstyle\displaystyle{\prod_{n=a}^{b}s_n=s_a\cdots s_b} \boxed{\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=a}^{b}s_k=s_a^{\unicode{x22F0}^{s_b}}}} b>a a>b \textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=a}^{b}s_k=1} \textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=a}^{b}x=^{b-a+1}x} a,b\in\mathbb{Z} \textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=a}^{\infty}s_k:= \lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=a}^{n}s_k\right)}. \textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{\infty}\frac{1}{k^s}} \textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{\infty}\frac{1}{n}} H=\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{\infty}\frac{1}{k}=\left(\frac{1}{2}\right)^{\left(\frac{1}{3}\right)^{\left(\frac{1}{4}\right)^{\unicode{x22F0}}}}} \textstyle\displaystyle{a_{n+1}=-\frac{\ln(a_n)}{\ln(n)}} \textstyle\displaystyle{a_n=\left(\frac{1}{n}\right)^{\left(\frac{1}{n+1}\right)^{\unicode{x22F0}}}} H H H \textstyle\displaystyle{\lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=2}^{2n}\frac{1}{k}\right)\neq\lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=2}^{2n+1}\frac{1}{k}\right)}. \begin{align}H_O&=\lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=2}^{2n+1}\frac{1}{k}\right)=0.6903471261\cdots\\H_E&=\lim_{n\rightarrow\infty}\left({\huge\varepsilon\normalsize}_{k=2}^{2n}\frac{1}{k}\right)=0.6583655992\cdots\end{align} E_n=\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{2n}\frac{1}{k}} O_n=\textstyle\displaystyle{{\huge\varepsilon\normalsize}_{k=2}^{2n+1}\frac{1}{k}} E_n O_n \begin{align}E_{n+1}&=\left(\log_{\frac{1}{2n-1}}(\cdots\log_{\frac{1}{2}}(E_n))\right)^{\left(\frac{1}{2n+1}\right)^{\left(\frac{1}{2n+2}\right)}}\\O_{n+1}&=\left(\log_{\frac{1}{2n}}(\cdots\log_{\frac{1}{2}}(O_n))\right)^{\left(\frac{1}{2n+2}\right)^{\left(\frac{1}{2n+3}\right)}}\end{align} H_E H_O H_E H_O","['real-analysis', 'sequences-and-series', 'limits', 'exponentiation']"
38,Strange functional equation: $f(x)+f(\cos(x))=x$,Strange functional equation:,f(x)+f(\cos(x))=x,"BACKGROUND: A while ago, I became obsessed for a period of time with the following functional equation: $$f(x)+f(\cos(x))=x$$ I am only considering the unique real analytic solution to this functional equation (it is indeed unique... all derivatives of $f$ at $w\approx 0.739$, the Dottie Number , can be determined by differentiating this equation repeatedly). Here is a graph of the function: I had already given up a long time ago when I accidentally came across my notes on this problem today, and I figured I would put it one MSE to see if anyone could find anything interesting that I missed. WHAT I FOUND: I am almost absolutely certain that there is no nice closed-form for this function, so I resorted to finding special values and other neater functional equations. So far, I have found the following special values of $f$ and its derivatives: $$f(w)=w/2$$ $$f'(w)=\frac{1}{1-\sqrt{1-w^2}}$$ $$f''(w)=\frac{1}{2-w^2}\frac{w}{1-\sqrt{1-w^2}}$$ $$f'(0)=1$$ $$f'(\pi/2)=2$$ $$f'(-\pi/2)=0$$ Here are some functional equations I have found for $f$: $$f(x+2\pi)-f(x)=2\pi$$ $$f(x+\pi)-f(x)=2\cos(x)+\pi$$ $$f(x)-f(-x)=2x$$ And here is a series representation for $f$, where $\cos^{\circ n}$ represents the cosine function composed $n$ times: $$f(x)=\frac{w}{2}+\sum_{n=0}^\infty \big(\cos^{\circ 2n}(x)-\cos^{\circ 2n+1}(x)\big)$$ QUESTIONS: This is a very open-ended question. I have a few unproven conjectures or particular unanswered questions about this function, but I really just want to see what interesting properties (especially special values, zeroes, maxima or minima, inflection points, and functional or differential equations) people can find. An example of one of my conjectures: by looking at graphs, I have conjectured that $f(x+a)-f(x)$ is a sinusoid or a sum of sinusoids for all $a$. It is easy to show that this must be periodic, but I would like to prove or disprove that it can be expressed as a sum of sinusoids. In particular, can we find an expression for $f(x+\pi/2)-f(x)$ as a sum of sinusoids, possibly involving other mathematical constants like $w$? Here is a graph of $f(x+\pi/2)-f(x)$: I appreciate any contributions!","BACKGROUND: A while ago, I became obsessed for a period of time with the following functional equation: $$f(x)+f(\cos(x))=x$$ I am only considering the unique real analytic solution to this functional equation (it is indeed unique... all derivatives of $f$ at $w\approx 0.739$, the Dottie Number , can be determined by differentiating this equation repeatedly). Here is a graph of the function: I had already given up a long time ago when I accidentally came across my notes on this problem today, and I figured I would put it one MSE to see if anyone could find anything interesting that I missed. WHAT I FOUND: I am almost absolutely certain that there is no nice closed-form for this function, so I resorted to finding special values and other neater functional equations. So far, I have found the following special values of $f$ and its derivatives: $$f(w)=w/2$$ $$f'(w)=\frac{1}{1-\sqrt{1-w^2}}$$ $$f''(w)=\frac{1}{2-w^2}\frac{w}{1-\sqrt{1-w^2}}$$ $$f'(0)=1$$ $$f'(\pi/2)=2$$ $$f'(-\pi/2)=0$$ Here are some functional equations I have found for $f$: $$f(x+2\pi)-f(x)=2\pi$$ $$f(x+\pi)-f(x)=2\cos(x)+\pi$$ $$f(x)-f(-x)=2x$$ And here is a series representation for $f$, where $\cos^{\circ n}$ represents the cosine function composed $n$ times: $$f(x)=\frac{w}{2}+\sum_{n=0}^\infty \big(\cos^{\circ 2n}(x)-\cos^{\circ 2n+1}(x)\big)$$ QUESTIONS: This is a very open-ended question. I have a few unproven conjectures or particular unanswered questions about this function, but I really just want to see what interesting properties (especially special values, zeroes, maxima or minima, inflection points, and functional or differential equations) people can find. An example of one of my conjectures: by looking at graphs, I have conjectured that $f(x+a)-f(x)$ is a sinusoid or a sum of sinusoids for all $a$. It is easy to show that this must be periodic, but I would like to prove or disprove that it can be expressed as a sum of sinusoids. In particular, can we find an expression for $f(x+\pi/2)-f(x)$ as a sum of sinusoids, possibly involving other mathematical constants like $w$? Here is a graph of $f(x+\pi/2)-f(x)$: I appreciate any contributions!",,"['real-analysis', 'functional-equations']"
39,Calculus over $\mathbb{Q}$,Calculus over,\mathbb{Q},"The mismatch between the sensitivity of 'mathematical calculus' and the flexibility of 'real world calculus' has been bothering me a bit recently. What I mean is this: in the real world, I can trust that calculus will work whether $\mathbb{R}$ is ""actually real"" or not. ""Continuity"" can be considered as relative to a certain scale, and this is good enough for many purposes. Mathematically of course, this is not the case. Weakening even to $\mathbb{Q}$ will cause, say, the intermediate value theorem to fail. But I guess what I'm really wondering is whether or not this is just a problem with the chosen definitions involved, and if a clean formalism exists that more accurately captures how/why calculus is so widely applicable in real-world problems. And so I come to my main questions: If we replace the idea of exactly hitting a number (as used in the IVT, EVT, and MVT) with ""getting arbitrarily close to it"", can we still cleanly and/or consistently develop calculus? It seems like $\mathbb{Q}$ could support some form of calculus with this, since as far as I can see the whole '$\forall\epsilon\exists\delta$' paradigm is left intact. The normal counterexample to the IVT in $\mathbb{Q}$ is given by asking for roots of $y=x^2-2$, but this would be avoided because $y$ gets arbitrarily close to $0$. Moreover, restricting to smaller and smaller intervals where $y$ gets arbitrarily close to $0$ can also show that it doesn't get arbitrarily close to any other number ""at the same time"". Could this be considered continuous? Is there a form of calculus that deals with finite error without actually lugging explicit error terms around (i.e. a 'fuzzy calculus')? As in ""any differences less than the tolerance $\epsilon_0$ are ignored""? I suppose this is somewhat related to nilpotent infinitesimals/dual numbers and big O notation, but since these still consider numbers of infinite precision they aren't quite what I'm looking for. I'd imagine that there's no way of doing this without getting into the same ugly issues of floating point numbers, but I figure it doesn't hurt to ask.","The mismatch between the sensitivity of 'mathematical calculus' and the flexibility of 'real world calculus' has been bothering me a bit recently. What I mean is this: in the real world, I can trust that calculus will work whether $\mathbb{R}$ is ""actually real"" or not. ""Continuity"" can be considered as relative to a certain scale, and this is good enough for many purposes. Mathematically of course, this is not the case. Weakening even to $\mathbb{Q}$ will cause, say, the intermediate value theorem to fail. But I guess what I'm really wondering is whether or not this is just a problem with the chosen definitions involved, and if a clean formalism exists that more accurately captures how/why calculus is so widely applicable in real-world problems. And so I come to my main questions: If we replace the idea of exactly hitting a number (as used in the IVT, EVT, and MVT) with ""getting arbitrarily close to it"", can we still cleanly and/or consistently develop calculus? It seems like $\mathbb{Q}$ could support some form of calculus with this, since as far as I can see the whole '$\forall\epsilon\exists\delta$' paradigm is left intact. The normal counterexample to the IVT in $\mathbb{Q}$ is given by asking for roots of $y=x^2-2$, but this would be avoided because $y$ gets arbitrarily close to $0$. Moreover, restricting to smaller and smaller intervals where $y$ gets arbitrarily close to $0$ can also show that it doesn't get arbitrarily close to any other number ""at the same time"". Could this be considered continuous? Is there a form of calculus that deals with finite error without actually lugging explicit error terms around (i.e. a 'fuzzy calculus')? As in ""any differences less than the tolerance $\epsilon_0$ are ignored""? I suppose this is somewhat related to nilpotent infinitesimals/dual numbers and big O notation, but since these still consider numbers of infinite precision they aren't quite what I'm looking for. I'd imagine that there's no way of doing this without getting into the same ugly issues of floating point numbers, but I figure it doesn't hurt to ask.",,"['calculus', 'real-analysis', 'continuity', 'infinitesimals']"
40,How much can we rearrange a series?,How much can we rearrange a series?,,"There's a well-known result that if $\sum a_n$ is conditionally convergent, then for any real $c$ there exists a permutation $\pi:\mathbb{N} \to \mathbb{N}$ such that $\sum a_{\pi(n)} = c$ . A consequence of this is that you cannot get away with rearranging infinite sums, in general. However , if the rearrangement is sufficiently tame, then we can get away this. For instance, if we rearrange only finitely many terms, we can get away with this without the value of the sum changing. Indeed, there is a stronger result that if we have a permutation $\pi$ and a uniform constant $M$ such that $|\pi(n) - n| < M$ for all positive integers $n$ , then $\sum a_n = \sum a_{\pi(n)}$ . This is not too hard to prove. My question is, essentially, can we get a ""maximally strong"" result of this sort? Specifically I ask the following question: Characterize permutations $\pi:\mathbb{N} \to \mathbb{N}$ with the following property: For any convergent infinite sum $\sum a_n$ , we have that $\sum a_n = \sum a_{\pi(n)}$ . Notice the order of quantifiers here. It's conceivable that the result I described in the second paragraph is the best we can do, in the sense that if $\sup_n |\pi(n) - n| = \infty$ , there exists a conditionally convergent $a_n$ with $\sum a_n \neq \sum a_{\pi(n)}$ . Perhaps we can do better, though. One guess is slightly changing the hypotheses as follows: there exists a uniform $M$ such that $|\pi(n) - n| < M$ for all $n \in \mathbb{N} \setminus S$ , where $S$ is a subset of $\mathbb{N}$ with $0$ asymptotic density .","There's a well-known result that if is conditionally convergent, then for any real there exists a permutation such that . A consequence of this is that you cannot get away with rearranging infinite sums, in general. However , if the rearrangement is sufficiently tame, then we can get away this. For instance, if we rearrange only finitely many terms, we can get away with this without the value of the sum changing. Indeed, there is a stronger result that if we have a permutation and a uniform constant such that for all positive integers , then . This is not too hard to prove. My question is, essentially, can we get a ""maximally strong"" result of this sort? Specifically I ask the following question: Characterize permutations with the following property: For any convergent infinite sum , we have that . Notice the order of quantifiers here. It's conceivable that the result I described in the second paragraph is the best we can do, in the sense that if , there exists a conditionally convergent with . Perhaps we can do better, though. One guess is slightly changing the hypotheses as follows: there exists a uniform such that for all , where is a subset of with asymptotic density .",\sum a_n c \pi:\mathbb{N} \to \mathbb{N} \sum a_{\pi(n)} = c \pi M |\pi(n) - n| < M n \sum a_n = \sum a_{\pi(n)} \pi:\mathbb{N} \to \mathbb{N} \sum a_n \sum a_n = \sum a_{\pi(n)} \sup_n |\pi(n) - n| = \infty a_n \sum a_n \neq \sum a_{\pi(n)} M |\pi(n) - n| < M n \in \mathbb{N} \setminus S S \mathbb{N} 0,"['real-analysis', 'sequences-and-series']"
41,"A proof of $\int_{0}^{1}\left( \frac{\ln t}{1-t}\right)^2\,\mathrm{d}t=\frac{\pi^2}{3}$",A proof of,"\int_{0}^{1}\left( \frac{\ln t}{1-t}\right)^2\,\mathrm{d}t=\frac{\pi^2}{3}","What is the proof of the following: $$\int_{0}^{1} \left(\frac{\ln t}{1-t}\right)^2 \,\mathrm{d}t=\frac{\pi^2}{3} \>?$$","What is the proof of the following: $$\int_{0}^{1} \left(\frac{\ln t}{1-t}\right)^2 \,\mathrm{d}t=\frac{\pi^2}{3} \>?$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
42,What's really going on behind calculus? [closed],What's really going on behind calculus? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I'm currently taking maths at A Level and I have found it strange that it is not explained why the differential of $x^n$ is $nx^{n-1}$, for example. I can see that it works through observation and first principle, but how can it be derived? And what about for an unknown function, not based on trigonometry, $e^x$ or polynomials? Is there some sort of intuition or derivation that can lead to a general answer, other than making observations? Another thing that I do not quite understand is why higher derivatives of some function is denoted by $\frac{d^ny}{dx^n}$. Sorry if this sounds really basic but I would appreciate any explanations/links to good resources (I've had a look online and couldn't find too much other than some nice explanations special cases etc). Thanks :)","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I'm currently taking maths at A Level and I have found it strange that it is not explained why the differential of $x^n$ is $nx^{n-1}$, for example. I can see that it works through observation and first principle, but how can it be derived? And what about for an unknown function, not based on trigonometry, $e^x$ or polynomials? Is there some sort of intuition or derivation that can lead to a general answer, other than making observations? Another thing that I do not quite understand is why higher derivatives of some function is denoted by $\frac{d^ny}{dx^n}$. Sorry if this sounds really basic but I would appreciate any explanations/links to good resources (I've had a look online and couldn't find too much other than some nice explanations special cases etc). Thanks :)",,"['calculus', 'real-analysis', 'soft-question', 'intuition']"
43,"Rigorous definition of ""differential""","Rigorous definition of ""differential""",,"When it comes to definitions, I will be very strict. Most textbooks tend to define differential of a function/variable like this: Let $f(x)$ be a differentiable function. By assuming that changes in $x$ are small enough, we can say: $$\Delta f(x)\approx  {f}'(x)\Delta x$$ Where $\Delta f(x)$ is the changes in the value of function. Now we define differential of $f(x)$ as follows: $$\mathrm{d}f(x):= {f}'(x)\mathrm{d} x$$ Where $\mathrm{d} f(x)$ is the differential of $f(x)$ and $\mathrm{d} x$ is the differential of $x$. What bothers me is this definition is completely circular. I mean we are defining differential by differential itself. Can we define differential more precisely and rigorously? P.S. Is it possible to define differential simply as the limit of a difference as the difference approaches zero?: $$\mathrm{d}x= \lim_{\Delta x \to 0}\Delta x$$ Thank you in advance. EDIT: I still think I didn't catch the best answer. I prefer the answer to be in the context of ""Calculus"" or ""Analysis"" rather than the ""Theory of Differential forms"". And again I don't want a circular definition. I think it is possible to define ""Differential"" with the use of ""Limits"" in some way. Thank you in advance. EDIT 2 (Answer to ""Mikhail Katz""'s comment): the account I gave in terms of the hyperreal number system which contains infinitesimals seems to respond to your concerns. I would be happy to elaborate if anything seems unclear. – Mikhail Katz Thank you for your help. I have two issues: First of all we define differential as $\mathrm{d} f(x)=f'(x)\mathrm{d} x$ then we deceive ourselves that $\mathrm{d} x$ is nothing but another representation of $\Delta x$ and then without clarifying the reason, we indeed treat $\mathrm{d} x$ as the differential of the variable $x$ and then we write the derivative of $f(x)$ as the ratio of $\mathrm{d} f(x)$ to $\mathrm{d} x$. So we literally (and also by stealthily screwing ourselves) defined ""Differential"" by another differential and it is circular. Secondly (at least I think) it could be possible to define differential without having any knowledge of the notion of derivative. So we can define ""Derivative"" and ""Differential"" independently and then deduce that the relation $f'{(x)}=\frac{\mathrm{d} f(x)}{\mathrm{d} x}$ is just a natural result of their definitions (using possibly the notion of limits) and is not related to the definition itself. I know the relation $\mathrm{d} f(x)=f'(x)\mathrm{d} x$ always works and it will always give us a way to calculate differentials. But I (as an strictly axiomaticist person) couldn't accept it as a definition of Differential. EDIT 3: Answer to comments: I am not aware of any textbook defining differentials like this. What kind of textbooks have you been reading? – Najib Idrissi which textbooks? – m_t_ Check ""Calculus and Analytic Geometry"", ""Thomas-Finney"", 9th edition, page 251 and ""Calculus: Early Transcendentals"", ""Stewart"", 8th edition, page 254 They literally defined differential by another differential.","When it comes to definitions, I will be very strict. Most textbooks tend to define differential of a function/variable like this: Let $f(x)$ be a differentiable function. By assuming that changes in $x$ are small enough, we can say: $$\Delta f(x)\approx  {f}'(x)\Delta x$$ Where $\Delta f(x)$ is the changes in the value of function. Now we define differential of $f(x)$ as follows: $$\mathrm{d}f(x):= {f}'(x)\mathrm{d} x$$ Where $\mathrm{d} f(x)$ is the differential of $f(x)$ and $\mathrm{d} x$ is the differential of $x$. What bothers me is this definition is completely circular. I mean we are defining differential by differential itself. Can we define differential more precisely and rigorously? P.S. Is it possible to define differential simply as the limit of a difference as the difference approaches zero?: $$\mathrm{d}x= \lim_{\Delta x \to 0}\Delta x$$ Thank you in advance. EDIT: I still think I didn't catch the best answer. I prefer the answer to be in the context of ""Calculus"" or ""Analysis"" rather than the ""Theory of Differential forms"". And again I don't want a circular definition. I think it is possible to define ""Differential"" with the use of ""Limits"" in some way. Thank you in advance. EDIT 2 (Answer to ""Mikhail Katz""'s comment): the account I gave in terms of the hyperreal number system which contains infinitesimals seems to respond to your concerns. I would be happy to elaborate if anything seems unclear. – Mikhail Katz Thank you for your help. I have two issues: First of all we define differential as $\mathrm{d} f(x)=f'(x)\mathrm{d} x$ then we deceive ourselves that $\mathrm{d} x$ is nothing but another representation of $\Delta x$ and then without clarifying the reason, we indeed treat $\mathrm{d} x$ as the differential of the variable $x$ and then we write the derivative of $f(x)$ as the ratio of $\mathrm{d} f(x)$ to $\mathrm{d} x$. So we literally (and also by stealthily screwing ourselves) defined ""Differential"" by another differential and it is circular. Secondly (at least I think) it could be possible to define differential without having any knowledge of the notion of derivative. So we can define ""Derivative"" and ""Differential"" independently and then deduce that the relation $f'{(x)}=\frac{\mathrm{d} f(x)}{\mathrm{d} x}$ is just a natural result of their definitions (using possibly the notion of limits) and is not related to the definition itself. I know the relation $\mathrm{d} f(x)=f'(x)\mathrm{d} x$ always works and it will always give us a way to calculate differentials. But I (as an strictly axiomaticist person) couldn't accept it as a definition of Differential. EDIT 3: Answer to comments: I am not aware of any textbook defining differentials like this. What kind of textbooks have you been reading? – Najib Idrissi which textbooks? – m_t_ Check ""Calculus and Analytic Geometry"", ""Thomas-Finney"", 9th edition, page 251 and ""Calculus: Early Transcendentals"", ""Stewart"", 8th edition, page 254 They literally defined differential by another differential.",,"['real-analysis', 'calculus', 'definition', 'differential', 'infinitesimals']"
44,Is the intersection of a closed set and a compact set always compact?,Is the intersection of a closed set and a compact set always compact?,,"I am going through Rudin's Principles of Mathematical Analysis in preparation for the masters exam, and I am seeking clarification on a corollary. Theorem 2.34 states that compact sets in metric spaces are closed. Theorem 2.35 states that closed subsets of compact spaces are compact. As a corollary, Rudin then states that if $L$ is closed and $K$ is compact, then their intersection $L \cap K$ is compact, citing 2.34 and 2.24(b) (intersections of closed sets are closed) to argue that $L \cap K$ is closed, and then using 2.35 to show that $L \cap K$ is compact as a closed subset of a compact set. Am I correct in believing that this corollary holds for metric spaces, and not in general topological spaces?","I am going through Rudin's Principles of Mathematical Analysis in preparation for the masters exam, and I am seeking clarification on a corollary. Theorem 2.34 states that compact sets in metric spaces are closed. Theorem 2.35 states that closed subsets of compact spaces are compact. As a corollary, Rudin then states that if $L$ is closed and $K$ is compact, then their intersection $L \cap K$ is compact, citing 2.34 and 2.24(b) (intersections of closed sets are closed) to argue that $L \cap K$ is closed, and then using 2.35 to show that $L \cap K$ is compact as a closed subset of a compact set. Am I correct in believing that this corollary holds for metric spaces, and not in general topological spaces?",,"['real-analysis', 'general-topology', 'metric-spaces']"
45,What are the main uses of convex functions?,What are the main uses of convex functions?,,"Up till now I have just learned that the concept of convexity in functions of one variable is used to complete the graphs of functions, meaning to locate points of inflexion and see if the graph is concave or convex in given intervals. Spivak's book mention that altough this is what we typically learn at calculus, the importance of this concept doesn't lie precisely on ploting graphs and it is really worth to assimilate the information. I'd like to know the main aplications of this concept and it's importance in general.","Up till now I have just learned that the concept of convexity in functions of one variable is used to complete the graphs of functions, meaning to locate points of inflexion and see if the graph is concave or convex in given intervals. Spivak's book mention that altough this is what we typically learn at calculus, the importance of this concept doesn't lie precisely on ploting graphs and it is really worth to assimilate the information. I'd like to know the main aplications of this concept and it's importance in general.",,"['real-analysis', 'soft-question', 'convex-analysis']"
46,"Integrability of Thomae's Function on $[0,1]$.",Integrability of Thomae's Function on .,"[0,1]","Consider the function $f: [0,1] \to \mathbb{R}$ where f(x)= \begin{cases} \frac 1q & \text{if } x\in \mathbb{Q} \text{ and } x=\frac pq \text{ in lowest terms}\\ 0 & \text{otherwise} \end{cases} Determine whether or not $g$ is in  $\mathscr{R}$ on $[0,1]$ and prove your assertion. For this problem you may consider $0= 0/1$ to be in lowest terms. Here's an attempt. I may have abused a bit of notation here, but the ideas are there. Proof: Let $M_i = \sup \limits_{x \in [x_{i-1},x_i]} f(x)$. Notice first that the lower Riemann sums are always $0$, since every interval contains an irrational number. Thus, to prove $f \in \mathscr{R}$, it suffices to prove that, given any $\epsilon >0$, $\sum \limits_{i \in P} M_i \Delta x_i < \epsilon$ for some partition. Let $\epsilon > 0 $ and $M  > \frac{2}{\epsilon}$. We first show that there exists $\eta(x,\frac{1}{M})$ so that $|f(x) - f(y)| < \frac{1}{M}$ if $|x-y| < \eta$. Fix $x \in (\mathbb{R} \setminus \mathbb{Q}) \cap [0,1]$. Now, consider the set $$R_{M} := \{ r \in \mathbb{Q} : r = \frac{p}{n}, n \leq M, p \leq n, p \in \mathbb{N} \}.$$ Clearly this set is finite, enumerate it as $\{q_1,\ldots, q_m\}$. So, let $$\eta(x,\frac{1}{M}) = \min_{i=1,\ldots, m} |x- q_i|.$$ We see then, $|f(x) - f(y)| < \frac{1}{M}$ on this $\eta$-neighborhood. After we choose that $\eta$ so that $x \in (\mathbb{R} \setminus \mathbb{Q}) \cap [0,1]$, is continuous in a $\eta$-neighborhood, we see  $$ A:= [0,1] \setminus R_M \subset \left( \bigcup_{ x \in ( \mathbb{R} \setminus \mathbb{Q}) \cap [0,1]} B_{\eta(x)} (x) \right) \cap [0,1].$$  Since $A$ is compact, we may take finite sub-covering, and let $\delta = \min \limits_{i=1,\ldots,n} \{\eta(x_i)\}$. Take a partition $P_1$ of $A$ so that $\Delta x_i < \delta$. Since $R_M$ is non-empty, we can take a partition $P_2$ of $R_M$ so that $\Delta x_i < \frac{\epsilon}{2m}.$ Moreover, we see that,  on $[0,1]$, $f$ is at most $1$. Let $P = P_1 \cup P_2$. Thus, \begin{eqnarray*} \sum_{i \in P} M_i \Delta x_i &=& \sum_{i \in P_1} M_i \Delta x_i + \sum_{i \in P_2} M_i \Delta x_i \\ &\leq& \frac{1}{M} \sum_{i \in P_1} \Delta x_i + \sum_{i \in P_2} \Delta x_i \\ &<& \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon  \end{eqnarray*} Comments? EDITED I think I resolved the issue.","Consider the function $f: [0,1] \to \mathbb{R}$ where f(x)= \begin{cases} \frac 1q & \text{if } x\in \mathbb{Q} \text{ and } x=\frac pq \text{ in lowest terms}\\ 0 & \text{otherwise} \end{cases} Determine whether or not $g$ is in  $\mathscr{R}$ on $[0,1]$ and prove your assertion. For this problem you may consider $0= 0/1$ to be in lowest terms. Here's an attempt. I may have abused a bit of notation here, but the ideas are there. Proof: Let $M_i = \sup \limits_{x \in [x_{i-1},x_i]} f(x)$. Notice first that the lower Riemann sums are always $0$, since every interval contains an irrational number. Thus, to prove $f \in \mathscr{R}$, it suffices to prove that, given any $\epsilon >0$, $\sum \limits_{i \in P} M_i \Delta x_i < \epsilon$ for some partition. Let $\epsilon > 0 $ and $M  > \frac{2}{\epsilon}$. We first show that there exists $\eta(x,\frac{1}{M})$ so that $|f(x) - f(y)| < \frac{1}{M}$ if $|x-y| < \eta$. Fix $x \in (\mathbb{R} \setminus \mathbb{Q}) \cap [0,1]$. Now, consider the set $$R_{M} := \{ r \in \mathbb{Q} : r = \frac{p}{n}, n \leq M, p \leq n, p \in \mathbb{N} \}.$$ Clearly this set is finite, enumerate it as $\{q_1,\ldots, q_m\}$. So, let $$\eta(x,\frac{1}{M}) = \min_{i=1,\ldots, m} |x- q_i|.$$ We see then, $|f(x) - f(y)| < \frac{1}{M}$ on this $\eta$-neighborhood. After we choose that $\eta$ so that $x \in (\mathbb{R} \setminus \mathbb{Q}) \cap [0,1]$, is continuous in a $\eta$-neighborhood, we see  $$ A:= [0,1] \setminus R_M \subset \left( \bigcup_{ x \in ( \mathbb{R} \setminus \mathbb{Q}) \cap [0,1]} B_{\eta(x)} (x) \right) \cap [0,1].$$  Since $A$ is compact, we may take finite sub-covering, and let $\delta = \min \limits_{i=1,\ldots,n} \{\eta(x_i)\}$. Take a partition $P_1$ of $A$ so that $\Delta x_i < \delta$. Since $R_M$ is non-empty, we can take a partition $P_2$ of $R_M$ so that $\Delta x_i < \frac{\epsilon}{2m}.$ Moreover, we see that,  on $[0,1]$, $f$ is at most $1$. Let $P = P_1 \cup P_2$. Thus, \begin{eqnarray*} \sum_{i \in P} M_i \Delta x_i &=& \sum_{i \in P_1} M_i \Delta x_i + \sum_{i \in P_2} M_i \Delta x_i \\ &\leq& \frac{1}{M} \sum_{i \in P_1} \Delta x_i + \sum_{i \in P_2} \Delta x_i \\ &<& \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon  \end{eqnarray*} Comments? EDITED I think I resolved the issue.",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'riemann-integration']"
47,Double limit of $\cos^{2n}(m! \pi x)$ at rationals and irrationals,Double limit of  at rationals and irrationals,\cos^{2n}(m! \pi x),"I stumbled upon this ""relation"" (is the name correct?): $$ \lim_{m \to \infty} \lim_{n \to \infty} \cos^{2n}(m! \pi x) = \begin{cases} 1,&x\text{ is rational}\\ 0,&x\text{ is irrational}\end{cases} $$ How is it called and why is it so? I'm really not asking for a proof since I fear it would be too complicated for me to understand, but rather for an ""intuition"".","I stumbled upon this ""relation"" (is the name correct?): $$ \lim_{m \to \infty} \lim_{n \to \infty} \cos^{2n}(m! \pi x) = \begin{cases} 1,&x\text{ is rational}\\ 0,&x\text{ is irrational}\end{cases} $$ How is it called and why is it so? I'm really not asking for a proof since I fear it would be too complicated for me to understand, but rather for an ""intuition"".",,"['real-analysis', 'limits', 'trigonometry', 'irrational-numbers', 'rational-numbers']"
48,"Compute $ I_{n}=\int_{-\infty}^\infty \frac{1-\cos x \cos 2x \cdots \cos nx}{x^2}\,dx$",Compute," I_{n}=\int_{-\infty}^\infty \frac{1-\cos x \cos 2x \cdots \cos nx}{x^2}\,dx","I'm very curious about the ways I may compute the following integral. I'd be very glad to know your approaching ways for this integral: $$ I_{n} \equiv \int_{-\infty}^\infty {1-\cos\left(x\right)\cos\left(2x\right)\ldots\cos\left(nx\right) \over x^{2}} \,{\rm d}x $$ According to W|A, $I_1=\pi$, $I_2=2\pi$, $I_3=3\pi$, and one may be tempted to think that it's about an arithmetical progression here, but things change (unfortunately) from $I_4$ that is $\frac{9 \pi}{2}$. This problem came to my mind when I was working on a different problem.","I'm very curious about the ways I may compute the following integral. I'd be very glad to know your approaching ways for this integral: $$ I_{n} \equiv \int_{-\infty}^\infty {1-\cos\left(x\right)\cos\left(2x\right)\ldots\cos\left(nx\right) \over x^{2}} \,{\rm d}x $$ According to W|A, $I_1=\pi$, $I_2=2\pi$, $I_3=3\pi$, and one may be tempted to think that it's about an arithmetical progression here, but things change (unfortunately) from $I_4$ that is $\frac{9 \pi}{2}$. This problem came to my mind when I was working on a different problem.",,"['calculus', 'real-analysis', 'integration']"
49,Longest 'increasing' path inside a square,Longest 'increasing' path inside a square,,"Say you have a function $f: [0,1] \to [0,1]$ such that: $f(0)=0$ $f(1)=1$ Let's just say that $f$ is everywhere differentiable (not really necessary) $f$ is increasing i.e order preserving Now what I'm trying to show is that the 'length' of such a function (i.e of it's graph) is always longer than $\sqrt{2}$ and always shorter than $2$ . The lower bound is pretty straightforward (shortest path is the diagonal of the unit square). Now I'm struggling with the upper bound. I'm convinced it should be trivial, but cannot for the life of me find a proof. Would greatly appreciate any help on the matter. Cheers!","Say you have a function such that: Let's just say that is everywhere differentiable (not really necessary) is increasing i.e order preserving Now what I'm trying to show is that the 'length' of such a function (i.e of it's graph) is always longer than and always shorter than . The lower bound is pretty straightforward (shortest path is the diagonal of the unit square). Now I'm struggling with the upper bound. I'm convinced it should be trivial, but cannot for the life of me find a proof. Would greatly appreciate any help on the matter. Cheers!","f: [0,1] \to [0,1] f(0)=0 f(1)=1 f f \sqrt{2} 2","['real-analysis', 'geometry', 'inequality']"
50,"For an irrational number $\alpha$, prove that the set $\{a+b\alpha: a,b\in \mathbb{Z}\}$ is dense in $\mathbb R$ [closed]","For an irrational number , prove that the set  is dense in  [closed]","\alpha \{a+b\alpha: a,b\in \mathbb{Z}\} \mathbb R","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I am not able to prove that this set is dense in $\mathbb{R}$ . Will be pleased if you help in a easiest way. $A=\{a+b\alpha: a,b\in \mathbb{Z}\}$ where $\alpha$ is a fixed irrational number.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I am not able to prove that this set is dense in . Will be pleased if you help in a easiest way. where is a fixed irrational number.","\mathbb{R} A=\{a+b\alpha: a,b\in \mathbb{Z}\} \alpha","['real-analysis', 'irrational-numbers', 'pigeonhole-principle']"
51,Lebesgue Integral of Non-Measurable Function,Lebesgue Integral of Non-Measurable Function,,"In what follows I'm only considering positive real valued functions. Everywhere I look about the definition of the Lebesgue integral it is required to consider a measurable function. Why do we not define the integral for non-measurable functions? From what I see we require measurablility of the simple functions that approximate f, not f itself. The definition I'm considering is given a measure space $X$ with measure $\mu$ and a measurable function $f$ we define $$ \int_E f \, \mathrm{d}\mu = \sup_{s \in S} \int_X s \,\mathrm{d}\mu $$ where $S = \{ s : X \to [0, \infty) \mid 0 \le s \le f, s \text{ is simple, measurable} \}$. For example consider $\mathbb{R}$ with the sigma algebra $\varnothing, \mathbb{R}$ with measure $\mu$ given by $\mu(\varnothing) = 0, \mu(\mathbb{R}) = 1$ and consider $f = \chi_{[0,1]}$ then why can't we say that $$ \int_{\mathbb{R}} f \,\mathrm{d} \mu = 0 $$ (since the only measurable simple function such that $0\le s \le f$ is $s = 0$) which would follow the definition above? Is this not well defined? In general I'm struggling to see why measurable functions (other than measurable simple functions) are used.","In what follows I'm only considering positive real valued functions. Everywhere I look about the definition of the Lebesgue integral it is required to consider a measurable function. Why do we not define the integral for non-measurable functions? From what I see we require measurablility of the simple functions that approximate f, not f itself. The definition I'm considering is given a measure space $X$ with measure $\mu$ and a measurable function $f$ we define $$ \int_E f \, \mathrm{d}\mu = \sup_{s \in S} \int_X s \,\mathrm{d}\mu $$ where $S = \{ s : X \to [0, \infty) \mid 0 \le s \le f, s \text{ is simple, measurable} \}$. For example consider $\mathbb{R}$ with the sigma algebra $\varnothing, \mathbb{R}$ with measure $\mu$ given by $\mu(\varnothing) = 0, \mu(\mathbb{R}) = 1$ and consider $f = \chi_{[0,1]}$ then why can't we say that $$ \int_{\mathbb{R}} f \,\mathrm{d} \mu = 0 $$ (since the only measurable simple function such that $0\le s \le f$ is $s = 0$) which would follow the definition above? Is this not well defined? In general I'm struggling to see why measurable functions (other than measurable simple functions) are used.",,"['real-analysis', 'measure-theory']"
52,Fundamental Theorem of Calculus,Fundamental Theorem of Calculus,,"The Fundamental Theorem of Calculus says the following: Theorem. If $f$ is the derivative of $F$ at every point on $[a,b]$ , then under suitable hypotheses we have that $$\int_{a}^{b} f(t) \ dt = F(b)-F(a)$$ Theorem. If $f$ is integrable on $[a,b]$ , then under suitable hypotheses we have that $$\frac{d}{dx} \int_{a}^{x} f(t) \ dt = f(x)$$ I am trying to put myself in the shoes of Poisson, Cauchy and Riemann. The first theorem is basically saying that to find the area under a curve, we need to find any anti-derivative and evaluate it at the endpoints? The second theorem is saying that we can view the integral as a function of $x$ and take its derivative to get $f(x)$ . Wasn't the goal of Poisson, Cauchy and Riemann to find the area under a curve? So they hypothesized the first theorem and then only later proposed the second theorem? Both theorems deal with finding the area under a curve (i.e. they are equivalent)? Do these theorems still hold under other types of integration (i.e. the Lebesgue integral)?","The Fundamental Theorem of Calculus says the following: Theorem. If is the derivative of at every point on , then under suitable hypotheses we have that Theorem. If is integrable on , then under suitable hypotheses we have that I am trying to put myself in the shoes of Poisson, Cauchy and Riemann. The first theorem is basically saying that to find the area under a curve, we need to find any anti-derivative and evaluate it at the endpoints? The second theorem is saying that we can view the integral as a function of and take its derivative to get . Wasn't the goal of Poisson, Cauchy and Riemann to find the area under a curve? So they hypothesized the first theorem and then only later proposed the second theorem? Both theorems deal with finding the area under a curve (i.e. they are equivalent)? Do these theorems still hold under other types of integration (i.e. the Lebesgue integral)?","f F [a,b] \int_{a}^{b} f(t) \ dt = F(b)-F(a) f [a,b] \frac{d}{dx} \int_{a}^{x} f(t) \ dt = f(x) x f(x)","['real-analysis', 'soft-question']"
53,Is it possible to turn this geometric demonstration of the area of a circle into a rigorous proof?,Is it possible to turn this geometric demonstration of the area of a circle into a rigorous proof?,,"In this New York Times article, Steven Strogatz offers the following argument for why the area of a circle is $\pi r^2$ . Suppose you divide the circle into an even number of pizza slices of equal arc length, and wedge them together in such a way that half of the slices have an arc at the bottom, and half of the slices have an arc at the top: Then, the base of the shape created has length $\pi r$ , and its height is $r$ . As the number of slices tends to infinity, the limiting case is that of a rectangle: Hence, the area of the circle is $\pi r^2$ . Although this argument is very geometrically appealing, it also seems fairly difficult to make rigorous. I suppose the most challenging part is showing that the base of the shape really does become arbitrarily flat, and its height becomes arbitrarily vertical, if that makes sense. How might we convert this intuitive argument into a rigorous proof?","In this New York Times article, Steven Strogatz offers the following argument for why the area of a circle is . Suppose you divide the circle into an even number of pizza slices of equal arc length, and wedge them together in such a way that half of the slices have an arc at the bottom, and half of the slices have an arc at the top: Then, the base of the shape created has length , and its height is . As the number of slices tends to infinity, the limiting case is that of a rectangle: Hence, the area of the circle is . Although this argument is very geometrically appealing, it also seems fairly difficult to make rigorous. I suppose the most challenging part is showing that the base of the shape really does become arbitrarily flat, and its height becomes arbitrarily vertical, if that makes sense. How might we convert this intuitive argument into a rigorous proof?",\pi r^2 \pi r r \pi r^2,"['real-analysis', 'calculus', 'geometry', 'circles']"
54,How to prove that $\sum\limits_{n=1}^\infty\frac{(n-1)!}{n\prod\limits_{i=1}^n(a+i)}=\sum\limits_{k=1}^\infty \frac{1}{(a+k)^2}$ for $a>-1$?,How to prove that  for ?,\sum\limits_{n=1}^\infty\frac{(n-1)!}{n\prod\limits_{i=1}^n(a+i)}=\sum\limits_{k=1}^\infty \frac{1}{(a+k)^2} a>-1,"A problem on my (last week's) real analysis homework boiled down to proving that, for $a>-1$, $$\sum_{n=1}^\infty\frac{(n-1)!}{n\prod\limits_{i=1}^n(a+i)}=\sum_{k=1}^\infty \frac{1}{(a+k)^2}.$$ Mathematica confirms this is true, but I couldn't even prove the convergence of the original series (the one on the left), much less demonstrate that it equaled this other sum; the ratio test is inconclusive, and the root test and others seem hopeless. It was (and is) quite a  frustrating problem. Can someone explain how to go about tackling this?","A problem on my (last week's) real analysis homework boiled down to proving that, for $a>-1$, $$\sum_{n=1}^\infty\frac{(n-1)!}{n\prod\limits_{i=1}^n(a+i)}=\sum_{k=1}^\infty \frac{1}{(a+k)^2}.$$ Mathematica confirms this is true, but I couldn't even prove the convergence of the original series (the one on the left), much less demonstrate that it equaled this other sum; the ratio test is inconclusive, and the root test and others seem hopeless. It was (and is) quite a  frustrating problem. Can someone explain how to go about tackling this?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
55,Why isn't Dominated Convergence Theorem taught in intro analysis,Why isn't Dominated Convergence Theorem taught in intro analysis,,In a course based off a book like Rudin's Principles of Mathematical Analysis that does non-measure theoretic analysis why isn't dominated convergence taught? It would be useful since continuous functions are Lebesgue measurable. Is there not a way to prove a non-measure theoretic version of this statement?,In a course based off a book like Rudin's Principles of Mathematical Analysis that does non-measure theoretic analysis why isn't dominated convergence taught? It would be useful since continuous functions are Lebesgue measurable. Is there not a way to prove a non-measure theoretic version of this statement?,,['real-analysis']
56,$L^1$ convergence gives a pointwise convergent subsequence,convergence gives a pointwise convergent subsequence,L^1,"I have been reading Terry Tao's notes on Real Analysis and there's a part he just says, but does not really explain, so I am wondering if someone here would. The notes are http://terrytao.wordpress.com/2010/10/02/245a-notes-4-modes-of-convergence/ and my particular question is from Section 4, Corollary 3. It goes as follows, Let $f_n \rightarrow f$ in $L^1$ then there exists a sub sequence $(f_{n_j}) \subset (f_n)$ such that $f_{n_j} \rightarrow f$ pointwise a.e. Moreover $(f_{n_j})$ converges almost uniformly to $f$. The proof he gives is simply that since $\|f_n-f\|_1 \rightarrow 0$ as $n \rightarrow \infty$ we can pick a sub sequence such that $\|f_{n_j}-f\|_1<2^{-j}$ which is enough to show pointwise a.e and almost uniform convergence. But what allows you to pick such a sub sequence is it maybe some Cauchy property or is it some weird construction? Then how do you go from that to pointwise a.e and even almost uniform convergence. I am assuming that for almost uniform, you do something similar to Egorov's theorem without the assumption the domain of $f$ has finite measure. Also I am aware that if you get almost uniform, you immediately have pointwise a.e, but I'd like to see how to get to both. Thank you.","I have been reading Terry Tao's notes on Real Analysis and there's a part he just says, but does not really explain, so I am wondering if someone here would. The notes are http://terrytao.wordpress.com/2010/10/02/245a-notes-4-modes-of-convergence/ and my particular question is from Section 4, Corollary 3. It goes as follows, Let $f_n \rightarrow f$ in $L^1$ then there exists a sub sequence $(f_{n_j}) \subset (f_n)$ such that $f_{n_j} \rightarrow f$ pointwise a.e. Moreover $(f_{n_j})$ converges almost uniformly to $f$. The proof he gives is simply that since $\|f_n-f\|_1 \rightarrow 0$ as $n \rightarrow \infty$ we can pick a sub sequence such that $\|f_{n_j}-f\|_1<2^{-j}$ which is enough to show pointwise a.e and almost uniform convergence. But what allows you to pick such a sub sequence is it maybe some Cauchy property or is it some weird construction? Then how do you go from that to pointwise a.e and even almost uniform convergence. I am assuming that for almost uniform, you do something similar to Egorov's theorem without the assumption the domain of $f$ has finite measure. Also I am aware that if you get almost uniform, you immediately have pointwise a.e, but I'd like to see how to get to both. Thank you.",,"['real-analysis', 'functional-analysis', 'convergence-divergence']"
57,Does a closed and bounded set in $\mathbb{R}$ necessarily contain its supremum and infimum?,Does a closed and bounded set in  necessarily contain its supremum and infimum?,\mathbb{R},"Does a nonempty closed and bounded set in $\mathbb{R}$ necessarily contain its supremum and infimum? My thought process: Closed and bounded means compact in $\mathbb{R}$, and a continuous function on a compact set admits a minimum and maximum. The identity function is continuous, so it admits a minimum and maximum on any compact set, which must be the minimum and maximum of the compact set. Is this valid? Is there a simpler proof/disproof?","Does a nonempty closed and bounded set in $\mathbb{R}$ necessarily contain its supremum and infimum? My thought process: Closed and bounded means compact in $\mathbb{R}$, and a continuous function on a compact set admits a minimum and maximum. The identity function is continuous, so it admits a minimum and maximum on any compact set, which must be the minimum and maximum of the compact set. Is this valid? Is there a simpler proof/disproof?",,"['real-analysis', 'general-topology']"
58,$f: \mathbb{R} \to \mathbb{R}$ that takes each value in $\mathbb{R}$ twice,that takes each value in  twice,f: \mathbb{R} \to \mathbb{R} \mathbb{R},Does there exist a continuous function $f: \mathbb{R} \to \mathbb{R}$ that takes each value in $\mathbb{R}$ exactly two times?,Does there exist a continuous function $f: \mathbb{R} \to \mathbb{R}$ that takes each value in $\mathbb{R}$ exactly two times?,,"['real-analysis', 'functions']"
59,Local maxima of Legendre polynomials,Local maxima of Legendre polynomials,,"When I plotted the (normalized) Legendre polynoials, I couldn't help noticing that all the local maxima lay on a really nice curve: What is the equation of the curve (and how can we arrive to that equation)?","When I plotted the (normalized) Legendre polynoials, I couldn't help noticing that all the local maxima lay on a really nice curve: What is the equation of the curve (and how can we arrive to that equation)?",,"['real-analysis', 'special-functions', 'orthogonal-polynomials']"
60,Differentiable function has measurable derivative?,Differentiable function has measurable derivative?,,"Let $f:[0,T] \to \mathbb{R}$ be a differentiable function. Is it true that $f'$ is measurable? If so, is this also true if $f$ is differentiable almost everywhere? Sorry for lack of effort but I don't have any clue about the answer.","Let $f:[0,T] \to \mathbb{R}$ be a differentiable function. Is it true that $f'$ is measurable? If so, is this also true if $f$ is differentiable almost everywhere? Sorry for lack of effort but I don't have any clue about the answer.",,"['real-analysis', 'measure-theory', 'derivatives']"
61,"If $\,\lim_{n\to\infty}f(nx)\,$ exists, for all $x\in\mathbb R$, then so does $\,\lim_{x\to\infty}f(x)\,$","If  exists, for all , then so does","\,\lim_{n\to\infty}f(nx)\, x\in\mathbb R \,\lim_{x\to\infty}f(x)\,","Let $\,f:\mathbb{R}\to\mathbb{R}$. The following limit exists for all $x \in \mathbb{R}$:  $$\lim_{n\to ∞} f(nx) ,$$ where $n \in \mathbb N$. Is it correct that: $$\lim_{x\to ∞} f(x) ,$$  exists if: a) $f$ any function, b) $f$ continuous on $\mathbb{R}$.","Let $\,f:\mathbb{R}\to\mathbb{R}$. The following limit exists for all $x \in \mathbb{R}$:  $$\lim_{n\to ∞} f(nx) ,$$ where $n \in \mathbb N$. Is it correct that: $$\lim_{x\to ∞} f(x) ,$$  exists if: a) $f$ any function, b) $f$ continuous on $\mathbb{R}$.",,"['calculus', 'real-analysis', 'limits', 'continuity']"
62,Is a bounded and continuous function uniformly continuous?,Is a bounded and continuous function uniformly continuous?,,"$f\colon(-1,1)\rightarrow \mathbb{R}$ is bounded and continuous does it mean that $f$ is uniformly continuous? Well, $f(x)=x\sin(1/x)$ does the job for counterexample? Please help!","$f\colon(-1,1)\rightarrow \mathbb{R}$ is bounded and continuous does it mean that $f$ is uniformly continuous? Well, $f(x)=x\sin(1/x)$ does the job for counterexample? Please help!",,"['real-analysis', 'examples-counterexamples', 'uniform-continuity']"
63,"What does ""test function"" mean?","What does ""test function"" mean?",,"I am trying to learn weak derivatives. In that, we call $\mathbb{C}^{\infty}_{c}$ functions as test functions and we use these functions in weak derivatives. I want to understand why these are called test functions and why the functions with these properties are needed. I have some idea about these but couldn't understand them properly. Also, I'll be happy if any one can suggest some good reference on this topic and Sobolev spaces.","I am trying to learn weak derivatives. In that, we call functions as test functions and we use these functions in weak derivatives. I want to understand why these are called test functions and why the functions with these properties are needed. I have some idea about these but couldn't understand them properly. Also, I'll be happy if any one can suggest some good reference on this topic and Sobolev spaces.",\mathbb{C}^{\infty}_{c},"['real-analysis', 'functional-analysis', 'sobolev-spaces', 'distribution-theory']"
64,Integration of a function with respect to another function.,Integration of a function with respect to another function.,,"What is the intuition/idea behind integration of a function with respect to another function? Say $$\int f(x)d(g(x)) \;\;\;\;\;?$$ or may be a more particular example $$\int x^2d(x^3)$$ My concern is not at the level of problem solving. To solve we could simply substitute $u=x^3$ and then $x^2=u^{2/3}$. My concern is rather about what meaning physical/geometrical does this impart? ADDED if you ask what kind of meaning I seek, integration of a function w.r.t a variable gives the area under the curve and above x-axis.","What is the intuition/idea behind integration of a function with respect to another function? Say $$\int f(x)d(g(x)) \;\;\;\;\;?$$ or may be a more particular example $$\int x^2d(x^3)$$ My concern is not at the level of problem solving. To solve we could simply substitute $u=x^3$ and then $x^2=u^{2/3}$. My concern is rather about what meaning physical/geometrical does this impart? ADDED if you ask what kind of meaning I seek, integration of a function w.r.t a variable gives the area under the curve and above x-axis.",,['real-analysis']
65,convergence in $L^p$ implies convergence in measure,convergence in  implies convergence in measure,L^p,"I am trying to show that if $f_n$ converges to $f$ in $L^p(X,\mu)$ then $f_n\to f$ in measure, where $1\le p \le \infty$ . Here is my attempt for $p\ge 1$ : Let $\varepsilon>0$ and define $A_{n,\varepsilon}=\lbrace x: \vert f_n(x)-f(x) \vert \ge \varepsilon\rbrace$ . I want to show $\mu (A_{n,\varepsilon})\to 0$ . $\Vert f_n-f \Vert_p=\left(\int _X \vert f_n-f\vert ^p\right)^{1/p}\ge \left(\int _{A_{n,\varepsilon}}\vert f_n-f\vert ^p\right)^{1/p}\ge \varepsilon \mu (A_{n,\varepsilon})^{1/p}$ so that $\mu (A_{n,\varepsilon})\le \left(\frac{\Vert f_n-f\Vert_p }{\varepsilon}\right)^{p}$ and the RHS tends to $0$ as $f_n\to f$ in the $L^p$ norm. How can I deal with the case $p=\infty$ ?","I am trying to show that if converges to in then in measure, where . Here is my attempt for : Let and define . I want to show . so that and the RHS tends to as in the norm. How can I deal with the case ?","f_n f L^p(X,\mu) f_n\to f 1\le p \le \infty p\ge 1 \varepsilon>0 A_{n,\varepsilon}=\lbrace x: \vert f_n(x)-f(x) \vert \ge \varepsilon\rbrace \mu (A_{n,\varepsilon})\to 0 \Vert f_n-f \Vert_p=\left(\int _X \vert f_n-f\vert ^p\right)^{1/p}\ge \left(\int _{A_{n,\varepsilon}}\vert f_n-f\vert ^p\right)^{1/p}\ge \varepsilon \mu (A_{n,\varepsilon})^{1/p} \mu (A_{n,\varepsilon})\le \left(\frac{\Vert f_n-f\Vert_p }{\varepsilon}\right)^{p} 0 f_n\to f L^p p=\infty","['real-analysis', 'analysis', 'solution-verification', 'lp-spaces']"
66,An integral by O. Furdui $\int_0^1 \log^2(\sqrt{1+x}-\sqrt{1-x}) \ dx$,An integral by O. Furdui,\int_0^1 \log^2(\sqrt{1+x}-\sqrt{1-x}) \ dx,"The following integral was proposed in a paper by O. Furdui, namely $$\int_0^1 \log^2(\sqrt{1+x}-\sqrt{1-x}) \ dx$$ and then the generalization $$\int_0^1 \log^2(\sqrt[k]{1+x}-\sqrt[k]{1-x}) \ dx$$ As regards the first integral, my approach was to combine the integration by parts and the variable change,  that is $\sqrt{1+x}-\sqrt{1-x} \mapsto x$, and then we get another integral that can be decomposed into $2$ integrals where the hardest part gets reduced to computing  $$\int_0^{\sqrt{2}/2}\frac{\arcsin(x)}{x} \ dx$$ that is pretty straightforward by variable change combined with the integration by parts. And here is a supplementary question $$\int_0^1 \log^3(\sqrt{1+x}-\sqrt{1-x}) \ dx$$ I'd be also interested in other approaching ways if possible.","The following integral was proposed in a paper by O. Furdui, namely $$\int_0^1 \log^2(\sqrt{1+x}-\sqrt{1-x}) \ dx$$ and then the generalization $$\int_0^1 \log^2(\sqrt[k]{1+x}-\sqrt[k]{1-x}) \ dx$$ As regards the first integral, my approach was to combine the integration by parts and the variable change,  that is $\sqrt{1+x}-\sqrt{1-x} \mapsto x$, and then we get another integral that can be decomposed into $2$ integrals where the hardest part gets reduced to computing  $$\int_0^{\sqrt{2}/2}\frac{\arcsin(x)}{x} \ dx$$ that is pretty straightforward by variable change combined with the integration by parts. And here is a supplementary question $$\int_0^1 \log^3(\sqrt{1+x}-\sqrt{1-x}) \ dx$$ I'd be also interested in other approaching ways if possible.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'closed-form']"
67,"What's the behaviour of $I_n=\int_0^1\left(\frac1{\log x}+\frac1{1-x}\right)^ndx$, as $n \to \infty$?","What's the behaviour of , as ?",I_n=\int_0^1\left(\frac1{\log x}+\frac1{1-x}\right)^ndx n \to \infty,"Set    $$ I_n :=\int_0^1\left(\frac{1}{\log x} + \frac{1}{1-x}\right)^n \:\mathrm{d}x \qquad n=1,2,3,\cdots. $$ We have $$I_1 =\gamma, \quad I_2 =\log (2 \pi) - \frac 32, \quad I_3 = 6 \log A - \frac{31}{24}, \quad I_4 = 2 \log A + \frac{5 \zeta(3)}{2\pi^{2}}- \frac{49}{72}, \quad ...$$ where $A$ is the Glaisher-Kinkelin constant defined by $$ \begin{equation} \displaystyle A :=\lim_{n\to\infty}\frac{1^22^2\cdots n^n}{e^{-n^2/4}n^{\frac{n^2+n}{2}+\frac{1}{12}}}=1.28242712\cdots. \end{equation} $$ I wonder if there is a ""simple"" equivalent for $I_n$ as $n$ tends to $+\infty$? Edit . I had designed the following integral $$\displaystyle \int_0^1\left(\frac{1}{\log x} + \frac{1}{1-x}\right)^2 \mathrm{d}x$$ which I submitted to American Mathematical Monthly ( March 2012 , problem 11629), the problem was then spread and came in this forum with different interesting solutions (I) . An interesting general formula for $I_n$ has been found (II) , but I don't think the latter formula is tractable for the above question on asymptotics .","Set    $$ I_n :=\int_0^1\left(\frac{1}{\log x} + \frac{1}{1-x}\right)^n \:\mathrm{d}x \qquad n=1,2,3,\cdots. $$ We have $$I_1 =\gamma, \quad I_2 =\log (2 \pi) - \frac 32, \quad I_3 = 6 \log A - \frac{31}{24}, \quad I_4 = 2 \log A + \frac{5 \zeta(3)}{2\pi^{2}}- \frac{49}{72}, \quad ...$$ where $A$ is the Glaisher-Kinkelin constant defined by $$ \begin{equation} \displaystyle A :=\lim_{n\to\infty}\frac{1^22^2\cdots n^n}{e^{-n^2/4}n^{\frac{n^2+n}{2}+\frac{1}{12}}}=1.28242712\cdots. \end{equation} $$ I wonder if there is a ""simple"" equivalent for $I_n$ as $n$ tends to $+\infty$? Edit . I had designed the following integral $$\displaystyle \int_0^1\left(\frac{1}{\log x} + \frac{1}{1-x}\right)^2 \mathrm{d}x$$ which I submitted to American Mathematical Monthly ( March 2012 , problem 11629), the problem was then spread and came in this forum with different interesting solutions (I) . An interesting general formula for $I_n$ has been found (II) , but I don't think the latter formula is tractable for the above question on asymptotics .",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'asymptotics']"
68,A Tough Series: $\sum_{n=0}^\infty2^na_{n+1}\sqrt{a_n}=\frac{\Gamma^2(\frac14)-4\cdot\Gamma^2(\frac34)}{8\sqrt{2\pi}}$,A Tough Series:,\sum_{n=0}^\infty2^na_{n+1}\sqrt{a_n}=\frac{\Gamma^2(\frac14)-4\cdot\Gamma^2(\frac34)}{8\sqrt{2\pi}},"If $\displaystyle a_0=\frac12$ and $\displaystyle a_{n+1}=\frac{1-\sqrt{1-a_n}}{1+\sqrt{1-2a_n}}$ , show that $$\sum_{n=0}^\infty2^na_{n+1}\sqrt{a_n}=\frac{\Gamma^2(\frac14)-4\cdot\Gamma^2(\frac34)}{8\sqrt{2\pi}}$$ The closed form for this series involves the square of gamma function, hence I try to connect it with integrals, which requires us first to find the explicit form from the recursion equation. But this recusion equation is highly non-linear. I try to multiply $1-\sqrt{1-2a_n}$ to rationalize the denominator but seems no help. I also tried some non-linear sub, such as $$\tan(x-y)=\frac{\tan x-\tan y}{1+\tan x\tan y}$$ where $x=\frac\pi4, \tan y=\sqrt{1-a_n}$ , but the $2a_n$ term inside the square root kills this attempt. If let $a_n=\sin^2\theta_n$ , then we get $$1-\cos^2\theta_{n+1}=\sin^2\theta_{n+1}=\frac{1-\cos\theta_n}{1+\sqrt{\cos2\theta_n}}$$ Is there any hint? Thank you!","If and , show that The closed form for this series involves the square of gamma function, hence I try to connect it with integrals, which requires us first to find the explicit form from the recursion equation. But this recusion equation is highly non-linear. I try to multiply to rationalize the denominator but seems no help. I also tried some non-linear sub, such as where , but the term inside the square root kills this attempt. If let , then we get Is there any hint? Thank you!","\displaystyle a_0=\frac12 \displaystyle a_{n+1}=\frac{1-\sqrt{1-a_n}}{1+\sqrt{1-2a_n}} \sum_{n=0}^\infty2^na_{n+1}\sqrt{a_n}=\frac{\Gamma^2(\frac14)-4\cdot\Gamma^2(\frac34)}{8\sqrt{2\pi}} 1-\sqrt{1-2a_n} \tan(x-y)=\frac{\tan x-\tan y}{1+\tan x\tan y} x=\frac\pi4, \tan y=\sqrt{1-a_n} 2a_n a_n=\sin^2\theta_n 1-\cos^2\theta_{n+1}=\sin^2\theta_{n+1}=\frac{1-\cos\theta_n}{1+\sqrt{\cos2\theta_n}}","['real-analysis', 'integration', 'sequences-and-series', 'recurrence-relations', 'elliptic-functions']"
69,Elementary proof that $|x|^p$ is convex.,Elementary proof that  is convex.,|x|^p,I'm writing some notes about analysis and want to use the fact that $|x|^p$ is convex for every $p>1$ to prove Minkowski's inequality. However I didn't wrote anything about derivatives nor limits yet. Is there a simple way to prove this? EDIT: The only non-trivial inequalities proved yet are the triangular inequality and the Cauchy-Schwarz inequality.,I'm writing some notes about analysis and want to use the fact that $|x|^p$ is convex for every $p>1$ to prove Minkowski's inequality. However I didn't wrote anything about derivatives nor limits yet. Is there a simple way to prove this? EDIT: The only non-trivial inequalities proved yet are the triangular inequality and the Cauchy-Schwarz inequality.,,"['real-analysis', 'inequality']"
70,A magnificent series for $\pi-333/106$,A magnificent series for,\pi-333/106,"Stated here without proof is the magnificent series $$\frac{48}{371} \sum_{k=0}^\infty \frac{118720 k^2+762311 k+1409424}{(4 k+9) (4 k+11) (4 k+13) (4 k+15) (4 k+17) (4 k+19) (4 k+21) (4 k+23)} \\=\pi-\frac{333}{106},$$ which proves that $\pi>333/106$ . I can only assume that the series is proven using the integral $$\pi-\frac{333}{106}=\frac{1}{530}\int_0^1 \frac{x^5(1-x)^6(197+462x^2)}{1+x^2}dx.$$ My attempts have been so far to split up the integral as $$\begin{align} 530J&=\int_0^1 \frac{x^5(1-x)^6(197+462x^2)}{1+x^2}dx\\ &=197\int_0^1 \frac{x^5(1-x)^6}{1+x^2}dx+462\int_0^1\frac{x^{7}(1-x)^6}{1+x^2}dx\\ &=197J_1+462J_2. \end{align}$$ Each remaining integral is turned into a series with $$\frac1{1+x^2}=\sum_{n\ge0}(-1)^n x^{2n}$$ so we have two series of the form $$f(p)=\sum_{n\ge0}(-1)^n\int_0^1 x^{p+2n}(1-x)^6dx=720\sum_{n\ge0}(-1)^n\frac{(p+2n)!}{(p+2n+7)!}.$$ Each factorial term is rewritten as $$\frac{s!}{(s+7)!}=\frac1{(s+1)(s+2)(s+3)(s+4)(s+5)(s+6)(s+7)},$$ so that $$f(p)=720\sum_{n\ge0}\frac{(-1)^n}{\prod_{k=1}^{7}(2n+p+k)}.$$ Then $$J_1=f(5)\\ J_2=f(7).$$ But how does one get from $530J=197f(5)+462f(7)$ to the series in question? Furthermore, how do we prove that $J=\pi-333/106$ ? I would assume that one would at some point use the binomial theorem then be left with a bunch of integrals like $$\int_0^1\frac{x^qdx}{1+x^2}$$ which I suppose are evaluable in terms of $\pi$ , but it seems like a great deal of cancellation/simplification would have to occur and I do not immediately see where this would happen. There has to be an easier way. Thanks!","Stated here without proof is the magnificent series which proves that . I can only assume that the series is proven using the integral My attempts have been so far to split up the integral as Each remaining integral is turned into a series with so we have two series of the form Each factorial term is rewritten as so that Then But how does one get from to the series in question? Furthermore, how do we prove that ? I would assume that one would at some point use the binomial theorem then be left with a bunch of integrals like which I suppose are evaluable in terms of , but it seems like a great deal of cancellation/simplification would have to occur and I do not immediately see where this would happen. There has to be an easier way. Thanks!","\frac{48}{371} \sum_{k=0}^\infty \frac{118720 k^2+762311 k+1409424}{(4 k+9) (4 k+11) (4 k+13) (4 k+15) (4 k+17) (4 k+19) (4 k+21) (4 k+23)} \\=\pi-\frac{333}{106}, \pi>333/106 \pi-\frac{333}{106}=\frac{1}{530}\int_0^1 \frac{x^5(1-x)^6(197+462x^2)}{1+x^2}dx. \begin{align}
530J&=\int_0^1 \frac{x^5(1-x)^6(197+462x^2)}{1+x^2}dx\\
&=197\int_0^1 \frac{x^5(1-x)^6}{1+x^2}dx+462\int_0^1\frac{x^{7}(1-x)^6}{1+x^2}dx\\
&=197J_1+462J_2.
\end{align} \frac1{1+x^2}=\sum_{n\ge0}(-1)^n x^{2n} f(p)=\sum_{n\ge0}(-1)^n\int_0^1 x^{p+2n}(1-x)^6dx=720\sum_{n\ge0}(-1)^n\frac{(p+2n)!}{(p+2n+7)!}. \frac{s!}{(s+7)!}=\frac1{(s+1)(s+2)(s+3)(s+4)(s+5)(s+6)(s+7)}, f(p)=720\sum_{n\ge0}\frac{(-1)^n}{\prod_{k=1}^{7}(2n+p+k)}. J_1=f(5)\\ J_2=f(7). 530J=197f(5)+462f(7) J=\pi-333/106 \int_0^1\frac{x^qdx}{1+x^2} \pi","['real-analysis', 'integration', 'sequences-and-series', 'pi', 'constants']"
71,Writing real numbers as sums of zeros and ones,Writing real numbers as sums of zeros and ones,,"Call a number $\delta\in(0,1)$ ""good"" if it satisfies the following property: Every real number $a\in(0,1)$ can be written as an infinite sum of the form:   $$a = \sum_{i=1}^\infty \delta^i a_i$$   where $a_i\in\{0,1\}$. What numbers are good? I know that $1/2$ is good, since when $\delta=1/2$, the $a_i$ are just the digits in binary representation of $a$. On the other hand, $1/3$ is not good, since in order to represent a real number in ternary, we also need the digit 2. It is easy to prove that every $\delta<1/2$ is not good, since the maximum number that can possibly be represented is $\frac{\delta}{1-\delta}$, and it is strictly smaller than 1. My conjecture is that every $\delta\in [1/2,1)$ is good. Is this true?","Call a number $\delta\in(0,1)$ ""good"" if it satisfies the following property: Every real number $a\in(0,1)$ can be written as an infinite sum of the form:   $$a = \sum_{i=1}^\infty \delta^i a_i$$   where $a_i\in\{0,1\}$. What numbers are good? I know that $1/2$ is good, since when $\delta=1/2$, the $a_i$ are just the digits in binary representation of $a$. On the other hand, $1/3$ is not good, since in order to represent a real number in ternary, we also need the digit 2. It is easy to prove that every $\delta<1/2$ is not good, since the maximum number that can possibly be represented is $\frac{\delta}{1-\delta}$, and it is strictly smaller than 1. My conjecture is that every $\delta\in [1/2,1)$ is good. Is this true?",,"['real-analysis', 'sequences-and-series']"
72,Does there exist a real function with domain $\Bbb{R}$ such that $f'(x)>0$ and $f''(x)+(f'(x))^2<0$ for all $x$?,Does there exist a real function with domain  such that  and  for all ?,\Bbb{R} f'(x)>0 f''(x)+(f'(x))^2<0 x,"Does there exist a real function $f(x)$ that satisfies the following properties? its domain is $\mathbb{R}$ $f'(x) > 0$ for all $x$ $f''(x) + (f'(x))^2 < 0$ for all $x$ The log function $\ln(x)$ gives some idea about conditions 2 and 3.  But for now, I did not find any example. Besides, I want to find a non-linear differential function $f(x)$ defined on $\mathbb{R}$ and that is: strictly increasing (quasi)-concave I think this is easier than the previous one. By looking at the graph, I guess this function exists, but I did not find one explicitly. EDIT 1 Thank you for all the comments, especially by @mihaild. I have found an example for these questions. An example for the 2nd question is $-e^{-x}$ . And an example for the 1st question is borrowing the idea of the 2nd one, which is the following. If $f(x)=-e^{g(x)}$ , then $f'(x)=-g'(x)e^{g(x)}$ , $f''(x)=-g''(x)e^{g(x)}-(g'(x))^2e^{g(x)}$ , and $f''(x)+(f'(x))^2=-g''(x)e^{g(x)}$ . So we just need $g(x)$ is defined on $\mathbb{R}$ and satisfies: $g'(x) <0$ for all $x$ , $g''(x) >0$ for all $x$ Then simply we choose $g(x)=e^{-x}$ . So an example is $-e^{e^{-x}}$ . This is a wrong computation. I will try to fix it. $(f’(x))^2$ should be $(g’(x))^2 e^{2g(x)}$ . EDIT 2 After checking my previous example and reading all comments again. I see there does not exist a real function for the 1st question. (Thanks a lot for a remark in @mihaild comment.) Indeed, assume that there exists $f(x)$ that satisfies the 1st condition. Then $g(x)=e^{f(x)}$ is a positive, strictly increasing and strictly concave function. But by its concavity $$g(x) \leq g(0)+ g'(0)x.$$ Since $g'(0)>0$ , we have $\displaystyle\lim_{x \to - \infty}g(x) =-\infty$ , which contradicts its positivity.","Does there exist a real function that satisfies the following properties? its domain is for all for all The log function gives some idea about conditions 2 and 3.  But for now, I did not find any example. Besides, I want to find a non-linear differential function defined on and that is: strictly increasing (quasi)-concave I think this is easier than the previous one. By looking at the graph, I guess this function exists, but I did not find one explicitly. EDIT 1 Thank you for all the comments, especially by @mihaild. I have found an example for these questions. An example for the 2nd question is . And an example for the 1st question is borrowing the idea of the 2nd one, which is the following. If , then , , and . So we just need is defined on and satisfies: for all , for all Then simply we choose . So an example is . This is a wrong computation. I will try to fix it. should be . EDIT 2 After checking my previous example and reading all comments again. I see there does not exist a real function for the 1st question. (Thanks a lot for a remark in @mihaild comment.) Indeed, assume that there exists that satisfies the 1st condition. Then is a positive, strictly increasing and strictly concave function. But by its concavity Since , we have , which contradicts its positivity.",f(x) \mathbb{R} f'(x) > 0 x f''(x) + (f'(x))^2 < 0 x \ln(x) f(x) \mathbb{R} -e^{-x} f(x)=-e^{g(x)} f'(x)=-g'(x)e^{g(x)} f''(x)=-g''(x)e^{g(x)}-(g'(x))^2e^{g(x)} f''(x)+(f'(x))^2=-g''(x)e^{g(x)} g(x) \mathbb{R} g'(x) <0 x g''(x) >0 x g(x)=e^{-x} -e^{e^{-x}} (f’(x))^2 (g’(x))^2 e^{2g(x)} f(x) g(x)=e^{f(x)} g(x) \leq g(0)+ g'(0)x. g'(0)>0 \displaystyle\lim_{x \to - \infty}g(x) =-\infty,"['real-analysis', 'calculus']"
73,The Fibonacci sum $\sum_{n=0}^\infty \frac{1}{F_{2^n}}$ generalized,The Fibonacci sum  generalized,\sum_{n=0}^\infty \frac{1}{F_{2^n}},"The evaluation, $$\sum_{n=0}^\infty \frac{1}{F_{2^n}}=\frac{7-\sqrt{5}}{2}=\left(\frac{1-\sqrt{5}}{2}\right)^3+\left(\frac{1+\sqrt{5}}{2}\right)^2$$ was recently asked in a post by Chris here . I like generalizations, and it turns out this is not a unique feature of the Fibonacci numbers. If we use the Pell numbers $P_m = 1,2,5,12,29,70,\dots$ then the sum is also an algebraic number of deg 2. In general, it seems for any positive rational b , then, $$\sum_{n=0}^\infty \frac{1}{\frac{1}{\sqrt{b^2+4}}\left( \left(\frac{b+\sqrt{b^2+4}}{2}\right)^{2^n}-\left(\frac{b-\sqrt{b^2+4}}{2}\right)^{2^n}\right)}=1+\frac{2}{b}+\frac{b-\sqrt{b^2+4}}{2}$$ where Fibonacci numbers are just the case b = 1, the Pell numbers b = 2, and so on. (For negative rational b , then one just uses the positive case of $\pm\sqrt{b^2+4}$.) Anyone knows how to prove/disprove the conjectured evaluation?","The evaluation, $$\sum_{n=0}^\infty \frac{1}{F_{2^n}}=\frac{7-\sqrt{5}}{2}=\left(\frac{1-\sqrt{5}}{2}\right)^3+\left(\frac{1+\sqrt{5}}{2}\right)^2$$ was recently asked in a post by Chris here . I like generalizations, and it turns out this is not a unique feature of the Fibonacci numbers. If we use the Pell numbers $P_m = 1,2,5,12,29,70,\dots$ then the sum is also an algebraic number of deg 2. In general, it seems for any positive rational b , then, $$\sum_{n=0}^\infty \frac{1}{\frac{1}{\sqrt{b^2+4}}\left( \left(\frac{b+\sqrt{b^2+4}}{2}\right)^{2^n}-\left(\frac{b-\sqrt{b^2+4}}{2}\right)^{2^n}\right)}=1+\frac{2}{b}+\frac{b-\sqrt{b^2+4}}{2}$$ where Fibonacci numbers are just the case b = 1, the Pell numbers b = 2, and so on. (For negative rational b , then one just uses the positive case of $\pm\sqrt{b^2+4}$.) Anyone knows how to prove/disprove the conjectured evaluation?",,"['real-analysis', 'sequences-and-series', 'limits', 'fibonacci-numbers']"
74,Why is the support defined as a closure?,Why is the support defined as a closure?,,"In the definition of the support of a real function $f$ on $X$, why is it important to consider the closure of the set $S=\{x\in X:f(x)\neq0\}$ and not just $S$ itself? Why is the closure of $S$ called the ""support"" of $f$ or how did this name come about?","In the definition of the support of a real function $f$ on $X$, why is it important to consider the closure of the set $S=\{x\in X:f(x)\neq0\}$ and not just $S$ itself? Why is the closure of $S$ called the ""support"" of $f$ or how did this name come about?",,"['real-analysis', 'general-topology']"
75,Does every triangle satisfy $\frac{abc}{R^3} \ln \left(\frac{a}{R}\right)\ln \left(\frac{b}{R}\right)\ln \left(\frac{c}{R}\right) > -\ln 2$?,Does every triangle satisfy ?,\frac{abc}{R^3} \ln \left(\frac{a}{R}\right)\ln \left(\frac{b}{R}\right)\ln \left(\frac{c}{R}\right) > -\ln 2,"If $a,b,c$ are the sides of a triangle inscribed in a circle of radius $R$ . Is it true that $$ \frac{abc}{R^3} \ln \left(\frac{a}{R}\right)\ln\left(\frac{b}{R}\right)\ln \left(\frac{c}{R}\right)  > - \ln 2 \tag 1 $$ I ran a Monte Carlo simulation for the expression on the LHS and obtained a minima of $- 0.6923259$ after a billion samples. This value is slightly greater than $\ln2 \approx 0.6931$ . Can this proved? Update 1 : Using the insight from the comment made by @Anon we can show the slightly weaker result that LHS $> - \frac{4\ln^2 2}{e} \approx -0.7069951$ . If one side of the triangle is less than $R$ and the other two sides are greater than $R$ then the LHS will be negative. Hence we need to minimize $x \ln x$ for one side and maximize it for the other two sides. The minimum value of $x\ln x$ occurs at $x = \frac{R}{e}$ and for the triangle of circum radius $R$ , since no side can exceed $2R$ hence $x\ln x < \frac{2R}{R} \ln \frac{2R}{R} = 2\ln 2$ . Hence the LHS of $(1)$ must be at least $> - \frac{4\ln^2 2}{e}$ . Update 2 : An improvement of the estimate in update 1 is as follows. If one side of a triangle is $a = \frac{1}{e}$ , what is the maximum possible length of the other sides. Simple algebraic manipulations give $b = c = \sqrt{2 + \sqrt{4 - \frac{1}{e^2}}}$ and substituting these in $(1)$ we get $$ LHS \ge -\frac{1}{e}\left(\sqrt{1 - \frac{1}{2e}} + \sqrt{1 + \frac{1}{2e}}\right)^2 \ln^2 \left(\sqrt{1 - \frac{1}{2e}} + \sqrt{1 + \frac{1}{2e}}\right) \approx -0.692325 $$ But must be noted that this may not be the minima since the true minima may occur at $a \ne \frac{1}{e}$ . Update 3 : Under the assumption that the minima will occur when the triangle is isosceles, we can prove that the minima is $-0.62918$ as shown by K.defaoite. I have separate and proof which I have posted below.","If are the sides of a triangle inscribed in a circle of radius . Is it true that I ran a Monte Carlo simulation for the expression on the LHS and obtained a minima of after a billion samples. This value is slightly greater than . Can this proved? Update 1 : Using the insight from the comment made by @Anon we can show the slightly weaker result that LHS . If one side of the triangle is less than and the other two sides are greater than then the LHS will be negative. Hence we need to minimize for one side and maximize it for the other two sides. The minimum value of occurs at and for the triangle of circum radius , since no side can exceed hence . Hence the LHS of must be at least . Update 2 : An improvement of the estimate in update 1 is as follows. If one side of a triangle is , what is the maximum possible length of the other sides. Simple algebraic manipulations give and substituting these in we get But must be noted that this may not be the minima since the true minima may occur at . Update 3 : Under the assumption that the minima will occur when the triangle is isosceles, we can prove that the minima is as shown by K.defaoite. I have separate and proof which I have posted below.","a,b,c R 
\frac{abc}{R^3} \ln \left(\frac{a}{R}\right)\ln\left(\frac{b}{R}\right)\ln \left(\frac{c}{R}\right) 
> - \ln 2 \tag 1
 - 0.6923259 \ln2 \approx 0.6931 > - \frac{4\ln^2 2}{e} \approx -0.7069951 R R x \ln x x\ln x x = \frac{R}{e} R 2R x\ln x < \frac{2R}{R} \ln \frac{2R}{R} = 2\ln 2 (1) > - \frac{4\ln^2 2}{e} a = \frac{1}{e} b = c = \sqrt{2 + \sqrt{4 - \frac{1}{e^2}}} (1) 
LHS \ge -\frac{1}{e}\left(\sqrt{1 - \frac{1}{2e}} + \sqrt{1 + \frac{1}{2e}}\right)^2
\ln^2 \left(\sqrt{1 - \frac{1}{2e}} + \sqrt{1 + \frac{1}{2e}}\right)
\approx -0.692325
 a \ne \frac{1}{e} -0.62918","['real-analysis', 'geometry', 'trigonometry', 'inequality', 'euclidean-geometry']"
76,Evaluate $\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}.$,Evaluate,\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}.,"Problem Evaluate $$\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}.$$ My solution Notice that $$\lim_{n \to \infty}\frac{n+n^2+n^3+\cdots +n^n}{n^n}=\lim_{n \to \infty}\frac{n(n^n-1)}{(n-1)n^n}=\lim_{n \to \infty}\frac{1-\dfrac{1}{n^n}}{1-\dfrac{1}{n}}=1,$$ and $$\lim_{n \to \infty}\frac{1+2^n+3^n+\cdots+n^n}{n^n}=\frac{e}{e-1}.$$ Hence, \begin{align*}\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}&=\lim_{n \to \infty}\frac{\dfrac{n+n^2+n^3+\cdots +n^n}{n^n}}{\dfrac{1+2^n+3^n+\cdots +n^n}{n^n}}\\&=\frac{\lim\limits_{n \to \infty}\dfrac{n+n^2+n^3+\cdots +n^n}{n^n}}{\lim\limits_{n \to \infty}\dfrac{1+2^n+3^n+\cdots +n^n}{n^n}}\\&=1-\frac{1}{e}.\end{align*} The solution posted above need to quote an uncommon limit. Is there another more simple and more direct solution?","Problem Evaluate My solution Notice that and Hence, The solution posted above need to quote an uncommon limit. Is there another more simple and more direct solution?","\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}. \lim_{n \to \infty}\frac{n+n^2+n^3+\cdots +n^n}{n^n}=\lim_{n \to \infty}\frac{n(n^n-1)}{(n-1)n^n}=\lim_{n \to \infty}\frac{1-\dfrac{1}{n^n}}{1-\dfrac{1}{n}}=1, \lim_{n \to \infty}\frac{1+2^n+3^n+\cdots+n^n}{n^n}=\frac{e}{e-1}. \begin{align*}\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}&=\lim_{n \to \infty}\frac{\dfrac{n+n^2+n^3+\cdots +n^n}{n^n}}{\dfrac{1+2^n+3^n+\cdots +n^n}{n^n}}\\&=\frac{\lim\limits_{n \to \infty}\dfrac{n+n^2+n^3+\cdots +n^n}{n^n}}{\lim\limits_{n \to \infty}\dfrac{1+2^n+3^n+\cdots +n^n}{n^n}}\\&=1-\frac{1}{e}.\end{align*}","['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'geometric-series']"
77,Is $\pi$ the best constant in this inequality?,Is  the best constant in this inequality?,\pi,"Let $E$ be the set of completely monotonous functions on $[0,+\infty)$, that is $f \in C^\infty([0,+\infty))$ and $\forall\, n\geq 0,\forall\, x\geq 0,\quad(-1)^nf^{(n)}(x)\geq 0.$. For $f\in E$ and $\lambda>0$, we have $$\eqalign{ \left(\int_0^\infty f(x)dx\right)^2&\leq \left(\int_0^\infty\frac{dx}{1+\lambda^2 x^2}\right)\left( \int_0^\infty f^2(x)dx+\lambda^2 \int_0^\infty x^2f^2(x)dx\right)\cr &\leq \frac{\pi}{2}\left( \frac{1}{\lambda}\int_0^\infty f^2(x)dx+\lambda \int_0^\infty x^2f^2(x)dx\right)\cr} $$ Now choosing $\lambda=\sqrt{\int  f^2(x)dx\Big/\int x^2f^2(x)dx}$ we find that $$ \left(\int_0^\infty f(x)dx\right)^2\le\pi \,\sqrt{\int_0^\infty  f^2(x)dx \int_0^\infty x^2f^2(x)dx}. $$ Noting, that we restrict our attention to functions $f\in E$, and that $t\mapsto \dfrac{1}{a^2+t^2}\notin E$. Is $\pi$ the best possible constant in this inequality? That is: What is the least $M>0$ such that $$ \left(\int_0^\infty f(x)dx\right)^2\le M \,\sqrt{\int_0^\infty  f^2(x)dx \int_0^\infty x^2f^2(x)dx}. $$ for every $f\in E$? Remark: Note that by choosing $f(x)=\dfrac{1}{(1+x)^2}$, we get $3\leq M$, so the best constant belongs to the interval $[3,\pi]$.","Let $E$ be the set of completely monotonous functions on $[0,+\infty)$, that is $f \in C^\infty([0,+\infty))$ and $\forall\, n\geq 0,\forall\, x\geq 0,\quad(-1)^nf^{(n)}(x)\geq 0.$. For $f\in E$ and $\lambda>0$, we have $$\eqalign{ \left(\int_0^\infty f(x)dx\right)^2&\leq \left(\int_0^\infty\frac{dx}{1+\lambda^2 x^2}\right)\left( \int_0^\infty f^2(x)dx+\lambda^2 \int_0^\infty x^2f^2(x)dx\right)\cr &\leq \frac{\pi}{2}\left( \frac{1}{\lambda}\int_0^\infty f^2(x)dx+\lambda \int_0^\infty x^2f^2(x)dx\right)\cr} $$ Now choosing $\lambda=\sqrt{\int  f^2(x)dx\Big/\int x^2f^2(x)dx}$ we find that $$ \left(\int_0^\infty f(x)dx\right)^2\le\pi \,\sqrt{\int_0^\infty  f^2(x)dx \int_0^\infty x^2f^2(x)dx}. $$ Noting, that we restrict our attention to functions $f\in E$, and that $t\mapsto \dfrac{1}{a^2+t^2}\notin E$. Is $\pi$ the best possible constant in this inequality? That is: What is the least $M>0$ such that $$ \left(\int_0^\infty f(x)dx\right)^2\le M \,\sqrt{\int_0^\infty  f^2(x)dx \int_0^\infty x^2f^2(x)dx}. $$ for every $f\in E$? Remark: Note that by choosing $f(x)=\dfrac{1}{(1+x)^2}$, we get $3\leq M$, so the best constant belongs to the interval $[3,\pi]$.",,"['real-analysis', 'inequality']"
78,When L'Hôpital's Rule Fails,When L'Hôpital's Rule Fails,,"I was discussing L'Hôpital's Rule with a Calculus I student earlier today. I mentioned that if the limit obtained by differentiating the numerator and denominator doesn't exist, then L'Hôpital's Rule tells us nothing about the original limit. A clear example of this is,   $$\lim\limits_{ x \to \infty  }{ \frac { x+\sin { x }  }{ x }  } =1.$$   However, L'Hôpital's Rule gives   $$\lim\limits_{ x\rightarrow \infty  }{ \frac { x+\sin { x }  }{ x }  } =\lim \limits_{ x\rightarrow \infty  }{ \frac { 1+\cos { x }  }{ 1 }  } =\lim\limits_{ x\rightarrow \infty  }{ \left( 1+\cos { x }  \right)  }, $$   which diverges by oscillation. I couldn't come up with an example that shows that if the limit from LH is infinite, then the original limit may be finite. This begs these two questions, Is it true that an infinite result from  L'Hôpital's Rule  does not imply an infinite limit? Is there a simple example where the LH is infinite, but the limit is actually finite?","I was discussing L'Hôpital's Rule with a Calculus I student earlier today. I mentioned that if the limit obtained by differentiating the numerator and denominator doesn't exist, then L'Hôpital's Rule tells us nothing about the original limit. A clear example of this is,   $$\lim\limits_{ x \to \infty  }{ \frac { x+\sin { x }  }{ x }  } =1.$$   However, L'Hôpital's Rule gives   $$\lim\limits_{ x\rightarrow \infty  }{ \frac { x+\sin { x }  }{ x }  } =\lim \limits_{ x\rightarrow \infty  }{ \frac { 1+\cos { x }  }{ 1 }  } =\lim\limits_{ x\rightarrow \infty  }{ \left( 1+\cos { x }  \right)  }, $$   which diverges by oscillation. I couldn't come up with an example that shows that if the limit from LH is infinite, then the original limit may be finite. This begs these two questions, Is it true that an infinite result from  L'Hôpital's Rule  does not imply an infinite limit? Is there a simple example where the LH is infinite, but the limit is actually finite?",,"['calculus', 'real-analysis', 'limits']"
79,Prove that $e^\pi+\frac{1}{\pi} < \pi^e+1$,Prove that,e^\pi+\frac{1}{\pi} < \pi^e+1,"Prove that: $$e^\pi+\frac{1}{\pi}< \pi^{e}+1$$ Using Wolfram Alpha $\pi e^{\pi}+1 \approx 73.698\ldots$ and $\pi(\pi^{e}+1) \approx 73.699\ldots$ Can this inequality be proven without brute-force estimations (anything of the sort $e\approx 2.7182...$ or $\pi \approx 3.1415...$ )? I've just seen this and I remembered I've seen the question asked here in an older paper, but I don't remember the details. Note that this is sharper because it can be written as: $$e^{\pi}-\pi^e<1-\frac{1}{\pi}<1$$ I've tried, but none of the methods in the linked question (which study the function $x^\frac{1}{x}$ ) can be applied here.","Prove that: Using Wolfram Alpha and Can this inequality be proven without brute-force estimations (anything of the sort or )? I've just seen this and I remembered I've seen the question asked here in an older paper, but I don't remember the details. Note that this is sharper because it can be written as: I've tried, but none of the methods in the linked question (which study the function ) can be applied here.",e^\pi+\frac{1}{\pi}< \pi^{e}+1 \pi e^{\pi}+1 \approx 73.698\ldots \pi(\pi^{e}+1) \approx 73.699\ldots e\approx 2.7182... \pi \approx 3.1415... e^{\pi}-\pi^e<1-\frac{1}{\pi}<1 x^\frac{1}{x},"['real-analysis', 'calculus', 'inequality', 'estimation']"
80,$\sin(x)$ infinite product formula: how did Euler prove it?,infinite product formula: how did Euler prove it?,\sin(x),"I know that $\sin(x)$ can be expressed as an infinite product, and I've seen proofs of it (e.g. Infinite product of sine function ). I found How was Euler able to create an infinite product for sinc by using its roots? which discusses how Euler might have found the equation, but I wonder how Euler could have proved it. $$\sin(x) = x\prod_{n=1}^\infty \left(1-\frac{x^2}{n^2\pi^2}\right)$$ So how did Euler derive this? I've seen a proof that requires Fourier series (something not know [formally] by Euler, I guess). I also know that this equation can be thought intuitively, and it's really true that it will have the same roots as the sine function, however it's not clear that the entire function converges to the sine function. So, even if Euler guessed it, how was his proof accepted, for this formula to calculate the zeta function for even integers? $$\zeta(2n) = (-1)^{n+1} \frac{B_{2n}(2\pi )^{2n}}{2(2n)!}$$ I've checked a proof of this result, and it requires the sine infinite product. Also, the Basel problem (solved by him) used this infinite product too, and he got famous by this proof, so the sine infinite product might have been accepted by the mathematical community at that time.","I know that $\sin(x)$ can be expressed as an infinite product, and I've seen proofs of it (e.g. Infinite product of sine function ). I found How was Euler able to create an infinite product for sinc by using its roots? which discusses how Euler might have found the equation, but I wonder how Euler could have proved it. $$\sin(x) = x\prod_{n=1}^\infty \left(1-\frac{x^2}{n^2\pi^2}\right)$$ So how did Euler derive this? I've seen a proof that requires Fourier series (something not know [formally] by Euler, I guess). I also know that this equation can be thought intuitively, and it's really true that it will have the same roots as the sine function, however it's not clear that the entire function converges to the sine function. So, even if Euler guessed it, how was his proof accepted, for this formula to calculate the zeta function for even integers? $$\zeta(2n) = (-1)^{n+1} \frac{B_{2n}(2\pi )^{2n}}{2(2n)!}$$ I've checked a proof of this result, and it requires the sine infinite product. Also, the Basel problem (solved by him) used this infinite product too, and he got famous by this proof, so the sine infinite product might have been accepted by the mathematical community at that time.",,"['calculus', 'real-analysis', 'complex-analysis', 'math-history']"
81,Find $f$ such that $f \star f(x) = \frac{1}{1-x}$.,Find  such that .,f f \star f(x) = \frac{1}{1-x},"I'm looking for a measurable function $f$ defined on $]0,1[$ such that : $$f \star f(x) = \int_{0}^1 f(x-y) f(y) \ \mathrm{d}y = \frac{1}{1-x}$$ for (almost) any $x \in ]0,1[$ . Is it possible to find or construct such a function f ? Eventually, we can define $f$ on $\mathbb{R}$ with $f=0$ outside of $]0,1[$ . Any help or advises on how to proceed are welcomed. I have already tried without sucess : looking for $f$ in a rational fraction form looking for $f$ in the form $\sum_{n} a_n x^n$ looking for $f$ in the form $e^{F(x)}$ with a suitable $F$ . I also started to consider fourier transform and fourier series (by periodisation on $]0,1[$ ) but it is difficult to define the fourier transform of the right hand side $\frac{1}{1-x}$ . Also about the regularity of $f$ , we can see that $f$ can't be in $L^1(]0,1[)$ because of the regularity properties of the convolution.","I'm looking for a measurable function defined on such that : for (almost) any . Is it possible to find or construct such a function f ? Eventually, we can define on with outside of . Any help or advises on how to proceed are welcomed. I have already tried without sucess : looking for in a rational fraction form looking for in the form looking for in the form with a suitable . I also started to consider fourier transform and fourier series (by periodisation on ) but it is difficult to define the fourier transform of the right hand side . Also about the regularity of , we can see that can't be in because of the regularity properties of the convolution.","f ]0,1[ f \star f(x) = \int_{0}^1 f(x-y) f(y) \ \mathrm{d}y = \frac{1}{1-x} x \in ]0,1[ f \mathbb{R} f=0 ]0,1[ f f \sum_{n} a_n x^n f e^{F(x)} F ]0,1[ \frac{1}{1-x} f f L^1(]0,1[)","['real-analysis', 'definite-integrals', 'lebesgue-measure', 'convolution', 'integral-equations']"
82,Number of local maxima of a function,Number of local maxima of a function,,"Let $z_j$ ($j=1,\dots, k$) be $k$ points on the complex plane none of which lies on the real line. Is it always true that the function  $$ F(x)=\sum_{j=1}^k \frac{1}{|x-z_j|^2} $$ has at most $k$ local maxima on the real line?","Let $z_j$ ($j=1,\dots, k$) be $k$ points on the complex plane none of which lies on the real line. Is it always true that the function  $$ F(x)=\sum_{j=1}^k \frac{1}{|x-z_j|^2} $$ has at most $k$ local maxima on the real line?",,"['real-analysis', 'complex-analysis', 'analysis']"
83,"Function $f:[0,1] \to [0,1]$ taking on each value in $[0,1]$ exactly twice",Function  taking on each value in  exactly twice,"f:[0,1] \to [0,1] [0,1]","I want to find a function $f:[0,1] \to [0,1]$ such that $f$ takes on each value in $[0,1]$ exactly twice. I think this means there are an infinite number of discontinuities. Can anyone help me figure this one out? Anyone have any pointers?","I want to find a function $f:[0,1] \to [0,1]$ such that $f$ takes on each value in $[0,1]$ exactly twice. I think this means there are an infinite number of discontinuities. Can anyone help me figure this one out? Anyone have any pointers?",,['real-analysis']
84,"Strengthening the intermediate value theorem to an ""intermediate component theorem""","Strengthening the intermediate value theorem to an ""intermediate component theorem""",,"Let $f$ be a continuous function on a closed Euclidean ball (in dimension $\ge 2$) that is negative in the center of the ball and positive on its boundary. On any path from the center of the ball to its boundary, there must be some point where $f=0$, by the intermediate value theorem. I would like to strengthen this, and say there exists an ""intermediate zero component"" between the center and the boundary: Some connected component of $\left\{f=0\right\}$ which any such path must traverse. Is it possible to prove this without using heavy tools such as the Jordan-Brouwer separation theorem?","Let $f$ be a continuous function on a closed Euclidean ball (in dimension $\ge 2$) that is negative in the center of the ball and positive on its boundary. On any path from the center of the ball to its boundary, there must be some point where $f=0$, by the intermediate value theorem. I would like to strengthen this, and say there exists an ""intermediate zero component"" between the center and the boundary: Some connected component of $\left\{f=0\right\}$ which any such path must traverse. Is it possible to prove this without using heavy tools such as the Jordan-Brouwer separation theorem?",,"['real-analysis', 'general-topology']"
85,Show that $\lim\limits_{n\to\infty}\frac{a_1+a_2+\dots+a_n}{n^2}$ exists and is independent of the choice of $a$,Show that  exists and is independent of the choice of,\lim\limits_{n\to\infty}\frac{a_1+a_2+\dots+a_n}{n^2} a,"Suppose $f:\mathbb{R}\to\mathbb{R}$ has period 1, and for some $q\in(0,1)$: $$|f(x)-f(y)|\leq q|x-y|\quad \forall x,y$$ Let $g(x)=x+f(x)$, for any $a\in\mathbb{R}$,define the following sequence: $$a_1=a,\quad a_2=g(a_1),\quad a_3=g(a_2),\quad \dots,\quad a_{n+1}=g(a_n)$$ Show that $$\lim_{n\to\infty}\frac{a_1+a_2+\dots+a_n}{n^2}$$ exists and does not depend on the choice of $a$. I don't even know what is the possible approach, any hints? Thanks!","Suppose $f:\mathbb{R}\to\mathbb{R}$ has period 1, and for some $q\in(0,1)$: $$|f(x)-f(y)|\leq q|x-y|\quad \forall x,y$$ Let $g(x)=x+f(x)$, for any $a\in\mathbb{R}$,define the following sequence: $$a_1=a,\quad a_2=g(a_1),\quad a_3=g(a_2),\quad \dots,\quad a_{n+1}=g(a_n)$$ Show that $$\lim_{n\to\infty}\frac{a_1+a_2+\dots+a_n}{n^2}$$ exists and does not depend on the choice of $a$. I don't even know what is the possible approach, any hints? Thanks!",,"['real-analysis', 'sequences-and-series', 'functions', 'recurrence-relations']"
86,Can we find this infinite root in term of elementary function? [closed],Can we find this infinite root in term of elementary function? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $f(x)=\left(x+f(x+1)\right)^\frac{1}{x}$. What is the value of $f(2)$ ? More precisely,  how to find the value of $$\sqrt{2+\sqrt[3]{3+\sqrt[4]{4+\cdots}}}~?$$ Thank you.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $f(x)=\left(x+f(x+1)\right)^\frac{1}{x}$. What is the value of $f(2)$ ? More precisely,  how to find the value of $$\sqrt{2+\sqrt[3]{3+\sqrt[4]{4+\cdots}}}~?$$ Thank you.",,"['calculus', 'real-analysis', 'analysis', 'algebra-precalculus', 'nested-radicals']"
87,Are Continuous Functions Always Differentiable?,Are Continuous Functions Always Differentiable?,,Are continuous functions always differentiable? Are there any examples in dimension $n > 1$ ?,Are continuous functions always differentiable? Are there any examples in dimension ?,n > 1,"['real-analysis', 'calculus']"
88,"Does uncountable summation, with a finite sum, ever occur in mathematics?","Does uncountable summation, with a finite sum, ever occur in mathematics?",,"Obviously, “most” of the terms must cancel out with opposite algebraic sign. You can contrive examples such as the sum of the members of R being 0, but does an uncountable sum, with a finite sum, ever occur naturally as part of a larger scenario (eg, proof of a theorem, or preparation for a definition)?","Obviously, “most” of the terms must cancel out with opposite algebraic sign. You can contrive examples such as the sum of the members of R being 0, but does an uncountable sum, with a finite sum, ever occur naturally as part of a larger scenario (eg, proof of a theorem, or preparation for a definition)?",,"['real-analysis', 'sequences-and-series', 'summation']"
89,Proving that $ 2 $ is the only real solution of $ 3^x+4^x=5^x $,Proving that  is the only real solution of, 2   3^x+4^x=5^x ,I would like to prove that the equation $ 3^x+4^x=5^x $ has only one real solution ($x=2$) I tried to study the function $ f(x)=5^x-4^x-3^x $ (in order to use the intermediate value theorem) but I am not able to find the sign of $ f'(x)= \ln(5)\times5^x-\ln(4)\times4^x-\ln(3)\times3^x $ and I can't see any other method to solve this exercise...,I would like to prove that the equation $ 3^x+4^x=5^x $ has only one real solution ($x=2$) I tried to study the function $ f(x)=5^x-4^x-3^x $ (in order to use the intermediate value theorem) but I am not able to find the sign of $ f'(x)= \ln(5)\times5^x-\ln(4)\times4^x-\ln(3)\times3^x $ and I can't see any other method to solve this exercise...,,"['calculus', 'real-analysis']"
90,Why does L'Hôpital's rule work for sequences?,Why does L'Hôpital's rule work for sequences?,,"Say, for the classic example, $\frac{\log(n)}{n}$, this sequence converges to zero, from applying L'Hôpital's rule.  Why does it work in the discrete setting, when the rule is about differentiable functions? Is it because at infinity, it doesn't matter that we relabel the discrete variable, $n$, with a continuous variable, $x$, and instead look at the limit of  $\frac{\log(x)}{x}$? But then what about the quotients of sequences that go to the indeterminate form $\frac{0}{0}$?  Why is it OK to use L'Hôpital's rule, as $n$ goes to zero? I haven't found anything on Wikipedia or Wolfram about the discrete setting. Thanks.","Say, for the classic example, $\frac{\log(n)}{n}$, this sequence converges to zero, from applying L'Hôpital's rule.  Why does it work in the discrete setting, when the rule is about differentiable functions? Is it because at infinity, it doesn't matter that we relabel the discrete variable, $n$, with a continuous variable, $x$, and instead look at the limit of  $\frac{\log(x)}{x}$? But then what about the quotients of sequences that go to the indeterminate form $\frac{0}{0}$?  Why is it OK to use L'Hôpital's rule, as $n$ goes to zero? I haven't found anything on Wikipedia or Wolfram about the discrete setting. Thanks.",,"['calculus', 'real-analysis', 'limits', 'convergence-divergence', 'indeterminate-forms']"
91,Proving that there exists an irrational number in between any given real numbers [duplicate],Proving that there exists an irrational number in between any given real numbers [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Density of irrationals I am trying to prove that there exists an irrational number between any two real numbers a and b. I already know that a rational number between the two of them exists. My idea was to say represent a and b as $a = \sqrt{2} x, b = \sqrt{2} y$ for some $x, y \in \mathbb{R}.$ We know that there exists $\frac{m}{n}$ s.t. $x < \frac{m}{n} < y,$ so multiplying everything by the root of 2, we have $$\sqrt{2}x = a < \frac{\sqrt{2}m}{n} < \sqrt{2}y = b,$$ and so we have an irrational number in between two reals. Is this 'legit?' Are there 'better/'more elegant ways to go about this? I would really appreciate seeing alternatives proofs of this, as many as possible.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Density of irrationals I am trying to prove that there exists an irrational number between any two real numbers a and b. I already know that a rational number between the two of them exists. My idea was to say represent a and b as $a = \sqrt{2} x, b = \sqrt{2} y$ for some $x, y \in \mathbb{R}.$ We know that there exists $\frac{m}{n}$ s.t. $x < \frac{m}{n} < y,$ so multiplying everything by the root of 2, we have $$\sqrt{2}x = a < \frac{\sqrt{2}m}{n} < \sqrt{2}y = b,$$ and so we have an irrational number in between two reals. Is this 'legit?' Are there 'better/'more elegant ways to go about this? I would really appreciate seeing alternatives proofs of this, as many as possible.",,['real-analysis']
92,Useful examples of pathological functions,Useful examples of pathological functions,,"What are some particularly well-known functions that exhibit pathological behavior at or near at least one value and are particularly useful as examples? For instance, if $f'(a) = b$, then $f(a)$ exists, $f$ is continuous at $a$, $f$ is differentiable at $a$, but $f'$ need not be continuous at $a$.  A function for which this is true is $f(x) = x^2 \sin(1/x)$ at $x=0$.","What are some particularly well-known functions that exhibit pathological behavior at or near at least one value and are particularly useful as examples? For instance, if $f'(a) = b$, then $f(a)$ exists, $f$ is continuous at $a$, $f$ is differentiable at $a$, but $f'$ need not be continuous at $a$.  A function for which this is true is $f(x) = x^2 \sin(1/x)$ at $x=0$.",,"['soft-question', 'big-list', 'calculus', 'real-analysis']"
93,Good First Course in real analysis book for self study,Good First Course in real analysis book for self study,,Does anybody know of a good book in real analysis for self study for a beginner? What about Analysis 1 by Terence Tao?,Does anybody know of a good book in real analysis for self study for a beginner? What about Analysis 1 by Terence Tao?,,"['real-analysis', 'reference-request', 'book-recommendation']"
94,How to prove periodicity of $\sin(x)$ or $\cos(x)$ starting from the Taylor series expansion?,How to prove periodicity of  or  starting from the Taylor series expansion?,\sin(x) \cos(x),"With the Taylor series representation of $\sin$ or $\cos$ as a starting point (and assuming no other knowledge about those functions), how can one: a. prove they are periodic? b. find the value of the period?","With the Taylor series representation of $\sin$ or $\cos$ as a starting point (and assuming no other knowledge about those functions), how can one: a. prove they are periodic? b. find the value of the period?",,"['calculus', 'real-analysis']"
95,Approximation to $ \sqrt{2}$,Approximation to, \sqrt{2},"I'm a first year Undergraduate student from India. Our professor is going to start a Real Analysis course in September and I was preparing for the initials. I tried and solved many problems, but this one has me confused. Probably the main reason for the confusion is that my book has cited it as Hardy's problem. If $\dfrac {m}{n}$ is a good approximation to $\sqrt{2}$, prove that $\dfrac{m+2n}{m+n}$ is a better one, and that the errors in the two cases are in opposite direction. Apply this result to show that the limit of the sequence $\dfrac{1}{1}$, $\dfrac{3}{2}$,$\dfrac{7}{5}$,$\dfrac{17}{12}$,$\dfrac{41}{29}$,.... is $ \sqrt{2}$. I need help regarding the first part of the problem, since the second part is obvious. The simpler the language, the better it is for me.","I'm a first year Undergraduate student from India. Our professor is going to start a Real Analysis course in September and I was preparing for the initials. I tried and solved many problems, but this one has me confused. Probably the main reason for the confusion is that my book has cited it as Hardy's problem. If $\dfrac {m}{n}$ is a good approximation to $\sqrt{2}$, prove that $\dfrac{m+2n}{m+n}$ is a better one, and that the errors in the two cases are in opposite direction. Apply this result to show that the limit of the sequence $\dfrac{1}{1}$, $\dfrac{3}{2}$,$\dfrac{7}{5}$,$\dfrac{17}{12}$,$\dfrac{41}{29}$,.... is $ \sqrt{2}$. I need help regarding the first part of the problem, since the second part is obvious. The simpler the language, the better it is for me.",,['real-analysis']
96,Why does the generalised derivative have to be a linear transformation?,Why does the generalised derivative have to be a linear transformation?,,I am starting to learn Real Analysis and I have come across the generalised definition of the derivative for higher dimensions. I realise that the derivative being a linear transformation nicely accommodates the one dimensional case where the derivative is just a constant at any point. I also understand it can't be as simple as multiplication by a constant for higher dimensions since you can approach a point along multiple curves in higher dimensions. But where did we hit upon the fact that it has to be linear? Why couldn't it be some other type of function? I would like to get an intuitive explanation.,I am starting to learn Real Analysis and I have come across the generalised definition of the derivative for higher dimensions. I realise that the derivative being a linear transformation nicely accommodates the one dimensional case where the derivative is just a constant at any point. I also understand it can't be as simple as multiplication by a constant for higher dimensions since you can approach a point along multiple curves in higher dimensions. But where did we hit upon the fact that it has to be linear? Why couldn't it be some other type of function? I would like to get an intuitive explanation.,,"['real-analysis', 'multivariable-calculus', 'derivatives', 'intuition', 'motivation']"
97,composition of two uniformly continuous functions.,composition of two uniformly continuous functions.,,"Let $f : \mathbb{R} \rightarrow \mathbb{R}$ and $g : \mathbb{R} \rightarrow \mathbb{R}$ are two uniform continuous functions. Which of the following options are correct and why? $f(g(x))$ is uniformly continuous. $f(g(x))$ is continuous but not uniformly continuous. $f(g(x))$ is continuous and bounded. My attempt: Every uniformly continuous function maps a Cauchy sequence to a Cauchy sequence. (Here I have a doubt, as the converse may not be true). So if $\{x_n\}$ be a Cauchy sequence, $\{f(x_n)\}$ and $\{g(f(x_n))\}$ both will be Cauchy sequence. So $g(f(x))$ will be uniformly continuous, i.e. 1 is true. Composite function of two continuous functions will be continuous. As 1 is true, 2 is false. $f(x) = x $ is uniformly continuous. $g(x) = \log(x)$ is uniformly continuous in $[1,\infty)$. So $g(f(x)) = \log(x)$ is uniformly continuous in $[1, \infty)$, but not in $\mathbb{R}$. Thank you for your help.","Let $f : \mathbb{R} \rightarrow \mathbb{R}$ and $g : \mathbb{R} \rightarrow \mathbb{R}$ are two uniform continuous functions. Which of the following options are correct and why? $f(g(x))$ is uniformly continuous. $f(g(x))$ is continuous but not uniformly continuous. $f(g(x))$ is continuous and bounded. My attempt: Every uniformly continuous function maps a Cauchy sequence to a Cauchy sequence. (Here I have a doubt, as the converse may not be true). So if $\{x_n\}$ be a Cauchy sequence, $\{f(x_n)\}$ and $\{g(f(x_n))\}$ both will be Cauchy sequence. So $g(f(x))$ will be uniformly continuous, i.e. 1 is true. Composite function of two continuous functions will be continuous. As 1 is true, 2 is false. $f(x) = x $ is uniformly continuous. $g(x) = \log(x)$ is uniformly continuous in $[1,\infty)$. So $g(f(x)) = \log(x)$ is uniformly continuous in $[1, \infty)$, but not in $\mathbb{R}$. Thank you for your help.",,"['real-analysis', 'continuity', 'uniform-continuity', 'function-and-relation-composition']"
98,The proofs of limit laws and derivative rules appear to tacitly assume that the limit exists in the first place,The proofs of limit laws and derivative rules appear to tacitly assume that the limit exists in the first place,,"Say I was trying to find the derivative of $x^2$ using differentiation from first principles. The usual argument would go something like this: If $f(x)=x^2$ , then \begin{align} f'(x) &= \lim_{h \to 0}\frac{(x+h)^2-x^2}{h} \\ &= \lim_{h \to 0}\frac{2hx+h^2}{h} \\ &= \lim_{h \to 0} 2x+h \end{align} As $h$ approaches $0$ , $2x+h$ approaches $2x$ , so $f'(x)=2x$ . Throughout this argument, I assumed that $$ \lim_{h \to 0}\frac{f(x+h)-f(x)}{h} $$ was actually a meaningful object—that the limit actually existed. I don't really understand what justifies this assumption. To me, sometimes the assumption that an object is well-defined can lead you to draw incorrect conclusions. For example, assuming that $\log(0)$ makes any sense, we can conclude that $$ \log(0)=\log(0)+\log(0) \implies \log(0)=0 \, . $$ So the assumption that $\log(0)$ represented anything meaningful led us to incorrectly conclude that it was equal to $0$ . Often, to prove that a limit exists, we manipulate it until we can write it in a familiar form. This can be seen in the proofs of the chain rule and product rule. But it often seems that that manipulation can only be justified if we know the limit exists in the first place! So what is really going on here? For another example, the chain rule is often stated as: Suppose that $g$ is differentiable at $x$ , and $f$ is differentiable at $g(x)$ . Then, $(f \circ g)$ is differentiable at $x$ , and $$ (f \circ g)'(x) = f'(g(x))g'(x) $$ If the proof that $(f \circ g)$ is differentiable at $x$ simply amounts to computing the derivative using the limit definition, then again I feel unsatisfied. Doesn't this computation again make the assumption that $(f \circ g)'(x)$ makes sense in the first place?","Say I was trying to find the derivative of using differentiation from first principles. The usual argument would go something like this: If , then As approaches , approaches , so . Throughout this argument, I assumed that was actually a meaningful object—that the limit actually existed. I don't really understand what justifies this assumption. To me, sometimes the assumption that an object is well-defined can lead you to draw incorrect conclusions. For example, assuming that makes any sense, we can conclude that So the assumption that represented anything meaningful led us to incorrectly conclude that it was equal to . Often, to prove that a limit exists, we manipulate it until we can write it in a familiar form. This can be seen in the proofs of the chain rule and product rule. But it often seems that that manipulation can only be justified if we know the limit exists in the first place! So what is really going on here? For another example, the chain rule is often stated as: Suppose that is differentiable at , and is differentiable at . Then, is differentiable at , and If the proof that is differentiable at simply amounts to computing the derivative using the limit definition, then again I feel unsatisfied. Doesn't this computation again make the assumption that makes sense in the first place?","x^2 f(x)=x^2 \begin{align} f'(x) &= \lim_{h \to
0}\frac{(x+h)^2-x^2}{h} \\ &= \lim_{h \to 0}\frac{2hx+h^2}{h} \\
&= \lim_{h \to 0} 2x+h \end{align} h 0 2x+h 2x f'(x)=2x 
\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}
 \log(0) 
\log(0)=\log(0)+\log(0) \implies \log(0)=0 \, .
 \log(0) 0 g x f g(x) (f \circ g) x 
(f \circ g)'(x) = f'(g(x))g'(x)
 (f \circ g) x (f \circ g)'(x)","['real-analysis', 'calculus', 'limits', 'derivatives']"
99,$-4\zeta(2)-2\zeta(3)+4\zeta(2)\zeta(3)+2\zeta(5)=S$,,-4\zeta(2)-2\zeta(3)+4\zeta(2)\zeta(3)+2\zeta(5)=S,"EDIT:  Due to the solution below, I edited the answer of the post.  Thanks!!!! Hi I am trying to calculate the infinite double sum  $$ S:=\sum_{j,k=1}^\infty \frac{H_j(H_{k+1}-1)}{jk(k+1)(j+k)}=-4\zeta(2)-2\zeta(3)+4\zeta(2)\zeta(3)+2\zeta(5),\quad H_n:=\sum_{k=1}^n\frac{1}{k}\ \ \ (\text{Harmonic Numbers}) $$ Thank you. I am not sure what to do, possibly write the sum as an integral and try working with the integral instead of the sum?  I was trying to figure out if we could write it as an integral representation in terms of logarithm functions.  But this will just give you this sum as the answer.  So I do not know how to calculate the zeta functions from the sum.  Note the Riemann Zeta function is given by  $$ \zeta(s)=\sum_{n=1}^\infty \frac{1}{n^s},\quad \zeta(2)=\frac{\pi^2}{6}. $$","EDIT:  Due to the solution below, I edited the answer of the post.  Thanks!!!! Hi I am trying to calculate the infinite double sum  $$ S:=\sum_{j,k=1}^\infty \frac{H_j(H_{k+1}-1)}{jk(k+1)(j+k)}=-4\zeta(2)-2\zeta(3)+4\zeta(2)\zeta(3)+2\zeta(5),\quad H_n:=\sum_{k=1}^n\frac{1}{k}\ \ \ (\text{Harmonic Numbers}) $$ Thank you. I am not sure what to do, possibly write the sum as an integral and try working with the integral instead of the sum?  I was trying to figure out if we could write it as an integral representation in terms of logarithm functions.  But this will just give you this sum as the answer.  So I do not know how to calculate the zeta functions from the sum.  Note the Riemann Zeta function is given by  $$ \zeta(s)=\sum_{n=1}^\infty \frac{1}{n^s},\quad \zeta(2)=\frac{\pi^2}{6}. $$",,"['real-analysis', 'integration', 'sequences-and-series', 'complex-analysis', 'harmonic-numbers']"
