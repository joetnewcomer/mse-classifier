,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Find the max. and min. points of $g(x,y,z) = x^2 + y^2 + z^2$",Find the max. and min. points of,"g(x,y,z) = x^2 + y^2 + z^2","The problem Let $X$ a submanifold of $\mathbb{R}^3$ defined by $$ X = \left\{ (x,y,z) \mid (x^2 + y^2 + z^2 - 5)^2 = 16(1-z^2) \right\} $$ Let $\Phi(x,y,z) = (x^2 + y^2 + z^2 - 5)^2 - 16(1-z^2) $ . Let $f$ a function defined by $$ f : \mathbb{R}^3 \rightarrow \mathbb{R} \mbox{, } (x,y,z) \mapsto x^2 + y^2 + z^2 $$ We define $g$ by the restriction of $f$ on $X$ : $ g = \left. f \right|_X $ Give the points in which the differential of $g$ is zero and deduce the max. and min. points. My works $X$ is compact and $g$ is continuous on $X$ , then $g$ has a global max. and min. point. Thus, $g$ is $\mathscr{C}^{\infty}$ on $X$ . Then, we can apply the method of Lagrange multipliers. We have $$ \frac{ \partial g }{ \partial x } = 2x \mbox{, } \frac{ \partial g }{ \partial y } = 2y \mbox{, } \frac{ \partial g }{ \partial z } = 2z $$ and $$ \frac{ \partial \Phi }{ \partial x } = 4x(x^2 + y^2 + z^2 - 5) \mbox{, } \frac{ \partial \Phi }{ \partial y } = 4y(x^2 + y^2 + z^2 - 5) \mbox{, } \frac{ \partial \Phi }{ \partial z } = 4z(x^2 + y^2 + z^2 + 3) $$ We get the following system $$ \left( \begin{array}{c} 2x\\ 2y\\ 2z\\ \end{array} \right) = \lambda \left( \begin{array}{c} 4x(x^2 + y^2 + z^2 - 5) \\ 4y(x^2 + y^2 + z^2 - 5) \\ 4z(x^2 + y^2 + z^2 + 3) \\ \end{array} \right) $$ with $\lambda$ a real scalar. And I don't know how to resolve this system... Actually, I am not sure if this is the system we have to get. I think I made a mistake somewhere but I don't know where. Thank you for your help.","The problem Let a submanifold of defined by Let . Let a function defined by We define by the restriction of on : Give the points in which the differential of is zero and deduce the max. and min. points. My works is compact and is continuous on , then has a global max. and min. point. Thus, is on . Then, we can apply the method of Lagrange multipliers. We have and We get the following system with a real scalar. And I don't know how to resolve this system... Actually, I am not sure if this is the system we have to get. I think I made a mistake somewhere but I don't know where. Thank you for your help.","X \mathbb{R}^3  X = \left\{ (x,y,z) \mid (x^2 + y^2 + z^2 - 5)^2 = 16(1-z^2) \right\}  \Phi(x,y,z) = (x^2 + y^2 + z^2 - 5)^2 - 16(1-z^2)  f  f : \mathbb{R}^3 \rightarrow \mathbb{R} \mbox{, } (x,y,z) \mapsto x^2 + y^2 + z^2  g f X  g = \left. f \right|_X  g X g X g g \mathscr{C}^{\infty} X  \frac{ \partial g }{ \partial x } = 2x \mbox{, } \frac{ \partial g }{ \partial y } = 2y \mbox{, } \frac{ \partial g }{ \partial z } = 2z   \frac{ \partial \Phi }{ \partial x } = 4x(x^2 + y^2 + z^2 - 5) \mbox{, } \frac{ \partial \Phi }{ \partial y } = 4y(x^2 + y^2 + z^2 - 5) \mbox{, } \frac{ \partial \Phi }{ \partial z } = 4z(x^2 + y^2 + z^2 + 3)   \left( \begin{array}{c}
2x\\ 2y\\ 2z\\
\end{array} \right) = \lambda \left( \begin{array}{c}
4x(x^2 + y^2 + z^2 - 5) \\
4y(x^2 + y^2 + z^2 - 5) \\
4z(x^2 + y^2 + z^2 + 3) \\
\end{array} \right)  \lambda","['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
1,Prove a limit in two variables is $0$,Prove a limit in two variables is,0,"I have the limit \begin{align} \lim_{(x,y) \rightarrow(0,0)} \frac{1-\cos(xy)}{\sqrt{x^2+y^2}(x^2+y^2)} \end{align} First thing I did is to use Taylor, so $$1-\cos(xy)=\frac{(xy)^2}{2} -\frac{(xy)^4}{4} + o((xy)^4)$$ Therefore the limit is $$\lim_{(x,y) \rightarrow (0,0)} \frac{1}{2} \frac{x^2y^2}{\sqrt{x^2+y^2}(x^2+y^2)}$$ Now, $\frac{y^2}{x^2+y^2} \leq 1$ , therefore I have $$\left|\frac{1}{2} \frac{x^2y^2}{\sqrt{x^2+y^2}(x^2+y^2)}\right| \leq \left|\frac{x^2}{\sqrt{x^2+y^2}}\right| \leq \frac{x^2}{|x|} = |x|$$ Since $|x| \rightarrow 0 $ as $(x,y) \rightarrow (0,0)$ , the limit is $0$ , correct?","I have the limit First thing I did is to use Taylor, so Therefore the limit is Now, , therefore I have Since as , the limit is , correct?","\begin{align}
\lim_{(x,y) \rightarrow(0,0)} \frac{1-\cos(xy)}{\sqrt{x^2+y^2}(x^2+y^2)}
\end{align} 1-\cos(xy)=\frac{(xy)^2}{2} -\frac{(xy)^4}{4} + o((xy)^4) \lim_{(x,y) \rightarrow (0,0)} \frac{1}{2} \frac{x^2y^2}{\sqrt{x^2+y^2}(x^2+y^2)} \frac{y^2}{x^2+y^2} \leq 1 \left|\frac{1}{2} \frac{x^2y^2}{\sqrt{x^2+y^2}(x^2+y^2)}\right| \leq \left|\frac{x^2}{\sqrt{x^2+y^2}}\right| \leq \frac{x^2}{|x|} = |x| |x| \rightarrow 0  (x,y) \rightarrow (0,0) 0","['calculus', 'limits', 'multivariable-calculus', 'solution-verification']"
2,Integral of sin(|x-y|) [closed],Integral of sin(|x-y|) [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I do not know how should I proceed to calculate the definite integral: $$ \int_{-\pi/2}^{\pi/2}\int_{-\pi/2}^{\pi/2} \sin\left(\,{\left\lvert\, {x - y\,}\right\rvert}\,\right)\, \mathrm{d}x\,\mathrm{d}y $$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I do not know how should I proceed to calculate the definite integral:","
\int_{-\pi/2}^{\pi/2}\int_{-\pi/2}^{\pi/2} \sin\left(\,{\left\lvert\,
{x - y\,}\right\rvert}\,\right)\,
\mathrm{d}x\,\mathrm{d}y
","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'trigonometric-integrals']"
3,"Given $a,b,x>0$, $x<y$, prove $(a^x+b^x)^{1/x} > (a^y+b^y)^{1/y}$ [duplicate]","Given , , prove  [duplicate]","a,b,x>0 x<y (a^x+b^x)^{1/x} > (a^y+b^y)^{1/y}","This question already has answers here : If $n\geq m$ then $(x^m+y^m)^{1/m} \ge (x^n+y^n)^{1/n}$ (2 answers) Closed 3 years ago . I'm thinking about proving $f(x) = (a^x+b^x)^{1/x}$ has negative derivative for all positive $x$ . $$f'(x) = \left(b^x+a^x\right)^\frac{1}{x}\left(\frac{b^x\ln\left(b\right)+a^x\ln\left(a\right)}{\left(b^x+a^x\right)x}-\frac{\ln\left(b^x+a^x\right)}{x^2}\right)$$ To prove this is negative, I need $$x(a^x\ln(a)+b^x\ln(b)) < (a^x+b^x)\ln(a^x+b^x)$$ which is equivalent to $${a^x}^{a^x}{b^x}^{b^x} < (a^x+b^x)^{a^x+b^x}$$ This looks like a special case of $$A^AB^B < (A^A+B^B)^{A^A+B^B}$$ for any $A,B>0$ . I'm convinced this is true, but I don't know how to prove it either.","This question already has answers here : If $n\geq m$ then $(x^m+y^m)^{1/m} \ge (x^n+y^n)^{1/n}$ (2 answers) Closed 3 years ago . I'm thinking about proving has negative derivative for all positive . To prove this is negative, I need which is equivalent to This looks like a special case of for any . I'm convinced this is true, but I don't know how to prove it either.","f(x) = (a^x+b^x)^{1/x} x f'(x) = \left(b^x+a^x\right)^\frac{1}{x}\left(\frac{b^x\ln\left(b\right)+a^x\ln\left(a\right)}{\left(b^x+a^x\right)x}-\frac{\ln\left(b^x+a^x\right)}{x^2}\right) x(a^x\ln(a)+b^x\ln(b)) < (a^x+b^x)\ln(a^x+b^x) {a^x}^{a^x}{b^x}^{b^x} < (a^x+b^x)^{a^x+b^x} A^AB^B < (A^A+B^B)^{A^A+B^B} A,B>0","['multivariable-calculus', 'inequality']"
4,Finding intervals for a two variable function,Finding intervals for a two variable function,,"I had a similar question about 1 year ago, and I find more properties about my question. Now I want to ask it again and so thankful for new  solutions. Suppose $$ H(x,y)=x e^{\pi y}-\frac{x(\pi-x)}{\pi}e^{xy}+(\pi-x)(\frac{\pi^2}{12}-1)e^{xy}-\frac{\pi^2}{12}x \sinh(\pi y)\\ +(1-\frac{\pi^2}{12})(\pi-x)-x$$ . I guess  there exist $\alpha\geq 0$ and $\beta\geq 0$ such that $$ H(x,y)\geq 0, \quad for \quad (x,y) \in [\alpha,\pi)\times [\beta, \infty).$$ I am looking for the best choices for $\alpha$ and $\beta$ . With several calculations and plot graphs, I guess that if $\alpha=0$ then $0.4<\beta<0.5$ . Also we have, $$1)\,H(0,y)=0, \qquad 2)\,H(\pi,y)= 0, \qquad 3)H(x,0)=\frac{1}{\pi}x^2-1.$$ Furthermore $H(x,0)\geq 0$ if $x\geq \sqrt{\pi}$ . I appreciate any solutions, comments and hints.","I had a similar question about 1 year ago, and I find more properties about my question. Now I want to ask it again and so thankful for new  solutions. Suppose . I guess  there exist and such that I am looking for the best choices for and . With several calculations and plot graphs, I guess that if then . Also we have, Furthermore if . I appreciate any solutions, comments and hints."," H(x,y)=x e^{\pi y}-\frac{x(\pi-x)}{\pi}e^{xy}+(\pi-x)(\frac{\pi^2}{12}-1)e^{xy}-\frac{\pi^2}{12}x \sinh(\pi y)\\ +(1-\frac{\pi^2}{12})(\pi-x)-x \alpha\geq 0 \beta\geq 0  H(x,y)\geq 0, \quad for \quad (x,y) \in [\alpha,\pi)\times [\beta, \infty). \alpha \beta \alpha=0 0.4<\beta<0.5 1)\,H(0,y)=0,
\qquad 2)\,H(\pi,y)= 0,
\qquad 3)H(x,0)=\frac{1}{\pi}x^2-1. H(x,0)\geq 0 x\geq \sqrt{\pi}","['calculus', 'multivariable-calculus', 'functions', 'inequality']"
5,"Show solution to BVP has a saddle-point at $(0,0)$",Show solution to BVP has a saddle-point at,"(0,0)","This problem was from a PDE class I took years ago: Let $\Omega=(-1,1)\times(-1,1)$ and consider the BVP $$ \left\{\begin{aligned} \Delta u&=0,~~x\in\Omega\\ u(-1,y)&=u(1,y)=0\\ u(x,-1)&=u(x,1)=f(x) \end{aligned}\right. $$ where $f:[-1,1]\to\mathbb{R}$ is even, strictly decreasing on $[0,1]$ , and satisfies $f(\pm 1)=0$ .  Show that $u$ has a saddle point at $u(0,0)$ and that $u(0,0)>0$ . I gave a rather hand-wavy argument about symmetry and the partial derivatives that I don't think is entirely accurate: Note that $f$ is even and decreasing on $[0,1].$ Since $f:[-1,1]\to \mathbb{R},$ $f$ has a maximum at $x=0,$ i.e. $f$ has a critical point there (since $f$ has bounded variation, and is increasing on $[0,1]$ , $f$ is differentiable a.e.). Since $u\equiv 0$ if $x=\pm 1,$ the directional derivative of $u$ pointing into $\Omega$ from the boundary is positive. In particular, $u$ has a critical point at $(0,0)$ by symmetry; from $x=\pm1$ it is increasing, but from $x=0,y=\pm1$ it is decreasing. Therefore $u(0,0)>0$ and $u$ has a saddle point there. I also graphed the problem for $f(x)=\{|1-x|,1-x^2,\cos(\pi/2 x)\}$ and used Mathematica's NDSolve to make numerical plots of the solution; each appeared geometrically to have a saddle point at $(0,0)$ . For example, with $f(x) = 1-x^2$ the solution looks like: I'd be interested in a proper solution, if only to satisfy my curiosity.","This problem was from a PDE class I took years ago: Let and consider the BVP where is even, strictly decreasing on , and satisfies .  Show that has a saddle point at and that . I gave a rather hand-wavy argument about symmetry and the partial derivatives that I don't think is entirely accurate: Note that is even and decreasing on Since has a maximum at i.e. has a critical point there (since has bounded variation, and is increasing on , is differentiable a.e.). Since if the directional derivative of pointing into from the boundary is positive. In particular, has a critical point at by symmetry; from it is increasing, but from it is decreasing. Therefore and has a saddle point there. I also graphed the problem for and used Mathematica's NDSolve to make numerical plots of the solution; each appeared geometrically to have a saddle point at . For example, with the solution looks like: I'd be interested in a proper solution, if only to satisfy my curiosity.","\Omega=(-1,1)\times(-1,1) 
\left\{\begin{aligned}
\Delta u&=0,~~x\in\Omega\\
u(-1,y)&=u(1,y)=0\\
u(x,-1)&=u(x,1)=f(x)
\end{aligned}\right.
 f:[-1,1]\to\mathbb{R} [0,1] f(\pm 1)=0 u u(0,0) u(0,0)>0 f [0,1]. f:[-1,1]\to \mathbb{R}, f x=0, f f [0,1] f u\equiv 0 x=\pm 1, u \Omega u (0,0) x=\pm1 x=0,y=\pm1 u(0,0)>0 u f(x)=\{|1-x|,1-x^2,\cos(\pi/2 x)\} (0,0) f(x) = 1-x^2","['multivariable-calculus', 'partial-differential-equations', 'harmonic-functions', 'boundary-value-problem']"
6,How do I find the minimum and maximum of a multivariable function given two constraints?,How do I find the minimum and maximum of a multivariable function given two constraints?,,"Find the minimum and maximum of $f(x, y, z) = y + 4z$ subject to two constraints, $3x + z = 5$ and $x^2 + y^2 = 1$ . Having a hard time figuring out how to do this problem. I think I'm doing it right but I can't seem to get the correct answer in exact terms. Here's what I got so far (the labels are respectively up and down is left to right so $g(x,y,z)=3x+z=5$ : $$f_x = g_xλ + h_x\delta$$ $$f_y = g_yλ + h_y\delta$$ $$f_z = g_zλ + h_z\delta$$ $$0 = (3)λ + (2x)\delta$$ $$1 = (0)λ + (2y)\delta$$ $$4 = (1)λ + (0)\delta$$ After doing the calculations I get $λ = 4$ . Now I solve for $x$ and $y$ by plugging in that value into the equations. I get: $$x=-\frac{6}{\delta}$$ $$y=\frac{1}{2\delta}$$ Then I plug it into $f(x,y,z)$ and get: $$\frac{6}{\delta}^2+\frac{1}{2\delta}^2=1$$ $$\delta = \frac{\sqrt{(145)}}{2}$$ So solving for $x$ and $y$ again: $$x=-\frac{-12}{\sqrt{145}}$$ $$y=\frac{1}{\sqrt{145}}$$ Then I solve for $z$ by using $g(x,y,z)$ : $$-\frac{-12}{\sqrt{145}}*3+z=5$$ $$z=5+\frac{36\sqrt{145}}{145}$$ So then my point is: $$(\frac{12}{\sqrt{145}},\frac{1}{\sqrt{145}},5+\frac{36\sqrt{145}}{145})$$ I plug this into $f(x,y,z)$ : $$f(x,y,z)=\frac{1}{\sqrt{145}}+4\left(5-\frac{36\sqrt{145}}{145}\right)$$ and get: $$f(x,y,z)=\sqrt{145}+20$$ And so I use the opposite point to get the other value (each value multiplied by -1): $$f(x,y,z)=-\frac{1}{\sqrt{145}}+4\left(5-\frac{36\sqrt{145}}{145}\right)$$ $$f(x,y,z)=-\sqrt{145}+20\quad $$ So my final answers are: $$maximum = \sqrt{145}+20\quad$$ $$minimum = -\sqrt{145}+20\quad$$ Yet they're both wrong. I have no clue what's happening. I've checked my calculations a lot of times. I must be missing steps somewhere. I don't know how to solve this problem. If you just want to provide a final answer that'll at least help me back track. Thank you.","Find the minimum and maximum of subject to two constraints, and . Having a hard time figuring out how to do this problem. I think I'm doing it right but I can't seem to get the correct answer in exact terms. Here's what I got so far (the labels are respectively up and down is left to right so : After doing the calculations I get . Now I solve for and by plugging in that value into the equations. I get: Then I plug it into and get: So solving for and again: Then I solve for by using : So then my point is: I plug this into : and get: And so I use the opposite point to get the other value (each value multiplied by -1): So my final answers are: Yet they're both wrong. I have no clue what's happening. I've checked my calculations a lot of times. I must be missing steps somewhere. I don't know how to solve this problem. If you just want to provide a final answer that'll at least help me back track. Thank you.","f(x, y, z) = y + 4z 3x + z = 5 x^2 + y^2 = 1 g(x,y,z)=3x+z=5 f_x = g_xλ + h_x\delta f_y = g_yλ + h_y\delta f_z = g_zλ + h_z\delta 0 = (3)λ + (2x)\delta 1 = (0)λ + (2y)\delta 4 = (1)λ + (0)\delta λ = 4 x y x=-\frac{6}{\delta} y=\frac{1}{2\delta} f(x,y,z) \frac{6}{\delta}^2+\frac{1}{2\delta}^2=1 \delta = \frac{\sqrt{(145)}}{2} x y x=-\frac{-12}{\sqrt{145}} y=\frac{1}{\sqrt{145}} z g(x,y,z) -\frac{-12}{\sqrt{145}}*3+z=5 z=5+\frac{36\sqrt{145}}{145} (\frac{12}{\sqrt{145}},\frac{1}{\sqrt{145}},5+\frac{36\sqrt{145}}{145}) f(x,y,z) f(x,y,z)=\frac{1}{\sqrt{145}}+4\left(5-\frac{36\sqrt{145}}{145}\right) f(x,y,z)=\sqrt{145}+20 f(x,y,z)=-\frac{1}{\sqrt{145}}+4\left(5-\frac{36\sqrt{145}}{145}\right) f(x,y,z)=-\sqrt{145}+20\quad  maximum = \sqrt{145}+20\quad minimum = -\sqrt{145}+20\quad","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'lagrange-multiplier']"
7,Calculus of Variations (Pattern Recognition and Machine Learning),Calculus of Variations (Pattern Recognition and Machine Learning),,"According to Paul Sinclair this answer is incorrect. Can anyone explain how to use the calculus of variations to show that given $$E[L]=\int \int (y(\textbf{x})-t)^2 p(\textbf{x},t) d\textbf{x} dt$$ we have $$ \frac{\delta E[L]}{\delta y(\textbf{x})} = 2\int(y(\textbf{x})-t)p(\textbf{x},t)dt$$",According to Paul Sinclair this answer is incorrect. Can anyone explain how to use the calculus of variations to show that given we have,"E[L]=\int \int (y(\textbf{x})-t)^2 p(\textbf{x},t) d\textbf{x} dt  \frac{\delta E[L]}{\delta y(\textbf{x})} = 2\int(y(\textbf{x})-t)p(\textbf{x},t)dt","['multivariable-calculus', 'partial-derivative', 'calculus-of-variations', 'euler-lagrange-equation']"
8,"Use the $\varepsilon - \delta$ definition of the limit to verify that $\lim_{(x,y)\to(2,5)} xy = 10$.",Use the  definition of the limit to verify that .,"\varepsilon - \delta \lim_{(x,y)\to(2,5)} xy = 10","Question: Use the $\varepsilon - \delta$ definition of the limit to verify that $\lim\limits_{(x,y)\to(2,5)}xy = 10$ . Hint given: $xy − 10 = (x − 2)(y − 5) + 5(x − 2) + 2(y − 5).$ My solution $\forall \space\varepsilon\gt0\space\space\exists\space\delta(\varepsilon)\gt0:|xy-10|\lt\varepsilon$ where $(x,y)\in D$ , whenever $0\lt\sqrt{(x-2)^2+(y-5)^2}\lt\delta$ Note: $(x-2)^2 \le (x-2)^2+(y-5)^2$ $\implies |x-2|\le\sqrt{(x-2)^2+(y-5)^2}$ , similarly, $|y-5|\le\sqrt{(x-2)^2+(y-5)^2}$ So if $(x,y)\in D\space$ and $\space0\lt\sqrt{(x-2)^2+(y-5)^2}\lt\delta$ . We choose $ \delta :=\frac{-7+\sqrt{49+4\varepsilon}}2$ . We obtain... $\begin{aligned}|xy-10|&=|(x − 2)(y − 5) + 5(x − 2) + 2(y − 5)|\\&\le|x − 2||y − 5|+ 5|x − 2| + 2|y − 5|\\&\le \left((x-2)^2 +(y-5)^2\right) +5\sqrt{(x-2)^2+(y-5)^2}+2\sqrt{(x-2)^2+(y-5)^2}\\&=\left(\sqrt{(x-2)^2+(y-5)^2}\right)^2+7\sqrt{(x-2)^2+(y-5)^2}\\&\lt \delta^2+7\delta\\&=\frac{49\pm\sqrt{49-4\varepsilon}+49+4\varepsilon}4+\frac{-98\pm14\sqrt{49+4\varepsilon}}4\\&=\frac{4\varepsilon}4\\&=\varepsilon\end{aligned}$ Comments This is a question on my Analysis 2 module. Would be great if someone could check my solution. Also, I realise you have to use a minimum for $\delta$ , but unsure on how to unpack it when writing the solution, would be great if anyone can make my understanding of this clearer.","Question: Use the definition of the limit to verify that . Hint given: My solution where , whenever Note: , similarly, So if and . We choose . We obtain... Comments This is a question on my Analysis 2 module. Would be great if someone could check my solution. Also, I realise you have to use a minimum for , but unsure on how to unpack it when writing the solution, would be great if anyone can make my understanding of this clearer.","\varepsilon - \delta \lim\limits_{(x,y)\to(2,5)}xy = 10 xy − 10 = (x − 2)(y − 5) + 5(x − 2) + 2(y − 5). \forall \space\varepsilon\gt0\space\space\exists\space\delta(\varepsilon)\gt0:|xy-10|\lt\varepsilon (x,y)\in D 0\lt\sqrt{(x-2)^2+(y-5)^2}\lt\delta (x-2)^2 \le (x-2)^2+(y-5)^2 \implies |x-2|\le\sqrt{(x-2)^2+(y-5)^2} |y-5|\le\sqrt{(x-2)^2+(y-5)^2} (x,y)\in D\space \space0\lt\sqrt{(x-2)^2+(y-5)^2}\lt\delta  \delta :=\frac{-7+\sqrt{49+4\varepsilon}}2 \begin{aligned}|xy-10|&=|(x − 2)(y − 5) + 5(x − 2) + 2(y − 5)|\\&\le|x − 2||y − 5|+ 5|x − 2| + 2|y − 5|\\&\le \left((x-2)^2 +(y-5)^2\right) +5\sqrt{(x-2)^2+(y-5)^2}+2\sqrt{(x-2)^2+(y-5)^2}\\&=\left(\sqrt{(x-2)^2+(y-5)^2}\right)^2+7\sqrt{(x-2)^2+(y-5)^2}\\&\lt \delta^2+7\delta\\&=\frac{49\pm\sqrt{49-4\varepsilon}+49+4\varepsilon}4+\frac{-98\pm14\sqrt{49+4\varepsilon}}4\\&=\frac{4\varepsilon}4\\&=\varepsilon\end{aligned} \delta","['real-analysis', 'limits', 'multivariable-calculus', 'solution-verification', 'epsilon-delta']"
9,On the definition of the directional derivative,On the definition of the directional derivative,,"In the multivariable calculus course I took the directional derivative of a multivariable function $f(x,y)$ at $(a,b)$ in the direction of the vector $\vec{s}$ was defined as the following: $$f_s(a,b) = \vec{\nabla f} \cdot \vec{u_s}$$ where $\vec{u_s}$ is the unit vector in the same direction of $\vec{s}$ . Now I have come across the following definition: $$\frac{d}{d\alpha} f(\vec{v} + \alpha\vec{s})$$ evaluated at $\alpha = 0$ $(\vec{v}$ is supposed to be the vector at which the derivative is evaluated). I am struggling to see why the two definition are equal.",In the multivariable calculus course I took the directional derivative of a multivariable function at in the direction of the vector was defined as the following: where is the unit vector in the same direction of . Now I have come across the following definition: evaluated at is supposed to be the vector at which the derivative is evaluated). I am struggling to see why the two definition are equal.,"f(x,y) (a,b) \vec{s} f_s(a,b) = \vec{\nabla f} \cdot \vec{u_s} \vec{u_s} \vec{s} \frac{d}{d\alpha} f(\vec{v} + \alpha\vec{s}) \alpha = 0 (\vec{v}",['multivariable-calculus']
10,How to use Dirac function to represent constraint in an integral?,How to use Dirac function to represent constraint in an integral?,,"Hello and I want to do integral under constraint like: $ \int_{f(x1,x2...xn) = 0}g(x1,x2,...xn)\rm dx1dx2...dxn $ some book told me I can use Dirac function to represent constraint as: $ \int_{x1,x2...xn}g(x1,x2,...xn)\delta(f(x1,x2...xn))\rm dx1dx2...dxn $ but when I try it in practice I find something make me confused, here is an example: I want to calculate circumference of an unit circle so I use integral as : $ \int_{x}\int_{y}\delta(x^2 + y^2 - 1)\rm dxdy $ and we know $ \delta(f(x)) = \sum_{i=0}^N\frac{\delta(x-xi)}{\vert{\frac{\partial{f(x)}}{\partial x}} \quad\vert x=xi} $ xi is root of f(x) so I take y as variable and $ y = \pm\sqrt{1-x^2}$ $\delta(x^2 + y^2 - 1)=\frac{\delta(y-\sqrt{1-x^2})}{2*\sqrt{1-x^2}} + \frac{\delta(y+\sqrt{1-x^2})}{2*\sqrt{1-x^2}}$ so $ \int_{x}\int_{y}\delta(x^2 + y^2 - 1)\rm dxdy = \int_{x=-1}^{1}\frac{1}{\sqrt{1-x^2}} \rm dx = \pi$ the constant 2 is missing, and if I change the constraint to $ \sqrt{x^2 + y^2} = 1 $ which is equivalent to $ x^2 + y^2 = 1$ , then $\delta(\sqrt{x^2 + y^2} - 1)=\frac{\delta(y-\sqrt{1-x^2})}{\sqrt{1-x^2}} + \frac{\delta(y+\sqrt{1-x^2})}{\sqrt{1-x^2}}$ it's two times than before, in fact different type of constraint which have same effect can result to different scalar so it make me confused how to integrate constraint to integral to avoid the scalar error?","Hello and I want to do integral under constraint like: some book told me I can use Dirac function to represent constraint as: but when I try it in practice I find something make me confused, here is an example: I want to calculate circumference of an unit circle so I use integral as : and we know xi is root of f(x) so I take y as variable and so the constant 2 is missing, and if I change the constraint to which is equivalent to , then it's two times than before, in fact different type of constraint which have same effect can result to different scalar so it make me confused how to integrate constraint to integral to avoid the scalar error?"," \int_{f(x1,x2...xn) = 0}g(x1,x2,...xn)\rm dx1dx2...dxn   \int_{x1,x2...xn}g(x1,x2,...xn)\delta(f(x1,x2...xn))\rm dx1dx2...dxn   \int_{x}\int_{y}\delta(x^2 + y^2 - 1)\rm dxdy   \delta(f(x)) = \sum_{i=0}^N\frac{\delta(x-xi)}{\vert{\frac{\partial{f(x)}}{\partial x}} \quad\vert x=xi}   y = \pm\sqrt{1-x^2} \delta(x^2 + y^2 - 1)=\frac{\delta(y-\sqrt{1-x^2})}{2*\sqrt{1-x^2}} + \frac{\delta(y+\sqrt{1-x^2})}{2*\sqrt{1-x^2}}  \int_{x}\int_{y}\delta(x^2 + y^2 - 1)\rm dxdy = \int_{x=-1}^{1}\frac{1}{\sqrt{1-x^2}} \rm dx = \pi  \sqrt{x^2 + y^2} = 1   x^2 + y^2 = 1 \delta(\sqrt{x^2 + y^2} - 1)=\frac{\delta(y-\sqrt{1-x^2})}{\sqrt{1-x^2}} + \frac{\delta(y+\sqrt{1-x^2})}{\sqrt{1-x^2}}","['integration', 'multivariable-calculus', 'distribution-theory', 'dirac-delta', 'constraints']"
11,What does it mean to differentiate $f(\theta_1)/f(\theta_2) = C$ with respect to theta?,What does it mean to differentiate  with respect to theta?,f(\theta_1)/f(\theta_2) = C,"In the book I am reading, at one point they differentiate Snell's Law with respect to ${\theta}$ : $$\frac{d}{d\theta}\Bigl(\frac{\sin(\theta_{1})}{\sin(\theta_{2})}=\frac{\eta_{2}}{\eta_{1}}\Bigr),$$ which they claim gives the result: $$\frac{\cos(\theta_{1})d\theta_{1}}{\cos(\theta_{2})d\theta_{2}}=\frac{\eta_{2}}{\eta_{1}}.$$ I have no reason not to believe that this is true, but I really don't understand what it means to differentiate two different variables ( $\theta_{1}$ and $\theta_{2}$ ) with respect to a third variable ( $\theta$ ). I suppose the two variables aren't necessarily completely independent as they are both values on the axis of $\theta$ , but I can't really grasp it. $\frac{\eta_{2}}{\eta_{1}}$ is also a constant, so why would it not go to $0$ ? Here is a link to the section of the book in question: http://www.pbr-book.org/3ed-2018/Reflection_Models/Specular_Reflection_and_Transmission.html#eq:spherical-L-transmitted and their definition of Snell's Law: http://www.pbr-book.org/3ed-2018/Reflection_Models/Specular_Reflection_and_Transmission.html#eq:snells-law I am new here, so apologies in advance if I have done something incorrectly in this post. Any nudge in the right direction would be really appreciated! EDIT: Actually, $\theta_{1}$ and $\theta_{2}$ are just functions of $\theta$ aren't they? Oops. So I guess I need to think of it like this: $$\frac{d}{d\theta}\Bigl(\frac{f(g(\theta))}{f(h(\theta))}=C\Bigr),$$","In the book I am reading, at one point they differentiate Snell's Law with respect to : which they claim gives the result: I have no reason not to believe that this is true, but I really don't understand what it means to differentiate two different variables ( and ) with respect to a third variable ( ). I suppose the two variables aren't necessarily completely independent as they are both values on the axis of , but I can't really grasp it. is also a constant, so why would it not go to ? Here is a link to the section of the book in question: http://www.pbr-book.org/3ed-2018/Reflection_Models/Specular_Reflection_and_Transmission.html#eq:spherical-L-transmitted and their definition of Snell's Law: http://www.pbr-book.org/3ed-2018/Reflection_Models/Specular_Reflection_and_Transmission.html#eq:snells-law I am new here, so apologies in advance if I have done something incorrectly in this post. Any nudge in the right direction would be really appreciated! EDIT: Actually, and are just functions of aren't they? Oops. So I guess I need to think of it like this:","{\theta} \frac{d}{d\theta}\Bigl(\frac{\sin(\theta_{1})}{\sin(\theta_{2})}=\frac{\eta_{2}}{\eta_{1}}\Bigr), \frac{\cos(\theta_{1})d\theta_{1}}{\cos(\theta_{2})d\theta_{2}}=\frac{\eta_{2}}{\eta_{1}}. \theta_{1} \theta_{2} \theta \theta \frac{\eta_{2}}{\eta_{1}} 0 \theta_{1} \theta_{2} \theta \frac{d}{d\theta}\Bigl(\frac{f(g(\theta))}{f(h(\theta))}=C\Bigr),","['calculus', 'multivariable-calculus', 'physics']"
12,How prove this limit via epsilon delta?,How prove this limit via epsilon delta?,,"$$\lim_{(x,y) \to (4,1)}{\frac{y}{2x-y}}=\frac{1}{7}$$ I know so far that $|x-4|<\delta\quad \&\quad |y-1|<\delta$ , and that I can use $$\bigg|\frac{y-1}{2x-y} + \frac{1}{2x-y} - \frac{1}{7}\bigg|$$ and I get one delta, but how to continue from here, or is it even correct?","I know so far that , and that I can use and I get one delta, but how to continue from here, or is it even correct?","\lim_{(x,y) \to (4,1)}{\frac{y}{2x-y}}=\frac{1}{7} |x-4|<\delta\quad \&\quad |y-1|<\delta \bigg|\frac{y-1}{2x-y} + \frac{1}{2x-y} - \frac{1}{7}\bigg|",['multivariable-calculus']
13,"Elliptic curve: Type of reduction mod 2, how can I show the curve has a cusp?","Elliptic curve: Type of reduction mod 2, how can I show the curve has a cusp?",,"I want to know what type of reduction the curve $E : y^2 = x^3 + 7x$ has at $p=2$ . From online search, I obtain that it has additive/cuspidal reduction. But this disagrees with my own computation, which means I must be doing something wrong. My own computation is this: Modulo 2, the curve becomes $y^2 = x^3 + x$ . This has a double root at $(1, 0)$ . So I make the change of coordinates $x' = x-1$ to shift the singular point to $(0, 0)$ , and the curve in the new coords (after relabelling $x'$ back to $x$ ) is $y^2 = x^3 + x^2$ . Rearranging, this is $$ x^3 + x^2 - y^2 = 0 $$ This can be viewed as the Taylor expansion of my curve at $(0, 0)$ , and so there is a double point, and the tangent lines are given by factorizing $(x^2 - y^2) = (x-y)(x+y)$ . From this, I conclude that there is split multiplicative reduction. What am I doing wrong? Does it have something to do with how over $\mathbb{F}_2$ , the tangent lines $(x-y)$ and $(x+y)$ are actually the same lines? I would really appreciate if you could tell me where I am wrong in this 'proof'. show me how to do it correctly. bonus helpfulness if you could direct me a good resource to understand this concept of computing the type of singular point properly/efficiently. Thank you very much!","I want to know what type of reduction the curve has at . From online search, I obtain that it has additive/cuspidal reduction. But this disagrees with my own computation, which means I must be doing something wrong. My own computation is this: Modulo 2, the curve becomes . This has a double root at . So I make the change of coordinates to shift the singular point to , and the curve in the new coords (after relabelling back to ) is . Rearranging, this is This can be viewed as the Taylor expansion of my curve at , and so there is a double point, and the tangent lines are given by factorizing . From this, I conclude that there is split multiplicative reduction. What am I doing wrong? Does it have something to do with how over , the tangent lines and are actually the same lines? I would really appreciate if you could tell me where I am wrong in this 'proof'. show me how to do it correctly. bonus helpfulness if you could direct me a good resource to understand this concept of computing the type of singular point properly/efficiently. Thank you very much!","E : y^2 = x^3 + 7x p=2 y^2 = x^3 + x (1, 0) x' = x-1 (0, 0) x' x y^2 = x^3 + x^2  x^3 + x^2 - y^2 = 0  (0, 0) (x^2 - y^2) = (x-y)(x+y) \mathbb{F}_2 (x-y) (x+y)","['multivariable-calculus', 'algebraic-geometry', 'elliptic-curves', 'projective-geometry']"
14,Very fundamental doubt in integration,Very fundamental doubt in integration,,"Let $f$ be a function of two variables $(x, y)$ . Now I want to integrate $\frac{df(x,y)}{dx}$ with respect to x, i.e $$\int_a^b \frac{df(x,y)}{dx} dx$$ If y isn't a function of x then this is straight forward and the answer is $$\int_a^b \frac{\partial f(x,y)}{\partial x} dx = f(b,y)-f(a,y)$$ But if $y$ is a function of $x$ , then $$\frac{df(x,y)}{dx}= \frac{\partial f(x,y)}{\partial x} + \frac{\partial f(x,y)}{\partial y} \frac{dy}{dx}$$ hence, $$\int_a^b \frac{df(x,y)}{dx} dx=  \int_a^b \frac{\partial f(x,y)}{\partial x}dx +\int_a^b \frac{\partial f(x,y)}{\partial y} \frac{dy}{dx}dx$$ $$\int_a^b \frac{df(x,y)}{dx} dx= f(b,y)-f(a,y)+ \int_{y(a)}^{y(b)} \frac{\partial f(x,y)}{\partial y} dy$$ $$\int_a^b \frac{df(x,y)}{dx} dx= f(b,y)-f(a,y)+ f(x,y(b))-f(x,y(a))$$ After this step it has become very confusing as RHS shouldn't come out to be a function of $x$ and also if instead of solving like this I simply consider that $$\int_a^b \frac{df(x,y)}{dx} dx = \int_{f(a,y(a))}^{f(b,y(b))}df(x,y)$$ then the answer comes out to be totally different. Please tell me where I am wrong. This has made me embarrass a lot.","Let be a function of two variables . Now I want to integrate with respect to x, i.e If y isn't a function of x then this is straight forward and the answer is But if is a function of , then hence, After this step it has become very confusing as RHS shouldn't come out to be a function of and also if instead of solving like this I simply consider that then the answer comes out to be totally different. Please tell me where I am wrong. This has made me embarrass a lot.","f (x, y) \frac{df(x,y)}{dx} \int_a^b \frac{df(x,y)}{dx} dx \int_a^b \frac{\partial f(x,y)}{\partial x} dx = f(b,y)-f(a,y) y x \frac{df(x,y)}{dx}= \frac{\partial f(x,y)}{\partial x} + \frac{\partial f(x,y)}{\partial y} \frac{dy}{dx} \int_a^b \frac{df(x,y)}{dx} dx=  \int_a^b \frac{\partial f(x,y)}{\partial x}dx +\int_a^b \frac{\partial f(x,y)}{\partial y} \frac{dy}{dx}dx \int_a^b \frac{df(x,y)}{dx} dx= f(b,y)-f(a,y)+ \int_{y(a)}^{y(b)} \frac{\partial f(x,y)}{\partial y} dy \int_a^b \frac{df(x,y)}{dx} dx= f(b,y)-f(a,y)+ f(x,y(b))-f(x,y(a)) x \int_a^b \frac{df(x,y)}{dx} dx = \int_{f(a,y(a))}^{f(b,y(b))}df(x,y)","['integration', 'multivariable-calculus', 'definite-integrals']"
15,"Implicit functions of $f(x,y,z)= x \sin z- y \cos z=0$",Implicit functions of,"f(x,y,z)= x \sin z- y \cos z=0","Let $U=\{(x,y,z) \in \mathbb{R}^{3} : x>0\}$ open and $f:U \rightarrow \mathbb{R}$ defined by $$f(x,y,z)= x \sin z- y \cos z.$$ Show that given $p=(x,y,z) \in \mathbb{R}^3$ if $f(p)=0$ then $\frac{\partial f}{\partial z}(p)\neq 0$ . Let $V=\{(x,y) \in \mathbb{R}^{2}: x>0\}$ show that there are infinitely many continuous functions $g:V \rightarrow \mathbb{R}$ such that $f(x,y, g(x,y))=0$ for all $(x,y) \in V$ , each $g$ is of class $\mathbb{C}^{\infty}$ and any two of them differ by a constant. for the first part, we have $f(x,y,z)=x \sin z- y \cos z =0$ , this is $$\sin z=\frac{y \cos z}{x}.$$ Deriving we have $$\frac{\partial f}{\partial z}=x \cos z+ y \sin z= x \cos z + y \sin z.$$ If we assume that $ \frac{\partial f}{\partial z}=0$ , we have $$x \cos z + y \sin z=x \cos z + y \frac{y \cos z}{x}=\cos z (x^2+y^2)=0$$ as $x>0$ , we have $\cos z=0$ then $z=\frac{(2n+1) \pi}{2}$ , which is a contradiction because $x \sin z- y \cos z =0$ . I don't know how to demonstrate the second, any help please?","Let open and defined by Show that given if then . Let show that there are infinitely many continuous functions such that for all , each is of class and any two of them differ by a constant. for the first part, we have , this is Deriving we have If we assume that , we have as , we have then , which is a contradiction because . I don't know how to demonstrate the second, any help please?","U=\{(x,y,z) \in \mathbb{R}^{3} : x>0\} f:U \rightarrow \mathbb{R} f(x,y,z)= x \sin z- y \cos z. p=(x,y,z) \in \mathbb{R}^3 f(p)=0 \frac{\partial f}{\partial z}(p)\neq 0 V=\{(x,y) \in \mathbb{R}^{2}: x>0\} g:V \rightarrow \mathbb{R} f(x,y, g(x,y))=0 (x,y) \in V g \mathbb{C}^{\infty} f(x,y,z)=x \sin z- y \cos z =0 \sin z=\frac{y \cos z}{x}. \frac{\partial f}{\partial z}=x \cos z+ y \sin z= x \cos z + y \sin z.  \frac{\partial f}{\partial z}=0 x \cos z + y \sin z=x \cos z + y \frac{y \cos z}{x}=\cos z (x^2+y^2)=0 x>0 \cos z=0 z=\frac{(2n+1) \pi}{2} x \sin z- y \cos z =0","['multivariable-calculus', 'partial-derivative']"
16,Converting integration dV in spherical coordinates for volume but not for surface?,Converting integration dV in spherical coordinates for volume but not for surface?,,"When calculating the volume of a spherical solid, i.e. a triple integral over angles and radius, the standard $dx\,dy\,dz$ gets converted into $f(x,y,z)r^2\sin\Phi \,d\Phi \,d\Theta \,dr$ . However, it seems that when we calculate a spherical surface integral, that is not the case, and we instead just have $f(x,y,z)\left|\frac{\delta r}{\delta \Phi}\times\frac{\delta r}{\delta \Theta}\right|\,d\Phi \,d\Theta$ . Why is that? I'm just confused about when I should ""convert"" when parametrizing a surface and when not.","When calculating the volume of a spherical solid, i.e. a triple integral over angles and radius, the standard gets converted into . However, it seems that when we calculate a spherical surface integral, that is not the case, and we instead just have . Why is that? I'm just confused about when I should ""convert"" when parametrizing a surface and when not.","dx\,dy\,dz f(x,y,z)r^2\sin\Phi \,d\Phi \,d\Theta \,dr f(x,y,z)\left|\frac{\delta r}{\delta \Phi}\times\frac{\delta r}{\delta \Theta}\right|\,d\Phi \,d\Theta","['integration', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
17,Evaluation Of Iterated Limit,Evaluation Of Iterated Limit,,"Let $f: \textrm{dom}(f) \rightarrow \mathbb{R}$ . Let $a \in \textrm{dom}(f)$ . Assume $f''(a)$ exists. Want To Prove $f''(a) = \displaystyle\lim_{h \rightarrow 0} \frac{f(a + h) + f(a - h) - 2f(a)}{h^2}$ . $f''(a)$ $= \displaystyle\lim_{h \rightarrow 0} \frac{f'(a + h) - f'(a)}{h}$ $= \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h)}{k} - \lim_{k \rightarrow 0} \frac{f(a + k) - f(a)}{k}}{h}$ $= \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \bigg( \frac{f(a + h + k) - f(a + h)}{k} - \frac{f(a + k) - f(a)}{k} \bigg)}{h}$ $= \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h) - f(a + k) + f(a)}{k}}{h}$ $= \displaystyle\lim_{h \rightarrow 0} \Bigg( \! \lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h) - f(a + k) + f(a)}{hk} \Bigg)$ In order to get the desired result, I would like to replace $k$ with $-h$ . I did this because as $h \rightarrow 0$ and $k \rightarrow 0$ , $k \rightarrow -h$ . However, this reasoning of "" $h \rightarrow 0$ and $k \rightarrow 0$ "" would be more applicable in the case of a two-variable limit as $(h, k) \rightarrow (0, 0)$ rather than an iterated limit as seen in the expression, which may not be equal: Link . So how can I justify replacing $k$ with $-h$ ? I thought about using the Moore-Osgood Theorem, which says the iterated limit is equal to the two-variable limit under a sufficient condition. However, I am unfamiliar with multivariable calculus and I am not sure how I would use it in this case. Unfortunately, I also cannot find a proof that follows Wikipedia's description: Link .","Let . Let . Assume exists. Want To Prove . In order to get the desired result, I would like to replace with . I did this because as and , . However, this reasoning of "" and "" would be more applicable in the case of a two-variable limit as rather than an iterated limit as seen in the expression, which may not be equal: Link . So how can I justify replacing with ? I thought about using the Moore-Osgood Theorem, which says the iterated limit is equal to the two-variable limit under a sufficient condition. However, I am unfamiliar with multivariable calculus and I am not sure how I would use it in this case. Unfortunately, I also cannot find a proof that follows Wikipedia's description: Link .","f: \textrm{dom}(f) \rightarrow \mathbb{R} a \in \textrm{dom}(f) f''(a) f''(a) = \displaystyle\lim_{h \rightarrow 0} \frac{f(a + h) + f(a - h) - 2f(a)}{h^2} f''(a) = \displaystyle\lim_{h \rightarrow 0} \frac{f'(a + h) - f'(a)}{h} = \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h)}{k} - \lim_{k \rightarrow 0} \frac{f(a + k) - f(a)}{k}}{h} = \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \bigg( \frac{f(a + h + k) - f(a + h)}{k} - \frac{f(a + k) - f(a)}{k} \bigg)}{h} = \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h) - f(a + k) + f(a)}{k}}{h} = \displaystyle\lim_{h \rightarrow 0} \Bigg( \! \lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h) - f(a + k) + f(a)}{hk} \Bigg) k -h h \rightarrow 0 k \rightarrow 0 k \rightarrow -h h \rightarrow 0 k \rightarrow 0 (h, k) \rightarrow (0, 0) k -h","['calculus', 'limits', 'multivariable-calculus', 'derivatives']"
18,"Compute the area of a surface, encountering a strange integral","Compute the area of a surface, encountering a strange integral",,"compute the area of this yellow surface, which is actually a paraboloid: $x^2+y^2=2az$ (yellow one), cutted by $(x^2+y^2)^2=2a^2xy$ (blue one) To compute the part be surrounded in the blue surface, use polor coordinates: \begin{cases} x=r\cos\theta\\ y=r\sin\theta\\ z=\frac{r^2}{2a}\\ \end{cases} use Gauss efficient to compute the area: \begin{cases} E=(\frac{\partial x}{\partial r})^2+(\frac{\partial y}{\partial r})^2+(\frac{\partial z}{\partial r})^2&=1+\frac{r}{a}\\  F=\frac{\partial x}{\partial r}\frac{\partial x}{\partial\theta}+\frac{\partial y}{\partial r}\frac{\partial y}{\partial\theta}+\frac{\partial z}{\partial r}\frac{\partial z}{\partial\theta}&=0\\ G=(\frac{\partial x}{\partial\theta})^2+(\frac{\partial y}{\partial\theta})^2+(\frac{\partial z}{\partial\theta})^2&=r^2\\  \end{cases} $$S=\iint\limits_D\sqrt{EG-F^2}drd\theta$$ where D becomes $\left\{ (r,\theta )|\theta \in \left[ 0,\frac{\pi}{2} \right] \cup \left[ \pi ,\frac{3}{2}\pi \right] ,\mathrm{r}\in \left[ 0,\mathrm{a}\sqrt{\sin 2\theta} \right] \right\}$ since the blue surface can be written as $r^2=a^2\sin2\theta$ . I can't find anything wrong until now, but this intergal is extreme complex, and Mathematica give me a non elementary solution. However, the standard solution given by the text book to this question is $\frac{20-3 \pi}{9} a^{2}$ How can i get that answer? Or what's wrong with my method?","compute the area of this yellow surface, which is actually a paraboloid: (yellow one), cutted by (blue one) To compute the part be surrounded in the blue surface, use polor coordinates: use Gauss efficient to compute the area: where D becomes since the blue surface can be written as . I can't find anything wrong until now, but this intergal is extreme complex, and Mathematica give me a non elementary solution. However, the standard solution given by the text book to this question is How can i get that answer? Or what's wrong with my method?","x^2+y^2=2az (x^2+y^2)^2=2a^2xy \begin{cases}
x=r\cos\theta\\
y=r\sin\theta\\
z=\frac{r^2}{2a}\\
\end{cases} \begin{cases}
E=(\frac{\partial x}{\partial r})^2+(\frac{\partial y}{\partial r})^2+(\frac{\partial z}{\partial r})^2&=1+\frac{r}{a}\\ 
F=\frac{\partial x}{\partial r}\frac{\partial x}{\partial\theta}+\frac{\partial y}{\partial r}\frac{\partial y}{\partial\theta}+\frac{\partial z}{\partial r}\frac{\partial z}{\partial\theta}&=0\\
G=(\frac{\partial x}{\partial\theta})^2+(\frac{\partial y}{\partial\theta})^2+(\frac{\partial z}{\partial\theta})^2&=r^2\\ 
\end{cases} S=\iint\limits_D\sqrt{EG-F^2}drd\theta \left\{ (r,\theta )|\theta \in \left[ 0,\frac{\pi}{2} \right] \cup \left[ \pi ,\frac{3}{2}\pi \right] ,\mathrm{r}\in \left[ 0,\mathrm{a}\sqrt{\sin 2\theta} \right] \right\} r^2=a^2\sin2\theta \frac{20-3 \pi}{9} a^{2}","['multivariable-calculus', 'surface-integrals']"
19,A curve in $\Bbb R^2$ with non zero curvature is characterized by it's curvature. Is there a contradiction through this example?,A curve in  with non zero curvature is characterized by it's curvature. Is there a contradiction through this example?,\Bbb R^2,"A curve in $\Bbb R^2$ with non zero curvature is characterized by it's curvature i.e.  Let $f: \Bbb R \rightarrow \Bbb R^2 ~\& ~g:\Bbb R \rightarrow \Bbb R^2$ be two $2$ times differentiable path-length parametrizations of curves $C_f$ and $C_g $ in $\Bbb R^3$ . If $k_f(s)=k_g(s) \ne 0 ~\forall ~s \in \Bbb R,$ then the two curves are identical except for probably their position in $\Bbb R^2$ Let us consider an example. Let $C_f$ be the curve parametrized by the function $f :\Bbb R \rightarrow \Bbb R^2~|~f(t)=(t,t^3) $ and $C_g$ be the curve parametrized by the function $g: \Bbb R \rightarrow \Bbb R^2~|~g(t)=(t,|t^3|).$ Then, I was able to compute that the curvatures $k_{C_f}(f(t))=k_{C_g}((g(t))=\dfrac {6t}{\sqrt {1+9t^4}}, t \in \Bbb R$ . But, if we sketch the images $\{(t,t^3): t \in \Bbb R \}$ and $\{(t,|t^3|): t \in \Bbb R \}$ , then they are not identical. Why does this seem to contradict the above theorem in bold?","A curve in with non zero curvature is characterized by it's curvature i.e.  Let be two times differentiable path-length parametrizations of curves and in . If then the two curves are identical except for probably their position in Let us consider an example. Let be the curve parametrized by the function and be the curve parametrized by the function Then, I was able to compute that the curvatures . But, if we sketch the images and , then they are not identical. Why does this seem to contradict the above theorem in bold?","\Bbb R^2 f: \Bbb R \rightarrow \Bbb R^2 ~\& ~g:\Bbb R \rightarrow \Bbb R^2 2 C_f C_g  \Bbb R^3 k_f(s)=k_g(s) \ne 0 ~\forall ~s \in \Bbb R, \Bbb R^2 C_f f :\Bbb R \rightarrow \Bbb R^2~|~f(t)=(t,t^3)  C_g g: \Bbb R \rightarrow \Bbb R^2~|~g(t)=(t,|t^3|). k_{C_f}(f(t))=k_{C_g}((g(t))=\dfrac {6t}{\sqrt {1+9t^4}}, t \in \Bbb R \{(t,t^3): t \in \Bbb R \} \{(t,|t^3|): t \in \Bbb R \}","['multivariable-calculus', 'differential-geometry', 'curves', 'parametrization']"
20,Application of the change of variables theorem on the n-ball.,Application of the change of variables theorem on the n-ball.,,"Let f: $\mathbb{R} \to \mathbb{R}$ be continuous function and $z \in \mathbb{R}^n$ . Show that $$ \int_Bf(\langle x,z \rangle)=\int_Bf(x_n|z|) $$ where $x=(x_1,...,x_n)$ and $B=\{x\in \mathbb{R}^n ; |x| \leq 1\}$ . My idea is to apply the change of variables theorem. I tried to find an orthogonal tranformation $h(x)=Qx$ such that $|detDh|=|detQ|=|\pm1|=1$ . But I couldn't find such transformation. I would appreciate any tips on how to find this transformation or a new idea on how to solve this problem.",Let f: be continuous function and . Show that where and . My idea is to apply the change of variables theorem. I tried to find an orthogonal tranformation such that . But I couldn't find such transformation. I would appreciate any tips on how to find this transformation or a new idea on how to solve this problem.,"\mathbb{R} \to \mathbb{R} z \in \mathbb{R}^n 
\int_Bf(\langle x,z \rangle)=\int_Bf(x_n|z|)
 x=(x_1,...,x_n) B=\{x\in \mathbb{R}^n ; |x| \leq 1\} h(x)=Qx |detDh|=|detQ|=|\pm1|=1","['multivariable-calculus', 'change-of-variable']"
21,"Calculate $\int_Sxyz \cdot dS$ where $S$ is the triangle $(1,0,0),(0,2,0),(0,1,1)$",Calculate  where  is the triangle,"\int_Sxyz \cdot dS S (1,0,0),(0,2,0),(0,1,1)","Calculate the integral: $$\int_Sxyz \cdot dS$$ where $S$ is the surface of the triangle with vertices: $P(1,0,0) ,Q(0,2,0),R(0,1,1).$ I've find the  plane where $P,Q,R$ are points in it: $$Π :(x,y,z)=r\vec {PQ}+t\vec {PR} +(1,0,0)$$ $$\text{Hence } ,\text{ Π: }2x+y+z=2$$ $$\Rightarrow z=2-y-2x$$ $$z_x^2=4,z_y^2=1$$ $$\int_SxyzdS=\int_0^1\int_0^{2-2x}xy\cdot (2-y-2x)\sqrt{1+4+1}\cdot dydx=\frac{\sqrt{6}}{15}.$$ But the answer is $\frac{\sqrt{6}}{30}$ What am i doing wrong? Thank you.",Calculate the integral: where is the surface of the triangle with vertices: I've find the  plane where are points in it: But the answer is What am i doing wrong? Thank you.,"\int_Sxyz \cdot dS S P(1,0,0) ,Q(0,2,0),R(0,1,1). P,Q,R Π :(x,y,z)=r\vec {PQ}+t\vec {PR} +(1,0,0) \text{Hence } ,\text{ Π: }2x+y+z=2 \Rightarrow z=2-y-2x z_x^2=4,z_y^2=1 \int_SxyzdS=\int_0^1\int_0^{2-2x}xy\cdot (2-y-2x)\sqrt{1+4+1}\cdot dydx=\frac{\sqrt{6}}{15}. \frac{\sqrt{6}}{30}","['integration', 'multivariable-calculus', 'multiple-integral']"
22,"If $s \ge 2,$ then $\sum\limits_{k \ge 1} \frac{(-1)^k}{k!} s_k = 0$ where $s_k = \sum\limits_{b_1+\dots+b_k=s-k} \prod\limits_i \frac{1}{b_i+1}.$",If  then  where,"s \ge 2, \sum\limits_{k \ge 1} \frac{(-1)^k}{k!} s_k = 0 s_k = \sum\limits_{b_1+\dots+b_k=s-k} \prod\limits_i \frac{1}{b_i+1}.","Show that if $s \ge 2,$ then $\sum\limits_{k \ge 1} \frac{(-1)^k}{k!} s_k = 0$ where $s_k = \sum\limits_{b_1+\dots+b_k=s-k} \prod\limits_i \frac{1}{b_i+1}$ and the sum is over all non-negative $b_i.$ I was working on a problem related to the probability of certain cycles appearing and managed to show that the result I needed was equivalent to $\frac{t}{n} = \sum\limits_{r \ge 1} \sum\limits_{a_1, \dots, a_r \ge 1} \frac{(-1)^{r+1}}{r!} \binom{t}{a_1, \dots, a_r} \prod\limits_{i=1}^r \frac{(a_i-1)!}{n^{a_i}}$ for all $n.$ The coefficient of $1/n$ is clearly $t,$ so I realized that it suffices to show the coefficient of $\frac{1}{n^s}$ is zero for all $s \ge 2.$ After rearranging and removing the chaff, the claim that these coefficients vanish is the equality in the title of this post. After all of the work I've done on my problem, I would hate to have to start over from scratch. Hopefully, the question I posed has a nice and simple proof. You can rewrite $s_k$ as $$\int\limits_{[0,1]^k}\left[\sum_{b_1+\dots+b_k = s-k} x_1^{b_1} \cdots x_k^{b_k} \right] dx_1 \dots dx_k,$$ but I'm not sure whether this will help. You can go one step further and sneak $\frac{(-1)^k}{k!}$ in there: $$\frac{(-1)^k}{k!}s_k = \int\limits_{[0,1]^k}\left[\sum_{b_1+\dots+b_k = s-k} \prod\limits_i -x_i^{i(b_i+1)-1)} \right] dx_1 \dots dx_k.$$ But that still doesn't make combining all of the integrals any easier.","Show that if then where and the sum is over all non-negative I was working on a problem related to the probability of certain cycles appearing and managed to show that the result I needed was equivalent to for all The coefficient of is clearly so I realized that it suffices to show the coefficient of is zero for all After rearranging and removing the chaff, the claim that these coefficients vanish is the equality in the title of this post. After all of the work I've done on my problem, I would hate to have to start over from scratch. Hopefully, the question I posed has a nice and simple proof. You can rewrite as but I'm not sure whether this will help. You can go one step further and sneak in there: But that still doesn't make combining all of the integrals any easier.","s \ge 2, \sum\limits_{k \ge 1} \frac{(-1)^k}{k!} s_k = 0 s_k = \sum\limits_{b_1+\dots+b_k=s-k} \prod\limits_i \frac{1}{b_i+1} b_i. \frac{t}{n} = \sum\limits_{r \ge 1} \sum\limits_{a_1, \dots, a_r \ge 1} \frac{(-1)^{r+1}}{r!} \binom{t}{a_1, \dots, a_r} \prod\limits_{i=1}^r \frac{(a_i-1)!}{n^{a_i}} n. 1/n t, \frac{1}{n^s} s \ge 2. s_k \int\limits_{[0,1]^k}\left[\sum_{b_1+\dots+b_k = s-k} x_1^{b_1} \cdots x_k^{b_k} \right] dx_1 \dots dx_k, \frac{(-1)^k}{k!} \frac{(-1)^k}{k!}s_k = \int\limits_{[0,1]^k}\left[\sum_{b_1+\dots+b_k = s-k} \prod\limits_i -x_i^{i(b_i+1)-1)} \right] dx_1 \dots dx_k.","['calculus', 'combinatorics', 'multivariable-calculus', 'summation']"
23,Potential function with a fixed set of minima and no poles.,Potential function with a fixed set of minima and no poles.,,"Can we find a continuous, almost-everywhere smooth, potential function with no poles, such that the set of local minima for the sum of these potentials around $n$ points are exactly those $n$ points? We are given a set of $n$ points $(x_0, \ldots, x_{n-1})$ in $\mathbb{R}^d$ . We consider the additive potential function $$g(x) = \sum_{i=1}^n f(x, x_i)$$ as a sum of potentials $f : \mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R}$ , where $f$ is continuous and almost everywhere smooth. We would like $g$ ’s minima to be exactly the $x_i$ . That is, any point $x$ where the gradient of $g$ vanishes is either one of the $x_i$ , a saddle point, or a maximum. $$\forall x, \nabla g(x) = \vec{0} \implies x \in \{x_1, \ldots, x_n\}~\vee~ \exists v, \lambda < 0, H(g)(x) v = \lambda v$$ By way of example, we can construct such a function with the gravitational potential. If we let $$f(x,y) = -\frac{1}{||x-y||},$$ I believe the property is respected. A point mass will always end up “falling” towards one of the attractors, or at least I believe that’s implied by Earnshaw’s theorem. I’m not 100% sure it works for all norms, but the Euclidian distance will do. For this potential, the $x_i$ aren’t just local minima, they are poles. We would like to find a better-behaved function which does not exhibit poles. One way to “cheat” is to cut-off the poles. Once we get close enough to one of the $x_i$ , the influence of the other points is low enough that the potential can be approximated as just the potential coming from that $x_i$ . This means that, for a given set of $x_i$ , there will always be $\epsilon > 0$ s.t. $f(x,y) = -\frac{1}{\epsilon + ||x-y||}$ works as intended. However, a third constraint is that we would like $f$ to not depend on the $x_i$ . Are there such functions? A famous non-example would be a standard multivariate Gaussian. If you sum three of those on the vertices of an equilateral triangle, counter-intuitively, the center is a stable equilibrium. In general, I suspect that $f$ cannot be smooth at $f(x,x)$ . Otherwise, $f$ would be locally quadratic and if three points in $x_i$ are sufficiently close to each other, we would expect their barycenter (w.r.t the norm induced by the Hessian of $f$ ) to be a minimum. However, this does not rule out solutions where $f$ is non-differentiable around $f(x,x)$ .","Can we find a continuous, almost-everywhere smooth, potential function with no poles, such that the set of local minima for the sum of these potentials around points are exactly those points? We are given a set of points in . We consider the additive potential function as a sum of potentials , where is continuous and almost everywhere smooth. We would like ’s minima to be exactly the . That is, any point where the gradient of vanishes is either one of the , a saddle point, or a maximum. By way of example, we can construct such a function with the gravitational potential. If we let I believe the property is respected. A point mass will always end up “falling” towards one of the attractors, or at least I believe that’s implied by Earnshaw’s theorem. I’m not 100% sure it works for all norms, but the Euclidian distance will do. For this potential, the aren’t just local minima, they are poles. We would like to find a better-behaved function which does not exhibit poles. One way to “cheat” is to cut-off the poles. Once we get close enough to one of the , the influence of the other points is low enough that the potential can be approximated as just the potential coming from that . This means that, for a given set of , there will always be s.t. works as intended. However, a third constraint is that we would like to not depend on the . Are there such functions? A famous non-example would be a standard multivariate Gaussian. If you sum three of those on the vertices of an equilateral triangle, counter-intuitively, the center is a stable equilibrium. In general, I suspect that cannot be smooth at . Otherwise, would be locally quadratic and if three points in are sufficiently close to each other, we would expect their barycenter (w.r.t the norm induced by the Hessian of ) to be a minimum. However, this does not rule out solutions where is non-differentiable around .","n n n (x_0, \ldots, x_{n-1}) \mathbb{R}^d g(x) = \sum_{i=1}^n f(x, x_i) f : \mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R} f g x_i x g x_i \forall x, \nabla g(x) = \vec{0} \implies x \in \{x_1, \ldots, x_n\}~\vee~ \exists v, \lambda < 0, H(g)(x) v = \lambda v f(x,y) = -\frac{1}{||x-y||}, x_i x_i x_i x_i \epsilon > 0 f(x,y) = -\frac{1}{\epsilon + ||x-y||} f x_i f f(x,x) f x_i f f f(x,x)","['multivariable-calculus', 'physics', 'maxima-minima', 'gradient-descent', 'hessian-matrix']"
24,Why is error term in the definition of derivative for $\mathbb{R}^n \to \mathbb{R}^m$ of order $o(h)$?,Why is error term in the definition of derivative for  of order ?,\mathbb{R}^n \to \mathbb{R}^m o(h),"The following intuition is usually supplied for a definition of derivative. We would like to approximate function $f(x)$ near some point $x_0$ with a linear map and we would like to show that as we get closer to $x_0$ , this approximation becomes good (for certain notion of good). The following equation is true in general (by definition of error term $\varepsilon(h)$ ). $$ f(x_0+h) - f(x_0) = L(h) + \varepsilon(h) $$ Now, as $L(h)$ and $\varepsilon$ are vectors of $\mathbb{R}^m$ , to compare them it is reasonable to use magnitude. So, ""intuitively"", we would like magnitude of relative error to become small as we approach $x_0$ . In other words, we would like: $$ \lim \limits_{h \to 0} R(h) = \lim \limits_{h \to 0}  \left( \frac{|\varepsilon(h)|}{|L(h)|} \right) = 0. $$ Question 1 : This expression does not make sense if $L(h) = 0$ which could certainly be a derivative for some function. How can in that case we say that function $0$ approximates function close to $x_0$ if relative error is undefined? Do we use some different criterion? Do we ignore this case? Now, if we assume limit is defined and exists, we can use that $|L(h)| \leq M|h|$ as it is a linear map $\mathbb{R}^n \to \mathbb{R}^m$ . Then, we get the following. $$ 0 = \lim \limits_{h \to 0} R(h)  \geq \frac{1}{M} \lim \limits_{h \to 0} \left( \frac{|\varepsilon(h)|}{|h|} \right)$$ From here it seems to be if we assume (?) limit exists and assume it is zero by out intuition on what good approximation is, we can show that it necessarily means that error is of order $o(h)$ . Question 2 : Is this reasoning valid? Question 3 : The other direction -- if error term is $o(h)$ -- does not seem to imply that relative error tends to $0$ . So, could it be that derivative is defined, but its relative error compared to non linear term does not vanish? How to interpret this with mindset of derivative being the best linear approximation? Appreciate your thoughts and comments.","The following intuition is usually supplied for a definition of derivative. We would like to approximate function near some point with a linear map and we would like to show that as we get closer to , this approximation becomes good (for certain notion of good). The following equation is true in general (by definition of error term ). Now, as and are vectors of , to compare them it is reasonable to use magnitude. So, ""intuitively"", we would like magnitude of relative error to become small as we approach . In other words, we would like: Question 1 : This expression does not make sense if which could certainly be a derivative for some function. How can in that case we say that function approximates function close to if relative error is undefined? Do we use some different criterion? Do we ignore this case? Now, if we assume limit is defined and exists, we can use that as it is a linear map . Then, we get the following. From here it seems to be if we assume (?) limit exists and assume it is zero by out intuition on what good approximation is, we can show that it necessarily means that error is of order . Question 2 : Is this reasoning valid? Question 3 : The other direction -- if error term is -- does not seem to imply that relative error tends to . So, could it be that derivative is defined, but its relative error compared to non linear term does not vanish? How to interpret this with mindset of derivative being the best linear approximation? Appreciate your thoughts and comments.",f(x) x_0 x_0 \varepsilon(h)  f(x_0+h) - f(x_0) = L(h) + \varepsilon(h)  L(h) \varepsilon \mathbb{R}^m x_0  \lim \limits_{h \to 0} R(h) = \lim \limits_{h \to 0}  \left( \frac{|\varepsilon(h)|}{|L(h)|} \right) = 0.  L(h) = 0 0 x_0 |L(h)| \leq M|h| \mathbb{R}^n \to \mathbb{R}^m  0 = \lim \limits_{h \to 0} R(h)  \geq \frac{1}{M} \lim \limits_{h \to 0} \left( \frac{|\varepsilon(h)|}{|h|} \right) o(h) o(h) 0,"['multivariable-calculus', 'derivatives', 'definition']"
25,Proof of interior gradient estimate for Laplace's equation,Proof of interior gradient estimate for Laplace's equation,,"I have a question about the proof of the estimate $$ |\nabla u(x_0)| \leq \frac{n}{R} \max_{\bar{B}_R(x_0)} |u| $$ where $u$ is assumed to be harmonic. Since $u_{x_i}$ is harmonic, by the mean value property and integration by parts, $$ u_{x_i}(x_0) = \frac{r}{\omega_n R^n}\int_{B_R(x_0)} u_{x_i}(y) dy = \frac{n}{\omega_n R^n}\int_{\partial B_R(x_0)} u(y) \nu_i dS_y. $$ Taking the absolute value, we obtain $$ |u_{x_i}(x_0)| \leq \frac{n}{\omega_n R^n} \int_{\partial B_R(x_0)} |u(y)| dS_y \leq \frac{n}{R}\max_{\bar{B}_R(x_0)} |u|. $$ I understand the preceding steps. What I don't understand is how this obviously proves the desired result. This is my attempt at obtaining the desired result: \begin{align*} |\nabla u(x_0)|^2 &= u_{x_1}^2(x_0) + \cdots + u_{x_n}^2(x_0) \\ &\leq \underbrace{\frac{n^2}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2 + \cdots + \frac{n^2}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2}_{\text{$n$ times}}\\ &=\frac{n^3}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2. \end{align*} Taking the square root, $$ |\nabla u(x_0)| \leq \left(\frac{n^3}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2\right)^{1/2} = \frac{n^{3/2}}{R}\max_{\bar{B}_R(x_0)}. $$ I'm not sure where my logic is wrong and I am aware this must be something simple...","I have a question about the proof of the estimate where is assumed to be harmonic. Since is harmonic, by the mean value property and integration by parts, Taking the absolute value, we obtain I understand the preceding steps. What I don't understand is how this obviously proves the desired result. This is my attempt at obtaining the desired result: Taking the square root, I'm not sure where my logic is wrong and I am aware this must be something simple...","
|\nabla u(x_0)| \leq \frac{n}{R} \max_{\bar{B}_R(x_0)} |u|
 u u_{x_i} 
u_{x_i}(x_0) = \frac{r}{\omega_n R^n}\int_{B_R(x_0)} u_{x_i}(y) dy = \frac{n}{\omega_n R^n}\int_{\partial B_R(x_0)} u(y) \nu_i dS_y.
 
|u_{x_i}(x_0)| \leq \frac{n}{\omega_n R^n} \int_{\partial B_R(x_0)} |u(y)| dS_y \leq \frac{n}{R}\max_{\bar{B}_R(x_0)} |u|.
 \begin{align*}
|\nabla u(x_0)|^2 &= u_{x_1}^2(x_0) + \cdots + u_{x_n}^2(x_0) \\
&\leq \underbrace{\frac{n^2}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2 + \cdots + \frac{n^2}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2}_{\text{n times}}\\
&=\frac{n^3}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2.
\end{align*} 
|\nabla u(x_0)| \leq \left(\frac{n^3}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2\right)^{1/2} = \frac{n^{3/2}}{R}\max_{\bar{B}_R(x_0)}.
","['multivariable-calculus', 'partial-differential-equations', 'harmonic-functions']"
26,Hint to an Inequality,Hint to an Inequality,,"I have the following problem in my book- Prove that for all non-negative real numbers $x, y, z$ - $$ 6(x+y-z)(x^2+y^2+z^2)+27xyz \le 10(x^2+y^2+z^2)^{3\over 2} $$ And the solution : $$\color{red}{10(x^2+y^2+z^2)^{3\over 2}-6(x+y-z)(x^2+y^2+z^2)}=(x^2+y^2+z^2)(10\sqrt{x^2+y^2+z^2}-6(x+y-z))$$ $$ =(x^2+y^2+z^2)\left(\color{red}{{10\over 3}\sqrt{(x^2+y^2+z^2)(2^2+2^2+1^2)}}-6(x+y-z)\right) $$ By C-S, $$ \ge(x^2+y^2+z^2)\left({10(2x+2y+z)\over 3}-6(x+y-z)\right)={(x^2+y^2+z^2)(2x+2y+28z)\over 3}$$ By AM-GM, $ x^2+y^2+z^2 \ge 9\sqrt[9]{x^8y^8z^2\over 4^8} $ and $ 2x+2y+28z\ge 9\sqrt[9]{4^8xyz^7} $ and multiplication accompanied by division with 3 ends the proof. The validity of the proof is undoubted, but I want to know that how the author comes up with such an idea ? What has the question reflected that hints the major steps to the proof in red? Note :- I want just the idea that leads to the proof, not a solution.","I have the following problem in my book- Prove that for all non-negative real numbers - And the solution : By C-S, By AM-GM, and and multiplication accompanied by division with 3 ends the proof. The validity of the proof is undoubted, but I want to know that how the author comes up with such an idea ? What has the question reflected that hints the major steps to the proof in red? Note :- I want just the idea that leads to the proof, not a solution.","x, y, z  6(x+y-z)(x^2+y^2+z^2)+27xyz \le 10(x^2+y^2+z^2)^{3\over 2}  \color{red}{10(x^2+y^2+z^2)^{3\over 2}-6(x+y-z)(x^2+y^2+z^2)}=(x^2+y^2+z^2)(10\sqrt{x^2+y^2+z^2}-6(x+y-z))  =(x^2+y^2+z^2)\left(\color{red}{{10\over 3}\sqrt{(x^2+y^2+z^2)(2^2+2^2+1^2)}}-6(x+y-z)\right)   \ge(x^2+y^2+z^2)\left({10(2x+2y+z)\over 3}-6(x+y-z)\right)={(x^2+y^2+z^2)(2x+2y+28z)\over 3}  x^2+y^2+z^2 \ge 9\sqrt[9]{x^8y^8z^2\over 4^8}   2x+2y+28z\ge 9\sqrt[9]{4^8xyz^7} ","['multivariable-calculus', 'inequality', 'proof-explanation', 'cauchy-schwarz-inequality', 'a.m.-g.m.-inequality']"
27,An area preserving diffeomorphism between a disk and an ellipse,An area preserving diffeomorphism between a disk and an ellipse,,"This is a self-answered question. I post it here since (embarrassingly) it took me some time to realize that the solution is obvious. Let $D \subseteq \mathbb R^2$ be the closed unit disk and let $E$ be an ellipse with the same area, i.e.  with minor and major axes of lengths $a<b$ and $ab=1$ . $$ E=\{(x,y) \, | \, \frac{x^2}{a^2} + \frac{y^2}{b^2} \le 1 \} $$ Question: Can we construct explicitly an area preserving diffeomorphism $f:D \to E$ ? (i.e. $Jf=1$ identically on $D$ ).","This is a self-answered question. I post it here since (embarrassingly) it took me some time to realize that the solution is obvious. Let be the closed unit disk and let be an ellipse with the same area, i.e.  with minor and major axes of lengths and . Question: Can we construct explicitly an area preserving diffeomorphism ? (i.e. identically on ).","D \subseteq \mathbb R^2 E a<b ab=1 
E=\{(x,y) \, | \, \frac{x^2}{a^2} + \frac{y^2}{b^2} \le 1 \}
 f:D \to E Jf=1 D","['multivariable-calculus', 'differential-geometry', 'euclidean-geometry', 'riemannian-geometry', 'area']"
28,Understanding the multidimensional chain rule,Understanding the multidimensional chain rule,,"I have some trouble in understanding the multidimensional chain rule. For differentiable functions $f,g$ , defined by $f: U \to V$ , $g:V\to\mathbb K^n$ where $U \subseteq \mathbb R^d$ , $V \subseteq \mathbb R^\nu$ are open, $$\Big(\mathrm D(g \circ f)\Big)(x)=\Big(\mathrm D g(f(x))\Big)\cdot\Big(\mathrm Df(x)\Big).$$ I have rather accepted this identity than understood it. I already saw some examples and understood the fact that I did not understand something completely. By differentiating a multidimensional function, one apparentely differentiates the components  separately and somehow adds everything? Unfortunately I don't know exactly how... any explanation would be highly appreciated.","I have some trouble in understanding the multidimensional chain rule. For differentiable functions , defined by , where , are open, I have rather accepted this identity than understood it. I already saw some examples and understood the fact that I did not understand something completely. By differentiating a multidimensional function, one apparentely differentiates the components  separately and somehow adds everything? Unfortunately I don't know exactly how... any explanation would be highly appreciated.","f,g f: U \to V g:V\to\mathbb K^n U \subseteq \mathbb R^d V \subseteq \mathbb R^\nu \Big(\mathrm D(g \circ f)\Big)(x)=\Big(\mathrm D g(f(x))\Big)\cdot\Big(\mathrm Df(x)\Big).","['multivariable-calculus', 'chain-rule']"
29,Multivariate Taylor series with Hessian evaluated at a linear combination of $x$ and $\Delta x$,Multivariate Taylor series with Hessian evaluated at a linear combination of  and,x \Delta x,"I found the following theorem, but I don't understand it and was unable to prove it. Is it true? Is there a proof for it? Theorem: Let $f : R^d → R$ be such that $f$ is twice-differentiable and has continuous derivatives in an open ball $B$ around the point $x ∈ R^d$ . Then for any small enough $∆x ∈ R^d$ such that $x + ∆x$ is also contained in the ball $B$ , we have the following: $$ f(x + \Delta x) = f(x) + \Delta x^T\nabla f|_x + \frac{1}{2}(\Delta x)^T (\nabla^2f|_w)(\Delta x) $$ Where $(\nabla^2f|_w)$ is the Hessian of $f$ evaluated at a point $w ∈ R^d$ that lies on the line connecting $x$ and $x + ∆x$ I understand that this is a second-order Taylor expansion of $f$ about $x$ , and I understand why it is in this form. But, I don't get is why the Hessian can be evaluated at the point $w$ rather than at $x$ . If it is a Taylor expansion about $x$ , shouldn't all derivatives be evaluated at $x$ ? Why is this expansion valid? For reference, this is where I found the theorem: https://www.cs.princeton.edu/courses/archive/fall18/cos597G/lecnotes/lecture3.pdf On page 2.","I found the following theorem, but I don't understand it and was unable to prove it. Is it true? Is there a proof for it? Theorem: Let be such that is twice-differentiable and has continuous derivatives in an open ball around the point . Then for any small enough such that is also contained in the ball , we have the following: Where is the Hessian of evaluated at a point that lies on the line connecting and I understand that this is a second-order Taylor expansion of about , and I understand why it is in this form. But, I don't get is why the Hessian can be evaluated at the point rather than at . If it is a Taylor expansion about , shouldn't all derivatives be evaluated at ? Why is this expansion valid? For reference, this is where I found the theorem: https://www.cs.princeton.edu/courses/archive/fall18/cos597G/lecnotes/lecture3.pdf On page 2.","f : R^d → R f B x ∈ R^d ∆x ∈ R^d x + ∆x B 
f(x + \Delta x) = f(x) + \Delta x^T\nabla f|_x + \frac{1}{2}(\Delta x)^T (\nabla^2f|_w)(\Delta x)
 (\nabla^2f|_w) f w ∈ R^d x x + ∆x f x w x x x","['multivariable-calculus', 'taylor-expansion']"
30,What's the gradient of a vector field?,What's the gradient of a vector field?,,"Imagine I have the following function $$ \vec{f}(\vec{x}) = x \vec{x}, x = | \vec{x} |, \vec{x} \in R^3 $$ That is, the function is essentially a quadratic function, but contains a vector direction as well. Intuitively from single variable calculus I would expect the gradient $ \nabla \vec{f} = (\partial \vec{f}/ \partial x_1,\partial \vec{f}/ \partial x_2,\partial \vec{f}/ \partial x_3) $ to be proportional to $2x$ , however I also would expect it to be a 3x3 matrix. My most naive attempt would be to do $$ \vec{f} = x_1^2 \vec{e}_1 + x_2^2 \vec{e}_2 + x_3^2 \vec{e}_3 $$ and say that $$ \nabla \vec{f} = 2 x_1 \vec{e}_1 + 2 x_2 \vec{e}_2 + 2 x_3 \vec{e}_3 $$ But it would mean that every gradient w.r.t. a vector would always be a diagonal matrix, which seems wrong to me. What I really want to create is the Jacobian $ \partial \vec{f}_i / \partial x_j $ but I think I get a little bit confused about what I do with the base vectors $ \vec{e_i} $ during the partial derivative.","Imagine I have the following function That is, the function is essentially a quadratic function, but contains a vector direction as well. Intuitively from single variable calculus I would expect the gradient to be proportional to , however I also would expect it to be a 3x3 matrix. My most naive attempt would be to do and say that But it would mean that every gradient w.r.t. a vector would always be a diagonal matrix, which seems wrong to me. What I really want to create is the Jacobian but I think I get a little bit confused about what I do with the base vectors during the partial derivative."," \vec{f}(\vec{x}) = x \vec{x}, x = | \vec{x} |, \vec{x} \in R^3   \nabla \vec{f} = (\partial \vec{f}/ \partial x_1,\partial \vec{f}/ \partial x_2,\partial \vec{f}/ \partial x_3)  2x  \vec{f} = x_1^2 \vec{e}_1 + x_2^2 \vec{e}_2 + x_3^2 \vec{e}_3   \nabla \vec{f} = 2 x_1 \vec{e}_1 + 2 x_2 \vec{e}_2 + 2 x_3 \vec{e}_3   \partial \vec{f}_i / \partial x_j   \vec{e_i} ","['multivariable-calculus', 'partial-derivative', 'jacobian']"
31,How to prove that supremum of strictly convex function is infinity?,How to prove that supremum of strictly convex function is infinity?,,"Suppose there is a strictly convex continuous function $f$ : $R^n$ $\rightarrow$ $R$ . Is the supremum of $f$ always infinity? How can we prove it? I am trying to come up with proof. If $x$ and $y$ are two points in $R^n$ , strictly convex implies $f(\alpha x_1 + (1-\alpha) x_2) $ < $\alpha f(x_1) + (1-\alpha)f(x_2)$ . Suppose $f$ is bounded. Case 1: The bound is attained at a point, say $x_0$ . Then for some $\alpha$ , some $x_1$ and $x_2$ s.t. $ (\alpha x_1 + (1-\alpha) x_2) = x_0$ : $f(x_0)$ < $\alpha f(x_1) + (1-\alpha)f(x_2)$ Therefore a contradiction. Case 2: The bound is not attained. Since the function is strictly convex, we know $f(x)$ approaches this bound as $x$ approaches $ \infty $ I don't know how to proceed after this step. Where can I find a contradiction in this case?","Suppose there is a strictly convex continuous function : . Is the supremum of always infinity? How can we prove it? I am trying to come up with proof. If and are two points in , strictly convex implies < . Suppose is bounded. Case 1: The bound is attained at a point, say . Then for some , some and s.t. : < Therefore a contradiction. Case 2: The bound is not attained. Since the function is strictly convex, we know approaches this bound as approaches I don't know how to proceed after this step. Where can I find a contradiction in this case?",f R^n \rightarrow R f x y R^n f(\alpha x_1 + (1-\alpha) x_2)  \alpha f(x_1) + (1-\alpha)f(x_2) f x_0 \alpha x_1 x_2  (\alpha x_1 + (1-\alpha) x_2) = x_0 f(x_0) \alpha f(x_1) + (1-\alpha)f(x_2) f(x) x  \infty ,"['multivariable-calculus', 'proof-explanation', 'convex-analysis', 'supremum-and-infimum']"
32,Compute the improper integral $\int_A \frac{dx dy dz}{(1+x^2z^2)(1+y^2z^2)}$ over an infinite cuboid,Compute the improper integral  over an infinite cuboid,\int_A \frac{dx dy dz}{(1+x^2z^2)(1+y^2z^2)},"Let $A := \{ (x, y, z) \in \mathbb{R}^{3} : 0 < x < 1, 0 < y < 1, 0 < z \} $ . Show that the function $f: A \to \mathbb{R}$ defined by $$f(x, y, z) := \frac{1}{(1+x^2z^2)(1+y^2z^2)}$$ is integrable in $A$ and calculate the integral $\int_{A} f$ . My attempt: Showing the integral exists is easy using the Comparison test for improper integrals. Now, for computing the integral first I've used Fubini's theorem, so I got that $$\int_{A} f = \int_{0}^{\infty} \left( \frac{\arctan(z)}{z} \right)^{2} \: dz \,,$$ which is not an elementary integral. Then I thought about using that $f$ is an even function, and I got that $$\int_{A} f = \frac{1}{8} \int_{D} f\,,$$ where $D:= \{ (x, y, z): -1 < x <1, -1 < y <1 \}$ and tried to use a change of variables to cylindrical coordinates...what doesn't make things easier. I also thought about non-linear changes of variables such as $u := xz$ , $v := yz$ , $z = z$ ... But I am no longer able to solve the integral.","Let . Show that the function defined by is integrable in and calculate the integral . My attempt: Showing the integral exists is easy using the Comparison test for improper integrals. Now, for computing the integral first I've used Fubini's theorem, so I got that which is not an elementary integral. Then I thought about using that is an even function, and I got that where and tried to use a change of variables to cylindrical coordinates...what doesn't make things easier. I also thought about non-linear changes of variables such as , , ... But I am no longer able to solve the integral.","A := \{ (x, y, z) \in \mathbb{R}^{3} : 0 < x < 1, 0 < y < 1, 0 < z \}  f: A \to \mathbb{R} f(x, y, z) := \frac{1}{(1+x^2z^2)(1+y^2z^2)} A \int_{A} f \int_{A} f = \int_{0}^{\infty} \left( \frac{\arctan(z)}{z} \right)^{2} \: dz \,, f \int_{A} f = \frac{1}{8} \int_{D} f\,, D:= \{ (x, y, z): -1 < x <1, -1 < y <1 \} u := xz v := yz z = z","['real-analysis', 'integration', 'multivariable-calculus', 'improper-integrals', 'multiple-integral']"
33,"Modern and classical definitions of continuity of a function at $x_0$. (James R. Munkres ""Analysis on Manifolds"")","Modern and classical definitions of continuity of a function at . (James R. Munkres ""Analysis on Manifolds"")",x_0,"I am reading ""Analysis on Manifolds"" by James R. Munkres. Munkres wrote two different definitions of continuity of a function at $x_0$ : Modern(?) definition: Let $X$ and $Y$ be metric spaces, with metrics $d_X$ and $d_Y$ , respectively. We say that a function $f : X \to Y$ is continuous at the point $x_0$ of $X$ if for each open set $V$ of $Y$ containing $f(x_0)$ , there is an open set $U$ containing $x_0$ such that $f(U) \subset V$ . Classical definition: Continuity may be formulated in a way that involves the metrics specifically. The function $f$ is continuous at $x_0$ if and only if the following holds: For each $\epsilon > 0$ , there is a corresponding $\delta > 0$ such that $$d_Y(f(x), f(x_0)) < \epsilon \text{ whenever } d_X(x, x_0) < \delta.$$ This is the classical "" $\epsilon$ - $\delta$ formulation of continuity."" After these definitions, Munkres wrote the following theorem without a proof: Theorem 3.6(b): Let $f, g : X \to \mathbb{R}$ be continuous at $x_0$ . Then $f + g$ and $f-g$ and $f \cdot g$ are continuous at $x_0$ ; and $f/g$ is continuous at $x_0$ if $g(x_0) \ne 0$ . My proof for $f + g$ is the following: Let $V_{f+g}$ be any open set of $\mathbb{R}$ containing $(f+g)(x_0)$ . Then, there exists $\epsilon > 0$ such that if $|y - (f(x_0) + g(x_0))| < \epsilon$ , then $y \in V_{f+g}$ . Let $V_f := \{y \in \mathbb{R} | |y - f(x_0)| < \frac{\epsilon}{2}\}$ and $V_g := \{y \in \mathbb{R} | |y - g(x_0)| < \frac{\epsilon}{2}\}$ . Then, $V_f$ and $V_g$ are open sets of $\mathbb{R}$ . So, there exist open sets $U_f$ and $U_g$ of $X$ containing $x_0$ such that $f(U_f) \subset V_f$ and $g(U_g) \subset V_g$ . Let $U_{f+g} := U_f \cap U_g$ . Then if $x \in U_{f+g}$ , then $f(x) \in f(U_{f+g}) \subset f(U_f) \subset V_f$ and $g(x) \in g(U_{f+g}) \subset g(U_g) \subset V_g$ . So, $|f(x) - f(x_0)| < \frac{\epsilon}{2}$ and $|g(x) - g(x_0)| < \frac{\epsilon}{2}$ . So, $|f(x) + g(x) - (f(x_0) + g(x_0))| \leq |f(x) - f(x_0)| + |g(x) - g(x_0)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$ . So, if $x \in U_{f+g}$ , then $(f + g)(x) \in V_{f+g}$ . So, $(f+g)(U_{f+g}) \subset V_{f+g}$ . Munkres said: ""Continuity may be formulated in a way that involves the metrics specifically."" and ""This is the classical ' $\epsilon$ - $\delta$ formulation of continuity.'"" Since I don't like the word ""classical"", I didn't want to use metrics specifically in the above proof. But I had to use metrics specifically. So my proof is ""classical"". Please give me a ""modern"" proof if it exists.","I am reading ""Analysis on Manifolds"" by James R. Munkres. Munkres wrote two different definitions of continuity of a function at : Modern(?) definition: Let and be metric spaces, with metrics and , respectively. We say that a function is continuous at the point of if for each open set of containing , there is an open set containing such that . Classical definition: Continuity may be formulated in a way that involves the metrics specifically. The function is continuous at if and only if the following holds: For each , there is a corresponding such that This is the classical "" - formulation of continuity."" After these definitions, Munkres wrote the following theorem without a proof: Theorem 3.6(b): Let be continuous at . Then and and are continuous at ; and is continuous at if . My proof for is the following: Let be any open set of containing . Then, there exists such that if , then . Let and . Then, and are open sets of . So, there exist open sets and of containing such that and . Let . Then if , then and . So, and . So, . So, if , then . So, . Munkres said: ""Continuity may be formulated in a way that involves the metrics specifically."" and ""This is the classical ' - formulation of continuity.'"" Since I don't like the word ""classical"", I didn't want to use metrics specifically in the above proof. But I had to use metrics specifically. So my proof is ""classical"". Please give me a ""modern"" proof if it exists.","x_0 X Y d_X d_Y f : X \to Y x_0 X V Y f(x_0) U x_0 f(U) \subset V f x_0 \epsilon > 0 \delta > 0 d_Y(f(x), f(x_0)) < \epsilon \text{ whenever } d_X(x, x_0) < \delta. \epsilon \delta f, g : X \to \mathbb{R} x_0 f + g f-g f \cdot g x_0 f/g x_0 g(x_0) \ne 0 f + g V_{f+g} \mathbb{R} (f+g)(x_0) \epsilon > 0 |y - (f(x_0) + g(x_0))| < \epsilon y \in V_{f+g} V_f := \{y \in \mathbb{R} | |y - f(x_0)| < \frac{\epsilon}{2}\} V_g := \{y \in \mathbb{R} | |y - g(x_0)| < \frac{\epsilon}{2}\} V_f V_g \mathbb{R} U_f U_g X x_0 f(U_f) \subset V_f g(U_g) \subset V_g U_{f+g} := U_f \cap U_g x \in U_{f+g} f(x) \in f(U_{f+g}) \subset f(U_f) \subset V_f g(x) \in g(U_{f+g}) \subset g(U_g) \subset V_g |f(x) - f(x_0)| < \frac{\epsilon}{2} |g(x) - g(x_0)| < \frac{\epsilon}{2} |f(x) + g(x) - (f(x_0) + g(x_0))| \leq |f(x) - f(x_0)| + |g(x) - g(x_0)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon x \in U_{f+g} (f + g)(x) \in V_{f+g} (f+g)(U_{f+g}) \subset V_{f+g} \epsilon \delta","['general-topology', 'multivariable-calculus', 'continuity']"
34,Gradient is perpendicular to level set and implicit function theorem,Gradient is perpendicular to level set and implicit function theorem,,"My lecture notes on the gradient state the following: For $f : U \to \mathbb{R}$ differentiable consider the level set $N_w = \{v \in U : f(v)=w\}$ where $U \in \mathbb{R}^n$ . Suppose that $c : I \to N_w \in U$ is a differentiable curve. Then $f \circ c = w$ and so $0=\frac{d}{dt}(f \circ c)=\langle grad f(c(t)),c'(t)\rangle \iff grad (c(t)) \perp c'(t)$ . Since this holds for any differentiable curve running through $N_w$ , we can say that the gradient vector of $f$ is perpendicular to the level sets $N_w$ . The proof is clear to me, but this brings up the question whether such a curve $c(t)$ exists. I've read a couple of other posts about this and it seems that the existence of such a function is guaranteed by the implicit function theorem. Implicit function theorem (IFT): Let $\Omega \subset \mathbb{R}^n \times \mathbb{R}^k = \mathbb{R}^{n+k}$ be open and $f : \Omega \to \mathbb{R}^k, (x,y) \to f(x,y)$ be continuously differentiable. Moreover, let $(a,b) \in \Omega$ be a point with $f(a,b) = 0$ , such that \begin{equation*} J^Y_f =  \begin{pmatrix} \frac{\partial f_1}{\partial y_1} & \cdots & \frac{\partial f_1}{\partial y_k} \\ \vdots  & \ddots  & \vdots \\ \frac{\partial f_k}{\partial y_1} & \cdots & \frac{\partial f_k}{\partial y_k}  \end{pmatrix} \end{equation*} satisfies the following equivalent conditions at $(a,b)$ : $(*) J^Y_f(a,b)$ is invertible $\iff$ rank $J^Y_f(a,b) = k \iff det J^Y_f(a,b) \neq 0$ . Then there are neighbourhoods $X \subset \mathbb{R}^n$ of $a$ and $Y \subset \mathbb{R}^k$ of $b$ with $X \times Y \subset \Omega$ as well as a continuously differentiable mapping $g : X \to Y$ with $f(x,y)=0$ for $(x,y) \in X \times Y \iff y=g(x)$ for $x \in X$ . Now the theorem gives a way to express some of the variables as a function of the others, but for the statement about the gradient we would need a curve $c(t)$ parameterized by $t$ where $c(t)$ is a vector in $\mathbb{R}^n$ . How can we parameterize the graph $(x,g(x))$ by $t$ ? Thanks a lot!","My lecture notes on the gradient state the following: For differentiable consider the level set where . Suppose that is a differentiable curve. Then and so . Since this holds for any differentiable curve running through , we can say that the gradient vector of is perpendicular to the level sets . The proof is clear to me, but this brings up the question whether such a curve exists. I've read a couple of other posts about this and it seems that the existence of such a function is guaranteed by the implicit function theorem. Implicit function theorem (IFT): Let be open and be continuously differentiable. Moreover, let be a point with , such that satisfies the following equivalent conditions at : is invertible rank . Then there are neighbourhoods of and of with as well as a continuously differentiable mapping with for for . Now the theorem gives a way to express some of the variables as a function of the others, but for the statement about the gradient we would need a curve parameterized by where is a vector in . How can we parameterize the graph by ? Thanks a lot!","f : U \to \mathbb{R} N_w = \{v \in U : f(v)=w\} U \in \mathbb{R}^n c : I \to N_w \in U f
\circ c = w 0=\frac{d}{dt}(f \circ c)=\langle grad f(c(t)),c'(t)\rangle \iff grad
(c(t)) \perp c'(t) N_w f N_w c(t) \Omega \subset \mathbb{R}^n \times \mathbb{R}^k =
\mathbb{R}^{n+k} f : \Omega \to \mathbb{R}^k, (x,y) \to
f(x,y) (a,b) \in
\Omega f(a,b) = 0 \begin{equation*} J^Y_f =  \begin{pmatrix} \frac{\partial
f_1}{\partial y_1} & \cdots & \frac{\partial f_1}{\partial y_k} \\
\vdots  & \ddots  & \vdots \\ \frac{\partial f_k}{\partial y_1} &
\cdots & \frac{\partial f_k}{\partial y_k}  \end{pmatrix}
\end{equation*} (a,b) (*) J^Y_f(a,b) \iff J^Y_f(a,b) = k \iff det
J^Y_f(a,b) \neq 0 X \subset \mathbb{R}^n a Y
\subset \mathbb{R}^k b X \times Y \subset \Omega g : X \to Y f(x,y)=0 (x,y) \in X \times Y \iff y=g(x) x \in X c(t) t c(t) \mathbb{R}^n (x,g(x)) t","['multivariable-calculus', 'parametrization', 'implicit-function-theorem']"
35,Continuity of this Piecewise function $f:\mathbb{R}^2\to \mathbb{R}$,Continuity of this Piecewise function,f:\mathbb{R}^2\to \mathbb{R},"I have shown that $f:\mathbb{R}^2\to\mathbb{R}$ given by $f(0,0) = 0$ and $\displaystyle f(x,y)=\frac{x|y|}{\sqrt{x^2+y^2}}$ if $(x,y)\ne (0,0)$ isn't differentiable at $(0,0)$ , now I'm trying to show whether it is continuous or not. My attempt: I must show that $\displaystyle\lim_{(x,y)\to(0,0)}\frac{x|y|}{\sqrt{x^2+y^2}}=f(0,0) = 0$ . But $x^2+y^2-2|xy| = (|x|-|y|)^{2}\ge 0$ so $\displaystyle|xy|\le \frac{x^2+y^2}{2}$ . So $\displaystyle\left|\frac{x|y|}{\sqrt{x^2+y^2}}\right| =\frac{|xy|}{\sqrt{x^2+y^2}}\le \frac{x^2+y^2}{\sqrt{x^2+y^2}} = (x^2+y^2)^{1/2}$ and once $(x^2+y^2)^{1/2}\to0$ as $(x,y)\to(0,0),$ we would have it. Is this correct?","I have shown that given by and if isn't differentiable at , now I'm trying to show whether it is continuous or not. My attempt: I must show that . But so . So and once as we would have it. Is this correct?","f:\mathbb{R}^2\to\mathbb{R} f(0,0) = 0 \displaystyle f(x,y)=\frac{x|y|}{\sqrt{x^2+y^2}} (x,y)\ne (0,0) (0,0) \displaystyle\lim_{(x,y)\to(0,0)}\frac{x|y|}{\sqrt{x^2+y^2}}=f(0,0) = 0 x^2+y^2-2|xy| = (|x|-|y|)^{2}\ge 0 \displaystyle|xy|\le \frac{x^2+y^2}{2} \displaystyle\left|\frac{x|y|}{\sqrt{x^2+y^2}}\right| =\frac{|xy|}{\sqrt{x^2+y^2}}\le \frac{x^2+y^2}{\sqrt{x^2+y^2}} = (x^2+y^2)^{1/2} (x^2+y^2)^{1/2}\to0 (x,y)\to(0,0),","['real-analysis', 'limits', 'multivariable-calculus', 'continuity', 'solution-verification']"
36,Doubt in buiding a bump function in a manifold,Doubt in buiding a bump function in a manifold,,"This definition of a bump function is given in ""Introduduction to Manifolds"" by Loring W. Tu : Given a point $ p $ in a manifold $ M^n$ , a bump function in $p$ supported in $V$ is any non-negative function $ \rho: M \rightarrow \mathbb{R} $ which is identically $ \mathbf{1} $ in some neighborhood of $ p $ with $ supp (\rho) \subset V $ . I understand the process of creating a $C^\infty$ bump function in $R$ and $R^n$ , but when I move on to manifolds, the following happens: Take $V$ a neighborhood of $p$ and $(\varphi,U)$ a chart over $p$ such that $V \subset U$ . We have a $C^\infty$ bump function $\rho:\mathbb{R}^n \rightarrow \mathbb{R}$ in $\varphi(p)$ which is identically $\mathbf{1}$ in the closed ball $B[\varphi(p),a]$ supported in $B[\varphi(p),b]$ with $a<b<d(\varphi(p),\partial \varphi(V))$ . And now, the composition $\rho \circ \varphi:U\rightarrow \mathbb{R}$ have domain $U$ , not $M$ as I wish. I'm forgetting to do something here? Like, consider the null extension of $\varphi$ over $M$ ... But, if this is the case, what guarantees me that the composition $\rho \circ \varphi:M\rightarrow \mathbb{R}$ will be differentiable? I know that $\varphi:U\rightarrow \varphi(U)$ a diffeomorphism, but I can't solve the domain issue. Thanks!","This definition of a bump function is given in ""Introduduction to Manifolds"" by Loring W. Tu : Given a point in a manifold , a bump function in supported in is any non-negative function which is identically in some neighborhood of with . I understand the process of creating a bump function in and , but when I move on to manifolds, the following happens: Take a neighborhood of and a chart over such that . We have a bump function in which is identically in the closed ball supported in with . And now, the composition have domain , not as I wish. I'm forgetting to do something here? Like, consider the null extension of over ... But, if this is the case, what guarantees me that the composition will be differentiable? I know that a diffeomorphism, but I can't solve the domain issue. Thanks!"," p   M^n p V  \rho: M \rightarrow \mathbb{R}   \mathbf{1}   p   supp (\rho) \subset V  C^\infty R R^n V p (\varphi,U) p V \subset U C^\infty \rho:\mathbb{R}^n \rightarrow \mathbb{R} \varphi(p) \mathbf{1} B[\varphi(p),a] B[\varphi(p),b] a<b<d(\varphi(p),\partial \varphi(V)) \rho \circ \varphi:U\rightarrow \mathbb{R} U M \varphi M \rho \circ \varphi:M\rightarrow \mathbb{R} \varphi:U\rightarrow \varphi(U)","['multivariable-calculus', 'differential-geometry', 'manifolds', 'vector-analysis']"
37,Calculating the volume of an ellipsoid with triple integrals,Calculating the volume of an ellipsoid with triple integrals,,"I am trying to find the volume of the ellipsoid $\left(\frac{x}{a}\right)^2 + \left(\frac{y}{b}\right)^2 + \left(\frac{z}{c}\right)^2=1$ by making the substitution $u=x/a$ , $v=y/b$ and $w=z/c$ . With this substitution, the equation becomes $u^2+v^2+w^2=1$ . Projecting this on the $uv$ -plane, we get a circle of radius $1$ . Hence, the triple integral is $I=\int_{u=-1}^{1} \int_{v=-\sqrt{1-u^2}}^{\sqrt{1-u^2}} \int_{w=-\sqrt{1-u^2-w^2}}^{\sqrt{1-u^2-w^2}} abc \, \mathrm{dw} \, \mathrm{dv} \, \mathrm{du}$ . The $abc$ comes from the Jacobian. Then, to evaluate this, I decided to switch to cylindrical coordinates. So, $u=r\cos\theta$ , $v=r\sin\theta$ , and $w=w$ . This gave me $I=\int_{\theta=0}^{2\pi}\int_{r=0}^{1}\int_{w=-\sqrt{1-r^2}}^{\sqrt{1-r^2}} abc r \, \mathrm{dw} \, \mathrm{dr} \, \mathrm{d\theta}$ , where the $r$ comes from the Jacobian of the second transformation. I then evaluated this: \begin{align*} I &= \int_{\theta=0}^{2\pi}\int_{r=0}^{1}\left[abcrw\right]_{w=-\sqrt{1-r^2}}^{\sqrt{1-r^2}} \, \mathrm{dr} \, \mathrm{d\theta} \\ &= \int_{\theta=0}^{2\pi}\int_{r=0}^{1} 2abcr\sqrt{1-r^2} \, \mathrm{dr} \, \mathrm{d\theta} \\ &= \int_{\theta=0}^{2\pi} \frac{2abc}{3} \, \mathrm{d\theta} \\ &= \frac{4\pi}{3}abc \end{align*} My answer is correct, which I am happy about, but is my solution correct? Is it clear to understand? Also, how else could I calculate the volume? I think spherical coordinates could also be useful, but I don't know if it would be easier than using cylindrical coordinates.","I am trying to find the volume of the ellipsoid by making the substitution , and . With this substitution, the equation becomes . Projecting this on the -plane, we get a circle of radius . Hence, the triple integral is . The comes from the Jacobian. Then, to evaluate this, I decided to switch to cylindrical coordinates. So, , , and . This gave me , where the comes from the Jacobian of the second transformation. I then evaluated this: My answer is correct, which I am happy about, but is my solution correct? Is it clear to understand? Also, how else could I calculate the volume? I think spherical coordinates could also be useful, but I don't know if it would be easier than using cylindrical coordinates.","\left(\frac{x}{a}\right)^2 + \left(\frac{y}{b}\right)^2 + \left(\frac{z}{c}\right)^2=1 u=x/a v=y/b w=z/c u^2+v^2+w^2=1 uv 1 I=\int_{u=-1}^{1} \int_{v=-\sqrt{1-u^2}}^{\sqrt{1-u^2}} \int_{w=-\sqrt{1-u^2-w^2}}^{\sqrt{1-u^2-w^2}} abc \, \mathrm{dw} \, \mathrm{dv} \, \mathrm{du} abc u=r\cos\theta v=r\sin\theta w=w I=\int_{\theta=0}^{2\pi}\int_{r=0}^{1}\int_{w=-\sqrt{1-r^2}}^{\sqrt{1-r^2}} abc r \, \mathrm{dw} \, \mathrm{dr} \, \mathrm{d\theta} r \begin{align*}
I &= \int_{\theta=0}^{2\pi}\int_{r=0}^{1}\left[abcrw\right]_{w=-\sqrt{1-r^2}}^{\sqrt{1-r^2}} \, \mathrm{dr} \, \mathrm{d\theta} \\
&= \int_{\theta=0}^{2\pi}\int_{r=0}^{1} 2abcr\sqrt{1-r^2} \, \mathrm{dr} \, \mathrm{d\theta} \\
&= \int_{\theta=0}^{2\pi} \frac{2abc}{3} \, \mathrm{d\theta} \\
&= \frac{4\pi}{3}abc
\end{align*}","['multivariable-calculus', 'multiple-integral']"
38,"Wrong codomain, why is this function called a composition?","Wrong codomain, why is this function called a composition?",,"I'm reading a book and it says: Definition: Let $\pmb f: \mathcal D_{\pmb f} \subset \mathbb R^n \to \mathbb R^m$ and $\pmb g:\mathcal D_{\pmb g} \subset\mathbb R^m \to \mathbb R^p$ be two maps and $\pmb x_0\ \in  \mathcal D_{\pmb f}$ a point such that $\pmb y_0=\pmb f(\pmb x_0)\in \mathcal D_{\pmb g}$ , so that the composite $$ \pmb h=\pmb g \circ \pmb f : \mathcal D_{\pmb h} \subset \mathbb R^n \to\mathbb R^p \tag 1 $$ for $\pmb x_0\in \mathcal D_h$ , is well defined. So far so good! And then: Theorem: Let $\pmb f$ be differentiable at $\pmb x_0 \in \mathcal D_{\pmb h}$ and $\pmb g$ differentiable at $\pmb y_0 = \pmb f(\pmb x_0)$ . Then $\pmb h = \pmb g\circ \pmb f$ is differentiable at $\pmb x_0$ and its Jacobian matrix is $$ \pmb J(\pmb g\circ \pmb f) (\pmb x_0) = \pmb J\pmb g(\pmb y_0)\pmb J\pmb f(\pmb x_0) \tag 2 $$ So far so good! But then this example is given: Let $\phi: I\subset \mathbb R\to \mathbb R$ be a differentiable map and $f:\mathbb R^2\to \mathbb R$ a scalar differentiable function. The composite $h(x)= f(x,\phi(x))$ is differentiable on $I$ , and $$ h'(x) = \frac{dh}{dx}(x)= \frac{\partial f}{\partial x}(x,\phi(x))+\frac{\partial f}{\partial y}(x,\phi(x))\phi'(x) \tag 3 $$ as follows from the theorem above, since $h=f \circ \Phi$ , with $\Phi:I\to \mathbb R^2$ , $\Phi(x)=(x,\phi(x))$ Note: The book used $\Phi$ and not $\phi$ in the last line, is it a typo? Here is my problem: I don't follow why $h$ is a composition of $f$ and $\phi$ . Because, according to $(1)$ the co-domain of the ""inner function"", $\pmb f$ , should be equal to the domain of the ""outer function"", $\pmb g$ . But, in this example, the ""inner function"" is $\phi$ with the co-domain $I\subset \mathbb R$ and this is not equal to the domain of the ""outer function"", $f$ , which is $\mathbb R^2$ . So why is $h$ a composition? Can somebody shed some light here?","I'm reading a book and it says: Definition: Let and be two maps and a point such that , so that the composite for , is well defined. So far so good! And then: Theorem: Let be differentiable at and differentiable at . Then is differentiable at and its Jacobian matrix is So far so good! But then this example is given: Let be a differentiable map and a scalar differentiable function. The composite is differentiable on , and as follows from the theorem above, since , with , Note: The book used and not in the last line, is it a typo? Here is my problem: I don't follow why is a composition of and . Because, according to the co-domain of the ""inner function"", , should be equal to the domain of the ""outer function"", . But, in this example, the ""inner function"" is with the co-domain and this is not equal to the domain of the ""outer function"", , which is . So why is a composition? Can somebody shed some light here?","\pmb f: \mathcal D_{\pmb f} \subset \mathbb R^n \to \mathbb R^m \pmb g:\mathcal D_{\pmb g} \subset\mathbb R^m \to \mathbb R^p \pmb x_0\ \in  \mathcal D_{\pmb f} \pmb y_0=\pmb f(\pmb x_0)\in \mathcal D_{\pmb g} 
\pmb h=\pmb g \circ \pmb f : \mathcal D_{\pmb h} \subset \mathbb R^n \to\mathbb R^p \tag 1
 \pmb x_0\in \mathcal D_h \pmb f \pmb x_0 \in \mathcal D_{\pmb h} \pmb g \pmb y_0 = \pmb f(\pmb x_0) \pmb h = \pmb g\circ \pmb f \pmb x_0 
\pmb J(\pmb g\circ \pmb f) (\pmb x_0) = \pmb J\pmb g(\pmb y_0)\pmb J\pmb f(\pmb x_0) \tag 2
 \phi: I\subset \mathbb R\to \mathbb R f:\mathbb R^2\to \mathbb R h(x)= f(x,\phi(x)) I 
h'(x) = \frac{dh}{dx}(x)=
\frac{\partial f}{\partial x}(x,\phi(x))+\frac{\partial f}{\partial y}(x,\phi(x))\phi'(x) \tag 3
 h=f \circ \Phi \Phi:I\to \mathbb R^2 \Phi(x)=(x,\phi(x)) \Phi \phi h f \phi (1) \pmb f \pmb g \phi I\subset \mathbb R f \mathbb R^2 h","['real-analysis', 'multivariable-calculus', 'functions', 'notation']"
39,"Does $\iint_{\mathbb{R}^2}\frac{\sin(x^2+y^2)}{x^2+y^2}\,dx\,dy$ converge?",Does  converge?,"\iint_{\mathbb{R}^2}\frac{\sin(x^2+y^2)}{x^2+y^2}\,dx\,dy","Consider $$\iint_{\mathbb{R}^2}\frac{\sin(x^2+y^2)}{x^2+y^2}\,dx\,dy$$ My try: changing to polar coordinates and then calculate the integral $$\int_{0}^{2\pi}\int_{0}^{\infty}\frac{\sin(r^2)}{r^2}\,dr\,d\theta\underset{r^2=u}{=}\pi\int_{0}^{\infty}\frac{\sin(u)}{u}\,du,$$ which converges but I saw in the solutions of a test that this integral diverges. Was I wrong?",Consider My try: changing to polar coordinates and then calculate the integral which converges but I saw in the solutions of a test that this integral diverges. Was I wrong?,"\iint_{\mathbb{R}^2}\frac{\sin(x^2+y^2)}{x^2+y^2}\,dx\,dy \int_{0}^{2\pi}\int_{0}^{\infty}\frac{\sin(r^2)}{r^2}\,dr\,d\theta\underset{r^2=u}{=}\pi\int_{0}^{\infty}\frac{\sin(u)}{u}\,du,","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
40,Finding Mass of Object Given Density,Finding Mass of Object Given Density,,"I need to find the mass of an object that lies above the disk $x^2 +y^2 \le 1$ in the $x$ - $y$ plane and below the sphere $x^2 + y^2 + z^2 = 4$ , if its density is $\rho(x, y, z)=2z$ . I know that the mass will be $\iiint_R 2z$ $dV$ , and I just need to determine the region $R$ which bounds the object. If I were to use spherical coordinates, I'd have $(r, \theta, \phi)$ where $0 \le r \le 1$ (since the radial distance is restricted by the disk), $0 \le \theta \le 2\pi$ (since we can complete a full revolution just fine), however I am unsure how to determine the upper limit of $\phi$ . Am I going in the right direction using spherical coordinates? And how would I find the upper limit of $\phi$ ? Thanks.","I need to find the mass of an object that lies above the disk in the - plane and below the sphere , if its density is . I know that the mass will be , and I just need to determine the region which bounds the object. If I were to use spherical coordinates, I'd have where (since the radial distance is restricted by the disk), (since we can complete a full revolution just fine), however I am unsure how to determine the upper limit of . Am I going in the right direction using spherical coordinates? And how would I find the upper limit of ? Thanks.","x^2 +y^2 \le 1 x y x^2 + y^2 + z^2 = 4 \rho(x, y, z)=2z \iiint_R 2z dV R (r, \theta, \phi) 0 \le r \le 1 0 \le \theta \le 2\pi \phi \phi","['multivariable-calculus', 'definite-integrals', 'coordinate-systems', 'multiple-integral', 'solid-geometry']"
41,"Line integral for vector field $F(x,y)=\left(-\frac{y}{x^2 + y^2} , \frac{x}{x^2+y^2}\right)$ with the curve $\vert x \vert + \vert y \vert = 5$",Line integral for vector field  with the curve,"F(x,y)=\left(-\frac{y}{x^2 + y^2} , \frac{x}{x^2+y^2}\right) \vert x \vert + \vert y \vert = 5","$Q)$ For a vector field $F(x,y)=\left(-\frac{y}{x^2 + y^2} , \frac{x}{x^2+y^2}\right)$ in $\mathbb{R}^2$ Find the $\int_{\vert x \vert + \vert y \vert = 5 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy $ In my lecture's note he claim that $\int_{\vert x \vert + \vert y \vert = 5 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy $ = $\int_{x^2 +  y^2 = 1 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy $ And then, he solved by parameterizing curve $x^2+y^2=1$ to $(cost, sint), [0 \leq t \leq 2\pi]$ But my doubt is firstly the vector field, $F$ is not defined at $(0,0)$ , Hence it is not simply connected. So we can't say the not conservative of the $F$ though $curl F = (0,0)$ . Therefore we can't guarantee the path independence, we can't claim that $\int_{\vert x \vert + \vert y \vert = 5 } = \int_{x^2 + y^2 = 1 } $ . Is my opinion right? If the lecture is right what is his ground justifying $\int_{\vert x \vert + \vert y \vert = 5 } = \int_{x^2 + y^2 = 1 } $ ? If the lecture's solution is false, How to Solve it? Any help would be appreciated. thanks.","For a vector field in Find the In my lecture's note he claim that = And then, he solved by parameterizing curve to But my doubt is firstly the vector field, is not defined at , Hence it is not simply connected. So we can't say the not conservative of the though . Therefore we can't guarantee the path independence, we can't claim that . Is my opinion right? If the lecture is right what is his ground justifying ? If the lecture's solution is false, How to Solve it? Any help would be appreciated. thanks.","Q) F(x,y)=\left(-\frac{y}{x^2 + y^2} , \frac{x}{x^2+y^2}\right) \mathbb{R}^2 \int_{\vert x \vert + \vert y \vert = 5 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy  \int_{\vert x \vert + \vert y \vert = 5 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy  \int_{x^2 +  y^2 = 1 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy  x^2+y^2=1 (cost, sint), [0 \leq t \leq 2\pi] F (0,0) F curl F = (0,0) \int_{\vert x \vert + \vert y \vert = 5 } = \int_{x^2 + y^2 = 1 }  \int_{\vert x \vert + \vert y \vert = 5 } = \int_{x^2 + y^2 = 1 } ",['multivariable-calculus']
42,"volume of solid generated by the regin bounded by curve $y=\sqrt{x},y=\frac{x-3}{2},y=0$ about $x$ axis",volume of solid generated by the regin bounded by curve  about  axis,"y=\sqrt{x},y=\frac{x-3}{2},y=0 x","Using sell method to find the volume of solid generated by revolving the region bounded by $$y=\sqrt{x},y=\frac{x-3}{2},y=0$$ about $x$ axis, is (using shell method) What I try: Solving two given curves $$\sqrt{x}=\frac{x-3}{2}\Longrightarrow x^2-10x+9=0$$ We have $x=1$ (Invalid) and $x=9$ (Valid). Put $x=9$ in $y=\sqrt{x}$ we have $y=3$ Now Volume of solid form by rotation about $x$ axis is $$=\int^{9}_{0}2\pi y\bigg(y^2-2y-3\bigg)dy$$ Is my Volume Integral is right? If not then how do I solve it? Help me please.","Using sell method to find the volume of solid generated by revolving the region bounded by about axis, is (using shell method) What I try: Solving two given curves We have (Invalid) and (Valid). Put in we have Now Volume of solid form by rotation about axis is Is my Volume Integral is right? If not then how do I solve it? Help me please.","y=\sqrt{x},y=\frac{x-3}{2},y=0 x \sqrt{x}=\frac{x-3}{2}\Longrightarrow x^2-10x+9=0 x=1 x=9 x=9 y=\sqrt{x} y=3 x =\int^{9}_{0}2\pi y\bigg(y^2-2y-3\bigg)dy","['calculus', 'multivariable-calculus', 'volume']"
43,"Find the saddle point of $F(x_1,x_2,x_3,y_1,y_2,y_3)=(x_1-2x_2+x_3)y_1+(2x_1-2x_3)y_2+(-x_1+x_2)y_3$",Find the saddle point of,"F(x_1,x_2,x_3,y_1,y_2,y_3)=(x_1-2x_2+x_3)y_1+(2x_1-2x_3)y_2+(-x_1+x_2)y_3","Finding the saddle points of $F(x_1,x_2,x_3,y_1,y_2,y_3)=(x_1-2x_2+x_3)y_1+(2x_1-2x_3)y_2$ + $(-x_1+x_2)y_3$ subject to the constraints $x_1+x_2+x_3=1, y_1+y_2+y_3=1$ . Show that the saddle point is $x=(\frac{1/3}{1/3},\frac{1/3}{1/3},\frac{1/3}{1/3}),y=(\frac{2}{7},\frac{1}{7},\frac{4}{7})$ . I know how to find the saddle point of a function with two variable by using $\Delta=(f_{12})^2-f_{11}f_{22}$ , then when $\Delta>0$ the point is a saddle point. But for this question, by using the constraints, we can reduce the variables from 6 to 4, but still can not use the $\Delta$ formula, and this question requires not to use lagrange multiplier. Thanks.","Finding the saddle points of + subject to the constraints . Show that the saddle point is . I know how to find the saddle point of a function with two variable by using , then when the point is a saddle point. But for this question, by using the constraints, we can reduce the variables from 6 to 4, but still can not use the formula, and this question requires not to use lagrange multiplier. Thanks.","F(x_1,x_2,x_3,y_1,y_2,y_3)=(x_1-2x_2+x_3)y_1+(2x_1-2x_3)y_2 (-x_1+x_2)y_3 x_1+x_2+x_3=1, y_1+y_2+y_3=1 x=(\frac{1/3}{1/3},\frac{1/3}{1/3},\frac{1/3}{1/3}),y=(\frac{2}{7},\frac{1}{7},\frac{4}{7}) \Delta=(f_{12})^2-f_{11}f_{22} \Delta>0 \Delta",[]
44,"Why does this ""gradient field test"" not work on the spin field $S / r^2$? (from Strang's Calculus)","Why does this ""gradient field test"" not work on the spin field ? (from Strang's Calculus)",S / r^2,"In section 15.2, Strang's Calculus explains that for any gradient field $\bf{F} = Mi + Nj$ , ${\partial M \over \partial y} = {\partial N \over \partial x}$ . (Strang calls this ""test D"" for identifying a vector field as being a gradient of some function.) This makes sense, since the components of a ""gradient field"" are the partial derivatives of some function $f$ , and we know that for any $f$ , ${\partial f \over \partial x \partial y} = {\partial f \over \partial y \partial x}$ . The gradient of $f = \tan^{-1}\left({y \over x}\right)$ is ${-y \over x^2 + y^2}i + {x \over x^2 + y^2}j$ . However, this vector field does not seem to pass ""test D"", since $$ {\partial \over \partial x}\left({-y \over x^2 + y^2}\right) = {2xy \over (x^2 + y^2)^2} $$ But $$ {\partial \over \partial y}\left({x \over x^2 + y^2}\right) = -{2xy \over (x^2 + y^2)^2} $$ I'm sure something is wrong with my reasoning, but I am struggling to find the mistake. Can anyone point it out?","In section 15.2, Strang's Calculus explains that for any gradient field , . (Strang calls this ""test D"" for identifying a vector field as being a gradient of some function.) This makes sense, since the components of a ""gradient field"" are the partial derivatives of some function , and we know that for any , . The gradient of is . However, this vector field does not seem to pass ""test D"", since But I'm sure something is wrong with my reasoning, but I am struggling to find the mistake. Can anyone point it out?","\bf{F} = Mi + Nj {\partial M \over \partial y} = {\partial N \over \partial x} f f {\partial f \over \partial x \partial y} = {\partial f \over \partial y \partial x} f = \tan^{-1}\left({y \over x}\right) {-y \over x^2 + y^2}i + {x \over x^2 + y^2}j 
{\partial \over \partial x}\left({-y \over x^2 + y^2}\right) = {2xy \over (x^2 + y^2)^2}
 
{\partial \over \partial y}\left({x \over x^2 + y^2}\right) = -{2xy \over (x^2 + y^2)^2}
","['calculus', 'multivariable-calculus', 'vector-analysis', 'vector-fields']"
45,"Integral $ \int_E \frac{1}{x^ay^b}dxdy \qquad E={x>0,y>0,xy \geq 1}$",Integral," \int_E \frac{1}{x^ay^b}dxdy \qquad E={x>0,y>0,xy \geq 1}","I'm asked to compute: $$ \int_E \frac{1}{x^ay^b}dxdy \qquad E={x>0,y>0,xy \geq} 1$$ but I'm really finding difficulties in doing it as the domain is unlimited. I can pretty much solve this kind of integrals when the domain is limited and there is a problem in one or more than one points, but how can I compute this in an efficient way? I am also a little bit concerned about polar coordinates: I think that it could be a good way to do it, but people here, on other questions, told me that I should avoid doing it to prevent other issues.  Can you please help me understanding how to proceed in problems like this one? Thanks in advance. EDIT We notice that we can express the domain as $E=\{x>0, y>\frac{1}{x}\}$ so we have: $$\int_E \frac{1}{x^ay^b}dxdy= \int_0^{+\infty}\left(\int_{\frac{1}{x}}^{+\infty}\frac{1}{x^ay^b}dy\right)dx=\int_0^{+\infty}\frac{1}{x^a}\left[ \frac{1}{(1-b)y^{b-1}} \right]_{\frac{1}{x}}^{\infty}dx$$ Let's suppose $b>1$ (otherwise the integral diverges): $$\frac{-1}{b-1}\int_0^{+\infty} \frac{1}{x^a}x^{b-1}dx=\frac{-1}{b-1}\int_0^{+\infty} \frac{1}{x^{a-b+1}}dx$$ But now we have something that diverges for all values of $a,b$ as it has both problems in $0$ and at $\infty$ . Is this correct?","I'm asked to compute: but I'm really finding difficulties in doing it as the domain is unlimited. I can pretty much solve this kind of integrals when the domain is limited and there is a problem in one or more than one points, but how can I compute this in an efficient way? I am also a little bit concerned about polar coordinates: I think that it could be a good way to do it, but people here, on other questions, told me that I should avoid doing it to prevent other issues.  Can you please help me understanding how to proceed in problems like this one? Thanks in advance. EDIT We notice that we can express the domain as so we have: Let's suppose (otherwise the integral diverges): But now we have something that diverges for all values of as it has both problems in and at . Is this correct?"," \int_E \frac{1}{x^ay^b}dxdy \qquad E={x>0,y>0,xy \geq} 1 E=\{x>0, y>\frac{1}{x}\} \int_E \frac{1}{x^ay^b}dxdy= \int_0^{+\infty}\left(\int_{\frac{1}{x}}^{+\infty}\frac{1}{x^ay^b}dy\right)dx=\int_0^{+\infty}\frac{1}{x^a}\left[ \frac{1}{(1-b)y^{b-1}} \right]_{\frac{1}{x}}^{\infty}dx b>1 \frac{-1}{b-1}\int_0^{+\infty} \frac{1}{x^a}x^{b-1}dx=\frac{-1}{b-1}\int_0^{+\infty} \frac{1}{x^{a-b+1}}dx a,b 0 \infty","['calculus', 'integration', 'limits', 'multivariable-calculus', 'improper-integrals']"
46,How we calculate the asymptotes of a general hyperbola?,How we calculate the asymptotes of a general hyperbola?,,"I was reading about the asymptotes of the following hyperbola: $$\frac{x^2}{a^2} - \frac{y^2}{b^2} = 1 \quad \quad (1)$$ and the book said that the inclined asymptotes are $ y = \pm \frac{b}{a} x $ and the book mentioned that you can find them by setting the RHS of eqn.(1) equal to $0$ but I do not know why we should do this, Could anyone explain this for me please? Also, I know that the correct way of finding the inclined asymptote (y = mx + c) for a function $f(x)$ is that if this line satisfies $$\lim_{x \to \pm \infty} [f(x) -  (mx + c)] = 0.$$ Then it is an asymptote. but I do not know how we found that line in the case of the hyperbola mentioned above. could anyone explain this for me please?","I was reading about the asymptotes of the following hyperbola: and the book said that the inclined asymptotes are and the book mentioned that you can find them by setting the RHS of eqn.(1) equal to but I do not know why we should do this, Could anyone explain this for me please? Also, I know that the correct way of finding the inclined asymptote (y = mx + c) for a function is that if this line satisfies Then it is an asymptote. but I do not know how we found that line in the case of the hyperbola mentioned above. could anyone explain this for me please?",\frac{x^2}{a^2} - \frac{y^2}{b^2} = 1 \quad \quad (1)  y = \pm \frac{b}{a} x  0 f(x) \lim_{x \to \pm \infty} [f(x) -  (mx + c)] = 0.,['calculus']
47,how does one prove $\operatorname{tr}(A^2(A^2)^T) \le\operatorname{tr}(AA^T)^2$?,how does one prove ?,\operatorname{tr}(A^2(A^2)^T) \le\operatorname{tr}(AA^T)^2,"I would like to prove that $\lim_{H \to 0_{k\times k}} \frac {H^2}{\vert {H} \vert} = 0_{k\times k}$ . here is my idea: since $M_{k\times k}(\Bbb R)$ is an inner product space with respect to the standard inner product defined by $(A,B)=\operatorname{tr}(AB^T)$ , the norm defined by the standard inner product is $\vert A \vert =\sqrt{(A,A)}$ hence if $\operatorname{tr}(A^2(A^2)^T) \le \operatorname{tr}(AA^T)^2$ is true for every matrix $A \in M_{k\times k}(\Bbb R)$ then: $$0 \le \lim_{H \to 0_{k\times k}} \vert \frac {H^2}{\vert {H} \vert}\vert = \lim_{H \to 0_{k\times k}} \frac {1} {\vert H \vert} \vert H^2 \vert = \lim_{H \to 0_{k\times k}} \frac {1} {\vert H \vert} \operatorname{tr}(H^2(H^2)^T)$$ $$\le \lim_{H \to 0_{k\times k}} \frac {1} {\vert H \vert} \vert H \vert^2=\lim_{H \to 0_{k\times k}} |H|=0$$ hence by the squeeze theorem we get $\lim_{H \to 0_{k\times k}} |\frac {H^2}{\vert {H} \vert}| = 0$ and this can only occur if $\lim_{H \to 0_{k\times k}} \frac {H^2}{\vert {H} \vert} = 0_{k\times k}$ .","I would like to prove that . here is my idea: since is an inner product space with respect to the standard inner product defined by , the norm defined by the standard inner product is hence if is true for every matrix then: hence by the squeeze theorem we get and this can only occur if .","\lim_{H \to 0_{k\times k}} \frac {H^2}{\vert {H} \vert} = 0_{k\times k} M_{k\times k}(\Bbb R) (A,B)=\operatorname{tr}(AB^T) \vert A \vert =\sqrt{(A,A)} \operatorname{tr}(A^2(A^2)^T) \le \operatorname{tr}(AA^T)^2 A \in M_{k\times k}(\Bbb R) 0 \le \lim_{H \to 0_{k\times k}} \vert \frac {H^2}{\vert {H} \vert}\vert = \lim_{H \to 0_{k\times k}} \frac {1} {\vert H \vert} \vert H^2 \vert = \lim_{H \to 0_{k\times k}} \frac {1} {\vert H \vert} \operatorname{tr}(H^2(H^2)^T) \le \lim_{H \to 0_{k\times k}} \frac {1} {\vert H \vert} \vert H \vert^2=\lim_{H \to 0_{k\times k}} |H|=0 \lim_{H \to 0_{k\times k}} |\frac {H^2}{\vert {H} \vert}| = 0 \lim_{H \to 0_{k\times k}} \frac {H^2}{\vert {H} \vert} = 0_{k\times k}","['real-analysis', 'multivariable-calculus', 'vector-analysis']"
48,Proving Lipschitz gradient of $f(x)=\sqrt{1+\|x\|^2}$ [closed],Proving Lipschitz gradient of  [closed],f(x)=\sqrt{1+\|x\|^2},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Given $f(x)=\sqrt{1+\|x\|^2}$ and $f:\mathbb{R}^n\to\mathbb{R}$ . Prove $f\in C_1^{1,1}$ , meaning $$\|\nabla f(x)-\nabla f(y)\|\leq\|x-y\|.$$ I've tried to see how to prove it for $n=2$ but got stuck at computing the norm of the hessian. I know if I can prove $\|\nabla^2f(x)\|_2\leq 1$ will be enough. I know that $\nabla f(x)=\frac{x}{\sqrt{1+\|x\|^2}}$ i thought maybe i can assume w.l.o.g that $\|x\|\geq \|y\|$ to prove this.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Given and . Prove , meaning I've tried to see how to prove it for but got stuck at computing the norm of the hessian. I know if I can prove will be enough. I know that i thought maybe i can assume w.l.o.g that to prove this.","f(x)=\sqrt{1+\|x\|^2} f:\mathbb{R}^n\to\mathbb{R} f\in C_1^{1,1} \|\nabla f(x)-\nabla f(y)\|\leq\|x-y\|. n=2 \|\nabla^2f(x)\|_2\leq 1 \nabla f(x)=\frac{x}{\sqrt{1+\|x\|^2}} \|x\|\geq \|y\|","['multivariable-calculus', 'lipschitz-functions']"
49,Calculate surface integral with force,Calculate surface integral with force,,"Let $S=\{(x,y,z)\in \mathbb{R} : x^2+y^2+z^2 = 11\}$ and $F(x,y,z)=(2\sqrt{11}x,y^2,z^2)$ . Calculate $$\int\int_SF(x,y,z)\;dS$$ I've calculated S parameterized, whose expression is: $$S(x,y) = (x,y,\sqrt{11-x^2-y^2})$$ Hence, $$r_x=\frac{\partial S}{\partial x}=(1,0,\frac{-x}{\sqrt{11-x^2-y^2}})$$ $$r_y=\frac{\partial S}{\partial y}=(0,1,\frac{-y}{\sqrt{11-x^2-y^2}})$$ $$||r_x \times r_y|| = \frac{\sqrt{11}}{\sqrt{11-x^2-y^2}}$$ So $dS = \frac{\sqrt{11}}{\sqrt{11-x^2-y^2}}dxdy$ and then $$\int\int_SF(x,y,z)\;dS = \int\int_SF(x,y,z)\;\frac{\sqrt{11}}{\sqrt{11-x^2-y^2}}dxdy$$ But then I don't know how to continue with this procedure. Another idea I have is to establish the following relationship: $$\int\int_SF\;dS= \int\int\int_V \nabla F\;dV$$ Any hint? Thanks in advance.","Let and . Calculate I've calculated S parameterized, whose expression is: Hence, So and then But then I don't know how to continue with this procedure. Another idea I have is to establish the following relationship: Any hint? Thanks in advance.","S=\{(x,y,z)\in \mathbb{R} : x^2+y^2+z^2 = 11\} F(x,y,z)=(2\sqrt{11}x,y^2,z^2) \int\int_SF(x,y,z)\;dS S(x,y) = (x,y,\sqrt{11-x^2-y^2}) r_x=\frac{\partial S}{\partial x}=(1,0,\frac{-x}{\sqrt{11-x^2-y^2}}) r_y=\frac{\partial S}{\partial y}=(0,1,\frac{-y}{\sqrt{11-x^2-y^2}}) ||r_x \times r_y|| = \frac{\sqrt{11}}{\sqrt{11-x^2-y^2}} dS = \frac{\sqrt{11}}{\sqrt{11-x^2-y^2}}dxdy \int\int_SF(x,y,z)\;dS = \int\int_SF(x,y,z)\;\frac{\sqrt{11}}{\sqrt{11-x^2-y^2}}dxdy \int\int_SF\;dS= \int\int\int_V \nabla F\;dV","['multivariable-calculus', 'vector-analysis']"
50,Is there a general method to compute the area/volume enclosed by an implicit curve/surface?,Is there a general method to compute the area/volume enclosed by an implicit curve/surface?,,"If I have an implicit function $f_2(x,y) = C$ of a closed curve or an implicit function $f_3(x,y,z) = C$ of a closed surface, is there a general manner in which I can compute the area or volume enclosed by the curve or surface, respectively? Or is this dependent on the choice of functions?","If I have an implicit function of a closed curve or an implicit function of a closed surface, is there a general manner in which I can compute the area or volume enclosed by the curve or surface, respectively? Or is this dependent on the choice of functions?","f_2(x,y) = C f_3(x,y,z) = C","['multivariable-calculus', 'differential-geometry', 'surfaces', 'curves', 'implicit-function']"
51,A subtle problem regarding tangent line when gradient is zero,A subtle problem regarding tangent line when gradient is zero,,"In the section of ""Applications of Partial Derivates"" in Adams' calculus book, 7th edition, page 756, there is a part which talks about ""Lagrange Multipliers"". It says: Suppose that $f$ and $g$ have continuous first partial derivatives near the point $P_0=(x_0,y_0)$ on the curve $C$ with equation $g(x,y)=0$ . Suppose also that, when restricted to points on $C$ , the function $f(x,y)$ has a local maximum or minimum value at $P0$ . Finally suppose that: (i) $P_0$ is not an endpoint of $C$ , and (ii) $∇g(P_0)≠0$ , Then there exists a number $λ_0$ such that $(x_0,y_0,λ_0)$ is a critical point of the Lagrangian function $L(x,y,λ)=f(x,y)+λg(x,y)$ . Proof: Together (i) and (ii) imply that $C$ is smooth enough to have a tangent line at $P_0$ … Can someone explain why the first sentence of the proof is true? I mean why is it that (i) and (ii) imply that $C$ is smooth enough to have a tangent line? Note that my main problem is with (ii). What happens to the smoothness and tangent line when $∇g(P_0)$ is zero and what difference does it make when it is non-zero? Is it a necessary condition for smoothness of $C$ on $P_0$ that $∇g(P_0)≠0$ ? Any help on the relevance of (ii) to the smoothness of $C$ on $P_0$ would be highly appreciated.","In the section of ""Applications of Partial Derivates"" in Adams' calculus book, 7th edition, page 756, there is a part which talks about ""Lagrange Multipliers"". It says: Suppose that and have continuous first partial derivatives near the point on the curve with equation . Suppose also that, when restricted to points on , the function has a local maximum or minimum value at . Finally suppose that: (i) is not an endpoint of , and (ii) , Then there exists a number such that is a critical point of the Lagrangian function . Proof: Together (i) and (ii) imply that is smooth enough to have a tangent line at … Can someone explain why the first sentence of the proof is true? I mean why is it that (i) and (ii) imply that is smooth enough to have a tangent line? Note that my main problem is with (ii). What happens to the smoothness and tangent line when is zero and what difference does it make when it is non-zero? Is it a necessary condition for smoothness of on that ? Any help on the relevance of (ii) to the smoothness of on would be highly appreciated.","f g P_0=(x_0,y_0) C g(x,y)=0 C f(x,y) P0 P_0 C ∇g(P_0)≠0 λ_0 (x_0,y_0,λ_0) L(x,y,λ)=f(x,y)+λg(x,y) C P_0 C ∇g(P_0) C P_0 ∇g(P_0)≠0 C P_0","['multivariable-calculus', 'lagrange-multiplier']"
52,"limit of multivariable function $f\left(x,y,z\right)=\left(x+y+z\right)\sin\left(\frac{1}{x}\right)\sin\left(\frac{1}{y}\right)$",limit of multivariable function,"f\left(x,y,z\right)=\left(x+y+z\right)\sin\left(\frac{1}{x}\right)\sin\left(\frac{1}{y}\right)","this is a question from my calculus homework: Check if the following function has a limit in $(0,0,0)$ and if it has, find the value: $$f\left(x,y,z\right)=\left(x+y+z\right)\sin\left(\frac{1}{x}\right)\sin\left(\frac{1}{y}\right)$$ I tried to use different approaches in order to show that the limits are not equal so the function doesn't have a limit, but obviously I always got to $0$ for example: $$x=y=z: \lim _{\left(z,z,z\right)\to \left(0,0,0\right)}\left(z+z+z\right)\sin\left(\frac{1}{z}\right)\sin\left(\frac{1}{z}\right)=\lim_{\left(z,z,z\right)\to \:\left(0,0,0\right)}3z\:\sin^2\left(\frac{1}{z}\right)$$ since $\lim _{z\to 0}3z=0$ and $\sin(\frac{1}{z})$ is a bounded function: $$\lim_{\left(z,z,z\right)\to \:\left(0,0,0\right)}3z\:\sin^2\left(\frac{1}{z}\right)=0$$ $$ $$ as well, I think converting coordinates won't help m I have no more idea then... $$ $$ sorry about my weak grammar so thankful for any help or hint","this is a question from my calculus homework: Check if the following function has a limit in and if it has, find the value: I tried to use different approaches in order to show that the limits are not equal so the function doesn't have a limit, but obviously I always got to for example: since and is a bounded function: as well, I think converting coordinates won't help m I have no more idea then... sorry about my weak grammar so thankful for any help or hint","(0,0,0) f\left(x,y,z\right)=\left(x+y+z\right)\sin\left(\frac{1}{x}\right)\sin\left(\frac{1}{y}\right) 0 x=y=z: \lim _{\left(z,z,z\right)\to \left(0,0,0\right)}\left(z+z+z\right)\sin\left(\frac{1}{z}\right)\sin\left(\frac{1}{z}\right)=\lim_{\left(z,z,z\right)\to \:\left(0,0,0\right)}3z\:\sin^2\left(\frac{1}{z}\right) \lim _{z\to 0}3z=0 \sin(\frac{1}{z}) \lim_{\left(z,z,z\right)\to \:\left(0,0,0\right)}3z\:\sin^2\left(\frac{1}{z}\right)=0    ","['limits', 'multivariable-calculus']"
53,Using the implicit function theorem to prove that a function attains a certain value,Using the implicit function theorem to prove that a function attains a certain value,,"Here' how I tried to do it but failed: I don't see what the implicit function theorem has to do with this (this exercise is after the section on the implicit function theorem), but anyway, this is what I got out of thinking about the problem. Since the matrix $Df(a)$ has rank $n$ , some $n$ by $n$ matrix formed of a collection of $n$ of its column vectors has nonzero determinant. The determinant of this matrix is a continuous function of its entries and since it is non-zero at $a$ , there is some neighborhood of $a$ on which it is non-zero, but I still don't know how I can use this. Suppose $c$ is in an $\varepsilon$ neighborhood of $0$ , since $f(a)=0$ , we have $\left|f\left(x\right)\right|< \varepsilon$ whenever $\left|x-a\right|\ <\delta$ for some $\delta$ and I should prove that $f$ actually attains the value $c$ for some point in this neighborhood but I don't see how I can do this.","Here' how I tried to do it but failed: I don't see what the implicit function theorem has to do with this (this exercise is after the section on the implicit function theorem), but anyway, this is what I got out of thinking about the problem. Since the matrix has rank , some by matrix formed of a collection of of its column vectors has nonzero determinant. The determinant of this matrix is a continuous function of its entries and since it is non-zero at , there is some neighborhood of on which it is non-zero, but I still don't know how I can use this. Suppose is in an neighborhood of , since , we have whenever for some and I should prove that actually attains the value for some point in this neighborhood but I don't see how I can do this.",Df(a) n n n n a a c \varepsilon 0 f(a)=0 \left|f\left(x\right)\right|< \varepsilon \left|x-a\right|\ <\delta \delta f c,"['real-analysis', 'multivariable-calculus', 'implicit-differentiation', 'implicit-function-theorem']"
54,"Show that the integral $ f(x,y)=1+\int_0^x\int_0^y f(u,v)dudv$ has at most one continuous solution",Show that the integral  has at most one continuous solution," f(x,y)=1+\int_0^x\int_0^y f(u,v)dudv","Show that the integral $$ f(x,y)=1+\int_0^x\int_0^y f(u,v)dudv$$ has at most one continuous solution for $0 \leq x \leq 1$ , $ 0\leq y <1$ . The integral can be written as $$I=1+\int_0^1\int_0^1 f(u,v)dudv.$$ Now if we take $f(u,v)$ to be an integrable function in the given region, certainly, we will get only one continuous solution or outcome. How to argue in general case? Suppose $f(u,v) = g(u,v)$ ,then $$\iint f(u,v)dudv = \iint g(u,v)dudv.$$ Therefore, $$1+\iint f(u,v)dudv = 1+\iint g(u,v)dudv.$$ This implies $$f(x,y)=g(x,y).$$ So only one continuous solution. Is it suitable argument??","Show that the integral has at most one continuous solution for , . The integral can be written as Now if we take to be an integrable function in the given region, certainly, we will get only one continuous solution or outcome. How to argue in general case? Suppose ,then Therefore, This implies So only one continuous solution. Is it suitable argument??"," f(x,y)=1+\int_0^x\int_0^y f(u,v)dudv 0 \leq x \leq 1  0\leq y <1 I=1+\int_0^1\int_0^1 f(u,v)dudv. f(u,v) f(u,v) = g(u,v) \iint f(u,v)dudv = \iint g(u,v)dudv. 1+\iint f(u,v)dudv = 1+\iint g(u,v)dudv. f(x,y)=g(x,y).","['integration', 'multivariable-calculus']"
55,$\int\limits_{0}^tu_tu^3d\tau=\frac{1}{4}u^4$,,\int\limits_{0}^tu_tu^3d\tau=\frac{1}{4}u^4,"I came across this statement(which I don't understand) when reading a solution to a different problem. I think I'm missing a small point here but I don't see it. Can you please help me with an explanation. Let $U\in\mathbb{R}^n$ be open. Suppose that $u=u(x,t)$ is a differentiable function from $U\times [0,\infty)\to\mathbb{R}$ with $u(x,0)=0$ Then $\int\limits_{0}^tu_tu^3d\tau=\frac{1}{4}u^4$ where $u_t$ is the derivative with respect to $t\in[0,\infty)$",I came across this statement(which I don't understand) when reading a solution to a different problem. I think I'm missing a small point here but I don't see it. Can you please help me with an explanation. Let be open. Suppose that is a differentiable function from with Then where is the derivative with respect to,"U\in\mathbb{R}^n u=u(x,t) U\times [0,\infty)\to\mathbb{R} u(x,0)=0 \int\limits_{0}^tu_tu^3d\tau=\frac{1}{4}u^4 u_t t\in[0,\infty)","['calculus', 'multivariable-calculus']"
56,Hadamard's lemma for arbitrary open subsets,Hadamard's lemma for arbitrary open subsets,,"I am struggling with the following. It regards the variants of Hadamard's lemma for smooth functions. I have found basically two statements. The usual one, which can be found e.g. on Wikipedia ( https://en.wikipedia.org/wiki/Hadamard%27s_lemma ) is the following: Hadamard's Lemma I: Let $U \subseteq \mathbb{R}^{n}$ be a star-convex neighborhood of a point $a$ . Let $f: U \rightarrow \mathbb{R}^{p}$ be a smooth map. Then for any $x \in U$ , one has $f(x) = f(a) + (h^{(a)}(x))(x-a)$ , for some smooth map $h^{(a)}: U \rightarrow Lin(\mathbb{R}^{n},\mathbb{R}^{p})$ . Next, I have found a version for arbitrary open set $U$ , but with a weaker statement (see Lemma 2.2.7 in Duistermaat - Kolk https://books.google.cz/books?id=5KXkkGTnaYIC ) which says the following: Hadamard's Lemma II: Let $U \subseteq \mathbb{R}^{n}$ be any neighborhood of point $a$ . Let $f: U \rightarrow \mathbb{R}^{p}$ be differentiable at $a$ . Then for any $x \in U$ , one has $f(x) = f(a) + (h^{(a)}(x))(x-a)$ , for some map $h^{(a)}: U \rightarrow Lin(\mathbb{R}^{n},\mathbb{R}^{p})$ continuous at $a$ . The actual question: Can the star-convexity of $U$ in assumptions of Lemma I be dropped? I understand how it is used in the proof (integration along line segments connecting $a$ to $x$ ). Are there some counter-examples? It follows from the proof of Lemma II that $h^{(a)}$ is continous on entire $U$ , but I am unable to prove that it is smooth. Thank you, Jan","I am struggling with the following. It regards the variants of Hadamard's lemma for smooth functions. I have found basically two statements. The usual one, which can be found e.g. on Wikipedia ( https://en.wikipedia.org/wiki/Hadamard%27s_lemma ) is the following: Hadamard's Lemma I: Let be a star-convex neighborhood of a point . Let be a smooth map. Then for any , one has , for some smooth map . Next, I have found a version for arbitrary open set , but with a weaker statement (see Lemma 2.2.7 in Duistermaat - Kolk https://books.google.cz/books?id=5KXkkGTnaYIC ) which says the following: Hadamard's Lemma II: Let be any neighborhood of point . Let be differentiable at . Then for any , one has , for some map continuous at . The actual question: Can the star-convexity of in assumptions of Lemma I be dropped? I understand how it is used in the proof (integration along line segments connecting to ). Are there some counter-examples? It follows from the proof of Lemma II that is continous on entire , but I am unable to prove that it is smooth. Thank you, Jan","U \subseteq \mathbb{R}^{n} a f: U \rightarrow \mathbb{R}^{p} x \in U f(x) = f(a) + (h^{(a)}(x))(x-a) h^{(a)}: U \rightarrow Lin(\mathbb{R}^{n},\mathbb{R}^{p}) U U \subseteq \mathbb{R}^{n} a f: U \rightarrow \mathbb{R}^{p} a x \in U f(x) = f(a) + (h^{(a)}(x))(x-a) h^{(a)}: U \rightarrow Lin(\mathbb{R}^{n},\mathbb{R}^{p}) a U a x h^{(a)} U","['calculus', 'multivariable-calculus', 'smooth-manifolds', 'smooth-functions']"
57,Closed form matrix derivative of $\operatorname{tr}(A\exp(X))$,Closed form matrix derivative of,\operatorname{tr}(A\exp(X)),"I am interested in finding a closed form expression for the following matrix derivative: $\frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A}\exp(\mathbf{X})\right)$ Where we assume that $\mathbf{A}$ and $\mathbf{X}$ are both symmetric $n\times n$ matrices. By applying the following identity: $\frac{\partial}{\partial \mathbf{X}} \operatorname{Tr}\left(\mathbf{A} \mathbf{X}^{k}\right)=\sum_{r=0}^{k-1}\left(\mathbf{X}^{r} \mathbf{A} \mathbf{X}^{k-r-1}\right)^{T}$ (found in https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) I figured one can use the definition of a matrix exponential to obtain the following expression: $\frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A}\exp(\mathbf{X})\right)=\sum_{k=0}^\infty\frac{\partial}{\partial \mathbf{X}} \operatorname{Tr}\left(\frac{1}{k!}\mathbf{A}\mathbf{X}^{k}\right)=\sum_{k=1}^{\infty}\sum_{r=0}^{k-1}\left(\mathbf{X}^{r} \mathbf{A} \mathbf{X}^{k-r-1}\right)^{T}$ Where we skip over $k=0$ since $\frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A})\right)=\mathbf{O}$ . I am not sure if this expression can be reduced to something which is in closed form. If you are able to suggest a method with which this might be possible, or a different method entirely, I would be helped greatly. Thanks in advance for your suggestions and help.","I am interested in finding a closed form expression for the following matrix derivative: Where we assume that and are both symmetric matrices. By applying the following identity: (found in https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) I figured one can use the definition of a matrix exponential to obtain the following expression: Where we skip over since . I am not sure if this expression can be reduced to something which is in closed form. If you are able to suggest a method with which this might be possible, or a different method entirely, I would be helped greatly. Thanks in advance for your suggestions and help.",\frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A}\exp(\mathbf{X})\right) \mathbf{A} \mathbf{X} n\times n \frac{\partial}{\partial \mathbf{X}} \operatorname{Tr}\left(\mathbf{A} \mathbf{X}^{k}\right)=\sum_{r=0}^{k-1}\left(\mathbf{X}^{r} \mathbf{A} \mathbf{X}^{k-r-1}\right)^{T} \frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A}\exp(\mathbf{X})\right)=\sum_{k=0}^\infty\frac{\partial}{\partial \mathbf{X}} \operatorname{Tr}\left(\frac{1}{k!}\mathbf{A}\mathbf{X}^{k}\right)=\sum_{k=1}^{\infty}\sum_{r=0}^{k-1}\left(\mathbf{X}^{r} \mathbf{A} \mathbf{X}^{k-r-1}\right)^{T} k=0 \frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A})\right)=\mathbf{O},"['multivariable-calculus', 'derivatives', 'matrix-calculus', 'matrix-exponential', 'scalar-fields']"
58,Determine where the Functions are Continous,Determine where the Functions are Continous,,"$$f(x,y)=\begin{cases}\dfrac{xy}{x^2+y^2}&(x,y)\neq(0,0)\\ 0 &(x,y)=(0,0)\end{cases} $$ I use graph tool to help me visualize this problem, and I think it is continuous for all the graph, but the answer is wrong. Can someone guide me how to solve this kind of question?","I use graph tool to help me visualize this problem, and I think it is continuous for all the graph, but the answer is wrong. Can someone guide me how to solve this kind of question?","f(x,y)=\begin{cases}\dfrac{xy}{x^2+y^2}&(x,y)\neq(0,0)\\
0 &(x,y)=(0,0)\end{cases} ","['multivariable-calculus', 'continuity']"
59,Choosing Bounds of Integration for a Triangle,Choosing Bounds of Integration for a Triangle,,"Suppose I have a region of integration boded by $x+y \le 8$ and $0 \le y \le x$ . I have graphed the bounds, and they form a right triangle with its hypotenuse following the $x$ -axis from $[0,8]$ and the right angle at $(4,4)$ . For clarity, the region I am looking at is the intersection of the red and green regions. I am having trouble figuring out what the bounds should be. I was thinking of splitting up the double integral into two double integrals. $\int^4_{0} \int^{y=x}_{y=0} f(x) dy dx + \int^8_{4} \int^{y=8-x}_{y=0} f(x) dy dx$ Is this correct?","Suppose I have a region of integration boded by and . I have graphed the bounds, and they form a right triangle with its hypotenuse following the -axis from and the right angle at . For clarity, the region I am looking at is the intersection of the red and green regions. I am having trouble figuring out what the bounds should be. I was thinking of splitting up the double integral into two double integrals. Is this correct?","x+y \le 8 0 \le y \le x x [0,8] (4,4) \int^4_{0} \int^{y=x}_{y=0} f(x) dy dx + \int^8_{4} \int^{y=8-x}_{y=0} f(x) dy dx","['integration', 'multivariable-calculus', 'definite-integrals', 'bounds-of-integration']"
60,A result in the solution of wave equation,A result in the solution of wave equation,,"Let $u$ be a smooth solution of the initial-value problem $$ \left\{ \begin{aligned} u_{tt} - u_{xx} &= 0\qquad \text{in}~ \mathbb{R}\times (0,\infty)\\ u=g,\quad u_t&=h\qquad \text{on}~\mathbb{R}\times \{t=0\} \end{aligned} \right. $$ Suppose that $g$ and $h$ are sufficiently smooth and have compact supports. \   Define $p(t) := \frac{1}{2} \int\limits_{-\infty}^\infty u_x(x,t)^2\,d x \quad\text{and}\quad k(t) := \frac{1}{2}\int\limits_{-\infty}^\infty u_t(x,t)^2\,d x\,.$ Prove that $E(t):= k(t) + p(t)$ is constant in $t\geq 0$ . Prove that $p(t)=k(t)$ for all large enough times $t$ . My Attempt: For the part 1, I was trying to prove that $\frac{d}{dt}E(t)=0$ . Hence we have, \begin{align} E^{'}(t)&=k^{'}(t)+p^{'}(t)\\ &=\int\limits_{-\infty}^\infty u_x.u_{xt}+\int\limits_{-\infty}^\infty u_t.u_{tt}\\ &=\int\limits_{-\infty}^\infty  u_x.u_{xt}+\int\limits_{-\infty}^\infty u_t.u_{xx} \end{align} But after that what should I do.. Also for part 2, I know that from the D'Alambert's Formula, we have $$u(x,t)=\frac{1}{2}[g(x+t)-g(x-t)]+\frac{1}{2}\int\limits_{x-t}^{x+t}h(y)dy \tag{1}$$ So I'm trying to prove that when $t\to\infty$ , $p(t)-k(t)=\frac{1}{2}\int\limits_{-\infty}^{\infty}u_x^2-u_t^2=0$ . But I'm having a hard time in differentiating $(1)$ to get the suitable values. Appreciate your help","Let be a smooth solution of the initial-value problem Suppose that and are sufficiently smooth and have compact supports. \   Define Prove that is constant in . Prove that for all large enough times . My Attempt: For the part 1, I was trying to prove that . Hence we have, But after that what should I do.. Also for part 2, I know that from the D'Alambert's Formula, we have So I'm trying to prove that when , . But I'm having a hard time in differentiating to get the suitable values. Appreciate your help","u 
\left\{
\begin{aligned}
u_{tt} - u_{xx} &= 0\qquad \text{in}~ \mathbb{R}\times (0,\infty)\\
u=g,\quad u_t&=h\qquad \text{on}~\mathbb{R}\times \{t=0\}
\end{aligned}
\right.
 g h p(t) := \frac{1}{2} \int\limits_{-\infty}^\infty u_x(x,t)^2\,d x \quad\text{and}\quad k(t) := \frac{1}{2}\int\limits_{-\infty}^\infty u_t(x,t)^2\,d x\,. E(t):= k(t) + p(t) t\geq 0 p(t)=k(t) t \frac{d}{dt}E(t)=0 \begin{align}
E^{'}(t)&=k^{'}(t)+p^{'}(t)\\
&=\int\limits_{-\infty}^\infty u_x.u_{xt}+\int\limits_{-\infty}^\infty u_t.u_{tt}\\
&=\int\limits_{-\infty}^\infty  u_x.u_{xt}+\int\limits_{-\infty}^\infty u_t.u_{xx}
\end{align} u(x,t)=\frac{1}{2}[g(x+t)-g(x-t)]+\frac{1}{2}\int\limits_{x-t}^{x+t}h(y)dy \tag{1} t\to\infty p(t)-k(t)=\frac{1}{2}\int\limits_{-\infty}^{\infty}u_x^2-u_t^2=0 (1)","['multivariable-calculus', 'partial-differential-equations']"
61,Evaluating a Triple Integral in Polar Coordinates,Evaluating a Triple Integral in Polar Coordinates,,"I'm trying to evaluate $\iiint_{E}\sqrt{3x^{2} + 3z^{2}}~dV$ where $E$ is the solid bounded by $y = 2x^{2} + 2z^{2}$ and $y=8$ . My thought was to covert this to polar coordinates using $x^{2} + z^{2} = r^{2}$ .  Then the solid would be a cone originating in the $xz$ -plane with radius of 0 which stretches along the $y$ -axis until it terminates as a circle of radius 2.  So I converted $\sqrt{3x^{2} + 3z^{2}}$ to $\sqrt{3r^{2}}$ to $r\sqrt{3}$ and set the integral up this way $$\int_{\theta = 0}^{\theta = 2\pi}\int_{r=0}^{r=2}\left(\int_{y=0}^{y=2r^{2}} r\sqrt{3} dy\right) r~dr~d\theta$$ which equals $\frac{128\sqrt{3}}{5}\pi$ .  But the correct answer is $\frac{256\sqrt{3}\pi}{15}$ , which means I'm off by a factor of $\frac{2}{3}$ . Any help figuring out what I'm doing wrong would be greatly appreciated!","I'm trying to evaluate where is the solid bounded by and . My thought was to covert this to polar coordinates using .  Then the solid would be a cone originating in the -plane with radius of 0 which stretches along the -axis until it terminates as a circle of radius 2.  So I converted to to and set the integral up this way which equals .  But the correct answer is , which means I'm off by a factor of . Any help figuring out what I'm doing wrong would be greatly appreciated!",\iiint_{E}\sqrt{3x^{2} + 3z^{2}}~dV E y = 2x^{2} + 2z^{2} y=8 x^{2} + z^{2} = r^{2} xz y \sqrt{3x^{2} + 3z^{2}} \sqrt{3r^{2}} r\sqrt{3} \int_{\theta = 0}^{\theta = 2\pi}\int_{r=0}^{r=2}\left(\int_{y=0}^{y=2r^{2}} r\sqrt{3} dy\right) r~dr~d\theta \frac{128\sqrt{3}}{5}\pi \frac{256\sqrt{3}\pi}{15} \frac{2}{3},['multivariable-calculus']
62,A generalization of L'Hospital's Rule,A generalization of L'Hospital's Rule,,"Let $\mathcal{O}\subset\mathbb{R}^n$ be open, let $f$ , $g:\mathcal{O}\to\mathbb{R}$ be twice continuously differentiable functions, and let $x_0\in\mathcal{O}$ . Suppose that $f(x_0)=g(x_0)=0$ and $\nabla f(x_0) = \nabla g(x_0) = 0$ . Suppose also that $\nabla^2 f(x_0) = \lambda\nabla^2 g(x_0)$ for some number $\lambda$ , and that $\nabla^2 g(x_0)$ is a positive definite Hessian matrix. Prove that $$\lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \lambda$$ Proof I'm tempted to use the second-order approximation of $f(x)$ and $g(x)$ , since that is the only place I can think of that the the hessians might appear. However, the approximation is given by $$f(x + h) \approx f(x) + \left<\nabla f(x), h\right> + \frac{1}{2}\left< \nabla^2 f(x)h,h\right>$$ I suppose I could say that $x_0 = x + h$ , so an approximation for $f(x_0)$ could work, since $f(x_0) = 0$ and $\nabla f(x_0) = 0$ . But I doubt that is valid for any $h\neq 0$ . I'll see what happens, anyway. The limit would then become, if I choose $x$ close enough to $x_0$ and $h$ small but nonzero: $$\lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \frac{\lim\limits_{x\to x_0} f(x)}{\lim\limits_{x\to x_0} g(x)} = \frac{\lim\limits_{h\to 0} \frac{1}{2} \left< \nabla^2 f(x_0 + h)h, h\right>}{\lim\limits_{h\to 0} \frac{1}{2} \left< \nabla^2 g(x_0 + h)h, h\right>} = \frac{\nabla^2 f(x_0)}{\nabla^2 g(x_0)} = \frac{\lambda \nabla^2 g(x_0)}{\nabla^2 g(x_0)} = \lambda$$ This feels both extremely sketchy and sort of on the right track.","Let be open, let , be twice continuously differentiable functions, and let . Suppose that and . Suppose also that for some number , and that is a positive definite Hessian matrix. Prove that Proof I'm tempted to use the second-order approximation of and , since that is the only place I can think of that the the hessians might appear. However, the approximation is given by I suppose I could say that , so an approximation for could work, since and . But I doubt that is valid for any . I'll see what happens, anyway. The limit would then become, if I choose close enough to and small but nonzero: This feels both extremely sketchy and sort of on the right track.","\mathcal{O}\subset\mathbb{R}^n f g:\mathcal{O}\to\mathbb{R} x_0\in\mathcal{O} f(x_0)=g(x_0)=0 \nabla f(x_0) = \nabla g(x_0) = 0 \nabla^2 f(x_0) = \lambda\nabla^2 g(x_0) \lambda \nabla^2 g(x_0) \lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \lambda f(x) g(x) f(x + h) \approx f(x) + \left<\nabla f(x), h\right> + \frac{1}{2}\left< \nabla^2 f(x)h,h\right> x_0 = x + h f(x_0) f(x_0) = 0 \nabla f(x_0) = 0 h\neq 0 x x_0 h \lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \frac{\lim\limits_{x\to x_0} f(x)}{\lim\limits_{x\to x_0} g(x)} = \frac{\lim\limits_{h\to 0} \frac{1}{2} \left< \nabla^2 f(x_0 + h)h, h\right>}{\lim\limits_{h\to 0} \frac{1}{2} \left< \nabla^2 g(x_0 + h)h, h\right>} = \frac{\nabla^2 f(x_0)}{\nabla^2 g(x_0)} = \frac{\lambda \nabla^2 g(x_0)}{\nabla^2 g(x_0)} = \lambda","['real-analysis', 'multivariable-calculus', 'proof-writing', 'solution-verification']"
63,Hessian in second-order Taylor approximation,Hessian in second-order Taylor approximation,,"The second order Taylor approximation for a function $f(\mathbf{x}^*)$ is presented to me as $$f(\mathbf{x}^* + h\mathbf{y}) = f(\mathbf{x}^*) + h \nabla f(\mathbf{x}^*)^T \mathbf{y} + \dfrac{1}{2} h^2 \mathbf{y}^T \nabla^2 f(\mathbf{x}^*) \mathbf{y} + O(h^3)$$ But from what I understand, shouldn't there be a Hessian matrix in the third term? Or is the Laplacian in the third term somehow the Hessian? I would greatly appreciate it if people would please take the time to clarify this.","The second order Taylor approximation for a function is presented to me as But from what I understand, shouldn't there be a Hessian matrix in the third term? Or is the Laplacian in the third term somehow the Hessian? I would greatly appreciate it if people would please take the time to clarify this.",f(\mathbf{x}^*) f(\mathbf{x}^* + h\mathbf{y}) = f(\mathbf{x}^*) + h \nabla f(\mathbf{x}^*)^T \mathbf{y} + \dfrac{1}{2} h^2 \mathbf{y}^T \nabla^2 f(\mathbf{x}^*) \mathbf{y} + O(h^3),"['multivariable-calculus', 'optimization', 'taylor-expansion', 'laplacian', 'hessian-matrix']"
64,"Prove that $|u(x,t)|\leq\sup\limits_{y\in R^n}|g(y)|$",Prove that,"|u(x,t)|\leq\sup\limits_{y\in R^n}|g(y)|","Let $$u(x,t)=\int\limits_{\mathbb{R}^n}\Phi(x-y,t)g(y)dy$$ where $$\Phi(x-y,t)=\frac{1}{(4\pi t)^{n/2}}e^{\frac{-|x-y|}{4t}}$$ (this is the solution for the homogeneous heat equation), where $g$ is a continuous function with $\lim\limits_{|x|\to\infty}g(x)=0$ . Prove that $$|u(x,t)|\leq\sup\limits_{y\in \mathbb{R}^n}|g(y)|$$ for all $x\in \mathbb{R}^n, t>0$ . My Attempt: For a fixed $x,t$ , we have $$e^{\frac{-|x-y|}{4t}}\leq 1$$ Hence \begin{aligned} \lvert u(x,t)\rvert &\leq\int\limits_{\mathbb{R}^n} \lvert \Phi(x-y)g(y) \rvert dy \\         &=\int\limits_{\mathbb{R}^n}|\Phi(x-y)||g(y)|dy\\         &\leq\frac{1}{(4\pi t)^{n/2}} \int\limits_{\mathbb{R}^n}|g(y)|dy\\         &\leq \sup\limits_{y\in \mathbb{R}^n}|g(y)|\frac{1}{(4\pi t)^{n/2}}\int\limits_{\mathbb{R}^n}dy \end{aligned} But after that how can I get rid of the integral?","Let where (this is the solution for the homogeneous heat equation), where is a continuous function with . Prove that for all . My Attempt: For a fixed , we have Hence But after that how can I get rid of the integral?","u(x,t)=\int\limits_{\mathbb{R}^n}\Phi(x-y,t)g(y)dy \Phi(x-y,t)=\frac{1}{(4\pi t)^{n/2}}e^{\frac{-|x-y|}{4t}} g \lim\limits_{|x|\to\infty}g(x)=0 |u(x,t)|\leq\sup\limits_{y\in \mathbb{R}^n}|g(y)| x\in \mathbb{R}^n, t>0 x,t e^{\frac{-|x-y|}{4t}}\leq 1 \begin{aligned}
\lvert u(x,t)\rvert &\leq\int\limits_{\mathbb{R}^n} \lvert \Phi(x-y)g(y) \rvert dy \\
        &=\int\limits_{\mathbb{R}^n}|\Phi(x-y)||g(y)|dy\\
        &\leq\frac{1}{(4\pi t)^{n/2}} \int\limits_{\mathbb{R}^n}|g(y)|dy\\
        &\leq \sup\limits_{y\in \mathbb{R}^n}|g(y)|\frac{1}{(4\pi t)^{n/2}}\int\limits_{\mathbb{R}^n}dy
\end{aligned}","['real-analysis', 'calculus', 'multivariable-calculus', 'partial-differential-equations']"
65,differentiability and continuity of multivariable functions,differentiability and continuity of multivariable functions,,"I'm trying to understand differentiability of multivariable functions. The textbook says, ""If the partial derivatives ƒx and ƒy of a function ƒ(x, y) are continuous throughout an open region R, then ƒ is differentiable at every point of R."" Hass, Joel R.; Heil, Christopher E.; Weir, Maurice D.. Thomas' Calculus (Page 818). Pearson Education. Kindle Edition. So in two dimensions, if something is continuous, it might not be differentiable, because it could be pointy (that's an official math term, right?) Couldn't that happen in three dimensions too? Also, I was wondering whether the converse of the above is true - i.e. if a multivariable function is differentiable, that means it's continuous and that the partial derivatives exist. And if not, what's the counterexample? Thank you!","I'm trying to understand differentiability of multivariable functions. The textbook says, ""If the partial derivatives ƒx and ƒy of a function ƒ(x, y) are continuous throughout an open region R, then ƒ is differentiable at every point of R."" Hass, Joel R.; Heil, Christopher E.; Weir, Maurice D.. Thomas' Calculus (Page 818). Pearson Education. Kindle Edition. So in two dimensions, if something is continuous, it might not be differentiable, because it could be pointy (that's an official math term, right?) Couldn't that happen in three dimensions too? Also, I was wondering whether the converse of the above is true - i.e. if a multivariable function is differentiable, that means it's continuous and that the partial derivatives exist. And if not, what's the counterexample? Thank you!",,"['multivariable-calculus', 'derivatives', 'continuity']"
66,"Two continuous functions over a closed, bounded and Jordan-Measurable set: Show that its graph Jordan-Measurable","Two continuous functions over a closed, bounded and Jordan-Measurable set: Show that its graph Jordan-Measurable",,"Let f,g: $A\subset\mathbb{R}^{n}\mapsto\mathbb{R}$ continuous functions over the Jordan-measurable, bounded and closed set A, such that $f(x)\leq g(x) \forall x\in A$ Show that $B=\{(x,y)\in\mathbb{R}^{n+1}|f(x)\leq y \leq g(x)\}$ is Jordan-measurable in $\mathbb{R}^{n+1}$ I have problems with the proof. I don't know how to complete this but we know that if the function is continuous, then f is uniformly continuous, then for every $\epsilon>0$ there is a $\delta>0$ such that if $|x-y|<\delta \implies |f(x)-f(y)<\epsilon|$ .Then maybe we can cover the graph with a finite number of rectangles but, to use this argument I must prove that the graph is continuous I guess... I will appreciate any help to end this proof because I'm a little lost.","Let f,g: continuous functions over the Jordan-measurable, bounded and closed set A, such that Show that is Jordan-measurable in I have problems with the proof. I don't know how to complete this but we know that if the function is continuous, then f is uniformly continuous, then for every there is a such that if .Then maybe we can cover the graph with a finite number of rectangles but, to use this argument I must prove that the graph is continuous I guess... I will appreciate any help to end this proof because I'm a little lost.","A\subset\mathbb{R}^{n}\mapsto\mathbb{R} f(x)\leq g(x) \forall x\in A B=\{(x,y)\in\mathbb{R}^{n+1}|f(x)\leq y \leq g(x)\} \mathbb{R}^{n+1} \epsilon>0 \delta>0 |x-y|<\delta \implies |f(x)-f(y)<\epsilon|","['real-analysis', 'measure-theory', 'multivariable-calculus']"
67,For what values of $a\in \mathbb{R}$ is the integral $\int\int_{\mathbb{R}^2} \frac{1}{(1+x^4+y^4)^a}dxdy$ convergent?,For what values of  is the integral  convergent?,a\in \mathbb{R} \int\int_{\mathbb{R}^2} \frac{1}{(1+x^4+y^4)^a}dxdy,"Well, first notice that the above integral equals to $2\int_{0}^\infty \int_0^\infty \frac{1}{(1+x^4+y^4)^a}dxdy$ since the integrand is an even function of both $x$ and $y$ . There are two approaches which I thought to do here. Change of variables: $x^2 = r\cos \theta , y^2 = r\sin \theta$ ; which I am not sure it's valid since there are value of theta which give the rhs to be negative, and we are in real calculus. The legitimate approach is to break the integral into: $$\int_0^1 \int_0^1+\int_0^1\int_1^\infty + \int_1^\infty\int_0^1+\int_1^\infty \int_1^\infty$$ I thought to compare the integrand with $x^2+y^2$ , i.e when $x,y \in [0,1]$ we know that $x^2 \ge x^4$ and when $x>1$ then the opposite follows. Seems a bit long calculation. Can anyone help me with this? Thanks!","Well, first notice that the above integral equals to since the integrand is an even function of both and . There are two approaches which I thought to do here. Change of variables: ; which I am not sure it's valid since there are value of theta which give the rhs to be negative, and we are in real calculus. The legitimate approach is to break the integral into: I thought to compare the integrand with , i.e when we know that and when then the opposite follows. Seems a bit long calculation. Can anyone help me with this? Thanks!","2\int_{0}^\infty \int_0^\infty \frac{1}{(1+x^4+y^4)^a}dxdy x y x^2 = r\cos \theta , y^2 = r\sin \theta \int_0^1 \int_0^1+\int_0^1\int_1^\infty + \int_1^\infty\int_0^1+\int_1^\infty \int_1^\infty x^2+y^2 x,y \in [0,1] x^2 \ge x^4 x>1","['integration', 'multivariable-calculus', 'improper-integrals']"
68,"Let $A=[0,1]\times[0,1], f:A\mapsto \mathbb{R} $ bounded in A. Then the set $D_{f,A}$ of discontinuities of $f$ in $A$ is closed?",Let  bounded in A. Then the set  of discontinuities of  in  is closed?,"A=[0,1]\times[0,1], f:A\mapsto \mathbb{R}  D_{f,A} f A","I think that the statement is true, but I don't know how to start yet. Could anyone help me or give me a hint? I will really appreciate that!","I think that the statement is true, but I don't know how to start yet. Could anyone help me or give me a hint? I will really appreciate that!",,"['real-analysis', 'integration', 'general-topology', 'multivariable-calculus', 'measurable-sets']"
69,"Show that the Volume of the set $A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\}$ is $1$",Show that the Volume of the set  is,"A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\} 1","QUESTION Show that the Volume of the set $A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\}$ is $1$ OR Let $f: \mathbb{R} \rightarrow [0,\infty)$ a density function and Let $A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\}$ show that $Vol(A)=1$ I have no idea about how the Volume of a set works, It would be great if anyone can help me. This is the definition of $Vol(A)$ I'm working with $$Vol(A)=\int\cdots\int \textbf 1_A(x_1,\ldots,x_n)dx_1\ldots dx_n$$","QUESTION Show that the Volume of the set is OR Let a density function and Let show that I have no idea about how the Volume of a set works, It would be great if anyone can help me. This is the definition of I'm working with","A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\} 1 f: \mathbb{R} \rightarrow [0,\infty) A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\} Vol(A)=1 Vol(A) Vol(A)=\int\cdots\int \textbf 1_A(x_1,\ldots,x_n)dx_1\ldots dx_n","['calculus', 'probability', 'probability-theory', 'measure-theory', 'multivariable-calculus']"
70,"Is the map $re^{i\theta} \to re^{i\alpha\theta}$ in $W^{1,2}$?",Is the map  in ?,"re^{i\theta} \to re^{i\alpha\theta} W^{1,2}","Let $\alpha>1$ be a real number, and consider the map $F$ defined on the closed unit two-dimensional disk $D \subseteq \mathbb{R}^2$ after removing a ray, by $re^{i\theta} \to re^{i\alpha\theta}$ . Does $F \in W^{1,2}(D,\mathbb{R}^2)$ ? When $\alpha=n$ is a positive integer, I proved that the answer is positive (see below, I hope I don't have a mistake). I am not sure what to do when $\alpha \notin \mathbb{N}$ . The main difference is that when $\alpha=n$ is a positive integer, we have that $F(z)=F(re^{i\theta})= re^{in\theta}=\frac{z^n}{|z|^{n-1}}$ is defined and smooth on the entire disk without the origin $D \setminus \{0\}$ . However, when $\alpha \notin \mathbb{N}$ , we have to use the complex logarithm in order to define $F$ , so (I think) we only get a well-defined map which smooth on the disk with a ray removed from it. Here is the computation that shows that $F \in W^{1,2}$ when $\alpha=n$ is an integer: $F(z)=\frac{z^n}{|z|^{n-1}}=\big( \frac{z}{|z|^{\frac{n-1}{n}}}\big)^n$ is the $n$ -th power of a continuous bounded function defined on the entire disk, and smooth on $D \setminus \{0\}$ . Thus, it suffices to show that $z \to \frac{z}{|z|^{\frac{n-1}{n}}}$ has a derivative which is in $L^2$ . Writing $\beta:=\frac{n-1}{n}$ , we need to differentiate $g(z)=\frac{z}{|z|^\beta}=(\frac{x}{(x^2+y^2)^{\frac{\beta}{2}}},\frac{y}{(x^2+y^2)^{\frac{\beta}{2}}})$ . By symmetry, it suffices to check $G(x,y)=\frac{x}{(x^2+y^2)^{\frac{\beta}{2}}}$ . $G_x=(x^2+y^2)^{-\frac{\beta}{2}}[1-\beta \frac{x^2}{x^2+y^2}]$ , so (since $0<\beta<1$ ), we get $|G_x| \le (x^2+y^2)^{-\frac{\beta}{2}}=r^{-\beta}$ . Thus $|G_x|^2 \le r^{-2\beta}$ , so $|G_x|^2 \in L^1$ iff $ \int_0^1 r^{-2\beta} rdr < \infty$ . Since $-1<1-2\beta=\frac{2-n}{n} \le 0$ , this holds. Similarly, $|G_y|=|(x^2+y^2)^{-\frac{\beta}{2}}\frac{2xy}{x^2+y^2}| \le (x^2+y^2)^{-\frac{\beta}{2}}$ , so we are done.","Let be a real number, and consider the map defined on the closed unit two-dimensional disk after removing a ray, by . Does ? When is a positive integer, I proved that the answer is positive (see below, I hope I don't have a mistake). I am not sure what to do when . The main difference is that when is a positive integer, we have that is defined and smooth on the entire disk without the origin . However, when , we have to use the complex logarithm in order to define , so (I think) we only get a well-defined map which smooth on the disk with a ray removed from it. Here is the computation that shows that when is an integer: is the -th power of a continuous bounded function defined on the entire disk, and smooth on . Thus, it suffices to show that has a derivative which is in . Writing , we need to differentiate . By symmetry, it suffices to check . , so (since ), we get . Thus , so iff . Since , this holds. Similarly, , so we are done.","\alpha>1 F D \subseteq \mathbb{R}^2 re^{i\theta} \to re^{i\alpha\theta} F \in W^{1,2}(D,\mathbb{R}^2) \alpha=n \alpha \notin \mathbb{N} \alpha=n F(z)=F(re^{i\theta})= re^{in\theta}=\frac{z^n}{|z|^{n-1}} D \setminus \{0\} \alpha \notin \mathbb{N} F F \in W^{1,2} \alpha=n F(z)=\frac{z^n}{|z|^{n-1}}=\big( \frac{z}{|z|^{\frac{n-1}{n}}}\big)^n n D \setminus \{0\} z \to \frac{z}{|z|^{\frac{n-1}{n}}} L^2 \beta:=\frac{n-1}{n} g(z)=\frac{z}{|z|^\beta}=(\frac{x}{(x^2+y^2)^{\frac{\beta}{2}}},\frac{y}{(x^2+y^2)^{\frac{\beta}{2}}}) G(x,y)=\frac{x}{(x^2+y^2)^{\frac{\beta}{2}}} G_x=(x^2+y^2)^{-\frac{\beta}{2}}[1-\beta \frac{x^2}{x^2+y^2}] 0<\beta<1 |G_x| \le (x^2+y^2)^{-\frac{\beta}{2}}=r^{-\beta} |G_x|^2 \le r^{-2\beta} |G_x|^2 \in L^1  \int_0^1 r^{-2\beta} rdr < \infty -1<1-2\beta=\frac{2-n}{n} \le 0 |G_y|=|(x^2+y^2)^{-\frac{\beta}{2}}\frac{2xy}{x^2+y^2}| \le (x^2+y^2)^{-\frac{\beta}{2}}","['real-analysis', 'complex-analysis', 'multivariable-calculus', 'sobolev-spaces', 'distribution-theory']"
71,Subharmonicity implies mean-value inequality,Subharmonicity implies mean-value inequality,,"If we take as definition that if $\Omega \subseteq \Bbb R^n$ is open and $u \in \mathscr{C}^2(\Omega)$ is subharmonic if $\triangle u \geq 0$ , then for every $x \in \Omega$ and small enough $r>0$ we have the inequality $$u(x) \leq \frac{1}{|B_r(x)|}\int_{B_r(x)} u(y)\,{\rm d}y,$$ where $|B_r(x)|$ denotes the volume of the ball. There's a proof for the harmonic case in Chapter 1 of Evans which is easy to follow and adapt, and he would first prove that $$u(x) \leq \frac{1}{|\partial B_r(x)|}\int_{\partial B_r(x)} u(y)\,{\rm d}S(y),$$ to then integrate using polar coordinates and get the result. This is all fine. I tried to give a direct proof avoiding the surface integral and got something absurd. Fix $x \in \Omega$ and define $\varphi\colon [0,\epsilon) \to \Bbb R$ (for some maximal $\epsilon>0$ which exists due to openness of $\Omega$ ) by $$\varphi(r) = \frac{1}{|B_r(x)|}\int_{B_r(x)} u(y)\,{\rm d}y.$$ Lebesgue's differentiation theorem says that $$ \varphi(0) = \lim_{r\to 0^+} \varphi(r) = u(x),$$ so our goal is to show that $\varphi'(r)\geq 0$ (as the desired conclusion is just $\varphi(0) \leq \varphi(r)$ for $r>0$ ). And this is done as follows: first we rewrite $\varphi(r)$ in a more convenient fashion, using that $|B_r(x)| = r^n|B_1(0)|$ and setting $y = x+rz$ with $z \in B_1(0)$ , so that ${\rm d}{y} = r^n \,{\rm d}{z}$ and $$  \varphi(r) = \frac{1}{|B_1(0)|} \int_{B_1(0)} u(x+rz)\,{\rm d}{z}.  $$ So differentiating under the integral (allowed by smoothness of $u$ and compactness of $B_1(0)$ ) gives $$  \varphi'(r) = \frac{1}{|B_1(0)|} \int_{B_1(0)} \langle \nabla u(x+rz),z\rangle\,{\rm d}{z}.    $$ To apply Green's identities, let $v(z) = u(x+rz)$ and $q(z) = \langle z,z\rangle/2$ , so that $\nabla v(z) = r\nabla u(x+rz)$ and $\nabla q(z) = z$ . Also $\triangle v(z) = r^2\triangle u(x+rz)$ . So \begin{align*}   \varphi'(r) &= \frac{1}{r|B_1(0)|}\int_{B_1(0)} \langle \nabla v(z),\nabla q(z)\rangle\,{\rm d}{z} \\ &= \frac{1}{r|B_1(0)|}\left( -\int_{B_1(0)} q(z)\triangle v(z)\,{\rm d}{z} + \int_{\partial B_1(0)} q(z)\frac{\partial v}{\partial \nu}(z)\,{\rm d}{S}(z)\right) \\  &= -\frac{r}{2|B_1(0)|} \int_{B_1(0)} \|z\|^2 \triangle u(x+rz)\,{\rm d}{z} \color{red}{\leq 0} \end{align*} I cannot find the mistake. What's the screw up?","If we take as definition that if is open and is subharmonic if , then for every and small enough we have the inequality where denotes the volume of the ball. There's a proof for the harmonic case in Chapter 1 of Evans which is easy to follow and adapt, and he would first prove that to then integrate using polar coordinates and get the result. This is all fine. I tried to give a direct proof avoiding the surface integral and got something absurd. Fix and define (for some maximal which exists due to openness of ) by Lebesgue's differentiation theorem says that so our goal is to show that (as the desired conclusion is just for ). And this is done as follows: first we rewrite in a more convenient fashion, using that and setting with , so that and So differentiating under the integral (allowed by smoothness of and compactness of ) gives To apply Green's identities, let and , so that and . Also . So I cannot find the mistake. What's the screw up?","\Omega \subseteq \Bbb R^n u \in \mathscr{C}^2(\Omega) \triangle u \geq 0 x \in \Omega r>0 u(x) \leq \frac{1}{|B_r(x)|}\int_{B_r(x)} u(y)\,{\rm d}y, |B_r(x)| u(x) \leq \frac{1}{|\partial B_r(x)|}\int_{\partial B_r(x)} u(y)\,{\rm d}S(y), x \in \Omega \varphi\colon [0,\epsilon) \to \Bbb R \epsilon>0 \Omega \varphi(r) = \frac{1}{|B_r(x)|}\int_{B_r(x)} u(y)\,{\rm d}y.  \varphi(0) = \lim_{r\to 0^+} \varphi(r) = u(x), \varphi'(r)\geq 0 \varphi(0) \leq \varphi(r) r>0 \varphi(r) |B_r(x)| = r^n|B_1(0)| y = x+rz z \in B_1(0) {\rm d}{y} = r^n \,{\rm d}{z}   \varphi(r) = \frac{1}{|B_1(0)|} \int_{B_1(0)} u(x+rz)\,{\rm d}{z}.   u B_1(0)   \varphi'(r) = \frac{1}{|B_1(0)|} \int_{B_1(0)} \langle \nabla u(x+rz),z\rangle\,{\rm d}{z}.     v(z) = u(x+rz) q(z) = \langle z,z\rangle/2 \nabla v(z) = r\nabla u(x+rz) \nabla q(z) = z \triangle v(z) = r^2\triangle u(x+rz) \begin{align*}
  \varphi'(r) &= \frac{1}{r|B_1(0)|}\int_{B_1(0)} \langle \nabla v(z),\nabla q(z)\rangle\,{\rm d}{z} \\ &= \frac{1}{r|B_1(0)|}\left( -\int_{B_1(0)} q(z)\triangle v(z)\,{\rm d}{z} + \int_{\partial B_1(0)} q(z)\frac{\partial v}{\partial \nu}(z)\,{\rm d}{S}(z)\right) \\  &= -\frac{r}{2|B_1(0)|} \int_{B_1(0)} \|z\|^2 \triangle u(x+rz)\,{\rm d}{z} \color{red}{\leq 0}
\end{align*}","['integration', 'multivariable-calculus', 'partial-differential-equations', 'solution-verification', 'harmonic-functions']"
72,"Solutions to PDE $\langle\nabla\psi,\nabla\psi+\mathbf{f} \rangle= 0$",Solutions to PDE,"\langle\nabla\psi,\nabla\psi+\mathbf{f} \rangle= 0","Consider the vector field $$ \mathbf{f}(x_1,x_2)=\begin{bmatrix}f_1(x_1,x_2)\\ f_2(x_1,x_2)\end{bmatrix}=\begin{bmatrix}\frac{1}{1+x_2}+x_1\\ \frac{1}{1+x_1}+x_2\end{bmatrix}. $$ I'm interested in finding a scalar function $\psi(x_1,x_2)$ such that: $$\tag{$\ast$}\label{ast} \langle\nabla\psi,\nabla\psi+\mathbf{f} \rangle= \frac{\partial\psi(x_1,x_2)}{\partial x_1} \left(f_1(x_1,x_2)+\frac{\partial\psi(x_1,x_2)}{\partial x_1}\right)+\frac{\partial\psi(x_1,x_2)}{\partial x_2} \left(f_2(x_1,x_2)+\frac{\partial\psi(x_1,x_2)}{\partial x_2}\right)=0 $$ My question. Does there exist a solution $\psi(x_1,x_2)$ to \eqref{ast}? If so, how to compute it? I'm aware that my question could be a trivial or naive one, but since I'm new to this kind of problems I would really appreciate any help/comment/suggestion. Thank you.","Consider the vector field I'm interested in finding a scalar function such that: My question. Does there exist a solution to \eqref{ast}? If so, how to compute it? I'm aware that my question could be a trivial or naive one, but since I'm new to this kind of problems I would really appreciate any help/comment/suggestion. Thank you.","
\mathbf{f}(x_1,x_2)=\begin{bmatrix}f_1(x_1,x_2)\\ f_2(x_1,x_2)\end{bmatrix}=\begin{bmatrix}\frac{1}{1+x_2}+x_1\\ \frac{1}{1+x_1}+x_2\end{bmatrix}.
 \psi(x_1,x_2) \tag{\ast}\label{ast}
\langle\nabla\psi,\nabla\psi+\mathbf{f} \rangle= \frac{\partial\psi(x_1,x_2)}{\partial x_1} \left(f_1(x_1,x_2)+\frac{\partial\psi(x_1,x_2)}{\partial x_1}\right)+\frac{\partial\psi(x_1,x_2)}{\partial x_2} \left(f_2(x_1,x_2)+\frac{\partial\psi(x_1,x_2)}{\partial x_2}\right)=0
 \psi(x_1,x_2)","['multivariable-calculus', 'partial-differential-equations', 'numerical-methods', 'hamilton-jacobi-equation']"
73,Calculating the normal vector to a space curve to construct a 3D plot,Calculating the normal vector to a space curve to construct a 3D plot,,"In trying to follow the calculation of the normal curvature of a curve $C$ at a point $P,$ using Geogebra and a somewhat random example, I got stuck. Here is what I did: The surface $S$ was set up as with domain boundaries for $-1<x<1$ and $-1<y<1$ as: $$f(x,y)=-x^2+\cos(x)+\cos(y)$$ The space curve $C\in \mathbb R^3$ was parametrized by $t$ with $-1<t<1$ as: $$C(t)=(t,t^2,f(x,y))$$ With $x=t$ and $y=t^2.$ The normal vector to the surface $\vec N$ was calculated as: $$\vec N(t)=\left (-\frac{\partial f}{\partial x}\frac{\partial x}{\partial t} ,-\frac{\partial f}{\partial y}\frac{\partial y}{\partial t},1\right)=\left(2t+\sin(t),\sin(t^2),1\right)$$ The tangent vector at point $P$ ( $\vec T\in T_PS)$ was calculated as $$\vec T(t) =\frac{d}{d t}C(t) =\left(1,2t,-2t-\sin(t)-2t\sin(t^2)\right)$$ This seemed to result in a plausible plot: However, the calculation of the normal vector to $C$ at $P$ was not so happy: $$\vec n=\frac{\vec T'}{\vert T\vert}=\frac{(0,2,-2\sin(t^2)-4t^2\cos(t^2)-2-\cos(t))}{\vert T\vert}$$ which would yield something like this (black arrow): clearly not orthogonal to $\vec T.$ Where did I go wrong in this calculation of the normal vector of $C$ at $P$ ? I would like to calculate this normal vector to the curve by differentiation; however, the only way I have been able to produce some plausible plot is by first calculating the binormal vector : $$\vec B=\frac{T\wedge T'}{|T\wedge T'|}$$ to later do the cross product of $\vec T$ with $\vec B:$","In trying to follow the calculation of the normal curvature of a curve at a point using Geogebra and a somewhat random example, I got stuck. Here is what I did: The surface was set up as with domain boundaries for and as: The space curve was parametrized by with as: With and The normal vector to the surface was calculated as: The tangent vector at point ( was calculated as This seemed to result in a plausible plot: However, the calculation of the normal vector to at was not so happy: which would yield something like this (black arrow): clearly not orthogonal to Where did I go wrong in this calculation of the normal vector of at ? I would like to calculate this normal vector to the curve by differentiation; however, the only way I have been able to produce some plausible plot is by first calculating the binormal vector : to later do the cross product of with","C P, S -1<x<1 -1<y<1 f(x,y)=-x^2+\cos(x)+\cos(y) C\in \mathbb R^3 t -1<t<1 C(t)=(t,t^2,f(x,y)) x=t y=t^2. \vec N \vec N(t)=\left (-\frac{\partial f}{\partial x}\frac{\partial x}{\partial t} ,-\frac{\partial f}{\partial y}\frac{\partial y}{\partial t},1\right)=\left(2t+\sin(t),\sin(t^2),1\right) P \vec T\in T_PS) \vec T(t) =\frac{d}{d t}C(t) =\left(1,2t,-2t-\sin(t)-2t\sin(t^2)\right) C P \vec n=\frac{\vec T'}{\vert T\vert}=\frac{(0,2,-2\sin(t^2)-4t^2\cos(t^2)-2-\cos(t))}{\vert T\vert} \vec T. C P \vec B=\frac{T\wedge T'}{|T\wedge T'|} \vec T \vec B:","['multivariable-calculus', 'differential-geometry']"
74,Multivariable Calculus: Equation satisfied by partial derivatives implies existence of a function,Multivariable Calculus: Equation satisfied by partial derivatives implies existence of a function,,"Im stuck in this question: Suppose $f:\mathbb{R}^{2}\setminus\{0\} \to \mathbb{R}$ is a differentiable function whose gradient is nowhere zero and that satisfies: $$-y\dfrac{\partial f}{\partial x} + x\dfrac{\partial f}{\partial y}=0$$ everywhere. Show that there is a differentiable function $F:(0,+\infty)\to \mathbb{R}$ so that $f(x)=F(||x||)$ . Geometrically, if the partial derivatives of $f$ satisfies the given condition, I get that the gradient vector in any point $(x,y)$ must be orthogonal to the vector $(-y,x)$ , which is the rotation of $90$ degrees counterclokwise of $(x,y)$ . So, $\nabla f \,(x,y)$ will be a scalar multiple of $(x,y)$ . Anyhow, this isn't helping me out to construct the desired function $F$ . Any tips?","Im stuck in this question: Suppose is a differentiable function whose gradient is nowhere zero and that satisfies: everywhere. Show that there is a differentiable function so that . Geometrically, if the partial derivatives of satisfies the given condition, I get that the gradient vector in any point must be orthogonal to the vector , which is the rotation of degrees counterclokwise of . So, will be a scalar multiple of . Anyhow, this isn't helping me out to construct the desired function . Any tips?","f:\mathbb{R}^{2}\setminus\{0\} \to \mathbb{R} -y\dfrac{\partial f}{\partial x} + x\dfrac{\partial f}{\partial y}=0 F:(0,+\infty)\to \mathbb{R} f(x)=F(||x||) f (x,y) (-y,x) 90 (x,y) \nabla f \,(x,y) (x,y) F","['real-analysis', 'calculus', 'multivariable-calculus']"
75,show this inequality with the sum $\sum_{i=1}^{n}x_{i}=n$,show this inequality with the sum,\sum_{i=1}^{n}x_{i}=n,"if $n>20$ ,I have prove let $x_{i}>0,i=1,2,\cdots,n$ ,and such $$x_{1}+x_{2}+\cdots+x_{n}=n$$ show that $$F(x_{1},x_{2},\cdots,x_{n})=\left(\prod_{i=1}^{n}x_{i}\right)\cdot (\sum_{i=1}^{n}x^3_{i})\le n$$ and find all  other postive integers $3\le n\le 19$ I want use $$F\left(\dfrac{x_{1}+x_{2}}{2},\dfrac{x_{1}+x_{2}}{2},x_{3},\cdots,x_{n}\right)-F(x_{1},x_{2},\cdots,x_{n})=x_{3}x_{4}\cdots x_{n}\left(\dfrac{(x_{1}+x_{2})^2}{4}\left(\dfrac{(x_{1}+x_{2})^3}{4}+\sum_{i=3}^{n}x^3_{i}\right)-x_{1}x_{2}\sum_{i=1}^nx^3_{i}\right)$$ if we show that $$\left(\dfrac{(x_{1}+x_{2})^2}{4}\left(\dfrac{(x_{1}+x_{2})^3}{4}+\sum_{i=3}^{n}x^3_{i}\right)-x_{1}x_{2}\sum_{i=1}^nx^3_{i}\right)\ge0$$ or $$\dfrac{(x_{1}+x_{2})^5}{16}-x_{1}x_{2}(x^3_{1}+x^3_{2})+\sum_{i=3}^{n}x^3_{i}\left(\dfrac{(x_{1}+x_{2})^2}{4}-x_{1}x_{2}\right)\ge0$$ or $$\dfrac{(x_{1}-x_{2})^2}{4}\left(\dfrac{1}{4}(x_{1}+x_{2})(x^2_{1}-10x_{1}x_{2}+x^2_{2})+\sum_{i=3}^{n}x^3_{i}\right)\ge 0$$","if ,I have prove let ,and such show that and find all  other postive integers I want use if we show that or or","n>20 x_{i}>0,i=1,2,\cdots,n x_{1}+x_{2}+\cdots+x_{n}=n F(x_{1},x_{2},\cdots,x_{n})=\left(\prod_{i=1}^{n}x_{i}\right)\cdot (\sum_{i=1}^{n}x^3_{i})\le n 3\le n\le 19 F\left(\dfrac{x_{1}+x_{2}}{2},\dfrac{x_{1}+x_{2}}{2},x_{3},\cdots,x_{n}\right)-F(x_{1},x_{2},\cdots,x_{n})=x_{3}x_{4}\cdots x_{n}\left(\dfrac{(x_{1}+x_{2})^2}{4}\left(\dfrac{(x_{1}+x_{2})^3}{4}+\sum_{i=3}^{n}x^3_{i}\right)-x_{1}x_{2}\sum_{i=1}^nx^3_{i}\right) \left(\dfrac{(x_{1}+x_{2})^2}{4}\left(\dfrac{(x_{1}+x_{2})^3}{4}+\sum_{i=3}^{n}x^3_{i}\right)-x_{1}x_{2}\sum_{i=1}^nx^3_{i}\right)\ge0 \dfrac{(x_{1}+x_{2})^5}{16}-x_{1}x_{2}(x^3_{1}+x^3_{2})+\sum_{i=3}^{n}x^3_{i}\left(\dfrac{(x_{1}+x_{2})^2}{4}-x_{1}x_{2}\right)\ge0 \dfrac{(x_{1}-x_{2})^2}{4}\left(\dfrac{1}{4}(x_{1}+x_{2})(x^2_{1}-10x_{1}x_{2}+x^2_{2})+\sum_{i=3}^{n}x^3_{i}\right)\ge 0","['multivariable-calculus', 'inequality', 'symmetric-polynomials']"
76,How to find the directional derivatives of these two functions?,How to find the directional derivatives of these two functions?,,"So I've been given these two functions, as well as the points and vectors. I am supposed to find the directional derivatives of said functions at the point in the direction of the given vector. The formula I have received to do so is included at the bottom of the photo. Could anyone tell me why I have gotten these questions wrong, and how I can change my work or anything to fix these?","So I've been given these two functions, as well as the points and vectors. I am supposed to find the directional derivatives of said functions at the point in the direction of the given vector. The formula I have received to do so is included at the bottom of the photo. Could anyone tell me why I have gotten these questions wrong, and how I can change my work or anything to fix these?",,"['calculus', 'multivariable-calculus', 'derivatives']"
77,Finding the work of friction force using line integrals,Finding the work of friction force using line integrals,,"A man in standing on the edge of a flat surface, $H^2$ meters above the ground ( $H>0$ ), which is also the highest and starting point of a parabolic skiing course, given by the simple parametrization: $$\gamma(t)=(t,t^2)\\t\in[-H,h]$$ When $h>0$ . At some moment he begins to ski, reaching the height $h^2<H^2$ twice: once before reaching the minimum point of the course (which is at ground level), and once again afterwards. We will define the second point he reaches to be the endpoint of the course. It is given that the course is not smooth, and the man feels a friction force $\vec{F}$ along the course. At every point of the course, the force is parallel to the parabola, and its orientation is opposite to the direction of the man. The force at every point of the course is given by $|\vec{F}|=\mu|\vec{N}|$ , when $\mu\in\mathbb{R}$ is a given constant, and $\vec{N}$ is the normal force at that point. I am required to compute the work of the friction force along the course. I don't have a problem with the physics of the problem (I hope), but with the math. I'll explain: I know that at every point $(t,t^2)$ of the course, the normal force $\vec{N}$ is given by $|\vec{N}|=mg\cos\alpha$ , when $\alpha$ is the angle between the $\hat{x}$ axis and the tangent line to the parabola. Therefore, at every point of the course, $\tan\alpha$ is given by $\tan\alpha = \frac{d}{dt}t^2=2t$ . The direction of $\vec{F}$ would be, therefore, $-\cos\alpha\hat{x}-\sin\alpha\hat{y}$ . In conclusion, the friction force is given by: $$\vec{F}=-\mu mg\cos\alpha\left(\cos\alpha,\sin\alpha\right)=-\mu mg\left(\frac{1}{1+4t^2},\frac{2t}{1+4t^2}\right)$$ Now: $$W_F\equiv\int_\gamma\vec{F}\ d \vec{r}=-\mu mg\int_{-H}^{h}\left(\frac{1}{1+4t^2},\frac{2t}{1+4t^2}\right)\cdot(1,2t)\ dt=-\mu mg\int_{-H}^{h}dt=-\mu mg(h+H)$$ Something seems odd here - how come $W_F$ is given by the the product of the size of the force, with the horizontal length of the curve? This would seem legitimate to me if the course was horizontal, but it's not. What am I missing here? Is my math incorrect, or maybe is it my physics? Thank you!","A man in standing on the edge of a flat surface, meters above the ground ( ), which is also the highest and starting point of a parabolic skiing course, given by the simple parametrization: When . At some moment he begins to ski, reaching the height twice: once before reaching the minimum point of the course (which is at ground level), and once again afterwards. We will define the second point he reaches to be the endpoint of the course. It is given that the course is not smooth, and the man feels a friction force along the course. At every point of the course, the force is parallel to the parabola, and its orientation is opposite to the direction of the man. The force at every point of the course is given by , when is a given constant, and is the normal force at that point. I am required to compute the work of the friction force along the course. I don't have a problem with the physics of the problem (I hope), but with the math. I'll explain: I know that at every point of the course, the normal force is given by , when is the angle between the axis and the tangent line to the parabola. Therefore, at every point of the course, is given by . The direction of would be, therefore, . In conclusion, the friction force is given by: Now: Something seems odd here - how come is given by the the product of the size of the force, with the horizontal length of the curve? This would seem legitimate to me if the course was horizontal, but it's not. What am I missing here? Is my math incorrect, or maybe is it my physics? Thank you!","H^2 H>0 \gamma(t)=(t,t^2)\\t\in[-H,h] h>0 h^2<H^2 \vec{F} |\vec{F}|=\mu|\vec{N}| \mu\in\mathbb{R} \vec{N} (t,t^2) \vec{N} |\vec{N}|=mg\cos\alpha \alpha \hat{x} \tan\alpha \tan\alpha = \frac{d}{dt}t^2=2t \vec{F} -\cos\alpha\hat{x}-\sin\alpha\hat{y} \vec{F}=-\mu mg\cos\alpha\left(\cos\alpha,\sin\alpha\right)=-\mu mg\left(\frac{1}{1+4t^2},\frac{2t}{1+4t^2}\right) W_F\equiv\int_\gamma\vec{F}\ d \vec{r}=-\mu mg\int_{-H}^{h}\left(\frac{1}{1+4t^2},\frac{2t}{1+4t^2}\right)\cdot(1,2t)\ dt=-\mu mg\int_{-H}^{h}dt=-\mu mg(h+H) W_F","['integration', 'multivariable-calculus', 'physics']"
78,Which is the function that the zero-set of which is Möbius strip?,Which is the function that the zero-set of which is Möbius strip?,,"Many surfaces can be expressed by implicit functions . As described later, the spherical surface, the side surface of the cylinder, and the torus can be expressed by implicit functions. However, I have never seen the implicit function of Möbius strip. As will be explained later, the parametrization of the Torus and Möbius strip seems very similar form. 【My Question】: Can  Möbius strip  (or ""Joint-around-portion removed version of Möbius strip""†) be expressed by implicit function? If possible, which is the function that the zero-set of which is Möbius strip (or ""Joint-around-portion removed version of Möbius strip"" †) ? †. Here, the ""Joint-around-portion removed version of Möbius strip"" represent the surface created by removing a narrow closed set near the joint from Möbius strip so as to be diffeomorphic to closed rectangle. For example, with ε> 0 as a small fixed constant, replace the domain of u in Box 2 with $0 \le u \le 2\pi - \epsilon$ I do not know much about whether the manifold, which is not orientable, can be expressed as a zero set of some functions. But, above-mantioned ""Joint-around-portion removed version of Möbius strip"" is topologically the closed rectangle (therefore closed and orientable) and 'almost' Möbius strip. Many surfaces can be expressed by implicit functions. The spherical surface is the zero set of following ${f}_{suf}$ , $${f}_{suf}(x,y,z)={x}^{2} + {y}^{2} +{z}^{2} -1 $$ The side-surfaxe of cylinder is the zero set of following $f_sils$ , and $$f_{sils}={x}^{2} + {y}^{2} -1$$ When $R \ge r >0 $ , the Torus defined by following parametrization is the zero set of following ${f}_{torus,R,r}$ $${f}_{torus,R,r} = \left(\sqrt{x^2 + y^2}-R\right)^2 + z^2 = r^2 .$$ The parametrization of above-mentioned Torus is: Box.1 (A Parametrization of  Torus, quoted with minor modification from the wikipedia ) $$\begin{align} x(\theta, \varphi) &= (R + r \cos \theta) \cos{\varphi}\\ y(\theta, \varphi) &= (R + r \cos \theta) \sin{\varphi}\\ z(\theta, \varphi) &= r \sin \theta \end{align}$$ where θ, φ are angles which make a full circle, so that their values start and end at the same point, R is the distance from the center of the tube to the center of the torus, r is the radius of the tube. The constant value $R$ and $r$ shall be $R \ge r >0 $ The parametrization of Torus looks to me very similar to that of Möbius strip. As described in Wikipedia, one way to represent the Möbius strip as a subset of three-dimensional Euclidean space is using following parametrization: Box.2 (A Parametrization of Möbius strip quoted from the Wikipedia with minor modification.) $$x(u,v)= \left(1+\frac{v}{2} \cos \frac{u}{2}\right)\cos u$$ $$y(u,v)= \left(1+\frac{v}{2} \cos\frac{u}{2}\right)\sin u$$ $$z(u,v)= \frac{v}{2}\sin \frac{u}{2}$$ where, $0 \le u< 2\pi$ , and, $-1 \le v\le 1$ . This creates a Möbius strip of width 1 whose center circle has radius 1, lies in the $xy$ -plane and is centered at $(0, 0, 0)$ . The parameter $u$ runs around the strip while $v$ moves from one edge to the other. The parametrization of the Torus and Möbius strip look same to me (If $R = 1$ , $r=v/2$ ,the essential difference is what is considered as a variable and what is considered as a constant.) Therefore, I think that Möbius strip(or almost part of Möbius strip)  be  expressed implicitly, by following fuctuion. $${f}_{torus,1,v/2} = \left(\sqrt{x^2 + y^2}-1\right)^2 + z^2 = (\frac{v}{2} )^2 $$ Try the slimier manner of 1578756 First  plug in $\;z=z(u,v) = \frac{v}{2}\sin(u/2) $ to ${f}_{torus,1,v/2}(x,y,z)$ to get $$\left(1-\sqrt{x^2+y^2}\right)^2+{(v/2)}^2\sin^{2} (u/2)={(v/2)}^{2} ,$$ which is $$\left(1-\sqrt{x^2+y^2}\right)^{2}={(v/2)}^{2}(1-\sin^{2}(v/2)).$$ The last expression can also be written as $$\left(\sqrt{x^2+y^2}-1\right)^2={(v/2)}^{2}(1-\sin^{2}(v/2))={(v/2)}^{2}(\cos^{2}(v/2)).$$ Clear away squares and rearrange to obtain $$\sqrt{x^2+y^2}=1+(v/2)\cos\theta,$$ or $$\sqrt{x^2+y^2}=1-(v/2)\cos\theta,$$ and from this you get $$\;x^2+y^2=(1+(v/2)\cos\theta)^2,$$ or $$\;x^2+y^2=(1-(v/2)\cos\theta)^2,$$ or Perhaps it seems to be necessary to distinguish between positive and negative values  of v / 2.  But I think this seems to be a natural. I forcibly separated the Mobius strip into front-side and back-side. It seems difficult to obtain θ geometrically unlike the torus. So, I'll divide the question of implicit function of catted out version of Mobius strip, later. P.S. I'm not very good at English, so I'm sorry if I have some impolite or unclear expressions. English review is also welcomed.","Many surfaces can be expressed by implicit functions . As described later, the spherical surface, the side surface of the cylinder, and the torus can be expressed by implicit functions. However, I have never seen the implicit function of Möbius strip. As will be explained later, the parametrization of the Torus and Möbius strip seems very similar form. 【My Question】: Can  Möbius strip  (or ""Joint-around-portion removed version of Möbius strip""†) be expressed by implicit function? If possible, which is the function that the zero-set of which is Möbius strip (or ""Joint-around-portion removed version of Möbius strip"" †) ? †. Here, the ""Joint-around-portion removed version of Möbius strip"" represent the surface created by removing a narrow closed set near the joint from Möbius strip so as to be diffeomorphic to closed rectangle. For example, with ε> 0 as a small fixed constant, replace the domain of u in Box 2 with I do not know much about whether the manifold, which is not orientable, can be expressed as a zero set of some functions. But, above-mantioned ""Joint-around-portion removed version of Möbius strip"" is topologically the closed rectangle (therefore closed and orientable) and 'almost' Möbius strip. Many surfaces can be expressed by implicit functions. The spherical surface is the zero set of following , The side-surfaxe of cylinder is the zero set of following , and When , the Torus defined by following parametrization is the zero set of following The parametrization of above-mentioned Torus is: Box.1 (A Parametrization of  Torus, quoted with minor modification from the wikipedia ) where θ, φ are angles which make a full circle, so that their values start and end at the same point, R is the distance from the center of the tube to the center of the torus, r is the radius of the tube. The constant value and shall be The parametrization of Torus looks to me very similar to that of Möbius strip. As described in Wikipedia, one way to represent the Möbius strip as a subset of three-dimensional Euclidean space is using following parametrization: Box.2 (A Parametrization of Möbius strip quoted from the Wikipedia with minor modification.) where, , and, . This creates a Möbius strip of width 1 whose center circle has radius 1, lies in the -plane and is centered at . The parameter runs around the strip while moves from one edge to the other. The parametrization of the Torus and Möbius strip look same to me (If , ,the essential difference is what is considered as a variable and what is considered as a constant.) Therefore, I think that Möbius strip(or almost part of Möbius strip)  be  expressed implicitly, by following fuctuion. Try the slimier manner of 1578756 First  plug in to to get which is The last expression can also be written as Clear away squares and rearrange to obtain or and from this you get or or Perhaps it seems to be necessary to distinguish between positive and negative values  of v / 2.  But I think this seems to be a natural. I forcibly separated the Mobius strip into front-side and back-side. It seems difficult to obtain θ geometrically unlike the torus. So, I'll divide the question of implicit function of catted out version of Mobius strip, later. P.S. I'm not very good at English, so I'm sorry if I have some impolite or unclear expressions. English review is also welcomed.","0 \le u \le 2\pi - \epsilon {f}_{suf} {f}_{suf}(x,y,z)={x}^{2} + {y}^{2} +{z}^{2} -1  f_sils f_{sils}={x}^{2} + {y}^{2} -1 R \ge r >0  {f}_{torus,R,r} {f}_{torus,R,r} = \left(\sqrt{x^2 + y^2}-R\right)^2 + z^2 = r^2 . \begin{align}
x(\theta, \varphi) &= (R + r \cos \theta) \cos{\varphi}\\
y(\theta, \varphi) &= (R + r \cos \theta) \sin{\varphi}\\
z(\theta, \varphi) &= r \sin \theta
\end{align} R r R \ge r >0  x(u,v)= \left(1+\frac{v}{2} \cos \frac{u}{2}\right)\cos u y(u,v)= \left(1+\frac{v}{2} \cos\frac{u}{2}\right)\sin u z(u,v)= \frac{v}{2}\sin \frac{u}{2} 0 \le u< 2\pi -1 \le v\le 1 xy (0, 0, 0) u v R = 1 r=v/2 {f}_{torus,1,v/2} = \left(\sqrt{x^2 + y^2}-1\right)^2 + z^2 = (\frac{v}{2} )^2  \;z=z(u,v) = \frac{v}{2}\sin(u/2)  {f}_{torus,1,v/2}(x,y,z) \left(1-\sqrt{x^2+y^2}\right)^2+{(v/2)}^2\sin^{2} (u/2)={(v/2)}^{2} , \left(1-\sqrt{x^2+y^2}\right)^{2}={(v/2)}^{2}(1-\sin^{2}(v/2)). \left(\sqrt{x^2+y^2}-1\right)^2={(v/2)}^{2}(1-\sin^{2}(v/2))={(v/2)}^{2}(\cos^{2}(v/2)). \sqrt{x^2+y^2}=1+(v/2)\cos\theta, \sqrt{x^2+y^2}=1-(v/2)\cos\theta, \;x^2+y^2=(1+(v/2)\cos\theta)^2, \;x^2+y^2=(1-(v/2)\cos\theta)^2,","['multivariable-calculus', 'differential-topology', 'surfaces', 'implicit-function', 'mobius-band']"
79,Find volumes of a unit sphere separated by plane $\frac{x}{\sqrt 2}$ $+$ $y + \frac {z}{\sqrt 2} = 1$,Find volumes of a unit sphere separated by plane,\frac{x}{\sqrt 2} + y + \frac {z}{\sqrt 2} = 1,"I'm having some trouble figuring out how to compute this answer.  The problem is as follows: The plane $\frac{x}{\sqrt 2}$ $+$ $y + \frac {z}{\sqrt 2}$ $ = 1$ cuts a unit sphere centered at the origin into 2 pieces.  Find the volume of both parts. I tried to solve this problem using cylindrical coordinates and a triple integral, but the issue is I don't know the projection of the plane intersection on the xy plane so it is difficult to see what r goes from.  My $\theta$ goes from $0$ to $2\pi$ and z is the difference of the unit sphere minus the plane equation.","I'm having some trouble figuring out how to compute this answer.  The problem is as follows: The plane cuts a unit sphere centered at the origin into 2 pieces.  Find the volume of both parts. I tried to solve this problem using cylindrical coordinates and a triple integral, but the issue is I don't know the projection of the plane intersection on the xy plane so it is difficult to see what r goes from.  My goes from to and z is the difference of the unit sphere minus the plane equation.",\frac{x}{\sqrt 2} + y + \frac {z}{\sqrt 2}  = 1 \theta 0 2\pi,['multivariable-calculus']
80,Vector derivative in a unitary normal vector dependent function,Vector derivative in a unitary normal vector dependent function,,"I am struggling in finding the paratial vector derivative of a function of a unitary normal vector of the form $E=\bf{\hat{n}}\cdot\bf{\hat{v}} = \hat{n}^T\hat{v}$ . Lets say we have $\bf{r_1}$ , $\bf{r_2}$ and $\bf{r_3}$ column-vectors and $\bf{\hat{n}} = \frac{(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}$ . $\bf{\hat{v}}$ is other unitary column  vector that is not relevant until the end. The partial derivative with respecto $\bf{r_1}$ would be given by $\partial_\bf{r_1} = \partial_\bf{r_1}\bf{\hat{n}}\bf{\hat{v}}$ , where $\partial_\bf{r_1}\bf{\hat{n}} = \frac{1}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}}\partial_\bf{r_1}(\bf{r_3 - r_1})\times(\bf{r_2 - r_1}) + (\bf{r_3 - r_1})\times(\bf{r_2 - r_1})\frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}}$ . By consideringthe skew-symmetric matrix $\bf{a} \times \bf{b}  = [\bf{a}]_xb = [\bf{b}]_xa$ ,and then, if $\bf{a = a(r_1)}$ and $\bf{b = b(r_1)}$ , $\bf\partial_{r1} = [a]_x(\partial_{r_1}b) - [b]_x(\partial_{r_1}a)$ . Consequently, $\partial_\bf{r_1}(\bf{r_3 - r_1})\times(\bf{r_2 - r_1}) = [r_3 - r_2]_x$ . My problem comes later, when finding the derivative in the modulus, $\frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}} = - \frac{(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|^3}[\bf{r_3-r_2}]_x = \frac{(\bf{r_3 - r_2})\times[(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})]^T}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|^3}$ where in the last equality I have transposed the numerator to convert the skew to a cross product. The transpose in the second cross produdct is kept for clarity. My main problem is in the evaluation $(\bf{r_3 - r_1})\times(\bf{r_2 - r_1})\frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}}$ . I think that the transposition leads to a row vector, such that the previous product results in a matrix. Without transposition the dimensions does not agree. I find this confusing. Finally the multiplication by $\bf\hat{v}$ leads to a vector as it is expected from the vector derivative of a scalar. Can someone clarify me the problem or show me where are my mistakes? Thank you so much.","I am struggling in finding the paratial vector derivative of a function of a unitary normal vector of the form . Lets say we have , and column-vectors and . is other unitary column  vector that is not relevant until the end. The partial derivative with respecto would be given by , where . By consideringthe skew-symmetric matrix ,and then, if and , . Consequently, . My problem comes later, when finding the derivative in the modulus, where in the last equality I have transposed the numerator to convert the skew to a cross product. The transpose in the second cross produdct is kept for clarity. My main problem is in the evaluation . I think that the transposition leads to a row vector, such that the previous product results in a matrix. Without transposition the dimensions does not agree. I find this confusing. Finally the multiplication by leads to a vector as it is expected from the vector derivative of a scalar. Can someone clarify me the problem or show me where are my mistakes? Thank you so much.",E=\bf{\hat{n}}\cdot\bf{\hat{v}} = \hat{n}^T\hat{v} \bf{r_1} \bf{r_2} \bf{r_3} \bf{\hat{n}} = \frac{(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|} \bf{\hat{v}} \bf{r_1} \partial_\bf{r_1} = \partial_\bf{r_1}\bf{\hat{n}}\bf{\hat{v}} \partial_\bf{r_1}\bf{\hat{n}} = \frac{1}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}}\partial_\bf{r_1}(\bf{r_3 - r_1})\times(\bf{r_2 - r_1}) + (\bf{r_3 - r_1})\times(\bf{r_2 - r_1})\frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}} \bf{a} \times \bf{b}  = [\bf{a}]_xb = [\bf{b}]_xa \bf{a = a(r_1)} \bf{b = b(r_1)} \bf\partial_{r1} = [a]_x(\partial_{r_1}b) - [b]_x(\partial_{r_1}a) \partial_\bf{r_1}(\bf{r_3 - r_1})\times(\bf{r_2 - r_1}) = [r_3 - r_2]_x \frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}} = - \frac{(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|^3}[\bf{r_3-r_2}]_x = \frac{(\bf{r_3 - r_2})\times[(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})]^T}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|^3} (\bf{r_3 - r_1})\times(\bf{r_2 - r_1})\frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}} \bf\hat{v},"['multivariable-calculus', 'differential-geometry', 'vectors', 'vector-analysis', 'matrix-calculus']"
81,Deriving the ideal equation for sluice gate problems,Deriving the ideal equation for sluice gate problems,,"I'm trying to figure this problem out, i know the correct answer, and i know the method, but for some reason i'm missing something and i'm just not getting there, any help would be greatly appreciated. Consider a Sluice Gate (lock gate) as pictured, A brief Description is. A large reservour if fluid of depth $h_1$ is held stationary in $x < 0$ to the left of a vectical gate at the origin of width W. The gate is raised and fluid streams steadily through the opening between the bottom of the gate and the base of the reservoir. Far upstream (the left of the gate) the fluid moves with constant velocity $u_1$ in the x horizontal direction and uniformly through the depth $h_1$ . far upstream,  (to the right of the gate) the fluid assumes a depth $h_2$ and moves with constant velocity $u_2$ in the horizontal x direction. The job is to Derive a relation expressing conservation of mass. Use Bernoulli's equation along a suitable streamline in order to show two >possible choices of $\frac{h_2}{h_1}$ are $\frac{h_2}{h_1} = 1$ and > $\frac{h_2}{h_1} =\frac{Fr^2+Fr\sqrt{Fr^2+8}}{4}$ where $Fr$ is the Froude number $Fr = \frac{u_1}{\sqrt{gh}}$ Then finally Find the net force on the raised gate So,  1. Deriving a relationship expressing conservation of mass is just understanding that as the flow empties from one side, it must rush into the other at the same rate,  and so we have $$Q = u_1 A_1 = u_2 A_2 \implies Q = u_1 h_1W = u_2 h_2 W$$ as our relation. this has been bugging me quite a bit, i'll come back to this. with This question, i believe the correct method is to pick a control volume at about the sluice, then combine the above conservation of mass equation with bernoulli's equation for a steady flow. so here i go... We express Bernoulli's equation as $$\frac{1}{2} \rho u^{2}_1+\rho g h_1 - \frac{1}{2}\rho u^{2}_{2}+\rho g h_2= p_{constant_1} - p_{constant_2} = P_{c}$$ where we have h is the elevation, $\rho$ the density and u is the flow velocity. we have that the pressure also cancels out. and the above equation happens because bernoulli's is a constant along any streamline in the flow. now using this line of logic, i find myself coming up with $$F = \rho g \frac{\left(1-\frac{h_2}{h_1}\right)^3}{2\left(1+\frac{h_2}{h_1}\right)}$$ which means im missing a step as the correct answer is actually $F_{net} = h_1^2 W\rho g \frac{\left(1-\frac{h_2}{h_1}\right)^3}{2\left(1+\frac{h_2}{h_1}\right)}$ This has been really bothering me for the last few, so any help would be greatly appreciated. Thank you for taking the time to read this.","I'm trying to figure this problem out, i know the correct answer, and i know the method, but for some reason i'm missing something and i'm just not getting there, any help would be greatly appreciated. Consider a Sluice Gate (lock gate) as pictured, A brief Description is. A large reservour if fluid of depth is held stationary in to the left of a vectical gate at the origin of width W. The gate is raised and fluid streams steadily through the opening between the bottom of the gate and the base of the reservoir. Far upstream (the left of the gate) the fluid moves with constant velocity in the x horizontal direction and uniformly through the depth . far upstream,  (to the right of the gate) the fluid assumes a depth and moves with constant velocity in the horizontal x direction. The job is to Derive a relation expressing conservation of mass. Use Bernoulli's equation along a suitable streamline in order to show two >possible choices of are and > where is the Froude number Then finally Find the net force on the raised gate So,  1. Deriving a relationship expressing conservation of mass is just understanding that as the flow empties from one side, it must rush into the other at the same rate,  and so we have as our relation. this has been bugging me quite a bit, i'll come back to this. with This question, i believe the correct method is to pick a control volume at about the sluice, then combine the above conservation of mass equation with bernoulli's equation for a steady flow. so here i go... We express Bernoulli's equation as where we have h is the elevation, the density and u is the flow velocity. we have that the pressure also cancels out. and the above equation happens because bernoulli's is a constant along any streamline in the flow. now using this line of logic, i find myself coming up with which means im missing a step as the correct answer is actually This has been really bothering me for the last few, so any help would be greatly appreciated. Thank you for taking the time to read this.",h_1 x < 0 u_1 h_1 h_2 u_2 \frac{h_2}{h_1} \frac{h_2}{h_1} = 1 \frac{h_2}{h_1} =\frac{Fr^2+Fr\sqrt{Fr^2+8}}{4} Fr Fr = \frac{u_1}{\sqrt{gh}} Q = u_1 A_1 = u_2 A_2 \implies Q = u_1 h_1W = u_2 h_2 W \frac{1}{2} \rho u^{2}_1+\rho g h_1 - \frac{1}{2}\rho u^{2}_{2}+\rho g h_2= p_{constant_1} - p_{constant_2} = P_{c} \rho F = \rho g \frac{\left(1-\frac{h_2}{h_1}\right)^3}{2\left(1+\frac{h_2}{h_1}\right)} F_{net} = h_1^2 W\rho g \frac{\left(1-\frac{h_2}{h_1}\right)^3}{2\left(1+\frac{h_2}{h_1}\right)},"['calculus', 'multivariable-calculus', 'physics', 'fluid-dynamics']"
82,A refinement of a famous inequality on the forum .,A refinement of a famous inequality on the forum .,,"It's related to a big problem Olympiad Inequality $\sum\limits_{cyc} \frac{x^4}{8x^3+5y^3} \geqslant \frac{x+y+z}{13}$ .I have this (For one time I take the time to check it ) Let $a,b,c>0$ such that $a+b+c=1$ then we have : $$\sum_{cyc}\frac{a^4}{8a^3+5b^3}> \sum_{cyc}\frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)}\geq \frac{3\tan\Big(\frac{1}{81}\Big)}{13\tan\Big(\frac{1}{27}\Big)} $$ The difficulty exceeds the level of an Olympiad I think . Furthermore I think that we cannot use Jensen's inequality (it's not homogeneous) and Cauchy-Schwarz is really too weak .Don't tell me that $\tan(x)\geq x $ is a good approximation in this case it will be a joke . Maybe we can prove this kind of inequality : $$\frac{a^4}{8a^3+5b^3}+\frac{b^4}{8b^3+5a^3}\geq \frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)}+\frac{\tan(b^4)}{8\tan(b^3)+5\tan(a^3)}$$ But even if it's works it doesn't decide the problem . I discouraged to use power series it's really awful. So comments and hints are welcome but don't try it alone . Thanks for sharing your time and knowledge. Update : I think it's not so hard if we remark that we have : $$\frac{a^4}{8a^3+5b^3}>\frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)}$$ For $a,b>0$ and $a+b<1$ Maybe someone can prove this and prove the LHS",It's related to a big problem Olympiad Inequality $\sum\limits_{cyc} \frac{x^4}{8x^3+5y^3} \geqslant \frac{x+y+z}{13}$ .I have this (For one time I take the time to check it ) Let such that then we have : The difficulty exceeds the level of an Olympiad I think . Furthermore I think that we cannot use Jensen's inequality (it's not homogeneous) and Cauchy-Schwarz is really too weak .Don't tell me that is a good approximation in this case it will be a joke . Maybe we can prove this kind of inequality : But even if it's works it doesn't decide the problem . I discouraged to use power series it's really awful. So comments and hints are welcome but don't try it alone . Thanks for sharing your time and knowledge. Update : I think it's not so hard if we remark that we have : For and Maybe someone can prove this and prove the LHS,"a,b,c>0 a+b+c=1 \sum_{cyc}\frac{a^4}{8a^3+5b^3}> \sum_{cyc}\frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)}\geq \frac{3\tan\Big(\frac{1}{81}\Big)}{13\tan\Big(\frac{1}{27}\Big)}  \tan(x)\geq x  \frac{a^4}{8a^3+5b^3}+\frac{b^4}{8b^3+5a^3}\geq \frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)}+\frac{\tan(b^4)}{8\tan(b^3)+5\tan(a^3)} \frac{a^4}{8a^3+5b^3}>\frac{\tan(a^4)}{8\tan(a^3)+5\tan(b^3)} a,b>0 a+b<1","['multivariable-calculus', 'trigonometry', 'inequality']"
83,Torsion Intuition,Torsion Intuition,,"Here is the formula that my instructor gave me to solve for torsion in Calc3: $\tau = \frac{-d\vec{B}}{dS} \cdot{\vec{N}}$ However, I'm having some trouble understanding the intuition behind this formula. I understand that Torsion measures the twist of a function through space, kinda like the normal measures the turn of a function. For example, if we have a spring, then the rise of the spring would represent the torsion. However, I do not understand where the $\cdot{\vec{N}}$ part comes from. Physically, a dot product represents the magnitude of the component of a vector that lies on another vector. But in this case, why do we care about the component of the derivative of the $\vec{B}$ that's on the Normal vector?","Here is the formula that my instructor gave me to solve for torsion in Calc3: However, I'm having some trouble understanding the intuition behind this formula. I understand that Torsion measures the twist of a function through space, kinda like the normal measures the turn of a function. For example, if we have a spring, then the rise of the spring would represent the torsion. However, I do not understand where the part comes from. Physically, a dot product represents the magnitude of the component of a vector that lies on another vector. But in this case, why do we care about the component of the derivative of the that's on the Normal vector?",\tau = \frac{-d\vec{B}}{dS} \cdot{\vec{N}} \cdot{\vec{N}} \vec{B},"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
84,Proving continuity of a partial derivative,Proving continuity of a partial derivative,,"Let $f(x,y) = xy(x^2-y^2)/(x^2+y^2)$ if $(x,y)\neq (0,0)$ and let $f(x,y) = 0$ if $(x,y)=(0,0)$ Prove that the partial derivative of f with respect to x is continuous at $(0,0)$ . I found the partial derivative to be equal to: $y(x^4+4x^2y^2-y^4)$ / $(x^2+y^2)^2$ Now I know I need to take the limit as x goes to 0. $\lim_{x\to0}$ $y(x^4+4x^2y^2-y^4)$ / $(x^2+y^2)^2$ It follows that this is equal to $-y$ which is continuous on $\mathbb{R}$ for all y . Thus our partial derivative is continuous at $(0,0)$",Let if and let if Prove that the partial derivative of f with respect to x is continuous at . I found the partial derivative to be equal to: / Now I know I need to take the limit as x goes to 0. / It follows that this is equal to which is continuous on for all y . Thus our partial derivative is continuous at,"f(x,y) = xy(x^2-y^2)/(x^2+y^2) (x,y)\neq (0,0) f(x,y) = 0 (x,y)=(0,0) (0,0) y(x^4+4x^2y^2-y^4) (x^2+y^2)^2 \lim_{x\to0} y(x^4+4x^2y^2-y^4) (x^2+y^2)^2 -y \mathbb{R} (0,0)","['calculus', 'limits', 'multivariable-calculus', 'proof-verification']"
85,"Minimize $f(x, y, z) = \frac{a}{x} + \frac{b}{y} +\frac{c}{z}$ with $x+y+z=1$",Minimize  with,"f(x, y, z) = \frac{a}{x} + \frac{b}{y} +\frac{c}{z} x+y+z=1","Problem: Minimize the function $$f(x, y, z) = \frac{a}{x} + \frac{b}{y} +\frac{c}{z}$$ where $a, b, c$ are constants and $a, b, c, x, y, z > 0$ . In addition, $$x+y+z=1$$ . I have a solution, but I'm not sure if it's right - in particular, what can I do to check it? (I can't plot something like this, and I can't seem to make Wolfram accept $a, b, c$ as constants) Solution: $$f(x, y, z) = f(x, y) = \frac{a}{x} + \frac{b}{y} +\frac{c}{1 - x - y}$$ Now finding when the partial derivatives vanish will give the turning points. $$f_x = -\frac{a}{x^2} + \frac{c}{(1-x-y)^2}$$ $$f_y = -\frac{b}{y^2} + \frac{c}{(1-x-y)^2}$$ If $f_x = f_y = 0$ , then $$\frac{a}{x^2} = \frac{b}{y^2} = \frac{c}{(1-x-y)^2} = \frac{c}{z^2}$$ So $$\begin{cases} x = \frac{\sqrt{a}}{\sqrt{a}+\sqrt{b}+\sqrt{c}}\\ y = \frac{\sqrt{b}}{\sqrt{a}+\sqrt{b}+\sqrt{c}}\\ z = \frac{\sqrt{c}}{\sqrt{a}+\sqrt{b}+\sqrt{c}} \end{cases}$$ and $f_{min}=(\sqrt{a}+\sqrt{b}+\sqrt{c})^2$","Problem: Minimize the function where are constants and . In addition, . I have a solution, but I'm not sure if it's right - in particular, what can I do to check it? (I can't plot something like this, and I can't seem to make Wolfram accept as constants) Solution: Now finding when the partial derivatives vanish will give the turning points. If , then So and","f(x, y, z) = \frac{a}{x} + \frac{b}{y} +\frac{c}{z} a, b, c a, b, c, x, y, z > 0 x+y+z=1 a, b, c f(x, y, z) = f(x, y) = \frac{a}{x} + \frac{b}{y} +\frac{c}{1 - x - y} f_x = -\frac{a}{x^2} + \frac{c}{(1-x-y)^2} f_y = -\frac{b}{y^2} + \frac{c}{(1-x-y)^2} f_x = f_y = 0 \frac{a}{x^2} = \frac{b}{y^2} = \frac{c}{(1-x-y)^2} = \frac{c}{z^2} \begin{cases}
x = \frac{\sqrt{a}}{\sqrt{a}+\sqrt{b}+\sqrt{c}}\\
y = \frac{\sqrt{b}}{\sqrt{a}+\sqrt{b}+\sqrt{c}}\\
z = \frac{\sqrt{c}}{\sqrt{a}+\sqrt{b}+\sqrt{c}}
\end{cases} f_{min}=(\sqrt{a}+\sqrt{b}+\sqrt{c})^2","['multivariable-calculus', 'optimization']"
86,Implicit differentiation of a trivariate function,Implicit differentiation of a trivariate function,,"In this post: Deriving the Formula of Total Derivative for Multivariate Functions , it is stated that the first derivative of a trivariate function $f(x,y(x),z(x))$ with respect to $x$ is $$\large \frac{df}{dx}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}$$ I have an implicit equation $f(x,y(x),z(x))=0$ As part of a calculation for $\frac{dy}{dx}$ , I differentiate both sides.  Doing this I get: $\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}=0$ which gives: $\large\frac{dy}{dx}=-\frac{\frac{\partial f}{\partial x}+\frac{\partial f}{\partial z}\frac{dz}{dx}}{\frac{\partial f}{\partial y}}$ To make sure I have the correct expression, I check this with a simple example: $f(x,y,z)=x+xy+yz-z^2=0$ where: $y=2x$ ; $z=5x$ So: $\frac{\partial f}{\partial x}=1+y$ ; $\frac{\partial f}{\partial y}=x+z$ ; $\frac{\partial f}{\partial z}=y-2z$ ; $\frac{dz}{dx}=5$ It is easy to solve $f(x,y,z)=x+xy+yz-z^2=0$ giving $x=\frac{1}{13}$ , $y=\frac{2}{13}$ and $z=\frac{5}{13}$ So $\frac{dy}{dx}=-\frac{(1+y)+5(y-2z)}{x+z}=4.16666$ , but we know it should be $2.0$ . Problem, so let's check the original equation: Substituting into: $\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}$ , I get: $(1+y)+(x+z)*2+(y-2z)*5$ which equals $-1$ , ie. not the $0$ I was expecting. What am I missing?","In this post: Deriving the Formula of Total Derivative for Multivariate Functions , it is stated that the first derivative of a trivariate function with respect to is I have an implicit equation As part of a calculation for , I differentiate both sides.  Doing this I get: which gives: To make sure I have the correct expression, I check this with a simple example: where: ; So: ; ; ; It is easy to solve giving , and So , but we know it should be . Problem, so let's check the original equation: Substituting into: , I get: which equals , ie. not the I was expecting. What am I missing?","f(x,y(x),z(x)) x \large \frac{df}{dx}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx} f(x,y(x),z(x))=0 \frac{dy}{dx} \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}=0 \large\frac{dy}{dx}=-\frac{\frac{\partial f}{\partial x}+\frac{\partial f}{\partial z}\frac{dz}{dx}}{\frac{\partial f}{\partial y}} f(x,y,z)=x+xy+yz-z^2=0 y=2x z=5x \frac{\partial f}{\partial x}=1+y \frac{\partial f}{\partial y}=x+z \frac{\partial f}{\partial z}=y-2z \frac{dz}{dx}=5 f(x,y,z)=x+xy+yz-z^2=0 x=\frac{1}{13} y=\frac{2}{13} z=\frac{5}{13} \frac{dy}{dx}=-\frac{(1+y)+5(y-2z)}{x+z}=4.16666 2.0 \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx} (1+y)+(x+z)*2+(y-2z)*5 -1 0","['multivariable-calculus', 'partial-derivative', 'implicit-differentiation', 'implicit-function']"
87,Show that $h(x)$ has a minimum at $0$,Show that  has a minimum at,h(x) 0,Let $h:\mathbb R^n\rightarrow\mathbb R$ be a function that satisfies $x\cdot\nabla h(x)\geq0~~\forall~x\in\mathbb R^n $ . How can I show that h has a minimum at zero? I know that at a stationary point $\nabla h(x)=0$ .,Let be a function that satisfies . How can I show that h has a minimum at zero? I know that at a stationary point .,h:\mathbb R^n\rightarrow\mathbb R x\cdot\nabla h(x)\geq0~~\forall~x\in\mathbb R^n  \nabla h(x)=0,"['multivariable-calculus', 'derivatives']"
88,Trying to find a converse (or counterexample of it) to implicit function theorem,Trying to find a converse (or counterexample of it) to implicit function theorem,,"The implicit function theorem stated in Spivak's Calculus of Manifolds is as follows: My question is: if the above $f$ is continuously differentiable on an open set of $(a,b)$ and $f(a,b)=0$ and moreover $\exists$ open set $A$ containing $a$ and open set $B$ containing $b$ and a continuously differentiable function $g$ such   that for each $x\in A$ there is a unique point $g(x) \in B$ such that $f(x,g(x))=0$ is it necessarily true that $M$ is invertible? I ask this because I was talking with my professor about this (and maybe I misheard) but he said that the converse to implicit function theorem is easy to see. However, I do not know where to begin with this.","The implicit function theorem stated in Spivak's Calculus of Manifolds is as follows: My question is: if the above is continuously differentiable on an open set of and and moreover open set containing and open set containing and a continuously differentiable function such   that for each there is a unique point such that is it necessarily true that is invertible? I ask this because I was talking with my professor about this (and maybe I misheard) but he said that the converse to implicit function theorem is easy to see. However, I do not know where to begin with this.","f (a,b) f(a,b)=0 \exists A a B b g x\in A g(x) \in B f(x,g(x))=0 M","['multivariable-calculus', 'examples-counterexamples', 'implicit-function-theorem']"
89,"Finding the critical points of $f(x,y)=(y-x^2)(y-2x^2)$",Finding the critical points of,"f(x,y)=(y-x^2)(y-2x^2)","Finding the critical points of $f(x,y)=(y-x^2)(y-2x^2)$ I know that $(a,b)$ is a critical point $\iff \nabla f(a,b)=(0,0)$ So $\nabla f(x,y)= (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})$ $\frac{\partial f}{\partial x}[(y-x^2)(y-2x^2)]=8x^3-6xy$ $\frac{\partial f}{\partial y}[(y-x^2)(y-2x^2)]=-3x^2+2y$ $\nabla f(x,y)=(8x^3-6xy, -3x^2+2y)=(0,0) \iff (x,y)=(0,0)$ So finding the Hessian matrix at $(0,0)$ $\frac{\partial ^2f}{\partial x^2}[(y-x^2)(y-2x^2)]=\frac{\partial    f}{\partial x}[8x^3-6xy]=24x^2-6y$ $\frac{\partial ^2f}{\partial y^2}[(y-x^2)(y-2x^2)]=\frac{\partial f}{\partial y}[-3x^2+2y]=2$ $\frac{\partial ^2f}{\partial x \partial y}[(y-x^2)(y-2x^2)]=\frac{\partial    f}{\partial x}[8x^3-6xy]=-6x$ $H(0,0)=\begin{pmatrix} 0 & 0 \\ 0 & 2 \end{pmatrix}$ Finding the eigenvalues for $H(0,0)$ : $\det(H(0,0)-\lambda I)= \begin{vmatrix} -\lambda & 0 \\ 0 & 2-\lambda \end{vmatrix} = \lambda^2-2\lambda$ $\lambda^2-2\lambda=0 \iff \begin{cases}        \lambda_1 = 0 \\       \lambda_2 = 2     \end{cases}$ So I can't really tell if $f(x,y)$ has any relative maximum nor minimum or a saddle point, and I don't know how to continue... Edit: I know that this is a duplicate post , but I don't understand the answer, and the question is 4 years old.","Finding the critical points of I know that is a critical point So So finding the Hessian matrix at Finding the eigenvalues for : So I can't really tell if has any relative maximum nor minimum or a saddle point, and I don't know how to continue... Edit: I know that this is a duplicate post , but I don't understand the answer, and the question is 4 years old.","f(x,y)=(y-x^2)(y-2x^2) (a,b) \iff \nabla f(a,b)=(0,0) \nabla f(x,y)= (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}) \frac{\partial f}{\partial x}[(y-x^2)(y-2x^2)]=8x^3-6xy \frac{\partial f}{\partial y}[(y-x^2)(y-2x^2)]=-3x^2+2y \nabla f(x,y)=(8x^3-6xy, -3x^2+2y)=(0,0) \iff (x,y)=(0,0) (0,0) \frac{\partial ^2f}{\partial x^2}[(y-x^2)(y-2x^2)]=\frac{\partial
   f}{\partial x}[8x^3-6xy]=24x^2-6y \frac{\partial ^2f}{\partial y^2}[(y-x^2)(y-2x^2)]=\frac{\partial f}{\partial y}[-3x^2+2y]=2 \frac{\partial ^2f}{\partial x \partial y}[(y-x^2)(y-2x^2)]=\frac{\partial
   f}{\partial x}[8x^3-6xy]=-6x H(0,0)=\begin{pmatrix}
0 & 0 \\
0 & 2
\end{pmatrix} H(0,0) \det(H(0,0)-\lambda I)= \begin{vmatrix}
-\lambda & 0 \\
0 & 2-\lambda
\end{vmatrix} = \lambda^2-2\lambda \lambda^2-2\lambda=0 \iff \begin{cases} 
      \lambda_1 = 0 \\
      \lambda_2 = 2 
   \end{cases} f(x,y)","['calculus', 'multivariable-calculus', 'partial-derivative', 'hessian-matrix']"
90,Express $\partial_x$ by $\partial_r$ and $\partial_{\phi}$,Express  by  and,\partial_x \partial_r \partial_{\phi},"I want to express $\partial_x$ and $\partial_y$ by $\partial_r$ and $\partial_{\phi}$ . From $$ \frac{\partial }{\partial r} = \frac{\partial x}{\partial r}\frac{\partial }{\partial x} + \frac{\partial y}{\partial r}\frac{\partial }{\partial y}$$ $$ \frac{\partial }{\partial \phi} = \frac{\partial x}{\partial \phi}\frac{\partial }{\partial x} + \frac{\partial y}{\partial \phi}\frac{\partial }{\partial y}$$ , substituting $x=r\cos\phi$ and $y=r\sin\phi$ , we get follows. $$ \frac{\partial }{\partial r} = \cos{\phi}\frac{\partial }{\partial x} +\sin\phi\frac{\partial }{\partial y}$$ $$ \frac{\partial }{\partial \phi} = -r\sin{\phi}\frac{\partial }{\partial x} + r\cos\phi\frac{\partial }{\partial y}$$ Rewrite this by matrix as below. $$ \begin{pmatrix} \frac{\partial }{\partial r}  \\ \frac{1}{r}\frac{\partial }{\partial \phi} \\ \end{pmatrix}= \begin{pmatrix} \cos\phi & \sin\phi \\ -\sin\phi & \cos\phi \\ \end{pmatrix} \begin{pmatrix} \frac{\partial }{\partial x}  \\ \frac{\partial }{\partial y} \\ \end{pmatrix}$$ $$\Leftrightarrow  \begin{pmatrix} \frac{\partial }{\partial x}  \\ \frac{\partial }{\partial y} \\ \end{pmatrix}= \begin{pmatrix} \cos\phi & -\sin\phi \\ \sin\phi & \cos\phi \\ \end{pmatrix} \begin{pmatrix} \frac{\partial }{\partial r}  \\ \frac{1}{r}\frac{\partial }{\partial \phi} \\ \end{pmatrix}$$ Then, we can express $\partial_x$ and $\partial_y$ by $\partial_r$ and $\partial_\phi$ , we get follows. $$ \frac{\partial }{\partial x} = \cos\phi\frac{\partial }{\partial r} - \frac{\sin \phi}{r}\frac{\partial }{\partial \phi}$$ $$ \frac{\partial }{\partial y} = \sin\phi\frac{\partial }{\partial r} + \frac{\cos\phi}{r}\frac{\partial }{\partial \phi}$$ However, if we directly calculate $\partial_x$ and $\partial_y$ from $ \frac{\partial }{\partial x} = \frac{\partial r}{\partial x}\frac{\partial }{\partial r} + \frac{\partial \phi}{\partial x}\frac{\partial }{\partial \phi}$ and $ \frac{\partial }{\partial y} = \frac{\partial r}{\partial y}\frac{\partial }{\partial r} + \frac{\partial \phi}{\partial y}\frac{\partial }{\partial \phi}$ , result does not meet with each other. For example, \begin{align}  &\frac{\partial \phi}{\partial x} \\ =&\frac{1}{r}\frac{\partial \phi}{\partial \cos\phi} \\ =&\frac{1}{r\sin\phi} \end{align} Here, I use the relationship, $\frac{\partial f(x)}{\partial x} = \left(\frac{\partial x}{\partial f(x)}\right)^{-1}$ . However, from the first calculation, this $\frac{\partial \phi}{\partial x}$ should be equal to $-\frac{\sin\phi}{r}$ .  What is the origin of this contradiction? I find this error (many times) when I'm scoring freshman's physics class test as a TA, however I cannot nicely explain why such latter calculation fails.","I want to express and by and . From , substituting and , we get follows. Rewrite this by matrix as below. Then, we can express and by and , we get follows. However, if we directly calculate and from and , result does not meet with each other. For example, Here, I use the relationship, . However, from the first calculation, this should be equal to .  What is the origin of this contradiction? I find this error (many times) when I'm scoring freshman's physics class test as a TA, however I cannot nicely explain why such latter calculation fails.","\partial_x \partial_y \partial_r \partial_{\phi}  \frac{\partial }{\partial r} = \frac{\partial x}{\partial r}\frac{\partial }{\partial x} + \frac{\partial y}{\partial r}\frac{\partial }{\partial y}  \frac{\partial }{\partial \phi} = \frac{\partial x}{\partial \phi}\frac{\partial }{\partial x} + \frac{\partial y}{\partial \phi}\frac{\partial }{\partial y} x=r\cos\phi y=r\sin\phi  \frac{\partial }{\partial r} = \cos{\phi}\frac{\partial }{\partial x} +\sin\phi\frac{\partial }{\partial y}  \frac{\partial }{\partial \phi} = -r\sin{\phi}\frac{\partial }{\partial x} + r\cos\phi\frac{\partial }{\partial y} 
\begin{pmatrix}
\frac{\partial }{\partial r}  \\
\frac{1}{r}\frac{\partial }{\partial \phi} \\
\end{pmatrix}=
\begin{pmatrix}
\cos\phi & \sin\phi \\
-\sin\phi & \cos\phi \\
\end{pmatrix}
\begin{pmatrix}
\frac{\partial }{\partial x}  \\
\frac{\partial }{\partial y} \\
\end{pmatrix} \Leftrightarrow 
\begin{pmatrix}
\frac{\partial }{\partial x}  \\
\frac{\partial }{\partial y} \\
\end{pmatrix}=
\begin{pmatrix}
\cos\phi & -\sin\phi \\
\sin\phi & \cos\phi \\
\end{pmatrix}
\begin{pmatrix}
\frac{\partial }{\partial r}  \\
\frac{1}{r}\frac{\partial }{\partial \phi} \\
\end{pmatrix} \partial_x \partial_y \partial_r \partial_\phi  \frac{\partial }{\partial x} = \cos\phi\frac{\partial }{\partial r} - \frac{\sin \phi}{r}\frac{\partial }{\partial \phi}  \frac{\partial }{\partial y} = \sin\phi\frac{\partial }{\partial r} + \frac{\cos\phi}{r}\frac{\partial }{\partial \phi} \partial_x \partial_y  \frac{\partial }{\partial x} = \frac{\partial r}{\partial x}\frac{\partial }{\partial r} + \frac{\partial \phi}{\partial x}\frac{\partial }{\partial \phi}  \frac{\partial }{\partial y} = \frac{\partial r}{\partial y}\frac{\partial }{\partial r} + \frac{\partial \phi}{\partial y}\frac{\partial }{\partial \phi} \begin{align}
 &\frac{\partial \phi}{\partial x} \\
=&\frac{1}{r}\frac{\partial \phi}{\partial \cos\phi} \\
=&\frac{1}{r\sin\phi}
\end{align} \frac{\partial f(x)}{\partial x} = \left(\frac{\partial x}{\partial f(x)}\right)^{-1} \frac{\partial \phi}{\partial x} -\frac{\sin\phi}{r}","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
91,Finding the resonance of a second order differential equation,Finding the resonance of a second order differential equation,,$$y'' + 100y = \frac{1}{3} \cos(bx) $$ with $$ y(0) = 0$$ and $$ y'(0) = 0.1$$ I believe the homogenous (general) solution is $$\frac{1}{100} \sin(10x)$$ however I am having trouble finding the inhomogeneous (particular) solution as well as the values of b at which resonance will occur (I think these two are intertwined).,with and I believe the homogenous (general) solution is however I am having trouble finding the inhomogeneous (particular) solution as well as the values of b at which resonance will occur (I think these two are intertwined).,y'' + 100y = \frac{1}{3} \cos(bx)   y(0) = 0  y'(0) = 0.1 \frac{1}{100} \sin(10x),['multivariable-calculus']
92,Triple integral for the volume of the cap of the solid sphere $x^2 + y^2 + z^2 \leq 10$ cut off by the plane $z=1$?,Triple integral for the volume of the cap of the solid sphere  cut off by the plane ?,x^2 + y^2 + z^2 \leq 10 z=1,"The question asks me to formulate a triple integral for the volume of the cap of the solid sphere $x^2 + y^2 + z^2 \leq 10$ cut off by the plane $z=1$ ? Here's the integral I formulated as an answer (I used cylindrical coordinates): $$\int_0^{2\pi} \int_0^\sqrt{10} \int_1^\sqrt{10-r^2} r \ dz \ dr \ d\theta$$ However, apparently the correct answer is: $$\int_0^{2\pi} \int_0^3 \int_1^\sqrt{10-r^2} r \ dz \ dr \ d\theta$$ The only difference is the $3$ , but where does the $3$ come from? $x^2 + y^2 + z^2 \leq 10$ implies we're dealing with a sphere with radius $\sqrt{10}$ . I have no idea where the $3$ is coming from really. Any help is appreciated.","The question asks me to formulate a triple integral for the volume of the cap of the solid sphere cut off by the plane ? Here's the integral I formulated as an answer (I used cylindrical coordinates): However, apparently the correct answer is: The only difference is the , but where does the come from? implies we're dealing with a sphere with radius . I have no idea where the is coming from really. Any help is appreciated.",x^2 + y^2 + z^2 \leq 10 z=1 \int_0^{2\pi} \int_0^\sqrt{10} \int_1^\sqrt{10-r^2} r \ dz \ dr \ d\theta \int_0^{2\pi} \int_0^3 \int_1^\sqrt{10-r^2} r \ dz \ dr \ d\theta 3 3 x^2 + y^2 + z^2 \leq 10 \sqrt{10} 3,"['calculus', 'integration', 'multivariable-calculus', 'multiple-integral', 'cylindrical-coordinates']"
93,Connecting two 3D functions with a curve,Connecting two 3D functions with a curve,,"Let’s say I have a helix as so $$f(t)=2\sin t\text{i}+2\cos t\text{j}+\frac t2\text{k}\qquad 0\leq t\leq8\pi$$ The end point is $P(0,2,4\pi)$ . I want to be able to connect the end of the helix with a horizontal tangent line but also keep the curve smooth. As of right now, if we differentiate $f(t)$ and substitute $t=8\pi$ to get the direction vector, the line is angled (i.e not completely horizontal). My plan is to insert an arc whose tangent line is also tangent to the end of the helix and ends with a horizontal tangent. Question: Is there a way to algebraically determine the equation of a circle or arc that connects the end of the helix with a horizontal line?","Let’s say I have a helix as so The end point is . I want to be able to connect the end of the helix with a horizontal tangent line but also keep the curve smooth. As of right now, if we differentiate and substitute to get the direction vector, the line is angled (i.e not completely horizontal). My plan is to insert an arc whose tangent line is also tangent to the end of the helix and ends with a horizontal tangent. Question: Is there a way to algebraically determine the equation of a circle or arc that connects the end of the helix with a horizontal line?","f(t)=2\sin t\text{i}+2\cos t\text{j}+\frac t2\text{k}\qquad 0\leq t\leq8\pi P(0,2,4\pi) f(t) t=8\pi","['multivariable-calculus', 'vectors']"
94,"How do I prove the limit $\frac{\sin(xy)}{\sqrt{x^2 + y^2}}$ as (x, y) approaches (0, 0) using $\epsilon - \delta$","How do I prove the limit  as (x, y) approaches (0, 0) using",\frac{\sin(xy)}{\sqrt{x^2 + y^2}} \epsilon - \delta,"I know that I can convert this limit to polar coordinates and solve the limit, but I want to see how I would do it using the $\epsilon - \delta$ definition of a limit. This is my work so far: We know that $$\left|{\frac{\sin(xy)}{\sqrt{x^2 + y^2}}} - 0\right| < \epsilon$$ and $$ \left| \sqrt{x^2 + y^2} \right| < \delta $$ Then, $$ \begin{align} \left|\sin(xy)\right| &< \epsilon \left|\sqrt{x^2 + y^2}\right| \\ \frac{\left|\sin(xy)\right|}{\epsilon} &< \left|\sqrt{x^2 + y^2}\right| \end{align} $$ I am stuck here, as normally I would get an expression that matches $\delta$ , but here the signs are switched.","I know that I can convert this limit to polar coordinates and solve the limit, but I want to see how I would do it using the definition of a limit. This is my work so far: We know that and Then, I am stuck here, as normally I would get an expression that matches , but here the signs are switched.","\epsilon - \delta \left|{\frac{\sin(xy)}{\sqrt{x^2 + y^2}}} - 0\right| < \epsilon 
\left| \sqrt{x^2 + y^2} \right| < \delta  
\begin{align}
\left|\sin(xy)\right| &< \epsilon \left|\sqrt{x^2 + y^2}\right| \\
\frac{\left|\sin(xy)\right|}{\epsilon} &< \left|\sqrt{x^2 + y^2}\right|
\end{align}
 \delta","['limits', 'multivariable-calculus', 'epsilon-delta']"
95,Integral of directional derivative.,Integral of directional derivative.,,"I have just learned about a proof for why the gradient ""shows the direction of steepest ascent"" involving: $\partial_{v_0} f(x_0) = (f \circ \gamma)^{\prime}(t_0) = \langle\nabla f(x_0), v_0\rangle$ with: $\gamma (t_0) = x_o, \gamma^{\prime}(t_0)=v_0$ which I now understand. A little later in the script, it is presented that: $f(x_1) - f(x_0) = \int^b_a \langle \nabla f (\gamma(t)), \gamma^{\prime}(t)\rangle dt$ . for every $C^1$ -curve $\gamma:[a,b] \rightarrow U \subset \mathbb{R}^n$ with $\gamma(a) = x_0, \gamma(b) = x_1$ , which is without proof or definition. Does anybody have an intuition or explanation as to why this holds true or maybe just a link to where I can read more?","I have just learned about a proof for why the gradient ""shows the direction of steepest ascent"" involving: with: which I now understand. A little later in the script, it is presented that: . for every -curve with , which is without proof or definition. Does anybody have an intuition or explanation as to why this holds true or maybe just a link to where I can read more?","\partial_{v_0} f(x_0) = (f \circ \gamma)^{\prime}(t_0) = \langle\nabla f(x_0), v_0\rangle \gamma (t_0) = x_o, \gamma^{\prime}(t_0)=v_0 f(x_1) - f(x_0) = \int^b_a \langle \nabla f (\gamma(t)), \gamma^{\prime}(t)\rangle dt C^1 \gamma:[a,b] \rightarrow U \subset \mathbb{R}^n \gamma(a) = x_0, \gamma(b) = x_1","['real-analysis', 'multivariable-calculus', 'derivatives', 'parametric']"
96,Total derivative only defined on open subset,Total derivative only defined on open subset,,"Let's say that $f: M \to\mathbb{R}^n$ , where $M \subset \mathbb{R}^m$ , is totally differentiable at point $a$ so that we have an open subeset $U$ with $a \in U \subset M$ . Why does $U$ have to be an open set? What's wrong with it if you assume that $U$ is closed and $a$ lies on the edge of subset $U$ ? edit Why is it not sufficient to assume that $a$ is a limit point if you want to show the uniqueness of the total derivative?","Let's say that , where , is totally differentiable at point so that we have an open subeset with . Why does have to be an open set? What's wrong with it if you assume that is closed and lies on the edge of subset ? edit Why is it not sufficient to assume that is a limit point if you want to show the uniqueness of the total derivative?",f: M \to\mathbb{R}^n M \subset \mathbb{R}^m a U a \in U \subset M U U a U a,"['multivariable-calculus', 'derivatives']"
97,Second derivative test with non zero result,Second derivative test with non zero result,,"Suppose function f(x,y) is of $C^2$ , and we know that $f_{xx}f_{yy}-f_{xy}^2$ does not equal to 0. Furthermore, $f_{xx}+f_{yy} \ge 0$ . Show that f does not have a strict local max. So the second derivative test tells us that it is either a saddle point, min, or max since it does not equal to 0. And we know that $f_{xy}^2 \ge 0$ . If $f_{xx}f_{yy} > f_{xy}^2 $ , then we know $f_{xx}f_{yy}-f_{xy}^2 > 0$ , then if $f_{xx}>0$ , local min. But it can be a local max if $f_{xx}<0$ . If $f_{xx}f_{yy} < f_{xy}^2 $ , then we know $f_{xx}f_{yy}-f_{xy}^2 < 0$ , then saddle point. How can I show $f_{xx}<0$ is not possible through $f_{xx}+f_{yy} \ge 0$ ?","Suppose function f(x,y) is of , and we know that does not equal to 0. Furthermore, . Show that f does not have a strict local max. So the second derivative test tells us that it is either a saddle point, min, or max since it does not equal to 0. And we know that . If , then we know , then if , local min. But it can be a local max if . If , then we know , then saddle point. How can I show is not possible through ?",C^2 f_{xx}f_{yy}-f_{xy}^2 f_{xx}+f_{yy} \ge 0 f_{xy}^2 \ge 0 f_{xx}f_{yy} > f_{xy}^2  f_{xx}f_{yy}-f_{xy}^2 > 0 f_{xx}>0 f_{xx}<0 f_{xx}f_{yy} < f_{xy}^2  f_{xx}f_{yy}-f_{xy}^2 < 0 f_{xx}<0 f_{xx}+f_{yy} \ge 0,"['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'maxima-minima']"
98,A $C^1$ fucntion $f$ on $\Bbb R^2$ that has no mixed second derivative,A  fucntion  on  that has no mixed second derivative,C^1 f \Bbb R^2,"Is there a $C^1$ function $f$ on $\Bbb R^2$ that have no mixed second derivatives? (Here, mixed second derivative means $f_{xy}$ or $f_{yx}$ .)","Is there a function on that have no mixed second derivatives? (Here, mixed second derivative means or .)",C^1 f \Bbb R^2 f_{xy} f_{yx},"['calculus', 'analysis', 'multivariable-calculus']"
99,How would I find the Jacobian matrix for this set of differential equations?,How would I find the Jacobian matrix for this set of differential equations?,,"I am working on a nonlinear dynamics problem set and I am uncertain of how to go about finding the Jacobian matrix for this set of equations. I know that you can use taylor expansion and I should end up with a matrix of partial derivatives... I think I have an answer, I just want to double check! Thank you so much! Here are the equations: $\dot{x} = y - x^3 - 3x^2 + I$ $\dot{y} = 1 - 5x^2 - y$ NB: I think that the jacobian is $\begin{bmatrix}3x^2 - 6x & 1\\ -10x & -1\end{bmatrix}$ and it will be evaluated at some point (x*, y*).","I am working on a nonlinear dynamics problem set and I am uncertain of how to go about finding the Jacobian matrix for this set of equations. I know that you can use taylor expansion and I should end up with a matrix of partial derivatives... I think I have an answer, I just want to double check! Thank you so much! Here are the equations: NB: I think that the jacobian is and it will be evaluated at some point (x*, y*).",\dot{x} = y - x^3 - 3x^2 + I \dot{y} = 1 - 5x^2 - y \begin{bmatrix}3x^2 - 6x & 1\\ -10x & -1\end{bmatrix},"['calculus', 'linear-algebra', 'multivariable-calculus', 'systems-of-equations', 'jacobian']"
