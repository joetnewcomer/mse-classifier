,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,When does the next complex split occur?,When does the next complex split occur?,,"So I was thinking about complex numbers and how they came about and someting interesting occured to me: the formation of complex numbers occurs because there exists a function (namely $f(x)=x^2$) that maps reals to a smaller subset of reals and therefore the by setting the equation $x^2 = p$ in $\{x\in\Bbb R:x\not\in f(\Bbb R)\}$.  We are forced to create a new mathemtical value, $i$ to handle the equation. Couldn't this extension occur elsewhere? For example, say there is some function $f_{\Bbb C}$ that maps the complex numbers to a smaller subset of the complex numbers then the expression $f_{\Bbb C}(x) = j$ in $\{z\in\Bbb C:z\not\in f_{\Bbb C}(\Bbb C)\}$ will require the creation of a new mathematical constant. Is this possible? Has it ever happened? This differs greatly from quaternions and other tools which are considered extensions of the complex plane. The most basic idea that comes to mind is the absolute value function. The equation: $|x| = c$ where $c$ is not a positive real number or $0$ but is a member of the complex plane. You would get numbers $p$, such that $|p| = -1$, and $t$ such that $|t| = i$. From there the expression $p\cdot t$ would be such that $|p\cdot t| = -i$. Then an entire algebra from these can be constructed. But the only reason I'm not satisfied with this answer is because Complex numbers were naturally developed from the basic hyperoperators (addition -- multiplication -- powers (produces complex numbers) --> tetration ---> pentation etc...) Is there some hyperoperator down the line that will produce the next complex number?","So I was thinking about complex numbers and how they came about and someting interesting occured to me: the formation of complex numbers occurs because there exists a function (namely $f(x)=x^2$) that maps reals to a smaller subset of reals and therefore the by setting the equation $x^2 = p$ in $\{x\in\Bbb R:x\not\in f(\Bbb R)\}$.  We are forced to create a new mathemtical value, $i$ to handle the equation. Couldn't this extension occur elsewhere? For example, say there is some function $f_{\Bbb C}$ that maps the complex numbers to a smaller subset of the complex numbers then the expression $f_{\Bbb C}(x) = j$ in $\{z\in\Bbb C:z\not\in f_{\Bbb C}(\Bbb C)\}$ will require the creation of a new mathematical constant. Is this possible? Has it ever happened? This differs greatly from quaternions and other tools which are considered extensions of the complex plane. The most basic idea that comes to mind is the absolute value function. The equation: $|x| = c$ where $c$ is not a positive real number or $0$ but is a member of the complex plane. You would get numbers $p$, such that $|p| = -1$, and $t$ such that $|t| = i$. From there the expression $p\cdot t$ would be such that $|p\cdot t| = -i$. Then an entire algebra from these can be constructed. But the only reason I'm not satisfied with this answer is because Complex numbers were naturally developed from the basic hyperoperators (addition -- multiplication -- powers (produces complex numbers) --> tetration ---> pentation etc...) Is there some hyperoperator down the line that will produce the next complex number?",,"['complex-analysis', 'complex-numbers', 'clifford-algebras']"
1,complex analysis/ Taylor expansion question,complex analysis/ Taylor expansion question,,Let $f:\mathbb C \rightarrow \mathbb C$ be holomorphic and $f(z)=f(-z)$ for all $z\in \mathbb C$. Show that there exists a holomorphic function $g$ such that $g(z^2)=f(z)$. If I take $g(z):=(f(z)+f(-z))/2$ then I can prove that $g(z)=\sum_0^\infty a_{2n}z^n$. Of course $g$ thus defined is holomorphic. But how does this show that  $g(^2)=f(z)$ as well?. I mean the coefficients of the two are different when you compare term by term. Can you guys help?. Or should this $g$ be defined differently? Thanks for your help.,Let $f:\mathbb C \rightarrow \mathbb C$ be holomorphic and $f(z)=f(-z)$ for all $z\in \mathbb C$. Show that there exists a holomorphic function $g$ such that $g(z^2)=f(z)$. If I take $g(z):=(f(z)+f(-z))/2$ then I can prove that $g(z)=\sum_0^\infty a_{2n}z^n$. Of course $g$ thus defined is holomorphic. But how does this show that  $g(^2)=f(z)$ as well?. I mean the coefficients of the two are different when you compare term by term. Can you guys help?. Or should this $g$ be defined differently? Thanks for your help.,,['complex-analysis']
2,"If f has an essential singularity, does $P\circ f$ have the same type of singulaity?","If f has an essential singularity, does  have the same type of singulaity?",P\circ f,"Let $z_0\in\mathbb C$, $f$ a function having an essential singularity at $z_0$ and $P$ a non-constant polynomial. Show that the composite $P\circ f$ has an essential singularity at $z_0$. I tried to solve it looking at Laurent series expansion. Let $$f(z)=\sum_{-\infty}^{\infty}a_n (z-z_0)^n$$ the Laurent expansion of $f$ for $0<|z-z_0|<r$, for some $r>0$. Let $$P(z)=\sum_{n=0}^{n=M}b_nz^n$$ the polynomial. So we get $$(P\circ f)(z)=b_0+b_1 f(z)+\ldots +b_M(f(z))^M$$ I think the RHS is well defined, since sums, products and powers of power series are well defined. Now I would like to show that RHS contains an infinite number of negative powers of $(z-z_0)$, but i don't know the way.","Let $z_0\in\mathbb C$, $f$ a function having an essential singularity at $z_0$ and $P$ a non-constant polynomial. Show that the composite $P\circ f$ has an essential singularity at $z_0$. I tried to solve it looking at Laurent series expansion. Let $$f(z)=\sum_{-\infty}^{\infty}a_n (z-z_0)^n$$ the Laurent expansion of $f$ for $0<|z-z_0|<r$, for some $r>0$. Let $$P(z)=\sum_{n=0}^{n=M}b_nz^n$$ the polynomial. So we get $$(P\circ f)(z)=b_0+b_1 f(z)+\ldots +b_M(f(z))^M$$ I think the RHS is well defined, since sums, products and powers of power series are well defined. Now I would like to show that RHS contains an infinite number of negative powers of $(z-z_0)$, but i don't know the way.",,"['complex-analysis', 'power-series']"
3,How to calculate $\sum_{k=1}^{k=n}\frac{\sin(kx)}{\sin^{k}(x)}$? (second question),How to calculate ? (second question),\sum_{k=1}^{k=n}\frac{\sin(kx)}{\sin^{k}(x)},"I asked yesterday for a hint on how to calculate  $$1+\sum_{k=1}^{k=n}\frac{\sin(kx)}{\sin^{k}(x)}$$ I worked on this problem for another couple of hours and now I am stuck again, I would greatly appriciate another hint/help on how to continue. This is what I done with the hint I got: $$\sum_{k=1}^{k=n}\frac{\sin(kx)}{\sin^{k}(x)}=\frac{1}{2i}\sum_{k=1}^{k=n}((\frac{e^{xi}}{\sin(x)})^{k}-(\frac{e^{-xi}}{\sin(x)})^{k})=\frac{1}{2i}(\underbrace{\sum_{k=0}^{k=n}(\frac{e^{xi}}{\sin(x)})^{k}}_{S_{1}}-\underbrace{\sum_{k=0}^{k=n}(\frac{e^{-xi}}{\sin(x)}}_{S_{2}})^{k})  $$ So I tried to calculate both sums and divide by $2i$: $$S_{1}=\sum_{k=0}^{k=n}(\frac{e^{xi}}{\sin(x)})^{k}=1\cdot\frac{(\frac{e^{xi}}{\sin(x)})^{n+1}-1}{\frac{e^{xi}}{\sin(x)}-1}=\frac{\frac{e^{ix(n+1)}-\sin^{n+1}(x)}{\sin^{n+1}(x)}}{\frac{e^{xi}-\sin(x)}{\sin(x)}}=\frac{e^{ix(n+1)}-\sin^{n+1}(x)}{\sin^{n+1}(x)}\cdot\frac{\sin(x)}{e^{xi}-\sin(x)}=\frac{e^{ix(n+1)}-\sin^{n+1}(x)}{\sin^{n}(x)(e^{xi}-\sin(x))}$$ and similirly: $$S_{2}=\sum_{k=0}^{k=n}(\frac{e^{-xi}}{\sin(x)})^{k}=1\cdot\frac{(\frac{e^{-xi}}{\sin(x)})^{n+1}-1}{\frac{e^{-xi}}{\sin(x)}-1}=\frac{\frac{e^{-ix(n+1)}-\sin^{n+1}(x)}{\sin^{n+1}(x)}}{\frac{e^{-xi}-\sin(x)}{\sin(x)}}=\frac{e^{-ix(n+1)}-\sin^{n+1}(x)}{\sin^{n+1}(x)}\cdot\frac{\sin(x)}{e^{-xi}-\sin(x)}=\frac{e^{-ix(n+1)}-\sin^{n+1}(x)}{\sin^{n}(x)(e^{-xi}-\sin(x))}$$ I then tooked the difference and arranged so I can divide by $2i$: $$S_{1}-S_{2}=\frac{e^{ix(n+1)}-\sin^{n+1}(x)}{\sin^{n}(x)(e^{xi}-\sin(x))}-\frac{e^{-ix(n+1)}-\sin^{n+1}(x)}{\sin^{n}(x)(e^{-xi}-\sin(x))}=\frac{(e^{ix(n+1)}-\sin^{n+1}(x))(e^{-xi}-\sin(x))-((e^{xi}-\sin(x))(e^{-ix(n+1)}-\sin^{n+1}(x))}{\sin^{n}(x)(e^{-xi}-\sin(x))(e^{xi}-\sin(x))}$$ $$=\frac{e^{ixn}-\sin(x)e^{ix(n+1)}-\sin^{n+1}(x)e^{-xi}+\sin^{n+2}(x)-e^{-ixn}+\sin^{n+1}(x)e^{xi}+\sin(x)e^{-ix(n+1)}-\sin^{n+2}(x)}{\sin^{n}(x)(e^{-xi}-\sin(x))(e^{xi}-\sin(x))}$$ $$=\frac{e^{ixn}-e^{-ixn}-\sin(x)(e^{ix(n+1)}-e^{-ix(n+1)})+\sin^{n+1}(x)(e^{xi}-e^{-xi})}{\sin^{n}(x)(e^{-xi}-\sin(x))(e^{xi}-\sin(x))}$$ and now I divided by $2i$: $$\implies\frac{s_{1}-s_{2}}{2i}=\frac{\sin(nx)-\sin(x)\sin(x(n+1))+\sin^{n+1}(x)\sin(x)}{\sin^{n}(x)(1-\sin(x)(e^{xi}+e^{-xi})+\sin^{2}(x))}$$ This is where I am stuck, I have high powers of $\sin(x)$ and also $\sin(nx)$ and  $\sin(x(n+1))$ that I do not know what to do with, I keep in mind that since I have to add $1$ to this sum its probably meant to make the expression into something nicer, but if I add $1$ now I will do the oppisate Any help or hint is appriciated!","I asked yesterday for a hint on how to calculate  $$1+\sum_{k=1}^{k=n}\frac{\sin(kx)}{\sin^{k}(x)}$$ I worked on this problem for another couple of hours and now I am stuck again, I would greatly appriciate another hint/help on how to continue. This is what I done with the hint I got: $$\sum_{k=1}^{k=n}\frac{\sin(kx)}{\sin^{k}(x)}=\frac{1}{2i}\sum_{k=1}^{k=n}((\frac{e^{xi}}{\sin(x)})^{k}-(\frac{e^{-xi}}{\sin(x)})^{k})=\frac{1}{2i}(\underbrace{\sum_{k=0}^{k=n}(\frac{e^{xi}}{\sin(x)})^{k}}_{S_{1}}-\underbrace{\sum_{k=0}^{k=n}(\frac{e^{-xi}}{\sin(x)}}_{S_{2}})^{k})  $$ So I tried to calculate both sums and divide by $2i$: $$S_{1}=\sum_{k=0}^{k=n}(\frac{e^{xi}}{\sin(x)})^{k}=1\cdot\frac{(\frac{e^{xi}}{\sin(x)})^{n+1}-1}{\frac{e^{xi}}{\sin(x)}-1}=\frac{\frac{e^{ix(n+1)}-\sin^{n+1}(x)}{\sin^{n+1}(x)}}{\frac{e^{xi}-\sin(x)}{\sin(x)}}=\frac{e^{ix(n+1)}-\sin^{n+1}(x)}{\sin^{n+1}(x)}\cdot\frac{\sin(x)}{e^{xi}-\sin(x)}=\frac{e^{ix(n+1)}-\sin^{n+1}(x)}{\sin^{n}(x)(e^{xi}-\sin(x))}$$ and similirly: $$S_{2}=\sum_{k=0}^{k=n}(\frac{e^{-xi}}{\sin(x)})^{k}=1\cdot\frac{(\frac{e^{-xi}}{\sin(x)})^{n+1}-1}{\frac{e^{-xi}}{\sin(x)}-1}=\frac{\frac{e^{-ix(n+1)}-\sin^{n+1}(x)}{\sin^{n+1}(x)}}{\frac{e^{-xi}-\sin(x)}{\sin(x)}}=\frac{e^{-ix(n+1)}-\sin^{n+1}(x)}{\sin^{n+1}(x)}\cdot\frac{\sin(x)}{e^{-xi}-\sin(x)}=\frac{e^{-ix(n+1)}-\sin^{n+1}(x)}{\sin^{n}(x)(e^{-xi}-\sin(x))}$$ I then tooked the difference and arranged so I can divide by $2i$: $$S_{1}-S_{2}=\frac{e^{ix(n+1)}-\sin^{n+1}(x)}{\sin^{n}(x)(e^{xi}-\sin(x))}-\frac{e^{-ix(n+1)}-\sin^{n+1}(x)}{\sin^{n}(x)(e^{-xi}-\sin(x))}=\frac{(e^{ix(n+1)}-\sin^{n+1}(x))(e^{-xi}-\sin(x))-((e^{xi}-\sin(x))(e^{-ix(n+1)}-\sin^{n+1}(x))}{\sin^{n}(x)(e^{-xi}-\sin(x))(e^{xi}-\sin(x))}$$ $$=\frac{e^{ixn}-\sin(x)e^{ix(n+1)}-\sin^{n+1}(x)e^{-xi}+\sin^{n+2}(x)-e^{-ixn}+\sin^{n+1}(x)e^{xi}+\sin(x)e^{-ix(n+1)}-\sin^{n+2}(x)}{\sin^{n}(x)(e^{-xi}-\sin(x))(e^{xi}-\sin(x))}$$ $$=\frac{e^{ixn}-e^{-ixn}-\sin(x)(e^{ix(n+1)}-e^{-ix(n+1)})+\sin^{n+1}(x)(e^{xi}-e^{-xi})}{\sin^{n}(x)(e^{-xi}-\sin(x))(e^{xi}-\sin(x))}$$ and now I divided by $2i$: $$\implies\frac{s_{1}-s_{2}}{2i}=\frac{\sin(nx)-\sin(x)\sin(x(n+1))+\sin^{n+1}(x)\sin(x)}{\sin^{n}(x)(1-\sin(x)(e^{xi}+e^{-xi})+\sin^{2}(x))}$$ This is where I am stuck, I have high powers of $\sin(x)$ and also $\sin(nx)$ and  $\sin(x(n+1))$ that I do not know what to do with, I keep in mind that since I have to add $1$ to this sum its probably meant to make the expression into something nicer, but if I add $1$ now I will do the oppisate Any help or hint is appriciated!",,"['complex-analysis', 'trigonometry', 'complex-numbers']"
4,How to deal with $|f(z)|^2$ under integral,How to deal with  under integral,|f(z)|^2,"Suppose we have $f(z)=\sum\limits_{n=0}^\infty c_nz^n$ for $|z|<R $ Prove that $\frac{1}{2\pi}\int_0^{2\pi}|f(re^{i\theta})|^2d\theta=\sum\limits_{n=0}^\infty|c_n|^2r^{2n}$, $r<R$ Got stuck for this proof ... I have no idea what happens to the series when putting $|f(e^{i\theta})|^2$ under the integral sign ....","Suppose we have $f(z)=\sum\limits_{n=0}^\infty c_nz^n$ for $|z|<R $ Prove that $\frac{1}{2\pi}\int_0^{2\pi}|f(re^{i\theta})|^2d\theta=\sum\limits_{n=0}^\infty|c_n|^2r^{2n}$, $r<R$ Got stuck for this proof ... I have no idea what happens to the series when putting $|f(e^{i\theta})|^2$ under the integral sign ....",,['complex-analysis']
5,Partial fraction of $\sec(z)$ from $\frac{\pi}{\sin (\pi z)}$,Partial fraction of  from,\sec(z) \frac{\pi}{\sin (\pi z)},"Given $$\frac{\pi}{\sin (\pi z)}=\sum_{n=-\infty}^\infty (-1)^n \frac{1}{z-n},$$ is there a fast way to get $$\sec(z)=\sum_{n=1}^\infty \frac{(-1)^n(2n-1)\pi}{z^2-(n-1/2)^2\pi^2}?$$ I've tried computing it, but I get a huge mess. Would anybody have some ideas? Thanks in advance.","Given $$\frac{\pi}{\sin (\pi z)}=\sum_{n=-\infty}^\infty (-1)^n \frac{1}{z-n},$$ is there a fast way to get $$\sec(z)=\sum_{n=1}^\infty \frac{(-1)^n(2n-1)\pi}{z^2-(n-1/2)^2\pi^2}?$$ I've tried computing it, but I get a huge mess. Would anybody have some ideas? Thanks in advance.",,"['complex-analysis', 'partial-fractions']"
6,"If composition of one function and the other holomorphic function is holomorphic, then the other should be holomorphic?","If composition of one function and the other holomorphic function is holomorphic, then the other should be holomorphic?",,"Actually, this is an exercise on Rudin's Real and Complex Analysis : Suppose $\Omega_1, \Omega_2$ are plane regions, $f$ and $g$ are nonconstant complex functions in $\Omega_1$, $\Omega_2$ resp. and $f(\Omega_1) \subset \Omega_2$, so that $h=g \circ f$ can be defined. If we know that $f$ and $h$ are holomorphic, is $g$ holomorphic as well? What if we know that $g$ and $h$ are holomorphic? The easiest example will be a constant function, but the problem does not allow this. I found it difficult to find counter examples. Can anyone help me?","Actually, this is an exercise on Rudin's Real and Complex Analysis : Suppose $\Omega_1, \Omega_2$ are plane regions, $f$ and $g$ are nonconstant complex functions in $\Omega_1$, $\Omega_2$ resp. and $f(\Omega_1) \subset \Omega_2$, so that $h=g \circ f$ can be defined. If we know that $f$ and $h$ are holomorphic, is $g$ holomorphic as well? What if we know that $g$ and $h$ are holomorphic? The easiest example will be a constant function, but the problem does not allow this. I found it difficult to find counter examples. Can anyone help me?",,['complex-analysis']
7,What are the subsets of the unit circle that can be the points in which a power series is convergent?,What are the subsets of the unit circle that can be the points in which a power series is convergent?,,"Let $A\subset\Bbb C$ be a subset of the unit circle. Consider the following condition on $A$. Cond. There exists a sequence $\{a_i\}_{i=1}^\infty$ of complex numbers such that $$\sum_{n=1}^\infty a_nz^n$$ is a power series with radius of convergence $1,$ and $A$ is exactly the subset of the unit circle in which the series converges. Are there any interesting conditions on a subset $A$ of the unit circle which imply, are implied by or are equivalent to Cond. ? I think all finite subsets of the circle have this property. What about the countable subsets? Does it have anything to do with measurability?","Let $A\subset\Bbb C$ be a subset of the unit circle. Consider the following condition on $A$. Cond. There exists a sequence $\{a_i\}_{i=1}^\infty$ of complex numbers such that $$\sum_{n=1}^\infty a_nz^n$$ is a power series with radius of convergence $1,$ and $A$ is exactly the subset of the unit circle in which the series converges. Are there any interesting conditions on a subset $A$ of the unit circle which imply, are implied by or are equivalent to Cond. ? I think all finite subsets of the circle have this property. What about the countable subsets? Does it have anything to do with measurability?",,"['complex-analysis', 'power-series', 'circles']"
8,$| \Gamma (iy) | = \sqrt{ \pi / \sinh (\pi y) } $ how to prove it?,how to prove it?,| \Gamma (iy) | = \sqrt{ \pi / \sinh (\pi y) } ,My Lecturer put it as a corollary of the theorem $\Gamma (z) \Gamma (1-z) = \pi/ \sin (\pi z) $. So how do I prove $| \Gamma (iy) | = \sqrt{ \pi / y \sinh (\pi y) } $ how to prove it? from the above theorem? Could you give me some hints? Thanks.,My Lecturer put it as a corollary of the theorem $\Gamma (z) \Gamma (1-z) = \pi/ \sin (\pi z) $. So how do I prove $| \Gamma (iy) | = \sqrt{ \pi / y \sinh (\pi y) } $ how to prove it? from the above theorem? Could you give me some hints? Thanks.,,['complex-analysis']
9,Evaluating the contour integral: $\oint_C \frac{\sin 2z}{(6z-\pi)^3}dz$,Evaluating the contour integral:,\oint_C \frac{\sin 2z}{(6z-\pi)^3}dz,"I am trying to evaluate the following integral, but don't know how to take the coefficient of $z$ out of the parenthesis to get it into the Cauchy integral form. Any help is appreciated. $$ \oint_C \frac{\sin 2z}{(6z-\pi)^3}dz$$","I am trying to evaluate the following integral, but don't know how to take the coefficient of $z$ out of the parenthesis to get it into the Cauchy integral form. Any help is appreciated. $$ \oint_C \frac{\sin 2z}{(6z-\pi)^3}dz$$",,"['complex-analysis', 'contour-integration']"
10,Computing a Laurent series,Computing a Laurent series,,"Let $$f(z) = \frac{1}{(2z-1)(z-3)} $$. Compute the Laurent series about the point z = 1 in the annular domain $$ \frac{1}{2} < |z-1| < 2$$ My attempt: I broke f(z) up into the partial fraction decomposition: $$ -\frac{2}{5(2z-1)} + \frac{1}{5(z-3)} = -\frac{2}{5}*\frac{1}{(1-\frac{(z+\frac{1}{2})}{2})} +\frac{1}{5}*\frac{1}{1-(z-2)} = $$ $$-\frac{2}{5}\sum_{n=0}^\infty(-1)^{n}\frac{(z+1)^{n}}{2^n}-\frac{1}{5}\sum_{n=0}^\infty(z-2)^n $$ And that was my answer. But I was told I was wrong, and I'm not sure where I went wrong in there. So if someone could point out where I went wrong, it would be greatly appreciated!","Let $$f(z) = \frac{1}{(2z-1)(z-3)} $$. Compute the Laurent series about the point z = 1 in the annular domain $$ \frac{1}{2} < |z-1| < 2$$ My attempt: I broke f(z) up into the partial fraction decomposition: $$ -\frac{2}{5(2z-1)} + \frac{1}{5(z-3)} = -\frac{2}{5}*\frac{1}{(1-\frac{(z+\frac{1}{2})}{2})} +\frac{1}{5}*\frac{1}{1-(z-2)} = $$ $$-\frac{2}{5}\sum_{n=0}^\infty(-1)^{n}\frac{(z+1)^{n}}{2^n}-\frac{1}{5}\sum_{n=0}^\infty(z-2)^n $$ And that was my answer. But I was told I was wrong, and I'm not sure where I went wrong in there. So if someone could point out where I went wrong, it would be greatly appreciated!",,['complex-analysis']
11,Applications of complex variables beyond undergrad syllabus,Applications of complex variables beyond undergrad syllabus,,"So complex numbers solve all polynomials, appear as eigenvalues, appear in intermediate calculations in solving cubics, relate trig to hyperbolic functions, can be used to contour integrate real functions more easily, can represent fourier series more compactly, describe calculations about wave phenomena, used in potential theory and conformal maps. But what are some unusual, not well known or just advanced applications of complex numbers that one would be unlikely to encounter in an undergraduate mathematics degree ?","So complex numbers solve all polynomials, appear as eigenvalues, appear in intermediate calculations in solving cubics, relate trig to hyperbolic functions, can be used to contour integrate real functions more easily, can represent fourier series more compactly, describe calculations about wave phenomena, used in potential theory and conformal maps. But what are some unusual, not well known or just advanced applications of complex numbers that one would be unlikely to encounter in an undergraduate mathematics degree ?",,"['complex-analysis', 'complex-numbers', 'big-list']"
12,$| f(z)| \le A + B \log {| z|}$ like inequality,like inequality,| f(z)| \le A + B \log {| z|},"Let $f$ be an entire non-constant complex function and let $A$ and $B$ be given positive real constants. Is it possible that $|f(z)| \le A + B\log{| z|}$ for all complex $z$ such that $| z| \ge 1$ ? I've been trying to solve using the fact that since $f$ is continuous in $\{z; |z|\le 1\}$ there exists $M=\sup\{ |f(z); |z|\le 1\}$, and then somehow use the Cauchy's integral formula for derivatives. Any ideas? Thanks","Let $f$ be an entire non-constant complex function and let $A$ and $B$ be given positive real constants. Is it possible that $|f(z)| \le A + B\log{| z|}$ for all complex $z$ such that $| z| \ge 1$ ? I've been trying to solve using the fact that since $f$ is continuous in $\{z; |z|\le 1\}$ there exists $M=\sup\{ |f(z); |z|\le 1\}$, and then somehow use the Cauchy's integral formula for derivatives. Any ideas? Thanks",,['complex-analysis']
13,Integral of $\frac{1}{\sin z}$ along a path,Integral of  along a path,\frac{1}{\sin z},"Suppose $\gamma$ is a simple, closed path, with $0$ in its interior and $\{\pi n:n\in\mathbb{Z}\setminus\{0\}\}\subset\mathbb{C}\setminus|\gamma|$. Find    $$ \int_{\gamma} \frac{1}{\sin z} dz  $$ Perhaps it's a simple question of Cauchy's formula or Cauchy's theorem. Thanks.","Suppose $\gamma$ is a simple, closed path, with $0$ in its interior and $\{\pi n:n\in\mathbb{Z}\setminus\{0\}\}\subset\mathbb{C}\setminus|\gamma|$. Find    $$ \int_{\gamma} \frac{1}{\sin z} dz  $$ Perhaps it's a simple question of Cauchy's formula or Cauchy's theorem. Thanks.",,['complex-analysis']
14,Polynomial of same degree,Polynomial of same degree,,"Let $p(z)$ and $q(z)$ are two polynomial of same degree and zeroes of $p(z)$ and $q(z)$ are inside open unit disc, $|p(z)|=|q(z)|$ on unit circle then show that $p(z)=\lambda q(z)$ where $|\lambda|=1$. Please just give a hint not the whole solution. Thank you.","Let $p(z)$ and $q(z)$ are two polynomial of same degree and zeroes of $p(z)$ and $q(z)$ are inside open unit disc, $|p(z)|=|q(z)|$ on unit circle then show that $p(z)=\lambda q(z)$ where $|\lambda|=1$. Please just give a hint not the whole solution. Thank you.",,"['complex-analysis', 'polynomials']"
15,The definition of $f(z)$ being analytic at point $\infty$,The definition of  being analytic at point,f(z) \infty,"Consider this function $f(z) = \frac{1}{1+z}$. We can define $f(\infty) = \lim_{z \rightarrow \infty}{f(z)}$, which is zero for this case. Since $f(\frac{1}{t}) \rightarrow \frac{t}{t+1}$ is analytic at point $t=0$, we can deduce that $f(z)$ is analytic at $z=\infty$. On the other hand, if we consider the type of singularity at $t=0$ of $f(\frac{1}{t})$. There is singularity because of the existence of $\frac{1}{t}$ but the singularity is removable. So the singularity at $z=\infty$ is removable. The definition is $f(\infty) = \lim_{z \rightarrow \infty}{f(z)}$. So how we discuss the singularity and think of it as removable if the value of $f(\infty)$ is taken from the limit?","Consider this function $f(z) = \frac{1}{1+z}$. We can define $f(\infty) = \lim_{z \rightarrow \infty}{f(z)}$, which is zero for this case. Since $f(\frac{1}{t}) \rightarrow \frac{t}{t+1}$ is analytic at point $t=0$, we can deduce that $f(z)$ is analytic at $z=\infty$. On the other hand, if we consider the type of singularity at $t=0$ of $f(\frac{1}{t})$. There is singularity because of the existence of $\frac{1}{t}$ but the singularity is removable. So the singularity at $z=\infty$ is removable. The definition is $f(\infty) = \lim_{z \rightarrow \infty}{f(z)}$. So how we discuss the singularity and think of it as removable if the value of $f(\infty)$ is taken from the limit?",,['complex-analysis']
16,Residue at $z=\infty$,Residue at,z=\infty,"I'm a bit confused at when to use the calculation of a residue at $z=\infty$ to calculate an integral of a function. Here is the example my book uses: In the positively oriented circle $|z-2|=1$, the integral of $$\frac{5z-2}{z(z-1)}$$ yields two residues, which give a value of $10\pi i$ for the integral, using the Cauchy Residue Theorem. I've got that down. The book later, calculates the residue at infinity, yielding the same answer... - one residue calculation! I can't seem to find what I'm missing here... Why is it that when considering this one bound, $2 \lt |z-2|\lt\infty$, we can use the residue at infinity to find the value of the integral... yet at the same time calculate a residue in three different bounds using the Cauchy Integral Theorem and find the same integral value? Thanks!","I'm a bit confused at when to use the calculation of a residue at $z=\infty$ to calculate an integral of a function. Here is the example my book uses: In the positively oriented circle $|z-2|=1$, the integral of $$\frac{5z-2}{z(z-1)}$$ yields two residues, which give a value of $10\pi i$ for the integral, using the Cauchy Residue Theorem. I've got that down. The book later, calculates the residue at infinity, yielding the same answer... - one residue calculation! I can't seem to find what I'm missing here... Why is it that when considering this one bound, $2 \lt |z-2|\lt\infty$, we can use the residue at infinity to find the value of the integral... yet at the same time calculate a residue in three different bounds using the Cauchy Integral Theorem and find the same integral value? Thanks!",,"['complex-analysis', 'residue-calculus']"
17,holomorphic function is real analytic?,holomorphic function is real analytic?,,"$f$ is a holomorphic function on $\mathbb C^n$. If we regard $f$ as a function $F$ from $\mathbb R^{2n} \to \mathbb R^2$, is it necessarily that $F$ is real analytic?","$f$ is a holomorphic function on $\mathbb C^n$. If we regard $f$ as a function $F$ from $\mathbb R^{2n} \to \mathbb R^2$, is it necessarily that $F$ is real analytic?",,"['real-analysis', 'complex-analysis', 'several-complex-variables']"
18,Show the existence of a complex differentiable function defined outside $|z|=4$ with derivative $\frac{z}{(z-1)(z-2)(z-3)}$,Show the existence of a complex differentiable function defined outside  with derivative,|z|=4 \frac{z}{(z-1)(z-2)(z-3)},"My attempt I wrote the given function as a sum of rational functions (via partial fraction decomposition), namely  $$ \frac{z}{(z-1)(z-2)(z-3)} = \frac{1/2}{z-1} + \frac{-2}{z-2} + \frac{3/2}{z-3}. $$ This then allows me to formally integrate the function. In particular,  I find that  $$ F(z) = 1/2 \log(z-1) - 2 \log(z-2) + 3/2 \log(z-3) $$ is a complex differentiable function on the set $\Omega = \{z \in \mathbb{C}: |z| > 4\}$ with the derivative we want.  So this seems  to answer the question, as far as I can tell. The question then asks if there is a complex differentiable function on $\Omega$ whose derivative is  $$ \frac{z^2}{(z-1)(z-2)(z-3)}. $$ Again, I can write this as a sum of rational functions and formally integrate to obtain the desired function on $\Omega$ with this particular derivative. Woo hoo. My question Is there more to this question that I'm not seeing? I was also able to write the first given derivative as a geometric series and show that this series converged for all $|z| > 3$, but I don't believe this helps me to say anything about the complex integral of this function.  In the case that it does, perhaps this is  an alternative avenue to head down? Any insight/confirmation that I'm not overlooking something significant would be much appreciated.  Note that this an old question  that often appears on study guides for complex analysis comps (one being my own), so that's in part why I'm thinking (hoping?) there may be something deeper here. For possible historical context, the question seems to date back to 1978 (see number 7 here): http://math.rice.edu/~idu/Sp05/cx_ucb.pdf Thanks for your time.","My attempt I wrote the given function as a sum of rational functions (via partial fraction decomposition), namely  $$ \frac{z}{(z-1)(z-2)(z-3)} = \frac{1/2}{z-1} + \frac{-2}{z-2} + \frac{3/2}{z-3}. $$ This then allows me to formally integrate the function. In particular,  I find that  $$ F(z) = 1/2 \log(z-1) - 2 \log(z-2) + 3/2 \log(z-3) $$ is a complex differentiable function on the set $\Omega = \{z \in \mathbb{C}: |z| > 4\}$ with the derivative we want.  So this seems  to answer the question, as far as I can tell. The question then asks if there is a complex differentiable function on $\Omega$ whose derivative is  $$ \frac{z^2}{(z-1)(z-2)(z-3)}. $$ Again, I can write this as a sum of rational functions and formally integrate to obtain the desired function on $\Omega$ with this particular derivative. Woo hoo. My question Is there more to this question that I'm not seeing? I was also able to write the first given derivative as a geometric series and show that this series converged for all $|z| > 3$, but I don't believe this helps me to say anything about the complex integral of this function.  In the case that it does, perhaps this is  an alternative avenue to head down? Any insight/confirmation that I'm not overlooking something significant would be much appreciated.  Note that this an old question  that often appears on study guides for complex analysis comps (one being my own), so that's in part why I'm thinking (hoping?) there may be something deeper here. For possible historical context, the question seems to date back to 1978 (see number 7 here): http://math.rice.edu/~idu/Sp05/cx_ucb.pdf Thanks for your time.",,"['complex-analysis', 'integration', 'partial-fractions', 'complex-integration']"
19,Showing that a curve is not rectifiable if its arc length is not a continuous function,Showing that a curve is not rectifiable if its arc length is not a continuous function,,"This is a (translated) proof from a textbook of the fact that arc length of a rectifiable curve is a continuous function. Let $\phi:[T_0,T_1]\rightarrow\mathbb C$ be a function whose real part and imaginary part are continuous, and $C$ be a curve represented by $\phi$. Suppose $C$ is rectifiable.  Define $f:[T_0,T_1]\rightarrow\mathbb R$ by $f(t) = L(C|[T_0,t])$, where $L(C|I)$ is the arc length of $C$ restricted to the interval $I$.  To show by contradiction that $f$ is continuous, assume $f$ is not continuous at $t_0\in [T_0, T_1]$.  Since $f$ is monotonously increasing, either of   $ \lim_{t\rightarrow t_0-0} f(t) < f(t_0) $ or $\lim_{t\rightarrow t_0+0} f(t) > f(t_0)$ holds. WLOG we may assume the former holds.  Here $t_0 > t$. Let $\epsilon_0 = f(t_0) - \lim_{t\rightarrow t_0-0}f(t)$.  By definition, there exists an infinite number of $t_j < \tilde{t_j} < t_{j+1} < \tilde {t}_{j+1}\quad(j = 1,2,\dots)$ s.t. $L(C|[t_j,\tilde{t}_j])>\epsilon_0/2$.  Then we have $L(C) = +\infty$.  Contradiction. I can't figure out why the sentence that begin with ""By definition"" is true.  Why are there such $t_j$'s? EDIT: In the book, $L(C)$ is defined to be the supremum (possibly $+\infty$) of $\sum_{j=1}^{n}|\phi(s_j)-\phi(s_{j-1})|$ for any partition $T_0 = s_0 < s_1 < \dots < s_n = T_1$.  $C$ is rectifiable iff $L(C) <\infty$.","This is a (translated) proof from a textbook of the fact that arc length of a rectifiable curve is a continuous function. Let $\phi:[T_0,T_1]\rightarrow\mathbb C$ be a function whose real part and imaginary part are continuous, and $C$ be a curve represented by $\phi$. Suppose $C$ is rectifiable.  Define $f:[T_0,T_1]\rightarrow\mathbb R$ by $f(t) = L(C|[T_0,t])$, where $L(C|I)$ is the arc length of $C$ restricted to the interval $I$.  To show by contradiction that $f$ is continuous, assume $f$ is not continuous at $t_0\in [T_0, T_1]$.  Since $f$ is monotonously increasing, either of   $ \lim_{t\rightarrow t_0-0} f(t) < f(t_0) $ or $\lim_{t\rightarrow t_0+0} f(t) > f(t_0)$ holds. WLOG we may assume the former holds.  Here $t_0 > t$. Let $\epsilon_0 = f(t_0) - \lim_{t\rightarrow t_0-0}f(t)$.  By definition, there exists an infinite number of $t_j < \tilde{t_j} < t_{j+1} < \tilde {t}_{j+1}\quad(j = 1,2,\dots)$ s.t. $L(C|[t_j,\tilde{t}_j])>\epsilon_0/2$.  Then we have $L(C) = +\infty$.  Contradiction. I can't figure out why the sentence that begin with ""By definition"" is true.  Why are there such $t_j$'s? EDIT: In the book, $L(C)$ is defined to be the supremum (possibly $+\infty$) of $\sum_{j=1}^{n}|\phi(s_j)-\phi(s_{j-1})|$ for any partition $T_0 = s_0 < s_1 < \dots < s_n = T_1$.  $C$ is rectifiable iff $L(C) <\infty$.",,"['calculus', 'real-analysis', 'complex-analysis']"
20,An addition property of Weierstrass $\wp$,An addition property of Weierstrass,\wp,"I want to show $$ \left( \begin{array}{ccccc} &1 &\wp(v) &\wp'(v) \\ &1 &\wp(w) &\wp'(w) \\ &1 &\wp(v+w) &-\wp'(v+w) \end{array} \right)=0 $$ where $\wp$ denotes the Weierstrass elliptic function.","I want to show $$ \left( \begin{array}{ccccc} &1 &\wp(v) &\wp'(v) \\ &1 &\wp(w) &\wp'(w) \\ &1 &\wp(v+w) &-\wp'(v+w) \end{array} \right)=0 $$ where $\wp$ denotes the Weierstrass elliptic function.",,"['complex-analysis', 'special-functions', 'elliptic-functions']"
21,Confusion about partial differentiation and Cauchy-Riemann equations,Confusion about partial differentiation and Cauchy-Riemann equations,,"In complex analysis class (using Stein's Complex Analysis), we learned about the derivation of the Cauchy-Riemann equations, and that made sense. We take a holomorphic $f : O \rightarrow \mathbb{C}$, where $O \subset \mathbb{C}$ is open, and split it as $f(x + iy) = u(x, y) + i v(x, y)$. Then after some computation we arrive at $\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}$ and $\frac{\partial v}{\partial x} = -\frac{\partial u}{\partial y}$. However, I haven't learned about multivariable calculus so I'm new to partial differentiation. The above makes sense to me, but in an exercise they ask us to prove that, in polar form, these equations take the form $\frac{\partial u}{\partial r} = \frac{1}{r} \frac{\partial v}{\partial \theta}$ and $\frac{1}{r} \frac{\partial u}{\partial \theta} = - \frac{\partial v}{\partial r}$. Now I'm confused. Do they mean the same $u$ and $v$ we had been using before? Maybe they similarly define $f(re^{i\theta}) = u(r, \theta) e^{i v(r, \theta)}$ and want me to derive the equations for that? The first option doesn't make any sense to me and I didn't succeed at attempting the second. I think I need a pretty thorough clarification on this issue, can anyone help?","In complex analysis class (using Stein's Complex Analysis), we learned about the derivation of the Cauchy-Riemann equations, and that made sense. We take a holomorphic $f : O \rightarrow \mathbb{C}$, where $O \subset \mathbb{C}$ is open, and split it as $f(x + iy) = u(x, y) + i v(x, y)$. Then after some computation we arrive at $\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}$ and $\frac{\partial v}{\partial x} = -\frac{\partial u}{\partial y}$. However, I haven't learned about multivariable calculus so I'm new to partial differentiation. The above makes sense to me, but in an exercise they ask us to prove that, in polar form, these equations take the form $\frac{\partial u}{\partial r} = \frac{1}{r} \frac{\partial v}{\partial \theta}$ and $\frac{1}{r} \frac{\partial u}{\partial \theta} = - \frac{\partial v}{\partial r}$. Now I'm confused. Do they mean the same $u$ and $v$ we had been using before? Maybe they similarly define $f(re^{i\theta}) = u(r, \theta) e^{i v(r, \theta)}$ and want me to derive the equations for that? The first option doesn't make any sense to me and I didn't succeed at attempting the second. I think I need a pretty thorough clarification on this issue, can anyone help?",,['complex-analysis']
22,Is line integration a generalization of the definite integral in $\mathbb{R}$?,Is line integration a generalization of the definite integral in ?,\mathbb{R},"Recently I've been writing integrals in the following way, for example $$\int\limits_{[0,1]} {{t^{y - 1}}{{\left( {1 - t} \right)}^{x - 1}}dt} $$ instead of $$\int\limits_0^1 {{t^{y - 1}}{{\left( {1 - t} \right)}^{x - 1}}dt} $$ or $$\int\limits_{\mathbb{R}} {\frac{1}{{1 + {t^2}}}dt} $$ instead of $$\int\limits_{ - \infty }^\infty  {\frac{1}{{1 + {t^2}}}dt} $$ I did this because I thought the new notation highlights the fact that we're integrating over a line interval and not only in the extremes of the interval, so as no to ""degrade"" the definite integral to $$\int\limits_a^b {f\left( t \right)dt}  = F\left( b \right) - F\left( a \right)$$ Although I know virtually nothing about it, I remembered that in complex integration you integrate over a line, a curve in $\mathbb{R}^2$ as opposed to integrating in $\mathbb{R}$ (an interval). It also rang a bell that integrating over $(a,b)$ is the opposite as integrating over $(b,a)$ (i.e. taking the inverse ""path"" over the line) and I'm guessing this also happens in complex integration, i.e, the path you take changes the value of the integral. So that's my doubt, is complex integration a generalization of the common integration in the real domain?","Recently I've been writing integrals in the following way, for example $$\int\limits_{[0,1]} {{t^{y - 1}}{{\left( {1 - t} \right)}^{x - 1}}dt} $$ instead of $$\int\limits_0^1 {{t^{y - 1}}{{\left( {1 - t} \right)}^{x - 1}}dt} $$ or $$\int\limits_{\mathbb{R}} {\frac{1}{{1 + {t^2}}}dt} $$ instead of $$\int\limits_{ - \infty }^\infty  {\frac{1}{{1 + {t^2}}}dt} $$ I did this because I thought the new notation highlights the fact that we're integrating over a line interval and not only in the extremes of the interval, so as no to ""degrade"" the definite integral to $$\int\limits_a^b {f\left( t \right)dt}  = F\left( b \right) - F\left( a \right)$$ Although I know virtually nothing about it, I remembered that in complex integration you integrate over a line, a curve in $\mathbb{R}^2$ as opposed to integrating in $\mathbb{R}$ (an interval). It also rang a bell that integrating over $(a,b)$ is the opposite as integrating over $(b,a)$ (i.e. taking the inverse ""path"" over the line) and I'm guessing this also happens in complex integration, i.e, the path you take changes the value of the integral. So that's my doubt, is complex integration a generalization of the common integration in the real domain?",,"['complex-analysis', 'integration']"
23,Fundamental domain for the group of transformations generated by $\tau \mapsto \tau + 2$ and $\tau \mapsto -1/\tau$,Fundamental domain for the group of transformations generated by  and,\tau \mapsto \tau + 2 \tau \mapsto -1/\tau,"Define the following fractional linear transformations (acting on elements of $\mathbb C$): $T_2:\tau \mapsto \tau + 2$ $S: \tau \mapsto -1/\tau$ Let $G$ be the group of transformations generated by $T_2$ and $S$. In my complex analysis textbook, there is a proof that for each point $\tau$ in the upper half-plane, there exists a mapping $g \in G$ such that $g(\tau) \in \mathcal F = \{\tau \in \mathbb C: |\Re(\tau)| \leq 1 \text{ and } \Im(\tau) \geq 0 \text{ and } |\tau| \geq 1 \}$. Suppose we are given some point $\tau$. The proof begins by choosing $g \in G$ such that $\Im(g(\tau))$ is maximal. My question is, how do we know that there exists such a mapping $g$? (What if, for every $g \in G$, we can find a $g_1 \in G$ such that $\Im(g(\tau)) < \Im(g_1(\tau))$?","Define the following fractional linear transformations (acting on elements of $\mathbb C$): $T_2:\tau \mapsto \tau + 2$ $S: \tau \mapsto -1/\tau$ Let $G$ be the group of transformations generated by $T_2$ and $S$. In my complex analysis textbook, there is a proof that for each point $\tau$ in the upper half-plane, there exists a mapping $g \in G$ such that $g(\tau) \in \mathcal F = \{\tau \in \mathbb C: |\Re(\tau)| \leq 1 \text{ and } \Im(\tau) \geq 0 \text{ and } |\tau| \geq 1 \}$. Suppose we are given some point $\tau$. The proof begins by choosing $g \in G$ such that $\Im(g(\tau))$ is maximal. My question is, how do we know that there exists such a mapping $g$? (What if, for every $g \in G$, we can find a $g_1 \in G$ such that $\Im(g(\tau)) < \Im(g_1(\tau))$?",,['complex-analysis']
24,Complex Analysis: Conformal Maps,Complex Analysis: Conformal Maps,,"Problem I'm trying to find a Möbius transformation to map the region $$\{z:|z-i|<\sqrt2 \text{ and } |z+i|<\sqrt{2} \}$$ onto $$\left\{z=re^{i\theta}:0<\theta<\frac{\pi}{2}, 0<r\right\}$$ Any help would be appreciated. Regards.",Problem I'm trying to find a Möbius transformation to map the region onto Any help would be appreciated. Regards.,"\{z:|z-i|<\sqrt2 \text{ and } |z+i|<\sqrt{2} \} \left\{z=re^{i\theta}:0<\theta<\frac{\pi}{2}, 0<r\right\}","['geometry', 'complex-analysis']"
25,"How to show $\mu(\overline{\mathbb{R}}) = \overline{\mathbb{R}}$ iff $a, b, c, d \in \mathbb{R}$ for $\mu(z) = \frac{az+b}{cz+d}$?",How to show  iff  for ?,"\mu(\overline{\mathbb{R}}) = \overline{\mathbb{R}} a, b, c, d \in \mathbb{R} \mu(z) = \frac{az+b}{cz+d}","Let $\mu(z) = \frac{az+b}{cz+d}$ be a Möbius transformation. I want to show that $$\mu(\overline{\mathbb{R}}) = \overline{\mathbb{R}} \iff a, b, c, d \in \mathbb{R}.$$ What would be an elegant, and hopefully short way to prove this statement? I have tried to show one implication first but then I arrive at a lot of different cases and I don't think the way to show the statement is to make an awkward case-by-case analysis. Other exercises given by the author of the lecture notes are much easier - e.g. I showed that $\mu(\mathbb{D}) = \mathbb{D}$ and $\mu(0)=0$ if and only if $\mu(z) = \zeta z$ for $\zeta \in S^1$, where $\mathbb{D}$ denotes the open unit disk in $\mathbb{C}$ and $S^1$ denotes the unit circle in $\mathbb{C}$ - which is why I believe there must be a more elegant way to approach this problem. Thanks for any answers in advance. EDIT: As Chris and yoyo have pointed out, the statement is not correct. The correct statement would probably be (correct me if I'm wrong again) $$\mu(\overline{\mathbb{R}}) = \overline{\mathbb{R}} \iff a, b, c, d \in \lambda \mathbb{R}$$ for some constant $\lambda \in \mathbb{C}$.","Let $\mu(z) = \frac{az+b}{cz+d}$ be a Möbius transformation. I want to show that $$\mu(\overline{\mathbb{R}}) = \overline{\mathbb{R}} \iff a, b, c, d \in \mathbb{R}.$$ What would be an elegant, and hopefully short way to prove this statement? I have tried to show one implication first but then I arrive at a lot of different cases and I don't think the way to show the statement is to make an awkward case-by-case analysis. Other exercises given by the author of the lecture notes are much easier - e.g. I showed that $\mu(\mathbb{D}) = \mathbb{D}$ and $\mu(0)=0$ if and only if $\mu(z) = \zeta z$ for $\zeta \in S^1$, where $\mathbb{D}$ denotes the open unit disk in $\mathbb{C}$ and $S^1$ denotes the unit circle in $\mathbb{C}$ - which is why I believe there must be a more elegant way to approach this problem. Thanks for any answers in advance. EDIT: As Chris and yoyo have pointed out, the statement is not correct. The correct statement would probably be (correct me if I'm wrong again) $$\mu(\overline{\mathbb{R}}) = \overline{\mathbb{R}} \iff a, b, c, d \in \lambda \mathbb{R}$$ for some constant $\lambda \in \mathbb{C}$.",,"['geometry', 'complex-analysis', 'projective-geometry']"
26,Help on normal family,Help on normal family,,"Suppose, I have a function $f(z)=\xi z$ where $|\xi|=1$ but $\xi$ is not a root of unity. Then, from the fact that the $n$-th iteration $f^{\circ n}(z)$, $z \in \mathbb{C}$, is dense on the circle around $0$ and radius $|z|$, how can I can deduce that $\{f^{\circ n}\}$ is a normal family? Thanks for any help!","Suppose, I have a function $f(z)=\xi z$ where $|\xi|=1$ but $\xi$ is not a root of unity. Then, from the fact that the $n$-th iteration $f^{\circ n}(z)$, $z \in \mathbb{C}$, is dense on the circle around $0$ and radius $|z|$, how can I can deduce that $\{f^{\circ n}\}$ is a normal family? Thanks for any help!",,['complex-analysis']
27,"Is there a $z$ for which $z$, $1+i$, $(1+i)z$ and $e^z$ are collinear?","Is there a  for which , ,  and  are collinear?",z z 1+i (1+i)z e^z,"Is there a $z$ for which $z$, $1+i$, $(1+i)z$ and $e^z$ are collinear? There is a close call around $z = .18 + 1.09i$ but I'd like to see a mathematical solution.","Is there a $z$ for which $z$, $1+i$, $(1+i)z$ and $e^z$ are collinear? There is a close call around $z = .18 + 1.09i$ but I'd like to see a mathematical solution.",,"['complex-analysis', 'complex-numbers']"
28,Some questions on hyperelliptic compact Riemann surfaces,Some questions on hyperelliptic compact Riemann surfaces,,"For genus > 1 hyperelliptic Riemann surface the definition guarantees that there is a degree 2 map from that to $\mathbb{P}^1$. Under this map the inverse image of the ""point at infinity"" has to be two distinct points - why? Why couldn't it be that the map has a double pole? (..as wanted that would also not have the injectivity and hence biholomorphism crisis..) Is the canonical map on a compact Riemann surface holomorphic? (..for the hyperelliptic compact Riemann surfaces one knows that it factors as a composition of the definition guaranteed degree 2 map (to $\mathbb{P}^1$) and the rational canonical curve and hence its not holomorphic..) The above question is motivated from my difficulty in piecing together the argument that all compact Riemann surfaces at genus 2 are hyperelliptic.  To prove that one wants to show that there the canonical curve is not injective.  For a genus 2 compact Riemann surface the canonical map is a map to  $\mathbb{P}^1$ (which is genus 0) which if were injective then something absurd would have to happen. But what? One possibile way I can think of the argument is if the canonical map were holomorphic then one could say that this map is surjective - since continuous image of a compact surface is compact and hence its closed in $\mathbb{P}^1$ and but its also open by being the image of a holomorphic map and hence by the connectedness of $\mathbb{P}^1$ it is surjective. So the canonical map for genus 2 Riemann surface would be a bijective holomorphism and which would in turn imply that topologically we have a homeomorphism from genus 2 to genus 0 and which is impossible. Hence proved by reductio absurdum. But I am not at all sure of the above argument! 4       One knows how the canonical map for hyperelliptic Riemann surfaces factors (as stated in question 2) and one also knows as to how they occur as normalizations of the algebraic curve $\{y^2 - \prod _{1}^{2g+2} (x-a_i) = 0\} U [0,0,1]$ (as a subset of $\mathbb{P}^2$) (..where the $a_i$ are distinct complex numbers..) From the above two facts how does it follow that if all these $a_i$ are permuted or affected by some $SL(2,\mathbb{Z})$ then the compact Riemann surface up there doesn't change?","For genus > 1 hyperelliptic Riemann surface the definition guarantees that there is a degree 2 map from that to $\mathbb{P}^1$. Under this map the inverse image of the ""point at infinity"" has to be two distinct points - why? Why couldn't it be that the map has a double pole? (..as wanted that would also not have the injectivity and hence biholomorphism crisis..) Is the canonical map on a compact Riemann surface holomorphic? (..for the hyperelliptic compact Riemann surfaces one knows that it factors as a composition of the definition guaranteed degree 2 map (to $\mathbb{P}^1$) and the rational canonical curve and hence its not holomorphic..) The above question is motivated from my difficulty in piecing together the argument that all compact Riemann surfaces at genus 2 are hyperelliptic.  To prove that one wants to show that there the canonical curve is not injective.  For a genus 2 compact Riemann surface the canonical map is a map to  $\mathbb{P}^1$ (which is genus 0) which if were injective then something absurd would have to happen. But what? One possibile way I can think of the argument is if the canonical map were holomorphic then one could say that this map is surjective - since continuous image of a compact surface is compact and hence its closed in $\mathbb{P}^1$ and but its also open by being the image of a holomorphic map and hence by the connectedness of $\mathbb{P}^1$ it is surjective. So the canonical map for genus 2 Riemann surface would be a bijective holomorphism and which would in turn imply that topologically we have a homeomorphism from genus 2 to genus 0 and which is impossible. Hence proved by reductio absurdum. But I am not at all sure of the above argument! 4       One knows how the canonical map for hyperelliptic Riemann surfaces factors (as stated in question 2) and one also knows as to how they occur as normalizations of the algebraic curve $\{y^2 - \prod _{1}^{2g+2} (x-a_i) = 0\} U [0,0,1]$ (as a subset of $\mathbb{P}^2$) (..where the $a_i$ are distinct complex numbers..) From the above two facts how does it follow that if all these $a_i$ are permuted or affected by some $SL(2,\mathbb{Z})$ then the compact Riemann surface up there doesn't change?",,"['complex-analysis', 'algebraic-geometry', 'differential-geometry', 'riemann-surfaces']"
29,A complex integral question,A complex integral question,,"The problem is in the Cauchy Integral Formula section in Gamelin's ""Complex Analysis"". $$ \oint_{|z-1|=3} \frac{dz}{z(z^2-4)e^z} $$ I have trouble with it because -2 is actually on the boundary.","The problem is in the Cauchy Integral Formula section in Gamelin's ""Complex Analysis"". $$ \oint_{|z-1|=3} \frac{dz}{z(z^2-4)e^z} $$ I have trouble with it because -2 is actually on the boundary.",,['complex-analysis']
30,Open Mapping Theorem (complex analysis),Open Mapping Theorem (complex analysis),,"In all the proofs I can find of the Open Mapping Theorem ( for example here ) at the outset it is mentioned that it is enough to prove that for all a in U, f(a) is contained in a disk that is itself contained in f(U). How is that enough? One needs to prove that for every open set U' (that is a subset of U) the theorem holds, however the U used in that opening statement is a connected open set (and thus a stricter condition). I find that every way I try and reconcile this I run into something non-trivial (e.g. Considering any open set a countable union of connected open sets. Can you do that in R n ? Does it require a finitely dimensioned space or any other such restriction?), even though the proof's statement suggests it is obvious that proving for U is enough. Stumped. Thanks. EDIT I'm aware that the statement proves that f(U) is open (as a union of open disks), that's not what I'm asking. My question is how is that enough to prove that f is open, i.e. f(V) is open for every V open subset of U (in particular since V may not be connected like U and certainly need not be a disk).","In all the proofs I can find of the Open Mapping Theorem ( for example here ) at the outset it is mentioned that it is enough to prove that for all a in U, f(a) is contained in a disk that is itself contained in f(U). How is that enough? One needs to prove that for every open set U' (that is a subset of U) the theorem holds, however the U used in that opening statement is a connected open set (and thus a stricter condition). I find that every way I try and reconcile this I run into something non-trivial (e.g. Considering any open set a countable union of connected open sets. Can you do that in R n ? Does it require a finitely dimensioned space or any other such restriction?), even though the proof's statement suggests it is obvious that proving for U is enough. Stumped. Thanks. EDIT I'm aware that the statement proves that f(U) is open (as a union of open disks), that's not what I'm asking. My question is how is that enough to prove that f is open, i.e. f(V) is open for every V open subset of U (in particular since V may not be connected like U and certainly need not be a disk).",,['complex-analysis']
31,Determining if $f(z)=(1-e^{1/z})^{-1}$ is holomorphic,Determining if  is holomorphic,f(z)=(1-e^{1/z})^{-1},"If we let $$f(z)=(1-e^{1/z})^{-1} ,$$ where $z$ is complex, I'm trying to use the Cauchy-Riemann equations to determine if $f$ is holomorphic. So I need to separate it into real and imaginary functions of $x$ and $y$ ($u(x,y)$ , $v(x,y)$ respectively), then partially differentiate. So far I have $$u(x,y)=\frac {1-e^{\frac {x}{x^{2}+y^{2}}} \cos(\frac{y}{x^{2}+y^{2}})}{1-2e^{x} \cos(\frac{y}{x^{2}+y^{2}})+e^{\frac {1}{x^{2}+y^{2}}}} $$ $$v(x,y)=\frac {-e^{\frac {x}{x^{2}+y^{2}}} \sin(\frac{y}{x^{2}+y^{2}})}{1-2e^{x} \cos(\frac{y}{x^{2}+y^{2}})+e^{\frac {1}{x^{2}+y^{2}}}} $$ This looks a bit of a nightmare to differentiate 4 times for a small part of a question, can anyone see how a quicker way to use the C-R equations to determine if $f$ is holomorphic?","If we let $$f(z)=(1-e^{1/z})^{-1} ,$$ where $z$ is complex, I'm trying to use the Cauchy-Riemann equations to determine if $f$ is holomorphic. So I need to separate it into real and imaginary functions of $x$ and $y$ ($u(x,y)$ , $v(x,y)$ respectively), then partially differentiate. So far I have $$u(x,y)=\frac {1-e^{\frac {x}{x^{2}+y^{2}}} \cos(\frac{y}{x^{2}+y^{2}})}{1-2e^{x} \cos(\frac{y}{x^{2}+y^{2}})+e^{\frac {1}{x^{2}+y^{2}}}} $$ $$v(x,y)=\frac {-e^{\frac {x}{x^{2}+y^{2}}} \sin(\frac{y}{x^{2}+y^{2}})}{1-2e^{x} \cos(\frac{y}{x^{2}+y^{2}})+e^{\frac {1}{x^{2}+y^{2}}}} $$ This looks a bit of a nightmare to differentiate 4 times for a small part of a question, can anyone see how a quicker way to use the C-R equations to determine if $f$ is holomorphic?",,['complex-analysis']
32,Harmonic functions and Cauchy's theorem,Harmonic functions and Cauchy's theorem,,"This is a doubt of mine on the basics of complex analysis. I encountered a certain statement involving integrating a harmonic function, which would be nice for my research attempts if proved. When I strengthened the assumption to that the function is holomorphic, I could very easily do it using Cauchy's theorem. Is it always possible to treat a harmonic function as the real or imaginary part of a holomorphic function, and draw consequences from Cauchy's theorem?","This is a doubt of mine on the basics of complex analysis. I encountered a certain statement involving integrating a harmonic function, which would be nice for my research attempts if proved. When I strengthened the assumption to that the function is holomorphic, I could very easily do it using Cauchy's theorem. Is it always possible to treat a harmonic function as the real or imaginary part of a holomorphic function, and draw consequences from Cauchy's theorem?",,['complex-analysis']
33,Möbius Transforms that preserve $\mathbb{H}$,Möbius Transforms that preserve,\mathbb{H},"I know that every möbius transform that preserves the upper half plane is of the form $m(z) = \frac{az+b}{cz+d}$, where $a,b,c,d \in \mathbb{R}$, or $m(z) = \frac{a\bar{z} + b}{c\bar{z} + d}$, where $a,b,c,d$ are all purely imaginary. In both cases $ad-bc = 1$. However, I could be wrong but I see that most books in the literature tend to exclude the latter case involving the complex conjugate. Is there any reason why?? It is pretty important that I understand this, as I need to use these facts when calculating the distance between two points in the upper half plane model of 2 dimensional hyperbolic space.","I know that every möbius transform that preserves the upper half plane is of the form $m(z) = \frac{az+b}{cz+d}$, where $a,b,c,d \in \mathbb{R}$, or $m(z) = \frac{a\bar{z} + b}{c\bar{z} + d}$, where $a,b,c,d$ are all purely imaginary. In both cases $ad-bc = 1$. However, I could be wrong but I see that most books in the literature tend to exclude the latter case involving the complex conjugate. Is there any reason why?? It is pretty important that I understand this, as I need to use these facts when calculating the distance between two points in the upper half plane model of 2 dimensional hyperbolic space.",,['complex-analysis']
34,Conformality of Inversion Map,Conformality of Inversion Map,,"I am trying to show that elements of the general Möbius group generated by an affine transformation $f(z) = az+b$, the inversion map $f(z)=\frac{1}{z}$ and complex conjugation $f(z)=\overline{z}$, where $a,b \in \mathbb{C}$, are conformal maps. So if I have a curve lying in the $x$-$y$ plane, then its image in the $u$-$v$ plane is another curve, and I want to show that the angles between the curves are preserved under the three transformations above. Because we speak of angles between curves, the problem reduces to proving that the angle between their tangents is preserved and hence to the simplest problem f showing that angles between straight lines are preserved under these maps. Now let $X_1$ and $X_2$ be straight lines. Then under an affine transformation and complex conjugation it is not hard to show that the angle between $X_1$ and $X_2$, let's call it $angle(X_1 , X_2)$ is preserved (Under complex conjugation the sign of the angle is reversed however). Now the inversion map is a bit more tricky. If my two lines in $x-y$ space pass through the $y$-axis at 1, viz. that $X_k$ has equation $\beta_k z + \overline{\beta_k z} + 1 = 0$ where $ k \in {0,1}$, then under inversion these straight lines get mapped to circles through the origin, and hence it is plain that the angles between the tangents of the circles at $(0,0)$ is preserved. However, what if my lines $X_1$ and $X_2$ are such that under inversion $X_1$ maps to a line while $X_2$ maps to a circle? This is where I am getting stuck. On the other hand, if I try to prove the conformality of the inversion map by saying that my $x$ and $y$ coordinate curves in $x-y$ space get mapped to $\frac{x}{x^2 + y^2}$ and $\frac{-y}{x^2 + y^2}$ in $u-v$ space, and then write vector equations of lines and compute partial derivates, the maths becomes ugly and the method is certainly not elegant. Anyone got any ideas? (Maybe involving Cauchy - Riemann Equations!!)","I am trying to show that elements of the general Möbius group generated by an affine transformation $f(z) = az+b$, the inversion map $f(z)=\frac{1}{z}$ and complex conjugation $f(z)=\overline{z}$, where $a,b \in \mathbb{C}$, are conformal maps. So if I have a curve lying in the $x$-$y$ plane, then its image in the $u$-$v$ plane is another curve, and I want to show that the angles between the curves are preserved under the three transformations above. Because we speak of angles between curves, the problem reduces to proving that the angle between their tangents is preserved and hence to the simplest problem f showing that angles between straight lines are preserved under these maps. Now let $X_1$ and $X_2$ be straight lines. Then under an affine transformation and complex conjugation it is not hard to show that the angle between $X_1$ and $X_2$, let's call it $angle(X_1 , X_2)$ is preserved (Under complex conjugation the sign of the angle is reversed however). Now the inversion map is a bit more tricky. If my two lines in $x-y$ space pass through the $y$-axis at 1, viz. that $X_k$ has equation $\beta_k z + \overline{\beta_k z} + 1 = 0$ where $ k \in {0,1}$, then under inversion these straight lines get mapped to circles through the origin, and hence it is plain that the angles between the tangents of the circles at $(0,0)$ is preserved. However, what if my lines $X_1$ and $X_2$ are such that under inversion $X_1$ maps to a line while $X_2$ maps to a circle? This is where I am getting stuck. On the other hand, if I try to prove the conformality of the inversion map by saying that my $x$ and $y$ coordinate curves in $x-y$ space get mapped to $\frac{x}{x^2 + y^2}$ and $\frac{-y}{x^2 + y^2}$ in $u-v$ space, and then write vector equations of lines and compute partial derivates, the maths becomes ugly and the method is certainly not elegant. Anyone got any ideas? (Maybe involving Cauchy - Riemann Equations!!)",,['complex-analysis']
35,A question on complex analysis,A question on complex analysis,,"Let $f,g:\mathbb C \to \mathbb C$ be two analytic functions such that $f(z)(g(z)+z^2)=0$ for all $z$ .Then prove that either $f(z)=0$ or $g(z)=-z^2$.","Let $f,g:\mathbb C \to \mathbb C$ be two analytic functions such that $f(z)(g(z)+z^2)=0$ for all $z$ .Then prove that either $f(z)=0$ or $g(z)=-z^2$.",,[]
36,Finding number of roots of a polynomial in the unit disk,Finding number of roots of a polynomial in the unit disk,,"I would like to know how to find the number of (complex) roots of the polynomal $f(z) = z^4+3z^2+z+1$ inside the unit disk. The usual way to solve such a problem, via Rouché's theorem does not work, at least not in an ""obvious way"". Any ideas? Thanks! edit: here is a rough idea I had: For any $\epsilon >0$, let $f_{\epsilon}(z) = z^4+3z^2+z+1-\epsilon$. By Rouché's theorem, for each such $\epsilon$, $f_{\epsilon}$ has exactly 2 roots inside the unit disc. Hence, by continuity, it follows that $f$ has 2 roots on the closed unit disc, so it remains to determine what happens on the boundary. Is this reasoning correct? what can be said about the boundary?","I would like to know how to find the number of (complex) roots of the polynomal $f(z) = z^4+3z^2+z+1$ inside the unit disk. The usual way to solve such a problem, via Rouché's theorem does not work, at least not in an ""obvious way"". Any ideas? Thanks! edit: here is a rough idea I had: For any $\epsilon >0$, let $f_{\epsilon}(z) = z^4+3z^2+z+1-\epsilon$. By Rouché's theorem, for each such $\epsilon$, $f_{\epsilon}$ has exactly 2 roots inside the unit disc. Hence, by continuity, it follows that $f$ has 2 roots on the closed unit disc, so it remains to determine what happens on the boundary. Is this reasoning correct? what can be said about the boundary?",,"['complex-analysis', 'polynomials']"
37,Number of roots of trigonometric polynomial,Number of roots of trigonometric polynomial,,"Exercise 1.8.5 of Berenstein-Gay ""Complex variables"" asks to count the number of zeroes in $(0,2\pi)$ of certain trigonometric polynomial. Towards the exercise, the book suggests to first show the following: Given $0\le a_0<\dots<a_n$, the roots of $$p(z)=a_0+a_1z+\dots+a_nz^n$$ lie in the unit disk. Do you see how to solve the above? (The exercise is used in several places later through the book, so I am very frustrated with myself for not being able to do this on my own. The original version of this posting asked for the whole exercise, which is somewhat unreasonable, as it is really several very different questions together. I am editing the question to address directly the part that has been answered. I may post the rest of 1.8.5 at a later date.)","Exercise 1.8.5 of Berenstein-Gay ""Complex variables"" asks to count the number of zeroes in $(0,2\pi)$ of certain trigonometric polynomial. Towards the exercise, the book suggests to first show the following: Given $0\le a_0<\dots<a_n$, the roots of $$p(z)=a_0+a_1z+\dots+a_nz^n$$ lie in the unit disk. Do you see how to solve the above? (The exercise is used in several places later through the book, so I am very frustrated with myself for not being able to do this on my own. The original version of this posting asked for the whole exercise, which is somewhat unreasonable, as it is really several very different questions together. I am editing the question to address directly the part that has been answered. I may post the rest of 1.8.5 at a later date.)",,['complex-analysis']
38,Deriving Bessel's Integrals,Deriving Bessel's Integrals,,"I'm trying to derive the integral form of the Bessel function by finding the $k$th coefficient of the Laurent series expansion of the function $f(z) =\exp [\lambda(z-\frac{1}{z})]$. I managed to get it down to the form $ J_k(\lambda) =\frac{1}{2\pi}\int_{0}^{2 \pi} e^{i(\lambda \sin\theta - k \theta)} d\theta =\frac{1}{2\pi}\int_{0}^{2 \pi} [\cos(\lambda \sin\theta - k \theta) +i\sin(\lambda \sin\theta - k \theta)]d\theta $ But, I need to show that this is equivalent to $ \frac{1}{2\pi}\int_{0}^{2 \pi} \cos(\lambda \sin\theta - k \theta)d\theta $. In other words, I need to show that $ \int_{0}^{2 \pi}\sin(\lambda \sin\theta - k \theta)d\theta =0 $ But I can't figure out how to do this. I tried expanding using trig identities and then writing sin and cos as Taylor Series and integrating term by term, but no luck. What am I missing?","I'm trying to derive the integral form of the Bessel function by finding the $k$th coefficient of the Laurent series expansion of the function $f(z) =\exp [\lambda(z-\frac{1}{z})]$. I managed to get it down to the form $ J_k(\lambda) =\frac{1}{2\pi}\int_{0}^{2 \pi} e^{i(\lambda \sin\theta - k \theta)} d\theta =\frac{1}{2\pi}\int_{0}^{2 \pi} [\cos(\lambda \sin\theta - k \theta) +i\sin(\lambda \sin\theta - k \theta)]d\theta $ But, I need to show that this is equivalent to $ \frac{1}{2\pi}\int_{0}^{2 \pi} \cos(\lambda \sin\theta - k \theta)d\theta $. In other words, I need to show that $ \int_{0}^{2 \pi}\sin(\lambda \sin\theta - k \theta)d\theta =0 $ But I can't figure out how to do this. I tried expanding using trig identities and then writing sin and cos as Taylor Series and integrating term by term, but no luck. What am I missing?",,['complex-analysis']
39,Polynomials with equal magnitude on the unit circle,Polynomials with equal magnitude on the unit circle,,"Let $p(z), q(z)$ be non-constant polynomials of the same degree n such that $|p(z)| = |q(z)|$ on the unit circle (where $|z| = 1$ ) and all their zeros are in the interior of the unit disk. I'm trying to prove that in that case it must be that $q(z) = ap(z)$ for some (complex) $a$ . My initial instinct is to try and somehow use the fact that for $f(z) = \frac{p(z)}{q(z)}$ we have $|f(z)| = c > 0$ on the boundary $|z| = 1$ and perhaps invoke the maximum modulus principle somehow, asserting that any singularity of $f(z)$ (where only $q(z) = 0$ ) inside the unit disk would be a pole, hence f(z) would tend to $\infty$ in a neighborhood of that point, contradicting the maximum modulus principle. But this seems off since (For example) my understanding is that to invoke max modulus principle I would need $f(z)$ to be continuous on the closure of the domain in question (the open disk), e.g. including any 'puncture points' where those aforementioned singularities would arise? Any direction would be appreciated how to approach this otherwise!","Let be non-constant polynomials of the same degree n such that on the unit circle (where ) and all their zeros are in the interior of the unit disk. I'm trying to prove that in that case it must be that for some (complex) . My initial instinct is to try and somehow use the fact that for we have on the boundary and perhaps invoke the maximum modulus principle somehow, asserting that any singularity of (where only ) inside the unit disk would be a pole, hence f(z) would tend to in a neighborhood of that point, contradicting the maximum modulus principle. But this seems off since (For example) my understanding is that to invoke max modulus principle I would need to be continuous on the closure of the domain in question (the open disk), e.g. including any 'puncture points' where those aforementioned singularities would arise? Any direction would be appreciated how to approach this otherwise!","p(z), q(z) |p(z)| = |q(z)| |z| = 1 q(z) = ap(z) a f(z) = \frac{p(z)}{q(z)} |f(z)| = c > 0 |z| = 1 f(z) q(z) = 0 \infty f(z)",['complex-analysis']
40,Asymptotic expansion of $\int_0^{2\pi} \frac{\cos(nt)}{1+t^2} dt$,Asymptotic expansion of,\int_0^{2\pi} \frac{\cos(nt)}{1+t^2} dt,"Question: Show that the integral $$I(n) = \int_0^{2\pi} \frac{\cos(nt)}{1+t^2} dt $$ has an asymptotic expansion of the form as $n\to\infty$ $$ f(n) = \sum_{k=0}^\infty a_k \frac{1}{n^{2k}} $$ Evaluate the coefficients $a_0, a_1, a_2$ . What I've done: I've used two ways to evaluate the integral: (1) I tried to relate to Riemann Lebesgue lemma. $I(n)$ is even in $n$ , so I changed variables from $t$ to $u = t - \pi$ and then got: $$I(n) = Re[ \int_{-\pi}^{\pi} \frac{\exp(-inu)\exp(-in\pi)}{1+(u+\pi)^2} dt] $$ then the function $\frac{\exp(-in\pi)}{1+(u+\pi)^2}$ is infinitely differentiable, so the integral decays like $O(n^{-k})$ for any $k\in N$ , so all coefficients, after expanding the integral in the inverse powers form, must vanish. My question: if my argument of vanishing coefficients holds, how do I show the integral do have expansion w.r.t. inverse powers of $n$ ? This seems arbitrary to me, especially that I don't know the coefficients $a_n$ . (2) I write $I(n) = Re[\int_0^{2\pi} (1+t^2)^{-1} e^{int} dt] =^{it = -s} = Re[\int_0^{-2\pi i} (1-s^2)^{-1} e^{-ns} i ds]$ , and then show integrals of the same integrand from ${-2\pi i}$ to ${-2\pi i +2\pi}$ (call it $J_1$ ) and $2\pi - 2\pi i$ to $2\pi$ (call it $J_2$ ) vanish (the first vanish at rate $O(\frac{1}{n})$ and the second $O(e^{-2n\pi})$ ). Then I write: $$I(n) = Re[\int_0^{2\pi} (1-s^2)^{-1} e^{-ns} ids]$$ Using Watson's lemma, I can easily show $I(n) = Re[i\sum_{m=0}^\infty \Gamma(2m+1)/n^{2m+1}] = 0 (*)$ . My question: I'm a bit unsure about my argument about showing the integral from 0 to $-2\pi i$ equal to the integral from 0 to $2\pi$ , since what I've shown is that $J_{1,2} = 0$ but not, say, $J_{1,2} = o(I(n))$ . Further, my result in $(*)$ is in odd powers of n, but the required form is in even powers, which made me suspicious about my answer. Could anyone verify this for me please? Thank you!","Question: Show that the integral has an asymptotic expansion of the form as Evaluate the coefficients . What I've done: I've used two ways to evaluate the integral: (1) I tried to relate to Riemann Lebesgue lemma. is even in , so I changed variables from to and then got: then the function is infinitely differentiable, so the integral decays like for any , so all coefficients, after expanding the integral in the inverse powers form, must vanish. My question: if my argument of vanishing coefficients holds, how do I show the integral do have expansion w.r.t. inverse powers of ? This seems arbitrary to me, especially that I don't know the coefficients . (2) I write , and then show integrals of the same integrand from to (call it ) and to (call it ) vanish (the first vanish at rate and the second ). Then I write: Using Watson's lemma, I can easily show . My question: I'm a bit unsure about my argument about showing the integral from 0 to equal to the integral from 0 to , since what I've shown is that but not, say, . Further, my result in is in odd powers of n, but the required form is in even powers, which made me suspicious about my answer. Could anyone verify this for me please? Thank you!","I(n) = \int_0^{2\pi} \frac{\cos(nt)}{1+t^2} dt  n\to\infty  f(n) = \sum_{k=0}^\infty a_k \frac{1}{n^{2k}}  a_0, a_1, a_2 I(n) n t u = t - \pi I(n) = Re[ \int_{-\pi}^{\pi} \frac{\exp(-inu)\exp(-in\pi)}{1+(u+\pi)^2} dt]  \frac{\exp(-in\pi)}{1+(u+\pi)^2} O(n^{-k}) k\in N n a_n I(n) = Re[\int_0^{2\pi} (1+t^2)^{-1} e^{int} dt] =^{it = -s} = Re[\int_0^{-2\pi i} (1-s^2)^{-1} e^{-ns} i ds] {-2\pi i} {-2\pi i +2\pi} J_1 2\pi - 2\pi i 2\pi J_2 O(\frac{1}{n}) O(e^{-2n\pi}) I(n) = Re[\int_0^{2\pi} (1-s^2)^{-1} e^{-ns} ids] I(n) = Re[i\sum_{m=0}^\infty \Gamma(2m+1)/n^{2m+1}] = 0 (*) -2\pi i 2\pi J_{1,2} = 0 J_{1,2} = o(I(n)) (*)","['integration', 'complex-analysis', 'asymptotics', 'physics']"
41,Question regarding rewriting sum of sine and cosine into a single cosine term,Question regarding rewriting sum of sine and cosine into a single cosine term,,"Let's say that we have the following function: $$ x(t) = A_1\cos(\omega t) + A_2\sin(\omega t) $$ Then we can rewrite it by defining a right triangle with an angle $\delta$ , which has adjacent side $A_1$ , opposite side $A_2$ , and hypotenuse $A = \sqrt{A_1^2 + A_2^2}$ . So $$ \delta = \arctan \frac{A_2}{A_1}. $$ It then follows that: $$ x(t) = A \biggl( \frac{A_1}{A}\cos(\omega t)  + \frac{A_2}{A}\sin(\omega t) \biggr)  = A \bigl( \cos(\delta)\cos(\omega t)  + \sin(\delta)\sin(\omega t) \bigr)  = A\cos(\omega t - \delta). $$ However, let's say we now have the following function: $$ x(t) = A_1e^{i\omega t} + A_2e^{-i \omega t}  = (A_1+A_2)\cos(\omega t) + (A_1-A_2)\,i\sin(\omega t). $$ It turns out that this can still be rewritten as $$ x(t) = A\cos(\omega t - \delta). $$ But how and why? Because we now have a complex amplitude $(A_1-A_2)i$ for the $\sin(\omega t)$ part. So using the same definition of the right triangle now doesn't make sense right? How are $A$ and $\delta$ supposed to be defined now compared to before? Reference:","Let's say that we have the following function: Then we can rewrite it by defining a right triangle with an angle , which has adjacent side , opposite side , and hypotenuse . So It then follows that: However, let's say we now have the following function: It turns out that this can still be rewritten as But how and why? Because we now have a complex amplitude for the part. So using the same definition of the right triangle now doesn't make sense right? How are and supposed to be defined now compared to before? Reference:","
x(t) = A_1\cos(\omega t) + A_2\sin(\omega t)
 \delta A_1 A_2 A = \sqrt{A_1^2 + A_2^2} 
\delta = \arctan \frac{A_2}{A_1}.
 
x(t) = A \biggl( \frac{A_1}{A}\cos(\omega t) 
+ \frac{A_2}{A}\sin(\omega t) \biggr) 
= A \bigl( \cos(\delta)\cos(\omega t) 
+ \sin(\delta)\sin(\omega t) \bigr) 
= A\cos(\omega t - \delta).
 
x(t) = A_1e^{i\omega t} + A_2e^{-i \omega t} 
= (A_1+A_2)\cos(\omega t) + (A_1-A_2)\,i\sin(\omega t).
 
x(t) = A\cos(\omega t - \delta).
 (A_1-A_2)i \sin(\omega t) A \delta","['complex-analysis', 'trigonometry']"
42,Is an entire function with this property necessarily a polynomial?,Is an entire function with this property necessarily a polynomial?,,"Given an entire function $f(z)$ , define $\displaystyle m(r):=\inf_{\vert z\vert=r}\vert f(z)\vert$ . If $f$ satisfies $$\limsup_{r\to+\infty} m(r)=+\infty,$$ can we assert that $f$ is a polynomial? This question comes to my mind after going through the discussion about the entire functions on Ahlfors. I tried to prove this by showing $\infty$ is a pole of $f$ but got stuck since $f$ may have infinitely many zeros. Now I tend to believe that the statement is not true, but neither can I find a counterexample.","Given an entire function , define . If satisfies can we assert that is a polynomial? This question comes to my mind after going through the discussion about the entire functions on Ahlfors. I tried to prove this by showing is a pole of but got stuck since may have infinitely many zeros. Now I tend to believe that the statement is not true, but neither can I find a counterexample.","f(z) \displaystyle m(r):=\inf_{\vert z\vert=r}\vert f(z)\vert f \limsup_{r\to+\infty} m(r)=+\infty, f \infty f f","['complex-analysis', 'entire-functions']"
43,Understanding Vladimir Maz'ya's Problem 72 what is $|dy|$?,Understanding Vladimir Maz'ya's Problem 72 what is ?,|dy|,"In the article "" Seventy Five (Thousand) Unsolved Problems in Analysis and Partial Differential Equations "" by Vladimir Maz’ya Problem 72 on page 36 is given as follows: Let $C$ be the unit circle let $u(x)$ be a function on the unit circle. Define the operator $A$ as follows $$ A(u)(x) = \int_{C} \frac{u(x)-u(y)}{|x-y|} |dy| $$ The spectrum of A is described in [79, Section 12.2.2]. Then the principal cauchy problem: $$  \frac{d}{dt} + A, t> 0 $$ Which is reminiscent of the modified zeta function $$ f(z) = \sum_{k=1}^{\infty} e^{-z \sum_{n=1}^{k} \frac{1}{n}} $$ Admits a meromorphic extension to the entire complex plane. Study the properties of this extension. So I have a couple questions here. What is $|dy|$ ? The only way I can try to make sense of this is that the sign of the integral should cancel out the sign of $dy$ for every point on the unit circle. I.E. this is the same as: $$ A(u)(x) = \int_{y \in C} \frac{u(x)-u(y)}{y|x-y|}  $$ But if that was true the author would've just gone ahead and said that. So I think I don't understand what $|dy|$ is supposed to be. The author makes a mention that this is ""reminiscent of the modified zeta function"" but not equal to. I assume this means the modified zeta function's meromorphic continuation is known? Is there a reference for this and or an explicit construction? The Reference [79] from the article is: Maz’ya, V., Nazarov, S., Plamenevskij, B.: Asymptotic Theory of Elliptic Baundary Value Problems in Singularly Perturbed Domains, vol. I. Birkh¨auser, Basel (2000) And I cannot get a hold of this article at this time. Some More Notes: Using approach0 I found this question where again a line integral on the unit circle is occuring and the symbol $|dz|$ indicates the line element here. So perhaps we need not know what $|dy|$ means at all to make sense of this (just consider it as a line integral over $C$ ). To be honest I'm still dissatisfied since usually with line elements $dl$ it is usually possible to expand $dl = a(x)dy + b(y)dy$ . In this particular case I'm still not sure how to crack open $|dz| = ?dx + ?dy$","In the article "" Seventy Five (Thousand) Unsolved Problems in Analysis and Partial Differential Equations "" by Vladimir Maz’ya Problem 72 on page 36 is given as follows: Let be the unit circle let be a function on the unit circle. Define the operator as follows The spectrum of A is described in [79, Section 12.2.2]. Then the principal cauchy problem: Which is reminiscent of the modified zeta function Admits a meromorphic extension to the entire complex plane. Study the properties of this extension. So I have a couple questions here. What is ? The only way I can try to make sense of this is that the sign of the integral should cancel out the sign of for every point on the unit circle. I.E. this is the same as: But if that was true the author would've just gone ahead and said that. So I think I don't understand what is supposed to be. The author makes a mention that this is ""reminiscent of the modified zeta function"" but not equal to. I assume this means the modified zeta function's meromorphic continuation is known? Is there a reference for this and or an explicit construction? The Reference [79] from the article is: Maz’ya, V., Nazarov, S., Plamenevskij, B.: Asymptotic Theory of Elliptic Baundary Value Problems in Singularly Perturbed Domains, vol. I. Birkh¨auser, Basel (2000) And I cannot get a hold of this article at this time. Some More Notes: Using approach0 I found this question where again a line integral on the unit circle is occuring and the symbol indicates the line element here. So perhaps we need not know what means at all to make sense of this (just consider it as a line integral over ). To be honest I'm still dissatisfied since usually with line elements it is usually possible to expand . In this particular case I'm still not sure how to crack open","C u(x) A  A(u)(x) = \int_{C} \frac{u(x)-u(y)}{|x-y|} |dy|    \frac{d}{dt} + A, t> 0   f(z) = \sum_{k=1}^{\infty} e^{-z \sum_{n=1}^{k} \frac{1}{n}}  |dy| dy  A(u)(x) = \int_{y \in C} \frac{u(x)-u(y)}{y|x-y|}   |dy| |dz| |dy| C dl dl = a(x)dy + b(y)dy |dz| = ?dx + ?dy","['complex-analysis', 'partial-differential-equations', 'analytic-continuation']"
44,A possible criterion for a function on a complex analytic space to be holomorphic,A possible criterion for a function on a complex analytic space to be holomorphic,,"Let $(X, \mathcal O)$ be a reduced complex analytic space, and $f:X \rightarrow \mathbb C$ be a continuous function. Assume that for all holomorphic mappings $F$ from unit disc $D \subset \mathbb C$ to $X$ the function $f \circ F$ is holomorphic on $D$ . Is it true that $f$ is holomorphic on $X$ ? The inspiration for this question comes from a form of Ruckert's Nullstellensatz, that states that a for a reduced (i.e. without nilpotent elements) analytic algebra $A$ and a non-zero element $s \in A$ there exists a homomorphism $\phi: A \rightarrow \mathcal O_1$ such that $\phi(s) \ne 0$ (here $\mathcal O_1$ denotes the algebra of convergent power series of one variable). That is, if $(X,\mathcal O)$ is a complex space, which is reduced at a point $x \in X$ , then a germ of analytic function at a point $x$ is completely determined by its compositions with holomorphic maps from a neighborhood of $0$ in $\mathbb C$ to $X$ that map $0$ to $x$ . I already understood that this concept can lead to an easy proof of the maximum modulus principle: if absolute value of a holomorphic function has local maximum at $x$ , then $f$ is constant in some neighborhood of $x$ . Indeed, then compositions of $f$ with maps from $D$ to $X$ that map $0$ to $x$ are all contants (equal to $f(x)$ ) due to maximum modulus principle on $D$ . Thus, the germ of $f$ at $x$ coincides with the germ of the constant function $f(x)$ . It is easy to see that this question (if the statement is indeed true) can lead to an easy proof of the fact that a limit of sequence of holomorphic functions that converges uniformly on compact sets is again holomorphic. The proof of this fact in the book of Gunning and Rossi is far more complicated. Finally, it is clear that the statement obviously holds for a complex manifold due to the fact that a function on an open set in $\mathbb C^n$ that is holomorphic in each variable separately is holomorphic. However, I can't come up neither with a proof nor with a counterexample for this question.","Let be a reduced complex analytic space, and be a continuous function. Assume that for all holomorphic mappings from unit disc to the function is holomorphic on . Is it true that is holomorphic on ? The inspiration for this question comes from a form of Ruckert's Nullstellensatz, that states that a for a reduced (i.e. without nilpotent elements) analytic algebra and a non-zero element there exists a homomorphism such that (here denotes the algebra of convergent power series of one variable). That is, if is a complex space, which is reduced at a point , then a germ of analytic function at a point is completely determined by its compositions with holomorphic maps from a neighborhood of in to that map to . I already understood that this concept can lead to an easy proof of the maximum modulus principle: if absolute value of a holomorphic function has local maximum at , then is constant in some neighborhood of . Indeed, then compositions of with maps from to that map to are all contants (equal to ) due to maximum modulus principle on . Thus, the germ of at coincides with the germ of the constant function . It is easy to see that this question (if the statement is indeed true) can lead to an easy proof of the fact that a limit of sequence of holomorphic functions that converges uniformly on compact sets is again holomorphic. The proof of this fact in the book of Gunning and Rossi is far more complicated. Finally, it is clear that the statement obviously holds for a complex manifold due to the fact that a function on an open set in that is holomorphic in each variable separately is holomorphic. However, I can't come up neither with a proof nor with a counterexample for this question.","(X, \mathcal O) f:X \rightarrow \mathbb C F D \subset \mathbb C X f \circ F D f X A s \in A \phi: A \rightarrow \mathcal O_1 \phi(s) \ne 0 \mathcal O_1 (X,\mathcal O) x \in X x 0 \mathbb C X 0 x x f x f D X 0 x f(x) D f x f(x) \mathbb C^n","['complex-analysis', 'complex-geometry', 'several-complex-variables']"
45,Solving $\int_0^\pi \frac{\cos(kn)}{1+k^2}dk$,Solving,\int_0^\pi \frac{\cos(kn)}{1+k^2}dk,"I want to solve the integral $\int_0^\pi \frac{\cos(kn)}{1+k^2}dk$ . In my attempt, I tried do Integration by Parts twice, but this didn't get me anywhere. Then, I thought about using the Residue theorem, but to do that I needed the limits of integration to be $-\infty$ to $\infty$ . What else can I do to analytically solve this? $$\int\limits_{0}^{\pi}\frac{\cos(kn)}{1+k^2}dk$$ \begin{align} \int\limits_{0}^{\pi}\frac{\cos(kn)}{1+k^2}dk &= \int\limits_{0}^{\pi}\frac{\cos(kn)}{(k+i)(k-i)}dk\\ &=\int\limits_{0}^{\pi}\frac i 2\left(\frac{\cos (k n)}{k+i}-\frac{\cos (k n)}{k-i}\right)dk\\ &=\frac{i}{2}\int\limits_{0}^{\pi}\frac{\cos(kn)}{k+i}dk-\frac{i}{2}\int\limits_{0}^{\pi}\frac{\cos(kn)}{k-i}dk \end{align} Consider the first integral term and perform the change of variables $k = x - i$ . \begin{align} \frac{i}{2}\int\limits_{0}^{\pi}\frac{\cos (k n)}{k+i} &= \frac{i}{2}\int\limits_{i}^{\pi+i} \frac{\cos (n (x-i))}{x} dx\\ &= \frac{i}{2}\int\limits_i^{\pi+i} \cosh (n) \frac {\cos (n x)}{x}+i\sinh (n) \frac{\sin (n x)} {x} dx\\ &=n\frac{i}{2}\cosh(n)\int\limits_{i}^{\pi+i}\frac{\cos(nx)}{nx}dx - n\frac{1}{2}\sinh(n)\int\limits_{i}^{\pi+i} \frac{\sin(nx)}{nx}dx  \end{align} Change of variables: $t = nx$ : \begin{align} &=n\frac{i}{2}\cosh(n)\int\limits_{n i}^{n(\pi + i)} \frac{\cos(t)}{t}dt - n\frac{1}{2}\sinh(n)\int\limits_{ni}^{n(\pi+i)}\frac{\sinh(t)}{t}dt\\ &= n\frac{i}{2}\cosh(n) \left ( Ci(n(\pi+i) - Ci(ni)) \right ) - n\frac{1}{2}\sinh(n)\left( Si(n(\pi+i)) - Si(ni)\right ) \end{align} Likewise, the second integral term of $(3)$ becomes: \begin{equation} \begin{split} &\frac{i}{2}\int\limits_{0}^{\pi}\frac{\cos(kn)}{k-i}dk \\ &=n\frac{i}{2}\cosh(n) \left ( Ci(n(\pi-i) - Ci(-ni)) \right ) + n\frac{1}{2}\sinh(n)\left( Si(n(\pi-i)) - Si(-ni)\right ) \end{split} \end{equation} Therefore, $(3) = (8) - (9)$ .","I want to solve the integral . In my attempt, I tried do Integration by Parts twice, but this didn't get me anywhere. Then, I thought about using the Residue theorem, but to do that I needed the limits of integration to be to . What else can I do to analytically solve this? Consider the first integral term and perform the change of variables . Change of variables: : Likewise, the second integral term of becomes: Therefore, .","\int_0^\pi \frac{\cos(kn)}{1+k^2}dk -\infty \infty \int\limits_{0}^{\pi}\frac{\cos(kn)}{1+k^2}dk \begin{align}
\int\limits_{0}^{\pi}\frac{\cos(kn)}{1+k^2}dk &= \int\limits_{0}^{\pi}\frac{\cos(kn)}{(k+i)(k-i)}dk\\
&=\int\limits_{0}^{\pi}\frac i 2\left(\frac{\cos (k n)}{k+i}-\frac{\cos (k n)}{k-i}\right)dk\\
&=\frac{i}{2}\int\limits_{0}^{\pi}\frac{\cos(kn)}{k+i}dk-\frac{i}{2}\int\limits_{0}^{\pi}\frac{\cos(kn)}{k-i}dk
\end{align} k = x - i \begin{align}
\frac{i}{2}\int\limits_{0}^{\pi}\frac{\cos (k n)}{k+i} &= \frac{i}{2}\int\limits_{i}^{\pi+i} \frac{\cos (n (x-i))}{x} dx\\
&= \frac{i}{2}\int\limits_i^{\pi+i} \cosh (n) \frac {\cos (n x)}{x}+i\sinh (n) \frac{\sin (n x)} {x} dx\\
&=n\frac{i}{2}\cosh(n)\int\limits_{i}^{\pi+i}\frac{\cos(nx)}{nx}dx - n\frac{1}{2}\sinh(n)\int\limits_{i}^{\pi+i} \frac{\sin(nx)}{nx}dx 
\end{align} t = nx \begin{align}
&=n\frac{i}{2}\cosh(n)\int\limits_{n i}^{n(\pi + i)} \frac{\cos(t)}{t}dt - n\frac{1}{2}\sinh(n)\int\limits_{ni}^{n(\pi+i)}\frac{\sinh(t)}{t}dt\\
&= n\frac{i}{2}\cosh(n) \left ( Ci(n(\pi+i) - Ci(ni)) \right ) - n\frac{1}{2}\sinh(n)\left( Si(n(\pi+i)) - Si(ni)\right )
\end{align} (3) \begin{equation}
\begin{split}
&\frac{i}{2}\int\limits_{0}^{\pi}\frac{\cos(kn)}{k-i}dk \\ &=n\frac{i}{2}\cosh(n) \left ( Ci(n(\pi-i) - Ci(-ni)) \right ) + n\frac{1}{2}\sinh(n)\left( Si(n(\pi-i)) - Si(-ni)\right )
\end{split}
\end{equation} (3) = (8) - (9)","['calculus', 'integration', 'complex-analysis', 'definite-integrals', 'trigonometric-integrals']"
46,"Solving $\int_{0}^{\infty}{\frac{\cos(tx^n)}{x^n+a}\, dx}$ via residues",Solving  via residues,"\int_{0}^{\infty}{\frac{\cos(tx^n)}{x^n+a}\, dx}","I was trying to evaluate $\int_{0}^{\infty}{\frac{\cos(tx^n)}{x^n+a}\, dx}$ , with a semicircle in the upper half plane we have : $$\oint{\frac{e^{itz^n}}{z^n+a}\, dz}=\int_{-\infty}^{\infty}{\frac{e^{itx^n}}{x^n+a}\, dx}+\int_{\Gamma}{f(z)\, dz=2\pi i\sum{\mathrm{Res}f(z)}}$$ I think $\int_{\Gamma}\to0$ as the radius goes to infinity. Then the residue when $z^n+a=0$ : $$\begin{align}\lim_{z\to a^{1/n}e^{\frac{i\pi}{n}(2k+1)}}{(z-a^{1/n}e^{\frac{i\pi}{n}(2k+1)})}\frac{e^{itz^n}}{z^n+a}=\frac{e^{itae^{i\pi(2k+1)}}}{n(a^{1/n}e^{\frac{i\pi}{n}(2k+1)})^{n-1}}=-a^{1/n-1}n^{-1}e^{-ita}e^{\frac{i\pi}{n}(2k+1)}\end{align}$$ But after summing, it doesn't seem like this arrives at the answer given here . Specifically the $\csc(\frac{\pi}{n})$ . EDIT : Since part of the numerator of $\sum_{k=0}^{n-1}{e^{\frac{i\pi}{n}(2k+1)}}$ is $1-e^{2\pi i}=0$","I was trying to evaluate , with a semicircle in the upper half plane we have : I think as the radius goes to infinity. Then the residue when : But after summing, it doesn't seem like this arrives at the answer given here . Specifically the . EDIT : Since part of the numerator of is","\int_{0}^{\infty}{\frac{\cos(tx^n)}{x^n+a}\, dx} \oint{\frac{e^{itz^n}}{z^n+a}\, dz}=\int_{-\infty}^{\infty}{\frac{e^{itx^n}}{x^n+a}\, dx}+\int_{\Gamma}{f(z)\, dz=2\pi i\sum{\mathrm{Res}f(z)}} \int_{\Gamma}\to0 z^n+a=0 \begin{align}\lim_{z\to a^{1/n}e^{\frac{i\pi}{n}(2k+1)}}{(z-a^{1/n}e^{\frac{i\pi}{n}(2k+1)})}\frac{e^{itz^n}}{z^n+a}=\frac{e^{itae^{i\pi(2k+1)}}}{n(a^{1/n}e^{\frac{i\pi}{n}(2k+1)})^{n-1}}=-a^{1/n-1}n^{-1}e^{-ita}e^{\frac{i\pi}{n}(2k+1)}\end{align} \csc(\frac{\pi}{n}) \sum_{k=0}^{n-1}{e^{\frac{i\pi}{n}(2k+1)}} 1-e^{2\pi i}=0","['integration', 'complex-analysis', 'contour-integration', 'residue-calculus']"
47,Papa Rudin $1.33$ theorem,Papa Rudin  theorem,1.33,"This is the definition which we need for the proof of the theorem : There is the theorem: If $f$ $\in$ $L^1(\mu)$ , then $|\int_X f d\mu|$ $\leq$ $\int_X |f| d\mu $ . There is the proof: Put $z$ $=$ $\int_x f d\mu$ . Since $z$ is a complex number, there is a complex number $\alpha$ , with $|\alpha|$ $=$ $1$ , such that $\alpha$$z$ $=$ $|z|$ . let $u$ be the real part of $\alpha$$f$ . Then $u$ $\leq$ $|\alpha f|$ $=$ $|f|$ . Hence $|\int_X f d\mu|$ $=$ $\alpha$ $\int_X f d\mu$ $=$ $\int_X \alpha f d\mu $ $=$ $\int_X u d\mu $ $\leq$ $\int_X |f| d\mu $ . I don't understand how we conclude that there is a complex number $\alpha$ , with $|\alpha|$ $=$ $1$ , such that $\alpha z $ $=$ $|z|$ , hence I also don't understand why should $|\alpha|$ be equal of $1$ . Any help would be appreciated.","This is the definition which we need for the proof of the theorem : There is the theorem: If , then . There is the proof: Put . Since is a complex number, there is a complex number , with , such that . let be the real part of . Then . Hence . I don't understand how we conclude that there is a complex number , with , such that , hence I also don't understand why should be equal of . Any help would be appreciated.",f \in L^1(\mu) |\int_X f d\mu| \leq \int_X |f| d\mu  z = \int_x f d\mu z \alpha |\alpha| = 1 \alphaz = |z| u \alphaf u \leq |\alpha f| = |f| |\int_X f d\mu| = \alpha \int_X f d\mu = \int_X \alpha f d\mu  = \int_X u d\mu  \leq \int_X |f| d\mu  \alpha |\alpha| = 1 \alpha z  = |z| |\alpha| 1,"['real-analysis', 'complex-analysis', 'analysis', 'complex-numbers', 'lebesgue-integral']"
48,Integrating $\int^{\infty}_0\frac{\ln^2(x)}{\left(x^2+1\right)^2}\text{ d}x$,Integrating,\int^{\infty}_0\frac{\ln^2(x)}{\left(x^2+1\right)^2}\text{ d}x,"I want to integrate $$I=\int^{\infty}_0\frac{\ln^2(x)}{\left(x^2+1\right)^2}\text{ d}x=\frac{\pi^3}{16}$$ using contour integration. I set up the function $$f(z)=\frac{\ln^3(z)}{\left(z^2+1\right)^2}$$ and used a keyhole contour with a branch cut about the positive real axis The integrals about $\Gamma$ and $\gamma$ I believe both go to $0$ if I did not make any mistakes in my calculations but afaik I should be right. The integrals about $T$ and $B$ is $$\int^{\infty}_{0}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{\left(\ln(x)+2\pi i\right)^3\text{ d}x}{\left(x^2+1\right)^2}$$ which expands and simplifies into $$\int^{\infty}_{0}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{6i\pi\ln^2(x)\text{ d}x}{\left(x^2+1\right)^2}-\int^{0}_{\infty}\frac{12\pi^2\ln(x)\text{ d}x}{\left(x^2+1\right)^2}-\int^{0}_{\infty}\frac{8i\pi^3\text{ d}x}{\left(x^2+1\right)^2}$$ $$\implies -6i\pi I +\int_{0}^{\infty}\frac{12\pi^2\ln(x)\text{ d}x}{\left(x^2+1\right)^2}+\int_{0}^{\infty}\frac{8i\pi^3\text{ d}x}{\left(x^2+1\right)^2}$$ Since $f(z)$ has poles of order $2$ at $z=\pm i$ I use the higher order residue formula and get $$\implies\mathop{\mathrm{Res}}_{z = i}\frac{\ln^3(z)}{\left(z^2+1\right)^2}=\lim_{z\to i}\frac{\text{d}}{\text{d}z}\left[\frac{\ln^3(z)}{\left(z+i\right)^2}\right]=\lim_{z\to i}\frac{\ln^{2}(z)\left(3i+3z-2z\ln(z)\right)}{z\left(z+i\right)^{3}}=-\frac{\pi^{3}}{32}-\frac{3i\pi^{2}}{16}$$ $$\implies\mathop{\mathrm{Res}}_{z = -i}\frac{\ln^3(z)}{\left(z^2+1\right)^2}=\lim_{z\to -i}\frac{\text{d}}{\text{d}z}\left[\frac{\ln^3(z)}{\left(z-i\right)^2}\right]=\lim_{z\to i}\frac{\ln^{2}(z)\left(3z-3i-2z\ln(z)\right)}{z\left(z-i\right)^{3}}=-\frac{\pi^{3}}{32}+\frac{3i\pi^{2}}{16}$$ which when added and multiplied with $2\pi i$ give $-\dfrac{i\pi^4}{8}$ . Equating only imaginary parts of both sides gives $$-6\pi I + 2\pi^4 = -\frac{\pi^4}{8}$$ $$\implies I=\frac{17\pi^3}{48}$$ This is obviously wrong. Plus, the real part is nonzero too so my final equation is nonsense. Where did I make a mistake? Thanks in advance!","I want to integrate using contour integration. I set up the function and used a keyhole contour with a branch cut about the positive real axis The integrals about and I believe both go to if I did not make any mistakes in my calculations but afaik I should be right. The integrals about and is which expands and simplifies into Since has poles of order at I use the higher order residue formula and get which when added and multiplied with give . Equating only imaginary parts of both sides gives This is obviously wrong. Plus, the real part is nonzero too so my final equation is nonsense. Where did I make a mistake? Thanks in advance!",I=\int^{\infty}_0\frac{\ln^2(x)}{\left(x^2+1\right)^2}\text{ d}x=\frac{\pi^3}{16} f(z)=\frac{\ln^3(z)}{\left(z^2+1\right)^2} \Gamma \gamma 0 T B \int^{\infty}_{0}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{\left(\ln(x)+2\pi i\right)^3\text{ d}x}{\left(x^2+1\right)^2} \int^{\infty}_{0}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{\ln^3(x)\text{ d}x}{\left(x^2+1\right)^2}+\int^{0}_{\infty}\frac{6i\pi\ln^2(x)\text{ d}x}{\left(x^2+1\right)^2}-\int^{0}_{\infty}\frac{12\pi^2\ln(x)\text{ d}x}{\left(x^2+1\right)^2}-\int^{0}_{\infty}\frac{8i\pi^3\text{ d}x}{\left(x^2+1\right)^2} \implies -6i\pi I +\int_{0}^{\infty}\frac{12\pi^2\ln(x)\text{ d}x}{\left(x^2+1\right)^2}+\int_{0}^{\infty}\frac{8i\pi^3\text{ d}x}{\left(x^2+1\right)^2} f(z) 2 z=\pm i \implies\mathop{\mathrm{Res}}_{z = i}\frac{\ln^3(z)}{\left(z^2+1\right)^2}=\lim_{z\to i}\frac{\text{d}}{\text{d}z}\left[\frac{\ln^3(z)}{\left(z+i\right)^2}\right]=\lim_{z\to i}\frac{\ln^{2}(z)\left(3i+3z-2z\ln(z)\right)}{z\left(z+i\right)^{3}}=-\frac{\pi^{3}}{32}-\frac{3i\pi^{2}}{16} \implies\mathop{\mathrm{Res}}_{z = -i}\frac{\ln^3(z)}{\left(z^2+1\right)^2}=\lim_{z\to -i}\frac{\text{d}}{\text{d}z}\left[\frac{\ln^3(z)}{\left(z-i\right)^2}\right]=\lim_{z\to i}\frac{\ln^{2}(z)\left(3z-3i-2z\ln(z)\right)}{z\left(z-i\right)^{3}}=-\frac{\pi^{3}}{32}+\frac{3i\pi^{2}}{16} 2\pi i -\dfrac{i\pi^4}{8} -6\pi I + 2\pi^4 = -\frac{\pi^4}{8} \implies I=\frac{17\pi^3}{48},"['complex-analysis', 'definite-integrals', 'improper-integrals', 'contour-integration']"
49,Show that a complex function is bounded,Show that a complex function is bounded,,"Let $\mathbb{D}= \{ z\in \mathbb{C}: |z|<1\}$ . For $t\in \mathbb{R}$ , let $f_t$ denote the holomorphic function on $\mathbb{D}$ defined by $f_t(z)= (\frac{1+z}{1-z})^{it}$ , $z\in \mathbb{D}$ with respect to the principal branch of the logarithm. Show that there is $C>0$ such that for all $t\in \mathbb{R}$ , we have $\sup_{z\in \mathbb{D}}|f_t(z)|<C^t$ I know that $\frac{1+z}{1-z}$ is a mobius transformation. Also, we have $f_t(z)= (\frac{1+z}{1-z})^{it} = \exp(it\frac{\log(1+z)}{\log(1-z)})) = \exp(it\log (1+z))\exp(-it\log (1-z))) = (1+z)^{it} (1-z)^{-it}$ . From here, I'm not sure how to proceed to bound such function. Thanks in advance!","Let . For , let denote the holomorphic function on defined by , with respect to the principal branch of the logarithm. Show that there is such that for all , we have I know that is a mobius transformation. Also, we have . From here, I'm not sure how to proceed to bound such function. Thanks in advance!",\mathbb{D}= \{ z\in \mathbb{C}: |z|<1\} t\in \mathbb{R} f_t \mathbb{D} f_t(z)= (\frac{1+z}{1-z})^{it} z\in \mathbb{D} C>0 t\in \mathbb{R} \sup_{z\in \mathbb{D}}|f_t(z)|<C^t \frac{1+z}{1-z} f_t(z)= (\frac{1+z}{1-z})^{it} = \exp(it\frac{\log(1+z)}{\log(1-z)})) = \exp(it\log (1+z))\exp(-it\log (1-z))) = (1+z)^{it} (1-z)^{-it},['complex-analysis']
50,Evaluate the definite integral $I = \int_0^\pi \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)}$,Evaluate the definite integral,I = \int_0^\pi \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)},"I'm studying for my upcoming complex analysis qualifying exam by working through problems in past exams. For this problem, I'd like to know (1) if my answer is correct and complete (i.e. whether I've made any errors/omissions), and (2) if there are any better/faster ways of evaluating this integral. Thanks! Problem: Evaluate the definite integral $$I = \int_0^\pi \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)}.$$ Attempted Solution: We first note that the integrand is an even function, thus $$I = \int_0^\pi \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)} = \frac{1}{2}\int_{-\pi}^{\pi} \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)}. $$ Let $z=e^{i\theta} \implies d\theta = \frac{dz}{iz}$ , then we have $$\sin\theta =\frac{e^{i\theta} - e^{-i\theta}}{2i} =\frac{z-z^{-1}}{2i} \implies \sin^2\theta = \frac{z^2-2+z^{-2}}{-4} $$ and $$\cos\theta = \frac{e^{i\theta} + e^{-i\theta}}{2} = \frac{z+z^{-1}}{2}. $$ We can now substitute such that for the unit circle $\gamma(\theta) = e^{i\theta}, \;\theta\in[-\pi,\pi]$ , we have \begin{align*} \frac{1}{2}\int_{-\pi}^{\pi} \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)} &= \frac{1}{2}\oint_\gamma \frac{(-\frac{1}{4})(z^2-2+z^{-2})}{10-3(z+z^{-1})}\frac{dz}{iz} \\ &= \frac{i}{8} \oint_\gamma \frac{z^2-2+z^{-2}}{z(10-3z-3z^{-1})} dz \\ &= \frac{i}{8} \oint_\gamma \frac{z^4 - 2z^2 + 1}{z^2(10z-3z^2 -3)} dz \\ &= -\frac{i}{24} \oint_\gamma \frac{z^4-2z^2 + 1}{z^2(z^2-\frac{10}{3}z + 1)} dz \\ &= -\frac{i}{24} \oint_\gamma \frac{z^4-2z^2 + 1}{z^2(z-3)(z-\frac{1}{3})} dz \end{align*} It's clear that our integrand has a pole of order 2 at $z=0$ , and two simple poles at $z=3$ and $z=1/3$ . Using the Residue Theorem, we can evaluate this integral as $2\pi i$ times the sum of the residues at $z=0$ and $z=1/3$ , disregarding $z=3$ since this pole is outside of the curve $\gamma$ . At the simple pole $z=1/3$ , we calculate \begin{align*} \text{Res}(1/3) &= \lim_{z\to\frac{1}{3}} (z-\frac{1}{3})\left(-\frac{i}{24} \frac{z^4-2z^2 + 1}{z^2(z-3)(z-\frac{1}{3})} \right) \\ &= -\frac{i}{24}\lim_{z\to\frac{1}{3}} \left( \frac{z^4-2z^2 + 1}{z^2(z-3)} \right) \\ &= \frac{i}{9} \end{align*} The pole at $z=0$ is of order 2, therefore we calculate \begin{align*} \text{Res}(0) &= \lim_{z\to 0}\frac{d}{dz} z^2 \left(-\frac{i}{24} \frac{z^4-2z^2 + 1}{z^2(z-3)(z-\frac{1}{3})} \right) \\ &= -\frac{i}{24} \lim_{z\to 0}\frac{d}{dz} \left( \frac{z^4-2z^2 + 1}{(z-3)(z-\frac{1}{3})} \right) \\ &= -\frac{i}{24} \lim_{z\to 0} \frac{(4z^3-4z)(z-3)(z-\frac{1}{3})-(z^4-2z^2+1)(2z-\frac{10}{3})}{(z-3)^2(z-\frac{1}{3})^2}\\ &= \frac{-5i}{36} \end{align*} Finally, we calculate $$ I=2\pi i\Big(\text{Res}(0) + \text{Res}(\frac{1}{3})\Big) = 2\pi i\Big(\frac{-5i}{36} + \frac{i}{9}\Big) = \frac{\pi}{18} $$","I'm studying for my upcoming complex analysis qualifying exam by working through problems in past exams. For this problem, I'd like to know (1) if my answer is correct and complete (i.e. whether I've made any errors/omissions), and (2) if there are any better/faster ways of evaluating this integral. Thanks! Problem: Evaluate the definite integral Attempted Solution: We first note that the integrand is an even function, thus Let , then we have and We can now substitute such that for the unit circle , we have It's clear that our integrand has a pole of order 2 at , and two simple poles at and . Using the Residue Theorem, we can evaluate this integral as times the sum of the residues at and , disregarding since this pole is outside of the curve . At the simple pole , we calculate The pole at is of order 2, therefore we calculate Finally, we calculate","I = \int_0^\pi \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)}. I = \int_0^\pi \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)} = \frac{1}{2}\int_{-\pi}^{\pi} \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)}.  z=e^{i\theta} \implies d\theta = \frac{dz}{iz} \sin\theta =\frac{e^{i\theta} - e^{-i\theta}}{2i} =\frac{z-z^{-1}}{2i} \implies \sin^2\theta = \frac{z^2-2+z^{-2}}{-4}  \cos\theta = \frac{e^{i\theta} + e^{-i\theta}}{2} = \frac{z+z^{-1}}{2}.  \gamma(\theta) = e^{i\theta}, \;\theta\in[-\pi,\pi] \begin{align*}
\frac{1}{2}\int_{-\pi}^{\pi} \frac{\sin^2(\theta) d\theta}{10-6\cos(\theta)} &= \frac{1}{2}\oint_\gamma \frac{(-\frac{1}{4})(z^2-2+z^{-2})}{10-3(z+z^{-1})}\frac{dz}{iz} \\
&= \frac{i}{8} \oint_\gamma \frac{z^2-2+z^{-2}}{z(10-3z-3z^{-1})} dz \\
&= \frac{i}{8} \oint_\gamma \frac{z^4 - 2z^2 + 1}{z^2(10z-3z^2 -3)} dz \\
&= -\frac{i}{24} \oint_\gamma \frac{z^4-2z^2 + 1}{z^2(z^2-\frac{10}{3}z + 1)} dz \\
&= -\frac{i}{24} \oint_\gamma \frac{z^4-2z^2 + 1}{z^2(z-3)(z-\frac{1}{3})} dz
\end{align*} z=0 z=3 z=1/3 2\pi i z=0 z=1/3 z=3 \gamma z=1/3 \begin{align*}
\text{Res}(1/3) &= \lim_{z\to\frac{1}{3}} (z-\frac{1}{3})\left(-\frac{i}{24} \frac{z^4-2z^2 + 1}{z^2(z-3)(z-\frac{1}{3})} \right) \\
&= -\frac{i}{24}\lim_{z\to\frac{1}{3}} \left( \frac{z^4-2z^2 + 1}{z^2(z-3)} \right) \\
&= \frac{i}{9}
\end{align*} z=0 \begin{align*}
\text{Res}(0) &= \lim_{z\to 0}\frac{d}{dz} z^2 \left(-\frac{i}{24} \frac{z^4-2z^2 + 1}{z^2(z-3)(z-\frac{1}{3})} \right) \\
&= -\frac{i}{24} \lim_{z\to 0}\frac{d}{dz} \left( \frac{z^4-2z^2 + 1}{(z-3)(z-\frac{1}{3})} \right) \\
&= -\frac{i}{24} \lim_{z\to 0} \frac{(4z^3-4z)(z-3)(z-\frac{1}{3})-(z^4-2z^2+1)(2z-\frac{10}{3})}{(z-3)^2(z-\frac{1}{3})^2}\\
&= \frac{-5i}{36}
\end{align*} 
I=2\pi i\Big(\text{Res}(0) + \text{Res}(\frac{1}{3})\Big) = 2\pi i\Big(\frac{-5i}{36} + \frac{i}{9}\Big) = \frac{\pi}{18}
","['complex-analysis', 'solution-verification', 'complex-integration']"
51,Are substitutions relating multiple integration variables in iterated integrals always valid?,Are substitutions relating multiple integration variables in iterated integrals always valid?,,"A standard technique of mine when evaluating iterated integrals, such as double integrals, is to invoke a substitution that relates the integration variables of both integrals. To demonstrate and clarify what I mean, I shall present three examples for double integrals: Example 1: Gaussian integral, $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$ $$I=\Gamma \left(\frac{1}{2}\right)^2 = \int_{0}^{\infty} \int_{0}^{\infty} \frac{1}{\sqrt{x t}}e^{-(t+x)}\,dx\,dt$$ We now enforce the substituion $t=sx \implies dt=x\,ds$ : $$\implies I = \int_{0}^{\infty}\int_{0}^{\infty}\frac{1}{\sqrt{s}}e^{-x (s+1)}\,dx\,ds=\int_{0}^{\infty}\frac{1}{\sqrt{s}(s+1)} \,ds=\pi$$ $$\implies \Gamma\left(\frac{1}{2}\right)=\int_{-\infty}^{\infty}e^{-x^2}\,dx=\sqrt{\pi}$$ This is a much more elementary approach than the common Jacobian polar coordinates method of evaluating the Gaussian integral and is how I originally discovered the solution for myself a few years ago prior to learning about the Jacobian. Example 2: In a previous answer of mine , I determined the following result: $$J=\frac{1}{2\pi}\int_{-\infty}^{\infty} \exp\left(\frac{x^2}{2}-\frac{y^2}{2}\right)K_0\left(\sqrt{(x-y)^2+\left(\frac{y^2}{2}-\frac{x^2}{2}\right)^2}\right)\,dy = \frac{e}{2} \sqrt{\pi}\operatorname{erfc}(1)$$ and is independent of $x$ . Here $K_0$ is the modified Bessel function of the second kind of order $0$ . A crucial part of my method of evaluation was to invoke a particular substitution on the integral: $$J=\frac{1}{2\pi}\int_0^\infty\int_0^\infty \frac{\cosh\left(xv\left(\frac{v^2}{4t}+1\right)\right)}{t}\exp\left(-t-\frac{1+2t+x^2}{4t}v^2-\frac{v^4}{16t}\right)\,dv\,dt$$ We now enforce the substitution $t = s v^2 \implies dt = v^2 \,ds$ : $$\begin{align}J&=\frac{1}{2\pi} \int_{0}^{\infty} \int_{0}^{\infty}\frac{\cosh\left(x v \left(\frac{1}{4s}+1\right)\right)}{s} \exp\left(-sv^2-\frac{1+2sv^2+x^2}{4s}-\frac{v^2}{16s}\right) \, dv \, ds\\&=\frac{1}{2\pi}\int_{0}^{\infty}\int_{0}^{\infty}\frac{\cosh\left(xv\left(\frac{1}{4s}+1\right)\right)}{s}\exp\left(-\frac{1+x^2}{4s}\right)\exp\left(-v^2 \frac{(1+4s)^2}{16s}\right)\,dv\,ds\end{align}$$ This allows the result to fall out readily, since: $$\int_{0}^{\infty}\cosh(a v)\exp\left(-v^2 b^2\right) \, dv=\frac{\sqrt{\pi}}{2b}\exp\left(\frac{a^2}{4b^2}\right)$$ $$\implies J = \frac{1}{2\pi} \int_{0}^{\infty} \frac{\exp \left(-\frac{1+x^2}{4s}\right)}{s} \cdot \frac{\sqrt{\pi}}{2\left(\frac{1+4s}{4\sqrt{s}}\right)}\exp \left(\frac{4s x^2 \left(\frac{1}{4s}+1\right)^2}{(1+4s)^2}\right)\, ds$$ This results in the integral being independent as we wanted since the $x^2$ terms cancel. $$\implies J = \frac{1}{\sqrt{\pi}} \int_{0}^{\infty} \frac{\exp\left(-\frac{1}{4s}\right)}{\sqrt{s} (1+4s)} \, ds\stackrel{s\,\mapsto\frac{1}{s}}{=}\frac{1}{\sqrt{\pi}} \int_{0}^{\infty} \frac{\exp\left(-\frac{s}{4}\right)}{\sqrt{s} (4+s)} \, ds=\frac{e}{2} \sqrt{\pi} \, \operatorname{erfc} (1)$$ Example 3: Derivation of the relationship between the Beta function and Gamma function. $\Gamma(x)\Gamma(y)=\Gamma(x+y)B(x,y)$ : $$\Gamma(x)\Gamma(y)=\int_{0}^{\infty}\int_{0}^{\infty} e^{-u-v}u^{x-1}v^{y-1}\,du\,dy$$ We now enforce the substitution $u=zt$ and $v=z(1-t)$ : $$\begin{align}\Gamma(x)\Gamma(y)&=\int_{0}^{\infty}\int_{0}^{1}e^{-z}(zt)^{x-1}(z(1-t))^{y-1} z\,dt\,dz\\ &=\int_{0}^{\infty}e^{-z}z^{x+y-1}\,dz \cdot \int_{0}^{1}t^{x-1}(1-t)^{y-1}\\&=\Gamma(x+y)B(x,y)\end{align}$$ This is interestingly the method mentioned in the derivation of this property on the Wikipedia page of the Beta function . In this question about this derivation, the comments and answer invoke the Jacobian, suggesting that such substitutions can be made rigorous using the Jacobian, however, I would appreciate seeing exactly how that works as I only know about the Jacobian based on my own self-teaching. So to summarise is it always valid to perform substitutions on iterated integrals, such as double or triple integrals in this way, whereby one or more integration variables are written in terms of an integration variable of a different integral in the iteration. I have never come across any issue in using this technique in the past, but have never found rigorous proof in any literature that states that such substitutions are valid; it surprises me that such a simple solution to the Gaussian integral is not often used.","A standard technique of mine when evaluating iterated integrals, such as double integrals, is to invoke a substitution that relates the integration variables of both integrals. To demonstrate and clarify what I mean, I shall present three examples for double integrals: Example 1: Gaussian integral, We now enforce the substituion : This is a much more elementary approach than the common Jacobian polar coordinates method of evaluating the Gaussian integral and is how I originally discovered the solution for myself a few years ago prior to learning about the Jacobian. Example 2: In a previous answer of mine , I determined the following result: and is independent of . Here is the modified Bessel function of the second kind of order . A crucial part of my method of evaluation was to invoke a particular substitution on the integral: We now enforce the substitution : This allows the result to fall out readily, since: This results in the integral being independent as we wanted since the terms cancel. Example 3: Derivation of the relationship between the Beta function and Gamma function. : We now enforce the substitution and : This is interestingly the method mentioned in the derivation of this property on the Wikipedia page of the Beta function . In this question about this derivation, the comments and answer invoke the Jacobian, suggesting that such substitutions can be made rigorous using the Jacobian, however, I would appreciate seeing exactly how that works as I only know about the Jacobian based on my own self-teaching. So to summarise is it always valid to perform substitutions on iterated integrals, such as double or triple integrals in this way, whereby one or more integration variables are written in terms of an integration variable of a different integral in the iteration. I have never come across any issue in using this technique in the past, but have never found rigorous proof in any literature that states that such substitutions are valid; it surprises me that such a simple solution to the Gaussian integral is not often used.","\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi} I=\Gamma \left(\frac{1}{2}\right)^2 = \int_{0}^{\infty} \int_{0}^{\infty} \frac{1}{\sqrt{x t}}e^{-(t+x)}\,dx\,dt t=sx \implies dt=x\,ds \implies I = \int_{0}^{\infty}\int_{0}^{\infty}\frac{1}{\sqrt{s}}e^{-x (s+1)}\,dx\,ds=\int_{0}^{\infty}\frac{1}{\sqrt{s}(s+1)} \,ds=\pi \implies \Gamma\left(\frac{1}{2}\right)=\int_{-\infty}^{\infty}e^{-x^2}\,dx=\sqrt{\pi} J=\frac{1}{2\pi}\int_{-\infty}^{\infty} \exp\left(\frac{x^2}{2}-\frac{y^2}{2}\right)K_0\left(\sqrt{(x-y)^2+\left(\frac{y^2}{2}-\frac{x^2}{2}\right)^2}\right)\,dy = \frac{e}{2} \sqrt{\pi}\operatorname{erfc}(1) x K_0 0 J=\frac{1}{2\pi}\int_0^\infty\int_0^\infty \frac{\cosh\left(xv\left(\frac{v^2}{4t}+1\right)\right)}{t}\exp\left(-t-\frac{1+2t+x^2}{4t}v^2-\frac{v^4}{16t}\right)\,dv\,dt t = s v^2 \implies dt = v^2 \,ds \begin{align}J&=\frac{1}{2\pi} \int_{0}^{\infty} \int_{0}^{\infty}\frac{\cosh\left(x v \left(\frac{1}{4s}+1\right)\right)}{s} \exp\left(-sv^2-\frac{1+2sv^2+x^2}{4s}-\frac{v^2}{16s}\right) \, dv \, ds\\&=\frac{1}{2\pi}\int_{0}^{\infty}\int_{0}^{\infty}\frac{\cosh\left(xv\left(\frac{1}{4s}+1\right)\right)}{s}\exp\left(-\frac{1+x^2}{4s}\right)\exp\left(-v^2 \frac{(1+4s)^2}{16s}\right)\,dv\,ds\end{align} \int_{0}^{\infty}\cosh(a v)\exp\left(-v^2 b^2\right) \, dv=\frac{\sqrt{\pi}}{2b}\exp\left(\frac{a^2}{4b^2}\right) \implies J = \frac{1}{2\pi} \int_{0}^{\infty} \frac{\exp \left(-\frac{1+x^2}{4s}\right)}{s} \cdot \frac{\sqrt{\pi}}{2\left(\frac{1+4s}{4\sqrt{s}}\right)}\exp \left(\frac{4s x^2 \left(\frac{1}{4s}+1\right)^2}{(1+4s)^2}\right)\, ds x^2 \implies J = \frac{1}{\sqrt{\pi}} \int_{0}^{\infty} \frac{\exp\left(-\frac{1}{4s}\right)}{\sqrt{s} (1+4s)} \, ds\stackrel{s\,\mapsto\frac{1}{s}}{=}\frac{1}{\sqrt{\pi}} \int_{0}^{\infty} \frac{\exp\left(-\frac{s}{4}\right)}{\sqrt{s} (4+s)} \, ds=\frac{e}{2} \sqrt{\pi} \, \operatorname{erfc} (1) \Gamma(x)\Gamma(y)=\Gamma(x+y)B(x,y) \Gamma(x)\Gamma(y)=\int_{0}^{\infty}\int_{0}^{\infty} e^{-u-v}u^{x-1}v^{y-1}\,du\,dy u=zt v=z(1-t) \begin{align}\Gamma(x)\Gamma(y)&=\int_{0}^{\infty}\int_{0}^{1}e^{-z}(zt)^{x-1}(z(1-t))^{y-1} z\,dt\,dz\\ &=\int_{0}^{\infty}e^{-z}z^{x+y-1}\,dz \cdot \int_{0}^{1}t^{x-1}(1-t)^{y-1}\\&=\Gamma(x+y)B(x,y)\end{align}","['real-analysis', 'calculus', 'integration', 'complex-analysis', 'jacobian']"
52,"Elementary proof that $\sum_{j=1}^{n} \prod_{k \neq j} \frac{1}{1+(a_j - a_k + i)^2} \in \mathbb{R}$ for $a_1, \dots, a_n \in \mathbb{R}$ distinct",Elementary proof that  for  distinct,"\sum_{j=1}^{n} \prod_{k \neq j} \frac{1}{1+(a_j - a_k + i)^2} \in \mathbb{R} a_1, \dots, a_n \in \mathbb{R}","By a straightforward contour integral, one can show that for $a_1, \dots, a_n \in \mathbb{R}$ distinct, we have $$\frac{1}{\pi} \int_{-\infty}^{\infty} \prod_{j=1}^{n} \frac{1}{1+(x-a_j)^2} \, dx = \sum_{j=1}^{n} \prod_{k \neq j} \frac{1}{1+(a_j - a_k  + i)^2}$$ and therefore the latter sum is real-valued. Is there an elementary (or at least purely algebraic) way to show this, without using complex analysis?","By a straightforward contour integral, one can show that for distinct, we have and therefore the latter sum is real-valued. Is there an elementary (or at least purely algebraic) way to show this, without using complex analysis?","a_1, \dots, a_n \in \mathbb{R} \frac{1}{\pi} \int_{-\infty}^{\infty} \prod_{j=1}^{n} \frac{1}{1+(x-a_j)^2} \, dx = \sum_{j=1}^{n} \prod_{k \neq j} \frac{1}{1+(a_j - a_k  + i)^2}","['abstract-algebra', 'complex-analysis', 'complex-numbers']"
53,Proof Explanation: Cauchy's Theorem (Homotopy Version) - Why take discs of radii $3\epsilon$?,Proof Explanation: Cauchy's Theorem (Homotopy Version) - Why take discs of radii ?,3\epsilon,"I have two questions about the proof of Cauchy's Theorem (Homotopy Version) in Stein & Shakarchi's Complex Analysis . This is Theorem $5.1$ , in Chapter $3$ . If you have a copy, see Pg. $93-95$ . By uniform continuity of $F$ , how do we get $\delta$ such that $$|s_1 - s_2| < \delta \implies \sup_{t\in [a,b]} |\gamma_{s_1}(t) - \gamma_{s_2}(t)| < \epsilon$$ My thoughts: By uniform continuity of $F$ , we have for every $\epsilon > 0$ , some $\delta > 0$ such that $\sqrt{(s_1-s_2)^2 + (t_1-t_2)^2}  < \delta$ implies $|F(s_1,t_1) - F(s_2,t_2)| = |\gamma_{s_1}(t_1) - \gamma_{s_2}(t_2)| < \epsilon$ . To prove the required implication above, we only need to show the continuity of the map $\varphi: [0,1] \to \mathcal C([a,b], \Bbb C)$ given by $\varphi(s) = F_s$ . $\mathcal C([a,b], \Bbb C)$ is endowed with the $\sup$ metric. What is the significance behind taking $3\epsilon$ and $2\epsilon$ in the proof? Why can't we take discs $\{D_0, \ldots, D_n\}$ of radii $\epsilon$ ? Pretty sure this has something to do with the claim from uniform continuity above, but I'm not able to figure it out. For $|s_1-s_2| < \delta$ , we have $|\gamma_{s_1}(t) - \gamma_{s_2}(t)| < \epsilon$ for all $t\in [a,b]$ , the two curves $\gamma_{s_1}$ and $\gamma_{s_2}$ could easily be contained in a union of discs of radii $\epsilon$ as well. Reference:","I have two questions about the proof of Cauchy's Theorem (Homotopy Version) in Stein & Shakarchi's Complex Analysis . This is Theorem , in Chapter . If you have a copy, see Pg. . By uniform continuity of , how do we get such that My thoughts: By uniform continuity of , we have for every , some such that implies . To prove the required implication above, we only need to show the continuity of the map given by . is endowed with the metric. What is the significance behind taking and in the proof? Why can't we take discs of radii ? Pretty sure this has something to do with the claim from uniform continuity above, but I'm not able to figure it out. For , we have for all , the two curves and could easily be contained in a union of discs of radii as well. Reference:","5.1 3 93-95 F \delta |s_1 - s_2| < \delta \implies \sup_{t\in [a,b]} |\gamma_{s_1}(t) - \gamma_{s_2}(t)| < \epsilon F \epsilon > 0 \delta > 0 \sqrt{(s_1-s_2)^2 + (t_1-t_2)^2}  < \delta |F(s_1,t_1) - F(s_2,t_2)| = |\gamma_{s_1}(t_1) - \gamma_{s_2}(t_2)| < \epsilon \varphi: [0,1] \to \mathcal C([a,b], \Bbb C) \varphi(s) = F_s \mathcal C([a,b], \Bbb C) \sup 3\epsilon 2\epsilon \{D_0, \ldots, D_n\} \epsilon |s_1-s_2| < \delta |\gamma_{s_1}(t) - \gamma_{s_2}(t)| < \epsilon t\in [a,b] \gamma_{s_1} \gamma_{s_2} \epsilon","['complex-analysis', 'analysis', 'proof-explanation']"
54,Is there a connection between the analytical properties of complex analytic functions and the complex numbers being complete algebraically?,Is there a connection between the analytical properties of complex analytic functions and the complex numbers being complete algebraically?,,I will first have to apologize that this will be a very fuzzy question. At this time I have no better way to formulate it. I am willing to reformulate it when I can better pinpoint what it is that gives me this idea. Is there a connection between the analytic properties of complex valued functions and the complex numbers being complete algebraically? Meromorphic functions we can write as $$f(z) = \frac{\prod_k (z-h_k)}{\prod_k (z-p_k)}$$ We can show that any expression of the form  (thanks to the fund theorem of algebra) $$\frac{1}{f(z)}+\frac{1}{g(z)}=\frac{h(z)}{f(z)g(z)}$$ Where $z\to h(z)$ is ensured to be factorable. Does this have any connection to  the overly nice properties of complex analytic functions?,I will first have to apologize that this will be a very fuzzy question. At this time I have no better way to formulate it. I am willing to reformulate it when I can better pinpoint what it is that gives me this idea. Is there a connection between the analytic properties of complex valued functions and the complex numbers being complete algebraically? Meromorphic functions we can write as We can show that any expression of the form  (thanks to the fund theorem of algebra) Where is ensured to be factorable. Does this have any connection to  the overly nice properties of complex analytic functions?,f(z) = \frac{\prod_k (z-h_k)}{\prod_k (z-p_k)} \frac{1}{f(z)}+\frac{1}{g(z)}=\frac{h(z)}{f(z)g(z)} z\to h(z),"['abstract-algebra', 'complex-analysis', 'complex-numbers', 'soft-question', 'meromorphic-functions']"
55,"Proving that $2\delta\sin(x)-\sin((N+1)x)+\delta^2\sin((N-1)x)=0$, for $N>1$ and $\delta\in[0,1]$, has real solutions","Proving that , for  and , has real solutions","2\delta\sin(x)-\sin((N+1)x)+\delta^2\sin((N-1)x)=0 N>1 \delta\in[0,1]","We have the equation \begin{equation} 2\delta\sin(x)-\sin((N+1)x)+\delta^2\sin((N-1)x) = 0, \end{equation} where $N$ is a positive integer greater than $1$ and $\delta\in[0,1]$ . Now, numerical studies indicate that the solutions to this equation are always real, but we have not been able to prove it. One idea was to use Rouche's Theorem , but that would require us to know about the number of real solutions in e.g. the interval $[0,\pi]$ , and it seems hard to implement anyway since the equation has three terms. Another idea was to rewrite the equation in a convenient way, let $x = a+bi$ and see that the only way to have the right hand side be a real number, was to put $b=0$ , but that just generated another equation that was hard to solve. Finally, we tried to argue in the following way: We know that for $\delta=0$ , the equation has only real solutions, and we know the number of them. If we increase $\delta$ and show that the maxima of the curve $\sin((N+1)x)+\delta^2\sin((N-1)x)$ in the interval $[0,\pi]$ lie above the curve $2\delta\sin(x)$ for all $\delta$ (or precisely on the curve for $\delta=1$ ), we will not ""lose"" any real solutions. But analyzing the maxima was not so straightforward either.","We have the equation where is a positive integer greater than and . Now, numerical studies indicate that the solutions to this equation are always real, but we have not been able to prove it. One idea was to use Rouche's Theorem , but that would require us to know about the number of real solutions in e.g. the interval , and it seems hard to implement anyway since the equation has three terms. Another idea was to rewrite the equation in a convenient way, let and see that the only way to have the right hand side be a real number, was to put , but that just generated another equation that was hard to solve. Finally, we tried to argue in the following way: We know that for , the equation has only real solutions, and we know the number of them. If we increase and show that the maxima of the curve in the interval lie above the curve for all (or precisely on the curve for ), we will not ""lose"" any real solutions. But analyzing the maxima was not so straightforward either.","\begin{equation}
2\delta\sin(x)-\sin((N+1)x)+\delta^2\sin((N-1)x) = 0,
\end{equation} N 1 \delta\in[0,1] [0,\pi] x = a+bi b=0 \delta=0 \delta \sin((N+1)x)+\delta^2\sin((N-1)x) [0,\pi] 2\delta\sin(x) \delta \delta=1","['complex-analysis', 'trigonometry']"
56,Understanding contour integral with branch cuts,Understanding contour integral with branch cuts,,"I was trying to understand contour integral which have or those involving branch cuts. Like $$I=\int_0^\infty\frac{dx}{x^3+1}$$ because the integrand is not even we cannot extend the integration to the whole real axis and then halve the result. However, suppose we look at the contour integral $$J=\oint_C\frac{\ln z \:dz}{x^3+1}$$ around the Keyhole contour . Everything seems nice until now. But then I get to know that ""There is a connection between $J$ and the original definite integral $I$ "" $$J=2\pi iI$$ I get the partial intuition from one of M.SE answer, but couldn't understand the logic. \begin{align}&\bbox[10px,#ffd]{\oint_C\frac{\ln z}{1+x^3}dz}=2\pi i\sum\text{Res}f(z)=\int_0^\infty\frac{\ln z}{1+z^3}dz+\int_\infty^0 \frac{\overbrace{\ln z+2\pi i}^\star}{1+z^3}dz=-2\pi i\int_0^\infty\frac{1}{1+z^3}dz\end{align} Here I don't understand the $\star$ . I think $\ln z=\ln r+i\theta+2\pi in$ used, but aren't we evaluate the integral on same branch? Then why to consider $2\pi i$ ? Some contour integral which have branch cuts use wedge-shaped contour. Where $\Gamma = \Gamma_1 + \Gamma_2 + \Gamma_3$ going in a straight line $\Gamma_1$ from $0$ to $R$ on the real axis, then a circular arc $\Gamma_2$ , then a straight line $\Gamma_3$ back to $0$ . But I couldn't understand which angle I should use for $\Gamma_3$ ? Like for $f(z) = \frac{z^n}{1+z^m}$ , it was suggested to use a wedge-shaped contour of angle $\frac{2\pi}{m}$ . Is there any trick or rule of thumb which help to decide the angle?","I was trying to understand contour integral which have or those involving branch cuts. Like because the integrand is not even we cannot extend the integration to the whole real axis and then halve the result. However, suppose we look at the contour integral around the Keyhole contour . Everything seems nice until now. But then I get to know that ""There is a connection between and the original definite integral "" I get the partial intuition from one of M.SE answer, but couldn't understand the logic. Here I don't understand the . I think used, but aren't we evaluate the integral on same branch? Then why to consider ? Some contour integral which have branch cuts use wedge-shaped contour. Where going in a straight line from to on the real axis, then a circular arc , then a straight line back to . But I couldn't understand which angle I should use for ? Like for , it was suggested to use a wedge-shaped contour of angle . Is there any trick or rule of thumb which help to decide the angle?","I=\int_0^\infty\frac{dx}{x^3+1} J=\oint_C\frac{\ln z \:dz}{x^3+1} J I J=2\pi iI \begin{align}&\bbox[10px,#ffd]{\oint_C\frac{\ln z}{1+x^3}dz}=2\pi i\sum\text{Res}f(z)=\int_0^\infty\frac{\ln z}{1+z^3}dz+\int_\infty^0 \frac{\overbrace{\ln z+2\pi i}^\star}{1+z^3}dz=-2\pi i\int_0^\infty\frac{1}{1+z^3}dz\end{align} \star \ln z=\ln r+i\theta+2\pi in 2\pi i \Gamma = \Gamma_1 + \Gamma_2 + \Gamma_3 \Gamma_1 0 R \Gamma_2 \Gamma_3 0 \Gamma_3 f(z) = \frac{z^n}{1+z^m} \frac{2\pi}{m}","['complex-analysis', 'contour-integration', 'residue-calculus', 'branch-cuts']"
57,Inverting the Hopf map from $\mathbb{CP}^1$ to $\mathbb{S}^2$,Inverting the Hopf map from  to,\mathbb{CP}^1 \mathbb{S}^2,"Let $$ \alpha=e^{i\varepsilon}\cos\left(\frac{\theta}{2}\right)\qquad\beta=e^{i\kappa}\sin\left(\frac{\theta}{2}\right) $$ be 2 generic complex numbers, we could find a Hopf map from $\mathbb{CP}^1$ to $\mathbb{S}^2$ : $$ \mathcal{H}\begin{bmatrix}\alpha\\\beta\end{bmatrix} = \begin{bmatrix}2\text{Re}(\bar\alpha\beta) \\ 2\text{Im}(\bar\alpha\beta)\\|\alpha|^2-|\beta|^2\end{bmatrix} $$ I wonder is this a bijection? It looks like given a vector in $\mathbb{S}^2\subset\mathbb{R}^3$ , we could solve for $\alpha$ and $\beta$ . However, I'm not pretty sure how we could express this inverse map that goes from $\mathbb{S}^2$ to $\mathbb{CP}^1$ . Thanks!","Let be 2 generic complex numbers, we could find a Hopf map from to : I wonder is this a bijection? It looks like given a vector in , we could solve for and . However, I'm not pretty sure how we could express this inverse map that goes from to . Thanks!","
\alpha=e^{i\varepsilon}\cos\left(\frac{\theta}{2}\right)\qquad\beta=e^{i\kappa}\sin\left(\frac{\theta}{2}\right)
 \mathbb{CP}^1 \mathbb{S}^2 
\mathcal{H}\begin{bmatrix}\alpha\\\beta\end{bmatrix}
=
\begin{bmatrix}2\text{Re}(\bar\alpha\beta) \\ 2\text{Im}(\bar\alpha\beta)\\|\alpha|^2-|\beta|^2\end{bmatrix}
 \mathbb{S}^2\subset\mathbb{R}^3 \alpha \beta \mathbb{S}^2 \mathbb{CP}^1","['complex-analysis', 'hopf-fibration']"
58,When Must the Rate of Convergence be Worse on the Boundary,When Must the Rate of Convergence be Worse on the Boundary,,"WLOG, let $$f(z)=\sum_{n=0}^\infty a_nz^n$$ be a holomorphic function on the unit disc with radius of convergence $1$ around $z=0$ . My question is about a sense in which we can say that the rate of convergence of the taylor expansion of $f$ around $0$ must be worse everywhere on the boundary of the disc of convergence than in the interior. If we define $$E_n(z) = \left|f(z) - \sum_{k=0}^n a_k z^k\right|$$ to be the error resulting from the $n$ -term truncation of the Taylor series. Then more specifically, the question can be stated as follows: Given some $z_1$ in the interior of the unit disc, and some $z_2$ on the boundary, and perhaps some conditions on $f$ , do we know that for all sufficiently large $n$ , $E_n(z_1) \leq E_n(z_2)$ ? I believe that this holds automatically if the Taylor series of $f$ diverges at $z_2$ , but either way the interesting case seems to me to be the one where the Taylor series converges at $z_2$ , so that can be assumed if you like. By Cauchy-Hadamard, we can know that for all $z$ in the interior of the unit disc (with $ 0<|z|=r < 1$ ), we have that $$\limsup_{n \to \infty} |a_nz^n|^{1/n} \leq r < 1 $$ and that we can not obviously say that about the boundary. But it seems a lot less trivial to show lower bounds on the error.","WLOG, let be a holomorphic function on the unit disc with radius of convergence around . My question is about a sense in which we can say that the rate of convergence of the taylor expansion of around must be worse everywhere on the boundary of the disc of convergence than in the interior. If we define to be the error resulting from the -term truncation of the Taylor series. Then more specifically, the question can be stated as follows: Given some in the interior of the unit disc, and some on the boundary, and perhaps some conditions on , do we know that for all sufficiently large , ? I believe that this holds automatically if the Taylor series of diverges at , but either way the interesting case seems to me to be the one where the Taylor series converges at , so that can be assumed if you like. By Cauchy-Hadamard, we can know that for all in the interior of the unit disc (with ), we have that and that we can not obviously say that about the boundary. But it seems a lot less trivial to show lower bounds on the error.",f(z)=\sum_{n=0}^\infty a_nz^n 1 z=0 f 0 E_n(z) = \left|f(z) - \sum_{k=0}^n a_k z^k\right| n z_1 z_2 f n E_n(z_1) \leq E_n(z_2) f z_2 z_2 z  0<|z|=r < 1 \limsup_{n \to \infty} |a_nz^n|^{1/n} \leq r < 1 ,"['complex-analysis', 'taylor-expansion']"
59,Defining the complex exponential,Defining the complex exponential,,"I was pondering a bit about how to define the exponential and trigonometric functions. The definitions I find the most appealing seem to be their definitions as the unique solutions to the ODEs : $y'=y,\ y_0=1$ ; $y''=-y,\ y_0=0/1$ and $y'_0=1/0$ . However, I was wondering if this could be extended as follows: $e^z:\mathbb{C}\to \mathbb{C}$ is the unique solution to the ODE $\frac{\mathbb{d}}{\mathbb{dz}}f = f$ with $f(0)=1$ . Then you define $\cos{x}=\Re(e^{ix})$ and $\sin{x}=\Im(e^{ix})$ . Is there a theorem that allows one to prove the existence of uniqueness in this case?","I was pondering a bit about how to define the exponential and trigonometric functions. The definitions I find the most appealing seem to be their definitions as the unique solutions to the ODEs : ; and . However, I was wondering if this could be extended as follows: is the unique solution to the ODE with . Then you define and . Is there a theorem that allows one to prove the existence of uniqueness in this case?","y'=y,\ y_0=1 y''=-y,\ y_0=0/1 y'_0=1/0 e^z:\mathbb{C}\to \mathbb{C} \frac{\mathbb{d}}{\mathbb{dz}}f = f f(0)=1 \cos{x}=\Re(e^{ix}) \sin{x}=\Im(e^{ix})","['complex-analysis', 'ordinary-differential-equations']"
60,When can functions of a complex variable be integrated 'normally'?,When can functions of a complex variable be integrated 'normally'?,,"In our complex functions lecture notes, the lecturer integrates the complex function $f(z) = \frac{1}{z}$ around a circle of abitrary radius (i.e. over $z = r e^{i \theta}$ ). He first does a normal contour integral: $$\oint_C f(z) dz = i \int_0^{2\pi}\frac{re^{i \theta}}{re^{i \theta}}d\theta = 2\pi i $$ But then he says 'we could have done this directly': $$\oint_C \frac{dz}{z} = \ln(z) |^{re^{2\pi i}}_r = 2\pi i$$ . I don't understand why this 'direct' method is allowed. I could understand if we was integrating $f(z) = z$ instead of $f(z) = \frac{1}{z}$ since the former is analytic everywhere (entire). However the latter is not-defined at $z=0$ so I'm not sure why this is allowed, he seems to just get the correct answer with his limits in the final step (which I don't understand why he used instead of $r$ and $r$ etc).","In our complex functions lecture notes, the lecturer integrates the complex function around a circle of abitrary radius (i.e. over ). He first does a normal contour integral: But then he says 'we could have done this directly': . I don't understand why this 'direct' method is allowed. I could understand if we was integrating instead of since the former is analytic everywhere (entire). However the latter is not-defined at so I'm not sure why this is allowed, he seems to just get the correct answer with his limits in the final step (which I don't understand why he used instead of and etc).",f(z) = \frac{1}{z} z = r e^{i \theta} \oint_C f(z) dz = i \int_0^{2\pi}\frac{re^{i \theta}}{re^{i \theta}}d\theta = 2\pi i  \oint_C \frac{dz}{z} = \ln(z) |^{re^{2\pi i}}_r = 2\pi i f(z) = z f(z) = \frac{1}{z} z=0 r r,"['integration', 'complex-analysis', 'complex-numbers', 'complex-integration', 'cauchy-integral-formula']"
61,Is there an analytic continuation at $z=R$?,Is there an analytic continuation at ?,z=R,"Assume $f(z)$ is analytic in $D_R(0)$ (in other words, the corresponding power series $\sum_{k=0}^{\infty}a_kz^k$ has the convergence radius equal to R). Suppose we make an analytic continuation to a point $z = r + i\ 0$ with $r < R$ (see the diagram below) and consider the power series representation for f(z) centered at $z = r$ . If it happens that the corresponding power series has the convergence radius exactly $R − r$ (the blue circle), In this case, can we claim that no analytic continuation exists at the point $z = R$ ? Thanks","Assume is analytic in (in other words, the corresponding power series has the convergence radius equal to R). Suppose we make an analytic continuation to a point with (see the diagram below) and consider the power series representation for f(z) centered at . If it happens that the corresponding power series has the convergence radius exactly (the blue circle), In this case, can we claim that no analytic continuation exists at the point ? Thanks",f(z) D_R(0) \sum_{k=0}^{\infty}a_kz^k z = r + i\ 0 r < R z = r R − r z = R,"['complex-analysis', 'analytic-continuation']"
62,Minimum of $|z_1| + |z_2|+ \dots + |z_n|$ when $z_1 + \dots + z_n = 1$ and all the arguments of $z_m$ are prescribed.,Minimum of  when  and all the arguments of  are prescribed.,|z_1| + |z_2|+ \dots + |z_n| z_1 + \dots + z_n = 1 z_m,"Let $-\frac{\pi}{2} \le \phi_1 < \dots < \phi_k < 0 < \phi_{k+1} < \dots < \phi_N < \frac{\pi}{2}$ be $N$ prescribed phases. Let $a_1, \dots, a_N$ be $N$ real numbers such that $$ a_1 e^{i \phi_1} + \dots + a_N e^{i \phi_N} = 1. $$ What is the minimum of $|a_1|+\dots+|a_N|$ ? The minimum is achieved when $a_m=0$ for $m \ne k, k+1$ . The proof can be easily understood in a geometrically point of view, as kindly suggested in the following answer. Many thanks to all the answers and comments.","Let be prescribed phases. Let be real numbers such that What is the minimum of ? The minimum is achieved when for . The proof can be easily understood in a geometrically point of view, as kindly suggested in the following answer. Many thanks to all the answers and comments.","-\frac{\pi}{2} \le \phi_1 < \dots < \phi_k < 0 < \phi_{k+1} < \dots < \phi_N < \frac{\pi}{2} N a_1, \dots, a_N N 
a_1 e^{i \phi_1} + \dots + a_N e^{i \phi_N} = 1.
 |a_1|+\dots+|a_N| a_m=0 m \ne k, k+1","['complex-analysis', 'optimization']"
63,"If $z_1,z_2,z_3,z_4$ are consecutive vertices of a quadrilateral that lie on a circle prove the following",If  are consecutive vertices of a quadrilateral that lie on a circle prove the following,"z_1,z_2,z_3,z_4","If $z_1,z_2,z_3,z_4$ are consecutive vertices of a quadrilateral that lie on a circle prove the following: $|z_1-z_3||z_2-z_4|=|z_1-z_2||z_3-z_4|+|z_1-z_4||z_2-z_3|$ . I know that you can prove this without using the cross-ratio, but I would like to complete this proof using it. Here are some things I know: The cross-ratio is real if and only if $(z_1,z_2,z_3,z_4)$ lie on the same circle. The cross-ratio is invariant under linear transformations, i.e. $(z_1,z_2,z_3,z_4)=(Tz_1,Tz_2,Tz_3,Tz_4)$ . I am not sure if (2) will be of any use, but I definitely think (1) would be helpful. Using (1) I can only get as far as: $|z_1-z_3||z_2-z_4|=C|z_1-z_4||z_2-z_3|$ , where $C \geq 0$ . Any help is appreciated. Edit: After playing with the equation for a while this is what I concluded: Of course by simple verification, one indeed finds that $(z_1-z_3)(z_2-z_4)=(z_1-z_2)(z_3-z_4)+(z_1-z_4)(z_2-z_3)$ . Then we have $$\frac{(z_1-z_3)(z_2-z_4)}{(z_1-z_4)(z_2-z_3)}=\frac{(z_1-z_2)(z_3-z_4)}{(z_1-z_4)(z_2-z_3)}+1.$$ Now our result is proven if we can show that both of these quotients are real and positive. Well, the real part follows from the fact that a cross-ratio is real if and only if all four points lie on the same circle. Now for the positive part: The quotient on the left is the cross ratio $(z_1,z_2,z_3,z_4)$ where $z_2 \rightarrow 1$ , $z_3 \rightarrow 0$ , $z_4 \rightarrow \infty$ . Now this transformation, which is bijective, takes the circle that passes through these points to the real line. Therefore the arc from $z_4$ to $z_2$ is taken to $[-\infty,1]$ , hence $(z_1,z_2,z_3,z_4) > 1$ . A similar argument works to show that the other cross-ratio is also positive. Is this reasoning correct? I guess we use the fact that continuous functions take connected sets to connected sets, so the image of these arcs must be connected.","If are consecutive vertices of a quadrilateral that lie on a circle prove the following: . I know that you can prove this without using the cross-ratio, but I would like to complete this proof using it. Here are some things I know: The cross-ratio is real if and only if lie on the same circle. The cross-ratio is invariant under linear transformations, i.e. . I am not sure if (2) will be of any use, but I definitely think (1) would be helpful. Using (1) I can only get as far as: , where . Any help is appreciated. Edit: After playing with the equation for a while this is what I concluded: Of course by simple verification, one indeed finds that . Then we have Now our result is proven if we can show that both of these quotients are real and positive. Well, the real part follows from the fact that a cross-ratio is real if and only if all four points lie on the same circle. Now for the positive part: The quotient on the left is the cross ratio where , , . Now this transformation, which is bijective, takes the circle that passes through these points to the real line. Therefore the arc from to is taken to , hence . A similar argument works to show that the other cross-ratio is also positive. Is this reasoning correct? I guess we use the fact that continuous functions take connected sets to connected sets, so the image of these arcs must be connected.","z_1,z_2,z_3,z_4 |z_1-z_3||z_2-z_4|=|z_1-z_2||z_3-z_4|+|z_1-z_4||z_2-z_3| (z_1,z_2,z_3,z_4) (z_1,z_2,z_3,z_4)=(Tz_1,Tz_2,Tz_3,Tz_4) |z_1-z_3||z_2-z_4|=C|z_1-z_4||z_2-z_3| C \geq 0 (z_1-z_3)(z_2-z_4)=(z_1-z_2)(z_3-z_4)+(z_1-z_4)(z_2-z_3) \frac{(z_1-z_3)(z_2-z_4)}{(z_1-z_4)(z_2-z_3)}=\frac{(z_1-z_2)(z_3-z_4)}{(z_1-z_4)(z_2-z_3)}+1. (z_1,z_2,z_3,z_4) z_2 \rightarrow 1 z_3 \rightarrow 0 z_4 \rightarrow \infty z_4 z_2 [-\infty,1] (z_1,z_2,z_3,z_4) > 1",['complex-analysis']
64,$\bar{\partial}$ operator of a function,operator of a function,\bar{\partial},"Let $\bar{\partial}$ be the Cauchy–Riemann operator that is for $z=x+i y$ , $$\bar{\partial} = \frac{1}{2i}\left( i\partial_x - \partial_y\right).$$ Now assume that $z=u+ve^{ia}$ , where $u$ and $v$ are real variables, $a\in (0,1)$ a real constant, and we want to calculate $\bar{\partial} f(z)$ in terms of $\partial_u$ and $\partial_v$ . My question is that the following claim correct? $$\bar{\partial} f(z)=\frac{1}{2 i \sin a} \left( e^{i a} \partial_u f-\partial_v f\right).$$ In my attempt, I used the chain rule. I found the same first term on r.h.s, but I got a different term for the second. Edit: Here is my calculation: Write $z=u+ve^{ia}=(u+v\cos a)+i (v\sin a)=u'+iv'$ . Then $$\bar{\partial} f(z)=\frac{1}{2 i} \left(i \partial_{u'} f-\partial_{v'} f\right).$$ By the chain rule, we have $\partial_{u'} f=\partial_{u} f + \frac{1}{\cos a} \partial_{v} f$ and $\partial_{v'} f=-\cot a \;\partial_{u} f + \frac{1}{\sin a} \partial_{v} f$ . When replacing we get $$\bar{\partial} f(z)=\frac{1}{2 i \sin a} \left( e^{i a} \partial_u f- (i \tan a -1)\partial_v f\right).$$","Let be the Cauchy–Riemann operator that is for , Now assume that , where and are real variables, a real constant, and we want to calculate in terms of and . My question is that the following claim correct? In my attempt, I used the chain rule. I found the same first term on r.h.s, but I got a different term for the second. Edit: Here is my calculation: Write . Then By the chain rule, we have and . When replacing we get","\bar{\partial} z=x+i y \bar{\partial} = \frac{1}{2i}\left( i\partial_x - \partial_y\right). z=u+ve^{ia} u v a\in (0,1) \bar{\partial} f(z) \partial_u \partial_v \bar{\partial} f(z)=\frac{1}{2 i \sin a} \left( e^{i a} \partial_u f-\partial_v f\right). z=u+ve^{ia}=(u+v\cos a)+i (v\sin a)=u'+iv' \bar{\partial} f(z)=\frac{1}{2 i} \left(i \partial_{u'} f-\partial_{v'} f\right). \partial_{u'} f=\partial_{u} f + \frac{1}{\cos a} \partial_{v} f \partial_{v'} f=-\cot a \;\partial_{u} f + \frac{1}{\sin a} \partial_{v} f \bar{\partial} f(z)=\frac{1}{2 i \sin a} \left( e^{i a} \partial_u f- (i \tan a -1)\partial_v f\right).","['complex-analysis', 'derivatives', 'coordinate-systems', 'chain-rule']"
65,Unique continuation at the boundary for harmonic functions in the plane,Unique continuation at the boundary for harmonic functions in the plane,,"Consider the set $U = (-1,1) \times \{ 0\} \subset \mathbb R^2$ and a continuous function $f : U \rightarrow \mathbb R$ . Then for any domain $\Omega \subset \mathbb R^2$ such that $U \subset \partial\Omega$ , there exist many functions $u$ harmonic in $\Omega$ such that $u|_U = f$ (as the Dirichlet problem is not completely determined). My question is what happens if we want to prescribe the value of $u$ on $U$ and also the value of $\partial_\nu u$ on $U$ ? Does there exist a (maximal) domain $\Omega$ and function $u$ harmonic in $\Omega$ with the desired behavior in $U$ ? I think, in general, there cannot exist a function $u$ and domain $\Omega$ satisfying this. For example, we could ask for $\partial_\nu u|_U=0$ and $u|_U(x)=\max(0,\vert x \vert -\frac 1 2)$ . Thus, by unique continuation at the boundary for harmonic functions, since $u$ and its gradient are both $0$ in an open set of the boundary then $u$ must be identically zero. But the prescribed value at $u$ is not identically zero, so there cannot exist any $u$ . I was wondering whether we can find some compatibility conditions on the prescribed values of $u$ and its normal derivative such that we can ensure the existence of some solution. Also, if there exists a solution, what can we say about the maximal domain where it is defined? Does anybody know about any reference on this kind of problem?","Consider the set and a continuous function . Then for any domain such that , there exist many functions harmonic in such that (as the Dirichlet problem is not completely determined). My question is what happens if we want to prescribe the value of on and also the value of on ? Does there exist a (maximal) domain and function harmonic in with the desired behavior in ? I think, in general, there cannot exist a function and domain satisfying this. For example, we could ask for and . Thus, by unique continuation at the boundary for harmonic functions, since and its gradient are both in an open set of the boundary then must be identically zero. But the prescribed value at is not identically zero, so there cannot exist any . I was wondering whether we can find some compatibility conditions on the prescribed values of and its normal derivative such that we can ensure the existence of some solution. Also, if there exists a solution, what can we say about the maximal domain where it is defined? Does anybody know about any reference on this kind of problem?","U = (-1,1) \times \{ 0\} \subset \mathbb R^2 f : U \rightarrow \mathbb R \Omega \subset \mathbb R^2 U \subset \partial\Omega u \Omega u|_U = f u U \partial_\nu u U \Omega u \Omega U u \Omega \partial_\nu u|_U=0 u|_U(x)=\max(0,\vert x \vert -\frac 1 2) u 0 u u u u","['complex-analysis', 'partial-differential-equations', 'reference-request', 'harmonic-analysis', 'harmonic-functions']"
66,Question about the stereographic projection. How to find the arc of great circle of the stereographic sphere S.,Question about the stereographic projection. How to find the arc of great circle of the stereographic sphere S.,,"Let $z$ , $w$ be complex numbers and let $P(z)$ , $P(w)$ be the corresponding points on the sphere $S$ , associated to $z$ , $w$ via the stereographic projection. Find the length of the arc of the great circle lying on $S$ and joining $P(z)$ to $P(w)$ . I started to consider the arc of the center should be the greatest length. But I'm so confused.","Let , be complex numbers and let , be the corresponding points on the sphere , associated to , via the stereographic projection. Find the length of the arc of the great circle lying on and joining to . I started to consider the arc of the center should be the greatest length. But I'm so confused.",z w P(z) P(w) S z w S P(z) P(w),"['complex-analysis', 'geometry']"
67,What's the bijection between scalar/inner products and (certain) almost complex structures (on $\mathbb R^2$)?,What's the bijection between scalar/inner products and (certain) almost complex structures (on )?,\mathbb R^2,"Asked on maths overflow here . What's the bijection between (equivalence classes of) scalar products (I guess 'scalar product' is the same as 'inner product') and a.c.s. (almost complex structure/s) on $\mathbb R^2$ ? From Example 1.2.12 of Daniel Huybrechts - Complex Geometry An Introduction. Assumptions and notation: I just pretend $V = \mathbb R^2$ literally instead of just an isomorphism. Let $\Phi(V)$ be the set of real symmetric positive definite $2 \times 2$ matrices. This set is in bijection with inner products on $V$ , I believe. We have according to this , $$\Phi(V) = \{\begin{bmatrix} h & f\\  f & g \end{bmatrix} \ | \ h+g, hg-f^2 > 0 \}_{h,f,g \in \mathbb R}$$ Let $\Gamma(V)$ be the (matrix representations of) a.c.s. on $V$ . We have, according to this , $$\{\begin{bmatrix} a & b\\  \frac{-1-a^2}{b} & -a \end{bmatrix}\}_{a,b \in \mathbb R, b \ne 0}=: \Gamma(V) \subseteq Auto_{\mathbb R}(V) \subseteq End_{\mathbb R}(V)$$ I understand that the ' rotation ' matrices in $V$ are $SO(2) := \{R(\theta) := \begin{bmatrix} \cos(\theta) & -\sin(\theta)\\  \sin(\theta) & \cos(\theta) \end{bmatrix}\}_{\theta \in \mathbb R}$ , though I'm not sure that Huybrechts has the same usage of the term 'rotation'. (I ask about this later.) Questions : A. For injectivity (except for the equivalence class): Given (equivalence class of) scalar product ( $[M]$ of) $M$ , choose unique $I$ that assigns $v$ to the one described. I'll call this map $\gamma: \Phi(V) \to \Gamma(V)$ , $\gamma(M)=I$ . (Later, $\tilde \gamma: \frac{\Phi(V)}{\tilde{}} \to \Gamma(V)$ , $\tilde \gamma([M])=I$ .) It's 'rotation by $\pi/2$ ' or something. In what way ? For $M=I_2$ (2x2 identity), then $I$ is indeed 'rotation by $\pi/2$ ', in the sense that it's $\begin{bmatrix} 0 & 1\\  -1 & 0 \end{bmatrix} \in SO(2) \cap \gamma(V)$ , which is the ' $R(\theta)$ ' , for $\theta = \pi/2$ . What exactly is the formula for $I=\begin{bmatrix} a & b\\  \frac{-1-a^2}{b} & -a \end{bmatrix} \in \Gamma(V)$ given $M = \begin{bmatrix} h & f\\  f & g \end{bmatrix} \in \Phi(V)$ ? I'm asking because 2a - I would exceed wolfram computation time 2b - I notice for a different $M$ I tried, $I$ isn't a 'rotation matrix' in the sense of $SO(2)$ . In fact, I believe the only 'rotation' matrices that are also a.c.s. are $\pm \begin{bmatrix} 0 & 1\\  -1 & 0 \end{bmatrix}$ , i.e. $SO(2) \cap \gamma(V) = \{\pm \begin{bmatrix} 0 & 1\\  -1 & 0 \end{bmatrix}\}$ . However, I think $I$ kind of 'rotates by $\pi/2$ ' in some other sense. 2c - I think $SO(2) \cap \gamma(V)$ isn't meant to be the image of $\gamma$ B. For surjectivity : I'll call whatever map we would have as $\phi: \Gamma(V) \to \Phi(V)$ , $\phi(I)=M$ Given an a.c.s. $I$ , what are some possible scalar products $M$ ? There's a comment that goes choosing the unique $M_v$ such that for some $v \in V \setminus 0$ , we have $\{v,I(v)\}$ as an orthonormal basis. I tried this out (long to type!), and the only thing missing was the positively oriented. I guess either $\{v,I(v)\}$ or $\{v,-I(v)\}$ is positively oriented though. So I'll let $M_v$ / $N_v \in \Phi(V)$ correspond to $\{v,I(v)\}$ / $\{v,-I(v)\}$ . Then by fixing $v$ (I ask about non-fixing of $v$ later), we have $\phi(I)=M_v$ or $N_v$ , whichever corresponds to positively oriented basis. I'll just call this $\phi(I)=L_v$ Is this right? Is $\phi$ supposedly an inverse (or right inverse or left inverse or whatever) to $\gamma$ (or $\tilde \gamma$ or whatever), in the sense that $\gamma(\phi(I)) = I$ for all (a.c.s.) $I \in \Gamma(V)$ ? This whole thing about the $v$ makes me think there's another equivalence relation going on here. Is there? This seems like we can have maps parametrised by the nonzero $v$ , namely $\phi_v: \Gamma(V) \to \Phi(V)$ . In this case, we might investigate if $\phi_v(I)=L_v=L_w=\phi_w(I)$ or at least if $[L_v]=[L_w]$ under the old equivalence relation of positive scalar $\lambda$ , i.e. $L_v = \lambda L_w$ . If this investigation turns out negative, then I think there's some problem like if 2 inner products are equivalent if they are from the same a.c.s. $I$ under $\phi_{\cdot}$ , but for possibly different $v$ and $w$ , then I think the equivalence class of $L_v$ under this new relation, which is $\{L_w\}_{w \ne 0}$ , might not be the same as the equivalence class of $L_v$ under the old relation, which is $\{\lambda L_v\}_{\lambda > 0}$ . Ideas: Perhaps there's some matrix thing here about how scalar products are in bijection with positive definite symmetric matrices and then almost complex structures are rotation matrices or something that are square roots of $-I_2$ . Like given pos def symmetric $B$ , there exists unique a.c.s. $J$ such that (something something). Perhaps this is related, but I'd rather not further analyse the question or read through the answer given that I've spent over a month on almost complex structures BEFORE we even put inner products on vector spaces . Please consider spoon-feeding me here.","Asked on maths overflow here . What's the bijection between (equivalence classes of) scalar products (I guess 'scalar product' is the same as 'inner product') and a.c.s. (almost complex structure/s) on ? From Example 1.2.12 of Daniel Huybrechts - Complex Geometry An Introduction. Assumptions and notation: I just pretend literally instead of just an isomorphism. Let be the set of real symmetric positive definite matrices. This set is in bijection with inner products on , I believe. We have according to this , Let be the (matrix representations of) a.c.s. on . We have, according to this , I understand that the ' rotation ' matrices in are , though I'm not sure that Huybrechts has the same usage of the term 'rotation'. (I ask about this later.) Questions : A. For injectivity (except for the equivalence class): Given (equivalence class of) scalar product ( of) , choose unique that assigns to the one described. I'll call this map , . (Later, , .) It's 'rotation by ' or something. In what way ? For (2x2 identity), then is indeed 'rotation by ', in the sense that it's , which is the ' ' , for . What exactly is the formula for given ? I'm asking because 2a - I would exceed wolfram computation time 2b - I notice for a different I tried, isn't a 'rotation matrix' in the sense of . In fact, I believe the only 'rotation' matrices that are also a.c.s. are , i.e. . However, I think kind of 'rotates by ' in some other sense. 2c - I think isn't meant to be the image of B. For surjectivity : I'll call whatever map we would have as , Given an a.c.s. , what are some possible scalar products ? There's a comment that goes choosing the unique such that for some , we have as an orthonormal basis. I tried this out (long to type!), and the only thing missing was the positively oriented. I guess either or is positively oriented though. So I'll let / correspond to / . Then by fixing (I ask about non-fixing of later), we have or , whichever corresponds to positively oriented basis. I'll just call this Is this right? Is supposedly an inverse (or right inverse or left inverse or whatever) to (or or whatever), in the sense that for all (a.c.s.) ? This whole thing about the makes me think there's another equivalence relation going on here. Is there? This seems like we can have maps parametrised by the nonzero , namely . In this case, we might investigate if or at least if under the old equivalence relation of positive scalar , i.e. . If this investigation turns out negative, then I think there's some problem like if 2 inner products are equivalent if they are from the same a.c.s. under , but for possibly different and , then I think the equivalence class of under this new relation, which is , might not be the same as the equivalence class of under the old relation, which is . Ideas: Perhaps there's some matrix thing here about how scalar products are in bijection with positive definite symmetric matrices and then almost complex structures are rotation matrices or something that are square roots of . Like given pos def symmetric , there exists unique a.c.s. such that (something something). Perhaps this is related, but I'd rather not further analyse the question or read through the answer given that I've spent over a month on almost complex structures BEFORE we even put inner products on vector spaces . Please consider spoon-feeding me here.","\mathbb R^2 V = \mathbb R^2 \Phi(V) 2 \times 2 V \Phi(V) = \{\begin{bmatrix}
h & f\\ 
f & g
\end{bmatrix} \ | \ h+g, hg-f^2 > 0 \}_{h,f,g \in \mathbb R} \Gamma(V) V \{\begin{bmatrix}
a & b\\ 
\frac{-1-a^2}{b} & -a
\end{bmatrix}\}_{a,b \in \mathbb R, b \ne 0}=: \Gamma(V) \subseteq Auto_{\mathbb R}(V) \subseteq End_{\mathbb R}(V) V SO(2) := \{R(\theta) := \begin{bmatrix}
\cos(\theta) & -\sin(\theta)\\ 
\sin(\theta) & \cos(\theta)
\end{bmatrix}\}_{\theta \in \mathbb R} [M] M I v \gamma: \Phi(V) \to \Gamma(V) \gamma(M)=I \tilde \gamma: \frac{\Phi(V)}{\tilde{}} \to \Gamma(V) \tilde \gamma([M])=I \pi/2 M=I_2 I \pi/2 \begin{bmatrix}
0 & 1\\ 
-1 & 0
\end{bmatrix} \in SO(2) \cap \gamma(V) R(\theta) \theta = \pi/2 I=\begin{bmatrix}
a & b\\ 
\frac{-1-a^2}{b} & -a
\end{bmatrix} \in \Gamma(V) M = \begin{bmatrix}
h & f\\ 
f & g
\end{bmatrix} \in \Phi(V) M I SO(2) \pm \begin{bmatrix}
0 & 1\\ 
-1 & 0
\end{bmatrix} SO(2) \cap \gamma(V) = \{\pm \begin{bmatrix}
0 & 1\\ 
-1 & 0
\end{bmatrix}\} I \pi/2 SO(2) \cap \gamma(V) \gamma \phi: \Gamma(V) \to \Phi(V) \phi(I)=M I M M_v v \in V \setminus 0 \{v,I(v)\} \{v,I(v)\} \{v,-I(v)\} M_v N_v \in \Phi(V) \{v,I(v)\} \{v,-I(v)\} v v \phi(I)=M_v N_v \phi(I)=L_v \phi \gamma \tilde \gamma \gamma(\phi(I)) = I I \in \Gamma(V) v v \phi_v: \Gamma(V) \to \Phi(V) \phi_v(I)=L_v=L_w=\phi_w(I) [L_v]=[L_w] \lambda L_v = \lambda L_w I \phi_{\cdot} v w L_v \{L_w\}_{w \ne 0} L_v \{\lambda L_v\}_{\lambda > 0} -I_2 B J","['linear-algebra', 'abstract-algebra', 'complex-analysis', 'complex-geometry', 'almost-complex']"
68,Incompatible charts on a complex manifold,Incompatible charts on a complex manifold,,"Consider the following subset of $\mathbb{R}^2$ . $$U=\{(x,y):x,y \in \mathbb{R}, x>0\}$$ We can make this into a 1-dimensional complex manifold by giving it a global chart on to the open set of complex numbers with positive real part. $$V=\{z \in \mathbb{C}:Re(z)>0\}$$ Consider the following two identifications, $\phi,\phi^{\prime}$ : $$1)\phi(x,y)=x+iy$$ $$2)\phi^{\prime}(x,y)=x+i\frac{y}{a}$$ where $a$ is any non-zero number. In general these two charts aren't holomorphically compatible, so they define two different complex manifolds. I have a hard time wrapping my head around this, what exactly is the difference between the two complex manifolds. I am thinking that they should be very similar since the charts are compatible when you consider them as smooth manifolds. Is there any good example where two incompatible charts give bizarrely different complex manifolds?","Consider the following subset of . We can make this into a 1-dimensional complex manifold by giving it a global chart on to the open set of complex numbers with positive real part. Consider the following two identifications, : where is any non-zero number. In general these two charts aren't holomorphically compatible, so they define two different complex manifolds. I have a hard time wrapping my head around this, what exactly is the difference between the two complex manifolds. I am thinking that they should be very similar since the charts are compatible when you consider them as smooth manifolds. Is there any good example where two incompatible charts give bizarrely different complex manifolds?","\mathbb{R}^2 U=\{(x,y):x,y \in \mathbb{R}, x>0\} V=\{z \in \mathbb{C}:Re(z)>0\} \phi,\phi^{\prime} 1)\phi(x,y)=x+iy 2)\phi^{\prime}(x,y)=x+i\frac{y}{a} a","['complex-analysis', 'differential-geometry', 'smooth-manifolds', 'complex-geometry', 'complex-manifolds']"
69,logarithm of complex number,logarithm of complex number,,Generally for logarithms If I have $2^4=16$ then it means $\log_2(16)=4$ (Here 2 is the base) So the value of logarithm basically tells us  how many times to multiply the base for the number. When we take ln it simply means base is e Now begins my question What is the logarithm of complex number? I thought since logarithm tells us how many times to multiply the base. If I would take logarithm I would get real numbers Because no matter the number if real numbers are multiplied the answer is real. But the book I have says its complex.It even has an derivation for it. Can someone explain logarithms of complex number relating real or at least share some resources? Thank you,Generally for logarithms If I have then it means (Here 2 is the base) So the value of logarithm basically tells us  how many times to multiply the base for the number. When we take ln it simply means base is e Now begins my question What is the logarithm of complex number? I thought since logarithm tells us how many times to multiply the base. If I would take logarithm I would get real numbers Because no matter the number if real numbers are multiplied the answer is real. But the book I have says its complex.It even has an derivation for it. Can someone explain logarithms of complex number relating real or at least share some resources? Thank you,2^4=16 \log_2(16)=4,"['complex-analysis', 'complex-numbers', 'logarithms']"
70,Deriving a Möbius transformation specified by three points,Deriving a Möbius transformation specified by three points,,"A Möbius transformation is given by $$f(z)=\frac{az+b}{cz+d}$$ with parameters $a$ , $b$ , $c$ , and $d$ . The Wikipedia article provides rules for finding these parameters based on three points $z_1$ , $z_2$ , and $z_3$ and their images $w_1$ , $w_2$ , and $w_3$ . It is my goal to understand how we can derive the equations which yield the parameters. Möbius transformations preserve the cross-ratio , so I assume we start with the cross-ratios of the original points and their images: $$(z,z_1;z_2,z_3)=(f(z),w_1;w_2,w_3)$$ which can be reformulated as $$\frac{(z-z_2)(z_1-z_3)}{(z_1-z_2)(z-z_3)}=\frac{(f(z)-w_2)(w_1-w_3)}{(w_1-w_2)(f(z)-w_3)}$$ I imagine the solution is obtained by reformulating this equation above somehow to solve for $f(z)$ . But how is this done? I could not find a proper tutorial for this online - most tutorials I find plug in specific points at this stage, but I would like to learn how the general approach is derived.","A Möbius transformation is given by with parameters , , , and . The Wikipedia article provides rules for finding these parameters based on three points , , and and their images , , and . It is my goal to understand how we can derive the equations which yield the parameters. Möbius transformations preserve the cross-ratio , so I assume we start with the cross-ratios of the original points and their images: which can be reformulated as I imagine the solution is obtained by reformulating this equation above somehow to solve for . But how is this done? I could not find a proper tutorial for this online - most tutorials I find plug in specific points at this stage, but I would like to learn how the general approach is derived.","f(z)=\frac{az+b}{cz+d} a b c d z_1 z_2 z_3 w_1 w_2 w_3 (z,z_1;z_2,z_3)=(f(z),w_1;w_2,w_3) \frac{(z-z_2)(z_1-z_3)}{(z_1-z_2)(z-z_3)}=\frac{(f(z)-w_2)(w_1-w_3)}{(w_1-w_2)(f(z)-w_3)} f(z)","['complex-analysis', 'mobius-transformation']"
71,How to find all abelian subgroups of Möbius transformations?,How to find all abelian subgroups of Möbius transformations?,,"How to find all abelian subgroups of Möbius transformations? It's a problem from conway's functions of one complex variable. The things I have known is that when two Möbius transformations commute,they will have the same fixed point. And I think this is related to abelian groups of matrix group since the composition of transformations is just like matrix multiplication. Any help will be thanked.","How to find all abelian subgroups of Möbius transformations? It's a problem from conway's functions of one complex variable. The things I have known is that when two Möbius transformations commute,they will have the same fixed point. And I think this is related to abelian groups of matrix group since the composition of transformations is just like matrix multiplication. Any help will be thanked.",,"['complex-analysis', 'group-theory', 'abelian-groups', 'conformal-geometry']"
72,My solution to this UW Madison qualifying exam problem,My solution to this UW Madison qualifying exam problem,,"Can someone please check my solution to this qualifying exam problem? Thanks!! For each of the following, either construct a holomorphic function $f$ in the unit disk $D=\{z\in\mathbb{C}|\,|z|<1\}$ with the stated properties, or show that no such function exists. 1.For each sequence $(a_n)\subset D$ such that $\lim_{n\rightarrow\infty}|a_n|=1$ , it follows that $\lim_{n\rightarrow\infty}|f(a_n)|+\infty$ . $|f'(0)|=2$ , $|f(z)|\leq 1$ for all $z$ such that $|z|=\frac{1}{2}$ , and $\Big|f\Big(\frac{3}{4}\Big)\Big|=\frac{5}{3}.$ 3. $|f(z)|\leq 1$ for all $z\in D$ , $f\Big(1-\frac{1}{n^2}\Big)=0$ for all $n\in\mathbb{Z}^+$ , and $f$ is not identically zero. My solution: 1.If such $f$ exists, first we claim the set of zeros of $D$ is finite. For otherwise, let $(z_n)$ be an infinite sequence consisting of distinct zeros of $f$ in $D$ such that $(z_n)$ converges to $z\in\mathbb{C}$ . If $z\in D$ , then $f(z)=0$ , which shows that $f$ is identically zero. If $z\notin D$ , then $|z_n|\rightarrow 1$ but $f(z_n)\rightarrow 0$ . Therefore, the set of zeros of $f$ is finite. Let $\xi_1,...,\xi_n$ be the zeros of $f$ repeated according to multiplicities, let $$g=\prod_{i=1}^n\frac{\xi_n-z}{1-\overline{\xi_n}z},$$ then $\frac{g}{f}$ is nonzero and holomorphic on $D$ and $\Big(\frac{g}{f}\Big)(a_n)\rightarrow 0$ if $(a_n)\subset D$ is a sequence such that $|a_n|\rightarrow 1$ . Let $h:\overline{D}\rightarrow\mathbb{C}$ be defined by $$h(z)=\frac{g}{f}(z)$$ if $|z|<1$ and $$h(z)=0$$ if $|z|=1$ , then $h$ is holomorphic on $D$ and continuous on $\overline{D}$ . However $h=0$ on $\partial D$ , so $h=0$ on $\overline{D}$ by the maximum modulus principle. Contradiction! 2.If such $f$ exists, define $g:D(0,2)\rightarrow\mathbb{C}$ by $g(z)=f\Big(\frac{z}{2}\Big)$ , then $|g'(0)|=1$ , $|g(z)|\leq 1$ for all $z$ such that $|z|=1$ and $\Big|g\Big(\frac{3}{2}\Big)\Big|=\frac{5}{3}$ . Cauchy integral formula says $$|g'(0)|=\bigg|\frac{1}{2\pi i}\int_{\partial D(0,1)}\frac{g(z)}{z}\,dz\bigg|,$$ which implies that $|g(z)|=1$ if $|z|=1$ . Furthermore, same as before, $g$ only has finitely many zeros in $D(0,1)$ . Let $(z_1,...,z_n)$ be the zeros of $g$ in $D(0,1)$ , repeated according to multiplicities. Let $$h=\prod_{i=1}^n\frac{z_i-z}{1-\overline{z_i}z},$$ if $n>0$ , and $h=1$ if $n=0$ ; then $\Big|\frac{g}{h}(z)\Big|=1$ and $\Big|\frac{h}{g}(z)\Big|=1$ if $|z|=1$ . Apply the maximum modulus principle to $\frac{f}{g}$ and $\frac{g}{h}$ , we see that there exists a $c\in\mathbb{C}$ with $|c|=1$ , and $g=ch$ . This shows that $n>0$ . Since $g$ has an analytic continuation to $D(0,2)$ , we have $|z_j|\leq\frac{1}{2}$ for each $j$ . Furthermore, we can compute that $$1=|g'(0)|=\bigg|\sum_{j=1}^n (1-|z_j|^2)z_1...z_{j-1}z_{j+1}...z_n \bigg|\leq n \frac{1}{2^{n-1}}.$$ However, the equality cannot be reached if $n>1$ , so $n=1$ . This shows that $1=1-|z_1|^2$ , hence $z_1=0$ . Therefore $g=cz$ for some constant $c\in\mathbb{C}$ with $|c|=1$ . Therefore, it is impossible that $\Big|f\Big(\frac{3}{4}\Big)\Big|=\frac{5}{3}.$ 3.Let $$f(z)=\prod_{n=1}^\infty \frac{\bigg(1-\frac{1}{n^2}\bigg)-z}{1-\bigg(1-\frac{1}{n^2}\bigg)z}.$$ Since $\sum_{n=1}^\infty\frac{1}{n^2}<\infty$ , the Blaschke product converges.","Can someone please check my solution to this qualifying exam problem? Thanks!! For each of the following, either construct a holomorphic function in the unit disk with the stated properties, or show that no such function exists. 1.For each sequence such that , it follows that . , for all such that , and 3. for all , for all , and is not identically zero. My solution: 1.If such exists, first we claim the set of zeros of is finite. For otherwise, let be an infinite sequence consisting of distinct zeros of in such that converges to . If , then , which shows that is identically zero. If , then but . Therefore, the set of zeros of is finite. Let be the zeros of repeated according to multiplicities, let then is nonzero and holomorphic on and if is a sequence such that . Let be defined by if and if , then is holomorphic on and continuous on . However on , so on by the maximum modulus principle. Contradiction! 2.If such exists, define by , then , for all such that and . Cauchy integral formula says which implies that if . Furthermore, same as before, only has finitely many zeros in . Let be the zeros of in , repeated according to multiplicities. Let if , and if ; then and if . Apply the maximum modulus principle to and , we see that there exists a with , and . This shows that . Since has an analytic continuation to , we have for each . Furthermore, we can compute that However, the equality cannot be reached if , so . This shows that , hence . Therefore for some constant with . Therefore, it is impossible that 3.Let Since , the Blaschke product converges.","f D=\{z\in\mathbb{C}|\,|z|<1\} (a_n)\subset D \lim_{n\rightarrow\infty}|a_n|=1 \lim_{n\rightarrow\infty}|f(a_n)|+\infty |f'(0)|=2 |f(z)|\leq 1 z |z|=\frac{1}{2} \Big|f\Big(\frac{3}{4}\Big)\Big|=\frac{5}{3}. |f(z)|\leq 1 z\in D f\Big(1-\frac{1}{n^2}\Big)=0 n\in\mathbb{Z}^+ f f D (z_n) f D (z_n) z\in\mathbb{C} z\in D f(z)=0 f z\notin D |z_n|\rightarrow 1 f(z_n)\rightarrow 0 f \xi_1,...,\xi_n f g=\prod_{i=1}^n\frac{\xi_n-z}{1-\overline{\xi_n}z}, \frac{g}{f} D \Big(\frac{g}{f}\Big)(a_n)\rightarrow 0 (a_n)\subset D |a_n|\rightarrow 1 h:\overline{D}\rightarrow\mathbb{C} h(z)=\frac{g}{f}(z) |z|<1 h(z)=0 |z|=1 h D \overline{D} h=0 \partial D h=0 \overline{D} f g:D(0,2)\rightarrow\mathbb{C} g(z)=f\Big(\frac{z}{2}\Big) |g'(0)|=1 |g(z)|\leq 1 z |z|=1 \Big|g\Big(\frac{3}{2}\Big)\Big|=\frac{5}{3} |g'(0)|=\bigg|\frac{1}{2\pi i}\int_{\partial D(0,1)}\frac{g(z)}{z}\,dz\bigg|, |g(z)|=1 |z|=1 g D(0,1) (z_1,...,z_n) g D(0,1) h=\prod_{i=1}^n\frac{z_i-z}{1-\overline{z_i}z}, n>0 h=1 n=0 \Big|\frac{g}{h}(z)\Big|=1 \Big|\frac{h}{g}(z)\Big|=1 |z|=1 \frac{f}{g} \frac{g}{h} c\in\mathbb{C} |c|=1 g=ch n>0 g D(0,2) |z_j|\leq\frac{1}{2} j 1=|g'(0)|=\bigg|\sum_{j=1}^n (1-|z_j|^2)z_1...z_{j-1}z_{j+1}...z_n \bigg|\leq n \frac{1}{2^{n-1}}. n>1 n=1 1=1-|z_1|^2 z_1=0 g=cz c\in\mathbb{C} |c|=1 \Big|f\Big(\frac{3}{4}\Big)\Big|=\frac{5}{3}. f(z)=\prod_{n=1}^\infty \frac{\bigg(1-\frac{1}{n^2}\bigg)-z}{1-\bigg(1-\frac{1}{n^2}\bigg)z}. \sum_{n=1}^\infty\frac{1}{n^2}<\infty",['complex-analysis']
73,Why is this assumption needed in Cauchy's theorem?,Why is this assumption needed in Cauchy's theorem?,,"I am studying complex analysis and Cauchy's theorem states: Suppose that a function $f$ is analytic in a simply connected domain $D$ and that $f'$ is continuous in $D$ . Then for every simple closed contour $C$ in $D$ , $\oint_C f(z)dz = 0$ Next after this theorem the book presents Cauchy-Goursat theorem which states that we don't actually need $f'$ to be continuous as assumption. My question: If it is given that function $f$ is analytic in a domain $D$ doesn't it mean that function $f$ is infinitely differentiable in that domain? Then we know that $f'$ is differentiable and so we know that $f'$ must be continuous. What I don't understand is why it is a big deal removing the assumption of continuous derivative if it is already implied by analyticity of the function. What am I missing?","I am studying complex analysis and Cauchy's theorem states: Suppose that a function is analytic in a simply connected domain and that is continuous in . Then for every simple closed contour in , Next after this theorem the book presents Cauchy-Goursat theorem which states that we don't actually need to be continuous as assumption. My question: If it is given that function is analytic in a domain doesn't it mean that function is infinitely differentiable in that domain? Then we know that is differentiable and so we know that must be continuous. What I don't understand is why it is a big deal removing the assumption of continuous derivative if it is already implied by analyticity of the function. What am I missing?",f D f' D C D \oint_C f(z)dz = 0 f' f D f f' f',"['complex-analysis', 'complex-integration']"
74,How to recognize the Laplace transform of a function with compact support?,How to recognize the Laplace transform of a function with compact support?,,"The question is pretty much self-contained in the title: is there some criterion for recognizing the Laplace transforms of compact-supported functions, other than the explicit computation of $\mathcal{L}^{-1}$ ? The question arises in a peculiar context: some integrals of oscillating functions can be converted into integrals of monotonic functions by exploiting the self-adjointness of the Laplace transform, for instance $$ \int_{0}^{+\infty}\frac{\sin(s)}{\sqrt{s}}\,ds = \int_{0}^{+\infty}\frac{dx}{\sqrt{\pi x}(1+x^2)} $$ and for numerical purposes the latter form is clearly more manageable than the former. On the other hand integrals of compact-supported functions are easier to handle through interpolation and quadrature, so it would be a nice thing to recognize in $\frac{1+e^{-\pi s}}{1+s^2}$ the Laplace transform of the chunk of the sine wave supported on $[0,\pi]$ , in order to compute $$ \int_{0}^{+\infty}\frac{1+e^{-\pi s}}{\sqrt{s}(1+s^2)}\,ds $$ by applying a quadrature scheme (as done here ) to $$ \int_{0}^{\pi}\frac{\sin(s)}{\sqrt{s}}\,ds. $$ The essence of the question is to understand which kinds of functions allow this trick.","The question is pretty much self-contained in the title: is there some criterion for recognizing the Laplace transforms of compact-supported functions, other than the explicit computation of ? The question arises in a peculiar context: some integrals of oscillating functions can be converted into integrals of monotonic functions by exploiting the self-adjointness of the Laplace transform, for instance and for numerical purposes the latter form is clearly more manageable than the former. On the other hand integrals of compact-supported functions are easier to handle through interpolation and quadrature, so it would be a nice thing to recognize in the Laplace transform of the chunk of the sine wave supported on , in order to compute by applying a quadrature scheme (as done here ) to The essence of the question is to understand which kinds of functions allow this trick.","\mathcal{L}^{-1}  \int_{0}^{+\infty}\frac{\sin(s)}{\sqrt{s}}\,ds = \int_{0}^{+\infty}\frac{dx}{\sqrt{\pi x}(1+x^2)}  \frac{1+e^{-\pi s}}{1+s^2} [0,\pi]  \int_{0}^{+\infty}\frac{1+e^{-\pi s}}{\sqrt{s}(1+s^2)}\,ds   \int_{0}^{\pi}\frac{\sin(s)}{\sqrt{s}}\,ds. ","['integration', 'complex-analysis', 'laplace-transform', 'approximation-theory']"
75,An analytic continuation of the square root along the unit circle,An analytic continuation of the square root along the unit circle,,"I want to find an analytic continuation of the square root along the unit circle but I am not sure whether I am doing it correctly. Let $C_0$ be the open disk of radius $1$ around $1$ , and let $f_0:C_0 \to \mathbb{C}$ be defined as $f_0(re^{i \varphi})=\sqrt{r} e^{i \frac{\varphi}{2}}$ , where $\varphi \in (-\pi,\pi]$ . Let $\gamma: [0,1] \to \mathbb{C}$ be the path given $\gamma(t)=e^{2 it \pi}$ . Find an analytic continuation of $f_0$ along $\gamma$ , i.e. a sequence $(C_k,f_k)_{k=0}^{n}$ of analytic continuations $f_k$ of $f_0$ such that the $C_k$ cover the image of $\gamma$ . Show that $f_n(1)=-f_0(1)$ . I tried to do this as follows. Since $f_0$ is holomorphic on $C_0$ we have the power series expansion $$ f_0(z)=\sum_{m=0}^{\infty} a^{(0)}_m (z-1)^m \tag{1} $$ where $a^{(0)}_m=\frac{1}{m!} \frac{\partial^m}{z^m} \sqrt{z} \big|_{z=1}$ . I wanted to analyticaly continue this series by defining $C_1=\{z \in \mathbb{C} \ | \ |z-e^{i \frac{\pi}{4}}|<1 \}$ and considering the function $$ f_1: C_1 \to \mathbb{C}, \ f_1(z)=\sum_{m=0}^{\infty} a^{(1)}_m (z-e^{i \frac{\pi}{4}})^m.  $$ where $a^{(1)}_m=\frac{1}{m!} \frac{\partial^m}{z^m} \sqrt{z} \big|_{z=e^{i \frac{\pi}{4}}}$ and $arg(z) \in (-\frac{3 \pi}{4}, \frac{5 \pi}{4}]$ . Let $z=re^{i\varphi} \in C_0 \cap C_1$ . With $z_1=e^{i \frac{\pi}{4}}$ I have $$ \sqrt{z}=\sqrt{z_1} \sqrt{\frac{z}{z_1}} =\sqrt{z_1} \sqrt{1+\frac{z}{z_1}-1} \underset{(1)}{=}\sqrt{z_1} \sum_{m=0}^{\infty} a^{(0)}_m (\frac{z}{z_1}-1)^m =\sum_{m=0}^{\infty} \frac{\sqrt{z_1}}{z^m_1} a^{(0)}_m (z-z_1)^m  $$ Since the series representation of $f_1$ is unique and since $f_0(z)=\sqrt{z}$ the functions $f_0,f_1$ agree on $C_0 \cap C_1$ . By iterating the steps above I can define disks $C_2, C_3,...C_8$ with centers $e^{i k\frac{\pi}{4}}$ and corresponding holomorphic functions $f_k$ , $k=2,...,8$ , each time requiring $arg(z) \in (-\pi+k \frac{\pi}{4},\pi+k \frac{\pi}{4}]$ for $z \in C_k$ . Upon considering the power series expansion centered at $e^{i \frac{8 \pi}{2}}=e^{i 2 \pi}$ I should get $f_8(e^{i 2 \pi})=\sqrt{1} e^{i \pi}=-1=-\sqrt{1} e^{i \cdot 0}=-f_0(1)$ . Am I on the right track here or is there an error?","I want to find an analytic continuation of the square root along the unit circle but I am not sure whether I am doing it correctly. Let be the open disk of radius around , and let be defined as , where . Let be the path given . Find an analytic continuation of along , i.e. a sequence of analytic continuations of such that the cover the image of . Show that . I tried to do this as follows. Since is holomorphic on we have the power series expansion where . I wanted to analyticaly continue this series by defining and considering the function where and . Let . With I have Since the series representation of is unique and since the functions agree on . By iterating the steps above I can define disks with centers and corresponding holomorphic functions , , each time requiring for . Upon considering the power series expansion centered at I should get . Am I on the right track here or is there an error?","C_0 1 1 f_0:C_0
\to \mathbb{C} f_0(re^{i \varphi})=\sqrt{r} e^{i
\frac{\varphi}{2}} \varphi \in (-\pi,\pi] \gamma:
[0,1] \to \mathbb{C} \gamma(t)=e^{2 it \pi} f_0 \gamma (C_k,f_k)_{k=0}^{n} f_k f_0 C_k \gamma f_n(1)=-f_0(1) f_0 C_0 
f_0(z)=\sum_{m=0}^{\infty} a^{(0)}_m (z-1)^m \tag{1}
 a^{(0)}_m=\frac{1}{m!} \frac{\partial^m}{z^m} \sqrt{z} \big|_{z=1} C_1=\{z \in \mathbb{C} \ | \ |z-e^{i \frac{\pi}{4}}|<1 \} 
f_1: C_1 \to \mathbb{C}, \ f_1(z)=\sum_{m=0}^{\infty} a^{(1)}_m (z-e^{i \frac{\pi}{4}})^m. 
 a^{(1)}_m=\frac{1}{m!} \frac{\partial^m}{z^m} \sqrt{z} \big|_{z=e^{i \frac{\pi}{4}}} arg(z) \in (-\frac{3 \pi}{4}, \frac{5 \pi}{4}] z=re^{i\varphi} \in C_0 \cap C_1 z_1=e^{i \frac{\pi}{4}} 
\sqrt{z}=\sqrt{z_1} \sqrt{\frac{z}{z_1}}
=\sqrt{z_1} \sqrt{1+\frac{z}{z_1}-1}
\underset{(1)}{=}\sqrt{z_1} \sum_{m=0}^{\infty} a^{(0)}_m (\frac{z}{z_1}-1)^m
=\sum_{m=0}^{\infty} \frac{\sqrt{z_1}}{z^m_1} a^{(0)}_m (z-z_1)^m 
 f_1 f_0(z)=\sqrt{z} f_0,f_1 C_0 \cap C_1 C_2, C_3,...C_8 e^{i k\frac{\pi}{4}} f_k k=2,...,8 arg(z) \in (-\pi+k \frac{\pi}{4},\pi+k \frac{\pi}{4}] z \in C_k e^{i \frac{8 \pi}{2}}=e^{i 2 \pi} f_8(e^{i 2 \pi})=\sqrt{1} e^{i \pi}=-1=-\sqrt{1} e^{i \cdot 0}=-f_0(1)","['complex-analysis', 'power-series', 'analyticity']"
76,Heat equation solution using Fourier transform,Heat equation solution using Fourier transform,,"I want to solve the equation $$x^2\frac{\partial^2 u(x,t)}{\partial x^2}+ax\frac{\partial u(x,t)}{\partial x}=\frac{\partial u(x,t)}{\partial t}$$ with $u(x,0)=f(x)$ for $0<x<\infty$ and $t>0$ . Substituting $U(y,t)=u(e^{-y},t)$ and $F(y)=f(e^{-y})$ we get $$ \frac{\partial^2 U(y,t)}{\partial y^2}+(1-a)\frac{\partial U(y,t)}{\partial y}=\frac{\partial U(y,t)}{\partial t},$$ with the solution $$\hat U(\xi,t)=\hat F(\xi)e^{(-4\pi\xi^2+(1-a)2\pi i\xi)t},$$ since $\hat U(\xi,0)=\hat F(\xi)$ . Taking the Fourier transform in the y variable (assuming that u satisfies the necessary conditions and $\hat {\frac{\partial U}{\partial t}}$ = $\frac{\partial}{\partial t}\hat U$ ), and using $$\hat F(\xi)=\int_{-\infty}^{\infty} F(x)e^{-2\pi xi\xi} dx=\int_{0}^{\infty} \frac{f(y)}{y}e^{2\pi i\log(y)\xi} dy,$$ we are supposed to get $$u(x,t)=\frac{1}{\sqrt{4\pi t}}\int_0^{\infty}e^{-(\log(v/x)+(1-a)t)^2/(4t)}f(v) \frac{dv}{v},$$ whereas I get $$\int_{-\infty}^{\infty} \int_0^{\infty} e^{(-4\pi^2\xi^2+(1-a)2\pi i\xi)t}e^{2\pi\log(y/x)i\xi} \frac{f(y)}{y} dyd\xi$$ and don't know how to simplify it.","I want to solve the equation with for and . Substituting and we get with the solution since . Taking the Fourier transform in the y variable (assuming that u satisfies the necessary conditions and = ), and using we are supposed to get whereas I get and don't know how to simplify it.","x^2\frac{\partial^2 u(x,t)}{\partial x^2}+ax\frac{\partial u(x,t)}{\partial x}=\frac{\partial u(x,t)}{\partial t} u(x,0)=f(x) 0<x<\infty t>0 U(y,t)=u(e^{-y},t) F(y)=f(e^{-y})  \frac{\partial^2 U(y,t)}{\partial y^2}+(1-a)\frac{\partial U(y,t)}{\partial y}=\frac{\partial U(y,t)}{\partial t}, \hat U(\xi,t)=\hat F(\xi)e^{(-4\pi\xi^2+(1-a)2\pi i\xi)t}, \hat U(\xi,0)=\hat F(\xi) \hat {\frac{\partial U}{\partial t}} \frac{\partial}{\partial t}\hat U \hat F(\xi)=\int_{-\infty}^{\infty} F(x)e^{-2\pi xi\xi} dx=\int_{0}^{\infty} \frac{f(y)}{y}e^{2\pi i\log(y)\xi} dy, u(x,t)=\frac{1}{\sqrt{4\pi t}}\int_0^{\infty}e^{-(\log(v/x)+(1-a)t)^2/(4t)}f(v) \frac{dv}{v}, \int_{-\infty}^{\infty} \int_0^{\infty} e^{(-4\pi^2\xi^2+(1-a)2\pi i\xi)t}e^{2\pi\log(y/x)i\xi} \frac{f(y)}{y} dyd\xi","['complex-analysis', 'fourier-analysis', 'fourier-transform', 'heat-equation']"
77,Holomorphic at infinity (definition),Holomorphic at infinity (definition),,"I struggle quite a bit with the usage of $\infty$ in complex analysis. In some cases, I can translate a definition involving infinity to equivalent statements using limits, or in the case of continuity I just make use of the topology defined on the Riemann sphere. However, what I don't understand is why the notion of differentiability is defined at the point infinity. A function $f(z)$ is said to be holomorphic at $\infty$ if $f(1/z)$ is holomorphic at $z=0$ . Same can be said about singularities. I just don't see why we would do this. For instance, when I'm asked to determine singularities, it often forget to check the point $\infty$ , because the definition feels so arbitrary: yea, let's check this random point which is actually a limit, and then somehow that tells us something? My source of confusion lies in the fact that I don't see why being holomorphic at $\infty$ tells us anything (well, I guess, besides that our function is bounded). I understand that being holomorphic around some complex number $a$ is useful, as we can then write our function as a power series around $a$ , of in the case of isolated singularities, we can work with Laurent expansion. But we don't have these things when it comes down to $\infty$ (except when we switch to $f(1/z)$ , but that's a whole different function now, no?) I hope someone understands my confusion and could shed some light on this issue.","I struggle quite a bit with the usage of in complex analysis. In some cases, I can translate a definition involving infinity to equivalent statements using limits, or in the case of continuity I just make use of the topology defined on the Riemann sphere. However, what I don't understand is why the notion of differentiability is defined at the point infinity. A function is said to be holomorphic at if is holomorphic at . Same can be said about singularities. I just don't see why we would do this. For instance, when I'm asked to determine singularities, it often forget to check the point , because the definition feels so arbitrary: yea, let's check this random point which is actually a limit, and then somehow that tells us something? My source of confusion lies in the fact that I don't see why being holomorphic at tells us anything (well, I guess, besides that our function is bounded). I understand that being holomorphic around some complex number is useful, as we can then write our function as a power series around , of in the case of isolated singularities, we can work with Laurent expansion. But we don't have these things when it comes down to (except when we switch to , but that's a whole different function now, no?) I hope someone understands my confusion and could shed some light on this issue.",\infty f(z) \infty f(1/z) z=0 \infty \infty a a \infty f(1/z),['complex-analysis']
78,Young inequality: Generalization on $\mathbb{T}$ space.,Young inequality: Generalization on  space.,\mathbb{T},"I'm interested in resolving this question that I find but on the $\mathbb{T}$ space.  ( Show that for any $f\in L^1$ and $g \in L^p(\mathbb R)$, $\lVert f ∗ g\rVert_p \leqslant \lVert f\rVert_1\lVert g\rVert_p$. ) Now my question is, is it possible? I mean I understand quite well the explanation that I found there but, if I take an $f\in L^1(\mathbb{T})$ and a $g\in L^p(\mathbb{T})$ is possible to prove, in the same way, that $f*g\in L^p$ for $1\le p\le \infty$ and that $||f*g||_{p}\le||f||_{1}||g||_{p}$ ? And if $p=\infty$ ? Is possible to prove that $||f*g||_{\infty}\le ||f||_{1}||g||_{\infty}$ if $f*g\in\mathbb{C(\mathbb{T})}$ ? Thanks you very much!","I'm interested in resolving this question that I find but on the space.  ( Show that for any $f\in L^1$ and $g \in L^p(\mathbb R)$, $\lVert f ∗ g\rVert_p \leqslant \lVert f\rVert_1\lVert g\rVert_p$. ) Now my question is, is it possible? I mean I understand quite well the explanation that I found there but, if I take an and a is possible to prove, in the same way, that for and that ? And if ? Is possible to prove that if ? Thanks you very much!",\mathbb{T} f\in L^1(\mathbb{T}) g\in L^p(\mathbb{T}) f*g\in L^p 1\le p\le \infty ||f*g||_{p}\le||f||_{1}||g||_{p} p=\infty ||f*g||_{\infty}\le ||f||_{1}||g||_{\infty} f*g\in\mathbb{C(\mathbb{T})},"['complex-analysis', 'measure-theory', 'lp-spaces', 'convolution', 'young-inequality']"
79,Rouché's Theorem with $h(z)=z^3+8z+23$,Rouché's Theorem with,h(z)=z^3+8z+23,"I've asked a similar question to this one, and got a nice answer, but now I am struggling with this one. Rouché's Theorem: If $f(z)$ and $(g(z)$ are analytic on and inside the contour $C$ and $|f(z)|>|g(z)|$ for all $z$ on C, then $f(z)$ and $f(z)+g(z)$ have the same number of zeros. I'm asked to show that $h(z)=z^3+8z+23$ has only one zero inside the contour $C_2(0)=\{z:|z-0|=3\}$ . Here is some visual evidence that this is true. Now, I've tried all kinds of choices for $f(z)$ and $g(z)$ , but none of them have worked. For example, if I let $f(z)=8z+23$ and $g(z)=z^3$ , then I can write $$|f(z)|=|8z+23|\ge||8z|-|23||=1$$ for all $z$ on the contour $C_3(0)$ . However, $$|g(z)|=|z^3|=|z|^3=27$$ for all $z$ on the contour $C_3(0)$ . Thus, I have not shown that $|f(z)|>|g(z)|$ for all $z$ on the contour $C_2(0)$ . Here's another image that shows $|f(z)|$ is not greater that $|g(z)|$ for all $z$ on the contour $C_3(0)$ . So, can someone give me an $f(z)$ and a $g(z)$ such that $|f(z)|>|g(z)|$ for all $z$ on the contour $C_3(0)$ ? And if so, can you share the strategy you used to find them? Thanks.","I've asked a similar question to this one, and got a nice answer, but now I am struggling with this one. Rouché's Theorem: If and are analytic on and inside the contour and for all on C, then and have the same number of zeros. I'm asked to show that has only one zero inside the contour . Here is some visual evidence that this is true. Now, I've tried all kinds of choices for and , but none of them have worked. For example, if I let and , then I can write for all on the contour . However, for all on the contour . Thus, I have not shown that for all on the contour . Here's another image that shows is not greater that for all on the contour . So, can someone give me an and a such that for all on the contour ? And if so, can you share the strategy you used to find them? Thanks.",f(z) (g(z) C |f(z)|>|g(z)| z f(z) f(z)+g(z) h(z)=z^3+8z+23 C_2(0)=\{z:|z-0|=3\} f(z) g(z) f(z)=8z+23 g(z)=z^3 |f(z)|=|8z+23|\ge||8z|-|23||=1 z C_3(0) |g(z)|=|z^3|=|z|^3=27 z C_3(0) |f(z)|>|g(z)| z C_2(0) |f(z)| |g(z)| z C_3(0) f(z) g(z) |f(z)|>|g(z)| z C_3(0),"['complex-analysis', 'rouches-theorem']"
80,An estimate on the coefficient of bounded schlicht functions,An estimate on the coefficient of bounded schlicht functions,,"Let $S=\{f:\mathbb{D}\to\mathbb{C}$ be a injective holomorphic map: $f(z)=z+a_2z^2+a_3z^3+a_4z^4+...\}$ . Suppose $f\in S$ and there exists $M>1$ such that $|f(z)|\leq M,\;\forall z\in\mathbb{D}$ .Show that $|a_2|\leq2\left(1-\frac1M\right)$ and $d(0,\partial f(\mathbb{D})\geq \frac{1}{2(1+\sqrt{1-\frac1M})}$ . My Idea:If we let $g(z)=\frac{1}{f(\frac1z)}=z-a_2+\frac{a_2^2-a_3}{z}+...\in \Sigma$ , where $\Sigma=\{g:\mathbb{C}\backslash\overline{\mathbb{D}}\to\mathbb{C}\big{|}g(z)=z+b_0+\frac{b_1}{z}+\frac{b_2}{z^2}+...\}$ .By the Gronwell Area Theorem, we have $\sum\limits_{n=1}^{+\infty}n|b_n|\leq 1$ . If we apply the theorem directly to $g(z)$ , we can not achieve a satisfactory result. However, if we let $g^*(z)=\sqrt{g(z^2)}=z+b_0^*+b_1^*\frac{1}{z}+...$ . This defines a monodromy branch. It is not hard to prove $g^*(z)$ is an injective holomorphic function, and with some computation we get $b_1^*=-\frac{a_2}{2}$ . Now, the Gronwell Area Theorem implies that $|a_2|\leq 2$ . This is a way to prove the de Branges's theorem for the coefficient of $a_2$ . I do think with some improvement of this idea and use the boundedness of $f$ , the above proposition will be solved. But I just can not figure out a way. The latter part of proposition, in my opinion, is  based on $|a_2|\leq2\left(1-\frac1M\right)$ . Actually, it's some what like the Koebe- $\frac14$ Theorem. Any solution or hint is highly appreciated!","Let be a injective holomorphic map: . Suppose and there exists such that .Show that and . My Idea:If we let , where .By the Gronwell Area Theorem, we have . If we apply the theorem directly to , we can not achieve a satisfactory result. However, if we let . This defines a monodromy branch. It is not hard to prove is an injective holomorphic function, and with some computation we get . Now, the Gronwell Area Theorem implies that . This is a way to prove the de Branges's theorem for the coefficient of . I do think with some improvement of this idea and use the boundedness of , the above proposition will be solved. But I just can not figure out a way. The latter part of proposition, in my opinion, is  based on . Actually, it's some what like the Koebe- Theorem. Any solution or hint is highly appreciated!","S=\{f:\mathbb{D}\to\mathbb{C} f(z)=z+a_2z^2+a_3z^3+a_4z^4+...\} f\in S M>1 |f(z)|\leq M,\;\forall z\in\mathbb{D} |a_2|\leq2\left(1-\frac1M\right) d(0,\partial f(\mathbb{D})\geq \frac{1}{2(1+\sqrt{1-\frac1M})} g(z)=\frac{1}{f(\frac1z)}=z-a_2+\frac{a_2^2-a_3}{z}+...\in \Sigma \Sigma=\{g:\mathbb{C}\backslash\overline{\mathbb{D}}\to\mathbb{C}\big{|}g(z)=z+b_0+\frac{b_1}{z}+\frac{b_2}{z^2}+...\} \sum\limits_{n=1}^{+\infty}n|b_n|\leq 1 g(z) g^*(z)=\sqrt{g(z^2)}=z+b_0^*+b_1^*\frac{1}{z}+... g^*(z) b_1^*=-\frac{a_2}{2} |a_2|\leq 2 a_2 f |a_2|\leq2\left(1-\frac1M\right) \frac14",['complex-analysis']
81,Use Cauchy integral to calculate,Use Cauchy integral to calculate,,"a) $\displaystyle\int_{\partial B_2(0)}\dfrac{e^z}{(z+1)(z-3)^2}dz$ Apply the Cauchy integral with $f(z)=\dfrac{e^z}{(z-3)^2}$ at $ z= -1  $ . Then: $$\int_{\partial B_2(0)}\dfrac{e^z}{(z+1)(z-3)^2}dz = 2\pi if(-1) =2 \pi \dfrac{e^{-1}}{(-4)^2}=2\pi i \dfrac{e^{-1}}{16}=\dfrac{1}{8}\pi i e^{-1} $$ b) $\displaystyle\int_{\partial B_2(0)}\dfrac{\sin z}{(z+i)}dz$ Apply the Cauchy integral with $f(z)=\sin z$ at $z=-i$ . $f(z)=\sin z$ is holomorphic inside $|z|=2$ . Then $$\int_{\partial B_2(0)}\dfrac{\sin z}{z+i}dz= 2\pi i f(-i)=2 \pi i \sin(-i)=2\pi i (-i \sinh(1)) = 2 \pi \sinh(1)$$ $\sin(-i)=-i\sinh(1)$ c) $\displaystyle\int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)}$ partial fractions: $\dfrac{1}{z^2+1} = \dfrac{1}{z^2-i^2}= \dfrac{1}{(z+i)(z-i)}=\dfrac{i/2}{z+i}-\dfrac{i/2}{z-i}$ Then $$\int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)} = \dfrac{i}{2} \int_{\partial B_2(-2i)}\dfrac{dz}{(z+i)}-\dfrac{i}{2} \int_{\partial B_2(-2i)}\dfrac{dz}{(z-i)}$$ $C= \partial B_2(-2i)$ Then $f(z)=1$ and $i$ y $-i$ its inside $\mathbb{C}$ , then would be: $$\int_C \dfrac{dz}{z-i} = 2 \pi i f(-i) = 2 \pi i $$ $$\int_C \dfrac{dz}{z-i} = 2 \pi i f(i) = 2 \pi i $$ Then $$\int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)}=0$$ d) $\displaystyle\int_{\partial B_1(0)}\dfrac{e^z}{(z-2)^3}dz $ Applying the Cauchy integral with $f(z)=e^z$ at $z=2$ . But 2 does not belong to the circle of radius 1, then the integral is 0. Am I correct? I would like to know if there is any mistake, I am just starting learning about this","a) Apply the Cauchy integral with at . Then: b) Apply the Cauchy integral with at . is holomorphic inside . Then c) partial fractions: Then Then and y its inside , then would be: Then d) Applying the Cauchy integral with at . But 2 does not belong to the circle of radius 1, then the integral is 0. Am I correct? I would like to know if there is any mistake, I am just starting learning about this",\displaystyle\int_{\partial B_2(0)}\dfrac{e^z}{(z+1)(z-3)^2}dz f(z)=\dfrac{e^z}{(z-3)^2}  z= -1   \int_{\partial B_2(0)}\dfrac{e^z}{(z+1)(z-3)^2}dz = 2\pi if(-1) =2 \pi \dfrac{e^{-1}}{(-4)^2}=2\pi i \dfrac{e^{-1}}{16}=\dfrac{1}{8}\pi i e^{-1}  \displaystyle\int_{\partial B_2(0)}\dfrac{\sin z}{(z+i)}dz f(z)=\sin z z=-i f(z)=\sin z |z|=2 \int_{\partial B_2(0)}\dfrac{\sin z}{z+i}dz= 2\pi i f(-i)=2 \pi i \sin(-i)=2\pi i (-i \sinh(1)) = 2 \pi \sinh(1) \sin(-i)=-i\sinh(1) \displaystyle\int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)} \dfrac{1}{z^2+1} = \dfrac{1}{z^2-i^2}= \dfrac{1}{(z+i)(z-i)}=\dfrac{i/2}{z+i}-\dfrac{i/2}{z-i} \int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)} = \dfrac{i}{2} \int_{\partial B_2(-2i)}\dfrac{dz}{(z+i)}-\dfrac{i}{2} \int_{\partial B_2(-2i)}\dfrac{dz}{(z-i)} C= \partial B_2(-2i) f(z)=1 i -i \mathbb{C} \int_C \dfrac{dz}{z-i} = 2 \pi i f(-i) = 2 \pi i  \int_C \dfrac{dz}{z-i} = 2 \pi i f(i) = 2 \pi i  \int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)}=0 \displaystyle\int_{\partial B_1(0)}\dfrac{e^z}{(z-2)^3}dz  f(z)=e^z z=2,"['complex-analysis', 'contour-integration', 'cauchy-integral-formula']"
82,Solve the following system of differential equations,Solve the following system of differential equations,,"I have to find $x(t)$ and $y(t)$ knowing that: $$\begin{cases} x'= y-\int_0^tx(u)\cos(t-u)\,du \\ y'=x+\cos(t) - 1 \end{cases}$$ Where $x=x(t)$ , $y=y(t)$ and $x(0)=1$ , $y(0)=0$ . I've noticed that the integral in the first equation is the convolution between $x(t)$ and $cos(t)$ , so I can apply the Laplace Transform to determine the originals $x$ and $y$ . $$\begin{cases} sF(s)-1=G(s)-F(s)\frac{s}{s^2+1} \\ sG(s)=F(s)+\frac{s}{s^2+1}-\frac{1}{s} \end{cases}$$ Is there another way to find the original functions $x$ and $y$ ? Thanks!","I have to find and knowing that: Where , and , . I've noticed that the integral in the first equation is the convolution between and , so I can apply the Laplace Transform to determine the originals and . Is there another way to find the original functions and ? Thanks!","x(t) y(t) \begin{cases} x'= y-\int_0^tx(u)\cos(t-u)\,du \\ y'=x+\cos(t) - 1 \end{cases} x=x(t) y=y(t) x(0)=1 y(0)=0 x(t) cos(t) x y \begin{cases} sF(s)-1=G(s)-F(s)\frac{s}{s^2+1} \\ sG(s)=F(s)+\frac{s}{s^2+1}-\frac{1}{s} \end{cases} x y","['calculus', 'complex-analysis']"
83,How can one distinguish the interior and exterior of a contour on a Riemann sphere?,How can one distinguish the interior and exterior of a contour on a Riemann sphere?,,"Maybe this is a stupid question but I have been confused by it for a long time... By residue theorem one sees the contour integral $$\oint \frac{dz}{z}=2\pi i\mathrm{Res}\left(\frac{1}{z},0\right)=2\pi i,$$ where the contour encircles the only singularity $z=0$ . If the singularity instead does not lie inside the contour then $$\oint \frac{dz}{z}=0.$$ But if we consider the Riemann sphere, any two contours can be deformed continuously to encircle the singularity. How can one distinguish these two cases? Or in another words, how can one distinguish the interior and exterior of a contour?","Maybe this is a stupid question but I have been confused by it for a long time... By residue theorem one sees the contour integral where the contour encircles the only singularity . If the singularity instead does not lie inside the contour then But if we consider the Riemann sphere, any two contours can be deformed continuously to encircle the singularity. How can one distinguish these two cases? Or in another words, how can one distinguish the interior and exterior of a contour?","\oint \frac{dz}{z}=2\pi i\mathrm{Res}\left(\frac{1}{z},0\right)=2\pi i, z=0 \oint \frac{dz}{z}=0.","['complex-analysis', 'contour-integration', 'riemann-surfaces']"
84,Real roots of partial sums of analytic function represented by infinite series :,Real roots of partial sums of analytic function represented by infinite series :,,"Consider the following polynomial: $$P_n(x)= \sum_{k=1}^n a_kx^k$$ We know (and can verify) that , for every $n$ , $P_n(x)$ has only real roots Now, define $$P(x)= \sum_{k=1}^\infty a_kx^k$$ We know that $P(x)$ analytic and entire . Question : Does this mean that $P(x)$ has only real roots ? If not please elaborate with examples Note: I'm not sure this is MSE or MO question (?) Edit: Converse of the above hypothesis isn't true","Consider the following polynomial: We know (and can verify) that , for every , has only real roots Now, define We know that analytic and entire . Question : Does this mean that has only real roots ? If not please elaborate with examples Note: I'm not sure this is MSE or MO question (?) Edit: Converse of the above hypothesis isn't true",P_n(x)= \sum_{k=1}^n a_kx^k n P_n(x) P(x)= \sum_{k=1}^\infty a_kx^k P(x) P(x),"['real-analysis', 'complex-analysis', 'polynomials', 'analytic-functions', 'entire-functions']"
85,Show that $|f(z)| \leq |z|$ on annulus,Show that  on annulus,|f(z)| \leq |z|,"Let $D = \{ z \in \mathbb{C} : 2 < |z| < 3 \}$ . Suppose that $f$ is holomorphic on $D$ and $f$ is continuous on $\overline{D}$ . Suppose that $\max \{ |f(z)| : |z| = 2\} \leq 2$ and $\max \{ |f(z)| : |z| = 3 \} \leq 3$ . Show that $\forall z \in D, |f(z)| \leq |z|$ . The solution I am thinking of I believe is wrong, but here is what it is. Assume $f$ is non-constant. By the maximum modulus principle $|f|$ assumes its maximum on $\partial D$ , Hence, $|f(z)| \leq 3, \forall z \in D$ . Define $g(z) = \frac{f(z)}{z}$ , which is analytic. Consider $r \in (2,3)$ . For $|z| = r$ , $|g(z)| \leq \frac{3}{r}$ . Since for $|z| = 2$ , $|g(z)| \leq 1$ , by the maximum modulus principle, $\forall |z| \in (2,r), |g(z)| \leq \frac{3}{r}$ . Let $r \rightarrow 3$ , then $\forall |z| \in (2,3), |g(z)| \leq 1 \implies |f(z)| \leq |z|$ . I essentially followed the same technique as in Schwarz's Lemma, but I am not sure if this is right.","Let . Suppose that is holomorphic on and is continuous on . Suppose that and . Show that . The solution I am thinking of I believe is wrong, but here is what it is. Assume is non-constant. By the maximum modulus principle assumes its maximum on , Hence, . Define , which is analytic. Consider . For , . Since for , , by the maximum modulus principle, . Let , then . I essentially followed the same technique as in Schwarz's Lemma, but I am not sure if this is right.","D = \{ z \in \mathbb{C} : 2 < |z| < 3 \} f D f \overline{D} \max \{ |f(z)| : |z| = 2\} \leq 2 \max \{ |f(z)| : |z| = 3 \} \leq 3 \forall z \in D, |f(z)| \leq |z| f |f| \partial D |f(z)| \leq 3, \forall z \in D g(z) = \frac{f(z)}{z} r \in (2,3) |z| = r |g(z)| \leq \frac{3}{r} |z| = 2 |g(z)| \leq 1 \forall |z| \in (2,r), |g(z)| \leq \frac{3}{r} r \rightarrow 3 \forall |z| \in (2,3), |g(z)| \leq 1 \implies |f(z)| \leq |z|",['complex-analysis']
86,Does $f{(x)} = a x^4 - x^3 + ax - a$ have exactly $1$ positive root given $a>0$?,Does  have exactly  positive root given ?,f{(x)} = a x^4 - x^3 + ax - a 1 a>0,"Let $$f{(x)} = a x^4 - x^3 + ax - a$$ where $a>0$ . We know that it contains $4$ roots. Also, by the Descartes' rule of signs , we know that There are either $1$ or $3$ positive roots. There is exactly $1$ negative root. I observed from plotting the function that it may contain exactly one positive root. Therefore, can we prove/disprove that the function always contain a pair of conjugate complex roots? Equivalently, can we prove/disprove that it has exactly $1$ positive root?","Let where . We know that it contains roots. Also, by the Descartes' rule of signs , we know that There are either or positive roots. There is exactly negative root. I observed from plotting the function that it may contain exactly one positive root. Therefore, can we prove/disprove that the function always contain a pair of conjugate complex roots? Equivalently, can we prove/disprove that it has exactly positive root?",f{(x)} = a x^4 - x^3 + ax - a a>0 4 1 3 1 1,"['complex-analysis', 'algebra-precalculus', 'polynomials', 'complex-numbers', 'roots']"
87,Finding number of zeros of $f(z) = z^{2019} + 8z + 7$ inside the unit disk.,Finding number of zeros of  inside the unit disk.,f(z) = z^{2019} + 8z + 7,"I'm trying to find the number of zeros of $f(z) = z^{2019} + 8z + 7$ inside the unit disk. I've tried to apply Rouche's Theorem, but no combination of terms seems to work. Also, the Argument Principle seems to fail because when I was computing the winding number of $f(\gamma)$ around the origin, I realized $f$ has a zero on the unit disk. Any help on this problem would be greatly appreciated!","I'm trying to find the number of zeros of inside the unit disk. I've tried to apply Rouche's Theorem, but no combination of terms seems to work. Also, the Argument Principle seems to fail because when I was computing the winding number of around the origin, I realized has a zero on the unit disk. Any help on this problem would be greatly appreciated!",f(z) = z^{2019} + 8z + 7 f(\gamma) f,['complex-analysis']
88,"Find $\lambda\in \mathbb{C}$ s.t. $\exists$ entire $f$, s.t. $f(z)=f(\lambda z),~\forall z\in\mathbb{C}$ [duplicate]","Find  s.t.  entire , s.t.  [duplicate]","\lambda\in \mathbb{C} \exists f f(z)=f(\lambda z),~\forall z\in\mathbb{C}","This question already has answers here : For which $a$ does the equation $f(z) = f(az) $ has a non constant solution $f$ (2 answers) Closed 4 years ago . I want to find all $\lambda \in \mathbb{C}$ such that there exists an entire non-constant function $f$ with $f(z)=f(\lambda z)$ $\forall z \in \mathbb{C}$ . My idea was the following: Since $f$ must be entire on the whole complex plane it can be represented as taylor series around $0$ . So I get then in total \begin{align*} \sum\limits_{n=0}^\infty \frac{f^{(n)}(0)}{n!}z^n=f(z)=f(\lambda z)=\sum\limits_{n=0}^\infty \frac{f^{(n)}(0)}{n!}\lambda^{n}z^n. \end{align*} Now I (hope I can) conclude that \begin{align*} f^{(n)}(0)=f^{(n)}(0)\lambda^{n}. \end{align*} Therefore, either $f^{(n)}(0)=0$ or $\lambda^{n}=1$ must hold. First I conclude that $|\lambda|=1$ must hold, therefore $\lambda=e^{i\theta}$ for some $\theta \in [0,2\pi].$ If now there exists $n\in \mathbb{N}$ such that $\theta=\frac{2\pi}{n}$ then I have $\lambda=e^{i\frac{2\pi}{n}}$ and therefore $\lambda^n=1$ . For such functions I might be able to define a entire function $f$ , such that all derivates in $0$ vanish except all $kn$ derivatives in $0$ . I just wonder if those are all and my thoughts were correct? Thanks in advance.","This question already has answers here : For which $a$ does the equation $f(z) = f(az) $ has a non constant solution $f$ (2 answers) Closed 4 years ago . I want to find all such that there exists an entire non-constant function with . My idea was the following: Since must be entire on the whole complex plane it can be represented as taylor series around . So I get then in total Now I (hope I can) conclude that Therefore, either or must hold. First I conclude that must hold, therefore for some If now there exists such that then I have and therefore . For such functions I might be able to define a entire function , such that all derivates in vanish except all derivatives in . I just wonder if those are all and my thoughts were correct? Thanks in advance.","\lambda \in \mathbb{C} f f(z)=f(\lambda z) \forall z \in \mathbb{C} f 0 \begin{align*}
\sum\limits_{n=0}^\infty \frac{f^{(n)}(0)}{n!}z^n=f(z)=f(\lambda z)=\sum\limits_{n=0}^\infty \frac{f^{(n)}(0)}{n!}\lambda^{n}z^n.
\end{align*} \begin{align*}
f^{(n)}(0)=f^{(n)}(0)\lambda^{n}.
\end{align*} f^{(n)}(0)=0 \lambda^{n}=1 |\lambda|=1 \lambda=e^{i\theta} \theta \in [0,2\pi]. n\in \mathbb{N} \theta=\frac{2\pi}{n} \lambda=e^{i\frac{2\pi}{n}} \lambda^n=1 f 0 kn 0","['complex-analysis', 'taylor-expansion', 'entire-functions']"
89,Solving $\int_0^{2\pi}e^{-it}\prod_{k=1}^n(e^{2kit}-1)dt$,Solving,\int_0^{2\pi}e^{-it}\prod_{k=1}^n(e^{2kit}-1)dt,"I'm trying to solve this integral: $$\int_0^{2\pi}e^{-it}\prod_{k=1}^n(e^{2kit}-1)dt$$ I tried working on the product as $\ln P$ on this form: $$\ln P=\sum_{k=1}^n\ln(e^{2kit}-1)$$ but it didn't lead to anywhere. If anyone has some indications or even an answer, I'd be really grateful.","I'm trying to solve this integral: I tried working on the product as on this form: but it didn't lead to anywhere. If anyone has some indications or even an answer, I'd be really grateful.",\int_0^{2\pi}e^{-it}\prod_{k=1}^n(e^{2kit}-1)dt \ln P \ln P=\sum_{k=1}^n\ln(e^{2kit}-1),"['calculus', 'integration', 'complex-analysis']"
90,Interesting identity for the value of an integral involving complex-valued square root.,Interesting identity for the value of an integral involving complex-valued square root.,,"I am looking for a simple proof of the following identity which holds by numerical tests: $$ \int_{-\infty}^\infty x\left(\sqrt{1+\frac{a}{1-x^2-2i\gamma x}}-\sqrt{1+\frac{a}{1-x^2+2i\gamma x} }\right)dx={i\pi a}\operatorname{sgn}\gamma, $$ where $a$ and $\gamma$ are real numbers. The result suggests using the residue theorem, but the residues at singularities of the integrand seem to be zero... Any hint or suggestion are appreciated.","I am looking for a simple proof of the following identity which holds by numerical tests: where and are real numbers. The result suggests using the residue theorem, but the residues at singularities of the integrand seem to be zero... Any hint or suggestion are appreciated.","
\int_{-\infty}^\infty x\left(\sqrt{1+\frac{a}{1-x^2-2i\gamma x}}-\sqrt{1+\frac{a}{1-x^2+2i\gamma x}
}\right)dx={i\pi a}\operatorname{sgn}\gamma,
 a \gamma","['complex-analysis', 'complex-integration']"
91,Integrating $\arcsin \left(\frac{1}{2r} \left(\left(\lambda^2+(r+t)^2\right)^{\frac{1}{2}}-\left(\lambda^2+(r-t)^2\right)^{\frac{1}{2}}\right)\right)$,Integrating,\arcsin \left(\frac{1}{2r} \left(\left(\lambda^2+(r+t)^2\right)^{\frac{1}{2}}-\left(\lambda^2+(r-t)^2\right)^{\frac{1}{2}}\right)\right),"In a mathematical physical problem, one has to deal with a non-trivial finite integral given by $$   f(r,R,\lambda) = \int_0^R \arcsin \left( \frac{1}{2r}  \left( \left( \lambda^2+(r+t)^2\right)^{\frac{1}{2}}  -\left( \lambda^2+(r-t)^2\right)^{\frac{1}{2}} \right) \right) \, \mathrm{d}t \, , $$ where $0<r<R$ and $\lambda>0$ are parameters. In particular, for $\lambda=0$ , it can easily be shown that $$ f(r,R,0) = \int_0^r \arcsin \left( \frac{t}{r} \right) \, \mathrm{d}t + \frac{\pi}{2} \int_r^R \mathrm{d} t = -r+\frac{\pi R}{2} \, . $$ Is there a way to deal with the above integral for arbitrary values of the parameter $\lambda$ ? Your suggestions and inputs are most welcome. Thank you","In a mathematical physical problem, one has to deal with a non-trivial finite integral given by where and are parameters. In particular, for , it can easily be shown that Is there a way to deal with the above integral for arbitrary values of the parameter ? Your suggestions and inputs are most welcome. Thank you","  
f(r,R,\lambda) = \int_0^R \arcsin \left( \frac{1}{2r} 
\left( \left( \lambda^2+(r+t)^2\right)^{\frac{1}{2}} 
-\left( \lambda^2+(r-t)^2\right)^{\frac{1}{2}}
\right) \right) \, \mathrm{d}t \, ,
 0<r<R \lambda>0 \lambda=0 
f(r,R,0) = \int_0^r \arcsin \left( \frac{t}{r} \right) \, \mathrm{d}t
+ \frac{\pi}{2} \int_r^R \mathrm{d} t
= -r+\frac{\pi R}{2} \, .
 \lambda","['real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
92,"Using Cauchy's integral formula to solve $\int_{|z| = 2} \frac{e^z}{z^2(z-1)}\,dz$",Using Cauchy's integral formula to solve,"\int_{|z| = 2} \frac{e^z}{z^2(z-1)}\,dz","On p. 116 in Complex Analysis by Gamelin, he has just introduced some examples of the application of the Cauchy Integral Formula (CIF). He then considers the integral $$\int_{|z| = 2} \frac{e^z}{z^2(z-1)}\,dz$$ which cannot immediately be solved using CIF. He introduces a way to solve this by cutting out two discs of radius $\epsilon$ centered at $0$ , and $1$ to obtain, $$\int_{|z| = 2} \frac{e^z}{z^2(z-1)}\,dz = \int_{|z| = \epsilon} \frac{e^z}{z^2(z-1)}\,dz + \int_{|z-1| = \epsilon} \frac{e^z}{z^2(z-1)}\,dz.$$ What makes it ok to do this cut out of discs? He do not mention his reasoning.My guess is that since we want the theorem to be applicable we need to take care of the singularities in some way and I do agree with him in applying the theorem to the two new integrals around the two epsilon discs. Is the ""formal"" argument for doing what he did that since the discs with radius $\epsilon$ union with the disc $\{|z| = 2\}$ 1 is a bounded domain with piecewise smooth boundary, and since the integrands $f(z) = \frac{e^z}{z-1}$ and $f(z) = \frac{e^z}{z^2}$ are analytic on $\{|z|=\epsilon\}$ and $\{|z-1|=\epsilon\}$ respectively, we can be guaranteed that it works? However, what is meant by it ? Do we let $\epsilon \to 0$ ? All help is appreciated, thank you!","On p. 116 in Complex Analysis by Gamelin, he has just introduced some examples of the application of the Cauchy Integral Formula (CIF). He then considers the integral which cannot immediately be solved using CIF. He introduces a way to solve this by cutting out two discs of radius centered at , and to obtain, What makes it ok to do this cut out of discs? He do not mention his reasoning.My guess is that since we want the theorem to be applicable we need to take care of the singularities in some way and I do agree with him in applying the theorem to the two new integrals around the two epsilon discs. Is the ""formal"" argument for doing what he did that since the discs with radius union with the disc 1 is a bounded domain with piecewise smooth boundary, and since the integrands and are analytic on and respectively, we can be guaranteed that it works? However, what is meant by it ? Do we let ? All help is appreciated, thank you!","\int_{|z| = 2} \frac{e^z}{z^2(z-1)}\,dz \epsilon 0 1 \int_{|z| = 2} \frac{e^z}{z^2(z-1)}\,dz = \int_{|z| = \epsilon} \frac{e^z}{z^2(z-1)}\,dz + \int_{|z-1| = \epsilon} \frac{e^z}{z^2(z-1)}\,dz. \epsilon \{|z| = 2\} f(z) = \frac{e^z}{z-1} f(z) = \frac{e^z}{z^2} \{|z|=\epsilon\} \{|z-1|=\epsilon\} \epsilon \to 0","['complex-analysis', 'contour-integration', 'cauchy-integral-formula']"
93,Compute the integral $\int_0^1 \frac{x^4}{\sqrt{x(1-x)}}dx$ using residue,Compute the integral  using residue,\int_0^1 \frac{x^4}{\sqrt{x(1-x)}}dx,"I'm trying to compute the following integral: $$ \int_{0}^{1}\frac{x^{4}}{\,\sqrt{\,{x\left(\,{1-x}\,\right)}\,}\,}\,\mathrm{d}x $$ So what I am try to do is using a dog bone contour, let me call D, so in the bottom part I have the negative sign, but the orientation is invert in consideration to the upper part, then because the only singularity that I have is on  x = $\infty$ I have that $$ \!\!\int_{D}f(z)\,dz = \!2\!\int_{0}^{1}\!\!\!f(x)dx + \left(\int_{\gamma_1}\!\! +\!\!\int_{\gamma_2}\right) \!\!f(\xi)d\xi = 2\pi i\, \mbox{Res}(f,\infty) $$ and then calculating the residue at infinity I have found this $3 \pi/8$ , but the answer is actually $35 \pi/128$ . Can someone help me figure what to do?","I'm trying to compute the following integral: So what I am try to do is using a dog bone contour, let me call D, so in the bottom part I have the negative sign, but the orientation is invert in consideration to the upper part, then because the only singularity that I have is on  x = I have that and then calculating the residue at infinity I have found this , but the answer is actually . Can someone help me figure what to do?","
\int_{0}^{1}\frac{x^{4}}{\,\sqrt{\,{x\left(\,{1-x}\,\right)}\,}\,}\,\mathrm{d}x
 \infty 
\!\!\int_{D}f(z)\,dz = \!2\!\int_{0}^{1}\!\!\!f(x)dx
+ \left(\int_{\gamma_1}\!\! +\!\!\int_{\gamma_2}\right)
\!\!f(\xi)d\xi = 2\pi i\, \mbox{Res}(f,\infty)
 3 \pi/8 35 \pi/128","['complex-analysis', 'contour-integration']"
94,$2\pi i\beta = \int\limits_{|z|=r} \frac{dz}{f(z)-z}$,,2\pi i\beta = \int\limits_{|z|=r} \frac{dz}{f(z)-z},"Let $\beta \in \mathbb{C^{*}}$ and $f(z) = z+ z^{k+1} - \beta z^{2k+1}$ . Show if if r is small enough then $$2\pi i\beta = \int\limits_{|z|=r} \frac{dz}{f(z)-z}$$ This is my input: I have to resolve it using residues aplications, so $$ \int\limits_{|z|=r} \frac{dz}{f(z)-z} =  \int\limits_{|z|=r} \frac{dz}{z+ z^{k+1} - \beta z^{2k+1}-z} = \int\limits_{|z|=r} \frac{dz}{ z^{k+1} - \beta z^{2k+1}} = \int\limits_{|z|=r} \frac{dz}{z^{k+1}(1-\beta z^{k})} $$ My question is, I have a lot of methods to resolve integrals by residues, in this case, I think I have to resolve it on unit disk for $z = e^{i \theta}$ I have to calculate his residue, but what are the singularities of $f(z)$ and what is the best method to resolve the exercise? Can you help me, please?","Let and . Show if if r is small enough then This is my input: I have to resolve it using residues aplications, so My question is, I have a lot of methods to resolve integrals by residues, in this case, I think I have to resolve it on unit disk for I have to calculate his residue, but what are the singularities of and what is the best method to resolve the exercise? Can you help me, please?",\beta \in \mathbb{C^{*}} f(z) = z+ z^{k+1} - \beta z^{2k+1} 2\pi i\beta = \int\limits_{|z|=r} \frac{dz}{f(z)-z}  \int\limits_{|z|=r} \frac{dz}{f(z)-z} =  \int\limits_{|z|=r} \frac{dz}{z+ z^{k+1} - \beta z^{2k+1}-z} = \int\limits_{|z|=r} \frac{dz}{ z^{k+1} - \beta z^{2k+1}} = \int\limits_{|z|=r} \frac{dz}{z^{k+1}(1-\beta z^{k})}  z = e^{i \theta} f(z),"['complex-analysis', 'contour-integration', 'residue-calculus', 'complex-integration']"
95,Evaluate $\int _0^{\infty }\frac{x^6}{\left(x^4+a^4\right)^2}dx$,Evaluate,\int _0^{\infty }\frac{x^6}{\left(x^4+a^4\right)^2}dx,"The function $$f\left(z\right)=\frac{z^6}{\left(z^4+a^4\right)^2}$$ Has the following poles of order 2: $$ z(k)=a \exp\left( \frac{\left(2k+1\right)}4 i\pi \right)$$ $f$ is even, therefore: $$\int _0^{+\infty }\frac{x^6}{\left(x^4+a^4\right)^2}dx =\frac{1}{2}\int _{-\infty }^{+\infty \:}\frac{x^6}{\left(x^4+a^4\right)^2}dx$$ $$\int _0^{+\infty }\frac{x^6}{\left(x^4+a^4\right)^2}dx=i\pi \sum _k\:Res\left(f,\:z\left(k\right)\right)$$ $$Res\left(f,\:z\left(k\right)\right)=\lim _{z\to z\left(k\right)}\left(\frac{1}{\left(2-1\right)!}\left(\frac{d}{dz}\right)^{2-1}\frac{z^6\left(z-z\left(k\right)\right)^2}{\left(z^4+a^4\right)^2}\right)$$ $$z^4+a^4=z^4-z_k^4\implies\dfrac{z^6(z-z_k)^2}{(z^4+a^4)^2}=\dfrac{z^6}{(z^3+z_k z^2+z_k^2 z+z_k^3)^2}$$ $$Res\left(f,\:z_k\right)=\lim _{z\to \:z_k}\left(\frac{d}{dz}\left(\frac{z^6}{\left(z^3+z_kz^2+z_k^2z+z_k^3\right)^2}\right)\right)$$ $$Res\left(f,\:z_k\right)=\frac{2z_kz^5\left(z^2+2z_kz+3z_k^2\right)}{\left(z^3+z_kz^2+z_k^2z+z_k^3\right)^3}=\frac{2z_k^6\cdot 6z_k^2}{\left(4z_k^3\right)^3}$$ $$Res\left(f,\:z_k\right)=\frac{12z_k^8}{64z_k^9}=\frac{3}{16z_k}$$ $$\int _0^{+\infty }\frac{x^6}{\left(x^4+a^4\right)^2}dx=\frac{3i\pi }{16a}\sum _{k=0}^n\:e^{-\frac{\left(2k+1\right)}{4}i\pi }$$ We consider only the residues within the upper half plane, that is to say those corresponding to $k=0$ and $k=1$ . $$\int _0^{+\infty \:}\frac{x^6}{\left(x^4+a^4\right)^2}dx=\frac{3i\pi \:}{16a}\left(e^{-\frac{i\pi }{4}\:\:}+e^{-\frac{3i\pi \:}{4}\:\:}\right)$$ $$\int _0^{+\infty \:}\frac{x^6}{\left(x^4+a^4\right)^2}dx=\frac{3i\pi \:}{16a}\left(\frac{\sqrt{2}}{2}\:-i\frac{\sqrt{2}}{2}-\frac{\sqrt{2}}{2}-i\frac{\sqrt{2}}{2}\right)$$ $$\int _0^{+\infty \:}\frac{x^6}{\left(x^4+a^4\right)^2}dx=\frac{3\pi \sqrt{2}\:}{16a}$$","The function Has the following poles of order 2: is even, therefore: We consider only the residues within the upper half plane, that is to say those corresponding to and .","f\left(z\right)=\frac{z^6}{\left(z^4+a^4\right)^2}  z(k)=a \exp\left( \frac{\left(2k+1\right)}4 i\pi \right) f \int _0^{+\infty }\frac{x^6}{\left(x^4+a^4\right)^2}dx =\frac{1}{2}\int _{-\infty }^{+\infty \:}\frac{x^6}{\left(x^4+a^4\right)^2}dx \int _0^{+\infty }\frac{x^6}{\left(x^4+a^4\right)^2}dx=i\pi \sum _k\:Res\left(f,\:z\left(k\right)\right) Res\left(f,\:z\left(k\right)\right)=\lim _{z\to z\left(k\right)}\left(\frac{1}{\left(2-1\right)!}\left(\frac{d}{dz}\right)^{2-1}\frac{z^6\left(z-z\left(k\right)\right)^2}{\left(z^4+a^4\right)^2}\right) z^4+a^4=z^4-z_k^4\implies\dfrac{z^6(z-z_k)^2}{(z^4+a^4)^2}=\dfrac{z^6}{(z^3+z_k z^2+z_k^2 z+z_k^3)^2} Res\left(f,\:z_k\right)=\lim _{z\to \:z_k}\left(\frac{d}{dz}\left(\frac{z^6}{\left(z^3+z_kz^2+z_k^2z+z_k^3\right)^2}\right)\right) Res\left(f,\:z_k\right)=\frac{2z_kz^5\left(z^2+2z_kz+3z_k^2\right)}{\left(z^3+z_kz^2+z_k^2z+z_k^3\right)^3}=\frac{2z_k^6\cdot 6z_k^2}{\left(4z_k^3\right)^3} Res\left(f,\:z_k\right)=\frac{12z_k^8}{64z_k^9}=\frac{3}{16z_k} \int _0^{+\infty }\frac{x^6}{\left(x^4+a^4\right)^2}dx=\frac{3i\pi }{16a}\sum _{k=0}^n\:e^{-\frac{\left(2k+1\right)}{4}i\pi } k=0 k=1 \int _0^{+\infty \:}\frac{x^6}{\left(x^4+a^4\right)^2}dx=\frac{3i\pi \:}{16a}\left(e^{-\frac{i\pi }{4}\:\:}+e^{-\frac{3i\pi \:}{4}\:\:}\right) \int _0^{+\infty \:}\frac{x^6}{\left(x^4+a^4\right)^2}dx=\frac{3i\pi \:}{16a}\left(\frac{\sqrt{2}}{2}\:-i\frac{\sqrt{2}}{2}-\frac{\sqrt{2}}{2}-i\frac{\sqrt{2}}{2}\right) \int _0^{+\infty \:}\frac{x^6}{\left(x^4+a^4\right)^2}dx=\frac{3\pi \sqrt{2}\:}{16a}","['integration', 'complex-analysis', 'rational-functions']"
96,Understanding the elementary factors in Weierstrass factorization theorem,Understanding the elementary factors in Weierstrass factorization theorem,,"An infinite product such as $\,\prod _{n}(z-c_{n})$ cannot converge. In order for it to converge, each factor $(z-c_{n})$ must approach 1 as $n\to \infty$ . So it stands to reason that one should seek a function that could be 0 at a prescribed point, yet remain near 1 when not at that point and furthermore introduce no more zeroes than those prescribed. This is where Weierstrass' elementary factors come in as they have these properties and serve the same purpose as the factors $(z-c_{n})$ above. So far I can follow, but then the elementary factors are presented as... $E_{n}(z)={\begin{cases}(1-z)&{\text{if }}n=0,\\(1-z)\exp \left({\frac  {z^{1}}{1}}+{\frac  {z^{2}}{2}}+\cdots +{\frac  {z^{n}}{n}}\right)&{\text{otherwise}}.\end{cases}}$ and it is stated that the utility of the elementary factors $E_{n}(z)$ lies in the following lemma... Lemma (15.8, Rudin) for $| z | ≤ 1$ , $n\in \mathbb {N}$ $\vert 1-E_{n}(z)\vert \leq \vert z\vert ^{{n+1}}$ I simply do not understand why the Lemma is important, or even what it is trying to prove, nor do I understand the value of using the $exp$ . There's no explanation surrounding it, so seeing it for the first time makes little sense to me. Can someone please re-explain the Lemma in terms that might help me understand it, and perhaps prove to me why it is necessary to prove that $E_{n}(z)$ converges?","An infinite product such as cannot converge. In order for it to converge, each factor must approach 1 as . So it stands to reason that one should seek a function that could be 0 at a prescribed point, yet remain near 1 when not at that point and furthermore introduce no more zeroes than those prescribed. This is where Weierstrass' elementary factors come in as they have these properties and serve the same purpose as the factors above. So far I can follow, but then the elementary factors are presented as... and it is stated that the utility of the elementary factors lies in the following lemma... Lemma (15.8, Rudin) for , I simply do not understand why the Lemma is important, or even what it is trying to prove, nor do I understand the value of using the . There's no explanation surrounding it, so seeing it for the first time makes little sense to me. Can someone please re-explain the Lemma in terms that might help me understand it, and perhaps prove to me why it is necessary to prove that converges?","\,\prod _{n}(z-c_{n}) (z-c_{n}) n\to \infty (z-c_{n}) E_{n}(z)={\begin{cases}(1-z)&{\text{if }}n=0,\\(1-z)\exp \left({\frac  {z^{1}}{1}}+{\frac  {z^{2}}{2}}+\cdots +{\frac  {z^{n}}{n}}\right)&{\text{otherwise}}.\end{cases}} E_{n}(z) | z | ≤ 1 n\in \mathbb {N} \vert 1-E_{n}(z)\vert \leq \vert z\vert ^{{n+1}} exp E_{n}(z)","['complex-analysis', 'convergence-divergence', 'infinite-product', 'weierstrass-factorization']"
97,Definition of a piecewise smooth path,Definition of a piecewise smooth path,,"In Apostol's ""Mathematical Analysis"" $($ page 435 $)$ , a piecewise smooth path in the complex plane, say $f$ , is defined as a path in the complex plane that has bounded derivative $f'$ which is continuous everywhere except possibly at a finite number of points, and at these exceptional points it is required that both right- and left-hand derivatives exist. Specifically, if $f:[a,b]\rightarrow C$ is a complex-valued function on the compact interval $[a,b]$ , then $f$ is said to be piecewise smooth if there exists a partition $\{x_0,x_1,...x_n\}$ of $[a,b]$ such that $f$ has a bounded and continuous derivative on each open subinterval $(x_{i-1},x_i)$ and has one-sided derivatives at the endpoints $x_{i-1}$ and $x_i$ . However, in the definition of a piecewise smooth path in most books on complex analysis, it is required that the restriction of $f$ to each compact subinterval $[x_{i-1},x_i]$ has a ""continuous"" derivative on $[x_{i-1},x_i]$ . The requirement in this definition is stronger than that of Apostol since at the endpoints $x_{i-1}$ and $x_i$ it requires that $\lim_{x\rightarrow x_{i-1}+}f'(x)=f'_{+}(x_{i-1})$ and $\lim_{x\rightarrow x_i-}f'(x)=f'_{-}(x_i)$ . I would like to know whether Apostol's definition is equivalent to this definition. Thanks for your feedback.","In Apostol's ""Mathematical Analysis"" page 435 , a piecewise smooth path in the complex plane, say , is defined as a path in the complex plane that has bounded derivative which is continuous everywhere except possibly at a finite number of points, and at these exceptional points it is required that both right- and left-hand derivatives exist. Specifically, if is a complex-valued function on the compact interval , then is said to be piecewise smooth if there exists a partition of such that has a bounded and continuous derivative on each open subinterval and has one-sided derivatives at the endpoints and . However, in the definition of a piecewise smooth path in most books on complex analysis, it is required that the restriction of to each compact subinterval has a ""continuous"" derivative on . The requirement in this definition is stronger than that of Apostol since at the endpoints and it requires that and . I would like to know whether Apostol's definition is equivalent to this definition. Thanks for your feedback.","( ) f f' f:[a,b]\rightarrow C [a,b] f \{x_0,x_1,...x_n\} [a,b] f (x_{i-1},x_i) x_{i-1} x_i f [x_{i-1},x_i] [x_{i-1},x_i] x_{i-1} x_i \lim_{x\rightarrow x_{i-1}+}f'(x)=f'_{+}(x_{i-1}) \lim_{x\rightarrow x_i-}f'(x)=f'_{-}(x_i)",['complex-analysis']
98,Calculate the Laurent Series for : $(8z+6+8i)/(2z^2-3z-4iz)$,Calculate the Laurent Series for :,(8z+6+8i)/(2z^2-3z-4iz),I have to find the Laurent-Series with $0<|z|<5/2$ and $z_0=0$ for: $$\frac{8z+6+8i}{2z^2-3z-4iz}$$ I already did this: $$ \frac{8z+6+8i}{2z^2-3z-4iz} = \frac{A}{2\cdot(z-0)}+ \frac{B}{2\cdot(z-1.5-2i)}\\ A=-2 \text{ and } B=6 \\ \frac{8z+6+8i}{2z^2-3z-4iz} = -1\cdot\frac{1}{z} + 3\cdot\frac{1}{z-1.5-2i} $$ I hope this is correct so far.  My book says the formula for a Laurent Series is : $$\sum_{k=1}^{\infty} a_k\cdot\frac{1}{[z-z_0]^k} + \sum_{k=0}^{\infty}b_k\cdot[z-z_0]^k=\sum_{k=-\infty}^{\infty} c_{k}\cdot[z-z_{0}]^{k}$$ But I have no Idea what i have to do now. I hope someone can help me. Thank you!,I have to find the Laurent-Series with and for: I already did this: I hope this is correct so far.  My book says the formula for a Laurent Series is : But I have no Idea what i have to do now. I hope someone can help me. Thank you!,"0<|z|<5/2 z_0=0 \frac{8z+6+8i}{2z^2-3z-4iz} 
\frac{8z+6+8i}{2z^2-3z-4iz} = \frac{A}{2\cdot(z-0)}+ \frac{B}{2\cdot(z-1.5-2i)}\\
A=-2 \text{ and } B=6 \\
\frac{8z+6+8i}{2z^2-3z-4iz} = -1\cdot\frac{1}{z} + 3\cdot\frac{1}{z-1.5-2i}
 \sum_{k=1}^{\infty} a_k\cdot\frac{1}{[z-z_0]^k} + \sum_{k=0}^{\infty}b_k\cdot[z-z_0]^k=\sum_{k=-\infty}^{\infty} c_{k}\cdot[z-z_{0}]^{k}","['complex-analysis', 'taylor-expansion', 'laurent-series']"
99,Residue of $f(z)=\frac{z}{\sin{\left(\frac{\pi}{z+1}\right)}}$ in all isolated singularities,Residue of  in all isolated singularities,f(z)=\frac{z}{\sin{\left(\frac{\pi}{z+1}\right)}},"I have this complex function: $$f(z)=\frac{z}{\sin\left(\frac{\pi}{z+1}\right)}$$ I'd like to compute residues in all isolate singularities. If I'm not mistaken $f$ has poles in $z=\frac{1}{k}-1$ and a non isolated singularity in $z=-1$ , because it is an accumulation point of poles. I tried to do something similar to this answer , but I don't seem to get a clean expression in terms of $\xi$ , where $\xi$ is $z-\frac{1}{k}+1$ . The best I can obtain is this: $$\frac{z}{\sin\left(\frac{k\xi+1-k}{k\xi+1}\right)}$$ Can you help me?","I have this complex function: I'd like to compute residues in all isolate singularities. If I'm not mistaken has poles in and a non isolated singularity in , because it is an accumulation point of poles. I tried to do something similar to this answer , but I don't seem to get a clean expression in terms of , where is . The best I can obtain is this: Can you help me?",f(z)=\frac{z}{\sin\left(\frac{\pi}{z+1}\right)} f z=\frac{1}{k}-1 z=-1 \xi \xi z-\frac{1}{k}+1 \frac{z}{\sin\left(\frac{k\xi+1-k}{k\xi+1}\right)},"['complex-analysis', 'residue-calculus', 'laurent-series']"
