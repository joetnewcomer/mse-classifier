,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is there an algebraic extension $K / \Bbb Q$ such that $\text{Aut}_{\Bbb Q}(K) \cong \Bbb Z$?,Is there an algebraic extension  such that ?,K / \Bbb Q \text{Aut}_{\Bbb Q}(K) \cong \Bbb Z,"Is there an algebraic field extension $K / \Bbb Q$ such that $\text{Aut}_{\Bbb Q}(K) \cong \Bbb Z$? Here I mean the field automorphisms (which are necessarily $\Bbb Q$-algebras automorphisms) of course. According to this answer , one can find some extension of $\Bbb Q$ whose automorphism group is $\Bbb Z$. But I've not seen that one can expect this extension to be algebraic. At least such an extension can't be normal, otherwise $\Bbb Z$ would be endowed with a topology turning it into a profinite group, which can't be countably infinite. (So typically, if we replace $\Bbb Q$ by $\Bbb F_p$, then the answer to the above question is no , because any algebraic extension of a finite field is Galois). Thank you!","Is there an algebraic field extension $K / \Bbb Q$ such that $\text{Aut}_{\Bbb Q}(K) \cong \Bbb Z$? Here I mean the field automorphisms (which are necessarily $\Bbb Q$-algebras automorphisms) of course. According to this answer , one can find some extension of $\Bbb Q$ whose automorphism group is $\Bbb Z$. But I've not seen that one can expect this extension to be algebraic. At least such an extension can't be normal, otherwise $\Bbb Z$ would be endowed with a topology turning it into a profinite group, which can't be countably infinite. (So typically, if we replace $\Bbb Q$ by $\Bbb F_p$, then the answer to the above question is no , because any algebraic extension of a finite field is Galois). Thank you!",,"['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
1,Finite fields containing cube root of unity and square root of $-3$,Finite fields containing cube root of unity and square root of,-3,"Let $q=p^n$. For what $q$ does $\mathbb F_q$ contain a primitive cube root of unity? Deduce for which $q$ the polynomial $x^2+x+1$ splits into linear factors in $\mathbb F_q[x]$. Use the quadratic formula when appropriate to find these factors. Over which fields is it inappropriate to use the quadratic formula? Which finite fields contain a square root of $-3$ and which do not? If $x$ is a primitive cube root of unity in $\mathbb F_q$, then $x\ne 1, x^2\ne 1$ but $x^3=1$ in $\mathbb F_q$. Equivalently, $x^3-1=0$ or $(x-1)(x^2+x+1)=0$. Since $x$ is primitive and $\mathbb F_q$ has no zero divisors, $x^2+x+1=0$. Now I believe the fields containing a primitive cube root of 1 are the fields over which this polynomial splits. So are the first two parts asking the same question? Anyway, I don't know how to describe the fields having either property. For the quadratic formula part, $x=\frac{-1\pm \sqrt{-3}}{2}$; this is valid provided $2\ne 0$ i.e. provided the characteristic isn't 2. So the polynomial splits iff $\sqrt{-3}$ lies in the field (provided the characteristic isn't 2). So is the third part asking the same as the first two? Let $F=\mathbb F_q$. $F$ contains a primitive cube root of unity $\iff$ $x^2+x+1$ has a root over $F$  that isn't equal to $1$ $\iff$$x^3-1$ has a root over $F$  that isn't equal to $1$ $\iff$ $F^\ast $ contains an element of order $3$ $\iff$ $3$ divides $p^n-1$ $\iff$  the conditions on $p,n$ from the answer of @lhf are met. So those conditions is the answer to 1. To get an answer for 2, we need to add to those $p$ and $n$ the values of $p,n$ for which $x^2+x+1$ has a root that is equal to $1$. Such values are $p=3, n$ is arbitrary. So the answer to 2 is the values from lhf's answer as well as $p=3, n$ arbitrary.","Let $q=p^n$. For what $q$ does $\mathbb F_q$ contain a primitive cube root of unity? Deduce for which $q$ the polynomial $x^2+x+1$ splits into linear factors in $\mathbb F_q[x]$. Use the quadratic formula when appropriate to find these factors. Over which fields is it inappropriate to use the quadratic formula? Which finite fields contain a square root of $-3$ and which do not? If $x$ is a primitive cube root of unity in $\mathbb F_q$, then $x\ne 1, x^2\ne 1$ but $x^3=1$ in $\mathbb F_q$. Equivalently, $x^3-1=0$ or $(x-1)(x^2+x+1)=0$. Since $x$ is primitive and $\mathbb F_q$ has no zero divisors, $x^2+x+1=0$. Now I believe the fields containing a primitive cube root of 1 are the fields over which this polynomial splits. So are the first two parts asking the same question? Anyway, I don't know how to describe the fields having either property. For the quadratic formula part, $x=\frac{-1\pm \sqrt{-3}}{2}$; this is valid provided $2\ne 0$ i.e. provided the characteristic isn't 2. So the polynomial splits iff $\sqrt{-3}$ lies in the field (provided the characteristic isn't 2). So is the third part asking the same as the first two? Let $F=\mathbb F_q$. $F$ contains a primitive cube root of unity $\iff$ $x^2+x+1$ has a root over $F$  that isn't equal to $1$ $\iff$$x^3-1$ has a root over $F$  that isn't equal to $1$ $\iff$ $F^\ast $ contains an element of order $3$ $\iff$ $3$ divides $p^n-1$ $\iff$  the conditions on $p,n$ from the answer of @lhf are met. So those conditions is the answer to 1. To get an answer for 2, we need to add to those $p$ and $n$ the values of $p,n$ for which $x^2+x+1$ has a root that is equal to $1$. Such values are $p=3, n$ is arbitrary. So the answer to 2 is the values from lhf's answer as well as $p=3, n$ arbitrary.",,"['abstract-algebra', 'ring-theory', 'field-theory', 'finite-fields']"
2,Question about proof in Neukirch's Algebraic Number Theory,Question about proof in Neukirch's Algebraic Number Theory,,"I was reading Proposition 2.2 in chapter I of Neukirch (page 6 in my edition), which states the following for an extension of rings $A\subseteq B$: (2.2) Proposition. Finitely many elements $b_1,\dots, b_n\in B$ are all integral over $A$ if and only if the ring $A[b_1,\dots,b_n]$ viewed as an $A$-module is finitely generated. Neukirch begins the proof by showing that if $b\in B$ is integral over $A$ then $A[b]$ is a finitely generated $A$-module. To do this, he notes that $b$ integral means there is some monic $f(x)\in A[x]$ of degree $n\geq 1$ such that $f(b)=0$. The claim is that $\{1,b,\dots,b^{n-1}\}$ form a generating set for $A[b]$. Neukirch proceeds to take a polynomial $g(x)\in A[x]$ (so that $g(b)$ is an arbritary element in $A[b])$ and states that ""we may then write $$ g(x)=q(x)f(x)+r(x) $$ for some $q(x),r(x)\in A[x]$ with $\deg(r(x))<n$"". Here is my problem : $A[x]$ is not a Euclidean domain in general. If $A$ is a field then sure, but if $A=\mathbb{Z}$ then $\mathbb{Z}[x]$ is not Euclidean so it would seem this step in the proof is not justified. What am I missing here?","I was reading Proposition 2.2 in chapter I of Neukirch (page 6 in my edition), which states the following for an extension of rings $A\subseteq B$: (2.2) Proposition. Finitely many elements $b_1,\dots, b_n\in B$ are all integral over $A$ if and only if the ring $A[b_1,\dots,b_n]$ viewed as an $A$-module is finitely generated. Neukirch begins the proof by showing that if $b\in B$ is integral over $A$ then $A[b]$ is a finitely generated $A$-module. To do this, he notes that $b$ integral means there is some monic $f(x)\in A[x]$ of degree $n\geq 1$ such that $f(b)=0$. The claim is that $\{1,b,\dots,b^{n-1}\}$ form a generating set for $A[b]$. Neukirch proceeds to take a polynomial $g(x)\in A[x]$ (so that $g(b)$ is an arbritary element in $A[b])$ and states that ""we may then write $$ g(x)=q(x)f(x)+r(x) $$ for some $q(x),r(x)\in A[x]$ with $\deg(r(x))<n$"". Here is my problem : $A[x]$ is not a Euclidean domain in general. If $A$ is a field then sure, but if $A=\mathbb{Z}$ then $\mathbb{Z}[x]$ is not Euclidean so it would seem this step in the proof is not justified. What am I missing here?",,"['abstract-algebra', 'number-theory', 'algebraic-number-theory', 'euclidean-domain']"
3,"Proving a ring $A$, generated by Noetherian subring and nilpotent element, is Noetherian again.","Proving a ring , generated by Noetherian subring and nilpotent element, is Noetherian again.",A,"I am studying some algebra during my spare time. In particular I am learning about Noetherian rings. A friend sent me the following excersise, and I am not able to solve it. Suppose that a ring $A$ is generated by a subring $R$ and a nilpotent element $n$ such that $R+nR=R+Rn$. (Dis)prove: $R$ left noetherian implies that $A$ is left noetherian. I believe that the statement above is correct. However I failed in trying to prove that  $A$ satisfies the ascending chain condition on ideals, or in proving that every ideal of $A$ is finitely generated. I also thought about using induction as the statement is trivial if $n=0$ but I do not see really how to progress from there on. Moreover I thought about writing $A$ as a ring isomorphic to a quotient of $R[X]$ and then using the Hilbert basis theorem, but I am not convinced. Would somebody be so kind to shed light on this problem? Thanks in advance!","I am studying some algebra during my spare time. In particular I am learning about Noetherian rings. A friend sent me the following excersise, and I am not able to solve it. Suppose that a ring $A$ is generated by a subring $R$ and a nilpotent element $n$ such that $R+nR=R+Rn$. (Dis)prove: $R$ left noetherian implies that $A$ is left noetherian. I believe that the statement above is correct. However I failed in trying to prove that  $A$ satisfies the ascending chain condition on ideals, or in proving that every ideal of $A$ is finitely generated. I also thought about using induction as the statement is trivial if $n=0$ but I do not see really how to progress from there on. Moreover I thought about writing $A$ as a ring isomorphic to a quotient of $R[X]$ and then using the Hilbert basis theorem, but I am not convinced. Would somebody be so kind to shed light on this problem? Thanks in advance!",,"['abstract-algebra', 'ring-theory', 'noetherian', 'nilpotence']"
4,"What is the name of ""group over 2 sets""?","What is the name of ""group over 2 sets""?",,"I remember finding a very useful algebraic structure that I loosely call ""group over 2 sets"" because I can't remember its real name or find it. I looked at all the algebraic structures on Wikipedia, but it wasn't mentioned there even though I remember it having its own page. Anyway, I'll define the algebraic structure more formally: It has 2 sets $P$ and $D$, where $D$ is a group over addition, 2 operations $+ : P \times D \rightarrow P$ and $- : P \times P \rightarrow D$ and few laws: $$ x + (y - x) = y \\ (y - x) + (x - y) = 0 \\ (x - x) = 0 \\ x + 0 = x $$ And possibly some more laws. I'm not sure I remember them all. I remember the structure being used for describing the relation between e.g. points and vectors (the set of points being P and vectors being D) or times = P and timedifferences = D. Does anyone know how this algebraic structure is called?","I remember finding a very useful algebraic structure that I loosely call ""group over 2 sets"" because I can't remember its real name or find it. I looked at all the algebraic structures on Wikipedia, but it wasn't mentioned there even though I remember it having its own page. Anyway, I'll define the algebraic structure more formally: It has 2 sets $P$ and $D$, where $D$ is a group over addition, 2 operations $+ : P \times D \rightarrow P$ and $- : P \times P \rightarrow D$ and few laws: $$ x + (y - x) = y \\ (y - x) + (x - y) = 0 \\ (x - x) = 0 \\ x + 0 = x $$ And possibly some more laws. I'm not sure I remember them all. I remember the structure being used for describing the relation between e.g. points and vectors (the set of points being P and vectors being D) or times = P and timedifferences = D. Does anyone know how this algebraic structure is called?",,"['abstract-algebra', 'group-theory']"
5,"Is the intersection of two subgroups, defined below, always trivial?","Is the intersection of two subgroups, defined below, always trivial?",,"Suppose, $G = \mathbb{Z} \ast H$, where $H$ is a torsion-free group. Suppose, $g \in G$ and $g \notin H$. Is $\langle\langle g \rangle \rangle \cap H$ always trivial? ($\ast$ stands for free product, and $\langle \langle \dots \rangle \rangle$ stands for normal closure) I failed to construct a counterexample, but I have no idea how to prove this statement too. Any help will be appreciated.","Suppose, $G = \mathbb{Z} \ast H$, where $H$ is a torsion-free group. Suppose, $g \in G$ and $g \notin H$. Is $\langle\langle g \rangle \rangle \cap H$ always trivial? ($\ast$ stands for free product, and $\langle \langle \dots \rangle \rangle$ stands for normal closure) I failed to construct a counterexample, but I have no idea how to prove this statement too. Any help will be appreciated.",,"['abstract-algebra', 'group-theory', 'normal-subgroups', 'free-product']"
6,Real roots of cubic polynomial,Real roots of cubic polynomial,,"Let's consider the polynomial $$f(X)=X^3+aX^2-(3+a)X+1\in\mathbb{Q}[X]$$ where $a\in\mathbb Z$. Simple observations show that it is irreducible and has 3 real roots. If $\alpha$ is one root we can even see that the splitting field is $\mathbb Q(\alpha)$ since a second root is $1/(1-\alpha)$. My question: Is there a way to write $1/(1-\alpha)$ as a linear combination of $\alpha$? And if so, does there exist a general method or trick to find it? So far I've tried to expand the fraction until I have an integer demoninator but without any success.","Let's consider the polynomial $$f(X)=X^3+aX^2-(3+a)X+1\in\mathbb{Q}[X]$$ where $a\in\mathbb Z$. Simple observations show that it is irreducible and has 3 real roots. If $\alpha$ is one root we can even see that the splitting field is $\mathbb Q(\alpha)$ since a second root is $1/(1-\alpha)$. My question: Is there a way to write $1/(1-\alpha)$ as a linear combination of $\alpha$? And if so, does there exist a general method or trick to find it? So far I've tried to expand the fraction until I have an integer demoninator but without any success.",,"['abstract-algebra', 'field-theory', 'irreducible-polynomials']"
7,Why is the condition that $\alpha$ is a complex root relevant in this exercise in Artin's Algebra?,Why is the condition that  is a complex root relevant in this exercise in Artin's Algebra?,\alpha,"In the second edition of Artin's algebra book, page 472, the following exercise is given: Let $\alpha$ be a complex root of $x^3-3x+4$.  Find the inverse of $\alpha^2+\alpha+1$ in the form $a\alpha^2+b\alpha+c$, with $a$, $b$, $c$ in $\mathbb{Q}$. The exercise itself is not difficult.  One can use the extended Euclidean Algorithm or brute force to find $a$, $b$ and $c$. I do not understand how the fact that $\alpha$ is a complex root of $x^3-3x+4$ relevant. As far as I can see, both the approaches do not make use of the fact that $\alpha$ is a complex root. There is possibly a simple explanation, but it eludes me. Can someone explain why this condition is there? Is it possible that the above condition gives a shorter way of solving the exercise?","In the second edition of Artin's algebra book, page 472, the following exercise is given: Let $\alpha$ be a complex root of $x^3-3x+4$.  Find the inverse of $\alpha^2+\alpha+1$ in the form $a\alpha^2+b\alpha+c$, with $a$, $b$, $c$ in $\mathbb{Q}$. The exercise itself is not difficult.  One can use the extended Euclidean Algorithm or brute force to find $a$, $b$ and $c$. I do not understand how the fact that $\alpha$ is a complex root of $x^3-3x+4$ relevant. As far as I can see, both the approaches do not make use of the fact that $\alpha$ is a complex root. There is possibly a simple explanation, but it eludes me. Can someone explain why this condition is there? Is it possible that the above condition gives a shorter way of solving the exercise?",,['abstract-algebra']
8,Zeros of power series with polynomially bounded integer coefficients,Zeros of power series with polynomially bounded integer coefficients,,"Consider the set $\mathbb{Z}_P[[X]]=\left\{ \sum_{n=0}^\infty a_n X^n \in \mathbb{Z}[[X]]\mid \exists k \in \mathbb{N_0}\colon (a_n)_{n\in\mathbb{N}_0} \in \mathcal{O}(n^k)\right\}$. It is easy to show that the elements of $\mathbb{Z}_P[[X]]$ have radius of convergence equal to 1, except for the polynomials. I am interested in the set $N_P =\left\{ \beta \in (-1,1) \mid \exists f(X) \in \mathbb{Z}_P[[X]] \colon f(\beta) = 0\right\}$. We have $\mathbb{A} \cap (-1,1) \subseteq N_P$, were $\mathbb{A}$ are the algebraic numbers. Question: What is known about the set $N_P$? Do we have $N_P = \mathbb{A} \cap (-1,1)$, or $N_P = (-1,1)$, or neither? Some things to note: If we consider uniformly bounded coefficents – that is, we replace $\mathcal{O}(n^k)$ above with $\mathcal{O}(1)$ – the functions we get are all rational functions ( see for example this paper by Borwein et al. ). The set of corresponding zeros is thus equal to $\mathbb{A} \cap (-1,1)$. Edit : I may have misunderstood the paper. I will follow up on this. Edit 2 : I definitely misunderstood the paper. The functions are not in general rational functions. The following theorem holds: For $\gamma \in (-1,1)$ there is a power series with integer coefficients $g(X) \in \mathbb{Z}[[X]]$ such that $g(\gamma) = 0$ ( see for example this math.stackexchange question ). The question is whether one can construct $g$ in such a way that the coefficients are polynomially bounded.","Consider the set $\mathbb{Z}_P[[X]]=\left\{ \sum_{n=0}^\infty a_n X^n \in \mathbb{Z}[[X]]\mid \exists k \in \mathbb{N_0}\colon (a_n)_{n\in\mathbb{N}_0} \in \mathcal{O}(n^k)\right\}$. It is easy to show that the elements of $\mathbb{Z}_P[[X]]$ have radius of convergence equal to 1, except for the polynomials. I am interested in the set $N_P =\left\{ \beta \in (-1,1) \mid \exists f(X) \in \mathbb{Z}_P[[X]] \colon f(\beta) = 0\right\}$. We have $\mathbb{A} \cap (-1,1) \subseteq N_P$, were $\mathbb{A}$ are the algebraic numbers. Question: What is known about the set $N_P$? Do we have $N_P = \mathbb{A} \cap (-1,1)$, or $N_P = (-1,1)$, or neither? Some things to note: If we consider uniformly bounded coefficents – that is, we replace $\mathcal{O}(n^k)$ above with $\mathcal{O}(1)$ – the functions we get are all rational functions ( see for example this paper by Borwein et al. ). The set of corresponding zeros is thus equal to $\mathbb{A} \cap (-1,1)$. Edit : I may have misunderstood the paper. I will follow up on this. Edit 2 : I definitely misunderstood the paper. The functions are not in general rational functions. The following theorem holds: For $\gamma \in (-1,1)$ there is a power series with integer coefficients $g(X) \in \mathbb{Z}[[X]]$ such that $g(\gamma) = 0$ ( see for example this math.stackexchange question ). The question is whether one can construct $g$ in such a way that the coefficients are polynomially bounded.",,"['abstract-algebra', 'power-series', 'formal-power-series']"
9,Definition of the Quantum plane and the Yang Baxter Equation,Definition of the Quantum plane and the Yang Baxter Equation,,"I was reading this on the quantum plane and the Yang Baxter equation. John Baez says that imposing $$ R(X\otimes X)= X\otimes X $$ $$ R(Y\otimes Y)= Y\otimes Y $$ $$ R(X\otimes Y)=q Y\otimes X $$ $$ R(Y\otimes X)=q X\otimes Y + (1-q^2) Y\otimes X $$ the resulting R-matrix satisfy the YBE, i.e. $$ \left(R\otimes id\right)\left(id\otimes R\right)\left(R\otimes id\right)=\left(id\otimes R\right)\left(R\otimes id\right)\left(id\otimes R\right) $$ I then wrote the matrix in the following base {$X\otimes X,Y\otimes Y,X\otimes Y,Y\otimes X$} obtaining $$ R=\left(\begin{array}{cccc} 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 0 & q\\ 0 & 0 & q & 1-q^{2} \end{array}\right) $$ But it doesn't seem to satisfy the YBE. What did I do wrong or what did I miss? And what is the relation between the definition of the quantum plane and the Yang Baxter Equation?","I was reading this on the quantum plane and the Yang Baxter equation. John Baez says that imposing $$ R(X\otimes X)= X\otimes X $$ $$ R(Y\otimes Y)= Y\otimes Y $$ $$ R(X\otimes Y)=q Y\otimes X $$ $$ R(Y\otimes X)=q X\otimes Y + (1-q^2) Y\otimes X $$ the resulting R-matrix satisfy the YBE, i.e. $$ \left(R\otimes id\right)\left(id\otimes R\right)\left(R\otimes id\right)=\left(id\otimes R\right)\left(R\otimes id\right)\left(id\otimes R\right) $$ I then wrote the matrix in the following base {$X\otimes X,Y\otimes Y,X\otimes Y,Y\otimes X$} obtaining $$ R=\left(\begin{array}{cccc} 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 0 & q\\ 0 & 0 & q & 1-q^{2} \end{array}\right) $$ But it doesn't seem to satisfy the YBE. What did I do wrong or what did I miss? And what is the relation between the definition of the quantum plane and the Yang Baxter Equation?",,"['abstract-algebra', 'matrix-equations', 'hopf-algebras', 'quantum-groups']"
10,"$ya^n - by = 1$ has one solution, then $xa-bx = 1$ also has one solution","has one solution, then  also has one solution",ya^n - by = 1 xa-bx = 1,"Let $(R, +, \cdot)$ be a ring and $a,b \in R, n \in \mathbb{N}^*$ such that $b^2 = b$ and the equation $ya^n - by = 1$ has one solution. Prove that the equation $xa - bx = 1$ also has one solution. Let $y$ be the solution of the equation $ya^n - by = 1$ . By multiplying the equation $ya^n - by = 1$ with $b$ we obtain: $$bya^n - by = b \iff bya^n - ya^n = b-1 \iff (b-1)ya^n = b-1.$$ If $(b-1)$ is inverible, then $ya^n = 1$ , so both $y$ and $a^n$ are invertible. But $ya^n - by = 1 \iff by=0 \implies b = 0$ . This means that the equation $xa - bx = 1$ has the solution $x = ya^{n-1}$ . I just don't know how to proceed if $(b-1)$ is not invertible.","Let be a ring and such that and the equation has one solution. Prove that the equation also has one solution. Let be the solution of the equation . By multiplying the equation with we obtain: If is inverible, then , so both and are invertible. But . This means that the equation has the solution . I just don't know how to proceed if is not invertible.","(R, +, \cdot) a,b \in R, n \in \mathbb{N}^* b^2 = b ya^n - by = 1 xa - bx = 1 y ya^n - by = 1 ya^n - by = 1 b bya^n - by = b \iff bya^n - ya^n = b-1 \iff (b-1)ya^n = b-1. (b-1) ya^n = 1 y a^n ya^n - by = 1 \iff by=0 \implies b = 0 xa - bx = 1 x = ya^{n-1} (b-1)","['abstract-algebra', 'ring-theory']"
11,Minimal ideal in commutative finite rings,Minimal ideal in commutative finite rings,,"Let $R$ be a commutative finite ring with identity, and let $I$ be a minimal ideal of $R$, that is, a non-zero ideal that there is no ideal strictly between $I$ and $0$. Now let $\{I_i\}_{i\in A}$ be a family of ideals of $R$ such that $I\subseteq \sum_{i\in A} I_i$. How can we show that there exists $j\in A$ such that $I\subseteq I_j$? If the statment is not true is there any condition under which it is true?","Let $R$ be a commutative finite ring with identity, and let $I$ be a minimal ideal of $R$, that is, a non-zero ideal that there is no ideal strictly between $I$ and $0$. Now let $\{I_i\}_{i\in A}$ be a family of ideals of $R$ such that $I\subseteq \sum_{i\in A} I_i$. How can we show that there exists $j\in A$ such that $I\subseteq I_j$? If the statment is not true is there any condition under which it is true?",,"['abstract-algebra', 'commutative-algebra', 'ideals', 'finite-rings']"
12,Number of isomorphism classes of $\Bbb Z[x] /(f)$ modules $M$ with $M \cong \Bbb Z^n$,Number of isomorphism classes of  modules  with,\Bbb Z[x] /(f) M M \cong \Bbb Z^n,"Let $f \in \Bbb Z[x]$ be a monic polynomial, $n$ is a positive integer. Consider all finitely generated $O=\Bbb Z[x]/(f(x))$ module $M$ such that $M \cong \Bbb Z^n$ as $\Bbb Z$-module, are there only finitely many isomorphism classes of such $M$ ? If $f$ is irreducible then $O$ is an order in a number field. Furthermore, assume $O$ is the algebraic integer ring in this number field then the answer is positive by finiteness of class number. I tried to deal with the general case, but there are several problems such as that a torsion free module may not be flat and that $\Bbb Z[x]/(x^2-1) \not \cong \Bbb Z[x]/(x-1) \oplus \Bbb Z[x]/(x+1)$). Is there any counterexample? If not, how do we compute the number of isomorphism classes using class number? Edit: Thanks for the answer below, there exists some counterexamples. Here is some of my ideas about a possible positive solution for some cases after posting this question. Firstly, as $\Bbb Z[x]$ is a $2$-dimensional regular ring, by standard argument (classification of f.g modules over Iwasawa algebra up to pseudo-isomorphism) there exists a morphism from $M$ to $\bigoplus_{i} \Bbb Z[x]/f_i^{n_i}$ which has finite kernel and cokernel and $f_i$ is irreducible. One example is the standard map from $\Bbb Z[x]/(x^2-1)$ to $\Bbb Z[x]/(x-1) \oplus \Bbb Z[x]/(x+1)$ with cokernel $\Bbb Z/2$. Maybe good controls for kernel and cokernel can help. A similiar idea is to regard $M$ as a coherent sheaf over $X=\text{Spec} \ O$, so when $O$ is an order , there exists an open subset $U$ of $X$ such that $X_U$ is normal with finite picard group and we have good understanding for $M|_U$. As there also exists a structure theorem for f.g modules on $PIR$ (Pricinpal Ideal Ring), this can also be done for general $O$. The remaining problem is to analysis $M$ on $X-U$ (a finite set).","Let $f \in \Bbb Z[x]$ be a monic polynomial, $n$ is a positive integer. Consider all finitely generated $O=\Bbb Z[x]/(f(x))$ module $M$ such that $M \cong \Bbb Z^n$ as $\Bbb Z$-module, are there only finitely many isomorphism classes of such $M$ ? If $f$ is irreducible then $O$ is an order in a number field. Furthermore, assume $O$ is the algebraic integer ring in this number field then the answer is positive by finiteness of class number. I tried to deal with the general case, but there are several problems such as that a torsion free module may not be flat and that $\Bbb Z[x]/(x^2-1) \not \cong \Bbb Z[x]/(x-1) \oplus \Bbb Z[x]/(x+1)$). Is there any counterexample? If not, how do we compute the number of isomorphism classes using class number? Edit: Thanks for the answer below, there exists some counterexamples. Here is some of my ideas about a possible positive solution for some cases after posting this question. Firstly, as $\Bbb Z[x]$ is a $2$-dimensional regular ring, by standard argument (classification of f.g modules over Iwasawa algebra up to pseudo-isomorphism) there exists a morphism from $M$ to $\bigoplus_{i} \Bbb Z[x]/f_i^{n_i}$ which has finite kernel and cokernel and $f_i$ is irreducible. One example is the standard map from $\Bbb Z[x]/(x^2-1)$ to $\Bbb Z[x]/(x-1) \oplus \Bbb Z[x]/(x+1)$ with cokernel $\Bbb Z/2$. Maybe good controls for kernel and cokernel can help. A similiar idea is to regard $M$ as a coherent sheaf over $X=\text{Spec} \ O$, so when $O$ is an order , there exists an open subset $U$ of $X$ such that $X_U$ is normal with finite picard group and we have good understanding for $M|_U$. As there also exists a structure theorem for f.g modules on $PIR$ (Pricinpal Ideal Ring), this can also be done for general $O$. The remaining problem is to analysis $M$ on $X-U$ (a finite set).",,"['abstract-algebra', 'number-theory']"
13,Find group with distinct Sylow $p$-subgroups that share a normalizer,Find group with distinct Sylow -subgroups that share a normalizer,p,"Here a question I thought of, but can't find an answer to Find two distinct Sylow $p$-subgroups (of a given $p$) $H_1$ and $H_2$ of $G$ such that $N_G(H_1) = N_G(H_2)$. I don't know if it's actually possible, so I should qualify the question with If no such pair exists, show why. Well, the easiest case would be if $H_1$ and $H_2$ were normal, however this would imply that $n_p = 1$. Hence, we'd only have one Sylow $p$-subgroup. My intuition says that such a pair does exist, however. Of course, it's merely intuition...","Here a question I thought of, but can't find an answer to Find two distinct Sylow $p$-subgroups (of a given $p$) $H_1$ and $H_2$ of $G$ such that $N_G(H_1) = N_G(H_2)$. I don't know if it's actually possible, so I should qualify the question with If no such pair exists, show why. Well, the easiest case would be if $H_1$ and $H_2$ were normal, however this would imply that $n_p = 1$. Hence, we'd only have one Sylow $p$-subgroup. My intuition says that such a pair does exist, however. Of course, it's merely intuition...",,"['abstract-algebra', 'group-theory', 'sylow-theory']"
14,Regular closure of $\mathbb{Q}(t)$,Regular closure of,\mathbb{Q}(t),"Let $K$ be the algebraic closure of $\mathbb{Q}(t)$, so $\overline{\mathbb{Q}(t)} = K$. How to describe the subfield L of K which is the regular closure of $\mathbb{Q}(t)$, so $L\cap{\overline{\mathbb{Q}}}=\mathbb{Q}$ and $L$ is maximal with respect to this property and being algebraic over $\mathbb{Q}(t)$?","Let $K$ be the algebraic closure of $\mathbb{Q}(t)$, so $\overline{\mathbb{Q}(t)} = K$. How to describe the subfield L of K which is the regular closure of $\mathbb{Q}(t)$, so $L\cap{\overline{\mathbb{Q}}}=\mathbb{Q}$ and $L$ is maximal with respect to this property and being algebraic over $\mathbb{Q}(t)$?",,"['abstract-algebra', 'field-theory']"
15,"$S$ a subring of $R$ has the property that if $x,y \in S$, $y \not= 0$ and $xz = y$ in $R$ then $z \in S$","a subring of  has the property that if ,  and  in  then","S R x,y \in S y \not= 0 xz = y R z \in S","I'm having trouble remembering if the property described in the title has an actual name. Phrased informally, if an element of $S \subset R$ partially factors in $S$, then that factorization is actually valid in $S$. For example, the integers embedded in the rationals don't possess this property, but the integers embedded in $\mathbb{Z}[\sqrt 5])$ (or many other algebraic extensions of the integers) do, even though many integers have factorizations lying entirely in  $\mathbb{Z}[\sqrt 5 ]) \setminus \mathbb{Z}$. To further illustrate, a slightly less trivial and rather artificial example of a ring with this property is the following: Let $R$ be a GCD domain, and consider the ring of polynomials over $R$, let's call it $R_{gcd}[X]$, in which multiplication is normal polynomial multiplication and addition is defined on the monomial basis as $aX^i + bX^i \rightarrow \gcd(a,b)X^i$.  Fix an $r \in R$ not a unit.  Gauss' lemma goes to show that the polynomials whose content has a factorization as a pure power of $r$ form a subring of $R_{gcd}[X]$, and moreover this subring  is easily seen to have the property described above.  Furthermore, if $R$ is not a UFD (say, the algebraic integers), then for some choices of $r$ there will exist factorizations of polynomials in the subring entirely in terms of polynomials not in the subring.","I'm having trouble remembering if the property described in the title has an actual name. Phrased informally, if an element of $S \subset R$ partially factors in $S$, then that factorization is actually valid in $S$. For example, the integers embedded in the rationals don't possess this property, but the integers embedded in $\mathbb{Z}[\sqrt 5])$ (or many other algebraic extensions of the integers) do, even though many integers have factorizations lying entirely in  $\mathbb{Z}[\sqrt 5 ]) \setminus \mathbb{Z}$. To further illustrate, a slightly less trivial and rather artificial example of a ring with this property is the following: Let $R$ be a GCD domain, and consider the ring of polynomials over $R$, let's call it $R_{gcd}[X]$, in which multiplication is normal polynomial multiplication and addition is defined on the monomial basis as $aX^i + bX^i \rightarrow \gcd(a,b)X^i$.  Fix an $r \in R$ not a unit.  Gauss' lemma goes to show that the polynomials whose content has a factorization as a pure power of $r$ form a subring of $R_{gcd}[X]$, and moreover this subring  is easily seen to have the property described above.  Furthermore, if $R$ is not a UFD (say, the algebraic integers), then for some choices of $r$ there will exist factorizations of polynomials in the subring entirely in terms of polynomials not in the subring.",,"['abstract-algebra', 'ring-theory', 'terminology']"
16,Can we learn things about coefficients of cyclotomic polynomials from Sylow Theory?,Can we learn things about coefficients of cyclotomic polynomials from Sylow Theory?,,"I am required to prove the following (which I believe was originally a result of Migotti): Let $p,q$ be distinct primes. Show that the $pq$th cyclotomic polynomial has coefficients all -1, 0 or 1. Show moreover that if n is a product of at most two distinct primes then the $n$th cyclotomic polynomial has coefficients -1,0 or 1. I have got an expression for the $pq$ case of $\frac{\Phi_q(t^p)}{\Phi_q(t)}$ where $\Phi_q(t) = 1+t+t^2+...+t^{q-1}$. I could slog through dividing that out, but it won't be pretty. I'm wondering if there is some deeper connection with Sylow theory that I could use, as the conditions look somewhat reminiscient of conditions on group order. Is it possible to formulate this usefully as a Sylow Theory problem?","I am required to prove the following (which I believe was originally a result of Migotti): Let $p,q$ be distinct primes. Show that the $pq$th cyclotomic polynomial has coefficients all -1, 0 or 1. Show moreover that if n is a product of at most two distinct primes then the $n$th cyclotomic polynomial has coefficients -1,0 or 1. I have got an expression for the $pq$ case of $\frac{\Phi_q(t^p)}{\Phi_q(t)}$ where $\Phi_q(t) = 1+t+t^2+...+t^{q-1}$. I could slog through dividing that out, but it won't be pretty. I'm wondering if there is some deeper connection with Sylow theory that I could use, as the conditions look somewhat reminiscient of conditions on group order. Is it possible to formulate this usefully as a Sylow Theory problem?",,"['abstract-algebra', 'galois-theory', 'sylow-theory', 'cyclotomic-polynomials', 'cyclotomic-fields']"
17,Can we find all primes $p$ that we can find a number $a \neq 1$ that has the same order in $\mathbb{Z}_p^{*}$ and $\mathbb{Z}_{p^2}^{*}$,Can we find all primes  that we can find a number  that has the same order in  and,p a \neq 1 \mathbb{Z}_p^{*} \mathbb{Z}_{p^2}^{*},"Supposed that there is a prime $p$, we may define $\mathbb{Z}_p^{*}$ as the multiplication group of integer modulo $p$. Each element in $\mathbb{Z}_p^{*}$, $\{1, 2, \ldots, p-1\}$, has finite order. Meanwhile, if we consider $\mathbb{Z}_{p^2}^{*}$, all $a \in \{1, 2,\ldots, p-1\}$ also have finite order in $\mathbb{Z}_{p^2}^{*}$, but the order of $a$ could be different from the order of respective $a$ in $\mathbb{Z}_p^{*}$. The problem is that, could we find all prime $p$ that there is a number $a \in \{2, \ldots, p-1\}$ that the order of $a$ in $\mathbb{Z}_p^{*}$ is the same as the order of $a$ in $\mathbb{Z}_{p^2}^{*}$? Edit : $a = 1$ definitely satisfies the condition for all primes $p$, so we should consider if there is another nontrivial one which satisfies the property.","Supposed that there is a prime $p$, we may define $\mathbb{Z}_p^{*}$ as the multiplication group of integer modulo $p$. Each element in $\mathbb{Z}_p^{*}$, $\{1, 2, \ldots, p-1\}$, has finite order. Meanwhile, if we consider $\mathbb{Z}_{p^2}^{*}$, all $a \in \{1, 2,\ldots, p-1\}$ also have finite order in $\mathbb{Z}_{p^2}^{*}$, but the order of $a$ could be different from the order of respective $a$ in $\mathbb{Z}_p^{*}$. The problem is that, could we find all prime $p$ that there is a number $a \in \{2, \ldots, p-1\}$ that the order of $a$ in $\mathbb{Z}_p^{*}$ is the same as the order of $a$ in $\mathbb{Z}_{p^2}^{*}$? Edit : $a = 1$ definitely satisfies the condition for all primes $p$, so we should consider if there is another nontrivial one which satisfies the property.",,['abstract-algebra']
18,Algebraic structure of Riemann Sphere,Algebraic structure of Riemann Sphere,,"Does Riemann sphere have any algebraic structure (Field, Ring, Algebra)? It appears that 'as a set', construction of Riemann Sphere is similar to the construction of the ring of polynomials over a field. At least the first step is same. Consider the set $ \mathbb{C} $. Insert a symbol $ \infty $ in that set. That is $ \hat{\mathbb{C}} = \mathbb{C} \cup \{\infty\} $ (Then define the topology suitably) On the other hand. Consider any field $ F $. Insert a symbol $ x $ in that set. (Then insert a whole lot more (all 'powers' of x, and formal sums of F-coefficient 'powers' of x etc.) Clearly, in Riemann Square, the extra symbol inserted in the set does not produce linearly independent elements $ \infty, \infty^2  ... $ But I am having a hard time understanding how the 'algebra' of the set $ \mathbb{C} $ is disrupted by this new symbol.","Does Riemann sphere have any algebraic structure (Field, Ring, Algebra)? It appears that 'as a set', construction of Riemann Sphere is similar to the construction of the ring of polynomials over a field. At least the first step is same. Consider the set $ \mathbb{C} $. Insert a symbol $ \infty $ in that set. That is $ \hat{\mathbb{C}} = \mathbb{C} \cup \{\infty\} $ (Then define the topology suitably) On the other hand. Consider any field $ F $. Insert a symbol $ x $ in that set. (Then insert a whole lot more (all 'powers' of x, and formal sums of F-coefficient 'powers' of x etc.) Clearly, in Riemann Square, the extra symbol inserted in the set does not produce linearly independent elements $ \infty, \infty^2  ... $ But I am having a hard time understanding how the 'algebra' of the set $ \mathbb{C} $ is disrupted by this new symbol.",,"['abstract-algebra', 'complex-analysis', 'ring-theory']"
19,Stabilisers of maximal ideals over an integral domain,Stabilisers of maximal ideals over an integral domain,,"Suppose that $R = \mathbb{C}[x_1, \dots, x_n]/I$ is a finitely generated commutative $\mathbb{C}$-algebra which is an integral domain, and suppose that $G \leqslant \text{GL}(n, \mathbb{C})$ is a finite group acting faithfully on $R$. Is it possible for every maximal ideal of $R$ to have a nontrivial stabiliser? Obviously, by the Nullstellensatz, every maximal ideal has the form $( x_1- \alpha_1, \dots, x_n - \alpha_n)/I$, where $I \subseteq ( x_1- \alpha_1, \dots, x_n - \alpha_n)$. I feel like there's some easy density argument that I'm missing, along the lines of the $\alpha_i$ having to satisfy a polynomial equation. I originally thought that $S_2$ acting naturally on $\mathbb{C}[x,y]/(x-y)$ gave a counterexample, but here $S_2$ is actually acting trivially! The integral domain hypothesis might not even be necessary, but I thought that I had a counterexample when $R$ wasn't a domain.","Suppose that $R = \mathbb{C}[x_1, \dots, x_n]/I$ is a finitely generated commutative $\mathbb{C}$-algebra which is an integral domain, and suppose that $G \leqslant \text{GL}(n, \mathbb{C})$ is a finite group acting faithfully on $R$. Is it possible for every maximal ideal of $R$ to have a nontrivial stabiliser? Obviously, by the Nullstellensatz, every maximal ideal has the form $( x_1- \alpha_1, \dots, x_n - \alpha_n)/I$, where $I \subseteq ( x_1- \alpha_1, \dots, x_n - \alpha_n)$. I feel like there's some easy density argument that I'm missing, along the lines of the $\alpha_i$ having to satisfy a polynomial equation. I originally thought that $S_2$ acting naturally on $\mathbb{C}[x,y]/(x-y)$ gave a counterexample, but here $S_2$ is actually acting trivially! The integral domain hypothesis might not even be necessary, but I thought that I had a counterexample when $R$ wasn't a domain.",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'finite-groups', 'invariant-theory']"
20,Classifying Groups of Order 28,Classifying Groups of Order 28,,"I am trying to classify groups of order 28. In the course of the problem, I am stuck in showing that three semidirect products are isomorphic to each other. In this problem, $G$ is a group of order 28, $H\in\mathrm{Syl}_{7}(G)$ is the unique Sylow 7-subgroup, and $K\in\mathrm{Syl}_{2}(G)$. I am working on the case where $K\cong \mathbb{Z}_{2}\times \mathbb{Z}_{2}$. We have the following groups to consider: $$K\cong\mathbb{Z}_{2}\times \mathbb{Z}_{2}=\left\langle a,b\:|\:a^{2}=b^{2}=(ab)^{2}=1\right\rangle$$ $$\mathrm{Aut}(H)\cong\mathbb{Z}_{6}=\left\langle x\:|\: x^{6}=1\right\rangle$$ Let $\psi_{j}: K\to \mathrm{Aut}(H)$, with $j\in\{1,2,3,4\}$, be defined as follows: $$\psi_{1}:\left\lbrace \begin{array}{c} a\mapsto 1\\ b\mapsto 1 \end{array}\right\rbrace \:\:\:\:\:\psi_{2}:\left\lbrace \begin{array}{c} a\mapsto x^{3}\\ b\mapsto 1 \end{array}\right\rbrace\:\:\:\:\:\psi_{3}:\left\lbrace \begin{array}{c} a\mapsto 1\\ b\mapsto x^{3} \end{array}\right\rbrace\:\:\:\:\:\psi_{4}:\left\lbrace \begin{array}{c} a\mapsto x^{3}\\ b\mapsto x^{3} \end{array}\right\rbrace$$ I know that because $\psi_{1}$ is trivial, we get $H\rtimes_{\psi_{1}}K\cong H\times K$. With all the previous work that I have done for this problem, this direct product determines the third isomorphism class for my isomorphism types. The problem statement tells me that there are four isomorphism types, so I only need one more. This means that we need $$H\rtimes_{\psi_{2}}K\cong H\rtimes_{\psi_{3}}K\cong H\rtimes_{\psi_{4}}K.$$ However, I do not know how to show that all these semidirect products are actually isomorphic. Thanks in advance for any help!","I am trying to classify groups of order 28. In the course of the problem, I am stuck in showing that three semidirect products are isomorphic to each other. In this problem, $G$ is a group of order 28, $H\in\mathrm{Syl}_{7}(G)$ is the unique Sylow 7-subgroup, and $K\in\mathrm{Syl}_{2}(G)$. I am working on the case where $K\cong \mathbb{Z}_{2}\times \mathbb{Z}_{2}$. We have the following groups to consider: $$K\cong\mathbb{Z}_{2}\times \mathbb{Z}_{2}=\left\langle a,b\:|\:a^{2}=b^{2}=(ab)^{2}=1\right\rangle$$ $$\mathrm{Aut}(H)\cong\mathbb{Z}_{6}=\left\langle x\:|\: x^{6}=1\right\rangle$$ Let $\psi_{j}: K\to \mathrm{Aut}(H)$, with $j\in\{1,2,3,4\}$, be defined as follows: $$\psi_{1}:\left\lbrace \begin{array}{c} a\mapsto 1\\ b\mapsto 1 \end{array}\right\rbrace \:\:\:\:\:\psi_{2}:\left\lbrace \begin{array}{c} a\mapsto x^{3}\\ b\mapsto 1 \end{array}\right\rbrace\:\:\:\:\:\psi_{3}:\left\lbrace \begin{array}{c} a\mapsto 1\\ b\mapsto x^{3} \end{array}\right\rbrace\:\:\:\:\:\psi_{4}:\left\lbrace \begin{array}{c} a\mapsto x^{3}\\ b\mapsto x^{3} \end{array}\right\rbrace$$ I know that because $\psi_{1}$ is trivial, we get $H\rtimes_{\psi_{1}}K\cong H\times K$. With all the previous work that I have done for this problem, this direct product determines the third isomorphism class for my isomorphism types. The problem statement tells me that there are four isomorphism types, so I only need one more. This means that we need $$H\rtimes_{\psi_{2}}K\cong H\rtimes_{\psi_{3}}K\cong H\rtimes_{\psi_{4}}K.$$ However, I do not know how to show that all these semidirect products are actually isomorphic. Thanks in advance for any help!",,"['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory', 'semidirect-product']"
21,Does the relation algebra have a sole sufficient operator?,Does the relation algebra have a sole sufficient operator?,,"In brief, does the relation algebra (defined here axiomatically) have a sole sufficient operator? Given a set $D$, define operators $^{-}$, $\wedge$,  $^{c}$, $\bullet $ on the set $\mathcal{P}(D^{2})$ as follows: $$ \begin{align} R^{-} &= \{ (x,y) \in D^{2} : (x,y) \notin R \}  \\ R \wedge S &= \{ (x,y) \in D^{2} : (x,y) \in R \wedge (x,y) \in S \} \\ R^{c} &= \{ (x,y) \in D^{2} : (y,x) \in R \} \\ R \bullet S &= \{ (x,y) \in D^{2} : \exists z \in D ( (x,z) \in S \wedge (z,y) \in R ) \}  \end{align} $$ Also define $$I = \{ (x,y) \in D^{2} : x = y \} $$ Is there a binary operator which (for any set $D$) can be combined with itself to produce $^{-}$, $\wedge$,  $^{c}$, $\bullet $ and $I$, analogous to how the Sheffer stroke can produce any Boolean operator? Alternatively, is there a proof that no such operator exists? If none exists, what is the smallest functionally complete set of operators? Thoughts so far I'm aware that the modal logics S4 and S5 have sole sufficient operators (I was a little surprised at this), but I'm not so familiar with the intuition behind their construction. Potentially a better understanding of them might help with constructing an SSO for the relation algebra. I don't know whether the modal logic K has a sole sufficient operator, but I suspect it doesn't. If there's a proof out there that K doesn't have a sole sufficient operator, it could be applied to the relation algebra. The relation algebra seems on a crude intuitive level to be a lot more complicated than K. When Post looked at the Boolean operators and how they relate to each other, he looked at properties of the operators preserved under composition (e.g. monotonic operators composed with themselves result in monotonic operators). A strategy for showing no sole sufficient operator exists for the relation algebra would be to find two mutually exclusive properties which are preserved under composition and possessed by at least one of $^{-}$, $\wedge$, $^{c}$, $\bullet$ and $I$.","In brief, does the relation algebra (defined here axiomatically) have a sole sufficient operator? Given a set $D$, define operators $^{-}$, $\wedge$,  $^{c}$, $\bullet $ on the set $\mathcal{P}(D^{2})$ as follows: $$ \begin{align} R^{-} &= \{ (x,y) \in D^{2} : (x,y) \notin R \}  \\ R \wedge S &= \{ (x,y) \in D^{2} : (x,y) \in R \wedge (x,y) \in S \} \\ R^{c} &= \{ (x,y) \in D^{2} : (y,x) \in R \} \\ R \bullet S &= \{ (x,y) \in D^{2} : \exists z \in D ( (x,z) \in S \wedge (z,y) \in R ) \}  \end{align} $$ Also define $$I = \{ (x,y) \in D^{2} : x = y \} $$ Is there a binary operator which (for any set $D$) can be combined with itself to produce $^{-}$, $\wedge$,  $^{c}$, $\bullet $ and $I$, analogous to how the Sheffer stroke can produce any Boolean operator? Alternatively, is there a proof that no such operator exists? If none exists, what is the smallest functionally complete set of operators? Thoughts so far I'm aware that the modal logics S4 and S5 have sole sufficient operators (I was a little surprised at this), but I'm not so familiar with the intuition behind their construction. Potentially a better understanding of them might help with constructing an SSO for the relation algebra. I don't know whether the modal logic K has a sole sufficient operator, but I suspect it doesn't. If there's a proof out there that K doesn't have a sole sufficient operator, it could be applied to the relation algebra. The relation algebra seems on a crude intuitive level to be a lot more complicated than K. When Post looked at the Boolean operators and how they relate to each other, he looked at properties of the operators preserved under composition (e.g. monotonic operators composed with themselves result in monotonic operators). A strategy for showing no sole sufficient operator exists for the relation algebra would be to find two mutually exclusive properties which are preserved under composition and possessed by at least one of $^{-}$, $\wedge$, $^{c}$, $\bullet$ and $I$.",,"['abstract-algebra', 'logic', 'relations', 'algebraic-logic']"
22,Is there an algebraic formula that gives this weird multiplication?: $(-x)\circ(-x)=(-x)\circ(x)$,Is there an algebraic formula that gives this weird multiplication?:,(-x)\circ(-x)=(-x)\circ(x),"I'd like to know whether it's possible to give an equivalent algebraic formula, in terms of normal algebraic operations (i.e. $+, -, ×, ÷, x^y$ ), if possible avoiding $|x|$ , for an operator $\circ$ , in the domain ℤ such that: \begin{array}{|r | r r r r | r r r | r r r r} \hline \circ & ... & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & ... \\ \hline \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} \\ -4 & ... & -16 & -12 & -8 & -1 & 0 & -4 & -8 & -12 & -16 & ... \\ -3 & ... & -12 & -9 & -6 & -1 & 0 & -3 & -6 & -9 & -12 & ... \\ -2 & ... & -8 & -6 & -4 & -1 & 0 & -2 & -4 & -6 & -8 & ... \\ \hline -1 & ... & -4 & -3 & -2 & -1 & 0 & -1 & -2 & -3 & -4 & ... \\ 0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... \\ 1 & ... & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & ... \\ \hline 2 & ... & -8 & -6 & -4 & -2 & 0 & 2 & 4 & 6 & 8 & ... \\ 3 & ... & -12 & -9 & -6 & -3 & 0 & 3 & 6 & 9 & 12 & ... \\ 4 & ... & -16 & -12 & -8 & -4 & 0 & 4 & 8 & 12 & 16 & ... \\ \vdots & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\ \end{array} As you see, this is some sort of weird multiplication where $(-)\cdot(-)=(-)\cdot(+)=(+)\cdot(-)=(-)$ but $(+)\cdot(+)=(+)$ . I am mostly interested in the subcase of the square in the middle and, if possible, all the $\circ$ operations where at least one of $-1$ , $0$ or $1$ is an argument. If that operation can satisfy at least the square at the middle, that will be enough for me. As a last resort I'm disposed to accept division by zero defined in such a way that for all $x$ , $x/0=0$ . But it is important to note that, even if it doesn't matter too much what happens outside the domain $\{-1, 0, 1\}$ , the operation must be defined for all integers: no modules, no restricted domains. If that isn't possible, what strategy shall I use to prove it? ps: Since I'm not a mathematician, I'd like to apologise for any formal or conceptual error I've made. Corrections, though, are more than encouraged.","I'd like to know whether it's possible to give an equivalent algebraic formula, in terms of normal algebraic operations (i.e. ), if possible avoiding , for an operator , in the domain ℤ such that: As you see, this is some sort of weird multiplication where but . I am mostly interested in the subcase of the square in the middle and, if possible, all the operations where at least one of , or is an argument. If that operation can satisfy at least the square at the middle, that will be enough for me. As a last resort I'm disposed to accept division by zero defined in such a way that for all , . But it is important to note that, even if it doesn't matter too much what happens outside the domain , the operation must be defined for all integers: no modules, no restricted domains. If that isn't possible, what strategy shall I use to prove it? ps: Since I'm not a mathematician, I'd like to apologise for any formal or conceptual error I've made. Corrections, though, are more than encouraged.","+, -, ×, ÷, x^y |x| \circ \begin{array}{|r | r r r r | r r r | r r r r}
\hline
\circ & ... & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & ... \\ \hline
\vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} \\
-4 & ... & -16 & -12 & -8 & -1 & 0 & -4 & -8 & -12 & -16 & ... \\
-3 & ... & -12 & -9 & -6 & -1 & 0 & -3 & -6 & -9 & -12 & ... \\
-2 & ... & -8 & -6 & -4 & -1 & 0 & -2 & -4 & -6 & -8 & ... \\ \hline
-1 & ... & -4 & -3 & -2 & -1 & 0 & -1 & -2 & -3 & -4 & ... \\
0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... \\
1 & ... & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & ... \\ \hline
2 & ... & -8 & -6 & -4 & -2 & 0 & 2 & 4 & 6 & 8 & ... \\
3 & ... & -12 & -9 & -6 & -3 & 0 & 3 & 6 & 9 & 12 & ... \\
4 & ... & -16 & -12 & -8 & -4 & 0 & 4 & 8 & 12 & 16 & ... \\
\vdots & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
\end{array} (-)\cdot(-)=(-)\cdot(+)=(+)\cdot(-)=(-) (+)\cdot(+)=(+) \circ -1 0 1 x x/0=0 \{-1, 0, 1\}","['abstract-algebra', 'group-theory', 'abelian-groups']"
23,"Let $G$ be a group such that $a^2 = a$ for all $a \in G$, Is $G$ an abelian group?","Let  be a group such that  for all , Is  an abelian group?",G a^2 = a a \in G G,"I tried to solve this by following: Since $G$ is a group, an inverse exists for every element in $G$. Multiply by inverse to $a$ on both sides of $a^2 = a$. We will get $a = i$, where $i$ is the identity element. This holds true for all $a*$, which implies $G$ contains only one distinct element i.e. $i.$ Hence $G$ is abelian. Is my approach correct?","I tried to solve this by following: Since $G$ is a group, an inverse exists for every element in $G$. Multiply by inverse to $a$ on both sides of $a^2 = a$. We will get $a = i$, where $i$ is the identity element. This holds true for all $a*$, which implies $G$ contains only one distinct element i.e. $i.$ Hence $G$ is abelian. Is my approach correct?",,"['abstract-algebra', 'group-theory', 'proof-verification']"
24,"Is ${\varprojlim}^1 (\mathbb{Z}, \cdot n_k)$ torsion(free)?",Is  torsion(free)?,"{\varprojlim}^1 (\mathbb{Z}, \cdot n_k)","Let $$\mathbb{Z} \xleftarrow{\cdot n_1} \mathbb{Z} \xleftarrow{\cdot n_2} \mathbb{Z} \xleftarrow{\cdot n_3} \cdots$$ be an inverse system with $n_i$ a natural number bigger 1. Actually I'm trying to find out if the ${\varprojlim}^1$ term is not a torsion group, where ${\varprojlim}^1 (\mathbb{Z}, \cdot n_k)$ is defined as the cokernel of the map  $$ \mu : \prod_{k \in \mathbb{N}} \mathbb{Z} \longrightarrow \prod_{k \in \mathbb{N}} \mathbb{Z} \text{ , } (z_k)_k \longmapsto (z_k - n_k \cdot z_{k+1})_k \text{ .}$$ In the literature I found similiar examples saying it follows by an easy argument that ${\varprojlim}^1 (\mathbb{Z}, \cdot n_k)$ is torsionfree, but I don't seem to see it even after I worked on it for a while. My first idea was to take this inverse system into an exact sequence $$ 0 \rightarrow \left(\mathbb{Z}, \cdot n_k \right) \rightarrow \left(\mathbb{Z}, \text{id} \right) \rightarrow \left( \mathbb{Z} \bigg/ \prod_{i=1} ^k n_i, \text{projections} \right) \rightarrow 0 $$ so by the long exact lim-lim$^1$ sequence the problem reduces to take a look at $\varprojlim \left(\mathbb{Z} \big/ \prod_{i=1} ^k n_i \right) \bigg/ \mathbb{Z}$ where $\mathbb{Z}$ is canonically embedded in $\varprojlim \left(\mathbb{Z} \big/ \prod_{i=1} ^k n_i \right)$ by taking a number $z$ to the sequence $(z)_k$. But also at this point I got stuck, so I tried working with the definition of ${\varprojlim}^1$ even so without any result. Has anybody a hint how to solve this problem? Thanks in advance. I've decoded the problem a little, so someone might have an idea for the important step (comments welcome). As I mentioned above we have ${\varprojlim}^1 (\mathbb{Z}, \cdot n_k) \cong \varprojlim \mathbb{Z} \big/ \prod_{i=1}^{k} n_i \bigg/ \mathbb{Z}$. Every element in $\mathbb{Z} \big/ \prod_{i=1}^{k} n_i$ is uniquely represented by an $0 \leq a_k < \prod_{i=1}^{k} n_i$, which we can uniquely write as $a_k = x_0 + x_1 n_1 + ... + x_{k-1} \prod_{i=1}^{k-1} n_i$, where $ 0 \leq x_j < \prod_{i=1}^{j} n_i$. Since $\varprojlim \mathbb{Z} \big/ \prod_{i=1}^{k} n_i$ is the kernel of the above map $\mu$ with $\prod_{k \in \mathbb{N}}\mathbb{Z} \big/ \prod_{i=1}^{k} n_i$ instead of $\prod_{k \in \mathbb{N}} \mathbb{Z}$, we have $a_{k+1} \equiv a_k \text{ mod } \prod_{i=1}^{k} n_i$ for all $k \in \mathbb{N}$, so $a_{k+1}$ has a (unique) representation $x_0 + x_1 n_1 + ... + x_{k-1} \prod_{i=1}^{k-1} n_i + x_k \prod_{i=1}^{k} n_i$. Then an element in $\varprojlim \mathbb{Z} \big/ \prod_{i=1}^{k} n_i$ is not an element of $\mathbb{Z}$ if $x_k \neq 0$ for infinitely many $k$ in the above representation. So let $a_k = x_0 + x_1 n_1 + ... + x_k \prod_{i=1}^{k} n_i$ for every $k \in \mathbb{N}$ and $z \in \mathbb{Z} \backslash \{0\}$. We like to show $([za_k])_k \notin \mathbb{Z}$. For every $k$ we have $$za_{k+1} = z (x_0 + ... + x_k \prod_{i=1}^{k} n_i) \equiv y_0 + ... + y_k \prod_{i=1}^k n_i \text{ mod } \prod_{i=1}^{k+1} n_i$$ for some $0 \leq y_j < \prod_{i=1}^j n_i$. (More precisely for each $x_m$ we can write $zx_m \equiv b_0 + ... + b_{m-1} \prod_{i=1}^{m-1} n_i \text{ mod } \prod_{i=1}^{m} n_i$ for $0 \leq b_j < \prod_{i=1}^j n_i$.) Now the question is if we can choose infintely many $x_j \neq 0$ such that there are infinitely many of those $y_j$ which are nonzero.","Let $$\mathbb{Z} \xleftarrow{\cdot n_1} \mathbb{Z} \xleftarrow{\cdot n_2} \mathbb{Z} \xleftarrow{\cdot n_3} \cdots$$ be an inverse system with $n_i$ a natural number bigger 1. Actually I'm trying to find out if the ${\varprojlim}^1$ term is not a torsion group, where ${\varprojlim}^1 (\mathbb{Z}, \cdot n_k)$ is defined as the cokernel of the map  $$ \mu : \prod_{k \in \mathbb{N}} \mathbb{Z} \longrightarrow \prod_{k \in \mathbb{N}} \mathbb{Z} \text{ , } (z_k)_k \longmapsto (z_k - n_k \cdot z_{k+1})_k \text{ .}$$ In the literature I found similiar examples saying it follows by an easy argument that ${\varprojlim}^1 (\mathbb{Z}, \cdot n_k)$ is torsionfree, but I don't seem to see it even after I worked on it for a while. My first idea was to take this inverse system into an exact sequence $$ 0 \rightarrow \left(\mathbb{Z}, \cdot n_k \right) \rightarrow \left(\mathbb{Z}, \text{id} \right) \rightarrow \left( \mathbb{Z} \bigg/ \prod_{i=1} ^k n_i, \text{projections} \right) \rightarrow 0 $$ so by the long exact lim-lim$^1$ sequence the problem reduces to take a look at $\varprojlim \left(\mathbb{Z} \big/ \prod_{i=1} ^k n_i \right) \bigg/ \mathbb{Z}$ where $\mathbb{Z}$ is canonically embedded in $\varprojlim \left(\mathbb{Z} \big/ \prod_{i=1} ^k n_i \right)$ by taking a number $z$ to the sequence $(z)_k$. But also at this point I got stuck, so I tried working with the definition of ${\varprojlim}^1$ even so without any result. Has anybody a hint how to solve this problem? Thanks in advance. I've decoded the problem a little, so someone might have an idea for the important step (comments welcome). As I mentioned above we have ${\varprojlim}^1 (\mathbb{Z}, \cdot n_k) \cong \varprojlim \mathbb{Z} \big/ \prod_{i=1}^{k} n_i \bigg/ \mathbb{Z}$. Every element in $\mathbb{Z} \big/ \prod_{i=1}^{k} n_i$ is uniquely represented by an $0 \leq a_k < \prod_{i=1}^{k} n_i$, which we can uniquely write as $a_k = x_0 + x_1 n_1 + ... + x_{k-1} \prod_{i=1}^{k-1} n_i$, where $ 0 \leq x_j < \prod_{i=1}^{j} n_i$. Since $\varprojlim \mathbb{Z} \big/ \prod_{i=1}^{k} n_i$ is the kernel of the above map $\mu$ with $\prod_{k \in \mathbb{N}}\mathbb{Z} \big/ \prod_{i=1}^{k} n_i$ instead of $\prod_{k \in \mathbb{N}} \mathbb{Z}$, we have $a_{k+1} \equiv a_k \text{ mod } \prod_{i=1}^{k} n_i$ for all $k \in \mathbb{N}$, so $a_{k+1}$ has a (unique) representation $x_0 + x_1 n_1 + ... + x_{k-1} \prod_{i=1}^{k-1} n_i + x_k \prod_{i=1}^{k} n_i$. Then an element in $\varprojlim \mathbb{Z} \big/ \prod_{i=1}^{k} n_i$ is not an element of $\mathbb{Z}$ if $x_k \neq 0$ for infinitely many $k$ in the above representation. So let $a_k = x_0 + x_1 n_1 + ... + x_k \prod_{i=1}^{k} n_i$ for every $k \in \mathbb{N}$ and $z \in \mathbb{Z} \backslash \{0\}$. We like to show $([za_k])_k \notin \mathbb{Z}$. For every $k$ we have $$za_{k+1} = z (x_0 + ... + x_k \prod_{i=1}^{k} n_i) \equiv y_0 + ... + y_k \prod_{i=1}^k n_i \text{ mod } \prod_{i=1}^{k+1} n_i$$ for some $0 \leq y_j < \prod_{i=1}^j n_i$. (More precisely for each $x_m$ we can write $zx_m \equiv b_0 + ... + b_{m-1} \prod_{i=1}^{m-1} n_i \text{ mod } \prod_{i=1}^{m} n_i$ for $0 \leq b_j < \prod_{i=1}^j n_i$.) Now the question is if we can choose infintely many $x_j \neq 0$ such that there are infinitely many of those $y_j$ which are nonzero.",,"['abstract-algebra', 'commutative-algebra', 'homological-algebra']"
25,cyclotomic field automorphism,cyclotomic field automorphism,,"$\mathbb F_9$ is the $8^{th}$ cyclotomic field over $\mathbb F_3$. My problem is, that the $8^{th}$ cyclotomic polynomial factors into $2$ (distinct) factors $f$ and $g$, which means, that there is no $\mathbb F_3$-automorphism of $\mathbb F_9$ which maps a zero of $f$ to a zero of $g$. My question is, why does this happen? From first sight I would have guessed that we could map a primitive $8^{th}$ root of unity to another, just by $\zeta\mapsto\zeta^{k}$. What is the problem with this mapping? (I'm not asking why the order of 3 in $\mathbb Z/8\mathbb Z$ is not $\phi(8)$ :), I just hope to get some intuition).","$\mathbb F_9$ is the $8^{th}$ cyclotomic field over $\mathbb F_3$. My problem is, that the $8^{th}$ cyclotomic polynomial factors into $2$ (distinct) factors $f$ and $g$, which means, that there is no $\mathbb F_3$-automorphism of $\mathbb F_9$ which maps a zero of $f$ to a zero of $g$. My question is, why does this happen? From first sight I would have guessed that we could map a primitive $8^{th}$ root of unity to another, just by $\zeta\mapsto\zeta^{k}$. What is the problem with this mapping? (I'm not asking why the order of 3 in $\mathbb Z/8\mathbb Z$ is not $\phi(8)$ :), I just hope to get some intuition).",,"['abstract-algebra', 'finite-fields', 'cyclotomic-fields']"
26,Maximal ideal contained in an ideal,Maximal ideal contained in an ideal,,"There is probably a simple answer to this but I can't for the life of me figure it out. Every ring in this question has a unit but isn't necessarily commutative. Let $R$ be a ring and let $I$ be a left ideal of $R$. In Basic Algebra II , Jacobson defines $$(I:R)=\{ b \in R \mid bR \subseteq I\}.$$ This is an ideal because it equals, $\text{ann}_R R/I$, the annihilator of the left $R$-module $R/I$. Because $R$ has a $1$ it follows that $(I:R)$ is contained in $I$. The author goes on to claim that $(I:R)$ contains every left ideal of $R$ properly contained in $I$, but I'm not seeing it. If $J$ is a left ideal of $R$ contained in $I$ why should it be that $J \subseteq (I:R)$? The ""obvious"" thing would be to say that that since $J$ is an ideal $JR \subseteq J \subseteq I$, but the rings here aren't necessarily commutative and $J$ is only a left ideal so this doesn't work. Is there any sort of reason this actually works given that $J$ is properly contained in $I$?","There is probably a simple answer to this but I can't for the life of me figure it out. Every ring in this question has a unit but isn't necessarily commutative. Let $R$ be a ring and let $I$ be a left ideal of $R$. In Basic Algebra II , Jacobson defines $$(I:R)=\{ b \in R \mid bR \subseteq I\}.$$ This is an ideal because it equals, $\text{ann}_R R/I$, the annihilator of the left $R$-module $R/I$. Because $R$ has a $1$ it follows that $(I:R)$ is contained in $I$. The author goes on to claim that $(I:R)$ contains every left ideal of $R$ properly contained in $I$, but I'm not seeing it. If $J$ is a left ideal of $R$ contained in $I$ why should it be that $J \subseteq (I:R)$? The ""obvious"" thing would be to say that that since $J$ is an ideal $JR \subseteq J \subseteq I$, but the rings here aren't necessarily commutative and $J$ is only a left ideal so this doesn't work. Is there any sort of reason this actually works given that $J$ is properly contained in $I$?",,"['abstract-algebra', 'ring-theory', 'ideals']"
27,What is the example of a module that is local but not endolocal?,What is the example of a module that is local but not endolocal?,,A module $M$ over a ring $R$ is called local if $M$ has a largest proper submodule. A module $M$ over a ring $R$ is called endolocal if $End_R(M)$ is local. I am trying to find an example for a local module but not endolocal.,A module $M$ over a ring $R$ is called local if $M$ has a largest proper submodule. A module $M$ over a ring $R$ is called endolocal if $End_R(M)$ is local. I am trying to find an example for a local module but not endolocal.,,"['abstract-algebra', 'modules']"
28,"$p$ the smallest prime divisor of $|G|$ , $x\in G$ an element of order $p$ . Suppose $h\in G$ such that $hxh^{-1}=x^{10} $ . Show that $p=3 $ .","the smallest prime divisor of  ,  an element of order  . Suppose  such that  . Show that  .",p |G| x\in G p h\in G hxh^{-1}=x^{10}  p=3 ,"Let $G$ be a finite group , $p$ the smallest prime divisor of $|G|$ , $x\in G$ an element of order $p$ . Suppose $h\in G$ such that $hxh^{-1}=x^{10} $ . Show that $p=3 $ . My solution : let $|G|=d$ . Claim 1 : $(10, p)=1 $ . proof : Otherwise $p=2 $or $p=5 $  both of which contradict $hxh^{-1}=x^{10} $ . So $(10,p)=1$ . claim 2: $(d,p-1)=1 $ . proof : $(d,p-1)$ is not $1$ then it is divisible by a prime $q$ . So $q\leq p-1<p $ and $q|d $ . Hence contradicting the hypothesis  that $p$ is smallest a prime dividing $d$ . Back to the main proof : $x=h^d x h^{-d}= h^{d-1} x^{10} h^{-(d-1)} =..=x^{10^d}$ . So $ x^{10^d-1}=1$ . It gives $10^d=1(\mod p)$ . By claim 1 and Fermat's theorem $10^{p-1}=1 (\mod p)$ Now by claim 2 we have $s, t \in \mathbb{Z}$ such that $ds+(p-1)t=1 $ . So $10=10^{ds+(p-1)t}=10^{ds}10^{(p-1)t}=1(\mod p)$ .So $p|9$ .But $p$ is prime . So $p=3 $ . QED . Please tell me if i'm missing out some cases or the solution is wrong .","Let $G$ be a finite group , $p$ the smallest prime divisor of $|G|$ , $x\in G$ an element of order $p$ . Suppose $h\in G$ such that $hxh^{-1}=x^{10} $ . Show that $p=3 $ . My solution : let $|G|=d$ . Claim 1 : $(10, p)=1 $ . proof : Otherwise $p=2 $or $p=5 $  both of which contradict $hxh^{-1}=x^{10} $ . So $(10,p)=1$ . claim 2: $(d,p-1)=1 $ . proof : $(d,p-1)$ is not $1$ then it is divisible by a prime $q$ . So $q\leq p-1<p $ and $q|d $ . Hence contradicting the hypothesis  that $p$ is smallest a prime dividing $d$ . Back to the main proof : $x=h^d x h^{-d}= h^{d-1} x^{10} h^{-(d-1)} =..=x^{10^d}$ . So $ x^{10^d-1}=1$ . It gives $10^d=1(\mod p)$ . By claim 1 and Fermat's theorem $10^{p-1}=1 (\mod p)$ Now by claim 2 we have $s, t \in \mathbb{Z}$ such that $ds+(p-1)t=1 $ . So $10=10^{ds+(p-1)t}=10^{ds}10^{(p-1)t}=1(\mod p)$ .So $p|9$ .But $p$ is prime . So $p=3 $ . QED . Please tell me if i'm missing out some cases or the solution is wrong .",,"['abstract-algebra', 'group-theory', 'finite-groups']"
29,(Revised) Prove that $\phi$ is a group homomorphism and find the kernel.,(Revised) Prove that  is a group homomorphism and find the kernel.,\phi,"Quoting "" Let $\phi : \Bbb Z \rightarrow \Bbb Z$ be given by $\phi(n) = 7n$. Prove that $\phi$ is a group homomorphism. Find the kernel, and the image of $\phi$."" My understanding: Part 1: Given $n,m \in \Bbb Z$, let's check that $\phi(n+m) =\phi(n)+\phi(m)$ $$\phi(n+m) = 7(n+m) = 7n + 7m = \phi(n)+\phi(m)$$ Therefore as the group operation in $\phi$ is preserved, $\phi$ is a group homomorphism. Part 2 (revised): $\phi$ is one-to-one as:  $\space \space \phi (n) = \phi(m) \Rightarrow 7n =7m \Rightarrow n=m  $. We also know that $\phi(e)=e$ by the property of homomorphism. Therefore, no other object than $0$ in the domain can map to $0$ in the codomain. It follows that: $$\ker(\phi)=\{ x \in \Bbb Z : \phi(x)=e\}=\{e\}$$ Part 3 (revised): I claim that the only possible image-elements of $\phi$ are the multiples of $7$ in $\mathbb Z$ denoted $7\mathbb Z$. To prove this I have to show that Im$(\phi) \subset 7 \Bbb Z$ and that $ 7 \Bbb Z \subset $ Im$(\phi)$. Proving Im$(\phi) \subset 7 \Bbb Z$ : $\forall y \in $ Im$(\phi)$ such that $  y =\phi(x)$ where $x \in 7 \Bbb Z$. It follows that $y \in 7 \Bbb Z$, therefore Im$(\phi) \subset 7 \Bbb Z$ Proving $ 7 \Bbb Z \subset $ Im$(\phi)$: $\forall y \in  7\Bbb Z$ such that $  y =7x$ where $x \in \phi(x)$. It follows that $y \in \phi(x)$, therefore $ 7 \Bbb Z \subset $ Im$(\phi)$. Any input to my understanding is much appreciated.","Quoting "" Let $\phi : \Bbb Z \rightarrow \Bbb Z$ be given by $\phi(n) = 7n$. Prove that $\phi$ is a group homomorphism. Find the kernel, and the image of $\phi$."" My understanding: Part 1: Given $n,m \in \Bbb Z$, let's check that $\phi(n+m) =\phi(n)+\phi(m)$ $$\phi(n+m) = 7(n+m) = 7n + 7m = \phi(n)+\phi(m)$$ Therefore as the group operation in $\phi$ is preserved, $\phi$ is a group homomorphism. Part 2 (revised): $\phi$ is one-to-one as:  $\space \space \phi (n) = \phi(m) \Rightarrow 7n =7m \Rightarrow n=m  $. We also know that $\phi(e)=e$ by the property of homomorphism. Therefore, no other object than $0$ in the domain can map to $0$ in the codomain. It follows that: $$\ker(\phi)=\{ x \in \Bbb Z : \phi(x)=e\}=\{e\}$$ Part 3 (revised): I claim that the only possible image-elements of $\phi$ are the multiples of $7$ in $\mathbb Z$ denoted $7\mathbb Z$. To prove this I have to show that Im$(\phi) \subset 7 \Bbb Z$ and that $ 7 \Bbb Z \subset $ Im$(\phi)$. Proving Im$(\phi) \subset 7 \Bbb Z$ : $\forall y \in $ Im$(\phi)$ such that $  y =\phi(x)$ where $x \in 7 \Bbb Z$. It follows that $y \in 7 \Bbb Z$, therefore Im$(\phi) \subset 7 \Bbb Z$ Proving $ 7 \Bbb Z \subset $ Im$(\phi)$: $\forall y \in  7\Bbb Z$ such that $  y =7x$ where $x \in \phi(x)$. It follows that $y \in \phi(x)$, therefore $ 7 \Bbb Z \subset $ Im$(\phi)$. Any input to my understanding is much appreciated.",,"['abstract-algebra', 'group-theory', 'proof-verification']"
30,Specific example of a certain claim in a paper,Specific example of a certain claim in a paper,,"In a paper of Heath-Brown, it is stated on page 24 that for any number field $K$, there exist ideals $\mathfrak{a_1, \dots, a_t}$ s.t. all fractional ideals can be written as the product of powers of the $\mathfrak{a_j}$ and a principal ideal uniquely (with some restrictions on the exponents). What would an example of $\mathfrak{a_1, \dots, a_t}$ be in the case $K = \mathbb{Q}(\sqrt{-5})$?","In a paper of Heath-Brown, it is stated on page 24 that for any number field $K$, there exist ideals $\mathfrak{a_1, \dots, a_t}$ s.t. all fractional ideals can be written as the product of powers of the $\mathfrak{a_j}$ and a principal ideal uniquely (with some restrictions on the exponents). What would an example of $\mathfrak{a_1, \dots, a_t}$ be in the case $K = \mathbb{Q}(\sqrt{-5})$?",,"['abstract-algebra', 'number-theory', 'algebraic-number-theory']"
31,Is there any algebraic structure in the set of all continuous pdf's in $\mathbb{R}^n$ under some operation?,Is there any algebraic structure in the set of all continuous pdf's in  under some operation?,\mathbb{R}^n,"It is possible to define an algebraic structure to the set of all continuous probability densities under certain operation ? Example: Let $D = \{f(x_1,...,x_n) \mbox{ | } \int f(x_1,...,x_n)dx_1,...,dx_n = 1 \}$ This set posses any algebraic structure under certain operations such as multiplication, division, composition or any other special operation ? I'm just curious about this.","It is possible to define an algebraic structure to the set of all continuous probability densities under certain operation ? Example: Let $D = \{f(x_1,...,x_n) \mbox{ | } \int f(x_1,...,x_n)dx_1,...,dx_n = 1 \}$ This set posses any algebraic structure under certain operations such as multiplication, division, composition or any other special operation ? I'm just curious about this.",,"['abstract-algebra', 'probability-theory', 'statistics', 'probability-distributions']"
32,$P(z)-zQ(z)=0$ having no solution in $\mathbb{C}$,having no solution in,P(z)-zQ(z)=0 \mathbb{C},"I am currently reading ""Iteration of Rational Functions"" by Alan F. Beardon and my Question is about the beginning of $§2.6.$ which deals with Fixed points. Given a rational function $R=P/Q$ where $P,Q$ are coprime polynomials. If $z_0 \neq \infty$ is fixed by $R$, then $Q(z_0)\neq 0$ and $P(z_0)=z_0Q(z_0).$ It is now stated that the fixed points of $R$ in $\mathbb{C}$ are the solutions of $P(z)-zQ(z)=0$. After that it is noted that this need not have any solutions in $\mathbb{C}$ and the counter example given is $z \mapsto z+\frac{1}{z}$. So far so good. My question is now that I do not see how to get this counterexample with the given solution.","I am currently reading ""Iteration of Rational Functions"" by Alan F. Beardon and my Question is about the beginning of $§2.6.$ which deals with Fixed points. Given a rational function $R=P/Q$ where $P,Q$ are coprime polynomials. If $z_0 \neq \infty$ is fixed by $R$, then $Q(z_0)\neq 0$ and $P(z_0)=z_0Q(z_0).$ It is now stated that the fixed points of $R$ in $\mathbb{C}$ are the solutions of $P(z)-zQ(z)=0$. After that it is noted that this need not have any solutions in $\mathbb{C}$ and the counter example given is $z \mapsto z+\frac{1}{z}$. So far so good. My question is now that I do not see how to get this counterexample with the given solution.",,"['abstract-algebra', 'complex-analysis']"
33,What does a generated algebra look like?,What does a generated algebra look like?,,"In Kemper's book 'A Course on Commutative Algebra' there is a question (2.5 specifically) where one has to show that given a field $K$, a sub-algebra $A\subseteq K[x_1, ..., x_n]$, if we have a $S\subseteq A$ which generates $A$, as an algebra, then $S$ is separating. I'm not actually interested in how to show this as it is straightforward once the definition of 'generated' is clear, but it rather confused me. I figured that $S$ generating $A$ as an algebra means that every $f\in A$ can be written as $\sum_{i = 1}^n g_i h_i$, where $g_i\in S$ and $h_i\in K[x_1, ..., x_n]$, just like one would generate a module. This did not yield a proof after some work, so I figured my definition must be wrong but Googling didn't provide any alternatives. I then looked at the solution. Kemper says this: ""That $S$ generates $A$ means that for every element $f\in A$ there exist finitely many elements $f_1, ..., f_m \in S$ and a polynomial $F\in K[T_1, ..., T_m]$ in $m$ indeterminates such that $f = F(f_1, ..., f_m)$. I suspect my mistake is simple, but I currently can't see the forest for the trees. Is how I generate elements based on such an $S$ incorrect? It seems like the natural way to generate elements based on how one would generate elements in modules to me. EDIT: After a little more searching, I discovered I should have been more thorough. The question had been asked before: Definition of a finitely generated $k$ - algebra","In Kemper's book 'A Course on Commutative Algebra' there is a question (2.5 specifically) where one has to show that given a field $K$, a sub-algebra $A\subseteq K[x_1, ..., x_n]$, if we have a $S\subseteq A$ which generates $A$, as an algebra, then $S$ is separating. I'm not actually interested in how to show this as it is straightforward once the definition of 'generated' is clear, but it rather confused me. I figured that $S$ generating $A$ as an algebra means that every $f\in A$ can be written as $\sum_{i = 1}^n g_i h_i$, where $g_i\in S$ and $h_i\in K[x_1, ..., x_n]$, just like one would generate a module. This did not yield a proof after some work, so I figured my definition must be wrong but Googling didn't provide any alternatives. I then looked at the solution. Kemper says this: ""That $S$ generates $A$ means that for every element $f\in A$ there exist finitely many elements $f_1, ..., f_m \in S$ and a polynomial $F\in K[T_1, ..., T_m]$ in $m$ indeterminates such that $f = F(f_1, ..., f_m)$. I suspect my mistake is simple, but I currently can't see the forest for the trees. Is how I generate elements based on such an $S$ incorrect? It seems like the natural way to generate elements based on how one would generate elements in modules to me. EDIT: After a little more searching, I discovered I should have been more thorough. The question had been asked before: Definition of a finitely generated $k$ - algebra",,"['abstract-algebra', 'commutative-algebra']"
34,"Prove $\mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{5}):\mathbb{Q}$ is a simple extension",Prove  is a simple extension,"\mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{5}):\mathbb{Q}","My problem is ""Prove $\mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{5}):\mathbb{Q}$ is a simple extension"" I met a similar problem ""Prove $\mathbb{Q}(\sqrt{2},\sqrt{3}):\mathbb{Q}$ is a simple extension"" and I showed that it is equal to Prove $\mathbb{Q}(\sqrt{2}+\sqrt{3}):\mathbb{Q}$. I let $u=\sqrt{2}+\sqrt{3}$ and deduce that $u-\sqrt{2}=\sqrt{3})$, then square it and prove $\sqrt{2} \in Q(\sqrt{2}+\sqrt{3})$. However, the same technique cannot be applied in the first problem since we have 3 square root. I actually found a proof for the first problem, by letting $u=\sqrt{2}+\sqrt{3}+\sqrt{5}$ and calculating $u,u^2,..., u^5$ and prove $\sqrt{2},\sqrt{3}\in Q(u)$. Though it's not so nice. Looking for a simpler answer","My problem is ""Prove $\mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{5}):\mathbb{Q}$ is a simple extension"" I met a similar problem ""Prove $\mathbb{Q}(\sqrt{2},\sqrt{3}):\mathbb{Q}$ is a simple extension"" and I showed that it is equal to Prove $\mathbb{Q}(\sqrt{2}+\sqrt{3}):\mathbb{Q}$. I let $u=\sqrt{2}+\sqrt{3}$ and deduce that $u-\sqrt{2}=\sqrt{3})$, then square it and prove $\sqrt{2} \in Q(\sqrt{2}+\sqrt{3})$. However, the same technique cannot be applied in the first problem since we have 3 square root. I actually found a proof for the first problem, by letting $u=\sqrt{2}+\sqrt{3}+\sqrt{5}$ and calculating $u,u^2,..., u^5$ and prove $\sqrt{2},\sqrt{3}\in Q(u)$. Though it's not so nice. Looking for a simpler answer",,"['abstract-algebra', 'field-theory', 'extension-field']"
35,Burnside / Cauchy-Frobenius Lemma for the automorphism group of a symmetric block design,Burnside / Cauchy-Frobenius Lemma for the automorphism group of a symmetric block design,,"I am quite familiar with the Burnside/Cauchy-Frobenius Lemma, which states that for a group $G$ acting on a set $X$, where $O$ is the number of orbits of $X$ under the action of $G$, we have: $$O=\frac{1}{|G|}\sum\limits_{g\in G}|\{x\in X: xg=x\}|,$$ that is, the number of orbits is found by finding the number of fixed points of the action for each group element and adding them up. I'm trying to write this lemma down in notation more friendly to the specific situation where we have a symmetric design $D$ and its automorphism group $\text{Aut}(D)$, but I am not entirely comfortable with what I came up with, and thought I would check with the community. I have already proven (and it is well-known) that any automorphism on a symmetric design $D$ fixes the same number of blocks and points, further blurring things. I believe $D$ is playing the role of $X$ above, and it makes sense to consider both blocks and points fixed by any element $\sigma\in\text{Aut}(D)$. I also think it makes sense that $\text{Aut}(D)$ is playing the role of $G$ above. So I arrive at the following statement for Burnside's Lemma in the context of a symmetric design and its automorphism group: $$O=\frac{1}{|\text{Aut}(D)|}\sum\limits_{\sigma\in\text{Aut}(D)}|\{p\in D:\sigma p=p\}|,$$ where $p$ represents a point. Does this look good to you? Main concern: $D$ isn't really a set, in the traditional sense - it is an incidence structure, so writing ""$p\in D$"" feels wrong/notationally abusive, but writing it out for blocks $b$ has the same problem. I suppose I could further muddy things by saying $X$ is the set of points on which our design lives, but then writing $p\in X$ seems to destroy the relation between my group ($\text{Aut}(D)$) and my set ($D$?... $X$?...) The questions: Does my statement of Burnside's Lemma for this situation look right? Is everything I'm doing consistent with a good understanding of how a design's automorphism group interacts with the design itself? Is my set in the summand reasonable and not too notationally weird?","I am quite familiar with the Burnside/Cauchy-Frobenius Lemma, which states that for a group $G$ acting on a set $X$, where $O$ is the number of orbits of $X$ under the action of $G$, we have: $$O=\frac{1}{|G|}\sum\limits_{g\in G}|\{x\in X: xg=x\}|,$$ that is, the number of orbits is found by finding the number of fixed points of the action for each group element and adding them up. I'm trying to write this lemma down in notation more friendly to the specific situation where we have a symmetric design $D$ and its automorphism group $\text{Aut}(D)$, but I am not entirely comfortable with what I came up with, and thought I would check with the community. I have already proven (and it is well-known) that any automorphism on a symmetric design $D$ fixes the same number of blocks and points, further blurring things. I believe $D$ is playing the role of $X$ above, and it makes sense to consider both blocks and points fixed by any element $\sigma\in\text{Aut}(D)$. I also think it makes sense that $\text{Aut}(D)$ is playing the role of $G$ above. So I arrive at the following statement for Burnside's Lemma in the context of a symmetric design and its automorphism group: $$O=\frac{1}{|\text{Aut}(D)|}\sum\limits_{\sigma\in\text{Aut}(D)}|\{p\in D:\sigma p=p\}|,$$ where $p$ represents a point. Does this look good to you? Main concern: $D$ isn't really a set, in the traditional sense - it is an incidence structure, so writing ""$p\in D$"" feels wrong/notationally abusive, but writing it out for blocks $b$ has the same problem. I suppose I could further muddy things by saying $X$ is the set of points on which our design lives, but then writing $p\in X$ seems to destroy the relation between my group ($\text{Aut}(D)$) and my set ($D$?... $X$?...) The questions: Does my statement of Burnside's Lemma for this situation look right? Is everything I'm doing consistent with a good understanding of how a design's automorphism group interacts with the design itself? Is my set in the summand reasonable and not too notationally weird?",,"['abstract-algebra', 'combinatorics', 'group-theory', 'permutations', 'combinatorial-designs']"
36,Prove that $ a\in Z(G) $,Prove that, a\in Z(G) ,"Let $ (G,\cdot ) $ be a group with $ |G|=2m+1,m\in \mathbb{N} $ and $ a\in G $ so that there exists $ n\in \mathbb{N} $ with $ a^{n}\cdot x=x\cdot a,\forall x\in G\setminus A $, where $ A=\left \{ a^{k}|k\in \mathbb{Z} \right \}. $ Prove that $ a\cdot x=x\cdot a,\forall x\in G. $ Obviously, if $ x\in A $, $ a\cdot x=x\cdot a. $ If $ x\notin A $, the only thing I've found is that $ a^{n}\cdot (x\cdot a^{n-1})^{2m}\cdot x\cdot a^{-1}=e $.","Let $ (G,\cdot ) $ be a group with $ |G|=2m+1,m\in \mathbb{N} $ and $ a\in G $ so that there exists $ n\in \mathbb{N} $ with $ a^{n}\cdot x=x\cdot a,\forall x\in G\setminus A $, where $ A=\left \{ a^{k}|k\in \mathbb{Z} \right \}. $ Prove that $ a\cdot x=x\cdot a,\forall x\in G. $ Obviously, if $ x\in A $, $ a\cdot x=x\cdot a. $ If $ x\notin A $, the only thing I've found is that $ a^{n}\cdot (x\cdot a^{n-1})^{2m}\cdot x\cdot a^{-1}=e $.",,"['abstract-algebra', 'group-theory', 'finite-groups']"
37,Tensor product and localisation of integers,Tensor product and localisation of integers,,"I have this doubt about tensor products. Let us fix $p$ a prime. Set $\mathbb{Z}[\frac{1}{p}]$ the localisation of $\mathbb Z$ in $p$ and $\mathbb{Z}_{(p)}$ the localisation at the prime ideal $(p)$. Now my question is the following: Is the canonical morphism of $\mathbb Z$-algebras:   $$\mathbb{Z}[\frac{1}{p}] \otimes_{\mathbb Z} \mathbb{Z}_{(p)}\rightarrow \mathbb Q$$   induced by the multiplication $\mathbb{Z}[\frac{1}{p}] \times\mathbb{Z}_{(p)}\rightarrow \mathbb Q,\ (x,y) \mapsto xy$,   an isomorphism? My guess is yes for every rationnal $\frac{a}{b}$ can be factored in the form $\prod_i p_i^{\alpha_i}$ with $\alpha_i\in\mathbb Z$, thus we can put every fraction $\frac{a}{b}$ under the form $\frac{m}{p^\alpha n}$ with $p \nmid n$ and $(m,n)=1$. I then construct the map: $$\frac{m}{p^\alpha n} \mapsto \frac{1}{p^\alpha}\otimes \frac{m}{n}$$ to be its inverse. Is this a valid proof? Are ther any more elegant proofs?","I have this doubt about tensor products. Let us fix $p$ a prime. Set $\mathbb{Z}[\frac{1}{p}]$ the localisation of $\mathbb Z$ in $p$ and $\mathbb{Z}_{(p)}$ the localisation at the prime ideal $(p)$. Now my question is the following: Is the canonical morphism of $\mathbb Z$-algebras:   $$\mathbb{Z}[\frac{1}{p}] \otimes_{\mathbb Z} \mathbb{Z}_{(p)}\rightarrow \mathbb Q$$   induced by the multiplication $\mathbb{Z}[\frac{1}{p}] \times\mathbb{Z}_{(p)}\rightarrow \mathbb Q,\ (x,y) \mapsto xy$,   an isomorphism? My guess is yes for every rationnal $\frac{a}{b}$ can be factored in the form $\prod_i p_i^{\alpha_i}$ with $\alpha_i\in\mathbb Z$, thus we can put every fraction $\frac{a}{b}$ under the form $\frac{m}{p^\alpha n}$ with $p \nmid n$ and $(m,n)=1$. I then construct the map: $$\frac{m}{p^\alpha n} \mapsto \frac{1}{p^\alpha}\otimes \frac{m}{n}$$ to be its inverse. Is this a valid proof? Are ther any more elegant proofs?",,"['abstract-algebra', 'proof-verification', 'tensor-products', 'localization']"
38,"Let $G$ be a finite abelian group with elements $a_1,a_2,\dots,a_n$. If $G$ has more than one element of order $2$ then $a_1a_2\dots a_n=1$.",Let  be a finite abelian group with elements . If  has more than one element of order  then .,"G a_1,a_2,\dots,a_n G 2 a_1a_2\dots a_n=1","Let $G$ be a finite abelian group with elements $a_1,a_2,\dots,a_n$. If $G$ has more than one element of order $2$ then $a_1a_2\dots a_n=1$. Attempt Clearly, if $a_i$ is not of order 2, the inverse of $a_i$ must be in the product. So the elements left are all of order $2$ or identity, say $b_1,b_2,\dots,b_m$ Since $b_i^2=1$ for $i=1,\dots,m$. Let $H=\{1,b_1,\dots,b_m\}$ Then $H \cong C_2\times C_2 \times\dots \times C_2$. For $C_2\times C_2$, it is indeed $V$-group $\{1,a,b,ab\}$. Clearly, $1abab=1$. Assume the result holds for direct product of less than $k$ cyclic groups of order $2$. Let $H$ be a direct product of $k$ cyclic groups of order $2$. Write $H=\langle b_1\rangle \times \dots \times \langle b_k\rangle$. Consider $K=\langle b_1\rangle \times \dots\times \langle b_{k-1} \rangle$. Then the result holds for $K$. Now I need to relate this result to $H$.","Let $G$ be a finite abelian group with elements $a_1,a_2,\dots,a_n$. If $G$ has more than one element of order $2$ then $a_1a_2\dots a_n=1$. Attempt Clearly, if $a_i$ is not of order 2, the inverse of $a_i$ must be in the product. So the elements left are all of order $2$ or identity, say $b_1,b_2,\dots,b_m$ Since $b_i^2=1$ for $i=1,\dots,m$. Let $H=\{1,b_1,\dots,b_m\}$ Then $H \cong C_2\times C_2 \times\dots \times C_2$. For $C_2\times C_2$, it is indeed $V$-group $\{1,a,b,ab\}$. Clearly, $1abab=1$. Assume the result holds for direct product of less than $k$ cyclic groups of order $2$. Let $H$ be a direct product of $k$ cyclic groups of order $2$. Write $H=\langle b_1\rangle \times \dots \times \langle b_k\rangle$. Consider $K=\langle b_1\rangle \times \dots\times \langle b_{k-1} \rangle$. Then the result holds for $K$. Now I need to relate this result to $H$.",,"['abstract-algebra', 'group-theory', 'finite-groups']"
39,Example of discrete subgroup of $\mathbb{R}^2$ where image of $X$ under 1st projection isn't discrete subset of $\mathbb{R}$?,Example of discrete subgroup of  where image of  under 1st projection isn't discrete subset of ?,\mathbb{R}^2 X \mathbb{R},"What is an example of a discrete subgroup $X$ of $(\mathbb{R}^2, +)$ where the image of $X$ under the 1st projection $\mathbb{R}^2 \to \mathbb{R}$ isn't a discrete subset of $\mathbb{R}$?","What is an example of a discrete subgroup $X$ of $(\mathbb{R}^2, +)$ where the image of $X$ under the 1st projection $\mathbb{R}^2 \to \mathbb{R}$ isn't a discrete subset of $\mathbb{R}$?",,['abstract-algebra']
40,Rank of Free Module over a Noncommutative Ring,Rank of Free Module over a Noncommutative Ring,,"There's a fairly standard proof that the rank of a free module $F$ over a commutative ring $R$ is well defined. We take a maximal ideal $I$ and note that $R/I$ is a field. Taking $R/I \otimes_R F$ gives a vector space of dimension rank of $F$, which gives the result. I was wondering where the proof breaks down in the non-commutative case (let's assume we have a unit). According to the wikipedia article on Division Rings, every module over a division ring is free with well-defined rank, and I don't see any issue with taking a maximal ideal (do we need to be careful with selecting maximal left/right/two-sided ideals?) or with taking the tensor product. If someone could point out what's wrong that would be greatly appreciated.","There's a fairly standard proof that the rank of a free module $F$ over a commutative ring $R$ is well defined. We take a maximal ideal $I$ and note that $R/I$ is a field. Taking $R/I \otimes_R F$ gives a vector space of dimension rank of $F$, which gives the result. I was wondering where the proof breaks down in the non-commutative case (let's assume we have a unit). According to the wikipedia article on Division Rings, every module over a division ring is free with well-defined rank, and I don't see any issue with taking a maximal ideal (do we need to be careful with selecting maximal left/right/two-sided ideals?) or with taking the tensor product. If someone could point out what's wrong that would be greatly appreciated.",,['abstract-algebra']
41,Modules finitely generated and of finite type (categorical meaning),Modules finitely generated and of finite type (categorical meaning),,"An object $C$ in an additive category admitting all filtered direct limits $\mathcal{C}$ is called ""of finite type"" if the canonical map $$\underrightarrow{\lim} Hom_{\mathcal{C}}(C,F(i))\to Hom_{\mathcal{C}}(C,\underrightarrow{\lim}F)$$ is injective for every $I$ directed poset for every functor $F:I\to \mathcal{C}$ In the case $\mathcal{C}$=Mod-R prove that this definition is equivalent to the definition of ""finitely generated"" The exercise has this strange hint: Use the fact that if $\mathcal{F}$ is the set of finitely generated submodules of a module C then  $$C/{\sum_{A\in\mathcal{F}}A}=\underrightarrow{\lim}C/A$$ I say that the hint is strange because $\displaystyle {\sum_{A\in\mathcal{F}}A}=C$","An object $C$ in an additive category admitting all filtered direct limits $\mathcal{C}$ is called ""of finite type"" if the canonical map $$\underrightarrow{\lim} Hom_{\mathcal{C}}(C,F(i))\to Hom_{\mathcal{C}}(C,\underrightarrow{\lim}F)$$ is injective for every $I$ directed poset for every functor $F:I\to \mathcal{C}$ In the case $\mathcal{C}$=Mod-R prove that this definition is equivalent to the definition of ""finitely generated"" The exercise has this strange hint: Use the fact that if $\mathcal{F}$ is the set of finitely generated submodules of a module C then  $$C/{\sum_{A\in\mathcal{F}}A}=\underrightarrow{\lim}C/A$$ I say that the hint is strange because $\displaystyle {\sum_{A\in\mathcal{F}}A}=C$",,"['abstract-algebra', 'category-theory', 'modules', 'limits-colimits']"
42,Commutator of two Lie subalgebras,Commutator of two Lie subalgebras,,"Let $\mathfrak{g}$ be a Lie algebra, and $\mathfrak{h},\mathfrak{m}$ two Lie subalgebras. Is it true that $[\mathfrak{h},\mathfrak{m}]$ is a Lie algebra in itself?. I have tried to develop a commutator of the form $[[a,b],[a',b']]$ (where $a,a'\in\mathfrak{h}$, and $b,b'\in\mathfrak{m}$) by applying repeatedly Jacobi's rule, but it seems that one of the subalgebras must be an ideal.","Let $\mathfrak{g}$ be a Lie algebra, and $\mathfrak{h},\mathfrak{m}$ two Lie subalgebras. Is it true that $[\mathfrak{h},\mathfrak{m}]$ is a Lie algebra in itself?. I have tried to develop a commutator of the form $[[a,b],[a',b']]$ (where $a,a'\in\mathfrak{h}$, and $b,b'\in\mathfrak{m}$) by applying repeatedly Jacobi's rule, but it seems that one of the subalgebras must be an ideal.",,"['abstract-algebra', 'lie-algebras']"
43,Understanding extension of scalars,Understanding extension of scalars,,"Let $V$ be a finite dimensional complex vector space. I recently asked how to find a Natural isomorphism between $\mathbb{C}\otimes_{\mathbb{R}}V$ and $V\oplus V.$ and got some very nice answers. In particular, I was told that the map $$c\otimes v \mapsto (\Re(c)v,\Im(c)v)$$ is a complex linear isomorphism. Here is the problem I am having with this: As I understand it, the scalar multiplication defined on the extension of scalars is given by $$c'(c\otimes v) = (c'c)\otimes v$$ and with this multiplication the above map is not complex linear. However, if I use $$c'(c\otimes v)=c\otimes (c'v)$$ as my scalar multiplication then the given map is indeed complex linear. So how do I reconcile this with the definition of scalar multiplication in an extension of scalars? Many thanks!","Let $V$ be a finite dimensional complex vector space. I recently asked how to find a Natural isomorphism between $\mathbb{C}\otimes_{\mathbb{R}}V$ and $V\oplus V.$ and got some very nice answers. In particular, I was told that the map $$c\otimes v \mapsto (\Re(c)v,\Im(c)v)$$ is a complex linear isomorphism. Here is the problem I am having with this: As I understand it, the scalar multiplication defined on the extension of scalars is given by $$c'(c\otimes v) = (c'c)\otimes v$$ and with this multiplication the above map is not complex linear. However, if I use $$c'(c\otimes v)=c\otimes (c'v)$$ as my scalar multiplication then the given map is indeed complex linear. So how do I reconcile this with the definition of scalar multiplication in an extension of scalars? Many thanks!",,"['abstract-algebra', 'vector-spaces', 'tensor-products']"
44,When the tensor product of two module elements is nonzero,When the tensor product of two module elements is nonzero,,"What are the main cases in which we can say that $a \otimes b \neq 0 \in A \otimes B$, where $A$ and $B$ are $R$ modules? It works for nonzero elements in free modules over an integral domain. Additional Question: What can we say about when all tensors are elementary? It is true when one of the factors is cyclic...is that the only reliable principle?","What are the main cases in which we can say that $a \otimes b \neq 0 \in A \otimes B$, where $A$ and $B$ are $R$ modules? It works for nonzero elements in free modules over an integral domain. Additional Question: What can we say about when all tensors are elementary? It is true when one of the factors is cyclic...is that the only reliable principle?",,"['abstract-algebra', 'soft-question', 'tensor-products']"
45,$0\to C'\to C\to C''\to0$ splits if $C\cong C'\oplus C''$ as a chain complex?,splits if  as a chain complex?,0\to C'\to C\to C''\to0 C\cong C'\oplus C'',"Question Given a unitary ring $A$ and an exact sequence $$0\to C'\xrightarrow iC\xrightarrow pC''\to0$$ in the Abelian category of chain complexes over $A$ , where $C,C',C''$ are chain complexes of finitely-generated free modules (I don't know whether this could be replaced by projective modules). If $C\cong C'\oplus C''$ as chain complexes, is it true that the original exact sequence splits in the Abelian category of chain complexes? Results If $A$ is a field or a PID , and that the complexes (the total complexes seen as $A$ -modules) are of finite rank, then the statement could be proved as follows: Let $\mathcal A$ be the Abelian category of chain complexes over $A$ . Take $\operatorname{Hom}_{\mathcal A}(C'',-)$ , we have an exact sequence: $$0\to\operatorname{Hom}(C'',C')\xrightarrow{i_*}\operatorname{Hom}(C'',C)\xrightarrow{p_*}\operatorname{Hom}(C'',C'')$$ Since $C\cong C'\oplus C''$ , we have $\operatorname{Hom}(C'',C)\cong\operatorname{Hom}(C'',C')\oplus\operatorname{Hom}(C'',C'')$ , and we should note that all these $\operatorname{Hom}$ 's are submodules of free modules, hence free ( $A$ is a PID) . It follows from dimension counting that $p_*$ is surjective, hence the original exact sequence splits. Backgrounds It's a generalization of Roth's theorem. Given matrices $A,B,C$ over a commutative ring $R$ and let $$P=\begin{bmatrix}B&0\\&C\end{bmatrix}$$ and $$P_A=\begin{bmatrix}B&A\\&C\end{bmatrix}$$ Then If $P,P_A$ are equivalent, then there exists $X,Y$ such that $A=BX-YC$ . If $B,C$ are square matrices and $P,P_A$ are similar, then there exists $X$ such that $A=BX-XC$ . The first statement follows from the question (if it's true): we consider two complexes $$K\colon\to0\to K_0=R^\bullet\xrightarrow CR^\bullet\to0\to$$ and $$L\colon\to R^\bullet\xrightarrow BL_0=R^\bullet\to0\to0\to$$ The chain homomorphism $f$ is given by the matrix $A\colon K_0\to L_0$ (and zero on any other degree). Consider the canonical exact sequence involving a mapping cone: $$0\to L\to\operatorname{cone}(f)\to K[-1]\to0$$ Note that the matrix associated to the boundary operator of $\operatorname{cone}(f)$ is $P_A$ (up to some signs), which means that $\operatorname{cone}(f)\cong L\oplus K[-1]$ . We apply the result of the question, and it follows directly that $f$ is null homotopic, hence we can solve the matrix equation. The second statement follows from the first statement. If $P,P_A$ are similar, then $T-P,T-P_A$ are equivalent over the ring $R[T]$ , hence there exists $P(T)\in\operatorname{Mat}(R[T])$ and $Q(T)\in\operatorname{Mat}(R[T])$ (we omit the computation of the magnitude of matrices) such that $(T-B)P(T)+Q(T)(T-C)=A$ . If we factor $Q(T)=(T-B)Q_1(T)+Q_0$ , we have $$(T-B)(P(T)+Q_1(T)(T-C)+Q_0)+BQ_0-Q_0C=A.$$ Compare the remainder term, we obtain $BQ_0-Q_0C=A$ . Maybe related I just found this post: A nonsplit short exact sequence of abelian groups with $B \cong A \oplus C$","Question Given a unitary ring and an exact sequence in the Abelian category of chain complexes over , where are chain complexes of finitely-generated free modules (I don't know whether this could be replaced by projective modules). If as chain complexes, is it true that the original exact sequence splits in the Abelian category of chain complexes? Results If is a field or a PID , and that the complexes (the total complexes seen as -modules) are of finite rank, then the statement could be proved as follows: Let be the Abelian category of chain complexes over . Take , we have an exact sequence: Since , we have , and we should note that all these 's are submodules of free modules, hence free ( is a PID) . It follows from dimension counting that is surjective, hence the original exact sequence splits. Backgrounds It's a generalization of Roth's theorem. Given matrices over a commutative ring and let and Then If are equivalent, then there exists such that . If are square matrices and are similar, then there exists such that . The first statement follows from the question (if it's true): we consider two complexes and The chain homomorphism is given by the matrix (and zero on any other degree). Consider the canonical exact sequence involving a mapping cone: Note that the matrix associated to the boundary operator of is (up to some signs), which means that . We apply the result of the question, and it follows directly that is null homotopic, hence we can solve the matrix equation. The second statement follows from the first statement. If are similar, then are equivalent over the ring , hence there exists and (we omit the computation of the magnitude of matrices) such that . If we factor , we have Compare the remainder term, we obtain . Maybe related I just found this post: A nonsplit short exact sequence of abelian groups with $B \cong A \oplus C$","A 0\to C'\xrightarrow iC\xrightarrow pC''\to0 A C,C',C'' C\cong C'\oplus C'' A A \mathcal A A \operatorname{Hom}_{\mathcal A}(C'',-) 0\to\operatorname{Hom}(C'',C')\xrightarrow{i_*}\operatorname{Hom}(C'',C)\xrightarrow{p_*}\operatorname{Hom}(C'',C'') C\cong C'\oplus C'' \operatorname{Hom}(C'',C)\cong\operatorname{Hom}(C'',C')\oplus\operatorname{Hom}(C'',C'') \operatorname{Hom} A p_* A,B,C R P=\begin{bmatrix}B&0\\&C\end{bmatrix} P_A=\begin{bmatrix}B&A\\&C\end{bmatrix} P,P_A X,Y A=BX-YC B,C P,P_A X A=BX-XC K\colon\to0\to K_0=R^\bullet\xrightarrow CR^\bullet\to0\to L\colon\to R^\bullet\xrightarrow BL_0=R^\bullet\to0\to0\to f A\colon K_0\to L_0 0\to L\to\operatorname{cone}(f)\to K[-1]\to0 \operatorname{cone}(f) P_A \operatorname{cone}(f)\cong L\oplus K[-1] f P,P_A T-P,T-P_A R[T] P(T)\in\operatorname{Mat}(R[T]) Q(T)\in\operatorname{Mat}(R[T]) (T-B)P(T)+Q(T)(T-C)=A Q(T)=(T-B)Q_1(T)+Q_0 (T-B)(P(T)+Q_1(T)(T-C)+Q_0)+BQ_0-Q_0C=A. BQ_0-Q_0C=A","['abstract-algebra', 'homological-algebra', 'exact-sequence']"
46,Is the number of isomorphism classes of quotients of a finite dimensional commutative ring over a field finite?,Is the number of isomorphism classes of quotients of a finite dimensional commutative ring over a field finite?,,"If $A$ is a finite dimensional unital and commutative algebra over some infinite field $k$, what is the number of isomophism classes of rings of the form $A/I$ where $I$ is a proper ideal of $A$? Is it finite? Certainly, for dimensions 7 and below (by the results of this paper ) the answer to the finite question will be yes, as the number of isomorphism classes of algebras of dimension $n \leq 6$ is finite. Is the number of isomorphism classes of such rings still finite if the dimension of $A$ is greater than 7?","If $A$ is a finite dimensional unital and commutative algebra over some infinite field $k$, what is the number of isomophism classes of rings of the form $A/I$ where $I$ is a proper ideal of $A$? Is it finite? Certainly, for dimensions 7 and below (by the results of this paper ) the answer to the finite question will be yes, as the number of isomorphism classes of algebras of dimension $n \leq 6$ is finite. Is the number of isomorphism classes of such rings still finite if the dimension of $A$ is greater than 7?",,"['abstract-algebra', 'ring-theory']"
47,In $D_{33}$ how do I find out number of elements of each order?,In  how do I find out number of elements of each order?,D_{33},"In $D_{33}$ i.e diehedral group of order 66. How do I find out number of elements of each order? The only idea I have is that possible order of any element can be 1,2,3,6,11,33,66. Now 1 is only for identity and since group is not cyclic 66 can not be order of any group. Thanks in advance to all.","In $D_{33}$ i.e diehedral group of order 66. How do I find out number of elements of each order? The only idea I have is that possible order of any element can be 1,2,3,6,11,33,66. Now 1 is only for identity and since group is not cyclic 66 can not be order of any group. Thanks in advance to all.",,"['abstract-algebra', 'group-theory']"
48,Galois Group Isomorphic to $S_3$.,Galois Group Isomorphic to .,S_3,"Let $f \in \mathbb{Q}[x]$ be an irreducible polynomial of degree 3. Suppose $f$ has one real root, we want to show that $$\text{Gal}(L/\mathbb{Q}) \cong S_3,$$ where $L$ is the splitting field of $f$. Since $f(x)$ is irreducible, $f(x)$ has 3 distinct roots $r_1, r_2$ and $r_3$. If we consider the discriminant, we note that $$\Delta = \prod_{i < j} (r_j - r_i)^2.$$ Since the roots $r_2$ and $r_3$ are distinct, we have that $\sqrt{\Delta} \not \in \mathbb{Q}$.The splitting field of $f$ is given by $\mathbb{Q}(r_1, \sqrt{\Delta})$. By the tower law, we have that $$[\mathbb{Q}(r_1, \sqrt{\Delta}) : \mathbb{Q}] = [\mathbb{Q}(r_1, \sqrt{\Delta}) : \mathbb{Q}(\sqrt{\Delta})] \cdot [\mathbb{Q}(\sqrt{\Delta}) : \mathbb{Q}] = 3 \cdot 2 =6.$$ We therefore have that the Galois group of $L/\mathbb{Q}$ is isomorphic to $S_3$. Is this is a legitimate proof? Also, how can I generalise this result to prove that the Galois group of an irreducible polynomial of degree $p$ with $p-2$ roots is $S_p$?","Let $f \in \mathbb{Q}[x]$ be an irreducible polynomial of degree 3. Suppose $f$ has one real root, we want to show that $$\text{Gal}(L/\mathbb{Q}) \cong S_3,$$ where $L$ is the splitting field of $f$. Since $f(x)$ is irreducible, $f(x)$ has 3 distinct roots $r_1, r_2$ and $r_3$. If we consider the discriminant, we note that $$\Delta = \prod_{i < j} (r_j - r_i)^2.$$ Since the roots $r_2$ and $r_3$ are distinct, we have that $\sqrt{\Delta} \not \in \mathbb{Q}$.The splitting field of $f$ is given by $\mathbb{Q}(r_1, \sqrt{\Delta})$. By the tower law, we have that $$[\mathbb{Q}(r_1, \sqrt{\Delta}) : \mathbb{Q}] = [\mathbb{Q}(r_1, \sqrt{\Delta}) : \mathbb{Q}(\sqrt{\Delta})] \cdot [\mathbb{Q}(\sqrt{\Delta}) : \mathbb{Q}] = 3 \cdot 2 =6.$$ We therefore have that the Galois group of $L/\mathbb{Q}$ is isomorphic to $S_3$. Is this is a legitimate proof? Also, how can I generalise this result to prove that the Galois group of an irreducible polynomial of degree $p$ with $p-2$ roots is $S_p$?",,"['abstract-algebra', 'polynomials']"
49,Show that giving a right-action of a group $G$ on a set $A$ is the same as giving a left-action of $G^{op}$ on A,Show that giving a right-action of a group  on a set  is the same as giving a left-action of  on A,G A G^{op},"This is a part of an exercise from ""Algebra: Chapter 0"" by Paolo Aluffi. First, I provide the necessary definitions. An action of a group $G$ on a set $A$ is a set-function $\rho: G \times A \to A$ such that $\forall g,h \in G \ \ \forall a \in A$ we have $\rho(e_G, a) = a, \ \ \  \rho(gh,a) = \rho(g, \rho(h,a))$ or, alternatively, An action of a group $G$ on a set $A$ is a homomorphism $\sigma: G \to S_A$ where $S_A$ denotes the symmetric group of $A$ . We can call this a left-action . A right-action would be defined the same way expect for ""associativity"" axiom: $\forall a \in A \ \ \ \forall g,h \in G \ \ \ a(gh) = (ag)h$ Next: An opposite group $G^{op}$ of a group $G$ is a group $(G, \times )$ where $a \times b = ba$ . I know that $G^{op} \cong G$ with $f, f(g) = g^{-1}$ being an isomorphism. Now , I need to show that giving a right-action of $G$ on a set A is the same as giving a homomorphism $G^{op} \to S_A$ , that is, a left-action of $G^{op}$ on $A$ . Any ideas?","This is a part of an exercise from ""Algebra: Chapter 0"" by Paolo Aluffi. First, I provide the necessary definitions. An action of a group on a set is a set-function such that we have or, alternatively, An action of a group on a set is a homomorphism where denotes the symmetric group of . We can call this a left-action . A right-action would be defined the same way expect for ""associativity"" axiom: Next: An opposite group of a group is a group where . I know that with being an isomorphism. Now , I need to show that giving a right-action of on a set A is the same as giving a homomorphism , that is, a left-action of on . Any ideas?","G A \rho: G \times A \to A \forall g,h \in G \ \ \forall a \in A \rho(e_G, a) = a, \ \ \  \rho(gh,a) = \rho(g, \rho(h,a)) G A \sigma: G \to S_A S_A A \forall a \in A \ \ \ \forall g,h \in G \ \ \ a(gh) = (ag)h G^{op} G (G, \times ) a \times b = ba G^{op} \cong G f, f(g) = g^{-1} G G^{op} \to S_A G^{op} A","['abstract-algebra', 'group-theory', 'group-actions']"
50,Is this an equivalent statement to the Fundamental Theorem of Algebra?,Is this an equivalent statement to the Fundamental Theorem of Algebra?,,"Is the following equivalent to the usual statement of the fundamental theorem of algebra: Let $$f(z)=c_nz^n+\cdots+c_1z+c_0$$ be a polynomial with complex coefficients. For all but finitely many $w \in \mathbb C$, $f(z)-w$ has $n$ distinct roots in $\mathbb C$. This seems different to just saying that $f(z)$ has, including multiplicities, $n$ roots. Because this statement does not leave the possibility that there could be finitely many points $w\in\mathbb C$ such that $f(z)=c_nz^n+\cdots+c_1z+(c_0-w)$ will not have $n$ roots. Any insight is appreciated!","Is the following equivalent to the usual statement of the fundamental theorem of algebra: Let $$f(z)=c_nz^n+\cdots+c_1z+c_0$$ be a polynomial with complex coefficients. For all but finitely many $w \in \mathbb C$, $f(z)-w$ has $n$ distinct roots in $\mathbb C$. This seems different to just saying that $f(z)$ has, including multiplicities, $n$ roots. Because this statement does not leave the possibility that there could be finitely many points $w\in\mathbb C$ such that $f(z)=c_nz^n+\cdots+c_1z+(c_0-w)$ will not have $n$ roots. Any insight is appreciated!",,"['abstract-algebra', 'complex-analysis', 'polynomials', 'roots']"
51,Finding a homomorphism from a subset of the fractions to the ring $\mathbb{Z}_p$.,Finding a homomorphism from a subset of the fractions to the ring .,\mathbb{Z}_p,"I'm practicing using the First Isomorphism Theorem for rings. Here is a question I got stuck on. Let $p$ be prime and let $T$ be the set of rational numbers (in lowest terms) whose denominators are not divisible by $p$. Let $I$ be the set of elements in $T$ whose numerators are divisible by $p$. Show that $T/I\cong \mathbb{Z}_p$. Basically I need a surjective homomorphism from $T$ to $\mathbb{Z}_p$ with kernel $I$. My thought was to define $f:T\to \mathbb{Z}_p$ by $f(m/n)=[m]_p$ but this turns out not to be a homomorphism. For example choosing $p=5$, now $f(\frac{3}{2}+\frac{1}{3})=f(\frac{11}{6})=[11]_5=[1]_5 \neq [4]_5=[3]_5+[1]_5=f(\frac{3}{2})+f(\frac{1}{3})$. I've also tried $f(m/n)=[mn]_p$, $f(m/n)=[n]_p$, and $f(m/n)=[m+n]_p$ but nothing seems to work. I would appreciate some hints as to what kind of a function I need to choose. Thanks!","I'm practicing using the First Isomorphism Theorem for rings. Here is a question I got stuck on. Let $p$ be prime and let $T$ be the set of rational numbers (in lowest terms) whose denominators are not divisible by $p$. Let $I$ be the set of elements in $T$ whose numerators are divisible by $p$. Show that $T/I\cong \mathbb{Z}_p$. Basically I need a surjective homomorphism from $T$ to $\mathbb{Z}_p$ with kernel $I$. My thought was to define $f:T\to \mathbb{Z}_p$ by $f(m/n)=[m]_p$ but this turns out not to be a homomorphism. For example choosing $p=5$, now $f(\frac{3}{2}+\frac{1}{3})=f(\frac{11}{6})=[11]_5=[1]_5 \neq [4]_5=[3]_5+[1]_5=f(\frac{3}{2})+f(\frac{1}{3})$. I've also tried $f(m/n)=[mn]_p$, $f(m/n)=[n]_p$, and $f(m/n)=[m+n]_p$ but nothing seems to work. I would appreciate some hints as to what kind of a function I need to choose. Thanks!",,"['abstract-algebra', 'ring-theory']"
52,$G_{\mathfrak a}(A)$ integral domain and $\bigcap \mathfrak a^n = 0$ implies $A$ is integral domain,integral domain and  implies  is integral domain,G_{\mathfrak a}(A) \bigcap \mathfrak a^n = 0 A,"This is Lemma 11.23 in Atiyah: For an ideal $\mathfrak a \subseteq A$ , define $G_{\mathfrak a} (A) = \bigoplus _{n=0} ^\infty \mathfrak a^n / \mathfrak a^{n+1}$ . The statement of the Lemma: Let $A$ be a ring, $\mathfrak a$ an ideal of $A$ such that $\bigcap_n \mathfrak a^n = 0$ . Suppose that $G_{\mathfrak a} (A)$ is an integral domain. Then $A$ is an integral domain. Proof: Let $x,y$ be nonzero elements of $A$ . Then since $\bigcap \mathfrak a^n = 0$ , there exist nonnegative integers $r,s$ such that $x \in \mathfrak a^r - \mathfrak a^{r+1}$ and $y \in \mathfrak a^s - \mathfrak a^{s+1}$ . My question is why should the powers of $\mathfrak a$ cover everything in $A$ to begin with? Certainly if this were true, and the intersection of all the powers is zero, then the above follows. So it seems that this depends on something along the lines of $$\bigcup \mathfrak a^n = A.$$ Does it have to do with the $\mathfrak a$ -adic topology where these powers of $\mathfrak a$ are the neighborhoods of $0$ ?","This is Lemma 11.23 in Atiyah: For an ideal , define . The statement of the Lemma: Let be a ring, an ideal of such that . Suppose that is an integral domain. Then is an integral domain. Proof: Let be nonzero elements of . Then since , there exist nonnegative integers such that and . My question is why should the powers of cover everything in to begin with? Certainly if this were true, and the intersection of all the powers is zero, then the above follows. So it seems that this depends on something along the lines of Does it have to do with the -adic topology where these powers of are the neighborhoods of ?","\mathfrak a \subseteq A G_{\mathfrak a} (A) = \bigoplus _{n=0} ^\infty \mathfrak a^n / \mathfrak a^{n+1} A \mathfrak a A \bigcap_n \mathfrak a^n = 0 G_{\mathfrak a} (A) A x,y A \bigcap \mathfrak a^n = 0 r,s x \in \mathfrak a^r - \mathfrak a^{r+1} y \in \mathfrak a^s - \mathfrak a^{s+1} \mathfrak a A \bigcup \mathfrak a^n = A. \mathfrak a \mathfrak a 0","['abstract-algebra', 'commutative-algebra', 'proof-explanation', 'graded-rings']"
53,"Show that $\mathbb{Q}(\sqrt{2}+\sqrt[3]{5})=\mathbb{Q}(\sqrt{2},\sqrt[3]{5})$",Show that,"\mathbb{Q}(\sqrt{2}+\sqrt[3]{5})=\mathbb{Q}(\sqrt{2},\sqrt[3]{5})","Show that $\mathbb{Q}(\sqrt{2}+\sqrt[3]{5})=\mathbb{Q}(\sqrt{2},\sqrt[3]{5})$ and find all $w\in \mathbb{Q}(\sqrt{2},\sqrt[3]{5})$ such that $\mathbb{Q}(w)=\mathbb{Q}(\sqrt{2},\sqrt[3]{5})$. It is clear that  $\mathbb{Q}(\sqrt{2}+\sqrt[3]{5}) \subseteq \mathbb{Q}(\sqrt{2},\sqrt[3]{5})$. But given any $x\in \mathbb{Q}(\sqrt{2},\sqrt[3]{5})$, how do I show that $x\in \mathbb{Q}(\sqrt{2}+\sqrt[3]{5})$? For the second question, is $w$ the associates of $\sqrt{2}+\sqrt[3]{5}$? How do I show that? Thanks!","Show that $\mathbb{Q}(\sqrt{2}+\sqrt[3]{5})=\mathbb{Q}(\sqrt{2},\sqrt[3]{5})$ and find all $w\in \mathbb{Q}(\sqrt{2},\sqrt[3]{5})$ such that $\mathbb{Q}(w)=\mathbb{Q}(\sqrt{2},\sqrt[3]{5})$. It is clear that  $\mathbb{Q}(\sqrt{2}+\sqrt[3]{5}) \subseteq \mathbb{Q}(\sqrt{2},\sqrt[3]{5})$. But given any $x\in \mathbb{Q}(\sqrt{2},\sqrt[3]{5})$, how do I show that $x\in \mathbb{Q}(\sqrt{2}+\sqrt[3]{5})$? For the second question, is $w$ the associates of $\sqrt{2}+\sqrt[3]{5}$? How do I show that? Thanks!",,"['abstract-algebra', 'field-theory']"
54,Subgroups of the Semi-Direct Product $\mathbb{Z}/7\mathbb{Z} \rtimes (\mathbb{Z}/7\mathbb{Z})^{\times}$,Subgroups of the Semi-Direct Product,\mathbb{Z}/7\mathbb{Z} \rtimes (\mathbb{Z}/7\mathbb{Z})^{\times},"I want to list all the subgroups of the semi-direct product $\mathbb{Z}/7\mathbb{Z} \rtimes (\mathbb{Z}/7\mathbb{Z})^{\times}$ , under the homomorphism $\theta: (\mathbb{Z}/7\mathbb{Z})^{\times} \rightarrow \mathrm{Aut}(\mathbb{Z}/7\mathbb{Z})$ , $\theta: a \mapsto \theta_{a}$ where $\theta_{a}(i)=ai$ . Until now, I know that the subgroups of $(\mathbb{Z}/7\mathbb{Z})^{\times}$ will be of orders $1, 2, 3$ or $6$ and moreover they will be unique (similarly, the cyclic group with $7$ elements only has the trivial subgroups). I was thinking that the subgroups of the semi-direct product would be semi-direct products of the subgroups of $\mathbb{Z}/7\mathbb{Z}$ and $(\mathbb{Z}/7\mathbb{Z})^{\times}$ . Is my claim correct? If not, what would be a way to compute those subgroups?","I want to list all the subgroups of the semi-direct product , under the homomorphism , where . Until now, I know that the subgroups of will be of orders or and moreover they will be unique (similarly, the cyclic group with elements only has the trivial subgroups). I was thinking that the subgroups of the semi-direct product would be semi-direct products of the subgroups of and . Is my claim correct? If not, what would be a way to compute those subgroups?","\mathbb{Z}/7\mathbb{Z} \rtimes (\mathbb{Z}/7\mathbb{Z})^{\times} \theta: (\mathbb{Z}/7\mathbb{Z})^{\times} \rightarrow \mathrm{Aut}(\mathbb{Z}/7\mathbb{Z}) \theta: a \mapsto \theta_{a} \theta_{a}(i)=ai (\mathbb{Z}/7\mathbb{Z})^{\times} 1, 2, 3 6 7 \mathbb{Z}/7\mathbb{Z} (\mathbb{Z}/7\mathbb{Z})^{\times}","['abstract-algebra', 'group-theory', 'cyclic-groups', 'semidirect-product']"
55,"Show that $\Bbb Q(\sqrt[p]{a}, \omega )=\Bbb Q(\sqrt[p]{a}+ \omega )$",Show that,"\Bbb Q(\sqrt[p]{a}, \omega )=\Bbb Q(\sqrt[p]{a}+ \omega )","Show that $\Bbb Q(\sqrt[p]{a}, \omega )=\Bbb Q(\sqrt[p]{a}+ \omega )$, where $\omega=e^{2\pi i/p}$ and $a$ is prime.  For simplicity let's call the left hand field $K$, and the right hand field $R$.  I know that the strategy is to show inclusion both ways to get the inequality.  I also know that $K$ is the splitting field of the polynomial of $\sqrt[p]{x}-a$, I have proven this. I can show that $R \subseteq K$ since I know that $\omega, \sqrt[p]{a} \in K$ implying that $\omega+\sqrt[p]{a} \in K$ implying $R \subseteq K$. I have trouble showing the opposite inclusion.  I know the goal is to show that $\omega, \sqrt[p]{a} \in R$ knowing that $\omega+\sqrt[p]{a} \in R$.  I tried multiple times to show this, mainly playing around with different polynomial and using the binomial theorem but I can't figure anything out for this. Any hints or help to show this is appreciated.  Thanks in advance","Show that $\Bbb Q(\sqrt[p]{a}, \omega )=\Bbb Q(\sqrt[p]{a}+ \omega )$, where $\omega=e^{2\pi i/p}$ and $a$ is prime.  For simplicity let's call the left hand field $K$, and the right hand field $R$.  I know that the strategy is to show inclusion both ways to get the inequality.  I also know that $K$ is the splitting field of the polynomial of $\sqrt[p]{x}-a$, I have proven this. I can show that $R \subseteq K$ since I know that $\omega, \sqrt[p]{a} \in K$ implying that $\omega+\sqrt[p]{a} \in K$ implying $R \subseteq K$. I have trouble showing the opposite inclusion.  I know the goal is to show that $\omega, \sqrt[p]{a} \in R$ knowing that $\omega+\sqrt[p]{a} \in R$.  I tried multiple times to show this, mainly playing around with different polynomial and using the binomial theorem but I can't figure anything out for this. Any hints or help to show this is appreciated.  Thanks in advance",,['abstract-algebra']
56,Alternative description of the sheafification,Alternative description of the sheafification,,"For me the sheafification of a given presheaf is this: Proposition-Definition: Given a presheaf $\mathscr{F}$, there is a sheaf $\mathscr{F}^+$ and a morphism $\theta \colon \mathscr{F} \to \mathscr{F}^+$, with the property that for any sheaf $\mathscr{G}$, and any morphism $\varphi \colon \mathscr{F} \to \mathscr{G}$, there is a unique morphism $\psi \colon \mathscr{F}^+ \to \mathscr{G}$ such that $\varphi = \psi \circ \theta$. Furthermore the pair $(\mathscr{F}^+, \theta)$ is unique up to unique isomorphism. $\mathscr{F}^+$ is called the sheaf associated to the presheaf $\mathscr{F}$. I know that this is equivalent to a universally repelling object in a certain category. But how could this be equal to the following? Suppose $\mathcal{F} \subset \mathscr{S}$, where $\mathscr{S}$ is a sheaf. Then we define    $$  \mathcal{F}^+ =  \left\{   s \in \mathscr{S}(U)   \,\middle|\,   \forall U \in \tau (X) :   \text{$s$ is locally in $\mathcal{F}(U)$}  \right\} $$   for a fixed topological space $X$. By locally in $\mathcal{F}(U)$ we mean that given $s \in \mathscr{S}(U)$ there exists an open covering $\{U_{\alpha}\}_{\alpha \in I}$ of $U$ such that $s|_{U_\alpha} \in \mathcal{F}(U_{\alpha}) \subset \mathscr{S}(U_{\alpha})$. The thing is that the only tool I have so far is the definition, so how can I get this? I know that I have to verify the universal property of my new $\mathcal{F}^{+}$ but I don't know how to do this. Thanks a lot in advance","For me the sheafification of a given presheaf is this: Proposition-Definition: Given a presheaf $\mathscr{F}$, there is a sheaf $\mathscr{F}^+$ and a morphism $\theta \colon \mathscr{F} \to \mathscr{F}^+$, with the property that for any sheaf $\mathscr{G}$, and any morphism $\varphi \colon \mathscr{F} \to \mathscr{G}$, there is a unique morphism $\psi \colon \mathscr{F}^+ \to \mathscr{G}$ such that $\varphi = \psi \circ \theta$. Furthermore the pair $(\mathscr{F}^+, \theta)$ is unique up to unique isomorphism. $\mathscr{F}^+$ is called the sheaf associated to the presheaf $\mathscr{F}$. I know that this is equivalent to a universally repelling object in a certain category. But how could this be equal to the following? Suppose $\mathcal{F} \subset \mathscr{S}$, where $\mathscr{S}$ is a sheaf. Then we define    $$  \mathcal{F}^+ =  \left\{   s \in \mathscr{S}(U)   \,\middle|\,   \forall U \in \tau (X) :   \text{$s$ is locally in $\mathcal{F}(U)$}  \right\} $$   for a fixed topological space $X$. By locally in $\mathcal{F}(U)$ we mean that given $s \in \mathscr{S}(U)$ there exists an open covering $\{U_{\alpha}\}_{\alpha \in I}$ of $U$ such that $s|_{U_\alpha} \in \mathcal{F}(U_{\alpha}) \subset \mathscr{S}(U_{\alpha})$. The thing is that the only tool I have so far is the definition, so how can I get this? I know that I have to verify the universal property of my new $\mathcal{F}^{+}$ but I don't know how to do this. Thanks a lot in advance",,"['abstract-algebra', 'algebraic-geometry', 'sheaf-theory']"
57,Is $\mathbb{C}^n$ a C*-algebra?,Is  a C*-algebra?,\mathbb{C}^n,"Hi I am new in learning C*-algebra. I know that under the usual $\mathbb{C}^n$-norm, $\mathbb{C}^n$ is a Banach Space. However, what will be an intuitive multiplication on $\mathbb{C}^n$ so that the usual $\mathbb{C}^n$-norm will be sub-multiplicative? In fact, I should prove that the usual $\mathbb{C}^n$-norm is a C* norm too. I was trying pointwise multiplication but I was stuck in the calculations. Edited(my calculations): I used for $u=(u_1,u_2,...,u_n),v=(v_1,v_2,...,v_n)\in \mathbb{C}^n$, $uv= (u_1v_1,u_2v_2,...,u_nv_n)$. So I am trying to proceed in proving $\|uv\|_n \le \|u\|_n \|v\|_n$. In particular I am stuck in proving that $\|uv\|_n=\|(u_1v_1,u_2v_2,...,u_nv_n)\|_n \le \|u\|_n \|v\|_n$. What I mean is I don't know how to ""split"" $(u_1v_1,u_2v_2,...,u_nv_n)$ into relations of $(u_1,u_2,...,u_n)$ and $(v_1,v_2,...,v_n)$. Another edit: By the answers below, $\mathbb{C}^n$ will be not a C*-algebra under the usual $\mathbb{C}^n$-norm.","Hi I am new in learning C*-algebra. I know that under the usual $\mathbb{C}^n$-norm, $\mathbb{C}^n$ is a Banach Space. However, what will be an intuitive multiplication on $\mathbb{C}^n$ so that the usual $\mathbb{C}^n$-norm will be sub-multiplicative? In fact, I should prove that the usual $\mathbb{C}^n$-norm is a C* norm too. I was trying pointwise multiplication but I was stuck in the calculations. Edited(my calculations): I used for $u=(u_1,u_2,...,u_n),v=(v_1,v_2,...,v_n)\in \mathbb{C}^n$, $uv= (u_1v_1,u_2v_2,...,u_nv_n)$. So I am trying to proceed in proving $\|uv\|_n \le \|u\|_n \|v\|_n$. In particular I am stuck in proving that $\|uv\|_n=\|(u_1v_1,u_2v_2,...,u_nv_n)\|_n \le \|u\|_n \|v\|_n$. What I mean is I don't know how to ""split"" $(u_1v_1,u_2v_2,...,u_nv_n)$ into relations of $(u_1,u_2,...,u_n)$ and $(v_1,v_2,...,v_n)$. Another edit: By the answers below, $\mathbb{C}^n$ will be not a C*-algebra under the usual $\mathbb{C}^n$-norm.",,"['abstract-algebra', 'operator-algebras']"
58,applications of (topological and algebraic) commutative diagrams in organic synthesis,applications of (topological and algebraic) commutative diagrams in organic synthesis,,"In algebraic topology, there are a lot of commutative diagrams and commutative diagrams up to homotopy. Different ways of compositions of maps in a commutative diagram are equal or homotopy equivalent. An example of commutative diagrams in algebraic topology In organic synthesis, there are some diagrams consisting of  synthetic routes. Different ways of chemical reactions produces the same product. An example of diagrams of synthetic routes Question: are there any really useful research topics about the application of commutative diagrams in mathematics into organic synthesis?","In algebraic topology, there are a lot of commutative diagrams and commutative diagrams up to homotopy. Different ways of compositions of maps in a commutative diagram are equal or homotopy equivalent. An example of commutative diagrams in algebraic topology In organic synthesis, there are some diagrams consisting of  synthetic routes. Different ways of chemical reactions produces the same product. An example of diagrams of synthetic routes Question: are there any really useful research topics about the application of commutative diagrams in mathematics into organic synthesis?",,"['abstract-algebra', 'algebraic-topology', 'applications', 'chemistry', 'diagram-chasing']"
59,Splitting of Short Exact sequence,Splitting of Short Exact sequence,,"Suppose $d_1d_2=n$ and let $0 \to d_1\mathbb Z_n \overset {i} \to \mathbb Z_n \stackrel {d_2\cdot} \to d_2\mathbb Z_n\to 0$ be a short exact sequence. Show that sequence splits iff $\gcd(d_1,d_2)=1$. Suppose s.e.s. split then $\mathbb Z_n=d_1\mathbb Z_n +d_2\mathbb Z_n$, hence $1=d_1a+d_2b$ hence $\gcd=1$. How to prove the converse?","Suppose $d_1d_2=n$ and let $0 \to d_1\mathbb Z_n \overset {i} \to \mathbb Z_n \stackrel {d_2\cdot} \to d_2\mathbb Z_n\to 0$ be a short exact sequence. Show that sequence splits iff $\gcd(d_1,d_2)=1$. Suppose s.e.s. split then $\mathbb Z_n=d_1\mathbb Z_n +d_2\mathbb Z_n$, hence $1=d_1a+d_2b$ hence $\gcd=1$. How to prove the converse?",,"['abstract-algebra', 'homological-algebra', 'exact-sequence']"
60,"$A\subseteq B$ integral domains with surjective multiplication, then the localization by all monic polynomials evaluated at some point is nonzero","integral domains with surjective multiplication, then the localization by all monic polynomials evaluated at some point is nonzero",A\subseteq B,"Let $A\subseteq B$ be two integral domains such that the multiplication function $m: A \times (B \setminus A) \to B$ , $m(x,y)=xy$ , is surjective. Let $S \subset A[x]$ be the set of all monic polynomials. For each $b \in B$ , let ${v}_b: A[x] \to B$ be the evaluation homomorphism, given by $v_b(f) = f(b)$ . Prove that there exists $b_0 \in B$ such that $({v}_{b_0}(S))^{-1}B \neq \{0\}$ . We look for some $b_0,b \in B, f \in S$ such that $\frac{b}{f(b_0)} \neq \frac{0}{1}$ . Thus we look for some elements $b_0, b \in B$ such that for all $g \in S$ , $b\cdot g(b_0)\neq 0 $ in $B$ . Since $B$ is an integral domain, this is equivalent to requiring that both $b, g(b_0)$ are nonzero. Hence the choice of $b$ doesn't seem to matter, and we only want to find a suitable $b_0 \in B$ that is not the root of any monic polynomial in $A[x]$ . Clearly we must have $b_0 \in B \setminus A$ , since otherwise we can take $g = x-b_0 \in S$ and get $b\cdot g(b_0) = 0 $ no matter how we choose $b$ . We didn't use our given information that $m$ is surjective and $A$ is an integral domain. The surjectivity implies that there are $x \in A, y \in B \setminus A$ , with $xy=1$ . Thus $x,y$ are units in $B$ . Which seems to be a dead-end.","Let be two integral domains such that the multiplication function , , is surjective. Let be the set of all monic polynomials. For each , let be the evaluation homomorphism, given by . Prove that there exists such that . We look for some such that . Thus we look for some elements such that for all , in . Since is an integral domain, this is equivalent to requiring that both are nonzero. Hence the choice of doesn't seem to matter, and we only want to find a suitable that is not the root of any monic polynomial in . Clearly we must have , since otherwise we can take and get no matter how we choose . We didn't use our given information that is surjective and is an integral domain. The surjectivity implies that there are , with . Thus are units in . Which seems to be a dead-end.","A\subseteq B m: A \times (B \setminus A) \to B m(x,y)=xy S \subset A[x] b \in B {v}_b: A[x] \to B v_b(f) = f(b) b_0 \in B ({v}_{b_0}(S))^{-1}B \neq \{0\} b_0,b \in B, f \in S \frac{b}{f(b_0)} \neq \frac{0}{1} b_0, b \in B g \in S b\cdot g(b_0)\neq 0  B B b, g(b_0) b b_0 \in B A[x] b_0 \in B \setminus A g = x-b_0 \in S b\cdot g(b_0) = 0  b m A x \in A, y \in B \setminus A xy=1 x,y B","['abstract-algebra', 'commutative-algebra', 'integral-domain', 'localization']"
61,Polynomials with some roots whose product is 1,Polynomials with some roots whose product is 1,,"Consider the complex coefficient polynomial equation \begin{eqnarray} x^n-\left(a_1+\binom{n}{1}\right)x^{n-1}+\cdots+(-1)^k\left(a_k+\binom{n}{k}\right)x^{n-k}+\cdots+(-1)^{n-1}\left(a_{n-1}+\binom{n}{n-1}\right)x+(-1)^n=0 \end{eqnarray} By Vieta Theorem, the product of its roots is 1. If we impose the condition that, among the $n$ roots, there exist $k$ roots (counted with multiplicity) whose product is 1, then $a_1, \cdots, a_{n-1}$ have to satisfy a polynomial equation $P(a_1, \cdots, a_{n-1})=0$, where $P\in\mathbb{C}[a_1, \cdots, a_{n-1}]$ has 0 as the constant term. Question: Under what condition does $P$ has nonzero linear term? The following are some easy examples I have worked out. If $k=1$, then that means one of the roots is 1. Plugging $x=1$ to the original polynomial equation yields that $P$ can be  \begin{eqnarray} \sum_{i=1}^{n-1} (-1)^ia_i \end{eqnarray} whose linear term is nonzero. For $k=2$, $n=3$ or $4$, $P$ also has nonzero linear term. If $k=2$ and $n=5$, then the original polynomial equation can be factored as \begin{eqnarray} (x^3+px^2+qx-1)(x^2+rx+1)=0 \end{eqnarray} Comparing coefficients, we have \begin{align*} a_1&=-p-r-5\\ a_2&=pr+q-9\\ a_3&=-p-qr-9\\ a_4&=q-r-5 \end{align*} According to Mathematica, $P$ is, up to a constant multiple,  \begin{eqnarray} -a_1^3+a_3 a_1^2+a_4 a_1^2+a_1^2+a_4^2 a_1-2 a_2 a_1+2 a_3 a_1-a_2 a_4 a_1-a_3 a_4 a_1-2 a_4 a_1-a_4^3+a_2^2+a_3^2+a_2 a_4^2+a_4^2-2 a_2 a_3+2 a_2 a_4-2 a_3 a_4\end{eqnarray} which does not have nonzero linear terms. For $k=3$, $n=6$, $P$ is \begin{eqnarray} a_1^3-a_4 a_1^2+3 a_1^2-4 a_2 a_1+6 a_3 a_1-12 a_4 a_1+a_3 a_5 a_1+22 a_5 a_1+a_5^3-a_3^2-a_2 a_5^2+3 a_5^2+4 a_2 a_4-12 a_2 a_5+6 a_3 a_5-4 a_4 a_5\end{eqnarray} which again does not have nonzero linear terms. My guess is that, if $k\geq 2$ and $n-k\geq$ 2, then $P$ does not have nonzero linear term, except the case $k=2$, $n=4$.","Consider the complex coefficient polynomial equation \begin{eqnarray} x^n-\left(a_1+\binom{n}{1}\right)x^{n-1}+\cdots+(-1)^k\left(a_k+\binom{n}{k}\right)x^{n-k}+\cdots+(-1)^{n-1}\left(a_{n-1}+\binom{n}{n-1}\right)x+(-1)^n=0 \end{eqnarray} By Vieta Theorem, the product of its roots is 1. If we impose the condition that, among the $n$ roots, there exist $k$ roots (counted with multiplicity) whose product is 1, then $a_1, \cdots, a_{n-1}$ have to satisfy a polynomial equation $P(a_1, \cdots, a_{n-1})=0$, where $P\in\mathbb{C}[a_1, \cdots, a_{n-1}]$ has 0 as the constant term. Question: Under what condition does $P$ has nonzero linear term? The following are some easy examples I have worked out. If $k=1$, then that means one of the roots is 1. Plugging $x=1$ to the original polynomial equation yields that $P$ can be  \begin{eqnarray} \sum_{i=1}^{n-1} (-1)^ia_i \end{eqnarray} whose linear term is nonzero. For $k=2$, $n=3$ or $4$, $P$ also has nonzero linear term. If $k=2$ and $n=5$, then the original polynomial equation can be factored as \begin{eqnarray} (x^3+px^2+qx-1)(x^2+rx+1)=0 \end{eqnarray} Comparing coefficients, we have \begin{align*} a_1&=-p-r-5\\ a_2&=pr+q-9\\ a_3&=-p-qr-9\\ a_4&=q-r-5 \end{align*} According to Mathematica, $P$ is, up to a constant multiple,  \begin{eqnarray} -a_1^3+a_3 a_1^2+a_4 a_1^2+a_1^2+a_4^2 a_1-2 a_2 a_1+2 a_3 a_1-a_2 a_4 a_1-a_3 a_4 a_1-2 a_4 a_1-a_4^3+a_2^2+a_3^2+a_2 a_4^2+a_4^2-2 a_2 a_3+2 a_2 a_4-2 a_3 a_4\end{eqnarray} which does not have nonzero linear terms. For $k=3$, $n=6$, $P$ is \begin{eqnarray} a_1^3-a_4 a_1^2+3 a_1^2-4 a_2 a_1+6 a_3 a_1-12 a_4 a_1+a_3 a_5 a_1+22 a_5 a_1+a_5^3-a_3^2-a_2 a_5^2+3 a_5^2+4 a_2 a_4-12 a_2 a_5+6 a_3 a_5-4 a_4 a_5\end{eqnarray} which again does not have nonzero linear terms. My guess is that, if $k\geq 2$ and $n-k\geq$ 2, then $P$ does not have nonzero linear term, except the case $k=2$, $n=4$.",,"['abstract-algebra', 'polynomials']"
62,Does a power-complete finite pasture exist?,Does a power-complete finite pasture exist?,,"Suppose we define a pasture to be an algebraic structure $\langle M, 0, +, \times, \wedge \rangle$ where $\langle M, 0, +, \times \rangle$ is a ring (not necessarily commutative or unital) $\wedge$ distributes over $\times$ on the left: $(a \times b) \wedge c = (a \wedge c) \times (b \wedge c)$ $\wedge$ distributes $+$ into $\times$ on the right: $a \wedge (b + c) = (a \wedge b) \times (a \wedge c)$ The idea is that a pasture is a bit like a field (in that it consists of a ring with additional structure), but goes off in a slightly different direction (by adding exponentiation instead of division). Now let's call $x \in M$ a perfect power if $x = y \wedge z$ for some $y, z \in M$. Moreover, let's say that $M$ is power-complete if all of its elements are perfect powers. For example, the trivial pasture $\{0\}$ is clearly power-complete. Question: Does a nontrivial power-complete finite pasture exist? I was inspired to ask this question after running a computer search for finite pastures and noticing that they tend to have few perfect powers. In fact, most of the pastures I found had a single perfect power, often (but not always) $0$. If my code is correct, then I have confirmed that no pasture of order $\le 8$ is power-complete, and moreover that no commutative unital pasture of order $\le 10$ is power-complete. Side note: for $2 \le n \le 10$, the number of non-isomorphic commutative unital pastures of order $n$ is given by $(2, 2, 10, 2, 4, 2, 36, 10, 4)$. This is not a sequence recognized by the OEIS. Edit: Thanks to a comment by @user60589, I have discovered a bug in my code which invalidates the above results. In fact, there are plenty of examples of power-complete pastures of order $\le 10$.","Suppose we define a pasture to be an algebraic structure $\langle M, 0, +, \times, \wedge \rangle$ where $\langle M, 0, +, \times \rangle$ is a ring (not necessarily commutative or unital) $\wedge$ distributes over $\times$ on the left: $(a \times b) \wedge c = (a \wedge c) \times (b \wedge c)$ $\wedge$ distributes $+$ into $\times$ on the right: $a \wedge (b + c) = (a \wedge b) \times (a \wedge c)$ The idea is that a pasture is a bit like a field (in that it consists of a ring with additional structure), but goes off in a slightly different direction (by adding exponentiation instead of division). Now let's call $x \in M$ a perfect power if $x = y \wedge z$ for some $y, z \in M$. Moreover, let's say that $M$ is power-complete if all of its elements are perfect powers. For example, the trivial pasture $\{0\}$ is clearly power-complete. Question: Does a nontrivial power-complete finite pasture exist? I was inspired to ask this question after running a computer search for finite pastures and noticing that they tend to have few perfect powers. In fact, most of the pastures I found had a single perfect power, often (but not always) $0$. If my code is correct, then I have confirmed that no pasture of order $\le 8$ is power-complete, and moreover that no commutative unital pasture of order $\le 10$ is power-complete. Side note: for $2 \le n \le 10$, the number of non-isomorphic commutative unital pastures of order $n$ is given by $(2, 2, 10, 2, 4, 2, 36, 10, 4)$. This is not a sequence recognized by the OEIS. Edit: Thanks to a comment by @user60589, I have discovered a bug in my code which invalidates the above results. In fact, there are plenty of examples of power-complete pastures of order $\le 10$.",,"['abstract-algebra', 'combinatorics', 'ring-theory', 'universal-algebra']"
63,"How to show $\frac{\mathbb{Z}_m\times \mathbb{Z}_n}{\langle (a,b)\rangle}\simeq \mathbb{Z}_{\frac mc}\times \mathbb{Z}_{\frac nd}$?",How to show ?,"\frac{\mathbb{Z}_m\times \mathbb{Z}_n}{\langle (a,b)\rangle}\simeq \mathbb{Z}_{\frac mc}\times \mathbb{Z}_{\frac nd}","Suppose we have the group $\mathbb{Z}_m\times\mathbb{Z}_n$ , and $(a,b)\in\mathbb{Z}_m\times\mathbb{Z}_n$ . We need to justify that (i) There exist $c, d$ such that $\langle (a,b)\rangle$ is isomorphic to the group $\mathbb{Z}_c\times \mathbb{Z}_d$ with $c\mid m, d\mid n$ . (ii) $\dfrac{\mathbb{Z}_m\times \mathbb{Z}_n}{\langle (a,b)\rangle}\simeq \mathbb{Z}_{\frac mc}\times \mathbb{Z}_{\frac nd}$ . How to show these ? The first one I tried as: Since $\langle (a,b)\rangle$ is cyclic there is $\alpha$ such that $\langle (a,b)\rangle\simeq \mathbb{Z}_\alpha$ . And then $\alpha\mid mn$ which means we can find two relatively prime $c,d$ such that $cd=\alpha, c\mid m, d\mid n$ and $\mathbb{Z}_\alpha\simeq \mathbb{Z}_c\times \mathbb{Z}_d$ . Then ?","Suppose we have the group , and . We need to justify that (i) There exist such that is isomorphic to the group with . (ii) . How to show these ? The first one I tried as: Since is cyclic there is such that . And then which means we can find two relatively prime such that and . Then ?","\mathbb{Z}_m\times\mathbb{Z}_n (a,b)\in\mathbb{Z}_m\times\mathbb{Z}_n c, d \langle (a,b)\rangle \mathbb{Z}_c\times \mathbb{Z}_d c\mid m, d\mid n \dfrac{\mathbb{Z}_m\times \mathbb{Z}_n}{\langle (a,b)\rangle}\simeq \mathbb{Z}_{\frac mc}\times \mathbb{Z}_{\frac nd} \langle (a,b)\rangle \alpha \langle (a,b)\rangle\simeq \mathbb{Z}_\alpha \alpha\mid mn c,d cd=\alpha, c\mid m, d\mid n \mathbb{Z}_\alpha\simeq \mathbb{Z}_c\times \mathbb{Z}_d","['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups', 'direct-product']"
64,Properties of group acting such that each non-trivial element fixes no point or exactly $p$ points,Properties of group acting such that each non-trivial element fixes no point or exactly  points,p,"Let $p$ be a prime and $G$ a faithful non-regular transitive finite group acting on $\Omega$ with $|\Omega| > p$ such that some element fixes no point, and that each nontrivial element fixing some point fixes exactly $p$ points. Also assume that for $g \notin N_G(G_{\alpha})$ we have    $$  G_{\alpha}^g \cap G_{\alpha} = 1 $$   and $p$ divides the order of $G_{\alpha}$. Let $\overline \alpha := \mbox{fix}(G_{\alpha})$ be the set of points fixed by $G_{\alpha}$ and assume that a Sylow $p$-subgroup $S$ of $G$ fixes $\overline \alpha$ and acts semi-regulary on $\Omega \setminus \overline \alpha$. a) Show $N_G(S) \subseteq N_G(G_{\alpha})$ and $N_G(S)' \subseteq G_{\alpha}$, b) $G$ has a normal subgroup $F$ of index $p$ [Hint: Grün's Theorem], c) Show that $F$ has $p$ orbits and acts as a Frobenious group on these orbits. Some facts I know: i) $S$ has order at least $p^2$. As $S$ acts semi-regulary on $\Omega \setminus \overline \alpha$ for each orbit $\Delta \subseteq \Omega \setminus \overline \alpha$ of $S$ we have $|\Delta| = |S : S_{\beta}| = |S|$ with $\beta \in \Delta$. Let $k$ denote the number of these orbits, and as $S$ fixes (1) $\overline \alpha$  $$  |\Omega| = |\overline \alpha| + k\cdot |S| = p + k \cdot |S| $$ and hence $p$ divides $|\Omega|$. As $G$ acts transitive $|\Omega| = |G : G_{\alpha}|$. So $p$ divides the index of $G_{\alpha}$ in $G$ and the order of $G_{\alpha}$, therefore $S$ has order at least $p^2$ as a Sylow $p$-subgroup. ii) $G_{\alpha}$ has index $p$ in its normalizer. Let $k = |N_G(G_{\alpha}) : G_{\alpha}|$. Then if $g \in N_G(G_{\alpha})$ we have $G_{\alpha} = G_{\alpha}^g = G_{\alpha^g}$, so that $G_{\alpha}$ fixes all points $\alpha^g$ with $g \in N_G(G_{\alpha})$, and the orbit of the normalizer has size $|N_G(G_{\alpha}) : G_{\alpha}|$. So at least $k$ points are fixed by $G_{\alpha}$. Further as $\alpha^g = \alpha$ implies $G_{\alpha} = G_{\alpha}^g$ exactly $k$ points are fixed by $G_{\alpha}$. By $h \in N_G(G_{\alpha}^g) \Leftrightarrow (G_{\alpha}^g)^h = G_{\alpha}^g \Leftrightarrow G_{\alpha}^{ghg^{-1}} = G_{\alpha} \Leftrightarrow ghg^{-1} \in N_G(G_{\alpha}) \Leftrightarrow h \in N_G(G_{\alpha})^g$ the normalizers of conjugates to $G_{\alpha}$ are isomorphic, hence $|N_G(G_{\alpha}^g) : G_{\alpha}^g| = k$ and we see that each conjugate of $G_{\alpha}$ fixes also exactly $k$ points. Let $g \in G$ and denote by $l$ the number of conjugates of $G_{\alpha}$ which contain $g$. By the above the number of points fixed by $g$ is precisely $kl$. But as the conjugates intersect trivially, if $g \ne 1$ and $g$ fixes some point then $l = 1$, and so $g$ fixes exactly $k$ points, i.e. the number of points fixed equals the index, and by supposition the number of points equals $p$, hence $k = p$. So this is all I got, hoping you can help me to solve the points a), b) and c)! (1) I guess this means $\overline \alpha^S = \overline \alpha$, i.e. $\overline \alpha$ is a union of orbits under $S$, and not necessarily that $S$ fixes each point from $\overline \alpha$. But note that either $S$ fixes each point from $\overline \alpha$, or if one point is not fixed, then $\overline \alpha$ must be an orbit of $S$ as $|\overline \alpha| = p$ and the size of each orbit must divide $|S|$. But maybe the case that $S \le G_{\alpha}$ could be excluded somehow. .","Let $p$ be a prime and $G$ a faithful non-regular transitive finite group acting on $\Omega$ with $|\Omega| > p$ such that some element fixes no point, and that each nontrivial element fixing some point fixes exactly $p$ points. Also assume that for $g \notin N_G(G_{\alpha})$ we have    $$  G_{\alpha}^g \cap G_{\alpha} = 1 $$   and $p$ divides the order of $G_{\alpha}$. Let $\overline \alpha := \mbox{fix}(G_{\alpha})$ be the set of points fixed by $G_{\alpha}$ and assume that a Sylow $p$-subgroup $S$ of $G$ fixes $\overline \alpha$ and acts semi-regulary on $\Omega \setminus \overline \alpha$. a) Show $N_G(S) \subseteq N_G(G_{\alpha})$ and $N_G(S)' \subseteq G_{\alpha}$, b) $G$ has a normal subgroup $F$ of index $p$ [Hint: Grün's Theorem], c) Show that $F$ has $p$ orbits and acts as a Frobenious group on these orbits. Some facts I know: i) $S$ has order at least $p^2$. As $S$ acts semi-regulary on $\Omega \setminus \overline \alpha$ for each orbit $\Delta \subseteq \Omega \setminus \overline \alpha$ of $S$ we have $|\Delta| = |S : S_{\beta}| = |S|$ with $\beta \in \Delta$. Let $k$ denote the number of these orbits, and as $S$ fixes (1) $\overline \alpha$  $$  |\Omega| = |\overline \alpha| + k\cdot |S| = p + k \cdot |S| $$ and hence $p$ divides $|\Omega|$. As $G$ acts transitive $|\Omega| = |G : G_{\alpha}|$. So $p$ divides the index of $G_{\alpha}$ in $G$ and the order of $G_{\alpha}$, therefore $S$ has order at least $p^2$ as a Sylow $p$-subgroup. ii) $G_{\alpha}$ has index $p$ in its normalizer. Let $k = |N_G(G_{\alpha}) : G_{\alpha}|$. Then if $g \in N_G(G_{\alpha})$ we have $G_{\alpha} = G_{\alpha}^g = G_{\alpha^g}$, so that $G_{\alpha}$ fixes all points $\alpha^g$ with $g \in N_G(G_{\alpha})$, and the orbit of the normalizer has size $|N_G(G_{\alpha}) : G_{\alpha}|$. So at least $k$ points are fixed by $G_{\alpha}$. Further as $\alpha^g = \alpha$ implies $G_{\alpha} = G_{\alpha}^g$ exactly $k$ points are fixed by $G_{\alpha}$. By $h \in N_G(G_{\alpha}^g) \Leftrightarrow (G_{\alpha}^g)^h = G_{\alpha}^g \Leftrightarrow G_{\alpha}^{ghg^{-1}} = G_{\alpha} \Leftrightarrow ghg^{-1} \in N_G(G_{\alpha}) \Leftrightarrow h \in N_G(G_{\alpha})^g$ the normalizers of conjugates to $G_{\alpha}$ are isomorphic, hence $|N_G(G_{\alpha}^g) : G_{\alpha}^g| = k$ and we see that each conjugate of $G_{\alpha}$ fixes also exactly $k$ points. Let $g \in G$ and denote by $l$ the number of conjugates of $G_{\alpha}$ which contain $g$. By the above the number of points fixed by $g$ is precisely $kl$. But as the conjugates intersect trivially, if $g \ne 1$ and $g$ fixes some point then $l = 1$, and so $g$ fixes exactly $k$ points, i.e. the number of points fixed equals the index, and by supposition the number of points equals $p$, hence $k = p$. So this is all I got, hoping you can help me to solve the points a), b) and c)! (1) I guess this means $\overline \alpha^S = \overline \alpha$, i.e. $\overline \alpha$ is a union of orbits under $S$, and not necessarily that $S$ fixes each point from $\overline \alpha$. But note that either $S$ fixes each point from $\overline \alpha$, or if one point is not fixed, then $\overline \alpha$ must be an orbit of $S$ as $|\overline \alpha| = p$ and the size of each orbit must divide $|S|$. But maybe the case that $S \le G_{\alpha}$ could be excluded somehow. .",,"['abstract-algebra', 'group-theory', 'finite-groups', 'permutations', 'group-actions']"
65,"Formula relating Euler characteristics $\chi(A)$, $\chi(X)$, $\chi(Y)$, $\chi(Y \cup_f X)$ when $X$ and $Y$ are finite.","Formula relating Euler characteristics , , ,  when  and  are finite.",\chi(A) \chi(X) \chi(Y) \chi(Y \cup_f X) X Y,"This is a followup to my question here . Let $A$ be the subcomplex of a CW complex $X$, let $Y$ be a CW complex, let $f: A \to Y$ be a cellular map, and let $Y \cup_f X$ be the pushout of $f$ and the inclusion $A \to X$. Is there a formula relating the Euler characteristics $\chi(A)$, $\chi(X)$, $\chi(Y)$, and $\chi(Y \cup_f X)$ when $X$ and $Y$ are finite?","This is a followup to my question here . Let $A$ be the subcomplex of a CW complex $X$, let $Y$ be a CW complex, let $f: A \to Y$ be a cellular map, and let $Y \cup_f X$ be the pushout of $f$ and the inclusion $A \to X$. Is there a formula relating the Euler characteristics $\chi(A)$, $\chi(X)$, $\chi(Y)$, and $\chi(Y \cup_f X)$ when $X$ and $Y$ are finite?",,"['abstract-algebra', 'general-topology']"
66,Natural proofs of theorems or exercises [closed],Natural proofs of theorems or exercises [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question Some mathematicians wants to hide their reasoning and proof process so that they appear smart. As most of the mathematics lovers I am not a genius and this is why I hate the magical proofs because they don't learn me anything. Do you have striking theorems or exercises you encoutered many times in your life and you never understood up until you finally got the right natural proof ?  If it's the case please feel free to share your knowledge. Even if it is an heuristic and not a proof For exemple here is a topic for natural Cauchy - Schwarz proofs : A natural proof of the Cauchy-Schwarz inequality I should start : Theorem. [PNT] If $p_n$ denotes the $n$-th prime number, then $p_n \sim n \log n$. Here is an heuristic based on the ideas of Euler. The excellent idea of Euler is to aknowledge that we have a better knowledge of the sums than the numbers. We all know that with a simple integral test we have :  $$\sum _{1 \leqslant k \leqslant n} \frac{1}{k} \sim \log n $$ The idea is then to find each integer $n$ with the primes. For the powers of a prime Euler simply wrote : $$ \frac{1}{1-\frac{1}{p}}= \sum_{ \alpha  \geqslant 0} \frac{1}{p^{\alpha}} $$  To find each $\displaystyle \frac{1}{n}$ we just have to take the product : $$ \prod_{ p\in \mathbb{P} \atop p \leqslant N} \frac{1}{1-\frac{1}{p}}= \prod_{ p\in \mathbb{P} \atop p\leqslant N} \sum_{ \alpha  \geqslant 0} \frac{1}{p^{\alpha}}$$ In the right hand side we find each $\frac{1}{n}$ pour tous les $n$ for $n$ that only involves primes such that $p \leqslant N$ so we basicly have every $1/n$ up to $n=N^2$.  Let's say : $$\prod_{ p\in \mathbb{P} \atop p \leqslant N} \frac{1}{1-\frac{1}{p}} \approx \sum_{n \leqslant N^2} \frac{1}{n} \sim \log N^2 \sim \log N$$ Taking the logarithm : $$-\sum_{p \leqslant N} \log \left( 1-\frac{1}{p} \right) \sim \log \log N $$ Since : $$\log \left( 1- \frac{1}{p}\right) \sim - \frac{1}{p}$$ by summation we have : $$ \sum_{p \leqslant N} \frac{1}{p} \sim \ln \ln N $$ A discrete derivation (but not licite still) gives us :  $$ \frac{1}{p_N}=\frac{1}{N \log N}$$ so that : $$ p_N \sim N \log N $$ it's the prime number theorem  ! It's a quite simple way of understanding the theorem I found very clear. Of course this is not a proof,","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question Some mathematicians wants to hide their reasoning and proof process so that they appear smart. As most of the mathematics lovers I am not a genius and this is why I hate the magical proofs because they don't learn me anything. Do you have striking theorems or exercises you encoutered many times in your life and you never understood up until you finally got the right natural proof ?  If it's the case please feel free to share your knowledge. Even if it is an heuristic and not a proof For exemple here is a topic for natural Cauchy - Schwarz proofs : A natural proof of the Cauchy-Schwarz inequality I should start : Theorem. [PNT] If $p_n$ denotes the $n$-th prime number, then $p_n \sim n \log n$. Here is an heuristic based on the ideas of Euler. The excellent idea of Euler is to aknowledge that we have a better knowledge of the sums than the numbers. We all know that with a simple integral test we have :  $$\sum _{1 \leqslant k \leqslant n} \frac{1}{k} \sim \log n $$ The idea is then to find each integer $n$ with the primes. For the powers of a prime Euler simply wrote : $$ \frac{1}{1-\frac{1}{p}}= \sum_{ \alpha  \geqslant 0} \frac{1}{p^{\alpha}} $$  To find each $\displaystyle \frac{1}{n}$ we just have to take the product : $$ \prod_{ p\in \mathbb{P} \atop p \leqslant N} \frac{1}{1-\frac{1}{p}}= \prod_{ p\in \mathbb{P} \atop p\leqslant N} \sum_{ \alpha  \geqslant 0} \frac{1}{p^{\alpha}}$$ In the right hand side we find each $\frac{1}{n}$ pour tous les $n$ for $n$ that only involves primes such that $p \leqslant N$ so we basicly have every $1/n$ up to $n=N^2$.  Let's say : $$\prod_{ p\in \mathbb{P} \atop p \leqslant N} \frac{1}{1-\frac{1}{p}} \approx \sum_{n \leqslant N^2} \frac{1}{n} \sim \log N^2 \sim \log N$$ Taking the logarithm : $$-\sum_{p \leqslant N} \log \left( 1-\frac{1}{p} \right) \sim \log \log N $$ Since : $$\log \left( 1- \frac{1}{p}\right) \sim - \frac{1}{p}$$ by summation we have : $$ \sum_{p \leqslant N} \frac{1}{p} \sim \ln \ln N $$ A discrete derivation (but not licite still) gives us :  $$ \frac{1}{p_N}=\frac{1}{N \log N}$$ so that : $$ p_N \sim N \log N $$ it's the prime number theorem  ! It's a quite simple way of understanding the theorem I found very clear. Of course this is not a proof,",,"['abstract-algebra', 'sequences-and-series', 'analysis', 'learning']"
67,"Let $G$ be a group, if $H=\{b\in G\ |\ bab^{-1} \in \langle a \rangle\} $ is H a subgroup of G?","Let  be a group, if  is H a subgroup of G?",G H=\{b\in G\ |\ bab^{-1} \in \langle a \rangle\} ,"I've seen this question around when $G\wedge\langle a \rangle$ are finite, but what if $G$ is infinite and $\langle a \rangle $ is finite? My approach: Show $H$ is closed, which just follows from the cyclicality of $\langle a \rangle$. In the case that $G$ is infinite I also have to show that $\forall b \in H,\ b^{-1} \in H$. I can't figure out how to show condition 2. I was hoping somone could help me out","I've seen this question around when $G\wedge\langle a \rangle$ are finite, but what if $G$ is infinite and $\langle a \rangle $ is finite? My approach: Show $H$ is closed, which just follows from the cyclicality of $\langle a \rangle$. In the case that $G$ is infinite I also have to show that $\forall b \in H,\ b^{-1} \in H$. I can't figure out how to show condition 2. I was hoping somone could help me out",,"['abstract-algebra', 'group-theory']"
68,How to show the commutator of $SO(n)$ is itself?,How to show the commutator of  is itself?,SO(n),"I am not very familiar with Lie groups, but I want to show that the commutator subgroup of $SO(3)$ is itself. I have looked up many different sources, and it seems to me that almost all of them require some notion of Lie algebra, so I am wondering if it is possible to show this without much knowledge of Lie groups.","I am not very familiar with Lie groups, but I want to show that the commutator subgroup of $SO(3)$ is itself. I have looked up many different sources, and it seems to me that almost all of them require some notion of Lie algebra, so I am wondering if it is possible to show this without much knowledge of Lie groups.",,"['abstract-algebra', 'lie-groups']"
69,Finding Ideals in $\begin{bmatrix} \mathbb{Q} & \mathbb{Q}\\ 0 & 0 \end{bmatrix}$,Finding Ideals in,\begin{bmatrix} \mathbb{Q} & \mathbb{Q}\\ 0 & 0 \end{bmatrix},"I am looking to find the left, right and two sided ideals of the ring R = $\begin{bmatrix} \mathbb{Q} & \mathbb{Q}\\ 0 & 0 \end{bmatrix}$ . It is in finding the left ideals that I am stuck. Finding the Right Ideals To find the right ideals I considered a nonzero element $\begin{bmatrix} a & b\\ 0 & 0 \end{bmatrix}$ of an arbitrary right ideal $I_{r}$ . I  then considered two cases. Case 1: $a\neq0$ We have $\begin{bmatrix} a & b\\ 0 & 0 \end{bmatrix}\begin{bmatrix} a^{-1} & 0\\ 0 & 0 \end{bmatrix}=\begin{bmatrix} 1 & 0\\ 0 & 0 \end{bmatrix}\in I_{r}$ . Hence for any $\begin{bmatrix} c & d\\ 0 & 0 \end{bmatrix} \in R$ we have: $\begin{bmatrix} c & d\\ 0 & 0 \end{bmatrix}=\begin{bmatrix} c & 0\\ 0 & 0 \end{bmatrix}\begin{bmatrix} 1 & 0\\ 0 & 0 \end{bmatrix}+\begin{bmatrix} 1 & 0\\ 0 & 0 \end{bmatrix}\begin{bmatrix} 0 & d\\ 0 & 0 \end{bmatrix} \in R$ . That is $I_{r}=R$ . Case 2: $a=0$ We have $K=\{b\in\mathbb{Q}\mid\begin{bmatrix} 0 & b\\ 0 & 0 \end{bmatrix}\in I_{r}\}$ is an additive subgroup of $\mathbb{Q}$ and $I_{r}=\begin{bmatrix} 0 & K\\ 0 & 0 \end{bmatrix}$ . Moreover one can verify that $\begin{bmatrix} 0 & K\\ 0 & 0 \end{bmatrix}$ is a right ideal of $R$ . Hence in this case any ideal is of the form $\begin{bmatrix} 0 & K\\ 0 & 0 \end{bmatrix}$ where $K$ is an additive subgroup of $\mathbb{Q}$ . Finding the Left Ideals I have tried a similar strategy as above. I considered a nonzero element $\begin{bmatrix} a' & b'\\ 0 & 0 \end{bmatrix}$ of an arbitrary left ideal $I_{l}$ . I then attempted to consider two cases. Case 1: $b'=0$ We have $K'=\{a'\in\mathbb{Q}\mid\begin{bmatrix} a' & 0\\ 0 & 0 \end{bmatrix}\in I_{r}\}$ is an additive subgroup of $\mathbb{Q}$ and $I_{l}=\begin{bmatrix} K' & 0\\ 0 & 0 \end{bmatrix}$ . Moreover one can verify that $\begin{bmatrix} K' & 0\\ 0 & 0 \end{bmatrix}$ is a leftideal of $R$ . Hence in this case any left ideal is of the form $\begin{bmatrix} K' & 0\\ 0 & 0 \end{bmatrix}$ where $K'$ is an additive subgroup of $\mathbb{Q}$ . It is for the second case that I am stuck, I have no idea how to proceed. Moreover when finding the ideals, I will simply check whether each left/right ideal I find is a right/left ideal. Overall is this the best way to have approached this question, it feels awfully long.","I am looking to find the left, right and two sided ideals of the ring R = . It is in finding the left ideals that I am stuck. Finding the Right Ideals To find the right ideals I considered a nonzero element of an arbitrary right ideal . I  then considered two cases. Case 1: We have . Hence for any we have: . That is . Case 2: We have is an additive subgroup of and . Moreover one can verify that is a right ideal of . Hence in this case any ideal is of the form where is an additive subgroup of . Finding the Left Ideals I have tried a similar strategy as above. I considered a nonzero element of an arbitrary left ideal . I then attempted to consider two cases. Case 1: We have is an additive subgroup of and . Moreover one can verify that is a leftideal of . Hence in this case any left ideal is of the form where is an additive subgroup of . It is for the second case that I am stuck, I have no idea how to proceed. Moreover when finding the ideals, I will simply check whether each left/right ideal I find is a right/left ideal. Overall is this the best way to have approached this question, it feels awfully long.",\begin{bmatrix} \mathbb{Q} & \mathbb{Q}\\ 0 & 0 \end{bmatrix} \begin{bmatrix} a & b\\ 0 & 0 \end{bmatrix} I_{r} a\neq0 \begin{bmatrix} a & b\\ 0 & 0 \end{bmatrix}\begin{bmatrix} a^{-1} & 0\\ 0 & 0 \end{bmatrix}=\begin{bmatrix} 1 & 0\\ 0 & 0 \end{bmatrix}\in I_{r} \begin{bmatrix} c & d\\ 0 & 0 \end{bmatrix} \in R \begin{bmatrix} c & d\\ 0 & 0 \end{bmatrix}=\begin{bmatrix} c & 0\\ 0 & 0 \end{bmatrix}\begin{bmatrix} 1 & 0\\ 0 & 0 \end{bmatrix}+\begin{bmatrix} 1 & 0\\ 0 & 0 \end{bmatrix}\begin{bmatrix} 0 & d\\ 0 & 0 \end{bmatrix} \in R I_{r}=R a=0 K=\{b\in\mathbb{Q}\mid\begin{bmatrix} 0 & b\\ 0 & 0 \end{bmatrix}\in I_{r}\} \mathbb{Q} I_{r}=\begin{bmatrix} 0 & K\\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & K\\ 0 & 0 \end{bmatrix} R \begin{bmatrix} 0 & K\\ 0 & 0 \end{bmatrix} K \mathbb{Q} \begin{bmatrix} a' & b'\\ 0 & 0 \end{bmatrix} I_{l} b'=0 K'=\{a'\in\mathbb{Q}\mid\begin{bmatrix} a' & 0\\ 0 & 0 \end{bmatrix}\in I_{r}\} \mathbb{Q} I_{l}=\begin{bmatrix} K' & 0\\ 0 & 0 \end{bmatrix} \begin{bmatrix} K' & 0\\ 0 & 0 \end{bmatrix} R \begin{bmatrix} K' & 0\\ 0 & 0 \end{bmatrix} K' \mathbb{Q},"['abstract-algebra', 'ring-theory', 'ideals']"
70,Show that $A[x] \cap A[x^{-1}]$ is integral over $A$. [duplicate],Show that  is integral over . [duplicate],A[x] \cap A[x^{-1}] A,"This question already has answers here : Prove that $B[x] \cap B[x^{-1}]$ is integral over $B$ (2 answers) Closed 7 years ago . Let $R$ be a commutative ring,  $A$ a subring of $R$, and $x$ a unit in $R$. Show that every $y \in A[x] \cap A[x^{-1}]$ is integral over $A$. I'm supposed to use the fact that there exists an integer $n$ such that the A-module $M = Ax +..... +Ax^{n}$ is stable under multiplication by $y$. How do I prove the existence of such an $n$ and then proceed using the claim?","This question already has answers here : Prove that $B[x] \cap B[x^{-1}]$ is integral over $B$ (2 answers) Closed 7 years ago . Let $R$ be a commutative ring,  $A$ a subring of $R$, and $x$ a unit in $R$. Show that every $y \in A[x] \cap A[x^{-1}]$ is integral over $A$. I'm supposed to use the fact that there exists an integer $n$ such that the A-module $M = Ax +..... +Ax^{n}$ is stable under multiplication by $y$. How do I prove the existence of such an $n$ and then proceed using the claim?",,"['abstract-algebra', 'commutative-algebra', 'algebraic-number-theory', 'integral-dependence']"
71,Augmented coalgebras,Augmented coalgebras,,"Let $C, \Delta$ be a coalgebra. Assume that it is coaugmented with coaugmentation $u\: : \: k\to C$ and co unit $\epsilon\: : \ C\to k$. Since $\epsilon\circ u=id $ we get $$ C=\text{Kern}(\epsilon )\oplus k $$ Set $\bar{C}:=\text{Kern}(\epsilon )$. Then $\bar{C}$ equipped with the reduced coproduct $\bar{\Delta}(x):=\Delta(x)-1\otimes x-x\otimes 1$ is a (non-coaugmented) coalgebra. Then we have a functor between the category of augmented coalgebras into the category of coalgebras. Does this functor defines an equivalences of categories? I ask this because a similar thing is true in the contest of algebras and aumented algebras. The idea is starting from a generaleal coalgebra $C',\Delta'$ choose a symbol 1 and define $C:=C'\oplus 1 k$, then define  $$ \Delta(x):=\Delta'(x)+1\otimes x+x\otimes 1 $$ Do you know some reference about that?  Does this process works in the contest of differential graded coalgebras?","Let $C, \Delta$ be a coalgebra. Assume that it is coaugmented with coaugmentation $u\: : \: k\to C$ and co unit $\epsilon\: : \ C\to k$. Since $\epsilon\circ u=id $ we get $$ C=\text{Kern}(\epsilon )\oplus k $$ Set $\bar{C}:=\text{Kern}(\epsilon )$. Then $\bar{C}$ equipped with the reduced coproduct $\bar{\Delta}(x):=\Delta(x)-1\otimes x-x\otimes 1$ is a (non-coaugmented) coalgebra. Then we have a functor between the category of augmented coalgebras into the category of coalgebras. Does this functor defines an equivalences of categories? I ask this because a similar thing is true in the contest of algebras and aumented algebras. The idea is starting from a generaleal coalgebra $C',\Delta'$ choose a symbol 1 and define $C:=C'\oplus 1 k$, then define  $$ \Delta(x):=\Delta'(x)+1\otimes x+x\otimes 1 $$ Do you know some reference about that?  Does this process works in the contest of differential graded coalgebras?",,"['abstract-algebra', 'hopf-algebras', 'coalgebras']"
72,Why is this sum well-defined?,Why is this sum well-defined?,,"Let $$S=\mathbb{N}[1/2]$$ be the set of rational numbers greater than $0$ which has a power of $2$ as its denominator. Let $R$ be any commutative ring. Let us consider $R^S,$ the infinite direct product. It is clear that we can write the elements as $\sum_{s\in S } a_s x_s$ with $a_s \in R.$ Consider the $R$-submodule $M$ which isformed by the elements $\sum a_s x_s$ which satisfy the following property: (*) For every real number $r >0,$ there is an $\epsilon >0 $ such that $a_s = 0$ if $r-\epsilon \leq s <r.$ Let now $\alpha = \sum a_s x_s$ and $\beta = \sum b_s x_s$ be two elements of $M.$ I want to show that (*) implies that for any $s \in S$ the products $$a_{s'}b_{s''}$$ with $s'+s''=s$ are almost all zero. Does anyone have a short proof of this fact?","Let $$S=\mathbb{N}[1/2]$$ be the set of rational numbers greater than $0$ which has a power of $2$ as its denominator. Let $R$ be any commutative ring. Let us consider $R^S,$ the infinite direct product. It is clear that we can write the elements as $\sum_{s\in S } a_s x_s$ with $a_s \in R.$ Consider the $R$-submodule $M$ which isformed by the elements $\sum a_s x_s$ which satisfy the following property: (*) For every real number $r >0,$ there is an $\epsilon >0 $ such that $a_s = 0$ if $r-\epsilon \leq s <r.$ Let now $\alpha = \sum a_s x_s$ and $\beta = \sum b_s x_s$ be two elements of $M.$ I want to show that (*) implies that for any $s \in S$ the products $$a_{s'}b_{s''}$$ with $s'+s''=s$ are almost all zero. Does anyone have a short proof of this fact?",,"['real-analysis', 'abstract-algebra', 'summation']"
73,Minimal free resolution of the twisted cubic,Minimal free resolution of the twisted cubic,,"This is exercise 13.15 in Harris' book ""A First Course..."". Let $X$ be the twisted cubic with ideal $I(X) = (XZ-Y^2,YW-Z^2,XZ-YW).$ Let $S(X)$ denote the homogeneous coordinate ring of $X$ and $S$ the homogeneous coordinate ring $k[X,Y,Z,W]$ of $\mathbb{P}^3$ (this is Harris' notation). The goal is to compute the Hilbert polynomial of the twisted cubic by producing a minimal free resolution of $S(X)$ . I know the polynomial is supposed to be $h_X(m) = 3m+1$ , so I know I am constructing the wrong resolution. Here is what I have: The polynomials generating the ideal of $X$ are all degree 2. We thus have an exact sequence of graded $S$ -modules $$ S(-2)^3 \to S \to S(X) \to 0$$ where $S(d)$ denotes the twist of $S$ by the integer $d$ . We know the kernel of the first map is generated by the relations that $XZ-Y^2$ , $YW-Z^2$ , $XW-YZ$ satisfy. I think I am having trouble producing these relations. For convenience put $$F_1 = XZ-Y^2 \qquad F_2=YW-Z^2 \qquad F_3 = XZ-YW.$$ The only relations I can think of are the tautological ones $F_iF_j = F_jF_i$ , i.e. the matrices $A=(F_2,-F_1,0)$ , $B=(F_3,0,-F_1)$ , $C=(0,F_3,-F2)$ , thought of as maps $S^3 \to S^3$ . Following Harris, this would give the exact sequence $$ S(-4)^3 \to S(-2)^3 \to S \to S(X) \to 0.$$ Then the matrices $A$ , $B$ , and $C$ satisfy the relation $-F_3A + F_2B = F_1C$ , i.e. the matrix $(-F_3,F_2,F_1)$ . This is the only relation I can think of, so I guess the kernel is a free module, so we have the exact sequence $$0 \to S(-6) \to S(-4)^3 \to S(-2)^3 \to S \to S(X) \to 0.$$ By p. 170 in Harris we should have $$h_X(m) = {m+3 \choose 3} - 3 {m+3-2 \choose 3} + 3 {m+3-4 \choose 3} - {m+3-6 \choose 3} = 8.$$ This is obviously wrong, so my free resolution is wrong.","This is exercise 13.15 in Harris' book ""A First Course..."". Let be the twisted cubic with ideal Let denote the homogeneous coordinate ring of and the homogeneous coordinate ring of (this is Harris' notation). The goal is to compute the Hilbert polynomial of the twisted cubic by producing a minimal free resolution of . I know the polynomial is supposed to be , so I know I am constructing the wrong resolution. Here is what I have: The polynomials generating the ideal of are all degree 2. We thus have an exact sequence of graded -modules where denotes the twist of by the integer . We know the kernel of the first map is generated by the relations that , , satisfy. I think I am having trouble producing these relations. For convenience put The only relations I can think of are the tautological ones , i.e. the matrices , , , thought of as maps . Following Harris, this would give the exact sequence Then the matrices , , and satisfy the relation , i.e. the matrix . This is the only relation I can think of, so I guess the kernel is a free module, so we have the exact sequence By p. 170 in Harris we should have This is obviously wrong, so my free resolution is wrong.","X I(X) = (XZ-Y^2,YW-Z^2,XZ-YW). S(X) X S k[X,Y,Z,W] \mathbb{P}^3 S(X) h_X(m) = 3m+1 X S  S(-2)^3 \to S \to S(X) \to 0 S(d) S d XZ-Y^2 YW-Z^2 XW-YZ F_1 = XZ-Y^2 \qquad F_2=YW-Z^2 \qquad F_3 = XZ-YW. F_iF_j = F_jF_i A=(F_2,-F_1,0) B=(F_3,0,-F_1) C=(0,F_3,-F2) S^3 \to S^3  S(-4)^3 \to S(-2)^3 \to S \to S(X) \to 0. A B C -F_3A + F_2B = F_1C (-F_3,F_2,F_1) 0 \to S(-6) \to S(-4)^3 \to S(-2)^3 \to S \to S(X) \to 0. h_X(m) = {m+3 \choose 3} - 3 {m+3-2 \choose 3} + 3 {m+3-4 \choose 3} - {m+3-6 \choose 3} = 8.","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'graded-modules']"
74,Is the ring of Laurent polynomials in $n$ noncommuting variables Noetherian?,Is the ring of Laurent polynomials in  noncommuting variables Noetherian?,n,"Suppose we have a Noetherian ring $R$. Is it true that the ring of Laurent polynomials $R\langle x_1,\,x_1^{-1},\ldots,\,x_n,\,x_n^{-1}\rangle$ in $n$ noncommuting variables is also Noetherian? If so, why?","Suppose we have a Noetherian ring $R$. Is it true that the ring of Laurent polynomials $R\langle x_1,\,x_1^{-1},\ldots,\,x_n,\,x_n^{-1}\rangle$ in $n$ noncommuting variables is also Noetherian? If so, why?",,"['abstract-algebra', 'ring-theory', 'noetherian']"
75,"Dimension of the affine variety associated to $\langle zw-y^2, xy-z^3 \rangle $",Dimension of the affine variety associated to,"\langle zw-y^2, xy-z^3 \rangle ","Find the dimension of the affine variety $V(I)$, where $I=\left\langle zw-y^2,xy-z^3\right\rangle \subseteq k[x,y,z,w]$, with $k$  algebraicaly closed field. I tried to solve the system $zw-y^2=0$, $xy-z^3=0$ and find the maximal dimension of coordinate subspaces, but I did not succeed. Could anyone help me? Thanks!","Find the dimension of the affine variety $V(I)$, where $I=\left\langle zw-y^2,xy-z^3\right\rangle \subseteq k[x,y,z,w]$, with $k$  algebraicaly closed field. I tried to solve the system $zw-y^2=0$, $xy-z^3=0$ and find the maximal dimension of coordinate subspaces, but I did not succeed. Could anyone help me? Thanks!",,"['abstract-algebra', 'algebraic-geometry', 'ring-theory', 'commutative-algebra']"
76,Smallest Two-Sided Nearrings,Smallest Two-Sided Nearrings,,"For those who are unfamilar with nearrings, here is a definition.  Note that there are left-nearrings (where only the left distribution property is assumed), and right-nearrings (where only the right distribution property is assumed) as well.  I only consider two-sided nearings. Definition. A two-sided nearring is a triplet $(N,+,\cdot)$ , where $(N,+)$ is a group and $(N,\cdot)$ is a semigroup such that we have both left and right distributive properties of the multiplication $\cdot$ over the addition $+$ , namely, $$x\cdot(y+z)=(x\cdot y)+(x\cdot z) \text{ and }(x+y)\cdot z=(x\cdot z)+(y\cdot z)$$ for $x,y,z\in N$ .  Of course, every additive group $(G,+)$ with identity $0_G$ can be made a two-sided nearring with the trivial multplication $g\cdot h:=0_G$ for all $g,h\in G$ .  Such a nearring is called a trivial two-sided nearring. My question is about nontrivial two-sided nearrings which are not rings (i.e., the addition $+$ is not commutative).  I know one which has $128$ elements: $N:=(\mathbb{Z}/8\mathbb{Z})\times (2\mathbb{Z}/8\mathbb{Z})\times (2\mathbb{Z}/8\mathbb{Z})$ , where $$\left(a_1,b_1,c_1\right)+\left(a_2,b_2,c_2\right):=\left(a_1+a_2+c_1b_2,b_1+b_2,c_1+c_2\right)$$ and $\left(a_1,b_1,c_1\right)\cdot\left(a_2,b_2,c_2\right)$ is given by $$\left(a_1b_2+a_2b_1+a_1c_2+a_2c_1+b_1b_2+b_2c_1+c_1c_2,b_1b_2+b_1c_2+b_2c_1+c_1c_2,0\right)\,,$$ for all $a_1,a_2\in\mathbb{Z}/8\mathbb{Z}$ , and $b_1,b_2,c_1,c_2\in2\mathbb{Z}/8\mathbb{Z}$ . Old Question. Can you find a nontrivial two-sided nearring which is not a ring with the minimum number of elements ?  This question has been answered here . The example seen in Eran's answer in the link above is a two-sided nearring which is not a ring with the smallest number of elements.  While the addition of this example is noncommutative, the multiplication is commutative.  Therefore, I am offering a bounty price for anybody who answers the question below. Bounty Question. What is a two-sided nearring with the minimum number of elements which is not a ring and whose multiplication is also noncommutative?  Please also prove that your example has the smallest number of elements amongst all two-sided nearing whose multiplication and addition are noncommutative. A Remark (which may or may not be helpful). If $(N,+,\cdot)$ is a two-sided nearing, then the subnearring $N^{\cdot 2}$ of $N$ generated by elements of the form $a\cdot b$ , where $a,b\in N$ , is a ring.  That is, $N^{\cdot 2}$ consists of all integer combinations of $a\cdot b$ , where $a,b\in N$ .  To show this, let $a,b,c,d\in N$ .  Then, we have $$(a+b)\cdot (c+d)= a\cdot(c+d)+b\cdot(c+d)=(a\cdot c+a\cdot d)+(b\cdot c+b\cdot d)$$ and $$(a+b)\cdot(c+d)=(a+b)\cdot c+(a+b)\cdot d=(a\cdot c+b\cdot c)+(a\cdot d+b\cdot d)\,,$$ whence $$a\cdot c+a\cdot d+b\cdot c+b\cdot d=a\cdot c+b\cdot c+a\cdot d+b\cdot d\,,$$ making $$a\cdot d+b\cdot c=b\cdot c+a\cdot d\,.$$ In particular, if $N$ is a two-sided nearring which is not a ring, then $N$ cannot have a multiplicative identity (otherwise, $N^{\cdot 2}=N$ , making $N$ a ring).","For those who are unfamilar with nearrings, here is a definition.  Note that there are left-nearrings (where only the left distribution property is assumed), and right-nearrings (where only the right distribution property is assumed) as well.  I only consider two-sided nearings. Definition. A two-sided nearring is a triplet , where is a group and is a semigroup such that we have both left and right distributive properties of the multiplication over the addition , namely, for .  Of course, every additive group with identity can be made a two-sided nearring with the trivial multplication for all .  Such a nearring is called a trivial two-sided nearring. My question is about nontrivial two-sided nearrings which are not rings (i.e., the addition is not commutative).  I know one which has elements: , where and is given by for all , and . Old Question. Can you find a nontrivial two-sided nearring which is not a ring with the minimum number of elements ?  This question has been answered here . The example seen in Eran's answer in the link above is a two-sided nearring which is not a ring with the smallest number of elements.  While the addition of this example is noncommutative, the multiplication is commutative.  Therefore, I am offering a bounty price for anybody who answers the question below. Bounty Question. What is a two-sided nearring with the minimum number of elements which is not a ring and whose multiplication is also noncommutative?  Please also prove that your example has the smallest number of elements amongst all two-sided nearing whose multiplication and addition are noncommutative. A Remark (which may or may not be helpful). If is a two-sided nearing, then the subnearring of generated by elements of the form , where , is a ring.  That is, consists of all integer combinations of , where .  To show this, let .  Then, we have and whence making In particular, if is a two-sided nearring which is not a ring, then cannot have a multiplicative identity (otherwise, , making a ring).","(N,+,\cdot) (N,+) (N,\cdot) \cdot + x\cdot(y+z)=(x\cdot y)+(x\cdot z) \text{ and }(x+y)\cdot z=(x\cdot z)+(y\cdot z) x,y,z\in N (G,+) 0_G g\cdot h:=0_G g,h\in G + 128 N:=(\mathbb{Z}/8\mathbb{Z})\times (2\mathbb{Z}/8\mathbb{Z})\times (2\mathbb{Z}/8\mathbb{Z}) \left(a_1,b_1,c_1\right)+\left(a_2,b_2,c_2\right):=\left(a_1+a_2+c_1b_2,b_1+b_2,c_1+c_2\right) \left(a_1,b_1,c_1\right)\cdot\left(a_2,b_2,c_2\right) \left(a_1b_2+a_2b_1+a_1c_2+a_2c_1+b_1b_2+b_2c_1+c_1c_2,b_1b_2+b_1c_2+b_2c_1+c_1c_2,0\right)\,, a_1,a_2\in\mathbb{Z}/8\mathbb{Z} b_1,b_2,c_1,c_2\in2\mathbb{Z}/8\mathbb{Z} (N,+,\cdot) N^{\cdot 2} N a\cdot b a,b\in N N^{\cdot 2} a\cdot b a,b\in N a,b,c,d\in N (a+b)\cdot (c+d)= a\cdot(c+d)+b\cdot(c+d)=(a\cdot c+a\cdot d)+(b\cdot c+b\cdot d) (a+b)\cdot(c+d)=(a+b)\cdot c+(a+b)\cdot d=(a\cdot c+b\cdot c)+(a\cdot d+b\cdot d)\,, a\cdot c+a\cdot d+b\cdot c+b\cdot d=a\cdot c+b\cdot c+a\cdot d+b\cdot d\,, a\cdot d+b\cdot c=b\cdot c+a\cdot d\,. N N N^{\cdot 2}=N N","['abstract-algebra', 'group-theory', 'ring-theory', 'finite-groups', 'finite-rings']"
77,Identifying semidirect products-groups of order 28,Identifying semidirect products-groups of order 28,,"So, I'm very new to working with semi-direct products.  I'm working on my algebra qual prep, and one of the questions was to identify all the groups of order 28.  I'm pretty sure I have the answer...the only problem is I'm not sure just what two of the groups look like! So,  starting with the sylow subgroups, I have that $n_2|7$ and $2|(n_2-1)$,  so we have $n_2=1$ or $n_2=7$.  Similarly, $n_7|4$ and $7|(n_7-1)$, so $n_7=1$, thus the Sylow 7 subgroup is unique, and therefore normal,  hence our group G is a semidirect product of its Sylow 2 subgroup and its sylow 7 subgroup. The two possibilities for the Sylow 2 subgroup are $\mathbb Z _2 \times \mathbb Z _2$ or $\mathbb Z_4$.  Neither are eliminated by a counting argument.  So, I'm looking for homomorphisms from the sylow 2 subgroup to the automorphism group of the sylow 7 subgroup, which is isomorphic to $\mathbb Z_6$ Case 1:  The sylow 2 subgroup is $\mathbb Z_2 \times \mathbb Z_2$.  The generators are $(0,1)$ and $(1,0)$.  These each have order 2, so the images must have order $1$ or $2$. So the images are either $0$ or $3$.  If the images of both are $0$, we have the trivial homomorphism, so this is actually a direct product and we have the abelian group $\mathbb Z_2 \times \mathbb Z_2 \times \mathbb Z_7$ Now, say $(1,0)$ goes to $3$ and $(0,1)$ goes to $0$.  Then $(1,1)$ goes to 3.  Similarly, we have as a case $(0,1)\to 3,(1,0)\to 0,(1,1)\to 3$ and $(1,0)\to 3,(0,1)\to 3,(1,1)\to 0$.  All of these are identical up to isomorphism as they send 2 indistinguishable elements to the same thing and the other to the identity, so up to isomorphism there is only one such semidirect product... Here's where I'm stuck, I don't know what this group is, what it looks like!  How do I figure that out? Case 2: The sylow 2 subgroup is $\mathbb Z_4$. Here we just have one generator, $1$, and it can go to either $0$ or $3$ as above.  If $0$ we have the abelian case.  What does the nonabelian case look like?","So, I'm very new to working with semi-direct products.  I'm working on my algebra qual prep, and one of the questions was to identify all the groups of order 28.  I'm pretty sure I have the answer...the only problem is I'm not sure just what two of the groups look like! So,  starting with the sylow subgroups, I have that $n_2|7$ and $2|(n_2-1)$,  so we have $n_2=1$ or $n_2=7$.  Similarly, $n_7|4$ and $7|(n_7-1)$, so $n_7=1$, thus the Sylow 7 subgroup is unique, and therefore normal,  hence our group G is a semidirect product of its Sylow 2 subgroup and its sylow 7 subgroup. The two possibilities for the Sylow 2 subgroup are $\mathbb Z _2 \times \mathbb Z _2$ or $\mathbb Z_4$.  Neither are eliminated by a counting argument.  So, I'm looking for homomorphisms from the sylow 2 subgroup to the automorphism group of the sylow 7 subgroup, which is isomorphic to $\mathbb Z_6$ Case 1:  The sylow 2 subgroup is $\mathbb Z_2 \times \mathbb Z_2$.  The generators are $(0,1)$ and $(1,0)$.  These each have order 2, so the images must have order $1$ or $2$. So the images are either $0$ or $3$.  If the images of both are $0$, we have the trivial homomorphism, so this is actually a direct product and we have the abelian group $\mathbb Z_2 \times \mathbb Z_2 \times \mathbb Z_7$ Now, say $(1,0)$ goes to $3$ and $(0,1)$ goes to $0$.  Then $(1,1)$ goes to 3.  Similarly, we have as a case $(0,1)\to 3,(1,0)\to 0,(1,1)\to 3$ and $(1,0)\to 3,(0,1)\to 3,(1,1)\to 0$.  All of these are identical up to isomorphism as they send 2 indistinguishable elements to the same thing and the other to the identity, so up to isomorphism there is only one such semidirect product... Here's where I'm stuck, I don't know what this group is, what it looks like!  How do I figure that out? Case 2: The sylow 2 subgroup is $\mathbb Z_4$. Here we just have one generator, $1$, and it can go to either $0$ or $3$ as above.  If $0$ we have the abelian case.  What does the nonabelian case look like?",,"['abstract-algebra', 'sylow-theory', 'semidirect-product']"
78,"Nonsingular curve $C$ of degree 4, exists rational function $f: C \to \mathbb{CP}^1$ of degree 2?","Nonsingular curve  of degree 4, exists rational function  of degree 2?",C f: C \to \mathbb{CP}^1,Suppose $C \subset \mathbb{CP}^2$ is a nonsingular curve of degree $4$. Does there exist a rational function $f: C \to \mathbb{CP}^1$ of degree $2$?,Suppose $C \subset \mathbb{CP}^2$ is a nonsingular curve of degree $4$. Does there exist a rational function $f: C \to \mathbb{CP}^1$ of degree $2$?,,"['abstract-algebra', 'geometry', 'algebraic-geometry', 'commutative-algebra', 'algebraic-curves']"
79,Looking for a non trivial homomorphism I,Looking for a non trivial homomorphism I,,"Is there a non trivial homomorphism $f: SU(2) \to O(2)$? Is there a concrete description of $Hom(SU(2), O(2))$?","Is there a non trivial homomorphism $f: SU(2) \to O(2)$? Is there a concrete description of $Hom(SU(2), O(2))$?",,"['abstract-algebra', 'lie-groups']"
80,Determining if $\mathbb{Z}[a]$ is a discrete subring of $\mathbb{C}$.,Determining if  is a discrete subring of .,\mathbb{Z}[a] \mathbb{C},"Let $a \in \mathbb{C}$ and consider the ring $\mathbb{Z}[a]$ . Is there some nice criterion which will tell me whether $\mathbb{Z}[a]$ is discrete in the sense that there is some $\delta >0$ such that, whenever $x \in \mathbb{Z}[a] \setminus \{0\}$ , one has $|x| \geq  \delta$ ?  It follows, then, that there is a smallest possible distance between any two points. I have been able to show some pretty easy things, such as: Fact 1 : A subring $R \subset \mathbb{C}$ is discrete if and only if $\{ |x| : x \in R\} \subset [0,\infty)$ is order isomorphic to $\mathbb{N}$ . Fact 2: The only discrete (unital) subring of $\mathbb{R}$ is $\mathbb{Z}$ . Fact 2 makes the case $a \in \mathbb{R}$ pretty easy: we get $\mathbb{Z}[a]$ discrete if and only if $a \in \mathbb{Z}$ . The case where $a \in \mathbb{C}$ is pure imaginary is also pretty easy. You need $\mathbb{Z}[a^2] \subset \mathbb{Z}[a]$ to be discrete, so you need $a^2$ an integer, i.e. $a = \pm \sqrt{n}i$ for some positive integer $n$ .  It's not hard to show that, for such $a$ , the ring $\mathbb{Z}[a]$ is also discrete. The next case I tried was the case where $a$ is a root of unity. If $a \neq 1$ is a third root of unity, things work out quite nicely: it turns out that $\mathbb{Z}[a]$ is a hexagonal lattice. However, if $a \neq 1$ is a  fifth root of unity, it gets difficult to compute by bare hands, and I am not too sure what happens.","Let and consider the ring . Is there some nice criterion which will tell me whether is discrete in the sense that there is some such that, whenever , one has ?  It follows, then, that there is a smallest possible distance between any two points. I have been able to show some pretty easy things, such as: Fact 1 : A subring is discrete if and only if is order isomorphic to . Fact 2: The only discrete (unital) subring of is . Fact 2 makes the case pretty easy: we get discrete if and only if . The case where is pure imaginary is also pretty easy. You need to be discrete, so you need an integer, i.e. for some positive integer .  It's not hard to show that, for such , the ring is also discrete. The next case I tried was the case where is a root of unity. If is a third root of unity, things work out quite nicely: it turns out that is a hexagonal lattice. However, if is a  fifth root of unity, it gets difficult to compute by bare hands, and I am not too sure what happens.","a \in \mathbb{C} \mathbb{Z}[a] \mathbb{Z}[a] \delta >0 x \in \mathbb{Z}[a] \setminus \{0\} |x| \geq  \delta R \subset \mathbb{C} \{ |x| : x \in R\} \subset [0,\infty) \mathbb{N} \mathbb{R} \mathbb{Z} a \in \mathbb{R} \mathbb{Z}[a] a \in \mathbb{Z} a \in \mathbb{C} \mathbb{Z}[a^2] \subset \mathbb{Z}[a] a^2 a = \pm \sqrt{n}i n a \mathbb{Z}[a] a a \neq 1 \mathbb{Z}[a] a \neq 1","['abstract-algebra', 'ring-theory', 'integer-lattices']"
81,$CL(O_S) \cong \mathbb{Z}/3\mathbb{Z}$.,.,CL(O_S) \cong \mathbb{Z}/3\mathbb{Z},"Let $F = \mathbb{Q}(T)$ and let $X$ be the set of all places of $F$, and let $S = \{w\} \subset X$ where $w$ is the place of $F$ corresponding to the maximal ideal $(T^3 - 2)$ of $\mathbb{Q}[T]$. Let $$O_S = \{f \in F: \text{ord}_v(f) \ge 0 \text{ for all }v \in X \setminus S\}$$$$= \{f(T)/(T^3 - 2)^n : n \ge 0,\text{ }f(T) \in \mathbb{Q}[T],\text{ deg}(f(T)) \le 3n\}.$$ How do I show that $\text{Cl}(O_s) \cong \mathbb{Z}/3\mathbb{Z}$?","Let $F = \mathbb{Q}(T)$ and let $X$ be the set of all places of $F$, and let $S = \{w\} \subset X$ where $w$ is the place of $F$ corresponding to the maximal ideal $(T^3 - 2)$ of $\mathbb{Q}[T]$. Let $$O_S = \{f \in F: \text{ord}_v(f) \ge 0 \text{ for all }v \in X \setminus S\}$$$$= \{f(T)/(T^3 - 2)^n : n \ge 0,\text{ }f(T) \in \mathbb{Q}[T],\text{ deg}(f(T)) \le 3n\}.$$ How do I show that $\text{Cl}(O_s) \cong \mathbb{Z}/3\mathbb{Z}$?",,"['abstract-algebra', 'number-theory']"
82,A question about the automorphisms of the alternating group $A_n$.,A question about the automorphisms of the alternating group .,A_n,"For $n\ge 3$. Let $A_n$ be the alternating group of degree $n$ acting on $\{1,2,\ldots,n\}$, and let $H_i$ be the isotropy subgroup of $i\in J_n$.  If $\phi:A_n\to A_n$ is automorphism, show that $\phi$ is induced by an inner automorphism of $S_n$ if and only if $\phi(H_1)=H_i$ for some $1\le i\le n$. Showing that $\phi$ induced by an inner automorphism of $S_n$ implies $\phi(H_1)=H_i$ is easy, but I'm stuck on the other direction. I've thought about looking at how the automorphisms act on $3$-cycles, since they generate $A_n$, but this seems messy and inefficient.","For $n\ge 3$. Let $A_n$ be the alternating group of degree $n$ acting on $\{1,2,\ldots,n\}$, and let $H_i$ be the isotropy subgroup of $i\in J_n$.  If $\phi:A_n\to A_n$ is automorphism, show that $\phi$ is induced by an inner automorphism of $S_n$ if and only if $\phi(H_1)=H_i$ for some $1\le i\le n$. Showing that $\phi$ induced by an inner automorphism of $S_n$ implies $\phi(H_1)=H_i$ is easy, but I'm stuck on the other direction. I've thought about looking at how the automorphisms act on $3$-cycles, since they generate $A_n$, but this seems messy and inefficient.",,"['abstract-algebra', 'group-theory']"
83,"If $R/I \times R/J$ is isomorphic to $R/(I\cap J)$ as $ R $-modules, then $I + J = R$. [duplicate]","If  is isomorphic to  as -modules, then . [duplicate]",R/I \times R/J R/(I\cap J)  R  I + J = R,"This question already has answers here : Converse to Chinese Remainder Theorem (2 answers) Closed 9 years ago . If $R$ is a commutative ring with identity and $I$ and $J$ are ideals of $R$ such that $R/I \times R/J$ is isomorphic to $R/(I\cap J)$ as $R$-modules, then $I + J = R$. I know this is the converse of CRT and there are many posts on this. However, I can't find a post that provides a complete answer to  this. This is the solution I found. However, I find it to be very complicated. What does $\otimes_R$ mean? What does tensor product mean? I have not learn tensor product. I prefer a solution without it and using only concepts from elementary ring theory course. Is there simpler answer to this?","This question already has answers here : Converse to Chinese Remainder Theorem (2 answers) Closed 9 years ago . If $R$ is a commutative ring with identity and $I$ and $J$ are ideals of $R$ such that $R/I \times R/J$ is isomorphic to $R/(I\cap J)$ as $R$-modules, then $I + J = R$. I know this is the converse of CRT and there are many posts on this. However, I can't find a post that provides a complete answer to  this. This is the solution I found. However, I find it to be very complicated. What does $\otimes_R$ mean? What does tensor product mean? I have not learn tensor product. I prefer a solution without it and using only concepts from elementary ring theory course. Is there simpler answer to this?",,"['abstract-algebra', 'ring-theory', 'ideals', 'chinese-remainder-theorem']"
84,"Prove if a non-trivial ring $R$ has a unique maximal left ideal $J$ , then $J$ is two-sided and is also the unique maximal right ideal in $R$.","Prove if a non-trivial ring  has a unique maximal left ideal  , then  is two-sided and is also the unique maximal right ideal in .",R J J R,"If a non-trivial ring $R$ has a unique maximal left ideal $J$ , then $J$ is two-sided and is also the unique maximal right ideal in $R$. I can prove that it is two sided, but I can't prove that it is unique. My proof: Let $r \in R$. Then $Jr$ is a left ideal. If $Jr = R$, then $jr = 1$ for some $j \in J$. Note that $rj \in J$ since J is a left ideal, so $rj \neq 1$. Using a lemma, $1 - rj$ is not left invertible, which is to say that $R(1-rj) \neq R$. But then R(1-rj) is contained in a maximal left ideal, i.e. $1-rj \in R(1-rj) \subseteq J$, so $1= rj +(1-rj) \in J$, which gives a contradiction. Hence $Jr \neq R$. This implies that $Jr$ is contained in $J$. Since the choice of $r$ is arbitrary, $jr \in J$ for all $j \in J$ and $r \in R$. Hence, $J$ is a two sided ideal. Can someone tell me how I can prove uniqueness?","If a non-trivial ring $R$ has a unique maximal left ideal $J$ , then $J$ is two-sided and is also the unique maximal right ideal in $R$. I can prove that it is two sided, but I can't prove that it is unique. My proof: Let $r \in R$. Then $Jr$ is a left ideal. If $Jr = R$, then $jr = 1$ for some $j \in J$. Note that $rj \in J$ since J is a left ideal, so $rj \neq 1$. Using a lemma, $1 - rj$ is not left invertible, which is to say that $R(1-rj) \neq R$. But then R(1-rj) is contained in a maximal left ideal, i.e. $1-rj \in R(1-rj) \subseteq J$, so $1= rj +(1-rj) \in J$, which gives a contradiction. Hence $Jr \neq R$. This implies that $Jr$ is contained in $J$. Since the choice of $r$ is arbitrary, $jr \in J$ for all $j \in J$ and $r \in R$. Hence, $J$ is a two sided ideal. Can someone tell me how I can prove uniqueness?",,"['abstract-algebra', 'ring-theory', 'ideals', 'noncommutative-algebra']"
85,Ideals Generated by polynomials,Ideals Generated by polynomials,,"So I am currently studying a course in commutative algebra and the main object that we are looking at are ideals generated by polynomials in n variables. But the one thing I don't understand when working with these ideals is when we reduce the generating set to something much simpler. For e.g. Consider the Ideal   $I$ = $<x^2-4x + 3, x^2 +x -2>$, then since $x -1$ is a common  factor of both the polynomials in the generating set we deduce that I is infact $<x-1>$. So my question is what is the criteria that applies when we are reducing the generating set to something much simpler. Based on what I understand, I am guessing in the above example that since every polynomial is divisible by $x-1$ we can say the ideal is generated by $x-1$ (wouldn't this result in the loss of any elements?). But I am not entirely convinced by my reasoning and would prefer to hear it from someone who understands this stuff better. Also using the same reasoning as above can we then say that the ideal $I$ = $<x^3 - x^2 + x>$ = $<x>$ ?","So I am currently studying a course in commutative algebra and the main object that we are looking at are ideals generated by polynomials in n variables. But the one thing I don't understand when working with these ideals is when we reduce the generating set to something much simpler. For e.g. Consider the Ideal   $I$ = $<x^2-4x + 3, x^2 +x -2>$, then since $x -1$ is a common  factor of both the polynomials in the generating set we deduce that I is infact $<x-1>$. So my question is what is the criteria that applies when we are reducing the generating set to something much simpler. Based on what I understand, I am guessing in the above example that since every polynomial is divisible by $x-1$ we can say the ideal is generated by $x-1$ (wouldn't this result in the loss of any elements?). But I am not entirely convinced by my reasoning and would prefer to hear it from someone who understands this stuff better. Also using the same reasoning as above can we then say that the ideal $I$ = $<x^3 - x^2 + x>$ = $<x>$ ?",,"['abstract-algebra', 'polynomials', 'ideals']"
86,Quadratic Integers in $\mathbb Q[\sqrt{-5}]$,Quadratic Integers in,\mathbb Q[\sqrt{-5}],"Can someone tell me if $\frac{3}{5}$, $2+3\sqrt{-5}$, $\frac{3+8\sqrt{-5}}{2}$, $\frac{3+8\sqrt{-5}}{5}$, $i\sqrt{-5}$ are all quadratic integers in $\mathbb Q[\sqrt{-5}]$. And if so why are they in $\mathbb Q[\sqrt{-5}]$.","Can someone tell me if $\frac{3}{5}$, $2+3\sqrt{-5}$, $\frac{3+8\sqrt{-5}}{2}$, $\frac{3+8\sqrt{-5}}{5}$, $i\sqrt{-5}$ are all quadratic integers in $\mathbb Q[\sqrt{-5}]$. And if so why are they in $\mathbb Q[\sqrt{-5}]$.",,"['abstract-algebra', 'number-theory', 'ring-theory', 'algebraic-number-theory']"
87,monoids of injections and surjections,monoids of injections and surjections,,"Let $X$ be an infinite set, and let $I(X)$ and $S(X)$ denote the monoids of injective and surjective maps from $X$ to itself, respectively.  How do $I(X)$ and $S(X)$ relate algebraically?  Is there any reason to suspect that they may be isomorphic aside from optimism?","Let $X$ be an infinite set, and let $I(X)$ and $S(X)$ denote the monoids of injective and surjective maps from $X$ to itself, respectively.  How do $I(X)$ and $S(X)$ relate algebraically?  Is there any reason to suspect that they may be isomorphic aside from optimism?",,"['abstract-algebra', 'elementary-set-theory', 'monoid']"
88,Show that there are only finitely many subgroups of $F$ in which $H$ can be of finite index.,Show that there are only finitely many subgroups of  in which  can be of finite index.,F H,"Result - Let $H$ be a finitely generated subgroup of the free group $F$. Show that there are only finitely many subgroups of $F$ in which $H$ can be of finite index. I encountered a similar result in Group theory Book by Bogopolski (pg 120), B- ""The number of subgroups of a finite index n in a finitely generated group is finite."" Does the Bogopolski result ( B ) also implies Result , the way Bogopolski proved B does not help in proving Result . So what should be the approach to prove Result .","Result - Let $H$ be a finitely generated subgroup of the free group $F$. Show that there are only finitely many subgroups of $F$ in which $H$ can be of finite index. I encountered a similar result in Group theory Book by Bogopolski (pg 120), B- ""The number of subgroups of a finite index n in a finitely generated group is finite."" Does the Bogopolski result ( B ) also implies Result , the way Bogopolski proved B does not help in proving Result . So what should be the approach to prove Result .",,"['abstract-algebra', 'group-theory']"
89,Is this restatement of Gauss's lemma correct?,Is this restatement of Gauss's lemma correct?,,"This seems extremely trivial but I want to make really sure so I'm posting it. This is a yes or no question.. (Sorry for posting this kind of question but I really wonder if I think wrong) Here is a Gauss's lemma (Dummit&Foote version) Let $R$ be a UFD and $F$ be the field of quotients of $R$ . Let $A,B\in F[X]$ be nonconstant polynomials such that $AB\in R[X]$ . Then, there are nonzero elements $r,s\in F$ such that $rA, sB$ both lie in $R[X]$ and $AB=(rA)(sB)$ . Is this statement equivalent to the below statement? Let $f,g\in F[X]$ be nonzero polynomials such that $fg\in R[X]$ . Then, there exists $r\in F\setminus\{0\}$ such that $rf$ and $\frac{1}{r} g$ both lie in $R[X]$ . (I'm asking this since I don't get why Dummit&Foote wrote two symbols $r,s$ rather than $r,\frac{1}{r}$ )","This seems extremely trivial but I want to make really sure so I'm posting it. This is a yes or no question.. (Sorry for posting this kind of question but I really wonder if I think wrong) Here is a Gauss's lemma (Dummit&Foote version) Let be a UFD and be the field of quotients of . Let be nonconstant polynomials such that . Then, there are nonzero elements such that both lie in and . Is this statement equivalent to the below statement? Let be nonzero polynomials such that . Then, there exists such that and both lie in . (I'm asking this since I don't get why Dummit&Foote wrote two symbols rather than )","R F R A,B\in F[X] AB\in R[X] r,s\in F rA, sB R[X] AB=(rA)(sB) f,g\in F[X] fg\in R[X] r\in F\setminus\{0\} rf \frac{1}{r} g R[X] r,s r,\frac{1}{r}",['abstract-algebra']
90,"If $I$ is a homogeneous ideal of $A$ contained in $A_+$, then $\sqrt{I} = \bigcap\limits_{I\subset P\in\text{Proj }A} P$?","If  is a homogeneous ideal of  contained in , then ?",I A A_+ \sqrt{I} = \bigcap\limits_{I\subset P\in\text{Proj }A} P,"EDIT: This is from an exercise of Vakil's Foundations of Algebraic Geometry . 4.5.H: Suppose $I$ is any homogeneous ideal of $S$ contained in $S_+$, and if $f$ is a homogeneous element of positive degree, show that $f$ vanishes on $V(I)$, i.e. $V(I)\subset V(f)$ iff $f^n\in I$ for some $n$. The definition of $V(I)$ is the set of all homogeneous prime ideals containing $I$ but not $S_+$. My thoughts: Now one direct is clear. I want to show the reverse. I think it translates to the following: $$\sqrt{I} = \bigcap_{I\subset P\in\text{Spec }A} P = \bigcap_{I\subset P\in\text{Proj }A} P.$$ But this seems to be false? How should I go about then?","EDIT: This is from an exercise of Vakil's Foundations of Algebraic Geometry . 4.5.H: Suppose $I$ is any homogeneous ideal of $S$ contained in $S_+$, and if $f$ is a homogeneous element of positive degree, show that $f$ vanishes on $V(I)$, i.e. $V(I)\subset V(f)$ iff $f^n\in I$ for some $n$. The definition of $V(I)$ is the set of all homogeneous prime ideals containing $I$ but not $S_+$. My thoughts: Now one direct is clear. I want to show the reverse. I think it translates to the following: $$\sqrt{I} = \bigcap_{I\subset P\in\text{Spec }A} P = \bigcap_{I\subset P\in\text{Proj }A} P.$$ But this seems to be false? How should I go about then?",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'graded-rings']"
91,"$M$ maximal in a ring $R$, what is $R/M$?","maximal in a ring , what is ?",M R R/M,"I just proved that if $R$ is a commutative ring with unity, then $M$ maximal ideal implies that $R/M$ is a field, and the converse is also true. I have the following questions: If $R$ is a ring with unity, would $R/M$ be a division ring? Is the converse true? If $R$ is a ring (possibly without unity), then what is $R/M$? If $R/M$ is a field, what we can say about $R$?","I just proved that if $R$ is a commutative ring with unity, then $M$ maximal ideal implies that $R/M$ is a field, and the converse is also true. I have the following questions: If $R$ is a ring with unity, would $R/M$ be a division ring? Is the converse true? If $R$ is a ring (possibly without unity), then what is $R/M$? If $R/M$ is a field, what we can say about $R$?",,"['abstract-algebra', 'ring-theory']"
92,An inequality with elementary symmetric polynomials,An inequality with elementary symmetric polynomials,,"Fix a natural number $n\geq 1$ . Let $a_1, \ldots, a_n$ be $n$ real numbers such that $a_i>0$ for each $i$ . Show that for each natural $k$ with $0\leq k\leq n$ $$e_k(a_1,\ldots, a_n)\geq\binom{n}{k}(a_1\cdot\cdots\cdot a_n)^{k/n}$$ where $e_k(x_1, \ldots, x_n)$ denotes the $k$ th symmetric polynomial with $n$ arguments and where we make the convention that $e_0(x_1,\ldots, x_n)=1$ . Here's the proof I gave: It suffices to prove the claim assuming that $a_1\cdot\cdots\cdot a_n=1$ . For if this is not the case, then setting $\overline{a_i}=a_i/(a_1\cdot\cdots\cdot a_n)^{1/n}$ will give $$\overline{a_1}\cdot\cdots\cdot\overline{a_n}=1$$ and the general claim will follow from the fact that $$e_k(\overline{a_1},\ldots,\overline{a_n})=\frac{1}{(a_1\cdot\cdots\cdot a_n)^{k/n}} e_k(a_1,\ldots,a_n)$$ Thus we prove that if $a_1\cdot\cdots\cdot a_n=1$ then $e_k(a_1,\ldots,a_n)\geq\binom{n}{k}$ . We induct on $n$ (we go through Pascal's Triangle row-by-row left-to-right). Note that this claim is trivial for $k=0$ or $k=n$ . Thus when $n=1$ , the claim holds for $k=0$ and $k=n$ . Now assume the claim is true for the natural number $m$ and all natural $k$ with $0\leq k\leq m$ ; we show that the claim is true for the natural number $m+1$ and all natural $k$ with $0\leq k\leq m+1$ . Since $$e_0(a_1,\ldots, a_{m+1})=1=\binom{m+1}{0}$$ the claim follows trivially. And since $$e_1(a_1,\ldots, a_{m+1})=a_1+\cdots+a_{m+1}\geq m+1$$ is a special case of the Arithmetic-Geometric Mean Inequality we have that this case holds as well. Without loss of generality we can assume that $a_1\leq \cdots\leq a_{m+1}$ . This ordering implies that $a_{m+1}\geq 1$ since a product of numbers strictly greater than 0 and strictly less than 1 cannot multiply to 1. Now note that $$e_{k+1}(a_1\ldots, a_{m+1})=e_{k+1}(a_1,\ldots,a_m)+a_{m+1}e_{k}(a_1,\ldots,a_m)$$ for $1\leq k+1\leq m$ . Thus $$e_{k+1}(a_1,\ldots, a_{m+1})\geq\binom{m}{k+1}+a_{m+1}\binom{m}{k}\geq\binom{m}{k+1}+\binom{m}{k}=\binom{m+1}{k+1}$$ And since $e_{m+1}(a_1,\ldots,a_{m+1})=a_1\cdot\cdots\cdot a_{m+1}=1=\binom{m+1}{m+1}$ , the claim follows by induction. Two questions. First, is this proof correct? Second, can someone come up with a slicker proof? Moving through Pascal's Triangle is kind of messy. Perhaps someone knows certain recursive relations between the symmetric polynomials that would make this easier, or other interesting inequalities (I suspect some identity with binomial coefficients will have to be used in another proof).","Fix a natural number . Let be real numbers such that for each . Show that for each natural with where denotes the th symmetric polynomial with arguments and where we make the convention that . Here's the proof I gave: It suffices to prove the claim assuming that . For if this is not the case, then setting will give and the general claim will follow from the fact that Thus we prove that if then . We induct on (we go through Pascal's Triangle row-by-row left-to-right). Note that this claim is trivial for or . Thus when , the claim holds for and . Now assume the claim is true for the natural number and all natural with ; we show that the claim is true for the natural number and all natural with . Since the claim follows trivially. And since is a special case of the Arithmetic-Geometric Mean Inequality we have that this case holds as well. Without loss of generality we can assume that . This ordering implies that since a product of numbers strictly greater than 0 and strictly less than 1 cannot multiply to 1. Now note that for . Thus And since , the claim follows by induction. Two questions. First, is this proof correct? Second, can someone come up with a slicker proof? Moving through Pascal's Triangle is kind of messy. Perhaps someone knows certain recursive relations between the symmetric polynomials that would make this easier, or other interesting inequalities (I suspect some identity with binomial coefficients will have to be used in another proof).","n\geq 1 a_1, \ldots, a_n n a_i>0 i k 0\leq k\leq n e_k(a_1,\ldots, a_n)\geq\binom{n}{k}(a_1\cdot\cdots\cdot a_n)^{k/n} e_k(x_1, \ldots, x_n) k n e_0(x_1,\ldots, x_n)=1 a_1\cdot\cdots\cdot a_n=1 \overline{a_i}=a_i/(a_1\cdot\cdots\cdot a_n)^{1/n} \overline{a_1}\cdot\cdots\cdot\overline{a_n}=1 e_k(\overline{a_1},\ldots,\overline{a_n})=\frac{1}{(a_1\cdot\cdots\cdot a_n)^{k/n}} e_k(a_1,\ldots,a_n) a_1\cdot\cdots\cdot a_n=1 e_k(a_1,\ldots,a_n)\geq\binom{n}{k} n k=0 k=n n=1 k=0 k=n m k 0\leq k\leq m m+1 k 0\leq k\leq m+1 e_0(a_1,\ldots, a_{m+1})=1=\binom{m+1}{0} e_1(a_1,\ldots, a_{m+1})=a_1+\cdots+a_{m+1}\geq m+1 a_1\leq \cdots\leq a_{m+1} a_{m+1}\geq 1 e_{k+1}(a_1\ldots, a_{m+1})=e_{k+1}(a_1,\ldots,a_m)+a_{m+1}e_{k}(a_1,\ldots,a_m) 1\leq k+1\leq m e_{k+1}(a_1,\ldots, a_{m+1})\geq\binom{m}{k+1}+a_{m+1}\binom{m}{k}\geq\binom{m}{k+1}+\binom{m}{k}=\binom{m+1}{k+1} e_{m+1}(a_1,\ldots,a_{m+1})=a_1\cdot\cdots\cdot a_{m+1}=1=\binom{m+1}{m+1}","['abstract-algebra', 'inequality']"
93,A free direct sum of a projective module,A free direct sum of a projective module,,"I want to prove that a left module $_RP$ is a projective generator if and only if a direct sum of (copies of) $P$ is free. My try is, first, to observe that $P$ is a generator if and only if for some positive integer $n$ and some left $R$-module $X$ we have $P^{(n)}$ is isomorphic to $R⊕X$. Also, $P$ is a direct summand of a free $R$-module. Henceforth I am stuck. Any help would be appreciated.","I want to prove that a left module $_RP$ is a projective generator if and only if a direct sum of (copies of) $P$ is free. My try is, first, to observe that $P$ is a generator if and only if for some positive integer $n$ and some left $R$-module $X$ we have $P^{(n)}$ is isomorphic to $R⊕X$. Also, $P$ is a direct summand of a free $R$-module. Henceforth I am stuck. Any help would be appreciated.",,"['abstract-algebra', 'modules', 'noncommutative-algebra', 'projective-module']"
94,Online Archive of Master Theses,Online Archive of Master Theses,,"I am thinking about taking the thesis route to complete my master in pure math. In anticipation of these in the coming semesters, here are my questions: Do you know of any links to archives of master-level theses, especially in Group Theory? I would like to get a feeling. I found one here but looks like it is for PhD level. Do you have any suggestions as to the possible topics, especially in Finite Group Theory, that I can bring up for discussion with my prof next semesters? (Shy not from suggesting any plain vanilla topics, for my school is only a non-flagship plain vanilla state school, and I am just an average plain vanilla student too -- but eager to move ahead.) Thank you very much.","I am thinking about taking the thesis route to complete my master in pure math. In anticipation of these in the coming semesters, here are my questions: Do you know of any links to archives of master-level theses, especially in Group Theory? I would like to get a feeling. I found one here but looks like it is for PhD level. Do you have any suggestions as to the possible topics, especially in Finite Group Theory, that I can bring up for discussion with my prof next semesters? (Shy not from suggesting any plain vanilla topics, for my school is only a non-flagship plain vanilla state school, and I am just an average plain vanilla student too -- but eager to move ahead.) Thank you very much.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'online-resources']"
95,"$\{p_i\}$ generate the $k$-algebra of symmetric polynomials in $k[t_1, \dots, t_n]$ and are algebraically independent over $k$",generate the -algebra of symmetric polynomials in  and are algebraically independent over,"\{p_i\} k k[t_1, \dots, t_n] k","Let $k$ be a field of characteristic $0$. For $j \ge 0$, let $p_j = t_1^j + \dots + t_n^j \in k[t_1, \dots, t_n]$. Prove that $p_1, \dots, p_n$ generate the $k$-algebra of symmetric polynomials in $k[t_1, \dots, t_n]$, and are algebraically independent over $k$. As to what I've tried so far, I've messed around with the Newton identities, but they haven't really got me anywhere. Am I going up the right alley? Or is there some other approach I should be taking?","Let $k$ be a field of characteristic $0$. For $j \ge 0$, let $p_j = t_1^j + \dots + t_n^j \in k[t_1, \dots, t_n]$. Prove that $p_1, \dots, p_n$ generate the $k$-algebra of symmetric polynomials in $k[t_1, \dots, t_n]$, and are algebraically independent over $k$. As to what I've tried so far, I've messed around with the Newton identities, but they haven't really got me anywhere. Am I going up the right alley? Or is there some other approach I should be taking?",,['abstract-algebra']
96,Why $|G:Z(G)|$ is finite in this question?,Why  is finite in this question?,|G:Z(G)|,"Suppose $G$ is a group such that the order of any nontrivial element of $G$ is infinite. Prove that if $G$ has a cyclic subgroup with finite index then $G$ is cyclic. my solution:suppose $|G:<x_0>|=n$ then $G=<x_0,g_1,...,g_n>$ .because $G=\dot{\cup}^{n}_{i=1}<x_0>g_i$,if I show that $|G:Z(G)|$ is finite,then by schur theorem ,we can have $G^{'}$ is finite,by hypothesis we should have $G^{'}=1$ and then $G$ is a abelian finitely generated group,so by fundamental theorem of abelian finitely generated group $G \cong \mathbb{Z} \times G_1 \times ... \times G_n $ where $G_i$ s are cyclic and are of order prime numbers,by the hypothesis we should have $G_i=1$ and then $G \cong \mathbb{Z}$ . now my problem is why $|G:Z(G)|$ is finite and how should I show this? it will be great if you help me with this,thanks.","Suppose $G$ is a group such that the order of any nontrivial element of $G$ is infinite. Prove that if $G$ has a cyclic subgroup with finite index then $G$ is cyclic. my solution:suppose $|G:<x_0>|=n$ then $G=<x_0,g_1,...,g_n>$ .because $G=\dot{\cup}^{n}_{i=1}<x_0>g_i$,if I show that $|G:Z(G)|$ is finite,then by schur theorem ,we can have $G^{'}$ is finite,by hypothesis we should have $G^{'}=1$ and then $G$ is a abelian finitely generated group,so by fundamental theorem of abelian finitely generated group $G \cong \mathbb{Z} \times G_1 \times ... \times G_n $ where $G_i$ s are cyclic and are of order prime numbers,by the hypothesis we should have $G_i=1$ and then $G \cong \mathbb{Z}$ . now my problem is why $|G:Z(G)|$ is finite and how should I show this? it will be great if you help me with this,thanks.",,"['abstract-algebra', 'group-theory']"
97,Homomorphism from a finitely generated module to a direct sum of modules,Homomorphism from a finitely generated module to a direct sum of modules,,"Let $R$ be a commutative ring with unit. If $M$ and $N_i$ are arbitrary $R$-modules, the module $\operatorname{Hom}_R(M,\bigoplus_{i\in I}N_i)$ is not isomorphic to $\bigoplus_{i\in I}\operatorname{Hom}(M,N_i)$ in general. But if $M$ is finitely generated, does the isomorphism hold?","Let $R$ be a commutative ring with unit. If $M$ and $N_i$ are arbitrary $R$-modules, the module $\operatorname{Hom}_R(M,\bigoplus_{i\in I}N_i)$ is not isomorphic to $\bigoplus_{i\in I}\operatorname{Hom}(M,N_i)$ in general. But if $M$ is finitely generated, does the isomorphism hold?",,"['abstract-algebra', 'modules', 'finitely-generated']"
98,Atiyah-MacDonald Ch. 4 exercise 20: what's the module analogue of $\sqrt{\mathfrak{a}+\mathfrak{b}} = \sqrt{\sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}}}$?,Atiyah-MacDonald Ch. 4 exercise 20: what's the module analogue of ?,\sqrt{\mathfrak{a}+\mathfrak{b}} = \sqrt{\sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}}},"Atiyah-MacDonald exercises 20-23 in chapter 4 develop a theory of primary decomposition for modules, in analogy with the theory developed in the chapter for rings. Exercise 20 begins with this definition: Definition: Given a (commutative, unital) ring $A$ and an $A$ -module $M$ , and a submodule $N\subset M$ , the radical of $N$ in $M$ is $$r_M(N) = \sqrt{\operatorname{Ann} M/N}$$ It then asks us to prove analogues to the formulas in exercise 1.13 for the radical of an ideal. Formula 1.13(v) is $$\sqrt{\mathfrak{a}+\mathfrak{b}} = \sqrt{\sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}}}$$ This is true by taking radicals in the pair of inclusions $\mathfrak{a}+\mathfrak{b}\subset \sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}}$ and $\sqrt{\mathfrak{a}+\mathfrak{b}} \supset \sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}}$ , the first of which is totally obvious and the second of which is because if $x^k\in\mathfrak{a}$ and $y^\ell\in\mathfrak{b}$ then $(x+y)^{k+\ell}\in\mathfrak{a}+\mathfrak{b}$ . It seems to me that the analogous formula in the module setting is $$ \label{eq}\tag{1} r_M(N+N') = \sqrt{r_M(N)+r_M(N')} $$ While the inclusion $r_M(N+N') \supset r_M(N)+r_M(N')$ is true here for more or less the same reason as $\sqrt{\mathfrak{a}+\mathfrak{b}} \supset \sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}}$ , and taking radicals gives $r_M(N+N')\supset \sqrt{r_M(N)+r_M(N')}$ , the other inclusion has been confounding me and leading me to wonder if this is not the right generalization of formula 1.13(v). So, question: Is this the right analogue of 1.13(v)? If so what's the proof of the inclusion $r_M(N+N') \subset \sqrt{r_M(N)+r_M(N')}$ ? If not, what's the right analogue? Thoughts: if this is the right analogue, and the proof is also analogous, the desired inclusion should follow by taking radicals in the inclusion $$\operatorname{Ann}\frac{M}{N+N'} \subset r_M(N)+r_M(N')$$ But I can't think of a reason for this. If $x\in A$ puts $M$ inside $N+N'$ , to prove this statement I'd need to decompose it as something $y+y'$ where a power of $y$ puts $M$ inside $N$ and a power of $y'$ puts $M$ inside $N'$ . How would I even start to seek such a decomposition?","Atiyah-MacDonald exercises 20-23 in chapter 4 develop a theory of primary decomposition for modules, in analogy with the theory developed in the chapter for rings. Exercise 20 begins with this definition: Definition: Given a (commutative, unital) ring and an -module , and a submodule , the radical of in is It then asks us to prove analogues to the formulas in exercise 1.13 for the radical of an ideal. Formula 1.13(v) is This is true by taking radicals in the pair of inclusions and , the first of which is totally obvious and the second of which is because if and then . It seems to me that the analogous formula in the module setting is While the inclusion is true here for more or less the same reason as , and taking radicals gives , the other inclusion has been confounding me and leading me to wonder if this is not the right generalization of formula 1.13(v). So, question: Is this the right analogue of 1.13(v)? If so what's the proof of the inclusion ? If not, what's the right analogue? Thoughts: if this is the right analogue, and the proof is also analogous, the desired inclusion should follow by taking radicals in the inclusion But I can't think of a reason for this. If puts inside , to prove this statement I'd need to decompose it as something where a power of puts inside and a power of puts inside . How would I even start to seek such a decomposition?","A A M N\subset M N M r_M(N) = \sqrt{\operatorname{Ann} M/N} \sqrt{\mathfrak{a}+\mathfrak{b}} = \sqrt{\sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}}} \mathfrak{a}+\mathfrak{b}\subset \sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}} \sqrt{\mathfrak{a}+\mathfrak{b}} \supset \sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}} x^k\in\mathfrak{a} y^\ell\in\mathfrak{b} (x+y)^{k+\ell}\in\mathfrak{a}+\mathfrak{b} 
\label{eq}\tag{1}
r_M(N+N') = \sqrt{r_M(N)+r_M(N')}
 r_M(N+N') \supset r_M(N)+r_M(N') \sqrt{\mathfrak{a}+\mathfrak{b}} \supset \sqrt{\mathfrak{a}}+\sqrt{\mathfrak{b}} r_M(N+N')\supset \sqrt{r_M(N)+r_M(N')} r_M(N+N') \subset \sqrt{r_M(N)+r_M(N')} \operatorname{Ann}\frac{M}{N+N'} \subset r_M(N)+r_M(N') x\in A M N+N' y+y' y M N y' M N'","['abstract-algebra', 'ring-theory', 'commutative-algebra', 'modules']"
99,Irreducible radical ideals are prime,Irreducible radical ideals are prime,,"Assume $R$ is a commutative ring and $I$ is a nonzero proper ideal of $R$ satisfying: $(1)$  If $I_1$ and $I_2$ are ideals such that $I = I_1 \cap I_2$, then $I = I_1$ or $I = I_2$; $(2)$ If $a^n \in I$, then $a \in I$. Prove that $I$ is prime. Here is my strategy thusfar.  Suppose $xy \in I$.  Ultimately, it seems the goal is to find ideals $I_x \ni x$ and $I_y \ni y$ such that $I = I_x \cap I_y$.  By $(2)$, either $x \in I$ or $y \in I$. I thought the natural thing to check was $I_x = I + (x)$ and $I_y = I + (y)$.  For then $I \subset I_x \cap I_y$ and $I_x I_y \subset I$.  Generally, $I_x I_y \subset I_x \cap I_y$, but the two are equal if $I_x + I_y = R$. So I thought I might be able to use $(2)$ somehow to prove $I + (x) + (y) = R$.  At the same time, I wonder whether this is simply not true and I am going about this the wrong way. Am I on the right track?  Can someone give me a hint that will help me get unstuck? Thanks.","Assume $R$ is a commutative ring and $I$ is a nonzero proper ideal of $R$ satisfying: $(1)$  If $I_1$ and $I_2$ are ideals such that $I = I_1 \cap I_2$, then $I = I_1$ or $I = I_2$; $(2)$ If $a^n \in I$, then $a \in I$. Prove that $I$ is prime. Here is my strategy thusfar.  Suppose $xy \in I$.  Ultimately, it seems the goal is to find ideals $I_x \ni x$ and $I_y \ni y$ such that $I = I_x \cap I_y$.  By $(2)$, either $x \in I$ or $y \in I$. I thought the natural thing to check was $I_x = I + (x)$ and $I_y = I + (y)$.  For then $I \subset I_x \cap I_y$ and $I_x I_y \subset I$.  Generally, $I_x I_y \subset I_x \cap I_y$, but the two are equal if $I_x + I_y = R$. So I thought I might be able to use $(2)$ somehow to prove $I + (x) + (y) = R$.  At the same time, I wonder whether this is simply not true and I am going about this the wrong way. Am I on the right track?  Can someone give me a hint that will help me get unstuck? Thanks.",,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
