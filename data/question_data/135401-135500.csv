,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Why does this not contradict 'Solutions of $\dot{x} = f(x)$ cannot intersect""?","Why does this not contradict 'Solutions of  cannot intersect""?",\dot{x} = f(x),"Say we have $\dot{x} = cos(x^2)$ and $x(0) = 1$, and also $\dot{y} = cos(y^2)$ and $y(0) = 0.5$ Then by the MVT we know that $0.5 < \frac{y(1)-y(0)}{1 - 0} < 1$, which implies that $y(0) = 0.5 < x(0) = 1< y(1)$. How does this not contradict that solutions of an autonomous ODE, $\dot{x} = f(x)$ with $f:\mathbb{R}^n \to \mathbb{R}^n$ continuously differentiable, cannot cross? I think this may be down to a misunderstanding in general of what the above theorem means. I thought it meant that solutions of the same ODE, for different initial conditions, cannot physically cross, even if this happens at different times, but in the above we seem to have different initial conditions, and a solution physically crossing the path of another solution. Is it something to do with the solutions being 1 dimensional?","Say we have $\dot{x} = cos(x^2)$ and $x(0) = 1$, and also $\dot{y} = cos(y^2)$ and $y(0) = 0.5$ Then by the MVT we know that $0.5 < \frac{y(1)-y(0)}{1 - 0} < 1$, which implies that $y(0) = 0.5 < x(0) = 1< y(1)$. How does this not contradict that solutions of an autonomous ODE, $\dot{x} = f(x)$ with $f:\mathbb{R}^n \to \mathbb{R}^n$ continuously differentiable, cannot cross? I think this may be down to a misunderstanding in general of what the above theorem means. I thought it meant that solutions of the same ODE, for different initial conditions, cannot physically cross, even if this happens at different times, but in the above we seem to have different initial conditions, and a solution physically crossing the path of another solution. Is it something to do with the solutions being 1 dimensional?",,"['ordinary-differential-equations', 'dynamical-systems']"
1,Reducing an SIR model,Reducing an SIR model,,"Past Paper Question: Given a SIRS model: \begin{align} \dfrac{dS}{dt} & = -\alpha IS+\gamma R \tag1 \\[8pt] \dfrac{dI}{dt} & = \alpha IS-\beta I\\[8pt] \dfrac{dR}{dt} & = \beta I-\gamma R, \end{align} where $S$, $I$ and $R$ denote the number of susceptible, infected and recovered individuals respectively and $t$ is time. Show that the total population N is constant, and hence reduce the system to two equations for $S$ and $I$. My Attempt: Since we are only looking over a small time interval, natural births and deaths are negligible hence we have a closed population so:   $$S\left( t\right) +I\left( t\right) +R\left( t\right) =N \tag4 $$ My Question: How do you use this to reduce the above to a system of two ODE's?","Past Paper Question: Given a SIRS model: \begin{align} \dfrac{dS}{dt} & = -\alpha IS+\gamma R \tag1 \\[8pt] \dfrac{dI}{dt} & = \alpha IS-\beta I\\[8pt] \dfrac{dR}{dt} & = \beta I-\gamma R, \end{align} where $S$, $I$ and $R$ denote the number of susceptible, infected and recovered individuals respectively and $t$ is time. Show that the total population N is constant, and hence reduce the system to two equations for $S$ and $I$. My Attempt: Since we are only looking over a small time interval, natural births and deaths are negligible hence we have a closed population so:   $$S\left( t\right) +I\left( t\right) +R\left( t\right) =N \tag4 $$ My Question: How do you use this to reduce the above to a system of two ODE's?",,"['ordinary-differential-equations', 'mathematical-modeling', 'biology']"
2,Find a basis for the set of solutions of the given system of differential equations,Find a basis for the set of solutions of the given system of differential equations,,"Here is what is given: $$ x' =  \begin{bmatrix}1&0 \\2&1\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}$$ As the title says, we need to find a basis for the set of solutions of this differential equation. Here is my attempt: I set up this system $$\begin{cases} x_1' = x_1 \\ x_2' = 2x_1 + x_2 \end{cases}$$ I then assumed these substitutions were made $y = x_1$ and therefore $y' = x_2$ From this, I saw that $y'' = 2y + y'$ . Therefore: $$y'' - y' - 2y = 0$$ I solved this characteristic equation and got these: $$\begin{cases} y_1 = e^{-t} \\ y_2 = e^{2t} \end{cases}$$ So for my basis I got these: $$\hat {x^1}  = \begin{bmatrix}e^{-t}\\-e^{-t}\end{bmatrix}$$ And: $$\hat {x^2} = \begin{bmatrix}e^{2t} \\ 2e^{2t} \end{bmatrix}$$ I then selected $t_0 = 0$ as a convenient value of t and then took the determinant of this matrix: $$\begin{bmatrix} 1 & 1 \\ -1 & 2 \end{bmatrix}$$ The determinant is equal to $3$ which means that $\hat {x^1}$ and $\hat {x^2}$ are linearly independent and therefore form a basis for the given differential equation. However, the answer in the back of my book is different (I can post it if anyone needs it). Is my method the wrong way to approach this problem? Did I make a logical or mathematical error? Any help is appreciated.","Here is what is given: As the title says, we need to find a basis for the set of solutions of this differential equation. Here is my attempt: I set up this system I then assumed these substitutions were made and therefore From this, I saw that . Therefore: I solved this characteristic equation and got these: So for my basis I got these: And: I then selected as a convenient value of t and then took the determinant of this matrix: The determinant is equal to which means that and are linearly independent and therefore form a basis for the given differential equation. However, the answer in the back of my book is different (I can post it if anyone needs it). Is my method the wrong way to approach this problem? Did I make a logical or mathematical error? Any help is appreciated.", x' =  \begin{bmatrix}1&0 \\2&1\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} \begin{cases} x_1' = x_1 \\ x_2' = 2x_1 + x_2 \end{cases} y = x_1 y' = x_2 y'' = 2y + y' y'' - y' - 2y = 0 \begin{cases} y_1 = e^{-t} \\ y_2 = e^{2t} \end{cases} \hat {x^1}  = \begin{bmatrix}e^{-t}\\-e^{-t}\end{bmatrix} \hat {x^2} = \begin{bmatrix}e^{2t} \\ 2e^{2t} \end{bmatrix} t_0 = 0 \begin{bmatrix} 1 & 1 \\ -1 & 2 \end{bmatrix} 3 \hat {x^1} \hat {x^2},"['linear-algebra', 'ordinary-differential-equations']"
3,VI Arnold ODE: Need help coming to grips with early notation in the book (phase velocity),VI Arnold ODE: Need help coming to grips with early notation in the book (phase velocity),,"This seems to be a somewhat common theme among users on here trying to start reading Arnold's ODE book. I have found a few similar questions but none have sufficiently answered my problems. Specifically I am having a hard time understanding the phase velocity, $v(x) = \dot{x} = \frac{d}{dt}|_{t=0}g^tx$. According to this post, Arnold's ODE computation of phase velocity , it is suggested that in the notation of $g^tx$, $x$ is considered a constant or equal to the initial value $x_0$. But how do I then reconcile this fact with the expression $\dot{x}=\frac{d}{dt}|_{t=0}g^tx$? In my simple world $\dot{x}=\frac{d}{dt}g^tx$ would make a lot more sense. I find this alteration to be especially fitting considering the examples put forth later, e.g. draw the integral curves of $v(x) = \dot{x} = -kx$. What am I missing here? Help would be greatly appreciated as I am reading this in my spare time and have no access to feedback other than from sources such as this site. Thank you! Edit by request: I have the Richard A. Silverman translated edition and the definition is found in subsection 1.4 on Vector Fields in Sec. 1 Phase Spaces and Phase Flows on page 7.","This seems to be a somewhat common theme among users on here trying to start reading Arnold's ODE book. I have found a few similar questions but none have sufficiently answered my problems. Specifically I am having a hard time understanding the phase velocity, $v(x) = \dot{x} = \frac{d}{dt}|_{t=0}g^tx$. According to this post, Arnold's ODE computation of phase velocity , it is suggested that in the notation of $g^tx$, $x$ is considered a constant or equal to the initial value $x_0$. But how do I then reconcile this fact with the expression $\dot{x}=\frac{d}{dt}|_{t=0}g^tx$? In my simple world $\dot{x}=\frac{d}{dt}g^tx$ would make a lot more sense. I find this alteration to be especially fitting considering the examples put forth later, e.g. draw the integral curves of $v(x) = \dot{x} = -kx$. What am I missing here? Help would be greatly appreciated as I am reading this in my spare time and have no access to feedback other than from sources such as this site. Thank you! Edit by request: I have the Richard A. Silverman translated edition and the definition is found in subsection 1.4 on Vector Fields in Sec. 1 Phase Spaces and Phase Flows on page 7.",,"['ordinary-differential-equations', 'mathematical-physics', 'classical-mechanics']"
4,Successive approximations for $y''= \frac{y'^2}{y}-\frac{y}{x^2}$,Successive approximations for,y''= \frac{y'^2}{y}-\frac{y}{x^2},"Successive approximation: $$\begin{align} y_{n+1}&=y_0+\int_{x_0}^xz_n(t)\,dt\\ z_{n+1}&=z_0+\int_{x_0}^xf(t,y_n(t),z_n(t))\,dt \end{align}$$ I try to use it on the 2nd-order differential equation as below: $$y''(x)= \frac{y'(x)^2}{y(x)}-\frac{y(x)}{x^2}$$ With BV: $y(1)=2,y(2)=1$ By defining $z=y'(x)$, we have:  $$\begin{align} z'(x)=f(x,y,z)= \frac{z^2(x)}{y(x)}-\frac{y(x)}{x^2} \\ z(1)=y'(1) \\ z(2)=y'(2) \end{align}$$ But here z(1) and z(2) are really unknowns (not given by boundary conditions), how can we use the Successive approximation method to get the approximation say $z_2(x)$ and $y_2(x)$? PS: I have already calculated the analytical solution to this equation already, hence this is a pure practice question on (Successive) approximation.","Successive approximation: $$\begin{align} y_{n+1}&=y_0+\int_{x_0}^xz_n(t)\,dt\\ z_{n+1}&=z_0+\int_{x_0}^xf(t,y_n(t),z_n(t))\,dt \end{align}$$ I try to use it on the 2nd-order differential equation as below: $$y''(x)= \frac{y'(x)^2}{y(x)}-\frac{y(x)}{x^2}$$ With BV: $y(1)=2,y(2)=1$ By defining $z=y'(x)$, we have:  $$\begin{align} z'(x)=f(x,y,z)= \frac{z^2(x)}{y(x)}-\frac{y(x)}{x^2} \\ z(1)=y'(1) \\ z(2)=y'(2) \end{align}$$ But here z(1) and z(2) are really unknowns (not given by boundary conditions), how can we use the Successive approximation method to get the approximation say $z_2(x)$ and $y_2(x)$? PS: I have already calculated the analytical solution to this equation already, hence this is a pure practice question on (Successive) approximation.",,"['calculus', 'ordinary-differential-equations', 'numerical-methods']"
5,Discretization of matrix differential equation $\dot{Q} = \frac{1}{2}\Omega Q + \frac{1}{2}Q\Omega^T$,Discretization of matrix differential equation,\dot{Q} = \frac{1}{2}\Omega Q + \frac{1}{2}Q\Omega^T,"We know that $\dot{x}(t) = Ax(t)$ can be discretized as follows $$x[k+1] = A_d x[k]$$ where $$A_d = e^{AT}$$ where $T$ is the sample time. If I have the following differential equation, how do I obtain its discretized form? (and how to find its $A_d$) $$\dot{Q} = \frac{1}{2}\Omega Q + \frac{1}{2}Q\Omega^T$$ where   $Q$ is symmetric and positive semidefinite matrix and $\Omega$ is skew symmetric matrix. I think  I can obtain the following: $$\dot{Q} = \frac{1}{2}\Omega Q - \frac{1}{2}Q\Omega$$ Then I have no idea how to discretize this in the form similar to the one in Wiki?","We know that $\dot{x}(t) = Ax(t)$ can be discretized as follows $$x[k+1] = A_d x[k]$$ where $$A_d = e^{AT}$$ where $T$ is the sample time. If I have the following differential equation, how do I obtain its discretized form? (and how to find its $A_d$) $$\dot{Q} = \frac{1}{2}\Omega Q + \frac{1}{2}Q\Omega^T$$ where   $Q$ is symmetric and positive semidefinite matrix and $\Omega$ is skew symmetric matrix. I think  I can obtain the following: $$\dot{Q} = \frac{1}{2}\Omega Q - \frac{1}{2}Q\Omega$$ Then I have no idea how to discretize this in the form similar to the one in Wiki?",,"['ordinary-differential-equations', 'numerical-methods']"
6,Solving the schrodinger equation in cylindrical coordinates,Solving the schrodinger equation in cylindrical coordinates,,"I'm trying to solve the Schrodinger equation in cylindrical coordinates using this potential $V=\frac{c}{\rho}$ where c is a constant. $$\psi(\rho,\phi,z)=R(\rho)\Phi(\phi)Z(z)$$ The $\phi$ and Z part are simple to solve but in the radial part I get this: $$ \rho\frac{1}{R}\frac{\partial}{\partial\rho}\left(\frac{\partial R}{\partial\rho}\right)-\frac{2m}{\hbar^2}V\rho^2+l^2\rho^2=m^2 $$ Where $l^2$ and $m^2$ are the separations constant when I separate the Z and $\phi$ part. I think the solution must be in terms in Bessel function like the H atom but I'm not sure and I don't know how to try to solve this. Thanks in advance for your help.","I'm trying to solve the Schrodinger equation in cylindrical coordinates using this potential $V=\frac{c}{\rho}$ where c is a constant. $$\psi(\rho,\phi,z)=R(\rho)\Phi(\phi)Z(z)$$ The $\phi$ and Z part are simple to solve but in the radial part I get this: $$ \rho\frac{1}{R}\frac{\partial}{\partial\rho}\left(\frac{\partial R}{\partial\rho}\right)-\frac{2m}{\hbar^2}V\rho^2+l^2\rho^2=m^2 $$ Where $l^2$ and $m^2$ are the separations constant when I separate the Z and $\phi$ part. I think the solution must be in terms in Bessel function like the H atom but I'm not sure and I don't know how to try to solve this. Thanks in advance for your help.",,"['ordinary-differential-equations', 'physics', 'mathematical-physics', 'quantum-mechanics']"
7,Linearly independent solutions of Cauchy-Euler differential equation,Linearly independent solutions of Cauchy-Euler differential equation,,"Are $y_1=x^3$ and $y_2=x^2|x|$ ($x\in \mathbb R$) linearly independent solutions of the Cauchy-Euler homogeneous differential equation: $x^2y''-4xy'+6y=0$ on $\mathbb R$? Clearly, $y_1$ and $y_2$ are linearly independent functions on $\mathbb R$ and on plugging, both satisfy the given differential equation. Hence, it appears that they are LI solutions of the given ODE, but solving this equation gives $x^2$ and $x^3$ as the two independent solutions and both of them are defined for all $x\in \mathbb R$ (which again contradicts the fact that the Cauchy-Euler equation is defined for $x>0$). Now, my confusion is: $1$. Can a second order ODE have three LI solutions viz. $x^2$, $x^3$ and $x^2|x|$ over $\mathbb R$? $2$. Though the coefficients of $y'$ and $y$ viz. $-4/x$ and $6/x^2$ are not defined at $x=0$ but its solutions are defined there. So, will it be correct to talk about the behaviour of solutions in the domain $x\in \mathbb R$? Thanks!","Are $y_1=x^3$ and $y_2=x^2|x|$ ($x\in \mathbb R$) linearly independent solutions of the Cauchy-Euler homogeneous differential equation: $x^2y''-4xy'+6y=0$ on $\mathbb R$? Clearly, $y_1$ and $y_2$ are linearly independent functions on $\mathbb R$ and on plugging, both satisfy the given differential equation. Hence, it appears that they are LI solutions of the given ODE, but solving this equation gives $x^2$ and $x^3$ as the two independent solutions and both of them are defined for all $x\in \mathbb R$ (which again contradicts the fact that the Cauchy-Euler equation is defined for $x>0$). Now, my confusion is: $1$. Can a second order ODE have three LI solutions viz. $x^2$, $x^3$ and $x^2|x|$ over $\mathbb R$? $2$. Though the coefficients of $y'$ and $y$ viz. $-4/x$ and $6/x^2$ are not defined at $x=0$ but its solutions are defined there. So, will it be correct to talk about the behaviour of solutions in the domain $x\in \mathbb R$? Thanks!",,['ordinary-differential-equations']
8,"Existence and Uniqueness Theorems: When to use Picard-Lindelöf, Lipschitz, etc.","Existence and Uniqueness Theorems: When to use Picard-Lindelöf, Lipschitz, etc.",,"I am currently reviewing some basic ordinary and partial differential equations for an upcoming oral exam and I am stuck at existence and uniqueness theorems. As far as I understand, one would like to know two things about a (relatively) complicated ODE before trying to find a solution. Does the ODE even have a solution? If it has a solution, is it unique? Consider the IVP: $$\frac{dy}{dt}=f(x,y); \space \space \space y(a)=b$$ Question1: Suppose $f$ satisfies the Lipschitz condition. What does that mean and   what does that tell me about the solution of my ODE? Question 2: When and how does the Picard-Lindelöf theorem come in? Does   Picard-Lindelöf only tell me about the uniqueness? Do I need Lipschitz   continuity in order to even use Picard-Lindelöf? If someone asked me to specify in which domain $D \subset \Bbb R^2$ the differential equation $(x^2+y^2)y'=y^2$ has a unique solution, how would I approach this problem (or any other similar problem)?","I am currently reviewing some basic ordinary and partial differential equations for an upcoming oral exam and I am stuck at existence and uniqueness theorems. As far as I understand, one would like to know two things about a (relatively) complicated ODE before trying to find a solution. Does the ODE even have a solution? If it has a solution, is it unique? Consider the IVP: $$\frac{dy}{dt}=f(x,y); \space \space \space y(a)=b$$ Question1: Suppose $f$ satisfies the Lipschitz condition. What does that mean and   what does that tell me about the solution of my ODE? Question 2: When and how does the Picard-Lindelöf theorem come in? Does   Picard-Lindelöf only tell me about the uniqueness? Do I need Lipschitz   continuity in order to even use Picard-Lindelöf? If someone asked me to specify in which domain $D \subset \Bbb R^2$ the differential equation $(x^2+y^2)y'=y^2$ has a unique solution, how would I approach this problem (or any other similar problem)?",,['ordinary-differential-equations']
9,How to calculate the partial derivative for dependent variables?,How to calculate the partial derivative for dependent variables?,,"There are two variables $o_1$ and $o_2$. $$ o_1 = w_{11}x_1 + w_{12}x_2 \\ o_2 = w_{21}x_1 + w_{22}x_2 $$ Then is it true that $\frac{\partial ({o_1+o_2})}{\partial o_1} = 1$? However I'm a bit confused about this. If that true, let's say $y=2x$ and $z=x+y$, does that mean $\frac{\partial  z}{\partial x} = 1$ is also true?","There are two variables $o_1$ and $o_2$. $$ o_1 = w_{11}x_1 + w_{12}x_2 \\ o_2 = w_{21}x_1 + w_{22}x_2 $$ Then is it true that $\frac{\partial ({o_1+o_2})}{\partial o_1} = 1$? However I'm a bit confused about this. If that true, let's say $y=2x$ and $z=x+y$, does that mean $\frac{\partial  z}{\partial x} = 1$ is also true?",,"['real-analysis', 'ordinary-differential-equations', 'derivatives', 'partial-derivative']"
10,Dirac delta function forcing in 1st order ODE?,Dirac delta function forcing in 1st order ODE?,,"We have $$\frac{dT}{dt}=-a(T-T_{\infty})-\delta (t-1)$$ where $T_{\infty}$ is a constant and $\delta$ is the Dirac delta. Determine the jump (discontinuity) condition for $T$ at $t=1$ and   hence find $T(t)$ for $t>1$. I'm just a little puzzled as this is first order. I have only ever done second order before. Is $T$ continuous at $t=1$ here? What is the jump condition? I think I just need a run down of what is actually going on here, for me this is very much a method at the moment, I see these questions I have a procedure to solve them rather than actually comprehending everything that's going on. My method doesn't work here do I can't do it. Any help is appreciated. Thank you.","We have $$\frac{dT}{dt}=-a(T-T_{\infty})-\delta (t-1)$$ where $T_{\infty}$ is a constant and $\delta$ is the Dirac delta. Determine the jump (discontinuity) condition for $T$ at $t=1$ and   hence find $T(t)$ for $t>1$. I'm just a little puzzled as this is first order. I have only ever done second order before. Is $T$ continuous at $t=1$ here? What is the jump condition? I think I just need a run down of what is actually going on here, for me this is very much a method at the moment, I see these questions I have a procedure to solve them rather than actually comprehending everything that's going on. My method doesn't work here do I can't do it. Any help is appreciated. Thank you.",,"['ordinary-differential-equations', 'dirac-delta']"
11,Solution to an ordinary differential equation,Solution to an ordinary differential equation,,The general solution to the equation $y''+by'+cy=0$ approaches to $0$ as $x$ approaches infinity if $b$ is negative $c$ is positive $b$ is positive $c$ is negative $b$ is positive $c$ is positive $b$ is negative $c$ is negative,The general solution to the equation $y''+by'+cy=0$ approaches to $0$ as $x$ approaches infinity if $b$ is negative $c$ is positive $b$ is positive $c$ is negative $b$ is positive $c$ is positive $b$ is negative $c$ is negative,,['ordinary-differential-equations']
12,"Solve $y'' + y = \sec^2 x, 0 < x < \frac{\pi}{2}$",Solve,"y'' + y = \sec^2 x, 0 < x < \frac{\pi}{2}","The equation: $y'' + y = \sec^2 x, 0 < x < \frac{\pi}{2}$ I can't seem to find $u'/v'$ values. My system of equations is: $u' \sin x + v'\cos x = 0$ $u' \cos x - v'\sin x  = \sec^2 x$ I believe that $u = ∫\sec xdx$ and $v = ∫\sec x \tan xdx$, but I cannot for the life of me get these equations to solve for those integrations. Any ideas? Thank you so much in advance.","The equation: $y'' + y = \sec^2 x, 0 < x < \frac{\pi}{2}$ I can't seem to find $u'/v'$ values. My system of equations is: $u' \sin x + v'\cos x = 0$ $u' \cos x - v'\sin x  = \sec^2 x$ I believe that $u = ∫\sec xdx$ and $v = ∫\sec x \tan xdx$, but I cannot for the life of me get these equations to solve for those integrations. Any ideas? Thank you so much in advance.",,"['integration', 'ordinary-differential-equations']"
13,heat equation in polar coordinates,heat equation in polar coordinates,,"Consider the heat equation in polar coordinates, $$\frac{\partial u}{\partial t}=h^2\left(\frac{\partial^2 u}{\partial r^2}+\frac{2}{r}\frac{\partial u}{\partial r}\right), t>0, 0<r<R.$$ A sphere of radius $R$ is initially at constant temperature $u_0$ throughout, then has surface temperature $u_1$ for $t>0$. Find the temperature throughout the sphere for $t>0$ and in particular in the center $u_c$. I can solve the heat equation when in cartesian form, however polar coordinates has always been my weakness and i have been stumped for a while, i have let $U(r,t)=F(r)G(t)$ then substituted into our original equation to get $$G'(t)+(\lambda^2)G(t)=0 \text{ where } \lambda=kh$$ $$F""(r) + F'(r)/r + k^2F(r)=0$$ i have the initial condition that $U(r,0)=u_0$ and the boundary conditions that $u(R,t)=u_1$ however i do not know where to go from here, any help is appreciated.","Consider the heat equation in polar coordinates, $$\frac{\partial u}{\partial t}=h^2\left(\frac{\partial^2 u}{\partial r^2}+\frac{2}{r}\frac{\partial u}{\partial r}\right), t>0, 0<r<R.$$ A sphere of radius $R$ is initially at constant temperature $u_0$ throughout, then has surface temperature $u_1$ for $t>0$. Find the temperature throughout the sphere for $t>0$ and in particular in the center $u_c$. I can solve the heat equation when in cartesian form, however polar coordinates has always been my weakness and i have been stumped for a while, i have let $U(r,t)=F(r)G(t)$ then substituted into our original equation to get $$G'(t)+(\lambda^2)G(t)=0 \text{ where } \lambda=kh$$ $$F""(r) + F'(r)/r + k^2F(r)=0$$ i have the initial condition that $U(r,0)=u_0$ and the boundary conditions that $u(R,t)=u_1$ however i do not know where to go from here, any help is appreciated.",,"['ordinary-differential-equations', 'polar-coordinates', 'heat-equation']"
14,Inverse laplace 2/((s-1)^2+1)^2,Inverse laplace 2/((s-1)^2+1)^2,,"Find inverse laplace transform of $$\frac{2}{((s-1)^2+1)^2}$$ I tried to decompose the fraction using $$\because (s-1)^2+1=s^2-2s+2$$ $$\rightarrow \frac{2}{((s-1)^2+1)^2}=\frac{As+B}{s^2-2s+2}+\frac{Cs+D}{(s^2-2s+2)^2}$$ yet I get D=2, which leads me back to the same exact equation, any help? (I have also tried the $\frac{-d}{ds}(L{\{sint\}})$ propertie, but failed since there's no variable s in the numerator)","Find inverse laplace transform of $$\frac{2}{((s-1)^2+1)^2}$$ I tried to decompose the fraction using $$\because (s-1)^2+1=s^2-2s+2$$ $$\rightarrow \frac{2}{((s-1)^2+1)^2}=\frac{As+B}{s^2-2s+2}+\frac{Cs+D}{(s^2-2s+2)^2}$$ yet I get D=2, which leads me back to the same exact equation, any help? (I have also tried the $\frac{-d}{ds}(L{\{sint\}})$ propertie, but failed since there's no variable s in the numerator)",,"['calculus', 'ordinary-differential-equations', 'laplace-transform']"
15,Check solution to DE,Check solution to DE,,"I was solving the following DE $$\frac{y+x}{x}=y'$$ and my solution was $$xdy = (y+x)dx$$ Letting $y=xv$ then $dy=xdv+vdx$ and so the above equation becomes $$x^2dv + xvdx  =xvdx+xdx$$ From here I get $$x^2dv=xdx$$ Now here is where things get interesting.  I can write this as  $$\int 1 dv = \int \frac{1}{x}dx$$ And get the solution $$y=x\ln(x)+cx$$ But when I took my exam, I didn't do this.... instead I did this: $$\int x^2dv = \int x dx$$ Which gives $$x^2v=\frac{1}{2}x^2+C$$ $$xy=\frac{1}{2}x^2+c$$ $$y=\frac{1}{2}x+\frac{c}{x}$$ How can I reconcile these apparently different solutions?","I was solving the following DE $$\frac{y+x}{x}=y'$$ and my solution was $$xdy = (y+x)dx$$ Letting $y=xv$ then $dy=xdv+vdx$ and so the above equation becomes $$x^2dv + xvdx  =xvdx+xdx$$ From here I get $$x^2dv=xdx$$ Now here is where things get interesting.  I can write this as  $$\int 1 dv = \int \frac{1}{x}dx$$ And get the solution $$y=x\ln(x)+cx$$ But when I took my exam, I didn't do this.... instead I did this: $$\int x^2dv = \int x dx$$ Which gives $$x^2v=\frac{1}{2}x^2+C$$ $$xy=\frac{1}{2}x^2+c$$ $$y=\frac{1}{2}x+\frac{c}{x}$$ How can I reconcile these apparently different solutions?",,"['ordinary-differential-equations', 'fake-proofs']"
16,"Solution to $\mathbf{x}'=A\mathbf{x}$, with $A$ anti-symmetric implies $|\mathbf{x}|=|\mathbf{x}_0|$","Solution to , with  anti-symmetric implies",\mathbf{x}'=A\mathbf{x} A |\mathbf{x}|=|\mathbf{x}_0|,"Let $A$ be anti-symmetric. That is, $A^T=-A$. Show that the solution to the $\mathbf{x}'=A\mathbf{x}$ satisfies $|\mathbf{x}|=|\mathbf{x}_0|$. Hint: Compute $d/dt|\mathbf{x}|^2$. The given solution:  $d/dt|\mathbf{x}|^2=2\mathbf{x}'\cdot\mathbf{x}=2A\mathbf{x}\cdot\mathbf{x}=2A|\mathbf{x}|^2=0,$ since $A\mathbf{x}\cdot\mathbf{x}=\mathbf{x}A^T\mathbf{x}=-\mathbf{x}A\mathbf{x}.$ Thus, the norm is preserved. So, I get up until the ""since"" part. Is that just true because $A$ is anti-symmetric? That man Also, how does that imply that the given quantity is $0?$ I've tried to see it. This is as far as I can go: We have that $A|\mathbf{x}|^2=-\mathbf{x}A\mathbf{x}=-\mathbf{x}\cdot\mathbf{x}'=-e^{tA}\mathbf{x_0}A\mathbf{x}$.","Let $A$ be anti-symmetric. That is, $A^T=-A$. Show that the solution to the $\mathbf{x}'=A\mathbf{x}$ satisfies $|\mathbf{x}|=|\mathbf{x}_0|$. Hint: Compute $d/dt|\mathbf{x}|^2$. The given solution:  $d/dt|\mathbf{x}|^2=2\mathbf{x}'\cdot\mathbf{x}=2A\mathbf{x}\cdot\mathbf{x}=2A|\mathbf{x}|^2=0,$ since $A\mathbf{x}\cdot\mathbf{x}=\mathbf{x}A^T\mathbf{x}=-\mathbf{x}A\mathbf{x}.$ Thus, the norm is preserved. So, I get up until the ""since"" part. Is that just true because $A$ is anti-symmetric? That man Also, how does that imply that the given quantity is $0?$ I've tried to see it. This is as far as I can go: We have that $A|\mathbf{x}|^2=-\mathbf{x}A\mathbf{x}=-\mathbf{x}\cdot\mathbf{x}'=-e^{tA}\mathbf{x_0}A\mathbf{x}$.",,"['linear-algebra', 'ordinary-differential-equations']"
17,Solving the differential equation $\frac{dy}{dx} = \frac{y^3}{2(xy^2-x^2)}$,Solving the differential equation,\frac{dy}{dx} = \frac{y^3}{2(xy^2-x^2)},"How can we solve the equation: $$\frac{dy}{dx} = \frac{y^3}{2(xy^2-x^2)}$$ I get the idea of dividing by $y^2$, But it doesn't become any more solvable (not homogenous). $$\frac{dy}{dx} = \frac{y}{2(x-\frac{x^2}{y^2})}$$ Substituting $\frac{x}{y} = t$ causes even more complications. I get an idea of the question to convert into homogenous, but cant form the equation. Please give me a hint!","How can we solve the equation: $$\frac{dy}{dx} = \frac{y^3}{2(xy^2-x^2)}$$ I get the idea of dividing by $y^2$, But it doesn't become any more solvable (not homogenous). $$\frac{dy}{dx} = \frac{y}{2(x-\frac{x^2}{y^2})}$$ Substituting $\frac{x}{y} = t$ causes even more complications. I get an idea of the question to convert into homogenous, but cant form the equation. Please give me a hint!",,"['ordinary-differential-equations', 'derivatives']"
18,Why is this function not a solution to this ODE?,Why is this function not a solution to this ODE?,,"If we consider the autonomous ODE: $\frac{dx}{dt} = f(x) = ax$, where $a$ is in $\mathbb{R}$, $x(0) = x_{0}$ then this has a unique solution for every initial condition $x(0) = x_{0}$ since $f$ is continuous and so is $\frac{df}{dx}$, namely: $x(t) = x_{0}e^{at}$. But if we just consider the initial condition $x(0) = 0$, fix a constant $b > 0$, and define a function: $z(t) = e^{a(t - b)}$    if $t > b$, and   $z(t) = 0$     if $t \leq b$. Now I differentiated and plugged it into the ODE (not realising there was a discontinuity at $t = b$) and found that it seemed to be a solution to the IVP ($x(0) = 0$) - if this was a solution to the IVP  for any choice of $b$, then it would violate uniqueness of solutions as we would have infinitely many choices of $b$ that we could make - so clearly it is not a solution to the IVP, since we already know $x(t) = 0$ for all $t$ is a solution to the IVP, and $z \neq x$. But why is $z(t)$ not a solution? Someone pointed out to me that it was because $z(t)$ was discontinuous at $t = b$ and was clearly not differentiable there - initially I found this satisfactory but then I looked at another example in my lecture notes: $\frac{dx}{dt} = x^2$, $x(0) = x_{0}$ The solution is $x(t) = \frac{x_{0}}{1 - x_{0}t}$, and it was said that the solution 'blows up' for $t = \frac{1}{x_{0}}$ as we are dividing by 0 there. But this solution is discontinuous and not differentiable at $t = \frac{1}{x_{0}}$ just like $z$, but is being recognised as a solution. Indeed, my lecture notes say the solution is only for 'finite time'. What does this mean? So do we say $z$ is a solution only for 'finite time'? What would/does this mean, that we ignore the point $t = b$? But then we still have the problem of this being a different solution to $x(t) = 0$. I am very confused by this and feel I must have misunderstood something somewhere. Hopefully I haven't been too vague or unclear - I'd be happy to clarify anything in the comments! Any help would be greatly appreciated! Thanks!","If we consider the autonomous ODE: $\frac{dx}{dt} = f(x) = ax$, where $a$ is in $\mathbb{R}$, $x(0) = x_{0}$ then this has a unique solution for every initial condition $x(0) = x_{0}$ since $f$ is continuous and so is $\frac{df}{dx}$, namely: $x(t) = x_{0}e^{at}$. But if we just consider the initial condition $x(0) = 0$, fix a constant $b > 0$, and define a function: $z(t) = e^{a(t - b)}$    if $t > b$, and   $z(t) = 0$     if $t \leq b$. Now I differentiated and plugged it into the ODE (not realising there was a discontinuity at $t = b$) and found that it seemed to be a solution to the IVP ($x(0) = 0$) - if this was a solution to the IVP  for any choice of $b$, then it would violate uniqueness of solutions as we would have infinitely many choices of $b$ that we could make - so clearly it is not a solution to the IVP, since we already know $x(t) = 0$ for all $t$ is a solution to the IVP, and $z \neq x$. But why is $z(t)$ not a solution? Someone pointed out to me that it was because $z(t)$ was discontinuous at $t = b$ and was clearly not differentiable there - initially I found this satisfactory but then I looked at another example in my lecture notes: $\frac{dx}{dt} = x^2$, $x(0) = x_{0}$ The solution is $x(t) = \frac{x_{0}}{1 - x_{0}t}$, and it was said that the solution 'blows up' for $t = \frac{1}{x_{0}}$ as we are dividing by 0 there. But this solution is discontinuous and not differentiable at $t = \frac{1}{x_{0}}$ just like $z$, but is being recognised as a solution. Indeed, my lecture notes say the solution is only for 'finite time'. What does this mean? So do we say $z$ is a solution only for 'finite time'? What would/does this mean, that we ignore the point $t = b$? But then we still have the problem of this being a different solution to $x(t) = 0$. I am very confused by this and feel I must have misunderstood something somewhere. Hopefully I haven't been too vague or unclear - I'd be happy to clarify anything in the comments! Any help would be greatly appreciated! Thanks!",,"['ordinary-differential-equations', 'proof-verification']"
19,Asymptotic Expansion or Perturbation of an Ordinary Differential Equation,Asymptotic Expansion or Perturbation of an Ordinary Differential Equation,,"Consider the following ordinary differential equation. $$u''+f(u)=0, \tag1$$ where $''$ stands for second derivative, and $f\in C^1(-\infty,\infty)$, $f(0)=0,\,f'(0)=1$.  Multiply both sides by $u'$ and integrate over $x$ from $a$ to $x$, $$\frac12u'(x)^2+F(u(x))=\frac12u'(a)^2+F(u(a))=\frac{c^2}2 \tag2$$ where $F(u)=\int_0^u f(t)dt$ and some positive constant $c$. Suppose $c$ is very small. It is plausible that $F(u)=\frac12u^2+O(u^3)$ and  $$\frac{c^2}2=\frac12u'(x)^2+F(u(x))\approx \frac12u'(x)^2+\frac12u^2$$ or as $u$ depends on both $c$ and $x$, $$u(c,x)\approx c\sin x, \tag3$$ and $$\lim_{c\rightarrow 0}\frac{u(c,x)}{c}=\sin x. \tag4$$ How does one rigorously justify this, perhaps in terms of asymptotic expansion and expand further in perturbative fashion?","Consider the following ordinary differential equation. $$u''+f(u)=0, \tag1$$ where $''$ stands for second derivative, and $f\in C^1(-\infty,\infty)$, $f(0)=0,\,f'(0)=1$.  Multiply both sides by $u'$ and integrate over $x$ from $a$ to $x$, $$\frac12u'(x)^2+F(u(x))=\frac12u'(a)^2+F(u(a))=\frac{c^2}2 \tag2$$ where $F(u)=\int_0^u f(t)dt$ and some positive constant $c$. Suppose $c$ is very small. It is plausible that $F(u)=\frac12u^2+O(u^3)$ and  $$\frac{c^2}2=\frac12u'(x)^2+F(u(x))\approx \frac12u'(x)^2+\frac12u^2$$ or as $u$ depends on both $c$ and $x$, $$u(c,x)\approx c\sin x, \tag3$$ and $$\lim_{c\rightarrow 0}\frac{u(c,x)}{c}=\sin x. \tag4$$ How does one rigorously justify this, perhaps in terms of asymptotic expansion and expand further in perturbative fashion?",,"['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
20,What is wrong with the solution?,What is wrong with the solution?,,"For given conditions, $\frac{\partial u}{\partial t}=0$ at $x=0$ & $u=0$ at $t=0$ . The solution of the differential equation $$\frac{\partial^2u}{\partial x\partial t}=e^{-t}\cos x$$ is given by a) $(1-e^{-t})\sin x\qquad $ b) $(-e^{-t})\sin x+g(x)\qquad $ c) $(1+e^{-t})\cos x\qquad $ d) $(-e^{-t})\sin x\qquad $ my try: $$\frac{\partial}{\partial x}\left(\frac{\partial u}{\partial t}\right)=e^{-t}\cos x$$ $$\int \partial \left(\frac{\partial u}{\partial t}\right)=\int e^{-t}\cos x\ \partial x$$ $$\frac{\partial u}{\partial t}  =e^{-t}\sin x+f(t)$$ putting $x=0$ & $\frac{\partial u}{\partial t} =0$ i get $f(t)=0$ so $$\frac{\partial u}{\partial t}  =e^{-t}\sin x$$ $$\int \partial u =\int e^{-t}\sin x\partial t$$ $$u=-e^{-t}\sin x+g(x)$$ now, putting $t=0$ & $u=0$ i get $g(x)=\sin x$ , so the solution is $$u=-e^{-t}\sin x+\sin x$$ $$=(1-e^{-t})\sin x$$ so the option is (a) but my book says the correct option is (d) i don't know why. please explain where i am wrong or what should be the right answer?  thank you very much","For given conditions, at & at . The solution of the differential equation is given by a) b) c) d) my try: putting & i get so now, putting & i get , so the solution is so the option is (a) but my book says the correct option is (d) i don't know why. please explain where i am wrong or what should be the right answer?  thank you very much",\frac{\partial u}{\partial t}=0 x=0 u=0 t=0 \frac{\partial^2u}{\partial x\partial t}=e^{-t}\cos x (1-e^{-t})\sin x\qquad  (-e^{-t})\sin x+g(x)\qquad  (1+e^{-t})\cos x\qquad  (-e^{-t})\sin x\qquad  \frac{\partial}{\partial x}\left(\frac{\partial u}{\partial t}\right)=e^{-t}\cos x \int \partial \left(\frac{\partial u}{\partial t}\right)=\int e^{-t}\cos x\ \partial x \frac{\partial u}{\partial t}  =e^{-t}\sin x+f(t) x=0 \frac{\partial u}{\partial t} =0 f(t)=0 \frac{\partial u}{\partial t}  =e^{-t}\sin x \int \partial u =\int e^{-t}\sin x\partial t u=-e^{-t}\sin x+g(x) t=0 u=0 g(x)=\sin x u=-e^{-t}\sin x+\sin x =(1-e^{-t})\sin x,"['calculus', 'ordinary-differential-equations', 'partial-derivative']"
21,Both sides differentiation expression with integral,Both sides differentiation expression with integral,,"I have a problem with derivation of some function. This is related to computing the Poincare Map of Logistic Population model with Periodic harvesting. This function is: $\phi(t,x_0) = x_0 + \int_{0}^{t} f(s,\phi(s,x_0)) ds $, where $\phi(t,x_0)$ is a function: $\mathbb{R}  \times \mathbb{R} \rightarrow \mathbb{R}$. I'm trying to differentiate  equation of this function both sides  with respect to $x_{0}$. In book from this equation comes from the result of this differentiation is: $\frac{\partial \phi}{\partial x_0} (t,x_0) = 1 + \int_0^{t} \frac{\partial f}{\partial x_0} (s, \phi(s,x_0)) \cdot \frac{\partial \phi}{\partial x_0} (s, x_0)ds$. I can't understand expression under the integral. In this book it is explained that it is clear from chain rule .Could someone explain to me this transformation in easy way? I will be grateful for your help Best regards","I have a problem with derivation of some function. This is related to computing the Poincare Map of Logistic Population model with Periodic harvesting. This function is: $\phi(t,x_0) = x_0 + \int_{0}^{t} f(s,\phi(s,x_0)) ds $, where $\phi(t,x_0)$ is a function: $\mathbb{R}  \times \mathbb{R} \rightarrow \mathbb{R}$. I'm trying to differentiate  equation of this function both sides  with respect to $x_{0}$. In book from this equation comes from the result of this differentiation is: $\frac{\partial \phi}{\partial x_0} (t,x_0) = 1 + \int_0^{t} \frac{\partial f}{\partial x_0} (s, \phi(s,x_0)) \cdot \frac{\partial \phi}{\partial x_0} (s, x_0)ds$. I can't understand expression under the integral. In this book it is explained that it is clear from chain rule .Could someone explain to me this transformation in easy way? I will be grateful for your help Best regards",,"['integration', 'ordinary-differential-equations', 'derivatives']"
22,Differential equation (Brachistochrone problem),Differential equation (Brachistochrone problem),,"I'm really only supposed to solve the differential equation $(1+(y')^2)y=k^2.$ I haven't encountered any problem with $(y')^2$. How do you start with a problem like this, I did try googling it but all I got was basic differential equation, should I substitute for something? Grateful for any help at this point!","I'm really only supposed to solve the differential equation $(1+(y')^2)y=k^2.$ I haven't encountered any problem with $(y')^2$. How do you start with a problem like this, I did try googling it but all I got was basic differential equation, should I substitute for something? Grateful for any help at this point!",,['ordinary-differential-equations']
23,How to formally prove that equilibrium cannot be reached in finite time?,How to formally prove that equilibrium cannot be reached in finite time?,,"Let $v:\left[0,1\right]\to\mathbb{R}$ be a Lipschitz continuous velocity, and let $t\mapsto u(t,x)$ be the (unique) solution of the initial problem $$x'(t)=v(x(t)),\;\;\; x(0)=x$$ defined on its maximal interval of existence, say $I_x$. Consider the flow $(\Phi_t)_{t\geq 0}$ given by $$\Phi_t(x)= \begin{cases}  u(t,x), &t\in I_x,\\ u(\sup I_x,\,x), &\text{otherwise}. \end{cases} $$ Now define the first hitting time $$\tau_y(x)=\inf\{t\geq 0:\, \Phi_t(x)=y\}\;\;\;(\text{with the convention: }\inf\emptyset:=\infty).$$ I would like to prove that if $y\in\left[0,1\right]$ satisfies $v(y)=0$ then $\tau_y(x)=\infty$ for all $x\in\left[0,1\right]\backslash\{y\}$. I have a problem with a proof of this. Let $y\in\left[0,1\right]$, and think about the case where $x<y$. Roughly speaking, I see that the Lipschitz continuity of $v$ forces that $s\mapsto\frac{d}{ds} \Phi_s(x)$ tends to $0$ as $s\mapsto \Phi_s(x)$ gets closer and closer to $y$. However, I have no idea how to formally show that $\Phi_s(x)=y$ cannot happen for some $s>0$. I would be grateful for any hints.","Let $v:\left[0,1\right]\to\mathbb{R}$ be a Lipschitz continuous velocity, and let $t\mapsto u(t,x)$ be the (unique) solution of the initial problem $$x'(t)=v(x(t)),\;\;\; x(0)=x$$ defined on its maximal interval of existence, say $I_x$. Consider the flow $(\Phi_t)_{t\geq 0}$ given by $$\Phi_t(x)= \begin{cases}  u(t,x), &t\in I_x,\\ u(\sup I_x,\,x), &\text{otherwise}. \end{cases} $$ Now define the first hitting time $$\tau_y(x)=\inf\{t\geq 0:\, \Phi_t(x)=y\}\;\;\;(\text{with the convention: }\inf\emptyset:=\infty).$$ I would like to prove that if $y\in\left[0,1\right]$ satisfies $v(y)=0$ then $\tau_y(x)=\infty$ for all $x\in\left[0,1\right]\backslash\{y\}$. I have a problem with a proof of this. Let $y\in\left[0,1\right]$, and think about the case where $x<y$. Roughly speaking, I see that the Lipschitz continuity of $v$ forces that $s\mapsto\frac{d}{ds} \Phi_s(x)$ tends to $0$ as $s\mapsto \Phi_s(x)$ gets closer and closer to $y$. However, I have no idea how to formally show that $\Phi_s(x)=y$ cannot happen for some $s>0$. I would be grateful for any hints.",,"['ordinary-differential-equations', 'dynamical-systems']"
24,"Stability of higher order ODE's, positive coefficients proof","Stability of higher order ODE's, positive coefficients proof",,"Given some linear constant coefficient DE with: $$ (a_0D^n+a_1D^{n-1}+...+a_{n-1}D+a_n)y=f(t) \tag{1} $$ Without loss of generality, we may assume that $a_0>0$, then:   $$ \textit{(1) is stable} \Rightarrow a_0,...,a_n >0 $$ I've found the above statement on the MIT online course on ODE's. Since there is no analytic way of finding the roots of an n-order polynomial, I've tried proving it by rewriting the characteristic equation in Horner's scheme for some root $Re{\lambda_i}<0$ (since it's stable, all roots must have negative real part) - but without success. I also lack any intuition for why this statement should be true. Any hints on how to prove it?","Given some linear constant coefficient DE with: $$ (a_0D^n+a_1D^{n-1}+...+a_{n-1}D+a_n)y=f(t) \tag{1} $$ Without loss of generality, we may assume that $a_0>0$, then:   $$ \textit{(1) is stable} \Rightarrow a_0,...,a_n >0 $$ I've found the above statement on the MIT online course on ODE's. Since there is no analytic way of finding the roots of an n-order polynomial, I've tried proving it by rewriting the characteristic equation in Horner's scheme for some root $Re{\lambda_i}<0$ (since it's stable, all roots must have negative real part) - but without success. I also lack any intuition for why this statement should be true. Any hints on how to prove it?",,"['ordinary-differential-equations', 'stability-in-odes']"
25,inhomogeneous heat equation with mixed boundary conditons,inhomogeneous heat equation with mixed boundary conditons,,"Solve $$U_{t}=U_{xx}+u$$ with mixed boundary conditions $$U_x(0,t)=0, U(l,t)=0$$ and initial condition $$U(x,0)=\varphi(x)$$ I know that I have to use separation of variables and I have an idea of how to do it when its either just Dirichlet or just Neumann but both together and with a source I have no idea any help would be appreciated.","Solve $$U_{t}=U_{xx}+u$$ with mixed boundary conditions $$U_x(0,t)=0, U(l,t)=0$$ and initial condition $$U(x,0)=\varphi(x)$$ I know that I have to use separation of variables and I have an idea of how to do it when its either just Dirichlet or just Neumann but both together and with a source I have no idea any help would be appreciated.",,"['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
26,Picard-Lindelöf Theorem application,Picard-Lindelöf Theorem application,,"I'm really lost here guys. I'd appreciate it if you could help. Consider the differential equation $$ \frac{dy}{dt} = f(t, y) \tag{1} $$ with $f$ satisfying the conditions of the Picard-Lindelöf Theorem . Also, $y_1(t) = 3\ ,\ t \in \mathbb{R}$ is a solution of $(1)$ . What can we conclude about the solution $y(t)$ which satisfies the initial condition $y(0) = 1$ ?","I'm really lost here guys. I'd appreciate it if you could help. Consider the differential equation with satisfying the conditions of the Picard-Lindelöf Theorem . Also, is a solution of . What can we conclude about the solution which satisfies the initial condition ?","
\frac{dy}{dt} = f(t, y) \tag{1}
 f y_1(t) = 3\ ,\ t \in \mathbb{R} (1) y(t) y(0) = 1","['ordinary-differential-equations', 'initial-value-problems']"
27,Laplace Transform for Solving Differential Equation,Laplace Transform for Solving Differential Equation,,"I solved the following task, but since I am new in this field I need to check if it is correct or if there is anything I am missing or doing wrong. Task : Solve differential equation using Laplace transform. $$y^{''}-y^{'}-2y=2t+1 \\y^\;(0)=1, \; y^{'}(0)=2$$ First i got the following equation : $$\mathcal{L}(y)=\frac{s^3+s^2+s+2}{s^2(s^2-s-2)}$$ Now this is the part that was kinda tricky. When i fractioned equation i got this : $$\frac{A}{s}+\frac{B}{s^2}+\frac{C}{s+1}+\frac{D}{s-2}$$ The fractions were : $$A=0,\;B=-1,\;C=1,\;D=0$$ Finally the result is : $$y(t)=-t + e^{-t}$$","I solved the following task, but since I am new in this field I need to check if it is correct or if there is anything I am missing or doing wrong. Task : Solve differential equation using Laplace transform. $$y^{''}-y^{'}-2y=2t+1 \\y^\;(0)=1, \; y^{'}(0)=2$$ First i got the following equation : $$\mathcal{L}(y)=\frac{s^3+s^2+s+2}{s^2(s^2-s-2)}$$ Now this is the part that was kinda tricky. When i fractioned equation i got this : $$\frac{A}{s}+\frac{B}{s^2}+\frac{C}{s+1}+\frac{D}{s-2}$$ The fractions were : $$A=0,\;B=-1,\;C=1,\;D=0$$ Finally the result is : $$y(t)=-t + e^{-t}$$",,"['ordinary-differential-equations', 'laplace-transform', 'fractions']"
28,Show that there aren't negative eigenvalues.,Show that there aren't negative eigenvalues.,,"I've been trying to solve this Sturm-Liouville theory problem. Show that the problem: $$\left\{\begin{matrix} y''+(x+\lambda)y = 0\\ y(0)=0\\y(1)=0\end{matrix}\right.$$ doesn't have nontrivial solutions if $\lambda<0$. It actually asks to show that the operator $L[y]:=y''+xy$ just doesn't have negative eigenvalues. I've tried the trick of multiplying the equation by $y$, and integrating, but it doesn't seem to work. An alternative approach or something will be thanked.","I've been trying to solve this Sturm-Liouville theory problem. Show that the problem: $$\left\{\begin{matrix} y''+(x+\lambda)y = 0\\ y(0)=0\\y(1)=0\end{matrix}\right.$$ doesn't have nontrivial solutions if $\lambda<0$. It actually asks to show that the operator $L[y]:=y''+xy$ just doesn't have negative eigenvalues. I've tried the trick of multiplying the equation by $y$, and integrating, but it doesn't seem to work. An alternative approach or something will be thanked.",,"['ordinary-differential-equations', 'sturm-liouville']"
29,Solve the non-linear differential equation,Solve the non-linear differential equation,,"I have been trying to solve the following differential equation: $$ \dot{y} = \frac{3x^2}{y-x^2+1}$$ Substituting $u=y-x^2+1$ we get $\dot{u}=\dot{y}-2x$ we get $\dot{u}=\frac{3x^2}{y}-2x$. But I can't get any further now, i have tried substituting $k=u-x^2$, but it doesn't help.","I have been trying to solve the following differential equation: $$ \dot{y} = \frac{3x^2}{y-x^2+1}$$ Substituting $u=y-x^2+1$ we get $\dot{u}=\dot{y}-2x$ we get $\dot{u}=\frac{3x^2}{y}-2x$. But I can't get any further now, i have tried substituting $k=u-x^2$, but it doesn't help.",,['ordinary-differential-equations']
30,Trigonometric functions and complex numbers,Trigonometric functions and complex numbers,,"I solving the inverse Laplace transform using the method of Heaviside. This is part of the problem: I understand the division between complex numbers and that $e^{it} = Cos(t) + iSin(t)$, but I got stuck in the third row.I developed the product but I don't quite understand how it came to $-Cos(t)+ \frac{1}{2}Sin(t)-Cos(t)+ \frac{1}{2}Sin(t)$","I solving the inverse Laplace transform using the method of Heaviside. This is part of the problem: I understand the division between complex numbers and that $e^{it} = Cos(t) + iSin(t)$, but I got stuck in the third row.I developed the product but I don't quite understand how it came to $-Cos(t)+ \frac{1}{2}Sin(t)-Cos(t)+ \frac{1}{2}Sin(t)$",,"['ordinary-differential-equations', 'trigonometry', 'complex-numbers', 'laplace-transform']"
31,prove the result of a Laplace transformation,prove the result of a Laplace transformation,,"I have to prove the next problem $$\mathcal{L}  \left(\int_{0}^{t}\frac{1-e^{-u}}{u}du,s\right) = \frac{1}{s}\log\left(1+\frac{1}{s}\right)$$ I'm quite new in the subject and I have troubles with this one. There's no need to put the step by step, just the initial ones to know that I'm going in the right way  (but if you want to,that would be great) Thanks!","I have to prove the next problem $$\mathcal{L}  \left(\int_{0}^{t}\frac{1-e^{-u}}{u}du,s\right) = \frac{1}{s}\log\left(1+\frac{1}{s}\right)$$ I'm quite new in the subject and I have troubles with this one. There's no need to put the step by step, just the initial ones to know that I'm going in the right way  (but if you want to,that would be great) Thanks!",,"['ordinary-differential-equations', 'laplace-transform', 'integers', 'laplace-method']"
32,Formula for the nth Derivative of a Differential Equation,Formula for the nth Derivative of a Differential Equation,,"I have the differential equation $$f'(x)=2xf(x)$$ With the initial condition that $f(0)=1$ I need to prove that the nth derivative evaluated at zero is equivalent to $n!/(n/2)!$ for even n. $$\text{Show  } f^{(n)}(0)=\frac{n!}{(n/2)!} \text{ for even values of n}$$ I have tried using the general Leibniz rule, which gives me that $$\left[ \sum_{k=0}^c \frac{c!}{k!(c-k)!}(2x)^{(k)}(f^{(c-k)}(x)) \right]_{x=0}=\frac{c!}{(c/2)!}$$  But then I don't know how to proceed from here, any guidance would be appreciated! Note: I know the solution is $e^{x^2}$ but the problem involves comparing the series expansion and differntial equation, so I need to be able to solve this part.","I have the differential equation $$f'(x)=2xf(x)$$ With the initial condition that $f(0)=1$ I need to prove that the nth derivative evaluated at zero is equivalent to $n!/(n/2)!$ for even n. $$\text{Show  } f^{(n)}(0)=\frac{n!}{(n/2)!} \text{ for even values of n}$$ I have tried using the general Leibniz rule, which gives me that $$\left[ \sum_{k=0}^c \frac{c!}{k!(c-k)!}(2x)^{(k)}(f^{(c-k)}(x)) \right]_{x=0}=\frac{c!}{(c/2)!}$$  But then I don't know how to proceed from here, any guidance would be appreciated! Note: I know the solution is $e^{x^2}$ but the problem involves comparing the series expansion and differntial equation, so I need to be able to solve this part.",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
33,Pendulum loss/gain of time per day given : $\ddot{\phi}+\frac{g}{l}\sin{\phi}=0$ and max displacement $5^{\angle}$,Pendulum loss/gain of time per day given :  and max displacement,\ddot{\phi}+\frac{g}{l}\sin{\phi}=0 5^{\angle},"Here is what i am given: The oscillations of a pendulum are described by the equation: $$\ddot{\phi}+\frac{g}{l}\sin{\phi}=0$$ where $\phi$ is the angle between the pendulum and the vertical axis, $l$ is the length of the pendulum, and $g$ is the acceleration of gravity. The pendulum of a longcase clock swings to a maximum angle of $5^{\angle}$ from the vertical. How many seconds does the clock gain or lose each day (1day=86400sec) if the clock is adjusted to keep perfect time when the angular swing is infintesimaly small? Hints: (1) Since the swing angle in the problem are small, simplify the equation for the pendulum while still keeping it nonlinear. (2)The relavite loss or gain of the clock is proportional to the relative change in the frequencies of the oscillations. (3) Don't forget to convert angles to radians. So i'm having trouble with the last few steps of the problem. I'll catch you up on where i am. Since the angle is small i used the first 2 terms of the MacLauren series to estimate $\sin\phi=\phi-\frac{\phi^3}{6}$ so our problem becomes  $$\ddot{\phi}-\omega^2(\frac{\phi^3}{6}-\phi)=0$$ where $\omega^2=\frac{g}{l}$ and $\omega$ is the angular frequency in $s^{-1}$. I dropped $\omega^2$ for now and will introduce it at the end since it is a constant and inserted a small parameter $\epsilon$. Rearranging gives: $$\ddot{\phi}+\phi=\epsilon\frac{\phi^3}{6}$$ From here i followed the steps for a nonlinear oscillator (like the Van der Pol oscillator) where we look for a solution in the form $\phi(t)=a(t)\cos(t+\psi(t))$ with $\dot{\phi}(t)=-a(t)\sin(t+\psi{t})$ where a(t) is the amplitude which is a function of time $t$ and $\psi(t)$ is a (phase?) constant dependent on initial conditions. I'm skipping a lot of steps here but a vague description of what i did was: differentiate, substitute into original problem, solve for $a'(t)$ and $\psi'(t)$  and then take their averages over one period ($2\pi$). If you're familiar with the method of averaging and certain solution methods for the Van der Pol or Wayleigh oscillators you probably understand what i did.  I end up with  $a'_{avg}(t)=0$ so that $a(t)=A$ (some constant dependent on initial conditions) and  $$\psi_{avg}'(t)=-\epsilon\frac{(a(t))^2}{16}$$.  This is a separable differential equation to which i find solution $$\psi(t)=\epsilon\frac{(a(t))^2}{16}(t+k)$$ where $k$ is an integration constant $k=\psi(0)$ (i think).  And since we determined that $a(t)=A$ i use this substition in $\psi$ also. Substituting this solution of $\psi$ back into our desired solution form gives $$\phi(t)=A\cos\left(t-\epsilon\frac{A^2}{16}(t+k)\right)$$ And since Simple Harmonic Oscillators generally have solutions in the form of $x(t)=A\cos(\omega t)$ i reinsert $\omega$ inside the solution to get $$\phi(t)=A\cos\left(\omega(t-\epsilon\frac{A^2}{16}(t+k))\right)$$ So that is where i am... I'm very UNconfident about those last couple steps and assumptions. I don't know how i'm supposed to find the loss (or gain) in time over the course of day. I'm also a bit unsure about that integration constant $k$. One of my classmates said that $k=0$ but i don't know how he got that and he can't justify it. I'm not sure how to use the given information to find a solution. I'm leaning toward this: Since we are given that ""the clock swings to a maximum angle of $5^{\angle}$ from the vertical"" it must be that $\phi(0)=5^{\angle}$ since the clock must have its maximum amplitude at $t=0$. But then i run into the problem with the ""$k$"" so i think i made some incorrect assumptions in the last few steps. Also, i'm not sure how that would help me find the time gain/loss per day. Is anyone familiar with this equation and this method for solving it?","Here is what i am given: The oscillations of a pendulum are described by the equation: $$\ddot{\phi}+\frac{g}{l}\sin{\phi}=0$$ where $\phi$ is the angle between the pendulum and the vertical axis, $l$ is the length of the pendulum, and $g$ is the acceleration of gravity. The pendulum of a longcase clock swings to a maximum angle of $5^{\angle}$ from the vertical. How many seconds does the clock gain or lose each day (1day=86400sec) if the clock is adjusted to keep perfect time when the angular swing is infintesimaly small? Hints: (1) Since the swing angle in the problem are small, simplify the equation for the pendulum while still keeping it nonlinear. (2)The relavite loss or gain of the clock is proportional to the relative change in the frequencies of the oscillations. (3) Don't forget to convert angles to radians. So i'm having trouble with the last few steps of the problem. I'll catch you up on where i am. Since the angle is small i used the first 2 terms of the MacLauren series to estimate $\sin\phi=\phi-\frac{\phi^3}{6}$ so our problem becomes  $$\ddot{\phi}-\omega^2(\frac{\phi^3}{6}-\phi)=0$$ where $\omega^2=\frac{g}{l}$ and $\omega$ is the angular frequency in $s^{-1}$. I dropped $\omega^2$ for now and will introduce it at the end since it is a constant and inserted a small parameter $\epsilon$. Rearranging gives: $$\ddot{\phi}+\phi=\epsilon\frac{\phi^3}{6}$$ From here i followed the steps for a nonlinear oscillator (like the Van der Pol oscillator) where we look for a solution in the form $\phi(t)=a(t)\cos(t+\psi(t))$ with $\dot{\phi}(t)=-a(t)\sin(t+\psi{t})$ where a(t) is the amplitude which is a function of time $t$ and $\psi(t)$ is a (phase?) constant dependent on initial conditions. I'm skipping a lot of steps here but a vague description of what i did was: differentiate, substitute into original problem, solve for $a'(t)$ and $\psi'(t)$  and then take their averages over one period ($2\pi$). If you're familiar with the method of averaging and certain solution methods for the Van der Pol or Wayleigh oscillators you probably understand what i did.  I end up with  $a'_{avg}(t)=0$ so that $a(t)=A$ (some constant dependent on initial conditions) and  $$\psi_{avg}'(t)=-\epsilon\frac{(a(t))^2}{16}$$.  This is a separable differential equation to which i find solution $$\psi(t)=\epsilon\frac{(a(t))^2}{16}(t+k)$$ where $k$ is an integration constant $k=\psi(0)$ (i think).  And since we determined that $a(t)=A$ i use this substition in $\psi$ also. Substituting this solution of $\psi$ back into our desired solution form gives $$\phi(t)=A\cos\left(t-\epsilon\frac{A^2}{16}(t+k)\right)$$ And since Simple Harmonic Oscillators generally have solutions in the form of $x(t)=A\cos(\omega t)$ i reinsert $\omega$ inside the solution to get $$\phi(t)=A\cos\left(\omega(t-\epsilon\frac{A^2}{16}(t+k))\right)$$ So that is where i am... I'm very UNconfident about those last couple steps and assumptions. I don't know how i'm supposed to find the loss (or gain) in time over the course of day. I'm also a bit unsure about that integration constant $k$. One of my classmates said that $k=0$ but i don't know how he got that and he can't justify it. I'm not sure how to use the given information to find a solution. I'm leaning toward this: Since we are given that ""the clock swings to a maximum angle of $5^{\angle}$ from the vertical"" it must be that $\phi(0)=5^{\angle}$ since the clock must have its maximum amplitude at $t=0$. But then i run into the problem with the ""$k$"" so i think i made some incorrect assumptions in the last few steps. Also, i'm not sure how that would help me find the time gain/loss per day. Is anyone familiar with this equation and this method for solving it?",,"['ordinary-differential-equations', 'physics', 'perturbation-theory']"
34,"Existence of periodic orbit of the ODE system $\dot{r}=r-r^3 \cos^2(\theta),\,\dot{\theta}= 1$",Existence of periodic orbit of the ODE system,"\dot{r}=r-r^3 \cos^2(\theta),\,\dot{\theta}= 1","Consider the system of ODEs (in polar coordinates):   $$\dot{r}=r-r^3 \cos^2(\theta)$$   $$\dot{\theta}= 1$$ If we take $r_1 = \frac{1}{4}$ then $\dot{r}> 0$, and if we take $r_2 = \frac{3}{2}$ then $\dot{r}<0$, so $S:=\{r\in \mathbb{R}:r_1 \le r\le r_2\}$ should be a positively invariant set. Thus there should be a periodic orbit in $S$. But either I'm mistaken or not, and if not then what does the orbit look like?","Consider the system of ODEs (in polar coordinates):   $$\dot{r}=r-r^3 \cos^2(\theta)$$   $$\dot{\theta}= 1$$ If we take $r_1 = \frac{1}{4}$ then $\dot{r}> 0$, and if we take $r_2 = \frac{3}{2}$ then $\dot{r}<0$, so $S:=\{r\in \mathbb{R}:r_1 \le r\le r_2\}$ should be a positively invariant set. Thus there should be a periodic orbit in $S$. But either I'm mistaken or not, and if not then what does the orbit look like?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
35,Finding the general solution of a non-homogeneous differential equation when three of its solutions are given,Finding the general solution of a non-homogeneous differential equation when three of its solutions are given,,"I've been given three solutions of a non-homogeneous differential equation: $$y_1(x)=1+e^{x^2},\;y_2(x)=1+xe^{x^2},\;y_3(x)=(1+x)e^{x^2}-1$$ I'm supposed to find the general solution of that differential equation. $[y''+p(x)y'+q(x)=r(x)]$ Now, $y_4=y_3-y_1$ and $y_5=y_3-y_2$ would give me solutions of the corresponding homogeneous differential equation. $[y''+p(x)y'+q(x)=0]$ So, I write the general solution of the homogeneous equation as $y=c_1y_4+c_2y_5$, and then proceed to differentiate the solution twice to remove $c_1$ and $c_2$. I hope to get the homogeneous equation from there, and then substituting one of the above three given solutions, I can find $r(x)$, and then using ' Variation of Parameters ', can find the general solution of the non-homogeneous equation. Problem is, I get a very ugly equation when I differentiate that general solution twice, in terms of $y_4$, $y_5$ and their derivatives. Substituting actual functions of $y_4$ and $y_5$ only makes it uglier. Cuz I don't see any cancellations happening. Is there a simpler method? EDIT: Please tell me if this approach is correct. The solution of homogeneous equation is $y=c_1y_4+c_2y_5$. I can write the solution of non-homogeneous equation as $y=c_1y_4+c_2y_5+y_p$, where $y_p$ is a particular solution of the non-homogeneous equation. The given three solutions act as particular solutions and I can just substitute one of them for $y_p$ and get my general solution.","I've been given three solutions of a non-homogeneous differential equation: $$y_1(x)=1+e^{x^2},\;y_2(x)=1+xe^{x^2},\;y_3(x)=(1+x)e^{x^2}-1$$ I'm supposed to find the general solution of that differential equation. $[y''+p(x)y'+q(x)=r(x)]$ Now, $y_4=y_3-y_1$ and $y_5=y_3-y_2$ would give me solutions of the corresponding homogeneous differential equation. $[y''+p(x)y'+q(x)=0]$ So, I write the general solution of the homogeneous equation as $y=c_1y_4+c_2y_5$, and then proceed to differentiate the solution twice to remove $c_1$ and $c_2$. I hope to get the homogeneous equation from there, and then substituting one of the above three given solutions, I can find $r(x)$, and then using ' Variation of Parameters ', can find the general solution of the non-homogeneous equation. Problem is, I get a very ugly equation when I differentiate that general solution twice, in terms of $y_4$, $y_5$ and their derivatives. Substituting actual functions of $y_4$ and $y_5$ only makes it uglier. Cuz I don't see any cancellations happening. Is there a simpler method? EDIT: Please tell me if this approach is correct. The solution of homogeneous equation is $y=c_1y_4+c_2y_5$. I can write the solution of non-homogeneous equation as $y=c_1y_4+c_2y_5+y_p$, where $y_p$ is a particular solution of the non-homogeneous equation. The given three solutions act as particular solutions and I can just substitute one of them for $y_p$ and get my general solution.",,['ordinary-differential-equations']
36,Solving a linear system of ODEs with repeated eigenvalues but distinct eigenvectors.,Solving a linear system of ODEs with repeated eigenvalues but distinct eigenvectors.,,"Suppose I have a linear system of ODEs given by the system: $$\dot{\vec{x}}= A\vec{x}$$ where $A$ is an $(n\times n)$ matrix and $\vec{x}$ is a $(n\times 1)$ column vector. Now suppose that the matrix A has repeated eigenvalues but has $n$ distinct linearly independent eigenvectors . Is there a simple way to solve the system? The link below (to wolfram alpha) shows a $(6\times 6)$ matrix where this is the case. See here for an example of a matrix where this happens . I know that if there are $n$ distinct eigenvalues $\lambda_1, \ldots , \lambda_n$, the general solution is given by $$\vec{x} = \big(c_1 v_1 e^{\lambda_1t}+\ldots+c_nv_ne^{\lambda_n t}\big)$$ where $v_i$ is the corresponding eigenvector for $\lambda_i$ and $c_i$ is a constant. Also, I know what to do when there are repeated eigenvalues, with corresponding repeated eigenvectors. Is there an a similar way to solve the system with repeated eigenvalues but distinct eigenvectors? Thanks.","Suppose I have a linear system of ODEs given by the system: $$\dot{\vec{x}}= A\vec{x}$$ where $A$ is an $(n\times n)$ matrix and $\vec{x}$ is a $(n\times 1)$ column vector. Now suppose that the matrix A has repeated eigenvalues but has $n$ distinct linearly independent eigenvectors . Is there a simple way to solve the system? The link below (to wolfram alpha) shows a $(6\times 6)$ matrix where this is the case. See here for an example of a matrix where this happens . I know that if there are $n$ distinct eigenvalues $\lambda_1, \ldots , \lambda_n$, the general solution is given by $$\vec{x} = \big(c_1 v_1 e^{\lambda_1t}+\ldots+c_nv_ne^{\lambda_n t}\big)$$ where $v_i$ is the corresponding eigenvector for $\lambda_i$ and $c_i$ is a constant. Also, I know what to do when there are repeated eigenvalues, with corresponding repeated eigenvectors. Is there an a similar way to solve the system with repeated eigenvalues but distinct eigenvectors? Thanks.",,['ordinary-differential-equations']
37,The differential equation $y''-q(x)y=0.$,The differential equation,y''-q(x)y=0.,"Let   $$y''-q(x)y=0$$be a differential equation with initial conditions  on $0\leq x<\infty,$ as $y(0)=1,y'(0)=1$ where $q(x)$ is a positive monotonically increasing continuous function. Then which of the following are true? $y(x)\rightarrow\infty$ as $x\rightarrow\infty$. $y'(x)\rightarrow\infty$ as $x\rightarrow\infty$. $y(x)$ has finitely many zeros in $[0,\infty)$. $y(x)$ has infinitely many zeros in $[0,\infty)$. Please don't mind, actually I am new in differential equation. I only know that by Picard's theorem the above differential equation has the unique solution, but I don't know what is the solution as I tried by direct hit and trial method. According to me the solution of above differential equation will be some thing in exponential form, so according to me its answer will be $a$, $b$, $c$. But I don't know the exact method. Please help me to solve the above problem. Thanks in advance.","Let   $$y''-q(x)y=0$$be a differential equation with initial conditions  on $0\leq x<\infty,$ as $y(0)=1,y'(0)=1$ where $q(x)$ is a positive monotonically increasing continuous function. Then which of the following are true? $y(x)\rightarrow\infty$ as $x\rightarrow\infty$. $y'(x)\rightarrow\infty$ as $x\rightarrow\infty$. $y(x)$ has finitely many zeros in $[0,\infty)$. $y(x)$ has infinitely many zeros in $[0,\infty)$. Please don't mind, actually I am new in differential equation. I only know that by Picard's theorem the above differential equation has the unique solution, but I don't know what is the solution as I tried by direct hit and trial method. According to me the solution of above differential equation will be some thing in exponential form, so according to me its answer will be $a$, $b$, $c$. But I don't know the exact method. Please help me to solve the above problem. Thanks in advance.",,['ordinary-differential-equations']
38,Solve differential equation using 'variation of parameters',Solve differential equation using 'variation of parameters',,"Given the differential equation $L \ u = f$, with $$L \ u = a_2(t) \frac{d^2u}{dt^2}+a_1(t)\frac{du}{dt}+a_0(t)u$$ with $a_i(t)$ sufficiently smooth and $a_2(t) \neq 0$ for every $t$. Suppose that $u_0$ is a solution to the homogenenous equation $L \ u=0$, with $u_0(t)\neq0$ for every $t$.  How can I then use the variation of parameters method by Lagrange to find the solution $u =u_0v$ ? And how can I find the general solution if the equation isn't homogeneous? Update : I've substituted $u=u_0v$ into the equation and found a new $2^{nd}$ order differential equation of the form  $$f = \frac{d^2v}{dt^2}(a_2(t)u_0)+\frac{dv}{dt}(2a_2(t)u_0'+a_1(t)u_0)+va_0(t)u_0$$ which I'm supposed to solve for $w$ using $w=v'$. But I don't seem to be able to proceed as I'm not sure how to solve the system $$\begin{equation} \begin{split} w =&\ v'\\ w'a_2(t)u_0+w(2a_2(t)u_0'+a_1(t)u_0)+v(a_0(t)u_0)=&\ f\ \\ \end{split} \end{equation}$$","Given the differential equation $L \ u = f$, with $$L \ u = a_2(t) \frac{d^2u}{dt^2}+a_1(t)\frac{du}{dt}+a_0(t)u$$ with $a_i(t)$ sufficiently smooth and $a_2(t) \neq 0$ for every $t$. Suppose that $u_0$ is a solution to the homogenenous equation $L \ u=0$, with $u_0(t)\neq0$ for every $t$.  How can I then use the variation of parameters method by Lagrange to find the solution $u =u_0v$ ? And how can I find the general solution if the equation isn't homogeneous? Update : I've substituted $u=u_0v$ into the equation and found a new $2^{nd}$ order differential equation of the form  $$f = \frac{d^2v}{dt^2}(a_2(t)u_0)+\frac{dv}{dt}(2a_2(t)u_0'+a_1(t)u_0)+va_0(t)u_0$$ which I'm supposed to solve for $w$ using $w=v'$. But I don't seem to be able to proceed as I'm not sure how to solve the system $$\begin{equation} \begin{split} w =&\ v'\\ w'a_2(t)u_0+w(2a_2(t)u_0'+a_1(t)u_0)+v(a_0(t)u_0)=&\ f\ \\ \end{split} \end{equation}$$",,['ordinary-differential-equations']
39,What type of functional equation is this?,What type of functional equation is this?,,"I'm trying to solve the following functional equation $f\left(x\right)=A\mbox{ exp}\left\{ \int\frac{1}{f\left(x\right)x^{2}+Bx}dx\right\}$ where $f\left(x\right):\mathbb{R}_{+}\rightarrow\mathbb{R}_{\geq0}$, A and B are constants in $\mathbb{R}$. Does anybody recognize this type of functional equation so I can look up for the solution? Alternatively, does anybody know how to solve it? Or a suggestion on how to start tackling this problem? Thank you! After the initial comments I realized the problem is equivalent to solving the following first-order nonlinear ODE: $f^{\prime}f=\frac{1}{x^{2}}\left(\left(Ax+Bx^{2}\right)f^{\prime}+Bf\right)$ This equation seems similar to an Abel differential equation of the second kind, i.e. $ff^{\prime}=g\left(x\right)f+h\left(x\right)$ although it's not quite the same. If anybody has an idea how to deal with it I would appreciate it!","I'm trying to solve the following functional equation $f\left(x\right)=A\mbox{ exp}\left\{ \int\frac{1}{f\left(x\right)x^{2}+Bx}dx\right\}$ where $f\left(x\right):\mathbb{R}_{+}\rightarrow\mathbb{R}_{\geq0}$, A and B are constants in $\mathbb{R}$. Does anybody recognize this type of functional equation so I can look up for the solution? Alternatively, does anybody know how to solve it? Or a suggestion on how to start tackling this problem? Thank you! After the initial comments I realized the problem is equivalent to solving the following first-order nonlinear ODE: $f^{\prime}f=\frac{1}{x^{2}}\left(\left(Ax+Bx^{2}\right)f^{\prime}+Bf\right)$ This equation seems similar to an Abel differential equation of the second kind, i.e. $ff^{\prime}=g\left(x\right)f+h\left(x\right)$ although it's not quite the same. If anybody has an idea how to deal with it I would appreciate it!",,"['calculus', 'ordinary-differential-equations', 'exponential-function', 'indefinite-integrals', 'problem-solving']"
40,convolution: how can I show that $(y*f)'(t) = (y'*f)(t) + y(0)f(t)$,convolution: how can I show that,(y*f)'(t) = (y'*f)(t) + y(0)f(t),"I have the following math problem from my intro to dif. eq. class: (so don't just give an answer) If the convolution $$ (y*f)(t) = \int_0^t y(t-v)f(v)\,dv$$   then show that $$ (y*f)'(t) = (y'*f)(t)+y(0)f(t) $$ So here's what I did: \begin{align*}   (y*f)'(t) &= \frac{d}{dt} \def\i#1{\int_0^t #1 \, dv}  \i{y(t-v)f(v)}\\             &= \i{\frac{d}{dt}\bigl(y(t-v)f(v)\bigr)}  \tag 1\\             &= \i{y'(t-v)f(v) + y(t-v)f'(v)} \tag 2\\ &= \i{y'(t-v)f(v)} + \i{y(t-v)f'(v)}\\ &= y(t-t)f(t)-y(t-0)f(0) - \i{y(t-v)f'(v)} + \i{t(t-v)f'(v)}  \tag 3\\ &= y(0)f(t)-y(t)f(0) \tag 4  \end{align*} where in Leibniz's Rule product rule integration by parts integrals cancel. So this is clearly wrong according to the question so where do I go wrong? EDIT: Also how can I better understand the meaning of $$ (y'*f)(t) $$ in comparison to $$ (y*f)(t) $$","I have the following math problem from my intro to dif. eq. class: (so don't just give an answer) If the convolution $$ (y*f)(t) = \int_0^t y(t-v)f(v)\,dv$$   then show that $$ (y*f)'(t) = (y'*f)(t)+y(0)f(t) $$ So here's what I did: \begin{align*}   (y*f)'(t) &= \frac{d}{dt} \def\i#1{\int_0^t #1 \, dv}  \i{y(t-v)f(v)}\\             &= \i{\frac{d}{dt}\bigl(y(t-v)f(v)\bigr)}  \tag 1\\             &= \i{y'(t-v)f(v) + y(t-v)f'(v)} \tag 2\\ &= \i{y'(t-v)f(v)} + \i{y(t-v)f'(v)}\\ &= y(t-t)f(t)-y(t-0)f(0) - \i{y(t-v)f'(v)} + \i{t(t-v)f'(v)}  \tag 3\\ &= y(0)f(t)-y(t)f(0) \tag 4  \end{align*} where in Leibniz's Rule product rule integration by parts integrals cancel. So this is clearly wrong according to the question so where do I go wrong? EDIT: Also how can I better understand the meaning of $$ (y'*f)(t) $$ in comparison to $$ (y*f)(t) $$",,"['ordinary-differential-equations', 'convolution']"
41,Why this equation does not develop a boundary layer?,Why this equation does not develop a boundary layer?,,"The equation I am talking about is $$ \epsilon  y''(x)+y(x)+1=0,y(0)=0,y(1)=1 $$ The $+1$ is not essential as $y(x)$ can be decomposed into $1 + y_1$, but is kept here for a more direct comparison with the other example below. This equation takes a more innocent look if multiplied by $1/\epsilon $ that yields $y''(x) + 1/ \epsilon  y(x) + 1/\epsilon = 0$, sine/cosine function follows (the figure below shows solution with $\epsilon $ = 0.01, which corresponds to a period $2 \pi \sqrt{\epsilon} = 0.62$). Some may say as $\epsilon $ decreases, the frequency gets higher and the curve get steeper, but there is no boundary layer for at least two reasons: first, this is a global behavior; second, typical boundary layers has an exponential rate of change that limit it to a narraw region (then merges smoothly with the outer solution). The example to be compared against is one with the second term $y(x)$ replaced by $y'(x)$, clearly boundary layer develops (figure also uses $\epsilon = 0.01$). Buy why? The only distinction is there is no first order term in the first equation, but this argument appears to be very superficial.","The equation I am talking about is $$ \epsilon  y''(x)+y(x)+1=0,y(0)=0,y(1)=1 $$ The $+1$ is not essential as $y(x)$ can be decomposed into $1 + y_1$, but is kept here for a more direct comparison with the other example below. This equation takes a more innocent look if multiplied by $1/\epsilon $ that yields $y''(x) + 1/ \epsilon  y(x) + 1/\epsilon = 0$, sine/cosine function follows (the figure below shows solution with $\epsilon $ = 0.01, which corresponds to a period $2 \pi \sqrt{\epsilon} = 0.62$). Some may say as $\epsilon $ decreases, the frequency gets higher and the curve get steeper, but there is no boundary layer for at least two reasons: first, this is a global behavior; second, typical boundary layers has an exponential rate of change that limit it to a narraw region (then merges smoothly with the outer solution). The example to be compared against is one with the second term $y(x)$ replaced by $y'(x)$, clearly boundary layer develops (figure also uses $\epsilon = 0.01$). Buy why? The only distinction is there is no first order term in the first equation, but this argument appears to be very superficial.",,"['ordinary-differential-equations', 'perturbation-theory']"
42,solving ODE with power law solution,solving ODE with power law solution,,If I have this equation: $$ 3R' ^2  =-2R \frac{d^2R}{dt^2}$$ How can I show that If $$R=R_0 t^\alpha $$ then $$\alpha = 2/5$$,If I have this equation: $$ 3R' ^2  =-2R \frac{d^2R}{dt^2}$$ How can I show that If $$R=R_0 t^\alpha $$ then $$\alpha = 2/5$$,,['ordinary-differential-equations']
43,Solving differential equation - how to find inhomogeneous solution,Solving differential equation - how to find inhomogeneous solution,,"Given the following $\ y'= \frac{3y^2-x^2}{2xy}$ I need to tell if the equation is linear, which I think it is because: $\ y'= \frac{3y^2-x^2}{2xy} = \frac{3y}{2x}-\frac{x}{2y}$ Now I need to solve the equation with separation of variables which is only possible with substitution. So I substitute with $\ u(x)= \frac{y}{x}$ and do the following $\ y'= \frac{3y}{2x}-\frac{x}{2y} => y'=\frac{3y}{2x}-\frac{1}{2u(x)} = \frac{3}{2x}y-\frac{1}{2u(x)}$ so I can use separation of variables and get the homogeneous solution and the inhomogeneous. For the homogeneous I got $\ y_h=e^cx^{3/2} = Cx^{3/2} $ but I really don't know how to get the inhomogeneous solution because I don't exactly know what to do with the substitution, I am glad for help.","Given the following $\ y'= \frac{3y^2-x^2}{2xy}$ I need to tell if the equation is linear, which I think it is because: $\ y'= \frac{3y^2-x^2}{2xy} = \frac{3y}{2x}-\frac{x}{2y}$ Now I need to solve the equation with separation of variables which is only possible with substitution. So I substitute with $\ u(x)= \frac{y}{x}$ and do the following $\ y'= \frac{3y}{2x}-\frac{x}{2y} => y'=\frac{3y}{2x}-\frac{1}{2u(x)} = \frac{3}{2x}y-\frac{1}{2u(x)}$ so I can use separation of variables and get the homogeneous solution and the inhomogeneous. For the homogeneous I got $\ y_h=e^cx^{3/2} = Cx^{3/2} $ but I really don't know how to get the inhomogeneous solution because I don't exactly know what to do with the substitution, I am glad for help.",,"['calculus', 'analysis', 'ordinary-differential-equations']"
44,The Wronskian of vector valued functions vs. the Wronskian of real valued functions.,The Wronskian of vector valued functions vs. the Wronskian of real valued functions.,,"Case I: When discussing, for example, two solutions $\phi_1(t)$ and $\phi_2(t)$ of a second order homogeneous ode the Wronskian $$ W[\phi_1,\phi_2](t)=\begin{vmatrix}\phi_1(t)&\phi_2(t)\\\phi_1'(t)&\phi_2'(t)\end{vmatrix} $$ of the solutions might be mentioned. Case II: Likewise, when discussing two solutions $$ \phi_1(t)=\begin{bmatrix}x_1(t)\\y_1(t)\end{bmatrix}\qquad\phi_2(t)=\begin{bmatrix}x_2(t)\\y_2(t)\end{bmatrix} $$ of a system of two first order linear ode's the Wronskian $$ W[\phi_1,\phi_2](t)=\begin{vmatrix}x_1(t)&x_2(t)\\y_1(t)&y_2(t)\end{vmatrix} $$ of the solutions might be mentioned. Now, in the first case the Wronskian is an array of derivatives, while in the second, it is an array of vector components. As in both cases the Wronskian provides information about linear independence of the solutions, it would appear that the two ""versions"" (array of derivatives vs. array of components) are not really different but I am missing the connection between the two.  Can anyone provide some insight into that connection?","Case I: When discussing, for example, two solutions and of a second order homogeneous ode the Wronskian of the solutions might be mentioned. Case II: Likewise, when discussing two solutions of a system of two first order linear ode's the Wronskian of the solutions might be mentioned. Now, in the first case the Wronskian is an array of derivatives, while in the second, it is an array of vector components. As in both cases the Wronskian provides information about linear independence of the solutions, it would appear that the two ""versions"" (array of derivatives vs. array of components) are not really different but I am missing the connection between the two.  Can anyone provide some insight into that connection?","\phi_1(t) \phi_2(t) 
W[\phi_1,\phi_2](t)=\begin{vmatrix}\phi_1(t)&\phi_2(t)\\\phi_1'(t)&\phi_2'(t)\end{vmatrix}
 
\phi_1(t)=\begin{bmatrix}x_1(t)\\y_1(t)\end{bmatrix}\qquad\phi_2(t)=\begin{bmatrix}x_2(t)\\y_2(t)\end{bmatrix}
 
W[\phi_1,\phi_2](t)=\begin{vmatrix}x_1(t)&x_2(t)\\y_1(t)&y_2(t)\end{vmatrix}
","['linear-algebra', 'ordinary-differential-equations', 'wronskian']"
45,What is known about the asymptotics of Riccati's equation?,What is known about the asymptotics of Riccati's equation?,,"I'm interested in examining the asymptotic behavior of Riccati equations of the form $$ y'(x) = f(x) + g(x) y^2(x) $$ for $x \to \infty$. I've done some digging but I can't seem to find a simple explanation of how to go about computing something like this. If that's too general, then take this example $$ y'(x) = 10x e^{-2x}  - \frac{10}{x^2} y^2 $$ It seems ""obvious"" just by looking at this that as $x \to \infty$ we have $y'(x) \to 0$ so $y \to \text{constant}$. Some numerical simulations have shown that $y \sim 0.17$ for large $x$ but is it possible to show this (most importantly, the actual value of the constant) directly from the equation?","I'm interested in examining the asymptotic behavior of Riccati equations of the form $$ y'(x) = f(x) + g(x) y^2(x) $$ for $x \to \infty$. I've done some digging but I can't seem to find a simple explanation of how to go about computing something like this. If that's too general, then take this example $$ y'(x) = 10x e^{-2x}  - \frac{10}{x^2} y^2 $$ It seems ""obvious"" just by looking at this that as $x \to \infty$ we have $y'(x) \to 0$ so $y \to \text{constant}$. Some numerical simulations have shown that $y \sim 0.17$ for large $x$ but is it possible to show this (most importantly, the actual value of the constant) directly from the equation?",,"['ordinary-differential-equations', 'asymptotics']"
46,Normalisation of Bessel functions,Normalisation of Bessel functions,,I've done the integration by parts and obtained $$ \frac{-1}{\alpha^2} \int z^2 J J'$$ but I have no idea how to use Bessel's equation to simplify this as it only appears to get far more complicated.,I've done the integration by parts and obtained $$ \frac{-1}{\alpha^2} \int z^2 J J'$$ but I have no idea how to use Bessel's equation to simplify this as it only appears to get far more complicated.,,"['calculus', 'integration', 'analysis', 'ordinary-differential-equations', 'special-functions']"
47,The integral of $\frac {1}{x}$ and basic differential equations,The integral of  and basic differential equations,\frac {1}{x},"In introductory calculus, when solving basic separable ordinary differential equations, we often use the following ""fact"" $$\int \frac 1x \ dx = \ln|x| + C$$ This ""fact"", however, is slightly misleading. The actual integral is the following: $$\int \frac 1x \ dx = \begin{cases}\ln \left|x \right| + C_1 & x < 0\\ \ln \left|x \right| + C_2 & x > 0 \end{cases}$$ My question, is, simply, as far as basic (that is, AP Calculus level) ordinary differential equations go, will using the former integral ever yield imprecise solutions? EDIT : I have received three close votes on this question since it is ""opinion based"". I do not understand how---I have a concrete mathematical question, the answer to which is certainly not an ""opinion"". I am presuming this is resulting from my usage of the word ""false"" in describing the first equation, which I have changed to ""slightly misleading"". If there are any other problems with this question, I would certainly love to know in the comments.","In introductory calculus, when solving basic separable ordinary differential equations, we often use the following ""fact"" $$\int \frac 1x \ dx = \ln|x| + C$$ This ""fact"", however, is slightly misleading. The actual integral is the following: $$\int \frac 1x \ dx = \begin{cases}\ln \left|x \right| + C_1 & x < 0\\ \ln \left|x \right| + C_2 & x > 0 \end{cases}$$ My question, is, simply, as far as basic (that is, AP Calculus level) ordinary differential equations go, will using the former integral ever yield imprecise solutions? EDIT : I have received three close votes on this question since it is ""opinion based"". I do not understand how---I have a concrete mathematical question, the answer to which is certainly not an ""opinion"". I am presuming this is resulting from my usage of the word ""false"" in describing the first equation, which I have changed to ""slightly misleading"". If there are any other problems with this question, I would certainly love to know in the comments.",,"['calculus', 'integration', 'ordinary-differential-equations']"
48,Peano's Existence Theorem - Constructing Maximum and Minimum Solutions,Peano's Existence Theorem - Constructing Maximum and Minimum Solutions,,"Suppose that $D=[a, b] \times \mathbb{R}$ is a strip in $\mathbb{R}^2$, and that $f(x, y)$ is continuous and bounded on $D$. Let $(x_0, y_0)$ be an interior point of $D$ (i.e. $a< x_0 <b$). Prove that there are two integral curves $y=\phi_1 (x)$ and $y=\phi_2 (x)$, i.e. the maximum and the minimum solutions to the equation $\frac{dy}{dx}=f(x, y)$, such that (1). $\phi_1 (x_0)=\phi_2 (x_0)=y_0$ and $\phi_1 (x)\geq \phi_2 (x), \forall x\in [a, b]$; (2). the region $\{(x, y)\in \mathbb{R}^2 | a\leq x \leq b, \phi_2 (x)\leq y\leq \phi_1 (x)\}$ can be completely filled by integral curves passing through $(x_0, y_0)$; (3). There are no solutions (integral curves) to $\frac{dy}{dx}=f(x, y)$ passing through $(x_0, y_0)$ that lies outside the region in (2). Below is a sketch of solution given by my TA. By Peano's existence theorem, since $f$ is continuous and bounded on $D$, there exists, in a neighborhood of $x_0$, some function $y=y(x)$ such that $\frac{dy}{dx}=f(x, y)$ and that $y(x_0)=y_0$. By fundamental theorem of calculus, we have that $$y'=f, y(x_0)=y_0 \iff y(x)=y_0+\int_{x_0}^{x} f(t, y(t))dt$$ Let $y_{max}$ be the largest integral curve and $y_{min}$ the smallest. Then $$y_{max}-y_{min}=\int_{x_0}^{x} \Big[f(t, y_{max}(t))-f(t, y_{min}(t))\Big] dt\geq 0$$ These may be constructed using an Euler approximation on the integral to recursively build the max and the min . Any integral curve in-between may also be created since we may take any value between the max and the min in the recursion. The idea he presented is very natural, but what is confusing me is the bolded part. I really have no idea how to build the max and the min recursively. Can anyone explain how to proceed? Any help is appreciated!","Suppose that $D=[a, b] \times \mathbb{R}$ is a strip in $\mathbb{R}^2$, and that $f(x, y)$ is continuous and bounded on $D$. Let $(x_0, y_0)$ be an interior point of $D$ (i.e. $a< x_0 <b$). Prove that there are two integral curves $y=\phi_1 (x)$ and $y=\phi_2 (x)$, i.e. the maximum and the minimum solutions to the equation $\frac{dy}{dx}=f(x, y)$, such that (1). $\phi_1 (x_0)=\phi_2 (x_0)=y_0$ and $\phi_1 (x)\geq \phi_2 (x), \forall x\in [a, b]$; (2). the region $\{(x, y)\in \mathbb{R}^2 | a\leq x \leq b, \phi_2 (x)\leq y\leq \phi_1 (x)\}$ can be completely filled by integral curves passing through $(x_0, y_0)$; (3). There are no solutions (integral curves) to $\frac{dy}{dx}=f(x, y)$ passing through $(x_0, y_0)$ that lies outside the region in (2). Below is a sketch of solution given by my TA. By Peano's existence theorem, since $f$ is continuous and bounded on $D$, there exists, in a neighborhood of $x_0$, some function $y=y(x)$ such that $\frac{dy}{dx}=f(x, y)$ and that $y(x_0)=y_0$. By fundamental theorem of calculus, we have that $$y'=f, y(x_0)=y_0 \iff y(x)=y_0+\int_{x_0}^{x} f(t, y(t))dt$$ Let $y_{max}$ be the largest integral curve and $y_{min}$ the smallest. Then $$y_{max}-y_{min}=\int_{x_0}^{x} \Big[f(t, y_{max}(t))-f(t, y_{min}(t))\Big] dt\geq 0$$ These may be constructed using an Euler approximation on the integral to recursively build the max and the min . Any integral curve in-between may also be created since we may take any value between the max and the min in the recursion. The idea he presented is very natural, but what is confusing me is the bolded part. I really have no idea how to build the max and the min recursively. Can anyone explain how to proceed? Any help is appreciated!",,"['real-analysis', 'ordinary-differential-equations', 'proof-explanation']"
49,Solving Euler-Lagrange Equation with delta function,Solving Euler-Lagrange Equation with delta function,,"I am trying to understand a physical system and have arrived at the following equation: $$\mathcal{S} = \int_{z = -\infty}^{z = \infty} dz \left\lbrace f_\rho[\rho] + \dfrac{m}{2} \bigg| \dfrac{\partial \rho}{\partial z} \bigg| ^2 + \dfrac{\rho ^2 (z) \delta(z-z_0)}{2}\right\rbrace$$ where I seek the $ \rho (z) $ that minimizes the value of $\mathcal{S}$ . $ \rho (z) $ is a function of $ z $ , with $ \rho ' = d\rho / dz$ . $ f [\rho] = a \rho ^2 + b \rho ^3 + c \rho ^4$ is a functional of $ \rho (z) $ , and $ f_\rho[\rho] = \frac{df}{d\rho} $ . I thus write an Euler-Lagrange equation: $$ \dfrac{\partial \mathcal{S}}{\partial \rho} = \dfrac{\partial}{\partial z} \left( \dfrac{\partial \mathcal{S}}{\partial \rho '}\right)$$ I have been unable to solve this equation for $ {\rho(z)} $ , and was wondering how to do so. I have managed to simplify the EL equation to: $$ \dfrac{\partial f_\rho [\rho]}{\partial \rho} + \rho \delta(z-z_0) = m \dfrac{\partial ^2 \rho}{\partial z^2} $$ but I don't know where to go from here. Any help would be much appreciated.","I am trying to understand a physical system and have arrived at the following equation: where I seek the that minimizes the value of . is a function of , with . is a functional of , and . I thus write an Euler-Lagrange equation: I have been unable to solve this equation for , and was wondering how to do so. I have managed to simplify the EL equation to: but I don't know where to go from here. Any help would be much appreciated.",\mathcal{S} = \int_{z = -\infty}^{z = \infty} dz \left\lbrace f_\rho[\rho] + \dfrac{m}{2} \bigg| \dfrac{\partial \rho}{\partial z} \bigg| ^2 + \dfrac{\rho ^2 (z) \delta(z-z_0)}{2}\right\rbrace  \rho (z)  \mathcal{S}  \rho (z)   z   \rho ' = d\rho / dz  f [\rho] = a \rho ^2 + b \rho ^3 + c \rho ^4  \rho (z)   f_\rho[\rho] = \frac{df}{d\rho}   \dfrac{\partial \mathcal{S}}{\partial \rho} = \dfrac{\partial}{\partial z} \left( \dfrac{\partial \mathcal{S}}{\partial \rho '}\right)  {\rho(z)}   \dfrac{\partial f_\rho [\rho]}{\partial \rho} + \rho \delta(z-z_0) = m \dfrac{\partial ^2 \rho}{\partial z^2} ,"['ordinary-differential-equations', 'calculus-of-variations', 'euler-lagrange-equation']"
50,Why is this system reversible? What does this mean?,Why is this system reversible? What does this mean?,,"Consider the system $$ \dot{x}=y,\qquad\dot{y}=-x+y^2. $$ Then, it is said that the system is reversible $(t\to -t, y\to -y)$. What does this mean? If I put this into the equations, I get $$ \dot{x}(-t)=y(-t),\qquad \dot{y}(-t)=-x(-t)+y(-t)^2. $$ So does reversible mean here that when replacing $t$ by $-t$ and $y$ by $-y$, the differential equations still hold?","Consider the system $$ \dot{x}=y,\qquad\dot{y}=-x+y^2. $$ Then, it is said that the system is reversible $(t\to -t, y\to -y)$. What does this mean? If I put this into the equations, I get $$ \dot{x}(-t)=y(-t),\qquad \dot{y}(-t)=-x(-t)+y(-t)^2. $$ So does reversible mean here that when replacing $t$ by $-t$ and $y$ by $-y$, the differential equations still hold?",,['ordinary-differential-equations']
51,Solve $3x(1-x^2)y^2\frac{dy}{dx}+(2x^2-1)y^3=ax^3$,Solve,3x(1-x^2)y^2\frac{dy}{dx}+(2x^2-1)y^3=ax^3,I am solving this linear Differential equation which can be easily solve by using the formulas for the Bernoulli's Equations I have solved till  $$\frac{dy}{dx}+\frac{(2x^2-1)y^3}{3x(1-x^2)y^2}=\frac{ax^3}{3x(1-x^2)y^2}$$ $$y^2\frac{dy}{dx}+\frac{(2x^2-1)y^3}{3x(1-x^2)}=\frac{ax^3}{3x(1-x^2)}$$ Substituting $y^3=t$ so the equation will be $$\frac{1}{3}\frac{dt}{dx}+\frac{(2x^2-1)t}{3x(1-x^2)}=\frac{ax^3}{3x(1-x^2)}$$ after this the integrating factor is $$\frac{1}{x\sqrt{1-x^2}}$$ But I am unable to solve it forward.,I am solving this linear Differential equation which can be easily solve by using the formulas for the Bernoulli's Equations I have solved till  $$\frac{dy}{dx}+\frac{(2x^2-1)y^3}{3x(1-x^2)y^2}=\frac{ax^3}{3x(1-x^2)y^2}$$ $$y^2\frac{dy}{dx}+\frac{(2x^2-1)y^3}{3x(1-x^2)}=\frac{ax^3}{3x(1-x^2)}$$ Substituting $y^3=t$ so the equation will be $$\frac{1}{3}\frac{dt}{dx}+\frac{(2x^2-1)t}{3x(1-x^2)}=\frac{ax^3}{3x(1-x^2)}$$ after this the integrating factor is $$\frac{1}{x\sqrt{1-x^2}}$$ But I am unable to solve it forward.,,"['calculus', 'ordinary-differential-equations']"
52,Solving differential equation describing motion in a pendulum,Solving differential equation describing motion in a pendulum,,"I've been looking at Simple Harmonic Motion in particularly the period of a pendulum. This may seem like physics but my question is tailored towards mathematics. The differential equation is: $${{d^2\theta}\over dt^2}+\sin\theta=0$$ Using the small angle approximation it is found that $T={2\pi}\sqrt{L\over g}$. Is it possible to solve the differential equation without using the small angle approximation? If so, what is the actual period of a pendulum?","I've been looking at Simple Harmonic Motion in particularly the period of a pendulum. This may seem like physics but my question is tailored towards mathematics. The differential equation is: $${{d^2\theta}\over dt^2}+\sin\theta=0$$ Using the small angle approximation it is found that $T={2\pi}\sqrt{L\over g}$. Is it possible to solve the differential equation without using the small angle approximation? If so, what is the actual period of a pendulum?",,"['ordinary-differential-equations', 'mathematical-physics']"
53,How to solve an ODE with $y^{-1}$ term,How to solve an ODE with  term,y^{-1},"My major is not Mathematics, but I came across the following ODE for $y(x)$: $$\left(y^3y^{\prime\prime\prime}\right)^\prime+\frac{5}{8}xy^\prime-\frac{1}{2}y+\frac{a}{y}=0,$$ where the prime denote the derivative, and $a$ is a positive constant, $0<a\le1$. This ODE is subjected to four initial conditions: $y^\prime(0)=y^{\prime\prime\prime}(0)=0$, and the other two $y(0)$ and $y^{\prime\prime}(0)$ will be determined by shooting (method) for boundary conditions (BCs) at infinity. To find the desired BCs at infinity, I try to figure out the far-field asymptotic behavior. In my problem, I know that the far-field BCs is quasi-steady behavior of this ODE. Case 1 . If I drop the first term and solve $$\frac{5}{8}xy^\prime-\frac{1}{2}y+\frac{a}{y}=0,$$ I obtain a general solution $$y=\sqrt{2a+e^{2c}x^{8/5}},$$ where $c$ is a free constant, and the negative one has been dropped due to $y>0$ in my problem.  At this point, can I say the far-field asymptotic behavior is $$y\sim e^cx^{4/5}.$$ Case 2 . If I drop all the nonlinear terms in the ODE and solve $$\frac{5}{8}xy^\prime-\frac{1}{2}y=0,$$ I got a simpler solution $$y=cx^{4/5}.$$ In this case, I might say the far-field asymptotic behavior is $$y\sim cx^{4/5}.$$ My first question is which case is right? Or both of them are wrong, how can I got it? Alternatively, if I requires $$\left(y^3y^{\prime\prime\prime}\right)^\prime \sim 0 \quad as \quad x \rightarrow \infty,$$ and let $y=Ax^\alpha$, by plug it in the above asymptotics, I have $$A^4x^{4\alpha-3}\alpha(\alpha-1)(\alpha-2)=0$$ which yields $\alpha=0,1,2$. Here, my second question is: could I say the far-field asymptotic behavior is $$y\sim Ax^2,$$ because this is the fastest growing one? Update Thanks for @Frits Veerman's answer, I understand that the far-field behavior is not affected by the last term $\frac{a}{y}$, which is my ""afraid"" term. So come to my third questions: whether or not the following derivation is right? Case 3 . Now, an alternative far-field behavior is produced $$\left(y^3y^{\prime\prime\prime}\right)^\prime \sim \frac{1}{2}y-\frac{5}{8}xy^\prime \quad as \quad x \rightarrow \infty,$$ Again let $y=Bx^\alpha$, by plug it in above asymptotics, I have $$\frac{1}{2}(1-\frac{5}{4}\alpha)=B^3\alpha(\alpha-1)(\alpha-2)(4\alpha-3)x^{3\alpha-4}.$$ For this equality, it must have $$3\alpha-4=0,$$ which gives $\alpha=\frac{4}{3}.$  Thus I obtain a different far-field behavior $$y=Bx^{\frac{4}{3}}.$$ Am I right? I think this question is related to the perturbation solution of ODE. Anyone can help me, or recommend some famous books or journal papers to me? Many thanks! Regards, Robin","My major is not Mathematics, but I came across the following ODE for $y(x)$: $$\left(y^3y^{\prime\prime\prime}\right)^\prime+\frac{5}{8}xy^\prime-\frac{1}{2}y+\frac{a}{y}=0,$$ where the prime denote the derivative, and $a$ is a positive constant, $0<a\le1$. This ODE is subjected to four initial conditions: $y^\prime(0)=y^{\prime\prime\prime}(0)=0$, and the other two $y(0)$ and $y^{\prime\prime}(0)$ will be determined by shooting (method) for boundary conditions (BCs) at infinity. To find the desired BCs at infinity, I try to figure out the far-field asymptotic behavior. In my problem, I know that the far-field BCs is quasi-steady behavior of this ODE. Case 1 . If I drop the first term and solve $$\frac{5}{8}xy^\prime-\frac{1}{2}y+\frac{a}{y}=0,$$ I obtain a general solution $$y=\sqrt{2a+e^{2c}x^{8/5}},$$ where $c$ is a free constant, and the negative one has been dropped due to $y>0$ in my problem.  At this point, can I say the far-field asymptotic behavior is $$y\sim e^cx^{4/5}.$$ Case 2 . If I drop all the nonlinear terms in the ODE and solve $$\frac{5}{8}xy^\prime-\frac{1}{2}y=0,$$ I got a simpler solution $$y=cx^{4/5}.$$ In this case, I might say the far-field asymptotic behavior is $$y\sim cx^{4/5}.$$ My first question is which case is right? Or both of them are wrong, how can I got it? Alternatively, if I requires $$\left(y^3y^{\prime\prime\prime}\right)^\prime \sim 0 \quad as \quad x \rightarrow \infty,$$ and let $y=Ax^\alpha$, by plug it in the above asymptotics, I have $$A^4x^{4\alpha-3}\alpha(\alpha-1)(\alpha-2)=0$$ which yields $\alpha=0,1,2$. Here, my second question is: could I say the far-field asymptotic behavior is $$y\sim Ax^2,$$ because this is the fastest growing one? Update Thanks for @Frits Veerman's answer, I understand that the far-field behavior is not affected by the last term $\frac{a}{y}$, which is my ""afraid"" term. So come to my third questions: whether or not the following derivation is right? Case 3 . Now, an alternative far-field behavior is produced $$\left(y^3y^{\prime\prime\prime}\right)^\prime \sim \frac{1}{2}y-\frac{5}{8}xy^\prime \quad as \quad x \rightarrow \infty,$$ Again let $y=Bx^\alpha$, by plug it in above asymptotics, I have $$\frac{1}{2}(1-\frac{5}{4}\alpha)=B^3\alpha(\alpha-1)(\alpha-2)(4\alpha-3)x^{3\alpha-4}.$$ For this equality, it must have $$3\alpha-4=0,$$ which gives $\alpha=\frac{4}{3}.$  Thus I obtain a different far-field behavior $$y=Bx^{\frac{4}{3}}.$$ Am I right? I think this question is related to the perturbation solution of ODE. Anyone can help me, or recommend some famous books or journal papers to me? Many thanks! Regards, Robin",,"['ordinary-differential-equations', 'mathematical-physics', 'perturbation-theory']"
54,Bessel function equation with derivation: $\frac{d}{dx} (xJ_{\alpha}(x)J_{\alpha+1}(x)) = x(J_{\alpha}^2(x)-J_{\alpha +1}^2(x))$,Bessel function equation with derivation:,\frac{d}{dx} (xJ_{\alpha}(x)J_{\alpha+1}(x)) = x(J_{\alpha}^2(x)-J_{\alpha +1}^2(x)),Let $J_{\alpha}(x)$be the Bessel function. Show the equality: $$\frac{d}{dx} \left( xJ_{\alpha}(x)J_{\alpha +1}(x)\right) = x \left( J_{\alpha}^2(x) -J_{\alpha +1}^2(x)\right)$$ How to start with it?,Let $J_{\alpha}(x)$be the Bessel function. Show the equality: $$\frac{d}{dx} \left( xJ_{\alpha}(x)J_{\alpha +1}(x)\right) = x \left( J_{\alpha}^2(x) -J_{\alpha +1}^2(x)\right)$$ How to start with it?,,['ordinary-differential-equations']
55,Differential equation for the amount of chemical in the pond at any time,Differential equation for the amount of chemical in the pond at any time,,"A pond intitally contains $1\,000\,000\,\rm gal$ of water and an unknown amount of an undesirable chemical. Water containing $0.01\,\rm g$ of this chemical per gallon flows into the pond at a rate of $300\,\rm gal/h$. The mixture flows out at the same rate, so the amount of water in the pond remains constant. Assume that the chemical is uniformly distributed throughout the pond.   Write a differential equation for the amount of chemical in the pond at any time. The answer key says the answer is $$\frac{\mathrm{d}q}{\mathrm{d}t} = 300(10^{-2}-10^{-6}q)$$ where $q$ is the amount of chemical in gallons. I don't get how they got this, and also how, if there is an unknown amount of chemical in the pond given the rate at which the amount of chemical changes per hour, can we tell the amount of chemical?","A pond intitally contains $1\,000\,000\,\rm gal$ of water and an unknown amount of an undesirable chemical. Water containing $0.01\,\rm g$ of this chemical per gallon flows into the pond at a rate of $300\,\rm gal/h$. The mixture flows out at the same rate, so the amount of water in the pond remains constant. Assume that the chemical is uniformly distributed throughout the pond.   Write a differential equation for the amount of chemical in the pond at any time. The answer key says the answer is $$\frac{\mathrm{d}q}{\mathrm{d}t} = 300(10^{-2}-10^{-6}q)$$ where $q$ is the amount of chemical in gallons. I don't get how they got this, and also how, if there is an unknown amount of chemical in the pond given the rate at which the amount of chemical changes per hour, can we tell the amount of chemical?",,"['ordinary-differential-equations', 'chemistry']"
56,Difficulty using Excel to solve the double pendulum problem using RK4 to solve four simultaneous first order ODEs,Difficulty using Excel to solve the double pendulum problem using RK4 to solve four simultaneous first order ODEs,,"I have set up a spreadsheet in Excel to solve the double pendulum problem using RK4 on the four first order DEs. However it isn't working, as the energy in the system is increasing drastically, instead of staying constant. I have spent days (literally) checking the Lagrangian and the spreadsheet entries, and I can't find any faults. I wonder if I am mis-applying the RK4 procedure. I can't find any examples of this in Excel - only Matlab, which is quite different, and I don't have it! I'm desperate and so thought I would post here. I have four first order DEs. The first two are simple  W'=Y,   X'=Z.    Y' & Z' are both very long with terms in W, X, Y & Z.    So, I have W'=f(t,Y)  X'=f(t,Z)  Y'=f(t,X,Y,Z)  Z'=f(t,X,Y,Z) I calculate Wn+1 = Wn + h/6*(J1 + 2*J2 + 2*J3 + J4)  Xn+1 = Xn + h/6*(K1 + 2*K2 + 2*K3 + K4)  Yn+1 = Yn + h/6*(L1 + 2*L2 + 2*L3 + L4)  Zn+1 = Zn + h/6*(M1 + 2*M2 + 2*M3 + M4) The values of these constants are worked out as follows: (Is this where I'm going wrong?) J1 = f(tn, Yn)  J2 = f(tn + h/2, Yn + h/2*L1)   J3 = f(tn + h/2, Yn + h/2*L2)  J4 = f(tn + h, Yn + h*L3) The values for K1, K2, K3 & K4 are calculated in the same way using Zn & M1, M2 M3 & M4. As Y is a function f(t, W, X, Y, Z), I calculate as follows: L1 = f(tn, Wn, Xn, Yn, Zn)  L2 = f(tn + h/2, Wn + h/2*J1, Xn + h/2*K1, Yn + h/2*L1, Zn + h/2*M1)  L3 = f(tn + h/2, Wn + h/2*J2, Xn + h/2*K2, Yn + h/2*L2, Zn + h/2*M2)  L4 = f(tn + h, Wn + h*J3, Xn + h*K3, Yn + h*L3, Zn + h*M3). The values for M1, M2, M3 & M4 used to calculate Z were computed in the exact same manner as for  L1 etc. above. If anyone can identify a flaw in my methodology and let me know I would be very grateful.","I have set up a spreadsheet in Excel to solve the double pendulum problem using RK4 on the four first order DEs. However it isn't working, as the energy in the system is increasing drastically, instead of staying constant. I have spent days (literally) checking the Lagrangian and the spreadsheet entries, and I can't find any faults. I wonder if I am mis-applying the RK4 procedure. I can't find any examples of this in Excel - only Matlab, which is quite different, and I don't have it! I'm desperate and so thought I would post here. I have four first order DEs. The first two are simple  W'=Y,   X'=Z.    Y' & Z' are both very long with terms in W, X, Y & Z.    So, I have W'=f(t,Y)  X'=f(t,Z)  Y'=f(t,X,Y,Z)  Z'=f(t,X,Y,Z) I calculate Wn+1 = Wn + h/6*(J1 + 2*J2 + 2*J3 + J4)  Xn+1 = Xn + h/6*(K1 + 2*K2 + 2*K3 + K4)  Yn+1 = Yn + h/6*(L1 + 2*L2 + 2*L3 + L4)  Zn+1 = Zn + h/6*(M1 + 2*M2 + 2*M3 + M4) The values of these constants are worked out as follows: (Is this where I'm going wrong?) J1 = f(tn, Yn)  J2 = f(tn + h/2, Yn + h/2*L1)   J3 = f(tn + h/2, Yn + h/2*L2)  J4 = f(tn + h, Yn + h*L3) The values for K1, K2, K3 & K4 are calculated in the same way using Zn & M1, M2 M3 & M4. As Y is a function f(t, W, X, Y, Z), I calculate as follows: L1 = f(tn, Wn, Xn, Yn, Zn)  L2 = f(tn + h/2, Wn + h/2*J1, Xn + h/2*K1, Yn + h/2*L1, Zn + h/2*M1)  L3 = f(tn + h/2, Wn + h/2*J2, Xn + h/2*K2, Yn + h/2*L2, Zn + h/2*M2)  L4 = f(tn + h, Wn + h*J3, Xn + h*K3, Yn + h*L3, Zn + h*M3). The values for M1, M2, M3 & M4 used to calculate Z were computed in the exact same manner as for  L1 etc. above. If anyone can identify a flaw in my methodology and let me know I would be very grateful.",,"['ordinary-differential-equations', 'runge-kutta-methods']"
57,Noncausal dynamical system,Noncausal dynamical system,,"The differential equation $$a_ny(t)^{(n)} + \dots + a_0y(t)^{(0)} = b_mu(t)^{(m)} + \dots + b_0u(t)^{(0)} $$ with $a_i,b_i \in \mathbb{R}$ and $y,u:\mathbb{R}\to\mathbb{R}$ describes a time-independent, linear, SISO system. Why is this system noncausal (that means not physically realizable) if $m > n$? For instance, this equation describes a noncausal system ($n = 0, m = 1$): $$y(t) = u'(t)$$ and using the definition of the derivative it becomes $$y(t) = \lim_{\Delta t \to 0}\frac{u(t + \Delta t) - u(t)}{\Delta t}$$ This means that the output signal at time $t$ depends from a future input signal value $u(t + \Delta t)$ and it makes sense to me that this system is not realizable. But why this other system is realizable ($n = 1, m = 1$)? $$y(t) = u'(t) + y'(t)\\\iff$$ $$y(t) = \lim_{\Delta t \to 0}\frac{u(t + \Delta t) - u(t)}{\Delta t} + \lim_{\Delta t \to 0}\frac{y(t + \Delta t) - y(t)}{\Delta t} $$ $y(t)$ depends from a future input signal value too.","The differential equation $$a_ny(t)^{(n)} + \dots + a_0y(t)^{(0)} = b_mu(t)^{(m)} + \dots + b_0u(t)^{(0)} $$ with $a_i,b_i \in \mathbb{R}$ and $y,u:\mathbb{R}\to\mathbb{R}$ describes a time-independent, linear, SISO system. Why is this system noncausal (that means not physically realizable) if $m > n$? For instance, this equation describes a noncausal system ($n = 0, m = 1$): $$y(t) = u'(t)$$ and using the definition of the derivative it becomes $$y(t) = \lim_{\Delta t \to 0}\frac{u(t + \Delta t) - u(t)}{\Delta t}$$ This means that the output signal at time $t$ depends from a future input signal value $u(t + \Delta t)$ and it makes sense to me that this system is not realizable. But why this other system is realizable ($n = 1, m = 1$)? $$y(t) = u'(t) + y'(t)\\\iff$$ $$y(t) = \lim_{\Delta t \to 0}\frac{u(t + \Delta t) - u(t)}{\Delta t} + \lim_{\Delta t \to 0}\frac{y(t + \Delta t) - y(t)}{\Delta t} $$ $y(t)$ depends from a future input signal value too.",,"['ordinary-differential-equations', 'derivatives']"
58,Particular Integral of $\frac{\partial^2 u}{\partial x^2}+2 \frac{\partial^2 u}{\partial x\partial y}+\frac{\partial^2 u}{\partial y^2}=x$.,Particular Integral of .,\frac{\partial^2 u}{\partial x^2}+2 \frac{\partial^2 u}{\partial x\partial y}+\frac{\partial^2 u}{\partial y^2}=x,"Question: The partial differential equation $$\frac{\partial^2 u}{\partial x^2}+2 \frac{\partial^2 u}{\partial x\partial y}+\frac{\partial^2 u}{\partial y^2}=x,$$ has only one particular integral. a particular integral which is linear in $x$ and $y$. a particular integral which is a quadratic polynomial is $x$ and $y$. more than one particular integral. Answer: Is the option (4) correct and all others are false?","Question: The partial differential equation $$\frac{\partial^2 u}{\partial x^2}+2 \frac{\partial^2 u}{\partial x\partial y}+\frac{\partial^2 u}{\partial y^2}=x,$$ has only one particular integral. a particular integral which is linear in $x$ and $y$. a particular integral which is a quadratic polynomial is $x$ and $y$. more than one particular integral. Answer: Is the option (4) correct and all others are false?",,"['ordinary-differential-equations', 'partial-differential-equations']"
59,Stiff Nonlinear Differential Equations,Stiff Nonlinear Differential Equations,,"As far as I know,  the concept of stiffness is hard to define rigorously, but there are plenty of handwavy descriptions and motivating examples in the literature when it comes to linear differential equations . At the same time I have never seen an explicit and straightforward definition of a stiff nonlinear differential equation . That being said,  I feel like there should be one, and I just haven't seen it yet.  To outline, my questions are: Is there such thing as stiff nonlinear differential equation ?   If so, how is it defined? The most straightforward approach to define one is to use linearization, but I am not sure if this is a good idea as the accuracy of linearization will probably have an decisive impact on the region of absolute stability.","As far as I know,  the concept of stiffness is hard to define rigorously, but there are plenty of handwavy descriptions and motivating examples in the literature when it comes to linear differential equations . At the same time I have never seen an explicit and straightforward definition of a stiff nonlinear differential equation . That being said,  I feel like there should be one, and I just haven't seen it yet.  To outline, my questions are: Is there such thing as stiff nonlinear differential equation ?   If so, how is it defined? The most straightforward approach to define one is to use linearization, but I am not sure if this is a good idea as the accuracy of linearization will probably have an decisive impact on the region of absolute stability.",,"['ordinary-differential-equations', 'soft-question', 'numerical-methods', 'stability-in-odes']"
60,What does the continuity of $f'$ tell us about $f$?,What does the continuity of  tell us about ?,f' f,"Suppose $f$ is differentiable on $\mathbb{R}$ and its derivative $f'$ is continuous on the interval $[a,b]$. What constraints on $f$ would such condition give us?","Suppose $f$ is differentiable on $\mathbb{R}$ and its derivative $f'$ is continuous on the interval $[a,b]$. What constraints on $f$ would such condition give us?",,"['real-analysis', 'ordinary-differential-equations', 'derivatives', 'continuity']"
61,Normal unit vector,Normal unit vector,,"I am looking at the following exercise: I have done the following about the second part, about the signed curvature of $\iota$ : The signed curvature of $\gamma$ is different from the signed curvature of $\iota$, right? So, let $\kappa_s$ be the signed curvature of $\gamma$ and $\kappa_{s, \iota}$ the signed curvature of $\iota$. We have $$\iota '(s)=\gamma '(s)-\gamma '(s)+(l-s)\gamma ''(s) \Rightarrow \iota '(s)=(l-s)\gamma '' (s) \tag{1}$$ We define as the unit tangent vector $\textbf{t}$, the signed unit normal vector $\textbf{n}_{s, \iota}$ and the signed curvature $\kappa_{s, \iota}$ of $\iota$ the corresponding quantities of the unit speed reparametrization of $\tilde{\iota}(a)$, where $a$ is the arc length of $\iota$.  So, $$\textbf{t}=\frac{\iota ' (s)}{\|\iota ' (s)\|}=\frac{\iota '(s)}{a'(s)}$$  Therefore, $$\iota '(s)=a'(s)\textbf{t}$$ We have that if $\iota$ is a unit-speed plane curve, then $$\textbf{n}_{s, \iota } ' =-\kappa_{s, \iota} \iota'$$ Generalizing this formula for a regular curve, not necessarily unit-speed, we have $$\textbf{n}_{s,\iota } ' =-\kappa_{s,\iota}a'(s)\textbf{t} \tag{2}$$ $$(1) \Rightarrow a'(s)\textbf{t}=(l-s)\gamma ''(s) \Rightarrow a'(s) \textbf{t}=(l-s)\kappa_s \textbf{n}_s$$ $$(2) \Rightarrow \textbf{n}_{s, \iota } '=-\kappa_{s, \iota} (l-s)\kappa_s \textbf{n}_s$$ Which is the relation between the normal unit vector of the curve and normal unit vector of the involute of the curve?","I am looking at the following exercise: I have done the following about the second part, about the signed curvature of $\iota$ : The signed curvature of $\gamma$ is different from the signed curvature of $\iota$, right? So, let $\kappa_s$ be the signed curvature of $\gamma$ and $\kappa_{s, \iota}$ the signed curvature of $\iota$. We have $$\iota '(s)=\gamma '(s)-\gamma '(s)+(l-s)\gamma ''(s) \Rightarrow \iota '(s)=(l-s)\gamma '' (s) \tag{1}$$ We define as the unit tangent vector $\textbf{t}$, the signed unit normal vector $\textbf{n}_{s, \iota}$ and the signed curvature $\kappa_{s, \iota}$ of $\iota$ the corresponding quantities of the unit speed reparametrization of $\tilde{\iota}(a)$, where $a$ is the arc length of $\iota$.  So, $$\textbf{t}=\frac{\iota ' (s)}{\|\iota ' (s)\|}=\frac{\iota '(s)}{a'(s)}$$  Therefore, $$\iota '(s)=a'(s)\textbf{t}$$ We have that if $\iota$ is a unit-speed plane curve, then $$\textbf{n}_{s, \iota } ' =-\kappa_{s, \iota} \iota'$$ Generalizing this formula for a regular curve, not necessarily unit-speed, we have $$\textbf{n}_{s,\iota } ' =-\kappa_{s,\iota}a'(s)\textbf{t} \tag{2}$$ $$(1) \Rightarrow a'(s)\textbf{t}=(l-s)\gamma ''(s) \Rightarrow a'(s) \textbf{t}=(l-s)\kappa_s \textbf{n}_s$$ $$(2) \Rightarrow \textbf{n}_{s, \iota } '=-\kappa_{s, \iota} (l-s)\kappa_s \textbf{n}_s$$ Which is the relation between the normal unit vector of the curve and normal unit vector of the involute of the curve?",,"['ordinary-differential-equations', 'vectors', 'curves', 'involutions']"
62,Boundary value problem and twice differentiable solutions,Boundary value problem and twice differentiable solutions,,"Let's consider a boundary value problem $$ u''(x) = f(u(x)) + g(x)$$ with boundary conditions $u(0) = u(1) = 0$. We assume that the functions $g \in \mathscr C[0,1]$ and $f \in \mathscr C(\mathbb R)$ are given, $u(x)$ remains unknown. How can we transform the equation to the integral form $$u(x) = \int_0^1 K(x,y) [f(u(y)) + g(y)] \,\textrm{d}y,$$ where the function $K$ depends on $f$ and $g$? I have tried to found an implicit formula for $K$ and failed - that's a technique I have discovered quite recently and haven't mastered yet. What additional assumption do I need in order to say that the solution $u(x)$ of my equation is unique? Maybe the Lipschitz condition for $f$ or its derivative or some type of regularity of $u$, like being twice differentiable ($\mathscr C^2$)?","Let's consider a boundary value problem $$ u''(x) = f(u(x)) + g(x)$$ with boundary conditions $u(0) = u(1) = 0$. We assume that the functions $g \in \mathscr C[0,1]$ and $f \in \mathscr C(\mathbb R)$ are given, $u(x)$ remains unknown. How can we transform the equation to the integral form $$u(x) = \int_0^1 K(x,y) [f(u(y)) + g(y)] \,\textrm{d}y,$$ where the function $K$ depends on $f$ and $g$? I have tried to found an implicit formula for $K$ and failed - that's a technique I have discovered quite recently and haven't mastered yet. What additional assumption do I need in order to say that the solution $u(x)$ of my equation is unique? Maybe the Lipschitz condition for $f$ or its derivative or some type of regularity of $u$, like being twice differentiable ($\mathscr C^2$)?",,['ordinary-differential-equations']
63,Extremal of a functional $I=\int\limits_0^{x_1} y^2(y')^2dx$,Extremal of a functional,I=\int\limits_0^{x_1} y^2(y')^2dx,"The extremal of the function $$I=\int\limits_0^{x_1} y^2(y')^2dx$$ that passes through $(0,0)$ and $(x_1,y_1)$ is a constant function a linear function of x part of a parabola part of an ellipse. Attempt: Using Euler-Lagrange equation, $$\frac{\partial f(x)}{\partial y}-\dfrac{d }{dx}\left(\frac{\partial f(x)}{\partial y'}\right)=0$$ I get $$y^2y''+y(y')^2=0$$ Solving letting $v=y'$ , $$\frac{y^2}2=cx+d,c=\frac{2x_1}{y_1^2}, d=0$$ Hence extremal is parabolic (right open). Am I correct? (CSIR June 2015, in case someone comes searching for it!)","The extremal of the function that passes through and is a constant function a linear function of x part of a parabola part of an ellipse. Attempt: Using Euler-Lagrange equation, I get Solving letting , Hence extremal is parabolic (right open). Am I correct? (CSIR June 2015, in case someone comes searching for it!)","I=\int\limits_0^{x_1} y^2(y')^2dx (0,0) (x_1,y_1) \frac{\partial f(x)}{\partial y}-\dfrac{d }{dx}\left(\frac{\partial f(x)}{\partial y'}\right)=0 y^2y''+y(y')^2=0 v=y' \frac{y^2}2=cx+d,c=\frac{2x_1}{y_1^2}, d=0","['ordinary-differential-equations', 'functional-equations', 'calculus-of-variations']"
64,A tricky Differential Equation,A tricky Differential Equation,,How do you solve $$\frac{dy}{dx} = \frac{y^3}{e^{2x} + y^2}$$ I just need a hint. Its not an exact differential nor a linear  D. E which  I can solve...,How do you solve $$\frac{dy}{dx} = \frac{y^3}{e^{2x} + y^2}$$ I just need a hint. Its not an exact differential nor a linear  D. E which  I can solve...,,"['ordinary-differential-equations', 'differential-algebra']"
65,Integration by substitution not working?,Integration by substitution not working?,,"I've been working on a set of problems for the past 6 hours and I'm about to explode from frustration! Either my book has errors, Wolfram Alpha is broken or integration by substitution does not work! I'm trying to solve the following ODE: $$ \frac{\mathrm dh}{\mathrm dt}=\frac{1-20kh+kh^2}{20h-h^2},\text{ where }k=0.01 $$ Here is what I have (the first line is the ODE I'm trying to solve, if it's unclear): $$\begin{align} \frac{\mathrm dh}{\mathrm dt}&=\frac{1-20kh+kh^2}{20h-h^2}\\ \frac{20h-h^2}{1-20kh+kh^2}\,\mathrm dh&=\mathrm dt\\[5pt] \frac{20h-h^2}{1-0.2+0.01h^2}\,\mathrm dh&=\mathrm dt,\text{ since }k=0.01\\[5pt] \frac{100\left(20h-h^2\right)}{100-20h+h^2}\,\mathrm dh=\mathrm dt\\[5pt] \int\frac{2000h-100h^2}{(h-10)^2}\,\mathrm dh&=\int\mathrm dt \end{align}$$ Let $u=h-10$ So, $h=u+10$ And, $\mathrm dh=\mathrm du$ $$\begin{align}\require{cancel} \int\left(\frac{2000(u+10)}{u^2}-\frac{100(u+10)^2}{u^2}\right)\,\mathrm du&=t+c\\[5pt] \int\left(\frac{2000u}{u^2}+\frac{20000}{u^2}-\frac{100(u^2+20u+100)}{u^2}\right)\,\mathrm du&=t+c\\[5pt] \int\left(\cancel{\frac{2000}u}+\frac{20000}{u^2}-100-\cancel{\frac{2000}u}-\frac{10000}{u^2}\right)\,\mathrm du&=t+c\\[5pt] \int\left(\frac{10000}{u^2}-100\right)\,\mathrm du&=t+c\\[5pt] -\frac{10000}u-100u&=t+c\\[5pt] -\frac{10000}{h-10}-100(h-10)&=t+c\\[5pt] -\frac{10000-100(h-10)^2}{h-10}&=t+c\\[5pt] -\frac{100\left(100+(h-10)^2\right)}{h-10}&=t+c\\[5pt] -\frac{100\left(100+\left(h^2-20h+100\right)\right)}{h-10}&=t+c\\[5pt] \frac{100\left(h^2-20h+200\right)}{10-h}&=t+c \end{align}$$ But according to Wolfram Alpha, that isn't the correct answer. http://wolframalpha.com/input/?i=integrate+%28100x%2820-x%29%2F%28x-10%29%5E2%29+with+respect+to+x And according to my book's solution manual, this is the answer (same as Wolfram Alpha's answer): (b) Letting $k=1/100$, separating variables and integrating (with the help of a CAS), we get $$\frac{100h(h-20)}{(h-10)^2}\,\mathrm dh=\mathrm dt\;\text{and}\;\frac{100\left(h^2-10h+100\right)}{10-h}=t+c$$ The equation on the right in the solutions manual is the same as what Wolfram Alpha outputs. However, it seems that there's a typo in the equation on the left. It should be $\frac{100h(20-h)}{(h-10)^2}$ instead of $\frac{100h(h-20)}{(h-10)^2}$. If we input that left equation in Wofram Alpha, we don't get the solution on the right: http://www.wolframalpha.com/input/?i=integrate+%28100x%28x-20%29%2F%28x-10%29%5E2%29+with+respect+to+x (see comments in the graydad's answer) What's going on? Why isn't my method giving me the right answer? For reference: question a solution to question a my solution to question a (which is correct)","I've been working on a set of problems for the past 6 hours and I'm about to explode from frustration! Either my book has errors, Wolfram Alpha is broken or integration by substitution does not work! I'm trying to solve the following ODE: $$ \frac{\mathrm dh}{\mathrm dt}=\frac{1-20kh+kh^2}{20h-h^2},\text{ where }k=0.01 $$ Here is what I have (the first line is the ODE I'm trying to solve, if it's unclear): $$\begin{align} \frac{\mathrm dh}{\mathrm dt}&=\frac{1-20kh+kh^2}{20h-h^2}\\ \frac{20h-h^2}{1-20kh+kh^2}\,\mathrm dh&=\mathrm dt\\[5pt] \frac{20h-h^2}{1-0.2+0.01h^2}\,\mathrm dh&=\mathrm dt,\text{ since }k=0.01\\[5pt] \frac{100\left(20h-h^2\right)}{100-20h+h^2}\,\mathrm dh=\mathrm dt\\[5pt] \int\frac{2000h-100h^2}{(h-10)^2}\,\mathrm dh&=\int\mathrm dt \end{align}$$ Let $u=h-10$ So, $h=u+10$ And, $\mathrm dh=\mathrm du$ $$\begin{align}\require{cancel} \int\left(\frac{2000(u+10)}{u^2}-\frac{100(u+10)^2}{u^2}\right)\,\mathrm du&=t+c\\[5pt] \int\left(\frac{2000u}{u^2}+\frac{20000}{u^2}-\frac{100(u^2+20u+100)}{u^2}\right)\,\mathrm du&=t+c\\[5pt] \int\left(\cancel{\frac{2000}u}+\frac{20000}{u^2}-100-\cancel{\frac{2000}u}-\frac{10000}{u^2}\right)\,\mathrm du&=t+c\\[5pt] \int\left(\frac{10000}{u^2}-100\right)\,\mathrm du&=t+c\\[5pt] -\frac{10000}u-100u&=t+c\\[5pt] -\frac{10000}{h-10}-100(h-10)&=t+c\\[5pt] -\frac{10000-100(h-10)^2}{h-10}&=t+c\\[5pt] -\frac{100\left(100+(h-10)^2\right)}{h-10}&=t+c\\[5pt] -\frac{100\left(100+\left(h^2-20h+100\right)\right)}{h-10}&=t+c\\[5pt] \frac{100\left(h^2-20h+200\right)}{10-h}&=t+c \end{align}$$ But according to Wolfram Alpha, that isn't the correct answer. http://wolframalpha.com/input/?i=integrate+%28100x%2820-x%29%2F%28x-10%29%5E2%29+with+respect+to+x And according to my book's solution manual, this is the answer (same as Wolfram Alpha's answer): (b) Letting $k=1/100$, separating variables and integrating (with the help of a CAS), we get $$\frac{100h(h-20)}{(h-10)^2}\,\mathrm dh=\mathrm dt\;\text{and}\;\frac{100\left(h^2-10h+100\right)}{10-h}=t+c$$ The equation on the right in the solutions manual is the same as what Wolfram Alpha outputs. However, it seems that there's a typo in the equation on the left. It should be $\frac{100h(20-h)}{(h-10)^2}$ instead of $\frac{100h(h-20)}{(h-10)^2}$. If we input that left equation in Wofram Alpha, we don't get the solution on the right: http://www.wolframalpha.com/input/?i=integrate+%28100x%28x-20%29%2F%28x-10%29%5E2%29+with+respect+to+x (see comments in the graydad's answer) What's going on? Why isn't my method giving me the right answer? For reference: question a solution to question a my solution to question a (which is correct)",,['ordinary-differential-equations']
66,Wave equation boundary condition,Wave equation boundary condition,,"I understand that: 1.Setting a boundary condition of the type $$u(x_0,t)=0$$ means that the wave will reflect and change sign. 2.Setting a boundary condition of the type $$\frac{du}{dx}(x_0,t)=0$$ means that the wave will reflect and not change sign. My question: What happens to the incoming wave if I set both types of boundary condition at the same point?","I understand that: 1.Setting a boundary condition of the type $$u(x_0,t)=0$$ means that the wave will reflect and change sign. 2.Setting a boundary condition of the type $$\frac{du}{dx}(x_0,t)=0$$ means that the wave will reflect and not change sign. My question: What happens to the incoming wave if I set both types of boundary condition at the same point?",,"['ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem']"
67,homogenous linear equation,homogenous linear equation,,I have an equation that I can't solve. I think it's homogenous. $$ y' =\frac{y^2 + x \sqrt{4x^2 + y^2}}{xy} $$,I have an equation that I can't solve. I think it's homogenous. $$ y' =\frac{y^2 + x \sqrt{4x^2 + y^2}}{xy} $$,,"['linear-algebra', 'ordinary-differential-equations', 'homogeneous-equation']"
68,Trivial solution of a differential equation,Trivial solution of a differential equation,,"I have the following ODE $$u''(x)+4x^{3/2}\ln x\,u'(x)+8x\,u(x) = 0$$ Can I say that $u(x)$ has a trivial solution when $x = 0$? I am a little confused as to what trivial solution means when it comes to differential equations.","I have the following ODE $$u''(x)+4x^{3/2}\ln x\,u'(x)+8x\,u(x) = 0$$ Can I say that $u(x)$ has a trivial solution when $x = 0$? I am a little confused as to what trivial solution means when it comes to differential equations.",,['ordinary-differential-equations']
69,How to find the characteristic number of a given integral equation?,How to find the characteristic number of a given integral equation?,,How to find the characteristic number of the following integral equation? $$y(x)= \lambda \int_{0}^{1} (3x-2)ty(t)dt$$,How to find the characteristic number of the following integral equation? $$y(x)= \lambda \int_{0}^{1} (3x-2)ty(t)dt$$,,"['ordinary-differential-equations', 'integral-equations']"
70,"If $x>0$we have $(1+x^2)f'(x)+(1+x)f(x)=1$ and $g'(x)=f(x), f(0)=g(0)=0$Prove:",If we have  and Prove:,"x>0 (1+x^2)f'(x)+(1+x)f(x)=1 g'(x)=f(x), f(0)=g(0)=0","If $x>0$ we have $(1+x^2)f'(x)+(1+x)f(x)=1$. And $g'(x)=f(x), f(0)=g(0)=0$ Prove that:$\displaystyle \frac14<\sum_{n=1}^{\infty}g(\frac1n)<1$ I tried solving the ODE,But it seems very complex.and I still have no idea about it.Could someone help me? Thanks!","If $x>0$ we have $(1+x^2)f'(x)+(1+x)f(x)=1$. And $g'(x)=f(x), f(0)=g(0)=0$ Prove that:$\displaystyle \frac14<\sum_{n=1}^{\infty}g(\frac1n)<1$ I tried solving the ODE,But it seems very complex.and I still have no idea about it.Could someone help me? Thanks!",,"['calculus', 'analysis', 'ordinary-differential-equations', 'inequality', 'power-series']"
71,When is a system called linear?,When is a system called linear?,,In real time systems / control engineering we have to solve exercises like this: Check if the following systems are linear: 1) $0.2\ddot{x}(t) - (t^2 + 2t -1) x(t) = 3 w(t)$ 2) $\ddot{x}(t) + \dot{x}(t) + x(t) = w(t)$ A student created the following solution: 1) $$\begin{align}&0.2 \frac{d^2 (h x_1 (t) + l x_2(t))}{d t^2} - (t^2 +2t-1)(h x_1(t) + l x_2(t))\\ =&0.2 h \ddot{x_1}(t) - (t^2 + 2t-1) h x_1(t) + 0.2 l \ddot{x}_2(t) - (t^2 + 2t-1) l x_2(t)\\ =& h w_1(t) + l w_2(t) \Rightarrow \text{linear}\end{align}$$ 2) not linear because of +1 see Laplace: $s^2 X(s) + s X(s) + X(s) + \frac{1}{s} = W(1) \Rightarrow \frac{X(s)}{W(s)} = \frac{1}{s^2 + s +1} - \frac{1}{s(s^2 + s + 1) W(s)} \Rightarrow$ no transfer function $\Rightarrow$ not linear. Could somebody please explain how one can decide for arbitrary systems if they are linear? I don't understand the steps which were done in (1) either.,In real time systems / control engineering we have to solve exercises like this: Check if the following systems are linear: 1) $0.2\ddot{x}(t) - (t^2 + 2t -1) x(t) = 3 w(t)$ 2) $\ddot{x}(t) + \dot{x}(t) + x(t) = w(t)$ A student created the following solution: 1) $$\begin{align}&0.2 \frac{d^2 (h x_1 (t) + l x_2(t))}{d t^2} - (t^2 +2t-1)(h x_1(t) + l x_2(t))\\ =&0.2 h \ddot{x_1}(t) - (t^2 + 2t-1) h x_1(t) + 0.2 l \ddot{x}_2(t) - (t^2 + 2t-1) l x_2(t)\\ =& h w_1(t) + l w_2(t) \Rightarrow \text{linear}\end{align}$$ 2) not linear because of +1 see Laplace: $s^2 X(s) + s X(s) + X(s) + \frac{1}{s} = W(1) \Rightarrow \frac{X(s)}{W(s)} = \frac{1}{s^2 + s +1} - \frac{1}{s(s^2 + s + 1) W(s)} \Rightarrow$ no transfer function $\Rightarrow$ not linear. Could somebody please explain how one can decide for arbitrary systems if they are linear? I don't understand the steps which were done in (1) either.,,"['ordinary-differential-equations', 'control-theory', 'linear-control']"
72,How do I solve $(x\cos y-y\sin y)dy+(x\sin y+y\cos y)dx=0$,How do I solve,(x\cos y-y\sin y)dy+(x\sin y+y\cos y)dx=0,"$(x\cos y-y\sin y)dy+(x\sin y+y\cos y)dx=0$ ATTEMPT: Rearranging the terms: $(x\cos ydy+y\cos ydx) -y\sin ydy+x\sin ydx=0$ Dividing by $\cos x$ we get: $(xdy+ydx)-y\tan ydy+x\tan ydx=0$ $ d(xy)-y\tan ydy+x\tan ydx=0$ But i am not able to simplify it further. I also tried partial derivatives: Let $(x\sin y+y\cos y)=M(x,y)$ and $(x\cos y-y\sin y)=N(x,y)$ This is not an exact differential equation as $\frac{\partial M(x,y)}{\partial y} \ne \frac{\partial N(x,y)}{\partial x}$ But $\frac{\frac{\partial M(x,y)}{\partial y} - \frac{\partial N(x,y)}{\partial x}}{N(x,y)}=1$ which is independent of $ x, y$. Can i use this result to solve my problem?","$(x\cos y-y\sin y)dy+(x\sin y+y\cos y)dx=0$ ATTEMPT: Rearranging the terms: $(x\cos ydy+y\cos ydx) -y\sin ydy+x\sin ydx=0$ Dividing by $\cos x$ we get: $(xdy+ydx)-y\tan ydy+x\tan ydx=0$ $ d(xy)-y\tan ydy+x\tan ydx=0$ But i am not able to simplify it further. I also tried partial derivatives: Let $(x\sin y+y\cos y)=M(x,y)$ and $(x\cos y-y\sin y)=N(x,y)$ This is not an exact differential equation as $\frac{\partial M(x,y)}{\partial y} \ne \frac{\partial N(x,y)}{\partial x}$ But $\frac{\frac{\partial M(x,y)}{\partial y} - \frac{\partial N(x,y)}{\partial x}}{N(x,y)}=1$ which is independent of $ x, y$. Can i use this result to solve my problem?",,['ordinary-differential-equations']
73,Finding general solution to Partial Differential Equations,Finding general solution to Partial Differential Equations,,"I am asked to find the general solution $f(x, y)$ of the partial differential equation: $\frac{\partial ^2 f}{\partial x \partial y}=e ^ {x+2y}$ I know these are relatively easy to solve, I haven't done them in a while and have forgotten how to go about solving them, I haven't yet found an good internet source that explains them straightforwardly. To attempt a solution, I first found the integral, $\int e^x e^y dx=e^x e^y +g(y)$ Next, integrating this with respect to $y$, $\int (e^x e^y +g(y))  \space dy$ solving this becomes, $ = e^x e^y +yg(y) +h(x)$ Is my reasoning correct? If I integrate a partial derivative with respect to $x$, will the constant become $g(y)$ and if I integrate a partial derivative with respect to $y$, will the content become $h(x)$?","I am asked to find the general solution $f(x, y)$ of the partial differential equation: $\frac{\partial ^2 f}{\partial x \partial y}=e ^ {x+2y}$ I know these are relatively easy to solve, I haven't done them in a while and have forgotten how to go about solving them, I haven't yet found an good internet source that explains them straightforwardly. To attempt a solution, I first found the integral, $\int e^x e^y dx=e^x e^y +g(y)$ Next, integrating this with respect to $y$, $\int (e^x e^y +g(y))  \space dy$ solving this becomes, $ = e^x e^y +yg(y) +h(x)$ Is my reasoning correct? If I integrate a partial derivative with respect to $x$, will the constant become $g(y)$ and if I integrate a partial derivative with respect to $y$, will the content become $h(x)$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative']"
74,Solving linear differential equations,Solving linear differential equations,,Find the general solution for the following equation: $$\frac{dy}{dt}+2ty=\sin(t)e^{-t^2}$$   Find a solution for which $y(0)=0$ First I found the integrating factor which is $e^{t^2}$ Multiplying both sides gives $$e^{t^2}\frac{dy}{dt}+e^{t^2}2ty=e^{t^2}\sin(t)e^{-t^2}$$ which simplifies to $$\frac{d}{dt}(e^{t^2}y)=\sin(t)$$ Integrating both sides gives $$e^{t^2}y=-\cos(t)$$ Now rearranging gives $$y(t)=\frac{-\cos(t)}{e^{t^2}}$$ However this doesnt give $y(0)=0$ could anyone help as to where I have gone wrong? thanks!,Find the general solution for the following equation: $$\frac{dy}{dt}+2ty=\sin(t)e^{-t^2}$$   Find a solution for which $y(0)=0$ First I found the integrating factor which is $e^{t^2}$ Multiplying both sides gives $$e^{t^2}\frac{dy}{dt}+e^{t^2}2ty=e^{t^2}\sin(t)e^{-t^2}$$ which simplifies to $$\frac{d}{dt}(e^{t^2}y)=\sin(t)$$ Integrating both sides gives $$e^{t^2}y=-\cos(t)$$ Now rearranging gives $$y(t)=\frac{-\cos(t)}{e^{t^2}}$$ However this doesnt give $y(0)=0$ could anyone help as to where I have gone wrong? thanks!,,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives', 'elementary-functions']"
75,Are Friedmann equations linear or nonlinear?,Are Friedmann equations linear or nonlinear?,,"I'm trying to improve my understanding of cosmology, and these 2 equations are basic . You can find them here: https://en.wikipedia.org/wiki/Friedmann_equations Also, if you could tell why they are or aren't linear, it would help me a lot.","I'm trying to improve my understanding of cosmology, and these 2 equations are basic . You can find them here: https://en.wikipedia.org/wiki/Friedmann_equations Also, if you could tell why they are or aren't linear, it would help me a lot.",,"['ordinary-differential-equations', 'mathematical-physics']"
76,Non linear system of differential equations,Non linear system of differential equations,,"Is there a specific name to the following type of non linear ODEs $\begin{array}{c} \dot{x}_1 &=   c_1 \, x_2\, x_3 \\  \dot{x}_2 &=   \,  c_2 x_1 x_3 \\ \dot{x}_3  &= c_3 \, x_2 x_1   \end{array} $ where $c_i$ are real constants. More specifically, for a higher dimensional version, $\dot{x}_i = \sum_{\begin{array}{c}r,s \\ r \neq s \neq i  \end{array}}  c_{ir} x_r x_s$ Are there any known transformations to simplify things? I conjecture the solutions will be elliptic functions of some sort, but you might not want to write them down. For the three dimensional case one can obtain $ \frac{d}{dt}(\frac{1}{2} ( x_2^2 + x_3^3) ) = 0$ $ \implies x_2^2 + x_3^3 = 2 C$ And use this to transform some of the equations, eventually finding  elliptic solutions for all the $x_i$. In higher dimensional case I would expect a similar situation.","Is there a specific name to the following type of non linear ODEs $\begin{array}{c} \dot{x}_1 &=   c_1 \, x_2\, x_3 \\  \dot{x}_2 &=   \,  c_2 x_1 x_3 \\ \dot{x}_3  &= c_3 \, x_2 x_1   \end{array} $ where $c_i$ are real constants. More specifically, for a higher dimensional version, $\dot{x}_i = \sum_{\begin{array}{c}r,s \\ r \neq s \neq i  \end{array}}  c_{ir} x_r x_s$ Are there any known transformations to simplify things? I conjecture the solutions will be elliptic functions of some sort, but you might not want to write them down. For the three dimensional case one can obtain $ \frac{d}{dt}(\frac{1}{2} ( x_2^2 + x_3^3) ) = 0$ $ \implies x_2^2 + x_3^3 = 2 C$ And use this to transform some of the equations, eventually finding  elliptic solutions for all the $x_i$. In higher dimensional case I would expect a similar situation.",,"['ordinary-differential-equations', 'multivariable-calculus', 'systems-of-equations', 'elliptic-functions']"
77,"Existence of a solution for a nonlinear ODE on $[0,\infty)$",Existence of a solution for a nonlinear ODE on,"[0,\infty)","I'd like to prove that the solution to the following IVP exists on $[0,\infty)$. The IVP is given by $$ \begin{cases}        y'(t) = y^2 \cos(t)-ye^t  \\       y(0)= y_0    \end{cases} $$ where $y_0 \in \mathbb{R}$. I've already established unique solvability in an interval around the origin, say $[0,\epsilon)$ for some $\epsilon>0$ because $f(y,t)=y^2 \cos(t)-ye^t$ is lipschitz in $y$ in a neighborhood of the origin. My usual strategy for showing a solution exists on such an interval is to try to find a upper/lower solution to make a bound for the solutions and thus use the bound to generate some information about what happens to the solution as $t \rightarrow \infty$. This, however, is difficult in this case because of the $e^t$ term. How might I show existence on $[0,\infty)$ given any initial condition, for $y_0$?","I'd like to prove that the solution to the following IVP exists on $[0,\infty)$. The IVP is given by $$ \begin{cases}        y'(t) = y^2 \cos(t)-ye^t  \\       y(0)= y_0    \end{cases} $$ where $y_0 \in \mathbb{R}$. I've already established unique solvability in an interval around the origin, say $[0,\epsilon)$ for some $\epsilon>0$ because $f(y,t)=y^2 \cos(t)-ye^t$ is lipschitz in $y$ in a neighborhood of the origin. My usual strategy for showing a solution exists on such an interval is to try to find a upper/lower solution to make a bound for the solutions and thus use the bound to generate some information about what happens to the solution as $t \rightarrow \infty$. This, however, is difficult in this case because of the $e^t$ term. How might I show existence on $[0,\infty)$ given any initial condition, for $y_0$?",,"['real-analysis', 'ordinary-differential-equations']"
78,transform integral to differential equations,transform integral to differential equations,,"I found a similar system of integral equations in a paper. It says that it can be solved by differentiating and then using standard techniques. My question is, how can I differentiate such a system in general? Do I have to differentiate for $x$ or $y$ or for both? $$f(y)+f(x) + xh(x) + \int_x^y zg(z)dz=a, \qquad \forall x <y $$ $$f(x)+h(x) + \int_x^\infty g(z)dz=1 $$ $$f(x), h(x),\qquad g(z) \geq 0$$ where $x$ and $y$ are variables in $\mathbb{R}$ and $f,g,h$ are functions to be determined. I would be very glad for any help on how to proceed here!","I found a similar system of integral equations in a paper. It says that it can be solved by differentiating and then using standard techniques. My question is, how can I differentiate such a system in general? Do I have to differentiate for $x$ or $y$ or for both? $$f(y)+f(x) + xh(x) + \int_x^y zg(z)dz=a, \qquad \forall x <y $$ $$f(x)+h(x) + \int_x^\infty g(z)dz=1 $$ $$f(x), h(x),\qquad g(z) \geq 0$$ where $x$ and $y$ are variables in $\mathbb{R}$ and $f,g,h$ are functions to be determined. I would be very glad for any help on how to proceed here!",,"['calculus', 'ordinary-differential-equations', 'systems-of-equations', 'integral-equations']"
79,Non linear second order ODE,Non linear second order ODE,,"I really need help solving this : $$y_{xx}-\left(y^{3}-y\right)-\varepsilon\frac{1}{2}\left(1-y^{2}\right)=0   $$ With boundary conditions : $$ y(\pm \infty  )=-1 $$ I need to find a solution that is accurate up to $O(\epsilon)$ i all so know that $$ y^{(k)}(\pm \infty) = 0\;\;\;\;\; for\; \; k>0 $$ Thanks alot! Please note that holding boundary condition is not easy and very important,  the answer given below is a great effort but unfortunatly it is not a good answer. any help would be great!","I really need help solving this : $$y_{xx}-\left(y^{3}-y\right)-\varepsilon\frac{1}{2}\left(1-y^{2}\right)=0   $$ With boundary conditions : $$ y(\pm \infty  )=-1 $$ I need to find a solution that is accurate up to $O(\epsilon)$ i all so know that $$ y^{(k)}(\pm \infty) = 0\;\;\;\;\; for\; \; k>0 $$ Thanks alot! Please note that holding boundary condition is not easy and very important,  the answer given below is a great effort but unfortunatly it is not a good answer. any help would be great!",,"['ordinary-differential-equations', 'asymptotics', 'nonlinear-system', 'perturbation-theory']"
80,Differential Equations Lectures or books from a theoretical perspective?,Differential Equations Lectures or books from a theoretical perspective?,,"I am looking for some differential equation lectures from a theoretical perspective, not a strictly computational one. I found the MIT 18.03 lectures which (as the professor says towards the end of the first lectures) ""is not going to be a course for those theoretically inclined."" I would prefer something more geared towards the theory of differential equations. Book and other resource recommendation are also welcome, but I would prefer lectures.","I am looking for some differential equation lectures from a theoretical perspective, not a strictly computational one. I found the MIT 18.03 lectures which (as the professor says towards the end of the first lectures) ""is not going to be a course for those theoretically inclined."" I would prefer something more geared towards the theory of differential equations. Book and other resource recommendation are also welcome, but I would prefer lectures.",,"['ordinary-differential-equations', 'reference-request']"
81,Solution of a variable-coefficient ODE system,Solution of a variable-coefficient ODE system,,"A system of ODE's is defined as: $$\frac{du}{dx}  =Au$$ where $u$ is a vector and $A$ is the coefficient matrix.  As we know, the solution is obtained by solving the eigenvalue problem: $$det(A-rI)  =0$$ where $I$ is the identity matrix. Here is the question: If $A$ were dependent on the independent parameter $x$, how would i get the solution? Would ""the eigenvalue solution"" still valid in this case? Best wishes..","A system of ODE's is defined as: $$\frac{du}{dx}  =Au$$ where $u$ is a vector and $A$ is the coefficient matrix.  As we know, the solution is obtained by solving the eigenvalue problem: $$det(A-rI)  =0$$ where $I$ is the identity matrix. Here is the question: If $A$ were dependent on the independent parameter $x$, how would i get the solution? Would ""the eigenvalue solution"" still valid in this case? Best wishes..",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
82,"Does there exist a function $\psi:[0,\infty) \rightarrow \mathbb{R}$ such that......?",Does there exist a function  such that......?,"\psi:[0,\infty) \rightarrow \mathbb{R}","Does there exist a function  $\psi:[0,\infty) \rightarrow \mathbb{R}$ such that $\psi (x) \geq 0$, $\psi'(x)\neq 0$, $\psi''(x) < 0$ and  \begin{equation*} \alpha\cdot \psi''(x)^2-\psi'(x)\,\psi'''(x)>0 \end{equation*} with $0<\alpha <1$?","Does there exist a function  $\psi:[0,\infty) \rightarrow \mathbb{R}$ such that $\psi (x) \geq 0$, $\psi'(x)\neq 0$, $\psi''(x) < 0$ and  \begin{equation*} \alpha\cdot \psi''(x)^2-\psi'(x)\,\psi'''(x)>0 \end{equation*} with $0<\alpha <1$?",,"['real-analysis', 'ordinary-differential-equations']"
83,problems with differential equation,problems with differential equation,,i have problems solving eq.  $$ u + \log(u-1) = \log (x); \quad u= \frac{y}{x}$$ which comes from solving diff equation  $$x \frac{dy}{dx} - y= x\frac{y-x}{y+x}$$ any hints? thanks in advance,i have problems solving eq.  $$ u + \log(u-1) = \log (x); \quad u= \frac{y}{x}$$ which comes from solving diff equation  $$x \frac{dy}{dx} - y= x\frac{y-x}{y+x}$$ any hints? thanks in advance,,['ordinary-differential-equations']
84,How to calculate the continuum limit of a discrete system?,How to calculate the continuum limit of a discrete system?,,"The question is based on the following excerpt from the book ""Symmetries and Integrability of Difference Equations"" Link: Book Excerpt Consider the discrete equation $$x_{n+1}+x_{n}+x_{n-1}=\frac{\alpha+\beta n}{x_{n}}+b.$$ To find the continuum limit of the above equation we introduce a small parameter $\epsilon$ such that $$\epsilon n=z,\qquad\qquad x_{n}=f(z),\qquad\qquad x_{n\pm 1}=f(z\pm\epsilon)$$ and then take the limit $$\epsilon\rightarrow 0,\qquad\qquad n\rightarrow\infty,\qquad\qquad\epsilon n\;\;\text{fixed}$$ which yields $$3f+\epsilon^{2}f''+\mathcal{O}(\epsilon^{4})=\frac{\alpha+\beta z/\epsilon}{f}+b.\qquad\qquad (*)$$ I have no idea on how one gets equation $(*)$. Any suggestions and hints will be appreciated.","The question is based on the following excerpt from the book ""Symmetries and Integrability of Difference Equations"" Link: Book Excerpt Consider the discrete equation $$x_{n+1}+x_{n}+x_{n-1}=\frac{\alpha+\beta n}{x_{n}}+b.$$ To find the continuum limit of the above equation we introduce a small parameter $\epsilon$ such that $$\epsilon n=z,\qquad\qquad x_{n}=f(z),\qquad\qquad x_{n\pm 1}=f(z\pm\epsilon)$$ and then take the limit $$\epsilon\rightarrow 0,\qquad\qquad n\rightarrow\infty,\qquad\qquad\epsilon n\;\;\text{fixed}$$ which yields $$3f+\epsilon^{2}f''+\mathcal{O}(\epsilon^{4})=\frac{\alpha+\beta z/\epsilon}{f}+b.\qquad\qquad (*)$$ I have no idea on how one gets equation $(*)$. Any suggestions and hints will be appreciated.",,"['calculus', 'ordinary-differential-equations', 'limits']"
85,Solve $(x^2 + 1)y'' - 6xy' + 10y =0$ using series method,Solve  using series method,(x^2 + 1)y'' - 6xy' + 10y =0,"Use series methods to solve: $(x^2 + 1)y'' - 6xy' + 10y =0$ a) Give the recursion formula b) Give the first two non-zero terms of the solution corresponding to $a_0 = 1$ and $a_1 = 0$ c) Give the first three non-zero terms of the solution corresponding to $a_0 = 0$ and $a_1 = 1$ Here is what I have so far, then I get stuck on the series when I am trying to get all of my $n = 0$ within each series. $y = \sum^{\infty}_{n=0} na_nX^n$ $y' = \sum^{\infty}_{n=1} na_nX^{n-1}$ $y'' = \sum^{\infty}_{n=2} n(n-1)a_nX^{n-2}$ so we have: $(x^2+1)[\sum^{\infty}_{n=2} n(n-1)a_nX^{n-2}] - (6x) \sum^{\infty}_{n=1} na_nX^{n-1} + 10\sum^{\infty}_{n=0} na_nX^n $ After doing some series manipulation I got: $= \sum^{\infty}_{n=2} n(n-1)a_nX^n + \sum^{\infty}_{n=0} (n+2)(n+1)a_{n+2}X^n - \sum^{\infty}_{n=0} 6na_nX^n + \sum^{\infty}_{n=0} 10na_nX^n$ Now I'm stuck on what to do with the first series that has $n=2$ because I don't want to mess up the powers.","Use series methods to solve: a) Give the recursion formula b) Give the first two non-zero terms of the solution corresponding to and c) Give the first three non-zero terms of the solution corresponding to and Here is what I have so far, then I get stuck on the series when I am trying to get all of my within each series. so we have: After doing some series manipulation I got: Now I'm stuck on what to do with the first series that has because I don't want to mess up the powers.",(x^2 + 1)y'' - 6xy' + 10y =0 a_0 = 1 a_1 = 0 a_0 = 0 a_1 = 1 n = 0 y = \sum^{\infty}_{n=0} na_nX^n y' = \sum^{\infty}_{n=1} na_nX^{n-1} y'' = \sum^{\infty}_{n=2} n(n-1)a_nX^{n-2} (x^2+1)[\sum^{\infty}_{n=2} n(n-1)a_nX^{n-2}] - (6x) \sum^{\infty}_{n=1} na_nX^{n-1} + 10\sum^{\infty}_{n=0} na_nX^n  = \sum^{\infty}_{n=2} n(n-1)a_nX^n + \sum^{\infty}_{n=0} (n+2)(n+1)a_{n+2}X^n - \sum^{\infty}_{n=0} 6na_nX^n + \sum^{\infty}_{n=0} 10na_nX^n n=2,"['sequences-and-series', 'ordinary-differential-equations', 'power-series']"
86,An application of Implicit Function Theorem in differential equations?,An application of Implicit Function Theorem in differential equations?,,"Let $f$ be a continuous function from $\Bbb R^3 \to \Bbb R$. By a solution of the differential equation $$f(x,y,\dot{y}) = 0$$ We mean a function $y\colon U \subset \Bbb R \to \Bbb R$ where $u$ is an open interval in $R$ that satisfies $f(x,y(x),\dot{y}(x))= 0$. Let $f\colon \Bbb R^3 \to\Bbb R$ be a smooth function. Let $f(0,0,0) = 0$. Give a sufficiency condition under which the differential equation $f(x,y,\dot{y})$ admits a solution satisfying the initial condition $y(0) = 0$ in a neighbourhood of $0$. I think I must use the Implicit Function Theorem but not sure how?","Let $f$ be a continuous function from $\Bbb R^3 \to \Bbb R$. By a solution of the differential equation $$f(x,y,\dot{y}) = 0$$ We mean a function $y\colon U \subset \Bbb R \to \Bbb R$ where $u$ is an open interval in $R$ that satisfies $f(x,y(x),\dot{y}(x))= 0$. Let $f\colon \Bbb R^3 \to\Bbb R$ be a smooth function. Let $f(0,0,0) = 0$. Give a sufficiency condition under which the differential equation $f(x,y,\dot{y})$ admits a solution satisfying the initial condition $y(0) = 0$ in a neighbourhood of $0$. I think I must use the Implicit Function Theorem but not sure how?",,['ordinary-differential-equations']
87,Chemical kinetics using Laplace transformation,Chemical kinetics using Laplace transformation,,"I have a simple chemical reaction $A\leftrightarrow B$ with forward rate $k_1$ and backward rate $k_2$. I can now write the differential equation of this system as following. $ \frac{dA}{dt} = -k_1A + k_2B, \quad \frac{dB}{dt} = k_1A - k_2B$ Assuming that reactant A initial concentration is $A_0$, I took the Laplace transform: $ sA(s) - A_0 = -k_1 A(s) +k_2B(s), \quad sB(s) - 0 = k_1A(s) - k_2B(s)$ So far so good. I was hoping to solve for $B(t)$ using any of these two equations, with the initial condition, $A(s) = \frac{A_0}{s}$. Now, for the first equation, $$ sA(s) - A_0 = -k_1 A(s) + k_2B(s) \\ A_0 - A_0 = -k_1\frac{A_0}{s} + k_2 B(s) \implies B(s) = \frac{k_1A_0}{k_2s}\\ $$ This is not correct! If I use the second equation, $$ B(s) = \frac{k_1A(s)}{s+k_2} = \frac{k_1A_0}{s(s+k_2)}$$ This gives me what I was expecting. Certainly, I missed something somewhere but I can't figure out what!","I have a simple chemical reaction $A\leftrightarrow B$ with forward rate $k_1$ and backward rate $k_2$. I can now write the differential equation of this system as following. $ \frac{dA}{dt} = -k_1A + k_2B, \quad \frac{dB}{dt} = k_1A - k_2B$ Assuming that reactant A initial concentration is $A_0$, I took the Laplace transform: $ sA(s) - A_0 = -k_1 A(s) +k_2B(s), \quad sB(s) - 0 = k_1A(s) - k_2B(s)$ So far so good. I was hoping to solve for $B(t)$ using any of these two equations, with the initial condition, $A(s) = \frac{A_0}{s}$. Now, for the first equation, $$ sA(s) - A_0 = -k_1 A(s) + k_2B(s) \\ A_0 - A_0 = -k_1\frac{A_0}{s} + k_2 B(s) \implies B(s) = \frac{k_1A_0}{k_2s}\\ $$ This is not correct! If I use the second equation, $$ B(s) = \frac{k_1A(s)}{s+k_2} = \frac{k_1A_0}{s(s+k_2)}$$ This gives me what I was expecting. Certainly, I missed something somewhere but I can't figure out what!",,"['ordinary-differential-equations', 'laplace-transform', 'chemistry']"
88,"Given $\frac{dy}{dx}=x^2+y^2$ and initial condition $\varphi (0)=1$, find the first 6 terms in the Taylor expansion solution $y=\varphi (x)$","Given  and initial condition , find the first 6 terms in the Taylor expansion solution",\frac{dy}{dx}=x^2+y^2 \varphi (0)=1 y=\varphi (x),"Given $\frac{dy}{dx}=x^2+y^2$ and initial condition $\varphi (0)=1$, use the method of reduction to an integral equation and successive approximation to find the first 6 terms in the Taylor expansion solution $y=\varphi (x)$. We have that if $f:D \rightarrow \mathbb{R}$ is continuous, $\varphi$ is defined and continuous on $I=\{x|x_0-h<x<x_0+h\}$ to $\mathbb{R}$, and $(x_0,y_0) \in D$ with $\varphi (x_0)=y_0$, then $\varphi$ is a solution of $\frac{d\varphi}{dx}=f[x,\varphi (x)]$ on $I$ only if $\varphi (x)=y_0+\int_{x_0}^{x} f[t,\varphi (t)] dt$ for $x \in I$. This is what I mean by reduction to an integral equation. EDIT: In case it helps anyone refine their answer, the answer should be $1+x+x^2+\frac{4}{3}x^3+\frac{7}{6}x^4+\frac{6}{5}x^5$. This comes out of the back of my book. Of course, my problem is that I cannot get to this answer.","Given $\frac{dy}{dx}=x^2+y^2$ and initial condition $\varphi (0)=1$, use the method of reduction to an integral equation and successive approximation to find the first 6 terms in the Taylor expansion solution $y=\varphi (x)$. We have that if $f:D \rightarrow \mathbb{R}$ is continuous, $\varphi$ is defined and continuous on $I=\{x|x_0-h<x<x_0+h\}$ to $\mathbb{R}$, and $(x_0,y_0) \in D$ with $\varphi (x_0)=y_0$, then $\varphi$ is a solution of $\frac{d\varphi}{dx}=f[x,\varphi (x)]$ on $I$ only if $\varphi (x)=y_0+\int_{x_0}^{x} f[t,\varphi (t)] dt$ for $x \in I$. This is what I mean by reduction to an integral equation. EDIT: In case it helps anyone refine their answer, the answer should be $1+x+x^2+\frac{4}{3}x^3+\frac{7}{6}x^4+\frac{6}{5}x^5$. This comes out of the back of my book. Of course, my problem is that I cannot get to this answer.",,"['real-analysis', 'ordinary-differential-equations', 'fixed-point-theorems']"
89,Solving Systems of Linear Differential Equations by Elimination,Solving Systems of Linear Differential Equations by Elimination,,"For a homework problem, we are provided: $\frac{dx}{dt}=-y + t$ $\frac{dy}{dt}=x-t$ Putting these into differential operator notation and separating the dependent variables from the independent: $Dx-y=t$ $Dy-x=-t$ My first inclination is to apply the D operator to the second equation to eliminate Dx and get: $D^2y+y=t-1$ I solve the homogenous part and end up with $y_c=C_1\cos(t) + C_2\sin(t).$ Using annihilator approach and method of undetermined coefficients, I determine that $y_p=t-1$. General solution for $y(t) = C_1\cos(t)+C_2\sin(t)+t-1$. After plugging $y$ into the second equation, I get $x(t)=-C_1\sin(t)+C_2\cos(t)+1+t$ Checking my answer against the back of the book, they show: $x(t) = C_1\cos(t)+C_2\sin(t)+t+1$ and $y(t)=C_1\sin(t)-C_2\cos(t)+t-1$ I can't seem to find what I did wrong.  Chegg solutions shows to eliminate y instead of x, and got the book's solution.  Does the variable chosen for elimination matter?  Halp!","For a homework problem, we are provided: $\frac{dx}{dt}=-y + t$ $\frac{dy}{dt}=x-t$ Putting these into differential operator notation and separating the dependent variables from the independent: $Dx-y=t$ $Dy-x=-t$ My first inclination is to apply the D operator to the second equation to eliminate Dx and get: $D^2y+y=t-1$ I solve the homogenous part and end up with $y_c=C_1\cos(t) + C_2\sin(t).$ Using annihilator approach and method of undetermined coefficients, I determine that $y_p=t-1$. General solution for $y(t) = C_1\cos(t)+C_2\sin(t)+t-1$. After plugging $y$ into the second equation, I get $x(t)=-C_1\sin(t)+C_2\cos(t)+1+t$ Checking my answer against the back of the book, they show: $x(t) = C_1\cos(t)+C_2\sin(t)+t+1$ and $y(t)=C_1\sin(t)-C_2\cos(t)+t-1$ I can't seem to find what I did wrong.  Chegg solutions shows to eliminate y instead of x, and got the book's solution.  Does the variable chosen for elimination matter?  Halp!",,"['ordinary-differential-equations', 'systems-of-equations']"
90,Prove a solution of a differential equation is bounded,Prove a solution of a differential equation is bounded,,"I am interested in the differential equation $$ (\mathcal{S})\left\{ \begin{array}{l} y'(t) = \displaystyle \frac{y^{2}(t)}{1+t^{2}+y^{2}(t)} \\[2mm] y(0) = \displaystyle \frac{3}{4} \end{array} \right. $$ Let $y$ be a maximal solution of $(\mathcal{S})$. I already proved that $y$ is defined on $\mathbb{R}$. However, I do not see how to prove that $\displaystyle \lim \limits_{t \to +\infty} y(t)$ is lower than $14$ and $\displaystyle \lim \limits_{t \to -\infty} y(t)$ is greater than $\frac{1}{3}$. A hint would be appreciated !","I am interested in the differential equation $$ (\mathcal{S})\left\{ \begin{array}{l} y'(t) = \displaystyle \frac{y^{2}(t)}{1+t^{2}+y^{2}(t)} \\[2mm] y(0) = \displaystyle \frac{3}{4} \end{array} \right. $$ Let $y$ be a maximal solution of $(\mathcal{S})$. I already proved that $y$ is defined on $\mathbb{R}$. However, I do not see how to prove that $\displaystyle \lim \limits_{t \to +\infty} y(t)$ is lower than $14$ and $\displaystyle \lim \limits_{t \to -\infty} y(t)$ is greater than $\frac{1}{3}$. A hint would be appreciated !",,"['real-analysis', 'ordinary-differential-equations']"
91,What time did the snow start? Snow Plow,What time did the snow start? Snow Plow,,"I am having trouble proceeding with the problem below. I have solved some stuff to a certain extent, but do not understand what to do from here. The problem statement is: One morning snow began to fall at a heavy and constant rate. A snowplow starts out at 8:00am and at 9:00am it has traveled 2 miles. By 10:00am it has traveled 3 miles. Assuming that the plow removes a constant volume of snow per hour, determine the time at which it started to snow. Hint : Let t denote the time since the snow started and T be the time when the snowplow started out. Let x be the distance the snowplow has traveled, and h the height of the snow which is a function of t . Assuming a constant volume of snow per hour is removed implies the speed of the plow times the height of the snow is a constant. Set up and solve differential equations involving dx/dt and dh/dt . My work so far:  $$ \frac {dh}{dt} = C$$ $$ \int {\frac {dh}{dt}} = \int {C}$$ $$ h(t) = Ct+Z $$ $$\frac {dx}{dt} = \frac E{h(t)}$$ $$\frac {dx}{dt} (Ct+Z) = E$$ Separating and integrating this equation, I get : $$x(t) = (\frac EC) \ln |Ct+Z| + F$$ I believe the the conditions are : x(0) = 0, x(1) = 2, x(2) = 3","I am having trouble proceeding with the problem below. I have solved some stuff to a certain extent, but do not understand what to do from here. The problem statement is: One morning snow began to fall at a heavy and constant rate. A snowplow starts out at 8:00am and at 9:00am it has traveled 2 miles. By 10:00am it has traveled 3 miles. Assuming that the plow removes a constant volume of snow per hour, determine the time at which it started to snow. Hint : Let t denote the time since the snow started and T be the time when the snowplow started out. Let x be the distance the snowplow has traveled, and h the height of the snow which is a function of t . Assuming a constant volume of snow per hour is removed implies the speed of the plow times the height of the snow is a constant. Set up and solve differential equations involving dx/dt and dh/dt . My work so far:  $$ \frac {dh}{dt} = C$$ $$ \int {\frac {dh}{dt}} = \int {C}$$ $$ h(t) = Ct+Z $$ $$\frac {dx}{dt} = \frac E{h(t)}$$ $$\frac {dx}{dt} (Ct+Z) = E$$ Separating and integrating this equation, I get : $$x(t) = (\frac EC) \ln |Ct+Z| + F$$ I believe the the conditions are : x(0) = 0, x(1) = 2, x(2) = 3",,"['calculus', 'ordinary-differential-equations']"
92,How to solve linear ODE? $e^{x^2}$ messing it up.,How to solve linear ODE?  messing it up.,e^{x^2},"How do you solve the ODE $\frac{dy}{dx} + 2xy = x^2$? It's a linear equation with $P(x)=2x$ but that mean the integrating factor $e^{\int P(x) dx} = e^{x^2}$ which is really bad because then you integrate $\int x^2e^{x^2}$ which I'm not sure is possible to integrate? By the way, I know there is some way to integrate $e^{x^2}$ involving spherical coordinates. I can't find any examples but if someone has a link to an explanation can they share it? Given the context the question appeared in it is unlikely we were expected to integrate difficult functions (the focus was on ODEs) but maybe there was a typo in the question :(","How do you solve the ODE $\frac{dy}{dx} + 2xy = x^2$? It's a linear equation with $P(x)=2x$ but that mean the integrating factor $e^{\int P(x) dx} = e^{x^2}$ which is really bad because then you integrate $\int x^2e^{x^2}$ which I'm not sure is possible to integrate? By the way, I know there is some way to integrate $e^{x^2}$ involving spherical coordinates. I can't find any examples but if someone has a link to an explanation can they share it? Given the context the question appeared in it is unlikely we were expected to integrate difficult functions (the focus was on ODEs) but maybe there was a typo in the question :(",,"['integration', 'ordinary-differential-equations']"
93,Inverse Laplace Transform of $\frac{s}{(s-a)^{3/2}}$,Inverse Laplace Transform of,\frac{s}{(s-a)^{3/2}},"Find the inverse laplace of: $\frac{s}{(s-a)^{3/2}}$ I tried working through this using partial fractions and convolution but I can't seem to get a requitible answer. How would I go about solving this? (by the way, we have yet to learn the integral definition of inverse laplace transforms, so we are expected not to use that.)","Find the inverse laplace of: $\frac{s}{(s-a)^{3/2}}$ I tried working through this using partial fractions and convolution but I can't seem to get a requitible answer. How would I go about solving this? (by the way, we have yet to learn the integral definition of inverse laplace transforms, so we are expected not to use that.)",,"['ordinary-differential-equations', 'inverse', 'laplace-transform', 'contour-integration']"
94,$\phi(x)-\psi(x)=(\phi(x_0)-\psi(x_0))e^{-\int_{x_0}^x a(t) dt}$,,\phi(x)-\psi(x)=(\phi(x_0)-\psi(x_0))e^{-\int_{x_0}^x a(t) dt},"I am looking at the following exercise: If $\phi, \psi$ solutions of the differential equation $y'+a(x)y=b(x)$ on an interval $I$, where $a,b$ continuous on $I$ and $x_0 \in I$, show that: $$\phi(x)-\psi(x)=(\phi(x_0)-\psi(x_0))e^{-\int_{x_0}^x a(t) dt}$$ There is the following remark: If $I$ is a closed and bounded interval, for example $I=[c,d]$ then the above exercise tells us that: if $\phi, \psi$ are ""near"" at $x_0$ then they are ""near"" at each $x \in I$ i.e. if $|\phi(x_0)-\psi(x_0)|< \epsilon$ for some $\epsilon>0$, then because of the fact that $a$ is continuous on $I$ we have that $\max_{x \in I} e^{-\int_{x_0}^x a(t)dt}$ exists and is a finite number, let $M:=\max_{x \in I} e^{-\int_{x_0}^x a(t)dt}$. Therefore, $|\phi(x)-\psi(x)|< \epsilon \cdot M , \forall x \in I$. Could you explain me how from the fact that $a$ is continuous on $I$ we deduce that $\max_{x \in I} e^{-\int_{x_0}^x a(t)dt}$ exists and is a finite number? Also, which would be the difference if we wouldn't have a closed and bounded interval?","I am looking at the following exercise: If $\phi, \psi$ solutions of the differential equation $y'+a(x)y=b(x)$ on an interval $I$, where $a,b$ continuous on $I$ and $x_0 \in I$, show that: $$\phi(x)-\psi(x)=(\phi(x_0)-\psi(x_0))e^{-\int_{x_0}^x a(t) dt}$$ There is the following remark: If $I$ is a closed and bounded interval, for example $I=[c,d]$ then the above exercise tells us that: if $\phi, \psi$ are ""near"" at $x_0$ then they are ""near"" at each $x \in I$ i.e. if $|\phi(x_0)-\psi(x_0)|< \epsilon$ for some $\epsilon>0$, then because of the fact that $a$ is continuous on $I$ we have that $\max_{x \in I} e^{-\int_{x_0}^x a(t)dt}$ exists and is a finite number, let $M:=\max_{x \in I} e^{-\int_{x_0}^x a(t)dt}$. Therefore, $|\phi(x)-\psi(x)|< \epsilon \cdot M , \forall x \in I$. Could you explain me how from the fact that $a$ is continuous on $I$ we deduce that $\max_{x \in I} e^{-\int_{x_0}^x a(t)dt}$ exists and is a finite number? Also, which would be the difference if we wouldn't have a closed and bounded interval?",,"['ordinary-differential-equations', 'continuity']"
95,Solve $y''-4y'+3y=\frac{2x+1}{x^2}e^x$,Solve,y''-4y'+3y=\frac{2x+1}{x^2}e^x,"I want to solve the linear second order nonhomgenous ODE $y''-4y'+3y=\frac{2x+1}{x^2}e^x$ I found the complementary homogenous solutions: $y_1=e^{3x}$ and $y_2=e^{x}$, the wronskian is $|W|=-2e^{4x}$ So as far as I know, we have 2 options to find a particular solution. We can either calculate this monster integral: $$y_p=e^{3x} \int \frac{e^{2x} \frac{2x+1}{x^2}}{2e^{4x}}dx-e^{x}\int \frac{e^{4x}\frac{2x+1}{x^2}}{2e^{4x}}dx$$ Or we can use the undetermined coefficients method, but for that we need to think of ""what form"" will our solution be. $\frac{2x+1}{x^2}e^x$ is not a simple function, it's not as easy as just a polynomial or just an exponential, what do we do in this case?","I want to solve the linear second order nonhomgenous ODE $y''-4y'+3y=\frac{2x+1}{x^2}e^x$ I found the complementary homogenous solutions: $y_1=e^{3x}$ and $y_2=e^{x}$, the wronskian is $|W|=-2e^{4x}$ So as far as I know, we have 2 options to find a particular solution. We can either calculate this monster integral: $$y_p=e^{3x} \int \frac{e^{2x} \frac{2x+1}{x^2}}{2e^{4x}}dx-e^{x}\int \frac{e^{4x}\frac{2x+1}{x^2}}{2e^{4x}}dx$$ Or we can use the undetermined coefficients method, but for that we need to think of ""what form"" will our solution be. $\frac{2x+1}{x^2}e^x$ is not a simple function, it's not as easy as just a polynomial or just an exponential, what do we do in this case?",,"['calculus', 'integration', 'ordinary-differential-equations']"
96,Simplify Laplace equation in rectangle geometry,Simplify Laplace equation in rectangle geometry,,Consider Laplace's equation in a rectangle as shown in the following figure. The boundary conditions are shown in the figure. The problem is solved in the case of a1 =a2=1 . Is there a way to simplify this problem to a simplified version such that can be solved analytically? (i.e. a rectangle with one constant $a$ using conformal mapping or any other method?). $$\nabla.a\nabla V = 0$$,Consider Laplace's equation in a rectangle as shown in the following figure. The boundary conditions are shown in the figure. The problem is solved in the case of a1 =a2=1 . Is there a way to simplify this problem to a simplified version such that can be solved analytically? (i.e. a rectangle with one constant $a$ using conformal mapping or any other method?). $$\nabla.a\nabla V = 0$$,,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'boundary-value-problem', 'quasiconformal-maps']"
97,$y'' - y' = e^x$ (Variation of Parameters),(Variation of Parameters),y'' - y' = e^x,"I've solved multiple differential equations in this practice set, and even a few with variation of parameters, but no matter how many times I restart this problem I can't get it. I must be doing something wrong in my approach: $$y''-y'=e^x.$$  1) First, I use the homogeneous differential $y'' - y'= 0$, which gives me the complementary solutions: $$y_c=c_1 +c_2e^x.$$ 2) Next, I need to determine the solution using variation of parameters of form: $$y_p = u_1 + u_2 e^x$$ $$y'_p = u'_1 + u'_2e^x + u_2e^x$$ I set $u'_1 + u'_2e^x = 0$; thus, $$ y''_p = u'_2e^x+u_2e^x,$$ and substitute in to the original equation, $y'' - y' = e^x$: $$ u'_2e^x+u_2e^x - u_2e^x = e^x$$ $$u'_2e^x = e^x$$ $$u'_2 = 1$$ And I can solve for the other expressions using substitution ($u'_2e^x = -u'_1$) and integration: $$ u_2 = x, u'_1 = -e^x, u_1 = -e^x$$ And using $y_p = u_1 + u_2e^x = -e^x +xe^x$ and my value for $y_c$: $$y = y_c + y_p = c_1 +c_2e^x - e^x + xe^x$$ This is definitely not the correct solution ($y = c_1 + c_2e^x + xe^x$). What did I do wrong?","I've solved multiple differential equations in this practice set, and even a few with variation of parameters, but no matter how many times I restart this problem I can't get it. I must be doing something wrong in my approach: $$y''-y'=e^x.$$  1) First, I use the homogeneous differential $y'' - y'= 0$, which gives me the complementary solutions: $$y_c=c_1 +c_2e^x.$$ 2) Next, I need to determine the solution using variation of parameters of form: $$y_p = u_1 + u_2 e^x$$ $$y'_p = u'_1 + u'_2e^x + u_2e^x$$ I set $u'_1 + u'_2e^x = 0$; thus, $$ y''_p = u'_2e^x+u_2e^x,$$ and substitute in to the original equation, $y'' - y' = e^x$: $$ u'_2e^x+u_2e^x - u_2e^x = e^x$$ $$u'_2e^x = e^x$$ $$u'_2 = 1$$ And I can solve for the other expressions using substitution ($u'_2e^x = -u'_1$) and integration: $$ u_2 = x, u'_1 = -e^x, u_1 = -e^x$$ And using $y_p = u_1 + u_2e^x = -e^x +xe^x$ and my value for $y_c$: $$y = y_c + y_p = c_1 +c_2e^x - e^x + xe^x$$ This is definitely not the correct solution ($y = c_1 + c_2e^x + xe^x$). What did I do wrong?",,"['ordinary-differential-equations', 'multivariable-calculus']"
98,Population differential equation,Population differential equation,,"So my friend and I got this question for our differential equations class and we cannot figure it out. Consider a population N (t) that is changing according to the following rules: the per capita birth rate is a constant, 2 the per capita death rate is an increasing function of the population, 0.25N the population is harvested at a constant rate, H (a) Using these rules, write the ODE that describes the rate of change of the population. (b) For what value of N is the rate of change of the population equal to zero? Your answer will be a function of H. (c) Sketch the direction field for the ODE when 0 < H < 4. what we have so far is  we got $dN/dt = 2N - 0.25N^2$ One thing is we do not know when the H gets added in or how to solve the above to integrate. Any help would be appreciated.","So my friend and I got this question for our differential equations class and we cannot figure it out. Consider a population N (t) that is changing according to the following rules: the per capita birth rate is a constant, 2 the per capita death rate is an increasing function of the population, 0.25N the population is harvested at a constant rate, H (a) Using these rules, write the ODE that describes the rate of change of the population. (b) For what value of N is the rate of change of the population equal to zero? Your answer will be a function of H. (c) Sketch the direction field for the ODE when 0 < H < 4. what we have so far is  we got $dN/dt = 2N - 0.25N^2$ One thing is we do not know when the H gets added in or how to solve the above to integrate. Any help would be appreciated.",,['ordinary-differential-equations']
99,Autonomous equation having $\frac{t^2}{1+t}$ as a solution,Autonomous equation having  as a solution,\frac{t^2}{1+t},"Find an autonomous equation having $\displaystyle\frac{t^2}{1+t}$ as a solution. So the desired function $f$ should depend only on $x$, if I'm not wrong in the form $x'=f(x)$, that means the goal is to write $t$ as a function of $x$, but this seems almost impossible, however I compute the derivative; $$\bigg(\frac{t^2}{1+t}\bigg)'=\frac{2t}{(1+t)}-\frac{t^2}{(1+t)^2}$$ and $$x=\frac{t^2}{1+t}=t-\frac{t}{(1+t)}$$ for example, if I try $\displaystyle -x^2=-\frac{t^2}{(1+t)^2}-t^2+\frac{2t}{(1+t)}$ the middle part is redundant, how can I get rid of it ?","Find an autonomous equation having $\displaystyle\frac{t^2}{1+t}$ as a solution. So the desired function $f$ should depend only on $x$, if I'm not wrong in the form $x'=f(x)$, that means the goal is to write $t$ as a function of $x$, but this seems almost impossible, however I compute the derivative; $$\bigg(\frac{t^2}{1+t}\bigg)'=\frac{2t}{(1+t)}-\frac{t^2}{(1+t)^2}$$ and $$x=\frac{t^2}{1+t}=t-\frac{t}{(1+t)}$$ for example, if I try $\displaystyle -x^2=-\frac{t^2}{(1+t)^2}-t^2+\frac{2t}{(1+t)}$ the middle part is redundant, how can I get rid of it ?",,['ordinary-differential-equations']
