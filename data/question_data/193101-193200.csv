,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,can't understand derviatives from searching all over the internet,can't understand derviatives from searching all over the internet,,I have tried to follow a lot of tutorials out there for explaining derviatives and show (understandable)examples but i couldn't understand any. Can anyone link me a useful and easy to follow tutorial or explain derviatives  for me?,I have tried to follow a lot of tutorials out there for explaining derviatives and show (understandable)examples but i couldn't understand any. Can anyone link me a useful and easy to follow tutorial or explain derviatives  for me?,,"['calculus', 'derivatives', 'reference-request']"
1,Doing the integral $\int_0^\infty \frac{x^{s-1}}{1+x} dx =\frac{\pi}{\sin \pi s }$,Doing the integral,\int_0^\infty \frac{x^{s-1}}{1+x} dx =\frac{\pi}{\sin \pi s },"Here $0<s < 1$ . It is a standard example in complex analysis. But I am curious whether it can be done without using complex analysis. At least the $s=1/2$ case is easy. With the change of variable $x = y^2$ , $$ \int_0^\infty \frac{x^{-1/2}}{1+x}dx = \int_0^\infty \frac{2 dy }{1+y^2} = \pi .  $$","Here . It is a standard example in complex analysis. But I am curious whether it can be done without using complex analysis. At least the case is easy. With the change of variable ,",0<s < 1 s=1/2 x = y^2  \int_0^\infty \frac{x^{-1/2}}{1+x}dx = \int_0^\infty \frac{2 dy }{1+y^2} = \pi .  ,"['integration', 'derivatives', 'trigonometry', 'improper-integrals', 'contour-integration']"
2,Find derivative of $\sqrt[3]{\frac{x^2}{x+1}}$,Find derivative of,\sqrt[3]{\frac{x^2}{x+1}},I should find the derivative of $$ \sqrt[3]{\frac{x^2}{x+1}} $$ I know how to deal with general derivatives (patterns for fractions or composition) but I don't know (and it wasn't on my exercises) how to deal with derivative of for example $$ (x+1)^{1/3} $$ I saw on that forum that in some cases people use inequalities but I think that there is a more general approach...,I should find the derivative of I know how to deal with general derivatives (patterns for fractions or composition) but I don't know (and it wasn't on my exercises) how to deal with derivative of for example I saw on that forum that in some cases people use inequalities but I think that there is a more general approach..., \sqrt[3]{\frac{x^2}{x+1}}   (x+1)^{1/3} ,['calculus']
3,Why in physics the elementary work is written as $\delta W$ instead of $dW$?,Why in physics the elementary work is written as  instead of ?,\delta W dW,"Why an elementary work is written $\delta W$ instead of $dW$ ? For example, it's often written $$\delta W=F\cdot dr$$ if $dr$ is the elementary displacement. Why don't we write as usual $dW=F\cdot dr$ ? I saw an answer here but it doesn't really answer to the question (at my opinion). By the way, since at the end $W_{AB}=\int_A^B \delta W$ , I really don't understand this $\delta W$ . Is there mathematically a sense ?","Why an elementary work is written instead of ? For example, it's often written if is the elementary displacement. Why don't we write as usual ? I saw an answer here but it doesn't really answer to the question (at my opinion). By the way, since at the end , I really don't understand this . Is there mathematically a sense ?",\delta W dW \delta W=F\cdot dr dr dW=F\cdot dr W_{AB}=\int_A^B \delta W \delta W,"['derivatives', 'differential-forms']"
4,High Order Derivative,High Order Derivative,,"Find the ninth derivative of the following function at $x=0$ : $$f(x) =\frac{\cos\left(4x^4\right)-1}{x^7}$$ So I did all the manipulations and I got the following Maclaurin Series: $$\sum _{n=0}^{\infty }\,(-1)^n\frac{16x^{8n-7}}{(2n)!}-1$$ So to have $8n-7=9$ , I got that $n=2$ . Therefore, I thought that: $$(-1)^2\frac{16x^9}{24}-1=\frac{16x^9}{24}-1$$ And, therefore, the ninth derivative would be: $$\frac{16\cdot 9!}{24}-1=241919$$ But this was incorrect. Any help?","Find the ninth derivative of the following function at : So I did all the manipulations and I got the following Maclaurin Series: So to have , I got that . Therefore, I thought that: And, therefore, the ninth derivative would be: But this was incorrect. Any help?","x=0 f(x) =\frac{\cos\left(4x^4\right)-1}{x^7} \sum _{n=0}^{\infty }\,(-1)^n\frac{16x^{8n-7}}{(2n)!}-1 8n-7=9 n=2 (-1)^2\frac{16x^9}{24}-1=\frac{16x^9}{24}-1 \frac{16\cdot 9!}{24}-1=241919","['calculus', 'derivatives', 'taylor-expansion']"
5,Antiderivative of $x\sqrt{1+x^2}$,Antiderivative of,x\sqrt{1+x^2},"I am attempting this problem given to me, but the answer key does not explain the answer.  The question asks me to find the antiderivative of $x\sqrt{1+x^2}$ . My attempt: $\frac{d}{dx}f(g(x)) = g'(x)f'(g(x))$ $g'(x)$ must be $x$ , therefore $g(x)=\frac{1}{2}x^2$ $f'(x)$ must equal $\sqrt{1+x^2}$ , therefore $f(x)=\frac{2}{3}(1+x^2)^{\frac{3}{2}}$ But then I saw that if $g(x)=\frac{1}{3}x^2$ , then $g'(x)=\frac{2}{3}x$ ,  and $f(x)=(1+x^2)^{\frac{3}{2}}$ , then $\frac{d}{dx}f(g(x))'=g'(x)f'(g(x))=(2/3)x(3/2)\sqrt{1+x^2}=x\sqrt{1+x^2}$ . But I saw this by chance; what would be a more general and effective way to do this?  Should I check $this$ , and then write $that$ , and put together $those$ , etc.? Or is the way I just 'saw' the way the numbers should fall together how one would normally approach this?","I am attempting this problem given to me, but the answer key does not explain the answer.  The question asks me to find the antiderivative of . My attempt: must be , therefore must equal , therefore But then I saw that if , then ,  and , then . But I saw this by chance; what would be a more general and effective way to do this?  Should I check , and then write , and put together , etc.? Or is the way I just 'saw' the way the numbers should fall together how one would normally approach this?",x\sqrt{1+x^2} \frac{d}{dx}f(g(x)) = g'(x)f'(g(x)) g'(x) x g(x)=\frac{1}{2}x^2 f'(x) \sqrt{1+x^2} f(x)=\frac{2}{3}(1+x^2)^{\frac{3}{2}} g(x)=\frac{1}{3}x^2 g'(x)=\frac{2}{3}x f(x)=(1+x^2)^{\frac{3}{2}} \frac{d}{dx}f(g(x))'=g'(x)f'(g(x))=(2/3)x(3/2)\sqrt{1+x^2}=x\sqrt{1+x^2} this that those,"['integration', 'derivatives', 'chain-rule']"
6,the relation $f^2+g^2=1$ is followed only by trigonometric functions for differentiable functions,the relation  is followed only by trigonometric functions for differentiable functions,f^2+g^2=1,"Let $U\subseteq \mathbb{C}$ be an open, convex and connected subset and let $f,g:U\to \mathbb{C}$ be two differentiable functions so that $f',g'$ are continuous functions ($f,g \in C^1$). I want to show that if $f^2+g^2=1$ over $U$ then there is a function $z:U\to \mathbb{C}$ ($z\in C^1$) such that $f(t)=\sin(z(t))$ and $g(t) = \cos(z(t))$. My try (the unimportant part): I took this question from a book, in which it's said to use the following statement (which I've already proved): if $f:U\to \mathbb{C}$ ($f\in C^1$) with $f(U)\neq \mathbb{C}$ then there exists a function $g:U\to \mathbb{C}$ ($g\in C^1$) and $C_0\in \mathbb{C}$ so that $f(t)=\exp(g(t))+C_0$. For my question it was advised that I should use this statement for the functions $f+ig , f-ig$ I'm not sure how to continue, please help me.","Let $U\subseteq \mathbb{C}$ be an open, convex and connected subset and let $f,g:U\to \mathbb{C}$ be two differentiable functions so that $f',g'$ are continuous functions ($f,g \in C^1$). I want to show that if $f^2+g^2=1$ over $U$ then there is a function $z:U\to \mathbb{C}$ ($z\in C^1$) such that $f(t)=\sin(z(t))$ and $g(t) = \cos(z(t))$. My try (the unimportant part): I took this question from a book, in which it's said to use the following statement (which I've already proved): if $f:U\to \mathbb{C}$ ($f\in C^1$) with $f(U)\neq \mathbb{C}$ then there exists a function $g:U\to \mathbb{C}$ ($g\in C^1$) and $C_0\in \mathbb{C}$ so that $f(t)=\exp(g(t))+C_0$. For my question it was advised that I should use this statement for the functions $f+ig , f-ig$ I'm not sure how to continue, please help me.",,"['complex-analysis', 'derivatives', 'trigonometry', 'complex-numbers']"
7,"Question about a step in the proof of: If $f:\mathbb R \to \mathbb R$ is differentiable, then it is continuous.","Question about a step in the proof of: If  is differentiable, then it is continuous.",f:\mathbb R \to \mathbb R,"If $f$ is differentiable at $x_0$, $$\lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}=f'(x_0).$$ So, $\lim_{x\to x_0} f(x)-f(x_0)=\lim_{x\to x_0} f'(x_0)(x-x_0)=0$. Thus, $\lim_{x\to x_0} f(x)=f(x_0)$. Thus, $f$ is continuous at $x_0$. Since $x_0$ was arbitrary, $f$ is continuous on $\mathbb R$. My question is: about the step going from $\lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}=f'(x_0)$ to $\lim_{x\to x_0} f(x)-f(x_0)=\lim_{x\to x_0} f'(x_0)(x-x_0)$. How are we allowed to do this? Are we multiplying through by $x-x_0$? Is this allowed? Is it creating a new limit on the right side?","If $f$ is differentiable at $x_0$, $$\lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}=f'(x_0).$$ So, $\lim_{x\to x_0} f(x)-f(x_0)=\lim_{x\to x_0} f'(x_0)(x-x_0)=0$. Thus, $\lim_{x\to x_0} f(x)=f(x_0)$. Thus, $f$ is continuous at $x_0$. Since $x_0$ was arbitrary, $f$ is continuous on $\mathbb R$. My question is: about the step going from $\lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}=f'(x_0)$ to $\lim_{x\to x_0} f(x)-f(x_0)=\lim_{x\to x_0} f'(x_0)(x-x_0)$. How are we allowed to do this? Are we multiplying through by $x-x_0$? Is this allowed? Is it creating a new limit on the right side?",,"['calculus', 'derivatives', 'proof-explanation']"
8,"Log function derivative, in terms of the function's output?","Log function derivative, in terms of the function's output?",,"given a function $$f(x) = \frac{1}{1+e^{-x}}$$ we can express its derivative in terms of the function's output: $$\frac{df}{dx} = f(x) - f(x)\cdot f(x)$$ But is it possible to express the derivative of the following function in terms of its output? $$f(x) = \ln(x+1)$$ the derivative is: $$\frac{df}{dx} = \frac{1}{x+1}$$ But it's in terms of the argument of the function, not its output value :( This would mean I need to store input argument in my code, and unfavorable, as it will reduce performance.","given a function we can express its derivative in terms of the function's output: But is it possible to express the derivative of the following function in terms of its output? the derivative is: But it's in terms of the argument of the function, not its output value :( This would mean I need to store input argument in my code, and unfavorable, as it will reduce performance.",f(x) = \frac{1}{1+e^{-x}} \frac{df}{dx} = f(x) - f(x)\cdot f(x) f(x) = \ln(x+1) \frac{df}{dx} = \frac{1}{x+1},['derivatives']
9,Proving the limit definition of derivative of $e^x$ using the squeeze theorem?,Proving the limit definition of derivative of  using the squeeze theorem?,e^x,"My friend and I were working to see if we could use the squeeze theorem to prove that the derivative of $e^x$ is $e^x$. We said that by definition, the derivative is $$\frac{d}{dx} e^x =\lim \limits_{h\to0} \frac{e^{x+h}-e^x}{h} = \lim \limits_{h\to0} \frac{e^{x}(e^h-1)}{h} =  \lim \limits_{h\to0} e^x *\lim \limits_{h\to0} \frac{(e^h-1)}{h}$$ $$\frac{1}{2}h+1 \leq \frac{(e^h-1)}{h} \leq |h| + e^h$$ $$\lim \limits_{h\to0} \frac{1}{2}h+1 = \lim \limits_{h\to0} |h| + e^h = 1$$ $$\lim \limits_{h\to0} e^x *\lim \limits_{h\to0} \frac{(e^h-1)}{h} = \lim \limits_{h\to0} e^x = e^x$$ However, this seems more simplified than any other proof we've seen, so we're wondering if there's a fault here. Neither of us have taken analysis yet, so we might be assuming something incorrect.","My friend and I were working to see if we could use the squeeze theorem to prove that the derivative of $e^x$ is $e^x$. We said that by definition, the derivative is $$\frac{d}{dx} e^x =\lim \limits_{h\to0} \frac{e^{x+h}-e^x}{h} = \lim \limits_{h\to0} \frac{e^{x}(e^h-1)}{h} =  \lim \limits_{h\to0} e^x *\lim \limits_{h\to0} \frac{(e^h-1)}{h}$$ $$\frac{1}{2}h+1 \leq \frac{(e^h-1)}{h} \leq |h| + e^h$$ $$\lim \limits_{h\to0} \frac{1}{2}h+1 = \lim \limits_{h\to0} |h| + e^h = 1$$ $$\lim \limits_{h\to0} e^x *\lim \limits_{h\to0} \frac{(e^h-1)}{h} = \lim \limits_{h\to0} e^x = e^x$$ However, this seems more simplified than any other proof we've seen, so we're wondering if there's a fault here. Neither of us have taken analysis yet, so we might be assuming something incorrect.",,"['calculus', 'limits', 'derivatives']"
10,"Is the zero function a differentiable, continuous and polynomial function?","Is the zero function a differentiable, continuous and polynomial function?",,"For $f(x): \mathbb{R}\rightarrow\mathbb{R}$ Differentiable, as the derivative will always be 0 Continuous, as it is just a horizontal line with no breaks Polynomial, as it can be written as $f(x) = 0 = (1x^n - 1x^n)$ Would any of my reasoning be wrong?","For $f(x): \mathbb{R}\rightarrow\mathbb{R}$ Differentiable, as the derivative will always be 0 Continuous, as it is just a horizontal line with no breaks Polynomial, as it can be written as $f(x) = 0 = (1x^n - 1x^n)$ Would any of my reasoning be wrong?",,"['calculus', 'derivatives', 'polynomials', 'continuity']"
11,"Show that the rectangle of largest possible area, for a given perimeter, is a square.","Show that the rectangle of largest possible area, for a given perimeter, is a square.",,"Show that the rectangle of largest possible area, for a given perimeter, is a square. My Attempt: Let $\textrm {length}=x$ and $\textrm {breadth}=y$. Then, Perimeter of rectangle $=2(x+y)$ Also, Area of rectangle $=x.y$ $$A=x.y$$ Now, $\dfrac {dA}{dx}=x.\dfrac {dy}{dx}+y.\dfrac {dx}{dx}$ $$=x.\dfrac {dy}{dx}+y$$","Show that the rectangle of largest possible area, for a given perimeter, is a square. My Attempt: Let $\textrm {length}=x$ and $\textrm {breadth}=y$. Then, Perimeter of rectangle $=2(x+y)$ Also, Area of rectangle $=x.y$ $$A=x.y$$ Now, $\dfrac {dA}{dx}=x.\dfrac {dy}{dx}+y.\dfrac {dx}{dx}$ $$=x.\dfrac {dy}{dx}+y$$",,"['calculus', 'derivatives', 'area', 'maxima-minima']"
12,A very basic question about the derivative of a quotient,A very basic question about the derivative of a quotient,,I have a quotient function of following type $$h(x)=\frac{f(x)}{g(x)}$$ I know that $f(x)$ increases with $x$ and $g(x)$ decreases with $x$ so can we conclude that the derivative of $h(x)$ is positive? Thanks in advance,I have a quotient function of following type $$h(x)=\frac{f(x)}{g(x)}$$ I know that $f(x)$ increases with $x$ and $g(x)$ decreases with $x$ so can we conclude that the derivative of $h(x)$ is positive? Thanks in advance,,"['calculus', 'derivatives']"
13,Polynomial such that $f''(x) \rightarrow2$ as $x\rightarrow\infty$ given some values what is $f(1)$?,Polynomial such that  as  given some values what is ?,f''(x) \rightarrow2 x\rightarrow\infty f(1),"Let $f$ be a polynomial such that $f''(x) \rightarrow2$ as $x\rightarrow\infty$, the minimum of f is attained at $3$, and $f(0)=3$, Then $f(1)$ equals. $(A) \ 1$ $(B) \ 2$ $(C) -1$ $(D) -2$ I am not sure how to deal with $f''(x) \rightarrow2$ as $x\rightarrow\infty$ . Give me a hint to try. EDIT : Work after hint Let $f(x)=ax^2+bx +c  $ $f''(x)=2\implies 2=2a \implies a=1 $ $f(0)=3=c$ $f(x)=x^2+bx +3  $ Using the fact that minima attained at 3 we have  $f(3)=12+3b=3 \implies b=-3$ $f(x) = x^2-3x+3$ $f(1) = 1-3+3 = 1$","Let $f$ be a polynomial such that $f''(x) \rightarrow2$ as $x\rightarrow\infty$, the minimum of f is attained at $3$, and $f(0)=3$, Then $f(1)$ equals. $(A) \ 1$ $(B) \ 2$ $(C) -1$ $(D) -2$ I am not sure how to deal with $f''(x) \rightarrow2$ as $x\rightarrow\infty$ . Give me a hint to try. EDIT : Work after hint Let $f(x)=ax^2+bx +c  $ $f''(x)=2\implies 2=2a \implies a=1 $ $f(0)=3=c$ $f(x)=x^2+bx +3  $ Using the fact that minima attained at 3 we have  $f(3)=12+3b=3 \implies b=-3$ $f(x) = x^2-3x+3$ $f(1) = 1-3+3 = 1$",,"['derivatives', 'maxima-minima']"
14,Derivative of $n!$ (factorial)? [duplicate],Derivative of  (factorial)? [duplicate],n!,This question already has answers here : Derivative of a factorial (5 answers) Closed 6 years ago . I have a theory that uses the gamma function: $$\Gamma(n)=\int_0^\infty x^{n-1}e^{-x} \space dx$$ Then I was inclined to think that perhaps the derivative is: $$x^{n-1}e^{-x}$$ But I'm not sure we can just drop the integral along with the bounds to get the derivative. Then I thought about taking the limit: $$\lim_{x\to\infty}x^{n-1}e^{-x}$$ But now we can't specify at what $x$ value we want to get the rate of change of. At this point I feel like I can't get any further on my own and would appreciate some insight. EDIT: Looking for derivative in terms of $n$ actually.,This question already has answers here : Derivative of a factorial (5 answers) Closed 6 years ago . I have a theory that uses the gamma function: $$\Gamma(n)=\int_0^\infty x^{n-1}e^{-x} \space dx$$ Then I was inclined to think that perhaps the derivative is: $$x^{n-1}e^{-x}$$ But I'm not sure we can just drop the integral along with the bounds to get the derivative. Then I thought about taking the limit: $$\lim_{x\to\infty}x^{n-1}e^{-x}$$ But now we can't specify at what $x$ value we want to get the rate of change of. At this point I feel like I can't get any further on my own and would appreciate some insight. EDIT: Looking for derivative in terms of $n$ actually.,,"['calculus', 'derivatives', 'definite-integrals']"
15,Relationship between $f(x)$ and $xf'(x)$,Relationship between  and,f(x) xf'(x),"Is the following statement true? Suppose we are given $f(x)$ defined and differentiable $\forall x \in \mathbb{R}$, $x\neq 0$. If $xf'(x)>0$, then $f$ has no minimum value. The statement seems reasonable, because if I choose $f(x) = -1/x^2$, then its derivative is $1/x^3$ so $xf'(x) = 1/x^2$ that is $>0$, for all $x\ne0$. And $-1/x^2$  clearly has no minimum value (its lower bound is $-\infty$). My problem is that I have no idea how I should continue going from this example to a general proof. Taken from a question of a calculus 1 exemption of 2011.","Is the following statement true? Suppose we are given $f(x)$ defined and differentiable $\forall x \in \mathbb{R}$, $x\neq 0$. If $xf'(x)>0$, then $f$ has no minimum value. The statement seems reasonable, because if I choose $f(x) = -1/x^2$, then its derivative is $1/x^3$ so $xf'(x) = 1/x^2$ that is $>0$, for all $x\ne0$. And $-1/x^2$  clearly has no minimum value (its lower bound is $-\infty$). My problem is that I have no idea how I should continue going from this example to a general proof. Taken from a question of a calculus 1 exemption of 2011.",,"['calculus', 'derivatives']"
16,"Finding the derivative of $f(x)=\int_1^{\infty}\frac{e^{-xy}}{y^2}dy,\:\:\:x\in(0,\infty)$",Finding the derivative of,"f(x)=\int_1^{\infty}\frac{e^{-xy}}{y^2}dy,\:\:\:x\in(0,\infty)","Let    $$f(x)=\int_1^{\infty}\frac{e^{-xy}}{y^2}dy,\:\:\:x\in(0,\infty)$$   Show that $f(x)$ is differentiable on $(0,\infty)$, find the formula for $f'(x)$? Is $f(x)$ twice differentiable? I'm thinking to define a sequence as follow $$f_n(x)=\int_1^n\frac{e^{-xy}}{y^2}dy.$$ To show $f_n(x)$ is differentiable, I'm thinking to show the following limit exists, $$\lim_{h\to 0}\int_1^n\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}.$$ To be able to pass the limit inside the integral, we can apply the Lebesgue dominated convergence theorem. So I want to see if  I can apply it. Since $\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}$ is bounded by $\frac{e^{-uy}}{y}$, (where $x\leq u\leq x+h$) which is integrable on $[1,\infty).$ Hence $f'_n(x)=\int_1^n\frac{d}{dx}(\frac{e^{-xy}}{y^2})dy=\int_1^n-\frac{e^{-xy}}{y}dy. $ Now, $\lim_{n\to\infty}\int_1^n\frac{-e^{-xy}}{y}dy=\int_{1}^{\infty}\frac{-e^{-xy}}{y}dy.$ However I see a problem here, since in fact, we have  $$f'(x)=\lim_{h\to 0}\lim_{n\to{\infty}}\int_1^n\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}.$$ But I'm not sure, if I'm allowed to interchange these two limits. I appreciate any hint or alternative proof.","Let    $$f(x)=\int_1^{\infty}\frac{e^{-xy}}{y^2}dy,\:\:\:x\in(0,\infty)$$   Show that $f(x)$ is differentiable on $(0,\infty)$, find the formula for $f'(x)$? Is $f(x)$ twice differentiable? I'm thinking to define a sequence as follow $$f_n(x)=\int_1^n\frac{e^{-xy}}{y^2}dy.$$ To show $f_n(x)$ is differentiable, I'm thinking to show the following limit exists, $$\lim_{h\to 0}\int_1^n\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}.$$ To be able to pass the limit inside the integral, we can apply the Lebesgue dominated convergence theorem. So I want to see if  I can apply it. Since $\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}$ is bounded by $\frac{e^{-uy}}{y}$, (where $x\leq u\leq x+h$) which is integrable on $[1,\infty).$ Hence $f'_n(x)=\int_1^n\frac{d}{dx}(\frac{e^{-xy}}{y^2})dy=\int_1^n-\frac{e^{-xy}}{y}dy. $ Now, $\lim_{n\to\infty}\int_1^n\frac{-e^{-xy}}{y}dy=\int_{1}^{\infty}\frac{-e^{-xy}}{y}dy.$ However I see a problem here, since in fact, we have  $$f'(x)=\lim_{h\to 0}\lim_{n\to{\infty}}\int_1^n\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}.$$ But I'm not sure, if I'm allowed to interchange these two limits. I appreciate any hint or alternative proof.",,"['real-analysis', 'integration', 'analysis', 'derivatives', 'lebesgue-integral']"
17,Does chain rule enable you yo calculat derivative of $|x^2|$ at x = 0,Does chain rule enable you yo calculat derivative of  at x = 0,|x^2|,"The question states as following:  ""Does the Chain Rules anble you to calculate the derivates of |x^2| and |x|^2 at x=0? Do these functoins have derivatives at x = 0? why?"" So using the chain rule for both of these I got two different answer. For $|x|^2$ I got $f'(x)= 2|x|*\frac {x}{|x|}$ i.e $2x$ For $|x^2|$ using the chain rule I got $f'(x)=\frac {2x}{|x|}$ Two very different answers. And and only one of these derivatives is differentiable at x = 0. Which I know is strange since I know for $y = x^2$ => $y' = 2x$ $y'$ is differentiable at x = 0. So how am I not able to reach the same conclusion using the chainrule? Thank you in advance!","The question states as following:  ""Does the Chain Rules anble you to calculate the derivates of |x^2| and |x|^2 at x=0? Do these functoins have derivatives at x = 0? why?"" So using the chain rule for both of these I got two different answer. For $|x|^2$ I got $f'(x)= 2|x|*\frac {x}{|x|}$ i.e $2x$ For $|x^2|$ using the chain rule I got $f'(x)=\frac {2x}{|x|}$ Two very different answers. And and only one of these derivatives is differentiable at x = 0. Which I know is strange since I know for $y = x^2$ => $y' = 2x$ $y'$ is differentiable at x = 0. So how am I not able to reach the same conclusion using the chainrule? Thank you in advance!",,"['derivatives', 'chain-rule']"
18,Is this complex analysis proof rigorous?,Is this complex analysis proof rigorous?,,"I was reading Complex Analysis by Serge Lang . The Theorem 1.1 of the third chapter states: Let $U$ be a connected open set, and let $f$ be a holomorphic function on $U$. If $f^{\prime}=0$ then $f$ is a constant. It was proven by considering a curve $\gamma$ and showing that $f(\alpha)=f(\beta)$, where $\alpha$ and $\beta$ are the points connected by $\gamma$ and the function $t\to f(\gamma (t))$ is differentiable. Next it shows that for all the points $\alpha, z_0, z_1, \ldots , z_n, \beta$, where $z_i$ is a end and a star point of two curves on a path, $f(z_i)=f(\alpha)$. In other words, if you take the derivative of a path, it's constant on every point joining two curves. Then it claims that this proves the theorem. Somehow this feels a bit odd since as I understood it, we would have to consider every possible path on the set $U$ to prove that $f$ is constant. Is this really rigorous? Thanks in advance.","I was reading Complex Analysis by Serge Lang . The Theorem 1.1 of the third chapter states: Let $U$ be a connected open set, and let $f$ be a holomorphic function on $U$. If $f^{\prime}=0$ then $f$ is a constant. It was proven by considering a curve $\gamma$ and showing that $f(\alpha)=f(\beta)$, where $\alpha$ and $\beta$ are the points connected by $\gamma$ and the function $t\to f(\gamma (t))$ is differentiable. Next it shows that for all the points $\alpha, z_0, z_1, \ldots , z_n, \beta$, where $z_i$ is a end and a star point of two curves on a path, $f(z_i)=f(\alpha)$. In other words, if you take the derivative of a path, it's constant on every point joining two curves. Then it claims that this proves the theorem. Somehow this feels a bit odd since as I understood it, we would have to consider every possible path on the set $U$ to prove that $f$ is constant. Is this really rigorous? Thanks in advance.",,"['complex-analysis', 'derivatives', 'connectedness']"
19,Theorem 7.18 in PMA Rudin: Existence of an everywhere continuous but nowhere differentiable real function on the real line,Theorem 7.18 in PMA Rudin: Existence of an everywhere continuous but nowhere differentiable real function on the real line,,"Here is Theorem 7.18 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: There exists a real continuous function on the real line which is nowhere differentiable. And, here is Rudin's proof ( steps wherein I've been unable to figure out on my own and hence would appreciate the help of the Math SE community): Define    $$\tag{34} \varphi(x) = \lvert x \rvert \qquad \qquad (-1 \leq x \leq 1) $$   and extend the definition of $\varphi(x)$ to all real $x$ by requiring that    $$ \tag{35} \varphi(x+2) = \varphi(x). $$   Then, for all $s$ and $t$,    $$\tag{36}  \lvert \varphi(s) - \varphi(t) \rvert \leq \lvert s-t \rvert. $$   [ How to obtain the inequality in (36)? ]    In particular, $\varphi$ is continuous on $\mathbb{R}^1$. Define    $$ \tag{37} f(x) = \sum_{n=0}^\infty \left( \frac{3}{4} \right)^n \varphi \left( 4^n x \right). $$   Since $0 \leq \varphi \leq 1$, Theorem 7.10 shows that the series (37) converges uniformly on $\mathbb{R}^1$. By Theorem 7.12, $f$ is continuous on $\mathbb{R}^1$. Now fix a real number $x$ and a positive integer $m$. Put    $$ \tag{38} \delta_m = \pm \frac{1}{2} \cdot 4^{-m} $$   where the sign is so chosen that no integer lies between $4^m x$ and $4^m \left( x + \delta_m \right)$. This can be done, since $4^m \left\lvert \delta_m \right\rvert = \frac{1}{2}$. Define    $$ \tag{39} \gamma_n = \frac{ \varphi \left( 4^n \left( x + \delta_m \right)  \right) - \varphi \left( 4^n x \right)  }{ \delta_m }. $$   When $n > m$, then $4^n \delta_m$ is an even integer, so that $\gamma_n = 0$. When $0 \leq n \leq m$, (36) implies that $\left\lvert \gamma_n \right\rvert \leq 4^n$. Since $\left\lvert \gamma_m \right\rvert = 4^m$ [ How? ], we conclude that    $$ \begin{align} \left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\  &\geq 3^m - \sum_{n=0}^{m-1} 3^n \\ &= \frac{1}{2} \left( 3^m + 1 \right). \end{align} $$   As $m \to \infty$, $\gamma_m \to 0$. It follows that $f$ is not differentiable at $x$. Here is Theorem 7.10 in Baby Rudin, 3rd edition: Suppose $\left\{ f_n \right\}$ is a sequence of functions defined on $E$, and suppose $$ \left\lvert f_n (x) \right\rvert \leq M_n \qquad \qquad (x \in E, \ n = 1, 2, 3, \ldots \ ). $$   Then $ \sum f_n $ converges uniformly on $E$ if $ \sum M_n$ converges. Note that the converse is not asserted ( and is, in fact, not true). And, here is Theorem 7.12: If $\left\{ f_n \right\}$ is a sequence of continuous functions on $E$, and if $f_n \to f$ uniformly on $E$, then $f$ is continuous on $E$. The rest of the proof I understand, I think. However, I would appreciate if someone could give the crux of the procedure involved in the construction of this particular example and also give a blueprint for constructing this class of functions.","Here is Theorem 7.18 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: There exists a real continuous function on the real line which is nowhere differentiable. And, here is Rudin's proof ( steps wherein I've been unable to figure out on my own and hence would appreciate the help of the Math SE community): Define    $$\tag{34} \varphi(x) = \lvert x \rvert \qquad \qquad (-1 \leq x \leq 1) $$   and extend the definition of $\varphi(x)$ to all real $x$ by requiring that    $$ \tag{35} \varphi(x+2) = \varphi(x). $$   Then, for all $s$ and $t$,    $$\tag{36}  \lvert \varphi(s) - \varphi(t) \rvert \leq \lvert s-t \rvert. $$   [ How to obtain the inequality in (36)? ]    In particular, $\varphi$ is continuous on $\mathbb{R}^1$. Define    $$ \tag{37} f(x) = \sum_{n=0}^\infty \left( \frac{3}{4} \right)^n \varphi \left( 4^n x \right). $$   Since $0 \leq \varphi \leq 1$, Theorem 7.10 shows that the series (37) converges uniformly on $\mathbb{R}^1$. By Theorem 7.12, $f$ is continuous on $\mathbb{R}^1$. Now fix a real number $x$ and a positive integer $m$. Put    $$ \tag{38} \delta_m = \pm \frac{1}{2} \cdot 4^{-m} $$   where the sign is so chosen that no integer lies between $4^m x$ and $4^m \left( x + \delta_m \right)$. This can be done, since $4^m \left\lvert \delta_m \right\rvert = \frac{1}{2}$. Define    $$ \tag{39} \gamma_n = \frac{ \varphi \left( 4^n \left( x + \delta_m \right)  \right) - \varphi \left( 4^n x \right)  }{ \delta_m }. $$   When $n > m$, then $4^n \delta_m$ is an even integer, so that $\gamma_n = 0$. When $0 \leq n \leq m$, (36) implies that $\left\lvert \gamma_n \right\rvert \leq 4^n$. Since $\left\lvert \gamma_m \right\rvert = 4^m$ [ How? ], we conclude that    $$ \begin{align} \left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\  &\geq 3^m - \sum_{n=0}^{m-1} 3^n \\ &= \frac{1}{2} \left( 3^m + 1 \right). \end{align} $$   As $m \to \infty$, $\gamma_m \to 0$. It follows that $f$ is not differentiable at $x$. Here is Theorem 7.10 in Baby Rudin, 3rd edition: Suppose $\left\{ f_n \right\}$ is a sequence of functions defined on $E$, and suppose $$ \left\lvert f_n (x) \right\rvert \leq M_n \qquad \qquad (x \in E, \ n = 1, 2, 3, \ldots \ ). $$   Then $ \sum f_n $ converges uniformly on $E$ if $ \sum M_n$ converges. Note that the converse is not asserted ( and is, in fact, not true). And, here is Theorem 7.12: If $\left\{ f_n \right\}$ is a sequence of continuous functions on $E$, and if $f_n \to f$ uniformly on $E$, then $f$ is continuous on $E$. The rest of the proof I understand, I think. However, I would appreciate if someone could give the crux of the procedure involved in the construction of this particular example and also give a blueprint for constructing this class of functions.",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'continuity']"
20,How to find the derivative of $\frac{1}{x^3}$ by definition of derivative,How to find the derivative of  by definition of derivative,\frac{1}{x^3},"$$f(x) = \frac{1}{x^3}$$   Find the derivative of $f$ by using the derivative definition. I have tried several pages of working, but nothing seemed to work until I actually used the quotient rule (definitions of derivative are soo complicated lol).","$$f(x) = \frac{1}{x^3}$$   Find the derivative of $f$ by using the derivative definition. I have tried several pages of working, but nothing seemed to work until I actually used the quotient rule (definitions of derivative are soo complicated lol).",,"['calculus', 'limits', 'derivatives']"
21,"If $g'(x)$ is constant, what is the fifth derivative of $f(g(x))$?","If  is constant, what is the fifth derivative of ?",g'(x) f(g(x)),"I just need some clarification about how to solve and approach this problem, we did it in a Calc 1474 class but I didn't understand how to approach and solve it.","I just need some clarification about how to solve and approach this problem, we did it in a Calc 1474 class but I didn't understand how to approach and solve it.",,"['calculus', 'derivatives', 'contest-math']"
22,Differentiability of Sin inverse,Differentiability of Sin inverse,,My book says that trigonometric functions and their inverses are differentiable in their domain. I am looking for $y=\sin^{-1}(x)$ to verify the same. $\sin^{-1}(x)$ has its  domain in interval [-1.1]. But if I look at the derivative of $\sin^{−1}(x)$ as follows $$\frac{1}{\sqrt{(1-x^2)}}$$ If my $x$ is $-1$ or $+1$ then the expression will becomes infinity. So I can say that it is not differentiable at $+1$ and $-1$. But I am contradicting my book. Am I going wrong somewhere in understanding this?,My book says that trigonometric functions and their inverses are differentiable in their domain. I am looking for $y=\sin^{-1}(x)$ to verify the same. $\sin^{-1}(x)$ has its  domain in interval [-1.1]. But if I look at the derivative of $\sin^{−1}(x)$ as follows $$\frac{1}{\sqrt{(1-x^2)}}$$ If my $x$ is $-1$ or $+1$ then the expression will becomes infinity. So I can say that it is not differentiable at $+1$ and $-1$. But I am contradicting my book. Am I going wrong somewhere in understanding this?,,"['calculus', 'trigonometry', 'derivatives']"
23,What is the derivative of $\frac{1}{f(x)g(x)}$?,What is the derivative of ?,\frac{1}{f(x)g(x)},"I feel like this is a very basic question, yet I struggle with it immensely. I know that $(f(x)*g(x)) = f(x)'g(x)+f(x)g(x)'$, but how to use that in order to figure out $\frac{1}{f(x)g(x)}$ ?","I feel like this is a very basic question, yet I struggle with it immensely. I know that $(f(x)*g(x)) = f(x)'g(x)+f(x)g(x)'$, but how to use that in order to figure out $\frac{1}{f(x)g(x)}$ ?",,['derivatives']
24,"Let $f$ be a polynomial with at least $k$ different roots, prove that $f'$ hast at least $k-1$ different roots","Let  be a polynomial with at least  different roots, prove that  hast at least  different roots",f k f' k-1,"I don't know how to write the proof to this problem: Let $f$ be a polynomial with at least $k$ different roots, prove that $f'$ has at least $k-1$ different roots. It's featured among a few exercises made to apply Rolle's theorem, but this one is about a general result and asks for higher degree of formality I guess... I don't know how to write the proof although I have the following intuition: $f(x)=x^k + g(x)$ $f'(x)=kx^{k-1} + g'(x)$ Any suggestions?","I don't know how to write the proof to this problem: Let $f$ be a polynomial with at least $k$ different roots, prove that $f'$ has at least $k-1$ different roots. It's featured among a few exercises made to apply Rolle's theorem, but this one is about a general result and asks for higher degree of formality I guess... I don't know how to write the proof although I have the following intuition: $f(x)=x^k + g(x)$ $f'(x)=kx^{k-1} + g'(x)$ Any suggestions?",,"['derivatives', 'polynomials', 'proof-writing', 'roots', 'rolles-theorem']"
25,Find the derivative of $y^3-xy^2+\cos xy=2$,Find the derivative of,y^3-xy^2+\cos xy=2,Find the derivative of $y^3-xy^2+\cos xy=2$ My Attempt: $$y^3-xy^2+\cos xy=2$$ $$\dfrac {d}{dx} [y^3-xy^2+\cos xy]=\dfrac {d}{dx} [2]$$ $$3y^2.\dfrac {dy}{dx} -[1.y^2+2xy\dfrac {dy}{dx}] + (-\sin xy) \dfrac {dy}{dx}(xy)=0$$ $$3y^2 \dfrac {dy}{dx} - y^2 - 2xy\dfrac {dy}{dx} - \sin xy (y+\dfrac {dy}{dx} x)=0$$ How do I proceed further?,Find the derivative of $y^3-xy^2+\cos xy=2$ My Attempt: $$y^3-xy^2+\cos xy=2$$ $$\dfrac {d}{dx} [y^3-xy^2+\cos xy]=\dfrac {d}{dx} [2]$$ $$3y^2.\dfrac {dy}{dx} -[1.y^2+2xy\dfrac {dy}{dx}] + (-\sin xy) \dfrac {dy}{dx}(xy)=0$$ $$3y^2 \dfrac {dy}{dx} - y^2 - 2xy\dfrac {dy}{dx} - \sin xy (y+\dfrac {dy}{dx} x)=0$$ How do I proceed further?,,"['calculus', 'derivatives']"
26,How would you compute the limit $\lim_{x\to\infty} \left(\frac{1}{x^x+x}\right)^{1/x}$?,How would you compute the limit ?,\lim_{x\to\infty} \left(\frac{1}{x^x+x}\right)^{1/x},"I understand that you would use a natural log to break up because it is an indeterminate form, but that is what is seemingly giving me trouble.","I understand that you would use a natural log to break up because it is an indeterminate form, but that is what is seemingly giving me trouble.",,"['calculus', 'limits', 'derivatives', 'indeterminate-forms']"
27,Differentiating $y=x^6e^{-4x^3}$,Differentiating,y=x^6e^{-4x^3},"Can some one explain, how to solve this derivative. I'm total beginner. Would be preferable if some one explain step-by-step. $$y=x^6e^{-4x^3}$$","Can some one explain, how to solve this derivative. I'm total beginner. Would be preferable if some one explain step-by-step. $$y=x^6e^{-4x^3}$$",,['derivatives']
28,On the use of Fubini's theorem in the proof of Rademacher's theorem,On the use of Fubini's theorem in the proof of Rademacher's theorem,,"I am reading Lectures on Lipschitz Analysis by Heinonen, where the proof of Rademacher's theorem is presented in Chapter 3. Let $\, f:\mathbb{R}^n \to \mathbb{R}$ be Lipschitz. In the first step of the proof, it is shown that the directional derivative of $f$ in direction $v \in \mathbb{R}^n$ exists for almost all $x \in \mathbb{R}^n$ . For any fixed $x,v \in \mathbb{R}^n$ the author defines a real-valued function $$ f_{x,v}(t)=f(x+tv)$$ which is Lipschitz, and hence differentiable at almost every $t$ . Then, it is claimed that after fixing $v$ , we can conclude from Fubini's theorem that $$ \lim_{t \to 0} \frac{f(x+tv) - f(x)}{t}$$ exists for almost every $x \in \mathbb{R}^n$ . Question: Why this conclusion follows from Fubini?","I am reading Lectures on Lipschitz Analysis by Heinonen, where the proof of Rademacher's theorem is presented in Chapter 3. Let be Lipschitz. In the first step of the proof, it is shown that the directional derivative of in direction exists for almost all . For any fixed the author defines a real-valued function which is Lipschitz, and hence differentiable at almost every . Then, it is claimed that after fixing , we can conclude from Fubini's theorem that exists for almost every . Question: Why this conclusion follows from Fubini?","\, f:\mathbb{R}^n \to \mathbb{R} f v \in \mathbb{R}^n x \in \mathbb{R}^n x,v \in \mathbb{R}^n  f_{x,v}(t)=f(x+tv) t v  \lim_{t \to 0} \frac{f(x+tv) - f(x)}{t} x \in \mathbb{R}^n","['real-analysis', 'measure-theory', 'derivatives', 'lipschitz-functions']"
29,"Differentiablity of the function $\min\{|x-2|,|x|,|x+2|\}.$ [closed]",Differentiablity of the function  [closed],"\min\{|x-2|,|x|,|x+2|\}.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question How to check differentiablity of the function $\min\{|x-2|,|x|,|x+2|\}?$ I only know that inside functions are not differentiable at $2,0,-2$ resp. Please help. Thanks a lot.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question How to check differentiablity of the function $\min\{|x-2|,|x|,|x+2|\}?$ I only know that inside functions are not differentiable at $2,0,-2$ resp. Please help. Thanks a lot.",,"['real-analysis', 'derivatives', 'absolute-value']"
30,Solve: $\int\frac{1}{3x}dx$,Solve:,\int\frac{1}{3x}dx,Solve: $$\int\frac{1}{3x}dx$$ The answer in the back of the book is: $$\frac{1}{3}\log_e|x|+c$$ I get this result when I remove the constant $\frac{1}{3}$ from the integral at the start and substitute u for x at the beginning. Also by differentiating the result I can return to the initial integral. But if at the beginning I do not remove the constant $\frac{1}{3}$from the integral and substitute $u$ for $3x$ I get a different result. $$\frac{1}{3} \log_e |3x| +c$$ Differentiating this does not bring me back to the initial integral. What is wrong with substituting $u$ for $3x$ here?,Solve: $$\int\frac{1}{3x}dx$$ The answer in the back of the book is: $$\frac{1}{3}\log_e|x|+c$$ I get this result when I remove the constant $\frac{1}{3}$ from the integral at the start and substitute u for x at the beginning. Also by differentiating the result I can return to the initial integral. But if at the beginning I do not remove the constant $\frac{1}{3}$from the integral and substitute $u$ for $3x$ I get a different result. $$\frac{1}{3} \log_e |3x| +c$$ Differentiating this does not bring me back to the initial integral. What is wrong with substituting $u$ for $3x$ here?,,"['calculus', 'integration', 'derivatives']"
31,100th derivative of $\frac{1+x^2}{1+\tan^2(x)}$ at point 0,100th derivative of  at point 0,\frac{1+x^2}{1+\tan^2(x)},$$\frac{\mathrm d^{100}}{\mathrm dx^{100}}\frac{1+x^2}{1+\tan^2(x)}$$ Without Taylor Is there a way to solve this problem by using General Leibniz rule. I tried but numerator make problem.,$$\frac{\mathrm d^{100}}{\mathrm dx^{100}}\frac{1+x^2}{1+\tan^2(x)}$$ Without Taylor Is there a way to solve this problem by using General Leibniz rule. I tried but numerator make problem.,,"['calculus', 'real-analysis', 'derivatives']"
32,How can I Differentiate $ y = x^{2/3} $ using first principles,How can I Differentiate  using first principles, y = x^{2/3} ,"How can I Differentiate $ y = x^{2/3} $ using first principles Using the normal rule to find derivative, I got: $dy/dx = ⅔x^{-⅓}$ I don't understand the first principles method. Someone please help. Thanks","How can I Differentiate $ y = x^{2/3} $ using first principles Using the normal rule to find derivative, I got: $dy/dx = ⅔x^{-⅓}$ I don't understand the first principles method. Someone please help. Thanks",,"['calculus', 'derivatives']"
33,How to find derivative of $\sin\sqrt{x}$ using difference quotient?,How to find derivative of  using difference quotient?,\sin\sqrt{x},"The definition of derivative of a function $f(x)$ is $$\lim_{h\to0} \frac{f(x+h)-f(x)}{h}$$ Using this definition, the derivative of $\sin\sqrt{x}$ will be: $$\lim_{h\to0} \frac{\sin\sqrt{x+h}-\sin\sqrt{x}}{h}$$ $$\lim_{h\to 0} \frac{\cos\left(\frac{\sqrt{x+h}+\sqrt{x}}{2}\right)\sin\left(\frac{\sqrt{x+h}-\sqrt{x}}{2}\right)}{h}$$ Now i got stuck. How to find the limit or simplify this expression? I get intuition that we have to use $$\lim_{x\to0}\frac{\sin x}{x} = 1$$ but that too is leading no where. I am unable to remove h from denominator. NOTE I know the derivative of $\sin\sqrt{x}$ is $\frac{\cos\sqrt{x}}{2\sqrt{x}}$ using chain rule, but this exercise was given to us for practice using division quotient.","The definition of derivative of a function $f(x)$ is $$\lim_{h\to0} \frac{f(x+h)-f(x)}{h}$$ Using this definition, the derivative of $\sin\sqrt{x}$ will be: $$\lim_{h\to0} \frac{\sin\sqrt{x+h}-\sin\sqrt{x}}{h}$$ $$\lim_{h\to 0} \frac{\cos\left(\frac{\sqrt{x+h}+\sqrt{x}}{2}\right)\sin\left(\frac{\sqrt{x+h}-\sqrt{x}}{2}\right)}{h}$$ Now i got stuck. How to find the limit or simplify this expression? I get intuition that we have to use $$\lim_{x\to0}\frac{\sin x}{x} = 1$$ but that too is leading no where. I am unable to remove h from denominator. NOTE I know the derivative of $\sin\sqrt{x}$ is $\frac{\cos\sqrt{x}}{2\sqrt{x}}$ using chain rule, but this exercise was given to us for practice using division quotient.",,"['calculus', 'limits', 'derivatives', 'limits-without-lhopital']"
34,"$g'(x) = cg(x)$. Are there any other functions, aside from ${e^{cx}}$, that satisfy the condition?",". Are there any other functions, aside from , that satisfy the condition?",g'(x) = cg(x) {e^{cx}},"Assume that $g:\mathbb R \longrightarrow  \mathbb R$ and $g'(x) = cg(x)$, where $c\in\mathbb R$ and $\forall\ x\in\mathbb R$. Are there any other functions, aside from ${e^{cx}}$, that satisfy the condition? P.S If there are not, how can it be proven?","Assume that $g:\mathbb R \longrightarrow  \mathbb R$ and $g'(x) = cg(x)$, where $c\in\mathbb R$ and $\forall\ x\in\mathbb R$. Are there any other functions, aside from ${e^{cx}}$, that satisfy the condition? P.S If there are not, how can it be proven?",,"['calculus', 'integration', 'derivatives']"
35,"If one of the Dini derivatives is bounded, then f is Lipschitz","If one of the Dini derivatives is bounded, then f is Lipschitz",,"If one of its Dini derivatives (say $D^+$ ) is bounded show that a function $f$ satisfies a Lipschitz condition, Definition of the upper right Dini derivative: $$D^{+}f(x) =  \limsup_{h\to\ 0^+} \frac{f(x+h) - f(x)}{h}$$ This is a question appearing in Royden's Real Analysis textbook. (Edition 3)","If one of its Dini derivatives (say ) is bounded show that a function satisfies a Lipschitz condition, Definition of the upper right Dini derivative: This is a question appearing in Royden's Real Analysis textbook. (Edition 3)","D^+ f D^{+}f(x) =
 \limsup_{h\to\ 0^+} \frac{f(x+h) - f(x)}{h}","['real-analysis', 'measure-theory', 'derivatives']"
36,What is the derivative of $x^y$ with respect to $y$?,What is the derivative of  with respect to ?,x^y y,"I'm looking for the derivative of $x^y$ with respect to $y$. I have done it by taking log of both sides, how do I do it if I try to write $\log e^{x^y} = x^y$?","I'm looking for the derivative of $x^y$ with respect to $y$. I have done it by taking log of both sides, how do I do it if I try to write $\log e^{x^y} = x^y$?",,"['calculus', 'derivatives']"
37,Why does a piecewise constant function not have a primitive function?,Why does a piecewise constant function not have a primitive function?,,"Let $$f(x)=\begin{cases} -1, \quad & a \leq x \leq 0 \\ 1, \quad &  0 \leq x \leq b \end{cases}$$ It says in my book because of the Darboux theorem: If $f:[a,b] \to \mathbb R$ is differentiable on segment $[a,b]$, then $\forall a_1, b_1 \in [a,b]: a_1<b_1\ \ \ \forall t \in [f'(a_1),f'(b_1)] \ : \exists c \in [a_1,b_1]: f'(c)=t$ I don't directly see this...","Let $$f(x)=\begin{cases} -1, \quad & a \leq x \leq 0 \\ 1, \quad &  0 \leq x \leq b \end{cases}$$ It says in my book because of the Darboux theorem: If $f:[a,b] \to \mathbb R$ is differentiable on segment $[a,b]$, then $\forall a_1, b_1 \in [a,b]: a_1<b_1\ \ \ \forall t \in [f'(a_1),f'(b_1)] \ : \exists c \in [a_1,b_1]: f'(c)=t$ I don't directly see this...",,"['calculus', 'integration', 'derivatives', 'riemann-integration']"
38,Differentiate the Function: $y=x^x$,Differentiate the Function:,y=x^x,$y=x^x$ Use $\frac{d}{dx}(a^x)=a^x \ln a$ My answer is: $x^x \ln x$ The book has the answer as $x^x\ (1+ \ln\ x)$ Am I missing a step?,$y=x^x$ Use $\frac{d}{dx}(a^x)=a^x \ln a$ My answer is: $x^x \ln x$ The book has the answer as $x^x\ (1+ \ln\ x)$ Am I missing a step?,,"['calculus', 'derivatives']"
39,Critical numbers of the function: $x\sqrt{5-x}$,Critical numbers of the function:,x\sqrt{5-x},"Let f(x) = $$\displaystyle f(x) = x\sqrt{5-x} $$ On the interval: [-6,4] Critical numbers are the the values of x in the domain of f for which f'(x) = 0 or f'(x) is undefined. Derivative of the function: $$ \frac{1}{2} \cdot x (5-x)^{\frac{-1}{2}} \cdot -1$$ $$ \frac {\frac{-x}{2}}{\sqrt{5-x}}$$ $f'(x) = 0$, when $x = 0, $ and is undefined when x= 5 Plugging in the roots of the derivative function and the end points of the interval into the original function: \begin{align*} f(0) & = 0\\ f(5) & = 0\\ f(-6) & = -6\sqrt{11}\\ f(4) & = 4 \cdot 1 = 4 \end{align*} So why is the 4 not the absolute maximum value? p.s. I assumed the first term goes to zero when taking a derivative by the product rule. I confused d/dx x = 1, with any number d/dd = 0","Let f(x) = $$\displaystyle f(x) = x\sqrt{5-x} $$ On the interval: [-6,4] Critical numbers are the the values of x in the domain of f for which f'(x) = 0 or f'(x) is undefined. Derivative of the function: $$ \frac{1}{2} \cdot x (5-x)^{\frac{-1}{2}} \cdot -1$$ $$ \frac {\frac{-x}{2}}{\sqrt{5-x}}$$ $f'(x) = 0$, when $x = 0, $ and is undefined when x= 5 Plugging in the roots of the derivative function and the end points of the interval into the original function: \begin{align*} f(0) & = 0\\ f(5) & = 0\\ f(-6) & = -6\sqrt{11}\\ f(4) & = 4 \cdot 1 = 4 \end{align*} So why is the 4 not the absolute maximum value? p.s. I assumed the first term goes to zero when taking a derivative by the product rule. I confused d/dx x = 1, with any number d/dd = 0",,"['calculus', 'derivatives']"
40,Finding all real solutions to the equation $3^x+4^x=5^x$,Finding all real solutions to the equation,3^x+4^x=5^x,"Find all real solutions to the equation $$3^x+4^x=5^x.$$ My attempt: It is evident that $x=2$ is a solution. However, I think that there are no other solutions. So, I define a function $f(x)=3^x+4^x-5^x$. Differentiating w.r.t $x$, we get $$f'(x)=3^x\ln 3+4^x\ln 4-5^x\ln 5,$$ but that doesn't take me anywhere. Please help. Thank you.","Find all real solutions to the equation $$3^x+4^x=5^x.$$ My attempt: It is evident that $x=2$ is a solution. However, I think that there are no other solutions. So, I define a function $f(x)=3^x+4^x-5^x$. Differentiating w.r.t $x$, we get $$f'(x)=3^x\ln 3+4^x\ln 4-5^x\ln 5,$$ but that doesn't take me anywhere. Please help. Thank you.",,"['calculus', 'derivatives']"
41,Find taylor of $\psi (z)$ where $(e^z-1)^2=z^2 \psi (z)$ - first 3 terms,Find taylor of  where  - first 3 terms,\psi (z) (e^z-1)^2=z^2 \psi (z),"I was asked to find the first three terms in the taylor series of $\psi (z)$ around $z=0$ where $(e^z-1)^2=z^2 \psi(z)$ and I'm having a few difficulties. My original idea was to say $\psi (z)=\frac{(e^z-1)^2}{z^2}$ and then find the taylor series of $(e^z-1)^2$ at $z=0$, find the taylor series of $\frac{1}{z^2}$ at $z=0$ and multiply. The problem is that there is no such thing as taylor series of $\frac{1}{z^2}$ at $z=0$ since it is not analytic, continuous, or even defined at $z=0$. So that was a bad idea. My second idea ""worked"", but I want to check my answer. The idea was to find the taylor series of $f(z)=(e^z-1)^2$. It's easy to see that $f(0)=f'(0)=0$ so the first 2 terms are $0$, and for any $n \geq 2$: $f^{(n)}(z)=2e^z$ and so $f^{(n)}(0)=2$ So the taylor series of $f$ around $z=0$ is $f(z)=\frac{2}{2!}z^2+\frac{2}{3!}z^3+\frac{2}{4!}z^4+...$ So we have $\frac{2}{2!}z^2+\frac{2}{3!}+\frac{2}{4!}z^4+...=z^2 \psi(z)$, and so $\psi(z)=\frac{2}{2!}+\frac{2}{3!}z+\frac{2}{4!}z^2+...$ and we specified the first three terms. Is this result correct? I'm questioning myself because it defies logic that it's not possible in the previous way (of finding the taylor of $\frac{1}{z^2}$) but i'm getting a correct result here.","I was asked to find the first three terms in the taylor series of $\psi (z)$ around $z=0$ where $(e^z-1)^2=z^2 \psi(z)$ and I'm having a few difficulties. My original idea was to say $\psi (z)=\frac{(e^z-1)^2}{z^2}$ and then find the taylor series of $(e^z-1)^2$ at $z=0$, find the taylor series of $\frac{1}{z^2}$ at $z=0$ and multiply. The problem is that there is no such thing as taylor series of $\frac{1}{z^2}$ at $z=0$ since it is not analytic, continuous, or even defined at $z=0$. So that was a bad idea. My second idea ""worked"", but I want to check my answer. The idea was to find the taylor series of $f(z)=(e^z-1)^2$. It's easy to see that $f(0)=f'(0)=0$ so the first 2 terms are $0$, and for any $n \geq 2$: $f^{(n)}(z)=2e^z$ and so $f^{(n)}(0)=2$ So the taylor series of $f$ around $z=0$ is $f(z)=\frac{2}{2!}z^2+\frac{2}{3!}z^3+\frac{2}{4!}z^4+...$ So we have $\frac{2}{2!}z^2+\frac{2}{3!}+\frac{2}{4!}z^4+...=z^2 \psi(z)$, and so $\psi(z)=\frac{2}{2!}+\frac{2}{3!}z+\frac{2}{4!}z^2+...$ and we specified the first three terms. Is this result correct? I'm questioning myself because it defies logic that it's not possible in the previous way (of finding the taylor of $\frac{1}{z^2}$) but i'm getting a correct result here.",,"['complex-analysis', 'derivatives', 'taylor-expansion']"
42,An odd function $f$ is differentiable at zero. Prove $f'(0)=0$?,An odd function  is differentiable at zero. Prove ?,f f'(0)=0,"I know that $f'$ of an even function is odd function, thus I have $f(x)=f(-x)$. However I'd no idea how to prove that $f'(0)=0$? Please answer my question...","I know that $f'$ of an even function is odd function, thus I have $f(x)=f(-x)$. However I'd no idea how to prove that $f'(0)=0$? Please answer my question...",,"['calculus', 'derivatives']"
43,"Why is $\frac{d^n}{dx^n}(y(x))$ the notation for the $n$th derivative of $y(x)$, instead of $\frac{d^n}{d^nx}(y(x))$?","Why is  the notation for the th derivative of , instead of ?",\frac{d^n}{dx^n}(y(x)) n y(x) \frac{d^n}{d^nx}(y(x)),I've always wondered why the numerator is $d^n$ while the denominator is $dx^n$ instead of $d^nx$ like the numerator. I must be missing something very obvious or fundamental. Is this notation derived from the chain rule in some way?,I've always wondered why the numerator is $d^n$ while the denominator is $dx^n$ instead of $d^nx$ like the numerator. I must be missing something very obvious or fundamental. Is this notation derived from the chain rule in some way?,,"['calculus', 'derivatives', 'notation', 'partial-derivative', 'differential']"
44,"How to find the derivative of the inverse function $g^{-1}$, when no formula for the function $g$ is given?","How to find the derivative of the inverse function , when no formula for the function  is given?",g^{-1} g,"If $g$ is a strictly increasing function such that $g(7)=3$ and $g'(7)=7$, find $(g^{-1})'(3)$. I'm not saying to just give me the answer. I want to understand what the problem is asking and how to do it. I really appreciate it.","If $g$ is a strictly increasing function such that $g(7)=3$ and $g'(7)=7$, find $(g^{-1})'(3)$. I'm not saying to just give me the answer. I want to understand what the problem is asking and how to do it. I really appreciate it.",,"['calculus', 'derivatives']"
45,"Trying to understand ""derivative or Jacobian of smooth map""","Trying to understand ""derivative or Jacobian of smooth map""",,"From some lecture notes I am trying to puzzle through .... ""... the derivative or Jacobian of a smooth map $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$ at a point $x$ is a linear map $Df: \mathbb{R}^m \rightarrow \mathbb{R}^n$.  In terms of partial derivatives,  $Df_x(X) = (\sum_j\partial_{x_j}f_1 \cdot X_j, \sum_j \partial_{x_j}f_2\cdot X_j, ...)$ ... "" I'm so confused I'm not even sure where to begin.  Well, first, shouldn't the derivative be a map $Df:\mathbb{R}^m\rightarrow \mathbb{R}^m\times\mathbb{R}^n$?  Third, I am familiar with 3D integral calculus, and the only Jacobian I heard discussed there doesn't look like this at all, except, of course, that they both involve partial derivatibes.  Also, I don't even know what $f_1 \cdot X_j$ means. Thanks.","From some lecture notes I am trying to puzzle through .... ""... the derivative or Jacobian of a smooth map $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$ at a point $x$ is a linear map $Df: \mathbb{R}^m \rightarrow \mathbb{R}^n$.  In terms of partial derivatives,  $Df_x(X) = (\sum_j\partial_{x_j}f_1 \cdot X_j, \sum_j \partial_{x_j}f_2\cdot X_j, ...)$ ... "" I'm so confused I'm not even sure where to begin.  Well, first, shouldn't the derivative be a map $Df:\mathbb{R}^m\rightarrow \mathbb{R}^m\times\mathbb{R}^n$?  Third, I am familiar with 3D integral calculus, and the only Jacobian I heard discussed there doesn't look like this at all, except, of course, that they both involve partial derivatibes.  Also, I don't even know what $f_1 \cdot X_j$ means. Thanks.",,"['differential-geometry', 'derivatives']"
46,If angular velocity $\omega=\sqrt{\frac{3g\sin\theta}{2a}}$ can I find angular acceleration $\alpha$ by differentiating $\omega$?,If angular velocity  can I find angular acceleration  by differentiating ?,\omega=\sqrt{\frac{3g\sin\theta}{2a}} \alpha \omega,It was my understanding that angular acceleration is the derivative of angular velocity. The reason I ask is Thanks.,It was my understanding that angular acceleration is the derivative of angular velocity. The reason I ask is Thanks.,,"['trigonometry', 'derivatives', 'classical-mechanics']"
47,"How to calculate a derivative using the ""Power Rule"" If it includes a negative exponent?","How to calculate a derivative using the ""Power Rule"" If it includes a negative exponent?",,"So my understanding of the power rule is that you take your problem with an exponent like this: $x^5 = 5x^4$ or for $x^n$, $f'(x)=nx^{n-1}$ However, it does not seem to be working for me when applied to a problem with a negative exponent. $q^{-3} -> -3q^{-2}$ is not a correct answer. I have tried converting it to a fraction also: $q^{-3} -> \frac{1}{q^3}$ -> power rule: $\frac{1}{2q^2}$ is not correct. How can we find the derivative of a power function when the exponent is negative? Thanks","So my understanding of the power rule is that you take your problem with an exponent like this: $x^5 = 5x^4$ or for $x^n$, $f'(x)=nx^{n-1}$ However, it does not seem to be working for me when applied to a problem with a negative exponent. $q^{-3} -> -3q^{-2}$ is not a correct answer. I have tried converting it to a fraction also: $q^{-3} -> \frac{1}{q^3}$ -> power rule: $\frac{1}{2q^2}$ is not correct. How can we find the derivative of a power function when the exponent is negative? Thanks",,"['calculus', 'limits', 'derivatives']"
48,How to prove that Δy/Δx = f(x+Δx)-f(x) / Δx?,How to prove that Δy/Δx = f(x+Δx)-f(x) / Δx?,,"How do I prove that $$\frac{\Delta y}{\Delta x} = \frac{f(x+Δx)-f(x)} {Δx}.$$ I know that this is the slope formula to find the derivative of a function $y=f(x)$ and I know that the formula for a line tangent to $f$ in $(x_1, y_1)$ is: $$m=\frac{y-y_1}{x-x_1}$$ But really I do not know  why we replace $y$ by $f(x+Δx)$ and $y1$ by $f(x)$. Clear example with images would be appreciated.","How do I prove that $$\frac{\Delta y}{\Delta x} = \frac{f(x+Δx)-f(x)} {Δx}.$$ I know that this is the slope formula to find the derivative of a function $y=f(x)$ and I know that the formula for a line tangent to $f$ in $(x_1, y_1)$ is: $$m=\frac{y-y_1}{x-x_1}$$ But really I do not know  why we replace $y$ by $f(x+Δx)$ and $y1$ by $f(x)$. Clear example with images would be appreciated.",,"['calculus', 'derivatives']"
49,First derivative of multiplied powers,First derivative of multiplied powers,,Wolfram Alfa shows $\frac{d}{dx}e^{4y} = 4e^{4y}$ but I do not understand how to get to that answer I have $e^{4y} = (e^4)^y$ So by the chain rule is it not the case that \begin{align} \frac{d}{dx}e^{4y} & = y(e^4)^{y - 1}\cdot4e^3 \\ & = y \cdot e^{4y - 4} \cdot 4e^3 \\ & = 4ye^{4y-1} \\ & = \frac{4ye^{4y}}{e} \end{align} Clearly somewhere I am making a big mess,Wolfram Alfa shows $\frac{d}{dx}e^{4y} = 4e^{4y}$ but I do not understand how to get to that answer I have $e^{4y} = (e^4)^y$ So by the chain rule is it not the case that \begin{align} \frac{d}{dx}e^{4y} & = y(e^4)^{y - 1}\cdot4e^3 \\ & = y \cdot e^{4y - 4} \cdot 4e^3 \\ & = 4ye^{4y-1} \\ & = \frac{4ye^{4y}}{e} \end{align} Clearly somewhere I am making a big mess,,"['calculus', 'derivatives', 'exponentiation']"
50,The derivative of $x^2 \cdot \cos(x)$,The derivative of,x^2 \cdot \cos(x),I want to know how to derive this function. Can someone explain the steps? I know most derivative rules but I'm clearly not seeing how this works: $$\frac{d}{dx}(\ x^2cos(x)) = x(2\cos(x) - x\sin(x))$$ If you could help me to understand which rules are used (even very basic ones).,I want to know how to derive this function. Can someone explain the steps? I know most derivative rules but I'm clearly not seeing how this works: $$\frac{d}{dx}(\ x^2cos(x)) = x(2\cos(x) - x\sin(x))$$ If you could help me to understand which rules are used (even very basic ones).,,[]
51,Find equation of a tangent on $y= \sin2x$,Find equation of a tangent on,y= \sin2x,"Find equation of a tangent on $y= \sin2x$ in intersections with $y=\frac{1}{2}$ What I calculated: Intersections: $$\sin2x= \frac{1}{2}$$ ... $$0=tg^2x-4tgx-1$$ $$tgx_{1}=2+\sqrt{3}$$ $$x_{1}=75+k\pi$$ $$tgx_{2}=2-\sqrt{3}$$ $$x_{2}=15+ k\pi$$ $$T_{1}(75+k\pi,\frac{1}{2})$$ and $$T_{2}(15+k\pi,\frac{1}{2})$$ Derivative of $y= sin2x$ is: $y'= 2cos2x$ $$ y'(75)= -\sqrt {3} =k_{t_{1}} $$  $$y'(15)= \sqrt {3} =k_{t_{2}}$$  formula for calculating tangent that I used: $y-y_{1}=k_{t}(x-x_{1})$ 1.)$T_{1}(75+k\pi,\frac{1}{2})$ $k_{t_{1}}=-\sqrt {3}$ $y_{1}-\frac{1}{2}=-\sqrt {3}(x-75-k\pi)$ $y_{1}=-\sqrt {3}x+75\sqrt {3}+\sqrt {3}k\pi+\frac{1}{2}$ 2.)$T_{2}(15+k\pi,\frac{1}{2})$ $k_{t_{2}}=\sqrt {3}$ $y_{2}-\frac{1}{2}=\sqrt {3}(x-15 -k\pi)$ $y_{2}=\sqrt {3}x-15\sqrt {3}-\sqrt {3}k\pi+\frac{1}{2}$ Both solutions are wrong; what I should actually get: $y_{1}=\sqrt{3}x-\frac{\sqrt{3}pi-6+12kpi\sqrt{3}}{12} $ and $y_{2}=-\sqrt{3}x-\frac{5\sqrt{3}pi+6+12kpi\sqrt{3}}{12};k=Z $ How could I possibly get that ?!","Find equation of a tangent on $y= \sin2x$ in intersections with $y=\frac{1}{2}$ What I calculated: Intersections: $$\sin2x= \frac{1}{2}$$ ... $$0=tg^2x-4tgx-1$$ $$tgx_{1}=2+\sqrt{3}$$ $$x_{1}=75+k\pi$$ $$tgx_{2}=2-\sqrt{3}$$ $$x_{2}=15+ k\pi$$ $$T_{1}(75+k\pi,\frac{1}{2})$$ and $$T_{2}(15+k\pi,\frac{1}{2})$$ Derivative of $y= sin2x$ is: $y'= 2cos2x$ $$ y'(75)= -\sqrt {3} =k_{t_{1}} $$  $$y'(15)= \sqrt {3} =k_{t_{2}}$$  formula for calculating tangent that I used: $y-y_{1}=k_{t}(x-x_{1})$ 1.)$T_{1}(75+k\pi,\frac{1}{2})$ $k_{t_{1}}=-\sqrt {3}$ $y_{1}-\frac{1}{2}=-\sqrt {3}(x-75-k\pi)$ $y_{1}=-\sqrt {3}x+75\sqrt {3}+\sqrt {3}k\pi+\frac{1}{2}$ 2.)$T_{2}(15+k\pi,\frac{1}{2})$ $k_{t_{2}}=\sqrt {3}$ $y_{2}-\frac{1}{2}=\sqrt {3}(x-15 -k\pi)$ $y_{2}=\sqrt {3}x-15\sqrt {3}-\sqrt {3}k\pi+\frac{1}{2}$ Both solutions are wrong; what I should actually get: $y_{1}=\sqrt{3}x-\frac{\sqrt{3}pi-6+12kpi\sqrt{3}}{12} $ and $y_{2}=-\sqrt{3}x-\frac{5\sqrt{3}pi+6+12kpi\sqrt{3}}{12};k=Z $ How could I possibly get that ?!",,"['calculus', 'derivatives']"
52,using definition of derivative,using definition of derivative,,"$f(x)=x^3-6x^2+9x-5$ is given. What is the value of $$\lim_{h\to0}\frac{[f'(1+2h)+f'(3-3h)]}{2h}$$ I tried to use the definition of derivative,and here it seems like the expression will be equal to something like the 2nd derivative of $f(x)$ but I'm confused with $2h$ and $-3h$.","$f(x)=x^3-6x^2+9x-5$ is given. What is the value of $$\lim_{h\to0}\frac{[f'(1+2h)+f'(3-3h)]}{2h}$$ I tried to use the definition of derivative,and here it seems like the expression will be equal to something like the 2nd derivative of $f(x)$ but I'm confused with $2h$ and $-3h$.",,['derivatives']
53,Intersection points of two polynomials,Intersection points of two polynomials,,"How to prove that two distinct polynomial functions of degree m and n, respectively,the graphs intersect in at most $max(m,n)$ points.","How to prove that two distinct polynomial functions of degree m and n, respectively,the graphs intersect in at most $max(m,n)$ points.",,"['polynomials', 'derivatives']"
54,Derivative of $ y = (2x - 3)^4 \cdot (x^2 + x + 1)^5$,Derivative of, y = (2x - 3)^4 \cdot (x^2 + x + 1)^5,"$$ y = (2x - 3)^4 \cdot (x^2 + x + 1)^5$$ I know that it should be the chain rule and product rule used together to get the answer $$ y = \frac{dx}{dy}((2x - 3)^4) \cdot (x^2 + x + 1)^5 +  \frac{dx}{dy}(x^2 + x + 1)^5 \cdot (2x - 3)^4 $$ this gives me something ridiculous like this $$8(2x-3)^3 \cdot (x^2 + x + 1)^5 + (x^2 + x + 1)^4 \cdot (2x+1) (2x-3)^4$$ This is wrong and I keep getting it, I don't know how to simplify it without expanding everything. The book Houdini's out $(2x -3)^3 (x^2 + x + 1)^4 (28x^2 - 12x - 7)$","$$ y = (2x - 3)^4 \cdot (x^2 + x + 1)^5$$ I know that it should be the chain rule and product rule used together to get the answer $$ y = \frac{dx}{dy}((2x - 3)^4) \cdot (x^2 + x + 1)^5 +  \frac{dx}{dy}(x^2 + x + 1)^5 \cdot (2x - 3)^4 $$ this gives me something ridiculous like this $$8(2x-3)^3 \cdot (x^2 + x + 1)^5 + (x^2 + x + 1)^4 \cdot (2x+1) (2x-3)^4$$ This is wrong and I keep getting it, I don't know how to simplify it without expanding everything. The book Houdini's out $(2x -3)^3 (x^2 + x + 1)^4 (28x^2 - 12x - 7)$",,['calculus']
55,$\ln X$ derivative problem?,derivative problem?,\ln X,"$f(x) = x \ln(\cos3x - x^3)$, then $f'(x)$ = ? I got $$\begin{align} &X * \left(\frac{1}{\cos3x - x^3}\right) * \left({-3}({\sin3x -3x^2})\right)\\  &= \frac{-(3x \sin3x) - 3x^3}{(\cos3x) -x^3}\\ \end{align}$$ and I don't know what to to next. The correct answer on my homework should be : $\ln(\cos3x) - (3x\tan3x) -3x^2$ But I have no idea how to arrive it. Please help.","$f(x) = x \ln(\cos3x - x^3)$, then $f'(x)$ = ? I got $$\begin{align} &X * \left(\frac{1}{\cos3x - x^3}\right) * \left({-3}({\sin3x -3x^2})\right)\\  &= \frac{-(3x \sin3x) - 3x^3}{(\cos3x) -x^3}\\ \end{align}$$ and I don't know what to to next. The correct answer on my homework should be : $\ln(\cos3x) - (3x\tan3x) -3x^2$ But I have no idea how to arrive it. Please help.",,['derivatives']
56,"Show $\frac{d}{dx} \int\limits^{a(x)}_{0} f(x,y)dy = \int\limits^{a}_{0} \frac{\partial f}{\partial x}dy + a'(x)f(x,a)$",Show,"\frac{d}{dx} \int\limits^{a(x)}_{0} f(x,y)dy = \int\limits^{a}_{0} \frac{\partial f}{\partial x}dy + a'(x)f(x,a)","I am trying to show that  $$ \frac{d}{dx} \int\limits^{a(x)}_{0} f(x,y)dy = \int\limits^{a}_{0}  \frac{\partial f}{\partial x}dy + a'(x)f(x,a) $$ I know this has something to do with the fundamental theorem of calculus but am having trouble making any sort of progress. If someone could point me in the right direction, it would be much appreciated.","I am trying to show that  $$ \frac{d}{dx} \int\limits^{a(x)}_{0} f(x,y)dy = \int\limits^{a}_{0}  \frac{\partial f}{\partial x}dy + a'(x)f(x,a) $$ I know this has something to do with the fundamental theorem of calculus but am having trouble making any sort of progress. If someone could point me in the right direction, it would be much appreciated.",,"['calculus', 'integration', 'derivatives']"
57,Simple chain rule application $y = (1-x^{-1})^{-1}$,Simple chain rule application,y = (1-x^{-1})^{-1},"I am not sure what is going wrong here. I have been doing other applications of the chain rule to cross check that I understand it properly ,but I still do not get a correct answer on this problem while I do on all others. $$y = (1-x^{-1})^{-1}$$ $$y' = -(1-x^{-1})^{-2} \cdot x^{-2}$$ This is wrong and it  gives a wrong answer, according to wolfram and my book the answer should just be the first part which breaks the chain rule and I do understand why this is acceptable in this specific case but no others.","I am not sure what is going wrong here. I have been doing other applications of the chain rule to cross check that I understand it properly ,but I still do not get a correct answer on this problem while I do on all others. $$y = (1-x^{-1})^{-1}$$ $$y' = -(1-x^{-1})^{-2} \cdot x^{-2}$$ This is wrong and it  gives a wrong answer, according to wolfram and my book the answer should just be the first part which breaks the chain rule and I do understand why this is acceptable in this specific case but no others.",,"['calculus', 'derivatives']"
58,Derivatives vs Integration,Derivatives vs Integration,,"Given that the continuous function $f: \Bbb R \longrightarrow \Bbb R$ satisfies   $$\int_0^\pi f(x) ~dx = \pi,$$   Find the exact value of   $$\int_0^{\pi^{1/6}} x^5 f(x^6) ~dx.$$ Let   $$g(t) = \int_t^{2t} \frac{x^2 + 1}{x + 1} ~dx.$$   Find $g'(t)$. For the first question: The way I understand this is that the area under $f(x)$ from $0$ to $\pi$ is $\pi$. Doesn't this mean that the function can be $f(x)=1$? Are there other functions that satisfy this definition? The second line in part one also confuses me, specifically the $x^6$ part! For the second question: Does this have to do something with the Second Fundamental Theory of Calculus? I see that there are two variables, $x$ and $t$, that are involved in this equation.","Given that the continuous function $f: \Bbb R \longrightarrow \Bbb R$ satisfies   $$\int_0^\pi f(x) ~dx = \pi,$$   Find the exact value of   $$\int_0^{\pi^{1/6}} x^5 f(x^6) ~dx.$$ Let   $$g(t) = \int_t^{2t} \frac{x^2 + 1}{x + 1} ~dx.$$   Find $g'(t)$. For the first question: The way I understand this is that the area under $f(x)$ from $0$ to $\pi$ is $\pi$. Doesn't this mean that the function can be $f(x)=1$? Are there other functions that satisfy this definition? The second line in part one also confuses me, specifically the $x^6$ part! For the second question: Does this have to do something with the Second Fundamental Theory of Calculus? I see that there are two variables, $x$ and $t$, that are involved in this equation.",,"['calculus', 'integration', 'derivatives']"
59,Switching between $h$ and $-h$ in the definition of derivative of a real function.,Switching between  and  in the definition of derivative of a real function.,h -h,"Suppose that $f$ is a real function and that $f'(a)$ exists: $$ f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}. $$ By replacing all instances of $h$ with $-h$ , I can get an equivalent definition $$ f'(a) = \lim_{-h \to 0} \frac{f(a-h) - f(a)}{-h}. $$ Question: Is it right to say that I can replace $\lim_{-h \to 0}$ with $\lim_{h \to 0}$ in the second definition because $-h$ tending to $0$ is the same as saying $h$ tends to $0$ from the left/bottom? Since $f'(a)$ exists, the left limit must be equal to the right limit and hence the switch is valid?","Suppose that is a real function and that exists: By replacing all instances of with , I can get an equivalent definition Question: Is it right to say that I can replace with in the second definition because tending to is the same as saying tends to from the left/bottom? Since exists, the left limit must be equal to the right limit and hence the switch is valid?",f f'(a)  f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}.  h -h  f'(a) = \lim_{-h \to 0} \frac{f(a-h) - f(a)}{-h}.  \lim_{-h \to 0} \lim_{h \to 0} -h 0 h 0 f'(a),"['calculus', 'derivatives']"
60,Finding the Derivative Of $f(x) = 7\ln(5xe^{-x})$,Finding the Derivative Of,f(x) = 7\ln(5xe^{-x}),The original question is $f(x) = 7\ln(5xe^{-x})$ I'm not sure if I have to use the chain rule to figure out $\ln(5xe^{-x})$ because $5xe^{-x}$ is one term within ln. My guess is that it's like this: $$7(-(e^{-x-1})/e^{-x})$$ or just simply $-7$. I'm specifically unsure with how to find the derivative of $5xe^{-x}$. I know that $e^x$'s derivative is simply $e^x(1)$ because the derivative of $x = 1$ so when I find the derivative of $e^{-x}$ I'd expect it to be $-1e^x$ and in my case $-5xe^{-x}$,The original question is $f(x) = 7\ln(5xe^{-x})$ I'm not sure if I have to use the chain rule to figure out $\ln(5xe^{-x})$ because $5xe^{-x}$ is one term within ln. My guess is that it's like this: $$7(-(e^{-x-1})/e^{-x})$$ or just simply $-7$. I'm specifically unsure with how to find the derivative of $5xe^{-x}$. I know that $e^x$'s derivative is simply $e^x(1)$ because the derivative of $x = 1$ so when I find the derivative of $e^{-x}$ I'd expect it to be $-1e^x$ and in my case $-5xe^{-x}$,,"['calculus', 'derivatives']"
61,Tangent of parabola,Tangent of parabola,,A parabola is given by the Cartesian equation: $y^2=16x$ So that can be written as $y = \pm\sqrt{16x}$ So the derivative (to find the gradient of the tangent at a point $x$) is $$\dfrac{dy}{dx} = \pm 2x^{-0.5}.$$ So when putting this (as $m$) in the equation $y - y_1 = m(x - x_1)$ Should I use the positive or negative derivative (which do I choose out of the plus or minus)?,A parabola is given by the Cartesian equation: $y^2=16x$ So that can be written as $y = \pm\sqrt{16x}$ So the derivative (to find the gradient of the tangent at a point $x$) is $$\dfrac{dy}{dx} = \pm 2x^{-0.5}.$$ So when putting this (as $m$) in the equation $y - y_1 = m(x - x_1)$ Should I use the positive or negative derivative (which do I choose out of the plus or minus)?,,"['calculus', 'derivatives']"
62,Derivative with trig functions and ln trickery,Derivative with trig functions and ln trickery,,"I am suppose to differentiate $y=(\sin x)^{\ln x}$ I have absolutely no idea, this was asked on a test and I just do not know how to do this I have forgotten the tricks I was suppose to memorize for the test.","I am suppose to differentiate $y=(\sin x)^{\ln x}$ I have absolutely no idea, this was asked on a test and I just do not know how to do this I have forgotten the tricks I was suppose to memorize for the test.",,['calculus']
63,Can I multiply *into* limits?,Can I multiply *into* limits?,,I learned a lot yesterday and it requires me to overhaul the way I think about derivatives.  So I have something worked out but it relies on the answer to this question. EDIT : Basically I need a simple proof that says $b \cdot [\lim_{h \to a} g(h)] = \lim_{h \to a} [b\cdot g(h)]$,I learned a lot yesterday and it requires me to overhaul the way I think about derivatives.  So I have something worked out but it relies on the answer to this question. EDIT : Basically I need a simple proof that says $b \cdot [\lim_{h \to a} g(h)] = \lim_{h \to a} [b\cdot g(h)]$,,"['calculus', 'limits', 'derivatives']"
64,A question about rational functions in complex analysis,A question about rational functions in complex analysis,,"In Ahlfors's complex analysis $$ R(z)=\frac{P(z)}{Q(z)} $$ given as the quotient of two polynomials. We assume, and this is essential, that $P(z)$ and $Q(z)$ have no common factors and hence no common zeros. $R(z)$ will be given the value $\infty$ at the zeros of $Q(z)$ . It must therefore be considered as a function with values in the extended plane, and as such it is continuous. The zeros of $Q(z)$ are called poles of $R(z)$ , and the order of a pole is by definition equal to the order of the corresponding zero of $Q(z)$ . The derivative $$ R^{\prime}(z)=\frac{P^{\prime}(z) Q(z)-Q^{\prime}(z) P(z)}{Q(z)^2} \tag{11} $$ exists only when $Q(z) \neq 0$ . However, as a rational function defined by the right-hand member of (11), $R^{\prime}(z)$ has the same poles as $R(z)$ , the order of each pole being increased by one. In case $Q(z)$ has multiple zeros, it should be noticed that the expression (11) does not appear in reduced form. I can understand these two statements, why the does   the order of each pole being increased by one I can't see why is this true, The second statement I really have no idea what he means.","In Ahlfors's complex analysis given as the quotient of two polynomials. We assume, and this is essential, that and have no common factors and hence no common zeros. will be given the value at the zeros of . It must therefore be considered as a function with values in the extended plane, and as such it is continuous. The zeros of are called poles of , and the order of a pole is by definition equal to the order of the corresponding zero of . The derivative exists only when . However, as a rational function defined by the right-hand member of (11), has the same poles as , the order of each pole being increased by one. In case has multiple zeros, it should be noticed that the expression (11) does not appear in reduced form. I can understand these two statements, why the does   the order of each pole being increased by one I can't see why is this true, The second statement I really have no idea what he means.","
R(z)=\frac{P(z)}{Q(z)}
 P(z) Q(z) R(z) \infty Q(z) Q(z) R(z) Q(z) 
R^{\prime}(z)=\frac{P^{\prime}(z) Q(z)-Q^{\prime}(z) P(z)}{Q(z)^2}
\tag{11}
 Q(z) \neq 0 R^{\prime}(z) R(z) Q(z)","['complex-analysis', 'derivatives', 'rational-functions']"
65,Gradient zero implies lipschitz with arbitrary constant,Gradient zero implies lipschitz with arbitrary constant,,"I can't work out if this is true or not: Let $f:\mathbb{R}^N\rightarrow \mathbb{R}$ be differentiable at zero with $\nabla f(0)=0$ and $f(0)=0$ . Is it true that for any $\delta > 0$ , there is a neighbourhood of $0$ over which $f$ is Lipschitz with constant $\delta$ ? i.e. does this hold: $$\forall \delta > 0, \exists r > 0 \text{ such that } \forall x, y \in B_r(0), |f(x)-f(y)|<\delta |x-y|$$ where $B_r(0)$ is the $r-$ ball around $0$ . I think it is true but I am struggling to prove it. I thought maybe I could use a proof by contradiction by assuming the opposite of the above and finding $x_n,y_n\rightarrow 0$ such that $|f(x_n)-f(y_n)|>\delta|x_n-y_n|$ to then somehow reach that $|\nabla f|>\delta$ but I can't get this to work. If it isn't true, I can't manage to make a counterexample. Note, I do not want to assume differentiability of $f$ for $x\neq 0$ .","I can't work out if this is true or not: Let be differentiable at zero with and . Is it true that for any , there is a neighbourhood of over which is Lipschitz with constant ? i.e. does this hold: where is the ball around . I think it is true but I am struggling to prove it. I thought maybe I could use a proof by contradiction by assuming the opposite of the above and finding such that to then somehow reach that but I can't get this to work. If it isn't true, I can't manage to make a counterexample. Note, I do not want to assume differentiability of for .","f:\mathbb{R}^N\rightarrow \mathbb{R} \nabla f(0)=0 f(0)=0 \delta > 0 0 f \delta \forall \delta > 0, \exists r > 0 \text{ such that } \forall x, y \in B_r(0), |f(x)-f(y)|<\delta |x-y| B_r(0) r- 0 x_n,y_n\rightarrow 0 |f(x_n)-f(y_n)|>\delta|x_n-y_n| |\nabla f|>\delta f x\neq 0","['limits', 'derivatives', 'epsilon-delta', 'lipschitz-functions']"
66,"Let $f$ be continuous in $[0,2]$ and differentiable in $(0,2)$ such that $|f'(x)|\leqslant1$ and $f(0)=1=f(2)$. Prove that $f(x)\geqslant0$.",Let  be continuous in  and differentiable in  such that  and . Prove that .,"f [0,2] (0,2) |f'(x)|\leqslant1 f(0)=1=f(2) f(x)\geqslant0","Let $f$ be a continuous function in $[0,2]$ and differentiable in $(0,2)$ , such that $|f'(x)|\leqslant1$ for all $x\in(0,2)$ and also $f(0)=1=f(2)$ . Prove that $f(x)\geqslant0$ for all $x\in(0,2)$ . My approach is to use Intermediate Mean value theorem and then we get, that there exists a point $c\in(0,2)$ such that $|f'(c)|=0$ , so $c$ is local extreme. If $c$ is local minimum than the proof is straight forward, bit I have difficulties proving if $c$ is local maximum. I also need to be sure that I can continue in this way or I need another way to prove the statement. Thanks a lot!","Let be a continuous function in and differentiable in , such that for all and also . Prove that for all . My approach is to use Intermediate Mean value theorem and then we get, that there exists a point such that , so is local extreme. If is local minimum than the proof is straight forward, bit I have difficulties proving if is local maximum. I also need to be sure that I can continue in this way or I need another way to prove the statement. Thanks a lot!","f [0,2] (0,2) |f'(x)|\leqslant1 x\in(0,2) f(0)=1=f(2) f(x)\geqslant0 x\in(0,2) c\in(0,2) |f'(c)|=0 c c c","['analysis', 'derivatives', 'maxima-minima', 'rolles-theorem', 'mean-value-theorem']"
67,"Does an undefined derivative always mean a vertical tangent line, and why do we define a tangent line when the derivative is undefined?","Does an undefined derivative always mean a vertical tangent line, and why do we define a tangent line when the derivative is undefined?",,"I am wondering whether an undefined derivative at a point implies that the tangent line to that point is vertical, and also how the tangent line could still exist if the derivative doesn't exist. For my first question, although I have read that vertical tangent lines will have an undefined derivative, I am wondering whether every single instance of a derivative being undefined at a point means that the tangent line at that point is vertical, and that there isn't another explanation for an undefined derivative. I am confused on this because my Professor intermediately concludes a vertical tangent line when the derivative is undefined, but it seems any situation where the derivative doesn't exist would lead to an undefined derivative, which may not necessarily have a vertical tangent line. For my second question, what is the reason why we regard the tangent line as existing when the derivative doesn't exist when we define the tangent line to a point to have slope equal to the derivative at that point? It seems that if a component of the tangent (its slope) doesn't exist, then the tangent line itself wouldn't exist. UPDATE: I am now wondering whether the derivative being of the form $a \over 0$ implies that there is a vertical tangent line after reading the answers. Can a derivative of the form $a \over 0 $ eventualize for any other reason that a vertical tangent line?","I am wondering whether an undefined derivative at a point implies that the tangent line to that point is vertical, and also how the tangent line could still exist if the derivative doesn't exist. For my first question, although I have read that vertical tangent lines will have an undefined derivative, I am wondering whether every single instance of a derivative being undefined at a point means that the tangent line at that point is vertical, and that there isn't another explanation for an undefined derivative. I am confused on this because my Professor intermediately concludes a vertical tangent line when the derivative is undefined, but it seems any situation where the derivative doesn't exist would lead to an undefined derivative, which may not necessarily have a vertical tangent line. For my second question, what is the reason why we regard the tangent line as existing when the derivative doesn't exist when we define the tangent line to a point to have slope equal to the derivative at that point? It seems that if a component of the tangent (its slope) doesn't exist, then the tangent line itself wouldn't exist. UPDATE: I am now wondering whether the derivative being of the form implies that there is a vertical tangent line after reading the answers. Can a derivative of the form eventualize for any other reason that a vertical tangent line?",a \over 0 a \over 0 ,"['calculus', 'derivatives', 'tangent-line']"
68,"Application of Derivatives , maxima minima jee mains 27th Aug Morning Shift 2021","Application of Derivatives , maxima minima jee mains 27th Aug Morning Shift 2021",,"This is the question Q) A wire of length 20 m is to be cut into two pieces. One of the pieces is to be made into a square and the other into a regular hexagon. Then the length of the side (in meters) of the hexagon, so that the combined area of the square and the hexagon is minimum, is: The way I tried solve is taking $x$ length of wire to make the square which means $20-x$ is the perimeter of the hexagon. So, For The sum of Areas $=$ Minimum $$\frac d{dx} \left[\left(\frac{x}{4}\right)^2 + \frac{3√3}{2} \left(\frac{20-x}{6}\right)^2\right]  = 0$$ From this we get $x = -(40√3 +80) $ And if we put $x$ in $\frac{20-x}6$ , we get $\frac{50+20\sqrt3}3$ Which isn't even in the options and, I'm unable to detect my mistake here, any help would be much appreciated. The other is taking $4a + 6b =20$ Which gives the correct value of $b$ which is $\frac{10}{3+2\sqrt3} $","This is the question Q) A wire of length 20 m is to be cut into two pieces. One of the pieces is to be made into a square and the other into a regular hexagon. Then the length of the side (in meters) of the hexagon, so that the combined area of the square and the hexagon is minimum, is: The way I tried solve is taking length of wire to make the square which means is the perimeter of the hexagon. So, For The sum of Areas Minimum From this we get And if we put in , we get Which isn't even in the options and, I'm unable to detect my mistake here, any help would be much appreciated. The other is taking Which gives the correct value of which is","x 20-x = \frac d{dx} \left[\left(\frac{x}{4}\right)^2 + \frac{3√3}{2} \left(\frac{20-x}{6}\right)^2\right]
 = 0 x = -(40√3 +80)  x \frac{20-x}6 \frac{50+20\sqrt3}3 4a + 6b =20 b \frac{10}{3+2\sqrt3} ",['derivatives']
69,Prove $\lim \limits_{h \to 0} \frac{a^h-1}{h} = \ln (a)$ without using L'hospital's.,Prove  without using L'hospital's.,\lim \limits_{h \to 0} \frac{a^h-1}{h} = \ln (a),"I'm a Calc 2 student and was curious as to why $\frac{d}{dx}a^x = a^x\ln a$ . Using the limit definition you can arrive at $\frac{d}{dx}a^x = \lim \limits_{h \to 0}  \frac{a^x(a^h-1)}{h}$ so the part of the limit involving $h$ must be $\ln (a)$ . I was thinking it'd be cool to use the definition of $e^x = \sum_{n=1}^\infty \frac{x^n}{n!}$ by setting $y=\sum_{n=1}^\infty \frac{x^n}{n!}$ and solving for $x$ which I'm guessing yields some sort of sum representation for $\ln(x)$ . Given the above I have a couple questions. How can I turn a limit expression such as $\lim \limits_{h \to 0}  \frac{(a^h-1)}{h}$ into a Riemann sum and vice versa? How would someone go about solving for $x$ in $y =\sum_{n=1}^\infty \frac{x^n}{n!}$ I know there are prob way easier ways to figure that out, but I'm curious as to whether this way of solving it works and how it pans out. Thanks.","I'm a Calc 2 student and was curious as to why . Using the limit definition you can arrive at so the part of the limit involving must be . I was thinking it'd be cool to use the definition of by setting and solving for which I'm guessing yields some sort of sum representation for . Given the above I have a couple questions. How can I turn a limit expression such as into a Riemann sum and vice versa? How would someone go about solving for in I know there are prob way easier ways to figure that out, but I'm curious as to whether this way of solving it works and how it pans out. Thanks.",\frac{d}{dx}a^x = a^x\ln a \frac{d}{dx}a^x = \lim \limits_{h \to 0}  \frac{a^x(a^h-1)}{h} h \ln (a) e^x = \sum_{n=1}^\infty \frac{x^n}{n!} y=\sum_{n=1}^\infty \frac{x^n}{n!} x \ln(x) \lim \limits_{h \to 0}  \frac{(a^h-1)}{h} x y =\sum_{n=1}^\infty \frac{x^n}{n!},"['limits', 'derivatives', 'logarithms', 'exponential-function', 'riemann-sum']"
70,Prove without using L'Hopital's rule that $f''(a)=0$ if $\lim_{x\to a}{\frac{f(x)-f(a)}{(x-a)^2}}=0$,Prove without using L'Hopital's rule that  if,f''(a)=0 \lim_{x\to a}{\frac{f(x)-f(a)}{(x-a)^2}}=0,"I'm trying to prove without using L'Hopital's rule that $f''(a)=0$ if $$\lim_{x\to a}{\frac{f(x)-f(a)}{(x-a)^2}}=0,$$ assuming $f$ is twice differentiable and $f''$ is continuous. My first instinct was to use the mean value theorem to define a real number $t$ such that $f'(t)=\frac{f(x)-f(a)}{x-a}$ , which gives $\lim_{x\to a}{\frac{f'(t)}{x-a}}=0$ . Since $t\to a$ as $x\to a$ , this gives $f'(a)=0$ , and $$\lim_{x\to a}{\frac{f'(t)}{x-a}}=\lim_{x\to a}{\frac{f'(t)}{f'(x)}}\times\lim_{x\to a}{\frac{f'(x)}{x-a}}=1\times f''(a)=0,$$ thus $f''(a)=0$ . I wasn't entirely convinced, however, and quickly found counterexamples such as $\lim_{x\to0}{\frac{\sin x^3}{\sin x}}=0$ (this doesn't exactly satisfy the mean value theorem, but I used this to disprove that $\lim_{x\to a}{\frac{f'(t)}{f'(x)}}=1$ in general). Are there any additional steps I should be taking? Or should I take an entirely different approach (if even possible without L'Hopital's rule)?","I'm trying to prove without using L'Hopital's rule that if assuming is twice differentiable and is continuous. My first instinct was to use the mean value theorem to define a real number such that , which gives . Since as , this gives , and thus . I wasn't entirely convinced, however, and quickly found counterexamples such as (this doesn't exactly satisfy the mean value theorem, but I used this to disprove that in general). Are there any additional steps I should be taking? Or should I take an entirely different approach (if even possible without L'Hopital's rule)?","f''(a)=0 \lim_{x\to a}{\frac{f(x)-f(a)}{(x-a)^2}}=0, f f'' t f'(t)=\frac{f(x)-f(a)}{x-a} \lim_{x\to a}{\frac{f'(t)}{x-a}}=0 t\to a x\to a f'(a)=0 \lim_{x\to a}{\frac{f'(t)}{x-a}}=\lim_{x\to a}{\frac{f'(t)}{f'(x)}}\times\lim_{x\to a}{\frac{f'(x)}{x-a}}=1\times f''(a)=0, f''(a)=0 \lim_{x\to0}{\frac{\sin x^3}{\sin x}}=0 \lim_{x\to a}{\frac{f'(t)}{f'(x)}}=1","['calculus', 'derivatives', 'limits-without-lhopital', 'mean-value-theorem']"
71,Property of Antiderivatives,Property of Antiderivatives,,"Consider two differentiable functions $f:\mathbb R \rightarrow \mathbb R$ and $g:\mathbb R \rightarrow \mathbb R$ . Suppose that there exists a $\delta$ such that for $x>\delta$ , $f'(x)>g'(x)$ . I am trying to show that this implies there exists a $\delta_2$ such that for $x>\delta_2$ , $f(x)>g(x)$ . So far, I have used $f(x)=\int_{\delta}^x f'(t)dt + f(\delta)$ and similarly for $g(x)$ to show that this obviously holds if $f(\delta)>g(\delta)$ . I am now trying to construct the case where $f(\delta)<g(\delta)$ .","Consider two differentiable functions and . Suppose that there exists a such that for , . I am trying to show that this implies there exists a such that for , . So far, I have used and similarly for to show that this obviously holds if . I am now trying to construct the case where .",f:\mathbb R \rightarrow \mathbb R g:\mathbb R \rightarrow \mathbb R \delta x>\delta f'(x)>g'(x) \delta_2 x>\delta_2 f(x)>g(x) f(x)=\int_{\delta}^x f'(t)dt + f(\delta) g(x) f(\delta)>g(\delta) f(\delta)<g(\delta),"['calculus', 'derivatives']"
72,On the usage of derivative in operator theory,On the usage of derivative in operator theory,,"In quantum mechanics, we work with linear operators on Hilbert spaces $\mathscr{H}$ . Suppose I have two bounded ones, defined on the same space $A, B: \mathscr{H}\to\mathscr{H}$ . It seems to me there is an ambiguity on the way to deal with the derivative. On one way, the operator $AB$ is usually interpreted as the composition $(A\circ B)f:=A(B(f))$ for every test function $f$ on $\mathscr{H}$ . If so, the derivative $D$ operator on $AB$ should act as follows $$ D[AB]f = D[(A\circ B)f]= D[A(Bf)]D(B(f)) $$ On the other way, the following right examples treat $AB$ as if it were literally a product of operators instead of a composition. In other words, the preferred way to compute the derivative is the Leibniz rule $$ D[AB]= AD[B]+BD[A] $$ $1^{\rm{st}}$ example, by E. Pisanty : The exponential of an operator $\hat A(t)$ does not obey the differential equation $$ \frac{d}{ dt}e^{\hat A(t)} \stackrel{?}{=} \frac{d \hat{A}}{ dt} e^{\hat A(t)} $$ that one might naively hope to satisfy. To see why this does not work, consider the series expansion of the exponential \begin{align*} \frac{d}{dt}e^{\hat A(t)} & = \frac{d}{dt}\sum_{n=0}^\infty \frac{1}{n!} = \sum_{n=0}^\infty \frac{1}{n!} \frac{d}{dt} \hat A^n(t), \end{align*} When we apply the product rule, we get the individual derivatives of each of the operators in the product, at their place within the product \begin{equation*} \frac{d}{dt} \hat A^n(t) = \frac{d\hat A}{dt} \hat A^{n-1}(t) +\hat A(t)\frac{d\hat A}{dt} \hat A^{n-2}(t) + \ \dots \ +\hat A^{n-2}(t)\frac{d\hat A}{dt} \hat A(t) +\hat A^{n-1}(t)\frac{d\hat A}{dt}  \end{equation*} This can simplify to just $n\frac{ d\hat A}{dt} \hat A^{n-1}(t)$ , in which case $\frac{d}{dt}e^{\hat A(t)} = \frac{d \hat A}{dt} \sum_{n=1}^\infty \frac{\hat A^{n-1}(t)}{(n-1)!} = \frac{d\hat A}{dt}e^{\hat{A}(t)}$ ,  but only under the condition that $\hat A(t)$ commute with its derivative $$ \left[\frac{ d\hat A}{ dt} , \hat A(t)\right] \stackrel{?}{=} 0 $$ In this case, $A^n$ is seen as a product of $A$ with itself $n$ times, instead of $A \circ A \circ \dots \circ A$ $n$ times. $2^{\rm{nd}}$ example, by Wikipedia : The expectation value of an observable $A$ , which is a Hermitian linear operator, for a given Schrödinger state $\vert\psi(t)\rangle$ , is given by ${\displaystyle \langle A\rangle _{t}=\langle \psi (t)|A|\psi (t)\rangle .}$ In the Schrödinger picture, the state $\vert\psi(t)\rangle$ at time $t$ is related to the state $\vert\psi(0)\rangle$ at time $0$ by a unitary time-evolution operator $U(t)$ : ${\displaystyle |\psi (t)\rangle =U(t)|\psi (0)\rangle .}$ In the Heisenberg picture, all state vectors are considered to remain constant at their initial values $\vert \psi(t)\rangle$ , whereas operators evolve with time according to ${\displaystyle A(t):=U^{\dagger }(t)AU(t)\,.}$ The Schrödinger equation for the time-evolution operator is $${\displaystyle {\frac {d}{dt}}U(t)=-{\frac {iH}{\hbar }}U(t)}$$ where $H$ is the Hamiltonian and $\hbar$ is the reduced Planck constant. It now follows that $${\displaystyle {\begin{aligned}{\frac {d}{dt}}A(t)&={\frac {i}{\hbar }}U^{\dagger }(t)HAU(t)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t)+{\frac {i}{\hbar }}U^{\dagger }(t)A(-H)U(t)\\&={\frac {i}{\hbar }}U^{\dagger }(t)HU(t)U^{\dagger }(t)AU(t)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t)-{\frac {i}{\hbar }}U^{\dagger }(t)AU(t)U^{\dagger }(t)HU(t)\\&={\frac {i}{\hbar }}\left(H(t)A(t)-A(t)H(t)\right)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t),\end{aligned}}}$$ where differentiation was carried out according to the product rule. I really don't understand","In quantum mechanics, we work with linear operators on Hilbert spaces . Suppose I have two bounded ones, defined on the same space . It seems to me there is an ambiguity on the way to deal with the derivative. On one way, the operator is usually interpreted as the composition for every test function on . If so, the derivative operator on should act as follows On the other way, the following right examples treat as if it were literally a product of operators instead of a composition. In other words, the preferred way to compute the derivative is the Leibniz rule example, by E. Pisanty : The exponential of an operator does not obey the differential equation that one might naively hope to satisfy. To see why this does not work, consider the series expansion of the exponential When we apply the product rule, we get the individual derivatives of each of the operators in the product, at their place within the product This can simplify to just , in which case ,  but only under the condition that commute with its derivative In this case, is seen as a product of with itself times, instead of times. example, by Wikipedia : The expectation value of an observable , which is a Hermitian linear operator, for a given Schrödinger state , is given by In the Schrödinger picture, the state at time is related to the state at time by a unitary time-evolution operator : In the Heisenberg picture, all state vectors are considered to remain constant at their initial values , whereas operators evolve with time according to The Schrödinger equation for the time-evolution operator is where is the Hamiltonian and is the reduced Planck constant. It now follows that where differentiation was carried out according to the product rule. I really don't understand","\mathscr{H} A, B: \mathscr{H}\to\mathscr{H} AB (A\circ B)f:=A(B(f)) f \mathscr{H} D AB 
D[AB]f = D[(A\circ B)f]= D[A(Bf)]D(B(f))
 AB 
D[AB]= AD[B]+BD[A]
 1^{\rm{st}} \hat A(t)  \frac{d}{ dt}e^{\hat A(t)} \stackrel{?}{=} \frac{d \hat{A}}{ dt} e^{\hat A(t)}  \begin{align*}
\frac{d}{dt}e^{\hat A(t)}
& = \frac{d}{dt}\sum_{n=0}^\infty \frac{1}{n!} = \sum_{n=0}^\infty \frac{1}{n!} \frac{d}{dt} \hat A^n(t),
\end{align*} \begin{equation*}
\frac{d}{dt} \hat A^n(t)
=
\frac{d\hat A}{dt} \hat A^{n-1}(t)
+\hat A(t)\frac{d\hat A}{dt} \hat A^{n-2}(t)
+ \ \dots \
+\hat A^{n-2}(t)\frac{d\hat A}{dt} \hat A(t)
+\hat A^{n-1}(t)\frac{d\hat A}{dt} 
\end{equation*} n\frac{ d\hat A}{dt} \hat A^{n-1}(t) \frac{d}{dt}e^{\hat A(t)}
= \frac{d \hat A}{dt} \sum_{n=1}^\infty \frac{\hat A^{n-1}(t)}{(n-1)!} = \frac{d\hat A}{dt}e^{\hat{A}(t)} \hat A(t)  \left[\frac{ d\hat A}{ dt} , \hat A(t)\right] \stackrel{?}{=} 0  A^n A n A \circ A \circ \dots \circ A n 2^{\rm{nd}} A \vert\psi(t)\rangle {\displaystyle \langle A\rangle _{t}=\langle \psi (t)|A|\psi (t)\rangle .} \vert\psi(t)\rangle t \vert\psi(0)\rangle 0 U(t) {\displaystyle |\psi (t)\rangle =U(t)|\psi (0)\rangle .} \vert \psi(t)\rangle {\displaystyle A(t):=U^{\dagger }(t)AU(t)\,.} {\displaystyle {\frac {d}{dt}}U(t)=-{\frac {iH}{\hbar }}U(t)} H \hbar {\displaystyle {\begin{aligned}{\frac {d}{dt}}A(t)&={\frac {i}{\hbar }}U^{\dagger }(t)HAU(t)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t)+{\frac {i}{\hbar }}U^{\dagger }(t)A(-H)U(t)\\&={\frac {i}{\hbar }}U^{\dagger }(t)HU(t)U^{\dagger }(t)AU(t)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t)-{\frac {i}{\hbar }}U^{\dagger }(t)AU(t)U^{\dagger }(t)HU(t)\\&={\frac {i}{\hbar }}\left(H(t)A(t)-A(t)H(t)\right)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t),\end{aligned}}}","['derivatives', 'operator-theory', 'mathematical-physics', 'quantum-mechanics']"
73,"Spivak, Ch. 18, Problem 45: Find all functions satisfying $f^{(n)}=f^{(n-1)}$.","Spivak, Ch. 18, Problem 45: Find all functions satisfying .",f^{(n)}=f^{(n-1)},"The following is a problem from Spivak's Calculus , Chapter 18 Find all functions satisfying (a) $f^{(n)}=f^{(n-1)}$ The solution manual says (a) We have $f^{(n-1)}(x)=ce^x$ , so $$f(x)=a_0+a_1x+...+a_{n-2}x^{n-2}+ce^x$$ Why did he conclude that $f^{(n-1)}(x)$ must be $ce^{x}$ ? How do we know this represents all solutions? I believe we can conclude that $ce^{x}$ is a solution to $f^{(n-1)}(x)=f^{(n)}(x)$ based on a previous problem (Problem 43, which I discuss below), but we never showed that this is the only solution. Here are the problems that came before 45 Problem 42: if we can find a root $\alpha$ with multiplicity $r$ of the equation $\sum\limits_{i=1}^n a_ix^i$ , then we also automatically have $r$ different roots of the differential equation $\sum\limits_{i=1}^n a_if^{(i)}(x)$ . These roots are $x^ke^{\alpha k}$ for $0\leq k\leq r-1$ . Also, any linear combination of these roots is a root, so we have infinite roots. A note at the end of the problem says that the set of these linear combinations represents all the possible solutions, though this is not proved here. Problem 43: This problem I found very strange and not well specified but here goes. If a function $f$ satisfies $f''-f=0$ and $f(0)=f'(0)=0$ then it follows that $f=0$ . Now the proof of this is done in three steps (though I will only talk about two here). First we show that $f^2-(f')^2=0$ follows from the initial assumptions. Second, we show that if $f\neq 0$ in some interval $(a,b)$ then either $f(x)=ce^x$ or else $f(x)=ce^{-x}$ for all $x$ in $(a,b)$ and some constant $c$ . Here is the solution manual proof of this Since $f(x)\neq 0$ for $x$ in $(a,b)$ , it follows from part $(a)$ that either $f'(x)=f(x)$ for all $x$ in $(a,b)$ or else $f'(x)=-f(x)$ for all $x$ in $(a,b)$ . Thus either $f(x)=ce^x$ or else $f(x)=ce^{-x}$ for all $x$ in $(a,b)$ . I found this proof to be a bit strange, though probably only because it skips so many intermediate steps. Here is my proof of this result $$(f')^2=f^2 \implies f=f' \text{ or } f=-f'$$ Case 1: $f'-f=0$ As per Problem 42, if we can find a solution to the polynomial $x-1=0$ then we will have a solution to $f'-f=0$ . Since $1$ is the solution to the polynomial, then $f(x)=e^x$ is a solution to the differential equation. We can easily show that for any constant $c$ , $f(x)=ce^x$ is also a solution. Case 2: $f'+f=0$ . Analogous proof shows that $f(x)=e^{-x}$ is a solution. In any case, going back to problem 45a, $f^{(n)}=f^{(n-1)}$ means that $f(x)=ce^x$ is a solution for any constant $c$ . But I don't see how we've shown that these are the only possible solutions.","The following is a problem from Spivak's Calculus , Chapter 18 Find all functions satisfying (a) The solution manual says (a) We have , so Why did he conclude that must be ? How do we know this represents all solutions? I believe we can conclude that is a solution to based on a previous problem (Problem 43, which I discuss below), but we never showed that this is the only solution. Here are the problems that came before 45 Problem 42: if we can find a root with multiplicity of the equation , then we also automatically have different roots of the differential equation . These roots are for . Also, any linear combination of these roots is a root, so we have infinite roots. A note at the end of the problem says that the set of these linear combinations represents all the possible solutions, though this is not proved here. Problem 43: This problem I found very strange and not well specified but here goes. If a function satisfies and then it follows that . Now the proof of this is done in three steps (though I will only talk about two here). First we show that follows from the initial assumptions. Second, we show that if in some interval then either or else for all in and some constant . Here is the solution manual proof of this Since for in , it follows from part that either for all in or else for all in . Thus either or else for all in . I found this proof to be a bit strange, though probably only because it skips so many intermediate steps. Here is my proof of this result Case 1: As per Problem 42, if we can find a solution to the polynomial then we will have a solution to . Since is the solution to the polynomial, then is a solution to the differential equation. We can easily show that for any constant , is also a solution. Case 2: . Analogous proof shows that is a solution. In any case, going back to problem 45a, means that is a solution for any constant . But I don't see how we've shown that these are the only possible solutions.","f^{(n)}=f^{(n-1)} f^{(n-1)}(x)=ce^x f(x)=a_0+a_1x+...+a_{n-2}x^{n-2}+ce^x f^{(n-1)}(x) ce^{x} ce^{x} f^{(n-1)}(x)=f^{(n)}(x) \alpha r \sum\limits_{i=1}^n a_ix^i r \sum\limits_{i=1}^n a_if^{(i)}(x) x^ke^{\alpha k} 0\leq k\leq r-1 f f''-f=0 f(0)=f'(0)=0 f=0 f^2-(f')^2=0 f\neq 0 (a,b) f(x)=ce^x f(x)=ce^{-x} x (a,b) c f(x)\neq 0 x (a,b) (a) f'(x)=f(x) x (a,b) f'(x)=-f(x) x (a,b) f(x)=ce^x f(x)=ce^{-x} x (a,b) (f')^2=f^2 \implies f=f' \text{ or } f=-f' f'-f=0 x-1=0 f'-f=0 1 f(x)=e^x c f(x)=ce^x f'+f=0 f(x)=e^{-x} f^{(n)}=f^{(n-1)} f(x)=ce^x c","['calculus', 'integration', 'derivatives', 'proof-explanation', 'exponential-function']"
74,The function $y=f(x)$ is represented parametrically by $x=t^5-5t^3-20t+7$ and $y=4t^3-3t^2-18t+3$ $(-2\lt t\lt2)$. The minimum of $y=f(x)$ occurs at,The function  is represented parametrically by  and  . The minimum of  occurs at,y=f(x) x=t^5-5t^3-20t+7 y=4t^3-3t^2-18t+3 (-2\lt t\lt2) y=f(x),"Question: The function $y=f(x)$ is represented parametrically by $x=t^5-5t^3-20t+7$ and $y=4t^3-3t^2-18t+3$ $(-2\lt t\lt2)$ . The minimum of $y=f(x)$ occurs at $t=...$ My Attempt: For extrema, I am calculating $\frac{dy}{dx}$ $\frac{dy}{dx}=\frac{dy}{dt}\frac{dt}{dx}=\frac{12t^2-6t-18}{5t^4-15t^2-20}=\frac{6(2t^2-t-3)}{5(t^4-3t^2-4)}=\frac{6(2t-3)(t+1)}{5(t^2-4)(t^2+1)}$ Around $t=\frac32, f'(x)$ sign changes from positive to negative. So $t=\frac32$ is local maxima. Around $t=-1, f'(x)$ sign changes from negative to positive. So $t=-1$ is local minima. But the answer given is $\frac32$ What's wrong in my approach?","Question: The function is represented parametrically by and . The minimum of occurs at My Attempt: For extrema, I am calculating Around sign changes from positive to negative. So is local maxima. Around sign changes from negative to positive. So is local minima. But the answer given is What's wrong in my approach?","y=f(x) x=t^5-5t^3-20t+7 y=4t^3-3t^2-18t+3 (-2\lt t\lt2) y=f(x) t=... \frac{dy}{dx} \frac{dy}{dx}=\frac{dy}{dt}\frac{dt}{dx}=\frac{12t^2-6t-18}{5t^4-15t^2-20}=\frac{6(2t^2-t-3)}{5(t^4-3t^2-4)}=\frac{6(2t-3)(t+1)}{5(t^2-4)(t^2+1)} t=\frac32, f'(x) t=\frac32 t=-1, f'(x) t=-1 \frac32","['calculus', 'derivatives', 'polynomials', 'maxima-minima', 'parametric']"
75,Fitting exponential model to data,Fitting exponential model to data,,"I have a series of measurements of some value, about 10000 numbers. The numbers come timestamped in real-life clock seconds, at regular time intervals. All real-life measurements have errors, both systematic and random sensor noise, this one is no exception. An oversimplified mathematical model of the physical process says that that at least the middle portion of the data is probably described by the following differential equation: $$\frac{df(t)}{dt} = \alpha·S^{\beta}·f(t)^{\beta}$$ where $\alpha$ and $\beta$ are some unknown constants i.e. they don’t change over time (we also know $0 < \beta < 1$ ), and S is a known constant $10^{-5}$ . The formula basically says the time derivative of the function is proportional to some power of the functions’s current value. How can I find values of $\alpha$ and $\beta$ which would best fit (e.g. minimum squared error or similar) the measured experimental data of $f(t)$ function? Update: according to a symbolic math software, the above differential equation has the following solution: $$f(t) = {(t·\alpha·S^\beta·( 1 - \beta ) + C1)^{\frac{1}{1-\beta}} }$$ But the question remains, how to best fit that function to the measured data to get these constant numbers? Here's the source data","I have a series of measurements of some value, about 10000 numbers. The numbers come timestamped in real-life clock seconds, at regular time intervals. All real-life measurements have errors, both systematic and random sensor noise, this one is no exception. An oversimplified mathematical model of the physical process says that that at least the middle portion of the data is probably described by the following differential equation: where and are some unknown constants i.e. they don’t change over time (we also know ), and S is a known constant . The formula basically says the time derivative of the function is proportional to some power of the functions’s current value. How can I find values of and which would best fit (e.g. minimum squared error or similar) the measured experimental data of function? Update: according to a symbolic math software, the above differential equation has the following solution: But the question remains, how to best fit that function to the measured data to get these constant numbers? Here's the source data",\frac{df(t)}{dt} = \alpha·S^{\beta}·f(t)^{\beta} \alpha \beta 0 < \beta < 1 10^{-5} \alpha \beta f(t) f(t) = {(t·\alpha·S^\beta·( 1 - \beta ) + C1)^{\frac{1}{1-\beta}} },"['derivatives', 'numerical-methods']"
76,Is a differentiable function the product of differentiable functions subject to certain conditions?,Is a differentiable function the product of differentiable functions subject to certain conditions?,,"Honestly this is just a case of me not being great at constructing 'pathological' differentiable functions and I am not sure how to search for what I am looking for. This is related to an algebraic method of constructing cotangents that I have seen before, but it has been a long time so I don't remember the details. Let $D$ be the set of functions $f$ on $\Bbb{R}$ which are differentiable at $0$ and have $f(0)=0$ . Suppose $f_0\in D$ has $f_0'(0)=0$ . Let $D\cdot D$ be the set of finite sums of products of elements of $D$ . (i.e. the product ideal) Is it the case that $f_0\in D\cdot D$ ? If not is there a counterexample? It is easy to show that $f_0\in C\cdot D$ where $C$ is the set of continuous functions $f$ with $f(0)=0$ but I can't see if it is true in the restricted case. Any help or references would be appreciated","Honestly this is just a case of me not being great at constructing 'pathological' differentiable functions and I am not sure how to search for what I am looking for. This is related to an algebraic method of constructing cotangents that I have seen before, but it has been a long time so I don't remember the details. Let be the set of functions on which are differentiable at and have . Suppose has . Let be the set of finite sums of products of elements of . (i.e. the product ideal) Is it the case that ? If not is there a counterexample? It is easy to show that where is the set of continuous functions with but I can't see if it is true in the restricted case. Any help or references would be appreciated",D f \Bbb{R} 0 f(0)=0 f_0\in D f_0'(0)=0 D\cdot D D f_0\in D\cdot D f_0\in C\cdot D C f f(0)=0,"['real-analysis', 'derivatives']"
77,Determine at what points the complex function $f\left(z\right)=e^{2x}\cos3x+ie^{3x}\sin2y$ is differentiable.,Determine at what points the complex function  is differentiable.,f\left(z\right)=e^{2x}\cos3x+ie^{3x}\sin2y,"Determine at what points the complex function $$f(z)=e^{2x}\cos3y+ie^{3x}\sin2y$$ is differentiable. The function is differentiable at $z_0 \in \mathbb C$ if $f$ is defined on a neighborhood of $z_0$ contained in $D_f=\mathbb C$ (in this example this condition is satisfied) and $\lim_{z \to z_0}\frac{f\left(z\right)-f\left(z_{0}\right)}{z-z_{0}}$ exists, but here computing the limit is difficult, so what's the the alternative solution? And generally when we are asked to find all points that a specific function is differentiable at such points what should we do?","Determine at what points the complex function is differentiable. The function is differentiable at if is defined on a neighborhood of contained in (in this example this condition is satisfied) and exists, but here computing the limit is difficult, so what's the the alternative solution? And generally when we are asked to find all points that a specific function is differentiable at such points what should we do?",f(z)=e^{2x}\cos3y+ie^{3x}\sin2y z_0 \in \mathbb C f z_0 D_f=\mathbb C \lim_{z \to z_0}\frac{f\left(z\right)-f\left(z_{0}\right)}{z-z_{0}},"['complex-analysis', 'derivatives', 'complex-numbers', 'cauchy-riemann-equations']"
78,"What is this derivative, with a diagonal matrix?","What is this derivative, with a diagonal matrix?",,"Given matrices $\textbf{A}, \textbf{B}, \textbf{C}$ and column vector $\textbf{v}$ , what is the derivative of $\langle \textbf{A} \text{diag}(\textbf{B}\textbf{v}), \textbf{C} \rangle$ with respect to $\textbf{B}$ ? $\text{diag}(\cdot)$ is a diagonal matrix with the argument as the diagonal and the brackets signify the inner product. I am trying to find the solution, but am hung up on differentiating the diagonal matrix. Any assistance would be greatly appreciated. Thank you very much.","Given matrices and column vector , what is the derivative of with respect to ? is a diagonal matrix with the argument as the diagonal and the brackets signify the inner product. I am trying to find the solution, but am hung up on differentiating the diagonal matrix. Any assistance would be greatly appreciated. Thank you very much.","\textbf{A}, \textbf{B}, \textbf{C} \textbf{v} \langle \textbf{A} \text{diag}(\textbf{B}\textbf{v}), \textbf{C} \rangle \textbf{B} \text{diag}(\cdot)","['calculus', 'matrices', 'derivatives', 'vectors']"
79,"How many solutions does $f'(x)=1/2$ have, given that $|f(x)-f(y)|\ge|x-y|$?","How many solutions does  have, given that ?",f'(x)=1/2 |f(x)-f(y)|\ge|x-y|,"Let $F: \mathbb R \to \mathbb R$ be a continuously differentiable function such that $$|f(x)-f(y)| \ge |x-y|,\ \  \forall x,y \in \mathbb R$$ Then $ f '(x)=1/2$ has how many solutions? The way I approached this problem was like this, $$\frac{|f(x) - f(y)|}{|x-y|} \ge 1.$$ So this implies that the slope of this function is either $1$ or more than $1$ , over $\mathbb R$ . And the differential equation given has the solution $$y = x/2 + c$$ implying that it has a slope of $1/2$ . Is it wrong that I am taking the given function and the solution of the differential equation as two distinct graphs and then looking at their solution? Edit: As it turns out, I am indeed wrong. This was a much simpler problem and I complicated it. I cannot take 2 different functions because they are talking about the same function. Thanks for the comments and replies.","Let be a continuously differentiable function such that Then has how many solutions? The way I approached this problem was like this, So this implies that the slope of this function is either or more than , over . And the differential equation given has the solution implying that it has a slope of . Is it wrong that I am taking the given function and the solution of the differential equation as two distinct graphs and then looking at their solution? Edit: As it turns out, I am indeed wrong. This was a much simpler problem and I complicated it. I cannot take 2 different functions because they are talking about the same function. Thanks for the comments and replies.","F: \mathbb R \to \mathbb R |f(x)-f(y)| \ge |x-y|,\ \  \forall x,y \in \mathbb R  f '(x)=1/2 \frac{|f(x) - f(y)|}{|x-y|} \ge 1. 1 1 \mathbb R y = x/2 + c 1/2","['real-analysis', 'derivatives']"
80,Find the nth derivative of $\cos(x^3)$,Find the nth derivative of,\cos(x^3),"This is a Calculus exam task: Let $f : \Bbb R \to \Bbb R$ be a function defined as $f(x) = \cos(x^3)$ . Calculate $f^{(25)}(0)$ . Searching for an answer online only yielded answers to questions such as $\cos^3(x)$ or $\cos(3x)$ , but not my example. I've tried deriving the function twice thus getting the following differential equation: $$y'' = -6x\sin(x^3)-9x^4y$$ While I do get only a single recursion by this method, I get stuck with a $(n-3)^{\text{rd}}$ derivative of $\sin(x^3)$ after applying the Leibniz formula: $$y^{(n)}=-6\sum_{k=0}^{n-2}\binom{n-2}{k}x^{(k)}\left(\sin(x^3)\right)^{(n-2-k)}-9\sum_{k=0}^{n-2}\binom{n-2}{k}(x^4)^{(k)}y^{(n-2-k)}$$ $$\Rightarrow y^{(n)}(0)=-6(n-2)\left(\sin(x^3)\right)^{(n-3)} -9(n-2)(n-3)(n-4)(n-5)y^{(n-6)}(0)$$ I'm confused over the derivative operator's precedence. My question is whether it is legal math if I immediately plug $x=0$ as the sine's argument and then derive, thus only getting the following: $$y^{(n)}(0)\stackrel{?}{=}-9(n-2)(n-3)(n-4)(n-5)y^{(n-6)}(0)$$ If the above is true, then the given problem is fairly easy to finish: $$y^{(25)}(0) = (-9)^4\space\frac{23!}{19\cdot18\cdot13\cdot12\cdot7\cdot6}\space y'(0) = 0$$ However, something tells me that's not the way to go; otherwise all derivatives of anything may as well be 0, as they would be treated as constants when deriving. I did try to derive the second derivative a few more times, but the Leibniz formula then becomes a hot mess express. When I apply the formula over $y^{(4)}$ , which I found was the first one to have the sine function obscured under a $y$ variable, I get a multiple recursion by the means of $y^{(n)}=\ldots y^{(n-4)}+\ldots y^{(n-6)}+\ldots y^{(n-7)}$ Are there any other means of solving this task, perhaps by not using the Leibniz formula at all?","This is a Calculus exam task: Let be a function defined as . Calculate . Searching for an answer online only yielded answers to questions such as or , but not my example. I've tried deriving the function twice thus getting the following differential equation: While I do get only a single recursion by this method, I get stuck with a derivative of after applying the Leibniz formula: I'm confused over the derivative operator's precedence. My question is whether it is legal math if I immediately plug as the sine's argument and then derive, thus only getting the following: If the above is true, then the given problem is fairly easy to finish: However, something tells me that's not the way to go; otherwise all derivatives of anything may as well be 0, as they would be treated as constants when deriving. I did try to derive the second derivative a few more times, but the Leibniz formula then becomes a hot mess express. When I apply the formula over , which I found was the first one to have the sine function obscured under a variable, I get a multiple recursion by the means of Are there any other means of solving this task, perhaps by not using the Leibniz formula at all?",f : \Bbb R \to \Bbb R f(x) = \cos(x^3) f^{(25)}(0) \cos^3(x) \cos(3x) y'' = -6x\sin(x^3)-9x^4y (n-3)^{\text{rd}} \sin(x^3) y^{(n)}=-6\sum_{k=0}^{n-2}\binom{n-2}{k}x^{(k)}\left(\sin(x^3)\right)^{(n-2-k)}-9\sum_{k=0}^{n-2}\binom{n-2}{k}(x^4)^{(k)}y^{(n-2-k)} \Rightarrow y^{(n)}(0)=-6(n-2)\left(\sin(x^3)\right)^{(n-3)} -9(n-2)(n-3)(n-4)(n-5)y^{(n-6)}(0) x=0 y^{(n)}(0)\stackrel{?}{=}-9(n-2)(n-3)(n-4)(n-5)y^{(n-6)}(0) y^{(25)}(0) = (-9)^4\space\frac{23!}{19\cdot18\cdot13\cdot12\cdot7\cdot6}\space y'(0) = 0 y^{(4)} y y^{(n)}=\ldots y^{(n-4)}+\ldots y^{(n-6)}+\ldots y^{(n-7)},"['calculus', 'derivatives']"
81,Finding all $a$ such that $f(x)=\sin2x-8(a+1)\sin x+(4a^2+8a-14)x$ is increasing and has no critical points,Finding all  such that  is increasing and has no critical points,a f(x)=\sin2x-8(a+1)\sin x+(4a^2+8a-14)x,"Find the set of all values of the parameter $a$ for which the function, $$f(x)=\sin\left(2x\right)-8\left(a+1\right)\sin\left(x\right)+\left(4a^2+8a-14\right)x$$ increases for all $x\in\Bbb{R}$ and has no critical points for all $x\in\Bbb{R}$ . Obviously, the first thing I did was to find the derivative of this function and simplify it a bit and I got: $$f'(x)=4\left(\;\cos^2x-2\left(a+1\right)\cos x+\left(a^2+2a-4\right)\;\right)$$ But now how do I proceed further, had it been a simple quadratic in $x$ . I would've calculated $D<0$ but this is a quadratic in $\cos x$ . Can I do the same here? Why or why not? How should I go ahead? Not just this, there are many instances where the quadratic is not in x, but in expressions like $e^{x}$ . Is there any general approach to solving these quadratics for things like no solutions etc-","Find the set of all values of the parameter for which the function, increases for all and has no critical points for all . Obviously, the first thing I did was to find the derivative of this function and simplify it a bit and I got: But now how do I proceed further, had it been a simple quadratic in . I would've calculated but this is a quadratic in . Can I do the same here? Why or why not? How should I go ahead? Not just this, there are many instances where the quadratic is not in x, but in expressions like . Is there any general approach to solving these quadratics for things like no solutions etc-",a f(x)=\sin\left(2x\right)-8\left(a+1\right)\sin\left(x\right)+\left(4a^2+8a-14\right)x x\in\Bbb{R} x\in\Bbb{R} f'(x)=4\left(\;\cos^2x-2\left(a+1\right)\cos x+\left(a^2+2a-4\right)\;\right) x D<0 \cos x e^{x},"['derivatives', 'trigonometry', 'inequality', 'contest-math', 'quadratics']"
82,Question about partial derivative at a point.,Question about partial derivative at a point.,,"I'm currently studying Partial Derivatives and ran into this problem: $f(x,y) = \begin{cases} \frac{x^2}{x^2+y^4}, & (x,y) \neq (0,0) \\ 0, & (x,y) = (0,0) \end{cases}$ In this question, we are supposed to find $\frac{\partial f}{\partial x}$ at $(0,0)$ . (Sorry if this question seems very easy, but I'm new to the material). From what I know, we use the definition as follows: $\frac{\partial f}{\partial x}(x_0,y_0) = \lim_{\delta x \to 0} \frac{f(x_0+\delta x,y_0) - f(x_0,y_0)}{\delta x}$ . At $(0,0)$ we have $f(x,y) = 0.$ Therefore, $\lim_{\delta x \to 0} \frac{0 - 0}{\delta x} = 0$ . Is this correct? Or do we do the following: $\lim_{\delta x \to 0} \frac{\frac{\delta x^2}{\delta x^2 + 0^4}-\lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^4}}{\delta x}$ We can observe that $\lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^4}$ does not exist as it converges to different values at different paths (try $x = y$ and $x = y^2$ ). So, $\frac{\partial f}{\partial x}(0,0)$ does not exist. Now I believe the first solution is correct because the question says that at $(0,0)$ , the function is equal to zero and so we only consider the zero while finding the limit at that point. But I'm not 100% sure. I talked about this with other students and I found conflicting answers. Would it be possible for someone to tell me which answer is the correct one? A brief explanation would be very much appreciated. Thank you.","I'm currently studying Partial Derivatives and ran into this problem: In this question, we are supposed to find at . (Sorry if this question seems very easy, but I'm new to the material). From what I know, we use the definition as follows: . At we have Therefore, . Is this correct? Or do we do the following: We can observe that does not exist as it converges to different values at different paths (try and ). So, does not exist. Now I believe the first solution is correct because the question says that at , the function is equal to zero and so we only consider the zero while finding the limit at that point. But I'm not 100% sure. I talked about this with other students and I found conflicting answers. Would it be possible for someone to tell me which answer is the correct one? A brief explanation would be very much appreciated. Thank you.","f(x,y) = \begin{cases} \frac{x^2}{x^2+y^4}, & (x,y) \neq (0,0) \\ 0, & (x,y) = (0,0) \end{cases} \frac{\partial f}{\partial x} (0,0) \frac{\partial f}{\partial x}(x_0,y_0) = \lim_{\delta x \to 0} \frac{f(x_0+\delta x,y_0) - f(x_0,y_0)}{\delta x} (0,0) f(x,y) = 0. \lim_{\delta x \to 0} \frac{0 - 0}{\delta x} = 0 \lim_{\delta x \to 0} \frac{\frac{\delta x^2}{\delta x^2 + 0^4}-\lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^4}}{\delta x} \lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^4} x = y x = y^2 \frac{\partial f}{\partial x}(0,0) (0,0)","['calculus', 'limits', 'derivatives', 'differential']"
83,Periodicity of derivatives of function,Periodicity of derivatives of function,,"Can we find a function $f:\mathbb{R}\to\mathbb{R}$ , such that it's derivatives repeat after a certain period $T$ . For example, exponential function $f(x)=Ae^x$ satisfies $f(x)=f'(x)=f''(x)=\cdots$ and in this case we can call the period to be $1$ . Now, I also found a function such that $f(x) \ne f'(x) $ but $f(x)=f''(x)$ and it repeats at a period of $2$ . The example is $f(x)=Ae^x+Be^{-x}$ . However, I was unable to find a function whose period is $3$ . Although, for period $4$ , we have trigonometric functions, but I think it would be possible to find a class of functions, in which we can find such functions according to our wish by just varying $T$ . It seems we can manipulate exponential functions so that it is possible for any $T$ . Any ideas would be appreciated! I found this too: Functions that are their Own nth Derivatives for Real $n$ but I'm not sure if this is what I am looking for, because I am looking for real valued functions, and the answer given here gives functions in terms of $n$ th roots of unity.","Can we find a function , such that it's derivatives repeat after a certain period . For example, exponential function satisfies and in this case we can call the period to be . Now, I also found a function such that but and it repeats at a period of . The example is . However, I was unable to find a function whose period is . Although, for period , we have trigonometric functions, but I think it would be possible to find a class of functions, in which we can find such functions according to our wish by just varying . It seems we can manipulate exponential functions so that it is possible for any . Any ideas would be appreciated! I found this too: Functions that are their Own nth Derivatives for Real $n$ but I'm not sure if this is what I am looking for, because I am looking for real valued functions, and the answer given here gives functions in terms of th roots of unity.",f:\mathbb{R}\to\mathbb{R} T f(x)=Ae^x f(x)=f'(x)=f''(x)=\cdots 1 f(x) \ne f'(x)  f(x)=f''(x) 2 f(x)=Ae^x+Be^{-x} 3 4 T T n,"['calculus', 'derivatives', 'exponential-function']"
84,"Prove that $x^2|\sin(\frac1x)|$ is absolutely continuous on $[0,1]$",Prove that  is absolutely continuous on,"x^2|\sin(\frac1x)| [0,1]","Could you help me prove that $f = x^2|\sin(\frac1x)|$ is absolutely continuous on $[0,1]$ ? I tried to prove this with this: f has a derivative f′ almost everywhere, the derivative is Lebesgue integrable, and $f(x)=f(a)+\int _{a}^{x}f'(t)\,dt$ for all x on [a,b]. The derivative of the function is: $2x \mid (\sin(1/x)) \mid - \sin(\frac2x)/(2 \mid \sin\frac1x\mid )$ . But I am not sure how to proceed, since there are many pointswhere $f$ is not differentiable. Thank you in advance!","Could you help me prove that is absolutely continuous on ? I tried to prove this with this: f has a derivative f′ almost everywhere, the derivative is Lebesgue integrable, and for all x on [a,b]. The derivative of the function is: . But I am not sure how to proceed, since there are many pointswhere is not differentiable. Thank you in advance!","f = x^2|\sin(\frac1x)| [0,1] f(x)=f(a)+\int _{a}^{x}f'(t)\,dt 2x \mid (\sin(1/x)) \mid - \sin(\frac2x)/(2 \mid \sin\frac1x\mid ) f","['real-analysis', 'integration']"
85,Finding the $n^{\text{th}}$ derivative of a beautiful function,Finding the  derivative of a beautiful function,n^{\text{th}},"I have to find the $n^{\text{th}}$ derivative of the following function $$y= \frac{x+2}{\sqrt[3]{1-x}}$$ I tried taking the derivative for a couple of times to find some patterns but it didn't help. I feel like I have to use some formulas for common function's $n^{\text{th}}$ derivative, but in my function-as you can see- there are 2 types of functions so I don't know what i should do. Also I don't know any series yet, I don't know if that was necessary, just in case. Could you please help me? Thanks.","I have to find the derivative of the following function I tried taking the derivative for a couple of times to find some patterns but it didn't help. I feel like I have to use some formulas for common function's derivative, but in my function-as you can see- there are 2 types of functions so I don't know what i should do. Also I don't know any series yet, I don't know if that was necessary, just in case. Could you please help me? Thanks.",n^{\text{th}} y= \frac{x+2}{\sqrt[3]{1-x}} n^{\text{th}},"['real-analysis', 'calculus', 'analysis', 'derivatives']"
86,"I am confused on the last step of ""differentiability implies continuity""","I am confused on the last step of ""differentiability implies continuity""",,"I am watching this video of the proof "" differentiability implies continuity "" for functions $ f: U\subset \mathbb R \to \mathbb R $ , and I have doubts about the last step. In the proof it is concluded that $$ \lim_{t \to c } ~ (f(t) - f(c)) = 0 .$$ From here they use limit laws to rewrite the left side as a difference of limits. $$ \lim_{t \to c } f(t) - \lim_{t \to c }  f(c) = 0. $$ My question is, using the limit law is only valid provided we know apriori that $ \lim_{t \to c }f(t) $ and $ \lim_{t \to c } f(c) $ exist. It is easy to show that $ \lim_{t \to c } f(c) $ exists, since $\lim_{t \to c } f(c) = f(c)$ , but I don't see where it is given in the hypothesis that $ \lim_{t \to c }f(t) $ exists.","I am watching this video of the proof "" differentiability implies continuity "" for functions , and I have doubts about the last step. In the proof it is concluded that From here they use limit laws to rewrite the left side as a difference of limits. My question is, using the limit law is only valid provided we know apriori that and exist. It is easy to show that exists, since , but I don't see where it is given in the hypothesis that exists.", f: U\subset \mathbb R \to \mathbb R   \lim_{t \to c } ~ (f(t) - f(c)) = 0 .  \lim_{t \to c } f(t) - \lim_{t \to c }  f(c) = 0.   \lim_{t \to c }f(t)   \lim_{t \to c } f(c)   \lim_{t \to c } f(c)  \lim_{t \to c } f(c) = f(c)  \lim_{t \to c }f(t) ,"['calculus', 'derivatives', 'continuity']"
87,Derivative of $\mbox{sgn}$,Derivative of,\mbox{sgn},"I get a different result than the book I'm reading for the derivative of the sign function. Let's define the sign function, $x \mapsto \mbox{sgn}(x)$ , as $$ \mbox{sgn} (x) =  \begin{cases}        1 & x > 0 \\      -1 & x < 0 \\    \end{cases}$$ Let $u(x)$ be the Heaviside step and $\delta(x)$ be the Dirac delta. By definition, $u'(x) = \delta(x)$ and $\delta(-x) = \delta(x)$ . I can rewrite $\mbox{sgn}(x) = u(t) - u(-t)$ . Differentiating, we get $$\mbox{sgn}'(x) = u'(x) - u'(-x)$$ So far the book and I agree. Here is where we disagree. I then proceed to say $$\mbox{sgn}'(x) = \delta(x) - \delta(-x) = 0$$ whereas the book says $$\mbox{sgn}'(x) = \delta(x) - [-\delta(x)] = 2 \delta(t)$$ This implies that $u'(-t) = -\delta(t)$ which I don't agree with.","I get a different result than the book I'm reading for the derivative of the sign function. Let's define the sign function, , as Let be the Heaviside step and be the Dirac delta. By definition, and . I can rewrite . Differentiating, we get So far the book and I agree. Here is where we disagree. I then proceed to say whereas the book says This implies that which I don't agree with.","x \mapsto \mbox{sgn}(x)  \mbox{sgn} (x) =  \begin{cases} 
      1 & x > 0 \\
     -1 & x < 0 \\
   \end{cases} u(x) \delta(x) u'(x) = \delta(x) \delta(-x) = \delta(x) \mbox{sgn}(x) = u(t) - u(-t) \mbox{sgn}'(x) = u'(x) - u'(-x) \mbox{sgn}'(x) = \delta(x) - \delta(-x) = 0 \mbox{sgn}'(x) = \delta(x) - [-\delta(x)] = 2 \delta(t) u'(-t) = -\delta(t)","['calculus', 'derivatives', 'distribution-theory', 'dirac-delta', 'step-function']"
88,How to get an original function from the limit definition of a derivative?,How to get an original function from the limit definition of a derivative?,,"Say I have $$\lim_{h\to0} \frac{e^h-1}{h}$$ If $$\frac{d}{dx}(e^x)|_{x=0} = \lim_{h\to0} \frac{e^{0+h}-e^0}{h},$$ how would I “back engineer” the derivative limit definition to satisfy the expression meaning expression-I equals expression-II. But we’re only given expression-II the limit expression so how do we find the “mystery” expression-I from expression-II. I basically want to remove the guess work required to satisfy the two expressions, and if there’s even a general “algorithm” to follow?","Say I have If how would I “back engineer” the derivative limit definition to satisfy the expression meaning expression-I equals expression-II. But we’re only given expression-II the limit expression so how do we find the “mystery” expression-I from expression-II. I basically want to remove the guess work required to satisfy the two expressions, and if there’s even a general “algorithm” to follow?","\lim_{h\to0} \frac{e^h-1}{h} \frac{d}{dx}(e^x)|_{x=0} =
\lim_{h\to0} \frac{e^{0+h}-e^0}{h},","['calculus', 'limits', 'derivatives', 'algorithms', 'reverse-math']"
89,Relations between partial derivatives used in the Maxwell relations,Relations between partial derivatives used in the Maxwell relations,,"In the context of Maxwell's relations, there are two frequently used expressions relating the partial derivatives of different thermodynamic quantities: $$\left(\frac{\partial x}{\partial y}\right)_{z} \left(\frac{\partial y}{\partial z}\right)_{x} \left(\frac{\partial z}{\partial x}\right)_{y}= -1 \tag{1}$$ $$\left(\frac{\partial y}{\partial x}\right)_{z}=1 \bigg{/} \left(\frac{\partial x}{\partial y}\right)_{z} \tag{2}$$ What is the mathematical background of these two expressions? Is there a simple demonstration for them?","In the context of Maxwell's relations, there are two frequently used expressions relating the partial derivatives of different thermodynamic quantities: What is the mathematical background of these two expressions? Is there a simple demonstration for them?",\left(\frac{\partial x}{\partial y}\right)_{z} \left(\frac{\partial y}{\partial z}\right)_{x} \left(\frac{\partial z}{\partial x}\right)_{y}= -1 \tag{1} \left(\frac{\partial y}{\partial x}\right)_{z}=1 \bigg{/} \left(\frac{\partial x}{\partial y}\right)_{z} \tag{2},"['derivatives', 'calculus']"
90,Differentiability of $f(x)$ at $0$ provided that $\lim_{x\to0} [f(2x)-f(-x)]/x$ exists? [duplicate],Differentiability of  at  provided that  exists? [duplicate],f(x) 0 \lim_{x\to0} [f(2x)-f(-x)]/x,"This question already has answers here : Prove that $f'(0)$ exists and $f'(0) = b/(a - 1)$ (3 answers) Closed 3 years ago . Assume that $f:\mathbb{R}\to\mathbb{R}$ is continuous at $x=0$ , and the limit \begin{equation} \lim_{x\to0}\frac{f(2x)-f(-x)}{x} \end{equation} exists. Is $f$ necessarily differentiable at 0? I was able to deduce that if $f$ is continuous in a neighbordhood of $0$ and has an isolated zero at $x=0$ , it must be differentiable (unless I made an error). I believe that this result holds if there exists some $\delta>0$ such that $f(x)$ does not change sign in the neiborhoods $-\delta<x<0$ and $0<x<\delta$ . But for the life of me I can't figure out a general proof (or a counterexample). Any help would be greatly appreicated!","This question already has answers here : Prove that $f'(0)$ exists and $f'(0) = b/(a - 1)$ (3 answers) Closed 3 years ago . Assume that is continuous at , and the limit exists. Is necessarily differentiable at 0? I was able to deduce that if is continuous in a neighbordhood of and has an isolated zero at , it must be differentiable (unless I made an error). I believe that this result holds if there exists some such that does not change sign in the neiborhoods and . But for the life of me I can't figure out a general proof (or a counterexample). Any help would be greatly appreicated!","f:\mathbb{R}\to\mathbb{R} x=0 \begin{equation}
\lim_{x\to0}\frac{f(2x)-f(-x)}{x}
\end{equation} f f 0 x=0 \delta>0 f(x) -\delta<x<0 0<x<\delta","['real-analysis', 'calculus', 'derivatives']"
91,Differentiable at $x=a$ implies continuous at $x=a$,Differentiable at  implies continuous at,x=a x=a,"Consider the function $$f(x)=\left\{\begin{array}{cc} x^2-4 & \text{ if }x\leq 2\\4x+3&\text{ if } x\gt 2\end{array}\right.$$ This function is differentiable at $x=2$ since $\lim_{h\to 0^{\pm}}\frac{f(2+h)-f(2)}{h}=4$ (EDIT: this actually isn't true but it is true that $\lim_{x\to 2^-}f^\prime (x)=4=\lim_{x\to 2^+}f^\prime(x)$ ); however, it's not continuous at $x=2$ . How is that possible, doesn't differentiability at $x=a$ imply continuity at $x=a$ ? This question came up when I tried to answer the question of finding $a$ and $b$ such that the function $$f(x)=\left\{\begin{array}{cc} ax^2-b & \text{ if }x\leq 2\\bx+3&\text{ if } x\gt 2\end{array}\right.$$ The solution is achieved by finding conditions on $a$ and $b$ such that it's continuous, and also such that the left/right derivates exist. The left/right derivative question gives $4a=b$ . With the condition of continuity you get the additional condition that $4a-b=2b+3$ , giving a unique solution. But doesn't differentiability imply continuity? What's wrong with just solving $4a=b$ like in the first example above?","Consider the function This function is differentiable at since (EDIT: this actually isn't true but it is true that ); however, it's not continuous at . How is that possible, doesn't differentiability at imply continuity at ? This question came up when I tried to answer the question of finding and such that the function The solution is achieved by finding conditions on and such that it's continuous, and also such that the left/right derivates exist. The left/right derivative question gives . With the condition of continuity you get the additional condition that , giving a unique solution. But doesn't differentiability imply continuity? What's wrong with just solving like in the first example above?",f(x)=\left\{\begin{array}{cc} x^2-4 & \text{ if }x\leq 2\\4x+3&\text{ if } x\gt 2\end{array}\right. x=2 \lim_{h\to 0^{\pm}}\frac{f(2+h)-f(2)}{h}=4 \lim_{x\to 2^-}f^\prime (x)=4=\lim_{x\to 2^+}f^\prime(x) x=2 x=a x=a a b f(x)=\left\{\begin{array}{cc} ax^2-b & \text{ if }x\leq 2\\bx+3&\text{ if } x\gt 2\end{array}\right. a b 4a=b 4a-b=2b+3 4a=b,"['calculus', 'limits', 'derivatives', 'continuity']"
92,Hard limit $\lim_{x\to\infty}\Bigg((x(x+1)(x+2))^{\frac{1}{3}}-\Big(2(x+1)-\Big(x^x(x+1)^{x+1}(x+2)^{x+2}\Big)^{\frac{1}{3x+3}}\Big)\Bigg)=0$,Hard limit,\lim_{x\to\infty}\Bigg((x(x+1)(x+2))^{\frac{1}{3}}-\Big(2(x+1)-\Big(x^x(x+1)^{x+1}(x+2)^{x+2}\Big)^{\frac{1}{3x+3}}\Big)\Bigg)=0,Prove that : $$\lim_{x\to\infty}\Bigg((x(x+1)(x+2))^{\frac{1}{3}}-\Big(2(x+1)-\Big(x^x(x+1)^{x+1}(x+2)^{x+2}\Big)^{\frac{1}{3x+3}}\Big)\Bigg)=0$$ I can prove that : $$\lim_{x\to\infty}\frac{(x(x+1)(x+2))^{\frac{1}{3}}-2(x+1)}{-\Big(x^x(x+1)^{x+1}(x+2)^{x+2}\Big)^{\frac{1}{3x+3}}}=1$$ Using the Hospital rule but it doesn't help here .Moreover I have tried power series without success . Any helps is welcome . Thanks in advance,Prove that : I can prove that : Using the Hospital rule but it doesn't help here .Moreover I have tried power series without success . Any helps is welcome . Thanks in advance,\lim_{x\to\infty}\Bigg((x(x+1)(x+2))^{\frac{1}{3}}-\Big(2(x+1)-\Big(x^x(x+1)^{x+1}(x+2)^{x+2}\Big)^{\frac{1}{3x+3}}\Big)\Bigg)=0 \lim_{x\to\infty}\frac{(x(x+1)(x+2))^{\frac{1}{3}}-2(x+1)}{-\Big(x^x(x+1)^{x+1}(x+2)^{x+2}\Big)^{\frac{1}{3x+3}}}=1,"['limits', 'derivatives', 'power-series', 'exponentiation']"
93,"The proof of $\frac{f(a)+f(b)}{2}\le\frac{1}{b-a}\int^b_af(x)dx$ provided that $f''(x)<0,x \in [a,b]$ and $f\in \mathcal C^2[a,b]$",The proof of  provided that  and,"\frac{f(a)+f(b)}{2}\le\frac{1}{b-a}\int^b_af(x)dx f''(x)<0,x \in [a,b] f\in \mathcal C^2[a,b]","This question is geometrically obvious, but I got a little trouble when proving it. Let's consider the Taylor expansion of $F(x)=\int^x_af(x)\mathrm dx$ at $a$ and $b$ and subsitute $b,a$ into it respectively. $$\int_a^bf(x)\mathrm dx=F_a(b)=f(a)(b-a)+f'(a)(b-a)^2/2+f''(\xi_1)(b-a)^3/6\\0=F_b(a)=\int^b_af(x)\mathrm dx+f(b)(a-b)+f'(b)(a-b)^2/2+f''(\xi_2)(a-b)^3/6$$ And subtract the second from the first $$\int_a^bf(x)\mathrm dx=F_a(b)-F_b(a)=-\int_a^bf(x)\mathrm dx+(b-a)(f(a)-f(b))+(f'(a)-f'(b))(b-a)^2/2+(f''(\xi_1)+f''(\xi_2))(b-a)^3/6$$ And divide both side by 2(b-a) and subsitute $f'(a)-f'(b)=-f(\xi_3)(b-a)$ into it $$\frac{1}{b-a}\int_a^bf(x)\mathrm dx=\frac{f(a)-f(b)}2-f''(\xi_3)(b-a)^2/4+(f''(\xi_1)+f''(\xi_2))(b-a)^2/12$$ since $f''(\xi_3)<0$ we have $$\frac{1}{b-a}\int_a^bf(x)\mathrm dx\ge\frac{f(a)-f(b)}2+(f''(\xi_1)+f''(\xi_2))(b-a)^2/12$$ I guess my approximation is too rough so I get an additional term $(f''(\xi_1)+f''(\xi_2))(b-a)^2/12$ . Maybe the glitch is that I use $f''(x)<0$ just at one point i.e. $\xi_3$ but actually it holds at every point in $[a,b]$ which is a decisive condition.","This question is geometrically obvious, but I got a little trouble when proving it. Let's consider the Taylor expansion of at and and subsitute into it respectively. And subtract the second from the first And divide both side by 2(b-a) and subsitute into it since we have I guess my approximation is too rough so I get an additional term . Maybe the glitch is that I use just at one point i.e. but actually it holds at every point in which is a decisive condition.","F(x)=\int^x_af(x)\mathrm dx a b b,a \int_a^bf(x)\mathrm dx=F_a(b)=f(a)(b-a)+f'(a)(b-a)^2/2+f''(\xi_1)(b-a)^3/6\\0=F_b(a)=\int^b_af(x)\mathrm dx+f(b)(a-b)+f'(b)(a-b)^2/2+f''(\xi_2)(a-b)^3/6 \int_a^bf(x)\mathrm dx=F_a(b)-F_b(a)=-\int_a^bf(x)\mathrm dx+(b-a)(f(a)-f(b))+(f'(a)-f'(b))(b-a)^2/2+(f''(\xi_1)+f''(\xi_2))(b-a)^3/6 f'(a)-f'(b)=-f(\xi_3)(b-a) \frac{1}{b-a}\int_a^bf(x)\mathrm dx=\frac{f(a)-f(b)}2-f''(\xi_3)(b-a)^2/4+(f''(\xi_1)+f''(\xi_2))(b-a)^2/12 f''(\xi_3)<0 \frac{1}{b-a}\int_a^bf(x)\mathrm dx\ge\frac{f(a)-f(b)}2+(f''(\xi_1)+f''(\xi_2))(b-a)^2/12 (f''(\xi_1)+f''(\xi_2))(b-a)^2/12 f''(x)<0 \xi_3 [a,b]","['integration', 'derivatives', 'numerical-methods']"
94,Derivative of $\left | x-\left \lfloor x+1 \right \rfloor \right |$ at $x = 1.5$?,Derivative of  at ?,\left | x-\left \lfloor x+1 \right \rfloor \right | x = 1.5,"Q: If $f(x)=\left | x-\left \lfloor x+1 \right \rfloor \right |$ , where $\left \lfloor x \right \rfloor$ denotes the greatest integer less than or equal to x and $\left | x \right |$ denotes the absolute value of x, then $f'(1.5)$ = I am not quite sure how the derivative of floor function and absolute function. I did some research and found out that the derivative of an absolute function is $\frac{\left \lfloor x \right \rfloor}{x}$ . But I am stuck with the floor function. What concept should I be aware of to solve this question?","Q: If , where denotes the greatest integer less than or equal to x and denotes the absolute value of x, then = I am not quite sure how the derivative of floor function and absolute function. I did some research and found out that the derivative of an absolute function is . But I am stuck with the floor function. What concept should I be aware of to solve this question?",f(x)=\left | x-\left \lfloor x+1 \right \rfloor \right | \left \lfloor x \right \rfloor \left | x \right | f'(1.5) \frac{\left \lfloor x \right \rfloor}{x},"['calculus', 'derivatives', 'absolute-value', 'ceiling-and-floor-functions']"
95,Is this argument in the proof of differentiability implying continuity valid?,Is this argument in the proof of differentiability implying continuity valid?,,"This is meant to be a very straightforward proof from an elementary analysis text, but I am not sure if the justification given in the proof is correct. The statement to prove is as follows: Let $f$ be defined on an open interval $I$ , and let $c \in I$ . If $f$ is differentiable at $c$ , then $f$ is also continuous at $c$ . The proof given is as follows: If $f$ is differentiable at $c$ , then there is some number $f'(c)$ such that $$\lim_{x\rightarrow c} \frac{f(x)-f(c)}{x-c} = f'(c)$$ It follows that $$ \begin{align} \lim_{x\rightarrow c} (f(x) - f(c)) &= \lim_{x\rightarrow c}\biggl(\frac{f(x)-f(c)}{(x-c)}(x-c)\biggl) \\ & = \lim_{x\rightarrow c} \frac{f(x)-f(c)}{(x-c)} \times \lim_{x\rightarrow c}{(x-c)}\\ &= f'(c) \times 0 \\ &= 0 \end{align}$$ Hence by the sum and multiple rules* for limits, $$\lim_{x \rightarrow c} f(x) = f(c)$$ This starred * comment refers to an earlier proved results about limits of functions which states that IF $\lim_{x \rightarrow c} f(x) = l$ and $\lim_{x \rightarrow c} g(x) = m$ , then $\lim_{x \rightarrow c} (f(x)-g(x)) = l - m$ Am I correct in thinking that using this result is invalid? For example, how do we know a priori that $\lim_{x \rightarrow c} f(x)$ exists? (If it exists then we can apply the limit rule, but before proving this I don't see how we can use the converse of the limit rule, which doesn't seem to be true to me?) My reasoning to complete the proof would be to use the sequential definition of a limit of a function: Since $\lim_{x\rightarrow c} (f(x) - f(c)) = 0$ , we have that for an arbitrary sequence $\{x_n\}$ such that $x_n$ converges to $c$ , the sequence $\{f(x_n) - f(c)\}$ converges to $0$ (by definition). Since $f(c)$ is a constant, we can conclude that the sequence $\{f(x_n)\}$ converges to $f(c)$ . THEN , since $\{x_n\}$ was arbitrary, this is true for every sequence converging to $c$ , and by the sequential definition of limits of a function, we can conclude that $\lim_{x \rightarrow c} f(x) = f(c)$ Is my previous paragraph correct or is it pedantry? I can see the argument seems very similar, but I am trying to be as precise as possible, and I can't quite see how using the limit rule actually applies in this case (at least not without additional reasoning).","This is meant to be a very straightforward proof from an elementary analysis text, but I am not sure if the justification given in the proof is correct. The statement to prove is as follows: Let be defined on an open interval , and let . If is differentiable at , then is also continuous at . The proof given is as follows: If is differentiable at , then there is some number such that It follows that Hence by the sum and multiple rules* for limits, This starred * comment refers to an earlier proved results about limits of functions which states that IF and , then Am I correct in thinking that using this result is invalid? For example, how do we know a priori that exists? (If it exists then we can apply the limit rule, but before proving this I don't see how we can use the converse of the limit rule, which doesn't seem to be true to me?) My reasoning to complete the proof would be to use the sequential definition of a limit of a function: Since , we have that for an arbitrary sequence such that converges to , the sequence converges to (by definition). Since is a constant, we can conclude that the sequence converges to . THEN , since was arbitrary, this is true for every sequence converging to , and by the sequential definition of limits of a function, we can conclude that Is my previous paragraph correct or is it pedantry? I can see the argument seems very similar, but I am trying to be as precise as possible, and I can't quite see how using the limit rule actually applies in this case (at least not without additional reasoning).",f I c \in I f c f c f c f'(c) \lim_{x\rightarrow c} \frac{f(x)-f(c)}{x-c} = f'(c)  \begin{align} \lim_{x\rightarrow c} (f(x) - f(c)) &= \lim_{x\rightarrow c}\biggl(\frac{f(x)-f(c)}{(x-c)}(x-c)\biggl) \\ & = \lim_{x\rightarrow c} \frac{f(x)-f(c)}{(x-c)} \times \lim_{x\rightarrow c}{(x-c)}\\ &= f'(c) \times 0 \\ &= 0 \end{align} \lim_{x \rightarrow c} f(x) = f(c) \lim_{x \rightarrow c} f(x) = l \lim_{x \rightarrow c} g(x) = m \lim_{x \rightarrow c} (f(x)-g(x)) = l - m \lim_{x \rightarrow c} f(x) \lim_{x\rightarrow c} (f(x) - f(c)) = 0 \{x_n\} x_n c \{f(x_n) - f(c)\} 0 f(c) \{f(x_n)\} f(c) \{x_n\} c \lim_{x \rightarrow c} f(x) = f(c),"['real-analysis', 'limits', 'derivatives', 'continuity']"
96,"If $f(x)=\cosh(x)+\sinh(x^2)$, what is$f^{(34)}(0)$?","If , what is?",f(x)=\cosh(x)+\sinh(x^2) f^{(34)}(0),"If $f(x)=\cosh(x)+\sinh(x^2)$ , what is $f^{(34)}(0)$ ? I know that: $h(t)=\sinh(t)=\left(\frac{e^t-e^{-t}}{2}\right)$ and $g(t)=\cosh(t)=\left(\frac{e^t+e^{-t}}{2}\right)$ So, I noticed this: $g'(t)=\sinh(t)$ $g''(t)=\cosh(t)$ Then: $g^{(34)}(t)=\cosh(t) \rightarrow g^{(34)} (0)= \cosh(0)=1 $ With the same idea, I know that $h^{(34)}(0)=\sinh(0)=0$ The problem that I am having is that I don't know what is the $34^{th}$ derivative of $\sinh(x^2)$ , can I   use $\sinh(x)$ somehow? Also, I know that: $\sinh(x)= \sum_{k=0}^{n} \frac{(x^2)^{1+2k}}{(1+2k)!}$","If , what is ? I know that: and So, I noticed this: Then: With the same idea, I know that The problem that I am having is that I don't know what is the derivative of , can I   use somehow? Also, I know that:",f(x)=\cosh(x)+\sinh(x^2) f^{(34)}(0) h(t)=\sinh(t)=\left(\frac{e^t-e^{-t}}{2}\right) g(t)=\cosh(t)=\left(\frac{e^t+e^{-t}}{2}\right) g'(t)=\sinh(t) g''(t)=\cosh(t) g^{(34)}(t)=\cosh(t) \rightarrow g^{(34)} (0)= \cosh(0)=1  h^{(34)}(0)=\sinh(0)=0 34^{th} \sinh(x^2) \sinh(x) \sinh(x)= \sum_{k=0}^{n} \frac{(x^2)^{1+2k}}{(1+2k)!},"['calculus', 'derivatives', 'taylor-expansion', 'hyperbolic-functions']"
97,Prove or disprove Average rate of change,Prove or disprove Average rate of change,,"Prove or disprove For any Real valued function $f$ whose domain is all real numbers and continuous on $\mathbb{R}$ there is no function $f$ exist satisfy the following condition: If $[a,b]$ and $[c,d]$ are two distinict intervals then the average rate of change on $[a,b]$ which is defined as $\frac{f(b)-f(a)}{b-a} $ doesn't equal average rate of change on $[c,d]$ . i.e. a function $f$ is continuous on $\mathbb{R}$ whose average rates of change are all distinct",Prove or disprove For any Real valued function whose domain is all real numbers and continuous on there is no function exist satisfy the following condition: If and are two distinict intervals then the average rate of change on which is defined as doesn't equal average rate of change on . i.e. a function is continuous on whose average rates of change are all distinct,"f \mathbb{R} f [a,b] [c,d] [a,b] \frac{f(b)-f(a)}{b-a}  [c,d] f \mathbb{R}","['real-analysis', 'calculus', 'derivatives']"
98,Properties of functions of mean zero,Properties of functions of mean zero,,"Let $f,g: \mathbb{R} \longrightarrow \mathbb{R}$ be differentiable functions and $a<b$ such that $$\frac{1}{b-a}\int_{a}^{b}f(x)\;dx=0 \quad \text{and} \quad \frac{1}{b-a}\int_{a}^{b}g(x)\;dx=0 \tag{1}.$$ So, I think that I can conclude that $$\int_{a}^{b}f'(x)\;dx=0 \tag{2}$$ Moreover, I can conclude that $$\int_{a}^{b}f'(x)\;dx=0 \Rightarrow f(b)-f(a)=0? \tag{3}$$ And $$g(b)\cdot f'(b)-g(a)\cdot f'(a)=0? \tag{4}$$ I ask this because I would like to conclude that $$g(x)\cdot f'(x)\Bigg|_{a}^{b} -\int_{a}^{b}f'(x)g'(x)\;dx=-\int_{a}^{b}f'(x)g'(x)\;dx. \tag{5}$$ These statements are in general true?","Let be differentiable functions and such that So, I think that I can conclude that Moreover, I can conclude that And I ask this because I would like to conclude that These statements are in general true?","f,g: \mathbb{R} \longrightarrow \mathbb{R} a<b \frac{1}{b-a}\int_{a}^{b}f(x)\;dx=0 \quad \text{and} \quad \frac{1}{b-a}\int_{a}^{b}g(x)\;dx=0 \tag{1}. \int_{a}^{b}f'(x)\;dx=0 \tag{2} \int_{a}^{b}f'(x)\;dx=0 \Rightarrow f(b)-f(a)=0? \tag{3} g(b)\cdot f'(b)-g(a)\cdot f'(a)=0? \tag{4} g(x)\cdot f'(x)\Bigg|_{a}^{b} -\int_{a}^{b}f'(x)g'(x)\;dx=-\int_{a}^{b}f'(x)g'(x)\;dx. \tag{5}","['real-analysis', 'integration', 'derivatives', 'definite-integrals']"
99,The Newton-Raphson Method for finding a correct monthly interest rate,The Newton-Raphson Method for finding a correct monthly interest rate,,"I am very new to this topic and just started to learn about this method. Trying to understand this method intuitively. In this document I found and interesting real life question: A loan of $A$ dollars is repaid by making $n$ equal monthly payments of $M$ dollars, starting a month after the loan is made. It can be shown that if the monthly interest rate is $r$ , then $$Ar=M\left(1-\frac1{(1+r)^n}\right).$$ A car loan of $10000$ dollars was repaid in $60$ monthly payments of $250$ dollars. Use the Newton Method to find the monthly interest rate correct to $4$ significant figures. Can somebody please explain intuitively why do we use this method in real life? If I understood correctly so far, we can make a guess and then find a very close number to the real answer, using this method. Appreciate your time and other interesting examples, ideally with a code example in R/Python. Thanks!","I am very new to this topic and just started to learn about this method. Trying to understand this method intuitively. In this document I found and interesting real life question: A loan of dollars is repaid by making equal monthly payments of dollars, starting a month after the loan is made. It can be shown that if the monthly interest rate is , then A car loan of dollars was repaid in monthly payments of dollars. Use the Newton Method to find the monthly interest rate correct to significant figures. Can somebody please explain intuitively why do we use this method in real life? If I understood correctly so far, we can make a guess and then find a very close number to the real answer, using this method. Appreciate your time and other interesting examples, ideally with a code example in R/Python. Thanks!",A n M r Ar=M\left(1-\frac1{(1+r)^n}\right). 10000 60 250 4,"['calculus', 'derivatives', 'newton-raphson']"
