,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Order of partial derivative in second derivative test?,Order of partial derivative in second derivative test?,,This is from wikipedia for the second derivative Hessian matrix test. From the determinant it seems to assume that $f_{xy} = f_{yx}.$ Why is this valid to assume? Is the test only valid for when $f_{xy} = f_{yx}?$,This is from wikipedia for the second derivative Hessian matrix test. From the determinant it seems to assume that Why is this valid to assume? Is the test only valid for when,f_{xy} = f_{yx}. f_{xy} = f_{yx}?,['multivariable-calculus']
1,"Prove that $\iint_S \vec{r}\cdot \vec{n}\,\mathrm{d}S=3 V(A)$",Prove that,"\iint_S \vec{r}\cdot \vec{n}\,\mathrm{d}S=3 V(A)","Let $S$ be a closed regular surface and $\vec{n}$ a vector field of unit normal vectors to the surface. Prove that the flux of the vector field $\vec{F}=\vec{r}$ is equal to $3\, V(A),$ where $V(A)$ is the volume of the interior $A$ of $S$ . I am looking for a proof that $\displaystyle \iint_S \vec{r}\cdot \vec{n}\,\mathrm{d}S= 3 V(A)$ , without use of Gauss divergence theorem. So far I can say that $$\iint_S \vec{r}\cdot \vec{n}\,\mathrm{d}S=\iint_D (\vec{r},\vec{r}_u,\vec{r}_v)\,\mathrm{d}u\,\mathrm{d}v,$$ where $(\vec{r},\vec{r}_u,\vec{r}_v)$ stands for the triple product of $\vec{r},\,\vec{r}_u,\,\vec{r}_v$ . Any ideas are welcomed. Thanks in advance for the help.","Let be a closed regular surface and a vector field of unit normal vectors to the surface. Prove that the flux of the vector field is equal to where is the volume of the interior of . I am looking for a proof that , without use of Gauss divergence theorem. So far I can say that where stands for the triple product of . Any ideas are welcomed. Thanks in advance for the help.","S \vec{n} \vec{F}=\vec{r} 3\, V(A), V(A) A S \displaystyle \iint_S \vec{r}\cdot \vec{n}\,\mathrm{d}S= 3 V(A) \iint_S \vec{r}\cdot \vec{n}\,\mathrm{d}S=\iint_D (\vec{r},\vec{r}_u,\vec{r}_v)\,\mathrm{d}u\,\mathrm{d}v, (\vec{r},\vec{r}_u,\vec{r}_v) \vec{r},\,\vec{r}_u,\,\vec{r}_v","['multivariable-calculus', 'surface-integrals', 'multiple-integral']"
2,Conflicting definition of the Hessian matrix: does the order of the partials of a Hessian matrix matter?,Conflicting definition of the Hessian matrix: does the order of the partials of a Hessian matrix matter?,,"On Wikipedia, the Hessian matrix is defined as, https://en.wikipedia.org/wiki/Hessian_matrix $ {\displaystyle \mathbf {H} ={\begin{bmatrix}{\dfrac {\partial ^{2}f}{\partial x_{1}^{2}}}&{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial x_{n}}}\\[2.2ex]{\dfrac {\partial ^{2}f}{\partial x_{2}\,\partial x_{1}}}&{\dfrac {\partial ^{2}f}{\partial x_{2}^{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{2}\,\partial x_{n}}}\\[2.2ex]\vdots &\vdots &\ddots &\vdots \\[2.2ex]{\dfrac {\partial ^{2}f}{\partial x_{n}\,\partial x_{1}}}&{\dfrac {\partial ^{2}f}{\partial x_{n}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{n}^{2}}}\end{bmatrix}}.}$ However, in the textbook ""Optimization"" by Chong and Zak, it is written, This is different! Or is it? Is there any problem if I exchanged the order of the partial derivative for all the off-diagonal terms? Which definition is correct?","On Wikipedia, the Hessian matrix is defined as, https://en.wikipedia.org/wiki/Hessian_matrix However, in the textbook ""Optimization"" by Chong and Zak, it is written, This is different! Or is it? Is there any problem if I exchanged the order of the partial derivative for all the off-diagonal terms? Which definition is correct?","
{\displaystyle \mathbf {H} ={\begin{bmatrix}{\dfrac {\partial ^{2}f}{\partial x_{1}^{2}}}&{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial x_{n}}}\\[2.2ex]{\dfrac {\partial ^{2}f}{\partial x_{2}\,\partial x_{1}}}&{\dfrac {\partial ^{2}f}{\partial x_{2}^{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{2}\,\partial x_{n}}}\\[2.2ex]\vdots &\vdots &\ddots &\vdots \\[2.2ex]{\dfrac {\partial ^{2}f}{\partial x_{n}\,\partial x_{1}}}&{\dfrac {\partial ^{2}f}{\partial x_{n}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{n}^{2}}}\end{bmatrix}}.}","['real-analysis', 'multivariable-calculus', 'optimization', 'definition', 'hessian-matrix']"
3,Is divergence theorem applicable for improper integrals?,Is divergence theorem applicable for improper integrals?,,"Consider the expression: $$\lim\limits_{\delta \to 0} \iiint_{V-\delta} \nabla \cdot \mathbf{F}(x,y,z)\ dV \tag1$$ where $\delta$ is a small volume inside volume $V$ Now can we apply the divergence theorem and write $(1)$ as: $$\iint_{\partial V} \mathbf{F}(x,y,z) \cdot \hat{n}\ dS +\lim\limits_{\delta \to 0} \iint_{\partial \delta} \mathbf{F}(x,y,z) \cdot \hat{n}\ dS$$",Consider the expression: where is a small volume inside volume Now can we apply the divergence theorem and write as:,"\lim\limits_{\delta \to 0} \iiint_{V-\delta} \nabla \cdot \mathbf{F}(x,y,z)\ dV \tag1 \delta V (1) \iint_{\partial V} \mathbf{F}(x,y,z) \cdot \hat{n}\ dS
+\lim\limits_{\delta \to 0} \iint_{\partial \delta} \mathbf{F}(x,y,z) \cdot \hat{n}\ dS","['calculus', 'multivariable-calculus', 'definite-integrals', 'improper-integrals', 'divergence-operator']"
4,Local Inversion theorem (first sentence of the proof),Local Inversion theorem (first sentence of the proof),,"In this local inversion theorem I want to prove that a map f between two Banach spaces E and F is a local diffeomorphism. In the proof it says : Without loss of generality we can consider the case where E= F. Sorry for this simple question, but why is that? How does this imply the general case where f goes from an open set U in E to F? In finite dimensions we could probably have an embedding of E in F or something similar...but I'm not sure how to justify this for general Banach spaces...","In this local inversion theorem I want to prove that a map f between two Banach spaces E and F is a local diffeomorphism. In the proof it says : Without loss of generality we can consider the case where E= F. Sorry for this simple question, but why is that? How does this imply the general case where f goes from an open set U in E to F? In finite dimensions we could probably have an embedding of E in F or something similar...but I'm not sure how to justify this for general Banach spaces...",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
5,Find a simple and smooth curve $C$ such that $\displaystyle \int_C\vec{F}\cdot d\vec{r}$ gets its maximum value,Find a simple and smooth curve  such that  gets its maximum value,C \displaystyle \int_C\vec{F}\cdot d\vec{r},"I've been trying to solve this problem for a while, but for too long couldn't I continue my partial solution. I would be glad if you could shed some light on my solution. The task : Given the vector field $\vec{F}(x,y)\equiv(P(x,y),Q(x,y))$ such that $P(x,y)=y^3-3y+xy^2$ and $Q(x,y)=3x-x^3+x^2y$ , which is defined on $D=\{(x,y)\ |\ x^2+y^2\leq2\}$ , find a simple and smooth curve $C$ from $A(1,1)$ to $B(-1,-1)$ which is inside $D$ such that: $$\int_C\vec{F}\cdot d\vec{r}$$ gets its maximum value. My solution Let $C$ and $C_0$ be two curves that satisfy the requirements of the question, such that they are the boundaries of a closed area $S\subseteq D$ . We will choose $C$ to be positively oriented, whereas $C_0$ will be negatively oriented. Now, as $\vec{F}\in C^1$ , we are able to use Green's Theorem . We can see that: $$\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}=6-3(x^2+y^2)$$ So according to the theorem, defining $\Gamma\equiv C\ \cup\ C_0$ : $$\oint_{\Gamma}\vec{F}\cdot d\vec{r}=\int_C\vec{F}\cdot d\vec{r}-\int_{C_0}\vec{F}\cdot d\vec{r}=\iint_Sr(6-3r^2)drd\theta$$ Accordingly: $$\int_C\vec{F}\cdot d\vec{r}=\int_{C_0}\vec{F}\cdot d\vec{r}+\iint_Sr(6-3r^2)drd\theta$$ Since $\displaystyle \iint_Sr(6-3r^2)drd\theta$ is always positive inside $D$ $(0\leq r\leq\sqrt2)$ , the obvious would be to choose $C_0$ such that $S$ is the biggest area we can fit inside $D$ . The problem is, that this might cause $\displaystyle \int_{C_0}\vec{F}\cdot d\vec{r}$ to be relatively small. That is where I got stuck. P.S.: I also looked at the field, geometrically. It seems that inside $r=\sqrt2$ it looks different than the outside. I bet that's a thing I should take into account, but I don't know how. Thanks!","I've been trying to solve this problem for a while, but for too long couldn't I continue my partial solution. I would be glad if you could shed some light on my solution. The task : Given the vector field such that and , which is defined on , find a simple and smooth curve from to which is inside such that: gets its maximum value. My solution Let and be two curves that satisfy the requirements of the question, such that they are the boundaries of a closed area . We will choose to be positively oriented, whereas will be negatively oriented. Now, as , we are able to use Green's Theorem . We can see that: So according to the theorem, defining : Accordingly: Since is always positive inside , the obvious would be to choose such that is the biggest area we can fit inside . The problem is, that this might cause to be relatively small. That is where I got stuck. P.S.: I also looked at the field, geometrically. It seems that inside it looks different than the outside. I bet that's a thing I should take into account, but I don't know how. Thanks!","\vec{F}(x,y)\equiv(P(x,y),Q(x,y)) P(x,y)=y^3-3y+xy^2 Q(x,y)=3x-x^3+x^2y D=\{(x,y)\ |\ x^2+y^2\leq2\} C A(1,1) B(-1,-1) D \int_C\vec{F}\cdot d\vec{r} C C_0 S\subseteq D C C_0 \vec{F}\in C^1 \frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}=6-3(x^2+y^2) \Gamma\equiv C\ \cup\ C_0 \oint_{\Gamma}\vec{F}\cdot d\vec{r}=\int_C\vec{F}\cdot d\vec{r}-\int_{C_0}\vec{F}\cdot d\vec{r}=\iint_Sr(6-3r^2)drd\theta \int_C\vec{F}\cdot d\vec{r}=\int_{C_0}\vec{F}\cdot d\vec{r}+\iint_Sr(6-3r^2)drd\theta \displaystyle \iint_Sr(6-3r^2)drd\theta D (0\leq r\leq\sqrt2) C_0 S D \displaystyle \int_{C_0}\vec{F}\cdot d\vec{r} r=\sqrt2","['integration', 'multivariable-calculus', 'greens-theorem']"
6,Proving monotone function of two variables is integrable,Proving monotone function of two variables is integrable,,"Let $f:[0,1]^2\rightarrow \mathbb R$ be a monotone function of two variables, that is, $x\leq x'$ and $y\leq y' \implies f(x,y)\leq f(x',y').$ Prove that $f$ is Riemann integrable. I want to ""copy"" and generalize the argument for the one dimensional case. Well, what I tried so far was to consider the partition $P=\{P_{ij}: i,j = 1,\cdots, N\}, N\in \mathbb N, $ given by $P_{ij} = (\frac{i-1}{N},\frac{i}{N})\times (\frac{j-1}{N},\frac{j}{N})$ . This is a partition of the square by small squares of area $1/N^2$ . As in the one dimensional case, I want the sum $R(f,P)-L(f,P)$ to telescope and be something like: $\frac{f(1,1)-f(0,0)}{N}$ where $f(1,1)$ and $f(0,0)$ , as we may notice, is the maximum and minimum of the function on the square. But, it doesn't seem that this sum will be telescoping, because for each small square, its maximum and minimum of the function is reached at the opposed diagonal vertices (on the right) and will always remain diagonal vertices which will not ""kill each other"". Is this the right approach? Any hint on how to prove this?","Let be a monotone function of two variables, that is, and Prove that is Riemann integrable. I want to ""copy"" and generalize the argument for the one dimensional case. Well, what I tried so far was to consider the partition given by . This is a partition of the square by small squares of area . As in the one dimensional case, I want the sum to telescope and be something like: where and , as we may notice, is the maximum and minimum of the function on the square. But, it doesn't seem that this sum will be telescoping, because for each small square, its maximum and minimum of the function is reached at the opposed diagonal vertices (on the right) and will always remain diagonal vertices which will not ""kill each other"". Is this the right approach? Any hint on how to prove this?","f:[0,1]^2\rightarrow \mathbb R x\leq x' y\leq y' \implies f(x,y)\leq f(x',y'). f P=\{P_{ij}: i,j = 1,\cdots, N\}, N\in \mathbb N,  P_{ij} = (\frac{i-1}{N},\frac{i}{N})\times (\frac{j-1}{N},\frac{j}{N}) 1/N^2 R(f,P)-L(f,P) \frac{f(1,1)-f(0,0)}{N} f(1,1) f(0,0)","['integration', 'multivariable-calculus']"
7,Lagrange multipliers - confused about when the constraint set has boundary points that need to be considered,Lagrange multipliers - confused about when the constraint set has boundary points that need to be considered,,"Consider the constraint $$S_1 = \{(x, y) \; |\; \sqrt{x} + \sqrt{y} = 1 \}$$ How to use Lagrange Multipliers, when the constraint surface has a boundary? In this case, after the Lagrange multiplier method gives candidates for maxima/minima, we need to check the ""boundary points"" of $S_1$ , namely, $(1,0)$ and $(0,1)$ to get the global max/min. I can see that these two are ""boundary points"" intuitively when I plot the curve. However, instead if the constraint set be $$S_2 = \{ (x, y) \; |\; x^2 + y^2 = 1\},$$ then in this question, one answer states that  for this constraint set, there is no ""boundary point"". Constrained Extrema: How to find end points of multivariable functions for global extrema The only difference I see is that  pictorially, one is a closed curve, but the other is not. However, I am unable to see what is the mathematical definition that will  allow me to conclude that $S_1$ has boundary points $(0, 1)$ and $(1,0)$ and $S_2$ has none? Q) What is the definition of ""end point"" or ""boundary point"" being used here that explains both $S_1$ , $S_2$ .","Consider the constraint How to use Lagrange Multipliers, when the constraint surface has a boundary? In this case, after the Lagrange multiplier method gives candidates for maxima/minima, we need to check the ""boundary points"" of , namely, and to get the global max/min. I can see that these two are ""boundary points"" intuitively when I plot the curve. However, instead if the constraint set be then in this question, one answer states that  for this constraint set, there is no ""boundary point"". Constrained Extrema: How to find end points of multivariable functions for global extrema The only difference I see is that  pictorially, one is a closed curve, but the other is not. However, I am unable to see what is the mathematical definition that will  allow me to conclude that has boundary points and and has none? Q) What is the definition of ""end point"" or ""boundary point"" being used here that explains both , .","S_1 = \{(x, y) \; |\; \sqrt{x} + \sqrt{y} = 1 \} S_1 (1,0) (0,1) S_2 = \{ (x, y) \; |\; x^2 + y^2 = 1\}, S_1 (0, 1) (1,0) S_2 S_1 S_2","['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'constraints']"
8,Using $|r|$ versus using $r$ for double integration,Using  versus using  for double integration,|r| r,"Suppose that I have a double integral where the integrand has variables $x$ and $y$ , and I am using the polar substitution $x=r\cos(\theta)$ and $y=r\sin(\theta)$ . Suppose the region of integration is $x^2 + y^2 \leq 1$ . By calculating the Jacobian $J = |r|$ I can replace the $dxdy$ by $|r|drd\theta$ . I understand why the limits for $\theta$ will be $0$ to $2\pi$ . Here is where I am a bit confused. I know that $x^2 + y^2 = r^2 \leq 1 \implies -1 \leq r \leq 1$ , and so this suggests $-1$ and $1$ will be the limits for $r$ . But if I say that $r$ is always positive (thinking of it as the 'radius'), then it ranges from $0$ to $1$ , and I can also say that the Jacobian $J = r$ instead of using the modulus. This is the approach I see people take when they do these kinds of integrals. So, does the value of the integral change depending on whether I use $J = r$ and limits $0 \leq r \leq 1$ , or use $J = |r|$ and limits $-1 \leq r \leq 1$ ? If it does change (and I am guessing that the non-modulus way is correct) then why is this so? Whilst I get that if I think about it intuitively, the 'radius' is always positive, I still don't see what is mathematically wrong about using $|r|$ and the $-1 \leq r \leq 1$ limits instead.","Suppose that I have a double integral where the integrand has variables and , and I am using the polar substitution and . Suppose the region of integration is . By calculating the Jacobian I can replace the by . I understand why the limits for will be to . Here is where I am a bit confused. I know that , and so this suggests and will be the limits for . But if I say that is always positive (thinking of it as the 'radius'), then it ranges from to , and I can also say that the Jacobian instead of using the modulus. This is the approach I see people take when they do these kinds of integrals. So, does the value of the integral change depending on whether I use and limits , or use and limits ? If it does change (and I am guessing that the non-modulus way is correct) then why is this so? Whilst I get that if I think about it intuitively, the 'radius' is always positive, I still don't see what is mathematically wrong about using and the limits instead.",x y x=r\cos(\theta) y=r\sin(\theta) x^2 + y^2 \leq 1 J = |r| dxdy |r|drd\theta \theta 0 2\pi x^2 + y^2 = r^2 \leq 1 \implies -1 \leq r \leq 1 -1 1 r r 0 1 J = r J = r 0 \leq r \leq 1 J = |r| -1 \leq r \leq 1 |r| -1 \leq r \leq 1,"['integration', 'multivariable-calculus', 'polar-coordinates']"
9,"Find extreme values for $f(x,y,z)=xyz$ given the constraints $g_{1}(x,y,z)=x+y+z-5$ and $g_{2}(x,y,z)=xy+yz+zx-8$ using Lagrange multipliers method.",Find extreme values for  given the constraints  and  using Lagrange multipliers method.,"f(x,y,z)=xyz g_{1}(x,y,z)=x+y+z-5 g_{2}(x,y,z)=xy+yz+zx-8","I want to calculate extreme values for $f(x,y,z)=xyz$ given the constraints $g_{1}(x,y,z)=x+y+z-5$ and $g_{2}(x,y,z)=xy+yz+zx-8$ using Lagrange multipliers method. Im skeptical about my solution for this problem which goes as follow. First I got: $$\nabla f= (yz)i+(xz)j+(xy)k,$$ $$\nabla g_{1}=i+j+k,$$ and $$\nabla g_{2}=(y+z)i+(x+z)j+(y+x)k.$$ So from having the equality $\nabla f = \lambda_{1} \nabla g_{1} + \lambda_{2} \nabla g_{2}$ I got the following equation system: \begin{align*} yz&= \lambda_{1} + \lambda_{2}(y+z) \\ xz&=\lambda_{1}  + \lambda_{2}(x+z)\\ xy&=\lambda_{1} + \lambda_{2}(y+z)\\ \end{align*} But Im really stucked finding the extrem values from the last equations system. So far,  I realize that if I sum up the three equations from the system and the way $g_{2}(x,y,z)$ is defined I obtained: $$xy+yz+zx=8=3(\lambda_{1})+3(\lambda_{2})(x+y+z).$$ But from the way $g_{1}(x,y,z)$ is defined I got that last equation is $$8=3(\lambda_{1})+3(\lambda_{2})(5).$$ So I found  by trial and error that $\lambda_{1}=1$ and $\lambda_{2}=\frac{1}{3}$ , im not sure if there is more possible values for $\lambda_{1}$ and $\lambda_{2}$ satysfing the last equation. Anyways, from here I have been trying to find the values for $x,y$ and $z$ substituting the values I obtained for $\lambda_{1}$ and $\lambda_{2}$ in the equation from the original system of equations. For example, from the first equation I got $$yz=1+\frac{1}{3}(y+z)$$ But finding $x,y$ and $z$ that way is hard. I´ve been thinking also that from some equation before I have that: $$8=3\lambda_{1}+3\lambda_{2}(x+y+z)=3+(x+y+z)$$ . The problem of finding $x,y$ and $z$ this way is that I got a lot of points satisfying this last equation. Just to list a few: $$(5,0,0),(0,5,0),(0,0,5),(1,1,3),...$$ . Basically, all the points $(x,y,z) \in \mathbb{R}^{3}$ which satisfy $g_{1}(x,y,z)=0$ but I can tell there is something wrong from the fact a lot of these points dont satisfy the equations system obtained from $\nabla f = \lambda_{1} \nabla g_{1} + \lambda_{2} \nabla g_{2}$ .","I want to calculate extreme values for given the constraints and using Lagrange multipliers method. Im skeptical about my solution for this problem which goes as follow. First I got: and So from having the equality I got the following equation system: But Im really stucked finding the extrem values from the last equations system. So far,  I realize that if I sum up the three equations from the system and the way is defined I obtained: But from the way is defined I got that last equation is So I found  by trial and error that and , im not sure if there is more possible values for and satysfing the last equation. Anyways, from here I have been trying to find the values for and substituting the values I obtained for and in the equation from the original system of equations. For example, from the first equation I got But finding and that way is hard. I´ve been thinking also that from some equation before I have that: . The problem of finding and this way is that I got a lot of points satisfying this last equation. Just to list a few: . Basically, all the points which satisfy but I can tell there is something wrong from the fact a lot of these points dont satisfy the equations system obtained from .","f(x,y,z)=xyz g_{1}(x,y,z)=x+y+z-5 g_{2}(x,y,z)=xy+yz+zx-8 \nabla f= (yz)i+(xz)j+(xy)k, \nabla g_{1}=i+j+k, \nabla g_{2}=(y+z)i+(x+z)j+(y+x)k. \nabla f = \lambda_{1} \nabla g_{1} + \lambda_{2} \nabla g_{2} \begin{align*}
yz&= \lambda_{1} + \lambda_{2}(y+z) \\
xz&=\lambda_{1}  + \lambda_{2}(x+z)\\
xy&=\lambda_{1} + \lambda_{2}(y+z)\\
\end{align*} g_{2}(x,y,z) xy+yz+zx=8=3(\lambda_{1})+3(\lambda_{2})(x+y+z). g_{1}(x,y,z) 8=3(\lambda_{1})+3(\lambda_{2})(5). \lambda_{1}=1 \lambda_{2}=\frac{1}{3} \lambda_{1} \lambda_{2} x,y z \lambda_{1} \lambda_{2} yz=1+\frac{1}{3}(y+z) x,y z 8=3\lambda_{1}+3\lambda_{2}(x+y+z)=3+(x+y+z) x,y z (5,0,0),(0,5,0),(0,0,5),(1,1,3),... (x,y,z) \in \mathbb{R}^{3} g_{1}(x,y,z)=0 \nabla f = \lambda_{1} \nabla g_{1} + \lambda_{2} \nabla g_{2}","['real-analysis', 'calculus', 'multivariable-calculus', 'lagrange-multiplier', 'maxima-minima']"
10,Can I use Fubini's theorem to show the divergence of an improper double integral?,Can I use Fubini's theorem to show the divergence of an improper double integral?,,"Background: Let $f:\mathbb{R}^2\rightarrow \mathbb{R}$ be defined as $$f(x,y)=\frac{x^2-y^2}{(x^2+y^2)^2}$$ and integrate $f$ over the unit square $U=[0,1]\times[0,1]$ , show that $$\iint_U |f|\text{ }dA=\infty$$ Attempt: Since the set $B=\{(r\cos\theta,r\sin\theta)\mid r\in[0,1],\theta\in[0,\pi/2] \}$ is contained in $U$ , we should have $\iint_B|f|dA\leq\iint_U|f|dA.$ But \begin{align} (*)\dots\iint_B|f|dA=\int_0^{\pi/2}\int_0^1 \frac{|\cos^2\theta-\sin^2\theta|}{r}drd\theta=\int_0^{\pi/2}|\cos^2\theta-\sin^2\theta|d\theta\cdot\int_0^1r^{-1}dr \end{align} Observe that $\int_0^1 r^{-1}dr$ diverges, and $\int_0^{\pi/2}|\cos^2\theta-\sin^2\theta|d\theta$ converges, the proof is completed. But clearly, we cannot have (at least based on the Fubini's theorem) the first equality in $(*)$ since $r^{-1}$ is not integrable on $[0,1]$ (i.e., if the hypothesis of the Fubini's theorem is not satisfied). Do I have a way to make the above proof correct?","Background: Let be defined as and integrate over the unit square , show that Attempt: Since the set is contained in , we should have But Observe that diverges, and converges, the proof is completed. But clearly, we cannot have (at least based on the Fubini's theorem) the first equality in since is not integrable on (i.e., if the hypothesis of the Fubini's theorem is not satisfied). Do I have a way to make the above proof correct?","f:\mathbb{R}^2\rightarrow \mathbb{R} f(x,y)=\frac{x^2-y^2}{(x^2+y^2)^2} f U=[0,1]\times[0,1] \iint_U |f|\text{ }dA=\infty B=\{(r\cos\theta,r\sin\theta)\mid r\in[0,1],\theta\in[0,\pi/2] \} U \iint_B|f|dA\leq\iint_U|f|dA. \begin{align}
(*)\dots\iint_B|f|dA=\int_0^{\pi/2}\int_0^1 \frac{|\cos^2\theta-\sin^2\theta|}{r}drd\theta=\int_0^{\pi/2}|\cos^2\theta-\sin^2\theta|d\theta\cdot\int_0^1r^{-1}dr
\end{align} \int_0^1 r^{-1}dr \int_0^{\pi/2}|\cos^2\theta-\sin^2\theta|d\theta (*) r^{-1} [0,1]","['integration', 'multivariable-calculus']"
11,"Spivak, Calculus on Manifolds 3-40","Spivak, Calculus on Manifolds 3-40",,"Looking for a hint to the following question: If $g: \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable and $\det g'(x) \neq 0,$ prove that in some open set containing $x$ we can write $g = T \circ g_n \circ \ldots \circ g_1,$ where $g_i$ is of the form $g_i(x) = (x^1, \ldots, f_i(x), \ldots, x^n),$ and $T$ is a linear transformation. Since $g$ is $C^1$ we can apply the inverse function theorem, and obtain an open set $A$ containing $x$ where $g$ is one to one. Since this problem is in the section on change of variables, I assume it can be used in the solution. I state it below as Spivak states it. Let $A \subset \mathbb{R}^n$ be an open set and $g: A \to \mathbb{R}^n$ be a 1-1, continuously differentiable function such that $\det g'(x) \neq 0$ for all $x \in A$ . If $f: g(A) \to \mathbb{R}$ is integrable, then $$\int_{g(A)}f = \int_Af \circ g|detg'|.$$ I've managed to produce the conditions on $g$ necessary for change of variables but I have no clue as to how to apply it. Considering just the representation of $g$ stated in the problem, it seems to be a composition taking $x$ to $g(x)$ coordinate by coordinate. Can anyone give me a hint?","Looking for a hint to the following question: If is continuously differentiable and prove that in some open set containing we can write where is of the form and is a linear transformation. Since is we can apply the inverse function theorem, and obtain an open set containing where is one to one. Since this problem is in the section on change of variables, I assume it can be used in the solution. I state it below as Spivak states it. Let be an open set and be a 1-1, continuously differentiable function such that for all . If is integrable, then I've managed to produce the conditions on necessary for change of variables but I have no clue as to how to apply it. Considering just the representation of stated in the problem, it seems to be a composition taking to coordinate by coordinate. Can anyone give me a hint?","g: \mathbb{R}^n \to \mathbb{R}^n \det g'(x) \neq 0, x g = T \circ g_n \circ \ldots \circ g_1, g_i g_i(x) = (x^1, \ldots, f_i(x), \ldots, x^n), T g C^1 A x g A \subset \mathbb{R}^n g: A \to \mathbb{R}^n \det g'(x) \neq 0 x \in A f: g(A) \to \mathbb{R} \int_{g(A)}f = \int_Af \circ g|detg'|. g g x g(x)","['real-analysis', 'linear-algebra', 'integration', 'multivariable-calculus', 'change-of-variable']"
12,integration by substitution of multiple variables,integration by substitution of multiple variables,,"I have an integral \begin{equation} \int_{\mathbb{R}^n}f(\mathbf{B}\mathbf{x})\mathrm{d}\mathbf{x} \end{equation} where $f: \mathbb{R}^m \rightarrow \mathbb{R}$ and $\mathbf{B}\in\mathbb{R}^{m\times n}$ . I also know \begin{equation} \int_{\mathbb{R}^m}f(\mathbf{u})\mathrm{d}\mathbf{u}. \end{equation} For $n=m$ , we have \begin{equation} \int_{\mathbb{R}^n}f(\mathbf{B}\mathbf{x})\mathrm{d}\mathbf{x} = \frac{1}{\det\mathbf{B}} \int_{\mathbb{R}^n}f(\mathbf{u})\mathrm{d}\mathbf{u}.  \end{equation} What do I do for $n\neq m$ ?","I have an integral where and . I also know For , we have What do I do for ?","\begin{equation}
\int_{\mathbb{R}^n}f(\mathbf{B}\mathbf{x})\mathrm{d}\mathbf{x}
\end{equation} f: \mathbb{R}^m \rightarrow \mathbb{R} \mathbf{B}\in\mathbb{R}^{m\times n} \begin{equation}
\int_{\mathbb{R}^m}f(\mathbf{u})\mathrm{d}\mathbf{u}.
\end{equation} n=m \begin{equation}
\int_{\mathbb{R}^n}f(\mathbf{B}\mathbf{x})\mathrm{d}\mathbf{x} = \frac{1}{\det\mathbf{B}} \int_{\mathbb{R}^n}f(\mathbf{u})\mathrm{d}\mathbf{u}. 
\end{equation} n\neq m","['calculus', 'integration', 'multivariable-calculus', 'substitution']"
13,"Prove $u(x,y,z)=f(t+r)/r+g(t-r)/r$ satisfies $u_{xx}+u_{yy}+u_{zz}=u_{tt}$.",Prove  satisfies .,"u(x,y,z)=f(t+r)/r+g(t-r)/r u_{xx}+u_{yy}+u_{zz}=u_{tt}","I'm trying to prove that any function $u=u(x,y,z)$ of the form $$u(x,y,z)=\frac{f(t+r)}{r}+\frac{g(t-r)}{r},$$ with $r^2=x^2+y^2+z^2$ , satisfies the differential equation $ u_{xx}+u_{yy}+u_{zz}=u_{tt}, $ but I'm stuck when relating the derivatives. From what I get, $$u_{tt} = \dfrac{1}{r}(f''+g''),$$ as $r$ does not depend on $t$ . Furthermore, $$ u_x = \dfrac{r_x}{r^2}\left( r(f'-g')-(f+g) \right), $$ so, if I'm not mistaken, $$u_{xx} = \dfrac{x^2}{r^3} (f''+g'') + \dfrac{r^2-3x^2}{r^4}(f'-g') + \dfrac{x^2-r^2}{r^5}(f+g). $$ (By symmetry, $u_{yy}$ and $u_{zz}$ have the same form). But I don't know how to relate this expression to the other second derivatives, so I think I'm missing something... Any ideas are greatly appreciated. Thanks in advance! EDIT: Corrected a $r^2$ term in the expression for $u_x$ . Also, thanks to @Klaramun's and @TedShifrin's suggestions, I managed to reduce the expression for $u_{xx}$ , noting that $f_x=f_y=f_z=f',\, g_x=g_y=g_z=g'$ , so I fixed that a couple lines up.","I'm trying to prove that any function of the form with , satisfies the differential equation but I'm stuck when relating the derivatives. From what I get, as does not depend on . Furthermore, so, if I'm not mistaken, (By symmetry, and have the same form). But I don't know how to relate this expression to the other second derivatives, so I think I'm missing something... Any ideas are greatly appreciated. Thanks in advance! EDIT: Corrected a term in the expression for . Also, thanks to @Klaramun's and @TedShifrin's suggestions, I managed to reduce the expression for , noting that , so I fixed that a couple lines up.","u=u(x,y,z) u(x,y,z)=\frac{f(t+r)}{r}+\frac{g(t-r)}{r}, r^2=x^2+y^2+z^2  u_{xx}+u_{yy}+u_{zz}=u_{tt},  u_{tt} = \dfrac{1}{r}(f''+g''), r t  u_x = \dfrac{r_x}{r^2}\left( r(f'-g')-(f+g) \right),  u_{xx} = \dfrac{x^2}{r^3} (f''+g'') + \dfrac{r^2-3x^2}{r^4}(f'-g') + \dfrac{x^2-r^2}{r^5}(f+g).  u_{yy} u_{zz} r^2 u_x u_{xx} f_x=f_y=f_z=f',\, g_x=g_y=g_z=g'","['calculus', 'multivariable-calculus', 'partial-differential-equations']"
14,$\iint f(x)g(y)dxdy =\int f(x)dx \int g(y)dy $ Why?,Why?,\iint f(x)g(y)dxdy =\int f(x)dx \int g(y)dy ,"$\iint f(x)g(y)dxdy =\int f(x)dx \int g(y)dy $ . How could this be proven I tried this with Fubini, so on the LHS we can think of $g(y)$ as constant and take it out of the integral and in the following step we can think ok $f(x)$ as a constant such. In other word $\int g(y) \big[\int f(x)dx\big]dy=\big[\int f(x)dx\big]\int g(y)dy$ . But I do not know how to come from $\int f(x)dx \int g(y)dy \Rightarrow\iint f(x)g(y)dxdy $ pls help",". How could this be proven I tried this with Fubini, so on the LHS we can think of as constant and take it out of the integral and in the following step we can think ok as a constant such. In other word . But I do not know how to come from pls help",\iint f(x)g(y)dxdy =\int f(x)dx \int g(y)dy  g(y) f(x) \int g(y) \big[\int f(x)dx\big]dy=\big[\int f(x)dx\big]\int g(y)dy \int f(x)dx \int g(y)dy \Rightarrow\iint f(x)g(y)dxdy ,['calculus']
15,Divergence Theorem - Cone,Divergence Theorem - Cone,,"Here's the question:  Evaluate the surface integral $\iint _S F\cdot n \space dA$ by the divergence theorem. $ \mathit F = [xy, yz, zx]$ , S the surface of the cone $x^2 + y^2 \le 4z^2, \space \space 0 \le z \le 2 $ This is my working for this question. $$\nabla F = y + z + x $$ Parametric equation of S: $$ (r,v,u) = (r\cos(v), r\sin(v), u) $$ The limits for each variable: $$ 0\le v \le 2\pi \quad 0 \le u  \le 2 \quad 0\le r \le 2u $$ Jacobian is $r$ The new divergence is $$\nabla F = r\cos (v) + r \sin (v) + u $$ Hence evaluating the triple integral yields $16\pi$ Now, my working out for the triple integral is correct but I am not sure about my parametric equation and limits. I do not know the correct answer to this question since my textbook only provides answers to odd-number problems. However, I came across this website http://www.slader.com/textbook/9780471488859-advanced-engineering-mathematics-9th-edition/463/problems/18/# and their answer is different to mine. Did I do something wrong?","Here's the question:  Evaluate the surface integral by the divergence theorem. , S the surface of the cone This is my working for this question. Parametric equation of S: The limits for each variable: Jacobian is The new divergence is Hence evaluating the triple integral yields Now, my working out for the triple integral is correct but I am not sure about my parametric equation and limits. I do not know the correct answer to this question since my textbook only provides answers to odd-number problems. However, I came across this website http://www.slader.com/textbook/9780471488859-advanced-engineering-mathematics-9th-edition/463/problems/18/# and their answer is different to mine. Did I do something wrong?","\iint _S F\cdot n \space dA  \mathit F = [xy, yz, zx] x^2 + y^2 \le 4z^2, \space \space 0 \le z \le 2  \nabla F = y + z + x   (r,v,u) = (r\cos(v), r\sin(v), u)   0\le v \le 2\pi \quad 0 \le u  \le 2 \quad 0\le r \le 2u  r \nabla F = r\cos (v) + r \sin (v) + u  16\pi","['integration', 'multivariable-calculus', 'conic-sections', 'parametric', 'divergence-operator']"
16,Interchanging mixed derivatives: is $D_{h_2} (D_{h_1} f)(a) = D_{h_1} (D_{h_2} f)(a)$ if $D_{h_2} (D_{h_1} f)$ is continuous at $a$?,Interchanging mixed derivatives: is  if  is continuous at ?,D_{h_2} (D_{h_1} f)(a) = D_{h_1} (D_{h_2} f)(a) D_{h_2} (D_{h_1} f) a,"Given that $f : E \subset \mathbb{R}^n \to \mathbb{R}$ and $h_1,h_2 \in \mathbb{R}^n$ such that $D_{h_1} f$ , $D_{h_2} f$ and $D_{h_2} (D_{h_1} f)$ exist in a small interval around $a$ and are continuous at $a$ , prove that $D_{h_1}(D_{h_2} f)(a)$ exists and that it is equal to $D_{h_2} (D_{h_1} f)(a)$ . We have by definition that $$ \begin{align} D_{h_1} (D_{h_2} f)(a) &= \lim \limits_{s \to 0} \frac{D_{h_2}f(a+s h_1) -D_{h_2} f(a)}{s}\\ &= \lim \limits_{s \to 0} \lim \limits_{t \to 0} \frac{(f(a+s h_1+t h_2)-f(a+s h_1))-(f(a+t h_2)-f(a))}{s t}, \end{align} $$ but I am stuck here. I think Lagrange's Mean Value Theorem is useful here, but I don't know how, or that if I am allowed to interchange the limits.","Given that and such that , and exist in a small interval around and are continuous at , prove that exists and that it is equal to . We have by definition that but I am stuck here. I think Lagrange's Mean Value Theorem is useful here, but I don't know how, or that if I am allowed to interchange the limits.","f : E \subset \mathbb{R}^n \to \mathbb{R} h_1,h_2 \in \mathbb{R}^n D_{h_1} f D_{h_2} f D_{h_2} (D_{h_1} f) a a D_{h_1}(D_{h_2} f)(a) D_{h_2} (D_{h_1} f)(a) 
\begin{align}
D_{h_1} (D_{h_2} f)(a) &= \lim \limits_{s \to 0} \frac{D_{h_2}f(a+s h_1) -D_{h_2} f(a)}{s}\\ &= \lim \limits_{s \to 0} \lim \limits_{t \to 0} \frac{(f(a+s h_1+t h_2)-f(a+s h_1))-(f(a+t h_2)-f(a))}{s t},
\end{align}
","['real-analysis', 'multivariable-calculus', 'partial-derivative']"
17,"Is the function $f(x,y) = \frac{x^2y^2}{x^2y^2 + (y-x)^2}$ if $(x,y) \neq (0,0)$, $f(0,0) = 0$ differentiable? Continuous?","Is the function  if ,  differentiable? Continuous?","f(x,y) = \frac{x^2y^2}{x^2y^2 + (y-x)^2} (x,y) \neq (0,0) f(0,0) = 0","Define $f(x,y) = \frac{x^2y^2}{x^2y^2 + (y-x)^2}$ if $(x,y) \neq (0,0)$, $f(0,0) = 0$ on $\mathbb{R}^2$. (a) For which vectors $u \neq 0$ does $f'(0,u)$ exist? Evaluate it when it exists (b) Do $D_1f, D_2f$ exist at $0$? (c) Is $f$ differentiable at $0$? (d) Is it continuous at $0$? My attempt : (a)$$f'((0,0),(u_1,u_2)) = \lim_{t \to 0} \frac{u_1^2 u_2^2t}{t^2 u_1^2 u_2^2 + (u_1-u_2)^2}$$ exists only if $u_1 \neq u_2$, and then equals $0$. (b) Since $D_1f(0,0) = f'(0,(1,0))$, it follows that $D_1f(0,0) = 0$. Similalrly for $D_2f(0,0)$ (c) No, not all directional derivatives exist, and also no because of (d) (d) $(1/n,1/n) \to 0$ but $f(1/n,1/n) = 1 \not \to 0 = f(0,0)$. Hence, $f$ isn't continuous at $0$. Is this correct?","Define $f(x,y) = \frac{x^2y^2}{x^2y^2 + (y-x)^2}$ if $(x,y) \neq (0,0)$, $f(0,0) = 0$ on $\mathbb{R}^2$. (a) For which vectors $u \neq 0$ does $f'(0,u)$ exist? Evaluate it when it exists (b) Do $D_1f, D_2f$ exist at $0$? (c) Is $f$ differentiable at $0$? (d) Is it continuous at $0$? My attempt : (a)$$f'((0,0),(u_1,u_2)) = \lim_{t \to 0} \frac{u_1^2 u_2^2t}{t^2 u_1^2 u_2^2 + (u_1-u_2)^2}$$ exists only if $u_1 \neq u_2$, and then equals $0$. (b) Since $D_1f(0,0) = f'(0,(1,0))$, it follows that $D_1f(0,0) = 0$. Similalrly for $D_2f(0,0)$ (c) No, not all directional derivatives exist, and also no because of (d) (d) $(1/n,1/n) \to 0$ but $f(1/n,1/n) = 1 \not \to 0 = f(0,0)$. Hence, $f$ isn't continuous at $0$. Is this correct?",,['multivariable-calculus']
18,Evaluating an Integral by converting into polar coordinates.,Evaluating an Integral by converting into polar coordinates.,,"Question. Evaluate the integral by converting into polar coordinate: $$I=\int_{0}^{\sqrt 3}\int_{0}^{\sqrt {4-y^2}}\frac{dx~dy}{4+x^2+y^2}$$ My Solution. Let $f(x,y)=\frac{1}{4+x^2+y^2}$ . Now the region of the integration is $S_1 \cup S_2$ as depicted in the following figure: $xy$ -plane"" /> Now $I=\iint_{S_1} f(x,y)~dxdy +\iint_{S_2} f(x,y)~dxdy=I_1+I_2.$ Hence if I change the coordinate into polar co ordinate by $x=r \cos \theta;~y=r\sin \theta$ where $0<r; 0\le\theta<2 \pi$ , $I_1=\int_{r=0}^{2}\int_{0}^{\pi/3}f ~rdrd\theta$ And $I_2=\int_{0}^{\sqrt 3}\int_{0}^{y}f(x,y)~dxdy$ . But I cannot figure out range of $r$ and $\theta$ when $(x,y)$ varies in $S_2$ . How can I find the range of $r$ and $\theta$ in the later case? Please help. Thank you.","Question. Evaluate the integral by converting into polar coordinate: My Solution. Let . Now the region of the integration is as depicted in the following figure: $xy$ -plane"" /> Now Hence if I change the coordinate into polar co ordinate by where , And . But I cannot figure out range of and when varies in . How can I find the range of and in the later case? Please help. Thank you.","I=\int_{0}^{\sqrt 3}\int_{0}^{\sqrt {4-y^2}}\frac{dx~dy}{4+x^2+y^2} f(x,y)=\frac{1}{4+x^2+y^2} S_1 \cup S_2 I=\iint_{S_1} f(x,y)~dxdy +\iint_{S_2} f(x,y)~dxdy=I_1+I_2. x=r \cos \theta;~y=r\sin \theta 0<r; 0\le\theta<2 \pi I_1=\int_{r=0}^{2}\int_{0}^{\pi/3}f ~rdrd\theta I_2=\int_{0}^{\sqrt 3}\int_{0}^{y}f(x,y)~dxdy r \theta (x,y) S_2 r \theta","['calculus', 'multivariable-calculus', 'multiple-integral']"
19,Volume enclosed by paraboloid and plane,Volume enclosed by paraboloid and plane,,"The volume, $V$, enclosed by paraboloid $z=x^2 + y^2$ and the plane $z=3-2y$ can be expressed as a triple integral. Determine the limits describing the enclosed volume. By evaluating the integral, show the volume is $8\pi$.$\\$ There is then a hint that $\cos^4\theta = \frac{1}{8}\cos4\theta + \frac{1}{2}\cos2\theta + \frac{3}{8}$. I worked out the limits in Cartesian as $V=\int_{y=-3}^{y=1}{\int_{x=-\sqrt{4-(y+1)^2}}^{x=\sqrt{4-(y+1)^2}}{\int_{z=x^2+y^2}^{z=3-2y}dz}dx}dy$ This looks pretty hard to compute though, so I then made the polar limits as  $V=\int_{R=0}^{R=2}{\int_{\theta=0}^{\theta=2\pi}{\int_{z=R^2}^{z=3-2Rsin\theta}R dz}d\theta}dR$, but when I compute this I don't get $8\pi$. I'm wondering if this is me making a calculation error, or if the limits are incorrect. I also never used the hint, so was wondering if anyone could shine a light on when that comes in handy. Thanks!","The volume, $V$, enclosed by paraboloid $z=x^2 + y^2$ and the plane $z=3-2y$ can be expressed as a triple integral. Determine the limits describing the enclosed volume. By evaluating the integral, show the volume is $8\pi$.$\\$ There is then a hint that $\cos^4\theta = \frac{1}{8}\cos4\theta + \frac{1}{2}\cos2\theta + \frac{3}{8}$. I worked out the limits in Cartesian as $V=\int_{y=-3}^{y=1}{\int_{x=-\sqrt{4-(y+1)^2}}^{x=\sqrt{4-(y+1)^2}}{\int_{z=x^2+y^2}^{z=3-2y}dz}dx}dy$ This looks pretty hard to compute though, so I then made the polar limits as  $V=\int_{R=0}^{R=2}{\int_{\theta=0}^{\theta=2\pi}{\int_{z=R^2}^{z=3-2Rsin\theta}R dz}d\theta}dR$, but when I compute this I don't get $8\pi$. I'm wondering if this is me making a calculation error, or if the limits are incorrect. I also never used the hint, so was wondering if anyone could shine a light on when that comes in handy. Thanks!",,"['integration', 'multivariable-calculus']"
20,Hessian matrix vs differential 2-form,Hessian matrix vs differential 2-form,,"Could someone clarify the convention that the second derivative of a scalar function $f: \Bbb R^n \rightarrow \Bbb R$ is sometimes defined as a linear operator $D^2f : \Bbb R^n \rightarrow L(\Bbb R^n, L(\Bbb R^n, \Bbb R))$, which is also identified with the Hessian matrix (filled with second order partial derivatives), and sometimes as a differential 2-form? It seems that both happens in vector calculus. Is the definition of derivative just context dependent, and the first example is just a case of ""standard derivative"" and the second one of the ""exterior derivative""? Or am I really confused about something?","Could someone clarify the convention that the second derivative of a scalar function $f: \Bbb R^n \rightarrow \Bbb R$ is sometimes defined as a linear operator $D^2f : \Bbb R^n \rightarrow L(\Bbb R^n, L(\Bbb R^n, \Bbb R))$, which is also identified with the Hessian matrix (filled with second order partial derivatives), and sometimes as a differential 2-form? It seems that both happens in vector calculus. Is the definition of derivative just context dependent, and the first example is just a case of ""standard derivative"" and the second one of the ""exterior derivative""? Or am I really confused about something?",,"['multivariable-calculus', 'vector-analysis', 'differential-forms', 'hessian-matrix']"
21,"Is $f(x,y)=\frac{y}{x^2 +1}$ uniformly continuous?",Is  uniformly continuous?,"f(x,y)=\frac{y}{x^2 +1}","Is $f(x,y)=\frac{y}{x^2 +1}$ uniformly continuous on $\mathbb{R}^2$? Intuitively it doesn't seem to be uniformly continuous because the denominator grows quadratically and the numerator grows linearly. I've tried to prove that $f$ is not uniformly continuous by using the sequential characterization of uniform continuity: Let $A\subset \mathbb{R}^n$ and a function $f:A \rightarrow \mathbb{R}^m$.  $f$ is uniformly continuous if and only if for any two sequences $\{x_k\}, \{y_k\}$ of points of $A$ such that $\{x_k - y_k\}\rightarrow 0$ then $\{f(x_k) - f(y_k)\}\rightarrow 0$. So I've tried to find two sequences such that $\{x_k - y_k\}\rightarrow 0$  but $\{f(x_k) - f(y_k)\}\not \rightarrow 0$. However none of the pairs of sequences that i have tried have been succesful. These are a couple of them: $\{k,k\}$ and $\{k+\frac{1}{\sqrt{k}},k\}$. $\{k,\frac{1}{k}\}$ and $\{k+\frac{1}{k},\frac{1}{k}\}$. $\{\frac{1}{\sqrt{k-1}},\frac{1}{k}\}$ and $\{\frac{1}{\sqrt{k}},\frac{1}{k}\}$ Any hint or a ""good"" pair of sequences to try would be appreciated.","Is $f(x,y)=\frac{y}{x^2 +1}$ uniformly continuous on $\mathbb{R}^2$? Intuitively it doesn't seem to be uniformly continuous because the denominator grows quadratically and the numerator grows linearly. I've tried to prove that $f$ is not uniformly continuous by using the sequential characterization of uniform continuity: Let $A\subset \mathbb{R}^n$ and a function $f:A \rightarrow \mathbb{R}^m$.  $f$ is uniformly continuous if and only if for any two sequences $\{x_k\}, \{y_k\}$ of points of $A$ such that $\{x_k - y_k\}\rightarrow 0$ then $\{f(x_k) - f(y_k)\}\rightarrow 0$. So I've tried to find two sequences such that $\{x_k - y_k\}\rightarrow 0$  but $\{f(x_k) - f(y_k)\}\not \rightarrow 0$. However none of the pairs of sequences that i have tried have been succesful. These are a couple of them: $\{k,k\}$ and $\{k+\frac{1}{\sqrt{k}},k\}$. $\{k,\frac{1}{k}\}$ and $\{k+\frac{1}{k},\frac{1}{k}\}$. $\{\frac{1}{\sqrt{k-1}},\frac{1}{k}\}$ and $\{\frac{1}{\sqrt{k}},\frac{1}{k}\}$ Any hint or a ""good"" pair of sequences to try would be appreciated.",,"['multivariable-calculus', 'uniform-continuity']"
22,Spivak Calculus on Manifolds - Problem 3-21 Help with details of proof,Spivak Calculus on Manifolds - Problem 3-21 Help with details of proof,,"3-21) ""Let $A \subseteq \mathbb{R}^n$ be a closed rectangle, and $C \subseteq A$. Show that $C$ is Jordan measurable if and only if for every $\varepsilon > 0$, there is a partition $P$ of $A$ such that $$\sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) < \varepsilon$$ where $\sigma_1$ is the collection of subrectangles $S$ determined by the partition $P$ such that they intersect $C$, and $\sigma_2$ consists of those which are contained in $C$."" So I think I have an outline of a solution but there are a few steps which I can't justify even though they seem to be true intuitively. proof of $\implies$ direction: Suppose $C$ is Jordan measurable and let $\varepsilon > 0$ be given. Then we can choose a collection of closed rectangles $\{U_i\}_{i \in \mathbb{N}}$ to cover the boundary of $C$ such that $\sum_{i=1}^{\infty} v(U_i) < \varepsilon$. Since bd($C$) is closed and bounded, it's compact thus it has a finite subcover $\{U_{\omega}\}_{\omega \in \Omega}$. Next, we construct a partition $P$ of $A$ using the endpoints of the closed intervals which make up the closed rectangles $U_{\omega}$; so that each $U_{\omega}$ is a union of subrectangles determined by $P$. Now, because $\sigma_2 $ is contained in $\sigma_1$, we have that $$\sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) = \sum_{S \in \sigma_1 - \sigma_2}v(S) $$ Now here is where I can't properly justify my next step: I want to claim that the RHS is $\leq \sum_{\omega \in \Omega} v(U_{\omega})$, because from the pictures I drew that's what it seems like... Assuming that step is true, we have that  \begin{align} \sum_{\omega \in \Omega} v(U_{\omega}) &\leq \sum_{i=1}^{\infty} v(U_i) \\ &< \varepsilon \end{align} Since $\varepsilon > 0$ was arbitrary, this completes the ""proof"" of this direction. For the other direction, for any given $\varepsilon > 0$, we can choose a partition $P$ such that $$ \sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) < \varepsilon$$ Once again since $\sigma_2 \subseteq \sigma_1$, we have that $$\sum_{S \in \sigma_1 - \sigma_2}v(S) < \varepsilon$$ Here, I would like to claim that $\sigma_1 - \sigma_2$ covers bd($C$), which once again seems like it should be true from the pictures I've drawn, but I can't prove it. If this is indeed true then we've just shown that bd($C$) has content $0$, which implies it has measure $0$; thus completing the proof. I'd appreciate any help in justifying those two steps of my proof, and also any comments on any other mistakes which are present. Thanks!","3-21) ""Let $A \subseteq \mathbb{R}^n$ be a closed rectangle, and $C \subseteq A$. Show that $C$ is Jordan measurable if and only if for every $\varepsilon > 0$, there is a partition $P$ of $A$ such that $$\sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) < \varepsilon$$ where $\sigma_1$ is the collection of subrectangles $S$ determined by the partition $P$ such that they intersect $C$, and $\sigma_2$ consists of those which are contained in $C$."" So I think I have an outline of a solution but there are a few steps which I can't justify even though they seem to be true intuitively. proof of $\implies$ direction: Suppose $C$ is Jordan measurable and let $\varepsilon > 0$ be given. Then we can choose a collection of closed rectangles $\{U_i\}_{i \in \mathbb{N}}$ to cover the boundary of $C$ such that $\sum_{i=1}^{\infty} v(U_i) < \varepsilon$. Since bd($C$) is closed and bounded, it's compact thus it has a finite subcover $\{U_{\omega}\}_{\omega \in \Omega}$. Next, we construct a partition $P$ of $A$ using the endpoints of the closed intervals which make up the closed rectangles $U_{\omega}$; so that each $U_{\omega}$ is a union of subrectangles determined by $P$. Now, because $\sigma_2 $ is contained in $\sigma_1$, we have that $$\sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) = \sum_{S \in \sigma_1 - \sigma_2}v(S) $$ Now here is where I can't properly justify my next step: I want to claim that the RHS is $\leq \sum_{\omega \in \Omega} v(U_{\omega})$, because from the pictures I drew that's what it seems like... Assuming that step is true, we have that  \begin{align} \sum_{\omega \in \Omega} v(U_{\omega}) &\leq \sum_{i=1}^{\infty} v(U_i) \\ &< \varepsilon \end{align} Since $\varepsilon > 0$ was arbitrary, this completes the ""proof"" of this direction. For the other direction, for any given $\varepsilon > 0$, we can choose a partition $P$ such that $$ \sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) < \varepsilon$$ Once again since $\sigma_2 \subseteq \sigma_1$, we have that $$\sum_{S \in \sigma_1 - \sigma_2}v(S) < \varepsilon$$ Here, I would like to claim that $\sigma_1 - \sigma_2$ covers bd($C$), which once again seems like it should be true from the pictures I've drawn, but I can't prove it. If this is indeed true then we've just shown that bd($C$) has content $0$, which implies it has measure $0$; thus completing the proof. I'd appreciate any help in justifying those two steps of my proof, and also any comments on any other mistakes which are present. Thanks!",,"['integration', 'multivariable-calculus']"
23,"Finding critical points of $f(x,y)= \sin x+\sin y + \cos(x+y)$",Finding critical points of,"f(x,y)= \sin x+\sin y + \cos(x+y)","Find the critical points of function$$ f(x,y)=\sin x + \sin y + \cos(x+y),$$   where $0<x<\dfrac{\pi}{2}$, $0<y<\dfrac{\pi}{2}$. What I have done: $$f_{x}=\cos(x)-\sin(x+y),\\ f_{y}=\cos(y)-\sin(x+y).$$ From $f_{x}=0$, $\cos(x)=\sin(x+y)$. From $f_{x}=0$, $\cos(y)=\sin(x+y)$. I do not know where to go from here. My attemps: $$\sin\left(\frac{\pi}{2}-x\right)=\sin(x+y)=\sin\left(\frac{\pi}{2}-y\right).$$","Find the critical points of function$$ f(x,y)=\sin x + \sin y + \cos(x+y),$$   where $0<x<\dfrac{\pi}{2}$, $0<y<\dfrac{\pi}{2}$. What I have done: $$f_{x}=\cos(x)-\sin(x+y),\\ f_{y}=\cos(y)-\sin(x+y).$$ From $f_{x}=0$, $\cos(x)=\sin(x+y)$. From $f_{x}=0$, $\cos(y)=\sin(x+y)$. I do not know where to go from here. My attemps: $$\sin\left(\frac{\pi}{2}-x\right)=\sin(x+y)=\sin\left(\frac{\pi}{2}-y\right).$$",,['multivariable-calculus']
24,"$\int_\gamma P\,dx+Q\,dy$ as the area of a domain",as the area of a domain,"\int_\gamma P\,dx+Q\,dy","Let $A$ be a domain in $\mathbb R^2$ whose boundary $\gamma $ is a smooth positively oriented curve. Find two functions $P,Q:\mathbb R^2\to \mathbb R$ such that $\int_\gamma P\,dx+Q\,dy$ is the area of $A$. Is it sufficient to apply Green's theorem and find $P$ and $Q$ with $Q_x-P_y=1$? So, will $Q(x,y)=x, P(x,y)=x$ do the job?","Let $A$ be a domain in $\mathbb R^2$ whose boundary $\gamma $ is a smooth positively oriented curve. Find two functions $P,Q:\mathbb R^2\to \mathbb R$ such that $\int_\gamma P\,dx+Q\,dy$ is the area of $A$. Is it sufficient to apply Green's theorem and find $P$ and $Q$ with $Q_x-P_y=1$? So, will $Q(x,y)=x, P(x,y)=x$ do the job?",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
25,Using Lagrange Multiplier to prove identity,Using Lagrange Multiplier to prove identity,,"Show that the maximum and minimum values of the function $u=x^2+y^2+xy$, where $ax^2+by^2=ab\ (a>b>0)$ are given by $$4(u-a)(u-b)=ab$$ My attempt- using Lagrange Multiplier method, $$F(x,y)= (x^2+y^2+xy)-\lambda(ax^2+by^2-ab)$$ $$dF= (2x+y-2a\lambda x)dx+(2y+x-2b\lambda y)dy$$  Equating to  zero to get, $$y/x= 2(a\lambda -1); y/x= 1/(2(b\lambda-1))$$ $$\implies 4(a\lambda-1)(b\lambda-1)=1$$ $$\implies 4(a^2\lambda-a)(b^2\lambda-b)=ab$$ I am stuck here. Finding $(x_0,y_0)$ which finds extrema for $u$ and then substituting to prove $$4(u-a)(u-b)=ab$$ is very tedious. Can someone help on how I should go about it?","Show that the maximum and minimum values of the function $u=x^2+y^2+xy$, where $ax^2+by^2=ab\ (a>b>0)$ are given by $$4(u-a)(u-b)=ab$$ My attempt- using Lagrange Multiplier method, $$F(x,y)= (x^2+y^2+xy)-\lambda(ax^2+by^2-ab)$$ $$dF= (2x+y-2a\lambda x)dx+(2y+x-2b\lambda y)dy$$  Equating to  zero to get, $$y/x= 2(a\lambda -1); y/x= 1/(2(b\lambda-1))$$ $$\implies 4(a\lambda-1)(b\lambda-1)=1$$ $$\implies 4(a^2\lambda-a)(b^2\lambda-b)=ab$$ I am stuck here. Finding $(x_0,y_0)$ which finds extrema for $u$ and then substituting to prove $$4(u-a)(u-b)=ab$$ is very tedious. Can someone help on how I should go about it?",,"['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
26,Same flux through every cross section.,Same flux through every cross section.,,"I got the following problem: Let $F$ be a continously differentiable vector field with $div(F)=0$. We define an integral curve as a curve that is tangent to the vector field F. Now let us take a curve C that is never tangent to F. When we look at the integral curves of F through C we get a surface, call it S. Prove that the flux of F through every section of S is equal. Any help? I tried to find a general parametrization to the surface F and compute the flux ""by hand"". I got that the flux is zero through every section but that seems odd, so I am probably mistaken. Plus, I haven't used that fact that the field is divergence free. Please help :)","I got the following problem: Let $F$ be a continously differentiable vector field with $div(F)=0$. We define an integral curve as a curve that is tangent to the vector field F. Now let us take a curve C that is never tangent to F. When we look at the integral curves of F through C we get a surface, call it S. Prove that the flux of F through every section of S is equal. Any help? I tried to find a general parametrization to the surface F and compute the flux ""by hand"". I got that the flux is zero through every section but that seems odd, so I am probably mistaken. Plus, I haven't used that fact that the field is divergence free. Please help :)",,"['calculus', 'multivariable-calculus']"
27,Partial Derivatives of Vector Valued Functions,Partial Derivatives of Vector Valued Functions,,"Let us say we have some differentiable vector field $F:\mathbb{R}^3 \to \mathbb{R}^3$. I have often seen the notation: $$ \frac{\partial F}{\partial x} $$ Is this accepted notation?  If so, is it usually a standin for the vector: $$\left(\begin{matrix} \frac{\partial F_1}{\partial x} \\ \frac{\partial F_2}{\partial x}\\ \frac{\partial F_3}{\partial x} \end{matrix}\right)$$ Moreover, could we use the limit definition of the partial derivative to compute this derivative? Places I have seen this notation are for example when one looks to calculate the surface area of a parametric $2$ - dim surface embedded in $\mathbb{R}^3$, where one takes the cross product of the partial derivative ""vectors"" of the map $u:\mathbb{R}^2 \to \mathbb{R}^3$ which parameterizes the surface.","Let us say we have some differentiable vector field $F:\mathbb{R}^3 \to \mathbb{R}^3$. I have often seen the notation: $$ \frac{\partial F}{\partial x} $$ Is this accepted notation?  If so, is it usually a standin for the vector: $$\left(\begin{matrix} \frac{\partial F_1}{\partial x} \\ \frac{\partial F_2}{\partial x}\\ \frac{\partial F_3}{\partial x} \end{matrix}\right)$$ Moreover, could we use the limit definition of the partial derivative to compute this derivative? Places I have seen this notation are for example when one looks to calculate the surface area of a parametric $2$ - dim surface embedded in $\mathbb{R}^3$, where one takes the cross product of the partial derivative ""vectors"" of the map $u:\mathbb{R}^2 \to \mathbb{R}^3$ which parameterizes the surface.",,"['multivariable-calculus', 'notation']"
28,"Maximize $f(x,y)=xy$ subject to $x^2-yx+y^2 = 1$",Maximize  subject to,"f(x,y)=xy x^2-yx+y^2 = 1","Use Lagrange multipliers method to find the maximum and minimum values of the function    $$f(x,y)=xy$$   on the curve   $$x^2-yx+y^2=1$$ Attempt: First I set let $g(x,y)=x^2-xy+y^2-1$ and set $$\nabla f=\lambda\nabla g$$ so $$(y,x)=\lambda(2x-y,2y-x)$$ then $$\begin{cases}  \lambda=\frac{y}{2x-y} & (1) \\ \lambda=\frac{x}{2y-x} & (2)\\ x^2-yx+y^2=1 \end{cases} $$ Solving $(1)$ and $(2)$ simultaneously, I get that $$y^2=x^2$$ Substitutiting into $(3)$ and following through with the arithmetic, I get four candidates for max and min, namely $$(1,1),(-1,-1),\big(-\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}}\big),\big(\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}}\big)$$  Evaluating these points on $f$, I get that the maximum value is $$1 \ \text{at} \ (\pm1,\pm1)$$ and the minimum value is $$-\frac{1}{3} \ \text{at} \ \big(\pm\frac{1}{\sqrt{3}},\mp\frac{1}{\sqrt{3}}\big)$$ Am I correct? I am unsure if there are indeed four critical points.","Use Lagrange multipliers method to find the maximum and minimum values of the function    $$f(x,y)=xy$$   on the curve   $$x^2-yx+y^2=1$$ Attempt: First I set let $g(x,y)=x^2-xy+y^2-1$ and set $$\nabla f=\lambda\nabla g$$ so $$(y,x)=\lambda(2x-y,2y-x)$$ then $$\begin{cases}  \lambda=\frac{y}{2x-y} & (1) \\ \lambda=\frac{x}{2y-x} & (2)\\ x^2-yx+y^2=1 \end{cases} $$ Solving $(1)$ and $(2)$ simultaneously, I get that $$y^2=x^2$$ Substitutiting into $(3)$ and following through with the arithmetic, I get four candidates for max and min, namely $$(1,1),(-1,-1),\big(-\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}}\big),\big(\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}}\big)$$  Evaluating these points on $f$, I get that the maximum value is $$1 \ \text{at} \ (\pm1,\pm1)$$ and the minimum value is $$-\frac{1}{3} \ \text{at} \ \big(\pm\frac{1}{\sqrt{3}},\mp\frac{1}{\sqrt{3}}\big)$$ Am I correct? I am unsure if there are indeed four critical points.",,"['multivariable-calculus', 'proof-verification']"
29,How to integrate $\cos \left(\frac{y-x}{y+x}\right)$?,How to integrate ?,\cos \left(\frac{y-x}{y+x}\right),"Suppose $D$ is a trapezoid with vertices $(0, 1)$, $(1, 0)$, $(0, 2)$, $(2, 0)$. How to calculate the following? $$\iint_D \cos \left(\frac{y-x}{y+x}\right) \, \mathrm d x \mathrm d y$$ P.S. Thanks a lot to Robert Z, I solved the problem now. My solution is this: $$u=y-x$$ $$v=y+x$$ $$\iint_Dcos(\frac{y-x}{y+x})dxdy = \int_1^2 \int_{-v}^v \cos(\frac{u}{v}) du dv = \int_1^2 2v\sin(1) dv = 3\sin(1)$$","Suppose $D$ is a trapezoid with vertices $(0, 1)$, $(1, 0)$, $(0, 2)$, $(2, 0)$. How to calculate the following? $$\iint_D \cos \left(\frac{y-x}{y+x}\right) \, \mathrm d x \mathrm d y$$ P.S. Thanks a lot to Robert Z, I solved the problem now. My solution is this: $$u=y-x$$ $$v=y+x$$ $$\iint_Dcos(\frac{y-x}{y+x})dxdy = \int_1^2 \int_{-v}^v \cos(\frac{u}{v}) du dv = \int_1^2 2v\sin(1) dv = 3\sin(1)$$",,"['calculus', 'integration', 'multivariable-calculus']"
30,"If $I = \int_{-\infty}^\infty(xu - 3tu^2)\mathrm{d}x$, show that $\frac{\mathrm{d}I}{\mathrm{d}t} = 0$.","If , show that .",I = \int_{-\infty}^\infty(xu - 3tu^2)\mathrm{d}x \frac{\mathrm{d}I}{\mathrm{d}t} = 0,"Exercise : Show that the integral $\int (xu-3tu^2) \mathrm{d}x$ remains unchanged (is a constant of motion) for the equation KdV. Attempt : Let $u$ be a solution of the KdV equation, thus satisfying : $$u_t + u_{xxx} + 6uu_x = 0$$ Let also $I = \int_{-\infty}^\infty (xu-3tu^2) \mathrm{d}x$ and then the derivative with respect to time, will be : $$\frac{\mathrm{d}I}{\mathrm{d}t}=\int_{-\infty}^\infty\frac{\partial}{\partial t}(xu-3tu^2)\mathrm{d}x=\int_{-\infty}^\infty(xu_t-3u^2-6tuu_t)\mathrm{d}x$$ $$=$$ $$\int_{-\infty}^\infty (-xu_{xxx} - 6xuu_x - 3u^2 + 6tuu_{xxx}+36tu^2u_x)\mathrm{d}x$$ How would one proceed now to show that $\frac{\mathrm{d}I}{\mathrm{d}x} = 0$, thus the expressions above are equal to $0$ ?","Exercise : Show that the integral $\int (xu-3tu^2) \mathrm{d}x$ remains unchanged (is a constant of motion) for the equation KdV. Attempt : Let $u$ be a solution of the KdV equation, thus satisfying : $$u_t + u_{xxx} + 6uu_x = 0$$ Let also $I = \int_{-\infty}^\infty (xu-3tu^2) \mathrm{d}x$ and then the derivative with respect to time, will be : $$\frac{\mathrm{d}I}{\mathrm{d}t}=\int_{-\infty}^\infty\frac{\partial}{\partial t}(xu-3tu^2)\mathrm{d}x=\int_{-\infty}^\infty(xu_t-3u^2-6tuu_t)\mathrm{d}x$$ $$=$$ $$\int_{-\infty}^\infty (-xu_{xxx} - 6xuu_x - 3u^2 + 6tuu_{xxx}+36tu^2u_x)\mathrm{d}x$$ How would one proceed now to show that $\frac{\mathrm{d}I}{\mathrm{d}x} = 0$, thus the expressions above are equal to $0$ ?",,"['integration', 'multivariable-calculus', 'partial-differential-equations', 'improper-integrals']"
31,"Are strictly concave functions robust to ""small changes""?","Are strictly concave functions robust to ""small changes""?",,"Let $f:\mathbb{R}^n\times \mathbb{R}^m\rightarrow \mathbb{R}$ be a continuous function. Suppose that there exists an $a_0\in\mathbb{R}^m$ such that $f\left(x,a_0\right)$ is strictly concave in $x$. Is it true that there exists a ball $A$ around $a_0$ such that $f\left(x,a\right)$ is concave in $x$ for $a\in A$?","Let $f:\mathbb{R}^n\times \mathbb{R}^m\rightarrow \mathbb{R}$ be a continuous function. Suppose that there exists an $a_0\in\mathbb{R}^m$ such that $f\left(x,a_0\right)$ is strictly concave in $x$. Is it true that there exists a ball $A$ around $a_0$ such that $f\left(x,a\right)$ is concave in $x$ for $a\in A$?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'convex-analysis']"
32,Surface integral using Gauss theorem,Surface integral using Gauss theorem,,"The exercise consists in determining $$\iint_Y \mathbf{F} \cdot \mathbf{N} \ \mathrm{d}S$$ where $$\mathbf{F}= (x^2 yz + x \sin z , x^2 + y(1 - \sin z ), x + y - xy z^2)$$ and $Y$ is the part of the conical surface $4x^2 + (y-1)^2 = z^2$ which lies between $z = 1$ and $z = 2$. The normal vector points away from the $z$-axis. I have tried to solve it using Gauss theorem, and I think this is the correct approach. Of course, the surface is not closed, and I need to add a ""lid"" and ""bottom"" to $Y$ to use Gauss theorem in the first place. Let $\sigma$ denote the lid, $\gamma$ the bottom and $K$ the whole surface. This yields $$\iint_Y \mathbf{F} \cdot \mathbf{N}  \ \mathrm{d}S + \iint_{\sigma} \mathbf{F} \cdot \mathbf{N} \ \mathrm{d}S + \iint_{\gamma} \mathbf{F} \cdot \mathbf{N} \ \mathrm{d}S =  \iiint_K \nabla \cdot \mathbf{F} \ \mathrm{d}x\mathrm{d}y\mathrm{d}z$$ where $\nabla \cdot \mathbf{F}$ is the divergence, which can be determined and it is $\nabla\cdot\mathbf{F} = 1$. This produces the triple integral $$\iiint_{4x^2 + (y-1)^2 \leq z^2} \ \mathrm{d}x\mathrm{d}y\mathrm{d}z$$ This is where I get stuck. I am unsure about how to solve the triple integral, and how to further solve the surface integrals for the lid and the bottom surface.","The exercise consists in determining $$\iint_Y \mathbf{F} \cdot \mathbf{N} \ \mathrm{d}S$$ where $$\mathbf{F}= (x^2 yz + x \sin z , x^2 + y(1 - \sin z ), x + y - xy z^2)$$ and $Y$ is the part of the conical surface $4x^2 + (y-1)^2 = z^2$ which lies between $z = 1$ and $z = 2$. The normal vector points away from the $z$-axis. I have tried to solve it using Gauss theorem, and I think this is the correct approach. Of course, the surface is not closed, and I need to add a ""lid"" and ""bottom"" to $Y$ to use Gauss theorem in the first place. Let $\sigma$ denote the lid, $\gamma$ the bottom and $K$ the whole surface. This yields $$\iint_Y \mathbf{F} \cdot \mathbf{N}  \ \mathrm{d}S + \iint_{\sigma} \mathbf{F} \cdot \mathbf{N} \ \mathrm{d}S + \iint_{\gamma} \mathbf{F} \cdot \mathbf{N} \ \mathrm{d}S =  \iiint_K \nabla \cdot \mathbf{F} \ \mathrm{d}x\mathrm{d}y\mathrm{d}z$$ where $\nabla \cdot \mathbf{F}$ is the divergence, which can be determined and it is $\nabla\cdot\mathbf{F} = 1$. This produces the triple integral $$\iiint_{4x^2 + (y-1)^2 \leq z^2} \ \mathrm{d}x\mathrm{d}y\mathrm{d}z$$ This is where I get stuck. I am unsure about how to solve the triple integral, and how to further solve the surface integrals for the lid and the bottom surface.",,"['multivariable-calculus', 'vector-analysis']"
33,Reconstruct a function $f: \mathbb{R}^3 \to \mathbb{R}$ from two identities on its partial derivatives,Reconstruct a function  from two identities on its partial derivatives,f: \mathbb{R}^3 \to \mathbb{R},"I'd like to find some non-constant function $f: \mathbb{R}^3 \to \mathbb{R}$ such that $$ \begin{cases}     \displaystyle \frac{ \partial f } { \partial y } = -2 \frac{ \partial f } { \partial x }, \\[4pt]     \displaystyle \frac{ \partial f } { \partial z } = - \frac x z \frac{ \partial f } { \partial x }. \end{cases} $$ If it helps, it can be assumed that $f$ is as smooth as needed. Given the nature of the underlying physical problem, I initially tried looking for solutions of the form $f(x,y,z) = ( a x + b y ) z^c$, to no avail (I get $f=0$). I then tried the more general form $f(x,y,z) = g(x,y) h(z)$, which did not pan out either. I tried more complicated forms using some symbolic computation software, but I could not find a suitable non-constant answer. I am starting to suspect that there are no non-constant functions satisfying these identities. However, I do not see how to go about proving that. My question is: can we find a non-constant function satisfying these identities? If not, how can we prove that the only solution would be a constant function?","I'd like to find some non-constant function $f: \mathbb{R}^3 \to \mathbb{R}$ such that $$ \begin{cases}     \displaystyle \frac{ \partial f } { \partial y } = -2 \frac{ \partial f } { \partial x }, \\[4pt]     \displaystyle \frac{ \partial f } { \partial z } = - \frac x z \frac{ \partial f } { \partial x }. \end{cases} $$ If it helps, it can be assumed that $f$ is as smooth as needed. Given the nature of the underlying physical problem, I initially tried looking for solutions of the form $f(x,y,z) = ( a x + b y ) z^c$, to no avail (I get $f=0$). I then tried the more general form $f(x,y,z) = g(x,y) h(z)$, which did not pan out either. I tried more complicated forms using some symbolic computation software, but I could not find a suitable non-constant answer. I am starting to suspect that there are no non-constant functions satisfying these identities. However, I do not see how to go about proving that. My question is: can we find a non-constant function satisfying these identities? If not, how can we prove that the only solution would be a constant function?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
34,Integrate $\pi (r^2-x^2)$,Integrate,\pi (r^2-x^2),We are told to integrate $\pi (r^2-x^2)$ from $-r$ to $r$ $$V=\int_{-r}^{r}\pi(r^2-x^2)dx=2\pi\int_0^r(r^2-x^2)dx=2\pi\left[r^2x-\frac{x^3}{3}\right]_0^r=2\pi\left(r^3-\frac{r^3}{3}\right)=\frac{4}{3}\pi r^3$$ why does the integral of $r^2$ equal $r^2 x$?,We are told to integrate $\pi (r^2-x^2)$ from $-r$ to $r$ $$V=\int_{-r}^{r}\pi(r^2-x^2)dx=2\pi\int_0^r(r^2-x^2)dx=2\pi\left[r^2x-\frac{x^3}{3}\right]_0^r=2\pi\left(r^3-\frac{r^3}{3}\right)=\frac{4}{3}\pi r^3$$ why does the integral of $r^2$ equal $r^2 x$?,,"['calculus', 'multivariable-calculus', 'definite-integrals']"
35,Show that $f_n$ is continuous at $0$,Show that  is continuous at,f_n 0,"The function in the Lemma $16.1$ is $$f(x)=\begin{cases}e^{-1/x}&\text{for }x > 0\\0&\text{for }x \leq 0\end{cases}$$ (a) To show that $a<e^a$ I have tried to look at the Taylor series of $e^x$ and thus come to the conclusion that $e^a=1+a+a^2/2!+a^3/3!+...$ and therefore $a<e^a$, is this fine? Using the help, we do $a=t/2n$ and so $\frac{t}{2n}<e^{t/2n}$ so $\frac{t^n}{(2n)^n}<e^{t/2}$ so $\frac{t^n}{e^t}<\frac{(2n)^n}{e^{t/2}}$, then, if $t=1/x$ then $\frac{1}{e^{1/x}x^n}< \frac{(2n)^n}{e^{1/2x}}$ and as $\frac{(2n)^n}{e^{1/2x}}\to 0$ if $x\to 0$ we conclude that $\lim_{x\to 0}f_n(x)=0=f_n(0)$. (b) $\lim_{x\to 0}\frac{f_n(x)-f_n(0)}{x}=\lim_{x\to 0}\frac{1}{e^{1/x}x^{n+1}}$, but I do not know how to calculate this limit, could I use what I did in (a)? ($\frac{1}{e^{1/x}x^n}< \frac{(2n)^n}{e^{1/2x}}$) (c) If $x\leq 0$ is clear, then consider $x>0$, from here we have that $f'_n(x)=\frac{x^{n-2}e^{-1/x}-e^{-1/x}nx^{n-1}}{x^{2n}}=x^{-n-2}e^{-1/x}-ne^{-1/x}x^{-n-1}=f_{n+2}(x)-nf_{n+1}(x)$ (d) How do I prove that $f$ is of class $C^{\infty}$? Can I use (c)? Thank you very much.","The function in the Lemma $16.1$ is $$f(x)=\begin{cases}e^{-1/x}&\text{for }x > 0\\0&\text{for }x \leq 0\end{cases}$$ (a) To show that $a<e^a$ I have tried to look at the Taylor series of $e^x$ and thus come to the conclusion that $e^a=1+a+a^2/2!+a^3/3!+...$ and therefore $a<e^a$, is this fine? Using the help, we do $a=t/2n$ and so $\frac{t}{2n}<e^{t/2n}$ so $\frac{t^n}{(2n)^n}<e^{t/2}$ so $\frac{t^n}{e^t}<\frac{(2n)^n}{e^{t/2}}$, then, if $t=1/x$ then $\frac{1}{e^{1/x}x^n}< \frac{(2n)^n}{e^{1/2x}}$ and as $\frac{(2n)^n}{e^{1/2x}}\to 0$ if $x\to 0$ we conclude that $\lim_{x\to 0}f_n(x)=0=f_n(0)$. (b) $\lim_{x\to 0}\frac{f_n(x)-f_n(0)}{x}=\lim_{x\to 0}\frac{1}{e^{1/x}x^{n+1}}$, but I do not know how to calculate this limit, could I use what I did in (a)? ($\frac{1}{e^{1/x}x^n}< \frac{(2n)^n}{e^{1/2x}}$) (c) If $x\leq 0$ is clear, then consider $x>0$, from here we have that $f'_n(x)=\frac{x^{n-2}e^{-1/x}-e^{-1/x}nx^{n-1}}{x^{2n}}=x^{-n-2}e^{-1/x}-ne^{-1/x}x^{-n-1}=f_{n+2}(x)-nf_{n+1}(x)$ (d) How do I prove that $f$ is of class $C^{\infty}$? Can I use (c)? Thank you very much.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'proof-verification', 'vector-analysis']"
36,What is the value of $\delta $ if $\epsilon=0.01$?,What is the value of  if ?,\delta  \epsilon=0.01,"Let $f(x,y) = \begin{cases} \frac{2x^2y+3xy^2}{x^2+y^2},  & \text{if  $(x,y)\neq(0,0)$} \\[2ex] 0, & \text{if $(x,y)=(0,0)$ } \end{cases}$ Then the condition on $\delta $ such that $\vert f(x,y)-f(0,0) \vert<0.01$ whenever $\sqrt {x^2+y^2}<\delta $ is- 1.$\delta <0.01$ 2.$\delta <0.001$ 3.$\delta <0.02$ 4.no such $\delta $ exists solution:since $f(0,0)=0$,then consider $\left\vert \frac{2x^2y+3xy^2}{x^2+y^2}-0\right\vert =\left\vert \frac{xy(2x+3y)}{x^2+y^2}\right\vert\le \frac{(2x+3y)}{2}$ as $xy\le \frac{x^2+y^2}{2}$ From here, how to proceed further...","Let $f(x,y) = \begin{cases} \frac{2x^2y+3xy^2}{x^2+y^2},  & \text{if  $(x,y)\neq(0,0)$} \\[2ex] 0, & \text{if $(x,y)=(0,0)$ } \end{cases}$ Then the condition on $\delta $ such that $\vert f(x,y)-f(0,0) \vert<0.01$ whenever $\sqrt {x^2+y^2}<\delta $ is- 1.$\delta <0.01$ 2.$\delta <0.001$ 3.$\delta <0.02$ 4.no such $\delta $ exists solution:since $f(0,0)=0$,then consider $\left\vert \frac{2x^2y+3xy^2}{x^2+y^2}-0\right\vert =\left\vert \frac{xy(2x+3y)}{x^2+y^2}\right\vert\le \frac{(2x+3y)}{2}$ as $xy\le \frac{x^2+y^2}{2}$ From here, how to proceed further...",,"['real-analysis', 'multivariable-calculus', 'continuity']"
37,Understanding Morse's Lemma,Understanding Morse's Lemma,,"Just to provide some context, I'm reading a proof of Morse's Lemma in a book called Topology and Geometry for Physicists, and it's not too difficult of a proof, but I don't understand one tiny part.  I'm goona try to phrase things in a way so that even if you don't know Morse's lemma, you may be able to see why the following statement is true (because I don't). Suppose $f: M \to \mathbb{R}$ is smooth, $0$ is a non degenerate critical point and we choose coordinates $(x_1, ... x_n)$ so that $$\frac{\partial^2 f}{\partial x^2_1}(0) \neq 0 \  \ \ \ {and} \ \ \ \ \ \ f(x) = \sum_{i =1}^n\sum_{j=1}^n{x_i x_j} h_{ij}$$ where $h_{11}(0) = \frac{\partial^2 f}{\partial x^2_j}(0)   \neq 0$ Since $h_{11}(0) \neq 0$ there exists a neighborhood of $0$, $N_0$ such that  $h_{11}(x) \neq 0$ on $N_0$ It is then claimed that if we let $$y_1 = \sqrt{|h_{11}(x_1, ... , x_n)|} \bigg[x_1 + \sum_{i=2}^n\frac{x_ih_{i1}}{h_{11}}\bigg]$$ then $(y_1, x_2, ... x_n)$ are also local coordinates for some open set $\hat{N_0} \subset N_0$ Woah ... how did that last line come about?","Just to provide some context, I'm reading a proof of Morse's Lemma in a book called Topology and Geometry for Physicists, and it's not too difficult of a proof, but I don't understand one tiny part.  I'm goona try to phrase things in a way so that even if you don't know Morse's lemma, you may be able to see why the following statement is true (because I don't). Suppose $f: M \to \mathbb{R}$ is smooth, $0$ is a non degenerate critical point and we choose coordinates $(x_1, ... x_n)$ so that $$\frac{\partial^2 f}{\partial x^2_1}(0) \neq 0 \  \ \ \ {and} \ \ \ \ \ \ f(x) = \sum_{i =1}^n\sum_{j=1}^n{x_i x_j} h_{ij}$$ where $h_{11}(0) = \frac{\partial^2 f}{\partial x^2_j}(0)   \neq 0$ Since $h_{11}(0) \neq 0$ there exists a neighborhood of $0$, $N_0$ such that  $h_{11}(x) \neq 0$ on $N_0$ It is then claimed that if we let $$y_1 = \sqrt{|h_{11}(x_1, ... , x_n)|} \bigg[x_1 + \sum_{i=2}^n\frac{x_ih_{i1}}{h_{11}}\bigg]$$ then $(y_1, x_2, ... x_n)$ are also local coordinates for some open set $\hat{N_0} \subset N_0$ Woah ... how did that last line come about?",,"['multivariable-calculus', 'smooth-manifolds', 'morse-theory']"
38,Calculate $\text{div} (f)$,Calculate,\text{div} (f),"Let $A=(a_{ij})$ be a positive definite real Hermitian $N\times N$-matrix. Consider $$f(x)=\frac{x}{|x|^2}e^{{-\frac{1}{2}}\langle Ax,x\rangle },\;\forall x\in \mathbb{R}^N\backslash\{0\}.$$ Why   $$\text{div} (f)(x)=\left(\frac{(N-2)}{|x|^2}-\frac{\langle Ax,x\rangle}{|x|^2} \right)e^{{-\frac{1}{2}}\langle Ax,x\rangle}?$$ Notice that $$\text{div} (f)(x)=\sum_{i=1}^N \frac{\partial f}{\partial x_i}(x).$$","Let $A=(a_{ij})$ be a positive definite real Hermitian $N\times N$-matrix. Consider $$f(x)=\frac{x}{|x|^2}e^{{-\frac{1}{2}}\langle Ax,x\rangle },\;\forall x\in \mathbb{R}^N\backslash\{0\}.$$ Why   $$\text{div} (f)(x)=\left(\frac{(N-2)}{|x|^2}-\frac{\langle Ax,x\rangle}{|x|^2} \right)e^{{-\frac{1}{2}}\langle Ax,x\rangle}?$$ Notice that $$\text{div} (f)(x)=\sum_{i=1}^N \frac{\partial f}{\partial x_i}(x).$$",,"['multivariable-calculus', 'partial-derivative', 'vector-analysis']"
39,Meaning of cross terms in multivariable Taylor expansion,Meaning of cross terms in multivariable Taylor expansion,,"The cross terms in the Taylor expansion of $f = f(x,y)$ in $(x_0,y_0)$  $$ f(x,y) = f(x_0,y_0) + \ldots + \frac{1}{2!}\bigg( \frac{\partial^2 f}{\partial x^2} (\Delta x)^2 + \color{green}{2\frac{\partial^2 f}{\partial x \partial y} \Delta x \Delta y} + \frac{\partial^2 f}{\partial y^2}(\Delta y)^2 \bigg) + \ldots \tag{1} $$ can be seen as to have arosen from the cross terms of  $$ f(x,y) = \sum_{n=0}^\infty \frac{1}{n!} \bigg[ \bigg( \Delta x \frac{\partial}{\partial x} + \Delta y \frac{\partial}{\partial y}\bigg)^n \ f(x,y)\bigg]_{x_0,y_0} , \tag{2} $$  where we consider the partial derivatives to only operate on $f$. I'm missing the interpretation of these cross terms. $(2)$ somewhat explains where the (algebraic structure of) these cross terms comes from, but it doesn't give me any insight as to why we need to consider the product of two changes. Moreover, I also don't grasp how my text arrives at $(2)$. Why are the cross terms included? I understand the operation of mixed derivatives in terms of calculations, but I want to know how I can interpret them. Furthermore, I also understand the concept of stationary points of multi-variable functions. I don't think the concept of stationary points are relevant here. I'm not necessarily looking for a geometric interpretation; as long as the relevance of the mixed partial derivative terms is made evident.","The cross terms in the Taylor expansion of $f = f(x,y)$ in $(x_0,y_0)$  $$ f(x,y) = f(x_0,y_0) + \ldots + \frac{1}{2!}\bigg( \frac{\partial^2 f}{\partial x^2} (\Delta x)^2 + \color{green}{2\frac{\partial^2 f}{\partial x \partial y} \Delta x \Delta y} + \frac{\partial^2 f}{\partial y^2}(\Delta y)^2 \bigg) + \ldots \tag{1} $$ can be seen as to have arosen from the cross terms of  $$ f(x,y) = \sum_{n=0}^\infty \frac{1}{n!} \bigg[ \bigg( \Delta x \frac{\partial}{\partial x} + \Delta y \frac{\partial}{\partial y}\bigg)^n \ f(x,y)\bigg]_{x_0,y_0} , \tag{2} $$  where we consider the partial derivatives to only operate on $f$. I'm missing the interpretation of these cross terms. $(2)$ somewhat explains where the (algebraic structure of) these cross terms comes from, but it doesn't give me any insight as to why we need to consider the product of two changes. Moreover, I also don't grasp how my text arrives at $(2)$. Why are the cross terms included? I understand the operation of mixed derivatives in terms of calculations, but I want to know how I can interpret them. Furthermore, I also understand the concept of stationary points of multi-variable functions. I don't think the concept of stationary points are relevant here. I'm not necessarily looking for a geometric interpretation; as long as the relevance of the mixed partial derivative terms is made evident.",,"['multivariable-calculus', 'taylor-expansion', 'partial-derivative']"
40,Find the volume of the solid bounded by $x=\sqrt{y^2+z^2}$ and $x=6-y^2-z^2$,Find the volume of the solid bounded by  and,x=\sqrt{y^2+z^2} x=6-y^2-z^2,"We are given an image: The left hand cone is $x=\sqrt{y^2+z^2}$, and right hand parabola is $x=6-y^2-z^2$ I see that I can fix $x$. $\sqrt{y^2+z^2}\leq x \leq 6-y^2-z^2$ Should I use cylindrical or spherical coordinates here? Or none? It is not obvious to me if I should be or not. Because the projection on the $yz$ plane has square roots in it, and so I think I'm supposed to be using cylindrical or spherical. Use a triple integral","We are given an image: The left hand cone is $x=\sqrt{y^2+z^2}$, and right hand parabola is $x=6-y^2-z^2$ I see that I can fix $x$. $\sqrt{y^2+z^2}\leq x \leq 6-y^2-z^2$ Should I use cylindrical or spherical coordinates here? Or none? It is not obvious to me if I should be or not. Because the projection on the $yz$ plane has square roots in it, and so I think I'm supposed to be using cylindrical or spherical. Use a triple integral",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
41,Transformation mapping vector to a gradient is diffeomorphism,Transformation mapping vector to a gradient is diffeomorphism,,"$f:U\to \mathbf{R}$ is $C^2$, $U$ is open subset of $\mathbf{R}^n$, for every $u\in U$ $D^2f(u)$ is positive definite. How do I show that the map $x\mapsto \nabla f(x)$ is a diffeomorphism?","$f:U\to \mathbf{R}$ is $C^2$, $U$ is open subset of $\mathbf{R}^n$, for every $u\in U$ $D^2f(u)$ is positive definite. How do I show that the map $x\mapsto \nabla f(x)$ is a diffeomorphism?",,"['real-analysis', 'multivariable-calculus']"
42,Partial Derivatives : Show that $\frac{∂x}{∂y}\frac{∂y}{∂z}\frac{∂z}{∂x}=-1$ [duplicate],Partial Derivatives : Show that  [duplicate],\frac{∂x}{∂y}\frac{∂y}{∂z}\frac{∂z}{∂x}=-1,"This question already has an answer here : Prove that $\frac{\partial x}{\partial y} \frac{\partial y}{\partial z} \frac{\partial z}{\partial x} = -1$ and verify ideal gas law (1 answer) Closed 6 years ago . Let $f : \mathbb{R}^3 \rightarrow \mathbb{R}$. How can i show that If $f(x,y,z)=0$ then $$\frac{∂x}{∂y}\frac{∂y}{∂z}\frac{∂z}{∂x}=-1 $$ Any help will be appreciated.","This question already has an answer here : Prove that $\frac{\partial x}{\partial y} \frac{\partial y}{\partial z} \frac{\partial z}{\partial x} = -1$ and verify ideal gas law (1 answer) Closed 6 years ago . Let $f : \mathbb{R}^3 \rightarrow \mathbb{R}$. How can i show that If $f(x,y,z)=0$ then $$\frac{∂x}{∂y}\frac{∂y}{∂z}\frac{∂z}{∂x}=-1 $$ Any help will be appreciated.",,"['calculus', 'multivariable-calculus', 'partial-derivative', 'chain-rule']"
43,The matrix identity $\nabla_A \text{tr}AB = B^T$ when A is symmetric,The matrix identity  when A is symmetric,\nabla_A \text{tr}AB = B^T,"Suppose $A,B \in \mathbb{R}^{n \times n}$, $f: \mathbb{R}^{n \times n} \to \mathbb{R}$. Define $\nabla_A f(A)) \in \mathbb{R}^{n \times n}$,  where $(\nabla_A f(A))_{ij} = \frac{\partial{f(A)}}{\partial{A_{ij}}} $. Consider the equation $\nabla_A \text{tr}AB = B^T$, according to this note . This can be proved as  $$ \begin{align} f(A) = \text{tr}AB  &=\sum_{i=1}^nA_{1i}B_{i1} +\sum_{i=1}^nA_{2i}B_{i2} + \cdots + \sum_{i=1}^nA_{ni}B_{in} \\ &= \sum_{j=1}^{n}\sum_{i=1}^{n} A_{ji}B_{ij}\\ \end{align} $$ If we assume A is symmetric matrix, I'm confused whether the following is right because it contradicts with $\nabla_A \text{tr}AB = B^T$. $\nabla_A \text{tr}AB = B + B^T$ because $A_{ij} = A_{ji}$ and $\frac{\partial{f(A)}}{\partial{A_{ij}}} = B_{ij} + B_{ji}$","Suppose $A,B \in \mathbb{R}^{n \times n}$, $f: \mathbb{R}^{n \times n} \to \mathbb{R}$. Define $\nabla_A f(A)) \in \mathbb{R}^{n \times n}$,  where $(\nabla_A f(A))_{ij} = \frac{\partial{f(A)}}{\partial{A_{ij}}} $. Consider the equation $\nabla_A \text{tr}AB = B^T$, according to this note . This can be proved as  $$ \begin{align} f(A) = \text{tr}AB  &=\sum_{i=1}^nA_{1i}B_{i1} +\sum_{i=1}^nA_{2i}B_{i2} + \cdots + \sum_{i=1}^nA_{ni}B_{in} \\ &= \sum_{j=1}^{n}\sum_{i=1}^{n} A_{ji}B_{ij}\\ \end{align} $$ If we assume A is symmetric matrix, I'm confused whether the following is right because it contradicts with $\nabla_A \text{tr}AB = B^T$. $\nabla_A \text{tr}AB = B + B^T$ because $A_{ij} = A_{ji}$ and $\frac{\partial{f(A)}}{\partial{A_{ij}}} = B_{ij} + B_{ji}$",,"['linear-algebra', 'multivariable-calculus', 'vector-analysis', 'matrix-calculus']"
44,Evaluating double integral.,Evaluating double integral.,,"Find the double integral $$I=\iint_D y dy dx$$ where $D$ is area bounded by $$D= \{(x,y): x^2+y^2\leq 1, x^2+y^2\leq2x, y\leq0 \}$$ First off, function I am evaluating is $z=y$ and it's just one plane in $\mathbb{R}^3$ . Now, this is the $D$ I am looking for: Now, I should integrate over the intersection of these two circles where $y<0$ . Obviously, I should use polar coordinates so $$x=r\cos(\theta)\\ y=r\sin(\theta)$$ , Jacobian is $r$ so it remains now to find the bounds of integration, however I am not quite sure how to do that, since the intersection point of two circles is $$(x,y)=\left(\frac{1}{2}, -\frac{\sqrt3}{2}\right)$$ I suppose that angle should go from $-\frac{\pi}{6}$ to $0$ but then, I could write out the equations of circles to get bounds for $r$ , where $r$ should go from the blue circle to red circle, which means $$r \in [2\cos\theta, 1]$$ , but I am not quite sure is that correct way to do it because when i calculate it this way I get the positive result $$I=\frac{5}{12}$$ , which is not what I expected since I am integrating over an area where $x$ is positive and $y$ is negative which means that I am actually finding a volume of body that's either in fourth or eighth octant, but, when I take a look at the function I am integrating I see that $z=y$ meaning that sign of $y$ will determine the sign of $z$ so I am finding a volume of a body that's located in eighth octant, so I expected a negative value here. Obviously, something is wrong, it might be $D$ or bounds of integration or calculations or ,the worst case probably, my reasoning. Any help is appreciated.","Find the double integral where is area bounded by First off, function I am evaluating is and it's just one plane in . Now, this is the I am looking for: Now, I should integrate over the intersection of these two circles where . Obviously, I should use polar coordinates so , Jacobian is so it remains now to find the bounds of integration, however I am not quite sure how to do that, since the intersection point of two circles is I suppose that angle should go from to but then, I could write out the equations of circles to get bounds for , where should go from the blue circle to red circle, which means , but I am not quite sure is that correct way to do it because when i calculate it this way I get the positive result , which is not what I expected since I am integrating over an area where is positive and is negative which means that I am actually finding a volume of body that's either in fourth or eighth octant, but, when I take a look at the function I am integrating I see that meaning that sign of will determine the sign of so I am finding a volume of a body that's located in eighth octant, so I expected a negative value here. Obviously, something is wrong, it might be or bounds of integration or calculations or ,the worst case probably, my reasoning. Any help is appreciated.","I=\iint_D y dy dx D D= \{(x,y): x^2+y^2\leq 1, x^2+y^2\leq2x, y\leq0 \} z=y \mathbb{R}^3 D y<0 x=r\cos(\theta)\\ y=r\sin(\theta) r (x,y)=\left(\frac{1}{2}, -\frac{\sqrt3}{2}\right) -\frac{\pi}{6} 0 r r r \in [2\cos\theta, 1] I=\frac{5}{12} x y z=y y z D","['integration', 'multivariable-calculus']"
45,How to get the Jacobian for Double Integrals,How to get the Jacobian for Double Integrals,,"So in my textbook, it has the following equation for when you are changing the variables in a double integral where $x=g(u, v)$ and $y=h(u, v)$ . \begin{align} \int \int_R f(x,y) dA = \int \int_S f(g(u, v), h(u, v)) \left|\frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial y}{\partial u} \frac{\partial x}{\partial v}\right| du dv \end{align} I understand that the following is the Jacobian, $\displaystyle \left|\frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial y}{\partial u} \frac{\partial x}{\partial v}\right|.$ The textbook also says that we get the above Jacobian from this determinant: $\begin{array}{|ccc|} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{array}.$ Why is the Jacobian this determinant? I don't understand where this determinant is coming from.","So in my textbook, it has the following equation for when you are changing the variables in a double integral where and . I understand that the following is the Jacobian, The textbook also says that we get the above Jacobian from this determinant: Why is the Jacobian this determinant? I don't understand where this determinant is coming from.","x=g(u, v) y=h(u, v) \begin{align}
\int \int_R f(x,y) dA = \int \int_S f(g(u, v), h(u, v)) \left|\frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial y}{\partial u} \frac{\partial x}{\partial v}\right| du dv
\end{align} \displaystyle \left|\frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial y}{\partial u} \frac{\partial x}{\partial v}\right|. \begin{array}{|ccc|} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{array}.","['multivariable-calculus', 'determinant', 'jacobian']"
46,Show that a smooth map $f:\mathbb{R}^m\to \mathbb{R}^n$ for $m>n$ cannot be injective,Show that a smooth map  for  cannot be injective,f:\mathbb{R}^m\to \mathbb{R}^n m>n,"Some caveats: I am looking for a solution that uses smoothness. I know a way to prove it assuming only Borsuk Ulam with only continuity assumed. I was hoping for a more elementary solution that uses differentiability specifically. Some things I have tried: I wanted to relate the problem to linear algebra somehow, maybe with some sort of partial converse to the inverse function theorem. In the examples I can think of where this would fail (say $f(x)=x^3$ with trivial linearization at $0$) there aren't many ""bad"" points. I am not sure how to pick the good ones however. Next, I tried something using Sard's theorem to find some regular values, and an $m-n$ dimensional manifold, after which the conclusion would be immediate. However, there is no reason to believe that these regular points should be in the image of $f$, i.e. a map  $$ f:\mathbb{R}^3\to \mathbb{R}^2 $$ taking everything to the real axis. Any thoughts and help would be appreciated.","Some caveats: I am looking for a solution that uses smoothness. I know a way to prove it assuming only Borsuk Ulam with only continuity assumed. I was hoping for a more elementary solution that uses differentiability specifically. Some things I have tried: I wanted to relate the problem to linear algebra somehow, maybe with some sort of partial converse to the inverse function theorem. In the examples I can think of where this would fail (say $f(x)=x^3$ with trivial linearization at $0$) there aren't many ""bad"" points. I am not sure how to pick the good ones however. Next, I tried something using Sard's theorem to find some regular values, and an $m-n$ dimensional manifold, after which the conclusion would be immediate. However, there is no reason to believe that these regular points should be in the image of $f$, i.e. a map  $$ f:\mathbb{R}^3\to \mathbb{R}^2 $$ taking everything to the real axis. Any thoughts and help would be appreciated.",,"['real-analysis', 'multivariable-calculus', 'differential-topology', 'geometric-topology']"
47,Working out the limits of a triple integral - a volume bounded by a plane in the first octant,Working out the limits of a triple integral - a volume bounded by a plane in the first octant,,"I'm attempting to do the following exercise: And am now fairly aware now that the hardest part about solving these types of problems is understanding the bounds you are integrating in. Firstly, I try and draw a horrendous sketch of what I believe we're integrating in. I note: Intercepts: $x = h/a$, $z=h$, $y=h/b$, found by setting two variables equal to $0$ and solving for the other one. With $x=0$, $z = -by + h$. With $y=0$, $z = -ax + h$. With $z=0$, $y = -ax/b + h/b$ Keeping this in mind, here is the sketch I drew out what I think this is: Now, where the confusing part comes in (assuming I'm making sense so far). I need to note the order of integration I'd find would be most straightforward. My plan of attack is to make a line from the ends of $x$ within the object at $z=0$, which would involve integrating $x$ first, then moving it along $y$, so that it feels the trapezoid-looking figure at $z=0$. Then, I'll integrate in $z$, and have the entire volume filled. So, I'll try and establish some limits. For $x$, our first line goes from $by = -ax +h$, which arranges for $x$ to $x=-\frac{b}{a}y+h/a$ to the minimum value of $x$, which is the intercept $x=h/a$. For $y$, our line is scaled in $y$ from the $xz$ plane to the $yz$ plane to fill the area of the base of this figure. Finally, for $z$ our volume is made from scaling the area of the trapezoid from 0 to $h$. Thus, our limits are: $$-(\frac{b}{a}y+h/a) \le x \le h/a$$ and for $y$.. according to my thinking, it integrates from $z = -ax+h$ to $z = -by + h$ which means one of my limits cannot be expressed in terms of $y$ and $z$ which means I can't get a integrate the final $z$ integral properly since I'll have an $x$ term in it. Where am I going wrong? Is the sketch wrong? Are my thought processes wrong? If so, which? I have a feeling my sketch is wrong, as this object doesn't exactly strike me as a plane. Also, the bottom figure is not necessarily a trapezoid, as $b \ne a$ necessarily.","I'm attempting to do the following exercise: And am now fairly aware now that the hardest part about solving these types of problems is understanding the bounds you are integrating in. Firstly, I try and draw a horrendous sketch of what I believe we're integrating in. I note: Intercepts: $x = h/a$, $z=h$, $y=h/b$, found by setting two variables equal to $0$ and solving for the other one. With $x=0$, $z = -by + h$. With $y=0$, $z = -ax + h$. With $z=0$, $y = -ax/b + h/b$ Keeping this in mind, here is the sketch I drew out what I think this is: Now, where the confusing part comes in (assuming I'm making sense so far). I need to note the order of integration I'd find would be most straightforward. My plan of attack is to make a line from the ends of $x$ within the object at $z=0$, which would involve integrating $x$ first, then moving it along $y$, so that it feels the trapezoid-looking figure at $z=0$. Then, I'll integrate in $z$, and have the entire volume filled. So, I'll try and establish some limits. For $x$, our first line goes from $by = -ax +h$, which arranges for $x$ to $x=-\frac{b}{a}y+h/a$ to the minimum value of $x$, which is the intercept $x=h/a$. For $y$, our line is scaled in $y$ from the $xz$ plane to the $yz$ plane to fill the area of the base of this figure. Finally, for $z$ our volume is made from scaling the area of the trapezoid from 0 to $h$. Thus, our limits are: $$-(\frac{b}{a}y+h/a) \le x \le h/a$$ and for $y$.. according to my thinking, it integrates from $z = -ax+h$ to $z = -by + h$ which means one of my limits cannot be expressed in terms of $y$ and $z$ which means I can't get a integrate the final $z$ integral properly since I'll have an $x$ term in it. Where am I going wrong? Is the sketch wrong? Are my thought processes wrong? If so, which? I have a feeling my sketch is wrong, as this object doesn't exactly strike me as a plane. Also, the bottom figure is not necessarily a trapezoid, as $b \ne a$ necessarily.",,"['integration', 'multivariable-calculus', 'definite-integrals', 'volume']"
48,Why is the outer unit normal unique for $C^k$ boundaries,Why is the outer unit normal unique for  boundaries,C^k,"Let $U$ be a bounded open set with $C^k$-boundary, i.e. there is a $C^k$-function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ s.t. $$U=\{x\in\mathbb{R}^n:f(x)>0\}, \quad \partial U=\{x\in\mathbb{R}^n:f(x)=0\} \ \ \text{and} \ \ \nabla f(x)\neq 0, \ x\in\partial U. $$ We can define outer unit normal $\nu$ at the boundary $\partial U$ as $$\nu(x)=-\frac{\nabla f(x)}{|\nabla f(x)|}, \ x\in\partial U.$$ My question is, why $\nu$ is well-defined, i.e. independent of the choice of $f$? If there is another function $g:\mathbb{R}^n\rightarrow\mathbb{R}$ satisfying the same conditions as $f$, does it follow that $\nu_f=\nu_g$? Can this be generalized for less smooth boundaries? All help is welcome.","Let $U$ be a bounded open set with $C^k$-boundary, i.e. there is a $C^k$-function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ s.t. $$U=\{x\in\mathbb{R}^n:f(x)>0\}, \quad \partial U=\{x\in\mathbb{R}^n:f(x)=0\} \ \ \text{and} \ \ \nabla f(x)\neq 0, \ x\in\partial U. $$ We can define outer unit normal $\nu$ at the boundary $\partial U$ as $$\nu(x)=-\frac{\nabla f(x)}{|\nabla f(x)|}, \ x\in\partial U.$$ My question is, why $\nu$ is well-defined, i.e. independent of the choice of $f$? If there is another function $g:\mathbb{R}^n\rightarrow\mathbb{R}$ satisfying the same conditions as $f$, does it follow that $\nu_f=\nu_g$? Can this be generalized for less smooth boundaries? All help is welcome.",,"['real-analysis', 'multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'vector-analysis']"
49,Computing Hessian using matrix notation efficiently,Computing Hessian using matrix notation efficiently,,"I answered this question , but I'd like to understand more details about the matrix notation behind it (and that's why I'm making another post). We have $f:\Bbb R^n\to \Bbb R$ given by $$f(\theta) \doteq \alpha e^{-\beta \theta^\top\theta}, $$alright. We want to compute the bilinear map ${\rm Hess} f (\theta)$. Since I recognize $g (\theta)\doteq\theta^\top \theta$ as $\langle \theta,\theta\rangle$ (of course, $\langle \cdot,\cdot\rangle$ denotes the usual scalar product), I see that $$Dg(\theta) = 2\langle \theta, \cdot \rangle = 2\theta^\top, $$and hence $\nabla g (\theta) = 2\theta $. Then chain rule gives $$\nabla f (\theta) = -2\alpha \beta e^{-\beta \theta^\top \theta}\theta $$as the OP of the linked question states, so far so good. I'm having trouble doing something similar to check that $${\rm Hess}f (\theta)=2\alpha \beta e^{-\beta \theta^\top\theta}(2\beta \color{blue}{\theta\theta^\top}-{\rm Id}_n).$$I do not want to use components as I did there. A simple attempt is to use the product rule together with ${\rm d}\theta ={\rm Id}_n $. Differentiating the expression for $\nabla f (\theta) $ we get $$-2\alpha\beta (e^{-\beta\theta^\top\theta}(-2\beta \theta^\top)\theta +e^{-\beta \theta^\top\theta}{\rm Id}_n) = 2\alpha \beta e^{-\beta \theta^\top\theta}(2\beta\color{red}{\theta^\top\theta}-{\rm Id}_n), $$but this doesn't compile, and I can't see why the order comes out wrong. So I'd like to know exactly what identification am I missing here. I also recognize $\theta\theta^\top$ as the matrix of the bilinear map $\theta \otimes \theta$, and I'm comfortable with tensor products, so you can come in with guns blazing, if needed. Thanks.","I answered this question , but I'd like to understand more details about the matrix notation behind it (and that's why I'm making another post). We have $f:\Bbb R^n\to \Bbb R$ given by $$f(\theta) \doteq \alpha e^{-\beta \theta^\top\theta}, $$alright. We want to compute the bilinear map ${\rm Hess} f (\theta)$. Since I recognize $g (\theta)\doteq\theta^\top \theta$ as $\langle \theta,\theta\rangle$ (of course, $\langle \cdot,\cdot\rangle$ denotes the usual scalar product), I see that $$Dg(\theta) = 2\langle \theta, \cdot \rangle = 2\theta^\top, $$and hence $\nabla g (\theta) = 2\theta $. Then chain rule gives $$\nabla f (\theta) = -2\alpha \beta e^{-\beta \theta^\top \theta}\theta $$as the OP of the linked question states, so far so good. I'm having trouble doing something similar to check that $${\rm Hess}f (\theta)=2\alpha \beta e^{-\beta \theta^\top\theta}(2\beta \color{blue}{\theta\theta^\top}-{\rm Id}_n).$$I do not want to use components as I did there. A simple attempt is to use the product rule together with ${\rm d}\theta ={\rm Id}_n $. Differentiating the expression for $\nabla f (\theta) $ we get $$-2\alpha\beta (e^{-\beta\theta^\top\theta}(-2\beta \theta^\top)\theta +e^{-\beta \theta^\top\theta}{\rm Id}_n) = 2\alpha \beta e^{-\beta \theta^\top\theta}(2\beta\color{red}{\theta^\top\theta}-{\rm Id}_n), $$but this doesn't compile, and I can't see why the order comes out wrong. So I'd like to know exactly what identification am I missing here. I also recognize $\theta\theta^\top$ as the matrix of the bilinear map $\theta \otimes \theta$, and I'm comfortable with tensor products, so you can come in with guns blazing, if needed. Thanks.",,"['multivariable-calculus', 'matrix-calculus', 'hessian-matrix']"
50,Lagrange Multiplier when one variable is equal to zero?,Lagrange Multiplier when one variable is equal to zero?,,"I want to solve a Lagrange multiplier problem, $$f(x,y) = x^2+y^2+2x+1$$ $$g(x,y)=x^2+y^2-16 $$ Where function $g$ is my constraint. $$f_x=2x+2, \ \ \ f_y=2y, \ \ \ g_x=2x\lambda, \ \ \ g_y=2y\lambda$$ $$ \begin{cases}  2x+2=2x\lambda \\ 2y=2y\lambda \\ x^2+y^2-16=0 \end{cases} $$ See, this is a very nasty system of equations. At any rate, I get $\lambda = 1$ because in this case, $y=0$. So I cannot do anything with this as far as algebra is concerned? How do I resolve a problem like this?","I want to solve a Lagrange multiplier problem, $$f(x,y) = x^2+y^2+2x+1$$ $$g(x,y)=x^2+y^2-16 $$ Where function $g$ is my constraint. $$f_x=2x+2, \ \ \ f_y=2y, \ \ \ g_x=2x\lambda, \ \ \ g_y=2y\lambda$$ $$ \begin{cases}  2x+2=2x\lambda \\ 2y=2y\lambda \\ x^2+y^2-16=0 \end{cases} $$ See, this is a very nasty system of equations. At any rate, I get $\lambda = 1$ because in this case, $y=0$. So I cannot do anything with this as far as algebra is concerned? How do I resolve a problem like this?",,"['multivariable-calculus', 'optimization', 'partial-derivative', 'lagrange-multiplier']"
51,Proof of formula for area enclosed by parametric curve,Proof of formula for area enclosed by parametric curve,,"Suppose that $\theta \in [0,2\pi]$ and $(x(\theta), y(\theta))$ define a closed parametric curve. There is a formula that says the area enclosed by this curve is equal to $\frac{1}{2} \int\limits_0^{2\pi} (x\frac{dy}{d\theta}-y\frac{dx}{d\theta}) d\theta$. I know that this is a simple consequence of Green's Theorem however I would like to know if there is a proof of this statement that does not rely on it. All the proofs I can find online assume that we can actually parametrise our curve by $x$ as well as $\theta$ but this is not the case for many curves defined parametrically so is there a way of getting around that?","Suppose that $\theta \in [0,2\pi]$ and $(x(\theta), y(\theta))$ define a closed parametric curve. There is a formula that says the area enclosed by this curve is equal to $\frac{1}{2} \int\limits_0^{2\pi} (x\frac{dy}{d\theta}-y\frac{dx}{d\theta}) d\theta$. I know that this is a simple consequence of Green's Theorem however I would like to know if there is a proof of this statement that does not rely on it. All the proofs I can find online assume that we can actually parametrise our curve by $x$ as well as $\theta$ but this is not the case for many curves defined parametrically so is there a way of getting around that?",,"['multivariable-calculus', 'area', 'parametric']"
52,Questions on Nonlinear Elliptic Theory by Schauder,Questions on Nonlinear Elliptic Theory by Schauder,,"I recently started to study about Elliptic theory and below is a brief introduction  my professor made: Let $\;u:\mathbb R^n \to \mathbb R\;$ and $\;f:\mathbb R \to \mathbb  R\;$ two functions which satisfy the following: $\;\Delta u=f(u(x))\;$ Boundary conditions over $\;\Omega\;$=open,bounded subset of $\;\mathbb R^n\;$ If $\;f\in C^\alpha\;$ then $\;u\in C^{2+\alpha}\;$. In addition   $\;{\vert u \vert}_{C^{2,\alpha}(\Omega)} \le K {\vert f  \vert}_{C^\alpha(\Omega)}\;$ where $\;K\;$ is a positive constant. He also mentioned that these estimates are due to Schauder and explained to me the ""bootstrap"" argument. Questions: I would like to study this result in more details but I can't find this Theorem anywhere. Are there any suggestions of books that might be helpful here? I was wondering if I could use the above in this system of equations: $\;\Delta u_i=f_{u_i}(u(x))\;$ where $\;f_{u_i}=\frac {\partial f}{\partial u_i}(u)\;\;\;\forall 1\le i \le m\;$ and $\;u:\mathbb R^n \to \mathbb R^m\;$. The fact that I have $\;f_{u_i}(u(x))\;$ instead of $\;f_{u_i}(u_i(x))\;$ confuses me a lot. EDIT: After the suggestion in the answer below I searched on Gilbarg & Trudinger 's book and I came across with this Theorem: It seems to me it's quite close to the introduction my professor made. Although the extra term $\;{\vert u \vert}_{0;B_2}\;$ confuses me a lot. I tried to read about the norms and the notation of this chapter but I'm having a really hard time getting my head around them. I would appreciate if somebody could enlighten me about these. Is Theorem 4.6 the right one? Any help would be valuable. Thanks in advance!","I recently started to study about Elliptic theory and below is a brief introduction  my professor made: Let $\;u:\mathbb R^n \to \mathbb R\;$ and $\;f:\mathbb R \to \mathbb  R\;$ two functions which satisfy the following: $\;\Delta u=f(u(x))\;$ Boundary conditions over $\;\Omega\;$=open,bounded subset of $\;\mathbb R^n\;$ If $\;f\in C^\alpha\;$ then $\;u\in C^{2+\alpha}\;$. In addition   $\;{\vert u \vert}_{C^{2,\alpha}(\Omega)} \le K {\vert f  \vert}_{C^\alpha(\Omega)}\;$ where $\;K\;$ is a positive constant. He also mentioned that these estimates are due to Schauder and explained to me the ""bootstrap"" argument. Questions: I would like to study this result in more details but I can't find this Theorem anywhere. Are there any suggestions of books that might be helpful here? I was wondering if I could use the above in this system of equations: $\;\Delta u_i=f_{u_i}(u(x))\;$ where $\;f_{u_i}=\frac {\partial f}{\partial u_i}(u)\;\;\;\forall 1\le i \le m\;$ and $\;u:\mathbb R^n \to \mathbb R^m\;$. The fact that I have $\;f_{u_i}(u(x))\;$ instead of $\;f_{u_i}(u_i(x))\;$ confuses me a lot. EDIT: After the suggestion in the answer below I searched on Gilbarg & Trudinger 's book and I came across with this Theorem: It seems to me it's quite close to the introduction my professor made. Although the extra term $\;{\vert u \vert}_{0;B_2}\;$ confuses me a lot. I tried to read about the norms and the notation of this chapter but I'm having a really hard time getting my head around them. I would appreciate if somebody could enlighten me about these. Is Theorem 4.6 the right one? Any help would be valuable. Thanks in advance!",,"['multivariable-calculus', 'partial-differential-equations', 'regularity-theory-of-pdes', 'elliptic-equations', 'elliptic-operators']"
53,"Derivative of evaluation along diagonal $g(x) := f(x,x)$",Derivative of evaluation along diagonal,"g(x) := f(x,x)","Let $f:\mathbb{R}^2 \to \mathbb{R}$ be a differentiable function. Define $g(x) := f(x,x)$. I want to prove that $g$ is differentiable as well and that $$Dg(x)h = Df(x,x)(h,h) = [\partial_1f(x,x) + \partial_2f(x,x)]h.$$ I was trying to prove the fact by directly using the definition of differentiability for functions of one variable. We have \begin{align*} g(x+h) - g(x) &= f(x+h,x+h) - f(x,x)\\ &= f(x+h,x+h) - f(x+h,x) + f(x+h,x) - f(x,x)\\ \end{align*} Now, if we devide by $h$ \begin{align*} \frac{f(x+h,x+h) - f(x+h,x)}{h} + \frac{f(x+h,x) - f(x,x)}{h}\\ \end{align*} and take the limit $h \to 0$ we get \begin{align*} \lim_{h \to 0} \frac{f(x+h,x+h) - f(x+h,x)}{h} + \partial_1f(x,x). \end{align*} Here I'm stuck. How can I show that the first limit equals $\partial_2f(x,x)$?","Let $f:\mathbb{R}^2 \to \mathbb{R}$ be a differentiable function. Define $g(x) := f(x,x)$. I want to prove that $g$ is differentiable as well and that $$Dg(x)h = Df(x,x)(h,h) = [\partial_1f(x,x) + \partial_2f(x,x)]h.$$ I was trying to prove the fact by directly using the definition of differentiability for functions of one variable. We have \begin{align*} g(x+h) - g(x) &= f(x+h,x+h) - f(x,x)\\ &= f(x+h,x+h) - f(x+h,x) + f(x+h,x) - f(x,x)\\ \end{align*} Now, if we devide by $h$ \begin{align*} \frac{f(x+h,x+h) - f(x+h,x)}{h} + \frac{f(x+h,x) - f(x,x)}{h}\\ \end{align*} and take the limit $h \to 0$ we get \begin{align*} \lim_{h \to 0} \frac{f(x+h,x+h) - f(x+h,x)}{h} + \partial_1f(x,x). \end{align*} Here I'm stuck. How can I show that the first limit equals $\partial_2f(x,x)$?",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
54,The inverse of triple integrands,The inverse of triple integrands,,"Firstly, I am an engineer by profession and not a student of mathematics. So, my mathematical abilities are severely handicapped. Having said that I could say that indefinite single integrands have inverses. For example, if $$\int f(x)\mathrm{d}x=g(x)+C$$ Then, I could write $$f(x)=\frac{\mathrm{d}g(x)}{\mathrm{d}x}$$ I understand that I could write this because  $$\int f(x)\mathrm{d}x=\int_{C_0}^{x}f(y)\mathrm{d}y=g(x)+C$$ Here in x represents a bound to the integration of the function $f(y)$ and hence the output is a function of that bound {$g(x)$}. But, just as a thought experiment, what about $$\iiint_{V}f(\vec{r})\mathrm{d}^3\vec{r}$$ Here in $V$ is a bound to the function $f(\vec{r})$, so I should be able to write the output as  $$\iiint_{V}f(\vec{r})\mathrm{d}^3\vec{r}=\rho\left(V\right)$$ Is there an inverse function that when applied on $\rho(V)$ with the knowledge of $V$ gives me back $f(\vec{r})$. I definitely don't know of an inverse function as such. If there isn't, my question is why isn't there one? Is it because, according to your mathemitical senses $f(\vec{r})$ lacks the uniqueness of being the only function which when bound into $V$ would result in $\rho(V)$? Like I said, I'm mathematically illiterate, just saying it again at the risk of sounding like a fool.","Firstly, I am an engineer by profession and not a student of mathematics. So, my mathematical abilities are severely handicapped. Having said that I could say that indefinite single integrands have inverses. For example, if $$\int f(x)\mathrm{d}x=g(x)+C$$ Then, I could write $$f(x)=\frac{\mathrm{d}g(x)}{\mathrm{d}x}$$ I understand that I could write this because  $$\int f(x)\mathrm{d}x=\int_{C_0}^{x}f(y)\mathrm{d}y=g(x)+C$$ Here in x represents a bound to the integration of the function $f(y)$ and hence the output is a function of that bound {$g(x)$}. But, just as a thought experiment, what about $$\iiint_{V}f(\vec{r})\mathrm{d}^3\vec{r}$$ Here in $V$ is a bound to the function $f(\vec{r})$, so I should be able to write the output as  $$\iiint_{V}f(\vec{r})\mathrm{d}^3\vec{r}=\rho\left(V\right)$$ Is there an inverse function that when applied on $\rho(V)$ with the knowledge of $V$ gives me back $f(\vec{r})$. I definitely don't know of an inverse function as such. If there isn't, my question is why isn't there one? Is it because, according to your mathemitical senses $f(\vec{r})$ lacks the uniqueness of being the only function which when bound into $V$ would result in $\rho(V)$? Like I said, I'm mathematically illiterate, just saying it again at the risk of sounding like a fool.",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
55,"$\int_\gamma{(x-y)dx + (x+y)dy}, \quad \gamma : x^2 + 2y^2 = 1 , \quad 0 \leq y $",,"\int_\gamma{(x-y)dx + (x+y)dy}, \quad \gamma : x^2 + 2y^2 = 1 , \quad 0 \leq y ","I'm asked to find  $$\int_\gamma{(x-y)dx + (x+y)dy}$$ where  $$\gamma : x^2 + 2y^2 = 1 , \quad 0 \leq y$$ (with positive direction) i.e the upper half of the ellipse $x^2 + 2y^2 = 1$. My attempt Let $\sigma = \gamma + \gamma_1$ where $\gamma_1 = (t,0) \quad , \quad 0\leq t\leq 1.$ Since $\sigma$ is both positive and closed, Greens Formula can be used with $\frac{dQ}{dx} - \frac{dP}{dy} = 2.$ $$\int_\gamma(x-y)dx + (x+y)dy = \int\int_D2dxdy - \int_{-1}^1t\cdot dt$$ $$= 2\cdot\frac{1}{2}\cdot (\frac{1}{\sqrt{2}}\pi) - 2 = \frac{\pi}{\sqrt{2}} - 2$$ where I evaluate the double integral simply by getting half of the area of the ellipsoid $ = \frac{1\cdot\frac{1}{\sqrt{2}}\cdot\pi}{2}.$ However, the answer is supposed to be $\frac{\pi}{\sqrt{2}}.$ What am I doing wrong?","I'm asked to find  $$\int_\gamma{(x-y)dx + (x+y)dy}$$ where  $$\gamma : x^2 + 2y^2 = 1 , \quad 0 \leq y$$ (with positive direction) i.e the upper half of the ellipse $x^2 + 2y^2 = 1$. My attempt Let $\sigma = \gamma + \gamma_1$ where $\gamma_1 = (t,0) \quad , \quad 0\leq t\leq 1.$ Since $\sigma$ is both positive and closed, Greens Formula can be used with $\frac{dQ}{dx} - \frac{dP}{dy} = 2.$ $$\int_\gamma(x-y)dx + (x+y)dy = \int\int_D2dxdy - \int_{-1}^1t\cdot dt$$ $$= 2\cdot\frac{1}{2}\cdot (\frac{1}{\sqrt{2}}\pi) - 2 = \frac{\pi}{\sqrt{2}} - 2$$ where I evaluate the double integral simply by getting half of the area of the ellipsoid $ = \frac{1\cdot\frac{1}{\sqrt{2}}\cdot\pi}{2}.$ However, the answer is supposed to be $\frac{\pi}{\sqrt{2}}.$ What am I doing wrong?",,"['multivariable-calculus', 'greens-theorem']"
56,"Limit of (x-y)/(x²-y²) as (x,y)->(1,1) using epsilon-delta?","Limit of (x-y)/(x²-y²) as (x,y)->(1,1) using epsilon-delta?",,"My textbook asks the question $$f(x,y) = \frac{x-y}{x^2-y^2}$$   Does $f(x,y)$ have a limit as $(x,y) \rightarrow (1,1)$? First step of finding a value is easy if we approach $(1,1)$ on the y-axis: $$\lim_{(x,0) \rightarrow (1,1)} f(x,y) = \lim_{(x,0) \rightarrow (1,1)} \frac{x-0}{x^2 - 0} = 1 $$ So if the limit exists it should be 1. Next is to examine it using the formal ($\epsilon$-$\delta$) definition: Choose $\delta,\epsilon > 0$ where $\delta = \delta(\epsilon)$ such that for all $x,y \in D_f$, $|f(x,y)-L| < \epsilon$ whenever $0 < \sqrt{(x-a)^2 + (y - b)^2} < \delta$ In this case the inequalities become $$ \left| \frac{x-y}{x^2-y^2} - 1\right| < \epsilon \quad \text{whenever} \quad 0 < \sqrt{(x-1)^2 + (y-1)^2} < \delta$$ Using the identity $(a+b)(a-b) = a^2 + b^2$ on the left inequality, and expanding the right inequality, we get $$ \left| \frac{1}{x+y} - 1\right| < \epsilon \quad \text{whenever} \quad 0 < \sqrt{x^2 + y^2 - 2x - 2y + 2} < \delta$$ But then I'm stuck, I can't get them to look like eachother, so I can neither prove nor disprove that a limit exists. What do I do from here?","My textbook asks the question $$f(x,y) = \frac{x-y}{x^2-y^2}$$   Does $f(x,y)$ have a limit as $(x,y) \rightarrow (1,1)$? First step of finding a value is easy if we approach $(1,1)$ on the y-axis: $$\lim_{(x,0) \rightarrow (1,1)} f(x,y) = \lim_{(x,0) \rightarrow (1,1)} \frac{x-0}{x^2 - 0} = 1 $$ So if the limit exists it should be 1. Next is to examine it using the formal ($\epsilon$-$\delta$) definition: Choose $\delta,\epsilon > 0$ where $\delta = \delta(\epsilon)$ such that for all $x,y \in D_f$, $|f(x,y)-L| < \epsilon$ whenever $0 < \sqrt{(x-a)^2 + (y - b)^2} < \delta$ In this case the inequalities become $$ \left| \frac{x-y}{x^2-y^2} - 1\right| < \epsilon \quad \text{whenever} \quad 0 < \sqrt{(x-1)^2 + (y-1)^2} < \delta$$ Using the identity $(a+b)(a-b) = a^2 + b^2$ on the left inequality, and expanding the right inequality, we get $$ \left| \frac{1}{x+y} - 1\right| < \epsilon \quad \text{whenever} \quad 0 < \sqrt{x^2 + y^2 - 2x - 2y + 2} < \delta$$ But then I'm stuck, I can't get them to look like eachother, so I can neither prove nor disprove that a limit exists. What do I do from here?",,"['multivariable-calculus', 'limits-without-lhopital', 'epsilon-delta']"
57,How is it justified to apply the chain rule to a function when the inputs themselves are functions?,How is it justified to apply the chain rule to a function when the inputs themselves are functions?,,"Given $u(x,v)$, $v(x,y)$, and $f(u,v)$ ($u$ is a function of $x$ and $v$, and $v$ itself is a function of $x$ and $y$), we want to find ${\partial f}/{\partial x}.$  I've seen this done as: ${\partial f}/{\partial x} = {\partial f}/{\partial u} \cdot {\partial u}/{\partial x} + {\partial f}/{\partial v} \cdot {\partial v}/{\partial x}$. ${\partial u}/{\partial x}$ is then found as if $v$ was a constant (not a function of $x$). However, this seems wrong to me.  The chain rule allows us to separate variables, but how does it allow us to treat functions of the variable we are differentiating against as constants? Nonetheless, this is a common approach in many CS papers, especially concerning machine learning and neural networks.  Backpropogation, a common ML/NN algorithm, seems to rely on it.   For a very clear example, see the derivation of ${\partial l}/{\partial x_i}$ in http://costapt.github.io/2016/07/09/batch-norm-alt/ . What is the proof or basis to treat $v$ as a constant when taking the partial derivative with respect to $u$?","Given $u(x,v)$, $v(x,y)$, and $f(u,v)$ ($u$ is a function of $x$ and $v$, and $v$ itself is a function of $x$ and $y$), we want to find ${\partial f}/{\partial x}.$  I've seen this done as: ${\partial f}/{\partial x} = {\partial f}/{\partial u} \cdot {\partial u}/{\partial x} + {\partial f}/{\partial v} \cdot {\partial v}/{\partial x}$. ${\partial u}/{\partial x}$ is then found as if $v$ was a constant (not a function of $x$). However, this seems wrong to me.  The chain rule allows us to separate variables, but how does it allow us to treat functions of the variable we are differentiating against as constants? Nonetheless, this is a common approach in many CS papers, especially concerning machine learning and neural networks.  Backpropogation, a common ML/NN algorithm, seems to rely on it.   For a very clear example, see the derivation of ${\partial l}/{\partial x_i}$ in http://costapt.github.io/2016/07/09/batch-norm-alt/ . What is the proof or basis to treat $v$ as a constant when taking the partial derivative with respect to $u$?",,"['multivariable-calculus', 'computer-science']"
58,"Finding range and inverse of $f(x,y) := (x\sqrt{y},y\sqrt{x})$",Finding range and inverse of,"f(x,y) := (x\sqrt{y},y\sqrt{x})","Consider the function $f: (0,\infty)\times (0,\infty) \to \mathbb{R}^2$ defined by $$f(x,y) := (x\sqrt{y},y\sqrt{x})$$ I know that in general it is hard to tell if $f$ is injective or not and to determine the image of $f$. So I started calculating a possible inverse function, where the domain is to be determined later. If $f(x,y) = (u,v)$ I got $$(x,y) = (u^{4/3}v^{-2/3},v^{4/3}u^{-2/3})$$ How do I now find $f((0,\infty)\times(0,\infty))$ and where that $f$ is invertible?","Consider the function $f: (0,\infty)\times (0,\infty) \to \mathbb{R}^2$ defined by $$f(x,y) := (x\sqrt{y},y\sqrt{x})$$ I know that in general it is hard to tell if $f$ is injective or not and to determine the image of $f$. So I started calculating a possible inverse function, where the domain is to be determined later. If $f(x,y) = (u,v)$ I got $$(x,y) = (u^{4/3}v^{-2/3},v^{4/3}u^{-2/3})$$ How do I now find $f((0,\infty)\times(0,\infty))$ and where that $f$ is invertible?",,['multivariable-calculus']
59,"How to calculate $\lim_{\varepsilon\rightarrow 1}\int_0^\varepsilon \int_0^z \int_0^y\frac 1 {1-x^3} \, dx \, dy \, dz$?",How to calculate ?,"\lim_{\varepsilon\rightarrow 1}\int_0^\varepsilon \int_0^z \int_0^y\frac 1 {1-x^3} \, dx \, dy \, dz","How to calculate the integral:   $$\lim_{\varepsilon\rightarrow 1}\int_0^\varepsilon \int_0^z \int_0^y\frac 1 {1-x^3} \, dx \, dy \, dz\quad?$$ One solution is about infinite series, but I don't fully understand that solution.  Any other approaches?","How to calculate the integral:   $$\lim_{\varepsilon\rightarrow 1}\int_0^\varepsilon \int_0^z \int_0^y\frac 1 {1-x^3} \, dx \, dy \, dz\quad?$$ One solution is about infinite series, but I don't fully understand that solution.  Any other approaches?",,['multivariable-calculus']
60,Volume from iterated integrals and two regions,Volume from iterated integrals and two regions,,"The prompt is to find the volume of the solid which is described the equations and is bounded. $$x^2+y^2+z^2=9 $$ $$x^2-3x+y^2=0 $$ The first one is a sphere with radius 3, the shadow is on the y-x plane. For the second on I tried using completing the squares. $$x^2-3x + y^2 =0  $$ $$x^2-3x+ 1/25 + y^2 = 1/25 $$ $$(x-1/5)^2 + y^2 = 1/25 $$ i dont know how to procede now. I also tried... $$x^2+y^2-3x = 0$$ $$r^2-3x = 0 $$ $$ r^2 = 3x$$ $$ r^2 = 3cos\theta$$ $$ r = \sqrt{3cos\theta} $$ $$ \int_0^3\int_0^{2\pi} \int_0^{\sqrt{3cos\theta}}x^2+y^2+z^9rdrd\theta dz$$  Please correct me if the method to get the radius, if its wrong? Im kinda new to calculus.","The prompt is to find the volume of the solid which is described the equations and is bounded. $$x^2+y^2+z^2=9 $$ $$x^2-3x+y^2=0 $$ The first one is a sphere with radius 3, the shadow is on the y-x plane. For the second on I tried using completing the squares. $$x^2-3x + y^2 =0  $$ $$x^2-3x+ 1/25 + y^2 = 1/25 $$ $$(x-1/5)^2 + y^2 = 1/25 $$ i dont know how to procede now. I also tried... $$x^2+y^2-3x = 0$$ $$r^2-3x = 0 $$ $$ r^2 = 3x$$ $$ r^2 = 3cos\theta$$ $$ r = \sqrt{3cos\theta} $$ $$ \int_0^3\int_0^{2\pi} \int_0^{\sqrt{3cos\theta}}x^2+y^2+z^9rdrd\theta dz$$  Please correct me if the method to get the radius, if its wrong? Im kinda new to calculus.",,"['calculus', 'multivariable-calculus', 'iterated-integrals']"
61,Differential Vector Element,Differential Vector Element,,"I am a little confused about differential element notations. Say we have a function $f$ defined on a 3D space. Would the following notations be identical? $$\int f(\vec{x}) \, d\vec{x}$$ $$\int f(x_{1},x_{2},x_{3}) \, d^3x$$ Intuitively the equivalence makes sense: if we sum over all combinations of $x_{1},x_{2},x_{3}$, we sum over all possible vectors. But notation wise how can we convert $d\vec{x}$ to $d^3x$. In certain cases would it be better to use one notation over the other?","I am a little confused about differential element notations. Say we have a function $f$ defined on a 3D space. Would the following notations be identical? $$\int f(\vec{x}) \, d\vec{x}$$ $$\int f(x_{1},x_{2},x_{3}) \, d^3x$$ Intuitively the equivalence makes sense: if we sum over all combinations of $x_{1},x_{2},x_{3}$, we sum over all possible vectors. But notation wise how can we convert $d\vec{x}$ to $d^3x$. In certain cases would it be better to use one notation over the other?",,"['calculus', 'integration', 'multivariable-calculus', 'indefinite-integrals']"
62,Divergence theorem application.,Divergence theorem application.,,"I need to prove that $$\lim_{r \to 0} \frac 1 {\operatorname{vol}(B_r(p))} \iint_{\partial B_r(p) } f \, dA = \operatorname{div}(f) $$ We have by the Divergence Theorem that $\dfrac 1 {\operatorname{vol}(B_r(p))} \iint_{\partial B_r(p)} f \, dA  =\dfrac 1 {\operatorname{vol}(B_r(p)} \iiint_{B_r(p)} \nabla f \, dV $ where $\nabla f = \dfrac{\partial f}{\partial x} +\dfrac{\partial f}{\partial y} +\dfrac{\partial f}{\partial z}$. Im not sure how to continue from here, I tried working with Spherical Coordinates but I can't compute this triple integral.","I need to prove that $$\lim_{r \to 0} \frac 1 {\operatorname{vol}(B_r(p))} \iint_{\partial B_r(p) } f \, dA = \operatorname{div}(f) $$ We have by the Divergence Theorem that $\dfrac 1 {\operatorname{vol}(B_r(p))} \iint_{\partial B_r(p)} f \, dA  =\dfrac 1 {\operatorname{vol}(B_r(p)} \iiint_{B_r(p)} \nabla f \, dV $ where $\nabla f = \dfrac{\partial f}{\partial x} +\dfrac{\partial f}{\partial y} +\dfrac{\partial f}{\partial z}$. Im not sure how to continue from here, I tried working with Spherical Coordinates but I can't compute this triple integral.",,['multivariable-calculus']
63,Evaluating surface integral (1) directly and (2) by applying Divergence Theorem give different resoluts,Evaluating surface integral (1) directly and (2) by applying Divergence Theorem give different resoluts,,"I have a vector field $\mathbf{A}(r) = \frac{1}{r^2}\mathbf{a}_r$. I am interested in finding the flux through a sphere enclosing some volume with radius $R$ and center at $r=0$. Calculating the divergence of vector field $\bf A$, I get zero divergence. Integrating the zero divergence over the volume of a sphere with radius $R$, I get zero net flux. But when I integrate the vector field over the surface, I get $4 \pi$. $$\Rightarrow \nabla \cdot \mathbf{A} = \frac{1}{r^2} \frac{\partial (r^2 \frac{1}{r^2})}{\partial r} = \frac{1}{r^2}\frac{\partial}{\partial r}(1)= 0$$ $$\int_V \nabla \cdot \mathbf{A}\ dV = \oint_S \mathbf{A}\cdot d\mathbf{S}$$ $$\int_V \nabla \cdot \mathbf{A}\ dV = \int_V 0 \ dV = 0 \,?$$ $$\oint_S \mathbf{A}\cdot d\mathbf{S} = \oint_S A\mathbf{a}_r\cdot \mathbf{n}_s dS = A(r)\oint_S dS$$ Sphere with radius R: $$A(R)(4\pi R^2) = \frac{1}{R^2}4\pi R^2= 4\pi \neq 0$$ If the volume integral of the divergence of the field is zero, it means that the field is constant inside the sphere, or that the field strength inside the sphere in total grows and shrinks by the same amount. If the surface flux integral of the vector field is equal to some positive constant, then the magnitude of the field is not constant on the boundary of the sphere. This means that the field at some point has to grow more than it shrinks inside the sphere, resulting in a divergence $\neq 0$. So obviously $4\pi \neq 0$... What is happening here?","I have a vector field $\mathbf{A}(r) = \frac{1}{r^2}\mathbf{a}_r$. I am interested in finding the flux through a sphere enclosing some volume with radius $R$ and center at $r=0$. Calculating the divergence of vector field $\bf A$, I get zero divergence. Integrating the zero divergence over the volume of a sphere with radius $R$, I get zero net flux. But when I integrate the vector field over the surface, I get $4 \pi$. $$\Rightarrow \nabla \cdot \mathbf{A} = \frac{1}{r^2} \frac{\partial (r^2 \frac{1}{r^2})}{\partial r} = \frac{1}{r^2}\frac{\partial}{\partial r}(1)= 0$$ $$\int_V \nabla \cdot \mathbf{A}\ dV = \oint_S \mathbf{A}\cdot d\mathbf{S}$$ $$\int_V \nabla \cdot \mathbf{A}\ dV = \int_V 0 \ dV = 0 \,?$$ $$\oint_S \mathbf{A}\cdot d\mathbf{S} = \oint_S A\mathbf{a}_r\cdot \mathbf{n}_s dS = A(r)\oint_S dS$$ Sphere with radius R: $$A(R)(4\pi R^2) = \frac{1}{R^2}4\pi R^2= 4\pi \neq 0$$ If the volume integral of the divergence of the field is zero, it means that the field is constant inside the sphere, or that the field strength inside the sphere in total grows and shrinks by the same amount. If the surface flux integral of the vector field is equal to some positive constant, then the magnitude of the field is not constant on the boundary of the sphere. This means that the field at some point has to grow more than it shrinks inside the sphere, resulting in a divergence $\neq 0$. So obviously $4\pi \neq 0$... What is happening here?",,"['multivariable-calculus', 'vector-analysis', 'surface-integrals']"
64,"Is the function $f(x, y)= \frac{x^3}{x³+y^2}$ continuous?",Is the function  continuous?,"f(x, y)= \frac{x^3}{x³+y^2}","I want to know if the function is continuous. $$f(x,y)=\begin{cases}\frac{x^3}{x^3+y^2},&(x,y)\neq(0,0)\\0,&(x,y)=(0,0)\end{cases}$$ First of all, I evaluate the limit by putting the point $(0,0)$ into the function: $$\lim_{(x,y)\to (0,0)}\frac{x^3}{x^3+y^2}$$ ? The result is 0/0. Then, I used the path rule. The first path: $y=x$ The result is $0$. The second path is $y=x^2$. The result is $1$. The third path is $y=0$. The result is $1$. Are my result correct? The book says that this limit exists.","I want to know if the function is continuous. $$f(x,y)=\begin{cases}\frac{x^3}{x^3+y^2},&(x,y)\neq(0,0)\\0,&(x,y)=(0,0)\end{cases}$$ First of all, I evaluate the limit by putting the point $(0,0)$ into the function: $$\lim_{(x,y)\to (0,0)}\frac{x^3}{x^3+y^2}$$ ? The result is 0/0. Then, I used the path rule. The first path: $y=x$ The result is $0$. The second path is $y=x^2$. The result is $1$. The third path is $y=0$. The result is $1$. Are my result correct? The book says that this limit exists.",,"['calculus', 'multivariable-calculus', 'continuity']"
65,Maximization in Two Variables,Maximization in Two Variables,,"I have not yet had the privilege of studying multivariable calculus, but I have made an educated guess about how to find the minimum or maximum of a function with two variables, for example, $x$ and $y$. Since, in three dimensions, a minimum or maximum would be represented by a tangent plane with no slope in any direction, could I treat $y$ as a constant and differentiate $z$ with respect to $x$, then treat $x$ as a constant and differentiate with respect to $y$, and find the places where both of these two are equal to zero? Sorry if this is just a stupid assumption... it may be one of those things that just seems correct but is actually wrong.","I have not yet had the privilege of studying multivariable calculus, but I have made an educated guess about how to find the minimum or maximum of a function with two variables, for example, $x$ and $y$. Since, in three dimensions, a minimum or maximum would be represented by a tangent plane with no slope in any direction, could I treat $y$ as a constant and differentiate $z$ with respect to $x$, then treat $x$ as a constant and differentiate with respect to $y$, and find the places where both of these two are equal to zero? Sorry if this is just a stupid assumption... it may be one of those things that just seems correct but is actually wrong.",,"['multivariable-calculus', 'optimization', 'maxima-minima']"
66,"Why if any path going toward a point yields the same limit, then limit at that point exist?","Why if any path going toward a point yields the same limit, then limit at that point exist?",,"We have the definition of limit of multivariable function. Basically, for any given $t$, we can find a $q$, so that for any point, whose distance to the point where we want to calculate the limit is less than $q$, $t>|f-L|$. Then we say $L$ is its limit. There is another way to state that the limit exists. That is, for any curve going toward the point, they have to yield the same limit. So my question is: why is ""any curve going toward the point has the same limit"" equivalent to ""limit exists""?","We have the definition of limit of multivariable function. Basically, for any given $t$, we can find a $q$, so that for any point, whose distance to the point where we want to calculate the limit is less than $q$, $t>|f-L|$. Then we say $L$ is its limit. There is another way to state that the limit exists. That is, for any curve going toward the point, they have to yield the same limit. So my question is: why is ""any curve going toward the point has the same limit"" equivalent to ""limit exists""?",,"['calculus', 'multivariable-calculus']"
67,"Green's theorem, double integral over a triangle","Green's theorem, double integral over a triangle",,"The original problem is given as thus Find $$\iint_Dx\,dxdy $$   where $D$ is a triangle with vertices $(0,2),(2,0),(3,3)$. Green's theorem says that $$\iint_D(G_x-F_y)dxdy = \int_{\partial D}Fdx+Gdy $$ I could parametrize the individual sides of the triangle as such: $$L_1 = (0,2)\to(2,0) : \begin{cases}x = t\\y=2-t\end{cases}0\leq t\leq 2  $$ $$L_2 = (2,0)\to (3,3) : \begin{cases}x = t+2\\y=3t\end{cases}0\leq t\leq 1$$ $$L_3 = (3,3)\to (0,2) : \begin{cases}x = 3-t\\y=3-\frac{t}{3}\end{cases}0\leq t\leq 3 $$ Is this parametrization correct? The confusing part is utilizing the theorem. I have to pick $F,G$ such that $G_x,F_y$ are continous in $D$, but $x = x-0 = 2x - x = (y+x)-y = \ldots$ there are endless possibilities. Is the theorem implying no matter which representation of $x$ I choose, the result will always be the same? For instance: $x = x-0 \Longrightarrow G = \frac{x^2}{2}\quad F = x\qquad$ or $\qquad x = (y+x)-y\Longrightarrow G=yx+\frac{x^2}{2}\quad F = \frac{y^2}{2}$","The original problem is given as thus Find $$\iint_Dx\,dxdy $$   where $D$ is a triangle with vertices $(0,2),(2,0),(3,3)$. Green's theorem says that $$\iint_D(G_x-F_y)dxdy = \int_{\partial D}Fdx+Gdy $$ I could parametrize the individual sides of the triangle as such: $$L_1 = (0,2)\to(2,0) : \begin{cases}x = t\\y=2-t\end{cases}0\leq t\leq 2  $$ $$L_2 = (2,0)\to (3,3) : \begin{cases}x = t+2\\y=3t\end{cases}0\leq t\leq 1$$ $$L_3 = (3,3)\to (0,2) : \begin{cases}x = 3-t\\y=3-\frac{t}{3}\end{cases}0\leq t\leq 3 $$ Is this parametrization correct? The confusing part is utilizing the theorem. I have to pick $F,G$ such that $G_x,F_y$ are continous in $D$, but $x = x-0 = 2x - x = (y+x)-y = \ldots$ there are endless possibilities. Is the theorem implying no matter which representation of $x$ I choose, the result will always be the same? For instance: $x = x-0 \Longrightarrow G = \frac{x^2}{2}\quad F = x\qquad$ or $\qquad x = (y+x)-y\Longrightarrow G=yx+\frac{x^2}{2}\quad F = \frac{y^2}{2}$",,"['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral', 'greens-theorem']"
68,"Find all the points on the cone $z^2=x^2+4y^2$ that are the closest to the point $(0,0,c)$",Find all the points on the cone  that are the closest to the point,"z^2=x^2+4y^2 (0,0,c)","From all the points on the cone $z^2=x^2+4y^2$ find the closest to the point $(0,0,c)$. State explicitly the minimal distance. $c$ is a constant. Lagrange multipliers can be used here. Let the constraint function $g=x^2+4y^2-z^2$. Let the minimization function $f=(x-0)^2+(y-0)^2+(z-c)^2$ as function of finding the distance in between 2 points ion 3d space. We're also going to keep $f$ in squares so we don't have to get the factor out $z$ from the ellipse equation. Then we have: $$ \nabla g=\langle2x, 8y,-2z \rangle\\ \nabla f=\langle 2x, 2y,2(z-c)\rangle $$ So: $$ \begin{cases} 2x=k\cdot2x\\ 2y=k\cdot 8y\\ 2(z-c)=k\cdot(-2z) \end{cases} $$ We have that $k=\frac{1}{4}$ then $x=0$ and $z=\frac{4c}{5}$. If we plug those values into the ellipse equation we get that: $$ c^2=\frac{125y^2}{4}\Rightarrow c=\pm\frac{\sqrt{125}y}{2} $$ So the points would be $(0,y,\frac{\sqrt{125}y}{2})$ and $(0,y,-\frac{\sqrt{125}y}{2})$. First I think something is wrong with my calculations (although I double checked it). Also I'm not sure what does this mean that we have 2 points and how am I supposed to know which one is the minimum.","From all the points on the cone $z^2=x^2+4y^2$ find the closest to the point $(0,0,c)$. State explicitly the minimal distance. $c$ is a constant. Lagrange multipliers can be used here. Let the constraint function $g=x^2+4y^2-z^2$. Let the minimization function $f=(x-0)^2+(y-0)^2+(z-c)^2$ as function of finding the distance in between 2 points ion 3d space. We're also going to keep $f$ in squares so we don't have to get the factor out $z$ from the ellipse equation. Then we have: $$ \nabla g=\langle2x, 8y,-2z \rangle\\ \nabla f=\langle 2x, 2y,2(z-c)\rangle $$ So: $$ \begin{cases} 2x=k\cdot2x\\ 2y=k\cdot 8y\\ 2(z-c)=k\cdot(-2z) \end{cases} $$ We have that $k=\frac{1}{4}$ then $x=0$ and $z=\frac{4c}{5}$. If we plug those values into the ellipse equation we get that: $$ c^2=\frac{125y^2}{4}\Rightarrow c=\pm\frac{\sqrt{125}y}{2} $$ So the points would be $(0,y,\frac{\sqrt{125}y}{2})$ and $(0,y,-\frac{\sqrt{125}y}{2})$. First I think something is wrong with my calculations (although I double checked it). Also I'm not sure what does this mean that we have 2 points and how am I supposed to know which one is the minimum.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'qcqp']"
69,"Evaluate $\lim_{n\to \infty}\{(1+\frac{2}{n})^n,(\sqrt{\frac{n+1}{4n-1}})\}$",Evaluate,"\lim_{n\to \infty}\{(1+\frac{2}{n})^n,(\sqrt{\frac{n+1}{4n-1}})\}","$$\lim_{n\to \infty}\left\{\left(1+\frac{2}{n}\right)^n,\;\sqrt{\frac{n+1}{4n-1}}\right\}$$ $$\lim_{n\to \infty}\left(1+\frac{2}{n}\right)^n=\lim_{n\to \infty}\left(1+\frac{2}{n}\right)^{\frac{n}{2}\cdot 2}$$ $m=\frac{n}{2}$ $$\lim_{n\to \infty}(1+\frac{2}{n})^{\frac{n}{2}\cdot 2}=\lim_{n\to \infty}(1+\frac{1}{m})^{m\cdot 2}=\lim_{n\to \infty}(1+\frac{1}{m})^{{m}^{2}}=e^2$$ $$\lim_{n\to \infty}\sqrt{\frac{n+1}{4n-1}}=\lim_{n\to \infty}\sqrt{\frac{\frac{n}{n}+\frac{1}{n}}{\frac{4n}{n}-\frac{1}{n}}}$$ Because square root is a continuos function $$\sqrt {lim_{n\to \infty}{\frac{\frac{n}{n}+\frac{1}{n}}{\frac{4n}{n}-\frac{1}{n}}}}=\sqrt {{\frac{1+0}{4-0}}}=\sqrt{\frac{1}{4}}=\frac{1}{2}$$ So $$\lim_{n\to \infty}\{(1+\frac{2}{n})^n,(\sqrt{\frac{n+1}{4n-1}})\}=(e^{2},\frac{1}{2})$$ Are all the moves correct?","$$\lim_{n\to \infty}\left\{\left(1+\frac{2}{n}\right)^n,\;\sqrt{\frac{n+1}{4n-1}}\right\}$$ $$\lim_{n\to \infty}\left(1+\frac{2}{n}\right)^n=\lim_{n\to \infty}\left(1+\frac{2}{n}\right)^{\frac{n}{2}\cdot 2}$$ $m=\frac{n}{2}$ $$\lim_{n\to \infty}(1+\frac{2}{n})^{\frac{n}{2}\cdot 2}=\lim_{n\to \infty}(1+\frac{1}{m})^{m\cdot 2}=\lim_{n\to \infty}(1+\frac{1}{m})^{{m}^{2}}=e^2$$ $$\lim_{n\to \infty}\sqrt{\frac{n+1}{4n-1}}=\lim_{n\to \infty}\sqrt{\frac{\frac{n}{n}+\frac{1}{n}}{\frac{4n}{n}-\frac{1}{n}}}$$ Because square root is a continuos function $$\sqrt {lim_{n\to \infty}{\frac{\frac{n}{n}+\frac{1}{n}}{\frac{4n}{n}-\frac{1}{n}}}}=\sqrt {{\frac{1+0}{4-0}}}=\sqrt{\frac{1}{4}}=\frac{1}{2}$$ So $$\lim_{n\to \infty}\{(1+\frac{2}{n})^n,(\sqrt{\frac{n+1}{4n-1}})\}=(e^{2},\frac{1}{2})$$ Are all the moves correct?",,['multivariable-calculus']
70,Decomposition of differentiable function as a dot product?,Decomposition of differentiable function as a dot product?,,"If $f:\mathbb{R}^n\to\mathbb{R}$ is a differentiable function with $f(0)=0$, then $f(\mathbf{x})=\mathbf{x}\cdot g(\mathbf{x})$ for some $g:\mathbb{R}^n\to\mathbb{R}^n$. I tried adapting the proof of Euler's homogeneous function theorem, but couldn't figure out a similar equation for a not necessarily homogeneous $f$. The reason I did this is because the right hand side looks like the result of differentiating $f(\alpha\mathbf{x})$ w.r.t. $\alpha$, where $g(\mathbf{x})$ is $Df(\mathbf{x})$. Any suggestions?","If $f:\mathbb{R}^n\to\mathbb{R}$ is a differentiable function with $f(0)=0$, then $f(\mathbf{x})=\mathbf{x}\cdot g(\mathbf{x})$ for some $g:\mathbb{R}^n\to\mathbb{R}^n$. I tried adapting the proof of Euler's homogeneous function theorem, but couldn't figure out a similar equation for a not necessarily homogeneous $f$. The reason I did this is because the right hand side looks like the result of differentiating $f(\alpha\mathbf{x})$ w.r.t. $\alpha$, where $g(\mathbf{x})$ is $Df(\mathbf{x})$. Any suggestions?",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'vector-analysis', 'homogeneous-equation']"
71,Solving $u_{t}+u_{xxx} = u\sin{x}$ using 1. Laplace transforms 2. KdV equation comparison,Solving  using 1. Laplace transforms 2. KdV equation comparison,u_{t}+u_{xxx} = u\sin{x},"I'm trying to solve: $$u_{t}+u_{xxx} = u\sin{x}$$ where $u=u(x,t)$, subject to some initial condition, $u(x,0) = f(x)$ First attempt - via Laplace transforms wrt $t$: $$\frac{\partial^3{U(x,s)}}{\partial{x}^3}+U(x,s)\left[s-\sin{x}\right]=u(x,0)=f(x)$$ I notice from the right hand side that the left must be a function of $x$ only.  Not sure where to go from here. Second attempt: The Pde seems to be similar to the KdV equation ( https://en.m.wikipedia.org/wiki/Korteweg%E2%80%93de_Vries_equation ). $$u_{t}+uu_x+\delta^2u_{xxx}=0$$ And comparing the two equations I notice that $u_x=-\sin{x}$ and $\delta^2=1$. - Can I do this here? Hence $$u(x,t)=\cos{x}+h(t)$$ Substituting this back into the Pde, I get the following: $$h_t(t)-(\sin{x})h(t)+\sin{x}(1-\cos{x})=0$$ This does not seem to have any possible solutions for $h(t)$. Questions: Is my working correct thus far? Can I take $u_x = \sin{x}$? If not why not? Are there not possible solutions for $h(t)$ with this assumption? What methods should I try for solving? Please give examples.","I'm trying to solve: $$u_{t}+u_{xxx} = u\sin{x}$$ where $u=u(x,t)$, subject to some initial condition, $u(x,0) = f(x)$ First attempt - via Laplace transforms wrt $t$: $$\frac{\partial^3{U(x,s)}}{\partial{x}^3}+U(x,s)\left[s-\sin{x}\right]=u(x,0)=f(x)$$ I notice from the right hand side that the left must be a function of $x$ only.  Not sure where to go from here. Second attempt: The Pde seems to be similar to the KdV equation ( https://en.m.wikipedia.org/wiki/Korteweg%E2%80%93de_Vries_equation ). $$u_{t}+uu_x+\delta^2u_{xxx}=0$$ And comparing the two equations I notice that $u_x=-\sin{x}$ and $\delta^2=1$. - Can I do this here? Hence $$u(x,t)=\cos{x}+h(t)$$ Substituting this back into the Pde, I get the following: $$h_t(t)-(\sin{x})h(t)+\sin{x}(1-\cos{x})=0$$ This does not seem to have any possible solutions for $h(t)$. Questions: Is my working correct thus far? Can I take $u_x = \sin{x}$? If not why not? Are there not possible solutions for $h(t)$ with this assumption? What methods should I try for solving? Please give examples.",,"['multivariable-calculus', 'partial-differential-equations', 'laplace-transform']"
72,Compute line integral and prove statement about simply connected space of set,Compute line integral and prove statement about simply connected space of set,,"Let C be circumference in $\mathbb{R^2}$ with centre in $(0,0)$ which we bypass counterclockwise. Compute : $$\oint_C -\frac{y}{x^2 + y^2}dx + \frac{x}{x^2 + y^2}dy$$ and then prove that set $\big\{(x, y) \big| 0 < x^2 + y^2 \le 1 \big\}$ is not simply connected space. So, I computed integral and obtain $2\pi$. Expression under integral sign is total derivative and I know that line integral of total derivative by closed curve in simply connected space is zero, but our integral is not zero. But my teacher said that is not full proof. Could you complement my solution or give any tips for this?","Let C be circumference in $\mathbb{R^2}$ with centre in $(0,0)$ which we bypass counterclockwise. Compute : $$\oint_C -\frac{y}{x^2 + y^2}dx + \frac{x}{x^2 + y^2}dy$$ and then prove that set $\big\{(x, y) \big| 0 < x^2 + y^2 \le 1 \big\}$ is not simply connected space. So, I computed integral and obtain $2\pi$. Expression under integral sign is total derivative and I know that line integral of total derivative by closed curve in simply connected space is zero, but our integral is not zero. But my teacher said that is not full proof. Could you complement my solution or give any tips for this?",,"['integration', 'multivariable-calculus', 'vector-analysis']"
73,Double integral substitution,Double integral substitution,,"I am looking for a substitution which will allow me to compute the double integral, $$\iint \frac{\cos(x-y)}{\sin(x+y)}\,dA,$$ over region bound by $x+y=\frac{\pi}{2}$, $x=0$, and $y=\frac{\pi}{4}$.","I am looking for a substitution which will allow me to compute the double integral, $$\iint \frac{\cos(x-y)}{\sin(x+y)}\,dA,$$ over region bound by $x+y=\frac{\pi}{2}$, $x=0$, and $y=\frac{\pi}{4}$.",,"['calculus', 'integration', 'multivariable-calculus']"
74,"$f: \mathbb{R^2} \to \mathbb{R}$ a $C^\infty$ function such that $f(x,0)=f(0,y)=0$ then exists $g$ such that $f(x,y)=xy\, g(x,y)$",a  function such that  then exists  such that,"f: \mathbb{R^2} \to \mathbb{R} C^\infty f(x,0)=f(0,y)=0 g f(x,y)=xy\, g(x,y)","I'm trying to solve the following exercise Let $f: \mathbb{R^2} \to \mathbb{R}$ a $C^\infty$ function such that $f(x,0)=f(0,y)=0$ $\forall x,y \in \mathbb{R}$ . Then there exists a $C^\infty$ function $g:\mathbb{R^2} \to \mathbb{R}$ satisfying $f(x,y)=xy\, g(x,y)$ $\forall x,y \in \mathbb{R}$ I thought first about expanding $f$ using the taylor theorem, we would have $$f(x,y)=f(0,0)+f'(0,0)(x.y)+\frac{f''(0,0)(x,y)^2}{2!}+r(x,y),$$ but $f(0,0)=0$ and $f_x(0,0)=f_y(0,0)=0$ since $f$ is zero on the $x$ and $y$ axis. $f''(0,0)(x,y)^2=f_{xx}(0,0)x^2+f_{xy}(0,0)xy+f_{yy}(0,0)y^2$ , now we have a term that has a product of $x$ and $y$ . But I don't know how (and if it is possible) to proceed from here.","I'm trying to solve the following exercise Let a function such that . Then there exists a function satisfying I thought first about expanding using the taylor theorem, we would have but and since is zero on the and axis. , now we have a term that has a product of and . But I don't know how (and if it is possible) to proceed from here.","f: \mathbb{R^2} \to \mathbb{R} C^\infty f(x,0)=f(0,y)=0 \forall x,y \in \mathbb{R} C^\infty g:\mathbb{R^2} \to \mathbb{R} f(x,y)=xy\, g(x,y) \forall x,y \in \mathbb{R} f f(x,y)=f(0,0)+f'(0,0)(x.y)+\frac{f''(0,0)(x,y)^2}{2!}+r(x,y), f(0,0)=0 f_x(0,0)=f_y(0,0)=0 f x y f''(0,0)(x,y)^2=f_{xx}(0,0)x^2+f_{xy}(0,0)xy+f_{yy}(0,0)y^2 x y","['real-analysis', 'multivariable-calculus', 'taylor-expansion', 'smooth-functions']"
75,Parameterization of a triangle in $\mathbb R^3$,Parameterization of a triangle in,\mathbb R^3,"I'm asked to find the area of a triangular region $T$ with vertices at $(1,1,0)$, $(2,1,2)$, and $(2,3,3)$. With the help of software, I was able to conjure up the following parameterization for $T$, $$\mathbf r(s,t)=(t+1,2s+1,s+2t)$$ where $t\in[0,1]$ and $s\in[0,t]$. Proceeding with this setup, I find the area to be $$\int_{t=0}^{t=1}\int_{s=0}^{s=t}\|\mathbf r_s\times\mathbf r_t\|\,\mathrm ds\,\mathrm dt=\sqrt{21}\int_0^1t\,\mathrm dt=\frac{\sqrt{21}}2$$ The problem is that I don't understand why this parameterization works. (My memory of the details of this part of vector calculus is a bit stale.) What I do understand is how to find the parametric equations for each edge of $T$. For the vertices $(1,1,0)$ and $(2,1,2)$, I have $\vec r_1=(1+t,1,2t)$; for $(1,1,0)$ and $(2,3,3)$, I have $\vec r_2=(1+t,1+2t,3t)$; and finally, for $(2,1,2)$ and $(2,3,3)$, I have $\vec r_3=(2,1+2t,2+t)$. In each case, $t\in[0,1]$. It seems my jury-rigged $\mathbf r$ relies heavily on $\vec r_2$, but I don't immediately see why that is the case.","I'm asked to find the area of a triangular region $T$ with vertices at $(1,1,0)$, $(2,1,2)$, and $(2,3,3)$. With the help of software, I was able to conjure up the following parameterization for $T$, $$\mathbf r(s,t)=(t+1,2s+1,s+2t)$$ where $t\in[0,1]$ and $s\in[0,t]$. Proceeding with this setup, I find the area to be $$\int_{t=0}^{t=1}\int_{s=0}^{s=t}\|\mathbf r_s\times\mathbf r_t\|\,\mathrm ds\,\mathrm dt=\sqrt{21}\int_0^1t\,\mathrm dt=\frac{\sqrt{21}}2$$ The problem is that I don't understand why this parameterization works. (My memory of the details of this part of vector calculus is a bit stale.) What I do understand is how to find the parametric equations for each edge of $T$. For the vertices $(1,1,0)$ and $(2,1,2)$, I have $\vec r_1=(1+t,1,2t)$; for $(1,1,0)$ and $(2,3,3)$, I have $\vec r_2=(1+t,1+2t,3t)$; and finally, for $(2,1,2)$ and $(2,3,3)$, I have $\vec r_3=(2,1+2t,2+t)$. In each case, $t\in[0,1]$. It seems my jury-rigged $\mathbf r$ relies heavily on $\vec r_2$, but I don't immediately see why that is the case.",,"['multivariable-calculus', 'area', 'surface-integrals']"
76,A triple integral problem,A triple integral problem,,"I am having difficulty trying to solve this triple integral problem: $$\iiint_V \sqrt {x^2+y^2} \sin(z^2)\,dx\,dy\,dz$$ where  $$V= \left\lbrace (x,y,z)\,\Big|\,\sqrt {x^2+y^2} \le z \le 3\right\rbrace.$$ I am thinking of making $z\,dz$ appear in the integral but cannot figure out how to do it. Could anyone help me? Thanks in advance!","I am having difficulty trying to solve this triple integral problem: $$\iiint_V \sqrt {x^2+y^2} \sin(z^2)\,dx\,dy\,dz$$ where  $$V= \left\lbrace (x,y,z)\,\Big|\,\sqrt {x^2+y^2} \le z \le 3\right\rbrace.$$ I am thinking of making $z\,dz$ appear in the integral but cannot figure out how to do it. Could anyone help me? Thanks in advance!",,"['integration', 'multivariable-calculus']"
77,Numerical integration over a triangle in 3D,Numerical integration over a triangle in 3D,,"I want to numerically integrate a surface discretized in triangles. Let's say I have a triangle with vertices: $(x_1,y_1,z_1)$, $(x_2,y_2,z_2)$ and $(x_3,y_3,z_3)$. In order to integrate it easily I would like to map it to the reference triangle defined as: $T_{ref} = \{(0,0,0),(1,0,0),(0,1,0)\}$. However, I do not the form of the jacobian in order to perform the change of variables over the integral. What I tried was to define the affine transformation: $$B\hat{x} + c = \begin{bmatrix} x_1-x_3 & x_2-x_3 & 0\\ y_1-y_3 & y_2-y_3 & 0\\z_1-z_3 & z_2-z_3 & 0\end{bmatrix}\begin{bmatrix}\hat{x}\\\hat{y}\\\hat{z}\end{bmatrix} + \begin{bmatrix}x_3\\y_3\\z_3\end{bmatrix}$$ and to define the jacobian as $B^T$ but of course it does not work because the determinant would be zero. Moreover, the third column of $B$ could be any value so it does not make sense to me. Can anyone explain to me how to integrate a triangle define in 3D by passing it to the reference triangle? Thanks.","I want to numerically integrate a surface discretized in triangles. Let's say I have a triangle with vertices: $(x_1,y_1,z_1)$, $(x_2,y_2,z_2)$ and $(x_3,y_3,z_3)$. In order to integrate it easily I would like to map it to the reference triangle defined as: $T_{ref} = \{(0,0,0),(1,0,0),(0,1,0)\}$. However, I do not the form of the jacobian in order to perform the change of variables over the integral. What I tried was to define the affine transformation: $$B\hat{x} + c = \begin{bmatrix} x_1-x_3 & x_2-x_3 & 0\\ y_1-y_3 & y_2-y_3 & 0\\z_1-z_3 & z_2-z_3 & 0\end{bmatrix}\begin{bmatrix}\hat{x}\\\hat{y}\\\hat{z}\end{bmatrix} + \begin{bmatrix}x_3\\y_3\\z_3\end{bmatrix}$$ and to define the jacobian as $B^T$ but of course it does not work because the determinant would be zero. Moreover, the third column of $B$ could be any value so it does not make sense to me. Can anyone explain to me how to integrate a triangle define in 3D by passing it to the reference triangle? Thanks.",,"['calculus', 'integration', 'multivariable-calculus', 'numerical-methods']"
78,Confused about derivation of center of mass formulae,Confused about derivation of center of mass formulae,,"So, not sure if I'm really dumb and not getting something obvious or if there is more complexity here...Let's do 2 objects first. Say C.O.M. is x°. Then $x° = \dfrac{x_1M_1 +x_2M_2}{M_1+M_2} $ I get the ""idea"" of this formula–you are in a sense averaging the value of the masses. But is there some blatantly obvious reason for why this is true? It can easily be derived from: $(x°-x_1)M_1 = (x_2-x°)M_2$ but I don't see why the initial statement ($x° = \dfrac{x_1M_1 +x_2M_2}{M_1+M_2} $) is a priori true. Is there a geometric intuition? If we are doing a continuous non-constant function: $\int_{a}^{x°}f(x)(x°-x)dx= \int_{x°}^{b}f(x)(x-x°)dx $ It can be derived that this is equal (if I did it right) to $\dfrac{\int_{a}^{b}xf(x)dx}{\int_{a}^{b}f(x)dx} $ But again, is there a more obvious geometric intuition? Thanks","So, not sure if I'm really dumb and not getting something obvious or if there is more complexity here...Let's do 2 objects first. Say C.O.M. is x°. Then $x° = \dfrac{x_1M_1 +x_2M_2}{M_1+M_2} $ I get the ""idea"" of this formula–you are in a sense averaging the value of the masses. But is there some blatantly obvious reason for why this is true? It can easily be derived from: $(x°-x_1)M_1 = (x_2-x°)M_2$ but I don't see why the initial statement ($x° = \dfrac{x_1M_1 +x_2M_2}{M_1+M_2} $) is a priori true. Is there a geometric intuition? If we are doing a continuous non-constant function: $\int_{a}^{x°}f(x)(x°-x)dx= \int_{x°}^{b}f(x)(x-x°)dx $ It can be derived that this is equal (if I did it right) to $\dfrac{\int_{a}^{b}xf(x)dx}{\int_{a}^{b}f(x)dx} $ But again, is there a more obvious geometric intuition? Thanks",,"['calculus', 'integration', 'multivariable-calculus', 'proof-verification', 'proof-explanation']"
79,Stokes theorem problem involving sketch,Stokes theorem problem involving sketch,,"The question is Let $S$ be that part of the surface of the paraboloid $z=x^2+y^2$ between the planes $z=1$ and $z=4$.  Using suitable diagrams, show how Stokes' theorem may be applied to this surface in order to write $\iint_s \nabla \times V.\hat ndS$ as the sum of two line integrals. I want to show clearly the direction of integration along the two curves assuming that the z components of $\hat n$ are positive. part (b) given $\vec{V}=x^3j+z^3k$ I want to evaluate both the surface intergal $\iint_s\nabla\times V.\hat n dS$ and the sum of the line intergrals $\int_{C_{1}} V.dR +\int_{C_{2}} V.dR$ where $C_1$ and $C_2$ are two curves bounding S, hence verifying stokes' theorem for this case. My questions are: 1) How does one find the upper and lower limits of the intergrals clearly one of the integrals should be z=4 and z=1 but how can one calculate the ones for x? 2) Do we need to calculate $\hat n$ and if so how? is it just $\frac{\nabla V}{|\nabla V|}=\hat n$ 3) What is the diagram supposed to look like because I'm finding it very hard to believe my diagram is correct? 4) Could someone show me the working for when evaluating the surface intergral specifically  update would we substitute the values of $z$ into $z=x^2+y^2$ to find the curves $C_1$ and $C_2$?.","The question is Let $S$ be that part of the surface of the paraboloid $z=x^2+y^2$ between the planes $z=1$ and $z=4$.  Using suitable diagrams, show how Stokes' theorem may be applied to this surface in order to write $\iint_s \nabla \times V.\hat ndS$ as the sum of two line integrals. I want to show clearly the direction of integration along the two curves assuming that the z components of $\hat n$ are positive. part (b) given $\vec{V}=x^3j+z^3k$ I want to evaluate both the surface intergal $\iint_s\nabla\times V.\hat n dS$ and the sum of the line intergrals $\int_{C_{1}} V.dR +\int_{C_{2}} V.dR$ where $C_1$ and $C_2$ are two curves bounding S, hence verifying stokes' theorem for this case. My questions are: 1) How does one find the upper and lower limits of the intergrals clearly one of the integrals should be z=4 and z=1 but how can one calculate the ones for x? 2) Do we need to calculate $\hat n$ and if so how? is it just $\frac{\nabla V}{|\nabla V|}=\hat n$ 3) What is the diagram supposed to look like because I'm finding it very hard to believe my diagram is correct? 4) Could someone show me the working for when evaluating the surface intergral specifically  update would we substitute the values of $z$ into $z=x^2+y^2$ to find the curves $C_1$ and $C_2$?.",,"['multivariable-calculus', 'vectors']"
80,Positive-definite derivative implies injective function,Positive-definite derivative implies injective function,,"Suppose $f:\mathbb R^n\to \mathbb R^n$ is differentiable and that $Df$ is positive-definite at every point. As homework, I need to prove $f$ is injective. I thought about proving by contradiction. If $f(a) = f(b) ,a\neq b$, consider the straight line  $\gamma:a\to b$ in $\mathbb R^n$, the composition $f\circ \gamma:\mathbb R\to \mathbb R$ (we take the codomain as $\mathbb R$ only) allows using the mean value theorem to find some $c\in (a,b)$ for which $(f\circ \gamma)^\prime (c)=0$. Then, by the chain rule $(f\circ \gamma)^\prime(c)=Df|_{\gamma c} \gamma^\prime(c)=0$. At this point I'm stuck. I don't see how to get to an inner product to contradict positive definiteness. Positive-definiteness implies invertibility whence $\gamma^\prime(c)=0$, which can't happen for a straight line, so it looks like the weaker hypothesis that $Df$ is everywhere invertible implies $f$ is injective. But that doesn't sound right...","Suppose $f:\mathbb R^n\to \mathbb R^n$ is differentiable and that $Df$ is positive-definite at every point. As homework, I need to prove $f$ is injective. I thought about proving by contradiction. If $f(a) = f(b) ,a\neq b$, consider the straight line  $\gamma:a\to b$ in $\mathbb R^n$, the composition $f\circ \gamma:\mathbb R\to \mathbb R$ (we take the codomain as $\mathbb R$ only) allows using the mean value theorem to find some $c\in (a,b)$ for which $(f\circ \gamma)^\prime (c)=0$. Then, by the chain rule $(f\circ \gamma)^\prime(c)=Df|_{\gamma c} \gamma^\prime(c)=0$. At this point I'm stuck. I don't see how to get to an inner product to contradict positive definiteness. Positive-definiteness implies invertibility whence $\gamma^\prime(c)=0$, which can't happen for a straight line, so it looks like the weaker hypothesis that $Df$ is everywhere invertible implies $f$ is injective. But that doesn't sound right...",,"['multivariable-calculus', 'derivatives']"
81,What is the meaning of the inner product of a vector and a gradient?,What is the meaning of the inner product of a vector and a gradient?,,"Recently I came across the following expression: $$ \langle \nabla f(x_1),x_2 \rangle$$ I do understand how to calculate the value of the expression. You take the derivatives of each of the entry in the function and then substitute $x_1$ in it and then take the dot product between the resulting value and $x_2$. But is there any meaning for this expression? What does this expression represent? Please help.","Recently I came across the following expression: $$ \langle \nabla f(x_1),x_2 \rangle$$ I do understand how to calculate the value of the expression. You take the derivatives of each of the entry in the function and then substitute $x_1$ in it and then take the dot product between the resulting value and $x_2$. But is there any meaning for this expression? What does this expression represent? Please help.",,"['multivariable-calculus', 'derivatives', 'vectors', 'scalar-fields']"
82,What is the relation between the Laplacian Operator and the Laplacian Matrix? [duplicate],What is the relation between the Laplacian Operator and the Laplacian Matrix? [duplicate],,This question already has answers here : Discrete Laplacian (2 answers) Closed 7 years ago . In what way are the Laplacian operator (defined on functions from $\Bbb{R}^n$ to $\Bbb{R}$ which are twice-differentiable) and the Laplacian matrix (defined on simple graphs) similar or related?,This question already has answers here : Discrete Laplacian (2 answers) Closed 7 years ago . In what way are the Laplacian operator (defined on functions from $\Bbb{R}^n$ to $\Bbb{R}$ which are twice-differentiable) and the Laplacian matrix (defined on simple graphs) similar or related?,,"['multivariable-calculus', 'graph-theory', 'laplacian', 'spectral-graph-theory']"
83,existence of a limit of a function $f:\mathbb{R}^2 \to \mathbb{R}$,existence of a limit of a function,f:\mathbb{R}^2 \to \mathbb{R},"Can anyone calculate the value of K. It seems 0 is the value as I have seen it in many questions but not sure. If anyone can arrive at this value, then please.","Can anyone calculate the value of K. It seems 0 is the value as I have seen it in many questions but not sure. If anyone can arrive at this value, then please.",,['multivariable-calculus']
84,Evaluating $\iint_V x^4+y^4+z^4$ with the divergence theorem,Evaluating  with the divergence theorem,\iint_V x^4+y^4+z^4,"Use Gauss divergence theorem to evaluate $$\iint_S \left(x^{4} + y^{4} + z^{4}\right)$$ over sphere S of radius $a$ . So I wrote this as $$ \begin{align} &a\iint_{\partial V} \Big(x^3 \hat{i}+y^3\hat{j}+y^3\hat{k}\Big)\cdot\Big(\frac{x\hat{i}+y\hat{j}+z\hat{k}}{a}\Big)\\ =\ &a\iiint_V \operatorname{div}(x^3,y^3,z^3)\\ =\ &3a\iiint_Va^2=3a^3\frac{4\pi}{3}a^3=4\pi a^6 \end{align}$$ But my answer is not matching. The answer key says it should be $\frac{12 \pi a^6}{5}$",Use Gauss divergence theorem to evaluate over sphere S of radius . So I wrote this as But my answer is not matching. The answer key says it should be,"\iint_S \left(x^{4} + y^{4} + z^{4}\right) a 
\begin{align}
&a\iint_{\partial V} \Big(x^3 \hat{i}+y^3\hat{j}+y^3\hat{k}\Big)\cdot\Big(\frac{x\hat{i}+y\hat{j}+z\hat{k}}{a}\Big)\\
=\ &a\iiint_V \operatorname{div}(x^3,y^3,z^3)\\
=\ &3a\iiint_Va^2=3a^3\frac{4\pi}{3}a^3=4\pi a^6
\end{align} \frac{12 \pi a^6}{5}","['multivariable-calculus', 'surface-integrals', 'divergence-theorem']"
85,"Find the biggest number $M$ such that the inequality $a^2+b^{1389} \ge Mab$ holds for every $a,b \in [0,1]$.",Find the biggest number  such that the inequality  holds for every .,"M a^2+b^{1389} \ge Mab a,b \in [0,1]","Find the biggest number $M$ such that the following inequality holds for every $a,b \in [0,1]$, $$a^2+b^{1389} \ge Mab$$ My attempt :We should find the minimum of $\frac{a}{b}+\frac{b^{1388}}{a}$.By putting $a=b=0^{+}$ we will get to $1 \ge M$ and clearly $M \ge 0$ but how much is $M$?","Find the biggest number $M$ such that the following inequality holds for every $a,b \in [0,1]$, $$a^2+b^{1389} \ge Mab$$ My attempt :We should find the minimum of $\frac{a}{b}+\frac{b^{1388}}{a}$.By putting $a=b=0^{+}$ we will get to $1 \ge M$ and clearly $M \ge 0$ but how much is $M$?",,"['multivariable-calculus', 'inequality', 'optimization', 'maxima-minima', 'a.m.-g.m.-inequality']"
86,Volume of this peculiar set,Volume of this peculiar set,,"What is the volume of the set $$S=\{x=(x_1,x_2,x_3,\ldots,x_n) \in \mathbb{R}^n \mid 0\leq x_1 \leq x_2\leq x_3\leq\cdots\leq x_n\leq1\}\text{?}$$ I think this is related to the volume of the unit ball in $\mathbb{R}^n$. Any ideas. Thanks beforehand.","What is the volume of the set $$S=\{x=(x_1,x_2,x_3,\ldots,x_n) \in \mathbb{R}^n \mid 0\leq x_1 \leq x_2\leq x_3\leq\cdots\leq x_n\leq1\}\text{?}$$ I think this is related to the volume of the unit ball in $\mathbb{R}^n$. Any ideas. Thanks beforehand.",,"['calculus', 'multivariable-calculus']"
87,Find the pdf of a bivariate transformation using change of variables,Find the pdf of a bivariate transformation using change of variables,,"Problem $X,Y\sim n(0,1)$. $X,Y$ are indpendent.  $U = X+Y$, $V=X-Y$. Find $f_{U,V}(u,v).$ Work I've made some progress, but there are a number of steps remaining and I'm not sure how to proceed. I know there's a formula for this sort of thing $$f_{U,V}(u,v) = f_{X,Y}(h1(u,v), h2(u,v))|J|$$ but I wanted to try working it out as explicitly as possible, using the definitions and integral change of variables in order to see what's going on under the hood. $$f_{U,V}(u,v)$$ $$= \frac{\partial}{\partial u \partial v} F_{U,V}(u,v)$$ by the second FTC. $$= \frac{\partial}{\partial u \partial v} \int_{\{(u,v): U < u, V < v\}} f_{U,V}(u,v)dudv $$ by the definition of $F_{U,V}$. So we'll change variables from U-V into X-Y coordinates. First we'll measure the area element $$dudv = \frac{\partial(u,v)}{\partial(x,y)}dxdy=2dxdy$$ using the Jacobian determinant, so that we can rescale appropriately the sum of the various volume chunks. Second, we change the integrand. Do we plug in values of $u=x+y$ and $v=x-y$ in the integrand? This seems like it would make sense given that we want to replace $u$ and $v$ with $x$ and $y$, and this is the correct equivalence between the two pairs. Third, we change the region.  $$R=\{(u,v):U < u, V < v\}$$ $$=\{(u,v):X+Y < u, X-Y < v\}$$ but I'm not sure what we're getting out of this. Fourth, we have to do something with the partial derivatives. Perhaps chain rule, or something cool with the FTC -- but I don't know how that would work if we've already changed out our $u$ and $v$ that we're supposed to be differentiating with respect to. A big part of learning how to do this is surely in getting used to the notation and the manipulations, so I'd most appreciate as explicit a solution as possible, showing how each manipulation works and is motivated.","Problem $X,Y\sim n(0,1)$. $X,Y$ are indpendent.  $U = X+Y$, $V=X-Y$. Find $f_{U,V}(u,v).$ Work I've made some progress, but there are a number of steps remaining and I'm not sure how to proceed. I know there's a formula for this sort of thing $$f_{U,V}(u,v) = f_{X,Y}(h1(u,v), h2(u,v))|J|$$ but I wanted to try working it out as explicitly as possible, using the definitions and integral change of variables in order to see what's going on under the hood. $$f_{U,V}(u,v)$$ $$= \frac{\partial}{\partial u \partial v} F_{U,V}(u,v)$$ by the second FTC. $$= \frac{\partial}{\partial u \partial v} \int_{\{(u,v): U < u, V < v\}} f_{U,V}(u,v)dudv $$ by the definition of $F_{U,V}$. So we'll change variables from U-V into X-Y coordinates. First we'll measure the area element $$dudv = \frac{\partial(u,v)}{\partial(x,y)}dxdy=2dxdy$$ using the Jacobian determinant, so that we can rescale appropriately the sum of the various volume chunks. Second, we change the integrand. Do we plug in values of $u=x+y$ and $v=x-y$ in the integrand? This seems like it would make sense given that we want to replace $u$ and $v$ with $x$ and $y$, and this is the correct equivalence between the two pairs. Third, we change the region.  $$R=\{(u,v):U < u, V < v\}$$ $$=\{(u,v):X+Y < u, X-Y < v\}$$ but I'm not sure what we're getting out of this. Fourth, we have to do something with the partial derivatives. Perhaps chain rule, or something cool with the FTC -- but I don't know how that would work if we've already changed out our $u$ and $v$ that we're supposed to be differentiating with respect to. A big part of learning how to do this is surely in getting used to the notation and the manipulations, so I'd most appreciate as explicit a solution as possible, showing how each manipulation works and is motivated.",,"['probability', 'integration', 'multivariable-calculus', 'random-variables', 'normal-distribution']"
88,Higher order partial derivatives of implicit function?,Higher order partial derivatives of implicit function?,,"Given the surface $z = f(x,y)$, with parameterization $x = u + v^2$, $y = u^2 - v^3$, $z = 2uv$ near the point $(3,3,4)$ which corresponds to the point $(2,1)$ in the $uv$-plane, find $\frac{\partial^2 f}{\partial x \partial y}(3,3)$. I'm familiar with using Jacobians to find first partial derivatives, but not how to find higher order partial derivatives of multivariate implicit functions .","Given the surface $z = f(x,y)$, with parameterization $x = u + v^2$, $y = u^2 - v^3$, $z = 2uv$ near the point $(3,3,4)$ which corresponds to the point $(2,1)$ in the $uv$-plane, find $\frac{\partial^2 f}{\partial x \partial y}(3,3)$. I'm familiar with using Jacobians to find first partial derivatives, but not how to find higher order partial derivatives of multivariate implicit functions .",,"['multivariable-calculus', 'partial-derivative', 'implicit-function-theorem']"
89,Proof regarding function giving the length of a curve restricted to an interval,Proof regarding function giving the length of a curve restricted to an interval,,"Consider a parametric curve $\gamma(t): [a,b] \subset \mathbb{R} \to \mathbb{R}^n$. Let the interval $[a,b]$ be divided by a partition $\mathscr{P}=\{ t_1=a, t_2,..,t_{N-1},t_N=b \}$. The lenght of the curve is defined as  $$L(\gamma):=\sup_{\mathscr{P}} \sum_{i=1}^{N}|\gamma(t_i)-\gamma(t_{i-1})|\tag{1}$$ Where $\mathscr{P}$ may vary among all the possible partition of $[a,b]$. Now consider a function $S(t):[a,b] \to [0,L(\gamma)]$ defined as $$S(t) := L(\gamma |_{[a,t]})$$ ($\gamma |_{[a,t]}$ is the restriction of $\gamma$ to the interval $[a,t]$). Given an $h>0$ how can I prove the following using the definition of lenght given above? $$S(t+h)-S(t)=L(\gamma|_{[t,t+h]}) \tag{2}$$ ($\gamma |_{[t,t+h]}$ is the restriction of $\gamma$ to the interval $[t,t+h]$). Edit : As said  the proof of $(2)$ should make use only of the definition of lenght as given above, that is $(1)$. In particular it should not use the formula  $$L(\gamma)=\int_{a}^{b} ||\gamma'(t)|| dt$$","Consider a parametric curve $\gamma(t): [a,b] \subset \mathbb{R} \to \mathbb{R}^n$. Let the interval $[a,b]$ be divided by a partition $\mathscr{P}=\{ t_1=a, t_2,..,t_{N-1},t_N=b \}$. The lenght of the curve is defined as  $$L(\gamma):=\sup_{\mathscr{P}} \sum_{i=1}^{N}|\gamma(t_i)-\gamma(t_{i-1})|\tag{1}$$ Where $\mathscr{P}$ may vary among all the possible partition of $[a,b]$. Now consider a function $S(t):[a,b] \to [0,L(\gamma)]$ defined as $$S(t) := L(\gamma |_{[a,t]})$$ ($\gamma |_{[a,t]}$ is the restriction of $\gamma$ to the interval $[a,t]$). Given an $h>0$ how can I prove the following using the definition of lenght given above? $$S(t+h)-S(t)=L(\gamma|_{[t,t+h]}) \tag{2}$$ ($\gamma |_{[t,t+h]}$ is the restriction of $\gamma$ to the interval $[t,t+h]$). Edit : As said  the proof of $(2)$ should make use only of the definition of lenght as given above, that is $(1)$. In particular it should not use the formula  $$L(\gamma)=\int_{a}^{b} ||\gamma'(t)|| dt$$",,"['calculus', 'multivariable-calculus']"
90,"Getting negative the correct answer in $\iint_R e^{x+y}dA, R=\{(x,y)\quad |\quad|x|+|y|\leq 1\}$",Getting negative the correct answer in,"\iint_R e^{x+y}dA, R=\{(x,y)\quad |\quad|x|+|y|\leq 1\}","$$\iint_R e^{x+y}dA, R=\{(x,y)\quad |\quad|x|+|y|\leq 1\}$$ I have tried setting $u=y-x$ and $v=x+y$. Then I calculated the jacobian as $-\frac{1}{2}$. So the integral becomes: $$-\frac{1}{2}\int_{-1}^1\int_{-1}^1e^vdvdu=e^{-1}-e$$ However, the book says that the answer should be $e-e^{-1}$. Any idea where I went wrong?","$$\iint_R e^{x+y}dA, R=\{(x,y)\quad |\quad|x|+|y|\leq 1\}$$ I have tried setting $u=y-x$ and $v=x+y$. Then I calculated the jacobian as $-\frac{1}{2}$. So the integral becomes: $$-\frac{1}{2}\int_{-1}^1\int_{-1}^1e^vdvdu=e^{-1}-e$$ However, the book says that the answer should be $e-e^{-1}$. Any idea where I went wrong?",,"['integration', 'multivariable-calculus']"
91,Proving that there exists a unique hyperplane that passes through $n$ points,Proving that there exists a unique hyperplane that passes through  points,n,"I'm trying to prove: Given $n$ points $x_0,...,x_{n-1}$ such that $x_1-x_0,...,x_{n-1}-x_0$ are linearly independent, then there exists a unique hyperplane that passes through the $n$ points. The problem is that I can only use the definition provided in the book, which is $$P=\{x\in \mathbb{R}^n:z\cdot x=c\}$$ for given $ c\in \mathbb{R},z\in \mathbb{R}^n\setminus \{0\}$ I'm at a loss here. Any help would be appreciated.","I'm trying to prove: Given $n$ points $x_0,...,x_{n-1}$ such that $x_1-x_0,...,x_{n-1}-x_0$ are linearly independent, then there exists a unique hyperplane that passes through the $n$ points. The problem is that I can only use the definition provided in the book, which is $$P=\{x\in \mathbb{R}^n:z\cdot x=c\}$$ for given $ c\in \mathbb{R},z\in \mathbb{R}^n\setminus \{0\}$ I'm at a loss here. Any help would be appreciated.",,"['linear-algebra', 'multivariable-calculus']"
92,Derivative notation between $D$ and $\nabla$,Derivative notation between  and,D \nabla,"Sorry if this is likely a duplicate question (and that I am asking two questions) but I am confused on some notation. For a function $f(x_1, x_2): \mathbb{R}^2 \to \mathbb{R}$, for simplicity, why does the product rule tell us that $$ \displaystyle Df(x_1, x_2) = \frac{\partial f(x_1, x_2)}{\partial x_1} \ dx_1 + \frac{\partial f(x_1, x_2)}{\partial x_2} \ dx_2 $$ Wouldn't we need a given functional form to know to apply the product rule (ie. if $f(x_1, x_2) = x_1 / x_2$ then we apply the quotient rule. Further given the above, since the codomain of $f$ is scalar, it does not seem to match the following identity: $$ \nabla f= [Df]^T $$ That is $Df$ is the transpose of the gradient of $f$, a vector. (edit: I read these in Microeconomic Theory by Mas-Collel, Whinston and Green)","Sorry if this is likely a duplicate question (and that I am asking two questions) but I am confused on some notation. For a function $f(x_1, x_2): \mathbb{R}^2 \to \mathbb{R}$, for simplicity, why does the product rule tell us that $$ \displaystyle Df(x_1, x_2) = \frac{\partial f(x_1, x_2)}{\partial x_1} \ dx_1 + \frac{\partial f(x_1, x_2)}{\partial x_2} \ dx_2 $$ Wouldn't we need a given functional form to know to apply the product rule (ie. if $f(x_1, x_2) = x_1 / x_2$ then we apply the quotient rule. Further given the above, since the codomain of $f$ is scalar, it does not seem to match the following identity: $$ \nabla f= [Df]^T $$ That is $Df$ is the transpose of the gradient of $f$, a vector. (edit: I read these in Microeconomic Theory by Mas-Collel, Whinston and Green)",,"['multivariable-calculus', 'differential-geometry', 'notation', 'vectors', 'differential-forms']"
93,Conceptual understanding of Line Integration,Conceptual understanding of Line Integration,,"From Calculus II, it's easy to understand what is basically being calculated by integration; the area under a curve. $\int_a^bf(x) \, dx =$  the area under $f(x)$ within the interval $[a,b]$. However, a line integral is a bit more complicated. I completely understand the computation of a line integral; it isn't too difficult to understand. However, I lack the understanding of what exactly is being calculated by it. Is it the area under the integrated curve, treating the path along which it is integrated as though it were the $x$-axis? A visual example would be greatly appreciated! Thanks in advance.","From Calculus II, it's easy to understand what is basically being calculated by integration; the area under a curve. $\int_a^bf(x) \, dx =$  the area under $f(x)$ within the interval $[a,b]$. However, a line integral is a bit more complicated. I completely understand the computation of a line integral; it isn't too difficult to understand. However, I lack the understanding of what exactly is being calculated by it. Is it the area under the integrated curve, treating the path along which it is integrated as though it were the $x$-axis? A visual example would be greatly appreciated! Thanks in advance.",,"['calculus', 'multivariable-calculus', 'definite-integrals']"
94,Show the triple integral given is equivalent to $\frac{15\pi}{16}$,Show the triple integral given is equivalent to,\frac{15\pi}{16},"Evaluate $$\iiint_E\;z \, dV$$ where E is enclosed between the spheres $x^2 + y^2 + z^2 = 1$ and $x^2 + y^2 + z^2 = 4$ in the first octant. I'll be honest. My first attempt didn't worked very well. My answer showed for my first attempt was 7pi/12 because the integral I set up was like this: $$\iiint _E\;z\;dV\\ 1\leq \rho\leq 2\\ 0\leq\theta\leq \frac{1}{2}\pi\\ 0\leq\phi\leq \frac{1}{2}\pi$$ Which transforms the integral into $$\int_{0}^{\frac{1}{2}\pi}\int_{0}^{\frac{1}{2}\pi}\int_{1}^{2}(\rho)^2 cos\phi\sin\phi\ d\rho\; d\theta \;d\phi\\$$ This problem was taken from Stewart Calculus 6th Edition. Like I said, I tried to solve this problem on my own and got an answer of $$\frac {7}{12}\pi$$ but when I browsed the answers section, it says its answer was $$\frac {15}{16}\pi$$ Did I do something wrong in my computation?","Evaluate $$\iiint_E\;z \, dV$$ where E is enclosed between the spheres $x^2 + y^2 + z^2 = 1$ and $x^2 + y^2 + z^2 = 4$ in the first octant. I'll be honest. My first attempt didn't worked very well. My answer showed for my first attempt was 7pi/12 because the integral I set up was like this: $$\iiint _E\;z\;dV\\ 1\leq \rho\leq 2\\ 0\leq\theta\leq \frac{1}{2}\pi\\ 0\leq\phi\leq \frac{1}{2}\pi$$ Which transforms the integral into $$\int_{0}^{\frac{1}{2}\pi}\int_{0}^{\frac{1}{2}\pi}\int_{1}^{2}(\rho)^2 cos\phi\sin\phi\ d\rho\; d\theta \;d\phi\\$$ This problem was taken from Stewart Calculus 6th Edition. Like I said, I tried to solve this problem on my own and got an answer of $$\frac {7}{12}\pi$$ but when I browsed the answers section, it says its answer was $$\frac {15}{16}\pi$$ Did I do something wrong in my computation?",,"['calculus', 'multivariable-calculus', 'definite-integrals', 'spherical-coordinates']"
95,Converting Ellipse Integration Boundaries To Cylindrical Coordinates,Converting Ellipse Integration Boundaries To Cylindrical Coordinates,,"I'm having the following integral, and I'm being asked to convert the integration boundaries to cylindrical coordinates. I've figured out that on XY-plane it's an ellipse having the following equation: $$ \frac{(x-\frac{a}{2})^2}{\frac{9a^2}{4}}+\frac{y^2}{\frac{9a^2}{2}} = 1 $$ I realize that $ 0\leq \theta \leq \frac{\pi}{2}$ and  $0 \leq z \leq r^2(\theta)$ But I don't seem to be able to reach an expression for $r(\theta)$. Answer says that $r(\theta) = 2acos(\theta)$. Any ideas? Thank you.","I'm having the following integral, and I'm being asked to convert the integration boundaries to cylindrical coordinates. I've figured out that on XY-plane it's an ellipse having the following equation: $$ \frac{(x-\frac{a}{2})^2}{\frac{9a^2}{4}}+\frac{y^2}{\frac{9a^2}{2}} = 1 $$ I realize that $ 0\leq \theta \leq \frac{\pi}{2}$ and  $0 \leq z \leq r^2(\theta)$ But I don't seem to be able to reach an expression for $r(\theta)$. Answer says that $r(\theta) = 2acos(\theta)$. Any ideas? Thank you.",,"['integration', 'multivariable-calculus', 'coordinate-systems']"
96,Multidimensional taylor series $sin (x^3y^2) $,Multidimensional taylor series,sin (x^3y^2) ,"A homework of mine was to compute the Taylor series of $f(x,y)=\sin(x^3y^2)$ around $(0,0)$ to the 25th order. I assumed, as $\sin(z)=\sum\limits^{\infty}_{k=0}(-1)^k\frac{z^{2k+1}}{(2k+1)!}$, that I could just plug in my $x^3y^2$ in the formula and that this was covered by the identity theorem for power series. Is this right? Can somebody give me the exact reasoning?","A homework of mine was to compute the Taylor series of $f(x,y)=\sin(x^3y^2)$ around $(0,0)$ to the 25th order. I assumed, as $\sin(z)=\sum\limits^{\infty}_{k=0}(-1)^k\frac{z^{2k+1}}{(2k+1)!}$, that I could just plug in my $x^3y^2$ in the formula and that this was covered by the identity theorem for power series. Is this right? Can somebody give me the exact reasoning?",,"['multivariable-calculus', 'power-series', 'taylor-expansion']"
97,Ellipsoid moment of inertia matrix,Ellipsoid moment of inertia matrix,,"Some background info: torque $\tau$ is defined as $$\tau = I*d\omega$$ Where $I$ is the moment of inertia matrix and $d\omega$ is an object's rotational acceleration. As I understand it, the inertia matrix acts just like mass in that it counteracts the torque (for example, if an object is spinning around the x axis, a big value of $I_{xx}$ means that the object needs more torque around the x axis in order to spin). However, angular momentum $M$ can be defined as $$M=I * \omega$$ Where $\omega$ is the rotational velocity. So it seems that torque is the time derivative of angular momentum. Using these facts, how would I find the moment of inertia matrix for an ellipsoid with uniform density of the form $$\frac{x^2}{a}+\frac{y^2}{b}+\frac{z^2}{c}≤9$$ with $a≠b≠c≠0$? Would I have to use spherical coordinates somehow? I'm not given any torque or angular velocity information. Any guidance is appreciated.","Some background info: torque $\tau$ is defined as $$\tau = I*d\omega$$ Where $I$ is the moment of inertia matrix and $d\omega$ is an object's rotational acceleration. As I understand it, the inertia matrix acts just like mass in that it counteracts the torque (for example, if an object is spinning around the x axis, a big value of $I_{xx}$ means that the object needs more torque around the x axis in order to spin). However, angular momentum $M$ can be defined as $$M=I * \omega$$ Where $\omega$ is the rotational velocity. So it seems that torque is the time derivative of angular momentum. Using these facts, how would I find the moment of inertia matrix for an ellipsoid with uniform density of the form $$\frac{x^2}{a}+\frac{y^2}{b}+\frac{z^2}{c}≤9$$ with $a≠b≠c≠0$? Would I have to use spherical coordinates somehow? I'm not given any torque or angular velocity information. Any guidance is appreciated.",,"['multivariable-calculus', 'physics', 'classical-mechanics', 'elliptic-integrals']"
98,Show me how to evaluate $\int_0^1\int_0^1\frac{[-\ln(x)]^s}{1-xy}dxdy=\frac{\zeta(s+2)}{\Gamma(s+2)}$,Show me how to evaluate,\int_0^1\int_0^1\frac{[-\ln(x)]^s}{1-xy}dxdy=\frac{\zeta(s+2)}{\Gamma(s+2)},"Double integrals $$\int_0^1\int_0^1\frac{[-\ln(x)]^s}{1-xy}dxdy=\frac{\zeta(s+2)}{\Gamma(s+2)} \tag1$$ $$\int_0^1\int_0^1\frac{[-\ln(xy)]^s}{1-xy}dxdy=\zeta(s+2)\Gamma(s+2) \tag2$$ Where $\sum_{n=0}^{\infty}\frac{1}{(n+1)^s}=\zeta(s)$, valid for $\Re(s)>1$ and $\Gamma(n+1)=n!$ valid for all non-negative integers and rational arguments. I came across these two double integrals during the time I was on Wolfram integrator, was trying to search for something. It didn't gave me the closed form, just the numerical values and the rest I had to find the closed form base on these values. I don't know how to prove these integrals, can somebody show me how to prove it, so I can learn from it, so next time I can independently do it myself. I hope these closed form are correct. P.s Please, try not to miss too many steps. Thank you!","Double integrals $$\int_0^1\int_0^1\frac{[-\ln(x)]^s}{1-xy}dxdy=\frac{\zeta(s+2)}{\Gamma(s+2)} \tag1$$ $$\int_0^1\int_0^1\frac{[-\ln(xy)]^s}{1-xy}dxdy=\zeta(s+2)\Gamma(s+2) \tag2$$ Where $\sum_{n=0}^{\infty}\frac{1}{(n+1)^s}=\zeta(s)$, valid for $\Re(s)>1$ and $\Gamma(n+1)=n!$ valid for all non-negative integers and rational arguments. I came across these two double integrals during the time I was on Wolfram integrator, was trying to search for something. It didn't gave me the closed form, just the numerical values and the rest I had to find the closed form base on these values. I don't know how to prove these integrals, can somebody show me how to prove it, so I can learn from it, so next time I can independently do it myself. I hope these closed form are correct. P.s Please, try not to miss too many steps. Thank you!",,"['integration', 'multivariable-calculus']"
99,Taking curl of Euler equation,Taking curl of Euler equation,,"Consider an inviscid incompressible ﬂow. Euler’s equation can be written as $$\frac{\partial \textbf u}{\partial t} + \textbf ω × \textbf u = −\textbf∇\bigg( \frac pρ + \frac 12 \textbf u^2 + V \bigg)$$ where the vorticity $\textbf ω = \textbf ∇ × \textbf u$. By taking the curl of this equation and using the vector identity $\textbf ∇ × (\textbf a × \textbf b) = (\textbf b \cdot \textbf ∇)\textbf a − (\textbf a\cdot \textbf∇)\textbf b + \textbf a(\textbf∇ \cdot \textbf b) − \textbf b(\textbf∇ \cdot \textbf a)$ show that $$\frac{D \textbf ω}{Dt} = (\textbf ω \cdot \textbf ∇)\textbf u$$ This is the vorticity equation. I am very stuck on this. Does taking the curl mean we have to do this:$$\textbf ∇ \times \bigg( \frac{\partial \textbf u}{\partial t} + \textbf ω × \textbf u \bigg)= −\textbf ∇ \times \bigg( \textbf∇\bigg( \frac pρ + \frac 12 \textbf u^2 + V \bigg) \bigg)$$ and the LHS becomes $$\textbf ∇ \times  \frac{\partial \textbf u}{\partial t}+ \textbf ∇ \times (\textbf ω × \textbf u )$$ and then use the identity? I was not sure if we can just kind off expand the LHS like how I did. But even still, after using the identity, it gets me nowhere.","Consider an inviscid incompressible ﬂow. Euler’s equation can be written as $$\frac{\partial \textbf u}{\partial t} + \textbf ω × \textbf u = −\textbf∇\bigg( \frac pρ + \frac 12 \textbf u^2 + V \bigg)$$ where the vorticity $\textbf ω = \textbf ∇ × \textbf u$. By taking the curl of this equation and using the vector identity $\textbf ∇ × (\textbf a × \textbf b) = (\textbf b \cdot \textbf ∇)\textbf a − (\textbf a\cdot \textbf∇)\textbf b + \textbf a(\textbf∇ \cdot \textbf b) − \textbf b(\textbf∇ \cdot \textbf a)$ show that $$\frac{D \textbf ω}{Dt} = (\textbf ω \cdot \textbf ∇)\textbf u$$ This is the vorticity equation. I am very stuck on this. Does taking the curl mean we have to do this:$$\textbf ∇ \times \bigg( \frac{\partial \textbf u}{\partial t} + \textbf ω × \textbf u \bigg)= −\textbf ∇ \times \bigg( \textbf∇\bigg( \frac pρ + \frac 12 \textbf u^2 + V \bigg) \bigg)$$ and the LHS becomes $$\textbf ∇ \times  \frac{\partial \textbf u}{\partial t}+ \textbf ∇ \times (\textbf ω × \textbf u )$$ and then use the identity? I was not sure if we can just kind off expand the LHS like how I did. But even still, after using the identity, it gets me nowhere.",,"['multivariable-calculus', 'vector-spaces', 'fluid-dynamics', 'euler-lagrange-equation']"
