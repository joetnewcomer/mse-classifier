,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Application for mean value theorem,Application for mean value theorem,,"$f(x)$ is three-times differentiable on $[a,b]$, how to show that there is $\varepsilon\in(a,b)$ such that $$f(b)=f(a)+\cfrac{1}{2}(b-a)[f'(a)+f'(b)]-\cfrac{1}{12}(b-a)^3f'''(\varepsilon)$$","$f(x)$ is three-times differentiable on $[a,b]$, how to show that there is $\varepsilon\in(a,b)$ such that $$f(b)=f(a)+\cfrac{1}{2}(b-a)[f'(a)+f'(b)]-\cfrac{1}{12}(b-a)^3f'''(\varepsilon)$$",,"['calculus', 'derivatives', 'taylor-expansion']"
1,Differentiating under the integral sign problem,Differentiating under the integral sign problem,,"Knowing that $$\int_0^\infty e^{-x^2}\,dx = \frac{\sqrt{\pi}}{2},$$ evaluate the integral $$\int_0^\infty e^{-x^2y+1}\,dx.$$ for $y > 0$","Knowing that $$\int_0^\infty e^{-x^2}\,dx = \frac{\sqrt{\pi}}{2},$$ evaluate the integral $$\int_0^\infty e^{-x^2y+1}\,dx.$$ for $y > 0$",,"['calculus', 'derivatives', 'improper-integrals']"
2,Is there any graphical explanation of the derivative of $\sin x$? [duplicate],Is there any graphical explanation of the derivative of ? [duplicate],\sin x,This question already has answers here : Intuitive understanding of the derivatives of $\sin x$ and $\cos x$ (17 answers) Closed 9 years ago . I'm trying to understand in a practical/graphical view the derivative of $\sin(x)$ (that results in $\cos(x)$). Is there any animation or illustration explaining that?,This question already has answers here : Intuitive understanding of the derivatives of $\sin x$ and $\cos x$ (17 answers) Closed 9 years ago . I'm trying to understand in a practical/graphical view the derivative of $\sin(x)$ (that results in $\cos(x)$). Is there any animation or illustration explaining that?,,"['calculus', 'intuition', 'derivatives']"
3,Why $\lim_{h→0+}\frac{ f (x + 2h) − f (x + h)}{h}$ is not a derivative?,Why  is not a derivative?,\lim_{h→0+}\frac{ f (x + 2h) − f (x + h)}{h},"Let $f :\mathbb{R}→\mathbb{R}$ be a continuous function with the property that $\lim_{h→0+}\frac{ f (x + 2h) − f (x + h)} {h} = 0$ , for all $x ∈ \mathbb R$ . Prove that $f$ is constant I’m not asking for a solution to this problem, I’m interested in why what is written cannot be considered a definition of a derivative. We can take $t=x+h$ and then $\lim_{h→0+}\frac{ f (t+h) − f (t)} {h} = 0$ and this is the definition of derivative. Or is the problem in $\lim_{h→0+}$ ,but $f$ is a continuous function, which means $\lim_{h→0+}f(x+h)=\lim_{h→0-}f(x+h)$ I will be glad if you can help me understand this problem.","Let be a continuous function with the property that , for all . Prove that is constant I’m not asking for a solution to this problem, I’m interested in why what is written cannot be considered a definition of a derivative. We can take and then and this is the definition of derivative. Or is the problem in ,but is a continuous function, which means I will be glad if you can help me understand this problem.","f :\mathbb{R}→\mathbb{R} \lim_{h→0+}\frac{ f (x + 2h) − f (x + h)}
{h} = 0 x ∈ \mathbb R f t=x+h \lim_{h→0+}\frac{ f (t+h) − f (t)} {h} = 0 \lim_{h→0+} f \lim_{h→0+}f(x+h)=\lim_{h→0-}f(x+h)","['real-analysis', 'derivatives']"
4,What does $f'(c) >0 $ mean at a point $x=c$?,What does  mean at a point ?,f'(c) >0  x=c,"I always understood that $f'(c) >0 $ at a point $x=c$ means that there exists a $\delta>0$ so that f is monotonically increasing in the interval $(c-\delta,c+\delta)$ . However in the book A basic course in real analysis they have given an example of the following form $f(x)=x+2x^2sin(1/x)$ for $x \ne 0$ and $f(0)=0$ they have asked to prove that $f'(0)=1$ but $f$ is not monotonic in any interval around $0$ . This example has made me even more confused. Can someone explain what does $f'(c)>0$ at the point $x=c$ even mean?",I always understood that at a point means that there exists a so that f is monotonically increasing in the interval . However in the book A basic course in real analysis they have given an example of the following form for and they have asked to prove that but is not monotonic in any interval around . This example has made me even more confused. Can someone explain what does at the point even mean?,"f'(c) >0  x=c \delta>0 (c-\delta,c+\delta) f(x)=x+2x^2sin(1/x) x \ne 0 f(0)=0 f'(0)=1 f 0 f'(c)>0 x=c","['real-analysis', 'derivatives', 'continuity']"
5,Implicit function differentiation methods,Implicit function differentiation methods,,"Q: If $$x\sqrt{1+y}+y\sqrt{1+x}=0$$ where $x,y\in\mathbb{R}$ then prove that: $$\dfrac{dy}{dx}=-\dfrac{1}{(1+x)^2}$$ My approach: $$x\sqrt{1+y}+y\sqrt{1+x}=0$$ $$\implies x\sqrt{1+y}=-y\sqrt{1+x}$$ $$\implies x^2(1+y)=y^2(1+x)$$ $$\implies x^2-y^2=xy(y-x)$$ $$\implies x+y=-xy$$ $$\implies y=-\frac{x}{1+x}$$ Now having converted to explicit form $y=f(x)$ we can differentiate and get the answer. I want to know if there is any other approach which avoids squaring or cancellation of terms?",Q: If where then prove that: My approach: Now having converted to explicit form we can differentiate and get the answer. I want to know if there is any other approach which avoids squaring or cancellation of terms?,"x\sqrt{1+y}+y\sqrt{1+x}=0 x,y\in\mathbb{R} \dfrac{dy}{dx}=-\dfrac{1}{(1+x)^2} x\sqrt{1+y}+y\sqrt{1+x}=0 \implies x\sqrt{1+y}=-y\sqrt{1+x} \implies x^2(1+y)=y^2(1+x) \implies x^2-y^2=xy(y-x) \implies x+y=-xy \implies y=-\frac{x}{1+x} y=f(x)","['calculus', 'derivatives', 'implicit-differentiation', 'implicit-function']"
6,Counterexample to the uniform convergence of a differentiable function sequence,Counterexample to the uniform convergence of a differentiable function sequence,,"I'm struggling trying to find a real function sequence $\{f_n\}_n$ such that $\forall n: f_n$ is defined on an open and limited interval $(a,b)$ ; $\forall n: f_n$ is everywhere differentiable (wrt $x$ ) on $(a,b)$ ; $\exists x_0\in (a,b)$ such that $\{f_n(x_0)\}_n$ converges; $\{f'_n\}_n$ is uniformly convergent on $(a,b)$ ; $\{f_n\}_n$ does not converges uniformly on $(a,b)$ . Now, conditions 2), 3), 4) are sufficient to guarantee pointwise convergence of $\{f_n\}_n$ on $(a,b)$ , let's say to a function $f:(a,b)\to\mathbb{R}$ , and the chance to interchange limit with differentiation in the sense that $$\forall x\in (a,b): f'(x)=\lim_n f'_n(x).$$ Moreover, under those same supposititions, we can have uniform convergence on every compact subinterval of $(a,b)$ . My goal is to find a function sequence that satisfies together all those five conditions: I suppose that the problem must come from openess of the functions' domain that messes up with the uniform convergence of the $\{f_n\}_n$ : unfortunately, all my attempts failed so I'm here to ask you some help to find a sequence like that. Any ideas?","I'm struggling trying to find a real function sequence such that is defined on an open and limited interval ; is everywhere differentiable (wrt ) on ; such that converges; is uniformly convergent on ; does not converges uniformly on . Now, conditions 2), 3), 4) are sufficient to guarantee pointwise convergence of on , let's say to a function , and the chance to interchange limit with differentiation in the sense that Moreover, under those same supposititions, we can have uniform convergence on every compact subinterval of . My goal is to find a function sequence that satisfies together all those five conditions: I suppose that the problem must come from openess of the functions' domain that messes up with the uniform convergence of the : unfortunately, all my attempts failed so I'm here to ask you some help to find a sequence like that. Any ideas?","\{f_n\}_n \forall n: f_n (a,b) \forall n: f_n x (a,b) \exists x_0\in (a,b) \{f_n(x_0)\}_n \{f'_n\}_n (a,b) \{f_n\}_n (a,b) \{f_n\}_n (a,b) f:(a,b)\to\mathbb{R} \forall x\in (a,b): f'(x)=\lim_n f'_n(x). (a,b) \{f_n\}_n","['real-analysis', 'derivatives', 'metric-spaces', 'uniform-convergence']"
7,A function with a point of slope of exactly $2$,A function with a point of slope of exactly,2,"Let $f:[0,1]\rightarrow\mathbb{R}$ be a function continuous on $[0,1]$ and differentiable on $(0,1)$ with $f(0)=f(1)$ and $f(\alpha)=f(\beta)+1$ for some $\alpha,\beta$ such that $0<\alpha<\beta<1$ . Prove that there exists some $\xi\in(0,1)$ such that $\lvert f^{\prime}(\xi)\rvert=2$ . I've come up with some approaches but none give me a complete solution: If $\beta-\alpha\leq\frac{1}{2}$ , then by the mean value theorem, there exists a point with a derivative less than or equal to $-2$ , and since $f(0)=f(1)$ by Rolle we also have a point with derivative $0$ so then by Darboux for any number $d$ in $[-2,0]$ there exists a point at which the derivative is exactly equal to $d$ . But I don't know how to deal with the case $\beta-\alpha>\frac{1}{2}$ . But also any other approach to finding a $c\in (0,1)$ for which $\lvert f^{\prime}(c)\rvert\geq 2$ would be sufficient for Darboux to finish the job... Let $g(x)=f(x)-2x$ be a function also defined on the segment $[0,1]$ then it is also continuous so by Weierstrass' extremum theorem it must reach its maximum value on its domain. And since $0=g(0)>g(1)=-2$ , the maximum isn't achieved at $1$ , so if I could also prove that it doesn't achive the max value at $0$ , then it would be that a maximum is achieved for some $x$ in $(0,1)$ , and from there by Fermat's theorem the derivative of $g$ should vanish, giving the desired claim. But again, I’m stuck, since I don’t know how to prove that the max isn't attained at $0$ , or equivalently that there exists $s\in(0,1)$ such that $g(s)>g(0)=0$ . I have been able to do the following: since $g(\alpha)-g(\beta)=1+2(\beta-\alpha)$ it must be that (a) $g(\alpha)\geq\frac{1}{2}+\beta-\alpha$ but then the right-hand side is strictly greater than zero which is sufficient for the claimed result; or (b) $-g(\beta)\geq\frac{1}{2}+\beta-\alpha$ with which I, once again, have no idea what to do with... I guess I could have taken $g(x)=f(x)+2x$ to get $f^{\prime}(\xi)=-2$ , since a point $\xi$ with slope $2$ will also give a point with slope $-2$ , because the function $f$ cannot be monotone because of $f(0)=f(1)$ . Right? I am sorry for such an elementary query, but I've tried this a couple of times and it just won't budge; I’m starting to develop serious self-esteem issues from this. I'm sure there's an elegant and easy solution and I just wish to see it so I can then deal with the frustration of not having come up with it myself. My deepest gratitudes to anyone willing to humour me with this problem.","Let be a function continuous on and differentiable on with and for some such that . Prove that there exists some such that . I've come up with some approaches but none give me a complete solution: If , then by the mean value theorem, there exists a point with a derivative less than or equal to , and since by Rolle we also have a point with derivative so then by Darboux for any number in there exists a point at which the derivative is exactly equal to . But I don't know how to deal with the case . But also any other approach to finding a for which would be sufficient for Darboux to finish the job... Let be a function also defined on the segment then it is also continuous so by Weierstrass' extremum theorem it must reach its maximum value on its domain. And since , the maximum isn't achieved at , so if I could also prove that it doesn't achive the max value at , then it would be that a maximum is achieved for some in , and from there by Fermat's theorem the derivative of should vanish, giving the desired claim. But again, I’m stuck, since I don’t know how to prove that the max isn't attained at , or equivalently that there exists such that . I have been able to do the following: since it must be that (a) but then the right-hand side is strictly greater than zero which is sufficient for the claimed result; or (b) with which I, once again, have no idea what to do with... I guess I could have taken to get , since a point with slope will also give a point with slope , because the function cannot be monotone because of . Right? I am sorry for such an elementary query, but I've tried this a couple of times and it just won't budge; I’m starting to develop serious self-esteem issues from this. I'm sure there's an elegant and easy solution and I just wish to see it so I can then deal with the frustration of not having come up with it myself. My deepest gratitudes to anyone willing to humour me with this problem.","f:[0,1]\rightarrow\mathbb{R} [0,1] (0,1) f(0)=f(1) f(\alpha)=f(\beta)+1 \alpha,\beta 0<\alpha<\beta<1 \xi\in(0,1) \lvert f^{\prime}(\xi)\rvert=2 \beta-\alpha\leq\frac{1}{2} -2 f(0)=f(1) 0 d [-2,0] d \beta-\alpha>\frac{1}{2} c\in (0,1) \lvert f^{\prime}(c)\rvert\geq 2 g(x)=f(x)-2x [0,1] 0=g(0)>g(1)=-2 1 0 x (0,1) g 0 s\in(0,1) g(s)>g(0)=0 g(\alpha)-g(\beta)=1+2(\beta-\alpha) g(\alpha)\geq\frac{1}{2}+\beta-\alpha -g(\beta)\geq\frac{1}{2}+\beta-\alpha g(x)=f(x)+2x f^{\prime}(\xi)=-2 \xi 2 -2 f f(0)=f(1)","['real-analysis', 'derivatives', 'continuity', 'rolles-theorem']"
8,Help differentating $f(x) = \sqrt\frac{x^2-1}{x^2+1}$,Help differentating,f(x) = \sqrt\frac{x^2-1}{x^2+1},"The equation I'm trying to differentiate is, $ f(x) = \sqrt\frac{x^2-1}{x^2+1}$ and I know the answer is meant to be $$=\frac{\frac{x\sqrt {x^2+1}}{\sqrt {x^2-1}}-\frac{x\sqrt {x^2-1}}{\sqrt {x^2+1}}}{x^2+1}$$ But when I do the working out I get this $$=\frac{(x^2-1)^\frac{1}{2}}{(x^2+1)^\frac{1}{2}}$$ $$=\frac{\frac{1}{2}(x^2-1)^\frac{-1}{2}\cdot2x\cdot(x^2+1)^\frac{1}{2}-(x^2-1)^\frac{1}{2}\cdot\frac{1}{2}(x^2+1)^\frac{-1}{2}\cdot2x}{x^2+1}$$ simplify $$=\frac{x(x^2-1)^\frac{-1}{2}\cdot(x^2+1)^\frac{1}{2}-(x^2-1)^\frac{1}{2}\cdot x(x^2+1)^\frac{-1}{2}}{x^2+1}$$ $$=\frac{\frac{\sqrt {x^2+1}}{x\sqrt {x^2-1}}-\frac{\sqrt {x^2-1}}{x\sqrt {x^2+1}}}{x^2+1}$$ As you can see two of my $x$ 's are in the wrong location, and I just can't figure out what I'm doing wrong.  Any help as to what steps I'm doing wrong or missing would be much appreciated.","The equation I'm trying to differentiate is, and I know the answer is meant to be But when I do the working out I get this simplify As you can see two of my 's are in the wrong location, and I just can't figure out what I'm doing wrong.  Any help as to what steps I'm doing wrong or missing would be much appreciated.", f(x) = \sqrt\frac{x^2-1}{x^2+1} =\frac{\frac{x\sqrt {x^2+1}}{\sqrt {x^2-1}}-\frac{x\sqrt {x^2-1}}{\sqrt {x^2+1}}}{x^2+1} =\frac{(x^2-1)^\frac{1}{2}}{(x^2+1)^\frac{1}{2}} =\frac{\frac{1}{2}(x^2-1)^\frac{-1}{2}\cdot2x\cdot(x^2+1)^\frac{1}{2}-(x^2-1)^\frac{1}{2}\cdot\frac{1}{2}(x^2+1)^\frac{-1}{2}\cdot2x}{x^2+1} =\frac{x(x^2-1)^\frac{-1}{2}\cdot(x^2+1)^\frac{1}{2}-(x^2-1)^\frac{1}{2}\cdot x(x^2+1)^\frac{-1}{2}}{x^2+1} =\frac{\frac{\sqrt {x^2+1}}{x\sqrt {x^2-1}}-\frac{\sqrt {x^2-1}}{x\sqrt {x^2+1}}}{x^2+1} x,['derivatives']
9,Derivative of a rotated vector with respect to the quaternion,Derivative of a rotated vector with respect to the quaternion,,"Let us say we have a right-handed unit quaternion, describing the rotation from frame $a$ to frame $b$ : $q_a^b$ .  The rotation matrix formed from this quaternion is $R\left( q_a^b \right)$ and describes a passive rotation.  That is, $R\left( q_a^b \right)v$ describes the same object $v$ in the new frame $b$ . The following expression is given in Michael Andre Bloesh's dissertation without explanation link - (unfortunately embargoed until April 2018) $$\frac{d}{dq_a^b} R\left( q_a^b \right)v = -\left( R\left( q_a^b \right)v  \right)^\times $$ where the $\left( \cdot \right)^\times $ notation is the skew-symmetric matrix. I played with these expressions numerically to confirm the above and   also discovered that the derivative of the active rotation is $$\frac{d}{dq_a^b} R\left( q_a^b \right)^\top v = R \left( q_a^b \right)^\top \left( v \right)^\times $$ which I guess makes some intuitive sense as well. While these expressions seem to work, how do I approach this problem in a principled way (i.e. not guessing and checking with numerical differentiation)?","Let us say we have a right-handed unit quaternion, describing the rotation from frame to frame : .  The rotation matrix formed from this quaternion is and describes a passive rotation.  That is, describes the same object in the new frame . The following expression is given in Michael Andre Bloesh's dissertation without explanation link - (unfortunately embargoed until April 2018) where the notation is the skew-symmetric matrix. I played with these expressions numerically to confirm the above and   also discovered that the derivative of the active rotation is which I guess makes some intuitive sense as well. While these expressions seem to work, how do I approach this problem in a principled way (i.e. not guessing and checking with numerical differentiation)?",a b q_a^b R\left( q_a^b \right) R\left( q_a^b \right)v v b \frac{d}{dq_a^b} R\left( q_a^b \right)v = -\left( R\left( q_a^b \right)v  \right)^\times  \left( \cdot \right)^\times  \frac{d}{dq_a^b} R\left( q_a^b \right)^\top v = R \left( q_a^b \right)^\top \left( v \right)^\times ,"['derivatives', 'differential-geometry', 'rotations', 'quaternions']"
10,"If $f$ has a zero and $|f''|\leq M$, then $f$ is monotone on $(-h,h)$, where $h=\sqrt{2|f(0)|/3M}$","If  has a zero and , then  is monotone on , where","f |f''|\leq M f (-h,h) h=\sqrt{2|f(0)|/3M}","Let $f$ be twice differentiable on $\mathbb R$ and let $M$ be a bound   of $f''$, $|f''|\leq M$ on $\mathbb R$. Assume $f(0)\neq0$ and define   $h=\sqrt{\frac{2|f(0)|}{3M}}$. Prove that if $f$ has a zero in $(-h,h)$, it's   monotone in $(-h,h)$. So let's assume there's a zero $a\in (-h,h)$ such that $f(a)=0$ and also there's $b\in (-h,h)$ such that $f'(b)=0,\ f''(b)\neq0$. We need to get a contradiction. This question is under the Taylor Expansion chapter although I can't really get anything out of $f(x)=f(0)+f'(0)x+\frac{f''(\xi_x)}{2}x^2$ nor by the expansions near $a,b$. I get that in $(-h,h)$ we have $\frac{M}{2}x^2<\frac{|f(0)|}{3}$ which may be related to the 2nd derivative term though.","Let $f$ be twice differentiable on $\mathbb R$ and let $M$ be a bound   of $f''$, $|f''|\leq M$ on $\mathbb R$. Assume $f(0)\neq0$ and define   $h=\sqrt{\frac{2|f(0)|}{3M}}$. Prove that if $f$ has a zero in $(-h,h)$, it's   monotone in $(-h,h)$. So let's assume there's a zero $a\in (-h,h)$ such that $f(a)=0$ and also there's $b\in (-h,h)$ such that $f'(b)=0,\ f''(b)\neq0$. We need to get a contradiction. This question is under the Taylor Expansion chapter although I can't really get anything out of $f(x)=f(0)+f'(0)x+\frac{f''(\xi_x)}{2}x^2$ nor by the expansions near $a,b$. I get that in $(-h,h)$ we have $\frac{M}{2}x^2<\frac{|f(0)|}{3}$ which may be related to the 2nd derivative term though.",,"['real-analysis', 'derivatives', 'taylor-expansion']"
11,"If $|\,f'(p)|<1$, prove that $p$ is an attracting fixed point for $f$","If , prove that  is an attracting fixed point for","|\,f'(p)|<1 p f","Suppose that $f:(a,b) \to (a,b)$ has a fixed point $p$ in $(a, b)$ and that $f$ is differentiable at $p$. Furthermore, assume that $|\,f'(p)|<1$. Question: How do I prove that $p$ is an attracting fixed point for $f$. I know that $p$ is an attracting fixed point for $f$ if there exists a $0<\delta<1$ such that $|\,f(x) - p|<|x - p|$ whenever $|x - p|<\delta, x \neq p$. I've tried to think of what the relationship of the derivative of $f$ with the fixed point $p$ would be, but I'm kind of stuck.. Thanks in advance!","Suppose that $f:(a,b) \to (a,b)$ has a fixed point $p$ in $(a, b)$ and that $f$ is differentiable at $p$. Furthermore, assume that $|\,f'(p)|<1$. Question: How do I prove that $p$ is an attracting fixed point for $f$. I know that $p$ is an attracting fixed point for $f$ if there exists a $0<\delta<1$ such that $|\,f(x) - p|<|x - p|$ whenever $|x - p|<\delta, x \neq p$. I've tried to think of what the relationship of the derivative of $f$ with the fixed point $p$ would be, but I'm kind of stuck.. Thanks in advance!",,"['real-analysis', 'derivatives', 'convergence-divergence', 'fixed-point-theorems']"
12,Can an asymptote be a tangent line?,Can an asymptote be a tangent line?,,"Consider this function and its horizontal asymptote. Can the asymptote (in blue) also be considered a tangent line to the curve (in red)? The slope of the curve definitely approaches zero as $x$ approaches $\pm\infty$, but does that mean that a horizontal tangent line exists? It isn't possible to find a point of tangency, so I'm not sure if it counts.","Consider this function and its horizontal asymptote. Can the asymptote (in blue) also be considered a tangent line to the curve (in red)? The slope of the curve definitely approaches zero as $x$ approaches $\pm\infty$, but does that mean that a horizontal tangent line exists? It isn't possible to find a point of tangency, so I'm not sure if it counts.",,"['calculus', 'derivatives']"
13,Spivak Calculus Chapter 9 Question 9(ii),Spivak Calculus Chapter 9 Question 9(ii),,"I am working out of the 3rd edition. The question is to find $f'(x)$ and $f'(x+3)$ for $f(x+3)=x^5$ My working is the following: $$\begin{eqnarray} f(x)&=&f((x+3)-3)\\ &=&(x-3)^5\\ \implies f'(x)&=&5(x-3)^4\\ \end{eqnarray}$$ According to the answers section in the book, this is correct. Now: $$\begin{eqnarray} f'(x+3)&=&f'((x)+3)\\ &=&5((x+3)-3)^4\\ &=&5x^4\\ \end{eqnarray}$$ However, the answer in the back of the book for $f'(x+3)$ is $0$. I can't understand how the result $0$ was obtained at all. Is this a problem with the book, or a problem with my working? I can't find an up to date errata list.","I am working out of the 3rd edition. The question is to find $f'(x)$ and $f'(x+3)$ for $f(x+3)=x^5$ My working is the following: $$\begin{eqnarray} f(x)&=&f((x+3)-3)\\ &=&(x-3)^5\\ \implies f'(x)&=&5(x-3)^4\\ \end{eqnarray}$$ According to the answers section in the book, this is correct. Now: $$\begin{eqnarray} f'(x+3)&=&f'((x)+3)\\ &=&5((x+3)-3)^4\\ &=&5x^4\\ \end{eqnarray}$$ However, the answer in the back of the book for $f'(x+3)$ is $0$. I can't understand how the result $0$ was obtained at all. Is this a problem with the book, or a problem with my working? I can't find an up to date errata list.",,"['calculus', 'derivatives']"
14,Convex function with discontinuous derivative,Convex function with discontinuous derivative,,"I am interested in this question: Find a differentiable convex function such that its derivative is not continuous. I found out that we cannot find such function if its domain is $\mathbb{R}$, since every differentiable convex function $f\colon \mathbb{R} \to \mathbb{R}$ is continuously differentiable (as proved here ). Therefore we have to look for multivariable functions, but it is not an easy work. Thank you very much.","I am interested in this question: Find a differentiable convex function such that its derivative is not continuous. I found out that we cannot find such function if its domain is $\mathbb{R}$, since every differentiable convex function $f\colon \mathbb{R} \to \mathbb{R}$ is continuously differentiable (as proved here ). Therefore we have to look for multivariable functions, but it is not an easy work. Thank you very much.",,"['derivatives', 'continuity', 'convex-analysis']"
15,cusp vs. corner? or both?,cusp vs. corner? or both?,,"I searched through books and internet and they all have general definitions of them as follows: Cusp: where the slope of the tangent line changed from -infinity to +infinity (or the other way around) Corner: left-sided and right-sided derivatives are different. And I saw a problem which was asking if there is a corner or a cusp given a graph. The graph looked like: f(x)=-x, if x<0 =sqrt(x), if x>=0 So in short, one branch was straight, and another branch was curved. I know the point where x=0 is not differentiable. But would it be considered a corner or a cusp? In my opinion, it should be a corner because it does not change from -infinity to +infinity. However, while I was searching, I saw an example of graph that looks similar to that, and the website was calling it a cusp (sorry I cannot find the image anymore). Also, this is another question, but if a cusp have a slope of either -infinity or +infinity, wouldn't it be a subcategory of vertical tangent?","I searched through books and internet and they all have general definitions of them as follows: Cusp: where the slope of the tangent line changed from -infinity to +infinity (or the other way around) Corner: left-sided and right-sided derivatives are different. And I saw a problem which was asking if there is a corner or a cusp given a graph. The graph looked like: f(x)=-x, if x<0 =sqrt(x), if x>=0 So in short, one branch was straight, and another branch was curved. I know the point where x=0 is not differentiable. But would it be considered a corner or a cusp? In my opinion, it should be a corner because it does not change from -infinity to +infinity. However, while I was searching, I saw an example of graph that looks similar to that, and the website was calling it a cusp (sorry I cannot find the image anymore). Also, this is another question, but if a cusp have a slope of either -infinity or +infinity, wouldn't it be a subcategory of vertical tangent?",,"['calculus', 'derivatives']"
16,Incorrect ways to determine if a piecewise function is differentiable at a point?,Incorrect ways to determine if a piecewise function is differentiable at a point?,,"Given a piecewise function, let's say: $$f(x)= \begin{cases} x^2 \quad x < 0 \\ x^3 \quad x \ge 0 \end{cases}$$ Let's say we know the function is continuous and we only deal with real numbers. I have seen people check if it's differentiable at 0 in two different ways, but I'm not sure if all are correct. So help me out figuring out which are actually correct ways to do it. Method 1: Use the definition of the derivative, one for x>0 and one for x<0. Check if the the limits you get are the same. Method 2: Simply differentiate the function like this: $$f(x)= \begin{cases} 2x\quad x < 0 \\ 3x^2 \quad x > 0 \end{cases}$$ Then check if it's continuous in 0 by checking the limit from both sides. Which methods are correct? And are there other (analytic) methods?","Given a piecewise function, let's say: $$f(x)= \begin{cases} x^2 \quad x < 0 \\ x^3 \quad x \ge 0 \end{cases}$$ Let's say we know the function is continuous and we only deal with real numbers. I have seen people check if it's differentiable at 0 in two different ways, but I'm not sure if all are correct. So help me out figuring out which are actually correct ways to do it. Method 1: Use the definition of the derivative, one for x>0 and one for x<0. Check if the the limits you get are the same. Method 2: Simply differentiate the function like this: $$f(x)= \begin{cases} 2x\quad x < 0 \\ 3x^2 \quad x > 0 \end{cases}$$ Then check if it's continuous in 0 by checking the limit from both sides. Which methods are correct? And are there other (analytic) methods?",,"['derivatives', 'piecewise-continuity']"
17,Related rates question using similar triangles,Related rates question using similar triangles,,"So I have this related rates problem here: A man $6$ feet tall is standing still in a gymnasium which has a ceiling that is $30$ feet high. Ten feet in front of him a bright light starts to fall from the ceiling and the distance it falls in $t$ seconds is $16t^2$ feet. After one second, what is the rate of change in the length of his shadow created by the falling light? Can someone help me set this up? It's similar to the shadow questions I used to do but the guy is still this time, he isn't walking. I'm stuck on how to represent the falling light. I drew a diagram trying to represent the problem. However, I feel like I have too many variables to relate and I can't figure how to write them in terms of another.","So I have this related rates problem here: A man feet tall is standing still in a gymnasium which has a ceiling that is feet high. Ten feet in front of him a bright light starts to fall from the ceiling and the distance it falls in seconds is feet. After one second, what is the rate of change in the length of his shadow created by the falling light? Can someone help me set this up? It's similar to the shadow questions I used to do but the guy is still this time, he isn't walking. I'm stuck on how to represent the falling light. I drew a diagram trying to represent the problem. However, I feel like I have too many variables to relate and I can't figure how to write them in terms of another.",6 30 t 16t^2,"['calculus', 'derivatives']"
18,Prove the Inequality $\frac{1}{1-x}-\frac{x(3-x)(2-x)(13x^4-50x^3+89x^2-84x+36)}{4(1-x)(2x(1-x))^2}<1$,Prove the Inequality,\frac{1}{1-x}-\frac{x(3-x)(2-x)(13x^4-50x^3+89x^2-84x+36)}{4(1-x)(2x(1-x))^2}<1,"Can anyone suggest any hints to prove the following inequality: $$\frac{1}{1-x} - \frac{x(3-x)(2-x)(13x^4 - 50x^3 + 89x^2 - 84x + 36)}{4(1-x)(2x(1-x))^2} < 1,$$ for all $x \in (0,1)$?","Can anyone suggest any hints to prove the following inequality: $$\frac{1}{1-x} - \frac{x(3-x)(2-x)(13x^4 - 50x^3 + 89x^2 - 84x + 36)}{4(1-x)(2x(1-x))^2} < 1,$$ for all $x \in (0,1)$?",,"['calculus', 'real-analysis', 'derivatives', 'inequality']"
19,Pork roast defrost using calculus,Pork roast defrost using calculus,,"I am really stuck on this problem for calculus and I could use some help A pork roast is removed from the freezer and left on the counter to defrost. The temperature of the pork roast was $−4^\circ C$ when it was removed from the freezer, and $t$ hours later was increasing at a rate of $$T′(t)=8e^{−0.3t} \quad^\circ C\text{/hour}.$$ Assume the pork roast is defrosted when its temperature reaches $11^\circ C$.  How long does it take for the pork roast to defrost? (Estimate answer rounded off to 3 decimal places.) I found the integral to be $$\frac{-80}{3} e^{-3(t/10)}+C$$ but I don't know where to go from here please help me if you can","I am really stuck on this problem for calculus and I could use some help A pork roast is removed from the freezer and left on the counter to defrost. The temperature of the pork roast was $−4^\circ C$ when it was removed from the freezer, and $t$ hours later was increasing at a rate of $$T′(t)=8e^{−0.3t} \quad^\circ C\text{/hour}.$$ Assume the pork roast is defrosted when its temperature reaches $11^\circ C$.  How long does it take for the pork roast to defrost? (Estimate answer rounded off to 3 decimal places.) I found the integral to be $$\frac{-80}{3} e^{-3(t/10)}+C$$ but I don't know where to go from here please help me if you can",,"['calculus', 'derivatives']"
20,Taking the derivative of a set function,Taking the derivative of a set function,,"Lets say $f(A)$ is a set function $A \to \mathbb{R}$ for a collection of sets $\mathcal{A}$. Is is possible to differentiate such a function, and if so, what theory is used to give this notion a rigorous foundation? Or maybe, is it only possible if $\mathcal{A}$ is a $\sigma-$field with measure $|\mu(A)|<\infty\;\forall A\in \mathcal{A}$. Here are my attempts and thoughts about this Let $\mathcal{A}$ be the collection of sets upon which $f(A)$ is defined. To make this easier, assume that $\mathcal{A}$ is a $\sigma$-Field. If I define a decreasing sequence of sets $A_i \in \mathcal{A}: A_1 \supset A_2 \supset A_3 ... \supset A$ so that $\lim\limits_{n \to \infty} A_i = A$, then I have something like the univariate $x\to \infty$. However, the problem is that $A \in \mathcal{A}$ need not define a metric space, so I can't just assign an absolute distance to each set in the decreasing sequence, right? If I could, that would essentially just translate this to univariate calculus. I think if I define a measure over the field $\mathcal{A}$ I could also make this easier, but that would be the Radon-Nikodym derivative (unless that is the derivative of a set function). I thought maybe the following could work too. Let's define distance between two sets $A,B$ in the domain of $f(\cdot)$ as: $$d(A,B) := \left|\sup_{x \in A, \;y \in B\cap A^c } |f(x)-f(y)| - \sup_{x \in B, \;y \in A\cap B^c } |f(x)-f(y)|\right|$$ With this, I thought we could define a derivative from this as: $$f'(A) := \lim_{n\to \infty} \frac{f(A_{i+1})-f(A_{i})}{d(A_{i+1},A_{i})}$$","Lets say $f(A)$ is a set function $A \to \mathbb{R}$ for a collection of sets $\mathcal{A}$. Is is possible to differentiate such a function, and if so, what theory is used to give this notion a rigorous foundation? Or maybe, is it only possible if $\mathcal{A}$ is a $\sigma-$field with measure $|\mu(A)|<\infty\;\forall A\in \mathcal{A}$. Here are my attempts and thoughts about this Let $\mathcal{A}$ be the collection of sets upon which $f(A)$ is defined. To make this easier, assume that $\mathcal{A}$ is a $\sigma$-Field. If I define a decreasing sequence of sets $A_i \in \mathcal{A}: A_1 \supset A_2 \supset A_3 ... \supset A$ so that $\lim\limits_{n \to \infty} A_i = A$, then I have something like the univariate $x\to \infty$. However, the problem is that $A \in \mathcal{A}$ need not define a metric space, so I can't just assign an absolute distance to each set in the decreasing sequence, right? If I could, that would essentially just translate this to univariate calculus. I think if I define a measure over the field $\mathcal{A}$ I could also make this easier, but that would be the Radon-Nikodym derivative (unless that is the derivative of a set function). I thought maybe the following could work too. Let's define distance between two sets $A,B$ in the domain of $f(\cdot)$ as: $$d(A,B) := \left|\sup_{x \in A, \;y \in B\cap A^c } |f(x)-f(y)| - \sup_{x \in B, \;y \in A\cap B^c } |f(x)-f(y)|\right|$$ With this, I thought we could define a derivative from this as: $$f'(A) := \lim_{n\to \infty} \frac{f(A_{i+1})-f(A_{i})}{d(A_{i+1},A_{i})}$$",,['calculus']
21,Inequality between the norm of derivative and the derivative of norm,Inequality between the norm of derivative and the derivative of norm,,"Let $x(t)=[x_1(t)~x_2(t)~\cdots ~x_n(t)]^T$, function $x_i:R\rightarrow R$ is differentiable, then it can be drawn that when $p=2$, $\|\frac{d}{dt}x(t)\|_p\geq \frac{d}{dt}\|x(t)\|_p$. I wonder if this inequality holds when $p\neq 2$, and if it does, show why.","Let $x(t)=[x_1(t)~x_2(t)~\cdots ~x_n(t)]^T$, function $x_i:R\rightarrow R$ is differentiable, then it can be drawn that when $p=2$, $\|\frac{d}{dt}x(t)\|_p\geq \frac{d}{dt}\|x(t)\|_p$. I wonder if this inequality holds when $p\neq 2$, and if it does, show why.",,"['calculus', 'real-analysis', 'derivatives', 'normed-spaces']"
22,Proving differentiability,Proving differentiability,,"I just had a question on proving differentiability by showing that the difference quotient exists. I understand in the case of a function like $f(x)=x^2$, where you end up with $((x+h)^2 - x^2)/h = 2x + h = 2x$ as h goes to infinity, but in the case of a function such as $1/x^n$, how do you address the ""n"" portion since you cannot expand and divide out the $h$ in the denominator?","I just had a question on proving differentiability by showing that the difference quotient exists. I understand in the case of a function like $f(x)=x^2$, where you end up with $((x+h)^2 - x^2)/h = 2x + h = 2x$ as h goes to infinity, but in the case of a function such as $1/x^n$, how do you address the ""n"" portion since you cannot expand and divide out the $h$ in the denominator?",,"['calculus', 'real-analysis', 'derivatives']"
23,Why do we write $df/dx$ instead of $df/dx(x)$?,Why do we write  instead of ?,df/dx df/dx(x),"I was just thinking about how, i.e., if $f\colon\mathbb R\to\mathbb R$ is defined by $f(x) = x^2$, then it's customary to write $$ \frac{df}{dx} = 2x. $$ But since the derivative is itself a function from $\mathbb R$ to $\mathbb R$, shouldn't we write $$ \frac{df}{dx}(x) = 2x? $$ After all, we would typically write $f'(x) = 2x$, not just $f' = 2x$, right?","I was just thinking about how, i.e., if $f\colon\mathbb R\to\mathbb R$ is defined by $f(x) = x^2$, then it's customary to write $$ \frac{df}{dx} = 2x. $$ But since the derivative is itself a function from $\mathbb R$ to $\mathbb R$, shouldn't we write $$ \frac{df}{dx}(x) = 2x? $$ After all, we would typically write $f'(x) = 2x$, not just $f' = 2x$, right?",,"['derivatives', 'notation']"
24,Directional derivative,Directional derivative,,"The governor Ralph has trouble on the bright side of Mercury. The temperature in the wall of the vessel, when it is in the position $(x, y, z)$ is given by $T(x, y, z)=e^{-x^2-2y^2-3z^2}$, where $x$, $y$ and $z$ are measured in meters. Now it is located at $(1,1,1)$. In what direction should it move to lower the temperature as quickly as possible? If the vessel is traveling with velocity $e^8$ per second, how quickly will temperature be decreased if it moves in that direction? Unfortunately the metal of the wall will break if it cooled with rate greater than $\sqrt{14}e^2$ degrees per second. Describe the set of possible directions where it can move to reduce the temperature at a rate not greater than the permissible. $$$$ I have done the following: The direction to that the vessel should move so that the temperature decreases as fast as possible is given by : $$-\nabla T(1, 1, 1)$$ Is this correct?? So, we have the following: $$\nabla T(x, y, z)=(-2x e^{-x^2-2y^2-3z^2}, -4ye^{-x^2-2y^2-3z^2}, -6ze^{-x^2-2y^2-3z^2}) \\ \Rightarrow -\nabla T(1, 1, 1)=(2e^{-6}, 4e^{-6}, 6e^{-6})$$ Is this correct?? Are we looking for the rate of change at the direction we found at the question $1$ ?? So, is it as followed?? $$grad f \cdot \overrightarrow{v}$$ where $\overline{v}$ is the unit vector of the qquestion $1$ ?? But how can we use the fact that the velocity is $e^8$ ?? Could you give me some hints what we could do ?? $$$$ EDIT: At the question $3$ are we looking for the directions $\overrightarrow{v}$ such that $$\nabla T \cdot \overrightarrow{v}\leq \sqrt{14}e^2$$ ??","The governor Ralph has trouble on the bright side of Mercury. The temperature in the wall of the vessel, when it is in the position $(x, y, z)$ is given by $T(x, y, z)=e^{-x^2-2y^2-3z^2}$, where $x$, $y$ and $z$ are measured in meters. Now it is located at $(1,1,1)$. In what direction should it move to lower the temperature as quickly as possible? If the vessel is traveling with velocity $e^8$ per second, how quickly will temperature be decreased if it moves in that direction? Unfortunately the metal of the wall will break if it cooled with rate greater than $\sqrt{14}e^2$ degrees per second. Describe the set of possible directions where it can move to reduce the temperature at a rate not greater than the permissible. $$$$ I have done the following: The direction to that the vessel should move so that the temperature decreases as fast as possible is given by : $$-\nabla T(1, 1, 1)$$ Is this correct?? So, we have the following: $$\nabla T(x, y, z)=(-2x e^{-x^2-2y^2-3z^2}, -4ye^{-x^2-2y^2-3z^2}, -6ze^{-x^2-2y^2-3z^2}) \\ \Rightarrow -\nabla T(1, 1, 1)=(2e^{-6}, 4e^{-6}, 6e^{-6})$$ Is this correct?? Are we looking for the rate of change at the direction we found at the question $1$ ?? So, is it as followed?? $$grad f \cdot \overrightarrow{v}$$ where $\overline{v}$ is the unit vector of the qquestion $1$ ?? But how can we use the fact that the velocity is $e^8$ ?? Could you give me some hints what we could do ?? $$$$ EDIT: At the question $3$ are we looking for the directions $\overrightarrow{v}$ such that $$\nabla T \cdot \overrightarrow{v}\leq \sqrt{14}e^2$$ ??",,"['calculus', 'derivatives', 'gradient-flows']"
25,Antiderivative of periodic function,Antiderivative of periodic function,,"I practise before the real analysis exam and I got stuck on few excercises. ""if $F$ is an antiderivative of a continuous periodic function and $F$ is bounded, then $F$ is periodic"" $~\to~$ one should give a proof or counterexample. (I'm convinced it's true, but how to prove it?)","I practise before the real analysis exam and I got stuck on few excercises. ""if is an antiderivative of a continuous periodic function and is bounded, then is periodic"" one should give a proof or counterexample. (I'm convinced it's true, but how to prove it?)",F F F ~\to~,"['real-analysis', 'derivatives']"
26,Rudin's chain rule: Why is continuity at interval necessary?,Rudin's chain rule: Why is continuity at interval necessary?,,"Theorem 5.5, Rudin's Principles of Mathematical analysis says: Suppose $f$ is continuous on $\color{red}{[a,b]}$ ,$ f'(x)$ exists at some point $x\in [a,b], g$ is defined on an interval $I$ which contains the range of $f$, and $g$ is differentiable at the  point $f(x)$. If $$h(t)=g(f(t)) \quad (a\le t \le b)$$ then $h$ is differentiable at $x$, and $$h'(x)=g'(f(x))f'(x)$$ I believe, I have understood the proof. But why is continuity at $[a,b]$ required? To me differentiablity at $x$ of $f$ and the same at $f(x)$ of $g$ is the only required conditions.","Theorem 5.5, Rudin's Principles of Mathematical analysis says: Suppose $f$ is continuous on $\color{red}{[a,b]}$ ,$ f'(x)$ exists at some point $x\in [a,b], g$ is defined on an interval $I$ which contains the range of $f$, and $g$ is differentiable at the  point $f(x)$. If $$h(t)=g(f(t)) \quad (a\le t \le b)$$ then $h$ is differentiable at $x$, and $$h'(x)=g'(f(x))f'(x)$$ I believe, I have understood the proof. But why is continuity at $[a,b]$ required? To me differentiablity at $x$ of $f$ and the same at $f(x)$ of $g$ is the only required conditions.",,"['real-analysis', 'derivatives']"
27,Why are there so many notations for differentiation?,Why are there so many notations for differentiation?,,"There are so many notations for differentiation. Some of them are: $$ f^\prime(x) \qquad \frac{d}{dx}(f(x))\qquad \frac{dy}{dx}\qquad \frac{df}{dx}\qquad D f(x)\qquad y^\prime\qquad D_x f(x) $$ Why are there so many ways to say ""the derivative of $f(x)$""? Is there a specific use for each notation? What is the difference between $\dfrac{d}{dx}$ and $\dfrac{dy}{dx}$? I am only asking this because I am worried that I might use the wrong notation sometimes. For example, I don't know when I should use $\dfrac{dy}{dx}$ instead of $D_xf(x)$, or vice versa. I thank you in advance for your answers.","There are so many notations for differentiation. Some of them are: $$ f^\prime(x) \qquad \frac{d}{dx}(f(x))\qquad \frac{dy}{dx}\qquad \frac{df}{dx}\qquad D f(x)\qquad y^\prime\qquad D_x f(x) $$ Why are there so many ways to say ""the derivative of $f(x)$""? Is there a specific use for each notation? What is the difference between $\dfrac{d}{dx}$ and $\dfrac{dy}{dx}$? I am only asking this because I am worried that I might use the wrong notation sometimes. For example, I don't know when I should use $\dfrac{dy}{dx}$ instead of $D_xf(x)$, or vice versa. I thank you in advance for your answers.",,"['calculus', 'derivatives', 'notation']"
28,Fundamental theorem of calculus with functions of $x$,Fundamental theorem of calculus with functions of,x,I have these two homework questions.Using fundamental theorem of calculus and basic (A-level) facts about intergration find $F'(x)$ when: $$F(x)=\int_{a(x)}^{b(x)} f(t) dt$$ I solved this by appling the chain rule and splitting the intergral (this might have a different name) $$F(x)=\int_{a(x)}^{b(x)} f(t) dt=F(x)=\int_{a(x)}^{c} f(t)dt+\int_c^{b(x)} f(t) dt =-\int_{c}^{a(x)} f(t)dt+\int_c^{b(x)} f(t) dt$$ So $$F'(x)=f(b(x))b'(x)-f(a(x))a'(x)$$ by the chain rule. Then question 2: $$F(x)=\left({\int_{a(x)}^{b(x)} f(t) dt}\right)^2$$ Am I correct in just saying $$F'(x)=\sqrt{f(b(x))b'(x)-f(a(x))a'(x)}$$ or is some more complex maths required? So my questions are: Is my answer for question one correct (and if not how would I go about fixing it)? Is my answer for question 2 correct (and if not how would I go about fixing it)? I know that homework questions are quite controversial but I have clearly pointed out this IS a homework question AND I have made an attempt at both of the questions. Any help would be greatly appreciated.,I have these two homework questions.Using fundamental theorem of calculus and basic (A-level) facts about intergration find $F'(x)$ when: $$F(x)=\int_{a(x)}^{b(x)} f(t) dt$$ I solved this by appling the chain rule and splitting the intergral (this might have a different name) $$F(x)=\int_{a(x)}^{b(x)} f(t) dt=F(x)=\int_{a(x)}^{c} f(t)dt+\int_c^{b(x)} f(t) dt =-\int_{c}^{a(x)} f(t)dt+\int_c^{b(x)} f(t) dt$$ So $$F'(x)=f(b(x))b'(x)-f(a(x))a'(x)$$ by the chain rule. Then question 2: $$F(x)=\left({\int_{a(x)}^{b(x)} f(t) dt}\right)^2$$ Am I correct in just saying $$F'(x)=\sqrt{f(b(x))b'(x)-f(a(x))a'(x)}$$ or is some more complex maths required? So my questions are: Is my answer for question one correct (and if not how would I go about fixing it)? Is my answer for question 2 correct (and if not how would I go about fixing it)? I know that homework questions are quite controversial but I have clearly pointed out this IS a homework question AND I have made an attempt at both of the questions. Any help would be greatly appreciated.,,"['calculus', 'derivatives', 'solution-verification']"
29,How do you determine the points of inflection for $f(x) = \frac{e^x}{1+e^x}$?,How do you determine the points of inflection for ?,f(x) = \frac{e^x}{1+e^x},"$$f(x) =\dfrac{e^x}{1+e^x}$$ I know we can find points of inflection using the second derivative test. The second derivative for the function above is $$f''(x) = \dfrac{e^x(1-e^x)}{(e^x+1)^3}$$ I have found one critical point for the second derivative which is $0$. I then determined that the function is concave up from $(-\infty,0)$ and concave down from $(0,\infty)$. I am now asked to find the points of inflection. How would I determine the exact points from where the function switches from concave up to concave down?","$$f(x) =\dfrac{e^x}{1+e^x}$$ I know we can find points of inflection using the second derivative test. The second derivative for the function above is $$f''(x) = \dfrac{e^x(1-e^x)}{(e^x+1)^3}$$ I have found one critical point for the second derivative which is $0$. I then determined that the function is concave up from $(-\infty,0)$ and concave down from $(0,\infty)$. I am now asked to find the points of inflection. How would I determine the exact points from where the function switches from concave up to concave down?",,"['calculus', 'derivatives']"
30,Find the order of the error for the approximation $f' '(x)$,Find the order of the error for the approximation,f' '(x),"Given $$f''(x) = \frac{ f(x+h) - 2f(x) + f(x-h)}{h^2}.$$ I realize that this is just an approximation - that it won't give the exact value of $f''(x)$ and therefore there is an error term. However, I have no clue how to go about this question. Any help would be much appreciated!","Given $$f''(x) = \frac{ f(x+h) - 2f(x) + f(x-h)}{h^2}.$$ I realize that this is just an approximation - that it won't give the exact value of $f''(x)$ and therefore there is an error term. However, I have no clue how to go about this question. Any help would be much appreciated!",,"['derivatives', 'approximation']"
31,Differentiate $y=x^x$,Differentiate,y=x^x,"How do you differentiate $$\large{f(x) = x^x}$$ The working I got was $$\ln f(x) = x \ln  x$$ which I am pretty fine...but I do not know how it advances to $$\frac{f'(x)}{f(x)} = x\begin{pmatrix} \frac 1 x\end{pmatrix} + \ln x$$ although the final answer can be, by multiplying $f(x)$ on both sides of the equation, $${f'(x)} = x^x\begin{bmatrix}x\begin{pmatrix} \frac 1 x\end{pmatrix} + \ln x\end{bmatrix}$$ UPDATE : SOLVED Indeed, $$\ln f(x) = x \ln  x$$ Differentiate both sides of the equation w.r.t $x$ $$\frac{f'(x)}{f(x)} = x\begin{pmatrix} \frac 1 x\end{pmatrix} + \ln x = 1 + \ln x$$ Bring the $f(x)$ over and you'll finally get $$f'(x) = x^x\begin {pmatrix} 1 + \ln x\end{pmatrix}$$","How do you differentiate The working I got was which I am pretty fine...but I do not know how it advances to although the final answer can be, by multiplying on both sides of the equation, UPDATE : SOLVED Indeed, Differentiate both sides of the equation w.r.t Bring the over and you'll finally get",\large{f(x) = x^x} \ln f(x) = x \ln  x \frac{f'(x)}{f(x)} = x\begin{pmatrix} \frac 1 x\end{pmatrix} + \ln x f(x) {f'(x)} = x^x\begin{bmatrix}x\begin{pmatrix} \frac 1 x\end{pmatrix} + \ln x\end{bmatrix} \ln f(x) = x \ln  x x \frac{f'(x)}{f(x)} = x\begin{pmatrix} \frac 1 x\end{pmatrix} + \ln x = 1 + \ln x f(x) f'(x) = x^x\begin {pmatrix} 1 + \ln x\end{pmatrix},"['calculus', 'derivatives', 'exponentiation']"
32,Calculate the first derivative without the function?,Calculate the first derivative without the function?,,"I am a complete newbie when it comes to advanced mathematics, and am trying to learn calculus on my own. I wanted to know - is it possible to calculate the first derivative if you don't know the function that created a curve, but you DO have all of the points along the curve? Edit: I created the curve using a cubic Spline interpolation If so, can you point me to a place where I can learn how this would be accomplished? Thanks!!","I am a complete newbie when it comes to advanced mathematics, and am trying to learn calculus on my own. I wanted to know - is it possible to calculate the first derivative if you don't know the function that created a curve, but you DO have all of the points along the curve? Edit: I created the curve using a cubic Spline interpolation If so, can you point me to a place where I can learn how this would be accomplished? Thanks!!",,"['calculus', 'derivatives']"
33,"Suppose $f:[0,\infty) \to \mathbb R$ is continuous and differentiable with $f(0)=0$ and $f'$ is increasing. Show that $g(x)$ is increasing.",Suppose  is continuous and differentiable with  and  is increasing. Show that  is increasing.,"f:[0,\infty) \to \mathbb R f(0)=0 f' g(x)","First Question: Show that $\displaystyle \frac{f(x)}{x} < f'(x)$ for $\forall 0<x$ My attempt: Clearly, $f$ is satifying conditions of mean value theorem. So, apply MVT to $f$ on $[0,x]$ . We get $$\frac {f(x)-f(0)}{x-0} =\frac {f(x)}{x} = f'(c)$$ for some $c \in (0,x)$ . And we know that $f'$ is increasing so for some $d$ such that $c<d$ we have $f'(c)<f'(d)$ . Hence we obtain $$\frac {f(x)}{x} < f'(d) $$ My FIRST question is how can I move into $f'(x)$ instead of having $f'(d)$ . Second Question: Suppose $f:[0,\infty) \to \mathbb R$ is continuous and differentiable  with $f(0)=0$ and derivative of $f$ is increasing. Show that $g(x)$ is increasing. $$ g(x):=\begin{cases}  \frac {f(x)}{x}& \text{ if } x>0\\   f'(0)& \text{ if } x= 0 \end{cases}$$ First I am really sorry that I couldn't write $g(x)$ as piecewise function. My attemp for second question: I need to show that if $x_1 < x_2 $ then $g(x_1)<g(x_2)$ . If $x_1=0$ then obviously $g(0)=f'(0)$ and since $0<x_2$ then $f'(0) < f'(x_2)$ so it is increasing. Now, I know that I have to show $\displaystyle \frac {f(x_1)}{x_1} <\frac {f(x_2)}{x_2}$ Pick $x_1 > 0$ then $g(x_1)=\displaystyle \frac {f(x_1)}{x_1} < f'(x_1) $ That is where I stuck. Could you help?","First Question: Show that for My attempt: Clearly, is satifying conditions of mean value theorem. So, apply MVT to on . We get for some . And we know that is increasing so for some such that we have . Hence we obtain My FIRST question is how can I move into instead of having . Second Question: Suppose is continuous and differentiable  with and derivative of is increasing. Show that is increasing. First I am really sorry that I couldn't write as piecewise function. My attemp for second question: I need to show that if then . If then obviously and since then so it is increasing. Now, I know that I have to show Pick then That is where I stuck. Could you help?","\displaystyle \frac{f(x)}{x} < f'(x) \forall 0<x f f [0,x] \frac {f(x)-f(0)}{x-0} =\frac {f(x)}{x} = f'(c) c \in (0,x) f' d c<d f'(c)<f'(d) \frac {f(x)}{x} < f'(d)  f'(x) f'(d) f:[0,\infty) \to \mathbb R f(0)=0 f g(x) 
g(x):=\begin{cases}
 \frac {f(x)}{x}& \text{ if } x>0\\ 
 f'(0)& \text{ if } x= 0
\end{cases} g(x) x_1 < x_2  g(x_1)<g(x_2) x_1=0 g(0)=f'(0) 0<x_2 f'(0) < f'(x_2) \displaystyle \frac {f(x_1)}{x_1} <\frac {f(x_2)}{x_2} x_1 > 0 g(x_1)=\displaystyle \frac {f(x_1)}{x_1} < f'(x_1) ","['calculus', 'derivatives']"
34,how is this differentiation possible?,how is this differentiation possible?,,A particle moves in the plane xy with constant acceleration a directed along the negative y–axis. The equation of motion of the particle has the form (where p and q are constants) $$y = px –{qx}^2$$ in my book they differentiated both sides with respect to time and solved it as $$\frac {dy}{dt}=p\frac{dx}{dt}+q\cdot 2x \cdot\frac{dx}{dt}$$ they later took $\frac{dx}{dt}$ as velocity in x-axis and solved it.... my question is that we dont know the relation of x with t.so how could we do $\frac{d}{dt}{qx}^2$ and get $q\cdot 2x\cdot\frac{dx}{dt}$ after differentiating? i have tried asking the question before and my ques got closed as i didnt know mathjax.so plz dont downvote and tell me what i can do to better edit the question.,A particle moves in the plane xy with constant acceleration a directed along the negative y–axis. The equation of motion of the particle has the form (where p and q are constants) in my book they differentiated both sides with respect to time and solved it as they later took as velocity in x-axis and solved it.... my question is that we dont know the relation of x with t.so how could we do and get after differentiating? i have tried asking the question before and my ques got closed as i didnt know mathjax.so plz dont downvote and tell me what i can do to better edit the question.,y = px –{qx}^2 \frac {dy}{dt}=p\frac{dx}{dt}+q\cdot 2x \cdot\frac{dx}{dt} \frac{dx}{dt} \frac{d}{dt}{qx}^2 q\cdot 2x\cdot\frac{dx}{dt},"['calculus', 'derivatives']"
35,Prove that $f(x)\leq 0$ for all $x\in [0;1]$,Prove that  for all,f(x)\leq 0 x\in [0;1],Let $f:[0;1]\to \mathbb{R}$ be a twice differentiable function on $(0;1)$ such that $f(0)=f(1)=0$ and $f''(x)+2f'(x)+f(x)\geq 0$ for all $x\in (a;b)$ .  Prove that $f(x)\leq 0$ for all $x\in [0;1]$ I was thinking about considering other function: $g(x)=f(x)e^{x}$ . Then we have $$g(0)=g(1)=0; g'(x)=[f(x)+f'(x)]e^x; g''(x)=[f(x)+2f'(x)+f''(x)]e^x$$ It seems relate to the problem's condition. But I don't know how to continue then. I need some help to clarify this problem. Thanks so much!,Let be a twice differentiable function on such that and for all .  Prove that for all I was thinking about considering other function: . Then we have It seems relate to the problem's condition. But I don't know how to continue then. I need some help to clarify this problem. Thanks so much!,f:[0;1]\to \mathbb{R} (0;1) f(0)=f(1)=0 f''(x)+2f'(x)+f(x)\geq 0 x\in (a;b) f(x)\leq 0 x\in [0;1] g(x)=f(x)e^{x} g(0)=g(1)=0; g'(x)=[f(x)+f'(x)]e^x; g''(x)=[f(x)+2f'(x)+f''(x)]e^x,"['calculus', 'derivatives', 'continuity']"
36,"Let $a,b,c\in\mathbb{Z}$, $1<a<10$, $c$ is a prime number and $f(x)=ax^2+bx+c$. If $f(f(1))=f(f(2))=f(f(3))$, find $f'(f(1))+f(f'(2))+f'(f(3))$","Let , ,  is a prime number and . If , find","a,b,c\in\mathbb{Z} 1<a<10 c f(x)=ax^2+bx+c f(f(1))=f(f(2))=f(f(3)) f'(f(1))+f(f'(2))+f'(f(3))","Let $a,b,c\in\mathbb{Z}$ , $1<a<10$ , $c$ is a prime number and $f(x)=ax^2+bx+c$ . If $f(f(1))=f(f(2))=f(f(3))$ , find $f'(f(1))+f(f'(2))+f'(f(3))$ My attempt: \begin{align*} f'(x)&=2ax+b\\ (f(f(x)))'&=f'(f(x))f'(x)\\ f'(f(x))&=\frac{(f(f(x)))'}{f'(x)}\\ \end{align*}","Let , , is a prime number and . If , find My attempt:","a,b,c\in\mathbb{Z} 1<a<10 c f(x)=ax^2+bx+c f(f(1))=f(f(2))=f(f(3)) f'(f(1))+f(f'(2))+f'(f(3)) \begin{align*}
f'(x)&=2ax+b\\
(f(f(x)))'&=f'(f(x))f'(x)\\
f'(f(x))&=\frac{(f(f(x)))'}{f'(x)}\\
\end{align*}","['calculus', 'derivatives', 'polynomials', 'quadratics']"
37,Refinements of the inequality $f(x)=x^{2(1-x)}+(1-x)^{2x}\leq 1$ for $0<x<0.5$,Refinements of the inequality  for,f(x)=x^{2(1-x)}+(1-x)^{2x}\leq 1 0<x<0.5,"Hi it's related to Showing the inequality $f(x)=x^{2(1-x)}+(1-x)^{2x}\leq 1$ for $0<x<1$ We want to show 1 : Let $0<x<0.5$ such that  then we have : $$f(x)=x^{2(1-x)}+(1-x)^{2x}\leq q(x)=(1-x)^{2x}+2^{2x+1}(1-x)x^2\leq 1$$ The Lhs is equivalent to : $$x^{2(1-x)}\leq h(x)=2^{2x+1}(1-x)x^2$$ Or : $$\ln\Big(x^{2(1-x)}\Big)\leq \ln\Big(2^{2x+1}(1-x)x^2\Big)$$ Making the difference of these logarithm and introducing the function : $$g(x)=\ln\Big(x^{2(1-x)}\Big)-\ln\Big(2^{2x+1}(1-x)x^2\Big)$$ The derivative is not hard to manipulate and we see that it's positive and $x=0.5$ is an extrema .The conclusion is : $$g(x)\leq g(0.5)=0$$ And we are done with the LHS. For the Rhs I use one of the lemma (7.1) due to Vasile Cirtoaje we have : $$(1-x)^{2x}\leq p(x)=1-4(1-x)x^{2}-2(1-x)x(1-2 x)\ln(1-x)$$ So we have : $$q(x)\leq p(x)+h(x)$$ We want to show that : $$p(x)+h(x)\leq 1$$ Wich is equivalent to : $$-2(x-1)x((4^x-2)x+(2x-1)\ln(1-x))\leq 0$$ It's not hard so I omitt here the proof of this fact . We are done . Question : Is it right ? Thanks in advance . Regards Max. 1 Vasile Cirtoaje, ""Proofs of three open inequalities with power-exponential functions"", The Journal of Nonlinear Sciences and its Applications (2011), Volume: 4, Issue: 2, page 130-137. https://eudml.org/doc/223938","Hi it's related to Showing the inequality $f(x)=x^{2(1-x)}+(1-x)^{2x}\leq 1$ for $0<x<1$ We want to show 1 : Let such that  then we have : The Lhs is equivalent to : Or : Making the difference of these logarithm and introducing the function : The derivative is not hard to manipulate and we see that it's positive and is an extrema .The conclusion is : And we are done with the LHS. For the Rhs I use one of the lemma (7.1) due to Vasile Cirtoaje we have : So we have : We want to show that : Wich is equivalent to : It's not hard so I omitt here the proof of this fact . We are done . Question : Is it right ? Thanks in advance . Regards Max. 1 Vasile Cirtoaje, ""Proofs of three open inequalities with power-exponential functions"", The Journal of Nonlinear Sciences and its Applications (2011), Volume: 4, Issue: 2, page 130-137. https://eudml.org/doc/223938",0<x<0.5 f(x)=x^{2(1-x)}+(1-x)^{2x}\leq q(x)=(1-x)^{2x}+2^{2x+1}(1-x)x^2\leq 1 x^{2(1-x)}\leq h(x)=2^{2x+1}(1-x)x^2 \ln\Big(x^{2(1-x)}\Big)\leq \ln\Big(2^{2x+1}(1-x)x^2\Big) g(x)=\ln\Big(x^{2(1-x)}\Big)-\ln\Big(2^{2x+1}(1-x)x^2\Big) x=0.5 g(x)\leq g(0.5)=0 (1-x)^{2x}\leq p(x)=1-4(1-x)x^{2}-2(1-x)x(1-2 x)\ln(1-x) q(x)\leq p(x)+h(x) p(x)+h(x)\leq 1 -2(x-1)x((4^x-2)x+(2x-1)\ln(1-x))\leq 0,"['derivatives', 'inequality', 'solution-verification', 'logarithms', 'exponentiation']"
38,Is a continuous function with zero “Taylor approximation” smooth?,Is a continuous function with zero “Taylor approximation” smooth?,,"Let $f:[0,\infty) \to [0,\infty)$ be a continuous, non-decreasing function, satisfying $f(0)=0$ and $f(x)=o(x^n)$ for any $n \ge 1$ . Suppose also that $f(x)>0$ for every $x>0$ . I also know that $f$ is smooth on the open interval $(0,\infty)$ . Must $f$ be infinitely (right) differentiable at zero? Comments: The Taylor series of $f$ does not have to converge to $f$ , e.g. in the famous example of $$ f(x) =\begin{cases}e^{-1/x^2} \text{ for } x >0 \\ 0 \text{    for } x = 0\end{cases} $$ In general, the existence of a polynomial approximation by itself does not imply differentiabiliy; in fact it does not even imply continuity (at $x>0$ ), as the following examples show: $f(x)=\chi_{\mathbb Q}(x)x^n$ or even $\chi_{\mathbb Q}(x)e^{-1/x^2}$ which also satisfies $f(x)=o(x^n)$ for any $n \ge 1$ .","Let be a continuous, non-decreasing function, satisfying and for any . Suppose also that for every . I also know that is smooth on the open interval . Must be infinitely (right) differentiable at zero? Comments: The Taylor series of does not have to converge to , e.g. in the famous example of In general, the existence of a polynomial approximation by itself does not imply differentiabiliy; in fact it does not even imply continuity (at ), as the following examples show: or even which also satisfies for any .","f:[0,\infty) \to [0,\infty) f(0)=0 f(x)=o(x^n) n \ge 1 f(x)>0 x>0 f (0,\infty) f f f 
f(x) =\begin{cases}e^{-1/x^2} \text{ for } x >0 \\
0 \text{    for } x = 0\end{cases}
 x>0 f(x)=\chi_{\mathbb Q}(x)x^n \chi_{\mathbb Q}(x)e^{-1/x^2} f(x)=o(x^n) n \ge 1","['real-analysis', 'calculus', 'derivatives', 'singularity']"
39,"Given any 2 curves, can we prove the existence of a common normal?","Given any 2 curves, can we prove the existence of a common normal?",,"One suggestion would be to say the every two curves will have a distance of closest approach, and this distance will be normal to both of them, but this is either: 1)what we are actually trying to prove 2) inherently wrong in the sense, there maybe 2 curves who don't have a common normal so their distance of closest approach wouldn't be normal to them.","One suggestion would be to say the every two curves will have a distance of closest approach, and this distance will be normal to both of them, but this is either: 1)what we are actually trying to prove 2) inherently wrong in the sense, there maybe 2 curves who don't have a common normal so their distance of closest approach wouldn't be normal to them.",,"['calculus', 'derivatives', 'curves', 'algebraic-curves', 'tangent-line']"
40,Extracting diagonal of $J^TJ$ via automatic differentiation like techniques,Extracting diagonal of  via automatic differentiation like techniques,J^TJ,"First of all please, let me know if this question is more suited for scicomp.stackexchange.com or or.stackexchange.com , and sorry for my English and math skills. Pretty often i do some numerical optimization stuff with Gauss-Newton and automatic differentiation. It's pretty handy because Gauss-Newton requires to solve following system: $$ J^TJ\Delta x = -J^Tr $$ where $r(x)$ is residuals function, $J$ is jacobian of $r$ , $\Delta x$ is step.  This system can be solved via Conjugate gradient method which requires $Av$ like products. Automatic differentiation allow me to compute $Jv$ and $J^Tv$ via forward and reverse mode of graph based automatic differentiation, so i can easily combine it with CG. Unfortunately CG also needs preconditioner for fast convergence. Obvious choice for preconditioner is diagonal of $J^TJ$ . My question is - how can i quickly compute diagonal of $J^TJ$ if i know expression graph for $r$ and have implementation of $J^Tv$ and $Jv$ for each elementary function? Without symbolic differentiation, if it possible.","First of all please, let me know if this question is more suited for scicomp.stackexchange.com or or.stackexchange.com , and sorry for my English and math skills. Pretty often i do some numerical optimization stuff with Gauss-Newton and automatic differentiation. It's pretty handy because Gauss-Newton requires to solve following system: where is residuals function, is jacobian of , is step.  This system can be solved via Conjugate gradient method which requires like products. Automatic differentiation allow me to compute and via forward and reverse mode of graph based automatic differentiation, so i can easily combine it with CG. Unfortunately CG also needs preconditioner for fast convergence. Obvious choice for preconditioner is diagonal of . My question is - how can i quickly compute diagonal of if i know expression graph for and have implementation of and for each elementary function? Without symbolic differentiation, if it possible.","
J^TJ\Delta x = -J^Tr
 r(x) J r \Delta x Av Jv J^Tv J^TJ J^TJ r J^Tv Jv","['derivatives', 'numerical-optimization', 'chain-rule']"
41,"Prove that there is $c \in (0,1)$ such that $e^{f'(c)}f(0)^{f(c)}=f(1)^{f(c)}$",Prove that there is  such that,"c \in (0,1) e^{f'(c)}f(0)^{f(c)}=f(1)^{f(c)}","Let $f:R \rightarrow (0, +\infty)$ be continuous and differentiable.  Prove that there is $c \in (0,1)$ such that: $$e^{f'(c)}f(0)^{f(c)}=f(1)^{f(c)}$$ Direct application of Lagrange's theorem doesn't tell me much. I was not successful with choosing some new function, too.","Let be continuous and differentiable.  Prove that there is such that: Direct application of Lagrange's theorem doesn't tell me much. I was not successful with choosing some new function, too.","f:R \rightarrow (0, +\infty) c \in (0,1) e^{f'(c)}f(0)^{f(c)}=f(1)^{f(c)}",['real-analysis']
42,"How to ensure when the derivative approaches zero, the integral approaches a constant?","How to ensure when the derivative approaches zero, the integral approaches a constant?",,"I asked a very similar question here . But now this is different. Suppose $f(t)$ is differentiable and $c$ is a finite constant, then the following statement looks correct, but in fact it is not: \begin{equation} \lim\limits_{t \to \infty} f'(t) = 0 \implies \lim\limits_{t \to \infty} f(t)=c  \end{equation} A counter-example is $f(t)=\ln(t)$ . Now the question is, what condition should be used for the above statement to be true?","I asked a very similar question here . But now this is different. Suppose is differentiable and is a finite constant, then the following statement looks correct, but in fact it is not: A counter-example is . Now the question is, what condition should be used for the above statement to be true?","f(t) c \begin{equation}
\lim\limits_{t \to \infty} f'(t) = 0 \implies \lim\limits_{t \to \infty} f(t)=c 
\end{equation} f(t)=\ln(t)","['real-analysis', 'derivatives']"
43,Rates of change,Rates of change,,"I’m having some trouble with part c) of the following questions, a) What is the rate of change of the area A of a square with respect to its side x? b) What is the rate of change of the area A of a circle with respect to its radius r? c) Explain why one answer is the perimeter of the figure but the other answer is not. So, knowing that if we have a square with side length $x$ , then the area of the square as a function of its side is $A(x)=x^2$ . The perimeter as a function of the side is $P(x)=4x$ . And the rate of change of the area wrt its side is $\frac{dA}{dx}=2x$ . With a circle, the area as a function of the radius is $A(r)=\pi(r^2)$ . And the rate of change of the area wrt its radius is $\frac{dA}{dr}=2\pi(r)$ . The circumference as a function of the radius is also $C(r)=2\pi(r)$ . Therefore it’s the circle that’s the figure with the rate of change of the area wrt its radius equal to its perimeter, and what I saw was that the square had a rate of change of area wrt its side equal to half the perimeter of the square, $\frac{dA}{dx}=2x=\frac{4x}{2}$ . I inscribed a circle in a square with radius equal to half the square’s side length and went through the same work and then arrived at this, $A(r)=\pi(\frac{x}{2})^2=\frac{\pi}{4}x^2$ and $C(\frac{x}{2})=2\pi(\frac{x}{2}$ , and that $\frac{dA}{dr}=\frac{\pi}{2}x$ . Somehow in this example, I don’t think it’s correct because the same fact about the rate of change of area wrt radius being equal to perimeter doesn’t hold. I appreciate any help in explaining this, thank you.","I’m having some trouble with part c) of the following questions, a) What is the rate of change of the area A of a square with respect to its side x? b) What is the rate of change of the area A of a circle with respect to its radius r? c) Explain why one answer is the perimeter of the figure but the other answer is not. So, knowing that if we have a square with side length , then the area of the square as a function of its side is . The perimeter as a function of the side is . And the rate of change of the area wrt its side is . With a circle, the area as a function of the radius is . And the rate of change of the area wrt its radius is . The circumference as a function of the radius is also . Therefore it’s the circle that’s the figure with the rate of change of the area wrt its radius equal to its perimeter, and what I saw was that the square had a rate of change of area wrt its side equal to half the perimeter of the square, . I inscribed a circle in a square with radius equal to half the square’s side length and went through the same work and then arrived at this, and , and that . Somehow in this example, I don’t think it’s correct because the same fact about the rate of change of area wrt radius being equal to perimeter doesn’t hold. I appreciate any help in explaining this, thank you.",x A(x)=x^2 P(x)=4x \frac{dA}{dx}=2x A(r)=\pi(r^2) \frac{dA}{dr}=2\pi(r) C(r)=2\pi(r) \frac{dA}{dx}=2x=\frac{4x}{2} A(r)=\pi(\frac{x}{2})^2=\frac{\pi}{4}x^2 C(\frac{x}{2})=2\pi(\frac{x}{2} \frac{dA}{dr}=\frac{\pi}{2}x,"['calculus', 'derivatives']"
44,General formula for Differentiation Operator,General formula for Differentiation Operator,,"I was considering the Operator $$ x\,\frac{\rm d}{{\rm d}x} $$ and applying it $n$ times to an arbitrary function $f(x)$. Is there a general formula for it? I started with the first few \begin{align} \left(x\,\frac{\rm d}{{\rm d}x}\right)^{1} f(x) &= x f' \\ \left(x\,\frac{\rm d}{{\rm d}x}\right)^{2} f(x) &= x f' + x^2 f'' \\ \left(x\,\frac{\rm d}{{\rm d}x}\right)^{3} f(x) &= x f' + 3x^2 f'' + x^3 f''' \\ \left(x\,\frac{\rm d}{{\rm d}x}\right)^{4} f(x) &= x f' + 7 x^2 f'' + 6x^3 f''' + x^4 f'''' \\ \vdots \end{align} but I don't see any pattern yet. Obviously the first and last coefficients are always $1$. I figured if I start with any $n$ of the form $$ \left(x\,\frac{\rm d}{{\rm d}x}\right)^{n} f(x) = \sum_{k=1}^n a_k \, x^k f^{(k)}(x) \tag{1} $$ where $a_1=a_n=1$, then recursively \begin{align} \left(x\,\frac{\rm d}{{\rm d}x}\right)^{n+1} f(x) &= xf^{(1)}(x) + \sum_{k=2}^n \left(k \, a_k + a_{k-1}\right) x^k f^{(k)}(x) + x^{n+1} f^{(n+1)}(x) \\ &= \sum_{k=1}^{n+1} \left(k \, a_k + a_{k-1}\right) x^k f^{(k)}(x) \tag{2} \end{align} where $a_0=a_{n+1}=0$. From (1) and (2) we get for fixed $k$ the coupled recurrence $$ a_k(n+1) = k\, a_k(n) + a_{k-1}(n) $$ which can be put in matrix form \begin{align} \begin{pmatrix} a_1(n+1) \\ a_2(n+1) \\ a_3(n+1) \\ \vdots \\ a_{n-1}(n+1) \\ a_n(n+1) \end{pmatrix} &= \begin{pmatrix} 1 & 0 & 0 & \dots & 0 & 0 \\ 1 & 2 & 0 &  \dots & 0 & 0 \\ 0 & 1 & 3 &  \dots & 0 & 0 \\ \vdots & \vdots & \vdots & \dots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & n-1 & 0 \\ 0 & 0 & 0 & \dots & 1 & n \end{pmatrix} \begin{pmatrix} a_1(n) \\ a_2(n) \\ a_3(n) \\ \vdots \\ a_{n-1}(n) \\ a_n(n) \end{pmatrix} \\ &=\begin{pmatrix} 1 & 0 & 0 & \dots & 0 & 0 \\ 1 & 2 & 0 &  \dots & 0 & 0 \\ 0 & 1 & 3 &  \dots & 0 & 0 \\ \vdots & \vdots & \vdots & \dots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & n-1 & 0 \\ 0 & 0 & 0 & \dots & 1 & n \end{pmatrix}^n \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \\ 0 \end{pmatrix} \, . \end{align} So either I'm too stupid or there is no obvious one.","I was considering the Operator $$ x\,\frac{\rm d}{{\rm d}x} $$ and applying it $n$ times to an arbitrary function $f(x)$. Is there a general formula for it? I started with the first few \begin{align} \left(x\,\frac{\rm d}{{\rm d}x}\right)^{1} f(x) &= x f' \\ \left(x\,\frac{\rm d}{{\rm d}x}\right)^{2} f(x) &= x f' + x^2 f'' \\ \left(x\,\frac{\rm d}{{\rm d}x}\right)^{3} f(x) &= x f' + 3x^2 f'' + x^3 f''' \\ \left(x\,\frac{\rm d}{{\rm d}x}\right)^{4} f(x) &= x f' + 7 x^2 f'' + 6x^3 f''' + x^4 f'''' \\ \vdots \end{align} but I don't see any pattern yet. Obviously the first and last coefficients are always $1$. I figured if I start with any $n$ of the form $$ \left(x\,\frac{\rm d}{{\rm d}x}\right)^{n} f(x) = \sum_{k=1}^n a_k \, x^k f^{(k)}(x) \tag{1} $$ where $a_1=a_n=1$, then recursively \begin{align} \left(x\,\frac{\rm d}{{\rm d}x}\right)^{n+1} f(x) &= xf^{(1)}(x) + \sum_{k=2}^n \left(k \, a_k + a_{k-1}\right) x^k f^{(k)}(x) + x^{n+1} f^{(n+1)}(x) \\ &= \sum_{k=1}^{n+1} \left(k \, a_k + a_{k-1}\right) x^k f^{(k)}(x) \tag{2} \end{align} where $a_0=a_{n+1}=0$. From (1) and (2) we get for fixed $k$ the coupled recurrence $$ a_k(n+1) = k\, a_k(n) + a_{k-1}(n) $$ which can be put in matrix form \begin{align} \begin{pmatrix} a_1(n+1) \\ a_2(n+1) \\ a_3(n+1) \\ \vdots \\ a_{n-1}(n+1) \\ a_n(n+1) \end{pmatrix} &= \begin{pmatrix} 1 & 0 & 0 & \dots & 0 & 0 \\ 1 & 2 & 0 &  \dots & 0 & 0 \\ 0 & 1 & 3 &  \dots & 0 & 0 \\ \vdots & \vdots & \vdots & \dots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & n-1 & 0 \\ 0 & 0 & 0 & \dots & 1 & n \end{pmatrix} \begin{pmatrix} a_1(n) \\ a_2(n) \\ a_3(n) \\ \vdots \\ a_{n-1}(n) \\ a_n(n) \end{pmatrix} \\ &=\begin{pmatrix} 1 & 0 & 0 & \dots & 0 & 0 \\ 1 & 2 & 0 &  \dots & 0 & 0 \\ 0 & 1 & 3 &  \dots & 0 & 0 \\ \vdots & \vdots & \vdots & \dots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & n-1 & 0 \\ 0 & 0 & 0 & \dots & 1 & n \end{pmatrix}^n \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \\ 0 \end{pmatrix} \, . \end{align} So either I'm too stupid or there is no obvious one.",,"['derivatives', 'summation', 'recurrence-relations', 'differential-operators']"
45,How to recover the covariant derivative from the pull back from that on the principal bundle,How to recover the covariant derivative from the pull back from that on the principal bundle,,"I am watching these lecture series by Fredric Schuller. Covariant derivatives - Lec 25 - Frederic Schuller @minute 01:10:11 When we arrive at the covariant derivative from the principal bundle $P$ by pulling back to the base manifold $M$ we have: $$\nabla _{T} S=dS(T)+\omega^{u, \phi} \triangleright S$$ where $S:u \to F$, $F$ any finite dimensional vector space on an open subset $u$ on $M$, $\phi: u \to P$, is the section on the principal G-bundle,  $T \in T_{x}M$, is a tangent vector at point $x$ in the base manifold $M$, and $\omega^{u, \phi}$ is the Lie algebra valued one-form on the principal bundle . Now my question is that how to recover, if possible step by step, the more familiar covariant derivative for e.g. a vector $V$ on the base manifold which is written as: $$\nabla_{\mu}V^{\nu}=\partial _{\mu}V^{\nu}+\Gamma_{\mu \lambda}^{\nu}V^{\lambda}$$ I am a bit confused when I plug a vector $V$ instead of $S$ in the general equation above and how should I put the exterior derivative of this one-form vector-valued object and operate it on another vector $T$ and also how to operate a connection one form pulled back to the base manifold on the vector $V$?","I am watching these lecture series by Fredric Schuller. Covariant derivatives - Lec 25 - Frederic Schuller @minute 01:10:11 When we arrive at the covariant derivative from the principal bundle $P$ by pulling back to the base manifold $M$ we have: $$\nabla _{T} S=dS(T)+\omega^{u, \phi} \triangleright S$$ where $S:u \to F$, $F$ any finite dimensional vector space on an open subset $u$ on $M$, $\phi: u \to P$, is the section on the principal G-bundle,  $T \in T_{x}M$, is a tangent vector at point $x$ in the base manifold $M$, and $\omega^{u, \phi}$ is the Lie algebra valued one-form on the principal bundle . Now my question is that how to recover, if possible step by step, the more familiar covariant derivative for e.g. a vector $V$ on the base manifold which is written as: $$\nabla_{\mu}V^{\nu}=\partial _{\mu}V^{\nu}+\Gamma_{\mu \lambda}^{\nu}V^{\lambda}$$ I am a bit confused when I plug a vector $V$ instead of $S$ in the general equation above and how should I put the exterior derivative of this one-form vector-valued object and operate it on another vector $T$ and also how to operate a connection one form pulled back to the base manifold on the vector $V$?",,"['derivatives', 'differential-geometry']"
46,"Which function satisfies $𝑓: \mathbb R → [−2, 2]$ with $(𝑓(0))^ 2 + (𝑓 ′ (0))^ 2 = 85$,also $f$ is twice differentiable?","Which function satisfies  with ,also  is twice differentiable?","𝑓: \mathbb R → [−2, 2] (𝑓(0))^ 2 + (𝑓 ′ (0))^ 2 = 85 f","Let $f:\mathbb R\to[−2,2]\,$ be a twice differentiable function    with $$\big(𝑓(0)\big)^2+\big(𝑓′(0)\big)^2=85.$$ Which of the following statements are necessarily TRUE? (A) There exist 𝑟, 𝑠 ∈ ℝ, where 𝑟 < 𝑠, such that 𝑓 is one-one on   the open interval (𝑟, 𝑠) (B) There exists $𝑥_0\in(−4, 0),\,$ such that $|f'(𝑥_0)|\le 1$ (C) $\lim_{x\to\infty}f(x)=1$ (D) There exists $\,\alpha\in(−4,4),\,$ such that $\,f(\alpha) + f''(\alpha)=0$ and $f'(\alpha)\ne 0$ Solution:I tried it by using the function $f'(x)=x^3+2x+9$ ,as it is satisfying $(𝑓(0))^ 2 + (𝑓 ′ (0))^ 2 = 85$ . On checking all the 4 options on the chosen function ,i'm getting $A,D$ as correct options. My query is that  the chosen function is not satisfying the definition of function( every element  of the domain has a unique image in codomain ) as $f(3)=43$ is not in $[-2,2]$ . Please provide a function which staisfies the definition in the problem.","Let be a twice differentiable function    with Which of the following statements are necessarily TRUE? (A) There exist 𝑟, 𝑠 ∈ ℝ, where 𝑟 < 𝑠, such that 𝑓 is one-one on   the open interval (𝑟, 𝑠) (B) There exists such that (C) (D) There exists such that and Solution:I tried it by using the function ,as it is satisfying . On checking all the 4 options on the chosen function ,i'm getting as correct options. My query is that  the chosen function is not satisfying the definition of function( every element  of the domain has a unique image in codomain ) as is not in . Please provide a function which staisfies the definition in the problem.","f:\mathbb R\to[−2,2]\, \big(𝑓(0)\big)^2+\big(𝑓′(0)\big)^2=85. 𝑥_0\in(−4, 0),\, |f'(𝑥_0)|\le 1 \lim_{x\to\infty}f(x)=1 \,\alpha\in(−4,4),\, \,f(\alpha) + f''(\alpha)=0 f'(\alpha)\ne 0 f'(x)=x^3+2x+9 (𝑓(0))^
2 + (𝑓
′
(0))^
2
= 85 A,D f(3)=43 [-2,2]","['real-analysis', 'calculus', 'derivatives', 'solution-verification', 'alternative-proof']"
47,How to find the point on an ellipse that is closest to the point A outside of the ellipse,How to find the point on an ellipse that is closest to the point A outside of the ellipse,,"I am working on a project where I need a formula to find the point $B(x,y)$ on the ellipse $x^2+4y^2=r^2$ that is closest to the point $A(x_0,y_0)$, where $A$ is known and is outside the ellipse. I have been trying to use the distance formula $d=\sqrt{(x-x_0)^2+(y-y_0)^2}$ but I have been running into some problems trying differentiate. Any help would be greatly appreciated.","I am working on a project where I need a formula to find the point $B(x,y)$ on the ellipse $x^2+4y^2=r^2$ that is closest to the point $A(x_0,y_0)$, where $A$ is known and is outside the ellipse. I have been trying to use the distance formula $d=\sqrt{(x-x_0)^2+(y-y_0)^2}$ but I have been running into some problems trying differentiate. Any help would be greatly appreciated.",,"['calculus', 'derivatives', 'conic-sections']"
48,Clairaut Theorem Counterexample,Clairaut Theorem Counterexample,,"Do you know one function $f:\mathbb{R}^2\to\mathbb{R}$ such that $f_{xy}(a,b)=f_{yx}(a,b)$ at point $(a,b)$ but $f_{xy}$ are not continuous at $(a,b)$?","Do you know one function $f:\mathbb{R}^2\to\mathbb{R}$ such that $f_{xy}(a,b)=f_{yx}(a,b)$ at point $(a,b)$ but $f_{xy}$ are not continuous at $(a,b)$?",,"['calculus', 'derivatives']"
49,Laplace transform of the derivative of the dirac delta function times another function,Laplace transform of the derivative of the dirac delta function times another function,,"I'm trying to solve a DE involving terms of the form $\dot{\delta}(t-k)f(t)$ and $\ddot{\delta}(t-k)f(t)$, where $k>0$. I would therefore like to find the Laplace transform of these terms. My approach so far has been to simply integrate by parts, ie in the case of $\dot{\delta}(t-k)f(t)$ I get $\mathcal{L}\left\{ \dot{\delta}\left(t-k\right)f\left(t\right)\right\} =\int_{0}^{\infty}\dot{\delta}\left(t-k\right)f\left(t\right)\exp\left(-ts\right)dt$ $=\left[\delta\left(t-k\right)f\left(t\right)\exp\left(-ts\right)\right]_{0}^{\infty}-\int_{0}^{\infty}\delta\left(t-k\right)\left[\dot{f}\left(t\right)\exp\left(-ts\right)-sf\left(t\right)\exp\left(-ts\right)\right]dt$ Since $\delta\left(t\right)=0$ for all $t\neq k$ and since $k>0$, it must hold that $\left[\delta\left(t-k\right)f\left(t\right)\exp\left(-ts\right)\right]_{0}^{\infty}=0$, so by the sifting property of $\delta(t)$ I get $\mathcal{L}\left\{ \dot{\delta}\left(t-k\right)f\left(t\right)\right\} = \left(sf\left(k\right)-\dot{f}\left(k\right)\right)\exp\left(-ks\right)$ Using the same approach, I get $\mathcal{L}\left\{ \ddot{\delta}\left(t-k\right)f\left(t\right)\right\} =\left(\ddot{f}\left(k\right)-2s\dot{f}\left(k\right)+s^{2}f\left(k\right)\right)\exp\left(-ks\right)$ My question has two parts: 1) Is this the correct approach? 2) What happens if $k=0$?","I'm trying to solve a DE involving terms of the form $\dot{\delta}(t-k)f(t)$ and $\ddot{\delta}(t-k)f(t)$, where $k>0$. I would therefore like to find the Laplace transform of these terms. My approach so far has been to simply integrate by parts, ie in the case of $\dot{\delta}(t-k)f(t)$ I get $\mathcal{L}\left\{ \dot{\delta}\left(t-k\right)f\left(t\right)\right\} =\int_{0}^{\infty}\dot{\delta}\left(t-k\right)f\left(t\right)\exp\left(-ts\right)dt$ $=\left[\delta\left(t-k\right)f\left(t\right)\exp\left(-ts\right)\right]_{0}^{\infty}-\int_{0}^{\infty}\delta\left(t-k\right)\left[\dot{f}\left(t\right)\exp\left(-ts\right)-sf\left(t\right)\exp\left(-ts\right)\right]dt$ Since $\delta\left(t\right)=0$ for all $t\neq k$ and since $k>0$, it must hold that $\left[\delta\left(t-k\right)f\left(t\right)\exp\left(-ts\right)\right]_{0}^{\infty}=0$, so by the sifting property of $\delta(t)$ I get $\mathcal{L}\left\{ \dot{\delta}\left(t-k\right)f\left(t\right)\right\} = \left(sf\left(k\right)-\dot{f}\left(k\right)\right)\exp\left(-ks\right)$ Using the same approach, I get $\mathcal{L}\left\{ \ddot{\delta}\left(t-k\right)f\left(t\right)\right\} =\left(\ddot{f}\left(k\right)-2s\dot{f}\left(k\right)+s^{2}f\left(k\right)\right)\exp\left(-ks\right)$ My question has two parts: 1) Is this the correct approach? 2) What happens if $k=0$?",,"['derivatives', 'laplace-transform', 'dirac-delta']"
50,theorem about inflection points in (polynomial times exponential),theorem about inflection points in (polynomial times exponential),,"We know that an exponential function ""kills"" a polynomial, sooner or later. Here I think of polynomials that are increasing and exponential functions that are decreasing. For instance: $x^2e^{-x^2}$ will go to zero as $x$ grows, as will $x^7e^{-x^2}$, $(C_1\cdot x^9+C_2\cdot x^{111})e^{-x^7-3x^9}$ or any similar such function. These functions all decrease, for large enough $x$, and go to zero, asymptotically (think of $C_1$, $C_2$ as positive). The way I set up and think about this problem, the exponential function can have no linear term in $x$. Furthermore, the polynomial should only has positive terms, and the exponential only negative terms. Now, my question is if there is a theorem saying that, after having reached its rightmost stationary point, and as x grows further, the function has only one inflection point, and changes exactly once from concave to convex, as it goes to zero? Is this an implication of the fact that the exponential function ultimately dominates the polynomial function? I would very much appreciate any insight on the generality of this ""hypothesis"". Thank you.","We know that an exponential function ""kills"" a polynomial, sooner or later. Here I think of polynomials that are increasing and exponential functions that are decreasing. For instance: $x^2e^{-x^2}$ will go to zero as $x$ grows, as will $x^7e^{-x^2}$, $(C_1\cdot x^9+C_2\cdot x^{111})e^{-x^7-3x^9}$ or any similar such function. These functions all decrease, for large enough $x$, and go to zero, asymptotically (think of $C_1$, $C_2$ as positive). The way I set up and think about this problem, the exponential function can have no linear term in $x$. Furthermore, the polynomial should only has positive terms, and the exponential only negative terms. Now, my question is if there is a theorem saying that, after having reached its rightmost stationary point, and as x grows further, the function has only one inflection point, and changes exactly once from concave to convex, as it goes to zero? Is this an implication of the fact that the exponential function ultimately dominates the polynomial function? I would very much appreciate any insight on the generality of this ""hypothesis"". Thank you.",,"['derivatives', 'polynomials', 'convex-analysis', 'exponential-function']"
51,Condition for $u\in C^k(\Omega)$ to be extendable to $u\in C^k(\overline{\Omega})$ for an open domain $\Omega\subset\Bbb R^n$,Condition for  to be extendable to  for an open domain,u\in C^k(\Omega) u\in C^k(\overline{\Omega}) \Omega\subset\Bbb R^n,"Fix some open $\Omega\subset\mathbb{R}^n$. Recall that for any subset $S\subset\mathbb{R}^n$, we define $$C^k(S)=\{u:S\to\mathbb{R}\ :\ \exists\hbox{ open }U\supseteq E,\ v\in C^k(U),\hbox{ s.t. }v\vert_S=u\}$$ Let $u\in C^k(\Omega)$ for some open $\Omega\subset\mathbb{R}^n$, such that for all $\lvert\alpha\rvert\le k$ we can extend $\partial^\alpha u$ continuously to $\overline{\Omega}$. My PDE professor claimed that this implies that $u\in C^k(\overline{\Omega})$, but I don't immediately see why this is true. I succeeded in proving using Tietze's Extension Theorem that if $\partial_j u$ exists on $\Omega$ can be extended continuously to $\overline{\Omega}$, then $u$ can be extended to $\mathbb{R}^n$ such that $\partial_j u\in C(\mathbb{R}^n)$, but I have no way of controlling the different extensions given by different derivatives, it seems.","Fix some open $\Omega\subset\mathbb{R}^n$. Recall that for any subset $S\subset\mathbb{R}^n$, we define $$C^k(S)=\{u:S\to\mathbb{R}\ :\ \exists\hbox{ open }U\supseteq E,\ v\in C^k(U),\hbox{ s.t. }v\vert_S=u\}$$ Let $u\in C^k(\Omega)$ for some open $\Omega\subset\mathbb{R}^n$, such that for all $\lvert\alpha\rvert\le k$ we can extend $\partial^\alpha u$ continuously to $\overline{\Omega}$. My PDE professor claimed that this implies that $u\in C^k(\overline{\Omega})$, but I don't immediately see why this is true. I succeeded in proving using Tietze's Extension Theorem that if $\partial_j u$ exists on $\Omega$ can be extended continuously to $\overline{\Omega}$, then $u$ can be extended to $\mathbb{R}^n$ such that $\partial_j u\in C(\mathbb{R}^n)$, but I have no way of controlling the different extensions given by different derivatives, it seems.",,"['real-analysis', 'derivatives', 'partial-differential-equations']"
52,Functional Derivative of a function,Functional Derivative of a function,,"I want to know the steps involved for the functional derivative that has been taken for the below objective function And the derivative for above objective function is give below These equations are from Generative Adversarial Networks, http://arxiv.org/pdf/1701.00160.pdf can be found on page 46 and 47. Please help me with the steps for the functional derivative of the objective function.","I want to know the steps involved for the functional derivative that has been taken for the below objective function And the derivative for above objective function is give below These equations are from Generative Adversarial Networks, http://arxiv.org/pdf/1701.00160.pdf can be found on page 46 and 47. Please help me with the steps for the functional derivative of the objective function.",,"['derivatives', 'machine-learning', 'frechet-derivative']"
53,How to prove that a tangent cuts through a point when it has an odd order?,How to prove that a tangent cuts through a point when it has an odd order?,,"I noticed that when a straight line which is a tangent to a polynomial at point $x_1$ is equated to the polynomial, we get a repeated root at point $x_1$. Let's define a cubic function: $$y=f(x)=ax^3+bx^2+cx+d$$ whose derivative is equal to: $$\frac{dy}{dx}=3ax^2+2bx+c$$ At point $x_1$, the slope is $3ax_1^2+2bx_1+c$. The tangent for point $x_1$ is thus: $$y-y_1=m(x-x_1)$$ $$\implies y-(ax^3_1+bx^2_1+cx_1+d)=(3ax_1^2+2bx_1+c)(x-x_1)$$ $$\implies y=(3ax_1^2+2bx_1+c)x-(2ax_1^3+bx_1^2-d)$$ This form of the tangent is general to every cubic. We can equate the tangent to the polnomial: $$ax^3+bx^2+cx+d=(3ax_1^2+2bx_1+c)x-(2ax_1^3+bx_1^2-d)$$ $$\implies ax^3+bx^2-(3ax_1^2+2bx_1)x+(2ax_1^3+bx_1^2)=0$$ Since we know $x-x_1$ is a factor, we can factor it out. $$\implies (x-x_1)[ax^2+bx+ax_1x-(2ax_1^2+bx_1)]=0$$ $$\implies (x-x_1)^2(ax+2ax_1+b)=0$$ Here the root is repeated. So going deeper into this I noticed that if $(x-x_1)$ is repeated thrice, then the tangent cuts through the line and that if it is repeated four times it touches the line. I came to the conjecture that if the root is repeated an even number times, the tangent will touch the curve, but if it is repeated an odd number of times, the tangent will cut through the curve. But how would one prove this conjecture?","I noticed that when a straight line which is a tangent to a polynomial at point $x_1$ is equated to the polynomial, we get a repeated root at point $x_1$. Let's define a cubic function: $$y=f(x)=ax^3+bx^2+cx+d$$ whose derivative is equal to: $$\frac{dy}{dx}=3ax^2+2bx+c$$ At point $x_1$, the slope is $3ax_1^2+2bx_1+c$. The tangent for point $x_1$ is thus: $$y-y_1=m(x-x_1)$$ $$\implies y-(ax^3_1+bx^2_1+cx_1+d)=(3ax_1^2+2bx_1+c)(x-x_1)$$ $$\implies y=(3ax_1^2+2bx_1+c)x-(2ax_1^3+bx_1^2-d)$$ This form of the tangent is general to every cubic. We can equate the tangent to the polnomial: $$ax^3+bx^2+cx+d=(3ax_1^2+2bx_1+c)x-(2ax_1^3+bx_1^2-d)$$ $$\implies ax^3+bx^2-(3ax_1^2+2bx_1)x+(2ax_1^3+bx_1^2)=0$$ Since we know $x-x_1$ is a factor, we can factor it out. $$\implies (x-x_1)[ax^2+bx+ax_1x-(2ax_1^2+bx_1)]=0$$ $$\implies (x-x_1)^2(ax+2ax_1+b)=0$$ Here the root is repeated. So going deeper into this I noticed that if $(x-x_1)$ is repeated thrice, then the tangent cuts through the line and that if it is repeated four times it touches the line. I came to the conjecture that if the root is repeated an even number times, the tangent will touch the curve, but if it is repeated an odd number of times, the tangent will cut through the curve. But how would one prove this conjecture?",,"['calculus', 'derivatives']"
54,Mean value theorem for convex functions,Mean value theorem for convex functions,,"Let $f$ be a real function with left and right derivatives $f'_-$ and $f'_+$ on the open interval $(a,b)$, and continuous on $[a,b]$ (e.g., let $f$ be convex on $[a,b]$). Then, Is there something like the mean value theorem for $f$?","Let $f$ be a real function with left and right derivatives $f'_-$ and $f'_+$ on the open interval $(a,b)$, and continuous on $[a,b]$ (e.g., let $f$ be convex on $[a,b]$). Then, Is there something like the mean value theorem for $f$?",,"['calculus', 'derivatives', 'convex-analysis']"
55,When do partial derivatives fail to commute?,When do partial derivatives fail to commute?,,"What are the conditions that a function $f(x,y)$ should satisfy for the partial derivatives $f_{xy}$ and $f_{yx}$ to be equal?","What are the conditions that a function $f(x,y)$ should satisfy for the partial derivatives $f_{xy}$ and $f_{yx}$ to be equal?",,"['derivatives', 'continuity', 'partial-derivative']"
56,Other example of non continuous derivative,Other example of non continuous derivative,,"I was trying to build an example of a function that is differentiable at $0$, and around $0$. But the derivative is not continuous at $0$ A family of functions that work is: (thank you Andrew D. Hwang for the general form) $$ f(x) = \left\{     \begin{array}{ll}         x^{1+\epsilon}\psi(x^{-\alpha}))  & \mbox{if } x\ne0 \\         0 & \mbox{if x=0}     \end{array} \right. $$ With $\psi$ a periodic and bounded function (or a modified trig function) and $\alpha>0,\epsilon>0$ Is there an example that does not belong to this family of functions? (I have found such examples, but I am not satisfied with them because of how I built them (they are not deeply different), so I'm still interested to get ideas!)","I was trying to build an example of a function that is differentiable at $0$, and around $0$. But the derivative is not continuous at $0$ A family of functions that work is: (thank you Andrew D. Hwang for the general form) $$ f(x) = \left\{     \begin{array}{ll}         x^{1+\epsilon}\psi(x^{-\alpha}))  & \mbox{if } x\ne0 \\         0 & \mbox{if x=0}     \end{array} \right. $$ With $\psi$ a periodic and bounded function (or a modified trig function) and $\alpha>0,\epsilon>0$ Is there an example that does not belong to this family of functions? (I have found such examples, but I am not satisfied with them because of how I built them (they are not deeply different), so I'm still interested to get ideas!)",,"['calculus', 'derivatives', 'continuity', 'examples-counterexamples']"
57,Finding Solutions (with only pen and paper),Finding Solutions (with only pen and paper),,"For what least value of $k$ does the equation:$$e^x=kx^2$$ Have 3 solutions? Let $f(x)=e^x$ and $g(x)=kx^2$. For a positive $k$, drawing a rough graph of $f(x)$ and $g(x)$ does show 2 solutions of the equation. But what parameters are we to use while finding the conditions on $k$? I used the idea that since there exist 3 solutions, there must be a point where  the rate of increase of the $g(x)$ is more than that of $f(x)$ and there must also be a point where the rate of increase of $g(x)$ becomes lesser than rate of increase of $f(x)$. But after differentiating, you get another inequality which deals with an exponential function being greater than the linear function. The limiting value of the slope of the line will be the slope of the tangent to the curve $e^x$ which passes through the origin. Again, this answer is in terms of another constant, say, $h$ where the tangent meets the curve. Please advice on how to proceed.","For what least value of $k$ does the equation:$$e^x=kx^2$$ Have 3 solutions? Let $f(x)=e^x$ and $g(x)=kx^2$. For a positive $k$, drawing a rough graph of $f(x)$ and $g(x)$ does show 2 solutions of the equation. But what parameters are we to use while finding the conditions on $k$? I used the idea that since there exist 3 solutions, there must be a point where  the rate of increase of the $g(x)$ is more than that of $f(x)$ and there must also be a point where the rate of increase of $g(x)$ becomes lesser than rate of increase of $f(x)$. But after differentiating, you get another inequality which deals with an exponential function being greater than the linear function. The limiting value of the slope of the line will be the slope of the tangent to the curve $e^x$ which passes through the origin. Again, this answer is in terms of another constant, say, $h$ where the tangent meets the curve. Please advice on how to proceed.",,"['calculus', 'derivatives', 'graphing-functions']"
58,Power series differentiability at endpoints,Power series differentiability at endpoints,,"I have the following problem: Find domain $I$ of the function defined by    $f(x)=\sum\limits_{n=1}^{\infty}(3^{\frac{1}{n^2}}-1)x^n.$   Investigate differentiability of $f(x)$ in the interior of $I$ and at endpoints in case they are in $I$. Using Cauchy–Hadamard theorem, one easily gets radius of convergence: $R=1$. The series converges at $1,-1$. It is easily proven that $f(x)$ is differentiable in $(-1,1)$. I have problems with endpoints. Term-by-term differentiating gives series which converges at $-1$ and diverges at $1$. But, as far as I know, this doesn't prove anything about differentiability of the initial series at these points. If I could've proven uniform convergence of differentiated series in $[-1,c]$, where $-1<c<1$, I would get differentiability at $-1$, but that still leaves $1$. So how do I investigate differentiability at endpoints? Are there some well-known facts that I'm missing? Thanks in advance.","I have the following problem: Find domain $I$ of the function defined by    $f(x)=\sum\limits_{n=1}^{\infty}(3^{\frac{1}{n^2}}-1)x^n.$   Investigate differentiability of $f(x)$ in the interior of $I$ and at endpoints in case they are in $I$. Using Cauchy–Hadamard theorem, one easily gets radius of convergence: $R=1$. The series converges at $1,-1$. It is easily proven that $f(x)$ is differentiable in $(-1,1)$. I have problems with endpoints. Term-by-term differentiating gives series which converges at $-1$ and diverges at $1$. But, as far as I know, this doesn't prove anything about differentiability of the initial series at these points. If I could've proven uniform convergence of differentiated series in $[-1,c]$, where $-1<c<1$, I would get differentiability at $-1$, but that still leaves $1$. So how do I investigate differentiability at endpoints? Are there some well-known facts that I'm missing? Thanks in advance.",,"['derivatives', 'power-series', 'uniform-convergence']"
59,Find the maximum area of a right triangle with a constant perimeter P.,Find the maximum area of a right triangle with a constant perimeter P.,,"I have been learning calculus from a tutor and I have been trying to solve a problem that he gave me. The problem is to find the maximum area of a right triangle with a constant perimeter $P$. To start solving this problem I wrote down the different equations for the area and perimeter of a right triangle. $A=\frac{a*b}{2}$ for area and $P= a+b+h$ for the perimeter. I decided to first find a side length by substituting $\sqrt{a^2+b^2}$ for $h$  and then solving for $a$ in the perimeter equation.  Here are the steps I took… $$P=a+b+\sqrt {a^2+b^2}$$ $$(P-a-b)^2=(\sqrt{a^2+b^2})^2$$ $$P^2+2ab-2aP-2bP=0$$ $$2a(b-P)=2bP-P^2$$ $$a=(\frac{1}2)(\frac{2bP-P^2}{b-P})$$ Now that I have the equation for $a$, I’m uncertain about how to proceed. I know that I could also solve for side length $b$ and put the two side length equations in for $a$ and $b$ in the area equation and get ... $$A=\frac{(\frac{1}{2})(\frac{2aP-P^2}{a-P})(\frac{1}{2})(\frac{2bP-P^2}{b-P})}{2}$$ or I could just substitute b in the equation and get… $$A=\frac{a*(\frac{1}{2})(\frac{2bP-P^2}{b-P})}{2}$$ I also know that once I have an equation for area I need to find its derivative and set it equal to zero and then solve for $P$. What I’m unsure about is which area equation I need and how to find its derivative. My tutor told me that I need to use both the chain rule and product rule in order to find the derivative. I can use both the chain rule and the product rule separately but I’m not sure how to use both on either equation.","I have been learning calculus from a tutor and I have been trying to solve a problem that he gave me. The problem is to find the maximum area of a right triangle with a constant perimeter $P$. To start solving this problem I wrote down the different equations for the area and perimeter of a right triangle. $A=\frac{a*b}{2}$ for area and $P= a+b+h$ for the perimeter. I decided to first find a side length by substituting $\sqrt{a^2+b^2}$ for $h$  and then solving for $a$ in the perimeter equation.  Here are the steps I took… $$P=a+b+\sqrt {a^2+b^2}$$ $$(P-a-b)^2=(\sqrt{a^2+b^2})^2$$ $$P^2+2ab-2aP-2bP=0$$ $$2a(b-P)=2bP-P^2$$ $$a=(\frac{1}2)(\frac{2bP-P^2}{b-P})$$ Now that I have the equation for $a$, I’m uncertain about how to proceed. I know that I could also solve for side length $b$ and put the two side length equations in for $a$ and $b$ in the area equation and get ... $$A=\frac{(\frac{1}{2})(\frac{2aP-P^2}{a-P})(\frac{1}{2})(\frac{2bP-P^2}{b-P})}{2}$$ or I could just substitute b in the equation and get… $$A=\frac{a*(\frac{1}{2})(\frac{2bP-P^2}{b-P})}{2}$$ I also know that once I have an equation for area I need to find its derivative and set it equal to zero and then solve for $P$. What I’m unsure about is which area equation I need and how to find its derivative. My tutor told me that I need to use both the chain rule and product rule in order to find the derivative. I can use both the chain rule and the product rule separately but I’m not sure how to use both on either equation.",,"['calculus', 'derivatives']"
60,"Function with derivatives with ""fixed"" values at $0$","Function with derivatives with ""fixed"" values at",0,"If $ \{a_n \}_{n \in \mathbb{N}} \subset \mathbb{R}$ is a non-constant sequence of real numbers, is possible to find a function $f \in \mathcal{C}^{\infty}_{b}(\mathbb{R})$ ( = space of bounded smooth functions) such that and $f^{(n)}(0)=a_n$ for every $n \in \mathbb{N}$? I am struggling with this problem, arisen from my sleepless nights, since a couple of days, but I have no idea of how to prove/disprove it. I only noticed that if I remove the hypothesis of boundedness and I assume $\limsup_{n} \sqrt[n]{a_n / n!}=0$ then $f(x) = \sum_{n \ge 1} \frac{a_n}{n!} x^n$ should work, but this doesn't help so much. Thanks in advance!","If $ \{a_n \}_{n \in \mathbb{N}} \subset \mathbb{R}$ is a non-constant sequence of real numbers, is possible to find a function $f \in \mathcal{C}^{\infty}_{b}(\mathbb{R})$ ( = space of bounded smooth functions) such that and $f^{(n)}(0)=a_n$ for every $n \in \mathbb{N}$? I am struggling with this problem, arisen from my sleepless nights, since a couple of days, but I have no idea of how to prove/disprove it. I only noticed that if I remove the hypothesis of boundedness and I assume $\limsup_{n} \sqrt[n]{a_n / n!}=0$ then $f(x) = \sum_{n \ge 1} \frac{a_n}{n!} x^n$ should work, but this doesn't help so much. Thanks in advance!",,"['calculus', 'real-analysis', 'derivatives']"
61,Prove that $f'(0)=L$.,Prove that .,f'(0)=L,"Let $f$ be continuous at $0$. Suppose $\displaystyle \lim _{x\rightarrow 0} \frac{f(2x)-f(x)}{x} =L$. Prove that $f'(0)=L$. My Work: $\displaystyle \Bigg|\frac{f(x)-f(0)}{x}-L\Bigg|=\Bigg|\frac{f(x)-f(2x)+f(2x)-f(0)}{x}-L\Bigg|$ $\displaystyle \leq \Bigg|\frac{f(x)-f(2x)}{x}-L\Bigg|+\Bigg|\frac{f(2x)-f(0)}{x}\Bigg|$. Since $f$ is continuous at $0, |f(2x)-f(0)|$ goes to $0$ as $x$ goes to $0$. Now I want to get rid of $|x|$ of the second term. How can I do it? Can somebody please help me?","Let $f$ be continuous at $0$. Suppose $\displaystyle \lim _{x\rightarrow 0} \frac{f(2x)-f(x)}{x} =L$. Prove that $f'(0)=L$. My Work: $\displaystyle \Bigg|\frac{f(x)-f(0)}{x}-L\Bigg|=\Bigg|\frac{f(x)-f(2x)+f(2x)-f(0)}{x}-L\Bigg|$ $\displaystyle \leq \Bigg|\frac{f(x)-f(2x)}{x}-L\Bigg|+\Bigg|\frac{f(2x)-f(0)}{x}\Bigg|$. Since $f$ is continuous at $0, |f(2x)-f(0)|$ goes to $0$ as $x$ goes to $0$. Now I want to get rid of $|x|$ of the second term. How can I do it? Can somebody please help me?",,"['calculus', 'real-analysis', 'derivatives', 'continuity']"
62,"For a function which is everywhere right-differentiable, what can be said about the existence of points where it is differentiable?","For a function which is everywhere right-differentiable, what can be said about the existence of points where it is differentiable?",,"We know that a function which is right-differentiable everywhere is also continuous almost-everywhere, but what about differentiability? For example, is there a function which is everywhere right-differentiable but nowhere-differentiable? Also, what happens if we weaken the assumption and say that a function is right-differentiable everywhere except on a countable set. How small can the set of points where it is non-differentiable be?","We know that a function which is right-differentiable everywhere is also continuous almost-everywhere, but what about differentiability? For example, is there a function which is everywhere right-differentiable but nowhere-differentiable? Also, what happens if we weaken the assumption and say that a function is right-differentiable everywhere except on a countable set. How small can the set of points where it is non-differentiable be?",,"['real-analysis', 'derivatives']"
63,Solve $f'(x) = 0$ and set up a sign chart for $f'$.,Solve  and set up a sign chart for .,f'(x) = 0 f',"I understand how my teacher got the two $x$ values, but why didn't he solve for $e^x=0$? I know he did $x=0$ which is $0$ $x+2=0$ which is $-2$ so why no $e^x=0$? is there even an answer for that? I don't think there is right?","I understand how my teacher got the two $x$ values, but why didn't he solve for $e^x=0$? I know he did $x=0$ which is $0$ $x+2=0$ which is $-2$ so why no $e^x=0$? is there even an answer for that? I don't think there is right?",,['derivatives']
64,Is this notation good for the chain rule derivative?,Is this notation good for the chain rule derivative?,,"When we take this derivative, for example: $$y = \log(\sin x)$$ We call $u = \sin x$, so we have: $$\frac{dy}{dx} = \frac{d y}{du}\frac{du}{dx} = \frac{1}{u}\cos x = \frac{\cos x}{\sin x}$$ But for me, it's better to do: $$\frac{d\log\color{Blue}{\sin x}}{d\color{Blue}{\sin x}}\frac{d\sin \color{Red}{x}}{d\color{Red}{x}} = \frac{1}{\color{Blue}{\sin x}}\cos \color{Red}{x}$$ It makes easy to do the 'pattern-matching' just by looking at the differentials. No substitution. I know that $\frac{d \log[\mbox{something}]}{d[\mbox{something}]} = \frac{1}{\mbox{something}}$ for example. However, it looks 'hairy' when I try with larger derivatives, like, for the function: $$(13x^2-5x+8)^{\frac{1}{2}}$$ we do: $$\frac{d(13x^2-5x+8)^{\frac{1}{2}}}{dx} = \frac{d\color{Green}{(13x^2-5x+8)}^{\frac{1}{2}}}{d\color{Green}{(13x^2-5x+8)}}\frac{d(13x^2-5x+8)}{dx} = \frac{1}{2\sqrt{\color{Green}{13x^2-5x+8}}}(26x -5)$$ but it's really better for me to do like this, instead of doing the bla bla bla of changing variables and stuff. But I'm afraid my teacher does not accept this. Is this notation/way of doing good for you guys? One more example : $$\frac{d}{dx}\sqrt{(\sin(7x+\ln(5x)))} = $$ $$\frac{d[\color{Blue}{\sin(7x+\ln(5x))}]^{1/2}}{d[\color{Blue}{\sin(7x+\ln(5x))}]}\frac{d[\sin\color{Red}{(7x+\ln(5x))}]}{d[\color{Red}{7x+\ln(5x)}]}\left[\frac{d[7\color{Purple}{x}]}{d[\color{Purple}{x}]} + \frac{d[\ln(\color{Purple}{5x})]}{d[\color{Purple}{5x}]}\frac{d[5x]}{d[x]}\right] = $$ $$\frac{1}{2}\left[\color{Blue}{\sin(7x+\ln(5x))}\right]^{-1/2}\cdot\cos(\color{Red}{7x+\ln(5x)})\left[7 + \frac{1}{\color{Purple}{5x}}\cdot 5\right]$$ So we get rid of the substitution! (づ｡◕‿‿◕｡)づ $\ \ u, v, y$ go away!","When we take this derivative, for example: $$y = \log(\sin x)$$ We call $u = \sin x$, so we have: $$\frac{dy}{dx} = \frac{d y}{du}\frac{du}{dx} = \frac{1}{u}\cos x = \frac{\cos x}{\sin x}$$ But for me, it's better to do: $$\frac{d\log\color{Blue}{\sin x}}{d\color{Blue}{\sin x}}\frac{d\sin \color{Red}{x}}{d\color{Red}{x}} = \frac{1}{\color{Blue}{\sin x}}\cos \color{Red}{x}$$ It makes easy to do the 'pattern-matching' just by looking at the differentials. No substitution. I know that $\frac{d \log[\mbox{something}]}{d[\mbox{something}]} = \frac{1}{\mbox{something}}$ for example. However, it looks 'hairy' when I try with larger derivatives, like, for the function: $$(13x^2-5x+8)^{\frac{1}{2}}$$ we do: $$\frac{d(13x^2-5x+8)^{\frac{1}{2}}}{dx} = \frac{d\color{Green}{(13x^2-5x+8)}^{\frac{1}{2}}}{d\color{Green}{(13x^2-5x+8)}}\frac{d(13x^2-5x+8)}{dx} = \frac{1}{2\sqrt{\color{Green}{13x^2-5x+8}}}(26x -5)$$ but it's really better for me to do like this, instead of doing the bla bla bla of changing variables and stuff. But I'm afraid my teacher does not accept this. Is this notation/way of doing good for you guys? One more example : $$\frac{d}{dx}\sqrt{(\sin(7x+\ln(5x)))} = $$ $$\frac{d[\color{Blue}{\sin(7x+\ln(5x))}]^{1/2}}{d[\color{Blue}{\sin(7x+\ln(5x))}]}\frac{d[\sin\color{Red}{(7x+\ln(5x))}]}{d[\color{Red}{7x+\ln(5x)}]}\left[\frac{d[7\color{Purple}{x}]}{d[\color{Purple}{x}]} + \frac{d[\ln(\color{Purple}{5x})]}{d[\color{Purple}{5x}]}\frac{d[5x]}{d[x]}\right] = $$ $$\frac{1}{2}\left[\color{Blue}{\sin(7x+\ln(5x))}\right]^{-1/2}\cdot\cos(\color{Red}{7x+\ln(5x)})\left[7 + \frac{1}{\color{Purple}{5x}}\cdot 5\right]$$ So we get rid of the substitution! (づ｡◕‿‿◕｡)づ $\ \ u, v, y$ go away!",,"['calculus', 'derivatives', 'notation']"
65,Differentiable function with non-differentiable inverse,Differentiable function with non-differentiable inverse,,"Is it possible to define a bijective function $f: \mathbb{R} \to \mathbb{R}$ that is differentiable at a point $x_0$ such that $f'(x_0) \ne 0$ , but $f^{-1}$ is not differentiable at $f(x_0)$ ? I think it is possible and I was trying to split functions for rationals and irrationals but most of those functions fail to be bijective. Any ideas?","Is it possible to define a bijective function that is differentiable at a point such that , but is not differentiable at ? I think it is possible and I was trying to split functions for rationals and irrationals but most of those functions fail to be bijective. Any ideas?",f: \mathbb{R} \to \mathbb{R} x_0 f'(x_0) \ne 0 f^{-1} f(x_0),"['calculus', 'real-analysis', 'derivatives']"
66,Prove the existence of $c$ such that $f'(c) = 2c(f(c) - f(0))$,Prove the existence of  such that,c f'(c) = 2c(f(c) - f(0)),"Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function s.t. $f'$ is continuous. Suppose $f'\left(\frac{1}{2}\right)=0$, prove that there is $c\in\left(0,\frac{1}{2}\right)$ s.t. $$f'(c)=2c(f(c)-f(0))$$","Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function s.t. $f'$ is continuous. Suppose $f'\left(\frac{1}{2}\right)=0$, prove that there is $c\in\left(0,\frac{1}{2}\right)$ s.t. $$f'(c)=2c(f(c)-f(0))$$",,"['calculus', 'real-analysis', 'derivatives']"
67,Notation of derivatives...,Notation of derivatives...,,I asked my teacher the difference between this notations. (1) $$\frac{dy}{dx}$$ (2) $$\frac{\delta y}{\delta x}$$ (3) $$\frac{\Delta y}{\Delta x}$$ He told me that there is no difference. I really don't think he's right... Question: I think that (1) and (2) is more like the convention expressing the limit of a fraction.  (3) instead really represent de ratio of the increments of y and x Am I right?,I asked my teacher the difference between this notations. (1) $$\frac{dy}{dx}$$ (2) $$\frac{\delta y}{\delta x}$$ (3) $$\frac{\Delta y}{\Delta x}$$ He told me that there is no difference. I really don't think he's right... Question: I think that (1) and (2) is more like the convention expressing the limit of a fraction.  (3) instead really represent de ratio of the increments of y and x Am I right?,,"['calculus', 'derivatives', 'notation']"
68,Using the Constant Function Theorem to prove the Increasing Function Theorem,Using the Constant Function Theorem to prove the Increasing Function Theorem,,"I quote Thomas W.Tucker: ... By the way, I view the Constant Function Theorem as even more basic than the IFT. It would be nice to use it as our theoretical cornerstone, but I know of no way to use it to get the IFT. ... $\,$ from $\,$ Rethinking Rigor in Calculus: The Role of the Mean Value Theorem - The American Mathematical Monthly, Vol. 104, No. 3 (Mar., 1997), page 233 $ \,$ where IFT means Increasing Function Theorem. For convenience: Increasing function $f$ means that $\,$if $c \lt d$, then $f(c) \le f(d)$; IFT: if $f'(x) \ge 0$ on $[a,b]$, then $f$ is increasing on $[a,b]$; CFT: if $f'(x)=0$ on $[a,b]$, then $f$ is constant on $[a,b]$. Is it impossible to get the IFT from the CFT ?","I quote Thomas W.Tucker: ... By the way, I view the Constant Function Theorem as even more basic than the IFT. It would be nice to use it as our theoretical cornerstone, but I know of no way to use it to get the IFT. ... $\,$ from $\,$ Rethinking Rigor in Calculus: The Role of the Mean Value Theorem - The American Mathematical Monthly, Vol. 104, No. 3 (Mar., 1997), page 233 $ \,$ where IFT means Increasing Function Theorem. For convenience: Increasing function $f$ means that $\,$if $c \lt d$, then $f(c) \le f(d)$; IFT: if $f'(x) \ge 0$ on $[a,b]$, then $f$ is increasing on $[a,b]$; CFT: if $f'(x)=0$ on $[a,b]$, then $f$ is constant on $[a,b]$. Is it impossible to get the IFT from the CFT ?",,"['calculus', 'real-analysis', 'derivatives', 'education']"
69,Sum polynomial and derivative,Sum polynomial and derivative,,How to prove that if polynomial $W(x)$ has $n$ real roots then $\forall a \in \mathbb{R}$ $a W(x)+W'(x)$ has more than $n-1$ roots I have no idea how to solve. Please some hint.,How to prove that if polynomial $W(x)$ has $n$ real roots then $\forall a \in \mathbb{R}$ $a W(x)+W'(x)$ has more than $n-1$ roots I have no idea how to solve. Please some hint.,,"['real-analysis', 'polynomials', 'derivatives']"
70,finding the derivative using quotient rule and product rule,finding the derivative using quotient rule and product rule,,"find dy/dx; a) $\frac{1-2x}{\sqrt{2+x}}$ b.) $3x(1-x^2)^{1/3}$ My attempt at a) use the quotient rule: so $dy/dx = -2 \sqrt{2+x}+ (1-2x)0.5(2+x)^{-1/2}$ but then I get stuck there, cannot simplify it, wolfram gives a nice simplified answer but not sure how to get it. b.) Product rule: $dy/dx= 3(1-x^2)^{0.5} + 3 \times 1/3 \times (1-x^2)^{-2/3}$ and again i can't seem to simplify that either to a nice wolfram answer.","find dy/dx; a) $\frac{1-2x}{\sqrt{2+x}}$ b.) $3x(1-x^2)^{1/3}$ My attempt at a) use the quotient rule: so $dy/dx = -2 \sqrt{2+x}+ (1-2x)0.5(2+x)^{-1/2}$ but then I get stuck there, cannot simplify it, wolfram gives a nice simplified answer but not sure how to get it. b.) Product rule: $dy/dx= 3(1-x^2)^{0.5} + 3 \times 1/3 \times (1-x^2)^{-2/3}$ and again i can't seem to simplify that either to a nice wolfram answer.",,"['calculus', 'derivatives']"
71,$\ell(f)>\int_{a}^{b}||f'(t)||dt$,,\ell(f)>\int_{a}^{b}||f'(t)||dt,"We know that if $f:[a,b] \rightarrow \mathbb{R}^{n}$ is a $C^1$ path then $$\ell(f)=\int_{a}^{b}\|f'(t)\|dt.$$ Moreover, in the proof of this result we use explicitly the continuity of $f'.$ I'm trying to find an example of a differentiable function (at every point of $[a,b]$ ) such that $f':[a,b] \rightarrow \mathbb{R}^n$ is not continuous and $$\ell(f) > \int_{a}^{b} \|f'(t)\|dt.$$ I found this answer , but if I'm not mistaken, the Cantor function is not differentiable at every point. Can anyone help me with this example?","We know that if is a path then Moreover, in the proof of this result we use explicitly the continuity of I'm trying to find an example of a differentiable function (at every point of ) such that is not continuous and I found this answer , but if I'm not mistaken, the Cantor function is not differentiable at every point. Can anyone help me with this example?","f:[a,b] \rightarrow \mathbb{R}^{n} C^1 \ell(f)=\int_{a}^{b}\|f'(t)\|dt. f'. [a,b] f':[a,b] \rightarrow \mathbb{R}^n \ell(f) > \int_{a}^{b} \|f'(t)\|dt.","['real-analysis', 'derivatives', 'arc-length']"
72,"For integer $x \ge 1$, does it follow that $\left(\frac{x^2+2x+1}{x^2+2x}\right)^{x+1} > \frac{x+2}{x+1}$","For integer , does it follow that",x \ge 1 \left(\frac{x^2+2x+1}{x^2+2x}\right)^{x+1} > \frac{x+2}{x+1},"Would I be correct that the answer here is yes? Here is my thinking: For $x \ge 1$ , $\dfrac{x+2}{x+1}$ is strictly decreasing. For $x \ge 1$ , $\left(\dfrac{x^2 + 2x + 1}{x^2 + 2x}\right)^{x+1}$ is strictly increasing $\dfrac{16}{9} > \dfrac{3}{2}$ To show that $\left(\dfrac{x^2 + 2x + 1}{x^2 + 2x}\right)^{x+1}$ is strictly increasing I take the derivative which is positive for $x > 0$","Would I be correct that the answer here is yes? Here is my thinking: For , is strictly decreasing. For , is strictly increasing To show that is strictly increasing I take the derivative which is positive for",x \ge 1 \dfrac{x+2}{x+1} x \ge 1 \left(\dfrac{x^2 + 2x + 1}{x^2 + 2x}\right)^{x+1} \dfrac{16}{9} > \dfrac{3}{2} \left(\dfrac{x^2 + 2x + 1}{x^2 + 2x}\right)^{x+1} x > 0,"['derivatives', 'inequality']"
73,Existence of smooth function with prescribed zeros and value of derivative at the origin,Existence of smooth function with prescribed zeros and value of derivative at the origin,,"Let $$ f(x) = \begin{cases} e^{-1/x} & x > 0 \\ 0 & x \leq 0. \end{cases} $$ Does there exist an infinitely differentiable function $h: \mathbb{R}^2 \to \mathbb{R}$ such that $$ h(x, 0) = 0, \quad h(x, f(x)) = 0, \quad x \in \mathbb{R} $$ and ${\partial h \over \partial y}(0, 0) \neq 0$ ? I do not seem to be able to come up with an example. My attempts at the problem Taking derivatives we get the following: $$ {\partial h \over \partial x}(x, 0) = 0 \quad (x \in \mathbb{R}), \quad  {\partial h \over \partial x}(x, e^{-1 \over x}) + e^{-1 \over x}{1 \over x^2}{\partial h \over \partial y}(x, e^{-1 \over x}) = 0  \quad (x > 0), $$ however neither equation gives information about the value of the partial derivatives of $h$ with respect to y at the origin. The function $h(x, y) = y(y - f(x))$ and other examples I have tried do not work. Using Taylor's theorem, $$ h(x, y) = \partial_yh(0, 0)y  + o(\|(x, y)\|), $$ and solving $y = e^{-1/x}$ for $x$ , we get $$ 0 = h\left ({-1 \over \ln y}, y\right) = \partial_yh(0, 0)y + o\left(\sqrt{y^2 + {1 \over (\ln y)^2}}\right). $$ The equation remains valid if $\partial_yh(0, 0) \neq 0$ because it is true that $$ y = o\left(\sqrt{y^2 + {1 \over (\ln y)^2}}\right), \quad y \to 0. $$ This doesn't lead to a contradiction. I've tried replacing $f(x)$ with other functions which vanish to order two at zero, but I get nothing better.","Let Does there exist an infinitely differentiable function such that and ? I do not seem to be able to come up with an example. My attempts at the problem Taking derivatives we get the following: however neither equation gives information about the value of the partial derivatives of with respect to y at the origin. The function and other examples I have tried do not work. Using Taylor's theorem, and solving for , we get The equation remains valid if because it is true that This doesn't lead to a contradiction. I've tried replacing with other functions which vanish to order two at zero, but I get nothing better.","
f(x) = \begin{cases}
e^{-1/x} & x > 0
\\
0 & x \leq 0.
\end{cases}
 h: \mathbb{R}^2 \to \mathbb{R} 
h(x, 0) = 0, \quad h(x, f(x)) = 0, \quad x \in \mathbb{R}
 {\partial h \over \partial y}(0, 0) \neq 0 
{\partial h \over \partial x}(x, 0) = 0 \quad (x \in \mathbb{R}), \quad  {\partial h \over \partial x}(x, e^{-1 \over x}) + e^{-1 \over x}{1 \over x^2}{\partial h \over \partial y}(x, e^{-1 \over x}) = 0  \quad (x > 0),
 h h(x, y) = y(y - f(x)) 
h(x, y) = \partial_yh(0, 0)y  + o(\|(x, y)\|),
 y = e^{-1/x} x 
0 = h\left ({-1 \over \ln y}, y\right) = \partial_yh(0, 0)y + o\left(\sqrt{y^2 + {1 \over (\ln y)^2}}\right).
 \partial_yh(0, 0) \neq 0 
y = o\left(\sqrt{y^2 + {1 \over (\ln y)^2}}\right), \quad y \to 0.
 f(x)","['real-analysis', 'calculus', 'derivatives', 'taylor-expansion']"
74,Nth order central difference for odd $n$,Nth order central difference for odd,n,"I am interested in the $n^{\mathrm{th}}$ -order central difference of an expression $f$ . The general form of the $n^{\mathrm{th}}$ -order central difference is given by $$\delta_h^n[f](x)=\sum_{i=0}^n(-1)^i {n \choose i}f(x+(\frac{n}{2}-i)h).$$ For odd $n$ , the central difference will have $h$ multiplied by non-integers, which can often be problematic. According to this Wiki article , the problem can be circumvented by taking the average of $\delta^n[f](x-\frac{h}{2})$ and $\delta^n[f](x+\frac{h}{2})$ . What does that actually mean? Does this mean that I adjust the generalized formula from $f(x+(\frac{n}{2}-i)h)$ (only this part?) to e.g., $\delta^n[f](x-\frac{h}{2})$ ? And if so, $i$ is just a bookkeeping device. For example, for the third order difference, does it mean that I use the generalized formula for $i = \{0,2\}$ and the adjusted formula for $i = \{1,3\}$ ? I know, of course, that it is also possible to use the explicit formula for, e.g., the third derivative. However, I am interested in the application of the generalized expression.","I am interested in the -order central difference of an expression . The general form of the -order central difference is given by For odd , the central difference will have multiplied by non-integers, which can often be problematic. According to this Wiki article , the problem can be circumvented by taking the average of and . What does that actually mean? Does this mean that I adjust the generalized formula from (only this part?) to e.g., ? And if so, is just a bookkeeping device. For example, for the third order difference, does it mean that I use the generalized formula for and the adjusted formula for ? I know, of course, that it is also possible to use the explicit formula for, e.g., the third derivative. However, I am interested in the application of the generalized expression.","n^{\mathrm{th}} f n^{\mathrm{th}} \delta_h^n[f](x)=\sum_{i=0}^n(-1)^i {n \choose i}f(x+(\frac{n}{2}-i)h). n h \delta^n[f](x-\frac{h}{2}) \delta^n[f](x+\frac{h}{2}) f(x+(\frac{n}{2}-i)h) \delta^n[f](x-\frac{h}{2}) i i = \{0,2\} i = \{1,3\}","['discrete-mathematics', 'derivatives', 'numerical-methods', 'finite-differences']"
75,Is there an example of a convex differentiable function which is not continuously differentiable?,Is there an example of a convex differentiable function which is not continuously differentiable?,,"It is easy to see that any convex differentiable function of one variable is also continuous differentiable. But the proof is based on monotony of the derivative and doesn't work in multiple dimensions. So the question is, has any convex differentiable function of many variables to be also continuously differentiable? Or is there some counterexample? Thanks in advance.","It is easy to see that any convex differentiable function of one variable is also continuous differentiable. But the proof is based on monotony of the derivative and doesn't work in multiple dimensions. So the question is, has any convex differentiable function of many variables to be also continuously differentiable? Or is there some counterexample? Thanks in advance.",,"['real-analysis', 'derivatives', 'convex-analysis']"
76,Cauchy's MVT-Lagrange's MVT-Rolle's theorem independence,Cauchy's MVT-Lagrange's MVT-Rolle's theorem independence,,"In many textbooks, the former two have been proved with the help of Rolle's theorem. However my teacher(and many sites as well) say that Rolle's theorem is a special case of LMVT and Cauchy's is a generalization. Can we prove Cauchy's MVT and LMVT without using Rolle's theorem? If not, should we admit that these are just applications of Rolle's theorem and hence yield no extra result? $\mathcal{Remark}$ I found an analogy which could be useful: Suppose a car is travelling at an average speed of 40 miles/hr. In the course of the travel, it has to, at some point, travel at exactly 40 miles an hour. This is exactly what LMVT has to say.","In many textbooks, the former two have been proved with the help of Rolle's theorem. However my teacher(and many sites as well) say that Rolle's theorem is a special case of LMVT and Cauchy's is a generalization. Can we prove Cauchy's MVT and LMVT without using Rolle's theorem? If not, should we admit that these are just applications of Rolle's theorem and hence yield no extra result? I found an analogy which could be useful: Suppose a car is travelling at an average speed of 40 miles/hr. In the course of the travel, it has to, at some point, travel at exactly 40 miles an hour. This is exactly what LMVT has to say.",\mathcal{Remark},"['calculus', 'derivatives']"
77,Differentiation of an integral depending on a parameter,Differentiation of an integral depending on a parameter,,"Let $f(t):=\int_0^{\pi/2} \arccos\frac{t-\tan^2x}{t+\tan^2x}\,dx$ , for $0\leq t\leq 1$ . I would like to differentiate $f$ with respect to $t$ by taking the partial of the integrand: $$ f'(t)  = \int_0^{\pi/2}\frac{\partial}{\partial t}    \left(\arccos\frac{t-\tan^2x}{t+\tan^2x}\right)\,dx  = -\int_0^{\pi/2} \frac{1}{\sqrt{t}}                    \frac{\tan x}{t+\tan^2x}\,dx. $$ I am not sure to be able to fully justify this step, in particular because $x$ can approach $\frac{\pi}2$ (from the left), where $\tan x\rightarrow +\infty$ , and $t$ can approach $0$ (from the right), where $\frac 1{\sqrt{t}}\rightarrow +\infty$ . Am I allowed to do this differentiation under the integral sign? I also would like to be pointed to some reference about differentiation under the integral sign, for the Riemann integral. Any help would be very appreciated.","Let , for . I would like to differentiate with respect to by taking the partial of the integrand: I am not sure to be able to fully justify this step, in particular because can approach (from the left), where , and can approach (from the right), where . Am I allowed to do this differentiation under the integral sign? I also would like to be pointed to some reference about differentiation under the integral sign, for the Riemann integral. Any help would be very appreciated.","f(t):=\int_0^{\pi/2} \arccos\frac{t-\tan^2x}{t+\tan^2x}\,dx 0\leq t\leq 1 f t 
f'(t)
 = \int_0^{\pi/2}\frac{\partial}{\partial t}
   \left(\arccos\frac{t-\tan^2x}{t+\tan^2x}\right)\,dx
 = -\int_0^{\pi/2} \frac{1}{\sqrt{t}}
                   \frac{\tan x}{t+\tan^2x}\,dx.
 x \frac{\pi}2 \tan x\rightarrow +\infty t 0 \frac 1{\sqrt{t}}\rightarrow +\infty","['real-analysis', 'derivatives', 'definite-integrals']"
78,"Find all the errors, if any, in the following L'Hospital's rule argument","Find all the errors, if any, in the following L'Hospital's rule argument",,"Let $f(x)=e^{-2x}(\cos x+2\sin x)$ and $g(x) = e^{-x}(\cos x+ \sin x).$ Find all the errors (if any) in the following L'Hôpital's rule argument: $\lim\limits_{x\to \infty}\dfrac{f(x)}{g(x)}=\lim\limits_{x\to \infty}\dfrac{f'(x)}{g'(x)}=\lim\limits_{x\to \infty} \dfrac{5}{2}e^{-x}=0.$ Here's my work. Recall the requirements for L'Hôpital's Rule: To argue that $\lim\limits_{x\to c}\dfrac{f(x)}{g(x)}=\lim\limits_{x\to c}\dfrac{f'(x)}{g'(x)},$ the following must be true: $1.$ $f(x)$ and $g(x)$ are differentiable on an open interval $I,$ but not necessarily at some point $c.$ $2.$ $\lim\limits_{x\to c}f(x)=\lim\limits_{x\to c}g(x)=0$ or $\pm \infty.$ $3.$ $g'(x)\neq 0\;\forall x\in I, x\neq c.$ $4.$ $\lim\limits_{x\to c}\dfrac{f'(x)}{g'(x)}$ exists. We show that $f(x)$ and $g(x)$ are differentiable on $(-\infty, \infty).$ We have that $f'(x) = e^{-2x}(-2(\cos x+2\sin x) +(-\sin x+2\cos x))=-5e^{-2x}\sin x\;\forall x\in \mathbb{R}.$ Also, $g'(x)=e^{-x}(-(\cos x+\sin x)+(-\sin x+\cos x))= -2e^{-x}\sin x\;\forall x\in\mathbb{R}.$ Note that when $g(x)=0,\dfrac{f(x)}{g(x)}$ is undefined. This occurs when $\cos x + \sin x = 0\Rightarrow \tan x = -1\Rightarrow x = \dfrac{3\pi}{4}+2n\pi,n\in\mathbb{Z}.$ Let $x_0$ be such that $\tan x_0 = -1.$ We thus have that $f(x_0)=e^{-2x_0}(-\dfrac{\sqrt{2}}{2}+\sqrt{2})$ and $g(x_0)=0.$ Hence $\dfrac{f(x_0)}{g(x_0)}$ is indeterminate.  Also, consider when $x_1= \tan^{-1} \left(-\dfrac{1}{2}\right)+2n\pi.$ Then $\dfrac{f(x_1)}{g(x_1)}=\dfrac{e^{-2x_1}\left(\cos \left(\tan^{-1}\left(\dfrac{1}{2}\right)\right)-2\sin \left(\tan^{-1}\left(\dfrac{1}{2}\right)\right)\right)}{e^{-x_1}[\cos (\tan^{-1} (\frac{1}{2}))-\sin (\tan^{-1}(\frac{1}{2}))]}\\ =e^{-x_1}\dfrac{\frac{2}{\sqrt{5}}-\frac{2}{\sqrt{5}}}{\frac{2}{\sqrt{5}}-\frac{1}{\sqrt{5}}}=0.$ Hence $\dfrac{f(x)}{g(x)}$ is not indeterminate for all $x\in\mathbb{N}$ such that $x=\tan^{-1} (-\dfrac{1}{2})+2n\pi.$ Now consider $g'(x)=-2e^{-x}\sin x.$ $g'(x)=0$ whenever $\sin x=0$ as $e^{-x}\neq 0\;\forall x\in \mathbb{R}.$ Thus, $g'(x)=0\Leftrightarrow x=n\pi,n\in\mathbb{N}.$ So this is another error. From above, we have that $\lim\limits_{x\to \infty}\dfrac{f'(x)}{g'(x)}$ does not exist since it is undefined whenever $x=n\pi,n\in\mathbb{N}$ and equal to $\lim\limits_{x\to \infty}\dfrac{-5e^{-2x}\sin x}{-2e^{-x}\sin x}=\lim\limits_{x\to \infty}\dfrac{5}{2}e^{-x}=0$ whenever $x\neq n\pi.$","Let and Find all the errors (if any) in the following L'Hôpital's rule argument: Here's my work. Recall the requirements for L'Hôpital's Rule: To argue that the following must be true: and are differentiable on an open interval but not necessarily at some point or exists. We show that and are differentiable on We have that Also, Note that when is undefined. This occurs when Let be such that We thus have that and Hence is indeterminate.  Also, consider when Then Hence is not indeterminate for all such that Now consider whenever as Thus, So this is another error. From above, we have that does not exist since it is undefined whenever and equal to whenever","f(x)=e^{-2x}(\cos x+2\sin x) g(x) = e^{-x}(\cos x+ \sin x). \lim\limits_{x\to \infty}\dfrac{f(x)}{g(x)}=\lim\limits_{x\to \infty}\dfrac{f'(x)}{g'(x)}=\lim\limits_{x\to \infty} \dfrac{5}{2}e^{-x}=0. \lim\limits_{x\to c}\dfrac{f(x)}{g(x)}=\lim\limits_{x\to c}\dfrac{f'(x)}{g'(x)}, 1. f(x) g(x) I, c. 2. \lim\limits_{x\to c}f(x)=\lim\limits_{x\to c}g(x)=0 \pm \infty. 3. g'(x)\neq 0\;\forall x\in I, x\neq c. 4. \lim\limits_{x\to c}\dfrac{f'(x)}{g'(x)} f(x) g(x) (-\infty, \infty). f'(x) = e^{-2x}(-2(\cos x+2\sin x) +(-\sin x+2\cos x))=-5e^{-2x}\sin x\;\forall x\in \mathbb{R}. g'(x)=e^{-x}(-(\cos x+\sin x)+(-\sin x+\cos x))= -2e^{-x}\sin x\;\forall x\in\mathbb{R}. g(x)=0,\dfrac{f(x)}{g(x)} \cos x + \sin x = 0\Rightarrow \tan x = -1\Rightarrow x = \dfrac{3\pi}{4}+2n\pi,n\in\mathbb{Z}. x_0 \tan x_0 = -1. f(x_0)=e^{-2x_0}(-\dfrac{\sqrt{2}}{2}+\sqrt{2}) g(x_0)=0. \dfrac{f(x_0)}{g(x_0)} x_1= \tan^{-1} \left(-\dfrac{1}{2}\right)+2n\pi. \dfrac{f(x_1)}{g(x_1)}=\dfrac{e^{-2x_1}\left(\cos \left(\tan^{-1}\left(\dfrac{1}{2}\right)\right)-2\sin \left(\tan^{-1}\left(\dfrac{1}{2}\right)\right)\right)}{e^{-x_1}[\cos (\tan^{-1} (\frac{1}{2}))-\sin (\tan^{-1}(\frac{1}{2}))]}\\
=e^{-x_1}\dfrac{\frac{2}{\sqrt{5}}-\frac{2}{\sqrt{5}}}{\frac{2}{\sqrt{5}}-\frac{1}{\sqrt{5}}}=0. \dfrac{f(x)}{g(x)} x\in\mathbb{N} x=\tan^{-1} (-\dfrac{1}{2})+2n\pi. g'(x)=-2e^{-x}\sin x. g'(x)=0 \sin x=0 e^{-x}\neq 0\;\forall x\in \mathbb{R}. g'(x)=0\Leftrightarrow x=n\pi,n\in\mathbb{N}. \lim\limits_{x\to \infty}\dfrac{f'(x)}{g'(x)} x=n\pi,n\in\mathbb{N} \lim\limits_{x\to \infty}\dfrac{-5e^{-2x}\sin x}{-2e^{-x}\sin x}=\lim\limits_{x\to \infty}\dfrac{5}{2}e^{-x}=0 x\neq n\pi.",['calculus']
79,"Find $\frac{d \rho}{d x}$ for $\rho = \rho(t,x(t),p(t))$",Find  for,"\frac{d \rho}{d x} \rho = \rho(t,x(t),p(t))","I got a question relating to this thread difference between implicit, explicit, and total time dependence Considering the reply in the top by Kostya, I konw what is the difference between $\frac{\partial \rho}{\partial t}$ and $\frac{d \rho}{d t}$ . What I want to know is what is $\frac{d \rho}{d x}$ for a function $\rho = \rho(t,x(t),p(t))$ ? In my opinion, we have $$\frac{{d\rho }}{{dx}} = \frac{{\partial \rho }}{{\partial t}}\frac{{dt}}{{dx}} + \frac{{\partial \rho }}{{\partial x}}$$ , if $x$ and $p$ are independent variables. But if that is the case, I get another confusion about integration by parts in calculating an integration in calculate time evolution of ensemble average so i guess I got some misunderstanding","I got a question relating to this thread difference between implicit, explicit, and total time dependence Considering the reply in the top by Kostya, I konw what is the difference between and . What I want to know is what is for a function ? In my opinion, we have , if and are independent variables. But if that is the case, I get another confusion about integration by parts in calculating an integration in calculate time evolution of ensemble average so i guess I got some misunderstanding","\frac{\partial \rho}{\partial t} \frac{d \rho}{d t} \frac{d \rho}{d x} \rho = \rho(t,x(t),p(t)) \frac{{d\rho }}{{dx}} = \frac{{\partial \rho }}{{\partial t}}\frac{{dt}}{{dx}} + \frac{{\partial \rho }}{{\partial x}} x p","['calculus', 'derivatives']"
80,Finding the $18th$ Derivative of a Particular Product at $x = 0$,Finding the  Derivative of a Particular Product at,18th x = 0,"Let $f_{1}(x) = e^{x^5}$ and $f_{2}(x) = e^{x^3}$ . Let $g(x) = f_{1}f_{2}$ . Find $g^{(18)}(0)$ . By series expansion at $x = 0$ : $f_{1}(x) = \sum_{k \ge 0} {x^{5k} \over k! }$ and $f_{2}(x) = \sum_{m \ge 0}{x^{3m} \over {m!}}$ , then $$g(x) = \sum_{k, m \ge 0}{x^{5k + 3m} \over {m!k!}}.$$ Substituting $5k + 3m = n$ we get $g(x) = \sum_{n \ge 0} \left( \sum_{5k + 3m = n}{1 \over {m!k!}} \right) x^{n} $ . Solving diophantine equation $5k + 3m = 18$ , there are two ordered pairs of non - negative integers $(k, m)$ : $(3, 1), (0, 6)$ . Thus, $g^{18}(0) = 18! \left[ { {1 \over {3!1!}} + {1 \over {0!6!}}} \right].$ Is there a general method for finding $n^{th}$ derivative of functions $\prod_{1 \le i \le n}f_{i}$ ? Obviously, if there are no solutions then a derivative of a function at some point will be $0$ . But what can be said when there are  infinitely many solutions? UPD: 01.08.2019 Consider function $f(x) = e^{1 \over 1 - x}$ . Then by expansion at 0: $$f(x) = e\sum_{n \ge 0} \sum_{x_{1} + 2x_{2} + \cdots  = n} {{1} \over {x_{1}!x_{2}!\cdots}} x^{n},$$ which gives an infinite diophantine equation. More general, it can be applied to functions of a form: $f(x)^{g(x)}.$ Referring  to my early question, what can be said about a derivative at $x = 0$ of a such function?","Let and . Let . Find . By series expansion at : and , then Substituting we get . Solving diophantine equation , there are two ordered pairs of non - negative integers : . Thus, Is there a general method for finding derivative of functions ? Obviously, if there are no solutions then a derivative of a function at some point will be . But what can be said when there are  infinitely many solutions? UPD: 01.08.2019 Consider function . Then by expansion at 0: which gives an infinite diophantine equation. More general, it can be applied to functions of a form: Referring  to my early question, what can be said about a derivative at of a such function?","f_{1}(x) = e^{x^5} f_{2}(x) = e^{x^3} g(x) = f_{1}f_{2} g^{(18)}(0) x = 0 f_{1}(x) = \sum_{k \ge 0} {x^{5k} \over k! } f_{2}(x) = \sum_{m \ge 0}{x^{3m} \over {m!}} g(x) = \sum_{k, m \ge 0}{x^{5k + 3m} \over {m!k!}}. 5k + 3m = n g(x) = \sum_{n \ge 0} \left( \sum_{5k + 3m = n}{1 \over {m!k!}} \right) x^{n}  5k + 3m = 18 (k, m) (3, 1), (0, 6) g^{18}(0) = 18! \left[ { {1 \over {3!1!}} + {1 \over {0!6!}}} \right]. n^{th} \prod_{1 \le i \le n}f_{i} 0 f(x) = e^{1 \over 1 - x} f(x) = e\sum_{n \ge 0} \sum_{x_{1} + 2x_{2} + \cdots  = n} {{1} \over {x_{1}!x_{2}!\cdots}} x^{n}, f(x)^{g(x)}. x = 0","['derivatives', 'solution-verification', 'power-series', 'diophantine-equations']"
81,Prove $ \frac{d^n}{dx^n}\ln(x)=\frac{(n-1)!(-1)^{n-1}}{x^n} $ by induction,Prove  by induction, \frac{d^n}{dx^n}\ln(x)=\frac{(n-1)!(-1)^{n-1}}{x^n} ,Prove $$ \frac{d^n}{dx^n}\ln(x)=\frac{(n-1)!(-1)^{n-1}}{x^n} $$ by induction. Attempt to solve Base case $n=1$ $$ \frac{d}{dx}\ln(x)=\frac{(1-1)!(-1)^{1-1}}{x^{1}}=\frac{1}{x} $$ which is true. Induction step Induction hypothesis: equation is true when $n=k$ $$ \frac{d^k}{dx^k}\ln(x)=\frac{(k-1)!(-1)^{k-1}}{x^k} $$ Induction conjecture: when $n=k+1$ $$ \frac{d^{k+1}}{dx^{k+1}} \ln(x) = \frac{(k+1-1)!(-1)^{k+1-1}}{x^{k+1}} $$ Proof of conjecture: By utilizing induction hypothesis: $$ \frac{d^{k+1}}{dx^{k+1}} \ln(x) = \frac{d}{dx} \frac{(k-1)!(-1)^{k-1}}{x^k}$$ $$ =\frac{d}{dx}(k-1)!(-1)^{k-1}x^{-k} $$ $$ = ((k-1)!(-1)^{k-1})(\frac{d}{dx}x^{-k}) $$ $$ =  ((k-1)!(-1)^{k-1})(-kx^{-(k+1)})$$ $$ = \frac{ -k(k-1)!(-1)^{k-1} }{ x^{k+1} } $$ Not quite sure if this is correct since not getting to the desired end result ? which should be: $$= \frac{(k+1-1)!(-1)^{k+1-1}}{x^{k+1}}$$,Prove by induction. Attempt to solve Base case which is true. Induction step Induction hypothesis: equation is true when Induction conjecture: when Proof of conjecture: By utilizing induction hypothesis: Not quite sure if this is correct since not getting to the desired end result ? which should be:, \frac{d^n}{dx^n}\ln(x)=\frac{(n-1)!(-1)^{n-1}}{x^n}  n=1  \frac{d}{dx}\ln(x)=\frac{(1-1)!(-1)^{1-1}}{x^{1}}=\frac{1}{x}  n=k  \frac{d^k}{dx^k}\ln(x)=\frac{(k-1)!(-1)^{k-1}}{x^k}  n=k+1  \frac{d^{k+1}}{dx^{k+1}} \ln(x) = \frac{(k+1-1)!(-1)^{k+1-1}}{x^{k+1}}   \frac{d^{k+1}}{dx^{k+1}} \ln(x) = \frac{d}{dx} \frac{(k-1)!(-1)^{k-1}}{x^k}  =\frac{d}{dx}(k-1)!(-1)^{k-1}x^{-k}   = ((k-1)!(-1)^{k-1})(\frac{d}{dx}x^{-k})   =  ((k-1)!(-1)^{k-1})(-kx^{-(k+1)})  = \frac{ -k(k-1)!(-1)^{k-1} }{ x^{k+1} }  = \frac{(k+1-1)!(-1)^{k+1-1}}{x^{k+1}},"['derivatives', 'proof-verification', 'induction']"
82,Product rule proof. Derivatives.,Product rule proof. Derivatives.,,"I've been asked to proof the product rule.  I am fine with that, my proof was accepted completely. Nevertheless, for me personally the very last step seems to be not totally incorrect, but, let say, ""intuitive"" or ""inductive"" to some extend.  Not to confuse people, will mention few fateful points of my reasoning: At some moment I've proved that: $$f(x + \delta) = A + B \times \delta + o(E(\delta))$$ where: $o(E(\delta))$ means ""of less order than $\delta$"", i.e. $\lim_{\delta \to 0}\frac{E(\delta)}{\delta} = 0$; $A = f(x)$; $B = f'(x)$. After that I took both $f(x + \delta)$ and $g(x + \delta)$ assumed to be differentiable at the point $x$. Hence: $$f(x + \delta) \times g(x + \delta) = [f(x) + f'(x)\delta + o(E(\delta))] \times [g(x) + g'(x)\delta + o(E(\delta))]$$ $$= f(x)g(x) + [f(x)g'(x) + g(x)f'(x)] \times \delta + \biggl( [f(x) + f'(x)\delta]o(E(\delta)) + [g(x) + g'(x)\delta]o(E(\delta)) + [o(E(\delta))]^2\biggr)$$ HERE GOES MY QUESTION: I've noticed that the last equation could be reformulated as: $$A = f(x)g(x)$$ $$B = f(x)g'(x) + g(x)f'(x)$$ $$o(E(\delta)) = \biggl( [f(x) + f'(x)\delta]o(E(\delta)) + [g(x) + g'(x)\delta]o(E(\delta)) + [o(E(\delta))]^2\biggr)$$ The question is: shouldn't I proof somehow (have no idea how, actually) that the pattern above is applicable here? Last step seems to be more of blinded guessing rather than an undeniable logical deduction...","I've been asked to proof the product rule.  I am fine with that, my proof was accepted completely. Nevertheless, for me personally the very last step seems to be not totally incorrect, but, let say, ""intuitive"" or ""inductive"" to some extend.  Not to confuse people, will mention few fateful points of my reasoning: At some moment I've proved that: $$f(x + \delta) = A + B \times \delta + o(E(\delta))$$ where: $o(E(\delta))$ means ""of less order than $\delta$"", i.e. $\lim_{\delta \to 0}\frac{E(\delta)}{\delta} = 0$; $A = f(x)$; $B = f'(x)$. After that I took both $f(x + \delta)$ and $g(x + \delta)$ assumed to be differentiable at the point $x$. Hence: $$f(x + \delta) \times g(x + \delta) = [f(x) + f'(x)\delta + o(E(\delta))] \times [g(x) + g'(x)\delta + o(E(\delta))]$$ $$= f(x)g(x) + [f(x)g'(x) + g(x)f'(x)] \times \delta + \biggl( [f(x) + f'(x)\delta]o(E(\delta)) + [g(x) + g'(x)\delta]o(E(\delta)) + [o(E(\delta))]^2\biggr)$$ HERE GOES MY QUESTION: I've noticed that the last equation could be reformulated as: $$A = f(x)g(x)$$ $$B = f(x)g'(x) + g(x)f'(x)$$ $$o(E(\delta)) = \biggl( [f(x) + f'(x)\delta]o(E(\delta)) + [g(x) + g'(x)\delta]o(E(\delta)) + [o(E(\delta))]^2\biggr)$$ The question is: shouldn't I proof somehow (have no idea how, actually) that the pattern above is applicable here? Last step seems to be more of blinded guessing rather than an undeniable logical deduction...",,"['calculus', 'derivatives', 'proof-verification', 'proof-writing']"
83,Modify discontinuous function to compute derivative,Modify discontinuous function to compute derivative,,"I'm creating a machine learning model to beat the house in a competitive game. In this setup, there is a lot of past data on previous games, where each game has: a winner, w (either A or B), gambling odds (x and y for teams A and B), and a probability that team A wins, p, which I compute. The gambling odds are how much reward I get after winning a bet. For example, if team A is projected by the house to have a much better chance of winning, they may set x=1.05, and y=7.5. If I bet \$1 on B and win, I get a net profit of \$7.5-\$1=\$6.5. If I bet \$1 on A or B and lose, then I get a net profit of -\$1. I can also choose to not bet for a net profit of \$0. The gambling odds are always greater than or equal to 1.0. I'm on mobile, so I can't latex this, but here's a drawing of the objective function I'm trying to maximize for each game Since the probability model is a neural network, I need the derivative of f wrt p, but that isn't defined because f is not continuous wrt p. Alternatively, I could just do cross entropy loss on p, but I don't have access to the actual probabilities, just the winner w, and I can't guarantee that the distribution created is therefore good against the house. Is there a way to modify my objective function that allows me to obtain the desired partial derivative? If not, what else can I do?","I'm creating a machine learning model to beat the house in a competitive game. In this setup, there is a lot of past data on previous games, where each game has: a winner, w (either A or B), gambling odds (x and y for teams A and B), and a probability that team A wins, p, which I compute. The gambling odds are how much reward I get after winning a bet. For example, if team A is projected by the house to have a much better chance of winning, they may set x=1.05, and y=7.5. If I bet \$1 on B and win, I get a net profit of \$7.5-\$1=\$6.5. If I bet \$1 on A or B and lose, then I get a net profit of -\$1. I can also choose to not bet for a net profit of \$0. The gambling odds are always greater than or equal to 1.0. I'm on mobile, so I can't latex this, but here's a drawing of the objective function I'm trying to maximize for each game Since the probability model is a neural network, I need the derivative of f wrt p, but that isn't defined because f is not continuous wrt p. Alternatively, I could just do cross entropy loss on p, but I don't have access to the actual probabilities, just the winner w, and I can't guarantee that the distribution created is therefore good against the house. Is there a way to modify my objective function that allows me to obtain the desired partial derivative? If not, what else can I do?",,"['calculus', 'derivatives', 'machine-learning', 'continuity']"
84,The Continuous Differential Operator,The Continuous Differential Operator,,"I was playing around a bit this morning until I realized that one could write the arbitrary derivative of $\cos x$ as $$(\cos x)^{(n)} = \cos\big(\frac{\pi}{2}(n-1) \big)\sin x + \cos\big(\frac{\pi}{2}n\big)\cos x$$ Similarly, writing $$\sin x=\cos \left(x-\frac{\pi}{2}\right)$$ we see that $$\frac{d^n}{dx^n}\sin x = \frac{d^n}{dx^n} \cos \left(x-\frac{\pi}{2}\right)$$ which evaluates to (I think I'm using the chain rule properly in this case): $$\cos\big(\frac{\pi}{2}(n-1) \big)\sin \left(x-\frac{\pi}{2}\right) + \cos\big(\frac{\pi}{2}n\big)\cos\left(x-\frac{\pi}{2}\right)$$ simplifying $$(\sin x)^{(n)} = \cos\big(\frac{\pi}{2}(n-1) \big)\cos x + \cos\big(\frac{\pi}{2}n\big)\sin x$$ where we have that $n \in \mathbb{R}$ These two results can be expressed as a complex exponential too. Because these functions happens to be continuous, in some ways the differential operator is now continuous for cosine and sine. I'm wondering if anyone can interpret this for me a I haven't studying real analysis and only have a nonrigorous calculus background. Question After doing some reading, I realized that because every function can be written as a Fourier transform, one ought to be able to simply use the above definitions to take the nth derivative of the fourier transform of an arbitrary function to an arbitrary degree of precision (by choosing how many terms to include). Is the hypothesis true? If so, can you summarize it in a way I would be able to understand and use. Assume I know the definition of the fourier transform. All I am looking for now is a general equation for the continuous differential operator of a function.","I was playing around a bit this morning until I realized that one could write the arbitrary derivative of $\cos x$ as $$(\cos x)^{(n)} = \cos\big(\frac{\pi}{2}(n-1) \big)\sin x + \cos\big(\frac{\pi}{2}n\big)\cos x$$ Similarly, writing $$\sin x=\cos \left(x-\frac{\pi}{2}\right)$$ we see that $$\frac{d^n}{dx^n}\sin x = \frac{d^n}{dx^n} \cos \left(x-\frac{\pi}{2}\right)$$ which evaluates to (I think I'm using the chain rule properly in this case): $$\cos\big(\frac{\pi}{2}(n-1) \big)\sin \left(x-\frac{\pi}{2}\right) + \cos\big(\frac{\pi}{2}n\big)\cos\left(x-\frac{\pi}{2}\right)$$ simplifying $$(\sin x)^{(n)} = \cos\big(\frac{\pi}{2}(n-1) \big)\cos x + \cos\big(\frac{\pi}{2}n\big)\sin x$$ where we have that $n \in \mathbb{R}$ These two results can be expressed as a complex exponential too. Because these functions happens to be continuous, in some ways the differential operator is now continuous for cosine and sine. I'm wondering if anyone can interpret this for me a I haven't studying real analysis and only have a nonrigorous calculus background. Question After doing some reading, I realized that because every function can be written as a Fourier transform, one ought to be able to simply use the above definitions to take the nth derivative of the fourier transform of an arbitrary function to an arbitrary degree of precision (by choosing how many terms to include). Is the hypothesis true? If so, can you summarize it in a way I would be able to understand and use. Assume I know the definition of the fourier transform. All I am looking for now is a general equation for the continuous differential operator of a function.",,"['trigonometry', 'derivatives', 'fourier-analysis']"
85,Question from applications of derivatives.,Question from applications of derivatives.,,Prove that the least perimeter of an isoceles triangle in which a circle of radius $r$ can be inscribed is $6r\sqrt3$. I have seen answer online on two sites. One is on meritnation but the problem is that answer is difficult and bad formatting. Other answer on topperlearning but that answer make uses of trigonometric functions. And I want to solve it without trignometric function. So please can someone provide easy method.,Prove that the least perimeter of an isoceles triangle in which a circle of radius $r$ can be inscribed is $6r\sqrt3$. I have seen answer online on two sites. One is on meritnation but the problem is that answer is difficult and bad formatting. Other answer on topperlearning but that answer make uses of trigonometric functions. And I want to solve it without trignometric function. So please can someone provide easy method.,,"['calculus', 'derivatives', 'inequality', 'circles', 'geometric-inequalities']"
86,Examples of interesting and creative problems about differentiation (in one variable),Examples of interesting and creative problems about differentiation (in one variable),,"Problems about differentiation of functions in one variable that we find in the majority of textbooks are usually boring, that is, they are only a simple application of very known rules. So, what I want in this post is examples of derivatives (functions of one variable) that are interesting to take. I'd like that the problems were original, but if they aren't, feel free to share the same way.","Problems about differentiation of functions in one variable that we find in the majority of textbooks are usually boring, that is, they are only a simple application of very known rules. So, what I want in this post is examples of derivatives (functions of one variable) that are interesting to take. I'd like that the problems were original, but if they aren't, feel free to share the same way.",,"['calculus', 'derivatives', 'examples-counterexamples', 'big-list']"
87,Understanding a particular proof of the derivative of $e^x$,Understanding a particular proof of the derivative of,e^x,"There are probably more efficient and easier proofs for same thing. This is proof I have to study for my exam. Theorem: $(e^x)'=e^x$ Proof: For $x\ge 0$ we have defined $f_0(x)=\lim_{n\to +\infty}\left(1+\frac{x}{n}\right)^n.$ Exponential function is now defined by $$ f(x)=e^x= \begin{cases} f_0(x),  & x\ge0 \\[4pt] \dfrac{1}{f_0(-x)}, & x\lt0 \end{cases}$$ For $x,c\gt0$ is $$\frac{\left(1+\frac{x}{n}\right)^n-\left(1+\frac{c}{n}\right)^n}{x-c}=\frac{1}{n}\left[\left(1+\frac{x}{n}\right)^{n-1}+\left(1+\frac{x}{n}\right)^{n-2}\left(1+\frac{c}{n}\right)+\cdots+\left(1+\frac{x}{n}\right)\left(1+\frac{c}{n}\right)^{n-2}+\left(1+\frac{c}{n}\right)^{n-1}\right]$$ We have $$\left(1+\frac{\min{(x,c)}}{n}\right)^{n-1} \le \frac{\left(1+\frac{x}{n}\right)^n-\left(1+\frac{c}{n}\right)^n}{x-c}\le  \left(1+\frac{\max{(x,c)}}{n}\right)^{n-1}$$ For $n \rightarrow +\infty$ we have $$f_0(\min{(x,c))} \le \frac{f_0(x)-f_0(c)}{x-c} \le f_0(\max{(x,c))}$$ Functions $x \rightarrow \max{(x,c)}$ and $x \rightarrow \min{(x,c)}$ are continuous so for $x \rightarrow c$ is $$f_0^{'}(c) = \lim_{x\to c} \frac{f_0(x)-f_0(c)}{x-c} = f_0(c)$$ Now , for $ x\lt 0$ we have $f(x)=\frac{1}{f_0(-x)}$ , so$$ f'(x)=-\frac{f_0^{'}(-x)(-1)}{[f_0(-x)]^2}=\frac{f_0(-x)}{[f_0(-x)]^2}=\frac{1}{f_0(-x)}=f(x)$$ In case $c=0$ we have to separately look at left and right limit of $\frac{f(x)-1}{x}$. For $x\gt 0$ we have $1\le \frac{f_0(x)-1}{x} \le f_0(x)$, because of continuity of $f_0$ in $0$ and $f_0(0) = 1$ we have $$ \lim_{x\to 0+} \frac{f(x)-1}{x}=1=f(0) $$ For $x<0$ we have $$\lim_{x\to 0-} \frac{f(x)-1}{x} = \lim_{x\to 0-} \frac{\frac{1}{f_0(-x)}-1}{x} = \lim_{-x\to 0+}\frac{f_0(-x)-1}{-x} \frac{1}{\lim_{-x\to 0+}f_0(-x)}=1=f(0)$$ Thus, $f'(0)=f(0)$. So, for $\forall x \in \mathbb{R}$ is $(e^x)'=e^x$. I don't understand couple of things about this proof: 1. How does $(1+\frac{\min{(x,c)}}{n})^{n-1}$ go to $f_0(\min{x,c})$. We haven't got exponent $n$, but $n-1$? 2. Why is this valid: $ 1\le \frac{f_0(x)-1}{x} \le f_0(x)$ for $x\gt0$ 3. Why is this valid : $\lim_{x\to 0-} \frac{\frac{1}{f_0(-x)}-1}{x} = \lim_{-x\to 0+}\frac{f_0(-x)-1}{-x} \frac{1}{\lim_{-x\to 0+}f_0(-x)}=1$ And one personal question, what do you think about this proof, and did your teacher in college requested something similar for you to know for exam?","There are probably more efficient and easier proofs for same thing. This is proof I have to study for my exam. Theorem: $(e^x)'=e^x$ Proof: For $x\ge 0$ we have defined $f_0(x)=\lim_{n\to +\infty}\left(1+\frac{x}{n}\right)^n.$ Exponential function is now defined by $$ f(x)=e^x= \begin{cases} f_0(x),  & x\ge0 \\[4pt] \dfrac{1}{f_0(-x)}, & x\lt0 \end{cases}$$ For $x,c\gt0$ is $$\frac{\left(1+\frac{x}{n}\right)^n-\left(1+\frac{c}{n}\right)^n}{x-c}=\frac{1}{n}\left[\left(1+\frac{x}{n}\right)^{n-1}+\left(1+\frac{x}{n}\right)^{n-2}\left(1+\frac{c}{n}\right)+\cdots+\left(1+\frac{x}{n}\right)\left(1+\frac{c}{n}\right)^{n-2}+\left(1+\frac{c}{n}\right)^{n-1}\right]$$ We have $$\left(1+\frac{\min{(x,c)}}{n}\right)^{n-1} \le \frac{\left(1+\frac{x}{n}\right)^n-\left(1+\frac{c}{n}\right)^n}{x-c}\le  \left(1+\frac{\max{(x,c)}}{n}\right)^{n-1}$$ For $n \rightarrow +\infty$ we have $$f_0(\min{(x,c))} \le \frac{f_0(x)-f_0(c)}{x-c} \le f_0(\max{(x,c))}$$ Functions $x \rightarrow \max{(x,c)}$ and $x \rightarrow \min{(x,c)}$ are continuous so for $x \rightarrow c$ is $$f_0^{'}(c) = \lim_{x\to c} \frac{f_0(x)-f_0(c)}{x-c} = f_0(c)$$ Now , for $ x\lt 0$ we have $f(x)=\frac{1}{f_0(-x)}$ , so$$ f'(x)=-\frac{f_0^{'}(-x)(-1)}{[f_0(-x)]^2}=\frac{f_0(-x)}{[f_0(-x)]^2}=\frac{1}{f_0(-x)}=f(x)$$ In case $c=0$ we have to separately look at left and right limit of $\frac{f(x)-1}{x}$. For $x\gt 0$ we have $1\le \frac{f_0(x)-1}{x} \le f_0(x)$, because of continuity of $f_0$ in $0$ and $f_0(0) = 1$ we have $$ \lim_{x\to 0+} \frac{f(x)-1}{x}=1=f(0) $$ For $x<0$ we have $$\lim_{x\to 0-} \frac{f(x)-1}{x} = \lim_{x\to 0-} \frac{\frac{1}{f_0(-x)}-1}{x} = \lim_{-x\to 0+}\frac{f_0(-x)-1}{-x} \frac{1}{\lim_{-x\to 0+}f_0(-x)}=1=f(0)$$ Thus, $f'(0)=f(0)$. So, for $\forall x \in \mathbb{R}$ is $(e^x)'=e^x$. I don't understand couple of things about this proof: 1. How does $(1+\frac{\min{(x,c)}}{n})^{n-1}$ go to $f_0(\min{x,c})$. We haven't got exponent $n$, but $n-1$? 2. Why is this valid: $ 1\le \frac{f_0(x)-1}{x} \le f_0(x)$ for $x\gt0$ 3. Why is this valid : $\lim_{x\to 0-} \frac{\frac{1}{f_0(-x)}-1}{x} = \lim_{-x\to 0+}\frac{f_0(-x)-1}{-x} \frac{1}{\lim_{-x\to 0+}f_0(-x)}=1$ And one personal question, what do you think about this proof, and did your teacher in college requested something similar for you to know for exam?",,"['real-analysis', 'derivatives', 'exponential-function']"
88,"Finding all $f:[0, \infty) \to [0, \infty)$ differentiable and convex with $f(0)=0$ and $f'(x)\cdot f\bigl(f(x)\bigr)=x$",Finding all  differentiable and convex with  and,"f:[0, \infty) \to [0, \infty) f(0)=0 f'(x)\cdot f\bigl(f(x)\bigr)=x","Find all functions $f:[0, \infty) \to [0, \infty)$ , differentiable and convex, such that $$f(0)=0 \tag1\label1$$ and $$ \ f'(x)\cdot f\bigl(f(x)\bigr)=x, \forall x \tag2\label2$$ Obviously, $f(x)=x$ is a solution, so I'm trying to find other solutions. From \eqref{2} we get $f(x) \gt 0, \ f'(x) \gt 0, \forall x \gt 0$ and $f'(0)=0$ therefore $f$ is strictly increasing. So far, I don't know how to use the convexity of $f$ , the definition of convexity doesn't seem to help. UPDATE: From \eqref{2} $f'(x)=\dfrac x{f\bigl(f(x)\bigr)}, \ \forall x \gt 0$ therefore $f$ is twice differentiable on $(0, \infty)$ .","Find all functions , differentiable and convex, such that and Obviously, is a solution, so I'm trying to find other solutions. From \eqref{2} we get and therefore is strictly increasing. So far, I don't know how to use the convexity of , the definition of convexity doesn't seem to help. UPDATE: From \eqref{2} therefore is twice differentiable on .","f:[0, \infty) \to [0, \infty) f(0)=0 \tag1\label1  \ f'(x)\cdot f\bigl(f(x)\bigr)=x, \forall x \tag2\label2 f(x)=x f(x) \gt 0, \ f'(x) \gt 0, \forall x \gt 0 f'(0)=0 f f f'(x)=\dfrac x{f\bigl(f(x)\bigr)}, \ \forall x \gt 0 f (0, \infty)",['derivatives']
89,Swapping limits: $\lim_{h\to 0}\lim_{n\to \infty}\frac {(1+1/n)^{hn}-1}{h}=\lim_{n\to\infty}\lim_{h\to 0}\frac {(1+1/n)^{hn}-1}{h}$,Swapping limits:,\lim_{h\to 0}\lim_{n\to \infty}\frac {(1+1/n)^{hn}-1}{h}=\lim_{n\to\infty}\lim_{h\to 0}\frac {(1+1/n)^{hn}-1}{h},"Almost a year ago I asked the question: How to differentiate $e^x$? And in the accepted answer, the following equality appeared: $$\lim_{h\to 0}\lim_{n\to \infty}\frac {(1+1/n)^{hn}-1}{h}=\lim_{n\to\infty}\lim_{h\to 0}\frac {(1+1/n)^{hn}-1}{h}$$ What allows one to switch limits in this case?","Almost a year ago I asked the question: How to differentiate $e^x$? And in the accepted answer, the following equality appeared: $$\lim_{h\to 0}\lim_{n\to \infty}\frac {(1+1/n)^{hn}-1}{h}=\lim_{n\to\infty}\lim_{h\to 0}\frac {(1+1/n)^{hn}-1}{h}$$ What allows one to switch limits in this case?",,"['real-analysis', 'derivatives', 'exponential-function']"
90,Relationship between $\sin(a+b)$ and derivative product rule?,Relationship between  and derivative product rule?,\sin(a+b),I noticed this interesting correlation between the sine angle addition formula and the derivative product rule. The sine addition formula is $$\sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)$$ The derivative product rule is $$(f(x)g(x))'=f'(x)g(x)+g'(x)f(x)$$ As many of you probably know the derivative of $\sin$ is $\cos$ so the sine addition formula could be rewritten as $$\sin(a+b)=\sin(a)\sin'(b)+\sin(b)\sin'(a)$$ I was wondering if there was any reason for this correlation between the two. I understand that the sine angle addition formula is about taking the sine of two different angles and that the derivative product rule is about multiplying two different functions and finding the derivative so there is very little relation between the two. Perhaps this is just a coincidence?,I noticed this interesting correlation between the sine angle addition formula and the derivative product rule. The sine addition formula is $$\sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)$$ The derivative product rule is $$(f(x)g(x))'=f'(x)g(x)+g'(x)f(x)$$ As many of you probably know the derivative of $\sin$ is $\cos$ so the sine addition formula could be rewritten as $$\sin(a+b)=\sin(a)\sin'(b)+\sin(b)\sin'(a)$$ I was wondering if there was any reason for this correlation between the two. I understand that the sine angle addition formula is about taking the sine of two different angles and that the derivative product rule is about multiplying two different functions and finding the derivative so there is very little relation between the two. Perhaps this is just a coincidence?,,"['calculus', 'trigonometry', 'derivatives']"
91,"A concave positive function on $[1,\infty)$ is uniformly continuous",A concave positive function on  is uniformly continuous,"[1,\infty)","Let $f$ be a concave positive function on $[1,\infty)$, then $f$ is uniformly continuous on $[1,\infty)$. This was a true or false problem that I couldn't prove to be true, so I'm thinking that maybe there is a counterexample. I know that for $f$ to be concave then $f''(x)\lt 0$ on $[1,\infty)$. Does $f(x)=\frac{1}{x-2}$ work as a counterexample? $f''(x)=-1\lt 0$ and at $x=2$ it is undefined so it wouldn't be uniformly continuous on the interval $[1,\infty)$, right? Uniformly Continuous: Let $E$ be a nonempty subset of $\mathbb R$ and $f:E\to \mathbb R$. The $f$ is uniformly continous on $E$ if and only if for every $\epsilon \gt 0$ there is a $\delta\gt 0$ such that $|x-a|\lt\delta$ and $x,a\in E$ imply $|f(x)-f(a)|\lt \epsilon$.","Let $f$ be a concave positive function on $[1,\infty)$, then $f$ is uniformly continuous on $[1,\infty)$. This was a true or false problem that I couldn't prove to be true, so I'm thinking that maybe there is a counterexample. I know that for $f$ to be concave then $f''(x)\lt 0$ on $[1,\infty)$. Does $f(x)=\frac{1}{x-2}$ work as a counterexample? $f''(x)=-1\lt 0$ and at $x=2$ it is undefined so it wouldn't be uniformly continuous on the interval $[1,\infty)$, right? Uniformly Continuous: Let $E$ be a nonempty subset of $\mathbb R$ and $f:E\to \mathbb R$. The $f$ is uniformly continous on $E$ if and only if for every $\epsilon \gt 0$ there is a $\delta\gt 0$ such that $|x-a|\lt\delta$ and $x,a\in E$ imply $|f(x)-f(a)|\lt \epsilon$.",,"['real-analysis', 'derivatives', 'examples-counterexamples', 'uniform-continuity']"
92,Finding the derivative of $f(x) = \frac{8}{\sqrt{x -2}}$ using first principles.,Finding the derivative of  using first principles.,f(x) = \frac{8}{\sqrt{x -2}},"How would you go about determining the derivative of ( $f(x) = \frac{8}{\sqrt{x -2}}$ ) using the limit definition of the derivative  (i.e. $\lim\limits_{h\to 0} = \frac{f(x+h) - f(x)}{h}$) as opposed to just applying the chain rule. So I'm thinking this counts as an algebra question but can't find too many examples dealing with simplification of polynomial expressions with fractional exponents.  I got as far as,  $\frac {1}{h}  (\frac{8}{\sqrt{x+h-2}} - \frac{8}{\sqrt{x-2}})$ In case I'm not using correct terminology or being unclear, what I mean is, can one algebraically eliminate the $h$ from the denominator in the above expression to take the value of the limit?","How would you go about determining the derivative of ( $f(x) = \frac{8}{\sqrt{x -2}}$ ) using the limit definition of the derivative  (i.e. $\lim\limits_{h\to 0} = \frac{f(x+h) - f(x)}{h}$) as opposed to just applying the chain rule. So I'm thinking this counts as an algebra question but can't find too many examples dealing with simplification of polynomial expressions with fractional exponents.  I got as far as,  $\frac {1}{h}  (\frac{8}{\sqrt{x+h-2}} - \frac{8}{\sqrt{x-2}})$ In case I'm not using correct terminology or being unclear, what I mean is, can one algebraically eliminate the $h$ from the denominator in the above expression to take the value of the limit?",,"['calculus', 'derivatives']"
93,Where did I go wrong on trying to solve this question on an exam?,Where did I go wrong on trying to solve this question on an exam?,,"I took an exam yesterday, and I almost for a fact know I got this question wrong. I couldn't figure it out, since my answer wasn't an answer choice, so I ended up guessing. An explanation of what I did wrong and how to properly solve this would be great appreciated! The question:  Evaluate $f'(\frac{\pi}{6})$, where $f(x)=\tan^{-1}(\sin 2x)$. So these are the steps that I did. First using chain rule$$\frac{d}{dx}\tan^{-1}(\sin 2x)=\frac{1}{1+(\sin 2x)^2}\cdot\frac{d}{dx}\sin(2x)$$ $$\frac{d}{dx}\sin(2x)=\cos(2x)\cdot2$$so $$\frac{d}{dx}\tan^{-1}(\sin 2x)=\frac{1}{1+(\sin 2x)^2}*2\cos(2x)$$ Plugging in $(\frac{\pi}{6})$ I ended with $$$$ $$\frac{d}{dx}\tan^{-1}\left(\sin\frac{\pi}{3}\right)=\frac{1}{1+(\sin\frac{\pi}{3})^2}\cdot2\cos\left(\frac{\pi}{3}\right)$$I ended with some answer that didn't match up with any of the answer choices, did I go about solving this wrong? If so, how would I solve it correctly?  Thanks in advance.","I took an exam yesterday, and I almost for a fact know I got this question wrong. I couldn't figure it out, since my answer wasn't an answer choice, so I ended up guessing. An explanation of what I did wrong and how to properly solve this would be great appreciated! The question:  Evaluate $f'(\frac{\pi}{6})$, where $f(x)=\tan^{-1}(\sin 2x)$. So these are the steps that I did. First using chain rule$$\frac{d}{dx}\tan^{-1}(\sin 2x)=\frac{1}{1+(\sin 2x)^2}\cdot\frac{d}{dx}\sin(2x)$$ $$\frac{d}{dx}\sin(2x)=\cos(2x)\cdot2$$so $$\frac{d}{dx}\tan^{-1}(\sin 2x)=\frac{1}{1+(\sin 2x)^2}*2\cos(2x)$$ Plugging in $(\frac{\pi}{6})$ I ended with $$$$ $$\frac{d}{dx}\tan^{-1}\left(\sin\frac{\pi}{3}\right)=\frac{1}{1+(\sin\frac{\pi}{3})^2}\cdot2\cos\left(\frac{\pi}{3}\right)$$I ended with some answer that didn't match up with any of the answer choices, did I go about solving this wrong? If so, how would I solve it correctly?  Thanks in advance.",,"['calculus', 'trigonometry', 'derivatives']"
94,Uncomfortable using Leibniz notation for the chain rule.,Uncomfortable using Leibniz notation for the chain rule.,,"I am working through the following solved problem which uses separation of variables to get two ODEs.  The problem is to show that $$\frac{1}{\sin\theta P}\frac{\mathrm{d}}{\mathrm{d}\theta}\left(\sin\theta\frac{\mathrm{d}P}{\mathrm{d}\theta}\right)-\frac{m^2}{\sin^2\theta}=-\lambda,$$ can be expressed as $$\left(1-x^2\right)\frac{\mathrm{d}^2P}{\mathrm{d}x^2}-2x\frac{\mathrm{d}P}{\mathrm{d}x}+\left[l\left(l+1\right)-\frac{m^2}{1-x^2}\right]P=0,$$ where $\lambda=l\left(l+1\right)$ and $x=\cos\theta$. I can solve the problem fine but I am uncomfortable with some of the notation that I used and want to know / understand better if it is correct. Essentially the answer involves stating that $$\frac{\mathrm{d}}{\mathrm{d}\theta}=\frac{\mathrm{d}x}{\mathrm{d}\theta}\frac{\mathrm{d}}{\mathrm{d}x}=-\sin\theta\frac{\mathrm{d}}{\mathrm{d}x},$$ and substituting it in. I can see that this is an application of the chain rule, but I don't feel comfortable in it being expressed this way.  I think what bothers me is the first part it is not specified what function we are taking the derivative of, which in this case is $x$ but then that would leave you with $$\frac{\mathrm{d}x}{\mathrm{d}\theta}=\frac{\mathrm{d}x}{\mathrm{d}\theta}=-\sin\theta,$$ which doesn't give you the manipulation that is needed to re-express the question.  This sort of manipulation gets used a lot and I would like to become more comfortable with understanding why it is valid to state such a thing as to me it just doesn't feel 100% correct.","I am working through the following solved problem which uses separation of variables to get two ODEs.  The problem is to show that $$\frac{1}{\sin\theta P}\frac{\mathrm{d}}{\mathrm{d}\theta}\left(\sin\theta\frac{\mathrm{d}P}{\mathrm{d}\theta}\right)-\frac{m^2}{\sin^2\theta}=-\lambda,$$ can be expressed as $$\left(1-x^2\right)\frac{\mathrm{d}^2P}{\mathrm{d}x^2}-2x\frac{\mathrm{d}P}{\mathrm{d}x}+\left[l\left(l+1\right)-\frac{m^2}{1-x^2}\right]P=0,$$ where $\lambda=l\left(l+1\right)$ and $x=\cos\theta$. I can solve the problem fine but I am uncomfortable with some of the notation that I used and want to know / understand better if it is correct. Essentially the answer involves stating that $$\frac{\mathrm{d}}{\mathrm{d}\theta}=\frac{\mathrm{d}x}{\mathrm{d}\theta}\frac{\mathrm{d}}{\mathrm{d}x}=-\sin\theta\frac{\mathrm{d}}{\mathrm{d}x},$$ and substituting it in. I can see that this is an application of the chain rule, but I don't feel comfortable in it being expressed this way.  I think what bothers me is the first part it is not specified what function we are taking the derivative of, which in this case is $x$ but then that would leave you with $$\frac{\mathrm{d}x}{\mathrm{d}\theta}=\frac{\mathrm{d}x}{\mathrm{d}\theta}=-\sin\theta,$$ which doesn't give you the manipulation that is needed to re-express the question.  This sort of manipulation gets used a lot and I would like to become more comfortable with understanding why it is valid to state such a thing as to me it just doesn't feel 100% correct.",,"['calculus', 'derivatives', 'notation']"
95,Find the derivative of $\frac{(2x−1)e^{−2x}}{(1−x)^2}$,Find the derivative of,\frac{(2x−1)e^{−2x}}{(1−x)^2},I need to find the derivative of $$\frac{(2x−1)e^{−2x}}{(1−x)^2}$$ I seems very complex to me so I'm wondering if there is a rule or formula I should be using? I attempted it using the chain rule first for the numerator (since I have $ ( 2 x- 1)$ multiplied by $e^{- 2 x}$ as my numerator) and then my plan was to use this rule: $(\frac{u}{v})′=\frac{vu′-uv′}{v^2}$. It gets messy and complicated. Could someone please explain how you'd attempt this problem?,I need to find the derivative of $$\frac{(2x−1)e^{−2x}}{(1−x)^2}$$ I seems very complex to me so I'm wondering if there is a rule or formula I should be using? I attempted it using the chain rule first for the numerator (since I have $ ( 2 x- 1)$ multiplied by $e^{- 2 x}$ as my numerator) and then my plan was to use this rule: $(\frac{u}{v})′=\frac{vu′-uv′}{v^2}$. It gets messy and complicated. Could someone please explain how you'd attempt this problem?,,['derivatives']
96,Gateaux and Frechet derivatives and related notions,Gateaux and Frechet derivatives and related notions,,"Let $X$ and $Y$ be normed real vector spaces, and $f : X \to Y$ a map. Let's say that: G) $f$ is Gateaux differentiable at $x_0 \in X$ if for all directions $v \in X$ the limit $f'(x_0)(v) := \lim_{t \searrow 0} t^{-1} [f(x_0 + t v) - f(x_0)]$ exists in $Y$. F) $f$ is Frechet differentiable at $x_0 \in X$ if there exists a bounded linear operator $A : X \to Y$ such that for all $h \in X$: $f(x+h) - f(x) = A h + o(\|h\|_{X})$ as $h \to 0$ in $X$. That's a little Landau-oh; the residual term $o(\|h\|_{X})$ is supposed to be bounded in terms of $\|h\|_X$ only, uniformly in $h / \|h\|_X$. Fix $x_0 \in X$. Once we know that $f$ is Gateaux differentiable at $x_0$, there are several obstructions to it being Frechet differentiable at $x_0$. Suppose $f$ is Gateaux differentiable at $x_0$ and the Gateaux differential $v \mapsto f'(x_0)(v)$ is one of the following: 0) exactly the Frechet derivative. 1) linear but not continuous in $v$. 2) continuous but not linear in $v$. 3) linear and continuous in $v$. Suppose the Gateaux derivative $f'(x) : X \to Y$ is a bounded linear functional for each $x \in X$ and $f'$ is one of the following: 4) continuous at $x_0$: $f'(x) \to f'(x_0)$ in operator norm whenever $x \to x_0$ in $X$. 5) continuous at $x_0$ weakly: $f'(x) \to f'(x_0)$ in operator norm whenever $x$ converges weakly to $x_0$. 6) weakly continuous at $x_0$: for each $v \in X$ fixed, one has $f'(x)(v) \to f'(x_0)(v)$ whenever $x \to x_0$ in $X$. You are welcome to answer any subset of the following questions . a) Is any of those notions redundant? b) Is any of those notions redundant if $X$ and $Y$ are Banach spaces? Provide examples, if such exist, for: c): 3) but not 0). d): 6) but not 5). e): 4) but not 5). f): $x \mapsto f'(x)$ is $C^1$ but not 0). g): where 5) occurs. h): where 6) occurs. Suppose in 4)--6), $f$ is Frechet differentiable at each $x$. i) In which case is $f$ necessarily Frechet differentiable at $x_0$? j) Same as i), but assuming $X$ and $Y$ are Banach spaces.","Let $X$ and $Y$ be normed real vector spaces, and $f : X \to Y$ a map. Let's say that: G) $f$ is Gateaux differentiable at $x_0 \in X$ if for all directions $v \in X$ the limit $f'(x_0)(v) := \lim_{t \searrow 0} t^{-1} [f(x_0 + t v) - f(x_0)]$ exists in $Y$. F) $f$ is Frechet differentiable at $x_0 \in X$ if there exists a bounded linear operator $A : X \to Y$ such that for all $h \in X$: $f(x+h) - f(x) = A h + o(\|h\|_{X})$ as $h \to 0$ in $X$. That's a little Landau-oh; the residual term $o(\|h\|_{X})$ is supposed to be bounded in terms of $\|h\|_X$ only, uniformly in $h / \|h\|_X$. Fix $x_0 \in X$. Once we know that $f$ is Gateaux differentiable at $x_0$, there are several obstructions to it being Frechet differentiable at $x_0$. Suppose $f$ is Gateaux differentiable at $x_0$ and the Gateaux differential $v \mapsto f'(x_0)(v)$ is one of the following: 0) exactly the Frechet derivative. 1) linear but not continuous in $v$. 2) continuous but not linear in $v$. 3) linear and continuous in $v$. Suppose the Gateaux derivative $f'(x) : X \to Y$ is a bounded linear functional for each $x \in X$ and $f'$ is one of the following: 4) continuous at $x_0$: $f'(x) \to f'(x_0)$ in operator norm whenever $x \to x_0$ in $X$. 5) continuous at $x_0$ weakly: $f'(x) \to f'(x_0)$ in operator norm whenever $x$ converges weakly to $x_0$. 6) weakly continuous at $x_0$: for each $v \in X$ fixed, one has $f'(x)(v) \to f'(x_0)(v)$ whenever $x \to x_0$ in $X$. You are welcome to answer any subset of the following questions . a) Is any of those notions redundant? b) Is any of those notions redundant if $X$ and $Y$ are Banach spaces? Provide examples, if such exist, for: c): 3) but not 0). d): 6) but not 5). e): 4) but not 5). f): $x \mapsto f'(x)$ is $C^1$ but not 0). g): where 5) occurs. h): where 6) occurs. Suppose in 4)--6), $f$ is Frechet differentiable at each $x$. i) In which case is $f$ necessarily Frechet differentiable at $x_0$? j) Same as i), but assuming $X$ and $Y$ are Banach spaces.",,"['derivatives', 'operator-theory', 'banach-spaces', 'normed-spaces']"
97,Is This A Derivative?,Is This A Derivative?,,"I am in a little over my head. This all began with my reading how each level of pascals triangle adds to $2^n$, where n=row# starting with n=0. I then though, ""wouldn't it be clever if the rows added to something else--like say $3^n$ instead?"" Or even better generalize it for any constant, $a^n$. All that was needed was to Multiply any given number in pascal's triangle by $a^n/2^n$ $$ \begin{array}{rcccccccccc} &    &    &    &    &    &  1\\\ &    &    &    &    &  \frac{a}{2} &    &    \frac{a}{2}\\\ &    &    &    &  \frac{a^2}{4} &    &  \frac{a^2}{2} &    &    \frac{a^2}{4}\\\ &    &    &      \frac{a^3}{8} &    &  \frac{3*a^3}{8} &    &  \frac{3*a^3}{8} &    &    \frac{a^3}{8}\\\ &    &      \frac{a^4}{16} &    &  \frac{a^4}{4} &    &  \frac{3*a^4}{8} &    &  \frac{a^4}{4} &    &      \frac{a^4}{16}\\\ &      \frac{a^5}{32} & &   \frac{5*a^5}{32} &    &   \frac{5*a^5}{16} &    &    \frac{5*a^5}{16}&    &  \frac{5*a^5}{32} &    &  \frac{a^5}{32}\\\ &    \frac{a^6}{64}&  &\frac{3*a^6}{32}&   &\frac{15*a^6}{64}& &\frac{5*a^6}{16}& &\frac{15*a^6}{64}& &\frac{3*a^6}{32}& &\frac{a^6}{64}\\\ &    & ... & & & &... & & & & ... &  \end{array} $$ Adding any row should give $a^n$. Placing said coefficients in front of a binomial expansion and solving for the binomial expression yields $$(a^n/2^n)*(x+y)^n$$ Letting a=2 makes pascals triangle, but every other value of ""a"" distorts every value and relation (except n=0 row for obvious reasons). This triangle creates some interesting relations that are shared with Pascal's triangle and are immediately obvious: every term in the middle column may be divided by ""a"" to yield the term above and to the right or left of it--just like Pascals triangle (a=2). Next is a really fascinating fluke:  $$ \frac{\partial \frac {a^4}{4}}{\partial a}=a^3  $$, which is the sum of the line above it.  $$ \frac{\partial \frac {a^2}{4}}{\partial a}=\frac {a}{2} $$, which is found in the line above it. I realize that ""a"" must have a definite value as a coefficient in order to have meaning, and it is not itself a function, but it seems curious that derivative relationship would show up in the relations between the coefficients of this modified triangle.  This serendipitous relation fascinates me to the point of asking ""whats up?"" here. Is this a derivative? Have derivative relationships popped up organically elsewhere in function theory?","I am in a little over my head. This all began with my reading how each level of pascals triangle adds to $2^n$, where n=row# starting with n=0. I then though, ""wouldn't it be clever if the rows added to something else--like say $3^n$ instead?"" Or even better generalize it for any constant, $a^n$. All that was needed was to Multiply any given number in pascal's triangle by $a^n/2^n$ $$ \begin{array}{rcccccccccc} &    &    &    &    &    &  1\\\ &    &    &    &    &  \frac{a}{2} &    &    \frac{a}{2}\\\ &    &    &    &  \frac{a^2}{4} &    &  \frac{a^2}{2} &    &    \frac{a^2}{4}\\\ &    &    &      \frac{a^3}{8} &    &  \frac{3*a^3}{8} &    &  \frac{3*a^3}{8} &    &    \frac{a^3}{8}\\\ &    &      \frac{a^4}{16} &    &  \frac{a^4}{4} &    &  \frac{3*a^4}{8} &    &  \frac{a^4}{4} &    &      \frac{a^4}{16}\\\ &      \frac{a^5}{32} & &   \frac{5*a^5}{32} &    &   \frac{5*a^5}{16} &    &    \frac{5*a^5}{16}&    &  \frac{5*a^5}{32} &    &  \frac{a^5}{32}\\\ &    \frac{a^6}{64}&  &\frac{3*a^6}{32}&   &\frac{15*a^6}{64}& &\frac{5*a^6}{16}& &\frac{15*a^6}{64}& &\frac{3*a^6}{32}& &\frac{a^6}{64}\\\ &    & ... & & & &... & & & & ... &  \end{array} $$ Adding any row should give $a^n$. Placing said coefficients in front of a binomial expansion and solving for the binomial expression yields $$(a^n/2^n)*(x+y)^n$$ Letting a=2 makes pascals triangle, but every other value of ""a"" distorts every value and relation (except n=0 row for obvious reasons). This triangle creates some interesting relations that are shared with Pascal's triangle and are immediately obvious: every term in the middle column may be divided by ""a"" to yield the term above and to the right or left of it--just like Pascals triangle (a=2). Next is a really fascinating fluke:  $$ \frac{\partial \frac {a^4}{4}}{\partial a}=a^3  $$, which is the sum of the line above it.  $$ \frac{\partial \frac {a^2}{4}}{\partial a}=\frac {a}{2} $$, which is found in the line above it. I realize that ""a"" must have a definite value as a coefficient in order to have meaning, and it is not itself a function, but it seems curious that derivative relationship would show up in the relations between the coefficients of this modified triangle.  This serendipitous relation fascinates me to the point of asking ""whats up?"" here. Is this a derivative? Have derivative relationships popped up organically elsewhere in function theory?",,"['derivatives', 'pattern-recognition']"
98,Derivatives on hidden layers in backpropagation (ANNs),Derivatives on hidden layers in backpropagation (ANNs),,"I'm working on understanding all the math used in artificial neural networks. I have gotten stuck at calculating the error function derivatives for hidden layers when performing backpropagation. On page 244 of Bishop's ""Pattern recognition and machine learning"", formula 5.55. The derivative of the error function for a hidden layer is given using a sum of derivatives over all units to which it sends connections. $$ \frac{\partial E_n}{\partial a_j} = \sum_k \frac{\partial E_n}{\partial a_k} \frac{\partial a_k}{\partial a_j}$$ I know the chain rule. If $a_j$ goes into only one other node, we can apply the chain rule to separate the parts. But what is the intuition behind summing these values for all nodes if the output goes into multiple nodes? Thanks","I'm working on understanding all the math used in artificial neural networks. I have gotten stuck at calculating the error function derivatives for hidden layers when performing backpropagation. On page 244 of Bishop's ""Pattern recognition and machine learning"", formula 5.55. The derivative of the error function for a hidden layer is given using a sum of derivatives over all units to which it sends connections. $$ \frac{\partial E_n}{\partial a_j} = \sum_k \frac{\partial E_n}{\partial a_k} \frac{\partial a_k}{\partial a_j}$$ I know the chain rule. If $a_j$ goes into only one other node, we can apply the chain rule to separate the parts. But what is the intuition behind summing these values for all nodes if the output goes into multiple nodes? Thanks",,"['derivatives', 'machine-learning', 'neural-networks']"
99,Determining partial derivatives and cross products for bicubic interpolation using function values only?,Determining partial derivatives and cross products for bicubic interpolation using function values only?,,"I'm trying to implement a bicubic interpolation algorithm. In order to calculate the interpolated values, I need to calculate sixteen coefficients used in the calculation process - and that's where I'm stumped. So far I've tried to use the calculation methods for univariate functions as explained by Paul Bourke in his article on interpolation methods, calculating the coefficients for each vertical coordinate individually, using this data to determine the function values at the selected Y coordinate, using those values to calculate the coefficients for the given slice of the function and calculate the function value for the given X coordinate. This technically works, but does not give the same results as expected. Using the first coefficient calculation method described by Bourke, the image is closer overall but includes visible artifacts: Using Catmull-Rom splines (as described by Bourke), image is smoother but differs far more from the example I'm trying to recreate): ""A Review  of Some Image Pixel Interpolation Algorithms"" by Don Lancaster and the Wikipedia article on bicubic interpolation show decidedly different results using same data values. Both describe what (I assume) should be the correct way of calculating the coefficients - the final formula itself is pretty clear, but relies on determining several partial derivatives and cross products. It has been several years since I had calculus and while I understand what a partial derivative is, I no longer remember how to actually calculate it from given function values. I'm completely clueless as to the cross products - the subject might not have been actually covered during the calculus and linear algebra courses I took. I'd appreciate advice as to how I should proceed to properly determine those values.","I'm trying to implement a bicubic interpolation algorithm. In order to calculate the interpolated values, I need to calculate sixteen coefficients used in the calculation process - and that's where I'm stumped. So far I've tried to use the calculation methods for univariate functions as explained by Paul Bourke in his article on interpolation methods, calculating the coefficients for each vertical coordinate individually, using this data to determine the function values at the selected Y coordinate, using those values to calculate the coefficients for the given slice of the function and calculate the function value for the given X coordinate. This technically works, but does not give the same results as expected. Using the first coefficient calculation method described by Bourke, the image is closer overall but includes visible artifacts: Using Catmull-Rom splines (as described by Bourke), image is smoother but differs far more from the example I'm trying to recreate): ""A Review  of Some Image Pixel Interpolation Algorithms"" by Don Lancaster and the Wikipedia article on bicubic interpolation show decidedly different results using same data values. Both describe what (I assume) should be the correct way of calculating the coefficients - the final formula itself is pretty clear, but relies on determining several partial derivatives and cross products. It has been several years since I had calculus and while I understand what a partial derivative is, I no longer remember how to actually calculate it from given function values. I'm completely clueless as to the cross products - the subject might not have been actually covered during the calculus and linear algebra courses I took. I'd appreciate advice as to how I should proceed to properly determine those values.",,"['algorithms', 'derivatives', 'interpolation', 'cross-product']"
