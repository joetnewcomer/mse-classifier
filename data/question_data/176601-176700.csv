,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proof of $\nabla\times\vec{E}=0\Rightarrow\exists\Phi:\vec{E}=\nabla\Phi$,Proof of,\nabla\times\vec{E}=0\Rightarrow\exists\Phi:\vec{E}=\nabla\Phi,Would anyone be able to help me prove these two statements involving vector fields? For a suitable region $$\nabla\times\vec{E}=0\Rightarrow\exists\Phi:\vec{E}=\nabla\Phi$$ and $$\nabla\cdot\vec{B}=0\Rightarrow\exists\vec{A}:\vec{B}=\nabla\times\vec{A}$$ Many thanks,Would anyone be able to help me prove these two statements involving vector fields? For a suitable region $$\nabla\times\vec{E}=0\Rightarrow\exists\Phi:\vec{E}=\nabla\Phi$$ and $$\nabla\cdot\vec{B}=0\Rightarrow\exists\vec{A}:\vec{B}=\nabla\times\vec{A}$$ Many thanks,,"['multivariable-calculus', 'vector-analysis']"
1,How to verify a solution to the Wave Equation?,How to verify a solution to the Wave Equation?,,"I have a wave equation: $$\frac{\partial^2u}{\partial t^2} = a^2 \frac{\partial^2u}{\partial x^2}.$$ How would I verify that the function $u(x,t)=\sin(x-at)$ satisfies the aforementioned wave equation?","I have a wave equation: $$\frac{\partial^2u}{\partial t^2} = a^2 \frac{\partial^2u}{\partial x^2}.$$ How would I verify that the function $u(x,t)=\sin(x-at)$ satisfies the aforementioned wave equation?",,"['multivariable-calculus', 'partial-differential-equations']"
2,Find the directions in which the directional derivative has the value 1,Find the directions in which the directional derivative has the value 1,,"Can anyone show me how to adjust my work below so that it is a correct answer?  This is question number 14.6.28 in the 7th edition of Stewart Calculus. Find the directions in which the directional derivative of $f(x,y)=ye^{-xy}$ at the point $(0,2)$ has the value 1. My work is: $\nabla f(x,y) = <-y^2 e^{-xy}, e^{-xy}(1-xy)>$ $\nabla f(0,2) = <-4,1>$ $|\nabla f(0,2)| = \sqrt{17}$ $D_v f(x,y)=|\nabla f|\cos{\theta}=\sqrt{17}\cos{\theta}$ $\sqrt{17} \cos{\theta}=1$ when $\cos{\theta}={{\sqrt{17}}/17}$ $\theta =\arccos{{\sqrt{17}}/17}\approx +1.326$ and $-1.326$ EDIT: I tried the following, based on suggestions below: $\vec{u}=<\cos{\theta},\sin{\theta}>$ $D_u f(x,y)=\nabla f(x,y)\cdot \vec{u}$ $D_u f(0,2)=\nabla f(0,2)\cdot \vec{u}=-4\cos{\theta}+\sin{\theta}=1$ I then plugged this into a spreadsheet and found that $-4\cos{\theta}+\sin{\theta}=1$ when $\theta = \pi/2 , 5\pi/2 , 9\pi/2 , ...$ and when $\theta = 4\pi/3 , 10\pi/3 , 16\pi/3 , ...$ Can anyone check the correctness of this approach? Also, I found this result experimentally.  If it is correct, I would rather be able to find it using calculus.","Can anyone show me how to adjust my work below so that it is a correct answer?  This is question number 14.6.28 in the 7th edition of Stewart Calculus. Find the directions in which the directional derivative of $f(x,y)=ye^{-xy}$ at the point $(0,2)$ has the value 1. My work is: $\nabla f(x,y) = <-y^2 e^{-xy}, e^{-xy}(1-xy)>$ $\nabla f(0,2) = <-4,1>$ $|\nabla f(0,2)| = \sqrt{17}$ $D_v f(x,y)=|\nabla f|\cos{\theta}=\sqrt{17}\cos{\theta}$ $\sqrt{17} \cos{\theta}=1$ when $\cos{\theta}={{\sqrt{17}}/17}$ $\theta =\arccos{{\sqrt{17}}/17}\approx +1.326$ and $-1.326$ EDIT: I tried the following, based on suggestions below: $\vec{u}=<\cos{\theta},\sin{\theta}>$ $D_u f(x,y)=\nabla f(x,y)\cdot \vec{u}$ $D_u f(0,2)=\nabla f(0,2)\cdot \vec{u}=-4\cos{\theta}+\sin{\theta}=1$ I then plugged this into a spreadsheet and found that $-4\cos{\theta}+\sin{\theta}=1$ when $\theta = \pi/2 , 5\pi/2 , 9\pi/2 , ...$ and when $\theta = 4\pi/3 , 10\pi/3 , 16\pi/3 , ...$ Can anyone check the correctness of this approach? Also, I found this result experimentally.  If it is correct, I would rather be able to find it using calculus.",,"['multivariable-calculus', 'vector-analysis']"
3,Proving vector calculus identity $\nabla \times (\mathbf a\times \mathbf b) =\cdots$ using Levi-Civita symbol,Proving vector calculus identity  using Levi-Civita symbol,\nabla \times (\mathbf a\times \mathbf b) =\cdots,"I want to prove the following relation $$\nabla \times (\mathbf a\times \mathbf b) = \mathbf a\nabla \cdot \mathbf  b + \mathbf  b \cdot \nabla \mathbf  a - \mathbf  b \nabla \cdot \mathbf  a - \mathbf a \cdot \nabla \mathbf  b$$ using the following Levi-Civita definition of cross product $$\mathbf{a} \times \mathbf{b} =\mathbf{e}_i \epsilon_{ijk}a_ib_j$$ where $\epsilon_{ijk} =\begin{cases} +1 & \text{if } i,j,k \text{ are in clockwise permutation}, \\ -1 & \text{if } i,j,k \text{ are in counterclockwise permutation, and} \\ \;\;\,0 & \text{if }i=j \text{ or } j=k \text{ or } k=i. \end{cases}$","I want to prove the following relation $$\nabla \times (\mathbf a\times \mathbf b) = \mathbf a\nabla \cdot \mathbf  b + \mathbf  b \cdot \nabla \mathbf  a - \mathbf  b \nabla \cdot \mathbf  a - \mathbf a \cdot \nabla \mathbf  b$$ using the following Levi-Civita definition of cross product $$\mathbf{a} \times \mathbf{b} =\mathbf{e}_i \epsilon_{ijk}a_ib_j$$ where $\epsilon_{ijk} =\begin{cases} +1 & \text{if } i,j,k \text{ are in clockwise permutation}, \\ -1 & \text{if } i,j,k \text{ are in counterclockwise permutation, and} \\ \;\;\,0 & \text{if }i=j \text{ or } j=k \text{ or } k=i. \end{cases}$",,"['multivariable-calculus', 'vector-analysis', 'index-notation']"
4,Is this definition missing some assumptions?,Is this definition missing some assumptions?,,"In his ""Calculus on manifolds"" Spivak first defines $n$-dimensional (Riemann-) integral over rectancles, then over Jordan measurable subsets of rectangles and finally extends it to open sets using partitions of unity. It's this final part I'm having some problems with. He defines a partition of unity for a set $A\subset \mathbb{R}^n$ to be family $\Phi$ of $C^{\infty}$ functions satisfying the following three axioms: $0\leq\phi(x)\leq 1$ $\forall \phi\in\Phi$ $\forall x\in A$ For every $x\in A$ there exists an open set $V$ s.t. only finitely many $\phi\in\Phi$ are nonzero in $V$ $\sum_{\phi\in\Phi} \phi(x)=1$ $\forall x\in A$ Moreover, $\Phi$ is subordinate to an open cover $\mathcal{O}$ of $A$ if for each $\phi\in\Phi$ we can find an open set $U\in\mathcal{O}$ s.t. $\phi=0$ outside some closed set contained in $U$. Suppose now that $A$ is open and we have an open cover $\mathcal{O}$ for $A$ s.t. $U\subseteq A$ for each $U\in\mathcal{O}$. We shall call such a cover admissible . Let $f:A\to R$ be locally bounded (i.e. every point has a neighbourhood $V$ s.t. $f$ bounded in $V$) and continuous almost everywhere (i.e. the set of discontinuties has measure $0$). Furthermore let $\Phi$ be a countable partition of unity for $A$ subordinate to $\mathcal{O}$. We say that $f$ is integrable over $A$ if $\sum_{\phi\in\Phi} \int_A \phi |f|$ converges. Spivak asserts that the integrals $\int_A \phi|f|$ all exist but I don't see why because we have thus far only defined integrals over bounded sets. Obviously we could assume that the open cover is composed of bounded sets or maybe instead that the functions $\phi$ have compact support but neither appears to be assumed in the book. So do we need some extra assumptions here or am I missing something?","In his ""Calculus on manifolds"" Spivak first defines $n$-dimensional (Riemann-) integral over rectancles, then over Jordan measurable subsets of rectangles and finally extends it to open sets using partitions of unity. It's this final part I'm having some problems with. He defines a partition of unity for a set $A\subset \mathbb{R}^n$ to be family $\Phi$ of $C^{\infty}$ functions satisfying the following three axioms: $0\leq\phi(x)\leq 1$ $\forall \phi\in\Phi$ $\forall x\in A$ For every $x\in A$ there exists an open set $V$ s.t. only finitely many $\phi\in\Phi$ are nonzero in $V$ $\sum_{\phi\in\Phi} \phi(x)=1$ $\forall x\in A$ Moreover, $\Phi$ is subordinate to an open cover $\mathcal{O}$ of $A$ if for each $\phi\in\Phi$ we can find an open set $U\in\mathcal{O}$ s.t. $\phi=0$ outside some closed set contained in $U$. Suppose now that $A$ is open and we have an open cover $\mathcal{O}$ for $A$ s.t. $U\subseteq A$ for each $U\in\mathcal{O}$. We shall call such a cover admissible . Let $f:A\to R$ be locally bounded (i.e. every point has a neighbourhood $V$ s.t. $f$ bounded in $V$) and continuous almost everywhere (i.e. the set of discontinuties has measure $0$). Furthermore let $\Phi$ be a countable partition of unity for $A$ subordinate to $\mathcal{O}$. We say that $f$ is integrable over $A$ if $\sum_{\phi\in\Phi} \int_A \phi |f|$ converges. Spivak asserts that the integrals $\int_A \phi|f|$ all exist but I don't see why because we have thus far only defined integrals over bounded sets. Obviously we could assume that the open cover is composed of bounded sets or maybe instead that the functions $\phi$ have compact support but neither appears to be assumed in the book. So do we need some extra assumptions here or am I missing something?",,"['multivariable-calculus', 'integration']"
5,"Evaluating a double integral with change of coordinates. $\int\!\!\int_{R} (x+y)\,dA$",Evaluating a double integral with change of coordinates.,"\int\!\!\int_{R} (x+y)\,dA","It looks simple, but I'm having a bit of difficulty with trying to obtain the proper change of coordinates. Evaluate the following integral $$\displaystyle\int\!\!\!\int_{R} (x+y)\,dA$$ with region R in the first quadrant bounded by $-1\le y-x\le 1$ and $1\le x^2+y^2 \le 4$ I've tried $u=x^2+y^2$ and $v=y-x$, but I can't seem to express $x+y$ in terms of $u$ and $v$. Is it possible to express $x+y$ in terms of $u$ and $v$ with the above substitutions? Or do we need another change of variables? What other change of coordinates is possible to approach this problem?  Based on what region $R$ looks like, I don't think polar coordinates would work for this even though there is an $x^2+y^2$ term (though I could be mistaken).","It looks simple, but I'm having a bit of difficulty with trying to obtain the proper change of coordinates. Evaluate the following integral $$\displaystyle\int\!\!\!\int_{R} (x+y)\,dA$$ with region R in the first quadrant bounded by $-1\le y-x\le 1$ and $1\le x^2+y^2 \le 4$ I've tried $u=x^2+y^2$ and $v=y-x$, but I can't seem to express $x+y$ in terms of $u$ and $v$. Is it possible to express $x+y$ in terms of $u$ and $v$ with the above substitutions? Or do we need another change of variables? What other change of coordinates is possible to approach this problem?  Based on what region $R$ looks like, I don't think polar coordinates would work for this even though there is an $x^2+y^2$ term (though I could be mistaken).",,"['calculus', 'integration', 'multivariable-calculus']"
6,Calculating volume with a triple integral,Calculating volume with a triple integral,,"I have the following problem. I need to find the volume of the shape that is bounded up by $$u(x,y)=13+x^2$$ and down by $$v(x,y)=7x+4y$$ and from its sides by the cylinder $$x^2+y^2=1.$$ Now I know I need to do to the integral $\displaystyle \iint\limits_D \left(u(x,y)-v(x,y) \right)dxdy$ But I dont really know how to define $D$, I know that is by the projection in $XY$ plane. I tried to draw it in the $XY$ plane, drew the cylinder and the plane $z=7x+4y$. But I don't really know where to go next. Some tips will really help me! :)","I have the following problem. I need to find the volume of the shape that is bounded up by $$u(x,y)=13+x^2$$ and down by $$v(x,y)=7x+4y$$ and from its sides by the cylinder $$x^2+y^2=1.$$ Now I know I need to do to the integral $\displaystyle \iint\limits_D \left(u(x,y)-v(x,y) \right)dxdy$ But I dont really know how to define $D$, I know that is by the projection in $XY$ plane. I tried to draw it in the $XY$ plane, drew the cylinder and the plane $z=7x+4y$. But I don't really know where to go next. Some tips will really help me! :)",,"['calculus', 'integration', 'multivariable-calculus']"
7,Weird integral with cylinders,Weird integral with cylinders,,"I have this weird integral to find. I am actually trying to find the volume that is described by these two equations. $$x^2+y^2=4$$ and $$x^2+z^2=4$$ for $$x\geq0, y\geq0, z\geq0$$ It is a weird object that has the plane $z=y$ as a divider for the two cylinders. My problems is that I can't find the integration limits. I can't even draw this thing properly.","I have this weird integral to find. I am actually trying to find the volume that is described by these two equations. $$x^2+y^2=4$$ and $$x^2+z^2=4$$ for $$x\geq0, y\geq0, z\geq0$$ It is a weird object that has the plane $z=y$ as a divider for the two cylinders. My problems is that I can't find the integration limits. I can't even draw this thing properly.",,"['integration', 'multivariable-calculus']"
8,What is the difference between these two derivative expressions?,What is the difference between these two derivative expressions?,,"is there a difference between $\frac{\partial^2 }{\partial x^2}$ and $(\frac{\partial }{\partial x})^{2}$? I have to tell if a differential equation is linear, and $(\frac{\partial }{\partial x})^{2}$ concerns me. If it's the same thing as second derivative, the equation would be linear.","is there a difference between $\frac{\partial^2 }{\partial x^2}$ and $(\frac{\partial }{\partial x})^{2}$? I have to tell if a differential equation is linear, and $(\frac{\partial }{\partial x})^{2}$ concerns me. If it's the same thing as second derivative, the equation would be linear.",,"['multivariable-calculus', 'notation', 'derivatives']"
9,Transforming the Laplace operator from Polar to Cartesian coordinates,Transforming the Laplace operator from Polar to Cartesian coordinates,,"I'm trying to find the error in my logic here. Let's say we are given the Laplace operator in polar coordinates: $$ \frac{\partial^2}{\partial r^2} + \frac{1}{r}\frac{\partial}{\partial r} + \frac{1}{r^2}\frac{\partial}{\partial \theta^2} \tag{1} $$ and we're interested in transforming back to Cartesian coordinates.  We could first make use of the usual coordinate transformation, namely $$ u = r \sin(\theta), ~~v = r \cos(\theta), \tag{2}$$ to write the partial derivatives in polar coordinates as follows, $$ \frac{\partial}{\partial r} = \sin(\theta) \frac{\partial}{\partial u} + \cos(\theta)\frac{\partial}{\partial v}, \tag{3}$$ $$ \frac{\partial}{\partial \theta} = r \cos(\theta) \frac{\partial}{\partial u} -  r \sin(\theta)\frac{\partial}{\partial v}. \tag{4}$$ I think we should then rewrite all $\theta$ and $r$ in terms of $u$ and $v$, making use of $r^2 = u^2 + v^2$, so that in particular $$ \sin(\theta) = \frac{u}{\sqrt{u^2 + v^2}}, \tag{5}$$ $$ \cos(\theta) = \frac{v}{\sqrt{u^2 + v^2}} \tag{6}.$$ With this, (3) and (4) then become $$ \frac{\partial}{\partial r} = \frac{u}{\sqrt{u^2 + v^2}} \frac{\partial}{\partial u} + \frac{v}{\sqrt{u^2 + v^2}} \frac{\partial}{\partial v}, \tag{3*}$$ $$ \frac{\partial}{\partial \theta} = v \frac{\partial}{\partial u} -  u \frac{\partial}{\partial v}. \tag{4*}$$ Our next task is to then compute $\frac{\partial^2}{\partial r^2}$ and $\frac{\partial^2}{\partial \theta^2}$, which we can carefully do by multiplying the right-hand sides of (3*) and (4*), keeping in mind that the operators will act on the coefficients. However, executing this and adding the terms together yields nothing that looks remotely near the known form, namely $$\frac{\partial^2}{\partial u^2} + \frac{\partial^2}{\partial v^2}.$$  So I'm wondering, what am I missing here? In addition to this particular example, I'm interested in any general information you have about transforming partial derivatives from polar to Cartesian coordinates.","I'm trying to find the error in my logic here. Let's say we are given the Laplace operator in polar coordinates: $$ \frac{\partial^2}{\partial r^2} + \frac{1}{r}\frac{\partial}{\partial r} + \frac{1}{r^2}\frac{\partial}{\partial \theta^2} \tag{1} $$ and we're interested in transforming back to Cartesian coordinates.  We could first make use of the usual coordinate transformation, namely $$ u = r \sin(\theta), ~~v = r \cos(\theta), \tag{2}$$ to write the partial derivatives in polar coordinates as follows, $$ \frac{\partial}{\partial r} = \sin(\theta) \frac{\partial}{\partial u} + \cos(\theta)\frac{\partial}{\partial v}, \tag{3}$$ $$ \frac{\partial}{\partial \theta} = r \cos(\theta) \frac{\partial}{\partial u} -  r \sin(\theta)\frac{\partial}{\partial v}. \tag{4}$$ I think we should then rewrite all $\theta$ and $r$ in terms of $u$ and $v$, making use of $r^2 = u^2 + v^2$, so that in particular $$ \sin(\theta) = \frac{u}{\sqrt{u^2 + v^2}}, \tag{5}$$ $$ \cos(\theta) = \frac{v}{\sqrt{u^2 + v^2}} \tag{6}.$$ With this, (3) and (4) then become $$ \frac{\partial}{\partial r} = \frac{u}{\sqrt{u^2 + v^2}} \frac{\partial}{\partial u} + \frac{v}{\sqrt{u^2 + v^2}} \frac{\partial}{\partial v}, \tag{3*}$$ $$ \frac{\partial}{\partial \theta} = v \frac{\partial}{\partial u} -  u \frac{\partial}{\partial v}. \tag{4*}$$ Our next task is to then compute $\frac{\partial^2}{\partial r^2}$ and $\frac{\partial^2}{\partial \theta^2}$, which we can carefully do by multiplying the right-hand sides of (3*) and (4*), keeping in mind that the operators will act on the coefficients. However, executing this and adding the terms together yields nothing that looks remotely near the known form, namely $$\frac{\partial^2}{\partial u^2} + \frac{\partial^2}{\partial v^2}.$$  So I'm wondering, what am I missing here? In addition to this particular example, I'm interested in any general information you have about transforming partial derivatives from polar to Cartesian coordinates.",,"['multivariable-calculus', 'partial-differential-equations', 'coordinate-systems', 'polar-coordinates', 'differential-operators']"
10,Iterated Integrals and Unbounded Regions,Iterated Integrals and Unbounded Regions,,"Context I am having difficulty finding the posterior distribution of a Bayesian model with two parameters, which involves evaluating a double integral over an unbounded region. I prefer not to post the model for now, since this question is not strictly related to this problem in particular, but I probably will end up looking for help in another question. Question I have been taught that Fubini's Theorem holds if the region of integration is bounded, but all the books in Statistical Inference I own and my teachers always evaluate such multiple improper integrals (expectations, densities, and so forth) as iterated single improper integrals, and most of the times those are Gamma, Beta or density functions. I recall from multivariable calculus that if a function $f(x,y)$ is positive and $D = [a,\infty)\times[b,\infty)$ then, if the limit exists: $$\int_D f = \lim_{(c,d) \to (\infty,\infty)}\int_b^d\int_a^c f(x,y) \, dx \, dy = \lim_{(c,d) \to (\infty,\infty)}\int_a^c\int_b^d f(x,y) \, dy \, dx $$ Which I think it is not quite ""the product"" of two single improper integrals, so there must be something I am missing. Whenever I have applied this theorem in the context of multivariable calculus (homework, exams) $f$ was always a carefully chosen function with anti-derivatives. However, as I have mentioned before, the improper double integrals I have come across in Statistical Inference are not that easy. So, for instance, would it be correct to do something like this? Let $f(x,y) = g(x)h(y)$ , $D = [0,\infty)\times[b,\infty)$ and $g(x) = x^{\alpha-1}e^{-x}$ : $$\int_D f = \int_b^\infty\int_0^\infty f(x,y) \, dx \, dy = \Gamma(\alpha) \int_b^\infty h(y) \, dy $$ If so, how can it be justified? Sorry for my imprecision and thanks in advance!","Context I am having difficulty finding the posterior distribution of a Bayesian model with two parameters, which involves evaluating a double integral over an unbounded region. I prefer not to post the model for now, since this question is not strictly related to this problem in particular, but I probably will end up looking for help in another question. Question I have been taught that Fubini's Theorem holds if the region of integration is bounded, but all the books in Statistical Inference I own and my teachers always evaluate such multiple improper integrals (expectations, densities, and so forth) as iterated single improper integrals, and most of the times those are Gamma, Beta or density functions. I recall from multivariable calculus that if a function is positive and then, if the limit exists: Which I think it is not quite ""the product"" of two single improper integrals, so there must be something I am missing. Whenever I have applied this theorem in the context of multivariable calculus (homework, exams) was always a carefully chosen function with anti-derivatives. However, as I have mentioned before, the improper double integrals I have come across in Statistical Inference are not that easy. So, for instance, would it be correct to do something like this? Let , and : If so, how can it be justified? Sorry for my imprecision and thanks in advance!","f(x,y) D = [a,\infty)\times[b,\infty) \int_D f = \lim_{(c,d) \to (\infty,\infty)}\int_b^d\int_a^c f(x,y) \, dx \, dy = \lim_{(c,d) \to (\infty,\infty)}\int_a^c\int_b^d f(x,y) \, dy \, dx  f f(x,y) = g(x)h(y) D = [0,\infty)\times[b,\infty) g(x) = x^{\alpha-1}e^{-x} \int_D f = \int_b^\infty\int_0^\infty f(x,y) \, dx \, dy = \Gamma(\alpha) \int_b^\infty h(y) \, dy ",['multivariable-calculus']
11,Integrating ray casting equation,Integrating ray casting equation,,"In some ray casting algorithm I want to integrate the following integral: $\int_{0}^{Z} \frac{c^x}{\|(\vec{o}+x \vec{d})-\vec{l}\|^3} dx$ $Z$ is a constant until which I want to integrate $\vec{o}$ is the origin of the ray. $\vec{d}$ is the direction of the ray. $\vec{o} + x \vec{d}$ is the position of the ray at time $x$. $\vec{l}$ is the position of a light. $\|(\vec{o}+x \vec{d})-\vec{l}\|$ is the distance between the current position and the light. $c^x$ is a weighting factor which I want to apply. When I move further away from the camera by increasing x, I want the weight to decrease ($c$ < 1). $c$ is a constant value (like 0.9). Solving it via a Riemann sum is not efficient enough on the Xbox. Therefore, I want to solve it analytically. However, as there is a norm of vectors involved together with some other ugly parts, I don't really know where to start solving this. Is it possible to solve this integral analytically? (If yes, how?) Is there a way to approximate this integral by using non-uniform step-sizes using some Riemann-style integration to increase efficiency of my current implementation? Maybe some other solutions? It doesn't have to be exact; as long as it is not instable for small changes to $Z$ and it somehow approximates the integral, it should be fine since it is used in a game I am working on (where importance of performance is bigger than importance of correctness).","In some ray casting algorithm I want to integrate the following integral: $\int_{0}^{Z} \frac{c^x}{\|(\vec{o}+x \vec{d})-\vec{l}\|^3} dx$ $Z$ is a constant until which I want to integrate $\vec{o}$ is the origin of the ray. $\vec{d}$ is the direction of the ray. $\vec{o} + x \vec{d}$ is the position of the ray at time $x$. $\vec{l}$ is the position of a light. $\|(\vec{o}+x \vec{d})-\vec{l}\|$ is the distance between the current position and the light. $c^x$ is a weighting factor which I want to apply. When I move further away from the camera by increasing x, I want the weight to decrease ($c$ < 1). $c$ is a constant value (like 0.9). Solving it via a Riemann sum is not efficient enough on the Xbox. Therefore, I want to solve it analytically. However, as there is a norm of vectors involved together with some other ugly parts, I don't really know where to start solving this. Is it possible to solve this integral analytically? (If yes, how?) Is there a way to approximate this integral by using non-uniform step-sizes using some Riemann-style integration to increase efficiency of my current implementation? Maybe some other solutions? It doesn't have to be exact; as long as it is not instable for small changes to $Z$ and it somehow approximates the integral, it should be fine since it is used in a game I am working on (where importance of performance is bigger than importance of correctness).",,"['calculus', 'integration', 'multivariable-calculus']"
12,Surface Area of $z = \sin(x)\sin(y)$,Surface Area of,z = \sin(x)\sin(y),"This seems like an easy problem, but I can't seem to get a closed form solution: What is the surface area of the surface defined by the equation $z = \sin(x)\sin(y)$, over some rectangular region bounded by the following equations: $y = -x+a$ $y = x+a$ $y = x-a$ $y = 2\pi-x-a$. Other useful information: $x\in[0,\pi]$, $y\in[0,\pi]$, $a\in[0,\pi]$. You can see the function on wolfram alpha here . I've tried $A = \int\int|\vec{r_x}\times\vec{r_y}|dxdy$, but taking $\vec{r} = \langle x,y,\sin(x)\sin(y) \rangle$, I can't get a closed form anti-derivative of the integrand: $$\begin{align}|\vec{r_x}\times\vec{r_y}| &= \sqrt{-2s\sin^2(x)\sin^2(y)+\sin^2(x)+\sin^2(y)+1} \\ &= \frac{1}{2}\sqrt{6-\cos(2(x+y))-\cos(2(x-y))}\end{align}$$ Thank you in advance for your time, it is much appreciated!","This seems like an easy problem, but I can't seem to get a closed form solution: What is the surface area of the surface defined by the equation $z = \sin(x)\sin(y)$, over some rectangular region bounded by the following equations: $y = -x+a$ $y = x+a$ $y = x-a$ $y = 2\pi-x-a$. Other useful information: $x\in[0,\pi]$, $y\in[0,\pi]$, $a\in[0,\pi]$. You can see the function on wolfram alpha here . I've tried $A = \int\int|\vec{r_x}\times\vec{r_y}|dxdy$, but taking $\vec{r} = \langle x,y,\sin(x)\sin(y) \rangle$, I can't get a closed form anti-derivative of the integrand: $$\begin{align}|\vec{r_x}\times\vec{r_y}| &= \sqrt{-2s\sin^2(x)\sin^2(y)+\sin^2(x)+\sin^2(y)+1} \\ &= \frac{1}{2}\sqrt{6-\cos(2(x+y))-\cos(2(x-y))}\end{align}$$ Thank you in advance for your time, it is much appreciated!",,['multivariable-calculus']
13,Why isn't continuity necessary for existance of partial derivatives?,Why isn't continuity necessary for existance of partial derivatives?,,"Consider the graph of a function. $$f(x,y) =\begin{cases} \frac{xy}{x^2+2y^2},  & \text{(x,y) $\neq$ (0,0)} \\ 0, & \text{elsewhere} \end{cases}$$ It is discontinuous at $(0,0)$ . As on path $y = mx$ , the function evaluates to $\frac{m}{1+2m^2}$ limit depends on the path and hence is not continuous. But its partial derivatives exist at $(0,0)$ . What's the difference between derivates and partial derivates that gave rise to this situation? Isn't a partial derivative much like a derivate of a 3D curve in a plane? Even the procedure to find PD becomes the same once we consider other variables constant.","Consider the graph of a function. It is discontinuous at . As on path , the function evaluates to limit depends on the path and hence is not continuous. But its partial derivatives exist at . What's the difference between derivates and partial derivates that gave rise to this situation? Isn't a partial derivative much like a derivate of a 3D curve in a plane? Even the procedure to find PD becomes the same once we consider other variables constant.","f(x,y) =\begin{cases}
\frac{xy}{x^2+2y^2},  & \text{(x,y) \neq (0,0)} \\
0, & \text{elsewhere}
\end{cases} (0,0) y = mx \frac{m}{1+2m^2} (0,0)","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
14,For which values of $a$ there is no relative extremum?,For which values of  there is no relative extremum?,a,"In a previous question , I explored the conditions under which the function $ f(x, y) = (y - x^2)(y - ax^3) $ does not have a relative extremum at the origin. I've found a similar but distinct problem formulation to be equally interesting. Here's the modified version: Let $f : \mathbb{R}^2 \to \mathbb{R}$ be a function defined by $f(x,y) = (y - x^2)(y - ax^2)$ at each $ (x,y) \in \mathbb{R}^2$ , where $a \in \mathbb{R}$ . What are the values of $ a $ for which $ f$ does not have a relative extremum at $(0,0)$ ? Here is my solution attempt: To find the values of $ a$ for which $ f $ does not have a relative extremum at $ (0, 0) $ , it is necessary to analyze the critical points of $ f $ at $ (0, 0) $ . $f(x, y) = \frac{y}{y^2} - axy^2 + 2xy + ax^4$ $\frac{\partial f}{\partial x} = -2(a+1)xy + 4ax^3$ $\frac{\partial f}{\partial y} = 2y - 2(a+1)x^2$ $\frac{\partial^2 f}{\partial x^2} = -2a + 12ax^2$ $\frac{\partial^2 f}{\partial y^2} = 2$ $\frac{\partial^2 f}{\partial x \partial y} = -2a + 2x$ $H = \begin{bmatrix} -2a + 12ax^2 & -2a + 2x \\ -2a + 2x & 2 \end{bmatrix}$ $H_1(0,0) = 0$ , first principle minor. $H_2(0,0) = 0$ , second principle minor. I have outlined my analysis and calculations above to determine the values of $a$ for which the function $f$ does not have a relative extremum at $(0, 0)$ . But I could not reach any conclusions. I would greatly appreciate it if members of the community could review my approach and calculations. Are there any errors or aspects I might have overlooked? Thank you in advance for your insights and assistance.","In a previous question , I explored the conditions under which the function does not have a relative extremum at the origin. I've found a similar but distinct problem formulation to be equally interesting. Here's the modified version: Let be a function defined by at each , where . What are the values of for which does not have a relative extremum at ? Here is my solution attempt: To find the values of for which does not have a relative extremum at , it is necessary to analyze the critical points of at . , first principle minor. , second principle minor. I have outlined my analysis and calculations above to determine the values of for which the function does not have a relative extremum at . But I could not reach any conclusions. I would greatly appreciate it if members of the community could review my approach and calculations. Are there any errors or aspects I might have overlooked? Thank you in advance for your insights and assistance."," f(x, y) = (y - x^2)(y - ax^3)  f : \mathbb{R}^2 \to \mathbb{R} f(x,y) = (y - x^2)(y - ax^2)  (x,y) \in \mathbb{R}^2 a \in \mathbb{R}  a   f (0,0)  a  f   (0, 0)   f   (0, 0)  f(x, y) = \frac{y}{y^2} - axy^2 + 2xy + ax^4 \frac{\partial f}{\partial x} = -2(a+1)xy + 4ax^3 \frac{\partial f}{\partial y} = 2y - 2(a+1)x^2 \frac{\partial^2 f}{\partial x^2} = -2a + 12ax^2 \frac{\partial^2 f}{\partial y^2} = 2 \frac{\partial^2 f}{\partial x \partial y} = -2a + 2x H = \begin{bmatrix} -2a + 12ax^2 & -2a + 2x \\ -2a + 2x & 2 \end{bmatrix} H_1(0,0) = 0 H_2(0,0) = 0 a f (0, 0)","['calculus', 'multivariable-calculus']"
15,Finding the tangent line to curve to the ellipse $(x-3)^2+\frac{(y-4)^2}{4}=1$ through the origin,Finding the tangent line to curve to the ellipse  through the origin,(x-3)^2+\frac{(y-4)^2}{4}=1,"I am told to find the two tangent lines to the ellipse that pass through the origin, but have been stuck for far too long with my approach, hence am thinking that my approach may be flawed. Here is what I have so far: If I interpret the ellipse as the level curve of some function $f(x,y)=1$ , then I can use the fact that the gradient vector is perpendicular to the ellipse at every point $(a,b)$ on it. Computing the partials, I get that the gradient vector at any point $(a,b)$ is $$\nabla f(a, b) = \left( 2(a-3), \frac{(b-4)}{2}\right),$$ thus, the tangent vector at $(a,b)$ is $$\left(-\frac{(b-4)}{2}, 2(a-3)\right),$$ thus the equation of any tangent line to the ellipse passing through the origin is $$k\left(-\frac{(b-4)}{2}, 2(a-3)\right), k\in \mathbb Z.$$ However, I really don't get how I'm supposed to find... another tangent line? Have I made an oversight in one of the steps of my reasoning? To resolve this, I tried also tried the approach of parametrizing the ellipse into a vector-valued function $$\vec r(t)=(\cos t +3, 2\sin t +4),$$ which can equivalently be interpreted as the orbit of some moving partical. Then, I can differentiate this to get the velocity function of the particle: $$\vec r(t) = (-2\sin t,\cos t).$$ But then, how am I to ensure that the tangent lines pass through the origin?","I am told to find the two tangent lines to the ellipse that pass through the origin, but have been stuck for far too long with my approach, hence am thinking that my approach may be flawed. Here is what I have so far: If I interpret the ellipse as the level curve of some function , then I can use the fact that the gradient vector is perpendicular to the ellipse at every point on it. Computing the partials, I get that the gradient vector at any point is thus, the tangent vector at is thus the equation of any tangent line to the ellipse passing through the origin is However, I really don't get how I'm supposed to find... another tangent line? Have I made an oversight in one of the steps of my reasoning? To resolve this, I tried also tried the approach of parametrizing the ellipse into a vector-valued function which can equivalently be interpreted as the orbit of some moving partical. Then, I can differentiate this to get the velocity function of the particle: But then, how am I to ensure that the tangent lines pass through the origin?","f(x,y)=1 (a,b) (a,b) \nabla f(a, b) = \left( 2(a-3), \frac{(b-4)}{2}\right), (a,b) \left(-\frac{(b-4)}{2}, 2(a-3)\right), k\left(-\frac{(b-4)}{2}, 2(a-3)\right), k\in \mathbb Z. \vec r(t)=(\cos t +3, 2\sin t +4), \vec r(t) = (-2\sin t,\cos t).","['linear-algebra', 'multivariable-calculus', 'conic-sections', 'parametrization']"
16,"Total derivative of f(x, g(x, y)) and its approximation","Total derivative of f(x, g(x, y)) and its approximation",,"I understand the steps to calculate the total derivative of f(x, g(x)) Related: Derivative of $f(x, g(x))$ with respect to $x$ I have three sub-questions related to calculating the total derivative of f(x, g(x, y)), (1) How do I calculate its total derivative, here's my attempt: $$ df=\Big(\frac{\partial{f}}{\partial x}+\frac{\partial{g}}{\partial x}\Big)dx+\frac{\partial{f}}{\partial y}dy $$ So applying a simple example of f(x, x+y) where g(x,y)=x+y $$ df=\Big(\frac{\partial{f}}{\partial x}+1\Big)dx+\frac{\partial{f}}{\partial y}dy $$ (2) Why do I not need to consider higher order terms? Looking at Taylor Series would it make it more accurate? (3) In terms of approximating the total derivative, is this logic correct? $$ df(x, x+y) = f(x+\Delta x, y+\Delta y) - f(x,y) \approx \Big(\frac{f(x+\Delta x, x+\Delta x + y)-f(x, x+y)}{\Delta x}+1\Big)\Delta x + \Big(\frac{f(x, x + y + \Delta y)-f(x, x+y)}{\Delta y}\Big)\Delta y $$","I understand the steps to calculate the total derivative of f(x, g(x)) Related: Derivative of $f(x, g(x))$ with respect to $x$ I have three sub-questions related to calculating the total derivative of f(x, g(x, y)), (1) How do I calculate its total derivative, here's my attempt: So applying a simple example of f(x, x+y) where g(x,y)=x+y (2) Why do I not need to consider higher order terms? Looking at Taylor Series would it make it more accurate? (3) In terms of approximating the total derivative, is this logic correct?","
df=\Big(\frac{\partial{f}}{\partial x}+\frac{\partial{g}}{\partial x}\Big)dx+\frac{\partial{f}}{\partial y}dy
 
df=\Big(\frac{\partial{f}}{\partial x}+1\Big)dx+\frac{\partial{f}}{\partial y}dy
 
df(x, x+y) = f(x+\Delta x, y+\Delta y) - f(x,y) \approx \Big(\frac{f(x+\Delta x, x+\Delta x + y)-f(x, x+y)}{\Delta x}+1\Big)\Delta x + \Big(\frac{f(x, x + y + \Delta y)-f(x, x+y)}{\Delta y}\Big)\Delta y
","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'implicit-differentiation']"
17,Prove the sign and zeroes of $Ax^2 + 2Bxy + Cy^2$ (without using the second derivative test),Prove the sign and zeroes of  (without using the second derivative test),Ax^2 + 2Bxy + Cy^2,"Let $$P(x,y) = Ax^2 + 2Bxy + Cy^2 \\A \neq 0 \quad x,y \in \mathbb R.$$ Without using the second derivative test, prove that If $AC - B^2 > 0$ , then (i) $P$ has no zeroes outside the origin and (ii) $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ for all $x,y$ If $AC - B^2 = 0$ , then (i) $P$ has zeroes outside the origin and (ii) outside these zeroes, $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ If $AC - B^2 < 0$ , then (i) $P$ has zeroes outside the origin (ii) there exist $x,y$ such that $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ and (iii) there also exist $x,y$ such that $\operatorname{sgn}(P) = -\operatorname{sgn}(A)$ Please note: The second derivative test of calculus is not available, as this question is intended as a lemma for proving the second derivative test This post is related but distinct My proof is below: I request verification, critique, or alternate proofs. Let $$f(x,y) = (x -\frac B A y)^2 \\g(y) = \left(\frac y A \right)^2.$$ Then $$P(x,y) = A \cdot [f(x,y) + (AC-B^2)g(y)]\\ \operatorname{sgn}(P(x,y)) = \operatorname{sgn}(A) \cdot \operatorname{sgn}[f(x,y) + (AC-B^2)g(y)].$$ Observe that the range of both $f$ and $g$ is $[0, \infty)$ . Case 1: If $AC - B^2 > 0$ , then $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ unless both $f$ and $g$ are zero.  But $g(y) = 0$ implies $y = 0$ , and $f(x,0) = 0$ implies $x = 0$ , so this can only happen at the origin. Case 2: If $AC - B^2 = 0$ , then $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ unless $f$ is zero, which happens for infinite values of $x, y$ . Case 3: If $AC - B^2 < 0$ , then for $|x| \gg |y|$ , the first term dominates, and $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ , and for $|y| \gg |x|$ , the second term dominates, and $\operatorname{sgn}(P) = -\operatorname{sgn}(A)$ .  Furthermore, for any fixed $y$ , then varying $x$ causes $f$ to take on its entire range, so there exist infinite values of $x,y$ which make $P$ zero.","Let Without using the second derivative test, prove that If , then (i) has no zeroes outside the origin and (ii) for all If , then (i) has zeroes outside the origin and (ii) outside these zeroes, If , then (i) has zeroes outside the origin (ii) there exist such that and (iii) there also exist such that Please note: The second derivative test of calculus is not available, as this question is intended as a lemma for proving the second derivative test This post is related but distinct My proof is below: I request verification, critique, or alternate proofs. Let Then Observe that the range of both and is . Case 1: If , then unless both and are zero.  But implies , and implies , so this can only happen at the origin. Case 2: If , then unless is zero, which happens for infinite values of . Case 3: If , then for , the first term dominates, and , and for , the second term dominates, and .  Furthermore, for any fixed , then varying causes to take on its entire range, so there exist infinite values of which make zero.","P(x,y) = Ax^2 + 2Bxy + Cy^2 \\A \neq 0 \quad x,y \in \mathbb R. AC - B^2 > 0 P \operatorname{sgn}(P) = \operatorname{sgn}(A) x,y AC - B^2 = 0 P \operatorname{sgn}(P) = \operatorname{sgn}(A) AC - B^2 < 0 P x,y \operatorname{sgn}(P) = \operatorname{sgn}(A) x,y \operatorname{sgn}(P) = -\operatorname{sgn}(A) f(x,y) = (x -\frac B A y)^2 \\g(y) = \left(\frac y A \right)^2. P(x,y) = A \cdot [f(x,y) + (AC-B^2)g(y)]\\ \operatorname{sgn}(P(x,y)) = \operatorname{sgn}(A) \cdot \operatorname{sgn}[f(x,y) + (AC-B^2)g(y)]. f g [0, \infty) AC - B^2 > 0 \operatorname{sgn}(P) = \operatorname{sgn}(A) f g g(y) = 0 y = 0 f(x,0) = 0 x = 0 AC - B^2 = 0 \operatorname{sgn}(P) = \operatorname{sgn}(A) f x, y AC - B^2 < 0 |x| \gg |y| \operatorname{sgn}(P) = \operatorname{sgn}(A) |y| \gg |x| \operatorname{sgn}(P) = -\operatorname{sgn}(A) y x f x,y P","['calculus', 'multivariable-calculus', 'polynomials', 'optimization', 'discriminant']"
18,Prove that $f(x) = \|x\|^2$ is smooth using the definition of total derivative,Prove that  is smooth using the definition of total derivative,f(x) = \|x\|^2,"Consider the function $f : \Bbb R^n \to \Bbb R$ given by $f(x) = \|x\|^2$ , for $x \in \Bbb R^n$ . Here $\|.\|$ is the Euclidean norm on $\Bbb R^n$ . Answer the following questions from the definition of the (total) derivative, i.e. without using partial derivatives. (a) Prove that $f$ is differentiable and calculate its derivative. (b) Prove that $Df$ is differentiable and calculate its derivative. (Hint: identify $Lin(\Bbb R^n; \Bbb R)$ with $\Bbb R^n$ via the inner product.) (c) Prove that $f$ is smooth My attempt: (a) Using the definition of total derivative $f(a+v)=\|a+v\|^2=\|a\|^2+\|v\|^2 + 2a^Tv=f(a)+\|v\|^2 + 2a^Tv$ $\lim_{v\to  0}\frac{|f(a+v)-f(a)-Lv|}{\|v\|}=\lim_{v\to  0}\frac{|\|v\|^2 + 2a^Tv-Lv|}{\|v\|}=0 $ if I take $L=2a^T$ ,which is a linear map $L(x)=2a^Tx, x \in \Bbb R^n$ . So $f$ is differentiable at $a$ , and therefore differentiable (meaning at any point) and the derivative is $Df(a)=2a^T$ i.e. $Df(a)(h)=2a^Th, h \in \Bbb R^n$ (b) I think they should ask about the derivative of Df(a) and not about the derivative of Df which is another thing (the map that assignes to each point the linear map called derivative at that point. the derivative of that map should be 0 since it is always the same map for every point) So let $g:=Df(a)=2a^T$ I want to calculate the derivative of $g $ at some point $\tilde a\in\Bbb R^n$ ie $Dg(\tilde a)$ , for this I need a $v\in\Bbb R^n $ in the definition $\lim_{v\to  0}\frac{|g(\tilde  a +v)-g(\tilde a)-Lv|}{\|v\|}$ $=\lim_{v\to  0}\frac{|Df(a)(\tilde a+v)-Df(a)(\tilde a)-Lv|}{\|v\|}$ $=\lim_{v\to  0}\frac{|Df(a)(\tilde a)+Df(a)(v)-Df(a)(\tilde a)-Lv|}{\|v\|}$ $=\lim_{v\to  0}\frac{|Df(a)( v)-Lv|}{\|v\|}$ if I take $ L=Df(a)$ ,which is a linear map $L(h)=Df(a)h, x \in \Bbb R^n$ I get that $D^2f(\tilde a)=D(Df(a))(\tilde a)=D(g)(\tilde a)=L=Df(a) \tag {*} $ My questions are (1) I think (a) is OK. What about (b) ? I am specially unsure about the correctness of the notation at (*).Was I right to fix the question? Should they have asked about the derivative of Df(a) ? I thought so because that is what I plugged in in the definition of total derivative at (b) in order to get the correct answer as stated above. This was my earlier attempt: In (a) I plugged in $f $ in the definition, and got the right answer In (b) I was tempted to blindly do the same and plug in $g=Df$ ,because they are asking to assess the differentiability of $Df$ but that was resulting in a absurd: In the denominator I was getting $|g(\tilde a+v)-g(\tilde a)-Lv|=|Df(\tilde a+v)-Df(\tilde a)-Lv|$ (and usign that $Df(a)=2a^T$ :) $=|2 (\tilde a+v)^T-2\tilde a^T-Lv|=|2v^T-Lv|$ which is absurd This is what the definition yields if I apply it verbatim. Therefore I changed the question and though that it should $Df(a)$ instead of $Df$ Moreover I am not sure I am using the hint here, am I? (2) How do I prove (c) ? I am unsure how to do this since I am not supposed to use partial derivatives. The definitions I am supposed to use are those in Appendix C of Lee's introduction to smooth manifolds, where smooth was  defined with partial derivatives as seen below: definition of smooth function: definition of total derivative: Edit: Rewriting (b) following the suggestions: $g=Df$ $|g(\tilde a+v)-g(\tilde a)-Lv|=|Df(\tilde a+v)-Df(\tilde a)-Lv|$ (and usign that $Df(a)(u)=2a^Tu=<a,u>$ but here I am unsure how to write Df(a)=? I can only thik of 2a^T, but I was advised to use Df(a)(u)=<a,u> so that I don't have to worry about the transpose but I am not sure how I can write Df(a) using <,> and not writting u, maybe $Df(a)=<a,\cdot>$ ) $=|2 <\tilde a+v,\cdot>-2<\tilde a,\cdot>-Lv|=|2<v,\cdot>-Lv|$ then $\lim_{v\to 0}\frac{|2<v,\cdot>-Lv|}{\|v\|}=0$ if $Lv=2<v,\cdot>$ which is a linear map $v \mapsto 2<v,\cdot> $ , But I was expecting to find the constant map Lv=2 instead,... how do I fix this?","Consider the function given by , for . Here is the Euclidean norm on . Answer the following questions from the definition of the (total) derivative, i.e. without using partial derivatives. (a) Prove that is differentiable and calculate its derivative. (b) Prove that is differentiable and calculate its derivative. (Hint: identify with via the inner product.) (c) Prove that is smooth My attempt: (a) Using the definition of total derivative if I take ,which is a linear map . So is differentiable at , and therefore differentiable (meaning at any point) and the derivative is i.e. (b) I think they should ask about the derivative of Df(a) and not about the derivative of Df which is another thing (the map that assignes to each point the linear map called derivative at that point. the derivative of that map should be 0 since it is always the same map for every point) So let I want to calculate the derivative of at some point ie , for this I need a in the definition if I take ,which is a linear map I get that My questions are (1) I think (a) is OK. What about (b) ? I am specially unsure about the correctness of the notation at (*).Was I right to fix the question? Should they have asked about the derivative of Df(a) ? I thought so because that is what I plugged in in the definition of total derivative at (b) in order to get the correct answer as stated above. This was my earlier attempt: In (a) I plugged in in the definition, and got the right answer In (b) I was tempted to blindly do the same and plug in ,because they are asking to assess the differentiability of but that was resulting in a absurd: In the denominator I was getting (and usign that :) which is absurd This is what the definition yields if I apply it verbatim. Therefore I changed the question and though that it should instead of Moreover I am not sure I am using the hint here, am I? (2) How do I prove (c) ? I am unsure how to do this since I am not supposed to use partial derivatives. The definitions I am supposed to use are those in Appendix C of Lee's introduction to smooth manifolds, where smooth was  defined with partial derivatives as seen below: definition of smooth function: definition of total derivative: Edit: Rewriting (b) following the suggestions: (and usign that but here I am unsure how to write Df(a)=? I can only thik of 2a^T, but I was advised to use Df(a)(u)=<a,u> so that I don't have to worry about the transpose but I am not sure how I can write Df(a) using <,> and not writting u, maybe ) then if which is a linear map , But I was expecting to find the constant map Lv=2 instead,... how do I fix this?","f : \Bbb R^n \to \Bbb R f(x) = \|x\|^2 x \in \Bbb R^n \|.\| \Bbb R^n f Df Lin(\Bbb R^n; \Bbb R) \Bbb R^n f f(a+v)=\|a+v\|^2=\|a\|^2+\|v\|^2 + 2a^Tv=f(a)+\|v\|^2 + 2a^Tv \lim_{v\to  0}\frac{|f(a+v)-f(a)-Lv|}{\|v\|}=\lim_{v\to  0}\frac{|\|v\|^2 + 2a^Tv-Lv|}{\|v\|}=0  L=2a^T L(x)=2a^Tx, x \in \Bbb R^n f a Df(a)=2a^T Df(a)(h)=2a^Th, h \in \Bbb R^n g:=Df(a)=2a^T g  \tilde a\in\Bbb R^n Dg(\tilde a) v\in\Bbb R^n  \lim_{v\to  0}\frac{|g(\tilde  a +v)-g(\tilde a)-Lv|}{\|v\|} =\lim_{v\to  0}\frac{|Df(a)(\tilde a+v)-Df(a)(\tilde a)-Lv|}{\|v\|} =\lim_{v\to  0}\frac{|Df(a)(\tilde a)+Df(a)(v)-Df(a)(\tilde a)-Lv|}{\|v\|} =\lim_{v\to  0}\frac{|Df(a)( v)-Lv|}{\|v\|}  L=Df(a) L(h)=Df(a)h, x \in \Bbb R^n D^2f(\tilde a)=D(Df(a))(\tilde a)=D(g)(\tilde a)=L=Df(a) \tag {*}  f  g=Df Df |g(\tilde a+v)-g(\tilde a)-Lv|=|Df(\tilde a+v)-Df(\tilde a)-Lv| Df(a)=2a^T =|2 (\tilde a+v)^T-2\tilde a^T-Lv|=|2v^T-Lv| Df(a) Df g=Df |g(\tilde a+v)-g(\tilde a)-Lv|=|Df(\tilde a+v)-Df(\tilde a)-Lv| Df(a)(u)=2a^Tu=<a,u> Df(a)=<a,\cdot> =|2 <\tilde a+v,\cdot>-2<\tilde a,\cdot>-Lv|=|2<v,\cdot>-Lv| \lim_{v\to 0}\frac{|2<v,\cdot>-Lv|}{\|v\|}=0 Lv=2<v,\cdot> v \mapsto 2<v,\cdot> ","['linear-algebra', 'multivariable-calculus', 'differential-geometry', 'smooth-manifolds']"
19,Multi variable integral : $\int_{0}^{1}\int_{0}^{\sqrt{1-x^2}} \frac{e^y}{\sqrt{1-x^2-y^2}} dy dx$,Multi variable integral :,\int_{0}^{1}\int_{0}^{\sqrt{1-x^2}} \frac{e^y}{\sqrt{1-x^2-y^2}} dy dx,"How can I solve the following multi variable integral: $\int_{0}^{1}\int_{0}^{\sqrt{1-x^2}} \frac{e^y}{\sqrt{1-x^2-y^2}} dy dx$ I have seen it in an exam. I tried to rewrite it in polar coordinates as follows, but to no avail. $x^2+y^2 = r ^2$ $y = r.sin(\theta)$ $x = r.cos(\theta)$ $dydx =  r dr d\theta$ $\int_{0}^{\pi/2}\int_{0}^{1} \frac{e^{r.sin\theta}}{\sqrt{1-r^2}} r dr d\theta$","How can I solve the following multi variable integral: I have seen it in an exam. I tried to rewrite it in polar coordinates as follows, but to no avail.",\int_{0}^{1}\int_{0}^{\sqrt{1-x^2}} \frac{e^y}{\sqrt{1-x^2-y^2}} dy dx x^2+y^2 = r ^2 y = r.sin(\theta) x = r.cos(\theta) dydx =  r dr d\theta \int_{0}^{\pi/2}\int_{0}^{1} \frac{e^{r.sin\theta}}{\sqrt{1-r^2}} r dr d\theta,['multivariable-calculus']
20,Prove Kepler's second law of planetary motion,Prove Kepler's second law of planetary motion,,"An object moves in $\mathbb R^3$ it's position $r(t)$ satisfies $$r''(t) = s(t)r(t)$$ for some scalar function $s$ (a central force field, in which all acceleration is directly towards or opposite the origin).  Show that (i) it's motion is confined to a plane and (ii) it sweeps out equal areas in equal time (Kepler's second law). My solution is below, to which I request verification and suggestions. Of course, solutions exist, as this problem is centuries old; this question is to verify or improve this solution. Solution: We first prove claim (i): the object's motion is confined to a plane. The object starts with arbitrary position $r(0)$ and velocity $r'(0)$ .  Let their span (which is a plane, a line, or the origin) be $S$ .  Clearly, $r''(0) \in S$ .  Since $r''$ has no component orthogonal to $S$ , $r'$ remains in $S$ , and therefore $r$ remains in $S$ . [1] To prove claim (ii), we introduce $x(t), y(t), z(t),$ and $s(t)$ and, to lighten notation, will omit writing the $(t)$ .  Assume WLOG that the object's plane is the $xy$ plane so $z = 0$ .  We then have $$\frac{\partial^2 x}{\partial t^2} = sx \\ \frac{\partial^2 y}{\partial t^2} = sy \\ z = 0.$$ The rate that area is swept, $a(t)$ , is proportional to $r(t) \times r'(t)$ , so $$a(t)k = x \frac{\partial y}{\partial t} - y \frac{\partial x}{\partial t}.$$ But $$\begin{align*} \frac{\partial a}{\partial t} &= x\frac{\partial^2 y}{\partial t^2}+\frac{\partial x}{\partial t}\frac{\partial y}{\partial t} - \frac{\partial y}{\partial t}\frac{\partial x}{\partial t} -  y\frac{\partial^2 x}{\partial t^2}\\ &= xsy - ysx \\ &= 0 \end{align*}$$ and therefore $a(t)$ is constant, completing the proof. Note: [1] While geometrically clear, I'm having trouble making this point rigorous.  I believe it requires the MVT, but I couldn't apply it successfully.  I also tried some form of ""continuous induction"" using arbitrary small values of $\Delta t$ , but this failed to achieve rigor as well.  Or is how I've written it good enough?","An object moves in it's position satisfies for some scalar function (a central force field, in which all acceleration is directly towards or opposite the origin).  Show that (i) it's motion is confined to a plane and (ii) it sweeps out equal areas in equal time (Kepler's second law). My solution is below, to which I request verification and suggestions. Of course, solutions exist, as this problem is centuries old; this question is to verify or improve this solution. Solution: We first prove claim (i): the object's motion is confined to a plane. The object starts with arbitrary position and velocity .  Let their span (which is a plane, a line, or the origin) be .  Clearly, .  Since has no component orthogonal to , remains in , and therefore remains in . [1] To prove claim (ii), we introduce and and, to lighten notation, will omit writing the .  Assume WLOG that the object's plane is the plane so .  We then have The rate that area is swept, , is proportional to , so But and therefore is constant, completing the proof. Note: [1] While geometrically clear, I'm having trouble making this point rigorous.  I believe it requires the MVT, but I couldn't apply it successfully.  I also tried some form of ""continuous induction"" using arbitrary small values of , but this failed to achieve rigor as well.  Or is how I've written it good enough?","\mathbb R^3 r(t) r''(t) = s(t)r(t) s r(0) r'(0) S r''(0) \in S r'' S r' S r S x(t), y(t), z(t), s(t) (t) xy z = 0 \frac{\partial^2 x}{\partial t^2} = sx \\ \frac{\partial^2 y}{\partial t^2} = sy \\ z = 0. a(t) r(t) \times r'(t) a(t)k = x \frac{\partial y}{\partial t} - y \frac{\partial x}{\partial t}. \begin{align*}
\frac{\partial a}{\partial t} &= x\frac{\partial^2 y}{\partial t^2}+\frac{\partial x}{\partial t}\frac{\partial y}{\partial t} - \frac{\partial y}{\partial t}\frac{\partial x}{\partial t} -  y\frac{\partial^2 x}{\partial t^2}\\
&= xsy - ysx \\
&= 0
\end{align*} a(t) \Delta t","['multivariable-calculus', 'solution-verification', 'vector-analysis', 'physics', 'mathematical-astronomy']"
21,Integration by parts - multivariate case,Integration by parts - multivariate case,,"As part of a larger problem, I realised I don't quite know how to do integration by parts in the multivariate case. I looked up some formulas, but I couldn't get them to apply. For example, how would you do integration by parts on: $$\int_{0}^{1}\int_{0}^{1}u_{xx} v\;dxdy$$ where $u$ and $v$ are functions of $x$ and $y$ and we are integrating across a unit square. The subscripts denote partial differentiation with respect to that variable. Along the boundary of the square we have $u=v$ . Many thanks!","As part of a larger problem, I realised I don't quite know how to do integration by parts in the multivariate case. I looked up some formulas, but I couldn't get them to apply. For example, how would you do integration by parts on: where and are functions of and and we are integrating across a unit square. The subscripts denote partial differentiation with respect to that variable. Along the boundary of the square we have . Many thanks!",\int_{0}^{1}\int_{0}^{1}u_{xx} v\;dxdy u v x y u=v,['multivariable-calculus']
22,"Gradients of $(u, v) \mapsto \frac12 \left\| A - u v^T \right\|_{\text{F}}^2$ via the chain rule",Gradients of  via the chain rule,"(u, v) \mapsto \frac12 \left\| A - u v^T \right\|_{\text{F}}^2","Given the matrix $A \in {\Bbb R}^{n \times m}$ , let the scalar field $f : {\Bbb R}^n \times {\Bbb R}^m \to {\Bbb R}_0^+$ be defined by $$ f (u, v) : = \frac12 \left\| A - u v^T \right\|_{\text{F}}^2 $$ where $\| \cdot \|_{\text{F}}$ denotes the Frobenius norm. Find the gradients $\nabla_u f$ and $\nabla_v f$ . Solution: Let $R=A-uv^T$ . The gradients are $\nabla_u f = - R v$ and $\nabla_v f = - R^T u $ . I am struggling. I have read about the differentials method and I have tried to apply rules from The Matrix Cookbook , but I always have problems with transposed matrices in the result. Is there a systematic way to solve these problems without making use of summations? If you can solve the derivative it would be great, but it would also be awesome if you just point me out to somewhere where this is explained properly","Given the matrix , let the scalar field be defined by where denotes the Frobenius norm. Find the gradients and . Solution: Let . The gradients are and . I am struggling. I have read about the differentials method and I have tried to apply rules from The Matrix Cookbook , but I always have problems with transposed matrices in the result. Is there a systematic way to solve these problems without making use of summations? If you can solve the derivative it would be great, but it would also be awesome if you just point me out to somewhere where this is explained properly","A \in {\Bbb R}^{n \times m} f : {\Bbb R}^n \times {\Bbb R}^m \to {\Bbb R}_0^+  f (u, v) : = \frac12 \left\| A - u v^T \right\|_{\text{F}}^2  \| \cdot \|_{\text{F}} \nabla_u f \nabla_v f R=A-uv^T \nabla_u f = - R v \nabla_v f = - R^T u ","['multivariable-calculus', 'derivatives', 'matrix-calculus', 'chain-rule', 'scalar-fields']"
23,multiple integral on domain,multiple integral on domain,,"The Question : Calculate $$\int_A z \ dx \ dy \ dz$$ Where $A$ is the cone : $\ A=\{0\leq z \leq 1,\ z^2\geq x^2+y^2 \}$ My try : I first try to establish the bounds. $z$ is from $0$ to $1$ . Once $z$ is fixed, I changed for polar coordinates and so: $z\geq r$ . We also know that a radius is positive so I deduced $r$ is from $0$ to $z$ . I finally set $\theta$ from $0$ to $2\pi$ because my cone is a full one I guess. It gives me : $$\int_0^1\left(\int_0^{2\pi} \left( \int_0^z zr\ dr \right) d\theta \right)dz$$ $$=\int_0^1\left(\int_0^{2\pi} \frac{z^3}{2} d\theta \right)dz$$ $$=2\pi \int_0^1 \frac{z^3}{2} dz$$ $$=\frac{2\pi}{8}$$ I have the feeling something is wrong, mostly when I find the bounds (the deductions about the bounds of $\theta$ and $r$ ). The difficulty for me in this case is that it's very difficult to have a visual interpretation. My question: Is my bounds searching process good or am I in the wrong direction ?","The Question : Calculate Where is the cone : My try : I first try to establish the bounds. is from to . Once is fixed, I changed for polar coordinates and so: . We also know that a radius is positive so I deduced is from to . I finally set from to because my cone is a full one I guess. It gives me : I have the feeling something is wrong, mostly when I find the bounds (the deductions about the bounds of and ). The difficulty for me in this case is that it's very difficult to have a visual interpretation. My question: Is my bounds searching process good or am I in the wrong direction ?","\int_A z \ dx \ dy \ dz A \ A=\{0\leq z \leq 1,\ z^2\geq x^2+y^2 \} z 0 1 z z\geq r r 0 z \theta 0 2\pi \int_0^1\left(\int_0^{2\pi} \left( \int_0^z zr\ dr \right) d\theta \right)dz =\int_0^1\left(\int_0^{2\pi} \frac{z^3}{2} d\theta \right)dz =2\pi \int_0^1 \frac{z^3}{2} dz =\frac{2\pi}{8} \theta r","['integration', 'multivariable-calculus', 'multiple-integral']"
24,Value of mixed partial derivatives at critical points.,Value of mixed partial derivatives at critical points.,,"Suppose $f(x,y):\mathbb{R}^2\rightarrow\mathbb{R}$ is smooth, and $$\left.\frac{\partial f}{\partial x}\right\vert_{(x_0,y_0)}=0\;,\qquad \left.\frac{\partial f}{\partial y}\right\vert_{(x_0,y_0)}=0$$ Can I claim : $$\left.\frac{\partial^2f}{\partial x\partial y}\right|_{(x_0,y_0)}=0$$ My intuition is that this is valid, and I have yet to find a conter-example. Here is my intuition: At extremas, i.e minimas and maximas, the gradient is 0. Taking a small step in any direction from the extrema (say in the x direction), the direction of the maximum slope will be either towards or away from the extremum (depending on whether it is either a maxima or minima). Then the slope in the direction perpendicular to that (y direction) is 0, hence $\left.\frac{\partial^2f}{\partial x\partial y}\right|_{(x_0,y_0)}=0$ I think the same logic is valid for saddle points as well, but I'm not sure. Thanks in advance! Edit: A counter-example has been given in the comments for this question (a saddle-point), now my question is basically if there is anything wrong with my logic for extremas.","Suppose is smooth, and Can I claim : My intuition is that this is valid, and I have yet to find a conter-example. Here is my intuition: At extremas, i.e minimas and maximas, the gradient is 0. Taking a small step in any direction from the extrema (say in the x direction), the direction of the maximum slope will be either towards or away from the extremum (depending on whether it is either a maxima or minima). Then the slope in the direction perpendicular to that (y direction) is 0, hence I think the same logic is valid for saddle points as well, but I'm not sure. Thanks in advance! Edit: A counter-example has been given in the comments for this question (a saddle-point), now my question is basically if there is anything wrong with my logic for extremas.","f(x,y):\mathbb{R}^2\rightarrow\mathbb{R} \left.\frac{\partial f}{\partial x}\right\vert_{(x_0,y_0)}=0\;,\qquad
\left.\frac{\partial f}{\partial y}\right\vert_{(x_0,y_0)}=0 \left.\frac{\partial^2f}{\partial x\partial y}\right|_{(x_0,y_0)}=0 \left.\frac{\partial^2f}{\partial x\partial y}\right|_{(x_0,y_0)}=0","['calculus', 'multivariable-calculus', 'partial-derivative']"
25,"Green's Theorem does not check out, spot the mistake.","Green's Theorem does not check out, spot the mistake.",,"We have a square on the plane of sides 2 from (-1,-1) to (1,1), and $P(x,y)=x^2+y^2,Q(x,y)=2x^2y$ . $$ \oint_L (x^2+y^2)dx+2x^2ydy=2\int_{-1}^1(x^2+1)dx+2\int_{-1}^12ydy=\frac{16}{3}+0=\frac{16}{3} $$ as on the horizontal lines $dy=0$ and $y=\pm1$ and in the direction [1,-1] $dx<0$ , and same reasoning on the verticals. With Green's Theorem the line integral becomes $$ \int_{-1}^1\int_{-1}^1(4xy-2y)dxdy=\int_{-1}^12ydy\int_{-1}^1(2x-1)dx=0\int_{-1}^1(2x-1)dx=0. $$ I think I made a mistake in the last Green's Theorem calculation, as to another nameless theorem (can't post the proof here as it's 2 pages in my textbook): For the closed line integral inside a simply connected region $S$ $$ \oint_L P(x,y)dx+Q(x,y)dy=0\iff \frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}\iff 2y=4xy, $$ which is not the case. But I am not sure where my mistake is, as I verified all conditions and calculations.","We have a square on the plane of sides 2 from (-1,-1) to (1,1), and . as on the horizontal lines and and in the direction [1,-1] , and same reasoning on the verticals. With Green's Theorem the line integral becomes I think I made a mistake in the last Green's Theorem calculation, as to another nameless theorem (can't post the proof here as it's 2 pages in my textbook): For the closed line integral inside a simply connected region which is not the case. But I am not sure where my mistake is, as I verified all conditions and calculations.","P(x,y)=x^2+y^2,Q(x,y)=2x^2y 
\oint_L (x^2+y^2)dx+2x^2ydy=2\int_{-1}^1(x^2+1)dx+2\int_{-1}^12ydy=\frac{16}{3}+0=\frac{16}{3}
 dy=0 y=\pm1 dx<0 
\int_{-1}^1\int_{-1}^1(4xy-2y)dxdy=\int_{-1}^12ydy\int_{-1}^1(2x-1)dx=0\int_{-1}^1(2x-1)dx=0.
 S 
\oint_L P(x,y)dx+Q(x,y)dy=0\iff \frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}\iff 2y=4xy,
","['multivariable-calculus', 'greens-theorem']"
26,Is the nearest point on a level set always in the direction of the gradient?,Is the nearest point on a level set always in the direction of the gradient?,,"Suppose I have a smooth function $f:\mathbb{R}^2\to \mathbb{R}$ and consider two level sets $$ \begin{align}     L_1 &= \{x \,:\, f(x) = c_1\} \\     L_2 &= \{x \,:\, f(x) = c_2\} \end{align} $$ Now take a point $p_1\in L_1$ , and consider the nearest neighbour $p_2\in L_2$ . Is it true that $p_2$ is always in the direction of the gradient at $p_1$ ? In other words, there always exists $\gamma \in\mathbb{R}$ such that $$ p_2 = p_1 + \gamma\nabla f(p_1) $$ Attempted Solution Consider $$ d(p_1, x) = \|p_1 - x\|^2 = \|p_1\|^2 - 2p_1^\top x + \|x\|^2 $$ I want to minimize this with respect to $x$ constrained with $f(x) = c_2$ . The lagrangian is $$ \mathcal{L} = d(p_1, x)  + \lambda(f(x) - c_2) $$ Take the derivative with respect to $x$ $$ \nabla_x d(p_1, x) = -2p_1 + 2x + \lambda \nabla f(x) $$ Setting this to zero gives $$ x = p_1 - \frac{1}{2}\lambda \nabla f(x) $$ But now the gradient is evaluated at $x$ for some reason.. which leads me to believe $$ p_2 = p_1 + \gamma \nabla f(p_2) $$ rather than $$ p_2 = p_1 + \gamma \nabla f(p_1) $$ Regarding regions of high curvature","Suppose I have a smooth function and consider two level sets Now take a point , and consider the nearest neighbour . Is it true that is always in the direction of the gradient at ? In other words, there always exists such that Attempted Solution Consider I want to minimize this with respect to constrained with . The lagrangian is Take the derivative with respect to Setting this to zero gives But now the gradient is evaluated at for some reason.. which leads me to believe rather than Regarding regions of high curvature","f:\mathbb{R}^2\to \mathbb{R} 
\begin{align}
    L_1 &= \{x \,:\, f(x) = c_1\} \\
    L_2 &= \{x \,:\, f(x) = c_2\}
\end{align}
 p_1\in L_1 p_2\in L_2 p_2 p_1 \gamma \in\mathbb{R} 
p_2 = p_1 + \gamma\nabla f(p_1)
 
d(p_1, x) = \|p_1 - x\|^2 = \|p_1\|^2 - 2p_1^\top x + \|x\|^2
 x f(x) = c_2 
\mathcal{L} = d(p_1, x)  + \lambda(f(x) - c_2)
 x 
\nabla_x d(p_1, x) = -2p_1 + 2x + \lambda \nabla f(x)
 
x = p_1 - \frac{1}{2}\lambda \nabla f(x)
 x 
p_2 = p_1 + \gamma \nabla f(p_2)
 
p_2 = p_1 + \gamma \nabla f(p_1)
","['calculus', 'multivariable-calculus', 'differential-geometry']"
27,Show that in gradient descent the gradient goes against 0 (Under certain conditions),Show that in gradient descent the gradient goes against 0 (Under certain conditions),,"So we have gradient descent: $$x^{(i+1)} = x^{(i)} - \tau \nabla f(x^{(i)})$$ And we gotta show that $$\left|\nabla f\left(x^{(j)}\right)\right| \to 0$$ The conditions are: $f: \mathbb R^n \to \mathbb R$ is differentiable $\nabla f: \mathbb R^n \to \mathbb R^n$ is lipschitz continuous, so there exists an $L > 0$ such that: $$|\nabla f(x)-\nabla f(y)| \leq L|x-y|,\ \forall x,y \in \mathbb R^n$$ $\tau < \frac{1}{L}$ I've been trying to solve this for like an hour and can't get any further, can someone point me in the right direction? I've been trying the following: Assume there exists an $\epsilon > 0$ , such that $|\nabla f(x^{(j)})| \geq \epsilon$ for infinitely many j but I haven't been able to find any contradiction with the assumptions.","So we have gradient descent: And we gotta show that The conditions are: is differentiable is lipschitz continuous, so there exists an such that: I've been trying to solve this for like an hour and can't get any further, can someone point me in the right direction? I've been trying the following: Assume there exists an , such that for infinitely many j but I haven't been able to find any contradiction with the assumptions.","x^{(i+1)} = x^{(i)} - \tau \nabla f(x^{(i)}) \left|\nabla f\left(x^{(j)}\right)\right| \to 0 f: \mathbb R^n \to \mathbb R \nabla f: \mathbb R^n \to \mathbb R^n L > 0 |\nabla f(x)-\nabla f(y)| \leq L|x-y|,\ \forall x,y \in \mathbb R^n \tau < \frac{1}{L} \epsilon > 0 |\nabla f(x^{(j)})| \geq \epsilon","['multivariable-calculus', 'continuity', 'lipschitz-functions', 'numerical-optimization', 'gradient-descent']"
28,Stokes theorem applied to planes,Stokes theorem applied to planes,,"Let $\textbf{F} = (y+z,-xz,y^2)$ . Let $S$ be the surface above the $xy$ plane and bounded by $2x + z = 6$ , $y = 2$ , $y = 0$ and $x = 0$ . Calculate $$\iint_S \text{curl } \textbf{F} \cdot d\textbf{S}$$ I wanted to do this two different ways, one involving Stokes' theorem and the other just manually doing each face. However, I don't have any idea how to parameterise it for the manual case. When I tried using Stokes' theorem, I trace the boundaries $$(3,2,0) \to (0,2,6) \to (0,0,6) \to (3,0,0) \to (3,2,0)$$ counterclockwise, but that gives me $9 + 0 + 9 + 0 = 18$ which is wrong. Apparently the correct answer is $-6$ . Any help would be greatly appreciated.","Let . Let be the surface above the plane and bounded by , , and . Calculate I wanted to do this two different ways, one involving Stokes' theorem and the other just manually doing each face. However, I don't have any idea how to parameterise it for the manual case. When I tried using Stokes' theorem, I trace the boundaries counterclockwise, but that gives me which is wrong. Apparently the correct answer is . Any help would be greatly appreciated.","\textbf{F} = (y+z,-xz,y^2) S xy 2x + z = 6 y = 2 y = 0 x = 0 \iint_S \text{curl } \textbf{F} \cdot d\textbf{S} (3,2,0) \to (0,2,6) \to (0,0,6) \to (3,0,0) \to (3,2,0) 9 + 0 + 9 + 0 = 18 -6","['multivariable-calculus', 'multiple-integral', 'stokes-theorem', 'curl']"
29,How to change coordinates and variables when doing double integral?,How to change coordinates and variables when doing double integral?,,"Let D be the bounded region in the first quadrant of $R^2$ bounded by curves $x^2 + 16y^2 = 16, x^2 + 16y^2 = 1, x = y$ and the positive y−axis. Describe $D$ in the $(u, v)$ −plane when $u = x^2 + 16y^2$ and $v =\frac yx,$ and calculate the integral $$\iint_{D} \frac{y}{x} dA$$ So in order to solve the integral I used the new coordinates $1\leq u \leq 16$ and $1\leq v \leq \infty$ . Afterwards I calculated the the determinant of the jacobian and got $$ |\det J| = 1+32\left(\frac{y}{x}\right)^2 = 2 + 32v^2$$ Then I did: $$\iint_{D} v (2 + 32v^2) dudv$$ But I got the wrong answer and instead in the answers they divided the determinant of jacobian. $$\iint_{D} \frac{v}{(2 + 32v^2)} dudv$$ Can someone explain why?",Let D be the bounded region in the first quadrant of bounded by curves and the positive y−axis. Describe in the −plane when and and calculate the integral So in order to solve the integral I used the new coordinates and . Afterwards I calculated the the determinant of the jacobian and got Then I did: But I got the wrong answer and instead in the answers they divided the determinant of jacobian. Can someone explain why?,"R^2 x^2 + 16y^2 = 16, x^2 + 16y^2 = 1, x = y D (u, v) u = x^2 + 16y^2 v =\frac yx, \iint_{D} \frac{y}{x} dA 1\leq u \leq 16 1\leq v \leq \infty  |\det J| = 1+32\left(\frac{y}{x}\right)^2 = 2 + 32v^2 \iint_{D} v (2 + 32v^2) dudv \iint_{D} \frac{v}{(2 + 32v^2)} dudv","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
30,Calculate flow integral using Stokes's theorem,Calculate flow integral using Stokes's theorem,,"Calculate the flow integral $$\iint_{Y} \text{curl} (\vec{F}) · \hat{N} dS$$ where $Y$ is part of the sphere $x^2 + y^2 + (z − 2)^2 = 8$ that lies above the $xy$ -plane and $\hat{N}$ is the outward unit normal vector on $Y$ and $$F(x, y , z) = (y^2 \cos(xz), x^3e^{yz} , −e^{−xyz}).$$ I got the answer $48\pi$ but the correct answer is $12\pi$ . What am I doing wrong?",Calculate the flow integral where is part of the sphere that lies above the -plane and is the outward unit normal vector on and I got the answer but the correct answer is . What am I doing wrong?,"\iint_{Y} \text{curl} (\vec{F}) · \hat{N} dS Y x^2 + y^2 + (z − 2)^2 = 8 xy \hat{N} Y F(x, y , z) = (y^2 \cos(xz), x^3e^{yz} , −e^{−xyz}). 48\pi 12\pi","['integration', 'multivariable-calculus', 'line-integrals', 'stokes-theorem']"
31,Which probability mass function has the largest Euclidean norm?,Which probability mass function has the largest Euclidean norm?,,"Fix $n > 1$ and let $[n] := \{ 1, 2, \dots, n \}$ . Which probability mass function (PMF) over $[n]$ has the largest $2$ -norm? Doodling for the cases $n \in \{2,3\}$ does suggest that the maximal $2$ -norm is attained when the support of the PMF is a singleton, i.e., at the ""corners"" of the probability simplex the minimal $2$ -norm is attained when the PMF is uniform over $[n]$ In essence, we have the following (non-convex) quadratic program (QP) $$ \begin{array}{ll} \underset {{\bf x} \in \Bbb R^n} {\text{maximize}} & \| {\bf x} \|_2^2 \\ \text{subject to} & {\bf 1}_n^\top {\bf x} = 1 \\ & {\bf x} \geq {\bf 0}_n \end{array} $$ One way of getting rid of ${\bf x} \geq {\bf 0}_n$ is to introduce $n$ new variables $y_i^2 := x_i$ and rewrite the QP above as follows $$ \begin{array}{ll} \underset {{\bf y} \in \Bbb R^n} {\text{maximize}} & \sum\limits_{i=1}^n y_i^4\\ \text{subject to} & \sum\limits_{i=1}^n y_i^2 = 1 \end{array} $$ where there is a single equality constraint. We define the Lagrangian $$ \mathcal L ({\bf y}, \mu) := \frac14 \sum\limits_{i=1}^n y_i^4 - \frac{\mu}{2} \left( \sum\limits_{i=1}^n y_i^2 - 1 \right) $$ Differentiating and finding where the partial derivatives vanish, we obtain $$ \begin{aligned} y_1 \left( y_1^2 - \mu \right) &= 0 \\ &\vdots \\ y_n \left( y_n^2 - \mu \right) &= 0 \\ \sum\limits_{i=1}^n y_i^2 &= 1 \end{aligned} $$ Note that $y_i = 0$ or $y_i^2 = \mu$ . Let $\operatorname{card} ({\bf y})$ denote the cardinality of the support, i.e., the number of non-zero entries of $\bf y$ . Hence, $$\sum\limits_{i=1}^n y_i^2 = \mu \operatorname{card} ({\bf y}) = 1$$ and, thus, $\color{blue}{x_i \in \left\{ 0, \dfrac{1}{\operatorname{card} ({\bf x})} \right\}}$ . Note that $\| {\bf x} \|_2^2 = \dfrac{1}{\operatorname{card} ({\bf x})}$ , which is maximal when $\color{blue}{\operatorname{card} ({\bf x}) = 1}$ . Is this correct?  If so, is there a more elegant way of showing that ${\bf x}_{\max} \in \{ {\bf e}_1, {\bf e}_2, \dots, {\bf e}_n \}$ ? Related Maximizing a positive semidefinite quadratic form over the standard simplex Analytical solution to a quadratic program over the standard simplex","Fix and let . Which probability mass function (PMF) over has the largest -norm? Doodling for the cases does suggest that the maximal -norm is attained when the support of the PMF is a singleton, i.e., at the ""corners"" of the probability simplex the minimal -norm is attained when the PMF is uniform over In essence, we have the following (non-convex) quadratic program (QP) One way of getting rid of is to introduce new variables and rewrite the QP above as follows where there is a single equality constraint. We define the Lagrangian Differentiating and finding where the partial derivatives vanish, we obtain Note that or . Let denote the cardinality of the support, i.e., the number of non-zero entries of . Hence, and, thus, . Note that , which is maximal when . Is this correct?  If so, is there a more elegant way of showing that ? Related Maximizing a positive semidefinite quadratic form over the standard simplex Analytical solution to a quadratic program over the standard simplex","n > 1 [n] := \{ 1, 2, \dots, n \} [n] 2 n \in \{2,3\} 2 2 [n]  \begin{array}{ll} \underset {{\bf x} \in \Bbb R^n} {\text{maximize}} & \| {\bf x} \|_2^2 \\ \text{subject to} & {\bf 1}_n^\top {\bf x} = 1 \\ & {\bf x} \geq {\bf 0}_n \end{array}  {\bf x} \geq {\bf 0}_n n y_i^2 := x_i  \begin{array}{ll} \underset {{\bf y} \in \Bbb R^n} {\text{maximize}} & \sum\limits_{i=1}^n y_i^4\\ \text{subject to} & \sum\limits_{i=1}^n y_i^2 = 1 \end{array}   \mathcal L ({\bf y}, \mu) := \frac14 \sum\limits_{i=1}^n y_i^4 - \frac{\mu}{2} \left( \sum\limits_{i=1}^n y_i^2 - 1 \right)   \begin{aligned} y_1 \left( y_1^2 - \mu \right) &= 0 \\ &\vdots \\ y_n \left( y_n^2 - \mu \right) &= 0 \\ \sum\limits_{i=1}^n y_i^2 &= 1 \end{aligned}  y_i = 0 y_i^2 = \mu \operatorname{card} ({\bf y}) \bf y \sum\limits_{i=1}^n y_i^2 = \mu \operatorname{card} ({\bf y}) = 1 \color{blue}{x_i \in \left\{ 0, \dfrac{1}{\operatorname{card} ({\bf x})} \right\}} \| {\bf x} \|_2^2 = \dfrac{1}{\operatorname{card} ({\bf x})} \color{blue}{\operatorname{card} ({\bf x}) = 1} {\bf x}_{\max} \in \{ {\bf e}_1, {\bf e}_2, \dots, {\bf e}_n \}","['multivariable-calculus', 'probability-distributions', 'optimization', 'quadratic-programming', 'non-convex-optimization']"
32,Reference request: When is the set of critical values of $f : \mathbb{R}^n \mapsto \mathbb{R}^m$ a countable set?,Reference request: When is the set of critical values of  a countable set?,f : \mathbb{R}^n \mapsto \mathbb{R}^m,"Given a continuously differentiable function $f : U \subset \mathbb{R}^n \mapsto \mathbb{R}^m$ (where $U$ is an open set), we say that $x$ is a critical point of $f$ if $Df(x)$ is not full rank. Let $C_f$ represent the set of all critical points of $f$ . From Sard's theorem, we know that when $f$ satisfies certain regularity conditions, then $ f(C_f) $ has measure $0$ in $\mathbb{R}^{m}$ . A generalisation of this result states that (again, under suitable regularity) if $ A_r = \{ x \in \mathbb{R}^n \mid \text{rank}Df(x) < r \} $ , then the Hausdorff dimension of $ f(A_r) $ is at most $r$ , but it can be arbitrarily close to $r$ . My question is, (*) what 'nice' functions are known to have the property that $ f(C_f) $ is actually always a countable set? For example, I think that this (*) would hold when $f : U \mapsto \mathbb{R}^m $ is analytic and $C_f \neq U$ . Also, in the case when $m=1$ and $C_f$ is a connected set, although we might expect that $f$ will be constant on $C_f$ , Whitney constructed a real-valued $C^1$ function of $2$ variables such that $C_f$ is an arc and $f(C_f)$ is not a constant and in fact contains an open set of $\mathbb{R}$ . The same paper mentions that when $f$ is 'smooth enough', then "" $f$ must be constant on any connected critical set, as shown by M. Morse and A. Sard in an unpublished paper "". But in my view, this shows that it is quite difficult to come up with such functions and I would expect 'generic' functions (in some appropriate sense) to be constant on each connected set of critical points, even given lower regularity. Any thoughts or references regarding this are welcome!","Given a continuously differentiable function (where is an open set), we say that is a critical point of if is not full rank. Let represent the set of all critical points of . From Sard's theorem, we know that when satisfies certain regularity conditions, then has measure in . A generalisation of this result states that (again, under suitable regularity) if , then the Hausdorff dimension of is at most , but it can be arbitrarily close to . My question is, (*) what 'nice' functions are known to have the property that is actually always a countable set? For example, I think that this (*) would hold when is analytic and . Also, in the case when and is a connected set, although we might expect that will be constant on , Whitney constructed a real-valued function of variables such that is an arc and is not a constant and in fact contains an open set of . The same paper mentions that when is 'smooth enough', then "" must be constant on any connected critical set, as shown by M. Morse and A. Sard in an unpublished paper "". But in my view, this shows that it is quite difficult to come up with such functions and I would expect 'generic' functions (in some appropriate sense) to be constant on each connected set of critical points, even given lower regularity. Any thoughts or references regarding this are welcome!",f : U \subset \mathbb{R}^n \mapsto \mathbb{R}^m U x f Df(x) C_f f f  f(C_f)  0 \mathbb{R}^{m}  A_r = \{ x \in \mathbb{R}^n \mid \text{rank}Df(x) < r \}   f(A_r)  r r  f(C_f)  f : U \mapsto \mathbb{R}^m  C_f \neq U m=1 C_f f C_f C^1 2 C_f f(C_f) \mathbb{R} f f,"['multivariable-calculus', 'reference-request', 'differential-topology']"
33,"Assume $\langle t_n u_n-t_m u_m, u_n-u_m \rangle \le 0$ for all $m, n \in \mathbb N$. How to show that $(u_n)$ is convergent in $\mathbb R^d$?",Assume  for all . How to show that  is convergent in ?,"\langle t_n u_n-t_m u_m, u_n-u_m \rangle \le 0 m, n \in \mathbb N (u_n) \mathbb R^d","Let $\langle \cdot, \cdot\rangle$ be the canonical inner product on $\mathbb R^d$ . I'm trying to solve below exercise, i.e., Let $(u_n) \subset \mathbb R^d$ and $(t_n) \subset \mathbb R_{>0}$ such that $(t_n)$ is non-decreasing. Assume that $$ \langle t_n u_n-t_m u_m, u_n-u_m \rangle \le 0 \quad \forall m, n \in \mathbb N. $$ Then $(u_n)$ is convergent in $\mathbb R^d$ . I'm able to solve the exercise in case $d=1$ . My proof relies on the ordering of $\mathbb R$ . Could you elaborate on how to tackle the case $d > 1$ ?","Let be the canonical inner product on . I'm trying to solve below exercise, i.e., Let and such that is non-decreasing. Assume that Then is convergent in . I'm able to solve the exercise in case . My proof relies on the ordering of . Could you elaborate on how to tackle the case ?","\langle \cdot, \cdot\rangle \mathbb R^d (u_n) \subset \mathbb R^d (t_n) \subset \mathbb R_{>0} (t_n) 
\langle t_n u_n-t_m u_m, u_n-u_m \rangle \le 0
\quad \forall m, n \in \mathbb N.
 (u_n) \mathbb R^d d=1 \mathbb R d > 1","['real-analysis', 'sequences-and-series', 'multivariable-calculus', 'convergence-divergence']"
34,Lagrange Multipliers to solve extrema,Lagrange Multipliers to solve extrema,,"I have a problem here that I can't solve it. Can anyone help me out here, please? Find extrema of function $f(x,y,z)= x^{2} yz$ over the unit sphere $x^{2}+y^{2}+z^{2} = 1$ . Here is my take on it: $$\begin{equation} \begin{aligned}\bigtriangledown f & = \:< \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}>\\ & =\: < 2xyz, x^{2}z, x^{2} y> \end{aligned} \end{equation}$$ $$ \begin{equation} \begin{aligned}\bigtriangledown g & = \:< \frac{\partial g}{\partial x}, \frac{\partial g}{\partial y}, \frac{\partial g}{\partial z}>\\ & =\: < 2x, 2y, 2z> \end{aligned} \end{equation}$$ since there are parallel , so the cross product of them equals 0. Now, to use the conventional Lagrange Multiplier method, I get : $$ \bigtriangledown f = \lambda \ast \bigtriangledown g $$ $$\begin{equation} \begin{aligned} \begin{cases}  &  2xyz= \lambda \ast 2x\\   &  x^{2} z=\lambda \ast 2y \\   &  x^{2} y= \lambda \ast 2z \end{cases} \end{aligned} \end{equation}$$ I can't solve x, y , and z from the system. Then I am thinking, what if I factor the common factor from both gradient vectors. $$\begin{aligned}  \frac{\bigtriangledown f }{x^{2} y z} & = \;<\frac{2}{x},\frac{1}{y},\frac{1}{z}> \\ \frac{\bigtriangledown g }{2} & = \;<x, y, z> \\ \frac{\bigtriangledown f }{x^{2} y z}\times \frac{\bigtriangledown g }{2} & = <\frac{z}{y}-\frac{y}{z},\frac{x}{z}-\frac{2z}{x},\frac{2y}{x}-\frac{x}{y}> \text{Cross Product} \end{aligned}$$ $$\begin{cases}  &  \frac{z}{y}-\frac{y}{z}=0 \\   &  \frac{x}{z}-\frac{2z}{x}=0 \\   &  \frac{2y}{x}-\frac{x}{y}=0  \end{cases}$$ Simplify further: $$  \begin{cases}  &  \frac{z}{y}=\frac{y}{z} \\   &  \frac{x}{z}=\frac{2z}{x} \\   &  \frac{2y}{x}=\frac{x}{y}  \end{cases}$$ Assume: \begin{aligned} \frac{y}{z} &= a\Rightarrow a = \frac{1}{a}\Rightarrow a^{2}= 1\Rightarrow a = \pm 1 \\ \frac{z}{x} &= b\Rightarrow 2b = \frac{1}{b}\Rightarrow b^{2}= \frac{1}{2}\Rightarrow b = \pm \sqrt{\frac{1}{2}}\\ \frac{y}{x} &= c\Rightarrow 2c = \frac{1}{c}\Rightarrow c^{2}= \frac{1}{2}\Rightarrow c = \pm \sqrt{\frac{1}{2}}\\ \end{aligned} Then, I have no clue what to do afterwards?","I have a problem here that I can't solve it. Can anyone help me out here, please? Find extrema of function over the unit sphere . Here is my take on it: since there are parallel , so the cross product of them equals 0. Now, to use the conventional Lagrange Multiplier method, I get : I can't solve x, y , and z from the system. Then I am thinking, what if I factor the common factor from both gradient vectors. Simplify further: Assume: Then, I have no clue what to do afterwards?","f(x,y,z)= x^{2} yz x^{2}+y^{2}+z^{2} = 1 \begin{equation}
\begin{aligned}\bigtriangledown f & = \:< \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}>\\ & =\: < 2xyz, x^{2}z, x^{2} y>
\end{aligned}
\end{equation}  \begin{equation}
\begin{aligned}\bigtriangledown g & = \:< \frac{\partial g}{\partial x}, \frac{\partial g}{\partial y}, \frac{\partial g}{\partial z}>\\ & =\: < 2x, 2y, 2z>
\end{aligned}
\end{equation} 
\bigtriangledown f = \lambda \ast \bigtriangledown g  \begin{equation}
\begin{aligned}
\begin{cases}
 &  2xyz= \lambda \ast 2x\\ 
 &  x^{2} z=\lambda \ast 2y \\ 
 &  x^{2} y= \lambda \ast 2z
\end{cases}
\end{aligned}
\end{equation} \begin{aligned} 
\frac{\bigtriangledown f }{x^{2} y z} & = \;<\frac{2}{x},\frac{1}{y},\frac{1}{z}> \\
\frac{\bigtriangledown g }{2} & = \;<x, y, z> \\
\frac{\bigtriangledown f }{x^{2} y z}\times \frac{\bigtriangledown g }{2} & = <\frac{z}{y}-\frac{y}{z},\frac{x}{z}-\frac{2z}{x},\frac{2y}{x}-\frac{x}{y}> \text{Cross Product}
\end{aligned} \begin{cases}
 &  \frac{z}{y}-\frac{y}{z}=0 \\ 
 &  \frac{x}{z}-\frac{2z}{x}=0 \\ 
 &  \frac{2y}{x}-\frac{x}{y}=0 
\end{cases}  
\begin{cases}
 &  \frac{z}{y}=\frac{y}{z} \\ 
 &  \frac{x}{z}=\frac{2z}{x} \\ 
 &  \frac{2y}{x}=\frac{x}{y} 
\end{cases} \begin{aligned}
\frac{y}{z} &= a\Rightarrow a = \frac{1}{a}\Rightarrow a^{2}= 1\Rightarrow a = \pm 1 \\
\frac{z}{x} &= b\Rightarrow 2b = \frac{1}{b}\Rightarrow b^{2}= \frac{1}{2}\Rightarrow b = \pm \sqrt{\frac{1}{2}}\\
\frac{y}{x} &= c\Rightarrow 2c = \frac{1}{c}\Rightarrow c^{2}= \frac{1}{2}\Rightarrow c = \pm \sqrt{\frac{1}{2}}\\
\end{aligned}","['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
35,Can a one-variable function generate a two-variable function with a certain property?,Can a one-variable function generate a two-variable function with a certain property?,,"Suppose a function $f$ of two variables is obeying $$ f(x,\,y) = f(xy,\,1)\;.\qquad\qquad\;\qquad (1) $$ Using $f$ , we can define a function $\varphi(z)$ of one variable, $$ \varphi(z) \equiv f(z,\,1)\;.\qquad\qquad\qquad\qquad (2) $$ Stated alternativaly, $$ \mbox{for}\;\forall\;f(x,\,y)\;\mbox{obeying (1)}\,,\;\;\exists\;\varphi(z)\;:\;\;\varphi(xy)=f(x,y)\;. $$ Now, the inverse problem: from an arbitrary $\varphi(z)$ , can we build an $f(x,y)$ satisfying (1) and linked to $\varphi$ via (2)?","Suppose a function of two variables is obeying Using , we can define a function of one variable, Stated alternativaly, Now, the inverse problem: from an arbitrary , can we build an satisfying (1) and linked to via (2)?","f 
f(x,\,y) = f(xy,\,1)\;.\qquad\qquad\;\qquad (1)
 f \varphi(z) 
\varphi(z) \equiv f(z,\,1)\;.\qquad\qquad\qquad\qquad (2)
 
\mbox{for}\;\forall\;f(x,\,y)\;\mbox{obeying (1)}\,,\;\;\exists\;\varphi(z)\;:\;\;\varphi(xy)=f(x,y)\;.
 \varphi(z) f(x,y) \varphi","['multivariable-calculus', 'functions']"
36,What is the meaning of the slope of the tangent line at a point to a parametrically defined curve that is not smooth at that point?,What is the meaning of the slope of the tangent line at a point to a parametrically defined curve that is not smooth at that point?,,"Suppose you have a parametric curve $r(t) = (x(t),y(t))$ . From my understanding, we typically require a smoothness condition that its derivative is not equal to the zero vector for all $t$ in $r(t)$ 's domain. Suppose we ignore that condition. Ignoring some potential uninteresting edge cases, it is easy to find two polynomials $x(t)$ , $y(t)$ such that there exists a $c$ so that $c$ is a non-repeated root of $x'$ and $y'$ . If we then define ""the slope of the line tangent to the curve $r(t)$ "" as $m = \lim_{t\rightarrow c} \frac{y'(t)}{x'(t)}$ then this limit will exist and the equation of the tangent line can be written as $$y = m(x-x(c)) + y(c).$$ It seems to me that this is the ""right"" way to try to define a ""tangent line"" and you could relax the smoothness condition to allow the case when $(x'(c),y'(c)) = (0,0)$ but the limit of $m$ exists at $c$ . My question is then, what am I actually doing here when I apply this procedure? Is this definition of a tangent line reasonable and does this slope really tell me anything? I suppose as a follow up, is this equation of the tangent line unique and independent of the parameterization?","Suppose you have a parametric curve . From my understanding, we typically require a smoothness condition that its derivative is not equal to the zero vector for all in 's domain. Suppose we ignore that condition. Ignoring some potential uninteresting edge cases, it is easy to find two polynomials , such that there exists a so that is a non-repeated root of and . If we then define ""the slope of the line tangent to the curve "" as then this limit will exist and the equation of the tangent line can be written as It seems to me that this is the ""right"" way to try to define a ""tangent line"" and you could relax the smoothness condition to allow the case when but the limit of exists at . My question is then, what am I actually doing here when I apply this procedure? Is this definition of a tangent line reasonable and does this slope really tell me anything? I suppose as a follow up, is this equation of the tangent line unique and independent of the parameterization?","r(t) = (x(t),y(t)) t r(t) x(t) y(t) c c x' y' r(t) m = \lim_{t\rightarrow c} \frac{y'(t)}{x'(t)} y = m(x-x(c)) + y(c). (x'(c),y'(c)) = (0,0) m c",['multivariable-calculus']
37,Minimize $x^2+y^2+z^2+w^2+2w(x+z)$ given certain constraints,Minimize  given certain constraints,x^2+y^2+z^2+w^2+2w(x+z),"I am trying to figure out how to show that the minimum of $f(x,y,z,w)=x^2+y^2+z^2+w^2+2w(x+z),$ given that $x+y+z+w=1,$$ $$x+y\geq 0.7,$ and $x\geq 0, y\geq 0, z \geq 0, \text{ and } w\geq 0,$ is 0.335 and that it is achieved when $x=y=0.35,$ $z=0.3,$ and $w=0.$ Note that the partial derivatives of $f$ are $$\frac{\partial f}{\partial x}=2x+2w,$$ $$\frac{\partial f}{\partial y}=2y,$$ $$\frac{\partial f}{\partial z}=2z+2w,$$ and $$\frac{\partial f}{\partial w}=2x+2z+2w.$$ All of these are positive when the variables $x, y, z, w$ are all positive, meaning that this function is increasing with respect to all of it's variables when they are positive. I think I can safely say that the minimum must occur when $x+y=0.7$ , which means that $z+w=0.3$ . Then $y=0.7-x$ and $w=0.3-z$ . Substituting these into $f$ gives us $$g(x,z):=f(x,0.7-x,z,0.3-z)=2x^2-0.8x-2xz+0.58.$$ I believe that minimizing $f$ with the original constraints is equivalent to minimizing $g$ assuming that $0\leq x\leq 1$ and $0\leq z\leq 0.3.$ This, in fact, does lead to the right answer, but I want to make sure I haven't made a mistake. Thank you in advance.","I am trying to figure out how to show that the minimum of given that and is 0.335 and that it is achieved when and Note that the partial derivatives of are and All of these are positive when the variables are all positive, meaning that this function is increasing with respect to all of it's variables when they are positive. I think I can safely say that the minimum must occur when , which means that . Then and . Substituting these into gives us I believe that minimizing with the original constraints is equivalent to minimizing assuming that and This, in fact, does lead to the right answer, but I want to make sure I haven't made a mistake. Thank you in advance.","f(x,y,z,w)=x^2+y^2+z^2+w^2+2w(x+z), x+y+z+w=1, x+y\geq 0.7, x\geq 0, y\geq 0, z \geq 0, \text{ and } w\geq 0, x=y=0.35, z=0.3, w=0. f \frac{\partial f}{\partial x}=2x+2w, \frac{\partial f}{\partial y}=2y, \frac{\partial f}{\partial z}=2z+2w, \frac{\partial f}{\partial w}=2x+2z+2w. x, y, z, w x+y=0.7 z+w=0.3 y=0.7-x w=0.3-z f g(x,z):=f(x,0.7-x,z,0.3-z)=2x^2-0.8x-2xz+0.58. f g 0\leq x\leq 1 0\leq z\leq 0.3.","['calculus', 'multivariable-calculus', 'optimization', 'convex-optimization', 'maxima-minima']"
38,Maximum/Minimum on sphere,Maximum/Minimum on sphere,,"I've attempted a problem in Spivak's Calculus on Manifolds and I can't find a solution online.  This is question 2-27, which is as follows Define $g,h:\{x\in\mathbb{R}^2:|x|\le1\}\rightarrow\mathbb{R}^3$ by $$\begin{split} g(x,y)&=(x,y,\sqrt{1-x^2-y^2})\\ h(x,y)&=(x,y,-\sqrt{1-x^2-y^2}) \end{split}$$ Show that the maximum of $f$ on $\{x\in\mathbb{R}^3:|x|=1\}$ is either the maximum of $f\circ g$ or the maximum of $f\circ h$ on $\{x\in\mathbb{R}^2:|x|\le1\}$ . My solution is as follws Assume that the maximum of $f$ occurs at some point $g(a)\in\mathbb{R}^3$ where $a\in\{x\in\mathbb{R}^2:|x|<1\}$ , and that $D_i\left(f\circ g\right)(a)$ exists.  Then by the chain rule $$D_i(f\circ g)(a)=D_if(g(a))\cdot D_ig(a)$$ since the maximum of $f$ appears at $g(a)$ , by theorem 2-6 $D_if(g(a))=0$ , and so $D_i(f\circ g)(a)=0$ .  This shows that the maximum of $f$ occurs at the maximum of $f\circ g$ . I have some issues with my solution, mainly that since the theorem only works in the interior so for points $|x|=1$ where $x\in\mathbb{R}^2$ the conclusion I've drawn doesn't hold on the boundary.  However, since it would seem that $f=g$ on the boundary it might not count? Also I'm not sure whether I've essentially done "" $D_if(b)=0$ so the maximum is at $b$ "", but I've tried to word it as similarly as the theorem to check that I haven't.","I've attempted a problem in Spivak's Calculus on Manifolds and I can't find a solution online.  This is question 2-27, which is as follows Define by Show that the maximum of on is either the maximum of or the maximum of on . My solution is as follws Assume that the maximum of occurs at some point where , and that exists.  Then by the chain rule since the maximum of appears at , by theorem 2-6 , and so .  This shows that the maximum of occurs at the maximum of . I have some issues with my solution, mainly that since the theorem only works in the interior so for points where the conclusion I've drawn doesn't hold on the boundary.  However, since it would seem that on the boundary it might not count? Also I'm not sure whether I've essentially done "" so the maximum is at "", but I've tried to word it as similarly as the theorem to check that I haven't.","g,h:\{x\in\mathbb{R}^2:|x|\le1\}\rightarrow\mathbb{R}^3 \begin{split}
g(x,y)&=(x,y,\sqrt{1-x^2-y^2})\\
h(x,y)&=(x,y,-\sqrt{1-x^2-y^2})
\end{split} f \{x\in\mathbb{R}^3:|x|=1\} f\circ g f\circ h \{x\in\mathbb{R}^2:|x|\le1\} f g(a)\in\mathbb{R}^3 a\in\{x\in\mathbb{R}^2:|x|<1\} D_i\left(f\circ g\right)(a) D_i(f\circ g)(a)=D_if(g(a))\cdot D_ig(a) f g(a) D_if(g(a))=0 D_i(f\circ g)(a)=0 f f\circ g |x|=1 x\in\mathbb{R}^2 f=g D_if(b)=0 b","['multivariable-calculus', 'solution-verification']"
39,Why is Stokes Theorem failing me?,Why is Stokes Theorem failing me?,,"I am really sorry if it's a stupid question, but I can't seem to get what I am doing wrong. I have a final approaching and for the most part I feel like I understand Stoke's theorem (or at least why it's so convenient). But I am unable to find what's wrong with what I am doing. Let $T$ be the triangle of vertices $(2, 0, 0)$ , $(0, 2, 0)$ and $(0, 0, 2)$ . Compute the flux (surface integral) of the vector field $F (x, y, z) = (z, z, z)$ accross $T$ . I have worked out the problems using segments, and got the expected answer of $4$ . I then tried to use Stokes theorem on it, but when I do that, I get zero. From my understanding of stokes, it should apply here since: I have a vector field I have a surface and a boundary (plane with $x+y+z = 2$ , and the triangle $T$ ) The boundary is simple and closed. But when I work it out, it reduces to double integral over projection of T of $(-1,1,0)$ (Curl of $F$ ) and $(1,1,1)$ (Normal Vector). Which simply gives 0.","I am really sorry if it's a stupid question, but I can't seem to get what I am doing wrong. I have a final approaching and for the most part I feel like I understand Stoke's theorem (or at least why it's so convenient). But I am unable to find what's wrong with what I am doing. Let be the triangle of vertices , and . Compute the flux (surface integral) of the vector field accross . I have worked out the problems using segments, and got the expected answer of . I then tried to use Stokes theorem on it, but when I do that, I get zero. From my understanding of stokes, it should apply here since: I have a vector field I have a surface and a boundary (plane with , and the triangle ) The boundary is simple and closed. But when I work it out, it reduces to double integral over projection of T of (Curl of ) and (Normal Vector). Which simply gives 0.","T (2, 0, 0) (0, 2, 0) (0, 0, 2) F (x, y, z) = (z, z, z) T 4 x+y+z = 2 T (-1,1,0) F (1,1,1)","['multivariable-calculus', 'vector-analysis']"
40,Determine a volume on the first octant using triple integrals,Determine a volume on the first octant using triple integrals,,"Consider the surface delimited by $z=9-y^2$ , $y=2x$ and $x=6$ on the first octant. How to find its volume using triple integrals? Seems like $x$ goes from $0$ to $6$ , $y$ goes from $0$ to $2x$ , and then $z$ goes from $0$ to $9-y^2$ .  In such case the integral would be $$\int_0^6 \int_0^{2x} \int_0^{9-y^2} dz\ dy\ dx$$ Which can be easily calculated. Are those limits correct? attaching a picture","Consider the surface delimited by , and on the first octant. How to find its volume using triple integrals? Seems like goes from to , goes from to , and then goes from to .  In such case the integral would be Which can be easily calculated. Are those limits correct? attaching a picture",z=9-y^2 y=2x x=6 x 0 6 y 0 2x z 0 9-y^2 \int_0^6 \int_0^{2x} \int_0^{9-y^2} dz\ dy\ dx,['multivariable-calculus']
41,Why is this integral not finite?,Why is this integral not finite?,,"This question is related to a previous one, I asked here , though I'm not going to explicitly state their relationship. Let $f:\mathbb{R} \to \mathbb{R}$ be a smooth compactly supported function and let $a_1$ and $a_2$ be two non-zero real numbers with non-zero solutions to the equation $a_1x + a_2y = 0 $ for $(x,y) \in [0,\infty)^2$ . I want to prove that the following integral below is not necessarily finite: $$\int_{ [0,\infty)^2 } f(a_1x+ a_2y) \ dx \ dy $$ The thing is that I ""sort of"" understand the intuition of why this is so, but I'm struggling to express it rigorously (as are the texts I've read claiming this fact ...) Let $K$ the set of $(x,y) \in [0,\infty)^2$ such that $a_1x + a_2y = 0$ . Because $K$ contains a non-zero vector, we can scale that non-zero vector by positive scalars and get a new non-zero solution for each scale. Therefore, $K$ is a ray. Now let $(x_t,y_t) \in [0,\infty)^2$ such that $a_1x_t +a_2y_t = t $ where $t$ lies in the support of $f$ . Then we should have something like \begin{align*} \int_{ [0,\infty)^2 } f(a_1x+ a_2y) \ dx \ dy & \geq \int_{K + (x_t,y_t)} f(a_1x+ a_2y) \ dx \ dy \newline &  = \int_{K+(x_t,y_t)} f(t) \ dx \  dy \end{align*} and we should have that the last integral is not finite. Of course, the last two integrals don't make sense so the inequality doesn't make sense. But it feels like it's capturing the idea. I would appreciate some help in completing my idea into a rigorous proof or by being given a totally different (and rigorous) argument.","This question is related to a previous one, I asked here , though I'm not going to explicitly state their relationship. Let be a smooth compactly supported function and let and be two non-zero real numbers with non-zero solutions to the equation for . I want to prove that the following integral below is not necessarily finite: The thing is that I ""sort of"" understand the intuition of why this is so, but I'm struggling to express it rigorously (as are the texts I've read claiming this fact ...) Let the set of such that . Because contains a non-zero vector, we can scale that non-zero vector by positive scalars and get a new non-zero solution for each scale. Therefore, is a ray. Now let such that where lies in the support of . Then we should have something like and we should have that the last integral is not finite. Of course, the last two integrals don't make sense so the inequality doesn't make sense. But it feels like it's capturing the idea. I would appreciate some help in completing my idea into a rigorous proof or by being given a totally different (and rigorous) argument.","f:\mathbb{R} \to \mathbb{R} a_1 a_2 a_1x + a_2y = 0  (x,y) \in [0,\infty)^2 \int_{ [0,\infty)^2 } f(a_1x+ a_2y) \ dx \ dy  K (x,y) \in [0,\infty)^2 a_1x + a_2y = 0 K K (x_t,y_t) \in [0,\infty)^2 a_1x_t +a_2y_t = t  t f \begin{align*}
\int_{ [0,\infty)^2 } f(a_1x+ a_2y) \ dx \ dy & \geq \int_{K + (x_t,y_t)} f(a_1x+ a_2y) \ dx \ dy \newline
&  = \int_{K+(x_t,y_t)} f(t) \ dx \  dy
\end{align*}","['real-analysis', 'multivariable-calculus', 'improper-integrals', 'distribution-theory']"
42,"Find the extrema of $f(x,y)=max(x,y)$ constrained to $\mathscr A=\{(x,y) ∈ \mathbb R^2 ∣ x^2+y^2=1\}$",Find the extrema of  constrained to,"f(x,y)=max(x,y) \mathscr A=\{(x,y) ∈ \mathbb R^2 ∣ x^2+y^2=1\}","I know the maxima occurs at $f(x,y) = 1$ at the points $(1,0)$ and $(0,1)$ ... but I can't seem to find all the minima. I've come to the conclusion that the minima occurs at a point $(a,a)$ where $a ∈ \mathbb R_<0$ , such that $2a^2=1\rightarrow a=\frac{\sqrt 2}{2} \lor a=-\frac{\sqrt 2}{2}$ . Does the minima occur at ( $-\frac{\sqrt 2}{2}, -\frac{\sqrt 2}{2}$ )?","I know the maxima occurs at at the points and ... but I can't seem to find all the minima. I've come to the conclusion that the minima occurs at a point where , such that . Does the minima occur at ( )?","f(x,y) = 1 (1,0) (0,1) (a,a) a ∈ \mathbb R_<0 2a^2=1\rightarrow a=\frac{\sqrt 2}{2} \lor a=-\frac{\sqrt 2}{2} -\frac{\sqrt 2}{2}, -\frac{\sqrt 2}{2}",['multivariable-calculus']
43,"How can I calculate the area of a circle centered at (2,2) with radius 2 using double integrals","How can I calculate the area of a circle centered at (2,2) with radius 2 using double integrals",,"This is what the graph loosk like. Obviously I know $\pi r^2$ but if I specifically wanted to find the area using double integrals in polar coordinates, how would I go about it? My guess is that $\theta$ goes from $0$ to $\pi/2$ and for $r$ I need to do: $(x-2)^2+(y-2)^2=4$ $x^2-4y+4+y^2-4y+4=4$ $(x^2+y^2)-4(x+y)=-4$ $r^2-4(r\cos\theta+r\sin\theta)=-4$ $r^2=4(r\cos\theta+r\sin\theta)-4$ Not exactly sure what to do from here.","This is what the graph loosk like. Obviously I know but if I specifically wanted to find the area using double integrals in polar coordinates, how would I go about it? My guess is that goes from to and for I need to do: Not exactly sure what to do from here.",\pi r^2 \theta 0 \pi/2 r (x-2)^2+(y-2)^2=4 x^2-4y+4+y^2-4y+4=4 (x^2+y^2)-4(x+y)=-4 r^2-4(r\cos\theta+r\sin\theta)=-4 r^2=4(r\cos\theta+r\sin\theta)-4,"['integration', 'multivariable-calculus']"
44,Easier way to calculate triple integral,Easier way to calculate triple integral,,So I had to calculate an integral that goes like that $$\iiint\  \frac{x^2}4+y^2 dxdydz$$ where the area is $$\frac{x^2}4+y^2=1$$ $$z=0$$ $$z=x+2y+5$$ I actually solved this triple integral and my answer is $\mathbf{5}\pi$ . I made it in two ways: I described the area as $$-2<x<2$$ $$-\sqrt{1-\frac{x^2}4}<y<\sqrt{1-\frac{x^2}4}$$ $$0<z<x+2y+5$$ And using polar form where I calculated ellipse's radius as $0<r<\frac{2}{\sqrt{4\sin^2\phi+\cos^2\phi}}$ and the change of ' $z$ ' as $0<z<\cos\phi r+2\sin\phi r +5$ where $ 0<\phi<2\pi $ But I feel like I may overcomplicate this? Is there a way to solve this triple integral easier/faster?,So I had to calculate an integral that goes like that where the area is I actually solved this triple integral and my answer is . I made it in two ways: I described the area as And using polar form where I calculated ellipse's radius as and the change of ' ' as where But I feel like I may overcomplicate this? Is there a way to solve this triple integral easier/faster?,\iiint\  \frac{x^2}4+y^2 dxdydz \frac{x^2}4+y^2=1 z=0 z=x+2y+5 \mathbf{5}\pi -2<x<2 -\sqrt{1-\frac{x^2}4}<y<\sqrt{1-\frac{x^2}4} 0<z<x+2y+5 0<r<\frac{2}{\sqrt{4\sin^2\phi+\cos^2\phi}} z 0<z<\cos\phi r+2\sin\phi r +5  0<\phi<2\pi ,"['integration', 'multivariable-calculus']"
45,"Find $x_1,x_2$ such that $\min_{x_1,x_2} x_1^2+2x_1x_2$ where $x_1,x_2$ are subject to constraint $x_1^2x_2 \ge 10$.",Find  such that  where  are subject to constraint .,"x_1,x_2 \min_{x_1,x_2} x_1^2+2x_1x_2 x_1,x_2 x_1^2x_2 \ge 10","I am attempting to solve a constrained optimization problem using Lagrange multipliers but am getting lost on how to resolve the equations the gradients output. The problem is the following: Find $x_1,x_2$ such that $\min_{x_1,x_2} x_1^2+2x_1x_2$ where $x_1,x_2$ are subject to constraint $x_1^2x_2 \ge 10$ . I have changed the constraint into the equality $x_1^2x_2-10-s^2=0$ and attained the gradients which result in 4 equations and 4 unknowns: \begin{align} x^2_1x_2-10-s^2 &= 0 \\ 2x_1+2x_2 &= \lambda (2x_1x_2) \\ 2x_1 &= \lambda x_1^2 \\ 0 &= \lambda(-2s) \end{align} But I am unsure of how to proceed from here. Additionally, I am struggling to find the dual problem.","I am attempting to solve a constrained optimization problem using Lagrange multipliers but am getting lost on how to resolve the equations the gradients output. The problem is the following: Find such that where are subject to constraint . I have changed the constraint into the equality and attained the gradients which result in 4 equations and 4 unknowns: But I am unsure of how to proceed from here. Additionally, I am struggling to find the dual problem.","x_1,x_2 \min_{x_1,x_2} x_1^2+2x_1x_2 x_1,x_2 x_1^2x_2 \ge 10 x_1^2x_2-10-s^2=0 \begin{align}
x^2_1x_2-10-s^2 &= 0 \\
2x_1+2x_2 &= \lambda (2x_1x_2) \\
2x_1 &= \lambda x_1^2 \\
0 &= \lambda(-2s)
\end{align}","['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
46,Compute $\iint_R \left|\cos(2x)-\cos(y)\right|\mathrm{d}x\mathrm{d}y$,Compute,\iint_R \left|\cos(2x)-\cos(y)\right|\mathrm{d}x\mathrm{d}y,"I need help computing the following integral: $$I = \iint_R \left|\cos(2x)-\cos(y)\right|\mathrm{d}x\mathrm{d}y$$ Where $R=[0,\pi]\times [0,\pi].$ I plugged this double integral in WA and I got $8$ as the result. Also I did it plotting $\cos(2x)-\cos(y)\geq 0$ and splitting the integral in intervals depending of the absolute value. My doubt is that I tried another way and it didn't work. Consider that $\cos(2x)-\cos(y) = -2\sin\left(\left(\frac{2x+y}{2}\right)\right)\sin\left(\left(\frac{2x-y}{2}\right)\right)$ and the change of variables $u=\frac{2x+y}{2}, v = \frac{2x-y}{2}$ . The absolute value of the jacobian determinant of this transformation is $1$ . Also, given that $0 \leq x \leq \pi$ and $0 \leq y \leq \pi$ , we can compute that $0 \leq \frac{2x+y}{2} = u \leq \frac{3 \pi}{2}$ and $-\frac{\pi}{2} \leq \frac{2x-y}{2}=v \leq \pi$ . In that case, we will get that $$I = \int_0^{\frac{3\pi}{2}} \int_{-\frac{\pi}{2}}^{\pi} \left|-2\sin(u)\sin(v)\right| \mathrm{d}v \mathrm{d}u$$ but the result of the last integral is $18$ according to WA. What is wrong with this method?","I need help computing the following integral: Where I plugged this double integral in WA and I got as the result. Also I did it plotting and splitting the integral in intervals depending of the absolute value. My doubt is that I tried another way and it didn't work. Consider that and the change of variables . The absolute value of the jacobian determinant of this transformation is . Also, given that and , we can compute that and . In that case, we will get that but the result of the last integral is according to WA. What is wrong with this method?","I = \iint_R \left|\cos(2x)-\cos(y)\right|\mathrm{d}x\mathrm{d}y R=[0,\pi]\times [0,\pi]. 8 \cos(2x)-\cos(y)\geq 0 \cos(2x)-\cos(y) = -2\sin\left(\left(\frac{2x+y}{2}\right)\right)\sin\left(\left(\frac{2x-y}{2}\right)\right) u=\frac{2x+y}{2}, v = \frac{2x-y}{2} 1 0 \leq x \leq \pi 0 \leq y \leq \pi 0 \leq \frac{2x+y}{2} = u \leq \frac{3 \pi}{2} -\frac{\pi}{2} \leq \frac{2x-y}{2}=v \leq \pi I = \int_0^{\frac{3\pi}{2}} \int_{-\frac{\pi}{2}}^{\pi} \left|-2\sin(u)\sin(v)\right| \mathrm{d}v \mathrm{d}u 18","['integration', 'multivariable-calculus', 'definite-integrals']"
47,"Differentiability at (0,0).","Differentiability at (0,0).",,"I always get stuck when I've to show something is differentiable,like in the following question: $f(x,y) = \begin{cases} \dfrac{x^3y^3}{x^4+y^4} & \text{if $(x,y)\neq(0,0)$} \\ 0 & \text{if $(x,y)=(0,0)$} \end{cases}$ show that f is differentiable at (0,0) ... First I thought of showing that partial derivatives exist for the point (0, 0). $$\frac{\partial f}{\partial x}(0,0)=\lim_{x\rightarrow 0}\frac{f(x,0)-f(0,0)}{x-0}=\lim_{x\rightarrow 0}\frac{\frac{0}{x^4}}{x}=\lim_{x\rightarrow 0}\frac{{0}}{x^5}= 0$$ later in relation to y: $$\frac{\partial f}{\partial y}(0,0)=\lim_{y\rightarrow 0}\frac{f(0,y)-f(0,0)}{y -0}=\lim_{y\rightarrow 0}\frac{\frac{0}{y^4}}{y}=\lim_{y\rightarrow 0}\frac{{0}}{y^5}= 0$$ alright both partial derivatives exists and are equal,so now we have to show that :they are continuous near (0,0).. but that doesn't prove the limit is differentiable, what can I do? I thought of using the following formula for a corollary. $$E(h,k)=f(x_0 +h, y_0 +k)-f(x_0, y_0)-\frac{\partial f}{\partial x}(x_0,y_0)h -\frac{\partial f}{\partial y}(x_0,y_0)k$$ however: $$E(h,k)=\frac{(x_0+h)^3(y_0+k)^3} {(x_0 + h)^4 + (y_0 +k)^4}  -\frac{x^3 y^3}{x^4 + y^4}-0h -0k$$","I always get stuck when I've to show something is differentiable,like in the following question: show that f is differentiable at (0,0) ... First I thought of showing that partial derivatives exist for the point (0, 0). later in relation to y: alright both partial derivatives exists and are equal,so now we have to show that :they are continuous near (0,0).. but that doesn't prove the limit is differentiable, what can I do? I thought of using the following formula for a corollary. however:","f(x,y) = \begin{cases} \dfrac{x^3y^3}{x^4+y^4} & \text{if (x,y)\neq(0,0)} \\ 0 & \text{if (x,y)=(0,0)} \end{cases} \frac{\partial f}{\partial x}(0,0)=\lim_{x\rightarrow 0}\frac{f(x,0)-f(0,0)}{x-0}=\lim_{x\rightarrow 0}\frac{\frac{0}{x^4}}{x}=\lim_{x\rightarrow 0}\frac{{0}}{x^5}= 0 \frac{\partial f}{\partial y}(0,0)=\lim_{y\rightarrow 0}\frac{f(0,y)-f(0,0)}{y -0}=\lim_{y\rightarrow 0}\frac{\frac{0}{y^4}}{y}=\lim_{y\rightarrow 0}\frac{{0}}{y^5}= 0 E(h,k)=f(x_0 +h, y_0 +k)-f(x_0, y_0)-\frac{\partial f}{\partial x}(x_0,y_0)h -\frac{\partial f}{\partial y}(x_0,y_0)k E(h,k)=\frac{(x_0+h)^3(y_0+k)^3} {(x_0 + h)^4 + (y_0 +k)^4}  -\frac{x^3 y^3}{x^4 + y^4}-0h -0k",['multivariable-calculus']
48,Does every compact set in a normed space with a non-trivial interior has 2 path connected points in the boundary?,Does every compact set in a normed space with a non-trivial interior has 2 path connected points in the boundary?,,"Let $k\subset\mathbb{R}^n$ be a compact space with $K^\circ \neq \emptyset$ . Are there necessarily $a,b\in\partial K$ such that $$[a,b]=\{a+t(b-a)\mid t\in[0,1]\}\subset K$$ This is a lemma I want to prove in order to use mean-value type theorems in a compact set. I would appreciate notes regarding my own proof attempt (which isn't formal enough I believe) as well as other proofs. Proof attempt: There exists a point $x\in K^\circ$ , so we can take $r=sup_r(\hat B _r(x)\subset K)$ and claim that $\partial K \cap\partial B_r(x)\neq\emptyset$ . Now we can take a point $a$ from this intersection and look at the interval $$[a,\infty)=\{a-t(x-a)\mid t\in[0,\infty]\}$$ Now we can take another supremum: $$s=sup_\alpha \{a+t(x-a)\mid t\in[0,\alpha]\}\subset K$$ Since $K$ is compact in a normed space it is bounded, so $s<\infty$ . It is also true that $s\geq2r$ . Because of that $s$ is well defined. We define $b=a-s(x-a)\in\partial K$ and we have $$a,b\in\partial K \ \ \ s.t \ \ \ [a,b]\subset K$$","Let be a compact space with . Are there necessarily such that This is a lemma I want to prove in order to use mean-value type theorems in a compact set. I would appreciate notes regarding my own proof attempt (which isn't formal enough I believe) as well as other proofs. Proof attempt: There exists a point , so we can take and claim that . Now we can take a point from this intersection and look at the interval Now we can take another supremum: Since is compact in a normed space it is bounded, so . It is also true that . Because of that is well defined. We define and we have","k\subset\mathbb{R}^n K^\circ \neq \emptyset a,b\in\partial K [a,b]=\{a+t(b-a)\mid t\in[0,1]\}\subset K x\in K^\circ r=sup_r(\hat B _r(x)\subset K) \partial K \cap\partial B_r(x)\neq\emptyset a [a,\infty)=\{a-t(x-a)\mid t\in[0,\infty]\} s=sup_\alpha \{a+t(x-a)\mid t\in[0,\alpha]\}\subset K K s<\infty s\geq2r s b=a-s(x-a)\in\partial K a,b\in\partial K \ \ \ s.t \ \ \ [a,b]\subset K","['multivariable-calculus', 'proof-writing', 'compactness', 'normed-spaces', 'supremum-and-infimum']"
49,"Evaluate $\int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} \,d z \,d y \,d x$",Evaluate,"\int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} \,d z \,d y \,d x","$$ \text { Evaluate } \int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} \,d z\, d y\, d x $$ How to solve this ? I tried this $\begin{aligned} & \int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} d z d y d x \\ =& \int_{0}^{1} \int_{0}^{1-x} \frac{\sqrt{x}}{\sqrt{y}}[2 \sqrt{z}]{0}^{1-x-y} d y d x \\ =& \int_{0}^{1} \int_{0}^{1-x} \frac{\sqrt{x}}{\sqrt{y}} 2 \sqrt{1-x-y} d y d x \\ =& \int_{0}^{1} \int_{0}^{1-x} 2 \sqrt{x} \frac{\sqrt{1-x-y}}{\sqrt{y}} d y d x \\=& \int_{0}^{1} \int_{0}^{1-x} 2 \sqrt{x} \sqrt{\frac{1-x-y}{y}} d y d x \end{aligned}$",How to solve this ? I tried this,"
\text { Evaluate } \int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} \,d z\, d y\, d x
 \begin{aligned} & \int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} d z d y d x \\
=& \int_{0}^{1} \int_{0}^{1-x} \frac{\sqrt{x}}{\sqrt{y}}[2 \sqrt{z}]{0}^{1-x-y} d y d x \\
=& \int_{0}^{1} \int_{0}^{1-x} \frac{\sqrt{x}}{\sqrt{y}} 2 \sqrt{1-x-y} d y d x \\
=& \int_{0}^{1} \int_{0}^{1-x} 2 \sqrt{x} \frac{\sqrt{1-x-y}}{\sqrt{y}} d y d x \\=& \int_{0}^{1} \int_{0}^{1-x} 2 \sqrt{x} \sqrt{\frac{1-x-y}{y}} d y d x \end{aligned}","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
50,"Prove that $\nabla_{-v}f(a) = -\nabla_{v}f(a)$, provided $\nabla_{v}f(a)$ exists.","Prove that , provided  exists.",\nabla_{-v}f(a) = -\nabla_{v}f(a) \nabla_{v}f(a),"Definition: $\nabla_{v}f(a)$ is said to be the directional derivative of $f$ at $a$ along the vector $v$ if $\forall \epsilon >0 \exists \delta>0$ s.t. $\forall h \in \mathbb{R}$ , $$0<|h|<\delta \implies \left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon$$ or equivalently, $$\lim_{h \to 0} \frac{f(a+hv) - f(a)}{h} = \nabla_{v}f(a).$$ Note that we cannot use the eqaution relating the directional derivative to a the gradient since $f$ may not be differentiable at $a$ . Here is my attempt using the $\epsilon - \delta$ definition. Suppose $\nabla_{v}f(a)$ exists. We want to show that $\nabla_{-v}f(a) = -\nabla_{v}f(a)$ . Let $\epsilon>0$ . Then there exists $\delta_{0} >0$ s.t. for any $h \in \mathbb{R}, 0<|h|<\delta_{0}$ , $$\left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon.$$ Now pick $\delta = $ (tbd). then for $0<|h|<\delta$ , $$\left|\left| \frac{f(a-hv)-f(a)}{h} + \nabla_{v}f(a) \right|\right|$$ Unfortunately, I got stuck here as i have no idea how to obtain an expression similar to $\left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon$ from $\left|\left| \frac{f(a-hv)-f(a)}{h} + \nabla_{v}f(a) \right|\right|$ . Any hints/ideas/suggestions?","Definition: is said to be the directional derivative of at along the vector if s.t. , or equivalently, Note that we cannot use the eqaution relating the directional derivative to a the gradient since may not be differentiable at . Here is my attempt using the definition. Suppose exists. We want to show that . Let . Then there exists s.t. for any , Now pick (tbd). then for , Unfortunately, I got stuck here as i have no idea how to obtain an expression similar to from . Any hints/ideas/suggestions?","\nabla_{v}f(a) f a v \forall \epsilon >0 \exists \delta>0 \forall h \in \mathbb{R} 0<|h|<\delta \implies \left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon \lim_{h \to 0} \frac{f(a+hv) - f(a)}{h} = \nabla_{v}f(a). f a \epsilon - \delta \nabla_{v}f(a) \nabla_{-v}f(a) = -\nabla_{v}f(a) \epsilon>0 \delta_{0} >0 h \in \mathbb{R}, 0<|h|<\delta_{0} \left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon. \delta =  0<|h|<\delta \left|\left| \frac{f(a-hv)-f(a)}{h} + \nabla_{v}f(a) \right|\right| \left|\left| \frac{f(a+hv)-f(a)}{h} - \nabla_{v}f(a) \right|\right|<\epsilon \left|\left| \frac{f(a-hv)-f(a)}{h} + \nabla_{v}f(a) \right|\right|","['real-analysis', 'multivariable-calculus']"
51,Divergence of a radial vector field,Divergence of a radial vector field,,"I am reading Modern Electrodynamics by Zangwill and cannot verify equation (1.61) [page 7]: \begin{equation} \nabla \cdot  \textbf{g}(r)=\textbf{g}^{\prime}\cdot \mathbf{\hat{r}}, \end{equation} where the vector field $\textbf{g}(r)$ is only nonzero in the radial direction. By using the divergence formula in Spherical coordinates, I get: \begin{align} \nabla \cdot  \textbf{g}(r)&=\frac{1}{r^2} \partial_r (r^2 g_r) + \frac{1}{r \sin \theta} \partial_{\theta} (g_{\theta} \sin \theta) + \frac{1}{r \sin \theta} \partial_{\phi} g_{\phi}\\ &=\frac{2}{r}g_r + \frac{d}{dr}g_r\\ &= \frac{2}{r}\textbf{g}\cdot \mathbf{\hat{r}}+\textbf{g}^{\prime}\cdot \mathbf{\hat{r}} \end{align} What is going wrong?","I am reading Modern Electrodynamics by Zangwill and cannot verify equation (1.61) [page 7]: where the vector field is only nonzero in the radial direction. By using the divergence formula in Spherical coordinates, I get: What is going wrong?","\begin{equation}
\nabla \cdot  \textbf{g}(r)=\textbf{g}^{\prime}\cdot \mathbf{\hat{r}},
\end{equation} \textbf{g}(r) \begin{align}
\nabla \cdot  \textbf{g}(r)&=\frac{1}{r^2} \partial_r (r^2 g_r) + \frac{1}{r \sin \theta} \partial_{\theta} (g_{\theta} \sin \theta) + \frac{1}{r \sin \theta} \partial_{\phi} g_{\phi}\\
&=\frac{2}{r}g_r + \frac{d}{dr}g_r\\
&= \frac{2}{r}\textbf{g}\cdot \mathbf{\hat{r}}+\textbf{g}^{\prime}\cdot \mathbf{\hat{r}}
\end{align}","['multivariable-calculus', 'vector-fields', 'spherical-coordinates', 'divergence-operator', 'electromagnetism']"
52,"Evaluate the integral $\iint_De^{-x^2}\,dx\,dy$",Evaluate the integral,"\iint_De^{-x^2}\,dx\,dy","Find $\iint_De^{-x^2}\,dx\,dy$ , where $D$ is the triangle formed by points $O(0,0), A(1,0), B(1,1)$ I'm totally lost on this one. I only have experience with circle functions, and converting them to polar and finding the bounds in $π$ and $r$ , but here its just points. Can anyone help me understand this? Do i just bound $x$ to $[0,1]$ and $y$ to $[0.1]$ ?","Find , where is the triangle formed by points I'm totally lost on this one. I only have experience with circle functions, and converting them to polar and finding the bounds in and , but here its just points. Can anyone help me understand this? Do i just bound to and to ?","\iint_De^{-x^2}\,dx\,dy D O(0,0), A(1,0), B(1,1) π r x [0,1] y [0.1]","['calculus', 'multivariable-calculus', 'multiple-integral']"
53,Invariance of divergence under arbitrary coordinate transformations,Invariance of divergence under arbitrary coordinate transformations,,"I've found the following proof which seems to have the conclusion, that the divergence is invariant under a general coordinate transformation when defined with the derivatives of the respective (transformed) components: Let $f:\mathbb{R}^n\longrightarrow \mathbb{R}^n$ and $J_f$ the jacobi matrix associate to $f$ . $\phi:\mathbb{R}^n \longrightarrow \mathbb{R}^n :(y_1,...,y_n)\longmapsto (x_1...,x_n)$ is a change of coordinates. So $x_i=\phi_i(y_1,...,y_n)$ , and $J_\phi$ is Jocobian matrix associate to $\phi$ . Let $g:\phi^{-1}\circ f\circ \phi:\mathbb{R}^n \longrightarrow \mathbb{R}^n: (y_1,...,y_n)\longmapsto (y_1...,y_n)$ ; the jacobian matrix associate to $g$ is $J_g$ . Chain rule implies: $J_g=J_\phi^{-1} J_f J_\phi$ . Now $div g=tr(J_g)=tr(J_\phi^{-1} J_f J_\phi)=tr(J_f)=div f$ So, the divergence is invariant under a coordinate transformation. But e.g. in spherical coordiantes the divergence clearly is NOT simply: $divf = tr(J_g) =\frac{\partial {g}_r}{\partial r} + \frac{\partial {g}_\theta}{\partial \theta}+\frac{\partial {g}_\phi}{\partial \phi}$ So I do not understand where the above proof breaks down or where the mistake is.","I've found the following proof which seems to have the conclusion, that the divergence is invariant under a general coordinate transformation when defined with the derivatives of the respective (transformed) components: Let and the jacobi matrix associate to . is a change of coordinates. So , and is Jocobian matrix associate to . Let ; the jacobian matrix associate to is . Chain rule implies: . Now So, the divergence is invariant under a coordinate transformation. But e.g. in spherical coordiantes the divergence clearly is NOT simply: So I do not understand where the above proof breaks down or where the mistake is.","f:\mathbb{R}^n\longrightarrow \mathbb{R}^n J_f f \phi:\mathbb{R}^n \longrightarrow \mathbb{R}^n :(y_1,...,y_n)\longmapsto (x_1...,x_n) x_i=\phi_i(y_1,...,y_n) J_\phi \phi g:\phi^{-1}\circ f\circ \phi:\mathbb{R}^n \longrightarrow \mathbb{R}^n: (y_1,...,y_n)\longmapsto (y_1...,y_n) g J_g J_g=J_\phi^{-1} J_f J_\phi div g=tr(J_g)=tr(J_\phi^{-1} J_f J_\phi)=tr(J_f)=div f divf = tr(J_g) =\frac{\partial {g}_r}{\partial r} + \frac{\partial {g}_\theta}{\partial \theta}+\frac{\partial {g}_\phi}{\partial \phi}","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'divergence-operator']"
54,Double integral of an off-centered circle.,Double integral of an off-centered circle.,,"I'm new to double integral and doing this problem: (from the Supplemental Problems, MIT) Express each double integral over the given region R as an iterated integral in polar coordinates. Use the method described in Notes I to supply the limits of integration. For some of them, it may be necessary to break the integral up into two parts. In each case, begin by sketching the region. b) The circle of radius 1, and center at (0, 1). What I have done so far: Since I know the area of a radius 1 circle is $\pi$ , the result must also be $\pi$ . I know the bounds must be from 0 to $\pi$ . Then, I do the following double integral: $$\int_0^{\pi}\int_0^{2sin\theta} 1 drd\theta $$ But the result is 4, which is a wrong answer. Therefore, my question is, where is the mistake? I do try to replace the $drd\theta$ term to $rdrd\theta$ and it gives me the correct answer, which is $\pi$ . But there are some problems, which do not use the $rdrd\theta$ term. For example, this problem: a) The region lying inside the circle with center at the origin and radius 2. and to the left of the vertical line through (−1, 0). The answer of this problem is: $$\int_{2\pi/3}^{4\pi/3} \int_{-sec\theta}^2 1 drd\theta$$ which does not include a $rdrd\theta$ term Furthermore, I will appreciate if anyone can help me to point out when to use the $drd\theta$ term and when to use $rdrd\theta$ term. P/S: Sorry for my bad English. I'm not a native speaker.","I'm new to double integral and doing this problem: (from the Supplemental Problems, MIT) Express each double integral over the given region R as an iterated integral in polar coordinates. Use the method described in Notes I to supply the limits of integration. For some of them, it may be necessary to break the integral up into two parts. In each case, begin by sketching the region. b) The circle of radius 1, and center at (0, 1). What I have done so far: Since I know the area of a radius 1 circle is , the result must also be . I know the bounds must be from 0 to . Then, I do the following double integral: But the result is 4, which is a wrong answer. Therefore, my question is, where is the mistake? I do try to replace the term to and it gives me the correct answer, which is . But there are some problems, which do not use the term. For example, this problem: a) The region lying inside the circle with center at the origin and radius 2. and to the left of the vertical line through (−1, 0). The answer of this problem is: which does not include a term Furthermore, I will appreciate if anyone can help me to point out when to use the term and when to use term. P/S: Sorry for my bad English. I'm not a native speaker.",\pi \pi \pi \int_0^{\pi}\int_0^{2sin\theta} 1 drd\theta  drd\theta rdrd\theta \pi rdrd\theta \int_{2\pi/3}^{4\pi/3} \int_{-sec\theta}^2 1 drd\theta rdrd\theta drd\theta rdrd\theta,"['multivariable-calculus', 'polar-coordinates', 'multiple-integral']"
55,"Solution Verification: Find Extrema points of the function $f(x,y)=2^{3x+8y}$ that are on $x^2+y^2=1$. [duplicate]",Solution Verification: Find Extrema points of the function  that are on . [duplicate],"f(x,y)=2^{3x+8y} x^2+y^2=1","This question already has answers here : Solving trigonometric equations of the form $a\sin x + b\cos x = c$ (7 answers) Closed 3 years ago . $f(x,y)=2^{3x+8y}$ $x^2+y^2=1$ Polar Coordinates approach: Let $x=cos(t), y=sin(t), 0\le t \le 2\pi$ (We can see that this solved the circle equation). Substituting them into my function: $f = 2^{3\cos(t)+8\sin(t)} \Longrightarrow f=e^{(3\cos(t)+8\sin(t))ln(2)}$ Taking derivative to find critical points: $f' = \ln(2)(-3\sin(t)+8\cos(t))e^{(3\cos(t)+8\sin(t))ln(2)}=0$ Now $e^x>0$ , so I need to find when $-3\sin(t)+8\cos(t)=0 \Longrightarrow 3 \sin(t)=8\cos(t) \Longrightarrow \tan(t) = \frac{8}{3} \Longrightarrow t=\arctan(\frac{8}{3}), \arctan(\frac{8}{3}) + \pi $ So I get two Points: $(\cos(\arctan(\frac{8}{3})) ,\sin(\arctan(\frac{8}{3})))$ $(\cos(\arctan(\frac{8}{3}) + \pi), \sin(\arctan(\frac{8}{3}))$ and now I need to find $f_{xx}, f_{yy}, f{xy}$ in order to know which point is minimum and which is maximum: $f_{x}=8^x3ln(2)$ $f_y = 256^y 8ln(2)$ $f_{xx} = 9\ln^2(2) 8^x$ $f_{yy} = 64ln^2(2) 256^y$ $f_{xy} = 0$ $f_{yx} = 0$ . for first point: $f_{xx} = 8.97$ , $f_{yy} = 5530.131$ .  So $f_{xx}*f_{yy} - 0 > 0$ , and $f_{xx} > 0 $ , which means it is a local minimum point. for second point: $f_{xx} = 2.03$ , $f_{yy} = 0.17$ . and same as above, it is a minimum point. Would appreciate any help in approving this solution or finding mistakes in it, and if there's any more efficient approaches, thanks in advance. EDIT: Approach from Parcly Taxel answer: Checking maximum and minimum for $3x+8y$ : $\phi (x,y,\lambda) = 3x+8y + \lambda(x^2+y^2-1)$ $\phi_x = 3 + 2\lambda x = 0 \Longrightarrow x= \frac{-3}{2\lambda}$ $\phi_y = 8 + 2 \lambda y = 0 \Longrightarrow y= \frac{-4}{\lambda}$ $\phi_{\lambda} = x^2 +y^2 -1 = 0 \Longrightarrow \frac{9}{4\lambda^2} + \frac{16}{\lambda^2}-1=0 \Longrightarrow 9+64=4\lambda^2 \Longrightarrow \lambda^2 = \frac{73}{4} \Longrightarrow \lambda = \pm \frac{\sqrt{73}}{2}$ So: $x= \pm 0.3511 $ $y = \pm 0.9363$ $(0.3511, 0.9363) , (-0.3511, -0.9364)$","This question already has answers here : Solving trigonometric equations of the form $a\sin x + b\cos x = c$ (7 answers) Closed 3 years ago . Polar Coordinates approach: Let (We can see that this solved the circle equation). Substituting them into my function: Taking derivative to find critical points: Now , so I need to find when So I get two Points: and now I need to find in order to know which point is minimum and which is maximum: . for first point: , .  So , and , which means it is a local minimum point. for second point: , . and same as above, it is a minimum point. Would appreciate any help in approving this solution or finding mistakes in it, and if there's any more efficient approaches, thanks in advance. EDIT: Approach from Parcly Taxel answer: Checking maximum and minimum for : So:","f(x,y)=2^{3x+8y} x^2+y^2=1 x=cos(t), y=sin(t), 0\le t \le 2\pi f = 2^{3\cos(t)+8\sin(t)} \Longrightarrow f=e^{(3\cos(t)+8\sin(t))ln(2)} f' = \ln(2)(-3\sin(t)+8\cos(t))e^{(3\cos(t)+8\sin(t))ln(2)}=0 e^x>0 -3\sin(t)+8\cos(t)=0 \Longrightarrow 3 \sin(t)=8\cos(t) \Longrightarrow \tan(t) = \frac{8}{3} \Longrightarrow t=\arctan(\frac{8}{3}), \arctan(\frac{8}{3}) + \pi  (\cos(\arctan(\frac{8}{3})) ,\sin(\arctan(\frac{8}{3}))) (\cos(\arctan(\frac{8}{3}) + \pi), \sin(\arctan(\frac{8}{3})) f_{xx}, f_{yy}, f{xy} f_{x}=8^x3ln(2) f_y = 256^y 8ln(2) f_{xx} = 9\ln^2(2) 8^x f_{yy} = 64ln^2(2) 256^y f_{xy} = 0 f_{yx} = 0 f_{xx} = 8.97 f_{yy} = 5530.131 f_{xx}*f_{yy} - 0 > 0 f_{xx} > 0  f_{xx} = 2.03 f_{yy} = 0.17 3x+8y \phi (x,y,\lambda) = 3x+8y + \lambda(x^2+y^2-1) \phi_x = 3 + 2\lambda x = 0 \Longrightarrow x= \frac{-3}{2\lambda} \phi_y = 8 + 2 \lambda y = 0 \Longrightarrow y= \frac{-4}{\lambda} \phi_{\lambda} = x^2 +y^2 -1 = 0 \Longrightarrow \frac{9}{4\lambda^2} + \frac{16}{\lambda^2}-1=0 \Longrightarrow 9+64=4\lambda^2 \Longrightarrow \lambda^2 = \frac{73}{4} \Longrightarrow \lambda = \pm \frac{\sqrt{73}}{2} x= \pm 0.3511  y = \pm 0.9363 (0.3511, 0.9363) , (-0.3511, -0.9364)","['multivariable-calculus', 'solution-verification', 'maxima-minima']"
56,"Calculating $\int_{\mathbb{R}^5}\frac{e^{-x^2-y^2-z^2}}{1+w^2+s^2}\,dx\,dy\,dz\,dw\,ds$",Calculating,"\int_{\mathbb{R}^5}\frac{e^{-x^2-y^2-z^2}}{1+w^2+s^2}\,dx\,dy\,dz\,dw\,ds","Calculate the following: $$\int_{\mathbb{R}^5}\frac{e^{-x^2-y^2-z^2}}{1+w^2+s^2}\,\mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z\,\mathrm{d}w\,\mathrm{d}s$$ I tried to do the following (based on the suggestion below): \begin{align} \int_{\mathbb{R}^5}\frac{e^{-x^2-y^2-z^2}}{1+w^2+s^2}\,\mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z\,\mathrm{d}w\,\mathrm{d}s &= \int_{\mathbb{R}^3}{e^{-x^2-y^2-z^2}}\mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z\int_{\mathbb{R}^2}\frac{1}{1+w^2+s^2}\,\mathrm{d}w\,\mathrm{d}s \\&=\pi^{3/2}\int_{0}^{2\pi}\int_{0}^{\infty}\frac{1}{1+r^2}r\,\mathrm{d}r\,\mathrm{d}\theta \\&=\pi^{3/2}\cdot2\pi\left(\frac{1}{2}\ln(1+r^2)\right)\bigg|_{0}^{\infty} \\&=\infty \end{align} but I'm not sure about it, would appreciate your help:)","Calculate the following: I tried to do the following (based on the suggestion below): but I'm not sure about it, would appreciate your help:)","\int_{\mathbb{R}^5}\frac{e^{-x^2-y^2-z^2}}{1+w^2+s^2}\,\mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z\,\mathrm{d}w\,\mathrm{d}s \begin{align}
\int_{\mathbb{R}^5}\frac{e^{-x^2-y^2-z^2}}{1+w^2+s^2}\,\mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z\,\mathrm{d}w\,\mathrm{d}s &= \int_{\mathbb{R}^3}{e^{-x^2-y^2-z^2}}\mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z\int_{\mathbb{R}^2}\frac{1}{1+w^2+s^2}\,\mathrm{d}w\,\mathrm{d}s
\\&=\pi^{3/2}\int_{0}^{2\pi}\int_{0}^{\infty}\frac{1}{1+r^2}r\,\mathrm{d}r\,\mathrm{d}\theta
\\&=\pi^{3/2}\cdot2\pi\left(\frac{1}{2}\ln(1+r^2)\right)\bigg|_{0}^{\infty}
\\&=\infty
\end{align}","['multivariable-calculus', 'multiple-integral']"
57,"Prove convexity of $f(x_1,x_2,x_3)=\frac{x_1^2+x_1+1}{x_2+x_3}$ over $\{(x_1,x_2,x_3) : x_2,x_3\gt0\}$",Prove convexity of  over,"f(x_1,x_2,x_3)=\frac{x_1^2+x_1+1}{x_2+x_3} \{(x_1,x_2,x_3) : x_2,x_3\gt0\}","Prove convexity of $$f(x_1,x_2,x_3)=\frac{x_1^2+x_1+1}{x_2+x_3}$$ over $\{(x_1,x_2,x_3) : x_2,x_3\gt0\}$ I am looking for an easy (or relatively easy) way to show $f$ is convex. I tried to split the function into: $$\frac{x_1^2}{x_2+x_3}+\frac{1}{x_2+x_3}+\frac{x_1}{x_2+x_3}$$ The first and the second functions are convex: The first is convex because it's quadratic-over-linear which is convex; The second is convex because it's linear change of variables of $\frac{1}{x}$ which is convex over $x>0$ . I stuck proving the third one. I found it's Hessian matrix. Let $g(x)=\frac{x_1}{x_2+x_3}$ . $$\nabla ^2g(x)= \begin{bmatrix} 0 & \frac{-1}{(x_2+x_3)^2} & \frac{-1}{(x_2+x_3)^2} \\ \frac{-1}{(x_2+x_3)^2} & \frac{2x_1}{(x_2+x_3)^3} & \frac{2x_1}{(x_2+x_3)^3} \\ \frac{-1}{(x_2+x_3)^2} & \frac{2x_1}{(x_2+x_3)^3} & \frac{2x_1}{(x_2+x_3)^3} \end{bmatrix} $$ It suffices to show $\nabla g(x)\succeq0$ , but I failed to do so. Last hope is calculating eigenvalues, but it seems too difficult. By the way, I can't use level-sets . Maybe there's an eaier way to show its convexity. Please advise. Thank you.","Prove convexity of over I am looking for an easy (or relatively easy) way to show is convex. I tried to split the function into: The first and the second functions are convex: The first is convex because it's quadratic-over-linear which is convex; The second is convex because it's linear change of variables of which is convex over . I stuck proving the third one. I found it's Hessian matrix. Let . It suffices to show , but I failed to do so. Last hope is calculating eigenvalues, but it seems too difficult. By the way, I can't use level-sets . Maybe there's an eaier way to show its convexity. Please advise. Thank you.","f(x_1,x_2,x_3)=\frac{x_1^2+x_1+1}{x_2+x_3} \{(x_1,x_2,x_3) : x_2,x_3\gt0\} f \frac{x_1^2}{x_2+x_3}+\frac{1}{x_2+x_3}+\frac{x_1}{x_2+x_3} \frac{1}{x} x>0 g(x)=\frac{x_1}{x_2+x_3} \nabla ^2g(x)=
\begin{bmatrix}
0 & \frac{-1}{(x_2+x_3)^2} & \frac{-1}{(x_2+x_3)^2} \\
\frac{-1}{(x_2+x_3)^2} & \frac{2x_1}{(x_2+x_3)^3} & \frac{2x_1}{(x_2+x_3)^3} \\
\frac{-1}{(x_2+x_3)^2} & \frac{2x_1}{(x_2+x_3)^3} & \frac{2x_1}{(x_2+x_3)^3}
\end{bmatrix}
 \nabla g(x)\succeq0","['multivariable-calculus', 'optimization', 'convex-analysis', 'convex-optimization', 'nonlinear-optimization']"
58,Converting polar unit vectors to Cartesian,Converting polar unit vectors to Cartesian,,"As far as I am aware, converting unit vectors from Cartesian to polar coordinates works as follows Express the transformation rules $$ x = r \cos \theta \\ y = r \sin \theta $$ Express the position vector in Cartesian coordinates $$ \vec{r} = x \vec{e}_x + y \vec{e}_y $$ Write the components of the position vector in polar coordinates $$ \vec{r} = r \cos \theta \vec{e}_x + r \sin \theta \vec{e}_y $$ The unit vectors in polar coordinates will be $$ \vec{e}_r = \frac{ \frac{\partial \vec{r}}{\partial r} }{\left| \frac{\partial \vec{r}}{\partial r} \right| } = \cos \theta \vec{e}_x + \sin \theta \vec{e}_y; \qquad \vec{e}_\theta = \frac{ \frac{\partial \vec{r}}{\partial \theta} }{\left| \frac{\partial \vec{r}}{\partial \theta} \right| } = -\sin \theta \vec{e}_x + \cos \theta \vec{e}_y $$ I'd say I'm quite confident about this transformation rule as it agrees with all the sources I can find on this topic. This makes me believe that the steps in the derivation are good and consistent. I am trying to do the same thing backwards, eg. start from polar coordinates and transform the unit vectors back to Cartesian. I've attempted the following steps, quite analogous to the ones above Express the transformation rules (with being aware of the limitation of $\tan^{-1}$ ) $$ r = \sqrt{x^2 + y^2} \\ \theta = \tan^{-1} (y/x) $$ Express the position vector in polar coordinates $$ \vec{r} = r \vec{e}_r $$ Write the components of the position vector in polar coordinates $$ \vec{r} = \sqrt{x^2+y^2} \vec{e}_r $$ The unit vectors in Cartesian coordinates will be $$ \vec{e}_x = \frac{ \frac{\partial \vec{r}}{\partial x} }{\left| \frac{\partial \vec{r}}{\partial x} \right| } =  \vec{e}_r ; \qquad \vec{e}_y = \frac{ \frac{\partial \vec{r}}{\partial y} }{\left| \frac{\partial \vec{r}}{\partial y} \right| } =  \vec{e}_r $$ Here I am pretty sure that my result is incorrect, seeing that I am getting the same exact vector for both $x$ and $y$ unit vectors, which makes no sense to me. However, I think I am faithfully following the necessary steps, and I don't think I've made a miscalculation anywhere. This makes me believe that there's a conceptual mistake somewhere. Could someone kindly point out to me what I did wrong, and how to do it correctly?","As far as I am aware, converting unit vectors from Cartesian to polar coordinates works as follows Express the transformation rules Express the position vector in Cartesian coordinates Write the components of the position vector in polar coordinates The unit vectors in polar coordinates will be I'd say I'm quite confident about this transformation rule as it agrees with all the sources I can find on this topic. This makes me believe that the steps in the derivation are good and consistent. I am trying to do the same thing backwards, eg. start from polar coordinates and transform the unit vectors back to Cartesian. I've attempted the following steps, quite analogous to the ones above Express the transformation rules (with being aware of the limitation of ) Express the position vector in polar coordinates Write the components of the position vector in polar coordinates The unit vectors in Cartesian coordinates will be Here I am pretty sure that my result is incorrect, seeing that I am getting the same exact vector for both and unit vectors, which makes no sense to me. However, I think I am faithfully following the necessary steps, and I don't think I've made a miscalculation anywhere. This makes me believe that there's a conceptual mistake somewhere. Could someone kindly point out to me what I did wrong, and how to do it correctly?", x = r \cos \theta \\ y = r \sin \theta   \vec{r} = x \vec{e}_x + y \vec{e}_y   \vec{r} = r \cos \theta \vec{e}_x + r \sin \theta \vec{e}_y   \vec{e}_r = \frac{ \frac{\partial \vec{r}}{\partial r} }{\left| \frac{\partial \vec{r}}{\partial r} \right| } = \cos \theta \vec{e}_x + \sin \theta \vec{e}_y; \qquad \vec{e}_\theta = \frac{ \frac{\partial \vec{r}}{\partial \theta} }{\left| \frac{\partial \vec{r}}{\partial \theta} \right| } = -\sin \theta \vec{e}_x + \cos \theta \vec{e}_y  \tan^{-1}  r = \sqrt{x^2 + y^2} \\ \theta = \tan^{-1} (y/x)   \vec{r} = r \vec{e}_r   \vec{r} = \sqrt{x^2+y^2} \vec{e}_r   \vec{e}_x = \frac{ \frac{\partial \vec{r}}{\partial x} }{\left| \frac{\partial \vec{r}}{\partial x} \right| } =  \vec{e}_r ; \qquad \vec{e}_y = \frac{ \frac{\partial \vec{r}}{\partial y} }{\left| \frac{\partial \vec{r}}{\partial y} \right| } =  \vec{e}_r  x y,['real-analysis']
59,"Global Maximum of $f(x,y)$ on a set $M$.",Global Maximum of  on a set .,"f(x,y) M","I want to investigate the function $f(x,y) = 4x^2 + 9y - \frac{1}{3}y^3$ on the set $M:= \{(x,y) \in \mathbb{R}^2 : y \geq |x|\}$ . In particular, I want to a) proof the existence of a global maximum on the set $M$ , b) find all points in $M$ where this maximal value is reached. First of all I did draw a picture of the set $M$ . (Since $M$ is not very hard to illustrate I do not post it here.) I really struggle with a). The only real theorem I know is that every continuous function on a compact set has a Min/Max. Since $M$ is not compact, I don't see how I can proof a). I thought about quadratic expansion, but that did not work. Considering b), I first took a look at the gradient \begin{align} \operatorname{grad} f(x,y) = \begin{pmatrix} 8x \\ 9 - y^2\end{pmatrix} \end{align} which is zero for $(0,3)$ and $(0,-3) \notin M$ . Using the Hessian Matrix $\mathcal{H}_f$ and evaluating it at $(0,3)$ leads to the fact that $(0,3)$ seems to be a saddle point. Hence, there looks to be no inner point, which might be interesting for the question. Looking at the boundaries of $M$ , as Fra suggested in his answer, I looked at $$f|_{\partial M}(x,y) = \begin{cases} g(x) := 4x^2 +9x - \frac{x^3}{3}, & \mbox{if } x \geq 0\\ h(x) := 4x^2 -9x + \frac{x^3}{3}, & \mbox{if } x < 0 \end{cases}$$ and found that $g$ has a maximum, namely $\max\{4x^2+9x - \frac{x^3}{3}\} = 162$ for $x = 9$ . $h$ as well has a maximum, $\max\{4x^2-9x + \frac{x^3}{3}\} = 162$ for $x = -9$ . So my answer to b) is $\{(9,9), (-9,9)\}$ with value $162$ . Can someone provide a solution/explanation for a)? Furthermore I would be glad if my solution for b) can be verified.","I want to investigate the function on the set . In particular, I want to a) proof the existence of a global maximum on the set , b) find all points in where this maximal value is reached. First of all I did draw a picture of the set . (Since is not very hard to illustrate I do not post it here.) I really struggle with a). The only real theorem I know is that every continuous function on a compact set has a Min/Max. Since is not compact, I don't see how I can proof a). I thought about quadratic expansion, but that did not work. Considering b), I first took a look at the gradient which is zero for and . Using the Hessian Matrix and evaluating it at leads to the fact that seems to be a saddle point. Hence, there looks to be no inner point, which might be interesting for the question. Looking at the boundaries of , as Fra suggested in his answer, I looked at and found that has a maximum, namely for . as well has a maximum, for . So my answer to b) is with value . Can someone provide a solution/explanation for a)? Furthermore I would be glad if my solution for b) can be verified.","f(x,y) = 4x^2 + 9y - \frac{1}{3}y^3 M:= \{(x,y) \in \mathbb{R}^2 : y \geq |x|\} M M M M M \begin{align}
\operatorname{grad} f(x,y) = \begin{pmatrix} 8x \\ 9 - y^2\end{pmatrix}
\end{align} (0,3) (0,-3) \notin M \mathcal{H}_f (0,3) (0,3) M f|_{\partial M}(x,y) = \begin{cases} g(x) := 4x^2 +9x - \frac{x^3}{3}, & \mbox{if } x \geq 0\\
h(x) := 4x^2 -9x + \frac{x^3}{3}, & \mbox{if } x < 0
\end{cases} g \max\{4x^2+9x - \frac{x^3}{3}\} = 162 x = 9 h \max\{4x^2-9x + \frac{x^3}{3}\} = 162 x = -9 \{(9,9), (-9,9)\} 162","['real-analysis', 'multivariable-calculus', 'maxima-minima', 'extreme-value-theorem']"
60,Why does Green's theorem fail for non simple curves?,Why does Green's theorem fail for non simple curves?,,"In looking at the proof of Green's theorem, it is not obvious to me why it must be a simple curve. I was thinking that perhaps it would still apply for a closed curve that crosses itself a countable number of times since then it could be broken up into the sum of countably many cases where it does apply. Does anybody have any insight on where it breaks down specifically with non-simple curves and or if it can be extended to countably many crossings?","In looking at the proof of Green's theorem, it is not obvious to me why it must be a simple curve. I was thinking that perhaps it would still apply for a closed curve that crosses itself a countable number of times since then it could be broken up into the sum of countably many cases where it does apply. Does anybody have any insight on where it breaks down specifically with non-simple curves and or if it can be extended to countably many crossings?",,['multivariable-calculus']
61,"Calculate $\partial^\alpha(x_j f(x_1,...,x_n))$",Calculate,"\partial^\alpha(x_j f(x_1,...,x_n))","Just a very basic computational question: Let $\alpha \in \mathbb{N}^n_0$ . Calculate $\partial^\alpha(x_j f(x_1,...,x_n))$ where $f :\mathbb{R}^n \to \mathbb{C}$ is smooth and $j \in \{1,...,n\}$ . Using Leibniz I get, $\partial^\alpha(x_j f(x_1,...,x_n))= x_j\partial^\alpha f+ \alpha_j \partial^{\alpha-e_j}f$ . But my book says $\partial^\alpha(x_j f(x_1,...,x_n))= x_j\partial^\alpha f+ \alpha_j \partial^{\alpha-\alpha_je_j}f$ . The difference being in the exponents of the partial derivatives in the second terms. Where did I go wrong?","Just a very basic computational question: Let . Calculate where is smooth and . Using Leibniz I get, . But my book says . The difference being in the exponents of the partial derivatives in the second terms. Where did I go wrong?","\alpha \in \mathbb{N}^n_0 \partial^\alpha(x_j f(x_1,...,x_n)) f :\mathbb{R}^n \to \mathbb{C} j \in \{1,...,n\} \partial^\alpha(x_j f(x_1,...,x_n))= x_j\partial^\alpha f+ \alpha_j \partial^{\alpha-e_j}f \partial^\alpha(x_j f(x_1,...,x_n))= x_j\partial^\alpha f+ \alpha_j \partial^{\alpha-\alpha_je_j}f","['real-analysis', 'multivariable-calculus']"
62,Get wrong answer on $\frac{\partial \mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}$ when using graph,Get wrong answer on  when using graph,\frac{\partial \mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}},"I can use the product rule to obtain $\frac{\partial \mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{x}^{\top} \frac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}+(\mathbf{A} \mathbf{x})^{\top} \frac{\partial \mathbf{x}}{\partial \mathbf{x}}$ , and get the right result $\mathbf{x}^{\top}\left(\mathbf{A}+\mathbf{A}^{\top}\right)$ . Here $x$ is a vector, $A$ is a matrix that is not a function of $x$ . However, when I try to calculate by using a graph, the result is different. I am not sure at which step I made the mistake. Please point out where I made it wrong. I used the graph attached ( red numbers are the derivatives) and the equations as follows. Let $ q_{2}=q_{1}x$ , $q_{1}=(A^{T} x)^{T}$ ,  and $k=A^Tx$ . I want to find $\frac{d q_{2}}{d x}$ $\frac{d q_{2}}{d x}=\frac{\partial q_{2}}{\partial x}+\frac{\partial q_{2}}{\partial q_{1}} \frac{\partial q_{1}}{\partial x}= q_{1}+x \frac{\partial q_{1}}{\partial x}= q_{1}+x \frac{\partial q_{1}}{\partial k} \frac{\partial k}{\partial x}  =x^TA+xIA^T = x^TA+xA^T \ne x^{\top}\left({A}+{A}^{\top}\right)$ , $I$ is the NxN identity matrix. The result I got here is different. I know it is not correct, but I couldn't figure out where I lost it. Thank you!","I can use the product rule to obtain , and get the right result . Here is a vector, is a matrix that is not a function of . However, when I try to calculate by using a graph, the result is different. I am not sure at which step I made the mistake. Please point out where I made it wrong. I used the graph attached ( red numbers are the derivatives) and the equations as follows. Let , ,  and . I want to find , is the NxN identity matrix. The result I got here is different. I know it is not correct, but I couldn't figure out where I lost it. Thank you!","\frac{\partial \mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{x}^{\top} \frac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}+(\mathbf{A} \mathbf{x})^{\top} \frac{\partial \mathbf{x}}{\partial \mathbf{x}} \mathbf{x}^{\top}\left(\mathbf{A}+\mathbf{A}^{\top}\right) x A x 
q_{2}=q_{1}x q_{1}=(A^{T} x)^{T} k=A^Tx \frac{d q_{2}}{d x} \frac{d q_{2}}{d x}=\frac{\partial q_{2}}{\partial x}+\frac{\partial q_{2}}{\partial q_{1}} \frac{\partial q_{1}}{\partial x}= q_{1}+x \frac{\partial q_{1}}{\partial x}= q_{1}+x \frac{\partial q_{1}}{\partial k} \frac{\partial k}{\partial x}  =x^TA+xIA^T = x^TA+xA^T \ne x^{\top}\left({A}+{A}^{\top}\right) I","['multivariable-calculus', 'derivatives', 'inner-products', 'quadratic-forms', 'scalar-fields']"
63,Write the integral $\int_0^\infty \left(\frac{1}{1 + x^2} \right)^\alpha x^\beta dx$ in terms of the Euler Beta Function?,Write the integral  in terms of the Euler Beta Function?,\int_0^\infty \left(\frac{1}{1 + x^2} \right)^\alpha x^\beta dx,"How can I write the integral $$\int_0^\infty \left(\frac{1}{1 + x^2} \right)^\alpha x^\beta dx$$ with $\alpha, \beta >0$ , in terms of the Euler Beta Function ? For which $\alpha, \beta$ does the integral actually converge? Maybe some change of variable can do the trick?","How can I write the integral with , in terms of the Euler Beta Function ? For which does the integral actually converge? Maybe some change of variable can do the trick?","\int_0^\infty \left(\frac{1}{1 + x^2} \right)^\alpha x^\beta dx \alpha, \beta >0 \alpha, \beta","['calculus', 'integration', 'multivariable-calculus', 'special-functions']"
64,Double integration using a change of variables for $\iint_D\sqrt{x}\ dx\ dy $,Double integration using a change of variables for,\iint_D\sqrt{x}\ dx\ dy ,"I need to calculate this double integral using change of variables $$\iint_D\sqrt{x}\ dx\ dy $$ with $D=\{(x,y) \mid x^2+y^2 < x\}$ . I thought of putting $x=r\cos\theta$ and $y=r\sin\theta$ . So $x^2+y^2<x$ will become $r<\cos\theta$ (should I suppose here that $r \ne 0$ ?). I'll just rewrite the integral as $$\int_0^{2\pi}\int_0^{\cos\theta}r\sqrt{r\cos\theta}$$ which is double with a bit of calculation. I would be very thankful if someone can tell me if my method is right. If not, how can I solve it? Thank you very much.","I need to calculate this double integral using change of variables with . I thought of putting and . So will become (should I suppose here that ?). I'll just rewrite the integral as which is double with a bit of calculation. I would be very thankful if someone can tell me if my method is right. If not, how can I solve it? Thank you very much.","\iint_D\sqrt{x}\ dx\ dy  D=\{(x,y) \mid x^2+y^2 < x\} x=r\cos\theta y=r\sin\theta x^2+y^2<x r<\cos\theta r \ne 0 \int_0^{2\pi}\int_0^{\cos\theta}r\sqrt{r\cos\theta}","['integration', 'multivariable-calculus', 'change-of-variable']"
65,Interchange integration and differentiation,Interchange integration and differentiation,,"Let $x>0$ . It can be shown that $$ \int_{0}^{\infty} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \lambda}dt = \frac{\Gamma(\lambda - \frac{1}{2})}{4^{-\lambda +1}\sqrt{\pi}}|x|^{-2\lambda +1}~\lambda > \frac{1}{2}.$$ Let $-1<\lambda <0 $ , then $$ -\int_{0}^{\infty} \frac{\partial}{\partial x} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda}{2}}dt = \int_{0}^{\infty} \frac{x}{2} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda + 2}{2} }dt,$$ which is integrable. I would like to prove that $$ -\frac{\partial}{\partial x} \int_{0}^{\infty} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda}{2}}dt = \int_{0}^{\infty} \frac{\partial}{\partial x} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda}{2}}dt. $$ But I can't find integrable function $f(t)$ such that $|\frac{\partial}{\partial x} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda}{2}}| \leq f(t)$ . Hence DTC can't be used. How can I justify differentiation under integral sign? I would really appreciate any hints or tips.","Let . It can be shown that Let , then which is integrable. I would like to prove that But I can't find integrable function such that . Hence DTC can't be used. How can I justify differentiation under integral sign? I would really appreciate any hints or tips.","x>0  \int_{0}^{\infty} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \lambda}dt = \frac{\Gamma(\lambda - \frac{1}{2})}{4^{-\lambda +1}\sqrt{\pi}}|x|^{-2\lambda +1}~\lambda > \frac{1}{2}. -1<\lambda <0   -\int_{0}^{\infty} \frac{\partial}{\partial x} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda}{2}}dt = \int_{0}^{\infty} \frac{x}{2} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda + 2}{2} }dt,  -\frac{\partial}{\partial x} \int_{0}^{\infty} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda}{2}}dt = \int_{0}^{\infty} \frac{\partial}{\partial x} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda}{2}}dt.  f(t) |\frac{\partial}{\partial x} \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}t^{- \frac{\lambda}{2}}| \leq f(t)","['real-analysis', 'multivariable-calculus', 'derivatives', 'lebesgue-integral']"
66,"Tangent planes for the function $f(x,y) = 1 - x^2 - y^2$",Tangent planes for the function,"f(x,y) = 1 - x^2 - y^2","I have to solve the following statements, finding the tangent planes to $f$ and its points of tangency: that contains the line in $\mathbb{R}^3$ that passes through the points $(3,0,3)$ and $(0,-3,3)$ and this other statement: that contains the point $(0,0,2)$ For the first statement, I tried first making the parametric equation and I found that $L(t) = (3-3t, -3t, 3)$ , and I also know that the equation of the plane is given by $$p(x,y) = \frac{\partial f}{\partial x}(x_0,y_0)(x-x_0) + \frac{\partial f}{\partial y}(x_0,y_0)(y-y_0) + f(x_0,y_0) = -2x_0 (x-x_0) -2y_0 (y-y_0) + (1 -x_0^2 - y_0^2)$$ but after that, I don´t see what else to do. Could you give me any hint for both statements?","I have to solve the following statements, finding the tangent planes to and its points of tangency: that contains the line in that passes through the points and and this other statement: that contains the point For the first statement, I tried first making the parametric equation and I found that , and I also know that the equation of the plane is given by but after that, I don´t see what else to do. Could you give me any hint for both statements?","f \mathbb{R}^3 (3,0,3) (0,-3,3) (0,0,2) L(t) = (3-3t, -3t, 3) p(x,y) = \frac{\partial f}{\partial x}(x_0,y_0)(x-x_0) + \frac{\partial f}{\partial y}(x_0,y_0)(y-y_0) + f(x_0,y_0) = -2x_0 (x-x_0) -2y_0 (y-y_0) + (1 -x_0^2 - y_0^2)","['calculus', 'multivariable-calculus', 'partial-derivative', 'plane-curves', 'tangent-line']"
67,"Study the uniform convergence of the integral $\int_0^\infty\frac{\sin \alpha x}{\alpha^2+x^2}\mathrm{d}x, \alpha\in(0,+\infty)$",Study the uniform convergence of the integral,"\int_0^\infty\frac{\sin \alpha x}{\alpha^2+x^2}\mathrm{d}x, \alpha\in(0,+\infty)","Study the uniform convergence of the integral $$\int_0^\infty\frac{\sin \alpha x}{\alpha^2+x^2}\mathrm{d}x,\quad \alpha\in(0,+\infty)$$ I have tried the Weierstrass criterion, but $\int_0^\infty\frac{1}{\alpha^2+x^2}\mathrm{d}x$ does not converge. Then I tried the Dirichlet criterion, but $\int_0^A \sin \alpha x$ are not bounded uniformly. So I turned to prove it does not converge uniformly. My attempt are as follows $\forall A>1$ , choose $\alpha=\frac{\pi}{4A}$ then $\forall x\in[A, 2A]$ $$ \left|\int_A^{2A}\frac{\sin \alpha x}{\alpha^2+x^2}\mathrm{d}x\right|>\frac{\sqrt{2}}{2}\frac{1}{\alpha}(\arctan\frac{2A}{\alpha}-\arctan\frac{A}{\alpha}) $$ but I cannot find a lower bound of RHS, so the method seems do not work. Is my way wrong? You can also suggest your way. Appreciate any help!","Study the uniform convergence of the integral I have tried the Weierstrass criterion, but does not converge. Then I tried the Dirichlet criterion, but are not bounded uniformly. So I turned to prove it does not converge uniformly. My attempt are as follows , choose then but I cannot find a lower bound of RHS, so the method seems do not work. Is my way wrong? You can also suggest your way. Appreciate any help!","\int_0^\infty\frac{\sin \alpha x}{\alpha^2+x^2}\mathrm{d}x,\quad \alpha\in(0,+\infty) \int_0^\infty\frac{1}{\alpha^2+x^2}\mathrm{d}x \int_0^A \sin \alpha x \forall A>1 \alpha=\frac{\pi}{4A} \forall x\in[A, 2A] 
\left|\int_A^{2A}\frac{\sin \alpha x}{\alpha^2+x^2}\mathrm{d}x\right|>\frac{\sqrt{2}}{2}\frac{1}{\alpha}(\arctan\frac{2A}{\alpha}-\arctan\frac{A}{\alpha})
","['integration', 'multivariable-calculus', 'convergence-divergence', 'improper-integrals', 'uniform-convergence']"
68,What is the obvious mistake of this double integration?,What is the obvious mistake of this double integration?,,"There is a definite double integral in Cartesian coordinates as follows: $$\int_{-2}^2\int_{-\sqrt{4-x^2}}^{\sqrt{4-x^2}} 2\sqrt{4-x^2}\ dydx\Rightarrow \int_{-2}^{2} 2y\sqrt{4-x^2}\rvert_{-\sqrt{4-x^2}}^{\sqrt{4-x^2}}  \ dx \Rightarrow 4\int_{-2}^{2} (4 - x^2)dx \Rightarrow 4(4x - \frac{x^3}{3})|_{-2}^{2} = \frac{128}{3} $$ and there is another double integral in polar coordinates which is supposed to be the exact same integral as the above, but the final answer is not 128/3. it took me several hours, but I cant figure out which step of this integration is wrong. $$ \int_{0}^{2\pi}\int_{0}^{2} 2r\sqrt{4-r^2{\cos^2\theta}} drd\theta  \xrightarrow[]{u = 4-r^2{\cos^2\theta}}  \int_{0}^{2\pi} - \frac{2}{3} \frac{(4-r^2{\cos^2\theta})^\frac{3}{2}}{\cos^2\theta}\Big|_{0}^{2}  \ d\theta \Rightarrow \int_{0}^{2\pi} (-\frac{2}{3}\frac{(4-4\cos^2\theta)^\frac{3}{2}}{\cos^2\theta} + \frac{2}{3}\frac{4^\frac{3}{2}}{\cos^2\theta} ) d\theta \Rightarrow \frac{16}{3}\int_{0}^{2\pi} \frac{1-(1-\cos^2\theta)^\frac{3}{2}}{\cos^2\theta} d\theta \Rightarrow  \frac{16}{3} \int_{0}^{2\pi} \frac{1-{\sin^3\theta}}{\cos^2\theta} d\theta \Rightarrow \frac{16}{3}\int_{0}^{2\pi} \frac{1}{\cos^2\theta} - \frac{\sin^3\theta}{\cos^2\theta} d\theta \Rightarrow \frac{16}{3}(\int_{0}^{2\pi} \frac{1}{\cos^2\theta} d\theta - \int_{0}^{2\pi} \frac{\sin^3\theta}{\cos^2\theta} d\theta) \Rightarrow \frac{16}{3}(\tan\theta - \int_{0}^{2\pi} \frac{\sin^3\theta}{\cos^2\theta} d\theta) \xrightarrow[]{u=\cos\theta} \frac{16}{3}(\tan\theta - \int \frac{1-u^2}{u^2} du) \Rightarrow \frac{16}{3}(\tan\theta - (-\sec\theta - \cos\theta))\Big|_{0}^{2\pi}\Rightarrow \frac{16}{3} ((0+1+1) - (0+1+1)) = 0 $$ I know there is a huge problem with the second one and my apologies if it's an elementary problem. But I would really appreciate if you could help me find the problem.","There is a definite double integral in Cartesian coordinates as follows: and there is another double integral in polar coordinates which is supposed to be the exact same integral as the above, but the final answer is not 128/3. it took me several hours, but I cant figure out which step of this integration is wrong. I know there is a huge problem with the second one and my apologies if it's an elementary problem. But I would really appreciate if you could help me find the problem.","\int_{-2}^2\int_{-\sqrt{4-x^2}}^{\sqrt{4-x^2}} 2\sqrt{4-x^2}\ dydx\Rightarrow \int_{-2}^{2} 2y\sqrt{4-x^2}\rvert_{-\sqrt{4-x^2}}^{\sqrt{4-x^2}}  \ dx \Rightarrow 4\int_{-2}^{2} (4 - x^2)dx \Rightarrow 4(4x - \frac{x^3}{3})|_{-2}^{2} = \frac{128}{3}   \int_{0}^{2\pi}\int_{0}^{2} 2r\sqrt{4-r^2{\cos^2\theta}} drd\theta
 \xrightarrow[]{u = 4-r^2{\cos^2\theta}}  \int_{0}^{2\pi} - \frac{2}{3} \frac{(4-r^2{\cos^2\theta})^\frac{3}{2}}{\cos^2\theta}\Big|_{0}^{2}  \ d\theta \Rightarrow \int_{0}^{2\pi} (-\frac{2}{3}\frac{(4-4\cos^2\theta)^\frac{3}{2}}{\cos^2\theta} + \frac{2}{3}\frac{4^\frac{3}{2}}{\cos^2\theta} ) d\theta \Rightarrow \frac{16}{3}\int_{0}^{2\pi} \frac{1-(1-\cos^2\theta)^\frac{3}{2}}{\cos^2\theta} d\theta \Rightarrow  \frac{16}{3} \int_{0}^{2\pi} \frac{1-{\sin^3\theta}}{\cos^2\theta} d\theta \Rightarrow \frac{16}{3}\int_{0}^{2\pi} \frac{1}{\cos^2\theta} - \frac{\sin^3\theta}{\cos^2\theta} d\theta \Rightarrow \frac{16}{3}(\int_{0}^{2\pi} \frac{1}{\cos^2\theta} d\theta - \int_{0}^{2\pi} \frac{\sin^3\theta}{\cos^2\theta} d\theta) \Rightarrow \frac{16}{3}(\tan\theta - \int_{0}^{2\pi} \frac{\sin^3\theta}{\cos^2\theta} d\theta) \xrightarrow[]{u=\cos\theta} \frac{16}{3}(\tan\theta - \int \frac{1-u^2}{u^2} du) \Rightarrow \frac{16}{3}(\tan\theta - (-\sec\theta - \cos\theta))\Big|_{0}^{2\pi}\Rightarrow \frac{16}{3} ((0+1+1) - (0+1+1)) = 0 ","['multivariable-calculus', 'definite-integrals', 'polar-coordinates', 'multiple-integral']"
69,How to transform quadratic terms to something like $u^2+v^2$?,How to transform quadratic terms to something like ?,u^2+v^2,"I am told that by the substitution of $x = (a\cos \theta)u - (b\sin \theta) v$ and $y = (a\sin \theta)u + (b\cos \theta)v$ , we can reduce $3x^2 + 2xy + 3y^2$ to $u^2 + v^2$ . I tried to plug in the substitutions, but I did not see how the quadratic term is reduce to $u^2 + v^2$ . What I got was something complicated containing $a$ , $b$ , $\cos \theta$ and $\sin \theta$ . I am asked to reduce $3x^2 + 2xy + 3y^2 - x - 2y$ to $u^2 + v^2$ by similar substitutions. I do not know how to construct this substitution. Could you explain how do find this kind of substitutions in general?","I am told that by the substitution of and , we can reduce to . I tried to plug in the substitutions, but I did not see how the quadratic term is reduce to . What I got was something complicated containing , , and . I am asked to reduce to by similar substitutions. I do not know how to construct this substitution. Could you explain how do find this kind of substitutions in general?",x = (a\cos \theta)u - (b\sin \theta) v y = (a\sin \theta)u + (b\cos \theta)v 3x^2 + 2xy + 3y^2 u^2 + v^2 u^2 + v^2 a b \cos \theta \sin \theta 3x^2 + 2xy + 3y^2 - x - 2y u^2 + v^2,"['calculus', 'multivariable-calculus']"
70,"Let $~v(x, y)~$ be the solution of $~\frac{𝜕^2𝑣}{𝜕𝑥^2}+\frac{𝜕^2𝑣}{𝜕𝑦^2}=0~$ on $~ℝ^2~$ , then which of the following is/are true?","Let  be the solution of  on  , then which of the following is/are true?","~v(x, y)~ ~\frac{𝜕^2𝑣}{𝜕𝑥^2}+\frac{𝜕^2𝑣}{𝜕𝑦^2}=0~ ~ℝ^2~","Problem: Let the function $~v(x, y)~$ be the solution of $~\frac{𝜕^2𝑣}{𝜕𝑥^2}+\frac{𝜕^2𝑣}{𝜕𝑦^2}=0~$ on $~ℝ^2~$ and $~v = x~$ on the unit circle. Then at the origin $(a)~~ v~ $ attains maximum and minimum on the boundary of the circle. $(b)~~ v~ $ does not attain maximum and minimum on the boundary of the circle. $(c)~~ v~ $ is equal to zero. $(d)~~ v~ $ tends to infinity. My approach: From the Maximum-Minimum Principle $($ if $u$ is a harmonic function on a bounded domain $Ω$ in $ℝ^n$ , then $u$ attains its maximum and minimum value on the boundary of $Ω)$ , it is clear that option $\bf (a)$ is true and hence option $\bf (b)$ is not. Now it is given that $~v = x~$ on the unit circle, so form here how to verify the last two options ?","Problem: Let the function be the solution of on and on the unit circle. Then at the origin attains maximum and minimum on the boundary of the circle. does not attain maximum and minimum on the boundary of the circle. is equal to zero. tends to infinity. My approach: From the Maximum-Minimum Principle if is a harmonic function on a bounded domain in , then attains its maximum and minimum value on the boundary of , it is clear that option is true and hence option is not. Now it is given that on the unit circle, so form here how to verify the last two options ?","~v(x, y)~ ~\frac{𝜕^2𝑣}{𝜕𝑥^2}+\frac{𝜕^2𝑣}{𝜕𝑦^2}=0~ ~ℝ^2~ ~v = x~ (a)~~ v~  (b)~~ v~  (c)~~ v~  (d)~~ v~  ( u Ω ℝ^n u Ω) \bf (a) \bf (b) ~v = x~","['real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
71,Quotient rule for multivariable functions,Quotient rule for multivariable functions,,"$\newcommand{\mbf}{\mathbf}$ Let $f,g:\mathbb R^n\to \mathbb R$ be differentiable at $a$ . $Df(a)$ is the unique linear transformation $\mathbb R^n\to\mathbb  R$ such that $$\lim_{\mbf h\to\mbf 0}\frac{|f(\mbf a+\mbf h)-\mbf f(\mbf a)-Df(\mbf a)(\mbf h)|}{|\mbf h|}=0.$$ I want to show that if $g(a)\neq 0$ , then $D(f/g)(a)=\dfrac{g(a)Df(a)-f(a)Dg(a)}{[g(a)]^2}$ . I've shown that $D(f\cdot g)(a)=g(a)Df(a)+f(a)Dg(a)$ . Let $h:\mathbb R^n\to\mathbb R$ be defined by $h(x)=\frac{1}{g(x)}$ . Then $$D(f/g)(a)=D(f\cdot h)(a)=h(a)Df(a)+f(a)Dh(a).$$ Since $h=q\circ g$ where $q:\mathbb R-\{0\}\to\mathbb R$ is defined by $q(x)=1/x$ , $$Dh(a)=D(q\circ g)(a)=Dq(g(a))\circ Dg(a)=D\frac1{g(a)}\circ Dg(a)$$ How can I simplify $D\frac 1{g(x)}$ to prove this?","Let be differentiable at . is the unique linear transformation such that I want to show that if , then . I've shown that . Let be defined by . Then Since where is defined by , How can I simplify to prove this?","\newcommand{\mbf}{\mathbf} f,g:\mathbb R^n\to \mathbb R a Df(a) \mathbb R^n\to\mathbb  R \lim_{\mbf h\to\mbf 0}\frac{|f(\mbf a+\mbf h)-\mbf f(\mbf a)-Df(\mbf a)(\mbf h)|}{|\mbf h|}=0. g(a)\neq 0 D(f/g)(a)=\dfrac{g(a)Df(a)-f(a)Dg(a)}{[g(a)]^2} D(f\cdot g)(a)=g(a)Df(a)+f(a)Dg(a) h:\mathbb R^n\to\mathbb R h(x)=\frac{1}{g(x)} D(f/g)(a)=D(f\cdot h)(a)=h(a)Df(a)+f(a)Dh(a). h=q\circ g q:\mathbb R-\{0\}\to\mathbb R q(x)=1/x Dh(a)=D(q\circ g)(a)=Dq(g(a))\circ Dg(a)=D\frac1{g(a)}\circ Dg(a) D\frac 1{g(x)}","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives']"
72,Can a polynomial have an isolated local minimum at a transcendental point?,Can a polynomial have an isolated local minimum at a transcendental point?,,"Let $f:\mathbb{R}^n \to \mathbb{R}$ be a polynomial with coefficients in $\mathbb{Q}$ . Is it possible for there to be a point $\textbf{a} \in \mathbb{R}^n$ with all transcendental coordinates such that $\textbf{a}$ is an isolated local minimum of f? By isolated I mean that there exists a neighborhood of $\textbf{a}$ such that for every $\textbf{b}$ in the neighborhood, either $f(\textbf{a}) < f(\textbf{b})$ or $\textbf{a} = \textbf{b}$ . In particular, I am thinking about the case when $f$ is non-negative and $f(\textbf{a}) = 0$ . It seems to me like such a local minimum shouldn't be possible. For $n = 1$ it is not possible. I am pretty sure it is not possible for $n=2$ . For example, if $f = (x - 2y)^2$ , then $f$ has a minimum at $(2\pi,\pi)$ , but it is not isolated. I can not figure out how to prove this for $n>2$ and I can't find any counterexamples. I have tried various ideas from calculus and for the case when $f(\textbf{a}) = 0$ , I have tried to make arguments about the dimension of the variety not being zero. I'm not very familiar with Algebraic Geometry though so maybe this idea doesn't work. Any thoughts would be appreciated.","Let be a polynomial with coefficients in . Is it possible for there to be a point with all transcendental coordinates such that is an isolated local minimum of f? By isolated I mean that there exists a neighborhood of such that for every in the neighborhood, either or . In particular, I am thinking about the case when is non-negative and . It seems to me like such a local minimum shouldn't be possible. For it is not possible. I am pretty sure it is not possible for . For example, if , then has a minimum at , but it is not isolated. I can not figure out how to prove this for and I can't find any counterexamples. I have tried various ideas from calculus and for the case when , I have tried to make arguments about the dimension of the variety not being zero. I'm not very familiar with Algebraic Geometry though so maybe this idea doesn't work. Any thoughts would be appreciated.","f:\mathbb{R}^n \to \mathbb{R} \mathbb{Q} \textbf{a} \in \mathbb{R}^n \textbf{a} \textbf{a} \textbf{b} f(\textbf{a}) < f(\textbf{b}) \textbf{a} = \textbf{b} f f(\textbf{a}) = 0 n = 1 n=2 f = (x - 2y)^2 f (2\pi,\pi) n>2 f(\textbf{a}) = 0","['multivariable-calculus', 'algebraic-geometry']"
73,Computing a 2 variable integral - switching the order of integration,Computing a 2 variable integral - switching the order of integration,,I have to compute this integral: $$\int_0^1 dy \int_{\sqrt{y}}^{1} e^{\frac{y}{x}} dx$$ Because we have not learn how to compute $\int e^{a}{x} dx$ (because it has something with gamma function etc..) it makes me think only of one option and is to flip the $dx \Leftrightarrow dy$ $\sqrt{y} = x \Rightarrow  y = x^2$ and thus $$ \int_0^1 dx \int_{x^2}^1 e^{\frac{y}{x}}dy = \int_0^1 dx (\frac{1}{x}e^{\frac{1}{x}} - \frac{1}{x}e^x)$$ Which again leads me to this gamma function.. ( $\Gamma$ ...) and we don't know how to work with it (not in our syllabus) Any help would be appreciated!! Thanks!,I have to compute this integral: Because we have not learn how to compute (because it has something with gamma function etc..) it makes me think only of one option and is to flip the and thus Which again leads me to this gamma function.. ( ...) and we don't know how to work with it (not in our syllabus) Any help would be appreciated!! Thanks!,\int_0^1 dy \int_{\sqrt{y}}^{1} e^{\frac{y}{x}} dx \int e^{a}{x} dx dx \Leftrightarrow dy \sqrt{y} = x \Rightarrow  y = x^2  \int_0^1 dx \int_{x^2}^1 e^{\frac{y}{x}}dy = \int_0^1 dx (\frac{1}{x}e^{\frac{1}{x}} - \frac{1}{x}e^x) \Gamma,"['calculus', 'integration', 'multivariable-calculus']"
74,Asymptotic Estimate of Vector Function,Asymptotic Estimate of Vector Function,,"I would like to compute the asymptotic limit of the of the following function $$f(x,\omega) = \frac{x - \omega\sqrt{1+|x|^2}}{x\cdot \omega - \sqrt{1+|x|^2}}$$ Where $x\in \mathbb{R}^3$ and $\omega \in \mathbb{S}^2 = \{y\in\mathbb{R}^3 \ | \ |y| = 1\}$ is a point on the unit sphere. More precisely, I need to estimate $||f(x,\cdot)||_{L^\infty(\mathbb{S^2)}} := \sup_{\omega\in\mathbb{S}^2}|f(x,\omega)|$ for large $|x|$ . I have the rough estimate \begin{align} |f(x,\omega)| &\leq \bigg|\frac{\frac{x}{\sqrt{1+|x|^2}} - \omega}{\frac{x\cdot\omega}{\sqrt{1+|x|^2}} - 1}\bigg| \leq \frac{2}{1 - \frac{|x|}{\sqrt{1+|x|^2}}} \\ &= \frac{2\sqrt{1+|x|^2}}{\sqrt{1+|x|^2} - |x|} = \mathcal{O}(|x|^2) \end{align} as $|x|\rightarrow \infty$ . But I am wondering if I can do better than this and obtain a sharper estimate (possibly $\mathcal{O}(|x|)$ or even $\mathcal{O}(1)$ )? Edit: To elaborate I am looking for a better function $g(|x|)$ such that \begin{align} \sup_{(\hat{x},\omega)\in\mathbb{S}^2\times\mathbb{S}^2}|f(|x|\hat{x},\omega)| \leq C g(|x|) \end{align} I suspect something like linear in $|x|$ like $g(|x|) = a|x| + b$ .","I would like to compute the asymptotic limit of the of the following function Where and is a point on the unit sphere. More precisely, I need to estimate for large . I have the rough estimate as . But I am wondering if I can do better than this and obtain a sharper estimate (possibly or even )? Edit: To elaborate I am looking for a better function such that I suspect something like linear in like .","f(x,\omega) = \frac{x - \omega\sqrt{1+|x|^2}}{x\cdot \omega - \sqrt{1+|x|^2}} x\in \mathbb{R}^3 \omega \in \mathbb{S}^2 = \{y\in\mathbb{R}^3 \ | \ |y| = 1\} ||f(x,\cdot)||_{L^\infty(\mathbb{S^2)}} := \sup_{\omega\in\mathbb{S}^2}|f(x,\omega)| |x| \begin{align}
|f(x,\omega)| &\leq \bigg|\frac{\frac{x}{\sqrt{1+|x|^2}} - \omega}{\frac{x\cdot\omega}{\sqrt{1+|x|^2}} - 1}\bigg| \leq \frac{2}{1 - \frac{|x|}{\sqrt{1+|x|^2}}}
\\
&= \frac{2\sqrt{1+|x|^2}}{\sqrt{1+|x|^2} - |x|} = \mathcal{O}(|x|^2)
\end{align} |x|\rightarrow \infty \mathcal{O}(|x|) \mathcal{O}(1) g(|x|) \begin{align}
\sup_{(\hat{x},\omega)\in\mathbb{S}^2\times\mathbb{S}^2}|f(|x|\hat{x},\omega)| \leq C g(|x|)
\end{align} |x| g(|x|) = a|x| + b","['multivariable-calculus', 'asymptotics', 'estimation', 'parameter-estimation']"
75,Conditional convergence for improper Riemann double integrals,Conditional convergence for improper Riemann double integrals,,"I'm reading Buck's advanced calculus. It says for improper integral of higher dimensions, conditional convergence is impossible, i.e., $\int\int_D f$ cannot exist without $\int\int_D|f|$ existing too. Then book only gives a sketch of proof as follow. Let $f_1=(|f|+f)/2$ and $f_2=(|f|-f)/2$ . We may assume that the integrals $\int\int_Df_i$ are each divergent. Since $f_1f_2=0$ , so that the sets where $f_1$ and $f_2$ are positive are disjoint. It is then possible to choose an expanding sequence of closed rectangles $\{D_n\}$ which favour $f_1$ over $f_2$ , so that $\int\int_{D_n} f_1$ diverges faster than $\int\int_{D_n} f_2$ , with the result that $\int\int_{D_n} f$ , which is their different, also diverge. But it feels like a almost exactly same proof can be used to show that single improper integral can not be conditional convergent too, but single integral can be convergent without being absolute convergent. For example, $\int^\infty_1 x^{-1}\sin x$ is conditional convergent but not absolutely convergent. So what is the essential difference between single integral and double integral which makes the conditional convergence for double integral impossible? Thanks.","I'm reading Buck's advanced calculus. It says for improper integral of higher dimensions, conditional convergence is impossible, i.e., cannot exist without existing too. Then book only gives a sketch of proof as follow. Let and . We may assume that the integrals are each divergent. Since , so that the sets where and are positive are disjoint. It is then possible to choose an expanding sequence of closed rectangles which favour over , so that diverges faster than , with the result that , which is their different, also diverge. But it feels like a almost exactly same proof can be used to show that single improper integral can not be conditional convergent too, but single integral can be convergent without being absolute convergent. For example, is conditional convergent but not absolutely convergent. So what is the essential difference between single integral and double integral which makes the conditional convergence for double integral impossible? Thanks.",\int\int_D f \int\int_D|f| f_1=(|f|+f)/2 f_2=(|f|-f)/2 \int\int_Df_i f_1f_2=0 f_1 f_2 \{D_n\} f_1 f_2 \int\int_{D_n} f_1 \int\int_{D_n} f_2 \int\int_{D_n} f \int^\infty_1 x^{-1}\sin x,"['integration', 'multivariable-calculus']"
76,Analysis on Manifolds problem 9.4 (Implicit function theorem),Analysis on Manifolds problem 9.4 (Implicit function theorem),,"From Analysis on Manifolds by  Munkres. Let $F:\mathbb{R^2} \to \mathbb{R}$ be of class $C^2$ , with $F(0,0)=0$ and $DF(0,0)=\begin{bmatrix} 2 & 3\end{bmatrix}$ . Let $G:\mathbb{R^3} \to \mathbb{R}$ be defined by the equation $$G(x,y,z)=F(x+2y+3z-1, x^3+y^2-z^2).$$ a) Note that $G(-2,3,-1)=F(0,0)=0$ . Show that one can solve the equation $G(x,y,z)=0$ for $z$ , say $z=g(x,y)$ , for $(x,y)$ in a neighborhood $B$ of $(-2,3)$ , such that $g(-2,3)=-1$ . b) Find $Dg(-2,3)$ . c) If $D_1D_1F=3$ and $D_1D_2F=-1$ and $D_2D_2F=5$ at $(0,0)$ , find $D_2D_1g(-2,3)$ .","From Analysis on Manifolds by  Munkres. Let be of class , with and . Let be defined by the equation a) Note that . Show that one can solve the equation for , say , for in a neighborhood of , such that . b) Find . c) If and and at , find .","F:\mathbb{R^2} \to \mathbb{R} C^2 F(0,0)=0 DF(0,0)=\begin{bmatrix}
2 & 3\end{bmatrix} G:\mathbb{R^3} \to \mathbb{R} G(x,y,z)=F(x+2y+3z-1, x^3+y^2-z^2). G(-2,3,-1)=F(0,0)=0 G(x,y,z)=0 z z=g(x,y) (x,y) B (-2,3) g(-2,3)=-1 Dg(-2,3) D_1D_1F=3 D_1D_2F=-1 D_2D_2F=5 (0,0) D_2D_1g(-2,3)","['multivariable-calculus', 'implicit-function-theorem']"
77,Are there other ways to find local extrema for multivariable functions without the second derivative test?,Are there other ways to find local extrema for multivariable functions without the second derivative test?,,"I've got a problem where I'm being asked to find the extrema for the equation $f(x,y) = \cos (y)e^x$ . Assuming I haven't missed anything, the first derivative with respect to x is identical to the given equation, which equals zero at $y = -\pi/2$ and $y = 3\pi/2$ . When I plug those points into the first derivative with respect to $y$ : $-\sin (y)e^x$ , there is seemingly no way to find a critical point given that neither $-\sin (y)$ nor $e^x$ can be zero. Is it still possible then, to find any local extrema or is the problem dead?","I've got a problem where I'm being asked to find the extrema for the equation . Assuming I haven't missed anything, the first derivative with respect to x is identical to the given equation, which equals zero at and . When I plug those points into the first derivative with respect to : , there is seemingly no way to find a critical point given that neither nor can be zero. Is it still possible then, to find any local extrema or is the problem dead?","f(x,y) = \cos (y)e^x y = -\pi/2 y = 3\pi/2 y -\sin (y)e^x -\sin (y) e^x","['calculus', 'multivariable-calculus', 'derivatives', 'trigonometry']"
78,Evaluating triple integral in a different sphere,Evaluating triple integral in a different sphere,,"I have been stuck with this problem for a while: Evaluate $$ \iiint_D\frac{1}{x^2 + y^2 + (z-1)^2}\,dx\,dy\,dz \,,$$ where $D$ is a sphere centered at $(0,0,0)$ with a radius of $1/2$ . I couldn't make any progress without the use of spherical coordinates. Furthermore substituting $D(x, y, z)$ with spherical coordinates centered at $(0,0,0)$ leads to a very complex integral. It is obvious that we need to substitute $x, y, z$ to spherical coordinates centered at $(0, 0, 1)$ so that our denominator simplifies to $r^2$ . The problem arises when you try to represent D(the $(0,0,0)$ radius $= 1/2$ sphere) with spherical coordinates, because they need to be centered at $(0,0,1)$ . Any help?","I have been stuck with this problem for a while: Evaluate where is a sphere centered at with a radius of . I couldn't make any progress without the use of spherical coordinates. Furthermore substituting with spherical coordinates centered at leads to a very complex integral. It is obvious that we need to substitute to spherical coordinates centered at so that our denominator simplifies to . The problem arises when you try to represent D(the radius sphere) with spherical coordinates, because they need to be centered at . Any help?"," \iiint_D\frac{1}{x^2 + y^2 + (z-1)^2}\,dx\,dy\,dz \,, D (0,0,0) 1/2 D(x, y, z) (0,0,0) x, y, z (0, 0, 1) r^2 (0,0,0) = 1/2 (0,0,1)","['integration', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
79,"Difference between ""work"" and ""flux/flow"" in multivariable calculus?","Difference between ""work"" and ""flux/flow"" in multivariable calculus?",,"I am in the multivariable calculus class now and trying to understand things. What is the difference between ""work"" and ""flux/flow"" in multivariable calculus? Exam questions usually ask: ""find work done..."" or ""find flux..."", so: Is work done, work actually done along a line, so use line integral (single integral) over vector field? Is flux/flow a flow through a surface so use one of: surface, green, stokes, divergence theorems (double/tripple integrals)? Can work done be calculaed using green, stokes or divergence theorems? Is work done actually the same thing as flux/flow, just another name?","I am in the multivariable calculus class now and trying to understand things. What is the difference between ""work"" and ""flux/flow"" in multivariable calculus? Exam questions usually ask: ""find work done..."" or ""find flux..."", so: Is work done, work actually done along a line, so use line integral (single integral) over vector field? Is flux/flow a flow through a surface so use one of: surface, green, stokes, divergence theorems (double/tripple integrals)? Can work done be calculaed using green, stokes or divergence theorems? Is work done actually the same thing as flux/flow, just another name?",,['multivariable-calculus']
80,"Computing $\int_{\textbf{R}^n} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2}$ for some $a\in \textbf{R}^n$.",Computing  for some .,"\int_{\textbf{R}^n} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2} a\in \textbf{R}^n","I'd like to compute the following (Riemann) integral: $$\int_{\textbf{R}^n} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2}$$ for some fixed $a\in \textbf{R}^n$ . To get started I computed the case in $\textbf{R}^2$ , letting $a=(p,q)$ and writing $x=(x,y)$ (pardon the abuse of notation.) We can write $\|(x,y)\|^2=x^2+y^2.$ So that \begin{align*} \int_{\textbf{R}^2} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2}&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(p^2x^2+2pqxy+q^2y^2)e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy\\ &=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}p^2x^2e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy + \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}2pqxy e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy\\&+ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}q^2y^2e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy. \end{align*} Applying Fubini's theorem and using the well known Gaussian integral, the first integral summand is $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}p^2x^2e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy=p^2\int_{\infty}^{\infty}x^2e^{-\frac{1}{2}x^2}\,dx \int_{\infty}^{\infty}e^{-\frac{1}{2}y^2}\,dy=2\pi p^2.$$ By symmetry, $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}q^2y^2e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy=2\pi q^2$$ If we factor out $2pq$ from the middle summand integral, and apply Fubini's theorem, we get $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}2pqxy e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy=2pq\int_{-\infty}^{\infty}xe^{-\frac{1}{2}x^2}\,dx\int_{-\infty}^{\infty}ye^{-\frac{1}{2}y^2}\,dy.$$ Both integrals are of odd functions taken over a domain symmetric about 0, so they both vanish. Then we get $$\int_{\textbf{R}^2} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2}=2\pi(p^2+q^2).$$ This leads me to the following guess in the $\textbf{R}^n$ case: if $a=(a_1,a_2,\ldots,a_n)$ , then $$\int_{\textbf{R}^n} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2}=(2\pi)^{\frac{n}{2}}(a_1^2+\ldots+a_n^2).$$ The justification for this guess would similar to the result I got from the above computation: if we write $x=(x_1,\ldots,x_n),$ then we can split the integral by linearity according to the multinomial expansion of $\langle x,a \rangle ^2.$ In this expansion, all integrals of functions containing products of mixed $x_i$ 's will vanish, since the maximal power of any term of mixed $x_i$ 's will be 1 by the definition of squares of multinomials. This will leave only integrals of squares of one such $x_i$ . Splitting each such integral by Fubini's theorem and evaluating will result in an $n$ -fold product of $\sqrt{2\pi},$ multiplied by the corresponding square $a_i^2.$ Is this guess justified correctly? If not, any hints in the right direction would be appreciated. I have a suspicion this is not the slickest way to justify this.","I'd like to compute the following (Riemann) integral: for some fixed . To get started I computed the case in , letting and writing (pardon the abuse of notation.) We can write So that Applying Fubini's theorem and using the well known Gaussian integral, the first integral summand is By symmetry, If we factor out from the middle summand integral, and apply Fubini's theorem, we get Both integrals are of odd functions taken over a domain symmetric about 0, so they both vanish. Then we get This leads me to the following guess in the case: if , then The justification for this guess would similar to the result I got from the above computation: if we write then we can split the integral by linearity according to the multinomial expansion of In this expansion, all integrals of functions containing products of mixed 's will vanish, since the maximal power of any term of mixed 's will be 1 by the definition of squares of multinomials. This will leave only integrals of squares of one such . Splitting each such integral by Fubini's theorem and evaluating will result in an -fold product of multiplied by the corresponding square Is this guess justified correctly? If not, any hints in the right direction would be appreciated. I have a suspicion this is not the slickest way to justify this.","\int_{\textbf{R}^n} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2} a\in \textbf{R}^n \textbf{R}^2 a=(p,q) x=(x,y) \|(x,y)\|^2=x^2+y^2. \begin{align*}
\int_{\textbf{R}^2} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2}&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(p^2x^2+2pqxy+q^2y^2)e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy\\
&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}p^2x^2e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy + \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}2pqxy e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy\\&+ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}q^2y^2e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy.
\end{align*} \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}p^2x^2e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy=p^2\int_{\infty}^{\infty}x^2e^{-\frac{1}{2}x^2}\,dx \int_{\infty}^{\infty}e^{-\frac{1}{2}y^2}\,dy=2\pi p^2. \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}q^2y^2e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy=2\pi q^2 2pq \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}2pqxy e^{-\frac{1}{2}x^2}e^{-\frac{1}{2}y^2}\,dxdy=2pq\int_{-\infty}^{\infty}xe^{-\frac{1}{2}x^2}\,dx\int_{-\infty}^{\infty}ye^{-\frac{1}{2}y^2}\,dy. \int_{\textbf{R}^2} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2}=2\pi(p^2+q^2). \textbf{R}^n a=(a_1,a_2,\ldots,a_n) \int_{\textbf{R}^n} \langle x,a \rangle ^2 e^{-\frac{1}{2} \|x\| ^2}=(2\pi)^{\frac{n}{2}}(a_1^2+\ldots+a_n^2). x=(x_1,\ldots,x_n), \langle x,a \rangle ^2. x_i x_i x_i n \sqrt{2\pi}, a_i^2.","['multivariable-calculus', 'definite-integrals', 'improper-integrals']"
81,Evaluation of an integral of the form $C({\bf r})=\int\frac{{\rm d}^dk}{(2\pi)^d}\frac{{\rm e}^{{\rm i}{\bf k}\cdot{\bf r}}}{|{\bf k}|^2+m^2}$,Evaluation of an integral of the form,C({\bf r})=\int\frac{{\rm d}^dk}{(2\pi)^d}\frac{{\rm e}^{{\rm i}{\bf k}\cdot{\bf r}}}{|{\bf k}|^2+m^2},"Consider a momentum-integral of the form in $d$ spatial dimensions: $$C({\bf r})=\int\frac{{\rm d}^dk}{(2\pi)^d}\frac{{\rm e}^{{\rm i}{\bf k}\cdot{\bf r}}}{|{\bf k}|^2+m^2}$$ where ${\bf r}=(x_1,x_2,\cdots,x_d)$ , ${\bf k}=(k_1,k_2,\cdots,k_d)$ and $m^2$ is a real number, could be positive, negative or zero. (Despite my notation, $m^2$ is any constant and has nothing to do with mass) See Eq. $(4.9)$ of this lecture note . Question Is this integral doable? I am interested in an exact analytical formula for extracting the results for different spatial dimensions $d$ (namely, $1,2$ and $3$ ). Update The existing answer by @DinosaurEgg, tells how to compute $C(\bf r)$ for $m^2>0$ . I would appreciate if someone can ameliorate expand on his answer by addressing how to handle the case $m^2<0$ .","Consider a momentum-integral of the form in spatial dimensions: where , and is a real number, could be positive, negative or zero. (Despite my notation, is any constant and has nothing to do with mass) See Eq. of this lecture note . Question Is this integral doable? I am interested in an exact analytical formula for extracting the results for different spatial dimensions (namely, and ). Update The existing answer by @DinosaurEgg, tells how to compute for . I would appreciate if someone can ameliorate expand on his answer by addressing how to handle the case .","d C({\bf r})=\int\frac{{\rm d}^dk}{(2\pi)^d}\frac{{\rm e}^{{\rm i}{\bf k}\cdot{\bf r}}}{|{\bf k}|^2+m^2} {\bf r}=(x_1,x_2,\cdots,x_d) {\bf k}=(k_1,k_2,\cdots,k_d) m^2 m^2 (4.9) d 1,2 3 C(\bf r) m^2>0 m^2<0","['multivariable-calculus', 'definite-integrals', 'mathematical-physics']"
82,"Evaluating $\iint_{[0,1]^2} \frac{2-4xy}{(9-xy)(8+xy)}dxdy$",Evaluating,"\iint_{[0,1]^2} \frac{2-4xy}{(9-xy)(8+xy)}dxdy","I am trying to compute the following double integral: $$I=\iint_S \frac{2-4xy}{(9-xy)(8+xy)}dxdy$$ with $S=[0,1]\times[0,1].$ What I have tried: I have written the integral as follows: $$I=I_1+I_2=-2\iint_S \frac{1}{(9-xy)}dxdy+2\iint_S \frac{1}{(8+xy)}dxdy$$ When trying to compute $I_1$ , I encountered this integral $\int_0^1-\frac{1}{x}\ln(1-\frac{x}{9})dx$ . I used an online calculator to solve it and the result involves the function $\operatorname{Li}(z)$ , which I am not familiar with. Another thing I have tried is a change of variables: $$u=9-xy$$ $$v=8+xy$$ The problem is that the Jacobian associated to this change of variables is null, so I cannot use it. My question: Could someone show me a way of computing this integral without having to use $\operatorname{Li}_2(z)$ ?","I am trying to compute the following double integral: with What I have tried: I have written the integral as follows: When trying to compute , I encountered this integral . I used an online calculator to solve it and the result involves the function , which I am not familiar with. Another thing I have tried is a change of variables: The problem is that the Jacobian associated to this change of variables is null, so I cannot use it. My question: Could someone show me a way of computing this integral without having to use ?","I=\iint_S \frac{2-4xy}{(9-xy)(8+xy)}dxdy S=[0,1]\times[0,1]. I=I_1+I_2=-2\iint_S \frac{1}{(9-xy)}dxdy+2\iint_S \frac{1}{(8+xy)}dxdy I_1 \int_0^1-\frac{1}{x}\ln(1-\frac{x}{9})dx \operatorname{Li}(z) u=9-xy v=8+xy \operatorname{Li}_2(z)","['integration', 'multivariable-calculus', 'change-of-variable']"
83,How is a multivariable function integrated,How is a multivariable function integrated,,"I am unsure about how integration is handled in the context of area integrals. For example, $$ \int_0^1 \!\int_0^{y=x-1} \! (x-y) dydx $$ Does the inner integral evaluate to $ xy - \frac{1}{2}y^2$ or to $-\frac12(x-y)^2$ - or are they the same thing? Indeed expanding out $\frac12(x-y)^2$ gives $xy -\frac12 y^2 - \frac12 x^2$ with the additional $\frac12 x^2$ , which makes sense as $x$ is held as a constant in the inner integral. This example is from a book wherein $\!\int_0^{y=x-1} \! (x-y)dy$ is evaluated to $[\frac12 (x - y)^2]$ . However, in a later example $\!\int_0^{y=x-1} \! (x^2-y^2)dy$ is evaluated to $[x^2y - \frac13y^3]$ . Why are they treated in different ways?","I am unsure about how integration is handled in the context of area integrals. For example, Does the inner integral evaluate to or to - or are they the same thing? Indeed expanding out gives with the additional , which makes sense as is held as a constant in the inner integral. This example is from a book wherein is evaluated to . However, in a later example is evaluated to . Why are they treated in different ways?", \int_0^1 \!\int_0^{y=x-1} \! (x-y) dydx   xy - \frac{1}{2}y^2 -\frac12(x-y)^2 \frac12(x-y)^2 xy -\frac12 y^2 - \frac12 x^2 \frac12 x^2 x \!\int_0^{y=x-1} \! (x-y)dy [\frac12 (x - y)^2] \!\int_0^{y=x-1} \! (x^2-y^2)dy [x^2y - \frac13y^3],"['integration', 'multivariable-calculus', 'area']"
84,Average angle between two randomly chosen vectors in a unit square,Average angle between two randomly chosen vectors in a unit square,,"Consider two randomly chosen vectors $(a,b)$ and $(c,d)$ within the unit square, where $a, b, c,$ and $d$ are chosen uniformly from $[0,1]$ . What is the expected angle between the vectors? Here's what I have so far. The angle between any two positive vectors $<a,b>$ and $<c, d>$ is $\arccos{\frac{ac + bd}{\sqrt{(a^2 + b^2)(c^2 + d^2)}}}$ . We just need to find the average value of this function over $a, b, c,d$ in range $[0,1]$ . This is equivalent to the quadrupal integral $$\iiiint_V \arccos{\frac{ac + bd}{\sqrt{(a^2 + b^2)(c^2 + d^2)}}} \,da\,db\,dc\,dd$$ . I can't find any way to compute this. I entered this into Mathematica and it wasn't able to output even a decimal approximation. I tried all the usual substitions to reduce this into a double integral but none of them worked. Converting to polar coordinates didn't work out nicely either. For one, the bounds become harder to work with and the jacobian is pretty nasty, so converting doesn't seem like it would help me. Probably the difficult part is working with the $\arccos$ function. I have no idea how to deal with it. Is it possible that there is an analytic solution to this integral? Even if someone found a numerical approximation, it would help me out.","Consider two randomly chosen vectors and within the unit square, where and are chosen uniformly from . What is the expected angle between the vectors? Here's what I have so far. The angle between any two positive vectors and is . We just need to find the average value of this function over in range . This is equivalent to the quadrupal integral . I can't find any way to compute this. I entered this into Mathematica and it wasn't able to output even a decimal approximation. I tried all the usual substitions to reduce this into a double integral but none of them worked. Converting to polar coordinates didn't work out nicely either. For one, the bounds become harder to work with and the jacobian is pretty nasty, so converting doesn't seem like it would help me. Probably the difficult part is working with the function. I have no idea how to deal with it. Is it possible that there is an analytic solution to this integral? Even if someone found a numerical approximation, it would help me out.","(a,b) (c,d) a, b, c, d [0,1] <a,b> <c, d> \arccos{\frac{ac + bd}{\sqrt{(a^2 + b^2)(c^2 + d^2)}}} a, b, c,d [0,1] \iiiint_V \arccos{\frac{ac + bd}{\sqrt{(a^2 + b^2)(c^2 + d^2)}}} \,da\,db\,dc\,dd \arccos","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral', 'geometric-probability']"
85,If the value of integral in the image below is π then what is the value of y?,If the value of integral in the image below is π then what is the value of y?,,"I could not simplify $$ \int_0^1 \sqrt{-1 + \sqrt{\frac{1+y}{x} - y}}\ dx $$ I tried integration it in an online integrator but trust me the result is seriously daunting to be back traced to $\pi$ as a value, thereby determining $y$ .  So I am looking for a rather clever trick to get through this one.","I could not simplify I tried integration it in an online integrator but trust me the result is seriously daunting to be back traced to as a value, thereby determining .  So I am looking for a rather clever trick to get through this one.","
\int_0^1 \sqrt{-1 + \sqrt{\frac{1+y}{x} - y}}\ dx
 \pi y","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'partial-fractions']"
86,Why is the Fisher information matrix both an expected outer product and a Hessian?,Why is the Fisher information matrix both an expected outer product and a Hessian?,,"If $X$ is a random variable distributed as $X \sim p(x ; {\theta^*})$ , the Fisher information matrix is defined as the expected outer product matrix: $$ I(\theta) = E_{X \sim p(x ; {\theta^*})} \left[ \,\left(\nabla_{{\theta}} \log p(X\,; {\theta}) \right)  \left(\nabla_{{\theta}} \log p(X\,; {\theta}) \right)^\top \,\right]. $$ However, it is also defined as the expected Hessian matrix of the negative log-likelihood: $$ I(\theta) = E_{X \sim p(x ; {\theta^*})} \left[ \frac{\partial^2}{(\partial\theta)(\partial\theta^\top)} \bigl(-\log p(X;\theta)\bigr) \right]. $$ I am puzzled by why these two definitions are equivalent. Specifically, I'm not sure why and (expected) outer product of first partial derivatives should be equal to the matrix of 2nd derivatives. Intuitions or derivations are appreciated!","If is a random variable distributed as , the Fisher information matrix is defined as the expected outer product matrix: However, it is also defined as the expected Hessian matrix of the negative log-likelihood: I am puzzled by why these two definitions are equivalent. Specifically, I'm not sure why and (expected) outer product of first partial derivatives should be equal to the matrix of 2nd derivatives. Intuitions or derivations are appreciated!","X X \sim p(x ; {\theta^*}) 
I(\theta) = E_{X \sim p(x ; {\theta^*})} \left[
\,\left(\nabla_{{\theta}} \log p(X\,; {\theta}) \right) 
\left(\nabla_{{\theta}} \log p(X\,; {\theta}) \right)^\top \,\right].
 
I(\theta) = E_{X \sim p(x ; {\theta^*})} \left[
\frac{\partial^2}{(\partial\theta)(\partial\theta^\top)} \bigl(-\log p(X;\theta)\bigr)
\right].
","['multivariable-calculus', 'statistical-inference', 'fisher-information']"
87,Show that $\sqrt{x^2+y^2}$ is not differentiable at the origin.,Show that  is not differentiable at the origin.,\sqrt{x^2+y^2},"This is very simple but I am having some trouble. Please indicate whether my proof is correct. We want to show that $f(x, y) = \sqrt{x^2 + y^2}$ . Is not differentiable at the origin. If it was differentiable, the partial derivatives at the origin would exist. However, for example, $$ \lim_{h \to 0} \frac{f(h,0) - f(0, 0)}{h} = \frac{|h|}{h} =  \begin{cases} 1, \quad \text{ if } h \to 0^+ \\ -1, \quad \text{ if } h \to 0^- \end{cases} $$ and hence the partial derivatives fail to exist. Thanks in advance.","This is very simple but I am having some trouble. Please indicate whether my proof is correct. We want to show that . Is not differentiable at the origin. If it was differentiable, the partial derivatives at the origin would exist. However, for example, and hence the partial derivatives fail to exist. Thanks in advance.","f(x, y) = \sqrt{x^2 + y^2} 
\lim_{h \to 0} \frac{f(h,0) - f(0, 0)}{h} = \frac{|h|}{h} = 
\begin{cases}
1, \quad \text{ if } h \to 0^+ \\
-1, \quad \text{ if } h \to 0^-
\end{cases}
","['real-analysis', 'multivariable-calculus', 'solution-verification']"
88,Line integral of vector field when Stokes' theorem cannot be applied directly,Line integral of vector field when Stokes' theorem cannot be applied directly,,"Define, on $\mathbb{R}^3 \setminus \{(0,0,z) \in \mathbb{R}^3 \ \mid \ z \in \mathbb{R} \}$ , the vector fields $$G(x,y,z) = \left(\frac{x}{x^2+y^2}, \frac{2y}{x^2+y^2}, 2 \right) $$ and $$H(x,y,z) = \left(\frac{-y}{x^2+2y^2}, \frac{x}{x^2+2y^2},3 \right). $$ How do we compute the line integrals of $G$ and $H$ over the closed curve at the intersection of the following surfaces: $$x+y+z = 2 \text{ and } z = x^2+y^2. $$ What is an easy way to solve these types of questions? We can see that $\text{curl}(G) = \text{curl}(H) = 0$ , so an idea could be to use the theorem of Stokes (although, I don't think that $H$ is conservative). However, what would be a good surface to choose? The vector fields are not defined on the entire of $\mathbb{R}^3$ , so the circle that is enclosed by the curve is not a good choice. Also, a parametrization of the curve would lead to integrals that are hard to solve.","Define, on , the vector fields and How do we compute the line integrals of and over the closed curve at the intersection of the following surfaces: What is an easy way to solve these types of questions? We can see that , so an idea could be to use the theorem of Stokes (although, I don't think that is conservative). However, what would be a good surface to choose? The vector fields are not defined on the entire of , so the circle that is enclosed by the curve is not a good choice. Also, a parametrization of the curve would lead to integrals that are hard to solve.","\mathbb{R}^3 \setminus \{(0,0,z) \in \mathbb{R}^3 \ \mid \ z \in \mathbb{R} \} G(x,y,z) = \left(\frac{x}{x^2+y^2}, \frac{2y}{x^2+y^2}, 2 \right)  H(x,y,z) = \left(\frac{-y}{x^2+2y^2}, \frac{x}{x^2+2y^2},3 \right).  G H x+y+z = 2 \text{ and } z = x^2+y^2.  \text{curl}(G) = \text{curl}(H) = 0 H \mathbb{R}^3","['multivariable-calculus', 'vector-fields']"
89,Showing expression of $dA$ in cartesian coordinates = $dA$ in cylindrical polar [duplicate],Showing expression of  in cartesian coordinates =  in cylindrical polar [duplicate],dA dA,"This question already has answers here : Explain $\iint \mathrm dx\,\mathrm dy = \iint r \,\mathrm \,d\alpha\,\mathrm dr$ (4 answers) Closed 4 years ago . In cartesian coordinates, $dA = dx\,dy$ . As $x=\rho \cos\phi\ $ and $y=\rho \sin \phi \ $ , then: $dx=  \cos\phi\, d\rho - \rho \sin\phi\, d\phi$ and $dy = \sin\phi\, d\rho + \rho \cos\phi \, d \phi  $ . So $dx\, dy = \rho\, d\rho\, d\phi(\cos^2\phi-\sin^2\phi) $ , (ignoring second order terms in $d\rho$ and $d\phi$ ) Why is this not equal to the correct result of $dA=\rho\, d\rho\, d\phi$ ?","This question already has answers here : Explain $\iint \mathrm dx\,\mathrm dy = \iint r \,\mathrm \,d\alpha\,\mathrm dr$ (4 answers) Closed 4 years ago . In cartesian coordinates, . As and , then: and . So , (ignoring second order terms in and ) Why is this not equal to the correct result of ?","dA = dx\,dy x=\rho \cos\phi\  y=\rho \sin \phi \  dx=  \cos\phi\, d\rho - \rho \sin\phi\, d\phi dy = \sin\phi\, d\rho + \rho \cos\phi \, d \phi   dx\, dy = \rho\, d\rho\, d\phi(\cos^2\phi-\sin^2\phi)  d\rho d\phi dA=\rho\, d\rho\, d\phi","['multivariable-calculus', 'cylindrical-coordinates', 'curvilinear-coordinates']"
90,Distance formula for generalized knight movement on infinite chessboard from a corner,Distance formula for generalized knight movement on infinite chessboard from a corner,,"Consider a chessboard infinite in positive x and y directions, all square has non-negative integer coordinates, and the only corner is at $(0,0)$ . A $(p,q)$ -knight is a piece that can move so that after each move one of the coordinate change by $p$ and the other change by $q$ (we will just call it a knight from now on). Set a knight at the corner $(0,0)$ , and assume that $(p,q)$ is such that every position on the board can be reached by the knight. For a position $(m,n)$ on the board, let $d(m,n)$ be the minimum number of moves needed for a knight from the corner to reach $(m,n)$ . Now the following claims are true: $\gcd(p,q)=1$ and $p,q$ are not both odd. This is necessary and sufficient conditions for every square to be reachable. Necessary is easily seen, for sufficient a sketch of the solution is in this question Can an $(a,b)$-knight reach every point on a chessboard? For every square on the board, every ways to reach it require the number to moves to have the same parity as $m+n$ , this is from black-white coloring. So $d(m,n)$ has the same parity as $m+n$ $d(m,n)\max(p,q)>=\max(m,n)$ , obviously. $d(m,n)(p+q)>=m+n$ So let's $B(m,n)$ be the smallest integer that satisfy all the constraints: $B(m,n)\max(p,q)>=\max(m,n)$ and $B(m,n)(p+q)>=m+n$ and $B(m,n)$ has the same parity as $m+n$ . Then we know that $d(m,n)>=B(m,n)$ for all $(m,n)$ . We make $B(m,n)$ the predicted value of $d(m,n)$ . DEFINITION: An ""awkward spot"" on the board is a position $(m,n)$ in which $d(m,n)$ is not equal to $B(m,n)$ . QUESTION: is it true that for all valid values of $(p,q)$ then the number of awkward spots are finite? Example: for the normal chess knight $(p,q)=(1,2)$ then you can check against this answer chess board knight distance (but need some small modification since we start from a corner) to see that the awkward spots are $(0,1),(1,0),(1,1),(2,2)$ so there are only a finite number of them. (I have heard suggestions to use Fourier transform but I have no clues what to do with it)","Consider a chessboard infinite in positive x and y directions, all square has non-negative integer coordinates, and the only corner is at . A -knight is a piece that can move so that after each move one of the coordinate change by and the other change by (we will just call it a knight from now on). Set a knight at the corner , and assume that is such that every position on the board can be reached by the knight. For a position on the board, let be the minimum number of moves needed for a knight from the corner to reach . Now the following claims are true: and are not both odd. This is necessary and sufficient conditions for every square to be reachable. Necessary is easily seen, for sufficient a sketch of the solution is in this question Can an $(a,b)$-knight reach every point on a chessboard? For every square on the board, every ways to reach it require the number to moves to have the same parity as , this is from black-white coloring. So has the same parity as , obviously. So let's be the smallest integer that satisfy all the constraints: and and has the same parity as . Then we know that for all . We make the predicted value of . DEFINITION: An ""awkward spot"" on the board is a position in which is not equal to . QUESTION: is it true that for all valid values of then the number of awkward spots are finite? Example: for the normal chess knight then you can check against this answer chess board knight distance (but need some small modification since we start from a corner) to see that the awkward spots are so there are only a finite number of them. (I have heard suggestions to use Fourier transform but I have no clues what to do with it)","(0,0) (p,q) p q (0,0) (p,q) (m,n) d(m,n) (m,n) \gcd(p,q)=1 p,q m+n d(m,n) m+n d(m,n)\max(p,q)>=\max(m,n) d(m,n)(p+q)>=m+n B(m,n) B(m,n)\max(p,q)>=\max(m,n) B(m,n)(p+q)>=m+n B(m,n) m+n d(m,n)>=B(m,n) (m,n) B(m,n) d(m,n) (m,n) d(m,n) B(m,n) (p,q) (p,q)=(1,2) (0,1),(1,0),(1,1),(2,2)","['combinatorics', 'elementary-number-theory', 'multivariable-calculus', 'recreational-mathematics', 'chessboard']"
91,"Solve $\operatorname{diag}(x) \nabla f(x)= A x, f(0)=0$",Solve,"\operatorname{diag}(x) \nabla f(x)= A x, f(0)=0","How to solve the following equation: $$ \operatorname{diag}(x)  \nabla f(x)= A x $$ where $f: \mathbb{R}^n \to \mathbb{R}$ and $A \in \mathbb{R}^{n \times n}$ with condition $f(0)=0$ .  Assume column vectors are used.  Here $\operatorname{diag}(x)$ for a vector $x$ is a squared matrix where $x$ forms a main diagonal. $\nabla f(x)$ is a gradient of $f$ . Here is my approach. $$ \nabla f(x) = \operatorname{diag}^{-1}(x)  A x. $$ Now using FTC and choosing $r(t)=(1-t)0+t*x$ \begin{align} f(x)=f(r(1))&= \int_0^1  \nabla f(r(t)) \cdot r'(t) \,\mathrm dt\\ &= \int_0^1  x^T \operatorname{diag}^{-1}(tx)  A tx \,\mathrm dt\\ &= x^T \operatorname{diag}^{-1}(x)  A x\\ &= \mathbb{1}^TA x \end{align} where $\mathbb{1}$ is a vector of of all ones. However, upon checking we have that $$ \nabla f(x)= \nabla  \mathbb{1}^TA x= A^T  \mathbb{1} $$ and now checkign the differential equation $$ \operatorname{diag}(x)  A^T  \mathbb{1}= A x . $$ However, I don't think the above equality is true.   I am not sure where I am making a mistake. Edit 2: From one of the answers, it appears that the solution exists only if, $A$ is a diagonal matrix.","How to solve the following equation: where and with condition .  Assume column vectors are used.  Here for a vector is a squared matrix where forms a main diagonal. is a gradient of . Here is my approach. Now using FTC and choosing where is a vector of of all ones. However, upon checking we have that and now checkign the differential equation However, I don't think the above equality is true.   I am not sure where I am making a mistake. Edit 2: From one of the answers, it appears that the solution exists only if, is a diagonal matrix.","
\operatorname{diag}(x)  \nabla f(x)= A x
 f: \mathbb{R}^n \to \mathbb{R} A \in \mathbb{R}^{n \times n} f(0)=0 \operatorname{diag}(x) x x \nabla f(x) f 
\nabla f(x) = \operatorname{diag}^{-1}(x)  A x.
 r(t)=(1-t)0+t*x \begin{align}
f(x)=f(r(1))&= \int_0^1  \nabla f(r(t)) \cdot r'(t) \,\mathrm dt\\
&= \int_0^1  x^T \operatorname{diag}^{-1}(tx)  A tx \,\mathrm dt\\
&= x^T \operatorname{diag}^{-1}(x)  A x\\
&= \mathbb{1}^TA x
\end{align} \mathbb{1} 
\nabla f(x)= \nabla  \mathbb{1}^TA x= A^T  \mathbb{1}
 
\operatorname{diag}(x)  A^T  \mathbb{1}= A x .
 A","['linear-algebra', 'multivariable-calculus', 'partial-differential-equations']"
92,Why is shear missing in Helmholtz theorem?,Why is shear missing in Helmholtz theorem?,,"Let $F_i$ be a well behaved vector field in $\mathbb{R}^3$ which rapidly vanishes at infinity. (I am using here the index notation.) By the Helmholtz theorem , knowledge of divergence $\partial_i F_i$ , and the curl $\partial_iF_j - \partial_j F_i$ of $F_i$ provides enough information to reconstruct the original vector field $F_i$ . I am wondering how is it that one can reconstruct $F_i$ only from divergence and curl, while the complete Jacobian $J_{ij} = \partial_j F_i$ corresponds additional information. Divergence is the trace of $J_{ij}$ , curl is skew-symmetric part, while the symmetric part $\partial_j F_i + \partial_i F_j$ , sometimes known as shear, does not participate in the Helmholtz theorem. I would like to intuitively understand this behavior. In the case of a scalar function $f$ , one needs to know all of its derivatives $\partial_i f$ in order to reconstruct $f$ via line integral. However, in the case of Helmholtz theorem, we are reconstructing $F_i$ as a volume integral. Does this make any difference?","Let be a well behaved vector field in which rapidly vanishes at infinity. (I am using here the index notation.) By the Helmholtz theorem , knowledge of divergence , and the curl of provides enough information to reconstruct the original vector field . I am wondering how is it that one can reconstruct only from divergence and curl, while the complete Jacobian corresponds additional information. Divergence is the trace of , curl is skew-symmetric part, while the symmetric part , sometimes known as shear, does not participate in the Helmholtz theorem. I would like to intuitively understand this behavior. In the case of a scalar function , one needs to know all of its derivatives in order to reconstruct via line integral. However, in the case of Helmholtz theorem, we are reconstructing as a volume integral. Does this make any difference?",F_i \mathbb{R}^3 \partial_i F_i \partial_iF_j - \partial_j F_i F_i F_i F_i J_{ij} = \partial_j F_i J_{ij} \partial_j F_i + \partial_i F_j f \partial_i f f F_i,"['multivariable-calculus', 'partial-derivative', 'vector-analysis', 'vector-fields', 'jacobian']"
93,$\frac{d\vec{r}}{dt} \cdot \frac{d^2\vec{r}}{dt^2} = \frac{1}{2} \frac{d}{dt}\left(\frac{d\vec{r}}{dt}\right)^2$,,\frac{d\vec{r}}{dt} \cdot \frac{d^2\vec{r}}{dt^2} = \frac{1}{2} \frac{d}{dt}\left(\frac{d\vec{r}}{dt}\right)^2,"why is this true? $\frac{d\vec{r}}{dt} \cdot \frac{d^2\vec{r}}{dt^2} = \frac{1}{2} \frac{d}{dt}\left(\frac{d\vec{r}}{dt}\right)^2$ My problem states that $\vec{F}$ is a conservative field, ie: $F = \nabla \phi$ for some scalar potential $\phi$ . $$\begin{align}\vec{F}&=m\vec{a}\\ &=m\frac{\mathrm{d}^2\vec{r}}{\mathrm{d}t^2} \end{align}$$ I now take the dot product of each side: $$\vec{F} \cdot \frac{\mathrm{d}\vec{r}}{\mathrm{d}t}= m\frac{\mathrm{d}^2\vec{r}}{\mathrm{d}t^2}\cdot \frac{\mathrm{d}\vec{r}}{\mathrm{d}t}$$ Now, the textbook says that: $$m\frac{\mathrm{d}\vec{r}}{\mathrm{d}t} \cdot \frac{\mathrm{d}^2\vec{r}}{\mathrm{d}t^2} = \frac{m}{2} \frac{\mathrm{d}}{\mathrm{d}t}\left(\frac{\mathrm{d}\vec{r}}{\mathrm{d}t}\right)^2$$ How did the textbook get this answer? : $\frac{m}{2} \frac{d}{dt}\left(\frac{d\vec{r}}{dt}\right)^2$ ? All of this leads to the result, which I'm OK with: $$\int \limits_{A}^{B} \vec{F} \cdot \mathrm{d}\vec{r} = \bigg[\frac{m}{2}v^2\bigg]^{B}_{A}$$","why is this true? My problem states that is a conservative field, ie: for some scalar potential . I now take the dot product of each side: Now, the textbook says that: How did the textbook get this answer? : ? All of this leads to the result, which I'm OK with:","\frac{d\vec{r}}{dt} \cdot \frac{d^2\vec{r}}{dt^2} = \frac{1}{2} \frac{d}{dt}\left(\frac{d\vec{r}}{dt}\right)^2 \vec{F} F = \nabla \phi \phi \begin{align}\vec{F}&=m\vec{a}\\
&=m\frac{\mathrm{d}^2\vec{r}}{\mathrm{d}t^2}
\end{align} \vec{F} \cdot \frac{\mathrm{d}\vec{r}}{\mathrm{d}t}= m\frac{\mathrm{d}^2\vec{r}}{\mathrm{d}t^2}\cdot \frac{\mathrm{d}\vec{r}}{\mathrm{d}t} m\frac{\mathrm{d}\vec{r}}{\mathrm{d}t} \cdot \frac{\mathrm{d}^2\vec{r}}{\mathrm{d}t^2} = \frac{m}{2} \frac{\mathrm{d}}{\mathrm{d}t}\left(\frac{\mathrm{d}\vec{r}}{\mathrm{d}t}\right)^2 \frac{m}{2} \frac{d}{dt}\left(\frac{d\vec{r}}{dt}\right)^2 \int \limits_{A}^{B} \vec{F} \cdot \mathrm{d}\vec{r} = \bigg[\frac{m}{2}v^2\bigg]^{B}_{A}","['multivariable-calculus', 'vector-analysis']"
94,Defining a function at some given point to make it continuous,Defining a function at some given point to make it continuous,,"So I have a function $$ f(x,y) = \frac{xy(x^2-y^2)}{x^2+y^2} $$ I have to define the function at $(0,0)$ so that it is continuous at origin. Is there any general approach we follow in this type of problems?  I am not sure but I think Sandwich Theorem has to be applied and some suitable function has to be taken .",So I have a function I have to define the function at so that it is continuous at origin. Is there any general approach we follow in this type of problems?  I am not sure but I think Sandwich Theorem has to be applied and some suitable function has to be taken .," f(x,y) = \frac{xy(x^2-y^2)}{x^2+y^2}  (0,0)",['multivariable-calculus']
95,"Triple integral $\iiint_D x^2yz \,dx\,dy\,dz$ over a strange area",Triple integral  over a strange area,"\iiint_D x^2yz \,dx\,dy\,dz","Fairly simple triple integral $$\iiint_D x^2yz \,dx\,dy\,dz$$ over the area $D = \{(x,y,z):0 \leq x \leq y+z \leq z \leq 1\}$ . I'm not sure how to interpret this area, this is what I have done so far: Since the area is strictly positive we get from $0 \leq x \leq y+z \leq z \leq 1$ $$\begin{align} 0 &\leq x \leq 1 \\ -z &\leq y \leq 0 \qquad \text{and} \\ 0 &\leq z \leq 1\end{align}$$ Which gives me the integral: $$\int_0^1 \int_{-z}^0 \int_0^1 (x^2yz) \,dx\,dy\,dz$$ This I can fairly easily calculate, giving me the final answer $\frac{1}{24}$ , (I dont have the key). I'm not sure my integration limits are correct, if not any pointers to how I can figure them out would be greatly appreciated. Thanks in advance.","Fairly simple triple integral over the area . I'm not sure how to interpret this area, this is what I have done so far: Since the area is strictly positive we get from Which gives me the integral: This I can fairly easily calculate, giving me the final answer , (I dont have the key). I'm not sure my integration limits are correct, if not any pointers to how I can figure them out would be greatly appreciated. Thanks in advance.","\iiint_D x^2yz \,dx\,dy\,dz D = \{(x,y,z):0 \leq x \leq y+z \leq z \leq 1\} 0 \leq x \leq y+z \leq z \leq 1 \begin{align} 0 &\leq x \leq 1 \\ -z &\leq y \leq 0 \qquad \text{and} \\ 0 &\leq z \leq 1\end{align} \int_0^1 \int_{-z}^0 \int_0^1 (x^2yz) \,dx\,dy\,dz \frac{1}{24}","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
96,"Boyd & Vandenberghe, Problem 9.6 — Gradient descent step for a quadratic function","Boyd & Vandenberghe, Problem 9.6 — Gradient descent step for a quadratic function",,"The question in Boyd & Vandenberghe's Convex Optimization is very simple — calculate the gradient descent step of a quadratic function. However, the given answer is very confusing. How can we directly get the equation ( yellow part in the picture given below ) without any further information?","The question in Boyd & Vandenberghe's Convex Optimization is very simple — calculate the gradient descent step of a quadratic function. However, the given answer is very confusing. How can we directly get the equation ( yellow part in the picture given below ) without any further information?",,"['linear-algebra', 'multivariable-calculus', 'convex-optimization', 'numerical-optimization', 'gradient-descent']"
97,"If $f \colon \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable and vanishing, it has $0$ gradient somewhere.","If  is differentiable and vanishing, it has  gradient somewhere.",f \colon \mathbb{R}^n \rightarrow \mathbb{R} 0,"I'm interested in ideas for improving and fixing the proof I wrote for the following theorem: Let $f \colon \mathbb{R}^n \to \mathbb{R} $ be differentiable, and $ \lim_{\| x \| \to \infty} f(x) = 0 $ . Then $\nabla f(x) = 0 $ for some $x \in \mathbb{R}^n$ . Here's the idea of the proof. First, since $f$ is differentiable, it is continuous. As $ \lim_{\| x \| \to \infty} f(x) = 0 $ , $\forall \varepsilon > 0, \exists r \in \mathbb{R} : |f(x) - 0| < \varepsilon$ whenever $\| x \| > r$ . If we choose $D = \{x \in \mathbb{R}^n : \| x \| \leq r \}$ , we can use the theorem that states that all continuous functions are bounded inside closed sets. In other words, there's a supremum of $|f(x)|$ in $D$ . Then we just look at the cases: if $f(x) = 0$ , so its gradient is always 0 and we're done. If $f$ varies in the set $D$ , there exist $a,b \in D$ such that $f(a) \neq f(b)$ , ie. $\exists \varepsilon_2 > 0$ so $| f(a) - f(b) | > \varepsilon_2 $ . If we choose $\varepsilon_2 > \varepsilon$ , $|f(x)|$ attains greater values in $D$ than outside it, and if we choose $c$ to be a point such that $$f(c) = \sup{\{f(x) : x \in D\}}$$ Then $|f(x)| \leq |f(c)|\quad \forall x \in D$ and as $f$ is differentiable, $\nabla f(c) = 0$ . There are more than a few issues I have with the formulation of the proof. First, "" $f$ attains greater values in $D$ than outside it"" seems a little ambiguous. Then the choosing of $c$ in a convenient way after having talked about it at such length... Additionally, I'd like to use the definition of differentiability that states that if $f$ is differentiable, it can be represented as $$f(x_0+h) = f(x_0) + Df(x_0)h + \varepsilon(h)\| h \|,\quad h \in \mathbb{R}^n $$ where $\varepsilon(h)\| h \| \to 0$ as $\| h \| \to 0$ , and where $Df(x)$ is the gradient in this case, or the Jacobian in a more general case. I'm almost certain you could bound the gradient $Df(c)$ to $0$ somehow using that definition, because it gives you a semi-explicit expression, instead of the verbal hand-waving I'm facing. There might've also been a method much simpler than this, but I couldn't exactly employ the mean value theorem easily here with the whole open domain. Maybe using the $D$ I defined there would've worked.","I'm interested in ideas for improving and fixing the proof I wrote for the following theorem: Let be differentiable, and . Then for some . Here's the idea of the proof. First, since is differentiable, it is continuous. As , whenever . If we choose , we can use the theorem that states that all continuous functions are bounded inside closed sets. In other words, there's a supremum of in . Then we just look at the cases: if , so its gradient is always 0 and we're done. If varies in the set , there exist such that , ie. so . If we choose , attains greater values in than outside it, and if we choose to be a point such that Then and as is differentiable, . There are more than a few issues I have with the formulation of the proof. First, "" attains greater values in than outside it"" seems a little ambiguous. Then the choosing of in a convenient way after having talked about it at such length... Additionally, I'd like to use the definition of differentiability that states that if is differentiable, it can be represented as where as , and where is the gradient in this case, or the Jacobian in a more general case. I'm almost certain you could bound the gradient to somehow using that definition, because it gives you a semi-explicit expression, instead of the verbal hand-waving I'm facing. There might've also been a method much simpler than this, but I couldn't exactly employ the mean value theorem easily here with the whole open domain. Maybe using the I defined there would've worked.","f \colon \mathbb{R}^n \to \mathbb{R}   \lim_{\| x \| \to \infty} f(x) = 0  \nabla f(x) = 0  x \in \mathbb{R}^n f  \lim_{\| x \| \to \infty} f(x) = 0  \forall \varepsilon > 0, \exists r \in \mathbb{R} : |f(x) - 0| < \varepsilon \| x \| > r D = \{x \in \mathbb{R}^n : \| x \| \leq r \} |f(x)| D f(x) = 0 f D a,b \in D f(a) \neq f(b) \exists \varepsilon_2 > 0 | f(a) - f(b) | > \varepsilon_2  \varepsilon_2 > \varepsilon |f(x)| D c f(c) = \sup{\{f(x) : x \in D\}} |f(x)| \leq |f(c)|\quad \forall x \in D f \nabla f(c) = 0 f D c f f(x_0+h) = f(x_0) + Df(x_0)h + \varepsilon(h)\| h \|,\quad h \in \mathbb{R}^n  \varepsilon(h)\| h \| \to 0 \| h \| \to 0 Df(x) Df(c) 0 D","['multivariable-calculus', 'derivatives', 'vector-analysis']"
98,"Let $f$ continuous, show that $\int_0^a (\int_0^x f(x,y) dy) dx = \int_0^a (\int_y^a f(x,y) dx) dy$","Let  continuous, show that","f \int_0^a (\int_0^x f(x,y) dy) dx = \int_0^a (\int_y^a f(x,y) dx) dy","$$\int_0^a \int_0^x f(x,y) dy dx = \int_0^a \int_y^a f(x,y) dx dy$$ I have tried to help myself, observing the graph and how the area is covered according to the limits, and managed to see that the area of the two integrals are symmetrical. But I am almost certain that that is not necessary to say that the integrals are equal, and I don't know how I could say it.","I have tried to help myself, observing the graph and how the area is covered according to the limits, and managed to see that the area of the two integrals are symmetrical. But I am almost certain that that is not necessary to say that the integrals are equal, and I don't know how I could say it.","\int_0^a \int_0^x f(x,y) dy dx = \int_0^a \int_y^a f(x,y) dx dy","['integration', 'multivariable-calculus']"
99,Differentiability of homogeneous functions in n variables,Differentiability of homogeneous functions in n variables,,"Suppose $f=f(x_1,x_2,...,x_n)$ is a homogeneous function $$f(Cx_1,Cx_2,...,Cx_n)=C^\lambda f(x_1,x_2,...,x_n)$$ 1) Is $f$ differentiable w/respect to all its arguments? 2) Is $f$ differentiable w/respect to all its arguments at the origin? 3) Is $f$ differentiable w/respect to all its arguments at a point, where some of the $x_i$ are negative or zero? 4) Are all partial derivatives of $f$ also differentiable, to what order and where? Perhaps Euler's theorem for homogeneous functions is related to the question... When is a homogeneous function also differentiable?","Suppose is a homogeneous function 1) Is differentiable w/respect to all its arguments? 2) Is differentiable w/respect to all its arguments at the origin? 3) Is differentiable w/respect to all its arguments at a point, where some of the are negative or zero? 4) Are all partial derivatives of also differentiable, to what order and where? Perhaps Euler's theorem for homogeneous functions is related to the question... When is a homogeneous function also differentiable?","f=f(x_1,x_2,...,x_n) f(Cx_1,Cx_2,...,Cx_n)=C^\lambda f(x_1,x_2,...,x_n) f f f x_i f","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
