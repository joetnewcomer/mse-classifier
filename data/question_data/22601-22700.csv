,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Let A and B be $n \times n$ real matrices with same minimal polynomial.,Let A and B be  real matrices with same minimal polynomial.,n \times n,"Let $A$ and $B$ be $n \times n$ real matrices with same minimal polynomial. Then (i) $A$ is similar to $B$. (ii) $A-B$ is singular. (iii) $A$ is diagonalizable if $B$ is so. (iv) $A$ and $B$ commute. I think only (iii) is the correct option, similar matrcies have same characteristics polynomial but the converse may not be true, $\begin{pmatrix}1&0\\0&0\end{pmatrix}\times \begin{pmatrix}0&1\\0&1\end{pmatrix}\ne \begin{pmatrix}0&1\\0&1\end{pmatrix}\times \begin{pmatrix}1&0\\0&0\end{pmatrix}$ though they have same minpoly $x(x-1)$ the same two matrices works as a counter example for (ii), am I right ?","Let $A$ and $B$ be $n \times n$ real matrices with same minimal polynomial. Then (i) $A$ is similar to $B$. (ii) $A-B$ is singular. (iii) $A$ is diagonalizable if $B$ is so. (iv) $A$ and $B$ commute. I think only (iii) is the correct option, similar matrcies have same characteristics polynomial but the converse may not be true, $\begin{pmatrix}1&0\\0&0\end{pmatrix}\times \begin{pmatrix}0&1\\0&1\end{pmatrix}\ne \begin{pmatrix}0&1\\0&1\end{pmatrix}\times \begin{pmatrix}1&0\\0&0\end{pmatrix}$ though they have same minpoly $x(x-1)$ the same two matrices works as a counter example for (ii), am I right ?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory']"
1,When is a matrix congruent to a diagonal matrix and how to find the congruent transformation?,When is a matrix congruent to a diagonal matrix and how to find the congruent transformation?,,"What matrix can be congruent to a diagonal matrix and how can we find the congruent transform and the diagonal matrix? One special case is when the congruence is also similarity. For example, for a normal matrix, we can use a transform which is both similar and congruent to convert it into a diagonal matrix. But is it the only case where congruence to a diagonal matrix is of interest? Thanks and regards!","What matrix can be congruent to a diagonal matrix and how can we find the congruent transform and the diagonal matrix? One special case is when the congruence is also similarity. For example, for a normal matrix, we can use a transform which is both similar and congruent to convert it into a diagonal matrix. But is it the only case where congruence to a diagonal matrix is of interest? Thanks and regards!",,"['linear-algebra', 'matrices']"
2,How do i prove that $\det(tI-A)$ is a polynomial?,How do i prove that  is a polynomial?,\det(tI-A),"In wikipedia, it's said "" $\det(tI-A)$ can be explicitly evaluated using exterior algebra"", but i have not learned exterior algebra yet and i just want to know whether it is polynomial, not how it looks like. How do i prove that $\det(tI-A)$ is a polynomial in $\mathbb{F}[t]$ where $\mathbb{F}$ is a field and $A$ is an $n\times n$ matrix?","In wikipedia, it's said "" can be explicitly evaluated using exterior algebra"", but i have not learned exterior algebra yet and i just want to know whether it is polynomial, not how it looks like. How do i prove that is a polynomial in where is a field and is an matrix?",\det(tI-A) \det(tI-A) \mathbb{F}[t] \mathbb{F} A n\times n,['linear-algebra']
3,Let $T: \mathbb{R}^3→\mathbb{R}^3$ be a linear transformation. Show that there is a line $L$ such that $T(L) = L$.,Let  be a linear transformation. Show that there is a line  such that .,T: \mathbb{R}^3→\mathbb{R}^3 L T(L) = L,Let $T: \mathbb{R}^3→\mathbb{R}^3$ be a linear transformation. Show that there is a line $L$ such that $T(L) = L$. I am totally stuck on it.how can I able to solve this problem?please  somebody help.thanks for your kind help.,Let $T: \mathbb{R}^3→\mathbb{R}^3$ be a linear transformation. Show that there is a line $L$ such that $T(L) = L$. I am totally stuck on it.how can I able to solve this problem?please  somebody help.thanks for your kind help.,,['linear-algebra']
4,Simplicity of eigenvalue,Simplicity of eigenvalue,,"I have a matrix $A$ and I introduce $(I+A)^{m},$ where $I$ is the identity matrix of same order with $A$ and $m$ is a positive integer. I want to show that if $(1+ \lambda )^m$ is a simple eigenvalue of $(I+A)^m$, that is, it is not a repeated root of the characteristic polynomial of $(I+A)^m,$ then $ \lambda$ is a simple eigenvalue of $A.$ Any help with this will be highly appreciated. Edit: what I mean by a simple eigenvalue. For an arbitrary matrix $A$ of order $n \times n,$  let $  \lbrace \lambda_1,\lambda_2,\lambda_3, \dots ,\lambda_k \rbrace $ be the set of distinct eigenvalues of $A.$ The characteristic polynomial of $A,$ denoted $p(\lambda),$ can be written in the factorized form \begin{equation} p(\lambda) = (\lambda_1-\lambda)^{n_1}(\lambda_2-\lambda)^{n_2} \dotsm (\lambda_k-\lambda)^{n_k}\end{equation} with $n_1+n_2+ \dotsm +n_k = n.$ The exponent $n_i,$ corresponding to each eigenvalue $\lambda_i,$ is called the algebraic multiplicity of $ \lambda_i$ and the dimension of the null space of $ A - \lambda_i I,  \dim(N(A - \lambda_i I )),$ is called the geometric multiplicity of $ \lambda_i.$  If $n_i=1$ for some $i \in \lbrace 1, 2, \dots, k \rbrace,$ then $ \lambda_i$ is said to be a simple eigenvalue of $A.$","I have a matrix $A$ and I introduce $(I+A)^{m},$ where $I$ is the identity matrix of same order with $A$ and $m$ is a positive integer. I want to show that if $(1+ \lambda )^m$ is a simple eigenvalue of $(I+A)^m$, that is, it is not a repeated root of the characteristic polynomial of $(I+A)^m,$ then $ \lambda$ is a simple eigenvalue of $A.$ Any help with this will be highly appreciated. Edit: what I mean by a simple eigenvalue. For an arbitrary matrix $A$ of order $n \times n,$  let $  \lbrace \lambda_1,\lambda_2,\lambda_3, \dots ,\lambda_k \rbrace $ be the set of distinct eigenvalues of $A.$ The characteristic polynomial of $A,$ denoted $p(\lambda),$ can be written in the factorized form \begin{equation} p(\lambda) = (\lambda_1-\lambda)^{n_1}(\lambda_2-\lambda)^{n_2} \dotsm (\lambda_k-\lambda)^{n_k}\end{equation} with $n_1+n_2+ \dotsm +n_k = n.$ The exponent $n_i,$ corresponding to each eigenvalue $\lambda_i,$ is called the algebraic multiplicity of $ \lambda_i$ and the dimension of the null space of $ A - \lambda_i I,  \dim(N(A - \lambda_i I )),$ is called the geometric multiplicity of $ \lambda_i.$  If $n_i=1$ for some $i \in \lbrace 1, 2, \dots, k \rbrace,$ then $ \lambda_i$ is said to be a simple eigenvalue of $A.$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
5,Prove Noether-Skolem theorem for $M_2(\mathbb{C})$ by calculation,Prove Noether-Skolem theorem for  by calculation,M_2(\mathbb{C}),"Noether-Skolem Theorem for the case the ring is $M_2(\mathbb{C})$ says that ""Every $\mathbb{C}$-algebra automorphism of $M_2(\mathbb{C})$ is inner."" Now, how to prove it by a direct calculation? I have done the following: Suppose $\phi: M_2(\mathbb{C})\to M_2(\mathbb{C})$ is a $\mathbb{C}$-algebra automorphism, then we want to find an invertible matrix $A=(\begin{array}{cc}x&y\\z&w\end{array})$, suppose $A^{-1}=(\begin{array}{cc}x'&y'\\z'&w'\end{array})$, such that $\phi(X)=AXA^{-1}, \forall X\in M_2(\mathbb{C})$. Then, since $\phi$ is uniquely determined by $\phi(E_{1,2}), \phi(E_{2,1})$, where $E_{i,j},1\leq i,j\leq 2$ are the matrix units for $M_2(\mathbb{C})$. Calculation shows that $\phi(E_{1,2})=(x\ z)^T(z'\ w'), \phi(E_{2,1})=(y\ w)^T(x'\ y')$. Then, how to proceed?","Noether-Skolem Theorem for the case the ring is $M_2(\mathbb{C})$ says that ""Every $\mathbb{C}$-algebra automorphism of $M_2(\mathbb{C})$ is inner."" Now, how to prove it by a direct calculation? I have done the following: Suppose $\phi: M_2(\mathbb{C})\to M_2(\mathbb{C})$ is a $\mathbb{C}$-algebra automorphism, then we want to find an invertible matrix $A=(\begin{array}{cc}x&y\\z&w\end{array})$, suppose $A^{-1}=(\begin{array}{cc}x'&y'\\z'&w'\end{array})$, such that $\phi(X)=AXA^{-1}, \forall X\in M_2(\mathbb{C})$. Then, since $\phi$ is uniquely determined by $\phi(E_{1,2}), \phi(E_{2,1})$, where $E_{i,j},1\leq i,j\leq 2$ are the matrix units for $M_2(\mathbb{C})$. Calculation shows that $\phi(E_{1,2})=(x\ z)^T(z'\ w'), \phi(E_{2,1})=(y\ w)^T(x'\ y')$. Then, how to proceed?",,"['linear-algebra', 'abstract-algebra', 'ring-theory']"
6,What does $c.c.$ mean in this proof?,What does  mean in this proof?,c.c.,"This is a proof from Wikipedia of Moore-Penrose inverse being the optimal solution of a least squares problem, in which there is a acronym $c.c.$ occurred in some of the equations. Mind if I ask what does that represent?","This is a proof from Wikipedia of Moore-Penrose inverse being the optimal solution of a least squares problem, in which there is a acronym $c.c.$ occurred in some of the equations. Mind if I ask what does that represent?",,"['linear-algebra', 'notation']"
7,Problem in Deducing Perspective Projection Matrix,Problem in Deducing Perspective Projection Matrix,,"I understand the traditional way(use similar triangle and make depth value linear) to deduce the perspective projection matrix. But I want to try another approach after I read this text: Fundamentals of Texture Mapping and Image Warping . On page 17, it says that a quad can be mapped to a square using projective transformation, which can be expressed as a rational linear mapping: $$\mathit{x} = \frac{\mathit{a}\mathit{u} + \mathit{b}\mathit{v} + \mathit{c}} {\mathit{g}\mathit{u} + \mathit{h}\mathit{v} + \mathit{i}}\\ \mathit{y} = \frac{\mathit{d}\mathit{u} + \mathit{e}\mathit{v} + \mathit{f}} {\mathit{g}\mathit{u} + \mathit{h}\mathit{v} + \mathit{i}}$$ After I substitute four vertices of the quad and square, I get a linear system. By solving the linear system I can get the projective transformation matrix. Similarly, I conceive that a 3D projective mapping can be denoted as a rational linear mapping as well. And this rational linear mapping can map a frustum to a NDC cube.(note that because of the use of homogeneous coordinate, the last element of matrix(right bottom one) can be set to 1) $$ \mathit{x} = \frac{\mathit{a}\mathit{u} + \mathit{b}\mathit{v} + \mathit{c}\mathit{w} + \mathit{d}} {\mathit{m}\mathit{u} + \mathit{n}\mathit{v} + \mathit{o}\mathit{w} + 1}\\ \mathit{y} = \frac{\mathit{e}\mathit{u} + \mathit{f}\mathit{v} + \mathit{g}\mathit{w} + \mathit{h}} {\mathit{m}\mathit{u} + \mathit{n}\mathit{v} + \mathit{o}\mathit{w} + 1}\\ \mathit{z} = \frac{\mathit{i}\mathit{u} + \mathit{j}\mathit{v} + \mathit{k}\mathit{w} + \mathit{l}} {\mathit{m}\mathit{u} + \mathit{n}\mathit{v} + \mathit{o}\mathit{w} + 1} $$ But when I try to solve this system, the result matrix is not as same as the one in 3D API(like OpenGL) specification. My question is: is there any extra properties of a perspective projection matrix that a rational linear mapping does not have? EDIT: I found that in the 2D version we have 8 unknowns(3*3, and one matrix element set to 1 excluded), which equals exactly the number of equations in the linear system(4 vertices in a quad, and 2 corrdinate components in each of them). However in the 3D version, the number of unknowns and the number of equations does not match. I suspect I misunderstand the rational linear mapping, and the reason why it can applied to a 2D version is just a coincidence. I will do more learn on this topic.","I understand the traditional way(use similar triangle and make depth value linear) to deduce the perspective projection matrix. But I want to try another approach after I read this text: Fundamentals of Texture Mapping and Image Warping . On page 17, it says that a quad can be mapped to a square using projective transformation, which can be expressed as a rational linear mapping: $$\mathit{x} = \frac{\mathit{a}\mathit{u} + \mathit{b}\mathit{v} + \mathit{c}} {\mathit{g}\mathit{u} + \mathit{h}\mathit{v} + \mathit{i}}\\ \mathit{y} = \frac{\mathit{d}\mathit{u} + \mathit{e}\mathit{v} + \mathit{f}} {\mathit{g}\mathit{u} + \mathit{h}\mathit{v} + \mathit{i}}$$ After I substitute four vertices of the quad and square, I get a linear system. By solving the linear system I can get the projective transformation matrix. Similarly, I conceive that a 3D projective mapping can be denoted as a rational linear mapping as well. And this rational linear mapping can map a frustum to a NDC cube.(note that because of the use of homogeneous coordinate, the last element of matrix(right bottom one) can be set to 1) $$ \mathit{x} = \frac{\mathit{a}\mathit{u} + \mathit{b}\mathit{v} + \mathit{c}\mathit{w} + \mathit{d}} {\mathit{m}\mathit{u} + \mathit{n}\mathit{v} + \mathit{o}\mathit{w} + 1}\\ \mathit{y} = \frac{\mathit{e}\mathit{u} + \mathit{f}\mathit{v} + \mathit{g}\mathit{w} + \mathit{h}} {\mathit{m}\mathit{u} + \mathit{n}\mathit{v} + \mathit{o}\mathit{w} + 1}\\ \mathit{z} = \frac{\mathit{i}\mathit{u} + \mathit{j}\mathit{v} + \mathit{k}\mathit{w} + \mathit{l}} {\mathit{m}\mathit{u} + \mathit{n}\mathit{v} + \mathit{o}\mathit{w} + 1} $$ But when I try to solve this system, the result matrix is not as same as the one in 3D API(like OpenGL) specification. My question is: is there any extra properties of a perspective projection matrix that a rational linear mapping does not have? EDIT: I found that in the 2D version we have 8 unknowns(3*3, and one matrix element set to 1 excluded), which equals exactly the number of equations in the linear system(4 vertices in a quad, and 2 corrdinate components in each of them). However in the 3D version, the number of unknowns and the number of equations does not match. I suspect I misunderstand the rational linear mapping, and the reason why it can applied to a 2D version is just a coincidence. I will do more learn on this topic.",,"['linear-algebra', 'projective-geometry']"
8,Generalized Eigenvalue Problem,Generalized Eigenvalue Problem,,"Consider a generalized Eigenvalue problem $Av = \lambda Bv$ where $A$ and $B$ are square matrices of the same dimension. It is known that $A$ is positive semidefinite, and that $B$ is diagonal with positive entries. It is clear that the generalized eigenvalues will be nonnegative. What else can one say about the eigenvalues of the generalized problem in terms of the eigenvalues of $A$ and the diagonals of $B$? Equivalently, what else can one say about the eigenvalues of $B^{-1}A$? It seems reasonable (skipping over zero eigenvalues) that $$ \lambda_{min}(B^{-1}A) \geq \lambda_{min}(A)/B_{max} $$ but I am unable to see how one could rigorously show this, and it is perhaps a conservative bound. Equivalently again, what could one say about the eigenvalues of $$ B^{-1/2}AB^{-1/2} $$ ?","Consider a generalized Eigenvalue problem $Av = \lambda Bv$ where $A$ and $B$ are square matrices of the same dimension. It is known that $A$ is positive semidefinite, and that $B$ is diagonal with positive entries. It is clear that the generalized eigenvalues will be nonnegative. What else can one say about the eigenvalues of the generalized problem in terms of the eigenvalues of $A$ and the diagonals of $B$? Equivalently, what else can one say about the eigenvalues of $B^{-1}A$? It seems reasonable (skipping over zero eigenvalues) that $$ \lambda_{min}(B^{-1}A) \geq \lambda_{min}(A)/B_{max} $$ but I am unable to see how one could rigorously show this, and it is perhaps a conservative bound. Equivalently again, what could one say about the eigenvalues of $$ B^{-1/2}AB^{-1/2} $$ ?",,['linear-algebra']
9,Why is the kernel of this strange polynomial homomorphism what it is?,Why is the kernel of this strange polynomial homomorphism what it is?,,"I've been trying to delve a little further into linear algebra, but I'm not following something I think is supposed to be obvious. Suppose $M_{m,n}(\mathbb{C})$ is the set of rectangular $m\times n$ matrices over $\mathbb{C}$, and let $S$ be the set of rank $1$ matrices. Furthermore, let $K\subset\mathbb{C}[S_{11},\dots,S_{mn}]$ be the ideal associated to $S$. Then the homomorphism $\mathbb{C}[S_{11},\dots,S_{mn}]\to\mathbb{C}[X_1,\dots,X_m,Y_1,\dots,Y_n]$ such that $S_{ij}\mapsto X_iY_j$ has kernel $K$. I don't follow the last claim. I'm used to the associated ideal of $S$ to be the polynomials in $\mathbb{C}[S_{11},\dots,S_{mn}]$ to be the ideal of polynomials which vanish on all points of $S$ for an algebraic set of zeroes, but that doesn't quite make sense with a set of rank $1$ matrices. Would someone be nice enough to explain why the kernel above is what it is? Thank you.","I've been trying to delve a little further into linear algebra, but I'm not following something I think is supposed to be obvious. Suppose $M_{m,n}(\mathbb{C})$ is the set of rectangular $m\times n$ matrices over $\mathbb{C}$, and let $S$ be the set of rank $1$ matrices. Furthermore, let $K\subset\mathbb{C}[S_{11},\dots,S_{mn}]$ be the ideal associated to $S$. Then the homomorphism $\mathbb{C}[S_{11},\dots,S_{mn}]\to\mathbb{C}[X_1,\dots,X_m,Y_1,\dots,Y_n]$ such that $S_{ij}\mapsto X_iY_j$ has kernel $K$. I don't follow the last claim. I'm used to the associated ideal of $S$ to be the polynomials in $\mathbb{C}[S_{11},\dots,S_{mn}]$ to be the ideal of polynomials which vanish on all points of $S$ for an algebraic set of zeroes, but that doesn't quite make sense with a set of rank $1$ matrices. Would someone be nice enough to explain why the kernel above is what it is? Thank you.",,"['linear-algebra', 'matrices', 'algebraic-geometry']"
10,Holomorphic function of a matrix,Holomorphic function of a matrix,,"A statement is made below. The questions are: (a) Is the statement true? (b) If it is, does it appear in the literature? Here is the statement. For any matrix $A$ in $M_n(\mathbb C)$, write $\Lambda(A)$ for the set of eigenvalues of $A$. Recall that there is a unique continuous $\mathbb C[X]$-algebra morphism  $$ \mathcal O(\Lambda(A))\to M_n(\mathbb C), $$ where $\mathcal O(\Lambda(A))$ is the algebra of those functions which are holomorphic on (some open neighborhood of) $\Lambda(A)$. Recall also that this morphism is usually denoted by $f\mapsto f(A)$. (Here $X$ is an indeterminate.) Let $U$ be an open subset of $\mathbb C$, let $U'$ be the open subset of $M_n(\mathbb C)$ defined by the condition  $$ \Lambda(A)\subset U, $$ and let $f$ be holomorphic on $U$. (The fact the $U'$ is open follows from Rouché's Theorem .) STATEMENT. The map $A\mapsto f(A)$ from $U'$ to $M_n(\mathbb C)$ is holomorphic.","A statement is made below. The questions are: (a) Is the statement true? (b) If it is, does it appear in the literature? Here is the statement. For any matrix $A$ in $M_n(\mathbb C)$, write $\Lambda(A)$ for the set of eigenvalues of $A$. Recall that there is a unique continuous $\mathbb C[X]$-algebra morphism  $$ \mathcal O(\Lambda(A))\to M_n(\mathbb C), $$ where $\mathcal O(\Lambda(A))$ is the algebra of those functions which are holomorphic on (some open neighborhood of) $\Lambda(A)$. Recall also that this morphism is usually denoted by $f\mapsto f(A)$. (Here $X$ is an indeterminate.) Let $U$ be an open subset of $\mathbb C$, let $U'$ be the open subset of $M_n(\mathbb C)$ defined by the condition  $$ \Lambda(A)\subset U, $$ and let $f$ be holomorphic on $U$. (The fact the $U'$ is open follows from Rouché's Theorem .) STATEMENT. The map $A\mapsto f(A)$ from $U'$ to $M_n(\mathbb C)$ is holomorphic.",,"['linear-algebra', 'matrices', 'reference-request', 'polynomials', 'several-complex-variables']"
11,Integer matrix with particular Jordan's form,Integer matrix with particular Jordan's form,,For teaching purposes I would like to find integer matrices with a particular Jordan's form. Is there some kind of technique to find nice examples? For example for $$\begin{pmatrix}1&1&0\\0&1&0\\0&0&1\end{pmatrix}.$$,For teaching purposes I would like to find integer matrices with a particular Jordan's form. Is there some kind of technique to find nice examples? For example for $$\begin{pmatrix}1&1&0\\0&1&0\\0&0&1\end{pmatrix}.$$,,"['linear-algebra', 'matrices']"
12,What's the distribution of eigenvalues of a real and symmetric Toeplitz matrix?,What's the distribution of eigenvalues of a real and symmetric Toeplitz matrix?,,"Can you please help me find the distribution of eigenvalues of a Toeplitz matrix $\mathbf{K}$ that is constructed as follows: $$\mathbf{K}=\left[                  \begin{array}{cccc}                    1 & \rho & \ldots & \, \, \rho^{N-1} \\                    \rho & 1 & \ldots &  \, \,\rho^{N-2}\\                    \vdots & \vdots & \ddots & \vdots \\                    \rho^{N-1} & \rho^{N-2} & \ldots & 1 \\                  \end{array}                \right].$$ where $0 \leq \rho < 1$. Thanks a lot in advance, Farzad","Can you please help me find the distribution of eigenvalues of a Toeplitz matrix $\mathbf{K}$ that is constructed as follows: $$\mathbf{K}=\left[                  \begin{array}{cccc}                    1 & \rho & \ldots & \, \, \rho^{N-1} \\                    \rho & 1 & \ldots &  \, \,\rho^{N-2}\\                    \vdots & \vdots & \ddots & \vdots \\                    \rho^{N-1} & \rho^{N-2} & \ldots & 1 \\                  \end{array}                \right].$$ where $0 \leq \rho < 1$. Thanks a lot in advance, Farzad",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'toeplitz-matrices']"
13,"Is there an accepted term for ""locally"" nilpotent linear operators?","Is there an accepted term for ""locally"" nilpotent linear operators?",,"Let $V$ be a vector space over a field $k$ (not necessarily finite-dimensional) and $T : V \to V$ a linear operator. Is there an accepted term for the following condition on $T$? For any $v \in V$ the subspace $\text{span}(v, Tv, T^2 v, ...)$ is finite-dimensional, and $T$ is nilpotent on any such subspace. For example, the differential operator $\frac{d}{dx}$ acting on $k[x]$ satisfies this condition but is not nilpotent. Motivation: When $\text{char}(k) = 0$, this condition ensures that the exponential $e^T : V \to V$ is well-defined without giving $V$ any additional structure, since $e^T v$ is a finite sum for any particular $v$.","Let $V$ be a vector space over a field $k$ (not necessarily finite-dimensional) and $T : V \to V$ a linear operator. Is there an accepted term for the following condition on $T$? For any $v \in V$ the subspace $\text{span}(v, Tv, T^2 v, ...)$ is finite-dimensional, and $T$ is nilpotent on any such subspace. For example, the differential operator $\frac{d}{dx}$ acting on $k[x]$ satisfies this condition but is not nilpotent. Motivation: When $\text{char}(k) = 0$, this condition ensures that the exponential $e^T : V \to V$ is well-defined without giving $V$ any additional structure, since $e^T v$ is a finite sum for any particular $v$.",,"['linear-algebra', 'terminology']"
14,Linear transformations and norm,Linear transformations and norm,,"I am studying Linear Algebra II, and I came across several questions in which, for a certain linear transformation ($T\colon\mathbf{V}\to\mathbf{V}$) I was told that: $$||T(a)|| \leq ||a||.$$ I am not completely certain how to use this information. For instance, consider the following question (please forgive my translation, it's the first time I write math in English): For a linear transformation $T\colon\mathbf{V}\to\mathbf{V}$in a unitary space [i.e., complex inner product space], such that $|c|=1$ for every eigenvalue $c$ of $T$; $||T(a)|| \leq ||a||$ for every vector $a$ in $\mathbf{V}$;   prove that T is a unitary operator. How does the fact that $||T(a)|| \leq ||a||$ help me? Thanks.","I am studying Linear Algebra II, and I came across several questions in which, for a certain linear transformation ($T\colon\mathbf{V}\to\mathbf{V}$) I was told that: $$||T(a)|| \leq ||a||.$$ I am not completely certain how to use this information. For instance, consider the following question (please forgive my translation, it's the first time I write math in English): For a linear transformation $T\colon\mathbf{V}\to\mathbf{V}$in a unitary space [i.e., complex inner product space], such that $|c|=1$ for every eigenvalue $c$ of $T$; $||T(a)|| \leq ||a||$ for every vector $a$ in $\mathbf{V}$;   prove that T is a unitary operator. How does the fact that $||T(a)|| \leq ||a||$ help me? Thanks.",,['linear-algebra']
15,"Let $S$ be a finite set of real numbers, and let $T$ be the set of all $n\times n$ matrices having entries in $S$.","Let  be a finite set of real numbers, and let  be the set of all  matrices having entries in .",S T n\times n S,"Let $S$ be a finite set of real numbers, and let $T$ be the set of all $n\times n$ matrices having entries in $S$ . Prove that $$\sum\limits_{A\in T}\mbox{trace}(A^2)=\sum\limits_{A\in T}(\mbox{trace}(A))^2$$ Here I tried to proceed with the eigenvalue but since $A$ is real matrix eigenvalues many not be real. In that case, how we can prove the result","Let be a finite set of real numbers, and let be the set of all matrices having entries in . Prove that Here I tried to proceed with the eigenvalue but since is real matrix eigenvalues many not be real. In that case, how we can prove the result",S T n\times n S \sum\limits_{A\in T}\mbox{trace}(A^2)=\sum\limits_{A\in T}(\mbox{trace}(A))^2 A,"['linear-algebra', 'trace']"
16,Norm of Hermiticity-preserving linear map attained on rank-1 state?,Norm of Hermiticity-preserving linear map attained on rank-1 state?,,"Given some linear map $T:(\mathbb C^{n\times n},\|\cdot\|_1)\to (\mathbb C^{m\times m},\|\cdot\|_1)$ --- where $\|\cdot\|_1={\rm tr}(|\,\cdot\,|)$ is the trace norm, i.e. the sum of the input's singular values --- the question is the following: If $T$ preserves Hermiticity (i.e. $T(A)^*=T(A^*)$ for all $A\in\mathbb C^{n\times n}$ ) is the operator norm of $T$ attained on a Hermitian matrix, that is, is it true that $$ \|T\|_{1\to 1}=\sup_{\substack{A\in\mathbb C^{n\times n}\\A\text{ Hermitian, }\|A\|_1\leq 1}}\|T(A)\|_1\,? $$ Two observations: 1. $\geq$ is obvious so we would only have to show $\leq$ . 2. Because the trace norm is convex and the trace norm unit ball is the convex hull of rank-1 matrices $uv^*$ (with $\|u\|=\|v\|=1$ ) this can be re-formulated as follows: If $T$ preserves Hermiticity , is it true that the operator norm of $T$ is attained on a rank-1 state, that is, $$ \sup_{v,w\in\mathbb C^n,\|v\|=\|w\|=1}\|T(vw^*)\|_1\leq\sup_{v\in\mathbb C^n,\|v\|=1}\|T(vv^*)\|_1\,? $$ This is true if $T$ is positive (i.e. maps positive semi-definite elements to positive semi-definite elements) as a consequence of the Russo-Dye theorem, cf. Theorem 3.39 in ""The Theory of Quantum Information"" by Watrous, 2018. However, the proof given therein breaks down if $T$ is not positive but only Hermiticity-preserving as it relies on the fact that $\|\sum_j u_jP_j\|\leq\|u\|_\infty\|\sum_j P_j\|$ for all $P_j\geq 0$ (Lemma 3.3 in his book). Interestingly, this property holds for the ""stronger"" completely bounded trace norm --- sometimes called diamond norm --- which is defined as $$ \|T\|_\diamond:=\max_{k\in\mathbb N}\|T\otimes{\rm id}_{\mathbb C^{k\times k}}\|_{1\to 1}\,. $$ The idea here (cf. Lemma 3.50 ff.) is that if $X$ attains the operator norm of $T$ , then the norm of $T\otimes{\rm id}$ on $$ \frac12\begin{pmatrix}0&X\\X^*&0\end{pmatrix} $$ is $\|T\|_{1\to 1}$ because $T(X^*)=T(X)^*$ . Using the spectral decomposition of this block matrix together with convexity of the norm yields ""the"" desired rank-1 state $uu^*$ . So in some sense having access to ""more dimensions"" allows for a symmetrization of the problem. However, numerics suggest that this is not necessary: using that every Hermiticity-preserving map is the difference of two completely positive maps (cf. Theorem 2.25) I wrote some rough Mathematica code generating random cp maps and finding $\sup_{u,v}\|T(uv^*)\|_1$ and $\sup_u\|T(uu^*)\|_1$ via random search: (*Initialization*)d = 4; n = 0; c = 0; cnew = 0; nmax = 20000; A = IdentityMatrix[d]; B = IdentityMatrix[d]; A0 = IdentityMatrix[d]; v = UnitVector[d, 1]; RND = ResourceFunction[""RandomUnitVector""][d, 2]; dmax = RandomInteger[{1, d^2}]; Vs1 = Table[RandomReal[{-1, 1}, {d, d}], {dmax}]; Vs2 = Table[RandomReal[{-1, 1}, {d, d}], {dmax}]; f1[X_] := Total[ConjugateTranspose[#] . X . # & /@ Vs1]; f2[X_] := Total[ConjugateTranspose[#] . X . # & /@ Vs2]; f[X_] := f1[X] - f2[X]; (*While loop for sup of||f(uv*)||*) While[n < nmax, RND = ResourceFunction[""RandomUnitVector""][d, 2];  A = Transpose[{RND[[1]]}] . {RND[[2]]};  cnew = Total[SingularValueList[f[A]]];  If[cnew > c, B = A; c = Max[c, cnew];   If[n > nmax/50, PrintTemporary[c]]; n = 0];  n++]; Print[""non-Herm="", c]; c = 0; n = 0; (*While loop for sup of||f(uu*)||*) While[n < nmax, v = ResourceFunction[""RandomUnitVector""][d];  A = Transpose[{v}] . {v};  cnew = Total[SingularValueList[f[A]]];  If[cnew > c, B = A; c = Max[c, cnew];   If[n > nmax/50, PrintTemporary[c]]; n = 0];  n++]; Print[""Herm="", c] All the times I ran this code I did not find a violation of the conjecture, i.e. not once was the norm approximation (on $uv^*$ ) larger than the rank-1-state value (on $uu^*$ ). What makes this problem feel so annoying is that it seems like it should be obvious but so far I have not been able to prove this, not even for some simple classes of maps such as distances of unitary channels (e.g., $A\mapsto A-UAU^*$ for some $U$ unitary) or Hadamard product maps (e.g., $A\mapsto X\circ A$ for $X$ Hermitian). Problems which prevent a straightforward proof are that the usual convexity arguments do not work because the convex hull operator of Hermitian and skew-Hermitian matrices from the unit ball are a strict subset of the unit ball (consider $e_1e_2^*$ ) $T$ preserving Hermiticity does not hand down any nice properties to $T^*(V)$ for $V$ unitary; at least I don't see why it should. Therefore there is no straightforward application of some kind of Russo-Dye argument","Given some linear map --- where is the trace norm, i.e. the sum of the input's singular values --- the question is the following: If preserves Hermiticity (i.e. for all ) is the operator norm of attained on a Hermitian matrix, that is, is it true that Two observations: 1. is obvious so we would only have to show . 2. Because the trace norm is convex and the trace norm unit ball is the convex hull of rank-1 matrices (with ) this can be re-formulated as follows: If preserves Hermiticity , is it true that the operator norm of is attained on a rank-1 state, that is, This is true if is positive (i.e. maps positive semi-definite elements to positive semi-definite elements) as a consequence of the Russo-Dye theorem, cf. Theorem 3.39 in ""The Theory of Quantum Information"" by Watrous, 2018. However, the proof given therein breaks down if is not positive but only Hermiticity-preserving as it relies on the fact that for all (Lemma 3.3 in his book). Interestingly, this property holds for the ""stronger"" completely bounded trace norm --- sometimes called diamond norm --- which is defined as The idea here (cf. Lemma 3.50 ff.) is that if attains the operator norm of , then the norm of on is because . Using the spectral decomposition of this block matrix together with convexity of the norm yields ""the"" desired rank-1 state . So in some sense having access to ""more dimensions"" allows for a symmetrization of the problem. However, numerics suggest that this is not necessary: using that every Hermiticity-preserving map is the difference of two completely positive maps (cf. Theorem 2.25) I wrote some rough Mathematica code generating random cp maps and finding and via random search: (*Initialization*)d = 4; n = 0; c = 0; cnew = 0; nmax = 20000; A = IdentityMatrix[d]; B = IdentityMatrix[d]; A0 = IdentityMatrix[d]; v = UnitVector[d, 1]; RND = ResourceFunction[""RandomUnitVector""][d, 2]; dmax = RandomInteger[{1, d^2}]; Vs1 = Table[RandomReal[{-1, 1}, {d, d}], {dmax}]; Vs2 = Table[RandomReal[{-1, 1}, {d, d}], {dmax}]; f1[X_] := Total[ConjugateTranspose[#] . X . # & /@ Vs1]; f2[X_] := Total[ConjugateTranspose[#] . X . # & /@ Vs2]; f[X_] := f1[X] - f2[X]; (*While loop for sup of||f(uv*)||*) While[n < nmax, RND = ResourceFunction[""RandomUnitVector""][d, 2];  A = Transpose[{RND[[1]]}] . {RND[[2]]};  cnew = Total[SingularValueList[f[A]]];  If[cnew > c, B = A; c = Max[c, cnew];   If[n > nmax/50, PrintTemporary[c]]; n = 0];  n++]; Print[""non-Herm="", c]; c = 0; n = 0; (*While loop for sup of||f(uu*)||*) While[n < nmax, v = ResourceFunction[""RandomUnitVector""][d];  A = Transpose[{v}] . {v};  cnew = Total[SingularValueList[f[A]]];  If[cnew > c, B = A; c = Max[c, cnew];   If[n > nmax/50, PrintTemporary[c]]; n = 0];  n++]; Print[""Herm="", c] All the times I ran this code I did not find a violation of the conjecture, i.e. not once was the norm approximation (on ) larger than the rank-1-state value (on ). What makes this problem feel so annoying is that it seems like it should be obvious but so far I have not been able to prove this, not even for some simple classes of maps such as distances of unitary channels (e.g., for some unitary) or Hadamard product maps (e.g., for Hermitian). Problems which prevent a straightforward proof are that the usual convexity arguments do not work because the convex hull operator of Hermitian and skew-Hermitian matrices from the unit ball are a strict subset of the unit ball (consider ) preserving Hermiticity does not hand down any nice properties to for unitary; at least I don't see why it should. Therefore there is no straightforward application of some kind of Russo-Dye argument","T:(\mathbb C^{n\times n},\|\cdot\|_1)\to (\mathbb C^{m\times m},\|\cdot\|_1) \|\cdot\|_1={\rm tr}(|\,\cdot\,|) T T(A)^*=T(A^*) A\in\mathbb C^{n\times n} T 
\|T\|_{1\to 1}=\sup_{\substack{A\in\mathbb C^{n\times n}\\A\text{ Hermitian, }\|A\|_1\leq 1}}\|T(A)\|_1\,?
 \geq \leq uv^* \|u\|=\|v\|=1 T T 
\sup_{v,w\in\mathbb C^n,\|v\|=\|w\|=1}\|T(vw^*)\|_1\leq\sup_{v\in\mathbb C^n,\|v\|=1}\|T(vv^*)\|_1\,?
 T T \|\sum_j u_jP_j\|\leq\|u\|_\infty\|\sum_j P_j\| P_j\geq 0 
\|T\|_\diamond:=\max_{k\in\mathbb N}\|T\otimes{\rm id}_{\mathbb C^{k\times k}}\|_{1\to 1}\,.
 X T T\otimes{\rm id} 
\frac12\begin{pmatrix}0&X\\X^*&0\end{pmatrix}
 \|T\|_{1\to 1} T(X^*)=T(X)^* uu^* \sup_{u,v}\|T(uv^*)\|_1 \sup_u\|T(uu^*)\|_1 uv^* uu^* A\mapsto A-UAU^* U A\mapsto X\circ A X e_1e_2^* T T^*(V) V","['linear-algebra', 'matrices', 'matrix-norms', 'quantum-information']"
17,Existence of a matrix satisfying a given constraint.,Existence of a matrix satisfying a given constraint.,,"Can we say that for any ordered pair $(A,Q)$ of matrices chosen from the set of all invertible square matrices of the same size, there exists a matrix $B$ such that $BAB=Q$ ? I understand that if only real matrices are allowed, then no such $B$ exists when $\det A$ and $\det Q$ have opposite signs. I want to know what can be said about the existence of such a matrix $B$ when $A$ and $Q$ are matrices with complex entries.","Can we say that for any ordered pair of matrices chosen from the set of all invertible square matrices of the same size, there exists a matrix such that ? I understand that if only real matrices are allowed, then no such exists when and have opposite signs. I want to know what can be said about the existence of such a matrix when and are matrices with complex entries.","(A,Q) B BAB=Q B \det A \det Q B A Q","['linear-algebra', 'matrices', 'determinant']"
18,Prove that if AB-BA=B then A and B share a common eigenvector,Prove that if AB-BA=B then A and B share a common eigenvector,,"To clarify, this is a question from a contest--It's from a Chinese contest, a problem from year 2010. $A$ and $B$ are operators $V\to V$ on the complex vector space $V$ .  I've attempted the problem but seem to have come to some contradictions: First we note $$AB=BA+B$$ And hence if $v$ is an eigenvector of $A$ s.t. $Av=av$ , which exists since $A$ is complex, we have $ABv=BAv+Bv=(a+1)Bv$ , and hence $Bv$ is also an eigenvector of $A$ , with eigenvalue $a+1$ . Proceeding inductively, we have an infinite amount of eigenvalues for $A$ , which cannot be possible because each eigenvalue is a root of the characteristic polynomial, which has finite degree, hence $Bv$ has to be zero for each eigen vector of $A$ . *not too sure about this part. Then it remains to show that $A$ has nontrivial kernel,but I'm not sure if my previous arguments have been made correctly.","To clarify, this is a question from a contest--It's from a Chinese contest, a problem from year 2010. and are operators on the complex vector space .  I've attempted the problem but seem to have come to some contradictions: First we note And hence if is an eigenvector of s.t. , which exists since is complex, we have , and hence is also an eigenvector of , with eigenvalue . Proceeding inductively, we have an infinite amount of eigenvalues for , which cannot be possible because each eigenvalue is a root of the characteristic polynomial, which has finite degree, hence has to be zero for each eigen vector of . *not too sure about this part. Then it remains to show that has nontrivial kernel,but I'm not sure if my previous arguments have been made correctly.",A B V\to V V AB=BA+B v A Av=av A ABv=BAv+Bv=(a+1)Bv Bv A a+1 A Bv A A,['linear-algebra']
19,"Is the canonical map $(\prod_{i \in I} V_i)\otimes (\prod_{j \in J} W_j)\to \prod_{(i,j)\in I\times J} V_i\otimes W_j$ injective?",Is the canonical map  injective?,"(\prod_{i \in I} V_i)\otimes (\prod_{j \in J} W_j)\to \prod_{(i,j)\in I\times J} V_i\otimes W_j","Let $k$ be a field and $\{V_i\}_{i \in I}$ and $\{W_j\}_{j \in J}$ be a collection of $k$ -vector spaces. We have a canonical map $$\left(\prod_{i \in I} V_i\right)\otimes \left(\prod_{j \in J} W_j\right)\to \prod_{(i,j)\in I\times J} V_i\otimes W_j: (v_i)_{i \in I}\otimes (w_j)_{j \in J}\mapsto (v_i\otimes w_j)_{(i,j)\in I\times J}.$$ Is this map injective in general? I can prove this for the direct sum, because there I can simply use a basis, however the direct product doesn't have a nice basis so I don't know how to proceed here. I tried tricks with linear independency, linear functionals etc. Thanks in advance for any help!","Let be a field and and be a collection of -vector spaces. We have a canonical map Is this map injective in general? I can prove this for the direct sum, because there I can simply use a basis, however the direct product doesn't have a nice basis so I don't know how to proceed here. I tried tricks with linear independency, linear functionals etc. Thanks in advance for any help!","k \{V_i\}_{i \in I} \{W_j\}_{j \in J} k \left(\prod_{i \in I} V_i\right)\otimes \left(\prod_{j \in J} W_j\right)\to \prod_{(i,j)\in I\times J} V_i\otimes W_j: (v_i)_{i \in I}\otimes (w_j)_{j \in J}\mapsto (v_i\otimes w_j)_{(i,j)\in I\times J}.","['linear-algebra', 'abstract-algebra', 'tensor-products']"
20,"For $a_i \in \mathbb{C}$, does $(1+a_1^k)(1+a_2^k)\cdots (1+a_n^k)=1$ for any positive integer $k$ imply $a_1=\cdots =a_n =0$?","For , does  for any positive integer  imply ?",a_i \in \mathbb{C} (1+a_1^k)(1+a_2^k)\cdots (1+a_n^k)=1 k a_1=\cdots =a_n =0,"For $a_i \in \mathbb{C}$ , if $(1+a_1^k)(1+a_2^k)\cdots (1+a_n^k)=1$ holds for all positive integers $k$ , does it follow that $a_1=\cdots =a_n =0$ ? In fact, I want to prove $|I+A^k|=1$ for any positive integer $k$ ,where $A$ is a $n \times n$ matrix implies $A$ is nilpotent.","For , if holds for all positive integers , does it follow that ? In fact, I want to prove for any positive integer ,where is a matrix implies is nilpotent.",a_i \in \mathbb{C} (1+a_1^k)(1+a_2^k)\cdots (1+a_n^k)=1 k a_1=\cdots =a_n =0 |I+A^k|=1 k A n \times n A,"['linear-algebra', 'matrix-equations']"
21,Proving there is no squared $n\times n$ (n odd) real matrix yielding minus the identity matrix,Proving there is no squared  (n odd) real matrix yielding minus the identity matrix,n\times n,"Prove or give a counterexample There is no $A \in \Bbb R^{3 \times 3}$ such that $A^2 = -\Bbb I_3$ Here's my attempt I suspect that the statement is true. To prove it, I used contradiction Assuming $A^2 = -\Bbb I_3$ holds, we take the determinant on both sides to get $$ \det A^2= (\det A)^2= (-1)^3 \det(\Bbb I_3) \Rightarrow \det A = \sqrt{-1}$$ So the determinant of $A$ is ill-defined over the real numbers, which is a contradiction (as the determinant of square matrices is well-defined over the real numbers). Do you all agree?","Prove or give a counterexample There is no such that Here's my attempt I suspect that the statement is true. To prove it, I used contradiction Assuming holds, we take the determinant on both sides to get So the determinant of is ill-defined over the real numbers, which is a contradiction (as the determinant of square matrices is well-defined over the real numbers). Do you all agree?",A \in \Bbb R^{3 \times 3} A^2 = -\Bbb I_3 A^2 = -\Bbb I_3  \det A^2= (\det A)^2= (-1)^3 \det(\Bbb I_3) \Rightarrow \det A = \sqrt{-1} A,['linear-algebra']
22,What is the advantage of LU decomposition over storing the elimination matrix directly?,What is the advantage of LU decomposition over storing the elimination matrix directly?,,"The answers to this question indicate that one of the advantages of LU decomposition is that the algorithm do not redo the elimination steps. Why do we insist on storing the factorized form of $L$ and $U$ ?  Why not just save $E$ and $U$ , where \begin{align} Ax = b &\implies EAx = Eb && \text{(multiply both sides by $E$)} \\ &\implies Ux = Eb && \text{(since $EA = U$)} \end{align} As you can see, we just need to store $U$ and the elimination matrix $E$ , so whenever we have a new matrix $b$ on the right-hand side, we can just multiply it out with $E$ , and use back-subsitution to solve for $x$ . What's advantage of storing $L$ and $U$ over storing $E$ and $U$ ?","The answers to this question indicate that one of the advantages of LU decomposition is that the algorithm do not redo the elimination steps. Why do we insist on storing the factorized form of and ?  Why not just save and , where As you can see, we just need to store and the elimination matrix , so whenever we have a new matrix on the right-hand side, we can just multiply it out with , and use back-subsitution to solve for . What's advantage of storing and over storing and ?","L U E U \begin{align}
Ax = b
&\implies EAx = Eb && \text{(multiply both sides by E)} \\
&\implies Ux = Eb && \text{(since EA = U)}
\end{align} U E b E x L U E U","['linear-algebra', 'matrices', 'matrix-decomposition']"
23,"Solution verification: Showing that $\|A\|_\infty = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|$ for $A \in \Bbb R^{n\times n}$",Solution verification: Showing that  for,"\|A\|_\infty = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| A \in \Bbb R^{n\times n}","$ \newcommand{\norm}[1]{\| #1 \|} \newcommand{\inorm}[1]{\norm{#1}_\infty} \newcommand{\abs}[1]{\left| #1 \right|} \newcommand{\para}[1]{\left( #1 \right)} \newcommand{\R}{\mathbb{R}} $ Context: This ultimately ties back to a homework assignment. (Specifically it's from Fundamentals of Matrix Calculations by Watkins, Exercise $2.1.30$ from $\S2.1$ but that's not really a huge deal, seems like a fairly common exercise.) The goal is to show that, knowing matrix $p$ -norms are induced by the corresponding vector norms, $\forall A \in \R^{n\times n}$ , $$\|A\|_\infty = \max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|$$ I have an approach and was just curious as to how valid it is; I just don't feel very sure of myself. I feel like I made some sort of small-yet-critical mistake somewhere, but I can't figure out where... My Attempt: Ultimately, we will show that $$\max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \le \|A\|_\infty \le \max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|$$ which will let me conclude with the desired equality ( $a \le b \le a \implies a=b$ ). From the definition of the $\infty$ -norm, with $A := (a_{i,j})_{1 \le i,j \le n} \in \R^{n \times n}$ and $x := (x_i)_{1 \le i \le n} \in \R^n$ , $$ \inorm{Ax} = \max_{1 \le i \le n} \abs{ \sum_{k=1}^n a_{i,k} x_k } $$ Applying the triangle inequality yields $$ \inorm{Ax} \le \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k} x_k|  = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \cdot | x_k| $$ Note that $$ |x_k| \le \max_{1 \le j \le n} |x_j| =: \norm{x}_\infty $$ and thus $$ \inorm{Ax} \le  \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \cdot \norm{x}_\infty  $$ $\norm{x}_\infty$ is independent of $i$ , so we factor it out and conclude $$ \inorm{Ax} \le \norm{x}_\infty \para{ \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| } $$ Thus, $$ \inorm{A} = \max_{x \ne \vec 0} \frac{\inorm{Ax}}{\inorm{x}} \le \frac{\displaystyle \norm{x}_\infty \para{ \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| }}{\norm{x}_\infty} = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| $$ Thus, with $\inorm{A}$ less than or equal to the desired expression, we need to find an $\hat x$ such that equality is achieved. Suppose in row $r$ the maximum is achieved; for every $k$ , let $$ \hat x_k = \begin{cases} +1 & \text{if } a_{r,k} \ge 0 \\ -1 & \text{if } a_{r,k} < 0 \end{cases} = \mathrm{sign}(a_{r,k}) $$ Define $\hat x := (\hat x_i)_{1 \le i \le n}$ as defined above; clearly $\inorm{\hat x}=1$ . This ensures $a_{r,k}\hat x_k = |a_{r,k}|$ . Then \begin{align*} \norm{A}_\infty &=  \max_{x \ne \vec 0} \frac{\inorm{Ax}}{\inorm{x}}  \tag{def. of matrix norm} \\ &\ge \frac{\inorm{A \hat x}}{\inorm{\hat x}} \tag{def. of maximum}\\ &= \inorm{A \hat x} \tag{$\inorm{\hat x} = 1$} \\ &= \max_{1 \le i \le n} \abs{ \sum_{k=1}^n a_{i,k} \hat x_k } \tag{definition}\\ &=  \abs{ \sum_{k=1}^n a_{r,k} \hat x_k } \tag{definition of $r$}\\ &=  \abs{ \sum_{k=1}^n |a_{r,k}| } \tag{choice of $\hat x_k$, $r$}\\ &=   \sum_{k=1}^n |a_{r,k}|  \tag{$|z| \ge 0 \implies \sum_i |z_i| \ge 0$}\\ &= \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \tag{def. of $r$} \end{align*} Thus what we have seen is that $$ \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \le \norm{A}_\infty \le \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| $$ thus letting us conclude equality: $$ \norm{A}_\infty = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| $$","Context: This ultimately ties back to a homework assignment. (Specifically it's from Fundamentals of Matrix Calculations by Watkins, Exercise from but that's not really a huge deal, seems like a fairly common exercise.) The goal is to show that, knowing matrix -norms are induced by the corresponding vector norms, , I have an approach and was just curious as to how valid it is; I just don't feel very sure of myself. I feel like I made some sort of small-yet-critical mistake somewhere, but I can't figure out where... My Attempt: Ultimately, we will show that which will let me conclude with the desired equality ( ). From the definition of the -norm, with and , Applying the triangle inequality yields Note that and thus is independent of , so we factor it out and conclude Thus, Thus, with less than or equal to the desired expression, we need to find an such that equality is achieved. Suppose in row the maximum is achieved; for every , let Define as defined above; clearly . This ensures . Then Thus what we have seen is that thus letting us conclude equality:","
\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\inorm}[1]{\norm{#1}_\infty}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\para}[1]{\left( #1 \right)}
\newcommand{\R}{\mathbb{R}}
 2.1.30 \S2.1 p \forall A \in \R^{n\times n} \|A\|_\infty = \max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \le \|A\|_\infty \le \max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| a \le b \le a \implies a=b \infty A := (a_{i,j})_{1 \le i,j \le n} \in \R^{n \times n} x := (x_i)_{1 \le i \le n} \in \R^n 
\inorm{Ax} = \max_{1 \le i \le n} \abs{ \sum_{k=1}^n a_{i,k} x_k }
 
\inorm{Ax} \le \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k} x_k|  = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \cdot | x_k|
 
|x_k| \le \max_{1 \le j \le n} |x_j| =: \norm{x}_\infty
 
\inorm{Ax} \le  \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \cdot \norm{x}_\infty 
 \norm{x}_\infty i 
\inorm{Ax} \le \norm{x}_\infty \para{ \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| }
 
\inorm{A} = \max_{x \ne \vec 0} \frac{\inorm{Ax}}{\inorm{x}} \le \frac{\displaystyle \norm{x}_\infty \para{ \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| }}{\norm{x}_\infty} = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|
 \inorm{A} \hat x r k 
\hat x_k = \begin{cases}
+1 & \text{if } a_{r,k} \ge 0 \\
-1 & \text{if } a_{r,k} < 0
\end{cases} = \mathrm{sign}(a_{r,k})
 \hat x := (\hat x_i)_{1 \le i \le n} \inorm{\hat x}=1 a_{r,k}\hat x_k = |a_{r,k}| \begin{align*}
\norm{A}_\infty
&=  \max_{x \ne \vec 0} \frac{\inorm{Ax}}{\inorm{x}}  \tag{def. of matrix norm} \\
&\ge \frac{\inorm{A \hat x}}{\inorm{\hat x}} \tag{def. of maximum}\\
&= \inorm{A \hat x} \tag{\inorm{\hat x} = 1} \\
&= \max_{1 \le i \le n} \abs{ \sum_{k=1}^n a_{i,k} \hat x_k } \tag{definition}\\
&=  \abs{ \sum_{k=1}^n a_{r,k} \hat x_k } \tag{definition of r}\\
&=  \abs{ \sum_{k=1}^n |a_{r,k}| } \tag{choice of \hat x_k, r}\\
&=   \sum_{k=1}^n |a_{r,k}|  \tag{|z| \ge 0 \implies \sum_i |z_i| \ge 0}\\
&= \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \tag{def. of r}
\end{align*} 
\max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \le \norm{A}_\infty \le \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|
 
\norm{A}_\infty = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|
","['linear-algebra', 'matrices', 'solution-verification', 'normed-spaces']"
24,"Is ""lines remain lines"" sufficient to characterise linearity in $\mathbb R^2$?","Is ""lines remain lines"" sufficient to characterise linearity in ?",\mathbb R^2,"Note : There are other questions on this site similar to this one, but they are either unanswered or have a different definition of ""lines  remain lines"". 3Blue1Brown uses the following intuitive definition of linearity in his video series on linear algebra: A linear transformation is a transformation which fixes the origin and keeps lines straight. I mainly care about this as an intuition for the plane, so I do not really want to generalise it to $\mathbb R^n$ . So my question is, if $f\colon\mathbb R^2\to\mathbb R^2$ is a map such that $f(\boldsymbol 0) = \boldsymbol 0$ , and for all $\boldsymbol a,\boldsymbol b\in\mathbb R^2$ , there exist $\boldsymbol u, \boldsymbol v\in\mathbb R^2$ such that $f(\boldsymbol a+\mathbb R\boldsymbol b) = \boldsymbol u+\mathbb R\boldsymbol v$ , can I show that $f$ is a linear map on $\mathbb R^2$ ? (here $\boldsymbol a+\mathbb R\boldsymbol b$ denotes the obvious coset $\{\boldsymbol a+t\boldsymbol b:t\in\mathbb R\}$ ). I've been playing around with these two properties a lot, but 2 doesn't seem to be strong enough to allow me to go from statements about lines to statements about individual vectors in an obvious way.","Note : There are other questions on this site similar to this one, but they are either unanswered or have a different definition of ""lines  remain lines"". 3Blue1Brown uses the following intuitive definition of linearity in his video series on linear algebra: A linear transformation is a transformation which fixes the origin and keeps lines straight. I mainly care about this as an intuition for the plane, so I do not really want to generalise it to . So my question is, if is a map such that , and for all , there exist such that , can I show that is a linear map on ? (here denotes the obvious coset ). I've been playing around with these two properties a lot, but 2 doesn't seem to be strong enough to allow me to go from statements about lines to statements about individual vectors in an obvious way.","\mathbb R^n f\colon\mathbb R^2\to\mathbb R^2 f(\boldsymbol 0) = \boldsymbol 0 \boldsymbol a,\boldsymbol b\in\mathbb R^2 \boldsymbol u, \boldsymbol v\in\mathbb R^2 f(\boldsymbol a+\mathbb R\boldsymbol b) = \boldsymbol u+\mathbb R\boldsymbol v f \mathbb R^2 \boldsymbol a+\mathbb R\boldsymbol b \{\boldsymbol a+t\boldsymbol b:t\in\mathbb R\}","['linear-algebra', 'vector-spaces', 'linear-transformations']"
25,"Let $S=\{AB-BA| A,B \in M_n(K)\}$ where $K$ is a field. Prove that $S$ is closed under matrix addition.",Let  where  is a field. Prove that  is closed under matrix addition.,"S=\{AB-BA| A,B \in M_n(K)\} K S","I know there is a result that $S=$ collection of all trace $0$ matrices, and that collection forms a vector space. But I want to prove it independently i.e. For any $A,B,C,D\in M_n(K)$ we have to find $E,F\in M_n(K)$ such that $(AB-BA)+(CD-DC)=EF-FE$ . But I don't know how to solve this. But the thing I can observe that the above equation gives rise to $n^2$ equation (equating each entries of matrices of both the sides) with $2n^2$ variables (Total number of entries of $E,F$ is $2n^2$ ).Can this problem be simplified if we choose special kind of $E$ say diagonal matrix. Edit-(This is valid only for $\Bbb{R}$ or $\Bbb{C}$ ) $AB-BA=(A+aI)(B+bI)-(B+bI)(A+aI)$ for all $a,b\in\Bbb{R}$ . And there is $a,b$ in $\Bbb{R}$ such that $A+aI,B+bI$ are invertible. Hence, we can assume $A,B$ to be invertible matrices i.e. $S=\{AB-BA|A,B\in GL_n(K)\}$","I know there is a result that collection of all trace matrices, and that collection forms a vector space. But I want to prove it independently i.e. For any we have to find such that . But I don't know how to solve this. But the thing I can observe that the above equation gives rise to equation (equating each entries of matrices of both the sides) with variables (Total number of entries of is ).Can this problem be simplified if we choose special kind of say diagonal matrix. Edit-(This is valid only for or ) for all . And there is in such that are invertible. Hence, we can assume to be invertible matrices i.e.","S= 0 A,B,C,D\in M_n(K) E,F\in M_n(K) (AB-BA)+(CD-DC)=EF-FE n^2 2n^2 E,F 2n^2 E \Bbb{R} \Bbb{C} AB-BA=(A+aI)(B+bI)-(B+bI)(A+aI) a,b\in\Bbb{R} a,b \Bbb{R} A+aI,B+bI A,B S=\{AB-BA|A,B\in GL_n(K)\}","['linear-algebra', 'matrices', 'trace']"
26,In $\mathbb{R}$ find the solutions of the equation $x^4-x^3-18x^2+3x+9=0$,In  find the solutions of the equation,\mathbb{R} x^4-x^3-18x^2+3x+9=0,"I couldn't solve this question and hence looked at the solution which goes as follows: $0$ is no a root of the equation. Hence: $$x^2-x-18+\frac{3}{x}+\frac{9}{x^2}=0. \text{ So,  }  x^2+\frac{9}{x^2}-(x-\frac{3}{x})-18=0$$ I state that $y=x-\frac{3}{x}$ , so we have that $y^2=x^2+\frac{9}{x^2}-6$ , in other words $x^2+\frac{9}{x^2}=y^2+6$ . Hence the equation is written as $y^2+6-y-18=0$ so $y^2-y-12=0$ , hence $y=4$ or $y=-3$ so $x=2\pm\sqrt{7}$ , or $x=\frac{-3\pm\sqrt{21}}{2}$ Could you please explain to me why the solution author of the solution thought of originally dividing by the equation by x and after that substituting $x-\frac{3}{x}$ with $y$ ? Also if you can think of a more intuitive approach could you please show it?","I couldn't solve this question and hence looked at the solution which goes as follows: is no a root of the equation. Hence: I state that , so we have that , in other words . Hence the equation is written as so , hence or so , or Could you please explain to me why the solution author of the solution thought of originally dividing by the equation by x and after that substituting with ? Also if you can think of a more intuitive approach could you please show it?","0 x^2-x-18+\frac{3}{x}+\frac{9}{x^2}=0. \text{ So,  } 
x^2+\frac{9}{x^2}-(x-\frac{3}{x})-18=0 y=x-\frac{3}{x} y^2=x^2+\frac{9}{x^2}-6 x^2+\frac{9}{x^2}=y^2+6 y^2+6-y-18=0 y^2-y-12=0 y=4 y=-3 x=2\pm\sqrt{7} x=\frac{-3\pm\sqrt{21}}{2} x-\frac{3}{x} y","['linear-algebra', 'contest-math']"
27,Is this type of matrix always positive semidefinite?,Is this type of matrix always positive semidefinite?,,"Let $b\in[0,1]^n$ , with $n\in\mathcal{N}^*$ . Consider the symmetric matrix $A=(a_{i,j})$ , defined by $$ a_{i,j}=\begin{cases}\min\lbrace b_i,...,b_j\rbrace &\mbox{if } i<j\\ \min \lbrace b_j,...,b_i\rbrace &\mbox{if } j\leq i\end{cases} $$ Is $A$ always positive semidefinite? For instance, the matrix generated by the vector $[0. 1, 0.2, 0.3]$ is $$     \left(\begin{matrix}     0.1 & 0.1 & 0.1 \\     0.1 & 0.2 & 0.2 \\     0.1 & 0.2 & 0.3 \\     \end{matrix}\right) $$ while the one generated by the vector $[0. 2, 0.1, 0.3]$ is $$     \left(\begin{matrix}     0.2 & 0.1 & 0.1 \\     0.1 & 0.1 & 0.1 \\     0.1 & 0.1 & 0.3 \\     \end{matrix}\right)\text{.}$$ So far, I have only managed to show that $A$ is positive semidefinite when $b$ is sorted (either in  increasing or decreasing order). The proof is by induction: The statement clearly holds if $n=1$ . Induction step: Let's denote $A(b)$ the matrix generated when following the procedure described above with a vector $b$ . Let $n\in\mathcal{N}^*$ .  Assume that all the matrices $M\in\lbrace A(b), b\in [0,1]^{i}, i\in\lbrace 1,...n\rbrace, b \mbox{ sorted}\rbrace$ are positive semidefinite. Consider then a sorted vector $b$ with $n+1$ elements and apply Sylvester's criterion: all the sub-matrices of $A(b)$ corresponding to principal minors belong to $\lbrace A(b), b\in [0,1]^{i}, i\in\lbrace 1,...n\rbrace, b \mbox{ sorted}\rbrace$ , so the result holds. I have also tried to generate counter-examples in the case where $b$ is not sorted, but unsuccessfully.","Let , with . Consider the symmetric matrix , defined by Is always positive semidefinite? For instance, the matrix generated by the vector is while the one generated by the vector is So far, I have only managed to show that is positive semidefinite when is sorted (either in  increasing or decreasing order). The proof is by induction: The statement clearly holds if . Induction step: Let's denote the matrix generated when following the procedure described above with a vector . Let .  Assume that all the matrices are positive semidefinite. Consider then a sorted vector with elements and apply Sylvester's criterion: all the sub-matrices of corresponding to principal minors belong to , so the result holds. I have also tried to generate counter-examples in the case where is not sorted, but unsuccessfully.","b\in[0,1]^n n\in\mathcal{N}^* A=(a_{i,j}) 
a_{i,j}=\begin{cases}\min\lbrace b_i,...,b_j\rbrace &\mbox{if } i<j\\
\min \lbrace b_j,...,b_i\rbrace &\mbox{if } j\leq i\end{cases}
 A [0. 1, 0.2, 0.3] 
    \left(\begin{matrix}
    0.1 & 0.1 & 0.1 \\
    0.1 & 0.2 & 0.2 \\
    0.1 & 0.2 & 0.3 \\
    \end{matrix}\right)
 [0. 2, 0.1, 0.3] 
    \left(\begin{matrix}
    0.2 & 0.1 & 0.1 \\
    0.1 & 0.1 & 0.1 \\
    0.1 & 0.1 & 0.3 \\
    \end{matrix}\right)\text{.} A b n=1 A(b) b n\in\mathcal{N}^* M\in\lbrace A(b), b\in [0,1]^{i}, i\in\lbrace 1,...n\rbrace, b \mbox{ sorted}\rbrace b n+1 A(b) \lbrace A(b), b\in [0,1]^{i}, i\in\lbrace 1,...n\rbrace, b \mbox{ sorted}\rbrace b","['linear-algebra', 'matrices', 'positive-semidefinite']"
28,Why are eigenvectors important for Deep Learning applications?,Why are eigenvectors important for Deep Learning applications?,,"I know it is quite of a trite question to ask about the importance of eigenvectors, but I do not understand how they can be relevant for Deep Learning and when we can use them. Any reference to the literature as well is of course appreciated.","I know it is quite of a trite question to ask about the importance of eigenvectors, but I do not understand how they can be relevant for Deep Learning and when we can use them. Any reference to the literature as well is of course appreciated.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'machine-learning', 'neural-networks']"
29,Axiom of choice and dual of a tensor product,Axiom of choice and dual of a tensor product,,"EDIT : This question (and other related questions) was also asked on mathoverflow : here . Let $V$ , and $W$ be vector spaces. By the universal property of the tensor product,  there is a canonical map from $V^*\otimes W^*$ into $(V\otimes W)^*$ (since the map $(\omega_1,\omega_2)\mapsto \omega_1\otimes\omega_2$ is bilinear from $V^*\times W^*$ into $(V\otimes W)^*$ . I have read that this map is actually injective by using some basis on $V$ and $W$ . Since the existence of basis for any arbitrary vector space relies on the axiom of choice, my question is : is the axiom of choice necessary to prove the injectivity of the canonical map $V^*\otimes W^*\to(V\otimes W)^*$ ?","EDIT : This question (and other related questions) was also asked on mathoverflow : here . Let , and be vector spaces. By the universal property of the tensor product,  there is a canonical map from into (since the map is bilinear from into . I have read that this map is actually injective by using some basis on and . Since the existence of basis for any arbitrary vector space relies on the axiom of choice, my question is : is the axiom of choice necessary to prove the injectivity of the canonical map ?","V W V^*\otimes W^* (V\otimes W)^* (\omega_1,\omega_2)\mapsto \omega_1\otimes\omega_2 V^*\times W^* (V\otimes W)^* V W V^*\otimes W^*\to(V\otimes W)^*","['linear-algebra', 'tensor-products', 'axiom-of-choice']"
30,Does the action of a linear map on $k$-dimensional subspaces determine it up to scaling?,Does the action of a linear map on -dimensional subspaces determine it up to scaling?,k,"Let $V$ be a real $d$ -dimensional vector space, and let $1 \le k \le d-1$ be a fixed integer. Let $A,B \in \text{Hom}(V,V)$ , and suppose that $AW=BW$ for every $k$ -dimensional subspace $W \le V$ . Is it true that $A=\lambda B$ for some $\lambda \in \mathbb R$ ? If not, can we characterize all such pairs $A,B$ ? Here are some partial results (proofs at the end): First, the answer is clearly positive for $k=1$ . Lemma 1: If at least one of $A$ and $B$ is invertible, then the answer is positive. Lemma 2: We always have $\text{Image}(A)=\text{Image}(B)$ . In particular, $\text{rank}(A)=\text{rank}(B)=r$ . Lemma 3: If $r \ge k$ or $r \le d-k$ , then $\ker(A)=\ker(B) $ . In particular, the above lemmas imply that if $r>k$ , then the answer is positive. Indeed, in that case, the kernels and images coincide, so we  can consider the quotient operators: $\tilde A,\tilde B:V/D \to H$ , where $D$ is the kernel, and $H$ is the image. Now $\tilde A, \tilde B$ are invertible operators between $r$ -dimensional spaces, and they satisfy the assumption for $k<r$ . Thus, by lemma 1, $\tilde A=\lambda \tilde B$ , which implies $ A=\lambda B$ . Edit: Here is a slick proof that the answer is positive in general: Let $v\in V$ and let $X(v)$ be the collection of $k$ -dimensional subspaces of $V$ that contain $v$ . Then $$\text{span} \{v\}=\bigcap_{W\in X(v)}W,$$ so $$A(\text{span} \{v\})=A(\bigcap_{W\in X(v)}W) \subseteq \bigcap_{W\in X(v)}AW=\bigcap_{W\in X(v)}BW \subseteq B(\text{span} \{v\}),$$ where the last containment follows from this answer .  This reduces the problem to the case where $k=1$ . Proof of Lemma 1: Suppose that $A$ is invertible. Then, we have $SW=W$ , where $S=A^{-1}B$ . Thus, every $k$ -dimensional subspace is $S$ -invariant, which implies $S$ is a multiple of the identity. Proof of Lemma 2: $\text{Image}(A)=\text{Image}(B)$ . Let $x=Av_1 \in \text{Image}(A)$ ; complete $v_1$ to a linearly independent set $v_1,\dots,v_k$ .  Then $$ x \in A(\text{span}\{v_1,\dots,v_k\})=B(\text{span}\{v_1,\dots,v_k\})\subseteq \text{Image}(B),$$ so $\text{Image}(A) \subseteq \text{Image}(B)$ . The other direction follows by symmetry. Proof of Lemma 3: If $r \ge k$ or $r \le d-k$ , then $\ker(A)=\ker(B) $ . First, suppose that $r \ge k$ , and let $v_1 \notin \ker A$ . Complete $v_1$ into a linearly independent set $v_1,\dots,v_k$ such that $A(\text{span}\{v_1,\dots,v_k\})$ is $k$ -dimensional. Then $B(\text{span}\{v_1,\dots,v_k\})$ is $k$ -dimensional, so $Bv_1 \neq 0$ . This shows $\ker(A)^c \subseteq \ker(B)^c$ , i.e. $\ker(B)\subseteq \ker(A)$ . The other direction follows by symmetry. Now, suppose that $r \le d-k$ . Then, since the nullity is $\ge k$ , every $v_1 \in \ker B$ can be completed into a linearly independent set $v_1,\dots,v_k$ , all in $\ker B$ . This implies that $A(\text{span}\{v_1,\dots,v_k\})=0$ , so $v_1 \in \ker A$ .","Let be a real -dimensional vector space, and let be a fixed integer. Let , and suppose that for every -dimensional subspace . Is it true that for some ? If not, can we characterize all such pairs ? Here are some partial results (proofs at the end): First, the answer is clearly positive for . Lemma 1: If at least one of and is invertible, then the answer is positive. Lemma 2: We always have . In particular, . Lemma 3: If or , then . In particular, the above lemmas imply that if , then the answer is positive. Indeed, in that case, the kernels and images coincide, so we  can consider the quotient operators: , where is the kernel, and is the image. Now are invertible operators between -dimensional spaces, and they satisfy the assumption for . Thus, by lemma 1, , which implies . Edit: Here is a slick proof that the answer is positive in general: Let and let be the collection of -dimensional subspaces of that contain . Then so where the last containment follows from this answer .  This reduces the problem to the case where . Proof of Lemma 1: Suppose that is invertible. Then, we have , where . Thus, every -dimensional subspace is -invariant, which implies is a multiple of the identity. Proof of Lemma 2: . Let ; complete to a linearly independent set .  Then so . The other direction follows by symmetry. Proof of Lemma 3: If or , then . First, suppose that , and let . Complete into a linearly independent set such that is -dimensional. Then is -dimensional, so . This shows , i.e. . The other direction follows by symmetry. Now, suppose that . Then, since the nullity is , every can be completed into a linearly independent set , all in . This implies that , so .","V d 1 \le k \le d-1 A,B \in \text{Hom}(V,V) AW=BW k W \le V A=\lambda B \lambda \in \mathbb R A,B k=1 A B \text{Image}(A)=\text{Image}(B) \text{rank}(A)=\text{rank}(B)=r r \ge k r \le d-k \ker(A)=\ker(B)  r>k \tilde A,\tilde B:V/D \to H D H \tilde A, \tilde B r k<r \tilde A=\lambda \tilde B  A=\lambda B v\in V X(v) k V v \text{span} \{v\}=\bigcap_{W\in X(v)}W, A(\text{span} \{v\})=A(\bigcap_{W\in X(v)}W) \subseteq \bigcap_{W\in X(v)}AW=\bigcap_{W\in X(v)}BW \subseteq B(\text{span} \{v\}), k=1 A SW=W S=A^{-1}B k S S \text{Image}(A)=\text{Image}(B) x=Av_1 \in \text{Image}(A) v_1 v_1,\dots,v_k  x \in A(\text{span}\{v_1,\dots,v_k\})=B(\text{span}\{v_1,\dots,v_k\})\subseteq \text{Image}(B), \text{Image}(A) \subseteq \text{Image}(B) r \ge k r \le d-k \ker(A)=\ker(B)  r \ge k v_1 \notin \ker A v_1 v_1,\dots,v_k A(\text{span}\{v_1,\dots,v_k\}) k B(\text{span}\{v_1,\dots,v_k\}) k Bv_1 \neq 0 \ker(A)^c \subseteq \ker(B)^c \ker(B)\subseteq \ker(A) r \le d-k \ge k v_1 \in \ker B v_1,\dots,v_k \ker B A(\text{span}\{v_1,\dots,v_k\})=0 v_1 \in \ker A","['linear-algebra', 'linear-transformations', 'symmetry', 'grassmannian', 'invariant-subspace']"
31,Why are the axis of an ellipsoid eigenvectors?,Why are the axis of an ellipsoid eigenvectors?,,"Consider an ellipsoid $\{x| x^TAx = 1\}$ . Let $A$ be a real symmetric matrix, and consider the eigen-decomposition $A=P\Lambda P^{-1} =P\Lambda P^T$ ,  where the matrix $P$ is orthogonal because $A$ is symmetric, and the diagonal matrix $\Lambda$ contains the eigenvalues of $A$ . Then the ellipsoid can be rewritten as: $$x^TP\Lambda P^Tx = (P^Tx)^T\Lambda(P^Tx) = y^T\Lambda y = 1$$ Since the matrix is diagonal, the quadratic form gives: $$y^T\Lambda y =\lambda_1y_1^2+...+\lambda_ny_n^2 = 1$$ This is clearly the equation of ellipsoid. My question is, why are the axes of this ellipsoid the eigenvectors of $A$ ?","Consider an ellipsoid . Let be a real symmetric matrix, and consider the eigen-decomposition ,  where the matrix is orthogonal because is symmetric, and the diagonal matrix contains the eigenvalues of . Then the ellipsoid can be rewritten as: Since the matrix is diagonal, the quadratic form gives: This is clearly the equation of ellipsoid. My question is, why are the axes of this ellipsoid the eigenvectors of ?",\{x| x^TAx = 1\} A A=P\Lambda P^{-1} =P\Lambda P^T P A \Lambda A x^TP\Lambda P^Tx = (P^Tx)^T\Lambda(P^Tx) = y^T\Lambda y = 1 y^T\Lambda y =\lambda_1y_1^2+...+\lambda_ny_n^2 = 1 A,"['linear-algebra', 'algebra-precalculus', 'geometry', 'eigenvalues-eigenvectors', 'conic-sections']"
32,Show that the $\text{Tr}(A)^2 = \text{Tr}(A^2)+\text{Sum of Eigenvalues} $,Show that the,\text{Tr}(A)^2 = \text{Tr}(A^2)+\text{Sum of Eigenvalues} ,"Let $A$ be a square $ m \times m $ with eigenvalues $\lambda_{i},...,\lambda_{m}$ . Show that: $$ [\text{Tr}(A)]^{2} =\text{Tr}(A^{2}) + \sum_{i \neq j} \lambda_{i}\lambda_{j} $$ Here is my attempt: LHS $$ [\text{Tr}(A)]^{2} = \sum_{i =1}^{m} \lambda_{i}\sum_{j =1}^{m}\lambda_{j} = \sum_{i =1}^{m}\sum_{j =1}^{m} \lambda_{i}\lambda_{j} $$ RHS It can be shown that $\lambda_{i}^{2}$ is an eigenvalue of $A^{2}$ so: $$ \text{Tr}(A^{2}) + \sum_{i \neq j} \lambda_{i}\lambda_{j} = \sum_{i =1}^{m} \lambda_{i}^{2} + \sum_{i \neq j} \lambda_{i}\lambda_{j} $$ $$ = \sum_{i =1}^{m}\sum_{j =1}^{m} \lambda_{i}\lambda_{j} $$ Does this make sense?",Let be a square with eigenvalues . Show that: Here is my attempt: LHS RHS It can be shown that is an eigenvalue of so: Does this make sense?,"A  m \times m  \lambda_{i},...,\lambda_{m} 
[\text{Tr}(A)]^{2} =\text{Tr}(A^{2}) + \sum_{i \neq j} \lambda_{i}\lambda_{j}
 
[\text{Tr}(A)]^{2} = \sum_{i =1}^{m} \lambda_{i}\sum_{j =1}^{m}\lambda_{j} = \sum_{i =1}^{m}\sum_{j =1}^{m} \lambda_{i}\lambda_{j}
 \lambda_{i}^{2} A^{2} 
\text{Tr}(A^{2}) + \sum_{i \neq j} \lambda_{i}\lambda_{j} = \sum_{i =1}^{m} \lambda_{i}^{2} + \sum_{i \neq j} \lambda_{i}\lambda_{j}
 
= \sum_{i =1}^{m}\sum_{j =1}^{m} \lambda_{i}\lambda_{j}
","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace']"
33,Which vector spaces are algebraic dual spaces?,Which vector spaces are algebraic dual spaces?,,"Let us say that a vector space $V$ is an algebraic dual space if there exist a vector space $U$ such that $V$ is isomorphic to $U^*$ , the vector space of all linear maps from $U$ to the corresponding field of scalars. It is known that if $V$ is finite-dimensional then $V$ and $V^*$ are isomorphic, hence $V$ is an algebraic dual space. On the other side, it is also known that the dimension of an algebraic dual space cannot be countably infinite, hence not all vector spaces are algebraic dual spaces. Question: Is there any characterization of vector spaces that are algebraic dual spaces? I am mostly interested in the case of vector spaces over reals.","Let us say that a vector space is an algebraic dual space if there exist a vector space such that is isomorphic to , the vector space of all linear maps from to the corresponding field of scalars. It is known that if is finite-dimensional then and are isomorphic, hence is an algebraic dual space. On the other side, it is also known that the dimension of an algebraic dual space cannot be countably infinite, hence not all vector spaces are algebraic dual spaces. Question: Is there any characterization of vector spaces that are algebraic dual spaces? I am mostly interested in the case of vector spaces over reals.",V U V U^* U V V V^* V,"['linear-algebra', 'vector-spaces', 'dual-spaces']"
34,Find eigenvectors of the $(n+1) \times (n+1)$-matrix,Find eigenvectors of the -matrix,(n+1) \times (n+1),"Find eigenvectors of the $(n+1) \times (n+1)$ -matrix: $$\left(\begin {array}{cccccccc} 0&0&0&0&0&0&-1&0\\ 0&0&0&0&0&-2&0&n\\ 0&0&0&0&-3&0&n-1&0 \\ 0&0&0&\ldots&0&n-2&0&0\\ 0&0&-(n-2)&0&\ldots&0 &0&0\\ 0&-(n-1)&0&3&0&0&0&0\\ -n&0&2&0 &0&0&0&0\\ 0&1&0&0&0&0&0&0\end {array}  \right)$$ The eigenvalues of the matrix are $ n, n-2, n-4, \ldots, -(n-2), -n$ see the question Let $e_{\lambda}$ denotes the eigenvector corresponding to the eigenvalues $\lambda$ . So far I have found $$ e_0=[a_0, a_1, \ldots, a_n], a_i=\begin{cases} 0, \text{ $i$ odd}\\ \displaystyle\binom{\frac{n}{2}}{ \frac{i}{2}}, \text{$i$ even} \end{cases} $$ For example for $n=8$ we have $e_0=[1, 0, 4, 0, 6, 0, 4, 0, 1]$ . Also $$ e_1=[a_0, a_2, \ldots, a_n], a_i=\begin{cases} 0, \text{ $i$ even}\\ \displaystyle\binom{\frac{n-1}{2}}{ \frac{i-1}{2}}, \text{$i$ odd} \end{cases}. $$ for example for $n=7$ we have $e_1=[0, 1, 0, 3, 0, 3, 0, 1]$ ,  and $$ e_2=[a_0, a_2, \ldots, a_n], a_i=\begin{cases} \displaystyle         \binom{\frac{n}{2}-1}{\frac{i}{2}-1}-\binom{\frac{n}{2}-1}{\frac{i}{2}},  i\text{ even} \\ \\       \displaystyle 2(-1)^{i-1}\binom{\frac{n}{2}-1}{\frac{i-1}{2}} , i\text{  odd}    \end{cases}. $$ for example for $n=8$ we have $e_2=[-1, 2, -2, 6, 0, 6, 2, 2, 1]$ . Question: What is  an eigenvector $e_{\lambda}$ (with integer coordinates) for arbitrary $\lambda \in \{ n, n-2, n-4, \ldots, -(n-2), -n\}$ ? I hope the problem was  already solved in 19th century.","Find eigenvectors of the -matrix: The eigenvalues of the matrix are see the question Let denotes the eigenvector corresponding to the eigenvalues . So far I have found For example for we have . Also for example for we have ,  and for example for we have . Question: What is  an eigenvector (with integer coordinates) for arbitrary ? I hope the problem was  already solved in 19th century.","(n+1) \times (n+1) \left(\begin {array}{cccccccc} 0&0&0&0&0&0&-1&0\\ 0&0&0&0&0&-2&0&n\\ 0&0&0&0&-3&0&n-1&0
\\ 0&0&0&\ldots&0&n-2&0&0\\ 0&0&-(n-2)&0&\ldots&0
&0&0\\ 0&-(n-1)&0&3&0&0&0&0\\ -n&0&2&0
&0&0&0&0\\ 0&1&0&0&0&0&0&0\end {array}  \right)  n, n-2, n-4, \ldots, -(n-2), -n e_{\lambda} \lambda 
e_0=[a_0, a_1, \ldots, a_n], a_i=\begin{cases} 0, \text{ i odd}\\ \displaystyle\binom{\frac{n}{2}}{ \frac{i}{2}}, \text{i even} \end{cases}
 n=8 e_0=[1, 0, 4, 0, 6, 0, 4, 0, 1] 
e_1=[a_0, a_2, \ldots, a_n], a_i=\begin{cases} 0, \text{ i even}\\ \displaystyle\binom{\frac{n-1}{2}}{ \frac{i-1}{2}}, \text{i odd} \end{cases}.
 n=7 e_1=[0, 1, 0, 3, 0, 3, 0, 1] 
e_2=[a_0, a_2, \ldots, a_n], a_i=\begin{cases} \displaystyle 
       \binom{\frac{n}{2}-1}{\frac{i}{2}-1}-\binom{\frac{n}{2}-1}{\frac{i}{2}},  i\text{ even} \\ \\
      \displaystyle 2(-1)^{i-1}\binom{\frac{n}{2}-1}{\frac{i-1}{2}} , i\text{  odd}
   \end{cases}.
 n=8 e_2=[-1, 2, -2, 6, 0, 6, 2, 2, 1] e_{\lambda} \lambda \in \{ n, n-2, n-4, \ldots, -(n-2), -n\}","['linear-algebra', 'combinatorics', 'matrices', 'eigenvalues-eigenvectors']"
35,When are almost block companion matrices which yield a given characteristic polynomial connected?,When are almost block companion matrices which yield a given characteristic polynomial connected?,,"This is motivated by this question Are matrices which yield a given characteristic polynomial and have specified structure connected? Let $\mathcal E \in M_n(\mathbb R)$ be a subset with following form: we first construct a block diagonal matrix in $M_n(\mathbb R)$ such that \begin{align*}   C = \begin{pmatrix} C_{k_1} & 0 & 0 & \cdots & 0 \\ 0 & C_{k_2} & 0 & \cdots & 0 \\ 0 & 0 & \ddots & \vdots & 0 \\ 0 & 0 & 0 & \cdots & C_{k_r} \end{pmatrix}, \end{align*} with $k_1 + k_2 + \dots + k_r = n$ such that each block $C_{k_j}$ is in the companion form \begin{align*} \begin{pmatrix} 0 & 0 & \cdots & 0 & -c_0 \\ 1 & 0 & \cdots & 0 & -c_1 \\ 0 & 1 & \ddots & \vdots & \vdots \\ 0 & 0 & \cdots & 1& -c_{k_j-1}  \end{pmatrix}. \end{align*} Now for each block we extend the last column to fill up the whole matrix. For example, suppose we have two blocks $C_1$ and $C_2$ with $C_1 \in \mathbb R^{2 \times 2}$ and $C_2 \in \mathbb R^{3 \times 3}$ , elements in $\mathcal E$ would look like \begin{align*} \begin{pmatrix} 0 & -a_1 & 0 & 0 & -b_1 \\ 1 & -a_2 & 0 & 0 & -b_2 \\ 0 & -a_3 & 0 & 0 & -b_3 \\ 0 & -a_4 & 1 & 0 & -b_4 \\ 0 & -a_5 & 0 &1 & -b_5 \end{pmatrix}. \end{align*} It is also clear for any monic $n^{th}$ degree real polynomial, we can at least find one realization in $\mathcal E$ since we can choose a matrix in block diagonal form. Let $f: \mathcal E \to \mathbb R^n$ be the map sending the coefficients of characteristic polynomial to $\mathbb R^n$ . Let $q(t) = t^n + a_{n-1} t^{n-1} + \dots + a_0$ be a fixed polynomial. I am wondering whether there are sufficient conditions on $a = (a_{n-1}, \dots, a_0)$ such that $f^{-1}(a)$ is a connected set? This question Are matrices which yield a given characteristic polynomial and have specified structure connected? asked a specific case, i.e, $n=4, k_1 = k_2 = 2$ . There is a very nice answer proving: as long as the polynomial has a real root, then it is connected. The technique by the answer does not seem to generalize. But I am very interested to know whether the same condition holds here: if $q(t)$ has a real root, then $f^{-1}(a)$ is connected where $a = (a_{n-1}, \dots, a_0)$ is the coefficient vector of $q(t)$ ? EDIT: This question might be too tricky to answer (This is the third time I put a bounty). But I would be happy to reward the bounty if someone gives an answer on a very special polynomial with coefficients vector of $a$ such that $f^{-1}(a)$ is connected. For example, is $f^{-1}((0, \dots, 0))$ connected, i.e., the polynomial with all roots to be $0$ or some other special polynomials?","This is motivated by this question Are matrices which yield a given characteristic polynomial and have specified structure connected? Let be a subset with following form: we first construct a block diagonal matrix in such that with such that each block is in the companion form Now for each block we extend the last column to fill up the whole matrix. For example, suppose we have two blocks and with and , elements in would look like It is also clear for any monic degree real polynomial, we can at least find one realization in since we can choose a matrix in block diagonal form. Let be the map sending the coefficients of characteristic polynomial to . Let be a fixed polynomial. I am wondering whether there are sufficient conditions on such that is a connected set? This question Are matrices which yield a given characteristic polynomial and have specified structure connected? asked a specific case, i.e, . There is a very nice answer proving: as long as the polynomial has a real root, then it is connected. The technique by the answer does not seem to generalize. But I am very interested to know whether the same condition holds here: if has a real root, then is connected where is the coefficient vector of ? EDIT: This question might be too tricky to answer (This is the third time I put a bounty). But I would be happy to reward the bounty if someone gives an answer on a very special polynomial with coefficients vector of such that is connected. For example, is connected, i.e., the polynomial with all roots to be or some other special polynomials?","\mathcal E \in M_n(\mathbb R) M_n(\mathbb R) \begin{align*}
  C = \begin{pmatrix}
C_{k_1} & 0 & 0 & \cdots & 0 \\
0 & C_{k_2} & 0 & \cdots & 0 \\
0 & 0 & \ddots & \vdots & 0 \\
0 & 0 & 0 & \cdots & C_{k_r}
\end{pmatrix},
\end{align*} k_1 + k_2 + \dots + k_r = n C_{k_j} \begin{align*}
\begin{pmatrix}
0 & 0 & \cdots & 0 & -c_0 \\
1 & 0 & \cdots & 0 & -c_1 \\
0 & 1 & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 1& -c_{k_j-1}
 \end{pmatrix}.
\end{align*} C_1 C_2 C_1 \in \mathbb R^{2 \times 2} C_2 \in \mathbb R^{3 \times 3} \mathcal E \begin{align*}
\begin{pmatrix}
0 & -a_1 & 0 & 0 & -b_1 \\
1 & -a_2 & 0 & 0 & -b_2 \\
0 & -a_3 & 0 & 0 & -b_3 \\
0 & -a_4 & 1 & 0 & -b_4 \\
0 & -a_5 & 0 &1 & -b_5
\end{pmatrix}.
\end{align*} n^{th} \mathcal E f: \mathcal E \to \mathbb R^n \mathbb R^n q(t) = t^n + a_{n-1} t^{n-1} + \dots + a_0 a = (a_{n-1}, \dots, a_0) f^{-1}(a) n=4, k_1 = k_2 = 2 q(t) f^{-1}(a) a = (a_{n-1}, \dots, a_0) q(t) a f^{-1}(a) f^{-1}((0, \dots, 0)) 0","['linear-algebra', 'general-topology', 'polynomials', 'connectedness', 'companion-matrices']"
36,Is this a natural transformation?,Is this a natural transformation?,,"Fix a field K and a 1-dimensional vector space $W$, and consider the functor $V\mapsto V\otimes W$ from the category of finite dimensional vector spaces to itself. In the book I am reading the author says $V$ and $V\otimes W$ are isomorphic but not naturally isomorphic. To me that means there is no natural transformation whose components are linear isomorphisms, from the identity functor to the $\otimes W$ functor. Let me fix a non-zero vector $w\in W$, and for each finite dimensional vector space $V$ consider the map $V\to V\otimes W$ given by $v\mapsto v\otimes w$. For any linear map $f\colon V_1\to V_2$ the diagram \begin{array} & V_1 &\longrightarrow &V_1\otimes W\\ \downarrow & & \ \ \ \ \ \downarrow \\ V_2 & {\longrightarrow} & V_2\otimes W \end{array} commutes because a vector $v_1\in V_1$ gets sent to $f(v_1)\otimes w$ in both cases, right? Isn't this the definition of a natural transformation? I know ""natural things"" have to do with morphisms being independent of choices, and my linear isomorphisms depend on a non-canonical choice of generator $w\in W$. But I can't see why, in a rigorous sense, the thing I just defined wouldn't be a natural transformation.","Fix a field K and a 1-dimensional vector space $W$, and consider the functor $V\mapsto V\otimes W$ from the category of finite dimensional vector spaces to itself. In the book I am reading the author says $V$ and $V\otimes W$ are isomorphic but not naturally isomorphic. To me that means there is no natural transformation whose components are linear isomorphisms, from the identity functor to the $\otimes W$ functor. Let me fix a non-zero vector $w\in W$, and for each finite dimensional vector space $V$ consider the map $V\to V\otimes W$ given by $v\mapsto v\otimes w$. For any linear map $f\colon V_1\to V_2$ the diagram \begin{array} & V_1 &\longrightarrow &V_1\otimes W\\ \downarrow & & \ \ \ \ \ \downarrow \\ V_2 & {\longrightarrow} & V_2\otimes W \end{array} commutes because a vector $v_1\in V_1$ gets sent to $f(v_1)\otimes w$ in both cases, right? Isn't this the definition of a natural transformation? I know ""natural things"" have to do with morphisms being independent of choices, and my linear isomorphisms depend on a non-canonical choice of generator $w\in W$. But I can't see why, in a rigorous sense, the thing I just defined wouldn't be a natural transformation.",,"['linear-algebra', 'category-theory']"
37,Pairwise negative dot product implies linear independence,Pairwise negative dot product implies linear independence,,"Let $v_1, ..., v_{m+1} \in \mathbb{R}^n$ such that $v_i \cdot v_j < 0$ if $i \ne j$. Show that $v_1, ..., v_m$ are linearly independent. My attempt: Assume $v_1, ..., v_m \in \mathbb{R}^n$ are linearly dependent with pairwise negative dot product. Let $$S = \text{span}({v_1, ..., v_m})$$ I want to show that for any $v \in \mathbb{R}^n$ there exists some $v_i$ such that $v \cdot v_i \ge 0$. Since $v = u + w$ where $u \in S$ and $w \in S^\perp$, it suffices to show that there exists some $v_i$ such that $u\cdot v_i \ge 0$. But I am stuck trying to prove this.","Let $v_1, ..., v_{m+1} \in \mathbb{R}^n$ such that $v_i \cdot v_j < 0$ if $i \ne j$. Show that $v_1, ..., v_m$ are linearly independent. My attempt: Assume $v_1, ..., v_m \in \mathbb{R}^n$ are linearly dependent with pairwise negative dot product. Let $$S = \text{span}({v_1, ..., v_m})$$ I want to show that for any $v \in \mathbb{R}^n$ there exists some $v_i$ such that $v \cdot v_i \ge 0$. Since $v = u + w$ where $u \in S$ and $w \in S^\perp$, it suffices to show that there exists some $v_i$ such that $u\cdot v_i \ge 0$. But I am stuck trying to prove this.",,['linear-algebra']
38,Spectral radius of a pair of operators which commute,Spectral radius of a pair of operators which commute,,"Let $E$ be an infinite-dimensional complex Hilbert space. The spectral radius of a commuting multivariable operator $A = (A_1,\cdots,A_n)\in\mathcal{L}(E)^n$ is given by  $$r_a(A_1,\cdots,A_n)=\displaystyle\lim_{m\to \infty}\left\|\displaystyle\sum_{|\alpha|=m}\frac{m!}{\alpha!}{A^*}^{\alpha}A^{\alpha}\right\|^{\frac{1}{2m}},$$ where $m\in\mathbb{N}^*,\;$ $\alpha = (\alpha_1, \alpha_2,...,\alpha_n) \in \mathbb{Z}_+^n;\;\alpha!: =\alpha_1!\cdots\alpha_n!,\;|\alpha|:=\displaystyle\sum_{j=1}^n|\alpha_j|$; $A^*=(A_1^*,\cdots,A_n^*)$ and $A^\alpha:=A_1^{\alpha_1} A_2^{\alpha_2}\cdots A_n^{\alpha_n}$. I claim that if $A_iA_j=A_jA_i$ for all $i,j$, then in general   $$r_a(A_1,\cdots,A_n)\neq r_a(A_1^*,\cdots,A_n^*).$$   I hope to find an explicit example which show that the claim is true. Note also that we have \begin{align*} r_a(A_1,\cdots,A_n)  &=\sup\{\|\lambda\|_2,\;\;\lambda \in \sigma_{ap}(A)\}, \end{align*}  where $$\sigma_{ap}(A)=\bigg\{\lambda\in \mathbb{C}^n: \;\exists\;(x_k)_k\subset E;\,\,\|x_k\|=1\;\;\hbox{such that}\;\;\\\lim_{k\longrightarrow \infty}\sum_{1\leq j\leq n}\|(A_j-\lambda_j)x_k\|=0\bigg\}.$$","Let $E$ be an infinite-dimensional complex Hilbert space. The spectral radius of a commuting multivariable operator $A = (A_1,\cdots,A_n)\in\mathcal{L}(E)^n$ is given by  $$r_a(A_1,\cdots,A_n)=\displaystyle\lim_{m\to \infty}\left\|\displaystyle\sum_{|\alpha|=m}\frac{m!}{\alpha!}{A^*}^{\alpha}A^{\alpha}\right\|^{\frac{1}{2m}},$$ where $m\in\mathbb{N}^*,\;$ $\alpha = (\alpha_1, \alpha_2,...,\alpha_n) \in \mathbb{Z}_+^n;\;\alpha!: =\alpha_1!\cdots\alpha_n!,\;|\alpha|:=\displaystyle\sum_{j=1}^n|\alpha_j|$; $A^*=(A_1^*,\cdots,A_n^*)$ and $A^\alpha:=A_1^{\alpha_1} A_2^{\alpha_2}\cdots A_n^{\alpha_n}$. I claim that if $A_iA_j=A_jA_i$ for all $i,j$, then in general   $$r_a(A_1,\cdots,A_n)\neq r_a(A_1^*,\cdots,A_n^*).$$   I hope to find an explicit example which show that the claim is true. Note also that we have \begin{align*} r_a(A_1,\cdots,A_n)  &=\sup\{\|\lambda\|_2,\;\;\lambda \in \sigma_{ap}(A)\}, \end{align*}  where $$\sigma_{ap}(A)=\bigg\{\lambda\in \mathbb{C}^n: \;\exists\;(x_k)_k\subset E;\,\,\|x_k\|=1\;\;\hbox{such that}\;\;\\\lim_{k\longrightarrow \infty}\sum_{1\leq j\leq n}\|(A_j-\lambda_j)x_k\|=0\bigg\}.$$",,"['linear-algebra', 'matrices', 'operator-theory']"
39,Is a module that is isomorphic to its dual necessarily free?,Is a module that is isomorphic to its dual necessarily free?,,"Suppose $M$ is a finitely generated module over an integral domain $R$. If there is a bilinear form $\langle-,-\rangle:M\times M\rightarrow R$ which induces an isomorphism $M\rightarrow \text{Hom}_R(M,R)$ via $m\mapsto \langle m,-\rangle$, is it possible to conclude that $M$ is a free $R$-module?","Suppose $M$ is a finitely generated module over an integral domain $R$. If there is a bilinear form $\langle-,-\rangle:M\times M\rightarrow R$ which induces an isomorphism $M\rightarrow \text{Hom}_R(M,R)$ via $m\mapsto \langle m,-\rangle$, is it possible to conclude that $M$ is a free $R$-module?",,"['linear-algebra', 'abstract-algebra']"
40,"Motivation for the term ""trace"" in linear algebra","Motivation for the term ""trace"" in linear algebra",,"I'm wondering what could be the motivation behind the term ""trace"", especially in the simple case of a trace of a matrix. Looking through some geometric interpretations and bearing in mind the traditional meanings of the word ""trace"" , I could not conceive a satisfactory answer.","I'm wondering what could be the motivation behind the term ""trace"", especially in the simple case of a trace of a matrix. Looking through some geometric interpretations and bearing in mind the traditional meanings of the word ""trace"" , I could not conceive a satisfactory answer.",,"['linear-algebra', 'matrices', 'terminology', 'math-history', 'trace']"
41,Every linear operator on $\mathbb{R}^5$ has an invariant 3-dimensional subspace,Every linear operator on  has an invariant 3-dimensional subspace,\mathbb{R}^5,"I am trying to determine whether the following statement is true or false: Every linear transformation on $\mathbb{R}^5$ has an invariant 3-dimensional subspace. Since $\dim(\mathbb{R}^5)=5$ then given any linear operator $T$ on $\mathbb{R}^5$ I know that $\deg(\text{char}_T(x))=5$, and hence,$\text{char}_T(x)$ has at least one real root, meaning that $T$ has at least one real eigenvalue, $\lambda$. Thus, $$\text{char}_T(x)=(x-\lambda)f(x),$$ where $f$ can be factored as the product of irreducibles into two quadratics, a quadratic and two linear factors, or 4 linear factors. I don't know where to go from there. Perhaps the statement is false? Thank you for your help!","I am trying to determine whether the following statement is true or false: Every linear transformation on $\mathbb{R}^5$ has an invariant 3-dimensional subspace. Since $\dim(\mathbb{R}^5)=5$ then given any linear operator $T$ on $\mathbb{R}^5$ I know that $\deg(\text{char}_T(x))=5$, and hence,$\text{char}_T(x)$ has at least one real root, meaning that $T$ has at least one real eigenvalue, $\lambda$. Thus, $$\text{char}_T(x)=(x-\lambda)f(x),$$ where $f$ can be factored as the product of irreducibles into two quadratics, a quadratic and two linear factors, or 4 linear factors. I don't know where to go from there. Perhaps the statement is false? Thank you for your help!",,"['linear-algebra', 'vector-spaces', 'linear-transformations', 'invariant-subspace']"
42,Tricky inequality on norms of vectors,Tricky inequality on norms of vectors,,"Let $v_0=(v_{0x},0),v_1=(v1x,0),w_0=r_0e^{i\theta_0},w_1=r_1e^{i\theta_1}\in \mathbb{R}^2$ be any four non-zero vectors such that: $v_{0x}>0$ and $v_{1x}>0$ $0<\theta_0<\pi$ and $0<\theta_1<\pi$ the following inequalities are satisfied: $$\displaystyle{\frac{||v_1||}{||v_0||}> \frac{||w_1||}{||w_0||}}\quad \textit{and }\frac{||v_1||}{||v_0||}> \frac{||v_1+w_1||}{||v_0+w_0||}$$ Now define, for every $t\in [0,1]$, the following vectors: $$\displaystyle{v_t=((1-t)v_{0x}+tv_{1x},0)}$$ $$\displaystyle{w_t=(r_0(1-t)+tr_1)e^{i((1-t)\theta_0+t\theta_1)}}$$ Is the following inequality $$\displaystyle{\frac{||v_{t_1}||}{||v_{t_0}||}\ge \frac{||v_{t_1}+w_{t_1}||}{||v_{t_0}+w_{t_0}||}}$$ verified for every $t_0,t_1\in [0,1],t_0<t_1$? If the answer is no, I'm searching for additional conditions on $v_1,v_2,w_1,w_2$ which will guarantee that this inequality is always verified. My attempt: I've tried many examples without findind a counterexample. But I couldn't find a general proof.","Let $v_0=(v_{0x},0),v_1=(v1x,0),w_0=r_0e^{i\theta_0},w_1=r_1e^{i\theta_1}\in \mathbb{R}^2$ be any four non-zero vectors such that: $v_{0x}>0$ and $v_{1x}>0$ $0<\theta_0<\pi$ and $0<\theta_1<\pi$ the following inequalities are satisfied: $$\displaystyle{\frac{||v_1||}{||v_0||}> \frac{||w_1||}{||w_0||}}\quad \textit{and }\frac{||v_1||}{||v_0||}> \frac{||v_1+w_1||}{||v_0+w_0||}$$ Now define, for every $t\in [0,1]$, the following vectors: $$\displaystyle{v_t=((1-t)v_{0x}+tv_{1x},0)}$$ $$\displaystyle{w_t=(r_0(1-t)+tr_1)e^{i((1-t)\theta_0+t\theta_1)}}$$ Is the following inequality $$\displaystyle{\frac{||v_{t_1}||}{||v_{t_0}||}\ge \frac{||v_{t_1}+w_{t_1}||}{||v_{t_0}+w_{t_0}||}}$$ verified for every $t_0,t_1\in [0,1],t_0<t_1$? If the answer is no, I'm searching for additional conditions on $v_1,v_2,w_1,w_2$ which will guarantee that this inequality is always verified. My attempt: I've tried many examples without findind a counterexample. But I couldn't find a general proof.",,"['linear-algebra', 'geometry', 'vector-spaces', 'euclidean-geometry']"
43,A matrix is a product of nilpotent matrices iff its not invertible,A matrix is a product of nilpotent matrices iff its not invertible,,"I just completed a homework problem which proves the following result: a matrix (with coefficients in some field) is a product of nilpotent matrices iff its not invertible. The proof was broken into several parts and was quite involved. I'm wondering if there's a very simple way to demonstrate this result (just the implication non-invertible then its a product of nilpotent matrices , as the other implication is trivial).","I just completed a homework problem which proves the following result: a matrix (with coefficients in some field) is a product of nilpotent matrices iff its not invertible. The proof was broken into several parts and was quite involved. I'm wondering if there's a very simple way to demonstrate this result (just the implication non-invertible then its a product of nilpotent matrices , as the other implication is trivial).",,"['linear-algebra', 'matrices']"
44,Winning strategy in $(2n+1) \times (2n+1)$ matrix game.,Winning strategy in  matrix game.,(2n+1) \times (2n+1),"Edit : A few minutes after posting this question (that I had been thinking about for about a day) I figured out the answer in the $3 \times 3$ case; see my answer below. However, the question might still be interesting in the more general $(2n+1) \times (2n+1)$ case, so I am still interested in the question more generally. Below is the original question. Fix some field $\mathbb K$. Two players, Eloise and Abelard, play a game. They start with an empty $3 \times 3$ matrix. $$ \begin{pmatrix}  * & * & * \\  * & * & * \\  * & * & * \end{pmatrix} $$ Their turns alternate: first Eloise, then Abelard, and so on. A turn consists of filling in one of the blanks in the matrix with an element of $\mathbb K$. After Eloise's first move, the matrix might look like $$ \begin{pmatrix}  * & 3 & * \\  * & * & * \\  * & * & * \end{pmatrix}; $$ after Abelard's first move, it could look like $$ \begin{pmatrix}  * & 3 & * \\  * & * & * \\  -5 & * & * \end{pmatrix}. $$ The game ends when the matrix is filled, so a game always lasts 9 turns. Eloise wins the game if the matrix is invertible -- that is, if its determinant is non-zero. Abelard wins if the matrix is singular -- that is, its determinant vanishes. Clearly, since the number of turns in the game is bounded, one of the players must have a winning strategy. My question is: Does Eloise or Abelard have a winning strategy? Does this depend on the field $\mathbb K$, and if so, in what way? The question is motivated by this question and answer where it is shown that if we work in a $2n \times 2n$ matrix instead, Abelard has a winning strategy: whenever Eloise plays a move in an odd row, Abelard plays the same move in the row below; and when Eloise plays a move in an even row, Abelard plays the same move in the row above. This ensures that for any $i$, rows $2i-1$ and $2i$ agree, so that the matrix is singular. Note that this strategy also works if Abelard just copies Eloise's moves on rows 1 and 2, and plays random moves (outside row 1 and 2) otherwise. The same strategy does not work in the odd case: if Abelard tries to make rows 1 and 2 of the $3 \times 3$ matrix, Eloise can fill up row 3 first, forcing Abelard to make a move in row 1 and 2 before Eloise has. Since all even numbers are easy, and a $1 \times 1$ matrix is a trivial case, this makes the $3 \times 3$ matrix the first non-trivial case; but of course we can ask the question for a $(2n +1) \times (2n+1)$ matrix for any $n$. I wrote a short, inefficient Python script to brute force the solution for $\mathbb K = \mathbb F_2$, and $\mathbb K = \mathbb F_3$. (It is so inefficient that $\mathbb K = \mathbb F_5$ is already a problem.) It turns out that in both these cases, Abelard has a winning strategy (assuming my script is correct), although I haven't been able to see a pattern to the results of the computation that suggest a human-understandable strategy. For one final observation: it appears that Eloise has an advantage, getting both the first and the last turn, but it turns out that her first turn is (essentially) determined. Namely, if she plays a zero, then Abelard can beat her. Without loss of generality, suppose she played the zero in the top left. Then Abelard can add a zero next to it, and Eloise is forced to save the row from becoming all zeros: $$ \begin{pmatrix}  0 & 0 & e_1 \\  * & * & * \\  * & * & * \end{pmatrix}. $$ Then, Abelard can play a zero in the middle left, forcing Eloise to similarly save the left column from becoming all zeros: $$ \begin{pmatrix}  0 & 0 & e_1 \\  0 & * & * \\  e_2 & * & * \end{pmatrix}. $$ But then Abelard wins by playing a 0 in the center field: $$ \det\begin{pmatrix}  0 & 0 & e_1 \\  0 & 0 & * \\  e_2 & * & * \end{pmatrix} = 0. $$ Hence, Eloise must play a non-zero move, and by scaling the entire matrix we might as well assume she plays a 1. Thus, we can start the game as $$ \begin{pmatrix}  1 & * & * \\  * & * & * \\  * & * & * \end{pmatrix} $$ instead, letting Abelard take the first move.","Edit : A few minutes after posting this question (that I had been thinking about for about a day) I figured out the answer in the $3 \times 3$ case; see my answer below. However, the question might still be interesting in the more general $(2n+1) \times (2n+1)$ case, so I am still interested in the question more generally. Below is the original question. Fix some field $\mathbb K$. Two players, Eloise and Abelard, play a game. They start with an empty $3 \times 3$ matrix. $$ \begin{pmatrix}  * & * & * \\  * & * & * \\  * & * & * \end{pmatrix} $$ Their turns alternate: first Eloise, then Abelard, and so on. A turn consists of filling in one of the blanks in the matrix with an element of $\mathbb K$. After Eloise's first move, the matrix might look like $$ \begin{pmatrix}  * & 3 & * \\  * & * & * \\  * & * & * \end{pmatrix}; $$ after Abelard's first move, it could look like $$ \begin{pmatrix}  * & 3 & * \\  * & * & * \\  -5 & * & * \end{pmatrix}. $$ The game ends when the matrix is filled, so a game always lasts 9 turns. Eloise wins the game if the matrix is invertible -- that is, if its determinant is non-zero. Abelard wins if the matrix is singular -- that is, its determinant vanishes. Clearly, since the number of turns in the game is bounded, one of the players must have a winning strategy. My question is: Does Eloise or Abelard have a winning strategy? Does this depend on the field $\mathbb K$, and if so, in what way? The question is motivated by this question and answer where it is shown that if we work in a $2n \times 2n$ matrix instead, Abelard has a winning strategy: whenever Eloise plays a move in an odd row, Abelard plays the same move in the row below; and when Eloise plays a move in an even row, Abelard plays the same move in the row above. This ensures that for any $i$, rows $2i-1$ and $2i$ agree, so that the matrix is singular. Note that this strategy also works if Abelard just copies Eloise's moves on rows 1 and 2, and plays random moves (outside row 1 and 2) otherwise. The same strategy does not work in the odd case: if Abelard tries to make rows 1 and 2 of the $3 \times 3$ matrix, Eloise can fill up row 3 first, forcing Abelard to make a move in row 1 and 2 before Eloise has. Since all even numbers are easy, and a $1 \times 1$ matrix is a trivial case, this makes the $3 \times 3$ matrix the first non-trivial case; but of course we can ask the question for a $(2n +1) \times (2n+1)$ matrix for any $n$. I wrote a short, inefficient Python script to brute force the solution for $\mathbb K = \mathbb F_2$, and $\mathbb K = \mathbb F_3$. (It is so inefficient that $\mathbb K = \mathbb F_5$ is already a problem.) It turns out that in both these cases, Abelard has a winning strategy (assuming my script is correct), although I haven't been able to see a pattern to the results of the computation that suggest a human-understandable strategy. For one final observation: it appears that Eloise has an advantage, getting both the first and the last turn, but it turns out that her first turn is (essentially) determined. Namely, if she plays a zero, then Abelard can beat her. Without loss of generality, suppose she played the zero in the top left. Then Abelard can add a zero next to it, and Eloise is forced to save the row from becoming all zeros: $$ \begin{pmatrix}  0 & 0 & e_1 \\  * & * & * \\  * & * & * \end{pmatrix}. $$ Then, Abelard can play a zero in the middle left, forcing Eloise to similarly save the left column from becoming all zeros: $$ \begin{pmatrix}  0 & 0 & e_1 \\  0 & * & * \\  e_2 & * & * \end{pmatrix}. $$ But then Abelard wins by playing a 0 in the center field: $$ \det\begin{pmatrix}  0 & 0 & e_1 \\  0 & 0 & * \\  e_2 & * & * \end{pmatrix} = 0. $$ Hence, Eloise must play a non-zero move, and by scaling the entire matrix we might as well assume she plays a 1. Thus, we can start the game as $$ \begin{pmatrix}  1 & * & * \\  * & * & * \\  * & * & * \end{pmatrix} $$ instead, letting Abelard take the first move.",,"['linear-algebra', 'determinant', 'game-theory', 'combinatorial-game-theory']"
45,Minimal specification of isometry in terms of norm preservation,Minimal specification of isometry in terms of norm preservation,,"Let $V,W$ be $n$-dimensional (real) inner product spaces, and let $T:V \to W$ be a linear map. Let $v_1,...,v_n$ be a basis of $V$. It is easy to see that if $|T(v)|_W=|v|_V$ for every $v \in \{v_1,...,v_n,v_1+v_2,v_1+v_3,...,v_{n-1}+v_n\}$, then $T$ is an isometry (a proof is provided below). In other words, after choosing wisely $k(n):=\frac{n(n+1)}{2}$ vectors, it is enough to verify $T$ preserves the norms of these special vectors, in order to conclude it's an isometry. Question: Is there no way to choose less than $k(n)$ vectors, in such a way that every linear map which preserves their norms is an isometry? I believe we cannot choose less vectors. I have some ""convincing evidence"" for the cases $n=1,2,3$ (see below), but I am not sure how to give a rigorous argument. Note that a ""wise choice"" of vectors does not have to be of the form of some vectors, and linear combinations of them (I do feel this it the most efficient method, but I don't see how to prove this). Even if we prove that this is the case, than we need to show we cannot do better than to work with only orthonormal bases. The partial ""evidence"": $n=1: k=1$. Obvious $n=2: k=3$. Take $V=W=\mathbb{R}^n$ with its standard inner product. Then, $T(e_1)=e_1, T(e_2)=\frac{e_1+e_2}{\sqrt 2}$ is a counter example. $n=3: k=6$. Then any matrix of the form $$ \begin{pmatrix} c & s & x \\  -s & c & y  \\  0 & 0 & z \\\end{pmatrix} $$ where $c^2+s^2=1,x^2+y^2+z^2=1, sx+cy=0$ preserves the norms $e_1,e_2,e_1+e_2,e_3,e_2+e_3$ but it's an isometry only if $|z|=1,x=y=0$. Proof that $k(n)=\frac{n(n+1)}{2}$ vectors are enough: Noting that $$ \langle u,v \rangle = \frac{1}{2}(|u+v|^2 - |u|^2 - |v|^2) ,$$ we obtain $$ \langle Tv_i,Tv_j \rangle = \frac{1}{2}(|Tv_i+Tv_j|^2 - |Tv_i|^2 - |Tv_j|^2) = \frac{1}{2}(|T(v_i+v_j)|^2 - |v_i|^2 - |v_j|^2)  $$ $$ = \frac{1}{2}(|v_i+v_j|^2 - |v_i|^2 - |v_j|^2) = \langle v_i,v_j \rangle,$$ thus $T$ is an isometry.","Let $V,W$ be $n$-dimensional (real) inner product spaces, and let $T:V \to W$ be a linear map. Let $v_1,...,v_n$ be a basis of $V$. It is easy to see that if $|T(v)|_W=|v|_V$ for every $v \in \{v_1,...,v_n,v_1+v_2,v_1+v_3,...,v_{n-1}+v_n\}$, then $T$ is an isometry (a proof is provided below). In other words, after choosing wisely $k(n):=\frac{n(n+1)}{2}$ vectors, it is enough to verify $T$ preserves the norms of these special vectors, in order to conclude it's an isometry. Question: Is there no way to choose less than $k(n)$ vectors, in such a way that every linear map which preserves their norms is an isometry? I believe we cannot choose less vectors. I have some ""convincing evidence"" for the cases $n=1,2,3$ (see below), but I am not sure how to give a rigorous argument. Note that a ""wise choice"" of vectors does not have to be of the form of some vectors, and linear combinations of them (I do feel this it the most efficient method, but I don't see how to prove this). Even if we prove that this is the case, than we need to show we cannot do better than to work with only orthonormal bases. The partial ""evidence"": $n=1: k=1$. Obvious $n=2: k=3$. Take $V=W=\mathbb{R}^n$ with its standard inner product. Then, $T(e_1)=e_1, T(e_2)=\frac{e_1+e_2}{\sqrt 2}$ is a counter example. $n=3: k=6$. Then any matrix of the form $$ \begin{pmatrix} c & s & x \\  -s & c & y  \\  0 & 0 & z \\\end{pmatrix} $$ where $c^2+s^2=1,x^2+y^2+z^2=1, sx+cy=0$ preserves the norms $e_1,e_2,e_1+e_2,e_3,e_2+e_3$ but it's an isometry only if $|z|=1,x=y=0$. Proof that $k(n)=\frac{n(n+1)}{2}$ vectors are enough: Noting that $$ \langle u,v \rangle = \frac{1}{2}(|u+v|^2 - |u|^2 - |v|^2) ,$$ we obtain $$ \langle Tv_i,Tv_j \rangle = \frac{1}{2}(|Tv_i+Tv_j|^2 - |Tv_i|^2 - |Tv_j|^2) = \frac{1}{2}(|T(v_i+v_j)|^2 - |v_i|^2 - |v_j|^2)  $$ $$ = \frac{1}{2}(|v_i+v_j|^2 - |v_i|^2 - |v_j|^2) = \langle v_i,v_j \rangle,$$ thus $T$ is an isometry.",,"['linear-algebra', 'combinatorics', 'symmetry', 'isometry']"
46,Number of path-connected components of the set of all $m$-th roots of the identity matrix,Number of path-connected components of the set of all -th roots of the identity matrix,m,"Let $m$ be a fixed natural number Let $X$ be the subset of $M_n( \Bbb C)$ defined by : $X=\{A:A^m=I_n\}$ count the number of arc connected components of $X$ Except to note that the eigenvalues of the matrices of $X$ are the $n$-th roots of the unit, I do not advance in the enumeration","Let $m$ be a fixed natural number Let $X$ be the subset of $M_n( \Bbb C)$ defined by : $X=\{A:A^m=I_n\}$ count the number of arc connected components of $X$ Except to note that the eigenvalues of the matrices of $X$ are the $n$-th roots of the unit, I do not advance in the enumeration",,"['linear-algebra', 'general-topology', 'combinatorics']"
47,Let $P \in M_n(\mathbb C)$ be idempotent. Prove that all nonzero singular values of $P$ satisfy $\sigma_i \ge 1$,Let  be idempotent. Prove that all nonzero singular values of  satisfy,P \in M_n(\mathbb C) P \sigma_i \ge 1,"I'm having some difficulty proving the following: Let $P \in M_n(\mathbb C)$ be idempotent. Prove that all nonzero singular values of $P$ satisfy $\sigma_i \ge 1$. By definition I know that $P$ being idempotent means $P^2 = P$. Likely I have to invoke the Singular Value Decomposition Theorem to prove the problem. So, let the singular value decomposition of $P$ be given by $$P = U\Sigma V^* = P^2 = U\Sigma V^*U\Sigma V^*.$$ By definition I know that the singular values of $P$ are the square roots of the eigenvalues of $P^*P.$ And unfortunately I am not sure where to go from here. I was inclined to say that $$\sigma_1 = \|P\|_2 = \|P^2\|_2 \le \|P\|_2\|P\|_2 = \sigma_1^2 \implies 1 \le \sigma_1,$$ for nonzero $\sigma_1$, but this doesn't tell me enough. What about $\sigma_2$? and further? Can anyone provide a hint?","I'm having some difficulty proving the following: Let $P \in M_n(\mathbb C)$ be idempotent. Prove that all nonzero singular values of $P$ satisfy $\sigma_i \ge 1$. By definition I know that $P$ being idempotent means $P^2 = P$. Likely I have to invoke the Singular Value Decomposition Theorem to prove the problem. So, let the singular value decomposition of $P$ be given by $$P = U\Sigma V^* = P^2 = U\Sigma V^*U\Sigma V^*.$$ By definition I know that the singular values of $P$ are the square roots of the eigenvalues of $P^*P.$ And unfortunately I am not sure where to go from here. I was inclined to say that $$\sigma_1 = \|P\|_2 = \|P^2\|_2 \le \|P\|_2\|P\|_2 = \sigma_1^2 \implies 1 \le \sigma_1,$$ for nonzero $\sigma_1$, but this doesn't tell me enough. What about $\sigma_2$? and further? Can anyone provide a hint?",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-decomposition']"
48,Alternative computation of eigenvalues of this tridiagonal matrix,Alternative computation of eigenvalues of this tridiagonal matrix,,"Consider the tridiagonal symmetric pd matrix $$ M=\begin{bmatrix} 2 & -1 &\dots \\ -1 & 2 &-1&\dots \\ \vdots & \ddots & \ddots & \ddots \\ 0 & \dots & -1 & 2 & -1 \\ 0  &\dots &\dots & -1 & 1\end{bmatrix}$$ The question is how to find eigenvalues and eigenvectors of $M$. I already know a way, which consists in short as introducing the sequence on the components $\phi_i$ of an eigenvector $\phi$ (the sequence is $-\phi_{k-1}+2\phi_k-\phi_{k+1}=\lambda \phi_k$). Then it is possible to find the general term and $\lambda$ using the polynomial associated with the sequence. After some manipulation, the final result is:  $$\lambda_k=2\Big(1-\cos\Big(\dfrac{(2k-1)\pi}{2n+1}\Big)\Big)$$ and the eigenvectors are given by $$\phi_k^{(i)}=\sin\Big(\dfrac{i(2k-1)\pi}{2n+1}\Big)$$ where $i$ is the component index and $k$ the index of the eigenvector. I was just wondering if someone knew another way of finding the eigendecomposition.","Consider the tridiagonal symmetric pd matrix $$ M=\begin{bmatrix} 2 & -1 &\dots \\ -1 & 2 &-1&\dots \\ \vdots & \ddots & \ddots & \ddots \\ 0 & \dots & -1 & 2 & -1 \\ 0  &\dots &\dots & -1 & 1\end{bmatrix}$$ The question is how to find eigenvalues and eigenvectors of $M$. I already know a way, which consists in short as introducing the sequence on the components $\phi_i$ of an eigenvector $\phi$ (the sequence is $-\phi_{k-1}+2\phi_k-\phi_{k+1}=\lambda \phi_k$). Then it is possible to find the general term and $\lambda$ using the polynomial associated with the sequence. After some manipulation, the final result is:  $$\lambda_k=2\Big(1-\cos\Big(\dfrac{(2k-1)\pi}{2n+1}\Big)\Big)$$ and the eigenvectors are given by $$\phi_k^{(i)}=\sin\Big(\dfrac{i(2k-1)\pi}{2n+1}\Big)$$ where $i$ is the component index and $k$ the index of the eigenvector. I was just wondering if someone knew another way of finding the eigendecomposition.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'alternative-proof', 'tridiagonal-matrices']"
49,Show that $\|A^{-1}\|\leq \|A\|^{n-1}$,Show that,\|A^{-1}\|\leq \|A\|^{n-1},"Suppose that $A$ is a matrix in $SL_n(\mathbb{R})$. Show that $\|A^{-1}\|\leq \|A\|^{n-1}$. By $\|A\|$, I mean the operator norm $\displaystyle\sup_{\|v\|=1} \|Av\|$.","Suppose that $A$ is a matrix in $SL_n(\mathbb{R})$. Show that $\|A^{-1}\|\leq \|A\|^{n-1}$. By $\|A\|$, I mean the operator norm $\displaystyle\sup_{\|v\|=1} \|Av\|$.",,"['linear-algebra', 'normed-spaces']"
50,How to find the matrix of a quadratic form?,How to find the matrix of a quadratic form?,,"I was wondering. If I have a bilinear symmetric form, it is easy to find its matrix. But, when I have a quadratic form, which is the procedure to do that? I heard that one possibility is: If $q$ is my quadratic form, then $$f(x,y) = \frac{1}{4}q(x+y) - \frac{1}{4}q(x-y)$$ is the bilinear symmetric form associated, so the method reduces to find the matrix of $f(x,y).$ The point is that seems a little noising... How to find, in practice, the matrix of quadratic form? Thanks in advance.","I was wondering. If I have a bilinear symmetric form, it is easy to find its matrix. But, when I have a quadratic form, which is the procedure to do that? I heard that one possibility is: If $q$ is my quadratic form, then $$f(x,y) = \frac{1}{4}q(x+y) - \frac{1}{4}q(x-y)$$ is the bilinear symmetric form associated, so the method reduces to find the matrix of $f(x,y).$ The point is that seems a little noising... How to find, in practice, the matrix of quadratic form? Thanks in advance.",,"['linear-algebra', 'quadratic-forms']"
51,How much can we tell about $\det(X)$ if we know $\det(I + X)$?,How much can we tell about  if we know ?,\det(X) \det(I + X),"What can we tell about $\det(X)$ if we know $\det(I + X)$? Will it give some kind of bound for $\det(X)$? In general, if we know the determinant of matrix $A + X$, where $A$ is a constant matrix, how much can we say about $\det(X)$? Thank you for the attention.","What can we tell about $\det(X)$ if we know $\det(I + X)$? Will it give some kind of bound for $\det(X)$? In general, if we know the determinant of matrix $A + X$, where $A$ is a constant matrix, how much can we say about $\det(X)$? Thank you for the attention.",,"['linear-algebra', 'matrices', 'determinant']"
52,Matrices and prime numbers,Matrices and prime numbers,,"Let $ p $ be a prime number and \begin{align} K=\left\{ \begin{pmatrix} a &b \\   c& d \end{pmatrix} \mid a,b,c,d \in \left\{0,1,\ldots,p-1 \right\}, \right. & a+d \equiv 1 \!\!\!\! \pmod p, \\ & ad-bc \equiv 0 \!\!\!\! \pmod p \left.\vphantom{\begin{pmatrix} a &b \\   c& d \end{pmatrix}} \right\}. \end{align}   Determine $\operatorname{card}(K) $. I have taken some particular cases: $ p=2,3,5 \text{ or } 7 $ and I've deduced that $\operatorname{card}(K)=p(p+1) $, but I can't extend the solution to the general case.","Let $ p $ be a prime number and \begin{align} K=\left\{ \begin{pmatrix} a &b \\   c& d \end{pmatrix} \mid a,b,c,d \in \left\{0,1,\ldots,p-1 \right\}, \right. & a+d \equiv 1 \!\!\!\! \pmod p, \\ & ad-bc \equiv 0 \!\!\!\! \pmod p \left.\vphantom{\begin{pmatrix} a &b \\   c& d \end{pmatrix}} \right\}. \end{align}   Determine $\operatorname{card}(K) $. I have taken some particular cases: $ p=2,3,5 \text{ or } 7 $ and I've deduced that $\operatorname{card}(K)=p(p+1) $, but I can't extend the solution to the general case.",,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'matrices', 'finite-fields']"
53,What is the geometric relationship between $A$ and $A^T$?,What is the geometric relationship between  and ?,A A^T,Posed a more specific way: Let $A \in \mathbb R ^ {m \times n}$ and $S_k$ be the unit $k$-sphere. What is the exact geometric relationship between $E_m = \{ A\vec x \mid \vec x \in S_m \}$ and $E_n = \{ A^T\vec y \mid \vec y \in S_n \}$?,Posed a more specific way: Let $A \in \mathbb R ^ {m \times n}$ and $S_k$ be the unit $k$-sphere. What is the exact geometric relationship between $E_m = \{ A\vec x \mid \vec x \in S_m \}$ and $E_n = \{ A^T\vec y \mid \vec y \in S_n \}$?,,"['linear-algebra', 'matrices', 'geometry']"
54,Prove the estimator $\hat{B}$ of ridge regression = mean of the posterior distribution under a Gaussian prior,Prove the estimator  of ridge regression = mean of the posterior distribution under a Gaussian prior,\hat{B},"I want to prove that the estimator of ridge regression is the mean of the posterior distribution under Gaussian prior. $$y \sim N(X\beta,\sigma^2I),\quad \text{prior }\beta \sim N(0,\gamma^2 I).$$ $$\hat{\beta} = \left(X^TX + \frac{\sigma^2}{\gamma^2}I\right)^{-1}X^Ty.$$ What I'm trying to show is want to show that $\mu$ = $\hat{B}$, for $\mu$ in $$-\frac{1}{2}(\beta - \mu)^T\Sigma^{-1}(\beta - \mu)$$ $\Sigma^{-1}$ is the covariance matrix for the posterior distribution $p(\beta\mid X,y)$. There is a solution to this question the last couple of lines on page 3 from http://ssli.ee.washington.edu/courses/ee511/HW/hw3_solns.pdf . I'm baffled as to how it does this. (The problem is exercise 3.6.) Edit: $\mu$ is the mean of the posterior. Edit2: Last couple of lines of problem 3.6 say ""which is the single $\beta$ term in the $p(\beta|y, X)$ equation."" What is the single $\beta$ term? This sentence makes no sense to me. I'm not sure what the relevance of saying something is the the singe $\beta$ term to this proof. Edit2 continued: For convenience, ""$m_b = \frac{1}{\sigma^2I}\Sigma_bX^Ty$, and $B^T\Sigma_b^{-1}m_b = \frac{1}{\sigma^2}B^tX^ty$, which is the single $\beta$ term in $p(\beta |X,y)$ equation."" (me: okay, how is this helpful to the proof?)","I want to prove that the estimator of ridge regression is the mean of the posterior distribution under Gaussian prior. $$y \sim N(X\beta,\sigma^2I),\quad \text{prior }\beta \sim N(0,\gamma^2 I).$$ $$\hat{\beta} = \left(X^TX + \frac{\sigma^2}{\gamma^2}I\right)^{-1}X^Ty.$$ What I'm trying to show is want to show that $\mu$ = $\hat{B}$, for $\mu$ in $$-\frac{1}{2}(\beta - \mu)^T\Sigma^{-1}(\beta - \mu)$$ $\Sigma^{-1}$ is the covariance matrix for the posterior distribution $p(\beta\mid X,y)$. There is a solution to this question the last couple of lines on page 3 from http://ssli.ee.washington.edu/courses/ee511/HW/hw3_solns.pdf . I'm baffled as to how it does this. (The problem is exercise 3.6.) Edit: $\mu$ is the mean of the posterior. Edit2: Last couple of lines of problem 3.6 say ""which is the single $\beta$ term in the $p(\beta|y, X)$ equation."" What is the single $\beta$ term? This sentence makes no sense to me. I'm not sure what the relevance of saying something is the the singe $\beta$ term to this proof. Edit2 continued: For convenience, ""$m_b = \frac{1}{\sigma^2I}\Sigma_bX^Ty$, and $B^T\Sigma_b^{-1}m_b = \frac{1}{\sigma^2}B^tX^ty$, which is the single $\beta$ term in $p(\beta |X,y)$ equation."" (me: okay, how is this helpful to the proof?)",,"['linear-algebra', 'regression', 'bayesian', 'regularization']"
55,Exists polynomial satisfying following?,Exists polynomial satisfying following?,,"Let $s, u \in M_m(\mathbb{k})$ be a pair of commuting matrices such that $s$ is a diagonal matrix and $u$ is a strictly triangular matrix (with zeros on the diagonal). Put $a = s + u$. Does there exist a polynomial $f(x) = c_1x + \cdots + c_dx^d \in \mathbb{k}[x]$, without constant term and such that one has $s = f(a)$ (a matrix equality), where $f(a) := c_1a + \cdots + c_da^d$?","Let $s, u \in M_m(\mathbb{k})$ be a pair of commuting matrices such that $s$ is a diagonal matrix and $u$ is a strictly triangular matrix (with zeros on the diagonal). Put $a = s + u$. Does there exist a polynomial $f(x) = c_1x + \cdots + c_dx^d \in \mathbb{k}[x]$, without constant term and such that one has $s = f(a)$ (a matrix equality), where $f(a) := c_1a + \cdots + c_da^d$?",,"['linear-algebra', 'matrices']"
56,Properties of matrices $M=UDU^*$ with $UU^*=Id$,Properties of matrices  with,M=UDU^* UU^*=Id,"I recently came across some matrices of the form $M=UDU^*$ (the superscript $*$ denotes the conjugate transpose), where $U \in \mathbb{C}^{r\times n}$ with $r<n$, $D \in \mathbb{C}^{n \times n}$ a diagonal matrix, and $UU^*=\text{Id} \in \mathbb{C}^{r \times r}$ the identity matrix. (Note that $U$ is rectangular, so the last condition is not that $U$ is unitary). I am interested in the eigenvalues of $M$, in particular how they are related to the eigenvalues of the matrix $D$. Any hints?","I recently came across some matrices of the form $M=UDU^*$ (the superscript $*$ denotes the conjugate transpose), where $U \in \mathbb{C}^{r\times n}$ with $r<n$, $D \in \mathbb{C}^{n \times n}$ a diagonal matrix, and $UU^*=\text{Id} \in \mathbb{C}^{r \times r}$ the identity matrix. (Note that $U$ is rectangular, so the last condition is not that $U$ is unitary). I am interested in the eigenvalues of $M$, in particular how they are related to the eigenvalues of the matrix $D$. Any hints?",,"['linear-algebra', 'matrices', 'hilbert-spaces', 'matrix-decomposition']"
57,Hyperplane in a complex vector space,Hyperplane in a complex vector space,,"My friend, who studies Physics, asked me about the meaning of ""functional"" so I gave the definition and some examples. To motivate its importance, I explained how a functional can be use to define a hyperplane without referring to a specific base of the space (A subset $H$ is a hyperplane iff there exists a non-trivial linear functional $x'$ and a scalar $c$ such that $x'(x) = c$ for all $x \in H$ ) and that it effectively divides the space into 3 parts e.g. $x'(x) < c$ , $x'(x) = c$ , and $x'(x) > c$ . I immediately notice that the argument works in real vector spaces but not the complex ones since complex numbers are not linearly ordered, thus the intuitive picture that hyperplanes ""divide space"" in the aforementioned sense seems to fail here. So, is there an intuitive way to visualize a complex hyperplane? For concreteness, you can assume that the space is a finite dimensional Hilbert space. Note that I am an undergraduate so I'd really appreciate some not too advanced answers (stuff like Hopf fibration would be considered too advanced for me, for example).","My friend, who studies Physics, asked me about the meaning of ""functional"" so I gave the definition and some examples. To motivate its importance, I explained how a functional can be use to define a hyperplane without referring to a specific base of the space (A subset is a hyperplane iff there exists a non-trivial linear functional and a scalar such that for all ) and that it effectively divides the space into 3 parts e.g. , , and . I immediately notice that the argument works in real vector spaces but not the complex ones since complex numbers are not linearly ordered, thus the intuitive picture that hyperplanes ""divide space"" in the aforementioned sense seems to fail here. So, is there an intuitive way to visualize a complex hyperplane? For concreteness, you can assume that the space is a finite dimensional Hilbert space. Note that I am an undergraduate so I'd really appreciate some not too advanced answers (stuff like Hopf fibration would be considered too advanced for me, for example).",H x' c x'(x) = c x \in H x'(x) < c x'(x) = c x'(x) > c,"['linear-algebra', 'functional-analysis', 'intuition', 'visualization']"
58,minimum eigenvalue for difference of two matrices,minimum eigenvalue for difference of two matrices,,"Let $A$ a symmetric positive definite matrix, and $B$ a matrix constructed from $A$ by setting all its off-diagonal elements to zero. Then is there a way to see for which values of positive scalars $a$ and $b$ $$C=aA-bB$$ is positive definite? Do we know something on the smallest eigenvalue of $C$? @uranix commented that I could be after some $\epsilon$ such that $\det(A-\epsilon B)=0$ and have $C_{\epsilon}$ be PSD. I looked at $A=[1,1;1,7]$ and found $\epsilon=1.37796$ but then the eigenvalues of $C_{1.37796}$ are $-3.02372$ and $0$ ...","Let $A$ a symmetric positive definite matrix, and $B$ a matrix constructed from $A$ by setting all its off-diagonal elements to zero. Then is there a way to see for which values of positive scalars $a$ and $b$ $$C=aA-bB$$ is positive definite? Do we know something on the smallest eigenvalue of $C$? @uranix commented that I could be after some $\epsilon$ such that $\det(A-\epsilon B)=0$ and have $C_{\epsilon}$ be PSD. I looked at $A=[1,1;1,7]$ and found $\epsilon=1.37796$ but then the eigenvalues of $C_{1.37796}$ are $-3.02372$ and $0$ ...",,"['linear-algebra', 'matrices']"
59,Mapping vector spaces over two different fields?,Mapping vector spaces over two different fields?,,"I was having linear algebra class and we have been discussing about a possible group homomorphism that might allow mapping between two vector spaces over two different fields This is also an extension of this question Suppose we have vector spaces $V$ and $W$ over some general field $\mathbb{F}_1$ and $\mathbb{F}_2$ and $T$ is a (linear) map from $V$ to $W$ In order to get around the issue of this vector space axiom becoming undefined because of c being in different fields $$T(c\mathbf{x})=cT(\mathbf{x})$$ What's the issue in doing this (adapting the definition of group homomorphism, where there are two groups $(G,@)$ and $(H,*)$ )? $$\phi (a @b)=\phi(a)*\phi(b)$$ to the context of vector space (where the fields are defined as $(\mathbb{F}_1,+,*)$ and $(\mathbb{F}_2,"",@)$ ) $$T(c_\mathbb{F_1}*\mathbf{x})=T(c_\mathbb{F_1})@T(\mathbf{x})=c_\mathbb{F_2}@T(\mathbf{x})$$ (The two cs are different because they are elements of different fields) It seems valid as long every element in $\mathbb{F}_2$ can be mapped from at least one in $\mathbb{F}_1$ . What subtleties have we overlooked? If this is valid is this still a linear algebra?","I was having linear algebra class and we have been discussing about a possible group homomorphism that might allow mapping between two vector spaces over two different fields This is also an extension of this question Suppose we have vector spaces and over some general field and and is a (linear) map from to In order to get around the issue of this vector space axiom becoming undefined because of c being in different fields What's the issue in doing this (adapting the definition of group homomorphism, where there are two groups and )? to the context of vector space (where the fields are defined as and ) (The two cs are different because they are elements of different fields) It seems valid as long every element in can be mapped from at least one in . What subtleties have we overlooked? If this is valid is this still a linear algebra?","V W \mathbb{F}_1 \mathbb{F}_2 T V W T(c\mathbf{x})=cT(\mathbf{x}) (G,@) (H,*) \phi (a @b)=\phi(a)*\phi(b) (\mathbb{F}_1,+,*) (\mathbb{F}_2,"",@) T(c_\mathbb{F_1}*\mathbf{x})=T(c_\mathbb{F_1})@T(\mathbf{x})=c_\mathbb{F_2}@T(\mathbf{x}) \mathbb{F}_2 \mathbb{F}_1","['linear-algebra', 'group-theory', 'vector-spaces']"
60,Automorphism group of the general affine group of the affine line over a finite field?,Automorphism group of the general affine group of the affine line over a finite field?,,"I am wondering what the structure of the automorphism group of the general affine group of the affine line over a finite field looks like. I'll make that a bit more precise: If $k$ is a finite field, and $\operatorname{AGL}_1(k)$ its group of affine transformations, i.e. maps of the form $$k\ \longrightarrow\ k:\ x\ \longmapsto\ ax+b,$$ with $a\in k^{\times}$ and $b\in k$, then what is the isomorphism type of $\operatorname{Aut}(\operatorname{AGL}_1(k))$? I know that $\operatorname{AGL}_1(k)\cong k\rtimes k^{\times}$, where the semi-direct product is given by the natural action of $k^{\times}$ on $k$ by multiplication. Also, as the center of $\operatorname{AGL}_1(k)$ is trivial, it is isomorphic to a subgroup of its isomorphism group. Any automorphism of $\operatorname{AGL}_1(k)$ restricts to a group automorphism of $k^{+}$, of which there are very many, unfortunately. What is a good way to approach this problem?","I am wondering what the structure of the automorphism group of the general affine group of the affine line over a finite field looks like. I'll make that a bit more precise: If $k$ is a finite field, and $\operatorname{AGL}_1(k)$ its group of affine transformations, i.e. maps of the form $$k\ \longrightarrow\ k:\ x\ \longmapsto\ ax+b,$$ with $a\in k^{\times}$ and $b\in k$, then what is the isomorphism type of $\operatorname{Aut}(\operatorname{AGL}_1(k))$? I know that $\operatorname{AGL}_1(k)\cong k\rtimes k^{\times}$, where the semi-direct product is given by the natural action of $k^{\times}$ on $k$ by multiplication. Also, as the center of $\operatorname{AGL}_1(k)$ is trivial, it is isomorphic to a subgroup of its isomorphism group. Any automorphism of $\operatorname{AGL}_1(k)$ restricts to a group automorphism of $k^{+}$, of which there are very many, unfortunately. What is a good way to approach this problem?",,"['linear-algebra', 'group-theory']"
61,What is the geometric interpretation of a vector squared?,What is the geometric interpretation of a vector squared?,,"I'm working through Introduction to Space Dynamics by William Tyrrell Thomson. I am having to do a lot of research to make it through even small parts, but I am unable to find information to make me confident enough to solve this question from the book: What is the geometric interpretation of $\left(\vec{a} + \vec{b}\right)^2$? To start, I'm considering a simplified form: $\vec{c}^2, c = \vec{a} + \vec{b}$ This is where I get stuck, as I have not been able to find how to handle a vector multiplied by itself. Information one place states that a vector multiplied by itself is the same as the dot product of a vector with itself: $\vec{c}\cdot\vec{c}$. Other places I've found information which makes me think that multiplying a vector by another vector in the sense one would multiply a scalar by a scalar is not a valid operation to perform. Which of these two is the case, or is it a third case I haven't considered? Am I approaching the problem incorrectly?","I'm working through Introduction to Space Dynamics by William Tyrrell Thomson. I am having to do a lot of research to make it through even small parts, but I am unable to find information to make me confident enough to solve this question from the book: What is the geometric interpretation of $\left(\vec{a} + \vec{b}\right)^2$? To start, I'm considering a simplified form: $\vec{c}^2, c = \vec{a} + \vec{b}$ This is where I get stuck, as I have not been able to find how to handle a vector multiplied by itself. Information one place states that a vector multiplied by itself is the same as the dot product of a vector with itself: $\vec{c}\cdot\vec{c}$. Other places I've found information which makes me think that multiplying a vector by another vector in the sense one would multiply a scalar by a scalar is not a valid operation to perform. Which of these two is the case, or is it a third case I haven't considered? Am I approaching the problem incorrectly?",,"['linear-algebra', 'geometry']"
62,Eigenvalues of $A(A+B)^{-1}$,Eigenvalues of,A(A+B)^{-1},"Given a positive semidefinite matrix $A$ and a positive definite matrix $B$ of the same dimension. Can we show that each eigenvalue: $$ \lambda\{A(A + B)^{-1}\} < 1$$ (in the scalar case, this is $\frac{a}{a + b} < 1$ when both $a$ and $b$ are positive which holds trivially)","Given a positive semidefinite matrix $A$ and a positive definite matrix $B$ of the same dimension. Can we show that each eigenvalue: $$ \lambda\{A(A + B)^{-1}\} < 1$$ (in the scalar case, this is $\frac{a}{a + b} < 1$ when both $a$ and $b$ are positive which holds trivially)",,"['linear-algebra', 'matrices']"
63,Solving a recurrence with diagonalization?,Solving a recurrence with diagonalization?,,"Considering the recurrence $F_n=F_{n-1}+3F_{n-2}-3F_{n-3}$ where $F_0=0$, $F_1=1$ and $F_2=2$. Use diagonalization to find a closed form expression for $F_n$. So I first continued the recurrence to find $F_3=5$, $F_4=8$, $F_5=17$ ... etc From this is I got a vector with three consecutive terms in the recurrence to be $u_k = \begin{pmatrix} -3F_{n-3} \\ 3F_{n-2} \\ F_{n-1}\end{pmatrix}$ From here get a matrix $A$  from the terms of the recurrence: $A = \begin{pmatrix} F_4 & F_3 & F_2 \\ F_3 & F_2 & F_1 \\ F_2 & F_1 & F_0 \end{pmatrix} =  \begin{pmatrix} 8 & 5 & 2 \\ 5 & 2 & 1 \\ 2 & 1 & 0 \end{pmatrix}$ Then the action of $A$ on $u_k$ would produce the $u_{k+1}$ term. [Correct me if I am wrong.] So, $Au_k = \begin{pmatrix} 8 & 5 & 2 \\ 5 & 2 & 1 \\ 2 & 1 & 0 \end{pmatrix} \begin{pmatrix} -3F_{n-3} \\ 3F_{n-2} \\ F_{n-1}\end{pmatrix} =  \begin{pmatrix} -24F_{n-3} + 15F_{n-2} + 2F_{n-1} \\ -15F_{n-3} + 6F_{n-2} + F_{n-1} \\ -6F_{n-3} + 3F_{n-2} + 0 \end{pmatrix}$ Then $u_0 = \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix}$ From this point on I am unsure how to follow through with completing the problem. I have followed this Fibonacci Recurrence Problem - Most Helpful to figure out what to do next but have fallen short. I see that the next step should be to find $\det(A - \lambda I)$ : $0 = \det(A - \lambda I) = \begin{vmatrix} 8-\lambda & 5 & 2 \\ 5 & 2-\lambda & 1 \\ 2 & 1 & -\lambda \end{vmatrix}$ $=(8-\lambda)\begin{vmatrix} 2-\lambda & 1 \\ 1 & -\lambda\end{vmatrix} - 5\begin{vmatrix} 5 & 1 \\ 2 & -\lambda\end{vmatrix} + 2\begin{vmatrix} 5 & 2-\lambda \\ 2 & 1\end{vmatrix}$ $= (8-\lambda)(-2\lambda^2-1)-5(-5\lambda-2)+2(1+2\lambda)$ $= -16\lambda^{2}-8 + 2\lambda^3 + \lambda + 29\lambda + 12$ $= 2\lambda^3-16\lambda^2+30\lambda+4$ $= 2(\lambda^3-8\lambda^2+15\lambda+2)$ $= 2((\lambda-5)(\lambda-3)\lambda+2)$ From the source above, I can't figure out what the next steps should be? Can anyone help? Thanks!","Considering the recurrence $F_n=F_{n-1}+3F_{n-2}-3F_{n-3}$ where $F_0=0$, $F_1=1$ and $F_2=2$. Use diagonalization to find a closed form expression for $F_n$. So I first continued the recurrence to find $F_3=5$, $F_4=8$, $F_5=17$ ... etc From this is I got a vector with three consecutive terms in the recurrence to be $u_k = \begin{pmatrix} -3F_{n-3} \\ 3F_{n-2} \\ F_{n-1}\end{pmatrix}$ From here get a matrix $A$  from the terms of the recurrence: $A = \begin{pmatrix} F_4 & F_3 & F_2 \\ F_3 & F_2 & F_1 \\ F_2 & F_1 & F_0 \end{pmatrix} =  \begin{pmatrix} 8 & 5 & 2 \\ 5 & 2 & 1 \\ 2 & 1 & 0 \end{pmatrix}$ Then the action of $A$ on $u_k$ would produce the $u_{k+1}$ term. [Correct me if I am wrong.] So, $Au_k = \begin{pmatrix} 8 & 5 & 2 \\ 5 & 2 & 1 \\ 2 & 1 & 0 \end{pmatrix} \begin{pmatrix} -3F_{n-3} \\ 3F_{n-2} \\ F_{n-1}\end{pmatrix} =  \begin{pmatrix} -24F_{n-3} + 15F_{n-2} + 2F_{n-1} \\ -15F_{n-3} + 6F_{n-2} + F_{n-1} \\ -6F_{n-3} + 3F_{n-2} + 0 \end{pmatrix}$ Then $u_0 = \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix}$ From this point on I am unsure how to follow through with completing the problem. I have followed this Fibonacci Recurrence Problem - Most Helpful to figure out what to do next but have fallen short. I see that the next step should be to find $\det(A - \lambda I)$ : $0 = \det(A - \lambda I) = \begin{vmatrix} 8-\lambda & 5 & 2 \\ 5 & 2-\lambda & 1 \\ 2 & 1 & -\lambda \end{vmatrix}$ $=(8-\lambda)\begin{vmatrix} 2-\lambda & 1 \\ 1 & -\lambda\end{vmatrix} - 5\begin{vmatrix} 5 & 1 \\ 2 & -\lambda\end{vmatrix} + 2\begin{vmatrix} 5 & 2-\lambda \\ 2 & 1\end{vmatrix}$ $= (8-\lambda)(-2\lambda^2-1)-5(-5\lambda-2)+2(1+2\lambda)$ $= -16\lambda^{2}-8 + 2\lambda^3 + \lambda + 29\lambda + 12$ $= 2\lambda^3-16\lambda^2+30\lambda+4$ $= 2(\lambda^3-8\lambda^2+15\lambda+2)$ $= 2((\lambda-5)(\lambda-3)\lambda+2)$ From the source above, I can't figure out what the next steps should be? Can anyone help? Thanks!",,"['linear-algebra', 'recurrence-relations', 'diagonalization']"
64,Matrix restoring (modulo n),Matrix restoring (modulo n),,"Let $m,n\geqslant 2$, and $A\in \mathcal{M}_n(\mathbb Z)$ such that $\det A \equiv 1 \pmod m$. Does it (necessarily) exist $M\in \mathrm{GL}_n(\mathbb Z)$ such that $A\equiv M \pmod m$?","Let $m,n\geqslant 2$, and $A\in \mathcal{M}_n(\mathbb Z)$ such that $\det A \equiv 1 \pmod m$. Does it (necessarily) exist $M\in \mathrm{GL}_n(\mathbb Z)$ such that $A\equiv M \pmod m$?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'elementary-number-theory', 'modular-arithmetic']"
65,How to extend an existing orthogonal set of vectors?,How to extend an existing orthogonal set of vectors?,,"Suppose I have $k$ vectors in $\mathbb R^n$ that are orthogonal to each other ($k \ll n$). Is there an efficient way to find another vector that is orthogonal to all these given vectors? If we put the $k$ vectors in a $k \times n$ matrix, denoted by $A$, then the problem is identical to finding a vector in the null space of $A$. Of course one can resort to the method of solving an underdetermined linear system to find the null space of $A$, but by doing this we have got all the vectors that span the null space of $A$. For $k \ll n$, this is definitely not efficient, since I only need one vector in the null space. I'm wondering if there exists a smarter way.","Suppose I have $k$ vectors in $\mathbb R^n$ that are orthogonal to each other ($k \ll n$). Is there an efficient way to find another vector that is orthogonal to all these given vectors? If we put the $k$ vectors in a $k \times n$ matrix, denoted by $A$, then the problem is identical to finding a vector in the null space of $A$. Of course one can resort to the method of solving an underdetermined linear system to find the null space of $A$, but by doing this we have got all the vectors that span the null space of $A$. For $k \ll n$, this is definitely not efficient, since I only need one vector in the null space. I'm wondering if there exists a smarter way.",,"['linear-algebra', 'algorithms']"
66,A $2\times2$ Matrix inequality,A  Matrix inequality,2\times2,"$M,N$ are  $2\times2$ real matrices, and $MN=NM$. Then, for any three real numbers $x,y,z$, we have $$4xz\det(xM^2+yMN+zN^2)\geq(4xz-y^2)\big(x\det(M)-z\det(N)\big)^2 $$ some thought: 1). calculate directly, we got  $$ \det(A-xB)=x^2\det(B)-\big(\operatorname{Tr}(A)\operatorname{Tr}(B)-\operatorname{Tr}(AB)\big)x+\det(A) $$  (where $A,B$ are $2\times2$  matrices). 2). $ 4xz\cdot \det(xM^2+yMN+zN^2)= 4x^3z\cdot \det(M-mN)\det(M-nN) $ But I don't know how to go ahead. Thanks a lot!","$M,N$ are  $2\times2$ real matrices, and $MN=NM$. Then, for any three real numbers $x,y,z$, we have $$4xz\det(xM^2+yMN+zN^2)\geq(4xz-y^2)\big(x\det(M)-z\det(N)\big)^2 $$ some thought: 1). calculate directly, we got  $$ \det(A-xB)=x^2\det(B)-\big(\operatorname{Tr}(A)\operatorname{Tr}(B)-\operatorname{Tr}(AB)\big)x+\det(A) $$  (where $A,B$ are $2\times2$  matrices). 2). $ 4xz\cdot \det(xM^2+yMN+zN^2)= 4x^3z\cdot \det(M-mN)\det(M-nN) $ But I don't know how to go ahead. Thanks a lot!",,"['linear-algebra', 'matrices', 'inequality', 'determinant']"
67,Constructing matrices $A$ and $B$ such that $(A B)^+ \neq B^+ A^+$,Constructing matrices  and  such that,A B (A B)^+ \neq B^+ A^+,How do I go about constructing two matrices $A$ and $B$ such that the pseudoinverse of $AB$ is not equal to the pseudoinverse of $B$ times the pseudoinverse of $A$ ?,How do I go about constructing two matrices and such that the pseudoinverse of is not equal to the pseudoinverse of times the pseudoinverse of ?,A B AB B A,"['linear-algebra', 'matrices', 'pseudoinverse']"
68,"Generalization of $\frac{a + b}{c + d} \leq \text{max}(\frac{a}{c}, \frac{b}{d})$",Generalization of,"\frac{a + b}{c + d} \leq \text{max}(\frac{a}{c}, \frac{b}{d})","I'm looking for a matrix version of the basic inequality for the ratio of two sums of positive numbers: $$\frac{a + b}{c + d} \leq \max\left\{\frac{a}{c}, \frac{b}{d}\right\}.$$ Specifically, I have positive semidefinite matrices $A, B \in \mathbb{R}^{d \times d}$ such that $(A + B)$ is invertible.  I'm looking for a bound on $$\|(A + B)^{-1}(A x + B y)\|.$$ In the more general problem (the problem I actually care about), $A_i\ (i = 1, \dotsc, n)$ is a finite sequence of positive semidefinite matrices and I want a bound on $$\rho_n \equiv \Big\|\Big(\sum_{i=1}^{n} A_i\Big)^{-1}\Big(\sum_{i=1}^{n} A_i x_i\Big)\Big\|.$$ If it helps, assume that $A_i$ is a projection matrix and that $\|\cdot\|$ is Euclidean norm; I suspect that in in this case, $\rho_n \leq d^{1/2} \max_{i} \|x_i\|$.","I'm looking for a matrix version of the basic inequality for the ratio of two sums of positive numbers: $$\frac{a + b}{c + d} \leq \max\left\{\frac{a}{c}, \frac{b}{d}\right\}.$$ Specifically, I have positive semidefinite matrices $A, B \in \mathbb{R}^{d \times d}$ such that $(A + B)$ is invertible.  I'm looking for a bound on $$\|(A + B)^{-1}(A x + B y)\|.$$ In the more general problem (the problem I actually care about), $A_i\ (i = 1, \dotsc, n)$ is a finite sequence of positive semidefinite matrices and I want a bound on $$\rho_n \equiv \Big\|\Big(\sum_{i=1}^{n} A_i\Big)^{-1}\Big(\sum_{i=1}^{n} A_i x_i\Big)\Big\|.$$ If it helps, assume that $A_i$ is a projection matrix and that $\|\cdot\|$ is Euclidean norm; I suspect that in in this case, $\rho_n \leq d^{1/2} \max_{i} \|x_i\|$.",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces']"
69,Are solutions of $\frac{1}{2}(A^T+A)x=b$ and $Ax=b$ related?,Are solutions of  and  related?,\frac{1}{2}(A^T+A)x=b Ax=b,"I saw some statements about these 2 systems while I was reading something about linear algebra. So I am curious if the solutions of these 2 systems are related. If it is, how are they related? Thanks for any suggestion!","I saw some statements about these 2 systems while I was reading something about linear algebra. So I am curious if the solutions of these 2 systems are related. If it is, how are they related? Thanks for any suggestion!",,"['linear-algebra', 'matrices']"
70,Pivot columns and basic variables,Pivot columns and basic variables,,Suppose we are solving a system of linear equation and arrive at the following (augmented) matrix: $ \begin{bmatrix}  1& 6 &  0 & 3 & 0 & 0\\   0& 0 &  1 & -4 & 0 & 5\\   0& 0 &  0 & 0 & 1 & 7 \end{bmatrix} $ The question that arises is then: Which variables are basic and which are free? I know how to pick them up: You look for pivot and non-pivot columns. But what I wonder is: Why is it that non-pivot columns are those containing free variables and pivot columns are those containing the basic variables?,Suppose we are solving a system of linear equation and arrive at the following (augmented) matrix: $ \begin{bmatrix}  1& 6 &  0 & 3 & 0 & 0\\   0& 0 &  1 & -4 & 0 & 5\\   0& 0 &  0 & 0 & 1 & 7 \end{bmatrix} $ The question that arises is then: Which variables are basic and which are free? I know how to pick them up: You look for pivot and non-pivot columns. But what I wonder is: Why is it that non-pivot columns are those containing free variables and pivot columns are those containing the basic variables?,,"['linear-algebra', 'systems-of-equations']"
71,Number of possibilities of $10\times10$ matrix,Number of possibilities of  matrix,10\times10,"If $A$ is a $10\times10$ matrix with entries from the set $\{0, 1, 2, 3\}$ and if $AA^T$ is of the form:   $$\begin{pmatrix} 0 & * & * & \cdots & * \\ * & 0 & * & \cdots & * \\ * & * & 0 & \cdots & * \\ \vdots & \vdots & \vdots & \cdots & \vdots \\ * & * & * & \cdots & 0 \end{pmatrix}$$ Then the number of such matrices $A$ is: A) $(4^3)^{10}$ B)$(4^2)^{10}$ C)$4^{10}$ D) $1$ Since all the diagonal elements of $AA^T$ is zero I could realize that it is a skew-symmetric matrix. But, I'm no able to understand how I can use this result for finding the possibilities of the original matrix. I would like hints rather than answers.","If $A$ is a $10\times10$ matrix with entries from the set $\{0, 1, 2, 3\}$ and if $AA^T$ is of the form:   $$\begin{pmatrix} 0 & * & * & \cdots & * \\ * & 0 & * & \cdots & * \\ * & * & 0 & \cdots & * \\ \vdots & \vdots & \vdots & \cdots & \vdots \\ * & * & * & \cdots & 0 \end{pmatrix}$$ Then the number of such matrices $A$ is: A) $(4^3)^{10}$ B)$(4^2)^{10}$ C)$4^{10}$ D) $1$ Since all the diagonal elements of $AA^T$ is zero I could realize that it is a skew-symmetric matrix. But, I'm no able to understand how I can use this result for finding the possibilities of the original matrix. I would like hints rather than answers.",,"['linear-algebra', 'matrices']"
72,"Relation between eigenvectors of covariance matrix and right Singular vectors of SVD, Diagonal matrix","Relation between eigenvectors of covariance matrix and right Singular vectors of SVD, Diagonal matrix",,"I have a $m \times n$ data matrix $X$, ($m$ instances and $n$ features) on which I calculate the Covariance matrix $C$ and perform eigenvalue decomposition. so $C=W \Sigma W'$ where $W$ are the eigenvectors and $\Sigma$ are the eigenvalues arranged in diagonal matrix. Next, I performed SVD (Singular Value Decomposition) of  $X$ , so $X=U \Sigma V'$. Now I noticed a strange thing, the eigenvectors $W$ and the right singular vectors $V$ are equal (at least in magnitude), there were some differences from 4th decimal point in the values (I used Matlab), but I guess that could just be numerical error. Every odd vector in $W$ and $V$ were showing opposite signs, while every even vector of $W$ and $V$ shows same sign !! So why does this happen ?? ie I know that EVD and SVD are somehow connected, from the formulation, it looks like SVD is generalised form of EVD but I am not sure about that. But why does $V$ vectors and $W$ vectors equal in magnitude ? what does this mean intuitively ? I have used PCA and SVD and have seen the term loading matrix used for both $W$ and $V$, so I knew that should be related, But can someone explain this ?? And what is the significance of left singular vectors ? are they useful anywhere ? Finally, I have a question about Diagonal matrices. Wikipedia in its page on orthogonal matrices says that a matrix Q is orthogonal if its transpose is equal to its inverse: or $Q' = Q^{-1}$ or $QQ' = Q'Q = I$. But for diagonal matrices the first equation does not hold true, but the second equation holds true, (this is because the inverse of diagonal matrix is 1/main diagonal elements). So does that mean a diagonal matrix is not orthogonal ? Can someone clarify this difference ?","I have a $m \times n$ data matrix $X$, ($m$ instances and $n$ features) on which I calculate the Covariance matrix $C$ and perform eigenvalue decomposition. so $C=W \Sigma W'$ where $W$ are the eigenvectors and $\Sigma$ are the eigenvalues arranged in diagonal matrix. Next, I performed SVD (Singular Value Decomposition) of  $X$ , so $X=U \Sigma V'$. Now I noticed a strange thing, the eigenvectors $W$ and the right singular vectors $V$ are equal (at least in magnitude), there were some differences from 4th decimal point in the values (I used Matlab), but I guess that could just be numerical error. Every odd vector in $W$ and $V$ were showing opposite signs, while every even vector of $W$ and $V$ shows same sign !! So why does this happen ?? ie I know that EVD and SVD are somehow connected, from the formulation, it looks like SVD is generalised form of EVD but I am not sure about that. But why does $V$ vectors and $W$ vectors equal in magnitude ? what does this mean intuitively ? I have used PCA and SVD and have seen the term loading matrix used for both $W$ and $V$, so I knew that should be related, But can someone explain this ?? And what is the significance of left singular vectors ? are they useful anywhere ? Finally, I have a question about Diagonal matrices. Wikipedia in its page on orthogonal matrices says that a matrix Q is orthogonal if its transpose is equal to its inverse: or $Q' = Q^{-1}$ or $QQ' = Q'Q = I$. But for diagonal matrices the first equation does not hold true, but the second equation holds true, (this is because the inverse of diagonal matrix is 1/main diagonal elements). So does that mean a diagonal matrix is not orthogonal ? Can someone clarify this difference ?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
73,Covariance- v. correlation-matrix based PCA,Covariance- v. correlation-matrix based PCA,,"In principal component analysis (PCA), one can choose either the covariance matrix or the correlation matrix to find the components. These give different results because, I suspect, the eigenvectors between both matrices are not equal. (Mathematically) similar matrices have the same eigenvalues, but not necessarily the same eigenvectors. Several questions: (1) Why this difference? (2) Does PCA make sense, if you can get two different answers? (3) Which of the two methods is 'best'? (4) Since PCA operates on standardized (not) raw data in both cases, i.e., scaled by their standard deviation, does it make sense to use the results to draw conclusions about the dominance of variation for the actual, unstandardized data?","In principal component analysis (PCA), one can choose either the covariance matrix or the correlation matrix to find the components. These give different results because, I suspect, the eigenvectors between both matrices are not equal. (Mathematically) similar matrices have the same eigenvalues, but not necessarily the same eigenvectors. Several questions: (1) Why this difference? (2) Does PCA make sense, if you can get two different answers? (3) Which of the two methods is 'best'? (4) Since PCA operates on standardized (not) raw data in both cases, i.e., scaled by their standard deviation, does it make sense to use the results to draw conclusions about the dominance of variation for the actual, unstandardized data?",,"['linear-algebra', 'statistics', 'eigenvalues-eigenvectors']"
74,How many parameters are required to specify a linear subspace?,How many parameters are required to specify a linear subspace?,,"A problem in Peter Lax's Linear Algebra involves looking at the family of $n\times n$ self-adjoint complex matrices and asking: on how many real parameters does the choice of such a matrix depend? This made me ask: On how many real parameters does a choice of a $k$ -dimensional subspace of $\mathbb{R}^n$ depend? Clearly the $n-1$ -dimensional subspaces as well as the $1$ -dimensional subspaces are an $n-1$ parameter family, since we can specify a $1$ -dimension subspace by choosing a vector up to scaling, and we can choose an $n-1$ -dimensional subspace by choosing its one-dimensional orthogonal complement. This suggests that the formula should be something like ${n\choose k}-1$ . I'm having trouble proving this...any ideas?","A problem in Peter Lax's Linear Algebra involves looking at the family of self-adjoint complex matrices and asking: on how many real parameters does the choice of such a matrix depend? This made me ask: On how many real parameters does a choice of a -dimensional subspace of depend? Clearly the -dimensional subspaces as well as the -dimensional subspaces are an parameter family, since we can specify a -dimension subspace by choosing a vector up to scaling, and we can choose an -dimensional subspace by choosing its one-dimensional orthogonal complement. This suggests that the formula should be something like . I'm having trouble proving this...any ideas?",n\times n k \mathbb{R}^n n-1 1 n-1 1 n-1 {n\choose k}-1,"['linear-algebra', 'vector-spaces']"
75,"Show that if some nontrivial linear combination of vectors $\vec{u}$ and $\vec{v}$ is $\vec{0}$, then $\vec{u}$ and $\vec{v}$ are parallel.","Show that if some nontrivial linear combination of vectors  and  is , then  and  are parallel.",\vec{u} \vec{v} \vec{0} \vec{u} \vec{v},"I've never been that great at writing proofs, but I'm getting a bit better. I think I have the answer correct, but I don't know if I'm missing anything. My logic seems right but there may be some minute detail that I'm leaving out. Can anybody give any feedback on this? Thanks. $\vec{0}$ being a nontrivial linear combination of $\vec{u}$ and $\vec{v}$ implies that there exists a non-zero $a$ or $b$ such that $a\vec{u}=-b\vec{v}$. Without loss of generality, assume $a\neq 0$. Then divide by $a$ and the equality holds: $\vec{u}=-\frac{b}{a}\vec{v}$. And since $-\frac{b}{a}\vec{v}$ is a scalar multiple of $\vec{u}$, it remains that $\vec{u}$ and $\vec{v}$ are parallel. More rigorous proof: \begin{align*} \vec{0}=a\vec{u}+b\vec{v}&\Longrightarrow a\neq 0\vee b\neq 0&&\text{Given}\\ &\Longrightarrow a\vec{u}=-b\vec{v}\\ &\Longrightarrow \vec{u}=-\frac{b}{a}\vec{v}&&\text{WLOG assume $a\neq 0$}\\ &\Longrightarrow \vec{u}\text{ and }\vec{v}\text{ are parallel.} \end{align*}","I've never been that great at writing proofs, but I'm getting a bit better. I think I have the answer correct, but I don't know if I'm missing anything. My logic seems right but there may be some minute detail that I'm leaving out. Can anybody give any feedback on this? Thanks. $\vec{0}$ being a nontrivial linear combination of $\vec{u}$ and $\vec{v}$ implies that there exists a non-zero $a$ or $b$ such that $a\vec{u}=-b\vec{v}$. Without loss of generality, assume $a\neq 0$. Then divide by $a$ and the equality holds: $\vec{u}=-\frac{b}{a}\vec{v}$. And since $-\frac{b}{a}\vec{v}$ is a scalar multiple of $\vec{u}$, it remains that $\vec{u}$ and $\vec{v}$ are parallel. More rigorous proof: \begin{align*} \vec{0}=a\vec{u}+b\vec{v}&\Longrightarrow a\neq 0\vee b\neq 0&&\text{Given}\\ &\Longrightarrow a\vec{u}=-b\vec{v}\\ &\Longrightarrow \vec{u}=-\frac{b}{a}\vec{v}&&\text{WLOG assume $a\neq 0$}\\ &\Longrightarrow \vec{u}\text{ and }\vec{v}\text{ are parallel.} \end{align*}",,"['linear-algebra', 'proof-writing']"
76,Showing equality in Cauchy-Schwarz inequality,Showing equality in Cauchy-Schwarz inequality,,"With $\mathbf{u,v}$ being vectors in $\mathbb{R}^n$ euclidean space, the Cauchy–Schwarz inequality is $$ {\left(\sum_{i=1}^{n} u_i v_i\right)}^2 \leq \left(\sum_{i=1}^{n} u_i^2\right)\left(\sum_{i=1}^{n} v_i^2\right) $$ further given that $\mathbf{u}=\lambda\mathbf{v}$, the csi looks like the following: $$ {\left(\sum_{i=1}^{n} \lambda v_i v_i\right)}^2 \leq \left(\sum_{i=1}^{n} (\lambda v_i)^2\right)\left(\sum_{i=1}^{n} v_i^2\right) $$ With equality applying in the Cauchy-Schwarz inequality only if $\mathbf{u,v}$ are linear dependent, how do I show that equality is given in this case? A start would be enough, I'm quite new to linear algebra Edit: Thanks so far! Rewriting the last line - following your advice - I get the following $$ {\lambda^2\left(\sum_{i=1}^{n} v_i^2\right)}^2 \leq \lambda^2\sum_{i=1}^{n} v_i^2\sum_{i=1}^{n} v_i^2 $$ Okay I'm not sure about the following, so make sure you have foul fruit nearby to throw at me: Canceling $\lambda^2$ this results in $$ {\left(\sum_{i=1}^{n} v_i^2\right)}^2 \leq \sum_{i=1}^{n} v_i^2\sum_{i=1}^{n} v_i^2 $$ with the inquality being wrong, equality applies... is that evidence enough?","With $\mathbf{u,v}$ being vectors in $\mathbb{R}^n$ euclidean space, the Cauchy–Schwarz inequality is $$ {\left(\sum_{i=1}^{n} u_i v_i\right)}^2 \leq \left(\sum_{i=1}^{n} u_i^2\right)\left(\sum_{i=1}^{n} v_i^2\right) $$ further given that $\mathbf{u}=\lambda\mathbf{v}$, the csi looks like the following: $$ {\left(\sum_{i=1}^{n} \lambda v_i v_i\right)}^2 \leq \left(\sum_{i=1}^{n} (\lambda v_i)^2\right)\left(\sum_{i=1}^{n} v_i^2\right) $$ With equality applying in the Cauchy-Schwarz inequality only if $\mathbf{u,v}$ are linear dependent, how do I show that equality is given in this case? A start would be enough, I'm quite new to linear algebra Edit: Thanks so far! Rewriting the last line - following your advice - I get the following $$ {\lambda^2\left(\sum_{i=1}^{n} v_i^2\right)}^2 \leq \lambda^2\sum_{i=1}^{n} v_i^2\sum_{i=1}^{n} v_i^2 $$ Okay I'm not sure about the following, so make sure you have foul fruit nearby to throw at me: Canceling $\lambda^2$ this results in $$ {\left(\sum_{i=1}^{n} v_i^2\right)}^2 \leq \sum_{i=1}^{n} v_i^2\sum_{i=1}^{n} v_i^2 $$ with the inquality being wrong, equality applies... is that evidence enough?",,"['linear-algebra', 'inequality']"
77,"Show $SL(2,\mathbb{Z})$ written as finite product of elements of a particular form",Show  written as finite product of elements of a particular form,"SL(2,\mathbb{Z})","Prove that any element of $SL(2,\mathbb{Z})$ can be represented by a finite product of matrices of the following form. $$\begin{pmatrix}1-ab & a^2\\ -b^2 & 1+ab\end{pmatrix}.$$  We are given that $SL(2,\mathbb{Z})$ is generated by $\begin{pmatrix}1 & 1\\ 0 & 1\end{pmatrix}$ and $ \begin{pmatrix}0 & -1\\ 1 & 0\end{pmatrix}$. When $a=1$, $b=0$ we get the first generator, not sure how to find the second. Its probably very easy but I am having trouble.","Prove that any element of $SL(2,\mathbb{Z})$ can be represented by a finite product of matrices of the following form. $$\begin{pmatrix}1-ab & a^2\\ -b^2 & 1+ab\end{pmatrix}.$$  We are given that $SL(2,\mathbb{Z})$ is generated by $\begin{pmatrix}1 & 1\\ 0 & 1\end{pmatrix}$ and $ \begin{pmatrix}0 & -1\\ 1 & 0\end{pmatrix}$. When $a=1$, $b=0$ we get the first generator, not sure how to find the second. Its probably very easy but I am having trouble.",,"['linear-algebra', 'abstract-algebra', 'group-theory']"
78,Matrix Norm set,Matrix Norm set,,"I need help with this problem: Let $\|\cdot\|$ and $\|\cdot\|^{\prime}$ two matrix norms, and consider the relation $$\|\cdot\| \leq \|\cdot\|^{\prime}\ \Leftrightarrow\ \|A\| \leq \|A\|^{\prime},$$ which provides a partial ordering of the set $\mathcal{N}$ of matrix norms defined over the ring $M_n$. If $\|\cdot\|$ and $\|\cdot\|^{\prime}$ are matrix norms subordinate to the vector norms $|\cdot|$ and $|\cdot|^{\prime}$, respectively, and if $\|A\| \leq \|A\|^{\prime}$ for all matrices $A\in M_n$ of rank 1, show that there exists a constant $c$ such that $$|v|\ =\ c|v|^{\prime},\;\; \mbox{for every vector }v.$$ Show that if a matrix norm is subordinate to two vector norms $|\cdot|$ and $|\cdot|^{\prime}$, then $|v|\ =\ c|v|^{\prime},\;\; \mbox{for every vector }v.$ Somebody knows how solve it? Thanks in advance","I need help with this problem: Let $\|\cdot\|$ and $\|\cdot\|^{\prime}$ two matrix norms, and consider the relation $$\|\cdot\| \leq \|\cdot\|^{\prime}\ \Leftrightarrow\ \|A\| \leq \|A\|^{\prime},$$ which provides a partial ordering of the set $\mathcal{N}$ of matrix norms defined over the ring $M_n$. If $\|\cdot\|$ and $\|\cdot\|^{\prime}$ are matrix norms subordinate to the vector norms $|\cdot|$ and $|\cdot|^{\prime}$, respectively, and if $\|A\| \leq \|A\|^{\prime}$ for all matrices $A\in M_n$ of rank 1, show that there exists a constant $c$ such that $$|v|\ =\ c|v|^{\prime},\;\; \mbox{for every vector }v.$$ Show that if a matrix norm is subordinate to two vector norms $|\cdot|$ and $|\cdot|^{\prime}$, then $|v|\ =\ c|v|^{\prime},\;\; \mbox{for every vector }v.$ Somebody knows how solve it? Thanks in advance",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
79,Finding a subspace whose intersections with other subpaces are trivial.,Finding a subspace whose intersections with other subpaces are trivial.,,"On p.24 of the John M. Lee's Introduction to Smooth Manifolds (2nd ed.), he constructs the smooth structure of the Grassmannian. And when he tries to show Hausdorff condition, he says that for any 2 $k$-dimensional subspaces $P_1$, $P_2$ of $\mathbb{R}^n$, it is always possible to find a $(n-k)$-dimensional subspace whose intersection with both $P_1$ and $P_2$ are trivial. My question: I think it is also intuitively obvious that we can always find $(n-k)$-dimensional subspace whose intersection with m subspaces  $P_1,\ldots,P_m$ are trivial, which is a more generalized situation. But I can't prove it rigorously. Could you help me for this generalized case? Thank you.","On p.24 of the John M. Lee's Introduction to Smooth Manifolds (2nd ed.), he constructs the smooth structure of the Grassmannian. And when he tries to show Hausdorff condition, he says that for any 2 $k$-dimensional subspaces $P_1$, $P_2$ of $\mathbb{R}^n$, it is always possible to find a $(n-k)$-dimensional subspace whose intersection with both $P_1$ and $P_2$ are trivial. My question: I think it is also intuitively obvious that we can always find $(n-k)$-dimensional subspace whose intersection with m subspaces  $P_1,\ldots,P_m$ are trivial, which is a more generalized situation. But I can't prove it rigorously. Could you help me for this generalized case? Thank you.",,"['linear-algebra', 'manifolds']"
80,Visualizing the four subspaces of a matrix,Visualizing the four subspaces of a matrix,,"Given a system of linear equations in the form $$AX=b$$ How can I go about visualizing the four fundamental sub-spaces - column space, row space, null space and left null space? In the same context, how can I visualize the orthogonality of row space and null space, and column space and the left null space?","Given a system of linear equations in the form $$AX=b$$ How can I go about visualizing the four fundamental sub-spaces - column space, row space, null space and left null space? In the same context, how can I visualize the orthogonality of row space and null space, and column space and the left null space?",,['linear-algebra']
81,Direct decomposition of vector space in image of map plus kernel of adjoint,Direct decomposition of vector space in image of map plus kernel of adjoint,,"Let $A:V\to W$ be a linear map with $V,W$ finite dimensional Hilbert spaces. Is it always true that  $$ \dim(\mathrm{Im}(A)) + \dim(\ker(A^*)) = \dim(W),$$ i.e. (since $\mathrm{Im}(A) \cap \ker(A^*) = 0$)  $$W = \mathrm{Im}(A) \oplus \ker (A^*)?$$ Notation: $A^*$ is the adjoint of $A$, $\mathrm{Im}$ and $\ker$ stand for Image and Kernel. I have something like this in mind, but don't find it in my linear algebra notes. Thanks","Let $A:V\to W$ be a linear map with $V,W$ finite dimensional Hilbert spaces. Is it always true that  $$ \dim(\mathrm{Im}(A)) + \dim(\ker(A^*)) = \dim(W),$$ i.e. (since $\mathrm{Im}(A) \cap \ker(A^*) = 0$)  $$W = \mathrm{Im}(A) \oplus \ker (A^*)?$$ Notation: $A^*$ is the adjoint of $A$, $\mathrm{Im}$ and $\ker$ stand for Image and Kernel. I have something like this in mind, but don't find it in my linear algebra notes. Thanks",,"['linear-algebra', 'reference-request']"
82,Injective linear transformation from a vector space to the dual space of its dual space,Injective linear transformation from a vector space to the dual space of its dual space,,I am currently trying to understand some concepts from some Linear Algebra. I seem to be having quite some difficulty understanding dual spaces and their dual spaces. I found this problem and was wondering how to get started on it. Let $V$ be a vector space over the field $F$. Let $V^{*}$ be the dual space of $V$ and let $V^{**}$ be the dual space of $V^{*}$. Show that there is an injective linear transformation  $\phi : V \rightarrow V^{**}$.,I am currently trying to understand some concepts from some Linear Algebra. I seem to be having quite some difficulty understanding dual spaces and their dual spaces. I found this problem and was wondering how to get started on it. Let $V$ be a vector space over the field $F$. Let $V^{*}$ be the dual space of $V$ and let $V^{**}$ be the dual space of $V^{*}$. Show that there is an injective linear transformation  $\phi : V \rightarrow V^{**}$.,,"['linear-algebra', 'abstract-algebra']"
83,A particular (functional) determinant calculation,A particular (functional) determinant calculation,,"One wants to calculate the quantity, $\det'(\frac{\partial}{\partial t} - i [\alpha, ])$ where the prime on the ""det"" means that one wants to do a product over only non-zero eigenvalues of the operator $\frac{\partial}{\partial t} - i [\alpha, ]$. This operator is acting on the adjoint representation of a Lie algebra. ($\alpha$ itself is in the adjoint representation of the Lie algebera) Now one claims that one can find an eigenbasis basis of matrix functions for $\alpha$ such that whose eigenvalues are $\{ \lambda _i \}_{i=1} ^ {i = n}$ and whose $t$ dependence is $\exp(\frac{i2\pi n t}{\beta})$ Can someone write down the exact equation into which the above translates? I would vaguely guess that if $X_i$ is such an eigenvector then because of the adjoint nature of the representation it means that $[\alpha, X_i] = \lambda _ i X_i$ But I don't seem to see exactly where to fix the exponential dependence. Now one claims that in this basis the determinant is equal to the following expression, $$\prod _{n \neq 0} \prod _ {i,j} [ \frac{i2\pi n}{\beta} - i (\lambda _ i - \lambda _j)]$$ and the above can apparently be simplified to give, $$\det'(\frac{\partial}{\partial t} - i [\alpha, ]) = \left ( \prod _{m \neq 0} \frac{i2\pi m}{\beta} \right )\prod _ {i,j} \frac{2}{\beta (\lambda _i - \lambda _j)}\sin (\frac{\beta (\lambda _ i - \lambda _j)}{2})$$ It would be great if someone can help explain the above simplification.","One wants to calculate the quantity, $\det'(\frac{\partial}{\partial t} - i [\alpha, ])$ where the prime on the ""det"" means that one wants to do a product over only non-zero eigenvalues of the operator $\frac{\partial}{\partial t} - i [\alpha, ]$. This operator is acting on the adjoint representation of a Lie algebra. ($\alpha$ itself is in the adjoint representation of the Lie algebera) Now one claims that one can find an eigenbasis basis of matrix functions for $\alpha$ such that whose eigenvalues are $\{ \lambda _i \}_{i=1} ^ {i = n}$ and whose $t$ dependence is $\exp(\frac{i2\pi n t}{\beta})$ Can someone write down the exact equation into which the above translates? I would vaguely guess that if $X_i$ is such an eigenvector then because of the adjoint nature of the representation it means that $[\alpha, X_i] = \lambda _ i X_i$ But I don't seem to see exactly where to fix the exponential dependence. Now one claims that in this basis the determinant is equal to the following expression, $$\prod _{n \neq 0} \prod _ {i,j} [ \frac{i2\pi n}{\beta} - i (\lambda _ i - \lambda _j)]$$ and the above can apparently be simplified to give, $$\det'(\frac{\partial}{\partial t} - i [\alpha, ]) = \left ( \prod _{m \neq 0} \frac{i2\pi m}{\beta} \right )\prod _ {i,j} \frac{2}{\beta (\lambda _i - \lambda _j)}\sin (\frac{\beta (\lambda _ i - \lambda _j)}{2})$$ It would be great if someone can help explain the above simplification.",,"['linear-algebra', 'functional-analysis', 'representation-theory', 'physics', 'determinant']"
84,Sieve of Atkin - algorithm for enumerating lattice points.,Sieve of Atkin - algorithm for enumerating lattice points.,,"Recently, I've been working towards implementing the Sieve of Atkin with significantly better performance than the version found on Wikipedia . From reading the original paper ( http://www.ams.org/mcom/2004-73-246/S0025-5718-03-01501-1/S0025-5718-03-01501-1.pdf ), it seems the author takes a different approach than Wikipedia: whereas Wikipedia's code is attempting to find solutions to each quadratic form by testing all possible values of x and y, the author instead (see page 1026, Algorithm 4.1-4.3 in the paper linked above) does the following: If we are trying to sieve primes out of numbers in the range [L, L+B) (i.e., the arithmetic progression of B numbers starting at L), he considers the annulus described by each quadratic form (take $4x^2 + y^2$ for instance): $60L \le 4x^2 + y^2 < 60L + 60B$ He then provides an algorithm which should enumerate all triples (x, y, k) for which: $4x^2 + y^2 = 60k + \delta$ My question is regarding this algorithm. Although the paper treats it as such, it is not obvious to me why we have the restriction (x mod 15, y mod 30), or how f and g are picked - he says in the description of the algorithm: Given positive integers $\delta < 60$, $f \le 15$, and $g \le 30$ such that $\delta \equiv 4f^2 + g^2$ (mod 60), ... Does this mean that we can simply pick one particular pair (f, g) for which the congruence holds, or if more than one pair exists does one need to enumerate over all pairs in order to generate all triples (x, y, k)?","Recently, I've been working towards implementing the Sieve of Atkin with significantly better performance than the version found on Wikipedia . From reading the original paper ( http://www.ams.org/mcom/2004-73-246/S0025-5718-03-01501-1/S0025-5718-03-01501-1.pdf ), it seems the author takes a different approach than Wikipedia: whereas Wikipedia's code is attempting to find solutions to each quadratic form by testing all possible values of x and y, the author instead (see page 1026, Algorithm 4.1-4.3 in the paper linked above) does the following: If we are trying to sieve primes out of numbers in the range [L, L+B) (i.e., the arithmetic progression of B numbers starting at L), he considers the annulus described by each quadratic form (take $4x^2 + y^2$ for instance): $60L \le 4x^2 + y^2 < 60L + 60B$ He then provides an algorithm which should enumerate all triples (x, y, k) for which: $4x^2 + y^2 = 60k + \delta$ My question is regarding this algorithm. Although the paper treats it as such, it is not obvious to me why we have the restriction (x mod 15, y mod 30), or how f and g are picked - he says in the description of the algorithm: Given positive integers $\delta < 60$, $f \le 15$, and $g \le 30$ such that $\delta \equiv 4f^2 + g^2$ (mod 60), ... Does this mean that we can simply pick one particular pair (f, g) for which the congruence holds, or if more than one pair exists does one need to enumerate over all pairs in order to generate all triples (x, y, k)?",,"['linear-algebra', 'number-theory', 'algorithms']"
85,"Showing that the dual space of bilinear maps $V \times W \to \mathbb{R}$ satisfies the tensor product property, for finite dimensional vector spaces.","Showing that the dual space of bilinear maps  satisfies the tensor product property, for finite dimensional vector spaces.",V \times W \to \mathbb{R},"Let $U,V$ and $W$ be finite dimensional vector spaces, and define $B$ to be the vector space of all bilinear maps $V \times W \to \mathbb{R}$. Given a bilinear map $\alpha : V \times W \rightarrow U$, define $\tilde{\alpha}: B^* \rightarrow U^{**}$ by $\alpha(\psi)(\sigma) = \psi (\sigma \circ \alpha)$. Define a map $\pi : V \times W \rightarrow B^*$ by $\pi(v,w) (f:V \times W \rightarrow  \mathbb{R}) = f(v,w).$ $\mathbf{CORRECTION:}$ $B$ should be the space of bilinear maps $V \times W \to \mathbb{R}$, not $V \times W \to U$ as previously stated. In order to show that $B^*$ satifies the universal property of the tensor product, I have to show that given a map $\alpha : V \times W \rightarrow U$, then there is a unique $\tilde{\alpha} : B^* \rightarrow U^{**}$ such that $\Theta \circ \tilde{\alpha} \circ \pi = \alpha$, where $\Theta:U^{**} \to U$ is the canonical isomorphism. It is quite clear that $\tilde{\alpha}$ defined above satisfies this property, but I am having trouble proving uniqueness. I would like to show that given $f:B^* \to U^{**}$ such that $\Theta\circ f \circ \pi = \alpha$, then $f= \tilde{\alpha}$, however I am getting nowhere. Any help would be appreciated, thank you.","Let $U,V$ and $W$ be finite dimensional vector spaces, and define $B$ to be the vector space of all bilinear maps $V \times W \to \mathbb{R}$. Given a bilinear map $\alpha : V \times W \rightarrow U$, define $\tilde{\alpha}: B^* \rightarrow U^{**}$ by $\alpha(\psi)(\sigma) = \psi (\sigma \circ \alpha)$. Define a map $\pi : V \times W \rightarrow B^*$ by $\pi(v,w) (f:V \times W \rightarrow  \mathbb{R}) = f(v,w).$ $\mathbf{CORRECTION:}$ $B$ should be the space of bilinear maps $V \times W \to \mathbb{R}$, not $V \times W \to U$ as previously stated. In order to show that $B^*$ satifies the universal property of the tensor product, I have to show that given a map $\alpha : V \times W \rightarrow U$, then there is a unique $\tilde{\alpha} : B^* \rightarrow U^{**}$ such that $\Theta \circ \tilde{\alpha} \circ \pi = \alpha$, where $\Theta:U^{**} \to U$ is the canonical isomorphism. It is quite clear that $\tilde{\alpha}$ defined above satisfies this property, but I am having trouble proving uniqueness. I would like to show that given $f:B^* \to U^{**}$ such that $\Theta\circ f \circ \pi = \alpha$, then $f= \tilde{\alpha}$, however I am getting nowhere. Any help would be appreciated, thank you.",,"['linear-algebra', 'vector-spaces', 'tensor-products']"
86,How many possibilities for eigenvectors are there for one eigenvalue?,How many possibilities for eigenvectors are there for one eigenvalue?,,"If I have a $2 \times 2$ matrix $A$, and then I find two eigenvalues $\lambda_1$ and $\lambda_2$ by subtracting $λI$ from $A$ and then taking the determinant=0(singular); to find $\lambda_1$ and $\lambda_2$. So for a one eigenvalue $\lambda_1$, how many possibilities are there for eigenvectors? in another words, how many solutions are there?","If I have a $2 \times 2$ matrix $A$, and then I find two eigenvalues $\lambda_1$ and $\lambda_2$ by subtracting $λI$ from $A$ and then taking the determinant=0(singular); to find $\lambda_1$ and $\lambda_2$. So for a one eigenvalue $\lambda_1$, how many possibilities are there for eigenvectors? in another words, how many solutions are there?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
87,Rank of an interesting matrix,Rank of an interesting matrix,,"Lets define: $U=\left \{  u_j\right \} , 1 \leq j\leq N= 2^{L},$ the set of all different binary sequences of length $L$. $V=\left \{  v_i\right \} , 1 \leq i\leq M=\binom{L}{k}2^{k},$ the set of all different gaped binary sequences with $k$ known bits and $L-k$ gaps. $A_{M*N}=[a_{i,j}]$ is a binary matrix defined as following: $$a_{i,j} = \left\{\begin{matrix} 1 & \text{if } v_i \text{ matches } u_j\\  0 & \text{otherwise } \end{matrix}\right.$$ and finally, $S_{M*M}=AA^{T}$ now, the question is that: i) What is the rank of matrix $S$ ? ii) What is the eigen decomposition for $S$ ? Here is an example for $L=2, k=1$: $$U = \left \{  00,01,10,11\right \} $$ $$V = \left \{  0.,1.,.0,.1\right \}  ^*$$ $$ A = \begin{bmatrix} 1 & 1 & 0 &0 \\  0 & 0 & 1 &1 \\  1 & 0 & 1 &0 \\  0 & 1 & 0 &1  \end{bmatrix}$$ $$ S = \begin{bmatrix} 2 & 0 & 1 &1 \\  0 & 2 & 1 &1 \\  1 & 1 & 2 &0 \\  1 & 1 & 0 &2  \end{bmatrix}$$ For the special case $k=1$, this has been previously solved by joriki and the solution can be found here . any comments or suggestion is appreciated. $^{*}$ here dots denote gaps. a gap can take any value, and each gaped sequence with $k$ known bits and $(L−K)$ gaps in $V$, exactly matches to $2^{L−k}$ sequences in U, hence the sum of elements in each row of $A$ is $2^{L−k}$.","Lets define: $U=\left \{  u_j\right \} , 1 \leq j\leq N= 2^{L},$ the set of all different binary sequences of length $L$. $V=\left \{  v_i\right \} , 1 \leq i\leq M=\binom{L}{k}2^{k},$ the set of all different gaped binary sequences with $k$ known bits and $L-k$ gaps. $A_{M*N}=[a_{i,j}]$ is a binary matrix defined as following: $$a_{i,j} = \left\{\begin{matrix} 1 & \text{if } v_i \text{ matches } u_j\\  0 & \text{otherwise } \end{matrix}\right.$$ and finally, $S_{M*M}=AA^{T}$ now, the question is that: i) What is the rank of matrix $S$ ? ii) What is the eigen decomposition for $S$ ? Here is an example for $L=2, k=1$: $$U = \left \{  00,01,10,11\right \} $$ $$V = \left \{  0.,1.,.0,.1\right \}  ^*$$ $$ A = \begin{bmatrix} 1 & 1 & 0 &0 \\  0 & 0 & 1 &1 \\  1 & 0 & 1 &0 \\  0 & 1 & 0 &1  \end{bmatrix}$$ $$ S = \begin{bmatrix} 2 & 0 & 1 &1 \\  0 & 2 & 1 &1 \\  1 & 1 & 2 &0 \\  1 & 1 & 0 &2  \end{bmatrix}$$ For the special case $k=1$, this has been previously solved by joriki and the solution can be found here . any comments or suggestion is appreciated. $^{*}$ here dots denote gaps. a gap can take any value, and each gaped sequence with $k$ known bits and $(L−K)$ gaps in $V$, exactly matches to $2^{L−k}$ sequences in U, hence the sum of elements in each row of $A$ is $2^{L−k}$.",,"['linear-algebra', 'matrices', 'graph-theory', 'computer-science']"
88,"Show that every real matrix $B$ such that $AB = BA$ has the form $B = aI + bA + cA^2$ for some real numbers $a$, $b$ and $c$.","Show that every real matrix  such that  has the form  for some real numbers ,  and .",B AB = BA B = aI + bA + cA^2 a b c,"I am trying to solve the following problem: Let $$A = \begin{pmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{pmatrix}.$$ Show that every real matrix $B$ such that $AB = BA$ has the form $$B = aI + bA + cA^2$$ for some real numbers $a$ , $b$ and $c$ . My attempt: Let us consider the set $$V=\{B\in\mathcal{M}_{3\times 3}(\mathbb{R}) : AB = BA\}.$$ It is easy to prove that $V$ is a subspace of $\mathcal{M}_{3\times 3}(\mathbb{R})$ . Likewise, we can verify that $\mathcal{B}=\{I,A,A^2\}$ is a linear independent subset of $V$ , because $x$ , $Ax$ , and $A^2x$ are linear independent vectors for $x=(1,0,0)^T$ . If we proved that the dimension of $V$ is $3$ , then $\mathcal{B}$ would be a basis of that subspace, and the proof would be done. However, I don't know how to show it. Can you give me an advice to do it, or tell me a different way to solve the problem?","I am trying to solve the following problem: Let Show that every real matrix such that has the form for some real numbers , and . My attempt: Let us consider the set It is easy to prove that is a subspace of . Likewise, we can verify that is a linear independent subset of , because , , and are linear independent vectors for . If we proved that the dimension of is , then would be a basis of that subspace, and the proof would be done. However, I don't know how to show it. Can you give me an advice to do it, or tell me a different way to solve the problem?","A = \begin{pmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{pmatrix}. B AB = BA B = aI + bA + cA^2 a b c V=\{B\in\mathcal{M}_{3\times 3}(\mathbb{R}) : AB = BA\}. V \mathcal{M}_{3\times 3}(\mathbb{R}) \mathcal{B}=\{I,A,A^2\} V x Ax A^2x x=(1,0,0)^T V 3 \mathcal{B}","['linear-algebra', 'matrices', 'vector-spaces']"
89,Finding a maximal vector space in a finite set,Finding a maximal vector space in a finite set,,"Let $X \subseteq \{0,1\}^n$ be a nonempty set of vectors of length $n \geq 1$ with binary components such that the zero vector $\vec{0}$ having all components equal to $0$ belongs to $X$ . Along with the sum $\oplus$ defined as the component-wise xor (modulo 2 sum) of two vectors (i.e. consider the usual GF(2) field), i.e. let $x_i, y_i$ be the $i$ -th component of $\vec{x}, \vec{j}$ respectively, then $$ (\vec{x} \oplus \vec{y})_i :=\quad  x_i + y_i\quad\text{(mod 2)} $$ I stumbled upon the problem of finding (one of) the biggest (w.r.t. set inclusion) vector spaces contained in $X$ . I tried to look around but I don't seem to be able to find any mention of this problem in literature, not even in its more general formulation involving arbitrary vectors (thus not restricting to binary components and the component-wise modulo 2 sum). (the considerations below on how I feel about the problem being NP-complete, and about the bounds are inexact and refer to a slightly different problem, which I asked here: Finding a biggest (in size) vector space in a finite set ) While the ""dual"" problem of finding the smallest vector subspace containing $X$ is well-known, and easy to solve, I feel like this problem of finding the biggest subset which is a vector space constitutes a harder problem, in terms of computational complexity. What can we say about the complexity of this problem? Is there any reduction from a NP-complete problem to the above problem? Is there any efficient way to solve this problem? Any information about this problem is welcome , I find no mentions of it in literature. Update with some more (useful) informations: The existence of a solution is obvious, since the set $\{\vec{0}\}$ is a vector space contained in $X$ by definition, and there are finitely many subsets of $X$ (since $X$ is finite). Note that it might not be unique, if multiple maximal spaces exist in $X$ , finding one of them is enough. It would also be useful to find some bounds on the size of the solution, for example, if $|X| \geq 2$ then we can infer that any solution has at least dimension 1. Maybe this reasoning extends to a general bound? If I were to guess such a bound would probably end up being very loose.","Let be a nonempty set of vectors of length with binary components such that the zero vector having all components equal to belongs to . Along with the sum defined as the component-wise xor (modulo 2 sum) of two vectors (i.e. consider the usual GF(2) field), i.e. let be the -th component of respectively, then I stumbled upon the problem of finding (one of) the biggest (w.r.t. set inclusion) vector spaces contained in . I tried to look around but I don't seem to be able to find any mention of this problem in literature, not even in its more general formulation involving arbitrary vectors (thus not restricting to binary components and the component-wise modulo 2 sum). (the considerations below on how I feel about the problem being NP-complete, and about the bounds are inexact and refer to a slightly different problem, which I asked here: Finding a biggest (in size) vector space in a finite set ) While the ""dual"" problem of finding the smallest vector subspace containing is well-known, and easy to solve, I feel like this problem of finding the biggest subset which is a vector space constitutes a harder problem, in terms of computational complexity. What can we say about the complexity of this problem? Is there any reduction from a NP-complete problem to the above problem? Is there any efficient way to solve this problem? Any information about this problem is welcome , I find no mentions of it in literature. Update with some more (useful) informations: The existence of a solution is obvious, since the set is a vector space contained in by definition, and there are finitely many subsets of (since is finite). Note that it might not be unique, if multiple maximal spaces exist in , finding one of them is enough. It would also be useful to find some bounds on the size of the solution, for example, if then we can infer that any solution has at least dimension 1. Maybe this reasoning extends to a general bound? If I were to guess such a bound would probably end up being very loose.","X \subseteq \{0,1\}^n n \geq 1 \vec{0} 0 X \oplus x_i, y_i i \vec{x}, \vec{j} 
(\vec{x} \oplus \vec{y})_i :=\quad  x_i + y_i\quad\text{(mod 2)}
 X X \{\vec{0}\} X X X X |X| \geq 2","['linear-algebra', 'vector-spaces', 'finite-fields', 'computational-complexity', 'np-complete']"
90,"Can there be ""polynomial spaces""?","Can there be ""polynomial spaces""?",,"I don't know how to better frame this question. Thinking about vector spaces and their role in basically everything Calculus touched, I can understand why they are so central, especially in areas like differential geometry. But Taylor's Theorem got me thinking; my personal view of what Taylor's Theorem tells us is that the derivative is not a ""one ocurrence"" phenomenon, it is a part of a larger set of objects that approximate the function to a polynomial. That is, the derivative is not the most central object, in a fundamental way, just the most convenient I guess? Since it is easier to deal with linear functions than with, say, quadratic or cubic functions, and of course, it is easier to work with vector spaces than with other kinds of spaces. Considering that vector spaces' structure is, in some sense, fine tuned, so that linear applications preserve their structure, what space would be such that quadratic applications preserve its structure? Or cubic applications? Are there such ""polynomial"" spaces? What properties might they have? Are they useful? My guess is that in principle they would be useful, because just as we can approximate functions to linear or quadratic functions and build powerful analysis from that, one might guess that we could approximate non-linear spaces with ""sums"" of these ""polynomial spaces"", perhaps even yielding a notion of ""derivatives for a space""? This might be a meaningless question but I would appreciate some insight. EDIT : For illustrating what I mean. Consider a vector space $V$ . Then a linear map $T$ from $V$ to another vector space preserves the structure of $V$ a vector space, that is, $$T(\alpha u+\beta v)=\alpha T(u)+\beta T(v), u,v\in V$$ Or in other words, the image of the vector space $V$ is also a vector space. What I'm looking for is a space $Q$ such that quadratic applications would have the property: $$T(\alpha u+\beta v)=\alpha^2 u^2+2\alpha \beta uv+\beta^2v^2, u,v \in Q$$ Or in other words, the image of Q by T is also a ""----"" space, that is, it preserves its structure, whichever it might be.","I don't know how to better frame this question. Thinking about vector spaces and their role in basically everything Calculus touched, I can understand why they are so central, especially in areas like differential geometry. But Taylor's Theorem got me thinking; my personal view of what Taylor's Theorem tells us is that the derivative is not a ""one ocurrence"" phenomenon, it is a part of a larger set of objects that approximate the function to a polynomial. That is, the derivative is not the most central object, in a fundamental way, just the most convenient I guess? Since it is easier to deal with linear functions than with, say, quadratic or cubic functions, and of course, it is easier to work with vector spaces than with other kinds of spaces. Considering that vector spaces' structure is, in some sense, fine tuned, so that linear applications preserve their structure, what space would be such that quadratic applications preserve its structure? Or cubic applications? Are there such ""polynomial"" spaces? What properties might they have? Are they useful? My guess is that in principle they would be useful, because just as we can approximate functions to linear or quadratic functions and build powerful analysis from that, one might guess that we could approximate non-linear spaces with ""sums"" of these ""polynomial spaces"", perhaps even yielding a notion of ""derivatives for a space""? This might be a meaningless question but I would appreciate some insight. EDIT : For illustrating what I mean. Consider a vector space . Then a linear map from to another vector space preserves the structure of a vector space, that is, Or in other words, the image of the vector space is also a vector space. What I'm looking for is a space such that quadratic applications would have the property: Or in other words, the image of Q by T is also a ""----"" space, that is, it preserves its structure, whichever it might be.","V T V V T(\alpha u+\beta v)=\alpha T(u)+\beta T(v), u,v\in V V Q T(\alpha u+\beta v)=\alpha^2 u^2+2\alpha \beta uv+\beta^2v^2, u,v \in Q",['linear-algebra']
91,Is a basis that almost diagonalizes a matrix 'close' to its eigenbasis?,Is a basis that almost diagonalizes a matrix 'close' to its eigenbasis?,,"Let $A\in\mathbb{R}^{n\times n}$ be diagonalisable (over $\mathbb{C}$ ) with pairwise distinct eigenvalues $\lambda_1,\ldots,\lambda_n\in\mathbb{C}$ , and suppose that $$\tag{1}S^{-1}\cdot A\cdot S = \mathrm{diag}[\lambda_1,\cdots,\lambda_d]=:\Lambda \qquad \text{for some invertible } S\in\mathbb{C}^{n\times n}.$$ Assume that there is another matrix $T\in\mathbb{R}^{n\times n}$ which 'quasi-diagonalises' $A$ in the sense that $$\tag{2}T^{-1}\cdot A\cdot T =\Lambda + \Delta \qquad \text{for some} \qquad \Delta\in\mathbb{R}^{n\times n} \ \text{ with } \ \|\Delta\|<\varepsilon$$ where $\varepsilon>0$ is 'small' and $\|\cdot\|$ is a matrix norm of your choice (on $\mathbb{C}^{n\times n}$ ). Question: Can we infer that for $\varepsilon>0$ small enough , the matrices $S$ and $T$ are close to one another in the sense that $$\tag{3}\mathrm{inf}\{\|T\cdot S^{-1} - D\cdot P\| \mid \text{$D\in\mathbb{C}^{n\times n}$ diagonal & invertible}, \  \text{$P$ permutation matrix}\} \ \lesssim \ \|\Delta\|$$ In other words, can we infer that for $\Lambda$ and $\tilde{\Lambda}:=\Lambda + \Delta$ almost identical , the columns of $S$ and $T$ (which define a basis for the almost identical representations $\Lambda$ and $\tilde{\Lambda}$ of the endomorphism $A$ ) almost coincide up to order and scale? Any references or hints, or indeed counterexamples, are appreciated. Edit: As demonstrated in Ruy's answer, the desired conclusion doesn't apply without additional conditions on $A$ ; are you aware of any such conditions that are as mild as possible?","Let be diagonalisable (over ) with pairwise distinct eigenvalues , and suppose that Assume that there is another matrix which 'quasi-diagonalises' in the sense that where is 'small' and is a matrix norm of your choice (on ). Question: Can we infer that for small enough , the matrices and are close to one another in the sense that In other words, can we infer that for and almost identical , the columns of and (which define a basis for the almost identical representations and of the endomorphism ) almost coincide up to order and scale? Any references or hints, or indeed counterexamples, are appreciated. Edit: As demonstrated in Ruy's answer, the desired conclusion doesn't apply without additional conditions on ; are you aware of any such conditions that are as mild as possible?","A\in\mathbb{R}^{n\times n} \mathbb{C} \lambda_1,\ldots,\lambda_n\in\mathbb{C} \tag{1}S^{-1}\cdot A\cdot S = \mathrm{diag}[\lambda_1,\cdots,\lambda_d]=:\Lambda \qquad \text{for some invertible } S\in\mathbb{C}^{n\times n}. T\in\mathbb{R}^{n\times n} A \tag{2}T^{-1}\cdot A\cdot T =\Lambda + \Delta \qquad \text{for some} \qquad \Delta\in\mathbb{R}^{n\times n} \ \text{ with } \ \|\Delta\|<\varepsilon \varepsilon>0 \|\cdot\| \mathbb{C}^{n\times n} \varepsilon>0 S T \tag{3}\mathrm{inf}\{\|T\cdot S^{-1} - D\cdot P\| \mid \text{D\in\mathbb{C}^{n\times n} diagonal & invertible}, \  \text{P permutation matrix}\} \ \lesssim \ \|\Delta\| \Lambda \tilde{\Lambda}:=\Lambda + \Delta S T \Lambda \tilde{\Lambda} A A","['linear-algebra', 'matrices', 'functional-analysis', 'numerical-methods', 'linear-transformations']"
92,Representing a linear transformation as a matrix in terms of a given basis,Representing a linear transformation as a matrix in terms of a given basis,,"I am new to linear algebra, I need help in understanding how to represent a linear transformation into standard basis of a matrix Consider $M_{2}(\mathbb{R}),$ the vector space of all $2 \times 2$ real matrices.  Let $$ A=\left(\begin{array}{cc} 1 & -1 \\ -1 & 1 \end{array}\right) $$ and if we  define $\mathcal{A}(B)=A B$ for any $B \in M_{2}(\mathbb{R})$ . Show that $\mathcal{A}$ is a linear transformation on $M_{2}(\mathbb{R})$ and find the matrix of $\mathcal{A}$ under the basis $E_{i j}, i, j=1,2$ I can show the linearity part by considering the action of this linear transformation on matrix $B+ \lambda C$ , in fact I know this will be true for any matrix $A$ . But how to represent this in terms of given basis. Note here I have taken basis $E_{i j}$ be the $2 \times 2$ matrix with $(i, j)^{\text {th }}$ entry 1 and other entries 0.","I am new to linear algebra, I need help in understanding how to represent a linear transformation into standard basis of a matrix Consider the vector space of all real matrices.  Let and if we  define for any . Show that is a linear transformation on and find the matrix of under the basis I can show the linearity part by considering the action of this linear transformation on matrix , in fact I know this will be true for any matrix . But how to represent this in terms of given basis. Note here I have taken basis be the matrix with entry 1 and other entries 0.","M_{2}(\mathbb{R}), 2 \times 2 
A=\left(\begin{array}{cc}
1 & -1 \\
-1 & 1
\end{array}\right)
 \mathcal{A}(B)=A B B \in M_{2}(\mathbb{R}) \mathcal{A} M_{2}(\mathbb{R}) \mathcal{A} E_{i j}, i, j=1,2 B+ \lambda C A E_{i j} 2 \times 2 (i, j)^{\text {th }}","['linear-algebra', 'matrices', 'linear-transformations', 'change-of-basis']"
93,Let $T:\textbf{R}^{n}\to\textbf{R}^{m}$ be a linear transformation. Show that there exists a number $M > 0$ such that $\|Tx\|\leq M\|x\|$.,Let  be a linear transformation. Show that there exists a number  such that .,T:\textbf{R}^{n}\to\textbf{R}^{m} M > 0 \|Tx\|\leq M\|x\|,"Let $T:\textbf{R}^{n}\to\textbf{R}^{m}$ be a linear transformation. Show that there exists a number $M > 0$ such that $\|Tx\|\leq M\|x\|$ . Conclude in particular that every linear transformation from $\textbf{R}^{n}$ to $\textbf{R}^{m}$ is continuous. My solution Let $[T]_{\mathcal{B}}^{\mathcal{B}'} = [T(e_{1})^{T},T(e_{2})^{T},\ldots,T(e_{n})^{T}]$ , where $\mathcal{B} = \{e_{1},e_{2},\ldots,e_{n}\}$ and $\mathcal{B}' = \{f_{1},f_{2},\ldots,f_{m}\}$ . Thus, according to the triangle inequality as well as the Cauchy-Schwarz inequality, we have that \begin{align*} \|Tx\| & = \|x_{1}T(e_{1})^{T} + x_{2}T(e_{2})^{T} + \ldots + x_{n}T(e_{n})^{T}\|\\\\ & \leq|x_{1}|\|T(e_{1})^{T}\| + |x_{2}|\|T(e_{2})^{T}\| + \ldots + |x_{n}|\|T(e_{n})^{T}\|\\\\ & = \langle(|x_{1}|,|x_{2}|,\ldots,|x_{n}|),(\|T(e_{1})^{T}\|,\|T(e_{2})^{t}\|,\ldots,\|T(e_{n})^{T}\|)\rangle\\\\ & \leq \sqrt{|x_{1}|^{2} + |x_{2}|^{2} + \ldots + |x_{n}|^{2}}\sqrt{\|T(e_{1})^{T}\|^{2} + \|T(e_{2})^{T}\|^{2} + \ldots + \|T(e_{n})^{T}\|^{2}} = M\|x\| \end{align*} where $M = \sqrt{\|T(e_{1})^{T}\|^{2} + \|T(e_{2})^{T}\|^{2} + \ldots + \|T(e_{n})^{T}\|^{2}}$ and we assume that $T\neq 0$ . If $T = 0$ , then any $M + 1$ does the job. Consequently, $T$ is continuous because it is Lipschitz. I would like to know if I am reasoning correctly. Any other solution is equally welcome.","Let be a linear transformation. Show that there exists a number such that . Conclude in particular that every linear transformation from to is continuous. My solution Let , where and . Thus, according to the triangle inequality as well as the Cauchy-Schwarz inequality, we have that where and we assume that . If , then any does the job. Consequently, is continuous because it is Lipschitz. I would like to know if I am reasoning correctly. Any other solution is equally welcome.","T:\textbf{R}^{n}\to\textbf{R}^{m} M > 0 \|Tx\|\leq M\|x\| \textbf{R}^{n} \textbf{R}^{m} [T]_{\mathcal{B}}^{\mathcal{B}'} = [T(e_{1})^{T},T(e_{2})^{T},\ldots,T(e_{n})^{T}] \mathcal{B} = \{e_{1},e_{2},\ldots,e_{n}\} \mathcal{B}' = \{f_{1},f_{2},\ldots,f_{m}\} \begin{align*}
\|Tx\| & = \|x_{1}T(e_{1})^{T} + x_{2}T(e_{2})^{T} + \ldots + x_{n}T(e_{n})^{T}\|\\\\
& \leq|x_{1}|\|T(e_{1})^{T}\| + |x_{2}|\|T(e_{2})^{T}\| + \ldots + |x_{n}|\|T(e_{n})^{T}\|\\\\
& = \langle(|x_{1}|,|x_{2}|,\ldots,|x_{n}|),(\|T(e_{1})^{T}\|,\|T(e_{2})^{t}\|,\ldots,\|T(e_{n})^{T}\|)\rangle\\\\
& \leq \sqrt{|x_{1}|^{2} + |x_{2}|^{2} + \ldots + |x_{n}|^{2}}\sqrt{\|T(e_{1})^{T}\|^{2} + \|T(e_{2})^{T}\|^{2} + \ldots + \|T(e_{n})^{T}\|^{2}}
= M\|x\|
\end{align*} M = \sqrt{\|T(e_{1})^{T}\|^{2} + \|T(e_{2})^{T}\|^{2} + \ldots + \|T(e_{n})^{T}\|^{2}} T\neq 0 T = 0 M + 1 T","['linear-algebra', 'solution-verification', 'cauchy-schwarz-inequality']"
94,Taylor expansion of a matrix exponential,Taylor expansion of a matrix exponential,,"Given two non-commuting matrices $X$ and $Y$ , consider the following quantity $$ e^{X+\lambda Y} $$ What is the Taylor series expansion of the above quantity about $\lambda=0$ ? Note that this is non-trivial since $X$ and $Y$ do not commute and is not simply $e^X(1+\lambda Y+ \lambda^2 Y/2 + \cdots)$ . One way to proceed is perhaps the Zassenhaus formula , which expresses the above quantity as an infinite product of matrix exponentials $$e^{(X+\lambda Y)}= e^{X}~ e^{\lambda Y} ~e^{-\frac{1}{2} \lambda[X,Y]} ~ e^{\frac{1}{6}(2\lambda^2[Y,[X,Y]]+\lambda [X,[X,Y]] )} ~ e^{\frac{-t^4}{24}(\lambda[[[X,Y],X],X] + 3\lambda^2[[[X,Y],X],Y] + 3\lambda^3[[[X,Y],Y],Y]) }\cdots$$ It can be seen that even at the first order in $\lambda$ in the Taylor expansion, there are  contributions from each of the exponentials. The question is : can these terms in the Taylor series be written in closed form, order-by-order? PS : I would eventually be interested in the trace, Tr[ $e^{X+\lambda Y}$ ].","Given two non-commuting matrices and , consider the following quantity What is the Taylor series expansion of the above quantity about ? Note that this is non-trivial since and do not commute and is not simply . One way to proceed is perhaps the Zassenhaus formula , which expresses the above quantity as an infinite product of matrix exponentials It can be seen that even at the first order in in the Taylor expansion, there are  contributions from each of the exponentials. The question is : can these terms in the Taylor series be written in closed form, order-by-order? PS : I would eventually be interested in the trace, Tr[ ].","X Y  e^{X+\lambda Y}  \lambda=0 X Y e^X(1+\lambda Y+ \lambda^2 Y/2 + \cdots) e^{(X+\lambda Y)}= e^{X}~ e^{\lambda Y} ~e^{-\frac{1}{2} \lambda[X,Y]} ~
e^{\frac{1}{6}(2\lambda^2[Y,[X,Y]]+\lambda [X,[X,Y]] )} ~
e^{\frac{-t^4}{24}(\lambda[[[X,Y],X],X] + 3\lambda^2[[[X,Y],X],Y] + 3\lambda^3[[[X,Y],Y],Y]) }\cdots \lambda e^{X+\lambda Y}","['linear-algebra', 'matrices', 'matrix-calculus', 'noncommutative-algebra', 'matrix-exponential']"
95,Does the Riemannian metric induced by a diffeomorphism $F$ exist for a reason other than the existence of vector field pushforwards?,Does the Riemannian metric induced by a diffeomorphism  exist for a reason other than the existence of vector field pushforwards?,F,"My book is Connections, Curvature, and Characteristic Classes by Loring W. Tu (I'll call this Volume 3), a sequel to both Differential Forms in Algebraic Topology by Loring W. Tu and Raoul Bott (Volume 2) and An Introduction to Manifolds by Loring W. Tu (Volume 1). Definition 1.5 gives the definition for Riemannian metric and Riemannian manifold. Example 1.9 says If $F : N \to M$ is a diffeomorphism and $< , >$ is a Riemannian metric on $M$ , then (1.3) defines an induced Riemannian metric $< , >'$ on $N$ . Here $N$ and $M$ are smooth manifolds that hopefully have dimensions . Note that the $F_*$ here indeed refers to the differential $F_*,p: T_pN \to T_{F(p)}M$ defined in Volume 1 Section 8.2 and not the latter half $F_*: TN \to TM$ of the bundle map $(F, F_*)$ , where $F_*$ is what would be known as $\tilde{F}$ in Volume 1 Section 12.3 . The following is my proof of Example 1.9 . Question 1: Is this proof correct? Question 2: If this proof is correct , then is there a way to do this without relying on pushforwards from Volume 1 or without injectivity of $F$ ? I guess we can come up with a similar proof for an embedding, but embeddings are injective. So we'll have to go with investigating local diffeomorphisms, local diffeomorphisms onto image, immersions, etc. I'm asking because the Example 1.10 seems to do similarly to Example 1.9 though the $F$ in Example 1.10 is not injective. If this proof is incorrect , then why? Proof: Notation from Volume 1 Section 2.4 : For a smooth manifold $N$ , let $\mathfrak X (N)$ be the set of smooth vector fields on $N$ , and let $C^{\infty}N$ be the set of smooth functions on $N$ (not germs ). We must show that A. (Not interested in proving this part, but I'm stating what is to be proven for completeness) For all $p \in N$ , the mapping $\langle , \rangle'_p: (T_pN)^2 \to \mathbb R$ is an inner product on $T_pN$ , where $\langle , \rangle'_p$ is given as follows: Let $u,v \in T_pN$ . Then $F_{*,p}u, F_{*,p}v \in T_{F(p)}M$ . Let $\langle , \rangle_{F(p)}: (T_{F(p)}M)^2 \to \mathbb R$ be the inner product on $T_{F(p)}M$ given by the Riemannian metric $\langle , \rangle$ on $M$ , at the point $F(p) \in M$ . Then $(\langle , \rangle'_p)(u,v) = \langle u, v \rangle'_p = \langle F_{*,p}u, F_{*,p}v \rangle_{F(p)}$ . B. $\langle X,Y\rangle' \in C^{\infty}N$ for all $X,Y \in \mathfrak X (N)$ , where $\langle X,Y\rangle': N \to \mathbb R$ , $\langle X,Y \rangle'(p)=\langle X_p,Y_p\rangle'_p$ $=\langle F_{*,p}X_p,F_{*,p}Y_p\rangle_{F(p)}$ . To prove B: Let $X,Y \in \mathfrak X (N)$ . Then, by Volume 1 Example 14.15 , $F_{*}X$ and $F_{*}Y$ are defined vector fields on $M$ . Hopefully, $F_{*}X$ and $F_{*}Y$ are smooth, i.e. $F_{*}X,F_{*}Y \in \mathfrak X (M)$ . (I ask about this step here .) $\langle A, B \rangle \in C^{\infty} M$ for all $A,B \in \mathfrak X(M)$ , by definition of $\langle , \rangle$ for $M$ ( Definition 1.5 ). $\langle F_{*}X,F_{*}Y \rangle \in C^{\infty}M$ , from (2) and (3). $\langle X,Y\rangle' = \langle F_{*}X,F_{*}Y \rangle \circ F$ , i.e. $\langle X,Y\rangle'$ is the pullback by $F$ of $\langle F_{*}X,F_{*}Y \rangle$ $\langle X,Y\rangle' \in C^{\infty}N$ , by Volume 1 Proposition 6.9 , by (4) and by smoothness of $F$ .","My book is Connections, Curvature, and Characteristic Classes by Loring W. Tu (I'll call this Volume 3), a sequel to both Differential Forms in Algebraic Topology by Loring W. Tu and Raoul Bott (Volume 2) and An Introduction to Manifolds by Loring W. Tu (Volume 1). Definition 1.5 gives the definition for Riemannian metric and Riemannian manifold. Example 1.9 says If is a diffeomorphism and is a Riemannian metric on , then (1.3) defines an induced Riemannian metric on . Here and are smooth manifolds that hopefully have dimensions . Note that the here indeed refers to the differential defined in Volume 1 Section 8.2 and not the latter half of the bundle map , where is what would be known as in Volume 1 Section 12.3 . The following is my proof of Example 1.9 . Question 1: Is this proof correct? Question 2: If this proof is correct , then is there a way to do this without relying on pushforwards from Volume 1 or without injectivity of ? I guess we can come up with a similar proof for an embedding, but embeddings are injective. So we'll have to go with investigating local diffeomorphisms, local diffeomorphisms onto image, immersions, etc. I'm asking because the Example 1.10 seems to do similarly to Example 1.9 though the in Example 1.10 is not injective. If this proof is incorrect , then why? Proof: Notation from Volume 1 Section 2.4 : For a smooth manifold , let be the set of smooth vector fields on , and let be the set of smooth functions on (not germs ). We must show that A. (Not interested in proving this part, but I'm stating what is to be proven for completeness) For all , the mapping is an inner product on , where is given as follows: Let . Then . Let be the inner product on given by the Riemannian metric on , at the point . Then . B. for all , where , . To prove B: Let . Then, by Volume 1 Example 14.15 , and are defined vector fields on . Hopefully, and are smooth, i.e. . (I ask about this step here .) for all , by definition of for ( Definition 1.5 ). , from (2) and (3). , i.e. is the pullback by of , by Volume 1 Proposition 6.9 , by (4) and by smoothness of .","F : N \to M < , > M < , >' N N M F_* F_*,p: T_pN \to T_{F(p)}M F_*: TN \to TM (F, F_*) F_* \tilde{F} F F N \mathfrak X (N) N C^{\infty}N N p \in N \langle , \rangle'_p: (T_pN)^2 \to \mathbb R T_pN \langle , \rangle'_p u,v \in T_pN F_{*,p}u, F_{*,p}v \in T_{F(p)}M \langle , \rangle_{F(p)}: (T_{F(p)}M)^2 \to \mathbb R T_{F(p)}M \langle , \rangle M F(p) \in M (\langle , \rangle'_p)(u,v) = \langle u, v \rangle'_p = \langle F_{*,p}u, F_{*,p}v \rangle_{F(p)} \langle X,Y\rangle' \in C^{\infty}N X,Y \in \mathfrak X (N) \langle X,Y\rangle': N \to \mathbb R \langle X,Y \rangle'(p)=\langle X_p,Y_p\rangle'_p =\langle F_{*,p}X_p,F_{*,p}Y_p\rangle_{F(p)} X,Y \in \mathfrak X (N) F_{*}X F_{*}Y M F_{*}X F_{*}Y F_{*}X,F_{*}Y \in \mathfrak X (M) \langle A, B \rangle \in C^{\infty} M A,B \in \mathfrak X(M) \langle , \rangle M \langle F_{*}X,F_{*}Y \rangle \in C^{\infty}M \langle X,Y\rangle' = \langle F_{*}X,F_{*}Y \rangle \circ F \langle X,Y\rangle' F \langle F_{*}X,F_{*}Y \rangle \langle X,Y\rangle' \in C^{\infty}N F","['linear-algebra', 'geometry']"
96,Prove all roots of $p_n(x)-x$ are real and distinct,Prove all roots of  are real and distinct,p_n(x)-x,"Given a polynomial series $\{p_n(x)\}_{n=1}^{\infty}$ in $\mathbb{R}[X]$ with initial value $p_1(x)=x^2-2$ . And $p_k(x)=p_1(p_{k-1}(x))=p_{k-1}(x)^2-2,\;k=2,3,\cdots$ . Prove that for each integer $n$ , all roots of $p_n(x)-x$ are real and distinct. This is an problem in my linear algebra textbook. I tried to figure out the relation of the roots between adjacent polynomial, but I couldn't find any useful result. Also I thought if it could be solved by induction, but it seems impracticable.","Given a polynomial series in with initial value . And . Prove that for each integer , all roots of are real and distinct. This is an problem in my linear algebra textbook. I tried to figure out the relation of the roots between adjacent polynomial, but I couldn't find any useful result. Also I thought if it could be solved by induction, but it seems impracticable.","\{p_n(x)\}_{n=1}^{\infty} \mathbb{R}[X] p_1(x)=x^2-2 p_k(x)=p_1(p_{k-1}(x))=p_{k-1}(x)^2-2,\;k=2,3,\cdots n p_n(x)-x","['linear-algebra', 'polynomials', 'roots']"
97,"If for some $n\in\mathbb{N}$, $T^{n+1}\left( X\right) =T^{n}\left( X\right)$ do we have $T^{n}\left(X\right) $ closed?","If for some ,  do we have  closed?",n\in\mathbb{N} T^{n+1}\left( X\right) =T^{n}\left( X\right) T^{n}\left(X\right) ,"Let $X$ be a Banach space, and let $T$ be a bounded operator on $X$ such that for some $n\in  %TCIMACRO{\U{2115} }% %BeginExpansion \mathbb{N} %EndExpansion $ , $T^{n+1}\left( X\right) =T^{n}\left( X\right) $ . Do we have $T^{n}\left( X\right) $ closed ?","Let be a Banach space, and let be a bounded operator on such that for some , . Do we have closed ?","X T X n\in 
%TCIMACRO{\U{2115} }%
%BeginExpansion
\mathbb{N}
%EndExpansion
 T^{n+1}\left( X\right) =T^{n}\left( X\right)  T^{n}\left( X\right) ","['linear-algebra', 'functional-analysis', 'operator-theory', 'banach-spaces']"
98,How to prove the range of $AA^T$ is the same as range of $A$?,How to prove the range of  is the same as range of ?,AA^T A,"I have seen quite a number of questions regarding similar issues, like this and this . However, all the answers were trying to approach the topic via a non-straightforward way, that is to prove the statement by proving $N(A) = N(AA^T)$ . This method is fine and do be easy to understand. But I am actually wondering if there is a straightforward way that we can prove this? Like if we assume $x \in R(A)$ , then if we can somehow show $x \in R(AA^T)$ holds, we proved the statement. I'd like to do this but can't quite push $x \in R(A)$ towards $x \in R(AA^T)$ .","I have seen quite a number of questions regarding similar issues, like this and this . However, all the answers were trying to approach the topic via a non-straightforward way, that is to prove the statement by proving . This method is fine and do be easy to understand. But I am actually wondering if there is a straightforward way that we can prove this? Like if we assume , then if we can somehow show holds, we proved the statement. I'd like to do this but can't quite push towards .",N(A) = N(AA^T) x \in R(A) x \in R(AA^T) x \in R(A) x \in R(AA^T),"['linear-algebra', 'matrices', 'linear-transformations']"
99,"Given $A\in\Bbb R^{n\times n}$, is $C_A := \{SAS^{-1} : S\in GL(n,\mathbb R)\}$ connected?","Given , is  connected?","A\in\Bbb R^{n\times n} C_A := \{SAS^{-1} : S\in GL(n,\mathbb R)\}","Let us define $\phi : GL(n,\Bbb R)\to C_A$, $\;\phi(S) = SAS^{-1}$ and the sets $$ E_\pm := \{S\in GL(n,\Bbb R) : \pm\det S > 0\}. $$ If $n$ is odd, then $\phi(E_+) = \phi(E_-) = C_A$ (because $\phi(-S) = \phi(S)$) and hence $C_A$ is connected. But what about even $n$? EDIT: I just saw that Lemma 1. If $\det A < 0$, then still $\phi(E_+) = \phi(E_-)$ and hence $C_A$ is connected. Proof. Indeed, if $T\in\phi(E_-)$, $T = \phi(S_0)$, $\det S_0 < 0$, then $\det(S_0A) > 0$ and $T = \phi(S_0A)\in\phi(E_+)$. The other inclusion is proved similarly. So the question reduces to $n$ even and $\det A\ge 0$. EDIT2: Here is another fact. Lemma 2. If there is $S_0\in E_-$ that commutes with $A$, then $C_A$ is connected. Proof. Let $T\in C_A$. Let us show that we can find a path within $C_A$ from $T$ to $A$. Let $T = SAS^{-1}$ with $S\in GL(n,\Bbb R)$. If $S\in E_+$, we find a path from $S$ to $I$ in $E_+$ and hence a path in $C_A$ from $T$ to $A$. If $S\in E_-$, we find a path within $E_-$ from $S$ to $S_0$. Its image under $\phi$ is again a path from $T$ to $A$ within $C_A$. EDIT 3: For arbitrary $\lambda\in\Bbb R$ we have $C_{A-\lambda I} = C_A - \lambda I$. As this is just a translation in $\Bbb R^{n\times n}$ of $C_A$ by $\lambda I$, it follows that $C_A$ is connected if and only if $C_{A-\lambda I}$ is connected. Therefore we can conclude the following: Let $J$ be the real Jordan form of $A$. Then $C_A = C_J$. If $A$ has a real eigenvalue $\lambda_0$ which appears in $J$ in a $k\times k$ Jordan block with $k$ odd, then $C_A$ is connected. Indeed, due to the above, we can shift $A$ and $J$ simultaneously and thus assume that $A$ and $J$ are invertible. Let $\tilde J$ be $J$, but with $-\lambda_0$'s instead of $\lambda_0$'s on the diagonal of the $k\times k$ Jordan block. Then $\tilde J$ commutes with $J$ and hence so does $\tilde JJ$. Since $\det(\tilde JJ) < 0$, $C_J = C_A$ is connected by Lemma 2. We summarize for the critical matrices: In the real Jordan form each Jordan block corresponding to a real eigenvalue has size $k\times k$ with $k$ even. I conjecture that the following are equivalent: $C_A$ is connected There exists $S\in E_-$ that commutes with $A$. There exists a Jordan block $J$ of $A$ for which $C_J$ is connected. There exists a real odd-sized Jordan block of $A$. I could only prove (4)$\Rightarrow$(3), (3)$\Rightarrow$(1), and (2)$\Rightarrow$(1) so far. Remark: This question is related to and motivated by Connectedness of matrix conjugacy classes of a fixed real $A$ but with the first column of $A$ invariant","Let us define $\phi : GL(n,\Bbb R)\to C_A$, $\;\phi(S) = SAS^{-1}$ and the sets $$ E_\pm := \{S\in GL(n,\Bbb R) : \pm\det S > 0\}. $$ If $n$ is odd, then $\phi(E_+) = \phi(E_-) = C_A$ (because $\phi(-S) = \phi(S)$) and hence $C_A$ is connected. But what about even $n$? EDIT: I just saw that Lemma 1. If $\det A < 0$, then still $\phi(E_+) = \phi(E_-)$ and hence $C_A$ is connected. Proof. Indeed, if $T\in\phi(E_-)$, $T = \phi(S_0)$, $\det S_0 < 0$, then $\det(S_0A) > 0$ and $T = \phi(S_0A)\in\phi(E_+)$. The other inclusion is proved similarly. So the question reduces to $n$ even and $\det A\ge 0$. EDIT2: Here is another fact. Lemma 2. If there is $S_0\in E_-$ that commutes with $A$, then $C_A$ is connected. Proof. Let $T\in C_A$. Let us show that we can find a path within $C_A$ from $T$ to $A$. Let $T = SAS^{-1}$ with $S\in GL(n,\Bbb R)$. If $S\in E_+$, we find a path from $S$ to $I$ in $E_+$ and hence a path in $C_A$ from $T$ to $A$. If $S\in E_-$, we find a path within $E_-$ from $S$ to $S_0$. Its image under $\phi$ is again a path from $T$ to $A$ within $C_A$. EDIT 3: For arbitrary $\lambda\in\Bbb R$ we have $C_{A-\lambda I} = C_A - \lambda I$. As this is just a translation in $\Bbb R^{n\times n}$ of $C_A$ by $\lambda I$, it follows that $C_A$ is connected if and only if $C_{A-\lambda I}$ is connected. Therefore we can conclude the following: Let $J$ be the real Jordan form of $A$. Then $C_A = C_J$. If $A$ has a real eigenvalue $\lambda_0$ which appears in $J$ in a $k\times k$ Jordan block with $k$ odd, then $C_A$ is connected. Indeed, due to the above, we can shift $A$ and $J$ simultaneously and thus assume that $A$ and $J$ are invertible. Let $\tilde J$ be $J$, but with $-\lambda_0$'s instead of $\lambda_0$'s on the diagonal of the $k\times k$ Jordan block. Then $\tilde J$ commutes with $J$ and hence so does $\tilde JJ$. Since $\det(\tilde JJ) < 0$, $C_J = C_A$ is connected by Lemma 2. We summarize for the critical matrices: In the real Jordan form each Jordan block corresponding to a real eigenvalue has size $k\times k$ with $k$ even. I conjecture that the following are equivalent: $C_A$ is connected There exists $S\in E_-$ that commutes with $A$. There exists a Jordan block $J$ of $A$ for which $C_J$ is connected. There exists a real odd-sized Jordan block of $A$. I could only prove (4)$\Rightarrow$(3), (3)$\Rightarrow$(1), and (2)$\Rightarrow$(1) so far. Remark: This question is related to and motivated by Connectedness of matrix conjugacy classes of a fixed real $A$ but with the first column of $A$ invariant",,"['linear-algebra', 'general-topology']"
