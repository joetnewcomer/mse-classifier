,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Spivak Calculus on Manifolds, Theorem 5-2","Spivak Calculus on Manifolds, Theorem 5-2",,"In the proof Theorem 5-2 of Spivak Calculus on Mannifolds how is \begin{align*} V_2\cap M=\{f(a):(a,0)\in V_1\}? \end{align*} (That $\{f(a):(a,0)\in V_1\}=\{g(a,0):(a,0)\in V_1\}$ is clear.) Edit :  Due to a comment, Theorem 5-2 proves the equivalence of the following two definitions of a $k$-dimensional regular submanifold in $\mathbb{R}^n$. Definition 1 .  A subset $M\subset\mathbb{R}^n$ is a $k$-dimensional submanifold if for every point $x\in M$ the following condition is satisfied: (M)  There is an open set $U\subset \mathbb R^n$ containing $x$, an open set $V\subset\mathbb{R}^n$, and a diffeomorphism $h:U\rightarrow V$ such that \begin{align*} h(U\cap M)&=V\cap(\mathbb{R}^k\times\{0\})\\ &=\{y\in V:y^{k+1}=\dots=y^n\}. \end{align*} Definition 2 .  A subset $M\subset\mathbb{R}^n$ is a $k$-dimensional submanifold if for every point $x\in M$ the following ""coordinate condition"" is satisfied: (C) There is an open set $U\subset \mathbb R^n$ containing $x$, an open set $W\subset\mathbb{R}^k$, and an injective differentiable function $f:W\rightarrow\mathbb{R}^n$ satisfying: $f(W)=M\cap U$. $f'(y)$ has maximum rank $k$ for each $y\in W$. $f^{-1}:f(W)\rightarrow W$ is continuous. The part of the proof mentioned above is proving Definition 2 $\Rightarrow$ Definition 1.","In the proof Theorem 5-2 of Spivak Calculus on Mannifolds how is \begin{align*} V_2\cap M=\{f(a):(a,0)\in V_1\}? \end{align*} (That $\{f(a):(a,0)\in V_1\}=\{g(a,0):(a,0)\in V_1\}$ is clear.) Edit :  Due to a comment, Theorem 5-2 proves the equivalence of the following two definitions of a $k$-dimensional regular submanifold in $\mathbb{R}^n$. Definition 1 .  A subset $M\subset\mathbb{R}^n$ is a $k$-dimensional submanifold if for every point $x\in M$ the following condition is satisfied: (M)  There is an open set $U\subset \mathbb R^n$ containing $x$, an open set $V\subset\mathbb{R}^n$, and a diffeomorphism $h:U\rightarrow V$ such that \begin{align*} h(U\cap M)&=V\cap(\mathbb{R}^k\times\{0\})\\ &=\{y\in V:y^{k+1}=\dots=y^n\}. \end{align*} Definition 2 .  A subset $M\subset\mathbb{R}^n$ is a $k$-dimensional submanifold if for every point $x\in M$ the following ""coordinate condition"" is satisfied: (C) There is an open set $U\subset \mathbb R^n$ containing $x$, an open set $W\subset\mathbb{R}^k$, and an injective differentiable function $f:W\rightarrow\mathbb{R}^n$ satisfying: $f(W)=M\cap U$. $f'(y)$ has maximum rank $k$ for each $y\in W$. $f^{-1}:f(W)\rightarrow W$ is continuous. The part of the proof mentioned above is proving Definition 2 $\Rightarrow$ Definition 1.",,"['calculus', 'multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
1,Multivariable Calculus Course Extra Credit Challenge,Multivariable Calculus Course Extra Credit Challenge,,"I am allowed to consult other resources/individuals as long as I mention your name/position on the problem sheet. I have no idea how to even begin. This is well beyond the scope of my course. My instructor wants his students to interact/discuss this with the mathematics community. Here it is: Let Ω be a convex region in R2 and let L be a line segment of length ι that connects points on the boundary of Ω. As we move one end of L around the boundary, the other end will also move about this boundary, and the midpoint of L will trace out a curve within Ω that bounds a (smaller) region Γ. Find an expression that relates the area of Γ to the area of Ω in terms of the length ι.","I am allowed to consult other resources/individuals as long as I mention your name/position on the problem sheet. I have no idea how to even begin. This is well beyond the scope of my course. My instructor wants his students to interact/discuss this with the mathematics community. Here it is: Let Ω be a convex region in R2 and let L be a line segment of length ι that connects points on the boundary of Ω. As we move one end of L around the boundary, the other end will also move about this boundary, and the midpoint of L will trace out a curve within Ω that bounds a (smaller) region Γ. Find an expression that relates the area of Γ to the area of Ω in terms of the length ι.",,['multivariable-calculus']
2,"Inner product, differential forms and surfaces (Stokes' theorem)","Inner product, differential forms and surfaces (Stokes' theorem)",,"I'm trying to understand how do you get the Kelvin-Stokes theorem \begin{equation} \int_{S} (\nabla\times \omega) \cdot \mathrm{d}S = \int_{\partial S} \omega \cdot \mathrm{d}r \end{equation} from the generalized Stokes' theorem \begin{equation} \int_{M} \mathrm{d} \omega = \int_{\partial M} \omega \end{equation} I am aware that this question is answered here . However, I am looking at a more specific part of the question. If I start looking at a vector $\omega = a \mathrm{d}x +  b \mathrm{d}y +  c \mathrm{d}z$, and take it's exterior derivative $\mathrm{d}\omega = \left(\partial_x b - \partial_y a \right) \mathrm{d}x \wedge \mathrm{d}y + \left(\partial_z c - \partial_y b \right) \mathrm{d}x \wedge \mathrm{d}y +\left(\partial_x c - \partial_z a \right) \mathrm{d}x \wedge \mathrm{d}z$ and notice that in the basis $\{ \mathrm{d}x^i \wedge \mathrm{d} x^j \}$ this is the cross product. But how to go from here - how to identify this with the inner product with $\mathrm{d}S$. How to even write what are the surface and line elements in general?","I'm trying to understand how do you get the Kelvin-Stokes theorem \begin{equation} \int_{S} (\nabla\times \omega) \cdot \mathrm{d}S = \int_{\partial S} \omega \cdot \mathrm{d}r \end{equation} from the generalized Stokes' theorem \begin{equation} \int_{M} \mathrm{d} \omega = \int_{\partial M} \omega \end{equation} I am aware that this question is answered here . However, I am looking at a more specific part of the question. If I start looking at a vector $\omega = a \mathrm{d}x +  b \mathrm{d}y +  c \mathrm{d}z$, and take it's exterior derivative $\mathrm{d}\omega = \left(\partial_x b - \partial_y a \right) \mathrm{d}x \wedge \mathrm{d}y + \left(\partial_z c - \partial_y b \right) \mathrm{d}x \wedge \mathrm{d}y +\left(\partial_x c - \partial_z a \right) \mathrm{d}x \wedge \mathrm{d}z$ and notice that in the basis $\{ \mathrm{d}x^i \wedge \mathrm{d} x^j \}$ this is the cross product. But how to go from here - how to identify this with the inner product with $\mathrm{d}S$. How to even write what are the surface and line elements in general?",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
3,Gauss-Green Theorem from generalized Stoke's Theorem.,Gauss-Green Theorem from generalized Stoke's Theorem.,,"I am trying to deduce the next identity (Green-Gauss theorem)  $$\int_\Omega \dfrac{\partial u}{\partial x_i} dx = \int_{\partial \Omega} uv_i dS$$ from the generalized Stoke's theorem for manifolds. Here $u$ is a $C^\infty$ function on an open bounded set $\Omega$ with $C^1$ boundary and $v_i$ denotes the i-th component of the unit normal vector field. I've been trying to define $n-1$ forms on $\Omega$ whose derivatives are $\dfrac{\partial u}{\partial x_i} dx$ to apply the theorem but I cannot prove that those forms have the same integral as $uv_i$ on $\partial \Omega$. Any ideas? Also, I would really appreciate references of the proofs of classical calculus integration results using the generalized Stoke's theorem. Thanks in advance.","I am trying to deduce the next identity (Green-Gauss theorem)  $$\int_\Omega \dfrac{\partial u}{\partial x_i} dx = \int_{\partial \Omega} uv_i dS$$ from the generalized Stoke's theorem for manifolds. Here $u$ is a $C^\infty$ function on an open bounded set $\Omega$ with $C^1$ boundary and $v_i$ denotes the i-th component of the unit normal vector field. I've been trying to define $n-1$ forms on $\Omega$ whose derivatives are $\dfrac{\partial u}{\partial x_i} dx$ to apply the theorem but I cannot prove that those forms have the same integral as $uv_i$ on $\partial \Omega$. Any ideas? Also, I would really appreciate references of the proofs of classical calculus integration results using the generalized Stoke's theorem. Thanks in advance.",,"['integration', 'multivariable-calculus', 'manifolds-with-boundary']"
4,Integration against divergence free vector fields,Integration against divergence free vector fields,,"Let $\chi:\Omega\to \mathbb{R}^n$ be a vector field on a bounded, smooth domain $\Omega \subset \mathbb{R}^n$. Assume that for any divergence free vector field $\eta:\Omega \to \mathbb{R}^n$ we have $ \int_\Omega \eta \cdot \chi =0 $ Does this imply that $\chi = 0$ ?","Let $\chi:\Omega\to \mathbb{R}^n$ be a vector field on a bounded, smooth domain $\Omega \subset \mathbb{R}^n$. Assume that for any divergence free vector field $\eta:\Omega \to \mathbb{R}^n$ we have $ \int_\Omega \eta \cdot \chi =0 $ Does this imply that $\chi = 0$ ?",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
5,Area of projected surface,Area of projected surface,,"Suppose I have a surface given by $z=f(x,t)$, $a\le x \le b$ and $c\le t \le d$. How can I find the area of the surface projected onto the $(x,z)$ plane? We may assume appropriate monotonicity conditions to have a nice projection.","Suppose I have a surface given by $z=f(x,t)$, $a\le x \le b$ and $c\le t \le d$. How can I find the area of the surface projected onto the $(x,z)$ plane? We may assume appropriate monotonicity conditions to have a nice projection.",,"['calculus', 'multivariable-calculus']"
6,Explanation of differential forms and notation,Explanation of differential forms and notation,,"I'm doing multivariable calculus and I'd love if someone could shed some light on things that confuse me. When we did integrals of real functions with real variables, the $dx$ that was in every integral wasn't really explained. It basically only had syntactical meaning. Now, it seems like the semantic meaning is what I'm having trouble with. What does $da$ where $a$ is any symbol actually mean? Sometimes it looks like it's the differential of a functions (if $a$ is a function), sometimes it's just a projector to a specific coordinate and sometimes it looks like it's just syntax and it's ignored. For example, when we did integrals like $\int xdx+x^2ydy$ and looked for a functions that produced that differential form we treated $xdx$ as, ""when we calculate the partial derivative over $x$ of function $f$ we get $x$"" (might not be correct due to bad English translation) but then when we looked at the ""change in angle form"" we treated $dx$ and $dy$ as something we need to calculate. Also, where does the $\partial$ symbol come into play? People usually read it as just $d$ so I'm wondering how related are those? I know this seems like I'm asking many question but what I'd really like to know is some intuitive explanation of the whole concept. Maybe someone else that was in my shoes before but ""gets it"" now can provide some tips?","I'm doing multivariable calculus and I'd love if someone could shed some light on things that confuse me. When we did integrals of real functions with real variables, the $dx$ that was in every integral wasn't really explained. It basically only had syntactical meaning. Now, it seems like the semantic meaning is what I'm having trouble with. What does $da$ where $a$ is any symbol actually mean? Sometimes it looks like it's the differential of a functions (if $a$ is a function), sometimes it's just a projector to a specific coordinate and sometimes it looks like it's just syntax and it's ignored. For example, when we did integrals like $\int xdx+x^2ydy$ and looked for a functions that produced that differential form we treated $xdx$ as, ""when we calculate the partial derivative over $x$ of function $f$ we get $x$"" (might not be correct due to bad English translation) but then when we looked at the ""change in angle form"" we treated $dx$ and $dy$ as something we need to calculate. Also, where does the $\partial$ symbol come into play? People usually read it as just $d$ so I'm wondering how related are those? I know this seems like I'm asking many question but what I'd really like to know is some intuitive explanation of the whole concept. Maybe someone else that was in my shoes before but ""gets it"" now can provide some tips?",,"['multivariable-calculus', 'differential-forms']"
7,Clarifying definition of outward unit normal,Clarifying definition of outward unit normal,,"I would like to figure out how to properly define the outward unit normal vector $\nu$ for a bounded domain $\Omega \subset \mathbb{R}^n$ with smooth boundary $\partial \Omega$ ($n \ge 2$). I am following Gilbarg and Trudinger's Elliptic Partial Differential Equations of Second Order . This function $\nu$ doesn't seem to be defined anywhere in Gilbarg and Trudinger, but it is used often, and even at the beginning of the text, when the divergence theorem is stated. First, I will start with a definition from section 6.2 in Gilbarg and Trudinger: Let $\Omega \subset \mathbb{R}^n$ be a bounded domain. We say $\partial \Omega$ is of class $C^k$ if and only if, for each $x_0 \in \partial \Omega$, there is a open ball $B(x_0) = B$ and an bijective function $\psi \colon B \to D \subseteq \mathbb{R}^n$ so that: $\psi(B \cap \Omega) \subseteq \mathbb{R}^n_+ = \{x \in \mathbb{R}^n | x_n > 0\}$, $\psi(B \cap \partial \Omega) \subseteq \partial \mathbb{R}^n_+ = \{x \in \mathbb{R}^n | x_n = 0\}$, $\psi \in C^k(B), \psi^{-1} \in C^k(D)$. Note: D is open in $\mathbb{R}^n$ by invariance of domain. Now, starting with this definition, I would like to come to a suitable definition for the outward unit normal (i.e., a definition that makes the divergence theorem hold). Here's what I have so far. For $x_0 \in \partial \Omega$, we can let $\psi = (\psi_1, \dots \psi_n) : B \to D$ be as in the definition above, and then notice that $B \cap \partial \Omega$ is a level set of the function $\psi_n$ ($\psi_n \equiv 0$ on $B \cap \partial \Omega$). Now, from multivariable calculus, I remember learning that $\nabla \psi_n(x_0)$ ""points perpendicular"" to the level set $B \cap \partial \Omega$. Intuitively, this seems to be to the property that we want $\nu$ to possess, so it seems like a good attempt to define  $\nu(x_0) = \frac{\nabla \psi_n(x_0)}{||\nabla \psi_n(x_0)||}$. However, I see two issues with trying to make this our definition for $\nu$. How do we know that $\nabla \psi_n (x_0) \neq 0$, so that we can make it into a unit vector? Since the map $\psi$ corresponding to a particular ball is not necessarily unique, how do we know that $\nu$ is well defined? Any comments are greatly appreciated!","I would like to figure out how to properly define the outward unit normal vector $\nu$ for a bounded domain $\Omega \subset \mathbb{R}^n$ with smooth boundary $\partial \Omega$ ($n \ge 2$). I am following Gilbarg and Trudinger's Elliptic Partial Differential Equations of Second Order . This function $\nu$ doesn't seem to be defined anywhere in Gilbarg and Trudinger, but it is used often, and even at the beginning of the text, when the divergence theorem is stated. First, I will start with a definition from section 6.2 in Gilbarg and Trudinger: Let $\Omega \subset \mathbb{R}^n$ be a bounded domain. We say $\partial \Omega$ is of class $C^k$ if and only if, for each $x_0 \in \partial \Omega$, there is a open ball $B(x_0) = B$ and an bijective function $\psi \colon B \to D \subseteq \mathbb{R}^n$ so that: $\psi(B \cap \Omega) \subseteq \mathbb{R}^n_+ = \{x \in \mathbb{R}^n | x_n > 0\}$, $\psi(B \cap \partial \Omega) \subseteq \partial \mathbb{R}^n_+ = \{x \in \mathbb{R}^n | x_n = 0\}$, $\psi \in C^k(B), \psi^{-1} \in C^k(D)$. Note: D is open in $\mathbb{R}^n$ by invariance of domain. Now, starting with this definition, I would like to come to a suitable definition for the outward unit normal (i.e., a definition that makes the divergence theorem hold). Here's what I have so far. For $x_0 \in \partial \Omega$, we can let $\psi = (\psi_1, \dots \psi_n) : B \to D$ be as in the definition above, and then notice that $B \cap \partial \Omega$ is a level set of the function $\psi_n$ ($\psi_n \equiv 0$ on $B \cap \partial \Omega$). Now, from multivariable calculus, I remember learning that $\nabla \psi_n(x_0)$ ""points perpendicular"" to the level set $B \cap \partial \Omega$. Intuitively, this seems to be to the property that we want $\nu$ to possess, so it seems like a good attempt to define  $\nu(x_0) = \frac{\nabla \psi_n(x_0)}{||\nabla \psi_n(x_0)||}$. However, I see two issues with trying to make this our definition for $\nu$. How do we know that $\nabla \psi_n (x_0) \neq 0$, so that we can make it into a unit vector? Since the map $\psi$ corresponding to a particular ball is not necessarily unique, how do we know that $\nu$ is well defined? Any comments are greatly appreciated!",,"['multivariable-calculus', 'partial-differential-equations']"
8,Application of Implicit Function Theorem,Application of Implicit Function Theorem,,"So I'm a set of practice problems regarding this but I don't quite understand how to think about this... Example of a problem: $x^3 (y^3 +z^3 )=0$ and $(x-y)^3 -z^2 -7=0$ A point lies on the surfaces $(1,-1,1)$ Show that in a neighborhood of this point, the curve of intersection of the surfaces can be described by a pair of equations $y=f(x)$, $z=g(x)$. Now we just covered the inverse function theorem and implicit function theorem for multi-variable vector-valued functions. I believe somehow I need to use the implicit function theorem..... but I don't know how.... for instance: let $F(x,y,z)= (x^3 (y^3 +z^3) , (x-y)^3 -z^2 -7)$ Using the implicit function theorem: $$\frac{\partial (f_1,f_2)}{\partial  y\partial  z}=      \begin{bmatrix} ∂f_1/∂y &  ∂f_1/∂z    \\  ∂f_2/∂y  & ∂f_2/∂z  \end{bmatrix} $$ Now: $\det [∂(f_1,f_2)/∂y∂z] = -6(x^3 y^2 z)+(9z^2 x^3 (x-y)^2 )$. Evaluated at the point $(1,-1,1)$ we find determinant not equal to zero. Thus we may apply the Implicit Function theorem. And now I'm confused. I feel that I should somehow set $z,y$ equal to something but its not clear what I should set equal to what... My book's proof of the theorem (which oddly enough contains no examples of application in this context) using cramer's rule [linear algebra tool which I understand] but I don't see how this would help. So I know I should be apply use to the implicit function theorem to find a $y=f(x)$ and $z=g(x)$ but I really don't see how.","So I'm a set of practice problems regarding this but I don't quite understand how to think about this... Example of a problem: $x^3 (y^3 +z^3 )=0$ and $(x-y)^3 -z^2 -7=0$ A point lies on the surfaces $(1,-1,1)$ Show that in a neighborhood of this point, the curve of intersection of the surfaces can be described by a pair of equations $y=f(x)$, $z=g(x)$. Now we just covered the inverse function theorem and implicit function theorem for multi-variable vector-valued functions. I believe somehow I need to use the implicit function theorem..... but I don't know how.... for instance: let $F(x,y,z)= (x^3 (y^3 +z^3) , (x-y)^3 -z^2 -7)$ Using the implicit function theorem: $$\frac{\partial (f_1,f_2)}{\partial  y\partial  z}=      \begin{bmatrix} ∂f_1/∂y &  ∂f_1/∂z    \\  ∂f_2/∂y  & ∂f_2/∂z  \end{bmatrix} $$ Now: $\det [∂(f_1,f_2)/∂y∂z] = -6(x^3 y^2 z)+(9z^2 x^3 (x-y)^2 )$. Evaluated at the point $(1,-1,1)$ we find determinant not equal to zero. Thus we may apply the Implicit Function theorem. And now I'm confused. I feel that I should somehow set $z,y$ equal to something but its not clear what I should set equal to what... My book's proof of the theorem (which oddly enough contains no examples of application in this context) using cramer's rule [linear algebra tool which I understand] but I don't see how this would help. So I know I should be apply use to the implicit function theorem to find a $y=f(x)$ and $z=g(x)$ but I really don't see how.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
9,Compute the volume bounded by a parametric surface,Compute the volume bounded by a parametric surface,,"Given a smooth (i.e. $C^1$ continuous) closed parametric surface $(x,y,z) = S(u,v)$, how do I compute the volume $\mathcal{V}$ bounded by it? After browsing through a couple of books, I'm sure it is related to the divergence theorem (Gauss theorem), or perhaps Fubini's theorem. But I couldn't a proper proof for the following steps: $$\mathcal{V} = \iiint_V \, dV = \iiint_V \, dx \, dy \, dz = \iiint_V \, dz \, dx \, dy = \iiint_V \frac{\partial}{\partial z} \left( z \right) \, dz \, dx \, dy = \iint_A \left( \int \frac{\partial}{\partial z} \left( z \right) \, dz \right) \, dx \, dy = \iint_A z \, dx \, dy.$$ In addition, I stumbled upon the expression $$\iint_S z \, \cos(\gamma) \, dS = \iint_A z \, dx \,dy,$$ where $\cos(\gamma)$ — referred to as a direction cosine — is the $z$-component of the normal vector $n$ to the surface $S$. I don't quite understand why the left- and right-hand side are equivalent? Anyway, provided that the above expressions are correct, the last step would be to rewrite $$\iint_A z \, dx \, dy = \iint_\Omega z \left( \frac{\partial x}{\partial u}\frac{\partial y}{\partial v} - \frac{\partial y}{\partial u}\frac{\partial x}{ \partial v} \right) \, du \, dv,$$ which I can compute.","Given a smooth (i.e. $C^1$ continuous) closed parametric surface $(x,y,z) = S(u,v)$, how do I compute the volume $\mathcal{V}$ bounded by it? After browsing through a couple of books, I'm sure it is related to the divergence theorem (Gauss theorem), or perhaps Fubini's theorem. But I couldn't a proper proof for the following steps: $$\mathcal{V} = \iiint_V \, dV = \iiint_V \, dx \, dy \, dz = \iiint_V \, dz \, dx \, dy = \iiint_V \frac{\partial}{\partial z} \left( z \right) \, dz \, dx \, dy = \iint_A \left( \int \frac{\partial}{\partial z} \left( z \right) \, dz \right) \, dx \, dy = \iint_A z \, dx \, dy.$$ In addition, I stumbled upon the expression $$\iint_S z \, \cos(\gamma) \, dS = \iint_A z \, dx \,dy,$$ where $\cos(\gamma)$ — referred to as a direction cosine — is the $z$-component of the normal vector $n$ to the surface $S$. I don't quite understand why the left- and right-hand side are equivalent? Anyway, provided that the above expressions are correct, the last step would be to rewrite $$\iint_A z \, dx \, dy = \iint_\Omega z \left( \frac{\partial x}{\partial u}\frac{\partial y}{\partial v} - \frac{\partial y}{\partial u}\frac{\partial x}{ \partial v} \right) \, du \, dv,$$ which I can compute.",,"['integration', 'multivariable-calculus']"
10,Counterexample for the Chain rule for the Gateaux-derivative,Counterexample for the Chain rule for the Gateaux-derivative,,"I'm reading the book of Drabek, Milota - Methods of Nonlinear Analysis , and at page 121, they state: but I can't manage to find such counterexample. For clarity the Gateaux derivative is defined in this way: I need some kind of hints about how to build such counterexample because I'm like going nowhere with my trials. According to me $f$ and $g$ can't be continuous, otherwise G-derivative would be Frechét-derivative and for this kind of derivative the chain rule holds. It is sufficient requiring that only one function is non-continuos?","I'm reading the book of Drabek, Milota - Methods of Nonlinear Analysis , and at page 121, they state: but I can't manage to find such counterexample. For clarity the Gateaux derivative is defined in this way: I need some kind of hints about how to build such counterexample because I'm like going nowhere with my trials. According to me $f$ and $g$ can't be continuous, otherwise G-derivative would be Frechét-derivative and for this kind of derivative the chain rule holds. It is sufficient requiring that only one function is non-continuos?",,"['multivariable-calculus', 'banach-spaces', 'examples-counterexamples', 'calculus-of-variations', 'gateaux-derivative']"
11,Multivariable limit with polar coordinates,Multivariable limit with polar coordinates,,"Polar coordinates do not reveal the behaviour of $f(x,y)$ when studying $$ \lim_{x^2 + y^2 \to \infty} \frac {xy}{e^{x^2y^2}} $$ In polar coordinates we have $$ \lim_{r^2 \to \infty} \frac 12 \frac { r^2 \sin (2 \varphi ) }{ e^{ \frac 14 r^4 \sin^2 (2\varphi) } } $$ which goes to zero independently of $\varphi$. However by letting $y = \frac 1x$, we clearly see that the limit cannot exist. Why didn't polar coordinates work out and what other way than  letting $y = \frac 1x$ can we show that it does not exist?","Polar coordinates do not reveal the behaviour of $f(x,y)$ when studying $$ \lim_{x^2 + y^2 \to \infty} \frac {xy}{e^{x^2y^2}} $$ In polar coordinates we have $$ \lim_{r^2 \to \infty} \frac 12 \frac { r^2 \sin (2 \varphi ) }{ e^{ \frac 14 r^4 \sin^2 (2\varphi) } } $$ which goes to zero independently of $\varphi$. However by letting $y = \frac 1x$, we clearly see that the limit cannot exist. Why didn't polar coordinates work out and what other way than  letting $y = \frac 1x$ can we show that it does not exist?",,['multivariable-calculus']
12,Using the Levi-Civita alternating tensor and suffix notation to concisely write the vector product rule.,Using the Levi-Civita alternating tensor and suffix notation to concisely write the vector product rule.,,"I am reading through a section on vector calculus in an electromagnetism book and it has started to use suffix notation and the Levi-Civita alternating tensor in order to prove some identities.  Some of the identities I am familiar with and others I am not.  The notation is new to me as are the concept of tensors and I am struggling to apply both to do things which I can already do. As an example it is stated in the book that it is much simpler and more concise to write the product rule as $$\left(\mathbf{A}\times\mathbf{B}\right)_i=\epsilon_{ijk}A_jB_k$$ and I know to work out the product rule I can use the determinant formula. What I am having trouble formulating in my head is how to actually read the above definition and get the correct expansion for the vector-product.  So to make sure I understood the notation I expanded it out on paper but ended up getting the wrong answer.  My interpretations and assumptions are as below: $$\left(\mathbf{A}\times\mathbf{B}\right)_i = \epsilon_{ijk}A_jB_k$$ $$ = \epsilon_{ijk}A_yB_z\mathbf{i} + \epsilon_{jki}A_zB_x\mathbf{j} + \epsilon_{kij}A_xB_y\mathbf{k} + \epsilon_{jik}A_yB_z\mathbf{j} + \epsilon_{kji}A_zB_x\mathbf{k} + \epsilon_{ikj}A_xB_y\mathbf{i} $$ From here it can be seen that the first three terms are correct, but the last three are not.  I don't understand how the notation links back to the correct unit vector.  Here I have just take the first subscript letter in $\epsilon_{ijk}$ to also represent the relevant unit vector.  So I could really from getting the first three terms correct using the notation correctly write out the last three terms but that is just because I know what it should be.  What I do not understand is how the subscripts of the tensor link up to the subscripts of the two vectors.  I hope that is clear, if it isn't please leave a comment and I will try to remove anything confusing.","I am reading through a section on vector calculus in an electromagnetism book and it has started to use suffix notation and the Levi-Civita alternating tensor in order to prove some identities.  Some of the identities I am familiar with and others I am not.  The notation is new to me as are the concept of tensors and I am struggling to apply both to do things which I can already do. As an example it is stated in the book that it is much simpler and more concise to write the product rule as $$\left(\mathbf{A}\times\mathbf{B}\right)_i=\epsilon_{ijk}A_jB_k$$ and I know to work out the product rule I can use the determinant formula. What I am having trouble formulating in my head is how to actually read the above definition and get the correct expansion for the vector-product.  So to make sure I understood the notation I expanded it out on paper but ended up getting the wrong answer.  My interpretations and assumptions are as below: $$\left(\mathbf{A}\times\mathbf{B}\right)_i = \epsilon_{ijk}A_jB_k$$ $$ = \epsilon_{ijk}A_yB_z\mathbf{i} + \epsilon_{jki}A_zB_x\mathbf{j} + \epsilon_{kij}A_xB_y\mathbf{k} + \epsilon_{jik}A_yB_z\mathbf{j} + \epsilon_{kji}A_zB_x\mathbf{k} + \epsilon_{ikj}A_xB_y\mathbf{i} $$ From here it can be seen that the first three terms are correct, but the last three are not.  I don't understand how the notation links back to the correct unit vector.  Here I have just take the first subscript letter in $\epsilon_{ijk}$ to also represent the relevant unit vector.  So I could really from getting the first three terms correct using the notation correctly write out the last three terms but that is just because I know what it should be.  What I do not understand is how the subscripts of the tensor link up to the subscripts of the two vectors.  I hope that is clear, if it isn't please leave a comment and I will try to remove anything confusing.",,"['multivariable-calculus', 'tensors', 'vectors']"
13,Del operator ($\nabla$) in spherical co-ordinate system,Del operator () in spherical co-ordinate system,\nabla,"I am teaching myself about vector fields and came across the following question: Is the following force field $\vec{F}$ conservative, where $\vec{F}(r,\theta,\varphi)$ is defined by: $$F_{r}=2ar\sin(\theta)\sin(\varphi),\:  F_{\theta}=ar\cos(\theta)\sin(\varphi),\: F_{\varphi}=ar\cos(\varphi)$$ A simple test to determine whether a force field is conservative is to see if the following is true: $$\nabla\times \vec{F}=\vec{0}$$ Where by abuse of notation we have: $$\nabla=\frac{\partial \hat{\boldsymbol{\imath}}}{\partial x}+\frac{\partial \hat{\boldsymbol{\jmath}}}{\partial y}+\frac{\partial \hat{\boldsymbol{k}}}{\partial z}$$ However, as we are using a curvilinear co-ordinate basis I'm not sure how $\nabla$ should be defined? Further to JohnD's answer , I have tried to derive his expression for $\nabla$, however, I have not managed to come to the right answer. Taking partial derivatives of $r$ with respect to $x,y$ and $z$ we get: \begin{align}\frac{\partial r}{\partial x}&=\frac{x}{\sqrt{x^{2}+y^{2}+z^{2}}}=\sin(\theta)\cos(\varphi) \\ \frac{\partial r}{\partial y}&= \frac{y}{\sqrt{x^{2}+y^{2}+z^{2}}}=\sin(\theta)\sin(\varphi) \\ \frac{\partial r}{\partial z} &= \frac{z}{\sqrt{x^{2}+y^{2}+z^{2}}}=\cos(\theta)\end{align} Our partial derivatives of $\theta$ with respect to our cartesian co-ordinates are: \begin{align}\frac{\partial \theta}{\partial x}&= \frac{z\frac{\partial r}{\partial x}}{\sqrt{r^{2}-z^{2}}}=\frac{r\cos(\theta)\sin(\theta)\cos(\varphi)}{r\sqrt{1-\cos^{2}(\theta)}}=\cos(\theta)\cos(\varphi) \\ \frac{\partial \theta}{\partial y}&=\frac{z\frac{\partial r}{\partial y}}{\sqrt{r^{2}-z^{2}}}=\frac{r\cos(\theta)\sin(\theta)\sin(\varphi)}{r\sqrt{1-\cos^{2}(\theta)}}=\cos(\theta)\sin(\varphi) \\ \frac{\partial \theta}{\partial z} &= \frac{z\frac{\partial r}{\partial z}-r}{r\sqrt{r^{2}-z^{2}}} = \frac{r\cos^{2}(\theta)-r}{r\sqrt{r^{2}-r^{2}\cos^{2}(\theta)}}=-\frac{\sin(\theta)}{r}\end{align} And finally partial derivatives of $\varphi$: \begin{align}\frac{\partial \varphi}{\partial x}&=-\frac{y}{x^{2}(1+\frac{y^{2}}{x^{2}})}=-\frac{r\sin(\theta)\sin(\varphi)}{r^{2}\sin^{2}(\theta)\cos^{2}(\theta)(1+\tan^{2}(\varphi))}=-\frac{\sin(\varphi)}{r} \\ \frac{\partial \varphi}{\partial y}&=\frac{1}{x(1+\frac{y^2}{x^{2}})}=\frac{1}{r\sin(\theta)\cos(\varphi)(1+\tan^{2}(\varphi))}=\frac{\cos(\varphi)}{r\sin(\theta)} \\ \frac{\partial \varphi}{\partial z}&=0\end{align} We therefore get: \begin{align}\frac{\partial}{\partial x}&\mapsto \sin(\theta)\cos(\varphi)\frac{\partial}{\partial r}+\cos(\theta)\cos(\varphi)\frac{\partial}{\partial \theta} - \frac{\sin(\varphi)}{r}\frac{\partial}{\partial \varphi} \\ \frac{\partial}{\partial y} &\mapsto \sin(\theta)\sin(\varphi)\frac{\partial}{\partial r} + \cos(\theta)\sin(\varphi)\frac{\partial}{\partial \theta} + \frac{\cos(\varphi)}{r\sin(\theta)}\frac{\partial}{\partial \varphi} \\ \frac{\partial}{\partial z} &\mapsto \cos(\theta)\frac{\partial}{\partial r} - \frac{\sin(\theta)}{r}\frac{\partial}{\partial \theta}\end{align} However summing coefficients of $\frac{\partial}{\partial r}$, $\frac{\partial}{\partial \theta}$ and $\frac{\partial}{\partial \varphi}$ doesn't give what is expected, so what have I done wrong?","I am teaching myself about vector fields and came across the following question: Is the following force field $\vec{F}$ conservative, where $\vec{F}(r,\theta,\varphi)$ is defined by: $$F_{r}=2ar\sin(\theta)\sin(\varphi),\:  F_{\theta}=ar\cos(\theta)\sin(\varphi),\: F_{\varphi}=ar\cos(\varphi)$$ A simple test to determine whether a force field is conservative is to see if the following is true: $$\nabla\times \vec{F}=\vec{0}$$ Where by abuse of notation we have: $$\nabla=\frac{\partial \hat{\boldsymbol{\imath}}}{\partial x}+\frac{\partial \hat{\boldsymbol{\jmath}}}{\partial y}+\frac{\partial \hat{\boldsymbol{k}}}{\partial z}$$ However, as we are using a curvilinear co-ordinate basis I'm not sure how $\nabla$ should be defined? Further to JohnD's answer , I have tried to derive his expression for $\nabla$, however, I have not managed to come to the right answer. Taking partial derivatives of $r$ with respect to $x,y$ and $z$ we get: \begin{align}\frac{\partial r}{\partial x}&=\frac{x}{\sqrt{x^{2}+y^{2}+z^{2}}}=\sin(\theta)\cos(\varphi) \\ \frac{\partial r}{\partial y}&= \frac{y}{\sqrt{x^{2}+y^{2}+z^{2}}}=\sin(\theta)\sin(\varphi) \\ \frac{\partial r}{\partial z} &= \frac{z}{\sqrt{x^{2}+y^{2}+z^{2}}}=\cos(\theta)\end{align} Our partial derivatives of $\theta$ with respect to our cartesian co-ordinates are: \begin{align}\frac{\partial \theta}{\partial x}&= \frac{z\frac{\partial r}{\partial x}}{\sqrt{r^{2}-z^{2}}}=\frac{r\cos(\theta)\sin(\theta)\cos(\varphi)}{r\sqrt{1-\cos^{2}(\theta)}}=\cos(\theta)\cos(\varphi) \\ \frac{\partial \theta}{\partial y}&=\frac{z\frac{\partial r}{\partial y}}{\sqrt{r^{2}-z^{2}}}=\frac{r\cos(\theta)\sin(\theta)\sin(\varphi)}{r\sqrt{1-\cos^{2}(\theta)}}=\cos(\theta)\sin(\varphi) \\ \frac{\partial \theta}{\partial z} &= \frac{z\frac{\partial r}{\partial z}-r}{r\sqrt{r^{2}-z^{2}}} = \frac{r\cos^{2}(\theta)-r}{r\sqrt{r^{2}-r^{2}\cos^{2}(\theta)}}=-\frac{\sin(\theta)}{r}\end{align} And finally partial derivatives of $\varphi$: \begin{align}\frac{\partial \varphi}{\partial x}&=-\frac{y}{x^{2}(1+\frac{y^{2}}{x^{2}})}=-\frac{r\sin(\theta)\sin(\varphi)}{r^{2}\sin^{2}(\theta)\cos^{2}(\theta)(1+\tan^{2}(\varphi))}=-\frac{\sin(\varphi)}{r} \\ \frac{\partial \varphi}{\partial y}&=\frac{1}{x(1+\frac{y^2}{x^{2}})}=\frac{1}{r\sin(\theta)\cos(\varphi)(1+\tan^{2}(\varphi))}=\frac{\cos(\varphi)}{r\sin(\theta)} \\ \frac{\partial \varphi}{\partial z}&=0\end{align} We therefore get: \begin{align}\frac{\partial}{\partial x}&\mapsto \sin(\theta)\cos(\varphi)\frac{\partial}{\partial r}+\cos(\theta)\cos(\varphi)\frac{\partial}{\partial \theta} - \frac{\sin(\varphi)}{r}\frac{\partial}{\partial \varphi} \\ \frac{\partial}{\partial y} &\mapsto \sin(\theta)\sin(\varphi)\frac{\partial}{\partial r} + \cos(\theta)\sin(\varphi)\frac{\partial}{\partial \theta} + \frac{\cos(\varphi)}{r\sin(\theta)}\frac{\partial}{\partial \varphi} \\ \frac{\partial}{\partial z} &\mapsto \cos(\theta)\frac{\partial}{\partial r} - \frac{\sin(\theta)}{r}\frac{\partial}{\partial \theta}\end{align} However summing coefficients of $\frac{\partial}{\partial r}$, $\frac{\partial}{\partial \theta}$ and $\frac{\partial}{\partial \varphi}$ doesn't give what is expected, so what have I done wrong?",,"['multivariable-calculus', 'vector-analysis']"
14,Is this an analog of the mean value theorem for vector-valued functions?,Is this an analog of the mean value theorem for vector-valued functions?,,"I was reading through various proofs of the multi-dimensional analogues of the mean value theorem. Suppose we have a $C^1$ function $f: \mathbb{R}^n \supseteq U \to \mathbb{R}^m$. I had thought there was a theorem that Given a ball $B\subset U$,  $x,y\in B$, there exists a point $\xi \in B$ such that $f(x)-f(y)= Df(\xi)(x-y)$ , although in general $\xi$ will not lie on the line between $x$ and $y$. But the way I had remembered to prove it is incorrect. Is this a theorem at all? Another related theorem is that \2. Suppose $C\subset U$ is convex. Given $x,y \in C$, suppose $\|Df(\xi)\| \leq \eta$ for all $\xi $ on the line between $x$ and $y$. Then $\|f(x) - f(y)\| \leq \eta \|x-y\|$. The proofs I'm reading for #2 are somewhat complicated (e.g. Rudin PMA pp. 113 and 219), and I wondered if there was a simpler one just leveraging #1. Thanks for clarifying this for me.","I was reading through various proofs of the multi-dimensional analogues of the mean value theorem. Suppose we have a $C^1$ function $f: \mathbb{R}^n \supseteq U \to \mathbb{R}^m$. I had thought there was a theorem that Given a ball $B\subset U$,  $x,y\in B$, there exists a point $\xi \in B$ such that $f(x)-f(y)= Df(\xi)(x-y)$ , although in general $\xi$ will not lie on the line between $x$ and $y$. But the way I had remembered to prove it is incorrect. Is this a theorem at all? Another related theorem is that \2. Suppose $C\subset U$ is convex. Given $x,y \in C$, suppose $\|Df(\xi)\| \leq \eta$ for all $\xi $ on the line between $x$ and $y$. Then $\|f(x) - f(y)\| \leq \eta \|x-y\|$. The proofs I'm reading for #2 are somewhat complicated (e.g. Rudin PMA pp. 113 and 219), and I wondered if there was a simpler one just leveraging #1. Thanks for clarifying this for me.",,"['multivariable-calculus', 'derivatives']"
15,"$f$ such that $\|Df - \text{Id}\|$ is close to zero, yet $f$ is not bijective","such that  is close to zero, yet  is not bijective",f \|Df - \text{Id}\| f,"A problem from my differential geometry class: Suppose $f:\mathbb{R}^2 \to \mathbb{R}^2$ is a $C^1$ mapping, and for every $x\in \mathbb{R}^2$ $$\| Df(x) - \text{Id} \| < 10^{-10}.$$ Prove or disprove: $f$ must be a bijection. My intuition tells me that there should be a counterexample. $f$ linear will not work, because either it is an isomorphism, or it has range of dimension $\leq 1$, in which case $\|Df - \text{Id} \| = \|f - \text{Id}\|$ will be too big. One idea I had is a function which on the complex plane would be expressed as $z\mapsto z^{\alpha}$, where $\alpha - 1$ is positive and very close to zero. This would certainly not be a bijection, but should not move anything on the unit circle too much. The derivative of such a function would be the matrix representing the complex number $\alpha (a+bi)^{\alpha -1}$. It seems like this matrix would be sort of tricky to come up with. $\alpha(a+bi)^{\alpha -1} :=\alpha e^{(\alpha-1) \log{(a+bi)}}$, and how can I simplify this? I know this function is not complex-differentiable at zero, but perhaps it is differentiable when viewed from $\mathbb{R}^2 \to \mathbb{R}^2$? Any hints or ideas? No need to feed me the answer. Thanks","A problem from my differential geometry class: Suppose $f:\mathbb{R}^2 \to \mathbb{R}^2$ is a $C^1$ mapping, and for every $x\in \mathbb{R}^2$ $$\| Df(x) - \text{Id} \| < 10^{-10}.$$ Prove or disprove: $f$ must be a bijection. My intuition tells me that there should be a counterexample. $f$ linear will not work, because either it is an isomorphism, or it has range of dimension $\leq 1$, in which case $\|Df - \text{Id} \| = \|f - \text{Id}\|$ will be too big. One idea I had is a function which on the complex plane would be expressed as $z\mapsto z^{\alpha}$, where $\alpha - 1$ is positive and very close to zero. This would certainly not be a bijection, but should not move anything on the unit circle too much. The derivative of such a function would be the matrix representing the complex number $\alpha (a+bi)^{\alpha -1}$. It seems like this matrix would be sort of tricky to come up with. $\alpha(a+bi)^{\alpha -1} :=\alpha e^{(\alpha-1) \log{(a+bi)}}$, and how can I simplify this? I know this function is not complex-differentiable at zero, but perhaps it is differentiable when viewed from $\mathbb{R}^2 \to \mathbb{R}^2$? Any hints or ideas? No need to feed me the answer. Thanks",,['multivariable-calculus']
16,A curve parametrized by arc length,A curve parametrized by arc length,,"Let $C$ be a plane curve parametrized by arc length by $\alpha(s)$,   $T(s)$ (unit tangent vector) and $N(s)$ (unit normal vector). Prove   that $$\frac{d}{ds} N(s)=-\kappa(s)T(s).$$ I know that since $C$ is a curve parametrized by arc length then the tangent vector $\alpha'(s)$ has unit length which equals $T(s) = \frac{\alpha'(s)}{||\alpha'(s)||}$, thus $N(s) = \frac{T'(s)}{||T'(s)||}$. How can I prove this? .","Let $C$ be a plane curve parametrized by arc length by $\alpha(s)$,   $T(s)$ (unit tangent vector) and $N(s)$ (unit normal vector). Prove   that $$\frac{d}{ds} N(s)=-\kappa(s)T(s).$$ I know that since $C$ is a curve parametrized by arc length then the tangent vector $\alpha'(s)$ has unit length which equals $T(s) = \frac{\alpha'(s)}{||\alpha'(s)||}$, thus $N(s) = \frac{T'(s)}{||T'(s)||}$. How can I prove this? .",,"['multivariable-calculus', 'differential-geometry']"
17,"For $a > 0$, $x \in \mathbb{R}^n$, show that $x \mapsto \frac{ax}{\sqrt{a^2 - |x|^2}}$ is smooth","For , , show that  is smooth",a > 0 x \in \mathbb{R}^n x \mapsto \frac{ax}{\sqrt{a^2 - |x|^2}},"Let $a > 0$, and set $B_a = \{x \in \mathbb{R}^n : |x|^2 < a \}$. Let $\phi : B_a \to \mathbb{R}^n$ be given by $\phi(x)  = \frac{ax}{\sqrt{a^2 - |x|^2}}$. Prove that $\phi$ is a diffeomorphism of $B_a$ onto $\mathbb{R}^n$. This problem is from Guillemin and Pollack's book on Differential Topology. I'm working through it as a self-study. It's clear that $\phi(x)$ is invertible, one can directly compute the inverse and find it to be $\phi^{-1}(y) = \frac{ay}{\sqrt{a^2 + |y|^2}}$ (check out this post: What is the inverse of the function $x \mapsto \frac{ax}{\sqrt{a^2 - |x|^2}}$? ). So, to see that $\phi$ is a diffeomorphism, I just need to see that it's smooth (i.e., it's component functions  $\frac{ax_i}{\sqrt{a^2 - |x|^2}}$ have continuous partial derivatives of all orders). Now, it's mostly obvious to me that each $\frac{ax_i}{\sqrt{a^2 - |x|^2}}$ is indeed smooth. Practically speaking, we should be able compute partial derivatives of this expression for as long as we like. We'd just have to use the quotient rule over and over, and we'd never have any singularities causing us trouble. But I'm looking for a more formal method for showing that $\phi$ is smooth. Any suggestions? Maybe there is a general technique/theorem for showing that ""nice"" component functions like these, which contain nothing more than fractions or radicals, are smooth? Thanks in advance for solutions and hints.","Let $a > 0$, and set $B_a = \{x \in \mathbb{R}^n : |x|^2 < a \}$. Let $\phi : B_a \to \mathbb{R}^n$ be given by $\phi(x)  = \frac{ax}{\sqrt{a^2 - |x|^2}}$. Prove that $\phi$ is a diffeomorphism of $B_a$ onto $\mathbb{R}^n$. This problem is from Guillemin and Pollack's book on Differential Topology. I'm working through it as a self-study. It's clear that $\phi(x)$ is invertible, one can directly compute the inverse and find it to be $\phi^{-1}(y) = \frac{ay}{\sqrt{a^2 + |y|^2}}$ (check out this post: What is the inverse of the function $x \mapsto \frac{ax}{\sqrt{a^2 - |x|^2}}$? ). So, to see that $\phi$ is a diffeomorphism, I just need to see that it's smooth (i.e., it's component functions  $\frac{ax_i}{\sqrt{a^2 - |x|^2}}$ have continuous partial derivatives of all orders). Now, it's mostly obvious to me that each $\frac{ax_i}{\sqrt{a^2 - |x|^2}}$ is indeed smooth. Practically speaking, we should be able compute partial derivatives of this expression for as long as we like. We'd just have to use the quotient rule over and over, and we'd never have any singularities causing us trouble. But I'm looking for a more formal method for showing that $\phi$ is smooth. Any suggestions? Maybe there is a general technique/theorem for showing that ""nice"" component functions like these, which contain nothing more than fractions or radicals, are smooth? Thanks in advance for solutions and hints.",,"['real-analysis', 'multivariable-calculus', 'partial-derivative']"
18,Improper Multiple Integral,Improper Multiple Integral,,"I am trying to solve the following exam problem: Let $s$ be a real number.  Find the condition under which the improper integral $$I:=\iint_{\mathbb R^2} \frac{dxdy}{(x^2-xy+y + 1)^s}$$ converges, and obtain the values of $I$ for the values of $s$ that makes $I$ converge. I tried to solve it in the following way. I transform the coordinate system from $(x,y)$ to $(\xi,\eta)$, so that the quadratic form $x^2-xy+y^2$ in the denominator may become of the form $a\xi^2+b\eta^2$.  Then I rewrite the integral by using a ""distorted"" polar coordinate (in the shape of an ellipse, not of a circle), and then $I$ is a multiple of the integral $$ J:=\int_0^\infty \frac{dr}{(r^2+1)^s}.$$ So $I$ converges if and only if $s>1/2$ (is it true?). By letting $\tan \phi := r$, I have $J = \int_0^{\pi/2}\cos^{s-2}\phi\ d\phi$. The problem is how to evaluate the last integral for general $s>1/2$.  I would appreciate if you could give a clue, or suggest some other way to attack the original problem.","I am trying to solve the following exam problem: Let $s$ be a real number.  Find the condition under which the improper integral $$I:=\iint_{\mathbb R^2} \frac{dxdy}{(x^2-xy+y + 1)^s}$$ converges, and obtain the values of $I$ for the values of $s$ that makes $I$ converge. I tried to solve it in the following way. I transform the coordinate system from $(x,y)$ to $(\xi,\eta)$, so that the quadratic form $x^2-xy+y^2$ in the denominator may become of the form $a\xi^2+b\eta^2$.  Then I rewrite the integral by using a ""distorted"" polar coordinate (in the shape of an ellipse, not of a circle), and then $I$ is a multiple of the integral $$ J:=\int_0^\infty \frac{dr}{(r^2+1)^s}.$$ So $I$ converges if and only if $s>1/2$ (is it true?). By letting $\tan \phi := r$, I have $J = \int_0^{\pi/2}\cos^{s-2}\phi\ d\phi$. The problem is how to evaluate the last integral for general $s>1/2$.  I would appreciate if you could give a clue, or suggest some other way to attack the original problem.",,"['calculus', 'integration', 'multivariable-calculus', 'improper-integrals']"
19,"Help with $\int\limits_{0}^{1}\int\limits_{2x}^{2}x^2\sin(y^4)\,dy\,dx$",Help with,"\int\limits_{0}^{1}\int\limits_{2x}^{2}x^2\sin(y^4)\,dy\,dx","I usually do my problems by myself and then check the solution with Wolfram Alpha, but in this situation, it's not helping me at all... I don't know if I got the wrong answer, or if wolfram is using some trig identity that I don't know of... \begin{align} \int\limits_{0}^{1}\int\limits_{2x}^{2}x^2\sin(y^4)\,dy\,dx&=\int\limits_{0}^{2}\int\limits_{0}^{y/2}x^2\sin(y^4)\,dx\,dy\\ &=\frac{1}{3}\int\limits_{0}^{2}\left[x^3\sin(y^4)\right]_{x=0}^{x=y/2}\,dy\\ &=\frac{1}{24}\int\limits_{0}^{2}y^3\sin(y^4)\,dy\\ &=\frac{1}{96}\left[\sin(y^4)\right]_{y=0}^{y=2}\\ &=\frac{\sin(16)}{96} \end{align} This is what I came up with, but wolfram is giving me the answer of $$\frac{\sin^2(8)}{48}$$ Needless to say, I'm not the best at remembering my trig identities. I didn't see anything on the wikipedia page of identities to give me any help either.","I usually do my problems by myself and then check the solution with Wolfram Alpha, but in this situation, it's not helping me at all... I don't know if I got the wrong answer, or if wolfram is using some trig identity that I don't know of... \begin{align} \int\limits_{0}^{1}\int\limits_{2x}^{2}x^2\sin(y^4)\,dy\,dx&=\int\limits_{0}^{2}\int\limits_{0}^{y/2}x^2\sin(y^4)\,dx\,dy\\ &=\frac{1}{3}\int\limits_{0}^{2}\left[x^3\sin(y^4)\right]_{x=0}^{x=y/2}\,dy\\ &=\frac{1}{24}\int\limits_{0}^{2}y^3\sin(y^4)\,dy\\ &=\frac{1}{96}\left[\sin(y^4)\right]_{y=0}^{y=2}\\ &=\frac{\sin(16)}{96} \end{align} This is what I came up with, but wolfram is giving me the answer of $$\frac{\sin^2(8)}{48}$$ Needless to say, I'm not the best at remembering my trig identities. I didn't see anything on the wikipedia page of identities to give me any help either.",,['multivariable-calculus']
20,Prove an identity about $\iint_S\mathbf{r}\wedge d\mathbf{S}$ using Stokes' theorem,Prove an identity about  using Stokes' theorem,\iint_S\mathbf{r}\wedge d\mathbf{S},"$$ \int_C\mathbf{r}(\mathbf{r}\cdot d\mathbf{r})=\iint_S\mathbf{r}\wedge d\mathbf{S} $$ With $\mathbf{r} = (x,y,z)$ being a 3-dimensional vector. How do you get this result using Stokes' theorem?","$$ \int_C\mathbf{r}(\mathbf{r}\cdot d\mathbf{r})=\iint_S\mathbf{r}\wedge d\mathbf{S} $$ With $\mathbf{r} = (x,y,z)$ being a 3-dimensional vector. How do you get this result using Stokes' theorem?",,"['differential-geometry', 'multivariable-calculus']"
21,"Find $u'(0)$ where $u(t) = f(2009t, t^{2009})$ given the differential of $f$ in $0$",Find  where  given the differential of  in,"u'(0) u(t) = f(2009t, t^{2009}) f 0","I'm having trouble with this exercise, probably don't have enough theoretical info. How do I approach this? Let $f : \mathbb{R}^2\rightarrow \mathbb{R}$ where $f$ - differentiable in $0$ and $df(0)(h) = 2h_{1}-7h_{2} $ for $h\in\mathbb{R}^2.$ Let $u : \mathbb{R} \rightarrow \mathbb{R} : u(t) = f(2009t, t^{2009})$. Find $u'(0)$ Thanks for your help!","I'm having trouble with this exercise, probably don't have enough theoretical info. How do I approach this? Let $f : \mathbb{R}^2\rightarrow \mathbb{R}$ where $f$ - differentiable in $0$ and $df(0)(h) = 2h_{1}-7h_{2} $ for $h\in\mathbb{R}^2.$ Let $u : \mathbb{R} \rightarrow \mathbb{R} : u(t) = f(2009t, t^{2009})$. Find $u'(0)$ Thanks for your help!",,['multivariable-calculus']
22,Gaussian linking coefficient definition,Gaussian linking coefficient definition,,"As I read from the wikipedia page of the linking number, it says that the linking number of two curves $\gamma_1$ and $\gamma_2$ in space can be found using the integral  $$\,\frac{1}{4\pi} \oint_{\gamma_1}\oint_{\gamma_2} \frac{\mathbf{r}_1 - \mathbf{r}_2}{|\mathbf{r}_1 - \mathbf{r}_2|^3} \cdot (d\mathbf{r}_1 \times d\mathbf{r}_2)$$ It says the integrand is the Jacobian of the Gaussian map $$\Gamma(s,t) = \frac{\gamma_1(s) - \gamma_2(t)}{|\gamma_1(s) - \gamma_2(t)|}$$ Following Ted's comment I am able to show that $$\oint_{\gamma_1}\oint_{\gamma_2} \left\|\frac{\partial\Gamma}{\partial s}\times \frac{\partial\Gamma}{\partial t}\right\|\,dsdt =  \oint_{\gamma_1}\oint_{\gamma_2} \frac{1}{|\mathbf{r}_1 - \mathbf{r}_2|^3} \left| (\mathbf{r}_1 - \mathbf{r}_2)\cdot \left (\frac{d\mathbf{r}_1}{ds} \times \frac{d\mathbf{r}_2}{dt}\right)\right| ds dt$$","As I read from the wikipedia page of the linking number, it says that the linking number of two curves $\gamma_1$ and $\gamma_2$ in space can be found using the integral  $$\,\frac{1}{4\pi} \oint_{\gamma_1}\oint_{\gamma_2} \frac{\mathbf{r}_1 - \mathbf{r}_2}{|\mathbf{r}_1 - \mathbf{r}_2|^3} \cdot (d\mathbf{r}_1 \times d\mathbf{r}_2)$$ It says the integrand is the Jacobian of the Gaussian map $$\Gamma(s,t) = \frac{\gamma_1(s) - \gamma_2(t)}{|\gamma_1(s) - \gamma_2(t)|}$$ Following Ted's comment I am able to show that $$\oint_{\gamma_1}\oint_{\gamma_2} \left\|\frac{\partial\Gamma}{\partial s}\times \frac{\partial\Gamma}{\partial t}\right\|\,dsdt =  \oint_{\gamma_1}\oint_{\gamma_2} \frac{1}{|\mathbf{r}_1 - \mathbf{r}_2|^3} \left| (\mathbf{r}_1 - \mathbf{r}_2)\cdot \left (\frac{d\mathbf{r}_1}{ds} \times \frac{d\mathbf{r}_2}{dt}\right)\right| ds dt$$",,"['calculus', 'differential-geometry', 'multivariable-calculus', 'knot-theory']"
23,Domain and range in two coordinates,Domain and range in two coordinates,,"let $f(x,y) = \sqrt{(x-1)(y-1)}$; what is the domain and the range? I wanted to know if I was doing the domain part right and needed help with the range. For the domain I got $(x-1)(y-1)$ are good as long as they are any real number. is that right?","let $f(x,y) = \sqrt{(x-1)(y-1)}$; what is the domain and the range? I wanted to know if I was doing the domain part right and needed help with the range. For the domain I got $(x-1)(y-1)$ are good as long as they are any real number. is that right?",,['multivariable-calculus']
24,How general formulation of Stoke's theorem relate to Kelvin-Stokes theorem,How general formulation of Stoke's theorem relate to Kelvin-Stokes theorem,,"I asked a similar question, but I realized the question is too vague and it's better to start a new one: We know that there are two usually used formulations of Stoke's theorem. One is vector calculus's usage of Stoke's theorem called Kelvin-Stokes theorem. And one is Stoke's theorem that involves manifolds and boundary of mainfolds and differential forms. The question is, how does one derive Kelvin-Stokes theorem from the general manifold formulation of Stoke's theorem?","I asked a similar question, but I realized the question is too vague and it's better to start a new one: We know that there are two usually used formulations of Stoke's theorem. One is vector calculus's usage of Stoke's theorem called Kelvin-Stokes theorem. And one is Stoke's theorem that involves manifolds and boundary of mainfolds and differential forms. The question is, how does one derive Kelvin-Stokes theorem from the general manifold formulation of Stoke's theorem?",,"['multivariable-calculus', 'manifolds', 'differential-forms']"
25,Prove not a violation of Stokes theorem,Prove not a violation of Stokes theorem,,"The question is as follows: Define the vector field ${\bf F}$ on the complement of the $z$-axis by   $${\bf F}(x,y,z)= \frac{-y{\bf i} +x{\bf j}}{x^{2}+y^{2}}.$$ i) Show that $\operatorname{curl}({\bf F}) = 0$. We have $$(0-0){\bf i} + (0-0){\bf j} + \left((x^{2}+y^{2})^{-1} -2x^{2}(x^{2}+y^{2})^{-2} + (x^{2}+y^{2})^{-1} -2y^{2}(x^{2}+y^{2})^{-2}\right) {\bf k}.$$ Collecting terms we have $0{\bf i} +0{\bf j}+0{\bf k}$. Thus $\operatorname{curl} ({\bf F})=0$. ii) Show by direct calculation $\displaystyle \oint_{C} {\bf F} \cdot {\bf dx} =2\pi$ for any horizontal circle $C$ centered at a point on the $z$-axis. I can't even figure this out... The main part of the question is even though i and ii are true why doesn't this contradict Stokes' theorem?","The question is as follows: Define the vector field ${\bf F}$ on the complement of the $z$-axis by   $${\bf F}(x,y,z)= \frac{-y{\bf i} +x{\bf j}}{x^{2}+y^{2}}.$$ i) Show that $\operatorname{curl}({\bf F}) = 0$. We have $$(0-0){\bf i} + (0-0){\bf j} + \left((x^{2}+y^{2})^{-1} -2x^{2}(x^{2}+y^{2})^{-2} + (x^{2}+y^{2})^{-1} -2y^{2}(x^{2}+y^{2})^{-2}\right) {\bf k}.$$ Collecting terms we have $0{\bf i} +0{\bf j}+0{\bf k}$. Thus $\operatorname{curl} ({\bf F})=0$. ii) Show by direct calculation $\displaystyle \oint_{C} {\bf F} \cdot {\bf dx} =2\pi$ for any horizontal circle $C$ centered at a point on the $z$-axis. I can't even figure this out... The main part of the question is even though i and ii are true why doesn't this contradict Stokes' theorem?",,"['multivariable-calculus', 'differential-topology']"
26,Is this vector-valued map Hölder-continuous?,Is this vector-valued map Hölder-continuous?,,"Pick $0<q<1$ and consider the map from $\mathbb{R}^n$ to $\mathbb{R}^n$ that sends $x$ to $|x|^{q-1}x$. Is this map Hölder-continuous (I guess with exponent $\leq q$)? In dimension one, I can exploit the homogeneity of the inequality defined by Hölder-continuity; but how can I proceed in higher dimension?","Pick $0<q<1$ and consider the map from $\mathbb{R}^n$ to $\mathbb{R}^n$ that sends $x$ to $|x|^{q-1}x$. Is this map Hölder-continuous (I guess with exponent $\leq q$)? In dimension one, I can exploit the homogeneity of the inequality defined by Hölder-continuity; but how can I proceed in higher dimension?",,"['calculus', 'multivariable-calculus', 'holder-spaces']"
27,Dimensions of a box of maximum volume inside an ellipsoid,Dimensions of a box of maximum volume inside an ellipsoid,,"Finding the dimensions of the maximum volume box inside the ellipsoid. I assume that the volume of a box, $V(x,y,z) = xyz$ (they did not give this to me, but this is the volume of a box right?) Ellipsoid: $$\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$$ and I use Lagrange multipliers to find an incorrect  answer, I end up getting $$x = \frac{\sqrt{a}}{\sqrt{3}}$$ $$y = \frac{\sqrt{b}}{\sqrt{3}}$$ $$z = \frac{\sqrt{c}}{\sqrt{3}}$$ the hint they give me is that $$\text{Max volume} = \frac{8abc}{3\sqrt{3}}$$ Could someone tell me where I am doing this wrong?","Finding the dimensions of the maximum volume box inside the ellipsoid. I assume that the volume of a box, $V(x,y,z) = xyz$ (they did not give this to me, but this is the volume of a box right?) Ellipsoid: $$\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$$ and I use Lagrange multipliers to find an incorrect  answer, I end up getting $$x = \frac{\sqrt{a}}{\sqrt{3}}$$ $$y = \frac{\sqrt{b}}{\sqrt{3}}$$ $$z = \frac{\sqrt{c}}{\sqrt{3}}$$ the hint they give me is that $$\text{Max volume} = \frac{8abc}{3\sqrt{3}}$$ Could someone tell me where I am doing this wrong?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'optimization', 'volume']"
28,Derivation of weak form for variational problem,Derivation of weak form for variational problem,,"My question is about understanding the derivation of the weak form of a variational problem (to be used for the solution via the finite element method). The problem is as follows (it is an image processing problem, $\mathbf{u}$ is a 2D vector field of displacements I is an image function (see Horn-Schunk optical flow algorithm)): Minimize the following functional w.r.t. the vector field $\mathbf{u}$ $$E(\mathbf{u}) = \int_\Omega \left\|\mathbf{u}\cdot\nabla I + \frac{\partial I}{\partial t}\right\|^2dx + \lambda\int_\Omega \nabla \mathbf{u} : \nabla \mathbf{u} dx$$ (: denotes the Frobenius inner product http://en.wikipedia.org/wiki/Frobenius_inner_product#Frobenius_product ) The apparent weak form of the problem is given in the following form ( http://code.google.com/p/debiosee/wiki/DemosOptiocFlowHornSchunck ): $$E(\mathbf{u}) = \int_\Omega \left(\mathbf{u}\cdot\nabla I + \frac{\partial I}{\partial t}\right) \left(\mathbf{v}\cdot\nabla I\right) dx + \lambda\int_\Omega\nabla\mathbf{u}:\nabla\mathbf{v}dx$$ Where $\mathbf{v}$ is the test function. What I would like to know is if there is a direct way to come to this conclusion from the original problem or the way to go is to formulate the weak form of the Euler-Lagrange equation of the original functional? Additionally I would like to know if any of you knew about a vector calculus textbook you could recommend for mastering these kinds of operations in their vector form without working out the derivation componentwise? Thanks a lot in advance! Peter","My question is about understanding the derivation of the weak form of a variational problem (to be used for the solution via the finite element method). The problem is as follows (it is an image processing problem, $\mathbf{u}$ is a 2D vector field of displacements I is an image function (see Horn-Schunk optical flow algorithm)): Minimize the following functional w.r.t. the vector field $\mathbf{u}$ $$E(\mathbf{u}) = \int_\Omega \left\|\mathbf{u}\cdot\nabla I + \frac{\partial I}{\partial t}\right\|^2dx + \lambda\int_\Omega \nabla \mathbf{u} : \nabla \mathbf{u} dx$$ (: denotes the Frobenius inner product http://en.wikipedia.org/wiki/Frobenius_inner_product#Frobenius_product ) The apparent weak form of the problem is given in the following form ( http://code.google.com/p/debiosee/wiki/DemosOptiocFlowHornSchunck ): $$E(\mathbf{u}) = \int_\Omega \left(\mathbf{u}\cdot\nabla I + \frac{\partial I}{\partial t}\right) \left(\mathbf{v}\cdot\nabla I\right) dx + \lambda\int_\Omega\nabla\mathbf{u}:\nabla\mathbf{v}dx$$ Where $\mathbf{v}$ is the test function. What I would like to know is if there is a direct way to come to this conclusion from the original problem or the way to go is to formulate the weak form of the Euler-Lagrange equation of the original functional? Additionally I would like to know if any of you knew about a vector calculus textbook you could recommend for mastering these kinds of operations in their vector form without working out the derivation componentwise? Thanks a lot in advance! Peter",,"['multivariable-calculus', 'partial-differential-equations', 'numerical-methods']"
29,Is it a continuous function on $\mathbb{R}^{2}$?,Is it a continuous function on ?,\mathbb{R}^{2},"Prove or disprove: let $f : \mathbb{R}^{2} \to \mathbb{R}$ be a mapping with the following properties: for each $y \in \mathbb{R}$ the function $x\mapsto f\left(x,y\right)$ is continuous on $\mathbb{R}$, and for each $x\in\mathbb{R}$ the function $y\mapsto f\left(x,y\right)$ is continuous on $\mathbb{R}$. Then $f$ is continuous on $\mathbb{R}^{2}$. My intuition says it's not true, but I can't think of a simple counterexample.","Prove or disprove: let $f : \mathbb{R}^{2} \to \mathbb{R}$ be a mapping with the following properties: for each $y \in \mathbb{R}$ the function $x\mapsto f\left(x,y\right)$ is continuous on $\mathbb{R}$, and for each $x\in\mathbb{R}$ the function $y\mapsto f\left(x,y\right)$ is continuous on $\mathbb{R}$. Then $f$ is continuous on $\mathbb{R}^{2}$. My intuition says it's not true, but I can't think of a simple counterexample.",,"['calculus', 'multivariable-calculus', 'continuity']"
30,On the continuity of Riemann Integral,On the continuity of Riemann Integral,,"I have the following equation $$g(y)=\int_{0}^{\infty} f(x,y) dx$$ I know that $f$ is continuous in $x$ and $y$. But I would like to infer that $g$ is continuous in $y$. Can I do this? EDIT: I wont write down the function here, since it is huge, but I can guarantee that: $f$ is differentiable in both $x$ and $y$ $\lim_{y\rightarrow \infty} f(x,y) = \infty$ I could try to solve the integral, but I do not know if I'm able too. Applying some theorem that guarantees continuity would be great, but now I see I would need further hypothesis.","I have the following equation $$g(y)=\int_{0}^{\infty} f(x,y) dx$$ I know that $f$ is continuous in $x$ and $y$. But I would like to infer that $g$ is continuous in $y$. Can I do this? EDIT: I wont write down the function here, since it is huge, but I can guarantee that: $f$ is differentiable in both $x$ and $y$ $\lim_{y\rightarrow \infty} f(x,y) = \infty$ I could try to solve the integral, but I do not know if I'm able too. Applying some theorem that guarantees continuity would be great, but now I see I would need further hypothesis.",,"['calculus', 'multivariable-calculus']"
31,Function on $\mathbb{R}^{2}-\{0\}$.,Function on .,\mathbb{R}^{2}-\{0\},"Does there exist any compactly supported function $f= (f_1,f_2): \mathbb R^2-\{0\}\to \mathbb R^2$ such that $$\frac{\partial}{\partial x_2}f_1=\frac{\partial}{\partial x_1}f_2.$$ Also there does not exists any function $F: U(\subset \mathbb R^2-\{0\})\to \mathbb R$ such that  $$\frac{\partial}{\partial x_1}F= f_1\text{ and } \frac{\partial}{\partial x_2}F= f_2.$$","Does there exist any compactly supported function $f= (f_1,f_2): \mathbb R^2-\{0\}\to \mathbb R^2$ such that $$\frac{\partial}{\partial x_2}f_1=\frac{\partial}{\partial x_1}f_2.$$ Also there does not exists any function $F: U(\subset \mathbb R^2-\{0\})\to \mathbb R$ such that  $$\frac{\partial}{\partial x_1}F= f_1\text{ and } \frac{\partial}{\partial x_2}F= f_2.$$",,"['calculus', 'real-analysis', 'multivariable-calculus', 'differential-forms', 'homology-cohomology']"
32,Triple integral over a three dimensional region,Triple integral over a three dimensional region,,"The integral $$\int_{-2}^2\int_{x^2}^4\int_0^{1-y/4} 1\ dz\ dy\ dx$$is a triple integral over a three dimensional region E,  Sketch E, and rewrite this integral with $y$ as a first variable and $z$ second variable and $x$ as a third variable. What I have done so far: $0\le z \le{1-y/4}$ $x^2\le y \le+4$ $-2 \le x\le+2 $","The integral $$\int_{-2}^2\int_{x^2}^4\int_0^{1-y/4} 1\ dz\ dy\ dx$$is a triple integral over a three dimensional region E,  Sketch E, and rewrite this integral with $y$ as a first variable and $z$ second variable and $x$ as a third variable. What I have done so far: $0\le z \le{1-y/4}$ $x^2\le y \le+4$ $-2 \le x\le+2 $",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', '3d']"
33,What is the difference between surface area and scalar surface integrals?,What is the difference between surface area and scalar surface integrals?,,What is the difference between the surface area of a paremetrized surface and the scalar surface integral of a function in $\mathbb{R}^3$? Are they not the same thing?,What is the difference between the surface area of a paremetrized surface and the scalar surface integral of a function in $\mathbb{R}^3$? Are they not the same thing?,,"['multivariable-calculus', 'integration', 'vector-spaces', 'surfaces']"
34,Integration by Parts with multi-variable functions,Integration by Parts with multi-variable functions,,"I want to prove that  $$ - \int\limits_{\mathbb R^n} y_t \Delta y =  \int\limits_{\mathbb R^n} \sum_{j=1}^n \left(\frac{\partial y}{\partial x_j}\right)\left(\frac{\partial y_t}{\partial x_j}\right) $$  Here $ \Delta y  =  \sum\limits_{j=1}^n \frac{\partial^2 y}{\partial x_j^2} $, and $y = y(t,x_1, \cdots x_n ) \in  C^\infty ([0,\infty) \times \mathbb R^n )$,  $y_t(t, \cdot) \in L^2$.","I want to prove that  $$ - \int\limits_{\mathbb R^n} y_t \Delta y =  \int\limits_{\mathbb R^n} \sum_{j=1}^n \left(\frac{\partial y}{\partial x_j}\right)\left(\frac{\partial y_t}{\partial x_j}\right) $$  Here $ \Delta y  =  \sum\limits_{j=1}^n \frac{\partial^2 y}{\partial x_j^2} $, and $y = y(t,x_1, \cdots x_n ) \in  C^\infty ([0,\infty) \times \mathbb R^n )$,  $y_t(t, \cdot) \in L^2$.",,['multivariable-calculus']
35,What is the ordinary definition of a saddle-point?,What is the ordinary definition of a saddle-point?,,"Suppose $f:\mathbb{R}^2 \to\mathbb{R}$ is a $\mathcal{C}^2$ function such that both first-order partial derivatives vanish at the origin. Under what circumstances would you say that $f$ has a saddle-point at the origin? If the Hessian of $f$ is nonsingular, then this question is not interesting, so I suppose I am really asking when a function $f$ having a stationary point at the origin and whose Hessian is singular there has a saddle-point. Consider as examples the obvious saddle-point $f(x,y) = x^4 - y^4$, the slightly less obvious case of $f(x,y) = x^3-y^3$, and the very peculiar case of $f(x,y) = \sin(1/(x^2 + y^2))\exp(-1/(x^2+y^2))$ extended to $\mathbb{R}^2$ by setting $f(0,0)=0$.","Suppose $f:\mathbb{R}^2 \to\mathbb{R}$ is a $\mathcal{C}^2$ function such that both first-order partial derivatives vanish at the origin. Under what circumstances would you say that $f$ has a saddle-point at the origin? If the Hessian of $f$ is nonsingular, then this question is not interesting, so I suppose I am really asking when a function $f$ having a stationary point at the origin and whose Hessian is singular there has a saddle-point. Consider as examples the obvious saddle-point $f(x,y) = x^4 - y^4$, the slightly less obvious case of $f(x,y) = x^3-y^3$, and the very peculiar case of $f(x,y) = \sin(1/(x^2 + y^2))\exp(-1/(x^2+y^2))$ extended to $\mathbb{R}^2$ by setting $f(0,0)=0$.",,"['calculus', 'multivariable-calculus']"
36,Inverse function theorem and its applications,Inverse function theorem and its applications,,"Let the space of all matrices over $\mathbb R$ of size $ n^2 $ , with the natural metric of $\mathbb R^{n^2 }$. Prove that there exist a neighborhood of the identity matrix, such that all the matrices in that ball have a square root, i.e   $$ \exists \varepsilon  > 0\,:\forall X \in B\left( {I,\varepsilon } \right)\,\,\exists \,Y:\,Y^2  = X . $$ Hint: Consider the function $ F(X) = X^2 $ and use the Inverse Function Theorem Help with this problem !! )=","Let the space of all matrices over $\mathbb R$ of size $ n^2 $ , with the natural metric of $\mathbb R^{n^2 }$. Prove that there exist a neighborhood of the identity matrix, such that all the matrices in that ball have a square root, i.e   $$ \exists \varepsilon  > 0\,:\forall X \in B\left( {I,\varepsilon } \right)\,\,\exists \,Y:\,Y^2  = X . $$ Hint: Consider the function $ F(X) = X^2 $ and use the Inverse Function Theorem Help with this problem !! )=",,"['calculus', 'real-analysis', 'multivariable-calculus', 'metric-spaces']"
37,Need help with line integral,Need help with line integral,,"Over a curve $C$ given by $(x^2+y^2)^2=30^2(x^2-y^2)$, What is  $$ \oint\limits_C |y|\,\mathrm ds. $$ I've tried working on it but I couldn't get the solution. Here's how I did it: Using polar coordinates $$ \begin{cases} x(t) &= 30 \sqrt{\cos 2t}\cdot\cos t \\ y(t) &= 30 \sqrt{\cos 2t}  \cdot\sin t \end{cases} $$ thereafter, $\mathrm ds = 30 \sqrt{ \sec 2t}\cdot \mathrm dt$. Finally, integral over $$ \oint\limits_C |y|\,ds = 2 \cdot 30^2\int\limits_{-\pi/4}^{\pi/4}\sqrt{\cos 2t} \cdot\sqrt{\sec 2t} \sin t\, \mathrm dt = 0. $$ I've spent many hours on this problem already. Will someone be kind enough to please help me? Thanks.","Over a curve $C$ given by $(x^2+y^2)^2=30^2(x^2-y^2)$, What is  $$ \oint\limits_C |y|\,\mathrm ds. $$ I've tried working on it but I couldn't get the solution. Here's how I did it: Using polar coordinates $$ \begin{cases} x(t) &= 30 \sqrt{\cos 2t}\cdot\cos t \\ y(t) &= 30 \sqrt{\cos 2t}  \cdot\sin t \end{cases} $$ thereafter, $\mathrm ds = 30 \sqrt{ \sec 2t}\cdot \mathrm dt$. Finally, integral over $$ \oint\limits_C |y|\,ds = 2 \cdot 30^2\int\limits_{-\pi/4}^{\pi/4}\sqrt{\cos 2t} \cdot\sqrt{\sec 2t} \sin t\, \mathrm dt = 0. $$ I've spent many hours on this problem already. Will someone be kind enough to please help me? Thanks.",,"['integration', 'multivariable-calculus']"
38,When is the determinant of a Hessian matrix positive?,When is the determinant of a Hessian matrix positive?,,"Let $f:\mathbb{R}^n\to \mathbb{R}^n$ be a $C^2$-function and let $H=\left(\frac{\partial^2f}{\partial x_i \partial x_j}\right)_{1\le i,j\le n}$ be its Hessian matrix. Suppose I know that $ \det H(x_1,\ldots,x_n)\ge 0$ for all $x=(x_1,\ldots,x_n)$. Does this have any geometric meaning for $f$? e.g., when $n=1$, this means that $f$ is convex. This no longer holds for $n\ge 2$, but when $f$ is convex the Hessian determinant is certainly positive, so perhaps one could wonder if a weaker property holds.","Let $f:\mathbb{R}^n\to \mathbb{R}^n$ be a $C^2$-function and let $H=\left(\frac{\partial^2f}{\partial x_i \partial x_j}\right)_{1\le i,j\le n}$ be its Hessian matrix. Suppose I know that $ \det H(x_1,\ldots,x_n)\ge 0$ for all $x=(x_1,\ldots,x_n)$. Does this have any geometric meaning for $f$? e.g., when $n=1$, this means that $f$ is convex. This no longer holds for $n\ge 2$, but when $f$ is convex the Hessian determinant is certainly positive, so perhaps one could wonder if a weaker property holds.",,"['real-analysis', 'multivariable-calculus', 'vector-analysis']"
39,Calculate surface integral of point charge located outside the surface,Calculate surface integral of point charge located outside the surface,,"Problem Given: $$\vec r = r(\sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta)$$ $S: | \vec r  | = a$, with $\hat n$ outwards $$\vec r_0 = \frac{3a}{5}(\hat x + \hat y - \hat z) = \frac{3a}{5}(1,1,-1)$$ $$\vec F = k \frac{\vec r - \vec r_0}{ {| \vec r - \vec r_0 |}^3 }$$ Calculate: $$\int_S \vec F \cdot \mathrm d \vec S $$ Solution (added after accepted answer) Thanks to the accepted answer which confirmed that the explicit calculation on/in $S$ is messy, and the suggestion of using translation invariance, I have noted down the solution below (let me know if you have further suggestions). An argument is used which concludes that the integrand is zero inside the sphere $S$. Using Gauss' theorem: $\int_S \vec F \cdot \mathrm d \vec S = \int_V \nabla \cdot \vec F \mathrm dV $. Since the calculation is not easy to explicitly calculate in (or on) $S$, instead it is shown that $\nabla \cdot \vec F = 0$ exterior to $S_{\epsilon}$, which is a sphere containing the singularity, and since $S$ is exterior to $S_{\epsilon}$ the integrand must be $0$ and so $\int_V \nabla \cdot \vec F \mathrm dV = 0$. $$S_{\epsilon}: |\vec r - \vec r_0 | = \epsilon, \epsilon > 0$$ translate the coordinate system so that $S_{\epsilon}$ is the origin in the translated system: $$\vec R = \vec r - \vec r_0 = R (\sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta)$$ $$F(\vec R) = k \frac{\vec R}{|\vec R|^3} = k \frac{1}{R^2} \hat R$$ $$S_{\epsilon}: |\vec R| = \epsilon$$ $$\nabla \cdot \vec F (\vec R) = \frac{1}{R^2} \frac{\partial}{\partial R} \left(R^2 \vec F_R \right) = \frac{1}{R^2} \frac{\partial}{\partial R} \left(R^2 k \frac{1}{R^2} \right) = 0$$, ($k$ is a constant) This is true for any value of $R$ except at the singularity ($\epsilon$ can be made as small as required), since $S$ does not contain the singularity, the integrand ($\nabla \cdot \vec F(\vec r)$ is 0 and the integral is therefore zero. My question (updated) I am having difficulties in explicitly calculating the value of this integral.  Specifically, the divergence of the field becomes messy, I am not able to see how I can use the symmetry of $S$ due to $\vec r_0$. (When applying the Gauss' theorem, I am stuck in evaluating the divergence of the field). I am able to argue for that this integral is indeed zero (the field only has one singularity and it is exterior to the sphere $S$). However, I am not able to explicitly show (by calculation) that this integral is zero. Thankful for any help. Please note that this is not homework, I am studying for an exam. My question (original) What is an easy way to calculate this integral? Any suggestions on the approaches below? My apologies if this is due to lack of some basic knowledge (I am back studying after 2.5 years) Intuitively I understand the integral is zero (the point charge is located outside the sphere, anything flowing into the sphere will also flow out), however, I have issues with the calculation. Calculate directly $$\int_S \vec F \cdot \mathrm d \vec S = \int_0^{\pi} \mathrm d \theta \int_0^{2\pi} \mathrm d \phi r^2 \sin {\theta} \vec F_r \cdot \hat r = \int_0^{\pi} \mathrm d \theta \int_0^{2\pi} \mathrm d \phi r^2 \sin \theta k \frac{a - \frac{3a}{5} \left(  \sin \theta \cos \phi + \sin \theta \sin \phi - \cos \theta) \right)}{ {\left| \vec r - \vec r_0 \right|}^3 }$$ Using Gauss theorem I am tempted to use Gauss theorem, $\int_S \vec F \cdot \mathrm d \vec S = \int_V \nabla \cdot \vec F \mathrm dV $, in spherical coordinates. However $\vec F$ has components also in $\hat \theta$ and $\hat \phi$ (due to $\vec r_0$) and ${\left| \vec r - \vec r_0 \right|}^3$ is not that nice to derivate. $$\vec F_r = k \frac{\vec r - \vec r_0}{ {\left| \vec r - \vec r_0 \right|}^3 } \cdot \hat r = k \frac{r - \frac{3a}{5} \left(  \sin \theta \cos \phi + \sin \theta \sin \phi - \cos \theta) \right)}{ {\left| \vec r - \vec r_0 \right|}^3 }$$ $$\vec F_{\theta} = k \frac{\vec r - \vec r_0}{ {\left| \vec r - \vec r_0 \right|}^3 } \cdot \hat \theta = k \frac{ \frac{3a}{5} \left( \cos \theta \cos \phi + \cos \theta \sin \phi - \sin \theta) \right)}{ {\left| \vec r - \vec r_0 \right|}^3 }$$ $$\vec F_{\phi} = k \frac{\vec r - \vec r_0}{ {\left| \vec r - \vec r_0 \right|}^3 } \cdot \hat \phi = k \frac{ \frac{3a}{5} \left( - \sin \phi + \cos \phi ) \right)}{ {\left| \vec r - \vec r_0 \right|}^3 }$$ $$\nabla \cdot \vec F = \frac{1}{r^2} \frac{\partial}{\partial r} \left(r^2 \vec F_r \right) + \frac{1}{r \sin \theta} \frac{\partial}{\partial \theta} \left( \sin \theta F_{\theta} \right) + \frac{1}{r \sin \theta} \frac{\partial F_{\phi}}{\partial \phi}$$","Problem Given: $$\vec r = r(\sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta)$$ $S: | \vec r  | = a$, with $\hat n$ outwards $$\vec r_0 = \frac{3a}{5}(\hat x + \hat y - \hat z) = \frac{3a}{5}(1,1,-1)$$ $$\vec F = k \frac{\vec r - \vec r_0}{ {| \vec r - \vec r_0 |}^3 }$$ Calculate: $$\int_S \vec F \cdot \mathrm d \vec S $$ Solution (added after accepted answer) Thanks to the accepted answer which confirmed that the explicit calculation on/in $S$ is messy, and the suggestion of using translation invariance, I have noted down the solution below (let me know if you have further suggestions). An argument is used which concludes that the integrand is zero inside the sphere $S$. Using Gauss' theorem: $\int_S \vec F \cdot \mathrm d \vec S = \int_V \nabla \cdot \vec F \mathrm dV $. Since the calculation is not easy to explicitly calculate in (or on) $S$, instead it is shown that $\nabla \cdot \vec F = 0$ exterior to $S_{\epsilon}$, which is a sphere containing the singularity, and since $S$ is exterior to $S_{\epsilon}$ the integrand must be $0$ and so $\int_V \nabla \cdot \vec F \mathrm dV = 0$. $$S_{\epsilon}: |\vec r - \vec r_0 | = \epsilon, \epsilon > 0$$ translate the coordinate system so that $S_{\epsilon}$ is the origin in the translated system: $$\vec R = \vec r - \vec r_0 = R (\sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta)$$ $$F(\vec R) = k \frac{\vec R}{|\vec R|^3} = k \frac{1}{R^2} \hat R$$ $$S_{\epsilon}: |\vec R| = \epsilon$$ $$\nabla \cdot \vec F (\vec R) = \frac{1}{R^2} \frac{\partial}{\partial R} \left(R^2 \vec F_R \right) = \frac{1}{R^2} \frac{\partial}{\partial R} \left(R^2 k \frac{1}{R^2} \right) = 0$$, ($k$ is a constant) This is true for any value of $R$ except at the singularity ($\epsilon$ can be made as small as required), since $S$ does not contain the singularity, the integrand ($\nabla \cdot \vec F(\vec r)$ is 0 and the integral is therefore zero. My question (updated) I am having difficulties in explicitly calculating the value of this integral.  Specifically, the divergence of the field becomes messy, I am not able to see how I can use the symmetry of $S$ due to $\vec r_0$. (When applying the Gauss' theorem, I am stuck in evaluating the divergence of the field). I am able to argue for that this integral is indeed zero (the field only has one singularity and it is exterior to the sphere $S$). However, I am not able to explicitly show (by calculation) that this integral is zero. Thankful for any help. Please note that this is not homework, I am studying for an exam. My question (original) What is an easy way to calculate this integral? Any suggestions on the approaches below? My apologies if this is due to lack of some basic knowledge (I am back studying after 2.5 years) Intuitively I understand the integral is zero (the point charge is located outside the sphere, anything flowing into the sphere will also flow out), however, I have issues with the calculation. Calculate directly $$\int_S \vec F \cdot \mathrm d \vec S = \int_0^{\pi} \mathrm d \theta \int_0^{2\pi} \mathrm d \phi r^2 \sin {\theta} \vec F_r \cdot \hat r = \int_0^{\pi} \mathrm d \theta \int_0^{2\pi} \mathrm d \phi r^2 \sin \theta k \frac{a - \frac{3a}{5} \left(  \sin \theta \cos \phi + \sin \theta \sin \phi - \cos \theta) \right)}{ {\left| \vec r - \vec r_0 \right|}^3 }$$ Using Gauss theorem I am tempted to use Gauss theorem, $\int_S \vec F \cdot \mathrm d \vec S = \int_V \nabla \cdot \vec F \mathrm dV $, in spherical coordinates. However $\vec F$ has components also in $\hat \theta$ and $\hat \phi$ (due to $\vec r_0$) and ${\left| \vec r - \vec r_0 \right|}^3$ is not that nice to derivate. $$\vec F_r = k \frac{\vec r - \vec r_0}{ {\left| \vec r - \vec r_0 \right|}^3 } \cdot \hat r = k \frac{r - \frac{3a}{5} \left(  \sin \theta \cos \phi + \sin \theta \sin \phi - \cos \theta) \right)}{ {\left| \vec r - \vec r_0 \right|}^3 }$$ $$\vec F_{\theta} = k \frac{\vec r - \vec r_0}{ {\left| \vec r - \vec r_0 \right|}^3 } \cdot \hat \theta = k \frac{ \frac{3a}{5} \left( \cos \theta \cos \phi + \cos \theta \sin \phi - \sin \theta) \right)}{ {\left| \vec r - \vec r_0 \right|}^3 }$$ $$\vec F_{\phi} = k \frac{\vec r - \vec r_0}{ {\left| \vec r - \vec r_0 \right|}^3 } \cdot \hat \phi = k \frac{ \frac{3a}{5} \left( - \sin \phi + \cos \phi ) \right)}{ {\left| \vec r - \vec r_0 \right|}^3 }$$ $$\nabla \cdot \vec F = \frac{1}{r^2} \frac{\partial}{\partial r} \left(r^2 \vec F_r \right) + \frac{1}{r \sin \theta} \frac{\partial}{\partial \theta} \left( \sin \theta F_{\theta} \right) + \frac{1}{r \sin \theta} \frac{\partial F_{\phi}}{\partial \phi}$$",,"['calculus', 'multivariable-calculus']"
40,Is Hessian with zero directions indefinite?,Is Hessian with zero directions indefinite?,,"I have a Hessian matrix $\mathbf{H}$ of a function $f(\mathbf{x})$ , evaluated at an extreme point $\mathbf{x}_0$ . Lets assume $\mathbf{H}$ is non-singular. I can show that there exists a direction $\mathbf{z} \neq \mathbf{0}$ for which $\mathbf{z}^T \mathbf{H} \mathbf{z}$ is zero. Does this imply that the Hessian is indefinite and $f$ has a saddle point at $\mathbf{x}_0$ ? My geometric intuition is that a saddle has cross-over lines between positive and negative regions. A minimum or maximum would not have such cross-over lines. Moreover, I would argue that since the Hessian is assumed to be non-singular, $\mathbf{z}$ is not in its kernel, so $\mathbf{H} \mathbf{z} \neq \mathbf{0}$ , so the expression $\mathbf{z}^T \mathbf{H} \mathbf{z}$ is not generally zero, but only in some directions.","I have a Hessian matrix of a function , evaluated at an extreme point . Lets assume is non-singular. I can show that there exists a direction for which is zero. Does this imply that the Hessian is indefinite and has a saddle point at ? My geometric intuition is that a saddle has cross-over lines between positive and negative regions. A minimum or maximum would not have such cross-over lines. Moreover, I would argue that since the Hessian is assumed to be non-singular, is not in its kernel, so , so the expression is not generally zero, but only in some directions.",\mathbf{H} f(\mathbf{x}) \mathbf{x}_0 \mathbf{H} \mathbf{z} \neq \mathbf{0} \mathbf{z}^T \mathbf{H} \mathbf{z} f \mathbf{x}_0 \mathbf{z} \mathbf{H} \mathbf{z} \neq \mathbf{0} \mathbf{z}^T \mathbf{H} \mathbf{z},"['linear-algebra', 'multivariable-calculus', 'hessian-matrix']"
41,A general form of a double integral,A general form of a double integral,,"I have difficulty evaluating the following double integral: $$\int_0^1\int_0^1\sqrt{(x-y)^2+m^2}\text{d}x\text{d}y$$ where m is a constant. I've tried to use substitution $x-y=m\tan u$ to calculate the inner part: $$\int\sec^3 u\text{d}u=\frac{m^2}{2}(\tan u\sec u+\ln|\tan u+\sec u|)+C$$ where $u=\arctan\frac{x-y}{m}$ , I also need to integrate the outer part. It seems that I made the problem even complex. I've also tried to use polar coordinates, but can I use substitution like this? $$x-y=r\cos\theta, m=r\sin\theta$$","I have difficulty evaluating the following double integral: where m is a constant. I've tried to use substitution to calculate the inner part: where , I also need to integrate the outer part. It seems that I made the problem even complex. I've also tried to use polar coordinates, but can I use substitution like this?","\int_0^1\int_0^1\sqrt{(x-y)^2+m^2}\text{d}x\text{d}y x-y=m\tan u \int\sec^3 u\text{d}u=\frac{m^2}{2}(\tan u\sec u+\ln|\tan u+\sec u|)+C u=\arctan\frac{x-y}{m} x-y=r\cos\theta, m=r\sin\theta","['calculus', 'integration', 'multivariable-calculus']"
42,Proving the formula for distance between a point and a parametric line,Proving the formula for distance between a point and a parametric line,,"Let $l$ be a line in $\mathbb{R}^3$ with parametric equation $\alpha(t) = a + t\boldsymbol{v}$ and $p$ an arbitrary point. Show that the (shortest) distance between $p$ and $l$ is given by $$d = \frac{||(\boldsymbol{p - a}) \times \boldsymbol{v}||}{||\boldsymbol{v}||}$$ I've attempted to render an image corresponding to the problem, where $Q$ is the point at the foot of a perpendicular dropped from $P$ to $l$ . (Please excuse capital $P$ instead of $p$ in the drawing). By the definition of $sin$ we have $d = \sin\theta||p - a||.$ But from there, I don't see why the cross product of $||p - a||$ with $\boldsymbol{v}$ divided by $||\boldsymbol{v}||$ would be related to $d$ or how it would get rid of the $\sin\theta$ . By the right hand rule, wouldn't the cross product of $p - a$ and $\boldsymbol{v}$ be sticking ""out"" in a direction unrelated to $d$ ? Thank you.","Let be a line in with parametric equation and an arbitrary point. Show that the (shortest) distance between and is given by I've attempted to render an image corresponding to the problem, where is the point at the foot of a perpendicular dropped from to . (Please excuse capital instead of in the drawing). By the definition of we have But from there, I don't see why the cross product of with divided by would be related to or how it would get rid of the . By the right hand rule, wouldn't the cross product of and be sticking ""out"" in a direction unrelated to ? Thank you.",l \mathbb{R}^3 \alpha(t) = a + t\boldsymbol{v} p p l d = \frac{||(\boldsymbol{p - a}) \times \boldsymbol{v}||}{||\boldsymbol{v}||} Q P l P p sin d = \sin\theta||p - a||. ||p - a|| \boldsymbol{v} ||\boldsymbol{v}|| d \sin\theta p - a \boldsymbol{v} d,"['linear-algebra', 'multivariable-calculus']"
43,Uniqueness of Jacobian,Uniqueness of Jacobian,,"Consider the simple stick-breaking function $$f : [0, 1]^2 \to S^3 : (v_1, v_2) \mapsto (x_1, x_2, x_3) = \big(v_1, (1 - v_1) v_2, (1 - v_1) (1 - v_2)\big),$$ where $S^N = \left\{(x_1, x_2, \ldots, x_N) \mid \sum_i x_i = 1, \forall i : x_i \geq 0\right\}$ is the $N$ -dimensional probability simplex. The inverse of this function should be $$f^{-1}(x_1, x_2, x_3) = \Big(x_1, \frac{x_2}{1 - x_1}\Big)$$ However, there is an equivalent formulation that makes use of the fact that $x_1 + x_2 + x_3 = 1$ $$f^{-1}_\mathrm{eq}(x_1, x_2, x_3) = \Big(x_1, \frac{x_2}{x_2 + x_3}\Big)$$ Both of these formulations should represent the same function, but if we compute the Jacobians for both functions, we get $$\begin{align*}   \mathcal{J}_{f^{-1}}(x_1, x_2, x_3) &= \begin{pmatrix}1 & 0 & 0 \\ \frac{x_2}{(1 - x_1)^2} & \frac{1}{1 - x_1} & 0 \end{pmatrix} \\   \mathcal{J}_{f^{-1}_\mathrm{eq}}(x_1, x_2, x_3) &= \begin{pmatrix}1 & 0 & 0 \\ 0 & \frac{x_3}{(x_2 + x_3)^2} & \frac{-x_2}{(x_2 + x_3)^2} \end{pmatrix} = \begin{pmatrix}1 & 0 & 0 \\ 0 & \frac{1}{1 - x_1} - \frac{x_2}{(1 - x_1)^2} & \frac{-x_2}{(1 - x_1)^2} \end{pmatrix}, \end{align*}$$ which seems to suggest that there is no unique Jacobian for $f^{-1} = f^{-1}_\mathrm{eq}$ . Note that the difference between the second rows in the Jacobian is a constant. For higher-dimensional variants of these functions, I noticed that this constant offset is different for every row. I am aware that constants are unique up to a constant term, but I thought derivatives would be unique (cf. this post with an answer for single-output functions). Therefore, I started wondering: are Jacobians actually unique? If yes, how should the example above be interpreted? If not, is there some specific notion of uniqueness like ""unique up to ...""?","Consider the simple stick-breaking function where is the -dimensional probability simplex. The inverse of this function should be However, there is an equivalent formulation that makes use of the fact that Both of these formulations should represent the same function, but if we compute the Jacobians for both functions, we get which seems to suggest that there is no unique Jacobian for . Note that the difference between the second rows in the Jacobian is a constant. For higher-dimensional variants of these functions, I noticed that this constant offset is different for every row. I am aware that constants are unique up to a constant term, but I thought derivatives would be unique (cf. this post with an answer for single-output functions). Therefore, I started wondering: are Jacobians actually unique? If yes, how should the example above be interpreted? If not, is there some specific notion of uniqueness like ""unique up to ...""?","f : [0, 1]^2 \to S^3 : (v_1, v_2) \mapsto (x_1, x_2, x_3) = \big(v_1, (1 - v_1) v_2, (1 - v_1) (1 - v_2)\big), S^N = \left\{(x_1, x_2, \ldots, x_N) \mid \sum_i x_i = 1, \forall i : x_i \geq 0\right\} N f^{-1}(x_1, x_2, x_3) = \Big(x_1, \frac{x_2}{1 - x_1}\Big) x_1 + x_2 + x_3 = 1 f^{-1}_\mathrm{eq}(x_1, x_2, x_3) = \Big(x_1, \frac{x_2}{x_2 + x_3}\Big) \begin{align*}
  \mathcal{J}_{f^{-1}}(x_1, x_2, x_3) &= \begin{pmatrix}1 & 0 & 0 \\ \frac{x_2}{(1 - x_1)^2} & \frac{1}{1 - x_1} & 0 \end{pmatrix} \\
  \mathcal{J}_{f^{-1}_\mathrm{eq}}(x_1, x_2, x_3) &= \begin{pmatrix}1 & 0 & 0 \\ 0 & \frac{x_3}{(x_2 + x_3)^2} & \frac{-x_2}{(x_2 + x_3)^2} \end{pmatrix} = \begin{pmatrix}1 & 0 & 0 \\ 0 & \frac{1}{1 - x_1} - \frac{x_2}{(1 - x_1)^2} & \frac{-x_2}{(1 - x_1)^2} \end{pmatrix},
\end{align*} f^{-1} = f^{-1}_\mathrm{eq}","['calculus', 'multivariable-calculus', 'jacobian']"
44,Definite integral of ydx + xdy giving two different results,Definite integral of ydx + xdy giving two different results,,"This comes from Blundell's ""Concepts in Thermal Physics"". Section 11, example 11.1. If I have $$df = d(xy) = ydx + xdy$$ Then one can do $$\int df = \int d(xy) = \int (ydx + xdy)$$ But $\int df = \Delta f$ and $\int (ydx + xdy) = \int ydx + \int xdy =  yx + xy = 2xy$ and $\int d(xy) = \Delta (xy)$ by the Fundamental Theorem of Calculus. I don't get how can it do $\Delta f = \int_{(0, 0)}^{(1, 1)} (xy) = (1 \times 1) - (0 \times 0) = 1 $ which is different than $\int_{(0, 0)}^{(1, 1)} (ydx + xdy) = 2(1 \times 1) - 2 (0 \times 0) = 2$ . What is wrong with my interpretation?","This comes from Blundell's ""Concepts in Thermal Physics"". Section 11, example 11.1. If I have Then one can do But and and by the Fundamental Theorem of Calculus. I don't get how can it do which is different than . What is wrong with my interpretation?","df = d(xy) = ydx + xdy \int df = \int d(xy) = \int (ydx + xdy) \int df = \Delta f \int (ydx + xdy) = \int ydx + \int xdy =  yx + xy = 2xy \int d(xy) = \Delta (xy) \Delta f = \int_{(0, 0)}^{(1, 1)} (xy) = (1 \times 1) - (0 \times 0) = 1  \int_{(0, 0)}^{(1, 1)} (ydx + xdy) = 2(1 \times 1) - 2 (0 \times 0) = 2","['calculus', 'integration', 'multivariable-calculus', 'functions']"
45,Showing that it is possible to Differentiate under the Integral Sign - Proof Trouble,Showing that it is possible to Differentiate under the Integral Sign - Proof Trouble,,"Suppose $f: [a,b] \times [c,d] \rightarrow \mathbb{R}$ is continuous and $\frac{\partial f}{\partial x}$ is continuous. Define $F(x) = \int_c^d f(x,y)dy$ . The purpose of this question is to arrive at the conclusion of being able to differentiate under the integral sign. The first part of the question asked me to show that $F$ was continuous. I've done that, but I'm stuck on this second part which is stated as so: Prove that $F$ is differentiable and that $F'(x) = \int_c^d \frac{\partial f}{\partial x}(x,y) dy$ . Hint: Let $\phi (t) = \int_c^d \frac{\partial f}{\partial x}(t,y) dy$ and let $\Phi (x) = \int_a^x \phi(t) dt$ . Show that $\phi$ is continuous and that $F(x) = \Phi (x) + \text{constant}$ . Proving that $F$ is differentiable: Going back to the definition, and since this is a function in one dimension, a function $f$ will be differentiable at a point $a$ if there is a unique number $m$ such that $$  \lim_{h \to 0} \frac{f(a+h) - f(a) - mh}{h} = 0.$$ So specifically to this question I can write: $$\lim_{h \to 0} \frac{F(a+h) - F(a) - mh}{h} = \lim_{h \to 0}\frac{\int_c^d f(a+h,y) dy - \int_c^df(a,y)dy - mh}{h} \\ = \lim_{h \to 0}\frac{\int_c^d f(a+h,y) - f(a,y) dy}{h}  - \lim_{h \to 0} \frac{mh}{h} = \lim_{h \to 0}\int_c^d \frac{f(a+h,y) - f(a,y)}{h}dy - m.$$ I want to bring the limit into the integral, but if I recall that is something that is not straightforward and needs to be proven, which we haven't done yet. But assuming that it is permitted I would be left with: $$\int_c^d \frac{\partial f}{\partial x}(a,y)dy - m = 0$$ But now I'm left in a problem because I'm trying to show $F$ is differentiable, but I'm claiming already that what I arrived at is the needed value. This comes up because I have to show that $F'(x) = \int_c^d \frac{\partial f}{\partial x}(x,y)dy$ . I see where the hint is going to lead me: Showing $\phi$ is continuous and $F(x) =  \Phi(x) + \text{const}$ , I would be able to use $\phi (t)$ being continuous to get $F'(x)$ . To show $\phi (t)$ is continuous the argument would be similar to what was done for $F$ : $$|\phi(t_1) - \phi(t_2)| = \bigg |\int_c^d \frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y) dy\bigg| \leq \int_c^d \bigg|\frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| dy \tag{$**$}$$ By uniform continuity for all $\frac{\epsilon}{d-c}$ there is a $\delta > 0$ such that for all $t_1,t_2 \in [a,b]$ , when $|t_1 - t_2| < \delta$ then $\bigg| \frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| < \frac{\epsilon}{d-c}$ . Therefore by $(**)$ we will have: $$\int_c^d \bigg|\frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| dy < \int_c^d \frac{\epsilon}{d-c} dy = \epsilon$$ Therefore $\phi(t)$ is continuous. I know from discussion and reading another solution, there is suppose to be some application of a chain rule, but I don't understand how it comes about. I know $F(x)$ is suppose to be some composition of functions but I don't see clearly what that composition is. I also don't understand how/why we use a parameterization in $t$ .","Suppose is continuous and is continuous. Define . The purpose of this question is to arrive at the conclusion of being able to differentiate under the integral sign. The first part of the question asked me to show that was continuous. I've done that, but I'm stuck on this second part which is stated as so: Prove that is differentiable and that . Hint: Let and let . Show that is continuous and that . Proving that is differentiable: Going back to the definition, and since this is a function in one dimension, a function will be differentiable at a point if there is a unique number such that So specifically to this question I can write: I want to bring the limit into the integral, but if I recall that is something that is not straightforward and needs to be proven, which we haven't done yet. But assuming that it is permitted I would be left with: But now I'm left in a problem because I'm trying to show is differentiable, but I'm claiming already that what I arrived at is the needed value. This comes up because I have to show that . I see where the hint is going to lead me: Showing is continuous and , I would be able to use being continuous to get . To show is continuous the argument would be similar to what was done for : By uniform continuity for all there is a such that for all , when then . Therefore by we will have: Therefore is continuous. I know from discussion and reading another solution, there is suppose to be some application of a chain rule, but I don't understand how it comes about. I know is suppose to be some composition of functions but I don't see clearly what that composition is. I also don't understand how/why we use a parameterization in .","f: [a,b] \times [c,d] \rightarrow \mathbb{R} \frac{\partial f}{\partial x} F(x) = \int_c^d f(x,y)dy F F F'(x) = \int_c^d \frac{\partial f}{\partial x}(x,y) dy \phi (t) = \int_c^d \frac{\partial f}{\partial x}(t,y) dy \Phi (x) = \int_a^x \phi(t) dt \phi F(x) = \Phi (x) + \text{constant} F f a m   \lim_{h \to 0} \frac{f(a+h) - f(a) - mh}{h} = 0. \lim_{h \to 0} \frac{F(a+h) - F(a) - mh}{h} = \lim_{h \to 0}\frac{\int_c^d f(a+h,y) dy - \int_c^df(a,y)dy - mh}{h} \\ = \lim_{h \to 0}\frac{\int_c^d f(a+h,y) - f(a,y) dy}{h}  - \lim_{h \to 0} \frac{mh}{h} = \lim_{h \to 0}\int_c^d \frac{f(a+h,y) - f(a,y)}{h}dy - m. \int_c^d \frac{\partial f}{\partial x}(a,y)dy - m = 0 F F'(x) = \int_c^d \frac{\partial f}{\partial x}(x,y)dy \phi F(x) =  \Phi(x) + \text{const} \phi (t) F'(x) \phi (t) F |\phi(t_1) - \phi(t_2)| = \bigg |\int_c^d \frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y) dy\bigg| \leq \int_c^d \bigg|\frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| dy \tag{**} \frac{\epsilon}{d-c} \delta > 0 t_1,t_2 \in [a,b] |t_1 - t_2| < \delta \bigg| \frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| < \frac{\epsilon}{d-c} (**) \int_c^d \bigg|\frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| dy < \int_c^d \frac{\epsilon}{d-c} dy = \epsilon \phi(t) F(x) t","['real-analysis', 'multivariable-calculus', 'derivatives']"
46,"Finding non-trivial real numbers which satisfy all three of: $\sum_{i=1}^{n}a_i=0,\sum_{i=1}^{n}{a_i}^3=0$ and $\sum_{i=1}^{n}\lvert a_i\rvert=1.$",Finding non-trivial real numbers which satisfy all three of:  and,"\sum_{i=1}^{n}a_i=0,\sum_{i=1}^{n}{a_i}^3=0 \sum_{i=1}^{n}\lvert a_i\rvert=1.","A question was asked recently on this site: Find bounds on $\ \displaystyle\sum_{i=1}^{n} {a_i} ^5\ $ if $\  \displaystyle\sum_{i=1}^{n} a_i = 0,\ \sum_{i=1}^{n} {a_i}^3 = 0\ $ and $\  \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1.$ I then asked for one example of $a_i$ that satisfy the three conditions in the question, and the user correctly gave the example: $a_1 = -0.5,\ a_2 = 0.5 $ and then he deleted the question. Now, the only other examples I can think of which satisfy all three conditions are $\left( \underbrace{\frac{-1}{2n},\ \frac{-1}{2n},\ \ldots,\ \frac{-1}{2n}}_{n\ \text{times}},\ \underbrace{\frac{1}{2n},\ \frac{1}{2n},\ \ldots,\ \frac{1}{2n}}_{n\ \text{times}} \right).\ $ This works for any $\ n\in\mathbb{N}.$ My question is: Are there any other examples satisfying all three conditions? I found some examples that convince me there are numbers that satisfy $\displaystyle\sum_{i=1}^{n} {a_i}^3 = 0\ $ and $\  \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1,\ $ for example $\ 0.40,\ -0.375, -0.225\ $ is close enough to convince me. However, these don't come close to satisfying the condition $\  \displaystyle\sum_{i=1}^{n} a_i = 0.$ Similarly, there are many numbers satisfying $\  \displaystyle\sum_{i=1}^{n} a_i = 0\ $ and $\  \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1:\ $ The positive terms must sum to $0.5$ and the negative terms must sum to $-0.5,$ but other than this there are no restrictions. And as an example where $\ \displaystyle\sum_{i=1}^{n} {a_i}^3 = 0\ $ and $\  \displaystyle\sum_{i=1}^{n} a_i = 0,\ $ there's $\left( \underbrace{\frac{-1}{2n},\ \frac{-1}{2n},\ \ldots,\ \frac{-1}{2n}}_{n\ \text{times}},\ \underbrace{\frac{1}{2n},\ \frac{1}{2n},\ \ldots,\ \frac{1}{2n}}_{n\ \text{times}} \right),$ but I'm sure there's other examples also. For example, $(-0.2,-0.3, 0.5)$ doesn't work because ${0.5}^3 > \vert {-0.2}^3 + {-0.3}^3\vert,\ $ however we can split the $0.5$ up into three (or more) positive numbers $a_i, i=3,\ldots, n,\ $ such that $ \displaystyle\sum_{i=3}^{n} a_i = 0.5\ $ where $n\geq 5.$ Since $\ \displaystyle\sum_{i=3}^{n} {a_i}^3\ $ has min value $\ n \left( \frac{0.5}{n} \right)^3\ $ (I think), and due to continuity of the multi-variable function $\ \displaystyle\sum_{i=3}^{n} {a_i}^3,\ $ there are clearly examples such that $ \vert {-0.2}^3 + {-0.3}^3\vert = \displaystyle\sum_{i=3}^{n} {a_i}^3.$","A question was asked recently on this site: Find bounds on if and I then asked for one example of that satisfy the three conditions in the question, and the user correctly gave the example: and then he deleted the question. Now, the only other examples I can think of which satisfy all three conditions are This works for any My question is: Are there any other examples satisfying all three conditions? I found some examples that convince me there are numbers that satisfy and for example is close enough to convince me. However, these don't come close to satisfying the condition Similarly, there are many numbers satisfying and The positive terms must sum to and the negative terms must sum to but other than this there are no restrictions. And as an example where and there's but I'm sure there's other examples also. For example, doesn't work because however we can split the up into three (or more) positive numbers such that where Since has min value (I think), and due to continuity of the multi-variable function there are clearly examples such that","\ \displaystyle\sum_{i=1}^{n} {a_i} ^5\  \
 \displaystyle\sum_{i=1}^{n} a_i = 0,\ \sum_{i=1}^{n} {a_i}^3 = 0\  \
 \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1. a_i a_1 = -0.5,\ a_2 = 0.5  \left( \underbrace{\frac{-1}{2n},\ \frac{-1}{2n},\ \ldots,\ \frac{-1}{2n}}_{n\ \text{times}},\ \underbrace{\frac{1}{2n},\ \frac{1}{2n},\ \ldots,\ \frac{1}{2n}}_{n\ \text{times}} \right).\  \ n\in\mathbb{N}. \displaystyle\sum_{i=1}^{n} {a_i}^3 = 0\  \
 \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1,\  \ 0.40,\ -0.375, -0.225\  \
 \displaystyle\sum_{i=1}^{n} a_i = 0. \
 \displaystyle\sum_{i=1}^{n} a_i = 0\  \
 \displaystyle\sum_{i=1}^{n} \lvert a_i \rvert = 1:\  0.5 -0.5, \ \displaystyle\sum_{i=1}^{n} {a_i}^3 = 0\  \
 \displaystyle\sum_{i=1}^{n} a_i = 0,\  \left( \underbrace{\frac{-1}{2n},\ \frac{-1}{2n},\ \ldots,\ \frac{-1}{2n}}_{n\ \text{times}},\ \underbrace{\frac{1}{2n},\ \frac{1}{2n},\ \ldots,\ \frac{1}{2n}}_{n\ \text{times}} \right), (-0.2,-0.3, 0.5) {0.5}^3 > \vert {-0.2}^3 + {-0.3}^3\vert,\  0.5 a_i, i=3,\ldots, n,\   \displaystyle\sum_{i=3}^{n} a_i = 0.5\  n\geq 5. \ \displaystyle\sum_{i=3}^{n} {a_i}^3\  \ n \left( \frac{0.5}{n} \right)^3\  \ \displaystyle\sum_{i=3}^{n} {a_i}^3,\   \vert {-0.2}^3 + {-0.3}^3\vert = \displaystyle\sum_{i=3}^{n} {a_i}^3.","['real-analysis', 'multivariable-calculus', 'inequality', 'a.m.-g.m.-inequality', 'holder-inequality']"
47,Computing the Jacobian for the change of variables from cartesian into spherical coordinates,Computing the Jacobian for the change of variables from cartesian into spherical coordinates,,"This is the question: My question is whether the answer is $\rho^2\sin\phi$ or if it is $-\rho^2\sin\phi$ or if it doesn't necessarily matter, and why not. I found a solution online that set up the determinant in the same way that I set up my determinant, however they got $\rho^2\sin\phi$ instead of $-\rho^2\sin\phi$ (which is what I got). This is the solution online: But I found another solution online that also set up the determinant the way that I set it up and it got $-\rho^2\sin\phi$ (which is what I got also). This  solution is: So I just want to know which is the correct answer and why. This is my solution:","This is the question: My question is whether the answer is or if it is or if it doesn't necessarily matter, and why not. I found a solution online that set up the determinant in the same way that I set up my determinant, however they got instead of (which is what I got). This is the solution online: But I found another solution online that also set up the determinant the way that I set it up and it got (which is what I got also). This  solution is: So I just want to know which is the correct answer and why. This is my solution:",\rho^2\sin\phi -\rho^2\sin\phi \rho^2\sin\phi -\rho^2\sin\phi -\rho^2\sin\phi,"['multivariable-calculus', 'determinant', 'spherical-coordinates', 'jacobian']"
48,"Use of the phrase ""tangent vector of a curve""","Use of the phrase ""tangent vector of a curve""",,"Let us understand a curve as a differentiable map $f : J \to \mathbb R^n$ defined on an open interval $J \subset \mathbb R$ . The derivative $f'(t_0)$ of $f$ at $t_0 \in J$ is given as the vector $(f'_1(t_0),\ldots,f'_n(t_0)) \in \mathbb R^n$ where the $f_i : J \to \mathbb R$ are the coordinate functions of $f$ . I think for $n > 1$ it is usual to say that $f'(t_0)$ is the tangent vector to the curve $f$ at the point $t_0$ or  the velocity vector of the curve $f$ at the point $t_0$ . For $n = 1$ one can find the wording velocity vector , but I have never seen that $f'(t_0)$ is called the tangent vector of $f  :J \to \mathbb R$ at $t_0$ . The definition of the derivative $f'(t_0)$ as the limit $\lim_{t \to t_0}\dfrac{f(t)-f(t_0)}{t-t_0}$ is nevertheless motivated by the concept of tangent by saying that $f'(t_0)$ is the slope of the tangent of the graph $G(f) = \{(t,f(t)) \mid t \in J \} \subset \mathbb R^2$ at the point $(t_0,f(t_0))$ . There is also a notational relation to the case $n > 1$ : If we consider the curve $\bar f : J \to \mathbb R^2, \bar f(t) = (t,f(t))$ , then we get $f'(t_0)$ as the second coordinate of the tangent vector $\bar f'(t_0) \in \mathbb R^2$ . Finally, if we consider a smooth ( $C^\infty$ ) curve $f$ and a point $t_0 \in J$ such that $f'(t_0) \ne 0$ , then $M  = f(J)$ is locally around $p_0 = f(t_0)$ a smooth one-dimensional submanifold of $\mathbb R^n$ . It has a tangent space $T_{p_0}M$ at $p_0$ which we may regard as a one-dimensional linear subspace of $\mathbb R^n$ and all $v \in T_{p_0}M$ are called tangent vectors at $M$ at $p_0$ . This suggests that all scalar multiples of the tangent vector $f'(t_0)$ can also be regarded as tangent vectors (which appears reasonable to me). I find this notationally confusing. The word ""tangent vector"" seems to have various different interpretations, but in the most elementary case $n = 1$ it is not used. Would it be better to avoid using the name ""tangent vector"" for $f'(t_0)$ , but to use the unambiguous ""velocity vector""? Perhaps somebody can help me to clarify my disorientation.","Let us understand a curve as a differentiable map defined on an open interval . The derivative of at is given as the vector where the are the coordinate functions of . I think for it is usual to say that is the tangent vector to the curve at the point or  the velocity vector of the curve at the point . For one can find the wording velocity vector , but I have never seen that is called the tangent vector of at . The definition of the derivative as the limit is nevertheless motivated by the concept of tangent by saying that is the slope of the tangent of the graph at the point . There is also a notational relation to the case : If we consider the curve , then we get as the second coordinate of the tangent vector . Finally, if we consider a smooth ( ) curve and a point such that , then is locally around a smooth one-dimensional submanifold of . It has a tangent space at which we may regard as a one-dimensional linear subspace of and all are called tangent vectors at at . This suggests that all scalar multiples of the tangent vector can also be regarded as tangent vectors (which appears reasonable to me). I find this notationally confusing. The word ""tangent vector"" seems to have various different interpretations, but in the most elementary case it is not used. Would it be better to avoid using the name ""tangent vector"" for , but to use the unambiguous ""velocity vector""? Perhaps somebody can help me to clarify my disorientation.","f : J \to \mathbb R^n J \subset \mathbb R f'(t_0) f t_0 \in J (f'_1(t_0),\ldots,f'_n(t_0)) \in \mathbb R^n f_i : J \to \mathbb R f n > 1 f'(t_0) f t_0 f t_0 n = 1 f'(t_0) f  :J \to \mathbb R t_0 f'(t_0) \lim_{t \to t_0}\dfrac{f(t)-f(t_0)}{t-t_0} f'(t_0) G(f) = \{(t,f(t)) \mid t \in J \} \subset \mathbb R^2 (t_0,f(t_0)) n > 1 \bar f : J \to \mathbb R^2, \bar f(t) = (t,f(t)) f'(t_0) \bar f'(t_0) \in \mathbb R^2 C^\infty f t_0 \in J f'(t_0) \ne 0 M  = f(J) p_0 = f(t_0) \mathbb R^n T_{p_0}M p_0 \mathbb R^n v \in T_{p_0}M M p_0 f'(t_0) n = 1 f'(t_0)","['calculus', 'multivariable-calculus', 'differential-geometry', 'notation']"
49,Stokes' theorem in higher dimensions,Stokes' theorem in higher dimensions,,"I'm trying to follow along with Stokes's theorem on page 46 and I'm not quite understanding where this comes from: $$ V=\int_{B} dx \wedge dy \wedge dz \wedge dt = - \int_{S} t dx \wedge dy \wedge dz $$ I can understand the negative due to the anticommutivity, $\int_{B} dx \wedge dy \wedge dz \wedge dt = - \int_{B} dt \wedge dx \wedge dy \wedge dz$ due to how there would be three interchanges But where does the $d$ for the $t$ go? Is it as simple as integrating with respect to $t$ but leaving all the rest the same? (So far, I've only applied $d$ to various forms and so I'm not sure how it works in reverse. -- I can understand the first line is Stokes' theorem, but how would you know that α is that? I'd know that the volume is $\int_{B} dx \wedge dy \wedge dz \wedge dt = \int_{domain} d\alpha $ and then I'd need α itself to integrate over the surface like $\int_{surface} \alpha$ . But how would we get $\alpha$ if given $d\alpha$ ?","I'm trying to follow along with Stokes's theorem on page 46 and I'm not quite understanding where this comes from: I can understand the negative due to the anticommutivity, due to how there would be three interchanges But where does the for the go? Is it as simple as integrating with respect to but leaving all the rest the same? (So far, I've only applied to various forms and so I'm not sure how it works in reverse. -- I can understand the first line is Stokes' theorem, but how would you know that α is that? I'd know that the volume is and then I'd need α itself to integrate over the surface like . But how would we get if given ?", V=\int_{B} dx \wedge dy \wedge dz \wedge dt = - \int_{S} t dx \wedge dy \wedge dz  \int_{B} dx \wedge dy \wedge dz \wedge dt = - \int_{B} dt \wedge dx \wedge dy \wedge dz d t t d \int_{B} dx \wedge dy \wedge dz \wedge dt = \int_{domain} d\alpha  \int_{surface} \alpha \alpha d\alpha,"['multivariable-calculus', 'differential-geometry', 'vector-analysis', 'stokes-theorem']"
50,integration by substitution from two variables to one,integration by substitution from two variables to one,,In lecture notes I have seen the substitution $t = x\cdot y$ applied for the following integral: $$\int_{0}^{\infty}e^{-\frac{y^2}{2}}\int_{0}^{\infty}e^{-\frac{(xy)^2}{2}}\cdot y \: dxdy =  \left( \int_{0}^{\infty}e^{-\frac{t^2}{2}}dt \right) ^2$$ Why can two variables be substituted by only one and what are the intermediate steps?,In lecture notes I have seen the substitution applied for the following integral: Why can two variables be substituted by only one and what are the intermediate steps?,t = x\cdot y \int_{0}^{\infty}e^{-\frac{y^2}{2}}\int_{0}^{\infty}e^{-\frac{(xy)^2}{2}}\cdot y \: dxdy =  \left( \int_{0}^{\infty}e^{-\frac{t^2}{2}}dt \right) ^2,"['calculus', 'integration', 'multivariable-calculus']"
51,Methods to Analytically Solve a Nonlinear PDE,Methods to Analytically Solve a Nonlinear PDE,,"I'm looking for suggestions to solve: $$ u_t = u_{xxxx}-3u(u_x)^2-\frac{3}{2}u^2u_{xx}+\frac{1}{2}u_{xx}+F $$ where $F$ is currently an unknown function of unknown type (might be linear, exponential, etc.). Ignoring that ambiguity (pretend it vanishes), I am unsure of any method to solve such a PDE. I attempted separation of variables but that didn't help because of the nonlinearities. I then tried seeking a steady-state solution however that didn't lead to anything fruitful. After that, I tried using the $1$ D and $2$ D Fourier Transforms in space and space and time respectively but again that didn't really help. In this case, I got terms that involved self-convolutions of either $u$ or $u_x$ and this made the equation even harder to solve. One last thought I had was to maybe use the Cole-Hopf Transform: i.e. introduce some new variable $w=\phi(u)$ . In doing this, I started calculating partials of $w$ that would appear in my PDE and the expressions I got seemed even more complicated. This kind of leads me to believe that this won't be a fruitful avenue either, however, I have never really tried using this method so I am wondering if I'm maybe doing something wrong. Here I found: $$ w_t = \phi'(u)u_t $$ $$ w_x = \phi'(u)u_x $$ $$ w_{xx} = \phi''(u)u_x^2+\phi'(u)u_{xx} $$ $$ w_{xxx} = \phi'''(u)u_x^3 + 3\phi''(u)u_xu_{xx}+\phi'(u)u_{xxx} $$ $$ w_{xxxx} = \phi^{(4)}(u)u_x^4 + 6\phi'''(u)u_x^2u_{xx}+3\phi''(u)u_{xx}^2+4\phi''(u)u_xu_{xxx}+\phi'(u)u_{xxxx} $$ Now I just don't really see how to use these equations to generate a simplified version of my original PDE or even how to definitively say that this won't help me in reducing the PDE. Any help or advice would be appreciated.","I'm looking for suggestions to solve: where is currently an unknown function of unknown type (might be linear, exponential, etc.). Ignoring that ambiguity (pretend it vanishes), I am unsure of any method to solve such a PDE. I attempted separation of variables but that didn't help because of the nonlinearities. I then tried seeking a steady-state solution however that didn't lead to anything fruitful. After that, I tried using the D and D Fourier Transforms in space and space and time respectively but again that didn't really help. In this case, I got terms that involved self-convolutions of either or and this made the equation even harder to solve. One last thought I had was to maybe use the Cole-Hopf Transform: i.e. introduce some new variable . In doing this, I started calculating partials of that would appear in my PDE and the expressions I got seemed even more complicated. This kind of leads me to believe that this won't be a fruitful avenue either, however, I have never really tried using this method so I am wondering if I'm maybe doing something wrong. Here I found: Now I just don't really see how to use these equations to generate a simplified version of my original PDE or even how to definitively say that this won't help me in reducing the PDE. Any help or advice would be appreciated.", u_t = u_{xxxx}-3u(u_x)^2-\frac{3}{2}u^2u_{xx}+\frac{1}{2}u_{xx}+F  F 1 2 u u_x w=\phi(u) w  w_t = \phi'(u)u_t   w_x = \phi'(u)u_x   w_{xx} = \phi''(u)u_x^2+\phi'(u)u_{xx}   w_{xxx} = \phi'''(u)u_x^3 + 3\phi''(u)u_xu_{xx}+\phi'(u)u_{xxx}   w_{xxxx} = \phi^{(4)}(u)u_x^4 + 6\phi'''(u)u_x^2u_{xx}+3\phi''(u)u_{xx}^2+4\phi''(u)u_xu_{xxx}+\phi'(u)u_{xxxx} ,"['multivariable-calculus', 'partial-differential-equations', 'transformation', 'nonlinear-dynamics']"
52,How to understand this helping lemma for mean value theorem which involves inner product?,How to understand this helping lemma for mean value theorem which involves inner product?,,"In Kolk's Multidimensional Real Analysis I: Differentiation He used the following helping lemma 2.5.1 to prove the following mean value theorem 2.5.3 Equivalent to the equation (2.16) is the equation: $\langle a, f(x)-f(x') - Df(\xi)(x-x') \rangle=0$ , from which we can conclude 2 cases: case 1, $f(x)-f(x') - Df(\xi)(x-x') = 0$ . It is easy to understand the meaning as in the 1-dimensional situation: we can find a point $\xi$ in between the segment $xx'$ at which the directional derivative in the direction $x-x'$ (or,  the rate of increase of the function $f$ at the point $\xi$ in this direction) is the same as $f(x)-f(x')$ case 2, $f(x)-f(x') - Df(\xi)(x-x') \neq 0$ but still orthogonal to $a$ . Questions: Is my interpretation of case 1 good enough? How to interpret case 2? Note that the reason the author used inner product to prove this lemma is to lay the foundation of proving the mean value theorem which uses ""norm"" in its statement.","In Kolk's Multidimensional Real Analysis I: Differentiation He used the following helping lemma 2.5.1 to prove the following mean value theorem 2.5.3 Equivalent to the equation (2.16) is the equation: , from which we can conclude 2 cases: case 1, . It is easy to understand the meaning as in the 1-dimensional situation: we can find a point in between the segment at which the directional derivative in the direction (or,  the rate of increase of the function at the point in this direction) is the same as case 2, but still orthogonal to . Questions: Is my interpretation of case 1 good enough? How to interpret case 2? Note that the reason the author used inner product to prove this lemma is to lay the foundation of proving the mean value theorem which uses ""norm"" in its statement.","\langle a, f(x)-f(x') - Df(\xi)(x-x') \rangle=0 f(x)-f(x') - Df(\xi)(x-x') = 0 \xi xx' x-x' f \xi f(x)-f(x') f(x)-f(x') - Df(\xi)(x-x') \neq 0 a","['real-analysis', 'multivariable-calculus', 'inner-products']"
53,The Integral of a Differential 1-form Along a Generic Curve,The Integral of a Differential 1-form Along a Generic Curve,,"Good day! Got stuck again on a Calc-III question, on the subject of differential 1-forms. I'll present the question and my thought process in trying to solve it up to now. Let $f: [a,b] \rightarrow \mathbb{R}$ be a continuous function of bounded variation such that $f(a) = f(b) = 0$ . Let $\varphi:= (t, f(t))$ for all $t \in [a,b]$ Prove that: $\int_\varphi (2xy + y)dx + x^2dy = \int_a^b f(x) dx$ First let us change notations a little by introducing - \begin{gather*}                 F: \mathbb{R}^2 \rightarrow \mathbb{R}^2 \\         (x, y) \rightarrow (2xy + y, x^2) \end{gather*} Then, $F \cdot d(x, y)$ is a differential 1-form. It is easily deducible that the we are to prove that $\int_\varphi F \cdot d(x, y) = \int_a^b f(x) dx$ . Now, $f$ is integrable over $[a,b]$ as it is continuous there. In addition, $\varphi$ is continuous as a collection of continuous functions, and it is also a collection of functions of bounded variation. Thus, $\varphi$ is rectifiable and therefore $F \cdot dx$ is integrable over $\varphi$ as $F$ is also continuous in $\mathbb{R}^2$ let alone in the image of $\varphi$ . As for the actual question at hand, the first thing that came to mind is to show that $F$ is a conservative field (specifically using the 1-form case of Poincare's lemma ). But as it turns out, (or immediately seen if you're used to these kind of processes unlike myself) the matrix $JF_a$ is asymmetrical for pretty much every $a$ : $\frac{\partial [F]_1}{\partial y}(x) = 2x + 1$ , $\frac{\partial [F]_2}{\partial x}(x) = 2x$ . As you can see it is almost symmetric but not quite. So in fact $F$ is not a conservative field. Even if we try to restrict its domain, I don't think this will yield anything useful. (I don't think any possibly helpful restriction is star-shaped anyhow) The next thing one might try is to show somehow that $\varphi$ is continuously differentiable but $f$ might very well not be differentiable in it's domain. At a last-ditch effort I tried computing the Rieman's sum of the left side and somehow get the right side, but I got no where with that except maybe a waste of a page. Thanks for reading! Would be grateful if anyone could lend a hint towards the solution. Have a great time for the rest of your day!","Good day! Got stuck again on a Calc-III question, on the subject of differential 1-forms. I'll present the question and my thought process in trying to solve it up to now. Let be a continuous function of bounded variation such that . Let for all Prove that: First let us change notations a little by introducing - Then, is a differential 1-form. It is easily deducible that the we are to prove that . Now, is integrable over as it is continuous there. In addition, is continuous as a collection of continuous functions, and it is also a collection of functions of bounded variation. Thus, is rectifiable and therefore is integrable over as is also continuous in let alone in the image of . As for the actual question at hand, the first thing that came to mind is to show that is a conservative field (specifically using the 1-form case of Poincare's lemma ). But as it turns out, (or immediately seen if you're used to these kind of processes unlike myself) the matrix is asymmetrical for pretty much every : , . As you can see it is almost symmetric but not quite. So in fact is not a conservative field. Even if we try to restrict its domain, I don't think this will yield anything useful. (I don't think any possibly helpful restriction is star-shaped anyhow) The next thing one might try is to show somehow that is continuously differentiable but might very well not be differentiable in it's domain. At a last-ditch effort I tried computing the Rieman's sum of the left side and somehow get the right side, but I got no where with that except maybe a waste of a page. Thanks for reading! Would be grateful if anyone could lend a hint towards the solution. Have a great time for the rest of your day!","f: [a,b] \rightarrow \mathbb{R} f(a) = f(b) = 0 \varphi:= (t, f(t)) t \in [a,b] \int_\varphi (2xy + y)dx + x^2dy = \int_a^b f(x) dx \begin{gather*}        
        F: \mathbb{R}^2 \rightarrow \mathbb{R}^2 \\
        (x, y) \rightarrow (2xy + y, x^2)
\end{gather*} F \cdot d(x, y) \int_\varphi F \cdot d(x, y) = \int_a^b f(x) dx f [a,b] \varphi \varphi F \cdot dx \varphi F \mathbb{R}^2 \varphi F JF_a a \frac{\partial [F]_1}{\partial y}(x) = 2x + 1 \frac{\partial [F]_2}{\partial x}(x) = 2x F \varphi f","['multivariable-calculus', 'differential-forms', 'line-integrals']"
54,Typo in Spivak Calculus on Manifolds not found in various errata?,Typo in Spivak Calculus on Manifolds not found in various errata?,,"Did I find an error on page 42 of my edition of Spivak's Calculus on Manifolds regarding computing the partial derivatives of the implicit function defined by the Implicit Function Theorem? I don't find this error listed in any errata online, but can't make sense of what is written. For convenience, Spivak states the Implicit Function Theorem as: Suppose $f: R^n \times R^m \to R^m$ is continuously differentiable in an open set containing $(a,b)$ and $f(a,b) = 0$ . Let $M$ be the $m \times m$ matrix $$ (D_{n+j}f^i(a,b)) \qquad 1\leq i,j \leq m.$$ If $\det{M} \neq 0$ , there is an open set $A \subset R^n$ containing $a$ and an open set $B \subset R^m$ containing $b$ , with the following property: for each $x \in A$ there is a unique $g(x) \in B$ such that $f(x,g(x)) = 0$ . The function $g$ is differentiable. Spivak's goes on to explain how to find the partial derivatives of $g$ by taking $D_j$ of both sides of $f(x,g(x)) = 0$ and he writes: $$0 = D_jf^i(x,g(x)) + \sum_{\alpha=1}^{m}D_{n+\alpha}f^i(x,g(x)) \cdot D_jg^\alpha(x) \\ i,j = 1, \ldots, m.$$ But shouldn't $j$ go from $1$ to $n$ , not from $1$ to $m$ ? If $g: R^n \to R^m$ , then each $g^\alpha$ has $n$ partial derivatives.","Did I find an error on page 42 of my edition of Spivak's Calculus on Manifolds regarding computing the partial derivatives of the implicit function defined by the Implicit Function Theorem? I don't find this error listed in any errata online, but can't make sense of what is written. For convenience, Spivak states the Implicit Function Theorem as: Suppose is continuously differentiable in an open set containing and . Let be the matrix If , there is an open set containing and an open set containing , with the following property: for each there is a unique such that . The function is differentiable. Spivak's goes on to explain how to find the partial derivatives of by taking of both sides of and he writes: But shouldn't go from to , not from to ? If , then each has partial derivatives.","f: R^n \times R^m \to R^m (a,b) f(a,b) = 0 M m \times m  (D_{n+j}f^i(a,b)) \qquad 1\leq i,j \leq m. \det{M} \neq 0 A \subset R^n a B \subset R^m b x \in A g(x) \in B f(x,g(x)) = 0 g g D_j f(x,g(x)) = 0 0 = D_jf^i(x,g(x)) + \sum_{\alpha=1}^{m}D_{n+\alpha}f^i(x,g(x)) \cdot D_jg^\alpha(x) \\
i,j = 1, \ldots, m. j 1 n 1 m g: R^n \to R^m g^\alpha n","['multivariable-calculus', 'solution-verification', 'manifolds']"
55,Showing $\lim_{h \to 0} \frac{\|a+h\| - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)}{\|h\|} = 0$,Showing,\lim_{h \to 0} \frac{\|a+h\| - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)}{\|h\|} = 0,"Show for any $a \neq 0 $ we have: $$\lim_{h \to 0} \frac{\|a+h\| - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)}{\|h\|} = 0$$ Note: all terms are vectors of any dimension. Hint: remember that $x - y = \frac{x^{2} - y ^{2}}{x+y}$ Attempt : I've gotten this far $$\frac{\|a+h\|^{2} - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)^{2}}{\|h\| \bigg(\|a+h\| + \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)\bigg)}$$ I expanded it form here, but I don't see anything that remotely looks like it will help in cancelling out the $\|h\|$ term. I do have Cauchy Schwartz available, but it won't make any inequality true if I apply it to $a \cdot h$","Show for any we have: Note: all terms are vectors of any dimension. Hint: remember that Attempt : I've gotten this far I expanded it form here, but I don't see anything that remotely looks like it will help in cancelling out the term. I do have Cauchy Schwartz available, but it won't make any inequality true if I apply it to",a \neq 0  \lim_{h \to 0} \frac{\|a+h\| - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)}{\|h\|} = 0 x - y = \frac{x^{2} - y ^{2}}{x+y} \frac{\|a+h\|^{2} - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)^{2}}{\|h\| \bigg(\|a+h\| + \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)\bigg)} \|h\| a \cdot h,"['real-analysis', 'multivariable-calculus']"
56,Soft question: What does it mean if the area integral of the divergence of a vector field over a region is negative?,Soft question: What does it mean if the area integral of the divergence of a vector field over a region is negative?,,"Let $\vec{F}(x,y)$ be a vector field over $\mathbb{R}^2$ and let $\nabla \cdot \vec{F}$ be its divergence.  Presume that $\delta$ is a circle in the $x,y$ plane.  If I compute: $$I = \int\int_\delta \nabla \cdot \vec{F}dA$$ and find that $I < 0$ , what does that mean?  Is it correct to say that the net flux of the vector field acrosss the boundary of that region is negative?  Is it further correct to say that $\vec{F}$ has more sinks than sources in that region?","Let be a vector field over and let be its divergence.  Presume that is a circle in the plane.  If I compute: and find that , what does that mean?  Is it correct to say that the net flux of the vector field acrosss the boundary of that region is negative?  Is it further correct to say that has more sinks than sources in that region?","\vec{F}(x,y) \mathbb{R}^2 \nabla \cdot \vec{F} \delta x,y I = \int\int_\delta \nabla \cdot \vec{F}dA I < 0 \vec{F}","['multivariable-calculus', 'soft-question']"
57,Proof with Double Integral with two variables,Proof with Double Integral with two variables,,"I need to prove that the following statement is true: $$\iint_D e^{-x^2-y^2}dxdy=4\left(\int_0^Re^{-t^2}dt\right)^2$$ where $ D = \{|x| \leq R, |y| \leq R\} $ and $ 0 \leq R.$ I've tried exchanging the variables to polar coordinates: $$x = t\cos\theta, \quad y = t\sin\theta$$ so $$t^2 = x^2 +y^2.$$ but I was unable to find the new integration limits for $t$ and $\theta$ . Any help with advancing the proof would be greatly appreciated!",I need to prove that the following statement is true: where and I've tried exchanging the variables to polar coordinates: so but I was unable to find the new integration limits for and . Any help with advancing the proof would be greatly appreciated!,"\iint_D e^{-x^2-y^2}dxdy=4\left(\int_0^Re^{-t^2}dt\right)^2  D = \{|x| \leq R, |y| \leq R\}   0 \leq R. x = t\cos\theta, \quad y = t\sin\theta t^2 = x^2 +y^2. t \theta","['multivariable-calculus', 'definite-integrals', 'multiple-integral']"
58,Generalizations of Poisson's Equation,Generalizations of Poisson's Equation,,"I'm currently reading a book in Multivariable Calculus, and there is a section on Applications of Calculus to Physics - Poisson's Equation. It states the following: We have the 3D version: Let $$u(x) = \iiint_{\Bbb{R}^3}\frac{p(x+y)}{|y|}d^3y$$ , where $p$ is a $C^2$ function on $\mathbb{R}^3$ , that vanished outside a bounded set. Then $u$ is of class $C^2$ and $\nabla^2u = -4\pi p$ . The 2D analog: Let $u(x) = \int p(x+y)\log{|y|}d^2y$ , where $p$ is a $C^2$ function on $\mathbb{R}^2$ , that vanished outside a bounded set. Then $u$ is of class $C^2$ and $\nabla^2u = 2\pi p$ . I was wondering how we could generalize this to 4 dimensions - or even n dimensions (if its possible). My conjecture for the 4D version is: Let $$u(x) = \iiiint_{\Bbb{R}^4}\frac{p(x+y)}{|y|^2}d^4y$$ , where $p$ is a $C^2$ function on $\mathbb{R}^4$ , that vanished outside a bounded set. Then $u$ is of class $C^2$ and $\nabla^2u = -4\pi^2 p$ . Is this conjecture correct? If not, what would the correct analog be, and if so, how could we prove it?","I'm currently reading a book in Multivariable Calculus, and there is a section on Applications of Calculus to Physics - Poisson's Equation. It states the following: We have the 3D version: Let , where is a function on , that vanished outside a bounded set. Then is of class and . The 2D analog: Let , where is a function on , that vanished outside a bounded set. Then is of class and . I was wondering how we could generalize this to 4 dimensions - or even n dimensions (if its possible). My conjecture for the 4D version is: Let , where is a function on , that vanished outside a bounded set. Then is of class and . Is this conjecture correct? If not, what would the correct analog be, and if so, how could we prove it?",u(x) = \iiint_{\Bbb{R}^3}\frac{p(x+y)}{|y|}d^3y p C^2 \mathbb{R}^3 u C^2 \nabla^2u = -4\pi p u(x) = \int p(x+y)\log{|y|}d^2y p C^2 \mathbb{R}^2 u C^2 \nabla^2u = 2\pi p u(x) = \iiiint_{\Bbb{R}^4}\frac{p(x+y)}{|y|^2}d^4y p C^2 \mathbb{R}^4 u C^2 \nabla^2u = -4\pi^2 p,"['real-analysis', 'multivariable-calculus', 'poissons-equation']"
59,Multivariable Calculus Limits,Multivariable Calculus Limits,,"$$\lim_{(x,y) \to (0,0)}= \frac{xy^3}{x^2-x^4 + y^4}$$ This is one of the limits that I have been tasked with calculating and I have tried searching similar limits on this website so I can learn how to solve this but I haven't found any... Also, I haven't been taught conversion to polar coordinates which might have made this easier. My best guess so far has been to use the sandwich theorem with the $|xy^3|$ on either side but that computes it to be zero and I'm not very sure of this method. $$\lim_{(x,y) \to (0,0)} \frac{x^3 + y^4}{x^2 + y^4}$$ And this is the second one: Now I did find a similar question to this but the method and answer were vastly different. Here I am thinking of using the two-path method with $x=0$ and $x=y$ which shows that the limit diverges (DNE). Would that be correct?","This is one of the limits that I have been tasked with calculating and I have tried searching similar limits on this website so I can learn how to solve this but I haven't found any... Also, I haven't been taught conversion to polar coordinates which might have made this easier. My best guess so far has been to use the sandwich theorem with the on either side but that computes it to be zero and I'm not very sure of this method. And this is the second one: Now I did find a similar question to this but the method and answer were vastly different. Here I am thinking of using the two-path method with and which shows that the limit diverges (DNE). Would that be correct?","\lim_{(x,y) \to (0,0)}= \frac{xy^3}{x^2-x^4 + y^4} |xy^3| \lim_{(x,y) \to (0,0)} \frac{x^3 + y^4}{x^2 + y^4} x=0 x=y","['multivariable-calculus', 'limits-without-lhopital']"
60,Find the extrema points of $ f\left(x\right)=\prod_{i=1}^{n}x_{i} $,Find the extrema points of, f\left(x\right)=\prod_{i=1}^{n}x_{i} ,"Let $ f:\mathbb{R}^n \to \mathbb{R} $ given by $ f\left(x\right)=\prod_{i=1}^{n}x_{i} $ I have to find the extrema points of $ f $ under the following constraint: $ S=\left\{ \left(x_{1},...,x_{n}\right):\sum_{i=1}^{n}x_{i}^{p}=1\right\}  $ Where $ p $ is some positive even integer. I tried to solve using Lagrange multipliers methood but it bacame too complicated so I doubt that thats the way to solve the question. I'd appreaciate some help. Thanks in advance.",Let given by I have to find the extrema points of under the following constraint: Where is some positive even integer. I tried to solve using Lagrange multipliers methood but it bacame too complicated so I doubt that thats the way to solve the question. I'd appreaciate some help. Thanks in advance.," f:\mathbb{R}^n \to \mathbb{R}   f\left(x\right)=\prod_{i=1}^{n}x_{i}   f   S=\left\{ \left(x_{1},...,x_{n}\right):\sum_{i=1}^{n}x_{i}^{p}=1\right\}    p ","['real-analysis', 'calculus', 'multivariable-calculus', 'extreme-value-theorem']"
61,Equation of Partial derivatives,Equation of Partial derivatives,,"I am currently learning multivariable calculus and the following was done in my lecture to prove one of the properties of Jacobian The property: ( $JJ'=1$ ), where $J$ is $\frac{\partial (x,y)}{\partial(u,v)}$ and $J'$ is $\frac{\partial(u,v)}{\partial (x,y)}$ The equation in the proof: $\partial u = \frac{\partial u}{\partial x} \partial x +\frac{\partial u}{\partial y} {\partial y}$ Then the professoor proceeded to divide by $\partial u$ on both sides so you end up with $\frac{\partial u}{\partial u}$ on the left hand side which is essential 1. He then did this for $v$ also. My question: $\partial u\ , \partial x\ , \partial y$ on their own makes no sense. And since partial derivative is defined for suppose u with respect to some other variable, dividing or multiplying by $\partial (some \ variable)$ should be wrong. What is the correct theory ?","I am currently learning multivariable calculus and the following was done in my lecture to prove one of the properties of Jacobian The property: ( ), where is and is The equation in the proof: Then the professoor proceeded to divide by on both sides so you end up with on the left hand side which is essential 1. He then did this for also. My question: on their own makes no sense. And since partial derivative is defined for suppose u with respect to some other variable, dividing or multiplying by should be wrong. What is the correct theory ?","JJ'=1 J \frac{\partial (x,y)}{\partial(u,v)} J' \frac{\partial(u,v)}{\partial (x,y)} \partial u = \frac{\partial u}{\partial x} \partial x +\frac{\partial u}{\partial y} {\partial y} \partial u \frac{\partial u}{\partial u} v \partial u\ , \partial x\ , \partial y \partial (some \ variable)","['calculus', 'multivariable-calculus']"
62,"Integrability of generalization of Thomae's function on $[0,1] \times [0,1]$",Integrability of generalization of Thomae's function on,"[0,1] \times [0,1]","Let $f:\left[0,1\right]\times\left[0,1\right]\to \mathbb{R}$ be defined by $$f\left ( x,y \right )=\begin{cases}  0& ,x\in\mathbb{I} \\   0& ,x\in\mathbb{Q},\hspace{2mm} y\in\mathbb{I}\\   \frac{1}{q}& ,x\in \mathbb{Q},\hspace{2mm} y=\frac{p}{q}\hspace{2mm} \text{in lowest terms} \end{cases}$$ Show that $f$ is integrable and $$\int_{\left[0,1\right]\times\left[0,1\right]}f=0$$ That was my tried. We know that $L(f,P)=0$ for any partition of $[0,1]\times[0,1]$ (This is easy to proof since the density of $\mathbb{I}$ on $\mathbb{R}$ ). Therefore, is enough to proof that given $\varepsilon$ there exist a partition $P$ such that $U(f,P)<\varepsilon$ . Then for any $\varepsilon$ there exist $n\in \mathbb{N}$ st $\frac{1}{n}<\frac{\varepsilon}{2}$ . Let $P$ the following partition $$P=(\{0,\frac{1}{n},\dots, \frac{n-1}{n},1\},\{0,1\})$$ . The volume $v(S)$ of each rectangle $S$ of the partition $P$ is $\frac{1}{n}$ , but i don't how to calculate $M_S(f)=\sup\{f(x,y):(x,y)\in S\}$ . This exercise was taken from Calculus on Manifolds - Michael Spivak, Chapter 3. Integration, exercise 3-7.","Let be defined by Show that is integrable and That was my tried. We know that for any partition of (This is easy to proof since the density of on ). Therefore, is enough to proof that given there exist a partition such that . Then for any there exist st . Let the following partition . The volume of each rectangle of the partition is , but i don't how to calculate . This exercise was taken from Calculus on Manifolds - Michael Spivak, Chapter 3. Integration, exercise 3-7.","f:\left[0,1\right]\times\left[0,1\right]\to \mathbb{R} f\left ( x,y \right )=\begin{cases}
 0& ,x\in\mathbb{I} \\ 
 0& ,x\in\mathbb{Q},\hspace{2mm} y\in\mathbb{I}\\ 
 \frac{1}{q}& ,x\in \mathbb{Q},\hspace{2mm} y=\frac{p}{q}\hspace{2mm} \text{in lowest terms}
\end{cases} f \int_{\left[0,1\right]\times\left[0,1\right]}f=0 L(f,P)=0 [0,1]\times[0,1] \mathbb{I} \mathbb{R} \varepsilon P U(f,P)<\varepsilon \varepsilon n\in \mathbb{N} \frac{1}{n}<\frac{\varepsilon}{2} P P=(\{0,\frac{1}{n},\dots, \frac{n-1}{n},1\},\{0,1\}) v(S) S P \frac{1}{n} M_S(f)=\sup\{f(x,y):(x,y)\in S\}","['real-analysis', 'multivariable-calculus', 'riemann-integration']"
63,"Using spherical coordinates, is there an equation of a sphere not centered at the origin? If so what is it?","Using spherical coordinates, is there an equation of a sphere not centered at the origin? If so what is it?",,"I am a high school teacher teaching Calculus for the first time actually, I am teaching Multivariable Calculus (Calculus 3). Its been a solid 15 years since I took Calculus 3. During a discussion of the text my class and I came up with a question none of us could answer. Each of the textbooks given examples of the spherical coordinates (of a sphere) is centered at the origin. If the center of a given sphere is not at origin, are there any changes in the spherical coordinates? If there is, how do I manipulate that? Now I am not certain if this is even considered, our text says it will not in rectangular coordinates, so I assume it won't in spherical. I am very rusty on this topic and would love additional insights or possible resources.","I am a high school teacher teaching Calculus for the first time actually, I am teaching Multivariable Calculus (Calculus 3). Its been a solid 15 years since I took Calculus 3. During a discussion of the text my class and I came up with a question none of us could answer. Each of the textbooks given examples of the spherical coordinates (of a sphere) is centered at the origin. If the center of a given sphere is not at origin, are there any changes in the spherical coordinates? If there is, how do I manipulate that? Now I am not certain if this is even considered, our text says it will not in rectangular coordinates, so I assume it won't in spherical. I am very rusty on this topic and would love additional insights or possible resources.",,"['multivariable-calculus', 'spherical-coordinates']"
64,"Prove that the cone $S = \{(x,y,z) \in \Bbb R^3 ~|~x^2+y^2-z^2=0 \}$ is not a smooth surface",Prove that the cone  is not a smooth surface,"S = \{(x,y,z) \in \Bbb R^3 ~|~x^2+y^2-z^2=0 \}","Prove that the cone $S = \{(x,y,z) \in \Bbb R^3 ~|~x^2+y^2-z^2=0 \}$ is not a smooth surface. The book I am reading - Vector calculus by Peter Baxandall gives the following hint : (Let $S \subseteq \Bbb R^m$ be a smooth surface. Then there exists an open subset $E \in \Bbb R^m$ such that $f$ is smooth on $E$ and $S$ is a level set of the function $f:E \subseteq \Bbb R^m \rightarrow \Bbb R$ ) Attempt: Suppose that $S$ is a smooth surface. Then, there exists a smooth function $f: E \subseteq \Bbb R^3 \rightarrow \Bbb R$ such that $$S=\{(x,y,z) \in E~|~f(x,y,z)=0 \}$$ Clearly $(0,0,0) \in S$ as $f(0,0,0)=0$ Consider the three differentiable paths $\alpha.\beta,\gamma$ in $S$ given by : $$\alpha(t)=\big(t,0,t\big),~~ \beta(t)=\big(0,t,t\big),~~\gamma(t)=\big(t,t,\sqrt 2 t\big), t\in [-1,1]$$ We  can see that $\forall t \in \Bbb R : \alpha(t),\beta(t),\gamma(t) \subseteq S$ because $ f( \alpha(t))= f(\beta(t))= f(\gamma(t))=0$ Then $\alpha'(t)=\big(1,0,1\big),~~ \beta'(t)=\big(0,1,1\big),~~\gamma'(t)=\big(1,1,\sqrt 2 \big)$ . We can see that these vectors are linearly independent. This is the direction specified in the book. How do I use this info to obtain a contradiction ? Intuitively, we should now obtain a contradiction by proving that $f$ is not a smooth function. Any hints on obtaining a contradiction will be deeply appreciated EDIT: Problem Statement $Q.11$ in the textbook with the hint :","Prove that the cone is not a smooth surface. The book I am reading - Vector calculus by Peter Baxandall gives the following hint : (Let be a smooth surface. Then there exists an open subset such that is smooth on and is a level set of the function ) Attempt: Suppose that is a smooth surface. Then, there exists a smooth function such that Clearly as Consider the three differentiable paths in given by : We  can see that because Then . We can see that these vectors are linearly independent. This is the direction specified in the book. How do I use this info to obtain a contradiction ? Intuitively, we should now obtain a contradiction by proving that is not a smooth function. Any hints on obtaining a contradiction will be deeply appreciated EDIT: Problem Statement in the textbook with the hint :","S = \{(x,y,z) \in \Bbb R^3 ~|~x^2+y^2-z^2=0 \} S \subseteq \Bbb R^m E \in \Bbb R^m f E S f:E \subseteq \Bbb R^m \rightarrow \Bbb R S f: E \subseteq \Bbb R^3 \rightarrow \Bbb R S=\{(x,y,z) \in E~|~f(x,y,z)=0 \} (0,0,0) \in S f(0,0,0)=0 \alpha.\beta,\gamma S \alpha(t)=\big(t,0,t\big),~~ \beta(t)=\big(0,t,t\big),~~\gamma(t)=\big(t,t,\sqrt 2 t\big), t\in [-1,1] \forall t \in \Bbb R : \alpha(t),\beta(t),\gamma(t) \subseteq S  f( \alpha(t))= f(\beta(t))= f(\gamma(t))=0 \alpha'(t)=\big(1,0,1\big),~~ \beta'(t)=\big(0,1,1\big),~~\gamma'(t)=\big(1,1,\sqrt 2 \big) f Q.11","['multivariable-calculus', 'differential-geometry', 'vector-analysis', 'smooth-functions', 'non-smooth-analysis']"
65,Mild Error on Page 32 Spivak's Calculus of Manifolds that I couldn't find online.,Mild Error on Page 32 Spivak's Calculus of Manifolds that I couldn't find online.,,"I'm just asking if this is actually an error, as I could not find it in any errata online, math stackexchange questions etc. In Spivak's Calculus on Manifolds, page 32, I believe there is a mild error in the statement of Theorem 2-9. The theorem states: ""Let $g_{1} ,..., g_{m}$ : $\Bbb{R}^{n} \rightarrow \Bbb{R}$ be continuously differentiable at $a$ and let $f:\Bbb{R}^{m} \rightarrow \Bbb{R}$ be differentiable at $(g_{1}(a), ... , g_{m}(a)) $ . Define the function $F:\Bbb{R}^{n} \rightarrow \Bbb{R}$ by $F(x) = f(g_{1}(x), ... , g_{m}(x)). $ Then $D_{i}F(a) = \sum_{j=1}^m D_{j}f(g_{1}(a), ... ,g_{m}(a))\cdot D_{i}g_{j}(a).$ "" I believe it is an error that the $g_{i}$ must be assumed to be continuously differentiable (as opposed to just differentiable), as he proves in Theorem 2-3 on page 20 that the function $g:\Bbb{R}^{n} \rightarrow \Bbb{R}^{m}, x\rightarrow(g_{1}(x), ... , g_{m}(x))$ is differentiable iff the $g_{i}$ are just differentiable, with no continuity requirement. Normally I would just ignore this and assume it is an error, but he explicitly states after the proof that this theorem is weaker than the chain rule because the $g_{i}$ must be continuously differentiable. Am I correct in assuming that by Theorem 2-3 they need not be?","I'm just asking if this is actually an error, as I could not find it in any errata online, math stackexchange questions etc. In Spivak's Calculus on Manifolds, page 32, I believe there is a mild error in the statement of Theorem 2-9. The theorem states: ""Let : be continuously differentiable at and let be differentiable at . Define the function by Then "" I believe it is an error that the must be assumed to be continuously differentiable (as opposed to just differentiable), as he proves in Theorem 2-3 on page 20 that the function is differentiable iff the are just differentiable, with no continuity requirement. Normally I would just ignore this and assume it is an error, but he explicitly states after the proof that this theorem is weaker than the chain rule because the must be continuously differentiable. Am I correct in assuming that by Theorem 2-3 they need not be?","g_{1} ,..., g_{m} \Bbb{R}^{n} \rightarrow \Bbb{R} a f:\Bbb{R}^{m} \rightarrow \Bbb{R} (g_{1}(a), ... , g_{m}(a))  F:\Bbb{R}^{n} \rightarrow \Bbb{R} F(x) = f(g_{1}(x), ... , g_{m}(x)).  D_{i}F(a) = \sum_{j=1}^m D_{j}f(g_{1}(a), ... ,g_{m}(a))\cdot D_{i}g_{j}(a). g_{i} g:\Bbb{R}^{n} \rightarrow \Bbb{R}^{m}, x\rightarrow(g_{1}(x), ... , g_{m}(x)) g_{i} g_{i}",['multivariable-calculus']
66,Find a closed surface for which $\iint_S \textbf{F} \cdot d\vec{S}$ is negative.,Find a closed surface for which  is negative.,\iint_S \textbf{F} \cdot d\vec{S},"$\text{div}\textbf{F} = x^2+y^2+1$ . Find a closed surface for which $$\iint_S \textbf{F} \cdot d\vec{S}$$ is negative or otherwise state why it's not possible. $\textbf{My attempt}$ : If $S$ and $D$ are a domain such that $\textbf{F}$ is continuous and differentiable on $S \cup D$ , then by the divergence theorem: $$\iint_S \textbf{F} \cdot d\vec{S} = \iiint_D  x^2+y^2+1 \ dV $$ The integral on the right is always positive and therefore there exists no such surface $S$ for which the flux integral becomes negative. Can anyone confirm that my approach is correct? Also, what if $S \cup D$ has singularities? Can we use the extended form of the divergence theorem: $$\iint_S \textbf{F} \cdot d\vec{S} = \iint_{S'} \textbf{F} \cdot d\vec{S} +\iiint_{D'}  x^2+y^2+1 \ dV $$ and conclude that there may exist such a surface for which $$\iint_S \textbf{F} \cdot d\vec{S} < 0$$ $S'$ encloses the singularity.",". Find a closed surface for which is negative or otherwise state why it's not possible. : If and are a domain such that is continuous and differentiable on , then by the divergence theorem: The integral on the right is always positive and therefore there exists no such surface for which the flux integral becomes negative. Can anyone confirm that my approach is correct? Also, what if has singularities? Can we use the extended form of the divergence theorem: and conclude that there may exist such a surface for which encloses the singularity.",\text{div}\textbf{F} = x^2+y^2+1 \iint_S \textbf{F} \cdot d\vec{S} \textbf{My attempt} S D \textbf{F} S \cup D \iint_S \textbf{F} \cdot d\vec{S} = \iiint_D  x^2+y^2+1 \ dV  S S \cup D \iint_S \textbf{F} \cdot d\vec{S} = \iint_{S'} \textbf{F} \cdot d\vec{S} +\iiint_{D'}  x^2+y^2+1 \ dV  \iint_S \textbf{F} \cdot d\vec{S} < 0 S',"['calculus', 'multivariable-calculus', 'vector-analysis', 'divergence-theorem']"
67,Why can't some functions be integrated?,Why can't some functions be integrated?,,"There are some functions (such as xtan(x)dx [without limits] ) which can't be integrated, why? How can one understand which function is integrable or not?","There are some functions (such as xtan(x)dx [without limits] ) which can't be integrated, why? How can one understand which function is integrable or not?",,"['calculus', 'integration', 'multivariable-calculus', 'functional-calculus']"
68,Are there function with one local extremum but no global extrema?,Are there function with one local extremum but no global extrema?,,"Is it possible to find function $f : \mathbb R^2 \to \mathbb R$ such that $f$ is $C^2$ and has one, and only one local extremum but no global extremum. I tried to make up an example with this function $f : (x,y) \mapsto e^{-(x^2+y^2)}+2x+2y$ whose graph looks like this : I tried to show that $f$ has only one global maximum by showing there exists only one point where $\nabla f(x,y) = 0$ : I obviously started by calculating, for $x, y \in \mathbb R$ , $\nabla f (x,y)$ : $$\nabla f (x,y) = \begin{bmatrix} -2xe^{-(x^2+y^2)}+2\\ -2ye^{-(x^2+y^2) }+2 \end{bmatrix}$$ I need to show that the $\nabla f(x,y) = 0$ has only one solution and that it is a local extremum. Any help would be apreciated!","Is it possible to find function such that is and has one, and only one local extremum but no global extremum. I tried to make up an example with this function whose graph looks like this : I tried to show that has only one global maximum by showing there exists only one point where : I obviously started by calculating, for , : I need to show that the has only one solution and that it is a local extremum. Any help would be apreciated!","f : \mathbb R^2 \to \mathbb R f C^2 f : (x,y) \mapsto e^{-(x^2+y^2)}+2x+2y f \nabla f(x,y) = 0 x, y \in \mathbb R \nabla f (x,y) \nabla f (x,y) = \begin{bmatrix} -2xe^{-(x^2+y^2)}+2\\ -2ye^{-(x^2+y^2) }+2 \end{bmatrix} \nabla f(x,y) = 0","['calculus', 'multivariable-calculus']"
69,"Determine if $\frac{x^2y}{x^2+y^2}$ is differentiable at $(x,y)=(0,0)$",Determine if  is differentiable at,"\frac{x^2y}{x^2+y^2} (x,y)=(0,0)","Let $k$ be a real number. Define $f\colon\Bbb{R}^2\to\Bbb{R}$ by $$f(x,y)=\begin{cases}\dfrac{x^ky}{x^2+y^2}&\text{if $(x,y)\neq(0,0)$},\\k-2&\text{if $(x,y)=(0,0)$}.\end{cases}$$ Find the value of $k$ such that $f$ is continuous at $(0,0)$ . For the value of $k$ found in part (1), determine whether $f$ is differentiable at $(0,0)$ . (Image that replaced text.) So, I have solved the part (1) and found that the $k=2$ . Then both partial derivatives $f_x(0,0)$ and $f_y(0,0)$ of the function $f(x,y)$ equal $0$ . I know that if the partial derivatives exist and continuous at $(0,0)$ , then the function is differentiable at $(0,0)$ . However, I also know that we can check the differentiability using this formula: Definition. Let $f\colon X\to\Bbb{R}$ where $X\subset\Bbb{R}^2$ is open, and let $\mathbf{a}\in X$ . Suppose $f_x(\mathbf{a}),f_y(\mathbf{a})$ exist. We say that $f$ is differentiable at $\mathbf{a}$ if $$\lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x})-[f(\mathbf{a})+f_x(\mathbf{a})(x_1-a_1)+f_y(\mathbf{a})(x_2-a_2)]}{\|\mathbf{x}-\mathbf{a}\|}=0.$$ (Image that replaced text.) And using, this formula I get that it is not continuous at $(0,0)$ (I have used $x=r\cos\alpha$ and $y = r\sin\alpha$ substitution to check it.) Can you please tell me which approach is right, and is the function differentiable at $(0,0)$ if $k=2$ ?","Let be a real number. Define by Find the value of such that is continuous at . For the value of found in part (1), determine whether is differentiable at . (Image that replaced text.) So, I have solved the part (1) and found that the . Then both partial derivatives and of the function equal . I know that if the partial derivatives exist and continuous at , then the function is differentiable at . However, I also know that we can check the differentiability using this formula: Definition. Let where is open, and let . Suppose exist. We say that is differentiable at if (Image that replaced text.) And using, this formula I get that it is not continuous at (I have used and substitution to check it.) Can you please tell me which approach is right, and is the function differentiable at if ?","k f\colon\Bbb{R}^2\to\Bbb{R} f(x,y)=\begin{cases}\dfrac{x^ky}{x^2+y^2}&\text{if (x,y)\neq(0,0)},\\k-2&\text{if (x,y)=(0,0)}.\end{cases} k f (0,0) k f (0,0) k=2 f_x(0,0) f_y(0,0) f(x,y) 0 (0,0) (0,0) f\colon X\to\Bbb{R} X\subset\Bbb{R}^2 \mathbf{a}\in X f_x(\mathbf{a}),f_y(\mathbf{a}) f \mathbf{a} \lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x})-[f(\mathbf{a})+f_x(\mathbf{a})(x_1-a_1)+f_y(\mathbf{a})(x_2-a_2)]}{\|\mathbf{x}-\mathbf{a}\|}=0. (0,0) x=r\cos\alpha y = r\sin\alpha (0,0) k=2","['multivariable-calculus', 'derivatives', 'partial-derivative']"
70,Multiplying two integrals becomes a double integral?,Multiplying two integrals becomes a double integral?,,I ran by this in a textbook: \begin{align*} I^2 & =\left(\int_{-\infty}^\infty e^{-x^2/2}dx\right)\left(\int_{-\infty}^\infty e^{-y^2/2}dy\right)\\ & =\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-x^2/2}e^{-y^2/2}dxdy=\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-(x^2+y^2)/2}dxdy \end{align*} How come this is valid? I assume it's not generally true because it doesn't seem like you can just go around mashing integrals together. Does it only work because they are independent variables? And therefore constants with respect to each other?,I ran by this in a textbook: How come this is valid? I assume it's not generally true because it doesn't seem like you can just go around mashing integrals together. Does it only work because they are independent variables? And therefore constants with respect to each other?,"\begin{align*}
I^2 & =\left(\int_{-\infty}^\infty e^{-x^2/2}dx\right)\left(\int_{-\infty}^\infty e^{-y^2/2}dy\right)\\
& =\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-x^2/2}e^{-y^2/2}dxdy=\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-(x^2+y^2)/2}dxdy
\end{align*}","['integration', 'multivariable-calculus', 'improper-integrals', 'multiple-integral']"
71,How to interpret a double integral?,How to interpret a double integral?,,"What can be the geometrical meaning of $$ \iint_R dA ~~~~~~~~~~ (1)$$ ? It is the particular case of $$\iint_R {f(x,y)} dA$$ where $f(x,y) = 1 $ . What I have found from my search is that (1) represents the area of the region R which is quite intuitive : if we have an area element which is a function of two variables then we must have to integrate it twice to get the area of the region. But if we see it from a different view point then it is the volume under the plane $z=1 $ and above the region R. Well, we can even extend this to triple integrals where $$ \iiint_S dV $$ represents the volume. The thing which is causing the doubt is the substitution of $ f(x,y)$ or $f(x,y,z)$ with $1$ . So, which interpretation is correct? Am I wrong somewhere in the very essence? Any help will be much appreciated.","What can be the geometrical meaning of ? It is the particular case of where . What I have found from my search is that (1) represents the area of the region R which is quite intuitive : if we have an area element which is a function of two variables then we must have to integrate it twice to get the area of the region. But if we see it from a different view point then it is the volume under the plane and above the region R. Well, we can even extend this to triple integrals where represents the volume. The thing which is causing the doubt is the substitution of or with . So, which interpretation is correct? Am I wrong somewhere in the very essence? Any help will be much appreciated."," \iint_R dA ~~~~~~~~~~ (1) \iint_R {f(x,y)} dA f(x,y) = 1  z=1   \iiint_S dV   f(x,y) f(x,y,z) 1","['multivariable-calculus', 'surface-integrals']"
72,Prove $\oint_\Gamma\vec\nabla f\cdot d\vec{r}=0$ when $\Gamma$ is the unit circle,Prove  when  is the unit circle,\oint_\Gamma\vec\nabla f\cdot d\vec{r}=0 \Gamma,"I have encountered a statement in my book which didn't seem quite right to me. It was written exactly like this: Let $f(x,y):\mathbb{R}^2\to\mathbb{R}$ be a differentiable function such that $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are continuous functions for all $(x,y)\neq(0,0)$ , and let $\Gamma$ be the unit circle, centered at $(0,0)$ . Then: $$\oint_\Gamma\vec\nabla f\cdot d\vec{r}=0$$ I tried to sit down and think why is this statement true. You all must have encountered before the infamous vector field: $$\vec{F}=\left(-\frac{y}{x^2+y^2},\frac{x}{x^2+y^2}\right)$$ This vector field is the gradient of the function $f(x,y)=\arctan(\frac yx)$ if I remember correctly. This function is of course problematic on the $y$ axis. However, I assume you can define $f$ on the $y$ axis, apart from the point $(0,0)$ , such that the partial derivatives of $f$ would exist and also be continuous there (I'm not sure about that, but the fact that the parital derivatives are clearly continuous for every point but $(0,0)$ has led me to that assumption). So $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are indeed continuous for all $(x,y)\neq(0,0)$ , but $\displaystyle \oint_\Gamma\vec\nabla f\cdot d\vec{r} $ would be equal to $\pm2 \pi$ (depends on the orientation of $\Gamma$ ). To conclude, I would like to know whether my example disproves the above statement, or maybe it is wrong. If it's not correct, I would be glad to hear why, and in addition, have a proof of the statement. I thought of many ways to prove it - using Green's Theorem and the fact that $\vec\nabla\times\vec\nabla f\equiv0$ , stating $\vec\nabla f$ is a conservative vector field (since it is derived from a gradient of a scalar potential function) and more; But of course I wouldn't go there before I know why my example is incorrect. Thanks!","I have encountered a statement in my book which didn't seem quite right to me. It was written exactly like this: Let be a differentiable function such that and are continuous functions for all , and let be the unit circle, centered at . Then: I tried to sit down and think why is this statement true. You all must have encountered before the infamous vector field: This vector field is the gradient of the function if I remember correctly. This function is of course problematic on the axis. However, I assume you can define on the axis, apart from the point , such that the partial derivatives of would exist and also be continuous there (I'm not sure about that, but the fact that the parital derivatives are clearly continuous for every point but has led me to that assumption). So and are indeed continuous for all , but would be equal to (depends on the orientation of ). To conclude, I would like to know whether my example disproves the above statement, or maybe it is wrong. If it's not correct, I would be glad to hear why, and in addition, have a proof of the statement. I thought of many ways to prove it - using Green's Theorem and the fact that , stating is a conservative vector field (since it is derived from a gradient of a scalar potential function) and more; But of course I wouldn't go there before I know why my example is incorrect. Thanks!","f(x,y):\mathbb{R}^2\to\mathbb{R} \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} (x,y)\neq(0,0) \Gamma (0,0) \oint_\Gamma\vec\nabla f\cdot d\vec{r}=0 \vec{F}=\left(-\frac{y}{x^2+y^2},\frac{x}{x^2+y^2}\right) f(x,y)=\arctan(\frac yx) y f y (0,0) f (0,0) \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} (x,y)\neq(0,0) \displaystyle \oint_\Gamma\vec\nabla f\cdot d\vec{r}  \pm2 \pi \Gamma \vec\nabla\times\vec\nabla f\equiv0 \vec\nabla f","['multivariable-calculus', 'vector-analysis']"
73,The derivative of a unit vector with respect to itself,The derivative of a unit vector with respect to itself,,"I just saw an engineering paper which claims that $$\frac{\partial \hat{\mathbf{x}}}{\partial \hat{\mathbf{x}}} \stackrel{?}{=} -S(\hat{\mathbf{x}})^2 = I - \hat{\mathbf{x}}\hat{\mathbf{x}}^T$$ where $\hat{\mathbf{x}}$ is a unit vector, $S(\hat{\mathbf{x}})$ is the skew symmetric matrix packing of $\hat{\mathbf{x}}$ for use in the cross-product, and I've used the $\stackrel{?}{=}$ symbol to represent the equality I'm calling into question. Is this right? I would have guessed that $$\frac{\partial \hat{\mathbf{x}}}{\partial \hat{\mathbf{x}}} = I$$ just like it is for the vector $\mathbf{x}$ , but I'm not sure if the fact that $\hat{\mathbf{x}}$ is a constrained vector somehow explains the appearance of the $-\hat{\mathbf{x}}\hat{\mathbf{x}}^T$ term. Any confirmation or correction is greatly appreciated.","I just saw an engineering paper which claims that where is a unit vector, is the skew symmetric matrix packing of for use in the cross-product, and I've used the symbol to represent the equality I'm calling into question. Is this right? I would have guessed that just like it is for the vector , but I'm not sure if the fact that is a constrained vector somehow explains the appearance of the term. Any confirmation or correction is greatly appreciated.",\frac{\partial \hat{\mathbf{x}}}{\partial \hat{\mathbf{x}}} \stackrel{?}{=} -S(\hat{\mathbf{x}})^2 = I - \hat{\mathbf{x}}\hat{\mathbf{x}}^T \hat{\mathbf{x}} S(\hat{\mathbf{x}}) \hat{\mathbf{x}} \stackrel{?}{=} \frac{\partial \hat{\mathbf{x}}}{\partial \hat{\mathbf{x}}} = I \mathbf{x} \hat{\mathbf{x}} -\hat{\mathbf{x}}\hat{\mathbf{x}}^T,"['multivariable-calculus', 'derivatives', 'matrix-calculus', 'projection-matrices']"
74,Intuition on directional derivative,Intuition on directional derivative,,"The directional derivative along the direction defined by the vector $\vec v = a \hat i + b \hat j$ $$\nabla_{\vec v} f = a\dfrac {\partial f}{\partial x} + b \dfrac{\partial f }{  \partial y} $$ I get the fact that a small nudge in the direction of $\vec v$ , $\,\,h\vec v$ is composed of two small nudges $      \partial x$ and $\partial y$ . But I don't get why the derivative should be the sum of ( $a$ times partial of $f$ wrt $x$ )+ ( $b$ times  partial of $f$ wrt to $y$ ) I got an absurd thought, shouldn't the ""net"" nudge be $\sqrt{(\partial x^2 +\partial y^2)}$ rather than $\partial x + \partial y$ . I know I'm totally wrong. Where am I going wrong, I did this because, when we consider the change in $f$ due to $x$ we consider moving a ""length"" $dx$ in $x$ direction, don't we?","The directional derivative along the direction defined by the vector I get the fact that a small nudge in the direction of , is composed of two small nudges and . But I don't get why the derivative should be the sum of ( times partial of wrt )+ ( times  partial of wrt to ) I got an absurd thought, shouldn't the ""net"" nudge be rather than . I know I'm totally wrong. Where am I going wrong, I did this because, when we consider the change in due to we consider moving a ""length"" in direction, don't we?","\vec v = a \hat i + b \hat j \nabla_{\vec v} f = a\dfrac {\partial f}{\partial x} + b \dfrac{\partial f }{  \partial y}  \vec v \,\,h\vec v       \partial x \partial y a f x b f y \sqrt{(\partial x^2 +\partial y^2)} \partial x + \partial y f x dx x","['calculus', 'multivariable-calculus', 'partial-derivative']"
75,Partial Derivative Disambiguation,Partial Derivative Disambiguation,,"There are at least two substantially different meanings to $\frac{\partial}{\partial x}f(x,\ y,\ z(x))$ .  The $\partial x$ could mean ""with respect to $x$ the independent variable,"" or it could mean ""with respect to the $x$ the first parameter of $f$ .""  I think this can be understood in light of a net income calculation. Suppose $x$ is an individual's taxable gross income, $y$ is her non-taxable gross income (gifts received, etc.), $z$ is her income tax, and $f$ is her net income, all over the same year.  Since net income depends on taxable gross income, non-taxable gross income, and income tax, as given by $f = x + y - z$ , and income tax depends on taxable gross income, as given by $z = .15x$ (using a single 15% tax bracket for simplicity), we can write the overall equation as $f(x,\ y,\ z(x)) = x + y - z(x)$ where $z(x) = .15x$ , and then consider the meaning of $\frac{\partial}{\partial x}f(x,\ y,\ z(x))$ . If we interpret $\partial x$ to mean ""with respect to $x$ the independent variable,"" then $\frac{\partial}{\partial x}f(x,\ y,\ z(x))$ represents the change in net income relative to a reported change in taxable gross income, whereas if we interpret $\partial x$ to mean ""with respect to the $x$ the first parameter of $f$ ,"" then $\frac{\partial}{\partial x}f(x,\ y,\ z(x))$ represents the change in net income relative to an unreported change in taxable gross income. I have given, just as I have learned, a binary explanation of this difference.  The $\partial x$ refers to either an independent variable or a parameter of $f$ .  My question is whether it is also acceptable for it to refer to something in between.  Let's assign a new color to the contents of each nested layer of a function's parentheses, so that the above example becomes $f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}})$ .  This disambiguates things by allowing us to refer to the change in net income relative to a reported change in taxable gross income with $\frac{\partial}{\partial \color{tan}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}})$ and the change in net income relative to an unreported change in taxable gross income with $\frac{\partial}{\partial \color{blue}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}})$ .  I think colors are less misleading than subscripts in this case, because $\color{tan}{\textrm{x}}$ and $\color{blue}{\textrm{x}}$ are the same algebraic entity; it's just that when the calculus eats an algebraic expression and spits out a new one, it sometimes chews up the two $x$ 's a bit differently. With this setup, the question can be asked quite succinctly; can $\frac{\partial}{\partial x}f(x,\ y,\ z(x),\ a(x,\ z(x)), b(x,\ z(x))$ also mean $\frac{\partial}{\partial \color{orange}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{), a(}}\color{orange}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{orange}{\textrm{)}}\color{blue}{\textrm{), b(}}\color{orange}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{orange}{\textrm{)}}\color{blue}{\textrm{)}}$ and/or $\frac{\partial}{\partial \color{lime}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{), a(}}\color{lime}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{lime}{\textrm{)}}\color{blue}{\textrm{), b(}}\color{red}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{red}{\textrm{)}}\color{blue}{\textrm{)}}$ , and/or have some other meaning drawn via a similar color hierarchy, or is a partial derivative unable to be taken with respect to orange or green (or red) $x$ , since they are neither independent variables nor parameters of $f$ ?","There are at least two substantially different meanings to .  The could mean ""with respect to the independent variable,"" or it could mean ""with respect to the the first parameter of .""  I think this can be understood in light of a net income calculation. Suppose is an individual's taxable gross income, is her non-taxable gross income (gifts received, etc.), is her income tax, and is her net income, all over the same year.  Since net income depends on taxable gross income, non-taxable gross income, and income tax, as given by , and income tax depends on taxable gross income, as given by (using a single 15% tax bracket for simplicity), we can write the overall equation as where , and then consider the meaning of . If we interpret to mean ""with respect to the independent variable,"" then represents the change in net income relative to a reported change in taxable gross income, whereas if we interpret to mean ""with respect to the the first parameter of ,"" then represents the change in net income relative to an unreported change in taxable gross income. I have given, just as I have learned, a binary explanation of this difference.  The refers to either an independent variable or a parameter of .  My question is whether it is also acceptable for it to refer to something in between.  Let's assign a new color to the contents of each nested layer of a function's parentheses, so that the above example becomes .  This disambiguates things by allowing us to refer to the change in net income relative to a reported change in taxable gross income with and the change in net income relative to an unreported change in taxable gross income with .  I think colors are less misleading than subscripts in this case, because and are the same algebraic entity; it's just that when the calculus eats an algebraic expression and spits out a new one, it sometimes chews up the two 's a bit differently. With this setup, the question can be asked quite succinctly; can also mean and/or , and/or have some other meaning drawn via a similar color hierarchy, or is a partial derivative unable to be taken with respect to orange or green (or red) , since they are neither independent variables nor parameters of ?","\frac{\partial}{\partial x}f(x,\ y,\ z(x)) \partial x x x f x y z f f = x + y - z z = .15x f(x,\ y,\ z(x)) = x + y - z(x) z(x) = .15x \frac{\partial}{\partial x}f(x,\ y,\ z(x)) \partial x x \frac{\partial}{\partial x}f(x,\ y,\ z(x)) \partial x x f \frac{\partial}{\partial x}f(x,\ y,\ z(x)) \partial x f f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}}) \frac{\partial}{\partial \color{tan}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}}) \frac{\partial}{\partial \color{blue}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}}) \color{tan}{\textrm{x}} \color{blue}{\textrm{x}} x \frac{\partial}{\partial x}f(x,\ y,\ z(x),\ a(x,\ z(x)), b(x,\ z(x)) \frac{\partial}{\partial \color{orange}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{), a(}}\color{orange}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{orange}{\textrm{)}}\color{blue}{\textrm{), b(}}\color{orange}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{orange}{\textrm{)}}\color{blue}{\textrm{)}} \frac{\partial}{\partial \color{lime}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{), a(}}\color{lime}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{lime}{\textrm{)}}\color{blue}{\textrm{), b(}}\color{red}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{red}{\textrm{)}}\color{blue}{\textrm{)}} x f","['calculus', 'multivariable-calculus', 'functions', 'definition', 'partial-derivative']"
76,Surface integral through a cube.,Surface integral through a cube.,,"I am doing some fluid dynamics work and the question asks: Consider the mass flow vector: $\rho \vec{u} = (4x^2y,xyz,yz^2)$ Compute the net mass outflow through the cube formed by the planes x=0, x=1, y=0, y=1, z=0, z=1 So I figure that in order to find the net mass outflow I compute the surface integral of the mass flow normal to each plane and add them all up. That is: $\iint_S (\rho \vec{u} \cdot \hat{n}) dA$ To make the work easier I use the divergence theorem, to replace the surface integral with a volume integral and a much shorter integration: Volume Integral: $ \rightarrow \iint_S (\rho \vec{u} \cdot \hat{n}) dA = \iiint_V (\nabla \cdot \rho \vec{u})dV$ $ \rightarrow \iiint_V(\nabla \cdot (4x^2y,xyz,yz^2))dV$ $ \rightarrow \int \limits_0^1 \int \limits_0^1 \int \limits_0^1(8xy+xz+2yz)dxdy dz$ $ \rightarrow\int \limits_0^1\int \limits_0^1(4y +\frac{z}{2}+2yz) dy dz$ $ \rightarrow\int \limits_0^1(2 +\frac{z}{2}+z) dz$ $ \rightarrow \iiint_V (\nabla \cdot \rho \vec{u})dV = \frac{11}{4}$ I am pretty sure this part is done correctly, however just to check I do the question the long way as well; obtaining 6 surface integrals: Surface Integral: $\hat{n} = (1,0,0) \; \; \; \; \; \int \limits_0^1 \int \limits_0^1 (4x^2y) dy dz = \int \limits_0^1 2x^2dz = 2x^2 $ $\hat{n} = (-1,0,0) \; \;  \int \limits_0^1 \int \limits_0^1 (-4x^2y) dy dz = \int \limits_0^1 -2x^2dz = -2x^2 $ $\hat{n} = (0,1,0) \; \; \; \; \; \int \limits_0^1 \int \limits_0^1 (xyz) dx dz = \int \limits_0^1 \frac{yz}{2}dz = \frac{y}{4} $ $\hat{n} = (0,-1,0) \; \; \int \limits_0^1 \int \limits_0^1 (-xyz) dx dz = \int \limits_0^1 \frac{-yz}{2}dz = \frac{-y}{4} $ $\hat{n} = (0,0,1) \; \; \; \; \; \int \limits_0^1 \int \limits_0^1 (yz^2) dy dx =  \frac{z^2}{2} $ $\hat{n} = (0,0,-1) \; \; \int \limits_0^1 \int \limits_0^1 (-yz^2) dy dx =  \frac{-z^2}{2} $ Summing all of these results in zero which contradicts the answer I obtained earlier of $\frac{11}{4}$ . What am i doing wrong can someone please explain, I am sure it is some trivial mistake but I have checked it over and am not sure. Thanks to all for your time!","I am doing some fluid dynamics work and the question asks: Consider the mass flow vector: Compute the net mass outflow through the cube formed by the planes x=0, x=1, y=0, y=1, z=0, z=1 So I figure that in order to find the net mass outflow I compute the surface integral of the mass flow normal to each plane and add them all up. That is: To make the work easier I use the divergence theorem, to replace the surface integral with a volume integral and a much shorter integration: Volume Integral: I am pretty sure this part is done correctly, however just to check I do the question the long way as well; obtaining 6 surface integrals: Surface Integral: Summing all of these results in zero which contradicts the answer I obtained earlier of . What am i doing wrong can someone please explain, I am sure it is some trivial mistake but I have checked it over and am not sure. Thanks to all for your time!","\rho \vec{u} = (4x^2y,xyz,yz^2) \iint_S (\rho \vec{u} \cdot \hat{n}) dA  \rightarrow \iint_S (\rho \vec{u} \cdot \hat{n}) dA =
\iiint_V (\nabla \cdot \rho \vec{u})dV  \rightarrow \iiint_V(\nabla \cdot (4x^2y,xyz,yz^2))dV  \rightarrow \int \limits_0^1 \int \limits_0^1 \int \limits_0^1(8xy+xz+2yz)dxdy dz  \rightarrow\int \limits_0^1\int \limits_0^1(4y +\frac{z}{2}+2yz) dy dz  \rightarrow\int \limits_0^1(2 +\frac{z}{2}+z) dz  \rightarrow \iiint_V (\nabla \cdot \rho \vec{u})dV = \frac{11}{4} \hat{n} = (1,0,0) \; \; \; \; \; \int \limits_0^1 \int \limits_0^1 (4x^2y) dy dz = \int \limits_0^1 2x^2dz = 2x^2  \hat{n} = (-1,0,0) \; \;  \int \limits_0^1 \int \limits_0^1 (-4x^2y) dy dz = \int \limits_0^1 -2x^2dz = -2x^2  \hat{n} = (0,1,0) \; \; \; \; \; \int \limits_0^1 \int \limits_0^1 (xyz) dx dz = \int \limits_0^1 \frac{yz}{2}dz = \frac{y}{4}  \hat{n} = (0,-1,0) \; \; \int \limits_0^1 \int \limits_0^1 (-xyz) dx dz = \int \limits_0^1 \frac{-yz}{2}dz = \frac{-y}{4}  \hat{n} = (0,0,1) \; \; \; \; \; \int \limits_0^1 \int \limits_0^1 (yz^2) dy dx =  \frac{z^2}{2}  \hat{n} = (0,0,-1) \; \; \int \limits_0^1 \int \limits_0^1 (-yz^2) dy dx =  \frac{-z^2}{2}  \frac{11}{4}","['multivariable-calculus', 'vector-analysis', 'fluid-dynamics']"
77,Computing the derivative of matrix inverse using the chain rule,Computing the derivative of matrix inverse using the chain rule,,"Let $\text{Inv}: GL_n(\mathbb{R})\to GL_n(\mathbb{R})$ be defined by $\text{Inv}(X)= X^{-1}.$ Problem $33$ in Chapter $5$ of Pugh's Real Mathematical Analysis states: ""Observe that $Y = \text{Inv}(X)$ solves the implicit function problem $F(X, Y)-I =0$ where $F(X, Y)= XY.$ Assume it is known that $\text{Inv}$ is smooth and use the chain rule to derive from this equation a formula for the derivative of $\text{Inv}.$"" My attempt: Differentiating both sides of the given equation wrt $X$ at an arbitrary $A \in M_{n}(\mathbb{R})$and using the chain rule for the $LHS$ we get $\left(\dfrac{\partial F}{\partial X}\right)_{A}+ \left(\dfrac{\partial F}{\partial Y}\right)_AY'(A) =0.$ My problem is in evaluating the partials of $F$ wrt $X$ and $Y.$ For $n=2,$ I wrote $X= \begin{pmatrix} x_{11} &x_{12}\\x_{21}&x_{22}\end{pmatrix}$ and $Y= \begin{pmatrix} y_{11} &y_{12}\\y_{21}&y_{22}\end{pmatrix}$ so that $F(X, Y) = XY =\begin{pmatrix} x_{11}y_{11}+x_{12}y_{21} &x_{11}y_{12}+x_{12}y_{22}\\x_{21}y_{11}+x_{22}y_{21}&x_{21}{y_{12}+x_{22}y_{22}}\end{pmatrix}$ then $\dfrac{\partial F}{\partial X} = \begin{pmatrix} Y^{T} &0_{2\times2}\\0_{2\times2}&Y^{T}\end{pmatrix} $ and $\dfrac{\partial F}{\partial Y} = X \otimes I_{2\times 2}$ where $\otimes$ denotes the Kronecker product . For $\dfrac{\partial F}{\partial X}$ to act on $A$, we must write $A$ as a $4 \times 1$ column vector $\begin{pmatrix} a_{11}&a_{12}&a_{21}&a_{22}\end{pmatrix}^{T}$ so that rewriting $\left(\dfrac{\partial F}{\partial X}\right)_{A} = \dfrac{\partial F}{\partial X}(A)$ as a $2\times 2$ matrix we get $AY$ and rewriting $\left(\dfrac{\partial F}{\partial Y}\right)_{A} = \dfrac{\partial F}{\partial Y}(A)$ we get $XA$. Plugging back in the above equation and using the fact that $Y= X^{-1}$ we get $AY'(A) = -X^{-1}AX^{-1}.$ My question: what have I done wrong that caused me to get stuck with an extra $A$ on the LHS? (From first principles I have proven that correct answer is $Y'(A) = -X^{-1}AX^{-1})$ but I am unable to get that formula from this approach. I am unable to pinpoint what I have made a mistake in. Is it the inconsistent treatment of $A$? I'd appreciate any pointers or hints. Thanks in advance.","Let $\text{Inv}: GL_n(\mathbb{R})\to GL_n(\mathbb{R})$ be defined by $\text{Inv}(X)= X^{-1}.$ Problem $33$ in Chapter $5$ of Pugh's Real Mathematical Analysis states: ""Observe that $Y = \text{Inv}(X)$ solves the implicit function problem $F(X, Y)-I =0$ where $F(X, Y)= XY.$ Assume it is known that $\text{Inv}$ is smooth and use the chain rule to derive from this equation a formula for the derivative of $\text{Inv}.$"" My attempt: Differentiating both sides of the given equation wrt $X$ at an arbitrary $A \in M_{n}(\mathbb{R})$and using the chain rule for the $LHS$ we get $\left(\dfrac{\partial F}{\partial X}\right)_{A}+ \left(\dfrac{\partial F}{\partial Y}\right)_AY'(A) =0.$ My problem is in evaluating the partials of $F$ wrt $X$ and $Y.$ For $n=2,$ I wrote $X= \begin{pmatrix} x_{11} &x_{12}\\x_{21}&x_{22}\end{pmatrix}$ and $Y= \begin{pmatrix} y_{11} &y_{12}\\y_{21}&y_{22}\end{pmatrix}$ so that $F(X, Y) = XY =\begin{pmatrix} x_{11}y_{11}+x_{12}y_{21} &x_{11}y_{12}+x_{12}y_{22}\\x_{21}y_{11}+x_{22}y_{21}&x_{21}{y_{12}+x_{22}y_{22}}\end{pmatrix}$ then $\dfrac{\partial F}{\partial X} = \begin{pmatrix} Y^{T} &0_{2\times2}\\0_{2\times2}&Y^{T}\end{pmatrix} $ and $\dfrac{\partial F}{\partial Y} = X \otimes I_{2\times 2}$ where $\otimes$ denotes the Kronecker product . For $\dfrac{\partial F}{\partial X}$ to act on $A$, we must write $A$ as a $4 \times 1$ column vector $\begin{pmatrix} a_{11}&a_{12}&a_{21}&a_{22}\end{pmatrix}^{T}$ so that rewriting $\left(\dfrac{\partial F}{\partial X}\right)_{A} = \dfrac{\partial F}{\partial X}(A)$ as a $2\times 2$ matrix we get $AY$ and rewriting $\left(\dfrac{\partial F}{\partial Y}\right)_{A} = \dfrac{\partial F}{\partial Y}(A)$ we get $XA$. Plugging back in the above equation and using the fact that $Y= X^{-1}$ we get $AY'(A) = -X^{-1}AX^{-1}.$ My question: what have I done wrong that caused me to get stuck with an extra $A$ on the LHS? (From first principles I have proven that correct answer is $Y'(A) = -X^{-1}AX^{-1})$ but I am unable to get that formula from this approach. I am unable to pinpoint what I have made a mistake in. Is it the inconsistent treatment of $A$? I'd appreciate any pointers or hints. Thanks in advance.",,"['multivariable-calculus', 'proof-verification', 'partial-derivative', 'matrix-calculus']"
78,gradient descent method with linear exact search we have $\nabla^t f(x^k)\nabla f(x^{k+1})=0$,gradient descent method with linear exact search we have,\nabla^t f(x^k)\nabla f(x^{k+1})=0,"Prove that in the gradient descent method with linear exact search we   have $\nabla^t f(x^k)\nabla f(x^{k+1})=0$ We know that $x^{k+1}$ is obtained by $x^{k+1} = x^k + \alpha_k \nabla f(x^k)$, for some $0<\alpha_k$ that minimizes $f(x^k + \alpha_k \nabla f(x^k))$ I don't see why they should be orthogonal at all. Did I understand something wrong?","Prove that in the gradient descent method with linear exact search we   have $\nabla^t f(x^k)\nabla f(x^{k+1})=0$ We know that $x^{k+1}$ is obtained by $x^{k+1} = x^k + \alpha_k \nabla f(x^k)$, for some $0<\alpha_k$ that minimizes $f(x^k + \alpha_k \nabla f(x^k))$ I don't see why they should be orthogonal at all. Did I understand something wrong?",,"['multivariable-calculus', 'derivatives', 'optimization', 'nonlinear-optimization']"
79,Continuity at a point for function of two variables,Continuity at a point for function of two variables,,"If a function of two variables is discontinuous at a particular point, say $(x,y)$, does this mean that the graph of that function has some hole around the point $(x,y,f(x,y))$? Is there any break in the graph at this point in certain direction? This question arises because I have one function which is discontinuous at $(0,0)$ but all of its partial derivatives and directional derivatives exist at $(0,0)$. While calculating its partial or directional derivatives, we naturally look in a certain plane with that point and specified direction and calculate the slope of the tangent line (as you would with one variable). In my example I have all directional derivatives, which seems to imply that there is no break around $(0,0,f(0,0))$ in any direction. Then why is the function discontinuous at $(0,0)$?","If a function of two variables is discontinuous at a particular point, say $(x,y)$, does this mean that the graph of that function has some hole around the point $(x,y,f(x,y))$? Is there any break in the graph at this point in certain direction? This question arises because I have one function which is discontinuous at $(0,0)$ but all of its partial derivatives and directional derivatives exist at $(0,0)$. While calculating its partial or directional derivatives, we naturally look in a certain plane with that point and specified direction and calculate the slope of the tangent line (as you would with one variable). In my example I have all directional derivatives, which seems to imply that there is no break around $(0,0,f(0,0))$ in any direction. Then why is the function discontinuous at $(0,0)$?",,['multivariable-calculus']
80,Computation of multiple improper integral.,Computation of multiple improper integral.,,"In my recent work, I need to the details of the computation of the following multiple improper integral: $$\iint_{[0,1]^2}e^{-\pi x^2y^2}dxdy-\iint_{[1,\infty)^2}e^{-\pi x^2y^2}dxdy.$$ As you see, the first one is a proper integral and the second one is improper integral. It is easy to know the convergence of the second one. At the beginning, I just use Wolfram mathematica 9.0 to get the finally result: $$\frac{1}{4}(\gamma+\log(4\pi)),$$  where $\gamma$ is a Euler-Gamma constant. But now I need to give the details of the computation. The wolfram mathematica tells us that: $$\iint_{[0,1]^2}e^{-\pi x^2y^2}dxdy=HypergeometricPFQ[(1/2,1/2);(3/2,3/2);-\pi];$$ $$\iint_{[1,\infty)^2}e^{-\pi x^2y^2}dxdy =HypergeometricPFQ[(1/2,1/2);(3/2,3/2)-\frac{1}{4}(\gamma+\log(4\pi)).$$ I do not know how to get the hyperbolic geometric function and  how to cancel it.  Any help and hints will welcome. Thanks a lot.","In my recent work, I need to the details of the computation of the following multiple improper integral: $$\iint_{[0,1]^2}e^{-\pi x^2y^2}dxdy-\iint_{[1,\infty)^2}e^{-\pi x^2y^2}dxdy.$$ As you see, the first one is a proper integral and the second one is improper integral. It is easy to know the convergence of the second one. At the beginning, I just use Wolfram mathematica 9.0 to get the finally result: $$\frac{1}{4}(\gamma+\log(4\pi)),$$  where $\gamma$ is a Euler-Gamma constant. But now I need to give the details of the computation. The wolfram mathematica tells us that: $$\iint_{[0,1]^2}e^{-\pi x^2y^2}dxdy=HypergeometricPFQ[(1/2,1/2);(3/2,3/2);-\pi];$$ $$\iint_{[1,\infty)^2}e^{-\pi x^2y^2}dxdy =HypergeometricPFQ[(1/2,1/2);(3/2,3/2)-\frac{1}{4}(\gamma+\log(4\pi)).$$ I do not know how to get the hyperbolic geometric function and  how to cancel it.  Any help and hints will welcome. Thanks a lot.",,"['multivariable-calculus', 'improper-integrals', 'hypergeometric-function']"
81,Different results after changing the order of integration with constant limits (Failure of Fubini's theorem),Different results after changing the order of integration with constant limits (Failure of Fubini's theorem),,"I have the following question $I_{1}=\int _{0}^{1}\int _{0}^{1}\ \frac{(x-y)}{(x+y)^{3}}\ dy\,dx$ Evaulating the above I get $I_{1}=0.5$ Now if I switch the order of integration $I_{2}=\int _{0}^{1}\int _{0}^{1}\ \frac{(x-y)}{(x+y)^{3}}\ dx\,dy$ I get $I_{2}=-0.5$ Why is the value negative after changing the order? Shouldn't the result be same for constant limits (as the region of space is the same for both the integrals) ?","I have the following question $I_{1}=\int _{0}^{1}\int _{0}^{1}\ \frac{(x-y)}{(x+y)^{3}}\ dy\,dx$ Evaulating the above I get $I_{1}=0.5$ Now if I switch the order of integration $I_{2}=\int _{0}^{1}\int _{0}^{1}\ \frac{(x-y)}{(x+y)^{3}}\ dx\,dy$ I get $I_{2}=-0.5$ Why is the value negative after changing the order? Shouldn't the result be same for constant limits (as the region of space is the same for both the integrals) ?",,"['integration', 'multivariable-calculus', 'order-of-integration']"
82,"Is the function $f(x,y) = \frac{xy^3}{x^4+y^2}$ a $C^1$-function?",Is the function  a -function?,"f(x,y) = \frac{xy^3}{x^4+y^2} C^1","Consider the function  $$ f(x,y) = \frac{xy^3}{x^4+y^2}. $$ with $f(0,0) = 0$.  Is this a $C^1$-function? Firstly we compute:  \begin{align*} & D_1f(x,y) = \frac{y^3(x^4+y^2)-xy^3(4x^3)}{(x^4+y^2)^2} = \frac{y^3(y^2-3x^4)}{(x^4+y^2)^2}, \\ & D_2f(x,y) = \frac{3xy^2(x^4+y^2)-xy^3(2y)}{(x^4+y^2)^2} = \frac{xy^2(3x^4+y^2)}{(x^4+y^2)^2},  \end{align*} and $D_1f(0,0)= D_2f(0,0) = 0$. The function is differentiable at $(0,0)$ with $f'(\vec{0}) = [0,0]$, since we can verify that:  \begin{align*} \frac{|f(x,y)|}{\|\vec{x}\|} & = \frac{|x||y|^3}{x^4+y^2} \cdot \frac{1}{\sqrt{x^2+y^2}} = \frac{|x||y|}{\frac{x^4}{y^2} + 1} \cdot \frac{1}{\sqrt{x^2+y^2}} = \frac{|x||y|}{\left( \frac{x^2}{y} \right)^2 + 1} \cdot \frac{1}{\|\vec{x}\|} \\ & \leq |x||y| \cdot \frac{1}{\|\vec{x}\|} \leq \|\vec{x}\|^2 \cdot \frac{1}{\|\vec{x}\|} = \|\vec{x}\| \to 0 \textrm{ als } \vec{x} \to \vec{0}.  \end{align*} In order to determine if it is $C^1$, we need to determine whether the partial derivatives are continuous on the entire domain. I am trying to determine this for the point $(0,0)$, and so I need to determine $$ \lim_{(x,y) \to (0,0)} \frac{y^3(y^2-3x^4)}{(x^4+y^2)^2},  $$ and  $$ \lim_{(x,y) \to (0,0)} \frac{xy^2(3x^4+y^2)}{(x^4+y^2)^2}.  $$ I'm stuck in this step, I don't know how to compute these limits.","Consider the function  $$ f(x,y) = \frac{xy^3}{x^4+y^2}. $$ with $f(0,0) = 0$.  Is this a $C^1$-function? Firstly we compute:  \begin{align*} & D_1f(x,y) = \frac{y^3(x^4+y^2)-xy^3(4x^3)}{(x^4+y^2)^2} = \frac{y^3(y^2-3x^4)}{(x^4+y^2)^2}, \\ & D_2f(x,y) = \frac{3xy^2(x^4+y^2)-xy^3(2y)}{(x^4+y^2)^2} = \frac{xy^2(3x^4+y^2)}{(x^4+y^2)^2},  \end{align*} and $D_1f(0,0)= D_2f(0,0) = 0$. The function is differentiable at $(0,0)$ with $f'(\vec{0}) = [0,0]$, since we can verify that:  \begin{align*} \frac{|f(x,y)|}{\|\vec{x}\|} & = \frac{|x||y|^3}{x^4+y^2} \cdot \frac{1}{\sqrt{x^2+y^2}} = \frac{|x||y|}{\frac{x^4}{y^2} + 1} \cdot \frac{1}{\sqrt{x^2+y^2}} = \frac{|x||y|}{\left( \frac{x^2}{y} \right)^2 + 1} \cdot \frac{1}{\|\vec{x}\|} \\ & \leq |x||y| \cdot \frac{1}{\|\vec{x}\|} \leq \|\vec{x}\|^2 \cdot \frac{1}{\|\vec{x}\|} = \|\vec{x}\| \to 0 \textrm{ als } \vec{x} \to \vec{0}.  \end{align*} In order to determine if it is $C^1$, we need to determine whether the partial derivatives are continuous on the entire domain. I am trying to determine this for the point $(0,0)$, and so I need to determine $$ \lim_{(x,y) \to (0,0)} \frac{y^3(y^2-3x^4)}{(x^4+y^2)^2},  $$ and  $$ \lim_{(x,y) \to (0,0)} \frac{xy^2(3x^4+y^2)}{(x^4+y^2)^2}.  $$ I'm stuck in this step, I don't know how to compute these limits.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'partial-derivative']"
83,Symmetry of second derivative - Sufficiency of twice-differentiability,Symmetry of second derivative - Sufficiency of twice-differentiability,,"Symmetry of second derivative states that for $u=u(x,y)$ if $u_x,u_y$ exists and $u_{xy},u_{yx}$ exists and continuous then $u_{xy}=u_{yx}$. I proved that statement using the mean value theorem. While I was looking in Wikipedia there is a section called ""Sufficiency of twice-differentiability"", if $u(x,y):E^{\text{open set}}\subset \Bbb R^2\to \Bbb R$ and $u_x,u_y,u_{yx}$ exists everywhere and $u_{yx}$ is continuous at a point in $E$ then $u_{xy}$ exists at that point and equal to $u_{yx}$. My question is, while I proved Symmetry of second derivative I had to assume the continuity of $u_{xy}$ and $u_{yx}$, so how can I prove that this is true without even assuming the existence of one of the second derivative? I'm sitting on this for a long time and I couldn't think on any starting point and would love help with this.","Symmetry of second derivative states that for $u=u(x,y)$ if $u_x,u_y$ exists and $u_{xy},u_{yx}$ exists and continuous then $u_{xy}=u_{yx}$. I proved that statement using the mean value theorem. While I was looking in Wikipedia there is a section called ""Sufficiency of twice-differentiability"", if $u(x,y):E^{\text{open set}}\subset \Bbb R^2\to \Bbb R$ and $u_x,u_y,u_{yx}$ exists everywhere and $u_{yx}$ is continuous at a point in $E$ then $u_{xy}$ exists at that point and equal to $u_{yx}$. My question is, while I proved Symmetry of second derivative I had to assume the continuity of $u_{xy}$ and $u_{yx}$, so how can I prove that this is true without even assuming the existence of one of the second derivative? I'm sitting on this for a long time and I couldn't think on any starting point and would love help with this.",,"['multivariable-calculus', 'partial-derivative']"
84,Using Lagrange Multipliers to find the minimum distance of a point to a plane,Using Lagrange Multipliers to find the minimum distance of a point to a plane,,"My exercise is as follows: Using Lagrange multipliers ﬁnd the distance from the point $(1,2,−1)$   to the plane given by the equation $x−y + z = 3. $ My thought process: Langrange Multipliers let you find the maximum and/or minimum of a function given a function as a constraint on your input. For example, if I'm told to find the maximum value of some plane given the constraint $x^2+y^2 = 1$, the only $x$ values I can take are ones on the unit circle. It does this by assuming that any intersecting contour lines of the function and constraining function must have the condition $\nabla f = \lambda \nabla g$. Intersecting contour lines are necessary for the constraining function and function to be equal to eachother. In this case, we are trying to find the distance from the point $(1,2,-1)$ to the plane of equation $x-y+z=3$. Now, given my intuition, $x-y+z=3$ ought to be $g(x,y,z)$ since in these types of problems some function equal to a constant will be this $g(x,y,z)$ (although I'm not exactly sure why). This leads me to ponder what form $f(x,y,z)$ will have such that $F(x,y,z) = f(x,y,z) - \lambda g(x,y,z)$. The minimum distance from a point to a plane should be a straight line, and that line should be perpendicular to the plane. That means it should be the normal vector, or gradient, of that plane. However, I don't know how that helps me. I need a function $f(x,y,z)$ to use. Here's my lecturer's answer: We want to minimize $$d = \sqrt {(x-1)^2+(y-2)^2 + (z+1)^2}$$ Equivalent to minimizing $(x-1)^2+(y-2)^2 + (z+1)^2$ with the same constraint. Thus, it is rendered to $$F(x,y,z) = (x-1)^2+(y-2)^2 + (z+1)^2 - \lambda (x-y+z-3)$$ Upon solving $\lambda$ apparently didn't need to be considered when minimizing, as it dropped out in row operations and back-substitutions from the simultaneous equations you get from considering the partial derivatives. This leaves me with the following questions: Why are we working with the distance from the origin to the point rather than the point to the plane? Is what is meant by ""minimize"" is find the smallest output value for the plane given the constraint its inputs must also satisfy $(x-1)^2+(y-2)^2 + (z+1)^2$? How did he reach the conclusion $\sqrt{(x-1)^2+(y-2)^2 + (z+1)^2}$ is equivalent to minimizing $(x-1)^2+(y-2)^2 + (z+1)^2$ subject to the same constraint? Why is my thinking in normal vectors a bad idea? The values that solve the linear equations are the points needed for minimum distance. Why is that?","My exercise is as follows: Using Lagrange multipliers ﬁnd the distance from the point $(1,2,−1)$   to the plane given by the equation $x−y + z = 3. $ My thought process: Langrange Multipliers let you find the maximum and/or minimum of a function given a function as a constraint on your input. For example, if I'm told to find the maximum value of some plane given the constraint $x^2+y^2 = 1$, the only $x$ values I can take are ones on the unit circle. It does this by assuming that any intersecting contour lines of the function and constraining function must have the condition $\nabla f = \lambda \nabla g$. Intersecting contour lines are necessary for the constraining function and function to be equal to eachother. In this case, we are trying to find the distance from the point $(1,2,-1)$ to the plane of equation $x-y+z=3$. Now, given my intuition, $x-y+z=3$ ought to be $g(x,y,z)$ since in these types of problems some function equal to a constant will be this $g(x,y,z)$ (although I'm not exactly sure why). This leads me to ponder what form $f(x,y,z)$ will have such that $F(x,y,z) = f(x,y,z) - \lambda g(x,y,z)$. The minimum distance from a point to a plane should be a straight line, and that line should be perpendicular to the plane. That means it should be the normal vector, or gradient, of that plane. However, I don't know how that helps me. I need a function $f(x,y,z)$ to use. Here's my lecturer's answer: We want to minimize $$d = \sqrt {(x-1)^2+(y-2)^2 + (z+1)^2}$$ Equivalent to minimizing $(x-1)^2+(y-2)^2 + (z+1)^2$ with the same constraint. Thus, it is rendered to $$F(x,y,z) = (x-1)^2+(y-2)^2 + (z+1)^2 - \lambda (x-y+z-3)$$ Upon solving $\lambda$ apparently didn't need to be considered when minimizing, as it dropped out in row operations and back-substitutions from the simultaneous equations you get from considering the partial derivatives. This leaves me with the following questions: Why are we working with the distance from the origin to the point rather than the point to the plane? Is what is meant by ""minimize"" is find the smallest output value for the plane given the constraint its inputs must also satisfy $(x-1)^2+(y-2)^2 + (z+1)^2$? How did he reach the conclusion $\sqrt{(x-1)^2+(y-2)^2 + (z+1)^2}$ is equivalent to minimizing $(x-1)^2+(y-2)^2 + (z+1)^2$ subject to the same constraint? Why is my thinking in normal vectors a bad idea? The values that solve the linear equations are the points needed for minimum distance. Why is that?",,"['multivariable-calculus', 'lagrange-multiplier', 'maxima-minima']"
85,Different behaviour of Newton's method for finding minimum of a function,Different behaviour of Newton's method for finding minimum of a function,,"Given the following function of two variables $$f(x, y) = 2x^2 − 2xy + y^2 + 2x − 2y$$ I wanted to use Newton's method to find a minimum of this function. I started from $(x_1, y_1) = (0, 0)$ and applied the following formula $$x_{k+1} = x_k - \alpha \left(\nabla(f(x_k)^{2}\right)^{-1} \nabla(f(x_k))$$ where $$ \alpha = \frac{\nabla(f(x_k))^{T} \nabla(f(x_k))}{\nabla(f(x_k))^{T}(\nabla(f(x_k)^{2})\nabla(f(x_k)) }$$ I obtained $(x_2, y_2) = \left(0, \frac{1}{5}\right)$ and $(x_3, y_3) = \left(0, 0\right)$. So I decided to check by induction if in general we have $(x_{2k}, y_{2k}) = \left(0, \frac{1}{5}\right)$ and $(x_{2k+1}, y_{2k+1}) = (0, 0)$. It turns out that this is true, so I will never obtain a minimum for $k \rightarrow \infty$. Can anyone explain me this strange behaviour of this method.","Given the following function of two variables $$f(x, y) = 2x^2 − 2xy + y^2 + 2x − 2y$$ I wanted to use Newton's method to find a minimum of this function. I started from $(x_1, y_1) = (0, 0)$ and applied the following formula $$x_{k+1} = x_k - \alpha \left(\nabla(f(x_k)^{2}\right)^{-1} \nabla(f(x_k))$$ where $$ \alpha = \frac{\nabla(f(x_k))^{T} \nabla(f(x_k))}{\nabla(f(x_k))^{T}(\nabla(f(x_k)^{2})\nabla(f(x_k)) }$$ I obtained $(x_2, y_2) = \left(0, \frac{1}{5}\right)$ and $(x_3, y_3) = \left(0, 0\right)$. So I decided to check by induction if in general we have $(x_{2k}, y_{2k}) = \left(0, \frac{1}{5}\right)$ and $(x_{2k+1}, y_{2k+1}) = (0, 0)$. It turns out that this is true, so I will never obtain a minimum for $k \rightarrow \infty$. Can anyone explain me this strange behaviour of this method.",,"['multivariable-calculus', 'optimization', 'numerical-optimization', 'quadratic-programming', 'newton-raphson']"
86,Dot product of a vector and del operator,Dot product of a vector and del operator,,How did this expansion come about. What is the physical significance of this expansion. And what is the significance of  $$\vec{A} . \nabla {\vec{A}}$$ And what does this mathematically represent.,How did this expansion come about. What is the physical significance of this expansion. And what is the significance of  $$\vec{A} . \nabla {\vec{A}}$$ And what does this mathematically represent.,,"['calculus', 'multivariable-calculus', 'vectors']"
87,Minimizing the function.,Minimizing the function.,,"Minimizing the following function $f(x_1,x_2,\cdots,x_n)=\prod\limits_i^n x_i^{x_i}$ such that $x_1+x_2+\cdots+x_n=P, 2\le x_i$ and $x_i$ are integers. My attempt: In my opinion we obtain the result when all $x_i's$ are almost equal i.e. $|x_i-x_j|\le 0$ for all $i$ and $j$. I am trying to solve by Lagrange's multiplier and I obtained a system of equation which looks messy.","Minimizing the following function $f(x_1,x_2,\cdots,x_n)=\prod\limits_i^n x_i^{x_i}$ such that $x_1+x_2+\cdots+x_n=P, 2\le x_i$ and $x_i$ are integers. My attempt: In my opinion we obtain the result when all $x_i's$ are almost equal i.e. $|x_i-x_j|\le 0$ for all $i$ and $j$. I am trying to solve by Lagrange's multiplier and I obtained a system of equation which looks messy.",,"['real-analysis', 'multivariable-calculus', 'integer-programming']"
88,"let $a_1,a_2,. . . ,a_n \in \mathbb{R^+} $ then prove that :",let  then prove that :,"a_1,a_2,. . . ,a_n \in \mathbb{R^+} ","Let $a_1,a_2,. . . ,a_n \in \mathbb{R^+} $. Prove that :   $$\frac{ a_{1} }{ a_{2} ^{2}+ a_{3} ^{2}+\cdots+ a_{n} ^{2}} +\frac{ a_{2} }{ a_{1} ^{2}+ a_{3} ^{2}+\cdots+ a_{n} ^{2}} +\cdots+\frac{ a_{n} }{ a_{1} ^{2}+ a_{2} ^{2}+\cdots+ a_{n-1} ^{2}}  \geq \frac{ 4}{ a_{1}+ a_{2} + a_{3}+\cdots+ a_{n}}$$ I do not know where to start. please help me .","Let $a_1,a_2,. . . ,a_n \in \mathbb{R^+} $. Prove that :   $$\frac{ a_{1} }{ a_{2} ^{2}+ a_{3} ^{2}+\cdots+ a_{n} ^{2}} +\frac{ a_{2} }{ a_{1} ^{2}+ a_{3} ^{2}+\cdots+ a_{n} ^{2}} +\cdots+\frac{ a_{n} }{ a_{1} ^{2}+ a_{2} ^{2}+\cdots+ a_{n-1} ^{2}}  \geq \frac{ 4}{ a_{1}+ a_{2} + a_{3}+\cdots+ a_{n}}$$ I do not know where to start. please help me .",,"['calculus', 'multivariable-calculus', 'inequality', 'convex-analysis', 'cauchy-schwarz-inequality']"
89,Lagrange multipliers with inequality constraints.,Lagrange multipliers with inequality constraints.,,"Find the maximum of $f(x,y,z)=(x+y+z)^3$ , in $\mathbb{R}^3$ with the following constraints: $x \ge 0,\ 3x+2y+z=1,\ z\ge x^2+y^2$ I know how to work with lagrange multipliers when the constraints are equalities(defining $h=f-\lambda_1 g_1-\lambda _2 g_2-...-\lambda_kg_k$ and solving $h=0$ .But what am I suppose to do when the constraints are inequalities?","Find the maximum of , in with the following constraints: I know how to work with lagrange multipliers when the constraints are equalities(defining and solving .But what am I suppose to do when the constraints are inequalities?","f(x,y,z)=(x+y+z)^3 \mathbb{R}^3 x \ge 0,\ 3x+2y+z=1,\ z\ge x^2+y^2 h=f-\lambda_1 g_1-\lambda _2 g_2-...-\lambda_kg_k h=0","['calculus', 'multivariable-calculus', 'inequality', 'optimization', 'lagrange-multiplier']"
90,Proving derivative of a function is a linear isomorphism on some subset of the domain,Proving derivative of a function is a linear isomorphism on some subset of the domain,,"Suppose $f:U\subseteq\mathbb{R^n}\to\mathbb{R^n}$ is of class $C^1$ and for some $a\in U$ , $Df(a)$ is a linear isomorphism. Prove there exists a $V\subseteq U$ such that it contains $a$ and also for all $p\in V$ , $Df(p)$ is a linear isomorphism. I know it has something to do with the inverse function theorem, but I can not figure out a way to prove it.","Suppose is of class and for some , is a linear isomorphism. Prove there exists a such that it contains and also for all , is a linear isomorphism. I know it has something to do with the inverse function theorem, but I can not figure out a way to prove it.",f:U\subseteq\mathbb{R^n}\to\mathbb{R^n} C^1 a\in U Df(a) V\subseteq U a p\in V Df(p),"['multivariable-calculus', 'partial-derivative', 'inverse-function', 'vector-space-isomorphism']"
91,Gradient of distance function has modulus 1,Gradient of distance function has modulus 1,,"In this article of Wikipedia it is stated that, if $\Omega$ is a subset of $\mathbb{R}^n$ with smooth boundary, then $$f(x)=\begin{cases} d(x,\partial\Omega),\;\;x\in\Omega\\ -d(x,\partial\Omega),\;\;x\notin \Omega \end{cases}$$ satisfies $|\nabla f(x)|=1$ for all $x\in\mathbb{R}^n$. Could you give me an outline of the proof? I read an answer in this site solving in fact this question, but I do not understand it (maybe not enough details, maybe my level of geometry is not good...) Motivation: Coarea formula says that, if $g\in L^1(\Omega)$, $u\in C^1(\bar{\Omega})$ and $|\nabla u|>0$, then $\int_{\Omega} g\,dx=\int_{\mathbb{R}}\int_{\{u=\lambda\}} g/|\nabla u|\,d\sigma\,d\lambda$. In the particular case $u(x)=d(x,\partial\Omega)$, I read that $\int_{\Omega} g\,dx=\int_{\mathbb{R}}\int_{\{u=\lambda\}} g\,d\sigma\,d\lambda$.","In this article of Wikipedia it is stated that, if $\Omega$ is a subset of $\mathbb{R}^n$ with smooth boundary, then $$f(x)=\begin{cases} d(x,\partial\Omega),\;\;x\in\Omega\\ -d(x,\partial\Omega),\;\;x\notin \Omega \end{cases}$$ satisfies $|\nabla f(x)|=1$ for all $x\in\mathbb{R}^n$. Could you give me an outline of the proof? I read an answer in this site solving in fact this question, but I do not understand it (maybe not enough details, maybe my level of geometry is not good...) Motivation: Coarea formula says that, if $g\in L^1(\Omega)$, $u\in C^1(\bar{\Omega})$ and $|\nabla u|>0$, then $\int_{\Omega} g\,dx=\int_{\mathbb{R}}\int_{\{u=\lambda\}} g/|\nabla u|\,d\sigma\,d\lambda$. In the particular case $u(x)=d(x,\partial\Omega)$, I read that $\int_{\Omega} g\,dx=\int_{\mathbb{R}}\int_{\{u=\lambda\}} g\,d\sigma\,d\lambda$.",,"['multivariable-calculus', 'differential-geometry']"
92,"Evaluate $\int_0^\infty {\rm Tr}\left( {\bf A}^{-1} (x \,{\bf A}+{\bf I})^{-1} -\frac{1}{2+x} {\bf I} \right) dx$",Evaluate,"\int_0^\infty {\rm Tr}\left( {\bf A}^{-1} (x \,{\bf A}+{\bf I})^{-1} -\frac{1}{2+x} {\bf I} \right) dx","I would like to evaluate the following integral. \begin{align} \int_0^\infty {\rm Tr}\left( {\bf A}^{-1} (x \,{\bf A}+{\bf I})^{-1} -\frac{1}{2+x} {\bf I} \right) dx \end{align} where ${\bf A} \in \mathbb{R}^{n \times n}$ is a real positive definte matrix and ${\bf I}\in \mathbb{R}^{n \times n}$  is an identity. Unfortunately, my matrix calculus is very weak.  So, any reference where I can look up this would also be great. For the scalar case, the integral is equal to \begin{align} \int_0^\infty  \frac{\frac{1}{a}}{ax+1}-\frac{1}{2+x} dx=\log(2)-\log(a). \end{align} Thanks.","I would like to evaluate the following integral. \begin{align} \int_0^\infty {\rm Tr}\left( {\bf A}^{-1} (x \,{\bf A}+{\bf I})^{-1} -\frac{1}{2+x} {\bf I} \right) dx \end{align} where ${\bf A} \in \mathbb{R}^{n \times n}$ is a real positive definte matrix and ${\bf I}\in \mathbb{R}^{n \times n}$  is an identity. Unfortunately, my matrix calculus is very weak.  So, any reference where I can look up this would also be great. For the scalar case, the integral is equal to \begin{align} \int_0^\infty  \frac{\frac{1}{a}}{ax+1}-\frac{1}{2+x} dx=\log(2)-\log(a). \end{align} Thanks.",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'matrix-calculus']"
93,Does a summation of two negative Gaussians with different standard deviations have a stationary point?,Does a summation of two negative Gaussians with different standard deviations have a stationary point?,,"I wanted to find the critical points of the (simple!) equation: $$ B(w; \mu_1, \sigma_1, \mu_2, \sigma_2 ) = 1 - e^{ - \beta_1 (w - \mu_1)^2 } - e^{ - \beta_2 (w - \mu_2)^2}$$ Where $\beta_i = \frac{1}{2 \sigma^2_i}$ usually called the precision parameter of the Gaussian or RBF. Anyway, what I wanted in particular to solve is: $$ B'(w; \mu_1, \sigma_1, \mu_2, \sigma_2 ) = - \beta_1 (w-\mu_1) e^{ - \beta_1 (w - \mu_1)^2 } - \beta_2(w - \mu_2)e^{ - \beta_2 (w - \mu_2)^2} = 0$$ looking at the equation above I can't seem to find an easy way to make it zero. To make the first term zero we can choose $w = \mu_1 $ but that doesn't make the second term zero. Similarly, the other way round. However, it is super intuitively obvious that there are 2 minimums and 1 maximum since there are two dips defined by the upside down Gaussian (and in between them some maximum). I plotted this in mathematica to confirm that there must be these critical points and it seems I am correct: however surprisingly I cannot find an easy way to express the critical points analytically. My initial goal is to at least have a way to express these 3 critical points for this 1 dimensional example if ever want to hope to find the critical points for higher dimensions. Anyone knows how to do this? My real intention is to be able to programmatically/systematically get the x-axis value between the two minimums that has exactly gradient equal to zero. Ideally able to generalize in higher dimensions. For 1D and 2D I can sort of eye ball what the value should be by plotting things and then asking mathematics to tell me which value of $x$ leads to $\nabla f'(x) = 0$.","I wanted to find the critical points of the (simple!) equation: $$ B(w; \mu_1, \sigma_1, \mu_2, \sigma_2 ) = 1 - e^{ - \beta_1 (w - \mu_1)^2 } - e^{ - \beta_2 (w - \mu_2)^2}$$ Where $\beta_i = \frac{1}{2 \sigma^2_i}$ usually called the precision parameter of the Gaussian or RBF. Anyway, what I wanted in particular to solve is: $$ B'(w; \mu_1, \sigma_1, \mu_2, \sigma_2 ) = - \beta_1 (w-\mu_1) e^{ - \beta_1 (w - \mu_1)^2 } - \beta_2(w - \mu_2)e^{ - \beta_2 (w - \mu_2)^2} = 0$$ looking at the equation above I can't seem to find an easy way to make it zero. To make the first term zero we can choose $w = \mu_1 $ but that doesn't make the second term zero. Similarly, the other way round. However, it is super intuitively obvious that there are 2 minimums and 1 maximum since there are two dips defined by the upside down Gaussian (and in between them some maximum). I plotted this in mathematica to confirm that there must be these critical points and it seems I am correct: however surprisingly I cannot find an easy way to express the critical points analytically. My initial goal is to at least have a way to express these 3 critical points for this 1 dimensional example if ever want to hope to find the critical points for higher dimensions. Anyone knows how to do this? My real intention is to be able to programmatically/systematically get the x-axis value between the two minimums that has exactly gradient equal to zero. Ideally able to generalize in higher dimensions. For 1D and 2D I can sort of eye ball what the value should be by plotting things and then asking mathematics to tell me which value of $x$ leads to $\nabla f'(x) = 0$.",,"['calculus', 'multivariable-calculus', 'optimization', 'normal-distribution', 'nonlinear-optimization']"
94,"Is $f(x)(y)$ the same as $f(x,y)$?",Is  the same as ?,"f(x)(y) f(x,y)","I have a slight confusion with the notation $f(x)(y)$ . Does $f(x)(y)$ mean the same as $f(x,y)$ ? Or does it mean something else? Particularly I saw the notation $f(x)(y)$ in the following definition for Eulerian derivative of a functional $J: \Omega \rightarrow J(\Omega)$ to the direction of the vector field $V$ : $$dJ(\Omega;V) = \lim_{t\rightarrow 0}\frac{J(\Omega_t)-J(\Omega)}{t}$$ where $\Omega_t = T_t(V)(\Omega).$",I have a slight confusion with the notation . Does mean the same as ? Or does it mean something else? Particularly I saw the notation in the following definition for Eulerian derivative of a functional to the direction of the vector field : where,"f(x)(y) f(x)(y) f(x,y) f(x)(y) J: \Omega \rightarrow J(\Omega) V dJ(\Omega;V) = \lim_{t\rightarrow 0}\frac{J(\Omega_t)-J(\Omega)}{t} \Omega_t = T_t(V)(\Omega).","['multivariable-calculus', 'notation']"
95,"Proof for the ""Fundamental Calculus Theorem"" for two variables.","Proof for the ""Fundamental Calculus Theorem"" for two variables.",,"I`ve been stuck with this proof and mostly because I'm not sure if I can do a certain step, but at least without formality, it sounds good. The problem goes like this: Be f $\in$C(R) such that $R=[a,b]\times[c,d]$, and we define $F(x,y)=\int^{x}_{a}\int^{y}_{c}f(u,v)du\cdot dv$. Prove that $\dfrac{\partial^{2}F}{\partial x \partial y}=\dfrac{\partial^{2}F}{\partial y \partial x}=f(x,y)$ Now the far that I got, formaly speaking, is to show that by taking the function f with an $x_o \in[a,b]$ fixed and then seeing that function like a composition of fuctions such that $f(h(y))=g(y)$ where $ h(y)=x_o\widehat e_{x}+y\widehat e_{y}$, then I could apply the first part of the fundamental calculus theorem, and show that exist $G(y)=\int^y_cg(v)dv$ with the propety that $G'(y)=g(y)=f(x_o,y)$. Here is were my intuition takes place but I'm stuck trying to write it. Because this contruction leds to verify the existance of $G(y)$ for every $x\in[a,b]$, then I want to define $M(x)=\int^d_cH(y)du=\int^d_c[\int^b_af(u,v)dv]du$ and show that its derivative is $H(y)$ (I suppose that I could use the Fubini's Theorem to show the existance of $M(x)$ with the benefit that would ensure $M's$ integrability in $[a,b]$. But because Fubini's theorem shows the existance of that function with integral intervals from a to b, Im not sure if that could apply to my definition of $M$). And finally with all of this, having the existance of $\dfrac{\partial F}{\partial x},\dfrac{\partial F}{\partial x}$ and $\dfrac{\partial^2F}{\partial x \partial y}$ being continuous by hypothesis, then by another theorem this implies that $\dfrac{\partial^{2}F}{\partial x \partial y}=\dfrac{\partial^{2}F}{\partial y \partial x}$. Well this is more intuitive than formal, I would aprecciate some comments from my idea, NOT a solution, or something that could let me advance with the proof. UPDATE: Justification for the iteration integral. Here is because it describes the function $$\frac{\partial}{\partial y}F(x,y) = \frac{\partial}{\partial y}\int_a^xG(u,y) \, du = \lim_{h\rightarrow 0}\dfrac{\int_a^xG(u,y+h)du-\int_a^xG(u,y)du}{h}$$ $$=\lim_{h\rightarrow 0}\int_a^x\dfrac{(G(u,y+h)-G(u,y))}{h}du$$ and because of the continuity of $G(u,y)$ in $[c,d]$ and its derivate on $y$ exists (for TFC on f) such that  $$\dfrac{\partial}{\partial y}G(u,y)=f(u,y)$$ and then exist ($\int_a^b \dfrac{\partial}{\partial y}G(x,y) dx$) for the integrability on $f$,  then applying the mean value theorem exists $\xi\in[y,y+h]$ such that $$\left|\int_a^b \frac{G(x,y+h) - G(x,y)}{h} \, dx - \int_a^b \dfrac{\partial}{\partial y}G(x,y) \, dx\right|\\  =  \left|\int_a^b \dfrac{\partial}{\partial y} G(x, \xi) \, dx - \int_a^b \dfrac{\partial}{\partial y}G(x,y) \, dx\right|\\ \leqslant  \int_a^b |\dfrac{\partial}{\partial y}G(x,\xi)  - \dfrac{\partial}{\partial y}G(x,y)| \, dx.$$ Since $f$ is continuous on $[a,b] \times [c,d]$, for its uniformly continuity, for any $\epsilon > 0$ there exists $\delta >0$ such that if $|h| < \delta$ then $ |\dfrac{\partial}{\partial y}G(x,\xi)  - \dfrac{\partial}{\partial y}G(x,y)|= |f(x,\xi)  - f(x,y)|< \epsilon/(b-a)$ and $$\int_a^b |\dfrac{\partial}{\partial y}G(x,\xi)  - \dfrac{\partial}{\partial y}G(x,y)| \, dx < \epsilon.$$ Thus, $$\lim_{h \rightarrow 0} \int_a^b \frac{G(x,y+h) - G(x,y)}{h} \, dx = \int_a^b \dfrac{\partial}{\partial y} G(x,y) \, dx \\ = \int_a^b \lim_{h \rightarrow 0} \frac{G(x,y+h) - G(x,y)}{h} \, dx . $$","I`ve been stuck with this proof and mostly because I'm not sure if I can do a certain step, but at least without formality, it sounds good. The problem goes like this: Be f $\in$C(R) such that $R=[a,b]\times[c,d]$, and we define $F(x,y)=\int^{x}_{a}\int^{y}_{c}f(u,v)du\cdot dv$. Prove that $\dfrac{\partial^{2}F}{\partial x \partial y}=\dfrac{\partial^{2}F}{\partial y \partial x}=f(x,y)$ Now the far that I got, formaly speaking, is to show that by taking the function f with an $x_o \in[a,b]$ fixed and then seeing that function like a composition of fuctions such that $f(h(y))=g(y)$ where $ h(y)=x_o\widehat e_{x}+y\widehat e_{y}$, then I could apply the first part of the fundamental calculus theorem, and show that exist $G(y)=\int^y_cg(v)dv$ with the propety that $G'(y)=g(y)=f(x_o,y)$. Here is were my intuition takes place but I'm stuck trying to write it. Because this contruction leds to verify the existance of $G(y)$ for every $x\in[a,b]$, then I want to define $M(x)=\int^d_cH(y)du=\int^d_c[\int^b_af(u,v)dv]du$ and show that its derivative is $H(y)$ (I suppose that I could use the Fubini's Theorem to show the existance of $M(x)$ with the benefit that would ensure $M's$ integrability in $[a,b]$. But because Fubini's theorem shows the existance of that function with integral intervals from a to b, Im not sure if that could apply to my definition of $M$). And finally with all of this, having the existance of $\dfrac{\partial F}{\partial x},\dfrac{\partial F}{\partial x}$ and $\dfrac{\partial^2F}{\partial x \partial y}$ being continuous by hypothesis, then by another theorem this implies that $\dfrac{\partial^{2}F}{\partial x \partial y}=\dfrac{\partial^{2}F}{\partial y \partial x}$. Well this is more intuitive than formal, I would aprecciate some comments from my idea, NOT a solution, or something that could let me advance with the proof. UPDATE: Justification for the iteration integral. Here is because it describes the function $$\frac{\partial}{\partial y}F(x,y) = \frac{\partial}{\partial y}\int_a^xG(u,y) \, du = \lim_{h\rightarrow 0}\dfrac{\int_a^xG(u,y+h)du-\int_a^xG(u,y)du}{h}$$ $$=\lim_{h\rightarrow 0}\int_a^x\dfrac{(G(u,y+h)-G(u,y))}{h}du$$ and because of the continuity of $G(u,y)$ in $[c,d]$ and its derivate on $y$ exists (for TFC on f) such that  $$\dfrac{\partial}{\partial y}G(u,y)=f(u,y)$$ and then exist ($\int_a^b \dfrac{\partial}{\partial y}G(x,y) dx$) for the integrability on $f$,  then applying the mean value theorem exists $\xi\in[y,y+h]$ such that $$\left|\int_a^b \frac{G(x,y+h) - G(x,y)}{h} \, dx - \int_a^b \dfrac{\partial}{\partial y}G(x,y) \, dx\right|\\  =  \left|\int_a^b \dfrac{\partial}{\partial y} G(x, \xi) \, dx - \int_a^b \dfrac{\partial}{\partial y}G(x,y) \, dx\right|\\ \leqslant  \int_a^b |\dfrac{\partial}{\partial y}G(x,\xi)  - \dfrac{\partial}{\partial y}G(x,y)| \, dx.$$ Since $f$ is continuous on $[a,b] \times [c,d]$, for its uniformly continuity, for any $\epsilon > 0$ there exists $\delta >0$ such that if $|h| < \delta$ then $ |\dfrac{\partial}{\partial y}G(x,\xi)  - \dfrac{\partial}{\partial y}G(x,y)|= |f(x,\xi)  - f(x,y)|< \epsilon/(b-a)$ and $$\int_a^b |\dfrac{\partial}{\partial y}G(x,\xi)  - \dfrac{\partial}{\partial y}G(x,y)| \, dx < \epsilon.$$ Thus, $$\lim_{h \rightarrow 0} \int_a^b \frac{G(x,y+h) - G(x,y)}{h} \, dx = \int_a^b \dfrac{\partial}{\partial y} G(x,y) \, dx \\ = \int_a^b \lim_{h \rightarrow 0} \frac{G(x,y+h) - G(x,y)}{h} \, dx . $$",,"['calculus', 'multivariable-calculus', 'proof-writing']"
96,"Is the function $f(x, y) :=$ $\sin(xy) \over x^2 + y^2$ Lebesgue-integrable?",Is the function   Lebesgue-integrable?,"f(x, y) := \sin(xy) \over x^2 + y^2","Given the function $f: \Bbb R^2 \rightarrow \Bbb R$, $f(x, y) := \begin{cases} \sin(xy) \over x^2 + y^2,  & \text{$(x, y) \in \Bbb R^2 \setminus ${0}$$} \\ 0, & \text{otherwise} \end{cases}$ decide whether it is Lebesgue-integrable or not. Hint: $\int_{(0, \infty)}$ $1 \over r$ $d\lambda(r) = \infty$. Where to start about something like that? I know that a function is Lebesgue integrable if it is measurable and if $f(x, y)_+$ and $f(x, y)_-$ are integrable. I think it is measurable since it is continuous. So now, I would have to determine what $f(x, y)_+$ is and prove (or disprove) that $\int_{\Bbb R} f(x, y)_+ < \infty$? Edit: Since I didn't receive further help in the comments, I am searching for an other approach. There is a similar problem to this: Prove that function is not Lebesgue integrable If you take a look at the answer with 8 upvotes, you'll find that the hint that I was given was applied there. So by switching to polar coordinates $(x, y) = (r \cos \phi, r \sin \phi)$, we would receive something like: $\int_0^{2\pi} \int_0^{\infty}$ $\sin(r^2 \ \cos \phi \ \sin \phi) \over r$ $dr \ d\phi.$ If there was a way to ""clear"" the numerator here, it would be fairly easy to apply the hint here too, which would yield that the function is not Lebesgue-integrable directly. But is there a way to do it? Edit 2: On the other hand, isn't $\int_0^{\infty}$ $\sin(r^2 \ \cos \phi \ \sin \phi) \over r$ $\le \int_0^{\infty}$ $1 \over r$?","Given the function $f: \Bbb R^2 \rightarrow \Bbb R$, $f(x, y) := \begin{cases} \sin(xy) \over x^2 + y^2,  & \text{$(x, y) \in \Bbb R^2 \setminus ${0}$$} \\ 0, & \text{otherwise} \end{cases}$ decide whether it is Lebesgue-integrable or not. Hint: $\int_{(0, \infty)}$ $1 \over r$ $d\lambda(r) = \infty$. Where to start about something like that? I know that a function is Lebesgue integrable if it is measurable and if $f(x, y)_+$ and $f(x, y)_-$ are integrable. I think it is measurable since it is continuous. So now, I would have to determine what $f(x, y)_+$ is and prove (or disprove) that $\int_{\Bbb R} f(x, y)_+ < \infty$? Edit: Since I didn't receive further help in the comments, I am searching for an other approach. There is a similar problem to this: Prove that function is not Lebesgue integrable If you take a look at the answer with 8 upvotes, you'll find that the hint that I was given was applied there. So by switching to polar coordinates $(x, y) = (r \cos \phi, r \sin \phi)$, we would receive something like: $\int_0^{2\pi} \int_0^{\infty}$ $\sin(r^2 \ \cos \phi \ \sin \phi) \over r$ $dr \ d\phi.$ If there was a way to ""clear"" the numerator here, it would be fairly easy to apply the hint here too, which would yield that the function is not Lebesgue-integrable directly. But is there a way to do it? Edit 2: On the other hand, isn't $\int_0^{\infty}$ $\sin(r^2 \ \cos \phi \ \sin \phi) \over r$ $\le \int_0^{\infty}$ $1 \over r$?",,"['integration', 'multivariable-calculus', 'lebesgue-integral', 'polar-coordinates']"
97,How prove this converse cauchy inequality of problem?,How prove this converse cauchy inequality of problem?,,"Problem: let $A$ and $B$ be positive real numbers such that $$-A\le a_{i}\le A,-B\le b_{i}\le B~(i=1,2,\cdots,n)$$ . Show that $$n\sum_{i=1}^{n}a^2_{i}b^2_{i}-\left(\sum_{i=1}^{n}a_{i}b_{i}\right)^2\le 2B^2\left[n\sum_{i=1}^{n}a^2_{i}-\left(\sum_{i=1}^{n}a_{i}\right)^2\right]+2A^2\left[n\sum_{i=1}^{n}b^2_{i}-\left(\sum_{i=1}^{n}b_{i}\right)^2\right]$$ maybe it relate this well known polya-szego inequality: $$\left(\sum_{k=1}^{n}a^2_{k}\right)\left(\sum_{k=1}^{n}b^2_{k}\right)\le\dfrac{1}{4}\left(\sqrt{\dfrac{M_{1}M_{2}}{m_{1}m_{2}}}+\sqrt{\dfrac{m_{1}m_{2}}{M_{1}M_{2}}}\right)^2\left(\sum_{k=1}^{n}a_{k}b_{k}\right)^2$$",Problem: let and be positive real numbers such that . Show that maybe it relate this well known polya-szego inequality:,"A B -A\le a_{i}\le A,-B\le b_{i}\le B~(i=1,2,\cdots,n) n\sum_{i=1}^{n}a^2_{i}b^2_{i}-\left(\sum_{i=1}^{n}a_{i}b_{i}\right)^2\le 2B^2\left[n\sum_{i=1}^{n}a^2_{i}-\left(\sum_{i=1}^{n}a_{i}\right)^2\right]+2A^2\left[n\sum_{i=1}^{n}b^2_{i}-\left(\sum_{i=1}^{n}b_{i}\right)^2\right] \left(\sum_{k=1}^{n}a^2_{k}\right)\left(\sum_{k=1}^{n}b^2_{k}\right)\le\dfrac{1}{4}\left(\sqrt{\dfrac{M_{1}M_{2}}{m_{1}m_{2}}}+\sqrt{\dfrac{m_{1}m_{2}}{M_{1}M_{2}}}\right)^2\left(\sum_{k=1}^{n}a_{k}b_{k}\right)^2","['multivariable-calculus', 'inequality', 'summation', 'cauchy-schwarz-inequality', 'sum-of-squares-method']"
98,Finding the range of the $n$-dimensional polar coordinate transform,Finding the range of the -dimensional polar coordinate transform,n,"Let $x=(x_1, \cdots, x_n)\in \mathbb{R}^n$ be Cartesian coordinates and $(r,\theta)=(r,\theta_1, \cdots , \theta_{n-1})$, where $r\in (0,\infty), \theta_1,\dots, \theta_{n-2}\in (0,\pi)$ and $\theta_{n-1}\in (-\pi,\pi)$ be polar coordinates. The coordinate transform is given by  $$\Psi:(r,\theta_1, \cdots, θ_{n - 1})\mapsto x=\begin{cases} x_1=r\cos{\theta_1}, \\ x_2=r\sin{\theta_1}\cos{\theta_2}, \\ x_3=r\sin{\theta_1}\sin{\theta_2}\cos{\theta_3}, \\ \vdots \\ x_{n-1}=r\sin{\theta_1}\sin{\theta_2}\dots \sin{\theta_{n-2}}\cos{\theta_{n-1}}, \\ x_n=r\sin{\theta_1}\sin{\theta_2}\dots \sin{\theta_{n-2}}\sin{\theta_{n-1}}. \end{cases}$$ I am trying to find the range of this function $\Psi$. I think it is $\mathbb{R}^n \backslash \{x:x_n=0,x_{n-1}\le 0\}$, but I cannot think of a way to show this explicitly from the definition of $\Psi$. I would greatly appreciate any help.","Let $x=(x_1, \cdots, x_n)\in \mathbb{R}^n$ be Cartesian coordinates and $(r,\theta)=(r,\theta_1, \cdots , \theta_{n-1})$, where $r\in (0,\infty), \theta_1,\dots, \theta_{n-2}\in (0,\pi)$ and $\theta_{n-1}\in (-\pi,\pi)$ be polar coordinates. The coordinate transform is given by  $$\Psi:(r,\theta_1, \cdots, θ_{n - 1})\mapsto x=\begin{cases} x_1=r\cos{\theta_1}, \\ x_2=r\sin{\theta_1}\cos{\theta_2}, \\ x_3=r\sin{\theta_1}\sin{\theta_2}\cos{\theta_3}, \\ \vdots \\ x_{n-1}=r\sin{\theta_1}\sin{\theta_2}\dots \sin{\theta_{n-2}}\cos{\theta_{n-1}}, \\ x_n=r\sin{\theta_1}\sin{\theta_2}\dots \sin{\theta_{n-2}}\sin{\theta_{n-1}}. \end{cases}$$ I am trying to find the range of this function $\Psi$. I think it is $\mathbb{R}^n \backslash \{x:x_n=0,x_{n-1}\le 0\}$, but I cannot think of a way to show this explicitly from the definition of $\Psi$. I would greatly appreciate any help.",,"['multivariable-calculus', 'polar-coordinates']"
99,Derivatives of a vector and its transpose,Derivatives of a vector and its transpose,,"Assuming that $f(x)=x^Tx-k^2=0$ holds for some $k$ and vector $x$, is it possible to derive that $$ u \nabla f = uIx $$ where $I$ is the identity matrix and $u$ is a lagrange multiplier? If I simply derive $f$ with respect to $x$, I get $$ u\left( Ix + x^T\right) $$ where I use that $\frac{d}{dx}x^T=I$, but it gives me that annoying extra term $ux^T$. I don't know if I'm doing something wrong or missing a trick where you can somehow ignore the last term. I'm a bit insecure in all of this, so any help is greatly appreciated!","Assuming that $f(x)=x^Tx-k^2=0$ holds for some $k$ and vector $x$, is it possible to derive that $$ u \nabla f = uIx $$ where $I$ is the identity matrix and $u$ is a lagrange multiplier? If I simply derive $f$ with respect to $x$, I get $$ u\left( Ix + x^T\right) $$ where I use that $\frac{d}{dx}x^T=I$, but it gives me that annoying extra term $ux^T$. I don't know if I'm doing something wrong or missing a trick where you can somehow ignore the last term. I'm a bit insecure in all of this, so any help is greatly appreciated!",,"['calculus', 'multivariable-calculus', 'matrix-calculus']"
