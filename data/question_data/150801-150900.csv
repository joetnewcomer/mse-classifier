,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Problem Uniqueness $-\Delta u + u^3 = 0$,Problem Uniqueness,-\Delta u + u^3 = 0,"Let $\Omega \subset \mathbb{R}^n$ open, bounded, connected, and with regular boundary. Assuming that $a(x) \geq 0$ for every $x \in \partial \Omega$ , show that there is at most a regular solution to the problem $$\left\{\begin{array}{rcl} -\Delta u + u^3 & = & 0, \ \ \mbox{in} \ \Omega\\ \dfrac{du}{d\nu}(x) + a(x)u(x) & = & h(x), \ \ \mbox{on} \ \partial \Omega. \end{array}\right.$$ Solution Proposal: Suppose there are $u$ and $v$ solutions to the above problem. Now consider $$w = u-v.$$ Since $u$ and $v$ are continuous functions in the compact $\overline{\Omega}$ , then they admit maximum and, consequently, there is $x_0 \in \overline{\Omega}$ such that $$w(x_0) = \max_{\overline{\Omega}} w.$$ Now, let's look at two cases: 1° case: Suppose that $x_0 \in \Omega$ . Since this point is the maximum, we have $\Delta w(x_0) \leq 0$ . Soon, $$u(x_0)^3 - v(x_0)^3 \leq 0 \ \ \Rightarrow \ \ [u(x_0) - v(x_0)][u^2(x_0) + u(x_0)v(x_0 ) + v^2(x_0)] \leq 0.$$ Assertion: $u^2(x_0) + u(x_0)v(x_0) + v^2(x_0) \geq 0$ ; Assume otherwise. Soon, $$0 \leq [u(x_0) + v(x_0)]^2 = u^2(x_0) + 2u(x_0)v(x_0) + v^2(x_0) < u(x_0)v(x_0). $$ As $u^2(x_0), v^2(x_0) \geq 0$ , we have an absurdity. So follows the statement. From the \textit{Assertion}, we have $u(x_0) \leq v(x_0)$ and, consequently, it follows that $w(x) \leq 0$ in $\overline{\Omega}$ since $w(x_0 ) \leq 0$ . Now note that \begin{eqnarray}\label{pv1} \nonumber \int_{\partial \Omega} \dfrac{\partial w}{\partial \nu} dS_x + \int_{\partial \Omega} a(x)w(x)dS_x = 0 & \Rightarrow & \int_{\Omega} \Delta w(x) dx = -\int_{\partial \Omega} a(x)w(x)dS_x\\ & \Rightarrow & \int_{\Omega} \Delta w(x) dx \geq 0. \end{eqnarray} Finally, as it counts \begin{equation}\label{pv2} \Delta w = [u(x)]^3 - [v(x)]^3 = \underbrace{[u(x) - v(x)]}_{\leq 0}\underbrace{[u^2 (x) + u(x)v(x) + v^2(x)]}_{\geq 0} \leq 0 \end{equation} in $\overline{\Omega}$ , we conclude that \begin{equation}\label{pv3} \int_{\Omega} \Delta w(x) dx \leq 0. \end{equation} So we have $\Delta w = 0$ in $\Omega$ . 2° case: Suppose that $x_0 \in \partial \Omega$ . [I couldn't show how in case 1!!!] Finally, as for cases 1 and 2 we have $\Delta w = 0$ in $\Omega$ , it follows that $$u^3 - v^3 = (u-v)(u^2 + uv + v^2) = 0 \ \ \mbox{em} \ \ \Omega.$$ So we have $\bullet$ If $u - v = 0$ in $\Omega$ , then $u = v$ in $\Omega$ ; $\bullet$ If $u^2 + uv + v^2 = 0$ in $\Omega$ , then $$0 \leq (u + v)^2 = u^2 + 2uv + v^2 = uv.$$ Hence, we conclude that $u = v = 0$ in $\Omega$ . Doubts: In this perspective, how to do case 2? Also, as I show $$u = v \ \ \mbox{in} \ \ \partial \Omega?$$","Let open, bounded, connected, and with regular boundary. Assuming that for every , show that there is at most a regular solution to the problem Solution Proposal: Suppose there are and solutions to the above problem. Now consider Since and are continuous functions in the compact , then they admit maximum and, consequently, there is such that Now, let's look at two cases: 1° case: Suppose that . Since this point is the maximum, we have . Soon, Assertion: ; Assume otherwise. Soon, As , we have an absurdity. So follows the statement. From the \textit{Assertion}, we have and, consequently, it follows that in since . Now note that Finally, as it counts in , we conclude that So we have in . 2° case: Suppose that . [I couldn't show how in case 1!!!] Finally, as for cases 1 and 2 we have in , it follows that So we have If in , then in ; If in , then Hence, we conclude that in . Doubts: In this perspective, how to do case 2? Also, as I show","\Omega \subset \mathbb{R}^n a(x) \geq 0 x \in \partial \Omega \left\{\begin{array}{rcl}
-\Delta u + u^3 & = & 0, \ \ \mbox{in} \ \Omega\\
\dfrac{du}{d\nu}(x) + a(x)u(x) & = & h(x), \ \ \mbox{on} \ \partial \Omega.
\end{array}\right. u v w = u-v. u v \overline{\Omega} x_0 \in \overline{\Omega} w(x_0) = \max_{\overline{\Omega}} w. x_0 \in \Omega \Delta w(x_0) \leq 0 u(x_0)^3 - v(x_0)^3 \leq 0 \ \ \Rightarrow \ \ [u(x_0) - v(x_0)][u^2(x_0) + u(x_0)v(x_0 ) + v^2(x_0)] \leq 0. u^2(x_0) + u(x_0)v(x_0) + v^2(x_0) \geq 0 0 \leq [u(x_0) + v(x_0)]^2 = u^2(x_0) + 2u(x_0)v(x_0) + v^2(x_0) < u(x_0)v(x_0).  u^2(x_0), v^2(x_0) \geq 0 u(x_0) \leq v(x_0) w(x) \leq 0 \overline{\Omega} w(x_0 ) \leq 0 \begin{eqnarray}\label{pv1}
\nonumber
\int_{\partial \Omega} \dfrac{\partial w}{\partial \nu} dS_x + \int_{\partial \Omega} a(x)w(x)dS_x = 0 & \Rightarrow & \int_{\Omega} \Delta w(x) dx = -\int_{\partial \Omega} a(x)w(x)dS_x\\
& \Rightarrow & \int_{\Omega} \Delta w(x) dx \geq 0.
\end{eqnarray} \begin{equation}\label{pv2}
\Delta w = [u(x)]^3 - [v(x)]^3 = \underbrace{[u(x) - v(x)]}_{\leq 0}\underbrace{[u^2 (x) + u(x)v(x) + v^2(x)]}_{\geq 0} \leq 0
\end{equation} \overline{\Omega} \begin{equation}\label{pv3}
\int_{\Omega} \Delta w(x) dx \leq 0.
\end{equation} \Delta w = 0 \Omega x_0 \in \partial \Omega \Delta w = 0 \Omega u^3 - v^3 = (u-v)(u^2 + uv + v^2) = 0 \ \ \mbox{em} \ \ \Omega. \bullet u - v = 0 \Omega u = v \Omega \bullet u^2 + uv + v^2 = 0 \Omega 0 \leq (u + v)^2 = u^2 + 2uv + v^2 = uv. u = v = 0 \Omega u = v \ \ \mbox{in} \ \ \partial \Omega?","['analysis', 'partial-differential-equations', 'laplacian']"
1,Prove that $\eta (1/2) \gt \frac{\sqrt 2 \pi^2}{48}$,Prove that,\eta (1/2) \gt \frac{\sqrt 2 \pi^2}{48},"Prove that $\eta (1/2) \gt \frac{\sqrt 2 \pi^2}{48}$ I am out of any good ideas, but the $\pi^2$ suggests some comparison with $\zeta(2)$ . Here is a bad try: $$\eta (1/2) \gt 1-\frac{1}{\sqrt 2} + \frac{1}{\sqrt 3}$$ Now it just remains to show that $\frac{\sqrt 2 \pi^2}{48} \lt 1-\frac{1}{\sqrt 2} + \frac{1}{\sqrt 3}$ . Any ideas?","Prove that I am out of any good ideas, but the suggests some comparison with . Here is a bad try: Now it just remains to show that . Any ideas?",\eta (1/2) \gt \frac{\sqrt 2 \pi^2}{48} \pi^2 \zeta(2) \eta (1/2) \gt 1-\frac{1}{\sqrt 2} + \frac{1}{\sqrt 3} \frac{\sqrt 2 \pi^2}{48} \lt 1-\frac{1}{\sqrt 2} + \frac{1}{\sqrt 3},"['analysis', 'inequality', 'zeta-functions']"
2,Time dependent Sobolev spaces and parabolic PDE,Time dependent Sobolev spaces and parabolic PDE,,"Let $X$ be a Banach space and $u \in L^p(0,T;X).$ Then $v \in L^p(0,T;X)$ is called weak derivative of $u$ if \begin{eqnarray}\tag{1} \int\limits_0^T u(t)\,\phi_t \,\mathrm dt = -\int\limits_0^T v(t)\,\phi(t)\,\mathrm dt \quad \text{for all } \phi \in C_c^{\infty}((0,T)).  \end{eqnarray} The weak derivative $v$ is denoted by $u'$ . While defining the weak formulation of Linear parabolic PDEs (for example, $u_t-\Delta u=0$ ), the weak formulation is defined for $u \in L^2(0,T;H^1_0(\Omega))$ and $u'\in L^2(0,T;H^{-1}(\Omega)).$ My doubts are the following: Why do we need $u'\in L^2(0,T;H^{-1}(\Omega))?$ and why not $u' \in L^2(0,T;H^1_0(\Omega))$ as in the definition of weak derivative? In view of (1), how to interpret $u'=v \in L^p(0,T;X^*)$ as the weak derivative of $u \in L^p(0,T;X)?$ P.S. : Notations are same as the ones used Chapter 5 and 7 of the book ""Partial Differential Equations'' by L.C. Evans.","Let be a Banach space and Then is called weak derivative of if The weak derivative is denoted by . While defining the weak formulation of Linear parabolic PDEs (for example, ), the weak formulation is defined for and My doubts are the following: Why do we need and why not as in the definition of weak derivative? In view of (1), how to interpret as the weak derivative of P.S. : Notations are same as the ones used Chapter 5 and 7 of the book ""Partial Differential Equations'' by L.C. Evans.","X u \in L^p(0,T;X). v \in L^p(0,T;X) u \begin{eqnarray}\tag{1}
\int\limits_0^T u(t)\,\phi_t \,\mathrm dt = -\int\limits_0^T v(t)\,\phi(t)\,\mathrm dt \quad \text{for all } \phi \in C_c^{\infty}((0,T)). 
\end{eqnarray} v u' u_t-\Delta u=0 u \in L^2(0,T;H^1_0(\Omega)) u'\in L^2(0,T;H^{-1}(\Omega)). u'\in L^2(0,T;H^{-1}(\Omega))? u' \in L^2(0,T;H^1_0(\Omega)) u'=v \in L^p(0,T;X^*) u \in L^p(0,T;X)?","['functional-analysis', 'analysis', 'partial-differential-equations', 'sobolev-spaces', 'parabolic-pde']"
3,Similarity reductions in Black Scholes PDE,Similarity reductions in Black Scholes PDE,,"Suppose that $V(S,I,t)$ satisfies the equality $$ \frac{\partial V}{\partial t}+\frac{1}{2}\sigma^2S^2 \frac{\partial^2V}{\partial S^2}+S\frac{\partial V}{\partial I}+rS\frac{\partial V}{\partial S}-rV=0 $$ Here $I=\int _0^t S dt$ . Now, let $R=S/I$ and $V(S,R,t)=I\,W(R,t)$ . Is it true that $W$ satisfies the following equality? $$\frac{\partial W}{\partial t}+\frac{1}{2}\sigma^2R^2\frac{\partial^2 W}{\partial R^2}+R(r-R)\frac{\partial W}{\partial R}-(r-R)W=0  $$","Suppose that satisfies the equality Here . Now, let and . Is it true that satisfies the following equality?","V(S,I,t) 
\frac{\partial V}{\partial t}+\frac{1}{2}\sigma^2S^2 \frac{\partial^2V}{\partial S^2}+S\frac{\partial V}{\partial I}+rS\frac{\partial V}{\partial S}-rV=0
 I=\int _0^t S dt R=S/I V(S,R,t)=I\,W(R,t) W \frac{\partial W}{\partial t}+\frac{1}{2}\sigma^2R^2\frac{\partial^2 W}{\partial R^2}+R(r-R)\frac{\partial W}{\partial R}-(r-R)W=0 
","['calculus', 'analysis', 'partial-differential-equations', 'partial-derivative']"
4,If $f_n \rightrightarrows f$ and every $f_n$ has antiderivative is that true that $f$ has antiderivative?,If  and every  has antiderivative is that true that  has antiderivative?,f_n \rightrightarrows f f_n f,"There is a sequence of functions $f_n$ and each of these functions has antiderivative. If they are uniformly convergent to $f$ is that true that $f$ has antiderivative? I know that is true in case of continuity and that every continuous function has antiderivative but on the other hand, I know that there are functions that are not continuous but have antiderivatives.","There is a sequence of functions and each of these functions has antiderivative. If they are uniformly convergent to is that true that has antiderivative? I know that is true in case of continuity and that every continuous function has antiderivative but on the other hand, I know that there are functions that are not continuous but have antiderivatives.",f_n f f,"['real-analysis', 'integration', 'analysis', 'uniform-convergence']"
5,"You can't subtract inequalities, but can you multiply by -1 then add?","You can't subtract inequalities, but can you multiply by -1 then add?",,"Suppose $\{x_n\}, \{y_n\}$ are Cauchy sequences in metric space $(X, d)$ . Prove that the sequence $\{a_n\}$ defined by $a_n = d(x_n, y_n)$ converges in $\mathbb{R}$ . My Question. Before I write my proof attempt, I'll ask my question: I have the idea for this proof in mind, but I'm not so good at writing proofs using slick manipulations of inequalities. I came up with a manipulation that I want to use, but I'm not sure if I'm allowed to. So, my question is not necessarily about how to prove the above statement. (It has been proved on stack exchange here and here .) Rather my question is whether or not I'm allowed to manipulate inequalities in the way I have below, starting at the line ""In symbols, we have..."" My Proof. Choose $\varepsilon >0$ . Because $\{x_n\}$ is Cauchy, there exists $N \in \mathbb{N}$ such that $d(x_n, x_m) < \varepsilon/8$ for all $n, m \geq N$ . A similar $M \in \mathbb{N}$ exists for sequence $\{y_n\}$ . Choose $\max\{N, M\}$ and, without loss of generality, assume $\max\{N, M\} = N$ . Then, for all $n \geq N$ , we have $x_n \in B_{\varepsilon/4}(x_N)$ and $y_n \in B_{\varepsilon/4}(y_N)$ . Let $$D = \{d(x,y) : x \in B_{\varepsilon/4}(x_N) \text{ and } y \in B_{\varepsilon/4}(y_N)\}.$$ Then for any $x_n, y_n$ with $n > N$ , the distance $d(x_n, y_n)$ is bounded above by $\sup D = d(x_N, y_N) + \varepsilon/2$ and is bounded below by $\inf D = d(x_N, y_N) - \varepsilon/2$ . In symbols, we have $$\begin{align} d(x_N, y_N) - \varepsilon/2 &< d(x_n, y_n) < d(x_N, y_N) + \varepsilon/2. \tag{1} \end{align}$$ Inequality (1) is still true for any other $m > N$ , so substituting such an $m$ then multiplying through by $-1$ , we obtain $$\begin{align} -d(x_N, y_N) + \varepsilon/2 &> -d(x_m, y_m) > -d(x_N, y_N) - \varepsilon/2. \tag{2} \end{align}$$ Rearranging (2) so that its inequality signs face the same direction as (1), we can add (1) and (2) to obtain $$\begin{align} -\varepsilon < d(x_n,y_n) - d(x_m, y_m) &< \varepsilon, \text{ i. e.} \\ |d(x_n, y_n) - d(x_m, y_m)| &< \varepsilon, \text{ meaning that} \\ |a_n - a_m| &< \varepsilon. \end{align}$$ Therefore, $\{a_n\}$ is Cauchy in $\mathbb{R}$ , and so $\{a_n\}$ converges. (qed)","Suppose are Cauchy sequences in metric space . Prove that the sequence defined by converges in . My Question. Before I write my proof attempt, I'll ask my question: I have the idea for this proof in mind, but I'm not so good at writing proofs using slick manipulations of inequalities. I came up with a manipulation that I want to use, but I'm not sure if I'm allowed to. So, my question is not necessarily about how to prove the above statement. (It has been proved on stack exchange here and here .) Rather my question is whether or not I'm allowed to manipulate inequalities in the way I have below, starting at the line ""In symbols, we have..."" My Proof. Choose . Because is Cauchy, there exists such that for all . A similar exists for sequence . Choose and, without loss of generality, assume . Then, for all , we have and . Let Then for any with , the distance is bounded above by and is bounded below by . In symbols, we have Inequality (1) is still true for any other , so substituting such an then multiplying through by , we obtain Rearranging (2) so that its inequality signs face the same direction as (1), we can add (1) and (2) to obtain Therefore, is Cauchy in , and so converges. (qed)","\{x_n\}, \{y_n\} (X, d) \{a_n\} a_n = d(x_n, y_n) \mathbb{R} \varepsilon >0 \{x_n\} N \in \mathbb{N} d(x_n, x_m) < \varepsilon/8 n, m \geq N M \in \mathbb{N} \{y_n\} \max\{N, M\} \max\{N, M\} = N n \geq N x_n \in B_{\varepsilon/4}(x_N) y_n \in B_{\varepsilon/4}(y_N) D = \{d(x,y) : x \in B_{\varepsilon/4}(x_N) \text{ and } y \in B_{\varepsilon/4}(y_N)\}. x_n, y_n n > N d(x_n, y_n) \sup D = d(x_N, y_N) + \varepsilon/2 \inf D = d(x_N, y_N) - \varepsilon/2 \begin{align} d(x_N, y_N) - \varepsilon/2 &< d(x_n, y_n) < d(x_N, y_N) + \varepsilon/2. \tag{1} \end{align} m > N m -1 \begin{align} -d(x_N, y_N) + \varepsilon/2 &> -d(x_m, y_m) > -d(x_N, y_N) - \varepsilon/2. \tag{2} \end{align} \begin{align} -\varepsilon < d(x_n,y_n) - d(x_m, y_m) &< \varepsilon, \text{ i. e.} \\ |d(x_n, y_n) - d(x_m, y_m)| &< \varepsilon, \text{ meaning that} \\ |a_n - a_m| &< \varepsilon. \end{align} \{a_n\} \mathbb{R} \{a_n\}","['real-analysis', 'analysis', 'inequality', 'metric-spaces']"
6,Prove function $g$ has a continuous expansion $f$ on $\mathbb{R}^2 \rightarrow \mathbb{R}$,Prove function  has a continuous expansion  on,g f \mathbb{R}^2 \rightarrow \mathbb{R},"I'm trying to expand the function $$g(x,y) = \arctan \left(\frac{1-xy}{x^2+y^2}\right)$$ to some function $f \colon \mathbb R^2 \rightarrow \mathbb R$ . We know already it's continuous on $\mathbb R^2 \backslash \{(0,0)\}$ , so to find a proper candidate for the value $f(0,0)$ , we can check any approaching limit. For example $$\lim_{x\to 0} f(x, 0)= \lim_{x\to 0} \arctan \frac{1}{x^2} = \frac{\pi}{2}$$ So if $f$ is continuous in $(0,0)$ , the value has to be equal to $\frac{\pi}{2}$ . The problem I'm having is how to now prove if $f$ actually is continuous. So for $\forall \epsilon > 0$ , there has to $\exists \delta > 0$ , so that $|(x,y) - (0,0)| < \delta \implies |f(x,y) - f(0,0)| < \epsilon$ . When I try to look at $$\left| \arctan \frac{1-xy}{x^2+y^2} - \frac{\pi}{2}\right|$$ I then kind of become stuck, trying to see what the value is less as. I hope my question is somewhat clear. I'm just searching for the next step in proving continuity if anybody can offer some help. Thank you all in advance. :)","I'm trying to expand the function to some function . We know already it's continuous on , so to find a proper candidate for the value , we can check any approaching limit. For example So if is continuous in , the value has to be equal to . The problem I'm having is how to now prove if actually is continuous. So for , there has to , so that . When I try to look at I then kind of become stuck, trying to see what the value is less as. I hope my question is somewhat clear. I'm just searching for the next step in proving continuity if anybody can offer some help. Thank you all in advance. :)","g(x,y) = \arctan \left(\frac{1-xy}{x^2+y^2}\right) f \colon \mathbb R^2 \rightarrow \mathbb R \mathbb R^2 \backslash \{(0,0)\} f(0,0) \lim_{x\to 0} f(x, 0)= \lim_{x\to 0} \arctan \frac{1}{x^2} = \frac{\pi}{2} f (0,0) \frac{\pi}{2} f \forall \epsilon > 0 \exists \delta > 0 |(x,y) - (0,0)| < \delta \implies |f(x,y) - f(0,0)| < \epsilon \left| \arctan \frac{1-xy}{x^2+y^2} - \frac{\pi}{2}\right|","['real-analysis', 'analysis', 'trigonometry', 'continuity']"
7,Is convergence in $\ell^p$ spaces equivalent to coordinate-wise convergence?,Is convergence in  spaces equivalent to coordinate-wise convergence?,\ell^p,"I'm a little rusty on this so I'm sorry if this has an obvious answer. My question is the following: let $\ell^p$ be the normed vector space of sequences $x = (x_n)_{n \in \mathbb{N}}$ such that $$ \|x\|_{p}\doteq\left(\sum_{n}\left|x_{n}\right|^{p}\right)^{1 / p} < \infty $$ Is it true that a sequence $(y_n)_{n \in \mathbb{N}}$ of elements of $\ell^p$ (i.e a sequence of sequences, with $y_1 = (y^1_{1}, y^1_2, \cdots), y_2 = (y^2_1, y^2_2, \cdots), \cdots$ ) converges to some $y = (y_1, \cdots) \in \ell^p$ if, and only if, $$\lim_{n \to \infty}y^n_i = y_ i$$ for every $i \in \mathbb{N}$ ? Does either implication hold? I know this is true if the norm topology coincides with the product topology on $\mathbb{R}^{\mathbb{N}}$ , but it's clear to me if that's true either. I'd appreciate any help! Thanks in advance.","I'm a little rusty on this so I'm sorry if this has an obvious answer. My question is the following: let be the normed vector space of sequences such that Is it true that a sequence of elements of (i.e a sequence of sequences, with ) converges to some if, and only if, for every ? Does either implication hold? I know this is true if the norm topology coincides with the product topology on , but it's clear to me if that's true either. I'd appreciate any help! Thanks in advance.","\ell^p x = (x_n)_{n \in \mathbb{N}} 
\|x\|_{p}\doteq\left(\sum_{n}\left|x_{n}\right|^{p}\right)^{1 / p} < \infty
 (y_n)_{n \in \mathbb{N}} \ell^p y_1 = (y^1_{1}, y^1_2, \cdots), y_2 = (y^2_1, y^2_2, \cdots), \cdots y = (y_1, \cdots) \in \ell^p \lim_{n \to \infty}y^n_i = y_ i i \in \mathbb{N} \mathbb{R}^{\mathbb{N}}","['real-analysis', 'general-topology', 'functional-analysis', 'analysis', 'topological-vector-spaces']"
8,why $2^{-n}h_n(x)=t_n(x)$ except in $V_n- K_n?$ Rudin RCA- Lusin's theorem,why  except in  Rudin RCA- Lusin's theorem,2^{-n}h_n(x)=t_n(x) V_n- K_n?,"I have some  confusion regarding Rudin RCA book, page number $55$ Lusin's  theorem  : Suppose $f$ is a complex   measurable  function on $X$ . $\mu(A) < \infty$ , $f(x)= 0$ if $x \notin A $ and $\epsilon >0$ .The  there  exists a $g \in C_c(X)$ such that $ \mu(\{x : f(x) \neq g(x)\} <\epsilon$ In the theorem of the proof it  is written that By Urysohn's lemma ,there are function $h_n$ such that $K_n\prec h_n \prec V_n.$ Define $$g(x) =\sum_{n=1}^{\infty}2^{-n}h_n(x) $$ for all $ x \in X$ This series converge uniformly on X , so $g$ is contnious.Also ,the support of $g$ lie in $\bar V$ . since $2^{-n}h_n(x)=t_n(x)$ except in $V_n- K_n$ My confusion : why $2^{-n}h_n(x)=t_n(x)$ except in $V_n- K_n?$ why not $2^{-n}h_n(x)=t_n(x)$ except in $X- V_n?$ My thinking : $h_n(x)= 2^nt_n(x)=\begin{cases} 1 \ \text{if  x} \in V_n \\ 0 \ \text{if  x} \notin V_n  \end{cases} = \begin{cases} 1 \ \text{if  x} \in V_n \\ 0 \ \text{if  x} \in  X- V_n  \end{cases}$ So  I think it should be $2^{-n}h_n(x)=t_n(x)$ except in $X- V_n$","I have some  confusion regarding Rudin RCA book, page number Lusin's  theorem  : Suppose is a complex   measurable  function on . , if and .The  there  exists a such that In the theorem of the proof it  is written that By Urysohn's lemma ,there are function such that Define for all This series converge uniformly on X , so is contnious.Also ,the support of lie in . since except in My confusion : why except in why not except in My thinking : So  I think it should be except in",55 f X \mu(A) < \infty f(x)= 0 x \notin A  \epsilon >0 g \in C_c(X)  \mu(\{x : f(x) \neq g(x)\} <\epsilon h_n K_n\prec h_n \prec V_n. g(x) =\sum_{n=1}^{\infty}2^{-n}h_n(x)   x \in X g g \bar V 2^{-n}h_n(x)=t_n(x) V_n- K_n 2^{-n}h_n(x)=t_n(x) V_n- K_n? 2^{-n}h_n(x)=t_n(x) X- V_n? h_n(x)= 2^nt_n(x)=\begin{cases} 1 \ \text{if  x} \in V_n \\ 0 \ \text{if  x} \notin V_n  \end{cases} = \begin{cases} 1 \ \text{if  x} \in V_n \\ 0 \ \text{if  x} \in  X- V_n  \end{cases} 2^{-n}h_n(x)=t_n(x) X- V_n,"['analysis', 'measure-theory', 'lebesgue-measure', 'measurable-functions', 'measurable-sets']"
9,A question on differentiability of a variant of Thomae function,A question on differentiability of a variant of Thomae function,,"Let $$f(x) = \begin{cases} \dfrac{1}{q^2},  & \text{if $x=\dfrac{p}{q} $ is rational and in lowest terms;} \\[2ex] 0, & \text{if $x$ is irrational} \end{cases}$$ Where is f continuous? Is f differentiable anywhere? My attempt: I can prove that $\lim_{x \to c}f(x)=0$ for any point real value c. for each positive number $\epsilon$ , the set $(c-1,c) \cup(c,c+1)$ contains only finitely many rational numbers p/q with $1/q^2 \geq \epsilon$ (namely , $q^2 \leq 1/\epsilon$ ). Then I let $\delta$ be the distance between $c$ and the closest such rational number. Then for all x satisfying $0<|x-c|<\delta$ , we have $0\leq f(x)=f(p/q)=1/q^2 \leq \epsilon$ . Thus, I prove that $\lim_{x \to c}f(x)=0$ for any point real value c. So this means f(x) is continuous at every irrational number, and discontinuous at every rational number. For differentiability, I only know f(x) is not differentiable at every rational number, because its discontinuous there. But how to prove f(x) also not differentiable at every irrational number? Hint: For any irrational number c, there are infinitely many fractions p/q in lowest terms such that $|c-p/q|<1/q^2$ . But I don't know this hint is is used in the continuous part ot differentiable part, since I already proved the continuous part.","Let Where is f continuous? Is f differentiable anywhere? My attempt: I can prove that for any point real value c. for each positive number , the set contains only finitely many rational numbers p/q with (namely , ). Then I let be the distance between and the closest such rational number. Then for all x satisfying , we have . Thus, I prove that for any point real value c. So this means f(x) is continuous at every irrational number, and discontinuous at every rational number. For differentiability, I only know f(x) is not differentiable at every rational number, because its discontinuous there. But how to prove f(x) also not differentiable at every irrational number? Hint: For any irrational number c, there are infinitely many fractions p/q in lowest terms such that . But I don't know this hint is is used in the continuous part ot differentiable part, since I already proved the continuous part.","f(x) =
\begin{cases}
\dfrac{1}{q^2},  & \text{if x=\dfrac{p}{q}  is rational and in lowest terms;} \\[2ex]
0, & \text{if x is irrational}
\end{cases} \lim_{x \to c}f(x)=0 \epsilon (c-1,c) \cup(c,c+1) 1/q^2 \geq \epsilon q^2 \leq 1/\epsilon \delta c 0<|x-c|<\delta 0\leq f(x)=f(p/q)=1/q^2 \leq \epsilon \lim_{x \to c}f(x)=0 |c-p/q|<1/q^2",['analysis']
10,$\int_0^\pi f(x)^2dx = \int_0^\pi f'(x)^2dx$?,?,\int_0^\pi f(x)^2dx = \int_0^\pi f'(x)^2dx,"Let $f \in C_{ℝ}([0, \pi])$ satisfy $f(0) = f(\pi) = 0$ and $f' \in L^2([0, \pi])$ . It can be shown (using Parseval's Theorem and the fact that for the Fourier coefficients we have $c^{f'}_n = inc^f_n$ ), that $$\int_0^π f(x)²dx ≤ \int_0^π f'(x)²dx.$$ (Ask me for a proof.) Now I'm asked to determine when actually equality holds, with the hint: ""extend $f$ to an odd function on $[-π, π]$ "". Without that hint, I arrive at some answer, namely those functions $2ic · \sin(x)$ for any $c ∈ ℝ$ . However I never used the hint, really.. So that throws me off. The solutions I have seem to satisfy, but have I missed others? (In terms of that hint, I do know that for odd functions $f$ , the Fourier coefficients satisfy $c^f_{-n} = -c^f_n$ , which will probably come into play at some point.)","Let satisfy and . It can be shown (using Parseval's Theorem and the fact that for the Fourier coefficients we have ), that (Ask me for a proof.) Now I'm asked to determine when actually equality holds, with the hint: ""extend to an odd function on "". Without that hint, I arrive at some answer, namely those functions for any . However I never used the hint, really.. So that throws me off. The solutions I have seem to satisfy, but have I missed others? (In terms of that hint, I do know that for odd functions , the Fourier coefficients satisfy , which will probably come into play at some point.)","f \in C_{ℝ}([0, \pi]) f(0) = f(\pi) = 0 f' \in L^2([0, \pi]) c^{f'}_n = inc^f_n \int_0^π f(x)²dx ≤ \int_0^π f'(x)²dx. f [-π, π] 2ic · \sin(x) c ∈ ℝ f c^f_{-n} = -c^f_n","['analysis', 'fourier-analysis']"
11,"Find surjective, continuous function such that diagram commutes [closed]","Find surjective, continuous function such that diagram commutes [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If we have a continuous function $g\colon X\to X$ with $g(Y)\subset Y$ and $h\colon Y\to Y$ , where $h$ is the restriction of $g$ to $Y\subset X$ , is there a continuous, surjective function $\pi\colon X\to Y$ such that $$ \pi\circ g = h\circ\pi? $$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If we have a continuous function with and , where is the restriction of to , is there a continuous, surjective function such that","g\colon X\to X g(Y)\subset Y h\colon Y\to Y h g Y\subset X \pi\colon X\to Y 
\pi\circ g = h\circ\pi?
",['analysis']
12,"$C[0,1]$ is separable: Theorem $11.2$ Carothers' Real Analysis",is separable: Theorem  Carothers' Real Analysis,"C[0,1] 11.2","I have some questions about the proof of Theorem 11.2 of Carothers' Real Analysis. The theorem states that $C[0,1]$ is separable. Why is $\|f-g\|_\infty \le\epsilon$ ? Why is $h$ selected the way it is, i.e. $h(k/n) = f(k/n)$ for $k = 0,1,2...,n$ but with $h(k/n)$ rational and satisfying $|h(k/n) - g(k/n)| < \epsilon$ ? I think the reason we can do this is because $\mathbb Q$ is dense in $\mathbb R$ , but I don't understand the motivation. How do we get $\|g-h\|_\infty < \epsilon$ ? Is the proof similar to (1)? The proof is concluded with: ""The set of all polygonal functions taking only rational values at nodes $(k/n)_{k=0}^n$ for some $n$ is countable."" Well, why is that - and how does this show that $C[0,1]$ is separable? To show that $C[0,1]$ is separable, I would expect a countable dense subset of $C[0,1]$ . Where is this set? In case someone is curious as to what Exercise $1$ is, I'm typing it below: For each $n$ , let $Q_n$ be the set of all polygonal functions that have nodes at $x = k/n$ , $k = 0,1,...,n$ and that take on only rational values at these points. Check that $Q_n$ is a countable set and hence that the union of the $Q_n$ 's is a countable dense set in $C[0,1]$ .","I have some questions about the proof of Theorem 11.2 of Carothers' Real Analysis. The theorem states that is separable. Why is ? Why is selected the way it is, i.e. for but with rational and satisfying ? I think the reason we can do this is because is dense in , but I don't understand the motivation. How do we get ? Is the proof similar to (1)? The proof is concluded with: ""The set of all polygonal functions taking only rational values at nodes for some is countable."" Well, why is that - and how does this show that is separable? To show that is separable, I would expect a countable dense subset of . Where is this set? In case someone is curious as to what Exercise is, I'm typing it below: For each , let be the set of all polygonal functions that have nodes at , and that take on only rational values at these points. Check that is a countable set and hence that the union of the 's is a countable dense set in .","C[0,1] \|f-g\|_\infty \le\epsilon h h(k/n) = f(k/n) k = 0,1,2...,n h(k/n) |h(k/n) - g(k/n)| < \epsilon \mathbb Q \mathbb R \|g-h\|_\infty < \epsilon (k/n)_{k=0}^n n C[0,1] C[0,1] C[0,1] 1 n Q_n x = k/n k = 0,1,...,n Q_n Q_n C[0,1]","['real-analysis', 'analysis', 'proof-explanation', 'separable-spaces']"
13,"Test whether the given sequence of function is uniform convergence in $(0,\infty)$",Test whether the given sequence of function is uniform convergence in,"(0,\infty)","Check the uniform convergence of the sequence of functions $f_n(x)=n\ln \left(\frac{1+nx}{nx}\right)$ on $(0,\infty)$ . I have found that the pointwise limit of the sequence of functions is $f(x)=\frac 1x$ . I'm stuck proving uniform convergence. I am unable to find $\displaystyle M_n=\sup_{x\in (0,\infty)}\left|n\ln \left(\frac{1+nx}{nx}\right)-\frac 1x\right|$ . Any hints, please?","Check the uniform convergence of the sequence of functions on . I have found that the pointwise limit of the sequence of functions is . I'm stuck proving uniform convergence. I am unable to find . Any hints, please?","f_n(x)=n\ln \left(\frac{1+nx}{nx}\right) (0,\infty) f(x)=\frac 1x \displaystyle M_n=\sup_{x\in (0,\infty)}\left|n\ln \left(\frac{1+nx}{nx}\right)-\frac 1x\right|","['real-analysis', 'sequences-and-series', 'analysis', 'uniform-convergence', 'sequence-of-function']"
14,Prerequisites for non-standard analysis,Prerequisites for non-standard analysis,,"Today my real analysis teacher mentioned the existence of ""non-standard analysis"" and explained on a very basic level what it studies and it immediately caught my attention. After that he mentioned that it is an advanced subject and that got my wondering: what are the necessary prerequisites to be able to learn non-standard analysis up to an intermediate/advanced level? Is real analysis and a little bit of abstract algebra enough?","Today my real analysis teacher mentioned the existence of ""non-standard analysis"" and explained on a very basic level what it studies and it immediately caught my attention. After that he mentioned that it is an advanced subject and that got my wondering: what are the necessary prerequisites to be able to learn non-standard analysis up to an intermediate/advanced level? Is real analysis and a little bit of abstract algebra enough?",,"['analysis', 'nonstandard-analysis']"
15,Prove $\frac{\sin x}{x^2}$ is uniformly continuous at $N(0;r)^c$ for any $r >0$,Prove  is uniformly continuous at  for any,\frac{\sin x}{x^2} N(0;r)^c r >0,"I tried $\left \vert \frac{\sin x}{x^2} -  \frac{\sin c}{c^2}\right \vert \leq \frac{1}{x^2} + \frac{1}{c^2} < \epsilon$ , but it doesn't help me much with $\vert x - c \vert < \delta$ . How can I prove this?","I tried , but it doesn't help me much with . How can I prove this?",\left \vert \frac{\sin x}{x^2} -  \frac{\sin c}{c^2}\right \vert \leq \frac{1}{x^2} + \frac{1}{c^2} < \epsilon \vert x - c \vert < \delta,"['analysis', 'uniform-continuity']"
16,"$L^p$ convergence of certain ""average"" function","convergence of certain ""average"" function",L^p,"Given a function $f \in L^2([0,1])$ . Let's fix some irrational number $\omega$ . For any $N \in \mathbb{Z}^{+}$ , let's define a function $g_{N}$ as follows: $$g_{N}(x) := \frac{1}{N}\sum_{j=1}^{N}f(x+j\omega)-\int_{0}^{1}f(y)dy$$ Prove that $\lim_{N \rightarrow \infty}||g_{N}||_{L^2(\mathbb{T})} = 0$ . Moreover, if we further assume $f$ is periodic with period $\omega$ , then $f$ must be constant. I'm interested in how the first claim can be proved. How does the ""irrational number"" assumption come into play here? Any hint/discussion would be appreciated!","Given a function . Let's fix some irrational number . For any , let's define a function as follows: Prove that . Moreover, if we further assume is periodic with period , then must be constant. I'm interested in how the first claim can be proved. How does the ""irrational number"" assumption come into play here? Any hint/discussion would be appreciated!","f \in L^2([0,1]) \omega N \in \mathbb{Z}^{+} g_{N} g_{N}(x) := \frac{1}{N}\sum_{j=1}^{N}f(x+j\omega)-\int_{0}^{1}f(y)dy \lim_{N \rightarrow \infty}||g_{N}||_{L^2(\mathbb{T})} = 0 f \omega f","['real-analysis', 'analysis', 'fourier-analysis', 'harmonic-analysis', 'ergodic-theory']"
17,Hardy-Littlewood maximal function of $\log |x|$.,Hardy-Littlewood maximal function of .,\log |x|,"Consider the function $f: \Bbb R^n \to \Bbb R$ defined by $f(x)=\log |x|$ for $x \neq 0$ and $f(0)=0$ . I'm trying to prove that the Hardy-Littlewood maximal function of $f$ , $Mf$ , equals $\infty$ . That is, for every $x \in \Bbb R^n$ , $$Mf(x) = \sup_{x \in B} \frac{1}{|B|} \int_B |f|  = \infty,$$ where the supremum is taken over all balls $B$ in $\Bbb R^n$ containing $x$ and $|B|$ is the Lebesgue measure of the ball. The reasoning I tried to use is that for any $x \in \Bbb R^n$ there is a ball $B$ containing $x$ and $0$ . Since it contains $0$ , $|f| \to \infty$ on $B$ , and then $\frac{1}{|B|} \int_B |f|  = \infty$ . But I don't know if that's true, do we need that $|f|$ is exactly $\infty$ on $B$ to say that the integral on $B$ is $\infty$ ? In that case, would we need to define $f(0)=\infty$ ?","Consider the function defined by for and . I'm trying to prove that the Hardy-Littlewood maximal function of , , equals . That is, for every , where the supremum is taken over all balls in containing and is the Lebesgue measure of the ball. The reasoning I tried to use is that for any there is a ball containing and . Since it contains , on , and then . But I don't know if that's true, do we need that is exactly on to say that the integral on is ? In that case, would we need to define ?","f: \Bbb R^n \to \Bbb R f(x)=\log |x| x \neq 0 f(0)=0 f Mf \infty x \in \Bbb R^n Mf(x) = \sup_{x \in B} \frac{1}{|B|} \int_B |f|  = \infty, B \Bbb R^n x |B| x \in \Bbb R^n B x 0 0 |f| \to \infty B \frac{1}{|B|} \int_B |f|  = \infty |f| \infty B B \infty f(0)=\infty","['real-analysis', 'analysis']"
18,"Let $K$ be a nonempty closed, convex subset of $R^d$. Prove that if $x\notin K$, then there exists a unique point $y\in K$ that is closest to $x$.","Let  be a nonempty closed, convex subset of . Prove that if , then there exists a unique point  that is closest to .",K R^d x\notin K y\in K x,"Let $K$ be a nonempty closed, convex subset of $R^d$ . Prove that if $x\notin K$ , then there exists a unique point $y\in K$ that is closest to $x$ . My attempt: Suppose y and z are 2 different point that are both closest to x. Then, $||x-y||=\mbox{inf}\{||x-a||: a \in S\}$ and $||x-z||=\mbox{inf}\{||x-b||: b \in S\}$ . Consider the midpoint $w$ of the line that joins y to z. Then $w=\frac{||y-z||}{2}=\frac{||y-x+x-z||}{2}\le \frac{||x-y||+||x-z||}{2}\le \frac{||x-a||+||x-b||}{2}$ for any $a \in S$ . I don't know how to go from here; I thought the midpoint would be zero since we are trying to show that these 2 points are the same point, so is the midpoint formula I used incorrect? Also can you use $a$ for both infimums or do I have to use $a$ and $b$ distinctly for y and z as I did?","Let be a nonempty closed, convex subset of . Prove that if , then there exists a unique point that is closest to . My attempt: Suppose y and z are 2 different point that are both closest to x. Then, and . Consider the midpoint of the line that joins y to z. Then for any . I don't know how to go from here; I thought the midpoint would be zero since we are trying to show that these 2 points are the same point, so is the midpoint formula I used incorrect? Also can you use for both infimums or do I have to use and distinctly for y and z as I did?",K R^d x\notin K y\in K x ||x-y||=\mbox{inf}\{||x-a||: a \in S\} ||x-z||=\mbox{inf}\{||x-b||: b \in S\} w w=\frac{||y-z||}{2}=\frac{||y-x+x-z||}{2}\le \frac{||x-y||+||x-z||}{2}\le \frac{||x-a||+||x-b||}{2} a \in S a a b,"['real-analysis', 'analysis', 'supremum-and-infimum']"
19,Are Rational Power of e is transcendental?,Are Rational Power of e is transcendental?,,It is well known to all of us that the rational powers of $e$ are irrational numbers. Many of the proofs proving this use a similar approach as proving $e$ irrational using Niven's Polynomials. Is it true that rational powers of $e$ are also transcendental numbers using proofs similar(as proved by Hermite) for proving $e$ transcendental? How to measure the irrationality measure of those rational powers of $e$ ?,It is well known to all of us that the rational powers of are irrational numbers. Many of the proofs proving this use a similar approach as proving irrational using Niven's Polynomials. Is it true that rational powers of are also transcendental numbers using proofs similar(as proved by Hermite) for proving transcendental? How to measure the irrationality measure of those rational powers of ?,e e e e e,"['analysis', 'irrational-numbers', 'rational-numbers', 'transcendental-numbers', 'irrationality-measure']"
20,Continuity problem - bounding $|f(x)|\leq|x|$,Continuity problem - bounding,|f(x)|\leq|x|,"Let $f$ be a function such that $|f(x)|\leq|x|$ for all $x\in\mathbb{R}$ . Prove that $f$ is continuous at the origin. So far, I have that in order for $f$ to be continuous at the origin we need a $\delta$ such that $|x|<\delta\Rightarrow |f(x)|<\epsilon$ , by the definition of continuity. Surely we can simply take $\delta=\epsilon$ in this case and the claimed result follows immediately?","Let be a function such that for all . Prove that is continuous at the origin. So far, I have that in order for to be continuous at the origin we need a such that , by the definition of continuity. Surely we can simply take in this case and the claimed result follows immediately?",f |f(x)|\leq|x| x\in\mathbb{R} f f \delta |x|<\delta\Rightarrow |f(x)|<\epsilon \delta=\epsilon,"['real-analysis', 'calculus']"
21,"Finding $f$ such that $\forall x,y \in \mathbb{R},f(x+y)f(x-y)=(f(x))^2$",Finding  such that,"f \forall x,y \in \mathbb{R},f(x+y)f(x-y)=(f(x))^2","Find all the possible continuous functions $f : \mathbb{R} \to \mathbb{C}$ such that $|f|=1$ and $$\forall x,y \in \mathbb{R},f(x+y)f(x-y)=(f(x))^2.$$ By taking suitable values for $x,y$ we have $f(0)=1,f(2x)=(f(x))^2,$ also $x \to e^{icx},c \in \mathbb{R}$ verifies the above functional equation. Tried to find $f(nx)$ in term of $f(x),$ but unfortunately didn't find anything. Any suggestions?",Find all the possible continuous functions such that and By taking suitable values for we have also verifies the above functional equation. Tried to find in term of but unfortunately didn't find anything. Any suggestions?,"f : \mathbb{R} \to \mathbb{C} |f|=1 \forall x,y \in \mathbb{R},f(x+y)f(x-y)=(f(x))^2. x,y f(0)=1,f(2x)=(f(x))^2, x \to e^{icx},c \in \mathbb{R} f(nx) f(x),","['real-analysis', 'analysis', 'functional-equations']"
22,Showing a set is dense in $l^1$,Showing a set is dense in,l^1,"I have been studying functional analysis and I came across this question: Show that $$F := \{(s_n)\in l^1 : (ns_n)\notin l^{\infty}\}$$ is dense in $l_1$ . So here is my approach and I would like some feedback if possible: $(e_n) \in l^1$ (a sequence with unit value in the nth position) is certainly part of this set as $(ne_n)$ is unbounded. We know already that the span of $(e_n)$ is dense in $l^1$ . Every other sequence in F can be represented by the span of $(e_n)$ . Hence, $(e_n)$ is dense in F itself. Hence, F is dense in $l^1$ . Is this correct? I am sorry that I can be very vague in structuring my arguments (I am new to mathematics) and was wondering if anyone could help me with structuring this better or correct me if I am wrong somewhere.","I have been studying functional analysis and I came across this question: Show that is dense in . So here is my approach and I would like some feedback if possible: (a sequence with unit value in the nth position) is certainly part of this set as is unbounded. We know already that the span of is dense in . Every other sequence in F can be represented by the span of . Hence, is dense in F itself. Hence, F is dense in . Is this correct? I am sorry that I can be very vague in structuring my arguments (I am new to mathematics) and was wondering if anyone could help me with structuring this better or correct me if I am wrong somewhere.",F := \{(s_n)\in l^1 : (ns_n)\notin l^{\infty}\} l_1 (e_n) \in l^1 (ne_n) (e_n) l^1 (e_n) (e_n) l^1,"['real-analysis', 'functional-analysis', 'analysis']"
23,Bounded operator such that image of ball is $(1-\varepsilon)$-dense is surjective.,Bounded operator such that image of ball is -dense is surjective.,(1-\varepsilon),"This is not a homework problem, I found it in some notes and I am curious on how to prove it. The statement is as follows: Let $T:X\to Y$ be a bounded linear operator between Banach spaces. If there is $\varepsilon>0$ and $R<\infty$ such that $T(B_X(R))$ is $(1-\varepsilon)$ -dense in $B_Y(1)$ then $T$ is surjective. Here $(1-\varepsilon)$ -dense means that for every point in $B_Y(1)$ the ball of radius $(1-\varepsilon)$ intersects $T(B_X(R))$ . What I've thought so far: It is enough to show that $T(B_X(R))$ contains some ball around $0\in Y$ . Assume that this is not the case so that for every $n$ the ball $B_Y(\frac{1}{n})$ is not contained in $T(B_X(R))$ . By the $(1-\varepsilon)$ -density we have that for any $y\in B_Y({\frac{1}{n}})\setminus T(B_X(R))$ there is some $z\in T(B_X(R))$ such that $d(y,z)<(1-\varepsilon)$ . Furthermore, we have $d(0,z)\le d(0,y)+d(y,z)<\frac{1}{n}+(1-\varepsilon)$ . If we let $n$ be large enough so that $(1-\varepsilon)+\frac{1}{n}<1$ then $z\in B_Y(1)$ . I don't know if this works or makes any sense and I don't know how to continue from here. Any help, hints or solutions would be appreciated.","This is not a homework problem, I found it in some notes and I am curious on how to prove it. The statement is as follows: Let be a bounded linear operator between Banach spaces. If there is and such that is -dense in then is surjective. Here -dense means that for every point in the ball of radius intersects . What I've thought so far: It is enough to show that contains some ball around . Assume that this is not the case so that for every the ball is not contained in . By the -density we have that for any there is some such that . Furthermore, we have . If we let be large enough so that then . I don't know if this works or makes any sense and I don't know how to continue from here. Any help, hints or solutions would be appreciated.","T:X\to Y \varepsilon>0 R<\infty T(B_X(R)) (1-\varepsilon) B_Y(1) T (1-\varepsilon) B_Y(1) (1-\varepsilon) T(B_X(R)) T(B_X(R)) 0\in Y n B_Y(\frac{1}{n}) T(B_X(R)) (1-\varepsilon) y\in B_Y({\frac{1}{n}})\setminus T(B_X(R)) z\in T(B_X(R)) d(y,z)<(1-\varepsilon) d(0,z)\le d(0,y)+d(y,z)<\frac{1}{n}+(1-\varepsilon) n (1-\varepsilon)+\frac{1}{n}<1 z\in B_Y(1)","['functional-analysis', 'analysis']"
24,Question about the notation $\{x \in \mathbb{Z}: p(x)\}$,Question about the notation,\{x \in \mathbb{Z}: p(x)\},"A book I'm currently working through has the following exercise. $1.5$ Write each of the following sets in the form $\{x \in \mathbb{Z} : p(x)\}$ , where $p(x)$ is a property concerning $x$ . $$(a)\ A = \{-1, -2, -3,...\}\\ (b)\ B = \{-3, -2, ..., 3\}\\ (c)\ C = \{-2, -1, 1, 2\} $$ I'm not exactly sure what the $p(x)$ is asserting here, do they want me to come up with a transformation for each $x$ by means of some function? The only way I can even conceive of answering these would be something like the following $(a)\ A = \{x\in \mathbb{Z}: x< 0\}$ But this isn't some transformation $p(x)$ . I don't know a function that is not piecewise who will take some $x$ and transform it to $-x$ but leaves $x$ positive if it's positive? $(b)\ B = \{x\in \mathbb{Z} : |x|\leq3\}$ $(c)\ C = \{x\in \mathbb{Z} : 0 \lt|x|\leq2\}$ Again neither of my solutions $(c)$ or $(b)$ are of the form $p(x)$ I'm just giving some hard rules on what x has to be here. If anyone has any ideas on how I can approach these differently I would appreciate it. Regrettably, my book comes with exactly zero solutions.","A book I'm currently working through has the following exercise. Write each of the following sets in the form , where is a property concerning . I'm not exactly sure what the is asserting here, do they want me to come up with a transformation for each by means of some function? The only way I can even conceive of answering these would be something like the following But this isn't some transformation . I don't know a function that is not piecewise who will take some and transform it to but leaves positive if it's positive? Again neither of my solutions or are of the form I'm just giving some hard rules on what x has to be here. If anyone has any ideas on how I can approach these differently I would appreciate it. Regrettably, my book comes with exactly zero solutions.","1.5 \{x \in \mathbb{Z} : p(x)\} p(x) x (a)\ A = \{-1, -2, -3,...\}\\
(b)\ B = \{-3, -2, ..., 3\}\\
(c)\ C = \{-2, -1, 1, 2\}
 p(x) x (a)\ A = \{x\in \mathbb{Z}: x< 0\} p(x) x -x x (b)\ B = \{x\in \mathbb{Z} : |x|\leq3\} (c)\ C = \{x\in \mathbb{Z} : 0 \lt|x|\leq2\} (c) (b) p(x)","['analysis', 'elementary-set-theory', 'solution-verification']"
25,Metric space with a constraint,Metric space with a constraint,,"For $\mathbb{R}^n$ with $n \ge 2,$ define $$d_{p,q}:= \left( \sum\limits_{i =1}^n |x_i - y_i|^p \right)^q$$ for $p,q \in \mathbb{R}.$ Prove that $ d_{p,q}  \text{ is a metric on } \mathbb{R}^n \iff 0 < q, \ pq \le 1.$ I am used to proving or disproving a metric space by validating all the four axioms of finiteness, definiteness, symmetry and triangle inequality without any constraints on the metric. I came across this exercise and I thought of sharing it here to generate ideas on how best to tackle it. Clearly, if $0 < q, pq \le 1$ then $d_{p,q}(x,y)$ is a metric space since all the four axioms are easily met. How do we prove the other direction that is if $d_{p,q} (x,y)$ is a metric we show that $0 <q, pq\le q$ ? We need to fulfill the two directions of the $""\iff""$ . Benevolent contributors I rely on you for your support.","For with define for Prove that I am used to proving or disproving a metric space by validating all the four axioms of finiteness, definiteness, symmetry and triangle inequality without any constraints on the metric. I came across this exercise and I thought of sharing it here to generate ideas on how best to tackle it. Clearly, if then is a metric space since all the four axioms are easily met. How do we prove the other direction that is if is a metric we show that ? We need to fulfill the two directions of the . Benevolent contributors I rely on you for your support.","\mathbb{R}^n n \ge 2, d_{p,q}:= \left( \sum\limits_{i =1}^n |x_i - y_i|^p \right)^q p,q \in \mathbb{R}.  d_{p,q}  \text{ is a metric on } \mathbb{R}^n \iff 0 < q, \ pq \le 1. 0 < q, pq \le 1 d_{p,q}(x,y) d_{p,q} (x,y) 0 <q, pq\le q ""\iff""","['real-analysis', 'functional-analysis', 'analysis', 'vector-spaces', 'metric-spaces']"
26,On a tempered distribution which comes from a locally integrable function,On a tempered distribution which comes from a locally integrable function,,"Let $ f\colon \mathbb{R}^n \to \mathbb{C} $ be a measurable function. We say that $ f $ is locally integrable if $ f|_K $ is integrable for all compact subsets $ K \subseteq \mathbb{R}^n $ . A locally integrable function $ f $ yields a distribution $ T_f $ , which is given by $$ \langle T_f, \phi \rangle = \int_{\mathbb{R}^n} f(x) \phi(x) \,dx $$ for $ \phi \in \mathscr{D}(\mathbb{R}^n) $ (test functions). Question. Suppose that $ T_f $ is tempered as a distribution. Then $ T_f $ is extended so that $ \langle T_f, \phi \rangle $ is defined for all $ \phi \in \mathscr{S}(\mathbb{R}^n) $ (rapidly decreasing functions). Is $ f \phi $ integrable for all $ \phi \in \mathscr{S}(\mathbb{R}^n) $ ? For $ \phi \in \mathscr{S}(\mathbb{R}^n) $ with $ f \phi $ integrable, is it true that $ \langle T_f, \phi \rangle = \int_{\mathbb{R}^n} f(x) \phi(x) \,dx $ ?","Let be a measurable function. We say that is locally integrable if is integrable for all compact subsets . A locally integrable function yields a distribution , which is given by for (test functions). Question. Suppose that is tempered as a distribution. Then is extended so that is defined for all (rapidly decreasing functions). Is integrable for all ? For with integrable, is it true that ?"," f\colon \mathbb{R}^n \to \mathbb{C}   f   f|_K   K \subseteq \mathbb{R}^n   f   T_f  
\langle T_f, \phi \rangle = \int_{\mathbb{R}^n} f(x) \phi(x) \,dx
  \phi \in \mathscr{D}(\mathbb{R}^n)   T_f   T_f   \langle T_f, \phi \rangle   \phi \in \mathscr{S}(\mathbb{R}^n)   f \phi   \phi \in \mathscr{S}(\mathbb{R}^n)   \phi \in \mathscr{S}(\mathbb{R}^n)   f \phi   \langle T_f, \phi \rangle = \int_{\mathbb{R}^n} f(x) \phi(x) \,dx ","['integration', 'analysis', 'distribution-theory']"
27,When can nets be replaced by sequences?,When can nets be replaced by sequences?,,"For now, my understanding is very unclear. So please help me. My first question is : Can a sequential space(e.g. a pseudometrizable space) be understood to be a space where any topological property(e.g. open, compact, and so on) can be described in terms of sequences instead of nets? If my question is unclear, then, in other words, when can sequences replace nets? I see an example in Wikipedia: In sequential spaces, a function is continuous if and only if it is sequentially continuous. So, in this case, we only need to consider sequences, not nets, to determine whether a function is continuous. I am reading so many things containing sequential spaces, uniform spaces, uniform properties to obtain a clear understanding, which is not that successful until now. All wiki or stack exchange posts seem to be perversely dodging my tantalizing question. My second question is: An bounded linear operator $T$ acting between two Banach spaces is called compact if for any bounded sequence $\{x_n\}$ , the sequence $\{Tx_n\}$ contains a convergent subsequence. In deriving this definition among many other alternative definitions, one use the fact that a metric space is totally bounded if any infinite sequence contains a Cauchy subsequence. In this case, ""a sequence"" is used instead of ""a net"", which means that it suffices to consider ""sequences"" instead of ""nets"" to determine whether a space is totally bounded. But, total boundedness is not a topological property.(Here, I came to know the concept of ""uniform property"" and ""uniform space"". But, these concepts seem to rather interrupt resolving my question.) Another example I found in Wikipedia is : A metric space is complete if and only if it is sequentially complete. Thus, in this case, one only needs to consider Cauchy sequences, instead of Cauchy nets. Here, completeness is not a topological property. But, we can replace nets with sequences. Is it also due to the fact that any metric space is a sequential space? I am guessing that the reason for a sequence to be used instead of a net is that the space concerned is a (pseudo-)metric space. I'd like to know the super-clear and simple reason or logic behind this, without being further covered by advanced concepts, which tends to make the issue more complicated. Thanks a lot.","For now, my understanding is very unclear. So please help me. My first question is : Can a sequential space(e.g. a pseudometrizable space) be understood to be a space where any topological property(e.g. open, compact, and so on) can be described in terms of sequences instead of nets? If my question is unclear, then, in other words, when can sequences replace nets? I see an example in Wikipedia: In sequential spaces, a function is continuous if and only if it is sequentially continuous. So, in this case, we only need to consider sequences, not nets, to determine whether a function is continuous. I am reading so many things containing sequential spaces, uniform spaces, uniform properties to obtain a clear understanding, which is not that successful until now. All wiki or stack exchange posts seem to be perversely dodging my tantalizing question. My second question is: An bounded linear operator acting between two Banach spaces is called compact if for any bounded sequence , the sequence contains a convergent subsequence. In deriving this definition among many other alternative definitions, one use the fact that a metric space is totally bounded if any infinite sequence contains a Cauchy subsequence. In this case, ""a sequence"" is used instead of ""a net"", which means that it suffices to consider ""sequences"" instead of ""nets"" to determine whether a space is totally bounded. But, total boundedness is not a topological property.(Here, I came to know the concept of ""uniform property"" and ""uniform space"". But, these concepts seem to rather interrupt resolving my question.) Another example I found in Wikipedia is : A metric space is complete if and only if it is sequentially complete. Thus, in this case, one only needs to consider Cauchy sequences, instead of Cauchy nets. Here, completeness is not a topological property. But, we can replace nets with sequences. Is it also due to the fact that any metric space is a sequential space? I am guessing that the reason for a sequence to be used instead of a net is that the space concerned is a (pseudo-)metric space. I'd like to know the super-clear and simple reason or logic behind this, without being further covered by advanced concepts, which tends to make the issue more complicated. Thanks a lot.",T \{x_n\} \{Tx_n\},"['real-analysis', 'functional-analysis', 'analysis', 'cauchy-sequences', 'nets']"
28,Riemann-Stieltjes integral with respect to a discontinuous function,Riemann-Stieltjes integral with respect to a discontinuous function,,"I have to find if the function $f(x)=x^2$ is Rieman-Stieltjes integrable respect to the function $g(x)=3x$ if $x\in[0,1)$ and $g(1)=4$ . Now, because $f$ is continuous and $g$ is of bounded variation, the integral indeed exists. To find the value of the integral, I chose to find the integral: $$\int_0^1f(x)d(g(x))-\int_0^1f(x)d(3x)$$ By definition of $g$ , this is: $$\int_0^1f(x)d(h(x))$$ Where $h(x)=g(x)-3x$ , so $h(x)=0$ for $x\in[0,1)$ and $h(1)=1$ Finding it by definition, if $\epsilon>0$ , and I chose the partition $P=\{0,1-\epsilon,1\}$ , then: $S(P,f,h)=f(0)(h(1-\epsilon)-h(0))+f(1-\epsilon)(h(1)-h(1-\epsilon))=f(1-\epsilon)=(1-\epsilon)^2$ From here, I could conclude that: $$\int_0^1f(x)d(h(x))=1$$ Is this correct? Because intuitively I thought it was going to be $0$ , or did I do something wrong?.","I have to find if the function is Rieman-Stieltjes integrable respect to the function if and . Now, because is continuous and is of bounded variation, the integral indeed exists. To find the value of the integral, I chose to find the integral: By definition of , this is: Where , so for and Finding it by definition, if , and I chose the partition , then: From here, I could conclude that: Is this correct? Because intuitively I thought it was going to be , or did I do something wrong?.","f(x)=x^2 g(x)=3x x\in[0,1) g(1)=4 f g \int_0^1f(x)d(g(x))-\int_0^1f(x)d(3x) g \int_0^1f(x)d(h(x)) h(x)=g(x)-3x h(x)=0 x\in[0,1) h(1)=1 \epsilon>0 P=\{0,1-\epsilon,1\} S(P,f,h)=f(0)(h(1-\epsilon)-h(0))+f(1-\epsilon)(h(1)-h(1-\epsilon))=f(1-\epsilon)=(1-\epsilon)^2 \int_0^1f(x)d(h(x))=1 0","['real-analysis', 'calculus', 'integration', 'analysis']"
29,Area between two given equations,Area between two given equations,,"Let $x^2 + y^2$ = 6 and $x = y^2$ . What is the area between them? I solved the two equations in terms of y and I graph it in Geogebra: https://www.geogebra.org/classic/ahnk4hfw Solving the two equations in terms of x, the graph is: https://www.geogebra.org/classic/nwznqk3c When I integrate in terms of y (the graph is the first one), the bounds are - $\sqrt 2$ and $\sqrt 2$ . The integrand is $ \sqrt {6-y^2} $ - $y^2$ . I got 4.63 as the area. $$\int_{-\sqrt 2}^{\sqrt 2} \sqrt {6-y^2} - y^2 \, dy = 4.63 $$ Although, I am not sure if I expressed the integral correctly because it seems that I am missing one function: the - $\sqrt {6-y^2} $ . When I follow the second graph, isn't it that the whole region is bounded by the two functions, and hence the area is the circle's area? Or should I only choose the positive square roots and not the negative ones? My last question is, what is the best approach of finding the integral of this case? Is it by integrating with respect to y or x? Do I only need to take the smaller region?","Let = 6 and . What is the area between them? I solved the two equations in terms of y and I graph it in Geogebra: https://www.geogebra.org/classic/ahnk4hfw Solving the two equations in terms of x, the graph is: https://www.geogebra.org/classic/nwznqk3c When I integrate in terms of y (the graph is the first one), the bounds are - and . The integrand is - . I got 4.63 as the area. Although, I am not sure if I expressed the integral correctly because it seems that I am missing one function: the - . When I follow the second graph, isn't it that the whole region is bounded by the two functions, and hence the area is the circle's area? Or should I only choose the positive square roots and not the negative ones? My last question is, what is the best approach of finding the integral of this case? Is it by integrating with respect to y or x? Do I only need to take the smaller region?","x^2 + y^2 x = y^2 \sqrt 2 \sqrt 2  \sqrt {6-y^2}  y^2 \int_{-\sqrt 2}^{\sqrt 2} \sqrt {6-y^2} - y^2 \, dy = 4.63  \sqrt {6-y^2} ",['calculus']
30,Converse of Dirichlet test: a result about existence,Converse of Dirichlet test: a result about existence,,"I am wondering whether the following converse to the Dirichlet's test of convergence is true. Let $\lambda_n$ be a sequence of complex numbers such that $\sum_{k=1}^n \lambda_k$ is unbounded as $n \to \infty$ . Then there exists a sequence $x_n \to 0$ such that $\sum \lambda_n x_n$ diverges. There is a delicate balance between $\lambda$ and $x$ , which makes it really hard to prove the above result or find a counterexample. Is the statement above true? In fact, if a counterexample exists, then there might be some possibilities of strengthening the Dirichlet test to a stronger result.","I am wondering whether the following converse to the Dirichlet's test of convergence is true. Let be a sequence of complex numbers such that is unbounded as . Then there exists a sequence such that diverges. There is a delicate balance between and , which makes it really hard to prove the above result or find a counterexample. Is the statement above true? In fact, if a counterexample exists, then there might be some possibilities of strengthening the Dirichlet test to a stronger result.",\lambda_n \sum_{k=1}^n \lambda_k n \to \infty x_n \to 0 \sum \lambda_n x_n \lambda x,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
31,"Using Stone-Weierstrass theorem prove that $f\equiv 0$ in $[0,1]$ under certain conditions.",Using Stone-Weierstrass theorem prove that  in  under certain conditions.,"f\equiv 0 [0,1]","Let $f$ be a continuous function over $[0,1]$ and $\displaystyle \int_0^1 f(x)\,dx=0$ . If for any positive integer $n$ , $\displaystyle \int_0^1 x^{12+3n}f(x)\,dx=0$ then using Stone-Weiersterass Theorem prove that $f\equiv 0$ in $[0,1]$ . Since $f$ is contnuous on $[0,1]$ so by Stone-Weierstrass Theorem there exists a sequence of polynomials $\{p_n(x)\}_n$ which converges uniformly to $f$ on $[0,1]$ .  That is  , $p_n\to f$ , implies $p_n(x)f(x)\to f^2(x)$ as $n\to \infty$ , which implies that $\displaystyle \int_0^1p_n(x)f(x)\,dx \to \int_0^1 f^2(x)\,dx$ . But from the given condition I can't say that $\displaystyle \int_0^1 f^2(x)\,dx =0$ . So how to proceed to solve this problem ? Any hint. please.","Let be a continuous function over and . If for any positive integer , then using Stone-Weiersterass Theorem prove that in . Since is contnuous on so by Stone-Weierstrass Theorem there exists a sequence of polynomials which converges uniformly to on .  That is  , , implies as , which implies that . But from the given condition I can't say that . So how to proceed to solve this problem ? Any hint. please.","f [0,1] \displaystyle \int_0^1 f(x)\,dx=0 n \displaystyle \int_0^1 x^{12+3n}f(x)\,dx=0 f\equiv 0 [0,1] f [0,1] \{p_n(x)\}_n f [0,1] p_n\to f p_n(x)f(x)\to f^2(x) n\to \infty \displaystyle \int_0^1p_n(x)f(x)\,dx \to \int_0^1 f^2(x)\,dx \displaystyle \int_0^1 f^2(x)\,dx =0","['real-analysis', 'functional-analysis', 'analysis', 'polynomials', 'weierstrass-approximation']"
32,Fourier transform of $\log|x|$ in $\mathbb{R}^2$ and the solution to $\Delta u = \delta_0$,Fourier transform of  in  and the solution to,\log|x| \mathbb{R}^2 \Delta u = \delta_0,"In my analysis class, we are covering tempered distributions now. I was given this two-part question in Fourier transforms of distributions. a. We are asked to compute the Fourier transform of $\log |x|$ as a tempered distribution in $\mathbb{R}^2$ . Here, $|x|$ is the 2d Euclidean norm of the vector $x$ . b. We are asked to find the fundamental solution to the Laplace equation in $\mathbb{R}^2$ , $\Delta u = \delta_0$ , where $u$ is a distribution and understood in the sense of weak solutions. Here, we take the Laplacian $\Delta u = -\frac{\partial^2 u}{\partial x^2}-\frac{\partial^2 u}{\partial y^2}$ . We take the Schwartz space of functions $S(\mathbb{R}^2)$ and its continuous dual space, $S'(\mathbb{R}^2)$ , the space of tempered distributions. For $T \in S'(\mathbb{R}^2)$ and $\phi \in S(\mathbb{R}^2)$ , we use the notation $$\langle T,\phi \rangle = T(\phi)$$ and when $T(x)$ is a function, we define $$\langle T,\phi \rangle = \int_{\mathbb{R}^2} T(x)\phi(x)dx$$ For the Foruier transform on tempered distributions, we define $$ \langle \mathcal{F}T,\phi \rangle = \langle T,\mathcal{F}\phi \rangle. $$ To be honest, I have no idea how to compute the Fourier transform of $\log|x|$ in $\mathbb{R}^2$ and how to use it to find the fundamental solution in part b, I do think I need to move to the Fourier domain in b but other than that I am lost. I thank anyone who can help with parts A and B.","In my analysis class, we are covering tempered distributions now. I was given this two-part question in Fourier transforms of distributions. a. We are asked to compute the Fourier transform of as a tempered distribution in . Here, is the 2d Euclidean norm of the vector . b. We are asked to find the fundamental solution to the Laplace equation in , , where is a distribution and understood in the sense of weak solutions. Here, we take the Laplacian . We take the Schwartz space of functions and its continuous dual space, , the space of tempered distributions. For and , we use the notation and when is a function, we define For the Foruier transform on tempered distributions, we define To be honest, I have no idea how to compute the Fourier transform of in and how to use it to find the fundamental solution in part b, I do think I need to move to the Fourier domain in b but other than that I am lost. I thank anyone who can help with parts A and B.","\log |x| \mathbb{R}^2 |x| x \mathbb{R}^2 \Delta u = \delta_0 u \Delta u = -\frac{\partial^2 u}{\partial x^2}-\frac{\partial^2 u}{\partial y^2} S(\mathbb{R}^2) S'(\mathbb{R}^2) T \in S'(\mathbb{R}^2) \phi \in S(\mathbb{R}^2) \langle T,\phi \rangle = T(\phi) T(x) \langle T,\phi \rangle = \int_{\mathbb{R}^2} T(x)\phi(x)dx  \langle \mathcal{F}T,\phi \rangle = \langle T,\mathcal{F}\phi \rangle.  \log|x| \mathbb{R}^2","['analysis', 'partial-differential-equations', 'fourier-analysis', 'fourier-transform', 'distribution-theory']"
33,Compute the spectral measure of an operator $(T\mathbf{a})_{n}=a_{n+1}-a_{n}$ on the space of square summable sequences,Compute the spectral measure of an operator  on the space of square summable sequences,(T\mathbf{a})_{n}=a_{n+1}-a_{n},"Consider the space $\mathcal{H}$ of square summable sequences $\mathbf{a}=\{a_{n}\}_{n=-\infty}^{\infty}$ . The operator is defined by $$(T\mathbf{a})_{n}=a_{n+1}-a_{n}.$$ I want to compute the spectral resolution of this operator. From this website, https://encyclopediaofmath.org/wiki/Spectral_resolution , the spectrum measure $\mu$ can define resolution by $\mu((-\infty, t))$ . So I guess the key is to get the spectral measure of this operator. Edit 1: I think I got at least the spectrum Given $\Psi\in\ell^{2}(\mathbb{Z})$ , we can define $\hat{\Psi}\in L^{2}([0,2\pi))$ by $\hat{\Psi}(\xi):=\sum_{n\in\mathbb{Z}}e^{in\xi}\Psi_{n},$ with $$\Psi_{n}=\dfrac{1}{2\pi}\int_{0}^{2\pi}e^{in\xi}d\xi.$$ Then, the map $$U:\ell^{2}(\mathbb{Z})\longrightarrow L^{2}([0,2\pi),\dfrac{d\xi}{2\pi}),\ \ \Psi\longrightarrow_{U}\hat{\Psi},$$ is unitary. We want to know what does $UTU^{-1}$ give us. Take $f(\xi)\in L^{2}([0,2\pi))$ , then $$(TU^{-1}f)(n)=\dfrac{1}{2\pi}\int_{0}^{2\pi}(e^{-i(n+1)\xi}-e^{-in\xi})f(\xi)d\xi=\dfrac{1}{2\pi}\int_{0}^{2\pi}e^{-in\xi}(e^{-i\xi}-1)f(\xi)d\xi,$$ which implies that $$(UTU^{-1})(\xi)=(e^{-i\xi}-1)f(\xi).$$ Now we have the form of our operator in Fourier space, we are equipped to determine its spectrum. Look at the resolvent: \begin{align*} R(T):=\{\lambda:(\lambda-T)\ \text{is invertible}\}&=\{\lambda:\lambda-e^{-i\xi}+1\neq 0\ \ \text{for all}\ \ \xi\in [0,2\pi)\}\\ &=\{\lambda:\lambda\neq e^{-i\xi}-1\ \ \ \text{for all}\ \ \xi\in [0,2\pi)\}\\ &=\mathbb{C}\setminus \{|\lambda+1|=1\}. \end{align*} This implies that the spectrum is $$\sigma(T)= \{|\lambda+1|=1\}.$$ We denote this set to be $\mathbb{S}_{1}^{*}(-1,1)$ . Am I correct? Edit 2: I possibly get the spectral measure. But it confuses me Now, for $f\in C(\mathbb{S}^{*}(-1,1))$ in Fourier space $f(T)$ is just multiplication by $f(e^{-i\xi}-1)$ . Fix $\Psi\in\ell^{2}(\mathbb{Z})$ , and let $\hat{\Psi}$ be the corresponding function in $L^{2}([0,2\pi))$ . Then \begin{align*} \int f(T)d\mu_{\Psi}=\langle \Psi|f(T)\psi\rangle_{\ell^{2}}&=\langle U^{-1}U\Psi|f(T)U^{-1}U\Psi\rangle_{\ell{^2}}\\ &=\Big\langle U\Psi\Bigg|\Big(Uf(T)U^{-1}\Big)U\Psi\rangle_{L^{2}}\\ &=\langle \hat{\Psi}|f(e^{-i\xi}-1)\hat{\Psi}\rangle_{L^{2}}\\ &=\dfrac{1}{2\pi}\int_{0}^{2\pi}\overline{\hat{\Psi}}f(e^{-i\xi}-1)\hat{\Psi}d\xi\\ &=\dfrac{1}{2\pi}\int_{0}^{2\pi}f(e^{-i\xi}-1)|\hat{\Psi}(\xi)|^{2}d\xi. \end{align*} Write $\lambda:=e^{-i\xi}-1$ , then $d\xi=\frac{d\lambda}{-e^{-i\xi}}$ and $\xi=i\log(\lambda+1)$ where $\log$ is taking over the principal branch (it's okay if just a general $\log$ since $\xi\in [0,2\pi]$ ). Then, \begin{align*} \int fd\mu_{\Psi}&=\dfrac{1}{2\pi}\int_{\mathbb{S}_{1}^{*}(-1,0)}f(\lambda)|\hat{\Psi}(i\log(\lambda+1))|^{2}\dfrac{d\lambda}{-e^{-i\xi}}, \end{align*} therefore, $$d\mu_{\Psi}=|\hat{\Psi}(i\log(\lambda+1))|^{2}\dfrac{d\lambda}{-e^{-i\xi}}\Bigg|_{\mathbb{S}_{1}^{*}(-1,0)}.$$ According to this website, https://encyclopediaofmath.org/wiki/Spectral_resolution , the spectrum measure $\mu$ can define resolution by $\mu((-\infty, t))$ . But you see my spectrum density is restricted to the unit circle. I am not sure how to go back to the real line. Are the above things correct? How can I proceed?","Consider the space of square summable sequences . The operator is defined by I want to compute the spectral resolution of this operator. From this website, https://encyclopediaofmath.org/wiki/Spectral_resolution , the spectrum measure can define resolution by . So I guess the key is to get the spectral measure of this operator. Edit 1: I think I got at least the spectrum Given , we can define by with Then, the map is unitary. We want to know what does give us. Take , then which implies that Now we have the form of our operator in Fourier space, we are equipped to determine its spectrum. Look at the resolvent: This implies that the spectrum is We denote this set to be . Am I correct? Edit 2: I possibly get the spectral measure. But it confuses me Now, for in Fourier space is just multiplication by . Fix , and let be the corresponding function in . Then Write , then and where is taking over the principal branch (it's okay if just a general since ). Then, therefore, According to this website, https://encyclopediaofmath.org/wiki/Spectral_resolution , the spectrum measure can define resolution by . But you see my spectrum density is restricted to the unit circle. I am not sure how to go back to the real line. Are the above things correct? How can I proceed?","\mathcal{H} \mathbf{a}=\{a_{n}\}_{n=-\infty}^{\infty} (T\mathbf{a})_{n}=a_{n+1}-a_{n}. \mu \mu((-\infty, t)) \Psi\in\ell^{2}(\mathbb{Z}) \hat{\Psi}\in L^{2}([0,2\pi)) \hat{\Psi}(\xi):=\sum_{n\in\mathbb{Z}}e^{in\xi}\Psi_{n}, \Psi_{n}=\dfrac{1}{2\pi}\int_{0}^{2\pi}e^{in\xi}d\xi. U:\ell^{2}(\mathbb{Z})\longrightarrow L^{2}([0,2\pi),\dfrac{d\xi}{2\pi}),\ \ \Psi\longrightarrow_{U}\hat{\Psi}, UTU^{-1} f(\xi)\in L^{2}([0,2\pi)) (TU^{-1}f)(n)=\dfrac{1}{2\pi}\int_{0}^{2\pi}(e^{-i(n+1)\xi}-e^{-in\xi})f(\xi)d\xi=\dfrac{1}{2\pi}\int_{0}^{2\pi}e^{-in\xi}(e^{-i\xi}-1)f(\xi)d\xi, (UTU^{-1})(\xi)=(e^{-i\xi}-1)f(\xi). \begin{align*}
R(T):=\{\lambda:(\lambda-T)\ \text{is invertible}\}&=\{\lambda:\lambda-e^{-i\xi}+1\neq 0\ \ \text{for all}\ \ \xi\in [0,2\pi)\}\\
&=\{\lambda:\lambda\neq e^{-i\xi}-1\ \ \ \text{for all}\ \ \xi\in [0,2\pi)\}\\
&=\mathbb{C}\setminus \{|\lambda+1|=1\}.
\end{align*} \sigma(T)= \{|\lambda+1|=1\}. \mathbb{S}_{1}^{*}(-1,1) f\in C(\mathbb{S}^{*}(-1,1)) f(T) f(e^{-i\xi}-1) \Psi\in\ell^{2}(\mathbb{Z}) \hat{\Psi} L^{2}([0,2\pi)) \begin{align*}
\int f(T)d\mu_{\Psi}=\langle \Psi|f(T)\psi\rangle_{\ell^{2}}&=\langle U^{-1}U\Psi|f(T)U^{-1}U\Psi\rangle_{\ell{^2}}\\
&=\Big\langle U\Psi\Bigg|\Big(Uf(T)U^{-1}\Big)U\Psi\rangle_{L^{2}}\\
&=\langle \hat{\Psi}|f(e^{-i\xi}-1)\hat{\Psi}\rangle_{L^{2}}\\
&=\dfrac{1}{2\pi}\int_{0}^{2\pi}\overline{\hat{\Psi}}f(e^{-i\xi}-1)\hat{\Psi}d\xi\\
&=\dfrac{1}{2\pi}\int_{0}^{2\pi}f(e^{-i\xi}-1)|\hat{\Psi}(\xi)|^{2}d\xi.
\end{align*} \lambda:=e^{-i\xi}-1 d\xi=\frac{d\lambda}{-e^{-i\xi}} \xi=i\log(\lambda+1) \log \log \xi\in [0,2\pi] \begin{align*}
\int fd\mu_{\Psi}&=\dfrac{1}{2\pi}\int_{\mathbb{S}_{1}^{*}(-1,0)}f(\lambda)|\hat{\Psi}(i\log(\lambda+1))|^{2}\dfrac{d\lambda}{-e^{-i\xi}},
\end{align*} d\mu_{\Psi}=|\hat{\Psi}(i\log(\lambda+1))|^{2}\dfrac{d\lambda}{-e^{-i\xi}}\Bigg|_{\mathbb{S}_{1}^{*}(-1,0)}. \mu \mu((-\infty, t))","['real-analysis', 'functional-analysis', 'analysis', 'operator-theory', 'hilbert-spaces']"
34,Understanding Formula for the Pair Correlation for the Farey Sequence,Understanding Formula for the Pair Correlation for the Farey Sequence,,"I'm reading this paper https://arxiv.org/pdf/math/0404114.pdf , and I'm unsure how to interpret the formula in Theorem 2 (1.5), regarding the pair correlation function of the sequence of Farey Fractions. Specifically, I don't understand the $\lambda$ that they used. In (1.6), they wrote as $\lambda \rightarrow \infty$ , $g_2(λ) = 1 + O(λ^{−1})$ . So this means the function approaches 1 as $\lambda$ goes to infinity. This seems reasonable as I could see the function asymptotically approaching 1, the problem is I'm not sure what $\lambda$ means in this context. The only time they mentioned $\lambda$ beforehand is on page 2 when they defined $R_F^{(v)}(\lambda_1,...,\lambda_{v-1})=2^{-v+1}R_F^{(v)}(\prod_{j=1}^{v-1}[-\lambda_j,\lambda_j]).$ My guess right now is that $\lambda$ refers to the interval for which the correlation measures are taken. Thus, as the interval of normalized Farey Fractions become arbitrarily large, the function converges to 1. Is that reasonable? Then $\lambda$ would represent a box in $\mathbb{R}^{v-1}$ for $v \geq3$ . Could someone give me some clarity on what this $\lambda$ represents and how this would change for $v\geq 3$ ? Thanks a lot.","I'm reading this paper https://arxiv.org/pdf/math/0404114.pdf , and I'm unsure how to interpret the formula in Theorem 2 (1.5), regarding the pair correlation function of the sequence of Farey Fractions. Specifically, I don't understand the that they used. In (1.6), they wrote as , . So this means the function approaches 1 as goes to infinity. This seems reasonable as I could see the function asymptotically approaching 1, the problem is I'm not sure what means in this context. The only time they mentioned beforehand is on page 2 when they defined My guess right now is that refers to the interval for which the correlation measures are taken. Thus, as the interval of normalized Farey Fractions become arbitrarily large, the function converges to 1. Is that reasonable? Then would represent a box in for . Could someone give me some clarity on what this represents and how this would change for ? Thanks a lot.","\lambda \lambda \rightarrow \infty g_2(λ) = 1 + O(λ^{−1}) \lambda \lambda \lambda R_F^{(v)}(\lambda_1,...,\lambda_{v-1})=2^{-v+1}R_F^{(v)}(\prod_{j=1}^{v-1}[-\lambda_j,\lambda_j]). \lambda \lambda \mathbb{R}^{v-1} v \geq3 \lambda v\geq 3","['number-theory', 'analysis', 'correlation']"
35,Confusion on the proofs of convolution theorem of Fourier Transform.,Confusion on the proofs of convolution theorem of Fourier Transform.,,"The convolution theorem of Fourier transform is stated as follows: Define $h(x):=f(x)*g(x)$ , then we have $$\hat{h}(k)=\hat{f}(k)\hat{g}(k).$$ I have a confusion of the proofs of this theorem. Most proofs go in the following way: Define $h(x):=f(x)*g(x)$ .  By definition, we have $$\hat{h}(k)=\int_{-\infty}^{\infty}h(x)e^{-ikx}dx.$$ Then, plugging in the integral expression of $f*g$ into the above integral yields us \begin{align*} \hat{h}(k)&=\int_{-\infty}^{\infty}h(x)e^{-ikx}dx\\ &=\int_{-\infty}^{\infty}\Big(\int_{-\infty}^{\infty}f(x-s)g(s)ds\Big)e^{-ikx}dx\\ &=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x-s)g(s)e^{-ikx}dsdx.\ \ \ \ \ \ (*) \end{align*} Then, most proofs directly change integral order in the question $(*)$ , so that \begin{align*} \hat{h}(k)&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x-s)g(s)e^{-ikx}dsdx\\ &=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x-s)g(s)e^{-ikx}dxds\\ &=\int_{-\infty}^{\infty}\Big(\int_{-\infty}^{\infty}f(x-s)e^{-ikx}dx\Big)g(s)ds. \end{align*} What remains is standard: note that by the shifting property, the inner integral is $$\int_{-\infty}^{\infty}f(x-s)e^{-ikx}dx=\hat{f}(k)e^{-iks},$$ and thus \begin{align*} \hat{h}(k)&=\int_{-\infty}^{\infty}\Big(\int_{-\infty}^{\infty}f(x-s)e^{-ikx}dx\Big)g(s)ds\\ &=\hat{f}(k)\int_{-\infty}^{\infty}g(s)e^{-iks}ds\\ &=\hat{f}(k)\hat{g}(k), \end{align*} where the last equality was obtained by definition. The proof is concluded. However, why could we change the integral order of (*) in the first place? Don't we need Fubini? I tried to use Fubini as follows: $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|f(x-s)g(s)e^{-ikx}|dsdx=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|f(x-s)g(s)|dsdx$$ but I do not know how to proceed further. I had expected that I can split the integral so that I would have two $L^{1}-$ norm, but since the first integral is $ds$ , I could not split the integral.. Do we directly assume the function is nice enough so that we can interchange the integral order? If so, what type of functions do we assume? Thank you!","The convolution theorem of Fourier transform is stated as follows: Define , then we have I have a confusion of the proofs of this theorem. Most proofs go in the following way: Define .  By definition, we have Then, plugging in the integral expression of into the above integral yields us Then, most proofs directly change integral order in the question , so that What remains is standard: note that by the shifting property, the inner integral is and thus where the last equality was obtained by definition. The proof is concluded. However, why could we change the integral order of (*) in the first place? Don't we need Fubini? I tried to use Fubini as follows: but I do not know how to proceed further. I had expected that I can split the integral so that I would have two norm, but since the first integral is , I could not split the integral.. Do we directly assume the function is nice enough so that we can interchange the integral order? If so, what type of functions do we assume? Thank you!","h(x):=f(x)*g(x) \hat{h}(k)=\hat{f}(k)\hat{g}(k). h(x):=f(x)*g(x) \hat{h}(k)=\int_{-\infty}^{\infty}h(x)e^{-ikx}dx. f*g \begin{align*}
\hat{h}(k)&=\int_{-\infty}^{\infty}h(x)e^{-ikx}dx\\
&=\int_{-\infty}^{\infty}\Big(\int_{-\infty}^{\infty}f(x-s)g(s)ds\Big)e^{-ikx}dx\\
&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x-s)g(s)e^{-ikx}dsdx.\ \ \ \ \ \ (*)
\end{align*} (*) \begin{align*}
\hat{h}(k)&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x-s)g(s)e^{-ikx}dsdx\\
&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x-s)g(s)e^{-ikx}dxds\\
&=\int_{-\infty}^{\infty}\Big(\int_{-\infty}^{\infty}f(x-s)e^{-ikx}dx\Big)g(s)ds.
\end{align*} \int_{-\infty}^{\infty}f(x-s)e^{-ikx}dx=\hat{f}(k)e^{-iks}, \begin{align*}
\hat{h}(k)&=\int_{-\infty}^{\infty}\Big(\int_{-\infty}^{\infty}f(x-s)e^{-ikx}dx\Big)g(s)ds\\
&=\hat{f}(k)\int_{-\infty}^{\infty}g(s)e^{-iks}ds\\
&=\hat{f}(k)\hat{g}(k),
\end{align*} \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|f(x-s)g(s)e^{-ikx}|dsdx=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|f(x-s)g(s)|dsdx L^{1}- ds","['real-analysis', 'analysis', 'fourier-analysis', 'proof-explanation', 'fourier-transform']"
36,"Distance from vectors in $\mathbb{Z}^d$ to the cube $[-1/2,1/2]^d$",Distance from vectors in  to the cube,"\mathbb{Z}^d [-1/2,1/2]^d","Let $\|\;\|_2$ be the Eucliden norm on $\mathbb{R}^d$ . Problem: Suppose $x\in Q:=\big[-\tfrac12,\tfrac12\big]^d$ . Is $$ \|x-k\|_2\geq \frac{1}{2\sqrt{d}}\|k\|_2 $$ for all $k\in\mathbb{Z}^d$ ? Notice that for all $x\in Q$ , $\|x-k\|_2\geq d(k,Q):=\inf_{x\in Q}\|x-k\|_2$ . So it is to enough to show that $$d(k,Q)\geq \frac{\|k\|_2}{2\sqrt{d}},\quad k\in\mathbb{Z}^d$$ I think this holds but I can't reproduce a proof at the moment. The relevance of this simple geometric result is that it provides a criteria for absolute and uniform convergence of the Poisson summation $$\sum_{k\in\mathbb{Z}^d}f(x+k)=\sum_{k\in\mathbb{Z}^d}\widehat{f}(k)e^{2\pi ik\cdot x}$$ in the $\mathbb{T}^d$ torus, where $f\in L_1(\mathbb{R}^d)$ . If $f$ can be bounded poitwise by an integrable decreasing radial function, i.e. $|f(x)|\leq \phi_0(\|x\|_2)$ , where $\phi_0$ is monotone non increasing and $\phi_0\circ\|\;\|_2\in L_1(\mathbb{R}^d)$ A proof or a good hint will be appreciated.","Let be the Eucliden norm on . Problem: Suppose . Is for all ? Notice that for all , . So it is to enough to show that I think this holds but I can't reproduce a proof at the moment. The relevance of this simple geometric result is that it provides a criteria for absolute and uniform convergence of the Poisson summation in the torus, where . If can be bounded poitwise by an integrable decreasing radial function, i.e. , where is monotone non increasing and A proof or a good hint will be appreciated.","\|\;\|_2 \mathbb{R}^d x\in Q:=\big[-\tfrac12,\tfrac12\big]^d 
\|x-k\|_2\geq \frac{1}{2\sqrt{d}}\|k\|_2
 k\in\mathbb{Z}^d x\in Q \|x-k\|_2\geq d(k,Q):=\inf_{x\in Q}\|x-k\|_2 d(k,Q)\geq \frac{\|k\|_2}{2\sqrt{d}},\quad k\in\mathbb{Z}^d \sum_{k\in\mathbb{Z}^d}f(x+k)=\sum_{k\in\mathbb{Z}^d}\widehat{f}(k)e^{2\pi ik\cdot x} \mathbb{T}^d f\in L_1(\mathbb{R}^d) f |f(x)|\leq \phi_0(\|x\|_2) \phi_0 \phi_0\circ\|\;\|_2\in L_1(\mathbb{R}^d)","['analysis', 'fourier-analysis', 'harmonic-analysis']"
37,Show that $\lim\limits_{n\to\infty}n\cos(n)$ is divergent by definition of limit.,Show that  is divergent by definition of limit.,\lim\limits_{n\to\infty}n\cos(n),"I am trying to split it into two cases, (1) $\cos(n)\geq 0$ and (2) $\cos(n)<0$ . Then for (1), I would like to show the limit diverges to $+\infty$ , and $-\infty$ for (2). Then I tried to formulate the definition of limit diverging to $\pm\infty$ , by using the Archimedean property, but it seems not working. Here are the details: Say for (1): $n\in\left[ 2k\pi,\pi/4+2k\pi\right]\cup\left[ 3\pi/4+2k\pi, 2\pi+2k\pi\right]$ , and I want to get something like $$\forall M\in \mathbb{R},\exists K\in \mathbb{N} \text{ s.t. } n\cos(n) >M \text{ for every } n\geq K$$ Then by Archimedean property, $$ \forall M\in \mathbb{R}, \exists K=[N\cos(N)]\text{ s.t. } K>M$$ But $\cos(n)$ actually depends on $n$ . Say for $n\geq K$ , when $\cos(n)$ is very small, $n\cos(n)$ may not be greater than $M$ .  I am wondering if there is any other possible way to tackle this problem. Or is there any way to find two subsequences that converging to different limits? Since $n\in \mathbb{N}$ , I find it kinda subtle to deal with cosine.","I am trying to split it into two cases, (1) and (2) . Then for (1), I would like to show the limit diverges to , and for (2). Then I tried to formulate the definition of limit diverging to , by using the Archimedean property, but it seems not working. Here are the details: Say for (1): , and I want to get something like Then by Archimedean property, But actually depends on . Say for , when is very small, may not be greater than .  I am wondering if there is any other possible way to tackle this problem. Or is there any way to find two subsequences that converging to different limits? Since , I find it kinda subtle to deal with cosine.","\cos(n)\geq 0 \cos(n)<0 +\infty -\infty \pm\infty n\in\left[ 2k\pi,\pi/4+2k\pi\right]\cup\left[ 3\pi/4+2k\pi, 2\pi+2k\pi\right] \forall M\in \mathbb{R},\exists K\in \mathbb{N} \text{ s.t. } n\cos(n) >M \text{ for every } n\geq K  \forall M\in \mathbb{R}, \exists K=[N\cos(N)]\text{ s.t. } K>M \cos(n) n n\geq K \cos(n) n\cos(n) M n\in \mathbb{N}","['real-analysis', 'analysis', 'convergence-divergence']"
38,"A question about $f:(0,1]\times[0,1]\rightarrow N_{l^1(\mathbb{R}^\omega)}(0,2)$",A question about,"f:(0,1]\times[0,1]\rightarrow N_{l^1(\mathbb{R}^\omega)}(0,2)","Let $L$ be the $l^1$ topology on the subset of $\mathbb{R}^\omega$ with finite $l^1$ norm. ( $\left\|x\right\|_{l^1(\mathbb{R}^\omega)}:=\sum_i|x_i|$ . $l^1$ topology is the metric topology induced by this norm.) Say $f:(0,1]\times[0,1]\rightarrow N_L(0,2)$ is any continuous function s.t. $f(t,0)\equiv 0$ and $f(t,1)\equiv(1,0,0...)$ . Then, would there always exist $\{t_n\},\{x_n\}$ s.t. $t_n\rightarrow 0$ and $f(t_n,x_n)$ converges pointwisely to a point in $N_L(0,\frac{1}{2})\setminus\{0\}$ ? ( $N_L(0,r)$ : the open ball of radius $r$ centered at 0, with respect to the metric of $L$ )","Let be the topology on the subset of with finite norm. ( . topology is the metric topology induced by this norm.) Say is any continuous function s.t. and . Then, would there always exist s.t. and converges pointwisely to a point in ? ( : the open ball of radius centered at 0, with respect to the metric of )","L l^1 \mathbb{R}^\omega l^1 \left\|x\right\|_{l^1(\mathbb{R}^\omega)}:=\sum_i|x_i| l^1 f:(0,1]\times[0,1]\rightarrow N_L(0,2) f(t,0)\equiv 0 f(t,1)\equiv(1,0,0...) \{t_n\},\{x_n\} t_n\rightarrow 0 f(t_n,x_n) N_L(0,\frac{1}{2})\setminus\{0\} N_L(0,r) r L","['general-topology', 'analysis']"
39,Completeness of $L^p$,Completeness of,L^p,"I have some doubts with the proof on this theorem (Bartle Elements of Integration) $\textbf{Theorem:}$ The vector space $L^p(X,\mathcal{F},\mu)$ is complete with the norm $$ \Vert \overline{f} \Vert_p = (\int \vert f \vert ^p )^{1/p} $$ I will write $f$ instead of your class. $\textbf{Proof:}$ Let $(f_n)$ a Cauchy sequence in $L^p$ . Then, exists a subsequence $(g_k)$ of $(f_n)$ such that $\Vert g_{k+1}-g_k \Vert < 2^{-k}$ . Define $$ (*)g(x) = \vert g_1(x) \vert + \sum_{i=1}^{\infty} \vert g_{k+1}(x) - g_k(x) \vert \implies \vert g(x) \vert ^p = (\vert g_1(x) \vert + \sum_{i=1}^{\infty} \vert g_{k+1}(x) - g_k(x) \vert)^p$$ Note that $g: X \rightarrow \overline{R}$ is measurable and $g \geq 0$ . Then by Fatou's lemma (notice that the sequence on the right converges to $\vert g \vert ^p$ ) $$ \int \vert g \vert^p d\mu \leq \text{liminf}\int (\vert g_1(x) \vert + \sum_{i=1}^{n} \vert g_{k+1}(x) - g_k(x) \vert)^p d\mu $$ $$ \implies (\int \vert g \vert^p d\mu)^{1/p} \leq \text{liminf} \lbrace \int (\vert g_1(x) \vert + \sum_{i=1}^{n} \vert g_{k+1}(x) - g_k(x) \vert)^p d\mu \rbrace^{1/p} $$ By Minkowski's and because $\Vert g_{k+1}-g_k \Vert < 2^{-k}$ we have $$ \implies (\int \vert g \vert^p d\mu)^{1/p} \leq \text{liminf} (\Vert g_1 \Vert_p + \sum_{i=1}^n \Vert g_{k+1} - g_k \Vert_p) = 1 + \Vert g_1 \Vert_p  $$ $\textbf{Here comes my first question}$ , in the book it says that if we consider $E=\{x \in X / g(x)< \infty \}$ then $E \in \mathcal{F}$ and $\mu(X \setminus E)=0$ . I can't understand why $\mu(X \setminus E)=0$ . After this he says that $g$ converges $\mu-a.e$ (I guess it will refer to (*)) and $g \chi_{E}$ is in $L^p$ . $\textbf{Here my second question}$ , because it is necessary to work with E?. I know that $g$ can take $\infty$ , but from the above we can see that $\int \vert g \vert^p d\mu < \infty $ because $g_1 \in L^p$ and so $\Vert g_1 \Vert_p < \infty$ and then $g \in L^p$ . Anyway, following the proof of the book, we define $f: X \rightarrow \mathbb{R}$ by $$f(x) = g_1(x) + \sum_{i=1}^{\infty} g_{k+1}(x)-g_k(x), \hspace{0.1cm} x\in E $$ $$f(x) = 0, \hspace{0.1cm} x\notin E $$ Applying the triangle inequality we have $\vert g_k \vert \leq \vert g_1 \vert + \sum_{i=1}^{k-1} \vert g_{j+1}-g_j \vert \leq g$ and then $\vert g_k \vert^p \leq g^p$ . Because $\vert g_k \vert^p \rightarrow \vert f \vert^p$ converge in $E$ (i.e $\mu$ -a.e) by the Dominated Convergence Theorem : $f \in L^p$ . Then we have $\vert f -g_k \vert \leq 2 \text{max}\{ \vert f \vert, \vert g_k \vert \}\leq 2g $ i.e $\vert f-g_k \vert^p \leq 2^p g^p$ , by the Dominated Convergence Theorem $$ 0 = \int \lim \vert f-g_k \vert^p = \lim \int \vert f-g_k \vert^p \implies \lim \Vert f-g_p \Vert_p = 0 $$ Then $g_k \rightarrow f$ in $L^p$ . Finally, because $(f_n)$ is Cauchy, for $\epsilon > 0$ , exists $n_0 \in \mathbb{N}$ such that $m,n>n_0$ implies $$ \int \vert f_m-f_n \vert^p < \Vert f_m - f_n \Vert_p <\epsilon^p $$ $\textbf{Here my third question}$ , in the book says that we have consider $k$ (bigger) such that $$ \int \vert f_m-g_k \vert^p < \epsilon^p $$ What means this ?, I understand that means that we can always choose a index of subsequence such that is greater than $n_0$ but I'm not sure. And then applying Fatou's Lemma $$ \int \vert f_m-f \vert^p \leq \text{liminf} \int \vert f_m-g_k \vert ^p  \leq \epsilon^p $$ And then $f_n \rightarrow f$ in $L^p$ , so $L^p$ is complete. Thank you very much for reading all this proof and I hope you can understand what my doubts were.","I have some doubts with the proof on this theorem (Bartle Elements of Integration) The vector space is complete with the norm I will write instead of your class. Let a Cauchy sequence in . Then, exists a subsequence of such that . Define Note that is measurable and . Then by Fatou's lemma (notice that the sequence on the right converges to ) By Minkowski's and because we have , in the book it says that if we consider then and . I can't understand why . After this he says that converges (I guess it will refer to (*)) and is in . , because it is necessary to work with E?. I know that can take , but from the above we can see that because and so and then . Anyway, following the proof of the book, we define by Applying the triangle inequality we have and then . Because converge in (i.e -a.e) by the Dominated Convergence Theorem : . Then we have i.e , by the Dominated Convergence Theorem Then in . Finally, because is Cauchy, for , exists such that implies , in the book says that we have consider (bigger) such that What means this ?, I understand that means that we can always choose a index of subsequence such that is greater than but I'm not sure. And then applying Fatou's Lemma And then in , so is complete. Thank you very much for reading all this proof and I hope you can understand what my doubts were.","\textbf{Theorem:} L^p(X,\mathcal{F},\mu)  \Vert \overline{f} \Vert_p = (\int \vert f \vert ^p )^{1/p}  f \textbf{Proof:} (f_n) L^p (g_k) (f_n) \Vert g_{k+1}-g_k \Vert < 2^{-k}  (*)g(x) = \vert g_1(x) \vert + \sum_{i=1}^{\infty} \vert g_{k+1}(x) - g_k(x) \vert \implies \vert g(x) \vert ^p = (\vert g_1(x) \vert + \sum_{i=1}^{\infty} \vert g_{k+1}(x) - g_k(x) \vert)^p g: X \rightarrow \overline{R} g \geq 0 \vert g \vert ^p  \int \vert g \vert^p d\mu \leq \text{liminf}\int (\vert g_1(x) \vert + \sum_{i=1}^{n} \vert g_{k+1}(x) - g_k(x) \vert)^p d\mu   \implies (\int \vert g \vert^p d\mu)^{1/p} \leq \text{liminf} \lbrace \int (\vert g_1(x) \vert + \sum_{i=1}^{n} \vert g_{k+1}(x) - g_k(x) \vert)^p d\mu \rbrace^{1/p}  \Vert g_{k+1}-g_k \Vert < 2^{-k}  \implies (\int \vert g \vert^p d\mu)^{1/p} \leq \text{liminf} (\Vert g_1 \Vert_p + \sum_{i=1}^n \Vert g_{k+1} - g_k \Vert_p) = 1 + \Vert g_1 \Vert_p   \textbf{Here comes my first question} E=\{x \in X / g(x)< \infty \} E \in \mathcal{F} \mu(X \setminus E)=0 \mu(X \setminus E)=0 g \mu-a.e g \chi_{E} L^p \textbf{Here my second question} g \infty \int \vert g \vert^p d\mu < \infty  g_1 \in L^p \Vert g_1 \Vert_p < \infty g \in L^p f: X \rightarrow \mathbb{R} f(x) = g_1(x) + \sum_{i=1}^{\infty} g_{k+1}(x)-g_k(x), \hspace{0.1cm} x\in E  f(x) = 0, \hspace{0.1cm} x\notin E  \vert g_k \vert \leq \vert g_1 \vert + \sum_{i=1}^{k-1} \vert g_{j+1}-g_j \vert \leq g \vert g_k \vert^p \leq g^p \vert g_k \vert^p \rightarrow \vert f \vert^p E \mu f \in L^p \vert f -g_k \vert \leq 2 \text{max}\{ \vert f \vert, \vert g_k \vert \}\leq 2g  \vert f-g_k \vert^p \leq 2^p g^p  0 = \int \lim \vert f-g_k \vert^p = \lim \int \vert f-g_k \vert^p \implies \lim \Vert f-g_p \Vert_p = 0  g_k \rightarrow f L^p (f_n) \epsilon > 0 n_0 \in \mathbb{N} m,n>n_0  \int \vert f_m-f_n \vert^p < \Vert f_m - f_n \Vert_p <\epsilon^p  \textbf{Here my third question} k  \int \vert f_m-g_k \vert^p < \epsilon^p  n_0  \int \vert f_m-f \vert^p \leq \text{liminf} \int \vert f_m-g_k \vert ^p  \leq \epsilon^p  f_n \rightarrow f L^p L^p","['analysis', 'measure-theory']"
40,"If $u\in H^s$, is $v=u^2$ gaining or losing ""regularity""?","If , is  gaining or losing ""regularity""?",u\in H^s v=u^2,"Let's consider a function $u\in H^s(\mathbb{R})$ for some $s\geq0$ , where $H^s$ denotes the standard $L^2$ -based Sobolev space. Now consider the function $v(x)=u^2(x)$ . I am wondering if $v(x)$ is gaining or losing regularity with respect to $u(x)$ . Specifically, I am wondering if $v\in H^m(\mathbb{R})$ with $m\geq s$ or $m\leq s$ ?. Is there any example of $u(x)$ such that $v(x)\notin H^s(\mathbb{R})$ ? Edit: Does the fact that $s\geq0$ plays any role in these kind of properties?","Let's consider a function for some , where denotes the standard -based Sobolev space. Now consider the function . I am wondering if is gaining or losing regularity with respect to . Specifically, I am wondering if with or ?. Is there any example of such that ? Edit: Does the fact that plays any role in these kind of properties?",u\in H^s(\mathbb{R}) s\geq0 H^s L^2 v(x)=u^2(x) v(x) u(x) v\in H^m(\mathbb{R}) m\geq s m\leq s u(x) v(x)\notin H^s(\mathbb{R}) s\geq0,"['analysis', 'partial-differential-equations']"
41,Proving non-differentiability of $f:\mathbb{R}^2 \to \mathbb{R}$,Proving non-differentiability of,f:\mathbb{R}^2 \to \mathbb{R},"Question: Given $f:\mathbb{R}^2 \to \mathbb{R}$ defined by $f(x, y) = \begin{cases} x,  & \text{if $y=x^2$} \\ 0, & \text{otherwise} \end{cases}$ , show $f$ is not differentiable at $(0, 0)$ . Attempt: I know a few things about $f$ : it is continuous at $(0, 0)$ and has continuous directional derivatives (but am yet to prove these). To prove non-differentiability, I need to show that there does not exist a linear mapping $A$ from $\mathbb{R}^2$ to $\mathbb{R}$ (which can be represented by the $2 \times 1$ matrix $\begin{bmatrix}     a \\     b \\     \end{bmatrix}$ ) so that $\lim_{h \to 0, h \in \mathbb{R}^2} \frac{\Vert f(x+h) - f(x) - Ah \Vert}{\Vert h \Vert} = 0$ where $x=(0, 0)$ . To do this, I considered the LHS of the equation and intend to show that it does not limit to $0$ . Letting $h=(h_1, h_2)$ gives $$\lim_{(h_1, h_2) \to 0} \frac{\Vert f((0, 0)+(h_1, h_2)) - f(0,0) - A(h_1, h_2) \Vert}{\Vert (h_1, h_2) \Vert}=\lim_{(h_1, h_2) \to 0} \frac{\Vert f(h_1, h_2) - A(h_1, h_2) \Vert}{\sqrt{h_1^2+h_2^2}}$$ however I am unsure of how to further evaluate this since we do not know $f(h_1, h_2)$ and I am unsure of what $A(h_1, h_2)$ evaluates to. Any help would be greatly appreciated.","Question: Given defined by , show is not differentiable at . Attempt: I know a few things about : it is continuous at and has continuous directional derivatives (but am yet to prove these). To prove non-differentiability, I need to show that there does not exist a linear mapping from to (which can be represented by the matrix ) so that where . To do this, I considered the LHS of the equation and intend to show that it does not limit to . Letting gives however I am unsure of how to further evaluate this since we do not know and I am unsure of what evaluates to. Any help would be greatly appreciated.","f:\mathbb{R}^2 \to \mathbb{R} f(x, y) =
\begin{cases}
x,  & \text{if y=x^2} \\
0, & \text{otherwise}
\end{cases} f (0, 0) f (0, 0) A \mathbb{R}^2 \mathbb{R} 2 \times 1 \begin{bmatrix}
    a \\
    b \\
    \end{bmatrix} \lim_{h \to 0, h \in \mathbb{R}^2} \frac{\Vert f(x+h) - f(x) - Ah \Vert}{\Vert h \Vert} = 0 x=(0, 0) 0 h=(h_1, h_2) \lim_{(h_1, h_2) \to 0} \frac{\Vert f((0, 0)+(h_1, h_2)) - f(0,0) - A(h_1, h_2) \Vert}{\Vert (h_1, h_2) \Vert}=\lim_{(h_1, h_2) \to 0} \frac{\Vert f(h_1, h_2) - A(h_1, h_2) \Vert}{\sqrt{h_1^2+h_2^2}} f(h_1, h_2) A(h_1, h_2)","['real-analysis', 'calculus', 'analysis', 'multivariable-calculus', 'continuity']"
42,Proving that $|f(x+h)-f(x)|\geq c|h|$ for every $x\in K$ compact set,Proving that  for every  compact set,|f(x+h)-f(x)|\geq c|h| x\in K,"Let $f: U \rightarrow \mathbb R^n$ be a $C^1$ function defined over an open set $U\subset \mathbb R^m$ . If $K\subset U$ is a compact set such that for every $x\in K$ the associated linear operator $f'(x): \mathbb R^m \rightarrow \mathbb R^n$ is injective, then exists $c>0, \delta>0$ such that $$|f(x+h)-f(x)|\geq c|h|$$ for every $x\in K$ and $|h|<\delta$ . What I did so far: Since for every $x \in K$ the operator $f'(x)$ is injective, then it is a bijection between $\mathbb R^m$ and its image. Thus, there must exist a constant $c$ (very likely to be dependent on x) satisfying $|f'(x)h|\geq 2c|h|$ . If I could, somehow, prove that this $c$ holds for every $x\in K$ or its bounded from above, I would be done; because in this case, since $f$ is $C^1$ , it is uniformly differentiable and we obtain $\delta>0 $ such that $|f(x+h)-f(x)-f'(x)h| < c|h|$ for every $|h|<\delta$ , $x\in K$ . Therefore, $|f(x+h)-f(x)|\geq |f'(x)h| - |f(x+h)-f(x)-f'(x)h|\geq 2c|h| - c|h| = c|h|$ and it's over. So the problem here is pretty much finding an argument to control, from above, the set of constants $c$ satisfying $|f'(x)h|\geq 2c|h|$ . How can I do this? Should I use that $x\mapsto |f'(x)| $ attains a minimum value over $K$ ? I tried this one, but I wasn't successful. Any help, hint?","Let be a function defined over an open set . If is a compact set such that for every the associated linear operator is injective, then exists such that for every and . What I did so far: Since for every the operator is injective, then it is a bijection between and its image. Thus, there must exist a constant (very likely to be dependent on x) satisfying . If I could, somehow, prove that this holds for every or its bounded from above, I would be done; because in this case, since is , it is uniformly differentiable and we obtain such that for every , . Therefore, and it's over. So the problem here is pretty much finding an argument to control, from above, the set of constants satisfying . How can I do this? Should I use that attains a minimum value over ? I tried this one, but I wasn't successful. Any help, hint?","f: U \rightarrow \mathbb R^n C^1 U\subset \mathbb R^m K\subset U x\in K f'(x): \mathbb R^m \rightarrow \mathbb R^n c>0, \delta>0 |f(x+h)-f(x)|\geq c|h| x\in K |h|<\delta x \in K f'(x) \mathbb R^m c |f'(x)h|\geq 2c|h| c x\in K f C^1 \delta>0  |f(x+h)-f(x)-f'(x)h| < c|h| |h|<\delta x\in K |f(x+h)-f(x)|\geq |f'(x)h| - |f(x+h)-f(x)-f'(x)h|\geq 2c|h| - c|h| = c|h| c |f'(x)h|\geq 2c|h| x\mapsto |f'(x)|  K","['real-analysis', 'analysis']"
43,Does $\sum_{n=1}^{\infty} a_n$ is absolutely convergent $\Rightarrow$ $\sum_{n=1}^{\infty} a_n\sin(nx)$ is absolutely and uniformly convergent?,Does  is absolutely convergent   is absolutely and uniformly convergent?,\sum_{n=1}^{\infty} a_n \Rightarrow \sum_{n=1}^{\infty} a_n\sin(nx),"I'm proving that if $\sum_{n=1}^{\infty} a_n$ is absolutely convergent $\Rightarrow$ $\sum_{n=1}^{\infty} a_n\sin(nx)$ is absolutely and uniformly convergent. I defined for a set $\mathbb{D} \subset \mathbb{R}$ the following sequence of functions: \begin{equation*} f_n: \mathbb{D} \to \mathbb{R}, \end{equation*} \begin{equation*} \phantom{1000}x \mapsto a_n\sin(nx) \end{equation*} Since $-1 \le \sin(nx) \le 1, $ then $\forall n \in \mathbb{N}, \forall x \in \mathbb{R} : |a_n\sin(nx)| \le |a_n| $ and $\sum_{n=1}^{\infty} a_n$ is absolutely convergent Therefore apliying M-Weierstrass test we got that $\sum_{n=1}^{\infty} a_n\sin(nx)$ is absolutely and uniformly convergent. Is my reasoning correct?",I'm proving that if is absolutely convergent is absolutely and uniformly convergent. I defined for a set the following sequence of functions: Since then and is absolutely convergent Therefore apliying M-Weierstrass test we got that is absolutely and uniformly convergent. Is my reasoning correct?,"\sum_{n=1}^{\infty} a_n \Rightarrow \sum_{n=1}^{\infty} a_n\sin(nx) \mathbb{D} \subset \mathbb{R} \begin{equation*}
f_n: \mathbb{D} \to \mathbb{R},
\end{equation*} \begin{equation*}
\phantom{1000}x \mapsto a_n\sin(nx)
\end{equation*} -1 \le \sin(nx) \le 1,  \forall n \in \mathbb{N}, \forall x \in \mathbb{R} : |a_n\sin(nx)| \le |a_n|  \sum_{n=1}^{\infty} a_n \sum_{n=1}^{\infty} a_n\sin(nx)","['real-analysis', 'sequences-and-series', 'analysis', 'uniform-convergence', 'absolute-convergence']"
44,"If $\nu \ll \mu$, there exists $E \in \cal F$ and $n \geq 1$ such that $\nu(E) >0$ and $n^{-1} \mu(A) \leq \nu(A) \leq n\mu(A)$ for all $A \subset E$.","If , there exists  and  such that  and  for all .",\nu \ll \mu E \in \cal F n \geq 1 \nu(E) >0 n^{-1} \mu(A) \leq \nu(A) \leq n\mu(A) A \subset E,"I'm thinking about the following question from an old prelim: Let $(X, \cal{F})$ be a measure space with $\sigma$ -finite measures $\mu$ and $\nu$ (it does not say whether these are signed measures). If $\nu \ll \mu$ and $\nu \neq 0$ , then there exists $E \in \cal F$ and $n\in \mathbb{N}$ such that $\nu(E) >0$ and $n^{-1} \mu(A) \leq \nu(A) \leq n\mu(A)$ for all $A \in \cal{F}$ with $A \subset E$ . Let's assume that these are positive measures. Since both are $\sigma$ -finite and $\nu \ll \mu$ , we can choose and extended $\mu$ -integrable function $f:X \to \mathbb{R}$ such that $d\nu = fd\mu$ . Then we hope to find $n\geq 1$ and $E \in \cal{F}$ such that $$ n^{-1} \mu(A) \leq \int_A f d\mu \leq n\mu(A) $$ for all measurable $A \subset E$ . Since $n \mu(A) = \int_A n d\mu$ and $n^{-1}\mu(A) = \int_A n^{-1} d\mu$ , I thought about looking at the set $E_n= \{ x \in X: n^{-1} \leq f(x) \leq n\}$ for appropriately chosen $n$ , but I'm not sure if this is the right idea.","I'm thinking about the following question from an old prelim: Let be a measure space with -finite measures and (it does not say whether these are signed measures). If and , then there exists and such that and for all with . Let's assume that these are positive measures. Since both are -finite and , we can choose and extended -integrable function such that . Then we hope to find and such that for all measurable . Since and , I thought about looking at the set for appropriately chosen , but I'm not sure if this is the right idea.","(X, \cal{F}) \sigma \mu \nu \nu \ll \mu \nu \neq 0 E \in \cal F n\in \mathbb{N} \nu(E) >0 n^{-1} \mu(A) \leq \nu(A) \leq n\mu(A) A \in \cal{F} A \subset E \sigma \nu \ll \mu \mu f:X \to \mathbb{R} d\nu = fd\mu n\geq 1 E \in \cal{F} 
n^{-1} \mu(A) \leq \int_A f d\mu \leq n\mu(A)
 A \subset E n \mu(A) = \int_A n d\mu n^{-1}\mu(A) = \int_A n^{-1} d\mu E_n= \{ x \in X: n^{-1} \leq f(x) \leq n\} n","['analysis', 'measure-theory']"
45,Derivative of generalized Taylor expansion of a function between Banach Spaces,Derivative of generalized Taylor expansion of a function between Banach Spaces,,"Let $E$ and $F$ be Banach spaces and let $f: E \to F$ be a $n+1$ times differentiable function. We define for a given $y\in E$ the Taylor expansion of $f$ as the following: $$ T_n(x,y)=\displaystyle\sum_{k=0}^{n}{\dfrac{d^kf_y(x-y)^k}{k!}} $$ where in this case we use the abusive notation of $(x-y)^k$ as the $k$ -tuple with all entries being $x-y$ . We have that $T_n(x,y): E \to F$ with respect to $x$ (with $y$ and $n$ being constant). Question . What is the derivative of this series at the point $y$ ?  ( $d_yT_n(x,y)=?$ ) Here is what I've worked out so far. By linearity of the derivative the problem becomes one of finding the derivative of $d^kf_y(x-y)^k$ with respect to $x$ , if I am not mistaken. In this case we have that $d^kf_y(x-y)^k$ is a multilinear map which means it has derivative equal to the map that sends a point $(s_1,s_2,....,s_k)$ to : $\displaystyle\sum_{i=1}^{k}{d^kf_y(x-y,...,s_i,...,x-y)}$ . using the symmetry of higher derivatives we get that this derivative is equal to the map $k \cdot d^kf_y(x-y,....,x-y,s)$ .  I believe this leads to a form similar to the derivative of the Taylor polynomial for real functions. Is this the correct way to approach this problem? Am I taking the derivative on the wrong variable? Any help is appreciated.","Let and be Banach spaces and let be a times differentiable function. We define for a given the Taylor expansion of as the following: where in this case we use the abusive notation of as the -tuple with all entries being . We have that with respect to (with and being constant). Question . What is the derivative of this series at the point ?  ( ) Here is what I've worked out so far. By linearity of the derivative the problem becomes one of finding the derivative of with respect to , if I am not mistaken. In this case we have that is a multilinear map which means it has derivative equal to the map that sends a point to : . using the symmetry of higher derivatives we get that this derivative is equal to the map .  I believe this leads to a form similar to the derivative of the Taylor polynomial for real functions. Is this the correct way to approach this problem? Am I taking the derivative on the wrong variable? Any help is appreciated.","E F f: E \to F n+1 y\in E f 
T_n(x,y)=\displaystyle\sum_{k=0}^{n}{\dfrac{d^kf_y(x-y)^k}{k!}}
 (x-y)^k k x-y T_n(x,y): E \to F x y n y d_yT_n(x,y)=? d^kf_y(x-y)^k x d^kf_y(x-y)^k (s_1,s_2,....,s_k) \displaystyle\sum_{i=1}^{k}{d^kf_y(x-y,...,s_i,...,x-y)} k \cdot d^kf_y(x-y,....,x-y,s)","['analysis', 'taylor-expansion', 'banach-spaces', 'normed-spaces']"
46,"Rudin, Riesz Representation Step X: Why do we need $|a|$?","Rudin, Riesz Representation Step X: Why do we need ?",|a|,"My question has to do with the inequalities at the end. However, I will summarize the step to give some context to my question. We want to show that for some complex functional $\Lambda $ on $C_c(X) $ , that $\Lambda f \le \int_X f d \mu$ when $f$ is real. Let $K$ be the support of $f$ .  Choose $[a, b]$ to be an interval containing the range of $f$ .  choose $\epsilon > 0$ and choose $y_i$ such that $y_i - y_{i-1} < \epsilon$ and $$y_0 < a < y_1 < \cdots < y_n = b.$$ Let $E_i = \{ x : y_{i-1} < f(x) \le y_i \} \cap K$ .  Then there are open sets $E_i \subset V_i$ such that $$\mu (V_i) < \mu (E_i) + \frac{\epsilon}{n}$$ and $f(x) < y_i + \epsilon$ for $x \in V_i$ .  Then using previous steps we find functions $h_i \prec V_i$ such that $\sum h_i = 1$ .  Then $\mu (K) \le \sum \Lambda h_i$ , and $h_i f \le (y_i + \epsilon)h_i$ .  Now we get a long string of equalities and inequalities. $$\hspace{-.5 in}\Lambda f = \sum^n \Lambda (h_if) \le \sum^n (y_i + \epsilon) \Lambda h_i \\  \hspace{.15in}= \sum ^n(|a| + y_i + \epsilon)\Lambda h_i - |a|\sum^n \Lambda h_i \\ \hspace{.55in}\le \sum^n(|a| + y_i + \epsilon)[\mu (E_i) + \frac{\epsilon}{n}] - |a| \mu(K) \\ \hspace{1.16in}= \sum^n(y_i - \epsilon)\mu(E_i) + 2\epsilon \mu(K) + \frac{\epsilon}{n}\sum^n(|a| + y_i + \epsilon) \\ \hspace{.15in}\le \int_X f d \mu + \epsilon[2 \mu(K) + |a| + b + \epsilon].$$ since this is true for any $\epsilon > 0$ , we have $$\Lambda f \le \int_x f d \mu.$$ My question is why do we inject $|a|$ into the inequality? What purpose does it serve? It appears to me that if we never introduced $|a|$ , second sum on line four would be $\frac{\epsilon}{n}\sum(y_i + \epsilon)$ which is less than $\frac{\epsilon}{n} \sum(b + \epsilon) = \epsilon(b + \epsilon.)$","My question has to do with the inequalities at the end. However, I will summarize the step to give some context to my question. We want to show that for some complex functional on , that when is real. Let be the support of .  Choose to be an interval containing the range of .  choose and choose such that and Let .  Then there are open sets such that and for .  Then using previous steps we find functions such that .  Then , and .  Now we get a long string of equalities and inequalities. since this is true for any , we have My question is why do we inject into the inequality? What purpose does it serve? It appears to me that if we never introduced , second sum on line four would be which is less than","\Lambda  C_c(X)  \Lambda f \le \int_X f d \mu f K f [a, b] f \epsilon > 0 y_i y_i - y_{i-1} < \epsilon y_0 < a < y_1 < \cdots < y_n = b. E_i = \{ x : y_{i-1} < f(x) \le y_i \} \cap K E_i \subset V_i \mu (V_i) < \mu (E_i) + \frac{\epsilon}{n} f(x) < y_i + \epsilon x \in V_i h_i \prec V_i \sum h_i = 1 \mu (K) \le \sum \Lambda h_i h_i f \le (y_i + \epsilon)h_i \hspace{-.5 in}\Lambda f = \sum^n \Lambda (h_if) \le \sum^n (y_i + \epsilon) \Lambda h_i \\
 \hspace{.15in}= \sum ^n(|a| + y_i + \epsilon)\Lambda h_i - |a|\sum^n \Lambda h_i \\
\hspace{.55in}\le \sum^n(|a| + y_i + \epsilon)[\mu (E_i) + \frac{\epsilon}{n}] - |a| \mu(K) \\
\hspace{1.16in}= \sum^n(y_i - \epsilon)\mu(E_i) + 2\epsilon \mu(K) + \frac{\epsilon}{n}\sum^n(|a| + y_i + \epsilon) \\
\hspace{.15in}\le \int_X f d \mu + \epsilon[2 \mu(K) + |a| + b + \epsilon]. \epsilon > 0 \Lambda f \le \int_x f d \mu. |a| |a| \frac{\epsilon}{n}\sum(y_i + \epsilon) \frac{\epsilon}{n} \sum(b + \epsilon) = \epsilon(b + \epsilon.)","['functional-analysis', 'analysis', 'proof-explanation', 'lebesgue-integral', 'riesz-representation-theorem']"
47,Countability of removable discontinuities,Countability of removable discontinuities,,I saw a problem of proving that number of removable discontinuities in a function is countable. I was not able to prove it and tried many things. Can anyone do it?  Thanks in advance.,I saw a problem of proving that number of removable discontinuities in a function is countable. I was not able to prove it and tried many things. Can anyone do it?  Thanks in advance.,,"['real-analysis', 'analysis', 'continuity']"
48,How to derive the Nautilus Gears equation?,How to derive the Nautilus Gears equation?,,"First have a look at some video on Nautilus Gears, e.g. https://www.youtube.com/watch?v=5Ex_Drh6Rpo . I want to derive the formula for this curve, which I know is just the logarithmic/exponential spiral. I can think of 3 conditions: The sum $r(\theta)+r(\phi)$ should be constant, where $\theta$ and $\phi$ are the corresponding angles over which the gears are rotated. The arc length of the curve between $0$ and $\theta$ should be equal to the arc length of the curve between $\phi$ and $2\pi$ for corresponding $\theta$ and $\phi$ . The tangent lines of the two gears should be equal when touching. It's easy to check that a logarithmic spiral fulfills these 3 conditions, but I want to derive the equation as a solution. As commented below, probably many solutions exist, but the above 3 conditions will be common to all. So I'm interested in the general differential equation and solutions, out of which the logarithmic spiral is only one particular solution. What additional constraints give rise to that log-spiral? Note that the ""teeth"" on the spiral can be neglected. Sorry for the lousy drawing; I did my best...","First have a look at some video on Nautilus Gears, e.g. https://www.youtube.com/watch?v=5Ex_Drh6Rpo . I want to derive the formula for this curve, which I know is just the logarithmic/exponential spiral. I can think of 3 conditions: The sum should be constant, where and are the corresponding angles over which the gears are rotated. The arc length of the curve between and should be equal to the arc length of the curve between and for corresponding and . The tangent lines of the two gears should be equal when touching. It's easy to check that a logarithmic spiral fulfills these 3 conditions, but I want to derive the equation as a solution. As commented below, probably many solutions exist, but the above 3 conditions will be common to all. So I'm interested in the general differential equation and solutions, out of which the logarithmic spiral is only one particular solution. What additional constraints give rise to that log-spiral? Note that the ""teeth"" on the spiral can be neglected. Sorry for the lousy drawing; I did my best...",r(\theta)+r(\phi) \theta \phi 0 \theta \phi 2\pi \theta \phi,"['geometry', 'analysis', 'differential-geometry']"
49,How much multivariable calculus do I need to read Evans’ PDE book?,How much multivariable calculus do I need to read Evans’ PDE book?,,"I am currently in a PDE class using Evans’ text covering roughly chapters 1-6 and have around two weeks off, in which I would like to further my understanding of prerequisite material if possible. I have some experience with functional analysis and have worked through Rudin’s first two books, save for chapter 10 in Rudin’s first text, a chapter on integration of differential forms. However, I have only taken a non rigorous course in multivariable calculus, so my lack of familiarity with multivariable calculus concepts in general, as well as those referenced in sections C.1-C.4. of the appendix in Evans, which cover Green’s formulas, integration by parts, and the coarea formula, have hindered my understanding of the text. For those who have worked through the PDE text, which references would be most practical to supplement my weak areas well enough given the time constraints and background? I have seen similar questions asked and long textbooks that cover multivariable calculus as a whole but spend a long time building up elementary concepts are usually recommended. For now, would it be sufficient to just review chapter 9 (which covers some basic multivariable calculus) and read chapter 10 in Rudin’s first text?","I am currently in a PDE class using Evans’ text covering roughly chapters 1-6 and have around two weeks off, in which I would like to further my understanding of prerequisite material if possible. I have some experience with functional analysis and have worked through Rudin’s first two books, save for chapter 10 in Rudin’s first text, a chapter on integration of differential forms. However, I have only taken a non rigorous course in multivariable calculus, so my lack of familiarity with multivariable calculus concepts in general, as well as those referenced in sections C.1-C.4. of the appendix in Evans, which cover Green’s formulas, integration by parts, and the coarea formula, have hindered my understanding of the text. For those who have worked through the PDE text, which references would be most practical to supplement my weak areas well enough given the time constraints and background? I have seen similar questions asked and long textbooks that cover multivariable calculus as a whole but spend a long time building up elementary concepts are usually recommended. For now, would it be sufficient to just review chapter 9 (which covers some basic multivariable calculus) and read chapter 10 in Rudin’s first text?",,"['analysis', 'multivariable-calculus']"
50,$C_0(M)^*$ where $C_0(M)$ is the space of functions vanishing at infinity can be identified with the set of regular Radon measures.,where  is the space of functions vanishing at infinity can be identified with the set of regular Radon measures.,C_0(M)^* C_0(M),"Let $M$ be a locally compact Hausdorﬀ space. A continuous real valued function $f \colon M \to \mathbb R$ is said to vanish at inﬁnity if, for every $\epsilon > 0$ , there exists a compact set $K \subset M$ such that $$\sup_{x\in M-K} |f(x)| < \epsilon$$ Denote by $C_0(M)$ the space of all continuous functions $f \colon M \to \mathbb R$ that vanish at inﬁnity (see Exercise 3.2.10). (a) Prove that $C_0(M)$ is a Banach space with the supremum norm. no problem here ''(b)The dual space $C_0(M)^∗$ can be identiﬁed with the space $\mathcal{M}(M)$ of signed Radon measures on M with the norm (1.1.4) [They refer to the total variation to be the norm], by the Riesz Representation Theorem (see [75, Thm. 3.15 & Ex. 3.35]). Here a signed Radon measure on $M$ is a signed Borel measure μ with the property that, for each Borel set $B \subset M$ and each $\epsilon$ >0, there exists a compact set $K \subset B$ such that | $\mu(A)−\mu(A \cap K)| <\epsilon$ for every Borel set $A \subset B$ .'' What exactly am I being asked to do here, other than quote the result of Riesz? I'm very confused here.  What exactly does it mean to prove spaces ''can be identified'' with another? (c) Prove that the map $\delta \colon M \to C_0(M)^∗$ , which assigns to each $x \in M$ the bounded linear functional $\delta_x \colon C_0(M) \to \mathbb R$ given by $\delta_x(f) := f(x)$ for $f \in C_0(M)$ , is a homeomorphism onto its image $\delta(M) \subset C_0({M})^∗$ , equipped with the weak $^*$ topology. Under the identiﬁcation in (b) this image is contained in the set $P(M) := \{\mu \in \mathcal{M}(M):  \mu \ge 0, \|\mu\| = \mu(M) = 1\}$ of Radon probability measures. Determine the weak $^*$ closure of the set $\delta(M) = \{ \delta_x | x \in M \} \subset$ $P(M)$ .","Let be a locally compact Hausdorﬀ space. A continuous real valued function is said to vanish at inﬁnity if, for every , there exists a compact set such that Denote by the space of all continuous functions that vanish at inﬁnity (see Exercise 3.2.10). (a) Prove that is a Banach space with the supremum norm. no problem here ''(b)The dual space can be identiﬁed with the space of signed Radon measures on M with the norm (1.1.4) [They refer to the total variation to be the norm], by the Riesz Representation Theorem (see [75, Thm. 3.15 & Ex. 3.35]). Here a signed Radon measure on is a signed Borel measure μ with the property that, for each Borel set and each >0, there exists a compact set such that | for every Borel set .'' What exactly am I being asked to do here, other than quote the result of Riesz? I'm very confused here.  What exactly does it mean to prove spaces ''can be identified'' with another? (c) Prove that the map , which assigns to each the bounded linear functional given by for , is a homeomorphism onto its image , equipped with the weak topology. Under the identiﬁcation in (b) this image is contained in the set of Radon probability measures. Determine the weak closure of the set .","M f \colon M \to \mathbb R \epsilon > 0 K \subset M \sup_{x\in M-K} |f(x)| < \epsilon C_0(M) f \colon M \to \mathbb R C_0(M) C_0(M)^∗ \mathcal{M}(M) M B \subset M \epsilon K \subset B \mu(A)−\mu(A \cap K)| <\epsilon A \subset B \delta \colon M \to C_0(M)^∗ x \in M \delta_x \colon C_0(M) \to \mathbb R \delta_x(f) := f(x) f \in C_0(M) \delta(M) \subset C_0({M})^∗ ^* P(M) := \{\mu \in \mathcal{M}(M): 
\mu \ge 0, \|\mu\| = \mu(M) = 1\} ^* \delta(M) = \{ \delta_x | x \in M \} \subset P(M)","['general-topology', 'functional-analysis', 'analysis', 'topological-vector-spaces']"
51,Real Analysis textbook indecision [duplicate],Real Analysis textbook indecision [duplicate],,"This question already has an answer here : Selecting the Real Analysis Textbooks (1 answer) Closed 3 years ago . What's the best book to study analysis after finishing Spivak's Calculus? I thought about Rudin's Principles of Mathematical Analysis(Which I guess would be much boring for me, but I don't have it so I can't tell if my supposition is right or not), Stein and Shakarchi's Real Analysis, Terence Tao's Analysis(if you recommend it, tell me which edition is the 'right', please) or Pugh's Real Mathematical Analysis? Additional info about my purpose:  - I tend to seek elegance in proofs.  - I want to grasp concepts the most deeply possible. - I don't like books that just jump steps without clear explanation, but I don't like books that are boring(books that focus too much in rigor, in the steps, in the ""you can prove it this way"". I like rigor and it's what - - - I'm seeking, but sometimes authors make it boring. I don't know if I made me intelligible).  - However, I want books that make me try to 'rediscover the subject';  mean, books that make me think hard on the subject even before he explains the matter.  - Books with super hard exercises are welcome.","This question already has an answer here : Selecting the Real Analysis Textbooks (1 answer) Closed 3 years ago . What's the best book to study analysis after finishing Spivak's Calculus? I thought about Rudin's Principles of Mathematical Analysis(Which I guess would be much boring for me, but I don't have it so I can't tell if my supposition is right or not), Stein and Shakarchi's Real Analysis, Terence Tao's Analysis(if you recommend it, tell me which edition is the 'right', please) or Pugh's Real Mathematical Analysis? Additional info about my purpose:  - I tend to seek elegance in proofs.  - I want to grasp concepts the most deeply possible. - I don't like books that just jump steps without clear explanation, but I don't like books that are boring(books that focus too much in rigor, in the steps, in the ""you can prove it this way"". I like rigor and it's what - - - I'm seeking, but sometimes authors make it boring. I don't know if I made me intelligible).  - However, I want books that make me try to 'rediscover the subject';  mean, books that make me think hard on the subject even before he explains the matter.  - Books with super hard exercises are welcome.",,"['real-analysis', 'analysis', 'reference-request', 'book-recommendation']"
52,Continuity over a Compact Implies Uniform Continuity,Continuity over a Compact Implies Uniform Continuity,,"There is a well-known theorem in mathematical analysis that says Suppose $f:M\to N$ is a function from a metric space $(M,d_M)$ to another metric space $(N,d_N)$ . Assume that $M$ is compact. Then $f$ is uniformly continuous over $(M,d_M)$ . For now, let us take $M=[a,b]$ , $N=\mathbb{R}$ , $d_M=d_N=|\cdot|$ . I have seen two different proofs for this case. T. A. Apostol, Calculus, Volume 1, 2nd Edition, Page 152, 1967. C. C. Pugh, Real Mathematical Analysis, 2nd Edition, Page 85, 2015. Apostol argues by contradiction using the method of bisections and the least upper bound property. Pugh also explains by contradiction but prefers to use a technique that one of my teachers called it continuous induction to prove that $[a\,\,\,b]$ is sequentially compact and then uses this property to prove the theorem. Both proofs can be found on the pages mentioned above. Recently, I noticed that Pugh has suggested another approach in exercise 43 of chapter 1 on page 52. However, I couldn't riddle it out. Here is the question Prove that a continuous function defined on an interval $[a\,\,\,b]$ is uniformaly continuous. Hint . Let $\epsilon>0$ be given. Think of $\epsilon$ as fixed and consider the sets \begin{align*}A(\delta)&=\{u\in[a,b]\,|\,\text{if}\,x,t\in[a,u]\,\text{and}\,|x-t|<\delta\,\text{then}\,|f(x)-f(t)|<\epsilon\}, \\ A&=\bigcup_{\delta>0}A(\delta). \end{align*} Using the least upper bound property, prove that $b\in A$ . Infer that $f$ is uniformly continuous. Can you shed some light on what Pugh is trying to suggest in the hint? Uniform Continuity In the definition of continuity we have that $$\forall x\in[a,b],\,\,\forall\epsilon>0,\,\,\exists\delta>0,\,\,\forall t\in[a,b]\,\wedge\,|t-x|<\delta\,\implies|f(t)-f(x)|<\epsilon$$ Here the delta depends on $x$ and $\epsilon$ . Now, fix $\epsilon$ and let $\Delta_{\epsilon}$ be the set that contains all values of $\delta$ corresponding to different $x$ 's. Then uniform continuity is just telling us that $\Delta_\epsilon$ has a minimum. Consequently, this means that there is a $\delta$ that works for all $x\in[a,b]$ . This leads to the following definition $$\forall\epsilon>0,\,\,\exists\delta>0,\,\,\forall x\in[a,b],\,\,\forall t\in[a,b]\,\wedge\,|t-x|<\delta\,\implies|f(t)-f(x)|<\epsilon$$ where $\delta$ only depends on $\epsilon$ .","There is a well-known theorem in mathematical analysis that says Suppose is a function from a metric space to another metric space . Assume that is compact. Then is uniformly continuous over . For now, let us take , , . I have seen two different proofs for this case. T. A. Apostol, Calculus, Volume 1, 2nd Edition, Page 152, 1967. C. C. Pugh, Real Mathematical Analysis, 2nd Edition, Page 85, 2015. Apostol argues by contradiction using the method of bisections and the least upper bound property. Pugh also explains by contradiction but prefers to use a technique that one of my teachers called it continuous induction to prove that is sequentially compact and then uses this property to prove the theorem. Both proofs can be found on the pages mentioned above. Recently, I noticed that Pugh has suggested another approach in exercise 43 of chapter 1 on page 52. However, I couldn't riddle it out. Here is the question Prove that a continuous function defined on an interval is uniformaly continuous. Hint . Let be given. Think of as fixed and consider the sets Using the least upper bound property, prove that . Infer that is uniformly continuous. Can you shed some light on what Pugh is trying to suggest in the hint? Uniform Continuity In the definition of continuity we have that Here the delta depends on and . Now, fix and let be the set that contains all values of corresponding to different 's. Then uniform continuity is just telling us that has a minimum. Consequently, this means that there is a that works for all . This leads to the following definition where only depends on .","f:M\to N (M,d_M) (N,d_N) M f (M,d_M) M=[a,b] N=\mathbb{R} d_M=d_N=|\cdot| [a\,\,\,b] [a\,\,\,b] \epsilon>0 \epsilon \begin{align*}A(\delta)&=\{u\in[a,b]\,|\,\text{if}\,x,t\in[a,u]\,\text{and}\,|x-t|<\delta\,\text{then}\,|f(x)-f(t)|<\epsilon\}, \\
A&=\bigcup_{\delta>0}A(\delta).
\end{align*} b\in A f \forall x\in[a,b],\,\,\forall\epsilon>0,\,\,\exists\delta>0,\,\,\forall t\in[a,b]\,\wedge\,|t-x|<\delta\,\implies|f(t)-f(x)|<\epsilon x \epsilon \epsilon \Delta_{\epsilon} \delta x \Delta_\epsilon \delta x\in[a,b] \forall\epsilon>0,\,\,\exists\delta>0,\,\,\forall x\in[a,b],\,\,\forall t\in[a,b]\,\wedge\,|t-x|<\delta\,\implies|f(t)-f(x)|<\epsilon \delta \epsilon","['analysis', 'continuity', 'uniform-continuity']"
53,How to upper bound of $\int_{I} \frac{1} {(1+|x|^2)^{d/2}} dx$?,How to upper bound of ?,\int_{I} \frac{1} {(1+|x|^2)^{d/2}} dx,"Formally, we  know  that $\int \frac{1}{(1+x^2)^{1/2}} dx = \log (x+ \sqrt{1+x^2}) + C$ some constant $C$ . Let $B>0$ and $I=[-B, B)^d \subset \mathbb R^d.$ My Question is : How to compute $I_1:=\int_{I} \frac{1} {(1+|x|^2)^{d/2}} dx$ ? Can we say that $I_1 \leq C_1 (\log 2B)$ ? $C_1$ is some constant. My attempt : $I_1= C_2 \int_0^{B} \frac{1}{(1+r^2)^{d/2}} r^{d-1} dr$ (please correct me if I'm wrong here). Now I might need to invoke now one dimensional formula? Edit : $\int \frac{1}{1+x^2} dx= \arctan x +c$ (this might need to the case $d=2$ )","Formally, we  know  that some constant . Let and My Question is : How to compute ? Can we say that ? is some constant. My attempt : (please correct me if I'm wrong here). Now I might need to invoke now one dimensional formula? Edit : (this might need to the case )","\int \frac{1}{(1+x^2)^{1/2}} dx = \log (x+ \sqrt{1+x^2}) + C C B>0 I=[-B, B)^d \subset \mathbb R^d. I_1:=\int_{I} \frac{1} {(1+|x|^2)^{d/2}} dx I_1 \leq C_1 (\log 2B) C_1 I_1= C_2 \int_0^{B} \frac{1}{(1+r^2)^{d/2}} r^{d-1} dr \int \frac{1}{1+x^2} dx= \arctan x +c d=2","['real-analysis', 'integration', 'analysis', 'multivariable-calculus']"
54,Exercise 4.3.12 in Understand analysis,Exercise 4.3.12 in Understand analysis,,"Let $F \subset R$ be a nonempty closed set and define $g(x) = \inf \{ | x - a | : a \in F \}$ . Show that g is continuous on $R$ . I follow a solution, and this is its approach to prove the statement. First, it proves $\forall x \in R$ , there exists a number $a_x \in F$ st, $g(x) = | x - a_x |$ , and uses the result to state that for a constant $x_0$ and its corresponding $a_0$ , $$ | g(x) - g(x_0) | = \inf \{| |x - a| - |x_0 - a_0| | : \forall a \in F \} \space (1) $$ By triangle inequality, it is straightforward to prove, $$|| x - a | - |x_0 - a_0|| \le |x - x_0 | + |a - a_0|$$ The rest of the proof is obvious following definition of continuity. I got stuck in the equation (1), though I obtained the equation, $$ | g(x) - g(x_0) | = |inf \{ | x - a | : a \in F\} - |x_0 - a_0| |$$ How to prove (1)? I also appreciate if you provide any other solution.","Let be a nonempty closed set and define . Show that g is continuous on . I follow a solution, and this is its approach to prove the statement. First, it proves , there exists a number st, , and uses the result to state that for a constant and its corresponding , By triangle inequality, it is straightforward to prove, The rest of the proof is obvious following definition of continuity. I got stuck in the equation (1), though I obtained the equation, How to prove (1)? I also appreciate if you provide any other solution.",F \subset R g(x) = \inf \{ | x - a | : a \in F \} R \forall x \in R a_x \in F g(x) = | x - a_x | x_0 a_0  | g(x) - g(x_0) | = \inf \{| |x - a| - |x_0 - a_0| | : \forall a \in F \} \space (1)  || x - a | - |x_0 - a_0|| \le |x - x_0 | + |a - a_0|  | g(x) - g(x_0) | = |inf \{ | x - a | : a \in F\} - |x_0 - a_0| |,['analysis']
55,When does equality hold in the triangle inequality?,When does equality hold in the triangle inequality?,,"We consider the supremum norm $\|f\|=\sup_{x\in [a,b]} |f(x)|$ in the space $B([a,b],\mathbb C)$ of all bounded functions $f: [a,b] \rightarrow \mathbb C$ .   We obviously have in general  that $\|f+g\| \leq \|f\|+\|g\|$ for $f,g \in B([a,b],\mathbb C)$ . However, it may happens that for some $f,g$ the equality holds $\|f+g\|=\|f\|+\|g\|$ (for example, but not only then, if $f$ and $g$ are  proportional with a positive constant factor). The problem is: find all $f,g \in B([a,b],\mathbb C)$ such that $\|f+g\|=\|f\|+\|g\|$ . Thanks.","We consider the supremum norm in the space of all bounded functions .   We obviously have in general  that for . However, it may happens that for some the equality holds (for example, but not only then, if and are  proportional with a positive constant factor). The problem is: find all such that . Thanks.","\|f\|=\sup_{x\in [a,b]} |f(x)| B([a,b],\mathbb C) f: [a,b] \rightarrow \mathbb C \|f+g\| \leq \|f\|+\|g\| f,g \in B([a,b],\mathbb C) f,g \|f+g\|=\|f\|+\|g\| f g f,g \in B([a,b],\mathbb C) \|f+g\|=\|f\|+\|g\|","['analysis', 'triangle-inequality']"
56,"Let $f(0) = -1, f(x+y) \leq -f(x)f(y)$, show that $f \text{ continuous in } \mathbb{R} \iff f \text{ continuous in } 0$","Let , show that","f(0) = -1, f(x+y) \leq -f(x)f(y) f \text{ continuous in } \mathbb{R} \iff f \text{ continuous in } 0","Let $f:\mathbb{R} \rightarrow \mathbb{R}, \\ f(0)=-1, \\ f(x+y) \leq -f(x)f(y).$ Show that $f \text{ continuous in } \mathbb{R} \iff f \text{ continuous in } 0$ . $\Rightarrow$ is trivial, as $0 \in \mathbb{R}$ . $\Leftarrow$ is pretty hard for me. You could begin with saying that for all $\varepsilon > 0$ , there is a $\delta > 0$ such that $|f(x)-f(0)|=|f(x)+1| < \varepsilon$ for all $|x| < \delta$ . But how do you go on? Or is there simply a better solution? Thanks in advance!","Let Show that . is trivial, as . is pretty hard for me. You could begin with saying that for all , there is a such that for all . But how do you go on? Or is there simply a better solution? Thanks in advance!","f:\mathbb{R} \rightarrow \mathbb{R}, \\ f(0)=-1, \\ f(x+y) \leq -f(x)f(y). f \text{ continuous in } \mathbb{R} \iff f \text{ continuous in } 0 \Rightarrow 0 \in \mathbb{R} \Leftarrow \varepsilon > 0 \delta > 0 |f(x)-f(0)|=|f(x)+1| < \varepsilon |x| < \delta","['analysis', 'functions', 'continuity', 'epsilon-delta']"
57,Reference Request - Functional Derivatives,Reference Request - Functional Derivatives,,"I am looking for some good references that introduce functional derivatives in a quick but rigorous way. Any suggestions? In addition, I saw somewhere that functional derivatives are related to Frechét derivatives. Is this accurate? How are they related?","I am looking for some good references that introduce functional derivatives in a quick but rigorous way. Any suggestions? In addition, I saw somewhere that functional derivatives are related to Frechét derivatives. Is this accurate? How are they related?",,"['functional-analysis', 'analysis', 'reference-request', 'calculus-of-variations']"
58,"$f$ is measurable. Prove that $f$ has to be constant on $(0,∞)$. [closed]",is measurable. Prove that  has to be constant on . [closed],"f f (0,∞)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $f : (0,\infty) \to \mathbb{R}$ be measurable and $0 < \lambda < 1$ . Suppose that $$f(x + y) = \lambda f(x) + (1 − \lambda)f(y)$$ holds for any $x, y > 0$ . Prove that $f$ has to be constant on $(0,\infty)$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let be measurable and . Suppose that holds for any . Prove that has to be constant on .","f : (0,\infty) \to \mathbb{R} 0 < \lambda < 1 f(x + y) = \lambda f(x) + (1 − \lambda)f(y) x, y > 0 f (0,\infty)","['real-analysis', 'analysis']"
59,Rudin's RCA Lebesgue-Radon-Nikodym,Rudin's RCA Lebesgue-Radon-Nikodym,,"While I was reading the proof of the theorem of Lebesgue-Radon-Nikodym presented in the Rudin's Real and Complex Analysis, there is a part I cannot catch the point clearly. It says: (1) Observe how the completeness of $L^2(\phi)$ was used to guarantee the existence of $g$ . (2) Observe also that although $g$ is defined uniquely as an element of $L^2(\phi)$ , $g$ is determined only a.e. $[\phi]$ as a point function on $X$ . Firstly, I guess that the completeness of $L^2(\phi)$ is necessary to apply Theorem 4.10: Every nonempty, closed, convex set $E$ in a Hilbert space $H$ contains a unique element of smallest norm. Then, we can apply Theorem 4.11 and 4.12 to guarantee that there exists a unique $g\in L^2(\phi)$ such that \begin{equation*} \int_Xfd\lambda = \int_Xfgd\phi \end{equation*} for every $f\in L^2(\phi)$ . It seems quite obvious, so I am not certain if it is all meant in the first sentence. Secondly, it is quite confusing to understand the meaning of ""as a point function on $X$ "" in the second sentence. I understand that the set of equivalence classes of $L^2(\phi)$ is a Hilbert space, so the uniqueness actually applies to the equivalence class of $g$ . Then, what does ""as a point function on $X$ "" mean? Does it mean as a point on $L^2(\phi)$ or as a constant function on X?","While I was reading the proof of the theorem of Lebesgue-Radon-Nikodym presented in the Rudin's Real and Complex Analysis, there is a part I cannot catch the point clearly. It says: (1) Observe how the completeness of was used to guarantee the existence of . (2) Observe also that although is defined uniquely as an element of , is determined only a.e. as a point function on . Firstly, I guess that the completeness of is necessary to apply Theorem 4.10: Every nonempty, closed, convex set in a Hilbert space contains a unique element of smallest norm. Then, we can apply Theorem 4.11 and 4.12 to guarantee that there exists a unique such that for every . It seems quite obvious, so I am not certain if it is all meant in the first sentence. Secondly, it is quite confusing to understand the meaning of ""as a point function on "" in the second sentence. I understand that the set of equivalence classes of is a Hilbert space, so the uniqueness actually applies to the equivalence class of . Then, what does ""as a point function on "" mean? Does it mean as a point on or as a constant function on X?","L^2(\phi) g g L^2(\phi) g [\phi] X L^2(\phi) E H g\in L^2(\phi) \begin{equation*}
\int_Xfd\lambda = \int_Xfgd\phi
\end{equation*} f\in L^2(\phi) X L^2(\phi) g X L^2(\phi)","['analysis', 'measure-theory']"
60,Is this function a non-zero function?,Is this function a non-zero function?,,"Assume that $(a_i,b_i)^T \in \mathbb{R}^2\setminus \{ (0,0) \}$ for $i=1, \dots, n$ and let $s_1, \dots, s_n \in \mathbb{R}\setminus \{0 \}$ be distinct constants. Define the function $$ f(t) = \sum_{i=1}^{n} a_i\sin(s_it) - b_i \cos(s_it). $$ Is this a nonzero function? It looks very much nonzero to me but somehow I cannot prove this. One just has to find a value $t$ such that $f(t) \neq 0$",Assume that for and let be distinct constants. Define the function Is this a nonzero function? It looks very much nonzero to me but somehow I cannot prove this. One just has to find a value such that,"(a_i,b_i)^T \in \mathbb{R}^2\setminus \{ (0,0) \} i=1, \dots, n s_1, \dots, s_n \in \mathbb{R}\setminus \{0 \} 
f(t) = \sum_{i=1}^{n} a_i\sin(s_it) - b_i \cos(s_it).
 t f(t) \neq 0",['analysis']
61,A problem from Analysis On Manifold by Munkres,A problem from Analysis On Manifold by Munkres,,"Let $A$ and $B$ be rectangles in $R^n$ and $R$ ,respectively.Let $S$ be a set contained in $A\times B$ .For each $y\in B$ ,let $$S_y=\{x|x\in A\ \text{and}\ (x,y)\in S\}.$$ We call $S_y$ a cross-section of $S$ .Show that if $S$ is rectifiable ,and if $S_y$ is rectifiable for each $y\in B$ ,then $$v(S)=\int_{y\in B}v(S_y)$$ According to this book,I should give a explanation of some words . Let $S$ be a bounded set in $R^n$ .If the constant function 1 is Riemann integrable over $S$ ,we say that $S$ is rectifiable and we define the ( $n$ -dimensional) volume of $S$ by the equation $$v(S)=\int_{S}1.$$ I have sloved this problem with @Matematleta help,but I find another question in zorich's Analysis(11.4.3.(2)) which can be seen,to some extent, as a inverse proposition of the above problem. If $S_y$ is rectifiable for each $y\in B$ ,and if $v(S_y)$ is integrable over $B$ .Can we assert that in this case the set $S$ is rectifiable ? I think the answer is affirmative,but I have no idea how to prove it.Any help will be thanked.","Let and be rectangles in and ,respectively.Let be a set contained in .For each ,let We call a cross-section of .Show that if is rectifiable ,and if is rectifiable for each ,then According to this book,I should give a explanation of some words . Let be a bounded set in .If the constant function 1 is Riemann integrable over ,we say that is rectifiable and we define the ( -dimensional) volume of by the equation I have sloved this problem with @Matematleta help,but I find another question in zorich's Analysis(11.4.3.(2)) which can be seen,to some extent, as a inverse proposition of the above problem. If is rectifiable for each ,and if is integrable over .Can we assert that in this case the set is rectifiable ? I think the answer is affirmative,but I have no idea how to prove it.Any help will be thanked.","A B R^n R S A\times B y\in B S_y=\{x|x\in A\ \text{and}\ (x,y)\in S\}. S_y S S S_y y\in B v(S)=\int_{y\in B}v(S_y) S R^n S S n S v(S)=\int_{S}1. S_y y\in B v(S_y) B S","['real-analysis', 'integration', 'analysis']"
62,Uniform convergence preserves continuity Proof verification,Uniform convergence preserves continuity Proof verification,,"Proposition: Let $f_n \rightrightarrows f $ and each $f_n$ is continuous at $x_0$ , then $f$ is continuous at $x_0$ I know there are some posts that have proof of this proposition, but I didn't find any that proves it with the sequential definition of continuity, instead, they use the $\delta - \epsilon$ definition. When I saw this problem, I thought it'd be more natural to prove it with the sequential definition of continuity because we are working with sequences. However, I'm not sure if my proof is correct. So here it is. First I want to justify this inequality (triangle inequality with 2 points?) $$|a - b| = |a+c-c+d-d-b| = |(a-d) + (d-c) + (c-b)| \leq |a-d| + |d-c| + |c-b| \tag{1}$$ So, suppose that $f_n: I \to \mathbb{R} \text{ where } I \subseteq \mathbb{R}$ , let $(x_n)$ be a sequence in $I$ such that $x_n \to x_0\text{ , } x_0 \in I$ . Since each $f_n$ is continuous,  we have $f_n(x_n) \to f_n(x_0)$ . So there exists an $N_1$ such that $$ \forall q \geq N_1 \rightarrow |f_n(x_q) - f_n(x_0)| < \epsilon /3$$ Since $f_n \rightrightarrows f$ there exists $N_2$ such that $$\forall n \geq N_2 \rightarrow |f_n(x_q) - f(x_q)| < \epsilon /3 \text{ and } |f_n(x_0) - f(x_0)| < \epsilon /3 \tag{2}$$ Let $N = max(N_1, N_2) $ and in the inequiality $(1)$ , let $a = f(x_q)$ , $b = f(x_0)$ , $c = f_n(x_0)$ and $d = f_n(x_q)$ so $\forall q,n \geq N$ we have $$|f(x_q) - f(x_0)| \leq |f_n(x_q) - f(x_q)| + |f_n(x_q) - f_n(x_0)| + |f_n(x_0) - f(x_0)| < \epsilon $$ (I also used $|a-b| = |b-a|$ ) Thus $f(x_q) \to f(x_0)$ and $f$ is continuous at $x_0$ . Is it correct? I don't feel so sure about it. Also. If the sequence didn't converge uniformly, but only point-wisely, in $(2)$ , there'd be different $N_2, N_3$ that would guarantee those inequalities, so If I chose an $N$ to be equal to the max of those two, I could guarantee those inequalities without the hypothesis of uniform continuity, and I could ""prove"" that point-wise convergence preserves continuity which is false, this bothers me a lot. If the proof is wrong(I think it's wrong), how can I fix it? Thanks for taking the time to read it.","Proposition: Let and each is continuous at , then is continuous at I know there are some posts that have proof of this proposition, but I didn't find any that proves it with the sequential definition of continuity, instead, they use the definition. When I saw this problem, I thought it'd be more natural to prove it with the sequential definition of continuity because we are working with sequences. However, I'm not sure if my proof is correct. So here it is. First I want to justify this inequality (triangle inequality with 2 points?) So, suppose that , let be a sequence in such that . Since each is continuous,  we have . So there exists an such that Since there exists such that Let and in the inequiality , let , , and so we have (I also used ) Thus and is continuous at . Is it correct? I don't feel so sure about it. Also. If the sequence didn't converge uniformly, but only point-wisely, in , there'd be different that would guarantee those inequalities, so If I chose an to be equal to the max of those two, I could guarantee those inequalities without the hypothesis of uniform continuity, and I could ""prove"" that point-wise convergence preserves continuity which is false, this bothers me a lot. If the proof is wrong(I think it's wrong), how can I fix it? Thanks for taking the time to read it.","f_n \rightrightarrows f  f_n x_0 f x_0 \delta - \epsilon |a - b| = |a+c-c+d-d-b| = |(a-d) + (d-c) + (c-b)| \leq |a-d| + |d-c| + |c-b| \tag{1} f_n: I \to \mathbb{R} \text{ where } I \subseteq \mathbb{R} (x_n) I x_n \to x_0\text{ , } x_0 \in I f_n f_n(x_n) \to f_n(x_0) N_1  \forall q \geq N_1 \rightarrow |f_n(x_q) - f_n(x_0)| < \epsilon /3 f_n \rightrightarrows f N_2 \forall n \geq N_2 \rightarrow |f_n(x_q) - f(x_q)| < \epsilon /3 \text{ and } |f_n(x_0) - f(x_0)| < \epsilon /3 \tag{2} N = max(N_1, N_2)  (1) a = f(x_q) b = f(x_0) c = f_n(x_0) d = f_n(x_q) \forall q,n \geq N |f(x_q) - f(x_0)| \leq |f_n(x_q) - f(x_q)| + |f_n(x_q) - f_n(x_0)| + |f_n(x_0) - f(x_0)| < \epsilon  |a-b| = |b-a| f(x_q) \to f(x_0) f x_0 (2) N_2, N_3 N","['calculus', 'sequences-and-series', 'analysis', 'proof-verification', 'uniform-convergence']"
63,Integrability of $\int_{\mathbb{R}^n} \frac{1-\cos(x\cdot y)}{|x|^{n+q}}dx$?,Integrability of ?,\int_{\mathbb{R}^n} \frac{1-\cos(x\cdot y)}{|x|^{n+q}}dx,Define $$I(y) = \int_{\mathbb{R}^n} \frac{1-\cos(x\cdot y)}{|x|^{n+q}}dx$$ where $0<q<2$ and $y\in\mathbb{R}^n \backslash \{ 0\} $ . I'm trying to show that $0<I(y)<\infty$ and $I(y) = c|y|^q$ for some constant c. My attempt We can switch to polar coordinates to deal with the integral in a one-dimensional setting. $$\int_{\mathbb{R}^n} \frac{1-\cos(x\cdot y)}{|x|^{n+q}}dx=\int_{0}^{\infty}\int_{S^{n-1}} \frac{1-\cos(r\cdot y)}{r^{n+q}} r^{n-1}d\sigma (x')dr \\ =\int_{S^{n-1}}\int_{0}^{\infty}\frac{1-\cos(r\cdot y)}{r^{n+q}} r^{n-1}drd\sigma (x') \\ = \text{vol}(S^{n-1})\int_{0}^{\infty} \frac{1-\cos(r\cdot y)}{r^{q+1}}dr$$ I'm not sure where to go from here... I would appreciate any help!,Define where and . I'm trying to show that and for some constant c. My attempt We can switch to polar coordinates to deal with the integral in a one-dimensional setting. I'm not sure where to go from here... I would appreciate any help!,I(y) = \int_{\mathbb{R}^n} \frac{1-\cos(x\cdot y)}{|x|^{n+q}}dx 0<q<2 y\in\mathbb{R}^n \backslash \{ 0\}  0<I(y)<\infty I(y) = c|y|^q \int_{\mathbb{R}^n} \frac{1-\cos(x\cdot y)}{|x|^{n+q}}dx=\int_{0}^{\infty}\int_{S^{n-1}} \frac{1-\cos(r\cdot y)}{r^{n+q}} r^{n-1}d\sigma (x')dr \\ =\int_{S^{n-1}}\int_{0}^{\infty}\frac{1-\cos(r\cdot y)}{r^{n+q}} r^{n-1}drd\sigma (x') \\ = \text{vol}(S^{n-1})\int_{0}^{\infty} \frac{1-\cos(r\cdot y)}{r^{q+1}}dr,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'lebesgue-integral']"
64,"$h(x)=\langle f(x),g(x)\rangle$ is differentiable in $c$ and $Dh(c)u=\langle Df(c)u,g(c)\rangle+\langle f(c),Dg(c)u\rangle$ with $u\in \mathbb{R}^m$.",is differentiable in  and  with .,"h(x)=\langle f(x),g(x)\rangle c Dh(c)u=\langle Df(c)u,g(c)\rangle+\langle f(c),Dg(c)u\rangle u\in \mathbb{R}^m","Let $G\subset\mathbb{R}^m$ where $G$ is open and $c\in G$ . If $f,g:G\to\mathbb{R}^n$ differentiable in $c$ then $h:G\to\mathbb{R}$ defined by $h(x)=\langle f(x),g(x)\rangle$ is differentiable in $c$ and $$Dh(c)u=\langle Df(c)u,g(c)\rangle+\langle f(c),Dg(c)u\rangle$$ with $u\in \mathbb{R}^m$ . I'm somewhat  lost in how to proof this. I know that since $f,g$ are differentiable in $c$ then we have existence of $Df(c)$ and $Dg(c)$ . As well since $\langle\cdot ,\cdot\rangle$ is bilinear , then $D\langle u,v\rangle(x,y)= \langle x,v\rangle+\langle u,y\rangle$ . I have tried connecting these to things but haven't been able to get anywhere.","Let where is open and . If differentiable in then defined by is differentiable in and with . I'm somewhat  lost in how to proof this. I know that since are differentiable in then we have existence of and . As well since is bilinear , then . I have tried connecting these to things but haven't been able to get anywhere.","G\subset\mathbb{R}^m G c\in G f,g:G\to\mathbb{R}^n c h:G\to\mathbb{R} h(x)=\langle f(x),g(x)\rangle c Dh(c)u=\langle Df(c)u,g(c)\rangle+\langle f(c),Dg(c)u\rangle u\in \mathbb{R}^m f,g c Df(c) Dg(c) \langle\cdot ,\cdot\rangle D\langle u,v\rangle(x,y)= \langle x,v\rangle+\langle u,y\rangle","['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
65,"Solve the PDE $xu_y-yu_x=0$ with $u(x,0)=x^2$ using the Method of Characteristics",Solve the PDE  with  using the Method of Characteristics,"xu_y-yu_x=0 u(x,0)=x^2","The following PDE is given: $xu_y-yu_x=0$ with $u(x,0)=x^2$ The following topics; $yU_x-xU_y=1, U(x,0)=0$ Solution of the PDE $yu_x+xu_y=0$ subject to the initial condition $u(x,0) = \exp \left(-\frac{x^2}{2}\right)$ Solving $-yu_x+xu_y = u$ using method of characteristics did not solve my troubles, because the PDE is not equal to 0, and the initial condition is different. I am stuck near the end of the problem. I used the method of characteristics for PDE: $x_t=-y, y_t=x, u_t=0, x(0,s)=s, y(0,s)=0, u(0,s)=s^2$ $x'(t)=-y(t)$ thus $ x''(t)=-y'(t)=-x(t)$ so $x(t)=c_{11}cos(t)+c_{12}sin(t)$ . Using $x(0,s)=s$ we get $c_{11}=s, c_{12}=0$ thus $x(t,s)=s\cos{t}$ Same procedure for $y'(t)=x(t), y''(t)=x'(t)=-y(t)$ thus $y(t,s)=s\cos{t}$ And for $u$ , we get $u(t,s)=s^2$ Now, how exactly do I solve for $t$ and $s$ to write $u$ as a function of $x$ and $y$ (and not $t$ and $s$ )? I cannot solve for $t$ or $s$ using $x$ and $y$ given that $x(t,s)=y(t,s)$ , i.e. they are identical to each other. That is my first question. My second question is: if the initial condition was $u(x,0)=x, x > 0$ instead of $u(x,0)=x^2$ (so no square, and without $x>0$ ), do we agree that it would only change the result for $u$ , i.e. for $x(t,s)$ and $y(t,s)$ , we would get the same result than here. Thank you for taking your time to help me. Edit: As @Mattos correctly points out, $y(t)=s\sin(t)$ . That changes a lot of things. Now, we can use $x^2+y^2=s^2(\cos{x}^2+\sin{x}^2)=s^2=u$ . Thus, $u(x,y)=x^2+y^2$ .","The following PDE is given: with The following topics; $yU_x-xU_y=1, U(x,0)=0$ Solution of the PDE $yu_x+xu_y=0$ subject to the initial condition $u(x,0) = \exp \left(-\frac{x^2}{2}\right)$ Solving $-yu_x+xu_y = u$ using method of characteristics did not solve my troubles, because the PDE is not equal to 0, and the initial condition is different. I am stuck near the end of the problem. I used the method of characteristics for PDE: thus so . Using we get thus Same procedure for thus And for , we get Now, how exactly do I solve for and to write as a function of and (and not and )? I cannot solve for or using and given that , i.e. they are identical to each other. That is my first question. My second question is: if the initial condition was instead of (so no square, and without ), do we agree that it would only change the result for , i.e. for and , we would get the same result than here. Thank you for taking your time to help me. Edit: As @Mattos correctly points out, . That changes a lot of things. Now, we can use . Thus, .","xu_y-yu_x=0 u(x,0)=x^2 x_t=-y, y_t=x, u_t=0, x(0,s)=s, y(0,s)=0, u(0,s)=s^2 x'(t)=-y(t)  x''(t)=-y'(t)=-x(t) x(t)=c_{11}cos(t)+c_{12}sin(t) x(0,s)=s c_{11}=s, c_{12}=0 x(t,s)=s\cos{t} y'(t)=x(t), y''(t)=x'(t)=-y(t) y(t,s)=s\cos{t} u u(t,s)=s^2 t s u x y t s t s x y x(t,s)=y(t,s) u(x,0)=x, x > 0 u(x,0)=x^2 x>0 u x(t,s) y(t,s) y(t)=s\sin(t) x^2+y^2=s^2(\cos{x}^2+\sin{x}^2)=s^2=u u(x,y)=x^2+y^2","['real-analysis', 'analysis']"
66,"Prove there are distinct $x_1,\,x_2,\cdots,\,x_n$ such that $ \sum_{i=1}^n\frac{p_i}{f'(x_i)}=\sum_{i=1}^n p_i. $",Prove there are distinct  such that,"x_1,\,x_2,\cdots,\,x_n  \sum_{i=1}^n\frac{p_i}{f'(x_i)}=\sum_{i=1}^n p_i. ","Suppose $f(x)$ is differentiable on $[0,\,1]$ , $f(0)=0$ , $f(1)=1$ and $p_1,\,p_2,\cdots,\,p_n$ are $n$ positive real numbers. Prove there are distinct $x_1,\,x_2,\cdots,\,x_n$ such that $$ \sum_{i=1}^n\frac{p_i}{f'(x_i)}=\sum_{i=1}^n p_i. $$ I can only prove some special cases.  Let $p=\sum_{i=1}^n p_i$ . It suffice to prove that $\sum_{i=1}^n\frac{p_i}{pf'(x_i)}=1$ . A proper choose is $f'(x_i)=\frac{np_i}{p}$ . From Darboux theorem, if $f'$ is large enough, these values can attain.","Suppose is differentiable on , , and are positive real numbers. Prove there are distinct such that I can only prove some special cases.  Let . It suffice to prove that . A proper choose is . From Darboux theorem, if is large enough, these values can attain.","f(x) [0,\,1] f(0)=0 f(1)=1 p_1,\,p_2,\cdots,\,p_n n x_1,\,x_2,\cdots,\,x_n 
\sum_{i=1}^n\frac{p_i}{f'(x_i)}=\sum_{i=1}^n p_i.
 p=\sum_{i=1}^n p_i \sum_{i=1}^n\frac{p_i}{pf'(x_i)}=1 f'(x_i)=\frac{np_i}{p} f'",['analysis']
67,Applications of this integral equation in Banach space,Applications of this integral equation in Banach space,,"I’m writing a paper on fixed point theorem, as an application of my main results, I will study this equation: \begin{equation}  \left\{\begin{matrix} u(t) &=&\int_{0}^{t} f(s,u(s),v(s))ds\,,\: t\in [0,a]  \\  v(t) &=&\int_{0}^{t} f(s,v(s),u(s))ds\,,\: t\in [0,a] \end{matrix}\right. \end{equation} where $a$ is a real number such that $a>0$ , $E$ a Banach space and $f :[0,a]\times E\times E\rightarrow  E$ a continuous map. Since I am new to doing research, I want to know if there is an application in physics, biology, population dynamics.. of this system - or this kind of system-. Are there any existing textbooks/articles/papers about this kind of equation? Any help would be very much appreciated. Edit: As it t mentionned by @Robert in his answer, the system is equivalent to the system of differential equations $$ \eqalign{u'(t) &= f(t,u(t),v(t))\cr             v'(t) &= f(t, v(t),u(t))\cr}$$ with initial conditions $u(0)=v(0)=0$ .","I’m writing a paper on fixed point theorem, as an application of my main results, I will study this equation: where is a real number such that , a Banach space and a continuous map. Since I am new to doing research, I want to know if there is an application in physics, biology, population dynamics.. of this system - or this kind of system-. Are there any existing textbooks/articles/papers about this kind of equation? Any help would be very much appreciated. Edit: As it t mentionned by @Robert in his answer, the system is equivalent to the system of differential equations with initial conditions .","\begin{equation}
 \left\{\begin{matrix}
u(t) &=&\int_{0}^{t} f(s,u(s),v(s))ds\,,\: t\in [0,a] 
\\ 
v(t) &=&\int_{0}^{t} f(s,v(s),u(s))ds\,,\: t\in [0,a] \end{matrix}\right.
\end{equation} a a>0 E f :[0,a]\times E\times E\rightarrow  E  \eqalign{u'(t) &= f(t,u(t),v(t))\cr
            v'(t) &= f(t, v(t),u(t))\cr} u(0)=v(0)=0","['integration', 'analysis', 'systems-of-equations', 'mathematical-physics']"
68,Determine values for which the general solution converges,Determine values for which the general solution converges,,"Textbook problem. Given the following general solution to a recurrence relation $$z_n = \alpha(1+\sqrt{3})^n + \beta(1-\sqrt{3})^n$$ For which values $\alpha, \beta$ does the solution converge? And determine the order of the rate of convergence for these values. By attempting to plot the sequence in some interval with varying values of $\alpha, \beta$ it seems like it will converge whenever $\alpha=0$ and $\beta = (-\infty, \infty)$ , but how can i go about determining this in a more rigorous way?","Textbook problem. Given the following general solution to a recurrence relation For which values does the solution converge? And determine the order of the rate of convergence for these values. By attempting to plot the sequence in some interval with varying values of it seems like it will converge whenever and , but how can i go about determining this in a more rigorous way?","z_n = \alpha(1+\sqrt{3})^n + \beta(1-\sqrt{3})^n \alpha, \beta \alpha, \beta \alpha=0 \beta = (-\infty, \infty)","['real-analysis', 'analysis']"
69,Characterization of the measures in a cocountable $\sigma$-algebra,Characterization of the measures in a cocountable -algebra,\sigma,"I found an interesting exercise in the preprint of Measure, integration and real analysis of Sheldon Axler . Exercise 12 of section 2C says: Suppose $X$ is a set and $\mathcal{S}$ is the $\sigma $ -algebra of all subsets $E$ of $X$ such that $E$ or $X\setminus E$ is countable. Give a complete description of the set of all measures in $(X,\mathcal{S})$ . My try: in first place every singleton is measurable so for every measure $\mu$ on $(X,\mathcal{S})$ there is a function $f_\mu :X\to [0,\infty ]$ such that $$ \mu (\{x\})=f_\mu (x),\quad \text{ for each }x\in X\tag1 $$ and note that if $A$ is a countable set then $$ \mu (A)=\sum_{x\in A}f_\mu (x)\tag2 $$ so it seems, at first glance, that the sets of all measures in $(X,\mathcal{S})$ can be represented by the set of functions $[0,\infty ]^X$ , and indeed this would be the case if $X$ is countable. Now I will try to go further describing more precisely any measure on $(X,\mathcal{S})$ , so let $P:=\{x\in X: f_\mu (x)>0\}$ and assume that $X$ is uncountable. Case 1: $P$ is countable and $\mu (P)<\infty $ , hence $\mu (P^\complement )=\mu (X)-\mu (P)$ so the measure of $P^\complement $ is determined by choosing a value for $\mu (X)\in[\mu (P),\infty ]$ . Case 2: $P$ is countable and $\mu (P)=\infty $ , hence $\mu (X )=\infty $ and the measure of $P^\complement $ can be chosen arbitrarily in $[0,\infty]$ . Case 3: $P$ is uncountable what implies that if $P$ is measurable then $\mu (P)=\mu (X)=\infty $ , because it can be shown that exists some $\epsilon >0$ such that $f_\mu(P)\cap (\epsilon ,\infty )$ is uncountable. In any case, being $P$ measurable or not, this means that every uncountable measurable set have infinite measure because it have an uncountable subset such that every singleton have positive measure. My questions: Is this characterization correct? For the case 2 I dont have a proof about the consistency of giving to $\mu(P^\complement)$ an arbitrary value on $[0,\infty]$ . If this would be correct, how I can prove it rigorously?","I found an interesting exercise in the preprint of Measure, integration and real analysis of Sheldon Axler . Exercise 12 of section 2C says: Suppose is a set and is the -algebra of all subsets of such that or is countable. Give a complete description of the set of all measures in . My try: in first place every singleton is measurable so for every measure on there is a function such that and note that if is a countable set then so it seems, at first glance, that the sets of all measures in can be represented by the set of functions , and indeed this would be the case if is countable. Now I will try to go further describing more precisely any measure on , so let and assume that is uncountable. Case 1: is countable and , hence so the measure of is determined by choosing a value for . Case 2: is countable and , hence and the measure of can be chosen arbitrarily in . Case 3: is uncountable what implies that if is measurable then , because it can be shown that exists some such that is uncountable. In any case, being measurable or not, this means that every uncountable measurable set have infinite measure because it have an uncountable subset such that every singleton have positive measure. My questions: Is this characterization correct? For the case 2 I dont have a proof about the consistency of giving to an arbitrary value on . If this would be correct, how I can prove it rigorously?","X \mathcal{S} \sigma  E X E X\setminus E (X,\mathcal{S}) \mu (X,\mathcal{S}) f_\mu :X\to [0,\infty ] 
\mu (\{x\})=f_\mu (x),\quad \text{ for each }x\in X\tag1
 A 
\mu (A)=\sum_{x\in A}f_\mu (x)\tag2
 (X,\mathcal{S}) [0,\infty ]^X X (X,\mathcal{S}) P:=\{x\in X: f_\mu (x)>0\} X P \mu (P)<\infty  \mu (P^\complement )=\mu (X)-\mu (P) P^\complement  \mu (X)\in[\mu (P),\infty ] P \mu (P)=\infty  \mu (X )=\infty  P^\complement  [0,\infty] P P \mu (P)=\mu (X)=\infty  \epsilon >0 f_\mu(P)\cap (\epsilon ,\infty ) P \mu(P^\complement) [0,\infty]","['analysis', 'measure-theory', 'proof-verification']"
70,Estimating $ f(N) = \sum_{m\in \mathbb{Z}} H_{m-N}^{(1)}(x) J_m(y)$ when $N$ is large?,Estimating  when  is large?, f(N) = \sum_{m\in \mathbb{Z}} H_{m-N}^{(1)}(x) J_m(y) N,"I want to find an estimate on the magnitude of the following function in terms of $x,y$ and $N$ when $N$ is large: $$ f(N) = \sum_{m\in \mathbb{Z}} H_{m-N}^{(1)}(x) J_m(y), \quad \quad (*) $$ where $H_{m}^{(1)}$ is the Hankel function of the first kind of order $m$ and $J_m$ is the Bessel funtion of order $m$ and $x>y>0$ . I am used to estimating Hankel and Bessel functions when $m$ is large and we can use \begin{align} |J_m(x)| & \sim \sqrt{\frac{1}{2 \pi m}} \bigg(\frac{ex}{2m}\bigg)^m, \\ |H_m^{(1)}(x)| & \sim \sqrt{\frac{2}{\pi m}} \bigg(\frac{ex}{2m}\bigg)^{-m}. \end{align} In particular, this gives $|H_m^{(1)}(x)J_m(y)| \sim \frac{1}{m}\bigg(\frac{y}{x}\bigg)^m$ as $m \to \infty$ . However, it seems the above relations don't seem to be of any use for the more complicated expression $(*)$ . Is there any way of getting an estimate on the function $f(N)$ as $N\to \infty$ or is this an impossible task?","I want to find an estimate on the magnitude of the following function in terms of and when is large: where is the Hankel function of the first kind of order and is the Bessel funtion of order and . I am used to estimating Hankel and Bessel functions when is large and we can use In particular, this gives as . However, it seems the above relations don't seem to be of any use for the more complicated expression . Is there any way of getting an estimate on the function as or is this an impossible task?","x,y N N 
f(N) = \sum_{m\in \mathbb{Z}} H_{m-N}^{(1)}(x) J_m(y), \quad \quad (*)
 H_{m}^{(1)} m J_m m x>y>0 m \begin{align}
|J_m(x)| & \sim \sqrt{\frac{1}{2 \pi m}} \bigg(\frac{ex}{2m}\bigg)^m, \\
|H_m^{(1)}(x)| & \sim \sqrt{\frac{2}{\pi m}} \bigg(\frac{ex}{2m}\bigg)^{-m}.
\end{align} |H_m^{(1)}(x)J_m(y)| \sim \frac{1}{m}\bigg(\frac{y}{x}\bigg)^m m \to \infty (*) f(N) N\to \infty","['analysis', 'inequality', 'partial-differential-equations', 'bessel-functions', 'upper-lower-bounds']"
71,"$h(x)=\min\{f(x),\,g(x)\}$, it's possible that $\int_0^{\infty}h(x)\,dx<+\infty$? $\int_0^\infty f(x)\,dx,\;\int_0^\infty g(x)\,dx$ are divergent.",", it's possible that ?  are divergent.","h(x)=\min\{f(x),\,g(x)\} \int_0^{\infty}h(x)\,dx<+\infty \int_0^\infty f(x)\,dx,\;\int_0^\infty g(x)\,dx","Suppose $f(x),\,g(x)\in C([0,\,\infty))$ which are nonnegative and decreasing, such that $\displaystyle \int_0^\infty f(x)\,dx=\int_0^\infty g(x)\,dx=+\infty$ . Let $h(x)=\min\{f(x),\,g(x)\}$ , it's possible that $\displaystyle\int_0^{\infty}h(x)\,dx<+\infty$ ? I just wander if there is an example? I can't find it.","Suppose which are nonnegative and decreasing, such that . Let , it's possible that ? I just wander if there is an example? I can't find it.","f(x),\,g(x)\in C([0,\,\infty)) \displaystyle \int_0^\infty f(x)\,dx=\int_0^\infty g(x)\,dx=+\infty h(x)=\min\{f(x),\,g(x)\} \displaystyle\int_0^{\infty}h(x)\,dx<+\infty",['analysis']
72,"Prove that $I(t)=\int_{0}^{+\infty}x^tf(x)\,dx $ is defined on $(-1,\,1)$ and has continuous derivative.",Prove that  is defined on  and has continuous derivative.,"I(t)=\int_{0}^{+\infty}x^tf(x)\,dx  (-1,\,1)","Suppose the improper integral $\displaystyle\int_{0}^{+\infty}xf(x)\,dx$ and $\displaystyle\int_{0}^{+\infty}\frac{f(x)}{x}\,dx$ are both convergent, prove that $$ I(t)=\int_{0}^{+\infty}x^tf(x)\,dx $$ is defined on $(-1,\,1)$ and has continuous derivative. If $f(x)$ is nonnegative, then by comparison test, it's easy to prove it. But how to attack the general case?","Suppose the improper integral and are both convergent, prove that is defined on and has continuous derivative. If is nonnegative, then by comparison test, it's easy to prove it. But how to attack the general case?","\displaystyle\int_{0}^{+\infty}xf(x)\,dx \displaystyle\int_{0}^{+\infty}\frac{f(x)}{x}\,dx 
I(t)=\int_{0}^{+\infty}x^tf(x)\,dx
 (-1,\,1) f(x)",['analysis']
73,"What does it mean for a function of two arguments $f: [0, \infty) \times M \to M$ to be continuous?",What does it mean for a function of two arguments  to be continuous?,"f: [0, \infty) \times M \to M","Suppose we have a metric space $(M,d)$ Let $f: [0, \infty) \times M \to M$ . What does it mean for $f$ to be continuous? Here are some suggestions: $f$ is continuous if for every open set $V \subseteq   M$ , $f^{-1}(V)$ is open in the product topolgy of $[0 , \infty) \times M$ , where the product topology is the topology generated by the natural topology on $[0, \infty)$ and the metric topology on $M$ . $f$ is continuous if for all sequences $(t_k, x_k) \to (t,x) $ , $f(t_k,x_k)$ converges to $f(t,x) \in M$ $f$ is continuous if for all $(t,x) \in [0 \times \infty) \times M$ , and for all $\epsilon >0$ , there exists a $\delta >0$ , such that if $(t',x') \in [0, \infty) \times M$ whenever $d((t,x), (t',x')) < \delta \implies  d(f(t,x), f(t',x')) < \epsilon$ Can someone check my definition? And are these conditions equivalent? Note: the last definition is messed up, I just noticed $d((t,x), (t',x'))$ doesn't make sense.","Suppose we have a metric space Let . What does it mean for to be continuous? Here are some suggestions: is continuous if for every open set , is open in the product topolgy of , where the product topology is the topology generated by the natural topology on and the metric topology on . is continuous if for all sequences , converges to is continuous if for all , and for all , there exists a , such that if whenever Can someone check my definition? And are these conditions equivalent? Note: the last definition is messed up, I just noticed doesn't make sense.","(M,d) f: [0, \infty) \times M \to M f f V \subseteq   M f^{-1}(V) [0 , \infty) \times
M [0, \infty) M f (t_k, x_k) \to (t,x)  f(t_k,x_k) f(t,x) \in M f (t,x) \in [0 \times \infty) \times M \epsilon >0 \delta >0 (t',x') \in [0, \infty) \times M d((t,x), (t',x')) < \delta \implies  d(f(t,x), f(t',x')) < \epsilon d((t,x), (t',x'))","['real-analysis', 'general-topology', 'analysis', 'definition']"
74,Local homeomorphism between $\mathbb{R}^2$ and the space of real $2\times2$ matrices of rank one and norm one,Local homeomorphism between  and the space of real  matrices of rank one and norm one,\mathbb{R}^2 2\times2,"Let $\mathcal{M}$ denote the space of all real $2\times 2$ matrices,   equipped with the norm $||A||=\sqrt{\mbox{tr}(A^TA)}$ , for $A\in\mathcal{M}$ (here $A^T$ denotes the transpose of the matrix $A$ and for any $2\times 2$ real matrix $B$ , $\mbox{tr}(B)$ denotes its   trace). Consider the map $F$ from $\mathbb{R}^2$ to $\mathcal{M}$ given by the formula, for any $(s,t)\in\mathbb{R}^2$ : $$F(s,t) =  \frac{1}{2}\cdot\begin{vmatrix}\cos(t)+\cos(s)&&\sin(t)+\sin(s)\\-\sin(t)+\sin(s)&&\cos(t)-\cos(s)\end{vmatrix}.$$ Denote by $\mathcal{N}\subset\mathcal{M}$ the space of all real $2\times2$ matrices of rank one and norm one. Prove that the image of the map $F$ is the space $\mathcal{N}$ and   that the map $F$ is a local homeomorphism to its image (the latter   with the induced topology). My attempt is as follows. Show that $\mbox{Im}(F)=\mathcal{N}$ means to show that $\mbox{Im}(F)\subseteq\mathcal{N}$ and $\mathcal{N}\subseteq\mbox{Im}(F)$ . For arbitrary tuple $(s,t)$ consider $$A = \frac{1}{2}\cdot\begin{vmatrix}\cos(t)+\cos(s)&&\sin(t)+\sin(s)\\-\sin(t)+\sin(s)&&\cos(t)-\cos(s)\end{vmatrix}, ~A\in\mbox{Im}(F).$$ Note that $\det(A)=0$ , therefore rows are linearly dependent and $\dim\mbox{Ker}(A)=1$ ( $\dim\mbox{Ker}(A)\neq2$ since $A$ isn't identically zero), so $\mbox{rk}(A)=1$ follows from the rank-nullity theorem. One can also check directly that $||A||=\sqrt{\mbox{tr}(A^TA)}=1$ , so we have proved that $\mbox{Im}(F)\subseteq\mathcal{N}$ . How can we show another inclusion -- that any $2\times2$ real matrix with rank one and norm one is apparently of that form? Show that $F$ is a local homeomorphism to $\mbox{Im}(F)$ . By definition, homeomorphism is a continuous bijection such that the inverse is also continuous. Here the real issues in my understanding begin. We can show that $F$ is onto by proving that $\mathcal{N}\subseteq\mbox{Im}(F)$ which is essentially my question above. However, how can $F$ be one-to-one and has an inverse if $\det(F)=0$ ? This immediately implies that matrix isn't invertible and has a non-empty kernel. I will very appreciate any help. Thanks in advance.","Let denote the space of all real matrices,   equipped with the norm , for (here denotes the transpose of the matrix and for any real matrix , denotes its   trace). Consider the map from to given by the formula, for any : Denote by the space of all real matrices of rank one and norm one. Prove that the image of the map is the space and   that the map is a local homeomorphism to its image (the latter   with the induced topology). My attempt is as follows. Show that means to show that and . For arbitrary tuple consider Note that , therefore rows are linearly dependent and ( since isn't identically zero), so follows from the rank-nullity theorem. One can also check directly that , so we have proved that . How can we show another inclusion -- that any real matrix with rank one and norm one is apparently of that form? Show that is a local homeomorphism to . By definition, homeomorphism is a continuous bijection such that the inverse is also continuous. Here the real issues in my understanding begin. We can show that is onto by proving that which is essentially my question above. However, how can be one-to-one and has an inverse if ? This immediately implies that matrix isn't invertible and has a non-empty kernel. I will very appreciate any help. Thanks in advance.","\mathcal{M} 2\times 2 ||A||=\sqrt{\mbox{tr}(A^TA)} A\in\mathcal{M} A^T A 2\times 2 B \mbox{tr}(B) F \mathbb{R}^2 \mathcal{M} (s,t)\in\mathbb{R}^2 F(s,t) =
 \frac{1}{2}\cdot\begin{vmatrix}\cos(t)+\cos(s)&&\sin(t)+\sin(s)\\-\sin(t)+\sin(s)&&\cos(t)-\cos(s)\end{vmatrix}. \mathcal{N}\subset\mathcal{M} 2\times2 F \mathcal{N} F \mbox{Im}(F)=\mathcal{N} \mbox{Im}(F)\subseteq\mathcal{N} \mathcal{N}\subseteq\mbox{Im}(F) (s,t) A = \frac{1}{2}\cdot\begin{vmatrix}\cos(t)+\cos(s)&&\sin(t)+\sin(s)\\-\sin(t)+\sin(s)&&\cos(t)-\cos(s)\end{vmatrix}, ~A\in\mbox{Im}(F). \det(A)=0 \dim\mbox{Ker}(A)=1 \dim\mbox{Ker}(A)\neq2 A \mbox{rk}(A)=1 ||A||=\sqrt{\mbox{tr}(A^TA)}=1 \mbox{Im}(F)\subseteq\mathcal{N} 2\times2 F \mbox{Im}(F) F \mathcal{N}\subseteq\mbox{Im}(F) F \det(F)=0","['real-analysis', 'analysis', 'multivariable-calculus']"
75,Least-square fitting to data (sine function) : what is the error of the derived fit parameters?,Least-square fitting to data (sine function) : what is the error of the derived fit parameters?,,"I have a set of data. I want to fit it to a sine function of the form : \begin{equation} f(x)=A sin(\omega x+B)+C \end{equation} I use the least-square method to find the appropriate fit-parameters which are $A$ , $B$ and $C$ . In this method, each term of the cost-function has a weight calculated from the error-bar of each point in my dataset. Now I want to calculate the visibility $V$ for the fitting curve. The visibility is defined by : \begin{equation} V=\frac{A}{C} \end{equation} I obtain a good value of $V=0.95$ , but now I want to know how to calculate $\Delta V$ , the error of the visibility. To get it, I need to know $\Delta A$ and $\Delta C$ . Do you know how to do it ? EDIT : Some people suggested to post the data, so here is the figure on the link below. Figure representing the data Basically, each point has a poissonnian error bar $\Delta Y= \sqrt{Y}$ . I did the weighted least-square method to obtain my fit-function which is the solid line you can see on this plot (there is two data-set actually, red and blue). The area in red/blue represent standard deviation of the distance in errorbar unit from the datapoints to the fit, multiplied by the poissonian error $\sqrt{Y}$ [I don't know if this is okay, maybe it's false to do like that].","I have a set of data. I want to fit it to a sine function of the form : I use the least-square method to find the appropriate fit-parameters which are , and . In this method, each term of the cost-function has a weight calculated from the error-bar of each point in my dataset. Now I want to calculate the visibility for the fitting curve. The visibility is defined by : I obtain a good value of , but now I want to know how to calculate , the error of the visibility. To get it, I need to know and . Do you know how to do it ? EDIT : Some people suggested to post the data, so here is the figure on the link below. Figure representing the data Basically, each point has a poissonnian error bar . I did the weighted least-square method to obtain my fit-function which is the solid line you can see on this plot (there is two data-set actually, red and blue). The area in red/blue represent standard deviation of the distance in errorbar unit from the datapoints to the fit, multiplied by the poissonian error [I don't know if this is okay, maybe it's false to do like that].","\begin{equation}
f(x)=A sin(\omega x+B)+C
\end{equation} A B C V \begin{equation}
V=\frac{A}{C}
\end{equation} V=0.95 \Delta V \Delta A \Delta C \Delta Y= \sqrt{Y} \sqrt{Y}","['analysis', 'mathematical-physics', 'least-squares', 'data-analysis', 'mean-square-error']"
76,"$|f(x,y)| \le K |x-y|$: is there a name for this property?",: is there a name for this property?,"|f(x,y)| \le K |x-y|","Here $f:\mathbb{R}^2 \to \mathbb{R}$ and $K>0$ is a constant. It's a bit like being contractive or Lipschitz, but not the same as either of those.","Here and is a constant. It's a bit like being contractive or Lipschitz, but not the same as either of those.",f:\mathbb{R}^2 \to \mathbb{R} K>0,"['analysis', 'continuity']"
77,Continuity of a general implicit function,Continuity of a general implicit function,,"I have an implicitly defined function $F(g_1(x_1),\dots,g_n(x_n))=0$ , on the $n$ -th dimension euclidean space, bounded and continuous in its $n$ arguments. $g_i$ are also real-valued bounded and continuous functions. What I'm trying to figure out is if I additionally need any conditions to guarantee that the implicit relation $x_i=f_i(x_1,\dots,x_{i-1},x_{i+1},\dots, x_n)$ is continuous in its arguments. I know that if $F$ was differentiable in all of its arguments, by the Implicit Function Theorem, the cross partial derivatives would exists and thus I would have continuity. However, in my case it is not necessarily true that $F$ is differentiable in its arguments. My intuition says that as everything is compositions of continuous functions, the implicit relation among variables should also be, but I want to make sure. Many thanks!","I have an implicitly defined function , on the -th dimension euclidean space, bounded and continuous in its arguments. are also real-valued bounded and continuous functions. What I'm trying to figure out is if I additionally need any conditions to guarantee that the implicit relation is continuous in its arguments. I know that if was differentiable in all of its arguments, by the Implicit Function Theorem, the cross partial derivatives would exists and thus I would have continuity. However, in my case it is not necessarily true that is differentiable in its arguments. My intuition says that as everything is compositions of continuous functions, the implicit relation among variables should also be, but I want to make sure. Many thanks!","F(g_1(x_1),\dots,g_n(x_n))=0 n n g_i x_i=f_i(x_1,\dots,x_{i-1},x_{i+1},\dots, x_n) F F","['real-analysis', 'analysis', 'continuity']"
78,Find the limit without using Lhopital,Find the limit without using Lhopital,,this question came out on my analysis exam: Evaluate $$\lim_{x\to 0}\left(\frac{5^{x^2}+7^{x^2}}{5^x+7^x}\right)^{\frac{1}{x}} $$ I did it using L'hopital rule but is there another way to do this?,this question came out on my analysis exam: Evaluate I did it using L'hopital rule but is there another way to do this?,\lim_{x\to 0}\left(\frac{5^{x^2}+7^{x^2}}{5^x+7^x}\right)^{\frac{1}{x}} ,"['calculus', 'analysis', 'limits-without-lhopital']"
79,Zeros of $ f''$,Zeros of, f'',"Let $ f : \mathbb{R} \to \mathbb{R} $ be a $C^2$ function such that $$ \lim_{x \to \pm \infty}{f(x)} = 0 $$ Prove that $f''$ has at least two zeros. Assume $f$ is not a constant. Than $f$ must have a stationary point, $a$ . Assume it's a max point. Than $f''$ must be negative in a neighborhood of that point. Now let's prove that $f''$ has at least one zero in $[-\infty, a ]$ ... From this my proof gets really messy...","Let be a function such that Prove that has at least two zeros. Assume is not a constant. Than must have a stationary point, . Assume it's a max point. Than must be negative in a neighborhood of that point. Now let's prove that has at least one zero in ... From this my proof gets really messy..."," f : \mathbb{R} \to \mathbb{R}  C^2  \lim_{x \to \pm \infty}{f(x)} = 0  f'' f f a f'' f'' [-\infty, a ]",['analysis']
80,$p_n > 0$ and $p_{n+1} \ge p_n$. Prove $\sum \frac{ p_n-p_{n-1}}{p_np_{n-1}^a}$ is convergent where $a > 0$.,and . Prove  is convergent where .,p_n > 0 p_{n+1} \ge p_n \sum \frac{ p_n-p_{n-1}}{p_np_{n-1}^a} a > 0,$p_n > 0$ and $p_{n+1} \ge p_n$ . Prove that $$\sum_{n=1}^{\infty} \frac{p_n-p_{n-1}}{p_np_{n-1}^a}$$ is convergent where $a > 0$ . I could prove $\sum_{n=1}^{\infty} \frac{p_{n-1} - p_n}{p_n^{1+a}}$ is convergent since $$\sum_{n=1}^{\infty} \frac{p_{n} - p_{n-1}}{p_n^{1+a}} \le \sum_{n=1}^{\infty}\int_{p_{n-1}}^{p_n} \frac{1}{x^{1+a}} \mathrm{d}x = \int_{p_0}^{\infty} \frac{1}{x^{1+a}} \mathrm{d}x $$ So I think this question may somehow be associated with the integral test.,and . Prove that is convergent where . I could prove is convergent since So I think this question may somehow be associated with the integral test.,p_n > 0 p_{n+1} \ge p_n \sum_{n=1}^{\infty} \frac{p_n-p_{n-1}}{p_np_{n-1}^a} a > 0 \sum_{n=1}^{\infty} \frac{p_{n-1} - p_n}{p_n^{1+a}} \sum_{n=1}^{\infty} \frac{p_{n} - p_{n-1}}{p_n^{1+a}} \le \sum_{n=1}^{\infty}\int_{p_{n-1}}^{p_n} \frac{1}{x^{1+a}} \mathrm{d}x = \int_{p_0}^{\infty} \frac{1}{x^{1+a}} \mathrm{d}x ,"['sequences-and-series', 'analysis']"
81,How weakly open imply open?,How weakly open imply open?,,"When I am seeing the proof of the statement ""Let $C$ be a convex set in a normed linear space $X$ . Then $C$ is closed if and only if $C$ is weakly closed."" Proof: Suppose that $C$ is weakly closed. Then $C^c$ , the complement of $C$ is weakly open and hence open . Hence $C$ is closed. my problem is in the 2nd line of the proof written in bold line. How weakly open imply open? Thanks.","When I am seeing the proof of the statement ""Let be a convex set in a normed linear space . Then is closed if and only if is weakly closed."" Proof: Suppose that is weakly closed. Then , the complement of is weakly open and hence open . Hence is closed. my problem is in the 2nd line of the proof written in bold line. How weakly open imply open? Thanks.",C X C C C C^c C C,"['functional-analysis', 'analysis']"
82,Counterexample of weak convergence,Counterexample of weak convergence,,"Let $E$ be a vector space over a field $K$ , $x\in E$ and a sequence $\{x_n \}_{n \in \mathbb{N}} \subset E$ . Question: I need to find a counter example of two different linear functionals $$\psi,\varphi\in E' $$ such that $$\lim_{n \to \infty} \psi(x_n) = \psi(x)$$ but $$\lim_{n \to \infty} \varphi(x_n) \neq \varphi(x)$$ Thanks!","Let be a vector space over a field , and a sequence . Question: I need to find a counter example of two different linear functionals such that but Thanks!","E K x\in E \{x_n \}_{n \in \mathbb{N}} \subset E \psi,\varphi\in E'  \lim_{n \to \infty} \psi(x_n) = \psi(x) \lim_{n \to \infty} \varphi(x_n) \neq \varphi(x)","['functional-analysis', 'analysis', 'convergence-divergence', 'weak-convergence']"
83,$L^2$ and $C^0$ norms combined,and  norms combined,L^2 C^0,"We know that $L^2$ and $C^0$ are complete metric spaces. Now consider for fixed $T$ the space of real-valued processes $X:\Omega\times [0,T] \rightarrow \mathbb R$ (with corresponding $\sigma$ -algebra) such that $$\Vert X \Vert = \mathbb E [\sup_{0 \leq s \leq T} |X_s| ^2] ^{1/2}< \infty$$ Is this a closed complete metric space? My attempt: It must be. This space is a subspace of $L^2$ and therefore every Cauchy sequence in the above sense has $L^2$ limit. However, I don't see if this space is closed (i.e. $L^2$ limit also has finite norm in the above sense). Thank you in advance!","We know that and are complete metric spaces. Now consider for fixed the space of real-valued processes (with corresponding -algebra) such that Is this a closed complete metric space? My attempt: It must be. This space is a subspace of and therefore every Cauchy sequence in the above sense has limit. However, I don't see if this space is closed (i.e. limit also has finite norm in the above sense). Thank you in advance!","L^2 C^0 T X:\Omega\times [0,T] \rightarrow \mathbb R \sigma \Vert X \Vert = \mathbb E [\sup_{0 \leq s \leq T} |X_s| ^2] ^{1/2}< \infty L^2 L^2 L^2","['functional-analysis', 'analysis', 'measure-theory', 'stochastic-processes', 'banach-spaces']"
84,Show directly that if $\{s_n\}$ is a Cauchy sequence then so is $\{|s_n|\}$. Conclude that $\{|s_n|\}$ converges whenever $\{s_n\}$ converges.,Show directly that if  is a Cauchy sequence then so is . Conclude that  converges whenever  converges.,\{s_n\} \{|s_n|\} \{|s_n|\} \{s_n\},"Show directly that if $\{s_n\}$ is a Cauchy sequence then so is $\{|s_n|\}$ . From this conclude that $\{|s_n|\}$ converges whenever $\{s_n\}$ converges. Let $\{s_n\}$ be a Cauchy sequence. Then by definition, for any given $\varepsilon>0$ there exists $m>0$ such that $|s_n-s_m|<\varepsilon$ for all $n\geq m$ . Then we have $$||s_n|-|s_m||\leq|s_n-s_m|$$ Therefore, from the definition $$||s_n|-|s_m||\leq|s_n-s_m|<\varepsilon$$ for all $n\geq m$ . Hence, $\{|s_n|\}$ is a Cauchy sequence. And then to prove that convergence of $\{s_n\}$ implies the convergence of $\{|s_n|\}$ : Let $\varepsilon>0$ . If $\{s_n\}$ converges to $L$ , then there exists $N$ such that $|s_n-L|<\varepsilon$ , whenever $n\geq N$ . Hence, for $n\geq N$ , we have $||s_n|-L|\leq |s_n-L|<\varepsilon$ . Thus $\{|s_n|\}$ converges to $|L|$ . That's how I proved but I'm not sure if I possibly made some mistakes or missed some steps!?","Show directly that if is a Cauchy sequence then so is . From this conclude that converges whenever converges. Let be a Cauchy sequence. Then by definition, for any given there exists such that for all . Then we have Therefore, from the definition for all . Hence, is a Cauchy sequence. And then to prove that convergence of implies the convergence of : Let . If converges to , then there exists such that , whenever . Hence, for , we have . Thus converges to . That's how I proved but I'm not sure if I possibly made some mistakes or missed some steps!?",\{s_n\} \{|s_n|\} \{|s_n|\} \{s_n\} \{s_n\} \varepsilon>0 m>0 |s_n-s_m|<\varepsilon n\geq m ||s_n|-|s_m||\leq|s_n-s_m| ||s_n|-|s_m||\leq|s_n-s_m|<\varepsilon n\geq m \{|s_n|\} \{s_n\} \{|s_n|\} \varepsilon>0 \{s_n\} L N |s_n-L|<\varepsilon n\geq N n\geq N ||s_n|-L|\leq |s_n-L|<\varepsilon \{|s_n|\} |L|,"['real-analysis', 'analysis', 'proof-verification', 'convergence-divergence']"
85,A difficulty in understanding the proof of Riemann Lebesgue lemma.,A difficulty in understanding the proof of Riemann Lebesgue lemma.,,The proof is given below: My questions are: 1- In the second line from below how do we get $2/|\lambda|$ (in the second term) from the line before  it. 2- What is the lemma trying to say?,The proof is given below: My questions are: 1- In the second line from below how do we get (in the second term) from the line before  it. 2- What is the lemma trying to say?,2/|\lambda|,"['real-analysis', 'calculus', 'analysis', 'fourier-analysis', 'fourier-series']"
86,Is $A=\{f(x)\in\mathbb{R} : ||x||=1\}$ an interval if $f:\mathbb{R}^2\mapsto \mathbb{R}$ is continous?,Is  an interval if  is continous?,A=\{f(x)\in\mathbb{R} : ||x||=1\} f:\mathbb{R}^2\mapsto \mathbb{R},"So I have $f:\mathbb{R}^2\mapsto \mathbb{R}$ a continous function and the set $A=\{f(x)\in\mathbb{R} : ||x||=1\}$ I have to prove that A is an interval, but I don't have any idea on how to do it. What specific property of intervals can use to compare it with A and see that A holds this property?","So I have a continous function and the set I have to prove that A is an interval, but I don't have any idea on how to do it. What specific property of intervals can use to compare it with A and see that A holds this property?",f:\mathbb{R}^2\mapsto \mathbb{R} A=\{f(x)\in\mathbb{R} : ||x||=1\},"['real-analysis', 'general-topology', 'analysis']"
87,Does this limit exist on $\mathbb R^2$,Does this limit exist on,\mathbb R^2,"$(x,y) \in \mathbb R^2$ $$\lim_{(x,y)\to(1,1)} \frac{(x-y)^{(x-y)}} {(x-y)}$$ Does the limit above exist? Neither I could compute it nor I could find directions which have different limit values. Can someone help me please? If this limit exists how can I compute it if doesn't exist which directions should I use? Thanks a lot in advance",Does the limit above exist? Neither I could compute it nor I could find directions which have different limit values. Can someone help me please? If this limit exists how can I compute it if doesn't exist which directions should I use? Thanks a lot in advance,"(x,y) \in \mathbb R^2 \lim_{(x,y)\to(1,1)} \frac{(x-y)^{(x-y)}} {(x-y)}","['calculus', 'analysis', 'multivariable-calculus']"
88,Comparing two summable conditions,Comparing two summable conditions,,Let $X$ be a Banach space and $(f_n)_{n\ge1}$ be a sequence in $X^*$ . There are two summable conditions: (1) $\sum_{n\ge1} f_n(x)$ is summable for each x in $X$ ; (2) $\sum_{n\ge1} \phi(f_n)$ is summable for each $\phi$ in $X^{**}$ ; Show that (2) implies (1) and (1) doesn't imply (2)(give a counterexample which satisfies (1) but fails (2)). That (2) implies (1) is trivial. But I don't know how to start with constructing a counterexample. Any help would be appreciated. Thanks in advance!,Let be a Banach space and be a sequence in . There are two summable conditions: (1) is summable for each x in ; (2) is summable for each in ; Show that (2) implies (1) and (1) doesn't imply (2)(give a counterexample which satisfies (1) but fails (2)). That (2) implies (1) is trivial. But I don't know how to start with constructing a counterexample. Any help would be appreciated. Thanks in advance!,X (f_n)_{n\ge1} X^* \sum_{n\ge1} f_n(x) X \sum_{n\ge1} \phi(f_n) \phi X^{**},"['functional-analysis', 'analysis', 'banach-spaces', 'examples-counterexamples']"
89,Proving continuity in the origin $\frac{xy}{\sqrt{x^2 + y^2}}$,Proving continuity in the origin,\frac{xy}{\sqrt{x^2 + y^2}},"Let $g: \mathbb{R^2} \to \mathbb{R}$ . How can I prove that $g$ is continuous in its origin, but not totally differentiable? If I take $$g(\frac{1}{n},\frac{1}{n}) = \frac{1}{n\sqrt{2}} \to 0 \text{ for } n \to \infty $$ Or rather: $$|x,y| \leq \frac{1}{2} (x^2 + y^2) $$ from which we can follow $$|g(x,y)| \leq \frac{1}{2} \sqrt{x^2 + y^2}$$ which proves continuity of $g$ in the origin $(0,0)$ . But how can I show that this function is not total differentiable? Can I do the following estimation? $$|g(x,y) - 0| = |y| \cdot \frac{x \cdot y}{\sqrt{x^2 + y^2}} \leq |y| \cdot 1 = |y| $$ From which it follows, that $$|y| \leq |\sqrt{x^2 + y^2}| = ||(x,y)|| $$","Let . How can I prove that is continuous in its origin, but not totally differentiable? If I take Or rather: from which we can follow which proves continuity of in the origin . But how can I show that this function is not total differentiable? Can I do the following estimation? From which it follows, that","g: \mathbb{R^2} \to \mathbb{R} g g(\frac{1}{n},\frac{1}{n}) = \frac{1}{n\sqrt{2}} \to 0 \text{ for } n \to \infty  |x,y| \leq \frac{1}{2} (x^2 + y^2)  |g(x,y)| \leq \frac{1}{2} \sqrt{x^2 + y^2} g (0,0) |g(x,y) - 0| = |y| \cdot \frac{x \cdot y}{\sqrt{x^2 + y^2}} \leq |y| \cdot 1 = |y|  |y| \leq |\sqrt{x^2 + y^2}| = ||(x,y)|| ","['analysis', 'functions']"
90,"Proving $(x^4-y^4) \cos (\frac{1}{\left\lVert (x,y) \right\rVert^3_2})$ is totally differentiable",Proving  is totally differentiable,"(x^4-y^4) \cos (\frac{1}{\left\lVert (x,y) \right\rVert^3_2})","How can one prove, that this function is totally differentiable on $\mathbb{R^2}$ and not continuous partially differentiable on $\mathbb{R^2}$ ? $$ f(x, y) := \begin{cases} (x^4-y^4)\cos\left(\dfrac{1}{\|(x,y)\|^3_2}\right), & (x, y) \neq (0,0); \\ 0, & (x, y) = (0, 0). \end{cases} $$ I know that to prove the total derivative one first has to check, whether this function is continuous and partially derivable, or not. I also know that I can use the following formula to prove that a function is totally differentiable: $$ \dfrac{f(x, y) - f(0, 0) - \left(\left(\dfrac{\partial f}{\partial x}\right)(0, 0)\left(\dfrac{\partial f}{\partial y}\right)(0, 0)\right)\cdot\left({x-0}\atop{y-0}\right)}{|(x, y) - (0, 0)|} $$ If this formula gives a $0$ , the function is totally differentiable. I am stuck though, since I can't even find out if the function is continuous, or what the total derivative would be. Can I substitute $x^4 = a $ and $y^4 = b$ and then we could follow: $$(a-b)\cos\left(\frac{1}{\left\|\sqrt{\sqrt{(x,y)}} \right\|^3_2}\right) =$$ $$ = (a-b)\cos\left(\frac{1}{\left\| \sqrt{(x,y)} \right\|^1_2}\right)$$ And then maybe l'Hospital (though I don't know how that would work for the denominator) and then calculating the limit for $n\to\infty$ , which would be $0\cdot 1$ (I think, because $\cos\left(\frac{1}{x}\right)\to 1$ for $\lim\to\infty$ ), which gives us $0$ , proving that the function is continuous.","How can one prove, that this function is totally differentiable on and not continuous partially differentiable on ? I know that to prove the total derivative one first has to check, whether this function is continuous and partially derivable, or not. I also know that I can use the following formula to prove that a function is totally differentiable: If this formula gives a , the function is totally differentiable. I am stuck though, since I can't even find out if the function is continuous, or what the total derivative would be. Can I substitute and and then we could follow: And then maybe l'Hospital (though I don't know how that would work for the denominator) and then calculating the limit for , which would be (I think, because for ), which gives us , proving that the function is continuous.","\mathbb{R^2} \mathbb{R^2} 
f(x, y) := \begin{cases} (x^4-y^4)\cos\left(\dfrac{1}{\|(x,y)\|^3_2}\right), & (x, y) \neq (0,0); \\ 0, & (x, y) = (0, 0). \end{cases}
 
\dfrac{f(x, y) - f(0, 0) - \left(\left(\dfrac{\partial f}{\partial x}\right)(0, 0)\left(\dfrac{\partial f}{\partial y}\right)(0, 0)\right)\cdot\left({x-0}\atop{y-0}\right)}{|(x, y) - (0, 0)|}
 0 x^4 = a  y^4 = b (a-b)\cos\left(\frac{1}{\left\|\sqrt{\sqrt{(x,y)}} \right\|^3_2}\right) =  = (a-b)\cos\left(\frac{1}{\left\| \sqrt{(x,y)} \right\|^1_2}\right) n\to\infty 0\cdot 1 \cos\left(\frac{1}{x}\right)\to 1 \lim\to\infty 0","['analysis', 'functions', 'derivatives', 'continuity', 'partial-derivative']"
91,"perimeter of sets. How do $P(E,\Omega)$ and $P(E\cap \Omega,\mathbb{R}^n)$ relate?",perimeter of sets. How do  and  relate?,"P(E,\Omega) P(E\cap \Omega,\mathbb{R}^n)","Let $\Omega$ be an open set in $\mathbb{R}^n$ and $E$ a Borel set. The relative perimeter of $E$ w.r.t. $\Omega$ is defined as $$P(E,\Omega)=\sup\left\{\int_{\Omega}\chi_E(x) \mathrm{div}\boldsymbol{\phi}(x) \, \mathrm{d}x : \boldsymbol{\phi}\in C_c^1(\Omega,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\Omega)}\le 1\right\},$$ see here https://en.wikipedia.org/wiki/Caccioppoli_set . Suppose that $E\cap \Omega$ is non-empty. Is $P(E,\Omega)=P(E\cap \Omega,\mathbb{R}^n)$ ? Or is $P(E,\Omega)=P(E\cap \Omega,\mathbb{R}^n)$ +some (n-1)-dimensional Hausdorff measure? It is $$P(E\cap \Omega,\mathbb{R}^n)=\sup\left\{\int_{\mathbb{R}^n}\chi_{E\cap\Omega}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\}$$ $$=\sup\left\{\int_{\mathbb{R}^n}\chi_{E}\chi_{\Omega}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\}$$ $$=\sup\left\{\int_{\Omega}\chi_{E}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\}$$ From there I conclude $P(E,\Omega)\le P(E\cap \Omega,\mathbb{R}^n)$ . However, does $P(E,\Omega)\ge P(E\cap \Omega,\mathbb{R}^n)$ hold? Or how do the perimeters relate?","Let be an open set in and a Borel set. The relative perimeter of w.r.t. is defined as see here https://en.wikipedia.org/wiki/Caccioppoli_set . Suppose that is non-empty. Is ? Or is +some (n-1)-dimensional Hausdorff measure? It is From there I conclude . However, does hold? Or how do the perimeters relate?","\Omega \mathbb{R}^n E E \Omega P(E,\Omega)=\sup\left\{\int_{\Omega}\chi_E(x) \mathrm{div}\boldsymbol{\phi}(x) \, \mathrm{d}x : \boldsymbol{\phi}\in C_c^1(\Omega,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\Omega)}\le 1\right\}, E\cap \Omega P(E,\Omega)=P(E\cap \Omega,\mathbb{R}^n) P(E,\Omega)=P(E\cap \Omega,\mathbb{R}^n) P(E\cap \Omega,\mathbb{R}^n)=\sup\left\{\int_{\mathbb{R}^n}\chi_{E\cap\Omega}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\} =\sup\left\{\int_{\mathbb{R}^n}\chi_{E}\chi_{\Omega}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\} =\sup\left\{\int_{\Omega}\chi_{E}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\} P(E,\Omega)\le P(E\cap \Omega,\mathbb{R}^n) P(E,\Omega)\ge P(E\cap \Omega,\mathbb{R}^n)","['integration', 'analysis', 'bounded-variation', 'geometric-measure-theory']"
92,Part of the proof of the first optimality condition. Showing that $f'(x*)=0$ for a local minimum $x*$.,Part of the proof of the first optimality condition. Showing that  for a local minimum .,f'(x*)=0 x*,"Let $x^*$ be a local minimum of a differentiable function $f(x)$ , i.e. there exists $r>0$ such that for all $y \in B_n(x^*,r)$ we have $f(y)\ge f(x^*)$ . Since $f$ is differentiable we have $$f(y)=f(x^*)+\langle f'(x^*),y-x^* \rangle + o(\parallel y-x^* \parallel) \ge f(x^*).$$ Thus, for all $\parallel s\parallel =1,$ we have $\langle f'(x^*),s\rangle =0$ . I don't understand how we get this final statement. We basically have $\langle f'(x^*),y-x^* \rangle + o(\parallel y-x^* \parallel) \ge 0$ , but how does this give for all $\parallel s \parallel =1,$ we have $\langle f'(x^*),s\rangle =0$ ?","Let be a local minimum of a differentiable function , i.e. there exists such that for all we have . Since is differentiable we have Thus, for all we have . I don't understand how we get this final statement. We basically have , but how does this give for all we have ?","x^* f(x) r>0 y \in B_n(x^*,r) f(y)\ge f(x^*) f f(y)=f(x^*)+\langle f'(x^*),y-x^* \rangle + o(\parallel y-x^* \parallel) \ge f(x^*). \parallel s\parallel =1, \langle f'(x^*),s\rangle =0 \langle f'(x^*),y-x^* \rangle + o(\parallel y-x^* \parallel) \ge 0 \parallel s \parallel =1, \langle f'(x^*),s\rangle =0","['calculus', 'analysis', 'derivatives']"
93,Exact form $\alpha$ and $dg=\alpha$,Exact form  and,\alpha dg=\alpha,"Let $\alpha= f_1(x_1,x_2)dx_1 + f_2(x_1,x_2)dx_2$ such that $d\alpha=0$ . Define a function $g$ by $$g(x_1,x_2)= \int_{0}^{x_1} f_1(t,x_2) dt + \int_{0}^{x_2} f_2(0,t)dt$$ Show that $dg=\alpha$ By $d\alpha=0$ we get $\dfrac{\partial{f_2}}{\partial{x_1}}-\dfrac{\partial{f_1}}{\partial{x_2}}=0$ and $$dg= \dfrac{\partial}{\partial{x_1}} ( \int_{0}^{x_1} f_1(t,x_2) dt ) dx_1+\dfrac{\partial}{\partial{x_2}}(\int_{0}^{x_1} f_1(t,x_2) dt) dx_2$$ $$+\dfrac{\partial}{\partial{x_1}}(\int_{0}^{x_2} f_2(0,t) dt) dx_1 +\dfrac{\partial}{\partial{x_2}}(\int_{0}^{x_2} f_2(0,t) dt) dx_2$$ By Leibniz integral rule $\dfrac{\partial}{\partial{x_1}} ( \int_{0}^{x_1} f_1(t,x_2) dt ) =f_1(x_1,x_2) + ( \int_{0}^{x_1}\dfrac{\partial}{\partial{x_1}} f_1(t,x_2) dt ) $ $\dfrac{\partial}{\partial{x_2}}(\int_{0}^{x_1} f_1(t,x_2) dt)=(\int_{0}^{x_1} \dfrac{\partial}{\partial{x_2}}f_1(t,x_2) dt)$ $\dfrac{\partial}{\partial{x_1}}(\int_{0}^{x_2} f_2(0,t) dt)=(\int_{0}^{x_2} \dfrac{\partial}{\partial{x_1}}f_2(0,t) dt)$ $\dfrac{\partial}{\partial{x_2}}(\int_{0}^{x_2} f_2(0,t) dt)=f_2(0,x_2)+(\int_{0}^{x_2}\dfrac{\partial}{\partial{x_2}} f_2(0,t) dt)$ Do i have any mistake? And can you continue to compute those and indicate $dg=\alpha$ ?",Let such that . Define a function by Show that By we get and By Leibniz integral rule Do i have any mistake? And can you continue to compute those and indicate ?,"\alpha= f_1(x_1,x_2)dx_1 + f_2(x_1,x_2)dx_2 d\alpha=0 g g(x_1,x_2)= \int_{0}^{x_1} f_1(t,x_2) dt + \int_{0}^{x_2} f_2(0,t)dt dg=\alpha d\alpha=0 \dfrac{\partial{f_2}}{\partial{x_1}}-\dfrac{\partial{f_1}}{\partial{x_2}}=0 dg= \dfrac{\partial}{\partial{x_1}} ( \int_{0}^{x_1} f_1(t,x_2) dt ) dx_1+\dfrac{\partial}{\partial{x_2}}(\int_{0}^{x_1} f_1(t,x_2) dt) dx_2 +\dfrac{\partial}{\partial{x_1}}(\int_{0}^{x_2} f_2(0,t) dt) dx_1 +\dfrac{\partial}{\partial{x_2}}(\int_{0}^{x_2} f_2(0,t) dt) dx_2 \dfrac{\partial}{\partial{x_1}} ( \int_{0}^{x_1} f_1(t,x_2) dt ) =f_1(x_1,x_2) + ( \int_{0}^{x_1}\dfrac{\partial}{\partial{x_1}} f_1(t,x_2) dt )  \dfrac{\partial}{\partial{x_2}}(\int_{0}^{x_1} f_1(t,x_2) dt)=(\int_{0}^{x_1} \dfrac{\partial}{\partial{x_2}}f_1(t,x_2) dt) \dfrac{\partial}{\partial{x_1}}(\int_{0}^{x_2} f_2(0,t) dt)=(\int_{0}^{x_2} \dfrac{\partial}{\partial{x_1}}f_2(0,t) dt) \dfrac{\partial}{\partial{x_2}}(\int_{0}^{x_2} f_2(0,t) dt)=f_2(0,x_2)+(\int_{0}^{x_2}\dfrac{\partial}{\partial{x_2}} f_2(0,t) dt) dg=\alpha","['analysis', 'multivariable-calculus', 'differential-forms', 'differential']"
94,"If $\{f_n\}$ converges uniformly on $(a,b)$, $\{f_{n}(a)\}$ and $\{f_{n}(b)\}$ converge pointwise, then it converges uniformly on $[a,b]$","If  converges uniformly on ,  and  converge pointwise, then it converges uniformly on","\{f_n\} (a,b) \{f_{n}(a)\} \{f_{n}(b)\} [a,b]","Prove that if $\{f_n\}$ converges uniformly on $(a,b)$, $\{f_{n}(a)\}$ and $\{f_{n}(b)\}$ converge pointwise. I want to show that $\{f_n\}$ converges uniformly on $[a,b]$ MY TRIAL: Let $\epsilon>0$, since $\{f_n\}$ converges uniformly on $(a,b)$, then $\exists\,N_1=N(\epsilon)$ s.t. $\forall \,n\geq N$,  $\forall x\in (a,b)$ \begin{align}\left|f_n(x)-f(x)\right|<\epsilon\end{align} Since $\{f_n\}$ converges pointwise when $x=a$, then $\exists\,N_2=N(\epsilon,a)$ s.t. $\forall \,n\geq N$, \begin{align}\left|f_n(a)-f(a)\right|<\epsilon\end{align} Also, $\{f_n\}$ converges pointwise when $x=b$, then $\exists\,N_3=N(\epsilon,b)$ s.t. $\forall \,n\geq N$, \begin{align}\left|f_n(b)-f(b)\right|<\epsilon\end{align} Hence, $\forall \,n\geq \max\{N_1,N_2,N_3\},$ $$\lim\limits_{n\to \infty}f_n(x)=\begin{cases}f(a) & x=a,\\f(x)&x\in(a,b),\\f(b)&x=b\end{cases}$$ Hence, $\{f_n\}$ converges uniformly on $[a,b]$. Please, I'm I right? If no, an alternative proof will be highly regarded. Thanks!","Prove that if $\{f_n\}$ converges uniformly on $(a,b)$, $\{f_{n}(a)\}$ and $\{f_{n}(b)\}$ converge pointwise. I want to show that $\{f_n\}$ converges uniformly on $[a,b]$ MY TRIAL: Let $\epsilon>0$, since $\{f_n\}$ converges uniformly on $(a,b)$, then $\exists\,N_1=N(\epsilon)$ s.t. $\forall \,n\geq N$,  $\forall x\in (a,b)$ \begin{align}\left|f_n(x)-f(x)\right|<\epsilon\end{align} Since $\{f_n\}$ converges pointwise when $x=a$, then $\exists\,N_2=N(\epsilon,a)$ s.t. $\forall \,n\geq N$, \begin{align}\left|f_n(a)-f(a)\right|<\epsilon\end{align} Also, $\{f_n\}$ converges pointwise when $x=b$, then $\exists\,N_3=N(\epsilon,b)$ s.t. $\forall \,n\geq N$, \begin{align}\left|f_n(b)-f(b)\right|<\epsilon\end{align} Hence, $\forall \,n\geq \max\{N_1,N_2,N_3\},$ $$\lim\limits_{n\to \infty}f_n(x)=\begin{cases}f(a) & x=a,\\f(x)&x\in(a,b),\\f(b)&x=b\end{cases}$$ Hence, $\{f_n\}$ converges uniformly on $[a,b]$. Please, I'm I right? If no, an alternative proof will be highly regarded. Thanks!",,"['real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence']"
95,Bredon's Cone Construction,Bredon's Cone Construction,,"Bredon has a slick proof based on the cone construction,  that if $X$ is a contractible topological space, then $H_i(X)=0$ for $i\neq 0$. My question has to do with a small detail: Let $F:X\times I\to X$ be a homotopy from $1_X$ to a constant function $x_0\in X,\ $ set $t'=1-t_0,$ and for each singular simplex $\sigma:\Delta_{n-1}\to X$ define $D\sigma:\Delta_n\to X$ by $D\sigma(t_0e_0+\cdots +t_ne_n)=F\left ( \sigma \left ( \frac{t_1}{t'}e_0+\cdots \frac{t_n}{t'}e_{n-1} \right ),t_0 \right ),\ $ It is then straightforward to prove that $\partial D+D\partial=1-\epsilon,$ as desired. Now, $D\sigma,$ as it stands, is not defined at $t_0=1$, so I guess that Bredon is assuming without comment that $D\sigma(e_0)=x_0.$ My question is: how do we prove continuity of $F$ at $e_0?$ It is clear that $\left \{ \frac{t_1}{t'}e_0+\cdots \frac{t_n}{t'}e_{n-1} \right \}$ is bounded by $1$ as $t_i$ ranges over $0\leq t_i\le 1$ with $\Sigma t_i=1.$ Then, if $z_n\to e_0\in \Delta_n,\ $ this induces a sequence $\lambda_n\in I$ such that $\lambda_n\to 1$ and a sequence $w_n\in \Delta_{n-1}.$ Now, using compactness, we can find an $x\in X$ and subsequences $w_{n_k},\lambda_{n_k}$ such that $\sigma (w_{n_k})\to x$ and $\lambda_{n_k}\to 1.$ Continuity of $F$ now implies that $F(\sigma (w_{n_k}),\lambda_{n_k})\to F(x,1)=x_0.$ Of course, this is not enough, since I was forced to pass to a subsequence. So, my questions are: is the case $t_0=1$ so trivial that Bredon left it out of his definition? How do we prove continuity of $D\sigma$ on all of $\Delta_n?$","Bredon has a slick proof based on the cone construction,  that if $X$ is a contractible topological space, then $H_i(X)=0$ for $i\neq 0$. My question has to do with a small detail: Let $F:X\times I\to X$ be a homotopy from $1_X$ to a constant function $x_0\in X,\ $ set $t'=1-t_0,$ and for each singular simplex $\sigma:\Delta_{n-1}\to X$ define $D\sigma:\Delta_n\to X$ by $D\sigma(t_0e_0+\cdots +t_ne_n)=F\left ( \sigma \left ( \frac{t_1}{t'}e_0+\cdots \frac{t_n}{t'}e_{n-1} \right ),t_0 \right ),\ $ It is then straightforward to prove that $\partial D+D\partial=1-\epsilon,$ as desired. Now, $D\sigma,$ as it stands, is not defined at $t_0=1$, so I guess that Bredon is assuming without comment that $D\sigma(e_0)=x_0.$ My question is: how do we prove continuity of $F$ at $e_0?$ It is clear that $\left \{ \frac{t_1}{t'}e_0+\cdots \frac{t_n}{t'}e_{n-1} \right \}$ is bounded by $1$ as $t_i$ ranges over $0\leq t_i\le 1$ with $\Sigma t_i=1.$ Then, if $z_n\to e_0\in \Delta_n,\ $ this induces a sequence $\lambda_n\in I$ such that $\lambda_n\to 1$ and a sequence $w_n\in \Delta_{n-1}.$ Now, using compactness, we can find an $x\in X$ and subsequences $w_{n_k},\lambda_{n_k}$ such that $\sigma (w_{n_k})\to x$ and $\lambda_{n_k}\to 1.$ Continuity of $F$ now implies that $F(\sigma (w_{n_k}),\lambda_{n_k})\to F(x,1)=x_0.$ Of course, this is not enough, since I was forced to pass to a subsequence. So, my questions are: is the case $t_0=1$ so trivial that Bredon left it out of his definition? How do we prove continuity of $D\sigma$ on all of $\Delta_n?$",,"['real-analysis', 'analysis', 'algebraic-topology', 'homology-cohomology']"
96,Boundedness of a subset using boundedness of linear functionals,Boundedness of a subset using boundedness of linear functionals,,"Let $S\subset X$ be a subset of a normed linear space such that $\sup_{x\in S} |f(x)|<\infty$ for all $f\in X^*$, the continuous dual of $X$. Prove that the set $S$ is bounded. By definition $S$ is bounded if $d(S)<M$ for some $0<M\in\mathbb{R}$, where $d(S)$ denotes the diameter. I already proved that this is equivalent to $\sup_{x\in S}\|x\|<\infty$ in the previuos exercise, so probably I should use that here, but I don't know how. I also thought about using Hahn-Banach: I know that for every $a\in S$ there is $f_a\in X^*$ such that $f_a(a)=\|a\|$ and $\|f_a\|=1$, but I don't get to use the second condition. What I can show is $$\|a\|=f_a(a)\leq\sup_{x\in S}|f_a(x)|<\infty.$$ But know I don't know what happen if I take the supremum on both sides. How do I see that the right side stays bounded?","Let $S\subset X$ be a subset of a normed linear space such that $\sup_{x\in S} |f(x)|<\infty$ for all $f\in X^*$, the continuous dual of $X$. Prove that the set $S$ is bounded. By definition $S$ is bounded if $d(S)<M$ for some $0<M\in\mathbb{R}$, where $d(S)$ denotes the diameter. I already proved that this is equivalent to $\sup_{x\in S}\|x\|<\infty$ in the previuos exercise, so probably I should use that here, but I don't know how. I also thought about using Hahn-Banach: I know that for every $a\in S$ there is $f_a\in X^*$ such that $f_a(a)=\|a\|$ and $\|f_a\|=1$, but I don't get to use the second condition. What I can show is $$\|a\|=f_a(a)\leq\sup_{x\in S}|f_a(x)|<\infty.$$ But know I don't know what happen if I take the supremum on both sides. How do I see that the right side stays bounded?",,"['functional-analysis', 'analysis', 'supremum-and-infimum', 'upper-lower-bounds', 'dual-spaces']"
97,Characterisation of Continous function using 2 open sets,Characterisation of Continous function using 2 open sets,,"Let $f:\mathbb  R\to \mathbb R$ be function . Define $2$ sets as A and B in $\mathbb R^2 $ as follows: A={$(x,y)|y<f(x)$},B={$(x,y)|y>f(x)$} . Then $f$ is continous on $\mathbb R$ iff A and B are open subset of $\mathbb R^2$ $\to $ to show A open that means for any $(a_1,a_2)\in A$ There exist some $r>0$ ball which contain in A .i.e $K=B_r((a_1,a_2))\subset A$ $(m,n)\in K$ As f is continous on R$\forall \epsilon >0,\exists \delta >0 $ such that $\forall x\in R$ $|x-y|<\delta$ then $|f(x)-f(y)|<\epsilon$ $f(a_1)>a_2$ I could not get idea to proceed further for any part. Actually I don't able to understand how is this set look like.And How to use continuty assumption Any help will be appreciated","Let $f:\mathbb  R\to \mathbb R$ be function . Define $2$ sets as A and B in $\mathbb R^2 $ as follows: A={$(x,y)|y<f(x)$},B={$(x,y)|y>f(x)$} . Then $f$ is continous on $\mathbb R$ iff A and B are open subset of $\mathbb R^2$ $\to $ to show A open that means for any $(a_1,a_2)\in A$ There exist some $r>0$ ball which contain in A .i.e $K=B_r((a_1,a_2))\subset A$ $(m,n)\in K$ As f is continous on R$\forall \epsilon >0,\exists \delta >0 $ such that $\forall x\in R$ $|x-y|<\delta$ then $|f(x)-f(y)|<\epsilon$ $f(a_1)>a_2$ I could not get idea to proceed further for any part. Actually I don't able to understand how is this set look like.And How to use continuty assumption Any help will be appreciated",,"['real-analysis', 'analysis', 'continuity']"
98,"Prove that $x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s - \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s$ is linear in $x$.",Prove that  is linear in .,"x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s - \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s x","Let $a$ be a positive real number and $f:[a,\infty)\to\mathbb{C}$ a continuous function.  Prove that the function $g:[a,\infty)\to\mathbb{R}$ defined by   $$g(x):=x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s - \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s\text{ for each }x\in[a,\infty)$$   is a linear function. I wonder if there is a solution involving only directly tackling the integrals (e.g., with integration by parts and/or with swapping the order of the integrals), without differentiating $g$.  My solution is to differentiate $g$ twice and show that $g''\equiv0$.  If $p$ and $q$ are constants such that $$g(x)=p(x-a)+q\text{ for all }x\geq a\,,$$ then it is not difficult to see that $q=0$.  It may be good to find out how the $p$ depends on $f$ and $a$. Spoiler: The constant $p$ does not depend on $f$ and $a$---it is just $0$.  Below is my solution. Using the Leibniz Integral Rule, we obtain $$g'(x)=\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s+\frac{1}{x}\,\int_a^x\,t\,f(t)\,\text{d}t-\int_a^x\,f(t)\,\text{d}t\,.$$  Thus, applying the Leibniz Integral Rule again yields $$g''(x)=\frac{1}{x^2}\,\int_a^x\,t\,f(t)\,\text{d}t-\frac{1}{x^2}\,\int_a^x\,t\,f(t)\,\text{d}t+f(x)-f(x)=0\,.$$  The result follows. In fact, from this calculation, $g'(a)=0$.  Therefore, $p=0$ as well.  Hence, $g\equiv 0$.  Consequently, we indeed have that $$x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s = \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s\text{ for every }x\in[a,\infty)\,.$$","Let $a$ be a positive real number and $f:[a,\infty)\to\mathbb{C}$ a continuous function.  Prove that the function $g:[a,\infty)\to\mathbb{R}$ defined by   $$g(x):=x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s - \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s\text{ for each }x\in[a,\infty)$$   is a linear function. I wonder if there is a solution involving only directly tackling the integrals (e.g., with integration by parts and/or with swapping the order of the integrals), without differentiating $g$.  My solution is to differentiate $g$ twice and show that $g''\equiv0$.  If $p$ and $q$ are constants such that $$g(x)=p(x-a)+q\text{ for all }x\geq a\,,$$ then it is not difficult to see that $q=0$.  It may be good to find out how the $p$ depends on $f$ and $a$. Spoiler: The constant $p$ does not depend on $f$ and $a$---it is just $0$.  Below is my solution. Using the Leibniz Integral Rule, we obtain $$g'(x)=\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s+\frac{1}{x}\,\int_a^x\,t\,f(t)\,\text{d}t-\int_a^x\,f(t)\,\text{d}t\,.$$  Thus, applying the Leibniz Integral Rule again yields $$g''(x)=\frac{1}{x^2}\,\int_a^x\,t\,f(t)\,\text{d}t-\frac{1}{x^2}\,\int_a^x\,t\,f(t)\,\text{d}t+f(x)-f(x)=0\,.$$  The result follows. In fact, from this calculation, $g'(a)=0$.  Therefore, $p=0$ as well.  Hence, $g\equiv 0$.  Consequently, we indeed have that $$x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s = \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s\text{ for every }x\in[a,\infty)\,.$$",,"['calculus', 'integration', 'analysis', 'definite-integrals', 'alternative-proof']"
99,"Find a function that maps (0,1) into, but not onto","Find a function that maps (0,1) into, but not onto",,"Find a $1–1$ function that maps $(0,1)$ into, but not necessarily onto, $S$. Where $S$ is the set of points in the open unit square; that is, $S = {(x, y) : 0 < x, y < 1}.$ (This is easy.) After thinking and rereading about an hour I give up trying answer the question. Especially, when the author states, that it is trivial. Nothing comes to my mind :(. Can you, please, help me? In general, are there any systematic ways to answer such kind of questions, rather than guessing a function (as it is so far in the textbook: Understanding Analysis, Stephen Abbott)?","Find a $1–1$ function that maps $(0,1)$ into, but not necessarily onto, $S$. Where $S$ is the set of points in the open unit square; that is, $S = {(x, y) : 0 < x, y < 1}.$ (This is easy.) After thinking and rereading about an hour I give up trying answer the question. Especially, when the author states, that it is trivial. Nothing comes to my mind :(. Can you, please, help me? In general, are there any systematic ways to answer such kind of questions, rather than guessing a function (as it is so far in the textbook: Understanding Analysis, Stephen Abbott)?",,"['analysis', 'elementary-set-theory']"
