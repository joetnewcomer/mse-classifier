,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How can we interpret the equality in the following example from stochastic calculus?,How can we interpret the equality in the following example from stochastic calculus?,,"We know from Itô's lemma that: $$\int_0^T w_t \, dw_t=\frac{w_T^2}{2}-\frac{T}{2},$$ where $w$ is a Wiener process. I don't know how we can interpret the equality!? It is equal for all $\omega\in\Omega$ ? Or in distribution? Or how? I would say it is true for (almost?) every $\omega$ , because the left side of the equation is $$\int_0^T w_t \, dw_t \overset{\circ}{=} \lim_{n\rightarrow\infty}^p \sum_k w_{t_k} \left[w_{t_{k+1}}-w_{t_k}\right],$$ by definition, where $0=t_0\leq t_1\leq t_2\leq\ldots\leq t_n=T$ , and in this definition we use the same Wiener process, just like in the right side of the first equation. So is it true if $\omega=\bar{\omega}$ is fixed, then $$\left(\int_0^T w_t \, dw_t \right) \left(\bar{\omega}\right) = \frac{w_T^2 \left(\bar{\omega}\right)}{2}-\frac{T}{2}\text{?}$$ There are some definitions about the equality of stochastic processes, what can we say about theme in this case? It is just an example, but I am rather interested in how we can interpret the equality in Itô's lemma!? (I hope I ask the same question and it is a relevant example.) Another train of thought motivated the previous question: Let the following series of random variables be: $$\xi_1,\xi_2,\xi_3,\ldots$$ which is convergent in stochastic convergence. It also means that there exists a $\xi$ random variable, where $$\mathbf{P}\left(\omega:\left|\xi_k-\xi \right| > \varepsilon \right) \longrightarrow 0$$ as $k\rightarrow\infty$ . But what is that $\xi$ ? Of course, if we have a conjecture about $\xi$ and we want to know that $\xi$ is a proper limit: $\lim_{k\rightarrow\infty}^p \xi_k = \xi$ , then we should check if the previous $\mathbf{P}\left(\omega:\left|\xi_k-\xi\right| > \varepsilon\right) \longrightarrow 0$ property holds. But if we only know that $(\xi_k)_k$ is convergent in the stochastic way, then how can we ""construct"" this $\xi$ ? What can we say if $\xi$ is a random process? The convergence should hold for every $t\in[0,T]$ where the process is defined? Sorry for the lot of questions, but I think they belong to the same topic.","We know from Itô's lemma that: where is a Wiener process. I don't know how we can interpret the equality!? It is equal for all ? Or in distribution? Or how? I would say it is true for (almost?) every , because the left side of the equation is by definition, where , and in this definition we use the same Wiener process, just like in the right side of the first equation. So is it true if is fixed, then There are some definitions about the equality of stochastic processes, what can we say about theme in this case? It is just an example, but I am rather interested in how we can interpret the equality in Itô's lemma!? (I hope I ask the same question and it is a relevant example.) Another train of thought motivated the previous question: Let the following series of random variables be: which is convergent in stochastic convergence. It also means that there exists a random variable, where as . But what is that ? Of course, if we have a conjecture about and we want to know that is a proper limit: , then we should check if the previous property holds. But if we only know that is convergent in the stochastic way, then how can we ""construct"" this ? What can we say if is a random process? The convergence should hold for every where the process is defined? Sorry for the lot of questions, but I think they belong to the same topic.","\int_0^T w_t \, dw_t=\frac{w_T^2}{2}-\frac{T}{2}, w \omega\in\Omega \omega \int_0^T w_t \, dw_t \overset{\circ}{=} \lim_{n\rightarrow\infty}^p \sum_k w_{t_k} \left[w_{t_{k+1}}-w_{t_k}\right], 0=t_0\leq t_1\leq t_2\leq\ldots\leq t_n=T \omega=\bar{\omega} \left(\int_0^T w_t \, dw_t \right) \left(\bar{\omega}\right) = \frac{w_T^2 \left(\bar{\omega}\right)}{2}-\frac{T}{2}\text{?} \xi_1,\xi_2,\xi_3,\ldots \xi \mathbf{P}\left(\omega:\left|\xi_k-\xi \right| > \varepsilon \right) \longrightarrow 0 k\rightarrow\infty \xi \xi \xi \lim_{k\rightarrow\infty}^p \xi_k = \xi \mathbf{P}\left(\omega:\left|\xi_k-\xi\right| > \varepsilon\right) \longrightarrow 0 (\xi_k)_k \xi \xi t\in[0,T]","['limits', 'measure-theory', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
1,Limit of functions measurable with respect to sub-sigma algebra [closed],Limit of functions measurable with respect to sub-sigma algebra [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $\{f_{\alpha}\}_{\alpha}$ be a net of functions in a measure space $(X,\mathcal F,\mu)$ . Suppose that each $f_{\alpha}$ is measurable with respect to a sub $\sigma$ -algebra $\mathcal G$ , and that the net has a limit, $f$ , in the weak-* topology of $L^{\infty}(X,\mathcal F,\mu)$ : that is, there is a $\mathcal F$ -measuable function $f$ so that $$\int f_{\alpha} \phi d\mu \to  \int f \phi d\mu$$ for every $\phi\in L^1(X,\mathcal F,\mu)$ ). Is $f$ $\mathcal G$ -measurable?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let be a net of functions in a measure space . Suppose that each is measurable with respect to a sub -algebra , and that the net has a limit, , in the weak-* topology of : that is, there is a -measuable function so that for every ). Is -measurable?","\{f_{\alpha}\}_{\alpha} (X,\mathcal F,\mu) f_{\alpha} \sigma \mathcal G f L^{\infty}(X,\mathcal F,\mu) \mathcal F f \int f_{\alpha} \phi d\mu \to  \int f \phi d\mu \phi\in L^1(X,\mathcal F,\mu) f \mathcal G","['measure-theory', 'weak-convergence', 'measurable-functions']"
2,When can we extend a measure from a sub $\sigma$-algebra to the $\sigma$-algebra?,When can we extend a measure from a sub -algebra to the -algebra?,\sigma \sigma,"Let $(X,\mathcal F)$ be a Borel measure space and let $\mathcal G\subseteq F$ be a Borel sub $\sigma$ -algebra (or maybe the topology generating $\mathcal G$ is included in the topology generating $\mathcal F$ ). Suppose that $(X,\mathcal G,\mu)$ is a measure space. Is it always possible to extend $\mu$ to the measure space $(X,\mathcal F,\mu)$ such that for any $\mathcal G$ measurable function $f$ on $X$ , $\mu(f)$ is preserved. Here is my motivation for this problem. If we take the Borel $\sigma$ -algebra generated by a Banach space $X$ to be $\mathcal F$ and the Borel $\sigma$ -algebra generated by the weak topology to be $\mathcal G$ then the constraint is satisfied, now it is easier to get statement like for all measure $\mu$ on $X$ , there is $x\in X$ such that for all continuous affine function $f$ on $X$ , $\mu(f)=f(x)$ (and things about uniqueness) in the weak topology and I want to see what they imply in the strong topology.","Let be a Borel measure space and let be a Borel sub -algebra (or maybe the topology generating is included in the topology generating ). Suppose that is a measure space. Is it always possible to extend to the measure space such that for any measurable function on , is preserved. Here is my motivation for this problem. If we take the Borel -algebra generated by a Banach space to be and the Borel -algebra generated by the weak topology to be then the constraint is satisfied, now it is easier to get statement like for all measure on , there is such that for all continuous affine function on , (and things about uniqueness) in the weak topology and I want to see what they imply in the strong topology.","(X,\mathcal F) \mathcal G\subseteq F \sigma \mathcal G \mathcal F (X,\mathcal G,\mu) \mu (X,\mathcal F,\mu) \mathcal G f X \mu(f) \sigma X \mathcal F \sigma \mathcal G \mu X x\in X f X \mu(f)=f(x)","['general-topology', 'functional-analysis', 'measure-theory']"
3,Lebesgue Integral and Lebesgue measure,Lebesgue Integral and Lebesgue measure,,"So far I have learned the definition of Lebesgue integration, and say we do Lebesgue integration of measurable function $f$ in the measure space $(X,M,\mu)$ . I notice that $\mu$ does not have to be the Lebesgue measure $m$ . Is that correct? In other words, is it ok to understand Lebesgue integration as a general technique and Lebesgue measure as just one tool that we can use for that technique, but the Lebesgue measure is not the only one we can use?","So far I have learned the definition of Lebesgue integration, and say we do Lebesgue integration of measurable function in the measure space . I notice that does not have to be the Lebesgue measure . Is that correct? In other words, is it ok to understand Lebesgue integration as a general technique and Lebesgue measure as just one tool that we can use for that technique, but the Lebesgue measure is not the only one we can use?","f (X,M,\mu) \mu m","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
4,Lebesgue differentiation theorem for Radon measures,Lebesgue differentiation theorem for Radon measures,,"Given any Radon measure $\mu$ on $\mathbb{R}^n$ and locally integrable function $f$ , the generalized version of the Lebesgue differentiation theorem states that $$ \lim_{r \downarrow 0} \frac{1}{\mu(B(x,r))} \int_{B(x,r)} f d \mu = f(x)  \; \; \; \; \mu-a.e   $$ where $B(x,r)$ is a Euclidean ball of radius $r$ . I'm looking for a reference that provides the same statement for balls constructed using other equivalent norms on $ \mathbb{R}^n$ . In particular, when $B(x,r)$ is a ball of radius $r$ with respect to the $ \| . \|_{\infty}$ norm. Edit : Please note that I am looking for a reference that states this for all radon measures.","Given any Radon measure on and locally integrable function , the generalized version of the Lebesgue differentiation theorem states that where is a Euclidean ball of radius . I'm looking for a reference that provides the same statement for balls constructed using other equivalent norms on . In particular, when is a ball of radius with respect to the norm. Edit : Please note that I am looking for a reference that states this for all radon measures.","\mu \mathbb{R}^n f  \lim_{r \downarrow 0} \frac{1}{\mu(B(x,r))} \int_{B(x,r)} f d \mu = f(x)  \; \; \; \; \mu-a.e    B(x,r) r  \mathbb{R}^n B(x,r) r  \| . \|_{\infty}","['real-analysis', 'functional-analysis', 'measure-theory', 'reference-request']"
5,Is the image of a separable metric space under a measurable map still separable?,Is the image of a separable metric space under a measurable map still separable?,,"This question stems from Problem 10, section 4.2 and Problem 9, section 13.1 of Real Analysis and Probability written by Dudley. Similar question has been posed in: Problem 10, section 4.2 of R.M. Dudley, Real Analysis and Probability . In Problem 10, section 4.2, we are asked to prove: Let $f$ be a Borel measurable function from a separable metric space $X$ onto a metric space $S$ with metric $e$ . Show that $(S, e)$ is separable. And in Problem 9, section 13.1, we are asked to prove: Let $(S, d)$ be a separable metric space and $(T, e)$ a metric space. Let $f$ be a Borel measurable function from $S$ into $T$ . Assuming the continuum hypothesis, prove that the range $f[S]$ is separable. I'm very confused about the difference between these two statements. It seems that the proof of the previous one does not need the continuum hypothesis. But I can't find such a proof. In my point of view, we can't prove Problem 10 in section 4.2 without assuming continuum hypothesis, if we follow the hint given by Dudley: If $S$ is not separable, then show that for some $ε>0$ , there is an uncountable subset $T$ of $S$ with $d(y, z)>ε$ for all $y\neq z$ in $T$ . Use Problem 9 (the statement and proof of Problem 9 is given in: Problem 9, section 4.2 of R.M. Dudley, Real Analysis and Probability ) to get a measurable function $g$ from $X$ onto $T$ . All $g^{-1}(A) (A \subset T)$ are Borel sets in $X$ . Thanks in advance to your help!","This question stems from Problem 10, section 4.2 and Problem 9, section 13.1 of Real Analysis and Probability written by Dudley. Similar question has been posed in: Problem 10, section 4.2 of R.M. Dudley, Real Analysis and Probability . In Problem 10, section 4.2, we are asked to prove: Let be a Borel measurable function from a separable metric space onto a metric space with metric . Show that is separable. And in Problem 9, section 13.1, we are asked to prove: Let be a separable metric space and a metric space. Let be a Borel measurable function from into . Assuming the continuum hypothesis, prove that the range is separable. I'm very confused about the difference between these two statements. It seems that the proof of the previous one does not need the continuum hypothesis. But I can't find such a proof. In my point of view, we can't prove Problem 10 in section 4.2 without assuming continuum hypothesis, if we follow the hint given by Dudley: If is not separable, then show that for some , there is an uncountable subset of with for all in . Use Problem 9 (the statement and proof of Problem 9 is given in: Problem 9, section 4.2 of R.M. Dudley, Real Analysis and Probability ) to get a measurable function from onto . All are Borel sets in . Thanks in advance to your help!","f X S e (S, e) (S, d) (T, e) f S T f[S] S ε>0 T S d(y, z)>ε y\neq z T g X T g^{-1}(A) (A \subset T) X","['functional-analysis', 'measure-theory']"
6,The mean ergodic theorem for weakly mixing extension,The mean ergodic theorem for weakly mixing extension,,"I got stuck with the following while going through the proof of Lemma 3.21 from the book 'Ergodic Theory: Independence and Dichotomies' by Kerr and Li. Problem: If $(X,\mu,T)$ is a weakly mixing extension of $(Y,\nu,S)$ , then $$\{f\in L^2(X|Y):f\circ T=f\}\subseteq L^{\infty}(Y).$$ Here I recall the following definitions which are used for the above problem. Definition 1: Let $(X,\mu)$ and $(Y,\nu)$ be two probability measure space. Let $T:X\rightarrow X$ and $S:Y\rightarrow Y$ be two invertible measure preserving transformations. We say that $(X,\mu,T)$ is an extension of $(Y,\nu,S)$ if there is a $T$ invariant conull set $X'\subseteq X$ and an equivariant measurable map $\pi : X'\rightarrow Y$ such that $\mu (\pi^{-1}(A))=\nu (A)$ for all measurable $A\subseteq Y$ . Definition 2: Given an extension $(X,\mu,T)\rightarrow (Y,\nu,S)$ , let $\mathbb{E}_Y:L^2(X)\rightarrow L^2(Y)$ be the conditional expectation. Then $L^2(X|Y)$ is the completion of $L^{\infty}(X)$ with respect to the norm $\|f\|:=\|\mathbb{E}_Y(f\cdot\overline{f})\|^{1/2}$ . Definition 3: An element $f\in L^2(X|Y)$ is said to be conditionally weakly mixing if the mean $$\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{s=-n}^{s=n}\|\mathbb{E}_Y((f\circ T^s)\cdot\overline{f})\|=0.$$ Definition 4: The extension $(X,\mu,T)\rightarrow (Y,\nu,S)$ is said to be weakly mixing if every element in $L^2(X|Y)$ orthogonal to $L^{\infty}(Y)$ is conditionally weakly mixing. Thanks in advance for any help or suggestion.","I got stuck with the following while going through the proof of Lemma 3.21 from the book 'Ergodic Theory: Independence and Dichotomies' by Kerr and Li. Problem: If is a weakly mixing extension of , then Here I recall the following definitions which are used for the above problem. Definition 1: Let and be two probability measure space. Let and be two invertible measure preserving transformations. We say that is an extension of if there is a invariant conull set and an equivariant measurable map such that for all measurable . Definition 2: Given an extension , let be the conditional expectation. Then is the completion of with respect to the norm . Definition 3: An element is said to be conditionally weakly mixing if the mean Definition 4: The extension is said to be weakly mixing if every element in orthogonal to is conditionally weakly mixing. Thanks in advance for any help or suggestion.","(X,\mu,T) (Y,\nu,S) \{f\in L^2(X|Y):f\circ T=f\}\subseteq L^{\infty}(Y). (X,\mu) (Y,\nu) T:X\rightarrow X S:Y\rightarrow Y (X,\mu,T) (Y,\nu,S) T X'\subseteq X \pi : X'\rightarrow Y \mu (\pi^{-1}(A))=\nu (A) A\subseteq Y (X,\mu,T)\rightarrow (Y,\nu,S) \mathbb{E}_Y:L^2(X)\rightarrow L^2(Y) L^2(X|Y) L^{\infty}(X) \|f\|:=\|\mathbb{E}_Y(f\cdot\overline{f})\|^{1/2} f\in L^2(X|Y) \lim_{n\rightarrow\infty}\frac{1}{n}\sum_{s=-n}^{s=n}\|\mathbb{E}_Y((f\circ T^s)\cdot\overline{f})\|=0. (X,\mu,T)\rightarrow (Y,\nu,S) L^2(X|Y) L^{\infty}(Y)","['functional-analysis', 'measure-theory', 'conditional-expectation', 'ergodic-theory']"
7,What's the distance between triangles?,What's the distance between triangles?,,"How can we define distance between, say, two of them? And if we the distance between two of them and the distance of another one to the first, do we know its distance tö rhe second? What ways are possible to induce a distance on them. The middle point seems a reasonable one but if the distance is non-zero they can still overlap. Is there a way to make their distance zero when they touch at an arbitrary point? Let's simplify things. Instead of triangles consider circles in three dimensional space.","How can we define distance between, say, two of them? And if we the distance between two of them and the distance of another one to the first, do we know its distance tö rhe second? What ways are possible to induce a distance on them. The middle point seems a reasonable one but if the distance is non-zero they can still overlap. Is there a way to make their distance zero when they touch at an arbitrary point? Let's simplify things. Instead of triangles consider circles in three dimensional space.",,"['measure-theory', 'hausdorff-distance', 'mahalanobis-distance']"
8,"show that the set of numbers in $(0, 1)$ that have a decimal expansion with one hundred consecutive 4s is a Borel set and find its Lebesgue measure",show that the set of numbers in  that have a decimal expansion with one hundred consecutive 4s is a Borel set and find its Lebesgue measure,"(0, 1)","I have solved the following problem and I would like to know if my proof is correct and/or if it could be improved, thanks. ""show that the set of numbers in $(0, 1)$ that have a decimal expansion with one hundred consecutive 4s is a Borel set and find its Lebesgue measure"" My proof: Let $E:=\{x\in (0,1):x\text{ has as decimal expansion with one hundred consecutive }4\text{s}\}$ and define (note that, for example, $0.\underbrace{44\dots4}_{99}5=0.\underbrace{44\dots4}_{100}99999\dots$ ): $E_0=[0.\underbrace{44\dots4}_{100}, 0.\underbrace{44\dots4}_{99}5]$ , $E_1=\bigcup_{n_1\in\{0,1,\dots,9\}}[0.n_1\underbrace{44\dots4}_{100}, 0.n_1\underbrace{44\dots4}_{99}5]$ $E_2=\bigcup_{n_1,n_2\in\{0,1,\dots,9\}}[0.n_1n_2\underbrace{44\dots4}_{100}, 0.n_1n_2\underbrace{44\dots4}_{99}5]$ $E_3=\bigcup_{n_1,n_2,n_3\in\{0,1,\dots,9\}}[0.n_1n_2n_3\underbrace{44\dots4}_{100}, 0.n_1n_2n_3\underbrace{44\dots4}_{99}5]$ $\vdots$ $E_k=\bigcup_{n_1,n_2,\dots, n_k\in\{0,1,\dots,9\}}[0.n_1n_2\dots n_k\underbrace{44\dots4}_{100}, 0.n_1n_2\dots n_k\underbrace{44\dots4}_{99}5]\  \dots$ ; now, closed intervals in $\mathbb{R}$ are Borel sets and each $E_j$ is the union of such intervals so the $E_j$ s are Borel sets too and since $E=\bigcup_{k=0}^{\infty}E_k$ we can conclude that $E$ is a Borel set. To determine the Lebesgue measure of $E$ represent the numbers in base $10^{100}$ : then we may envision $(0,1)$ as a segment divided into $10^{100}$ smaller segments of length $\frac{1}{10^{100}}$ . Of these sub-segments the $\underbrace{444\dots 44}_{100}$ th one must be taken, since it represent numbers in $(0,1)$ whose decimal expansion begins with one hundred $4$ s. Each one of the remaining $10^{100}-1$ sub-segments can be divided into $10^{100}$ sub-segments, each of length $(\frac{1}{10^{100}})^2$ and of these, as before, we have to take the $\underbrace{444\dots 44}_{100}$ th one, because it contains a run of one hundred $4$ s and continuing in this fashion we have that $\mu(E) =1\cdot\frac{1}{10^{100}}+ (10^{100}-1)\cdot (\frac{1}{10^{100}})^2+(10^{100}-1)^2 (\frac{1}{10^{100}})^3+\dots +(10^{100}-1)^k (\frac{1}{10^{100}})^{k+1} +\dots =\sum_{k=0}^{\infty}+(10^{100}-1)^k (\frac{1}{10^{100}})^{k+1} =\frac{1}{10^{100}}\sum_{k=0}^{\infty} (1-\frac{1}{100})^k=\frac{1}{10^{100}}\cdot\frac{1}{1-(1-\frac{1}{100})}=\frac{1}{10^{100}}\cdot 10^{100}=1.\ \square$","I have solved the following problem and I would like to know if my proof is correct and/or if it could be improved, thanks. ""show that the set of numbers in that have a decimal expansion with one hundred consecutive 4s is a Borel set and find its Lebesgue measure"" My proof: Let and define (note that, for example, ): , ; now, closed intervals in are Borel sets and each is the union of such intervals so the s are Borel sets too and since we can conclude that is a Borel set. To determine the Lebesgue measure of represent the numbers in base : then we may envision as a segment divided into smaller segments of length . Of these sub-segments the th one must be taken, since it represent numbers in whose decimal expansion begins with one hundred s. Each one of the remaining sub-segments can be divided into sub-segments, each of length and of these, as before, we have to take the th one, because it contains a run of one hundred s and continuing in this fashion we have that","(0, 1) E:=\{x\in (0,1):x\text{ has as decimal expansion with one hundred consecutive }4\text{s}\} 0.\underbrace{44\dots4}_{99}5=0.\underbrace{44\dots4}_{100}99999\dots E_0=[0.\underbrace{44\dots4}_{100}, 0.\underbrace{44\dots4}_{99}5] E_1=\bigcup_{n_1\in\{0,1,\dots,9\}}[0.n_1\underbrace{44\dots4}_{100}, 0.n_1\underbrace{44\dots4}_{99}5] E_2=\bigcup_{n_1,n_2\in\{0,1,\dots,9\}}[0.n_1n_2\underbrace{44\dots4}_{100}, 0.n_1n_2\underbrace{44\dots4}_{99}5] E_3=\bigcup_{n_1,n_2,n_3\in\{0,1,\dots,9\}}[0.n_1n_2n_3\underbrace{44\dots4}_{100}, 0.n_1n_2n_3\underbrace{44\dots4}_{99}5] \vdots E_k=\bigcup_{n_1,n_2,\dots, n_k\in\{0,1,\dots,9\}}[0.n_1n_2\dots n_k\underbrace{44\dots4}_{100}, 0.n_1n_2\dots n_k\underbrace{44\dots4}_{99}5]\  \dots \mathbb{R} E_j E_j E=\bigcup_{k=0}^{\infty}E_k E E 10^{100} (0,1) 10^{100} \frac{1}{10^{100}} \underbrace{444\dots 44}_{100} (0,1) 4 10^{100}-1 10^{100} (\frac{1}{10^{100}})^2 \underbrace{444\dots 44}_{100} 4 \mu(E) =1\cdot\frac{1}{10^{100}}+ (10^{100}-1)\cdot (\frac{1}{10^{100}})^2+(10^{100}-1)^2 (\frac{1}{10^{100}})^3+\dots +(10^{100}-1)^k (\frac{1}{10^{100}})^{k+1} +\dots =\sum_{k=0}^{\infty}+(10^{100}-1)^k (\frac{1}{10^{100}})^{k+1} =\frac{1}{10^{100}}\sum_{k=0}^{\infty} (1-\frac{1}{100})^k=\frac{1}{10^{100}}\cdot\frac{1}{1-(1-\frac{1}{100})}=\frac{1}{10^{100}}\cdot 10^{100}=1.\ \square","['real-analysis', 'measure-theory', 'solution-verification', 'lebesgue-measure']"
9,Showing a collection of half open intervals generate a borel sigma algebra,Showing a collection of half open intervals generate a borel sigma algebra,,"Let $\mathcal{H} = \{ (a, \infty) : a \in \mathbb{R} \} $ . Let $\mathcal{B}$ be borel sigma algebra generated by the family of open sets. I have already shown that $\mathcal{B}$ is also generated by open intervals. Now I want to show that $\sigma( \mathcal{H} ) = \mathcal{B} $ . TRY: We know intervals of the form $(a, \infty) $ are open sets. Hence, $\mathcal{H} \subseteq \mathcal{B} $ and so $\sigma( \mathcal{H} ) \subseteq \mathcal{B} $ If $\mathcal{I}$ is the collection of all intervals of the form $(a,b]$ , we have shown in my class that $\sigma( \mathcal{I} ) = \mathcal{B} $ . Consequently, if we can show that $\sigma( \mathcal{I} ) \subset \sigma( \mathcal{H} ) $ , then we are done, and to do this it is enough to show that $\mathcal{I} \subset \sigma( \mathcal{H} )$ . But this follows since we know $$(a,b] =  (a, \infty) \cap (b, \infty)^c$$ Is this a correct solution?","Let . Let be borel sigma algebra generated by the family of open sets. I have already shown that is also generated by open intervals. Now I want to show that . TRY: We know intervals of the form are open sets. Hence, and so If is the collection of all intervals of the form , we have shown in my class that . Consequently, if we can show that , then we are done, and to do this it is enough to show that . But this follows since we know Is this a correct solution?","\mathcal{H} = \{ (a, \infty) : a \in \mathbb{R} \}  \mathcal{B} \mathcal{B} \sigma( \mathcal{H} ) = \mathcal{B}  (a, \infty)  \mathcal{H} \subseteq \mathcal{B}  \sigma( \mathcal{H} ) \subseteq \mathcal{B}  \mathcal{I} (a,b] \sigma( \mathcal{I} ) = \mathcal{B}  \sigma( \mathcal{I} ) \subset \sigma( \mathcal{H} )  \mathcal{I} \subset \sigma( \mathcal{H} ) (a,b] =  (a, \infty) \cap (b, \infty)^c",[]
10,Fenchel Dual of indicator function,Fenchel Dual of indicator function,,"It is well known that if I have the indicator function $$\iota_S(x)=\cases{ 0 \text{ if } x \in S \\ +\infty \text{ else} }$$ of a convex set $S$ , then this is a convex functional and its Fenchel dual is the support function $$\sigma_S(y) = \sup_{x\in S} \langle y,x \rangle .$$ One can then use this in convex optimisations problems to find the dual of the problem itself. So, if I have $$\min_{x\in C} \psi(x) \text{ s.t.} Ax=b,$$ then I can rewrite the problem as $\min_x \psi(x) + \iota_C(x) + \iota_S(Ax)$ , where $\iota_s(x) = \cases {0 \text{ if } x = b,\\ +\infty \text{ else}}$ and compute the dual of my problem, using Fenchel-Rockafeller duality as $\sup_y -\sigma_S(-y) - (\psi+\iota_c)^\star(A^\star y).$ In this simple case, it's easy to see that $-\sigma_S(-y)= \inf_{x\in S} \langle y,x \rangle = \langle y,b \rangle $ (as $x \in S \iff x = b$ ). My question then is: what happens if $S$ , instead of being $S=\{ x : x = b\}$ is a set denoting inequality constraints? For instance, $S=\{ x: x \geq \gamma\}$ ? Is there a standard/straight-forward way of characteristing the Fenchel dual? More precisely, if I have a problem $\min_{x\in C} \psi(x) \text{ s.t } Ax \geq \gamma$ what would the dual be? If our space were the space of Radon Measures and $A$ were a linear operator (say, the integral of the measure with respect to a fixed measurable function $g$ ) how would the constraint that $\int dQ g \geq \gamma$ impact the dual? Intuitively, the new $-\sigma_S(-u)$ would be $\inf_{Q\in S} \int u dQ$ . If $u=g$ then clearly this infimum can be $\gamma$ , otherwise it is $-\infty$ . But, how do I reason about all the possible measures and functionals I can plug in this infimum? If $u = \lambda g$ with $\lambda >0$ , is it then true that $\inf_{Q\in S}\int u dQ = \inf_{\lambda>0} \lambda \gamma$ ? Is it arbitrarily small then? What about other (non-linear) transformations of $g$ ? Thank you for any suggestion/reference you might provide me!","It is well known that if I have the indicator function of a convex set , then this is a convex functional and its Fenchel dual is the support function One can then use this in convex optimisations problems to find the dual of the problem itself. So, if I have then I can rewrite the problem as , where and compute the dual of my problem, using Fenchel-Rockafeller duality as In this simple case, it's easy to see that (as ). My question then is: what happens if , instead of being is a set denoting inequality constraints? For instance, ? Is there a standard/straight-forward way of characteristing the Fenchel dual? More precisely, if I have a problem what would the dual be? If our space were the space of Radon Measures and were a linear operator (say, the integral of the measure with respect to a fixed measurable function ) how would the constraint that impact the dual? Intuitively, the new would be . If then clearly this infimum can be , otherwise it is . But, how do I reason about all the possible measures and functionals I can plug in this infimum? If with , is it then true that ? Is it arbitrarily small then? What about other (non-linear) transformations of ? Thank you for any suggestion/reference you might provide me!","\iota_S(x)=\cases{ 0 \text{ if } x \in S \\ +\infty \text{ else} } S \sigma_S(y) = \sup_{x\in S} \langle y,x \rangle . \min_{x\in C} \psi(x) \text{ s.t.} Ax=b, \min_x \psi(x) + \iota_C(x) + \iota_S(Ax) \iota_s(x) = \cases {0 \text{ if } x = b,\\ +\infty \text{ else}} \sup_y -\sigma_S(-y) - (\psi+\iota_c)^\star(A^\star y). -\sigma_S(-y)= \inf_{x\in S} \langle y,x \rangle = \langle y,b \rangle  x \in S \iff x = b S S=\{ x : x = b\} S=\{ x: x \geq \gamma\} \min_{x\in C} \psi(x) \text{ s.t } Ax \geq \gamma A g \int dQ g \geq \gamma -\sigma_S(-u) \inf_{Q\in S} \int u dQ u=g \gamma -\infty u = \lambda g \lambda >0 \inf_{Q\in S}\int u dQ = \inf_{\lambda>0} \lambda \gamma g","['functional-analysis', 'measure-theory', 'convex-optimization', 'duality-theorems']"
11,Generalized second derivative of a concave and piecewise $C^2$ function,Generalized second derivative of a concave and piecewise  function,C^2,"It is mentioned in page 20 of this paper that if $f: \mathbb{R}_+ \to \mathbb R$ is a concave and piecewise $C^2$ function, then the generalized second derivative of $f$ is a signed measure $\mu_f$ such that $\mu_f(\mathrm{d}r) \leq f''(r)\mathrm{d}r$ . I am wondering if anyone can help me understand the relation $\mu_f(\mathrm{d}r) \leq f''(r)\mathrm{d}r$ . As far as know, a concave function $f$ has the left hand derivative $f'_{-}(x)$ for every $x$ , and $f'_{-}$ is left-continuous and non-increasing, so we can define a Borel measure on $\mathbb{R}_+$ by the prescription $$\mu_f([x,y)) = f'_{-}(x) - f'_{-}(y),$$ and we can also denote this measure by $f''(\mathrm{d}x)$ . May I know how the authors of the aforementioned paper can claim the sentence in bold letters?","It is mentioned in page 20 of this paper that if is a concave and piecewise function, then the generalized second derivative of is a signed measure such that . I am wondering if anyone can help me understand the relation . As far as know, a concave function has the left hand derivative for every , and is left-continuous and non-increasing, so we can define a Borel measure on by the prescription and we can also denote this measure by . May I know how the authors of the aforementioned paper can claim the sentence in bold letters?","f: \mathbb{R}_+ \to \mathbb R C^2 f \mu_f \mu_f(\mathrm{d}r) \leq f''(r)\mathrm{d}r \mu_f(\mathrm{d}r) \leq f''(r)\mathrm{d}r f f'_{-}(x) x f'_{-} \mathbb{R}_+ \mu_f([x,y)) = f'_{-}(x) - f'_{-}(y), f''(\mathrm{d}x)","['real-analysis', 'functional-analysis', 'measure-theory', 'convex-analysis', 'weak-derivatives']"
12,Is there a probability measure on $2^{\mathfrak{c}}$?,Is there a probability measure on ?,2^{\mathfrak{c}},"Is there a probability measure on a set whose cardinality is $2^{\mathfrak{c}}$ ? I considered a real-valued stochastic process $S(t),t\in[0,1]$ . If $t_0$ is a constant, $S(t_0)$ is an ordinary random variable. But suppose the set of all sample curves of $S(t)$ is $SC$ . We have $Card(SC)=2^{\mathfrak{c}}$ . Let $SP = \{f(t_0)| f\in SC\}$ . I think the probability distribution of random variable $S(t_0)$ should be consistent with the measure of different values in the set $SP$ . But $Card(SP)=2^{\mathfrak{c}}$ and the probability measure I know is defined on the set of real numbers. How to understand the meaning of the set $SP$ and the relationship between $SP$ and $S(t_0)$ ?","Is there a probability measure on a set whose cardinality is ? I considered a real-valued stochastic process . If is a constant, is an ordinary random variable. But suppose the set of all sample curves of is . We have . Let . I think the probability distribution of random variable should be consistent with the measure of different values in the set . But and the probability measure I know is defined on the set of real numbers. How to understand the meaning of the set and the relationship between and ?","2^{\mathfrak{c}} S(t),t\in[0,1] t_0 S(t_0) S(t) SC Card(SC)=2^{\mathfrak{c}} SP = \{f(t_0)| f\in SC\} S(t_0) SP Card(SP)=2^{\mathfrak{c}} SP SP S(t_0)","['probability', 'measure-theory', 'elementary-set-theory', 'cardinals']"
13,Space of compact operators is the only proper closed two sided ideal of the space of all bounded operators.,Space of compact operators is the only proper closed two sided ideal of the space of all bounded operators.,,"Let $\mathcal H$ be a Hilbert space. Then $\mathcal K(\mathcal H)$ is the only proper closed two sided ideal of $\mathcal B(\mathcal H).$ I am following Rajendra Bhatia's notes on Functional Analysis. The following hint has been given in the book. Let $\mathcal I$ be a proper closed two sided ideal of $\mathcal B(\mathcal H).$ If $\mathcal I$ contains a positive operator $A$ that is not compact then there exists $\varepsilon \gt 0$ such that $P(\varepsilon, \infty)$ is infinite-dimensional, where $P$ is the projection valued measure associated with $A$ . Let $\mathcal M$ be it's range and let $V$ be the unitary operator from $\mathcal H$ onto $\mathcal M.$ Since $A(\mathcal M) = \mathcal M,$ we have $$V^*AV (\mathcal H) = V^*A(\mathcal M) = V^*(\mathcal M) = \mathcal H.$$ Then for every $x \in \mathcal H$ we have $$\|V^*AV x\| \geq \varepsilon \|x\|.$$ Hence $V^*AV$ is invertible and $V^*AV \in \mathcal I.$ So it follows that $\mathcal I = \mathcal B (\mathcal H).$ In the above proof I don't understand few things. Here they are $:$ $(1)$ Why does there exist $\varepsilon \gt 0$ such that the range of the projection $P(\varepsilon, \infty)$ is infinite-dimensional? $(2)$ How to guarantee the existence of an unitary operator $V : \mathcal H \longrightarrow \mathcal M\ $ ? $(3)$ Why do we have $A(\mathcal M) = \mathcal M\ $ ? I have understood the rest of the part in the proof quite clearly. Could anyone please help me understanding the proof? Thanks for your time. EDIT $:$ I have managed to answer the first question. Here it is $:$ Suppose for every $\varepsilon \gt 0,$ the range of $P(\varepsilon,\infty)$ is finite dimensional. Then for every $n \in \mathbb N,$ $AP\left (\frac {1} {n}, \infty \right )$ is a finite rank operator. Now we have $$AP \left (\frac {1} {n},\infty \right ) = \left (\int t\ dP \right ) \left (\int \chi_{\left (\frac {1} {n}, \infty \right )}\ dP \right ) = \int t\ \chi_{\left (\frac {1} {n} , \infty \right )}\ dP.$$ Since the integrand $t\ \chi_{\left (\frac {1} {n} , \infty \right )}$ increases to $t,$ by MCT we have $$A P \left (\frac {1} {n}, \infty \right ) \xrightarrow {\text {NORM}} A.$$ But then $A$ becomes compact as it is the norm limit of a sequence of finite rank operators, a contradiction. Hence there should exist some $\varepsilon \gt 0$ such that the range of $P(\varepsilon, \infty)$ is infinite-dimensional. For $(3)$ we first note that $$\begin{align*}AP \left (\varepsilon,\infty \right ) & = \left (\int t\ dP \right ) \left (\int \chi_{\left (\varepsilon, \infty \right )}\ dP \right ) \\ & = \int t\ \chi_{\left (\varepsilon , \infty \right )}\ dP \\ & = \int \chi_{\left (\varepsilon , \infty \right )}\ t\ dP \\ & = \left (\int \chi_{\left (\varepsilon, \infty \right )}\ dP \right ) \left (\int t\ dP \right ) \\ & = P(\varepsilon, \infty) A \end{align*}$$ Now for any $x \in \mathcal M = \text {ran} \left (P(\varepsilon,  \infty) \right )$ we have $$Ax  = A P(\varepsilon, \infty) x = P (\varepsilon,  \infty) A x \in \mathcal M$$ proving that $A(\mathcal M) \subseteq \mathcal M.$ But I don't know how to show the reverse inclusion i.e. how to show that $\mathcal M \subseteq A(\mathcal M).$ RE-EDIT $:$ For the other part of the inclusion in $(3)$ we first note that the map $t \mapsto \frac {1} {t}$ is a bounded measurable function on $(\varepsilon,  \infty)$ and hence we have $$P(\varepsilon,  \infty) = \displaystyle {\int_{(\varepsilon, \infty)} dP = \left (\int_{(\varepsilon, \infty)} t\ dP \right ) \left (\int_{(\varepsilon, \infty)} t^{-1}\ dP \right ) = A E(\varepsilon, \infty) B}$$ where $B = \displaystyle {\int_{(\varepsilon, \infty)} t^{-1}\ dP.}$ This shows that $\mathcal M \subseteq A(\mathcal M).$","Let be a Hilbert space. Then is the only proper closed two sided ideal of I am following Rajendra Bhatia's notes on Functional Analysis. The following hint has been given in the book. Let be a proper closed two sided ideal of If contains a positive operator that is not compact then there exists such that is infinite-dimensional, where is the projection valued measure associated with . Let be it's range and let be the unitary operator from onto Since we have Then for every we have Hence is invertible and So it follows that In the above proof I don't understand few things. Here they are Why does there exist such that the range of the projection is infinite-dimensional? How to guarantee the existence of an unitary operator ? Why do we have ? I have understood the rest of the part in the proof quite clearly. Could anyone please help me understanding the proof? Thanks for your time. EDIT I have managed to answer the first question. Here it is Suppose for every the range of is finite dimensional. Then for every is a finite rank operator. Now we have Since the integrand increases to by MCT we have But then becomes compact as it is the norm limit of a sequence of finite rank operators, a contradiction. Hence there should exist some such that the range of is infinite-dimensional. For we first note that Now for any we have proving that But I don't know how to show the reverse inclusion i.e. how to show that RE-EDIT For the other part of the inclusion in we first note that the map is a bounded measurable function on and hence we have where This shows that","\mathcal H \mathcal K(\mathcal H) \mathcal B(\mathcal H). \mathcal I \mathcal B(\mathcal H). \mathcal I A \varepsilon \gt 0 P(\varepsilon, \infty) P A \mathcal M V \mathcal H \mathcal M. A(\mathcal M) = \mathcal M, V^*AV (\mathcal H) = V^*A(\mathcal M) = V^*(\mathcal M) = \mathcal H. x \in \mathcal H \|V^*AV x\| \geq \varepsilon \|x\|. V^*AV V^*AV \in \mathcal I. \mathcal I = \mathcal B (\mathcal H). : (1) \varepsilon \gt 0 P(\varepsilon, \infty) (2) V : \mathcal H \longrightarrow \mathcal M\  (3) A(\mathcal M) = \mathcal M\  : : \varepsilon \gt 0, P(\varepsilon,\infty) n \in \mathbb N, AP\left (\frac {1} {n}, \infty \right ) AP \left (\frac {1} {n},\infty \right ) = \left (\int t\ dP \right ) \left (\int \chi_{\left (\frac {1} {n}, \infty \right )}\ dP \right ) = \int t\ \chi_{\left (\frac {1} {n} , \infty \right )}\ dP. t\ \chi_{\left (\frac {1} {n} , \infty \right )} t, A P \left (\frac {1} {n}, \infty \right ) \xrightarrow {\text {NORM}} A. A \varepsilon \gt 0 P(\varepsilon, \infty) (3) \begin{align*}AP \left (\varepsilon,\infty \right ) & = \left (\int t\ dP \right ) \left (\int \chi_{\left (\varepsilon, \infty \right )}\ dP \right ) \\ & = \int t\ \chi_{\left (\varepsilon , \infty \right )}\ dP \\ & = \int \chi_{\left (\varepsilon , \infty \right )}\ t\ dP \\ & = \left (\int \chi_{\left (\varepsilon, \infty \right )}\ dP \right ) \left (\int t\ dP \right ) \\ & = P(\varepsilon, \infty) A \end{align*} x \in \mathcal M = \text {ran} \left (P(\varepsilon,  \infty) \right ) Ax  = A P(\varepsilon, \infty) x = P (\varepsilon,  \infty) A x \in \mathcal M A(\mathcal M) \subseteq \mathcal M. \mathcal M \subseteq A(\mathcal M). : (3) t \mapsto \frac {1} {t} (\varepsilon,  \infty) P(\varepsilon,  \infty) = \displaystyle {\int_{(\varepsilon, \infty)} dP = \left (\int_{(\varepsilon, \infty)} t\ dP \right ) \left (\int_{(\varepsilon, \infty)} t^{-1}\ dP \right ) = A E(\varepsilon, \infty) B} B = \displaystyle {\int_{(\varepsilon, \infty)} t^{-1}\ dP.} \mathcal M \subseteq A(\mathcal M).","['measure-theory', 'proof-explanation', 'ideals', 'spectral-theory', 'compact-operators']"
14,Is $\mu$ a measure,Is  a measure,\mu,"a) $\mu(E)=\lim_{n\rightarrow\infty} \frac{\#[\{2,4,\dots\}\cap \{1,\dots,n\}]}{n}=\lim_{n\rightarrow\infty}\frac{n/2}{n}=1/2$ and $\mu(O)=1/2$ by same argument. $$\mu(S)=\lim_{n\rightarrow\infty}\frac{\#[ \{1,4,9,\dots\}\cap \{1,\dots,n\}]}{n}=\lim_{n\rightarrow\infty}\frac{\sqrt n}{n}=0$$ b) Let $A,B$ be disjoint sets in $\mathcal A$ . $$\begin{split}\mu(A\cup B)&=\lim_{n\rightarrow\infty} \frac{\#[(A\cup B)\cap \{1,\dots,n\}]}{n}&&\text{definition}\\ &=\lim_{n\rightarrow\infty} \frac{\#[A\cap \{1,\dots,n\} \cup B \cap \{1,\dots,n\}]}{n}&&\text{DeMorgan's Law}\\ &=\lim_{n\rightarrow\infty} \frac{\#[A\cap \{1,\dots,n\}] +\#[ B \cap \{1,\dots,n\}]}{n}&&\text{$A$ and $B$ are disjoint}\\ &=\mu(A)+\mu(B)\end{split}$$ c) The two requirements for a measure are $\mu:\mathcal A\rightarrow[0,\infty]$ . This is satisfied since $\mu(B)$ is between 0 and 1. If $B_1,B_2,\dots$ are disjoint elements of $\mathcal A$ then $\mu\left(\bigcup_{i=1}^\infty B_i\right)=\sum_{i=1}^\infty \mu(B_i)$ . This follows inductively from part 2. I recognize this is a pretty simple exercise but have I made any big oversights?",a) and by same argument. b) Let be disjoint sets in . c) The two requirements for a measure are . This is satisfied since is between 0 and 1. If are disjoint elements of then . This follows inductively from part 2. I recognize this is a pretty simple exercise but have I made any big oversights?,"\mu(E)=\lim_{n\rightarrow\infty} \frac{\#[\{2,4,\dots\}\cap \{1,\dots,n\}]}{n}=\lim_{n\rightarrow\infty}\frac{n/2}{n}=1/2 \mu(O)=1/2 \mu(S)=\lim_{n\rightarrow\infty}\frac{\#[ \{1,4,9,\dots\}\cap \{1,\dots,n\}]}{n}=\lim_{n\rightarrow\infty}\frac{\sqrt n}{n}=0 A,B \mathcal A \begin{split}\mu(A\cup B)&=\lim_{n\rightarrow\infty} \frac{\#[(A\cup B)\cap \{1,\dots,n\}]}{n}&&\text{definition}\\
&=\lim_{n\rightarrow\infty} \frac{\#[A\cap \{1,\dots,n\} \cup B \cap \{1,\dots,n\}]}{n}&&\text{DeMorgan's Law}\\
&=\lim_{n\rightarrow\infty} \frac{\#[A\cap \{1,\dots,n\}] +\#[ B \cap \{1,\dots,n\}]}{n}&&\text{A and B are disjoint}\\
&=\mu(A)+\mu(B)\end{split} \mu:\mathcal A\rightarrow[0,\infty] \mu(B) B_1,B_2,\dots \mathcal A \mu\left(\bigcup_{i=1}^\infty B_i\right)=\sum_{i=1}^\infty \mu(B_i)","['measure-theory', 'elementary-set-theory']"
15,Meager or measure zero $A\subset \mathbb R$ with $A+A=A$ and $A-A=\mathbb R$,Meager or measure zero  with  and,A\subset \mathbb R A+A=A A-A=\mathbb R,"Is there a meager or measure zero set $A\subset \mathbb R$ with $A+A=A$ and $A-A=\mathbb R$ ? I am using sumset notation $$A+A=\{a+b:a,b\in A\}$$ $$A-A=\{a-b:a,b\in A\}$$ In more algebraic terms, $A$ must be an additive submonoid that generates $\mathbb R$ as a group. (At least, assuming $0\in A,$ which is harmless.) It is a classic exercise to construct a meager measure zero set $A$ such that $A-A=\mathbb R.$ For example, take $A$ to be the set of numbers with no $9$ in any decimal expansion. But this doesn't satisfy $A+A=A.$ For any discontinuous additive function $f:\mathbb R\to\mathbb R$ the set $A=\{x:f(x)\geq 0\}$ satisfies the conditions except it only has inner measure zero. I want the outer measure to be zero.","Is there a meager or measure zero set with and ? I am using sumset notation In more algebraic terms, must be an additive submonoid that generates as a group. (At least, assuming which is harmless.) It is a classic exercise to construct a meager measure zero set such that For example, take to be the set of numbers with no in any decimal expansion. But this doesn't satisfy For any discontinuous additive function the set satisfies the conditions except it only has inner measure zero. I want the outer measure to be zero.","A\subset \mathbb R A+A=A A-A=\mathbb R A+A=\{a+b:a,b\in A\} A-A=\{a-b:a,b\in A\} A \mathbb R 0\in A, A A-A=\mathbb R. A 9 A+A=A. f:\mathbb R\to\mathbb R A=\{x:f(x)\geq 0\}","['measure-theory', 'baire-category', 'additive-combinatorics']"
16,New non-overlapping finite cover,New non-overlapping finite cover,,"I need help in the following problem Let $B_{i}$ be a finite family of open balls in $\mathbb{R}^n$ . Show that there exists a finite family of disjoint open balls $B'_{k}$ such that $\bigcup_{k}{B'_{k}} ⊃ \bigcup_{i}{B_{i}}$ and $\sum_{k} {r(B'_{k})} ≤ \sum_{i} r(B_{i})$ where $r$ is the radius. For the disjoint balls $B_{i}$ will keep them, what about the overlapping ones? How I can find this new cover? and does the result remain valid if we have a countable family of open balls with finite sum of the radii instead?","I need help in the following problem Let be a finite family of open balls in . Show that there exists a finite family of disjoint open balls such that and where is the radius. For the disjoint balls will keep them, what about the overlapping ones? How I can find this new cover? and does the result remain valid if we have a countable family of open balls with finite sum of the radii instead?",B_{i} \mathbb{R}^n B'_{k} \bigcup_{k}{B'_{k}} ⊃ \bigcup_{i}{B_{i}} \sum_{k} {r(B'_{k})} ≤ \sum_{i} r(B_{i}) r B_{i},"['real-analysis', 'measure-theory']"
17,A fat Cantor set essentially containing a positive measure subset and uncountably many of its translates,A fat Cantor set essentially containing a positive measure subset and uncountably many of its translates,,"Edit: In writing out more details, I realized I needed to make a slight modification to the question. I switched all proper containments from the old version of the question to ""essential containment"". This question arises in looking for a counterexample to a claim made in the book by Dunford and Schwartz, but seems interesting in its own right. The question I really want to answer is: ""Does there exist a Lebesgue measurable set $C \subseteq [0,1]$ with $m(C)>0$ s.t. for every Lebesgue measurable $m(S \cap C^c) = 0$ s.t. $m(S) >0$ , the set $\{x : m((x+S) \cap C^c) =0\}$ is Lebesgue null?"" It would suffice to show the following claim which seems possibly true: ""Let $C$ a fat Cantor set. Then for every $S \subseteq C$ s.t. $m(S)>0$ , the set $\{x : m((x +S) \cap C^c)=0\}$ is countable."" Edit: Due to a request in the comments, I'll post a simplification of the claim made in Dunford and Schwartz that inspired the question. Let $(A, \mathcal{A},\alpha), (B, \mathcal{B},\beta)$ $\sigma$ -finite measure spaces and $(A \times B, \mathcal{A} \otimes \mathcal{B}, \alpha \times \beta)$ their product. Let $f : A \times B \to \mathbb{R}$ measurable. Then there exists a sequence $\phi_n$ of finite linear combinations of indicators on measurable rectangles (sets of the form $S \times T, S \in \mathcal{A}, T \in \mathcal{B}$ ) s.t. $\phi_n \to f$ pointwise a.e. and $|\phi_n| \leq |f|$ . If we have a set $C$ of the above type, let $D := \{(x,x) + (c,0) : c \in C, 0 \leq x \leq 1\}.$ Then the above property shows that if $\phi$ is a finite linear combination of indicators of measurable rectangles s.t. $|\phi| \leq 1_D$ , then $\phi = 0$ a.e. This contradicts the Dunford and Schwartz claim as we can show that $m(D)>0$ .","Edit: In writing out more details, I realized I needed to make a slight modification to the question. I switched all proper containments from the old version of the question to ""essential containment"". This question arises in looking for a counterexample to a claim made in the book by Dunford and Schwartz, but seems interesting in its own right. The question I really want to answer is: ""Does there exist a Lebesgue measurable set with s.t. for every Lebesgue measurable s.t. , the set is Lebesgue null?"" It would suffice to show the following claim which seems possibly true: ""Let a fat Cantor set. Then for every s.t. , the set is countable."" Edit: Due to a request in the comments, I'll post a simplification of the claim made in Dunford and Schwartz that inspired the question. Let -finite measure spaces and their product. Let measurable. Then there exists a sequence of finite linear combinations of indicators on measurable rectangles (sets of the form ) s.t. pointwise a.e. and . If we have a set of the above type, let Then the above property shows that if is a finite linear combination of indicators of measurable rectangles s.t. , then a.e. This contradicts the Dunford and Schwartz claim as we can show that .","C \subseteq [0,1] m(C)>0 m(S \cap C^c) = 0 m(S) >0 \{x : m((x+S) \cap C^c) =0\} C S \subseteq C m(S)>0 \{x : m((x +S) \cap C^c)=0\} (A, \mathcal{A},\alpha), (B, \mathcal{B},\beta) \sigma (A \times B, \mathcal{A} \otimes \mathcal{B}, \alpha \times \beta) f : A \times B \to \mathbb{R} \phi_n S \times T, S \in \mathcal{A}, T \in \mathcal{B} \phi_n \to f |\phi_n| \leq |f| C D := \{(x,x) + (c,0) : c \in C, 0 \leq x \leq 1\}. \phi |\phi| \leq 1_D \phi = 0 m(D)>0","['real-analysis', 'measure-theory', 'cantor-set']"
18,An $\mathcal{S}$-measurable function defined on $\mathcal{S}=\{\bigcup_{k\in K}E_k:K\subset\mathbb{Z^+}\}$ must be constant on each $E_k$,An -measurable function defined on  must be constant on each,\mathcal{S} \mathcal{S}=\{\bigcup_{k\in K}E_k:K\subset\mathbb{Z^+}\} E_k,"I have proved the following statement(s) and I would like to know if my proof is correct and/or how it could be improved, thanks. ""Suppose $X$ is a set and $E_1, E_2,\dots $ is a disjoint sequence of subsets of $X$ such that $\bigcup_{k=1}^{\infty}E_k=X$ . Let $\mathcal{S}=\{\bigcup_{k\in K}E_k:K\subset\mathbb{Z^+}\}$ . (a) Show that $\mathcal{S}$ is a $\sigma$ -algebra on $X$ . (b) Prove that a function $f:X\to\mathbb{R}$ is $\mathcal{S}$ -measurable if and only if the function is constant on $E_k$ for every $k\in\mathbb{Z^+}$ . My proof: (a) If we take $K=\emptyset$ then $K\subset\mathbb{Z^+}$ so $\bigcup_{k\in K}E_k\in\mathcal{S}$ and $\bigcup_{k\in K}E_k=\emptyset$ so $\emptyset\in\mathcal{S}$ . Let $A\in\mathcal{S}$ : then $A=\bigcup_{k\in K_A}E_k$ for some $K_A\subset\mathbb{Z^+}$ so $X\setminus A=\bigcup_{k=1}^{\infty}E_k\setminus\bigcup_{k\in K_A}E_k=\bigcup_{k\in\mathbb{Z^+}}E_k\setminus\bigcup_{k\in K_A}E_k=\bigcup_{k\in\mathbb{Z^+}\setminus K_A}E_k$ and $\mathbb{Z^+}\setminus K_A\subset\mathbb{Z^+}$ so $A\in\mathcal{S}$ . Let $A_1,A_2,\dots \in\mathcal{S}$ : then $A_1=\bigcup_{k\in K_{A_1}}E_k, A_2=\bigcup_{k\in K_{A_2}}E_k,\dots, A_n=\bigcup_{k\in K_{A_n}}E_k,\dots$ so $\bigcup_{n=1}^{\infty}A_n=\bigcup_{n=1}^{\infty}\bigcup_{k\in K_{A_n}}E_k=\bigcup_{k\in K_{A_1}}E_k\cup\bigcup_{k\in K_{A_2}}E_k\cup\dots=\bigcup_{k\in K_A}E_k$ , where $K_A=\bigcup_{n=1}^{\infty} K_{A_n}\subset\mathbb{Z^+}$ so $\bigcup_{k\in K_A}E_k\in\mathcal{S}$ hence $\bigcup_{n=1}^{\infty}A_n\in\mathcal{S}$ . (b) $\fbox{$\Rightarrow$}$ Let $f:X\to\mathbb{R}$ be an $\mathcal{S}$ -measurable function and suppose for sake of contradiction there existed some $E_{K}, K\in\mathbb{Z^+}$ such that $f$ is not constant on it, ie $f(E_K)=\{y_1,y_2\}, y_1\neq y_2$ : then since $\{y_1\}$ is a Borel measurable subset of $\mathbb{R}$ we have that $f^{-1}(\{y_1\})\in\mathcal{S}$ so $f^{-1}(\{y_1\})=\bigcup_{k\in K_{y_1}}E_k$ for some $K_{y_1}\subset \mathbb{Z^+}$ hence we have $E_K\cap\bigcup_{k\in K_{y_1}} E_k\neq\emptyset$ and since the $E_k$ s form a partition of $X$ this can only be if $K\in K_{y_1}$ ie $E_K\subseteq\bigcup_{k\in K_{y_1}} E_k$ thus $f(E_k)=\{y_1,y_2\}\subseteq f(\bigcup_{k\in K_{y_1}}E_k)=\{y_1\}$ , contradiction. So, if $f:X\to\mathbb{R}$ is $S$ -measurable it must be constant on each $E_k, k\in\mathbb{Z^+}$ , as desired. $\fbox{$\Leftarrow$}$ Now, let $f:X\to\mathbb{R}$ be a function which is constant on each $E_k$ , ie $f(X)=\{y_1=f(E_1), y_2=f(E_2),\dots,y_n=f(E_n),\dots\}$ : then if $B$ is a Borel subset of $\mathbb{R}$ with $B\cap f(X)=\emptyset$ we have $f^{-1}(B)=\emptyset\in\mathcal{S}$ ; if $B\cap f(X)=f(X)$ we have $f^{-1}(B)=X\in\mathcal{S}$ ; if $B\cap f(X)\subsetneqq f(X), B\cap f(X)=\{y_{j_1},y_{j_2},\dots, y_{j_n},\dots \}, \{j_1,\dots,j_n,\dots\}\subset \mathbb{Z^+}$ we have $f^{-1}(B)=\bigcup_{k\in \{j_1,\dots,j_n.\dots\}}E_k\in\mathcal{S}$ . So since in all possible cases $f^{-1}(B)\in\mathcal{S}$ we can conclude that $f$ is $\mathcal{S}$ -measurable, as desired.","I have proved the following statement(s) and I would like to know if my proof is correct and/or how it could be improved, thanks. ""Suppose is a set and is a disjoint sequence of subsets of such that . Let . (a) Show that is a -algebra on . (b) Prove that a function is -measurable if and only if the function is constant on for every . My proof: (a) If we take then so and so . Let : then for some so and so . Let : then so , where so hence . (b) Let be an -measurable function and suppose for sake of contradiction there existed some such that is not constant on it, ie : then since is a Borel measurable subset of we have that so for some hence we have and since the s form a partition of this can only be if ie thus , contradiction. So, if is -measurable it must be constant on each , as desired. Now, let be a function which is constant on each , ie : then if is a Borel subset of with we have ; if we have ; if we have . So since in all possible cases we can conclude that is -measurable, as desired.","X E_1, E_2,\dots  X \bigcup_{k=1}^{\infty}E_k=X \mathcal{S}=\{\bigcup_{k\in K}E_k:K\subset\mathbb{Z^+}\} \mathcal{S} \sigma X f:X\to\mathbb{R} \mathcal{S} E_k k\in\mathbb{Z^+} K=\emptyset K\subset\mathbb{Z^+} \bigcup_{k\in K}E_k\in\mathcal{S} \bigcup_{k\in K}E_k=\emptyset \emptyset\in\mathcal{S} A\in\mathcal{S} A=\bigcup_{k\in K_A}E_k K_A\subset\mathbb{Z^+} X\setminus A=\bigcup_{k=1}^{\infty}E_k\setminus\bigcup_{k\in K_A}E_k=\bigcup_{k\in\mathbb{Z^+}}E_k\setminus\bigcup_{k\in K_A}E_k=\bigcup_{k\in\mathbb{Z^+}\setminus K_A}E_k \mathbb{Z^+}\setminus K_A\subset\mathbb{Z^+} A\in\mathcal{S} A_1,A_2,\dots \in\mathcal{S} A_1=\bigcup_{k\in K_{A_1}}E_k, A_2=\bigcup_{k\in K_{A_2}}E_k,\dots, A_n=\bigcup_{k\in K_{A_n}}E_k,\dots \bigcup_{n=1}^{\infty}A_n=\bigcup_{n=1}^{\infty}\bigcup_{k\in K_{A_n}}E_k=\bigcup_{k\in K_{A_1}}E_k\cup\bigcup_{k\in K_{A_2}}E_k\cup\dots=\bigcup_{k\in K_A}E_k K_A=\bigcup_{n=1}^{\infty} K_{A_n}\subset\mathbb{Z^+} \bigcup_{k\in K_A}E_k\in\mathcal{S} \bigcup_{n=1}^{\infty}A_n\in\mathcal{S} \fbox{\Rightarrow} f:X\to\mathbb{R} \mathcal{S} E_{K}, K\in\mathbb{Z^+} f f(E_K)=\{y_1,y_2\}, y_1\neq y_2 \{y_1\} \mathbb{R} f^{-1}(\{y_1\})\in\mathcal{S} f^{-1}(\{y_1\})=\bigcup_{k\in K_{y_1}}E_k K_{y_1}\subset \mathbb{Z^+} E_K\cap\bigcup_{k\in K_{y_1}} E_k\neq\emptyset E_k X K\in K_{y_1} E_K\subseteq\bigcup_{k\in K_{y_1}} E_k f(E_k)=\{y_1,y_2\}\subseteq f(\bigcup_{k\in K_{y_1}}E_k)=\{y_1\} f:X\to\mathbb{R} S E_k, k\in\mathbb{Z^+} \fbox{\Leftarrow} f:X\to\mathbb{R} E_k f(X)=\{y_1=f(E_1), y_2=f(E_2),\dots,y_n=f(E_n),\dots\} B \mathbb{R} B\cap f(X)=\emptyset f^{-1}(B)=\emptyset\in\mathcal{S} B\cap f(X)=f(X) f^{-1}(B)=X\in\mathcal{S} B\cap f(X)\subsetneqq f(X), B\cap f(X)=\{y_{j_1},y_{j_2},\dots, y_{j_n},\dots \}, \{j_1,\dots,j_n,\dots\}\subset \mathbb{Z^+} f^{-1}(B)=\bigcup_{k\in \{j_1,\dots,j_n.\dots\}}E_k\in\mathcal{S} f^{-1}(B)\in\mathcal{S} f \mathcal{S}","['real-analysis', 'measure-theory', 'solution-verification']"
19,Find a counterexample of: If $1_{A}+1_{B}$ is a random variable then $A$ and $B$ are measurable,Find a counterexample of: If  is a random variable then  and  are measurable,1_{A}+1_{B} A B,"I was trying to prove the following proposition, if $1_{A}+1_{B}$ is a random variable then $A$ and $B$ are measurable. Where $1_{A}$ is the indicator, function given by \begin{equation} 1_{A}(x) =  \begin{cases} 1, \ \   x \in A \\ 0, \ \   x \notin A \end{cases} \end{equation} I proved that the converse is true. Here's the proof: Let $A$ and $B$ be measurable sets, i.e, $A$ , $B \in \mathbb{F}$ , where $\mathbb{F}$ is a $\sigma$ -algebra.  Let $x \in \mathbb{R}$ , If $x<0$ then $(1_{A}+1_{B}\leq x ) =  \emptyset \in  \mathbb{F} $ if $x \in [0,1)$ then $(1_{A}+1_{B}\leq x ) = (A \cup B)^{\complement} \in \mathbb{F} $ if $x \in [1,2) $ then $ (1_{A}+1_{B} \leq x ) = (A \cap B )^{ \complement } \in \mathbb{F} $ if $x \geq 2  $ then $ (1_{A}+1_{B} \leq x ) = \Omega \in \mathbb{F} $ This implies that $1_{A}+1_{B}$ is random variable. But not able to prove that if $1_{A}+1_{B}$ is a random variable then $A$ and $B$ are measurable. I don't know where to start. I have the following Questions: Do you know a counterexample to this proposition? Do you have an idea that could help me to prove it? Thanks in advance.","I was trying to prove the following proposition, if is a random variable then and are measurable. Where is the indicator, function given by I proved that the converse is true. Here's the proof: Let and be measurable sets, i.e, , , where is a -algebra.  Let , If then if then if then if then This implies that is random variable. But not able to prove that if is a random variable then and are measurable. I don't know where to start. I have the following Questions: Do you know a counterexample to this proposition? Do you have an idea that could help me to prove it? Thanks in advance.","1_{A}+1_{B} A B 1_{A} \begin{equation}
1_{A}(x) = 
\begin{cases}
1, \ \   x \in A \\
0, \ \   x \notin A
\end{cases}
\end{equation} A B A B \in \mathbb{F} \mathbb{F} \sigma x \in \mathbb{R} x<0 (1_{A}+1_{B}\leq x ) =  \emptyset \in  \mathbb{F}  x \in [0,1) (1_{A}+1_{B}\leq x ) = (A \cup B)^{\complement} \in \mathbb{F}  x \in [1,2)   (1_{A}+1_{B} \leq x ) = (A \cap B )^{ \complement } \in \mathbb{F}  x \geq 2    (1_{A}+1_{B} \leq x ) = \Omega \in \mathbb{F}  1_{A}+1_{B} 1_{A}+1_{B} A B","['measure-theory', 'random-variables', 'examples-counterexamples']"
20,Prerequisites for rigorous Fourier analysis,Prerequisites for rigorous Fourier analysis,,"My side project for 2021 is to get a grasp for Fourier analysis. At first I thought that perhaps I should take an application heavy approach and do the heavy math later on. But the more I dwell deeper into the subject I find myself needing proper rigorous proofs and reasoning for the material I am presented. As Fourier analysis seems to rely heavily upon both measure theory and functional analysis, what books/material would you recommend to work on in order to build a solid & rigid foundations in both measure theory and functional analysis? Books and the material should preferably be self-contained. Bonus points if the book/material has been going around long enough, so that I can find discussion regarding the practice problems - mega bonus points if the material happens to have a solution manual. Currently my plan is to first go through both baby Rudin and Ahlfors ( Principles of Mathematical Analysis , Walter Rudin, Comples Analysis , Lars Ahlfors) in order to check that I don't have any serious holes in my understanding of real and complex analysis. From there I was planning to use Functional Analysis by Rudin as my primary source of functional analysis, and a yet undecided book for measure theory. I am also happy to takes notes and comments on this general study plan!","My side project for 2021 is to get a grasp for Fourier analysis. At first I thought that perhaps I should take an application heavy approach and do the heavy math later on. But the more I dwell deeper into the subject I find myself needing proper rigorous proofs and reasoning for the material I am presented. As Fourier analysis seems to rely heavily upon both measure theory and functional analysis, what books/material would you recommend to work on in order to build a solid & rigid foundations in both measure theory and functional analysis? Books and the material should preferably be self-contained. Bonus points if the book/material has been going around long enough, so that I can find discussion regarding the practice problems - mega bonus points if the material happens to have a solution manual. Currently my plan is to first go through both baby Rudin and Ahlfors ( Principles of Mathematical Analysis , Walter Rudin, Comples Analysis , Lars Ahlfors) in order to check that I don't have any serious holes in my understanding of real and complex analysis. From there I was planning to use Functional Analysis by Rudin as my primary source of functional analysis, and a yet undecided book for measure theory. I am also happy to takes notes and comments on this general study plan!",,"['functional-analysis', 'measure-theory', 'soft-question', 'fourier-analysis']"
21,Sublevel sets with boundary of Lebesgue measure zero,Sublevel sets with boundary of Lebesgue measure zero,,"so I was wondering if the following is true: consider two smooth convex functions $f,g: \mathbb{R}^n\to \mathbb{R}$ and consider the sublevel sets \begin{equation} \{f\leq g\}. \end{equation} I don't mind assuming compactness, if that's neccesary. If $g$ is either constant or linear then these are convex sets, thus their boundary has Lebesgue measure zero. In general this is true for $g$ concaive, but is it true for smooth convex $g$ ? What if we also assume $g$ strongly convex? If not, could you provide a counterexample?","so I was wondering if the following is true: consider two smooth convex functions and consider the sublevel sets I don't mind assuming compactness, if that's neccesary. If is either constant or linear then these are convex sets, thus their boundary has Lebesgue measure zero. In general this is true for concaive, but is it true for smooth convex ? What if we also assume strongly convex? If not, could you provide a counterexample?","f,g: \mathbb{R}^n\to \mathbb{R} \begin{equation}
\{f\leq g\}.
\end{equation} g g g g","['measure-theory', 'convex-analysis']"
22,Cartesian product of Vitali set and {0},Cartesian product of Vitali set and {0},,"I am doing a measure theory problem which says: Let $V$ be the Vitali set in $\mathbb{R}$ . Deduce that $E=V \times \lbrace0\rbrace$ is Lebesgue measurable in $\mathbb{R}^2$ but it's not a Borel set. I think I did well the first part. Let $V$ be a Vitali set. We know that $V \subset [0,1]$ . Let $\varepsilon>0$ . We can take the rectangle $[0,1] \times (-\varepsilon,\varepsilon)$ , which contains $E=V \times \lbrace0\rbrace$ , and is measurable. Taking limit when $\varepsilon \rightarrow 0$ , we have a measure $0$ set. Since the Lebesgue measure is complete, every subset of a $0$ measure set has measure $0$ , so $E=V \times \lbrace0\rbrace$ is measuable. My problem is with the second part. I have an idea, but I don't know if it's correct or not: We know that the Vitali set is not Borel. We can see $E$ as the inverse image of the inclusion of $\mathbb{R}$ in the $X$ axis of $\mathbb{R}^2$ , which is a measurable function. So, I tried the typical proof by contradiction: Suppose $E=V \times \lbrace0\rbrace$ is Borel. Then, $g^{-1}(E)$ has to be borel. But that is $V$ ! So $E$ is not Borel. Is this ok? Thanks in advance","I am doing a measure theory problem which says: Let be the Vitali set in . Deduce that is Lebesgue measurable in but it's not a Borel set. I think I did well the first part. Let be a Vitali set. We know that . Let . We can take the rectangle , which contains , and is measurable. Taking limit when , we have a measure set. Since the Lebesgue measure is complete, every subset of a measure set has measure , so is measuable. My problem is with the second part. I have an idea, but I don't know if it's correct or not: We know that the Vitali set is not Borel. We can see as the inverse image of the inclusion of in the axis of , which is a measurable function. So, I tried the typical proof by contradiction: Suppose is Borel. Then, has to be borel. But that is ! So is not Borel. Is this ok? Thanks in advance","V \mathbb{R} E=V \times \lbrace0\rbrace \mathbb{R}^2 V V \subset [0,1] \varepsilon>0 [0,1] \times (-\varepsilon,\varepsilon) E=V \times \lbrace0\rbrace \varepsilon \rightarrow 0 0 0 0 E=V \times \lbrace0\rbrace E \mathbb{R} X \mathbb{R}^2 E=V \times \lbrace0\rbrace g^{-1}(E) V E","['measure-theory', 'lebesgue-measure', 'borel-sets']"
23,"Show that if $f\in\mathcal L^1((a,b))$ has a weak derivative, then it has a continuous modification","Show that if  has a weak derivative, then it has a continuous modification","f\in\mathcal L^1((a,b))","Let $\lambda$ denote the Lebesgue measure on $\mathcal B(\mathbb R)$ , $a,b\in\mathbb R$ with $a<b$ , $I:=(a,b)$ , $X,Y$ be $\mathbb R$ -Banach spaces and $\iota$ be a continuous embedding of $X$ into $Y$ . If $p\in[1,\infty]$ , say $f\in\mathcal L^1(I,X)$ has a weak derivative in $L^p(I,Y)$ if there is a $g\in\mathcal L^p(I,Y)$ with $$\int_I\varphi'\iota f\:{\rm d}\lambda=-\int_I\varphi g\:{\rm d}\lambda\;\;\;\text{for all }\varphi\in C_c^\infty(I).$$ In that case, $f':=g$ . I would like to show that if $f\in\mathcal L^1(I,X)$ has a weak derivative in $L^1(I,Y)$ , then there is a $\tilde f\in C(I,Y)$ with $\iota f=\tilde f$ $\lambda$ -a.e. if $Y=X'$ and $X$ is continuously embedded into a Hilbert space $H$ via $\kappa:X\to H$ , then in the situation of (1.) there is a $\tilde f\in C(I,Y)$ with $\kappa f=\tilde f$ $\lambda$ -a.e. Regarding (1.): It's easy to see that if $g\in\mathcal L^1(I,Y)$ and $c\in Y$ , then $$f(t):=c+\int_{(a,\:t)}g\:{\rm d}\lambda\;\;\;\text{for }t\in\overline I$$ is continuous and weakly differentiable with $f'=g$ . Now, in the situation of (1.), by the aforementioned fact, $$g(t):=\int_{(a,\:t)}f'\:{\rm d}\lambda\;\;\;\text{for }t\in\overline I$$ is continuous and weakly differentiable with $$g'=f'\tag2.$$ Now, in Mathematical Tools for the Study of the Incompressible Navier-Stokes Equations and Related Models (Corollary II.4.2), the special case $X=Y=\mathbb R$ is considered and we can find the following argumentation The mentioned lemma is the following: Maybe I'm missing something, but I don't get why the argumentation is that ""complicated"". By $(1)$ and $(2)$ , it holds $$\int_I\varphi'(\iota f-g)\:{\rm d}\lambda=0\;\;\;\text{for all }\varphi\in C_c^\infty(I)\tag3.$$ Now, if $\varphi\in C_c^\infty(I)$ , then $$\psi(t):=\int_a^t\varphi(s)\:{\rm d}s\;\;\;\text{for }t\in I$$ is obviously in $C_c^\infty(I)$ as well and we've got $\psi'=\varphi$ . So, we can replace "" $\varphi'$ "" in $(3)$ by "" $\varphi$ "". And now the du Bois-Reymond lemma should immediately yield $\iota f=g$ $\lambda$ -a.e.. Am I missing something? Regarding (2.): At the moment, I've got no idea why this follows and would need some help to tackle that problem.","Let denote the Lebesgue measure on , with , , be -Banach spaces and be a continuous embedding of into . If , say has a weak derivative in if there is a with In that case, . I would like to show that if has a weak derivative in , then there is a with -a.e. if and is continuously embedded into a Hilbert space via , then in the situation of (1.) there is a with -a.e. Regarding (1.): It's easy to see that if and , then is continuous and weakly differentiable with . Now, in the situation of (1.), by the aforementioned fact, is continuous and weakly differentiable with Now, in Mathematical Tools for the Study of the Incompressible Navier-Stokes Equations and Related Models (Corollary II.4.2), the special case is considered and we can find the following argumentation The mentioned lemma is the following: Maybe I'm missing something, but I don't get why the argumentation is that ""complicated"". By and , it holds Now, if , then is obviously in as well and we've got . So, we can replace "" "" in by "" "". And now the du Bois-Reymond lemma should immediately yield -a.e.. Am I missing something? Regarding (2.): At the moment, I've got no idea why this follows and would need some help to tackle that problem.","\lambda \mathcal B(\mathbb R) a,b\in\mathbb R a<b I:=(a,b) X,Y \mathbb R \iota X Y p\in[1,\infty] f\in\mathcal L^1(I,X) L^p(I,Y) g\in\mathcal L^p(I,Y) \int_I\varphi'\iota f\:{\rm d}\lambda=-\int_I\varphi g\:{\rm d}\lambda\;\;\;\text{for all }\varphi\in C_c^\infty(I). f':=g f\in\mathcal L^1(I,X) L^1(I,Y) \tilde f\in C(I,Y) \iota f=\tilde f \lambda Y=X' X H \kappa:X\to H \tilde f\in C(I,Y) \kappa f=\tilde f \lambda g\in\mathcal L^1(I,Y) c\in Y f(t):=c+\int_{(a,\:t)}g\:{\rm d}\lambda\;\;\;\text{for }t\in\overline I f'=g g(t):=\int_{(a,\:t)}f'\:{\rm d}\lambda\;\;\;\text{for }t\in\overline I g'=f'\tag2. X=Y=\mathbb R (1) (2) \int_I\varphi'(\iota f-g)\:{\rm d}\lambda=0\;\;\;\text{for all }\varphi\in C_c^\infty(I)\tag3. \varphi\in C_c^\infty(I) \psi(t):=\int_a^t\varphi(s)\:{\rm d}s\;\;\;\text{for }t\in I C_c^\infty(I) \psi'=\varphi \varphi' (3) \varphi \iota f=g \lambda","['real-analysis', 'functional-analysis', 'measure-theory', 'partial-differential-equations', 'sobolev-spaces']"
24,Integration on a subset,Integration on a subset,,"Definition: Let $(X, \mathcal{A}, μ)$ be a measure space. Let $f:X\rightarrow \mathbb{R}$ be a measurable function. Let $E\in \mathcal{A}$ . Then $$\int_E fdμ:=\int_X (f\chi_E)dμ.$$ Let $(X, \mathcal{A}, μ)$ be a measure space. Let $f:X\rightarrow \mathbb{R}$ be a measurable function. Let $E\in \mathcal{A}$ . Assume that $\exists M>0$ s.t $$|f(x)|\le M,  \forall x\in E.$$ Then prove that $$|\int_E f dμ| \le Mμ(E).$$ Please help me in this proof. Definition is given above the proof of the statement. My try: $$|\int_E f dμ|=|\int_X (f \chi_E) dμ|$$ $$ \le \int_X |(f \chi_E)| dμ$$ $$= \int_X |f| |\chi_E| dμ$$ $$ \le  \int_X  M |\chi_E| dμ $$ $$= M \int_X |\chi_E| dμ$$ $$= M μ(E) $$",Definition: Let be a measure space. Let be a measurable function. Let . Then Let be a measure space. Let be a measurable function. Let . Assume that s.t Then prove that Please help me in this proof. Definition is given above the proof of the statement. My try:,"(X, \mathcal{A}, μ) f:X\rightarrow \mathbb{R} E\in \mathcal{A} \int_E fdμ:=\int_X (f\chi_E)dμ. (X, \mathcal{A}, μ) f:X\rightarrow \mathbb{R} E\in \mathcal{A} \exists M>0 |f(x)|\le M,  \forall x\in E. |\int_E f dμ| \le Mμ(E). |\int_E f dμ|=|\int_X (f \chi_E) dμ|  \le \int_X |(f \chi_E)| dμ = \int_X |f| |\chi_E| dμ  \le  \int_X  M |\chi_E| dμ  = M \int_X |\chi_E| dμ = M μ(E) ","['integration', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure', 'measurable-functions']"
25,Show that $\mathcal{F}$ and $\mathcal{G}$ are independent $\sigma$-algebras,Show that  and  are independent -algebras,\mathcal{F} \mathcal{G} \sigma,"We have a probability space $(\Omega, \mathcal{A}, \mathbb{P})$ , let $(A_n)_{n \geq 1}$ be a sequence of independent sub- $\sigma$ -algebras of $\mathcal{A}$ . We consider: $\mathcal{F}:= \sigma \left ( \bigcup_{n even} \mathcal{A}_n\right )$ and $\mathcal{G}:= \sigma \left ( \bigcup_{n odd} \mathcal{A}_n\right )$ I want to show that $\mathcal{F}$ and $\mathcal{G}$ are independent. For that I want to use a proposition and check that: $\mathcal{F}$ and $\mathcal{G}$ are independent $\iff \forall F \in \sigma(\mathcal{F}) = \{\emptyset, \mathcal{F}, \mathcal{F}^c, \Omega\}, \forall G \in \sigma(\mathcal{G}) = \{\emptyset, \mathcal{G}, \mathcal{G}^c, \Omega\} \implies \mathbb{P}[F \cap G] = \mathbb{P}[F] \mathbb{P} [G]$ . Now I would check for each combination the conditions hold: Case 1: If both $F = G = \emptyset$ then the statement obviously holds since by defintion $\mathbb{P}[\emptyset] = 0$ . Case 2: If either $F$ or $G$ equals the empty set, the condition hold by the same argument. Case 3: If both $F = G = \Omega$ the condition holds again by a similar argument. Now the harder cases are those if I have to consider for example: $F =  \mathcal{F}$ and $G = \mathcal{G}$ . How can I proceed here?","We have a probability space , let be a sequence of independent sub- -algebras of . We consider: and I want to show that and are independent. For that I want to use a proposition and check that: and are independent . Now I would check for each combination the conditions hold: Case 1: If both then the statement obviously holds since by defintion . Case 2: If either or equals the empty set, the condition hold by the same argument. Case 3: If both the condition holds again by a similar argument. Now the harder cases are those if I have to consider for example: and . How can I proceed here?","(\Omega, \mathcal{A}, \mathbb{P}) (A_n)_{n \geq 1} \sigma \mathcal{A} \mathcal{F}:= \sigma \left ( \bigcup_{n even} \mathcal{A}_n\right ) \mathcal{G}:= \sigma \left ( \bigcup_{n odd} \mathcal{A}_n\right ) \mathcal{F} \mathcal{G} \mathcal{F} \mathcal{G} \iff \forall F \in \sigma(\mathcal{F}) = \{\emptyset, \mathcal{F}, \mathcal{F}^c, \Omega\}, \forall G \in \sigma(\mathcal{G}) = \{\emptyset, \mathcal{G}, \mathcal{G}^c, \Omega\} \implies \mathbb{P}[F \cap G] = \mathbb{P}[F] \mathbb{P} [G] F = G = \emptyset \mathbb{P}[\emptyset] = 0 F G F = G = \Omega F =  \mathcal{F} G = \mathcal{G}","['measure-theory', 'independence']"
26,Invariant $\sigma$-algebras and Skew-Products,Invariant -algebras and Skew-Products,\sigma,"$\newcommand{\mc}{\mathcal}$ $\newcommand{\Z}{\mathbb Z}$ Let $(X, \mc X, \mu)$ be a probability space and $G$ be a countable group. Let $T:G\times X\to X$ be a measure preserving action of $G$ on $X$ . Let $\nu$ be a probability measure on $G$ such that the support of $\nu$ contains a generating set of $G$ . Equip $G^\Z$ with the probability measure $\nu^\Z$ and let $\sigma:G^\Z\to G^\Z$ be the shift map. We will write an element of $G^\Z$ as $g = (g_n)_{n\in \Z}$ . We define a map $\Phi:X\times G^\Z\to X\times G^\Z$ as $$ \Phi(x, g) = (T^{g_0}x, \sigma(g)) $$ By Fubini's theorem it follows that $\Phi$ preserves the measure $\mu\times \nu^{\Z}$ . Let $\mc I$ be the $\sigma$ -algebra on $X$ consisting of all the $G$ -invariant subsets of $X$ . Question. Can we describe the $\sigma$ -algebra of $\Phi$ -invariant subsets of $X\times G^{\mathbb Z}$ in terms of $\mc I$ . It is clear that for any $A\in \mc I$ we have $A\times G^\Z$ is $\Phi$ -invariant. Perhaps these are all the $\Phi$ -invariant subsets. The question is important since in order to get interesting information by applying ergodic theorems to the map $\Phi$ it would be useful to know the $\Phi$ -invariant $\sigma$ -algebra.",Let be a probability space and be a countable group. Let be a measure preserving action of on . Let be a probability measure on such that the support of contains a generating set of . Equip with the probability measure and let be the shift map. We will write an element of as . We define a map as By Fubini's theorem it follows that preserves the measure . Let be the -algebra on consisting of all the -invariant subsets of . Question. Can we describe the -algebra of -invariant subsets of in terms of . It is clear that for any we have is -invariant. Perhaps these are all the -invariant subsets. The question is important since in order to get interesting information by applying ergodic theorems to the map it would be useful to know the -invariant -algebra.,"\newcommand{\mc}{\mathcal} \newcommand{\Z}{\mathbb Z} (X, \mc X, \mu) G T:G\times X\to X G X \nu G \nu G G^\Z \nu^\Z \sigma:G^\Z\to G^\Z G^\Z g = (g_n)_{n\in \Z} \Phi:X\times G^\Z\to X\times G^\Z 
\Phi(x, g) = (T^{g_0}x, \sigma(g))
 \Phi \mu\times \nu^{\Z} \mc I \sigma X G X \sigma \Phi X\times G^{\mathbb Z} \mc I A\in \mc I A\times G^\Z \Phi \Phi \Phi \Phi \sigma","['measure-theory', 'ergodic-theory']"
27,$\sigma$-algebra generated by function,-algebra generated by function,\sigma,I have no idea how to calculate $\sigma$ -algebra generated by $Y=\sin(\pi x)$ . I'm looking for hints and suggestions to calculating the $\sigma$ -algebra.,I have no idea how to calculate -algebra generated by . I'm looking for hints and suggestions to calculating the -algebra.,\sigma Y=\sin(\pi x) \sigma,['measure-theory']
28,Does measurability on product space implies strong measurability?,Does measurability on product space implies strong measurability?,,"In Giovanni's book, A First Course in Sobolev Spaces, he says Theorem 8.28. Let $I \subset \mathbb{R}$ be an open interval, let $E \subset \mathbb{R}^n$ be a Lebegue measurable set, and let $1 \le p < \infty$ . Then $L^p(I; L^p(E))$ can be identificed with $L^p(E \times I)$ . I understand most of the proof, except that given $u \in L^p(E \times I)$ , $v(t) := u(\cdot\,; t)$ is a strongly measurable in $L^p(E \times I)$ . Since he defines $T: L^p(E \times I) \to L^p(I; L^p(E))$ with $T(u) = v$ , it would not make sense if $v$ is not strongly measurable. I tried to define a sequence of simple functions in $L^p(I; L^p(E))$ to approximate $v$ , but no luck. I'd appreciate any help! Thank you.","In Giovanni's book, A First Course in Sobolev Spaces, he says Theorem 8.28. Let be an open interval, let be a Lebegue measurable set, and let . Then can be identificed with . I understand most of the proof, except that given , is a strongly measurable in . Since he defines with , it would not make sense if is not strongly measurable. I tried to define a sequence of simple functions in to approximate , but no luck. I'd appreciate any help! Thank you.","I \subset \mathbb{R} E \subset \mathbb{R}^n 1 \le p < \infty L^p(I; L^p(E)) L^p(E \times I) u \in L^p(E \times I) v(t) := u(\cdot\,; t) L^p(E \times I) T: L^p(E \times I) \to L^p(I; L^p(E)) T(u) = v v L^p(I; L^p(E)) v","['real-analysis', 'integration', 'functional-analysis', 'measure-theory', 'bochner-spaces']"
29,Multiplier Operator Norm Estimate,Multiplier Operator Norm Estimate,,"I'm using Muscalu and Schlag's textbook to study harmonic analysis by myself, where they give the following exercise about multipliers' estimate: Consider a sequence of complex numbers $\{m_{n}\}_{n \in \mathbb{Z}}$ that satisfies: $$\sum_{n \in \mathbb{Z}}|m_{n}-m_{n-1}| \leq B, \ \lim_{n \rightarrow -\infty}m_{n} = 0$$ where $B > 0$ . Now let's define a multiplier operator $T$ : $$Tf(x)= \sum_{n \in \mathbb{Z}}m_{n}\hat{f}(n)e^{2\pi inx}$$ Then for any trigonometric polynomial $f$ and any $p \in (1,\infty)$ , there exists some constant $C_p > 0$ , such that: $$||Tf||_{L^p} \leq C_{p}B||f||_{L^p}$$ Any ideas on this? What I have tried is to try generalizing the claim to functions in $L^p$ . However, it seems that I can't find a proof or a counter-example for the general case....","I'm using Muscalu and Schlag's textbook to study harmonic analysis by myself, where they give the following exercise about multipliers' estimate: Consider a sequence of complex numbers that satisfies: where . Now let's define a multiplier operator : Then for any trigonometric polynomial and any , there exists some constant , such that: Any ideas on this? What I have tried is to try generalizing the claim to functions in . However, it seems that I can't find a proof or a counter-example for the general case....","\{m_{n}\}_{n \in \mathbb{Z}} \sum_{n \in \mathbb{Z}}|m_{n}-m_{n-1}| \leq B, \ \lim_{n \rightarrow -\infty}m_{n} = 0 B > 0 T Tf(x)= \sum_{n \in \mathbb{Z}}m_{n}\hat{f}(n)e^{2\pi inx} f p \in (1,\infty) C_p > 0 ||Tf||_{L^p} \leq C_{p}B||f||_{L^p} L^p","['measure-theory', 'inequality', 'fourier-analysis', 'normed-spaces', 'harmonic-analysis']"
30,reflexivity of Bochner space,reflexivity of Bochner space,,"Let $(\Omega, \mathscr F, \mu)$ be a $\sigma$ -finite measure space and $X$ be a Banach space, and assume that $X^*$ has the Radon-Nikodym property with respect to $(\Omega, \mathscr F, \mu)$ . I'm trying to prove the reflexivity of the Bochner space $L^p(\Omega;X)$ when $1< p <\infty$ . The following is my proof: Let $1/p + 1/q =1 $ and consider the map $\Phi_{p,X} : L^q(\Omega; X^*) \to L^p(\Omega; X)^*  $ defined by $$\langle f, \Phi_{p,X}g  \rangle := \int_\Omega \langle f,g\rangle \,d\mu, \quad f\in L^p(\Omega; X),\;g\in L^q(\Omega; X^*) .$$ Then by the Radon-Nikodym property, $\Phi_{p,X}$ is an isometric isomorphism. Note that, for all $f\in L^p(\Omega; X)$ and $g\in L^q(\Omega; X^*) $ we have $$ \langle f, \Phi_{p,X}g  \rangle = \int_\Omega \langle f,g\rangle \,d\mu  =  \int_\Omega \langle g,j_Xf\rangle \,d\mu  =  \langle g, \Phi_{q,X^*}(j_Xf)  \rangle, $$ where $j_X:X\to X^{**}$ is the canonical injection. Now, let $J:L^p(\Omega; X)\to L^p(\Omega; X)^{**} $ be the canonical injection.  Then for all $\Lambda \in L^p(\Omega; X)^* $ and $f\in L^p(\Omega; X) $ , we have $$ \langle \Lambda, Jf \rangle = \langle f,\Lambda \rangle  = \langle f,\Phi_{p,X}\Phi_{p,X}^{-1}\Lambda \rangle = \langle \Phi_{p,X}^{-1}\Lambda,\Phi_{q,X^*}(j_Xf) \rangle = \langle \Lambda,(\Phi_{p,X}^{-1})^*\Phi_{q,X^*}(j_Xf) \rangle, $$ where $(\Phi_{p,X}^{-1})^*$ is the adjoint operator of $\Phi_{p,X}^{-1}$ . Therefore $J$ is surjective. That is, $L^p(\Omega; X)$ is reflexive. Is the above proof correct ? Edited: I found that reflexivity of $X$ is also needed.","Let be a -finite measure space and be a Banach space, and assume that has the Radon-Nikodym property with respect to . I'm trying to prove the reflexivity of the Bochner space when . The following is my proof: Let and consider the map defined by Then by the Radon-Nikodym property, is an isometric isomorphism. Note that, for all and we have where is the canonical injection. Now, let be the canonical injection.  Then for all and , we have where is the adjoint operator of . Therefore is surjective. That is, is reflexive. Is the above proof correct ? Edited: I found that reflexivity of is also needed.","(\Omega, \mathscr F, \mu) \sigma X X^* (\Omega, \mathscr F, \mu) L^p(\Omega;X) 1< p <\infty 1/p + 1/q =1  \Phi_{p,X} : L^q(\Omega; X^*) \to L^p(\Omega; X)^*   \langle f, \Phi_{p,X}g  \rangle := \int_\Omega \langle f,g\rangle \,d\mu, \quad f\in L^p(\Omega; X),\;g\in L^q(\Omega; X^*) . \Phi_{p,X} f\in L^p(\Omega; X) g\in L^q(\Omega; X^*)   \langle f, \Phi_{p,X}g  \rangle = \int_\Omega \langle f,g\rangle \,d\mu  =  \int_\Omega \langle g,j_Xf\rangle \,d\mu  =  \langle g, \Phi_{q,X^*}(j_Xf)  \rangle,  j_X:X\to X^{**} J:L^p(\Omega; X)\to L^p(\Omega; X)^{**}  \Lambda \in L^p(\Omega; X)^*  f\in L^p(\Omega; X)   \langle \Lambda, Jf \rangle = \langle f,\Lambda \rangle  = \langle f,\Phi_{p,X}\Phi_{p,X}^{-1}\Lambda \rangle = \langle \Phi_{p,X}^{-1}\Lambda,\Phi_{q,X^*}(j_Xf) \rangle = \langle \Lambda,(\Phi_{p,X}^{-1})^*\Phi_{q,X^*}(j_Xf) \rangle,  (\Phi_{p,X}^{-1})^* \Phi_{p,X}^{-1} J L^p(\Omega; X) X","['measure-theory', 'solution-verification', 'radon-nikodym', 'bochner-spaces', 'reflexive-space']"
31,Convergence of sequence of sets is empty set iff limit of symmetric difference of sets exists.,Convergence of sequence of sets is empty set iff limit of symmetric difference of sets exists.,,"I'am trying to solve one of the exercises of Halmos's measure theory book, but I can't figure it out. Exercises goes as follows: Let $(E_n)_{n\in\mathbb{N}}$ sequence of sets. Define the sequence $(D_n)_{n\in\mathbb{N}}$ as follows: $$D_1=E_1,D_2=D_1\triangle E_2,~~~D_n=D_{n-1}\triangle E_n,~~\forall n\in\mathbb{N}_{\geq 3}.$$ Prove that the sequence $(D_n)$ converge to some set, say $A$ , if and only if $\lim_{n}E_n=\emptyset$ . I try to solve it with definitions of limes superior and limes inferior for sequences of sets but I'm struggling with understand what is the role of symmetric difference in limits of sequences of sets. My attempt: For sufficiency part, let $\lim_{n}D_n=A$ this implies that $\limsup_nD_n-\liminf_nD_n=\emptyset$ . So? What is this means? I also try to derive a contradiction with assuming that $\lim_nE_n\neq\emptyset$ but the sequence $(E_n)$ converge to some set, say U. How can I proceed from here? Can I derive a contradiction or is there any proof directly?","I'am trying to solve one of the exercises of Halmos's measure theory book, but I can't figure it out. Exercises goes as follows: Let sequence of sets. Define the sequence as follows: Prove that the sequence converge to some set, say , if and only if . I try to solve it with definitions of limes superior and limes inferior for sequences of sets but I'm struggling with understand what is the role of symmetric difference in limits of sequences of sets. My attempt: For sufficiency part, let this implies that . So? What is this means? I also try to derive a contradiction with assuming that but the sequence converge to some set, say U. How can I proceed from here? Can I derive a contradiction or is there any proof directly?","(E_n)_{n\in\mathbb{N}} (D_n)_{n\in\mathbb{N}} D_1=E_1,D_2=D_1\triangle E_2,~~~D_n=D_{n-1}\triangle E_n,~~\forall n\in\mathbb{N}_{\geq 3}. (D_n) A \lim_{n}E_n=\emptyset \lim_{n}D_n=A \limsup_nD_n-\liminf_nD_n=\emptyset \lim_nE_n\neq\emptyset (E_n)","['measure-theory', 'convergence-divergence']"
32,Integration with respect to a finitely-additive measure.,Integration with respect to a finitely-additive measure.,,"I found an explanation of how to integrate with respect to a finitely-additive measure (taken from a book) : If $M$ is an algebra (not necessarely a $\sigma$ -algebra!) on $X$ and $\mu : M → [0, ∞]$ is a finitely additive (not necesseraly countabley additive!) measure on $X$ , we can still define a notion of integral as follows: we say that a measurable function $u : X → [−∞, ∞]$ is integrable if there exists a sequence ${s_n }$ of measurable simple functions, each of them bounded and vanishing outside a set of finite measure (depending on $n$ ), such that $$\lim\limits_{n\to\infty} \mu ({x ∈ X : |u (x) − s_n (x)| > \epsilon}) = 0$$ for each $\epsilon >0$ , and $$\lim\limits_{n,l\to\infty}\int_X|s_n − s_l | d\mu = 0.(*)$$ It may be shown that for every $E \in M$ the limit $\lim\limits_{n\to\infty}\int_E s_n d\mu$ exists in $\mathbb{R}$ and does not depend on the particular sequence ${s_n}$ . The integral of $u$ over the measurable set E is defined by $$\int_E u d\mu := \lim\limits_{n\to\infty}\int_E s_n d\mu.$$ Now I think the fact that $\lim\limits_{n\to\infty}\int_E s_n d\mu$ exists is because by $(*)$ $\{\int_X s_nd\mu\}_{n\ge 1}$ is a Cauchy sequence and $\mathbb{R}$ is complete (is that correct?). However I am not sure how to show that the limit does not depend on the particular sequence. If I take two sequences $s_n$ and $\hat{s}_n$ ,  then $$\int_E (s_n-\hat{s}_n)d\mu = \sum\limits_{i=1}^I c_i \mu(E_i)-\sum\limits_{j=1}^J \hat{c}_j\mu(\hat{E}_j)$$ and then I don't know how to conclude.","I found an explanation of how to integrate with respect to a finitely-additive measure (taken from a book) : If is an algebra (not necessarely a -algebra!) on and is a finitely additive (not necesseraly countabley additive!) measure on , we can still define a notion of integral as follows: we say that a measurable function is integrable if there exists a sequence of measurable simple functions, each of them bounded and vanishing outside a set of finite measure (depending on ), such that for each , and It may be shown that for every the limit exists in and does not depend on the particular sequence . The integral of over the measurable set E is defined by Now I think the fact that exists is because by is a Cauchy sequence and is complete (is that correct?). However I am not sure how to show that the limit does not depend on the particular sequence. If I take two sequences and ,  then and then I don't know how to conclude.","M \sigma X \mu : M → [0, ∞] X u : X → [−∞, ∞] {s_n } n \lim\limits_{n\to\infty} \mu
({x ∈ X : |u (x) − s_n (x)| > \epsilon}) = 0 \epsilon >0 \lim\limits_{n,l\to\infty}\int_X|s_n − s_l | d\mu = 0.(*) E \in M \lim\limits_{n\to\infty}\int_E s_n d\mu \mathbb{R} {s_n} u \int_E u d\mu := \lim\limits_{n\to\infty}\int_E s_n d\mu. \lim\limits_{n\to\infty}\int_E s_n d\mu (*) \{\int_X s_nd\mu\}_{n\ge 1} \mathbb{R} s_n \hat{s}_n \int_E (s_n-\hat{s}_n)d\mu = \sum\limits_{i=1}^I c_i \mu(E_i)-\sum\limits_{j=1}^J \hat{c}_j\mu(\hat{E}_j)","['integration', 'measure-theory']"
33,"$\forall\varepsilon>0$ there exists a subset $F$ of $[0,1]$ that is closed, contains only irrational numbers and $|F|>1-\varepsilon$","there exists a subset  of  that is closed, contains only irrational numbers and","\forall\varepsilon>0 F [0,1] |F|>1-\varepsilon","I have proved the following result and I would like to know if I have made any mistakes (in particular, I am not sure that the set $F$ I have built is necessarily closed). Thank you. ""Let $\varepsilon >0$ . Prove there exists a subset $F$ of $[0,1]$ such that $F$ is closed, every elements of $F$ is an irrational number and $|F|>1-\varepsilon$ "" (NOTE: $|\cdot|$ denotes the outer measure) Let $r_1,r_2,\dots $ be the sequence that contains every rational number in $[0,1]$ , $\varepsilon >0$ and consider the set $F:=[0,1]-\bigcup_{k=1}^{\infty}(r_k-\frac{\varepsilon}{4\cdot 2^k},r_k+\frac{\varepsilon}{4\cdot 2^k})$ . Then $F$ is closed, being the complement of the open set $\bigcup_{k=1}^{\infty}(r_k-\frac{\varepsilon}{4\cdot 2^k},r_k+\frac{\varepsilon}{4\cdot 2^k})$ in $[0,1]$ , contains only irrational number since we have taken away all the rational ones, and $|F|\geq |[0,1]|-|\bigcup_{k=1}^{\infty}(r_k-\frac{\varepsilon}{4\cdot 2^k},r_k+\frac{\varepsilon}{4\cdot 2^k})|\geq (1-0)-\frac{\varepsilon}{2}\sum_{k=1}^{\infty}\frac{1}{2^k}=1-\frac{\varepsilon}{2}>1-\varepsilon$ .","I have proved the following result and I would like to know if I have made any mistakes (in particular, I am not sure that the set I have built is necessarily closed). Thank you. ""Let . Prove there exists a subset of such that is closed, every elements of is an irrational number and "" (NOTE: denotes the outer measure) Let be the sequence that contains every rational number in , and consider the set . Then is closed, being the complement of the open set in , contains only irrational number since we have taken away all the rational ones, and .","F \varepsilon >0 F [0,1] F F |F|>1-\varepsilon |\cdot| r_1,r_2,\dots  [0,1] \varepsilon >0 F:=[0,1]-\bigcup_{k=1}^{\infty}(r_k-\frac{\varepsilon}{4\cdot 2^k},r_k+\frac{\varepsilon}{4\cdot 2^k}) F \bigcup_{k=1}^{\infty}(r_k-\frac{\varepsilon}{4\cdot 2^k},r_k+\frac{\varepsilon}{4\cdot 2^k}) [0,1] |F|\geq |[0,1]|-|\bigcup_{k=1}^{\infty}(r_k-\frac{\varepsilon}{4\cdot 2^k},r_k+\frac{\varepsilon}{4\cdot 2^k})|\geq (1-0)-\frac{\varepsilon}{2}\sum_{k=1}^{\infty}\frac{1}{2^k}=1-\frac{\varepsilon}{2}>1-\varepsilon","['real-analysis', 'measure-theory', 'solution-verification', 'outer-measure']"
34,Are signed measures badly defined?,Are signed measures badly defined?,,"I'm reading Real Analysis - Modern Techniques and Their Applications by Folland. On page 85, signed measures are defined as follows. Let $(X, \mathcal{M})$ be a measurable space. A signed measure on $(X, \mathcal{M})$ is a function $\nu : \mathcal{M} \rightarrow [-\infty,\infty]$ such that: $\nu(\varnothing) = 0$ ; $\nu$ assumes at most one of the values $\pm \infty$ ; if $\{E_j\}$ is a sequence of disjoint sets in $\mathcal{M}$ , then $\nu(\bigcup_1^\infty E_j) = \sum_1^\infty \nu(E_j)$ , where the latter sum converges absolutely if $\nu(\bigcup_1^\infty E_j)$ is finite. My issue is with the third condition. Specifically, what is it saying if $\nu(\bigcup_1^\infty E_j)$ is not finite? The summation might not even make sense, depending on the values of the $\nu(E_j)$ (e.g. if it is the alternating series $\sum (-1)^j$ ). Is this condition saying that the sum will always make sense? Or is it only saying the equality holds when the sum makes sense? How can this condition be formulated correctly? And is there a textbook with a precise, rigorous formulation? (I couldn't find one.)","I'm reading Real Analysis - Modern Techniques and Their Applications by Folland. On page 85, signed measures are defined as follows. Let be a measurable space. A signed measure on is a function such that: ; assumes at most one of the values ; if is a sequence of disjoint sets in , then , where the latter sum converges absolutely if is finite. My issue is with the third condition. Specifically, what is it saying if is not finite? The summation might not even make sense, depending on the values of the (e.g. if it is the alternating series ). Is this condition saying that the sum will always make sense? Or is it only saying the equality holds when the sum makes sense? How can this condition be formulated correctly? And is there a textbook with a precise, rigorous formulation? (I couldn't find one.)","(X, \mathcal{M}) (X, \mathcal{M}) \nu : \mathcal{M} \rightarrow [-\infty,\infty] \nu(\varnothing) = 0 \nu \pm \infty \{E_j\} \mathcal{M} \nu(\bigcup_1^\infty E_j) = \sum_1^\infty \nu(E_j) \nu(\bigcup_1^\infty E_j) \nu(\bigcup_1^\infty E_j) \nu(E_j) \sum (-1)^j","['real-analysis', 'measure-theory', 'definition', 'signed-measures']"
35,Inequality for perimeter of Minkowski sum?,Inequality for perimeter of Minkowski sum?,,"This following is a small part of a rather large problem that has been bugging me. Let $\Omega$ be any bounded open set in $\mathbb{R}^{2}$ of finite perimeter. Is it necessarily true that $$\mathcal{H}^{1}(\partial (\Omega + B_{\delta})) \leq \mathcal{H}^{1}(\partial \Omega)+\mathcal{H}^{1}(\partial B_{\delta}) = \mathcal{H}^{1}(\partial \Omega)+2\pi\delta?$$ Here $\mathcal{H}^{1}$ is the Hausdorff measure, $B_{\delta}$ is the ball of radius $\delta$ and $ \Omega + B_{\delta}$ is the Minkowski sum of $\Omega$ and $B_{\delta}$ . Here $\partial \Omega$ denotes the topological boundary of $\Omega$ . The question is motivated by the a note somewhere that we have equality if $\Omega$ is convex. I can't seem to construct a counter example but nor a proof.","This following is a small part of a rather large problem that has been bugging me. Let be any bounded open set in of finite perimeter. Is it necessarily true that Here is the Hausdorff measure, is the ball of radius and is the Minkowski sum of and . Here denotes the topological boundary of . The question is motivated by the a note somewhere that we have equality if is convex. I can't seem to construct a counter example but nor a proof.",\Omega \mathbb{R}^{2} \mathcal{H}^{1}(\partial (\Omega + B_{\delta})) \leq \mathcal{H}^{1}(\partial \Omega)+\mathcal{H}^{1}(\partial B_{\delta}) = \mathcal{H}^{1}(\partial \Omega)+2\pi\delta? \mathcal{H}^{1} B_{\delta} \delta  \Omega + B_{\delta} \Omega B_{\delta} \partial \Omega \Omega \Omega,"['measure-theory', 'geometric-measure-theory']"
36,Different definitions of ring and field.,Different definitions of ring and field.,,"From Hanh and Rosenthal ""Set functions"" (1948, p.3): A system of sets which is closed with respect to addition and intersection is called a ring . A system of sets which is closed with respect to addition and subtraction is called a field . And by ""addition"" I understand ""union"". From Taylor ""Introduction to measure and integration"" (digital printed version, 2008, p.15): ...we see that a ring is a class of sets closed under the operations of union, intersection, and difference and... Thus a ring is a field if and only if it is closed under the operation of taking the complement. Very different definitions for me. I know that a ring (or field) in the algebra and in the set theory has different definitions (I am not sure). But here in both cases we talk about a class of sets. Question: what is going wrong? Maybe the last too definitions are more modern? Or there are some problems in my understanding?","From Hanh and Rosenthal ""Set functions"" (1948, p.3): A system of sets which is closed with respect to addition and intersection is called a ring . A system of sets which is closed with respect to addition and subtraction is called a field . And by ""addition"" I understand ""union"". From Taylor ""Introduction to measure and integration"" (digital printed version, 2008, p.15): ...we see that a ring is a class of sets closed under the operations of union, intersection, and difference and... Thus a ring is a field if and only if it is closed under the operation of taking the complement. Very different definitions for me. I know that a ring (or field) in the algebra and in the set theory has different definitions (I am not sure). But here in both cases we talk about a class of sets. Question: what is going wrong? Maybe the last too definitions are more modern? Or there are some problems in my understanding?",,"['measure-theory', 'set-theory']"
37,Sigma algebra of the set of continuous functions,Sigma algebra of the set of continuous functions,,In some reading I saw: Let $T$ be an index subset of $\mathbb{R}$ . Denote $C$ as the set of continuous functions from $T\to \mathbb{R}$ . Then $\mathcal{C}:=\sigma(C)$ is the same as the Borel sigma-algebra generated by the topology of uniform convergence on compact sets. May I ask where I can see more discussion about this?,In some reading I saw: Let be an index subset of . Denote as the set of continuous functions from . Then is the same as the Borel sigma-algebra generated by the topology of uniform convergence on compact sets. May I ask where I can see more discussion about this?,T \mathbb{R} C T\to \mathbb{R} \mathcal{C}:=\sigma(C),"['general-topology', 'measure-theory']"
38,Relationship between finite Radon measures and bounded variation measures,Relationship between finite Radon measures and bounded variation measures,,"I cannot figure out which is the relationship between Radon measures and measures that have bounded variation. I think it's almost a matter of definition, but I cannot find a proper source to which refer to. In particular, I was given the following definitions: given $\Omega \subset \mathbb{R}^n$ and $\mathcal{B}(\Omega)$ the $\sigma$ -algebra of Borel sets and $\Omega$ locally compact separeble space we say that a set function $\mu$ is a real Radon measure if it is a measure on the spaces $(K,\mathcal{B}(K))$ $\forall K \subset \Omega \text{ compact } $ that the same set function $\mu$ is a finite Radon measure if it is also a measure $\mathcal{B}(\Omega)$ First, I assume (for the confidence I place on my professor) that those definitions are equivalent in this setting to the more common ones based on the regularity of the measure. Moreover, given $\Omega \subset \mathbb{R}^n$ open nonempty, I was given this form of the Riesz's representation theorem: $\forall L$ linear and bouned functional on $C_0(\Omega;\mathbb{R}^m)$ $\exists!$ $\mathbb{R}^m$ -valued Radon measure $\mu$ such that the operatorial norm of $L$ is equal to the total variation of $\mu$ and $$\forall v \in C_0(\Omega;\mathbb{R}^m) \quad Lv=\int_{\Omega}\sum_{j=1}^{m}v_j \ d\mu_j$$ My problem arise when considering the space $\mathcal{M}(\Omega;\mathbb{R}^m)$ of bounded variation measures (I do not report here the definition fot the sake of brevity, but it is the usual one that can be found also here https://en.wikipedia.org/wiki/Vector_measure ). My professor states that $\mathcal{M}(\Omega;\mathbb{R}^m)$ is the topological dual of $C_0(\Omega;\mathbb{R}^m)$ by the Riesz's theorem above stated, but this means that every finite Radon measure is bounded variation. How can be this? I know that every real valued measure (no $\pm \infty $ values allowed) is bounded variation (cf. Rudin, RCA, p.118 thm 6.4), but this is not true for $\mathbb{R}^m$ -vector valued measures in general (am I wrong?) and so we are asserting that any $\mathbb{R}^m$ -vector valued finite Radon measure in the sense above is also bounded variation, so any finite Radon measure is bounded variation? Is this the sense of those definitions? Thanks for the attention!","I cannot figure out which is the relationship between Radon measures and measures that have bounded variation. I think it's almost a matter of definition, but I cannot find a proper source to which refer to. In particular, I was given the following definitions: given and the -algebra of Borel sets and locally compact separeble space we say that a set function is a real Radon measure if it is a measure on the spaces that the same set function is a finite Radon measure if it is also a measure First, I assume (for the confidence I place on my professor) that those definitions are equivalent in this setting to the more common ones based on the regularity of the measure. Moreover, given open nonempty, I was given this form of the Riesz's representation theorem: linear and bouned functional on -valued Radon measure such that the operatorial norm of is equal to the total variation of and My problem arise when considering the space of bounded variation measures (I do not report here the definition fot the sake of brevity, but it is the usual one that can be found also here https://en.wikipedia.org/wiki/Vector_measure ). My professor states that is the topological dual of by the Riesz's theorem above stated, but this means that every finite Radon measure is bounded variation. How can be this? I know that every real valued measure (no values allowed) is bounded variation (cf. Rudin, RCA, p.118 thm 6.4), but this is not true for -vector valued measures in general (am I wrong?) and so we are asserting that any -vector valued finite Radon measure in the sense above is also bounded variation, so any finite Radon measure is bounded variation? Is this the sense of those definitions? Thanks for the attention!","\Omega \subset \mathbb{R}^n \mathcal{B}(\Omega) \sigma \Omega \mu (K,\mathcal{B}(K)) \forall K \subset \Omega \text{ compact }  \mu \mathcal{B}(\Omega) \Omega \subset \mathbb{R}^n \forall L C_0(\Omega;\mathbb{R}^m) \exists! \mathbb{R}^m \mu L \mu \forall v \in C_0(\Omega;\mathbb{R}^m) \quad Lv=\int_{\Omega}\sum_{j=1}^{m}v_j \ d\mu_j \mathcal{M}(\Omega;\mathbb{R}^m) \mathcal{M}(\Omega;\mathbb{R}^m) C_0(\Omega;\mathbb{R}^m) \pm \infty  \mathbb{R}^m \mathbb{R}^m","['measure-theory', 'geometric-measure-theory', 'bounded-variation']"
39,Monotone class theorem to show the property for nonnegative measurable functions,Monotone class theorem to show the property for nonnegative measurable functions,,"The Monotone Class theorem: Suppose $\mathcal{A}_0$ is an algebra, $\mathcal{A}$ is the smallest σ-algebra containing $\mathcal{A}_0$ , and $\mathcal{M}$ is the smallest monotone class containing $\mathcal{A}_0$ . Then $\mathcal{M}=\mathcal{A}$ . Suppose we want to prove a property $P$ for all nonnegative $\mathcal{S}$ -measurable functions $Z$ . Can we prove the property holds for simple functions or bounded continuous functions and use Monotone Class Theorem to extend the result to all nonnegative measurable functions? I know the technique holds for bounded functions, and I wonder if we may extend it to the unbounded cases? A similar discussion appears in Monotone class theorem for unbounded functions but they are talking about general measurable functions instead of nonnegative ones.","The Monotone Class theorem: Suppose is an algebra, is the smallest σ-algebra containing , and is the smallest monotone class containing . Then . Suppose we want to prove a property for all nonnegative -measurable functions . Can we prove the property holds for simple functions or bounded continuous functions and use Monotone Class Theorem to extend the result to all nonnegative measurable functions? I know the technique holds for bounded functions, and I wonder if we may extend it to the unbounded cases? A similar discussion appears in Monotone class theorem for unbounded functions but they are talking about general measurable functions instead of nonnegative ones.",\mathcal{A}_0 \mathcal{A} \mathcal{A}_0 \mathcal{M} \mathcal{A}_0 \mathcal{M}=\mathcal{A} P \mathcal{S} Z,"['measure-theory', 'stochastic-processes', 'random-variables']"
40,Is this set $\left\lbrace\frac{1}{n}: n \in \mathbb{N}\right\rbrace$ a $G_\delta$ set?,Is this set  a  set?,\left\lbrace\frac{1}{n}: n \in \mathbb{N}\right\rbrace G_\delta,"Is the set $$\left\lbrace\frac{1}{n}: n \in \mathbb{N}\right\rbrace,$$ a $G_{\delta}$ set? I know that this set is $F_\sigma$ , but not sure if it is $G_\delta$ . Edit: Thanks to everyone who commented very quickly. I guess my attempt would be to create the sets $A_n$ such that $A_n = (1-\frac{1}{n},1+\frac{1}{n}) \cup (\frac{1}{2}-\frac{1}{n},\frac{1}{2}+\frac{1}{n}) \cup \cdots$ . Then taking the intersection over all natural numbers leads to the desired set. Is this correct?","Is the set a set? I know that this set is , but not sure if it is . Edit: Thanks to everyone who commented very quickly. I guess my attempt would be to create the sets such that . Then taking the intersection over all natural numbers leads to the desired set. Is this correct?","\left\lbrace\frac{1}{n}: n \in \mathbb{N}\right\rbrace, G_{\delta} F_\sigma G_\delta A_n A_n = (1-\frac{1}{n},1+\frac{1}{n}) \cup (\frac{1}{2}-\frac{1}{n},\frac{1}{2}+\frac{1}{n}) \cup \cdots","['real-analysis', 'measure-theory']"
41,Integration of a radial function over a bounded domain,Integration of a radial function over a bounded domain,,"Let $\Omega$ be a bounded domain in $\mathbb{R}^N$ . Let $f(x)=|x|^\alpha$ . Then $f\in L^1(\Omega)$ if $\alpha>-N$ . The above fact seems to hold for the following reason. Since $\Omega$ is bounded, there exists $R>0$ such that $\Omega\subset B(0,R)$ . Then, we have \begin{align*} I&=\int_{\Omega}f(x)dx\\ &\leq\int_{B(0,R)}|x|^\alpha dx\\ &=\frac{r^{\alpha+N}}{\alpha+N}\Bigg|_{0}^{R}\\ &=\frac{R^{\alpha+N}}{\alpha+N}, \end{align*} where $\alpha+N>0$ . Then we get $f\in L^1(\Omega)$ . Kindly inform me, if the above argument seems fine with you. Thanks.","Let be a bounded domain in . Let . Then if . The above fact seems to hold for the following reason. Since is bounded, there exists such that . Then, we have where . Then we get . Kindly inform me, if the above argument seems fine with you. Thanks.","\Omega \mathbb{R}^N f(x)=|x|^\alpha f\in L^1(\Omega) \alpha>-N \Omega R>0 \Omega\subset B(0,R) \begin{align*}
I&=\int_{\Omega}f(x)dx\\
&\leq\int_{B(0,R)}|x|^\alpha dx\\
&=\frac{r^{\alpha+N}}{\alpha+N}\Bigg|_{0}^{R}\\
&=\frac{R^{\alpha+N}}{\alpha+N},
\end{align*} \alpha+N>0 f\in L^1(\Omega)","['integration', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure', 'riemann-integration']"
42,structure of sigma algebra generated by a set,structure of sigma algebra generated by a set,,"i don't have idea of solve this problem. this problem from ""Problems in Real and Functional Analysis by Alberto Torchinsky"" Given a nonempty class ${\cal C}$ of subsets of $X$ , let ${\cal{C}}_1=\left \{A\subset X\ ; \ A\in{\cal{C}}\ \mbox{or}\ A^c\in {\cal{C}}\right \}$ , ${\cal{C}}_2=\left \{\bigcap_n A_n \ ; \ A_n\in{\cal{C}}_1\right \}$ , and ${\cal{C}}_3=\left \{\bigcup_n A_n \ ; \ A_n\in{\cal{C}}_2\right \}$ . Prove that ${\cal{C}}_3={\cal M}(C)$ . which ${\cal M}(C)$ is $\sigma$ -algebra generated by ${\cal C}$ I know structure of sigma-algebra generated by finite set.","i don't have idea of solve this problem. this problem from ""Problems in Real and Functional Analysis by Alberto Torchinsky"" Given a nonempty class of subsets of , let , , and . Prove that . which is -algebra generated by I know structure of sigma-algebra generated by finite set.",{\cal C} X {\cal{C}}_1=\left \{A\subset X\ ; \ A\in{\cal{C}}\ \mbox{or}\ A^c\in {\cal{C}}\right \} {\cal{C}}_2=\left \{\bigcap_n A_n \ ; \ A_n\in{\cal{C}}_1\right \} {\cal{C}}_3=\left \{\bigcup_n A_n \ ; \ A_n\in{\cal{C}}_2\right \} {\cal{C}}_3={\cal M}(C) {\cal M}(C) \sigma {\cal C},"['real-analysis', 'measure-theory']"
43,Dual problem for Prokhorov metric,Dual problem for Prokhorov metric,,"In optimal transport, one of the fundamentals of the theory is that the problem defining the Wasserstein/Kantorovich metric admits a dual problem. Namely that $$ \inf_{\pi \in \Pi(\mu,\nu)} \int_{X \times Y} c(x,y) d\pi = \sup_{\varphi \in C(X)} \int_X \varphi(x) + \int_Y \varphi^c(y) $$ The Prokhorov-Levy metric, defined as $$ \rho(\mu,\nu) = \inf_{\varepsilon >0 } \{ \mu(A) \leq \nu(A^\varepsilon) + \varepsilon : \forall A \in \mathcal{B}(X) \} $$ where $A^\varepsilon = \bigcup_{a \in A} B(a,\varepsilon)$ . This metric also metrizes weak* convergence in $\mathcal{P}(X)$ , and is also defined by an optimization problem. Is there any research into a dual problem for this? It seems hard to find any modern references on this metric at all. I tried writing the objective in terms of $\varepsilon +$ indicators of the constraints, but attempting to apply some sort of Fenchel duality produces a conjugate that depends on the primal (I can detail this if there is interest, it doesn't seem to work).","In optimal transport, one of the fundamentals of the theory is that the problem defining the Wasserstein/Kantorovich metric admits a dual problem. Namely that The Prokhorov-Levy metric, defined as where . This metric also metrizes weak* convergence in , and is also defined by an optimization problem. Is there any research into a dual problem for this? It seems hard to find any modern references on this metric at all. I tried writing the objective in terms of indicators of the constraints, but attempting to apply some sort of Fenchel duality produces a conjugate that depends on the primal (I can detail this if there is interest, it doesn't seem to work)."," \inf_{\pi \in \Pi(\mu,\nu)} \int_{X \times Y} c(x,y) d\pi = \sup_{\varphi \in C(X)} \int_X \varphi(x) + \int_Y \varphi^c(y)   \rho(\mu,\nu) = \inf_{\varepsilon >0 } \{ \mu(A) \leq \nu(A^\varepsilon) + \varepsilon : \forall A \in \mathcal{B}(X) \}  A^\varepsilon = \bigcup_{a \in A} B(a,\varepsilon) \mathcal{P}(X) \varepsilon +","['measure-theory', 'optimization', 'convex-analysis', 'optimal-transport']"
44,How to write any member of a sigma algebra as a subset of a countable union of members of the generating algebra,How to write any member of a sigma algebra as a subset of a countable union of members of the generating algebra,,"I’m currently working my way through Real Analysis by Folland and, in his proof of theorem 1.14 he does something I don’t quite understand: Take some algebra $\mathcal{A}$ on some set $X$ which generates some $\sigma$ -algebra $\mathcal{M}$ . If we have $E \in \mathcal{M}$ then we write $E \subset A$ , where $A := \cup_{1}^{\infty} A_j$ for some $A_j \in \mathcal{A}$ . Now, I believe this is possible because $X \in \mathcal{A}$ and $E \subset X$ . However, what I don’t understand is that he then goes on to say that, given some $\epsilon > 0$ we can choose the $A_j$ ’s in such a way that $\mu(A) \leq \mu(E) + \epsilon$ for a measure $\mu$ on $(X, \mathcal{M})$ . How can we make such a choice?","I’m currently working my way through Real Analysis by Folland and, in his proof of theorem 1.14 he does something I don’t quite understand: Take some algebra on some set which generates some -algebra . If we have then we write , where for some . Now, I believe this is possible because and . However, what I don’t understand is that he then goes on to say that, given some we can choose the ’s in such a way that for a measure on . How can we make such a choice?","\mathcal{A} X \sigma \mathcal{M} E \in \mathcal{M} E \subset A A := \cup_{1}^{\infty} A_j A_j \in \mathcal{A} X \in \mathcal{A} E \subset X \epsilon > 0 A_j \mu(A) \leq \mu(E) + \epsilon \mu (X, \mathcal{M})","['real-analysis', 'measure-theory', 'outer-measure']"
45,Prove that $\ell^p$ is complete.,Prove that  is complete.,\ell^p,"Prove that $(\ell^p, \|\cdot\|_p)$ is complete. My attempt $:$ Let $\sum\limits_{n=0}^{\infty} x^{(n)}$ be an absolutely summable series in $\ell^p.$ Let $x^{(n)} = \left (x_j^{(n)} \right )_{j \geq 0}.$ Need to show that $\sum\limits_{n=0}^{\infty} x^{(n)}$ converges in $\ell^p.$ Let $M = \sum\limits_{n=0}^{\infty} \left \|x^{(n)} \right \|_p.$ Let $(y^{(n)})_{n \geq 1}$ be the sequence of partial sums of the series $\sum\limits_{n=0}^{\infty} x^{(n)}.$ So for each $n \geq 0$ we have $$y^{(n)} = \sum\limits_{k=0}^{n} x^{(k)} = \left (\sum\limits_{k=0}^{n} x_j^{(k)} \right )_{j \geq 0} = \left (y_j^{(n)} \right )_{j \geq 0}.$$ where $y_j^{(n)} = \sum\limits_{k=0}^{n} x_j^{(k)}, j \geq 0.$ Now for each $j \geq 0$ consider the sequence $\left (z_j^{(n)} \right )_{n \geq 0}$ defined by $$z_j^{(n)} = \sum\limits_{k=0}^{n} \left |x_j^{(k)} \right |,\ n \geq 0.$$ Then it is easy to see that $\left (z_j^{(n)} \right )_{n \geq 0}$ is increasing and bounded above by $M$ for all $j \geq 0$ and hence it is convergent for all $j \geq 0.$ Let $z_j : = \lim\limits_{n \to \infty} z_j^{(n)},\ j \geq 0.$ For each $n \geq 0$ define the sequence $\left |x^{(n)} \right | : = \left (\left |x_j^{(n)} \right | \right )_{j \geq 0}.$ Let $z^{(n)} : = \sum\limits_{k=0}^{n} \left |x^{(k)} \right | = \left (\sum\limits_{k=0}^{n} \left |x_j^{(k)} \right | \right )_{j \geq 0} = \left (z_j^{(n)} \right )_{j \geq 0},\ n \geq 0.$ Then by Minkowski's inequality it follows that $$\left \|z^{(n)} \right \|_p \leq \sum\limits_{k=0}^{n} \left \|x^{(k)} \right \|_p \leq M.$$ Now for each $j \geq 0$ we have $\left (z_j^{(n)} \right )_{n \geq 0}$ increases to $z_j.$ So by MCT it follows that $$\sum\limits_{j=0}^{\infty} z_j^p = \lim\limits_{n \to \infty} \sum\limits_{j=0}^{\infty} {z_j^{(n)}}^p = \lim\limits_{n \to \infty} \left \|z^{(n)} \right \|_p^{p} \leq M^p.$$ This shows that $\|z\|_p \leq M,$ where $z = (z_j)_{j \geq 0}.$ Hence $z \in \ell^p.$ Now since $\left (z_j^{(n)} \right )_{n \geq 0}$ is convergent for each $j \geq 0,$ it follows that $\left (y_j^{(n)} \right )_{n \geq 0}$ is convergent for each $j \geq 0.$ Let $y_j = \lim\limits_{n \to \infty} y_j^{(n)},\ j \geq 0.$ Let $y = (y_j)_{j \geq 0}.$ If we can prove that $y \in \ell^p$ and $\left \|y^{(n)} - y \right \|_p \to 0$ as $n \to \infty$ then we are through. Now $$\left |y_j^{(n)} \right | \leq \sum\limits_{k=0}^{n} \left |x_j^{(k)} \right | = z_j^{(n)}.$$ Hence we have $$\left |y_j^{(n)} \right |^p \leq {z_j^{(n)}}^p.$$ Since $\left \|z^{(n)} \right \|_p \lt \infty$ it follows from DCT that $y \in \ell^p$ and $$\left \|y^{(n)} - y \right \|_p \to 0\ \text {as}\ n \to \infty.$$ Is my attempt correct at all? Please check it. Thanks in advance.",Prove that is complete. My attempt Let be an absolutely summable series in Let Need to show that converges in Let Let be the sequence of partial sums of the series So for each we have where Now for each consider the sequence defined by Then it is easy to see that is increasing and bounded above by for all and hence it is convergent for all Let For each define the sequence Let Then by Minkowski's inequality it follows that Now for each we have increases to So by MCT it follows that This shows that where Hence Now since is convergent for each it follows that is convergent for each Let Let If we can prove that and as then we are through. Now Hence we have Since it follows from DCT that and Is my attempt correct at all? Please check it. Thanks in advance.,"(\ell^p, \|\cdot\|_p) : \sum\limits_{n=0}^{\infty} x^{(n)} \ell^p. x^{(n)} = \left (x_j^{(n)} \right )_{j \geq 0}. \sum\limits_{n=0}^{\infty} x^{(n)} \ell^p. M = \sum\limits_{n=0}^{\infty} \left \|x^{(n)} \right \|_p. (y^{(n)})_{n \geq 1} \sum\limits_{n=0}^{\infty} x^{(n)}. n \geq 0 y^{(n)} = \sum\limits_{k=0}^{n} x^{(k)} = \left (\sum\limits_{k=0}^{n} x_j^{(k)} \right )_{j \geq 0} = \left (y_j^{(n)} \right )_{j \geq 0}. y_j^{(n)} = \sum\limits_{k=0}^{n} x_j^{(k)}, j \geq 0. j \geq 0 \left (z_j^{(n)} \right )_{n \geq 0} z_j^{(n)} = \sum\limits_{k=0}^{n} \left |x_j^{(k)} \right |,\ n \geq 0. \left (z_j^{(n)} \right )_{n \geq 0} M j \geq 0 j \geq 0. z_j : = \lim\limits_{n \to \infty} z_j^{(n)},\ j \geq 0. n \geq 0 \left |x^{(n)} \right | : = \left (\left |x_j^{(n)} \right | \right )_{j \geq 0}. z^{(n)} : = \sum\limits_{k=0}^{n} \left |x^{(k)} \right | = \left (\sum\limits_{k=0}^{n} \left |x_j^{(k)} \right | \right )_{j \geq 0} = \left (z_j^{(n)} \right )_{j \geq 0},\ n \geq 0. \left \|z^{(n)} \right \|_p \leq \sum\limits_{k=0}^{n} \left \|x^{(k)} \right \|_p \leq M. j \geq 0 \left (z_j^{(n)} \right )_{n \geq 0} z_j. \sum\limits_{j=0}^{\infty} z_j^p = \lim\limits_{n \to \infty} \sum\limits_{j=0}^{\infty} {z_j^{(n)}}^p = \lim\limits_{n \to \infty} \left \|z^{(n)} \right \|_p^{p} \leq M^p. \|z\|_p \leq M, z = (z_j)_{j \geq 0}. z \in \ell^p. \left (z_j^{(n)} \right )_{n \geq 0} j \geq 0, \left (y_j^{(n)} \right )_{n \geq 0} j \geq 0. y_j = \lim\limits_{n \to \infty} y_j^{(n)},\ j \geq 0. y = (y_j)_{j \geq 0}. y \in \ell^p \left \|y^{(n)} - y \right \|_p \to 0 n \to \infty \left |y_j^{(n)} \right | \leq \sum\limits_{k=0}^{n} \left |x_j^{(k)} \right | = z_j^{(n)}. \left |y_j^{(n)} \right |^p \leq {z_j^{(n)}}^p. \left \|z^{(n)} \right \|_p \lt \infty y \in \ell^p \left \|y^{(n)} - y \right \|_p \to 0\ \text {as}\ n \to \infty.","['functional-analysis', 'measure-theory', 'solution-verification', 'complete-spaces']"
46,How to show a null-set is a $\sigma$-algebra if and only if $\mu(\Omega)=0$?,How to show a null-set is a -algebra if and only if ?,\sigma \mu(\Omega)=0,"I am having a bit of trouble with my assigment on Measures and Integrals. First I have been giving the measure space $(\Omega, \mathcal{A}, \mu)$ and the $\mu$ -null set (or empty-set): $\mathcal{N}_\mu=\{N\subseteq \Omega| \quad \exists A \in \mathcal{A}: N \subseteq A \text{ and } \mu(A)=0\}$ . I have proved the following properties: $\emptyset \in \mathcal{N}_\mu$ and If $N_n\in \mathcal{N}_\mu, \quad n=1,2,... ,$ then $\bigcup_{n=1}^\infty N_n \in \mathcal{N}_\mu$ . The last thing I need to show is that: $\mathcal{N}_\mu$ is a $\sigma$ -algebra if and only if $\mu(\Omega)=0$ . Maybe some of you guys can give me a hint on what to do, to get started on the proof? Thank you.","I am having a bit of trouble with my assigment on Measures and Integrals. First I have been giving the measure space and the -null set (or empty-set): . I have proved the following properties: and If then . The last thing I need to show is that: is a -algebra if and only if . Maybe some of you guys can give me a hint on what to do, to get started on the proof? Thank you.","(\Omega, \mathcal{A}, \mu) \mu \mathcal{N}_\mu=\{N\subseteq \Omega| \quad \exists A \in \mathcal{A}: N \subseteq A \text{ and } \mu(A)=0\} \emptyset \in \mathcal{N}_\mu N_n\in \mathcal{N}_\mu, \quad n=1,2,... , \bigcup_{n=1}^\infty N_n \in \mathcal{N}_\mu \mathcal{N}_\mu \sigma \mu(\Omega)=0",[]
47,On the convexity/concavity of c-concave functions,On the convexity/concavity of c-concave functions,,"Given a cost function $c:\mathbb{R}^d\times \mathbb{R}^d\rightarrow \mathbb{R}^+$ we say a function $\phi:\mathbb{R}^d\rightarrow \mathbb{R}$ is c-concave when there exists some $\psi:\mathbb{R}^d\rightarrow \mathbb{R}\cup\{-\infty\}$ , $\psi\not\equiv -\infty$ such that $$ \phi(x) = \inf_{y\in\mathbb{R}^d} [c(x,y)-\psi(y)]. $$ If the cost function is the usual euclidean metric $c(x,y) = \frac{1}{2}|x-y|^2$ then we know a function $\phi(x)$ is c-concave iff $$ \frac{x^2}{2}-\phi(x) \quad \text{is convex.} $$ I was wondering whether there was some way to characterize the convexity/concavity of a c-concave function regarding the concavity/convexity of the cost. In particular I am specially interested in the case in which $c(x-y)$ is a concave function (and a distance indeed). Thanks in advance!","Given a cost function we say a function is c-concave when there exists some , such that If the cost function is the usual euclidean metric then we know a function is c-concave iff I was wondering whether there was some way to characterize the convexity/concavity of a c-concave function regarding the concavity/convexity of the cost. In particular I am specially interested in the case in which is a concave function (and a distance indeed). Thanks in advance!","c:\mathbb{R}^d\times \mathbb{R}^d\rightarrow \mathbb{R}^+ \phi:\mathbb{R}^d\rightarrow \mathbb{R} \psi:\mathbb{R}^d\rightarrow \mathbb{R}\cup\{-\infty\} \psi\not\equiv -\infty  \phi(x) = \inf_{y\in\mathbb{R}^d} [c(x,y)-\psi(y)].  c(x,y) = \frac{1}{2}|x-y|^2 \phi(x)  \frac{x^2}{2}-\phi(x) \quad \text{is convex.}  c(x-y)","['measure-theory', 'optimal-transport']"
48,"If $f_n$ is uniformly integrable, then $\lim_{t\to\infty}\int|f_n|I_{|f_n|>t} = 0$ for all $n$","If  is uniformly integrable, then  for all",f_n \lim_{t\to\infty}\int|f_n|I_{|f_n|>t} = 0 n,"Show that if $\{f_n\}_{n\geq 1}$ is a uniformly integrable family of functions, then $\lim_{t\to\infty}\int|f_n|I_{|f_n|>t} = 0$ for all $n$ . I feel like this should fall right out of the definition of uniform integrability, but it's not coming to me for some reason. The definition of uniform integrability I'm working with is; $\{f_n\}_{n\geq1}$ is uniformly integrable if for all $\epsilon>0$ there is a $\delta>0$ such that if $\mu(A)<\delta$ then $\int_A|f_n|<\epsilon$ , for all $f_n$ . It's not clear to me why I should be able to get $\mu\left(\{x:|f_n(x)|>t\} \right)<\delta$ for every $n$ . Any thoughts are appreciated. Thanks in advance.","Show that if is a uniformly integrable family of functions, then for all . I feel like this should fall right out of the definition of uniform integrability, but it's not coming to me for some reason. The definition of uniform integrability I'm working with is; is uniformly integrable if for all there is a such that if then , for all . It's not clear to me why I should be able to get for every . Any thoughts are appreciated. Thanks in advance.",\{f_n\}_{n\geq 1} \lim_{t\to\infty}\int|f_n|I_{|f_n|>t} = 0 n \{f_n\}_{n\geq1} \epsilon>0 \delta>0 \mu(A)<\delta \int_A|f_n|<\epsilon f_n \mu\left(\{x:|f_n(x)|>t\} \right)<\delta n,"['integration', 'measure-theory', 'uniform-integrability']"
49,"Proof Verification: Bartle's ""Elements of Integration"" - Exercise 4.S","Proof Verification: Bartle's ""Elements of Integration"" - Exercise 4.S",,"I had to solve this question for a group assignment, and my group is suspicious of my proof for this exercise. Can anyone help me confirm if this proof is correct? Bartle's Elements of Integration - Exercise 4.S: Let $f: X \to \mathbb{R}\cup\{-\infty, \infty\} = \overline{\mathbb{R}}$ be a non-negative measurable function, where $X$ is a measure space, such that $\int f d \mu < \infty$ . Then, for every $\varepsilon > 0$ , there exists a measurable set $E$ , with $\mu(E) < \infty$ , such that $\int f d \mu \leq \int_E f d \mu + \varepsilon$ . So here is my proof: Let $\varepsilon > 0$ . Then there is a simple function $\varphi$ such that $\int \varphi d \mu > \int f d \mu - \varepsilon$ and $\phi(x) \leq f(x)$ for each $x \in X$ (this follows from Bartle's definition of the integral). Let $\{a_1, \dots, a_n\}$ be the set of the distinct positive values $\varphi$ attains, and $E_i = \varphi^{-1}(a_i)$ . Then we can write $\varphi = \sum_{i=1}^{n}a_i \chi_{E_i}$ - where $\chi_A$ is the characteristic function of $A$ . For each $E_i$ , $\mu(E_i) < \infty$ , because $a_i \mu(E_i) \leq \int \phi d \mu \leq \int f d \mu < \infty $ , and $a_i \neq 0$ . Then $E = E_1 \cup \cdots \cup E_n$ is such that $\mu(E) < \infty$ and $\varphi \chi_E = \varphi$ . From this, we have that $\varphi(x) = \varphi(x) \chi_E(x) \leq f(x) \chi_E(x)$ . Thus: $$\int_Efd \mu = \int f \chi_E d \mu \geq \int \varphi d \mu > \int f d \mu - \varepsilon$$ And from this follows that $\int f d \mu < \int_E f d \mu + \varepsilon$ . Is this proof correct?","I had to solve this question for a group assignment, and my group is suspicious of my proof for this exercise. Can anyone help me confirm if this proof is correct? Bartle's Elements of Integration - Exercise 4.S: Let be a non-negative measurable function, where is a measure space, such that . Then, for every , there exists a measurable set , with , such that . So here is my proof: Let . Then there is a simple function such that and for each (this follows from Bartle's definition of the integral). Let be the set of the distinct positive values attains, and . Then we can write - where is the characteristic function of . For each , , because , and . Then is such that and . From this, we have that . Thus: And from this follows that . Is this proof correct?","f: X \to \mathbb{R}\cup\{-\infty, \infty\} = \overline{\mathbb{R}} X \int f d \mu < \infty \varepsilon > 0 E \mu(E) < \infty \int f d \mu \leq \int_E f d \mu + \varepsilon \varepsilon > 0 \varphi \int \varphi d \mu > \int f d \mu - \varepsilon \phi(x) \leq f(x) x \in X \{a_1, \dots, a_n\} \varphi E_i = \varphi^{-1}(a_i) \varphi = \sum_{i=1}^{n}a_i \chi_{E_i} \chi_A A E_i \mu(E_i) < \infty a_i \mu(E_i) \leq \int \phi d \mu \leq \int f d \mu < \infty  a_i \neq 0 E = E_1 \cup \cdots \cup E_n \mu(E) < \infty \varphi \chi_E = \varphi \varphi(x) = \varphi(x) \chi_E(x) \leq f(x) \chi_E(x) \int_Efd \mu = \int f \chi_E d \mu \geq \int \varphi d \mu > \int f d \mu - \varepsilon \int f d \mu < \int_E f d \mu + \varepsilon","['measure-theory', 'solution-verification', 'lebesgue-integral']"
50,Maximal Ergodic Theorem for flows?,Maximal Ergodic Theorem for flows?,,"This is Petersen 2.2.1. Let $\{T(t,x)\}_{t \in \mathbb{R}}$ be a family of one-parameter invertible measure preserving transformations on $(X, \mathcal{M}, \mu)$ a measure space. Let $f : X \rightarrow \mathbb{R}$ be an $L^1(\mu)$ function. The goal is to formulate and prove the Maximal Ergodic Theorem for flows. My idea is that this looks like the Hardy-Littlewood theorem, so try to just shove it into that. That is, let $$A_r(f)(x) = \frac{1}{|B(r,0)|} \int_{B(r,0)} f(T(s,x)) d\lambda(s),$$ and let $$f^*(x) = \sup_{r > 0} A_r(f)(x).$$ Then the formulation would be $$ \int_{\{f^* > 0\}} f d\mu \geq 0,$$ and the proof would be something like the Hardy-Littlewood theorem (except now you're going to have to use the invertible measure preserving transformation to move between $\mathbb{R}$ and $X$ so that you can use the Vitali covering lemma). Is this a reasonable formulation/idea? It also asks for the Pointwise Ergodic theorem, but if my hunch is correct for the above then I'm pretty sure this is going to be something like the Lebesgue differentiation theorem. Based on the discussion in Ergodic Theorem and flow I have a feeling this might not be the correct approach, but I am still curious.","This is Petersen 2.2.1. Let be a family of one-parameter invertible measure preserving transformations on a measure space. Let be an function. The goal is to formulate and prove the Maximal Ergodic Theorem for flows. My idea is that this looks like the Hardy-Littlewood theorem, so try to just shove it into that. That is, let and let Then the formulation would be and the proof would be something like the Hardy-Littlewood theorem (except now you're going to have to use the invertible measure preserving transformation to move between and so that you can use the Vitali covering lemma). Is this a reasonable formulation/idea? It also asks for the Pointwise Ergodic theorem, but if my hunch is correct for the above then I'm pretty sure this is going to be something like the Lebesgue differentiation theorem. Based on the discussion in Ergodic Theorem and flow I have a feeling this might not be the correct approach, but I am still curious.","\{T(t,x)\}_{t \in \mathbb{R}} (X, \mathcal{M}, \mu) f : X \rightarrow \mathbb{R} L^1(\mu) A_r(f)(x) = \frac{1}{|B(r,0)|} \int_{B(r,0)} f(T(s,x)) d\lambda(s), f^*(x) = \sup_{r > 0} A_r(f)(x).  \int_{\{f^* > 0\}} f d\mu \geq 0, \mathbb{R} X","['measure-theory', 'dynamical-systems', 'ergodic-theory']"
51,Integral over decreasing sequence of open sets,Integral over decreasing sequence of open sets,,"Let $G : T \subset \mathbb{R}^n \rightarrow \mathbb{R}^n$ , $T$ is an open set and suppose that $G$ is a $C^1$ -diffeomorphism in the sense that $G$ is injective and $\det DG(x) \neq 0$ for all $x \in T$ , where $DG(x)$ is the total derivative of $G$ at $x \in T$ . Let $B \subset T$ be a borel set of finite measure and suppose $U_n \subset T$ is a decreasing sequence of open sets, such that $B \subset U_n$ for all $n$ , and $m(U_n \setminus B) < \frac{1}{2^n}$ for all $n$ . In particular we have that $B \subset \cap_{n \geq 1} U_n = U$ and $m(U \setminus B) = 0$ where $m$ is the lebesgue measure. Why is it necessarily the case that dominated convergence theorem can be applied to show that $$\lim_{n \rightarrow \infty} \int_{U_n} |\det DG(x)|dx = \int_{B} |\det DG(x)|dx$$ ? This seems to imply to me that when $\int_{B} |\det DG(x)|dx < \infty$ , $\int_{U_N} |\det DG(x)| dx <\infty$ for some $N \in \mathbb{N}$ (and thus for all $n \geq N$ ) but I have been unable to see why this is true. Edit: For reference, this particular worry came from reading page 73 of Folland (Real Analysis : Modern Techniques and their Applications - 1984) and I have attached the section in that page which I did not understand the argument of : https://i.sstatic.net/a91Mt.jpg","Let , is an open set and suppose that is a -diffeomorphism in the sense that is injective and for all , where is the total derivative of at . Let be a borel set of finite measure and suppose is a decreasing sequence of open sets, such that for all , and for all . In particular we have that and where is the lebesgue measure. Why is it necessarily the case that dominated convergence theorem can be applied to show that ? This seems to imply to me that when , for some (and thus for all ) but I have been unable to see why this is true. Edit: For reference, this particular worry came from reading page 73 of Folland (Real Analysis : Modern Techniques and their Applications - 1984) and I have attached the section in that page which I did not understand the argument of : https://i.sstatic.net/a91Mt.jpg",G : T \subset \mathbb{R}^n \rightarrow \mathbb{R}^n T G C^1 G \det DG(x) \neq 0 x \in T DG(x) G x \in T B \subset T U_n \subset T B \subset U_n n m(U_n \setminus B) < \frac{1}{2^n} n B \subset \cap_{n \geq 1} U_n = U m(U \setminus B) = 0 m \lim_{n \rightarrow \infty} \int_{U_n} |\det DG(x)|dx = \int_{B} |\det DG(x)|dx \int_{B} |\det DG(x)|dx < \infty \int_{U_N} |\det DG(x)| dx <\infty N \in \mathbb{N} n \geq N,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure', 'determinant']"
52,"Do finitely additive measures solve the ""problem of measure""?","Do finitely additive measures solve the ""problem of measure""?",,"Does there exist a unique function $\mu$ satisfying the following properties? $\mu:\mathcal P(\mathbb R)\to [0,\infty]$ $\mu(A+x)=\mu(A) \qquad\qquad$ for all $A\in\mathcal P(\mathbb R),x\in\mathbb R$ $\mu([a,b])=b-a\qquad\qquad\,$ for all $a<b\in\mathbb R$ $\mu\big(⨃_{i=1}^N A_i\big)=\sum_{i=1}^N \mu(A_i)\;\,$ for all finite pairwise-disjoint families $(A_i)$ with each $A_i\in\mathcal P(\mathbb R)$ If (4) is strengthened to include countably infinite disjoint unions, such a function famously does not exist (which Terrence Tao has called the ""problem of measure,"" hence the question title). The standard solution is to replace every occurence of $\mathcal P(\mathbb R)$ above with $\mathcal B(\mathbb R)$ ; then Borel measure is the unique function on $\mathcal B(\mathbb R)$ that satisfies the above conditions (with 4 strengthened to countable additivity). I'm curious whether this alternative approach (using only finitely additive measures) would work instead.","Does there exist a unique function satisfying the following properties? for all for all for all finite pairwise-disjoint families with each If (4) is strengthened to include countably infinite disjoint unions, such a function famously does not exist (which Terrence Tao has called the ""problem of measure,"" hence the question title). The standard solution is to replace every occurence of above with ; then Borel measure is the unique function on that satisfies the above conditions (with 4 strengthened to countable additivity). I'm curious whether this alternative approach (using only finitely additive measures) would work instead.","\mu \mu:\mathcal P(\mathbb R)\to [0,\infty] \mu(A+x)=\mu(A) \qquad\qquad A\in\mathcal P(\mathbb R),x\in\mathbb R \mu([a,b])=b-a\qquad\qquad\, a<b\in\mathbb R \mu\big(⨃_{i=1}^N A_i\big)=\sum_{i=1}^N \mu(A_i)\;\, (A_i) A_i\in\mathcal P(\mathbb R) \mathcal P(\mathbb R) \mathcal B(\mathbb R) \mathcal B(\mathbb R)","['measure-theory', 'lebesgue-measure', 'borel-measures']"
53,Functions on a group. Functions of the inverse element.,Functions on a group. Functions of the inverse element.,,"Consider a Lie group $G$ and a Hilbert space $\mathbb{V}$ equipped with a dot product $~~\langle~,~\rangle$ . Let ${^G{\cal{L}}}$ be a space of square-integrable functions mapping $G$ to $\mathbb{V}$ : $$  {^G{\cal{L}}}=\left\{ f:~~ G\longrightarrow\mathbb{V}\qquad\Big{|}\quad  \int dg \langle f(g), f(g)\rangle < \infty     \right\}.  $$ We can map each $ f$ to a function of the inverse argument: $$  f(x)\longmapsto\varphi(x)\equiv f(x^{-1})~,\quad x\in G \qquad\qquad\qquad\qquad (*)  $$ or, in short: $$  f\longmapsto\varphi\equiv f\circ\hat{\zeta}~,  $$ where $$  \hat{\zeta} x\,=\,x^{-1}  $$ is the inversion operation on $G$ . Doing this for all $f$ , we obtain a new space of functions: $$  {\cal{L}}^{G}=\left\{~\varphi:~G\longrightarrow{\mathbb{V}}~~\Big{|}~~\varphi= f\circ\hat{\zeta}~,~~f\in{^G{\cal{L}}}  \right\}~. \qquad\qquad\qquad (**)  $$ To examine if $~^{G}{\cal{L}}$ and ${\cal{L}}^{G}$ are copies of the same functional space, check the square-integrability of the new functions: $$   \int dg \langle \varphi(g),\varphi(g)\rangle=\int d(g^{-1}) \langle \varphi(g^{-1}) , \varphi(g^{-1})\rangle=   \int d(g^{-1}) \langle f(g),f(g)\rangle  $$ For unimodular groups, the measure is invariant and $d(g^{-1}) = dg $ , wherefrom $$   \int dg \langle \varphi(g),\varphi(g)\rangle=   \int dg \langle f(g),f(g)\rangle < \infty~,  $$ so the spaces $^{G}{\cal{L}}$ and ${\cal{L}}^{G}$ coincide. For what nonunimodular groups would this outcome stay valid? Say, for compact ones? For locally compact ones? PS. I understand that, by Haar's theorem, if a group is locally compact (i.e. if its identity element has a compact neighborhood), then it admits a unique left-invariant and a unique right-invariant measure (unique -- up to multiplication by a positive constant). Under the transformation $g\to g^{-1}$ , a left-invariant measure transforms into a right-invariant one, and vice versa. Can this fact be somehow employed here?","Consider a Lie group and a Hilbert space equipped with a dot product . Let be a space of square-integrable functions mapping to : We can map each to a function of the inverse argument: or, in short: where is the inversion operation on . Doing this for all , we obtain a new space of functions: To examine if and are copies of the same functional space, check the square-integrability of the new functions: For unimodular groups, the measure is invariant and , wherefrom so the spaces and coincide. For what nonunimodular groups would this outcome stay valid? Say, for compact ones? For locally compact ones? PS. I understand that, by Haar's theorem, if a group is locally compact (i.e. if its identity element has a compact neighborhood), then it admits a unique left-invariant and a unique right-invariant measure (unique -- up to multiplication by a positive constant). Under the transformation , a left-invariant measure transforms into a right-invariant one, and vice versa. Can this fact be somehow employed here?","G \mathbb{V} ~~\langle~,~\rangle {^G{\cal{L}}} G \mathbb{V} 
 {^G{\cal{L}}}=\left\{ f:~~ G\longrightarrow\mathbb{V}\qquad\Big{|}\quad  \int dg \langle f(g), f(g)\rangle < \infty     \right\}.
   f 
 f(x)\longmapsto\varphi(x)\equiv f(x^{-1})~,\quad x\in G \qquad\qquad\qquad\qquad (*)
  
 f\longmapsto\varphi\equiv f\circ\hat{\zeta}~,
  
 \hat{\zeta} x\,=\,x^{-1}
  G f 
 {\cal{L}}^{G}=\left\{~\varphi:~G\longrightarrow{\mathbb{V}}~~\Big{|}~~\varphi= f\circ\hat{\zeta}~,~~f\in{^G{\cal{L}}}
 \right\}~. \qquad\qquad\qquad (**)
  ~^{G}{\cal{L}} {\cal{L}}^{G} 
  \int dg \langle \varphi(g),\varphi(g)\rangle=\int d(g^{-1}) \langle \varphi(g^{-1}) , \varphi(g^{-1})\rangle=
  \int d(g^{-1}) \langle f(g),f(g)\rangle
  d(g^{-1}) = dg  
  \int dg \langle \varphi(g),\varphi(g)\rangle=
  \int dg \langle f(g),f(g)\rangle < \infty~,
  ^{G}{\cal{L}} {\cal{L}}^{G} g\to g^{-1}","['group-theory', 'measure-theory', 'lie-groups']"
54,A counterexample to Junghenn's Principles of Analysis $4.29$(d),A counterexample to Junghenn's Principles of Analysis (d),4.29,"Problem: The following is Exercise $4.29$ (d) in Hugo Junghenn's Principles of Analysis: Use Jensen's inequality to verify the following for a probability measure $\mu:$ $$\|f\|_1\log(\|f\|_1)\leq\log(\|f\log(f)\|_1\quad\text{where }f>0.$$ I tried the problem for a while and now I think that the claim is incorrect. I have cooked up the following counterexample. Consider the probability space $((0,1),\mathcal B,\mu)$ , where $\mathcal B$ is the Borel $\sigma$ -field of $(0,1)$ and $\mu$ is the Lebesgue measure. Next, let $f(x)=x^2$ for $x\in(0,1)$ . Then $f>0$ everywhere and we have $$\|f\|_1\log(\|f\|_1)=\frac{1}{3}\log\left(\frac{1}{3}\right).$$ On the other hand, using integration by parts and the fact that $x^3\log(x)\to0$ as $x\searrow0$ , we see that $$\|f\log(f)\|_1=\int_0^1 x^2|\log(x^2)|\,dx=2\int_0^1x^2|\log(x)|\,dx=\frac{2}{9}.$$ But then $\log(2/9)<3^{-1}\log(3^{-1})$ , hence the inequality does not hold. However, I do believe that the author meant to ask to prove $$\|f\|_1\log(\|f\|_1)\leq\|f\log(f)\|_1,$$ which is an easy consequence of Jensen's inequality taking the convex function to be $x\log(x).$ My Question: Do you agree with my counterexample above? If not, I would like to ask if I am wrong and the inequality does hold, or if there is another, this time correct, counterexample. Thank you very much for your time and appreciate all the feedback and help.","Problem: The following is Exercise (d) in Hugo Junghenn's Principles of Analysis: Use Jensen's inequality to verify the following for a probability measure I tried the problem for a while and now I think that the claim is incorrect. I have cooked up the following counterexample. Consider the probability space , where is the Borel -field of and is the Lebesgue measure. Next, let for . Then everywhere and we have On the other hand, using integration by parts and the fact that as , we see that But then , hence the inequality does not hold. However, I do believe that the author meant to ask to prove which is an easy consequence of Jensen's inequality taking the convex function to be My Question: Do you agree with my counterexample above? If not, I would like to ask if I am wrong and the inequality does hold, or if there is another, this time correct, counterexample. Thank you very much for your time and appreciate all the feedback and help.","4.29 \mu: \|f\|_1\log(\|f\|_1)\leq\log(\|f\log(f)\|_1\quad\text{where }f>0. ((0,1),\mathcal B,\mu) \mathcal B \sigma (0,1) \mu f(x)=x^2 x\in(0,1) f>0 \|f\|_1\log(\|f\|_1)=\frac{1}{3}\log\left(\frac{1}{3}\right). x^3\log(x)\to0 x\searrow0 \|f\log(f)\|_1=\int_0^1 x^2|\log(x^2)|\,dx=2\int_0^1x^2|\log(x)|\,dx=\frac{2}{9}. \log(2/9)<3^{-1}\log(3^{-1}) \|f\|_1\log(\|f\|_1)\leq\|f\log(f)\|_1, x\log(x).","['real-analysis', 'integration', 'measure-theory', 'solution-verification', 'examples-counterexamples']"
55,Integrability of a periodic function based on $\int_0^1 |f(a+t)-f(b+t)| dt$,Integrability of a periodic function based on,\int_0^1 |f(a+t)-f(b+t)| dt,"Proposition: Let $f$ be a measurable function with period $1$ on the real line such $\int_0^1 |f(a+t)-f(b+t)| dt$ is bounded uniformly for all $a, b \in \mathbb{R}$ . Show that $f$ is integrable on $[0, 1]$ . [Hint: Use $a = x$ , $b = −x$ , integrate with respect to $x$ , and change variables to $ξ=x+t$ , $η=−x+t$ .] First of all what does it is bounded uniformly for all $a, b \in \mathbb{R}$ ? Does it mean that for all $a, b \in \mathbb{R}$ , $\int_0^1 |f(a+t)-f(b+t)| dt \le M$ for a single $M$ ? And how does it help to solve the exercise? How the hint is useful when nothing cancels out to reach $\int_0^1 |f(t)| dt$ with the use of the hint?","Proposition: Let be a measurable function with period on the real line such is bounded uniformly for all . Show that is integrable on . [Hint: Use , , integrate with respect to , and change variables to , .] First of all what does it is bounded uniformly for all ? Does it mean that for all , for a single ? And how does it help to solve the exercise? How the hint is useful when nothing cancels out to reach with the use of the hint?","f 1 \int_0^1 |f(a+t)-f(b+t)| dt a, b \in \mathbb{R} f [0, 1] a = x b = −x x ξ=x+t η=−x+t a, b \in \mathbb{R} a, b \in \mathbb{R} \int_0^1 |f(a+t)-f(b+t)| dt \le M M \int_0^1 |f(t)| dt",['real-analysis']
56,Does a spectral theorem exist for linear operator pencils?,Does a spectral theorem exist for linear operator pencils?,,"I was wondering if a version of the spectral theorem (the projection valued measure case) holds for linear pencils of the form $$ A-\lambda B $$ where $A,B$ are self-adjoint on some Hilbert space $\mathcal{H}$ (and possibly unbounded) but $B$ is strictly positive? If so, is there a good reference on this? Note that under ""nice"" conditions, $B^{-1}A$ is self-adjoint on a weighted inner product (put factors $B^{1/2}$ in the inner product of $\mathcal{H}$ - this is straightforward to prove rigorously), so it may be the case that $B^{-1}A$ is usually studied instead.","I was wondering if a version of the spectral theorem (the projection valued measure case) holds for linear pencils of the form where are self-adjoint on some Hilbert space (and possibly unbounded) but is strictly positive? If so, is there a good reference on this? Note that under ""nice"" conditions, is self-adjoint on a weighted inner product (put factors in the inner product of - this is straightforward to prove rigorously), so it may be the case that is usually studied instead.","
A-\lambda B
 A,B \mathcal{H} B B^{-1}A B^{1/2} \mathcal{H} B^{-1}A","['complex-analysis', 'functional-analysis', 'measure-theory', 'hilbert-spaces', 'spectral-theory']"
57,A real analysis qualifying exam problem,A real analysis qualifying exam problem,,I was doing a real analysis problem set when I get stuck on this problem. $1<p<\infty$ $f\in L^p(\mathbb{R})$ $\alpha>1-\frac1p$ show that $$\sum_{n=1}^\infty \int_{n}^{n+n^{-\alpha}}|f(x+y)|dy<\infty$$ for a.e. $x\in\mathbb{R}$ . I have tried integrating the series w.r.t. $x$ on a bounded interval and try to show that it is finite using Holder inequality. But it doesn't give the things I want. Any hint or suggestion on it?,I was doing a real analysis problem set when I get stuck on this problem. show that for a.e. . I have tried integrating the series w.r.t. on a bounded interval and try to show that it is finite using Holder inequality. But it doesn't give the things I want. Any hint or suggestion on it?,1<p<\infty f\in L^p(\mathbb{R}) \alpha>1-\frac1p \sum_{n=1}^\infty \int_{n}^{n+n^{-\alpha}}|f(x+y)|dy<\infty x\in\mathbb{R} x,"['real-analysis', 'measure-theory']"
58,Radon Nikodym property (Banach valued) is separably determined $\sigma-$finite case,Radon Nikodym property (Banach valued) is separably determined finite case,\sigma-,"Im reading Analysis in Banach spaces ( https://www.springer.com/gp/book/9783319485195 ) and I was trying to provide a different demostration of lemma 1.3.17, which is used in order to prove that the Radon Nikodym property is separably determined (theorem 1.3.18). The lemma says: Suppose that X has the RNP with respect to a $\sigma-$ finite measure space $(S,\mathcal{A},\mu)$ , and let $\mathcal{B}$ be a sub $-\sigma-$ algebra of $\mathcal{A}$ . Then X has the RNP with respect to $(S,\mathcal{B},\mu_{\restriction{\mathcal{B}}})$ . They prove it using results of other section, so I would like to prove it in a self-contained form. Modifying a finite version from these lectures notes ( http://wwwarchive.math.psu.edu/huff/ , chapter 7, lemma 7.5.1) it's easy to prove it if the sub $-\sigma-$ algebra is $\sigma-$ finite: Lemma: Let $(\Omega,\Sigma,\mu)$ be a finite measure space. For every sub $-\sigma-$ algebra $\Sigma_0$ of $\Sigma$ there exists a unique function $E_0: \mathcal{L}^{1}(\mu,X)\longmapsto \mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}},X)$ such that $$\displaystyle\int_{A}f d\mu = \displaystyle\int_{A} (E_0 f) d\mu$$ for every $A\in\Sigma_0$ and every $f\in \mathcal{L}^{1}(\mu,X).$ In addition, $E_0$ it's a  bounded and linear operator of $||E_0||\leq1.$ Unicity follows from previous result. Let $A\in\Sigma$ be fixed, then the map $E \longmapsto \mu(A\cap E)$ with $E\in\Sigma_0,$ defines a finite positive measure $\Sigma_0$ which is absolute continuous with respect $\mu_{\restriction{\Sigma_0}}.$ By Radon-Nikodym theorem there exists a $g_A \in \mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}})$ such that $$\mu(A\cap E)=\displaystyle\int_{E}g_A d\mu \hspace{0.2cm} \text{ for all } E\in\Sigma_0.$$ For a simple function $f=\displaystyle\sum_{j=1}^{n}\alpha_j \chi_{A_j},$ we define $$E_0(f)=\displaystyle\sum_{j=1}^{n}\alpha_j g_{A_j}.$$ It's easy to see that $E_0$ it's linear and continuous over the simple functions with $||E_0||\leq1$ and that $\displaystyle\int_{E}f d\mu = \displaystyle\int_{E} (E_0 f) d\mu$ for all $E\in\Sigma_0.$ Therefore it admits a unique externsion to a bounded and linear operator $E_0:\mathcal{L}^{1}(\mu,X) \longmapsto\mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}},X)$ with $||E_0||\leq1.$ If $f_n$ it's a sequence of simple functions such that $||f_n-f||_1 \xrightarrow[n\to\infty]{}0$ for $f\in \mathcal{L}^{1}(\mu,X),$ thens $||E_0f_n-E_0f||_1 \xrightarrow[n\to\infty]{}0$ and for all $E\in\Sigma_0$ $$\displaystyle\int_{E}(E_0 f)d\mu = \displaystyle\lim_{n\to\infty}\displaystyle\int_{E}(E_0 f_n)d\mu=\displaystyle\lim_{n\to\infty}\displaystyle\int_{E}f_n d\mu=\displaystyle\int_{E}f d\mu.$$ Also I know that if $(\Omega,\Sigma,\mu)$ is a $\sigma-$ finite measure space and $\mathcal{B}$ is a sub $-\sigma-$ algebra, we can find a disjoint decomposition $\Omega=S_0 \cup S_1$ such that the restriction of $\mu$ to $\mathcal{B}_{\restriction{S_0}}$ is $\sigma-$ finite and the restriction of $\mu$ to $\mathcal{B}_{\restriction{S_1}}$ is purely infinite. I doubt that it's possible, because the authors would have done it, but I have to prove theorem 1.3.18, and this lemma is used, so I need to prove it. Other solution would be see the way it's used in the theorem and see if I need to prove something easier only; in fact i think we can assume the sub $-\sigma-$ algebra is countably generated, but cant see how that helps. Anyway, any comments would help. Thanks a lot in advance.","Im reading Analysis in Banach spaces ( https://www.springer.com/gp/book/9783319485195 ) and I was trying to provide a different demostration of lemma 1.3.17, which is used in order to prove that the Radon Nikodym property is separably determined (theorem 1.3.18). The lemma says: Suppose that X has the RNP with respect to a finite measure space , and let be a sub algebra of . Then X has the RNP with respect to . They prove it using results of other section, so I would like to prove it in a self-contained form. Modifying a finite version from these lectures notes ( http://wwwarchive.math.psu.edu/huff/ , chapter 7, lemma 7.5.1) it's easy to prove it if the sub algebra is finite: Lemma: Let be a finite measure space. For every sub algebra of there exists a unique function such that for every and every In addition, it's a  bounded and linear operator of Unicity follows from previous result. Let be fixed, then the map with defines a finite positive measure which is absolute continuous with respect By Radon-Nikodym theorem there exists a such that For a simple function we define It's easy to see that it's linear and continuous over the simple functions with and that for all Therefore it admits a unique externsion to a bounded and linear operator with If it's a sequence of simple functions such that for thens and for all Also I know that if is a finite measure space and is a sub algebra, we can find a disjoint decomposition such that the restriction of to is finite and the restriction of to is purely infinite. I doubt that it's possible, because the authors would have done it, but I have to prove theorem 1.3.18, and this lemma is used, so I need to prove it. Other solution would be see the way it's used in the theorem and see if I need to prove something easier only; in fact i think we can assume the sub algebra is countably generated, but cant see how that helps. Anyway, any comments would help. Thanks a lot in advance.","\sigma- (S,\mathcal{A},\mu) \mathcal{B} -\sigma- \mathcal{A} (S,\mathcal{B},\mu_{\restriction{\mathcal{B}}}) -\sigma- \sigma- (\Omega,\Sigma,\mu) -\sigma- \Sigma_0 \Sigma E_0: \mathcal{L}^{1}(\mu,X)\longmapsto \mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}},X) \displaystyle\int_{A}f d\mu = \displaystyle\int_{A} (E_0 f) d\mu A\in\Sigma_0 f\in \mathcal{L}^{1}(\mu,X). E_0 ||E_0||\leq1. A\in\Sigma E \longmapsto \mu(A\cap E) E\in\Sigma_0, \Sigma_0 \mu_{\restriction{\Sigma_0}}. g_A \in \mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}}) \mu(A\cap E)=\displaystyle\int_{E}g_A d\mu \hspace{0.2cm} \text{ for all } E\in\Sigma_0. f=\displaystyle\sum_{j=1}^{n}\alpha_j \chi_{A_j}, E_0(f)=\displaystyle\sum_{j=1}^{n}\alpha_j g_{A_j}. E_0 ||E_0||\leq1 \displaystyle\int_{E}f d\mu = \displaystyle\int_{E} (E_0 f) d\mu E\in\Sigma_0. E_0:\mathcal{L}^{1}(\mu,X) \longmapsto\mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}},X) ||E_0||\leq1. f_n ||f_n-f||_1 \xrightarrow[n\to\infty]{}0 f\in \mathcal{L}^{1}(\mu,X), ||E_0f_n-E_0f||_1 \xrightarrow[n\to\infty]{}0 E\in\Sigma_0 \displaystyle\int_{E}(E_0 f)d\mu = \displaystyle\lim_{n\to\infty}\displaystyle\int_{E}(E_0 f_n)d\mu=\displaystyle\lim_{n\to\infty}\displaystyle\int_{E}f_n d\mu=\displaystyle\int_{E}f d\mu. (\Omega,\Sigma,\mu) \sigma- \mathcal{B} -\sigma- \Omega=S_0 \cup S_1 \mu \mathcal{B}_{\restriction{S_0}} \sigma- \mu \mathcal{B}_{\restriction{S_1}} -\sigma-","['real-analysis', 'integration', 'functional-analysis', 'measure-theory', 'banach-spaces']"
59,What is the 'limit' of a delta prime boundary condition?,What is the 'limit' of a delta prime boundary condition?,,"I recently came across the concept of delta prime boundary conditions, which can be imposed on a function $\psi$ at the origin in one dimensional space (say, in the context of partial differential equations): $\frac{\partial\psi}{\partial x}$ is 'continuous' at $0$ , in the sense that $\frac{\partial\psi}{\partial x}|_{0^+}=\frac{\partial\psi}{\partial x}|_{0^-}=:\psi'(0)$ , meaning that both one sided limits exist and are equal (although $\psi$ doesn't have to be differentiable in the usual sense). The double sided limits of $\psi (0)$ exist and we have - $$\psi(0^+)-\psi(0^-)=-\sigma \psi'(0)$$ Where $\sigma\in\mathbb R$ . Note that $\psi$ doesn't have to be continuous at $0$ (again - it need not be differentiable - only have one sided derivatives). This gives me a certain boundary condition for any choice of $\sigma$ . My question is - what happens to the condition as $\sigma\rightarrow \infty$ ? Is there a sense in which this limit can exist? What would be the corresponding boundary condition? It seems to me that for me to take this limit (and if I want a bounded solution), I must have in the corresponding boundary condition that $\psi'(0)=0$ . But what I'm interested in is - does $\psi$ now have to be continuous at $0$ ? We now have something of the form: $$\psi(0^+)-\psi(0^-)=""-\infty\cdot 0""$$ Naively, we can't estimate the RHS and claim if $\psi$ is continuous at $0$ . But maybe anyone here knows of a 'proper' way to take this limit (via distribution theory or something) so that we can know if maybe $\psi$ also needs to be continuous at $0$ for some reason? This question is (partially on purpose) a bit vague and not too well defined - I have not stated which space of functions $\psi$ belongs to, etc. For me, this is part of the question - what properties would you require $\psi$ to fulfill for this question to 'make sense'? In what context can we say anything meaningful about this limit? There's probably more than one 'correct' answer to this - I'm interesting in anything you can suggest. I'd be happy to hear all kinds of answers - intuitions, formal proofs (this is probably best), and even references to papers/books which do something similar to what I described. Keep in mind that I'm pretty new to this world of content, so ideally the more detailed the explanation - the better. Thanks in advance!","I recently came across the concept of delta prime boundary conditions, which can be imposed on a function at the origin in one dimensional space (say, in the context of partial differential equations): is 'continuous' at , in the sense that , meaning that both one sided limits exist and are equal (although doesn't have to be differentiable in the usual sense). The double sided limits of exist and we have - Where . Note that doesn't have to be continuous at (again - it need not be differentiable - only have one sided derivatives). This gives me a certain boundary condition for any choice of . My question is - what happens to the condition as ? Is there a sense in which this limit can exist? What would be the corresponding boundary condition? It seems to me that for me to take this limit (and if I want a bounded solution), I must have in the corresponding boundary condition that . But what I'm interested in is - does now have to be continuous at ? We now have something of the form: Naively, we can't estimate the RHS and claim if is continuous at . But maybe anyone here knows of a 'proper' way to take this limit (via distribution theory or something) so that we can know if maybe also needs to be continuous at for some reason? This question is (partially on purpose) a bit vague and not too well defined - I have not stated which space of functions belongs to, etc. For me, this is part of the question - what properties would you require to fulfill for this question to 'make sense'? In what context can we say anything meaningful about this limit? There's probably more than one 'correct' answer to this - I'm interesting in anything you can suggest. I'd be happy to hear all kinds of answers - intuitions, formal proofs (this is probably best), and even references to papers/books which do something similar to what I described. Keep in mind that I'm pretty new to this world of content, so ideally the more detailed the explanation - the better. Thanks in advance!","\psi \frac{\partial\psi}{\partial x} 0 \frac{\partial\psi}{\partial x}|_{0^+}=\frac{\partial\psi}{\partial x}|_{0^-}=:\psi'(0) \psi \psi (0) \psi(0^+)-\psi(0^-)=-\sigma \psi'(0) \sigma\in\mathbb R \psi 0 \sigma \sigma\rightarrow \infty \psi'(0)=0 \psi 0 \psi(0^+)-\psi(0^-)=""-\infty\cdot 0"" \psi 0 \psi 0 \psi \psi","['measure-theory', 'partial-differential-equations', 'mathematical-physics', 'distribution-theory', 'boundary-value-problem']"
60,Outer Measure of Cartesian Product with Interval,Outer Measure of Cartesian Product with Interval,,"(Apologises in advance if this has already been asked, but I looked around and couldn’t find anything that answered my question). Let $\lambda_m^*$ denote the Lebesgue outer measure on $\mathbb{R}^m$ , and $[a,b]$ be an interval of $\mathbb{R}$ . If $A$ is a (not necessarily Lebesgue measurable) subset of $\mathbb{R}^n$ , is it possible to say that: $\lambda_{n+1}^*(A \times [a,b]) = \lambda_n^*(A) (b - a)$ ? It is pretty straight forward to see that the left hand side is less than or equal to the right hand side (that’s true for arbitrary Cartesian products), and that equality holds if $A$ is Lebesgue measurable. But what about the general case? I’m not sure what the best way to find either a proof or a counterexample is, so some help would be much appreciated.","(Apologises in advance if this has already been asked, but I looked around and couldn’t find anything that answered my question). Let denote the Lebesgue outer measure on , and be an interval of . If is a (not necessarily Lebesgue measurable) subset of , is it possible to say that: ? It is pretty straight forward to see that the left hand side is less than or equal to the right hand side (that’s true for arbitrary Cartesian products), and that equality holds if is Lebesgue measurable. But what about the general case? I’m not sure what the best way to find either a proof or a counterexample is, so some help would be much appreciated.","\lambda_m^* \mathbb{R}^m [a,b] \mathbb{R} A \mathbb{R}^n \lambda_{n+1}^*(A \times [a,b]) = \lambda_n^*(A) (b - a) A","['measure-theory', 'lebesgue-measure', 'products', 'outer-measure']"
61,Relationship between Product of $L^p$ spaces an product measures,Relationship between Product of  spaces an product measures,L^p,"Let $(X,\Sigma,\mu)$ and $(Y,\Gamma,\nu)$ be $\sigma$ -finite measure spaces.  Is there a relationship between $L^p_{\mu}(\Sigma)\times L^p_{\nu}(\Gamma)$ , equipped with the product of their $L^p$ -topologies, and the space $L^p_{\mu\otimes \nu}(\Sigma \otimes \Gamma)$ ? More generally, let $(X_i,\Sigma_i,\mu_i)$ be a countably infinite familly of $\sigma$ -finite measure spaces, then is $$ \prod_{i=1}^{\infty} L_{\mu}^p(\Sigma_i) \mbox{ dense in } L^p_{\bigotimes_{i=1}^{\infty} \mu_i}\left(\bigotimes_{i=1}^{\infty}\Sigma_i\right)? $$ Ideas and Update: Initially, my intuition was (as noted in the comment below) to pass through the completed injective tensor product $L_{\mu}^p(\Sigma)\otimes_{\epsilon} L^p_{\nu}(\Gamma)$ . Though now I doubt that works since according to this post and some results of Grothendieck, if $p=1$ then $L_{\mu}^p(\Sigma)\otimes_{\epsilon} L^p_{\nu}(\Gamma)\cong L_{\mu}^p(X;L_{\nu}^p(\Gamma))$ (the Pettis-Gel'fand integrable functions).","Let and be -finite measure spaces.  Is there a relationship between , equipped with the product of their -topologies, and the space ? More generally, let be a countably infinite familly of -finite measure spaces, then is Ideas and Update: Initially, my intuition was (as noted in the comment below) to pass through the completed injective tensor product . Though now I doubt that works since according to this post and some results of Grothendieck, if then (the Pettis-Gel'fand integrable functions).","(X,\Sigma,\mu) (Y,\Gamma,\nu) \sigma L^p_{\mu}(\Sigma)\times L^p_{\nu}(\Gamma) L^p L^p_{\mu\otimes \nu}(\Sigma \otimes \Gamma) (X_i,\Sigma_i,\mu_i) \sigma 
\prod_{i=1}^{\infty} L_{\mu}^p(\Sigma_i) \mbox{ dense in } L^p_{\bigotimes_{i=1}^{\infty} \mu_i}\left(\bigotimes_{i=1}^{\infty}\Sigma_i\right)?
 L_{\mu}^p(\Sigma)\otimes_{\epsilon} L^p_{\nu}(\Gamma) p=1 L_{\mu}^p(\Sigma)\otimes_{\epsilon} L^p_{\nu}(\Gamma)\cong L_{\mu}^p(X;L_{\nu}^p(\Gamma))","['integration', 'general-topology', 'functional-analysis', 'measure-theory', 'fubini-tonelli-theorems']"
62,Can we claim that $\int\limits_{X} f(x)d\mu=\int\limits_{X_1} f(x)d\mu$?,Can we claim that ?,\int\limits_{X} f(x)d\mu=\int\limits_{X_1} f(x)d\mu,"Let $(X,\mathfrak{M},\mu)$ be a measure space. Let $X_1\in\mathfrak{M}$ with $\mu(X\setminus X_1)=0$ and $f:X_1\to [0,+\infty]$ be a measurable function.  Can I conclude that $$\int\limits_{X} f(x)d\mu=\int\limits_{X_1} f(x)d\mu$$ The integral on the RHS makes sense but on the LHS does not because function $f(x)$ is defined on $X_1\subset X$ . Even $\mu(X\setminus X_1)=0$ I don't think that this notation is rigorous. Would be grateful to read your comments, please!","Let be a measure space. Let with and be a measurable function.  Can I conclude that The integral on the RHS makes sense but on the LHS does not because function is defined on . Even I don't think that this notation is rigorous. Would be grateful to read your comments, please!","(X,\mathfrak{M},\mu) X_1\in\mathfrak{M} \mu(X\setminus X_1)=0 f:X_1\to [0,+\infty] \int\limits_{X} f(x)d\mu=\int\limits_{X_1} f(x)d\mu f(x) X_1\subset X \mu(X\setminus X_1)=0","['real-analysis', 'measure-theory', 'lebesgue-integral']"
63,Are partitions of unity needed in manifolds to define a measure coinciding with the Lebesgue measure under coordinate systems?,Are partitions of unity needed in manifolds to define a measure coinciding with the Lebesgue measure under coordinate systems?,,"Consider a Hausdorff smooth manifold $M^n$ . In the book Riemannian Geometry by Gallot, Hulin and Lafontaine, proposition 6.1.4 reads as follows: If $M$ is compact (or locally compact and countable at infinity), there always exists a density. (...) In such context, I understand that the idea for defining a density would be: let $\{\rho_i\}$ be a partition of unity strictly subordinate to a smooth atlas $\{(U_i,\phi_i)\}$ . Given a local chart $(U_i,\phi_i)$ and a Lebesgue measurable $E \subset \phi_i(U_i)$ , define $\mu_i(E)=\sum_j \int_{\phi_i(U_i \cap U_j)} \rho_j \chi_E dm=m(E)$ , where m is the Lebesgue measure on $\mathbb{R}^n$ . I understand that such definition allows us to prove the second axiom of densities. (i.e., the Change of Variables Theorem holds for continuous functions supported on the intersection of local charts) On the other hand, it does not seem clear to me whether the definition $\mu_i(E)=m(E)$ would likewise define a density for a non-paracompact $M$ . In other words, my question is: could we use such definition and obtain a density in a non-paracompact manifold? Edit: the problem was that I guessed the idea for defining the density wrongly! The correct definition for a measurable $E \subset \phi_i(U_i)$ would be $\mu_i(E)=\sum_j \int_{\phi_j(U_j)} \chi_{\phi_j \circ \phi_i^{-1}(E)} (\rho_j \circ \phi_j^{-1}) dm \neq m(E)$ , which is reasonable, as it takes into account how the volume is deformed through the diffeomorphisms between charts.","Consider a Hausdorff smooth manifold . In the book Riemannian Geometry by Gallot, Hulin and Lafontaine, proposition 6.1.4 reads as follows: If is compact (or locally compact and countable at infinity), there always exists a density. (...) In such context, I understand that the idea for defining a density would be: let be a partition of unity strictly subordinate to a smooth atlas . Given a local chart and a Lebesgue measurable , define , where m is the Lebesgue measure on . I understand that such definition allows us to prove the second axiom of densities. (i.e., the Change of Variables Theorem holds for continuous functions supported on the intersection of local charts) On the other hand, it does not seem clear to me whether the definition would likewise define a density for a non-paracompact . In other words, my question is: could we use such definition and obtain a density in a non-paracompact manifold? Edit: the problem was that I guessed the idea for defining the density wrongly! The correct definition for a measurable would be , which is reasonable, as it takes into account how the volume is deformed through the diffeomorphisms between charts.","M^n M \{\rho_i\} \{(U_i,\phi_i)\} (U_i,\phi_i) E \subset \phi_i(U_i) \mu_i(E)=\sum_j \int_{\phi_i(U_i \cap U_j)} \rho_j \chi_E dm=m(E) \mathbb{R}^n \mu_i(E)=m(E) M E \subset \phi_i(U_i) \mu_i(E)=\sum_j \int_{\phi_j(U_j)} \chi_{\phi_j \circ \phi_i^{-1}(E)} (\rho_j \circ \phi_j^{-1}) dm \neq m(E)","['measure-theory', 'smooth-manifolds']"
64,Radon-Nikodym Derivative: Proposition 3.9 Folland,Radon-Nikodym Derivative: Proposition 3.9 Folland,,"Suppose that $\nu$ is a $\sigma$ -finite signed measure and $\mu, \lambda$ are $\sigma$ -finite positive measures on $(X, \mathcal{M})$ such that $\nu \ll \mu$ and $\mu \ll \lambda$ . Then, the following hold: (a) If $g \in L^{1}(\nu),$ then $g(d \nu / d \mu) \in L^{1}(\mu)$ (the function $g$ multiplied by $d \nu / d \mu$ ) and $$ \int g d \nu=\int g \frac{d \nu}{d \mu} d \mu $$ (b) We have $\nu \ll \lambda$ (this is obvious from $\nu \ll \mu$ and $\mu \ll \lambda$ ), and $$ \frac{d \nu}{d \lambda}=\frac{d \nu}{d \mu} \frac{d \mu}{d \lambda} \text{ holds for } \lambda \text {-a.e. } $$ I have no qualms with the proof of (a). Here is the proof for (b), and I am having difficulty understanding the part in bold: For any measurable set $E$ , note that by definition we have $$ \nu(E) = \int_E \left(\frac{d\nu}{d\lambda}\right)d\lambda. $$ Use (a) by replacing measures $\nu,\mu$ by the measures $\mu, \lambda$ , and we obtain that for $g \in L^1(\mu)$ , $$\int g d\mu = \int g \left(\frac{d\mu}{d\lambda}\right)d\lambda.$$ $\textbf{Letting $g = \chi_E \left(\frac{d\nu}{d\mu}\right)$, we obtain that (how do we show $g \in L^1(\mu)$? This is needed for us to use (a).)}$ $$\int \chi_E \left(\frac{d\nu}{d\mu}\right) d\mu = \int \chi_E \left(\frac{d\nu}{d\mu}\right) \left(\frac{d\mu}{d\lambda}\right)d\lambda.$$ We also have by definition: $$ \nu(E) = \int_E \left(\frac{d\nu}{d\mu}\right)d\mu := \int \chi_E \left(\frac{d\nu}{d\mu}\right)d\mu. $$ Thus, we conclude: $$\nu(E) = \int \chi_E \left(\frac{d\nu}{d\mu}\right) d\mu = \int \chi_E \left(\frac{d\nu}{d\mu}\right) \left(\frac{d\mu}{d\lambda}\right)d\lambda.$$ Note that we have shown $$ \nu(E) = \int_E \left(\frac{d\nu}{d\lambda}\right)d\lambda = \int \chi_E \left(\frac{d\nu}{d\mu}\right) \left(\frac{d\mu}{d\lambda}\right)d\lambda. $$ We can now apply proposition 1 to give the desired conclusion. (This is irrelevant to the question, but) Proposition 1 states: (a) If $f \in L^1$ , then $\{ x:f(x) \neq 0\}$ is $\sigma$ -finite. (b) If $f,g \in L^1$ , then $\int_E f = \int_E g$ for all $E \in \mathcal{M}$ $\iff$ $\int ||f-g|| = 0$ $\iff$ $f = g$ a.e..","Suppose that is a -finite signed measure and are -finite positive measures on such that and . Then, the following hold: (a) If then (the function multiplied by ) and (b) We have (this is obvious from and ), and I have no qualms with the proof of (a). Here is the proof for (b), and I am having difficulty understanding the part in bold: For any measurable set , note that by definition we have Use (a) by replacing measures by the measures , and we obtain that for , We also have by definition: Thus, we conclude: Note that we have shown We can now apply proposition 1 to give the desired conclusion. (This is irrelevant to the question, but) Proposition 1 states: (a) If , then is -finite. (b) If , then for all a.e..","\nu \sigma \mu, \lambda \sigma (X, \mathcal{M}) \nu \ll \mu \mu \ll \lambda g \in L^{1}(\nu), g(d \nu / d \mu) \in L^{1}(\mu) g d \nu / d \mu 
\int g d \nu=\int g \frac{d \nu}{d \mu} d \mu
 \nu \ll \lambda \nu \ll \mu \mu \ll \lambda 
\frac{d \nu}{d \lambda}=\frac{d \nu}{d \mu} \frac{d \mu}{d \lambda} \text{ holds for } \lambda \text {-a.e. }
 E 
\nu(E) = \int_E \left(\frac{d\nu}{d\lambda}\right)d\lambda.
 \nu,\mu \mu, \lambda g \in L^1(\mu) \int g d\mu = \int g \left(\frac{d\mu}{d\lambda}\right)d\lambda. \textbf{Letting g = \chi_E \left(\frac{d\nu}{d\mu}\right), we obtain that (how do we show g \in L^1(\mu)? This is needed for us to use (a).)} \int \chi_E \left(\frac{d\nu}{d\mu}\right) d\mu = \int \chi_E \left(\frac{d\nu}{d\mu}\right) \left(\frac{d\mu}{d\lambda}\right)d\lambda. 
\nu(E) = \int_E \left(\frac{d\nu}{d\mu}\right)d\mu := \int \chi_E \left(\frac{d\nu}{d\mu}\right)d\mu.
 \nu(E) = \int \chi_E \left(\frac{d\nu}{d\mu}\right) d\mu = \int \chi_E \left(\frac{d\nu}{d\mu}\right) \left(\frac{d\mu}{d\lambda}\right)d\lambda. 
\nu(E) = \int_E \left(\frac{d\nu}{d\lambda}\right)d\lambda = \int \chi_E \left(\frac{d\nu}{d\mu}\right) \left(\frac{d\mu}{d\lambda}\right)d\lambda.
 f \in L^1 \{ x:f(x) \neq 0\} \sigma f,g \in L^1 \int_E f = \int_E g E \in \mathcal{M} \iff \int ||f-g|| = 0 \iff f = g","['real-analysis', 'measure-theory', 'solution-verification', 'radon-nikodym']"
65,Question on Radon-Nikodym derivatives,Question on Radon-Nikodym derivatives,,"In Folland's Real Analysis, there is this theorem: Suppose that $\nu$ is a $\sigma$ -finite signed measure and $\mu, \lambda$ are $\sigma$ -finite positive measures on $(X, \mathcal{M})$ such that $\nu \ll \mu$ and $\mu \ll \lambda$ . Then, the following hold: (a) If $g \in L^{1}(\nu),$ then $g(d \nu / d \mu) \in L^{1}(\mu)$ (the function $g$ multiplied by $d \nu / d \mu$ ) and $$ \int g d \nu=\int g \frac{d \nu}{d \mu} d \mu $$ (b) We have $\nu \ll \lambda$ (this is obvious from $\nu \ll \mu$ and $\mu \ll \lambda$ ), and $$ \frac{d \nu}{d \lambda}=\frac{d \nu}{d \mu} \frac{d \mu}{d \lambda} \text{ holds for } \lambda \text {-a.e. } $$ The proof is given as follows: Let $E \in \mathcal{M}$ . By considering $\nu^{+}$ and $\nu^{-}$ separately, we may assume that $\nu \geq 0$ . The equation $\int g d \nu=\int g(d \nu / d \mu) d \mu$ is true when $g=\chi_{E}$ by definition of $d \nu / d \mu$ . That is, noting that $d \nu = f d \mu$ , we have $\int \chi_E d \nu = \int \chi_E f d \mu$ . This is exactly $\int g d \nu=\int g(d \nu / d \mu) d \mu$ for $g=\chi_{E}$ . It is therefore true for simple functions by linearity of integrals, then for nonnegative measurable functions by the monotone convergence theorem, and finally for functions in $L^{1}(\nu)$ by linearity (adding/subtracting the positive/negative parts of the real/imaginary parts of $g, g \frac{d \nu}{d \mu}$ , etc.) again. Replacing $\nu, \mu$ by $\mu, \lambda$ and setting $g=\chi_{E}(d \nu / d \mu),$ we obtain from (a): $$ \nu(E)=\int_{E} \frac{d \nu}{d \mu} d \mu=\int_{E} \frac{d \nu}{d \mu} \frac{d \mu}{d \lambda} d \lambda $$ for all $E \in \mathcal{M},$ $\textbf{therefore $(d \nu / d \lambda)=(d \nu / d \mu)(d \mu / d \lambda)$ holds for $\lambda$-a.e. by proposition 1}$ . I don't understand the last part, the part in boldface. For reference, proposition 1 states: (a) If $f \in L^1$ , then $\{ x:f(x) \neq 0\}$ is $\sigma$ -finite. (b) If $f,g \in L^1$ , then $\int_E f = \int_E g$ for all $E \in \mathcal{M}$ $\iff$ $\int ||f-g|| = 0$ $\iff$ $f = g$ a.e.. Surely, in order to use this proposition, one must have $\int_{E} \frac{d \nu}{d \mu} d \lambda=\int_{E} \frac{d \nu}{d \mu} \frac{d \mu}{d \lambda} d \lambda $ ? Is there some connection between $\mu$ and $\lambda$ that I am missing?","In Folland's Real Analysis, there is this theorem: Suppose that is a -finite signed measure and are -finite positive measures on such that and . Then, the following hold: (a) If then (the function multiplied by ) and (b) We have (this is obvious from and ), and The proof is given as follows: Let . By considering and separately, we may assume that . The equation is true when by definition of . That is, noting that , we have . This is exactly for . It is therefore true for simple functions by linearity of integrals, then for nonnegative measurable functions by the monotone convergence theorem, and finally for functions in by linearity (adding/subtracting the positive/negative parts of the real/imaginary parts of , etc.) again. Replacing by and setting we obtain from (a): for all . I don't understand the last part, the part in boldface. For reference, proposition 1 states: (a) If , then is -finite. (b) If , then for all a.e.. Surely, in order to use this proposition, one must have ? Is there some connection between and that I am missing?","\nu \sigma \mu, \lambda \sigma (X, \mathcal{M}) \nu \ll \mu \mu \ll \lambda g \in L^{1}(\nu), g(d \nu / d \mu) \in L^{1}(\mu) g d \nu / d \mu 
\int g d \nu=\int g \frac{d \nu}{d \mu} d \mu
 \nu \ll \lambda \nu \ll \mu \mu \ll \lambda 
\frac{d \nu}{d \lambda}=\frac{d \nu}{d \mu} \frac{d \mu}{d \lambda} \text{ holds for } \lambda \text {-a.e. }
 E \in \mathcal{M} \nu^{+} \nu^{-} \nu \geq 0 \int g d \nu=\int g(d \nu / d \mu) d \mu g=\chi_{E} d \nu / d \mu d \nu = f d \mu \int \chi_E d \nu = \int \chi_E f d \mu \int g d \nu=\int g(d \nu / d \mu) d \mu g=\chi_{E} L^{1}(\nu) g, g \frac{d \nu}{d \mu} \nu, \mu \mu, \lambda g=\chi_{E}(d \nu / d \mu), 
\nu(E)=\int_{E} \frac{d \nu}{d \mu} d \mu=\int_{E} \frac{d \nu}{d \mu} \frac{d \mu}{d \lambda} d \lambda
 E \in \mathcal{M}, \textbf{therefore (d \nu / d \lambda)=(d \nu / d \mu)(d \mu / d \lambda) holds for \lambda-a.e. by proposition 1} f \in L^1 \{ x:f(x) \neq 0\} \sigma f,g \in L^1 \int_E f = \int_E g E \in \mathcal{M} \iff \int ||f-g|| = 0 \iff f = g \int_{E} \frac{d \nu}{d \mu} d \lambda=\int_{E} \frac{d \nu}{d \mu} \frac{d \mu}{d \lambda} d \lambda  \mu \lambda","['real-analysis', 'measure-theory', 'solution-verification', 'radon-nikodym']"
66,"$f$ absolutely continuous and $f'\in L^3[0,1]$, which values of $\alpha $ does $\lim_{x\rightarrow 0^+} x^{-\alpha}f(x)=0$?","absolutely continuous and , which values of  does ?","f f'\in L^3[0,1] \alpha  \lim_{x\rightarrow 0^+} x^{-\alpha}f(x)=0","I'm working my way through some old analysis quals at my university and I came across this question. Let $f$ be absolutely continuous on $[0,1]$ with $f(0)=0$ and $f'\in L^3([0,1])$ . For which values of $\alpha$ does $$ \lim_{x\rightarrow 0^+} x^{-\alpha}f(x)=0$$ for all such $f$ ? I have tried the following approach. Using the FTOC for Lebesgue integrals and Holder's inequality: $x^{-\alpha}f(x)=\int_0^xf'(t)x^{-\alpha}dt\leq ||\chi_{[0,x]}|f'(t)|^3||_{3,[0,1]}x^{-\alpha/3}$ . The quanitity on the right hand side will go to zero if $\alpha>0$ . So this doens't seem to be super helpful. How should I proceed?",I'm working my way through some old analysis quals at my university and I came across this question. Let be absolutely continuous on with and . For which values of does for all such ? I have tried the following approach. Using the FTOC for Lebesgue integrals and Holder's inequality: . The quanitity on the right hand side will go to zero if . So this doens't seem to be super helpful. How should I proceed?,"f [0,1] f(0)=0 f'\in L^3([0,1]) \alpha  \lim_{x\rightarrow 0^+} x^{-\alpha}f(x)=0 f x^{-\alpha}f(x)=\int_0^xf'(t)x^{-\alpha}dt\leq ||\chi_{[0,x]}|f'(t)|^3||_{3,[0,1]}x^{-\alpha/3} \alpha>0","['measure-theory', 'lebesgue-integral', 'lp-spaces', 'absolute-continuity']"
67,Show that $\mathcal{F}$ is an algebra but not a $\sigma$-algebra.,Show that  is an algebra but not a -algebra.,\mathcal{F} \sigma,"Let $\Omega = \mathbb{R}$ and $\mathcal{F}$ be the collection of all finite unions of disjoint intervals of the form $(a,b]\cap\mathbb{R}$ , $-\infty\leq a < b\leq \infty$ . Show that $\mathcal{F}$ is an algebra but not a $\sigma$ -algebra. MY ATTEMPT Indeed, $\Omega\in\mathcal{F}$ : it suffices to take $a = -\infty$ and $b = \infty$ . If $A = (a,b]$ , then $A^{c} = (-\infty,a]\cup(b,\infty)\in\mathcal{F}$ because $(-\infty,a]\in\mathcal{F}$ , $(b,\infty)\in\mathcal{F}$ and the union is finite. Finally, if $A = (a,b]$ and $B = (c,d]$ , then one has that \begin{align*} A\cap B = (a,b]\cap(c,d] = \begin{cases} \varnothing & \text{if}\,\,(a\geq d)\vee(b\leq c)\\\\ (c,b] & \text{if}\,\,a \leq c < b \leq d\\\\ (a,d] & \text{if}\,\,c \leq a < d\leq b \end{cases} \end{align*} which clearly belongs to $\mathcal{F}$ . But $\mathcal{F}$ is not a $\sigma$ -algebra. Indeed, it suffices to consider the sequence of sets: \begin{align*} S_{n} = \left(a,b - \frac{1}{n}\right]\in\mathcal{F} \Rightarrow \bigcup_{n\in\mathbb{N}} S_{n} = (a,b)\not\in\mathcal{F} \end{align*} Consequently, $\mathcal{F}$ is not a $\sigma$ -algebra. Is there any theoretical flaw in my reasoning? Any contribution is appreciated.","Let and be the collection of all finite unions of disjoint intervals of the form , . Show that is an algebra but not a -algebra. MY ATTEMPT Indeed, : it suffices to take and . If , then because , and the union is finite. Finally, if and , then one has that which clearly belongs to . But is not a -algebra. Indeed, it suffices to consider the sequence of sets: Consequently, is not a -algebra. Is there any theoretical flaw in my reasoning? Any contribution is appreciated.","\Omega = \mathbb{R} \mathcal{F} (a,b]\cap\mathbb{R} -\infty\leq a < b\leq \infty \mathcal{F} \sigma \Omega\in\mathcal{F} a = -\infty b = \infty A = (a,b] A^{c} = (-\infty,a]\cup(b,\infty)\in\mathcal{F} (-\infty,a]\in\mathcal{F} (b,\infty)\in\mathcal{F} A = (a,b] B = (c,d] \begin{align*}
A\cap B = (a,b]\cap(c,d] =
\begin{cases}
\varnothing & \text{if}\,\,(a\geq d)\vee(b\leq c)\\\\
(c,b] & \text{if}\,\,a \leq c < b \leq d\\\\
(a,d] & \text{if}\,\,c \leq a < d\leq b
\end{cases}
\end{align*} \mathcal{F} \mathcal{F} \sigma \begin{align*}
S_{n} = \left(a,b - \frac{1}{n}\right]\in\mathcal{F} \Rightarrow \bigcup_{n\in\mathbb{N}} S_{n} = (a,b)\not\in\mathcal{F}
\end{align*} \mathcal{F} \sigma","['measure-theory', 'proof-writing', 'solution-verification']"
68,Help with finding the Lebesgue decomposition of measures,Help with finding the Lebesgue decomposition of measures,,"Consider the increasing, right-continuous function $$ F(x) =  \begin{cases} 0      &x < 0 \\               1+x    &x \geq 0 \end{cases} $$ and let $\nu = \nu_F$ be the associated Borel measure on $\mathbb R$ (so $\nu((a,b]) = F(b)-F(a)$ ).  Find the Lebesgue decomposition of $\nu$ with respect to: (a) $m$ (b) $\delta$ , the Dirac measure at 0 (c) the Cantor measure $\mu$ I think I have a solution to (a), but I am lost as to how to do parts (b) and (c). I'd appreciate any help. This is my attempt for (a): We want to find measures $\rho$ and $\lambda$ such that $$\nu = \rho + \lambda$$ with $\rho << m$ and $\lambda \perp m$ . By the Radon-Nikodym Theorem, $\exists F'$ such that $$\rho(A) = \int_A F'(x) dx. $$ Since $F(x)$ is increasing then for some interval $(a,b] \in \mathcal B$ , $$\rho((a,b]) = \int_a^b F'(x) dx \leq F(b)-F(a) = \nu((a,b]). $$ Clearly, $\rho << m $ . Next define $$\lambda = \nu-\rho.$$ To show that $\lambda \perp m$ , let $E = \{0\}$ and $F = \mathbb R - \{0\}.$ We want to show that $m(E)=\lambda(F)=0$ . It is trivial that $m(E) = 0$ . To show $\lambda(F) =0$ , notice $\lambda \geq 0$ since $\rho(A) \leq \nu(A) \ \ \forall A \in \mathcal B.$ Hence on $(-\infty,0):$ $$ \lambda((-\infty,0))=\nu(-\infty,0)-\rho(-\infty,0)\geq 0  $$ But since $\nu(-\infty,0)=0$ , then $\rho((-\infty,0)=0 \implies \lambda((-\infty,0) = 0$ . Finally on $(0, \infty),$ note that $\nu = \rho$ . Hence $$ \lambda((0,\infty)) = \nu((0,\infty))-\rho((0,\infty))=0$$","Consider the increasing, right-continuous function and let be the associated Borel measure on (so ).  Find the Lebesgue decomposition of with respect to: (a) (b) , the Dirac measure at 0 (c) the Cantor measure I think I have a solution to (a), but I am lost as to how to do parts (b) and (c). I'd appreciate any help. This is my attempt for (a): We want to find measures and such that with and . By the Radon-Nikodym Theorem, such that Since is increasing then for some interval , Clearly, . Next define To show that , let and We want to show that . It is trivial that . To show , notice since Hence on But since , then . Finally on note that . Hence"," F(x) = 
\begin{cases} 0      &x < 0 \\
              1+x    &x \geq 0
\end{cases}
 \nu = \nu_F \mathbb R \nu((a,b]) = F(b)-F(a) \nu m \delta \mu \rho \lambda \nu = \rho + \lambda \rho << m \lambda \perp m \exists F' \rho(A) = \int_A F'(x) dx.  F(x) (a,b] \in \mathcal B \rho((a,b]) = \int_a^b F'(x) dx \leq F(b)-F(a) = \nu((a,b]).  \rho << m  \lambda = \nu-\rho. \lambda \perp m E = \{0\} F = \mathbb R - \{0\}. m(E)=\lambda(F)=0 m(E) = 0 \lambda(F) =0 \lambda \geq 0 \rho(A) \leq \nu(A) \ \ \forall A \in \mathcal B. (-\infty,0):  \lambda((-\infty,0))=\nu(-\infty,0)-\rho(-\infty,0)\geq 0   \nu(-\infty,0)=0 \rho((-\infty,0)=0 \implies \lambda((-\infty,0) = 0 (0, \infty), \nu = \rho  \lambda((0,\infty)) = \nu((0,\infty))-\rho((0,\infty))=0","['real-analysis', 'measure-theory', 'lebesgue-measure', 'radon-nikodym']"
69,Proof Check: Multiplication operator on $L^{p}(\mathbb R^{d})$ is closed,Proof Check: Multiplication operator on  is closed,L^{p}(\mathbb R^{d}),"Let $m : \mathbb R^{d} \to \mathbb K$ measurable and $A: \operatorname{dom}(A) \to L^{p}(\mathbb R^{d})$ such that $Af(x)=m(x)f(x)$ and $\operatorname{dom}(A):=\{f \in L^{p}(\mathbb R^{d}):mf \in L^{p}(\mathbb R^{d})\}$ . Show that $A$ is a closed operator. My proof: Let $(f_{n})_{n \in \mathbb N}\subseteq \operatorname{dom}(A)$ with $f_{n} \xrightarrow{L^{p}} f$ and $Af_{n} \xrightarrow{ L^{p}} z$ . Then by $L^{p}$ convergence there exists a measurable $A_{k}$ and a subsequence $(f_{n(k)})_{k\in \mathbb N}$ such that $\mu(A_{k})=0$ and $f_{n(k)}(x)\xrightarrow{k \to \infty} f(x),\; \forall x \in A_{k}^{c}$ . Now we want to show that $mf=z$ a.e., thus consider $$\lvert \lvert mf-z\rvert\rvert_{p}^{p}\\=\int\limits_{\mathbb R^{d}}\lvert m(x)f(x)-z(x)\rvert^{p}dx\\=\int\limits_{A_{k}^{c}}\lvert m(x)f(x)-z(x)\rvert^{p}dx\\=\int\limits_{A_{k}^{c}}\lim\limits_{k\to \infty}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\ =\int\limits_{A_{k}^{c}}\liminf\limits_{k\to \infty}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\ \leq \liminf\limits_{k\to \infty} \int\limits_{A_{k}^{c}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\\leq \liminf\limits_{k\to \infty} \int\limits_{\mathbb R^{d}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx \\ =\lim\limits_{k\to \infty} \int\limits_{\mathbb R^{d}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx=0.$$ Thus $mf = z$ a.e. and since $z \in L^{p}$ , it follows that $f \in \operatorname{dom}(T)$ and hence $A$ is closed. The critical part of my proof was to use Fatou's Lemma. Are there any other alternative ways to prove this and is my proof at all correct?","Let measurable and such that and . Show that is a closed operator. My proof: Let with and . Then by convergence there exists a measurable and a subsequence such that and . Now we want to show that a.e., thus consider Thus a.e. and since , it follows that and hence is closed. The critical part of my proof was to use Fatou's Lemma. Are there any other alternative ways to prove this and is my proof at all correct?","m : \mathbb R^{d} \to \mathbb K A: \operatorname{dom}(A) \to L^{p}(\mathbb R^{d}) Af(x)=m(x)f(x) \operatorname{dom}(A):=\{f \in L^{p}(\mathbb R^{d}):mf \in L^{p}(\mathbb R^{d})\} A (f_{n})_{n \in \mathbb N}\subseteq \operatorname{dom}(A) f_{n} \xrightarrow{L^{p}} f Af_{n} \xrightarrow{ L^{p}} z L^{p} A_{k} (f_{n(k)})_{k\in \mathbb N} \mu(A_{k})=0 f_{n(k)}(x)\xrightarrow{k \to \infty} f(x),\; \forall x \in A_{k}^{c} mf=z \lvert \lvert mf-z\rvert\rvert_{p}^{p}\\=\int\limits_{\mathbb R^{d}}\lvert m(x)f(x)-z(x)\rvert^{p}dx\\=\int\limits_{A_{k}^{c}}\lvert m(x)f(x)-z(x)\rvert^{p}dx\\=\int\limits_{A_{k}^{c}}\lim\limits_{k\to \infty}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\ =\int\limits_{A_{k}^{c}}\liminf\limits_{k\to \infty}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\ \leq \liminf\limits_{k\to \infty} \int\limits_{A_{k}^{c}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\\leq \liminf\limits_{k\to \infty} \int\limits_{\mathbb R^{d}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx \\ =\lim\limits_{k\to \infty} \int\limits_{\mathbb R^{d}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx=0. mf = z z \in L^{p} f \in \operatorname{dom}(T) A","['real-analysis', 'integration', 'functional-analysis', 'measure-theory', 'lp-spaces']"
70,"Evaluating S depending upon following condition: Calculate the sum $S=\Sigma \Sigma \Sigma x_{i} x_{j} x_{k},$",Evaluating S depending upon following condition: Calculate the sum,"S=\Sigma \Sigma \Sigma x_{i} x_{j} x_{k},","Suppose that $x_{1}, x_{2}, \ldots, x_{n}(n>2)$ are real numbers such that $x_{i}=x_{n-i+1}$ for $1 \leq i \leq n .$ Consider the sum $S=\Sigma \Sigma \Sigma x_{i} x_{j} x_{k},$ where summations are taken over all i, $j, k: 1 \leq i, j, k \leq n$ and $i, j, k$ are all distinct. Then S equals $S=\sum \Sigma x_{i} x_{j}\left(L-x_{i}-x_{j}\right) i \neq j$ where $L=x_{1}+x_{2}+\ldots+x_{n}$ $=L \sum \Sigma x_{i} x_{j}-\sum \Sigma x_{i}^{2} x_{j}-\sum \Sigma x_{i} x_{j}^{2}$ $=\mathrm{L} \sum \mathrm{x}_{\mathrm{i}}\left(\mathrm{L}-\mathrm{x}_{\mathrm{i}}\right)-\Sigma \mathrm{x}_{\mathrm{i}}^{2}\left(\mathrm{L}-\mathrm{x}_{\mathrm{i}}\right)-\Sigma \mathrm{x}_{\mathrm{i}}\left(\mathrm{M}-\mathrm{x}_{\mathrm{i}}^{2}\right)$ where $\mathrm{M}=\mathrm{x}_{1}^{2}+\mathrm{x}_{2}^{2}+\ldots .+\mathrm{x}_{\mathrm{n}}^{2}$ $=\mathrm{L}^{3}-3 \mathrm{LM}+2 \Sigma \mathrm{x}_{\mathrm{i}}^{3}$ What to do next?","Suppose that are real numbers such that for Consider the sum where summations are taken over all i, and are all distinct. Then S equals where where What to do next?","x_{1}, x_{2}, \ldots, x_{n}(n>2) x_{i}=x_{n-i+1} 1 \leq i \leq n . S=\Sigma \Sigma \Sigma x_{i} x_{j} x_{k}, j, k: 1 \leq i, j, k \leq n i, j, k S=\sum \Sigma x_{i} x_{j}\left(L-x_{i}-x_{j}\right) i \neq j L=x_{1}+x_{2}+\ldots+x_{n} =L \sum \Sigma x_{i} x_{j}-\sum \Sigma x_{i}^{2} x_{j}-\sum \Sigma x_{i} x_{j}^{2} =\mathrm{L} \sum \mathrm{x}_{\mathrm{i}}\left(\mathrm{L}-\mathrm{x}_{\mathrm{i}}\right)-\Sigma \mathrm{x}_{\mathrm{i}}^{2}\left(\mathrm{L}-\mathrm{x}_{\mathrm{i}}\right)-\Sigma \mathrm{x}_{\mathrm{i}}\left(\mathrm{M}-\mathrm{x}_{\mathrm{i}}^{2}\right) \mathrm{M}=\mathrm{x}_{1}^{2}+\mathrm{x}_{2}^{2}+\ldots .+\mathrm{x}_{\mathrm{n}}^{2} =\mathrm{L}^{3}-3 \mathrm{LM}+2 \Sigma \mathrm{x}_{\mathrm{i}}^{3}","['sequences-and-series', 'measure-theory']"
71,A question of why these integrands can be taken out of the integrals.,A question of why these integrands can be taken out of the integrals.,,"Rudin says that (2) can be rewritten as (4), as seen below. However, both integrands in (2) respectively depend on $x$ and $y$ , which implies that (2) could not be rewritten as (4). Is there any problem in my understanding? Thanks a lot.","Rudin says that (2) can be rewritten as (4), as seen below. However, both integrands in (2) respectively depend on and , which implies that (2) could not be rewritten as (4). Is there any problem in my understanding? Thanks a lot.",x y,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'product-measure']"
72,Characteristic functions are not in $H^{1/2}$.,Characteristic functions are not in .,H^{1/2},"This is an exercise from Lieb, Loss. I need to show that no characteristic function of a set $A\subset\mathbb{R}^n$ with a finite positive measure is in $H^1(\mathbb{R}^n)$ or even in $H^{\frac{1}{2}}(\mathbb{R}^n)$ . I know that that the distributional partial derivatives of such a function must be zero. But I don't see the contradiction since in that case the derivatives are in ${\rm L}^2(\mathbb{R}^n)$ . I would also like some help with the second part. According to the book, a function is in $H^{\frac{1}{2}}(\mathbb{R}^n)$ if and only if $\int\limits_{\mathbb{R}^n}(1+2\pi|k|)|\hat{f}(k)|^2{\rm d}k$ is finite. But for an arbitrary $A$ , how can I know enough about $\chi_A$ to show that this integral isn't finite?","This is an exercise from Lieb, Loss. I need to show that no characteristic function of a set with a finite positive measure is in or even in . I know that that the distributional partial derivatives of such a function must be zero. But I don't see the contradiction since in that case the derivatives are in . I would also like some help with the second part. According to the book, a function is in if and only if is finite. But for an arbitrary , how can I know enough about to show that this integral isn't finite?",A\subset\mathbb{R}^n H^1(\mathbb{R}^n) H^{\frac{1}{2}}(\mathbb{R}^n) {\rm L}^2(\mathbb{R}^n) H^{\frac{1}{2}}(\mathbb{R}^n) \int\limits_{\mathbb{R}^n}(1+2\pi|k|)|\hat{f}(k)|^2{\rm d}k A \chi_A,"['measure-theory', 'distribution-theory']"
73,Measurability of a group action on a probability measure.,Measurability of a group action on a probability measure.,,"Consider a measure space $(X,\mathcal{B})$ where $\mathcal{B}$ is the Borel $\sigma$ -algebra of some topology on $X$ . Suppose that the topological group $G$ acts continuously on $X$ and $G$ is given its Borel $\sigma$ -algebra, $\mathcal{A}$ . Lastly suppose that we have some measure on $X$ , $Q$ . My question is, under what conditions is the map $f:G \rightarrow \mathbb{R}$ defined by $f(g) = Q(g^{-1}B)$ measurable for all (fixed) $B \in \mathcal{B}$ ? I've tried to see if I could show this for open $B$ and then extend this to all sets in $\mathcal{B}$ but I haven't been able to show it for the open case. Thanks for your help.","Consider a measure space where is the Borel -algebra of some topology on . Suppose that the topological group acts continuously on and is given its Borel -algebra, . Lastly suppose that we have some measure on , . My question is, under what conditions is the map defined by measurable for all (fixed) ? I've tried to see if I could show this for open and then extend this to all sets in but I haven't been able to show it for the open case. Thanks for your help.","(X,\mathcal{B}) \mathcal{B} \sigma X G X G \sigma \mathcal{A} X Q f:G \rightarrow \mathbb{R} f(g) = Q(g^{-1}B) B \in \mathcal{B} B \mathcal{B}","['group-theory', 'measure-theory']"
74,"Meaning of $\int_\Omega\langle\phi,Du(x)\rangle$, in the definition of BV space","Meaning of , in the definition of BV space","\int_\Omega\langle\phi,Du(x)\rangle","I was going through the definition of bounded variation functions on Wikipedia : https://en.wikipedia.org/wiki/Bounded_variation Definition . Let $\Omega$ be an open subset of $\mathbb{R}^n$ . A function $u\in L^1(\Omega)$ is said of bounded variation if there exists a finite vector Radon measure $Du\in \mathcal{M}(\Omega,\mathbb{R}^n)$ such that the following equality holds : $$\int_\Omega u(x)\text{div}\phi(x)dx=-\int_\Omega\langle\phi,Du(x)\rangle\quad\forall \phi\in C_c^1(\Omega,\mathbb{R}^n).$$ I am very confused with this definition, since I am not sure what $\int_\Omega\langle\phi,Du(x)\rangle$ is. I never saw such a notation, and I could not find any reference to this 'vector measure' and corresponding Lebesgue integral. It will be very appreciated if anyone could give me some explanation on this.","I was going through the definition of bounded variation functions on Wikipedia : https://en.wikipedia.org/wiki/Bounded_variation Definition . Let be an open subset of . A function is said of bounded variation if there exists a finite vector Radon measure such that the following equality holds : I am very confused with this definition, since I am not sure what is. I never saw such a notation, and I could not find any reference to this 'vector measure' and corresponding Lebesgue integral. It will be very appreciated if anyone could give me some explanation on this.","\Omega \mathbb{R}^n u\in L^1(\Omega) Du\in \mathcal{M}(\Omega,\mathbb{R}^n) \int_\Omega u(x)\text{div}\phi(x)dx=-\int_\Omega\langle\phi,Du(x)\rangle\quad\forall \phi\in C_c^1(\Omega,\mathbb{R}^n). \int_\Omega\langle\phi,Du(x)\rangle","['measure-theory', 'bounded-variation']"
75,When is a probability density function square-integrable?,When is a probability density function square-integrable?,,"Consider a measure space $(X, \mathcal{F}, \mu)$ and let $f \in L^1(X, \mathcal{F}, \mu)$ with $f(x) >0$ for all $x \in X$ be a probability density function.  As discussed in this question, $f$ need not be in $L^2(X, \mathcal{F}, \mu)$ . Moreover, $f$ can be continuous and differentiable and still not be square-integrable. My question is if there are ""simple"" assumptions one can make about $f$ such that it lies in $L^2(X, \mathcal{F}, \mu)$ .","Consider a measure space and let with for all be a probability density function.  As discussed in this question, need not be in . Moreover, can be continuous and differentiable and still not be square-integrable. My question is if there are ""simple"" assumptions one can make about such that it lies in .","(X, \mathcal{F}, \mu) f \in L^1(X, \mathcal{F}, \mu) f(x) >0 x \in X f L^2(X, \mathcal{F}, \mu) f f L^2(X, \mathcal{F}, \mu)","['probability', 'measure-theory', 'lp-spaces']"
76,A set has measure zero iff for every $\epsilon>0$ there is a countable covering of open rectangles such that $ \sum_{i=1}^\infty v(Q_i)<\epsilon $,A set has measure zero iff for every  there is a countable covering of open rectangles such that,\epsilon>0  \sum_{i=1}^\infty v(Q_i)<\epsilon ,"What shown below is a reference from ""Analysis on manifolds"" by James R. Munkres. Definition Let $A$ a subset of $\Bbb{R}^n$ . We say $A$ has measure zero in $\Bbb{R}^n$ if for every $\epsilon>0$ , there is a covering $Q_1,Q_2,...$ of $A$ by countably many rectangles such that $$ \sum_{i=1}^\infty v(Q_i)<\epsilon $$ Theorem A set $A$ has measure zero in $\Bbb{R}^n$ if and only if for every $\epsilon>0$ there is a countable covering of $A$ by open rectangles $\overset{°}Q_1,\overset{°}Q_2,...$ such that $$ \sum_{i=1}^\infty v(Q_i)<\epsilon  $$ Proof . If the open rectangles $\overset{°}Q_1,\overset{°}Q_2,...$ cover $A$ , then so the rectangles $Q_1,Q_2,...$ . Thus the given condition implies that $A$ has measure zero. Conversely, suppose $A$ has measure zero. Cover $A$ by rectangles $Q'_1,Q'_2,...,$ of total volume $\frac{\epsilon}2$ . For each $i$ , chose a rectangle $Q_i$ such that $$ 1.\quad Q'_i\subset\overset{°}Q_i\text{ and }v(Q_i)\le 2v(Q'_i) $$ (This we can do because $v(Q)$ is a continuous function of the end points of the component intervals of $Q$ ). Then the open rectangles $\overset{°}Q_1,\overset{°}Q_2,...$ cover $A$ and $\sum v(Q_i)<\epsilon$ . So I don't understand why it is possible to make the rectangles $Q_i$ such that they respect the condition $1$ and so I ask to well explain this: naturally I don't understand Munkres explanation and so you can or to explain better what Munkres said or to show another explanation. So could someone help me, please?","What shown below is a reference from ""Analysis on manifolds"" by James R. Munkres. Definition Let a subset of . We say has measure zero in if for every , there is a covering of by countably many rectangles such that Theorem A set has measure zero in if and only if for every there is a countable covering of by open rectangles such that Proof . If the open rectangles cover , then so the rectangles . Thus the given condition implies that has measure zero. Conversely, suppose has measure zero. Cover by rectangles of total volume . For each , chose a rectangle such that (This we can do because is a continuous function of the end points of the component intervals of ). Then the open rectangles cover and . So I don't understand why it is possible to make the rectangles such that they respect the condition and so I ask to well explain this: naturally I don't understand Munkres explanation and so you can or to explain better what Munkres said or to show another explanation. So could someone help me, please?","A \Bbb{R}^n A \Bbb{R}^n \epsilon>0 Q_1,Q_2,... A 
\sum_{i=1}^\infty v(Q_i)<\epsilon
 A \Bbb{R}^n \epsilon>0 A \overset{°}Q_1,\overset{°}Q_2,... 
\sum_{i=1}^\infty v(Q_i)<\epsilon 
 \overset{°}Q_1,\overset{°}Q_2,... A Q_1,Q_2,... A A A Q'_1,Q'_2,..., \frac{\epsilon}2 i Q_i 
1.\quad Q'_i\subset\overset{°}Q_i\text{ and }v(Q_i)\le 2v(Q'_i)
 v(Q) Q \overset{°}Q_1,\overset{°}Q_2,... A \sum v(Q_i)<\epsilon Q_i 1","['calculus', 'integration', 'measure-theory']"
77,Explain step in Galindo and Pascual (Quantum Mechanics I) proof of self-adjointness of the momentum operator in QM,Explain step in Galindo and Pascual (Quantum Mechanics I) proof of self-adjointness of the momentum operator in QM,,"In the book Quantum Mechanics (Volume I) by Galindo & Pascual, they define the domain of the QM momentum operator on the Hilbert space $\mathcal{H}=L^2(\mathbb{R})$ as \begin{equation*}   D(P)=\Biggl\{\psi\in\mathcal{H}: \psi\text{ absolutely continuous,}   \int_{-\infty}^\infty\!dx\,\Biggl\lvert\frac{d\psi(x)}{dx}\Biggr\rvert^2<\infty\Biggr\} \end{equation*} and the momentum operator $P$ by $$(P\psi)(x)=-i\frac{d\psi(x)}{dx}.$$ They go on to prove that $P$ is densely defined and symmetric. To prove that $P$ is self-adjoint, they attempt to show that $D(P^\dagger)\subseteq D(P)$ . Here are the next couple of lines of the proof: ... consider a function $\psi\in D(P^\dagger)$ and define $\psi_1=P^\dagger\psi$ ; then $$\langle\psi|P|\varphi\rangle=\langle\psi_1|\varphi\rangle,\quad\forall\varphi\in D(P)$$ can be rewritten as \begin{equation*}   \begin{split}     \langle\psi|P|\varphi\rangle     &=\int_{-\infty}^\infty\!dx\,\psi_1^*(x)\varphi(x)\\     &=i\int_{-\infty}^\infty\!dx\Biggl[\frac{d}{dx}     \Biggl(i\int_0^x\!dt\,\psi_1(t)+c\Biggr)^*\Biggr]\varphi(x),   \end{split} \end{equation*} where $c$ is an arbitrary constant. Choosing $\varphi\in C^\infty_0$ , integrating by parts, and taking into account that $\varphi$ is zero outside a finite interval, we obtain \begin{equation}\tag{2.16}   \int_{-\infty}^\infty\!dx\Biggl(\psi(x)-i\int_0^x\!dt\,\psi_1(t)-c\Biggr)^*   \Biggl(-i\frac{d\varphi(x)}{dx}\Biggr)=0,\quad\forall\varphi\in C^\infty_0. \end{equation} [So far, this seems OK to me. It is the next statement that I don't follow:] Since $C^\infty_0$ is dense in $L^2(\mathbb{R})$ , the first factor of the integrand in (2.16) must be a constant and hence, with a convenient choice $c_0$ for $c$ , we can write almost everywhere \begin{equation}\tag{2.17}   \psi(x)=c_0+i\int_0^x\!dt\,\psi_1(t), \end{equation} [and it goes on from there] I want to concentrate on the validity of going from (2.16) to (2.17). I understand that, with total lack of rigor, if we have $$\int h\varphi'=0\quad\forall\varphi\in C^\infty_0$$ we'd like to do an integration by parts and write $$\int h'\varphi=\int h\varphi'=0\quad\forall\varphi\in C^\infty_0$$ from which we would get that $h'=0$ almost everywhere hence $h=c$ almost everywhere. But I don't see how to apply that here since I don't know that $\psi$ is differentiable a.e. or even a.e. on a compact interval. It even looks like a version of the DuBois-Raymond theorem from variational calculus, but I only know that for continuous functions on a compact interval, so it would seem to not apply here. So, my questions are: how do you get from (2.16) to (2.17)? what element of $L^2(\mathbb{R})$ would they be talking about when they say that $C^\infty_0$ is dense in $L^2(\mathbb{R})$ ?","In the book Quantum Mechanics (Volume I) by Galindo & Pascual, they define the domain of the QM momentum operator on the Hilbert space as and the momentum operator by They go on to prove that is densely defined and symmetric. To prove that is self-adjoint, they attempt to show that . Here are the next couple of lines of the proof: ... consider a function and define ; then can be rewritten as where is an arbitrary constant. Choosing , integrating by parts, and taking into account that is zero outside a finite interval, we obtain [So far, this seems OK to me. It is the next statement that I don't follow:] Since is dense in , the first factor of the integrand in (2.16) must be a constant and hence, with a convenient choice for , we can write almost everywhere [and it goes on from there] I want to concentrate on the validity of going from (2.16) to (2.17). I understand that, with total lack of rigor, if we have we'd like to do an integration by parts and write from which we would get that almost everywhere hence almost everywhere. But I don't see how to apply that here since I don't know that is differentiable a.e. or even a.e. on a compact interval. It even looks like a version of the DuBois-Raymond theorem from variational calculus, but I only know that for continuous functions on a compact interval, so it would seem to not apply here. So, my questions are: how do you get from (2.16) to (2.17)? what element of would they be talking about when they say that is dense in ?","\mathcal{H}=L^2(\mathbb{R}) \begin{equation*}
  D(P)=\Biggl\{\psi\in\mathcal{H}: \psi\text{ absolutely continuous,}
  \int_{-\infty}^\infty\!dx\,\Biggl\lvert\frac{d\psi(x)}{dx}\Biggr\rvert^2<\infty\Biggr\}
\end{equation*} P (P\psi)(x)=-i\frac{d\psi(x)}{dx}. P P D(P^\dagger)\subseteq D(P) \psi\in D(P^\dagger) \psi_1=P^\dagger\psi \langle\psi|P|\varphi\rangle=\langle\psi_1|\varphi\rangle,\quad\forall\varphi\in D(P) \begin{equation*}
  \begin{split}
    \langle\psi|P|\varphi\rangle
    &=\int_{-\infty}^\infty\!dx\,\psi_1^*(x)\varphi(x)\\
    &=i\int_{-\infty}^\infty\!dx\Biggl[\frac{d}{dx}
    \Biggl(i\int_0^x\!dt\,\psi_1(t)+c\Biggr)^*\Biggr]\varphi(x),
  \end{split}
\end{equation*} c \varphi\in C^\infty_0 \varphi \begin{equation}\tag{2.16}
  \int_{-\infty}^\infty\!dx\Biggl(\psi(x)-i\int_0^x\!dt\,\psi_1(t)-c\Biggr)^*
  \Biggl(-i\frac{d\varphi(x)}{dx}\Biggr)=0,\quad\forall\varphi\in C^\infty_0.
\end{equation} C^\infty_0 L^2(\mathbb{R}) c_0 c \begin{equation}\tag{2.17}
  \psi(x)=c_0+i\int_0^x\!dt\,\psi_1(t),
\end{equation} \int h\varphi'=0\quad\forall\varphi\in C^\infty_0 \int h'\varphi=\int h\varphi'=0\quad\forall\varphi\in C^\infty_0 h'=0 h=c \psi L^2(\mathbb{R}) C^\infty_0 L^2(\mathbb{R})","['functional-analysis', 'measure-theory', 'hilbert-spaces', 'lebesgue-integral']"
78,Average distance between two points on $U(n)$,Average distance between two points on,U(n),"I'm interested in the following problem : Compute the average distance between two points independently chosen at random on $U(n)$ . What i have done so far : Observe that : $$\begin{array}{lll} \int_{U(n)}\int_{U(n)}d(g_1,g_2)d\mu_1d\mu_2&=&\int\int d(g_1g_2^{-1},1)d\mu_1d\mu_2\\ &=&\int\int d(g,1) d\mu d\mu_2\\ &=&\int d(g,1) d\mu\\ \end{array}$$ where : the second equality comes from the bi-invariance of the Haar-measure; the last equality comes from the fact that $\mu_2({U(n)})=1$ . Now, $g\mapsto d(g,1)$ is invariant by conjugation so Weyl's integration formula applies : $$\int_{U(n)} d(g,1) d\mu = \int_{T} d(t,1) u(t) dt =\frac{1}{n!}\int_{[-\pi;\pi]^n} \sqrt{\theta_1^2+...+\theta_n^2}\prod_{i\neq j}\left|e^{i\theta_i}-e^{i\theta_j}\right|\frac{d\theta_1}{2\pi}...\frac{d\theta_n}{2\pi}$$ This is because, if $t=diag(e^{i\theta_1},...e^{i\theta_n})$ with $\theta_i\in[-\pi,\pi]$ , then $d(t,1)=\sqrt{\theta_1^2+\ldots+\theta_n^2}$ . Indeed, $\gamma:[0,1]\to U(n)$ , $\gamma(s)=diag(e^{is\theta_1},...e^{is\theta_n})$ is a minimal geodesic of length $||(\theta_1,...,\theta_n)||=\sqrt{\theta_1^2+...+\theta_n^2}$ . Questions : Are my reasoning and computation so far correct ? If they are, how can I compute that monstrosity ?! I mean, even in the simplest non trivial case where $n=2$ : $$\int_{U(2)} d(g,1)d\mu = \frac{1}{2\pi^2}\int_{[-\pi;\pi]^2} \sqrt{x^2+y^2}\sin\left(\frac{x-y}{2}\right)^2 dx\,dy$$ Wolfram and back-of-the-envelope simulations with Sage suggest the result is $\approx 2,48$ . Is this the best we can expect ?! If exact computation is not manageable for general $n$ , how can i compute at least an equivalent when $n\to \infty$ ? I'd like to understand how much the unitary case deviates from abelian case of same rank. (Simulations on Sage suggest that the average distance between two points on the standard torus $T_n$ grows like $O(\sqrt{n})$ .) How can I do that ? Surely, these questions must have been investigated but i don't where to look for. What are the key-words/references for these questions ? EDIT2 : i think i can answer question 3. for the torus case ! Indeed, let $(X_i)_{i\in\mathbb{N}}$ be a sequence of i.i.d. variables with uniform distribution $\mathcal{U}([-\pi;\pi])$ . Then $\frac{X_1^2+\ldots+X_n^2}{n}\rightarrow^{a.s.} E[x_1^2]=\frac{\pi^2}{3}$ (law of large numbers). Therefore, $$\int_{[-\pi,\pi]^n}\sqrt{x_1^2+...+x_n^2}\frac{dx_1}{2\pi}...\frac{dx_n}{2\pi} \sim \sqrt{n}\sqrt{\frac{\pi^2}{3}}=\frac{\pi}{\sqrt{3}}\sqrt{n}$$ If this is correct, this settles question 3. for the torus case.","I'm interested in the following problem : Compute the average distance between two points independently chosen at random on . What i have done so far : Observe that : where : the second equality comes from the bi-invariance of the Haar-measure; the last equality comes from the fact that . Now, is invariant by conjugation so Weyl's integration formula applies : This is because, if with , then . Indeed, , is a minimal geodesic of length . Questions : Are my reasoning and computation so far correct ? If they are, how can I compute that monstrosity ?! I mean, even in the simplest non trivial case where : Wolfram and back-of-the-envelope simulations with Sage suggest the result is . Is this the best we can expect ?! If exact computation is not manageable for general , how can i compute at least an equivalent when ? I'd like to understand how much the unitary case deviates from abelian case of same rank. (Simulations on Sage suggest that the average distance between two points on the standard torus grows like .) How can I do that ? Surely, these questions must have been investigated but i don't where to look for. What are the key-words/references for these questions ? EDIT2 : i think i can answer question 3. for the torus case ! Indeed, let be a sequence of i.i.d. variables with uniform distribution . Then (law of large numbers). Therefore, If this is correct, this settles question 3. for the torus case.","U(n) \begin{array}{lll}
\int_{U(n)}\int_{U(n)}d(g_1,g_2)d\mu_1d\mu_2&=&\int\int d(g_1g_2^{-1},1)d\mu_1d\mu_2\\
&=&\int\int d(g,1) d\mu d\mu_2\\
&=&\int d(g,1) d\mu\\
\end{array} \mu_2({U(n)})=1 g\mapsto d(g,1) \int_{U(n)} d(g,1) d\mu = \int_{T} d(t,1) u(t) dt =\frac{1}{n!}\int_{[-\pi;\pi]^n} \sqrt{\theta_1^2+...+\theta_n^2}\prod_{i\neq j}\left|e^{i\theta_i}-e^{i\theta_j}\right|\frac{d\theta_1}{2\pi}...\frac{d\theta_n}{2\pi} t=diag(e^{i\theta_1},...e^{i\theta_n}) \theta_i\in[-\pi,\pi] d(t,1)=\sqrt{\theta_1^2+\ldots+\theta_n^2} \gamma:[0,1]\to U(n) \gamma(s)=diag(e^{is\theta_1},...e^{is\theta_n}) ||(\theta_1,...,\theta_n)||=\sqrt{\theta_1^2+...+\theta_n^2} n=2 \int_{U(2)} d(g,1)d\mu = \frac{1}{2\pi^2}\int_{[-\pi;\pi]^2} \sqrt{x^2+y^2}\sin\left(\frac{x-y}{2}\right)^2 dx\,dy \approx 2,48 n n\to \infty T_n O(\sqrt{n}) (X_i)_{i\in\mathbb{N}} \mathcal{U}([-\pi;\pi]) \frac{X_1^2+\ldots+X_n^2}{n}\rightarrow^{a.s.} E[x_1^2]=\frac{\pi^2}{3} \int_{[-\pi,\pi]^n}\sqrt{x_1^2+...+x_n^2}\frac{dx_1}{2\pi}...\frac{dx_n}{2\pi} \sim \sqrt{n}\sqrt{\frac{\pi^2}{3}}=\frac{\pi}{\sqrt{3}}\sqrt{n}","['integration', 'group-theory', 'measure-theory', 'reference-request', 'unitary-matrices']"
79,Barycentric Projection,Barycentric Projection,,"Hi I'm reading Ambrosio : Gradient Flows in Metric Spaces 2nd Edition. Let $X$ be a Polish space and let $\mathcal{P}(X)$ be Borel probability measures on $X$ , analogously define $\mathcal{P}(X\times X)$ . Let $\gamma \in \mathcal{P}(X\times X)$ have 1st marginal $\mu\in \mathcal{P}(X)$ , and admit the following disintegration w.r.t to $\mu$ : $$ \gamma=\int_X \gamma_{x_1}d\mu(x_1). $$ Then Ambrosio defines (Page 126 definition 5.4.2) the barycentric projection $\overline{\gamma}:X \to X$ as $$\overline{\gamma}(x_1)=\int_X x_2 d\gamma_{x_1}(x_2).$$ I'm REALLY struggling to see the meaning of this projection/what role it plays. To me this projection just maps an element in $X$ to ""the $\gamma$ conditional distribution (conditioned that the first element is $x_1$ ?) "". Does anyone have some more knowledge about how to view this projection ?","Hi I'm reading Ambrosio : Gradient Flows in Metric Spaces 2nd Edition. Let be a Polish space and let be Borel probability measures on , analogously define . Let have 1st marginal , and admit the following disintegration w.r.t to : Then Ambrosio defines (Page 126 definition 5.4.2) the barycentric projection as I'm REALLY struggling to see the meaning of this projection/what role it plays. To me this projection just maps an element in to ""the conditional distribution (conditioned that the first element is ?) "". Does anyone have some more knowledge about how to view this projection ?",X \mathcal{P}(X) X \mathcal{P}(X\times X) \gamma \in \mathcal{P}(X\times X) \mu\in \mathcal{P}(X) \mu  \gamma=\int_X \gamma_{x_1}d\mu(x_1).  \overline{\gamma}:X \to X \overline{\gamma}(x_1)=\int_X x_2 d\gamma_{x_1}(x_2). X \gamma x_1,"['measure-theory', 'convex-optimization', 'optimal-control', 'optimal-transport', 'barycentric-coordinates']"
80,Logarithmic Laplace transform of the uniform measure on a convex set,Logarithmic Laplace transform of the uniform measure on a convex set,,"This question comes from a lemma (whose proof is left as an exercise) which I came across when reading the continuous exponential-weighting algorithm. Let $\mathcal K\subset\mathbb R^d$ be a compact convex set with finite volume: $|\mathcal K|<\infty$ . Let $u\in\mathbb R^d$ be a fixed vector and define $x^*\in\arg\min_{x\in\mathcal K}\langle x, u\rangle$ . Prove that $$ -\log\left(\frac{1}{|\mathcal K|}\int_{\mathcal K}e^{-\langle x-x^*, u\rangle}dx\right)\leq 1+\max\left(0, d\log\left(\sup_{x,y\in\mathcal K}\langle x-y, u\rangle\right)\right). $$ Can someone give me some hint how to prove this inequality? I feel confused how $d$ appears in the bound. Thanks!",This question comes from a lemma (whose proof is left as an exercise) which I came across when reading the continuous exponential-weighting algorithm. Let be a compact convex set with finite volume: . Let be a fixed vector and define . Prove that Can someone give me some hint how to prove this inequality? I feel confused how appears in the bound. Thanks!,"\mathcal K\subset\mathbb R^d |\mathcal K|<\infty u\in\mathbb R^d x^*\in\arg\min_{x\in\mathcal K}\langle x, u\rangle 
-\log\left(\frac{1}{|\mathcal K|}\int_{\mathcal K}e^{-\langle x-x^*, u\rangle}dx\right)\leq 1+\max\left(0, d\log\left(\sup_{x,y\in\mathcal K}\langle x-y, u\rangle\right)\right).
 d","['probability', 'functional-analysis', 'measure-theory', 'convex-geometry']"
81,"Approximating multi-variate continuous functions that map to [0,1] by polynomials","Approximating multi-variate continuous functions that map to [0,1] by polynomials",,"The Stone-Weierstrass Theorem says that the polynomials are dense in C[X] under the sup norm, where X is any Hausdorff space. There have been several previous posts asking about approximations of $f \in C[X]$ with polynomials $P_n = \{p | p $ is a polynomial of degree less than or equal to $n\}$ for a fixed degree $n$ . It seems like this is a problem that has certain bounds for $\inf_{p \in P_n} ||f - p||_\infty < \epsilon(n)$ , where $\epsilon(n)$ is on the order $O(1/n)$ ( 1 , 2 ) I have two questions: If we restrict the output of $f: [-a, b] \rightarrow [0,1]$ , are there any tighter bounds that can be said? Clearly, $\inf_{p \in P_n} ||f - p||_\infty < 1/2$ for the trivial case $p=1/2$ . Can we prove the same bounds for $f: [-a, b]^n -> \mathbb{R}$ and a multi-variate polynomial $P(x_1, ... x_n)$ as we did in the one-dimensional case? What if we restrict the range to $f: [-a, b]^n -> [0,1]$ ? I am not sure how one would go about the first question, other than to note that $f$ is 1-Lipshitz and by the above posts there are potentially some tighter bounds there for Lipschitz functions. For the second question, I showed that the polynomials are still dense in $C[X]$ , which makes me think there might be an analgous way to prove the error bounds as well? This is probably all well-understood, but I'm not well-read on approximation theory. Any guidance would be wonderful.","The Stone-Weierstrass Theorem says that the polynomials are dense in C[X] under the sup norm, where X is any Hausdorff space. There have been several previous posts asking about approximations of with polynomials is a polynomial of degree less than or equal to for a fixed degree . It seems like this is a problem that has certain bounds for , where is on the order ( 1 , 2 ) I have two questions: If we restrict the output of , are there any tighter bounds that can be said? Clearly, for the trivial case . Can we prove the same bounds for and a multi-variate polynomial as we did in the one-dimensional case? What if we restrict the range to ? I am not sure how one would go about the first question, other than to note that is 1-Lipshitz and by the above posts there are potentially some tighter bounds there for Lipschitz functions. For the second question, I showed that the polynomials are still dense in , which makes me think there might be an analgous way to prove the error bounds as well? This is probably all well-understood, but I'm not well-read on approximation theory. Any guidance would be wonderful.","f \in C[X] P_n = \{p | p  n\} n \inf_{p \in P_n} ||f - p||_\infty < \epsilon(n) \epsilon(n) O(1/n) f: [-a, b] \rightarrow [0,1] \inf_{p \in P_n} ||f - p||_\infty < 1/2 p=1/2 f: [-a, b]^n -> \mathbb{R} P(x_1, ... x_n) f: [-a, b]^n -> [0,1] f C[X]","['real-analysis', 'measure-theory', 'polynomials', 'reference-request', 'approximation-theory']"
82,Measure Induced by Integral is Regular if Original Measure Regular,Measure Induced by Integral is Regular if Original Measure Regular,,"I am trying to prove the following question: Let $(E,\mathcal{A},\mu)$ be measure space, $g$ positive measurable function. Then we have that $\nu := \int_{A}gd\mu$ is a positive measure on $\mathcal{A}$ . Suppose $E$ is now locally compact and Hausdorff and that open sets of $E$ are $\sigma$ - compact. Suppose also that $\mathcal{A}$ is the Borel $\sigma$ -algebra, $\mu$ is a regular measure, and $g\in L^p(\mu)$ for some $p\in [1,\infty]$ . Then show that $\nu$ is regular. As for this question, my attempt is as follows, and yet I am not sure if it is correct. Let $V$ be an open set, $V = \cup_{n\ge 1} H_n$ , where $H_n$ are compact set. Then, let $H_1\prec f_1\prec V$ . Denote $K_i$ as the support for $f_i$ and define $H_1\cup \dots\cup H_n \cup K_1 \cup\dots \cup H_n \prec f_{n+1}\prec V$ . Then $(f_n)$ is increasing sequence and its limit is $\mathbb{1}_{V}$ . Now, by monotone convergence, if q is the conjugate component of $p$ $$\nu(V) = \lim_{n\to \infty} \int f_n d\nu = \lim \int f_ng\ d\mu = \int g \mathbb{1}_V \ d\mu \leq \|g\|_p \left(\int \mathbb{1}_V \ d\mu\right)^{1/q} = \|g\|_p (\nu(V))^{1/q}$$ Now let $A\in \mathcal{A}$ be a Borel set, $\epsilon >0$ , then by regularity of $\mu$ , $\exists K$ compact, $V$ open s.t. $K\subset A \subset V$ and $\nu(V\setminus K) < \epsilon$ . Note that compact sets are closed, so $V\setminus K$ is open, then $\nu(V\setminus K) \leq \|g\|_p (\nu(V\setminus K))^{1/q} < \|g\|_p \epsilon ^{1/q}$ , and this gives the regularity of $\nu$ . I do realize that there may be flaws in this proof and I would really appreciate it if someone could help. Thanks!","I am trying to prove the following question: Let be measure space, positive measurable function. Then we have that is a positive measure on . Suppose is now locally compact and Hausdorff and that open sets of are - compact. Suppose also that is the Borel -algebra, is a regular measure, and for some . Then show that is regular. As for this question, my attempt is as follows, and yet I am not sure if it is correct. Let be an open set, , where are compact set. Then, let . Denote as the support for and define . Then is increasing sequence and its limit is . Now, by monotone convergence, if q is the conjugate component of Now let be a Borel set, , then by regularity of , compact, open s.t. and . Note that compact sets are closed, so is open, then , and this gives the regularity of . I do realize that there may be flaws in this proof and I would really appreciate it if someone could help. Thanks!","(E,\mathcal{A},\mu) g \nu := \int_{A}gd\mu \mathcal{A} E E \sigma \mathcal{A} \sigma \mu g\in L^p(\mu) p\in [1,\infty] \nu V V = \cup_{n\ge 1} H_n H_n H_1\prec f_1\prec V K_i f_i H_1\cup \dots\cup H_n \cup K_1 \cup\dots \cup H_n \prec f_{n+1}\prec V (f_n) \mathbb{1}_{V} p \nu(V) = \lim_{n\to \infty} \int f_n d\nu = \lim \int f_ng\ d\mu = \int g \mathbb{1}_V \ d\mu \leq \|g\|_p \left(\int \mathbb{1}_V \ d\mu\right)^{1/q} = \|g\|_p (\nu(V))^{1/q} A\in \mathcal{A} \epsilon >0 \mu \exists K V K\subset A \subset V \nu(V\setminus K) < \epsilon V\setminus K \nu(V\setminus K) \leq \|g\|_p (\nu(V\setminus K))^{1/q} < \|g\|_p \epsilon ^{1/q} \nu","['measure-theory', 'lebesgue-integral']"
83,Can the Borel sets be recovered from the Lebesgue sets and the null sets?,Can the Borel sets be recovered from the Lebesgue sets and the null sets?,,"Suppose that one is given a set $X$ equipped with a pair of $\sigma$ -algebras $\mathcal{L}$ and $\mathcal{N}$ . Suppose that $\mathcal{L}$ and $\mathcal{N}$ came from putting a nice topology on $X$ , then putting a nice measure on the Borel sets $\mathcal{B}$ of $X$ , then letting $\mathcal{L}$ be the completion of $\mathcal{B}$ , then letting $\mathcal{N}$ be the measure-zero sets in $\mathcal{L}$ , then forgetting the measure, then forgetting the topology. Is there any hope that one could recover $\mathcal{B}$ from $\mathcal{L}$ and $\mathcal{N}$ ? If the topology is not assumed to be nice, then the answer is no, because for example the set of all usual Lebesgue subsets of $\mathbb{R}$ is the set of all Borel subsets of $\mathbb{R}$ when $\mathbb{R}$ is equipped with the weird topology referenced here .","Suppose that one is given a set equipped with a pair of -algebras and . Suppose that and came from putting a nice topology on , then putting a nice measure on the Borel sets of , then letting be the completion of , then letting be the measure-zero sets in , then forgetting the measure, then forgetting the topology. Is there any hope that one could recover from and ? If the topology is not assumed to be nice, then the answer is no, because for example the set of all usual Lebesgue subsets of is the set of all Borel subsets of when is equipped with the weird topology referenced here .",X \sigma \mathcal{L} \mathcal{N} \mathcal{L} \mathcal{N} X \mathcal{B} X \mathcal{L} \mathcal{B} \mathcal{N} \mathcal{L} \mathcal{B} \mathcal{L} \mathcal{N} \mathbb{R} \mathbb{R} \mathbb{R},['measure-theory']
84,What is the distance from $G$ to centre of gravity set $S$?,What is the distance from  to centre of gravity set ?,G S,"Let: $$G=\left\{x\in \mathbb R^n: x=(x_1,...,x_{n-1},0) \right\},$$ $S$ - cone based on the set $A\subset G$ which is bounded set, $$A'=\left\{x'\in \mathbb R^{n-1}: x'=(x_1,...,x_{n-1}) \right\}, \lambda_{n-1}(A')<\infty$$ Moreover assume that the tip of the cone $S$ lies at a distance $g$ from $G$ . What is the distance from $G$ to centre of gravity set $S$ ? If $A\subset \mathbb R^n$ is a measurable set then the point $x=(x_1,...,x_n)$ where $x_i=\frac{1}{\lambda_n(A)} \int_A y_i d \lambda(y_1,y_2,...,y_n)$ is the centre of gravity. But I don't know how to use this knowledge to do this task. Can you help me?","Let: - cone based on the set which is bounded set, Moreover assume that the tip of the cone lies at a distance from . What is the distance from to centre of gravity set ? If is a measurable set then the point where is the centre of gravity. But I don't know how to use this knowledge to do this task. Can you help me?","G=\left\{x\in \mathbb R^n: x=(x_1,...,x_{n-1},0) \right\}, S A\subset G A'=\left\{x'\in \mathbb R^{n-1}: x'=(x_1,...,x_{n-1}) \right\}, \lambda_{n-1}(A')<\infty S g G G S A\subset \mathbb R^n x=(x_1,...,x_n) x_i=\frac{1}{\lambda_n(A)} \int_A y_i d \lambda(y_1,y_2,...,y_n)","['real-analysis', 'integration', 'measure-theory']"
85,$L^{p}$ spaces for complex measures,spaces for complex measures,L^{p},"Let $(\Omega,\mathscr{A})$ be a measurable space and $\mu\colon\mathscr{A}\to\mathbb{C}$ a complex measure (i.e. a $\sigma$ -additive function from $\mathscr{A}$ to $\mathbb{C}$ ). Then $\mu$ can be decomposed into a (unique) real and imaginary part. In other words, there are (finite) signed measures $\mu_{1},\mu_{i}\colon\mathscr{A}\to\mathbb{R}$ such that $\mu=\mu_{1}+i\mu_{i}$ . The Jordan decomposition lets us split the real and imaginary part of $\mu$ into finite positive measures $\mu_{1}^{+},\mu_{1}^{-},\mu_{i}^{+},\mu_{i}^{-}\colon\mathscr{A}\to\mathbb{R}_{\geq0}$ such that $\mu_{1}=\mu_{1}^{+}-\mu_{1}^{-}$ and $\mu_{i}=\mu_{i}^{+}-\mu_{i}^{-}$ . So we have $$\mu=(\mu_{1}^{+}-\mu_{1}^{-})+i(\mu_{i}^{+}-\mu_{i}^{-}).$$ Then a measurable complex valued function $f\colon\Omega\to\mathbb{C}$ is called integrable with respect to the complex measure $\mu$ if and only if the real and imaginary part of $f$ are both integrable with respect to the measures $\mu_{1}^{+}$ , $\mu_{1}^{-}$ , $\mu_{i}^{+}$ and $\mu_{i}^{-}$ . If $f=f_{1}+if_{i}$ (with $f_{1}$ and $f_{i}$ real-valued) is integrable with respect to $\mu$ , then its integral is defined to be \begin{align*}\int_{\Omega}f \ \text{d}\mu&:=\int_{\Omega}f_{1} \ \text{d}\mu_{1}^{+}-\int_{\Omega}f_{1} \ \text{d}\mu_{1}^{-}+i\int_{\Omega}f_{1} \ \text{d}\mu_{i}^{+}-i\int_{\Omega}f_{1} \ \text{d}\mu_{i}^{-}\\ &=\int_{\Omega}f_{i} \ \text{d}\mu_{1}^{+}-\int_{\Omega}f_{i} \ \text{d}\mu_{1}^{-}+i\int_{\Omega}f_{i} \ \text{d}\mu_{i}^{+}-i\int_{\Omega}f_{i} \ \text{d}\mu_{i}^{-}. \end{align*} I was studying complex measures, but the concept of $L^{p}$ spaces is very unclear to me in this case. I can't find any proper literature about it either. Can someone explain to me what is going on here? I have a few questions in particular: Can we make sense of the space $L^{p}(\Omega,\mathscr{A},\mu)$ for $1\leq p\leq\infty$ ? Can we define a $p$ -norm on this space? Is this norm also complete? In the case $p=2$ , do we still have a (complete) inner-product space? I think the total variation $|\mu|$ (which is a finite positive measure) of a complex measure $\mu$ has something to do with it, but I can't figure it out the details myself. Any help is greatly appreciated!","Let be a measurable space and a complex measure (i.e. a -additive function from to ). Then can be decomposed into a (unique) real and imaginary part. In other words, there are (finite) signed measures such that . The Jordan decomposition lets us split the real and imaginary part of into finite positive measures such that and . So we have Then a measurable complex valued function is called integrable with respect to the complex measure if and only if the real and imaginary part of are both integrable with respect to the measures , , and . If (with and real-valued) is integrable with respect to , then its integral is defined to be I was studying complex measures, but the concept of spaces is very unclear to me in this case. I can't find any proper literature about it either. Can someone explain to me what is going on here? I have a few questions in particular: Can we make sense of the space for ? Can we define a -norm on this space? Is this norm also complete? In the case , do we still have a (complete) inner-product space? I think the total variation (which is a finite positive measure) of a complex measure has something to do with it, but I can't figure it out the details myself. Any help is greatly appreciated!","(\Omega,\mathscr{A}) \mu\colon\mathscr{A}\to\mathbb{C} \sigma \mathscr{A} \mathbb{C} \mu \mu_{1},\mu_{i}\colon\mathscr{A}\to\mathbb{R} \mu=\mu_{1}+i\mu_{i} \mu \mu_{1}^{+},\mu_{1}^{-},\mu_{i}^{+},\mu_{i}^{-}\colon\mathscr{A}\to\mathbb{R}_{\geq0} \mu_{1}=\mu_{1}^{+}-\mu_{1}^{-} \mu_{i}=\mu_{i}^{+}-\mu_{i}^{-} \mu=(\mu_{1}^{+}-\mu_{1}^{-})+i(\mu_{i}^{+}-\mu_{i}^{-}). f\colon\Omega\to\mathbb{C} \mu f \mu_{1}^{+} \mu_{1}^{-} \mu_{i}^{+} \mu_{i}^{-} f=f_{1}+if_{i} f_{1} f_{i} \mu \begin{align*}\int_{\Omega}f \ \text{d}\mu&:=\int_{\Omega}f_{1} \ \text{d}\mu_{1}^{+}-\int_{\Omega}f_{1} \ \text{d}\mu_{1}^{-}+i\int_{\Omega}f_{1} \ \text{d}\mu_{i}^{+}-i\int_{\Omega}f_{1} \ \text{d}\mu_{i}^{-}\\
&=\int_{\Omega}f_{i} \ \text{d}\mu_{1}^{+}-\int_{\Omega}f_{i} \ \text{d}\mu_{1}^{-}+i\int_{\Omega}f_{i} \ \text{d}\mu_{i}^{+}-i\int_{\Omega}f_{i} \ \text{d}\mu_{i}^{-}.
\end{align*} L^{p} L^{p}(\Omega,\mathscr{A},\mu) 1\leq p\leq\infty p p=2 |\mu| \mu","['measure-theory', 'lebesgue-integral', 'lp-spaces', 'complex-integration', 'total-variation']"
86,Evaulate $\lim_{n \to \infty} \int_0^\infty ne^{-nx} \sin(1/x)dx$,Evaulate,\lim_{n \to \infty} \int_0^\infty ne^{-nx} \sin(1/x)dx,"Now, I know that the question was answered here: $\int_0^\infty ne^{-nx}\sin\left(\frac1{x}\right)\;dx\to ?$ as $n\to\infty$ But I'm looking for more of a measure theoretical approach, so if someone can point me in the right direction, I would be grateful. (i.e. use of Dominated Convergence Theorem or Monotone Convergence Theorem) My rough idea has been the following: We can't use MCT because the functions are not pointwise increasing.  We can't use DCT since there there is no function to bound $ne^{-nx} sin(1/x)$ . But, it seems that on $[\ln 2, \infty)$ , $e^{-x} \geq ne^{-nx}$ , and on $[\ln (\frac{n+1}{n}, \ln \frac{n}{n-1}]$ , $ne^{-nx} \geq me^{-mx}$ for all $n, m \in \mathbb{N}$ (I don't have proof of this, and it seems hard to prove). So we can use DCT on integral restricted such intervals. That is, we can use DCT on the integral $$\int_{\mathbb{R}} ne^{-nx} \sin(\frac{1}{x}) \chi_{[\ln(n+1/n), \ln (n/n-1)]}dx$$ And on these intervals, the integrals are all 0. Since these intervals partitions $[0, \infty)$ , it follows our integral is 0. I don't know if the idea is correct, and even if it was, it wouldn't be an elegant solution. So I'm hoping someone could point me in the right direction for a clean solution. Thanks! EDIT: If we let $u = nx$ , we get the integral $$\int^\infty_0 e^{-u} \sin(\frac{n}{u}) du$$ . We could apply DCT to this to get 0? Another EDIT: But $\sin(\frac{n}{u})$ is not convergent, so we cannot apply DCT.","Now, I know that the question was answered here: $\int_0^\infty ne^{-nx}\sin\left(\frac1{x}\right)\;dx\to ?$ as $n\to\infty$ But I'm looking for more of a measure theoretical approach, so if someone can point me in the right direction, I would be grateful. (i.e. use of Dominated Convergence Theorem or Monotone Convergence Theorem) My rough idea has been the following: We can't use MCT because the functions are not pointwise increasing.  We can't use DCT since there there is no function to bound . But, it seems that on , , and on , for all (I don't have proof of this, and it seems hard to prove). So we can use DCT on integral restricted such intervals. That is, we can use DCT on the integral And on these intervals, the integrals are all 0. Since these intervals partitions , it follows our integral is 0. I don't know if the idea is correct, and even if it was, it wouldn't be an elegant solution. So I'm hoping someone could point me in the right direction for a clean solution. Thanks! EDIT: If we let , we get the integral . We could apply DCT to this to get 0? Another EDIT: But is not convergent, so we cannot apply DCT.","ne^{-nx} sin(1/x) [\ln 2, \infty) e^{-x} \geq ne^{-nx} [\ln (\frac{n+1}{n}, \ln \frac{n}{n-1}] ne^{-nx} \geq me^{-mx} n, m \in \mathbb{N} \int_{\mathbb{R}} ne^{-nx} \sin(\frac{1}{x}) \chi_{[\ln(n+1/n), \ln (n/n-1)]}dx [0, \infty) u = nx \int^\infty_0 e^{-u} \sin(\frac{n}{u}) du \sin(\frac{n}{u})","['integration', 'measure-theory', 'lebesgue-integral']"
87,Space of positive measures Polish for a weak-star topology?,Space of positive measures Polish for a weak-star topology?,,"Let $X$ be a second-countable locally compact Hausdorff space. Denote by $C_c(X) \subseteq C_0(X) \subseteq C_b(X)$ the spaces of continuous functions with compact support, vanishing at infinity and those that are bounded respectively. Denote by $M(X) = C_0(X)' = C_c(X)'$ the space of signed Radon measures on $X$ , where $C_0(X)$ or $C_c(X)$ carry the supremum norm ( $C_c$ is dense in $C_0$ ). On $M(X)$ we can consider the weak $^*$ topologies $w^*_c \subseteq w^*_0 \subseteq w^*_b$ relative to $C_c$ , $C_0$ and $C_b$ . On the set of positive Radon measures $M^+(X)$ , the topologies $w^*_c$ and $w^*_b$ are Polish (i.e. completely metrizable and separable). A compatible complete metric for $w^*_b$ on $M^+$ is given by the Kantorovich-Rubinstein norm [Bogachev, ""Measure Theory II"", Theorem 8.3.2]. A compatible complete metric for $w^*_c$ on $M^+$ is typically constructed from a countable dense subset $\{ f_n \mid n \in \mathbb{N} \} \subseteq C_c(X)$ ( $C_0$ and $C_c$ are separable), e.g. $d(\mu, \nu) = \sum_{n=1}^\infty \frac{1}{2^n} min(1, |\int f_n d\mu - \int f_n d\nu|)$ . In the literature on measure and probablity theory I mostly find discussions around these two topologies ( $w^*_c$ is called the vague topology and $w^*_b$ the narrow topology). Of course, $w^*_0 = w^*_c$ on norm-bounded sets of $M(X)$ , but the topology $w^*_0$ seems to be more natural on $M(X)$ , simply because $(M(X), w^*_0)$ is the weak $^*$ -dual of a Banach space ( $C_0$ ); $C_c$ is only a normed space. On $M^+(X)$ these three topologies are generally different. For instance, if $X = \mathbb{N}$ , then (i) $n \delta_n \to 0$ for $w^*_c$ but not for $w^*_0$ ( $n \delta_n$ is not norm bounded) and (ii) $\delta_n \to 0$ for $w^*_0$ but not for $w^*_b$ ( $\delta_n$ is not tight). Questions: Is the $w^*_0$ topology also Polish on $M^+(X)$ ? Can anyone provide a reference for properties of this topology (on all of $M(X)$ or $M^+(X)$ )? [Note that the one-point compactification $X \to X_\infty$ induces an embedding of $M^+(X)$ as a closed subset of the Polish space $(M^+(X_\infty), w^*)$ with $w^* = \sigma(M(X_\infty), C(X_\infty))$ , but the subspace topology induced on $M^+(X)$ is finer than $w^*_0$ (it can be shown that $w^* = w^*_b$ on $M^+(X)$ ).]","Let be a second-countable locally compact Hausdorff space. Denote by the spaces of continuous functions with compact support, vanishing at infinity and those that are bounded respectively. Denote by the space of signed Radon measures on , where or carry the supremum norm ( is dense in ). On we can consider the weak topologies relative to , and . On the set of positive Radon measures , the topologies and are Polish (i.e. completely metrizable and separable). A compatible complete metric for on is given by the Kantorovich-Rubinstein norm [Bogachev, ""Measure Theory II"", Theorem 8.3.2]. A compatible complete metric for on is typically constructed from a countable dense subset ( and are separable), e.g. . In the literature on measure and probablity theory I mostly find discussions around these two topologies ( is called the vague topology and the narrow topology). Of course, on norm-bounded sets of , but the topology seems to be more natural on , simply because is the weak -dual of a Banach space ( ); is only a normed space. On these three topologies are generally different. For instance, if , then (i) for but not for ( is not norm bounded) and (ii) for but not for ( is not tight). Questions: Is the topology also Polish on ? Can anyone provide a reference for properties of this topology (on all of or )? [Note that the one-point compactification induces an embedding of as a closed subset of the Polish space with , but the subspace topology induced on is finer than (it can be shown that on ).]","X C_c(X) \subseteq C_0(X) \subseteq C_b(X) M(X) = C_0(X)' = C_c(X)' X C_0(X) C_c(X) C_c C_0 M(X) ^* w^*_c \subseteq w^*_0 \subseteq w^*_b C_c C_0 C_b M^+(X) w^*_c w^*_b w^*_b M^+ w^*_c M^+ \{ f_n \mid n \in \mathbb{N} \} \subseteq C_c(X) C_0 C_c d(\mu, \nu) = \sum_{n=1}^\infty \frac{1}{2^n} min(1, |\int f_n d\mu - \int f_n d\nu|) w^*_c w^*_b w^*_0 = w^*_c M(X) w^*_0 M(X) (M(X), w^*_0) ^* C_0 C_c M^+(X) X = \mathbb{N} n \delta_n \to 0 w^*_c w^*_0 n \delta_n \delta_n \to 0 w^*_0 w^*_b \delta_n w^*_0 M^+(X) M(X) M^+(X) X \to X_\infty M^+(X) (M^+(X_\infty), w^*) w^* = \sigma(M(X_\infty), C(X_\infty)) M^+(X) w^*_0 w^* = w^*_b M^+(X)","['functional-analysis', 'measure-theory', 'reference-request', 'descriptive-set-theory']"
88,Show $[fg]_1\leq p^{\frac{1}{p}}(p')^{(\frac{1}{p'})}[f]_p[g]_{p'}$ in weak $L^P$ norm,Show  in weak  norm,[fg]_1\leq p^{\frac{1}{p}}(p')^{(\frac{1}{p'})}[f]_p[g]_{p'} L^P,"Let $f: X \rightarrow \mathbb{R}$ be a measurable function where $(X,\mu)$ is a measure space and say that $f \in L^{p,\infty}$ $\iff$ $[f]_p < \infty$ where $[f]_p   =  \sup_{t>0} t \mu(\{x : |f(x)| > t \})^{\frac{1}{p}} < \infty$ . In my measure theory class we are calling $[*]_p$ the weak $L^p$ norm. Now then, suppose $1<p<\infty$ and let $p'$ be the dual exponent of $p$ . Prove that: $[fg]_1\leq p^{\frac{1}{p}}(p')^{(\frac{1}{p'})}[f]_p[g]_{p'}$ My attempt: I'm first trying to reduce the case to where $[f]_p=[g]_p=1$ . I've also noticed that I can prove the above inquality if $p^{\frac{1}{p}}(p')^{(\frac{1}{p'})}=2$ , and to get the constant $p^{\frac{1}{p}}(p')^{(\frac{1}{p'})}$ I was going to optimize the function $h(x,y)=x^{-p}+y^{-p}$ subject to the constraint $xy= \lambda$ ... But I don't know maybe i'm going about this all wrong... Insight appreciated!","Let be a measurable function where is a measure space and say that where . In my measure theory class we are calling the weak norm. Now then, suppose and let be the dual exponent of . Prove that: My attempt: I'm first trying to reduce the case to where . I've also noticed that I can prove the above inquality if , and to get the constant I was going to optimize the function subject to the constraint ... But I don't know maybe i'm going about this all wrong... Insight appreciated!","f: X \rightarrow \mathbb{R} (X,\mu) f \in L^{p,\infty} \iff [f]_p < \infty [f]_p   =  \sup_{t>0} t \mu(\{x : |f(x)| > t \})^{\frac{1}{p}} < \infty [*]_p L^p 1<p<\infty p' p [fg]_1\leq p^{\frac{1}{p}}(p')^{(\frac{1}{p'})}[f]_p[g]_{p'} [f]_p=[g]_p=1 p^{\frac{1}{p}}(p')^{(\frac{1}{p'})}=2 p^{\frac{1}{p}}(p')^{(\frac{1}{p'})} h(x,y)=x^{-p}+y^{-p} xy= \lambda",['real-analysis']
89,Does the Perron-Frobenius operator preserve continuity?,Does the Perron-Frobenius operator preserve continuity?,,"Let $(X,\mu)$ be a $\sigma$ -finite measure space. For $f\in L^{1}(X,\mu)$ , define a signed measure $\nu_{f}$ on $X$ by $$\nu_{f}(A):=\int_{T^{-1}(A)}f \ d\mu$$ for all measurable $A\subset X$ . Then $\nu_{f}$ is finite, so by the Radon-Nikodym theorem, there exists a unique (up to $\mu$ -a.e. equivalence) element $P_{T}f\in L^{1}(X,\mu)$ such that $$\nu_{f}(A)=\int_{A}P_{T}f \ d\mu$$ for all measurable $A\subset X$ . The operator $P_{T}\colon L^{1}(X,\mu)\to L^{1}(X,\mu)$ is called the Perron-Frobenius operator . One can show that $P_{T}$ is a linear contraction w.r.t. $\|\cdot\|_{1}$ . My question: Suppose that $X=[0,1]$ , equipped with the Borel sigma algebra and Lebesgue measure. Let $T\colon X\to X$ be a homeomorphism (or, if needed, something stronger). Suppose that $f\colon[0,1]\to\mathbb{R}$ is continuous. Is it then true that $P_{T}f$ is continuous ( $\mu$ -a.e.)? I tried to use density of $C[0,1]$ in $L^{1}[0,1]$ , but I'm not sure if that is the way to go. Maybe there is an easier way? Any suggestions are greatly appreciated.","Let be a -finite measure space. For , define a signed measure on by for all measurable . Then is finite, so by the Radon-Nikodym theorem, there exists a unique (up to -a.e. equivalence) element such that for all measurable . The operator is called the Perron-Frobenius operator . One can show that is a linear contraction w.r.t. . My question: Suppose that , equipped with the Borel sigma algebra and Lebesgue measure. Let be a homeomorphism (or, if needed, something stronger). Suppose that is continuous. Is it then true that is continuous ( -a.e.)? I tried to use density of in , but I'm not sure if that is the way to go. Maybe there is an easier way? Any suggestions are greatly appreciated.","(X,\mu) \sigma f\in L^{1}(X,\mu) \nu_{f} X \nu_{f}(A):=\int_{T^{-1}(A)}f \ d\mu A\subset X \nu_{f} \mu P_{T}f\in L^{1}(X,\mu) \nu_{f}(A)=\int_{A}P_{T}f \ d\mu A\subset X P_{T}\colon L^{1}(X,\mu)\to L^{1}(X,\mu) P_{T} \|\cdot\|_{1} X=[0,1] T\colon X\to X f\colon[0,1]\to\mathbb{R} P_{T}f \mu C[0,1] L^{1}[0,1]","['measure-theory', 'continuity', 'lebesgue-measure', 'lp-spaces', 'radon-nikodym']"
90,Help with an exercise in analysis and measure theory.,Help with an exercise in analysis and measure theory.,,"Let $(f_ {n})$ be a sequence of functions from $\mathbb{R}$ to $\mathbb{R}$ . Prove that $\displaystyle\{x\in X:(f_{n}(x))\text{ converges in}\ \mathbb{R}\}=\bigcap_{k=1}^{\infty} \bigcup_{n=1}^{\infty} \bigcap_{p=1}^{\infty} \{x\in X:|f_{n}(x)-f_{n+p}(x)|<\frac{1}{k}\}$ My attempt was $(f_{n}(x))\text{ converges}$ iff $\forall \epsilon>0:\exists n\in \mathbb{N}:\forall m\in \mathbb{N}: m>n\rightarrow |f_n(x)-f_m(x)|<\epsilon$ iff $\forall k\in \mathbb{N}:\exists n\in \mathbb{N}:\forall p\in \mathbb{N}:|f_n(x)-f_{n+p}(x)|<\frac{1}{k}$ iff $\displaystyle x\in\bigcap_{k=1}^{\infty} \bigcup_{n=1}^{\infty} \bigcap_{p=1}^{\infty} \{x\in X:|f_{n}(x)-f_{n+p}(x)|<\frac{1}{k}\}$ I did this, but my teacher told me that I was not demonstrating anything with this. It's wrong? I only used the Cauchy criteria and the archimedean property.","Let be a sequence of functions from to . Prove that My attempt was iff iff iff I did this, but my teacher told me that I was not demonstrating anything with this. It's wrong? I only used the Cauchy criteria and the archimedean property.",(f_ {n}) \mathbb{R} \mathbb{R} \displaystyle\{x\in X:(f_{n}(x))\text{ converges in}\ \mathbb{R}\}=\bigcap_{k=1}^{\infty} \bigcup_{n=1}^{\infty} \bigcap_{p=1}^{\infty} \{x\in X:|f_{n}(x)-f_{n+p}(x)|<\frac{1}{k}\} (f_{n}(x))\text{ converges} \forall \epsilon>0:\exists n\in \mathbb{N}:\forall m\in \mathbb{N}: m>n\rightarrow |f_n(x)-f_m(x)|<\epsilon \forall k\in \mathbb{N}:\exists n\in \mathbb{N}:\forall p\in \mathbb{N}:|f_n(x)-f_{n+p}(x)|<\frac{1}{k} \displaystyle x\in\bigcap_{k=1}^{\infty} \bigcup_{n=1}^{\infty} \bigcap_{p=1}^{\infty} \{x\in X:|f_{n}(x)-f_{n+p}(x)|<\frac{1}{k}\},"['real-analysis', 'measure-theory', 'solution-verification']"
91,Does a function being uniformly expanding imply it is exact?,Does a function being uniformly expanding imply it is exact?,,"I will begin this question by mentioning what the terms mean in the statement. I have a probability space $(X,\mathcal{B},\mu)$ and a measurable map $T:X\to X$ . I assume that $T$ is bijective, and $T^{-1}$ is also measurable. Suppose in addition that $X$ is a bounded metric space with metric $d$ . By uniformly expanding, I mean there is a constant $C>1$ such that $d(Tx,Ty)\ge C d(x,y)$ for all $x,y\in X$ . Exactness has two formulations in ergodic theory; I will state them both. Firstly, we say that a nonsingular map $T$ is exact if $A\in \bigcap_{n\ge 0}T^{-n}(\mathcal{B})$ implies $\mu(A)\in \{0,1\}$ . If $T$ is measure-preserving, then $T$ is exact if one has $T(A)\in \mathcal{B}$ for all $A\in \mathcal{B}$ , and if $\mu(A)>0$ , then $\lim_{n\to \infty}\mu(T^nA)=1$ . Now that the definitions have been stated, is it true that if $T:X\to X$ is as described in the first paragraph, then it is exact? To me, it seems as if it should be true intuitively if considering $X$ to be some interval and $\mu$ to be the Lebesgue measure (from the second definition of exactness), however I am unsure if it is true in the generality above ( $X$ bounded metric space, $\mu$ a probability measure). Thanks in advance!","I will begin this question by mentioning what the terms mean in the statement. I have a probability space and a measurable map . I assume that is bijective, and is also measurable. Suppose in addition that is a bounded metric space with metric . By uniformly expanding, I mean there is a constant such that for all . Exactness has two formulations in ergodic theory; I will state them both. Firstly, we say that a nonsingular map is exact if implies . If is measure-preserving, then is exact if one has for all , and if , then . Now that the definitions have been stated, is it true that if is as described in the first paragraph, then it is exact? To me, it seems as if it should be true intuitively if considering to be some interval and to be the Lebesgue measure (from the second definition of exactness), however I am unsure if it is true in the generality above ( bounded metric space, a probability measure). Thanks in advance!","(X,\mathcal{B},\mu) T:X\to X T T^{-1} X d C>1 d(Tx,Ty)\ge C d(x,y) x,y\in X T A\in \bigcap_{n\ge 0}T^{-n}(\mathcal{B}) \mu(A)\in \{0,1\} T T T(A)\in \mathcal{B} A\in \mathcal{B} \mu(A)>0 \lim_{n\to \infty}\mu(T^nA)=1 T:X\to X X \mu X \mu","['measure-theory', 'dynamical-systems', 'ergodic-theory']"
92,Stochastic integral is progressive,Stochastic integral is progressive,,"If I want to show that a stochastic integral, namely $$Y_t=\int_0^t\phi_s dB_s$$ where B is the standard Brownian motion, is progressive, is it enough to show that the integrand $\phi_s$ it is progressive? Could you provide some comments?","If I want to show that a stochastic integral, namely where B is the standard Brownian motion, is progressive, is it enough to show that the integrand it is progressive? Could you provide some comments?",Y_t=\int_0^t\phi_s dB_s \phi_s,"['measure-theory', 'stochastic-processes', 'brownian-motion', 'stochastic-integrals']"
93,Generated $\sigma$-algebras with cylinder set doesn't contain the space of continuous functions,Generated -algebras with cylinder set doesn't contain the space of continuous functions,\sigma,"Consider $\mathbb R^{[0,1]}$ the space of all functions from $[0,1]$  to $\mathbb R$  and the cylindrical sigma algebra $\mathcal B$  on it. The question is: how to prove that $C[0,1]\notin \mathcal B$.","Consider $\mathbb R^{[0,1]}$ the space of all functions from $[0,1]$  to $\mathbb R$  and the cylindrical sigma algebra $\mathcal B$  on it. The question is: how to prove that $C[0,1]\notin \mathcal B$.",,"['measure-theory', 'elementary-set-theory', 'stochastic-processes']"
94,$\phi:\mathbb{R}^n\rightarrow\mathbb{R}$ with a given condition is measurable.,with a given condition is measurable.,\phi:\mathbb{R}^n\rightarrow\mathbb{R},"$\phi:\mathbb{R}^n\rightarrow\mathbb{R}$ is continuous at all points in $\mathbb{R}^n-Y$ with $Y\subset\mathbb{R}^n$ of measure zero. Then $\phi$ is measurable. In the problem, the measure is the Lebesgue measure. Proof) Let $c\in\mathbb{R}$ . We prove $E:=\{x\in\mathbb{R}^n:\phi(x)>c\}=(E-Y)\cup(E\cap Y)$ is measurable. First, $(E\cap Y)\subset Y$ , so $m^*(E\cap Y)\leq m^*(Y)=0$ and $E\cap Y$ is measurable as it is measure zero. Next, for all $x\in E-Y$ , $\phi$ is continuous at $x$ , so $\exists\delta>0$ such that $B_{\delta}(x)\subset E$ . Then $E-Y=(\bigcup\limits_{x\in E-Y}B_{\delta}(x))-Y=(\bigcup\limits_{x_n\in E-Y}B_{\delta}(x_n))-Y$ . For the last equality, we used the Lindeloff covering theorem to take countable balls. Then $E-Y$ is measurable as open sets are measurable, the union of countably many open sets is measurable, $Y$ is measurbale and the difference bewteen measurable sets is measurable. I was wondering if this solutions is correct.","is continuous at all points in with of measure zero. Then is measurable. In the problem, the measure is the Lebesgue measure. Proof) Let . We prove is measurable. First, , so and is measurable as it is measure zero. Next, for all , is continuous at , so such that . Then . For the last equality, we used the Lindeloff covering theorem to take countable balls. Then is measurable as open sets are measurable, the union of countably many open sets is measurable, is measurbale and the difference bewteen measurable sets is measurable. I was wondering if this solutions is correct.",\phi:\mathbb{R}^n\rightarrow\mathbb{R} \mathbb{R}^n-Y Y\subset\mathbb{R}^n \phi c\in\mathbb{R} E:=\{x\in\mathbb{R}^n:\phi(x)>c\}=(E-Y)\cup(E\cap Y) (E\cap Y)\subset Y m^*(E\cap Y)\leq m^*(Y)=0 E\cap Y x\in E-Y \phi x \exists\delta>0 B_{\delta}(x)\subset E E-Y=(\bigcup\limits_{x\in E-Y}B_{\delta}(x))-Y=(\bigcup\limits_{x_n\in E-Y}B_{\delta}(x_n))-Y E-Y Y,"['real-analysis', 'measure-theory']"
95,"Integral with dm(x,y) with respect to measure: what is the difference between $\int f(x)dx$ and $\int f(x) dm(x)$?","Integral with dm(x,y) with respect to measure: what is the difference between  and ?",\int f(x)dx \int f(x) dm(x),"What is the difference between $\int f(x)dx$ and $\int f(x) dm(x)$ ? In an exercise I was asked to compute $\int\limits_{[0,1]\times[0,1]}xe^{xy}dm(x,y)$ and I did it using the definition $\int f(x,y)dm(x,y)=\int\limits_0^\infty m(\{(x,y): f(x,y)>t\})dt$ but then the TA told me that $dm(x,y)$ is the same as $d\vec{(x,y)}$ and by using Tonelli (since $f(x,y)>0$ in this case) the integral is simply $\int\limits_0^1\int\limits_0^1 f(x,y)dydx$ So my question is : why do we use two different notations for the same thing? And what exactly is $dm(x,y)$ ?",What is the difference between and ? In an exercise I was asked to compute and I did it using the definition but then the TA told me that is the same as and by using Tonelli (since in this case) the integral is simply So my question is : why do we use two different notations for the same thing? And what exactly is ?,"\int f(x)dx \int f(x) dm(x) \int\limits_{[0,1]\times[0,1]}xe^{xy}dm(x,y) \int f(x,y)dm(x,y)=\int\limits_0^\infty m(\{(x,y): f(x,y)>t\})dt dm(x,y) d\vec{(x,y)} f(x,y)>0 \int\limits_0^1\int\limits_0^1 f(x,y)dydx dm(x,y)","['integration', 'measure-theory', 'lebesgue-integral']"
96,Can we resolve this issue of applying the Lagrange multiplier theorem?,Can we resolve this issue of applying the Lagrange multiplier theorem?,,"Let $I$ be a finite nonempty set, $p,q_i$ be probability densities on a measurable space $(E,\mathcal E,\lambda)$ for $i\in I$ , $w_i:E\to[0,1]$ be measurable with $\{q_i=0\}\subseteq\{w_ip=0\}$ for $i\in I$ with $\{p>0\}\subseteq\{\sum_iw_i=1\}$ and $g\in\mathcal L^1(\lambda)$ with $\{p=0\}\subseteq\{g=0\}$ . How can we minimize $$\sum_{i\in I}\frac1{n_i}\int_{\{\:q_i\:>\:0\:\}}\frac{|w_ig|^2}{q_i}\:{\rm d}\lambda\tag1$$ with respect to $(w_i)_{i\in I}$ ? If $p,q_i$ would be (strictly) positive for all $i\in I$ , $(1)$ could be written as $$\int\sum_{i\in I}\frac1{n_i}\frac{|w_ig|^2}{q_i}\:{\rm d}\lambda\tag2$$ and it is obvious that it's sufficient to perform a pointwise minimization of the integrand in $(2)$ . Moreover, since $g$ is a constant in this context, we are then left with the following problem: Given $(q_i)_{i\in I}\subseteq[0,\infty)$ , minimize $$\sum_{i\in I}\frac1{n_i}\frac{w_i^2}{q_i}\tag3$$ subject to $$\sum_{i\in I}w_i=1\tag4$$ with respect to $(w_i)_{i\in I}\subseteq[0,\infty)$ . This problem is easily solved by the Lagrange multiplier theorem yielding the minimizer $$w_i=\frac{n_iq_i}{\sum_{j=1}^kn_jq_j}\tag5.$$ Can we apply the same argumentation to the original problem?","Let be a finite nonempty set, be probability densities on a measurable space for , be measurable with for with and with . How can we minimize with respect to ? If would be (strictly) positive for all , could be written as and it is obvious that it's sufficient to perform a pointwise minimization of the integrand in . Moreover, since is a constant in this context, we are then left with the following problem: Given , minimize subject to with respect to . This problem is easily solved by the Lagrange multiplier theorem yielding the minimizer Can we apply the same argumentation to the original problem?","I p,q_i (E,\mathcal E,\lambda) i\in I w_i:E\to[0,1] \{q_i=0\}\subseteq\{w_ip=0\} i\in I \{p>0\}\subseteq\{\sum_iw_i=1\} g\in\mathcal L^1(\lambda) \{p=0\}\subseteq\{g=0\} \sum_{i\in I}\frac1{n_i}\int_{\{\:q_i\:>\:0\:\}}\frac{|w_ig|^2}{q_i}\:{\rm d}\lambda\tag1 (w_i)_{i\in I} p,q_i i\in I (1) \int\sum_{i\in I}\frac1{n_i}\frac{|w_ig|^2}{q_i}\:{\rm d}\lambda\tag2 (2) g (q_i)_{i\in I}\subseteq[0,\infty) \sum_{i\in I}\frac1{n_i}\frac{w_i^2}{q_i}\tag3 \sum_{i\in I}w_i=1\tag4 (w_i)_{i\in I}\subseteq[0,\infty) w_i=\frac{n_iq_i}{\sum_{j=1}^kn_jq_j}\tag5.","['measure-theory', 'optimization', 'nonlinear-optimization', 'lagrange-multiplier']"
97,Radon-Nikodym derivative of pushforward measure,Radon-Nikodym derivative of pushforward measure,,"I am trying to show that two measures are equivalent. This is the same as showing that the Radon-Nikodym derivative is bounded from above and below. I'll break it down so that things are easy to follow. If you need more information, please do ask! Setup: Consider the probability space $(X,\rho$ ) and the map $R:X\to X$ . Let $Y\subset X$ with $\rho(Y)>0$ . Assumption 1: There exists a measure $\mu_Y$ on $Y$ such that $\mu_Y$ is equivalent to $\rho|_Y$ , where $\rho|_Y$ denotes the restriction of the measure $\rho$ to $Y$ (suitably normalised). Assumption 2: There exist; (i) A probability space $(Z=Y\times K,\nu)$ where $\nu=\mu_Y\times \kappa$ for some suitably normalised measure $\kappa$ on some space $K$ . (ii) An ergodic invariant function $F:Z\to Z$ and a measurable surjection $s:Z\to X$ such that $s\circ F=R\circ s$ . Assumption 3: Define the measure $\mu=s_*\nu:=\nu(s^{-1}(\cdot))$ on $X$ . It is assumed that $R$ is ergodic and invariant with respect to $\mu$ . Moreover, we assume that $R^{-1}A=A$ implies $\rho(A)\in\{0,1\}$ , but invariance of R with respect to $\rho$ fails. Question: Can we say that $\mu$ and $\rho$ are equivalent? My ""attempt"": I think a similar sort of argument for $\frac{d\mu}{d\rho}=\frac{ds_*\nu}{d\rho}$ to the following could work. If we define the measure $m=\rho|_Y\times \kappa$ , note that $$\frac{1}{C}\le \frac{d\nu}{dm}= \frac{d\mu_Y}{d\rho|_Y}\le C$$ for some $C>0$ , so that $\nu$ and $m$ are equivalent. So far however I am not sure how to get the result. Any help would be very much appreciated! Thanks!","I am trying to show that two measures are equivalent. This is the same as showing that the Radon-Nikodym derivative is bounded from above and below. I'll break it down so that things are easy to follow. If you need more information, please do ask! Setup: Consider the probability space ) and the map . Let with . Assumption 1: There exists a measure on such that is equivalent to , where denotes the restriction of the measure to (suitably normalised). Assumption 2: There exist; (i) A probability space where for some suitably normalised measure on some space . (ii) An ergodic invariant function and a measurable surjection such that . Assumption 3: Define the measure on . It is assumed that is ergodic and invariant with respect to . Moreover, we assume that implies , but invariance of R with respect to fails. Question: Can we say that and are equivalent? My ""attempt"": I think a similar sort of argument for to the following could work. If we define the measure , note that for some , so that and are equivalent. So far however I am not sure how to get the result. Any help would be very much appreciated! Thanks!","(X,\rho R:X\to X Y\subset X \rho(Y)>0 \mu_Y Y \mu_Y \rho|_Y \rho|_Y \rho Y (Z=Y\times K,\nu) \nu=\mu_Y\times \kappa \kappa K F:Z\to Z s:Z\to X s\circ F=R\circ s \mu=s_*\nu:=\nu(s^{-1}(\cdot)) X R \mu R^{-1}A=A \rho(A)\in\{0,1\} \rho \mu \rho \frac{d\mu}{d\rho}=\frac{ds_*\nu}{d\rho} m=\rho|_Y\times \kappa \frac{1}{C}\le \frac{d\nu}{dm}= \frac{d\mu_Y}{d\rho|_Y}\le C C>0 \nu m","['measure-theory', 'dynamical-systems', 'ergodic-theory', 'radon-nikodym']"
98,Operator commutes with translation is a convolution with a measure. Stein.,Operator commutes with translation is a convolution with a measure. Stein.,,"Hi! following the thread of Bounded linear operators that commute with translation I can't prove that $\left\|m\right\|=\left\|T\right\|$ I have only one inequality: $\left\|T(f)\right\|_{L^1}\leq \left\|f\right\|_{L^1}\left\| \mu\right\|$ . Then, $\left\|T\right\|\leq \left\|\mu\right\|$ For the second, I thought of taking $f =\delta$ Dirac delta function which satisfies $\int_{\mathbb{R}^n}\delta(x)=1$ and $\delta\ast \mu=\mu$ to have to $\left\|T\right\|\geq \left\|T(\delta)\right\|_{L^1}=\left\|\delta\ast \mu\right\|_{L^1}=\left\|\mu\right\|_{L^1}$ then $\left\|T\right\|$ is a upper bound for $\left\|\mu\right\|_{L^1}$ then $\left\|\mu\right\|\leq \left\|T\right\|$ But I don't know if the above is legal. Actualization 1: Is the following valid? In Bounded linear operators that commute with translation have this: $\lim_{k} \int f(y)T\phi_{\epsilon_k}(x-y)dy=\int f(y)\mu(x-y)dy$ with $x=2y$ $\lim_{k} \int f(y)T\phi_{\epsilon_k}(y)dy=\int f(y)\mu(y)dy$ then \begin{align*} &\left\|\mu\right\|=\sup_{|f|_{L^1}\leq 1} \left|\int f(x)d\mu(x)\right|\\ &=\sup_{|f|_{L^1}\leq 1}\lim_{k} \left|\int f(x) T\phi_{\epsilon_k}(x)dx\right|\\ &\leq  \int|T\phi_{\epsilon_k}(x)|dx\\ &=|T\phi_{\epsilon_k}|_{L^1}\leq \left\|T\right\| |\phi_{\epsilon_k}|_{L^1}\\ &=\left\|T\right\| \end{align*} Therefore $\left\|\mu\right\|\leq \left\|T\right\|$ . It is correct? This argument is based on the demonstration of the Grafakos Classical Fourier Analysis book. Theorem 2.5.8. (characterization of multipliers for p = 1), equation 2.5.14 page 153 pd: $d\mu(x)=\mu(x)dx$ ? Actualization 2. I am trying to understand the demonstration of link Bounded linear operators that commute with translation . I have the following with some doubts: $L^1:=L^1(\mathbb{R}^n)$ Let $\phi\in L^1$ such that $\int_{R^n}\phi(x)dx=1$ and, for any $\epsilon>0$ ,\ $\phi_{\epsilon}:=\epsilon^{-n}\phi(x/\epsilon)$ . Is easy to see that $\|\phi_{\epsilon}\|_{1}=\|\phi\|_{1}$ lemma. Let $\phi\in L^1$ , $\|\phi\|_{1}=1$ . For any $f\in L^p,\ 1\leq p<\infty,\  \lim_{\epsilon\to 0}\|f\ast \phi_{\epsilon}-f\|_{p}=0$ . Observation: $\{\phi_{\epsilon}\}_{\epsilon>0}$ is an aproximation identity. By the lemma, $\lim_{\epsilon\to 0} \int_{R^n}f\ast \phi_{\epsilon}(x)dx=\lim_{R^n}f(x)dx$ . Now, as $\|\phi_{\epsilon}\|_{1}=1$ then $\|T\phi_{\epsilon}\|_{1}$ is bounded in $L^1$ because $T$ is bounded. Because $L^1$ is naturally embedded in the space of finite Borel measures,  wich is the dual of the space $C_{0}:=C_{0}(R^n)$ of continuous  functions that tend to zero at infinity, we obtain that the family $T\phi_{\epsilon}$ lies in a fixed multiple  of the unit ball of $(C_{0})^{\ast}$ . By the Banach-Alaoglu theorem, this is a $weak^{\ast}$ compact set. Therefore, some subsequence of $T\phi_{\epsilon}$ converges  in the $weak^{\ast}$ topology to a measure $\mu$ . That is, for some $\epsilon_{k}\to 0$ , and all $g\in C_{0}$ , we have $$\lim_{k\to\infty}\int_{R^n}g(x)T\phi_{\epsilon_{k}}(x)dx=\int_{R^n}g(x)\mu(x)dx$$ (Here I used $d\mu(x)=\mu(x)dx$ that which I don't know if it's true ... Now, as $T$ is linear, commute with traslations, in addition  of the fact that $C_{0}$ is dense in $L^1$ and $T\phi_{\epsilon_{k}}\to \mu$ , $weak^{\ast}$ convergence, it has: \begin{align*} f\ast \mu(x)&=\int_{\mathbb{R}^n}f(y)\mu(x-y)dy\\ &=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y)(T\phi_{\epsilon_{k}})(x-y)dy\\ &=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y)\tau_{y} (T\phi_{\epsilon_{k}}(x))dy\\ &=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y) T(\tau_{y}\phi_{\epsilon_{k}})(x)dy\\ &=\lim_{k\to\infty}\int_{\mathbb{R}^n}f(y)T(\phi_{\epsilon_{k}}(x-y))dy\\ &=\lim_{k\to\infty}T\left(\int_{\mathbb{R}^n}f(y)(\phi_{\epsilon_{k}}(x-y))dy\right)\\ &=Tf(x) \end{align*} In the above, I do not know if the following is correct: $$\text{If } \lim_{k\to\infty}\int_{R^n}f(y)T\phi_{\epsilon_{k}}(y)dy=\int_{R^n}f(y)\mu(y)dy$$ for any $f\in L^1$ (because $C_0$ is dense in $L^1$ ) Implies $$\lim_{k\to\infty}\int_{R^n}f(y)T\phi_{\epsilon_{k}}(x-y)dy=\int_{R^n}f(y)\mu(x-y)dy ?$$ thanks","Hi! following the thread of Bounded linear operators that commute with translation I can't prove that I have only one inequality: . Then, For the second, I thought of taking Dirac delta function which satisfies and to have to then is a upper bound for then But I don't know if the above is legal. Actualization 1: Is the following valid? In Bounded linear operators that commute with translation have this: with then Therefore . It is correct? This argument is based on the demonstration of the Grafakos Classical Fourier Analysis book. Theorem 2.5.8. (characterization of multipliers for p = 1), equation 2.5.14 page 153 pd: ? Actualization 2. I am trying to understand the demonstration of link Bounded linear operators that commute with translation . I have the following with some doubts: Let such that and, for any ,\ . Is easy to see that lemma. Let , . For any . Observation: is an aproximation identity. By the lemma, . Now, as then is bounded in because is bounded. Because is naturally embedded in the space of finite Borel measures,  wich is the dual of the space of continuous  functions that tend to zero at infinity, we obtain that the family lies in a fixed multiple  of the unit ball of . By the Banach-Alaoglu theorem, this is a compact set. Therefore, some subsequence of converges  in the topology to a measure . That is, for some , and all , we have (Here I used that which I don't know if it's true ... Now, as is linear, commute with traslations, in addition  of the fact that is dense in and , convergence, it has: In the above, I do not know if the following is correct: for any (because is dense in ) Implies thanks","\left\|m\right\|=\left\|T\right\| \left\|T(f)\right\|_{L^1}\leq \left\|f\right\|_{L^1}\left\| \mu\right\| \left\|T\right\|\leq \left\|\mu\right\| f =\delta \int_{\mathbb{R}^n}\delta(x)=1 \delta\ast \mu=\mu \left\|T\right\|\geq \left\|T(\delta)\right\|_{L^1}=\left\|\delta\ast \mu\right\|_{L^1}=\left\|\mu\right\|_{L^1} \left\|T\right\| \left\|\mu\right\|_{L^1} \left\|\mu\right\|\leq \left\|T\right\| \lim_{k} \int f(y)T\phi_{\epsilon_k}(x-y)dy=\int f(y)\mu(x-y)dy x=2y \lim_{k} \int f(y)T\phi_{\epsilon_k}(y)dy=\int f(y)\mu(y)dy \begin{align*}
&\left\|\mu\right\|=\sup_{|f|_{L^1}\leq 1} \left|\int f(x)d\mu(x)\right|\\
&=\sup_{|f|_{L^1}\leq 1}\lim_{k} \left|\int f(x) T\phi_{\epsilon_k}(x)dx\right|\\
&\leq  \int|T\phi_{\epsilon_k}(x)|dx\\
&=|T\phi_{\epsilon_k}|_{L^1}\leq \left\|T\right\| |\phi_{\epsilon_k}|_{L^1}\\
&=\left\|T\right\|
\end{align*} \left\|\mu\right\|\leq \left\|T\right\| d\mu(x)=\mu(x)dx L^1:=L^1(\mathbb{R}^n) \phi\in L^1 \int_{R^n}\phi(x)dx=1 \epsilon>0 \phi_{\epsilon}:=\epsilon^{-n}\phi(x/\epsilon) \|\phi_{\epsilon}\|_{1}=\|\phi\|_{1} \phi\in L^1 \|\phi\|_{1}=1 f\in L^p,\ 1\leq p<\infty,\ 
\lim_{\epsilon\to 0}\|f\ast \phi_{\epsilon}-f\|_{p}=0 \{\phi_{\epsilon}\}_{\epsilon>0} \lim_{\epsilon\to 0} \int_{R^n}f\ast \phi_{\epsilon}(x)dx=\lim_{R^n}f(x)dx \|\phi_{\epsilon}\|_{1}=1 \|T\phi_{\epsilon}\|_{1} L^1 T L^1 C_{0}:=C_{0}(R^n) T\phi_{\epsilon} (C_{0})^{\ast} weak^{\ast} T\phi_{\epsilon} weak^{\ast} \mu \epsilon_{k}\to 0 g\in C_{0} \lim_{k\to\infty}\int_{R^n}g(x)T\phi_{\epsilon_{k}}(x)dx=\int_{R^n}g(x)\mu(x)dx d\mu(x)=\mu(x)dx T C_{0} L^1 T\phi_{\epsilon_{k}}\to \mu weak^{\ast} \begin{align*}
f\ast \mu(x)&=\int_{\mathbb{R}^n}f(y)\mu(x-y)dy\\
&=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y)(T\phi_{\epsilon_{k}})(x-y)dy\\
&=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y)\tau_{y} (T\phi_{\epsilon_{k}}(x))dy\\
&=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y) T(\tau_{y}\phi_{\epsilon_{k}})(x)dy\\
&=\lim_{k\to\infty}\int_{\mathbb{R}^n}f(y)T(\phi_{\epsilon_{k}}(x-y))dy\\
&=\lim_{k\to\infty}T\left(\int_{\mathbb{R}^n}f(y)(\phi_{\epsilon_{k}}(x-y))dy\right)\\
&=Tf(x)
\end{align*} \text{If } \lim_{k\to\infty}\int_{R^n}f(y)T\phi_{\epsilon_{k}}(y)dy=\int_{R^n}f(y)\mu(y)dy f\in L^1 C_0 L^1 \lim_{k\to\infty}\int_{R^n}f(y)T\phi_{\epsilon_{k}}(x-y)dy=\int_{R^n}f(y)\mu(x-y)dy ?","['functional-analysis', 'measure-theory', 'fourier-analysis', 'operator-theory']"
99,Equivalence of two definitions of Lebesgue integration,Equivalence of two definitions of Lebesgue integration,,"I'm quite new to measure theory, and so far I've come across two different definitions for the integral of a real-valued function over a measure space. I'm having trouble showing that the two definitions are equivalent. Suppose $(X,\Sigma,\mu)$ is a measure space. The first approach starts with defining a simple function on $X$ as any function of the form $g=\sum_{i=0}^n a_i\chi E_i$ ,  where $a_0,a_1,\ldots,a_n$ are constants in $\mathbb{R}$ , $E_0,E_1,\ldots,E_n$ are measurable subsets of $X$ with finite measure, and $\chi E_i$ is the characteristic function of $E_i$ . For such a function, we define $$\int g =\sum_{i=0}^n a_i\mu E_i.$$ Next, if $f$ is a nonnegative, real-valued function defined on a conegligible/conull subset of $X$ , we say that $f$ is integrable if there is a non-decreasing sequence $\langle f_n\rangle_{n\in\mathbb{N}}$ of non-negative simple functions such that $\sup_{n\in\mathbb{N}}\int f_n<\infty$ and $\lim_{n\to\infty}f_n =f$ almost everywhere.  In this case, we set $$\int f = \sup\left\{\int g:g\text{ is a simple function and }g\leq f\text{ almost everywhere}\right\}.$$ Finally, if $f$ is an arbitrary real-valued function defined on a conegligible subset of $X$ , we say that $f$ is integrable if we can write $f=f_1-f_2$ where $f_1$ and $f_2$ are nonnegative, integrable functions (in the sense defined earlier). In this case, we set $\int f= \int f_1- \int f_2$ . For the second approach, we start by defining a quasi-simple function on $X$ as any function of the form $g=\sum_{i=0}^{\color{red}\infty} a_i \chi G_i$ where $\langle G_i\rangle_{i\in\color{red}{\mathbb{N}}}$ is a $\color{red}{\text{partition}}$ of $X$ into measurable sets, $\langle a_i\rangle_{i\in\color{red}{\mathbb{N}}}$ is a sequence in $\mathbb{R}$ , and $$\sum_{i=0}^\infty |a_i|\mu G_i<\infty.$$ In the above sum, we count $0\cdot \infty$ as $0$ , so that $G_i$ can have infinite measure so long as $a_i=0$ . For such a function, we define $$\int g=\sum_{i=0}^\infty a_i\mu G_i.$$ We say a real-valued function $f$ defined on a conegligible subset of $X$ is integrable if the two values $$\sup\left\{\int g:g\text{ is a quasi-simple function and }g\leq f\text{ almost everywhere}\right\}$$ $$\inf\left\{\int h:h\text{ is a quasi-simple function and }h\geq f\text{ almost everywhere}\right\}$$ are equal, in which case we set $\int f$ equal to the common value. I noticed that the difference in approach between these two definitions becomes more apparent when considering the case in which $f$ is unbounded. Indeed, if we suppose $f$ is unbounded below, then although there are quasi-simple functions which are less than $f$ , there are no simple functions less than $f$ (so that splitting up $f$ into $f_1-f_2$ as done in the first definition is necessary). I would greatly appreciate any help in showing that these two definitions are equivalent. For further context, showing the equivalence of these two definitions is exercise 122Yd in Fremlin's first volume of Measure Theory .","I'm quite new to measure theory, and so far I've come across two different definitions for the integral of a real-valued function over a measure space. I'm having trouble showing that the two definitions are equivalent. Suppose is a measure space. The first approach starts with defining a simple function on as any function of the form ,  where are constants in , are measurable subsets of with finite measure, and is the characteristic function of . For such a function, we define Next, if is a nonnegative, real-valued function defined on a conegligible/conull subset of , we say that is integrable if there is a non-decreasing sequence of non-negative simple functions such that and almost everywhere.  In this case, we set Finally, if is an arbitrary real-valued function defined on a conegligible subset of , we say that is integrable if we can write where and are nonnegative, integrable functions (in the sense defined earlier). In this case, we set . For the second approach, we start by defining a quasi-simple function on as any function of the form where is a of into measurable sets, is a sequence in , and In the above sum, we count as , so that can have infinite measure so long as . For such a function, we define We say a real-valued function defined on a conegligible subset of is integrable if the two values are equal, in which case we set equal to the common value. I noticed that the difference in approach between these two definitions becomes more apparent when considering the case in which is unbounded. Indeed, if we suppose is unbounded below, then although there are quasi-simple functions which are less than , there are no simple functions less than (so that splitting up into as done in the first definition is necessary). I would greatly appreciate any help in showing that these two definitions are equivalent. For further context, showing the equivalence of these two definitions is exercise 122Yd in Fremlin's first volume of Measure Theory .","(X,\Sigma,\mu) X g=\sum_{i=0}^n a_i\chi E_i a_0,a_1,\ldots,a_n \mathbb{R} E_0,E_1,\ldots,E_n X \chi E_i E_i \int g =\sum_{i=0}^n a_i\mu E_i. f X f \langle f_n\rangle_{n\in\mathbb{N}} \sup_{n\in\mathbb{N}}\int f_n<\infty \lim_{n\to\infty}f_n =f \int f = \sup\left\{\int g:g\text{ is a simple function and }g\leq f\text{ almost everywhere}\right\}. f X f f=f_1-f_2 f_1 f_2 \int f= \int f_1- \int f_2 X g=\sum_{i=0}^{\color{red}\infty} a_i \chi G_i \langle G_i\rangle_{i\in\color{red}{\mathbb{N}}} \color{red}{\text{partition}} X \langle a_i\rangle_{i\in\color{red}{\mathbb{N}}} \mathbb{R} \sum_{i=0}^\infty |a_i|\mu G_i<\infty. 0\cdot \infty 0 G_i a_i=0 \int g=\sum_{i=0}^\infty a_i\mu G_i. f X \sup\left\{\int g:g\text{ is a quasi-simple function and }g\leq f\text{ almost everywhere}\right\} \inf\left\{\int h:h\text{ is a quasi-simple function and }h\geq f\text{ almost everywhere}\right\} \int f f f f f f f_1-f_2","['integration', 'measure-theory', 'lebesgue-integral']"
