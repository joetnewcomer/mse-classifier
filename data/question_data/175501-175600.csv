,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Gradient of $\lVert \mathbf{H}^{\dagger}\mathbf{H} - \mathbf{B}\rVert_F^2$,Gradient of,\lVert \mathbf{H}^{\dagger}\mathbf{H} - \mathbf{B}\rVert_F^2,"I'm trying to get the derivative of $$f(\mathbf{H})=\lVert \mathbf{H}^{\dagger}\mathbf{H} - \mathbf{B}\rVert^2_F$$ with respect to $\mathbf{H}$ , where $\mathbf{H}^\dagger$ denotes the pseudo-inverse of $\mathbf{H}$ . I know that $\nabla f = \mathbf{J}_{\mathbf{H}^{\dagger}\mathbf{H}}(\mathbf{H})^T \cdot \text{vect}\left(2(\mathbf{H}^{\dagger}\mathbf{H}-\mathbf{B}) \right)$ where $\mathbf{J}_{\mathbf{H}^{\dagger}\mathbf{H}}(\mathbf{H})$ is the Jacobian Matrix of $\mathbf{H}^{\dagger}\mathbf{H}$ . By the other hand, from ""Matrix Cook Book"", I know that $d(\mathbf{H}^\dagger\mathbf{H}) = d(\mathbf{H}^\dagger)\mathbf{H} + \mathbf{H}^\dagger d(\mathbf{H}) $ and from Derivative of pseudoinverse with respect to original matrix I know that $\eqalign{d\mathbf{H}^\dagger &= \mathbf{H}^\dagger (\mathbf{H}^\dagger)^T\,d\mathbf{H}^T\,(\mathbf{I}-\mathbf{H}\mathbf{H}^\dagger) + (\mathbf{I}-\mathbf{H}^\dagger \mathbf{H})\,d\mathbf{H}^T\,(\mathbf{H}^\dagger)^T\mathbf{H}^\dagger -\mathbf{H}^\dagger\,d\mathbf{H}\,\mathbf{H}^\dagger \cr }$ But I don't know how to use the last two differential, I mean, how can I obtain $\mathbf{J}_{\mathbf{H}^{\dagger}\mathbf{H}}(\mathbf{H})$ or $\partial (\mathbf{H}^{\dagger}\mathbf{H})_{(i,j)}/\partial \mathbf{H}_{(k,l)}$ ?","I'm trying to get the derivative of with respect to , where denotes the pseudo-inverse of . I know that where is the Jacobian Matrix of . By the other hand, from ""Matrix Cook Book"", I know that and from Derivative of pseudoinverse with respect to original matrix I know that But I don't know how to use the last two differential, I mean, how can I obtain or ?","f(\mathbf{H})=\lVert \mathbf{H}^{\dagger}\mathbf{H} - \mathbf{B}\rVert^2_F \mathbf{H} \mathbf{H}^\dagger \mathbf{H} \nabla f = \mathbf{J}_{\mathbf{H}^{\dagger}\mathbf{H}}(\mathbf{H})^T \cdot \text{vect}\left(2(\mathbf{H}^{\dagger}\mathbf{H}-\mathbf{B}) \right) \mathbf{J}_{\mathbf{H}^{\dagger}\mathbf{H}}(\mathbf{H}) \mathbf{H}^{\dagger}\mathbf{H} d(\mathbf{H}^\dagger\mathbf{H}) = d(\mathbf{H}^\dagger)\mathbf{H} + \mathbf{H}^\dagger d(\mathbf{H})  \eqalign{d\mathbf{H}^\dagger &= \mathbf{H}^\dagger (\mathbf{H}^\dagger)^T\,d\mathbf{H}^T\,(\mathbf{I}-\mathbf{H}\mathbf{H}^\dagger) + (\mathbf{I}-\mathbf{H}^\dagger \mathbf{H})\,d\mathbf{H}^T\,(\mathbf{H}^\dagger)^T\mathbf{H}^\dagger -\mathbf{H}^\dagger\,d\mathbf{H}\,\mathbf{H}^\dagger \cr
} \mathbf{J}_{\mathbf{H}^{\dagger}\mathbf{H}}(\mathbf{H}) \partial (\mathbf{H}^{\dagger}\mathbf{H})_{(i,j)}/\partial \mathbf{H}_{(k,l)}","['multivariable-calculus', 'derivatives', 'matrix-calculus', 'pseudoinverse']"
1,integration of double integral with euler number include it.,integration of double integral with euler number include it.,,Hello i have a hard time with integration of double integral with euler number with power included in it. I have $$\int_0^1\int_0^2x^2ye^{xy}dxdy$$ so i am trying to use substitution for solving it. as follows i say $u = y$ $du = 1$ $v=ye^{xy}$ i am not sure for $dv$ here is it $e^{xy}$ or it is $\frac{e^{xy}}{x}$ ? Then i use the $Integration$ $by$ $parts$ $formula$ $$\int_a^budv=uv|_a^b-\int_a^bvdu$$ so from here i have $$\int_0^1x^2ye^{xy}|_0^2-\int_0^2x^2\frac{e^{xy}}{x}$$ a first thing i notice here i am left with 2 integrals one with respect to $dx$ and another with respect to $dydx$ that is confusing me in the first place so i am continue to integrate now i think i don't need $u$ $substitution$ to finish the integral with respect to $dy$ it must be : $$\int_0^1(2e^{2x}|_0^2-e^{2x}+e^0 )dx$$ From here i must do another $u$ $sub$ for x this time. So i am having many steps that i am not sure that are correct if someone can help me to understand whats happening here. Thank you in advance.,Hello i have a hard time with integration of double integral with euler number with power included in it. I have so i am trying to use substitution for solving it. as follows i say i am not sure for here is it or it is ? Then i use the so from here i have a first thing i notice here i am left with 2 integrals one with respect to and another with respect to that is confusing me in the first place so i am continue to integrate now i think i don't need to finish the integral with respect to it must be : From here i must do another for x this time. So i am having many steps that i am not sure that are correct if someone can help me to understand whats happening here. Thank you in advance.,\int_0^1\int_0^2x^2ye^{xy}dxdy u = y du = 1 v=ye^{xy} dv e^{xy} \frac{e^{xy}}{x} Integration by parts formula \int_a^budv=uv|_a^b-\int_a^bvdu \int_0^1x^2ye^{xy}|_0^2-\int_0^2x^2\frac{e^{xy}}{x} dx dydx u substitution dy \int_0^1(2e^{2x}|_0^2-e^{2x}+e^0 )dx u sub,"['calculus', 'integration', 'multivariable-calculus']"
2,Solve $\frac{dx}{y^2(x-y)}=\frac{dy}{x^2(x-y)}=\frac{dz}{z(x^2+y^2)}$,Solve,\frac{dx}{y^2(x-y)}=\frac{dy}{x^2(x-y)}=\frac{dz}{z(x^2+y^2)},"I am trying to solve the simultaneous differential equations $$\frac{dx}{y^2(x-y)}=\frac{dy}{x^2(x-y)}=\frac{dz}{z(x^2+y^2)}.$$ From the first two fractions, we can get the equation $x^3 - y^3 = c$ , but I'm not able to get a second equation with $z$ .","I am trying to solve the simultaneous differential equations From the first two fractions, we can get the equation , but I'm not able to get a second equation with .",\frac{dx}{y^2(x-y)}=\frac{dy}{x^2(x-y)}=\frac{dz}{z(x^2+y^2)}. x^3 - y^3 = c z,"['multivariable-calculus', 'partial-differential-equations', 'systems-of-equations']"
3,What is the inverse operation of a gradient?,What is the inverse operation of a gradient?,,"I notice that the function $$f(x,y,x;a,b,c) = ke^{-a/x-b/y-c/z}$$ has partial derivatives $$\nabla f = \begin{bmatrix} \partial f / \partial x \\  \partial f / \partial y \\  \partial f / \partial z \end{bmatrix} = \begin{bmatrix} a f / x^2 \\  b f / y^2 \\  c f / z^2 \end{bmatrix}$$ I want an operation, let's call it $\beta(\nabla f)$ , that does this: $$\beta(\nabla f)=ke^{-a/x-b/y-c/z}$$ I.e., takes the gradient and returns the original function. I was thinking along the lines of $$\beta(\nabla f)=\int\int\int \nabla f\cdot \mathbf{n} \:\:dx \:dy\:dz \:\:; \: \mathbf{n} = [1,1,1]$$ But it seems pretty arbitrary to throw that $\mathbf{n}$ in there. Plus, this would basically be an indefinite volume integral, which I'm not sure exists (volume integrals always have to be definite integrals, right?).","I notice that the function has partial derivatives I want an operation, let's call it , that does this: I.e., takes the gradient and returns the original function. I was thinking along the lines of But it seems pretty arbitrary to throw that in there. Plus, this would basically be an indefinite volume integral, which I'm not sure exists (volume integrals always have to be definite integrals, right?).","f(x,y,x;a,b,c) = ke^{-a/x-b/y-c/z} \nabla f = \begin{bmatrix}
\partial f / \partial x
\\ 
\partial f / \partial y
\\ 
\partial f / \partial z
\end{bmatrix} = \begin{bmatrix}
a f / x^2
\\ 
b f / y^2
\\ 
c f / z^2
\end{bmatrix} \beta(\nabla f) \beta(\nabla f)=ke^{-a/x-b/y-c/z} \beta(\nabla f)=\int\int\int \nabla f\cdot \mathbf{n} \:\:dx \:dy\:dz \:\:; \: \mathbf{n} = [1,1,1] \mathbf{n}","['multivariable-calculus', 'vector-analysis', 'indefinite-integrals', 'inverse-function']"
4,Non-existence of the potential function,Non-existence of the potential function,,"I am wondering why this theorem is not true when $(f,g)$ are defined on a more general open set $U$ which is not necessarily the entire plane or some disc. What is an example of a vector field $$F(x,y)=(f(x,y),g(x,y))$$ defined on some open set $U$ satisfying the condition $$\frac{\partial f}{\partial y} = \frac{\partial g}{\partial x}$$ but $F$ does not have a potential function?",I am wondering why this theorem is not true when are defined on a more general open set which is not necessarily the entire plane or some disc. What is an example of a vector field defined on some open set satisfying the condition but does not have a potential function?,"(f,g) U F(x,y)=(f(x,y),g(x,y)) U \frac{\partial f}{\partial y} = \frac{\partial g}{\partial x} F","['multivariable-calculus', 'partial-derivative', 'examples-counterexamples', 'vector-fields']"
5,Finding a vector equation for the tangent line of curve formed by the intersection of two cylinders,Finding a vector equation for the tangent line of curve formed by the intersection of two cylinders,,"I am currently working through questions in the textbook ""Stewart Calculus: Early Transcendentals, 8th Edition"". Fortunately, the textbook also has a solution manual but I'm having a hard time understanding why they did what they did. The question is to find a vector equation for the tangent line to the curve of intersection of the cylinders $x^2+y^2=25$ and $y^2+z^2=20$ at the point $(3,4,2)$ . This was their solution: I'm confused about the first part where they mention that the projection is contained in the circle $x^2+y^2=25$ . I graphed the two cylinders using GeoGabra and got: From the looks of it, the projection of the intersection doesn't look like a circle. Unless they literally meant contained within that circle which I guess is true but could we have also said that this curve was contained within $x^2+y^2=36$ and working with $x=6$ $\cos{t}$ and $y=6$ $\sin{t}$ for the rest of the problem and have that work as well? I'm having a hard time wrapping my head around using the projection when the entire trace isn't covered in the intersection like in the question above where there is a small portion in red that isn't intersected by the other cylinder. I hope my question makes sense, please let me know if there are parts I should expand on.","I am currently working through questions in the textbook ""Stewart Calculus: Early Transcendentals, 8th Edition"". Fortunately, the textbook also has a solution manual but I'm having a hard time understanding why they did what they did. The question is to find a vector equation for the tangent line to the curve of intersection of the cylinders and at the point . This was their solution: I'm confused about the first part where they mention that the projection is contained in the circle . I graphed the two cylinders using GeoGabra and got: From the looks of it, the projection of the intersection doesn't look like a circle. Unless they literally meant contained within that circle which I guess is true but could we have also said that this curve was contained within and working with and for the rest of the problem and have that work as well? I'm having a hard time wrapping my head around using the projection when the entire trace isn't covered in the intersection like in the question above where there is a small portion in red that isn't intersected by the other cylinder. I hope my question makes sense, please let me know if there are parts I should expand on.","x^2+y^2=25 y^2+z^2=20 (3,4,2) x^2+y^2=25 x^2+y^2=36 x=6 \cos{t} y=6 \sin{t}","['calculus', 'multivariable-calculus', 'vectors']"
6,Maximum of $ab+2bc+3ca$ with $a^4+b^4+c^4=1$,Maximum of  with,ab+2bc+3ca a^4+b^4+c^4=1,"Let $a,b,c\in \mathbb R^+$ with $a^4+b^4+c^4=1$ . What is the maximal value $ab+2bc+3ca$ can take? I tried using Cauchy-Schwarz several different ways and the best upper bound I got was $\sqrt{14}$ , but it was never sharp. Numerical search suggests that the maximum occurs at about $a=0.763316$ , $b=0.697312$ , $c=0.80698$ with $ab+2bc+3ca=3.505647$ , though I couldn't find any valuable relation between these numbers and the rationals.","Let with . What is the maximal value can take? I tried using Cauchy-Schwarz several different ways and the best upper bound I got was , but it was never sharp. Numerical search suggests that the maximum occurs at about , , with , though I couldn't find any valuable relation between these numbers and the rationals.","a,b,c\in \mathbb R^+ a^4+b^4+c^4=1 ab+2bc+3ca \sqrt{14} a=0.763316 b=0.697312 c=0.80698 ab+2bc+3ca=3.505647","['multivariable-calculus', 'inequality', 'optimization', 'lagrange-multiplier', 'maxima-minima']"
7,Multidimensionally Lipschitz continuous iff Lipschitz continuous in every coordinate,Multidimensionally Lipschitz continuous iff Lipschitz continuous in every coordinate,,"Definition . A function $f$ defined on a set $S \subseteq \mathbb R$ is said to be Lipschitz continuous on $S$ if there exists an $L \geq 0$ such that $$\|f(x_1) - f(x_2)\| \le L\|x_1 - x_2\|$$ for all $x_1$ and $x_2$ in $S$ such that $x_1 \ne x_2$ . Consider some function $f: \mathbb R^n \to \mathbb R^n$ taking several variables $x^{(k)}$ with $1 < k \leq n$ as argument. Let $f_i: \mathbb R^n \to \mathbb R$ be the function of $i$ -th coordinate of the function value of $f$ . Are the following statements true? If $f$ is Lipschitz continuous, then $f_i$ is Lipschitz continuous $\forall i \in \{1, \dots, n\}$ . If $f_i$ is Lipschitz continuous $\forall i \in \{1, \dots, n\}$ , then $f$ is Lipschitz continuous.","Definition . A function defined on a set is said to be Lipschitz continuous on if there exists an such that for all and in such that . Consider some function taking several variables with as argument. Let be the function of -th coordinate of the function value of . Are the following statements true? If is Lipschitz continuous, then is Lipschitz continuous . If is Lipschitz continuous , then is Lipschitz continuous.","f S \subseteq \mathbb R S L \geq 0 \|f(x_1) - f(x_2)\| \le L\|x_1 - x_2\| x_1 x_2 S x_1 \ne x_2 f: \mathbb R^n \to \mathbb R^n x^{(k)} 1 < k \leq n f_i: \mathbb R^n \to \mathbb R i f f f_i \forall i \in \{1, \dots, n\} f_i \forall i \in \{1, \dots, n\} f","['multivariable-calculus', 'continuity', 'lipschitz-functions']"
8,Approximately inverting functions with Monte Carlo methods,Approximately inverting functions with Monte Carlo methods,,"I have the following problem: Let $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ with $n>m$ be a smooth function. I want to find input vectors $x \in \mathbb{R}^n$ which yield a given output $y\in \mathbb{R}^m$ , so $f(x)=y$ . My idea was to start at a random vector $x_0 \in \mathbb{R}^n$ and try to iteratively decrease the $L^2$ -norm of the differences, so that for any $n$ we have $||f(x_{n+1})-y)||_2<||f(x_n)-y)||_2$ . Would this be possible with some kind of Monte Carlo approach? Or are there any existing approaches that could be used for tackling this problem? Thanks in advance!","I have the following problem: Let with be a smooth function. I want to find input vectors which yield a given output , so . My idea was to start at a random vector and try to iteratively decrease the -norm of the differences, so that for any we have . Would this be possible with some kind of Monte Carlo approach? Or are there any existing approaches that could be used for tackling this problem? Thanks in advance!",f:\mathbb{R}^n \rightarrow \mathbb{R}^m n>m x \in \mathbb{R}^n y\in \mathbb{R}^m f(x)=y x_0 \in \mathbb{R}^n L^2 n ||f(x_{n+1})-y)||_2<||f(x_n)-y)||_2,['multivariable-calculus']
9,Evaluate $\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy$,Evaluate,\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy,"Evaluate, where $D=\{(x,y): 0\le x \le 1 , 0\le y \le x\}$,   $$\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy$$ My solution and question $$\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy = \int_0^1\frac{\sin x}{x}\;\mathrm dx\int_0^x\mathrm dy =\\= \int_0^1\sin x \;\mathrm dx  = [-\cos x ]_0^1=\ 1-\cos 1$$ Is it correct? Thanks in advance.","Evaluate, where $D=\{(x,y): 0\le x \le 1 , 0\le y \le x\}$,   $$\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy$$ My solution and question $$\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy = \int_0^1\frac{\sin x}{x}\;\mathrm dx\int_0^x\mathrm dy =\\= \int_0^1\sin x \;\mathrm dx  = [-\cos x ]_0^1=\ 1-\cos 1$$ Is it correct? Thanks in advance.",,['calculus']
10,"Compute $\frac{d}{dx} \left\{ u^H M^{-1}(x) \ A \ M^{-1}(x) \ u\right\} $, where $M(x) = \left(B + xA \right) ; \ M^H(x) = M(x)$","Compute , where",\frac{d}{dx} \left\{ u^H M^{-1}(x) \ A \ M^{-1}(x) \ u\right\}  M(x) = \left(B + xA \right) ; \ M^H(x) = M(x),"Problem : Let $x \in \mathbb{R}$, $u \in \mathbb{C}^{n \times 1}$, $B \in \mathbb{C}^{n \times n}$, $A \in \mathbb{C}^{n \times n}$, and    $M(x) = \left(B + xA \right) ; \ M^H(x) = M(x)$. Obtain   \begin{align} \frac{d}{dx} f(x) = \frac{d}{dx} \left\{ u^H M^{-1}(x) \ A \ M^{-1}(x) \ u\right\} \ . \end{align}","Problem : Let $x \in \mathbb{R}$, $u \in \mathbb{C}^{n \times 1}$, $B \in \mathbb{C}^{n \times n}$, $A \in \mathbb{C}^{n \times n}$, and    $M(x) = \left(B + xA \right) ; \ M^H(x) = M(x)$. Obtain   \begin{align} \frac{d}{dx} f(x) = \frac{d}{dx} \left\{ u^H M^{-1}(x) \ A \ M^{-1}(x) \ u\right\} \ . \end{align}",,"['linear-algebra', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
11,"multidimensional chain rule, example","multidimensional chain rule, example",,"Given $f:\mathbb{R}^3\to\mathbb{R}$, $f(u,v,w)=uv+vw-uw$ and $g:\mathbb{R}^2\to\mathbb{R}^3$, $g(x,y)=(x+y, x+y^2, x^2+y)$, I have to give $D(f\circ g)(x,y)$ I calculated like this: $Df(u,v,w)=(v-w, u+w, v-u)$ and $Dg(x,y)=\begin{pmatrix} 1&1&2x\\ 1&2y& 1\\ 0&0&0\end{pmatrix}$ Furthermore it is $Df(g(x,y))=(x+y^2-x^2-y, x+y+x+y^2, x+y^2-x-y)$ Now with the multidimensional chain rule, we have: $D(f\circ g)(x,y)=Df(g(x,y))\cdot Dg(x,y)$ $(y^2-x^2+x+y, y^2+y+2x, y^2-y)\cdot\begin{pmatrix} 1&1&2x\\ 1&2y& 1\\ 0&0&0\end{pmatrix}$ $$=(\color{red}{y^2-x^2+x-y+y^2+y+2x}, \color{blue}{y^2-x^2+x-y+2y^3+2y^2+2xy}, \color{orange}{2xy^2-2x^3+2x^2-2xy+y^2+y+2x})$$ $$=(\color{red}{2y^2+3x-x^2}, \color{blue}{2y^3+3y^2-y-x^2+x+4xy}, \color{orange}{y^2+y-2x^3+2x^2+2x+2xy^2-2xy})$$ Which is wrong accoring to: http://www.wolframalpha.com/input/?dataset=&i=d%2Fdx+ReplaceAll%5Buv%2Bvw-uw,%7Bu-%3Ex%2By,v-%3Ex%2By%5E2,w-%3Ex%5E2%2By%7D%5D [You might check Multidimensional chain rule, online calculator for further reference.] But I do not see my mistake myself, unfortunatly. Also I have a question, because I got a little bit confused by the notation. If you write $g:\mathbb{R}^2\to\mathbb{R}^3$, $g(x,y)=(x+y, x+y^2, x^2+y)$, dont you have to be more clear and note $(x+y, x+y^2, x^2+y)^t$, since a vector in $\mathbb{R}^3$ should be normally noted like $\begin{pmatrix}x\\y\\z\end{pmatrix}$ this [where I just use $x,y,z$ because of simplicity], or do I mess something up here? Thanks in advance.","Given $f:\mathbb{R}^3\to\mathbb{R}$, $f(u,v,w)=uv+vw-uw$ and $g:\mathbb{R}^2\to\mathbb{R}^3$, $g(x,y)=(x+y, x+y^2, x^2+y)$, I have to give $D(f\circ g)(x,y)$ I calculated like this: $Df(u,v,w)=(v-w, u+w, v-u)$ and $Dg(x,y)=\begin{pmatrix} 1&1&2x\\ 1&2y& 1\\ 0&0&0\end{pmatrix}$ Furthermore it is $Df(g(x,y))=(x+y^2-x^2-y, x+y+x+y^2, x+y^2-x-y)$ Now with the multidimensional chain rule, we have: $D(f\circ g)(x,y)=Df(g(x,y))\cdot Dg(x,y)$ $(y^2-x^2+x+y, y^2+y+2x, y^2-y)\cdot\begin{pmatrix} 1&1&2x\\ 1&2y& 1\\ 0&0&0\end{pmatrix}$ $$=(\color{red}{y^2-x^2+x-y+y^2+y+2x}, \color{blue}{y^2-x^2+x-y+2y^3+2y^2+2xy}, \color{orange}{2xy^2-2x^3+2x^2-2xy+y^2+y+2x})$$ $$=(\color{red}{2y^2+3x-x^2}, \color{blue}{2y^3+3y^2-y-x^2+x+4xy}, \color{orange}{y^2+y-2x^3+2x^2+2x+2xy^2-2xy})$$ Which is wrong accoring to: http://www.wolframalpha.com/input/?dataset=&i=d%2Fdx+ReplaceAll%5Buv%2Bvw-uw,%7Bu-%3Ex%2By,v-%3Ex%2By%5E2,w-%3Ex%5E2%2By%7D%5D [You might check Multidimensional chain rule, online calculator for further reference.] But I do not see my mistake myself, unfortunatly. Also I have a question, because I got a little bit confused by the notation. If you write $g:\mathbb{R}^2\to\mathbb{R}^3$, $g(x,y)=(x+y, x+y^2, x^2+y)$, dont you have to be more clear and note $(x+y, x+y^2, x^2+y)^t$, since a vector in $\mathbb{R}^3$ should be normally noted like $\begin{pmatrix}x\\y\\z\end{pmatrix}$ this [where I just use $x,y,z$ because of simplicity], or do I mess something up here? Thanks in advance.",,"['multivariable-calculus', 'proof-verification']"
12,"Why is this function not differentiable at $ (1,1)$?",Why is this function not differentiable at ?," (1,1)","I have spent over two hours trying to understand why this function is not differentiable at $(1,1)$!  $$f(x,y)=\begin{cases}x+y & x\ne y\\x+1 &x=y \end{cases}$$ Supposedly we ought to prove it through using the following equation: $$\lim_{(h,k)\to(0,0)} \frac{[f(1+h,1+k)-f(1,1)-h(\partial_xf(1,1))-k(\partial_yf (1,1))]}{\sqrt{h^2+k^2}} $$ with: $$\frac{\partial f}{\partial x}(1,1) = 1, \quad \frac{\partial f}{\partial y}(1,1) = 1$$ the limit is $0$ when $h \neq k$ But supposedly when $h = k$ the limit is different from $0$ which proves it is not differentiable at $(1,1)$ but no matter what I do I can't seem to get a result different from $0$ when $h = k$ Any help would be much appreciated!","I have spent over two hours trying to understand why this function is not differentiable at $(1,1)$!  $$f(x,y)=\begin{cases}x+y & x\ne y\\x+1 &x=y \end{cases}$$ Supposedly we ought to prove it through using the following equation: $$\lim_{(h,k)\to(0,0)} \frac{[f(1+h,1+k)-f(1,1)-h(\partial_xf(1,1))-k(\partial_yf (1,1))]}{\sqrt{h^2+k^2}} $$ with: $$\frac{\partial f}{\partial x}(1,1) = 1, \quad \frac{\partial f}{\partial y}(1,1) = 1$$ the limit is $0$ when $h \neq k$ But supposedly when $h = k$ the limit is different from $0$ which proves it is not differentiable at $(1,1)$ but no matter what I do I can't seem to get a result different from $0$ when $h = k$ Any help would be much appreciated!",,"['multivariable-calculus', 'partial-derivative']"
13,"Finding Absolute Minimum and Absolute Maximum of $f(x,y)=xy$",Finding Absolute Minimum and Absolute Maximum of,"f(x,y)=xy","Let $\ f(x,y)=xy$. Use the method of Lagrange multipliers to find the maximum and minimum values of the function f on the circle $\ x^2+y^2=1$ First we note that the function $f$ is continuous and the set $S={(x,y):x^2+y^2=1}$ is compact, hence extrema are guaranteed.  Using the method Lagrange multipliers, I set $\nabla f=\lambda\nabla g$, where $g(x,y)=x^2+y^2-1$. Following through the calculations, I arrived at four critical points: $$\Big(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\Big),\Big(\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\Big),\Big(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\Big),\Big(-\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\Big)$$ Substituting these points into the function $f$, I obtained a maximum at  $$\Big(\pm\frac{1}{\sqrt{2}},\pm\frac{1}{\sqrt{2}}\Big)=\frac{1}{2}$$ and a minimum at $$\Big(\pm\frac{1}{\sqrt{2}},\mp\frac{1}{\sqrt{2}}\Big)=-\frac{1}{2}$$ My question is, how do we now find the absolute maximum and absolute minimum of the function $f$ on the unit disc $x^2+y^2\leq 1$? My attempt so far: We want to find all the critical points. So to find stationary points, we set  $$\nabla f=\vec{0}$$ Solving this, we find that $(0,0)$ is a stationary point. So, $f(0,0)=0$. Hence the absolute maximum is $\frac{1}{2}$ and the absolute minimum is $-\frac{1}{2}$, as these are all the critical points of $f$. This does not sit well with me, as I am unsure of my working/logic. Can this be improved on?","Let $\ f(x,y)=xy$. Use the method of Lagrange multipliers to find the maximum and minimum values of the function f on the circle $\ x^2+y^2=1$ First we note that the function $f$ is continuous and the set $S={(x,y):x^2+y^2=1}$ is compact, hence extrema are guaranteed.  Using the method Lagrange multipliers, I set $\nabla f=\lambda\nabla g$, where $g(x,y)=x^2+y^2-1$. Following through the calculations, I arrived at four critical points: $$\Big(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\Big),\Big(\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\Big),\Big(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\Big),\Big(-\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\Big)$$ Substituting these points into the function $f$, I obtained a maximum at  $$\Big(\pm\frac{1}{\sqrt{2}},\pm\frac{1}{\sqrt{2}}\Big)=\frac{1}{2}$$ and a minimum at $$\Big(\pm\frac{1}{\sqrt{2}},\mp\frac{1}{\sqrt{2}}\Big)=-\frac{1}{2}$$ My question is, how do we now find the absolute maximum and absolute minimum of the function $f$ on the unit disc $x^2+y^2\leq 1$? My attempt so far: We want to find all the critical points. So to find stationary points, we set  $$\nabla f=\vec{0}$$ Solving this, we find that $(0,0)$ is a stationary point. So, $f(0,0)=0$. Hence the absolute maximum is $\frac{1}{2}$ and the absolute minimum is $-\frac{1}{2}$, as these are all the critical points of $f$. This does not sit well with me, as I am unsure of my working/logic. Can this be improved on?",,['multivariable-calculus']
14,Curl and Conservative relationship specifically for the unit radial vector field,Curl and Conservative relationship specifically for the unit radial vector field,,"Consider the vector field $$\vec{F} = \frac{x}{r}\hat{x} + \frac{y}{r}\hat{y} + \frac{z}{r}\hat{z}$$ with $r = \sqrt{x^2 + y^2 + z^2}$. If I can find a function $V$ such that $\vec{F} = \nabla V$, then the unit radial vector field, by definition, will be called conservative. I find that $V = \sqrt{x^2 + y^2 + z^2}$ works and therefore, $\vec{F}$ is conservative. Now let's attack the problem using the curl. Can we use the curl to determine if $\vec{F}$ is conservative? Yes, however in addition to the curl being zero, we have to add in the necessary condition that the domain of $\vec{F}$ be simply connected. I find that $\nabla \times \vec{F} = \vec{0}$. Therefore I conclude on any simply connected domain not containing the origin, there is some potential $V$ out there. The largest simply connected domain that I could have could be, for instance, all of $\mathbb{R}^3$ except half the plane $x = 0$ (the full plane $x = 0$ is too much but if we remove just half of it, the domain is still simply connected). Question 1: In my first paragraph, I found a potential function. However, my textbook lacks details on where I can call $\vec{F}$ conservative. It basically says, if you can find a $V$, then $\vec{F} = \nabla V$ is called conservative on it's domain. It's domain is all 3-tuples of real numbers except the origin. So should I call $\vec{F}$ conservative over all of $\mathbb{R}^3$ except the origin? But my second paragraph (this one) says that I should call $\vec{F}$ conservative over any simply connected domains? Which one do I go with? Question 2: My 2nd issue comes when I look at a counterclockwise vector field divided by $r^2$ $$\vec{G} = \frac{-y}{r^2} \hat{x} + \frac{x}{r^2}\hat{y}$$ I find that $\nabla \times \vec{G} = \vec{0}$. This is good. So over any simply connected domain not containing the origin, I can call $\vec{G}$ conservative and find some potential function for it. Now let's consider for fun $$ \oint_{\text{unit circle}} \vec{G} \cdot d\vec{s} $$ For 1 revolution, this integral is $2\pi$. For $n$ revolutions, this integral is $2\pi n$. For conservative vector fields, any circulation should always give $0$. This shows us (at least somewhat) why $\vec{G}$ can't be called conservative on domains that contain the origin. But interestingly $$\oint_{\text{unit circle}} \vec{F} \cdot d\vec{s} = 0 $$ Does this mean, even though the theory says that we call $\vec{F}$ conservative over simply connected domains, the unit radial field is an exception? We can call the unit radial field conservative over all of $\mathbb{R}^3$ , including the origin ?","Consider the vector field $$\vec{F} = \frac{x}{r}\hat{x} + \frac{y}{r}\hat{y} + \frac{z}{r}\hat{z}$$ with $r = \sqrt{x^2 + y^2 + z^2}$. If I can find a function $V$ such that $\vec{F} = \nabla V$, then the unit radial vector field, by definition, will be called conservative. I find that $V = \sqrt{x^2 + y^2 + z^2}$ works and therefore, $\vec{F}$ is conservative. Now let's attack the problem using the curl. Can we use the curl to determine if $\vec{F}$ is conservative? Yes, however in addition to the curl being zero, we have to add in the necessary condition that the domain of $\vec{F}$ be simply connected. I find that $\nabla \times \vec{F} = \vec{0}$. Therefore I conclude on any simply connected domain not containing the origin, there is some potential $V$ out there. The largest simply connected domain that I could have could be, for instance, all of $\mathbb{R}^3$ except half the plane $x = 0$ (the full plane $x = 0$ is too much but if we remove just half of it, the domain is still simply connected). Question 1: In my first paragraph, I found a potential function. However, my textbook lacks details on where I can call $\vec{F}$ conservative. It basically says, if you can find a $V$, then $\vec{F} = \nabla V$ is called conservative on it's domain. It's domain is all 3-tuples of real numbers except the origin. So should I call $\vec{F}$ conservative over all of $\mathbb{R}^3$ except the origin? But my second paragraph (this one) says that I should call $\vec{F}$ conservative over any simply connected domains? Which one do I go with? Question 2: My 2nd issue comes when I look at a counterclockwise vector field divided by $r^2$ $$\vec{G} = \frac{-y}{r^2} \hat{x} + \frac{x}{r^2}\hat{y}$$ I find that $\nabla \times \vec{G} = \vec{0}$. This is good. So over any simply connected domain not containing the origin, I can call $\vec{G}$ conservative and find some potential function for it. Now let's consider for fun $$ \oint_{\text{unit circle}} \vec{G} \cdot d\vec{s} $$ For 1 revolution, this integral is $2\pi$. For $n$ revolutions, this integral is $2\pi n$. For conservative vector fields, any circulation should always give $0$. This shows us (at least somewhat) why $\vec{G}$ can't be called conservative on domains that contain the origin. But interestingly $$\oint_{\text{unit circle}} \vec{F} \cdot d\vec{s} = 0 $$ Does this mean, even though the theory says that we call $\vec{F}$ conservative over simply connected domains, the unit radial field is an exception? We can call the unit radial field conservative over all of $\mathbb{R}^3$ , including the origin ?",,"['multivariable-calculus', 'vector-fields', 'curl']"
15,how to define the divergence operator of a matrix?,how to define the divergence operator of a matrix?,,"This question related to my previous question here For vector-valued function $u(x_1,x_2)=(u_1(x_1,x_2),u_2(x_1,x_2))$ we represents  the symmetric gradient $\mathcal E$ as follows $$ \mathcal E(u)= \begin{bmatrix} \partial_1 u_1 & \frac12(\partial_2 u_1+\partial_1 u_2)\\ \frac12(\partial_2 u_1+\partial_1 u_2) & \partial_2 u_2 \end{bmatrix} $$ my question: how to define the divergence of $\mathcal E(u)$? should $div(\mathcal E(u))$ be a 2 by 2 matrix or a 2 by 1 vector?","This question related to my previous question here For vector-valued function $u(x_1,x_2)=(u_1(x_1,x_2),u_2(x_1,x_2))$ we represents  the symmetric gradient $\mathcal E$ as follows $$ \mathcal E(u)= \begin{bmatrix} \partial_1 u_1 & \frac12(\partial_2 u_1+\partial_1 u_2)\\ \frac12(\partial_2 u_1+\partial_1 u_2) & \partial_2 u_2 \end{bmatrix} $$ my question: how to define the divergence of $\mathcal E(u)$? should $div(\mathcal E(u))$ be a 2 by 2 matrix or a 2 by 1 vector?",,"['linear-algebra', 'multivariable-calculus', 'vectors', 'tensors']"
16,PDE - Wave Equation,PDE - Wave Equation,,"Let $A(x) = (a^{ij}(x))_{ij}$ , $i, j = 1, 2, 3$ be a smooth matrix-valued function on $\mathbb{R}^3$ such that $a^{ij} = a^{ji}$ , and there exists constants $c, C$ such that $c|\zeta|^2 \le \langle A(x)\zeta, \zeta \rangle \le C|\zeta|^2$ for all $(x, \zeta) \in \mathbb{R}^3$ x $\mathbb{R}^3$ . Show that if $u$ is a smooth solution to the variable-coefficient wave equation: $\partial_t ^2 u$ - div $(A \nabla u) = 0$ , such that $u$ and its derivatives decay rapidly as $|x| \rightarrow \infty$ (say, bounded by $|x|^{-3/2 - \epsilon}$ for some $\epsilon > 0$ ), then: $E(t) := \frac{1}{2} \int_{\mathbb{R}^3} (|\partial_t u|^2 + \langle A \nabla_x u, \nabla_x u \rangle) dx$ is conserved. This is what I've done so far: $E(t) := \frac{1}{2} \int_{\mathbb{R}^3} (|d_t u|^2 + \langle A \nabla u, \nabla u \rangle) dx$ = $\int_{\mathbb{R}^3}(u_t u_{tt} + \frac{1}{2} [A \nabla u_t \cdot \nabla u + A \nabla u \cdot \nabla u_t])dx$ = $\int_{\mathbb{R}^3} (u_t \nabla \cdot (A \nabla u) + \frac{1}{2}[A \nabla u_t \cdot u + A \nabla u \cdot \nabla u_t])dx$ But I don't know where to go for the next step. I am honestly completely stuck on this part. Any help/guidance on where to go next would be great. Thank you.","Let , be a smooth matrix-valued function on such that , and there exists constants such that for all x . Show that if is a smooth solution to the variable-coefficient wave equation: - div , such that and its derivatives decay rapidly as (say, bounded by for some ), then: is conserved. This is what I've done so far: = = But I don't know where to go for the next step. I am honestly completely stuck on this part. Any help/guidance on where to go next would be great. Thank you.","A(x) = (a^{ij}(x))_{ij} i, j = 1, 2, 3 \mathbb{R}^3 a^{ij} = a^{ji} c, C c|\zeta|^2 \le \langle A(x)\zeta, \zeta \rangle \le C|\zeta|^2 (x, \zeta) \in \mathbb{R}^3 \mathbb{R}^3 u \partial_t ^2 u (A \nabla u) = 0 u |x| \rightarrow \infty |x|^{-3/2 - \epsilon} \epsilon > 0 E(t) := \frac{1}{2} \int_{\mathbb{R}^3} (|\partial_t u|^2 + \langle A \nabla_x u, \nabla_x u \rangle) dx E(t) := \frac{1}{2} \int_{\mathbb{R}^3} (|d_t u|^2 + \langle A \nabla u, \nabla u \rangle) dx \int_{\mathbb{R}^3}(u_t u_{tt} + \frac{1}{2} [A \nabla u_t \cdot \nabla u + A \nabla u \cdot \nabla u_t])dx \int_{\mathbb{R}^3} (u_t \nabla \cdot (A \nabla u) + \frac{1}{2}[A \nabla u_t \cdot u + A \nabla u \cdot \nabla u_t])dx","['multivariable-calculus', 'partial-differential-equations', 'wave-equation']"
17,"Evaluate $\iint_D2x-2y \ dx \, dy$ using polar coordinates.",Evaluate  using polar coordinates.,"\iint_D2x-2y \ dx \, dy","Evaluate $$\int_\gamma y^2\,dx+x^2\,dy,$$ where $\gamma:(x-a)^2+(y-b)^2=r^2$ , running one revolution counterclockwise. I have that $(P,Q)=(y^2,x^2)$ and $\frac{\partial Q}{\partial x}=2x$ , $\frac{\partial P}{\partial y}=2y.$ By Greens theorem I have $$\int_\gamma P\,dx+Q\,dy=\iint_D\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) \ dx\,dy=\iint_D(2x-2y) \ dx \, dy.$$ Polar coordinates: $$\left\{   \begin{array}{rcr}     x & = & R\cos{\theta}+a \\     y & = & R\sin{\theta}+b\\   \end{array} \right.\implies E:\left\{   \begin{array}{rcr}     0 \leq R \leq r \\     0 \leq \theta \leq 2\pi   \end{array} \right.$$ So my integral is $$2\iint_Dx-y \ dx\,dy=\int_0^{2\pi}\int_0^r R(\cos\theta-\sin\theta) + a-b =4\pi r(a-b).$$ The correct answer is $2\pi r^2(a-b),$ I'm off by a factor of $\dfrac r2$ . Can't find the error.","Evaluate where , running one revolution counterclockwise. I have that and , By Greens theorem I have Polar coordinates: So my integral is The correct answer is I'm off by a factor of . Can't find the error.","\int_\gamma y^2\,dx+x^2\,dy, \gamma:(x-a)^2+(y-b)^2=r^2 (P,Q)=(y^2,x^2) \frac{\partial Q}{\partial x}=2x \frac{\partial P}{\partial y}=2y. \int_\gamma P\,dx+Q\,dy=\iint_D\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) \ dx\,dy=\iint_D(2x-2y) \ dx \, dy. \left\{
  \begin{array}{rcr}
    x & = & R\cos{\theta}+a \\
    y & = & R\sin{\theta}+b\\
  \end{array}
\right.\implies E:\left\{
  \begin{array}{rcr}
    0 \leq R \leq r \\
    0 \leq \theta \leq 2\pi
  \end{array}
\right. 2\iint_Dx-y \ dx\,dy=\int_0^{2\pi}\int_0^r R(\cos\theta-\sin\theta) + a-b =4\pi r(a-b). 2\pi r^2(a-b), \dfrac r2","['multivariable-calculus', 'solution-verification']"
18,If $f: \mathbb{R}^n \rightarrow \mathbb{R}$ such that $\alpha \|x-y\| \leq \|f(x)-f(y)\|$ then $f(\mathbb R^n)$ is closed.,If  such that  then  is closed.,f: \mathbb{R}^n \rightarrow \mathbb{R} \alpha \|x-y\| \leq \|f(x)-f(y)\| f(\mathbb R^n),"Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ a $\mathcal{C}^1$ function such that there exists $\alpha > 0$: $$ \alpha \|x-y\| \leq \|f(x)-f(y)\|, \forall x,y \in \mathbb{R}^n$$   1)Show that $f(\mathbb{R}^n)$ is a closed set. I don't know how to approach this exercise. I tried to  take a Cauchy sequence $(y_n) = f(x_n)$, then as $\mathbb R^n$ is a Banach space, I immediately have that $(x_n)$ converges, thus $f(x_n)$ converges in $f(\mathbb{R}^n)$, thus is closed. But I don't know how to show this rigorously, and I am unsure if my idea is correct.","Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ a $\mathcal{C}^1$ function such that there exists $\alpha > 0$: $$ \alpha \|x-y\| \leq \|f(x)-f(y)\|, \forall x,y \in \mathbb{R}^n$$   1)Show that $f(\mathbb{R}^n)$ is a closed set. I don't know how to approach this exercise. I tried to  take a Cauchy sequence $(y_n) = f(x_n)$, then as $\mathbb R^n$ is a Banach space, I immediately have that $(x_n)$ converges, thus $f(x_n)$ converges in $f(\mathbb{R}^n)$, thus is closed. But I don't know how to show this rigorously, and I am unsure if my idea is correct.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
19,Why do these vectors span the tangent plane?,Why do these vectors span the tangent plane?,,"In the question Orthonormal basis for a tangent plane , one of the commenters notes that for a manifold $M$ described by the graph of an arbitrary smooth function $f:U\subseteq \mathbb{R}^2 \to\mathbb{R}$, one possible basis of the tangent plane is given by $$ \begin{align} v_1 &= (-f_y, f_x , 0) \\ v_2 &= (f_x, f_y, f_{x}^2 + f_{y}^2) \end{align} $$ but I can't understand how those vectors were arrived at, i.e. what is meant by ''one vector along the level curves of $f$  and another vector along the direction of greatest change''. Can anyone please explain?","In the question Orthonormal basis for a tangent plane , one of the commenters notes that for a manifold $M$ described by the graph of an arbitrary smooth function $f:U\subseteq \mathbb{R}^2 \to\mathbb{R}$, one possible basis of the tangent plane is given by $$ \begin{align} v_1 &= (-f_y, f_x , 0) \\ v_2 &= (f_x, f_y, f_{x}^2 + f_{y}^2) \end{align} $$ but I can't understand how those vectors were arrived at, i.e. what is meant by ''one vector along the level curves of $f$  and another vector along the direction of greatest change''. Can anyone please explain?",,"['calculus', 'multivariable-calculus', 'differential-geometry']"
20,Is the gradient of a vector product a row or a column?,Is the gradient of a vector product a row or a column?,,"I am trying to define the gradient of $x^Ty$ with respect of $x$ where both $x, y$ are column vectors $\in \mathbb{R}^m$. $\frac{\partial x^Ty}{\partial x} = [\frac{\partial x^Ty}{\partial x_1} , \frac{\partial x^Ty}{\partial x_2} , ... , \frac{\partial x^Ty}{\partial x_m}] = [y_1, y_2, ..., y_m] = y^T$ or is it = $[\frac{\partial x^Ty}{\partial x_1} , \frac{\partial x^Ty}{\partial x_2} , ... , \frac{\partial x^Ty}{\partial x_m}]^T = [y_1, y_2, ..., y_m]^T = y$ I am quite confused since I haven't been exposed to multivariate calculus before.","I am trying to define the gradient of $x^Ty$ with respect of $x$ where both $x, y$ are column vectors $\in \mathbb{R}^m$. $\frac{\partial x^Ty}{\partial x} = [\frac{\partial x^Ty}{\partial x_1} , \frac{\partial x^Ty}{\partial x_2} , ... , \frac{\partial x^Ty}{\partial x_m}] = [y_1, y_2, ..., y_m] = y^T$ or is it = $[\frac{\partial x^Ty}{\partial x_1} , \frac{\partial x^Ty}{\partial x_2} , ... , \frac{\partial x^Ty}{\partial x_m}]^T = [y_1, y_2, ..., y_m]^T = y$ I am quite confused since I haven't been exposed to multivariate calculus before.",,"['multivariable-calculus', 'derivatives', 'matrix-calculus']"
21,How to solve this polynomial optimization problem using KKT conditions?,How to solve this polynomial optimization problem using KKT conditions?,,"I am trying to find a general recipe for a nonlinear constrained optimization problems. Please correct where wrong or where parts are missing. $$ \begin{aligned} &\min &f(x,y)&=xy+y^3\\ &\text{such that }&x &\geq y^2\\ &&x+y&\leq 2 \end{aligned} $$ Write in standard maximization problem form $$ \begin{aligned} &\max &&-xy-y^3\\ &\text{such that }&y^2-x &\leq 0\\ &&x+y&\leq 2 \end{aligned} $$ Find feasible points where Kuhn-Tucker CQ (constraint qualification) does not hold. $$ \text{gradient 1st constraint: } (-1, 2y)\\ \text{gradient 2nd constraint: } (1,1) $$ gradient 1st constraint is never linearly dependent. (never equal to $(0,0)$ for any $y$) gradient 2nd constraint is never linearly dependent. Both gradients of constraints are linearly dependent if $y=-\frac{1}{2}$ Note that the 2nd constraint in this case is binding if $x=2\frac{1}{2}$. Then the 1st constraint is not binding: $y^2-x=\frac{1}{4}-2\frac{1}{2}\neq 0$. As CQ qualification does only apply to binding constraints (and it is not binding here), we conclude CQ holds for all feasible points. What do you do if you find a feasible point where CQ does not hold? (where the point is binding for all(?) constraints) If there are more than 2 contraints, do you check them pairwise too? Write down the KT conditions First write down the Lagrangian: $\mathcal{L}=-xy-y^3-\lambda(y^2-x)-\mu(x+y-2)$ Take partial derivatives, set them equal to $0$ $$ \begin{aligned} \text{KT conditions: }&-y +\lambda-\mu&=0\\\ &-x-3y^2-2\lambda y-\mu&=0\\ &\lambda,\mu&\geq 0\\ &\lambda(y^2-x)&=0\\ &\mu(x+y-2)&=0 \end{aligned} $$ Find all points satisfying the KT conditions Especially the 3rd constraint is important, it induces 4 cases: (a) $\lambda =0,\mu=0$ (b) $\lambda =0,\mu>0$ (c) $\lambda>0, \mu=0$ (d) $\lambda>0, \mu>0$ (a): $\lambda =0,\mu=0\stackrel{eq. 1}{\implies} y=0\stackrel{eq. 2}{\implies}x=0$ which satisfy all conditions. (b):$\lambda =0,\mu>0\stackrel{eq. 5}{\implies}x+y=2\iff -x = y-2$. Also $eq. 1$ implies $y=-\mu$. We substitute this in the 2nd equation. $y-2-3y^2+y=0$, solving this gives, $$ 3y^2-2y+2=0\implies y=\frac{2\pm\sqrt{4-24}}{6}\notin\mathbb{R}\text{ contradiction} $$ (c) $\lambda>0, \mu=0\stackrel{eq. 4}{\implies} y^2-x=0\implies -x=-y^2$. Also $eq. 1$ implies $\lambda = y$. Plug this in $eq. 2$ gives, $$ -y^2-3y^2-2y^2=0\iff y=0, \text{ contradiction as }y=\lambda>0 $$ (d) $\lambda>0, \mu>0$, implies $y^2-x=0\implies x=y^2$ and $x+y-2=0$. Plug in the first in the second equation here, $y^2 + y - 2=0$ $$ y^2+y-2=0\implies y=\frac{1}{2}(-1\pm\sqrt{9})=1\text{ or } 2. $$ Then $x=1$ or $4$. Thus $(x,y)=(1,1)\text{ or }(4,-2)$. Let $(x,y)=(1,1)$, $\lambda-\mu=1\implies \lambda=1+\mu$. Plug in into $eq. 2$ $$ -1-3-2\lambda-\mu=0\\ \mu=-2 \text{ contradiction as }\mu>0 $$ Let $(x,y)=(4,-2)$, then $-\mu=-2-\lambda$ substituting gin $eq. 2$ gives $\lambda=6,\mu=8$ We now have 2 points: $(0,0)$ with $\lambda=0, \mu=0$ and $(4,-2)$ with $\lambda=6, \mu=8$ Apply Extreme Value Theorem (EVT) if possible The feasible set is compact and $xy+y^3$ is continuous so EVT applies. $$ f(0,0)=0, \quad f(4,-2)=-16\implies \min \text{ at } (4,-2) $$ Done. (questions are in italic )","I am trying to find a general recipe for a nonlinear constrained optimization problems. Please correct where wrong or where parts are missing. $$ \begin{aligned} &\min &f(x,y)&=xy+y^3\\ &\text{such that }&x &\geq y^2\\ &&x+y&\leq 2 \end{aligned} $$ Write in standard maximization problem form $$ \begin{aligned} &\max &&-xy-y^3\\ &\text{such that }&y^2-x &\leq 0\\ &&x+y&\leq 2 \end{aligned} $$ Find feasible points where Kuhn-Tucker CQ (constraint qualification) does not hold. $$ \text{gradient 1st constraint: } (-1, 2y)\\ \text{gradient 2nd constraint: } (1,1) $$ gradient 1st constraint is never linearly dependent. (never equal to $(0,0)$ for any $y$) gradient 2nd constraint is never linearly dependent. Both gradients of constraints are linearly dependent if $y=-\frac{1}{2}$ Note that the 2nd constraint in this case is binding if $x=2\frac{1}{2}$. Then the 1st constraint is not binding: $y^2-x=\frac{1}{4}-2\frac{1}{2}\neq 0$. As CQ qualification does only apply to binding constraints (and it is not binding here), we conclude CQ holds for all feasible points. What do you do if you find a feasible point where CQ does not hold? (where the point is binding for all(?) constraints) If there are more than 2 contraints, do you check them pairwise too? Write down the KT conditions First write down the Lagrangian: $\mathcal{L}=-xy-y^3-\lambda(y^2-x)-\mu(x+y-2)$ Take partial derivatives, set them equal to $0$ $$ \begin{aligned} \text{KT conditions: }&-y +\lambda-\mu&=0\\\ &-x-3y^2-2\lambda y-\mu&=0\\ &\lambda,\mu&\geq 0\\ &\lambda(y^2-x)&=0\\ &\mu(x+y-2)&=0 \end{aligned} $$ Find all points satisfying the KT conditions Especially the 3rd constraint is important, it induces 4 cases: (a) $\lambda =0,\mu=0$ (b) $\lambda =0,\mu>0$ (c) $\lambda>0, \mu=0$ (d) $\lambda>0, \mu>0$ (a): $\lambda =0,\mu=0\stackrel{eq. 1}{\implies} y=0\stackrel{eq. 2}{\implies}x=0$ which satisfy all conditions. (b):$\lambda =0,\mu>0\stackrel{eq. 5}{\implies}x+y=2\iff -x = y-2$. Also $eq. 1$ implies $y=-\mu$. We substitute this in the 2nd equation. $y-2-3y^2+y=0$, solving this gives, $$ 3y^2-2y+2=0\implies y=\frac{2\pm\sqrt{4-24}}{6}\notin\mathbb{R}\text{ contradiction} $$ (c) $\lambda>0, \mu=0\stackrel{eq. 4}{\implies} y^2-x=0\implies -x=-y^2$. Also $eq. 1$ implies $\lambda = y$. Plug this in $eq. 2$ gives, $$ -y^2-3y^2-2y^2=0\iff y=0, \text{ contradiction as }y=\lambda>0 $$ (d) $\lambda>0, \mu>0$, implies $y^2-x=0\implies x=y^2$ and $x+y-2=0$. Plug in the first in the second equation here, $y^2 + y - 2=0$ $$ y^2+y-2=0\implies y=\frac{1}{2}(-1\pm\sqrt{9})=1\text{ or } 2. $$ Then $x=1$ or $4$. Thus $(x,y)=(1,1)\text{ or }(4,-2)$. Let $(x,y)=(1,1)$, $\lambda-\mu=1\implies \lambda=1+\mu$. Plug in into $eq. 2$ $$ -1-3-2\lambda-\mu=0\\ \mu=-2 \text{ contradiction as }\mu>0 $$ Let $(x,y)=(4,-2)$, then $-\mu=-2-\lambda$ substituting gin $eq. 2$ gives $\lambda=6,\mu=8$ We now have 2 points: $(0,0)$ with $\lambda=0, \mu=0$ and $(4,-2)$ with $\lambda=6, \mu=8$ Apply Extreme Value Theorem (EVT) if possible The feasible set is compact and $xy+y^3$ is continuous so EVT applies. $$ f(0,0)=0, \quad f(4,-2)=-16\implies \min \text{ at } (4,-2) $$ Done. (questions are in italic )",,"['multivariable-calculus', 'optimization', 'nonlinear-optimization', 'maxima-minima', 'non-convex-optimization']"
22,"Show that $ \int_{S^2} (xy)^2\, dS = \frac{4\pi}{15}$",Show that," \int_{S^2} (xy)^2\, dS = \frac{4\pi}{15}","I have been constructing some examples related to spherical harmonics.  I wanted to know the $L^2$ norm of the function $f(x,y,z) \in S^2 = \{ x^2 + y^2 + z^2 = 1\}$. Wikipedia suggests this normalization: $$ \int_{S^2} (xy)^2\, dS  = \frac{4\pi}{15}$$ Are there any derivations that do not involve spherical coordinates ? I am hoping for a more algebraic derivation.  Maybe it will use spherical coordinates implicitly. \begin{eqnarray*} x &=& \cos \theta \cos \phi \\ y &=& \sin \theta \cos \phi \\ z &=& \sin \phi \end{eqnarray*} For smaller degree terms there is a trick.  I would do the integral of $1$: $$ \lvert\lvert\,1 \,\rvert\rvert^2 = \frac{1}{4\pi}\int_{S^2} 1 \, dS   = 1$$ The averages of homogenous linear terms (or cubic terms) is zero.  Quadratics have a trick: $$ \lvert\lvert\,x \,\rvert\rvert^2 = \lvert\lvert\,y \,\rvert\rvert^2 = \lvert\lvert\,z \,\rvert\rvert^2 =  \frac{1}{4\pi}\int_{S^2} x^2 \, dS   = \frac{1}{3} \frac{1}{4\pi}\int_{S^2} (x^2 + y^2 + z^2) \, dS   = \frac{1}{3}$$ I don't think there's such a trick for quadratic polynomials of 4th degree","I have been constructing some examples related to spherical harmonics.  I wanted to know the $L^2$ norm of the function $f(x,y,z) \in S^2 = \{ x^2 + y^2 + z^2 = 1\}$. Wikipedia suggests this normalization: $$ \int_{S^2} (xy)^2\, dS  = \frac{4\pi}{15}$$ Are there any derivations that do not involve spherical coordinates ? I am hoping for a more algebraic derivation.  Maybe it will use spherical coordinates implicitly. \begin{eqnarray*} x &=& \cos \theta \cos \phi \\ y &=& \sin \theta \cos \phi \\ z &=& \sin \phi \end{eqnarray*} For smaller degree terms there is a trick.  I would do the integral of $1$: $$ \lvert\lvert\,1 \,\rvert\rvert^2 = \frac{1}{4\pi}\int_{S^2} 1 \, dS   = 1$$ The averages of homogenous linear terms (or cubic terms) is zero.  Quadratics have a trick: $$ \lvert\lvert\,x \,\rvert\rvert^2 = \lvert\lvert\,y \,\rvert\rvert^2 = \lvert\lvert\,z \,\rvert\rvert^2 =  \frac{1}{4\pi}\int_{S^2} x^2 \, dS   = \frac{1}{3} \frac{1}{4\pi}\int_{S^2} (x^2 + y^2 + z^2) \, dS   = \frac{1}{3}$$ I don't think there's such a trick for quadratic polynomials of 4th degree",,"['multivariable-calculus', 'differential-geometry', 'spherical-harmonics']"
23,How to find a form whose exterior derivative is an exact differential form?,How to find a form whose exterior derivative is an exact differential form?,,"I think it will help me if look at examples. So, in other words, let's say we have an exact differential form: $$\alpha = (yz-z)dx + (xz+z)dy + (xy-x+y)dz$$ How can I find a differential form $\omega$ such that the exterior derivative of $\omega$ is $\alpha$? Is it just integration? And if so, how would that look? I don't yet have a good understanding of integrating differential forms. And is it a different or more difficult process if we have an exact 2-form? For instance, say we have the 2-form: $$\beta = 2xy^2 dxdy +z dydz$$ Thanks very much!","I think it will help me if look at examples. So, in other words, let's say we have an exact differential form: $$\alpha = (yz-z)dx + (xz+z)dy + (xy-x+y)dz$$ How can I find a differential form $\omega$ such that the exterior derivative of $\omega$ is $\alpha$? Is it just integration? And if so, how would that look? I don't yet have a good understanding of integrating differential forms. And is it a different or more difficult process if we have an exact 2-form? For instance, say we have the 2-form: $$\beta = 2xy^2 dxdy +z dydz$$ Thanks very much!",,"['multivariable-calculus', 'differential-forms']"
24,"How does $F(X_1,X_2)$ change with $\epsilon$ change in $X_1$ when $X_1$ and $X_2$ are entangled?",How does  change with  change in  when  and  are entangled?,"F(X_1,X_2) \epsilon X_1 X_1 X_2","I have the following function $F$ $$F(X_1,X_2)=\frac{X_1}{X_1+X_2}$$ Where $X_1$ and $X_2$ are functions of a variable $k$. I'm trying to investigate how $F$ changes with a $\epsilon$ change in $X_1$. However, I cannot simply do $$F(X_1+\epsilon,X_2)=\frac{X_1+\epsilon}{X_1+\epsilon+X_2}$$ Since if $X_1$ changes (due to a change in $k$) $X_2$ will also change. Hence, it makes sense (to me) to do the following. Define, $A:=\frac{dX_1}{dk}$ and $B:=\frac{dX_2}{dk}$. Then we have $$F(X_1+\epsilon,X_2)==\frac{X_1+\epsilon}{X_1+\epsilon+X_2+\epsilon(B/A)}$$ For instance if $B=2A$, then a change of $\epsilon$ in $X_1$ will come with a $2\epsilon$ change in $X_2$. Does this make sense?","I have the following function $F$ $$F(X_1,X_2)=\frac{X_1}{X_1+X_2}$$ Where $X_1$ and $X_2$ are functions of a variable $k$. I'm trying to investigate how $F$ changes with a $\epsilon$ change in $X_1$. However, I cannot simply do $$F(X_1+\epsilon,X_2)=\frac{X_1+\epsilon}{X_1+\epsilon+X_2}$$ Since if $X_1$ changes (due to a change in $k$) $X_2$ will also change. Hence, it makes sense (to me) to do the following. Define, $A:=\frac{dX_1}{dk}$ and $B:=\frac{dX_2}{dk}$. Then we have $$F(X_1+\epsilon,X_2)==\frac{X_1+\epsilon}{X_1+\epsilon+X_2+\epsilon(B/A)}$$ For instance if $B=2A$, then a change of $\epsilon$ in $X_1$ will come with a $2\epsilon$ change in $X_2$. Does this make sense?",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
25,Curl-less fields and divergence-less fields,Curl-less fields and divergence-less fields,,"I was wondering if anyone could explain some intuition or proof of the two following statements. $\nabla \times \textbf{F} = 0 \leftrightarrow \textbf{F} = \nabla V$ $\nabla \cdot \textbf{G} = 0 \leftrightarrow \textbf{G} = \nabla \times \textbf{A}$ I understand what they imply when the fundamental theorems of calculus are considered, I am just curious to why one implies the other.","I was wondering if anyone could explain some intuition or proof of the two following statements. $\nabla \times \textbf{F} = 0 \leftrightarrow \textbf{F} = \nabla V$ $\nabla \cdot \textbf{G} = 0 \leftrightarrow \textbf{G} = \nabla \times \textbf{A}$ I understand what they imply when the fundamental theorems of calculus are considered, I am just curious to why one implies the other.",,"['multivariable-calculus', 'vectors', 'vector-analysis', 'divergence-operator']"
26,Mixed Derivative bounded by Laplacian in 2D?,Mixed Derivative bounded by Laplacian in 2D?,,"Suppose that I have a $C^2$ odd periodic function $u$ on $[0,1]^2$. Is it true that $\| \partial_1 \partial_2 u \|_\infty \leq C \|\Delta u\|_\infty$ for some constant $C$ independent of $u$? The norm is the usual sup norm: $\|u\|_\infty = \text{sup}_{x\in [0,1]^2} |u(x)|$.","Suppose that I have a $C^2$ odd periodic function $u$ on $[0,1]^2$. Is it true that $\| \partial_1 \partial_2 u \|_\infty \leq C \|\Delta u\|_\infty$ for some constant $C$ independent of $u$? The norm is the usual sup norm: $\|u\|_\infty = \text{sup}_{x\in [0,1]^2} |u(x)|$.",,"['multivariable-calculus', 'inequality', 'normed-spaces', 'harmonic-analysis', 'laplacian']"
27,Find the maximum or minimum values of $x^2 + y^2 + z^2$ subject to the conditions $ax^2 + by^2 + cz^2 = 1$ and $lx + my + nz = 0$?,Find the maximum or minimum values of  subject to the conditions  and ?,x^2 + y^2 + z^2 ax^2 + by^2 + cz^2 = 1 lx + my + nz = 0,"Further it is asked to interpret the result geometrically. I attempted this using Lagrange multiplier method but it becomes very difficult to solve as there are so many variables, it simply seems unmanageable. I want to know if there is an alternative to such problems other than the Lagrange method. My approach: $F(x,y,z,λ,μ) = x^2+y^2+z^2 + λ(ax^2+by^2+cz^2)+μ(lx + my+nz)$. Then I solved the following equations to get the extreme points $\partial F/\partial x = 0$ $\partial F/\partial y = 0$ $\partial F/\partial z = 0$ $\partial F/\partial λ = 0$ $\partial F/\partial μ = 0$ and then solving above five to get μ and λ and then calculate $(x,y,z)$ for the extreme points accordingly. It becomes too cumbersome to manage all these variables. I want to know if there is any other way to arrive at the solution a bit more comfortably. Thanks. P.S. I am not writing the entire solution here for obvious reasons. I can share the image of the same if required. My understanding of geometric interpretation is that it is asked to find the nearest and farthest point from origin where the ellipsoid and the plane intersects?","Further it is asked to interpret the result geometrically. I attempted this using Lagrange multiplier method but it becomes very difficult to solve as there are so many variables, it simply seems unmanageable. I want to know if there is an alternative to such problems other than the Lagrange method. My approach: $F(x,y,z,λ,μ) = x^2+y^2+z^2 + λ(ax^2+by^2+cz^2)+μ(lx + my+nz)$. Then I solved the following equations to get the extreme points $\partial F/\partial x = 0$ $\partial F/\partial y = 0$ $\partial F/\partial z = 0$ $\partial F/\partial λ = 0$ $\partial F/\partial μ = 0$ and then solving above five to get μ and λ and then calculate $(x,y,z)$ for the extreme points accordingly. It becomes too cumbersome to manage all these variables. I want to know if there is any other way to arrive at the solution a bit more comfortably. Thanks. P.S. I am not writing the entire solution here for obvious reasons. I can share the image of the same if required. My understanding of geometric interpretation is that it is asked to find the nearest and farthest point from origin where the ellipsoid and the plane intersects?",,"['calculus', 'multivariable-calculus', 'lagrange-multiplier', 'maxima-minima']"
28,Calculating the volume element,Calculating the volume element,,"Let $M$ be a smooth oriented $k$-manifold on $\mathbb{R}^n$, with a parametrization $g\colon U \to \mathbb{R}^n$ where $U \subseteq \mathbb{R}^k$. Suppose we have a continuous function $f\colon \mathbb{R}^n \to \mathbb{R}$ with compact support, and we wish to integrate it over $M$. We want then to calculate: $$\int_M f dV$$ Where $dV$ is a volume element, a differential $k$-form particular to this manifold $M$. I've read on wikipedia that the pullback of the volume element is given by: $$g^*(dV) = \left|g'^T \cdot g'\right|dx_1 \wedge\cdots \wedge dx_k$$ Where $dx_i$ are the usual projection forms in $\mathbb{R}^k$ and $g'$ is the Jacobian matrix of $g$. I would like to understand why though. I'm not even sure what a formal definition of the volume element of a $k$-manifold on $\mathbb{R}^n$ is. Sure, if $k = n$, then this will simply be $dV = dx_1\wedge \cdots \wedge dx_n$ with the projections happening in $\mathbb{R}^n$. The pullback is easy to calculate then, as it commutes with exterior products and exterior derivatives. Outside this realm, I'm quite lost on what's going on. What is the length element of a curve in $\mathbb{R}^2$? Or the area element of a surface in $\mathbb{R}^3$? How does all of this generalizes to $k$-manifolds over the $n$-space?","Let $M$ be a smooth oriented $k$-manifold on $\mathbb{R}^n$, with a parametrization $g\colon U \to \mathbb{R}^n$ where $U \subseteq \mathbb{R}^k$. Suppose we have a continuous function $f\colon \mathbb{R}^n \to \mathbb{R}$ with compact support, and we wish to integrate it over $M$. We want then to calculate: $$\int_M f dV$$ Where $dV$ is a volume element, a differential $k$-form particular to this manifold $M$. I've read on wikipedia that the pullback of the volume element is given by: $$g^*(dV) = \left|g'^T \cdot g'\right|dx_1 \wedge\cdots \wedge dx_k$$ Where $dx_i$ are the usual projection forms in $\mathbb{R}^k$ and $g'$ is the Jacobian matrix of $g$. I would like to understand why though. I'm not even sure what a formal definition of the volume element of a $k$-manifold on $\mathbb{R}^n$ is. Sure, if $k = n$, then this will simply be $dV = dx_1\wedge \cdots \wedge dx_n$ with the projections happening in $\mathbb{R}^n$. The pullback is easy to calculate then, as it commutes with exterior products and exterior derivatives. Outside this realm, I'm quite lost on what's going on. What is the length element of a curve in $\mathbb{R}^2$? Or the area element of a surface in $\mathbb{R}^3$? How does all of this generalizes to $k$-manifolds over the $n$-space?",,"['integration', 'multivariable-calculus', 'manifolds', 'smooth-manifolds', 'differential-forms']"
29,Intuition behind grad operator,Intuition behind grad operator,,"I'm pretty sure this question has a very simple answer and I am just not understanding the geometric picture very well, however I can across a question on the gradient operator which made me a bit unsure as to what it does. The gradient of a scalar field points in the direction of greatest rate of change. Now if I have a surface given by $f(x,y,z)$, then the normal to the sruface can be found by taking the gradient of $f$ at that point. Intuitively, this also makes sense to me. The change is zero along the tangent plane, so the greatest rate of change must be normal to the surface. However I was doing a question which gave a surface and then asked in what direction a marble would roll if placed at a particular point on the surface. Now I initially thought that the marble would roll in the direction of greatest rate of change- i.e. a ball on a hill will roll down where the potential will decrease most rapidly. I would take the gradient and then take the direction in which f was decreasing as opposed to increasing. However if the gradient is normal to the surface, then a marble would be ejected directly off of the surface? This is not what happens! Now I have a hunch as to what might be the problem here although I am not sure. I think the problem is two-fold. First, the mathematical description allows the possibility of the marble passing through the surface or coming off of it. Secondly, I think maybe I am mixing up surfaces and scalar fields which are not surfaces. Perhaps in the physical case I actually have a scalar field (not a surface) describing the potential energy AND a surface on which the marble is constrained to move, and the marble will move in the direction of greatest rate of decrease of potential energy while still being constrained on the surface. That seems about I think right...","I'm pretty sure this question has a very simple answer and I am just not understanding the geometric picture very well, however I can across a question on the gradient operator which made me a bit unsure as to what it does. The gradient of a scalar field points in the direction of greatest rate of change. Now if I have a surface given by $f(x,y,z)$, then the normal to the sruface can be found by taking the gradient of $f$ at that point. Intuitively, this also makes sense to me. The change is zero along the tangent plane, so the greatest rate of change must be normal to the surface. However I was doing a question which gave a surface and then asked in what direction a marble would roll if placed at a particular point on the surface. Now I initially thought that the marble would roll in the direction of greatest rate of change- i.e. a ball on a hill will roll down where the potential will decrease most rapidly. I would take the gradient and then take the direction in which f was decreasing as opposed to increasing. However if the gradient is normal to the surface, then a marble would be ejected directly off of the surface? This is not what happens! Now I have a hunch as to what might be the problem here although I am not sure. I think the problem is two-fold. First, the mathematical description allows the possibility of the marble passing through the surface or coming off of it. Secondly, I think maybe I am mixing up surfaces and scalar fields which are not surfaces. Perhaps in the physical case I actually have a scalar field (not a surface) describing the potential energy AND a surface on which the marble is constrained to move, and the marble will move in the direction of greatest rate of decrease of potential energy while still being constrained on the surface. That seems about I think right...",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
30,Closest point on a plane to a surface. And vice versa.,Closest point on a plane to a surface. And vice versa.,,"Find the point on $z=1-2x^2-y^2$ closest to $2x+3y+z=12$ using Lagrange multipliers. Point on surface closest to a plane using Lagrange multipliers Although the methods used in the answers are helpful and do work, my professor told me that the way he wants us to do this problem is to recognize that the distance will be minimized when the normal vector to one point on the surface is parallel to the normal vector of the plane. To find the normal vector to the surface at a point $(x,y,z)$ we write our function implicitly and take the gradient of the new function (gradient is parallel to level curve): $$G(x,y,z,)=z+2x^2+y^2=1$$ $$\nabla G=\langle4x,2y,1\rangle$$ The normal vector of the plane is, $$\langle 2,3,1 \rangle$$ Hence we have, $$\langle 4x,2y,1 \rangle=\lambda \langle 2,3,1 \rangle$$ And that, $$z+2x^2+y^2=1$$ Question $1$: The part I don't understand is the claim that the distance will be minimized when the normals are parallel. And how is this using Lagrange multipliers, May someone please explain. Question $2$ Find the point on $2x+3y+z=12$ closest to $z=1-2x^2-y^2$ using Lagrange multipliers. I suppose we have use the fact that the normal of the plane is $\langle 2,3,1 \rangle$ with the fact that the point closest to the plane on the surface is $(\frac{1}{2},\frac{3}{2},-\frac{7}{4})$  to come up with the equation of the line that goes through our two closest points in terms of $t$ and then substitute values of $x$, $y$, $z$ into our plane equation to come up with the point on the plane. But again, I don't see where Lagrange multipliers comes into play.","Find the point on $z=1-2x^2-y^2$ closest to $2x+3y+z=12$ using Lagrange multipliers. Point on surface closest to a plane using Lagrange multipliers Although the methods used in the answers are helpful and do work, my professor told me that the way he wants us to do this problem is to recognize that the distance will be minimized when the normal vector to one point on the surface is parallel to the normal vector of the plane. To find the normal vector to the surface at a point $(x,y,z)$ we write our function implicitly and take the gradient of the new function (gradient is parallel to level curve): $$G(x,y,z,)=z+2x^2+y^2=1$$ $$\nabla G=\langle4x,2y,1\rangle$$ The normal vector of the plane is, $$\langle 2,3,1 \rangle$$ Hence we have, $$\langle 4x,2y,1 \rangle=\lambda \langle 2,3,1 \rangle$$ And that, $$z+2x^2+y^2=1$$ Question $1$: The part I don't understand is the claim that the distance will be minimized when the normals are parallel. And how is this using Lagrange multipliers, May someone please explain. Question $2$ Find the point on $2x+3y+z=12$ closest to $z=1-2x^2-y^2$ using Lagrange multipliers. I suppose we have use the fact that the normal of the plane is $\langle 2,3,1 \rangle$ with the fact that the point closest to the plane on the surface is $(\frac{1}{2},\frac{3}{2},-\frac{7}{4})$  to come up with the equation of the line that goes through our two closest points in terms of $t$ and then substitute values of $x$, $y$, $z$ into our plane equation to come up with the point on the plane. But again, I don't see where Lagrange multipliers comes into play.",,['multivariable-calculus']
31,Generalized Dirichlet Energy,Generalized Dirichlet Energy,,"Question What is the corresponding Euler-Lagrange equation that minimizes the energy $$E[u] = \frac{1}{2}\int_{\Omega} \|\nabla^k u \|^2 \> dx$$ where $\Omega \subseteq \mathbb{R}^n$ and $u: \Omega  \mapsto \mathbb{R}$. Conjecture $$\Delta^{k} u = 0$$ Background For the standard Dirichlet Energy $$E[u] = \frac{1}{2}\int_{\Omega} \|\nabla u \|^2 \> dV$$ we can prove that harmonic functions minimize this energy. A standard way to do this is to assume we have a function $w$ such that $\Delta w = 0$ for $x \in \Omega$. This means $$\int_\Omega \Delta w(w-u) \> dx = 0$$ $$\int_\Omega \nabla u \cdot \nabla w \> dx - \int_\Omega \|\nabla w\|^2 \> dx = 0$$ $$\frac{1}{2} \int_\Omega \|\nabla u\|^2 + \|\nabla w\|^2 \> dx - \int_\Omega \|\nabla w\|^2 \> dx \ge 0$$ $$\frac{1}{2} \int_\Omega \|\nabla u\|^2  \ge \frac{1}{2} \int_\Omega \|\nabla w\|^2 \> dx$$ Since we didn't lose generality, this argument works for any choice of $u$. I suspect a similar technique should show the more general version, but I haven't been able to work out the identities. The reason I am interested is this more general energy can be used to enforce more continuity at the handles of a mesh deformation. I'm going to keep looking into this and maybe post my own answer if no one beats me to it.","Question What is the corresponding Euler-Lagrange equation that minimizes the energy $$E[u] = \frac{1}{2}\int_{\Omega} \|\nabla^k u \|^2 \> dx$$ where $\Omega \subseteq \mathbb{R}^n$ and $u: \Omega  \mapsto \mathbb{R}$. Conjecture $$\Delta^{k} u = 0$$ Background For the standard Dirichlet Energy $$E[u] = \frac{1}{2}\int_{\Omega} \|\nabla u \|^2 \> dV$$ we can prove that harmonic functions minimize this energy. A standard way to do this is to assume we have a function $w$ such that $\Delta w = 0$ for $x \in \Omega$. This means $$\int_\Omega \Delta w(w-u) \> dx = 0$$ $$\int_\Omega \nabla u \cdot \nabla w \> dx - \int_\Omega \|\nabla w\|^2 \> dx = 0$$ $$\frac{1}{2} \int_\Omega \|\nabla u\|^2 + \|\nabla w\|^2 \> dx - \int_\Omega \|\nabla w\|^2 \> dx \ge 0$$ $$\frac{1}{2} \int_\Omega \|\nabla u\|^2  \ge \frac{1}{2} \int_\Omega \|\nabla w\|^2 \> dx$$ Since we didn't lose generality, this argument works for any choice of $u$. I suspect a similar technique should show the more general version, but I haven't been able to work out the identities. The reason I am interested is this more general energy can be used to enforce more continuity at the handles of a mesh deformation. I'm going to keep looking into this and maybe post my own answer if no one beats me to it.",,"['multivariable-calculus', 'partial-differential-equations', 'calculus-of-variations']"
32,Textbook for Multivariable and/or Vector Calculus,Textbook for Multivariable and/or Vector Calculus,,"I'm looking for a tetxbook that covers Multivariable Calculus and/or Vector Calculus theoretically. I have done Analysis (single-variable) at the level of Introduction to Real Analysis by Bartle and Sherbet and Principles of Mathematical Analysis by Walter Rudin. Except for the theory of integration. I haven't done a lot of exercises from ""Baby Rudin"" but I have done a course on Topology so some of the exercises will be approachable now, though I may have to go and revise Topology to refresh my concepts/recall the theorems. Anyway, I'm looking for a textbook that extends the material in the aforementioned books to Multivariable and/or Vector Calculus. Any suggestions?","I'm looking for a tetxbook that covers Multivariable Calculus and/or Vector Calculus theoretically. I have done Analysis (single-variable) at the level of Introduction to Real Analysis by Bartle and Sherbet and Principles of Mathematical Analysis by Walter Rudin. Except for the theory of integration. I haven't done a lot of exercises from ""Baby Rudin"" but I have done a course on Topology so some of the exercises will be approachable now, though I may have to go and revise Topology to refresh my concepts/recall the theorems. Anyway, I'm looking for a textbook that extends the material in the aforementioned books to Multivariable and/or Vector Calculus. Any suggestions?",,"['multivariable-calculus', 'reference-request', 'book-recommendation']"
33,How can I solve this triple integral $\iiint_{B} y\;dxdydz$ on a defined set?,How can I solve this triple integral  on a defined set?,\iiint_{B} y\;dxdydz,"Calculate $$\iiint_{B} y\;dxdydz.$$ The set is $\;B=\{(x,y,z) \in \mathbb R^3$; $\; x^2+y^2+4z^2\le12$, $-x^2+y^2+4z^2\le6$, $y\ge 0 \}$. I know that B is defined by a real ellipsoid, an elliptical hyperboloid and by the positive half-space of y, so I tried to use the cylindrical coordinate system but I can't find the correct limits of integration. How can I change the equations? I hope you'll help me. Thanks a lot!","Calculate $$\iiint_{B} y\;dxdydz.$$ The set is $\;B=\{(x,y,z) \in \mathbb R^3$; $\; x^2+y^2+4z^2\le12$, $-x^2+y^2+4z^2\le6$, $y\ge 0 \}$. I know that B is defined by a real ellipsoid, an elliptical hyperboloid and by the positive half-space of y, so I tried to use the cylindrical coordinate system but I can't find the correct limits of integration. How can I change the equations? I hope you'll help me. Thanks a lot!",,"['calculus', 'integration', 'multivariable-calculus', 'lebesgue-integral', 'multiple-integral']"
34,"How is ""expressing"" a differential operator ""in cylindrical coordinates"" rigorously defined?","How is ""expressing"" a differential operator ""in cylindrical coordinates"" rigorously defined?",,"I'm a mathematician (with little knowledge of differential geometry) trying to study physics. One of the greatest problems is the language regarding coordinate transformations. I tend to think of such transformations as functions (diffeomorphisms), whereas physicists just rename the arguments, e.g. $f(x,y,z)=f(\rho,\varphi,z)$. I've gotten used to that and for some things (e.g. integration) it works just fine. But I can't get my head around the transformation (?) of differential operators. For example: Let $f:\mathbb{R}^3\backslash\big(\{0\}\times\{0\}\times\mathbb{R}\big)\rightarrow\mathbb{R}$ be smooth. I have before me the statement that ""in cyclindrical coordinates"" $$\nabla f=\vec{\mathbf{e}}_\rho\frac{\partial f}{\partial\rho}+\vec{\mathbf{e}}_\varphi\frac{1}{\rho}\frac{\partial f}{\partial\varphi}+\vec{\mathbf{e}}_z\frac{\partial f}{\partial z}.$$ Now you probably consider me a pedant, but I can only try to understand that by introducing the mapping $\theta:]0,\infty[\times\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}^3$, $$\theta(\rho,\varphi,z)=(\rho\cos\varphi,\rho\sin\varphi,z).$$ My understanding is that the above equation is actually $$\nabla(f\circ\theta)=\vec{\mathbf{e}}_\rho\partial_1(f\circ\theta)+\vec{\mathbf{e}}_\varphi\frac{1}{\rho}\partial_2(f\circ\theta)+\vec{\mathbf{e}}_z\partial_3(f\circ\theta).$$ Or is it $\left(\nabla f\right)\circ\theta$ instead of $\nabla(f\circ\theta)$? And what are these vectors $\vec{\mathbf{e}}_\rho,\vec{\mathbf{e}}_\varphi,\vec{\mathbf{e}}_z$? I read that $\vec{\mathbf{e}}_\varphi$ is the ""unit vector in $\varphi$-direction"". But what does that even mean? How can one express these ""unit vectors"" using $\theta$? There is a lot in the literature about how to derive such equations, but I can't really use it because I don't understand the meaning behind the symbols and I don't really understand the point of it all.","I'm a mathematician (with little knowledge of differential geometry) trying to study physics. One of the greatest problems is the language regarding coordinate transformations. I tend to think of such transformations as functions (diffeomorphisms), whereas physicists just rename the arguments, e.g. $f(x,y,z)=f(\rho,\varphi,z)$. I've gotten used to that and for some things (e.g. integration) it works just fine. But I can't get my head around the transformation (?) of differential operators. For example: Let $f:\mathbb{R}^3\backslash\big(\{0\}\times\{0\}\times\mathbb{R}\big)\rightarrow\mathbb{R}$ be smooth. I have before me the statement that ""in cyclindrical coordinates"" $$\nabla f=\vec{\mathbf{e}}_\rho\frac{\partial f}{\partial\rho}+\vec{\mathbf{e}}_\varphi\frac{1}{\rho}\frac{\partial f}{\partial\varphi}+\vec{\mathbf{e}}_z\frac{\partial f}{\partial z}.$$ Now you probably consider me a pedant, but I can only try to understand that by introducing the mapping $\theta:]0,\infty[\times\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}^3$, $$\theta(\rho,\varphi,z)=(\rho\cos\varphi,\rho\sin\varphi,z).$$ My understanding is that the above equation is actually $$\nabla(f\circ\theta)=\vec{\mathbf{e}}_\rho\partial_1(f\circ\theta)+\vec{\mathbf{e}}_\varphi\frac{1}{\rho}\partial_2(f\circ\theta)+\vec{\mathbf{e}}_z\partial_3(f\circ\theta).$$ Or is it $\left(\nabla f\right)\circ\theta$ instead of $\nabla(f\circ\theta)$? And what are these vectors $\vec{\mathbf{e}}_\rho,\vec{\mathbf{e}}_\varphi,\vec{\mathbf{e}}_z$? I read that $\vec{\mathbf{e}}_\varphi$ is the ""unit vector in $\varphi$-direction"". But what does that even mean? How can one express these ""unit vectors"" using $\theta$? There is a lot in the literature about how to derive such equations, but I can't really use it because I don't understand the meaning behind the symbols and I don't really understand the point of it all.",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
35,What is the difference between the gradient and the gradient vector?,What is the difference between the gradient and the gradient vector?,,"Here is the where my confusion starts.... When they are speaking of curves, they are using the terms ""gradient"" and ""slope"" for slope. But when it comes to surfaces, it was stated that the gradient will give the normal vector to a surface... In the first case it was related to something about the slope of the tangent, and in the second case it was related to something about normal vector. I am getting confused. Is it possible for some one to clear on this topic..","Here is the where my confusion starts.... When they are speaking of curves, they are using the terms ""gradient"" and ""slope"" for slope. But when it comes to surfaces, it was stated that the gradient will give the normal vector to a surface... In the first case it was related to something about the slope of the tangent, and in the second case it was related to something about normal vector. I am getting confused. Is it possible for some one to clear on this topic..",,"['multivariable-calculus', 'tangent-line']"
36,"$\lim_{(x,y)\to (0,0)} \frac{\sin(x^2+y^2)}{x^2+y^2}$",,"\lim_{(x,y)\to (0,0)} \frac{\sin(x^2+y^2)}{x^2+y^2}","I must admit that I've forgotten how to do multivariable limits.  Nevertheless I need to know whether the following exists: $$\lim_{(x,y)\to (0,0)} \frac{\sin(x^2+y^2)}{x^2+y^2}$$ Would it be as simple as defining a function $(x^2+y^2)\mapsto z$.  Then $$\lim_{(x,y)\to (0,0)} \frac{\sin(x^2+y^2)}{x^2+y^2} = \lim_{z\to 0^+} \frac{\sin(z)}{z} = 1?$$","I must admit that I've forgotten how to do multivariable limits.  Nevertheless I need to know whether the following exists: $$\lim_{(x,y)\to (0,0)} \frac{\sin(x^2+y^2)}{x^2+y^2}$$ Would it be as simple as defining a function $(x^2+y^2)\mapsto z$.  Then $$\lim_{(x,y)\to (0,0)} \frac{\sin(x^2+y^2)}{x^2+y^2} = \lim_{z\to 0^+} \frac{\sin(z)}{z} = 1?$$",,['multivariable-calculus']
37,Can a two-variables function be an odd function in one variable?,Can a two-variables function be an odd function in one variable?,,"Like in single variable, we use $f(-x)=-f(x)$ to show that a function is odd. Similarly, for two variables, we can use $f(-x,-y)=-f(x,y)$. If we have a two variable function like this $f(x,y)=x\cos({\sqrt{x^2+(y+a)^2}})$. So, $f(-x,y)=(-x)\cos({\sqrt{(-x)^2+(y+a)^2}})=-f(x,y)$. Can we say this is an odd function only in $x$?","Like in single variable, we use $f(-x)=-f(x)$ to show that a function is odd. Similarly, for two variables, we can use $f(-x,-y)=-f(x,y)$. If we have a two variable function like this $f(x,y)=x\cos({\sqrt{x^2+(y+a)^2}})$. So, $f(-x,y)=(-x)\cos({\sqrt{(-x)^2+(y+a)^2}})=-f(x,y)$. Can we say this is an odd function only in $x$?",,['multivariable-calculus']
38,Cannot Find Mistake In Lagrange Multipliers Problem,Cannot Find Mistake In Lagrange Multipliers Problem,,"I am working on a problem asking me to use the method of constrained extrema to find the global maximum and minimum for the following functions: $$f(x,y) = xy$$ constrained to  $$g(x,y) = 4x^2 + 2xy + y^2 - 4$$ So far I have done the calculations and solved the problem and have come up with an answer that does not make sense however I cannot see where I may have made a calculation error. The first thing I did was find the gradient of both f(x,y) and g(x,y) and equate them to get the following result: $$ y = 8x\lambda + 2y\lambda$$ $$ x = 2x\lambda + 2y\lambda$$ I multiplied the first equation by x and the second equation by y then divided both by lambda to get the following: $$ \frac{xy}{\lambda} = 8x^2 + 2xy$$ $$ \frac{xy}{\lambda} = 2xy + 2y^2$$ As the left hand side of both equations are equal I produced the following: $$ 8x^2 + 2xy = 2xy + 2y^2$$ I then simplified this and did the algebra to come up with a solution y = 2x which I then substituted back into g(x,y) and got the solution $$ x = \pm \frac{2}{\sqrt{12}}$$ And I believe this result is incorrect as when you plug these values into f(x,y) you get the same answer of one third for both values. This must be incorrect as one of these values should be a maximum and the other a minimum as asked in the question. Thank you, Michael","I am working on a problem asking me to use the method of constrained extrema to find the global maximum and minimum for the following functions: $$f(x,y) = xy$$ constrained to  $$g(x,y) = 4x^2 + 2xy + y^2 - 4$$ So far I have done the calculations and solved the problem and have come up with an answer that does not make sense however I cannot see where I may have made a calculation error. The first thing I did was find the gradient of both f(x,y) and g(x,y) and equate them to get the following result: $$ y = 8x\lambda + 2y\lambda$$ $$ x = 2x\lambda + 2y\lambda$$ I multiplied the first equation by x and the second equation by y then divided both by lambda to get the following: $$ \frac{xy}{\lambda} = 8x^2 + 2xy$$ $$ \frac{xy}{\lambda} = 2xy + 2y^2$$ As the left hand side of both equations are equal I produced the following: $$ 8x^2 + 2xy = 2xy + 2y^2$$ I then simplified this and did the algebra to come up with a solution y = 2x which I then substituted back into g(x,y) and got the solution $$ x = \pm \frac{2}{\sqrt{12}}$$ And I believe this result is incorrect as when you plug these values into f(x,y) you get the same answer of one third for both values. This must be incorrect as one of these values should be a maximum and the other a minimum as asked in the question. Thank you, Michael",,"['multivariable-calculus', 'lagrange-multiplier']"
39,What is the most general way to think about Integrals?,What is the most general way to think about Integrals?,,"Given a single-variable scalar function, $f : \mathbb{R} \to \mathbb{R}$ The ""area under the curve"" (of the graph of the function $f$ in $\mathbb{R^2}$) is given by $$\int_{a}^{b} f(x) \ dx = Area$$ Given a multi-variable scalar function, $g: \mathbb{R^2} \to \mathbb{R}$ The ""volume under the curve"" (of the graph of the function $g$ in $\mathbb{R^3}$) is given by $$\int\int_{a}^{b} g(x, y) \ dx \ dy = Volume$$ But when thinking about integrals in this sense as translating from $\ Lines \to Areas \to Volumes$, (i.e. taking the Riemann Sum of an infinite number of infinitesimal line segments to get an area, or taking the Riemann Sum of an infinite number of infinitesimal areas to get a volume) it's a pretty ""applied"" approach. I mean you need a to plot the function $f$ as a graph for explanations such as ""area under the curve"" or ""volume under the curve"" to make any sort of sense. I'm sure that this way of thinking about Integrals breaks down at some point. For example, what would the double integral of the single-variable function I gave in the first example ""represent"" . $$\int\int_{a}^{b} f(x) \ dx^2 = \ ???$$ The graph of $f$ can be plotted in $\mathbb{R^2}$, by means of a vector-function (correct me if I'm wrong here), however the graph of $f$ only exists in $\mathbb{R^2}$, and ""Volumes"" only have any meaning in $\mathbb{R^3}$ so there can be no way that the double integral of a single-variable scalar function could represent a ""volume under the curve"" My Question Is there a more ""Pure"" Mathematical approach to thinking about integrals? Because I'm sure that this more ""applied"" way of thinking about Integrals cannot be the most general. What would be the most general, and pure mathematical way to think about Integrals, and specifically Multiple Integrals? If you have spotted any gaps in my understanding, please feel free to comment below, as an undergraduate student, majoring in Pure Mathematics, I'm always looking to improve.","Given a single-variable scalar function, $f : \mathbb{R} \to \mathbb{R}$ The ""area under the curve"" (of the graph of the function $f$ in $\mathbb{R^2}$) is given by $$\int_{a}^{b} f(x) \ dx = Area$$ Given a multi-variable scalar function, $g: \mathbb{R^2} \to \mathbb{R}$ The ""volume under the curve"" (of the graph of the function $g$ in $\mathbb{R^3}$) is given by $$\int\int_{a}^{b} g(x, y) \ dx \ dy = Volume$$ But when thinking about integrals in this sense as translating from $\ Lines \to Areas \to Volumes$, (i.e. taking the Riemann Sum of an infinite number of infinitesimal line segments to get an area, or taking the Riemann Sum of an infinite number of infinitesimal areas to get a volume) it's a pretty ""applied"" approach. I mean you need a to plot the function $f$ as a graph for explanations such as ""area under the curve"" or ""volume under the curve"" to make any sort of sense. I'm sure that this way of thinking about Integrals breaks down at some point. For example, what would the double integral of the single-variable function I gave in the first example ""represent"" . $$\int\int_{a}^{b} f(x) \ dx^2 = \ ???$$ The graph of $f$ can be plotted in $\mathbb{R^2}$, by means of a vector-function (correct me if I'm wrong here), however the graph of $f$ only exists in $\mathbb{R^2}$, and ""Volumes"" only have any meaning in $\mathbb{R^3}$ so there can be no way that the double integral of a single-variable scalar function could represent a ""volume under the curve"" My Question Is there a more ""Pure"" Mathematical approach to thinking about integrals? Because I'm sure that this more ""applied"" way of thinking about Integrals cannot be the most general. What would be the most general, and pure mathematical way to think about Integrals, and specifically Multiple Integrals? If you have spotted any gaps in my understanding, please feel free to comment below, as an undergraduate student, majoring in Pure Mathematics, I'm always looking to improve.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'soft-question']"
40,Difference between Advanced Calculus and Calculus on Manifolds?,Difference between Advanced Calculus and Calculus on Manifolds?,,"This is an interesting distinction that I don't fully grasp yet. There's quite some books on the topic of the so-called ""Advanced Calculus"". Some of the most famous of these are the books by Edwards, by Widder, by Friedman, by Sternberg and Loomis, and a number of other authors. Then there's the books on the so-called ""Calculus on Manifolds"" - famous books like Spivak's or Munkres' come to mind. Now my question is: what's the difference here? This misunderstanding is strengthened by the fact that there also seem to be books out there that deal with both; Hubbard & Hubbard's ""Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach"" is an example of such a book. So what's going on here?","This is an interesting distinction that I don't fully grasp yet. There's quite some books on the topic of the so-called ""Advanced Calculus"". Some of the most famous of these are the books by Edwards, by Widder, by Friedman, by Sternberg and Loomis, and a number of other authors. Then there's the books on the so-called ""Calculus on Manifolds"" - famous books like Spivak's or Munkres' come to mind. Now my question is: what's the difference here? This misunderstanding is strengthened by the fact that there also seem to be books out there that deal with both; Hubbard & Hubbard's ""Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach"" is an example of such a book. So what's going on here?",,"['multivariable-calculus', 'reference-request', 'soft-question']"
41,What does it mean to say that a vector is 'coordinate invariant'?,What does it mean to say that a vector is 'coordinate invariant'?,,"In my lecture notes, it says that a vector is 'coordinate invariant' if it's properties do not depend on the choice of basis used to represent them. I understand that the basis of a vector space is a set of linearly independent vectors which span the vector space, but I'm struggling to interpret the given definition of coordinate invariance. If anyone could clear this up for me, perhaps with an example, that would be greatly appreciated.","In my lecture notes, it says that a vector is 'coordinate invariant' if it's properties do not depend on the choice of basis used to represent them. I understand that the basis of a vector space is a set of linearly independent vectors which span the vector space, but I'm struggling to interpret the given definition of coordinate invariance. If anyone could clear this up for me, perhaps with an example, that would be greatly appreciated.",,"['calculus', 'multivariable-calculus', 'vector-spaces', 'vectors', 'definition']"
42,Use the change of variables to evaluate the double-integral $\iint_R4(x+y)e^{x-y}\;dA$,Use the change of variables to evaluate the double-integral,\iint_R4(x+y)e^{x-y}\;dA,"Use the change of variables indicated to evaluate the double-integral $\iint_R4(x+y)e^{x-y}\;dA$ where $R$ is the interior of the triangle whose vertices are $(-1,1)$, $(0,0)$, and $(1,1)$; $x=\frac{1}{2}(u+v)$ and $y=\frac{1}{2}(u-v)$ I know I have to find the following components for the formula: $$\iint_S f(g(u,v),h(u,v)) \left|\frac{\partial(x,y)}{\partial(u,v,)}\right| \ du \; dv$$ The Jacobian is $\frac{1}{2}$ Next I replaced $x$ and $y$ in my integral with the change of variable to give me the following double integral: $$\iint 4\left[\frac{u}{2}+\frac{v}{2}+\frac{u}{2}-\frac{v}{2}\right] e^{\frac{u}{2}+\frac{v}{2}-\frac{u}{2}+\frac{v}{2}}\;\left(\frac{1}{2}\right)\;du\;dv$$ After simplifying the above I got: $$2\iint ue^v\;du\;dv$$ I'm completely confused with how to map $(x,y)$ to $(u,v)$ so that I can identify my limits of integration.","Use the change of variables indicated to evaluate the double-integral $\iint_R4(x+y)e^{x-y}\;dA$ where $R$ is the interior of the triangle whose vertices are $(-1,1)$, $(0,0)$, and $(1,1)$; $x=\frac{1}{2}(u+v)$ and $y=\frac{1}{2}(u-v)$ I know I have to find the following components for the formula: $$\iint_S f(g(u,v),h(u,v)) \left|\frac{\partial(x,y)}{\partial(u,v,)}\right| \ du \; dv$$ The Jacobian is $\frac{1}{2}$ Next I replaced $x$ and $y$ in my integral with the change of variable to give me the following double integral: $$\iint 4\left[\frac{u}{2}+\frac{v}{2}+\frac{u}{2}-\frac{v}{2}\right] e^{\frac{u}{2}+\frac{v}{2}-\frac{u}{2}+\frac{v}{2}}\;\left(\frac{1}{2}\right)\;du\;dv$$ After simplifying the above I got: $$2\iint ue^v\;du\;dv$$ I'm completely confused with how to map $(x,y)$ to $(u,v)$ so that I can identify my limits of integration.",,"['calculus', 'integration', 'multivariable-calculus']"
43,Newtons Method in 2D,Newtons Method in 2D,,"I need to learn how to use Newtons Method in the 2nd dimension for a research report, but have had a hard time finding any information on the topic that is not in python code. I have found the equation: $$x_{n+1}=x_n-J^{-1}f(x_n)$$ I do not really know where to go from here though. I wanted to know where the equation came from, how to use it, why the inverse Jacobian is used, and any other useful information that can be explained to me. Any help would be greatly appreciated.","I need to learn how to use Newtons Method in the 2nd dimension for a research report, but have had a hard time finding any information on the topic that is not in python code. I have found the equation: $$x_{n+1}=x_n-J^{-1}f(x_n)$$ I do not really know where to go from here though. I wanted to know where the equation came from, how to use it, why the inverse Jacobian is used, and any other useful information that can be explained to me. Any help would be greatly appreciated.",,"['multivariable-calculus', 'newton-raphson']"
44,A curve of constant curvature and zero torsion must be a circle,A curve of constant curvature and zero torsion must be a circle,,From Elementary Differential Geometry by Pressley I don't understand the last paragraph. Why does it show $\gamma$ lies on the sphere $\mathcal S$ with center $\mathbf a$ and radius $1/\kappa$?,From Elementary Differential Geometry by Pressley I don't understand the last paragraph. Why does it show $\gamma$ lies on the sphere $\mathcal S$ with center $\mathbf a$ and radius $1/\kappa$?,,"['multivariable-calculus', 'differential-geometry', '3d', 'curves', 'curvature']"
45,Find maximum and minimum values of an equation on an elipse,Find maximum and minimum values of an equation on an elipse,,"I need some help with this. I've been struggling through this last chapter of my Calc III class, and I'm not sure how to do this (although, it doesn't seem like it should be difficult to do) $$ \text{Find the maximum and minimum values of }f(x, y) = 4x + y\text{ on the ellipse } x^{2} + 49y^{2} = 1 \\ \text{Maximum = _____}\\ \text{Minimum = _____} $$ I know that I can find the critical point by taking partial derivatives so  $$ \frac{\partial{f}}{\partial{x}} = 4 \\ \frac{\partial{f}}{\partial{y}} = 1  $$ Which gives us the critical point $(4,1)$. Here's where I get stuck, I know that we can then determine max/min from the equation $D = f_{xx} * f_{yy} - f_{xy}^2$ and based on the value of $D$, we know whether it is a max, a min, or a ""saddle"" point. It doesn't seem to apply in this situation, because I have an ellipse that I have to use as a constraint. What do I do next? UPDATE: Based on everyone's suggestion, I've been looking into Lagrange multipliers. I think I'm still stuck. $$ \bigtriangledown{f} = \lambda \bigtriangledown{g} \\ f_x = 4 = \lambda 2 * x\\ f_y = 1 = \lambda 98 * y \\ \text{we need to solve by using ratios of derivatives to remove } \lambda\\ \frac{4}{1} = \frac{\lambda  2  x}{\lambda 98 y} \\ 196 y = x\\ \text{Now we use our ellipse to solve}\\ (196y)^2 + 49y^2 = 1 \\ 38445 y^2 = 1 \\ y = \sqrt{\frac{1}{38465}} = y_0\\ \text{plug y back into our ellipse} \\ x^2 + 49 (\frac{1}{38465}) = 1\\ x = \sqrt{1- \frac{49}{38465}} = x_0 $$ I'm feeling somewhat accomplished, but I have no idea if what I'm doing is correct, or what to do with these new numbers","I need some help with this. I've been struggling through this last chapter of my Calc III class, and I'm not sure how to do this (although, it doesn't seem like it should be difficult to do) $$ \text{Find the maximum and minimum values of }f(x, y) = 4x + y\text{ on the ellipse } x^{2} + 49y^{2} = 1 \\ \text{Maximum = _____}\\ \text{Minimum = _____} $$ I know that I can find the critical point by taking partial derivatives so  $$ \frac{\partial{f}}{\partial{x}} = 4 \\ \frac{\partial{f}}{\partial{y}} = 1  $$ Which gives us the critical point $(4,1)$. Here's where I get stuck, I know that we can then determine max/min from the equation $D = f_{xx} * f_{yy} - f_{xy}^2$ and based on the value of $D$, we know whether it is a max, a min, or a ""saddle"" point. It doesn't seem to apply in this situation, because I have an ellipse that I have to use as a constraint. What do I do next? UPDATE: Based on everyone's suggestion, I've been looking into Lagrange multipliers. I think I'm still stuck. $$ \bigtriangledown{f} = \lambda \bigtriangledown{g} \\ f_x = 4 = \lambda 2 * x\\ f_y = 1 = \lambda 98 * y \\ \text{we need to solve by using ratios of derivatives to remove } \lambda\\ \frac{4}{1} = \frac{\lambda  2  x}{\lambda 98 y} \\ 196 y = x\\ \text{Now we use our ellipse to solve}\\ (196y)^2 + 49y^2 = 1 \\ 38445 y^2 = 1 \\ y = \sqrt{\frac{1}{38465}} = y_0\\ \text{plug y back into our ellipse} \\ x^2 + 49 (\frac{1}{38465}) = 1\\ x = \sqrt{1- \frac{49}{38465}} = x_0 $$ I'm feeling somewhat accomplished, but I have no idea if what I'm doing is correct, or what to do with these new numbers",,"['multivariable-calculus', 'optimization']"
46,Correct order of taking dot product and derivatives in spherical coordinates,Correct order of taking dot product and derivatives in spherical coordinates,,I tried to derive definition of divergence in spherical coordinates from gradient and got: $${\vec \nabla \cdot \vec A=\bigg (\frac{\partial}{\partial r}\hat r+\frac{1}{r}\frac{\partial}{\partial \theta}\hat \theta+\frac{1}{r\sin\theta}\frac{\partial}{\partial\phi}\hat\phi\bigg)\cdot\bigg(A_{r}\hat r+A_{\theta}\hat \theta+A_{\phi}\hat\phi\bigg)}$$ $${=\frac{\partial A_{r}}{\partial r}+\frac{1}{r}\frac{\partial A_{\theta}}{\partial\theta}+\frac{1}{r\sin\theta}\frac{\partial A_{\phi}}{\partial\phi}}$$ But according to Wikipedia and other sources this be: $${\vec \nabla \cdot \vec A =\frac{1}{r^2} \frac{\partial (r^2 A_{r})}{\partial r}+\frac{1}{r\sin\theta}\frac{\partial (A_{\theta}\sin\theta)}{\partial\theta}+\frac{1}{r\sin\theta}\frac{\partial A_{\phi}}{\partial\phi}}$$ Why should we first take the derivative and then the dot product? Why do we take first dot product when we have cartesian coordinates? I think I read that it is because vectors themselves are functions of the coordinates but I do not understand what are consequences of this.,I tried to derive definition of divergence in spherical coordinates from gradient and got: $${\vec \nabla \cdot \vec A=\bigg (\frac{\partial}{\partial r}\hat r+\frac{1}{r}\frac{\partial}{\partial \theta}\hat \theta+\frac{1}{r\sin\theta}\frac{\partial}{\partial\phi}\hat\phi\bigg)\cdot\bigg(A_{r}\hat r+A_{\theta}\hat \theta+A_{\phi}\hat\phi\bigg)}$$ $${=\frac{\partial A_{r}}{\partial r}+\frac{1}{r}\frac{\partial A_{\theta}}{\partial\theta}+\frac{1}{r\sin\theta}\frac{\partial A_{\phi}}{\partial\phi}}$$ But according to Wikipedia and other sources this be: $${\vec \nabla \cdot \vec A =\frac{1}{r^2} \frac{\partial (r^2 A_{r})}{\partial r}+\frac{1}{r\sin\theta}\frac{\partial (A_{\theta}\sin\theta)}{\partial\theta}+\frac{1}{r\sin\theta}\frac{\partial A_{\phi}}{\partial\phi}}$$ Why should we first take the derivative and then the dot product? Why do we take first dot product when we have cartesian coordinates? I think I read that it is because vectors themselves are functions of the coordinates but I do not understand what are consequences of this.,,"['calculus', 'multivariable-calculus', 'spherical-coordinates']"
47,Is $\sum_{k\leqslant n} f'(k)f'(n-k) \asymp f'(n)f(n)$ when $f'$ is positive decreasing?,Is  when  is positive decreasing?,\sum_{k\leqslant n} f'(k)f'(n-k) \asymp f'(n)f(n) f',"In this answer of a question of mine, the user Homegrown Tomato   gave a nice argument that somewhat shows that $$\int_{\substack{t+s\leqslant x \\ t,s \geqslant 0}} f'(t)f'(s)dtds \asymp f(x)^2.$$ I actually improved this statement (under the hypotheses of my   question) and concluded that in fact it's asymptotic to $C\cdot f(x)^2$ for some positive real constant $C$, but I'm not sure whether   that's beside the point of my actual question here or not. I'm trying to estimate $\sum_{k\leqslant n} f'(k)f'(n-k)$ for $C^0$ decreasing $f'$, like $f'(x) = 1/\log{x}$, or $x^{-1},x^{-1/2},x^{-1/2}\log{x}$, etc. Unlike the case of $\sum_{k\leqslant n} f'(k)$, where to use the integral as an estimate is almost direct, I think something like $$ \sum_{k\leqslant n} f'(k)f'(n-k) \overset{?}\sim \int_{0\leqslant t\leqslant n} f'(t)f'(n-t)dt \tag{1}$$ must be true, but I was not able to prove. Well, still on that thought, the integration here is occuring on part of the boundary of a triangle, the same triangle of the integration on my previous question that I quoted above! So I thought in to try to relate the two questions, using something like Stokes or Reynolds' transport theorem , in order to get $$\int_{0\leqslant t\leqslant x} f'(t)f'(x-t)dt \overset{?}\approx \frac{d}{dx}\int_{\substack{t+s\leqslant x \\ t,s \geqslant 0}} f'(t)f'(s)dtds \overset{?}\approx f'(x)f(x) \tag{2}.$$ So what I'm asking is: Is there a way to formalize (1) and (2) , or at least one of them? And if no (or even if yes), there is another way (an easier, preferably haha) to show the statement of the question? In the worst-case scenario, a counterexample will be welcome hahaha","In this answer of a question of mine, the user Homegrown Tomato   gave a nice argument that somewhat shows that $$\int_{\substack{t+s\leqslant x \\ t,s \geqslant 0}} f'(t)f'(s)dtds \asymp f(x)^2.$$ I actually improved this statement (under the hypotheses of my   question) and concluded that in fact it's asymptotic to $C\cdot f(x)^2$ for some positive real constant $C$, but I'm not sure whether   that's beside the point of my actual question here or not. I'm trying to estimate $\sum_{k\leqslant n} f'(k)f'(n-k)$ for $C^0$ decreasing $f'$, like $f'(x) = 1/\log{x}$, or $x^{-1},x^{-1/2},x^{-1/2}\log{x}$, etc. Unlike the case of $\sum_{k\leqslant n} f'(k)$, where to use the integral as an estimate is almost direct, I think something like $$ \sum_{k\leqslant n} f'(k)f'(n-k) \overset{?}\sim \int_{0\leqslant t\leqslant n} f'(t)f'(n-t)dt \tag{1}$$ must be true, but I was not able to prove. Well, still on that thought, the integration here is occuring on part of the boundary of a triangle, the same triangle of the integration on my previous question that I quoted above! So I thought in to try to relate the two questions, using something like Stokes or Reynolds' transport theorem , in order to get $$\int_{0\leqslant t\leqslant x} f'(t)f'(x-t)dt \overset{?}\approx \frac{d}{dx}\int_{\substack{t+s\leqslant x \\ t,s \geqslant 0}} f'(t)f'(s)dtds \overset{?}\approx f'(x)f(x) \tag{2}.$$ So what I'm asking is: Is there a way to formalize (1) and (2) , or at least one of them? And if no (or even if yes), there is another way (an easier, preferably haha) to show the statement of the question? In the worst-case scenario, a counterexample will be welcome hahaha",,"['real-analysis', 'integration', 'sequences-and-series', 'multivariable-calculus', 'asymptotics']"
48,Computing line integral using Stokes´theorem,Computing line integral using Stokes´theorem,,"Use Stokes´ theorem to show that $$\int_C ydx+zdy+xdz=\pi a^2\sqrt{3}$$ where $C$ is the curve of intersection of the sphere $x^2+y^2+z^2=a^2$ and the plane $x+y+z=0$ My attempt: By Stokes´ theorem I know that $$\int_S (\nabla \times F) \cdot n \ dS=\int_c F \cdot  d\alpha$$  In this case the intersection curve $C$ is a circle and $S$ is ""half"" of the sphere using $r(u,v)=(acos(u)sin(v), a sin(u)cos(u), acos(v))$ $0\le v \le \pi$, $-\pi/4 \le u \le 3\pi/4$ as a parametrization of the sphere and computing $\nabla \times F=(-1,-1,-1)$ (where $F=(y,z,x)$), and $${\partial r\over \partial u}\times {\partial r\over \partial v}$$ the surface integral becomes: $$\int_{-\pi/4}^{3\pi/4}\int_{0}^{\pi}({a^2sin(v)sin(2u)\over 2}-a^2sin(v)sin^2(u)+{a^2sin(2v)\over 2})dv\ du$$, but after computing the integral I don´t get the answer, Can you please tell me where is my mistake?","Use Stokes´ theorem to show that $$\int_C ydx+zdy+xdz=\pi a^2\sqrt{3}$$ where $C$ is the curve of intersection of the sphere $x^2+y^2+z^2=a^2$ and the plane $x+y+z=0$ My attempt: By Stokes´ theorem I know that $$\int_S (\nabla \times F) \cdot n \ dS=\int_c F \cdot  d\alpha$$  In this case the intersection curve $C$ is a circle and $S$ is ""half"" of the sphere using $r(u,v)=(acos(u)sin(v), a sin(u)cos(u), acos(v))$ $0\le v \le \pi$, $-\pi/4 \le u \le 3\pi/4$ as a parametrization of the sphere and computing $\nabla \times F=(-1,-1,-1)$ (where $F=(y,z,x)$), and $${\partial r\over \partial u}\times {\partial r\over \partial v}$$ the surface integral becomes: $$\int_{-\pi/4}^{3\pi/4}\int_{0}^{\pi}({a^2sin(v)sin(2u)\over 2}-a^2sin(v)sin^2(u)+{a^2sin(2v)\over 2})dv\ du$$, but after computing the integral I don´t get the answer, Can you please tell me where is my mistake?",,"['multivariable-calculus', 'vector-analysis', 'solution-verification', 'surface-integrals']"
49,solution of $y^{\prime \prime} + y^n = 1$ [closed],solution of  [closed],y^{\prime \prime} + y^n = 1,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I am not able to figure out the solution for the differential solution $$y^{\prime \prime} + y^n = 1$$ I want to specifically find an answer for $$y^{\prime \prime} + y^2= 1$$and $$y^{\prime \prime} + y^3 = 1$$ If anyone can help!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I am not able to figure out the solution for the differential solution $$y^{\prime \prime} + y^n = 1$$ I want to specifically find an answer for $$y^{\prime \prime} + y^2= 1$$and $$y^{\prime \prime} + y^3 = 1$$ If anyone can help!",,"['calculus', 'multivariable-calculus', 'dynamical-systems', 'calculus-of-variations']"
50,Integrating over the two form,Integrating over the two form,,"Let $A=(0,1)^2$. Let $\alpha:A\to\Bbb R^3$ be given by the equation $$\alpha(u,v)=(u,v,u^2+v^2+1)$$ Let $Y$ be the image set of $\alpha$. Evaluate the integral over $Y_\alpha$ of the 2-form  $x_2dx_2\land dx_3+x_1 x_3 dx_1\land dx_3$. Can someone please give me a useful hint for solving this? Thanks in advance!","Let $A=(0,1)^2$. Let $\alpha:A\to\Bbb R^3$ be given by the equation $$\alpha(u,v)=(u,v,u^2+v^2+1)$$ Let $Y$ be the image set of $\alpha$. Evaluate the integral over $Y_\alpha$ of the 2-form  $x_2dx_2\land dx_3+x_1 x_3 dx_1\land dx_3$. Can someone please give me a useful hint for solving this? Thanks in advance!",,"['real-analysis', 'multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
51,Clarification on unit normal vector to a graph of a function,Clarification on unit normal vector to a graph of a function,,"In solving for the unit normal vector $\hat{n}$ of a surface $z=h(x,y)$, the unit normal vector is defined as follows: $$\hat{n}=\frac{\nabla f}{|\nabla f|}$$ with $f=f(x,y,z)=z-h(x,y)$ and $|\nabla f|\neq 0$. My question is, is it also correct to define $f$ as $f=h(x,y)-z$? I think they are the same since $f=z-h(x,y)=0$ and $f=h(x,y)-z=0$ but the following example shows they yield different answers. Suppose $z=\sqrt{x^2+y^2}$. Case 1: $f=z-\sqrt{x^2+y^2}$. Then $\nabla f=-\frac{x}{\sqrt{x^2+y^2}}\vec{i}-\frac{y}{\sqrt{x^2+y^2}}\vec{j}+\vec{k}$ and $|\nabla f|=\sqrt{2}$. Thus $$\hat{n}=-\frac{x}{\sqrt{2(x^2+y^2)}}\vec{i}-\frac{y}{\sqrt{2(x^2+y^2)}}\vec{j}+\frac{1}{\sqrt{2}}\vec{k}$$  Case 2: $f=\sqrt{x^2+y^2}-z$. Then $\nabla f=\frac{x}{\sqrt{x^2+y^2}}\vec{i}+\frac{y}{\sqrt{x^2+y^2}}\vec{j}-\vec{k}$ and $|\nabla f|=\sqrt{2}$. Thus $$\hat{n}=\frac{x}{\sqrt{2(x^2+y^2)}}\vec{i}+\frac{y}{\sqrt{2(x^2+y^2)}}\vec{j}-\frac{1}{\sqrt{2}}\vec{k}$$ This shows that the $\vec{i}$, $\vec{j}$ and $\vec{k}$ components are of opposite sign to which I am confused.","In solving for the unit normal vector $\hat{n}$ of a surface $z=h(x,y)$, the unit normal vector is defined as follows: $$\hat{n}=\frac{\nabla f}{|\nabla f|}$$ with $f=f(x,y,z)=z-h(x,y)$ and $|\nabla f|\neq 0$. My question is, is it also correct to define $f$ as $f=h(x,y)-z$? I think they are the same since $f=z-h(x,y)=0$ and $f=h(x,y)-z=0$ but the following example shows they yield different answers. Suppose $z=\sqrt{x^2+y^2}$. Case 1: $f=z-\sqrt{x^2+y^2}$. Then $\nabla f=-\frac{x}{\sqrt{x^2+y^2}}\vec{i}-\frac{y}{\sqrt{x^2+y^2}}\vec{j}+\vec{k}$ and $|\nabla f|=\sqrt{2}$. Thus $$\hat{n}=-\frac{x}{\sqrt{2(x^2+y^2)}}\vec{i}-\frac{y}{\sqrt{2(x^2+y^2)}}\vec{j}+\frac{1}{\sqrt{2}}\vec{k}$$  Case 2: $f=\sqrt{x^2+y^2}-z$. Then $\nabla f=\frac{x}{\sqrt{x^2+y^2}}\vec{i}+\frac{y}{\sqrt{x^2+y^2}}\vec{j}-\vec{k}$ and $|\nabla f|=\sqrt{2}$. Thus $$\hat{n}=\frac{x}{\sqrt{2(x^2+y^2)}}\vec{i}+\frac{y}{\sqrt{2(x^2+y^2)}}\vec{j}-\frac{1}{\sqrt{2}}\vec{k}$$ This shows that the $\vec{i}$, $\vec{j}$ and $\vec{k}$ components are of opposite sign to which I am confused.",,"['multivariable-calculus', 'vectors']"
52,Finding the local extrema of a function,Finding the local extrema of a function,,"One of our final exam exercise sheets features this particular exercise : Find the extrema of : $2xy^3+y-x^2=0$, where $y=y(x)$ . As I thought it, this exercise involves the implicit function theorem, so I defined $F(x,y(x))=2xy^3+y-x^2=0$. Then I computed $\frac{\partial F}{\partial x}=y^3+3xy^2y'-x=0 $ and $\frac{\partial F}{\partial y}=6xy^2+1=0 $ and got $y'=\frac{x-y^3}{3y^2x}$ . But I'm not really sure on how to go about finding the extrema of $y(x)$ .. Any ideas ?","One of our final exam exercise sheets features this particular exercise : Find the extrema of : $2xy^3+y-x^2=0$, where $y=y(x)$ . As I thought it, this exercise involves the implicit function theorem, so I defined $F(x,y(x))=2xy^3+y-x^2=0$. Then I computed $\frac{\partial F}{\partial x}=y^3+3xy^2y'-x=0 $ and $\frac{\partial F}{\partial y}=6xy^2+1=0 $ and got $y'=\frac{x-y^3}{3y^2x}$ . But I'm not really sure on how to go about finding the extrema of $y(x)$ .. Any ideas ?",,"['multivariable-calculus', 'implicit-differentiation']"
53,The differential of the sum $\sum x_i^2$,The differential of the sum,\sum x_i^2,"I have the following equation which is from a famous paper in economics: $$ \sum_{i=1}^n x_i \, dx_i=\frac{1}{2} d\Big[\sum_{i=1}^n x_{i}^2\Big]=XH \, dX+\frac{1}{2}d\big[X^2H\big] $$ Can you tell me how to get to the second and the third equality sign. We have that $$  \sum_i^n x_i=X   $$ and $$ \sum_i^n \Big(\frac{x_i}{X} \Big)^2=H $$ I think the ""d"" $dx_i$ is the same as in a derivative.","I have the following equation which is from a famous paper in economics: $$ \sum_{i=1}^n x_i \, dx_i=\frac{1}{2} d\Big[\sum_{i=1}^n x_{i}^2\Big]=XH \, dX+\frac{1}{2}d\big[X^2H\big] $$ Can you tell me how to get to the second and the third equality sign. We have that $$  \sum_i^n x_i=X   $$ and $$ \sum_i^n \Big(\frac{x_i}{X} \Big)^2=H $$ I think the ""d"" $dx_i$ is the same as in a derivative.",,"['multivariable-calculus', 'derivatives', 'notation']"
54,What's the fourth term in the multivariable Taylor expansion?,What's the fourth term in the multivariable Taylor expansion?,,"For a function $f: \Bbb R^n \to R$, the $2$nd order Taylor expansion is: $$f(\mathbf x+\mathbf h) \approx f(\mathbf x)+ Df(\mathbf x) \mathbf h + \frac{1}{2}\mathbf h^T H(f)(\mathbf x) \mathbf h$$ What's the next term (the last term in the $3$rd order Taylor expansion)? I can see that $Df(\mathbf x)$ is a row vector, so to make it scalar, we need to right multiply a column vector.  And similarly $Hf(\mathbf x)$ is an $n\times n$ matrix so to make it a scalar we need to left and right multiply by vectors (row and column, respectively).  I can't think what the next term would have to be, though.","For a function $f: \Bbb R^n \to R$, the $2$nd order Taylor expansion is: $$f(\mathbf x+\mathbf h) \approx f(\mathbf x)+ Df(\mathbf x) \mathbf h + \frac{1}{2}\mathbf h^T H(f)(\mathbf x) \mathbf h$$ What's the next term (the last term in the $3$rd order Taylor expansion)? I can see that $Df(\mathbf x)$ is a row vector, so to make it scalar, we need to right multiply a column vector.  And similarly $Hf(\mathbf x)$ is an $n\times n$ matrix so to make it a scalar we need to left and right multiply by vectors (row and column, respectively).  I can't think what the next term would have to be, though.",,"['multivariable-calculus', 'taylor-expansion']"
55,"If f ' = 0, then f is constant?","If f ' = 0, then f is constant?",,"I'm a little confused.  After finishing the online multi-variable calculus course from the MIT OCW offerings (I wanted to brush up on the subject more concretely, after my Analysis II course), I looked at another brisk course on a single-variable calculus course. The hope was to revisit calculus, after a couple years of rigorous analysis courses. But, my question is:  in the MIT OCW course, the prof. had mentioned a few times that: If the first order derivatives of f were 0, then f is not actually constant, but rather it is just not changing, to first order.  There are obviously higher order terms in its Taylor development. But when I return to single-variable calculus, I have seen several times now that some theorems and proofs argue that if f' = 0, then f is constant.  But isn't f just...not changing, to first order - and that it's not actually constant?  It may or may not have higher order, non-vanishing terms in its Taylor development. Thanks in advance,","I'm a little confused.  After finishing the online multi-variable calculus course from the MIT OCW offerings (I wanted to brush up on the subject more concretely, after my Analysis II course), I looked at another brisk course on a single-variable calculus course. The hope was to revisit calculus, after a couple years of rigorous analysis courses. But, my question is:  in the MIT OCW course, the prof. had mentioned a few times that: If the first order derivatives of f were 0, then f is not actually constant, but rather it is just not changing, to first order.  There are obviously higher order terms in its Taylor development. But when I return to single-variable calculus, I have seen several times now that some theorems and proofs argue that if f' = 0, then f is constant.  But isn't f just...not changing, to first order - and that it's not actually constant?  It may or may not have higher order, non-vanishing terms in its Taylor development. Thanks in advance,",,"['calculus', 'multivariable-calculus', 'derivatives', 'taylor-expansion', 'partial-derivative']"
56,The Laplacian of the composition of a function with an orthogonal transformation,The Laplacian of the composition of a function with an orthogonal transformation,,"Define a ""rotation of u"" by $R_{A}u\doteq u\circ A $ with A an orthogonal $n\times n$ matrix and ""$\circ$"" means composition. Show that $\Delta (u\circ A )=(\Delta u)\circ A$. (Note: $u(x)\in C^{m\geqslant 2} (\mathbb{R}^{n}) $) I tried to treat u as merely a number in $\mathbb{R}^{n}$ and then take the Laplacian to each entry of the matrix. Then I compared the LHS and the RHS and found they are equal. Is this correct? I felt this way is kind of trivial as you don't use the condition ""orthogonal"".","Define a ""rotation of u"" by $R_{A}u\doteq u\circ A $ with A an orthogonal $n\times n$ matrix and ""$\circ$"" means composition. Show that $\Delta (u\circ A )=(\Delta u)\circ A$. (Note: $u(x)\in C^{m\geqslant 2} (\mathbb{R}^{n}) $) I tried to treat u as merely a number in $\mathbb{R}^{n}$ and then take the Laplacian to each entry of the matrix. Then I compared the LHS and the RHS and found they are equal. Is this correct? I felt this way is kind of trivial as you don't use the condition ""orthogonal"".",,"['linear-algebra', 'multivariable-calculus', 'laplacian']"
57,A strictly convex function defines an implicit function with non-positive second derivative,A strictly convex function defines an implicit function with non-positive second derivative,,"Problem. Let $F\colon \mathbb{R}^2 \to \mathbb R$ be a non-negative, $C^2$ function which is also strictly convex, meaning that    $$ F(\lambda P + (1-\lambda)Q) < \lambda F(P) + (1-\lambda)F(Q), \qquad \forall P\ne Q \in \mathbb R^2, \,\, \forall \lambda \in (0,1).  $$   Assume $F(0,0)=0$. (1) Prove that the equation $F(x,y)=F(0,1)$ defines implicitly a $C^2$ function $y=\phi(x)$ in a neighbourhood of $(0,1)$ with $\phi(0)=1$. (2) Show that $\phi^{\prime\prime}(0)\le 0$. Point (1) is an application of Implicit function theorem. Indeed we have that $$ \frac{\partial F}{\partial y}(0,1) = \lim_{t \to 0} \frac{F(0,1)-F(0,1-t)}{t} = \lim_{t \to 0^+} \frac{F(0,1)-F(0,1-t)}{t}. $$ Now using strictly convexity and $F(0,0)=0$ we have for $t \in (0,1)$  $$ \frac{F(0,1)-F(0,1-t)}{t} = \frac{F(0,1)-F(t(0,0) + (1-t)(0,1))}{t} > \frac{F(0,1)- (1-t)F(0,1)}{t} = F(0,1) \ge 0 $$ which is non negative by assumption. Therefore, $F_y(0,1) \ne 0$ and the claim now follows. Can I ask you what you think about this? Is this correct? EDIT : I am now doubtful about this first point. I have proved that the difference quotients are strictly bounded from below by a non negative constant. But this is not enough to conclude the derivative is nonzero, right?    I have just realized that $F(0,1)$ cannot be $0$ because a strictly convex function has at most one global minimum (which in this case will be $(0,0)$). Indeed, if we had $a\ne b$ global minima then    $$ f(ta+(1-t)b)<tf(a)+(1-t)f(b) \le tf(b)+(1-t)f(b) = f(b) $$    hence $b$ would not be a minimum. Is this eventually correct? Concerning point 2 I am a bit in trouble. If I am not wrong I have computed $$ \phi^{\prime\prime}(0) = - \frac{F_y^2F_{xx}-2F_{xy}F_xF_y+F_{yy}F_x^2}{F_y^3}(0,1)  $$ but I do not know how to estimate the sign of this expression... Could you help me, please? Thanks in advance.","Problem. Let $F\colon \mathbb{R}^2 \to \mathbb R$ be a non-negative, $C^2$ function which is also strictly convex, meaning that    $$ F(\lambda P + (1-\lambda)Q) < \lambda F(P) + (1-\lambda)F(Q), \qquad \forall P\ne Q \in \mathbb R^2, \,\, \forall \lambda \in (0,1).  $$   Assume $F(0,0)=0$. (1) Prove that the equation $F(x,y)=F(0,1)$ defines implicitly a $C^2$ function $y=\phi(x)$ in a neighbourhood of $(0,1)$ with $\phi(0)=1$. (2) Show that $\phi^{\prime\prime}(0)\le 0$. Point (1) is an application of Implicit function theorem. Indeed we have that $$ \frac{\partial F}{\partial y}(0,1) = \lim_{t \to 0} \frac{F(0,1)-F(0,1-t)}{t} = \lim_{t \to 0^+} \frac{F(0,1)-F(0,1-t)}{t}. $$ Now using strictly convexity and $F(0,0)=0$ we have for $t \in (0,1)$  $$ \frac{F(0,1)-F(0,1-t)}{t} = \frac{F(0,1)-F(t(0,0) + (1-t)(0,1))}{t} > \frac{F(0,1)- (1-t)F(0,1)}{t} = F(0,1) \ge 0 $$ which is non negative by assumption. Therefore, $F_y(0,1) \ne 0$ and the claim now follows. Can I ask you what you think about this? Is this correct? EDIT : I am now doubtful about this first point. I have proved that the difference quotients are strictly bounded from below by a non negative constant. But this is not enough to conclude the derivative is nonzero, right?    I have just realized that $F(0,1)$ cannot be $0$ because a strictly convex function has at most one global minimum (which in this case will be $(0,0)$). Indeed, if we had $a\ne b$ global minima then    $$ f(ta+(1-t)b)<tf(a)+(1-t)f(b) \le tf(b)+(1-t)f(b) = f(b) $$    hence $b$ would not be a minimum. Is this eventually correct? Concerning point 2 I am a bit in trouble. If I am not wrong I have computed $$ \phi^{\prime\prime}(0) = - \frac{F_y^2F_{xx}-2F_{xy}F_xF_y+F_{yy}F_x^2}{F_y^3}(0,1)  $$ but I do not know how to estimate the sign of this expression... Could you help me, please? Thanks in advance.",,"['multivariable-calculus', 'implicit-differentiation']"
58,Local minimum and gradient [duplicate],Local minimum and gradient [duplicate],,"This question already has answers here : Proof of: If $x_0\in \mathbb R^n$ is a local minimum of $f$, then $\nabla f(x_0) = 0$. (2 answers) Closed 9 years ago . But the proof here below is specially elegant. Is there any function $f$ such that $f$ has a local minimum at $x$ but $\nabla f(x) \neq 0$? Only assumption on $f$ is that it has to be differentiable at $x$ so that I can write $\nabla f(x)\neq 0$  to ask you this question.","This question already has answers here : Proof of: If $x_0\in \mathbb R^n$ is a local minimum of $f$, then $\nabla f(x_0) = 0$. (2 answers) Closed 9 years ago . But the proof here below is specially elegant. Is there any function $f$ such that $f$ has a local minimum at $x$ but $\nabla f(x) \neq 0$? Only assumption on $f$ is that it has to be differentiable at $x$ so that I can write $\nabla f(x)\neq 0$  to ask you this question.",,"['multivariable-calculus', 'optimization']"
59,Tips on resolving a Lagrange Multipliers equation system,Tips on resolving a Lagrange Multipliers equation system,,"I'm having a very hard time resolving the system of equations after using the Lagrange Multipliers optimization method. For instance: The plane $ x + y + 2z = 2 $ intersects the paraboloid $ z = x^2 + y^2 $ over an ellipse. Find the ellipse points that are nearer and farther from the origin. I know that the Lagrange equation is going to be: $ L(x,y,z) = \sqrt{x^2 + y^2 + z^2} - L_1(x + y + 2z - 2)-L_2(z - x^2 - y^2)$ For simplicity it can be rewritten as: $ L(x,y,z) = x^2 + y^2 + z^2 - L_1(x + y + 2z - 2)-L_2(z - x^2 - y^2)$ And that the equation system will be: $ \frac{\partial L}{\partial x} = 2x - L_1 + 2 L_2 x = 0 $ $ \frac{\partial L}{\partial y} = 2y - L_1 + 2 L_2 y = 0 $ $ \frac{\partial L}{\partial z} = 2z - 2L_1 - L_2 = 0 $ $ \frac{\partial L}{\partial L_1} = x + y + 2z - 2 = 0 $ $ \frac{\partial L}{\partial L_2} = z - x^2 - y^2 = 0 $ Solving this system is really giving me a hard time, so instead of asking for the answer (which it could also help to see what are your procedures) I'd like to know if there are any known tips that could lead on a solution for the system.","I'm having a very hard time resolving the system of equations after using the Lagrange Multipliers optimization method. For instance: The plane $ x + y + 2z = 2 $ intersects the paraboloid $ z = x^2 + y^2 $ over an ellipse. Find the ellipse points that are nearer and farther from the origin. I know that the Lagrange equation is going to be: $ L(x,y,z) = \sqrt{x^2 + y^2 + z^2} - L_1(x + y + 2z - 2)-L_2(z - x^2 - y^2)$ For simplicity it can be rewritten as: $ L(x,y,z) = x^2 + y^2 + z^2 - L_1(x + y + 2z - 2)-L_2(z - x^2 - y^2)$ And that the equation system will be: $ \frac{\partial L}{\partial x} = 2x - L_1 + 2 L_2 x = 0 $ $ \frac{\partial L}{\partial y} = 2y - L_1 + 2 L_2 y = 0 $ $ \frac{\partial L}{\partial z} = 2z - 2L_1 - L_2 = 0 $ $ \frac{\partial L}{\partial L_1} = x + y + 2z - 2 = 0 $ $ \frac{\partial L}{\partial L_2} = z - x^2 - y^2 = 0 $ Solving this system is really giving me a hard time, so instead of asking for the answer (which it could also help to see what are your procedures) I'd like to know if there are any known tips that could lead on a solution for the system.",,"['multivariable-calculus', 'lagrange-multiplier']"
60,"Check my proof $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \sin(x^2+y^2) \, dx \, dy$ diverges",Check my proof  diverges,"\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \sin(x^2+y^2) \, dx \, dy","I am trying to prove that $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \sin(x^2+y^2)\, dx\,dy$ diverges and I did it like this: $x=r\cos \theta$, $y=r\sin\theta$, $\theta \in [0,2\pi]$, $r\in [0,\infty]$, and the jacobian is $r$: $$\int_{-\infty}^\infty \int_{-\infty}^\infty \sin(x^2+y^2) \, dx \, dy=\int_0^{2\pi}\int_{0}^\infty r\sin(r^2) \, dr \, d\theta = 2\pi \int_0^\infty r\sin(r^2) \, dr$$ Use the variable change $v=r^2$, $dv=2r \, dr$ and so: $$2\pi \int_0^\infty r\sin(r^2)\,dr = 2\pi \int_{0}^{\infty}r\sin(v)\,\frac{dv}{2r}=\pi \int_0^\infty \sin(v) \, dv $$ I checked wolfram, and it says that $\int_0^\infty \sin(x) \, dx$ diverges, so I must conclude that the original integral diverges as well. But how do I actually prove that $\int_0^\infty \sin(x)\,dx$ diverges?","I am trying to prove that $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \sin(x^2+y^2)\, dx\,dy$ diverges and I did it like this: $x=r\cos \theta$, $y=r\sin\theta$, $\theta \in [0,2\pi]$, $r\in [0,\infty]$, and the jacobian is $r$: $$\int_{-\infty}^\infty \int_{-\infty}^\infty \sin(x^2+y^2) \, dx \, dy=\int_0^{2\pi}\int_{0}^\infty r\sin(r^2) \, dr \, d\theta = 2\pi \int_0^\infty r\sin(r^2) \, dr$$ Use the variable change $v=r^2$, $dv=2r \, dr$ and so: $$2\pi \int_0^\infty r\sin(r^2)\,dr = 2\pi \int_{0}^{\infty}r\sin(v)\,\frac{dv}{2r}=\pi \int_0^\infty \sin(v) \, dv $$ I checked wolfram, and it says that $\int_0^\infty \sin(x) \, dx$ diverges, so I must conclude that the original integral diverges as well. But how do I actually prove that $\int_0^\infty \sin(x)\,dx$ diverges?",,"['calculus', 'integration', 'multivariable-calculus', 'improper-integrals']"
61,Multiple integral over a disc,Multiple integral over a disc,,"I would need some help on this integration problem: $$I=\int_0^{2\pi}\int_0^{R}\int_0^{2\pi}\int_0^{R}\exp(-a\ r_{12})	\ r_1 \ r_2 \,\mathrm{d}r_1\,\mathrm{d}\phi_1\,\mathrm{d}r_2\,\mathrm{d}\phi_2$$ Here, $r_1, r_2, \phi_1$ and $\phi_2$ are the polar coordinates for the integral on a disc with radius R. And $r_{12}$ is the distance between the coordinates, which can be calculated by $$r_{12}=\sqrt{r_1^2+r_2^2-2r_1r_2\cos(\phi_1-\phi_2)}.$$ Below a plot of the problem for $a=1$, $R=1$ and fixed coordinates of the first point at $(0,0)$:","I would need some help on this integration problem: $$I=\int_0^{2\pi}\int_0^{R}\int_0^{2\pi}\int_0^{R}\exp(-a\ r_{12})	\ r_1 \ r_2 \,\mathrm{d}r_1\,\mathrm{d}\phi_1\,\mathrm{d}r_2\,\mathrm{d}\phi_2$$ Here, $r_1, r_2, \phi_1$ and $\phi_2$ are the polar coordinates for the integral on a disc with radius R. And $r_{12}$ is the distance between the coordinates, which can be calculated by $$r_{12}=\sqrt{r_1^2+r_2^2-2r_1r_2\cos(\phi_1-\phi_2)}.$$ Below a plot of the problem for $a=1$, $R=1$ and fixed coordinates of the first point at $(0,0)$:",,"['integration', 'multivariable-calculus', 'exponential-function', 'polar-coordinates']"
62,Set up a double integral over the region $0\le y\le |x|$,Set up a double integral over the region,0\le y\le |x|,"Let $f(x,y) = x^2y + xy^2 $ and let $$R = \{ (x,y) : |x| \leq 1 , \; \; 0 \leq y \leq |x| \} $$ Want: $\int_R f $ My setting up $$ \int_{-1}^1 \int_{-x}^x f dydx $$ Is this the correct integral that I need to compute?","Let $f(x,y) = x^2y + xy^2 $ and let $$R = \{ (x,y) : |x| \leq 1 , \; \; 0 \leq y \leq |x| \} $$ Want: $\int_R f $ My setting up $$ \int_{-1}^1 \int_{-x}^x f dydx $$ Is this the correct integral that I need to compute?",,['calculus']
63,Chain rule quesition: proving that the Weingarten map is self-adjoint,Chain rule quesition: proving that the Weingarten map is self-adjoint,,"I'm reading through the proof in this paper ( http://www.math.leidenuniv.nl/scripties/JaibiBach.pdf ) but I'm stuck at the line: ""Using the chain rule we get: $L_p(\phi_v) = -Dn(\phi_v) = - \frac {\partial}{\partial v} (n \circ \phi)$"" How does the second equality follow by the chain rule? My working is: $-Dn(\phi_v) = - \frac{d}{dt} (n \circ \frac {\partial \phi}{\partial v}) = \frac {dn}{dt} ((\frac {\partial \phi}{\partial v})(p)) \frac {d} {dt}( \frac {\partial \phi}{\partial v}) (p) $ but I don't know how to continue this.","I'm reading through the proof in this paper ( http://www.math.leidenuniv.nl/scripties/JaibiBach.pdf ) but I'm stuck at the line: ""Using the chain rule we get: $L_p(\phi_v) = -Dn(\phi_v) = - \frac {\partial}{\partial v} (n \circ \phi)$"" How does the second equality follow by the chain rule? My working is: $-Dn(\phi_v) = - \frac{d}{dt} (n \circ \frac {\partial \phi}{\partial v}) = \frac {dn}{dt} ((\frac {\partial \phi}{\partial v})(p)) \frac {d} {dt}( \frac {\partial \phi}{\partial v}) (p) $ but I don't know how to continue this.",,"['multivariable-calculus', 'differential-geometry', 'riemannian-geometry']"
64,Two variable function with four different stationary points,Two variable function with four different stationary points,,"Let $f(x,y)$ has continuous second partial derivative. Define $$D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-f_{xy}(x,y)^2.$$ If $(x_0,y_0)$ is a stationary point of a function $f(x,y$, then the  second partial derivative test asserts the following: (1) If $D(x_0,y_0)>0$ and $f_{xx}(x_0,y_0)>0$, then $(x_0,y_0)$ is a minimum point. (2) If $D(x_0,y_0)>0$ and $f_{xx}(x_0,y_0)<0$, then $(x_0,y_0)$ is a maximum point. (3) If $D(x_0,y_0)<0$, then $(x_0,y_0)$ is a saddle point. (4) If $D(x_0,y_0)=0$, then this test is inconclusive, and $(x_0,y_0)$ could be any of a minimum, maximum or saddle point. My question is: Could one give a function $f(x,y)$ with exactly four different stationary points that satisfy $(1),(2),(3)$, and, $(4)$? I try some function but I haven't found such function. For example, $$f(x,y)=x^3-\frac{1}{2}x^2+y^3-\frac{1}{2}y^2$$ has four different stationary points but it doesn't satisfy $(4)$. Meanwhile, $$g(x,y)=x^3-x^2+\frac{1}{4}y^4-\frac{1}{3}y^3$$ has four different stationary points but it doesn't satisfy $(3)$.","Let $f(x,y)$ has continuous second partial derivative. Define $$D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-f_{xy}(x,y)^2.$$ If $(x_0,y_0)$ is a stationary point of a function $f(x,y$, then the  second partial derivative test asserts the following: (1) If $D(x_0,y_0)>0$ and $f_{xx}(x_0,y_0)>0$, then $(x_0,y_0)$ is a minimum point. (2) If $D(x_0,y_0)>0$ and $f_{xx}(x_0,y_0)<0$, then $(x_0,y_0)$ is a maximum point. (3) If $D(x_0,y_0)<0$, then $(x_0,y_0)$ is a saddle point. (4) If $D(x_0,y_0)=0$, then this test is inconclusive, and $(x_0,y_0)$ could be any of a minimum, maximum or saddle point. My question is: Could one give a function $f(x,y)$ with exactly four different stationary points that satisfy $(1),(2),(3)$, and, $(4)$? I try some function but I haven't found such function. For example, $$f(x,y)=x^3-\frac{1}{2}x^2+y^3-\frac{1}{2}y^2$$ has four different stationary points but it doesn't satisfy $(4)$. Meanwhile, $$g(x,y)=x^3-x^2+\frac{1}{4}y^4-\frac{1}{3}y^3$$ has four different stationary points but it doesn't satisfy $(3)$.",,['multivariable-calculus']
65,Priorities in Multiple Integration,Priorities in Multiple Integration,,"When doing a multiple integral it is often helpful/necessary to use a change of variables. In some cases the change is guided by making the region of the integration simpler - for example, turning a parallelogram into the unit square. In others, it is guided by making the integrand itself simpler, which is always the goal in single-integration. I was wondering if there are any general principles for deciding between these two priorities. Also, is changing variables in multiple integration just generally ""harder""? U-substitutions are obviously useful to me, but I can't shake off the feeling that most of the substitution examples in my multivariable calc book are somewhat contrived... how much is it actually used in practice?","When doing a multiple integral it is often helpful/necessary to use a change of variables. In some cases the change is guided by making the region of the integration simpler - for example, turning a parallelogram into the unit square. In others, it is guided by making the integrand itself simpler, which is always the goal in single-integration. I was wondering if there are any general principles for deciding between these two priorities. Also, is changing variables in multiple integration just generally ""harder""? U-substitutions are obviously useful to me, but I can't shake off the feeling that most of the substitution examples in my multivariable calc book are somewhat contrived... how much is it actually used in practice?",,[]
66,Setting up Triple Integral,Setting up Triple Integral,,"Evaluate the integral $$\int x^2+y^2+z^2 \, dV$$ over the region within the cone $z^2=x^2+y^2$  and the sphere $x^2+y^2+z^2=z$. I started to convert everything to cylindrical coordinates but it turned out to be a bigger mess to evaluate so I converted instead to spherical coordinates. I know theta goes from $0$ to $2\pi$ and $\phi$ ranges from $0$ to $\pi/4$. I'm struggling to find my values for $\rho$. I solve the equations for the cone and the sphere to get $\rho = \sqrt{z}$ and when I was solving the cone equation the $\rho$ cancelled. Can someone help me figure out my $\rho$ integral? I can evaluate the integral itself.","Evaluate the integral $$\int x^2+y^2+z^2 \, dV$$ over the region within the cone $z^2=x^2+y^2$  and the sphere $x^2+y^2+z^2=z$. I started to convert everything to cylindrical coordinates but it turned out to be a bigger mess to evaluate so I converted instead to spherical coordinates. I know theta goes from $0$ to $2\pi$ and $\phi$ ranges from $0$ to $\pi/4$. I'm struggling to find my values for $\rho$. I solve the equations for the cone and the sphere to get $\rho = \sqrt{z}$ and when I was solving the cone equation the $\rho$ cancelled. Can someone help me figure out my $\rho$ integral? I can evaluate the integral itself.",,['multivariable-calculus']
67,Solving exercise with Leibniz rule,Solving exercise with Leibniz rule,,"I'm asked to prove that if $f(x) = \left(\displaystyle\int_0^x e^{-t^2}dt \right)^2$ and $g(x) = \displaystyle\int_0^1 \displaystyle\frac{e^{-x^2(t^2+1)}}{t^2+1}dt$ then $f'(x)+g'(x)=0$ and conclude that $f(x)+g(x)=\displaystyle\frac{\pi}{4}$. I'm having some problems proving the first equality, but here's what I tried: For $f$ I just applied the chain rule: $\displaystyle\frac{d}{dx}\left(\displaystyle\int_0^x e^{-t^2}dt \right)^2 = 2\left(\displaystyle\int_0^x e^{-t^2}dt \right)\left(\displaystyle\frac{d}{dx}\displaystyle\int_0^x e^{-t^2}dt\right) = 2\left(\displaystyle\int_0^x e^{-t^2}dt \right)\left(e^{-x^2}\right) = 2e^{-x^2}\displaystyle\int_0^x e^{-t^2}dt$. And for $g$ I used Leibniz's Rule  $\displaystyle\frac{d}{dx}\displaystyle\int_0^1 \displaystyle\frac{e^{-x^2(t^2+1)}}{t^2+1}dt = \displaystyle\int_0^1 \displaystyle\frac{\partial}{\partial x}\left(\displaystyle\frac{e^{-x^2(t^2+1)}}{t^2+1}\right)dt $ $= \displaystyle\int_0^1 \displaystyle\frac{-2x(t^2+1)e^{-x^2(t^2+1)}}{t^2+1}dt = -2x\displaystyle\int_0^1 e^{-x^2(t^2+1)}dt$. Now, how can I prove that $f'+g'=0$?. I can't add the former integrals, to do so I should have both integrals from 0 to 1, which I tried to do as follows: Considering the first integral, taking the change of variables $u=\displaystyle\frac{t}{x}$ I'll have $\displaystyle\frac{du}{dx} =-\displaystyle\frac{t}{x^2}=-\displaystyle\frac{u}{x}$ and then for the first integral I'd have considering the change of variables $2e^{-x^2}\displaystyle\int_0^x e^{-t^2} dt= 2e^{-x^2}\displaystyle\int_0^1 e^{-u^2x^2}\left(-\displaystyle\frac{u}{x}\right)du$. But the latter one it seems to me that it isn't closer at all to an integral that I could add to the second one and have zero as a result. Did I mess up with the derivatives?, or the way I'm trying to solve the problem isn't the way at all? Thanks.","I'm asked to prove that if $f(x) = \left(\displaystyle\int_0^x e^{-t^2}dt \right)^2$ and $g(x) = \displaystyle\int_0^1 \displaystyle\frac{e^{-x^2(t^2+1)}}{t^2+1}dt$ then $f'(x)+g'(x)=0$ and conclude that $f(x)+g(x)=\displaystyle\frac{\pi}{4}$. I'm having some problems proving the first equality, but here's what I tried: For $f$ I just applied the chain rule: $\displaystyle\frac{d}{dx}\left(\displaystyle\int_0^x e^{-t^2}dt \right)^2 = 2\left(\displaystyle\int_0^x e^{-t^2}dt \right)\left(\displaystyle\frac{d}{dx}\displaystyle\int_0^x e^{-t^2}dt\right) = 2\left(\displaystyle\int_0^x e^{-t^2}dt \right)\left(e^{-x^2}\right) = 2e^{-x^2}\displaystyle\int_0^x e^{-t^2}dt$. And for $g$ I used Leibniz's Rule  $\displaystyle\frac{d}{dx}\displaystyle\int_0^1 \displaystyle\frac{e^{-x^2(t^2+1)}}{t^2+1}dt = \displaystyle\int_0^1 \displaystyle\frac{\partial}{\partial x}\left(\displaystyle\frac{e^{-x^2(t^2+1)}}{t^2+1}\right)dt $ $= \displaystyle\int_0^1 \displaystyle\frac{-2x(t^2+1)e^{-x^2(t^2+1)}}{t^2+1}dt = -2x\displaystyle\int_0^1 e^{-x^2(t^2+1)}dt$. Now, how can I prove that $f'+g'=0$?. I can't add the former integrals, to do so I should have both integrals from 0 to 1, which I tried to do as follows: Considering the first integral, taking the change of variables $u=\displaystyle\frac{t}{x}$ I'll have $\displaystyle\frac{du}{dx} =-\displaystyle\frac{t}{x^2}=-\displaystyle\frac{u}{x}$ and then for the first integral I'd have considering the change of variables $2e^{-x^2}\displaystyle\int_0^x e^{-t^2} dt= 2e^{-x^2}\displaystyle\int_0^1 e^{-u^2x^2}\left(-\displaystyle\frac{u}{x}\right)du$. But the latter one it seems to me that it isn't closer at all to an integral that I could add to the second one and have zero as a result. Did I mess up with the derivatives?, or the way I'm trying to solve the problem isn't the way at all? Thanks.",,"['multivariable-calculus', 'definite-integrals']"
68,"Given integral $\iint_D (e^{x^2 + y^2}) \,dx \,dy$ in the domain $D = \{(x, y) : x^2 + y^2 \le 2, 0 \le y \le x\}.$ Move to polar coordinates.",Given integral  in the domain  Move to polar coordinates.,"\iint_D (e^{x^2 + y^2}) \,dx \,dy D = \{(x, y) : x^2 + y^2 \le 2, 0 \le y \le x\}.","Given integral $\iint_D (e^{x^2 + y^2}) \,dx \,dy$ in the domain $D = \{(x, y) : x^2 + y^2 \le 2, 0 \le y \le x\}.$ Move to polar coordinates. First of all I tried to find the domain of $x$ and $y$: $0 \le y \le x$ (given) $0 \le x^2 + y^2 \le 2 \implies -y^2 \le x^2 \le 2 - y^2 \implies y \le x \le \sqrt{2 - y^2}$. If I rewrite the integral: $$\int^0_x\int_y^{\sqrt{2-y^2}} e^{x^2 + y^2} \,dx\,dy,$$ I'm getting that $x$ dependent with $y$, and $y$ dependent with $x$. I guess this is why I need to move to polar coordinates. But How can I do it? Thanks in advance.","Given integral $\iint_D (e^{x^2 + y^2}) \,dx \,dy$ in the domain $D = \{(x, y) : x^2 + y^2 \le 2, 0 \le y \le x\}.$ Move to polar coordinates. First of all I tried to find the domain of $x$ and $y$: $0 \le y \le x$ (given) $0 \le x^2 + y^2 \le 2 \implies -y^2 \le x^2 \le 2 - y^2 \implies y \le x \le \sqrt{2 - y^2}$. If I rewrite the integral: $$\int^0_x\int_y^{\sqrt{2-y^2}} e^{x^2 + y^2} \,dx\,dy,$$ I'm getting that $x$ dependent with $y$, and $y$ dependent with $x$. I guess this is why I need to move to polar coordinates. But How can I do it? Thanks in advance.",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
69,What if $\operatorname{div}f=0$?,What if ?,\operatorname{div}f=0,"Say, we have a function $f\in C^1(\mathbb R^2, \mathbb R^2)$ such that $\operatorname{div}f=0$. According to the divergence theorem the flux through the boundary surface of any solid region equals zero. So for $f(x,y)=(y^2,x^2)$ the flux through the boundary surface on the picture (sorry for its thickness, please treat it as a line) is zero. The result (if I interpret the theorem correctly) seems to be quite surprising. It looks like can also get non-zero flux by zero divergence. For example, $$g(x,y)=(-\frac{x}{x^2+y^2},-\frac{y}{x^2+y^2})$$ (see the next picture) has $\operatorname{div}g=0$ yet the flux is clearly negative. The function $g$ isn't continuous at $(0,0)$ and therefore not $C^1$. My first question is: are there any other cases where divergence is zero yet the flux isn't? The reasons I'm asking is the exercise I came across: Compute the surface integral   $$\int_{U}F  \cdot dS$$   where $F(x,y)=(y^3, z^3, x^3)$ and $U$ is the unit sphere. I didn't expect the exercise to be doable mentally (by simply noting that $\operatorname{div}F=0$ and concluding the integral is zero) yet $F$ is clearly $C^1$ so the divergence theorem seems to be applicable. My second question is: am I overlooking something in the exercise?","Say, we have a function $f\in C^1(\mathbb R^2, \mathbb R^2)$ such that $\operatorname{div}f=0$. According to the divergence theorem the flux through the boundary surface of any solid region equals zero. So for $f(x,y)=(y^2,x^2)$ the flux through the boundary surface on the picture (sorry for its thickness, please treat it as a line) is zero. The result (if I interpret the theorem correctly) seems to be quite surprising. It looks like can also get non-zero flux by zero divergence. For example, $$g(x,y)=(-\frac{x}{x^2+y^2},-\frac{y}{x^2+y^2})$$ (see the next picture) has $\operatorname{div}g=0$ yet the flux is clearly negative. The function $g$ isn't continuous at $(0,0)$ and therefore not $C^1$. My first question is: are there any other cases where divergence is zero yet the flux isn't? The reasons I'm asking is the exercise I came across: Compute the surface integral   $$\int_{U}F  \cdot dS$$   where $F(x,y)=(y^3, z^3, x^3)$ and $U$ is the unit sphere. I didn't expect the exercise to be doable mentally (by simply noting that $\operatorname{div}F=0$ and concluding the integral is zero) yet $F$ is clearly $C^1$ so the divergence theorem seems to be applicable. My second question is: am I overlooking something in the exercise?",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
70,Green function of the first quadrant,Green function of the first quadrant,,"Find the Green function of the first quadrant $x_1>0, x_2>0$ . HINT: Use the Green function of the half space $\Omega:=\left\{x\in\mathbb{R}^n : x_n > 0\right\}$ which is given by \begin{align*} G_n(x,y)&:=E_n(x_1-y_1,\cdots,x_{n-1}-y_{n-1},x_n-y_n)\\ &\quad\ -E_n(x_1-y_1,\cdots,x_{n-1}-y_{n-1},x_n+y_n). \end{align*} How I can find the Green function of the first quadrant? The difference to the half space is that I have another boundary…",Find the Green function of the first quadrant . HINT: Use the Green function of the half space which is given by How I can find the Green function of the first quadrant? The difference to the half space is that I have another boundary…,"x_1>0, x_2>0 \Omega:=\left\{x\in\mathbb{R}^n : x_n > 0\right\} \begin{align*}
G_n(x,y)&:=E_n(x_1-y_1,\cdots,x_{n-1}-y_{n-1},x_n-y_n)\\
&\quad\ -E_n(x_1-y_1,\cdots,x_{n-1}-y_{n-1},x_n+y_n).
\end{align*}","['multivariable-calculus', 'potential-theory']"
71,Product integration of differential forms,Product integration of differential forms,,"Let $\alpha, \beta$ two forms continuous with compact supports and maximum degree on surfaces oriented $M, N $ respectively. consider $\pi_M:M\times N \rightarrow M$ and $\pi_N:M\times N \rightarrow N$. Prove that $$\int_{M\times N}\pi^*_M\alpha \wedge\pi^*_N\beta=(\int_M \alpha )\ .(\int_N \beta ) $$ any suggestions to address the problem, is welcome","Let $\alpha, \beta$ two forms continuous with compact supports and maximum degree on surfaces oriented $M, N $ respectively. consider $\pi_M:M\times N \rightarrow M$ and $\pi_N:M\times N \rightarrow N$. Prove that $$\int_{M\times N}\pi^*_M\alpha \wedge\pi^*_N\beta=(\int_M \alpha )\ .(\int_N \beta ) $$ any suggestions to address the problem, is welcome",,"['integration', 'multivariable-calculus', 'differential-forms']"
72,Use polar coordinates to find the volume of the given solid,Use polar coordinates to find the volume of the given solid,,Use polar coordinates to find the volume of the given solid bounded by the paraboloid $z=1+2x^2+2y^2$ and the plane $z=7$ in the first octant. I did it. Is that right ? $$\int_0^{\pi \over 2} \int_0^{\sqrt{3}}(7-(1+2r^2))r dr d\theta = \frac{9\pi}{4} $$ Thanks,Use polar coordinates to find the volume of the given solid bounded by the paraboloid $z=1+2x^2+2y^2$ and the plane $z=7$ in the first octant. I did it. Is that right ? $$\int_0^{\pi \over 2} \int_0^{\sqrt{3}}(7-(1+2r^2))r dr d\theta = \frac{9\pi}{4} $$ Thanks,,"['calculus', 'multivariable-calculus', 'polar-coordinates']"
73,Taking the line integral of a region with holes with Green's Theorem,Taking the line integral of a region with holes with Green's Theorem,,"I am having trouble understanding how the author arrived at (4) from the step above it. The step above it consists of two double integrals. The first double integral is over the region $R_1$ and the second is over the region $R_2$. In the next step, they are converted to two line integrals: the first over the curve $C_1$ and the second over the curve $C_2$. However, the curve $C_1$ doesn't seem to cover the region $R_1$ and the curve $C_2$ doesn't seem to cover the region $R_2$. $C_1$ is the perimeter of the entire shape - not just the perimeter of $R_1$ and $C_2$ is the perimeter of the inner oval - not the perimeter of $R_2$.","I am having trouble understanding how the author arrived at (4) from the step above it. The step above it consists of two double integrals. The first double integral is over the region $R_1$ and the second is over the region $R_2$. In the next step, they are converted to two line integrals: the first over the curve $C_1$ and the second over the curve $C_2$. However, the curve $C_1$ doesn't seem to cover the region $R_1$ and the curve $C_2$ doesn't seem to cover the region $R_2$. $C_1$ is the perimeter of the entire shape - not just the perimeter of $R_1$ and $C_2$ is the perimeter of the inner oval - not the perimeter of $R_2$.",,"['calculus', 'integration', 'multivariable-calculus']"
74,Calculate integral applying Stokes' theorem,Calculate integral applying Stokes' theorem,,"I am trying to solve the following exercise: Let $F$ be the vector field defined by $F(x,y,z)=(-y,yz^2,x^2z)$ and $S \subset \mathbb R^3$ the surface defined as $S=\{x^2+y^2+z^2=4, z\geq 0\}$, oriented according to the exterior normal vector. Calculate: $\iint_S (\nabla\times F).dS$. The attempt at a solution: I've calculated the curl, it's not an easy integral to calculate. I can't apply Stokes' theorem because it is not a closed surface, but if I consider the surface $S^{*}=\{x^2+y^2+z^2=4, z\geq 0\} \cup \{ x^2+y^2\leq 4, z=0\}$, then this is a closed surface and $F$ is of class $C^1$, so Stokes'theorem says that: $\iint_S^{*} (\nabla\times F).dS=\int_CF.ds$ where $C$ is the boundary of the surface $S^{*}$. Now, my original integral is $\iint_S (\nabla\times F).dS=\iint_S^{*} (\nabla\times F).dS-\iint_D (\nabla\times F).dS $, where $D=\{ x^2+y^2\leq 4, z=0\}$. But as $D$ is a closed surface, I can also apply Stokes' theorem, so $\iint_D (\nabla\times F).dS=\int_{C'} F.ds $, where $C'$ is the boundary of $D$. Now, my question is: isn't $C=C'$?, I mean, the curve boundary of $S^{*}$ is the same boundary than the one of $D$. If this is the case $\iint_S (\nabla\times F).dS=\int_C F.ds-\int_{C'} F.ds=\int_C F.ds-\int_{C} F.ds=0$. Could someone tell me if my solution is correct?","I am trying to solve the following exercise: Let $F$ be the vector field defined by $F(x,y,z)=(-y,yz^2,x^2z)$ and $S \subset \mathbb R^3$ the surface defined as $S=\{x^2+y^2+z^2=4, z\geq 0\}$, oriented according to the exterior normal vector. Calculate: $\iint_S (\nabla\times F).dS$. The attempt at a solution: I've calculated the curl, it's not an easy integral to calculate. I can't apply Stokes' theorem because it is not a closed surface, but if I consider the surface $S^{*}=\{x^2+y^2+z^2=4, z\geq 0\} \cup \{ x^2+y^2\leq 4, z=0\}$, then this is a closed surface and $F$ is of class $C^1$, so Stokes'theorem says that: $\iint_S^{*} (\nabla\times F).dS=\int_CF.ds$ where $C$ is the boundary of the surface $S^{*}$. Now, my original integral is $\iint_S (\nabla\times F).dS=\iint_S^{*} (\nabla\times F).dS-\iint_D (\nabla\times F).dS $, where $D=\{ x^2+y^2\leq 4, z=0\}$. But as $D$ is a closed surface, I can also apply Stokes' theorem, so $\iint_D (\nabla\times F).dS=\int_{C'} F.ds $, where $C'$ is the boundary of $D$. Now, my question is: isn't $C=C'$?, I mean, the curve boundary of $S^{*}$ is the same boundary than the one of $D$. If this is the case $\iint_S (\nabla\times F).dS=\int_C F.ds-\int_{C'} F.ds=\int_C F.ds-\int_{C} F.ds=0$. Could someone tell me if my solution is correct?",,['multivariable-calculus']
75,Show that 2 surfaces are tangent in a given point,Show that 2 surfaces are tangent in a given point,,"Show that the surfaces $ \Large\frac{x^2}{a^2} + \Large\frac{y^2}{b^2} = \Large\frac{z^2}{c^2}$ and $ x^2 + y^2+ \left(z - \Large\frac{b^2 + c^2}{c} \right)^2 = \Large\frac{b^2}{c^2} \small(b^2 + c^2)$ are tangent at the point $(0, ±b,c)$ To show that 2 surfaces are tangent, is it necessary and suficient to show that both points are in both surfaces and the tangent plane at those points is the same? Because if we just show that both points are in both surfaces, the surfaces could just intercept each other. And if we just show the tangent plane is the same at 2 points of the surfaces, those points do not need to be the same. Thanks!","Show that the surfaces $ \Large\frac{x^2}{a^2} + \Large\frac{y^2}{b^2} = \Large\frac{z^2}{c^2}$ and $ x^2 + y^2+ \left(z - \Large\frac{b^2 + c^2}{c} \right)^2 = \Large\frac{b^2}{c^2} \small(b^2 + c^2)$ are tangent at the point $(0, ±b,c)$ To show that 2 surfaces are tangent, is it necessary and suficient to show that both points are in both surfaces and the tangent plane at those points is the same? Because if we just show that both points are in both surfaces, the surfaces could just intercept each other. And if we just show the tangent plane is the same at 2 points of the surfaces, those points do not need to be the same. Thanks!",,"['calculus', 'multivariable-calculus', 'surfaces']"
76,Taylor polynomial about the origin,Taylor polynomial about the origin,,"Find the 3rd degree Taylor polynomial about the origin of $$f(x,y)=\sin (x)\ln(1+y)$$ So I used this formula to calculate it $$p=f(0,0)+(f_x(0,0)x+f_y(0,0)y)+(\frac{1}{2}f_{xx}(0,0)x^2+f_{xy}(0,0)xy+\frac{1}{2}f_{yy}(0,0)y^2)+(\frac{1}{6}f_{xxx}(0,0)x^3+\frac{1}{2}f_{xxy}(0,0)x^2y+\frac{1}{2}f_{xyy}(0,0)xy^2+\frac{1}{6}f_{yyy}(0,0)y^3)$$ I get $x(\ln(1)+y-\frac{\ln(1)x^2}{6}-\frac{y^2}{2})$ But as you can see, this is a very tedious task (especially if I have to do this on my midterm). There exists a Taylor series for $\sin(x)$ and $\ln(1+y)$. If I only keep the terms with degree $\le 3$, I have $$\sin(x)\ln(1+y)=(x-\frac{x^3}{3!})(y-\frac{y^2}{2}+\frac{y^3}{3}) \\=xy-\frac{xy^2}{2}$$ (I multiply the two and remove terms with degree > 3 from the answer) The two polynomials are different. Is the second method even a valid way to determine Taylor polynomial?","Find the 3rd degree Taylor polynomial about the origin of $$f(x,y)=\sin (x)\ln(1+y)$$ So I used this formula to calculate it $$p=f(0,0)+(f_x(0,0)x+f_y(0,0)y)+(\frac{1}{2}f_{xx}(0,0)x^2+f_{xy}(0,0)xy+\frac{1}{2}f_{yy}(0,0)y^2)+(\frac{1}{6}f_{xxx}(0,0)x^3+\frac{1}{2}f_{xxy}(0,0)x^2y+\frac{1}{2}f_{xyy}(0,0)xy^2+\frac{1}{6}f_{yyy}(0,0)y^3)$$ I get $x(\ln(1)+y-\frac{\ln(1)x^2}{6}-\frac{y^2}{2})$ But as you can see, this is a very tedious task (especially if I have to do this on my midterm). There exists a Taylor series for $\sin(x)$ and $\ln(1+y)$. If I only keep the terms with degree $\le 3$, I have $$\sin(x)\ln(1+y)=(x-\frac{x^3}{3!})(y-\frac{y^2}{2}+\frac{y^3}{3}) \\=xy-\frac{xy^2}{2}$$ (I multiply the two and remove terms with degree > 3 from the answer) The two polynomials are different. Is the second method even a valid way to determine Taylor polynomial?",,"['multivariable-calculus', 'taylor-expansion']"
77,How to prove that the circumference is $C^k$?,How to prove that the circumference is ?,C^k,"Notation: $U$ is a open subset of $\mathbb{R}^n$; $\partial U$ is a boundary of $U$; A $C^k$ function is a function $k$-times continuosly differentiable; $B(x_0,r)$ is a ball in $\mathbb{R}^n$ with center $x_0$ and radius $r$; $x=(x_1,...,x_n)$ is a point of $\mathbb{R}^n$. Definition: We say $\partial U$ is $C^k$ is for each point $x_0\in \partial U$ there exist $r>0$ and a $C^k$ function $\gamma:\mathbb{R}^{n-1}\to \mathbb{R}$ such that - upon relabeling and reorienting the coordinates axis if necessary - we have $$U\cap B(x_0,r)=\{x\in B(x_0,r);\;x_n>\gamma(x_1,...,x_{n-1})\}$$ (Lawrence C. Evans, Partial Differential Equations) Problem: Let $U=\{x_1^2+x_2^2<1\}$. How to prove that $\partial U$ is $C^k$? My interpretation of problem (probably wrong) can be summarized in the following picture. Thanks.","Notation: $U$ is a open subset of $\mathbb{R}^n$; $\partial U$ is a boundary of $U$; A $C^k$ function is a function $k$-times continuosly differentiable; $B(x_0,r)$ is a ball in $\mathbb{R}^n$ with center $x_0$ and radius $r$; $x=(x_1,...,x_n)$ is a point of $\mathbb{R}^n$. Definition: We say $\partial U$ is $C^k$ is for each point $x_0\in \partial U$ there exist $r>0$ and a $C^k$ function $\gamma:\mathbb{R}^{n-1}\to \mathbb{R}$ such that - upon relabeling and reorienting the coordinates axis if necessary - we have $$U\cap B(x_0,r)=\{x\in B(x_0,r);\;x_n>\gamma(x_1,...,x_{n-1})\}$$ (Lawrence C. Evans, Partial Differential Equations) Problem: Let $U=\{x_1^2+x_2^2<1\}$. How to prove that $\partial U$ is $C^k$? My interpretation of problem (probably wrong) can be summarized in the following picture. Thanks.",,"['multivariable-calculus', 'partial-differential-equations']"
78,Divergence of stress tensor in momentum transfer equation,Divergence of stress tensor in momentum transfer equation,,"Let suppose that we work in a 2D cartesian coordinates. what will be x and y components of $\nabla.\left[-p I+\mu \left(\nabla \text{u}+(\nabla \text{u})^T\right)-\frac{2}{3} \mu (\nabla.\text{u})  I \right]$ $p=p(x,y)$ is pressure, $\mu$ is viscosity and constant, $I$ is identity tensor and $\text{u}=u \hat i +v \hat j$ is velocity field.","Let suppose that we work in a 2D cartesian coordinates. what will be x and y components of $\nabla.\left[-p I+\mu \left(\nabla \text{u}+(\nabla \text{u})^T\right)-\frac{2}{3} \mu (\nabla.\text{u})  I \right]$ $p=p(x,y)$ is pressure, $\mu$ is viscosity and constant, $I$ is identity tensor and $\text{u}=u \hat i +v \hat j$ is velocity field.",,"['multivariable-calculus', 'tensor-products', 'vector-analysis', 'tensors', 'differential-operators']"
79,Maximum and minimum function on circle,Maximum and minimum function on circle,,"Find minimum and maximum value of function $f(x,y) = 3x+4y + |x-y|$ on circle $$\left\{ (x,y): x^2+y^2 = 1 \right\}$$ I used polar coordinate system. So I have $x = \cos t$ and $y=\sin t$ where $ t \in [0, 2 \pi)$. Then i exploited definition of absolute function and i got: $$h(t) = \begin{cases} 4 \cos t + 3 \sin t \quad t \in [0, \frac{\pi}{4}] \cup [\frac{5}{4} \pi,2 \pi) \\ 2 \cos t + 5 \sin t \quad t \in (\frac{\pi}{4}, \frac{5}{4} \pi) \end{cases}$$ Hence i received following critical points (earlier i computed first derivative): $$\cos t = \pm \frac{4}{5} \vee \cos t = \pm \frac{2}{ \sqrt{29} }$$ Then i computed second derivative and after all i received that in $( \frac{2}{ \sqrt{29} } , \frac{5}{ \sqrt{29} }) $ is maximum equal $\sqrt{29}$ and in $(- \frac{4}{ 5 } ,- \frac{3}{5}  )$ is minimum equal $- \frac{23}{5}$. I examined my resolution in wolfram alpha: How you can see, this resolutions are very different than my. Even the crital points are other. Could you tell me where i have mistake?","Find minimum and maximum value of function $f(x,y) = 3x+4y + |x-y|$ on circle $$\left\{ (x,y): x^2+y^2 = 1 \right\}$$ I used polar coordinate system. So I have $x = \cos t$ and $y=\sin t$ where $ t \in [0, 2 \pi)$. Then i exploited definition of absolute function and i got: $$h(t) = \begin{cases} 4 \cos t + 3 \sin t \quad t \in [0, \frac{\pi}{4}] \cup [\frac{5}{4} \pi,2 \pi) \\ 2 \cos t + 5 \sin t \quad t \in (\frac{\pi}{4}, \frac{5}{4} \pi) \end{cases}$$ Hence i received following critical points (earlier i computed first derivative): $$\cos t = \pm \frac{4}{5} \vee \cos t = \pm \frac{2}{ \sqrt{29} }$$ Then i computed second derivative and after all i received that in $( \frac{2}{ \sqrt{29} } , \frac{5}{ \sqrt{29} }) $ is maximum equal $\sqrt{29}$ and in $(- \frac{4}{ 5 } ,- \frac{3}{5}  )$ is minimum equal $- \frac{23}{5}$. I examined my resolution in wolfram alpha: How you can see, this resolutions are very different than my. Even the crital points are other. Could you tell me where i have mistake?",,"['calculus', 'multivariable-calculus', 'optimization']"
80,Triple Integral over a shifted sphere,Triple Integral over a shifted sphere,,"I am interested in the following: Let $f(x,y,z)$ be a given (known) function in Cartesian coordinates, and let $B_d(p_0) = \{ y: ||y-p_0|| < d \}$ (i.e., a sphere centered at p_0). I want to numerically compute in MATLAB $$\int \int \int_{B_d(p_0)} f(x,y,z) dx dy dz$$ Now, I remember that if $p_0 = 0$, this isn't too bad: I just change to spherical coordinates and compute $$\int_0^{2 \pi} \int_0^{\pi} \int_0^d f(r,\theta,\phi) r^2 \sin(\theta) dr d\theta d \phi$$ where $\theta$ is my polar angle (from the positive z-axis down) and $\phi$ is my azimuthal angle (from the positive x-axis in the x-y  plane). In this form, this is just an integral over a cube , and I can just call integral3 on this, with my integrand being $f(r,\theta,\phi) r^2 \sin(\theta)$. But, if I have a circle centered elsewhere, it seems a bit trickier. I thought I could shift variables as such: Let $p_0 = (x_0,y_0,z_0)$. Then, set $x' = x-x_0, y' = y-y_0, z' = z-z_0$. I believe that this linear shift won't affect the integration, so now I have $$\int \int \int_{B_d(0)} f(x',y',z') dx' dy' dz'$$ This I can now switch to polar coordinates as $$\int_0^{2\pi} \int_0^{\pi} \int_0^d f(r',\theta',\phi') r'^2 \sin(\theta') dr d\theta' d\phi'$$. with $r' = x'^2 + y'^2 + z'^2$, $\theta' = arccos(\frac{z'}{r'})$, $\phi' = arctan\frac{y'}{x'})$.  I guess I'm worried about how to actually input this; there has to be some additional shifting or changing going on, because if I just tell MATLAB to integrate $f(r',\theta',\phi')$, over a sphere of radius 0, it looks to me like it would be no different than integrating over $(r,\theta,\phi)$ without the shift. Is there some change to the Jacobian or something I need to do to reflect this shift of variables to this shifted polar coordinate system? Would I first define a function $f(x,y,z)$, then define a new shifted function $g(x',y',z') = f(x,y,z)$, then spherical coordinate $g$? For example, let's say we want to integrate $f(x,y,z) = \sin(x) \sin(y) \sin(z)$ over a ball of radius 1 centered at (2,3,4). First, I can define $x' = x-2, y' = y-3, z' = z-4$, and I have $f(x',y',z') = \sin(x'+2) \sin(y'+3) \sin(z'+4)$. Then, I can define my spherical coordinate system, and I replace $x' = r' \sin(\theta') \cos(\phi'), y' = r' \sin(\theta') \sin(\phi')$ and $z' = r' \cos(\theta')$. This leads to the final integrand being (including the Jacboian from the spherical coordinate change) $$F(r',\theta',\phi') = \sin(r'\sin(\theta')\cos(\phi')+2) \sin(r' \sin(\theta') \sin(\phi')+3) \sin(r' \cos(\theta')+4) r'^2 \sin(\theta')$$ This function i can then apply a Matlab routine like integral3 over $0\leq r' \leq 1$, $0 \leq \theta' \leq \pi$ and $0 \leq \phi' \leq 2\pi$. Does this sound correct? Thanks!","I am interested in the following: Let $f(x,y,z)$ be a given (known) function in Cartesian coordinates, and let $B_d(p_0) = \{ y: ||y-p_0|| < d \}$ (i.e., a sphere centered at p_0). I want to numerically compute in MATLAB $$\int \int \int_{B_d(p_0)} f(x,y,z) dx dy dz$$ Now, I remember that if $p_0 = 0$, this isn't too bad: I just change to spherical coordinates and compute $$\int_0^{2 \pi} \int_0^{\pi} \int_0^d f(r,\theta,\phi) r^2 \sin(\theta) dr d\theta d \phi$$ where $\theta$ is my polar angle (from the positive z-axis down) and $\phi$ is my azimuthal angle (from the positive x-axis in the x-y  plane). In this form, this is just an integral over a cube , and I can just call integral3 on this, with my integrand being $f(r,\theta,\phi) r^2 \sin(\theta)$. But, if I have a circle centered elsewhere, it seems a bit trickier. I thought I could shift variables as such: Let $p_0 = (x_0,y_0,z_0)$. Then, set $x' = x-x_0, y' = y-y_0, z' = z-z_0$. I believe that this linear shift won't affect the integration, so now I have $$\int \int \int_{B_d(0)} f(x',y',z') dx' dy' dz'$$ This I can now switch to polar coordinates as $$\int_0^{2\pi} \int_0^{\pi} \int_0^d f(r',\theta',\phi') r'^2 \sin(\theta') dr d\theta' d\phi'$$. with $r' = x'^2 + y'^2 + z'^2$, $\theta' = arccos(\frac{z'}{r'})$, $\phi' = arctan\frac{y'}{x'})$.  I guess I'm worried about how to actually input this; there has to be some additional shifting or changing going on, because if I just tell MATLAB to integrate $f(r',\theta',\phi')$, over a sphere of radius 0, it looks to me like it would be no different than integrating over $(r,\theta,\phi)$ without the shift. Is there some change to the Jacobian or something I need to do to reflect this shift of variables to this shifted polar coordinate system? Would I first define a function $f(x,y,z)$, then define a new shifted function $g(x',y',z') = f(x,y,z)$, then spherical coordinate $g$? For example, let's say we want to integrate $f(x,y,z) = \sin(x) \sin(y) \sin(z)$ over a ball of radius 1 centered at (2,3,4). First, I can define $x' = x-2, y' = y-3, z' = z-4$, and I have $f(x',y',z') = \sin(x'+2) \sin(y'+3) \sin(z'+4)$. Then, I can define my spherical coordinate system, and I replace $x' = r' \sin(\theta') \cos(\phi'), y' = r' \sin(\theta') \sin(\phi')$ and $z' = r' \cos(\theta')$. This leads to the final integrand being (including the Jacboian from the spherical coordinate change) $$F(r',\theta',\phi') = \sin(r'\sin(\theta')\cos(\phi')+2) \sin(r' \sin(\theta') \sin(\phi')+3) \sin(r' \cos(\theta')+4) r'^2 \sin(\theta')$$ This function i can then apply a Matlab routine like integral3 over $0\leq r' \leq 1$, $0 \leq \theta' \leq \pi$ and $0 \leq \phi' \leq 2\pi$. Does this sound correct? Thanks!",,"['multivariable-calculus', 'integration', 'spherical-coordinates']"
81,A differential form to compute the k-volume of a k-parallelogram in n dimensions,A differential form to compute the k-volume of a k-parallelogram in n dimensions,,"Computing the k-volume of a k-parallelogram (i.e. a parallelogram spanned by k n-dimensional vectors) in n dimensions is straightforward: Let $P=[\overrightarrow{v_1},...,\overrightarrow{v_k}]$, then the k-volume is equal to $\sqrt{det(P^{T}P)}$. For instance, if n=3 and k=2, then this formula reduces to the norm of the cross-product and thus gives the 2-volume (area) of the parallelogram spanned by two 3-D vectors. Now this fact is very useful when trying to find the k-volumes of manifolds (e.g. areas of 2-D surfaces in 3-D) by integration. My question is, is there differential form or a form field that would serve the same purpose? In $\mathbb{R}^3$ for instance, the elementary 2-forms give you the signed area of the ""projection"" of the 2-parallelogram onto (x,y) or (x,z) or (y,z) planes depending on the form you use, but not the area of 2-parallelogram living in the space.","Computing the k-volume of a k-parallelogram (i.e. a parallelogram spanned by k n-dimensional vectors) in n dimensions is straightforward: Let $P=[\overrightarrow{v_1},...,\overrightarrow{v_k}]$, then the k-volume is equal to $\sqrt{det(P^{T}P)}$. For instance, if n=3 and k=2, then this formula reduces to the norm of the cross-product and thus gives the 2-volume (area) of the parallelogram spanned by two 3-D vectors. Now this fact is very useful when trying to find the k-volumes of manifolds (e.g. areas of 2-D surfaces in 3-D) by integration. My question is, is there differential form or a form field that would serve the same purpose? In $\mathbb{R}^3$ for instance, the elementary 2-forms give you the signed area of the ""projection"" of the 2-parallelogram onto (x,y) or (x,z) or (y,z) planes depending on the form you use, but not the area of 2-parallelogram living in the space.",,"['linear-algebra', 'multivariable-calculus', 'differential-forms']"
82,The implication of zero mixed partial derivatives for multivariate function's minimization,The implication of zero mixed partial derivatives for multivariate function's minimization,,"Suppose $f(\textbf x)=f(x_1,x_2) $ has mixed partial derivatives $f''_{12}=f''_{21}=0$, so can I say: there exist $f_1(x_1)$ and $f_2(x_2)$ such that  $\min_{\textbf x} f(\textbf x)\equiv \min_{x_1}f_1(x_1)+ \min_{x_2}f_2(x_2)$? Or even further, as follows:  $$f(\textbf x)\equiv f_1(x_1)+ f_2(x_2)$$ A positive simple case is $f(x_1,x_2)=x_1^2+x_2^3$. I can not think of any opposite  cases, but I am not so sure about it and may need a proof.","Suppose $f(\textbf x)=f(x_1,x_2) $ has mixed partial derivatives $f''_{12}=f''_{21}=0$, so can I say: there exist $f_1(x_1)$ and $f_2(x_2)$ such that  $\min_{\textbf x} f(\textbf x)\equiv \min_{x_1}f_1(x_1)+ \min_{x_2}f_2(x_2)$? Or even further, as follows:  $$f(\textbf x)\equiv f_1(x_1)+ f_2(x_2)$$ A positive simple case is $f(x_1,x_2)=x_1^2+x_2^3$. I can not think of any opposite  cases, but I am not so sure about it and may need a proof.",,"['multivariable-calculus', 'optimization', 'partial-derivative']"
83,l'Hôpital's Rule and Multivariable Limits,l'Hôpital's Rule and Multivariable Limits,,"I decided to post another message regarding this problem because I still didn't understand it at all: Can someone give me an example of function $f(x,y),g(x,y)$ for which: $\lim\limits_{r\to 0^+} \dfrac {f(r\cos \theta , r\sin \theta ) }{ g(r\cos\theta , r\sin \theta)} =\dfrac{0}{0}$, and $\lim\limits_{r\to 0^+} \dfrac {\frac{\mathrm df(r\cos \theta , r\sin \theta )}{\mathrm dr} }{ \frac{\mathrm dg(r\cos\theta , r\sin \theta )}{\mathrm dr} } = C $ for some constant $C$ , but the actual limit $\lim\limits_{(x,y)\to (0,0)} \dfrac {f(x,y)}{g(x,y)}$ does not exist at all? All I need is an example of a case where l'Hôpital's rule for multivariable limits when appearing in polar coordinates is not helpful (the reason such an example must exist is because $\theta$ can also depend on $r$ , but when using l'Hôpital's rule wrt $r$, we consider $\theta$ to be a constant... ). Hope someone will be able to help me this time. Thanks in advance. Just to clarify things- This is not a homework question... Only something I thought about...","I decided to post another message regarding this problem because I still didn't understand it at all: Can someone give me an example of function $f(x,y),g(x,y)$ for which: $\lim\limits_{r\to 0^+} \dfrac {f(r\cos \theta , r\sin \theta ) }{ g(r\cos\theta , r\sin \theta)} =\dfrac{0}{0}$, and $\lim\limits_{r\to 0^+} \dfrac {\frac{\mathrm df(r\cos \theta , r\sin \theta )}{\mathrm dr} }{ \frac{\mathrm dg(r\cos\theta , r\sin \theta )}{\mathrm dr} } = C $ for some constant $C$ , but the actual limit $\lim\limits_{(x,y)\to (0,0)} \dfrac {f(x,y)}{g(x,y)}$ does not exist at all? All I need is an example of a case where l'Hôpital's rule for multivariable limits when appearing in polar coordinates is not helpful (the reason such an example must exist is because $\theta$ can also depend on $r$ , but when using l'Hôpital's rule wrt $r$, we consider $\theta$ to be a constant... ). Hope someone will be able to help me this time. Thanks in advance. Just to clarify things- This is not a homework question... Only something I thought about...",,['multivariable-calculus']
84,Find the volume of the region bounded by $z = x^2 + y^2$ and $z = 10 - x^2 - 2y^2$,Find the volume of the region bounded by  and,z = x^2 + y^2 z = 10 - x^2 - 2y^2,So these are two paraboloids. My guess is I would want to find the intersection of these two which would be $2x^2 + 3y^2 = 10$ and construct a triple integral based on its projection. No idea how to do this but the answer comes out to be $\dfrac{50\pi}{\sqrt{6}}$.,So these are two paraboloids. My guess is I would want to find the intersection of these two which would be $2x^2 + 3y^2 = 10$ and construct a triple integral based on its projection. No idea how to do this but the answer comes out to be $\dfrac{50\pi}{\sqrt{6}}$.,,['multivariable-calculus']
85,Chain rule to find geodesics,Chain rule to find geodesics,,"I'm reading these notes but I don't know how is the chain rule being applied on page 92. It says that we start with $\dfrac{dv}{du}=\dfrac{v'}{u'}$, having ruled out the case where $u'=0$. Then, he differentiates this wrt $u$ using the chain rule and goes this way $$\dfrac{d^2v}{du^2}=\dfrac{d}{dv}\left(\dfrac{v'}{u'}\right)=\dfrac{u'v''-v'u''}{u'^2}\dfrac{1}{u'}$$ I don't really know why he got that. I understand $u$ and $v$ are functions of $t$, one does not depend on the other, so... Thanks for any help.","I'm reading these notes but I don't know how is the chain rule being applied on page 92. It says that we start with $\dfrac{dv}{du}=\dfrac{v'}{u'}$, having ruled out the case where $u'=0$. Then, he differentiates this wrt $u$ using the chain rule and goes this way $$\dfrac{d^2v}{du^2}=\dfrac{d}{dv}\left(\dfrac{v'}{u'}\right)=\dfrac{u'v''-v'u''}{u'^2}\dfrac{1}{u'}$$ I don't really know why he got that. I understand $u$ and $v$ are functions of $t$, one does not depend on the other, so... Thanks for any help.",,['multivariable-calculus']
86,Partial Derivatives vs Implicit Differentiation,Partial Derivatives vs Implicit Differentiation,,"The question is: Let $G(x,y)=x^2y^4-3x^4y$. (i) Find the first partial derivatives $G_x$ and $G_y$. (ii) Using (i) above, find $\frac{dy}{dx}$. (iii) If $G(x,y)=0$, confirm your answer in part (ii) above, finding $\frac{dy}{dx}$ using implicit differentiation.","The question is: Let $G(x,y)=x^2y^4-3x^4y$. (i) Find the first partial derivatives $G_x$ and $G_y$. (ii) Using (i) above, find $\frac{dy}{dx}$. (iii) If $G(x,y)=0$, confirm your answer in part (ii) above, finding $\frac{dy}{dx}$ using implicit differentiation.",,['multivariable-calculus']
87,Stokes' and Divergence Theorem Problems,Stokes' and Divergence Theorem Problems,,"I have 2 questions on stokes and divergence theorem each. I think I have done both and I just want to make sure that I did them correctly. Question 1 Let $C$ be the boundary of the surface $S={(x,y,z):z=4-x^2-y^2,x^2+y^2\le4, x,y\ge0}$ with orientation related by the right-hand rule to the upward orientation of $S$. For $E(x,y,z)=[3yz,zx,2xy]^T$, apply Stoke's Theorem to calculate the closed line integral $\int E\cdot Tds$ My Work So by Stokes' theorem I know  $$\int E\cdot Tds = \iint(\nabla \times E)\cdot dS$$ where $dS=[-\frac{\partial z}{\partial x}, -\frac{\partial z}{\partial y}, 1]$. So $\nabla \times E=[2x,0,-2x]$ and $dS=[-2x,-2y,1]$ which leads to the double integral  $$\iint -4x^2-2x  \mathrm{d}x\mathrm{d}y$$  but we can change this into polar coordinates. So we have  $$\int_0^{2\pi}\int_0^2 (-4(r\cos\theta)^2-2(r\cos\theta))rdrd\theta$$  which equals $-\frac{32\pi}{3}$. Question 2 Apply the Divergence Theorem to calculate the normal surface integral of $E$ over $S$, where $S$ is the sphere of radius 3 centered at $(0,0,0)$ oriented outward and $E(x,y,z)=[x^3+yz,y^3+zx,z^3+xy]^T$ My Work Divergence theorem says  $$\iint E\cdot \vec ndr=\iiint(\nabla \cdot E)dxdydz.$$  So the $\nabla \cdot E = 3(x^2+y^2+z^2)$ which screams spherical coordinates. So my triple integral is now  $$\int_0^3\int_0^{2\pi}\int_0^\pi (3p^2)p^2\sin(\phi)d\phi d\theta dp= \frac{2916\pi}{5}$$ Are both of those correct?","I have 2 questions on stokes and divergence theorem each. I think I have done both and I just want to make sure that I did them correctly. Question 1 Let $C$ be the boundary of the surface $S={(x,y,z):z=4-x^2-y^2,x^2+y^2\le4, x,y\ge0}$ with orientation related by the right-hand rule to the upward orientation of $S$. For $E(x,y,z)=[3yz,zx,2xy]^T$, apply Stoke's Theorem to calculate the closed line integral $\int E\cdot Tds$ My Work So by Stokes' theorem I know  $$\int E\cdot Tds = \iint(\nabla \times E)\cdot dS$$ where $dS=[-\frac{\partial z}{\partial x}, -\frac{\partial z}{\partial y}, 1]$. So $\nabla \times E=[2x,0,-2x]$ and $dS=[-2x,-2y,1]$ which leads to the double integral  $$\iint -4x^2-2x  \mathrm{d}x\mathrm{d}y$$  but we can change this into polar coordinates. So we have  $$\int_0^{2\pi}\int_0^2 (-4(r\cos\theta)^2-2(r\cos\theta))rdrd\theta$$  which equals $-\frac{32\pi}{3}$. Question 2 Apply the Divergence Theorem to calculate the normal surface integral of $E$ over $S$, where $S$ is the sphere of radius 3 centered at $(0,0,0)$ oriented outward and $E(x,y,z)=[x^3+yz,y^3+zx,z^3+xy]^T$ My Work Divergence theorem says  $$\iint E\cdot \vec ndr=\iiint(\nabla \cdot E)dxdydz.$$  So the $\nabla \cdot E = 3(x^2+y^2+z^2)$ which screams spherical coordinates. So my triple integral is now  $$\int_0^3\int_0^{2\pi}\int_0^\pi (3p^2)p^2\sin(\phi)d\phi d\theta dp= \frac{2916\pi}{5}$$ Are both of those correct?",,"['integration', 'multivariable-calculus', 'vector-analysis', 'polar-coordinates', 'spherical-coordinates']"
88,Calculus 3 Explained,Calculus 3 Explained,,"I am trying to learn some calculus 3 and I understand HOW to do the problems but I just don't understand WHY I'm doing what I'm doing. So does anyone have any good recommendations on books that are really down to earth, and explain the concepts in terms that humans can understand. Here are the topics that I want to understand: A. Calculation of Geometric Quantities    1) Surface Area    2) Arc Length    3) Curvature of Paths       a) velocity       b) speed       c) acceleration          i. tangential component          ii. normal component B. Line integrals of vector fields    1) fundamental theorem of line integrals    2) green's theorem    3) finding the underlying scalar field for a conservative vector field    4) direct calculation of a line integral","I am trying to learn some calculus 3 and I understand HOW to do the problems but I just don't understand WHY I'm doing what I'm doing. So does anyone have any good recommendations on books that are really down to earth, and explain the concepts in terms that humans can understand. Here are the topics that I want to understand: A. Calculation of Geometric Quantities    1) Surface Area    2) Arc Length    3) Curvature of Paths       a) velocity       b) speed       c) acceleration          i. tangential component          ii. normal component B. Line integrals of vector fields    1) fundamental theorem of line integrals    2) green's theorem    3) finding the underlying scalar field for a conservative vector field    4) direct calculation of a line integral",,"['calculus', 'reference-request', 'integration', 'multivariable-calculus']"
89,"How to find the boundary curve of a surface, like the Möbius strip?","How to find the boundary curve of a surface, like the Möbius strip?",,"I feel like I am missing a key piece of intuition in trying to understand this. I have just recently started using Stoke's theorem and I struggle to see what the boundary curve of surfaces are. In some cases it is easy... like a hemisphere for example. But what about the boundary curve of the surface given below Could someone explain what this boundary curve of the surface below and just some basics of how to find it. I have read and re-read my book but all  of the examples in there are things like a hemisphere. Wait so i'm not so sure it is a torus anymore, it's been a long day, the surface is given by: $$x=[1+u\cos (t)]\cos (2t), \quad y=[1+u\cos (t)]\sin (2t), \quad z=u\sin (t)$$ $$- \frac{1}{2} \le u \le \frac{1}{2} \quad ,  0 \le t \le \pi$$ Thanks :-)","I feel like I am missing a key piece of intuition in trying to understand this. I have just recently started using Stoke's theorem and I struggle to see what the boundary curve of surfaces are. In some cases it is easy... like a hemisphere for example. But what about the boundary curve of the surface given below Could someone explain what this boundary curve of the surface below and just some basics of how to find it. I have read and re-read my book but all  of the examples in there are things like a hemisphere. Wait so i'm not so sure it is a torus anymore, it's been a long day, the surface is given by: $$x=[1+u\cos (t)]\cos (2t), \quad y=[1+u\cos (t)]\sin (2t), \quad z=u\sin (t)$$ $$- \frac{1}{2} \le u \le \frac{1}{2} \quad ,  0 \le t \le \pi$$ Thanks :-)",,['multivariable-calculus']
90,continuously differential parametrization of a tricky surface,continuously differential parametrization of a tricky surface,,"I'm looking for a continuously differentiable parametrization of $$x^3+y^2-z^2=1$$ but I'm actually totally stuck. If the $x$ term were quadratic instead of cubic, it would be simple: $$(x,y,z)=(\sqrt{t^2+1}\cos\theta, \sqrt{t^2+1}\sin\theta, t)$$ But with the cubic term there, I'm stuck. I naturally thought about $$(x,y,z)=(\sqrt[3]{t^2+1}\cos^{\frac{2}{3}}\theta, \sqrt{t^2+1}\sin\theta, t)$$ but this isn't continuously differentiable in $\theta$. Hints or suggestions?","I'm looking for a continuously differentiable parametrization of $$x^3+y^2-z^2=1$$ but I'm actually totally stuck. If the $x$ term were quadratic instead of cubic, it would be simple: $$(x,y,z)=(\sqrt{t^2+1}\cos\theta, \sqrt{t^2+1}\sin\theta, t)$$ But with the cubic term there, I'm stuck. I naturally thought about $$(x,y,z)=(\sqrt[3]{t^2+1}\cos^{\frac{2}{3}}\theta, \sqrt{t^2+1}\sin\theta, t)$$ but this isn't continuously differentiable in $\theta$. Hints or suggestions?",,['multivariable-calculus']
91,find length of curve of intersection,find length of curve of intersection,,"I have come to a dead end on a problem and I need someone to tell me either if I did it correctly, or how to fix it if I did not.  This is Stewart Calculus 7th edition, problem 13.3.12. Here is the problem definition: ""Find, correct to four decimal places, the length of the curve of intersection of the cylinder $4x^2 + y^2 = 4$ and the plane $x + y + z = 2$."" The answer I got is 13.5191, but I am asking for people here to review this because I reached a seemingly unsolvable integral and I had to resort to using my calculator. Here is my work: $$x^2 +(1/4)y^2 = 1$$ $$\begin{align*}x &= \cos t \\ y &= 2 \sin t  \\ z &= 2 - \cos t - 2\sin t \end{align*}$$ for $ 0 \le t \le 2\pi $. $$ r'(t) = (-\sin t, 2\cos t, (\sin t - 2 \cos t)) $$ Then $$  L = \int_a^b |r'(t)| dt $$ I then squared each component of $r'(t)$, summed the components, and took the square root to define the integral. I simplified twice to get: $$ \int_0^{2\pi} \sqrt{2\sin^2 t + 8\cos^2 t - 4\sin t \cos t} dt $$ For the interval 0 to $2\pi$, the calculator gives 13.51908915 Can anyone show me the solution to this?","I have come to a dead end on a problem and I need someone to tell me either if I did it correctly, or how to fix it if I did not.  This is Stewart Calculus 7th edition, problem 13.3.12. Here is the problem definition: ""Find, correct to four decimal places, the length of the curve of intersection of the cylinder $4x^2 + y^2 = 4$ and the plane $x + y + z = 2$."" The answer I got is 13.5191, but I am asking for people here to review this because I reached a seemingly unsolvable integral and I had to resort to using my calculator. Here is my work: $$x^2 +(1/4)y^2 = 1$$ $$\begin{align*}x &= \cos t \\ y &= 2 \sin t  \\ z &= 2 - \cos t - 2\sin t \end{align*}$$ for $ 0 \le t \le 2\pi $. $$ r'(t) = (-\sin t, 2\cos t, (\sin t - 2 \cos t)) $$ Then $$  L = \int_a^b |r'(t)| dt $$ I then squared each component of $r'(t)$, summed the components, and took the square root to define the integral. I simplified twice to get: $$ \int_0^{2\pi} \sqrt{2\sin^2 t + 8\cos^2 t - 4\sin t \cos t} dt $$ For the interval 0 to $2\pi$, the calculator gives 13.51908915 Can anyone show me the solution to this?",,"['multivariable-calculus', 'plane-curves', 'parametric']"
92,Using the chain rule to compute $f_\theta$ given $f_x$ and $f_y$,Using the chain rule to compute  given  and,f_\theta f_x f_y,"This is an exam practice question. Suppose that $f(x, y)$ is a function of two variables with $f_x(0, 2) = 2$ and $f_y(0, 2) = -1$. Using the chain rule, compute the numerical value of $f_\theta(r\cos\theta, r\sin\theta)$ at $r=2$, $\theta=\pi/2$. So for this question, I have $x = r\cos\theta$ and $y = r\sin\theta$. Using the chain rule, I have: \[ f_\theta(r\cos\theta, r\sin\theta) = f_x(x,y)\frac d{d\theta}(r\cos\theta) + f_y(x,y)\frac d{d\theta}(r\sin\theta) \] \[ f_\theta(2\cos\pi/2, 2\sin\pi/2) = (2)(-2\sin\pi/2) - 1(2\cos\pi/2) = -4 \] Do I have the right idea here? Or am I totally off?","This is an exam practice question. Suppose that $f(x, y)$ is a function of two variables with $f_x(0, 2) = 2$ and $f_y(0, 2) = -1$. Using the chain rule, compute the numerical value of $f_\theta(r\cos\theta, r\sin\theta)$ at $r=2$, $\theta=\pi/2$. So for this question, I have $x = r\cos\theta$ and $y = r\sin\theta$. Using the chain rule, I have: \[ f_\theta(r\cos\theta, r\sin\theta) = f_x(x,y)\frac d{d\theta}(r\cos\theta) + f_y(x,y)\frac d{d\theta}(r\sin\theta) \] \[ f_\theta(2\cos\pi/2, 2\sin\pi/2) = (2)(-2\sin\pi/2) - 1(2\cos\pi/2) = -4 \] Do I have the right idea here? Or am I totally off?",,['multivariable-calculus']
93,iterated integration and shuffle product,iterated integration and shuffle product,,"Recall that a $(r,s)$-shuffle $\sigma$ is a permutation of $r+s$ letters such that $\sigma^{-1}(1) < \cdots< \sigma^{-1}(r)$ and $\sigma^{-1}(r+1) < \cdots < \sigma^{-1}(r+s)$. If $f_1(t), f_2(t),\ldots,f_r(t)$ are piecewise continuous functions, define inductively $$\int_a^bf_1\;dt\cdots f_r\;dt = \int_a^b \left(\int_a^t f_1\;dt \cdots f_{r-1}\;dt\right)f_r(t)\;dt.$$ Can I have some help in showing the following formula: $$\left(\int_a^bf_1\;dt\cdots f_r\;dt\right)\left(\int_a^bf_{r+1}\;dt\cdots f_{r+s}\;dt\right)=\sum \int_a^bf_{\sigma(1)}\;dt\cdots f_{\sigma(r+s)}\;dt$$ where the sum in the right hand side runs through all $(r,s)$ shuffles. Thanks.","Recall that a $(r,s)$-shuffle $\sigma$ is a permutation of $r+s$ letters such that $\sigma^{-1}(1) < \cdots< \sigma^{-1}(r)$ and $\sigma^{-1}(r+1) < \cdots < \sigma^{-1}(r+s)$. If $f_1(t), f_2(t),\ldots,f_r(t)$ are piecewise continuous functions, define inductively $$\int_a^bf_1\;dt\cdots f_r\;dt = \int_a^b \left(\int_a^t f_1\;dt \cdots f_{r-1}\;dt\right)f_r(t)\;dt.$$ Can I have some help in showing the following formula: $$\left(\int_a^bf_1\;dt\cdots f_r\;dt\right)\left(\int_a^bf_{r+1}\;dt\cdots f_{r+s}\;dt\right)=\sum \int_a^bf_{\sigma(1)}\;dt\cdots f_{\sigma(r+s)}\;dt$$ where the sum in the right hand side runs through all $(r,s)$ shuffles. Thanks.",,"['multivariable-calculus', 'differential-forms']"
94,Partial differentiability and Continuity,Partial differentiability and Continuity,,"If a function has, say, partial derivatives up to order n, can you conclude continuity of some or all  derivatives of lower order? Especially, if a function has partial derivatives of any order is it automatically smooth?","If a function has, say, partial derivatives up to order n, can you conclude continuity of some or all  derivatives of lower order? Especially, if a function has partial derivatives of any order is it automatically smooth?",,['multivariable-calculus']
95,Changing coordinates of vector fields,Changing coordinates of vector fields,,"For tangent vectors at a single point, the transformation law is given by $\tilde{X}^j = \frac{\partial \tilde{x}^j}{\partial x^i} (p) X^i$, where $\tilde{X}^j$ are the components in the new coordinates, $X^i$ components of the old coordinates, $\frac{\partial \tilde{x}^j}{\partial x^i}$ is the partial derivative of the transition function, and $p$ is the coordinate representation of the point in the old coordinates. So I figure that to get a vector field in new coordinates, what I do is take a point $\tilde{p}$ in my new coordinates, transform it back using the transition map to get $p$ in old coordinates, and apply the transformation law above. I try to do a concrete example, and find that there is a mistake somewhere. Suppose I have the vector field $x \partial_x + y \partial_y$ in standard coordinates. I want to write this in polar coordinates as $f(\theta, r) \partial_r + g(\theta, r) \partial_\theta$. for some functions $f, g$. Applying the transformation law at some fixed point $(x, y)$, I get that the component of $\partial_r$ is $2x \cdot \frac{1}{2}(x^2 + y^2)^{-1/2} \cdot x + 2y \cdot \frac{1}{2}(x^2 + y^2)^{-1/2} \cdot y = \sqrt{x^2 + y^2}$ and for $\partial_\theta$, I have $-yx^{-2} \frac{1}{1+(y/x)^2} \cdot x + x^{-1} \frac{1}{1 + (y/x)^2} \cdot y = 0$ Thus, at any point $(x, y)$, the tangent vector $x \partial_x + y \partial_y$ is given by $\sqrt{x^2 + y^2} \partial_r + 0 \cdot \partial_\theta$. So for any $(r, \theta)$ in polar coordinates, I get $(r \cos \theta, r \sin \theta)$ in standard coordinates, which gives me $r \partial_r$ as my vector field. But this is clearly wrong. For $x \partial_x + y \partial_y$ is the vector field which attaches the vector $(x, y)$ at the point $(x, y)$, while $r \partial_r$ attaches a horizontal vector of length $r$ at each point. Where have I made my mistake? EDIT: I guess the computation is correct. I still feel it is a bit lengthy, is there any quick way to compute a change of coordinates from standard to polar? (i.e. substitute $x$ to $r \cos \theta$ in certain places, etc.)","For tangent vectors at a single point, the transformation law is given by $\tilde{X}^j = \frac{\partial \tilde{x}^j}{\partial x^i} (p) X^i$, where $\tilde{X}^j$ are the components in the new coordinates, $X^i$ components of the old coordinates, $\frac{\partial \tilde{x}^j}{\partial x^i}$ is the partial derivative of the transition function, and $p$ is the coordinate representation of the point in the old coordinates. So I figure that to get a vector field in new coordinates, what I do is take a point $\tilde{p}$ in my new coordinates, transform it back using the transition map to get $p$ in old coordinates, and apply the transformation law above. I try to do a concrete example, and find that there is a mistake somewhere. Suppose I have the vector field $x \partial_x + y \partial_y$ in standard coordinates. I want to write this in polar coordinates as $f(\theta, r) \partial_r + g(\theta, r) \partial_\theta$. for some functions $f, g$. Applying the transformation law at some fixed point $(x, y)$, I get that the component of $\partial_r$ is $2x \cdot \frac{1}{2}(x^2 + y^2)^{-1/2} \cdot x + 2y \cdot \frac{1}{2}(x^2 + y^2)^{-1/2} \cdot y = \sqrt{x^2 + y^2}$ and for $\partial_\theta$, I have $-yx^{-2} \frac{1}{1+(y/x)^2} \cdot x + x^{-1} \frac{1}{1 + (y/x)^2} \cdot y = 0$ Thus, at any point $(x, y)$, the tangent vector $x \partial_x + y \partial_y$ is given by $\sqrt{x^2 + y^2} \partial_r + 0 \cdot \partial_\theta$. So for any $(r, \theta)$ in polar coordinates, I get $(r \cos \theta, r \sin \theta)$ in standard coordinates, which gives me $r \partial_r$ as my vector field. But this is clearly wrong. For $x \partial_x + y \partial_y$ is the vector field which attaches the vector $(x, y)$ at the point $(x, y)$, while $r \partial_r$ attaches a horizontal vector of length $r$ at each point. Where have I made my mistake? EDIT: I guess the computation is correct. I still feel it is a bit lengthy, is there any quick way to compute a change of coordinates from standard to polar? (i.e. substitute $x$ to $r \cos \theta$ in certain places, etc.)",,['multivariable-calculus']
96,Proof of Green's third identity,Proof of Green's third identity,,"I don't completely understand the proof of the following result, which is also called Green's third identity: Let $D\subset{\mathbb R^m}$ be a bounded domain of class $C^1$ and let $u\in C^2(\bar{D})$ be harmonic in $D$. Then   $$u(x)=\int_{\partial D}\bigg\{\frac{\partial u}{\partial\nu}(y)\Phi(x,y)-u(y)\frac{\partial \Phi(x,y)}{\partial\nu}\bigg\}ds(y),\quad x\in D$$   where   $$\Phi(x,y):= \begin{cases} \dfrac{1}{2\pi}\ln\dfrac{1}{|x-y|},\quad m=2\\ \dfrac{1}{4\pi}\dfrac{1}{|x-y|},\quad m=3. \end{cases}$$ Proof . For $x\in D$ choose a sphere $\Omega(x;r):=\{y\in{\mathbb R}^m:|y-x|=r\}$ of radius $r$ such that $\Omega(x;r)\subset D$ and direct the unit normal $\nu$ to $\Omega(x;r)$ into the interior of $\Omega(x;r)$. Apply Green's second identity to the harmonic functions $u$ and $\Phi(x,\cdot)$ in the domain $\{y\in D:|y-x|>r\}$ to obtain $$ \int_{\partial D\cup\Omega(x;r)}\bigg\{u(y)\frac{\partial\Phi(x,y)}{\partial\nu(y)}-\frac{\partial u}{\partial\nu}(y)\Phi(x,y)\bigg\}ds(y)=0. $$ Now we have $$ \int_{\Omega(x;r)}\bigg\{u(y)\frac{\partial\Phi(x,y)}{\partial\nu(y)}-\frac{\partial u}{\partial\nu}(y)\Phi(x,y)\bigg\}ds(y)=\int_{\partial D}\bigg\{\frac{\partial u}{\partial\nu}(y)\Phi(x,y)-u(y)\frac{\partial\Phi(x,y)}{\partial\nu(y)}\bigg\}ds(y) $$ Here is my question : How can I show that  $$ \lim_{r\to 0}\int_{\Omega(x;r)}\bigg\{u(y)\frac{\partial\Phi(x,y)}{\partial\nu(y)}-\frac{\partial u}{\partial\nu}(y)\Phi(x,y)\bigg\}ds(y)=u(x)? $$","I don't completely understand the proof of the following result, which is also called Green's third identity: Let $D\subset{\mathbb R^m}$ be a bounded domain of class $C^1$ and let $u\in C^2(\bar{D})$ be harmonic in $D$. Then   $$u(x)=\int_{\partial D}\bigg\{\frac{\partial u}{\partial\nu}(y)\Phi(x,y)-u(y)\frac{\partial \Phi(x,y)}{\partial\nu}\bigg\}ds(y),\quad x\in D$$   where   $$\Phi(x,y):= \begin{cases} \dfrac{1}{2\pi}\ln\dfrac{1}{|x-y|},\quad m=2\\ \dfrac{1}{4\pi}\dfrac{1}{|x-y|},\quad m=3. \end{cases}$$ Proof . For $x\in D$ choose a sphere $\Omega(x;r):=\{y\in{\mathbb R}^m:|y-x|=r\}$ of radius $r$ such that $\Omega(x;r)\subset D$ and direct the unit normal $\nu$ to $\Omega(x;r)$ into the interior of $\Omega(x;r)$. Apply Green's second identity to the harmonic functions $u$ and $\Phi(x,\cdot)$ in the domain $\{y\in D:|y-x|>r\}$ to obtain $$ \int_{\partial D\cup\Omega(x;r)}\bigg\{u(y)\frac{\partial\Phi(x,y)}{\partial\nu(y)}-\frac{\partial u}{\partial\nu}(y)\Phi(x,y)\bigg\}ds(y)=0. $$ Now we have $$ \int_{\Omega(x;r)}\bigg\{u(y)\frac{\partial\Phi(x,y)}{\partial\nu(y)}-\frac{\partial u}{\partial\nu}(y)\Phi(x,y)\bigg\}ds(y)=\int_{\partial D}\bigg\{\frac{\partial u}{\partial\nu}(y)\Phi(x,y)-u(y)\frac{\partial\Phi(x,y)}{\partial\nu(y)}\bigg\}ds(y) $$ Here is my question : How can I show that  $$ \lim_{r\to 0}\int_{\Omega(x;r)}\bigg\{u(y)\frac{\partial\Phi(x,y)}{\partial\nu(y)}-\frac{\partial u}{\partial\nu}(y)\Phi(x,y)\bigg\}ds(y)=u(x)? $$",,['multivariable-calculus']
97,Meaning of a partial derivative here?,Meaning of a partial derivative here?,,"I am given a 'tariff' function for two countries, $i=1, 2$. Both players can select a tariff between 0 and 100. If player $i$ selects $x_i$ and player $j$ selects $x_j$, country $i$ gets a payoff of $$2000+60x_i+x_ix_j-{x_i}^{2}-90x_j.$$ I need to find a Nash equilibrium for this game. Taking a derivative with respect to player 1, I get $$\pi_1'=60+x_2-2x_1,$$ so the payoff is maximized when $$x_1=\frac{60+x_2}{2}.$$ Symmetrically, for $x_2$, $$x_2=\frac{60+x_1}{2}.$$ Substituting one equation into the other, we get $x=60$ for both players. This makes sense, as both countries' lowering the tariff, to, say, 30 would increase their profits (which is something mentioned by the textbook -- equilibrium not necessarily optimal). My question is, what exactly is the meaning of the profit function for player 1 differentiated with respect to $x_2$? I get $x_1=90$ if I set the derivative equal to $0$, and I am wondering if this has any significance within the game theoretic model.","I am given a 'tariff' function for two countries, $i=1, 2$. Both players can select a tariff between 0 and 100. If player $i$ selects $x_i$ and player $j$ selects $x_j$, country $i$ gets a payoff of $$2000+60x_i+x_ix_j-{x_i}^{2}-90x_j.$$ I need to find a Nash equilibrium for this game. Taking a derivative with respect to player 1, I get $$\pi_1'=60+x_2-2x_1,$$ so the payoff is maximized when $$x_1=\frac{60+x_2}{2}.$$ Symmetrically, for $x_2$, $$x_2=\frac{60+x_1}{2}.$$ Substituting one equation into the other, we get $x=60$ for both players. This makes sense, as both countries' lowering the tariff, to, say, 30 would increase their profits (which is something mentioned by the textbook -- equilibrium not necessarily optimal). My question is, what exactly is the meaning of the profit function for player 1 differentiated with respect to $x_2$? I get $x_1=90$ if I set the derivative equal to $0$, and I am wondering if this has any significance within the game theoretic model.",,"['multivariable-calculus', 'game-theory', 'nash-equilibrium']"
98,"how to evaluate $\int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy$",how to evaluate,"\int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy","How to evaluate: \begin{align*} &\int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 4\log_{\pi}{\sqrt{x}} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy \end{align*} My attempt(almost a complete solution) $$ I := \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 4\log_{\pi}{\sqrt{x}} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy $$ $$ = \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 2\log_{\pi}{x} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy $$ $$ = \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy $$ $$ = \ln^{2}{\pi} \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy $$ Change of variables: $$ \begin{cases} u = xy \\ v = y \end{cases} \quad , \quad \frac{\partial (u, v)}{\partial (x, y)} = \begin{vmatrix} y & x \\ 0 & 1  \end{vmatrix} = y = v $$ Thus: $$ \begin{cases} x = \frac{u}{v} \\ y = v \end{cases} \quad \text{and} \quad \left| \frac{\partial (x, y)}{\partial (u, v)} \right| = \frac{1}{v} $$ So: \begin{align*} I &= \ln^{2}{\pi} \iint_{D} \frac{2 - \frac{u}{v} - v}{\sqrt{u}(1+u) \ln^{2}{u}} \cdot \frac{1}{v} \, du \, dv \end{align*} Where: $D = \left\{ (u, v) \, | \, 0 \leq u \leq v \leq 1 \right\}$ Therefore: $$ I = \ln^{2}{\pi} \int_{0}^{1} \frac{1}{\sqrt{u}(1+u) \ln^{2}{u}} \left[ \int_{u}^{1} \left( \frac{2}{v} - \frac{u}{v^{2}} - 1 \right) \, dv \right] \, du $$ $$ = \ln^{2}{\pi} \int_{0}^{1} \frac{1}{\sqrt{u}(1+u) \ln^{2}{u}} \left[ \left( 2 \ln{v} + \frac{u}{v} - v \right) \bigg|_{u}^{1} \right] \, du $$ $$ = 2 \ln^{2}{\pi} \int_{0}^{1} \frac{u-1-\ln{u}}{\sqrt{u}(1+u) \ln^{2}{u}} \, du \quad \left( \text{let } u = e^{-t} \right) $$ $$ = 2 \ln^{2}{\pi} \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{e^{-t} - 1 + t}{t^{2}} \, dt $$ Let: $$ J(\lambda) := \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{e^{-\lambda t} - 1 + \lambda t}{t^{2}} \, dt \quad (\lambda \geq 0) $$ Then: \begin{align*} J'(\lambda) &= \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{1 - e^{-\lambda t}}{t} \, dt \end{align*} And: $$ J''(\lambda) = \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot e^{-\lambda t} \, dt $$ $$ = \int_{0}^{+\infty} \sum_{n=0}^{+\infty} (-1)^{n} \cdot e^{-(\lambda + n + \frac{1}{2}) t} \, dt $$ $$ = \sum_{n=0}^{+\infty} (-1)^{n} \int_{0}^{+\infty} e^{-(\lambda + n + \frac{1}{2}) t} \, dt $$ $$ = \sum_{n=0}^{+\infty} \frac{(-1)^{n}}{\lambda + n + \frac{1}{2}} $$ Given: $$ J'(0) = 0 \Rightarrow J'(\lambda) = \sum_{n=0}^{+\infty} (-1)^{n} \cdot \ln{\left(1 + \frac{2\lambda}{2n+1}\right)} $$ And: $$ J(0) = 0 \Rightarrow J(\lambda) = \sum_{n=0}^{+\infty} (-1)^{n} \cdot \left[ \left( \lambda + \frac{2n+1}{2} \right) \ln{\left(1 + \frac{2\lambda}{2n+1}\right)} - \lambda \right] $$ So: \begin{align*} I &= 2 \ln^{2}{\pi} \cdot J(1) \\ &= 2 \ln^{2}{\pi} \cdot \sum_{n=0}^{+\infty} (-1)^{n} \cdot \left[ \frac{2n+3}{2} \ln{\left(1 + \frac{2}{2n+1}\right)} - 1 \right] \end{align*} I don't know how to evaluate the last sum, and can someone check if my proof is correct?","How to evaluate: My attempt(almost a complete solution) Change of variables: Thus: So: Where: Therefore: Let: Then: And: Given: And: So: I don't know how to evaluate the last sum, and can someone check if my proof is correct?","\begin{align*}
&\int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 4\log_{\pi}{\sqrt{x}} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy
\end{align*} 
I := \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 4\log_{\pi}{\sqrt{x}} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy
 
= \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 2\log_{\pi}{x} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy
 
= \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy
 
= \ln^{2}{\pi} \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy
 
\begin{cases}
u = xy \\
v = y
\end{cases}
\quad , \quad \frac{\partial (u, v)}{\partial (x, y)} = \begin{vmatrix}
y & x \\
0 & 1 
\end{vmatrix} = y = v
 
\begin{cases}
x = \frac{u}{v} \\
y = v
\end{cases}
\quad \text{and} \quad \left| \frac{\partial (x, y)}{\partial (u, v)} \right| = \frac{1}{v}
 \begin{align*}
I &= \ln^{2}{\pi} \iint_{D} \frac{2 - \frac{u}{v} - v}{\sqrt{u}(1+u) \ln^{2}{u}} \cdot \frac{1}{v} \, du \, dv
\end{align*} D = \left\{ (u, v) \, | \, 0 \leq u \leq v \leq 1 \right\} 
I = \ln^{2}{\pi} \int_{0}^{1} \frac{1}{\sqrt{u}(1+u) \ln^{2}{u}} \left[ \int_{u}^{1} \left( \frac{2}{v} - \frac{u}{v^{2}} - 1 \right) \, dv \right] \, du
 
= \ln^{2}{\pi} \int_{0}^{1} \frac{1}{\sqrt{u}(1+u) \ln^{2}{u}} \left[ \left( 2 \ln{v} + \frac{u}{v} - v \right) \bigg|_{u}^{1} \right] \, du
 
= 2 \ln^{2}{\pi} \int_{0}^{1} \frac{u-1-\ln{u}}{\sqrt{u}(1+u) \ln^{2}{u}} \, du \quad \left( \text{let } u = e^{-t} \right)
 
= 2 \ln^{2}{\pi} \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{e^{-t} - 1 + t}{t^{2}} \, dt
 
J(\lambda) := \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{e^{-\lambda t} - 1 + \lambda t}{t^{2}} \, dt \quad (\lambda \geq 0)
 \begin{align*}
J'(\lambda) &= \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{1 - e^{-\lambda t}}{t} \, dt
\end{align*} 
J''(\lambda) = \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot e^{-\lambda t} \, dt
 
= \int_{0}^{+\infty} \sum_{n=0}^{+\infty} (-1)^{n} \cdot e^{-(\lambda + n + \frac{1}{2}) t} \, dt
 
= \sum_{n=0}^{+\infty} (-1)^{n} \int_{0}^{+\infty} e^{-(\lambda + n + \frac{1}{2}) t} \, dt
 
= \sum_{n=0}^{+\infty} \frac{(-1)^{n}}{\lambda + n + \frac{1}{2}}
 
J'(0) = 0 \Rightarrow J'(\lambda) = \sum_{n=0}^{+\infty} (-1)^{n} \cdot \ln{\left(1 + \frac{2\lambda}{2n+1}\right)}
 
J(0) = 0 \Rightarrow J(\lambda) = \sum_{n=0}^{+\infty} (-1)^{n} \cdot \left[ \left( \lambda + \frac{2n+1}{2} \right) \ln{\left(1 + \frac{2\lambda}{2n+1}\right)} - \lambda \right]
 \begin{align*}
I &= 2 \ln^{2}{\pi} \cdot J(1) \\
&= 2 \ln^{2}{\pi} \cdot \sum_{n=0}^{+\infty} (-1)^{n} \cdot \left[ \frac{2n+3}{2} \ln{\left(1 + \frac{2}{2n+1}\right)} - 1 \right]
\end{align*}","['integration', 'sequences-and-series', 'multivariable-calculus', 'definite-integrals', 'closed-form']"
99,Equivalent condition for the following monotone-like condition for multivariate function,Equivalent condition for the following monotone-like condition for multivariate function,,"Let $f:\mathbb R^n\times \mathbb R^n\rightarrow \mathbb R$ be a continuously twice differentiable function. I am considering the following condition (1) on $f$ : For any $x,x'\in\mathbb R^n$ , If it is either $f(x,x')>f(x,x)$ or $f(x',x)>f(x,x)$ , we should have $f(x',x')\geq\max\{f(x,x'),f(x',x)\}$ . Equivalently, $$\tag{1}\max\{f(x,x'),f(x',x)\}>f(x,x)\Rightarrow f(x',x')\geq\max\{f(x,x'),f(x',x)\},~\forall x,x'\in\mathbb R^n.$$ My question is : Is there an equivalent condition to (1)? One possibility is that the monotonicity of partial derivatives is related to some of the inequalities in (1). More specifically, let the following inequalities be true for all $x,x'\in\mathbb R^n$ such that $x\neq x'$ : $$(x'-x)(D_xf(x,x')-D_xf(x,x))>0~\textrm{and}$$ $$(x'-x)(D_yf(x',x)-D_yf(x,x))>0~\textrm.$$ According to my previous question ( link ) this monotonicity implies the following: $$f(x,x')>f(x,x)\Rightarrow f(x',x')>f(x',x).$$ We have the relationship between $f(x',x')-f(x',x)$ and $f(x,x')-f(x,x)$ . But what is missing to answer (1) is the relationship between $f(x',x')-f(x,x')$ and $f(x,x')-f(x,x)$ , that I cannot find from my previous question. Any condition that reveals the relationship between $f(x',x')-f(x,x')$ and $f(x,x')-f(x,x)$ ? Any comment and answer will be highly appreciated.","Let be a continuously twice differentiable function. I am considering the following condition (1) on : For any , If it is either or , we should have . Equivalently, My question is : Is there an equivalent condition to (1)? One possibility is that the monotonicity of partial derivatives is related to some of the inequalities in (1). More specifically, let the following inequalities be true for all such that : According to my previous question ( link ) this monotonicity implies the following: We have the relationship between and . But what is missing to answer (1) is the relationship between and , that I cannot find from my previous question. Any condition that reveals the relationship between and ? Any comment and answer will be highly appreciated.","f:\mathbb R^n\times \mathbb R^n\rightarrow \mathbb R f x,x'\in\mathbb R^n f(x,x')>f(x,x) f(x',x)>f(x,x) f(x',x')\geq\max\{f(x,x'),f(x',x)\} \tag{1}\max\{f(x,x'),f(x',x)\}>f(x,x)\Rightarrow f(x',x')\geq\max\{f(x,x'),f(x',x)\},~\forall x,x'\in\mathbb R^n. x,x'\in\mathbb R^n x\neq x' (x'-x)(D_xf(x,x')-D_xf(x,x))>0~\textrm{and} (x'-x)(D_yf(x',x)-D_yf(x,x))>0~\textrm. f(x,x')>f(x,x)\Rightarrow f(x',x')>f(x',x). f(x',x')-f(x',x) f(x,x')-f(x,x) f(x',x')-f(x,x') f(x,x')-f(x,x) f(x',x')-f(x,x') f(x,x')-f(x,x)","['multivariable-calculus', 'functions', 'monotone-functions']"
