,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"An exercise from my brother: $\int_{-1}^1\frac{\ln (2x-1)}{\sqrt[\large 6]{x(1-x)(1-2x)^4}}\,dx$",An exercise from my brother:,"\int_{-1}^1\frac{\ln (2x-1)}{\sqrt[\large 6]{x(1-x)(1-2x)^4}}\,dx","My brother asked me to calculate the following integral before we had dinner and I have been working to calculate it since then ($\pm\, 4$ hours). He said, it has a beautiful closed form but I doubt it and I guess he has tried to trick me again (as usual). So I am curious, what is the closed form ( if any ) of the following integral: \begin{equation} \int_{-1}^1\frac{\ln (2x-1)}{\sqrt[\large 6]{x(1-x)(1-2x)^4}}\,dx \end{equation} I have tried by parts method, partial fractions (stupid idea), converting into series (nothing familiar), many substitutions such as: $u=2x-1$, $u=1-x$, $x=\cos^2\theta$, etc, but I failed and got nothing. Wolfram Alpha also doesn't give an answer. Either he is lying to me or telling the truth, I don't know. Could anyone here please help me to obtain the closed form of the integral with any methods ( whatever it takes )? Any help would be greatly appreciated. Thank you.","My brother asked me to calculate the following integral before we had dinner and I have been working to calculate it since then ($\pm\, 4$ hours). He said, it has a beautiful closed form but I doubt it and I guess he has tried to trick me again (as usual). So I am curious, what is the closed form ( if any ) of the following integral: \begin{equation} \int_{-1}^1\frac{\ln (2x-1)}{\sqrt[\large 6]{x(1-x)(1-2x)^4}}\,dx \end{equation} I have tried by parts method, partial fractions (stupid idea), converting into series (nothing familiar), many substitutions such as: $u=2x-1$, $u=1-x$, $x=\cos^2\theta$, etc, but I failed and got nothing. Wolfram Alpha also doesn't give an answer. Either he is lying to me or telling the truth, I don't know. Could anyone here please help me to obtain the closed form of the integral with any methods ( whatever it takes )? Any help would be greatly appreciated. Thank you.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
1,Every increasing function from a certain set to itself has at least one fixed point,Every increasing function from a certain set to itself has at least one fixed point,,"I need a hint for the following question: Let $S$ be a nonempty ordered set such that every nonempty subset $E\subseteq S$  has both a least upper bound and a greatest lower bound. Suppose $f:S \rightarrow S$ is a monotonically increasing function. Show that there exists an $x\in S$ so that $f(x)=x$. My reasoning so far has been as follows: I wanted to prove this by contradiction. If we assume the statement is false, this implies that for each $x\in S$, $f(x) > x$ or $f(x) < x$. Let $A:=\{x:f(x)<x\}$, $B:=\{x:f(x)>x\}$. Let's assume $A$ is empty. Then it is easy to show a contradiction. We know that $\sup(B)\in B$. But that means that $f(\sup(B))>\sup(B)$ which is impossible. The same idea applies if we assume $B$ is empty. What I am trying to do is prove the case where both $A$ and $B$ are nonempty. To prove the first part I haven't even used the monoticity of the function $f$, so I know that I have to use it at this point. That means trying to find a contradiction by finding $x,y\in S$, $x\le y$, $f(x)>f(y)$. I was trying to look at the supremum and infimum of $A$ and $B$ but that didn't seem to lead me anywhere. I would appreciate hints!","I need a hint for the following question: Let $S$ be a nonempty ordered set such that every nonempty subset $E\subseteq S$  has both a least upper bound and a greatest lower bound. Suppose $f:S \rightarrow S$ is a monotonically increasing function. Show that there exists an $x\in S$ so that $f(x)=x$. My reasoning so far has been as follows: I wanted to prove this by contradiction. If we assume the statement is false, this implies that for each $x\in S$, $f(x) > x$ or $f(x) < x$. Let $A:=\{x:f(x)<x\}$, $B:=\{x:f(x)>x\}$. Let's assume $A$ is empty. Then it is easy to show a contradiction. We know that $\sup(B)\in B$. But that means that $f(\sup(B))>\sup(B)$ which is impossible. The same idea applies if we assume $B$ is empty. What I am trying to do is prove the case where both $A$ and $B$ are nonempty. To prove the first part I haven't even used the monoticity of the function $f$, so I know that I have to use it at this point. That means trying to find a contradiction by finding $x,y\in S$, $x\le y$, $f(x)>f(y)$. I was trying to look at the supremum and infimum of $A$ and $B$ but that didn't seem to lead me anywhere. I would appreciate hints!",,"['real-analysis', 'order-theory', 'fixed-point-theorems']"
2,A doubt in the rigorous definition of limits. [duplicate],A doubt in the rigorous definition of limits. [duplicate],,"This question already has answers here : Why does the definition of limits of a function have strict inequality? (3 answers) Closed 3 years ago . I studied the definition of limits today, and I think I mostly understood it, but I have a little doubt. In the definition: $f(x)$ is defined on some open interval containing $a$, except at possibly $a$. So, $\lim_{x\to a} f(x) = L $ if and only if for every number $\varepsilon>0$, there exists a corresponding number $\delta>0$ such that:  $$\text{If   } 0<|x-a|<\delta \text{ then } |f(x)-L|<\varepsilon$$ First of all, if you see any mistake in my definition or understanding, please tell. So my question is, why can't we write $0<|x-a|\leq\delta$ and $|f(x)-L|\leq\varepsilon$ in the last line? What kinds of problems can arise from this? If the definitions are equivalent, can you explain/prove me why?","This question already has answers here : Why does the definition of limits of a function have strict inequality? (3 answers) Closed 3 years ago . I studied the definition of limits today, and I think I mostly understood it, but I have a little doubt. In the definition: $f(x)$ is defined on some open interval containing $a$, except at possibly $a$. So, $\lim_{x\to a} f(x) = L $ if and only if for every number $\varepsilon>0$, there exists a corresponding number $\delta>0$ such that:  $$\text{If   } 0<|x-a|<\delta \text{ then } |f(x)-L|<\varepsilon$$ First of all, if you see any mistake in my definition or understanding, please tell. So my question is, why can't we write $0<|x-a|\leq\delta$ and $|f(x)-L|\leq\varepsilon$ in the last line? What kinds of problems can arise from this? If the definitions are equivalent, can you explain/prove me why?",,"['calculus', 'real-analysis', 'limits', 'definition']"
3,"Show that $d(x,y)=\min \{1,|x-y|\}$ is a metric on $\mathbb R$",Show that  is a metric on,"d(x,y)=\min \{1,|x-y|\} \mathbb R","Is the following just a matter of showing the 3 properties that make up a metric?? Define d on $\Bbb R\times\Bbb R$ by $d(x,y)=\min \{1,|x-y|\}$. Show that $d$ is a metric on $\Bbb R$ $d(x,y)=0$ if $x=y$ $d(x,y)=d(y,x)$ for every $x,y \in X$ $d(x,y)\le d(x,z)+d(z,y)$ for every $x,y,z \in X$","Is the following just a matter of showing the 3 properties that make up a metric?? Define d on $\Bbb R\times\Bbb R$ by $d(x,y)=\min \{1,|x-y|\}$. Show that $d$ is a metric on $\Bbb R$ $d(x,y)=0$ if $x=y$ $d(x,y)=d(y,x)$ for every $x,y \in X$ $d(x,y)\le d(x,z)+d(z,y)$ for every $x,y,z \in X$",,"['real-analysis', 'metric-spaces']"
4,Absolutely convergent series has constant sum,Absolutely convergent series has constant sum,,"Show that the sum of an absolutely convergent series does not change if the terms are rearranged. Let the absolutely convergent series be $a_1,a_2,\ldots$, and let the rearrangement be $b_1,b_2,\ldots$. By the Cauchy criterion of convergence, since the series $|a_1|,|a_2|,\ldots$ converges, it follows that for any $\epsilon$, there exists $N$ such that $|a_m|+|a_{m+1}|+\ldots+|a_n|<\epsilon$ for all $n>m>N$. We'll show that the series $b_i$ converges. Let $N'$ be the greatest index such that $b_{N'}$ comes from one of $a_1,a_2,\ldots,a_N$. Then for all $n>m>N'$, we have $|b_m+b_{m+1}+\ldots+b_n|\leq |b_m|+|b_{m+1}|+\ldots+|b_n|<\epsilon$. So the series $b_i$ converges. How can we show it converges to the same sum as $a_i$?","Show that the sum of an absolutely convergent series does not change if the terms are rearranged. Let the absolutely convergent series be $a_1,a_2,\ldots$, and let the rearrangement be $b_1,b_2,\ldots$. By the Cauchy criterion of convergence, since the series $|a_1|,|a_2|,\ldots$ converges, it follows that for any $\epsilon$, there exists $N$ such that $|a_m|+|a_{m+1}|+\ldots+|a_n|<\epsilon$ for all $n>m>N$. We'll show that the series $b_i$ converges. Let $N'$ be the greatest index such that $b_{N'}$ comes from one of $a_1,a_2,\ldots,a_N$. Then for all $n>m>N'$, we have $|b_m+b_{m+1}+\ldots+b_n|\leq |b_m|+|b_{m+1}|+\ldots+|b_n|<\epsilon$. So the series $b_i$ converges. How can we show it converges to the same sum as $a_i$?",,['real-analysis']
5,"If $f:\mathbb{R}^n \to \mathbb{R}^n$ is continuous with convex image, and locally 1-1, must it be globally 1-1?","If  is continuous with convex image, and locally 1-1, must it be globally 1-1?",f:\mathbb{R}^n \to \mathbb{R}^n,"For $f:\mathbb{R}\to \mathbb{R}$ which is continuous, being locally 1-1 implies being globally 1-1, see here .  This is not true for a general mapping $f:\mathbb{R}^n\to \mathbb{R}^n$.  My intuition as to the source of this incongruity is that while continuity preserves connectedness, it does not preserve convexity. This illustration (from Buck's Advanced Calculus ) of a locally injective, not globally injective function from $\mathbb{R}^2$ to $\mathbb{R}^2$ somewhat supports my intuition...note the image is not convex. If it were convex, it would seem there would need to be some ""critical point"" (not assuming the existence of the derivative) where $f$ was not locally 1-1. Is there a theorem that if $f: S \to \mathbb{R}^n$ is continuous, for $S\subset \mathbb{R}^n$ convex, with convex image, locally 1-1, then it must be 1-1 on all of $S$?","For $f:\mathbb{R}\to \mathbb{R}$ which is continuous, being locally 1-1 implies being globally 1-1, see here .  This is not true for a general mapping $f:\mathbb{R}^n\to \mathbb{R}^n$.  My intuition as to the source of this incongruity is that while continuity preserves connectedness, it does not preserve convexity. This illustration (from Buck's Advanced Calculus ) of a locally injective, not globally injective function from $\mathbb{R}^2$ to $\mathbb{R}^2$ somewhat supports my intuition...note the image is not convex. If it were convex, it would seem there would need to be some ""critical point"" (not assuming the existence of the derivative) where $f$ was not locally 1-1. Is there a theorem that if $f: S \to \mathbb{R}^n$ is continuous, for $S\subset \mathbb{R}^n$ convex, with convex image, locally 1-1, then it must be 1-1 on all of $S$?",,"['calculus', 'real-analysis', 'general-topology', 'continuity']"
6,Proving a function is continuous on all irrational numbers,Proving a function is continuous on all irrational numbers,,"Let $\langle r_n\rangle$ be an enumeration of the set $\mathbb Q$ of rational numbers such that $r_n \neq r_m\,$ if $\,n\neq m.$ $$\text{Define}\; f: \mathbb R \to \mathbb R\;\text{by}\;\displaystyle f(x) = \sum_{r_n \leq x} 1/2^n,\;x\in \mathbb R.$$   Prove that $f$ is continuous at each point of $\mathbb Q^c$ and discontinuous at each point of $\mathbb Q$. I find this question very challenging and have no idea even how to start off with the proof. Please suggest a proof or any hint.","Let $\langle r_n\rangle$ be an enumeration of the set $\mathbb Q$ of rational numbers such that $r_n \neq r_m\,$ if $\,n\neq m.$ $$\text{Define}\; f: \mathbb R \to \mathbb R\;\text{by}\;\displaystyle f(x) = \sum_{r_n \leq x} 1/2^n,\;x\in \mathbb R.$$   Prove that $f$ is continuous at each point of $\mathbb Q^c$ and discontinuous at each point of $\mathbb Q$. I find this question very challenging and have no idea even how to start off with the proof. Please suggest a proof or any hint.",,"['real-analysis', 'sequences-and-series', 'analysis']"
7,Continuous function proof by definition,Continuous function proof by definition,,"Prove that if $f$ is defined for $x\ge 0$ by $f(x)=\sqrt x$, then $f$   is continuous at every point of its domain. Definition of a continuous function is: Let $A\subseteq\mathbb{R}$ and let $f:A\to\mathbb{R}$. Denote $c\in A$. Then $f(x)$ is continuous at $c$ iff for every $\varepsilon>0$, $\exists$ $\delta>0$ such that $|x-c|<\delta\implies |f(x)-f(c)|<\varepsilon.$ My attempt: We know that the function $f: x\to \mathbb{R}$, where $x\in [0,\infty)$ is defined to be $f(x)=\sqrt x$. So, for $0\le x<\infty$, then  $|f(x)-f(c)|=|\sqrt x - f(c)|$ and I can't continue since I don't necessarily know what $c$ is in this case.","Prove that if $f$ is defined for $x\ge 0$ by $f(x)=\sqrt x$, then $f$   is continuous at every point of its domain. Definition of a continuous function is: Let $A\subseteq\mathbb{R}$ and let $f:A\to\mathbb{R}$. Denote $c\in A$. Then $f(x)$ is continuous at $c$ iff for every $\varepsilon>0$, $\exists$ $\delta>0$ such that $|x-c|<\delta\implies |f(x)-f(c)|<\varepsilon.$ My attempt: We know that the function $f: x\to \mathbb{R}$, where $x\in [0,\infty)$ is defined to be $f(x)=\sqrt x$. So, for $0\le x<\infty$, then  $|f(x)-f(c)|=|\sqrt x - f(c)|$ and I can't continue since I don't necessarily know what $c$ is in this case.",,"['real-analysis', 'functions', 'continuity']"
8,Asymptotic expansion of a series,Asymptotic expansion of a series,,"I am interested in the asymptotics, as $x$ tends to $0$, of $$f(x) = \sum_{n=1}^\infty \frac{1}{n}\frac{1}{(e^{nx}-1)^2}$$ This function is well defined for every $x > 0$ (for example, use $e^{nx}-1 \geq nx$). Furthermore, Lebesgue's dominated convergence theorem shows that,  $$ f(x) \sim \frac{\zeta(3)}{x^2} $$ as $x$ tends to $0$, where  $\displaystyle\zeta(3) = \sum_{n=1}^\infty \frac{1}{n^3}$ is Apéry's constant . Could you help me get a more precise asymptotic expansion of $f(x)$ as $x$ tends to $0$ ? (best would be with $o(1)$)","I am interested in the asymptotics, as $x$ tends to $0$, of $$f(x) = \sum_{n=1}^\infty \frac{1}{n}\frac{1}{(e^{nx}-1)^2}$$ This function is well defined for every $x > 0$ (for example, use $e^{nx}-1 \geq nx$). Furthermore, Lebesgue's dominated convergence theorem shows that,  $$ f(x) \sim \frac{\zeta(3)}{x^2} $$ as $x$ tends to $0$, where  $\displaystyle\zeta(3) = \sum_{n=1}^\infty \frac{1}{n^3}$ is Apéry's constant . Could you help me get a more precise asymptotic expansion of $f(x)$ as $x$ tends to $0$ ? (best would be with $o(1)$)",,"['real-analysis', 'sequences-and-series', 'asymptotics']"
9,Stuck on existence proofs involving measurability and simple functions,Stuck on existence proofs involving measurability and simple functions,,"Some classmates and I have been working through a sequence of problems in Royden's real analysis text, which are in the chapter on Lebesgue measurable functions revolving around the Sequential Pointwise Limits and Simple Function approximations. We have some of them done, but are stuck on others. For each of the problems, we assume we have $I$ a closed/bounded interval. Let $E$ be a measurable subset of $I$ . Let $\epsilon > 0$ . Show that there is a step function $h$ on $I$ and a measurable subset $F$ of $I$ for which $h=\chi_E$ on $F$ and $m(I\setminus F)<\epsilon$ Let $\psi$ be a simple function defined on $I$ . Let $\epsilon > 0$ . Show that there is a step function $h$ on $I$ and a measurable subset $F$ of $I$ for which $h=\psi$ on $F$ and $m(I\setminus F)<\epsilon$ Let $f$ be a bounded measurable function defined on $I$ . Let $\epsilon > 0$ . Show that there is a step function $h$ on $I$ and a measurable subset $F$ of $I$ for which $|h-f|<\epsilon$ on $F$ and $m(I\setminus F)<\epsilon$ . Clearly all these problems are very similar and build upon one another. You are then showing existence of a step function on $F \subset I$ where $m(I\setminus F)<\epsilon$ . In the first you show that $\chi_E$ exists. Then you show a simple function $\psi$ exists which we know is of the form $\psi=\sum_{k=1}^n a_k \chi_{E_k}$ . Then you do it for any bounded measurable function $f$ . Ideas for 1: There exists a finite open cover of $I$ ( $O=\bigcup_{k=1}^n I_k$ ), which should also cover $E \subset I \subset O$ with the property that $m(O\setminus I)<\epsilon$ . If we can set $F=(O\setminus E)^c$ we could try to show that $m(I\setminus F) = m(O\setminus E)<\epsilon$ . Also we notice that $\chi_E = \chi_O$ on $F$ . Though we haven't quite put it all together. Ideas for 2: Let $\psi=\sum_{k=1}^n a_k \chi_{I_k}$ where again $O=\bigcup_{k=1}^n I_k$ is an open cover of $I$ . We also know there is a closed $F_i \subset I_i$ where $m(I_i \setminus F_i)<\frac \epsilon n$ which could lead to $m(I \setminus F) \le m(\bigcup_{i=1}^n I_i \setminus \bigcup_{i=1}^n F_i) = \sum_{i=1}^n m(I_i \setminus F_i) < \sum_{i=1}^n \frac \epsilon n = \epsilon$ Ideas for 3: Use simple approximation theorem which says that $f$ is measurable on $E$ iff exists a sequence of simple functions which converge p.w. on $E$ to $f$ s.t. $|\psi_n|\le|f|$ on $E$ for all $n$ . Any suggestions towards putting the ideas together or simpler solutions would be greatly appreciated!","Some classmates and I have been working through a sequence of problems in Royden's real analysis text, which are in the chapter on Lebesgue measurable functions revolving around the Sequential Pointwise Limits and Simple Function approximations. We have some of them done, but are stuck on others. For each of the problems, we assume we have a closed/bounded interval. Let be a measurable subset of . Let . Show that there is a step function on and a measurable subset of for which on and Let be a simple function defined on . Let . Show that there is a step function on and a measurable subset of for which on and Let be a bounded measurable function defined on . Let . Show that there is a step function on and a measurable subset of for which on and . Clearly all these problems are very similar and build upon one another. You are then showing existence of a step function on where . In the first you show that exists. Then you show a simple function exists which we know is of the form . Then you do it for any bounded measurable function . Ideas for 1: There exists a finite open cover of ( ), which should also cover with the property that . If we can set we could try to show that . Also we notice that on . Though we haven't quite put it all together. Ideas for 2: Let where again is an open cover of . We also know there is a closed where which could lead to Ideas for 3: Use simple approximation theorem which says that is measurable on iff exists a sequence of simple functions which converge p.w. on to s.t. on for all . Any suggestions towards putting the ideas together or simpler solutions would be greatly appreciated!",I E I \epsilon > 0 h I F I h=\chi_E F m(I\setminus F)<\epsilon \psi I \epsilon > 0 h I F I h=\psi F m(I\setminus F)<\epsilon f I \epsilon > 0 h I F I |h-f|<\epsilon F m(I\setminus F)<\epsilon F \subset I m(I\setminus F)<\epsilon \chi_E \psi \psi=\sum_{k=1}^n a_k \chi_{E_k} f I O=\bigcup_{k=1}^n I_k E \subset I \subset O m(O\setminus I)<\epsilon F=(O\setminus E)^c m(I\setminus F) = m(O\setminus E)<\epsilon \chi_E = \chi_O F \psi=\sum_{k=1}^n a_k \chi_{I_k} O=\bigcup_{k=1}^n I_k I F_i \subset I_i m(I_i \setminus F_i)<\frac \epsilon n m(I \setminus F) \le m(\bigcup_{i=1}^n I_i \setminus \bigcup_{i=1}^n F_i) = \sum_{i=1}^n m(I_i \setminus F_i) < \sum_{i=1}^n \frac \epsilon n = \epsilon f E E f |\psi_n|\le|f| E n,"['real-analysis', 'analysis', 'measure-theory']"
10,"Does $f_{n}(x)=n\cos^n x \sin x$ uniformly converge for $x \in [0,\frac{\pi}{2}]$?",Does  uniformly converge for ?,"f_{n}(x)=n\cos^n x \sin x x \in [0,\frac{\pi}{2}]","I want to check whether the following function is uniformly converges: $f_n(x)=n\cos^nx\sin x$ for $x \in \left[0,\frac{\pi}{2} \right]$. I proved that the $\lim \limits_{n \to \infty}f_{n}(x)=0$ for every $x$. I'd love your help with the uniformly continues convergence.  I always get confused with it.  I already showed that $|f_n(x) - 0|< \epsilon$. What else should I show or how should I refute the claim? Thanks a lot.","I want to check whether the following function is uniformly converges: $f_n(x)=n\cos^nx\sin x$ for $x \in \left[0,\frac{\pi}{2} \right]$. I proved that the $\lim \limits_{n \to \infty}f_{n}(x)=0$ for every $x$. I'd love your help with the uniformly continues convergence.  I always get confused with it.  I already showed that $|f_n(x) - 0|< \epsilon$. What else should I show or how should I refute the claim? Thanks a lot.",,"['calculus', 'real-analysis', 'convergence-divergence']"
11,Is there a geometric interpretation of the exponential function of real numbers?,Is there a geometric interpretation of the exponential function of real numbers?,,"I can visualize the exponential function with the graph $y = e^x$, but I can do that for almost any function. In addition to its graph, the function $f(x) = x^n$ can be visualized as the volume of a box with sides of length $x$ in n-dimensional space, and the trigonometric functions can be interpreted as side lengths of certain right triangles. Is there a similar geometric interpretation of the exponential function?","I can visualize the exponential function with the graph $y = e^x$, but I can do that for almost any function. In addition to its graph, the function $f(x) = x^n$ can be visualized as the volume of a box with sides of length $x$ in n-dimensional space, and the trigonometric functions can be interpreted as side lengths of certain right triangles. Is there a similar geometric interpretation of the exponential function?",,"['real-analysis', 'geometry', 'visualization']"
12,"How can I study the convergence of the improper integral $\int_{0}^{ \infty} \frac{\sin(x)}{x+1} \, \mathrm dx\,$?",How can I study the convergence of the improper integral ?,"\int_{0}^{ \infty} \frac{\sin(x)}{x+1} \, \mathrm dx\,","I need to study the convergence of the following improper integral: $$\int_{0}^{\infty} \dfrac{\sin(x)}{x+1}\, \mathrm dx$$ I did the following: $$ -1 \leq \sin(x)  \leq 1  \\ \implies \dfrac{-1}{x+1}  \leq \dfrac{\sin(x)}{x+1}  \leq \dfrac{1}{x+1} \\    \implies \left|\dfrac{\sin(x)}{x+1}\right|  \leq \dfrac{1}{x+1} \\  \implies \int_{0}^{\infty} \left|\dfrac{\sin(x)}{x+1}\right| \, \mathrm dx   \leq \int_{0}^{\infty}\dfrac{1}{x+1}\, \mathrm dx = \infty  $$ I planned to use the comparison criterion and then the absolute convergence criterion. However, the idea did not work for me.","I need to study the convergence of the following improper integral: I did the following: I planned to use the comparison criterion and then the absolute convergence criterion. However, the idea did not work for me.","\int_{0}^{\infty} \dfrac{\sin(x)}{x+1}\, \mathrm dx  -1 \leq \sin(x)  \leq 1  \\
\implies \dfrac{-1}{x+1}  \leq \dfrac{\sin(x)}{x+1}  \leq \dfrac{1}{x+1} \\   
\implies \left|\dfrac{\sin(x)}{x+1}\right|  \leq \dfrac{1}{x+1} \\
 \implies \int_{0}^{\infty} \left|\dfrac{\sin(x)}{x+1}\right| \, \mathrm dx   \leq \int_{0}^{\infty}\dfrac{1}{x+1}\, \mathrm dx = \infty  ","['real-analysis', 'integration', 'convergence-divergence', 'improper-integrals']"
13,Continuous extension of XOR,Continuous extension of XOR,,"The $\text{XOR}$ function is a function from $\{0,1\}^2$ to $\{0,1\}$ , defined as: $\text{XOR}(0,0)=\text{XOR}(1,1)=0$ $\text{XOR}(1,0)=\text{XOR}(0,1)=1$ I am interested in finding an extension of this function to $\mathbb{R}$ . To be more specific, I am looking for a function $f$ from $\mathbb{R}^2$ to $\mathbb{R}$ with the following properties (for all $x,y,z\in\mathbb{R}$ ): $f(x,y)=f(y,x)$ $f(x,0)=x$ $f(x,x)=0$ $f(x,f(y,z))=f(f(x,y),z)$ $f$ is continuous Does such a function exist? If yes, how do I construct it? If not, how do I prove so?","The function is a function from to , defined as: I am interested in finding an extension of this function to . To be more specific, I am looking for a function from to with the following properties (for all ): is continuous Does such a function exist? If yes, how do I construct it? If not, how do I prove so?","\text{XOR} \{0,1\}^2 \{0,1\} \text{XOR}(0,0)=\text{XOR}(1,1)=0 \text{XOR}(1,0)=\text{XOR}(0,1)=1 \mathbb{R} f \mathbb{R}^2 \mathbb{R} x,y,z\in\mathbb{R} f(x,y)=f(y,x) f(x,0)=x f(x,x)=0 f(x,f(y,z))=f(f(x,y),z) f","['real-analysis', 'functions']"
14,A Taylor theorem for Hölder continuous function?,A Taylor theorem for Hölder continuous function?,,"Let $ C^{m,s}_{b} $ be the space of bounded function $ u: \mathbb{R} \rightarrow \mathbb{R} $ which satisfies \begin{alignat*}{2} \bigg| u^{(m)}(x) - u^{(m)}(y) \bigg| \leq C | x - y |^{s}. \end{alignat*} I'm looking for a Taylor - type theorem which could bound the remainder of the following polynomial $$ u(x) = \sum_{k \geq N} \frac{(x-y)^{k}}{k!} u^{(k)}(y) + R_{N}(x) $$ by $$| R_{N}(x) | \leq |x-y|^{m+s}.$$ In the simple case that $ N=1 $ I know that $$ u(x) = u(y) + u'(y)(x-y) + \int_{0}^{1} \Big( u'\big(y + t(x-y) \big) - u'(y) \Big)dt (x-y) .$$ But I have yet been successful in finding the right expression for the arbitrary case. Could anyone please shed some light on the problem? Thanks in advance!",Let be the space of bounded function which satisfies I'm looking for a Taylor - type theorem which could bound the remainder of the following polynomial by In the simple case that I know that But I have yet been successful in finding the right expression for the arbitrary case. Could anyone please shed some light on the problem? Thanks in advance!," C^{m,s}_{b}   u: \mathbb{R} \rightarrow \mathbb{R}  \begin{alignat*}{2}
\bigg| u^{(m)}(x) - u^{(m)}(y) \bigg| \leq C | x - y |^{s}.
\end{alignat*}  u(x) = \sum_{k \geq N} \frac{(x-y)^{k}}{k!} u^{(k)}(y) + R_{N}(x)  | R_{N}(x) | \leq |x-y|^{m+s}.  N=1   u(x) = u(y) + u'(y)(x-y) + \int_{0}^{1} \Big( u'\big(y + t(x-y) \big) - u'(y) \Big)dt (x-y) .","['real-analysis', 'analysis', 'holder-spaces']"
15,Must a continuous and periodic functions have a smallest period?,Must a continuous and periodic functions have a smallest period?,,"Let $D\subset\mathbb R$ and let $T\in(0,+\infty)$ . A function $f\colon D\longrightarrow\mathbb R$ is called a periodic function with period $T$ if, for each $x\in D$ , $x+T\in D$ and $f(x+T)=f(x)$ . If $D\subset\mathbb R$ and $f\colon D\longrightarrow\mathbb R$ is continuous and periodic, must there be, among all periods of $f$ , a minimal one? Questions like this one have been posted here before , but in each case, as far as I can see, the domain of $f$ was $\mathbb R$ , which implies that the set $P$ of periods, together with $0$ and $-P$ , is a subgroup of $(\mathbb{R},+)$ . Using that (together with continuity), it is easy to see that a minimal period must exist indeed. But I don't know whether it is true or not in the general case.","Let and let . A function is called a periodic function with period if, for each , and . If and is continuous and periodic, must there be, among all periods of , a minimal one? Questions like this one have been posted here before , but in each case, as far as I can see, the domain of was , which implies that the set of periods, together with and , is a subgroup of . Using that (together with continuity), it is easy to see that a minimal period must exist indeed. But I don't know whether it is true or not in the general case.","D\subset\mathbb R T\in(0,+\infty) f\colon D\longrightarrow\mathbb R T x\in D x+T\in D f(x+T)=f(x) D\subset\mathbb R f\colon D\longrightarrow\mathbb R f f \mathbb R P 0 -P (\mathbb{R},+)","['real-analysis', 'continuity', 'periodic-functions']"
16,"Precise definition of an ""algebraic function""","Precise definition of an ""algebraic function""",,"Remark. I'd like to avoid the ""ring of formal expressions"" viewpoint for this question. I know we can avoid these kinds of questions by working ""purely algebraically"" and in particular by taking the algebraic closure of $K(x).$ But I don't want to do this here. If I understand the wikipedia article correctly, a function $f$ is algebraic iff there's a polynomial $P(x,y)$ such that the equation $P(x,f(x)) = 0$ is true for all $x$ in the domain of $f$. For example, the function $$[0,\infty) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$$ is algebraic because $y^2-x$ has this property. However that article is pretty vague and I'm not sure I quite understand what's being said. In particular, which of the following functions would be considered algebraic? $(0,\infty) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $(0,1) \cup (2,3) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $(0,1) \cup (2,3) \rightarrow \mathbb{R}, x \mapsto \begin{cases}\sqrt{x} & x \in (0,1), \\ -\sqrt{x} & x \in (2,3)\end{cases}$ $(0,e) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $[0,\infty) \rightarrow \mathbb{R}, x \mapsto \begin{cases}\sqrt{x} & x \in \mathbb{Q}, \\ -\sqrt{x} & x \notin \mathbb{Q}\end{cases}$ These pertain to different issues, namely: Does a function have to be defined on the largest possible domain to be considered algebraic? Does it have to be defined on a connected set? If defined on a disconnected set, do we require that it be extensible to an algebraic function defined on a connected set? Can non-algebraic real numbers be used to define the domain of an ""algebraic"" function? Do they have to be continuous? Question: What are the standards here?","Remark. I'd like to avoid the ""ring of formal expressions"" viewpoint for this question. I know we can avoid these kinds of questions by working ""purely algebraically"" and in particular by taking the algebraic closure of $K(x).$ But I don't want to do this here. If I understand the wikipedia article correctly, a function $f$ is algebraic iff there's a polynomial $P(x,y)$ such that the equation $P(x,f(x)) = 0$ is true for all $x$ in the domain of $f$. For example, the function $$[0,\infty) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$$ is algebraic because $y^2-x$ has this property. However that article is pretty vague and I'm not sure I quite understand what's being said. In particular, which of the following functions would be considered algebraic? $(0,\infty) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $(0,1) \cup (2,3) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $(0,1) \cup (2,3) \rightarrow \mathbb{R}, x \mapsto \begin{cases}\sqrt{x} & x \in (0,1), \\ -\sqrt{x} & x \in (2,3)\end{cases}$ $(0,e) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $[0,\infty) \rightarrow \mathbb{R}, x \mapsto \begin{cases}\sqrt{x} & x \in \mathbb{Q}, \\ -\sqrt{x} & x \notin \mathbb{Q}\end{cases}$ These pertain to different issues, namely: Does a function have to be defined on the largest possible domain to be considered algebraic? Does it have to be defined on a connected set? If defined on a disconnected set, do we require that it be extensible to an algebraic function defined on a connected set? Can non-algebraic real numbers be used to define the domain of an ""algebraic"" function? Do they have to be continuous? Question: What are the standards here?",,"['real-analysis', 'algebraic-geometry', 'polynomials', 'algebraic-curves']"
17,How to find degree of a differential equation.,How to find degree of a differential equation.,,"I have a differential equation, $$e^{\large y^\prime} = x + x^3 + x^5 + y,$$ I need to find the degree of this equation. Using Wikipedia definition , In mathematics, the degree of a differential equation is the power of its highest derivative, after the equation has been made rational and integral in all of its derivatives. I would say that the degree is one because if I take $\log $ on both sides I get $${y^\prime} = \log(x + x^3 + x^5 + y).$$ My teacher says that the degree is not defined because this DE cannot be represented as sum of polynomials in derivatives of $y$. When I asked, what if we take $\log$ on both sides, he says that we are not allowed to perform any operations on the DE, that will change DE of which we have to find the degree. This contradicts the definition by Wikipedia. Who is correct? What is the degree of this DE, $1$ or not defined?","I have a differential equation, $$e^{\large y^\prime} = x + x^3 + x^5 + y,$$ I need to find the degree of this equation. Using Wikipedia definition , In mathematics, the degree of a differential equation is the power of its highest derivative, after the equation has been made rational and integral in all of its derivatives. I would say that the degree is one because if I take $\log $ on both sides I get $${y^\prime} = \log(x + x^3 + x^5 + y).$$ My teacher says that the degree is not defined because this DE cannot be represented as sum of polynomials in derivatives of $y$. When I asked, what if we take $\log$ on both sides, he says that we are not allowed to perform any operations on the DE, that will change DE of which we have to find the degree. This contradicts the definition by Wikipedia. Who is correct? What is the degree of this DE, $1$ or not defined?",,['calculus']
18,"Proving $f(x)=1/x$ on $(0,1 )$ is not uniformly continuous",Proving  on  is not uniformly continuous,"f(x)=1/x (0,1 )","My questions are about the reasoning made in the note http://folk.uib.no/st00895/MAT112-V12/unif-kont.pdf (which is in Norwegian). To prove that $f(x)=\frac{1}{x}$ is not uniformly continuous, the authors use the following ""result"" (Sats 2.12 in the note, which I translate below): Result 2.12: If for every $h>0$ we have that $|f(x+h)-f(x)|$ is unbounded on $I$, then $f$ is not uniformly continuous on $I$. Proof: The result follows directly from the definition of uniform continuity. In Example 2.14 (Eksempel 2.14), the authors look at $$|f(x+h)-f(x)|=\left|\frac{1}{x+h}-\frac{1}{x}\right|=\left|\frac{h}{x(x+h)}\right|.$$ They then claim that the above quantity is not bounded for any $h>0$, since $$\lim_{x\rightarrow 0}\left|\frac{h}{x(x+h)}\right|=\infty.$$ Question 1: Is it not possible to choose $h=x^2$, thereby obtaining $$\frac{h}{x(x+h)}=\frac{1}{\frac{x}{h}(x+h)}=\frac{1}{x(1/x+1)}=\frac{1}{x(1/x+1)}=\frac{1}{1+x}\rightarrow1 \text{ as }x\rightarrow 0.$$ Therefore, |f(x+h)-f(x)| is not unbounded, so we cannot use the result ""Result 2.12"". Is my argumentation correct?? Is it OK to choose $h$ like I have done? Question 2 : However, it seems correct to me that my argumentation is all you need to prove that  $f(x)=\frac{1}{x}$ is not uniformly continuous on (0,1). Here $x_1=x,x_2=x+x^2$, so that $|x_1-x_2|$ can be made arbitrarily small. However $|f(x_1)-f(x_2)|=1$, which stays the same regardless of how small we make $|x_1-x_2|$. Is this correct?","My questions are about the reasoning made in the note http://folk.uib.no/st00895/MAT112-V12/unif-kont.pdf (which is in Norwegian). To prove that $f(x)=\frac{1}{x}$ is not uniformly continuous, the authors use the following ""result"" (Sats 2.12 in the note, which I translate below): Result 2.12: If for every $h>0$ we have that $|f(x+h)-f(x)|$ is unbounded on $I$, then $f$ is not uniformly continuous on $I$. Proof: The result follows directly from the definition of uniform continuity. In Example 2.14 (Eksempel 2.14), the authors look at $$|f(x+h)-f(x)|=\left|\frac{1}{x+h}-\frac{1}{x}\right|=\left|\frac{h}{x(x+h)}\right|.$$ They then claim that the above quantity is not bounded for any $h>0$, since $$\lim_{x\rightarrow 0}\left|\frac{h}{x(x+h)}\right|=\infty.$$ Question 1: Is it not possible to choose $h=x^2$, thereby obtaining $$\frac{h}{x(x+h)}=\frac{1}{\frac{x}{h}(x+h)}=\frac{1}{x(1/x+1)}=\frac{1}{x(1/x+1)}=\frac{1}{1+x}\rightarrow1 \text{ as }x\rightarrow 0.$$ Therefore, |f(x+h)-f(x)| is not unbounded, so we cannot use the result ""Result 2.12"". Is my argumentation correct?? Is it OK to choose $h$ like I have done? Question 2 : However, it seems correct to me that my argumentation is all you need to prove that  $f(x)=\frac{1}{x}$ is not uniformly continuous on (0,1). Here $x_1=x,x_2=x+x^2$, so that $|x_1-x_2|$ can be made arbitrarily small. However $|f(x_1)-f(x_2)|=1$, which stays the same regardless of how small we make $|x_1-x_2|$. Is this correct?",,"['real-analysis', 'uniform-continuity']"
19,How to evaluate $ \sum\limits_{n=1}^{\infty} \left( \frac{H_{n}}{(n+1)^2.2^n} \right)$,How to evaluate, \sum\limits_{n=1}^{\infty} \left( \frac{H_{n}}{(n+1)^2.2^n} \right),"Evaluate $$ \sum_{n=1}^{\infty} \left( \dfrac{H_{n}}{(n+1)^2.2^n} \right)$$ Where $H_{n}$ is the $n^{th}$ Harmonic Number, i.e., $H_{n} = \displaystyle \sum _{k=1}^n \frac{1}{k}$ I tried to use the Integral Representation for the Harmonic number i.e., $$ H_{n} = \int_{0}^1 \dfrac {1-x^n}{1-x} \mathrm{d}x $$ and then interchanging the summation and integral signs, but it further complicated the problem. I also tried to use a result from my previous problem, i.e., $$ \sum_{n=1}^{\infty} \dfrac{1}{n^2.2^n} = \dfrac{\pi^2}{12} - \dfrac{\ln^2 2}{2} $$ but no significant progress so far. Any help will be appreciated. Thanks!","Evaluate Where is the Harmonic Number, i.e., I tried to use the Integral Representation for the Harmonic number i.e., and then interchanging the summation and integral signs, but it further complicated the problem. I also tried to use a result from my previous problem, i.e., but no significant progress so far. Any help will be appreciated. Thanks!", \sum_{n=1}^{\infty} \left( \dfrac{H_{n}}{(n+1)^2.2^n} \right) H_{n} n^{th} H_{n} = \displaystyle \sum _{k=1}^n \frac{1}{k}  H_{n} = \int_{0}^1 \dfrac {1-x^n}{1-x} \mathrm{d}x   \sum_{n=1}^{\infty} \dfrac{1}{n^2.2^n} = \dfrac{\pi^2}{12} - \dfrac{\ln^2 2}{2} ,"['calculus', 'real-analysis']"
20,"Prove that $\sum_{k=2}^{\infty} \frac{k^s}{k^{2s}-1}> \frac{3\sqrt{3}}{2}(\zeta(2s)-1),\space s>1$",Prove that,"\sum_{k=2}^{\infty} \frac{k^s}{k^{2s}-1}> \frac{3\sqrt{3}}{2}(\zeta(2s)-1),\space s>1","What ways would you propose for getting the inequality below? $$\sum_{k=2}^{\infty} \frac{k^s}{k^{2s}-1}> \frac{3\sqrt{3}}{2}(\zeta(2s)-1),\space s>1$$ The left side may be written as $$\sum_{k=2}^{\infty} \frac{k^s}{k^{2s}-1}>\zeta(s)-1$$ but how do we prove then that $$\zeta(s)-1> \frac{3\sqrt{3}}{2}(\zeta(2s)-1)$$ ? Can we do it without using integrals?","What ways would you propose for getting the inequality below? $$\sum_{k=2}^{\infty} \frac{k^s}{k^{2s}-1}> \frac{3\sqrt{3}}{2}(\zeta(2s)-1),\space s>1$$ The left side may be written as $$\sum_{k=2}^{\infty} \frac{k^s}{k^{2s}-1}>\zeta(s)-1$$ but how do we prove then that $$\zeta(s)-1> \frac{3\sqrt{3}}{2}(\zeta(2s)-1)$$ ? Can we do it without using integrals?",,"['calculus', 'real-analysis', 'sequences-and-series', 'inequality']"
21,For which real numbers $c$ is $\frac{e^x+e^{-x}}{2} \le e^{cx^2}$ for all real numbers $x$?,For which real numbers  is  for all real numbers ?,c \frac{e^x+e^{-x}}{2} \le e^{cx^2} x,"This question comes from the 1980 Putnam exam. My work is shown below. For all integers $n \ge 1$, \begin{align} (2n)!&=n!\cdot\underbrace{(n+1)(n+2)(n+3)\cdots(2n-2)(2n-1)(2n)}_{n \text{ terms}} \\  &\ge n! \cdot \underbrace{2 \cdot 2 \cdot 2\cdots 2\cdot2 \cdot 2}_{n \text{ terms}} \\ &=2^nn! \\ \implies \frac{1}{(2n)!} &\le \frac{1}{2^nn!}.\end{align} So we have \begin{align} \frac{e^x+e^{-x}}{2} = \cosh(x) &= \sum_{n=0}^{\infty} \frac{x^{2n}}{(2n)!} \\ &\le \sum_{n=0}^{\infty}  \frac{x^{2n}}{2^nn!} \\ &= \sum_{n=0}^{\infty}  \frac{\left(\frac{x^2}{2} \right)^n}{n!} \\ &= e^{\frac{1}{2}x^2} \end{align} Therefore, $\boxed{c \ge \frac{1}{2}}$. How do I know that it is $\ge$, not $\le$? Other than that, how did I do on my work?","This question comes from the 1980 Putnam exam. My work is shown below. For all integers $n \ge 1$, \begin{align} (2n)!&=n!\cdot\underbrace{(n+1)(n+2)(n+3)\cdots(2n-2)(2n-1)(2n)}_{n \text{ terms}} \\  &\ge n! \cdot \underbrace{2 \cdot 2 \cdot 2\cdots 2\cdot2 \cdot 2}_{n \text{ terms}} \\ &=2^nn! \\ \implies \frac{1}{(2n)!} &\le \frac{1}{2^nn!}.\end{align} So we have \begin{align} \frac{e^x+e^{-x}}{2} = \cosh(x) &= \sum_{n=0}^{\infty} \frac{x^{2n}}{(2n)!} \\ &\le \sum_{n=0}^{\infty}  \frac{x^{2n}}{2^nn!} \\ &= \sum_{n=0}^{\infty}  \frac{\left(\frac{x^2}{2} \right)^n}{n!} \\ &= e^{\frac{1}{2}x^2} \end{align} Therefore, $\boxed{c \ge \frac{1}{2}}$. How do I know that it is $\ge$, not $\le$? Other than that, how did I do on my work?",,"['calculus', 'real-analysis']"
22,Let $(f(x))^2$ and $(f(x))^3$ are $C^{\infty}$. Prove or disprove that $f$ is $C^{\infty}$.,Let  and  are . Prove or disprove that  is .,(f(x))^2 (f(x))^3 C^{\infty} f C^{\infty},"Suppose $f(x), -\infty < x < +\infty$ , is a real valued function such that both $(f(x))^2$ and $(f(x))^3$ are $C^{\infty}$ . Must $f$ be $C^{\infty}$ ? I had seen this exercise somewhere on internet long ago, it is quite disturbing to post exercises where I block completely (and this is not lack of trying). Someone could just give me a way to solve it ? Thanks.","Suppose , is a real valued function such that both and are . Must be ? I had seen this exercise somewhere on internet long ago, it is quite disturbing to post exercises where I block completely (and this is not lack of trying). Someone could just give me a way to solve it ? Thanks.","f(x), -\infty < x < +\infty (f(x))^2 (f(x))^3 C^{\infty} f C^{\infty}",['real-analysis']
23,"proof that $\hat{f}(x,y)=f(x-y)$ is measurable if $f$ is measurable, Stein & Shakarchi Prop 3.9","proof that  is measurable if  is measurable, Stein & Shakarchi Prop 3.9","\hat{f}(x,y)=f(x-y) f","I am following Stein and Shakarchi's book on analysis ( Real Analysis: Measure Theory, Integration, and Hilbert Spaces ) and in Proposition 3.9 on p.86 they present a proof that if $f$ is a measurable function on $\mathbb{R}^d$ then $\hat{f}(x,y)=f(x-y)$ is measurable on $\mathbb{R}^d \times \mathbb{R}^d$. The strange thing is, I can follow all the arguments in the proof but I cannot make sense of all of it, I can't string the facts together. A snipped of the proof is as follows: (from Google books) The book concludes the proof by saying that any measurable set $E$ can be written as a difference of a $G_\delta$ and a set of measure 0. Alright so if I proceed with this then because $E=\{z \in \mathbb{R}^d:f(z)<a \} $ as defined in the book is measurable, then it can be written as $A-B$ where $A$ is a $G_\delta$ set while $m(B)=0.$ Now how do I relate this to $\tilde{E}$ as defined in the proof? help very much appreciated!","I am following Stein and Shakarchi's book on analysis ( Real Analysis: Measure Theory, Integration, and Hilbert Spaces ) and in Proposition 3.9 on p.86 they present a proof that if $f$ is a measurable function on $\mathbb{R}^d$ then $\hat{f}(x,y)=f(x-y)$ is measurable on $\mathbb{R}^d \times \mathbb{R}^d$. The strange thing is, I can follow all the arguments in the proof but I cannot make sense of all of it, I can't string the facts together. A snipped of the proof is as follows: (from Google books) The book concludes the proof by saying that any measurable set $E$ can be written as a difference of a $G_\delta$ and a set of measure 0. Alright so if I proceed with this then because $E=\{z \in \mathbb{R}^d:f(z)<a \} $ as defined in the book is measurable, then it can be written as $A-B$ where $A$ is a $G_\delta$ set while $m(B)=0.$ Now how do I relate this to $\tilde{E}$ as defined in the proof? help very much appreciated!",,"['real-analysis', 'integration']"
24,One approach to showing the total variation of an absolutely continuous function f is the integral of |f'|,One approach to showing the total variation of an absolutely continuous function f is the integral of |f'|,,"I'm trying to solve the following problem in preparation for an exam: Suppose $f$ is absolutely continuous on [a,b]. Show that $$ V[f; a,b] = \int_{[a,b]} |f'| dx$$ There is a suggestion to define $F(x) = V[f; a,x]$ and show $F \pm f$ are absolutely continuous and nondecreasing, then obtain a relationship between $F'$ and $f'$. It's the last part that is perplexing -- I have succeeded (I think) in demonstrating $F \pm f$ AC and nondecreasing -- but I don't see how to derive a (useful) relationship between $F'$ and $f'$. Any suggestions? Am I missing something obvious?","I'm trying to solve the following problem in preparation for an exam: Suppose $f$ is absolutely continuous on [a,b]. Show that $$ V[f; a,b] = \int_{[a,b]} |f'| dx$$ There is a suggestion to define $F(x) = V[f; a,x]$ and show $F \pm f$ are absolutely continuous and nondecreasing, then obtain a relationship between $F'$ and $f'$. It's the last part that is perplexing -- I have succeeded (I think) in demonstrating $F \pm f$ AC and nondecreasing -- but I don't see how to derive a (useful) relationship between $F'$ and $f'$. Any suggestions? Am I missing something obvious?",,['real-analysis']
25,About uniformly continuity for bounded and continuous function,About uniformly continuity for bounded and continuous function,,"Let $f:\left\{x\in\mathbb{R}^n\vert\parallel x\parallel<1\right\}\rightarrow\mathbb{R}$ be an one-to-one bounded continuous function. I want to construct such $f$ which is not uniformly continuous. In this case, I thought I can construct $f$ with a restriction $n=2$. But I'm confused because $f$ is bounded so I can't use functions like $\frac{1}{x}$ on $(0,1)$. To top that off, $f$ is even one-to-one so I gave up and now I'm writing this to get some help from you who is smarter than me. Please give me some help to solve this problem. Thanks.","Let $f:\left\{x\in\mathbb{R}^n\vert\parallel x\parallel<1\right\}\rightarrow\mathbb{R}$ be an one-to-one bounded continuous function. I want to construct such $f$ which is not uniformly continuous. In this case, I thought I can construct $f$ with a restriction $n=2$. But I'm confused because $f$ is bounded so I can't use functions like $\frac{1}{x}$ on $(0,1)$. To top that off, $f$ is even one-to-one so I gave up and now I'm writing this to get some help from you who is smarter than me. Please give me some help to solve this problem. Thanks.",,"['real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
26,Continuity of the inverse $f^{-1}$ at $f(x)$ when $f$ is bijective and continuous at $x$.,Continuity of the inverse  at  when  is bijective and continuous at .,f^{-1} f(x) f x,"Prove or disprove: Let $f:\mathbb{R}\to\mathbb{R}$ be bijective and $f$ is continuous at $x$. Then $f^{-1}$ is continuous at $f(x)$. Any hints are welcome. If this is false, I would like to have a counterexample. I tried the $\epsilon,\delta$ argument and I know this statement may not be true. But this problem only tells the continuity at a single point, and the function may not be monotone, for example: $$f:\mathbb{R}\to\mathbb{R},\quad f(x)=\begin{cases}x,&x\in\mathbb{Q}\\ -x,&x\notin\mathbb{Q}\end{cases}$$ This function is bijective and is only continuous at $0$, and it is not monotone.","Prove or disprove: Let $f:\mathbb{R}\to\mathbb{R}$ be bijective and $f$ is continuous at $x$. Then $f^{-1}$ is continuous at $f(x)$. Any hints are welcome. If this is false, I would like to have a counterexample. I tried the $\epsilon,\delta$ argument and I know this statement may not be true. But this problem only tells the continuity at a single point, and the function may not be monotone, for example: $$f:\mathbb{R}\to\mathbb{R},\quad f(x)=\begin{cases}x,&x\in\mathbb{Q}\\ -x,&x\notin\mathbb{Q}\end{cases}$$ This function is bijective and is only continuous at $0$, and it is not monotone.",,"['real-analysis', 'continuity', 'examples-counterexamples', 'inverse']"
27,"$\sum a_n$ be convergent but not absolutely convergent, $\sum_{n=1}^{\infty} a_n=0$","be convergent but not absolutely convergent,",\sum a_n \sum_{n=1}^{\infty} a_n=0,"Let $\sum a_n$ be convergent but not absolutely convergent, $\sum_{n=1}^{\infty} a_n=0$,$s_k$ denotes the partial sum then could anyone tell me which of the following is/are correct? $1.$ $s_k=0$ for infinitely many $k$ $2$. $s_k>0$ and $<0$ for infnitely many $k$ 3.$s_k>0$ for all $k$ 4.$s_k>0$ for all but finitely many $k$ if we take $a_n=(-1)^n{1\over n}$ then $\sum a_n$ is convergent but not absolutely convergent,but I don't know $\sum_{n=1}^{\infty} a_n=0$? so I am puzzled could any one tell me how to proceed?","Let $\sum a_n$ be convergent but not absolutely convergent, $\sum_{n=1}^{\infty} a_n=0$,$s_k$ denotes the partial sum then could anyone tell me which of the following is/are correct? $1.$ $s_k=0$ for infinitely many $k$ $2$. $s_k>0$ and $<0$ for infnitely many $k$ 3.$s_k>0$ for all $k$ 4.$s_k>0$ for all but finitely many $k$ if we take $a_n=(-1)^n{1\over n}$ then $\sum a_n$ is convergent but not absolutely convergent,but I don't know $\sum_{n=1}^{\infty} a_n=0$? so I am puzzled could any one tell me how to proceed?",,"['real-analysis', 'sequences-and-series']"
28,Uniform Convergence: The two definitions,Uniform Convergence: The two definitions,,"I know of the two equivalent definitions for uniform convergence. Namely: $f_n(x)$ converges uniformly to $f(x)$ if either: a)$$ \forall \epsilon \exists N \in \mathbb N s.t. \forall x \in D \ \forall n \geq N: |f_n(x) - f| < \epsilon $$ or b) $$\lim_{n \rightarrow \infty} \sup_{x\in D} |f_n(x) - f(x)| = 0 $$ While I see why in the case of a) the same ""speed of convergence"" is guaranteed since one epsilon is chosen for all $x$, I unfortunately cannot make sense of b) on an intuitive level. So I see that b) states that the biggest difference between the sequences of the functions and the limit functions must become zero for n going to infinity. But how does that also guarantee this convergence process is of ""uniform speed"" for all x?","I know of the two equivalent definitions for uniform convergence. Namely: $f_n(x)$ converges uniformly to $f(x)$ if either: a)$$ \forall \epsilon \exists N \in \mathbb N s.t. \forall x \in D \ \forall n \geq N: |f_n(x) - f| < \epsilon $$ or b) $$\lim_{n \rightarrow \infty} \sup_{x\in D} |f_n(x) - f(x)| = 0 $$ While I see why in the case of a) the same ""speed of convergence"" is guaranteed since one epsilon is chosen for all $x$, I unfortunately cannot make sense of b) on an intuitive level. So I see that b) states that the biggest difference between the sequences of the functions and the limit functions must become zero for n going to infinity. But how does that also guarantee this convergence process is of ""uniform speed"" for all x?",,[]
29,Are functions that map dense sets to dense sets continuous?,Are functions that map dense sets to dense sets continuous?,,"Suppose $X$ and $Y$ are separable metric spaces, and that $f:X\rightarrow Y$ is a bijective function that maps every countable dense set of $X$ to a dense set of $Y$. Are functions with this property necessarily continuous? If not, if $g:X\rightarrow Y$ is bijective and Borel do we obtain that property (dense sets map to dense sets)? Thanks for any information.","Suppose $X$ and $Y$ are separable metric spaces, and that $f:X\rightarrow Y$ is a bijective function that maps every countable dense set of $X$ to a dense set of $Y$. Are functions with this property necessarily continuous? If not, if $g:X\rightarrow Y$ is bijective and Borel do we obtain that property (dense sets map to dense sets)? Thanks for any information.",,"['real-analysis', 'general-topology', 'measure-theory']"
30,Integral-Summation inequality.,Integral-Summation inequality.,,"The following question was in an entrance exam: Show that, if $n\gt0$ , then: $$\int_{{\rm e}^{1/n}}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}=\frac{2}{n^2{\rm e}}$$ You are allowed to assume $\lim_{x\to\infty}{\frac{\ln{x}}{x}}=0$ . Hence explain why, if $1\lt a\lt b$ , then: $$\int_{b}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}\lt\int_{a}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}$$ Deduce that: $$\sum_{n=1}^{N}{\frac{1}{n^{2}}}<\frac{\rm e}{2}\int_{{\rm e}^{1/N}}^{\infty}{\left(\frac{1-x^{-N}}{x^{2}-x}\right)\ln{x}\:dx},$$ Where $N\in\mathbb{N}:N\gt1$ . The first part I believe I integrate by parts, such that: $$\int{\frac{\ln{x}}{x^{n+1}}\:dx}=\frac{1}{n}\int{x^{-n-1}\:dx}-\frac{x^{-n}\ln{x}}{n}$$ Clearly $\int{x^{-n-1}\:dx}=-\frac{x^{-n}}{n}$ , so we have: $$\int_{{\rm e}^{1/n}}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}=\left.\frac{x^{-n}(n\ln{x}+1)}{n^{2}}\right|_{{\rm e}^{1/n}}^{\infty}=\frac{{\rm e}^{-\frac{n}{n}}(\frac{n}{n}+1)}{n^{2}}-0=\frac{2}{n^{2}{\rm e}}$$ As required. To prove the next inequality, all that is required is to demonstrate that $\left.\frac{\ln{x}}{x^{n+1}}\right|_{x=1}\geq0$ , $\lim_{x\to\infty}{\frac{\ln{x}}{x^{n+1}}}\geq0$ , and $\left(\frac{\ln{x}}{x^{n+1}}\right)'\neq0$ , $\forall x\in[1,\infty)$ and $\forall n\gt0$ . The first inequality is verified simply by observing that $\ln{1}=0$ , therefore $\frac{\ln{1}}{1^{n+1}}=0$ , $\forall n$ .The second inequality can be written as: $$\lim_{x\to\infty}{\left(\frac{1}{x^{n}}\frac{\ln{x}}{x}\right)},$$ And as we know $\lim_{x\to\infty}{\frac{\ln{x}}{x}}=0$ , and $\lim_{x\to\infty}{\frac{1}{x^{n}}}=0$ , the second inequality must also be true. To verify the second inequality we simply differentiate using the quotient rule and look for critical points: $$\frac{d}{dx}{\left(\frac{\ln{x}}{x^{n+1}}\right)}=\frac{x^{n}-(n+1)x^{n}\ln{x}}{x^{2n+2}}=\frac{1-(n+1)\ln{x}}{x^{n+2}}$$ As $\ln{x}$ is a monotonically increasing function, and at $x=1$ , $\ln{x}=0$ , we can show that $\forall n\gt 0$ , and $x\gt1$ , $\left(\frac{\ln{x}}{x^{n+1}}\right)\leq 0$ , therefore, the function is positive for all values of $x\gt 0$ , which means by the fundamental theorem of calculus that if $1\lt a\lt b$ , then $\forall n\gt0$ : $$\int_{b}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}<\int_{a}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}$$ As required. However, I am stuck for the final part of the question. My first thought was that as $\frac{1}{n^{2}}$ is monotonically decreasing, $\forall n\gt0$ ; any integral performed over the region $(0,\infty)$ will have a positive error term, therefore, we can replace the summation with $\int_{1}^{N}{\frac{1}{n^2}\:dn}=\left.-\frac{1}{n}\right|_{1}^{N}=-\frac{1}{N}+1$ . I am not sure how to perform the second integral, however. Thanks in advance!","The following question was in an entrance exam: Show that, if , then: You are allowed to assume . Hence explain why, if , then: Deduce that: Where . The first part I believe I integrate by parts, such that: Clearly , so we have: As required. To prove the next inequality, all that is required is to demonstrate that , , and , and . The first inequality is verified simply by observing that , therefore , .The second inequality can be written as: And as we know , and , the second inequality must also be true. To verify the second inequality we simply differentiate using the quotient rule and look for critical points: As is a monotonically increasing function, and at , , we can show that , and , , therefore, the function is positive for all values of , which means by the fundamental theorem of calculus that if , then : As required. However, I am stuck for the final part of the question. My first thought was that as is monotonically decreasing, ; any integral performed over the region will have a positive error term, therefore, we can replace the summation with . I am not sure how to perform the second integral, however. Thanks in advance!","n\gt0 \int_{{\rm e}^{1/n}}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}=\frac{2}{n^2{\rm e}} \lim_{x\to\infty}{\frac{\ln{x}}{x}}=0 1\lt a\lt b \int_{b}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}\lt\int_{a}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx} \sum_{n=1}^{N}{\frac{1}{n^{2}}}<\frac{\rm e}{2}\int_{{\rm e}^{1/N}}^{\infty}{\left(\frac{1-x^{-N}}{x^{2}-x}\right)\ln{x}\:dx}, N\in\mathbb{N}:N\gt1 \int{\frac{\ln{x}}{x^{n+1}}\:dx}=\frac{1}{n}\int{x^{-n-1}\:dx}-\frac{x^{-n}\ln{x}}{n} \int{x^{-n-1}\:dx}=-\frac{x^{-n}}{n} \int_{{\rm e}^{1/n}}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}=\left.\frac{x^{-n}(n\ln{x}+1)}{n^{2}}\right|_{{\rm e}^{1/n}}^{\infty}=\frac{{\rm e}^{-\frac{n}{n}}(\frac{n}{n}+1)}{n^{2}}-0=\frac{2}{n^{2}{\rm e}} \left.\frac{\ln{x}}{x^{n+1}}\right|_{x=1}\geq0 \lim_{x\to\infty}{\frac{\ln{x}}{x^{n+1}}}\geq0 \left(\frac{\ln{x}}{x^{n+1}}\right)'\neq0 \forall x\in[1,\infty) \forall n\gt0 \ln{1}=0 \frac{\ln{1}}{1^{n+1}}=0 \forall n \lim_{x\to\infty}{\left(\frac{1}{x^{n}}\frac{\ln{x}}{x}\right)}, \lim_{x\to\infty}{\frac{\ln{x}}{x}}=0 \lim_{x\to\infty}{\frac{1}{x^{n}}}=0 \frac{d}{dx}{\left(\frac{\ln{x}}{x^{n+1}}\right)}=\frac{x^{n}-(n+1)x^{n}\ln{x}}{x^{2n+2}}=\frac{1-(n+1)\ln{x}}{x^{n+2}} \ln{x} x=1 \ln{x}=0 \forall n\gt 0 x\gt1 \left(\frac{\ln{x}}{x^{n+1}}\right)\leq 0 x\gt 0 1\lt a\lt b \forall n\gt0 \int_{b}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx}<\int_{a}^{\infty}{\frac{\ln{x}}{x^{n+1}}\:dx} \frac{1}{n^{2}} \forall n\gt0 (0,\infty) \int_{1}^{N}{\frac{1}{n^2}\:dn}=\left.-\frac{1}{n}\right|_{1}^{N}=-\frac{1}{N}+1","['calculus', 'real-analysis', 'integration', 'inequality', 'definite-integrals']"
31,Question about a proof in Evans,Question about a proof in Evans,,"On page 57. in Partial Differential Equation by Lawrence C. Evans, he prove the maximum principle for the Cauchy problem of the heat equation, i.e. (I quote) Suppose $u\in C^2_1(\mathbb{R}^n\times (0,T])\cap C(\mathbb{R}^n\times [0,T])$ solves $u_t-\Delta u= 0$ in $\mathbb{R}^n\times (0,T)$ and $u=g$ on $\mathbb{R}^n\times \{t=0\}$. Moreover, u satisfies the growth estimate $$u(x,t)\le Ae^{a|x|^2}$$ for $x\in\mathbb{R}^n,0\le t\le T$ for constants $A,a>0$. Then $$\sup_{\mathbb{R}^n\times [0,T]}u = \sup_{\mathbb{R}^n}g$$ In the proof they define $v(x,t):=u(x,t)-\frac{\mu}{(T+\epsilon -t)^\frac{n}{2}}\exp{\frac{|x-y|^2}{4(T+\epsilon -t)}}$ The proof consists of several steps. First they show for $4aT<1$ that $\max_{\overline{U_T}} v= \max_{\Gamma_T}v$, where $U_T:=B^0(y,r)\times (0,T]$ for fixed $r>0$. If $x\in \mathbb{R}^n$ then $v(x,0)\le g(x)$ Now in equation $(29)$ they say: for $r$ selected sufficiently large, we have $v(x,t)\le A\exp{a(|y|+r)^2}-\mu (4(a+\gamma))^{\frac{n}{2}}\exp{(a+\gamma)r^2}\le \sup_{\mathbb{R}^n}g$. Why is this all less or equal the supremum of $g$, for large $r$? And why can we conclude with all these facts, that $v(y,t)\le \sup_{\mathbb{R}^n} g$ for all $y\in \mathbb{R}^n$ and $0\le t\le T$?","On page 57. in Partial Differential Equation by Lawrence C. Evans, he prove the maximum principle for the Cauchy problem of the heat equation, i.e. (I quote) Suppose $u\in C^2_1(\mathbb{R}^n\times (0,T])\cap C(\mathbb{R}^n\times [0,T])$ solves $u_t-\Delta u= 0$ in $\mathbb{R}^n\times (0,T)$ and $u=g$ on $\mathbb{R}^n\times \{t=0\}$. Moreover, u satisfies the growth estimate $$u(x,t)\le Ae^{a|x|^2}$$ for $x\in\mathbb{R}^n,0\le t\le T$ for constants $A,a>0$. Then $$\sup_{\mathbb{R}^n\times [0,T]}u = \sup_{\mathbb{R}^n}g$$ In the proof they define $v(x,t):=u(x,t)-\frac{\mu}{(T+\epsilon -t)^\frac{n}{2}}\exp{\frac{|x-y|^2}{4(T+\epsilon -t)}}$ The proof consists of several steps. First they show for $4aT<1$ that $\max_{\overline{U_T}} v= \max_{\Gamma_T}v$, where $U_T:=B^0(y,r)\times (0,T]$ for fixed $r>0$. If $x\in \mathbb{R}^n$ then $v(x,0)\le g(x)$ Now in equation $(29)$ they say: for $r$ selected sufficiently large, we have $v(x,t)\le A\exp{a(|y|+r)^2}-\mu (4(a+\gamma))^{\frac{n}{2}}\exp{(a+\gamma)r^2}\le \sup_{\mathbb{R}^n}g$. Why is this all less or equal the supremum of $g$, for large $r$? And why can we conclude with all these facts, that $v(y,t)\le \sup_{\mathbb{R}^n} g$ for all $y\in \mathbb{R}^n$ and $0\le t\le T$?",,"['real-analysis', 'partial-differential-equations']"
32,"M compact  $p\in M$ , there exist $f:M-p\to M-p$ continuous bijection but not homeomorphism?","M compact   , there exist  continuous bijection but not homeomorphism?",p\in M f:M-p\to M-p,"Let M be a compact metric space. We know that if $ g:M\to M$ is a continuous bijection then it's a homeomorphism. But I want to know, if I have a continuous bijection $ f:M - \left\{ p \right\} \to M - \left\{ p \right\} $, then it's true that can always be extended to a continuous bijection $M\to M$ or not? clearly I assume that  $ M - \left\{ p \right\} $ it's under the restricted metric of M. EDITED: Even knowing that M it's the one point compactification and that the open sets of M are all the open sets of M-p , and the complement of compacts of M-p , even with that I can't prove the result. Maybe it's not true. I'm not sure, if you want to use that you are welcome, and maybe it's false and I need a counterexample :/ I'll also change the name of the post","Let M be a compact metric space. We know that if $ g:M\to M$ is a continuous bijection then it's a homeomorphism. But I want to know, if I have a continuous bijection $ f:M - \left\{ p \right\} \to M - \left\{ p \right\} $, then it's true that can always be extended to a continuous bijection $M\to M$ or not? clearly I assume that  $ M - \left\{ p \right\} $ it's under the restricted metric of M. EDITED: Even knowing that M it's the one point compactification and that the open sets of M are all the open sets of M-p , and the complement of compacts of M-p , even with that I can't prove the result. Maybe it's not true. I'm not sure, if you want to use that you are welcome, and maybe it's false and I need a counterexample :/ I'll also change the name of the post",,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness', 'examples-counterexamples']"
33,Norm on a Hölder's space,Norm on a Hölder's space,,"I want to prove that Hölder space is a Banach space under the ""Hölder Norm"" ie. $\|\cdot\|_{C^{k,\alpha}}$.  Any hints would be appreciable .","I want to prove that Hölder space is a Banach space under the ""Hölder Norm"" ie. $\|\cdot\|_{C^{k,\alpha}}$.  Any hints would be appreciable .",,"['real-analysis', 'functional-analysis', 'holder-spaces']"
34,A limit with integral,A limit with integral,,"We are given a continuous function $f \colon [0,1] \to \mathbb{R}$. What is the value, if it exists, of the limit $$\lim_{t \to +\infty} \frac{1}{t} \log \int_0^1 \cosh (t f(x))\, \mathrm{d}x \ ?$$ PS: this is not homework. It's a question contained in a Ph.D test, and I am unable to make progress toward the solution :-(","We are given a continuous function $f \colon [0,1] \to \mathbb{R}$. What is the value, if it exists, of the limit $$\lim_{t \to +\infty} \frac{1}{t} \log \int_0^1 \cosh (t f(x))\, \mathrm{d}x \ ?$$ PS: this is not homework. It's a question contained in a Ph.D test, and I am unable to make progress toward the solution :-(",,"['calculus', 'real-analysis', 'integration', 'limits']"
35,Uniformly distributed rationals,Uniformly distributed rationals,,"Is there any algorithm, function or formula $f(n)$, which is a bijection between the positive integers and the rationals in $(0,1)$, with the condition, that for all real numbers $a,b,x$ with $0<a<b<1<x$, if we let $i(x)$ be the number of distinct integers $0<n_j<x$ which satisfy $a<f(n_j)<b$, then we have $\lim_{x\rightarrow\infty}i(x)/x=b-a$?","Is there any algorithm, function or formula $f(n)$, which is a bijection between the positive integers and the rationals in $(0,1)$, with the condition, that for all real numbers $a,b,x$ with $0<a<b<1<x$, if we let $i(x)$ be the number of distinct integers $0<n_j<x$ which satisfy $a<f(n_j)<b$, then we have $\lim_{x\rightarrow\infty}i(x)/x=b-a$?",,"['real-analysis', 'analysis', 'equidistribution']"
36,Is $f(2x)/f(x)$ nonincreasing for concave functions with $f(0)=0$?,Is  nonincreasing for concave functions with ?,f(2x)/f(x) f(0)=0,"I have a question about concave functions. Let $f:R_+\rightarrow R_+$ be any nonidentically zero, nondecreasing, continuous, concave function with $f(0)=0$. Do we have that the ratio function $f(2x)/f(x)$ is nonincreasing on $(0,+\infty)$?","I have a question about concave functions. Let $f:R_+\rightarrow R_+$ be any nonidentically zero, nondecreasing, continuous, concave function with $f(0)=0$. Do we have that the ratio function $f(2x)/f(x)$ is nonincreasing on $(0,+\infty)$?",,"['calculus', 'real-analysis', 'analysis', 'inequality']"
37,Functions $f(x)$ for which the set of functions $af(x+b)$ is closed under addition,Functions  for which the set of functions  is closed under addition,f(x) af(x+b),"Let $f(x)$ be a continuous function from $\mathbb{R}$ to $\mathbb{R}$ . Consider the set of functions $af(x+b)$ for $a,b \in \mathbb{R}$ . If that set is closed under addition, what can $f$ be? I suspect the possible $f$ 's are exponentials, sinusoids, constants and their products. (For example, the sum of any two sinusoids with the same period, each arbitrarily shifted and scaled, is another sinusoid with the same period - and I couldn't find any other periodic function with that property - that was my original motivation for the problem.) But I don't know how to prove there are no others. Another funny note is that $f(x)=x$ yields a set which is almost closed under addition, except its additive closure also includes nonzero constant functions, which don't have the form $a(x+b)$ .","Let be a continuous function from to . Consider the set of functions for . If that set is closed under addition, what can be? I suspect the possible 's are exponentials, sinusoids, constants and their products. (For example, the sum of any two sinusoids with the same period, each arbitrarily shifted and scaled, is another sinusoid with the same period - and I couldn't find any other periodic function with that property - that was my original motivation for the problem.) But I don't know how to prove there are no others. Another funny note is that yields a set which is almost closed under addition, except its additive closure also includes nonzero constant functions, which don't have the form .","f(x) \mathbb{R} \mathbb{R} af(x+b) a,b \in \mathbb{R} f f f(x)=x a(x+b)","['real-analysis', 'functions', 'functional-equations']"
38,Why does $\int_{-x}^x f^\alpha\leq f(-x)+f(x)$ imply $f=0$?,Why does  imply ?,\int_{-x}^x f^\alpha\leq f(-x)+f(x) f=0,"Let $0\leq f\in C(\mathbb{R})$ , and for some $\alpha>1$ , we have \begin{equation*}\begin{aligned} \int_{-x}^x f^\alpha\leq f(-x)+f(x), \forall\ x\in [0,+\infty). \end{aligned}\end{equation*} Prove that $f\equiv 0$ . Let $F(x)=\int_{-x}^x f^\alpha$ , then $F'(x)=f^\alpha(x)+f^\alpha(-x) \leq [f(x)+f(-x)]^\alpha$ . How to use the assumption? What to do next? Any ideas?","Let , and for some , we have Prove that . Let , then . How to use the assumption? What to do next? Any ideas?","0\leq f\in C(\mathbb{R}) \alpha>1 \begin{equation*}\begin{aligned}
\int_{-x}^x f^\alpha\leq f(-x)+f(x), \forall\ x\in [0,+\infty).
\end{aligned}\end{equation*} f\equiv 0 F(x)=\int_{-x}^x f^\alpha F'(x)=f^\alpha(x)+f^\alpha(-x)
\leq [f(x)+f(-x)]^\alpha","['real-analysis', 'calculus', 'integration']"
39,"'Amount' of nowhere-differentiable functions in $C([0,1])$?",'Amount' of nowhere-differentiable functions in ?,"C([0,1])","A well-known consequence of the Baire Category Theorem that the set of nowhere-differentiable continuous functions is dense in $C([0,1])$ . This is often cited as 'almost all continuous functions are nowhere differentiable' (see here ), but to me this seems like a strange way of stating the fact, akin to saying that 'almost all real numbers are rational' just because the rationals are dense in the reals. By the Weierstrass Approximation Theorem, the set of polynomials is also dense in $C([0,1])$ , so is it correct to say that almost all continuous functions are polynomials? This statement seems to contradict the original statement about nowhere-differentiable functions. I was wondering if there was some way to remedy my confusion using a measure on $C([0,1])$ , since the Lebesgue measure on $[0,1]$ clarifies what it means to say 'almost all' in the context of real numbers; that a property holds for all real numbers outside a set of measure $0$ . In this context, certainly not 'almost all' real numbers are rational, almost all real numbers would be irrational. If this is the right way of thinking about 'sizes' of subsets of $C([0,1])$ , what would be the 'right' measure to use? If not a measure, is there another way to formalize the idea of 'almost all' in the context of $C([0,1])$ ?","A well-known consequence of the Baire Category Theorem that the set of nowhere-differentiable continuous functions is dense in . This is often cited as 'almost all continuous functions are nowhere differentiable' (see here ), but to me this seems like a strange way of stating the fact, akin to saying that 'almost all real numbers are rational' just because the rationals are dense in the reals. By the Weierstrass Approximation Theorem, the set of polynomials is also dense in , so is it correct to say that almost all continuous functions are polynomials? This statement seems to contradict the original statement about nowhere-differentiable functions. I was wondering if there was some way to remedy my confusion using a measure on , since the Lebesgue measure on clarifies what it means to say 'almost all' in the context of real numbers; that a property holds for all real numbers outside a set of measure . In this context, certainly not 'almost all' real numbers are rational, almost all real numbers would be irrational. If this is the right way of thinking about 'sizes' of subsets of , what would be the 'right' measure to use? If not a measure, is there another way to formalize the idea of 'almost all' in the context of ?","C([0,1]) C([0,1]) C([0,1]) [0,1] 0 C([0,1]) C([0,1])","['real-analysis', 'measure-theory', 'baire-category']"
40,"Does there exist a continuous function $f$ such that $f(x)+f(x^2)=x$ for $x\in[0,1]$?",Does there exist a continuous function  such that  for ?,"f f(x)+f(x^2)=x x\in[0,1]","Let $f$ be a continuous real valued function from $[0,1]$ such that $$f(x)+f(x^2)=x$$ for all $x\in [0,1]$ . Does there exist such a function? Plugging $x=0$ and $x=1$ respectively in the given equation we obtain $f(0)=0$ and $f(1)=\frac{1}{2}$ . By the intermediate value theorem, $f$ attains any value between $0$ and $1$ . Moreover, the range of $f$ is $[m,M]$ where $m$ (resp. $M$ ) is the minimum (resp. maximum) value of the function over $[0,1]$ . How to use these facts to decide whether such function exists or not? Please give some hint to proceed. Thank you.","Let be a continuous real valued function from such that for all . Does there exist such a function? Plugging and respectively in the given equation we obtain and . By the intermediate value theorem, attains any value between and . Moreover, the range of is where (resp. ) is the minimum (resp. maximum) value of the function over . How to use these facts to decide whether such function exists or not? Please give some hint to proceed. Thank you.","f [0,1] f(x)+f(x^2)=x x\in [0,1] x=0 x=1 f(0)=0 f(1)=\frac{1}{2} f 0 1 f [m,M] m M [0,1]","['real-analysis', 'continuity', 'power-series', 'examples-counterexamples', 'functional-equations']"
41,Prove that $\sum\limits_{k=1}^{n}\frac{|z_{k}|^2}{|z_{k+1}-z_{k}|^2}\ge 1$ for complex numbers $z_k$'s,Prove that  for complex numbers 's,\sum\limits_{k=1}^{n}\frac{|z_{k}|^2}{|z_{k+1}-z_{k}|^2}\ge 1 z_k,"Let $n \geq 3$ . Let $z_{i}$ (for $i=1,2,\cdots,n$ ) be complex numbers. Assume that $z_{k+1} - z_k \ne 0$ for $k=1,2, \cdots, n-1$ and $z_n  - z_1 \ne 0$ . Show that $$\sum_{k=1}^{n}\dfrac{|z_{k}|^2}{|z_{k+1}-z_{k}|^2}\ge 1\tag{1},$$ where $z_{n+1}=z_{1}$ . I think can use the Cauchy-Schwarz inequality to solve it. But I think this complex inequality can't hold: $$\left(\sum_{k=1}^{n}|z_{k}|\right)^2\ge \sum_{k=1}^{n}|z_{k+1}-z_{k}|^2.$$ So how do I prove $(1)$ ?",Let . Let (for ) be complex numbers. Assume that for and . Show that where . I think can use the Cauchy-Schwarz inequality to solve it. But I think this complex inequality can't hold: So how do I prove ?,"n \geq 3 z_{i} i=1,2,\cdots,n z_{k+1} - z_k \ne 0 k=1,2, \cdots, n-1 z_n  - z_1 \ne 0 \sum_{k=1}^{n}\dfrac{|z_{k}|^2}{|z_{k+1}-z_{k}|^2}\ge 1\tag{1}, z_{n+1}=z_{1} \left(\sum_{k=1}^{n}|z_{k}|\right)^2\ge \sum_{k=1}^{n}|z_{k+1}-z_{k}|^2. (1)","['real-analysis', 'inequality']"
42,Show that $\limsup \frac{\epsilon_1+\cdots +\epsilon_n}{n}\geq 0$？,Show that ？,\limsup \frac{\epsilon_1+\cdots +\epsilon_n}{n}\geq 0,"Suppose $a_n$ is a sequence such that $a_n \downarrow 0$,   $\epsilon_n=-1$ or $1$ for all $n$, the series $\sum a_n$ diverges but the series $\sum_{n=1}^\infty \epsilon_n a_n$ converges. Is it true that $\limsup \frac{\epsilon_1+\cdots +\epsilon_n}{n}\geq 0$? Question history: Originally, this question is about the assertion $\limsup \frac{\epsilon_1+\cdots +\epsilon_n}{n}=0$, which has been proved wrong by some wonderful answers. Note $a_n$ is non-increasing to zero here . Some people forget this assumption.","Suppose $a_n$ is a sequence such that $a_n \downarrow 0$,   $\epsilon_n=-1$ or $1$ for all $n$, the series $\sum a_n$ diverges but the series $\sum_{n=1}^\infty \epsilon_n a_n$ converges. Is it true that $\limsup \frac{\epsilon_1+\cdots +\epsilon_n}{n}\geq 0$? Question history: Originally, this question is about the assertion $\limsup \frac{\epsilon_1+\cdots +\epsilon_n}{n}=0$, which has been proved wrong by some wonderful answers. Note $a_n$ is non-increasing to zero here . Some people forget this assumption.",,"['real-analysis', 'sequences-and-series', 'analysis']"
43,"Analytic ""Lagrange"" interpolation for a countably infinite set of points?","Analytic ""Lagrange"" interpolation for a countably infinite set of points?",,"Suppose I have a finite set of points on the real plane, and I want to find the univariate polynomial interpolating all of them. Lagrange interpolation gives me the least-degree polynomial going through all of those. Is there an analogous construct for a countably infinite, sparse set of points on the real plane, instead using analytic functions and power series? There is obviously some difficulty in forming a perfect analogy, as Lagrange interpolation yields the ""lowest degree"" polynomial interpolating the points, whereas there is no such thing as a ""lowest degree"" power series. However, perhaps there is some generalized measure of the complexity of a power series that is decently workable, and which restricts to the lowest-degree polynomial in the finite case. If so, how does this work? Is there an easy way to obtain the nth coefficient of the power series from the points?","Suppose I have a finite set of points on the real plane, and I want to find the univariate polynomial interpolating all of them. Lagrange interpolation gives me the least-degree polynomial going through all of those. Is there an analogous construct for a countably infinite, sparse set of points on the real plane, instead using analytic functions and power series? There is obviously some difficulty in forming a perfect analogy, as Lagrange interpolation yields the ""lowest degree"" polynomial interpolating the points, whereas there is no such thing as a ""lowest degree"" power series. However, perhaps there is some generalized measure of the complexity of a power series that is decently workable, and which restricts to the lowest-degree polynomial in the finite case. If so, how does this work? Is there an easy way to obtain the nth coefficient of the power series from the points?",,"['real-analysis', 'analytic-functions', 'lagrange-interpolation', 'interpolation-theory']"
44,Almost equivalent definitions of the Riemann–Stieltjes integral,Almost equivalent definitions of the Riemann–Stieltjes integral,,"Below, I will present two definitions of the Riemann–Stieltjes integral, the second of which is more general. My question concerns the relationship between these two definitions. Definition 1 : Let $f,g:[a,b] \to \mathbb{R}$. For a partition $P=\{x_0, x_1,x_2 \cdots x_{n-1},x_n\}$ of $[a,b]$, consider the sum $$S(P,f,g)  \stackrel{\rm def}{=} \sum_{i=0}^{n-1} f(c_i) \left[g(x_{i+1}) - g(x_{i})\right]$$ where we have ""sample points"" $c_i \in [x_i, x_{i+1}]$. $f$ is then said to be Riemann–Stieltjes integrable with respect to $g$ if there is a real number $L$ with the following property: for all $\epsilon>0$ there is a $\delta>0$ such that for any partition $P$ with $\text{sup}_{{0\leq i \leq n-1}}(x_{i+1} - x_i) < \delta$ and any sequence of points $\{c_i\}_{{0\leq i \leq n-1}, c_i \in [x_i, x_{i+1}]}$ we have $$\left|S(P,f,g) -  L\right| < \epsilon$$ Definition 2 : We modify the above definition so that it is like this instead: for all $\epsilon>0$ there is partition $P_{\epsilon}$ such that any refinement $P' \supset P_{\epsilon}$ satisfies $$\left|S(P',f,g) -  L\right| < \epsilon$$ independent of the sequence of points $\{c_i\}_{{0\leq i \leq n-1}, c_i \in [x_i, x_{i+1}]}$ we choose. Remark : The first definition implies the second. Simply let $P_{\epsilon}$ be any partition with $\text{sup}_{{0\leq i \leq n-1}}(x_{i+1} - x_i) < \delta$. However, interestingly, the second definition does not imply the first. Take $$g(x) = \begin{cases} 0  & x \in [0, \frac 12) \\ 1, & x \in [\frac 12, 1] \end{cases}$$ $$f(x) = \begin{cases} 0  & x \in [0, \frac 12] \\ 1, & x \in (\frac 12, 1] \end{cases}$$ as a counterexample. For this example, the integral exists and is equal to $0$ in the sense of the second definition by ensuring our chosen partition $P{_\epsilon}$ is such that $\frac 12 \in P_{\epsilon}$. This ensures $g(x_{i+1}) - g(x_i) = 0$ except in the interval $[x_k, \frac 12]$; however, this interval does not affect the sum since $f \equiv 0$ in $[x_k, \frac 12]$. Conversely, for the first definition we needn't have $\frac 12 \in P$. $\frac 12$ may be in the interior of some subinterval $[x_i, x_{i+1}]$ (ie., $x_i < \frac 12 < x_{i+1}$). This would mean that $g(x_{i+1}) - g(x_i) = 1$, and depending on the ""sample point"" $c_i$ we choose in this subinterval, the sum may be $1$ or $0$. This can happen regardless of how fine the partition is, and hence the integral does not exist. Problem: Are there any regularity conditions we can impose on $g$ to ensure the equivalence of the above definitions? Strict monotonicity is a natural example. If that doesn't work, consider stronger conditions (e.g., $g$ is homeomorphism onto its image, or a $C^{1}$ diffeomorphism).","Below, I will present two definitions of the Riemann–Stieltjes integral, the second of which is more general. My question concerns the relationship between these two definitions. Definition 1 : Let $f,g:[a,b] \to \mathbb{R}$. For a partition $P=\{x_0, x_1,x_2 \cdots x_{n-1},x_n\}$ of $[a,b]$, consider the sum $$S(P,f,g)  \stackrel{\rm def}{=} \sum_{i=0}^{n-1} f(c_i) \left[g(x_{i+1}) - g(x_{i})\right]$$ where we have ""sample points"" $c_i \in [x_i, x_{i+1}]$. $f$ is then said to be Riemann–Stieltjes integrable with respect to $g$ if there is a real number $L$ with the following property: for all $\epsilon>0$ there is a $\delta>0$ such that for any partition $P$ with $\text{sup}_{{0\leq i \leq n-1}}(x_{i+1} - x_i) < \delta$ and any sequence of points $\{c_i\}_{{0\leq i \leq n-1}, c_i \in [x_i, x_{i+1}]}$ we have $$\left|S(P,f,g) -  L\right| < \epsilon$$ Definition 2 : We modify the above definition so that it is like this instead: for all $\epsilon>0$ there is partition $P_{\epsilon}$ such that any refinement $P' \supset P_{\epsilon}$ satisfies $$\left|S(P',f,g) -  L\right| < \epsilon$$ independent of the sequence of points $\{c_i\}_{{0\leq i \leq n-1}, c_i \in [x_i, x_{i+1}]}$ we choose. Remark : The first definition implies the second. Simply let $P_{\epsilon}$ be any partition with $\text{sup}_{{0\leq i \leq n-1}}(x_{i+1} - x_i) < \delta$. However, interestingly, the second definition does not imply the first. Take $$g(x) = \begin{cases} 0  & x \in [0, \frac 12) \\ 1, & x \in [\frac 12, 1] \end{cases}$$ $$f(x) = \begin{cases} 0  & x \in [0, \frac 12] \\ 1, & x \in (\frac 12, 1] \end{cases}$$ as a counterexample. For this example, the integral exists and is equal to $0$ in the sense of the second definition by ensuring our chosen partition $P{_\epsilon}$ is such that $\frac 12 \in P_{\epsilon}$. This ensures $g(x_{i+1}) - g(x_i) = 0$ except in the interval $[x_k, \frac 12]$; however, this interval does not affect the sum since $f \equiv 0$ in $[x_k, \frac 12]$. Conversely, for the first definition we needn't have $\frac 12 \in P$. $\frac 12$ may be in the interior of some subinterval $[x_i, x_{i+1}]$ (ie., $x_i < \frac 12 < x_{i+1}$). This would mean that $g(x_{i+1}) - g(x_i) = 1$, and depending on the ""sample point"" $c_i$ we choose in this subinterval, the sum may be $1$ or $0$. This can happen regardless of how fine the partition is, and hence the integral does not exist. Problem: Are there any regularity conditions we can impose on $g$ to ensure the equivalence of the above definitions? Strict monotonicity is a natural example. If that doesn't work, consider stronger conditions (e.g., $g$ is homeomorphism onto its image, or a $C^{1}$ diffeomorphism).",,"['real-analysis', 'integration', 'riemann-integration', 'stieltjes-integral']"
45,"Theorem 6.10 in Baby Rudin: If $f$ is bounded on $[a, b]$ with only finitely many points of discontinuity at which $\alpha$ is continuous, then","Theorem 6.10 in Baby Rudin: If  is bounded on  with only finitely many points of discontinuity at which  is continuous, then","f [a, b] \alpha","Here is Theorem 6.10 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is bounded on $[a, b]$, $f$ has only finitely many points of discontinuity on $[a, b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$. Here $\alpha$ is a monotonically increasing function, and by $f \in \mathscr{R}(\alpha)$ we mean the integral $\int_a^b f(x) \mathrm{d} \alpha(x)$ exists. First of all, here are Definitions 6.1 and 6.2 in Baby Rudin, 3rd edition: Definition 6.1: Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we mean a finite set of points $x_0, x_1, \ldots, x_n$, where    $$ a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b.$$   We write    $$ \Delta x_i = x_i - x_{i-1} \qquad (i = 1, \ldots, n). $$    Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put   $$ \begin{align}  M_i &= \sup f(x) \qquad (x_{i-1} \leq x \leq x_i), \\ m_i &= \inf f(x) \qquad (x_{i-1} \leq x \leq x_i), \\ U(P, f) &= \sum_{i=1}^n M_i \Delta x_i, \\ L(P, f) &= \sum_{i=1}^n m_i \Delta x_i, \end{align}  $$   and finally    $$  \begin{align} \tag{1} \overline{\int_a^b} f dx &= \inf U(P, f), \\ \tag{2} \underline{\int_a^b} f dx &= \sup L(P, f),\\\, \end{align} $$   where the $\inf$ and the $\sup$ are taken over all partitions $P$ of $[a, b]$. The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively. If the upper and lower integrals are equal, we say that $f$ is Riemann-integrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemann-integrable functions), and we denote the common value of (1) and (2) by    $$ \tag{3} \int_a^b f dx, $$   or by    $$ \tag{4} \int_a^b f(x) dx. $$   This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that    $$ m \leq f(x) \leq M \qquad (a \leq x \leq b). $$   Hence, for every $P$,    $$ m(b-a) \leq L(P, f) \leq U(P, f) \leq M (b-a), $$   so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. . . . Definition 6.2: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write    $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$   It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put    $$  \begin{align} U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\ L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i,  \end{align} $$   where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define    $$ \begin{align} \tag{5} \overline{\int_a^b} f d \alpha = \inf U(P, f, \alpha), \\ \tag{6} \underline{\int_a^b} f d \alpha = \sup L(P, f, \alpha), \\\, \end{align} $$   the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by    $$ \tag{7} \int_a^b f d \alpha $$   or sometimes by    $$ \tag{8} \int_a^b f(x) d \alpha(x). $$   This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Put $M = \sup \left\vert f(x) \right\vert$, let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $\left[ u_j, v_j \right] \subset [a, b]$ such that the sum of the corresponding differences $\alpha\left(v_j\right) - \alpha \left( u_j \right)$ is less than $\varepsilon$. Furthermore, we can place these intervals in such a way that every point of $E \cap (a, b)$ lies in the interior of some $\left[ u_j, v_j \right]$. Remove the segments $\left( u_j, v_j \right)$ from $[a, b]$. The remaining set $K$ is compact. Hence $f$ is uniformly continuous on $K$, and there exists $\delta > 0$ such that $\left\vert f(s) - f(t) \right\vert < \varepsilon$ if $s \in K$, $t \in K$, $\left\vert s-t \right\vert < \delta$. Now form a partition $P = \left\{ x_0, x_1, \ldots, x_n \right\}$ of $[a, b]$, as follows: Each $u_j$ occurs in $P$. Each $v_j$ occurs in $P$. No point of any segment $\left( u_j, v_j \right)$ occurs in $P$. If $x_{i-1}$ is not one of the $u_j$, then $\Delta \alpha_i < \delta$. Note that $M_i - m_i \leq 2M$ for every $i$, and that $M_i - m_i \leq \varepsilon$ unless $x_{i-1}$ is one of the $u_j$. Hence, as in the proof of Theorem 6.8,    $$ U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon.$$   Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}(\alpha)$. Here is Theorem 6.8 in Baby Rudin, 3rd edition: If $f$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$ on $[a, b]$. And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Choose $\eta > 0$ so that    $$ \left[ \alpha(b) - \alpha(a) \right] \eta < \varepsilon.$$   Since $f$ is uniformly continuous on $[a, b]$ (Theorem 4.19), there exists a $\delta > 0$ such that    $$ \vert f(x) - f(t) \vert < \eta \tag{16}$$   if $x \in [a, b]$, $t \in [a, b]$, and $\vert x-t \vert < \delta$. If $P$ is any partition of $[a, b]$ such that $\Delta x_i < \delta$ for all $i$, then (16) implies that    $$ M_i - m_i \leq \eta \qquad (i = 1, \ldots, n) \tag{17} $$   and therefore    $$ U(P, f, \alpha) - L(P, f, \alpha) = \sum_{i=1}^n \left( M_i - m_i \right) \Delta \alpha_i \leq \eta \sum_{i=1}^n \Delta \alpha_i = \eta \left[ \alpha(b) - \alpha(a) \right] < \varepsilon. $$   By Theorem 6.6, $f \in \mathscr{R}(\alpha)$. Here is Theorem 6.6 in Baby Rudin, 3rd edition: $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ such that    $$ U(P, f, \alpha) - L(P, f, \alpha) < \varepsilon.$$ Finally, here is Theorem 4.19 in Baby Rudin, 3rd edition: Let $f$ be a continuous mapping of a compact metric space $X$ into a metric space $Y$. Then $f$ is uniformly continuous on $X$. Now I have the following questions: Can we make Rudin's proof of Theorem 6.10 more explicit and rigorous (perhaps by modifying its presentation in some way)? And, is there any alternative proof of this very theorem (preferably using the same machinary that Rudin has developed so far in the book)?","Here is Theorem 6.10 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is bounded on $[a, b]$, $f$ has only finitely many points of discontinuity on $[a, b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$. Here $\alpha$ is a monotonically increasing function, and by $f \in \mathscr{R}(\alpha)$ we mean the integral $\int_a^b f(x) \mathrm{d} \alpha(x)$ exists. First of all, here are Definitions 6.1 and 6.2 in Baby Rudin, 3rd edition: Definition 6.1: Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we mean a finite set of points $x_0, x_1, \ldots, x_n$, where    $$ a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b.$$   We write    $$ \Delta x_i = x_i - x_{i-1} \qquad (i = 1, \ldots, n). $$    Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put   $$ \begin{align}  M_i &= \sup f(x) \qquad (x_{i-1} \leq x \leq x_i), \\ m_i &= \inf f(x) \qquad (x_{i-1} \leq x \leq x_i), \\ U(P, f) &= \sum_{i=1}^n M_i \Delta x_i, \\ L(P, f) &= \sum_{i=1}^n m_i \Delta x_i, \end{align}  $$   and finally    $$  \begin{align} \tag{1} \overline{\int_a^b} f dx &= \inf U(P, f), \\ \tag{2} \underline{\int_a^b} f dx &= \sup L(P, f),\\\, \end{align} $$   where the $\inf$ and the $\sup$ are taken over all partitions $P$ of $[a, b]$. The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively. If the upper and lower integrals are equal, we say that $f$ is Riemann-integrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemann-integrable functions), and we denote the common value of (1) and (2) by    $$ \tag{3} \int_a^b f dx, $$   or by    $$ \tag{4} \int_a^b f(x) dx. $$   This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that    $$ m \leq f(x) \leq M \qquad (a \leq x \leq b). $$   Hence, for every $P$,    $$ m(b-a) \leq L(P, f) \leq U(P, f) \leq M (b-a), $$   so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. . . . Definition 6.2: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write    $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$   It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put    $$  \begin{align} U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\ L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i,  \end{align} $$   where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define    $$ \begin{align} \tag{5} \overline{\int_a^b} f d \alpha = \inf U(P, f, \alpha), \\ \tag{6} \underline{\int_a^b} f d \alpha = \sup L(P, f, \alpha), \\\, \end{align} $$   the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by    $$ \tag{7} \int_a^b f d \alpha $$   or sometimes by    $$ \tag{8} \int_a^b f(x) d \alpha(x). $$   This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Put $M = \sup \left\vert f(x) \right\vert$, let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $\left[ u_j, v_j \right] \subset [a, b]$ such that the sum of the corresponding differences $\alpha\left(v_j\right) - \alpha \left( u_j \right)$ is less than $\varepsilon$. Furthermore, we can place these intervals in such a way that every point of $E \cap (a, b)$ lies in the interior of some $\left[ u_j, v_j \right]$. Remove the segments $\left( u_j, v_j \right)$ from $[a, b]$. The remaining set $K$ is compact. Hence $f$ is uniformly continuous on $K$, and there exists $\delta > 0$ such that $\left\vert f(s) - f(t) \right\vert < \varepsilon$ if $s \in K$, $t \in K$, $\left\vert s-t \right\vert < \delta$. Now form a partition $P = \left\{ x_0, x_1, \ldots, x_n \right\}$ of $[a, b]$, as follows: Each $u_j$ occurs in $P$. Each $v_j$ occurs in $P$. No point of any segment $\left( u_j, v_j \right)$ occurs in $P$. If $x_{i-1}$ is not one of the $u_j$, then $\Delta \alpha_i < \delta$. Note that $M_i - m_i \leq 2M$ for every $i$, and that $M_i - m_i \leq \varepsilon$ unless $x_{i-1}$ is one of the $u_j$. Hence, as in the proof of Theorem 6.8,    $$ U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon.$$   Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}(\alpha)$. Here is Theorem 6.8 in Baby Rudin, 3rd edition: If $f$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$ on $[a, b]$. And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Choose $\eta > 0$ so that    $$ \left[ \alpha(b) - \alpha(a) \right] \eta < \varepsilon.$$   Since $f$ is uniformly continuous on $[a, b]$ (Theorem 4.19), there exists a $\delta > 0$ such that    $$ \vert f(x) - f(t) \vert < \eta \tag{16}$$   if $x \in [a, b]$, $t \in [a, b]$, and $\vert x-t \vert < \delta$. If $P$ is any partition of $[a, b]$ such that $\Delta x_i < \delta$ for all $i$, then (16) implies that    $$ M_i - m_i \leq \eta \qquad (i = 1, \ldots, n) \tag{17} $$   and therefore    $$ U(P, f, \alpha) - L(P, f, \alpha) = \sum_{i=1}^n \left( M_i - m_i \right) \Delta \alpha_i \leq \eta \sum_{i=1}^n \Delta \alpha_i = \eta \left[ \alpha(b) - \alpha(a) \right] < \varepsilon. $$   By Theorem 6.6, $f \in \mathscr{R}(\alpha)$. Here is Theorem 6.6 in Baby Rudin, 3rd edition: $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ such that    $$ U(P, f, \alpha) - L(P, f, \alpha) < \varepsilon.$$ Finally, here is Theorem 4.19 in Baby Rudin, 3rd edition: Let $f$ be a continuous mapping of a compact metric space $X$ into a metric space $Y$. Then $f$ is uniformly continuous on $X$. Now I have the following questions: Can we make Rudin's proof of Theorem 6.10 more explicit and rigorous (perhaps by modifying its presentation in some way)? And, is there any alternative proof of this very theorem (preferably using the same machinary that Rudin has developed so far in the book)?",,"['real-analysis', 'integration', 'analysis', 'definite-integrals', 'riemann-integration']"
46,Everywhere Super Dense Subset of $\mathbb{R}$,Everywhere Super Dense Subset of,\mathbb{R},"First, this question is motivated by the imprecise question: Is there a sensible notion of parity (evenness and oddness) for real numbers? Here are some properties a notion of parity for $\mathbb{R}$ should have: It should be an equivalence relation on $\mathbb{R}$ with exactly two equivalence classes. Each equivalence class should be dense in $\mathbb{R}$. There should be some kind of symmetry between the two equivalence classes. (This is intentionally imprecise.) If we divide $\mathbb{R}$ into rationals and irrationals, this lacks symmetry, since the rationals have measure zero and are countable, while the rationals have infinite measure and are uncountable. So we could require that each equivalence class be uncountable and/or have positive (or infinite) measure. Then take one equivalence class to be the rationals, union with countably many fat cantor sets with positive measure, and the other class its complement. Then both classes are now dense, uncountable, and have infinite measure. But again, symmetry is lacking. One of these classes is ""uncountable everywhere"" and has ""positive measure everywhere"" while the other does not. I propose some definitions: A subset $A \subset \mathbb{R}$ is uncountable everywhere if for any open interval $(a,b)$, the intersection $A \cap (a,b)$ is uncountable. A subset $A \subset \mathbb{R}$ has positive measure everywhere if for every open interval $(a,b)$, the intersection $A \cap (a,b)$ has positive measure. We can see immediately that having positive measure everywhere implies being uncountable everywhere, since a positive measure set must be uncountable. Finally, my questions: Is there a partition of $\mathbb{R}$ into two sets that are both uncountable everywhere? Is there a partition of $\mathbb{R}$ into two sets that both have positive measure everywhere? Is there a partition of $\mathbb{R}$ into two sets that split each interval $(a,b)$ into two parts of equal measure? Potential generalizations for extra credit: What about partitions of $\mathbb{R}$ with $n$ equivalence classes, where $n \in \mathbb{N}$, or even with countably many, or even uncountably many equivalence classes? Note: If you've seen these definitions of uncountable everywhere or positive measure everywhere somewhere under a different name, please let me know. I've never found anything where other people were thinking about these notions. EDIT: By ""measure,"" I mean Lebesgue outer measure, so we don't have to worry about anything being measurable.","First, this question is motivated by the imprecise question: Is there a sensible notion of parity (evenness and oddness) for real numbers? Here are some properties a notion of parity for $\mathbb{R}$ should have: It should be an equivalence relation on $\mathbb{R}$ with exactly two equivalence classes. Each equivalence class should be dense in $\mathbb{R}$. There should be some kind of symmetry between the two equivalence classes. (This is intentionally imprecise.) If we divide $\mathbb{R}$ into rationals and irrationals, this lacks symmetry, since the rationals have measure zero and are countable, while the rationals have infinite measure and are uncountable. So we could require that each equivalence class be uncountable and/or have positive (or infinite) measure. Then take one equivalence class to be the rationals, union with countably many fat cantor sets with positive measure, and the other class its complement. Then both classes are now dense, uncountable, and have infinite measure. But again, symmetry is lacking. One of these classes is ""uncountable everywhere"" and has ""positive measure everywhere"" while the other does not. I propose some definitions: A subset $A \subset \mathbb{R}$ is uncountable everywhere if for any open interval $(a,b)$, the intersection $A \cap (a,b)$ is uncountable. A subset $A \subset \mathbb{R}$ has positive measure everywhere if for every open interval $(a,b)$, the intersection $A \cap (a,b)$ has positive measure. We can see immediately that having positive measure everywhere implies being uncountable everywhere, since a positive measure set must be uncountable. Finally, my questions: Is there a partition of $\mathbb{R}$ into two sets that are both uncountable everywhere? Is there a partition of $\mathbb{R}$ into two sets that both have positive measure everywhere? Is there a partition of $\mathbb{R}$ into two sets that split each interval $(a,b)$ into two parts of equal measure? Potential generalizations for extra credit: What about partitions of $\mathbb{R}$ with $n$ equivalence classes, where $n \in \mathbb{N}$, or even with countably many, or even uncountably many equivalence classes? Note: If you've seen these definitions of uncountable everywhere or positive measure everywhere somewhere under a different name, please let me know. I've never found anything where other people were thinking about these notions. EDIT: By ""measure,"" I mean Lebesgue outer measure, so we don't have to worry about anything being measurable.",,['real-analysis']
47,Can we always extend a Holder-boundary continuous function to whole domain?,Can we always extend a Holder-boundary continuous function to whole domain?,,"Let $\Omega\subseteq\mathbb{R}^{n}$ be a smooth domain, and let $f\in C^{\alpha}\left(\partial\Omega\right),$ where $\alpha\in\left(0,1\right).$ Do we always have that there exists a function $\widetilde{f}\in C^{\alpha}\left(\overline{\Omega}\right)$ so that $\left.\widetilde{f}\right|_{\partial\Omega}\equiv f?$ Note that, if $\alpha>1$ the result is true and can be found in the book by Gilbarg + Trundinger (Lemma 6.38, p 137).","Let $\Omega\subseteq\mathbb{R}^{n}$ be a smooth domain, and let $f\in C^{\alpha}\left(\partial\Omega\right),$ where $\alpha\in\left(0,1\right).$ Do we always have that there exists a function $\widetilde{f}\in C^{\alpha}\left(\overline{\Omega}\right)$ so that $\left.\widetilde{f}\right|_{\partial\Omega}\equiv f?$ Note that, if $\alpha>1$ the result is true and can be found in the book by Gilbarg + Trundinger (Lemma 6.38, p 137).",,"['real-analysis', 'functional-analysis', 'differential-geometry', 'geometric-measure-theory']"
48,Is the following Harmonic Number Identity true?,Is the following Harmonic Number Identity true?,,"Is the following identity true? $$ \sum_{n=1}^\infty \frac{H_nx^n}{n^3} = \frac12\zeta(3)\ln x-\frac18\ln^2x\ln^2(1-x)+\frac12\ln x\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^2}-\operatorname{Li}_3(x)\right] + \operatorname{Li}_4(x)-\frac{\pi^2}{12}\operatorname{Li}_2(x)-\frac12\operatorname{Li}_3(1-x)\ln x+\frac{\pi^4}{60}$$ In this accepted answer , @Tunk Fey proved the above. (See $(4)$). However, I have the following $3$ queries : Why can we add the integrals after the substitution $x \mapsto 1-x$ in the following step? I doubt it since $\int f(x) \ \mathrm{d}x \neq \int f(1-x) \ \mathrm{d}x$ in general. Why do we omit the constant of integration in the following step? We should add the constant since it will affect the summation. $$\begin{align} \color{red}{\int\frac{\ln x\ln^2(1-x)}{x}\ dx}&=-\int\frac{\ln (1-x)\ln^2 x}{1-x}\ dx\\ &=\int\sum_{n=1}^\infty H_n x^n\ln^2x\ dx\\ &=\sum_{n=1}^\infty H_n \int x^n\ln^2x\ dx\\ &=\sum_{n=1}^\infty H_n \frac{\partial^2}{\partial n^2}\left[\int x^n\ dx\right]\\ &=\sum_{n=1}^\infty H_n \frac{\partial^2}{\partial n^2}\left[\frac {x^{n+1}}{n+1}\right]\\ &=\sum_{n=1}^\infty H_n \left[\frac{x^{n+1}\ln^2x}{n+1}-2\frac{x^{n+1}\ln x}{(n+1)^2}+2\frac{x^{n+1}}{(n+1)^3}\right]\\ &=\ln^2x\sum_{n=1}^\infty\frac{H_n x^{n+1}}{n+1}-2\ln x\sum_{n=1}^\infty\frac{H_n x^{n+1}}{(n+1)^2}+2\sum_{n=1}^\infty\frac{H_n x^{n+1}}{(n+1)^3}\\ &=\frac12\ln^2x\ln^2(1-x)-2\ln x\left[\sum_{n=1}^\infty\frac{H_{n+1} x^{n+1}}{(n+1)^2}-\sum_{n=1}^\infty\frac{x^{n+1}}{(n+1)^3}\right]\\&+2\left[\sum_{n=1}^\infty\frac{H_{n+1} x^{n+1}}{(n+1)^3}-\sum_{n=1}^\infty\frac{x^{n+1}}{(n+1)^4}\right]\\ &=\frac12\ln^2x\ln^2(1-x)-2\ln x\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^2}-\sum_{n=1}^\infty\frac{x^{n}}{n^3}\right]\\&+2\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^3}-\sum_{n=1}^\infty\frac{x^{n}}{n^4}\right]\\ &=\frac12\ln^2x\ln^2(1-x)-2\ln x\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^2}-\operatorname{Li}_3(x)\right]\\&+2\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^3}-\operatorname{Li}_4(x)\right]. \end{align}$$ Is the identity even true, since putting $x=\dfrac{1}{2}$ gives a numerically different result than the correct result , as pointed out by the user @Super Abound in the comments of that answer. Please help.","Is the following identity true? $$ \sum_{n=1}^\infty \frac{H_nx^n}{n^3} = \frac12\zeta(3)\ln x-\frac18\ln^2x\ln^2(1-x)+\frac12\ln x\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^2}-\operatorname{Li}_3(x)\right] + \operatorname{Li}_4(x)-\frac{\pi^2}{12}\operatorname{Li}_2(x)-\frac12\operatorname{Li}_3(1-x)\ln x+\frac{\pi^4}{60}$$ In this accepted answer , @Tunk Fey proved the above. (See $(4)$). However, I have the following $3$ queries : Why can we add the integrals after the substitution $x \mapsto 1-x$ in the following step? I doubt it since $\int f(x) \ \mathrm{d}x \neq \int f(1-x) \ \mathrm{d}x$ in general. Why do we omit the constant of integration in the following step? We should add the constant since it will affect the summation. $$\begin{align} \color{red}{\int\frac{\ln x\ln^2(1-x)}{x}\ dx}&=-\int\frac{\ln (1-x)\ln^2 x}{1-x}\ dx\\ &=\int\sum_{n=1}^\infty H_n x^n\ln^2x\ dx\\ &=\sum_{n=1}^\infty H_n \int x^n\ln^2x\ dx\\ &=\sum_{n=1}^\infty H_n \frac{\partial^2}{\partial n^2}\left[\int x^n\ dx\right]\\ &=\sum_{n=1}^\infty H_n \frac{\partial^2}{\partial n^2}\left[\frac {x^{n+1}}{n+1}\right]\\ &=\sum_{n=1}^\infty H_n \left[\frac{x^{n+1}\ln^2x}{n+1}-2\frac{x^{n+1}\ln x}{(n+1)^2}+2\frac{x^{n+1}}{(n+1)^3}\right]\\ &=\ln^2x\sum_{n=1}^\infty\frac{H_n x^{n+1}}{n+1}-2\ln x\sum_{n=1}^\infty\frac{H_n x^{n+1}}{(n+1)^2}+2\sum_{n=1}^\infty\frac{H_n x^{n+1}}{(n+1)^3}\\ &=\frac12\ln^2x\ln^2(1-x)-2\ln x\left[\sum_{n=1}^\infty\frac{H_{n+1} x^{n+1}}{(n+1)^2}-\sum_{n=1}^\infty\frac{x^{n+1}}{(n+1)^3}\right]\\&+2\left[\sum_{n=1}^\infty\frac{H_{n+1} x^{n+1}}{(n+1)^3}-\sum_{n=1}^\infty\frac{x^{n+1}}{(n+1)^4}\right]\\ &=\frac12\ln^2x\ln^2(1-x)-2\ln x\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^2}-\sum_{n=1}^\infty\frac{x^{n}}{n^3}\right]\\&+2\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^3}-\sum_{n=1}^\infty\frac{x^{n}}{n^4}\right]\\ &=\frac12\ln^2x\ln^2(1-x)-2\ln x\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^2}-\operatorname{Li}_3(x)\right]\\&+2\left[\sum_{n=1}^\infty\frac{H_{n} x^{n}}{n^3}-\operatorname{Li}_4(x)\right]. \end{align}$$ Is the identity even true, since putting $x=\dfrac{1}{2}$ gives a numerically different result than the correct result , as pointed out by the user @Super Abound in the comments of that answer. Please help.",,"['calculus', 'real-analysis', 'sequences-and-series', 'harmonic-numbers', 'polylogarithm']"
49,Find the value of sum (n/2^n) [duplicate],Find the value of sum (n/2^n) [duplicate],,"This question already has answers here : What does $\sum_{k=0}^\infty \frac{k}{2^k}$ converge to? (3 answers) Closed 7 years ago . I have the series $\sum_{n=0}^\infty \frac{n}{2^n}$. I must show that it converges to 2. I was given a hint to take the derivative of $\sum_{n=0}^\infty x^n$ and multiply by $x$ , which gives $\sum_{n=1}^\infty nx^n$ , or $\sum_{n=0}^\infty nx^n$. Clearly if I take $x=\frac{1}{2}$ , the series is  $\sum_{n=0}^\infty \frac{n}{2^n}$. How do I proceed from here?","This question already has answers here : What does $\sum_{k=0}^\infty \frac{k}{2^k}$ converge to? (3 answers) Closed 7 years ago . I have the series $\sum_{n=0}^\infty \frac{n}{2^n}$. I must show that it converges to 2. I was given a hint to take the derivative of $\sum_{n=0}^\infty x^n$ and multiply by $x$ , which gives $\sum_{n=1}^\infty nx^n$ , or $\sum_{n=0}^\infty nx^n$. Clearly if I take $x=\frac{1}{2}$ , the series is  $\sum_{n=0}^\infty \frac{n}{2^n}$. How do I proceed from here?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
50,The Topology Associated to Convergence in Measure,The Topology Associated to Convergence in Measure,,"I'm attending a course on measure theory this semester. While proposing different kinds of convergence (in measure, almost everywhere and in $L^{p}$), our professor stressed (and proved) the fact that convergence almost everywhere is not topological, but claimed that convergence in measure is. As pointed out in a few questions on this site and wikipedia ( 1 , 2 , 3 ), in the case where $(\Omega, \mathcal{F}, \mu)$ is a finite measure space, convergence in measure can be described by a pseudometric (hence a topology). However, I haven't found an answer to why at least a topology should exist in the case where $\mu$ is an arbitrary measure. Wikipedia ( 3 ) claims that their pseudometric works for arbitrary measures, but their proposed function can take $\infty$ as a value, which I believe isn't allowed for metrics. To sum up: let $(\Omega, \mathcal{F}, \mu)$ be a (not necessarily finite) measure space, does there exist a topology $\mathcal{T}$ on the set of measurable functions $f : \Omega \to \mathbb{R}$ such that a sequence of measurable functions $(f_{n})_{n}$ converges to a measurable function $f$ in measure if and only if it converges to $f$ in the topology $\mathcal{T}$? Extra: Is this topology unique? Thank you for your help! I've had introductory courses in topology (metric spaces), Banach (Hilbert) spaces and now measure theory.","I'm attending a course on measure theory this semester. While proposing different kinds of convergence (in measure, almost everywhere and in $L^{p}$), our professor stressed (and proved) the fact that convergence almost everywhere is not topological, but claimed that convergence in measure is. As pointed out in a few questions on this site and wikipedia ( 1 , 2 , 3 ), in the case where $(\Omega, \mathcal{F}, \mu)$ is a finite measure space, convergence in measure can be described by a pseudometric (hence a topology). However, I haven't found an answer to why at least a topology should exist in the case where $\mu$ is an arbitrary measure. Wikipedia ( 3 ) claims that their pseudometric works for arbitrary measures, but their proposed function can take $\infty$ as a value, which I believe isn't allowed for metrics. To sum up: let $(\Omega, \mathcal{F}, \mu)$ be a (not necessarily finite) measure space, does there exist a topology $\mathcal{T}$ on the set of measurable functions $f : \Omega \to \mathbb{R}$ such that a sequence of measurable functions $(f_{n})_{n}$ converges to a measurable function $f$ in measure if and only if it converges to $f$ in the topology $\mathcal{T}$? Extra: Is this topology unique? Thank you for your help! I've had introductory courses in topology (metric spaces), Banach (Hilbert) spaces and now measure theory.",,"['real-analysis', 'general-topology', 'measure-theory']"
51,Why do we give $C_c^\infty(\mathbb{R}^d)$ the topology induced by all good seminorms?,Why do we give  the topology induced by all good seminorms?,C_c^\infty(\mathbb{R}^d),"Briefly, my question boils down to the following: What benefits do we gain from considering the space of test functions in the topology induced by all ""good"" seminorms, as opposed to other topologies which are more common or more easily described? More precisely: So far as I understand, the topology on the space $C_c^\infty(\mathbb{R}^d)$ of test functions is usually constructed as follows: For each compact $K \subset \mathbb{R}^d$ we define the ""smooth topology"" on $C_c^\infty(K)$ to be the topology generated by the family of seminorms $\{\|\cdot\|_{C^k(K)}\}_{k \in \mathbb{N}}$, where $\|\cdot\|_{C^k(\mathbb{R}^d)}$ is defined by $$ \| f \|_{C^k(K)} = \sup_{x \in \mathbb{R}^d} \left\{ \sum_{j=0}^k \left|\nabla^j f(x)\right| \right\} $$ We then define a seminorm $\| \cdot \|$ on $C_c^\infty(\mathbb{R}^d)$ to be ""good"" if, for each compact $K \subset \mathbb{R}^d$, it is continuous function on $C_c^\infty(K)$ with respect to the smooth topology. Finally, we topologise $C_c^\infty(\mathbb{R}^d)$ by giving it the initial topology induced by the family $\mathscr{G}$ of all good seminorms. (The construction above is the one given by Prof. Tao here on his blog. Any mistakes made in the reproduction above are mine.) What I would like to know is: How is the topology induced by the ""good"" seminorms different from the one induced by the family $\mathscr{F} = \left\{ \|\cdot\|_{C_c^k(\mathbb{R}^d)} \right\}_{k \in \mathbb{N}}$. I believe we have the inclusion $\mathscr{F} \subset \mathscr{G}$, and I can't really tell what we gain by considering the other good seminorms. In fact, I'm not really sure what the other good seminorms look like. It appears we can obtain some other good seminorms by considering functionals of the form $$  \eta(f) = \int_{\mathbb{R}^d} |f(x)g(x)| ~dm(x) $$ for suitable functions $g$. But again, I'm not sure what these contribute. If the topology induced by $\mathscr{F}$ is different from the one induced by $\mathscr{G}$, why do we prefer the latter? Any insight will be much appreciated.","Briefly, my question boils down to the following: What benefits do we gain from considering the space of test functions in the topology induced by all ""good"" seminorms, as opposed to other topologies which are more common or more easily described? More precisely: So far as I understand, the topology on the space $C_c^\infty(\mathbb{R}^d)$ of test functions is usually constructed as follows: For each compact $K \subset \mathbb{R}^d$ we define the ""smooth topology"" on $C_c^\infty(K)$ to be the topology generated by the family of seminorms $\{\|\cdot\|_{C^k(K)}\}_{k \in \mathbb{N}}$, where $\|\cdot\|_{C^k(\mathbb{R}^d)}$ is defined by $$ \| f \|_{C^k(K)} = \sup_{x \in \mathbb{R}^d} \left\{ \sum_{j=0}^k \left|\nabla^j f(x)\right| \right\} $$ We then define a seminorm $\| \cdot \|$ on $C_c^\infty(\mathbb{R}^d)$ to be ""good"" if, for each compact $K \subset \mathbb{R}^d$, it is continuous function on $C_c^\infty(K)$ with respect to the smooth topology. Finally, we topologise $C_c^\infty(\mathbb{R}^d)$ by giving it the initial topology induced by the family $\mathscr{G}$ of all good seminorms. (The construction above is the one given by Prof. Tao here on his blog. Any mistakes made in the reproduction above are mine.) What I would like to know is: How is the topology induced by the ""good"" seminorms different from the one induced by the family $\mathscr{F} = \left\{ \|\cdot\|_{C_c^k(\mathbb{R}^d)} \right\}_{k \in \mathbb{N}}$. I believe we have the inclusion $\mathscr{F} \subset \mathscr{G}$, and I can't really tell what we gain by considering the other good seminorms. In fact, I'm not really sure what the other good seminorms look like. It appears we can obtain some other good seminorms by considering functionals of the form $$  \eta(f) = \int_{\mathbb{R}^d} |f(x)g(x)| ~dm(x) $$ for suitable functions $g$. But again, I'm not sure what these contribute. If the topology induced by $\mathscr{F}$ is different from the one induced by $\mathscr{G}$, why do we prefer the latter? Any insight will be much appreciated.",,"['real-analysis', 'general-topology', 'functional-analysis', 'distribution-theory', 'topological-vector-spaces']"
52,Chebyshev sets in finite dimension are closed and convex,Chebyshev sets in finite dimension are closed and convex,,"Prove a finite-dimensional converse to the “best approximation theorem”: Let $K$ be a subset of a finite-dimensional Hilbert space $H$ which satisfies the following property: for each $x \in H$ there exists a unique point $y \in K$ such that $d(x, y) = d(x, K)$ (sets with this property are known as Chebyshev sets). Then the set $K$ is closed and convex. It would useful to point why this result is for finite dimensional spaces only.","Prove a finite-dimensional converse to the “best approximation theorem”: Let $K$ be a subset of a finite-dimensional Hilbert space $H$ which satisfies the following property: for each $x \in H$ there exists a unique point $y \in K$ such that $d(x, y) = d(x, K)$ (sets with this property are known as Chebyshev sets). Then the set $K$ is closed and convex. It would useful to point why this result is for finite dimensional spaces only.",,"['real-analysis', 'functional-analysis', 'convex-analysis']"
53,Convergence of a sequence of functions and their inverses,Convergence of a sequence of functions and their inverses,,"consider a sequence of continuous and bijective functions $f_n:\mathbb{R}\rightarrow\mathbb{R}$, such that their inverses $f^{-1}_n:\mathbb{R}\rightarrow\mathbb{R}$ are continuous as well. Furthermore let us assume the function $f(x):=\lim\limits_{n\rightarrow \infty}f_n(x)$ is continuous, bijective and his inverse $f^{-1}$ is continuous. I want to prove or disprove that in this case the following relation holds: $$f^{-1}(x):=\lim\limits_{n\rightarrow \infty}f_n^{-1}(x)$$ By now I couldn't prove the statement, but there are a couple of examples which seems to confirm this claim. Do you have any idea? Best regards","consider a sequence of continuous and bijective functions $f_n:\mathbb{R}\rightarrow\mathbb{R}$, such that their inverses $f^{-1}_n:\mathbb{R}\rightarrow\mathbb{R}$ are continuous as well. Furthermore let us assume the function $f(x):=\lim\limits_{n\rightarrow \infty}f_n(x)$ is continuous, bijective and his inverse $f^{-1}$ is continuous. I want to prove or disprove that in this case the following relation holds: $$f^{-1}(x):=\lim\limits_{n\rightarrow \infty}f_n^{-1}(x)$$ By now I couldn't prove the statement, but there are a couple of examples which seems to confirm this claim. Do you have any idea? Best regards",,['real-analysis']
54,Is the symmetric definition of the derivative equivalent?,Is the symmetric definition of the derivative equivalent?,,"Is the symmetric definition of the derivative (below) equivalent to the usual one? \begin{equation} \lim_{h\to0}\frac{f(x+h)-f(x-h)}{2h} \end{equation} I've seen it used before in my computational physics class. I assumed it was equivalent but it seems like it wouldn't matter if there were a hole at $x=h$ in the symmetric derivative, whereas with the usual one it wouldn't be defined. Which is kinda interesting... If they're not equivalent - is there a good reason as to why we should use the common one? Or is the symmetric one actually more useful in some sense because it ""doesn't care"" about holes?","Is the symmetric definition of the derivative (below) equivalent to the usual one? \begin{equation} \lim_{h\to0}\frac{f(x+h)-f(x-h)}{2h} \end{equation} I've seen it used before in my computational physics class. I assumed it was equivalent but it seems like it wouldn't matter if there were a hole at $x=h$ in the symmetric derivative, whereas with the usual one it wouldn't be defined. Which is kinda interesting... If they're not equivalent - is there a good reason as to why we should use the common one? Or is the symmetric one actually more useful in some sense because it ""doesn't care"" about holes?",,"['real-analysis', 'calculus', 'derivatives', 'definition']"
55,A limit evaluating to $2 K$ (Catalan's constant),A limit evaluating to  (Catalan's constant),2 K,Experimentally I discovered the limit below that says that $$\lim_{n\to\infty} \int_0^{\pi/2} \frac{1}{\displaystyle \cos\left(\frac{x}{2}\right)\left(\cos\left(\frac{x}{2}\right)-\cos\left(\frac{x}{2^2}\right)\right)\cdots \left(\cos\left(\frac{x}{2}\right)-\cos\left(\frac{x}{2^{2n+1}}\right)\right)}+\cdots$$ $$+\frac{1}{\displaystyle \cos\left(\frac{x}{2^{2n+1}}\right)\left(\cos\left(\frac{x}{2^{2n+1}}\right)-\cos\left(\frac{x}{2}\right)\right)\cdots \left(\cos\left(\frac{x}{2^{2n+1}}\right)-\cos\left(\frac{x}{2^{2n}}\right)\right)} \ dx=2 K$$ or as @robjohn suggested $$\lim_{n\to\infty}{\Large\int}_0^{\pi/2}\operatorname*{\Large\sum}_{j=1}^{2n+1}\left[\cos\left(\frac{x}{2^j}\right)\prod_{\substack{k=1\\k\ne j}}^{2n+1}\left(\cos\left(\frac{x}{2^j}\right)-\cos\left(\frac{x}{2^k}\right)\right)\right]^{-1}\mathrm{d}x=2 K$$ Do you see any easy way of proving this result?,Experimentally I discovered the limit below that says that $$\lim_{n\to\infty} \int_0^{\pi/2} \frac{1}{\displaystyle \cos\left(\frac{x}{2}\right)\left(\cos\left(\frac{x}{2}\right)-\cos\left(\frac{x}{2^2}\right)\right)\cdots \left(\cos\left(\frac{x}{2}\right)-\cos\left(\frac{x}{2^{2n+1}}\right)\right)}+\cdots$$ $$+\frac{1}{\displaystyle \cos\left(\frac{x}{2^{2n+1}}\right)\left(\cos\left(\frac{x}{2^{2n+1}}\right)-\cos\left(\frac{x}{2}\right)\right)\cdots \left(\cos\left(\frac{x}{2^{2n+1}}\right)-\cos\left(\frac{x}{2^{2n}}\right)\right)} \ dx=2 K$$ or as @robjohn suggested $$\lim_{n\to\infty}{\Large\int}_0^{\pi/2}\operatorname*{\Large\sum}_{j=1}^{2n+1}\left[\cos\left(\frac{x}{2^j}\right)\prod_{\substack{k=1\\k\ne j}}^{2n+1}\left(\cos\left(\frac{x}{2^j}\right)-\cos\left(\frac{x}{2^k}\right)\right)\right]^{-1}\mathrm{d}x=2 K$$ Do you see any easy way of proving this result?,,"['real-analysis', 'calculus', 'limits', 'definite-integrals', 'catalans-constant']"
56,If $f(x)+\frac{f'(x)}{x}\to0$ then $f(x)\to0$,If  then,f(x)+\frac{f'(x)}{x}\to0 f(x)\to0,I honestly have no idea how to solve this one. Can we use the mean value theorem? $$\lim_{x\to \infty}\left(f(x)+\frac{f'(x)}{x}\right)=0 \implies \lim_{x\to \infty}f(x)=0$$,I honestly have no idea how to solve this one. Can we use the mean value theorem? $$\lim_{x\to \infty}\left(f(x)+\frac{f'(x)}{x}\right)=0 \implies \lim_{x\to \infty}f(x)=0$$,,"['real-analysis', 'limits']"
57,Proving that $\int_0^1\frac{x \log^2(1-x)}{1+x^2} \ dx = \frac{35}{32}\zeta(3)+\frac{1}{24}\log^3(2) -\frac{5}{96} \pi^2 \log(2)$,Proving that,\int_0^1\frac{x \log^2(1-x)}{1+x^2} \ dx = \frac{35}{32}\zeta(3)+\frac{1}{24}\log^3(2) -\frac{5}{96} \pi^2 \log(2),"Could we possibly prove this result without using the polylogarithm ? I know how to do it by   polylogarithm means, but I want a different way. Is that possible? $$\int_0^1\frac{x \log^2(1-x)}{1+x^2} \ dx = \frac{35}{32}\zeta(3)+\frac{1}{24}\log^3(2) -\frac{5}{96} \pi^2 \log(2)$$","Could we possibly prove this result without using the polylogarithm ? I know how to do it by   polylogarithm means, but I want a different way. Is that possible? $$\int_0^1\frac{x \log^2(1-x)}{1+x^2} \ dx = \frac{35}{32}\zeta(3)+\frac{1}{24}\log^3(2) -\frac{5}{96} \pi^2 \log(2)$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
58,Dirac delta sequences,Dirac delta sequences,,"Is it true that any sequence of real functions $(\delta_n)_n$, such that $$\lim_{n\to\infty} \delta_n(x) = 0 \qquad \forall\,x\ne 0$$ and  $$\int_{-\infty}^\infty \delta_n(x)\,dx = 1 \ ,$$ tends to a delta function, $$\lim_{n\to\infty} \delta_n(x) = \delta(x)$$ in a sense that $$\lim_{n\to\infty} \int_{-\infty}^\infty \phi(x)\,\delta_n(x) \, dx = \phi(0)$$ for any real test function $\phi$? If no, what else should one assume so that the sequence $(\delta_n)_n$ necessarly tends to a delta function?","Is it true that any sequence of real functions $(\delta_n)_n$, such that $$\lim_{n\to\infty} \delta_n(x) = 0 \qquad \forall\,x\ne 0$$ and  $$\int_{-\infty}^\infty \delta_n(x)\,dx = 1 \ ,$$ tends to a delta function, $$\lim_{n\to\infty} \delta_n(x) = \delta(x)$$ in a sense that $$\lim_{n\to\infty} \int_{-\infty}^\infty \phi(x)\,\delta_n(x) \, dx = \phi(0)$$ for any real test function $\phi$? If no, what else should one assume so that the sequence $(\delta_n)_n$ necessarly tends to a delta function?",,"['real-analysis', 'dirac-delta', 'sequence-of-function']"
59,If $\{f(0)\}^{2} + \{f'(0)\}^{2} = 4$ then there is a $c$ with $f(c) + f''(c) = 0$,If  then there is a  with,\{f(0)\}^{2} + \{f'(0)\}^{2} = 4 c f(c) + f''(c) = 0,"This is from Putnam: If a function $f: \mathbb{R} \to [-1, 1]$ is such that $f''(x)$ exists for all $x \in \mathbb{R}$ and $\{f(0)\}^{2} + \{f'(0)\}^{2} = 4$ then prove that there is a point $c$ such that $f(c) + f''(c) = 0$. Now the condition concerning values of $f(0), f'(0)$ (and the fact that range of $f$ is subset of $[-1, 1]$) implies that $f'(0) \neq 0$ so that function is not a constant. If we take $g(x) = \{f(x)\}^{2} + \{f'(x)\}^{2}$ then we can see that $g' = 2f'(f + f'')$. One hope of solving the problem is to show that $g$ attains local extrema at some point different from an extrema of $f$. This would ensure that $g'$ vanishes without making $f'$ vanish and that will lead to $f + f'' = 0$. However the constraint on $f(0), f'(0)$ does not help in analyzing the extrema of $f$ or $g$. Another line of thought which could be helpful here is to consider $F(x) = f'(x)\sin x - f(x)\cos x$ so that $F'(x) = \sin x\{f(x) + f''(x)\}$ but in this case I am not able to see how to use the condition on values $f(0), f'(0)$ to impose some constraint on $F(x)$. Perhaps both the approaches which came to my mind are not in right direction (or may be they are, but I am unable to see). Please provide any hints. Update : Based on the answer by Sandeep Thilakan, I have dropped the requirement for continuity of $f''(x)$.","This is from Putnam: If a function $f: \mathbb{R} \to [-1, 1]$ is such that $f''(x)$ exists for all $x \in \mathbb{R}$ and $\{f(0)\}^{2} + \{f'(0)\}^{2} = 4$ then prove that there is a point $c$ such that $f(c) + f''(c) = 0$. Now the condition concerning values of $f(0), f'(0)$ (and the fact that range of $f$ is subset of $[-1, 1]$) implies that $f'(0) \neq 0$ so that function is not a constant. If we take $g(x) = \{f(x)\}^{2} + \{f'(x)\}^{2}$ then we can see that $g' = 2f'(f + f'')$. One hope of solving the problem is to show that $g$ attains local extrema at some point different from an extrema of $f$. This would ensure that $g'$ vanishes without making $f'$ vanish and that will lead to $f + f'' = 0$. However the constraint on $f(0), f'(0)$ does not help in analyzing the extrema of $f$ or $g$. Another line of thought which could be helpful here is to consider $F(x) = f'(x)\sin x - f(x)\cos x$ so that $F'(x) = \sin x\{f(x) + f''(x)\}$ but in this case I am not able to see how to use the condition on values $f(0), f'(0)$ to impose some constraint on $F(x)$. Perhaps both the approaches which came to my mind are not in right direction (or may be they are, but I am unable to see). Please provide any hints. Update : Based on the answer by Sandeep Thilakan, I have dropped the requirement for continuity of $f''(x)$.",,"['calculus', 'real-analysis']"
60,Infinitely differentiable function with divergent Taylor series?,Infinitely differentiable function with divergent Taylor series?,,"I'd greatly appreciate it if someone could provide examples of the following: 1) A infinitely differentiable function whose Taylor series does not converge to the function. 2) An infinitely differentiable function whose Taylor series diverges. My differential equations text says that ""most"" smooth functions are of one of these types. What can be done if we weaken the infinitely differentiable condition? (Please do not use $f(x) = e^{-1/x^2}$ as an example!)","I'd greatly appreciate it if someone could provide examples of the following: 1) A infinitely differentiable function whose Taylor series does not converge to the function. 2) An infinitely differentiable function whose Taylor series diverges. My differential equations text says that ""most"" smooth functions are of one of these types. What can be done if we weaken the infinitely differentiable condition? (Please do not use $f(x) = e^{-1/x^2}$ as an example!)",,"['real-analysis', 'convergence-divergence', 'taylor-expansion']"
61,Taylor series (or equivalent at $\epsilon\to0$) of the integral over $x$ of a function of $x$ and $\epsilon$,Taylor series (or equivalent at ) of the integral over  of a function of  and,\epsilon\to0 x x \epsilon,"I have a function $f$ of two arguments, defined as $$ f(x,\epsilon)=\epsilon\left( e^{-\frac{(x-\epsilon)^2}{2}} - e^{-\frac{x^2}{2}}\right) + \frac{1-\epsilon}{\sqrt{1+\epsilon}}\left( e^{-\frac{x^2}{2(1+\epsilon)}} - e^{-\frac{(x-\epsilon^2)^2}{2(1+\epsilon)}}\right) $$ for $x\in\mathbb{R}$ and $\epsilon\in(0,1)$; and I would like to get an equivalent (or even, ideally, a Taylor series expansion with 2 terms or more) of the quantity $$\Delta(\epsilon) \stackrel{\rm{}def}{=}\int_\mathbb{R}|f(x,\epsilon)| dx$$ for $\epsilon\to 0^+$. Doing a series expansion wrt $\epsilon$ of $|f(x,\epsilon)|$ hints that it should be of the form $\kappa \epsilon^3 + o(\epsilon^3)$, ""as"" $|f(x,\epsilon)|=\kappa(x)\epsilon^3 + O_x(\epsilon^4)$ (where the constants in the big-Oh notation depend on $x$, and $x\mapsto \kappa(x)\in L_1$)... but the expansion I get has not an explicit form, and then I fall short of arguments when it comes to integrating the ""$O_x(\epsilon^4)$"" on $\mathbb R$ wrt $x$. I did try for an expression as power series (wrt $\epsilon$) and using the related theorems to swap integration and summation, but again the coefficients $(P_k(x))_k$ of the said power series are functions of $x$ and might -- for all I know -- behave very badly once integrated wrt $x$. (A numerical plot and curve fitting also suggests that $\kappa \epsilon^3 + o(\epsilon^3)$ is likely to be the right solution.) Any help would be appreciated... Thanks! E.g, with Mathematica: Series[eps*( -(1/(E^(x^2/2) Sqrt[2 Pi])) + 1/(E^((x - eps)^2/2) Sqrt[2 Pi]) ) + (1-eps)*( 1/(E^(x^2/(2 (1 + eps))) Sqrt[2 Pi] Sqrt[1 + eps]) - 1/(E^((x - eps^2)^2/(2 (1 + eps))) Sqrt[2 Pi] Sqrt[1 + eps]) ), {eps,0,3}]; Edit there was a typo in the formula (not the code): the third gaussian had a $(1+\epsilon)^2$ instead of $(1+\epsilon)$ in the denominator of the argument. Essentially, the first two gaussian functions are pdfs with variance $1$, the last 2 with variances $\sqrt{1+\epsilon}$.","I have a function $f$ of two arguments, defined as $$ f(x,\epsilon)=\epsilon\left( e^{-\frac{(x-\epsilon)^2}{2}} - e^{-\frac{x^2}{2}}\right) + \frac{1-\epsilon}{\sqrt{1+\epsilon}}\left( e^{-\frac{x^2}{2(1+\epsilon)}} - e^{-\frac{(x-\epsilon^2)^2}{2(1+\epsilon)}}\right) $$ for $x\in\mathbb{R}$ and $\epsilon\in(0,1)$; and I would like to get an equivalent (or even, ideally, a Taylor series expansion with 2 terms or more) of the quantity $$\Delta(\epsilon) \stackrel{\rm{}def}{=}\int_\mathbb{R}|f(x,\epsilon)| dx$$ for $\epsilon\to 0^+$. Doing a series expansion wrt $\epsilon$ of $|f(x,\epsilon)|$ hints that it should be of the form $\kappa \epsilon^3 + o(\epsilon^3)$, ""as"" $|f(x,\epsilon)|=\kappa(x)\epsilon^3 + O_x(\epsilon^4)$ (where the constants in the big-Oh notation depend on $x$, and $x\mapsto \kappa(x)\in L_1$)... but the expansion I get has not an explicit form, and then I fall short of arguments when it comes to integrating the ""$O_x(\epsilon^4)$"" on $\mathbb R$ wrt $x$. I did try for an expression as power series (wrt $\epsilon$) and using the related theorems to swap integration and summation, but again the coefficients $(P_k(x))_k$ of the said power series are functions of $x$ and might -- for all I know -- behave very badly once integrated wrt $x$. (A numerical plot and curve fitting also suggests that $\kappa \epsilon^3 + o(\epsilon^3)$ is likely to be the right solution.) Any help would be appreciated... Thanks! E.g, with Mathematica: Series[eps*( -(1/(E^(x^2/2) Sqrt[2 Pi])) + 1/(E^((x - eps)^2/2) Sqrt[2 Pi]) ) + (1-eps)*( 1/(E^(x^2/(2 (1 + eps))) Sqrt[2 Pi] Sqrt[1 + eps]) - 1/(E^((x - eps^2)^2/(2 (1 + eps))) Sqrt[2 Pi] Sqrt[1 + eps]) ), {eps,0,3}]; Edit there was a typo in the formula (not the code): the third gaussian had a $(1+\epsilon)^2$ instead of $(1+\epsilon)$ in the denominator of the argument. Essentially, the first two gaussian functions are pdfs with variance $1$, the last 2 with variances $\sqrt{1+\epsilon}$.",,"['real-analysis', 'integration', 'power-series', 'taylor-expansion']"
62,Smooth Pac-Man Curve?,Smooth Pac-Man Curve?,,"Idle curiosity and a basic understanding of the last example here led me to this polar curve: $$r(\theta) = \exp\left(10\frac{|2\theta|-1-||2\theta|-1|}{|2\theta|}\right)\qquad\theta\in(-\pi,\pi]$$ which Wolfram Alpha shows to look like this: The curve is not defined at $\theta=0$, but we can augment with $r(0)=0$. If we do, then despite appearances, the curve is smooth at $\theta=0$. It is also smooth at the back where two arcs meet. However it is not differentiable at the mouth corners. Again out of idle curiosity, can someone propose a polar equation that produces a smooth-everywhere Pac-Man on $(-\pi.\pi]$? No piece-wise definitions please, but absolute value is OK.","Idle curiosity and a basic understanding of the last example here led me to this polar curve: $$r(\theta) = \exp\left(10\frac{|2\theta|-1-||2\theta|-1|}{|2\theta|}\right)\qquad\theta\in(-\pi,\pi]$$ which Wolfram Alpha shows to look like this: The curve is not defined at $\theta=0$, but we can augment with $r(0)=0$. If we do, then despite appearances, the curve is smooth at $\theta=0$. It is also smooth at the back where two arcs meet. However it is not differentiable at the mouth corners. Again out of idle curiosity, can someone propose a polar equation that produces a smooth-everywhere Pac-Man on $(-\pi.\pi]$? No piece-wise definitions please, but absolute value is OK.",,"['real-analysis', 'recreational-mathematics', 'polar-coordinates', 'analyticity']"
63,A set with a finite integral of measure zero?,A set with a finite integral of measure zero?,,"Prove, or give a counter example: Let $\mu$ be a finite positive borel measure on $\mathbb{R}$. Then $\int (x-y)^{-2} d \mu (y) = \infty $ almost everywhere on $\mu$ (for the selection of x's). This is a question I had in an exam, and the answer is supposed to be presented in less than 30 words, so there must be something quite simple I'm missing.","Prove, or give a counter example: Let $\mu$ be a finite positive borel measure on $\mathbb{R}$. Then $\int (x-y)^{-2} d \mu (y) = \infty $ almost everywhere on $\mu$ (for the selection of x's). This is a question I had in an exam, and the answer is supposed to be presented in less than 30 words, so there must be something quite simple I'm missing.",,"['real-analysis', 'measure-theory']"
64,Asymptotic expansion of the inverse of $x\mapsto x+x^{\small\sqrt2}+x^2$ near zero,Asymptotic expansion of the inverse of  near zero,x\mapsto x+x^{\small\sqrt2}+x^2,"This is a follow-up to my previous question ""Asymptotic expansion of the inverse of $x\mapsto x+x^\phi$ near zero"" . Consider a continuous real-valued monotone increasing function $f:\mathbb R^+\to\mathbb R^+$ satisfying $f\big(x+x^{\small\sqrt2}+x^2\big)=x.$ I am interested in an asymptotic expansion of $f(z)$ for $z\to0^+$ in terms of powers of $z$ . I was able to find a few initial terms by manually balancing coefficients: $$f(z)=z-z^{\small\sqrt2}+\sqrt2\;z^{\small\unicode{x202f}2\unicode{x202f}\sqrt2-1}-z^2+\mathcal O\big(z^{\small\unicode{x202f}3\unicode{x202f}\sqrt2-2}\big), \quad z\to0^+.\tag{$\diamond$}$$ Computing next terms in an ad hoc fashion quickly becomes tedious, so I am looking for a more systematic approach that would allow to obtain a general formula for the terms of this series. I expect it to be a mix of integer powers of $z$ and irrational powers involving $\sqrt2$ . Also, I would like to know the radius of convergence of that series. More generally, I am looking for a uniform approach for inverting generalized polynomials of a single variable that may contain both rational and irrational powers.","This is a follow-up to my previous question ""Asymptotic expansion of the inverse of near zero"" . Consider a continuous real-valued monotone increasing function satisfying I am interested in an asymptotic expansion of for in terms of powers of . I was able to find a few initial terms by manually balancing coefficients: Computing next terms in an ad hoc fashion quickly becomes tedious, so I am looking for a more systematic approach that would allow to obtain a general formula for the terms of this series. I expect it to be a mix of integer powers of and irrational powers involving . Also, I would like to know the radius of convergence of that series. More generally, I am looking for a uniform approach for inverting generalized polynomials of a single variable that may contain both rational and irrational powers.","x\mapsto x+x^\phi f:\mathbb R^+\to\mathbb R^+ f\big(x+x^{\small\sqrt2}+x^2\big)=x. f(z) z\to0^+ z f(z)=z-z^{\small\sqrt2}+\sqrt2\;z^{\small\unicode{x202f}2\unicode{x202f}\sqrt2-1}-z^2+\mathcal O\big(z^{\small\unicode{x202f}3\unicode{x202f}\sqrt2-2}\big), \quad z\to0^+.\tag{\diamond} z \sqrt2","['real-analysis', 'sequences-and-series', 'asymptotics', 'inverse-function']"
65,Prove that $\lim_{a \to \infty} \sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = 1$.,Prove that .,\lim_{a \to \infty} \sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = 1,"The above sum (without the $\lim$ notation) is convergent $\forall a \in \Bbb{N}^+$ , because: $$ \sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = \sum_{n=1}^{\infty} \frac{n! n! \dots n!}{n^n n^n \dots n^n} \stackrel{\quad \text{because} \\ \forall n \in \Bbb{N}^+ \ n! \le n^n}{\le} \sum_{n=1}^{\infty} \frac{n!}{n^n} \approx \\ \stackrel{\quad \text{Stirling-} \\ \text{approximation}}{\approx}    \sum_{n=1}^{\infty} \frac{\sqrt{2\pi n}\left(\frac{n}{e}\right)^n}{n^n} = \sum_{n=1}^{\infty} \frac{\sqrt{2\pi n}}{e^n} = \sqrt{2\pi} \sum_{n=1}^{\infty}\frac{\sqrt{n}}{e^n} = \\ = \sqrt{2\pi} \ Li_{-\frac{1}{2}}\left(\frac{1}{e}\right) \approx 1.7728 $$ It is also decreasing $\forall a \in \Bbb{N}^+$ : $$\sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = \sum_{n=1}^{\infty} \left(\frac{(n!)^{a-1}}{n^{(a-1)n}}\right)\frac{n!}{n^n} \le \sum_{n=1}^{\infty} \frac{(n!)^{a-1}}{n^{(a-1)n}}$$ Because the left side is multiplied by a term that is always $\le 1$ , namely $$\forall n \in \Bbb{N}^+ \quad \frac{n!}{n^n} \le 1$$ So the limit exists: $$\exists \lim_{a \to \infty} \sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = ? \in \Bbb{R}^+_0$$ I examined the sum in Python and wrote a code to calculate it up to $a=20$ , each time adding up the sum up to $n=1000$ : import math  def sum_term(n,a):     return (pow(math.factorial(n),a))/pow(n,a*n)  for a in range(1,21):     value = 0     for n in range(1,1001):         value += sum_term(n,a)     print(""a = "" + str(a) + "" | lim = "" + str(value)) And its output was: a = 1 | lim = 1.879853862175259 a = 2 | lim = 1.3099287490030924 a = 3 | lim = 1.1368584537249211 a = 4 | lim = 1.065018132743388 a = 5 | lim = 1.0317992491522754 a = 6 | lim = 1.0157461094449747 a = 7 | lim = 1.0078393253936435 a = 8 | lim = 1.0039122029986458 a = 9 | lim = 1.0019544471210995 a = 10 | lim = 1.0009768562327848 a = 11 | lim = 1.000488346517213 a = 12 | lim = 1.0002441551281933 a = 13 | lim = 1.0001220735353726 a = 14 | lim = 1.0000610358724382 a = 15 | lim = 1.0000305177372775 a = 16 | lim = 1.0000152588244295 a = 17 | lim = 1.0000076294023905 a = 18 | lim = 1.0000038146990122 a = 19 | lim = 1.000001907349021 a = 20 | lim = 1.0000009536744026 It is pretty convincing that the limit tends to $1$ , however, I want to prove this mathematically. Is there a way to do this with perhaps the Squeeze theorem or other methods?","The above sum (without the notation) is convergent , because: It is also decreasing : Because the left side is multiplied by a term that is always , namely So the limit exists: I examined the sum in Python and wrote a code to calculate it up to , each time adding up the sum up to : import math  def sum_term(n,a):     return (pow(math.factorial(n),a))/pow(n,a*n)  for a in range(1,21):     value = 0     for n in range(1,1001):         value += sum_term(n,a)     print(""a = "" + str(a) + "" | lim = "" + str(value)) And its output was: a = 1 | lim = 1.879853862175259 a = 2 | lim = 1.3099287490030924 a = 3 | lim = 1.1368584537249211 a = 4 | lim = 1.065018132743388 a = 5 | lim = 1.0317992491522754 a = 6 | lim = 1.0157461094449747 a = 7 | lim = 1.0078393253936435 a = 8 | lim = 1.0039122029986458 a = 9 | lim = 1.0019544471210995 a = 10 | lim = 1.0009768562327848 a = 11 | lim = 1.000488346517213 a = 12 | lim = 1.0002441551281933 a = 13 | lim = 1.0001220735353726 a = 14 | lim = 1.0000610358724382 a = 15 | lim = 1.0000305177372775 a = 16 | lim = 1.0000152588244295 a = 17 | lim = 1.0000076294023905 a = 18 | lim = 1.0000038146990122 a = 19 | lim = 1.000001907349021 a = 20 | lim = 1.0000009536744026 It is pretty convincing that the limit tends to , however, I want to prove this mathematically. Is there a way to do this with perhaps the Squeeze theorem or other methods?","\lim \forall a \in \Bbb{N}^+ 
\sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = \sum_{n=1}^{\infty} \frac{n! n! \dots n!}{n^n n^n \dots n^n} \stackrel{\quad \text{because} \\ \forall n \in \Bbb{N}^+ \ n! \le n^n}{\le} \sum_{n=1}^{\infty} \frac{n!}{n^n} \approx \\ \stackrel{\quad \text{Stirling-} \\ \text{approximation}}{\approx}  
 \sum_{n=1}^{\infty} \frac{\sqrt{2\pi n}\left(\frac{n}{e}\right)^n}{n^n} = \sum_{n=1}^{\infty} \frac{\sqrt{2\pi n}}{e^n} = \sqrt{2\pi} \sum_{n=1}^{\infty}\frac{\sqrt{n}}{e^n} = \\ = \sqrt{2\pi} \ Li_{-\frac{1}{2}}\left(\frac{1}{e}\right) \approx 1.7728
 \forall a \in \Bbb{N}^+ \sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = \sum_{n=1}^{\infty} \left(\frac{(n!)^{a-1}}{n^{(a-1)n}}\right)\frac{n!}{n^n} \le \sum_{n=1}^{\infty} \frac{(n!)^{a-1}}{n^{(a-1)n}} \le 1 \forall n \in \Bbb{N}^+ \quad \frac{n!}{n^n} \le 1 \exists \lim_{a \to \infty} \sum_{n=1}^{\infty} \frac{(n!)^a}{n^{an}} = ? \in \Bbb{R}^+_0 a=20 n=1000 1","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'summation']"
66,Extended limit laws for step by step evaluation of limits,Extended limit laws for step by step evaluation of limits,,"Most textbooks of calculus / real analysis present a version of limit laws which help us to infer about limit of sum or product of functions provided the limits of individual functions are known to exist. Such rules also go by the name algebra of limits . In this post and my answer to it I provide an extended version of these laws which are more helpful in step by step evaluation of a limit. Before I state those laws it is better to give some introductory remarks. If $f$ is a real valued function defined in certain deleted neighborhood of point $a$ then the limiting behavior of $f(x) $ as $x\to a$ can be of one of the following types: $\lim_{x\to a} f(x) $ exists. Although it is redundant to state, to avoid confusion / ambiguity this means that the limit exists as a finite real number. We also say that $f(x) $ converges to a real number as $x\to a$ . Example $\lim_{x\to 0}x$ . $f(x) \to \infty $ as $x\to a$ . We say that $f(x) $ diverges to $\infty $ as $x\to a$ and some prefer to write this in symbols as $\lim_{x\to a} f(x) =\infty$ . Example $\lim_{x\to 0} (1/x^2)$ . $f(x) \to - \infty $ as $x\to a$ . We say that $f(x) $ diverges to $-\infty $ as $x\to a $ and some prefer to write this in symbols as $\lim_{x\to a} f(x) =-\infty$ . Example $\lim_{x\to 0} (-1/x^2)$ . $f(x) $ oscillates finitely as $x\to a$ . More formally this means that $f$ is bounded in some deleted neighborhood of $a$ and there exist at least two distinct real numbers $A$ and $B$ and two sequences $\{a_n\}, \{b_n\} $ of numbers in the deleted neighborhood of $a$ such that $$\lim_{n\to\infty} a_n=a=\lim_{n\to\infty} b_n$$ and $$\lim_{n\to\infty} f(a_n) =A, \lim_{n\to\infty} f(b_n) =B$$ Example $\lim_{x\to 0}(1/x)-\lfloor 1/x\rfloor$ . $f(x) $ oscillates infinitely as $x\to a$ . This means that there is a sequence $\{a_n\} $ of numbers in deleted neighborhood of $a$ such that $$\lim_{n\to\infty} a_n=a, \lim_{n\to \infty} |f(a_n) |=\infty$$ and yet neither $f(x) \to\infty $ nor $f(x) \to-\infty $ as $x\to a$ . Example $\lim_{x\to 0}(1/x)\sin(1/x)$ . The above list is exhaustive and consists of mutually exclusive possibilities. Sometimes the second and third options are combined together and one says that $f(x) $ diverges as $x\to a$ . Similarly the fourth and fifth options can be combined to say that $f(x) $ oscillates as $x\to a$ . Now we come to extended limit laws. Theorem 1 : Let $f, g$ be functions defined in a certain deleted neighborhood of $a$ and let $\lim_{x\to a} f(x) $ exist and be equal to $L$ . Then the limiting behavior of $f(x) \pm g(x) $ as $x\to a$ is of exactly the same type  as that of $g(x) $ and we can write $$\lim_{x\to a} \{f(x) \pm g(x) \}=\lim_{x\to a} f(x) \pm\lim_{x\to a} g(x) =L\pm\lim_{x\to a} g(x) $$ The case of divergence can be same or opposite (as regards to sign of $\infty$ ) depending on the sign $\pm$ which combines $f, g$ . Theorem 2 : Let $f, g$ be defined in a certain deleted neighborhood of $a$ and let $\lim_{x\to a} f(x) =L\neq 0$ . Then the limiting behavior of $f(x) g(x) $ as $x\to a$ is of exactly the same type as that of $g(x) $ and we can write $$\lim_{x\to a} f(x) g(x) =\lim_{x\to a} f(x) \cdot \lim_{x\to a} g(x) =L\lim_{x\to a} g(x) $$ The case of divergence can be same or opposite according as $L>0$ or $L<0$ . Also the case of convergence holds when $L=0$ but other cases can't be guaranteed when $L=0$ . Both these theorems can be used to evaluate the limit of a complicated expression in a step by step manner by handling one term or one factor at a time whose limit is known thereby reducing the expression to a simpler form at each step. Each step is justified on the basis of the term/factor whose limit is known irrespective of the behavior of other terms/factors. Moreover the theorems indicate that each step is reversible and hence holds unconditionally. This is better than using the standard limit laws which basically say that the limit has to be applied simultaneously on each part of the expression on the condition that each part has a limit and parts occurring as denominator have non-zero limit. I will provide proof of one of the theorems as an answer (to be marked community wiki). I expect users to provide other point of views regarding these theorems and any improvements in my question and answer are also welcome. Note : The above is a more formal and detailed version of the rules presented in this answer and it is based on a request in a comment to another question.","Most textbooks of calculus / real analysis present a version of limit laws which help us to infer about limit of sum or product of functions provided the limits of individual functions are known to exist. Such rules also go by the name algebra of limits . In this post and my answer to it I provide an extended version of these laws which are more helpful in step by step evaluation of a limit. Before I state those laws it is better to give some introductory remarks. If is a real valued function defined in certain deleted neighborhood of point then the limiting behavior of as can be of one of the following types: exists. Although it is redundant to state, to avoid confusion / ambiguity this means that the limit exists as a finite real number. We also say that converges to a real number as . Example . as . We say that diverges to as and some prefer to write this in symbols as . Example . as . We say that diverges to as and some prefer to write this in symbols as . Example . oscillates finitely as . More formally this means that is bounded in some deleted neighborhood of and there exist at least two distinct real numbers and and two sequences of numbers in the deleted neighborhood of such that and Example . oscillates infinitely as . This means that there is a sequence of numbers in deleted neighborhood of such that and yet neither nor as . Example . The above list is exhaustive and consists of mutually exclusive possibilities. Sometimes the second and third options are combined together and one says that diverges as . Similarly the fourth and fifth options can be combined to say that oscillates as . Now we come to extended limit laws. Theorem 1 : Let be functions defined in a certain deleted neighborhood of and let exist and be equal to . Then the limiting behavior of as is of exactly the same type  as that of and we can write The case of divergence can be same or opposite (as regards to sign of ) depending on the sign which combines . Theorem 2 : Let be defined in a certain deleted neighborhood of and let . Then the limiting behavior of as is of exactly the same type as that of and we can write The case of divergence can be same or opposite according as or . Also the case of convergence holds when but other cases can't be guaranteed when . Both these theorems can be used to evaluate the limit of a complicated expression in a step by step manner by handling one term or one factor at a time whose limit is known thereby reducing the expression to a simpler form at each step. Each step is justified on the basis of the term/factor whose limit is known irrespective of the behavior of other terms/factors. Moreover the theorems indicate that each step is reversible and hence holds unconditionally. This is better than using the standard limit laws which basically say that the limit has to be applied simultaneously on each part of the expression on the condition that each part has a limit and parts occurring as denominator have non-zero limit. I will provide proof of one of the theorems as an answer (to be marked community wiki). I expect users to provide other point of views regarding these theorems and any improvements in my question and answer are also welcome. Note : The above is a more formal and detailed version of the rules presented in this answer and it is based on a request in a comment to another question.","f a f(x)  x\to a \lim_{x\to a} f(x)  f(x)  x\to a \lim_{x\to 0}x f(x) \to \infty  x\to a f(x)  \infty  x\to a \lim_{x\to a} f(x) =\infty \lim_{x\to 0} (1/x^2) f(x) \to - \infty  x\to a f(x)  -\infty  x\to a  \lim_{x\to a} f(x) =-\infty \lim_{x\to 0} (-1/x^2) f(x)  x\to a f a A B \{a_n\}, \{b_n\}  a \lim_{n\to\infty} a_n=a=\lim_{n\to\infty} b_n \lim_{n\to\infty} f(a_n) =A, \lim_{n\to\infty} f(b_n) =B \lim_{x\to 0}(1/x)-\lfloor 1/x\rfloor f(x)  x\to a \{a_n\}  a \lim_{n\to\infty} a_n=a, \lim_{n\to \infty} |f(a_n) |=\infty f(x) \to\infty  f(x) \to-\infty  x\to a \lim_{x\to 0}(1/x)\sin(1/x) f(x)  x\to a f(x)  x\to a f, g a \lim_{x\to a} f(x)  L f(x) \pm g(x)  x\to a g(x)  \lim_{x\to a} \{f(x) \pm g(x) \}=\lim_{x\to a} f(x) \pm\lim_{x\to a} g(x) =L\pm\lim_{x\to a} g(x)  \infty \pm f, g f, g a \lim_{x\to a} f(x) =L\neq 0 f(x) g(x)  x\to a g(x)  \lim_{x\to a} f(x) g(x) =\lim_{x\to a} f(x) \cdot \lim_{x\to a} g(x) =L\lim_{x\to a} g(x)  L>0 L<0 L=0 L=0","['calculus', 'real-analysis', 'limits']"
67,Does $a_n = n\sin n$ have a convergent subsequence?,Does  have a convergent subsequence?,a_n = n\sin n,"I know that $\sin n$ has a convergent subsequence since it is bounded, but does the unbounded sequence $n\sin n$ have a convergent subsequence? Given a subsequence of $\sin n$ that tends to zero, it is still possible that when we multiply the convergent subsequence $\sin(n_k)$ by $n_k$, the limit may not be $0$.","I know that $\sin n$ has a convergent subsequence since it is bounded, but does the unbounded sequence $n\sin n$ have a convergent subsequence? Given a subsequence of $\sin n$ that tends to zero, it is still possible that when we multiply the convergent subsequence $\sin(n_k)$ by $n_k$, the limit may not be $0$.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
68,An alternative way to define improper integrals,An alternative way to define improper integrals,,"Improper Riemann integrals are usually defined via limits. Standard Definition : Let $f:[0, \infty) \to \mathbb{R}$. We say $f$ is improper Riemann integrable on $[0, \infty)$ if it is proper Riemann integrable on compact intervals and the following limit exists $$ \lim_{t \to \infty} \int_{0}^{t} f(x) \ dx$$ Instead of this, can we define the improper integral with ""partitions"" of $[0, \infty)$ as we do with the normal Riemann integral? Preliminaries : A partition of $[0, \infty)$ is a strictly increasing sequence $p:\mathbb{N}_{\geq 1} \to [0, \infty)$ with $p(1) = 0$ and $$\lim_{n \to \infty} p(n) = +\infty$$ A tagging of a given partition $p$ is any sequence $t:\mathbb{N}_{\geq 1} \to [0, \infty)$ such that $t(n) \in [p(n), p(n+1)]$ for all $n \geq 1$. A refinement of a partition $p$ is a partition $p'$ which contains $p$ as a subsequence. The mesh of a partition $p$ is the quantity $\sup\{p(n+1) - p(n): n \in \mathbb{N}_{\geq 1}\}$. It is denoted as $||p||$. $||p|| = +\infty$ is possible. There's two ways we can go about our definition. Definition 1 : Let $f:[0, \infty) \to \mathbb{R}$. We say $f$ is improper Riemann integrable on $[0, \infty)$ if there is a real number $L$ such that for all $\epsilon>0$ there is a partition $p_{\epsilon}$ such that for every refinement $p_{\epsilon}'$ of $p_{\epsilon}$ and any tagging $t$ of $p_{\epsilon}'$ the sum  $$S(f, p_{\epsilon}', t) \stackrel{\text{def}}{=} \sum_{n=1}^{\infty} [p_{\epsilon}'(n+1) - p_{\epsilon}'(n)]f(t(n))$$ converges and $$|L - S(f, p_{\epsilon}', t)| < \epsilon$$ Definition 2 : Let $f:[0, \infty) \to \mathbb{R}$. We say $f$ is improper Riemann integrable on $[0, \infty)$ if there is a real number $L$ such that for all $\epsilon>0$ there is a $\delta>0$ such that for every partition $p_{\delta}$ with $||p_{\delta}||<\delta$ and any tagging $t$ of $p_{\delta}$, the sum  $$S(f, p_{\delta}, t) \stackrel{\text{def}}{=} \sum_{n=1}^{\infty} [p_{\delta}(n+1) - p_{\delta}(n)]f(t(n))$$ converges and $$|L - S(f, p_{\delta}, t)| < \epsilon$$ Problem : Are all of these definitions equivalent? Partial answers are fine.","Improper Riemann integrals are usually defined via limits. Standard Definition : Let $f:[0, \infty) \to \mathbb{R}$. We say $f$ is improper Riemann integrable on $[0, \infty)$ if it is proper Riemann integrable on compact intervals and the following limit exists $$ \lim_{t \to \infty} \int_{0}^{t} f(x) \ dx$$ Instead of this, can we define the improper integral with ""partitions"" of $[0, \infty)$ as we do with the normal Riemann integral? Preliminaries : A partition of $[0, \infty)$ is a strictly increasing sequence $p:\mathbb{N}_{\geq 1} \to [0, \infty)$ with $p(1) = 0$ and $$\lim_{n \to \infty} p(n) = +\infty$$ A tagging of a given partition $p$ is any sequence $t:\mathbb{N}_{\geq 1} \to [0, \infty)$ such that $t(n) \in [p(n), p(n+1)]$ for all $n \geq 1$. A refinement of a partition $p$ is a partition $p'$ which contains $p$ as a subsequence. The mesh of a partition $p$ is the quantity $\sup\{p(n+1) - p(n): n \in \mathbb{N}_{\geq 1}\}$. It is denoted as $||p||$. $||p|| = +\infty$ is possible. There's two ways we can go about our definition. Definition 1 : Let $f:[0, \infty) \to \mathbb{R}$. We say $f$ is improper Riemann integrable on $[0, \infty)$ if there is a real number $L$ such that for all $\epsilon>0$ there is a partition $p_{\epsilon}$ such that for every refinement $p_{\epsilon}'$ of $p_{\epsilon}$ and any tagging $t$ of $p_{\epsilon}'$ the sum  $$S(f, p_{\epsilon}', t) \stackrel{\text{def}}{=} \sum_{n=1}^{\infty} [p_{\epsilon}'(n+1) - p_{\epsilon}'(n)]f(t(n))$$ converges and $$|L - S(f, p_{\epsilon}', t)| < \epsilon$$ Definition 2 : Let $f:[0, \infty) \to \mathbb{R}$. We say $f$ is improper Riemann integrable on $[0, \infty)$ if there is a real number $L$ such that for all $\epsilon>0$ there is a $\delta>0$ such that for every partition $p_{\delta}$ with $||p_{\delta}||<\delta$ and any tagging $t$ of $p_{\delta}$, the sum  $$S(f, p_{\delta}, t) \stackrel{\text{def}}{=} \sum_{n=1}^{\infty} [p_{\delta}(n+1) - p_{\delta}(n)]f(t(n))$$ converges and $$|L - S(f, p_{\delta}, t)| < \epsilon$$ Problem : Are all of these definitions equivalent? Partial answers are fine.",,"['real-analysis', 'integration', 'improper-integrals']"
69,Generalized inverse of a function,Generalized inverse of a function,,"It is well-known that if a function is strictly increasing, then it has an inverse function. I also see the concept of ""generalized inverse"" in the litarature, which has the definition  $$f^{-1}(x)=\inf\{y: f(y)>x\}.$$ What is the motivation of definition and can you give me examples which has not ordinary inverse but has generalized inverse?","It is well-known that if a function is strictly increasing, then it has an inverse function. I also see the concept of ""generalized inverse"" in the litarature, which has the definition  $$f^{-1}(x)=\inf\{y: f(y)>x\}.$$ What is the motivation of definition and can you give me examples which has not ordinary inverse but has generalized inverse?",,"['calculus', 'real-analysis', 'probability-theory']"
70,A smooth nowhere analytic function such that all derivatives are monotone,A smooth nowhere analytic function such that all derivatives are monotone,,"Related questions that might provide some context: (1) (2) (3) (4) Let's restrict our attention to real-valued functions on an open unit interval $f:(0,1)\to\mathbb R$. There are examples $\!^{[1]}$ $\!^{[2]}$ of smooth (class $C^\infty$) functions that are nowhere real-analytic . Is there a smooth nowhere analytic function such that the function itself and its derivatives of any order are monotone ?","Related questions that might provide some context: (1) (2) (3) (4) Let's restrict our attention to real-valued functions on an open unit interval $f:(0,1)\to\mathbb R$. There are examples $\!^{[1]}$ $\!^{[2]}$ of smooth (class $C^\infty$) functions that are nowhere real-analytic . Is there a smooth nowhere analytic function such that the function itself and its derivatives of any order are monotone ?",,"['real-analysis', 'continuity', 'examples-counterexamples', 'analyticity', 'monotone-functions']"
71,An Odd Mean Value Theorem Problem,An Odd Mean Value Theorem Problem,,"If $f: [x_1,x_2] \to \mathbb{R}$ is differentiable, show for some $c \in (x_1,x_2)$ that  $$ \frac{1}{x_1-x_2} \left| \begin{matrix} x_1 & x_2 \\ f(x_1) & f(x_2) \end{matrix} \right|=f(c)-cf'(c) $$ My attempt: Actually taking the determinant, multiplying by a negative, and carrying across the denominator on the left gives $$ x_1f(x_2)-x_2f(x_1)=(-f(c)+cf'(c)) \cdot (x_2-x_1) $$ and this screams Mean Value Theorem. So I took the function $g(x)=(x_2+x_1-x)f(x)$ which is clearly differentiable on $[x_1,x_2]$, then by the Mean Value Theorem, we know there is a $c \in (x_1,x_2)$ such that  $$ g(x_2)-g(x_1)=g'(c)(x_2-x_1) $$ But for our function $g(x)$, we know $g(x_2)=x_1f(x_2)$ and $g(x_1)=x_2f(x_1)$. Moreover, $g'(x)= - f(x) + (x_2+x_1-x)f'(x)$. Then this gives $$ x_1f(x_2)-x_2f(x_1)=(-f(c)+(x_2+x_1-c)f'(c))(x_2-x_1) $$ which is so close to what we wanted to show that I do not see how this could not be the correct approach. Have I missed something or is the result false?","If $f: [x_1,x_2] \to \mathbb{R}$ is differentiable, show for some $c \in (x_1,x_2)$ that  $$ \frac{1}{x_1-x_2} \left| \begin{matrix} x_1 & x_2 \\ f(x_1) & f(x_2) \end{matrix} \right|=f(c)-cf'(c) $$ My attempt: Actually taking the determinant, multiplying by a negative, and carrying across the denominator on the left gives $$ x_1f(x_2)-x_2f(x_1)=(-f(c)+cf'(c)) \cdot (x_2-x_1) $$ and this screams Mean Value Theorem. So I took the function $g(x)=(x_2+x_1-x)f(x)$ which is clearly differentiable on $[x_1,x_2]$, then by the Mean Value Theorem, we know there is a $c \in (x_1,x_2)$ such that  $$ g(x_2)-g(x_1)=g'(c)(x_2-x_1) $$ But for our function $g(x)$, we know $g(x_2)=x_1f(x_2)$ and $g(x_1)=x_2f(x_1)$. Moreover, $g'(x)= - f(x) + (x_2+x_1-x)f'(x)$. Then this gives $$ x_1f(x_2)-x_2f(x_1)=(-f(c)+(x_2+x_1-c)f'(c))(x_2-x_1) $$ which is so close to what we wanted to show that I do not see how this could not be the correct approach. Have I missed something or is the result false?",,"['real-analysis', 'derivatives']"
72,Give examples of clopen (open and closed) sets,Give examples of clopen (open and closed) sets,,"I was wondering if someone could help with the following problems: (a) Give an example of a topological space $(X,\mathcal{T})$ and a subset $A$ of $X$ which is both open and closed. (b) Give another example where $A$ is neither empty nor the whole of $X$. (c) Give an example of a topological space $(X,\mathcal{T})$ and a subset $A$ of $X$ which is neither open nor closed. My answers: (a) Let $X$ be any set with any topology $\mathcal{T}$, let $A=\emptyset$. (b) Let $X=(1,2)\cup(3,4)$, $A=(1,2)$ and $\mathcal{T}$ be the relative topology inherited from the usual topology on $\mathbb{R}$. (c) Let $X=\mathbb{R}$ with the usual metric and let $U_{n}=(0,1+\frac{1}{n})$.Then take $A=\displaystyle\bigcap_{n=1}^{\infty} U_{n}=(0,1]$.","I was wondering if someone could help with the following problems: (a) Give an example of a topological space $(X,\mathcal{T})$ and a subset $A$ of $X$ which is both open and closed. (b) Give another example where $A$ is neither empty nor the whole of $X$. (c) Give an example of a topological space $(X,\mathcal{T})$ and a subset $A$ of $X$ which is neither open nor closed. My answers: (a) Let $X$ be any set with any topology $\mathcal{T}$, let $A=\emptyset$. (b) Let $X=(1,2)\cup(3,4)$, $A=(1,2)$ and $\mathcal{T}$ be the relative topology inherited from the usual topology on $\mathbb{R}$. (c) Let $X=\mathbb{R}$ with the usual metric and let $U_{n}=(0,1+\frac{1}{n})$.Then take $A=\displaystyle\bigcap_{n=1}^{\infty} U_{n}=(0,1]$.",,"['real-analysis', 'general-topology', 'metric-spaces', 'examples-counterexamples']"
73,Real Induction Over Multiple Variables?,Real Induction Over Multiple Variables?,,"I've seen in several different places* that one can use normal mathematical induction to prove the truth of a statement that relies not on just one variable (say, $x$ ,) but multiple variables (for example, $a$ , $b$ , and $c$ .)  Can one use real induction (described in this paper ) in a similar manner? *Some Examples: Mathematics Stack Exchange — i. e.:  here on this site: How does one determine which variables to do induction on? Induction for statements with more than one variable. Other Site: Proof method: Multidimensional induction (from MathBlog )","I've seen in several different places* that one can use normal mathematical induction to prove the truth of a statement that relies not on just one variable (say, ,) but multiple variables (for example, , , and .)  Can one use real induction (described in this paper ) in a similar manner? *Some Examples: Mathematics Stack Exchange — i. e.:  here on this site: How does one determine which variables to do induction on? Induction for statements with more than one variable. Other Site: Proof method: Multidimensional induction (from MathBlog )",x a b c,"['real-analysis', 'induction', 'problem-solving', 'real-numbers']"
74,"Prove that there is no function $f:\Bbb{R}\to\Bbb{R}$ with $f(0)>0$ such that $\forall x,y\in\Bbb{R}, f(x+y)\geq f(x)+y f(f(x))$",Prove that there is no function  with  such that,"f:\Bbb{R}\to\Bbb{R} f(0)>0 \forall x,y\in\Bbb{R}, f(x+y)\geq f(x)+y f(f(x))","I got this problem: Prove that there is no function $f:\Bbb{R}\to\Bbb{R}$ with $f(0)>0$ such that $\forall x,y\in\Bbb{R}, f(x+y)\geq f(x)+y f(f(x))$. (Hint: the solution involves limits at infinity) I tried to prove it (by contradiction) but I failed. Thanks on any help.","I got this problem: Prove that there is no function $f:\Bbb{R}\to\Bbb{R}$ with $f(0)>0$ such that $\forall x,y\in\Bbb{R}, f(x+y)\geq f(x)+y f(f(x))$. (Hint: the solution involves limits at infinity) I tried to prove it (by contradiction) but I failed. Thanks on any help.",,"['calculus', 'real-analysis', 'limits', 'functional-equations']"
75,Will this sequence of polynomials converge to a Hermite polynomial pointwise?,Will this sequence of polynomials converge to a Hermite polynomial pointwise?,,"While trying to solve this question my testing lead to an observation that I found interesting in its own right. Consider the linear transformation $L:P\to P$ from the space of polynomial functions $p\in\Bbb{R}[x]$ to itself defined by setting $L(p)=p+p'$ , and its iterates $$ L^n(p)=\sum_{i=0}^\infty\binom{n}{i}p^{(i)}. $$ Write $G_{m,n}(x):=L^n(x^m)$ . These polynomials are monic of degree $m$ . If we then make the linear substitution $$ H_{m,n}(x):=\frac1{n^{m/2}}G_{m,n}(\sqrt n x-n) $$ we get another sequence of monic polynomials of degree $m$ . It seems to me that we have the limit $$ \lim_{n\to\infty}H_{m,n}(x)=He_m(x), $$ where the polynomial $He_m(x)$ is the so called probabilists' Hermite polynomial . Here the convergence can be thought of as either pointwise or in terms of the coefficients of the polynomials. Can you prove this? Is it known? The evidence that I have supports this very strongly, for we can calculate that $$ \begin{aligned} H_{1,n}(x)&=x,\\ H_{2,n}(x)&=x^2-1,\\ H_{3,n}(x)&=x^3-3x+\frac2{\sqrt{n}},\\ H_{4,n}(x)&=x^4-6x^2+\frac8{\sqrt{n}}x+3-\frac6n,\\ H_{5,n}(x)&=x^5-10x^3+\frac{20}{\sqrt{n}}x^2+\left(15-\frac{30}n\right)x+\frac{24}{n^{3/2}}-\frac{20}{\sqrt n}. \end{aligned} $$ Taking the limit as $n\to\infty$ is trivial here, and the results agree with $He_m(x)$ . Furthermore, the operator $L$ commutes with differentiation, so if we assume that the limit $\tilde{H}_m(x)=\lim_{n\to\infty}H_{m,n}(x)$ exists as a polynomial for all $m$ , then the chain rule gives us as a consequence of $Dx^m=mx^{m-1}$ that $$ \tilde{H}_m'(x)=m\tilde{H}_{m-1}(x). $$ This is one of properties listed of the probabilist's Hermite polynomials in that Wikipedia-article. If only we could determine the constant term, then this might lead to a proof by induction. A follow-up question is related to the conjecture I made while trying to answer that other question. The answer by George Lowther shows that for any monic polynomial $p$ of degree $m$ the polynomials $L^n(p)$ have $m$ distinct real roots for all large enough integers $n$ . If we number these zeros as $z_{1,n}>z_{2,n}>\cdots>z_{m,n}$ , then will the limits $$ z_i=\lim_{n\to\infty}\frac{z_{i,n}+n}{\sqrt n} $$ exist, and agree with the zeros of $He_m(x)$ . The evidence that I have for this is not as strong. It does look like the contribution of the leading term of $p$ in $L^n(p)$ will dominate the others for large $n$ . However, I am very rusty at estimating the error terms, and don't have that result about the limits yet either :-(","While trying to solve this question my testing lead to an observation that I found interesting in its own right. Consider the linear transformation from the space of polynomial functions to itself defined by setting , and its iterates Write . These polynomials are monic of degree . If we then make the linear substitution we get another sequence of monic polynomials of degree . It seems to me that we have the limit where the polynomial is the so called probabilists' Hermite polynomial . Here the convergence can be thought of as either pointwise or in terms of the coefficients of the polynomials. Can you prove this? Is it known? The evidence that I have supports this very strongly, for we can calculate that Taking the limit as is trivial here, and the results agree with . Furthermore, the operator commutes with differentiation, so if we assume that the limit exists as a polynomial for all , then the chain rule gives us as a consequence of that This is one of properties listed of the probabilist's Hermite polynomials in that Wikipedia-article. If only we could determine the constant term, then this might lead to a proof by induction. A follow-up question is related to the conjecture I made while trying to answer that other question. The answer by George Lowther shows that for any monic polynomial of degree the polynomials have distinct real roots for all large enough integers . If we number these zeros as , then will the limits exist, and agree with the zeros of . The evidence that I have for this is not as strong. It does look like the contribution of the leading term of in will dominate the others for large . However, I am very rusty at estimating the error terms, and don't have that result about the limits yet either :-(","L:P\to P p\in\Bbb{R}[x] L(p)=p+p' 
L^n(p)=\sum_{i=0}^\infty\binom{n}{i}p^{(i)}.
 G_{m,n}(x):=L^n(x^m) m 
H_{m,n}(x):=\frac1{n^{m/2}}G_{m,n}(\sqrt n x-n)
 m 
\lim_{n\to\infty}H_{m,n}(x)=He_m(x),
 He_m(x) 
\begin{aligned}
H_{1,n}(x)&=x,\\
H_{2,n}(x)&=x^2-1,\\
H_{3,n}(x)&=x^3-3x+\frac2{\sqrt{n}},\\
H_{4,n}(x)&=x^4-6x^2+\frac8{\sqrt{n}}x+3-\frac6n,\\
H_{5,n}(x)&=x^5-10x^3+\frac{20}{\sqrt{n}}x^2+\left(15-\frac{30}n\right)x+\frac{24}{n^{3/2}}-\frac{20}{\sqrt n}.
\end{aligned}
 n\to\infty He_m(x) L \tilde{H}_m(x)=\lim_{n\to\infty}H_{m,n}(x) m Dx^m=mx^{m-1} 
\tilde{H}_m'(x)=m\tilde{H}_{m-1}(x).
 p m L^n(p) m n z_{1,n}>z_{2,n}>\cdots>z_{m,n} 
z_i=\lim_{n\to\infty}\frac{z_{i,n}+n}{\sqrt n}
 He_m(x) p L^n(p) n","['real-analysis', 'polynomials', 'orthogonal-polynomials']"
76,"Meaning of ""almost everywhere"" in measure theory.","Meaning of ""almost everywhere"" in measure theory.",,"I'm slightly confused about the term almost everywhere as it is used in Folland's real analysis. Given a measure space $(X, \mathcal{M}, \mu)$  Suppose $f \equiv g$, $\mu$-almost everywhere where $f, g : X \to \mathbb{R}$. Does this mean that $$\mu(\{x : f(x) \ne g(x) \}) = 0$$ Or that there exists a measurable set $E$ such that $\{x : f(x) \ne g(x) \} \subseteq E$ and $\mu(E) = 0$? This issue came up when my professor was proving the following theorem from Folland: To prove (a) $\implies$ $\mu$ is complete, he took a null set $N \in \mathcal{M}$ and said for any $E \subseteq N$, $1_E \equiv 0$ almost everywhere.  This part confused me, because how can we know if $E$ is measurable?","I'm slightly confused about the term almost everywhere as it is used in Folland's real analysis. Given a measure space $(X, \mathcal{M}, \mu)$  Suppose $f \equiv g$, $\mu$-almost everywhere where $f, g : X \to \mathbb{R}$. Does this mean that $$\mu(\{x : f(x) \ne g(x) \}) = 0$$ Or that there exists a measurable set $E$ such that $\{x : f(x) \ne g(x) \} \subseteq E$ and $\mu(E) = 0$? This issue came up when my professor was proving the following theorem from Folland: To prove (a) $\implies$ $\mu$ is complete, he took a null set $N \in \mathcal{M}$ and said for any $E \subseteq N$, $1_E \equiv 0$ almost everywhere.  This part confused me, because how can we know if $E$ is measurable?",,"['real-analysis', 'measure-theory']"
77,What is meant by an open boundary when specifying boundary conditions of PDEs?,What is meant by an open boundary when specifying boundary conditions of PDEs?,,"When speaking about boundary conditions of  PDEs, one speaks about Dirichlet, Neumann or Cauchy boundary conditions specified over the boundary which can be closed or open .  For example,  we say that for hyperbolic partial differential equations, specification of Cauchy boundary condition on an open boundary gives rise to unique, stable solutions. What is meant by an open boundary here? How can a boundary of the region on which we solve our PDE  be open ? Surely the region is a specified region of space and obviously has a closed boundary. (part of which may be at infinity of course)","When speaking about boundary conditions of  PDEs, one speaks about Dirichlet, Neumann or Cauchy boundary conditions specified over the boundary which can be closed or open .  For example,  we say that for hyperbolic partial differential equations, specification of Cauchy boundary condition on an open boundary gives rise to unique, stable solutions. What is meant by an open boundary here? How can a boundary of the region on which we solve our PDE  be open ? Surely the region is a specified region of space and obviously has a closed boundary. (part of which may be at infinity of course)",,"['real-analysis', 'general-topology', 'multivariable-calculus', 'partial-differential-equations']"
78,Prove that Cantor function is Hölder continuous,Prove that Cantor function is Hölder continuous,,"Let $C_k$ be the set obtained in the $k-$th stage of building the Cantor set, where $$C_1=[0,\frac{1}{3}]\cup[\frac{1}{3},\frac{2}{3}]$$ $$C_2=[0,\frac{1}{9}]\cup[\frac{2}{9},\frac{1}{3}]\cup[\frac{2}{3},\frac{7}{9}]\cup[\frac{8}{9},1]$$ $$\dots$$ So we have $$C_k=\bigcup_{j=1}^{2^k}I_{k,j}$$ where $I_{k,j}$ are pairwise disjoint closed intervals of length $3^{-k}$ in $[0,1]$, let $$[0,1]\setminus C_k=\bigcup_{j=1}^{2^k-1}J_{k,j}$$ where $J_{k,j}$ are pairwise disjoint open intervals and each $J_{k,j}$ are located between intervals $I_{k,j},I_{k,j+1}$ define $$F_k:[0,1]\to[0,1]$$ where $$F_k(0)=0,F_k(1)=1,F_k(x)=\frac{j}{2^k}\quad x\in J_{k,j}$$ and $F_k$ is linear on each subinterval of $C_k$. We can show that $F_k$ converges to the Cantor function $F:[0,1]\to[0,1]$. Moreover, it can be shown that $$|F_k(x)-F_k(y)|\leq\frac{3^k}{2^k}|x-y|$$ How can we show that $F$ is Hölder continuous of order $\log_32$ ?","Let $C_k$ be the set obtained in the $k-$th stage of building the Cantor set, where $$C_1=[0,\frac{1}{3}]\cup[\frac{1}{3},\frac{2}{3}]$$ $$C_2=[0,\frac{1}{9}]\cup[\frac{2}{9},\frac{1}{3}]\cup[\frac{2}{3},\frac{7}{9}]\cup[\frac{8}{9},1]$$ $$\dots$$ So we have $$C_k=\bigcup_{j=1}^{2^k}I_{k,j}$$ where $I_{k,j}$ are pairwise disjoint closed intervals of length $3^{-k}$ in $[0,1]$, let $$[0,1]\setminus C_k=\bigcup_{j=1}^{2^k-1}J_{k,j}$$ where $J_{k,j}$ are pairwise disjoint open intervals and each $J_{k,j}$ are located between intervals $I_{k,j},I_{k,j+1}$ define $$F_k:[0,1]\to[0,1]$$ where $$F_k(0)=0,F_k(1)=1,F_k(x)=\frac{j}{2^k}\quad x\in J_{k,j}$$ and $F_k$ is linear on each subinterval of $C_k$. We can show that $F_k$ converges to the Cantor function $F:[0,1]\to[0,1]$. Moreover, it can be shown that $$|F_k(x)-F_k(y)|\leq\frac{3^k}{2^k}|x-y|$$ How can we show that $F$ is Hölder continuous of order $\log_32$ ?",,"['real-analysis', 'measure-theory', 'holder-spaces']"
79,A function is $L^2$-differentiable if and only if $\xi\widehat{f}(\xi) \in L^2$.,A function is -differentiable if and only if .,L^2 \xi\widehat{f}(\xi) \in L^2,"In this previous question, I defined $L^p$ derivatives of functions in $L^p(\mathbb{R}^n)$. I've been struggling for a while now to prove the following: If $f \in L^2(\mathbb{R})$, then $f$ is $L^2$-differentiable if and only if $\xi\widehat{f} \in L^2(\mathbb{R})$, where $\widehat{f}$ is the Fourier transform of $f$. When this happens, then $\widehat{f^{\prime}}(\xi) = 2\pi{}i\xi{}\widehat{f}(\xi)$. Here's my problem. Say that we're first trying to show that if $f'$ exists, then $\xi{}\widehat{f} \in L^2(\mathbb{R})$. Under this assumption, we know that $\widehat{f'}$ exists and is in $L^2$, so it would suffice to show that it equals $2\pi{}i\xi{}\widehat{f}$.  The naive approach would be to compute $$\widehat{f'}(\xi) = \int_{\mathbb{R}}f'(x)e^{\large -2\pi{}ix\xi}dx = ({2\pi{}ix\xi})\int_{\mathbb{R}}f(x)e^{\large -2\pi{}ix\xi}dx = ({2\pi{}ix\xi})\widehat{f}(\xi),$$ and we're done. But I believe (and please correct me if I'm wrong!) that there are two fundamental mistakes here: $1$) The Fourier transform of an $L^2$ function $f$ is not given by the formula above, but rather by a limit of Fourier transforms (in the above formula) of (say) $L^1 \cap L^2$ functions which converge (in $L^2$) to $f$. $2$) The integration by parts above only holds when the other function, namely $e^{\large -2\pi{}ix\xi}$, has compact support. Then I tried approximating $f'$ or $f$ by convolutions and the like, but I always arrive at a double integral which does not converge absolutely, and I'd have to interchange the order of integration. Please help me!","In this previous question, I defined $L^p$ derivatives of functions in $L^p(\mathbb{R}^n)$. I've been struggling for a while now to prove the following: If $f \in L^2(\mathbb{R})$, then $f$ is $L^2$-differentiable if and only if $\xi\widehat{f} \in L^2(\mathbb{R})$, where $\widehat{f}$ is the Fourier transform of $f$. When this happens, then $\widehat{f^{\prime}}(\xi) = 2\pi{}i\xi{}\widehat{f}(\xi)$. Here's my problem. Say that we're first trying to show that if $f'$ exists, then $\xi{}\widehat{f} \in L^2(\mathbb{R})$. Under this assumption, we know that $\widehat{f'}$ exists and is in $L^2$, so it would suffice to show that it equals $2\pi{}i\xi{}\widehat{f}$.  The naive approach would be to compute $$\widehat{f'}(\xi) = \int_{\mathbb{R}}f'(x)e^{\large -2\pi{}ix\xi}dx = ({2\pi{}ix\xi})\int_{\mathbb{R}}f(x)e^{\large -2\pi{}ix\xi}dx = ({2\pi{}ix\xi})\widehat{f}(\xi),$$ and we're done. But I believe (and please correct me if I'm wrong!) that there are two fundamental mistakes here: $1$) The Fourier transform of an $L^2$ function $f$ is not given by the formula above, but rather by a limit of Fourier transforms (in the above formula) of (say) $L^1 \cap L^2$ functions which converge (in $L^2$) to $f$. $2$) The integration by parts above only holds when the other function, namely $e^{\large -2\pi{}ix\xi}$, has compact support. Then I tried approximating $f'$ or $f$ by convolutions and the like, but I always arrive at a double integral which does not converge absolutely, and I'd have to interchange the order of integration. Please help me!",,"['real-analysis', 'functional-analysis', 'fourier-analysis']"
80,"Show f is uniformly continuous on $(a,b)$ if it is continuous and $\lim\limits_{x\to a^+}f(x)$ and $\lim\limits_{x\to b^-}f(x)$ exist",Show f is uniformly continuous on  if it is continuous and  and  exist,"(a,b) \lim\limits_{x\to a^+}f(x) \lim\limits_{x\to b^-}f(x)","Let $f:(a,b)\to\mathbb{R}$ be continuous at all $x\in(a,b)$. If $\lim\limits_{x\to b^-}f(x)$ and $\lim\limits_{x\to a^+}f(x)$ exist in $\mathbb R$, how can we prove that $f$ is uniformly continuous on $(a,b)$? This is my attempt, but I'm just not certain it is correct: Let $\epsilon>0$. It is clear that $[a+\epsilon,b-\epsilon]\subseteq(a,b)$. It is known that $f(x)$ is continuous on $(a,b)$, so it is uniformly continuous on the bounded interval $[a+\epsilon,b-\epsilon]$. But, $[a+\epsilon,b-\epsilon] \iff (a,b)$. Thus $f$ is uniformly continuous on $(a,b)$. Is my proof correct? Is there a better way?","Let $f:(a,b)\to\mathbb{R}$ be continuous at all $x\in(a,b)$. If $\lim\limits_{x\to b^-}f(x)$ and $\lim\limits_{x\to a^+}f(x)$ exist in $\mathbb R$, how can we prove that $f$ is uniformly continuous on $(a,b)$? This is my attempt, but I'm just not certain it is correct: Let $\epsilon>0$. It is clear that $[a+\epsilon,b-\epsilon]\subseteq(a,b)$. It is known that $f(x)$ is continuous on $(a,b)$, so it is uniformly continuous on the bounded interval $[a+\epsilon,b-\epsilon]$. But, $[a+\epsilon,b-\epsilon] \iff (a,b)$. Thus $f$ is uniformly continuous on $(a,b)$. Is my proof correct? Is there a better way?",,"['real-analysis', 'functions', 'continuity']"
81,Find the maximum and minimum of $\sum_{i=1}^{n-1}x_ix_{i+1}$ subject to $\sum_{i=1}^nx_i^2=1$.,Find the maximum and minimum of  subject to .,\sum_{i=1}^{n-1}x_ix_{i+1} \sum_{i=1}^nx_i^2=1,"Find the maximum and minimum of $$ \sum_{i=1}^{n-1}x_ix_{i+1} $$ subject to  $$ \sum_{i=1}^nx_i^2=1 $$ for all $n\in\mathbb{N}-\{1,0\}$.","Find the maximum and minimum of $$ \sum_{i=1}^{n-1}x_ix_{i+1} $$ subject to  $$ \sum_{i=1}^nx_i^2=1 $$ for all $n\in\mathbb{N}-\{1,0\}$.",,"['real-analysis', 'analysis', 'inequality', 'optimization', 'contest-math']"
82,Prove that: $ \int_{0}^{1} \ln \sqrt{\frac{1+\cos x}{1-\sin x}}\le \ln 2$,Prove that:, \int_{0}^{1} \ln \sqrt{\frac{1+\cos x}{1-\sin x}}\le \ln 2,"I plan to prove the following integral inequality: $$ \int_{0}^{1} \ln \sqrt{\frac{1+\cos x}{1-\sin x}}\le \ln 2$$ Since we have to deal with a convex function on this interval i thought of considering the area of the trapeze that can be formed if we unify the points $(0, f(0))$ and $(1, f(1))$, where the function $f(x) =\ln \sqrt{\frac{1+\cos x}{1-\sin x}}$,  but things are ugly even if the method itself isn't complicated. So, I'm looking for something better if possible.","I plan to prove the following integral inequality: $$ \int_{0}^{1} \ln \sqrt{\frac{1+\cos x}{1-\sin x}}\le \ln 2$$ Since we have to deal with a convex function on this interval i thought of considering the area of the trapeze that can be formed if we unify the points $(0, f(0))$ and $(1, f(1))$, where the function $f(x) =\ln \sqrt{\frac{1+\cos x}{1-\sin x}}$,  but things are ugly even if the method itself isn't complicated. So, I'm looking for something better if possible.",,"['calculus', 'real-analysis', 'integration', 'inequality', 'definite-integrals']"
83,Convex functions in integral inequality,Convex functions in integral inequality,,"Let $\mu,\sigma>0$ and define the function $f$ as follows: $$ f(x) = \frac{1}{\sigma\sqrt{2\pi}}\mathrm \exp\left(-\frac{(x-\mu)^2}{2\sigma ^2}\right) $$ How can I show that $$ \int\limits_{-\infty}^\infty x\log|x|f(x)\mathrm dx\geq \underbrace{\left(\int\limits_{-\infty}^\infty x f(x)\mathrm dx\right)}_\mu\cdot\left(\int\limits_{-\infty}^\infty \log|x| f(x)\mathrm dx\right) $$ which is also equivalent to $\mathsf E[ X\log|X|]\geq \underbrace{\mathsf EX}_\mu\cdot\mathsf E\log|X|$ for a random variable $X\sim\mathscr N(\mu,\sigma^2).$","Let $\mu,\sigma>0$ and define the function $f$ as follows: $$ f(x) = \frac{1}{\sigma\sqrt{2\pi}}\mathrm \exp\left(-\frac{(x-\mu)^2}{2\sigma ^2}\right) $$ How can I show that $$ \int\limits_{-\infty}^\infty x\log|x|f(x)\mathrm dx\geq \underbrace{\left(\int\limits_{-\infty}^\infty x f(x)\mathrm dx\right)}_\mu\cdot\left(\int\limits_{-\infty}^\infty \log|x| f(x)\mathrm dx\right) $$ which is also equivalent to $\mathsf E[ X\log|X|]\geq \underbrace{\mathsf EX}_\mu\cdot\mathsf E\log|X|$ for a random variable $X\sim\mathscr N(\mu,\sigma^2).$",,"['real-analysis', 'probability', 'inequality']"
84,"Evaluate $\int_{0}^{1}\frac{K(x)\ln\left(1-x^2\right)}{\sqrt{1-x^2}}\text{d}x,\int_{0}^{1}\frac{xK(x)^2\ln\left(1-x^2\right)}{\sqrt{1-x^2}}\text{d}x$",Evaluate,"\int_{0}^{1}\frac{K(x)\ln\left(1-x^2\right)}{\sqrt{1-x^2}}\text{d}x,\int_{0}^{1}\frac{xK(x)^2\ln\left(1-x^2\right)}{\sqrt{1-x^2}}\text{d}x","I am recently interested in integrals containing an elliptic integral $K(x)$ , which is defined by $\int_{0}^{1} \frac{1}{\sqrt{1-t^2}\sqrt{1-x^2t^2}  }\text{d}t$ for $|x|<1$ and $x$ is the elliptic modulus. Then I come across the following two integrals $$ \begin{aligned} &\int_{0}^{1} \frac{K(x)\ln\left ( 1-x^2 \right ) }{ \sqrt{1-x^2} }\text{d}x=-\frac{\Gamma\left(\frac14\right)^4}{24},\\ &\int_{0}^{1} \frac{xK(x)^2\ln\left ( 1-x^2 \right ) }{ \sqrt{1-x^2} }\text{d}x=-\frac{\pi^4}{4}\,_4F_3\left ( \frac12,\frac12,\frac12,\frac12;1,1,1;1 \right ). \end{aligned} $$ by estimation. Where generalized hypergeometric function ( $\,_pF_q$ ) is used. For the first one, I write $$ \begin{aligned} \int_{0}^{1} \frac{K(x)\ln\left ( 1-x^2 \right ) }{ \sqrt{1-x^2} }\text{d}x &=\frac{\pi}{2} \frac{\mathrm{d}}{\mathrm{d}n} \frac{\sqrt{\pi}\,\Gamma\left ( \frac{n+2}{2}  \right ) }{ \Gamma\left ( \frac{n+3}{2}  \right ) }\,_3F_2 \left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right )\Bigg|_{n=-1}\\ &=\frac{\pi}{2}\left [ -\pi\ln(2)\cdot\frac{\pi}{\Gamma\left ( \frac34 \right )^2 }  +\pi \frac{\mathrm{d}}{\mathrm{d} n} \,_3F_2 \left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right )\Bigg|_{n=-1}\right ] . \end{aligned} $$ Edit: We just need to prove $$ \,_3F_2 \left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right ) \Bigg|_{n=-1} =-\frac{(\pi-3\ln(2))\Gamma\left ( \frac14 \right )^4 }{12\pi^3}. $$ Or we equivalently write( $H_n$ denotes harmonic numbers) $$ \sum_{n=0}^{\infty} \frac{\left ( \frac12 \right )_n^3 }{ (n!)^3}H_n=\frac{(\pi-3\ln(2))\Gamma\left ( \frac14 \right )^4 }{6\pi^3}. $$ Which has been evaluated here . Question : Can we verify these closed-forms? Thanks for reaching my hand.","I am recently interested in integrals containing an elliptic integral , which is defined by for and is the elliptic modulus. Then I come across the following two integrals by estimation. Where generalized hypergeometric function ( ) is used. For the first one, I write Edit: We just need to prove Or we equivalently write( denotes harmonic numbers) Which has been evaluated here . Question : Can we verify these closed-forms? Thanks for reaching my hand.","K(x) \int_{0}^{1} \frac{1}{\sqrt{1-t^2}\sqrt{1-x^2t^2}  }\text{d}t |x|<1 x 
\begin{aligned}
&\int_{0}^{1} \frac{K(x)\ln\left ( 1-x^2 \right ) }{
\sqrt{1-x^2} }\text{d}x=-\frac{\Gamma\left(\frac14\right)^4}{24},\\
&\int_{0}^{1} \frac{xK(x)^2\ln\left ( 1-x^2 \right ) }{
\sqrt{1-x^2} }\text{d}x=-\frac{\pi^4}{4}\,_4F_3\left ( \frac12,\frac12,\frac12,\frac12;1,1,1;1 \right ).
\end{aligned}
 \,_pF_q 
\begin{aligned}
\int_{0}^{1} \frac{K(x)\ln\left ( 1-x^2 \right ) }{
\sqrt{1-x^2} }\text{d}x
&=\frac{\pi}{2} \frac{\mathrm{d}}{\mathrm{d}n} \frac{\sqrt{\pi}\,\Gamma\left ( \frac{n+2}{2}  \right ) }{
\Gamma\left ( \frac{n+3}{2}  \right ) }\,_3F_2
\left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right )\Bigg|_{n=-1}\\
&=\frac{\pi}{2}\left [ -\pi\ln(2)\cdot\frac{\pi}{\Gamma\left ( \frac34 \right )^2 } 
+\pi \frac{\mathrm{d}}{\mathrm{d} n} \,_3F_2
\left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right )\Bigg|_{n=-1}\right ] .
\end{aligned}
 
\,_3F_2
\left ( \frac12,\frac12,\frac12;1,\frac{3+n}{2} ;1 \right )
\Bigg|_{n=-1}
=-\frac{(\pi-3\ln(2))\Gamma\left ( \frac14 \right )^4 }{12\pi^3}.
 H_n 
\sum_{n=0}^{\infty} \frac{\left ( \frac12 \right )_n^3 }{
(n!)^3}H_n=\frac{(\pi-3\ln(2))\Gamma\left ( \frac14 \right )^4 }{6\pi^3}.
","['real-analysis', 'calculus', 'integration', 'hypergeometric-function', 'elliptic-integrals']"
85,Can rationals be approximated by increasingly large-denominator rationals?,Can rationals be approximated by increasingly large-denominator rationals?,,"Lets denote by $A_n$ the set of all relatively prime fractions with denominator $n\,.$ Then one should observe that the members of $A_n$ become more densely populated in the real line as $n \to \infty$ . To see this, note that $$A_1 = \mathbb{Z}, A_2 = \Big\{\frac{1}{2}, \frac{3}{2}, \dots,\Big\},\dots,A_{56} = \Big\{\frac{1}{56},\frac{3}{56}, \frac{5}{56}, \dots\Big\},\; \dots$$ Thus, it would seem intuitively true that the statement I have proposed is true, but seeing how I could find $N \in \mathbb{N}$ so that this is true is elusive to me. Any suggestions here?","Lets denote by the set of all relatively prime fractions with denominator Then one should observe that the members of become more densely populated in the real line as . To see this, note that Thus, it would seem intuitively true that the statement I have proposed is true, but seeing how I could find so that this is true is elusive to me. Any suggestions here?","A_n n\,. A_n n \to \infty A_1 = \mathbb{Z}, A_2 = \Big\{\frac{1}{2}, \frac{3}{2}, \dots,\Big\},\dots,A_{56} = \Big\{\frac{1}{56},\frac{3}{56}, \frac{5}{56}, \dots\Big\},\; \dots N \in \mathbb{N}","['real-analysis', 'elementary-number-theory']"
86,How to compute $\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx$ or $\sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}$,How to compute  or,\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx \sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}},How to tackle $$I=\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx\ ?$$ This integral came up while I was working on finding $\sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}$ . First attempt : By writing $\text{Li}_2(x^2)=-\int_0^1\frac{x^2\ln(y)}{1-x^2y}dy$ we have $$I=-\int_0^1\ln(y)\left(\int_0^1\frac{x\arcsin^2(x)}{1-x^2y}dx\right)dy$$ and Mathematica gave a complicated expression for the inner integral and that made me stop. Second attempt: $x=\sin\theta$ $$I=\int_0^{\pi/2}\theta^2\cot\theta\ \text{Li}_2(\sin^2\theta)d\theta$$ $$=\sum_{n=1}^\infty\frac{1}{n^2}\int_0^{\pi/2}\theta^2\cot\theta \sin^{2n}(\theta) d\theta$$ and I have no idea how to continue. Any suggestion? Thanks How $I$ appeared in my calculations: Since $$\frac{\arcsin x}{\sqrt{1-x^2}}=\sum_{n=1}^\infty\frac{(2x)^{2n-1}}{n{2n\choose n}}$$ we can write $$\frac{2\sqrt{x}\arcsin \sqrt{x}}{\sqrt{1-x}}=\sum_{n=1}^\infty\frac{2^{2n}x^{n}}{n{2n\choose n}}$$ Divide both sides by $x$ then $\int_0^y$ we have $$\sum_{n=1}^\infty\frac{2^{2n}y^n}{n^2{2n\choose n}}=2\int_0^y \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}dx$$ Next multiply both sides by $\frac{\text{Li}_2(y)}{y}$ then $\sum_{n=1}^\infty$ and use that $\int_0^1 y^{n-1}\text{Li}_2(y)dy=\frac{\zeta(2)}{n^2}-\frac{H_n}{n^2}$ we get $$\sum_{n=1}^\infty\frac{\zeta(2)2^{2n}}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^4{2n\choose n}}=2\int_0^1\int_0^y \frac{\arcsin \sqrt{x}\text{Li}_2(y)}{y\sqrt{x}\sqrt{1-x}}dxdy$$ $$=2\int_0^1 \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\int_x^1\frac{\text{Li}_2(y)}{y}dy\right)dx$$ $$=2\int_0^1 \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\zeta(3)-\text{Li}_3(x)\right)dx$$ $$\overset{\sqrt{x}\to x}{=}4\int_0^1\frac{\arcsin x}{\sqrt{1-x^2}}(\zeta(3)-\text{Li}_3(x^2))dx$$ $$\overset{\text{IBP}}{=}4\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx$$ Substitute $\sum_{n=1}^\infty\frac{\zeta(2)2^{2n}}{n^3{2n\choose n}}=15\ln(2)\zeta(4)-\frac72\zeta(2)\zeta(3)$ we get $$\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^4{2n\choose n}}=15\ln(2)\zeta(4)-\frac72\zeta(2)\zeta(3)-4\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx$$,How to tackle This integral came up while I was working on finding . First attempt : By writing we have and Mathematica gave a complicated expression for the inner integral and that made me stop. Second attempt: and I have no idea how to continue. Any suggestion? Thanks How appeared in my calculations: Since we can write Divide both sides by then we have Next multiply both sides by then and use that we get Substitute we get,I=\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx\ ? \sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}} \text{Li}_2(x^2)=-\int_0^1\frac{x^2\ln(y)}{1-x^2y}dy I=-\int_0^1\ln(y)\left(\int_0^1\frac{x\arcsin^2(x)}{1-x^2y}dx\right)dy x=\sin\theta I=\int_0^{\pi/2}\theta^2\cot\theta\ \text{Li}_2(\sin^2\theta)d\theta =\sum_{n=1}^\infty\frac{1}{n^2}\int_0^{\pi/2}\theta^2\cot\theta \sin^{2n}(\theta) d\theta I \frac{\arcsin x}{\sqrt{1-x^2}}=\sum_{n=1}^\infty\frac{(2x)^{2n-1}}{n{2n\choose n}} \frac{2\sqrt{x}\arcsin \sqrt{x}}{\sqrt{1-x}}=\sum_{n=1}^\infty\frac{2^{2n}x^{n}}{n{2n\choose n}} x \int_0^y \sum_{n=1}^\infty\frac{2^{2n}y^n}{n^2{2n\choose n}}=2\int_0^y \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}dx \frac{\text{Li}_2(y)}{y} \sum_{n=1}^\infty \int_0^1 y^{n-1}\text{Li}_2(y)dy=\frac{\zeta(2)}{n^2}-\frac{H_n}{n^2} \sum_{n=1}^\infty\frac{\zeta(2)2^{2n}}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^4{2n\choose n}}=2\int_0^1\int_0^y \frac{\arcsin \sqrt{x}\text{Li}_2(y)}{y\sqrt{x}\sqrt{1-x}}dxdy =2\int_0^1 \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\int_x^1\frac{\text{Li}_2(y)}{y}dy\right)dx =2\int_0^1 \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\zeta(3)-\text{Li}_3(x)\right)dx \overset{\sqrt{x}\to x}{=}4\int_0^1\frac{\arcsin x}{\sqrt{1-x^2}}(\zeta(3)-\text{Li}_3(x^2))dx \overset{\text{IBP}}{=}4\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx \sum_{n=1}^\infty\frac{\zeta(2)2^{2n}}{n^3{2n\choose n}}=15\ln(2)\zeta(4)-\frac72\zeta(2)\zeta(3) \sum_{n=1}^\infty\frac{2^{2n}H_n}{n^4{2n\choose n}}=15\ln(2)\zeta(4)-\frac72\zeta(2)\zeta(3)-4\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx,"['real-analysis', 'integration', 'trigonometry', 'harmonic-numbers', 'polylogarithm']"
87,Towards a Little proof of Fermat's last theorem,Towards a Little proof of Fermat's last theorem,,"A final version of this article was posted here , on 1/29/2020. Question: can you check if my reasoning below makes sense and has no major flaws? Update : I fixed an issue in my definition of $G$ : we must exclude $u=w$ and $v=w$ . This has impacts on the charts too, with the new definition of $G$ . I don't claim to have a proof here, just a potential path to a proof, and it is by no means elementary if one wants to make my arguments mathematically rigorous. It might look like what Fermat could have written when saying ""my proof is too long to fit in the margin of my letter"". Certainly, Fermat did not get a proof either. At best, I think you can (maybe) derive from my discussion below, that the number of solutions (if any) is bounded in certain ways -- a much weaker result than Andrew Wiles' final solution to this problem. But I don't think there are flaws in my reasoning, contrarily to most would-be ""simple proofs"" regularly published and based on high-school arithmetic, such as here . Hopefully, my perspective here brings some new light on this 300-old problem, and the methodology could be applied to other Diophantine equations. Anyway, here is how it goes. We are interested in solving $$u^n + v^n = w^n$$ where $u, v, w > 0$ are integers, and $n>2$ is an integer. We start with the following generating function: $$G_M(x) = \frac{1}{M^\alpha}\sum_{0<u,v,w\leq M, \\ u\neq w, v \neq w} x^{(u^n+v^n-w^n)^2}.$$ It is still unclear to me if $\alpha$ should be $0$ , I am still doing research on this.  This function has a Taylor series expansion $$G_M(x) = \sum_{k=0}^\infty h_k x^{k^2},$$ where $h_k$ is the number of ways (combinations of $u, v, w$ ) that $k$ can be written as $k=u^n + v^n - w^n$ . We all know that if $n>2$ , then $h_0 = 0$ regardless of $M$ (that's Fermat's Last Theorem.) If $n=3,\alpha=0$ and $M=100$ , then $h_1=4$ , as we have $(6^3 + 8^3 - 9^3)^2 = 1$ $(8^3 + 6^3 - 9^3)^2 = 1$ $(9^3 + 10^3 - 12^3)^2 = 1$ $(10^3 + 9^3 - 12^3)^2 = 1$ If $n=3,\alpha=0$ and $M=200$ , then $h_1=12$ : in addition to the four previous solutions, we also have $(64^3 + 94^3 - 103^3)^2 = 1$ $(94^3 + 64^3 - 103^3)^2 = 1$ $(71^3 + 138^3 - 144^3)^2 = 1$ $(138^3 + 71^3 - 144^3)^2 = 1$ $(73^3 + 144^3 - 150^3)^2 = 1$ $(144^3 + 73^3 - 150^3)^2 = 1$ $(138^3 + 175^3 - 172^3)^2 = 1$ $(175^3 + 138^3 - 172^3)^2 = 1$ If $h_1\rightarrow\infty$ as $M\rightarrow\infty$ and the growth follows a power law ( $h_1 \sim M^\alpha$ ), then we must have $\alpha\neq 0$ . Note that $h_2$ could follow a power low with a different $\alpha$ , this is a tricky problem. But at first glance, there seems to be enough smoothness in the way growth occurs among $h_0, h_1, h_2$ and so on, so that it is possible to find a suitable candidate for $\alpha$ . Indeed a simple rule consists in choosing $\alpha$ such that $G_M(\frac{1}{2}) = 1$ , always . Table for the coefficients $h_k$ Assuming $n=3, \alpha=0$ . The table reads as follows (example): $$G_{800}(x) = 24 x + 10x^4 + x^9 + 7 x^{36} + 4 x^{49}+30 x^{64}+\cdots$$ Main fact : There is no solution to $u^n+v^n=w^n$ (with $0<u,v,w\leq M$ ) if and only if $G_M(0) = 0$ . This result is trivial. Here $n$ is assumed to be fixed. Of course we are interested in $$G(x) = \lim_{M\rightarrow\infty} G_M(x), \mbox{ for } |x|<1.$$ First, note that the case $n=2$ leads to a singularity, and $G$ does not exist if $n=2$ , at least not with $\alpha=0$ (but maybe with $\alpha=1$ ). Also $n$ can be a real number, but it must be larger than $2$ . For instance, it seems that $n=2.5$ works, in the sense that it does not lead to a singularity for $G$ . Also, we are interested in $x$ close to zero, say $-0.5\leq x \leq 0.5$ . Finally, $G(x)$ is properly defined (to be proved, may not be easy!) if $|x|<1$ and $n>2$ . If $n$ is not an integer, there is no Taylor approximation for $G_M$ , as the successive powers in the Taylor expansion would be positive real numbers, but not integers (in that case it means $G_M(x)$ is defined only for $0\leq x <1$ .) Below is the plot for $G_M(x)$ with $-0.5<x<0.5, n = 3,\alpha=0$ and $M=200$ . Note that as $M\rightarrow\infty$ , the function $G_M$ tends to a straight line around $x=0$ , with $G(0)=0$ . This suggests that if there are solutions to $u^n + v^n = z^n$ , with $n=3$ , then the number of solutions must be $o(M)$ . The same is true if you plot the same chart for any $n>2$ . Of course, this assumes that $G$ does not have a singularity at $x=0$ . Also, if some $(u,v,w)$ is a solution, any multiple is also a solution: so the number of solutions should be at least $O(M)$ . This suggests that indeed, no solution exists. By contrast, the plot below corresponds to $n=2, \alpha=0, M = 200$ . Clearly, $G_M(0) > 0$ , proving that $u^2 + v^2 = w^2$ has many, many solutions, even for $0<u,v,w\leq 200$ . Below is the source code (Perl) used to compute $G_M$ . It is easy to implement it in a distributed environment. $M=200; $ n=2; $alpha=0;    for ( $u=1; $ u<= $M; $ u++) {   for ( $v=1; $ v<= $M; $ v++) {     for ( $w=1; $ w<= $M; $ w++) {       if (( $u != $ w) && ( $v != $ w)) { $z=($ u** $n+$ v** $n-$ w** $n)**2;         $ hash{$z}++;       }     }   } }   open(OUT,"">fermat.txt""); for ( $x=-0.5; $ x<=0.5; $x+=0.01) {   $G=0;   foreach $z (keys(%hash)) {     if ($z<20) { $G+=$hash{$z}*($x**$z); }   }   $G=$G/($M**$alpha);   print OUT ""$x\t$ G\n""; } close(OUT); This code is running very slowly because it generates a huge hash table. If we are only interested in the first few coefficients $h_k$ 's, then the following change in the triple loop significantly improves the speed of the calculations: for ( $u=1; $ u<= $M; $ u++) {   for ( $v=1; $ v<= $M; $ v++) {     for ( $w=1; $ w<= $M; $ w++) {       if (( $u != $ w) && ( $v != $ w)) { $z=($ u** $n+$ v** $n-$ w** $n)**2;         if ($ z < 2000) { $hash{$ z}++;         }       }     }   } } Note: I did this work not because of my interest in Fermat's last theorem, but as I was exploring generating functions for sums of squares. The methodology is similar in both cases, though a little simpler for sums of squares.","A final version of this article was posted here , on 1/29/2020. Question: can you check if my reasoning below makes sense and has no major flaws? Update : I fixed an issue in my definition of : we must exclude and . This has impacts on the charts too, with the new definition of . I don't claim to have a proof here, just a potential path to a proof, and it is by no means elementary if one wants to make my arguments mathematically rigorous. It might look like what Fermat could have written when saying ""my proof is too long to fit in the margin of my letter"". Certainly, Fermat did not get a proof either. At best, I think you can (maybe) derive from my discussion below, that the number of solutions (if any) is bounded in certain ways -- a much weaker result than Andrew Wiles' final solution to this problem. But I don't think there are flaws in my reasoning, contrarily to most would-be ""simple proofs"" regularly published and based on high-school arithmetic, such as here . Hopefully, my perspective here brings some new light on this 300-old problem, and the methodology could be applied to other Diophantine equations. Anyway, here is how it goes. We are interested in solving where are integers, and is an integer. We start with the following generating function: It is still unclear to me if should be , I am still doing research on this.  This function has a Taylor series expansion where is the number of ways (combinations of ) that can be written as . We all know that if , then regardless of (that's Fermat's Last Theorem.) If and , then , as we have If and , then : in addition to the four previous solutions, we also have If as and the growth follows a power law ( ), then we must have . Note that could follow a power low with a different , this is a tricky problem. But at first glance, there seems to be enough smoothness in the way growth occurs among and so on, so that it is possible to find a suitable candidate for . Indeed a simple rule consists in choosing such that , always . Table for the coefficients Assuming . The table reads as follows (example): Main fact : There is no solution to (with ) if and only if . This result is trivial. Here is assumed to be fixed. Of course we are interested in First, note that the case leads to a singularity, and does not exist if , at least not with (but maybe with ). Also can be a real number, but it must be larger than . For instance, it seems that works, in the sense that it does not lead to a singularity for . Also, we are interested in close to zero, say . Finally, is properly defined (to be proved, may not be easy!) if and . If is not an integer, there is no Taylor approximation for , as the successive powers in the Taylor expansion would be positive real numbers, but not integers (in that case it means is defined only for .) Below is the plot for with and . Note that as , the function tends to a straight line around , with . This suggests that if there are solutions to , with , then the number of solutions must be . The same is true if you plot the same chart for any . Of course, this assumes that does not have a singularity at . Also, if some is a solution, any multiple is also a solution: so the number of solutions should be at least . This suggests that indeed, no solution exists. By contrast, the plot below corresponds to . Clearly, , proving that has many, many solutions, even for . Below is the source code (Perl) used to compute . It is easy to implement it in a distributed environment. n=2; $alpha=0;    for ( u<= u++) {   for ( v<= v++) {     for ( w<= w++) {       if (( w) && ( w)) { u** v** w** hash{$z}++;       }     }   } }   open(OUT,"">fermat.txt""); for ( x<=0.5; G\n""; } close(OUT); This code is running very slowly because it generates a huge hash table. If we are only interested in the first few coefficients 's, then the following change in the triple loop significantly improves the speed of the calculations: for ( u<= u++) {   for ( v<= v++) {     for ( w<= w++) {       if (( w) && ( w)) { u** v** w** z < 2000) { z}++;         }       }     }   } } Note: I did this work not because of my interest in Fermat's last theorem, but as I was exploring generating functions for sums of squares. The methodology is similar in both cases, though a little simpler for sums of squares.","G u=w v=w G u^n + v^n = w^n u, v, w > 0 n>2 G_M(x) = \frac{1}{M^\alpha}\sum_{0<u,v,w\leq M, \\ u\neq w, v \neq w} x^{(u^n+v^n-w^n)^2}. \alpha 0 G_M(x) = \sum_{k=0}^\infty h_k x^{k^2}, h_k u, v, w k k=u^n + v^n - w^n n>2 h_0 = 0 M n=3,\alpha=0 M=100 h_1=4 (6^3 + 8^3 - 9^3)^2 = 1 (8^3 + 6^3 - 9^3)^2 = 1 (9^3 + 10^3 - 12^3)^2 = 1 (10^3 + 9^3 - 12^3)^2 = 1 n=3,\alpha=0 M=200 h_1=12 (64^3 + 94^3 - 103^3)^2 = 1 (94^3 + 64^3 - 103^3)^2 = 1 (71^3 + 138^3 - 144^3)^2 = 1 (138^3 + 71^3 - 144^3)^2 = 1 (73^3 + 144^3 - 150^3)^2 = 1 (144^3 + 73^3 - 150^3)^2 = 1 (138^3 + 175^3 - 172^3)^2 = 1 (175^3 + 138^3 - 172^3)^2 = 1 h_1\rightarrow\infty M\rightarrow\infty h_1 \sim M^\alpha \alpha\neq 0 h_2 \alpha h_0, h_1, h_2 \alpha \alpha G_M(\frac{1}{2}) = 1 h_k n=3, \alpha=0 G_{800}(x) = 24 x + 10x^4 + x^9 + 7 x^{36} + 4 x^{49}+30 x^{64}+\cdots u^n+v^n=w^n 0<u,v,w\leq M G_M(0) = 0 n G(x) = \lim_{M\rightarrow\infty} G_M(x), \mbox{ for } |x|<1. n=2 G n=2 \alpha=0 \alpha=1 n 2 n=2.5 G x -0.5\leq x \leq 0.5 G(x) |x|<1 n>2 n G_M G_M(x) 0\leq x <1 G_M(x) -0.5<x<0.5, n = 3,\alpha=0 M=200 M\rightarrow\infty G_M x=0 G(0)=0 u^n + v^n = z^n n=3 o(M) n>2 G x=0 (u,v,w) O(M) n=2, \alpha=0, M = 200 G_M(0) > 0 u^2 + v^2 = w^2 0<u,v,w\leq 200 G_M M=200;
 u=1;  M;  v=1;  M;  w=1;  M;  u !=  v !=  z=( n+ n- n)**2;
         x=-0.5;  x+=0.01) {
  G=0;
  foreach z (keys(%hash)) {
    if (z<20) { G+=hash{z}*(x**z); }
  }
  G=G/(M**alpha);
  print OUT ""x\t h_k u=1;  M;  v=1;  M;  w=1;  M;  u !=  v !=  z=( n+ n- n)**2;
        if ( hash{","['real-analysis', 'number-theory', 'diophantine-equations', 'generating-functions']"
88,"Rudin Exercise 2.7: union of subsets of a metric space, and closure thereof","Rudin Exercise 2.7: union of subsets of a metric space, and closure thereof",,"I am trying to solve exercise 7 in Chapter 2 of Rudin and was hoping someone could look over my proof. Let $A_1, A_2, A_3, \ldots$ be subsets of a metric space. a) Let $B_n = \bigcup\limits_{i=1}^n A_i$ , prove that $\overline{B}_n = \bigcup\limits_{i=1}^n \overline{A}_i$ , for $n = 1, 2, 3, \ldots$ b) If $B = \bigcup\limits_{i=1}^{\infty} A_i$ , prove that $\overline{B} \supset \bigcup\limits_{i=1}^{\infty} \overline{A}_i$ . Show, by an example, that this inclusion can be proper. Here is my attempt. a) The closure of a set is the smallest closed set containing it. Thus, for all $i$ , $\overline{A}_i$ is closed. Further, the finite union of closed sets is closed. Thus, $\bigcup\limits_{i=1}^n \overline{A}_i$ is closed. Furthermore, $\overline{A}_i = A_i \cup (A_i)'$ , where $(A_i)'$ is the set of limit points of $A$ . Thus, $A_i \subset \overline{A}_i$ for all $i$ , which implies that $\bigcup\limits_{i=1}^n A_i \subset \bigcup\limits_{i=1}^n \overline{A}_i$ , i.e., $B_n \subset \bigcup\limits_{i=1}^n \overline{A}_i$ . By Theorem $2.27$ , for any metric space $X$ where $E, F \subset X$ , if $E \subset F$ where $F$ is closed, then $\overline{E} \subset F$ . Therefore, we deduce that $\overline{B}_n \subset \bigcup\limits_{i=1}^n \overline{A}_i$ . Furthermore, $A_i \subset \bigcup\limits_{i=1}^n A_i$ for any $i$ , meaning that $A_i \subset B_n$ for any $i$ . But $B_n \subset \overline{B}_n$ , meaning that \begin{align*} A_i \subset B_n \subset \overline{B}_n, \end{align*} i.e., $A_i \subset \overline{B}_n$ , where $\overline{B}_n$ is closed. Thus, Theorem 2.27 gives that $\overline{A}_i \subset \overline{B}_n$ for any $i$ , and hence that $\bigcup\limits_{i=1}^n \overline{A}_i \subset \overline{B}_n$ . Thus, $\overline{B}_n \subset \bigcup\limits_{i=1}^n \overline{A}_i$ and $\bigcup\limits_{i=1}^n \overline{A}_i \subset \overline{B}_n$ , so $\overline{B}_n = \bigcup\limits_{i=1}^n \overline{A}_i$ . b) Let $B = \bigcup\limits_{i=1}^{\infty} A_i$ . Since $A_i \subset \bigcup\limits_{i=1}^{\infty} A_i$ , $A_i \subset B$ . But $B \subset \overline{B}$ , so \begin{align*} A_i \subset B \subset \overline{B}, \end{align*} hence, \begin{align*} A_i \subset \overline{B}. \end{align*} But $\overline{B}$ is closed, so by Theorem $2.27$ , we have \begin{align*} \overline{A}_i \subset \overline{B}, \end{align*} for any $i$ , which implies that \begin{align*} \bigcup\limits_{i=1}^{\infty} \overline{A}_i \subset \overline{B}, \end{align*} which can be written as \begin{align*} \overline{B} \supset \bigcup\limits_{i=1}^{\infty} \overline{A}_i. \end{align*} As for an example to show that this inclusion can be proper, let us consider: \begin{align*} A_i = \left[\frac{1}{i}, 1\right], \end{align*} in which case each $A_i$ is closed, so $\overline{A}_i = A_i$ , meaning that their infinite unions are the same. That is, \begin{align*} \bigcup\limits_{i=1}^{\infty} A_i = \bigcup\limits_{i=1}^{\infty} \overline{A}_i.  \end{align*} However, we have: \begin{align*} \bigcup\limits_{i=1}^{\infty} A_i = \bigcup\limits_{i=1}^{\infty} \left[\frac{1}{i}, 1\right] = (0,1].  \end{align*} However, for the left-hand side, we get: \begin{align*} B = \bigcup\limits_{i=1}^{\infty} A_i \implies \overline{B} = \overline{\bigcup\limits_{i=1}^{\infty} A_i} = \overline{(0,1]} = [0,1]. \end{align*} Since $(0,1] \subset [0,1]$ , $\bigcup\limits_{i=1}^{\infty} \overline{A_i}$ is properly contained in $B$ . Any feedback would be greatly appreciated.","I am trying to solve exercise 7 in Chapter 2 of Rudin and was hoping someone could look over my proof. Let be subsets of a metric space. a) Let , prove that , for b) If , prove that . Show, by an example, that this inclusion can be proper. Here is my attempt. a) The closure of a set is the smallest closed set containing it. Thus, for all , is closed. Further, the finite union of closed sets is closed. Thus, is closed. Furthermore, , where is the set of limit points of . Thus, for all , which implies that , i.e., . By Theorem , for any metric space where , if where is closed, then . Therefore, we deduce that . Furthermore, for any , meaning that for any . But , meaning that i.e., , where is closed. Thus, Theorem 2.27 gives that for any , and hence that . Thus, and , so . b) Let . Since , . But , so hence, But is closed, so by Theorem , we have for any , which implies that which can be written as As for an example to show that this inclusion can be proper, let us consider: in which case each is closed, so , meaning that their infinite unions are the same. That is, However, we have: However, for the left-hand side, we get: Since , is properly contained in . Any feedback would be greatly appreciated.","A_1, A_2, A_3, \ldots B_n = \bigcup\limits_{i=1}^n A_i \overline{B}_n = \bigcup\limits_{i=1}^n \overline{A}_i n = 1, 2, 3, \ldots B = \bigcup\limits_{i=1}^{\infty} A_i \overline{B} \supset \bigcup\limits_{i=1}^{\infty} \overline{A}_i i \overline{A}_i \bigcup\limits_{i=1}^n \overline{A}_i \overline{A}_i = A_i \cup (A_i)' (A_i)' A A_i \subset \overline{A}_i i \bigcup\limits_{i=1}^n A_i \subset \bigcup\limits_{i=1}^n \overline{A}_i B_n \subset \bigcup\limits_{i=1}^n \overline{A}_i 2.27 X E, F \subset X E \subset F F \overline{E} \subset F \overline{B}_n \subset \bigcup\limits_{i=1}^n \overline{A}_i A_i \subset \bigcup\limits_{i=1}^n A_i i A_i \subset B_n i B_n \subset \overline{B}_n \begin{align*}
A_i \subset B_n \subset \overline{B}_n,
\end{align*} A_i \subset \overline{B}_n \overline{B}_n \overline{A}_i \subset \overline{B}_n i \bigcup\limits_{i=1}^n \overline{A}_i \subset \overline{B}_n \overline{B}_n \subset \bigcup\limits_{i=1}^n \overline{A}_i \bigcup\limits_{i=1}^n \overline{A}_i \subset \overline{B}_n \overline{B}_n = \bigcup\limits_{i=1}^n \overline{A}_i B = \bigcup\limits_{i=1}^{\infty} A_i A_i \subset \bigcup\limits_{i=1}^{\infty} A_i A_i \subset B B \subset \overline{B} \begin{align*}
A_i \subset B \subset \overline{B},
\end{align*} \begin{align*}
A_i \subset \overline{B}.
\end{align*} \overline{B} 2.27 \begin{align*}
\overline{A}_i \subset \overline{B},
\end{align*} i \begin{align*}
\bigcup\limits_{i=1}^{\infty} \overline{A}_i \subset \overline{B},
\end{align*} \begin{align*}
\overline{B} \supset \bigcup\limits_{i=1}^{\infty} \overline{A}_i.
\end{align*} \begin{align*}
A_i = \left[\frac{1}{i}, 1\right],
\end{align*} A_i \overline{A}_i = A_i \begin{align*}
\bigcup\limits_{i=1}^{\infty} A_i = \bigcup\limits_{i=1}^{\infty} \overline{A}_i. 
\end{align*} \begin{align*}
\bigcup\limits_{i=1}^{\infty} A_i = \bigcup\limits_{i=1}^{\infty} \left[\frac{1}{i}, 1\right] = (0,1]. 
\end{align*} \begin{align*}
B = \bigcup\limits_{i=1}^{\infty} A_i \implies \overline{B} = \overline{\bigcup\limits_{i=1}^{\infty} A_i} = \overline{(0,1]} = [0,1].
\end{align*} (0,1] \subset [0,1] \bigcup\limits_{i=1}^{\infty} \overline{A_i} B",['real-analysis']
89,Can a sequence converges modulo every r>0 but diverge?,Can a sequence converges modulo every r>0 but diverge?,,"Is it possible to have a sequence $\{x_n\}$ of real numbers which diverge to $\infty$ (and has no other finite limit points), but satisfy the condition that $x_n\pmod{r}$ converges for every real $r>0$ ? I'm aware of related results such as ""the fractional parts of $\{n\alpha\}_n$ are dense in $[0,1]$ (and thus do not converge)"" and more general versions of these statements, giving counterexamples for select values of $r$ . However, I'm wondering if the ""for every $r>0$ "" part of the statement makes the existence of such an $\{x_n\}$ impossible. I feel this is the case, but have been unable to come up with a rigorous proof.","Is it possible to have a sequence of real numbers which diverge to (and has no other finite limit points), but satisfy the condition that converges for every real ? I'm aware of related results such as ""the fractional parts of are dense in (and thus do not converge)"" and more general versions of these statements, giving counterexamples for select values of . However, I'm wondering if the ""for every "" part of the statement makes the existence of such an impossible. I feel this is the case, but have been unable to come up with a rigorous proof.","\{x_n\} \infty x_n\pmod{r} r>0 \{n\alpha\}_n [0,1] r r>0 \{x_n\}",['real-analysis']
90,Is it possible that $\sum a_{\sigma(n)}$ converges iff $\sum b_{\sigma(n)}$ diverges for every permutation $\sigma :\mathbb N\to \mathbb N$?,Is it possible that  converges iff  diverges for every permutation ?,\sum a_{\sigma(n)} \sum b_{\sigma(n)} \sigma :\mathbb N\to \mathbb N,"Here is my question: Is it possible to find a pair of series $\sum a_n, \sum b_n,$ each having rearrangements that converge conditionally, such that $\sum a_{\sigma(n)}$ converges iff $\sum b_{\sigma(n)}$ diverges for every permutation $\sigma :\mathbb N\to \mathbb N$? What if we consider more than just two sets (in finite case rest will probably follow from induction)? I intentionally do not divide in cases like ""B is divergent but has limit point on extended real number line"" since I think they will be probably similar, but please correct me if I am wrong. As always, feel free to retag.","Here is my question: Is it possible to find a pair of series $\sum a_n, \sum b_n,$ each having rearrangements that converge conditionally, such that $\sum a_{\sigma(n)}$ converges iff $\sum b_{\sigma(n)}$ diverges for every permutation $\sigma :\mathbb N\to \mathbb N$? What if we consider more than just two sets (in finite case rest will probably follow from induction)? I intentionally do not divide in cases like ""B is divergent but has limit point on extended real number line"" since I think they will be probably similar, but please correct me if I am wrong. As always, feel free to retag.",,"['real-analysis', 'sequences-and-series', 'conditional-convergence']"
91,Function satisfying $\lim_{x\to 0}\frac{f(ax)}{f(x)}=a$,Function satisfying,\lim_{x\to 0}\frac{f(ax)}{f(x)}=a,"In this answer https://math.stackexchange.com/a/2101475/72031 I proved that we can sometimes avoid the limit $$\lim_{x\to 0}\frac{\sin x} {x} = 1\tag{1}$$ and instead use the simpler limit $$\lim_{x\to 0}\cos x=1\tag{2}$$ to evaluate many limits. Specifically we can use $(2)$ to show that $$\lim_{x\to 0}\frac{\sin nx} {\sin x} =n\tag{3}$$ for all rational $n$. My contention is that the above limit can not be established for irrational values of $n$ just by using $(2)$ and it necessarily requires the use of $(1)$. Based on this thought I pose the following problem: Let $f:\mathbb{R} \to\mathbb{R} $ be continuous with $f(0)=0$ and $$\lim_{x\to 0}\frac{f(ax)}{f(x)}=a\tag{4}$$ for all non-zero real values of $a$. Does this imply that $f'(0)$ exist and is nonzero? I think this is false,  but I could not find an easy counter-example. So either a proof or a counter-example is desired.","In this answer https://math.stackexchange.com/a/2101475/72031 I proved that we can sometimes avoid the limit $$\lim_{x\to 0}\frac{\sin x} {x} = 1\tag{1}$$ and instead use the simpler limit $$\lim_{x\to 0}\cos x=1\tag{2}$$ to evaluate many limits. Specifically we can use $(2)$ to show that $$\lim_{x\to 0}\frac{\sin nx} {\sin x} =n\tag{3}$$ for all rational $n$. My contention is that the above limit can not be established for irrational values of $n$ just by using $(2)$ and it necessarily requires the use of $(1)$. Based on this thought I pose the following problem: Let $f:\mathbb{R} \to\mathbb{R} $ be continuous with $f(0)=0$ and $$\lim_{x\to 0}\frac{f(ax)}{f(x)}=a\tag{4}$$ for all non-zero real values of $a$. Does this imply that $f'(0)$ exist and is nonzero? I think this is false,  but I could not find an easy counter-example. So either a proof or a counter-example is desired.",,"['calculus', 'real-analysis']"
92,"$\{f_n\}$ is uniformly integrable if and only if $\sup_n \int |f_n|\,d\mu < \infty$ and $\{f_n\}$ is uniformly absolutely continuous?",is uniformly integrable if and only if  and  is uniformly absolutely continuous?,"\{f_n\} \sup_n \int |f_n|\,d\mu < \infty \{f_n\}","Let $(X, \mathcal{A}, \mu)$ be a measure space. A family of measurable functions $\{f_n\}$ is uniformly integrable if given $\epsilon$ there exists $M$ such that$$\int_{\{x : |f_n(x)| > M\}} |f_n(x)|\,d\mu < \epsilon$$for each $n$. The sequence is uniformly absolutely continuous if given $\epsilon$ there exists $\delta$ such that$$\left|\int_A f_n\,d\mu\right| < \epsilon$$for each $n$ if $\mu(A) < \delta$. Suppose $\mu$ is a finite measure. How do I see that $\{f_n\}$ is uniformly integrable if and only if $\sup_n \int |f_n|\,d\mu < \infty$ and $\{f_n\}$ is uniformly absolutely continuous?","Let $(X, \mathcal{A}, \mu)$ be a measure space. A family of measurable functions $\{f_n\}$ is uniformly integrable if given $\epsilon$ there exists $M$ such that$$\int_{\{x : |f_n(x)| > M\}} |f_n(x)|\,d\mu < \epsilon$$for each $n$. The sequence is uniformly absolutely continuous if given $\epsilon$ there exists $\delta$ such that$$\left|\int_A f_n\,d\mu\right| < \epsilon$$for each $n$ if $\mu(A) < \delta$. Suppose $\mu$ is a finite measure. How do I see that $\{f_n\}$ is uniformly integrable if and only if $\sup_n \int |f_n|\,d\mu < \infty$ and $\{f_n\}$ is uniformly absolutely continuous?",,"['real-analysis', 'integration', 'sequences-and-series', 'probability-theory', 'measure-theory']"
93,Prove that Lebesgue measurable set is the union of a Borel measurable set and a set of Lebesgue measure zero,Prove that Lebesgue measurable set is the union of a Borel measurable set and a set of Lebesgue measure zero,,"Let $A$ be a Lebesgue measurable subset of $\Bbb R$. 1) Show that there exists a Borel measurable subset $B$ of $\Bbb R$ such that $A\subseteq B$ and such that $l^*(B\setminus A)=0$. 2) Show that every Lebesgue measurable set is the union of a Borel measurable set (with same measure) and a set of Lebesgue measure zero. (Note that $l^*$ denote the outer measure. ) I already showed that if $C$ is a Lebesgue measurable subset of $\Bbb R$ and $\epsilon \gt 0$, then there exists an open set $G_{\epsilon}\supseteq C$ such that $l^*(C)\le l^*(G_{\epsilon})\le l^*(C)+\epsilon$. Moreover, if $D$ is a Lebesgue measurable subset of $\Bbb R$, if $\epsilon \gt 0$, and if $D\subseteq I_n=(n,n+1]$, then there exists a compact set $K_{\epsilon} \subseteq D$ such that $l^*(K_{\epsilon})\le l^*(D)\le l^*(K_{\epsilon})+\epsilon$. My thought is to somehow manipulate the inequality and have $\epsilon$ shrinking, but I'm stuck to show this and come up with the result. Could someone help to provide a proof please? Any help is appreciated. Thanks.","Let $A$ be a Lebesgue measurable subset of $\Bbb R$. 1) Show that there exists a Borel measurable subset $B$ of $\Bbb R$ such that $A\subseteq B$ and such that $l^*(B\setminus A)=0$. 2) Show that every Lebesgue measurable set is the union of a Borel measurable set (with same measure) and a set of Lebesgue measure zero. (Note that $l^*$ denote the outer measure. ) I already showed that if $C$ is a Lebesgue measurable subset of $\Bbb R$ and $\epsilon \gt 0$, then there exists an open set $G_{\epsilon}\supseteq C$ such that $l^*(C)\le l^*(G_{\epsilon})\le l^*(C)+\epsilon$. Moreover, if $D$ is a Lebesgue measurable subset of $\Bbb R$, if $\epsilon \gt 0$, and if $D\subseteq I_n=(n,n+1]$, then there exists a compact set $K_{\epsilon} \subseteq D$ such that $l^*(K_{\epsilon})\le l^*(D)\le l^*(K_{\epsilon})+\epsilon$. My thought is to somehow manipulate the inequality and have $\epsilon$ shrinking, but I'm stuck to show this and come up with the result. Could someone help to provide a proof please? Any help is appreciated. Thanks.",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
94,"$U\subset [0,\infty)$ is open and unbounded $\Rightarrow \exists x$ such that $U\cap \{nx;n\in \mathbb N\}$ is infinite.",is open and unbounded  such that  is infinite.,"U\subset [0,\infty) \Rightarrow \exists x U\cap \{nx;n\in \mathbb N\}","I want to show that: Let $U\subset [0,\infty)$ be open and unbounded. Show that there is a number $x\in (0,\infty)$ such that $U\cap \{nx;n\in \mathbb N\}$ is infinite. Because of $U$ is open, $U$ is a countable union of open intervals. if $U$ contain an interval $(a,\infty)$, we are done. But if all intervals which contain in $U$ was bounded, what we can do? can somebody give me a hint?","I want to show that: Let $U\subset [0,\infty)$ be open and unbounded. Show that there is a number $x\in (0,\infty)$ such that $U\cap \{nx;n\in \mathbb N\}$ is infinite. Because of $U$ is open, $U$ is a countable union of open intervals. if $U$ contain an interval $(a,\infty)$, we are done. But if all intervals which contain in $U$ was bounded, what we can do? can somebody give me a hint?",,"['real-analysis', 'general-topology', 'metric-spaces']"
95,To find continuous functions on $\mathbb R$ which preserve certain algebraic structures,To find continuous functions on  which preserve certain algebraic structures,\mathbb R,"Can we determine all non-constant continuous functions $f:\mathbb R \to \mathbb R$ such that for every subgroup $G$ of $(\mathbb R,+)$, $f(G)$ is also a subgroup of $(\mathbb R,+) $ ? And similarly, characterize all continuous functions on $\mathbb R$ which preserves subrings and also those which preserves $\mathbb Q$-vector subspaces of $\mathbb R$ ?","Can we determine all non-constant continuous functions $f:\mathbb R \to \mathbb R$ such that for every subgroup $G$ of $(\mathbb R,+)$, $f(G)$ is also a subgroup of $(\mathbb R,+) $ ? And similarly, characterize all continuous functions on $\mathbb R$ which preserves subrings and also those which preserves $\mathbb Q$-vector subspaces of $\mathbb R$ ?",,"['real-analysis', 'linear-algebra']"
96,Real Analysis book with pictures and ideas of proofs [duplicate],Real Analysis book with pictures and ideas of proofs [duplicate],,This question already has answers here : Visual book of real analysis (2 answers) Closed 2 years ago . I am taking real analysis course in my graduate class of Maths. My classes will start in 3 months. I have studied real analysis but not  very rigorously. Whenever I see theorem I have no idea on how to prove it or even how do I start it. I understand when I read them. But I cannot reproduce them in my own way when needed. Basically I want to redo it again before my actual classes. Can anyone suggest me book which contains pictorial and diagrams for proofs and also idea of proof is given with each proof. I feel like as I lack tools for proofs. This will help me a lot Thanks,This question already has answers here : Visual book of real analysis (2 answers) Closed 2 years ago . I am taking real analysis course in my graduate class of Maths. My classes will start in 3 months. I have studied real analysis but not  very rigorously. Whenever I see theorem I have no idea on how to prove it or even how do I start it. I understand when I read them. But I cannot reproduce them in my own way when needed. Basically I want to redo it again before my actual classes. Can anyone suggest me book which contains pictorial and diagrams for proofs and also idea of proof is given with each proof. I feel like as I lack tools for proofs. This will help me a lot Thanks,,"['real-analysis', 'reference-request', 'soft-question', 'book-recommendation']"
97,"Proof of $(0,1)$ is not compact with usual metric.",Proof of  is not compact with usual metric.,"(0,1)","In the proof we say $\left\{\left(\frac1n,1\right):n\geq 1\right\}$ is an infinite cover with no finite subcover. But, $(0,1)$ set also belongs to cover mentioned above.  We can say $\{(0,1)\}$ is a subcover of mentioned above cover. I am not able to understand what I am doing wrong.","In the proof we say $\left\{\left(\frac1n,1\right):n\geq 1\right\}$ is an infinite cover with no finite subcover. But, $(0,1)$ set also belongs to cover mentioned above.  We can say $\{(0,1)\}$ is a subcover of mentioned above cover. I am not able to understand what I am doing wrong.",,"['real-analysis', 'general-topology']"
98,Is this sequence bounded ? (An open problem between my schoolmates !),Is this sequence bounded ? (An open problem between my schoolmates !),,"Let $f$ be a smooth function (say $\mathcal{C}^{\infty}$) in its two real variables ($t$ and $T$). I consider the following sequence defined by   $$A_n:=\lim_{T \to \infty} \int_{0}^{1} e^{-n t} f(t,T)dt \quad \quad (n\geq 1)$$   where we suppose that this limit exists and is finite for all integer $n$.   I would like to conclude that the sequence $(A_n)$ is bounded. Is that true ? This is not an obvious problem : assuming certain conditions on $f$ then the exercise becomes classic. However, very little can be said about $(A_n)$ in the general case. In fact I deeply think that the answer is no and that we can find a $f$ such that $A_n$ tends to infinity. If one takes $f(t,T)=2\sin(tT)/\pi t$, then $(A_n)$ is the constant sequence equal to $1$. This shows that $f$ does not necessarily tend to $0$. Thanks for your help !","Let $f$ be a smooth function (say $\mathcal{C}^{\infty}$) in its two real variables ($t$ and $T$). I consider the following sequence defined by   $$A_n:=\lim_{T \to \infty} \int_{0}^{1} e^{-n t} f(t,T)dt \quad \quad (n\geq 1)$$   where we suppose that this limit exists and is finite for all integer $n$.   I would like to conclude that the sequence $(A_n)$ is bounded. Is that true ? This is not an obvious problem : assuming certain conditions on $f$ then the exercise becomes classic. However, very little can be said about $(A_n)$ in the general case. In fact I deeply think that the answer is no and that we can find a $f$ such that $A_n$ tends to infinity. If one takes $f(t,T)=2\sin(tT)/\pi t$, then $(A_n)$ is the constant sequence equal to $1$. This shows that $f$ does not necessarily tend to $0$. Thanks for your help !",,"['real-analysis', 'integration', 'limits', 'distribution-theory']"
99,Countable sum of measures is a measure,Countable sum of measures is a measure,,"Prove that if $\mu_1, \mu_2, \dots$ are measures on a measurable space and $a_1, a_2, \dots \in [0,\infty)$, then $\sum_{n=1}^\infty a_n\mu_n$ is also a measure. I need some help justifying the third equality in the final line of my proof. My idea was to break this into finite and infinite cases and use facts about absolute convergence, but I'm not sure of the details. My solution: First, we let $(X, \mu_n, \mathcal A)$ be a measure space for all $n\in \mathbb N$ and define $\nu_n = a_n\mu_n$.  Since $\nu_n(\emptyset) = a_n\mu_n(\emptyset) = a_n\cdot0=0$ and $$\nu_n(\cup_{i=1}^\infty A_i) = a_n\mu_n(\cup_{i=1}^\infty A_i) = a_n\sum_{i=1}^\infty \mu_n(A_i) = \sum_{i=1}^\infty a_n\mu_n(A_i) = \sum_{i=1}^\infty \nu_n(A_i),$$ we know that $\nu_n$ is a measure for all $n\in \mathbb N$. So we are reduced to the case that a countable sum of measures is again a measure. Let $\mu = \sum_{n=1}^\infty \nu_n$. Since $\nu_n(\emptyset) = 0$ for all $n \in \mathbb N$, then $\mu(\emptyset) = \sum_{n=1}^\infty \nu_n(\emptyset) = 0$. So, we show that $\mu$ is countably additive. That is, if $A_i\in \mathcal A$ for all $i \in \mathbb N$ are pairwise disjoint, we show $\mu(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty \mu(A_i)$. Then, \begin{align*} \mu(\cup_{i=1}^\infty A_i) &= \sum_{n=1}^\infty \nu_n(\cup_{i=1}^\infty A_i) =\sum_{n=1}^\infty\sum_{i=1}^\infty \nu_n(A_i) = \sum_{i=1}^\infty\sum_{n=1}^\infty \nu_n(A_i) = \sum_{i=1}^\infty \mu(A_i). \\ \end{align*}","Prove that if $\mu_1, \mu_2, \dots$ are measures on a measurable space and $a_1, a_2, \dots \in [0,\infty)$, then $\sum_{n=1}^\infty a_n\mu_n$ is also a measure. I need some help justifying the third equality in the final line of my proof. My idea was to break this into finite and infinite cases and use facts about absolute convergence, but I'm not sure of the details. My solution: First, we let $(X, \mu_n, \mathcal A)$ be a measure space for all $n\in \mathbb N$ and define $\nu_n = a_n\mu_n$.  Since $\nu_n(\emptyset) = a_n\mu_n(\emptyset) = a_n\cdot0=0$ and $$\nu_n(\cup_{i=1}^\infty A_i) = a_n\mu_n(\cup_{i=1}^\infty A_i) = a_n\sum_{i=1}^\infty \mu_n(A_i) = \sum_{i=1}^\infty a_n\mu_n(A_i) = \sum_{i=1}^\infty \nu_n(A_i),$$ we know that $\nu_n$ is a measure for all $n\in \mathbb N$. So we are reduced to the case that a countable sum of measures is again a measure. Let $\mu = \sum_{n=1}^\infty \nu_n$. Since $\nu_n(\emptyset) = 0$ for all $n \in \mathbb N$, then $\mu(\emptyset) = \sum_{n=1}^\infty \nu_n(\emptyset) = 0$. So, we show that $\mu$ is countably additive. That is, if $A_i\in \mathcal A$ for all $i \in \mathbb N$ are pairwise disjoint, we show $\mu(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty \mu(A_i)$. Then, \begin{align*} \mu(\cup_{i=1}^\infty A_i) &= \sum_{n=1}^\infty \nu_n(\cup_{i=1}^\infty A_i) =\sum_{n=1}^\infty\sum_{i=1}^\infty \nu_n(A_i) = \sum_{i=1}^\infty\sum_{n=1}^\infty \nu_n(A_i) = \sum_{i=1}^\infty \mu(A_i). \\ \end{align*}",,['real-analysis']
