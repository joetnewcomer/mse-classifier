,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Studying $\lim_{x \rightarrow 0^+} \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)}$,Studying,\lim_{x \rightarrow 0^+} \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)},"Study the following limit with $\alpha$ and $\beta$ parameters: $$\lim_{x \rightarrow 0^+} \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)}$$ My attempt: $$\frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)} \;\sim_{0}\; \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}}$$ Now I use Hospital three times and I have: $$\frac{(-12+96x^2-64x^4)e^{-2x^2}+\cos(x)+6 \beta}{\alpha (\alpha-1)(\alpha-2)x^{\alpha-3}}\;\sim_0\; \frac{-11 +6 \beta}{\alpha (\alpha-1)(\alpha-2)x^{\alpha-3}}$$ Now if $\beta = \frac{11}{6}$ and $\alpha \gt 3$ , I have $0 \cdot \infty$ indeterminate form. What can we say about the case $\beta = \frac{11}{6}$ and $\alpha \gt 3$ ?","Study the following limit with and parameters: My attempt: Now I use Hospital three times and I have: Now if and , I have indeterminate form. What can we say about the case and ?",\alpha \beta \lim_{x \rightarrow 0^+} \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)} \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}\cos(x^2)} \;\sim_{0}\; \frac{xe^{-2x^2}-\sin(x)+\beta x^3}{x^{\alpha}} \frac{(-12+96x^2-64x^4)e^{-2x^2}+\cos(x)+6 \beta}{\alpha (\alpha-1)(\alpha-2)x^{\alpha-3}}\;\sim_0\; \frac{-11 +6 \beta}{\alpha (\alpha-1)(\alpha-2)x^{\alpha-3}} \beta = \frac{11}{6} \alpha \gt 3 0 \cdot \infty \beta = \frac{11}{6} \alpha \gt 3,['limits']
1,How do I know when I'm done factoring?,How do I know when I'm done factoring?,,"I was attempting to factor $$\frac{3x^3 + x^2}{x^3 + x^2 + x}$$ I gave up and looked up a solution which was $$\frac{x(3x + 1)}{x^2 + x + 1}$$ which allowed me to solve for the limit as $x$ approaches $0$ , which is $0$ . However, at one point when factoring myself I ended up with $$\frac{3x^2 + 1}{x^2 + x + 1}$$ If THIS were factored enough, the limit would be $1$ , but it wasn't. So, I'm just curious how I'm supposed to know that I'm done factoring.","I was attempting to factor I gave up and looked up a solution which was which allowed me to solve for the limit as approaches , which is . However, at one point when factoring myself I ended up with If THIS were factored enough, the limit would be , but it wasn't. So, I'm just curious how I'm supposed to know that I'm done factoring.",\frac{3x^3 + x^2}{x^3 + x^2 + x} \frac{x(3x + 1)}{x^2 + x + 1} x 0 0 \frac{3x^2 + 1}{x^2 + x + 1} 1,"['calculus', 'limits', 'factoring']"
2,$(a^n-1)(b^n-1)$ can't be a square for all $n$ unless $ab$ is a square,can't be a square for all  unless  is a square,(a^n-1)(b^n-1) n ab,"Let $a,b$ be positive integers $>1$ such that $(a^n-1)(b^n-1)$ is a square for all $n\ge 1$ . Prove that $ab$ is a perfect square. I'm not asking for a solution to this problem because I already know one. What I'm asking is would the following approach work? or rather I want a particular limit to equal zero. Assume $(a^n-1)(b^n-1)$ is a perfect square $\forall n\ge1$ and let $x_n=\sqrt{(a^n-1)(b^n-1)}$ . Note that $x_n\in \mathbb N $ Now $$x_{n+2}=\sqrt{(a^{n+2}-1)(b^{n+2}-1)}=ab\sqrt{\left(a^{n}-\frac{1}{a^2}\right)\left(b^{n}-\frac{1}{b^2}\right)}$$ and $ab\cdot x_n=ab\sqrt{(a^n-1)(b^n-1)}$ . Define $d_n=abx_n-x_{n+2}$ and let us assume for the moment $$\lim_{n\to \infty}d_n=\lim_{n\to \infty}abx_n-x_{n+2}=0$$ Therefore $(d_n)$ converges to $0$ , but $d_n\in \mathbb N $ hence $d_n$ is eventually $0$ . Meaning for big enough $N$ , we have $\forall n\ge N$ $$d_n=abx_n-x_{n+2}=0$$ in other words $abx_n=x_{n+2}$ . From this I think there is a lot of ways to get a contradiction but this is what I did, $$\forall n\ge N' \quad  ab\mid x_{n}\mid x_n^2=(a^n-1)(b^n-1)=(ab)^n-a^n-b^n+1$$ which means $a\mid ab\mid a^n+b^n-1\implies a\mid b^n-1$ . In particular $$\forall p\ge N' \quad b^p\equiv  1\pmod a$$ Where $p$ is a prime. But this implies $\operatorname{ord}_ab\mid p$ hence $r=\operatorname{ord}_ab=1$ or $p$ . Using the very same method we can see $s=\operatorname{ord}_ba=1$ or $p$ both gives a contradiction. If one of $r$ or $s$ is $p$ the contradiction is immediate since $p\mid \phi(a)$ or $\phi(b)$ but clearly this not okay for large enough $p$ . Thus both of $r$ and $s$ is $1$ . But this means $$a\mid b-1 \text { and } b\mid a-1$$ Hence $a+b\le a+b-2\implies 0\le -2$ a contradiction. I think that the solution doesn't contain errors (If it did tell me). The only thing that I'm not sure of is $\lim d_n=0$ because I just counldn't evaluate $$\lim_{n\to \infty}\sqrt{(a^n-1)(b^n-1)}-\sqrt{\left(a^{n}-\frac{1}{a^2}\right)\left(b^{n}-\frac{1}{b^2}\right)}$$ You can clearly multiply by the conjugate but after expanding you get $-\infty /\infty$ .","Let be positive integers such that is a square for all . Prove that is a perfect square. I'm not asking for a solution to this problem because I already know one. What I'm asking is would the following approach work? or rather I want a particular limit to equal zero. Assume is a perfect square and let . Note that Now and . Define and let us assume for the moment Therefore converges to , but hence is eventually . Meaning for big enough , we have in other words . From this I think there is a lot of ways to get a contradiction but this is what I did, which means . In particular Where is a prime. But this implies hence or . Using the very same method we can see or both gives a contradiction. If one of or is the contradiction is immediate since or but clearly this not okay for large enough . Thus both of and is . But this means Hence a contradiction. I think that the solution doesn't contain errors (If it did tell me). The only thing that I'm not sure of is because I just counldn't evaluate You can clearly multiply by the conjugate but after expanding you get .","a,b >1 (a^n-1)(b^n-1) n\ge 1 ab (a^n-1)(b^n-1) \forall n\ge1 x_n=\sqrt{(a^n-1)(b^n-1)} x_n\in \mathbb N  x_{n+2}=\sqrt{(a^{n+2}-1)(b^{n+2}-1)}=ab\sqrt{\left(a^{n}-\frac{1}{a^2}\right)\left(b^{n}-\frac{1}{b^2}\right)} ab\cdot x_n=ab\sqrt{(a^n-1)(b^n-1)} d_n=abx_n-x_{n+2} \lim_{n\to \infty}d_n=\lim_{n\to \infty}abx_n-x_{n+2}=0 (d_n) 0 d_n\in \mathbb N  d_n 0 N \forall n\ge N d_n=abx_n-x_{n+2}=0 abx_n=x_{n+2} \forall n\ge N' \quad  ab\mid x_{n}\mid x_n^2=(a^n-1)(b^n-1)=(ab)^n-a^n-b^n+1 a\mid ab\mid a^n+b^n-1\implies a\mid b^n-1 \forall p\ge N' \quad b^p\equiv  1\pmod a p \operatorname{ord}_ab\mid p r=\operatorname{ord}_ab=1 p s=\operatorname{ord}_ba=1 p r s p p\mid \phi(a) \phi(b) p r s 1 a\mid b-1 \text { and } b\mid a-1 a+b\le a+b-2\implies 0\le -2 \lim d_n=0 \lim_{n\to \infty}\sqrt{(a^n-1)(b^n-1)}-\sqrt{\left(a^{n}-\frac{1}{a^2}\right)\left(b^{n}-\frac{1}{b^2}\right)} -\infty /\infty","['limits', 'elementary-number-theory', 'perfect-powers']"
3,Is this limit solvable using Stolz’s theorem ? $\lim_\limits{n\to+\infty}\left(\frac12+\frac3{2^2}+\dots+\frac{2n−1}{2^n}\right)$,Is this limit solvable using Stolz’s theorem ?,\lim_\limits{n\to+\infty}\left(\frac12+\frac3{2^2}+\dots+\frac{2n−1}{2^n}\right),"I would like to ask if this example is solvable using Stolz’s theorems, because I have a $2^n$ expression at the bottom and a quadratic expression at the top, so it doesn't work for me, but maybe I'm calculating wrong.","I would like to ask if this example is solvable using Stolz’s theorems, because I have a expression at the bottom and a quadratic expression at the top, so it doesn't work for me, but maybe I'm calculating wrong.",2^n,"['calculus', 'sequences-and-series', 'limits', 'power-series']"
4,Is Wolfram Alpha correct about this limit of an oscillating function?,Is Wolfram Alpha correct about this limit of an oscillating function?,,"I'm struggling with the following limit $$\lim_{x \to \infty} e^x\big(1 + \sin(x)\big).$$ First of all I checked the solution using Wolfram . To my surprise Wolfram says that this limit exists and is equal to $\infty$ . This goes against my intuition being honest. I decided to try to show that this limit does not exist using Heine definition. I found the following sequences $$x_n = n\pi, \qquad y_n = \bigg(2n + \frac{3}{2}\bigg)\pi.$$ Of course both $x_n$ and $y_n$ tend to $\infty$ whenever $n \to \infty$ . Moreover we have $\sin(x_n) = 0$ and $\sin(y_n) = -1$ for all $n \in \mathbb{N}$ . The first limit is easy, $$\lim_{n \to \infty} e^{x_n}\big(1 + \sin(x_n)\big) = \infty.$$ The problems appears when computing the next limit, which is $$\lim_{n \to \infty} e^{y_n}\big(1 + \sin(y_n)\big).$$ We clearly have an indeterminate form, i.e. $\infty \times 0$ . How can I solve my problem? Does this limit exist?","I'm struggling with the following limit First of all I checked the solution using Wolfram . To my surprise Wolfram says that this limit exists and is equal to . This goes against my intuition being honest. I decided to try to show that this limit does not exist using Heine definition. I found the following sequences Of course both and tend to whenever . Moreover we have and for all . The first limit is easy, The problems appears when computing the next limit, which is We clearly have an indeterminate form, i.e. . How can I solve my problem? Does this limit exist?","\lim_{x \to \infty} e^x\big(1 + \sin(x)\big). \infty x_n = n\pi, \qquad y_n = \bigg(2n + \frac{3}{2}\bigg)\pi. x_n y_n \infty n \to \infty \sin(x_n) = 0 \sin(y_n) = -1 n \in \mathbb{N} \lim_{n \to \infty} e^{x_n}\big(1 + \sin(x_n)\big) = \infty. \lim_{n \to \infty} e^{y_n}\big(1 + \sin(y_n)\big). \infty \times 0","['real-analysis', 'limits', 'solution-verification', 'wolfram-alpha']"
5,Evaluate the limit of a sequence by Riemann sums and mean value theorem,Evaluate the limit of a sequence by Riemann sums and mean value theorem,,"Calculate: $\displaystyle \lim_{n \rightarrow \infty} \left( \frac{n \pi}{4} - \left( \frac{n^2}{n^2+1^2} + \frac{n^2}{n^2+2^2} + \cdots \frac{n^2}{n^2+n^2} \right) \right)$ . I solved it by taking into account that $\displaystyle \int_0^{1} \frac{1}{1+x^2} \mathrm{d}x = \frac{\pi}{4}$ and let the given sequence be: $a_n= \displaystyle \frac{n \pi}{4} - \left( \frac{n^2}{n^2+1^2} + \frac{n^2}{n^2+2^2} + \cdots + \frac{n^2}{n^2+n^2} \right)$ Let $f(x) = \frac{1}{1+x^2}$ , then: $a_n = \displaystyle \frac{n \pi}{4} - \sum_{i=1}^n \frac{1}{1+\left( \frac{i}{n} \right)^2} = n \int_0^{1} f(x) \mathrm{d}x - \sum_{i=1}^n f\left( \frac{i}{n} \right) = n \sum_{i=1}^n \int_{\frac{i-1}{n}}^{\frac{i}{n}} f(x) \mathrm{d}x - n \sum_{i=1}^n \int_{\frac{i-1}{n}}^{\frac{i}{n}} f\left( \frac{i}{n} \right) \mathrm{d}x = n \sum_{i=1}^n \int_{\frac{i-1}{n}}^{\frac{i}{n}} \left( f(x)- f\left( \frac{i}{n} \right) \right) \mathrm{d}x$ Using Mean Value Theorem and doing a lot of calculations, I finally get that the limit is $\displaystyle \frac{1}{4}$ . Is it correct? Is there an easier method to solve the problem?","Calculate: . I solved it by taking into account that and let the given sequence be: Let , then: Using Mean Value Theorem and doing a lot of calculations, I finally get that the limit is . Is it correct? Is there an easier method to solve the problem?",\displaystyle \lim_{n \rightarrow \infty} \left( \frac{n \pi}{4} - \left( \frac{n^2}{n^2+1^2} + \frac{n^2}{n^2+2^2} + \cdots \frac{n^2}{n^2+n^2} \right) \right) \displaystyle \int_0^{1} \frac{1}{1+x^2} \mathrm{d}x = \frac{\pi}{4} a_n= \displaystyle \frac{n \pi}{4} - \left( \frac{n^2}{n^2+1^2} + \frac{n^2}{n^2+2^2} + \cdots + \frac{n^2}{n^2+n^2} \right) f(x) = \frac{1}{1+x^2} a_n = \displaystyle \frac{n \pi}{4} - \sum_{i=1}^n \frac{1}{1+\left( \frac{i}{n} \right)^2} = n \int_0^{1} f(x) \mathrm{d}x - \sum_{i=1}^n f\left( \frac{i}{n} \right) = n \sum_{i=1}^n \int_{\frac{i-1}{n}}^{\frac{i}{n}} f(x) \mathrm{d}x - n \sum_{i=1}^n \int_{\frac{i-1}{n}}^{\frac{i}{n}} f\left( \frac{i}{n} \right) \mathrm{d}x = n \sum_{i=1}^n \int_{\frac{i-1}{n}}^{\frac{i}{n}} \left( f(x)- f\left( \frac{i}{n} \right) \right) \mathrm{d}x \displaystyle \frac{1}{4},"['sequences-and-series', 'limits', 'riemann-sum', 'mean-value-theorem']"
6,"Help with proof that every point $x \in [0,1]$ is an accumulation point of given sequence",Help with proof that every point  is an accumulation point of given sequence,"x \in [0,1]","I was aksed to show that every point $x \in [0,1]$ is an accumulation point of the sequence $$ v_n= \sum_{k=0}^Kz_k10^{-k-1} \text{, where } n= \sum_{k=0}^Kz_k10^k. $$ So for example $v_{123}=0.321$ or $v_{3210}=0.0123$ . I think I understand the question and got the idea but I don't quite know how to write down the proof. Here is my attempt - hopefully not a mess: Proof. Let $x \in (0,1)$ be arbitrary.The point $x$ can be written as $$ x=0.x_0x_1x_2x_3 \cdots \text {, where } 0 \leq x_i \leq 9, \forall i \in \mathbb N. $$ Now define the subsequence $$ y_k =0.x_0x_1x_2x_3 \cdots x_k. $$ Then we get $|y_k-x| \lt \varepsilon, \forall \varepsilon \gt 0$ if we choose $N= \lceil \frac{1}{n} \rceil$ and $lim_{k \to \infty} y_k=x$ . If now $x_i=z_i, \forall i \in \mathbb N$ , we get an $n_k$ with $$ n_k= \sum_{i=0}^Kz_i10=\cdots x_3x_2x_1x_0 \implies v_{n_k}= \sum_{i=0}^Kz_i10^{-i-1}=0.x_0x_1x_2x_3 \cdots. $$ Then $$ lim_{k \to \infty}v_{n_k}=x $$ and the point $x$ is an accumulation point of the sequence $v_n$ . For $x=0$ , define $$ v_{n_k} =0.000 \cdots 1. $$ Then $|v_{n_k}-x|=|v_{n_k}-0| \leq 10^{-k+1} \lt \varepsilon$ with $N= \lceil \frac{1}{n} \rceil$ . For $x=1$ define $$ v_{n_k}=0.999 \cdots 9 $$ Then $|v_{n_k}-x|=|v_{n_k}-1| \leq 10^{-k+1} \lt \varepsilon$ with $N= \lceil \frac{1}{n} \rceil$ . That means every $x \in [0,1]$ is an accumulation point of the sequence $v_n$ . $$\tag*{$\blacksquare$}$$ I'm really struggling with this one for a long time now, so thank you very much in advance!","I was aksed to show that every point is an accumulation point of the sequence So for example or . I think I understand the question and got the idea but I don't quite know how to write down the proof. Here is my attempt - hopefully not a mess: Proof. Let be arbitrary.The point can be written as Now define the subsequence Then we get if we choose and . If now , we get an with Then and the point is an accumulation point of the sequence . For , define Then with . For define Then with . That means every is an accumulation point of the sequence . I'm really struggling with this one for a long time now, so thank you very much in advance!","x \in [0,1] 
v_n= \sum_{k=0}^Kz_k10^{-k-1} \text{, where } n= \sum_{k=0}^Kz_k10^k.
 v_{123}=0.321 v_{3210}=0.0123 x \in (0,1) x 
x=0.x_0x_1x_2x_3 \cdots \text {, where } 0 \leq x_i \leq 9, \forall i \in \mathbb N.
 
y_k =0.x_0x_1x_2x_3 \cdots x_k.
 |y_k-x| \lt \varepsilon, \forall \varepsilon \gt 0 N= \lceil \frac{1}{n} \rceil lim_{k \to \infty} y_k=x x_i=z_i, \forall i \in \mathbb N n_k 
n_k= \sum_{i=0}^Kz_i10=\cdots x_3x_2x_1x_0 \implies v_{n_k}= \sum_{i=0}^Kz_i10^{-i-1}=0.x_0x_1x_2x_3 \cdots.
 
lim_{k \to \infty}v_{n_k}=x
 x v_n x=0 
v_{n_k} =0.000 \cdots 1.
 |v_{n_k}-x|=|v_{n_k}-0| \leq 10^{-k+1} \lt \varepsilon N= \lceil \frac{1}{n} \rceil x=1 
v_{n_k}=0.999 \cdots 9
 |v_{n_k}-x|=|v_{n_k}-1| \leq 10^{-k+1} \lt \varepsilon N= \lceil \frac{1}{n} \rceil x \in [0,1] v_n \tag*{\blacksquare}","['sequences-and-series', 'limits', 'analysis', 'solution-verification', 'proof-writing']"
7,Classify infinities,Classify infinities,,"In mathematics, "" $\infty - \infty$ "" is an indetermination because $\infty$ can be reached by lots of different ways, and we can define some of them as ""bigger"" than the others. Counter-intuitively, we also have that $\infty = \infty + \infty = \infty^2 = e^\infty = \infty^\infty$ . What a mess ! Is there a way to ""classify"" the different infinities, in order to apply classical algebra to infinities ? Is there some kind of ""infinity-algebra"" ? For instance, can we define: $$\infty_0 := \lim_{x\rightarrow +\infty} x$$ We could consider that "" $x\rightarrow +\infty$ "" $ \Leftrightarrow  $ "" $x\rightarrow \infty_0$ "", and thus: $$\lim_{x\rightarrow \infty_0} x^2 = \infty_0^2$$ We would also have: $$\lim_{x\rightarrow 0} \frac{1}{x} = \lim_{x\rightarrow \infty_0} x = \infty_0 =\ ""\frac{1}{0}""$$ and so on.","In mathematics, "" "" is an indetermination because can be reached by lots of different ways, and we can define some of them as ""bigger"" than the others. Counter-intuitively, we also have that . What a mess ! Is there a way to ""classify"" the different infinities, in order to apply classical algebra to infinities ? Is there some kind of ""infinity-algebra"" ? For instance, can we define: We could consider that "" "" "" "", and thus: We would also have: and so on.","\infty - \infty \infty \infty = \infty + \infty = \infty^2 = e^\infty = \infty^\infty \infty_0 := \lim_{x\rightarrow +\infty} x x\rightarrow +\infty  \Leftrightarrow   x\rightarrow \infty_0 \lim_{x\rightarrow \infty_0} x^2 = \infty_0^2 \lim_{x\rightarrow 0} \frac{1}{x} = \lim_{x\rightarrow \infty_0} x = \infty_0 =\ ""\frac{1}{0}""",['limits']
8,"Prove that $ \lim_{t\to\infty} \frac{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\cos(\omega xt)\, dx}{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\, dx}=0.$",Prove that," \lim_{t\to\infty} \frac{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\cos(\omega xt)\, dx}{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\, dx}=0.","I have a continuous family of functions $\rho_t(x)$ that converge pointwise to a Gaussian $$ \lim_{t\to\infty}\rho_t(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}. $$ I would like to prove that $$ \lim_{t\to\infty} \frac{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\cos(\omega xt)\, dx}{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\, dx}=0. $$ Usually, one would argue that the numerator has a rapidly oscillating function at large $t$ multiplied by a function that is becoming smooth and should decay to zero, but the $e^{xt}$ term makes that argument difficult. You can assume $\rho_t(x)$ is well-behaved (whatever smoothness condition is necessary).","I have a continuous family of functions that converge pointwise to a Gaussian I would like to prove that Usually, one would argue that the numerator has a rapidly oscillating function at large multiplied by a function that is becoming smooth and should decay to zero, but the term makes that argument difficult. You can assume is well-behaved (whatever smoothness condition is necessary).","\rho_t(x) 
\lim_{t\to\infty}\rho_t(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}.
 
\lim_{t\to\infty} \frac{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\cos(\omega xt)\, dx}{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\, dx}=0.
 t e^{xt} \rho_t(x)","['integration', 'limits', 'fourier-transform', 'central-limit-theorem', 'large-deviation-theory']"
9,Find $\lim \limits_{x\to 0} \frac{\sin({\pi \sqrt{\cos x})}}{x} $,Find,\lim \limits_{x\to 0} \frac{\sin({\pi \sqrt{\cos x})}}{x} ,"So I am having trouble finding this limit: $$\lim \limits_{x\to 0} \frac{\sin({\pi \sqrt{\cos x})}}{x}$$ The problem is I can't use the derivative of the composition of two functions nor can I use other techniques like l'Hôpital's theorem. I tried numerous techniques to calculate this limit but in vain so if you have any simple idea that is in the scope of my knowledge ( I am a pre-calculus student ), please do let me know without actually answering the question.","So I am having trouble finding this limit: The problem is I can't use the derivative of the composition of two functions nor can I use other techniques like l'Hôpital's theorem. I tried numerous techniques to calculate this limit but in vain so if you have any simple idea that is in the scope of my knowledge ( I am a pre-calculus student ), please do let me know without actually answering the question.",\lim \limits_{x\to 0} \frac{\sin({\pi \sqrt{\cos x})}}{x},"['algebra-precalculus', 'limits', 'trigonometry', 'limits-without-lhopital']"
10,What can you say about the limit of a series as a linear transformation?,What can you say about the limit of a series as a linear transformation?,,"I have a question. Let $V$ be the space of convergent real infinite sequences. I know that the following function: $lim: V \rightarrow \textbf{R}, lim(a_n) = lim_{n\rightarrow \infty} a_n$ is a linear transformation (Because $lim(a_n + b_n) = lim(a_n) + lim(b_n), and\ lim(ca_n) = clim(a_n)$ ). Is there anything special about this from a linear algebra perspective? The only thing I could find is that it's a functional. Is there anything special you can say about this from an abstract algebra persepctive? Thanks!",I have a question. Let be the space of convergent real infinite sequences. I know that the following function: is a linear transformation (Because ). Is there anything special about this from a linear algebra perspective? The only thing I could find is that it's a functional. Is there anything special you can say about this from an abstract algebra persepctive? Thanks!,"V lim: V \rightarrow \textbf{R}, lim(a_n) = lim_{n\rightarrow \infty} a_n lim(a_n + b_n) = lim(a_n) + lim(b_n), and\ lim(ca_n) = clim(a_n)","['real-analysis', 'linear-algebra', 'abstract-algebra', 'limits']"
11,Limit of a product of two functions is $0$,Limit of a product of two functions is,0,"If $f(x)$ and $g(x)$ two real valued functions such that $\lim\limits_{x\to c}f(x)$ exists and is finite, $g(x)$ is bounded and $$\lim_{x\to c}f(x) g(x) =0$$ then prove that $\lim\limits_{x\to c}f(x)=0$ or $\lim\limits_{x\to c}g(x)=0$ or both. My try: let $\lim\limits_{x\to c}f(x)\neq 0$ then $$\lim\limits_{x\to c} g(x) =\frac{\lim\limits_{x\to c}f(x) g(x)   }{\lim\limits_{x\to c}f(x)   }$$ and hence we have $$\lim_{x\to c} g(x) =0  $$ What if $\lim\limits_{x\to c}g(x)\neq 0$ ? Thank you.","If and two real valued functions such that exists and is finite, is bounded and then prove that or or both. My try: let then and hence we have What if ? Thank you.",f(x) g(x) \lim\limits_{x\to c}f(x) g(x) \lim_{x\to c}f(x) g(x) =0 \lim\limits_{x\to c}f(x)=0 \lim\limits_{x\to c}g(x)=0 \lim\limits_{x\to c}f(x)\neq 0 \lim\limits_{x\to c} g(x) =\frac{\lim\limits_{x\to c}f(x) g(x)   }{\lim\limits_{x\to c}f(x)   } \lim_{x\to c} g(x) =0   \lim\limits_{x\to c}g(x)\neq 0,"['real-analysis', 'calculus']"
12,Radius and interval of convergence for $\sum_{n=2}^\infty \frac{x^{2n}}{n\ln^2(n)}$,Radius and interval of convergence for,\sum_{n=2}^\infty \frac{x^{2n}}{n\ln^2(n)},"I was requested to show radius and interval of convergence for the series $$\sum_{n=2}^\infty \frac{x^{2n}}{n\ln^2(n)}$$ and was wondering if my result is correct. This was my approach. $i)$ $a_n := \frac{x^{2n}}{n\ln^2(n)} = \frac{(x^2)^n}{n\ln^2(n)}$ . We have $$|\frac{a_{n+1}}{a_n}| = \frac{|(x^2)^{n+1}|}{(n+1)\ln^2(n+1)}\times\frac{n \ln^2(n)}{|(x^2)^n|}$$ $$= \frac{n\ln^2(n)}{(n+1) \ln^2(n+1)}|x^2|$$ Because $\lim_{n\to\infty} (\frac{\ln x}{\ln (x+1)})^2 = 1$ (through L'Hopital) we have $$\lim_{n\to\infty} \frac{n\ln^2(n)}{(n+1) \ln^2(n+1)}= \lim_{n\to\infty} \Big((\frac{\ln(n)}{ln(n+1)})^2\Big) \times \lim_{n\to\infty}\frac{n}{n+1} =1 \times 1=1$$ and therefore $|\frac{a_{n+1}}{a_n}| \to |x^2|$ when $n \to \infty$ . $ii)$ $|x^2|=x^2 <1 \iff x \in(-1, 1) \equiv |x| <1$ . This already implies a radius of convergence of $1$ . $iii)$ $x=1 \implies a_n = \frac{1}{n\ln^2(n)}$ . Since $$\int_{2}^\infty \frac{1}{x \ln^2x} dx= \lim_{t\to\infty}\int_2^t \frac{1}{u^2}du \tag{$u=\ln x$}$$ $$= \lim_{t\to\infty} (-\frac{1}{t} + \frac{1}{2}) = \frac{1}{2}$$ we know the series will converge when $x=1$ via the integral test. For $x=-1$ , $a_n=\frac{((-1)^2)^n}{n \ln^2 n} = \frac{1}{n \ln^2 n}$ , the same case we already examined. Therefore,the interval of convergence is $[-1, 1]$ . Final answer . $R= 1, I=[-1, 1]$ . Is this answer correct?","I was requested to show radius and interval of convergence for the series and was wondering if my result is correct. This was my approach. . We have Because (through L'Hopital) we have and therefore when . . This already implies a radius of convergence of . . Since we know the series will converge when via the integral test. For , , the same case we already examined. Therefore,the interval of convergence is . Final answer . . Is this answer correct?","\sum_{n=2}^\infty \frac{x^{2n}}{n\ln^2(n)} i) a_n := \frac{x^{2n}}{n\ln^2(n)} = \frac{(x^2)^n}{n\ln^2(n)} |\frac{a_{n+1}}{a_n}| = \frac{|(x^2)^{n+1}|}{(n+1)\ln^2(n+1)}\times\frac{n \ln^2(n)}{|(x^2)^n|} = \frac{n\ln^2(n)}{(n+1) \ln^2(n+1)}|x^2| \lim_{n\to\infty} (\frac{\ln x}{\ln (x+1)})^2 = 1 \lim_{n\to\infty} \frac{n\ln^2(n)}{(n+1) \ln^2(n+1)}= \lim_{n\to\infty} \Big((\frac{\ln(n)}{ln(n+1)})^2\Big) \times \lim_{n\to\infty}\frac{n}{n+1} =1 \times 1=1 |\frac{a_{n+1}}{a_n}| \to |x^2| n \to \infty ii) |x^2|=x^2 <1 \iff x \in(-1, 1) \equiv |x| <1 1 iii) x=1 \implies a_n = \frac{1}{n\ln^2(n)} \int_{2}^\infty \frac{1}{x \ln^2x} dx= \lim_{t\to\infty}\int_2^t \frac{1}{u^2}du \tag{u=\ln x} = \lim_{t\to\infty} (-\frac{1}{t} + \frac{1}{2}) = \frac{1}{2} x=1 x=-1 a_n=\frac{((-1)^2)^n}{n \ln^2 n} = \frac{1}{n \ln^2 n} [-1, 1] R= 1, I=[-1, 1]","['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'power-series']"
13,$g(x) = \lim \limits_{t \to x}f(t)$. Show $g$ is continuous on $\mathbb{R}$.,. Show  is continuous on .,g(x) = \lim \limits_{t \to x}f(t) g \mathbb{R},"Let the function $f : \mathbb{R} \to \mathbb{R}$ , $\exists \lim \limits_{x \to c}f(x)$ for all $c\in\mathbb{R}$ . Define $g : \mathbb{R} \to \mathbb{R}$ by $g(x) = \lim \limits_{t \to x}f(t)$ . Show $g$ is continuous on $\mathbb{R}$ . I tried the different proof comparing the method solution suggested. From here my proof begins. $(pf)$ Fixed $\forall c \in \mathbb{R}$ , Say $\lim \limits_{x \to c}f(x) = g(c)$ . By definition, For all $\epsilon >0$ , $\exists \delta >0$ $s.t.$ $\forall x(0 < \vert x-c \vert <\delta  \Rightarrow \vert f(x) - g(c) \vert < \epsilon )$ ( $*$ ). Considering the $ I = \{x \vert \vert x-c \vert <\delta \}$ $\vert g(x)-g(c) \vert = \vert \lim \limits_{t \to x}f(t) - \lim \limits_{t \to x}g(c) \vert = \vert \lim \limits_{t \to x}(f(t)-g(c)) \vert <\epsilon$ , $\forall x \in I$ by the $(*)$ Hence $g$ is continuous at $\forall c\in \mathbb{R}$ (In other words, $g$ is continuous on $\mathbb{R}$ ) Is my proof right? If not, Please let me know which point I made mistakes. Regards.","Let the function , for all . Define by . Show is continuous on . I tried the different proof comparing the method solution suggested. From here my proof begins. Fixed , Say . By definition, For all , ( ). Considering the , by the Hence is continuous at (In other words, is continuous on ) Is my proof right? If not, Please let me know which point I made mistakes. Regards.",f : \mathbb{R} \to \mathbb{R} \exists \lim \limits_{x \to c}f(x) c\in\mathbb{R} g : \mathbb{R} \to \mathbb{R} g(x) = \lim \limits_{t \to x}f(t) g \mathbb{R} (pf) \forall c \in \mathbb{R} \lim \limits_{x \to c}f(x) = g(c) \epsilon >0 \exists \delta >0 s.t. \forall x(0 < \vert x-c \vert <\delta  \Rightarrow \vert f(x) - g(c) \vert < \epsilon ) *  I = \{x \vert \vert x-c \vert <\delta \} \vert g(x)-g(c) \vert = \vert \lim \limits_{t \to x}f(t) - \lim \limits_{t \to x}g(c) \vert = \vert \lim \limits_{t \to x}(f(t)-g(c)) \vert <\epsilon \forall x \in I (*) g \forall c\in \mathbb{R} g \mathbb{R},"['real-analysis', 'limits', 'solution-verification']"
14,Limit when both left and right hand sides are undefined?,Limit when both left and right hand sides are undefined?,,"I read here that $$\lim_{x \to 0} \sqrt{x} = 0$$ This is because the function $$f(x) = \sqrt{x} = \begin{cases}       \sqrt{x} & \text{if}\ x \geq 0 \\       \text{undefined} & \text{otherwise}     \end{cases}$$ is undefined for negative values of $x$ , so the limit can't be taken in the first place. But, what if both left and right sides are undefined? Consider $$ g(x) =      \begin{cases}       0 & \text{if}\ x = 0 \\       \text{undefined} & \text{otherwise}     \end{cases} $$ What will be $$\lim_{x \to 0} g(x)$$ now?","I read here that This is because the function is undefined for negative values of , so the limit can't be taken in the first place. But, what if both left and right sides are undefined? Consider What will be now?","\lim_{x \to 0} \sqrt{x} = 0 f(x) = \sqrt{x} =
\begin{cases}
      \sqrt{x} & \text{if}\ x \geq 0 \\
      \text{undefined} & \text{otherwise}
    \end{cases} x 
g(x) = 
    \begin{cases}
      0 & \text{if}\ x = 0 \\
      \text{undefined} & \text{otherwise}
    \end{cases}
 \lim_{x \to 0} g(x)","['real-analysis', 'calculus', 'limits', 'epsilon-delta']"
15,Who will win in this choosing nested intervals game? (a.k.a. Banach–Mazur game),Who will win in this choosing nested intervals game? (a.k.a. Banach–Mazur game),,"Question Alice and Bob are playing a game. The rules are as follows: First Alice chooses a compact interval $A_1\subset\mathbb R$ (in this question, intervals contain more than one points, they are not allowed to choose single-point sets,) then Bob chooses a compact interval $B_1\subset A_1$ with $|B_1|\leq\frac12|A_1|$ (here $|I|$ means the length of the compact interval $I$ .) After that, Alice chooses a compact interval $A_2\subset B_1$ with $|A_2|\leq\frac12|B_1|$ and so on. Hence they get a sequence of nested compact intervals $$A_1\supset B_1\supset A_2\supset B_2\supset \cdots\supset A_n\supset B_n\supset \cdots,$$ with $\lim_{n\to\infty} |A_n|=\lim_{n\to\infty} |B_n|=0$ . By nested interval theorem , there is a unique $x\in\mathbb R$ such that $$\{x\}=\left(\bigcap_{n=1}^\infty A_n\right) \bigcap \left(\bigcap_{n=1}^\infty B_n\right).$$ If $x$ is rational, then Alice wins; if $x$ is irrational, then Bob wins. Decide whether there exists a winning strategy for either of them. Background and thoughts This problem is from my imagination. It is clear that for any $x\in\mathbb R$ , whether $x$ is rational or not, we can always find a sequence of nested intervals $I_n=\left[x-\frac1n,x+\frac1n\right]$ such that $\{x\}=\cap_n I_n$ . The key point is that, Alice and Bob are both very very clever, and they are free to change the center of the intervals to any numbers they want. For example, if Alice choose $A_1=[-1,1]$ , which is centered at $0\in\mathbb Q$ , then Bob can find $B_1\subset A_1$ such that $B_1$ is centered at an irrational number; and then Alice can find $A_2\subset B_1$ such that $A_2$ is centered at a rational number; and so on... I cannot determine who will win in this kind of procedure. It seems that Bob will win, because irrational numbers are much more than rational numbers. But I can't figure out a rigorous proof. Any help would be appreciated!","Question Alice and Bob are playing a game. The rules are as follows: First Alice chooses a compact interval (in this question, intervals contain more than one points, they are not allowed to choose single-point sets,) then Bob chooses a compact interval with (here means the length of the compact interval .) After that, Alice chooses a compact interval with and so on. Hence they get a sequence of nested compact intervals with . By nested interval theorem , there is a unique such that If is rational, then Alice wins; if is irrational, then Bob wins. Decide whether there exists a winning strategy for either of them. Background and thoughts This problem is from my imagination. It is clear that for any , whether is rational or not, we can always find a sequence of nested intervals such that . The key point is that, Alice and Bob are both very very clever, and they are free to change the center of the intervals to any numbers they want. For example, if Alice choose , which is centered at , then Bob can find such that is centered at an irrational number; and then Alice can find such that is centered at a rational number; and so on... I cannot determine who will win in this kind of procedure. It seems that Bob will win, because irrational numbers are much more than rational numbers. But I can't figure out a rigorous proof. Any help would be appreciated!","A_1\subset\mathbb R B_1\subset A_1 |B_1|\leq\frac12|A_1| |I| I A_2\subset B_1 |A_2|\leq\frac12|B_1| A_1\supset B_1\supset A_2\supset B_2\supset \cdots\supset A_n\supset B_n\supset \cdots, \lim_{n\to\infty} |A_n|=\lim_{n\to\infty} |B_n|=0 x\in\mathbb R \{x\}=\left(\bigcap_{n=1}^\infty A_n\right) \bigcap \left(\bigcap_{n=1}^\infty B_n\right). x x x\in\mathbb R x I_n=\left[x-\frac1n,x+\frac1n\right] \{x\}=\cap_n I_n A_1=[-1,1] 0\in\mathbb Q B_1\subset A_1 B_1 A_2\subset B_1 A_2","['real-analysis', 'limits', 'analysis', 'real-numbers', 'game-theory']"
16,"stronger variant of ""$l^p$ spaces are increasing in $p$""： $\bigcup_{1\leq p\lneq q}l^p\neq l^q$","stronger variant of "" spaces are increasing in ""：",l^p p \bigcup_{1\leq p\lneq q}l^p\neq l^q,"Problem Let $\mathbb{K}$ be either $\mathbb{R}$ or $\mathbb{C}$ . Let $x\in\mathbb{K}^\mathbb{N}$ be any sequence in $\mathbb{K}$ . Let the $p$ -norm be defined by $$\lVert x\rVert_p:=\left(\sum_{n\in\mathbb{N}}\left\lvert x_n\right\rvert^p\right)^\frac{1}{p}$$ for $1\leq p\lneq\infty$ and $$\lVert x\rVert_\infty:=\lim_{p\to\infty}\lVert x\rVert_p=\sup_{n\in\mathbb{N}}\left\lvert x_n\right\rvert$$ for $p=\infty$ . The $l^p$ -space is defined by $$l^p:=l^p(\mathbb{K}):=\left\{x\in\mathbb{K}^\mathbb{N}\ \mid\ \lVert x\rVert_p\lneq\infty\right\}$$ Let $1\leq q\leq\infty$ . Prove: $$\bigcup_{1\leq p\lneq q}l^p\subsetneq l^q$$ My Attempt I can prove the following weaker statement: Let $1\leq p\lneq q\leq\infty$ , then $l^p\subsetneq l^q$ . $$\forall x\in\mathbb{K}^\mathbb{N}: \lVert x\rVert_q\leq\lVert x\rVert_p\ \Rightarrow\ \left(\lVert x\rVert_p\lneq\infty \Rightarrow \lVert x\rVert_p\lneq\infty\right)\ \Rightarrow\ l^p\subseteq l^q$$ $1\leq p\lneq q\lneq \infty$ : Consider $x=\left(\frac{1}{n^{\frac{1}{p}}}\right)_{n\in\mathbb{N}}$ , then $\lVert x\rVert_p=\infty$ and $\lVert x\rVert_q\lneq\infty$ , because $\sum_{n\in\infty}\frac{1}{n^r}\lneq\infty \Leftrightarrow r>1$ . Therefore $x\in l_q$ and $x\notin l_p$ . $1\leq p\lneq q=\infty$ : Consider $x=(1)_{n\in\mathbb{N}}$ , then $\lVert x\rVert_\infty=1\lneq\infty$ and $\lVert x\rVert_p=\infty$ . Therefore $x\in l_\infty$ and $x\notin l_p$ . Question How can I prove $$\bigcup_{1\leq p\lneq q}l^p\neq l^q$$ for $1\leq p\lneq q\lneq \infty$ ? Remark A discussion of the weaker statement can be found here .","Problem Let be either or . Let be any sequence in . Let the -norm be defined by for and for . The -space is defined by Let . Prove: My Attempt I can prove the following weaker statement: Let , then . : Consider , then and , because . Therefore and . : Consider , then and . Therefore and . Question How can I prove for ? Remark A discussion of the weaker statement can be found here .",\mathbb{K} \mathbb{R} \mathbb{C} x\in\mathbb{K}^\mathbb{N} \mathbb{K} p \lVert x\rVert_p:=\left(\sum_{n\in\mathbb{N}}\left\lvert x_n\right\rvert^p\right)^\frac{1}{p} 1\leq p\lneq\infty \lVert x\rVert_\infty:=\lim_{p\to\infty}\lVert x\rVert_p=\sup_{n\in\mathbb{N}}\left\lvert x_n\right\rvert p=\infty l^p l^p:=l^p(\mathbb{K}):=\left\{x\in\mathbb{K}^\mathbb{N}\ \mid\ \lVert x\rVert_p\lneq\infty\right\} 1\leq q\leq\infty \bigcup_{1\leq p\lneq q}l^p\subsetneq l^q 1\leq p\lneq q\leq\infty l^p\subsetneq l^q \forall x\in\mathbb{K}^\mathbb{N}: \lVert x\rVert_q\leq\lVert x\rVert_p\ \Rightarrow\ \left(\lVert x\rVert_p\lneq\infty \Rightarrow \lVert x\rVert_p\lneq\infty\right)\ \Rightarrow\ l^p\subseteq l^q 1\leq p\lneq q\lneq \infty x=\left(\frac{1}{n^{\frac{1}{p}}}\right)_{n\in\mathbb{N}} \lVert x\rVert_p=\infty \lVert x\rVert_q\lneq\infty \sum_{n\in\infty}\frac{1}{n^r}\lneq\infty \Leftrightarrow r>1 x\in l_q x\notin l_p 1\leq p\lneq q=\infty x=(1)_{n\in\mathbb{N}} \lVert x\rVert_\infty=1\lneq\infty \lVert x\rVert_p=\infty x\in l_\infty x\notin l_p \bigcup_{1\leq p\lneq q}l^p\neq l^q 1\leq p\lneq q\lneq \infty,"['sequences-and-series', 'functional-analysis', 'limits', 'lp-spaces', 'supremum-and-infimum']"
17,Monotone converge recursive question,Monotone converge recursive question,,"Define the sequence $(x_n)$ recursively by setting $$ \begin{align}  x_1 &= \sqrt{2} \\ x_{n+1} &= \sqrt{2 + x_n} \text{ for all $n \in 1,2,3,\ldots$}  \end{align} $$ (a) Show that the sequence $(x_n)$ converges. (b) Let $\lambda = \lim_{n \to \infty} x_n$ . Show $\lambda^2 - \lambda - 2 = 0$ . Hi, I was wondering if the proof to this question for part(a) is correct. We can prove this by the monotone convergence theorem. We can show that the sequence is monotone increasing and bounded above. We know that $x_1 = \sqrt{2}$ and $x_2 = \sqrt{2+\sqrt{2}}$ . Hence, $x_1 \leq x_2 \leq 2$ , which will serve as our base case for our induction step. Suppose $x_n \leq x_{n+1} \leq 2$ . $x_n \leq x_{n+1} \leq 2 \iff x_n + 2 \leq x_{n+1} + 2 \leq 4 \iff \sqrt{x_n + 2} \leq \sqrt{x_{n+1} + 2} \leq 2$ . Therefore, $x_n \leq x_{n+1} \leq 2 \implies x_{n+1} \leq x_{n+2} \leq 2$ . By induction, we know that the sequence is increasing. We also have shown that it's bounded. Therefore, by the monotone convergence theorem, $\lim x_n$ converges. Also, I am pretty sure the sequence converges to 2, but I don't know how to prove that. It would make part (b) so much easier. I was thinking of doing something with $\sup \{x_1, x_2, x_3, ...\}$","Define the sequence recursively by setting (a) Show that the sequence converges. (b) Let . Show . Hi, I was wondering if the proof to this question for part(a) is correct. We can prove this by the monotone convergence theorem. We can show that the sequence is monotone increasing and bounded above. We know that and . Hence, , which will serve as our base case for our induction step. Suppose . . Therefore, . By induction, we know that the sequence is increasing. We also have shown that it's bounded. Therefore, by the monotone convergence theorem, converges. Also, I am pretty sure the sequence converges to 2, but I don't know how to prove that. It would make part (b) so much easier. I was thinking of doing something with","(x_n) 
\begin{align} 
x_1 &= \sqrt{2} \\ x_{n+1} &= \sqrt{2 + x_n} \text{ for all n \in 1,2,3,\ldots} 
\end{align}
 (x_n) \lambda = \lim_{n \to \infty} x_n \lambda^2 - \lambda - 2 = 0 x_1 = \sqrt{2} x_2 = \sqrt{2+\sqrt{2}} x_1 \leq x_2 \leq 2 x_n \leq x_{n+1} \leq 2 x_n \leq x_{n+1} \leq 2 \iff x_n + 2 \leq x_{n+1} + 2 \leq 4 \iff \sqrt{x_n + 2} \leq \sqrt{x_{n+1} + 2} \leq 2 x_n \leq x_{n+1} \leq 2 \implies x_{n+1} \leq x_{n+2} \leq 2 \lim x_n \sup \{x_1, x_2, x_3, ...\}","['real-analysis', 'limits', 'analysis', 'solution-verification', 'recursion']"
18,Limit Evaluation using Taylor Polynomials.,Limit Evaluation using Taylor Polynomials.,,"Consider the following limit problem: $$\lim \limits _{x\to 0}\frac{e^x-1}{x}.$$ It's known that $$e^x=\lim \limits _{n\to \infty}\sum \limits _{r=0}^n\frac{x^r}{r!}\qquad \forall x\in N_0,$$ where $N_0$ is a deleted neighbourhood of $0$ . So the original limit gets converted into the following: $$\lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\sum \limits _{r=0}^n\frac{x^r}{r!}-1}{x},$$ which simplifies to: \begin{align*}\lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\left (1+\sum \limits _{r=1}^n\frac{x^r}{r!}\right )-1}{x} & =\lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\sum \limits _{r=1}^n\frac{x^r}{r!}}{x} \\ & =\lim \limits _{x\to 0}\left (\lim \limits _{n\to \infty}1+\sum \limits _{r=2}^n\frac{x^{r-1}}{r!}\right ) \\ & =1. \end{align*} My only problem with the above method is how can we say that the last limit will for sure be $1$ ? Precisely, how are we so sure that the last summation( $\displaystyle \sum \limits _{r=2}^n\frac{x^{r-1}}{r!}$ ) will be $0$ when both limits (of $n$ and of $x$ ) are applied to it? I'm still in high school so I don't know how to evaluate limits using epsilon-delta definition. Is there any other way(other than using  epsilon-delta definition) to convince myself that the last summation indeed is $0$ in the limit? PS: I know other more efficient ways to approach the above limit problem, however I have a similar type of problem in every limit I evaluate using taylor expansions. So I'm using this limit as a placeholder for my general problem.","Consider the following limit problem: It's known that where is a deleted neighbourhood of . So the original limit gets converted into the following: which simplifies to: My only problem with the above method is how can we say that the last limit will for sure be ? Precisely, how are we so sure that the last summation( ) will be when both limits (of and of ) are applied to it? I'm still in high school so I don't know how to evaluate limits using epsilon-delta definition. Is there any other way(other than using  epsilon-delta definition) to convince myself that the last summation indeed is in the limit? PS: I know other more efficient ways to approach the above limit problem, however I have a similar type of problem in every limit I evaluate using taylor expansions. So I'm using this limit as a placeholder for my general problem.","\lim \limits _{x\to 0}\frac{e^x-1}{x}. e^x=\lim \limits _{n\to \infty}\sum \limits _{r=0}^n\frac{x^r}{r!}\qquad \forall x\in N_0, N_0 0 \lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\sum \limits _{r=0}^n\frac{x^r}{r!}-1}{x}, \begin{align*}\lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\left (1+\sum \limits _{r=1}^n\frac{x^r}{r!}\right )-1}{x} & =\lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\sum \limits _{r=1}^n\frac{x^r}{r!}}{x} \\
& =\lim \limits _{x\to 0}\left (\lim \limits _{n\to \infty}1+\sum \limits _{r=2}^n\frac{x^{r-1}}{r!}\right ) \\
& =1.
\end{align*} 1 \displaystyle \sum \limits _{r=2}^n\frac{x^{r-1}}{r!} 0 n x 0","['calculus', 'algebra-precalculus', 'limits', 'taylor-expansion', 'limits-without-lhopital']"
19,"Follow up question to question involving $\lim_{n\to \infty}\frac{\int_0^1 \left(x^2-x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}$",Follow up question to question involving,"\lim_{n\to \infty}\frac{\int_0^1 \left(x^2-x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}","I tried to answer Q 4482921 by elementary calculus but got stuck : The first step in my solution is to replace $x$ by $2u$ so that $dx=2du$ . The integral becomes $$\lim_{n\to \infty}\frac{\int_0^1 \left(x^2-x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}= \lim_{n\to \infty}\frac{\int_0^\frac{1}{2}\left(4u^2-2u-2\right)^n \ 2du}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}$$ $$=2\left(1-\lim_{n\to \infty}\frac{\int_ \frac{1}{2}^1 \left(4x^2-2x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}\right)$$ Now I need to prove that $\displaystyle\lim_{n\to \infty}\frac{\int_ \frac{1}{2}^1 \left(4x^2-2x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n\, dx} =0.$ It certainly looks like it in the graph, and I also found that $$\int_{\frac 12}^1 \left(4 x^2-2 x-2\right)^n\, dx= \int_{-1}^{0} \left(4 x^2-2 x-2\right)^n\, dx.$$","I tried to answer Q 4482921 by elementary calculus but got stuck : The first step in my solution is to replace by so that . The integral becomes Now I need to prove that It certainly looks like it in the graph, and I also found that","x 2u dx=2du \lim_{n\to \infty}\frac{\int_0^1 \left(x^2-x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}= \lim_{n\to \infty}\frac{\int_0^\frac{1}{2}\left(4u^2-2u-2\right)^n \ 2du}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx} =2\left(1-\lim_{n\to \infty}\frac{\int_
\frac{1}{2}^1 \left(4x^2-2x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \,
dx}\right) \displaystyle\lim_{n\to \infty}\frac{\int_
\frac{1}{2}^1 \left(4x^2-2x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n\, dx} =0. \int_{\frac 12}^1 \left(4 x^2-2 x-2\right)^n\, dx= \int_{-1}^{0} \left(4 x^2-2 x-2\right)^n\, dx.","['calculus', 'limits', 'definite-integrals']"
20,Use of limits of sequences for the non-existence,Use of limits of sequences for the non-existence,,"Let $f(x) = x +1$ for $x \geq 0$ and $f(x) = x- 1$ for $x < 0$ . I want to use the limits to show that $\lim_{x \rightarrow 0} f(x)$ does not exist. I tried with using $x_n = \frac{1}{n} \rightarrow 0$ (as $n \rightarrow \infty$ ) which leads to $f(x) \rightarrow \pm 1$ (as $x \rightarrow 0$ ). Is this enough to show the desired conclusion ? Edit : The following excerpt is from Wade's Analysis book. I wonder how the expression form for $f(x_n)$ was followed, whether it's a typo or not.","Let for and for . I want to use the limits to show that does not exist. I tried with using (as ) which leads to (as ). Is this enough to show the desired conclusion ? Edit : The following excerpt is from Wade's Analysis book. I wonder how the expression form for was followed, whether it's a typo or not.",f(x) = x +1 x \geq 0 f(x) = x- 1 x < 0 \lim_{x \rightarrow 0} f(x) x_n = \frac{1}{n} \rightarrow 0 n \rightarrow \infty f(x) \rightarrow \pm 1 x \rightarrow 0 f(x_n),"['real-analysis', 'limits', 'sequence-of-function']"
21,Showing $\lim_{x\to\infty} x^n P(X >x)=0$,Showing,\lim_{x\to\infty} x^n P(X >x)=0,"Hello I am trying to self-teach myself probability by follow a lecture course and example sheets. However, I am completely stuck on the following problem. Let X be a real-valued random variable. Suppose that the moment-generating function $m(\theta) = E(e^{\theta X})$ is finite for some $θ > 0$ . Show that for $\forall n > 0$ , $\lim_{x\to\infty} x^n P(X > x) = 0$ . My attempt :  I am not even sure how I should use the moment-generating function in relation to the limit. The only step I can see so far is $P(X > x)  = 1 - F(x)$ . Any hints in the right direction would be greatly appreciated :) Update: Using Markov Inequality for the variable $e^{\theta X}$ which is non negative random I can say $P(e^{\theta X} \geq x) \leq \frac{E(e^{\theta X})}{x}$ which is a much better place to start. The answer is below for those who don't want spoilers.","Hello I am trying to self-teach myself probability by follow a lecture course and example sheets. However, I am completely stuck on the following problem. Let X be a real-valued random variable. Suppose that the moment-generating function is finite for some . Show that for , . My attempt :  I am not even sure how I should use the moment-generating function in relation to the limit. The only step I can see so far is . Any hints in the right direction would be greatly appreciated :) Update: Using Markov Inequality for the variable which is non negative random I can say which is a much better place to start. The answer is below for those who don't want spoilers.",m(\theta) = E(e^{\theta X}) θ > 0 \forall n > 0 \lim_{x\to\infty} x^n P(X > x) = 0 P(X > x)  = 1 - F(x) e^{\theta X} P(e^{\theta X} \geq x) \leq \frac{E(e^{\theta X})}{x},"['probability', 'limits']"
22,Can we say $f(x)=\frac{x}{\ln(x^2)}$ has a removable discontinuity at $x=0$?,Can we say  has a removable discontinuity at ?,f(x)=\frac{x}{\ln(x^2)} x=0,I came across a function $$f(x)=\frac{x}{\ln(x^2)}$$ It is evident that $f(0)$ is undefined. But we have: $$\lim _{x \rightarrow 0^{+}} f(x)=0=\lim _{x \rightarrow 0^{-}} f(x)$$ So is $x=0$ a removable discontinuity?,I came across a function It is evident that is undefined. But we have: So is a removable discontinuity?,f(x)=\frac{x}{\ln(x^2)} f(0) \lim _{x \rightarrow 0^{+}} f(x)=0=\lim _{x \rightarrow 0^{-}} f(x) x=0,"['algebra-precalculus', 'limits', 'functions', 'continuity']"
23,Evaluating $\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x^3-8}$ without Hopital rule,Evaluating  without Hopital rule,\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x^3-8},"I want to evaluate this limit without applying Hopital rule, $$\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x^3-8}$$ After expanding the denominator I got, $$\frac{1}{12}\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x-2}$$ Here  I can introduce numerator as $f(x)$ And say the limit is in the form $\lim_{x\to 2}\frac{f(x)-f(2)}{x-2}=f'(2)$ But it is very similar to Hopital rule (if not the same). I noticed that the fraction is in the form $2.\frac{\sqrt[3]A-\sqrt B}{A-B}$ or $2.\frac{M^2-N^3}{M^6-N^6}$ but not sure if this helps.","I want to evaluate this limit without applying Hopital rule, After expanding the denominator I got, Here  I can introduce numerator as And say the limit is in the form But it is very similar to Hopital rule (if not the same). I noticed that the fraction is in the form or but not sure if this helps.",\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x^3-8} \frac{1}{12}\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x-2} f(x) \lim_{x\to 2}\frac{f(x)-f(2)}{x-2}=f'(2) 2.\frac{\sqrt[3]A-\sqrt B}{A-B} 2.\frac{M^2-N^3}{M^6-N^6},"['calculus', 'limits', 'limits-without-lhopital']"
24,Showing that positive central difference limit implies function is strictly increasing,Showing that positive central difference limit implies function is strictly increasing,,"Suppose $f:(a,b)\rightarrow \mathbb{R}$ is continuous, and that $$\lim_{h \rightarrow 0} \frac{f(x+h)-f(x-h)}{2h}$$ Exists for all $x\in(a,b)$ and is strictly greater than zero. How can I show that $f$ must be strictly increasing? I am comfortable with the usual MVT proof that a positive derivative implies a function is increasing, however, in this case, I cannot assume that the function is differentiable - so can't use the MVT in its standard form at least. I've had two ideas: Go right back to foundations and show that the MVT still holds in this case - by showing that Rolle's theorem still holds (does it?). Use the definition of a limit directly with an $\epsilon - \delta$ argument, showing that for all $|h|<\delta$ , $\frac{f(x+h)-f(x-h)}{2h}>0$ so somehow the function must be increasing. Any thoughts?","Suppose is continuous, and that Exists for all and is strictly greater than zero. How can I show that must be strictly increasing? I am comfortable with the usual MVT proof that a positive derivative implies a function is increasing, however, in this case, I cannot assume that the function is differentiable - so can't use the MVT in its standard form at least. I've had two ideas: Go right back to foundations and show that the MVT still holds in this case - by showing that Rolle's theorem still holds (does it?). Use the definition of a limit directly with an argument, showing that for all , so somehow the function must be increasing. Any thoughts?","f:(a,b)\rightarrow \mathbb{R} \lim_{h \rightarrow 0} \frac{f(x+h)-f(x-h)}{2h} x\in(a,b) f \epsilon - \delta |h|<\delta \frac{f(x+h)-f(x-h)}{2h}>0","['real-analysis', 'limits', 'derivatives', 'mean-value-theorem']"
25,Limits involving exponents,Limits involving exponents,,"I don't understand this statement from Wolfram Alpha: Since $5^{2k+1}$ grows asymptotically slower than $3^{4k+1}$ as $k$ approaches $\infty$ , $$\lim_{k\to\infty} 3^{-4k-1}\cdot 5^{2k+1} = 0.$$ Doesn't $5^x$ increase faster than $3^x$ ? Does it have something to do with $$3^{-4k-1}\cdot 5^{2k+1}=\frac{5^{2k+1}}{3^{4k+1}}=\left(\frac{5}{3}\right)^{2k+1}\cdot\left(\frac{1}{3}\right)^{2k} \quad ?$$","I don't understand this statement from Wolfram Alpha: Since grows asymptotically slower than as approaches , Doesn't increase faster than ? Does it have something to do with",5^{2k+1} 3^{4k+1} k \infty \lim_{k\to\infty} 3^{-4k-1}\cdot 5^{2k+1} = 0. 5^x 3^x 3^{-4k-1}\cdot 5^{2k+1}=\frac{5^{2k+1}}{3^{4k+1}}=\left(\frac{5}{3}\right)^{2k+1}\cdot\left(\frac{1}{3}\right)^{2k} \quad ?,"['calculus', 'limits', 'limits-without-lhopital']"
26,Is this correct notation for the area under a curve using a limit?,Is this correct notation for the area under a curve using a limit?,,"I've just had a resource tell me the following: The area under a curve from x=a to x=b is A= $lim_{\Delta x{\rightarrow}0}\sum_{k=a}^{b}f(x_{k})\Delta x$ I generally see $x_{k}=a+k\Delta x$ , but in that case I'm struggling to understand how this is correct if the sum limit is b, shouldn't it instead be the number of subintervals? ie $lim_{\Delta x{\rightarrow}0}\sum_{k=1}^{N}f(x_{k})\Delta x$ , where $N=\frac{b}{\Delta x}$ either this is wrong or i am misinterpreting what $x_{k}$ is. Thanks","I've just had a resource tell me the following: The area under a curve from x=a to x=b is A= I generally see , but in that case I'm struggling to understand how this is correct if the sum limit is b, shouldn't it instead be the number of subintervals? ie , where either this is wrong or i am misinterpreting what is. Thanks",lim_{\Delta x{\rightarrow}0}\sum_{k=a}^{b}f(x_{k})\Delta x x_{k}=a+k\Delta x lim_{\Delta x{\rightarrow}0}\sum_{k=1}^{N}f(x_{k})\Delta x N=\frac{b}{\Delta x} x_{k},"['integration', 'limits']"
27,"Derivative at a boundary or limit point, Heine definition of limit","Derivative at a boundary or limit point, Heine definition of limit",,"I have recently read this post on the differentiability of a function at a boundary point. The answer recieving the top vote created a new definition that can lead to the derivative at a limit point. Definition Let $S\subseteq \mathbb{R}$ , and $f$ a $\mathbb{R}$ -valued function defined over $S$ . Let $x\in S$ be a limit point of $S$ . Then we say that $f$ is differentiable at $x$ if there exists a linear function $L$ such that for every sequence of points $x_n\in S$ different from $x$ but converging to $x$ , we have that $$ \lim_{n\to\infty} \frac{f(x_n) - f(x) - L(x_n-x)}{|x_n - x|} = 0 $$ While I understand the basic idea of this definition, I am a little bit confused with the absolute value bar in the denominator and the linear function $L$ in the numerator. When I first saw this definition, it reminded me of Heine's definition of limit, so I proposed the definition down below. Definition Let $S\subseteq \mathbb{R}$ , and $f$ a $\mathbb{R}$ -valued function defined over $S$ . Let $x\in S$ be a limit point of $S$ . Then we say that $f$ is differentiable at $x$ , if there exists a value k such that for every sequence of points $x_{n} \in S$ different from x but converging to x, we have $$\lim_{n \to \infty} \frac{f(x_{n}) - f(x)}{x_{n} - x} = k$$ Since x is a limit point, I think the above definition is a direct result of Heine's definition of limit. So is this definition accurate? If so, why did the original answerer include an absolute value bar and adopted the linear function? Also, can we as well use Heine definition of limit to define continuity? Although this is not related to the question above (you don't have to answer), I wonder if we limit a function's domain to all rational numbers, theorems such as the intermediate value theorem will still hold.","I have recently read this post on the differentiability of a function at a boundary point. The answer recieving the top vote created a new definition that can lead to the derivative at a limit point. Definition Let , and a -valued function defined over . Let be a limit point of . Then we say that is differentiable at if there exists a linear function such that for every sequence of points different from but converging to , we have that While I understand the basic idea of this definition, I am a little bit confused with the absolute value bar in the denominator and the linear function in the numerator. When I first saw this definition, it reminded me of Heine's definition of limit, so I proposed the definition down below. Definition Let , and a -valued function defined over . Let be a limit point of . Then we say that is differentiable at , if there exists a value k such that for every sequence of points different from x but converging to x, we have Since x is a limit point, I think the above definition is a direct result of Heine's definition of limit. So is this definition accurate? If so, why did the original answerer include an absolute value bar and adopted the linear function? Also, can we as well use Heine definition of limit to define continuity? Although this is not related to the question above (you don't have to answer), I wonder if we limit a function's domain to all rational numbers, theorems such as the intermediate value theorem will still hold.",S\subseteq \mathbb{R} f \mathbb{R} S x\in S S f x L x_n\in S x x  \lim_{n\to\infty} \frac{f(x_n) - f(x) - L(x_n-x)}{|x_n - x|} = 0  L S\subseteq \mathbb{R} f \mathbb{R} S x\in S S f x x_{n} \in S \lim_{n \to \infty} \frac{f(x_{n}) - f(x)}{x_{n} - x} = k,"['real-analysis', 'calculus', 'limits', 'derivatives']"
28,"Let $a$ be a set accumulation point $X$. Show that there is either an increasing sequence, or a decreasing sequence of...","Let  be a set accumulation point . Show that there is either an increasing sequence, or a decreasing sequence of...",a X,"Let $a$ be a set accumulation point $X$ . Show that there is either an increasing sequence, or a decreasing sequence of points $x_n \in X$ with $lim_{x_n}=a$ . Let $A_n = (a -\frac{1}{n}, a)$ and $B_n = (a, a +\frac{1}{n})$ , as $a \in A'$ then one of these sets has infinite elements of $A$ , if $A_n$ is infinite we can define $(x_n)$ increasing with $lim x_n = a$ otherwise we define $(y_n)$ decreasing, both with limit $a$ . That's right? is enough? Thanks.","Let be a set accumulation point . Show that there is either an increasing sequence, or a decreasing sequence of points with . Let and , as then one of these sets has infinite elements of , if is infinite we can define increasing with otherwise we define decreasing, both with limit . That's right? is enough? Thanks.","a X x_n \in X lim_{x_n}=a A_n = (a -\frac{1}{n}, a) B_n = (a, a +\frac{1}{n}) a \in A' A A_n (x_n) lim x_n = a (y_n) a","['real-analysis', 'limits', 'solution-verification', 'problem-solving']"
29,"Find two sequences $(a_n)$ and $(a_n)$ such that lim$(a_n)$ = lim$(b_n)$ = 0, but lim$(a_n)^{(b_n)}$ = 2022","Find two sequences  and  such that lim = lim = 0, but lim = 2022",(a_n) (a_n) (a_n) (b_n) (a_n)^{(b_n)},"This is a problem from my real analysis class. I've tried beginning with guesses for the sequences $(a_n)$ and $(b_n)$ to obtain an indeterminate form $0^0$ so that I can take the natural log of $(a_n)^{(b_n)}$ . Then using log properties, I tried to apply L'Hospital's rule and find the limit. I think this is probably the right approach, but so far my guesses have proved inaccurate (i.e. $(a_n)$ = $\frac{1}{n}$ or $\frac{1}{n^2}$ ; have also tried playing around with putting 2022 or ln(2022) in both the numerator and denominator of one of the sequences). I've also tried working backwards, i.e. starting with lim( $(a_n)^{(b_n)}$ ) = 2022 to try to find an $(a_n)$ and $(b_n)$ that work, but neither approach has proved particularly helpful. Hoping for a way to think about this logically to understand the form of the desired sequences.","This is a problem from my real analysis class. I've tried beginning with guesses for the sequences and to obtain an indeterminate form so that I can take the natural log of . Then using log properties, I tried to apply L'Hospital's rule and find the limit. I think this is probably the right approach, but so far my guesses have proved inaccurate (i.e. = or ; have also tried playing around with putting 2022 or ln(2022) in both the numerator and denominator of one of the sequences). I've also tried working backwards, i.e. starting with lim( ) = 2022 to try to find an and that work, but neither approach has proved particularly helpful. Hoping for a way to think about this logically to understand the form of the desired sequences.",(a_n) (b_n) 0^0 (a_n)^{(b_n)} (a_n) \frac{1}{n} \frac{1}{n^2} (a_n)^{(b_n)} (a_n) (b_n),"['real-analysis', 'sequences-and-series', 'limits']"
30,Bertsekas' Penalty Approach's Proof of Necessary Conditions of the Equality Constrant Problem using Lagrange Multipliers,Bertsekas' Penalty Approach's Proof of Necessary Conditions of the Equality Constrant Problem using Lagrange Multipliers,,"I am having some trouble understanding the proof of the Necessary Conditions of the ECP using Lagrange Multipliers in Bertsekas' book Nonlinear programming (1999). The part relevant to my inquiry is the following partial statement Proposition 3.1.1: (Lagrange Multiplier Theorem - Necessary Conditions) Let $x^*$ be a local minimum of $f$ subject to $h(x) = 0$ , and assume  that the constraint gradients $\nabla h_1(x^*), \dots, \nabla h_m(x^*)$ are linearly independent. Then there exists a unique vector $\lambda^*=(\lambda_1^*, \dots, \lambda_m^*)$ called Lagrange multiplier vector , such that $$\nabla f(x^*)+\sum_{i=1}^{m}\lambda_i^*\nabla h_i(x^*)=0$$ [...] Regarding notation, we have the declaration at the start of the section NECESSARY CONDITIONS FOR EQUALITY CONSTRAINTS . We assume $f:\mathbb{R}^n →\mathbb{R}^n$ , $h_i:\mathbb{R}^n→\mathbb{R}$ for $i\in{1,\dots,m}$ are continuously differentiable functions. [...] For notational convenience, we introduce the constraint function $h:\mathbb{R}^n→\mathbb{R}^m$ , where $$h=(h_1,\dots,h_m)$$ For the record, $x^*$ is a local minimum of $f$ if it is the minimizer within a neighborhood of $x^*$ . A feasible point is a point that satisfies the constraints given by the problem. Now, I am going to write down a transcription of the proof presented in the book using the penalty approach. Penalty Approach Here we approximate the original constrainted problem by an unconstrained optimization problem that involves a penalty for violation of the constraints. In particular, for $k\in\mathbb{N}$ , we introduce the cost function $$F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2$$ where $x^*$ is the local minimum of the constrained problem and $\alpha$ is some positive scalar. [...] Since $x^*$ is a local minimum, we can select $\epsilon>0$ such that $f(x^*)\leq f(x)$ for all feasible $x$ in the closed sphere $$S=\{x : ‖x-x^*‖ \leq \epsilon\}$$ Let $x^k$ be an optimal solution of the problem of minimizing $F^k$ subject to $x\in S$ [An optimal solution exists because of Weierstrass' theorem [...]]. We will show that the sequence $\{x^k\}$ converges to $x^*$ . We have for all $k$ $$F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2\leq F^k(x^*)=f(x^*) \tag{1}$$ and since $f(x^k)$ is bounded over $S$ , we obtain $$\lim_{k→\infty}‖h(x^k)‖ = 0$$ otherwise the left hand side of Eq. (1) would become unbounded above as $k→\infty$ . Therefore, every limit point $\bar{x}$ of $\{x^k\}$ satisfies $h(\bar{x})=0$ . Furthermore, Eq. (1) yields $f(x^k)+\frac{\alpha}{2}‖x^k-x^*‖^2\leq f(x^*)$ for all $k$ , so by taking the limit as $k→\infty$ , we obtain $$f(\bar{x})+\frac{\alpha}{2}‖\bar{x}-x^*‖^2\leq f(x^*)$$ [...] My question is very simple regarding to 2 usages of the concept of limit: Regarding to this statement We have for all $k$ $$F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2\leq F^k(x^*)=f(x^*) \tag{1}$$ and since $f(x^k)$ is bounded over $S$ , we obtain $$\lim_{k→\infty}‖h(x^k)‖ = 0$$ I don't see why can we take the limit to be equal to $0$ . As far as I am concerned, we don't know if the sequence $\{x^k\}$ converges at all. Regarding this statement Furthermore, Eq. (1) yields $f(x^k)+\frac{\alpha}{2}‖x^k-x^*‖^2\leq f(x^*)$ for all $k$ , so by taking the limit as $k→\infty$ , we obtain $$f(\bar{x})+\frac{\alpha}{2}‖\bar{x}-x^*‖^2\leq f(x^*)$$ By the same reason as before, I can't quite justify that $\{x^k\}$ converges. I have tried modifying the proof by taking a subsequence that converges, as I do see that by being a bounded sequence, $\{x^k\}$ must have a limit point. And by using this subsequence, I see no problem at all when asserting the previous statements. However, I am probably missing something in the original proof, so I would appreciate if someone could shed some light on this. Thank you in advance.","I am having some trouble understanding the proof of the Necessary Conditions of the ECP using Lagrange Multipliers in Bertsekas' book Nonlinear programming (1999). The part relevant to my inquiry is the following partial statement Proposition 3.1.1: (Lagrange Multiplier Theorem - Necessary Conditions) Let be a local minimum of subject to , and assume  that the constraint gradients are linearly independent. Then there exists a unique vector called Lagrange multiplier vector , such that [...] Regarding notation, we have the declaration at the start of the section NECESSARY CONDITIONS FOR EQUALITY CONSTRAINTS . We assume , for are continuously differentiable functions. [...] For notational convenience, we introduce the constraint function , where For the record, is a local minimum of if it is the minimizer within a neighborhood of . A feasible point is a point that satisfies the constraints given by the problem. Now, I am going to write down a transcription of the proof presented in the book using the penalty approach. Penalty Approach Here we approximate the original constrainted problem by an unconstrained optimization problem that involves a penalty for violation of the constraints. In particular, for , we introduce the cost function where is the local minimum of the constrained problem and is some positive scalar. [...] Since is a local minimum, we can select such that for all feasible in the closed sphere Let be an optimal solution of the problem of minimizing subject to [An optimal solution exists because of Weierstrass' theorem [...]]. We will show that the sequence converges to . We have for all and since is bounded over , we obtain otherwise the left hand side of Eq. (1) would become unbounded above as . Therefore, every limit point of satisfies . Furthermore, Eq. (1) yields for all , so by taking the limit as , we obtain [...] My question is very simple regarding to 2 usages of the concept of limit: Regarding to this statement We have for all and since is bounded over , we obtain I don't see why can we take the limit to be equal to . As far as I am concerned, we don't know if the sequence converges at all. Regarding this statement Furthermore, Eq. (1) yields for all , so by taking the limit as , we obtain By the same reason as before, I can't quite justify that converges. I have tried modifying the proof by taking a subsequence that converges, as I do see that by being a bounded sequence, must have a limit point. And by using this subsequence, I see no problem at all when asserting the previous statements. However, I am probably missing something in the original proof, so I would appreciate if someone could shed some light on this. Thank you in advance.","x^* f h(x) = 0 \nabla h_1(x^*), \dots, \nabla h_m(x^*) \lambda^*=(\lambda_1^*, \dots, \lambda_m^*) \nabla f(x^*)+\sum_{i=1}^{m}\lambda_i^*\nabla h_i(x^*)=0 f:\mathbb{R}^n →\mathbb{R}^n h_i:\mathbb{R}^n→\mathbb{R} i\in{1,\dots,m} h:\mathbb{R}^n→\mathbb{R}^m h=(h_1,\dots,h_m) x^* f x^* k\in\mathbb{N} F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2 x^* \alpha x^* \epsilon>0 f(x^*)\leq f(x) x S=\{x : ‖x-x^*‖ \leq \epsilon\} x^k F^k x\in S \{x^k\} x^* k F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2\leq F^k(x^*)=f(x^*) \tag{1} f(x^k) S \lim_{k→\infty}‖h(x^k)‖ = 0 k→\infty \bar{x} \{x^k\} h(\bar{x})=0 f(x^k)+\frac{\alpha}{2}‖x^k-x^*‖^2\leq f(x^*) k k→\infty f(\bar{x})+\frac{\alpha}{2}‖\bar{x}-x^*‖^2\leq f(x^*) k F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2\leq F^k(x^*)=f(x^*) \tag{1} f(x^k) S \lim_{k→\infty}‖h(x^k)‖ = 0 0 \{x^k\} f(x^k)+\frac{\alpha}{2}‖x^k-x^*‖^2\leq f(x^*) k k→\infty f(\bar{x})+\frac{\alpha}{2}‖\bar{x}-x^*‖^2\leq f(x^*) \{x^k\} \{x^k\}","['limits', 'optimization', 'proof-explanation', 'lagrange-multiplier']"
31,$\lim_{n\rightarrow \infty} n^{2} \sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)}$.,.,\lim_{n\rightarrow \infty} n^{2} \sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)},"Find $$\lim_{n\rightarrow \infty} n^{2}\sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)}.$$ My approach: \begin{align*} I& =\lim_{n\rightarrow \infty} n^{2}\sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)}=\lim_{n\rightarrow \infty}\frac{1}{n}\sum_{k=1}^{n}\frac{1}{(1+(\frac{k}{n})^2)(1+\sqrt{1+(\frac{k}{n})^2})}\\ & =\int_{0}^{1} \frac{1}{(1+x^2)(1+\sqrt{1+x^2})}dx \Longrightarrow  \end{align*} \begin{align*}I & =\int_{0}^{1} \left(\frac{1}{x^2+1}-\frac{1}{\sqrt{x^2+1}}+\frac{1}{1+\sqrt{x^2+1}}\right)dx\\ & =\arctan(1)-\ln(1+\sqrt{2})+\int_{0}^{1}\frac{1}{1+\sqrt{x^2+1}}dx\end{align*} and at this point I am stuck. Any help, please?","Find My approach: and at this point I am stuck. Any help, please?","\lim_{n\rightarrow \infty} n^{2}\sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)}. \begin{align*}
I& =\lim_{n\rightarrow \infty} n^{2}\sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)}=\lim_{n\rightarrow \infty}\frac{1}{n}\sum_{k=1}^{n}\frac{1}{(1+(\frac{k}{n})^2)(1+\sqrt{1+(\frac{k}{n})^2})}\\ & =\int_{0}^{1} \frac{1}{(1+x^2)(1+\sqrt{1+x^2})}dx \Longrightarrow 
\end{align*} \begin{align*}I & =\int_{0}^{1} \left(\frac{1}{x^2+1}-\frac{1}{\sqrt{x^2+1}}+\frac{1}{1+\sqrt{x^2+1}}\right)dx\\ & =\arctan(1)-\ln(1+\sqrt{2})+\int_{0}^{1}\frac{1}{1+\sqrt{x^2+1}}dx\end{align*}","['integration', 'sequences-and-series']"
32,Show that $x_n$ is monotone and bounded: $x_{1} = 1$ and $x_{n+1}$ = $\sqrt{4x_n -1}$,Show that  is monotone and bounded:  and  =,x_n x_{1} = 1 x_{n+1} \sqrt{4x_n -1},"A sequence is given by $X_{1} = 1$ and $X_{n+1}$ = $\sqrt{4x_n -1}$ for $n \in \mathbb{N}$ . Show that $x_n$ is monotone and bounded. Find $\lim n \to \infty$ $x_n$ . Proposed solutions: I claim that $|x_n| \leq 4$ for $n \in \mathbb{N}$ By induction: Base case: $x_1 = 1 < x_2 = \sqrt{3} < 4 $ Induction hypothesis: $|x_n| \leq 4$ , prove that $|x_{n+1}| \leq 4$ $|x_{n+1}| = |\sqrt{4x_n -1}| \leq |\sqrt{4x_n}| = 2 |\sqrt{x_n}| \leq 4$ Induction step: $|x_n| \leq 4$ for all $n \in \mathbb{N}$ Now, show that the sequence is increasing??? $\lim n \to \infty$ $x_n = \lim n \to \infty \sqrt{4x_n -1} = L$ $\lim n \to \infty$ $4x_n - 1 = L^2$ $4L - 1 = L^2$ $L^2 - 4L + 1 = 0$ $L = 2 + \sqrt{3}$ (by the quadratic formula)","A sequence is given by and = for . Show that is monotone and bounded. Find . Proposed solutions: I claim that for By induction: Base case: Induction hypothesis: , prove that Induction step: for all Now, show that the sequence is increasing??? (by the quadratic formula)",X_{1} = 1 X_{n+1} \sqrt{4x_n -1} n \in \mathbb{N} x_n \lim n \to \infty x_n |x_n| \leq 4 n \in \mathbb{N} x_1 = 1 < x_2 = \sqrt{3} < 4  |x_n| \leq 4 |x_{n+1}| \leq 4 |x_{n+1}| = |\sqrt{4x_n -1}| \leq |\sqrt{4x_n}| = 2 |\sqrt{x_n}| \leq 4 |x_n| \leq 4 n \in \mathbb{N} \lim n \to \infty x_n = \lim n \to \infty \sqrt{4x_n -1} = L \lim n \to \infty 4x_n - 1 = L^2 4L - 1 = L^2 L^2 - 4L + 1 = 0 L = 2 + \sqrt{3},"['real-analysis', 'limits', 'analysis', 'solution-verification']"
33,The total variation of a continuous $f$ satisfies: $\lim_{\epsilon \to \infty} T_\epsilon ^1(f) = T_0^1(f)$.,The total variation of a continuous  satisfies: .,f \lim_{\epsilon \to \infty} T_\epsilon ^1(f) = T_0^1(f),"This is essentially a supremum and limits question. I've encountered the following claim: If $f$ is continuous on $[0,1]$ , then $f$ satisfies: $\lim_{\epsilon \to 0^+} T_\epsilon ^1(f) = T_0^1(f)$ . where: $T_a^b(f) := \sup\big\{\sum_{i=1}^n|f(x_i)-f(y_i)| \mid \{(x_i,y_i)\}_{i=1}^k \text{are a partition of [a,b]}\big\}$ . I realize that I have to use the formal definition of the limit to get around the fact that $\epsilon$ is a part of the partition's definition, so I tried using supremum definitions to get to something like the following: Denoting the internal sum (i.e. the per-partition variation) by $V(f,P)$ , we want to show that for every $\epsilon'>0$ there exists an $N>0$ such that for all $n>N$ : $$| \sup\big\{V(f,P) \mid \ P \text{ is a partition of } [0, 1]\big\} - \sup\big\{V(f,P) \mid \ P \text{ is a partition of } [\frac{1}{n}, 1]\big\} | <\epsilon'$$ but I'm not sure what to do with the supremums (my inuition is that somehow I can bound this by $|f(\frac{1}{k})-f(0)|$ where $k$ will be chosen such that $|\frac{1}{k}-0|\leq\delta$ and $\delta$ matches $\epsilon'$ from $f$ 's continuity around $0$ , so that we obtain $|f(\frac{1}{k})-f(0)| < \epsilon'$ ), but as I'm a bit rusty on handling supremums in such situations, I'm not sure how to move from the supremums inequality to this. Edit: I think the following comes close to solving it: Let $P$ by a partition of $[\frac{1}{n},1]$ . Denote by $P^+$ the partition of $[0,1]$ such that its first two points are $\{0,\frac{1}{n}\}$ and the remaining are the points in $P$ after $x=\frac{1}{n}$ . Then we have: $$\sup_{P}V(f,P^+)-\sup_PV(f,P)\leq \sup_P\big(V(f,P^+)-V(f,P)\big) \leq \sup_P\big(|f\big(\frac{1}{n}\big)-f(0)|\big)\leq\epsilon'$$ for $n$ sufficiently large, by continuity of $f$ around 0. The first inequality follows from: $$\sup(A+B)=\sup A+\sup B$$ where, in our case: $$A:= V(f,P^+)-V(f,P)$$ $$B:=V(f,P)$$ It doesn't solve it though because the partitions on $[0,1]$ are limited to those of the type $P^+$ . Any advice? Thanks!","This is essentially a supremum and limits question. I've encountered the following claim: If is continuous on , then satisfies: . where: . I realize that I have to use the formal definition of the limit to get around the fact that is a part of the partition's definition, so I tried using supremum definitions to get to something like the following: Denoting the internal sum (i.e. the per-partition variation) by , we want to show that for every there exists an such that for all : but I'm not sure what to do with the supremums (my inuition is that somehow I can bound this by where will be chosen such that and matches from 's continuity around , so that we obtain ), but as I'm a bit rusty on handling supremums in such situations, I'm not sure how to move from the supremums inequality to this. Edit: I think the following comes close to solving it: Let by a partition of . Denote by the partition of such that its first two points are and the remaining are the points in after . Then we have: for sufficiently large, by continuity of around 0. The first inequality follows from: where, in our case: It doesn't solve it though because the partitions on are limited to those of the type . Any advice? Thanks!","f [0,1] f \lim_{\epsilon \to 0^+} T_\epsilon ^1(f) = T_0^1(f) T_a^b(f) := \sup\big\{\sum_{i=1}^n|f(x_i)-f(y_i)| \mid \{(x_i,y_i)\}_{i=1}^k \text{are a partition of [a,b]}\big\} \epsilon V(f,P) \epsilon'>0 N>0 n>N | \sup\big\{V(f,P) \mid \ P \text{ is a partition of } [0, 1]\big\} - \sup\big\{V(f,P) \mid \ P \text{ is a partition of } [\frac{1}{n}, 1]\big\} | <\epsilon' |f(\frac{1}{k})-f(0)| k |\frac{1}{k}-0|\leq\delta \delta \epsilon' f 0 |f(\frac{1}{k})-f(0)| < \epsilon' P [\frac{1}{n},1] P^+ [0,1] \{0,\frac{1}{n}\} P x=\frac{1}{n} \sup_{P}V(f,P^+)-\sup_PV(f,P)\leq \sup_P\big(V(f,P^+)-V(f,P)\big) \leq \sup_P\big(|f\big(\frac{1}{n}\big)-f(0)|\big)\leq\epsilon' n f \sup(A+B)=\sup A+\sup B A:= V(f,P^+)-V(f,P) B:=V(f,P) [0,1] P^+","['real-analysis', 'calculus', 'limits', 'supremum-and-infimum']"
34,Question about the limit $\lim\limits_{+\infty} \tfrac{x^4}{1+x^4(\cos(x))^2}$ and result given by walpha,Question about the limit  and result given by walpha,\lim\limits_{+\infty} \tfrac{x^4}{1+x^4(\cos(x))^2},I have a problem evaluating this limit $$\lim\limits_{x\to +\infty} \dfrac{x^4}{1+x^4(\cos(x))^2}$$ I'm still not able to get value Wolframalpha gives : $2$ . but if this was true then the reciprocal of this function should tend to $\dfrac 1 2$ but the reciprocal is $$\dfrac{1}{x^4} + \cos^2(x)$$ which has no limit since first term tends to $0$ and second has no limit. Where am I mistaken ? thanks for help.,I have a problem evaluating this limit I'm still not able to get value Wolframalpha gives : . but if this was true then the reciprocal of this function should tend to but the reciprocal is which has no limit since first term tends to and second has no limit. Where am I mistaken ? thanks for help.,\lim\limits_{x\to +\infty} \dfrac{x^4}{1+x^4(\cos(x))^2} 2 \dfrac 1 2 \dfrac{1}{x^4} + \cos^2(x) 0,"['calculus', 'limits']"
35,Function of $x$ which diverges for $x\to c$ while derivative converges to a constant,Function of  which diverges for  while derivative converges to a constant,x x\to c,"Is there a function $f(x)$ of one real variable and a constant $c$ with the following two properties: $\lim_{x\to c} f'(x)$ converges to some finite value $\lim_{x\to c} f(x) = \infty$ With $\lim_{x\to\infty}$ , I could take $f'(x) = \frac{x}{x+1}$ . For the limes going to constant $c$ , I tried a trival replacement using $f'(x) = \frac{1/(x-c)}{1/(x-c) + 1}$ , but then end up with $f$ not being divergent.","Is there a function of one real variable and a constant with the following two properties: converges to some finite value With , I could take . For the limes going to constant , I tried a trival replacement using , but then end up with not being divergent.",f(x) c \lim_{x\to c} f'(x) \lim_{x\to c} f(x) = \infty \lim_{x\to\infty} f'(x) = \frac{x}{x+1} c f'(x) = \frac{1/(x-c)}{1/(x-c) + 1} f,"['calculus', 'limits']"
36,For this limit $\lim_{x \to 5}{\sqrt{x-1}}=2$ find $ δ$,For this limit  find,\lim_{x \to 5}{\sqrt{x-1}}=2  δ,"Question on delta epsilon definition of a limit For the limit $\lim_{x \to 5}{\sqrt{x-1}}=2$ , find a $δ > 0$ that works for $ε = 1$ . I'm getting $δ=1$ , But I'm not 100% confirm that my ans is correct. Please help in this question.","Question on delta epsilon definition of a limit For the limit , find a that works for . I'm getting , But I'm not 100% confirm that my ans is correct. Please help in this question.",\lim_{x \to 5}{\sqrt{x-1}}=2 δ > 0 ε = 1 δ=1,"['calculus', 'limits', 'epsilon-delta']"
37,Using limits properties when proving using epsilon-delta definition.,Using limits properties when proving using epsilon-delta definition.,,"I want to prove $\lim\limits_{x \to 5} \sqrt{2x+6} = 4$ using the epsilon-delta definition. My intuition to proving this is to use the root rule and square both ends of the equation to end up with: $\lim\limits_{x \to 5} 2x+6 = 16$ I would then continue to prove the later limit as I would any other. Is this a valid way to approach this proof, or am I missing something?","I want to prove using the epsilon-delta definition. My intuition to proving this is to use the root rule and square both ends of the equation to end up with: I would then continue to prove the later limit as I would any other. Is this a valid way to approach this proof, or am I missing something?",\lim\limits_{x \to 5} \sqrt{2x+6} = 4 \lim\limits_{x \to 5} 2x+6 = 16,"['calculus', 'limits', 'proof-writing', 'epsilon-delta']"
38,Limit of the sequence by recurrence of mean of the last two values.,Limit of the sequence by recurrence of mean of the last two values.,,"The sequence is defined by recurrence: $x_1=1$ , $x_2=2$ \begin{equation} x_{n+1} = \frac{1}{2}(x_{n}+x_{n-1}) \end{equation} It is asked to calculate the limit. I have seen that the sequence is bounded by 1 and 2. On the other hand, I have seen that the sequence of the pairs, $x_{2n}$ , is decreasing and the sequence of the odd, $x_{2n-1}$ , is increasing. Therefore both are convergent. I have seen that both limits are equal, therefore the original sequence is convergent with the same limit. But I won't be able to prove what the limit is.","The sequence is defined by recurrence: , It is asked to calculate the limit. I have seen that the sequence is bounded by 1 and 2. On the other hand, I have seen that the sequence of the pairs, , is decreasing and the sequence of the odd, , is increasing. Therefore both are convergent. I have seen that both limits are equal, therefore the original sequence is convergent with the same limit. But I won't be able to prove what the limit is.","x_1=1 x_2=2 \begin{equation}
x_{n+1} = \frac{1}{2}(x_{n}+x_{n-1})
\end{equation} x_{2n} x_{2n-1}","['sequences-and-series', 'limits', 'recurrence-relations']"
39,"Spivak Calculus, Ch. 5 Limits: Understanding the mathematical logic underlying conclusion of proof of uniqueness of limit of function","Spivak Calculus, Ch. 5 Limits: Understanding the mathematical logic underlying conclusion of proof of uniqueness of limit of function",,"In Spivak's Calculus, Ch. 5 on Limits, there is the following theorem about the uniqueness of a limit of a function near a point: A function cannot approach two different limits near $a$ . In other words, if $f$ approaches $l$ near $a$ , and $f$ approaches $m$ near $a$ , then $l=m$ Here is the proof: ""f approaches $l$ near $a$ "" $$\forall \epsilon>0\ \exists\delta_1>0, |x-a|<\delta_1\implies |f(x)-l|<\epsilon$$ ""f approaches $m$ near $a$ "" $$\forall \epsilon>0\ \exists\delta_2>0, |x-a|<\delta_2\implies |f(x)-m|<\epsilon$$ $\delta=min(\delta_1, \delta_2)$ $$\implies \forall \epsilon>0\ \exists\delta>0, |x-a|<\delta\implies |f(x)-m|<\epsilon,|f(x)-l|<\epsilon\tag{1}$$ Assume $m\neq l$ Choose $\epsilon=\frac{|m-l|}{2}$ . $$|x-a|<\delta \implies |f(x)-m|<\frac{|m-l|}{2}, |f(x)-l|<\frac{|m-l|}{2}$$ \begin{align*} |m-l|&=|m-f(x)+f(x)-l|\\      &\leq |m-f(x)|+|f(x)-l|\\      &< \frac{|m-l|}{2}+\frac{|m-l|}{2}\\      &= |m-l| \end{align*} Therefore, with this reasoning we've reached the conclusion that $|m-l|<|m-l|$ , which is a contradiction. I am fine with the proof itself. My questions are about the actual logic that allows me to conclude that the proof was successful. Questions: When we say the conclusion is a contradiction, what is it contradicting? The fact that a number is smaller than itself? So here some basic axiom of numbers is being contradicted? More importantly, what is the exact statement that because of our conclusion, is now shown to be false? I believe it is the statement $(1)$ above. What is the negation of statement $(1)$ exactly? In general terms I believe it is ""we found an $\epsilon$ for which there is no $\delta$ such that $|x-a|<\delta\implies|f(x)-m|<\epsilon,|f(x)-l|<\epsilon$ "". Reformulating this statement: $$\exists\epsilon>0\ \text{such that}\ \nexists \delta>0\ \text{such that}\ |x-a|<\delta\implies |f(x)-m|<\epsilon,|f(x)-l|<\epsilon$$ Is this the correct conclusion? Ie, the negation of $(1)$ , which is true? Finally, as an extra if anyone can give their two cents: I haven't studied mathematical logic, but I think it would be a good idea, though I am not sure what to study (propositional logic?). What does one study so that questions of the type I am asking above aren't an issue anymore?","In Spivak's Calculus, Ch. 5 on Limits, there is the following theorem about the uniqueness of a limit of a function near a point: A function cannot approach two different limits near . In other words, if approaches near , and approaches near , then Here is the proof: ""f approaches near "" ""f approaches near "" Assume Choose . Therefore, with this reasoning we've reached the conclusion that , which is a contradiction. I am fine with the proof itself. My questions are about the actual logic that allows me to conclude that the proof was successful. Questions: When we say the conclusion is a contradiction, what is it contradicting? The fact that a number is smaller than itself? So here some basic axiom of numbers is being contradicted? More importantly, what is the exact statement that because of our conclusion, is now shown to be false? I believe it is the statement above. What is the negation of statement exactly? In general terms I believe it is ""we found an for which there is no such that "". Reformulating this statement: Is this the correct conclusion? Ie, the negation of , which is true? Finally, as an extra if anyone can give their two cents: I haven't studied mathematical logic, but I think it would be a good idea, though I am not sure what to study (propositional logic?). What does one study so that questions of the type I am asking above aren't an issue anymore?","a f l a f m a l=m l a \forall \epsilon>0\ \exists\delta_1>0, |x-a|<\delta_1\implies |f(x)-l|<\epsilon m a \forall \epsilon>0\ \exists\delta_2>0, |x-a|<\delta_2\implies |f(x)-m|<\epsilon \delta=min(\delta_1, \delta_2) \implies \forall \epsilon>0\ \exists\delta>0, |x-a|<\delta\implies |f(x)-m|<\epsilon,|f(x)-l|<\epsilon\tag{1} m\neq l \epsilon=\frac{|m-l|}{2} |x-a|<\delta \implies |f(x)-m|<\frac{|m-l|}{2}, |f(x)-l|<\frac{|m-l|}{2} \begin{align*}
|m-l|&=|m-f(x)+f(x)-l|\\
     &\leq |m-f(x)|+|f(x)-l|\\
     &< \frac{|m-l|}{2}+\frac{|m-l|}{2}\\
     &= |m-l|
\end{align*} |m-l|<|m-l| (1) (1) \epsilon \delta |x-a|<\delta\implies|f(x)-m|<\epsilon,|f(x)-l|<\epsilon \exists\epsilon>0\ \text{such that}\ \nexists \delta>0\ \text{such that}\ |x-a|<\delta\implies |f(x)-m|<\epsilon,|f(x)-l|<\epsilon (1)","['calculus', 'limits', 'limits-without-lhopital']"
40,Bounded minimum value problem (from an exam today in Australia).,Bounded minimum value problem (from an exam today in Australia).,,"A question appeared in today's exam in Victoria (Australia), in which a graphical calculator was also allowed to be used (just for context). This question is causing some 'controversy', both in the interpretation of the wording, as well as the actual answer. Consider the function $g_a(x)=\sin(\frac{x}{a})+\cos(ax), a\in \mathbb{Z}^{+}. \text{ and } x\in \mathbb{R}$ . State the greatest possible minimum value of $g_a$ (1 mark). (This is not necessarily word for word (except for the last sentence), but this was indeed the premise of the question) For this, I stated that this value is $-\sqrt2$ , occurring for $a=1$ . Upon graphing, if we call the minimum value $m$ , it becomes clear that $-2<m\leq-\sqrt{2}$ . (The $-2$ can be seen as this only occurs upon the superposition of minima, which never occurs but does get approached ). Now, a solution posted online is that: $$\lim_{a\to \infty}g_a(x)=\cos(ax) \\ \therefore \min(g_a)=-1$$ However, to me this limit is nonsensical as the value of $x$ for which the minimum occurs increases as $a\to \infty$ sufficiently so as for there to always exist a value of $x$ such that $-2<g_a(x)\leq-\sqrt{2}$ , even if this occurs for very large (negative or positive) values of $x$ . This can be seen just by graphing on something like Desmos, where if zoomed out, you will see the value go below $-1$ . What is a mathematically rigorous way of showing this, seeing as this was given to Year 12s?","A question appeared in today's exam in Victoria (Australia), in which a graphical calculator was also allowed to be used (just for context). This question is causing some 'controversy', both in the interpretation of the wording, as well as the actual answer. Consider the function . State the greatest possible minimum value of (1 mark). (This is not necessarily word for word (except for the last sentence), but this was indeed the premise of the question) For this, I stated that this value is , occurring for . Upon graphing, if we call the minimum value , it becomes clear that . (The can be seen as this only occurs upon the superposition of minima, which never occurs but does get approached ). Now, a solution posted online is that: However, to me this limit is nonsensical as the value of for which the minimum occurs increases as sufficiently so as for there to always exist a value of such that , even if this occurs for very large (negative or positive) values of . This can be seen just by graphing on something like Desmos, where if zoomed out, you will see the value go below . What is a mathematically rigorous way of showing this, seeing as this was given to Year 12s?","g_a(x)=\sin(\frac{x}{a})+\cos(ax), a\in \mathbb{Z}^{+}. \text{ and } x\in \mathbb{R} g_a -\sqrt2 a=1 m -2<m\leq-\sqrt{2} -2 \lim_{a\to \infty}g_a(x)=\cos(ax) \\ \therefore \min(g_a)=-1 x a\to \infty x -2<g_a(x)\leq-\sqrt{2} x -1","['limits', 'functions', 'upper-lower-bounds']"
41,About a claim in limits,About a claim in limits,,"Let $a_n\geq 0$ . If $\displaystyle \lim_{n\to +\infty} a_n=L$ then $\displaystyle\lim_{n\to +\infty} \sqrt[k]{a_n}=\sqrt[k]{L}$ , $(k\in N)$ I am trying to show this using that: $a^k-b^k=(a-b)(a^{k-1}+a^{k-2}b+...+b^{k-1})$ .","Let . If then , I am trying to show this using that: .",a_n\geq 0 \displaystyle \lim_{n\to +\infty} a_n=L \displaystyle\lim_{n\to +\infty} \sqrt[k]{a_n}=\sqrt[k]{L} (k\in N) a^k-b^k=(a-b)(a^{k-1}+a^{k-2}b+...+b^{k-1}),"['calculus', 'limits', 'proof-writing']"
42,"Question in my work in the problem ""if $f:[a,b]\to\mathbb{R}$ has finite limit in every point of $[a,b]$ then $f$ is bounded""","Question in my work in the problem ""if  has finite limit in every point of  then  is bounded""","f:[a,b]\to\mathbb{R} [a,b] f","Let $f:[a,b]\to\mathbb{R}$ a function such that $f$ has finite limit in every point $x \in [a,b]$ , prove that $f$ is bounded. I was thinking if this problem could be solved in the following way: since by hypothesis $f$ has finite limit for every $x \in [a,b]$ , I can define $\tilde{f}:[a,b]\to \mathbb{R}$ such that $\tilde{f}$ has the same value of the limits of $f$ in every point of $[a,b]$ . By doing this, $\tilde{f}$ is continuous in all $[a,b]$ and since $[a,b]$ is compact it follows that $\tilde{f}$ has absolute maximum and minimum in $[a,b]$ and these maximum and minimum are bounds for $f$ in $[a,b]$ , hence $f$ is bounded. Could this work? If this is wrong, can someone explain me why it doesn't work? Moreover, is there a way to write formally the function $\tilde{f}$ ?","Let a function such that has finite limit in every point , prove that is bounded. I was thinking if this problem could be solved in the following way: since by hypothesis has finite limit for every , I can define such that has the same value of the limits of in every point of . By doing this, is continuous in all and since is compact it follows that has absolute maximum and minimum in and these maximum and minimum are bounds for in , hence is bounded. Could this work? If this is wrong, can someone explain me why it doesn't work? Moreover, is there a way to write formally the function ?","f:[a,b]\to\mathbb{R} f x \in [a,b] f f x \in [a,b] \tilde{f}:[a,b]\to \mathbb{R} \tilde{f} f [a,b] \tilde{f} [a,b] [a,b] \tilde{f} [a,b] f [a,b] f \tilde{f}","['real-analysis', 'limits', 'solution-verification']"
43,Limit - Multivariable Calculus,Limit - Multivariable Calculus,,"I'm having difficulty finding this limit: $\lim_{(x,y) \to (0,0)} \sqrt{\frac{{|y|}}{|x|}}|y|^{m}$ , where $m$ is a positive real number. I actually stumbled upon this in the middle of a question, where I reduced the original limit to it. Now, I'm stuck. Wolfram says this limit equals $0$ (and it should be zero), which I ""intuitively"" understand (because the exponent of the numerator is greater than the denominator, we can even generalize it to $\lim_{(x,y) \to (0,0)} \frac{x^{a}}{y^{b}}$ , where $a > b$ ). My approaches were polar coordinates, which reduced it to $r^{m}(\sin{\theta})^{m}\sqrt{\tan{\theta}}$ , but this is actually unbounded; and some manipulations to use the squeeze theorem, but nothing great happened. Thanks in advance.","I'm having difficulty finding this limit: , where is a positive real number. I actually stumbled upon this in the middle of a question, where I reduced the original limit to it. Now, I'm stuck. Wolfram says this limit equals (and it should be zero), which I ""intuitively"" understand (because the exponent of the numerator is greater than the denominator, we can even generalize it to , where ). My approaches were polar coordinates, which reduced it to , but this is actually unbounded; and some manipulations to use the squeeze theorem, but nothing great happened. Thanks in advance.","\lim_{(x,y) \to (0,0)} \sqrt{\frac{{|y|}}{|x|}}|y|^{m} m 0 \lim_{(x,y) \to (0,0)} \frac{x^{a}}{y^{b}} a > b r^{m}(\sin{\theta})^{m}\sqrt{\tan{\theta}}","['calculus', 'limits', 'multivariable-calculus']"
44,Evaluating $\lim_{x\to 0}\frac{1-\cos(x^2)}{x^3(4^x-1)}$,Evaluating,\lim_{x\to 0}\frac{1-\cos(x^2)}{x^3(4^x-1)},I need to calculate $$\lim_{x\rightarrow 0}\frac{1-\cos(x^2)}{x^3(4^x-1)}$$ and the options are: (a) $\frac 12 \ln 2\quad$ (b) $\ln 2\quad$ (c) $\ln 4\quad$ (d) $1 - \frac 12 \ln \left( \frac{e^2}{4}\right)$ . The answers would are given to be $b$ and $d$ I tried to solve it in the following manner: \begin{align}\lim_{x\rightarrow 0}\frac{1-\cos(x^2)}{x^3(4^x-1)} &=\lim_{x\rightarrow 0}(\frac{2\sin^2(\frac{x^2}{2})}{x^4}\cdot\frac{x}{4^x-1})\\ &=\lim_{x\rightarrow 0}(\frac{2\sin^2(\frac{x^2}{2})}{(\frac{x^2}{2})^2\cdot 4}\cdot\frac{x}{4^x-1})\\ &=\lim_{x\rightarrow 0}(\frac{1}{2}\frac{\sin^2(\frac{x^2}{2})}{(\frac{x^2}{2})^2})\lim_{x\rightarrow 0}(\frac{x}{4^x-1})\\ &=\frac{1}{2}\frac{1}{\ln(4)}\\ &=\frac{1}{4}\log_2(e). \end{align} Is my solution correct? Or am I missing something?,I need to calculate and the options are: (a) (b) (c) (d) . The answers would are given to be and I tried to solve it in the following manner: Is my solution correct? Or am I missing something?,"\lim_{x\rightarrow 0}\frac{1-\cos(x^2)}{x^3(4^x-1)} \frac 12 \ln 2\quad \ln 2\quad \ln 4\quad 1 - \frac 12 \ln \left( \frac{e^2}{4}\right) b d \begin{align}\lim_{x\rightarrow 0}\frac{1-\cos(x^2)}{x^3(4^x-1)} &=\lim_{x\rightarrow 0}(\frac{2\sin^2(\frac{x^2}{2})}{x^4}\cdot\frac{x}{4^x-1})\\
&=\lim_{x\rightarrow 0}(\frac{2\sin^2(\frac{x^2}{2})}{(\frac{x^2}{2})^2\cdot 4}\cdot\frac{x}{4^x-1})\\
&=\lim_{x\rightarrow 0}(\frac{1}{2}\frac{\sin^2(\frac{x^2}{2})}{(\frac{x^2}{2})^2})\lim_{x\rightarrow 0}(\frac{x}{4^x-1})\\
&=\frac{1}{2}\frac{1}{\ln(4)}\\
&=\frac{1}{4}\log_2(e).
\end{align}","['calculus', 'limits', 'trigonometry', 'limits-without-lhopital']"
45,Sequence $a_1=2$ and $a_n=\frac{2}{1+a_{n-1}}$ is convergent or divergent? [duplicate],Sequence  and  is convergent or divergent? [duplicate],a_1=2 a_n=\frac{2}{1+a_{n-1}},"This question already has answers here : Prove that sequence $ x_{(n+1)}= \frac {a}{1+x_n}$ is convergent to positive root of $x^2+x-a=0,$where $a >0$ and $x_1 >0$ [duplicate] (2 answers) Closed 2 years ago . Given a sequence with $a_1=2$ and $a_n=\frac{a_1}{1+a_{n-1}}$ , I have to check whether this sequence is convergent or divergent. I have reached the conclusion till now that its subsequence $a_{2n}$ is an increasing sequence and bounded above, while the subsequence $a_{2n-1}$ is a decreasing sequence and bounded below. So, both the subsequences are convergent. But I am not able to find $\lim_{n\to\infty} a_{2n}$ or $\lim_{n\to\infty} a_{2n-1}$ . Can you help me to proceed further or maybe find another way to show its convergence or divergence?","This question already has answers here : Prove that sequence $ x_{(n+1)}= \frac {a}{1+x_n}$ is convergent to positive root of $x^2+x-a=0,$where $a >0$ and $x_1 >0$ [duplicate] (2 answers) Closed 2 years ago . Given a sequence with and , I have to check whether this sequence is convergent or divergent. I have reached the conclusion till now that its subsequence is an increasing sequence and bounded above, while the subsequence is a decreasing sequence and bounded below. So, both the subsequences are convergent. But I am not able to find or . Can you help me to proceed further or maybe find another way to show its convergence or divergence?",a_1=2 a_n=\frac{a_1}{1+a_{n-1}} a_{2n} a_{2n-1} \lim_{n\to\infty} a_{2n} \lim_{n\to\infty} a_{2n-1},"['real-analysis', 'sequences-and-series']"
46,"Prove$\lim \limits_{x \to \infty} e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx$ exists and find it, for constant $P>0$ and $Q'(x) \to 0$ as $x \to \infty$","Prove exists and find it, for constant  and  as",\lim \limits_{x \to \infty} e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx P>0 Q'(x) \to 0 x \to \infty,"I'm looking to prove $\lim \limits_{x \to \infty} e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx$ exists and determine what it is, for constant $P>0$ and a continuous $Q'(x)$ s.t. $Q'(x) \to 0$ as $x \to \infty$ I can explain what would happen but I'm not sure how to put it rigorously. Consider the successive applications of integration by parts to $e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx$ : $$\Rightarrow Q'(x)\cdot\frac{1}{P^2}-e^{-Px}\int \left[ Q''(x)\int \frac{e^{Px}}{P} \right]$$ $$\Rightarrow Q'(x)\cdot\frac{1}{P^2}-Q''(x)\cdot\frac{1}{P^3}+ e^{-Px}\int \left[ Q'''(x)\int\int \frac{e^{Px}}{P} \right]$$ And so on. Since the $e^{-Px}$ term will keep on cancelling the effect of $e^{Px}$ for successive integration by parts, it seems therefore the convergence of the function will mainly be determined by our supposition that $Q'(x) \to 0$ as $x \to \infty$ , so it seems to me that the function converges to $0$ , but I don't know how to state this rigorously. How exactly would I begin to show this, since it seems to me we can't estimate how $Q''(x), Q'''(x), ...$ behave other than the fact they approach $0$ as $x \to \infty$ ? This got me stuck on estimating the value of the integral at the end of each application of integration by parts, so any help is appreciated!","I'm looking to prove exists and determine what it is, for constant and a continuous s.t. as I can explain what would happen but I'm not sure how to put it rigorously. Consider the successive applications of integration by parts to : And so on. Since the term will keep on cancelling the effect of for successive integration by parts, it seems therefore the convergence of the function will mainly be determined by our supposition that as , so it seems to me that the function converges to , but I don't know how to state this rigorously. How exactly would I begin to show this, since it seems to me we can't estimate how behave other than the fact they approach as ? This got me stuck on estimating the value of the integral at the end of each application of integration by parts, so any help is appreciated!","\lim \limits_{x \to \infty} e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx P>0 Q'(x) Q'(x) \to 0 x \to \infty e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx \Rightarrow Q'(x)\cdot\frac{1}{P^2}-e^{-Px}\int \left[ Q''(x)\int \frac{e^{Px}}{P} \right] \Rightarrow Q'(x)\cdot\frac{1}{P^2}-Q''(x)\cdot\frac{1}{P^3}+ e^{-Px}\int \left[ Q'''(x)\int\int \frac{e^{Px}}{P} \right] e^{-Px} e^{Px} Q'(x) \to 0 x \to \infty 0 Q''(x), Q'''(x), ... 0 x \to \infty","['real-analysis', 'calculus', 'limits', 'convergence-divergence']"
47,Computing $\underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx$.,Computing .,\underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx,"I am interested in $$ \underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 f(x,n) dx = \underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx, $$ where $q\in(0,1)$ . Numerical computation suggests that it is 0. My approach so far has been to express the second factor of $f(x,n)$ as a Gauss hypergeometric function. $$\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1} = {}_1F_0\left((1-n);(1-q)x^{\frac{1}{1-n}}\right)$$ $$ \int x^\frac{1}{1-n}{}_1F_{0}\left((1-n);(1-q)x^\frac{1}{1-n}\right)dx = {}_2F_{1}\left((1-n);(2-n);(3-n);(1-q)x^\frac{1}{1-n}\right) $$ I would appreciate any hint, how to compute the limit. Thank you. Edit: Based on the suggestions below, I apply the Dominated Convergence theorem: $$\underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 f(x,n)dx = \int_{0}^1 \underset{n\rightarrow+\infty}{\lim}f(x,n)dx. $$ $\underset{n\rightarrow+\infty}{\lim} x^{\frac{1}{1-n}} = 1$ and $\underset{n\rightarrow+\infty}{\lim} \left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1} = 0$ . Thus, I conclude that $$\underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx = 0.$$","I am interested in where . Numerical computation suggests that it is 0. My approach so far has been to express the second factor of as a Gauss hypergeometric function. I would appreciate any hint, how to compute the limit. Thank you. Edit: Based on the suggestions below, I apply the Dominated Convergence theorem: and . Thus, I conclude that"," \underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 f(x,n) dx = \underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx,  q\in(0,1) f(x,n) \left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1} = {}_1F_0\left((1-n);(1-q)x^{\frac{1}{1-n}}\right)  \int x^\frac{1}{1-n}{}_1F_{0}\left((1-n);(1-q)x^\frac{1}{1-n}\right)dx = {}_2F_{1}\left((1-n);(2-n);(3-n);(1-q)x^\frac{1}{1-n}\right)  \underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 f(x,n)dx = \int_{0}^1 \underset{n\rightarrow+\infty}{\lim}f(x,n)dx.  \underset{n\rightarrow+\infty}{\lim} x^{\frac{1}{1-n}} = 1 \underset{n\rightarrow+\infty}{\lim} \left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1} = 0 \underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx = 0.","['calculus', 'integration', 'limits', 'hypergeometric-function']"
48,Prove a multivariable limit of a quotient using the epison-delta definition,Prove a multivariable limit of a quotient using the epison-delta definition,,"I have to prove that $\displaystyle\lim_{(x,y)\to(1,0)} \frac{(x-1)^3\cos{y}}{(x-1)^2+y^2}=0$ using the epislon-delta definition of a limit. Here's what I figured: $\delta>|x-1| \Rightarrow \delta>(x-1)^2\Rightarrow \delta^3>|(x-1)^3|$ . Also, $\delta>|y|\geq|cos{y}| \Rightarrow \delta^2>y^2$ . So $\delta^4>|(x-1)^3\cos{y}|$ and $2\delta^2>|(x-1)^2+y^2|$ . Obviously, $\delta^4>|\frac{(x-1)^3\cos{y}}{(x-1)^2+y^2}|$ if $(x-1)^2+y^2\geq1.$ But how do I account for it otherwise? I can't think of anything. I'm having issues proving the limits of quotients in general. Thank you for your help!","I have to prove that using the epislon-delta definition of a limit. Here's what I figured: . Also, . So and . Obviously, if But how do I account for it otherwise? I can't think of anything. I'm having issues proving the limits of quotients in general. Thank you for your help!","\displaystyle\lim_{(x,y)\to(1,0)} \frac{(x-1)^3\cos{y}}{(x-1)^2+y^2}=0 \delta>|x-1| \Rightarrow \delta>(x-1)^2\Rightarrow \delta^3>|(x-1)^3| \delta>|y|\geq|cos{y}| \Rightarrow \delta^2>y^2 \delta^4>|(x-1)^3\cos{y}| 2\delta^2>|(x-1)^2+y^2| \delta^4>|\frac{(x-1)^3\cos{y}}{(x-1)^2+y^2}| (x-1)^2+y^2\geq1.","['calculus', 'limits', 'multivariable-calculus', 'epsilon-delta']"
49,Upper bound of $\left(1+\dfrac{1}{2}\right)\left(1+\dfrac{1}{4}\right)...\left(1+\dfrac{1}{2^n}\right)$ [duplicate],Upper bound of  [duplicate],\left(1+\dfrac{1}{2}\right)\left(1+\dfrac{1}{4}\right)...\left(1+\dfrac{1}{2^n}\right),"This question already has answers here : Proof that a sequence is convergent (4 answers) Closed 2 years ago . In a calculus book that I'm reading, there is a problem as follows: Prove that this sequence $x_n=\left(1+\dfrac{1}{2}\right)\left(1+\dfrac{1}{4}\right)...\left(1+\dfrac{1}{2^n}\right)$ is monotonic, bounded and then converges. I can prove that this sequence is increasing, but I found no way to find the upper bound for $x_n$ . Please give me some hint to find the upper bound, and may be the limit of this sequence also. Thanks","This question already has answers here : Proof that a sequence is convergent (4 answers) Closed 2 years ago . In a calculus book that I'm reading, there is a problem as follows: Prove that this sequence is monotonic, bounded and then converges. I can prove that this sequence is increasing, but I found no way to find the upper bound for . Please give me some hint to find the upper bound, and may be the limit of this sequence also. Thanks",x_n=\left(1+\dfrac{1}{2}\right)\left(1+\dfrac{1}{4}\right)...\left(1+\dfrac{1}{2^n}\right) x_n,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'upper-lower-bounds']"
50,"Determine real numbers a,b and c such that they verify a certain equation","Determine real numbers a,b and c such that they verify a certain equation",,up to this point I've determined c this way: The issue here is that I cannot figure out how to proceed the same way with the other two variables a and b. Is there something I'm missing from the start? Sorry if it's hard to understand I can elaborate if necessary.,up to this point I've determined c this way: The issue here is that I cannot figure out how to proceed the same way with the other two variables a and b. Is there something I'm missing from the start? Sorry if it's hard to understand I can elaborate if necessary.,,"['limits', 'problem-solving', 'real-numbers', 'partial-fractions']"
51,"$f(x)=\cos x-x$. If $\lim_{n\rightarrow \infty }|f(a_{n})|=0$, then $\lim_{n\rightarrow \infty }a_{n}$ converges?",". If , then  converges?",f(x)=\cos x-x \lim_{n\rightarrow \infty }|f(a_{n})|=0 \lim_{n\rightarrow \infty }a_{n},"Here, we have a very well-known function $f(x)=\cos x-x$ . I know there is only one solution for $f(x)=0$ . Let it be $a\approx 0.739$ . Now, I'm trying to prove that if we have $a_{n+1}=\cos a_{n}$ for all $n$ , then $a_{n}$ converges to $a$ . (Just with MVT) As a result, I got the fact "" $\left | a_{n+2}-a_{n+1} \right |\leq \left | a_{n+1}-a_{n} \right |$ for all $n$ ."" So, I broke it into two cases s.t $\square $ :"" $\left | a_{n+2}-a_{n+1} \right |<  \left | a_{n+1}-a_{n} \right |$ for any $n$ ,"" and $\bigcirc $ :"" $\left | a_{n+2}-a_{n+1} \right |=   \left | a_{n+1}-a_{n} \right |$ for any $n$ ."" In case $\square $ , we can get $\lim_{n\rightarrow \infty }|f(a_{n})|=0$ . But, I'm not certain if it is safe to say that ""since $f(x)=0$ has a unique solution $x=a$ , we have $\lim a_{n}=a$ "" So, here's my general question: For any continuous & differentiable function $f(x)$ that has a unique solution $x=a$ for $f(x)=0$ , if $\lim_{n\rightarrow \infty }|f(a_{n})|=0$ , then is $\lim a_{n}=a$ ?","Here, we have a very well-known function . I know there is only one solution for . Let it be . Now, I'm trying to prove that if we have for all , then converges to . (Just with MVT) As a result, I got the fact "" for all ."" So, I broke it into two cases s.t :"" for any ,"" and :"" for any ."" In case , we can get . But, I'm not certain if it is safe to say that ""since has a unique solution , we have "" So, here's my general question: For any continuous & differentiable function that has a unique solution for , if , then is ?",f(x)=\cos x-x f(x)=0 a\approx 0.739 a_{n+1}=\cos a_{n} n a_{n} a \left | a_{n+2}-a_{n+1} \right |\leq \left | a_{n+1}-a_{n} \right | n \square  \left | a_{n+2}-a_{n+1} \right |<  \left | a_{n+1}-a_{n} \right | n \bigcirc  \left | a_{n+2}-a_{n+1} \right |=   \left | a_{n+1}-a_{n} \right | n \square  \lim_{n\rightarrow \infty }|f(a_{n})|=0 f(x)=0 x=a \lim a_{n}=a f(x) x=a f(x)=0 \lim_{n\rightarrow \infty }|f(a_{n})|=0 \lim a_{n}=a,"['real-analysis', 'limits', 'convergence-divergence', 'recursion']"
52,"Calculating $\lim_{(x,y)\rightarrow (\infty,\infty)}(x^2+y^2)e^{-(x+y)}$",Calculating,"\lim_{(x,y)\rightarrow (\infty,\infty)}(x^2+y^2)e^{-(x+y)}","Please help me calculate the: $$\lim_{(x,y)\rightarrow (\infty,\infty)}(x^2+y^2)e^{-(x+y)}$$ My attempt is: First method: for $t>0$ we have $e^t\geq \frac{t^3}{3}$ now i have: $$(x^2+y^2)e^{-(x+y)}\leq\frac{3(x^2+y^2)}{(x+y)}\rightarrow 0, (x,y)\rightarrow(\infty,\infty)$$ but i dint know how to continue second method:the polar coord $$\lim_{(x,y)\rightarrow (\infty,\infty)}(x^2+y^2)e^{-(x+y)}=\lim_{r\rightarrow \infty}(r^2)e^{-r(cos\theta+sin\theta)}=\lim_{r\rightarrow \infty}\frac{r^2}{e^{r(cos\theta+sin\theta)}}=\frac{1}{cos\theta+\sin\theta}\lim_{r\rightarrow \infty}\frac{2r}{e^r}=\frac{2}{cos\theta+\sin\theta}\lim_{r\rightarrow \infty}\frac{1}{e^r}=\frac{2}{cos\theta+\sin\theta}\cdot0=0$$ i use the L'Hopital rule,but i dint know its correct","Please help me calculate the: My attempt is: First method: for we have now i have: but i dint know how to continue second method:the polar coord i use the L'Hopital rule,but i dint know its correct","\lim_{(x,y)\rightarrow (\infty,\infty)}(x^2+y^2)e^{-(x+y)} t>0 e^t\geq \frac{t^3}{3} (x^2+y^2)e^{-(x+y)}\leq\frac{3(x^2+y^2)}{(x+y)}\rightarrow 0, (x,y)\rightarrow(\infty,\infty) \lim_{(x,y)\rightarrow (\infty,\infty)}(x^2+y^2)e^{-(x+y)}=\lim_{r\rightarrow \infty}(r^2)e^{-r(cos\theta+sin\theta)}=\lim_{r\rightarrow \infty}\frac{r^2}{e^{r(cos\theta+sin\theta)}}=\frac{1}{cos\theta+\sin\theta}\lim_{r\rightarrow \infty}\frac{2r}{e^r}=\frac{2}{cos\theta+\sin\theta}\lim_{r\rightarrow \infty}\frac{1}{e^r}=\frac{2}{cos\theta+\sin\theta}\cdot0=0","['limits', 'analysis']"
53,evaluating limit of $\lim_{x \to 0}\frac{1-\cos(x)}{\sin(x)(e^x-1)}$,evaluating limit of,\lim_{x \to 0}\frac{1-\cos(x)}{\sin(x)(e^x-1)},"This is for evaluating limit of $\lim_{x \to 0}\frac{1-\cos(x)}{\sin(x)(e^x-1)}$ . It's easy with evaluating using L'Hôpital's Rule, but I want to use Taylor series. I can see here $\cos(x)=\sum_{n=0}^{\infty}\frac{(-1)^nx^{2n}}{(2n)!}$ , $\sin(x)=\sum_{n=0}^{\infty}\frac{(-1)^nx^{2n+1}}{(2n+1)!}$ , also $e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!}$ . So I use these to find it. What to do next?","This is for evaluating limit of . It's easy with evaluating using L'Hôpital's Rule, but I want to use Taylor series. I can see here , , also . So I use these to find it. What to do next?",\lim_{x \to 0}\frac{1-\cos(x)}{\sin(x)(e^x-1)} \cos(x)=\sum_{n=0}^{\infty}\frac{(-1)^nx^{2n}}{(2n)!} \sin(x)=\sum_{n=0}^{\infty}\frac{(-1)^nx^{2n+1}}{(2n+1)!} e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!},"['calculus', 'limits', 'taylor-expansion']"
54,How can i solve $\lim _{x\to +\infty } \left(\frac{3+x^{\alpha}}{2+x}\right)^{x}$ without de L'Hopital?,How can i solve  without de L'Hopital?,\lim _{x\to +\infty } \left(\frac{3+x^{\alpha}}{2+x}\right)^{x},"I'm having some trouble with this limit: $$\lim _{x\to +\infty } \left(\frac{3+x^{\alpha}}{2+x}\right)^{x}$$ The parameter $\alpha \in \mathbb{R}$ . I've tried dividing the problem in three different cases: $\alpha = 1, \alpha < 1$ and $\alpha > 1$ . For the first two the calculation looks pretty easy, as I, first, rewrite both numerator and denominator as the well known limit $(\theta/x + 1)^x \to e^\theta$ : $$\frac{\left( 3 + x \right)^x}{\left( 2 + x \right)^x} = \frac{\left( 3/x + 1 \right)^x}{\left( 2/x + 1 \right)^x}\to \frac{e^3}{e^2} = e .$$ Then I see that for $\alpha > 1$ : $$\left(\frac{3}{x^\alpha}+1\right)^x = \left(\frac{3}{x^\alpha} + 1 \right)^{x^\alpha x^{1-\alpha}}\to e^{3x^{1-\alpha}}\to1$$ and $$\left(\frac{2}{x^\alpha}+x^{1-\alpha}\right)^x \to 0.$$ The $\alpha < 1$ case, instead, gives me the indeterminate form $\frac\infty\infty$ , which I'm unable to solve. This exercise is given in the ""known limits"" sections so I guess I can solve it without using de L'Hopital.","I'm having some trouble with this limit: The parameter . I've tried dividing the problem in three different cases: and . For the first two the calculation looks pretty easy, as I, first, rewrite both numerator and denominator as the well known limit : Then I see that for : and The case, instead, gives me the indeterminate form , which I'm unable to solve. This exercise is given in the ""known limits"" sections so I guess I can solve it without using de L'Hopital.","\lim _{x\to +\infty } \left(\frac{3+x^{\alpha}}{2+x}\right)^{x} \alpha \in \mathbb{R} \alpha = 1, \alpha < 1 \alpha > 1 (\theta/x + 1)^x \to e^\theta \frac{\left( 3 + x \right)^x}{\left( 2 + x \right)^x} = \frac{\left( 3/x + 1 \right)^x}{\left( 2/x + 1 \right)^x}\to \frac{e^3}{e^2} = e . \alpha > 1 \left(\frac{3}{x^\alpha}+1\right)^x = \left(\frac{3}{x^\alpha} + 1 \right)^{x^\alpha x^{1-\alpha}}\to e^{3x^{1-\alpha}}\to1 \left(\frac{2}{x^\alpha}+x^{1-\alpha}\right)^x \to 0. \alpha < 1 \frac\infty\infty","['calculus', 'limits', 'limits-without-lhopital']"
55,Show that $\lim \sqrt{1-\frac{1}{n}} = 1$ by definition,Show that  by definition,\lim \sqrt{1-\frac{1}{n}} = 1,Is my attempt correct? Proof . Let $\varepsilon > 0$ . Take $N > \frac{1}{\varepsilon}$ and let $n \geq N$ . Then $\begin{align}\displaystyle\left\lvert \sqrt{1- \frac{1}{n}} -1 \right\rvert  &= \left\lvert \frac{\sqrt{n-1}}{\sqrt{n}} -1 \right\rvert\\\\ &= \left\lvert \frac{\sqrt{n-1} - \sqrt{n}}{\sqrt{n}} \right\rvert\\\\ &= \left\lvert \frac{(\sqrt{n-1} - \sqrt{n})(\sqrt{n-1} + \sqrt{n})}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\ &= \left\lvert \frac{(n-1)-n}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\ &= \left\lvert \frac{-1}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\ &= \frac{1}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})}\\\\ & < \frac{1}{\sqrt{n}(\sqrt{n})}= \frac{1}{n} \leq \frac{1}{N} < \varepsilon\end{align}$,Is my attempt correct? Proof . Let . Take and let . Then,"\varepsilon > 0 N > \frac{1}{\varepsilon} n \geq N \begin{align}\displaystyle\left\lvert \sqrt{1- \frac{1}{n}} -1 \right\rvert 
&= \left\lvert \frac{\sqrt{n-1}}{\sqrt{n}} -1 \right\rvert\\\\
&= \left\lvert \frac{\sqrt{n-1} - \sqrt{n}}{\sqrt{n}} \right\rvert\\\\
&= \left\lvert \frac{(\sqrt{n-1} - \sqrt{n})(\sqrt{n-1} + \sqrt{n})}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\
&= \left\lvert \frac{(n-1)-n}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\
&= \left\lvert \frac{-1}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\
&= \frac{1}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})}\\\\
& < \frac{1}{\sqrt{n}(\sqrt{n})}= \frac{1}{n} \leq \frac{1}{N} < \varepsilon\end{align}","['real-analysis', 'sequences-and-series', 'limits']"
56,"Calculate $\lim_{n\to\infty}\sum_{\ \ \ \ i,j\ge0 \\ i^2+j^2\le n^2} \frac{1}{n^2+i^2+j^2}$",Calculate,"\lim_{n\to\infty}\sum_{\ \ \ \ i,j\ge0 \\ i^2+j^2\le n^2} \frac{1}{n^2+i^2+j^2}","Calculate $$\lim_{n\to\infty}\sum_{\ \ \ \ i,j\ge0 \\ i^2+j^2\le n^2} \frac{1}{n^2+i^2+j^2}$$ It smells like a Riemann sum for a double integral. $$\lim_{n\to\infty}\sum_{\ \ \ \ i,j\ge0 \\ i^2+j^2\le n^2} \frac{1}{n^2+i^2+j^2} = \lim_{n\to\infty}\frac{1}{n^2}\sum_{\ \ \ \ i,j\ge0 \\ i^2+j^2\le n^2} \frac{1}{1+(i/n)^2+(j/n)^2} \overset{?}= \iint\limits_{[0,1]^2} \frac{1}{1+x^2+y^2}\mathrm dx\mathrm dy$$ I know how to calculate the last integral, but I'm not sure if the (?) part is correct. Doesn't $i^2+j^2\le n$ have any special job to do here?","Calculate It smells like a Riemann sum for a double integral. I know how to calculate the last integral, but I'm not sure if the (?) part is correct. Doesn't have any special job to do here?","\lim_{n\to\infty}\sum_{\ \ \ \ i,j\ge0 \\ i^2+j^2\le n^2} \frac{1}{n^2+i^2+j^2} \lim_{n\to\infty}\sum_{\ \ \ \ i,j\ge0 \\ i^2+j^2\le n^2} \frac{1}{n^2+i^2+j^2} = \lim_{n\to\infty}\frac{1}{n^2}\sum_{\ \ \ \ i,j\ge0 \\ i^2+j^2\le n^2} \frac{1}{1+(i/n)^2+(j/n)^2} \overset{?}= \iint\limits_{[0,1]^2} \frac{1}{1+x^2+y^2}\mathrm dx\mathrm dy i^2+j^2\le n","['calculus', 'limits', 'multivariable-calculus', 'summation', 'multiple-integral']"
57,Switching quantifies for the definition of a limit.,Switching quantifies for the definition of a limit.,,"For my analysis homework I am asked to prove or disprove that if you switch the quantifiers of the definition of a limit, then the definitions are equivalent. More specifically we are asked to prove or disprove for $\lim_{x\to c}f(x)=L$ that these definitions are equivalent, $\forall \epsilon>0\; \exists\delta>0\; s.t. \forall x\in A\; \text{if}\; |x-c|<\delta, \text{then}\; |f(x)-L|<\epsilon $ $\exists\delta>0\; \forall \epsilon>0  s.t. \forall x\in A\; \text{if}\; |x-c|<\delta, \text{then}\; |f(x)-L|<\epsilon $ My idea is that they are not equivalent. My reasoning is that in the second definition there is an existence claim for delta followed by for all epsilon. This would imply that epsilon depends on delta which would make every limit exist because there is a delta for every epsilon meaning $|x-c|<\delta$ would always imply $|f(x)-L|<\epsilon$ . I am wondering if this is correct reasoning or if I am missing something. Thank you!","For my analysis homework I am asked to prove or disprove that if you switch the quantifiers of the definition of a limit, then the definitions are equivalent. More specifically we are asked to prove or disprove for that these definitions are equivalent, My idea is that they are not equivalent. My reasoning is that in the second definition there is an existence claim for delta followed by for all epsilon. This would imply that epsilon depends on delta which would make every limit exist because there is a delta for every epsilon meaning would always imply . I am wondering if this is correct reasoning or if I am missing something. Thank you!","\lim_{x\to c}f(x)=L \forall \epsilon>0\; \exists\delta>0\; s.t. \forall x\in A\; \text{if}\; |x-c|<\delta, \text{then}\; |f(x)-L|<\epsilon  \exists\delta>0\; \forall \epsilon>0  s.t. \forall x\in A\; \text{if}\; |x-c|<\delta, \text{then}\; |f(x)-L|<\epsilon  |x-c|<\delta |f(x)-L|<\epsilon","['real-analysis', 'limits', 'solution-verification', 'definition', 'epsilon-delta']"
58,{Proof Check} Let $x_n$ be a convergent sequence. if $x_n \geq 0$ $\forall n \in \mathbb{N}$ then $\lim x_n \geq 0$,{Proof Check} Let  be a convergent sequence. if   then,x_n x_n \geq 0 \forall n \in \mathbb{N} \lim x_n \geq 0,"Please check and point out if there are logical flaws in my argument. I'm learning analysis but I'm not that confident yet. Proof: Suppose $x_n \geq 0 $ $\forall n \in \mathbb{N}$ and let $\lim x_n = x < 0$ . Then $|x| > 0$ . Take $\varepsilon = |x|$ then $\exists N \in \mathbb{N}$ s.t. $\forall n \geq N$ , $$|x_n - x| \leq |x_n| + |x| < \varepsilon = |x|$$ $$0 \leq |x_n| = x_n < |x| - |x| = 0$$ This is a contradiction, thus $x \geq 0$ .","Please check and point out if there are logical flaws in my argument. I'm learning analysis but I'm not that confident yet. Proof: Suppose and let . Then . Take then s.t. , This is a contradiction, thus .",x_n \geq 0  \forall n \in \mathbb{N} \lim x_n = x < 0 |x| > 0 \varepsilon = |x| \exists N \in \mathbb{N} \forall n \geq N |x_n - x| \leq |x_n| + |x| < \varepsilon = |x| 0 \leq |x_n| = x_n < |x| - |x| = 0 x \geq 0,['real-analysis']
59,Proof regarding convergence,Proof regarding convergence,,"I am supposed to show that the sequence converges and to find the limit. We've just started going over sequences and induction so I'm very limited in what I know/can use and I was wondering if the general pattern of the proof is okay and if it's correct.. Also it took me quiet a bit of time to prove it this way and I was wondering if there are any shortcuts I can do in the future? $$ \{a_n\}=\left\{  \begin{array}{lcl} a_1 = 3\\ a_{n+1} = 1+\sqrt{6+a_n}\\ \end{array} \right.$$ First we show by induction $ \{a_n\} $ is increasing, that is, to prove that the following hold for every $n∈N$ $1+\sqrt{6+a_n}\ge a_n$ $\sqrt{6+a_n}\ge a_n-1$ $6+a_{n}\ge\left(a_{n}+1\right)^{2}$ $6+a_{n}\ge a_{n}^{2}-2a_{n}+1$ $0\ge a_{n}^{2}-3a_{n}-5$ Base case: $n=1$ $1+\sqrt{6+3}\ge 3$ $4\ge 3$ Inductive step: Assume $0\ge a_{n}^{2}-3a_{n}-5$ , we show $0\ge a_{n+1}^{2}-3a_{n+1}-5$ $0\ge a_{n}^{2}-3a_{n}-5$ $6+a_{n}\ge a_{n}^{2}-2a_{n}+1$ $\sqrt{6+a_{n}}\ge a_{n}-1$ $0\ge a_{n}-1-\sqrt{6+a_{n}}$ $0\ge1+2\sqrt{6+a_{n}}+6+a_{n}-3-3\sqrt{6+a_{n}}-5$ $0\ge\left(1+\sqrt{6+a_{n}}\right)^{2}-3\left(1+\sqrt{6+a_{n}}\right)-5$ $0\ge a_{n+1}^{2}-3a_{n+1}-5$ Therefore by induction, $\{a_n\}$ is increasing for every $n∈N$ . Now we show by induction $\{a_n\}$ is bounded, that is, $a_n ≤ 4.2$ for every $n∈N$ . Base case: $n=1$ , which is trivially true. Inductive step: Assume $a_n ≤ 4.2$ , we show $a_{n+1} ≤ 4.2$ . $a_n ≤ 4.2$ $6+a_{n}\le10.2$ $\sqrt{6+a_{n}}\le\sqrt{10.2}$ $\sqrt{6+a_{n}}\le3.2$ $1+\sqrt{6+a_{n}}\le4.2$ Therefore by induction $\{a_n\}$ is bounded from above. As $\{a_n\}$ is both bounded and incresing for every $n∈N$ , $\{a_n\}$ converges and $\lim _{n\to \infty }\left(a_n\right)=L$ for some $L∈R$ As $\lim _{n\to \infty }\left(a_n\right)=L$ , then $\lim _{n\to \infty }\left(a_{n+1}\right)=L$ , therefore $L\ =\ 1+\sqrt{6+L}$ $L\ -1=\ \sqrt{6+L}$ $\left(L\ -1\right)^{2}=\ 6+L$ $L^{2}-2L+1=\ 6+L$ $L^{2}-3L-5=\ 0$ $L = \dfrac{ 3 \pm \sqrt{(-3)^2 - 4*(-5)}}{ 2 }$ $L_1 = 4.19258$ $L_2 = −1.19258$ As $a_n ≥ 0$ for every $n∈N$ , $L = 4.19258$","I am supposed to show that the sequence converges and to find the limit. We've just started going over sequences and induction so I'm very limited in what I know/can use and I was wondering if the general pattern of the proof is okay and if it's correct.. Also it took me quiet a bit of time to prove it this way and I was wondering if there are any shortcuts I can do in the future? First we show by induction is increasing, that is, to prove that the following hold for every Base case: Inductive step: Assume , we show Therefore by induction, is increasing for every . Now we show by induction is bounded, that is, for every . Base case: , which is trivially true. Inductive step: Assume , we show . Therefore by induction is bounded from above. As is both bounded and incresing for every , converges and for some As , then , therefore As for every ,"," \{a_n\}=\left\{ 
\begin{array}{lcl}
a_1 = 3\\
a_{n+1} = 1+\sqrt{6+a_n}\\
\end{array}
\right.  \{a_n\}  n∈N 1+\sqrt{6+a_n}\ge a_n \sqrt{6+a_n}\ge a_n-1 6+a_{n}\ge\left(a_{n}+1\right)^{2} 6+a_{n}\ge a_{n}^{2}-2a_{n}+1 0\ge a_{n}^{2}-3a_{n}-5 n=1 1+\sqrt{6+3}\ge 3 4\ge 3 0\ge a_{n}^{2}-3a_{n}-5 0\ge a_{n+1}^{2}-3a_{n+1}-5 0\ge a_{n}^{2}-3a_{n}-5 6+a_{n}\ge a_{n}^{2}-2a_{n}+1 \sqrt{6+a_{n}}\ge a_{n}-1 0\ge a_{n}-1-\sqrt{6+a_{n}} 0\ge1+2\sqrt{6+a_{n}}+6+a_{n}-3-3\sqrt{6+a_{n}}-5 0\ge\left(1+\sqrt{6+a_{n}}\right)^{2}-3\left(1+\sqrt{6+a_{n}}\right)-5 0\ge a_{n+1}^{2}-3a_{n+1}-5 \{a_n\} n∈N \{a_n\} a_n ≤ 4.2 n∈N n=1 a_n ≤ 4.2 a_{n+1} ≤ 4.2 a_n ≤ 4.2 6+a_{n}\le10.2 \sqrt{6+a_{n}}\le\sqrt{10.2} \sqrt{6+a_{n}}\le3.2 1+\sqrt{6+a_{n}}\le4.2 \{a_n\} \{a_n\} n∈N \{a_n\} \lim _{n\to \infty }\left(a_n\right)=L L∈R \lim _{n\to \infty }\left(a_n\right)=L \lim _{n\to \infty }\left(a_{n+1}\right)=L L\ =\ 1+\sqrt{6+L} L\ -1=\ \sqrt{6+L} \left(L\ -1\right)^{2}=\ 6+L L^{2}-2L+1=\ 6+L L^{2}-3L-5=\ 0 L = \dfrac{ 3 \pm \sqrt{(-3)^2 - 4*(-5)}}{ 2 } L_1 = 4.19258 L_2 = −1.19258 a_n ≥ 0 n∈N L = 4.19258","['calculus', 'sequences-and-series', 'limits', 'proof-writing', 'solution-verification']"
60,Thomae's function,Thomae's function,,"I am self-learning Real Analysis from Stephen Abbott's Understanding Analysis. I'd like to ask, if my below proof is technically correct and rigorous. Problem 4.2.3. Consider the Thomae's function $t(x)$ defined as follows: \begin{align*} t(x) = \begin{cases} 1 & \text{ if } x = 0\\ \frac{1}{n} & \text { if } x = \frac{m}{n} \in \mathbf{Q}\setminus \{0\}\\ 0 & \text { if } x \notin \mathbf{Q} \end{cases} \end{align*} (a) Construct three different sequences $(x_n)$ , $(y_n)$ and $(z_n)$ each of which converges to $1$ without using the number $1$ as a term in the sequence. (b) Now, compute $\lim t(x_n)$ , $\lim t(y_n)$ and $\lim t(z_n)$ . (c) Make an educated conjecture for $\lim_{x \to 1} t(x)$ and use the definition of functional limits to verify the claim. Given $\epsilon > 0$ , consider the set of points $\{x \in \mathbf{R}, t(x)\ge \epsilon\}$ . Argue that all the points in this set are isolated. Proof. Consider $x_n:= \frac{n}{n+1}$ . The first few terms of the sequence are, \begin{align*} (x_n) = \frac{1}{2}, \frac{2}{3}, \frac{3}{4},\frac{4}{5},\ldots \end{align*} Consider $y_n:= 1$ . The first few terms of the sequence are, \begin{align*} (y_n) = 1,1,1,1,1, \ldots \end{align*} Consider $(z_n):= \frac{n+\sqrt{2}}{n}$ . The first few terms of the sequence are, \begin{align*} (z_n) = \frac{1+\sqrt{2}}{1}, \frac{2+\sqrt{2}}{2}, \frac{3+\sqrt{2}}{3}, \frac{4+\sqrt{2}}{4}, \ldots \end{align*} (b) The corresponding image sequences are: \begin{align*} t(x_n) &= \frac{1}{2}, \frac{1}{3},\frac{1}{4},\frac{1}{5},\ldots\\ t(y_n) &= 1,1,1,1,\ldots\\ t(z_n) &= 0,0,0,0,\ldots \end{align*} Thus, $\lim_{x_n \to 1}t(x_n) = \lim_{z_n \to 1}t(z_n) \ne 1 = t(1)$ (c) We conjecture that $\lim_{x \to 1} t(x)$ does not exist. Clearly, from the sequential characterization of functional limits, for all sequences $(x_n)$ in the domain of $t$ , whenever $(x_n) \to 1$ and $x_n \ne 1$ , we have $t(x_n) \to L$ , then $\lim_{x_n \to 1}t(x_n) = L$ . Clearly, this condition is not satisfied. $\lim_{x \to 1}t(x)$ is not well-defined.","I am self-learning Real Analysis from Stephen Abbott's Understanding Analysis. I'd like to ask, if my below proof is technically correct and rigorous. Problem 4.2.3. Consider the Thomae's function defined as follows: (a) Construct three different sequences , and each of which converges to without using the number as a term in the sequence. (b) Now, compute , and . (c) Make an educated conjecture for and use the definition of functional limits to verify the claim. Given , consider the set of points . Argue that all the points in this set are isolated. Proof. Consider . The first few terms of the sequence are, Consider . The first few terms of the sequence are, Consider . The first few terms of the sequence are, (b) The corresponding image sequences are: Thus, (c) We conjecture that does not exist. Clearly, from the sequential characterization of functional limits, for all sequences in the domain of , whenever and , we have , then . Clearly, this condition is not satisfied. is not well-defined.","t(x) \begin{align*}
t(x) = \begin{cases}
1 & \text{ if } x = 0\\
\frac{1}{n} & \text { if } x = \frac{m}{n} \in \mathbf{Q}\setminus \{0\}\\
0 & \text { if } x \notin \mathbf{Q}
\end{cases}
\end{align*} (x_n) (y_n) (z_n) 1 1 \lim t(x_n) \lim t(y_n) \lim t(z_n) \lim_{x \to 1} t(x) \epsilon > 0 \{x \in \mathbf{R}, t(x)\ge \epsilon\} x_n:= \frac{n}{n+1} \begin{align*}
(x_n) = \frac{1}{2}, \frac{2}{3}, \frac{3}{4},\frac{4}{5},\ldots
\end{align*} y_n:= 1 \begin{align*}
(y_n) = 1,1,1,1,1, \ldots
\end{align*} (z_n):= \frac{n+\sqrt{2}}{n} \begin{align*}
(z_n) = \frac{1+\sqrt{2}}{1}, \frac{2+\sqrt{2}}{2}, \frac{3+\sqrt{2}}{3}, \frac{4+\sqrt{2}}{4}, \ldots
\end{align*} \begin{align*}
t(x_n) &= \frac{1}{2}, \frac{1}{3},\frac{1}{4},\frac{1}{5},\ldots\\
t(y_n) &= 1,1,1,1,\ldots\\
t(z_n) &= 0,0,0,0,\ldots
\end{align*} \lim_{x_n \to 1}t(x_n) = \lim_{z_n \to 1}t(z_n) \ne 1 = t(1) \lim_{x \to 1} t(x) (x_n) t (x_n) \to 1 x_n \ne 1 t(x_n) \to L \lim_{x_n \to 1}t(x_n) = L \lim_{x \to 1}t(x)","['real-analysis', 'limits', 'solution-verification']"
61,Show that the following is only continuous at x=0,Show that the following is only continuous at x=0,,What is this question asking? How does rational and irrational affect it? What are the absolute values used for in this solution? Why is the series irrational when $c\ne0$ for any nonzero rational number but then the series is made to be all irrational values? What is this question?,What is this question asking? How does rational and irrational affect it? What are the absolute values used for in this solution? Why is the series irrational when for any nonzero rational number but then the series is made to be all irrational values? What is this question?,c\ne0,"['calculus', 'limits', 'continuity']"
62,Find the asymptotic analysis of $\frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}- \sqrt{5}$,Find the asymptotic analysis of,\frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}- \sqrt{5},"Given a recursion $a_{n+ 1}= a_{n}^{2}- 2$ with $a_{0}= 3.$ Find the asymptotic analysis of $$\frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}- \sqrt{5}$$ Maybe this helps - https://artofproblemsolving.com/community/c4h2308107p18323223 - And this is my way of thinking, I use two inequalities $$\sqrt{5}\leq \frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}\leq\sqrt{5}+ \frac{2}{3}\cdot\left ( \varphi- 1 \right)^{-2^{n+ 1}}$$ with $\varphi= \frac{3+ \sqrt{5}}{2}.$ From the recurrence sequence $a_{n+ 1}= a_{n}^{2}- 2\Rightarrow a_{n+ 1}= \varphi^{2^{n}}+ \varphi^{-2^{n}},$ we have $$\frac{a_{n+ 1}^{2}}{\prod_{i= 0}^{n+ 1}a_{i}}= \frac{\varphi^{2^{n+ 1}}+ \varphi^{-2^{n+ 1}}}{\prod_{i= 1}^{n}\left ( \varphi^{2^{i}}+ \frac{1}{\varphi^{2^{i}}} \right )}= (\varphi- \frac{1}{\varphi})\left ( 1+ \frac{2\varphi^{-2^{n+ 2}}}{1- \varphi^{-2^{n+ 2}}} \right )> \varphi- \frac{1}{\varphi}= \sqrt{5}$$ Because I'm busy now.. OK, I'll get it done by tonight, I haven't posted the solution for the rightside inequality, and how can we use it to find the $o\left ( 1 \right ).$ I need to the help, thanks a real lot ! Edit . The rightside inequality is just single variable of $f\left ( n \right )\leq g\left ( n \right )$ with $$f\left ( n \right )= \sqrt{5}+ \frac{2}{3}\cdot\varphi^{-2^{n+ 1}}, g\left ( n \right )= \sqrt{5}\left ( 1+ \frac{2\varphi^{-2^{n+ 2}}}{1- \varphi^{-2^{n+ 2}}} \right ), 0< \varphi^{-2^{n+ 1}}\leq\frac{7-3\sqrt{5}}{2}$$","Given a recursion with Find the asymptotic analysis of Maybe this helps - https://artofproblemsolving.com/community/c4h2308107p18323223 - And this is my way of thinking, I use two inequalities with From the recurrence sequence we have Because I'm busy now.. OK, I'll get it done by tonight, I haven't posted the solution for the rightside inequality, and how can we use it to find the I need to the help, thanks a real lot ! Edit . The rightside inequality is just single variable of with","a_{n+ 1}= a_{n}^{2}- 2 a_{0}= 3. \frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}- \sqrt{5} \sqrt{5}\leq \frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}\leq\sqrt{5}+ \frac{2}{3}\cdot\left ( \varphi- 1 \right)^{-2^{n+ 1}} \varphi= \frac{3+ \sqrt{5}}{2}. a_{n+ 1}= a_{n}^{2}- 2\Rightarrow a_{n+ 1}= \varphi^{2^{n}}+ \varphi^{-2^{n}}, \frac{a_{n+ 1}^{2}}{\prod_{i= 0}^{n+ 1}a_{i}}= \frac{\varphi^{2^{n+ 1}}+ \varphi^{-2^{n+ 1}}}{\prod_{i= 1}^{n}\left ( \varphi^{2^{i}}+ \frac{1}{\varphi^{2^{i}}} \right )}= (\varphi- \frac{1}{\varphi})\left ( 1+ \frac{2\varphi^{-2^{n+ 2}}}{1- \varphi^{-2^{n+ 2}}} \right )> \varphi- \frac{1}{\varphi}= \sqrt{5} o\left ( 1 \right ). f\left ( n \right )\leq g\left ( n \right ) f\left ( n \right )= \sqrt{5}+ \frac{2}{3}\cdot\varphi^{-2^{n+ 1}}, g\left ( n \right )= \sqrt{5}\left ( 1+ \frac{2\varphi^{-2^{n+ 2}}}{1- \varphi^{-2^{n+ 2}}} \right ), 0< \varphi^{-2^{n+ 1}}\leq\frac{7-3\sqrt{5}}{2}","['limits', 'inequality']"
63,Infinite Limit Involving a Factorial,Infinite Limit Involving a Factorial,,"$$\lim_{x\to\infty} \left(\frac{x+1}{(x!)^{\frac{1}{x-1}}}\right) $$ I'm pretty sure that this approaches infinity, yet I'm having trouble proving it. Hope someone can help, thanks.","I'm pretty sure that this approaches infinity, yet I'm having trouble proving it. Hope someone can help, thanks.",\lim_{x\to\infty} \left(\frac{x+1}{(x!)^{\frac{1}{x-1}}}\right) ,"['limits', 'factorial']"
64,Ultralimits vs limits of subsequences,Ultralimits vs limits of subsequences,,"Let $(x_n)_{n \geq 1} \subset \mathbb{R}$ be a sequence of real numbers, and let $\omega \subset \mathcal{P}(\mathbb{N})$ be a non-principal ultrafilter. We say that $x$ is an ultralimit of $(x_n)_{n \geq 1}$ along $\omega$ if for all $\varepsilon > 0$ we have $\{ n \geq 1 : |x - x_n| < \varepsilon \} \in \omega$ . Suppose that there exists a set $\{ n_k : k \geq 1 \} \in \omega$ such that the subsequence $(x_{n_k})_{k \geq 1}$ has converges to $x$ (in the standard sense). Then it follows that $x$ is the ultralimit $(x_n)_{n \geq 1}$ along $\omega$ , since $\omega$ is non-principal and so taking away finitely many elements from $(n_k)_{k \geq 1}$ we are still in $\omega$ . Conversely, if $x$ is the ultralimit of $(x_n)_{n \geq 1}$ along $\omega$ , we can find a subsequence $(x_{n_k})_{k \geq 1}$ converging to $x$ , for instance we can choose inductively $n_k > n_{k-1}$ such that $n_k \in \{ n \geq 1 : |x_n - x| < 1/k \}$ . Although this subsequence is constructed using sets in $\omega$ , it is not clear to me that the resulting set $\{ n_k : k \geq 1 \}$ should be in $\omega$ . So my question is: if $x$ is the ultralimit of $(x_n)_{n \geq 1}$ along $\omega$ , can we always choose a subsequence $(x_{n_k})_{k \geq 1}$ converging to $x$ so that $\{ n_k : k \geq 1 \} \in \omega$ ?","Let be a sequence of real numbers, and let be a non-principal ultrafilter. We say that is an ultralimit of along if for all we have . Suppose that there exists a set such that the subsequence has converges to (in the standard sense). Then it follows that is the ultralimit along , since is non-principal and so taking away finitely many elements from we are still in . Conversely, if is the ultralimit of along , we can find a subsequence converging to , for instance we can choose inductively such that . Although this subsequence is constructed using sets in , it is not clear to me that the resulting set should be in . So my question is: if is the ultralimit of along , can we always choose a subsequence converging to so that ?",(x_n)_{n \geq 1} \subset \mathbb{R} \omega \subset \mathcal{P}(\mathbb{N}) x (x_n)_{n \geq 1} \omega \varepsilon > 0 \{ n \geq 1 : |x - x_n| < \varepsilon \} \in \omega \{ n_k : k \geq 1 \} \in \omega (x_{n_k})_{k \geq 1} x x (x_n)_{n \geq 1} \omega \omega (n_k)_{k \geq 1} \omega x (x_n)_{n \geq 1} \omega (x_{n_k})_{k \geq 1} x n_k > n_{k-1} n_k \in \{ n \geq 1 : |x_n - x| < 1/k \} \omega \{ n_k : k \geq 1 \} \omega x (x_n)_{n \geq 1} \omega (x_{n_k})_{k \geq 1} x \{ n_k : k \geq 1 \} \in \omega,"['sequences-and-series', 'functional-analysis', 'limits', 'filters']"
65,How to find the limit of $\frac{\frac{1 - 2n}{n}}{5 + 3^{-n}}$,How to find the limit of,\frac{\frac{1 - 2n}{n}}{5 + 3^{-n}},"I have an intuition that the limit is $\frac{-2}{5}$ , by noting that $\frac{1}{n} \to 0$ and $3^{-n} \to 0$ as $n \to \infty$ . But I do not know how to show this formally, i.e. we can always find a starting index $n$ such that $\left| \frac{2}{5} - \frac{\frac{1 - 2n}{n}}{5 + 3^{-n}} \right| < \epsilon$ for a given epsilon. By manipulating the absolute value I managed to get it to form $\left| \frac{-2n3^{-n} - 5}{5n3^{-n} + 25n}\right|$ . I suppose the squeeze theorem is one way to go, but as of now I do not know what my two other sequences should be.","I have an intuition that the limit is , by noting that and as . But I do not know how to show this formally, i.e. we can always find a starting index such that for a given epsilon. By manipulating the absolute value I managed to get it to form . I suppose the squeeze theorem is one way to go, but as of now I do not know what my two other sequences should be.",\frac{-2}{5} \frac{1}{n} \to 0 3^{-n} \to 0 n \to \infty n \left| \frac{2}{5} - \frac{\frac{1 - 2n}{n}}{5 + 3^{-n}} \right| < \epsilon \left| \frac{-2n3^{-n} - 5}{5n3^{-n} + 25n}\right|,"['real-analysis', 'sequences-and-series', 'limits']"
66,"Prove that $f$ is differentiable on $[0,∞)$",Prove that  is differentiable on,"f [0,∞)","Assuming that $e^x$ is differentiable on $\mathbb{R}$ , prove that $$f(x)=\begin{cases}\dfrac{x}{1+e^{1/x}},&x \neq 0\\ 0, &x=0\end{cases}$$ is differentiable on $[0,∞)$ . Is $f$ differentiable at $0$ ? Here is my textbook problem. when I asked some help from our madam she said that this is not differentiable on $[0,∞)$ it should be $(0,\infty)$ and she said $[0,∞)$ would be typo But I think she is wrong but I don't know exactly whether I am correct or not I know $|x|$ is not differentiable at $0$ but it is differentiable on $[0,\infty)$ or $(-\infty,0]$ Am I wrong?","Assuming that is differentiable on , prove that is differentiable on . Is differentiable at ? Here is my textbook problem. when I asked some help from our madam she said that this is not differentiable on it should be and she said would be typo But I think she is wrong but I don't know exactly whether I am correct or not I know is not differentiable at but it is differentiable on or Am I wrong?","e^x \mathbb{R} f(x)=\begin{cases}\dfrac{x}{1+e^{1/x}},&x \neq 0\\ 0, &x=0\end{cases} [0,∞) f 0 [0,∞) (0,\infty) [0,∞) |x| 0 [0,\infty) (-\infty,0]","['real-analysis', 'limits', 'derivatives']"
67,Limit points of the set $\{\frac {\varphi(n) }n : n\in \mathbb{N}\}$,Limit points of the set,\{\frac {\varphi(n) }n : n\in \mathbb{N}\},"Let $\varphi(n)$ denotes the cardinality of the set $\{a | 1\le a\le n, (a,n)=1\}$ where $(a , n)$ denotes the gcd of $a$ and $n$ . Which of the following is NOT true ? $(a)$ There exist infinitely many $n$ such that $\varphi (n)\gt \varphi(n+1)$ $(b)$ There exist infinitely many $n$ such that $\varphi (n)\lt \varphi(n+1)$ $(c)$ There exist $N\in \mathbb{N}$ such that $N\gt 2$ and for all $n\gt N$ , $\varphi (N)\lt \varphi(n)$ $(d)$ The set $\{ \frac {\varphi (n)}{n} : n\in \mathbb{N} \} $ has finitely many limit points. My Thinking: $(a)$ is true  by selecting $n=p$ ( an odd prime greater than $3$ ) Justification: Since $p$ is odd, $p+1$ is even. Now $\varphi(p)=p-1$ and $\varphi(p+1)\le p+1-\frac{p+1}{2}=\frac{p+1}2$ So $\varphi(p+1)\le \frac{p+1}2 \lt p-1=\varphi(p)$ By the infinitudes of primes, the result follows. $(b)$ Is true by taking $n=p-1$ where $p$ is an odd prime greater than $3$ and following the same line of reasoning. $(c)$ This statement (if true ) means that the set $\{\varphi(n) : n\gt N\}$ is bounded below. I think it is true My argument : Let $N=6$ . Then $\varphi(6)=2$ If $n\gt 6$ and is prime  , then clearly $\varphi(n)\gt 2$ Otherwise, if $n$ is composite, then let $n=p_1^{r_1} p_2^{r_2}...p_k^{r_k}$ . $\varphi(n)=p_1^{r_1-1}p_2^{r_2-1}...p_k^{r_k-1}(p_1-1)(p_2-1)..(p_k-1)$ If $\phi(n)=2$ , then the distinct primes can be just $2$ and $3$ . So the above equality reduces to $2=\phi(n)=2^{r_1}3^{r_2-1}$ when both the factors $2$ and $3$ are present . This gives $r_1=r_2=1$ and so $n=6$ When only $3$ present ,  then we get $n=3$ and if the factor is only $2$ , then $n=4$ . So the result follows as $N=6$ works $(d)$ This is NOT true. Justification :- Let $\{p_i\}$ be the enumeration of primes. Let an index $k$ be fixed and $i$ be the running index. Then if the only prime factors of $n_i$ are $p_k, p_i$ , then $\frac{\varphi(n_i)}{n_i}=\big( 1-1/p_k\big)\big(1-1/p_i\big)$ So ,letting $i\to \infty$ , we have $\lim_{i\to \infty}\frac{\varphi(n_i)}{n_i}=1-1/{p_k}$ Since $p_k$ is arbitrary, there are infinitely many limit points (maybe uncountably many) limits points of the given set. Please check if my work is correct or the answers can be shortened in any way. Thanks for your time.","Let denotes the cardinality of the set where denotes the gcd of and . Which of the following is NOT true ? There exist infinitely many such that There exist infinitely many such that There exist such that and for all , The set has finitely many limit points. My Thinking: is true  by selecting ( an odd prime greater than ) Justification: Since is odd, is even. Now and So By the infinitudes of primes, the result follows. Is true by taking where is an odd prime greater than and following the same line of reasoning. This statement (if true ) means that the set is bounded below. I think it is true My argument : Let . Then If and is prime  , then clearly Otherwise, if is composite, then let . If , then the distinct primes can be just and . So the above equality reduces to when both the factors and are present . This gives and so When only present ,  then we get and if the factor is only , then . So the result follows as works This is NOT true. Justification :- Let be the enumeration of primes. Let an index be fixed and be the running index. Then if the only prime factors of are , then So ,letting , we have Since is arbitrary, there are infinitely many limit points (maybe uncountably many) limits points of the given set. Please check if my work is correct or the answers can be shortened in any way. Thanks for your time.","\varphi(n) \{a | 1\le a\le n, (a,n)=1\} (a , n) a n (a) n \varphi (n)\gt \varphi(n+1) (b) n \varphi (n)\lt \varphi(n+1) (c) N\in \mathbb{N} N\gt 2 n\gt N \varphi (N)\lt \varphi(n) (d) \{ \frac {\varphi (n)}{n} : n\in \mathbb{N} \}  (a) n=p 3 p p+1 \varphi(p)=p-1 \varphi(p+1)\le p+1-\frac{p+1}{2}=\frac{p+1}2 \varphi(p+1)\le \frac{p+1}2 \lt p-1=\varphi(p) (b) n=p-1 p 3 (c) \{\varphi(n) : n\gt N\} N=6 \varphi(6)=2 n\gt 6 \varphi(n)\gt 2 n n=p_1^{r_1} p_2^{r_2}...p_k^{r_k} \varphi(n)=p_1^{r_1-1}p_2^{r_2-1}...p_k^{r_k-1}(p_1-1)(p_2-1)..(p_k-1) \phi(n)=2 2 3 2=\phi(n)=2^{r_1}3^{r_2-1} 2 3 r_1=r_2=1 n=6 3 n=3 2 n=4 N=6 (d) \{p_i\} k i n_i p_k, p_i \frac{\varphi(n_i)}{n_i}=\big( 1-1/p_k\big)\big(1-1/p_i\big) i\to \infty \lim_{i\to \infty}\frac{\varphi(n_i)}{n_i}=1-1/{p_k} p_k","['limits', 'elementary-number-theory', 'solution-verification']"
68,How to find the limit of $f(x)$.,How to find the limit of .,f(x),"Let $$f(x) = {\frac{e^{e^x - 1} - \frac{1}{1 - x}}{\ln(\frac{1 + x}{1-x}) - 2\sin x}}$$ I need to find a $ \lim_{x \to 0}f(x)$ . As I understand, we have to do some Taylor series decomposition and problem should be pretty easy. But I have had some troubles with it. P.S : I can decompose $\ln(\frac{1 + x}{1-x})$ as $\ln(1 + \frac{2x}{1-x})$ to use some standard Taylor formulas. I know $\sin x$ Taylor series, but I've got some troubles with $e^{e^x-1}$ and some further transformations.","Let I need to find a . As I understand, we have to do some Taylor series decomposition and problem should be pretty easy. But I have had some troubles with it. P.S : I can decompose as to use some standard Taylor formulas. I know Taylor series, but I've got some troubles with and some further transformations.",f(x) = {\frac{e^{e^x - 1} - \frac{1}{1 - x}}{\ln(\frac{1 + x}{1-x}) - 2\sin x}}  \lim_{x \to 0}f(x) \ln(\frac{1 + x}{1-x}) \ln(1 + \frac{2x}{1-x}) \sin x e^{e^x-1},"['real-analysis', 'limits']"
69,"Show that $\lim_n \int_A f(nx)dx =\lambda(A) \frac1T \int_0^T f(t) dt$ for a measurable, bounded, T-periodic function.","Show that  for a measurable, bounded, T-periodic function.",\lim_n \int_A f(nx)dx =\lambda(A) \frac1T \int_0^T f(t) dt,"I found this exercice : Let $f : \mathbb{R} \to \mathbb{R}$ a measurable bounded and T-periodic function. Show that for every bounded borel set A, $$\lim_n \int_A f(nx)dx =\lambda(A) \frac1T \int_0^T f(t) dt$$ I can prove it when A is an bounded interval of $\mathbb{R}$ . But, I can't prove the general result. My first idea was to use that the borel set is generated by open intervals. I proved that the null set satifies the property, if $A$ and $B$ satifies the property then $A\setminus B$ also, if $(A_n)$ satifies the property and $A_n\subset A_{n+1}$ then $\cup A_n$ also. But, I wasn't able to prove that is $A_1, ..., A_N$ satisfies the property then $\cap_{i=1}^N A_n$ also. My second idea was to use the definition of the Lebesgue measure as the infimum of lenght of covering by open intervals. But I obtained only one inequality.","I found this exercice : Let a measurable bounded and T-periodic function. Show that for every bounded borel set A, I can prove it when A is an bounded interval of . But, I can't prove the general result. My first idea was to use that the borel set is generated by open intervals. I proved that the null set satifies the property, if and satifies the property then also, if satifies the property and then also. But, I wasn't able to prove that is satisfies the property then also. My second idea was to use the definition of the Lebesgue measure as the infimum of lenght of covering by open intervals. But I obtained only one inequality.","f : \mathbb{R} \to \mathbb{R} \lim_n \int_A f(nx)dx =\lambda(A) \frac1T \int_0^T f(t) dt \mathbb{R} A B A\setminus B (A_n) A_n\subset A_{n+1} \cup A_n A_1, ..., A_N \cap_{i=1}^N A_n","['limits', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure', 'periodic-functions']"
70,Evaluate $\lim_ {n \to\infty} n (n^ {\frac {1} {n}}-2^ {\frac {1} {n}}) ^a$ as $a$ varies in the reals,Evaluate  as  varies in the reals,\lim_ {n \to\infty} n (n^ {\frac {1} {n}}-2^ {\frac {1} {n}}) ^a a,How would you evaluate $\lim_ {n \to\infty} n (n^ {\frac {1} {n}}-2^ {\frac {1} {n}}) ^a$ with $n$ an integer and $a$ a real parameter? I tried to apply the most common criteria and to compare this with some other easier functions but without success. Thank you,How would you evaluate with an integer and a real parameter? I tried to apply the most common criteria and to compare this with some other easier functions but without success. Thank you,\lim_ {n \to\infty} n (n^ {\frac {1} {n}}-2^ {\frac {1} {n}}) ^a n a,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'real-numbers']"
71,Find the sum of the series $\displaystyle \sum_{n \ge 0} \frac{2^{3n}}{5^{n-1}}$ and specify if it is convergent or divergent.,Find the sum of the series  and specify if it is convergent or divergent.,\displaystyle \sum_{n \ge 0} \frac{2^{3n}}{5^{n-1}},"I have to find the sum of the following series and specify if it is convergent or divergent: $$\sum_{n \ge 0} \frac{2^{3n}}{5^{n-1}}$$ This is what I have so far: $$\sum_{n \ge 0} \frac{2^{3n}}{5^{n-1}} = \sum_{n \ge 0} \frac{5 \cdot 2^{3n}}{5^{n}} = \sum_{n \ge 0} \frac{5 \cdot (2^{3})^{n}}{5^{n}} = 5 \cdot \sum_{n \ge 0} \bigg (\frac{8}{5} \bigg )^n$$ From here I said that: $$\frac{8}{5} > 1 \Rightarrow \bigg (\frac{8}{5} \bigg) ^ {n} \rightarrow \infty \text{ as } n \rightarrow \infty$$ And because of this I concluded that the sum of the series is $\infty$ , thus the series is divergent. However, I am not very satisfied with this last part. It doesn't feel right to jump from $$\bigg (\frac{8}{5} \bigg) ^ {n} \rightarrow \infty \text{ as } n \rightarrow \infty$$ to $$\sum_{n = 0}^{\infty} \frac{2^{3n}}{5^{n-1}} = \infty$$ What extra steps am I missing?","I have to find the sum of the following series and specify if it is convergent or divergent: This is what I have so far: From here I said that: And because of this I concluded that the sum of the series is , thus the series is divergent. However, I am not very satisfied with this last part. It doesn't feel right to jump from to What extra steps am I missing?","\sum_{n \ge 0} \frac{2^{3n}}{5^{n-1}} \sum_{n \ge 0} \frac{2^{3n}}{5^{n-1}}
= \sum_{n \ge 0} \frac{5 \cdot 2^{3n}}{5^{n}}
= \sum_{n \ge 0} \frac{5 \cdot (2^{3})^{n}}{5^{n}}
= 5 \cdot \sum_{n \ge 0} \bigg (\frac{8}{5} \bigg )^n \frac{8}{5} > 1 \Rightarrow \bigg (\frac{8}{5} \bigg) ^ {n} \rightarrow \infty \text{ as } n \rightarrow \infty \infty \bigg (\frac{8}{5} \bigg) ^ {n} \rightarrow \infty \text{ as } n \rightarrow \infty \sum_{n = 0}^{\infty} \frac{2^{3n}}{5^{n-1}} = \infty","['real-analysis', 'sequences-and-series']"
72,Find the Limit.?,Find the Limit.?,,"What is the limit of $Y_{n} := \frac{1}{3}Y_{n-1} + \frac{2}{3}Y_{n-2} $ for $n > 2$ and $Y_{1}<Y_{2}$ . I have tried solving, but the limit tends to exist between $y_{1}$ and $y_{2}$ and hence I am not able to evaluate what would be the ratio between which they would lie?","What is the limit of for and . I have tried solving, but the limit tends to exist between and and hence I am not able to evaluate what would be the ratio between which they would lie?",Y_{n} := \frac{1}{3}Y_{n-1} + \frac{2}{3}Y_{n-2}  n > 2 Y_{1}<Y_{2} y_{1} y_{2},"['sequences-and-series', 'limits']"
73,Limit of Sum related question,Limit of Sum related question,,"For $a\in R$ & $a \ne 1$ $$\mathop {\lim }\limits_{n \to \infty } \frac{{{1^a} + {2^a} + ... + {n^a}}}{{{{\left( {n + 1} \right)}^{a - 1}}\left[ {\left( {na + 1} \right) + \left( {na + 2} \right) + .. + \left( {na + n} \right)} \right]}} = \frac{1}{{60}}$$ , then find the value of $a$ . My approach is as follows: $$\mathop {\lim }\limits_{n \to \infty } \frac{{{1^a} + {2^a} + ... + {n^a}}}{{{{\left( {n + 1} \right)}^{a - 1}}\left[ {\left( {{n^2}a + \frac{{n\left( {n + 1} \right)}}{2}} \right)} \right]}} = \frac{1}{{60}}$$ How to I convert it into limit of sum so that I can proceed with the integration?","For & , then find the value of . My approach is as follows: How to I convert it into limit of sum so that I can proceed with the integration?",a\in R a \ne 1 \mathop {\lim }\limits_{n \to \infty } \frac{{{1^a} + {2^a} + ... + {n^a}}}{{{{\left( {n + 1} \right)}^{a - 1}}\left[ {\left( {na + 1} \right) + \left( {na + 2} \right) + .. + \left( {na + n} \right)} \right]}} = \frac{1}{{60}} a \mathop {\lim }\limits_{n \to \infty } \frac{{{1^a} + {2^a} + ... + {n^a}}}{{{{\left( {n + 1} \right)}^{a - 1}}\left[ {\left( {{n^2}a + \frac{{n\left( {n + 1} \right)}}{2}} \right)} \right]}} = \frac{1}{{60}},"['calculus', 'limits']"
74,Epsilon Delta proof for rational function containing radicals,Epsilon Delta proof for rational function containing radicals,,"I am having trouble constructing the delta-epsilon proof for this limit. I am currently trying to find the delta in terms of epsilon: $$\lim_{x \to 1} \frac{x^3-1}{\sqrt{x}-1}$$ So far I have gotten to $$6-\epsilon < (x-1)*\frac{x^2+x+1}{\sqrt{x}-1} < 6+\epsilon$$ I am having trouble bounding the fraction $\frac{x^2+x+1}{\sqrt{x}-1}$ so that I can isolate $x-1$ and find the delta. It has a vertical asymptote at x=1, which makes it difficult to find an upper/lower bound. Am I on the right track to this problem? If not, how would I go about finding the delta?","I am having trouble constructing the delta-epsilon proof for this limit. I am currently trying to find the delta in terms of epsilon: So far I have gotten to I am having trouble bounding the fraction so that I can isolate and find the delta. It has a vertical asymptote at x=1, which makes it difficult to find an upper/lower bound. Am I on the right track to this problem? If not, how would I go about finding the delta?",\lim_{x \to 1} \frac{x^3-1}{\sqrt{x}-1} 6-\epsilon < (x-1)*\frac{x^2+x+1}{\sqrt{x}-1} < 6+\epsilon \frac{x^2+x+1}{\sqrt{x}-1} x-1,"['calculus', 'algebra-precalculus', 'limits', 'epsilon-delta']"
75,Limit of $f(x)= -x \tanh(x) +\log(2 \cosh (x))$ at $+\infty$,Limit of  at,f(x)= -x \tanh(x) +\log(2 \cosh (x)) +\infty,I am trying to calculate the limit for $x$ going to $+\infty$ of the following function . The problem originated from a physical model actually $$f(x)= -x\tanh(x) +\log(2\cosh(x))$$ I get an indeterminate form of the form $-\infty +\infty $ that I don't know how to solve due to the complexity of the functions. Any idea?,I am trying to calculate the limit for going to of the following function . The problem originated from a physical model actually I get an indeterminate form of the form that I don't know how to solve due to the complexity of the functions. Any idea?,x +\infty f(x)= -x\tanh(x) +\log(2\cosh(x)) -\infty +\infty ,"['calculus', 'limits']"
76,Proof $f$ has a minimum if $f$ is continuous and $\lim_{x \rightarrow \infty}(f(x))=\infty=\lim_{x \rightarrow -\infty}(f(x))$,Proof  has a minimum if  is continuous and,f f \lim_{x \rightarrow \infty}(f(x))=\infty=\lim_{x \rightarrow -\infty}(f(x)),"I have a question about the validity of this proof I wrote: Claim : If $f$ is continuous on $\mathbb{R}$ and $\lim_{x \rightarrow \infty}(f(x))=\infty=\lim_{x \rightarrow -\infty}(f(x))$ , then $f$ has a minimum. My proof : Lower Bound Theorem) If $g$ is continuous on $[a,b]$ , then $\exists z \in [a,b]: \forall x \in [a,b]: f(z)\leq f(x)$ (1) $\forall N_1: \exists n_1>0: \text{if } x>n_1 \text{ then } f(x)>N_1$ (Definition) (2) $\forall N_2: \exists n_2<0: \text{if } x<n_2 \text{ then } f(x)>N_2$ (Definition) (3) Given $N_1, N_2$ : Let $I = [n_2, n_1]$ (4) $\therefore f$ is continuous on $I$ (5) $\therefore \exists y \in I: \forall x \in I:f(y) \leq f(x)$ (Using Lower Bound Theorem) (6) If $x \in \mathbb{R}$ but $x \notin I$ , then $x < n_2$ or $x>n_1$ . (7) $\therefore f(x) > N_1$ or $f(x) > N_2$ (8) Choose $N_1, N_2: f(y)<N_1$ and $f(y)<N_2$ (9) $\therefore \forall x \in \mathbb{R}:f(y) \leq f(x)$ Thanks for reading. I'm not sure about line 8. I already defined $N_1$ and $N_2$ in line 3, and constructed $y$ based on that in line 5. So if I redefine $N_1$ and $N_2$ based on $y$ in line 8, that could potentially redefine $y$ in line 5, which could redefine $N_1$ and $N_2$ in line 8, which could redefine $y$ in line 5, etc. etc. I'm pretty sure either this is just a little technicality that can be fixed easily but I'm not sure how, or it's not a problem at all and I'm overthinking. Can someone help?","I have a question about the validity of this proof I wrote: Claim : If is continuous on and , then has a minimum. My proof : Lower Bound Theorem) If is continuous on , then (1) (Definition) (2) (Definition) (3) Given : Let (4) is continuous on (5) (Using Lower Bound Theorem) (6) If but , then or . (7) or (8) Choose and (9) Thanks for reading. I'm not sure about line 8. I already defined and in line 3, and constructed based on that in line 5. So if I redefine and based on in line 8, that could potentially redefine in line 5, which could redefine and in line 8, which could redefine in line 5, etc. etc. I'm pretty sure either this is just a little technicality that can be fixed easily but I'm not sure how, or it's not a problem at all and I'm overthinking. Can someone help?","f \mathbb{R} \lim_{x \rightarrow \infty}(f(x))=\infty=\lim_{x \rightarrow -\infty}(f(x)) f g [a,b] \exists z \in [a,b]: \forall x \in [a,b]: f(z)\leq f(x) \forall N_1: \exists n_1>0: \text{if } x>n_1 \text{ then } f(x)>N_1 \forall N_2: \exists n_2<0: \text{if } x<n_2 \text{ then } f(x)>N_2 N_1, N_2 I = [n_2, n_1] \therefore f I \therefore \exists y \in I: \forall x \in I:f(y) \leq f(x) x \in \mathbb{R} x \notin I x < n_2 x>n_1 \therefore f(x) > N_1 f(x) > N_2 N_1, N_2: f(y)<N_1 f(y)<N_2 \therefore \forall x \in \mathbb{R}:f(y) \leq f(x) N_1 N_2 y N_1 N_2 y y N_1 N_2 y","['real-analysis', 'limits']"
77,Can I pass the limit under the integral?,Can I pass the limit under the integral?,,"I couldn't find the question on the website. I need to determine the integral $\lim_{n\rightarrow \infty }\int_{\left[ 0,1\right] }f_{n}d\mu $ , if it exists, where $\mu $ is the Lebesgue measure and $$ f_{n}=\frac{\sin \left( \left( x+\frac{1}{n}\right) ^{2}\right) -\sin \left( \left( x-\frac{1}{n}\right) ^{2}\right) }{\sin \frac{1}{n}} $$ Now I can see that $$\frac{\sin \left( \left( x+\frac{1}{n}\right) ^{2}\right) -\sin \left( \left( x-\frac{1}{n}\right) ^{2}\right) }{\sin  \frac{1}{n}}=\frac{\sin \left( \left( x+\frac{1}{n}\right) ^{2}\right) -\sin \left( \left( x-\frac{1}{n}\right) ^{2}\right) }{\frac{2}{n}}2\frac{\frac{1}{n}}{\sin \frac{1}{n}}$$ and using the fact that $g^{\prime }\left( x\right) =\lim_{h\rightarrow 0}\frac{g\left( x+h\right) -g\left( x-h\right) }{2h}$ for $g\left( x\right) =\sin \left( x^{2}\right) $ and that $\frac{\frac{1}{n}}{\sin \frac{1}{n}}\longrightarrow 1$ , we conclude that $f_{n}\longrightarrow 4x\cos x^{2}$ but I am having trouble moving the limit under the integral sign. What I think is that, since this sequence of functions converges pointwise, they must be bounded by a finite function $% m\left( x\right) $ , which is Lebesgue integrable and then we can apply the DCT. Is this okay? I tried to show that convergence is uniform but my hand wouldn't move.","I couldn't find the question on the website. I need to determine the integral , if it exists, where is the Lebesgue measure and Now I can see that and using the fact that for and that , we conclude that but I am having trouble moving the limit under the integral sign. What I think is that, since this sequence of functions converges pointwise, they must be bounded by a finite function , which is Lebesgue integrable and then we can apply the DCT. Is this okay? I tried to show that convergence is uniform but my hand wouldn't move.","\lim_{n\rightarrow \infty }\int_{\left[
0,1\right] }f_{n}d\mu  \mu  
f_{n}=\frac{\sin \left( \left( x+\frac{1}{n}\right) ^{2}\right) -\sin \left(
\left( x-\frac{1}{n}\right) ^{2}\right) }{\sin \frac{1}{n}}
 \frac{\sin \left( \left( x+\frac{1}{n}\right)
^{2}\right) -\sin \left( \left( x-\frac{1}{n}\right) ^{2}\right) }{\sin 
\frac{1}{n}}=\frac{\sin \left( \left( x+\frac{1}{n}\right) ^{2}\right) -\sin
\left( \left( x-\frac{1}{n}\right) ^{2}\right) }{\frac{2}{n}}2\frac{\frac{1}{n}}{\sin \frac{1}{n}} g^{\prime }\left( x\right)
=\lim_{h\rightarrow 0}\frac{g\left( x+h\right) -g\left( x-h\right) }{2h} g\left( x\right) =\sin \left( x^{2}\right)  \frac{\frac{1}{n}}{\sin \frac{1}{n}}\longrightarrow 1 f_{n}\longrightarrow 4x\cos x^{2} %
m\left( x\right) ","['limits', 'measure-theory', 'lebesgue-integral']"
78,The value of the following product is?,The value of the following product is?,,"Evaluate the following product: $$\newcommand{\T}[1]{\frac{\sin\frac{\theta}{#1}}{\tan^2\frac{\theta}{#1}\tan\frac{2\theta}{#1} + \tan\frac{\theta}{#1}}} \\ P(\theta) = \T{2} \times \T{2^2} \times \T{2^3} \times .... \infty$$ For $\theta = \frac \pi 4$ Simplified, $P(\theta)$ is $$P(\theta) = \lim_{n \to \infty}\prod_{r=1}^n T(\theta,r)= \lim_{n \to \infty}\prod_{r=1}^n\T{2^r}$$ The denominator can be simplified as follows: $$D = \tan\frac{\theta}{2^r}\left( \tan\frac{\theta}{2^r}\tan\frac{\theta}{2^{r-1}} + 1\right) \\ = \tan\frac{\theta}{2^{r-1}} - \tan\frac{\theta}{2^{r}}$$ After this, $P(\theta)$ becomes $$P(\theta) = \lim_{n \to \infty}\prod_{r=1}^n \frac{\sin\frac{\theta}{2^r}}{\tan\frac{\theta}{2^{r-1}}- \tan\frac{\theta}{2^r}}$$ One more detail I found out is that $\lim_{n \to \infty} T(\theta,n) = 1$ , but I couldn't proceed further from here. Any hints/solutions are appreciated. EDIT : After the hints in the comments, $T(\theta, r)$ resolves to $\cos \frac \theta {2^{r-1}} \cos \frac \theta {2^r}$ as follows (assuming $\frac \theta {2^r} = t$ ) $$\begin{gather} T(\theta, n) = \frac{\sin t}{\tan^2t\tan 2t + \tan t} \\ = \frac{\cos t}{\tan t \tan 2t + 1} \\ = \frac{\cos t(1-\tan^2t)}{1+\tan^2t} \\ = \cos t \cos 2t \\ = \cos \frac \theta {2^{r-1}} \cos \frac \theta {2^r} \end{gather}$$ Now, $$P(\theta) = \lim_{n \to \infty} \frac{ \left( \cos\theta\cos\frac\theta2... \cos \frac{\theta}{2^n} \right)^2 }{\cos\theta} = \frac{\sin^2\theta}{2^{2n}\sin^2 \frac \theta {2^n}\cos \theta} = \frac{\sin^2 \theta}{\theta^2 \cos \theta}$$ Therefore, $$\boxed{P(\pi/4) = \frac{8\sqrt2}{\pi^2}}$$ However, the answer mentioned in the textbook is $\frac{2}{\pi}$ . Where am I going wrong? (I think there's a silly mistake somewhere here; just not able to find it :(","Evaluate the following product: For Simplified, is The denominator can be simplified as follows: After this, becomes One more detail I found out is that , but I couldn't proceed further from here. Any hints/solutions are appreciated. EDIT : After the hints in the comments, resolves to as follows (assuming ) Now, Therefore, However, the answer mentioned in the textbook is . Where am I going wrong? (I think there's a silly mistake somewhere here; just not able to find it :(","\newcommand{\T}[1]{\frac{\sin\frac{\theta}{#1}}{\tan^2\frac{\theta}{#1}\tan\frac{2\theta}{#1} + \tan\frac{\theta}{#1}}} \\
P(\theta) = \T{2} \times \T{2^2} \times \T{2^3} \times .... \infty \theta = \frac \pi 4 P(\theta) P(\theta) = \lim_{n \to \infty}\prod_{r=1}^n T(\theta,r)= \lim_{n \to \infty}\prod_{r=1}^n\T{2^r} D = \tan\frac{\theta}{2^r}\left( \tan\frac{\theta}{2^r}\tan\frac{\theta}{2^{r-1}} + 1\right) \\
= \tan\frac{\theta}{2^{r-1}} - \tan\frac{\theta}{2^{r}} P(\theta) P(\theta) = \lim_{n \to \infty}\prod_{r=1}^n \frac{\sin\frac{\theta}{2^r}}{\tan\frac{\theta}{2^{r-1}}- \tan\frac{\theta}{2^r}} \lim_{n \to \infty} T(\theta,n) = 1 T(\theta, r) \cos \frac \theta {2^{r-1}} \cos \frac \theta {2^r} \frac \theta {2^r} = t \begin{gather}
T(\theta, n) = \frac{\sin t}{\tan^2t\tan 2t + \tan t} \\
= \frac{\cos t}{\tan t \tan 2t + 1} \\
= \frac{\cos t(1-\tan^2t)}{1+\tan^2t} \\
= \cos t \cos 2t \\
= \cos \frac \theta {2^{r-1}} \cos \frac \theta {2^r}
\end{gather} P(\theta) = \lim_{n \to \infty} \frac{ \left( \cos\theta\cos\frac\theta2... \cos \frac{\theta}{2^n} \right)^2 }{\cos\theta} = \frac{\sin^2\theta}{2^{2n}\sin^2 \frac \theta {2^n}\cos \theta} = \frac{\sin^2 \theta}{\theta^2 \cos \theta} \boxed{P(\pi/4) = \frac{8\sqrt2}{\pi^2}} \frac{2}{\pi}","['sequences-and-series', 'limits', 'trigonometry', 'limits-without-lhopital']"
79,Changing limit and derivative operator,Changing limit and derivative operator,,I was trying to solve the following problem: Let $f:\mathbb R\rightarrow \mathbb R$ be a differentiable function such that $\lim_{x \to \infty} f(x)=1$ and $\lim_{x \to \infty}f'(x)=a$ . Then find the value of $a$ . My brother who is not a pure math student suggested that take derivative on first equation you will get $a=0$ . I am not sure whether we can do that or not. Can we do that? I don't think it's the correct way of solving this.,I was trying to solve the following problem: Let be a differentiable function such that and . Then find the value of . My brother who is not a pure math student suggested that take derivative on first equation you will get . I am not sure whether we can do that or not. Can we do that? I don't think it's the correct way of solving this.,f:\mathbb R\rightarrow \mathbb R \lim_{x \to \infty} f(x)=1 \lim_{x \to \infty}f'(x)=a a a=0,"['real-analysis', 'limits', 'derivatives']"
80,Evaluating limits of integrals,Evaluating limits of integrals,,How to evaluate $$\lim_{n \to \infty}\sum_{m=1}^{\infty}\int_{0}^{\infty} \left(\frac{ m+x}{(m^n+x^n)^n} \right )dx$$ I made the substitution $$x = mt$$ and factored out $$m^{-(n^2-2)}$$ . I got this: $$\lim_{n \to \infty} \left(\sum_{m=1}^{\infty}m^{-(n^{2}-2)}\right)\int_{0}^{\infty} (1+t)(1+t^n)^{-n} dt $$ After that I tried the substitution: $$t^{n}=tan^{2}\theta$$ but after substitution I got two beta integrals after which I couldn't proceed further. I got the following: $$\frac{2}{n}\lim_{n \to \infty} \left(\sum_{m=1}^{\infty}m^{-(n^{2}-2)}\right)\int_{0}^{\frac{π}{2}} (\sin^{(\frac{2}{n}-1)}{\theta}\cos^{(2n-\frac{2}{n}-1)}\theta + \sin^{(\frac{4}{n}-1)}{\theta}\cos^{(2n-\frac{4}{n}-1)}\theta)  d\theta $$ I couldn't proceed further.The answer is 3/2. Could someone clarify ? Thank you.,How to evaluate I made the substitution and factored out . I got this: After that I tried the substitution: but after substitution I got two beta integrals after which I couldn't proceed further. I got the following: I couldn't proceed further.The answer is 3/2. Could someone clarify ? Thank you.,"\lim_{n \to \infty}\sum_{m=1}^{\infty}\int_{0}^{\infty} \left(\frac{ m+x}{(m^n+x^n)^n} \right )dx x = mt m^{-(n^2-2)} \lim_{n \to \infty} \left(\sum_{m=1}^{\infty}m^{-(n^{2}-2)}\right)\int_{0}^{\infty} (1+t)(1+t^n)^{-n} dt  t^{n}=tan^{2}\theta \frac{2}{n}\lim_{n \to \infty} \left(\sum_{m=1}^{\infty}m^{-(n^{2}-2)}\right)\int_{0}^{\frac{π}{2}} (\sin^{(\frac{2}{n}-1)}{\theta}\cos^{(2n-\frac{2}{n}-1)}\theta +
\sin^{(\frac{4}{n}-1)}{\theta}\cos^{(2n-\frac{4}{n}-1)}\theta)
 d\theta ","['limits', 'definite-integrals']"
81,Can we recover left-hand derivative from right-hand derivative,Can we recover left-hand derivative from right-hand derivative,,"Consider a convex function $f(x)$ on interval $(a,b) \subseteq \mathbb{R}$ . According to ""A user's guide to measure theoretic probability"" by Pollard (see Appendix C), its right-hand $D_{+}(x)$ and left-hand $D_{-}(x)$ derivatives $D_{-}(x_0) = \lim_{x \rightarrow x_0^{-}} \frac{f(x) - f(x_0)}{x - x_0} , \quad D_{+}(x_0) = \lim_{x \rightarrow x_0^{+}} \frac{f(x) - f(x_0)}{x - x_0}$ exist at any $x_0 \in (a,b)$ . Further, we know that $D_{+}(x)$ is increasing and right-continuous, and $D_{-}(x)$ is increasing and left-continuous, w.r.t. domain $(a,b)$ . My question is following: given $D_{+}(x)$ , can we recover $D_{-}(x)$ ? If no, then under what additional conditions it is possible? My current guess is to do the recovery as: $D_{-}(x_0) = \lim_{x \rightarrow x_0^{-}} D_{+}(x)$ Yet, I'm not sure if the left-hand limit exist at $D_{+}(x_0)$ . Is the above statement correct? How can we prove it?","Consider a convex function on interval . According to ""A user's guide to measure theoretic probability"" by Pollard (see Appendix C), its right-hand and left-hand derivatives exist at any . Further, we know that is increasing and right-continuous, and is increasing and left-continuous, w.r.t. domain . My question is following: given , can we recover ? If no, then under what additional conditions it is possible? My current guess is to do the recovery as: Yet, I'm not sure if the left-hand limit exist at . Is the above statement correct? How can we prove it?","f(x) (a,b) \subseteq \mathbb{R} D_{+}(x) D_{-}(x) D_{-}(x_0)
=
\lim_{x \rightarrow x_0^{-}}
\frac{f(x) - f(x_0)}{x - x_0}
,
\quad
D_{+}(x_0)
=
\lim_{x \rightarrow x_0^{+}}
\frac{f(x) - f(x_0)}{x - x_0} x_0 \in (a,b) D_{+}(x) D_{-}(x) (a,b) D_{+}(x) D_{-}(x) D_{-}(x_0) = \lim_{x \rightarrow x_0^{-}} D_{+}(x) D_{+}(x_0)","['functional-analysis', 'limits', 'derivatives', 'convex-analysis']"
82,Prove $\lim_{n\mapsto 0}[(\psi(n)+\gamma)\psi^{(1)}(n)-\frac12\psi^{(2)}(n)]=2\zeta(3)$,Prove,\lim_{n\mapsto 0}[(\psi(n)+\gamma)\psi^{(1)}(n)-\frac12\psi^{(2)}(n)]=2\zeta(3),How to prove that $$\lim_{n\mapsto 0}[(\psi(n)+\gamma)\psi^{(1)}(n)-\frac12\psi^{(2)}(n)]=2\zeta(3)\ ?$$ I encountered this limit while I was trying to solve $\int_0^1\frac{\ln x\ln(1-x)}{x(1-x)}dx$ using the derivative of beta function but I have no idea how to tackle this limit. We know that this integral is very simple : $$\int_0^1\frac{\ln x\ln(1-x)}{x(1-x)}dx=\int_0^1\frac{\ln x\ln(1-x)}{x}dx+\underbrace{\int_0^1\frac{\ln x\ln(1-x)}{1-x}dx}_{1-x\to x}$$ $$=2\int_0^1\frac{\ln x\ln(1-x)}{x}dx=2\zeta(3)$$ but using integration does not always work for high-power log integrals and beta function would be the right tool but my problem is only when $n\to 0$ . Any help would be appreciated. Note: No solutions using asymptotic expansion please.,How to prove that I encountered this limit while I was trying to solve using the derivative of beta function but I have no idea how to tackle this limit. We know that this integral is very simple : but using integration does not always work for high-power log integrals and beta function would be the right tool but my problem is only when . Any help would be appreciated. Note: No solutions using asymptotic expansion please.,\lim_{n\mapsto 0}[(\psi(n)+\gamma)\psi^{(1)}(n)-\frac12\psi^{(2)}(n)]=2\zeta(3)\ ? \int_0^1\frac{\ln x\ln(1-x)}{x(1-x)}dx \int_0^1\frac{\ln x\ln(1-x)}{x(1-x)}dx=\int_0^1\frac{\ln x\ln(1-x)}{x}dx+\underbrace{\int_0^1\frac{\ln x\ln(1-x)}{1-x}dx}_{1-x\to x} =2\int_0^1\frac{\ln x\ln(1-x)}{x}dx=2\zeta(3) n\to 0,"['limits', 'gamma-function', 'harmonic-numbers', 'beta-function', 'polygamma']"
83,Is Analysis I by Terence Tao incomplete?,Is Analysis I by Terence Tao incomplete?,,"I am trying self-study real analysis using Analysis I by Terence Tao . I came across some of the following: Chapter 1: Introduction, page 8: Limit interchange is always untrustworthy? (See Proposition $11.15 .3$ for an answer. But there is no Proposition 11.15.3 . Chapter 11 has only 10 sections. Another example from chapter1 page 10: Since $1 \neq 0,$ we thus seem to have shown that interchange of derivatives is untrustworthy. But are there any other circumstances in which the interchange of derivatives is legitimate? (See Theorem $11.37 .4$ and Exercise $11.37 .1$ for some answers.) Again I could find neither Theorem 11.37.4 nor Exercise 11.37.1 . Am I missing something or is there is some mistake in the book?","I am trying self-study real analysis using Analysis I by Terence Tao . I came across some of the following: Chapter 1: Introduction, page 8: Limit interchange is always untrustworthy? (See Proposition for an answer. But there is no Proposition 11.15.3 . Chapter 11 has only 10 sections. Another example from chapter1 page 10: Since we thus seem to have shown that interchange of derivatives is untrustworthy. But are there any other circumstances in which the interchange of derivatives is legitimate? (See Theorem and Exercise for some answers.) Again I could find neither Theorem 11.37.4 nor Exercise 11.37.1 . Am I missing something or is there is some mistake in the book?","11.15 .3 1 \neq 0, 11.37 .4 11.37 .1","['real-analysis', 'limits', 'reference-request', 'book-recommendation']"
84,Evaluate $\lim_{x\to 0} \frac{f(x^3)}{x}$,Evaluate,\lim_{x\to 0} \frac{f(x^3)}{x},"Let $f:\mathbb{R}\to \mathbb{R}$ be a function such that $|f(x)|\leq 2|x|$ for every $x \in \mathbb{R}$ . Evaluate $\lim_{x\to 0} \frac{f(x^3)}{x}$ . According to the answer key, it is $0$ (which matches mine). I am not so sure about my solution (below) though. Rewritting $\lim_{x\to 0} \frac{f(x^3)}{x}$ yields $$\lim_{x\to 0} \frac{f(x^3)}{x} = \lim_{x\to 0} 2\frac{|x^3|}{x} = 2\lim_{x\to 0} \frac{|x^3|}{x}$$ Analyzing one-sided limits, we have: $$\lim_{x\to 0^-}  \frac{-x^3}{x}=\lim_{x\to 0^-} -x^2=0$$ and $$\lim_{x\to 0^+}  \frac{x^3}{x}=\lim_{x\to 0^+} x^2=0$$ Both one-sided limits exist and are equal, therefore $$2\lim_{x\to 0} \frac{|x^3|}{x}= 2\cdot0=0$$ . Is my solution correct?","Let be a function such that for every . Evaluate . According to the answer key, it is (which matches mine). I am not so sure about my solution (below) though. Rewritting yields Analyzing one-sided limits, we have: and Both one-sided limits exist and are equal, therefore . Is my solution correct?",f:\mathbb{R}\to \mathbb{R} |f(x)|\leq 2|x| x \in \mathbb{R} \lim_{x\to 0} \frac{f(x^3)}{x} 0 \lim_{x\to 0} \frac{f(x^3)}{x} \lim_{x\to 0} \frac{f(x^3)}{x} = \lim_{x\to 0} 2\frac{|x^3|}{x} = 2\lim_{x\to 0} \frac{|x^3|}{x} \lim_{x\to 0^-}  \frac{-x^3}{x}=\lim_{x\to 0^-} -x^2=0 \lim_{x\to 0^+}  \frac{x^3}{x}=\lim_{x\to 0^+} x^2=0 2\lim_{x\to 0} \frac{|x^3|}{x}= 2\cdot0=0,"['calculus', 'limits']"
85,Prove: $\lim_{t \to \infty} \frac{1}{t}\ln\big(\int_0^1 e^{-tf(x)}dx\big) = -\min \ f(x)$,Prove:,\lim_{t \to \infty} \frac{1}{t}\ln\big(\int_0^1 e^{-tf(x)}dx\big) = -\min \ f(x),"Let $f$ be a continuous function $[0,1] \rightarrow \mathbb{R}$ . Prove: $\forall \ t> 0;\frac{1}{t}\ln\Big(\int_0^1 e^{-tf(x)}dx\big) \le -\min \ f(x)$ $\lim_{t \to \infty} \frac{1}{t}\ln\big(\int_0^1 e^{-tf(x)}dx\big) = -\min \ f(x)$ I solved 1) Let $L =\min \ f(x)$ so : $\frac{1}{t}\ln(\int_0^1 e^{-tf(x)}dx) \le \frac{1}{t}\ln(\int_0^1 e^{-tL}dx) = \frac{1}{t}\ln(e^{-tL}) = \frac{-tL}{t} = -L$ But I'm not sure how to bound it from below or how to use another method to show the equality",Let be a continuous function . Prove: I solved 1) Let so : But I'm not sure how to bound it from below or how to use another method to show the equality,"f [0,1] \rightarrow \mathbb{R} \forall \ t> 0;\frac{1}{t}\ln\Big(\int_0^1 e^{-tf(x)}dx\big) \le -\min \ f(x) \lim_{t \to \infty} \frac{1}{t}\ln\big(\int_0^1 e^{-tf(x)}dx\big) = -\min \ f(x) L =\min \ f(x) \frac{1}{t}\ln(\int_0^1 e^{-tf(x)}dx) \le \frac{1}{t}\ln(\int_0^1 e^{-tL}dx) = \frac{1}{t}\ln(e^{-tL}) = \frac{-tL}{t} = -L","['calculus', 'integration', 'limits']"
86,Evaluate the limit for $p<1$,Evaluate the limit for,p<1,"I found this limit calculation problem in a book. For a real number $p\geq 0$ we have $$\lim_{n\rightarrow \infty}\frac{\left (1^{1^p}2^{2^p}\dots n^{n^p}\right )^{\frac{1}{n^{p+1}}}}{n^{\frac{1}{p+1}}}=e^{-\frac{1}{(p+1)^2}}$$ After taking logarithm this is equivalent to showing $$\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p \log k-\frac{1}{p+1}\log n\rightarrow -\frac{1}{(p+1)^2}$$ as $n\rightarrow \infty$ Now we know $$\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p \log \left (\frac{k}{n} \right )\rightarrow \int_0^1x^p\log x  \ dx=-\frac{1}{(p+1)^2}$$ To balance we have to evaluate the limit of $$\left (\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right ) \log n$$ Now observe the sum in brackets is the error of the Riemann sum associated to the Riemann Integral $\displaystyle{\int_0^1x^p dx}$ In the interval $\left [\frac{k}{n},\frac{k+1}{n} \right ]$ if we apply MVT to the function $x^p$ we get $$\left |x^p-\left (\frac{k}{n} \right )^p \right |\leq \left |\left (\frac{k+1}{n} \right )^p -\left (\frac{k}{n} \right )^p\right |=\frac{|p z_k^{p-1} |}{n}$$ for some $z_{k}\in \left [\frac{k}{n},\frac{k+1}{n} \right ] $ So we get $$\sup_{x\in \left [\frac{k}{n},\frac{k+1}{n} \right ]}\left |x^p-\left (\frac{k}{n} \right )^p \right |\leq \frac{p}{n}$$ if $p\geq 1$ Then we have $$\left | \sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right |=\left |\sum_{k=0}^{n-1} \int_{\frac{k}{n}}^{\frac{k+1}{n}}\left(x^p - \left ( \frac{k}{n}\right )^p \right )dx \right |\leq \frac{p}{n}$$ $$\implies \left (\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right ) \log n\rightarrow 0$$ as $n\rightarrow \infty$ and we are done. I am really having trouble with the $p<1$ case. Some help will be very appreciated.",I found this limit calculation problem in a book. For a real number we have After taking logarithm this is equivalent to showing as Now we know To balance we have to evaluate the limit of Now observe the sum in brackets is the error of the Riemann sum associated to the Riemann Integral In the interval if we apply MVT to the function we get for some So we get if Then we have as and we are done. I am really having trouble with the case. Some help will be very appreciated.,"p\geq 0 \lim_{n\rightarrow \infty}\frac{\left (1^{1^p}2^{2^p}\dots n^{n^p}\right )^{\frac{1}{n^{p+1}}}}{n^{\frac{1}{p+1}}}=e^{-\frac{1}{(p+1)^2}} \sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p \log k-\frac{1}{p+1}\log n\rightarrow -\frac{1}{(p+1)^2} n\rightarrow \infty \sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p \log \left (\frac{k}{n} \right )\rightarrow \int_0^1x^p\log x  \ dx=-\frac{1}{(p+1)^2} \left (\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right ) \log n \displaystyle{\int_0^1x^p dx} \left [\frac{k}{n},\frac{k+1}{n} \right ] x^p \left |x^p-\left (\frac{k}{n} \right )^p \right |\leq \left |\left (\frac{k+1}{n} \right )^p -\left (\frac{k}{n} \right )^p\right |=\frac{|p z_k^{p-1} |}{n} z_{k}\in \left [\frac{k}{n},\frac{k+1}{n} \right ]  \sup_{x\in \left [\frac{k}{n},\frac{k+1}{n} \right ]}\left |x^p-\left (\frac{k}{n} \right )^p \right |\leq \frac{p}{n} p\geq 1 \left | \sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right |=\left |\sum_{k=0}^{n-1} \int_{\frac{k}{n}}^{\frac{k+1}{n}}\left(x^p - \left ( \frac{k}{n}\right )^p \right )dx \right |\leq \frac{p}{n} \implies \left (\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right ) \log n\rightarrow 0 n\rightarrow \infty p<1","['real-analysis', 'limits', 'analysis']"
87,Proof related to the compostion of limits: am I proving it correctly?,Proof related to the compostion of limits: am I proving it correctly?,,"Let $(X,d_{X})$ , $(Y,d_{Y})$ , $(Z,d_{Z})$ be metric spaces, and let $x_{0}\in X$ , $y_{0}\in Y$ and $z_{0}\in Z$ . Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be functions, and let $E$ be a set. If we have $\displaystyle\lim_{x\rightarrow x_{0};x\in E}f(x) = y_{0}$ and $\displaystyle\lim_{y\rightarrow y_{0};y\in f(E)}g(y) = z_{0}$ , conclude that $\displaystyle\lim_{x\rightarrow x_{0};x\in E}(g\circ f)(x) = z_{0}$ . MY ATTEMPT According to the definition of limit, for every $\varepsilon > 0$ , there is a $\delta_{1} > 0$ such that for every $y\in f(E)$ we have that \begin{align*} d_{Y}(y,y_{0}) < \delta_{1} \Rightarrow d_{Z}(g(y)),z_{0}) < \varepsilon \end{align*} Simlarly, for every $\delta_{1} > 0$ there is a $\delta > 0$ such that for every $x\in E$ one has that \begin{align*} d_{X}(x,x_{0}) < \delta \Rightarrow d_{Y}(f(x),y_{0}) < \delta_{1} \end{align*} Since $y\in f(E)$ , we can assume that $y = f(x_{0})$ where $x_{0}\in E$ . Gathering both results, we conclude that for every $\varepsilon > 0$ , there is a $\delta > 0$ such that whenever $x\in E$ it results that \begin{align*} d_{X}(x,x_{0}) < \delta \Rightarrow d_{Y}(f(x_{0}),y_{0}) < \delta_{1} \Rightarrow d_{Z}(g(f(x_{0})),z_{0}) < \varepsilon \end{align*} that is to say, $(g\circ f)(x)$ approaches $z_{0}$ as $x$ approaches $x_{0}$ . I am mainly interested at knowing if I am writing rigorously and properly the proposed statement. Can someone tell me so?","Let , , be metric spaces, and let , and . Let and be functions, and let be a set. If we have and , conclude that . MY ATTEMPT According to the definition of limit, for every , there is a such that for every we have that Simlarly, for every there is a such that for every one has that Since , we can assume that where . Gathering both results, we conclude that for every , there is a such that whenever it results that that is to say, approaches as approaches . I am mainly interested at knowing if I am writing rigorously and properly the proposed statement. Can someone tell me so?","(X,d_{X}) (Y,d_{Y}) (Z,d_{Z}) x_{0}\in X y_{0}\in Y z_{0}\in Z f:X\rightarrow Y g:Y\rightarrow Z E \displaystyle\lim_{x\rightarrow x_{0};x\in E}f(x) = y_{0} \displaystyle\lim_{y\rightarrow y_{0};y\in f(E)}g(y) = z_{0} \displaystyle\lim_{x\rightarrow x_{0};x\in E}(g\circ f)(x) = z_{0} \varepsilon > 0 \delta_{1} > 0 y\in f(E) \begin{align*}
d_{Y}(y,y_{0}) < \delta_{1} \Rightarrow d_{Z}(g(y)),z_{0}) < \varepsilon
\end{align*} \delta_{1} > 0 \delta > 0 x\in E \begin{align*}
d_{X}(x,x_{0}) < \delta \Rightarrow d_{Y}(f(x),y_{0}) < \delta_{1}
\end{align*} y\in f(E) y = f(x_{0}) x_{0}\in E \varepsilon > 0 \delta > 0 x\in E \begin{align*}
d_{X}(x,x_{0}) < \delta \Rightarrow d_{Y}(f(x_{0}),y_{0}) < \delta_{1} \Rightarrow d_{Z}(g(f(x_{0})),z_{0}) < \varepsilon
\end{align*} (g\circ f)(x) z_{0} x x_{0}","['limits', 'metric-spaces', 'solution-verification']"
88,"Generalisation of $ \sum \frac {1}{k}-\ln n=\gamma$ to $0 \lt\alpha \lt1 , \sum \frac{1}{k^\alpha}-f(n)= \beta$",Generalisation of  to," \sum \frac {1}{k}-\ln n=\gamma 0 \lt\alpha \lt1 , \sum \frac{1}{k^\alpha}-f(n)= \beta","looking at Find the value of : $\lim\limits_{n\rightarrow\infty}\left({2\sqrt n}-\sum\limits_{k=1}^n\frac{1}{\sqrt k}\right)$ and knowing that for $$\alpha=1 ,\lim_{ n \to \infty}  \sum_{k=1}^n \frac {1}{k^\alpha}-\ln n=\gamma$$ Makes one wonder if there are other results for $0 \lt \alpha \lt 1 $ $$\lim_{ n \to \infty}  \sum_{k=1}^n \frac {1}{k^\alpha}-f(n)=\beta$$ where $f,\beta$ are determined by the value of $\alpha$ is there a topic that relates to some results similar to above?",looking at Find the value of : $\lim\limits_{n\rightarrow\infty}\left({2\sqrt n}-\sum\limits_{k=1}^n\frac{1}{\sqrt k}\right)$ and knowing that for Makes one wonder if there are other results for where are determined by the value of is there a topic that relates to some results similar to above?,"\alpha=1 ,\lim_{ n \to \infty}  \sum_{k=1}^n \frac {1}{k^\alpha}-\ln n=\gamma 0 \lt \alpha \lt 1  \lim_{ n \to \infty}  \sum_{k=1}^n \frac {1}{k^\alpha}-f(n)=\beta f,\beta \alpha","['sequences-and-series', 'limits', 'asymptotics', 'divergent-series']"
89,Prove that $(a_1-a_2)+(a_2-a_3)+....$ converges iff ${a_n}$ converges,Prove that  converges iff  converges,(a_1-a_2)+(a_2-a_3)+.... {a_n},"Let $b_n=a_n-a_{n+1}$ . We first assume that ${a_n}$ converges so , $\lim(b_n)=\lim(a_n)-\lim(a_{n+1})$ , hence $\lim(b_n)=0$ . Now let $s_m$ and $s_n$ be the consecutive partial sums of ${b_n}$ . So $|s_n-s_m| = |a_{n+1}-a_{m}|=|a_{n+1}-L+L-a_m|< \epsilon$ for $N\ge M \ge M(\epsilon)$ .So the series is cauchy convergent . Now let us assume that the series is convergent so the sequence formed by the partial sums are convergent hence $s_M=a_1-a_{M+1}$ hence $-\lim(s_M) +a_1 = \lim(a_{M+1})$ .So $A=-S+a_1$ . Can someone go through my attempt and point out my mistake instead of suggesting another answer.","Let . We first assume that converges so , , hence . Now let and be the consecutive partial sums of . So for .So the series is cauchy convergent . Now let us assume that the series is convergent so the sequence formed by the partial sums are convergent hence hence .So . Can someone go through my attempt and point out my mistake instead of suggesting another answer.",b_n=a_n-a_{n+1} {a_n} \lim(b_n)=\lim(a_n)-\lim(a_{n+1}) \lim(b_n)=0 s_m s_n {b_n} |s_n-s_m| = |a_{n+1}-a_{m}|=|a_{n+1}-L+L-a_m|< \epsilon N\ge M \ge M(\epsilon) s_M=a_1-a_{M+1} -\lim(s_M) +a_1 = \lim(a_{M+1}) A=-S+a_1,"['real-analysis', 'sequences-and-series', 'limits']"
90,Limit Points of a sequence on the Circle Group,Limit Points of a sequence on the Circle Group,,"I'm reading ch 7 of Nadkarni's book Spectral Theory of Dynamical Systems and I've come across this statement in a proof I'm currently trying to understand: If $z\in S^1$ (where $S^1$ denotes the Circle Group) and $z^{n_k}$ , with $k\in\mathbb{N}$ , has only a finite set of limit points of the form $e^{2\pi i \frac{p}{q}}$ (where $p$ and $q$ are integers and $q>0$ ), then for some integer $p_0$ , $z^{-p_0n_k}\rightarrow 1$ as $k\rightarrow\infty$ . I don't really understand, why this holds and would very much appreciate any input on this.","I'm reading ch 7 of Nadkarni's book Spectral Theory of Dynamical Systems and I've come across this statement in a proof I'm currently trying to understand: If (where denotes the Circle Group) and , with , has only a finite set of limit points of the form (where and are integers and ), then for some integer , as . I don't really understand, why this holds and would very much appreciate any input on this.",z\in S^1 S^1 z^{n_k} k\in\mathbb{N} e^{2\pi i \frac{p}{q}} p q q>0 p_0 z^{-p_0n_k}\rightarrow 1 k\rightarrow\infty,"['general-topology', 'limits', 'intuition', 'topological-groups']"
91,What's the limit of the string?,What's the limit of the string?,,"Find $$\lim_{n\to\infty} (x_n\sqrt{n})^{\sqrt{n^2-1}},$$ where $ x_{n+1} = \frac{x_n}{\sqrt{1+x_n^2}}$ and $x_1 = 2$ . I showed $x_n \to 0$ , $x_n\sqrt{n} \to 1$ , but i don't know how to solve limit properly.","Find where and . I showed , , but i don't know how to solve limit properly.","\lim_{n\to\infty} (x_n\sqrt{n})^{\sqrt{n^2-1}},  x_{n+1} = \frac{x_n}{\sqrt{1+x_n^2}} x_1 = 2 x_n \to 0 x_n\sqrt{n} \to 1","['calculus', 'sequences-and-series', 'limits', 'limits-without-lhopital']"
92,Taking Limits of Expressions of Different Variables in Proof of L'Hopital's Rule?,Taking Limits of Expressions of Different Variables in Proof of L'Hopital's Rule?,,"In my real analysis textbook, they present the proof of L'Hopital's by using Cauchy's mean value theorem. The part in the proof I am struggling to work out rigorously is a particular implication involving equivalent limits using different variables. We have assumed that $f(c)=g(c)=0$ , $f$ and $g$ are differentiable on a neighbourhood of $c$ , and that the limit $\lim_{x \to c} \frac{f'(x)}{g'(x)} = l$ . In the proof we consider a neighbourhood of $c$ , and we already know $f$ and $g$ are differentiable on the neighbourhood. We define $y$ to be a specific point such that $y > c$ . Then $f$ and $g$ satisfy Cauchy's Mean Value Theorem on $[c,y]$ . We then know that $$\frac{f'(z)}{g'(z)} = \frac{f(y)-f(c)}{g(y)-g(c)} = \frac{f(y)}{g(y)}$$ So now we let $y \to c^+$ and it follows that $z \to c^+$ . By simply renaming the domain variable, we know that $$\lim_{z \to c^+} \frac{f'(z)}{g'(z)} = l$$ My book then says: It follows that $$\lim_{y \to c^+} \frac{f(y)}{g(y)} = l$$ This step is intuitively obvious to me; and my reasoning is that by the way we have defined $y$ and $c$ , the expressions $\frac{f'(z)}{g'(z)}$ and $\frac{f(y)}{g(y)}$ are exactly equal for any corresponding values of $y$ and $c$ , and thus the sequences defined by these two expressions are exactly equal. Thus the limits are obviously equal. Is this reasoning correct? I also wanted to clarify how, if it is possible, I can take the limits of both the rhs and lhs of an equation when they are expressed using different variables? For example, if we have $f(x) = g(x)$ , then I can simply say $\lim_{x \to c} f(x) = \lim_{x \to c} g(x) = m$ . This is easy since there is only one domain variable. But in my particular case, how could I simultaneously take the limits of both sides of $\frac{f'(z)}{g'(z)} = \frac{f(y)}{g(y)}$ ? Is this even possible? The only justification I can think of is the one I described above, where I deduce that the two sequences must be the same for the particular limits I have chosen, and thus must have equal limits. In other words, I don't know a general method for taking the limit of equivalent expressions of different variables?","In my real analysis textbook, they present the proof of L'Hopital's by using Cauchy's mean value theorem. The part in the proof I am struggling to work out rigorously is a particular implication involving equivalent limits using different variables. We have assumed that , and are differentiable on a neighbourhood of , and that the limit . In the proof we consider a neighbourhood of , and we already know and are differentiable on the neighbourhood. We define to be a specific point such that . Then and satisfy Cauchy's Mean Value Theorem on . We then know that So now we let and it follows that . By simply renaming the domain variable, we know that My book then says: It follows that This step is intuitively obvious to me; and my reasoning is that by the way we have defined and , the expressions and are exactly equal for any corresponding values of and , and thus the sequences defined by these two expressions are exactly equal. Thus the limits are obviously equal. Is this reasoning correct? I also wanted to clarify how, if it is possible, I can take the limits of both the rhs and lhs of an equation when they are expressed using different variables? For example, if we have , then I can simply say . This is easy since there is only one domain variable. But in my particular case, how could I simultaneously take the limits of both sides of ? Is this even possible? The only justification I can think of is the one I described above, where I deduce that the two sequences must be the same for the particular limits I have chosen, and thus must have equal limits. In other words, I don't know a general method for taking the limit of equivalent expressions of different variables?","f(c)=g(c)=0 f g c \lim_{x \to c} \frac{f'(x)}{g'(x)} = l c f g y y > c f g [c,y] \frac{f'(z)}{g'(z)} = \frac{f(y)-f(c)}{g(y)-g(c)} = \frac{f(y)}{g(y)} y \to c^+ z \to c^+ \lim_{z \to c^+} \frac{f'(z)}{g'(z)} = l \lim_{y \to c^+} \frac{f(y)}{g(y)} = l y c \frac{f'(z)}{g'(z)} \frac{f(y)}{g(y)} y c f(x) = g(x) \lim_{x \to c} f(x) = \lim_{x \to c} g(x) = m \frac{f'(z)}{g'(z)} = \frac{f(y)}{g(y)}","['real-analysis', 'limits', 'proof-explanation']"
93,Find $\lim _{x\to \infty }\left(x\left(\arctan2x\:-\arccos\left(\frac{1}{x}\right)\right)\right)$,Find,\lim _{x\to \infty }\left(x\left(\arctan2x\:-\arccos\left(\frac{1}{x}\right)\right)\right),Find $$\lim _{x\to \infty }\left(x\left(\arctan(2x)-\arccos\left(\frac{1}{x}\right)\right)\right)$$ My idea was to let $t=\frac{1}{x}$ then I get $$\lim _{t\to 0 }...$$ but i did not know what to do with $\arctan(2x)$ any hint how to solve this ? thanks,Find My idea was to let then I get but i did not know what to do with any hint how to solve this ? thanks,\lim _{x\to \infty }\left(x\left(\arctan(2x)-\arccos\left(\frac{1}{x}\right)\right)\right) t=\frac{1}{x} \lim _{t\to 0 }... \arctan(2x),"['calculus', 'limits']"
94,Does $n^{1000000}/2^{n}$ converge as $n\rightarrow+\infty$? [duplicate],Does  converge as ? [duplicate],n^{1000000}/2^{n} n\rightarrow+\infty,"This question already has answers here : How to prove that exponential grows faster than polynomial? (15 answers) Closed 4 years ago . Does $n^{1000000}/2^{n}$ converge as $n\rightarrow+\infty$ ? To solve this problem, I am only allowed to use elementary properties of limits. This is what I tried. For $n > 1000000$ , one has that \begin{align*} \frac{n^{1000000}}{2^{n}} = \left(\frac{n}{2}\right)^{1000000}\times\left(\frac{1}{2}\right)^{n-1000000} \end{align*} But then I get stuck. This is not homework. Could someone help me out?","This question already has answers here : How to prove that exponential grows faster than polynomial? (15 answers) Closed 4 years ago . Does converge as ? To solve this problem, I am only allowed to use elementary properties of limits. This is what I tried. For , one has that But then I get stuck. This is not homework. Could someone help me out?","n^{1000000}/2^{n} n\rightarrow+\infty n > 1000000 \begin{align*}
\frac{n^{1000000}}{2^{n}} = \left(\frac{n}{2}\right)^{1000000}\times\left(\frac{1}{2}\right)^{n-1000000}
\end{align*}","['real-analysis', 'limits', 'limits-without-lhopital']"
95,"Does the following limit exist? $\lim \limits_{(x,y) \to (0,0)} \left(\frac{(x^4+y^4) \sin(\frac{1}{x})}{x^2+y^2}\right)$",Does the following limit exist?,"\lim \limits_{(x,y) \to (0,0)} \left(\frac{(x^4+y^4) \sin(\frac{1}{x})}{x^2+y^2}\right)","I'm having some trouble solving the following limit: $\lim \limits_{(x,y) \to (0,0)} \left(\frac{(x^4+y^4) \sin(\frac{1}{x})}{x^2+y^2}\right)$ Wolfram is suggesting that it doesn't exist. https://www.wolframalpha.com/input/?i=limit%5B%28x%5E4%2By%5E4%29sin%281%2Fx%29%2F%28x%5E2%2By%5E2%29%2C+x+to+0%2C+y+to+0%5D Can you point out where I am making a mistake? $0 \le \lvert \frac{(x^4+y^4) \sin(\frac{1}{x})}{x^2+y^2} \rvert \le \lvert \frac{x^4+y^4}{x^2+y^2} \rvert \le \lvert \frac{x^4+y^4+2x^2y^2}{x^2+y^2} \rvert = \lvert x^2+y^2 \rvert \to 0 $ as $(x,y) \to (0,0)$ By the squeeze theorem I get that the limit is $0$ . Any help is appreciated c:",I'm having some trouble solving the following limit: Wolfram is suggesting that it doesn't exist. https://www.wolframalpha.com/input/?i=limit%5B%28x%5E4%2By%5E4%29sin%281%2Fx%29%2F%28x%5E2%2By%5E2%29%2C+x+to+0%2C+y+to+0%5D Can you point out where I am making a mistake? as By the squeeze theorem I get that the limit is . Any help is appreciated c:,"\lim \limits_{(x,y) \to (0,0)} \left(\frac{(x^4+y^4) \sin(\frac{1}{x})}{x^2+y^2}\right) 0 \le \lvert \frac{(x^4+y^4) \sin(\frac{1}{x})}{x^2+y^2} \rvert \le \lvert \frac{x^4+y^4}{x^2+y^2} \rvert \le \lvert \frac{x^4+y^4+2x^2y^2}{x^2+y^2} \rvert = \lvert x^2+y^2 \rvert \to 0  (x,y) \to (0,0) 0","['real-analysis', 'limits']"
96,Prove that $\lim\limits_{n\to\infty} \prod\limits_{k=1}^n \sin(k)$ converges to $0$,Prove that  converges to,\lim\limits_{n\to\infty} \prod\limits_{k=1}^n \sin(k) 0,"$$\lim_{n\to\infty} \prod_{k=1}^n \sin(k) =0$$ How can I show that this in fact is true? I know that when I take the absolute value of this product than it is strictly decreasing and bounded between 0 and 1, but I want to show that this limit is actually 0.","How can I show that this in fact is true? I know that when I take the absolute value of this product than it is strictly decreasing and bounded between 0 and 1, but I want to show that this limit is actually 0.",\lim_{n\to\infty} \prod_{k=1}^n \sin(k) =0,"['sequences-and-series', 'limits', 'limits-without-lhopital', 'infinite-product']"
97,Accumulation points of a recursive sequence $a_n=\cos\left(\frac{2\pi n}{3}\right)\sqrt[n]{1+2^n+(-3)^n}$,Accumulation points of a recursive sequence,a_n=\cos\left(\frac{2\pi n}{3}\right)\sqrt[n]{1+2^n+(-3)^n},"Find all the accumulation points of the sequence $(a_n)_{n=1}^{\infty}$ defined recursively: $$a_n=\cos\left(\frac{2\pi n}{3}\right)\sqrt[n]{1+2^n+(-3)^n}$$ My attempt: $$\cos\left(\frac{2\pi n}{3}\right)\in\left\{-\frac{1}{2},1\right\}$$ $$\sqrt[n]{1+2^n+(-3)^n}=\sqrt[n]{(-3)^n\left(\left(-\frac{1}{3}\right)^n+\left(-\frac{2}{3}\right)^n+1\right)}=-3\sqrt[n]{\left(-\frac{1}{3}\right)^n+\left(-\frac{2}{3}\right)^n+1}$$ I tried to manipulate with the above expression and apply the squeeze theorem, but I haven't managed to get anything useful for this problem, except: $$\sqrt[n]{1+2^n+(-3)^n}<\sqrt[n]{3\cdot3^n}=3\sqrt[n]{3}$$ I also considered artithmetic inequalities and the comparison: $1+2^n\leq(1+2)^n$ , unsuccessfully. I would definitely examine the cases $n\pmod{6}$ or $n\pmod{3}$ . How can I move further from this point and what can I do with the $n-\text{th}$ root? I thought I could see how one of the factors behaves since $\cos\left(\frac{2\pi n}{3}\right)$ alternates for $n\in\mathbb N$ . Thank you in advance!","Find all the accumulation points of the sequence defined recursively: My attempt: I tried to manipulate with the above expression and apply the squeeze theorem, but I haven't managed to get anything useful for this problem, except: I also considered artithmetic inequalities and the comparison: , unsuccessfully. I would definitely examine the cases or . How can I move further from this point and what can I do with the root? I thought I could see how one of the factors behaves since alternates for . Thank you in advance!","(a_n)_{n=1}^{\infty} a_n=\cos\left(\frac{2\pi n}{3}\right)\sqrt[n]{1+2^n+(-3)^n} \cos\left(\frac{2\pi n}{3}\right)\in\left\{-\frac{1}{2},1\right\} \sqrt[n]{1+2^n+(-3)^n}=\sqrt[n]{(-3)^n\left(\left(-\frac{1}{3}\right)^n+\left(-\frac{2}{3}\right)^n+1\right)}=-3\sqrt[n]{\left(-\frac{1}{3}\right)^n+\left(-\frac{2}{3}\right)^n+1} \sqrt[n]{1+2^n+(-3)^n}<\sqrt[n]{3\cdot3^n}=3\sqrt[n]{3} 1+2^n\leq(1+2)^n n\pmod{6} n\pmod{3} n-\text{th} \cos\left(\frac{2\pi n}{3}\right) n\in\mathbb N","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
98,Find $\lim\limits_{n \to \infty} \int\limits_0^n \frac1{1 + n^2 \cos^2 x} dx$.,Find .,\lim\limits_{n \to \infty} \int\limits_0^n \frac1{1 + n^2 \cos^2 x} dx,"I have to find the limit: $$\lim\limits_{n \to \infty} \displaystyle\int_0^n \dfrac{1}{1 + n^2 \cos^2 x} dx$$ How should I approach this? I kept looking for some appropriate bounds (for the Squeeze Theorem) that I could use to determine the the limit, but I didn't come up with anything useful.","I have to find the limit: How should I approach this? I kept looking for some appropriate bounds (for the Squeeze Theorem) that I could use to determine the the limit, but I didn't come up with anything useful.",\lim\limits_{n \to \infty} \displaystyle\int_0^n \dfrac{1}{1 + n^2 \cos^2 x} dx,['calculus']
99,Proving the convergence of an infinite product,Proving the convergence of an infinite product,,"I’m trying to prove the Taylor series of $e$ using binomial expansion: $$e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \dots$$ The steps I’ve tried so far are: $$\left(1+\frac{1}{n}\right)^n \\ = 1 + \binom{n}{1}\frac{1}{n} + \binom{n}{2}\frac{1}{n^2} + \binom{n}{3}\frac{1}{n^3} + \dots + \binom{n}{n}\frac{1}{n^n} \\ = 1+\dfrac {n}{1!}\left( \dfrac {1}{n}\right) +\dfrac {n\left( n-1\right) }{2!}\left( \dfrac {1}{n^{2}}\right) + \dfrac {n\left( n-1\right) \left( n-2\right) }{3!}\left( \dfrac {1}{n^{3}}\right) +\dots +\dfrac {n\left( n-1\right) \left( n-2\right) \dots 1}{n!}\left( \dfrac {1}{n^{n}}\right) \\ = 1+\dfrac {1}{1!}+\dfrac {1}{2!}\left( 1-\dfrac {1}{n}\right) +\dfrac {1}{3!}\left( 1-\dfrac {1}{n}\right) \left( 1-\dfrac {2}{n}\right) +\ldots +\dfrac {1}{n!}\left( 1-\dfrac {1}{n}\right) \left( 1-\dfrac {2}{n}\right) \dots \left( 1-\dfrac {n-1}{n}\right) $$ As $n \to \infty$ , the last term becomes $$\lim_{n\to\infty} \left(\frac{1}{n!} \prod_{k=1}^n \left(1-\frac{k-1}{n}\right)\right)$$ and the infinite product on the right of the term should be proven to be convergent and equals to $1$ . So how do I prove that?","I’m trying to prove the Taylor series of using binomial expansion: The steps I’ve tried so far are: As , the last term becomes and the infinite product on the right of the term should be proven to be convergent and equals to . So how do I prove that?","e e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \dots \left(1+\frac{1}{n}\right)^n \\
= 1 + \binom{n}{1}\frac{1}{n} + \binom{n}{2}\frac{1}{n^2} + \binom{n}{3}\frac{1}{n^3} + \dots + \binom{n}{n}\frac{1}{n^n} \\
= 1+\dfrac {n}{1!}\left( \dfrac {1}{n}\right) +\dfrac {n\left( n-1\right) }{2!}\left( \dfrac {1}{n^{2}}\right) + \dfrac {n\left( n-1\right) \left( n-2\right) }{3!}\left( \dfrac {1}{n^{3}}\right) +\dots +\dfrac {n\left( n-1\right) \left( n-2\right) \dots 1}{n!}\left( \dfrac {1}{n^{n}}\right) \\
= 1+\dfrac {1}{1!}+\dfrac {1}{2!}\left( 1-\dfrac {1}{n}\right) +\dfrac {1}{3!}\left( 1-\dfrac {1}{n}\right) \left( 1-\dfrac {2}{n}\right) +\ldots +\dfrac {1}{n!}\left( 1-\dfrac {1}{n}\right) \left( 1-\dfrac {2}{n}\right) \dots
\left( 1-\dfrac {n-1}{n}\right)  n \to \infty \lim_{n\to\infty} \left(\frac{1}{n!} \prod_{k=1}^n \left(1-\frac{k-1}{n}\right)\right) 1","['limits', 'taylor-expansion', 'binomial-theorem', 'infinite-product', 'euler-mascheroni-constant']"
