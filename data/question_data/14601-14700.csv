,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Connecting the two definitions of $e$,Connecting the two definitions of,e,"There are two different ways I've been taught to understand the meaning of $e$ . $e$ can be defined as the total growth from continuously compounding interest in a single period. To make the growth rate the same as the dollar amount, consider I start with $\$1$ . Then, $a_n$ represents the amount I have after compounding $n$ times. \begin{align} a_n&=1\cdot\left(1+\frac{1}{n}\right)^n\\[5pt] e=a_{\max}&=\lim_{n \to \infty}\left(1+\frac{1}{n}\right)^n \end{align} As we increase the number of times we compound the interest, the final amount (in the case that we start with $\$1$ ), approaches $e$ . All exponential functions increase at a rate proportional to their current value. For example, consider the derivative of $2^x$ : \begin{align} \frac{d\left(2^x\right)}{dx}&=\lim_{h \to 0}\frac{2^{x+h}-2^x}{h}\\[5pt] &=\lim_{h \to 0}\frac{2^x2^h-2^x}{h}\\[5pt] &=\lim_{h \to 0}\frac{2^x\left(2^h-1\right)}{h}\\[5pt] &=2^x\lim_{h \to 0}\frac{\left(2^h-1\right)}{h} \end{align} From this we can see that the rate at which $2^x$ changes is proportional to its current value, with a proportionality constant of $$\lim_{h \to 0}\frac{\left(2^h-1\right)}{h}.$$ For $2^x$ , this constant happens to be $\ln(2)$ (why is this the case?). $e$ can be defined as the base where this proportionality constant is $1$ ; therefore, the rate $e^x$ grows at is exactly its current value. My question is this: how can the same constant have two (apparently) separate definitions? Is there a way of looking at $e$ that will combine these two definitions, and make it obvious as to why they are true? Also, why is the proportionality constant of any exponential functions growth rate the natural log of its base? Is there a way to understand why this is the case intuitively?","There are two different ways I've been taught to understand the meaning of . can be defined as the total growth from continuously compounding interest in a single period. To make the growth rate the same as the dollar amount, consider I start with . Then, represents the amount I have after compounding times. As we increase the number of times we compound the interest, the final amount (in the case that we start with ), approaches . All exponential functions increase at a rate proportional to their current value. For example, consider the derivative of : From this we can see that the rate at which changes is proportional to its current value, with a proportionality constant of For , this constant happens to be (why is this the case?). can be defined as the base where this proportionality constant is ; therefore, the rate grows at is exactly its current value. My question is this: how can the same constant have two (apparently) separate definitions? Is there a way of looking at that will combine these two definitions, and make it obvious as to why they are true? Also, why is the proportionality constant of any exponential functions growth rate the natural log of its base? Is there a way to understand why this is the case intuitively?","e e \1 a_n n \begin{align}
a_n&=1\cdot\left(1+\frac{1}{n}\right)^n\\[5pt]
e=a_{\max}&=\lim_{n \to \infty}\left(1+\frac{1}{n}\right)^n
\end{align} \1 e 2^x \begin{align}
\frac{d\left(2^x\right)}{dx}&=\lim_{h \to 0}\frac{2^{x+h}-2^x}{h}\\[5pt]
&=\lim_{h \to 0}\frac{2^x2^h-2^x}{h}\\[5pt]
&=\lim_{h \to 0}\frac{2^x\left(2^h-1\right)}{h}\\[5pt]
&=2^x\lim_{h \to 0}\frac{\left(2^h-1\right)}{h}
\end{align} 2^x \lim_{h \to 0}\frac{\left(2^h-1\right)}{h}. 2^x \ln(2) e 1 e^x e","['calculus', 'logarithms', 'exponential-function']"
1,A problem related to $2x^3-3x^2-x+\frac{3}{2}=0$,A problem related to,2x^3-3x^2-x+\frac{3}{2}=0,let $f(x)=2x^3-3x^2-x+\frac{3}{2}$ .Then prove that $$\int_{1/8}^{7/8} f(f(x)) \text d x\neq \frac{3}{4}$$ Factorising $$f(x)=2(x-1.5)(x-\frac{1}{\sqrt{2}})(x+\frac{1}{\sqrt{2}})$$ $$f(f(x))=2(f(x)-1.5)(f^2(x)-0.5)$$ But i don't see anything nice from this. Maybe if i could prove that $$f(f(x+\frac{1}{2}))=-f(f(x-\frac{1}{2}))$$ then the integral would turn out to be zero. also i dont think we have to find the exact value of integral for second part if we could just set up an inequality or prove that the integral is negative we are done! Please note that i am intersted in a proof without actually finding $f(f(x))$,let .Then prove that Factorising But i don't see anything nice from this. Maybe if i could prove that then the integral would turn out to be zero. also i dont think we have to find the exact value of integral for second part if we could just set up an inequality or prove that the integral is negative we are done! Please note that i am intersted in a proof without actually finding,f(x)=2x^3-3x^2-x+\frac{3}{2} \int_{1/8}^{7/8} f(f(x)) \text d x\neq \frac{3}{4} f(x)=2(x-1.5)(x-\frac{1}{\sqrt{2}})(x+\frac{1}{\sqrt{2}}) f(f(x))=2(f(x)-1.5)(f^2(x)-0.5) f(f(x+\frac{1}{2}))=-f(f(x-\frac{1}{2})) f(f(x)),"['calculus', 'integration', 'polynomials', 'definite-integrals', 'roots']"
2,Understanding Epsilon-Delta Proof,Understanding Epsilon-Delta Proof,,"I'm trying to understand the proof of the following limit using epsilon-delta definition. $$\lim _{x\to3} x^2 = 9$$ In Stewart Calculus, the proof goes like this: I'm confused at the following points: 1) How the author arrives at this result: $|x-3| < \varepsilon/C = \delta$ ? Specifically, how, $$|(x+3)(x-3)| < \varepsilon$$ and $$|(x+3)(x-3)| < C|(x-3)|$$ leads to $$|x-3| < \varepsilon/C = \delta.$$ This is the part I've trouble understanding. 2) What's the role played by $\delta=\min(1, \varepsilon/7)$ ? I am trying to understand this epsilon-delta proofs for a week but to no avail.","I'm trying to understand the proof of the following limit using epsilon-delta definition. In Stewart Calculus, the proof goes like this: I'm confused at the following points: 1) How the author arrives at this result: ? Specifically, how, and leads to This is the part I've trouble understanding. 2) What's the role played by ? I am trying to understand this epsilon-delta proofs for a week but to no avail.","\lim _{x\to3} x^2 = 9 |x-3| < \varepsilon/C = \delta |(x+3)(x-3)| < \varepsilon |(x+3)(x-3)| < C|(x-3)| |x-3| < \varepsilon/C = \delta. \delta=\min(1, \varepsilon/7)","['calculus', 'limits', 'proof-explanation', 'epsilon-delta']"
3,Is this logic behind the derivative of $x^x$ a coincidence?,Is this logic behind the derivative of  a coincidence?,x^x,"As many of you probably know, when you take the derivative of $x^x$ , you cannot treat it as either a exponential function ( $a^x$ ) or like a function of the form $x^a$ . If you treat it like $x^a$ then you get an aswer of $xx^{x-1}$ , which is just $x^x$ . Treat it like $a^x$ , and you get $x^xln(x)$ . The correct derivative is $x^x(ln(x)+1)$ , which you get by doing implicit differentiation after taking the natural log of both side. This is the same as $x^xln(x)+x^x$ , which happens to be the combination of the ""wrong"" derivatives you get when treating $x^x$ as the 2 different types of functions I specified earlier. Is there any real logic behind this or is it coincidence. Also, could this also be applied to other function hybrids like $x^x$ ?","As many of you probably know, when you take the derivative of , you cannot treat it as either a exponential function ( ) or like a function of the form . If you treat it like then you get an aswer of , which is just . Treat it like , and you get . The correct derivative is , which you get by doing implicit differentiation after taking the natural log of both side. This is the same as , which happens to be the combination of the ""wrong"" derivatives you get when treating as the 2 different types of functions I specified earlier. Is there any real logic behind this or is it coincidence. Also, could this also be applied to other function hybrids like ?",x^x a^x x^a x^a xx^{x-1} x^x a^x x^xln(x) x^x(ln(x)+1) x^xln(x)+x^x x^x x^x,"['calculus', 'derivatives']"
4,Solving $\int_1^2 e^{1-\frac{1}{(x-1)^2}} + 2 + \frac{1}{\sqrt{1-\log(x-1)}}\:dx$ with a clean trick,Solving  with a clean trick,\int_1^2 e^{1-\frac{1}{(x-1)^2}} + 2 + \frac{1}{\sqrt{1-\log(x-1)}}\:dx,"A friend and I have been swapping difficult integrals for the holidays  to stump each other and he recently sent me this one that I haven't been able to figure out (mission accomplished, I guess :) ). I've tried a few substitutions of the form $$x-1 = f(t)$$ but if they cancel out one side, they won't simplify on the other because of the presence of both the exponential and the log. At best I could simplify the problem to $$2 + \int_0^1 e^{1-\frac{1}{x^2}} + \frac{1}{\sqrt{1-\log x}}\:dx$$ by shifting the integral over to the interval $[0,1]$ to see if I could spot any patterns. The integral on the right evaluates to $1$ , which is a surprisingly clean answer. Wolfram gives a complicated looking antiderivative but one of the rules of our little game was that we would invoke no special functions beyond the standard transcendentals and hyperbolics/trig. Even if this was the intended solution, I'm not sure how to simplify the bound at $1$ with the $\operatorname{erf}$ s I suspect he had some clean trick in mind since that was the theme of the game, but I'm stumped.","A friend and I have been swapping difficult integrals for the holidays  to stump each other and he recently sent me this one that I haven't been able to figure out (mission accomplished, I guess :) ). I've tried a few substitutions of the form but if they cancel out one side, they won't simplify on the other because of the presence of both the exponential and the log. At best I could simplify the problem to by shifting the integral over to the interval to see if I could spot any patterns. The integral on the right evaluates to , which is a surprisingly clean answer. Wolfram gives a complicated looking antiderivative but one of the rules of our little game was that we would invoke no special functions beyond the standard transcendentals and hyperbolics/trig. Even if this was the intended solution, I'm not sure how to simplify the bound at with the s I suspect he had some clean trick in mind since that was the theme of the game, but I'm stumped.","x-1 = f(t) 2 + \int_0^1 e^{1-\frac{1}{x^2}} + \frac{1}{\sqrt{1-\log x}}\:dx [0,1] 1 1 \operatorname{erf}","['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
5,Tricky Integral involving radicals,Tricky Integral involving radicals,,I am trying to evaluate the following definite integral (for $a>0$): $$I=\int_{0}^{1}{{{\left( {{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}-x \right)}^{2}}dx}$$ Neither the substitution $u={{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}$ nor $u={{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}-x$ are appropriate.I have also tried Feynman’s trick (differentiated with respect to a) but I didn’t get any success. Thanks in advance.,I am trying to evaluate the following definite integral (for $a>0$): $$I=\int_{0}^{1}{{{\left( {{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}-x \right)}^{2}}dx}$$ Neither the substitution $u={{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}$ nor $u={{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}-x$ are appropriate.I have also tried Feynman’s trick (differentiated with respect to a) but I didn’t get any success. Thanks in advance.,,['calculus']
6,Is this integral $\int_0^\infty\frac{\cos(a x+ 2b \arctan x)}{x^2+1}dx$ exactly zero when $b\in\mathbb{N}$?,Is this integral  exactly zero when ?,\int_0^\infty\frac{\cos(a x+ 2b \arctan x)}{x^2+1}dx b\in\mathbb{N},"I recently encountered this integral $$\int_0^\infty\frac{\cos(a x+ 2b \arctan x)}{x^2+1}dx$$ which suspiciously close to 0 for nonzero integer values of $b$, as indicated by numerical calculations. When $b$ is not an integer it is not 0. Based on my experiments I conjecture that $$\int_0^\infty\frac{\cos(a x+ 2b \arctan x)}{x^2+1}dx=0,~~~~b\in \mathbb{Z}\setminus \{0\}.$$ Question. Is the above conjecture true?","I recently encountered this integral $$\int_0^\infty\frac{\cos(a x+ 2b \arctan x)}{x^2+1}dx$$ which suspiciously close to 0 for nonzero integer values of $b$, as indicated by numerical calculations. When $b$ is not an integer it is not 0. Based on my experiments I conjecture that $$\int_0^\infty\frac{\cos(a x+ 2b \arctan x)}{x^2+1}dx=0,~~~~b\in \mathbb{Z}\setminus \{0\}.$$ Question. Is the above conjecture true?",,"['calculus', 'integration', 'definite-integrals', 'contour-integration', 'conjectures']"
7,"Why is the ""correct"" proof of the chain rule correct? What is actually happening here?","Why is the ""correct"" proof of the chain rule correct? What is actually happening here?",,"There is a correct and an incorrect proof going around when it comes to the Chain Rule (see below). The problem with the incorrect proof is that $g(x)-g(a)$ might be $0$ if $x\to a$ creating a division by zero. Question I can't get my head around why the correct proof solves the problem of the incorrect proof. Why can we just define a function $E$ and suddenly all our problems disappear? I just don't really get what actually happens in the correct proof. It just didn't ""click"" in my brain yet. Any help would be much appreciated. By the way; is my ""correct proof"" below indeed correct? Incorrect proof: $$\lim \limits_{x \to a}\frac{f(g(x))-f(g(a))}{x-a}=\lim \limits_{x \to a}\frac{f(g(x))-f(g(a))}{g(x)-g(a)}\times\frac{g(x)-g(a)}{x-a}=f'(g(x))g'(x)$$ Correct proof: We first define a function $E$ $$E(0)=0$$ $$E(g(x)-g(a))=\frac{f(g(x))-f(g(a))}{g(x)-g(a)}-f'(g(x))$$ In any case: $$f(g(x))-f(g(a))=(E(g(x)-g(a))+f'(g(x)))\times(g(x)-g(a))$$ Dividing by $x-a$ and taking the limit we get: $$\begin{align} \frac{d}{dx}f(g(x))&=\lim \limits_{x \to a}\frac{f(g(x))-f(g(a))}{x-a}\\ &=\lim \limits_{x \to a}(E(g(x)-g(a))+f'(g(x)))\times\frac{g(x)-g(a)}{x-a}\\&=f'(g(x)g'(x) \end{align}$$ EDIT: In other words: we basically state that when $g(x)=g(a)$: $$\frac{f(g(x))-f(g(a))}{g(x)-g(a)}-f'(g(x))=0$$ But why can we state that? As I understand it, this is true for the limit, but why are we allowed to also state it for the actual value?","There is a correct and an incorrect proof going around when it comes to the Chain Rule (see below). The problem with the incorrect proof is that $g(x)-g(a)$ might be $0$ if $x\to a$ creating a division by zero. Question I can't get my head around why the correct proof solves the problem of the incorrect proof. Why can we just define a function $E$ and suddenly all our problems disappear? I just don't really get what actually happens in the correct proof. It just didn't ""click"" in my brain yet. Any help would be much appreciated. By the way; is my ""correct proof"" below indeed correct? Incorrect proof: $$\lim \limits_{x \to a}\frac{f(g(x))-f(g(a))}{x-a}=\lim \limits_{x \to a}\frac{f(g(x))-f(g(a))}{g(x)-g(a)}\times\frac{g(x)-g(a)}{x-a}=f'(g(x))g'(x)$$ Correct proof: We first define a function $E$ $$E(0)=0$$ $$E(g(x)-g(a))=\frac{f(g(x))-f(g(a))}{g(x)-g(a)}-f'(g(x))$$ In any case: $$f(g(x))-f(g(a))=(E(g(x)-g(a))+f'(g(x)))\times(g(x)-g(a))$$ Dividing by $x-a$ and taking the limit we get: $$\begin{align} \frac{d}{dx}f(g(x))&=\lim \limits_{x \to a}\frac{f(g(x))-f(g(a))}{x-a}\\ &=\lim \limits_{x \to a}(E(g(x)-g(a))+f'(g(x)))\times\frac{g(x)-g(a)}{x-a}\\&=f'(g(x)g'(x) \end{align}$$ EDIT: In other words: we basically state that when $g(x)=g(a)$: $$\frac{f(g(x))-f(g(a))}{g(x)-g(a)}-f'(g(x))=0$$ But why can we state that? As I understand it, this is true for the limit, but why are we allowed to also state it for the actual value?",,"['calculus', 'derivatives', 'chain-rule']"
8,$\int (x^2+1)/(x^4+1)\ dx$ [closed],[closed],\int (x^2+1)/(x^4+1)\ dx,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have Divided the numerator and Denominator by $x^2$ to get $\dfrac{1+x^{-2}}{x^2+x^{-2}}$ then changed it into $(1+(x^{-2}))/[(x-x^{-1})^2 +2]$ then took $x-(1/x)$ as $u$ and Differentiated it with respect to $x$ to get $dx=du/(1+x^{-2})$  Finally I got this expression: $$ \int\frac{x^2+1}{x^4+1} \, dx = \int (u^2+2)^{-1} \, du $$ After this I need help!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have Divided the numerator and Denominator by $x^2$ to get $\dfrac{1+x^{-2}}{x^2+x^{-2}}$ then changed it into $(1+(x^{-2}))/[(x-x^{-1})^2 +2]$ then took $x-(1/x)$ as $u$ and Differentiated it with respect to $x$ to get $dx=du/(1+x^{-2})$  Finally I got this expression: $$ \int\frac{x^2+1}{x^4+1} \, dx = \int (u^2+2)^{-1} \, du $$ After this I need help!",,"['calculus', 'integration', 'polynomials', 'integration-by-parts']"
9,What does the d represent in the equation of a plane ($ax+by+cz+d=0$)?,What does the d represent in the equation of a plane ()?,ax+by+cz+d=0,"Look at the following problem: Consider the plane $\alpha$ defined by $x-2y+z+3=0$ and $A(0;0;2)$. Write an equation of the plane that is parallel to $\alpha$ and goes   through A. My book states the solution is: The direction vector of $\alpha$ is $\vec{n}(1;-2;1)$ so the equation   of the plane is $x-2y+z+d=0$. A is in the plane, so: $$0 - 2 \cdot 0 + 2 +d = 0 \Leftrightarrow d = -2$$ So the equation is: $x-2y+z-2=0$ What was the reasoning behind this? What does $d$ represent?","Look at the following problem: Consider the plane $\alpha$ defined by $x-2y+z+3=0$ and $A(0;0;2)$. Write an equation of the plane that is parallel to $\alpha$ and goes   through A. My book states the solution is: The direction vector of $\alpha$ is $\vec{n}(1;-2;1)$ so the equation   of the plane is $x-2y+z+d=0$. A is in the plane, so: $$0 - 2 \cdot 0 + 2 +d = 0 \Leftrightarrow d = -2$$ So the equation is: $x-2y+z-2=0$ What was the reasoning behind this? What does $d$ represent?",,"['calculus', 'geometry']"
10,Discuss the monotonicity of the following function without using differentiation.,Discuss the monotonicity of the following function without using differentiation.,,Can I discuss the monotonicity of the following function without using differentiation? $$f(x) = x + \frac{9}{x}$$ Could anyone help me?,Can I discuss the monotonicity of the following function without using differentiation? $$f(x) = x + \frac{9}{x}$$ Could anyone help me?,,['calculus']
11,Why does the antiderivative of $\frac{1}{x}$ have to be $0$ at precisely $x=1$? (when $C = 0$),Why does the antiderivative of  have to be  at precisely ? (when ),\frac{1}{x} 0 x=1 C = 0,"Why does $\log{x}$ represent the area below the graph of $f(x)=\frac{1}{x}$ from $1$ to $x$? What's so special about $1$ in this case? Of course I understand that $\log{1}=0$ and I also understand that you cannot start at $x=0$ because $f(0)$ is not defined. Still I can't get my head around of why it has to be $1$. Also, this further implies that part of the antiderivative of $f(x)=\frac{1}{x}$ has to be negative (as part of the function of $\log{x}$ is negative). But why is this necessary? The background of this question is that my calculus-book (Calculus, a complete course) starts with noticing that $f(x)=\frac{1}{x}$ is not an antiderivative of a polynomial function and then attempts to define a antiderivative which ends up being $\log{x}$. It does this all before even addressing the fundamental theorem of calculus or techniques of integration. They then simply define $log(1)$ to be $0$ without even knowing what that function really is yet. So I am stuck with kind of a circular reasoning where log(1)=0 because we defined it that way, but I don't understand why we define it that way. When calculating areas under graphs by taking the limit of a sum (instead of by integration), you would start at $x=0$ right? So in short: Why does the antiderivative of $\frac{1}{x}$ have to be $0$ at precisely $x=1$? (when $C = 0$). Why do we define it that way? I am looking for some kind of deeper understanding; something still didn't ""click"". So some real background on what's going on here would be much appreciated. Thanks!","Why does $\log{x}$ represent the area below the graph of $f(x)=\frac{1}{x}$ from $1$ to $x$? What's so special about $1$ in this case? Of course I understand that $\log{1}=0$ and I also understand that you cannot start at $x=0$ because $f(0)$ is not defined. Still I can't get my head around of why it has to be $1$. Also, this further implies that part of the antiderivative of $f(x)=\frac{1}{x}$ has to be negative (as part of the function of $\log{x}$ is negative). But why is this necessary? The background of this question is that my calculus-book (Calculus, a complete course) starts with noticing that $f(x)=\frac{1}{x}$ is not an antiderivative of a polynomial function and then attempts to define a antiderivative which ends up being $\log{x}$. It does this all before even addressing the fundamental theorem of calculus or techniques of integration. They then simply define $log(1)$ to be $0$ without even knowing what that function really is yet. So I am stuck with kind of a circular reasoning where log(1)=0 because we defined it that way, but I don't understand why we define it that way. When calculating areas under graphs by taking the limit of a sum (instead of by integration), you would start at $x=0$ right? So in short: Why does the antiderivative of $\frac{1}{x}$ have to be $0$ at precisely $x=1$? (when $C = 0$). Why do we define it that way? I am looking for some kind of deeper understanding; something still didn't ""click"". So some real background on what's going on here would be much appreciated. Thanks!",,"['calculus', 'integration']"
12,How to show that $\int_{0}^{\infty}{(2x)^4\over (1-x^2+x^4)^4}\mathrm dx=\int_{0}^{\infty}{(2x)^4\over (1-x^2+x^4)^3}\mathrm dx=3\pi$,How to show that,\int_{0}^{\infty}{(2x)^4\over (1-x^2+x^4)^4}\mathrm dx=\int_{0}^{\infty}{(2x)^4\over (1-x^2+x^4)^3}\mathrm dx=3\pi,"Consider   $$I=\int_{0}^{\infty}{(2x)^4\over (1-x^2+x^4)^4}\mathrm dx \qquad J=\int_{0}^{\infty}{(2x)^4\over (1-x^2+x^4)^3}\mathrm dx$$   I want to show that $I=3\pi$ and that $I=J$. First, we noticed that $x^4+x^2+1=(x^2-x+1)(x^2+x+1)$ So it gives us an idea to try and factorise $x^4-x^2+1$ but cannot find any factors. Integrate $I$ (We try some substitutions to see where it will get us to) $x=\sqrt{u}$ then $dx={2\over \sqrt{u}}du$ $$I=16\cdot{1\over 2}\int_{0}^{\infty}{u^{3/2}\over (1-u+u^2)^4}\mathrm du$$ $u=\tan(y)$ then $du=\sec^2(y)dy$ $$I=8\int_{0}^{\pi/2}{\tan^{3/2}(y)\over (1-\tan(y)+\tan^2(y))^4}{\mathrm dy\over \cos^2(y)}$$ then simplified down to $$I=128\int_{0}^{\pi/2}{\cos^6(y)\tan^{3/2}(y)\over (2-\sin(2y))^4}\mathrm dy$$ we further simplified down to $$I={128\over 2^{3/2}}\int_{0}^{\pi/2}{\cos^3(y)\sin^{3/2}(2y)\over (2-\sin(2y))^4}\mathrm dy$$ Not so sure what is the next step.","Consider   $$I=\int_{0}^{\infty}{(2x)^4\over (1-x^2+x^4)^4}\mathrm dx \qquad J=\int_{0}^{\infty}{(2x)^4\over (1-x^2+x^4)^3}\mathrm dx$$   I want to show that $I=3\pi$ and that $I=J$. First, we noticed that $x^4+x^2+1=(x^2-x+1)(x^2+x+1)$ So it gives us an idea to try and factorise $x^4-x^2+1$ but cannot find any factors. Integrate $I$ (We try some substitutions to see where it will get us to) $x=\sqrt{u}$ then $dx={2\over \sqrt{u}}du$ $$I=16\cdot{1\over 2}\int_{0}^{\infty}{u^{3/2}\over (1-u+u^2)^4}\mathrm du$$ $u=\tan(y)$ then $du=\sec^2(y)dy$ $$I=8\int_{0}^{\pi/2}{\tan^{3/2}(y)\over (1-\tan(y)+\tan^2(y))^4}{\mathrm dy\over \cos^2(y)}$$ then simplified down to $$I=128\int_{0}^{\pi/2}{\cos^6(y)\tan^{3/2}(y)\over (2-\sin(2y))^4}\mathrm dy$$ we further simplified down to $$I={128\over 2^{3/2}}\int_{0}^{\pi/2}{\cos^3(y)\sin^{3/2}(2y)\over (2-\sin(2y))^4}\mathrm dy$$ Not so sure what is the next step.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
13,Evaluate $\lim_{n \to \infty} \int_{1}^{2}\frac{\sin(nx)}{x}dx$,Evaluate,\lim_{n \to \infty} \int_{1}^{2}\frac{\sin(nx)}{x}dx,"I have to compute $$\lim_{n \to \infty} \int_{1}^{2}\frac{\sin(nx)}{x}dx$$ I have tried to tackle it in different ways but I'm getting nowhere. In particular, I used substitution to obtain $$\lim_{n \to \infty} \int_{1}^{2}\frac{\sin(nx)}{x}dx = \lim_{n \to \infty} \int_{n}^{2n}\frac{\sin(u)}{u}du$$ But from here I'm not sure about what to do. I've found information about $\int_0^\infty\frac{\sin(nx)}{x}dx$, but I don't see if and how I could relate my integral with that one. Any hints? Thanks","I have to compute $$\lim_{n \to \infty} \int_{1}^{2}\frac{\sin(nx)}{x}dx$$ I have tried to tackle it in different ways but I'm getting nowhere. In particular, I used substitution to obtain $$\lim_{n \to \infty} \int_{1}^{2}\frac{\sin(nx)}{x}dx = \lim_{n \to \infty} \int_{n}^{2n}\frac{\sin(u)}{u}du$$ But from here I'm not sure about what to do. I've found information about $\int_0^\infty\frac{\sin(nx)}{x}dx$, but I don't see if and how I could relate my integral with that one. Any hints? Thanks",,"['calculus', 'limits', 'definite-integrals']"
14,Is there a formula for the area under $\tanh(x)$?,Is there a formula for the area under ?,\tanh(x),I understand trigonometry but I've never used hyperbolic functions before. Is there a formula for the area under $\tanh(x)$? I've looked on Wikipedia and Wolfram but they don't say if there's a formula or not. I tried to work it out myself and I got this far: $\tanh(x) = {\sinh(x)\over\cosh(x)} = {1-e^{-2x}\over 1+e^{-2x}} = {e^{2x}-1\over e^{2x}+1} = {e^{2x}+1-2\over e^{2x}+1} = 1-{2\over e^{2x}+1}$ Now I'm stuck. I don't know if I'm on the right track or not.,I understand trigonometry but I've never used hyperbolic functions before. Is there a formula for the area under $\tanh(x)$? I've looked on Wikipedia and Wolfram but they don't say if there's a formula or not. I tried to work it out myself and I got this far: $\tanh(x) = {\sinh(x)\over\cosh(x)} = {1-e^{-2x}\over 1+e^{-2x}} = {e^{2x}-1\over e^{2x}+1} = {e^{2x}+1-2\over e^{2x}+1} = 1-{2\over e^{2x}+1}$ Now I'm stuck. I don't know if I'm on the right track or not.,,"['calculus', 'area', 'hyperbolic-functions']"
15,How do you solve this limit involving definite integration?,How do you solve this limit involving definite integration?,,"$$ \lim \limits_{r \to \infty} \frac {r^C \int_0^{\frac{\pi}{2}} x^r \sin(x)\, dx}{\int_0^{\frac{\pi}{2}} x^r \cos(x)\, dx} = L$$ Find the value of $\pi L - C$, given that $C\in\mathbb{R}$ and $L>0$. My approach: I tried to apply integration by parts to both the numerator and denominator to get a recurring relation, hoping to cancel something off, but to no avail. I'm not getting any other method to solve it, so any help will be appreciated.","$$ \lim \limits_{r \to \infty} \frac {r^C \int_0^{\frac{\pi}{2}} x^r \sin(x)\, dx}{\int_0^{\frac{\pi}{2}} x^r \cos(x)\, dx} = L$$ Find the value of $\pi L - C$, given that $C\in\mathbb{R}$ and $L>0$. My approach: I tried to apply integration by parts to both the numerator and denominator to get a recurring relation, hoping to cancel something off, but to no avail. I'm not getting any other method to solve it, so any help will be appreciated.",,"['calculus', 'integration', 'limits', 'definite-integrals']"
16,"Calculus Paradox. I mean, what's wrong with what I think? [closed]","Calculus Paradox. I mean, what's wrong with what I think? [closed]",,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Is not calculus based on the paradox that the closest point to a point A is a distinct point B which is the point A itself? For example, if we consider the limit, $$ \lim_{x\to2} \frac{x^2-4}{x-2} $$ It's evaluated by first cancelling out the $(x-2)$ common factor and then we substitute the value $2$ in the function of $x$. It's like, at first, we're considering that $x$ is nearly equal to $2$ but not $2$, but then we substitute the value of $2$. So, what's happening here? Similarly, in derivatives we're considering the tangent to a curve which intersects the curve at one distinct point. That's how we get the exact slope. But, at times, we're considering a point $A$ which is close to point $B$ (and point $B$ and point $A$ are different). But, we know that the concept of derivatives is legit by experimental evidence. So, how does it all actually work?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Is not calculus based on the paradox that the closest point to a point A is a distinct point B which is the point A itself? For example, if we consider the limit, $$ \lim_{x\to2} \frac{x^2-4}{x-2} $$ It's evaluated by first cancelling out the $(x-2)$ common factor and then we substitute the value $2$ in the function of $x$. It's like, at first, we're considering that $x$ is nearly equal to $2$ but not $2$, but then we substitute the value of $2$. So, what's happening here? Similarly, in derivatives we're considering the tangent to a curve which intersects the curve at one distinct point. That's how we get the exact slope. But, at times, we're considering a point $A$ which is close to point $B$ (and point $B$ and point $A$ are different). But, we know that the concept of derivatives is legit by experimental evidence. So, how does it all actually work?",,"['calculus', 'limits', 'derivatives']"
17,Integral involving a trig. term,Integral involving a trig. term,,"I came across the following integral. $$ \int\frac{dx}{1+\sin x} $$ I have no idea how to solve it! I went for the obvious substitution of $u=1+\sin x$, but then you get an annoying $\cos x$ kicking around. I tried to eliminate this by writing $\cos x=\sqrt{1-(u-1)^2}$, but I couldn't get this idea to work.","I came across the following integral. $$ \int\frac{dx}{1+\sin x} $$ I have no idea how to solve it! I went for the obvious substitution of $u=1+\sin x$, but then you get an annoying $\cos x$ kicking around. I tried to eliminate this by writing $\cos x=\sqrt{1-(u-1)^2}$, but I couldn't get this idea to work.",,"['calculus', 'integration']"
18,"Using the Mean Value Theorem, prove that $|\sin{a} - \sin{b}| \leq |a - b|$ $\forall a, b \in \mathbb{R}$","Using the Mean Value Theorem, prove that","|\sin{a} - \sin{b}| \leq |a - b| \forall a, b \in \mathbb{R}","Using the Mean Value Theorem, prove that $|\sin{a} - \sin{b}| \leq |a - b|$ $\forall a, b \in \mathbb{R}$. I'm working towards figuring out an approach for finding that $|\sin{a} - \sin{b}| \leq |a - b|$ $\forall a, b \in \mathbb{R}$, but I've not yet included an application of the MVT, and I believe that my approach has some redundancy (or at least isn't that elegant). Furthermore, I'm not even so certain what I've written is at all very helpful in proving this conclusion. $$ \begin{align*} \\ \text{Assume } \forall \sin{x} \implies \sin{x} = \sin{(x \bmod 2\pi)} \\ \text{case: }     a &> b \wedge a \leq \pi \wedge b \leq \pi \implies \\  1 &\geq \sin{a} \geq 0 \wedge  1 \geq \sin{b} \geq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 1 \\ \\ \text{case: }     a &> b \wedge a \geq \pi \wedge b \leq \pi \implies \\ -1 &\leq \sin{a} \leq 0 \wedge  1 \geq \sin{b} \geq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 2 \\ \\ \text{case: }     a &> b \wedge a \geq \pi \wedge b \geq \pi \implies \\ -1 &\leq \sin{a} \leq 0 \wedge -1 \leq \sin{b} \leq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 1 \\ \\ \text{case: }      a &= b \implies \\   0 &= |\sin{a} - \sin{b}| = |a - b| \\ \\ \text{case: }     a &< b \wedge a \leq \pi \wedge b \leq \pi \implies \\  1 &\geq \sin{a} \geq 0 \wedge  1 \geq \sin{b} \geq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 1 \\ \\ \text{case: }     a &< b \wedge a \leq \pi \wedge b \geq \pi \implies \\  1 &\geq \sin{a} \geq 0 \wedge -1 \leq \sin{b} \leq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 2 \\ \\ \text{case: } a &< b \wedge a \geq \pi \wedge b \geq \pi \implies \\ -1 &\leq \sin{a} \leq 0 \wedge -1 \leq \sin{b} \leq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 1 \end{align*} $$ I find it's fairly intuitive for $|a - b| \geq 2$ that $|\sin{a} - \sin{b}| \leq 2$, considering $\sin{x} \leq 1 \text{ } \forall x \in \mathbb{R}$. But grasping and considering cases for $|a - b| < 2$ seems a little less intuitive, as it is perhaps conceivable (but not necessarily true) that $|\sin{a} - \sin{b}| > |a - b|$ for some values where $|a - b| < 2$. Insight? Edit: I've refined my proof to the following structure: The Mean Value Theorem states: a function $f$ which is continuous on the closed interval $[a, b]$ $^{\textbf{(1)}}$ and differentiable on the open interval $(a, b)$ $^{\textbf{(2)}}$ has at least one value $c: a < c < b$ where $f'(c) = \dfrac{f(b) - f(a)}{b - a}$. Set $f(x) = \sin{x} \implies f(x)$ is continuous and differentiable $\forall x \in \mathbb{R}$ and all sub-intervals $^{\textbf{(1, 2)}}$ $ \therefore$ when $\exists a, b: b < c < a \implies \exists f'(c) = \dfrac{f(a) - f(b)}{a - b} \implies \cos{c} = \dfrac{\sin{a} - \sin{b}}{a - b}$. Take the absolute value of both sides of this equality to find $\dfrac{|\sin{a} - \sin{b}|}{|a - b|} = |\cos{c}|$, and since $\dfrac{|\sin{a} - \sin{b}|}{|a - b|} = \dfrac{|\sin{b} - \sin{a}|}{|b - a|}$, this holds true $\forall a, b \in \mathbb{R}$. Since $|\cos{x}| \leq 1 \text{ } \forall x \in \mathbb{R} \implies |\cos{c}| \leq 1 \implies \dfrac{|\sin{a} - \sin{b}|}{|a - b|} \leq 1.$ Multiplying across the inequality by $|a - b|$ finds the result: $|\sin{a} - \sin{b}|  \leq |a - b|$.","Using the Mean Value Theorem, prove that $|\sin{a} - \sin{b}| \leq |a - b|$ $\forall a, b \in \mathbb{R}$. I'm working towards figuring out an approach for finding that $|\sin{a} - \sin{b}| \leq |a - b|$ $\forall a, b \in \mathbb{R}$, but I've not yet included an application of the MVT, and I believe that my approach has some redundancy (or at least isn't that elegant). Furthermore, I'm not even so certain what I've written is at all very helpful in proving this conclusion. $$ \begin{align*} \\ \text{Assume } \forall \sin{x} \implies \sin{x} = \sin{(x \bmod 2\pi)} \\ \text{case: }     a &> b \wedge a \leq \pi \wedge b \leq \pi \implies \\  1 &\geq \sin{a} \geq 0 \wedge  1 \geq \sin{b} \geq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 1 \\ \\ \text{case: }     a &> b \wedge a \geq \pi \wedge b \leq \pi \implies \\ -1 &\leq \sin{a} \leq 0 \wedge  1 \geq \sin{b} \geq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 2 \\ \\ \text{case: }     a &> b \wedge a \geq \pi \wedge b \geq \pi \implies \\ -1 &\leq \sin{a} \leq 0 \wedge -1 \leq \sin{b} \leq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 1 \\ \\ \text{case: }      a &= b \implies \\   0 &= |\sin{a} - \sin{b}| = |a - b| \\ \\ \text{case: }     a &< b \wedge a \leq \pi \wedge b \leq \pi \implies \\  1 &\geq \sin{a} \geq 0 \wedge  1 \geq \sin{b} \geq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 1 \\ \\ \text{case: }     a &< b \wedge a \leq \pi \wedge b \geq \pi \implies \\  1 &\geq \sin{a} \geq 0 \wedge -1 \leq \sin{b} \leq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 2 \\ \\ \text{case: } a &< b \wedge a \geq \pi \wedge b \geq \pi \implies \\ -1 &\leq \sin{a} \leq 0 \wedge -1 \leq \sin{b} \leq 0 \implies \\  0 &\leq |\sin{a} - \sin{b}| \leq 1 \end{align*} $$ I find it's fairly intuitive for $|a - b| \geq 2$ that $|\sin{a} - \sin{b}| \leq 2$, considering $\sin{x} \leq 1 \text{ } \forall x \in \mathbb{R}$. But grasping and considering cases for $|a - b| < 2$ seems a little less intuitive, as it is perhaps conceivable (but not necessarily true) that $|\sin{a} - \sin{b}| > |a - b|$ for some values where $|a - b| < 2$. Insight? Edit: I've refined my proof to the following structure: The Mean Value Theorem states: a function $f$ which is continuous on the closed interval $[a, b]$ $^{\textbf{(1)}}$ and differentiable on the open interval $(a, b)$ $^{\textbf{(2)}}$ has at least one value $c: a < c < b$ where $f'(c) = \dfrac{f(b) - f(a)}{b - a}$. Set $f(x) = \sin{x} \implies f(x)$ is continuous and differentiable $\forall x \in \mathbb{R}$ and all sub-intervals $^{\textbf{(1, 2)}}$ $ \therefore$ when $\exists a, b: b < c < a \implies \exists f'(c) = \dfrac{f(a) - f(b)}{a - b} \implies \cos{c} = \dfrac{\sin{a} - \sin{b}}{a - b}$. Take the absolute value of both sides of this equality to find $\dfrac{|\sin{a} - \sin{b}|}{|a - b|} = |\cos{c}|$, and since $\dfrac{|\sin{a} - \sin{b}|}{|a - b|} = \dfrac{|\sin{b} - \sin{a}|}{|b - a|}$, this holds true $\forall a, b \in \mathbb{R}$. Since $|\cos{x}| \leq 1 \text{ } \forall x \in \mathbb{R} \implies |\cos{c}| \leq 1 \implies \dfrac{|\sin{a} - \sin{b}|}{|a - b|} \leq 1.$ Multiplying across the inequality by $|a - b|$ finds the result: $|\sin{a} - \sin{b}|  \leq |a - b|$.",,"['calculus', 'trigonometry', 'absolute-value']"
19,Asymptotics of $\int_{0}^{+\infty}\!\!\frac{dx}{\sinh^2(\epsilon \sqrt{x^2+1}) } $ for $\epsilon$ near $0$,Asymptotics of  for  near,\int_{0}^{+\infty}\!\!\frac{dx}{\sinh^2(\epsilon \sqrt{x^2+1}) }  \epsilon 0,"How to find an asymptotic expansion, for $\epsilon$ near $0$, of the following integral $$ I(\epsilon):=\int_{0}^{+\infty}\frac 1{\sinh^2 (\epsilon \sqrt{x^2+1}) } {\rm d}x. $$ As $\epsilon \rightarrow 0$, I have easily obtained $$ I(\epsilon) \sim \frac{\pi}{2\:\epsilon^2}.  $$ Some clear steps leading to an extended expansion would be appreciated.","How to find an asymptotic expansion, for $\epsilon$ near $0$, of the following integral $$ I(\epsilon):=\int_{0}^{+\infty}\frac 1{\sinh^2 (\epsilon \sqrt{x^2+1}) } {\rm d}x. $$ As $\epsilon \rightarrow 0$, I have easily obtained $$ I(\epsilon) \sim \frac{\pi}{2\:\epsilon^2}.  $$ Some clear steps leading to an extended expansion would be appreciated.",,"['calculus', 'integration', 'definite-integrals', 'asymptotics', 'hyperbolic-functions']"
20,How to evaluate $\lim\limits_{x\to 0} \frac{\sin x - x + x^3/6}{x^3}$,How to evaluate,\lim\limits_{x\to 0} \frac{\sin x - x + x^3/6}{x^3},I'm unsure as to how to evaluate: $$\lim\limits_{x\to 0} \frac{\sin x - x + \frac{x^3}{6}}{x^3}$$ The $\lim\limits_{x\to 0}$ of both the numerator and denominator equal $0$. Taking the derivative of both ends of the fraction we get: $$\lim\limits_{x\to 0} \frac{x^2 + 2\cos x -2}{6x^2}$$ But I don't know how to evaluate this? Many thanks for any help.,I'm unsure as to how to evaluate: $$\lim\limits_{x\to 0} \frac{\sin x - x + \frac{x^3}{6}}{x^3}$$ The $\lim\limits_{x\to 0}$ of both the numerator and denominator equal $0$. Taking the derivative of both ends of the fraction we get: $$\lim\limits_{x\to 0} \frac{x^2 + 2\cos x -2}{6x^2}$$ But I don't know how to evaluate this? Many thanks for any help.,,"['calculus', 'limits', 'trigonometry', 'derivatives']"
21,Infinitely Many Circles in an Equilateral Triangle,Infinitely Many Circles in an Equilateral Triangle,,"In the figure there are infinitely many circles approaching the vertices of an equilateral triangle, each circle touching other circles and sides of the triangle. If the triangle has sides of length 1, find the total area occupied by the circles. I need to find the total area of the circles. I know this is going to have something to do with summation as a value approaches infinity, but I'm not entirely sure how to approach the problem. Here's what I have so far: I know that the radius of the central inscribed circle is $ \frac{\sqrt{3}}{6} $ . As such, the area of the first circle is $$ A = \pi\left(\frac{\sqrt{3}}{6}\right)^2. $$ Because there are three ""branches"" of infinite circles, I'm assuming that the answer will look something like: $$ A = \pi\left(\frac{\sqrt{3}}{6}\right)^2 + 3 \sum_{1}^{\infty}\text{something}.$$","In the figure there are infinitely many circles approaching the vertices of an equilateral triangle, each circle touching other circles and sides of the triangle. If the triangle has sides of length 1, find the total area occupied by the circles. I need to find the total area of the circles. I know this is going to have something to do with summation as a value approaches infinity, but I'm not entirely sure how to approach the problem. Here's what I have so far: I know that the radius of the central inscribed circle is . As such, the area of the first circle is Because there are three ""branches"" of infinite circles, I'm assuming that the answer will look something like:", \frac{\sqrt{3}}{6}   A = \pi\left(\frac{\sqrt{3}}{6}\right)^2.   A = \pi\left(\frac{\sqrt{3}}{6}\right)^2 + 3 \sum_{1}^{\infty}\text{something}.,"['calculus', 'sequences-and-series', 'geometry', 'summation']"
22,Division isn't associative,Division isn't associative,,"Consider the following fraction: $$\frac{\frac{\frac{a}{b}}{c}}{a}$$ How to explain that: $$\frac{\frac{\frac{a}{b}}{c}}{a} \ne \frac{\frac{a^2}{b}}{c}$$ Obviously, you get different results. But a-priori, one might think that $a:b:c:d$ is associative. Is there any good way proving/explaining the difference, other then showing the different results? Thanks.","Consider the following fraction: $$\frac{\frac{\frac{a}{b}}{c}}{a}$$ How to explain that: $$\frac{\frac{\frac{a}{b}}{c}}{a} \ne \frac{\frac{a^2}{b}}{c}$$ Obviously, you get different results. But a-priori, one might think that $a:b:c:d$ is associative. Is there any good way proving/explaining the difference, other then showing the different results? Thanks.",,"['calculus', 'algebra-precalculus']"
23,Jordan measure zero discontinuities a necessary condition for integrability,Jordan measure zero discontinuities a necessary condition for integrability,,"The following theorem is well known: Theorem: A function $f: [a,b] \to \mathbb R$ is Riemann integrable if and only if its set of discontinuities has Lebesgue measure zero. Now if we change Riemann integrable to Lebesgue integrable, this theorem is false since $\chi_{\mathbb Q}$ is Lebesgue integrable and has full measure discontinuities. We also know that if we change Lebesgue measure to Jordan measure, then having Jordan zero-measure discontinuities is sufficient for integrability. My question: Does every Riemann integrable function necessarily have zero Jordan measure discontinuities?","The following theorem is well known: Theorem: A function $f: [a,b] \to \mathbb R$ is Riemann integrable if and only if its set of discontinuities has Lebesgue measure zero. Now if we change Riemann integrable to Lebesgue integrable, this theorem is false since $\chi_{\mathbb Q}$ is Lebesgue integrable and has full measure discontinuities. We also know that if we change Lebesgue measure to Jordan measure, then having Jordan zero-measure discontinuities is sufficient for integrability. My question: Does every Riemann integrable function necessarily have zero Jordan measure discontinuities?",,"['calculus', 'integration', 'measure-theory']"
24,Are discontinuous functions integrable? And integral of every continuous function continuous?,Are discontinuous functions integrable? And integral of every continuous function continuous?,,According to me answer of second part is yes as integration simply means area under curve.,According to me answer of second part is yes as integration simply means area under curve.,,"['calculus', 'integration']"
25,How to find the value of $\sqrt{1\sqrt{2\sqrt{3 \cdots}}}$?,How to find the value of ?,\sqrt{1\sqrt{2\sqrt{3 \cdots}}},"I thought up this question recently, and I think I've figured out the partial sum: $$ S_n := \left(n\prod_{k=2}^{n-1} k^{2^{n-k}}\right)^{2^{-k}}. $$ But I don't even quite know if I'm on the right track. If I am, how do I find the limit of the above equation, and if not, how can I find it another way? Thanks.","I thought up this question recently, and I think I've figured out the partial sum: $$ S_n := \left(n\prod_{k=2}^{n-1} k^{2^{n-k}}\right)^{2^{-k}}. $$ But I don't even quite know if I'm on the right track. If I am, how do I find the limit of the above equation, and if not, how can I find it another way? Thanks.",,"['calculus', 'products']"
26,What exactly does $\frac{dx}{dy}$ mean?,What exactly does  mean?,\frac{dx}{dy},"I asked 3 professors at my university and none gave me a clear cut answer, but instead merely told me qualities of this notation. Here is what I understand so far from what they told me: 1)Treat the top variable as as variable when finding the derivative 2)Treat the bottom variable as a constant when finding derivative 3)It it said ""Find x with respect to y"", but what exactly does that mean? What does it mean for something to be in respect to something else? It seems like $\frac{dx}{dy}$ notation changes according to values in the problem. For instance, If $y = x^3 + 2x$ and $\frac{dx}{dt} = 5$, find $\frac{dy}{dt}$ when $x=2$. Why do the values of $\frac{dx}{dy}$ change in this problem and how do I solve this?","I asked 3 professors at my university and none gave me a clear cut answer, but instead merely told me qualities of this notation. Here is what I understand so far from what they told me: 1)Treat the top variable as as variable when finding the derivative 2)Treat the bottom variable as a constant when finding derivative 3)It it said ""Find x with respect to y"", but what exactly does that mean? What does it mean for something to be in respect to something else? It seems like $\frac{dx}{dy}$ notation changes according to values in the problem. For instance, If $y = x^3 + 2x$ and $\frac{dx}{dt} = 5$, find $\frac{dy}{dt}$ when $x=2$. Why do the values of $\frac{dx}{dy}$ change in this problem and how do I solve this?",,['calculus']
27,Limit problem in book: $\lim_{n\to\infty}\frac{1+\sqrt{2}+...+\sqrt{n}}{n^{3/2}}$,Limit problem in book:,\lim_{n\to\infty}\frac{1+\sqrt{2}+...+\sqrt{n}}{n^{3/2}},My book states that the $$ \lim_{n\to\infty}\frac{1+\sqrt{2}+...+\sqrt{n}}{n^{3/2}}= \lim_{n\to\infty}\frac{1}{n}\left[\frac{1+\sqrt{2}+...+\sqrt{n}}{n^{1/2}}\right]=\frac{2}{3}. $$ But I just don't see it. I was thinking that if the $$\lim_{n\to\infty}\frac{1}{n}=0$$ then the whole thing would equal 0 but I don't see how to get $\frac{2}{3}$. Thanks,My book states that the $$ \lim_{n\to\infty}\frac{1+\sqrt{2}+...+\sqrt{n}}{n^{3/2}}= \lim_{n\to\infty}\frac{1}{n}\left[\frac{1+\sqrt{2}+...+\sqrt{n}}{n^{1/2}}\right]=\frac{2}{3}. $$ But I just don't see it. I was thinking that if the $$\lim_{n\to\infty}\frac{1}{n}=0$$ then the whole thing would equal 0 but I don't see how to get $\frac{2}{3}$. Thanks,,"['calculus', 'limits']"
28,Function is equal to its own derivative [duplicate],Function is equal to its own derivative [duplicate],,"This question already has answers here : Prove that $C e^x$ is the only set of functions for which $f(x) = f'(x)$ (9 answers) Closed 10 years ago . We all know that derivative of $e^x$ is $e^x$. Is exponential function only function that has such property? If yes how to prove that there are no other functions. If no, what are other functions? Help me please","This question already has answers here : Prove that $C e^x$ is the only set of functions for which $f(x) = f'(x)$ (9 answers) Closed 10 years ago . We all know that derivative of $e^x$ is $e^x$. Is exponential function only function that has such property? If yes how to prove that there are no other functions. If no, what are other functions? Help me please",,"['calculus', 'derivatives']"
29,Proof of Raabe's test,Proof of Raabe's test,,"Let $\sum a_n$ be a series of non-negative terms and let $$L = \lim_{n\to\infty}n\left(1-\frac{a_{n+1}}{a_n}\right)$$ Prove that the series converges (resp. diverges) if $L > 1$ (resp. $L<1$). I've tried, for example, that when $L<1$, $$n\left(1-\frac{a_{n+1}}{a_n}-L\right)= \frac{n(a_n-a_{n+1}-La_n)}{a_n}\ge\frac{n(a_n-a_{n+1}-a_n)}{a_n}=\frac{-na_{n+1}}{a_n}$$ and then using epsilons and the sort, but I can't get anywhere. Any tips? P.S. using Kummer's test doesn't count","Let $\sum a_n$ be a series of non-negative terms and let $$L = \lim_{n\to\infty}n\left(1-\frac{a_{n+1}}{a_n}\right)$$ Prove that the series converges (resp. diverges) if $L > 1$ (resp. $L<1$). I've tried, for example, that when $L<1$, $$n\left(1-\frac{a_{n+1}}{a_n}-L\right)= \frac{n(a_n-a_{n+1}-La_n)}{a_n}\ge\frac{n(a_n-a_{n+1}-a_n)}{a_n}=\frac{-na_{n+1}}{a_n}$$ and then using epsilons and the sort, but I can't get anywhere. Any tips? P.S. using Kummer's test doesn't count",,"['calculus', 'sequences-and-series', 'limits']"
30,Evaluating the Integral: $\int_0^\infty \frac{( \frac{1}{2} - \cos x )}{x} dx$,Evaluating the Integral:,\int_0^\infty \frac{( \frac{1}{2} - \cos x )}{x} dx,"Evaluating the Integral: $\int_0^\infty\left[\frac{1}{2} - \cos\left(x\right)\right]\,{\rm dx \over x}$ I came upon this limit: $\lim_{x\rightarrow\infty} -Ci(x) + Ci(1/x) +\ln(x)$, is it $\gamma$ ? Here $ Ci(x) = \gamma + \ln x + \int_0^x \frac{\cos t -1}{t} dt $ is the cosine integral and $\gamma$ is the Euler constant. The Limit and the Integral appear to be equal.","Evaluating the Integral: $\int_0^\infty\left[\frac{1}{2} - \cos\left(x\right)\right]\,{\rm dx \over x}$ I came upon this limit: $\lim_{x\rightarrow\infty} -Ci(x) + Ci(1/x) +\ln(x)$, is it $\gamma$ ? Here $ Ci(x) = \gamma + \ln x + \int_0^x \frac{\cos t -1}{t} dt $ is the cosine integral and $\gamma$ is the Euler constant. The Limit and the Integral appear to be equal.",,['calculus']
31,Show Continuity Using Epsilon Delta Definition [closed],Show Continuity Using Epsilon Delta Definition [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Using Epsilon Delta Definition show that $f(x)=x^2$ is continuous on all R, i.e. that $$\lim_{x\to a} f(x)=f(a)$$ for each $a$ that is an element of the reals. b) do the same for $$   g(x)=\begin{cases} x^2,\quad  x \geq 0,\\                           2x, \quad  x < 0. \end{cases}$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Using Epsilon Delta Definition show that $f(x)=x^2$ is continuous on all R, i.e. that $$\lim_{x\to a} f(x)=f(a)$$ for each $a$ that is an element of the reals. b) do the same for $$   g(x)=\begin{cases} x^2,\quad  x \geq 0,\\                           2x, \quad  x < 0. \end{cases}$$",,"['calculus', 'continuity']"
32,solving the integral $ \int_0^\infty \dfrac{\sin xt}{x(x^2+1)} dx $,solving the integral, \int_0^\infty \dfrac{\sin xt}{x(x^2+1)} dx ,How to solve the following integral by differential equations techniques? $$ \int_0^\infty \dfrac{\sin xt}{x(x^2+1)} dx $$,How to solve the following integral by differential equations techniques? $$ \int_0^\infty \dfrac{\sin xt}{x(x^2+1)} dx $$,,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
33,Does $\int_{0}^{\infty} \frac{\sin (\tan x)}{x} \ dx $ converge?,Does  converge?,\int_{0}^{\infty} \frac{\sin (\tan x)}{x} \ dx ,Does $ \displaystyle \int_{0}^{\infty} \ \frac{\sin (\tan x)}{x} dx $ converge? $ \displaystyle \int_{0}^{\infty} \frac{\sin (\tan x)}{x} \ dx = \int_{0}^{\frac{\pi}{2}} \frac{\sin (\tan x)}{x} \ dx + \sum_{n=1}^{\infty} \int_{\pi(n-\frac{1}{2})}^{\pi(n+\frac{1}{2})} \frac{\sin (\tan x)}{x} \ dx $ The first integral converges since $\displaystyle \frac{\sin (\tan x)}{x}$ has a removable singularity at $x=0$ and is bounded near $ \displaystyle \frac{\pi}{2}$. And $ \displaystyle \int_{\pi(n-\frac{1}{2})}^{\pi(n+\frac{1}{2})} \frac{\sin (\tan x)}{x} \ dx$ converges since $\displaystyle \frac{\sin (\tan x)}{x}$ is bounded near $\pi(n-\frac{1}{2})$ and $\pi(n+\frac{1}{2})$. But does $ \displaystyle \sum_{n=1}^{\infty} \int_{\pi(n-\frac{1}{2})}^{\pi(n+\frac{1}{2})} \frac{\sin (\tan x)}{x} \ dx$ converge?,Does $ \displaystyle \int_{0}^{\infty} \ \frac{\sin (\tan x)}{x} dx $ converge? $ \displaystyle \int_{0}^{\infty} \frac{\sin (\tan x)}{x} \ dx = \int_{0}^{\frac{\pi}{2}} \frac{\sin (\tan x)}{x} \ dx + \sum_{n=1}^{\infty} \int_{\pi(n-\frac{1}{2})}^{\pi(n+\frac{1}{2})} \frac{\sin (\tan x)}{x} \ dx $ The first integral converges since $\displaystyle \frac{\sin (\tan x)}{x}$ has a removable singularity at $x=0$ and is bounded near $ \displaystyle \frac{\pi}{2}$. And $ \displaystyle \int_{\pi(n-\frac{1}{2})}^{\pi(n+\frac{1}{2})} \frac{\sin (\tan x)}{x} \ dx$ converges since $\displaystyle \frac{\sin (\tan x)}{x}$ is bounded near $\pi(n-\frac{1}{2})$ and $\pi(n+\frac{1}{2})$. But does $ \displaystyle \sum_{n=1}^{\infty} \int_{\pi(n-\frac{1}{2})}^{\pi(n+\frac{1}{2})} \frac{\sin (\tan x)}{x} \ dx$ converge?,,"['calculus', 'improper-integrals']"
34,"Integral of a rational function: Proof of $\sqrt{C}\,\int_{0}^{+\infty }{{{y^2}\over{y^2\,C+y^4-2\,y^2+1}}\;\mathrm dy}= {{\pi}\over{2}}$?",Integral of a rational function: Proof of ?,"\sqrt{C}\,\int_{0}^{+\infty }{{{y^2}\over{y^2\,C+y^4-2\,y^2+1}}\;\mathrm dy}= {{\pi}\over{2}}","I suspect that $$\sqrt{C}\,\int_{0}^{+\infty }{{{y^2}\over{y^2\,C+y^4-2\,y^2+1}}\;\mathrm dy}=  {{\pi}\over{2}}$$ for $C>0$. I tried $C=1$ , $C=2$ , $C=42$ , and $C=\frac{1}{1000}$ with Wolfram Alpha. But how to prove it?","I suspect that $$\sqrt{C}\,\int_{0}^{+\infty }{{{y^2}\over{y^2\,C+y^4-2\,y^2+1}}\;\mathrm dy}=  {{\pi}\over{2}}$$ for $C>0$. I tried $C=1$ , $C=2$ , $C=42$ , and $C=\frac{1}{1000}$ with Wolfram Alpha. But how to prove it?",,"['calculus', 'definite-integrals', 'improper-integrals', 'symbolic-computation']"
35,what if take limit to negative infinity in the definition of e as a limit,what if take limit to negative infinity in the definition of e as a limit,,"By definition $$\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n = e.$$ But what about a similar limit where $n$ tends to negative infinity, i.e, $$\lim_{n\to -\infty}\left(1+\frac{1}{n}\right)^n?$$ Why does that equal to $e$ too? Could someone give a simple proof?","By definition $$\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n = e.$$ But what about a similar limit where $n$ tends to negative infinity, i.e, $$\lim_{n\to -\infty}\left(1+\frac{1}{n}\right)^n?$$ Why does that equal to $e$ too? Could someone give a simple proof?",,"['calculus', 'limits', 'exponential-function']"
36,Trigonometry inflection point,Trigonometry inflection point,,"Can anyone help me find the points of inflection in the following function in the interval between $0$ and $2\pi$ $f(x)=\sqrt{2}x^2-4\sin(x)$ for my first derivative I got $f'(x)=2\sqrt{2}x-4\cos(x)$ $f''(x)=2\sqrt{2}+4\sin(x)$ $\sin(x)=\frac{-\sqrt{2}}{2}$ inflection points would be $x=\frac{5\pi}{4},\frac{7\pi}{4}$ but would this be correct","Can anyone help me find the points of inflection in the following function in the interval between $0$ and $2\pi$ $f(x)=\sqrt{2}x^2-4\sin(x)$ for my first derivative I got $f'(x)=2\sqrt{2}x-4\cos(x)$ $f''(x)=2\sqrt{2}+4\sin(x)$ $\sin(x)=\frac{-\sqrt{2}}{2}$ inflection points would be $x=\frac{5\pi}{4},\frac{7\pi}{4}$ but would this be correct",,"['calculus', 'derivatives']"
37,Volume between two paraboloids,Volume between two paraboloids,,"Find the volume of the solid enclosed by the paraboloids $z=9(x^2+y^2)$  and $z=32−9(x^2+y^2)$ I'm not sure how to even find the volume enclosed to begin with. I know that the paraboloids intersect when $$9(r^2) = 32−9(r^2) \implies r = \frac43 \implies z = 16$$ If this is the plane where the two intersect, then the bounds are $16 \leq z\leq 32−9(x^2+y^2)$ Am I correct in this?","Find the volume of the solid enclosed by the paraboloids $z=9(x^2+y^2)$  and $z=32−9(x^2+y^2)$ I'm not sure how to even find the volume enclosed to begin with. I know that the paraboloids intersect when $$9(r^2) = 32−9(r^2) \implies r = \frac43 \implies z = 16$$ If this is the plane where the two intersect, then the bounds are $16 \leq z\leq 32−9(x^2+y^2)$ Am I correct in this?",,"['calculus', 'multivariable-calculus']"
38,Evaluating $\lim_{x\rightarrow 0} \frac{\ln(1-x)+\sin(x)}{x^2 e^x} $,Evaluating,\lim_{x\rightarrow 0} \frac{\ln(1-x)+\sin(x)}{x^2 e^x} ,"I got this exercise which I quite frankly can't wrap my head around $$\lim_{x\rightarrow 0} \frac{\ln(1-x)+\sin(x)}{x^2 e^x} $$ The result should be:$\ -\frac{1}{2} $ I tried by derivating the whole function and that led me nowhere. If I put in $\ x = 0 $, I get $\ \frac{0}{0} $ which means I can apply L'Hôpital's rule but that led me to the wrong answer as well. Obviously I'm doing something wrong. I had a look at Wolfram Alphas step-by-step solution to this exercise but I didn't really understand it. I'm appreciative with any possible help. Thanks, Michael.","I got this exercise which I quite frankly can't wrap my head around $$\lim_{x\rightarrow 0} \frac{\ln(1-x)+\sin(x)}{x^2 e^x} $$ The result should be:$\ -\frac{1}{2} $ I tried by derivating the whole function and that led me nowhere. If I put in $\ x = 0 $, I get $\ \frac{0}{0} $ which means I can apply L'Hôpital's rule but that led me to the wrong answer as well. Obviously I'm doing something wrong. I had a look at Wolfram Alphas step-by-step solution to this exercise but I didn't really understand it. I'm appreciative with any possible help. Thanks, Michael.",,"['calculus', 'limits']"
39,change of variables for definite integrals,change of variables for definite integrals,,First of all I would like to start off by asking why do they have different change of variable formulas for definite integrals than indefinite...why cant we just integrate using U substitution as we normally do in indefinite integral and then sub the original U value back and use that integrand for definite integral? I was at one point understanding integration but not when they started coming up with different formulas for definite integrals in U-substitution I got lost and resulted to just forcibly memorizing the formulas... I dont get why for U substitution they sub the upper and lower bounds into U from the original function to find the new upper and lower bounds with the function U. I know that because if you dont want to sub the original value of U in and want to instead stick to U as your function you must use those new upper and lower bound but if you sub in the original value for U then you can use your old upper and lower bound values. My question is what or how does plugging your old lower and upper bound values into U give you the new values of your new function thats expressed as U... Why do they make such a big deal out of it and complicate it when all they have to do is same U sub as indefinite integral and then plug original value of U in and go from there...are these math people just making excuses to come up with more work or is there more logic behind it?,First of all I would like to start off by asking why do they have different change of variable formulas for definite integrals than indefinite...why cant we just integrate using U substitution as we normally do in indefinite integral and then sub the original U value back and use that integrand for definite integral? I was at one point understanding integration but not when they started coming up with different formulas for definite integrals in U-substitution I got lost and resulted to just forcibly memorizing the formulas... I dont get why for U substitution they sub the upper and lower bounds into U from the original function to find the new upper and lower bounds with the function U. I know that because if you dont want to sub the original value of U in and want to instead stick to U as your function you must use those new upper and lower bound but if you sub in the original value for U then you can use your old upper and lower bound values. My question is what or how does plugging your old lower and upper bound values into U give you the new values of your new function thats expressed as U... Why do they make such a big deal out of it and complicate it when all they have to do is same U sub as indefinite integral and then plug original value of U in and go from there...are these math people just making excuses to come up with more work or is there more logic behind it?,,"['calculus', 'algebra-precalculus', 'integration', 'definite-integrals']"
40,Proof that series diverges,Proof that series diverges,,"Prove that $\displaystyle\sum_{n=1}^\infty\frac{1}{n(1+1/2+\cdots+1/n)}$ diverges. I think the only way to prove this is to find another series to compare using the comparison or limit tests. So far, I have been unable to find such a series.","Prove that $\displaystyle\sum_{n=1}^\infty\frac{1}{n(1+1/2+\cdots+1/n)}$ diverges. I think the only way to prove this is to find another series to compare using the comparison or limit tests. So far, I have been unable to find such a series.",,"['calculus', 'divergent-series']"
41,Question about an integrable singularity,Question about an integrable singularity,,"I've been trying to understand a concept of an integrable singularity. So far what I have discovered was that the case of the singularity occurs when there is a point where the integration becomes infinite. Am I right? Also, is there anything else that would add to the definition of the integrable singularity? Thanks.","I've been trying to understand a concept of an integrable singularity. So far what I have discovered was that the case of the singularity occurs when there is a point where the integration becomes infinite. Am I right? Also, is there anything else that would add to the definition of the integrable singularity? Thanks.",,['calculus']
42,Is this a justified expression for the integral of the floor function?,Is this a justified expression for the integral of the floor function?,,"Mathematica seems to agree with me in general with saying that $\displaystyle\int \lfloor x \rfloor dx = \frac{\lfloor x\rfloor (\lfloor x\rfloor-1)}{2}+\lfloor x\rfloor \{ x \}+C = \frac{\lfloor x\rfloor(2 x-\lfloor x\rfloor-1)}{2}+C$; that is, letting $I(x) = \frac{\lfloor x\rfloor(2 x-\lfloor x\rfloor-1)}{2}$ and checking whether $\displaystyle\int_a^b \lfloor x \rfloor dx = I(b)-I(a)$ returned true for all counts, decimals included. What exactly is preventing this from being true in general? Is the reason there is no indefinite integral due to the discontinuity in the floor function, even though Mathematica defines the definite integral along any interval?","Mathematica seems to agree with me in general with saying that $\displaystyle\int \lfloor x \rfloor dx = \frac{\lfloor x\rfloor (\lfloor x\rfloor-1)}{2}+\lfloor x\rfloor \{ x \}+C = \frac{\lfloor x\rfloor(2 x-\lfloor x\rfloor-1)}{2}+C$; that is, letting $I(x) = \frac{\lfloor x\rfloor(2 x-\lfloor x\rfloor-1)}{2}$ and checking whether $\displaystyle\int_a^b \lfloor x \rfloor dx = I(b)-I(a)$ returned true for all counts, decimals included. What exactly is preventing this from being true in general? Is the reason there is no indefinite integral due to the discontinuity in the floor function, even though Mathematica defines the definite integral along any interval?",,"['calculus', 'integration', 'ceiling-and-floor-functions']"
43,Evaluating a convergent improper triple integral over the unit sphere,Evaluating a convergent improper triple integral over the unit sphere,,"In Exercise 5 (f) of Angus Taylor's Advanced calculus (p. 659) one is asked to find the value of the following integral if convergent: $$I:=\underset{R}{\iiint}\dfrac{x^2 y^2 z^2}{r^{17/2}}\mathrm dV$$ where $R$ is the unit sphere $x^2+y^2+z^2\leq 1$ and $r^2=x^2+y^2+z^2$. Observing that $\dfrac{x^2 y^2 z^2}{r^{17/2}}\leq \dfrac{r^6}{r^{17/2}}=r^{-5/2}$ I proved that $I$ is convergent. Using spherical co-ordinates $r$, $\theta $, $\phi $ i.e. $$\begin{align*}x&=r\sin \phi \cos \theta\\y&=r\sin \phi \sin \theta\\z&=r\cos \theta\end{align*}$$ I transformed the integral $I$ into $$I=\int\nolimits_0^{2\pi }\left(\int_0^{\pi }\left(\lim_{\delta \to 0}\int_{\delta }^1\left(r^2 \sin \phi\right)\dfrac{x^2 y^2 z^2}{r^{17/2}}\;\mathrm dr\right)\;\mathrm d\phi \right)\;\mathrm d\theta$$ $$=\lim_{\delta \to 0}\left( \int_{\delta }^1 r^{-1/2}\mathrm dr\right)\int_0^{2\pi }\cos^4 \theta \sin^2 \theta \mathrm d\theta\int_0^{\pi }\sin^5 \mathrm d\phi $$ $$=2\cdot \dfrac18 \pi \cdot \dfrac{16}{15}=\dfrac4{15}\pi $$ In the solutions the answer is $\dfrac8{105}\pi$. Since sometimes there are a few book typos (in the exercises) to prevent undue copying, I ask the following Question: What is the correct solution, $\dfrac4{15}\pi $ or $\dfrac8{105}\pi $? UPDATE (Correction) : instead of $z=r\cos \theta $ it is $z=r\cos \phi $ See a comment from whuber . The integral $I$ is transformed into $$I=\int_0^{2\pi }\left(\int_0^{\pi }\left(\lim_{\delta \to 0}\int_{\delta }^1\left(r^2 \sin \phi\right)\dfrac{x^2 y^2 z^2}{r^{17/2}}\;\mathrm dr\right)\;\mathrm d\phi \right)\;\mathrm d\theta$$ Since $$(r^2 \sin \phi )\dfrac{x^2 y^2 z^2}{r^{17/2}}=(r^2\sin \phi )\dfrac1{r^{17/2}}\left( r\sin \phi \cos \theta \right) ^{2}\left( r\sin \phi \sin \theta \right) ^{2}\left( r\cos \phi \right) ^{2}$$ $=r^{-1/2}\cos ^{2}\theta \cdot\sin ^{2}\theta \cdot\cos ^{2}\phi \cdot\sin ^{5}\phi $, the transformed integral becomes (if I am right): $$I=\left(\lim_{\delta \to 0} \int_{\delta }^1 r^{-1/2}\mathrm dr\right)\int_0^{2\pi }\cos^2\theta\cdot\sin^2\theta \;\mathrm d\theta \int_0^{\pi }\cos^2 \phi \cdot\sin^5 \phi \;\mathrm d\phi$$ $$=2\cdot \dfrac14 \pi \cdot \dfrac{16}{105}=\dfrac8{105}\pi$$ The correct solution will be $\dfrac8{105}\pi $ as in the book.","In Exercise 5 (f) of Angus Taylor's Advanced calculus (p. 659) one is asked to find the value of the following integral if convergent: $$I:=\underset{R}{\iiint}\dfrac{x^2 y^2 z^2}{r^{17/2}}\mathrm dV$$ where $R$ is the unit sphere $x^2+y^2+z^2\leq 1$ and $r^2=x^2+y^2+z^2$. Observing that $\dfrac{x^2 y^2 z^2}{r^{17/2}}\leq \dfrac{r^6}{r^{17/2}}=r^{-5/2}$ I proved that $I$ is convergent. Using spherical co-ordinates $r$, $\theta $, $\phi $ i.e. $$\begin{align*}x&=r\sin \phi \cos \theta\\y&=r\sin \phi \sin \theta\\z&=r\cos \theta\end{align*}$$ I transformed the integral $I$ into $$I=\int\nolimits_0^{2\pi }\left(\int_0^{\pi }\left(\lim_{\delta \to 0}\int_{\delta }^1\left(r^2 \sin \phi\right)\dfrac{x^2 y^2 z^2}{r^{17/2}}\;\mathrm dr\right)\;\mathrm d\phi \right)\;\mathrm d\theta$$ $$=\lim_{\delta \to 0}\left( \int_{\delta }^1 r^{-1/2}\mathrm dr\right)\int_0^{2\pi }\cos^4 \theta \sin^2 \theta \mathrm d\theta\int_0^{\pi }\sin^5 \mathrm d\phi $$ $$=2\cdot \dfrac18 \pi \cdot \dfrac{16}{15}=\dfrac4{15}\pi $$ In the solutions the answer is $\dfrac8{105}\pi$. Since sometimes there are a few book typos (in the exercises) to prevent undue copying, I ask the following Question: What is the correct solution, $\dfrac4{15}\pi $ or $\dfrac8{105}\pi $? UPDATE (Correction) : instead of $z=r\cos \theta $ it is $z=r\cos \phi $ See a comment from whuber . The integral $I$ is transformed into $$I=\int_0^{2\pi }\left(\int_0^{\pi }\left(\lim_{\delta \to 0}\int_{\delta }^1\left(r^2 \sin \phi\right)\dfrac{x^2 y^2 z^2}{r^{17/2}}\;\mathrm dr\right)\;\mathrm d\phi \right)\;\mathrm d\theta$$ Since $$(r^2 \sin \phi )\dfrac{x^2 y^2 z^2}{r^{17/2}}=(r^2\sin \phi )\dfrac1{r^{17/2}}\left( r\sin \phi \cos \theta \right) ^{2}\left( r\sin \phi \sin \theta \right) ^{2}\left( r\cos \phi \right) ^{2}$$ $=r^{-1/2}\cos ^{2}\theta \cdot\sin ^{2}\theta \cdot\cos ^{2}\phi \cdot\sin ^{5}\phi $, the transformed integral becomes (if I am right): $$I=\left(\lim_{\delta \to 0} \int_{\delta }^1 r^{-1/2}\mathrm dr\right)\int_0^{2\pi }\cos^2\theta\cdot\sin^2\theta \;\mathrm d\theta \int_0^{\pi }\cos^2 \phi \cdot\sin^5 \phi \;\mathrm d\phi$$ $$=2\cdot \dfrac14 \pi \cdot \dfrac{16}{105}=\dfrac8{105}\pi$$ The correct solution will be $\dfrac8{105}\pi $ as in the book.",,"['calculus', 'integration', 'multivariable-calculus', 'improper-integrals']"
44,"A curious result from Mathematica on $\int\sin^5x\cos^7x \,dx$",A curious result from Mathematica on,"\int\sin^5x\cos^7x \,dx","While creating an answer key for my Calc 2 students and trying to save a bit of time, I plugged a trigonometric integral into Mathematica and was a little confused about how it approached the problem. The integral was $$ \int\sin^5x\cos^7x \,dx $$ The typical way to evaluate this integral would be to use the Pythagorean identity and a substitution. Something like this: \begin{align*} \int\sin^5x\cos^7x \,dx &= \int (\sin x)\big(\sin^4 x\big)\cos^7x\,dx \\ &= \int(\sin x)\big(1-\cos^2 x\big)^2\cos^7 x\, dx \\ &= -\int \big(1-u^2\big)^2u^7\, du \\ &= -\int u^7-2u^9+u^{11}\,du \\ &= -\frac{u^8}{8}+\frac{u^{10}}{5}-\frac{u^{12}}{12}+C \\ &= -\frac{\cos^8 x}{8}+\frac{\cos^{10}x}{5}-\frac{\cos^{12}x}{12}+C \end{align*} Where I've used the substitution $u=\cos x$ . We also could have used $u=\sin x$ to get $$ \int\sin^5x\cos^7x \,dx = \frac{\sin^6 x}{6} -\frac{3\sin^8 x}{8}+\frac{3\sin^{10}}{10}-\frac{\sin^{12} x}{12}+C $$ which is equivalent up to an added constant. When I plugged the integral into Mathematica, it spat out $$ -\frac{5\cos 2x}{1024} -\frac{5\cos 4x}{8192} +\frac{5\cos 6x}{6144} +\frac{\cos 8x}{4096} -\frac{\cos 10x}{10240} -\frac{\cos 12x}{24576} +C $$ I know this is equivalent to the other two answers. It's a bit tedious to do, but playing around with identities will show this. My question is how could we arrive at this result in a natural way? From the arguments of $\cos$ being even multiples of $x$ and the powers of 2 showing up in the denominators, it's clear that the power reducing identities are being used. (That is, $\cos^2 x = \frac{1+\cos 2x}2$ and/or $\sin^2 x = \frac {1-\cos 2x}2$ .) Typically we would use these if we were evaluating $\int \sin^mx\cos^nx\,dx$ , where $m$ and $n$ are both even. But with $m$ and $n$ both odd, I'm perplexed. I haven't been able to find an approach that doesn't require applying these identities after integrating. I've attempted to rewrite the integrand in terms of even powers of $\sin$ and $\cos$ , but that was a dead end. I also tried rewriting the integral as \begin{align*} \int\sin^5x\cos^7x \,dx &= \frac 1{32} \int \big(\sin^5 2x\big) \big(\cos^2 x\big)\,dx \\ &= \frac 1{64} \int \big(\sin^5 2x\big) \big(1+\cos 2x\big)\,dx  \end{align*} but this didn't quite get me there. I'd appreciate any help figuring this out, thanks!","While creating an answer key for my Calc 2 students and trying to save a bit of time, I plugged a trigonometric integral into Mathematica and was a little confused about how it approached the problem. The integral was The typical way to evaluate this integral would be to use the Pythagorean identity and a substitution. Something like this: Where I've used the substitution . We also could have used to get which is equivalent up to an added constant. When I plugged the integral into Mathematica, it spat out I know this is equivalent to the other two answers. It's a bit tedious to do, but playing around with identities will show this. My question is how could we arrive at this result in a natural way? From the arguments of being even multiples of and the powers of 2 showing up in the denominators, it's clear that the power reducing identities are being used. (That is, and/or .) Typically we would use these if we were evaluating , where and are both even. But with and both odd, I'm perplexed. I haven't been able to find an approach that doesn't require applying these identities after integrating. I've attempted to rewrite the integrand in terms of even powers of and , but that was a dead end. I also tried rewriting the integral as but this didn't quite get me there. I'd appreciate any help figuring this out, thanks!","
\int\sin^5x\cos^7x \,dx
 \begin{align*}
\int\sin^5x\cos^7x \,dx
&= \int (\sin x)\big(\sin^4 x\big)\cos^7x\,dx \\
&= \int(\sin x)\big(1-\cos^2 x\big)^2\cos^7 x\, dx \\
&= -\int \big(1-u^2\big)^2u^7\, du \\
&= -\int u^7-2u^9+u^{11}\,du \\
&= -\frac{u^8}{8}+\frac{u^{10}}{5}-\frac{u^{12}}{12}+C \\
&= -\frac{\cos^8 x}{8}+\frac{\cos^{10}x}{5}-\frac{\cos^{12}x}{12}+C
\end{align*} u=\cos x u=\sin x 
\int\sin^5x\cos^7x \,dx = \frac{\sin^6 x}{6} -\frac{3\sin^8 x}{8}+\frac{3\sin^{10}}{10}-\frac{\sin^{12} x}{12}+C
 
-\frac{5\cos 2x}{1024}
-\frac{5\cos 4x}{8192}
+\frac{5\cos 6x}{6144}
+\frac{\cos 8x}{4096}
-\frac{\cos 10x}{10240}
-\frac{\cos 12x}{24576}
+C
 \cos x \cos^2 x = \frac{1+\cos 2x}2 \sin^2 x = \frac {1-\cos 2x}2 \int \sin^mx\cos^nx\,dx m n m n \sin \cos \begin{align*}
\int\sin^5x\cos^7x \,dx
&= \frac 1{32} \int \big(\sin^5 2x\big) \big(\cos^2 x\big)\,dx \\
&= \frac 1{64} \int \big(\sin^5 2x\big) \big(1+\cos 2x\big)\,dx 
\end{align*}","['calculus', 'trigonometric-integrals']"
45,How can I determine this polynomial of degree 3?,How can I determine this polynomial of degree 3?,,"I want to find a polynomial of degree 3 which passes through $(2,0) ,(-2,4),(-4,8)$ and has a minimum on the y-axis. I wrote $f(x)=ax^3+bx^2+cx+d$ , then we need to satisfy $f(2)=0$ $f(-2)=4$ $f(-4)=8$ $f'(0)=0$ However, solving this system of equations gives me $$f(x)=-\frac{1}{4}x^3-\frac{5}{6}x^2+\frac{16}{3}$$ I know this is wrong because $$f ''(0)=2(-5/6)<0$$ means we don‘t have a minimum on the y axis. Where is my mistake?","I want to find a polynomial of degree 3 which passes through and has a minimum on the y-axis. I wrote , then we need to satisfy However, solving this system of equations gives me I know this is wrong because means we don‘t have a minimum on the y axis. Where is my mistake?","(2,0) ,(-2,4),(-4,8) f(x)=ax^3+bx^2+cx+d f(2)=0 f(-2)=4 f(-4)=8 f'(0)=0 f(x)=-\frac{1}{4}x^3-\frac{5}{6}x^2+\frac{16}{3} f
''(0)=2(-5/6)<0","['calculus', 'solution-verification', 'maxima-minima']"
46,Evaluate $\lim\limits_{x\to 0} \frac{x(1-\cos x)}{x - \sin(x)}$ without Taylor series or L'Hôpital's rule?,Evaluate  without Taylor series or L'Hôpital's rule?,\lim\limits_{x\to 0} \frac{x(1-\cos x)}{x - \sin(x)},I want to evaluate $\lim\limits_{x\to 0} \frac{x(1-\cos x)}{x - \sin(x)}$ . We know that its plot is: And also with attention to the Taylor series we know that its series expansion at $x=0$ is: $$3 - x^2/10 - x^4/4200 + O(x^6)$$ So the limit is $3$ . But I want to evaluate it without Taylor series or L'Hôpital's rule. What I tried: $$\begin{aligned} \lim\limits_{x\to 0} \frac{x(1-\cos x)}{x - \sin(x)}& =\\ &\lim_{x\to 0}\frac{x\cdot2\sin^2\frac{x}{2}}{x - \sin x} = \lim_{x\to 0}\frac{2x\cdot\sin^2\frac{x}{2}}{x\cdot(1 - \frac{\sin x}{x})} =  \\&\lim_{x\to 0}\frac{2\sin^2\frac{x}{2}}{1 - \frac{\sin x}{x}} \end{aligned}$$ But that did not help to evaluate the indeterminate form. Also I want to know is there a geometric representation for $x - \sin(x)$ which helps us to evaluate the limit?,I want to evaluate . We know that its plot is: And also with attention to the Taylor series we know that its series expansion at is: So the limit is . But I want to evaluate it without Taylor series or L'Hôpital's rule. What I tried: But that did not help to evaluate the indeterminate form. Also I want to know is there a geometric representation for which helps us to evaluate the limit?,"\lim\limits_{x\to 0} \frac{x(1-\cos x)}{x - \sin(x)} x=0 3 - x^2/10 - x^4/4200 + O(x^6) 3 \begin{aligned}
\lim\limits_{x\to 0} \frac{x(1-\cos x)}{x - \sin(x)}& =\\
&\lim_{x\to 0}\frac{x\cdot2\sin^2\frac{x}{2}}{x - \sin x} = \lim_{x\to 0}\frac{2x\cdot\sin^2\frac{x}{2}}{x\cdot(1 - \frac{\sin x}{x})} = 
\\&\lim_{x\to 0}\frac{2\sin^2\frac{x}{2}}{1 - \frac{\sin x}{x}}
\end{aligned} x - \sin(x)","['calculus', 'limits', 'trigonometry', 'limits-without-lhopital']"
47,Solving $\int_0^4 \frac{\left(y^2-4 y+5\right) \sin (y-2)}{2 y^2-8 y+11} d y$,Solving,\int_0^4 \frac{\left(y^2-4 y+5\right) \sin (y-2)}{2 y^2-8 y+11} d y,$$\int_0^4 \frac{\left(y^2-4 y+5\right) \sin (y-2)}{2 y^2-8 y+11} d y$$ Here is my attempt $$ \begin{aligned} & y-2=t \Rightarrow d y=d t \\ & y^2-4 y+4=t^2 \Rightarrow 2 t^2=2 y^2-8 y+8 \\ & {y^2-4 y+5=t^2+1} \\ & 2 y^2-8 y+11=2 t^2+3 \\ & 2 y^2-8 y+10+1=2 t^3+3 \\ & 2\left(y^2-4 y+5\right)+1=2 t^3+3 . \\ & {\int_0^4 \frac{2\left(t^2+1\right) \sin t}{2\left(t^2+1\right)+1} d t} \\ & \Rightarrow \int_0^4\frac{\sin t}{\left.1+\frac{1}{2\left(t^2+1\right)}\right)} d t \end{aligned} $$ No other substitution seems to work. Any help or quickies on this one would be appreciated. Thanks,Here is my attempt No other substitution seems to work. Any help or quickies on this one would be appreciated. Thanks,"\int_0^4 \frac{\left(y^2-4 y+5\right) \sin (y-2)}{2 y^2-8 y+11} d y 
\begin{aligned}
& y-2=t \Rightarrow d y=d t \\
& y^2-4 y+4=t^2 \Rightarrow 2 t^2=2 y^2-8 y+8 \\
& {y^2-4 y+5=t^2+1} \\
& 2 y^2-8 y+11=2 t^2+3 \\
& 2 y^2-8 y+10+1=2 t^3+3 \\
& 2\left(y^2-4 y+5\right)+1=2 t^3+3 . \\
& {\int_0^4 \frac{2\left(t^2+1\right) \sin t}{2\left(t^2+1\right)+1} d t} \\
& \Rightarrow \int_0^4\frac{\sin t}{\left.1+\frac{1}{2\left(t^2+1\right)}\right)} d t
\end{aligned}
","['calculus', 'integration', 'definite-integrals']"
48,Prove or disprove the equality of these two integrals,Prove or disprove the equality of these two integrals,,"Let $\alpha$ be an arbitrary positive real number in: $$ F_1 = \int_0^1 x^2 \left[ \int_{-1}^{+1} \frac{e^{-\alpha\sqrt{1+x^2+2xy}}(xy+1)}{(1+x^2+2xy)^{3/2}}dy\right]dx $$ $$ F_2 = \int_1^\infty x^2 \left[ \int_{-1}^{+1} \frac{e^{-\alpha\sqrt{1+x^2-2xy}}(xy-1)}{(1+x^2-2xy)^{3/2}}dy\right]dx $$ Prove or disprove that $F_1 = F_2$ . Source of the problem are the equations (3) and (4) in A Paradox of Newtonian Gravitation and Laplace’s Solution by Amitabha Ghosh and Ujjal Dey. They have done already numerical experiments that seem to confirm equality. Quote: an analytical proof showing that F1 and F2 are exactly equal will be an interesting mathematical exercise. And that's it. I have no idea how to proceed. Progress so far I promised myself not to do numerical experiments. But the outcome of the inner integral - the one between square brackets - is indeed terrible. So what else would be an option? With MAPLE 8 some values in the publication can be reproduced. for k from 0 to 10 do alpha := k*0.1; g(x,alpha) := int(exp(-alpha*sqrt(1+x^2+2*x*y))*(x*y+1)/(1+x^2+2*x*y)^(3/2),y=-1..1); F1 := evalf(int(g(x,alpha)*x^2,x=0..1)); F2 := evalf(int(-g(x,alpha)*x^2,x=1..10^3));  end do; The special case $\alpha = 0$ gives $\,F_1=\frac{2}{3}\,$ and $\,F_2=0\,$ exactly. So it appears that nearby $\alpha = 0$ the integrals must be unequal. But MAPLE keeps calculating endlessly for those low values and I had to manually stop the program. Feynman trick . $F_1$ and $F_2$ are both a function of $\alpha$ . We can take derivatives under the integral sign and see what happens near $\alpha=0$ . $$ \left.\frac{dF_1}{d\alpha}\right|_{\alpha=0} = - \int_0^1 x^2 \left[ \int_{-1}^{+1} \frac{xy+1}{1+x^2+2xy}dy\right]dx = -\frac{1}{2} \\ \Longrightarrow \quad dF_1 = -\frac{1}{2}d\alpha $$ $$ \left.\frac{dF_2}{d\alpha}\right|_{\alpha=0} = - \int_1^\infty x^2 \left[ \int_{-1}^{+1} \frac{xy-1}{1+x^2-2xy}dy\right]dx = \infty \\ \Longrightarrow \quad dF_2 = \infty\,d\alpha $$ Leading to the following heuristics. $F_1(\alpha)$ is somewhat decreasing from $F_1(0)=2/3$ to lower values, but the increase in $F_2(0)=0$ is infinitely large at that place. Based upon this, it's impossible to keep up appearances )-: we can actually say nothing yet whether the outcome is $\,F_1(\alpha) = F_2(\alpha)\,$ for all $\,\alpha \gt 0\,$ .","Let be an arbitrary positive real number in: Prove or disprove that . Source of the problem are the equations (3) and (4) in A Paradox of Newtonian Gravitation and Laplace’s Solution by Amitabha Ghosh and Ujjal Dey. They have done already numerical experiments that seem to confirm equality. Quote: an analytical proof showing that F1 and F2 are exactly equal will be an interesting mathematical exercise. And that's it. I have no idea how to proceed. Progress so far I promised myself not to do numerical experiments. But the outcome of the inner integral - the one between square brackets - is indeed terrible. So what else would be an option? With MAPLE 8 some values in the publication can be reproduced. for k from 0 to 10 do alpha := k*0.1; g(x,alpha) := int(exp(-alpha*sqrt(1+x^2+2*x*y))*(x*y+1)/(1+x^2+2*x*y)^(3/2),y=-1..1); F1 := evalf(int(g(x,alpha)*x^2,x=0..1)); F2 := evalf(int(-g(x,alpha)*x^2,x=1..10^3));  end do; The special case gives and exactly. So it appears that nearby the integrals must be unequal. But MAPLE keeps calculating endlessly for those low values and I had to manually stop the program. Feynman trick . and are both a function of . We can take derivatives under the integral sign and see what happens near . Leading to the following heuristics. is somewhat decreasing from to lower values, but the increase in is infinitely large at that place. Based upon this, it's impossible to keep up appearances )-: we can actually say nothing yet whether the outcome is for all .","\alpha 
F_1 = \int_0^1 x^2 \left[ \int_{-1}^{+1} \frac{e^{-\alpha\sqrt{1+x^2+2xy}}(xy+1)}{(1+x^2+2xy)^{3/2}}dy\right]dx  
F_2 = \int_1^\infty x^2 \left[ \int_{-1}^{+1} \frac{e^{-\alpha\sqrt{1+x^2-2xy}}(xy-1)}{(1+x^2-2xy)^{3/2}}dy\right]dx
 F_1 = F_2 \alpha = 0 \,F_1=\frac{2}{3}\, \,F_2=0\, \alpha = 0 F_1 F_2 \alpha \alpha=0 
\left.\frac{dF_1}{d\alpha}\right|_{\alpha=0} =
- \int_0^1 x^2 \left[ \int_{-1}^{+1} \frac{xy+1}{1+x^2+2xy}dy\right]dx = -\frac{1}{2} \\
\Longrightarrow \quad dF_1 = -\frac{1}{2}d\alpha
 
\left.\frac{dF_2}{d\alpha}\right|_{\alpha=0} =
- \int_1^\infty x^2 \left[ \int_{-1}^{+1} \frac{xy-1}{1+x^2-2xy}dy\right]dx = \infty \\
\Longrightarrow \quad dF_2 = \infty\,d\alpha
 F_1(\alpha) F_1(0)=2/3 F_2(0)=0 \,F_1(\alpha) = F_2(\alpha)\, \,\alpha \gt 0\,","['calculus', 'definite-integrals']"
49,Finding a bound for $\sum_{\text{cyc}}\frac{\sin B+\sin C}A$ if $\triangle ABC$ is not obtuse.,Finding a bound for  if  is not obtuse.,\sum_{\text{cyc}}\frac{\sin B+\sin C}A \triangle ABC,The following question appeared in a JEE mock exam held two days ago. Question: $\triangle ABC$ is not obtuse then value of $\displaystyle\sum_{\text{cyc}}\frac{\sin B+\sin C}A$ must be greater than A) $\frac6\pi$ B) $3$ C) $\frac{12}\pi$ D) $\frac1\pi$ My Attempt: I first tried with sine rule $$\frac a{\sin A}=\frac b{\sin B}=\frac c{\sin C}$$ But couldn't do anything with it. Then I used $\sin B+\sin C=2\sin\left(\frac{B+C}2\right)\cos\left(\frac{B-C}2\right)=2\cos\frac A2\cos\left(\frac{B-C}2\right)$ But couldn't finish this approach either. Then I tried using Jensen inequality but in vain. Then I thought I would assume a function and find its minimum value. But couldn't decide what function to take.,The following question appeared in a JEE mock exam held two days ago. Question: is not obtuse then value of must be greater than A) B) C) D) My Attempt: I first tried with sine rule But couldn't do anything with it. Then I used But couldn't finish this approach either. Then I tried using Jensen inequality but in vain. Then I thought I would assume a function and find its minimum value. But couldn't decide what function to take.,\triangle ABC \displaystyle\sum_{\text{cyc}}\frac{\sin B+\sin C}A \frac6\pi 3 \frac{12}\pi \frac1\pi \frac a{\sin A}=\frac b{\sin B}=\frac c{\sin C} \sin B+\sin C=2\sin\left(\frac{B+C}2\right)\cos\left(\frac{B-C}2\right)=2\cos\frac A2\cos\left(\frac{B-C}2\right),"['calculus', 'trigonometry', 'inequality', 'contest-math', 'triangles']"
50,Find a closed form for the following integral:,Find a closed form for the following integral:,,I have to evalute a closed form for the following integral: $$\int_0^1\left\{\frac{(-1)^{\lfloor\frac{1}{x}\rfloor}}{x}\right\}dx$$ where $\{x\}$ is the fractional part. I thought of using a substitution $y=1/x$ this will lead me to: $$\int_\infty^1\frac{\left\{y(-1)^{\lfloor y\rfloor}\right\}}{-y^2}dy$$ And now I have no idea how to proceed.,I have to evalute a closed form for the following integral: where is the fractional part. I thought of using a substitution this will lead me to: And now I have no idea how to proceed.,\int_0^1\left\{\frac{(-1)^{\lfloor\frac{1}{x}\rfloor}}{x}\right\}dx \{x\} y=1/x \int_\infty^1\frac{\left\{y(-1)^{\lfloor y\rfloor}\right\}}{-y^2}dy,"['calculus', 'integration']"
51,Differents ways to evaluate the sum $\sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}}$,Differents ways to evaluate the sum,\sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}},"Evaluate $$\sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}}$$ My approach: Let $$x=\sqrt{12+ \sqrt{12+\sqrt{12+\cdots}}}$$ so, we have that $$\sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}}\iff \sqrt{12+x}=x \implies 12+x=x^{2} \iff (x+3)(x-4)=0$$ So, the answer is $\boxed{4}$ . Is correct my solution? Can you show other ways for to solve this problem? Can you suggest me any  textbooks with similar problems? Thank you so much!","Evaluate My approach: Let so, we have that So, the answer is . Is correct my solution? Can you show other ways for to solve this problem? Can you suggest me any  textbooks with similar problems? Thank you so much!",\sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}} x=\sqrt{12+ \sqrt{12+\sqrt{12+\cdots}}} \sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}}\iff \sqrt{12+x}=x \implies 12+x=x^{2} \iff (x+3)(x-4)=0 \boxed{4},['calculus']
52,"Evaluate $\int_0^1\left(\frac{x-1}{x+1}\right)^n\frac{1}{\ln x} \,dx$",Evaluate,"\int_0^1\left(\frac{x-1}{x+1}\right)^n\frac{1}{\ln x} \,dx","Evaluate: $$ I(n) = \int_0^1\left(\frac{x-1}{x+1}\right)^n\frac{1}{\ln x} \,dx$$ For $n \in \mathbb N$ . I started by noticing that $\int_0^1 x^y \,dy = (x-1)/\ln x$ So I can convert this into a double integral as follows: $$I(n) = \int_0^1\int_0^1\frac{(x-1)^{n-1}x^y}{(x+1)^n}\,dy\,dx$$ Changing the order of integration and plugging in $x \rightarrow \cos 2\theta$ , I get: $$I(n) = 2(-1)^n\int_0^1 \int_0^{\pi/4}\tan^{2n-1}\theta \cos^y 2\theta \,d\theta \,dy$$ I have a feeling that I can somehow transform the inner integral into a Beta function, but the $\pi/4$ and $2\theta$ are a bit irritating and hard to get rid of. Any methods to tackle this?","Evaluate: For . I started by noticing that So I can convert this into a double integral as follows: Changing the order of integration and plugging in , I get: I have a feeling that I can somehow transform the inner integral into a Beta function, but the and are a bit irritating and hard to get rid of. Any methods to tackle this?"," I(n) = \int_0^1\left(\frac{x-1}{x+1}\right)^n\frac{1}{\ln x} \,dx n \in \mathbb N \int_0^1 x^y \,dy = (x-1)/\ln x I(n) = \int_0^1\int_0^1\frac{(x-1)^{n-1}x^y}{(x+1)^n}\,dy\,dx x \rightarrow \cos 2\theta I(n) = 2(-1)^n\int_0^1 \int_0^{\pi/4}\tan^{2n-1}\theta \cos^y 2\theta \,d\theta \,dy \pi/4 2\theta","['calculus', 'integration', 'definite-integrals']"
53,A limit involving the integer nearest to $n$-th power,A limit involving the integer nearest to -th power,n,"Find all $x\in\mathbb{R}$ such that $$\lim_{n\to\infty}|x^n-\langle x^n\rangle|=0$$ where $\langle t\rangle$ is the integer nearest to $t$ (eg. $\langle\frac{1}{3}\rangle=0$ , $\langle\frac{8}{3}\rangle=3$ , $\langle k+\frac{1}{2}\rangle$ is not defined for $k\in\mathbb{Z}$ ). I found this somewhere in the internet (today I searched again in IMO shortlists and didn't found, so it's probably not from there), tried to solve for a long time, but without nontrivial results (numbers $x\in\mathbb{Z}$ and $x\in(0,1)$ satisfy this, but I have no idea how to examine e.g. $x=\sqrt{2}$ ).","Find all such that where is the integer nearest to (eg. , , is not defined for ). I found this somewhere in the internet (today I searched again in IMO shortlists and didn't found, so it's probably not from there), tried to solve for a long time, but without nontrivial results (numbers and satisfy this, but I have no idea how to examine e.g. ).","x\in\mathbb{R} \lim_{n\to\infty}|x^n-\langle x^n\rangle|=0 \langle t\rangle t \langle\frac{1}{3}\rangle=0 \langle\frac{8}{3}\rangle=3 \langle k+\frac{1}{2}\rangle k\in\mathbb{Z} x\in\mathbb{Z} x\in(0,1) x=\sqrt{2}","['calculus', 'limits']"
54,"Show that $\forall x>0,\;\frac{1}{x}+x\geq 2$",Show that,"\forall x>0,\;\frac{1}{x}+x\geq 2","Using optimality conditions, we want to show that for all positive $x$ , we have $$\frac{1}{x}+x\geq 2\quad (1).$$ I think that the above problem is equivalent to showing that $1/x+x-2\geq 0, \forall x>0$ . Let $f(x)=1/x+x-2$ . Then I used the optimality conditions to get: $$f'(x)=-\frac{1}{x^2}+1$$ which is equal to zero iff $x=1$ . And also $$f''(x)=-\frac{2}{x^3}$$ which is positive iff $x$ is negative or negative iff $x$ is positive. So, for $x>0$ we have a maximum point at $x=1$ . But can we show that $f$ is non-negative? I am not sure what I did wrong here. I'd appreciate any help. Thank you.","Using optimality conditions, we want to show that for all positive , we have I think that the above problem is equivalent to showing that . Let . Then I used the optimality conditions to get: which is equal to zero iff . And also which is positive iff is negative or negative iff is positive. So, for we have a maximum point at . But can we show that is non-negative? I am not sure what I did wrong here. I'd appreciate any help. Thank you.","x \frac{1}{x}+x\geq 2\quad (1). 1/x+x-2\geq 0, \forall x>0 f(x)=1/x+x-2 f'(x)=-\frac{1}{x^2}+1 x=1 f''(x)=-\frac{2}{x^3} x x x>0 x=1 f","['calculus', 'optimization', 'nonlinear-optimization', 'maxima-minima']"
55,Curvature of curve: equivalence between tangent vector and angle definitions,Curvature of curve: equivalence between tangent vector and angle definitions,,"I know that curvature for some curve $C$ defined parametrically is: $$\kappa=\left\|{d\vec{T}\over ds}\right\|$$ Which basically is the rate at which the tangent vector to the curve changes, as the arclength of the curve changes. In another source, I saw the definition of curvature as the following: If $P_1$ and $P_2$ are two points on the curve, $|P_1P_2|$ is the arclength between those two points, and $\Phi$ is the limit of the angle between tangent vectors at the points $P_1$ and $P_2$ (as it goes to zero I assume), then the curvature is defined as: $$\kappa=\lim_{|P_1P_2|\to 0}{\Phi\over |P_1P_2|}$$ Which basically means, the rate at which the angle of tangent vectors in global frame of reference changes, as the arclength of the curve changes. I assume that this second definition can be rewritten using the notation from the first example as: $$\kappa={d\phi\over ds}$$ Where $\phi$ is the angle between the vector tangent to the curve, and some constant global axis of reference (which could be the x axis, but realy it could be any line or vector on the same plane). Given the second (weird in my opinion) definition of curvature, I can't see how those two definitions can be equivalent. Maybe they are not, I don't know. May be they are; if yes, how? Also, here's a picture of the section from the book where the second definition appears in (it's not in English): Note that the text agrees with yet another definition of curvature, which I am aware of: $\kappa=\frac1r$ , where $r$ is radius of curvature.","I know that curvature for some curve defined parametrically is: Which basically is the rate at which the tangent vector to the curve changes, as the arclength of the curve changes. In another source, I saw the definition of curvature as the following: If and are two points on the curve, is the arclength between those two points, and is the limit of the angle between tangent vectors at the points and (as it goes to zero I assume), then the curvature is defined as: Which basically means, the rate at which the angle of tangent vectors in global frame of reference changes, as the arclength of the curve changes. I assume that this second definition can be rewritten using the notation from the first example as: Where is the angle between the vector tangent to the curve, and some constant global axis of reference (which could be the x axis, but realy it could be any line or vector on the same plane). Given the second (weird in my opinion) definition of curvature, I can't see how those two definitions can be equivalent. Maybe they are not, I don't know. May be they are; if yes, how? Also, here's a picture of the section from the book where the second definition appears in (it's not in English): Note that the text agrees with yet another definition of curvature, which I am aware of: , where is radius of curvature.",C \kappa=\left\|{d\vec{T}\over ds}\right\| P_1 P_2 |P_1P_2| \Phi P_1 P_2 \kappa=\lim_{|P_1P_2|\to 0}{\Phi\over |P_1P_2|} \kappa={d\phi\over ds} \phi \kappa=\frac1r r,"['calculus', 'differential-geometry', 'vectors', 'curves', 'curvature']"
56,Behaviour of $\sum_{n=1} \frac{1}{n!} e^{an}\log(n)$ as $a \to \infty$,Behaviour of  as,\sum_{n=1} \frac{1}{n!} e^{an}\log(n) a \to \infty,"I am interested in the behavior of the following sum \begin{align} \sum_{n=1} \frac{1}{n!} e^{an}\log(n), \end{align} as $a$ approaches infinity. Using $\log(n) \le n$ we can the following upper bound \begin{align} \sum_{n=1} \frac{1}{n!} e^{an}\log(n) \le   e^{a+e^a}.  \end{align} However, I think the true behavior is more like  $e^{\log(a)+e^a}$. However, I am not sure how to show whether this is true or not.","I am interested in the behavior of the following sum \begin{align} \sum_{n=1} \frac{1}{n!} e^{an}\log(n), \end{align} as $a$ approaches infinity. Using $\log(n) \le n$ we can the following upper bound \begin{align} \sum_{n=1} \frac{1}{n!} e^{an}\log(n) \le   e^{a+e^a}.  \end{align} However, I think the true behavior is more like  $e^{\log(a)+e^a}$. However, I am not sure how to show whether this is true or not.",,"['calculus', 'sequences-and-series']"
57,Integral inequality $\frac{1}{2M}\le\int_0^1xf(x)dx\le1-\frac{1}{2M}$,Integral inequality,\frac{1}{2M}\le\int_0^1xf(x)dx\le1-\frac{1}{2M},"Let $f:[0,1]\rightarrow[0,\infty)$ be a continuous function such that $\int_0^1f(x)dx=1$, and let $M=\max f(x)$. Show that: $$\frac{1}{2M}\le\int_0^1xf(x)dx\le1-\frac{1}{2M}$$ We have $1=\int_0^1f(x)dx\le\int_0^1Mdx=M$, so $M\ge1$. I tried several inequalities, but none of them seem to be working in this case. I could use a hint. Thank you!","Let $f:[0,1]\rightarrow[0,\infty)$ be a continuous function such that $\int_0^1f(x)dx=1$, and let $M=\max f(x)$. Show that: $$\frac{1}{2M}\le\int_0^1xf(x)dx\le1-\frac{1}{2M}$$ We have $1=\int_0^1f(x)dx\le\int_0^1Mdx=M$, so $M\ge1$. I tried several inequalities, but none of them seem to be working in this case. I could use a hint. Thank you!",,"['calculus', 'integration', 'integral-inequality']"
58,"Showing the sequence converges $a_{1}=\frac{1}{2}$, $a_{n+1}=\frac{1}{2+a_{n}}$","Showing the sequence converges ,",a_{1}=\frac{1}{2} a_{n+1}=\frac{1}{2+a_{n}},"Showing the sequence converges $a_{1}=\frac{1}{2}$,  $a_{n+1}=\frac{1}{2+a_{n}}$. I already know that if $(a_{n})$ converges then it does to  $\sqrt{2}-1$.But i dont't know how to prove that this sequence cenverges. EDIT I think that the subsequence $(a_{2n+1})$ is monotonic decreasing and the subsequence $(a_{2n})$ is monotonic increasing.","Showing the sequence converges $a_{1}=\frac{1}{2}$,  $a_{n+1}=\frac{1}{2+a_{n}}$. I already know that if $(a_{n})$ converges then it does to  $\sqrt{2}-1$.But i dont't know how to prove that this sequence cenverges. EDIT I think that the subsequence $(a_{2n+1})$ is monotonic decreasing and the subsequence $(a_{2n})$ is monotonic increasing.",,"['calculus', 'sequences-and-series']"
59,"How to calculate this definite integral :- $\int_0^\pi \frac{x}{a^2\cos^2x + b^2\sin^2x} \,dx$?",How to calculate this definite integral :- ?,"\int_0^\pi \frac{x}{a^2\cos^2x + b^2\sin^2x} \,dx","This question was in a test I took: calculate the value of the integral $$\int_0^\pi \dfrac{x}{a^2\cos^2x + b^2\sin^2x} \,dx.$$ I was not able to solve it. Now, I've tried to do some substitutions for $x$ but honestly I don't actually know how to proceed with this integral. As I have not solved such questions before, I don't know the general direction in which I need to go so I don't know which efforts to include here and which to not. All I know is that those constants a & b are the source of much of the trouble for me but I don't know how to get rid of them. I also want to know whether there is only a single way to solve it (which maybe I've not practised enough) or - like many other problems - it can be solved by more than one method. I would really appreciate if I find different solutions of this problem here but (if you can help it) please don't include any incredibly tough/ esoteric theorems or concepts/ higher-level stuff that I can't be expected to know at my current level. Thanks!","This question was in a test I took: calculate the value of the integral $$\int_0^\pi \dfrac{x}{a^2\cos^2x + b^2\sin^2x} \,dx.$$ I was not able to solve it. Now, I've tried to do some substitutions for $x$ but honestly I don't actually know how to proceed with this integral. As I have not solved such questions before, I don't know the general direction in which I need to go so I don't know which efforts to include here and which to not. All I know is that those constants a & b are the source of much of the trouble for me but I don't know how to get rid of them. I also want to know whether there is only a single way to solve it (which maybe I've not practised enough) or - like many other problems - it can be solved by more than one method. I would really appreciate if I find different solutions of this problem here but (if you can help it) please don't include any incredibly tough/ esoteric theorems or concepts/ higher-level stuff that I can't be expected to know at my current level. Thanks!",,"['calculus', 'integration', 'definite-integrals']"
60,Logarithmic series as $\sum_{n=3}^{\infty}(-1)^n\ln\!\left(1+\frac1{2n}\right) \!\ln \!\left(\frac{2n^2+n-6}{2n^2+n-10}\!\right)$,Logarithmic series as,\sum_{n=3}^{\infty}(-1)^n\ln\!\left(1+\frac1{2n}\right) \!\ln \!\left(\frac{2n^2+n-6}{2n^2+n-10}\!\right),"Inspired by this question , I've designed the following series. $$ \begin{align} S&=\sum_{n=3}^{\infty}\ln\!\left(1+\frac1{2n}\right) \!\ln \!\left(\frac{2n^2+n-6}{2n^2+n-10}\!\right) \tag1 \\\\ T&=\sum_{n=3}^{\infty}(-1)^n\ln\!\left(1+\frac1{2n}\right) \!\ln \!\left(\frac{2n^2+n-6}{2n^2+n-10}\!\right) \tag2 \end{align} $$ Each one admits a nice closed form. Q1. What are their closed forms? Q2. To which family would you tell these series belong to ?","Inspired by this question , I've designed the following series. $$ \begin{align} S&=\sum_{n=3}^{\infty}\ln\!\left(1+\frac1{2n}\right) \!\ln \!\left(\frac{2n^2+n-6}{2n^2+n-10}\!\right) \tag1 \\\\ T&=\sum_{n=3}^{\infty}(-1)^n\ln\!\left(1+\frac1{2n}\right) \!\ln \!\left(\frac{2n^2+n-6}{2n^2+n-10}\!\right) \tag2 \end{align} $$ Each one admits a nice closed form. Q1. What are their closed forms? Q2. To which family would you tell these series belong to ?",,"['calculus', 'sequences-and-series', 'definite-integrals', 'closed-form']"
61,A complex function integral,A complex function integral,,"I got a problem in an exam. We need to caculate following limit: $$\lim_{x\rightarrow \infty} \int_{L_x}\frac{\cos z}{z^2+1} dz,$$ where $L_x$ is the line from $-x+2i$ to $x+2i$ . Of course we can calculate the integrals of the real part and imaginary part seperately. But it would be extremely complicated. I think it can't be the right solution since it wouldn't be done in the time of an exam.",I got a problem in an exam. We need to caculate following limit: where is the line from to . Of course we can calculate the integrals of the real part and imaginary part seperately. But it would be extremely complicated. I think it can't be the right solution since it wouldn't be done in the time of an exam.,"\lim_{x\rightarrow \infty} \int_{L_x}\frac{\cos z}{z^2+1} dz, L_x -x+2i x+2i","['calculus', 'complex-analysis']"
62,A rigorous proof for a Riemann-like sum,A rigorous proof for a Riemann-like sum,,"I am asked to find the limit of the following sum: $$ \lim_{n\rightarrow\infty}\sum_{k=1}^{n}\sin{\left(\frac{k}{n}\right)}\sin{\left(\frac{k}{n^2}\right)} $$ My attempt: For $n\rightarrow\infty$ and for $1\leq k\leq n$ : $$ \sin{\frac{k}{n^2}}\approx \frac{k}{n^2} $$ So $$ \sin{\left(\frac{k}{n}\right)}\sin{\left(\frac{k}{n^2}\right)}\approx \frac{k}{n^2}\sin{\left(\frac{k}{n}\right)}=\frac{1}{n}\frac{k}{n}\sin{\left(\frac{k}{n}\right)} $$ Thus, we get a reimann sum, so the limit would be equal to: $$ \lim_{n\rightarrow \infty}\sum_{k=1}^{n}\frac{1}{n}\frac{k}{n}\sin{\left(\frac{k}{n}\right)}=\int_{0}^{1}x\sin{x}dx=\sin{1}-\cos{1} $$ Now the question is, how can I prove this rigorously because the way ""I did it"" is obviously incorrect, if someone can guide me through a rigorous proof, it would be perfect. Thanks in advance.","I am asked to find the limit of the following sum: My attempt: For and for : So Thus, we get a reimann sum, so the limit would be equal to: Now the question is, how can I prove this rigorously because the way ""I did it"" is obviously incorrect, if someone can guide me through a rigorous proof, it would be perfect. Thanks in advance.","
\lim_{n\rightarrow\infty}\sum_{k=1}^{n}\sin{\left(\frac{k}{n}\right)}\sin{\left(\frac{k}{n^2}\right)}
 n\rightarrow\infty 1\leq k\leq n 
\sin{\frac{k}{n^2}}\approx \frac{k}{n^2}
 
\sin{\left(\frac{k}{n}\right)}\sin{\left(\frac{k}{n^2}\right)}\approx \frac{k}{n^2}\sin{\left(\frac{k}{n}\right)}=\frac{1}{n}\frac{k}{n}\sin{\left(\frac{k}{n}\right)}
 
\lim_{n\rightarrow \infty}\sum_{k=1}^{n}\frac{1}{n}\frac{k}{n}\sin{\left(\frac{k}{n}\right)}=\int_{0}^{1}x\sin{x}dx=\sin{1}-\cos{1}
","['calculus', 'integration', 'sequences-and-series', 'riemann-sum']"
63,"How to explain why Integration by parts apparently ""fails"" in the case of $\int \frac{f'(x)}{f(x)}dx$ without resorting to definite integrals?","How to explain why Integration by parts apparently ""fails"" in the case of  without resorting to definite integrals?",\int \frac{f'(x)}{f(x)}dx,"Integrating by parts the following integral $$I=\int \frac{f'(x)}{f(x)}dx$$ gives us $$\begin{align*} I&=\int \frac{f'(x)}{f(x)}\,dx\\ &=\int\frac1{f(x)}f'(x)\,dx\\ &=\frac1{f(x)}f(x)-\int\left(\frac1{f(x)}\right)'f(x)\,dx\\ &=1+\int \frac{f'(x)}{f(x)}\,dx, \end{align*} $$ so $$I=1+I\implies0=1,$$ which seems like a contradiction but is in reality a mistake as we can see by being somewhat more rigorous: $$ \begin{align*} I&=\int_{a}^x \frac{f'(t)}{f(t)}\,dt\\ &=\int_{a}^x\frac1{f(t)}f'(t)\,dt\\ &=\left[\frac1{f(t)}f(t)\right]_a^x-\int_{a}^x\left(\frac1{f(t)}\right)'f(t)\,dt, \end{align*}$$ so $I=I.$ How do I explain this to a student-of economics for what it's worth-who has not still learned about definite Integrals? (I suspect I could insert some constant on the upper part of the ""failed"" relations but I am not sure where or how to properly explain it. I also understand that in a couple of lessons we will talk about definitive integrals but what can I say now-That indefinite integrals are in reality a form of definite ones? )","Integrating by parts the following integral $$I=\int \frac{f'(x)}{f(x)}dx$$ gives us $$\begin{align*} I&=\int \frac{f'(x)}{f(x)}\,dx\\ &=\int\frac1{f(x)}f'(x)\,dx\\ &=\frac1{f(x)}f(x)-\int\left(\frac1{f(x)}\right)'f(x)\,dx\\ &=1+\int \frac{f'(x)}{f(x)}\,dx, \end{align*} $$ so $$I=1+I\implies0=1,$$ which seems like a contradiction but is in reality a mistake as we can see by being somewhat more rigorous: $$ \begin{align*} I&=\int_{a}^x \frac{f'(t)}{f(t)}\,dt\\ &=\int_{a}^x\frac1{f(t)}f'(t)\,dt\\ &=\left[\frac1{f(t)}f(t)\right]_a^x-\int_{a}^x\left(\frac1{f(t)}\right)'f(t)\,dt, \end{align*}$$ so $I=I.$ How do I explain this to a student-of economics for what it's worth-who has not still learned about definite Integrals? (I suspect I could insert some constant on the upper part of the ""failed"" relations but I am not sure where or how to properly explain it. I also understand that in a couple of lessons we will talk about definitive integrals but what can I say now-That indefinite integrals are in reality a form of definite ones? )",,"['calculus', 'integration', 'education', 'integration-by-parts']"
64,Sum of subgradients belongs to subgradient of sums?,Sum of subgradients belongs to subgradient of sums?,,"I was going through this page : https://www.stats.ox.ac.uk/~lienart/blog_opti_basics.html , and at the end of part 1 ""Subgradient and First-order Optimality Condition"", the author says: Before moving on, it is useful to note (and not too hard to convince oneself) that the following inclusion holds for the subdifferential of a sum:   $\sum_i ∂f_i⊆∂∑_if_i$. Can anyone explain what this means? If $f_i$ is not differentiable, then it can have multiple values of subgradient, right? Then what does the sum of subgradients of the functions $f_i$ amount to? And how do we show the above result?","I was going through this page : https://www.stats.ox.ac.uk/~lienart/blog_opti_basics.html , and at the end of part 1 ""Subgradient and First-order Optimality Condition"", the author says: Before moving on, it is useful to note (and not too hard to convince oneself) that the following inclusion holds for the subdifferential of a sum:   $\sum_i ∂f_i⊆∂∑_if_i$. Can anyone explain what this means? If $f_i$ is not differentiable, then it can have multiple values of subgradient, right? Then what does the sum of subgradients of the functions $f_i$ amount to? And how do we show the above result?",,"['calculus', 'continuity', 'convex-optimization', 'subgradient']"
65,Stuck with this limit of a sum: $\lim _{n \to \infty} \left(\frac{a^{n}-b^{n}}{a^{n}+b^{n}}\right)$.,Stuck with this limit of a sum: .,\lim _{n \to \infty} \left(\frac{a^{n}-b^{n}}{a^{n}+b^{n}}\right),"Here's the limit: $$\lim _{n \to \infty}  \left(\frac{a^{n}-b^{n}}{a^{n}+b^{n}}\right)$$ The conditions are $b>0$ and $a>0$. I tried this with the case that $a>b$: $$\lim _{n \to \infty}  \left(\frac{1-\frac{b^{n}}{a^{n}}}{1+\frac{b^{n}}{a^{n}}}\right)$$ It gives me the result $1$. But, in the case of $b>a$, I don't find a solution. Thanks for your attention.","Here's the limit: $$\lim _{n \to \infty}  \left(\frac{a^{n}-b^{n}}{a^{n}+b^{n}}\right)$$ The conditions are $b>0$ and $a>0$. I tried this with the case that $a>b$: $$\lim _{n \to \infty}  \left(\frac{1-\frac{b^{n}}{a^{n}}}{1+\frac{b^{n}}{a^{n}}}\right)$$ It gives me the result $1$. But, in the case of $b>a$, I don't find a solution. Thanks for your attention.",,"['calculus', 'analysis', 'limits', 'summation']"
66,Evaluating the sum $\sum_{n=2}^{\infty}\ln[1-1/n^2]$,Evaluating the sum,\sum_{n=2}^{\infty}\ln[1-1/n^2],"For convenience the sum is again $$ \sum_{n=2}^{\infty}\ln[1-1/n^2]=\sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)} $$ I first tried solving using a definite integral, since this seems to make telescoping easier to see, $$ \sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)}=\sum_{n=2}^{\infty}\ln(n^2-1)-\ln(n^2)= \sum_{n=2}^{\infty}\int_{n^2}^{n^2-1}\frac{dx}{x} $$ But writing out the first few terms doesn't make any cancellation obvious because of the $n^2$ terms. I also tried futzing around with log rules and got things down to  $$ \sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)}= \sum_{n=2}^{\infty}\ln((n-1)(n+1))-\ln(n^2)= \sum_{n=2}^{\infty}\ln(n-1)+\ln(n+1)-\ln(n^2) $$ The first few terms of which are $$ \sum_{n=2}^{4}\ln\frac{(n^2-1)}{(n^2)}=[\ln1+\ln3-2\ln 2]+[\ln2+\ln4-2\ln 3]+[\ln3+\ln5-2\ln 4]+...\\ =\ln 2+\ln 5-2\ln 4 $$ Which leads to the guess that  $$ \sum_{n=2}^{4}\ln\frac{(n^2-1)}{(n^2)}=\ln (N-1)+\ln(N+2)-2\ln(N)\\ =\ln(\frac{(N-1)(N+2)}{N^2})\rightarrow 0 $$ Which means I'm wrong. Should I soldiering on looking for a pattern through more computation, or is there a more expedient/elegant way to evaluate the sum?","For convenience the sum is again $$ \sum_{n=2}^{\infty}\ln[1-1/n^2]=\sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)} $$ I first tried solving using a definite integral, since this seems to make telescoping easier to see, $$ \sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)}=\sum_{n=2}^{\infty}\ln(n^2-1)-\ln(n^2)= \sum_{n=2}^{\infty}\int_{n^2}^{n^2-1}\frac{dx}{x} $$ But writing out the first few terms doesn't make any cancellation obvious because of the $n^2$ terms. I also tried futzing around with log rules and got things down to  $$ \sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)}= \sum_{n=2}^{\infty}\ln((n-1)(n+1))-\ln(n^2)= \sum_{n=2}^{\infty}\ln(n-1)+\ln(n+1)-\ln(n^2) $$ The first few terms of which are $$ \sum_{n=2}^{4}\ln\frac{(n^2-1)}{(n^2)}=[\ln1+\ln3-2\ln 2]+[\ln2+\ln4-2\ln 3]+[\ln3+\ln5-2\ln 4]+...\\ =\ln 2+\ln 5-2\ln 4 $$ Which leads to the guess that  $$ \sum_{n=2}^{4}\ln\frac{(n^2-1)}{(n^2)}=\ln (N-1)+\ln(N+2)-2\ln(N)\\ =\ln(\frac{(N-1)(N+2)}{N^2})\rightarrow 0 $$ Which means I'm wrong. Should I soldiering on looking for a pattern through more computation, or is there a more expedient/elegant way to evaluate the sum?",,"['calculus', 'sequences-and-series']"
67,Evaluation of $\int^{\pi/2}_{0} \frac{x \tan(x)}{\sec(x)+\tan(x)}dx$,Evaluation of,\int^{\pi/2}_{0} \frac{x \tan(x)}{\sec(x)+\tan(x)}dx,"Evaluate the given integral: $$\int^{\pi/2}_0 \frac{x \tan(x)}{\sec(x)+\tan(x)}dx$$ I multiplied and divided by $\sec(x)+\tan(x)$ to get denominator as $1$ but  In calculation of integral, $x$ is creating problem. Is there any way to eliminate $x$ here, like it would have been eliminated if upper limit was $\pi$?","Evaluate the given integral: $$\int^{\pi/2}_0 \frac{x \tan(x)}{\sec(x)+\tan(x)}dx$$ I multiplied and divided by $\sec(x)+\tan(x)$ to get denominator as $1$ but  In calculation of integral, $x$ is creating problem. Is there any way to eliminate $x$ here, like it would have been eliminated if upper limit was $\pi$?",,"['calculus', 'integration', 'definite-integrals']"
68,When are differentials actually useful?,When are differentials actually useful?,,"I think of differentials as a way to approximate $\Delta y$ in a function $y = f(x)$ for a certain $\Delta x$. The way I understood it, the derivative itself is not a ratio because you can't get $\frac{dy}{dx}$ by taking the ratio of the limits of the numerator and denominator separately. However, once you do have $\frac{dy}{dx}$, you can then think of $dx$ as $\Delta x$ and of $dy$ as the change in $y$ for a slope $\frac{dy}{dx}$ over a certain $\Delta x$. My problem is that I don't know why differentials are useful. The examples I saw are along the lines of approximating the max error in a sphere's volume if we know that the radius we're given (let's say 20) has a possible error of let's say 0.01. In this kind of example, it seems to me we're better off computing $V(20.01) - V(20)$, instead of $\Delta V \approx dV = V' \cdot \Delta x$. In the first case at least we get an exact maximum error instead of an approximation. So my question is : When are differentials actually useful? Are they ever anything more than at best a time saver over hand computations? Are they useful at all in today's world with Wolfram Alpha and other strong computational engines around?","I think of differentials as a way to approximate $\Delta y$ in a function $y = f(x)$ for a certain $\Delta x$. The way I understood it, the derivative itself is not a ratio because you can't get $\frac{dy}{dx}$ by taking the ratio of the limits of the numerator and denominator separately. However, once you do have $\frac{dy}{dx}$, you can then think of $dx$ as $\Delta x$ and of $dy$ as the change in $y$ for a slope $\frac{dy}{dx}$ over a certain $\Delta x$. My problem is that I don't know why differentials are useful. The examples I saw are along the lines of approximating the max error in a sphere's volume if we know that the radius we're given (let's say 20) has a possible error of let's say 0.01. In this kind of example, it seems to me we're better off computing $V(20.01) - V(20)$, instead of $\Delta V \approx dV = V' \cdot \Delta x$. In the first case at least we get an exact maximum error instead of an approximation. So my question is : When are differentials actually useful? Are they ever anything more than at best a time saver over hand computations? Are they useful at all in today's world with Wolfram Alpha and other strong computational engines around?",,['calculus']
69,Prove that $(fg)^{(n)} = \sum_{k=0}^n \binom{n}{k}f^{(k)}(x)g^{(n-k)}(x)$,Prove that,(fg)^{(n)} = \sum_{k=0}^n \binom{n}{k}f^{(k)}(x)g^{(n-k)}(x),Assume $f$ and $g$ are differentiable at $x$. Prove that $(fg)^{(n)} = \sum_{k=0}^n \binom{n}{k}f^{(k)}(x)g^{(n-k)}(x)$ I am assuming here $fg = f(x) g(x)$. Then we can prove this via induction. If $n = 0$ we have $1 = 1$ which is true. Now assume it is true for some $m$ we need to show it is possible for $m+1$. We have $(fg)^m = \sum_{k=0}^m f^{(k)}(x)g^{m-k}(x)$. How do we show that?,Assume $f$ and $g$ are differentiable at $x$. Prove that $(fg)^{(n)} = \sum_{k=0}^n \binom{n}{k}f^{(k)}(x)g^{(n-k)}(x)$ I am assuming here $fg = f(x) g(x)$. Then we can prove this via induction. If $n = 0$ we have $1 = 1$ which is true. Now assume it is true for some $m$ we need to show it is possible for $m+1$. We have $(fg)^m = \sum_{k=0}^m f^{(k)}(x)g^{m-k}(x)$. How do we show that?,,['calculus']
70,Prove that the integral of an even function is odd,Prove that the integral of an even function is odd,,"I'm given the function $$F(x)=\int \limits _0^xf(t)\,dt$$ and that $f(t)$ is an even function. The assignment asks me to prove that $F(x)$ is an odd function. I've tried doing something like $$F(-x)=\int \limits _0^{-x}f(-t)\,dt=\int \limits _0^{-x}f(t)\,dt=-\int \limits _{-x}^0f(t)\,dt$$ and now since $f(t)$ is even the area in the intervals $[-x,0]$ and $[0,x]$ should be the same,but I'm not sure whether this is a rigorous enough proof. Any tips?","I'm given the function and that is an even function. The assignment asks me to prove that is an odd function. I've tried doing something like and now since is even the area in the intervals and should be the same,but I'm not sure whether this is a rigorous enough proof. Any tips?","F(x)=\int \limits _0^xf(t)\,dt f(t) F(x) F(-x)=\int \limits _0^{-x}f(-t)\,dt=\int \limits _0^{-x}f(t)\,dt=-\int \limits _{-x}^0f(t)\,dt f(t) [-x,0] [0,x]","['calculus', 'integration', 'definite-integrals']"
71,Evaluate the integral $\int \frac{1}{\sqrt[3]{(x+1)^2(x-2)^2}}\mathrm dx$,Evaluate the integral,\int \frac{1}{\sqrt[3]{(x+1)^2(x-2)^2}}\mathrm dx,"What substitution is useful for this integral? $$\int \frac{1}{\sqrt[3]{(x+1)^2(x-2)^2}}\mathrm dx$$ Substitutions $u=x^{\frac{2}{3}},u=(x+1)^{\frac{2}{3}},u=(x-2)^{\frac{2}{3}}$ are not working. Can't find useful trigonometric substitution.","What substitution is useful for this integral? $$\int \frac{1}{\sqrt[3]{(x+1)^2(x-2)^2}}\mathrm dx$$ Substitutions $u=x^{\frac{2}{3}},u=(x+1)^{\frac{2}{3}},u=(x-2)^{\frac{2}{3}}$ are not working. Can't find useful trigonometric substitution.",,"['calculus', 'integration']"
72,Why is the following NOT a proof of The Chain Rule?,Why is the following NOT a proof of The Chain Rule?,,"In Leibniz notation of the chain rule,  $$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$$ Where $y\left ( u\left ( x \right ) \right )$ is a composite function of x. I understand that the du 's don't simply cancel out because $\frac{dy}{du}$ and $\frac{du}{dx}$ are defined as specific limits making the numerator and denominator infinitesimals and thus making the whole thing indeterminate and inoperable on. But applying the definition of a derivative, we can express the above like so: $$\lim_{\Delta x\to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta u\to 0} \frac{\Delta y}{\Delta u} \cdot \lim_{\Delta x\to 0} \frac{\Delta u}{\Delta x}$$ At this point we can't use the Product Law of Limits to combine the two limits on the right. But if we consider a coordinate system u vs x , doesn't $\Delta u \rightarrow 0$ when $\Delta x \rightarrow 0$ ? And if so,  then whenever $\Delta u \rightarrow 0$ we necessarily have $\Delta x \rightarrow 0$. Then can't we rewrite the above limit equation as: $$\lim_{\Delta x\to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x\to 0} \frac{\Delta y}{\Delta u} \cdot \lim_{\Delta x\to 0} \frac{\Delta u}{\Delta x}$$ And then can't we use the Product Law of Limits to say: $$\lim_{\Delta x\to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x\to 0} \left (  \frac{\Delta y}{\Delta u} \cdot \frac{\Delta u}{\Delta x} \right )$$ And since $\frac{\Delta y}{\Delta u}$ and $\frac{\Delta u}{\Delta x}$ within the quantity who's limit is being taken are no longer ""quotients"" infinitesimals, Δu 's can cancel, leaving us with: $$\lim_{\Delta x\to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x\to 0}   \frac{\Delta y}{\Delta x} $$ Which in Leibniz notations looks like: $$\frac{dy}{dx} = \frac{dy}{dx}$$ Q.E.D (Since we manipulated the right hand side of the equation into looking the same as the left hand side).","In Leibniz notation of the chain rule,  $$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$$ Where $y\left ( u\left ( x \right ) \right )$ is a composite function of x. I understand that the du 's don't simply cancel out because $\frac{dy}{du}$ and $\frac{du}{dx}$ are defined as specific limits making the numerator and denominator infinitesimals and thus making the whole thing indeterminate and inoperable on. But applying the definition of a derivative, we can express the above like so: $$\lim_{\Delta x\to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta u\to 0} \frac{\Delta y}{\Delta u} \cdot \lim_{\Delta x\to 0} \frac{\Delta u}{\Delta x}$$ At this point we can't use the Product Law of Limits to combine the two limits on the right. But if we consider a coordinate system u vs x , doesn't $\Delta u \rightarrow 0$ when $\Delta x \rightarrow 0$ ? And if so,  then whenever $\Delta u \rightarrow 0$ we necessarily have $\Delta x \rightarrow 0$. Then can't we rewrite the above limit equation as: $$\lim_{\Delta x\to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x\to 0} \frac{\Delta y}{\Delta u} \cdot \lim_{\Delta x\to 0} \frac{\Delta u}{\Delta x}$$ And then can't we use the Product Law of Limits to say: $$\lim_{\Delta x\to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x\to 0} \left (  \frac{\Delta y}{\Delta u} \cdot \frac{\Delta u}{\Delta x} \right )$$ And since $\frac{\Delta y}{\Delta u}$ and $\frac{\Delta u}{\Delta x}$ within the quantity who's limit is being taken are no longer ""quotients"" infinitesimals, Δu 's can cancel, leaving us with: $$\lim_{\Delta x\to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x\to 0}   \frac{\Delta y}{\Delta x} $$ Which in Leibniz notations looks like: $$\frac{dy}{dx} = \frac{dy}{dx}$$ Q.E.D (Since we manipulated the right hand side of the equation into looking the same as the left hand side).",,"['calculus', 'limits', 'derivatives', 'recreational-mathematics', 'chain-rule']"
73,To show that $e^x > 1+x$ for any $x\ne 0$ [duplicate],To show that  for any  [duplicate],e^x > 1+x x\ne 0,"This question already has answers here : Simplest or nicest proof that $1+x \le e^x$ (26 answers) Closed 8 years ago . $$e^x>1+x$$ is  what I want to show. So let's define a function: $$h\left(x\right)=e^x-x-1$$ and investigate its derivative: $$h'\left(x\right)=e^x-1$$. Easy to see that at $x=0$ it has a critical point and it is a minimum, and therefore for any other value that isn't $0$ we'll get a value that is bigger then the minimum $f(0)=0$, in other words, for each $x\ne 0$, $h\left(x\right)>0$, meaning $e^x-x-1>0$ meaning that: $$e^x>x+1$$","This question already has answers here : Simplest or nicest proof that $1+x \le e^x$ (26 answers) Closed 8 years ago . $$e^x>1+x$$ is  what I want to show. So let's define a function: $$h\left(x\right)=e^x-x-1$$ and investigate its derivative: $$h'\left(x\right)=e^x-1$$. Easy to see that at $x=0$ it has a critical point and it is a minimum, and therefore for any other value that isn't $0$ we'll get a value that is bigger then the minimum $f(0)=0$, in other words, for each $x\ne 0$, $h\left(x\right)>0$, meaning $e^x-x-1>0$ meaning that: $$e^x>x+1$$",,"['calculus', 'inequality', 'derivatives', 'proof-verification', 'exponential-function']"
74,Determining $\int \frac{(x-1)\sqrt{x^4+2x^3-x^2+2x+1}}{x^2(x+1)}dx$,Determining,\int \frac{(x-1)\sqrt{x^4+2x^3-x^2+2x+1}}{x^2(x+1)}dx,"$$\int \frac{(x-1)\sqrt{x^4+2x^3-x^2+2x+1}}{x^2(x+1)}dx$$ Attempt: Simplification of the root factor: $$\sqrt{x^4+2x^3-x^2+2x+1}=\frac{1}{x}\sqrt{\left(x^2+\frac{1}{x^2}\right)+2\left(x+\frac{1}{x}\right)-1}.$$ Arranging the rest of the factors as: $$\frac{x-1}{x^2(x+1)}=\frac{x^2-1}{x^2(x+1)^2}$$ Now I did the following substitution: let $(x+\frac{1}{x})=t$, so $(1-\frac{1}{x^2})dx=dt$ Arranging the integral: $$I=\int \frac{\sqrt{t^2+2t-3}}{t+2}dt.$$ But what to do next? I tried integration by parts for this but couldn't simplify my result.","$$\int \frac{(x-1)\sqrt{x^4+2x^3-x^2+2x+1}}{x^2(x+1)}dx$$ Attempt: Simplification of the root factor: $$\sqrt{x^4+2x^3-x^2+2x+1}=\frac{1}{x}\sqrt{\left(x^2+\frac{1}{x^2}\right)+2\left(x+\frac{1}{x}\right)-1}.$$ Arranging the rest of the factors as: $$\frac{x-1}{x^2(x+1)}=\frac{x^2-1}{x^2(x+1)^2}$$ Now I did the following substitution: let $(x+\frac{1}{x})=t$, so $(1-\frac{1}{x^2})dx=dt$ Arranging the integral: $$I=\int \frac{\sqrt{t^2+2t-3}}{t+2}dt.$$ But what to do next? I tried integration by parts for this but couldn't simplify my result.",,"['calculus', 'integration', 'indefinite-integrals']"
75,Proof of identity: cross product of three vectors,Proof of identity: cross product of three vectors,,"A book I'm reading contains the following (paraphrased) \begin{equation} (a \times b) \times c = (a \cdot c)b - (b \cdot c)a \end{equation} This is supposed to follow from: \begin{equation} (a \times b) \cdot (c \times d) = (a \cdot c)(b \cdot d) - (a \cdot d)(b \cdot c) \end{equation} Where in both equations $a, b, c, d$ are all vectors. The question is how to prove this?","A book I'm reading contains the following (paraphrased) \begin{equation} (a \times b) \times c = (a \cdot c)b - (b \cdot c)a \end{equation} This is supposed to follow from: \begin{equation} (a \times b) \cdot (c \times d) = (a \cdot c)(b \cdot d) - (a \cdot d)(b \cdot c) \end{equation} Where in both equations $a, b, c, d$ are all vectors. The question is how to prove this?",,"['calculus', 'linear-algebra', 'cross-product']"
76,Showing y≈x for small x if y=log(x+1),Showing y≈x for small x if y=log(x+1),,"Given: $y=\log(1+x)$ Show that $y≈x$ if $x$ gets small (less than 1). I don't think we're supposed to use Taylor series (because they were never formally introduced in class), but I do think we have to differentiate and show that the derivative of $\log(1+x)$ is approximately equal to $\log(1+x)$ on the interval $0$ to $1$. How should I show this?","Given: $y=\log(1+x)$ Show that $y≈x$ if $x$ gets small (less than 1). I don't think we're supposed to use Taylor series (because they were never formally introduced in class), but I do think we have to differentiate and show that the derivative of $\log(1+x)$ is approximately equal to $\log(1+x)$ on the interval $0$ to $1$. How should I show this?",,['calculus']
77,Integrate $\int\frac{dx}{(x^2+1)\sqrt{x^2+2}}$,Integrate,\int\frac{dx}{(x^2+1)\sqrt{x^2+2}},I would like some guidance regarding the following integral: $$\int\frac{dx}{(x^2+1)\sqrt{x^2+2}}$$ EDIT: The upper problem was derived from the following integral $$\int\frac{\sqrt{x^2+2}}{x^2+1}dx$$ Where I rationalized the numerator which followed into: $$\int\frac{dx}{\sqrt{x^2+2}}+\int\frac{dx}{(x^2+1)\sqrt{x^2+2}}$$,I would like some guidance regarding the following integral: $$\int\frac{dx}{(x^2+1)\sqrt{x^2+2}}$$ EDIT: The upper problem was derived from the following integral $$\int\frac{\sqrt{x^2+2}}{x^2+1}dx$$ Where I rationalized the numerator which followed into: $$\int\frac{dx}{\sqrt{x^2+2}}+\int\frac{dx}{(x^2+1)\sqrt{x^2+2}}$$,,"['calculus', 'integration', 'indefinite-integrals']"
78,"Evaluate $\int_0^{2 \pi} \sin \theta \cos^2 \theta \,d\theta$",Evaluate,"\int_0^{2 \pi} \sin \theta \cos^2 \theta \,d\theta","The problem is to integrate $$\int_0^{2 \pi} \sin \theta \cos^2 \theta \,d\theta.$$ What I have tried is substituting $t := \cos \theta$, but the new limits of integration are equal to each other. How do I resolve this?","The problem is to integrate $$\int_0^{2 \pi} \sin \theta \cos^2 \theta \,d\theta.$$ What I have tried is substituting $t := \cos \theta$, but the new limits of integration are equal to each other. How do I resolve this?",,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'trigonometric-integrals']"
79,A closed-form of product the gamma functions containing $\pi$ and $\phi$,A closed-form of product the gamma functions containing  and,\pi \phi,"Playing with gamma functions by randomly inputting numbers to Wolfram Alpha , I got the following beautiful result \begin{equation} \frac{\Gamma\left(\frac{3}{10}\right)\Gamma\left(\frac{4}{10}\right)}{\Gamma\left(\frac{2}{10}\right)}=\frac{\sqrt[\large5]{4}\cdot\sqrt{\pi}}{\phi} \end{equation} where $\phi$ is golden ratio. Could anyone here please help me to prove it by hand? I mean without using table for the specific values of $\Gamma(x)$ except for $\Gamma\left(\frac{1}{2}\right)$. As usual, preferably with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.","Playing with gamma functions by randomly inputting numbers to Wolfram Alpha , I got the following beautiful result \begin{equation} \frac{\Gamma\left(\frac{3}{10}\right)\Gamma\left(\frac{4}{10}\right)}{\Gamma\left(\frac{2}{10}\right)}=\frac{\sqrt[\large5]{4}\cdot\sqrt{\pi}}{\phi} \end{equation} where $\phi$ is golden ratio. Could anyone here please help me to prove it by hand? I mean without using table for the specific values of $\Gamma(x)$ except for $\Gamma\left(\frac{1}{2}\right)$. As usual, preferably with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.",,"['calculus', 'special-functions', 'closed-form', 'gamma-function']"
80,Integrate : $\int(\sin x+\cos x)^ndx$,Integrate :,\int(\sin x+\cos x)^ndx,Problem : $$\int(\sin x+\cos x)^n\ dx$$ I am not getting any clue how to integrate this. Please help . I will be grateful to you. Thanks.,Problem : $$\int(\sin x+\cos x)^n\ dx$$ I am not getting any clue how to integrate this. Please help . I will be grateful to you. Thanks.,,"['calculus', 'integration', 'indefinite-integrals']"
81,Interpreting higher order differentials,Interpreting higher order differentials,,"I'm trying to understand Taylor's Theorem for functions of $n$ variables, but all this higher dimensionality is causing me trouble. One of my problems is understanding the higher order differentials. For example, if I have a function $f(x, y)$, then it's first differential is: $$df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy.$$ To me this quantity is saying that: A differential change in the value of function $f(x,y)$ is equal to how fast function $f(x,y)$ is changing   with respect to $x$ multiplied by a differential change in the    $x$-coordinate plus how fast function $f(x,y)$ is changing with   respect to $y$ multiplied by a differential change in the    $y$-coordinate. This seems intuitive. But when we get into higher order differentials I get confused: $$d^2f= \frac{\partial^2 f}{\partial y ^2}dy^2 + 2\frac{\partial^2 f}{\partial y \partial x}dy\:dx + \frac{\partial^2 f}{\partial x ^2}dx^2$$ How would one interpret this quantity? What about even higher order differentials? say $d^3f$ or $d^{1500 }f$ =) Thank you for any help! =)","I'm trying to understand Taylor's Theorem for functions of $n$ variables, but all this higher dimensionality is causing me trouble. One of my problems is understanding the higher order differentials. For example, if I have a function $f(x, y)$, then it's first differential is: $$df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy.$$ To me this quantity is saying that: A differential change in the value of function $f(x,y)$ is equal to how fast function $f(x,y)$ is changing   with respect to $x$ multiplied by a differential change in the    $x$-coordinate plus how fast function $f(x,y)$ is changing with   respect to $y$ multiplied by a differential change in the    $y$-coordinate. This seems intuitive. But when we get into higher order differentials I get confused: $$d^2f= \frac{\partial^2 f}{\partial y ^2}dy^2 + 2\frac{\partial^2 f}{\partial y \partial x}dy\:dx + \frac{\partial^2 f}{\partial x ^2}dx^2$$ How would one interpret this quantity? What about even higher order differentials? say $d^3f$ or $d^{1500 }f$ =) Thank you for any help! =)",,"['calculus', 'multivariable-calculus', 'derivatives', 'taylor-expansion']"
82,"Can you find this limit in a ""nicer"" way?","Can you find this limit in a ""nicer"" way?",,"I'm trying to show that: $$\lim\limits_{n\to\infty}{n(\sqrt[n]{n}-1)} = \infty$$ From what I've tried now, all I end up with is basically rewriting the left term as: $$\lim\limits_{n\to\infty}{\frac{\sqrt[n]{n}-1}{\frac{1}{n}}}$$ and then applying De L'Hôpital's rule (which gets really messy considering that we're deriving $\sqrt[n]{n}$, since $\frac{d}{dn}\sqrt[n]{n} = -n^{\frac{1}{n}-2}(\ln(n)-1)$). Is there any ""nice and quick"" way to solve this?","I'm trying to show that: $$\lim\limits_{n\to\infty}{n(\sqrt[n]{n}-1)} = \infty$$ From what I've tried now, all I end up with is basically rewriting the left term as: $$\lim\limits_{n\to\infty}{\frac{\sqrt[n]{n}-1}{\frac{1}{n}}}$$ and then applying De L'Hôpital's rule (which gets really messy considering that we're deriving $\sqrt[n]{n}$, since $\frac{d}{dn}\sqrt[n]{n} = -n^{\frac{1}{n}-2}(\ln(n)-1)$). Is there any ""nice and quick"" way to solve this?",,"['calculus', 'limits', 'derivatives']"
83,"Find the value of $\lim_{n\to \infty } \,\cos (1) \cos \left(\frac{1}{2}\right) \cos \left(\frac{1}{4}\right)\cdots \cos \left(\frac{1}{2^n}\right)$ [duplicate]",Find the value of  [duplicate],"\lim_{n\to \infty } \,\cos (1) \cos \left(\frac{1}{2}\right) \cos \left(\frac{1}{4}\right)\cdots \cos \left(\frac{1}{2^n}\right)","This question already has answers here : Finding the limit $\lim \limits_{n \to \infty}\ (\cos \frac x 2 \cdot\cos \frac x 4\cdot \cos \frac x 8\cdots \cos \frac x {2^n}) $ (3 answers) Closed 10 years ago . $$\lim_{n\to \infty } \,\cos (1) \cos \left(\frac{1}{2}\right) \cos \left(\frac{1}{4}\right)\cdots \cos \left(\frac{1}{2^n}\right)$$ How would you evaluate this limit? Is it just equivalent to $$\prod_{n=0}^\infty {\cos \left( \frac{1}{2^{n}} \right)}$$","This question already has answers here : Finding the limit $\lim \limits_{n \to \infty}\ (\cos \frac x 2 \cdot\cos \frac x 4\cdot \cos \frac x 8\cdots \cos \frac x {2^n}) $ (3 answers) Closed 10 years ago . $$\lim_{n\to \infty } \,\cos (1) \cos \left(\frac{1}{2}\right) \cos \left(\frac{1}{4}\right)\cdots \cos \left(\frac{1}{2^n}\right)$$ How would you evaluate this limit? Is it just equivalent to $$\prod_{n=0}^\infty {\cos \left( \frac{1}{2^{n}} \right)}$$",,"['calculus', 'limits']"
84,Find a point on a parabola that's closest to another point.,Find a point on a parabola that's closest to another point.,,"Find the point on the parabola $3x^2+4x-8$ that is closest to the point $(-2,-3)$. My plan for this problem was to use the distance formula and then that the derivative to get my answer. I'm having a little trouble along the way. $$ d = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}.$$","Find the point on the parabola $3x^2+4x-8$ that is closest to the point $(-2,-3)$. My plan for this problem was to use the distance formula and then that the derivative to get my answer. I'm having a little trouble along the way. $$ d = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}.$$",,['calculus']
85,"For a convex function, the average value lies between $f((a+b)/2)$ and $(f(a) + f(b))/2$","For a convex function, the average value lies between  and",f((a+b)/2) (f(a) + f(b))/2,"Suppose that $f\in C^2$, $f''(x)\geq 0$ $\,\,\,\forall x \in [a,b]$. I want to show that $$\frac{1}{2}(b-a)(f(a)+f(b))\leq \int_a^bf(t)\,dt\leq (b-a)f\left(\frac{a+b}{2}\right).$$If we divide by $b-a$, we see that the left term is less than the right term by definition of convexity, and it remains to show that the average value of the function lies between $f\left(\frac{a+b}{2}\right)$ and $\frac{1}{2}(f(a)+f(b))$. The mean value theorem for integrals implies that the average value is attained at some point $c\in (a,b)$. But it's not clear to me why $f(c)$ should lie in between two other points. Perhaps there's another theorem about integration we should apply. Any ideas?","Suppose that $f\in C^2$, $f''(x)\geq 0$ $\,\,\,\forall x \in [a,b]$. I want to show that $$\frac{1}{2}(b-a)(f(a)+f(b))\leq \int_a^bf(t)\,dt\leq (b-a)f\left(\frac{a+b}{2}\right).$$If we divide by $b-a$, we see that the left term is less than the right term by definition of convexity, and it remains to show that the average value of the function lies between $f\left(\frac{a+b}{2}\right)$ and $\frac{1}{2}(f(a)+f(b))$. The mean value theorem for integrals implies that the average value is attained at some point $c\in (a,b)$. But it's not clear to me why $f(c)$ should lie in between two other points. Perhaps there's another theorem about integration we should apply. Any ideas?",,"['calculus', 'integration', 'derivatives']"
86,For which $\alpha$ this sum converges? $\sum_{n=3}^{\infty} {\frac{1}{n \cdot \ln(n) \cdot \ln(\ln(n))^{\alpha}}}$,For which  this sum converges?,\alpha \sum_{n=3}^{\infty} {\frac{1}{n \cdot \ln(n) \cdot \ln(\ln(n))^{\alpha}}},"Given: $$\sum_{n=3}^{\infty} {\frac{1}{n \cdot \ln(n) \cdot \ln(\ln(n))^{\alpha}}}$$ I am asked: For what values of $\alpha$ does this summation converge? So I said, $f(n) = \frac{1}{n \cdot \ln(n) \cdot \ln(\ln(n))^{\alpha}}$. $f(n)$ is obviously monotonically decreasing function. Then, by using the integral test , this summation converges if and only if $I = \int_{3}^{\infty} {\frac{dn}{n \cdot \ln(n) \cdot \ln(\ln(n))^{\alpha}}}$ converges. (has a value) But I am finding this very hard to go on with. Any direction will be appreciated!","Given: $$\sum_{n=3}^{\infty} {\frac{1}{n \cdot \ln(n) \cdot \ln(\ln(n))^{\alpha}}}$$ I am asked: For what values of $\alpha$ does this summation converge? So I said, $f(n) = \frac{1}{n \cdot \ln(n) \cdot \ln(\ln(n))^{\alpha}}$. $f(n)$ is obviously monotonically decreasing function. Then, by using the integral test , this summation converges if and only if $I = \int_{3}^{\infty} {\frac{dn}{n \cdot \ln(n) \cdot \ln(\ln(n))^{\alpha}}}$ converges. (has a value) But I am finding this very hard to go on with. Any direction will be appreciated!",,['calculus']
87,"if $\sum a_{n}$ is a convergent series, what about $\sum \frac{a_{n}}{1+|a_{n}|}$?","if  is a convergent series, what about ?",\sum a_{n} \sum \frac{a_{n}}{1+|a_{n}|},"Suppose that $\sum a_{n}$ is convergent series of real numbers. Either prove that $\sum b_{n}$ converges, or give a counter-example, when we define $b_{n}$ by $\frac{a_{n}}{1+|a_{n}|}$.","Suppose that $\sum a_{n}$ is convergent series of real numbers. Either prove that $\sum b_{n}$ converges, or give a counter-example, when we define $b_{n}$ by $\frac{a_{n}}{1+|a_{n}|}$.",,['calculus']
88,I need to prove that $f$ continuous at $(x)=0$ using a $\epsilon$-$\delta$ proof,I need to prove that  continuous at  using a - proof,f (x)=0 \epsilon \delta,"I need to prove that $f$  continuous at $(x)=0$ using a $\epsilon$-$\delta$ proof $$   f(x, y) = \begin{cases} x^2sin(\frac1x),&x\neq 0 \\ 0,&x = 0 \end{cases} $$","I need to prove that $f$  continuous at $(x)=0$ using a $\epsilon$-$\delta$ proof $$   f(x, y) = \begin{cases} x^2sin(\frac1x),&x\neq 0 \\ 0,&x = 0 \end{cases} $$",,"['calculus', 'limits', 'continuity']"
89,"Show this equation has at least one root in $(0,1)$",Show this equation has at least one root in,"(0,1)","Let $ax^2+bx+c=0$ be a quadratic equation, where $a,b,c\in\mathbb{R}$. If $2a+3b+6c=0$, then show that this equation will have atleast one root in $(0,1)$. I think it involves either Rolle's Theorem or Lagrange's Mean Value Theorem, but can't think further. Please help, and yes, thanks in advance!","Let $ax^2+bx+c=0$ be a quadratic equation, where $a,b,c\in\mathbb{R}$. If $2a+3b+6c=0$, then show that this equation will have atleast one root in $(0,1)$. I think it involves either Rolle's Theorem or Lagrange's Mean Value Theorem, but can't think further. Please help, and yes, thanks in advance!",,['calculus']
90,Prove that a continuous function $f:\mathbb R\to \mathbb R$ is injective if and only if it has no extrema,Prove that a continuous function  is injective if and only if it has no extrema,f:\mathbb R\to \mathbb R,"I have a homework problem where I need to prove that a continuous function $f:\mathbb R\to \mathbb R$ is injective if and only if it has no extrema (local or global). So far what I have is: We'll assume that $f$ is injective and assume that it has an extrema, $(x_{0}, f(x_{0}))$. Since $x_0$ is an extrema, there is a neighborhood of $x_0$ such that for each $x$ in the neighborhood, $f(x)\leq f(x_0)$ or $f(x)\geq f(x_0)$. This is where I got stuck. Intuitively I understand that that on each side of $x_0$, there have to be two points whose values are the same, which contradicts the assumption that $f$ is injective - I'm having extreme difficulty proving it, though. If we assume that $f$ has no extrema, then I've tried to show that it is also a strictly monotonic function, but I'm having difficulty proving this as well. I'd appreciate any help.","I have a homework problem where I need to prove that a continuous function $f:\mathbb R\to \mathbb R$ is injective if and only if it has no extrema (local or global). So far what I have is: We'll assume that $f$ is injective and assume that it has an extrema, $(x_{0}, f(x_{0}))$. Since $x_0$ is an extrema, there is a neighborhood of $x_0$ such that for each $x$ in the neighborhood, $f(x)\leq f(x_0)$ or $f(x)\geq f(x_0)$. This is where I got stuck. Intuitively I understand that that on each side of $x_0$, there have to be two points whose values are the same, which contradicts the assumption that $f$ is injective - I'm having extreme difficulty proving it, though. If we assume that $f$ has no extrema, then I've tried to show that it is also a strictly monotonic function, but I'm having difficulty proving this as well. I'd appreciate any help.",,['calculus']
91,Improper use of Stokes and Divergence Theorem. Find the problem,Improper use of Stokes and Divergence Theorem. Find the problem,,"Could someone point out what is wrong with this equality? Assume that $\mathbf{F}$ is continuous (and hence, its partial derivatives). $$\begin{align} \oint \mathbf{F}\cdot d\mathbf{s} & =^\text{by Stokes} \iint_S \nabla \times \mathbf{F} \cdot d\mathbf{S} \\ &=^\text{by Div} \iiint_V \nabla\cdot( \nabla \times \mathbf{F} ) \, dV \\ &=\iiint_V 0 \,dV \\ &=0\\ &\implies \oint \mathbf{F}\cdot d\mathbf{s}= 0 \; \forall \mathbf{F} \end{align}$$ Since we assumed $\mathbf{F}$ and its partials are all continuous. But obviously this is wrong if $\mathbf{F}$ is non-conservative. But everything seems to agree. What went wrong? EDIT . For a refinement of the problem. Let me specifically state that $S$ is a closed surface with a boundary curve that is also closed. So $V$ here is the volume of that surface and since $S$ is closed it has a volume","Could someone point out what is wrong with this equality? Assume that $\mathbf{F}$ is continuous (and hence, its partial derivatives). $$\begin{align} \oint \mathbf{F}\cdot d\mathbf{s} & =^\text{by Stokes} \iint_S \nabla \times \mathbf{F} \cdot d\mathbf{S} \\ &=^\text{by Div} \iiint_V \nabla\cdot( \nabla \times \mathbf{F} ) \, dV \\ &=\iiint_V 0 \,dV \\ &=0\\ &\implies \oint \mathbf{F}\cdot d\mathbf{s}= 0 \; \forall \mathbf{F} \end{align}$$ Since we assumed $\mathbf{F}$ and its partials are all continuous. But obviously this is wrong if $\mathbf{F}$ is non-conservative. But everything seems to agree. What went wrong? EDIT . For a refinement of the problem. Let me specifically state that $S$ is a closed surface with a boundary curve that is also closed. So $V$ here is the volume of that surface and since $S$ is closed it has a volume",,"['calculus', 'multivariable-calculus']"
92,"How to integrate $\int \frac{e^x dx}{1\,+\,e^{2x}}$",How to integrate,"\int \frac{e^x dx}{1\,+\,e^{2x}}","Ok, I give up, I have tried with $u$-substitution and integration by parts but I can't solve it. The integral is: $$\int{\frac{e^x dx}{1+e^{2x}}}$$ I have tried $u=e^x$, $u=e^{2x}$ and also integration by parts but I can't solve it. The result should be: $$\arctan(e^x)$$","Ok, I give up, I have tried with $u$-substitution and integration by parts but I can't solve it. The integral is: $$\int{\frac{e^x dx}{1+e^{2x}}}$$ I have tried $u=e^x$, $u=e^{2x}$ and also integration by parts but I can't solve it. The result should be: $$\arctan(e^x)$$",,"['calculus', 'integration', 'exponential-function']"
93,"If $\sum\limits_{k=1}^{\infty}a_k=S $, then $ a_4+a_3+a_2+a_1+a_8+a_7+a_6+a_5+\dots=?$","If , then",\sum\limits_{k=1}^{\infty}a_k=S   a_4+a_3+a_2+a_1+a_8+a_7+a_6+a_5+\dots=?,"if we know that $\sum\limits_{k=1}^{\infty}a_k=S$,  what can we say about the convergence of $$a_4+a_3+a_2+a_1+a_8+a_7+a_6+a_5+a_{12}+a_{11}+a_{10}+a_{9}+\dots$$ ? If it does converges, what is the sum (in terms of $S$)? As per the first question -  it clearly converges since the number of terms in each parentheses is bounded (by 4) and the $(a_n)_{n=1}^\infty$ tends to zero as $n\to\infty$. Second question is where I'm struggling. We don't know that $\sum\limits_{k=1}^{\infty}a_k$ absolutely converges so I don't know what can we say about it's sum. Thanks for your help.","if we know that $\sum\limits_{k=1}^{\infty}a_k=S$,  what can we say about the convergence of $$a_4+a_3+a_2+a_1+a_8+a_7+a_6+a_5+a_{12}+a_{11}+a_{10}+a_{9}+\dots$$ ? If it does converges, what is the sum (in terms of $S$)? As per the first question -  it clearly converges since the number of terms in each parentheses is bounded (by 4) and the $(a_n)_{n=1}^\infty$ tends to zero as $n\to\infty$. Second question is where I'm struggling. We don't know that $\sum\limits_{k=1}^{\infty}a_k$ absolutely converges so I don't know what can we say about it's sum. Thanks for your help.",,"['calculus', 'sequences-and-series']"
94,"How to prove $\int_0^1 \frac{1+x^{30}}{1+x^{60}} dx = 1 + \frac{c}{31}$, where $0 < c < 1$","How to prove , where",\int_0^1 \frac{1+x^{30}}{1+x^{60}} dx = 1 + \frac{c}{31} 0 < c < 1,"This is an exercise from Apostol (p.285) that I'm having trouble with (in fact, I'm having trouble with the whole section): Prove that $\displaystyle{\int_0^1 \frac{1+x^{30}}{1+x^{60}} = 1 + \frac{c}{31}}, \qquad \text{where } 0 < c < 1.$ This comes from the section of Exercises following Taylor expansions, and Taylor's formula with error term.  It seems like the approach should involve getting this as the error term of the Taylor expansion of a function that we know something about?  I'm having trouble making much more progress than that. Updated Progress: From the definition of the error term of the Taylor expansion we have: $$E_n (x) = \frac{1}{n!} \int_a^x (x-t)^n f^{(n+1)} (t) dt$$ Alternatively, we may express this in the Lagrange form of the error term (derived from the weighted mean value theorem of integrals): $$E_n (x) = \frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1} \qquad \text{where } a < c < x.$$ Now, let $a = 0$, $x = 1$, $n = 30$, $f^{(n+1)}(t) = \dfrac{1+t^{30}}{(1+t^{60})(1-t)^{30}}$, and set the two alternative expressions of $E_n(x)$ equal to each other: $$\begin{align*} \implies & \frac{1}{n!} \int_a^x (x-t)^n f^{(n+1)}(t) dt & = & \dfrac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1}\\ \implies & \frac{1}{30!} \int_0^1 (1-t)^{30} \dfrac{1+t^{30}}{(1+t^{60})(1-t)^{30}} & = & \dfrac{1}{31!} \dfrac{1+c^{30}}{(1+c^{60})(1-c)^{30}}\\ \implies & \int_0^1 \dfrac{1+t^{30}}{1+t^{60}} & = & \frac{1}{31} \frac{1+c^{30}}{(1+c^{60})(1-c)^{30}}. \end{align*} $$ I cannot seem to get from here to $=1 + \dfrac{c}{31}$. If someone can show me pretty explicitly, step-by-step how to tackle this problem, I'd appreciate it.  Dealing with the error term on Taylor expansions is giving me quite a bit of trouble, so hopefully seeing a full solution will help clear things up.","This is an exercise from Apostol (p.285) that I'm having trouble with (in fact, I'm having trouble with the whole section): Prove that $\displaystyle{\int_0^1 \frac{1+x^{30}}{1+x^{60}} = 1 + \frac{c}{31}}, \qquad \text{where } 0 < c < 1.$ This comes from the section of Exercises following Taylor expansions, and Taylor's formula with error term.  It seems like the approach should involve getting this as the error term of the Taylor expansion of a function that we know something about?  I'm having trouble making much more progress than that. Updated Progress: From the definition of the error term of the Taylor expansion we have: $$E_n (x) = \frac{1}{n!} \int_a^x (x-t)^n f^{(n+1)} (t) dt$$ Alternatively, we may express this in the Lagrange form of the error term (derived from the weighted mean value theorem of integrals): $$E_n (x) = \frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1} \qquad \text{where } a < c < x.$$ Now, let $a = 0$, $x = 1$, $n = 30$, $f^{(n+1)}(t) = \dfrac{1+t^{30}}{(1+t^{60})(1-t)^{30}}$, and set the two alternative expressions of $E_n(x)$ equal to each other: $$\begin{align*} \implies & \frac{1}{n!} \int_a^x (x-t)^n f^{(n+1)}(t) dt & = & \dfrac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1}\\ \implies & \frac{1}{30!} \int_0^1 (1-t)^{30} \dfrac{1+t^{30}}{(1+t^{60})(1-t)^{30}} & = & \dfrac{1}{31!} \dfrac{1+c^{30}}{(1+c^{60})(1-c)^{30}}\\ \implies & \int_0^1 \dfrac{1+t^{30}}{1+t^{60}} & = & \frac{1}{31} \frac{1+c^{30}}{(1+c^{60})(1-c)^{30}}. \end{align*} $$ I cannot seem to get from here to $=1 + \dfrac{c}{31}$. If someone can show me pretty explicitly, step-by-step how to tackle this problem, I'd appreciate it.  Dealing with the error term on Taylor expansions is giving me quite a bit of trouble, so hopefully seeing a full solution will help clear things up.",,['calculus']
95,Computing $f(x)$ if $\int_1^{xy} f(t) dt = y \int_1^x f(t) dt + x\int_1^y f(t) dt$ and $f(1) = 3$,Computing  if  and,f(x) \int_1^{xy} f(t) dt = y \int_1^x f(t) dt + x\int_1^y f(t) dt f(1) = 3,"This is a problem from Apostol, Calculus, Volume I , Chapter 6.9 (p. 238), that I was hoping someone could help with: A function $f$, continuous on the positive real axis, has the property that $$\int_1^{xy} f(t) dt = y \int_1^x f(t) dt + x \int_1^y f(t) dt$$   for all $x > 0$ and all $y > 0$.  If $f(1) = 3$, compute $f(x)$ for each $x > 0$. My initial thought was to use properties of the integral to write: $$\begin{align*}  \int_1^{xy} f(t) dt & = \int_1^x f(t) dt + \int_x^{xy} f(t) dt \\ & = \int_1^x f(t) dt + x \int_1^y f(xt) dt \end{align*}$$ and $$\begin{align*} \int_1^{xy} f(t) dt & = \int_1^y f(t) dt + \int_y^{xy} f(t) dt \\ & = \int_1^y f(t) dt + y \int_1^x f(yt) dt \end{align*}$$ Then each of these is also $=y \int_1^x f(t)dt + x \int_1^ f(t)dt$ from the given equation of the problem.  I cannot seem to pull an equation for $f(x)$ out of this though (I might be missing some very simple manipulation...). I also thought in terms of letting $A(x) = \int_1^x f(t)dt$, so the problem is giving us a functional equation: $$ A(xy) = yA(x) + xA(y)$$ This has obvious similarities to the functional equation of the logarithm. This problem is in the section of the text with the definition of the logarithm as an integral.  I'm self-studying, so this isn't homework.  Hints or full solutions are equally welcome.  Thanks for the help.","This is a problem from Apostol, Calculus, Volume I , Chapter 6.9 (p. 238), that I was hoping someone could help with: A function $f$, continuous on the positive real axis, has the property that $$\int_1^{xy} f(t) dt = y \int_1^x f(t) dt + x \int_1^y f(t) dt$$   for all $x > 0$ and all $y > 0$.  If $f(1) = 3$, compute $f(x)$ for each $x > 0$. My initial thought was to use properties of the integral to write: $$\begin{align*}  \int_1^{xy} f(t) dt & = \int_1^x f(t) dt + \int_x^{xy} f(t) dt \\ & = \int_1^x f(t) dt + x \int_1^y f(xt) dt \end{align*}$$ and $$\begin{align*} \int_1^{xy} f(t) dt & = \int_1^y f(t) dt + \int_y^{xy} f(t) dt \\ & = \int_1^y f(t) dt + y \int_1^x f(yt) dt \end{align*}$$ Then each of these is also $=y \int_1^x f(t)dt + x \int_1^ f(t)dt$ from the given equation of the problem.  I cannot seem to pull an equation for $f(x)$ out of this though (I might be missing some very simple manipulation...). I also thought in terms of letting $A(x) = \int_1^x f(t)dt$, so the problem is giving us a functional equation: $$ A(xy) = yA(x) + xA(y)$$ This has obvious similarities to the functional equation of the logarithm. This problem is in the section of the text with the definition of the logarithm as an integral.  I'm self-studying, so this isn't homework.  Hints or full solutions are equally welcome.  Thanks for the help.",,[]
96,"Trig integral $\int ( \cos{x} + \sin{x}\cos{x}) \, dx $",Trig integral,"\int ( \cos{x} + \sin{x}\cos{x}) \, dx ","Assume we have: $$ \int (\cos{x} + \sin{x}\cos{x}) \, dx$$ Two ways to do it: Use $$\sin{x}\cos{x} = \frac{ \sin{2x} }{2} $$ Then $$ \int \left(\cos{x} + \frac{\sin{2x}}{2} \right) \, dx = \sin{x} - \frac{ \cos{2x} }{ 4 } + C. $$ The other way, just see that $ u = \sin(x), du = \cos(x)dx $ , $$ \int ( \cos{x} + \sin{x}\cos{x}) \, dx = \sin{x} + \frac{\sin^2{x}}{2} + C. $$ Now the part I don't see fully is, why aren't these results completely equal? Taking the 2nd result, \begin{align} \sin{x} + \frac{\sin^2{x}}{2} &= \sin{x} + \frac{1}{2} \, \left( \frac{1 - \cos{2x}}{2} \right) \\ &= \sin{x} + \frac{1}{4} - \frac{\cos{2x}}{4}. \end{align} So you have to absorb $\frac{1}{4}$ into $C$ for them to be equal. Shouldn't they be equal straight away?","Assume we have: Two ways to do it: Use Then The other way, just see that , Now the part I don't see fully is, why aren't these results completely equal? Taking the 2nd result, So you have to absorb into for them to be equal. Shouldn't they be equal straight away?"," \int (\cos{x} + \sin{x}\cos{x}) \, dx \sin{x}\cos{x} = \frac{ \sin{2x} }{2}   \int \left(\cos{x} + \frac{\sin{2x}}{2} \right) \, dx = \sin{x} - \frac{ \cos{2x} }{ 4 } + C.   u = \sin(x), du = \cos(x)dx   \int ( \cos{x} + \sin{x}\cos{x}) \, dx = \sin{x} + \frac{\sin^2{x}}{2} + C.  \begin{align}
\sin{x} + \frac{\sin^2{x}}{2} &= \sin{x} + \frac{1}{2} \, \left( \frac{1 - \cos{2x}}{2} \right) \\
&= \sin{x} + \frac{1}{4} - \frac{\cos{2x}}{4}.
\end{align} \frac{1}{4} C","['calculus', 'integration', 'trigonometry']"
97,What sort of mathematical methods and models are used to model the brain,What sort of mathematical methods and models are used to model the brain,,"What sorts of mathematical tools, models and methods and theoretical frameworks do people use to simulate the function of the brain's neural networks? What mathematical properties do different brains have?","What sorts of mathematical tools, models and methods and theoretical frameworks do people use to simulate the function of the brain's neural networks? What mathematical properties do different brains have?",,"['calculus', 'general-topology', 'graph-theory', 'mathematical-modeling', 'network-flow']"
98,Example of a smooth function with zero derivative that is not constant,Example of a smooth function with zero derivative that is not constant,,"One of false beliefs in this question on Math Overflow is ""If f is a smooth function with df=0, then f is constant"". What is a counterexample to this statement? Can it be made correct by adding some restriction, e.g. that f is a function from reals to reals?","One of false beliefs in this question on Math Overflow is ""If f is a smooth function with df=0, then f is constant"". What is a counterexample to this statement? Can it be made correct by adding some restriction, e.g. that f is a function from reals to reals?",,['calculus']
99,Evaluating the integral $\int\limits_{0}^{\infty} \Bigl\lfloor{\frac{n}{e^{x}}\Bigr\rfloor} \ dx $,Evaluating the integral,\int\limits_{0}^{\infty} \Bigl\lfloor{\frac{n}{e^{x}}\Bigr\rfloor} \ dx ,"How to evaluate this integral: $$\int_{0}^{\infty} \biggl\lfloor{\frac{n}{e^{x}}\biggr\rfloor} \ dx, $$where $n \in \mathbb{N}$. The same integral when asked to evaluate for $n=2$ (say) i can do it by splitting the limits from $x = 0$ to $x = \log{2}$ where $e^{x}$ takes the value 1, and then from $x= \log{2}$ to $x = \infty$. But how to do this for the general case $n$. I thought of two ways: Using induction on $n$. This will not work! Splitting the limits from $x =0$, to $x = \log{n}$ also seems to cause some problems for me.","How to evaluate this integral: $$\int_{0}^{\infty} \biggl\lfloor{\frac{n}{e^{x}}\biggr\rfloor} \ dx, $$where $n \in \mathbb{N}$. The same integral when asked to evaluate for $n=2$ (say) i can do it by splitting the limits from $x = 0$ to $x = \log{2}$ where $e^{x}$ takes the value 1, and then from $x= \log{2}$ to $x = \infty$. But how to do this for the general case $n$. I thought of two ways: Using induction on $n$. This will not work! Splitting the limits from $x =0$, to $x = \log{n}$ also seems to cause some problems for me.",,['calculus']
