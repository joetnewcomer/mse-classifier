,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is there a way to solve this differential equation with two functions?,Is there a way to solve this differential equation with two functions?,,"Is there a method to simplify a differential equation with two variables? Specifically I am looking for a way to relate x(t) and y(t) in the following equation and I have a strong urge to say x(t)=y(t) but obviously there are answers where x(t)$\neq$y(t) satisfying the condition. How would you suggest to approach this problem? Or is it impossible to relate x(t) and y(t) simpler than the given equation? $$\frac{x''(t)}{x(t)} = \frac{y''(t)}{y(t)}$$ note: sorry for the tags, I don't know which category this DE falls into.","Is there a method to simplify a differential equation with two variables? Specifically I am looking for a way to relate x(t) and y(t) in the following equation and I have a strong urge to say x(t)=y(t) but obviously there are answers where x(t)$\neq$y(t) satisfying the condition. How would you suggest to approach this problem? Or is it impossible to relate x(t) and y(t) simpler than the given equation? $$\frac{x''(t)}{x(t)} = \frac{y''(t)}{y(t)}$$ note: sorry for the tags, I don't know which category this DE falls into.",,"['ordinary-differential-equations', 'partial-differential-equations', 'differential']"
1,Trigonometric identities from differential equation,Trigonometric identities from differential equation,,"If one knows that the solutions of $y''+ y=0$ are two functions $s(x)$ and $c(x)$, and we know that $s(0)=0$, $s'(0)=1$, $c(0)=1$, $c'(0)=0$, then how can one start to prove that $s(x+a)=s(x)c(a)+c(x)s(a)$? What is it that has to be observed in order for one to understand how to initiate the proof? One is not supposed to use the actual trigonometric functions in this case. Here we assume that we know nothing about sin and cos.","If one knows that the solutions of $y''+ y=0$ are two functions $s(x)$ and $c(x)$, and we know that $s(0)=0$, $s'(0)=1$, $c(0)=1$, $c'(0)=0$, then how can one start to prove that $s(x+a)=s(x)c(a)+c(x)s(a)$? What is it that has to be observed in order for one to understand how to initiate the proof? One is not supposed to use the actual trigonometric functions in this case. Here we assume that we know nothing about sin and cos.",,"['ordinary-differential-equations', 'trigonometry']"
2,Prove that $H(x)=x^Tx$ is constant along solutions of the system,Prove that  is constant along solutions of the system,H(x)=x^Tx,"Consider the diff. equation $\dot{x}=A(x)x$ where $A(x)$ is a matrix with real values of dimension n x n. Prove that the function $H(x)=x^Tx$ is constant, along solutions of the system if $A(x)^T+A(x)=0$. Also prove that the origin is Lyaponov stable fixed point if $A(x)^T+A(x)<0$ The function is constant if the derivative is set equal to zero. I have really no idea how to solve this and I would appreciate it a lot if someone could take a minute to show me how to prove this so that I can learn from it.","Consider the diff. equation $\dot{x}=A(x)x$ where $A(x)$ is a matrix with real values of dimension n x n. Prove that the function $H(x)=x^Tx$ is constant, along solutions of the system if $A(x)^T+A(x)=0$. Also prove that the origin is Lyaponov stable fixed point if $A(x)^T+A(x)<0$ The function is constant if the derivative is set equal to zero. I have really no idea how to solve this and I would appreciate it a lot if someone could take a minute to show me how to prove this so that I can learn from it.",,"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
3,How to solve this Riccati equation $4xy'=4x^2y^2-4y-1$?,How to solve this Riccati equation ?,4xy'=4x^2y^2-4y-1,I am trying to solve the equation below $$4xy'=4x^2y^2-4y-1$$ So far I am having a hard time in spotting a specific solution which I can use to find the general solution. Any ideas?,I am trying to solve the equation below $$4xy'=4x^2y^2-4y-1$$ So far I am having a hard time in spotting a specific solution which I can use to find the general solution. Any ideas?,,['ordinary-differential-equations']
4,Different formulations of chemical kinetics giving different solution trajectories,Different formulations of chemical kinetics giving different solution trajectories,,"I am reading a textbook by Keith J. Laidler titled 'Chemical Kinetics' (3rd ed.). Two different differential forms are given for the reaction (pp30, pp38): $ 2A \leftrightarrow B $ with forward rate $k_1$ and backward rate $k_2$. $\frac{dx}{dt} = k_1(a_0 - 2x)^2 - k_2x $ $\frac{dx}{dt} = k_1(a_0 - x) - k_2 (x/2) $ The second formulation is intuitive to me i.e. A will lose 1 molecule to produce 1/2 molecule of B. In the first, I am bit confused about $(a_0 - 2x)^2$ term, shouldn't it be $(a_0-x)^2$? I was hoping to get the same solution trajectories for both equations, but I am not. With values $a_0=1$, $k_1 = 2$ and $k_2 = 1$, solution at $t=10$ for the first equation is approximately 0.3048 while for second, it is 0.8.","I am reading a textbook by Keith J. Laidler titled 'Chemical Kinetics' (3rd ed.). Two different differential forms are given for the reaction (pp30, pp38): $ 2A \leftrightarrow B $ with forward rate $k_1$ and backward rate $k_2$. $\frac{dx}{dt} = k_1(a_0 - 2x)^2 - k_2x $ $\frac{dx}{dt} = k_1(a_0 - x) - k_2 (x/2) $ The second formulation is intuitive to me i.e. A will lose 1 molecule to produce 1/2 molecule of B. In the first, I am bit confused about $(a_0 - 2x)^2$ term, shouldn't it be $(a_0-x)^2$? I was hoping to get the same solution trajectories for both equations, but I am not. With values $a_0=1$, $k_1 = 2$ and $k_2 = 1$, solution at $t=10$ for the first equation is approximately 0.3048 while for second, it is 0.8.",,"['ordinary-differential-equations', 'chemistry']"
5,Help with a contradiction I found in an equation group,Help with a contradiction I found in an equation group,,"The following is a homework problem but I seem to find a contradiction in it. My problem By the third equation, $\frac{{\partial u}}{{\partial t}} = f'(x + t) - g'(x - t)$, and $\frac{{\partial u}}{{\partial t}}(x,0) = f'(x) - g'(x)=0$, thus $f(x) - g(x) = C$ for some constant $C$. Note here $f'(x),g'(x)$ are differentiations with respect to $x$, not $t$. Together with the second equation, we further have $f(x) = \frac{1}{2}(1 + C - |x|)$, $g(x) = \frac{1}{2}(1 - C- |x|)$. Now by the first equation, $u(1,t) = f(1 + t) + g(1 - t) = \frac{1}{2}(1 + C - |1 + t|) + \frac{1}{2}(1 - C - |1 - t|) = 0$, which gives $|1 + t| + |1 - t| = 2$. Apparently, this does not hold for all $t>0$ . Is the problem indeed having some ""problem"", or I go wrong somewhere in the above calculation? Thank you!","The following is a homework problem but I seem to find a contradiction in it. My problem By the third equation, $\frac{{\partial u}}{{\partial t}} = f'(x + t) - g'(x - t)$, and $\frac{{\partial u}}{{\partial t}}(x,0) = f'(x) - g'(x)=0$, thus $f(x) - g(x) = C$ for some constant $C$. Note here $f'(x),g'(x)$ are differentiations with respect to $x$, not $t$. Together with the second equation, we further have $f(x) = \frac{1}{2}(1 + C - |x|)$, $g(x) = \frac{1}{2}(1 - C- |x|)$. Now by the first equation, $u(1,t) = f(1 + t) + g(1 - t) = \frac{1}{2}(1 + C - |1 + t|) + \frac{1}{2}(1 - C - |1 - t|) = 0$, which gives $|1 + t| + |1 - t| = 2$. Apparently, this does not hold for all $t>0$ . Is the problem indeed having some ""problem"", or I go wrong somewhere in the above calculation? Thank you!",,['ordinary-differential-equations']
6,Homogenous differential equation of the first order,Homogenous differential equation of the first order,,"I cannot solve this question... excuse my bad english! Boiling water cools down proportionally to the temperature the water has at the specific point of time. What temperature does the water have after 8 hours if it has a temperature of 60 degrees Celsius after 4 hours. This is how far I have got: $T'(t) = -kT$.   $k$ is a constant, $T$ is the temperature after $t$ hours. $T'(t) + kT = 0$ $T = Ce^{-kt}$     general solution, $C$ is a constant $T(4) = Ce^{-4k}$ $60 = Ce^{-4k}$ What is $T(8)$??? Thanks in advance!","I cannot solve this question... excuse my bad english! Boiling water cools down proportionally to the temperature the water has at the specific point of time. What temperature does the water have after 8 hours if it has a temperature of 60 degrees Celsius after 4 hours. This is how far I have got: $T'(t) = -kT$.   $k$ is a constant, $T$ is the temperature after $t$ hours. $T'(t) + kT = 0$ $T = Ce^{-kt}$     general solution, $C$ is a constant $T(4) = Ce^{-4k}$ $60 = Ce^{-4k}$ What is $T(8)$??? Thanks in advance!",,['ordinary-differential-equations']
7,Perpendicular Gradients,Perpendicular Gradients,,Suppose $f:\mathbb{R}^2\to \mathbb{R}$ is smooth. Further suppose $\nabla f$ vanishes no where. When is it possible to find a smooth non-singular $g:\mathbb{R}^2\to \mathbb{R}$ satisfying $\nabla f\cdot \nabla g = 0$?,Suppose $f:\mathbb{R}^2\to \mathbb{R}$ is smooth. Further suppose $\nabla f$ vanishes no where. When is it possible to find a smooth non-singular $g:\mathbb{R}^2\to \mathbb{R}$ satisfying $\nabla f\cdot \nabla g = 0$?,,"['ordinary-differential-equations', 'differential-geometry', 'differential-topology']"
8,Equilibrium Points Second Order Differential,Equilibrium Points Second Order Differential,,Attempt: I get the system of the two first order equations (first order in $w$) by considering the different signs the first derivative takes. Problem is by equilibrium points: do I just set the first derivative to 0 in the (*) equation or do I get them from the two first-order equations? I just need someone to tell me how to obtain the equilibrium points. Thanks.,Attempt: I get the system of the two first order equations (first order in $w$) by considering the different signs the first derivative takes. Problem is by equilibrium points: do I just set the first derivative to 0 in the (*) equation or do I get them from the two first-order equations? I just need someone to tell me how to obtain the equilibrium points. Thanks.,,"['calculus', 'ordinary-differential-equations']"
9,Question about assumptions for Picard-Lindelöf Theorem in Zeidler's functional analysis text,Question about assumptions for Picard-Lindelöf Theorem in Zeidler's functional analysis text,,"In Zeidler's text on functional analysis pg.24 he wrote... The Picard Lindelöf Theorem: Assume the following: (a) the function $F: S \to \mathbb{R}$ is continuous and the partial   derivative  $F_u:S \to \mathbb{R}$ is also continuous (b) we set $M = \max_{(x,u)\in S} |F(x,u)|$ and $L = \max_{(x,u)\in S} |F_u(x,u)|$ and we choose real number $h$ in such a way that $$0 < h \leq r, hM \leq r, hL < 1$$ Then he goes on by stating that there exist unique solution for IVP of the type $u' = F(x,u)$, $u(x_0) = u_0$ on the square $S$ where $S = \{(x,u) \in \mathbb{R^2}: |x - x_0|\leq r, |u - u_0|\leq r\}, r >0$ My question is where is the usual condition about Lipschitz continuity in the assumption of the theorem? For example, Wolfram http://mathworld.wolfram.com/PicardsExistenceTheorem.html I am guessing $L$ must indirectly imply Lipschitz continuity of function $F$, but why is it stated in this particular way?","In Zeidler's text on functional analysis pg.24 he wrote... The Picard Lindelöf Theorem: Assume the following: (a) the function $F: S \to \mathbb{R}$ is continuous and the partial   derivative  $F_u:S \to \mathbb{R}$ is also continuous (b) we set $M = \max_{(x,u)\in S} |F(x,u)|$ and $L = \max_{(x,u)\in S} |F_u(x,u)|$ and we choose real number $h$ in such a way that $$0 < h \leq r, hM \leq r, hL < 1$$ Then he goes on by stating that there exist unique solution for IVP of the type $u' = F(x,u)$, $u(x_0) = u_0$ on the square $S$ where $S = \{(x,u) \in \mathbb{R^2}: |x - x_0|\leq r, |u - u_0|\leq r\}, r >0$ My question is where is the usual condition about Lipschitz continuity in the assumption of the theorem? For example, Wolfram http://mathworld.wolfram.com/PicardsExistenceTheorem.html I am guessing $L$ must indirectly imply Lipschitz continuity of function $F$, but why is it stated in this particular way?",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'proof-verification', 'fixed-point-theorems']"
10,Coupled second-order differential equations,Coupled second-order differential equations,,"I am trying to solve the following system of coupled ODEs: \begin{align} -x^2 f'' - 3xf' + (1-2a)f - (a+1)x^2g'' + (2-4a)xg' + (4a-2)g &= 0,\\ (a-1)x^2 f'' + (4a+2)xf' + (12-6a)f + 12xg' + (12a-24)g &= 0, \end{align} where $f$ and $g$ are function of $x$ and $a$ is a constant. What method do you suggest for solving this system? Any suggestion will be appreciated! Thanks!","I am trying to solve the following system of coupled ODEs: \begin{align} -x^2 f'' - 3xf' + (1-2a)f - (a+1)x^2g'' + (2-4a)xg' + (4a-2)g &= 0,\\ (a-1)x^2 f'' + (4a+2)xf' + (12-6a)f + 12xg' + (12a-24)g &= 0, \end{align} where $f$ and $g$ are function of $x$ and $a$ is a constant. What method do you suggest for solving this system? Any suggestion will be appreciated! Thanks!",,"['ordinary-differential-equations', 'systems-of-equations']"
11,homogeneous first order differential equation,homogeneous first order differential equation,,"is there a method to solve $$\dfrac{dy}{dx} = f(x,y)$$, where $f(x,y)$ is a homogeneous function. I found some examples like $f(x,y)=(x+y)^2$ where it can be solved after converting it to Ricatti's equation. thanks","is there a method to solve $$\dfrac{dy}{dx} = f(x,y)$$, where $f(x,y)$ is a homogeneous function. I found some examples like $f(x,y)=(x+y)^2$ where it can be solved after converting it to Ricatti's equation. thanks",,['ordinary-differential-equations']
12,Help needed for solving differential equation,Help needed for solving differential equation,,"I need help with solving this differential equation taken from Ordinary Differential Equations by Tenenbaum and Pollard. $$ xy'-y-x\sin(y/x)=0 $$ I used the subsitution, $u=y/x$ to obtain, $$x^2\,du-x\sin(u)\,dx=0$$ $$\csc(u)\,du-\frac{1}{x}\,dx=0$$ Then by integrating, $$\ln|\csc(u)+\cot(u)|+\ln|x|=C$$ $$|\csc u+\cot u||x|=D$$ and rewriting $u$, $$\left|\frac{1+\cos (y/x)}{\sin (y/x)}\right||x|=D\quad (x\neq0\,,\frac{y}{x}\neq \pm n\pi)$$ However, the provided solution is $y=2x\arctan cx$.  How do I transform the solution I obtained, and how do I deal with the absolute values?","I need help with solving this differential equation taken from Ordinary Differential Equations by Tenenbaum and Pollard. $$ xy'-y-x\sin(y/x)=0 $$ I used the subsitution, $u=y/x$ to obtain, $$x^2\,du-x\sin(u)\,dx=0$$ $$\csc(u)\,du-\frac{1}{x}\,dx=0$$ Then by integrating, $$\ln|\csc(u)+\cot(u)|+\ln|x|=C$$ $$|\csc u+\cot u||x|=D$$ and rewriting $u$, $$\left|\frac{1+\cos (y/x)}{\sin (y/x)}\right||x|=D\quad (x\neq0\,,\frac{y}{x}\neq \pm n\pi)$$ However, the provided solution is $y=2x\arctan cx$.  How do I transform the solution I obtained, and how do I deal with the absolute values?",,['ordinary-differential-equations']
13,Asymptotics in differential equations,Asymptotics in differential equations,,"While learning about Bessel functions I've came up with the following argument: Since the Bessel equation is $$y''+\frac{1}{x} y'+ \left(1-\frac{\alpha^2}{x^2} \right) y=0, $$   one might expect that in the limit $x \to \infty$ we have $y''+y \approx 0$, which is the equation modelling simple harmonic motion (aka linear combinations of sines and cosines) of period $2 \pi$. Thus, we should have $y \approx C_1 \cos x+ C_2 \sin x$ for large $x$. I'm aware that there is a branch of mathematics, called asymptotics (?) that deals with such ideas. Nonetheless I have several questions: How can you know what terms are to be omitted as $x$ gets large? perhaps $y'$ grows faster than $\frac{1}{x}$ decays (so that the middle term cannot be omitted). Looking at the graph of the Bessel function $J_0(x)$ one can see that the amplitude of the oscillation decays. Is there a way to predict that, and know the rate at which $C_1,C_2$ decay? In general, is there a rigorous statement of the form: If $y' \approx G(x,y)$ is an approximation of the differential equation $y'=F(x,y)$ at the point $x_0$, with a solution $y=\varphi$, then the original equation has a solution $y$ which is $\approx \varphi$ near $x_0$? Hopefully with a precise notion of what ""$\approx$"" means. Thank you!","While learning about Bessel functions I've came up with the following argument: Since the Bessel equation is $$y''+\frac{1}{x} y'+ \left(1-\frac{\alpha^2}{x^2} \right) y=0, $$   one might expect that in the limit $x \to \infty$ we have $y''+y \approx 0$, which is the equation modelling simple harmonic motion (aka linear combinations of sines and cosines) of period $2 \pi$. Thus, we should have $y \approx C_1 \cos x+ C_2 \sin x$ for large $x$. I'm aware that there is a branch of mathematics, called asymptotics (?) that deals with such ideas. Nonetheless I have several questions: How can you know what terms are to be omitted as $x$ gets large? perhaps $y'$ grows faster than $\frac{1}{x}$ decays (so that the middle term cannot be omitted). Looking at the graph of the Bessel function $J_0(x)$ one can see that the amplitude of the oscillation decays. Is there a way to predict that, and know the rate at which $C_1,C_2$ decay? In general, is there a rigorous statement of the form: If $y' \approx G(x,y)$ is an approximation of the differential equation $y'=F(x,y)$ at the point $x_0$, with a solution $y=\varphi$, then the original equation has a solution $y$ which is $\approx \varphi$ near $x_0$? Hopefully with a precise notion of what ""$\approx$"" means. Thank you!",,"['ordinary-differential-equations', 'asymptotics']"
14,Solving Riccati equation $v'=av^2+bv+c$,Solving Riccati equation,v'=av^2+bv+c,"I am trying to solve Riccati equation $$ v(t)'=av(t)^2+bv(t)+c $$ where $a$,$b$,$c$ are real-valued constants. Wolfram|Alpha gives the solution ( here ), but I am not able to reproduce the result. Thank you in advance.","I am trying to solve Riccati equation $$ v(t)'=av(t)^2+bv(t)+c $$ where $a$,$b$,$c$ are real-valued constants. Wolfram|Alpha gives the solution ( here ), but I am not able to reproduce the result. Thank you in advance.",,['ordinary-differential-equations']
15,"Prove that a classical solution of $-\langle\nabla,A\nabla u\rangle=f$ is also a weak one",Prove that a classical solution of  is also a weak one,"-\langle\nabla,A\nabla u\rangle=f","Let $\Omega\subseteq\mathbb{R}^n$ a domain $f\in L^2(\Omega)$ $A:\Omega\to\mathbb{R}^{n\times n}$ be Borel-measurable and $A(x)$ be symmetric, for all $x\in\Omega$ $u\in C^2(\Omega)$ with $A\nabla u\in L^1_\text{loc}(\Omega)$ and $$-\langle\nabla,A\nabla u\rangle=f\;\;\;\text{in }\Omega\tag{1}$$ How can we show, that $$-\int_\Omega\langle A\nabla u,\nabla\varphi\rangle\;d\lambda^n=\int_\Omega f\varphi\;d\lambda^n\;\;\;\text{for all }\varphi\in C_0^1(\Omega)\tag{2}\;?$$ Clearly, from $(1)$ we get $$-\int_\Omega\langle\nabla,A\nabla u\rangle\varphi\;d\lambda^n=\int_\Omega f\varphi\;d\lambda^n\;,$$ but I've no idea how we can show $$-\int_\Omega\langle\nabla,A\nabla u\rangle\varphi\;d\lambda^n=-\int_\Omega\langle A\nabla u,\nabla\varphi\rangle\;d\lambda^n$$ I assume we need to apply one of Green's identities, but how?","Let $\Omega\subseteq\mathbb{R}^n$ a domain $f\in L^2(\Omega)$ $A:\Omega\to\mathbb{R}^{n\times n}$ be Borel-measurable and $A(x)$ be symmetric, for all $x\in\Omega$ $u\in C^2(\Omega)$ with $A\nabla u\in L^1_\text{loc}(\Omega)$ and $$-\langle\nabla,A\nabla u\rangle=f\;\;\;\text{in }\Omega\tag{1}$$ How can we show, that $$-\int_\Omega\langle A\nabla u,\nabla\varphi\rangle\;d\lambda^n=\int_\Omega f\varphi\;d\lambda^n\;\;\;\text{for all }\varphi\in C_0^1(\Omega)\tag{2}\;?$$ Clearly, from $(1)$ we get $$-\int_\Omega\langle\nabla,A\nabla u\rangle\varphi\;d\lambda^n=\int_\Omega f\varphi\;d\lambda^n\;,$$ but I've no idea how we can show $$-\int_\Omega\langle\nabla,A\nabla u\rangle\varphi\;d\lambda^n=-\int_\Omega\langle A\nabla u,\nabla\varphi\rangle\;d\lambda^n$$ I assume we need to apply one of Green's identities, but how?",,"['real-analysis', 'integration', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
16,Laplace Transform of derivative squared,Laplace Transform of derivative squared,,"I'm trying to solve a problem while I'm studying Control Theory and I came up with a difficult question. $ \mathcal{L}\left[y'(t)^2 \right] $ Basically I need to find the Laplace Transform of this problem. In essence the differential equation I am attempting to solve looks like this, $ y'(t) =a\,\sqrt{y(t)} $ I couldn't find anything on regular Laplace Tables and I tried doing the integral on my own but it led me nowhere. I could go both ways, either get the transform of $y'(t)^2$ or the transform of $\sqrt{y(t)}$.","I'm trying to solve a problem while I'm studying Control Theory and I came up with a difficult question. $ \mathcal{L}\left[y'(t)^2 \right] $ Basically I need to find the Laplace Transform of this problem. In essence the differential equation I am attempting to solve looks like this, $ y'(t) =a\,\sqrt{y(t)} $ I couldn't find anything on regular Laplace Tables and I tried doing the integral on my own but it led me nowhere. I could go both ways, either get the transform of $y'(t)^2$ or the transform of $\sqrt{y(t)}$.",,"['ordinary-differential-equations', 'laplace-transform', 'mathematical-modeling']"
17,Differential equations: IVP application,Differential equations: IVP application,,"I am given $$y'=0.05y-800$$ I am asked to: (a) Find all constant solutions of the differential equation. (b) Suppose $y = M$ is your constant solution from (a). Plot two solutions of the differential equation where one satisfies $ 0 < y(0) <M$ and the other satisfying $y(0) > M$. Use this sketch for parts (c), (d) and (e). Here is what I did. For part a) I equated $y'$ to $0$ since I needed the constant solution for $y'$ and i got $y=16000$. For part b, I'm truly confused as to why there are two solutions for this one. I have the key answer for these problems but at some point I don't how they got the answer and my exam is fast approaching. I need to know how. Please help. Thanks!","I am given $$y'=0.05y-800$$ I am asked to: (a) Find all constant solutions of the differential equation. (b) Suppose $y = M$ is your constant solution from (a). Plot two solutions of the differential equation where one satisfies $ 0 < y(0) <M$ and the other satisfying $y(0) > M$. Use this sketch for parts (c), (d) and (e). Here is what I did. For part a) I equated $y'$ to $0$ since I needed the constant solution for $y'$ and i got $y=16000$. For part b, I'm truly confused as to why there are two solutions for this one. I have the key answer for these problems but at some point I don't how they got the answer and my exam is fast approaching. I need to know how. Please help. Thanks!",,"['calculus', 'ordinary-differential-equations', 'multivariable-calculus']"
18,"differential equation, partial derivatives: Why is the following true?","differential equation, partial derivatives: Why is the following true?",,"$$x_1'=f_1(t,x_1,...,x_n) \\ x_2'=f_2(t,x_1,...,x_n) \\ ....... \\x_n'=f_n(t,x_1,...,x_n)$$ This a system of equations, now the text book says let's differentiate the first equation of the system with respect to $t$. Then we get, (I dont understand how this is ) $$x_1''= \frac{\partial f_1}{\partial t}+ \sum_{i=1}^{n}\frac{\partial f_1}{\partial x_i}x_i'$$ Now this is confusing for me because if when take an example(differentiating with respect to $t$, $x_1$ is seen as a constant then?): $$x_1'=2t^2x_1$$  $$x_1'=4tx_1 \neq 4tx_1+ 2t^2??$$","$$x_1'=f_1(t,x_1,...,x_n) \\ x_2'=f_2(t,x_1,...,x_n) \\ ....... \\x_n'=f_n(t,x_1,...,x_n)$$ This a system of equations, now the text book says let's differentiate the first equation of the system with respect to $t$. Then we get, (I dont understand how this is ) $$x_1''= \frac{\partial f_1}{\partial t}+ \sum_{i=1}^{n}\frac{\partial f_1}{\partial x_i}x_i'$$ Now this is confusing for me because if when take an example(differentiating with respect to $t$, $x_1$ is seen as a constant then?): $$x_1'=2t^2x_1$$  $$x_1'=4tx_1 \neq 4tx_1+ 2t^2??$$",,"['ordinary-differential-equations', 'derivatives', 'partial-derivative']"
19,Thickness of the Boundary Layer,Thickness of the Boundary Layer,,Given an ODE $$\epsilon y''+2xy'=x \cos(x)$$ with boundary condition $y(\pm {\pi \over 2})=2$ Where is the boundary layer and what is the thickness of it?,Given an ODE $$\epsilon y''+2xy'=x \cos(x)$$ with boundary condition $y(\pm {\pi \over 2})=2$ Where is the boundary layer and what is the thickness of it?,,"['ordinary-differential-equations', 'perturbation-theory', 'boundary-layer']"
20,How to solve this 2nd order ODE,How to solve this 2nd order ODE,,"Consider $$\epsilon y''+yy'-y=0$$ with boundary conditions $y(0)=0$ and $y(1)=3$. I showed that the outer solution is $y_{in}(x)=x+2+O(\epsilon)$. Than for the inner solution, I wish to solve the follow ODE $$Y''(X)+Y(X) \cdot Y'(X) = 0$$ with only one boundary condition $y(0)=0$ and $X={x \over \epsilon}$. Can anyone show me how to do it?","Consider $$\epsilon y''+yy'-y=0$$ with boundary conditions $y(0)=0$ and $y(1)=3$. I showed that the outer solution is $y_{in}(x)=x+2+O(\epsilon)$. Than for the inner solution, I wish to solve the follow ODE $$Y''(X)+Y(X) \cdot Y'(X) = 0$$ with only one boundary condition $y(0)=0$ and $X={x \over \epsilon}$. Can anyone show me how to do it?",,['ordinary-differential-equations']
21,Differential equation of a pendulum,Differential equation of a pendulum,,"Consider the nonlinear differential equation of the pendulum $$\frac{d^2\theta}{dt^2}+\sin \theta=0$$ with $\theta(0)=\frac{\pi}3$ and $\theta'(0)=0$. Using the series method, find the first four nonzero terms of the solution. Here is what I found from Maple so far: Text-only (for the series solution): \begin{align} \theta(t)&=\frac{\pi}3-\frac 12\sin\left(\frac{\pi}3\right)t^2+\frac 1{24}\sin\left(\frac{\pi}3\right)\cos\left(\frac{\pi}3\right)t^4+O(t^6)\\ &=\frac{\pi}3-\frac{\sqrt{3}}4t^2+\frac{\sqrt{3}}{96}t^4+O(t^6) \end{align} But how can one find this solution by hand, using the series method?","Consider the nonlinear differential equation of the pendulum $$\frac{d^2\theta}{dt^2}+\sin \theta=0$$ with $\theta(0)=\frac{\pi}3$ and $\theta'(0)=0$. Using the series method, find the first four nonzero terms of the solution. Here is what I found from Maple so far: Text-only (for the series solution): \begin{align} \theta(t)&=\frac{\pi}3-\frac 12\sin\left(\frac{\pi}3\right)t^2+\frac 1{24}\sin\left(\frac{\pi}3\right)\cos\left(\frac{\pi}3\right)t^4+O(t^6)\\ &=\frac{\pi}3-\frac{\sqrt{3}}4t^2+\frac{\sqrt{3}}{96}t^4+O(t^6) \end{align} But how can one find this solution by hand, using the series method?",,['ordinary-differential-equations']
22,Nonlinear Partial DE,Nonlinear Partial DE,,"In my work I have faced with following partial differential equation $$\left(\frac{\partial u}{\partial x}\right)^2-\left(\frac{\partial u}{\partial y}\right)^2+f(x,y)\frac{\partial u}{\partial x}\frac{\partial u}{\partial y}=0$$ where $f(x,y)=\frac{1}{2}\frac{(y^2-x^2)^2}{xy(x^2+y^2)}$, $u=u(x,y)$ I've tried to solve it by method of characteristic , but it was too complicated. Maybe someone knows something about this type of PDE? Anyway I would be grateful for any hints.","In my work I have faced with following partial differential equation $$\left(\frac{\partial u}{\partial x}\right)^2-\left(\frac{\partial u}{\partial y}\right)^2+f(x,y)\frac{\partial u}{\partial x}\frac{\partial u}{\partial y}=0$$ where $f(x,y)=\frac{1}{2}\frac{(y^2-x^2)^2}{xy(x^2+y^2)}$, $u=u(x,y)$ I've tried to solve it by method of characteristic , but it was too complicated. Maybe someone knows something about this type of PDE? Anyway I would be grateful for any hints.",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-physics']"
23,Solve first order nonlinear differential equations,Solve first order nonlinear differential equations,,"I want to solve this nonlinear 1-st order ODE, $$\frac{1}{1+x}=(\frac{1}{x-y}-\frac{1}{y})\frac{dy}{dx}$$ I find it non-separable, and Wolfram Alpha does not give me a closed form solution, but the following plots. I am a little rusty on solving ODEs, can someone tell me the method to solve this one? A variable transformation or any other trick? Or do I need initial values to see the behavior of the system (directional fields)? Thanks in advance. Update Now the modified code is as follows, f[x_] = NDSolveValue[{(1 + x) (0.5 y[x]^-0.5 (x - y[x])^0.5 -         0.5 y[x]^0.5 (x - y[x])^-0.5) == -y[x] (x - y[x])/y'[x],     y[1] == 0.5}, y[x], {x, 0, 2}] But the result is still error, Infinite expression 1/0. encountered. >> NDSolveValue::ndnum: Encountered non-numerical value for a derivative at x == 1.`. >> Why in your code, we do not have the indeterminate problem? And the direction field I want is like the one below:","I want to solve this nonlinear 1-st order ODE, $$\frac{1}{1+x}=(\frac{1}{x-y}-\frac{1}{y})\frac{dy}{dx}$$ I find it non-separable, and Wolfram Alpha does not give me a closed form solution, but the following plots. I am a little rusty on solving ODEs, can someone tell me the method to solve this one? A variable transformation or any other trick? Or do I need initial values to see the behavior of the system (directional fields)? Thanks in advance. Update Now the modified code is as follows, f[x_] = NDSolveValue[{(1 + x) (0.5 y[x]^-0.5 (x - y[x])^0.5 -         0.5 y[x]^0.5 (x - y[x])^-0.5) == -y[x] (x - y[x])/y'[x],     y[1] == 0.5}, y[x], {x, 0, 2}] But the result is still error, Infinite expression 1/0. encountered. >> NDSolveValue::ndnum: Encountered non-numerical value for a derivative at x == 1.`. >> Why in your code, we do not have the indeterminate problem? And the direction field I want is like the one below:",,"['ordinary-differential-equations', 'problem-solving', 'nonlinear-system', 'boundary-value-problem']"
24,No. of linearly independent bounded solutions,No. of linearly independent bounded solutions,,Let $V$ be the set of all bounded  solutions of the ODE $u''(t)-4u(t)=0$ $ where$ $t \epsilon \Bbb R$ . Then $V$ is a)real vector space of dimension 2 b)real vector space of dimension 1 c)contains only trivial solution $u=0$ Here I've got the solution as $y=$ ${ae^{-2x}}$ $+$ ${be^{2x}}$ .If we take $V$ as the set of all solutions of the given ODE then option a) will be true.But here $V$ is defined as the set of bounded solutions .So what is the no. of linearly independent bounded solutions?Also u=0 is a solution of the given ODE.,Let be the set of all bounded  solutions of the ODE . Then is a)real vector space of dimension 2 b)real vector space of dimension 1 c)contains only trivial solution Here I've got the solution as .If we take as the set of all solutions of the given ODE then option a) will be true.But here is defined as the set of bounded solutions .So what is the no. of linearly independent bounded solutions?Also u=0 is a solution of the given ODE.,V u''(t)-4u(t)=0  where t \epsilon \Bbb R V u=0 y= {ae^{-2x}} + {be^{2x}} V V,['ordinary-differential-equations']
25,Is a mapping a homeomorphism,Is a mapping a homeomorphism,,"I'm considering the mapping $\Psi: C^2([0,1])$ to $C^1([0,1])$ via: $f(x) \mapsto f(x)+x\cdot f'(x)$. Is this mapping a homeomorphism?  It should be continuous given that, for any sequence $(f_n) \in C^2([0,1])$ such that $(f_n) \to f$, it is necessary for $C^2$ convergence that the functions, their first, and their second derivatives uniformly converge.  Thus: $$\|(f_n-f) +x\cdot (f_n'-f')\|_\infty \le \|(f_n-f) + (f_n'-f')\|_\infty \le \|f_n-f\|_\infty +\|f_n'-f'\|_\infty$$ where the right-hand side goes to zero, and an analogous argument holds for the derivative sequence. But is it a homeomorphism?  I'm having trouble with a sequential argument for the inverse, and feel like I may just be missing something.  It seems intuitive that $\Psi$ should be onto due to solutions of first-order linear ODEs existing.  Do things break with injectivity?  Thanks for any guidance!","I'm considering the mapping $\Psi: C^2([0,1])$ to $C^1([0,1])$ via: $f(x) \mapsto f(x)+x\cdot f'(x)$. Is this mapping a homeomorphism?  It should be continuous given that, for any sequence $(f_n) \in C^2([0,1])$ such that $(f_n) \to f$, it is necessary for $C^2$ convergence that the functions, their first, and their second derivatives uniformly converge.  Thus: $$\|(f_n-f) +x\cdot (f_n'-f')\|_\infty \le \|(f_n-f) + (f_n'-f')\|_\infty \le \|f_n-f\|_\infty +\|f_n'-f'\|_\infty$$ where the right-hand side goes to zero, and an analogous argument holds for the derivative sequence. But is it a homeomorphism?  I'm having trouble with a sequential argument for the inverse, and feel like I may just be missing something.  It seems intuitive that $\Psi$ should be onto due to solutions of first-order linear ODEs existing.  Do things break with injectivity?  Thanks for any guidance!",,"['real-analysis', 'general-topology', 'ordinary-differential-equations', 'differential-topology', 'recreational-mathematics']"
26,Short question about definition in ODEs,Short question about definition in ODEs,,"Hello I just have a short question about a remark made in my first class of intro ODE. My Professor was just motivating with a simple example, he wrote, $$\frac{dy}{dx}=y(x)$$ So of course it was clear that he was referring in general to $y(x)=ce^{x}$ where $c \in \mathbb{R}$ and then he also added and for all $ \ x \in \mathbb{R}$. Now here is where I got briefly confused , because I thought for example consider $y(x)=e^9$, then $\frac{dy}{dx}=0 \neq e^{9}$ So what I took this to mean is that $y(x)=ce^x$ is the solution, and if you evaluate it at any x the equation is true, i.e. simply because $e^x$ was the solution? I just want to make sure that is what is meant in this context, or if there was some error in my understanding/notation? Thanks all.","Hello I just have a short question about a remark made in my first class of intro ODE. My Professor was just motivating with a simple example, he wrote, $$\frac{dy}{dx}=y(x)$$ So of course it was clear that he was referring in general to $y(x)=ce^{x}$ where $c \in \mathbb{R}$ and then he also added and for all $ \ x \in \mathbb{R}$. Now here is where I got briefly confused , because I thought for example consider $y(x)=e^9$, then $\frac{dy}{dx}=0 \neq e^{9}$ So what I took this to mean is that $y(x)=ce^x$ is the solution, and if you evaluate it at any x the equation is true, i.e. simply because $e^x$ was the solution? I just want to make sure that is what is meant in this context, or if there was some error in my understanding/notation? Thanks all.",,['ordinary-differential-equations']
27,General solution for differential equation,General solution for differential equation,,"I have the following differential equation ; $$\frac{dx\left(t\right)}{dt}=ay\left(t\right)-bx\left(t\right)$$ where $a$ and $b$ are positive constant terms. $t$ indicates time. I am trying to solve in the following manner ; First, I put $bx(t)$ in the LHS and multiply two sides by $e^{bt}$ ; $$\frac{dx\left(t\right)}{dt}e^{bt}+bx\left(t\right)e^{bt}=ay\left(t\right)e^{bt}$$ After, I write ; $$\frac{d\left(x\left(t\right)e^{bt}\right)}{dt}=ay\left(t\right)e^{bt}$$ My objective is to solve this integral between time $0$ and $\infty$ ; $$\int_{0}^{\infty}\frac{d\left(x\left(t\right)e^{bt}\right)}{dt}=\int_{0}^{\infty}ay\left(t\right)e^{bt}$$ which yields $$\left[x\left(t\right)e^{bt}\right]_{0}^{\infty}=\int_{0}^{\infty}ay\left(t\right)e^{bt}$$ The problem is that the left hand side explodes and goes to infity. How can I solve this differential equation in a correct way ?","I have the following differential equation ; $$\frac{dx\left(t\right)}{dt}=ay\left(t\right)-bx\left(t\right)$$ where $a$ and $b$ are positive constant terms. $t$ indicates time. I am trying to solve in the following manner ; First, I put $bx(t)$ in the LHS and multiply two sides by $e^{bt}$ ; $$\frac{dx\left(t\right)}{dt}e^{bt}+bx\left(t\right)e^{bt}=ay\left(t\right)e^{bt}$$ After, I write ; $$\frac{d\left(x\left(t\right)e^{bt}\right)}{dt}=ay\left(t\right)e^{bt}$$ My objective is to solve this integral between time $0$ and $\infty$ ; $$\int_{0}^{\infty}\frac{d\left(x\left(t\right)e^{bt}\right)}{dt}=\int_{0}^{\infty}ay\left(t\right)e^{bt}$$ which yields $$\left[x\left(t\right)e^{bt}\right]_{0}^{\infty}=\int_{0}^{\infty}ay\left(t\right)e^{bt}$$ The problem is that the left hand side explodes and goes to infity. How can I solve this differential equation in a correct way ?",,"['calculus', 'ordinary-differential-equations']"
28,Why are there four independent solutions of Mathieu equation instead of two?,Why are there four independent solutions of Mathieu equation instead of two?,,"Consider Mathieu equation: $$\frac{d^2}{d\xi^2}R(\xi)+(a-2q\cos(2\xi))R(\xi)=0.$$ It's a second order ODE, so it should have two linearly independent solutions. One of the choices is to denote one as Mathieu sine $\operatorname{S}(a,q,\xi)$ and another as Mathieu cosine $\operatorname{C}(a,q,\xi)$. But many books instead take some strange analogy to Bessel functions. Namely, they say that two functions, cosine-elliptic $\operatorname{ce}_r(\xi,q)$ and sine-elliptic $\operatorname{se}_r(\xi,q)$ are solutions of the first kind analogous to one Bessel $\operatorname{J}_\nu(x)$ function and two another functions $\operatorname{fe}_r(\xi,q)$ and $\operatorname{ge}_r(\xi,q)$ are solutions of the second kind analogous to one Neumann $\operatorname{Y}_\nu(x)$ function. Actually, the analogy with Bessel functions seems to go only for modified aka radial Mathieu equation, but the number of functions is the same for angular equation. But sine-elliptic $\operatorname{se}$ and cosine-elliptic $\operatorname{ce}$ are already linearly independent, why are they both in the first kind? And why is there another pair of linearly independent functions $\operatorname{fe}$ and $\operatorname{ge}$? What is so much different between Mathieu and Bessel equations that the former has twice the number of solutions than the latter? My guess is that the pairs $\operatorname{se}$ and $\operatorname{fe}$ and similarly $\operatorname{ce}$ and $\operatorname{ge}$ correspond to $\operatorname{S}$ and $\operatorname{C}$ correspondingly, but have differing sets of characteristic exponents: the functions of the first kind have real characteristic exponents (corresponding to bands in solid-state physics) while those of the second kind have complex ones (corresponding to band gaps). Is this right?","Consider Mathieu equation: $$\frac{d^2}{d\xi^2}R(\xi)+(a-2q\cos(2\xi))R(\xi)=0.$$ It's a second order ODE, so it should have two linearly independent solutions. One of the choices is to denote one as Mathieu sine $\operatorname{S}(a,q,\xi)$ and another as Mathieu cosine $\operatorname{C}(a,q,\xi)$. But many books instead take some strange analogy to Bessel functions. Namely, they say that two functions, cosine-elliptic $\operatorname{ce}_r(\xi,q)$ and sine-elliptic $\operatorname{se}_r(\xi,q)$ are solutions of the first kind analogous to one Bessel $\operatorname{J}_\nu(x)$ function and two another functions $\operatorname{fe}_r(\xi,q)$ and $\operatorname{ge}_r(\xi,q)$ are solutions of the second kind analogous to one Neumann $\operatorname{Y}_\nu(x)$ function. Actually, the analogy with Bessel functions seems to go only for modified aka radial Mathieu equation, but the number of functions is the same for angular equation. But sine-elliptic $\operatorname{se}$ and cosine-elliptic $\operatorname{ce}$ are already linearly independent, why are they both in the first kind? And why is there another pair of linearly independent functions $\operatorname{fe}$ and $\operatorname{ge}$? What is so much different between Mathieu and Bessel equations that the former has twice the number of solutions than the latter? My guess is that the pairs $\operatorname{se}$ and $\operatorname{fe}$ and similarly $\operatorname{ce}$ and $\operatorname{ge}$ correspond to $\operatorname{S}$ and $\operatorname{C}$ correspondingly, but have differing sets of characteristic exponents: the functions of the first kind have real characteristic exponents (corresponding to bands in solid-state physics) while those of the second kind have complex ones (corresponding to band gaps). Is this right?",,"['ordinary-differential-equations', 'special-functions']"
29,Find solution of problem - Method of characteristics,Find solution of problem - Method of characteristics,,"I want to find the solution of the problem:  $$(t+u(x,t))u_x(x,t)+tu_t(x,t)=x-t, x \in \mathbb{R}, t>1 \\ u(x,1)=1+x, x\in \mathbb{R}$$ I have tried the following: $$(x(0), t(0))=(x_0, 1)$$ We will find a curve $(x(s), t(s)), s \in \mathbb{R}$ such that $\sigma (s)=u(x(s),t(s))$ $\sigma'(s)=\frac{d}{ds}(u(x(s), t(s)))=u_x(x(s), t(s))x'(s)+u_t(x(s), t(s))t'(s)$ We choose $x'(s)=t(s)+u(x(s), t(s))=t(s)+\sigma (s), s \in \mathbb{R}, x(0)=x_0 \\ t'(s)=t(s), s \in \mathbb{R}, t(0)=1$ Then $\sigma'(s)=(t(s)+u(x(s), t(s)))u_x(x(s), t(s))+t(s)u_t(x(s), t(s))=x(s)-t(s), s \in \mathbb{R} \\ \sigma (0)=u(x(0), t(0))=u(x_0, 1)=1+x_0$ $$t'(s)=t(s) \Rightarrow t=ce^s \\ t(0)=1 \Rightarrow c=1 \\ \text{ So, } t(s)=e^s$$ So we get $$x'(s)=t(s)+\sigma(s)=e^s +\sigma (s) \\ \sigma'(s)=x(s)-t(s) \Rightarrow \sigma'(s)=x(s)-e^s$$ $$\sigma'(s)=x(s)-e^s \Rightarrow \sigma''(s)=x'(s)-e^s=e^s+\sigma (s)-e^s \Rightarrow \sigma''(s)=\sigma (s) \Rightarrow \sigma (s)=c_1e^s+c_2 e^{-s} \\ \sigma (0)=1+x_0 \Rightarrow c_1+c_2=1+x_0 \Rightarrow c_1=1+x_0-c_2 \\ \Rightarrow \sigma (s)=(1+x_0-c_2)e^s+c_2e^{-s}$$ $$\Rightarrow x'(s)=e^s+(1+x_0-c_2)e^s+c_2e^{-s}=(2+x_0-c_2)e^s+c_2e^{-s} \\ \Rightarrow x(s)=(2+x_0-c_2)e^s-c_2e^{-s}+c_3 \\ \Rightarrow x(0)=x_0 \Rightarrow 2+x_0-c_2-c_2+c_3=x_0 \Rightarrow -2c_2+c_3=-2 \Rightarrow c_3=-2+2c_2 \\ \Rightarrow x(s)=(2+x_0-c_2)e^s-c_2e^{-s}-2+2c_2$$ If $\overline{s}$ is the value of $s$, such that $(x(\overline{s}), t(\overline{s}))=(x_1, t_1)$, then we have $$\left.\begin{matrix} (2+x_0-c_2)e^{\overline{s}}-c_2e^{-\overline{s}}-2+2c_2=x_1  \\  e^{\overline{s}}=t_1 \end{matrix}\right\} \Rightarrow \left.\begin{matrix} (2+x_0-c_2)t_1-c_2\frac{1}{t_1}-2+2c_2=x_1  \\  e^{\overline{s}}=t_1 \end{matrix}\right\} \Rightarrow \left.\begin{matrix} t_1+(1+x_0-c_2)t_1-c_2\frac{1}{t_1}-2+2c_2=x_1  \\  e^{\overline{s}}=t_1 \end{matrix}\right\} \Rightarrow \left.\begin{matrix} (1+x_0-c_2)t_1=x_1-t_1+\frac{c_2}{t_1}+2-2c_2  \\  e^{\overline{s}}=t_1 \end{matrix}\right\}$$ So for $s=\overline{s}$ we have $$\sigma (\overline{s})=u(x(\overline{s}),t(\overline{s}))=u(x_1, t_1)=(1+x_0-c_2)e^{\overline{s}}+c_2e^{-\overline{s}}=(1+x_0-c_2)t_1+\frac{c_2}{t_1}=x_1-t_1+\frac{c_2}{t_1}+2-2c_2+\frac{c_2}{t_1}=x_1-t_1+2\frac{c_2}{t_1}+2-2c_2$$ So the solution is $$u(x,t)=x-t+2\frac{c_2}{t}+2-2c_2$$ Is it right? Can the solution have the constant $c_2$ ?","I want to find the solution of the problem:  $$(t+u(x,t))u_x(x,t)+tu_t(x,t)=x-t, x \in \mathbb{R}, t>1 \\ u(x,1)=1+x, x\in \mathbb{R}$$ I have tried the following: $$(x(0), t(0))=(x_0, 1)$$ We will find a curve $(x(s), t(s)), s \in \mathbb{R}$ such that $\sigma (s)=u(x(s),t(s))$ $\sigma'(s)=\frac{d}{ds}(u(x(s), t(s)))=u_x(x(s), t(s))x'(s)+u_t(x(s), t(s))t'(s)$ We choose $x'(s)=t(s)+u(x(s), t(s))=t(s)+\sigma (s), s \in \mathbb{R}, x(0)=x_0 \\ t'(s)=t(s), s \in \mathbb{R}, t(0)=1$ Then $\sigma'(s)=(t(s)+u(x(s), t(s)))u_x(x(s), t(s))+t(s)u_t(x(s), t(s))=x(s)-t(s), s \in \mathbb{R} \\ \sigma (0)=u(x(0), t(0))=u(x_0, 1)=1+x_0$ $$t'(s)=t(s) \Rightarrow t=ce^s \\ t(0)=1 \Rightarrow c=1 \\ \text{ So, } t(s)=e^s$$ So we get $$x'(s)=t(s)+\sigma(s)=e^s +\sigma (s) \\ \sigma'(s)=x(s)-t(s) \Rightarrow \sigma'(s)=x(s)-e^s$$ $$\sigma'(s)=x(s)-e^s \Rightarrow \sigma''(s)=x'(s)-e^s=e^s+\sigma (s)-e^s \Rightarrow \sigma''(s)=\sigma (s) \Rightarrow \sigma (s)=c_1e^s+c_2 e^{-s} \\ \sigma (0)=1+x_0 \Rightarrow c_1+c_2=1+x_0 \Rightarrow c_1=1+x_0-c_2 \\ \Rightarrow \sigma (s)=(1+x_0-c_2)e^s+c_2e^{-s}$$ $$\Rightarrow x'(s)=e^s+(1+x_0-c_2)e^s+c_2e^{-s}=(2+x_0-c_2)e^s+c_2e^{-s} \\ \Rightarrow x(s)=(2+x_0-c_2)e^s-c_2e^{-s}+c_3 \\ \Rightarrow x(0)=x_0 \Rightarrow 2+x_0-c_2-c_2+c_3=x_0 \Rightarrow -2c_2+c_3=-2 \Rightarrow c_3=-2+2c_2 \\ \Rightarrow x(s)=(2+x_0-c_2)e^s-c_2e^{-s}-2+2c_2$$ If $\overline{s}$ is the value of $s$, such that $(x(\overline{s}), t(\overline{s}))=(x_1, t_1)$, then we have $$\left.\begin{matrix} (2+x_0-c_2)e^{\overline{s}}-c_2e^{-\overline{s}}-2+2c_2=x_1  \\  e^{\overline{s}}=t_1 \end{matrix}\right\} \Rightarrow \left.\begin{matrix} (2+x_0-c_2)t_1-c_2\frac{1}{t_1}-2+2c_2=x_1  \\  e^{\overline{s}}=t_1 \end{matrix}\right\} \Rightarrow \left.\begin{matrix} t_1+(1+x_0-c_2)t_1-c_2\frac{1}{t_1}-2+2c_2=x_1  \\  e^{\overline{s}}=t_1 \end{matrix}\right\} \Rightarrow \left.\begin{matrix} (1+x_0-c_2)t_1=x_1-t_1+\frac{c_2}{t_1}+2-2c_2  \\  e^{\overline{s}}=t_1 \end{matrix}\right\}$$ So for $s=\overline{s}$ we have $$\sigma (\overline{s})=u(x(\overline{s}),t(\overline{s}))=u(x_1, t_1)=(1+x_0-c_2)e^{\overline{s}}+c_2e^{-\overline{s}}=(1+x_0-c_2)t_1+\frac{c_2}{t_1}=x_1-t_1+\frac{c_2}{t_1}+2-2c_2+\frac{c_2}{t_1}=x_1-t_1+2\frac{c_2}{t_1}+2-2c_2$$ So the solution is $$u(x,t)=x-t+2\frac{c_2}{t}+2-2c_2$$ Is it right? Can the solution have the constant $c_2$ ?",,"['ordinary-differential-equations', 'partial-differential-equations']"
30,$y=\cos(m \arcsin x)$ Validity of solution $\dfrac {dy} {dx}$ when $x=0$?,Validity of solution  when ?,y=\cos(m \arcsin x) \dfrac {dy} {dx} x=0,"$y=\cos(m \arcsin x)$, for $ -1 < x < 1$ I want to find the value of $\dfrac {dy} {dx}$ when $x=0$ using the following way: $=> \arccos y = m\arcsin x$ $=> - \dfrac {1} {\sqrt {1-y^2}} \dfrac {dy} {dx}= m \dfrac {1} {\sqrt{1-x^2}}$ Now $x=0 => y=1$ the above equation is not valid. But when I square both side and rearrange it: i.e. $(1-x^2)(\dfrac {dy} {dx})^2 = m^2 (1-y^2)$ Now I can put $x=0, y=1$ and it gives a valid value. But the final valid equation is deduced from the equation that is not valid in the first place, For e.g. I multipled both sides by $(1-y^2)$ which leads to $\dfrac {0}{0}$ in the LHS. So does that mean the final form is also invalid and I should not use its value?","$y=\cos(m \arcsin x)$, for $ -1 < x < 1$ I want to find the value of $\dfrac {dy} {dx}$ when $x=0$ using the following way: $=> \arccos y = m\arcsin x$ $=> - \dfrac {1} {\sqrt {1-y^2}} \dfrac {dy} {dx}= m \dfrac {1} {\sqrt{1-x^2}}$ Now $x=0 => y=1$ the above equation is not valid. But when I square both side and rearrange it: i.e. $(1-x^2)(\dfrac {dy} {dx})^2 = m^2 (1-y^2)$ Now I can put $x=0, y=1$ and it gives a valid value. But the final valid equation is deduced from the equation that is not valid in the first place, For e.g. I multipled both sides by $(1-y^2)$ which leads to $\dfrac {0}{0}$ in the LHS. So does that mean the final form is also invalid and I should not use its value?",,"['algebra-precalculus', 'ordinary-differential-equations']"
31,How many conditions do we need for a problem to have an unique solution?,How many conditions do we need for a problem to have an unique solution?,,"How do we know how many initial and boundary conditions we need for a problem to have an unique solution ?? For example if we have the problem $$u_{tt}-u_{xxtt}(x,t)-u_{xx}(x,t)=f(x,t), 0<x<1, t>0$$ how many conditions do we need??","How do we know how many initial and boundary conditions we need for a problem to have an unique solution ?? For example if we have the problem $$u_{tt}-u_{xxtt}(x,t)-u_{xx}(x,t)=f(x,t), 0<x<1, t>0$$ how many conditions do we need??",,"['ordinary-differential-equations', 'partial-differential-equations']"
32,Find the general solution of the differential equation: $\frac{d^2y}{dt^2}-4t \frac{dy}{dt}+(4t^2-2)y=0$,Find the general solution of the differential equation:,\frac{d^2y}{dt^2}-4t \frac{dy}{dt}+(4t^2-2)y=0,"Q. Using method of reduction, find the general solution of the differential equation: $$\frac{d^2y}{dt^2}-4t \frac{dy}{dt}+(4t^2-2)y=0$$ and $$ y_1(t)=e^{t^2}$$ I proceeded by the way my professor taught us, he said find $u(t)$ $$u(t)=\frac{e^{-\int -4t dt}}{(e^{t^2})^2}=\frac{e^{2t}}{e^{2t^2}}=e^{2t-2t^2} $$ Then we need to find $y_2(t)$ where $$ y_2(t)=e^{t^2} \int e^{2t-2t^2}  $$ but i'm having trouble integrating the right hand side, so either I did something wrong or I'm just simply stuck. If anyone wants the answer, it is: $$ y(t)=(c_1+c_2t)e^{t^2} $$","Q. Using method of reduction, find the general solution of the differential equation: $$\frac{d^2y}{dt^2}-4t \frac{dy}{dt}+(4t^2-2)y=0$$ and $$ y_1(t)=e^{t^2}$$ I proceeded by the way my professor taught us, he said find $u(t)$ $$u(t)=\frac{e^{-\int -4t dt}}{(e^{t^2})^2}=\frac{e^{2t}}{e^{2t^2}}=e^{2t-2t^2} $$ Then we need to find $y_2(t)$ where $$ y_2(t)=e^{t^2} \int e^{2t-2t^2}  $$ but i'm having trouble integrating the right hand side, so either I did something wrong or I'm just simply stuck. If anyone wants the answer, it is: $$ y(t)=(c_1+c_2t)e^{t^2} $$",,['ordinary-differential-equations']
33,Method of reduction,Method of reduction,,"I'm learning about the process of method of reduction and while I understand almost everything, there is one part I cannot figure out. So the book uses the example: $$ (1-t^2)\frac{d^2y}{dt^2}+2t\frac{dy}{dt}-2y=0 $$ where $y(0)=3$ and $y'(0)=-4$ on the interval $-1<t<1 $ The book says $y_1(t)=t$ is clearly a solution but apparently, I cannot understand the reason why. If anyone can explain, that would be really helpful because most of the problems I've been assigned usually give a solution but just incase, my professor gives a problem that does not have a solution, then I can determine myself.","I'm learning about the process of method of reduction and while I understand almost everything, there is one part I cannot figure out. So the book uses the example: $$ (1-t^2)\frac{d^2y}{dt^2}+2t\frac{dy}{dt}-2y=0 $$ where $y(0)=3$ and $y'(0)=-4$ on the interval $-1<t<1 $ The book says $y_1(t)=t$ is clearly a solution but apparently, I cannot understand the reason why. If anyone can explain, that would be really helpful because most of the problems I've been assigned usually give a solution but just incase, my professor gives a problem that does not have a solution, then I can determine myself.",,"['integration', 'ordinary-differential-equations']"
34,Solve $(y')^2=(y/c)^2-1$,Solve,(y')^2=(y/c)^2-1,"Can someone help me solve $(y')^2=(y/c)^2-1$?  WolframAlpha is giving me $\frac 12(c^2 e^{(x/c)-k}+e^{k-(x/c)})$.  One book I have is giving me $y=c\cdot \cosh(\frac {x+b}c)$ -- but that one won't work for the HW problem I'm solving because with the conditions $y(0)=y(D)=0$ it gives $y$ as identically $0$ -- so it must not be the most general form (though it does clearly solve this).  I'm also seeing elsewhere that the answer should be $y_0 + A\cosh(k(x-x_0))$, which would work for these boundary conditions, but I'm having trouble verifying that it actually solves this ODE. I'm just not at all good at solving nonlinear ODEs.","Can someone help me solve $(y')^2=(y/c)^2-1$?  WolframAlpha is giving me $\frac 12(c^2 e^{(x/c)-k}+e^{k-(x/c)})$.  One book I have is giving me $y=c\cdot \cosh(\frac {x+b}c)$ -- but that one won't work for the HW problem I'm solving because with the conditions $y(0)=y(D)=0$ it gives $y$ as identically $0$ -- so it must not be the most general form (though it does clearly solve this).  I'm also seeing elsewhere that the answer should be $y_0 + A\cosh(k(x-x_0))$, which would work for these boundary conditions, but I'm having trouble verifying that it actually solves this ODE. I'm just not at all good at solving nonlinear ODEs.",,"['ordinary-differential-equations', 'solution-verification']"
35,How to find $y$ from $y' = e^{2x}-e^x y$?,How to find  from ?,y y' = e^{2x}-e^x y,"The problem asks me to find $y(x)$ from the equation $$y' = e^{2x}-e^x y$$ The $y'$ is $dy/dx$ right, so wouldn't the correct step be to integrate right away? If not, should I change some terms before integrating? I'm fairly new to this, and am unaware of rules so please be clear in explanation, thank you very much.","The problem asks me to find $y(x)$ from the equation $$y' = e^{2x}-e^x y$$ The $y'$ is $dy/dx$ right, so wouldn't the correct step be to integrate right away? If not, should I change some terms before integrating? I'm fairly new to this, and am unaware of rules so please be clear in explanation, thank you very much.",,"['calculus', 'integration', 'ordinary-differential-equations']"
36,Solving $r^2 u_{rr} + 2ru_{r} + r^{2}u = 0$ directly,Solving  directly,r^2 u_{rr} + 2ru_{r} + r^{2}u = 0,"The problem I am working on boils to solve the differential equation $$r^{2}u_{rr} + 2ru_{r} + r^{2}u = 0.$$ The solution to this equation is the spherical Bessel function $u(r) = \sin(r)/r$. However, let's suppose I didn't recognize the solution was a spherical Bessel function, is there a way to solve this ODE directly?","The problem I am working on boils to solve the differential equation $$r^{2}u_{rr} + 2ru_{r} + r^{2}u = 0.$$ The solution to this equation is the spherical Bessel function $u(r) = \sin(r)/r$. However, let's suppose I didn't recognize the solution was a spherical Bessel function, is there a way to solve this ODE directly?",,['ordinary-differential-equations']
37,Fredholm Integral Equations - Sturm-Lioville & Green Function Theory?,Fredholm Integral Equations - Sturm-Lioville & Green Function Theory?,,"In an ODE's book one is given a 2nd order ode boundary value problem like $$y'' + A(x)y' + B(x)y = f(x), y(a) = y_a, y(b) = y_b$$ and might be told to analyze it with a Green function or via Sturm-Liouville theory, and invoke all this orthogonal function theory. For example randomly converting an ode to S-L form . It looks to me that GF & S-L theory very naturally arise when you convert the ode to a Fredholm integral equation, as in it looks so obvious these methods are the only thing to use. Further there are some Fredholm theorems that apparently justify what you're doing. My question, is it 'correct' to view S-L (homogeneous) & GF (inhomogeneous) theory as actually being part of the theory of integral equations, not differential equations, and to view the Fredholm theorems as the real source of justification for things like Fourier, Legendre polynomials being orthogonal and complete, the spectral theorem being an integral equation theorem? Thanks","In an ODE's book one is given a 2nd order ode boundary value problem like $$y'' + A(x)y' + B(x)y = f(x), y(a) = y_a, y(b) = y_b$$ and might be told to analyze it with a Green function or via Sturm-Liouville theory, and invoke all this orthogonal function theory. For example randomly converting an ode to S-L form . It looks to me that GF & S-L theory very naturally arise when you convert the ode to a Fredholm integral equation, as in it looks so obvious these methods are the only thing to use. Further there are some Fredholm theorems that apparently justify what you're doing. My question, is it 'correct' to view S-L (homogeneous) & GF (inhomogeneous) theory as actually being part of the theory of integral equations, not differential equations, and to view the Fredholm theorems as the real source of justification for things like Fourier, Legendre polynomials being orthogonal and complete, the spectral theorem being an integral equation theorem? Thanks",,"['ordinary-differential-equations', 'integral-equations']"
38,How to solve differential equation $\frac{d}{dx}\left(\frac{\lambda y'}{\sqrt{1+y'^2}}\right)=1$,How to solve differential equation,\frac{d}{dx}\left(\frac{\lambda y'}{\sqrt{1+y'^2}}\right)=1,"My task is to solve for $y$ from: $$\frac{d}{dx}\left(\frac{\lambda y'}{\sqrt{1+y'^2}}\right)=1$$ I have been given the answer, but I would like to calculate this myself also. $\lambda$ is a constant. How should I proceed? The answer according to source material: Integrating with respect to $x$ we get: $$x+C_1= \lambda\sin(\theta)$$ $$y=-\lambda\cos(\theta)+C_2$$","My task is to solve for $y$ from: $$\frac{d}{dx}\left(\frac{\lambda y'}{\sqrt{1+y'^2}}\right)=1$$ I have been given the answer, but I would like to calculate this myself also. $\lambda$ is a constant. How should I proceed? The answer according to source material: Integrating with respect to $x$ we get: $$x+C_1= \lambda\sin(\theta)$$ $$y=-\lambda\cos(\theta)+C_2$$",,['ordinary-differential-equations']
39,Why is the solution to PDE $au_x + bu_y = 0$ $f(bx-ay)$ and not $f(bx+ay)$?,Why is the solution to PDE   and not ?,au_x + bu_y = 0 f(bx-ay) f(bx+ay),"Why is the solution to the following PDE $$au_x + bu_y = 0,$$ where $a$ and $b$ are both non-zero $f(bx-ay)$ instead of $f(bx+ay)$? I just do not understand the logic behind the solution.","Why is the solution to the following PDE $$au_x + bu_y = 0,$$ where $a$ and $b$ are both non-zero $f(bx-ay)$ instead of $f(bx+ay)$? I just do not understand the logic behind the solution.",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative']"
40,"Existences and uniqueness theorem, finding a unique solution of a first order ODE","Existences and uniqueness theorem, finding a unique solution of a first order ODE",,"Consider the initial value problem $$y′=3x(y−1)^{1/3},y(x_{0})=y_{0}$$ Using the existness and uniqueness theorem, for what points ($x_{0}$,$y_{0}$) imply that the above IVP  has a unique solution on some open interval that contains $x_{0}$? What i tried Using the seperable equation method,  I got  a solution of $$y=1+(x^{2}+c)^{1.5}$$ Im unsure of how to find the value of $c$ given the inital conditions and whether is it necessary to find the value of $c$ for this problem. While using the  existence and uniqueness theorem, $y_{0}$ must not be equal to $1$ in order for a unique solution to exist. But from here im unsure of how to find the points  ($x_{0}$,$y_{0}$), I was thinking that since i already know that $y_{0}$ does not equals to $1$, i must combine this result with the above solution that i got  in order to get $x_{0}$ and hence the point ($x_{0}$,$y_{0}$) but im unsure how to do so, especially when there is a $c$ involved. Could anyone explain. Thanks","Consider the initial value problem $$y′=3x(y−1)^{1/3},y(x_{0})=y_{0}$$ Using the existness and uniqueness theorem, for what points ($x_{0}$,$y_{0}$) imply that the above IVP  has a unique solution on some open interval that contains $x_{0}$? What i tried Using the seperable equation method,  I got  a solution of $$y=1+(x^{2}+c)^{1.5}$$ Im unsure of how to find the value of $c$ given the inital conditions and whether is it necessary to find the value of $c$ for this problem. While using the  existence and uniqueness theorem, $y_{0}$ must not be equal to $1$ in order for a unique solution to exist. But from here im unsure of how to find the points  ($x_{0}$,$y_{0}$), I was thinking that since i already know that $y_{0}$ does not equals to $1$, i must combine this result with the above solution that i got  in order to get $x_{0}$ and hence the point ($x_{0}$,$y_{0}$) but im unsure how to do so, especially when there is a $c$ involved. Could anyone explain. Thanks",,['ordinary-differential-equations']
41,Additional formula for tangent function?,Additional formula for tangent function?,,"I am having some trouble with this one problem. The question is: Suppose we wish to find a real-valued, differentiable function $F(x)$ that satisfies the functional equation $$F(x+y) =\frac{F(x)+F(y)}{1-(F(x)*F(y))}$$ Show that $F$ necessarily satisfies $F(0) = 0$. Hint: Use the above to get an expression for F(0+0) and then use the fact that we seek F to be real-valued. Set $a = F'(0)$. Show that $F$ must satisfy the diff  $\frac{dF}{dx} = a(1+F(x)^2)$ Hint: Differentiate the above WRT to $y$ then set $y=0$ I don't quite understand how to show that $F(0)=0$ and how to use the fact that $F$ is real valued to solve the problem. Any help would be appreciated, thanks!","I am having some trouble with this one problem. The question is: Suppose we wish to find a real-valued, differentiable function $F(x)$ that satisfies the functional equation $$F(x+y) =\frac{F(x)+F(y)}{1-(F(x)*F(y))}$$ Show that $F$ necessarily satisfies $F(0) = 0$. Hint: Use the above to get an expression for F(0+0) and then use the fact that we seek F to be real-valued. Set $a = F'(0)$. Show that $F$ must satisfy the diff  $\frac{dF}{dx} = a(1+F(x)^2)$ Hint: Differentiate the above WRT to $y$ then set $y=0$ I don't quite understand how to show that $F(0)=0$ and how to use the fact that $F$ is real valued to solve the problem. Any help would be appreciated, thanks!",,['ordinary-differential-equations']
42,Radioactive decay of an element A into an element B,Radioactive decay of an element A into an element B,,"It's well known that the rate of decay of an element is proportional to its amount. Suppose we have a radioactive element $A$ which decays into a radioactive element $B$ ($B$ also decays). If the initial amount of $A$ is $A_0$ and there is no element $B$ at the beginning find a formula for the amount of $B$ at time $t$, $B(t)$. I figured that the amount of $A$ that turned into $B$ in time $t$ is $t\dfrac{dA(t)}{dt}$. So the rate of change of $B$ is $\dfrac{d}{dt}\left(t\dfrac{dA(t)}{dt}\right)=\dfrac{dA(t)}{dt}+t\dfrac{d^2A(t)}{dt^2}=\dfrac{dB(t)}{dt}=bB(t)$ and therefore $$bB(t)=aA(t)+ta\dfrac{dA(t)}{dt}$$ where $a,b$ are the decay constants.I don't know if my thinking is correct, probably not because when I solve this DE I get a wrong answer which says that at time $t=0$ $B(t)\not=0$.","It's well known that the rate of decay of an element is proportional to its amount. Suppose we have a radioactive element $A$ which decays into a radioactive element $B$ ($B$ also decays). If the initial amount of $A$ is $A_0$ and there is no element $B$ at the beginning find a formula for the amount of $B$ at time $t$, $B(t)$. I figured that the amount of $A$ that turned into $B$ in time $t$ is $t\dfrac{dA(t)}{dt}$. So the rate of change of $B$ is $\dfrac{d}{dt}\left(t\dfrac{dA(t)}{dt}\right)=\dfrac{dA(t)}{dt}+t\dfrac{d^2A(t)}{dt^2}=\dfrac{dB(t)}{dt}=bB(t)$ and therefore $$bB(t)=aA(t)+ta\dfrac{dA(t)}{dt}$$ where $a,b$ are the decay constants.I don't know if my thinking is correct, probably not because when I solve this DE I get a wrong answer which says that at time $t=0$ $B(t)\not=0$.",,['ordinary-differential-equations']
43,Solving an ODE using shooting method,Solving an ODE using shooting method,,"I am trying to solve the following ODE for my maths project: $$ y'' = \frac{\alpha}{2}y^3 - \frac{3}{2}y^2 + y - \frac{3}{x} y'$$ under the following boundary conditions: $$ y'(0) = 0 \\ y(x) \rightarrow y \_ \equiv 0\ \text{as}\ x \rightarrow \infty $$ As a first step, I converted this problem into a set of coupled ODEs: $$ \frac{dy}{dx} = z \\ \frac{dz}{dx} = \frac{\alpha}{2}y^3 - \frac{3}{2}y^2 + y - \frac{3}{x} z$$ under the following boundary conditions: $$ z(0) = 0 \\ y(x) \rightarrow y \_ \equiv 0\ \text{as}\ x \rightarrow \infty$$ Next, my source tells me to use the shooting method to convert the BVP into an IVP, which means that I have to use two initial guesses of $ y(0) $ to be able to use the secant method to find the appropriate value of $ y(0) $. Now, my question is, according to my source, I can avoid the singularity at x = 0 using Taylor expansion as follows: $$ y(r_{0}) = y_{0} + \frac{1}{16} r_{0}^{2} (2y_{0} - 3y_{0}^{2} + \alpha y_{0}^{3}) $$ I see how you can estimate $ y(r_{0}) $ where $ r_{0} $ is a tiny distance away from the origin, but I don't really see how they derived this expression. Could anyone help me out?","I am trying to solve the following ODE for my maths project: $$ y'' = \frac{\alpha}{2}y^3 - \frac{3}{2}y^2 + y - \frac{3}{x} y'$$ under the following boundary conditions: $$ y'(0) = 0 \\ y(x) \rightarrow y \_ \equiv 0\ \text{as}\ x \rightarrow \infty $$ As a first step, I converted this problem into a set of coupled ODEs: $$ \frac{dy}{dx} = z \\ \frac{dz}{dx} = \frac{\alpha}{2}y^3 - \frac{3}{2}y^2 + y - \frac{3}{x} z$$ under the following boundary conditions: $$ z(0) = 0 \\ y(x) \rightarrow y \_ \equiv 0\ \text{as}\ x \rightarrow \infty$$ Next, my source tells me to use the shooting method to convert the BVP into an IVP, which means that I have to use two initial guesses of $ y(0) $ to be able to use the secant method to find the appropriate value of $ y(0) $. Now, my question is, according to my source, I can avoid the singularity at x = 0 using Taylor expansion as follows: $$ y(r_{0}) = y_{0} + \frac{1}{16} r_{0}^{2} (2y_{0} - 3y_{0}^{2} + \alpha y_{0}^{3}) $$ I see how you can estimate $ y(r_{0}) $ where $ r_{0} $ is a tiny distance away from the origin, but I don't really see how they derived this expression. Could anyone help me out?",,['ordinary-differential-equations']
44,How to solve $y=(xy'+2y)^2$?,How to solve ?,y=(xy'+2y)^2,What kind of differential equation is this thing and how to solve it? $$y=(xy'+2y)^2$$ $$y=x^2y'^2+4xyy'+4y^2$$,What kind of differential equation is this thing and how to solve it? $$y=(xy'+2y)^2$$ $$y=x^2y'^2+4xyy'+4y^2$$,,['ordinary-differential-equations']
45,Form of initial conditions to an ODE,Form of initial conditions to an ODE,,"Can an ODE of the form (i.e. second order ODE): $$ x''= \ x$$ only have initial values of the form: $$x(0)=A \ \ \ ,\ \ \ x'(0)=B $$ I mean, can we have an initial condition like $ \ \  x(0)=A \ \ \ ,\ \ \ x''(0)=B $ , i.e., involving a second order initial condition?","Can an ODE of the form (i.e. second order ODE): $$ x''= \ x$$ only have initial values of the form: $$x(0)=A \ \ \ ,\ \ \ x'(0)=B $$ I mean, can we have an initial condition like $ \ \  x(0)=A \ \ \ ,\ \ \ x''(0)=B $ , i.e., involving a second order initial condition?",,['ordinary-differential-equations']
46,Phase Portrait of DE's,Phase Portrait of DE's,,How would I graph the phase portrait of $$ x' = x^2+y^2-2 \qquad y' = y-x^2 $$ ? Could someone provide some insight by hand or perhaps a computer-generated image?,How would I graph the phase portrait of $$ x' = x^2+y^2-2 \qquad y' = y-x^2 $$ ? Could someone provide some insight by hand or perhaps a computer-generated image?,,"['linear-algebra', 'ordinary-differential-equations']"
47,A problem on infinite domain diffusion equation,A problem on infinite domain diffusion equation,,"Consider the following problem $$u_t-u_{xx}=p(x,t), -\infty<x<\infty,t>0$$ $$u(x,0)=0$$ $$u\rightarrow0 \text{ as } x\rightarrow \pm \infty$$ This can be solved using many sub problems as follows. Let $\zeta>0, \tau >0$. Consider the neighborhood $\zeta\leq x \leq \Delta \zeta+\zeta,\tau\leq t \leq \Delta \tau+\tau$ and $p(\zeta,\tau) $ is constant in this neighborhood. Now we build a new problem as follows. $$u_{1t}-u_{1xx}=p(\zeta,\tau) \delta(x-\zeta)\delta(t-\tau)d\zeta d\tau, -\infty<x<\infty,t>0$$ $$u_1(x,0)=0$$ $$u_1\rightarrow0 \text{ as } x\rightarrow \pm \infty$$ Then the solution is $$u_1(x,t)=\frac{p(\zeta,\tau) d\zeta d\tau}{\sqrt{4\pi(t-\tau)}}exp(-\frac{(x-\zeta)^2}{4(t-\tau)})$$ So the solution to main problem is $$u(x,t)=\int_0^t \int_{-\infty}^{\infty}\frac{p(\zeta,\tau)}{\sqrt{4\pi(t-\tau)}}exp(-\frac{(x-\zeta)^2}{4(t-\tau)}) d\zeta d\tau$$ I am not sure whether everything I have done here is right. But a similar way of solving is given below. $$u_{1t}-u_{1xx}=p(\zeta,\tau) \delta(x-\zeta)\delta(t-\tau), -\infty<x<\infty,t>0$$ $$u_1(x,0)=0$$ $$u_1\rightarrow0 \text{ as } x\rightarrow \pm \infty$$ Then the solution is $$u_1(x,t)=\frac{p(\zeta,\tau) }{\sqrt{4\pi(t-\tau)}}exp(-\frac{(x-\zeta)^2}{4(t-\tau)})$$  $$u(x,t)=\int_0^t \int_{-\infty}^{\infty}\frac{p(\zeta,\tau)}{\sqrt{4\pi(t-\tau)}}exp(-\frac{(x-\zeta)^2}{4(t-\tau)}) d\zeta d\tau$$ Are both of these methods correct or one of them is? Any help will me much appreciated as this has confused me for many days! thanks!","Consider the following problem $$u_t-u_{xx}=p(x,t), -\infty<x<\infty,t>0$$ $$u(x,0)=0$$ $$u\rightarrow0 \text{ as } x\rightarrow \pm \infty$$ This can be solved using many sub problems as follows. Let $\zeta>0, \tau >0$. Consider the neighborhood $\zeta\leq x \leq \Delta \zeta+\zeta,\tau\leq t \leq \Delta \tau+\tau$ and $p(\zeta,\tau) $ is constant in this neighborhood. Now we build a new problem as follows. $$u_{1t}-u_{1xx}=p(\zeta,\tau) \delta(x-\zeta)\delta(t-\tau)d\zeta d\tau, -\infty<x<\infty,t>0$$ $$u_1(x,0)=0$$ $$u_1\rightarrow0 \text{ as } x\rightarrow \pm \infty$$ Then the solution is $$u_1(x,t)=\frac{p(\zeta,\tau) d\zeta d\tau}{\sqrt{4\pi(t-\tau)}}exp(-\frac{(x-\zeta)^2}{4(t-\tau)})$$ So the solution to main problem is $$u(x,t)=\int_0^t \int_{-\infty}^{\infty}\frac{p(\zeta,\tau)}{\sqrt{4\pi(t-\tau)}}exp(-\frac{(x-\zeta)^2}{4(t-\tau)}) d\zeta d\tau$$ I am not sure whether everything I have done here is right. But a similar way of solving is given below. $$u_{1t}-u_{1xx}=p(\zeta,\tau) \delta(x-\zeta)\delta(t-\tau), -\infty<x<\infty,t>0$$ $$u_1(x,0)=0$$ $$u_1\rightarrow0 \text{ as } x\rightarrow \pm \infty$$ Then the solution is $$u_1(x,t)=\frac{p(\zeta,\tau) }{\sqrt{4\pi(t-\tau)}}exp(-\frac{(x-\zeta)^2}{4(t-\tau)})$$  $$u(x,t)=\int_0^t \int_{-\infty}^{\infty}\frac{p(\zeta,\tau)}{\sqrt{4\pi(t-\tau)}}exp(-\frac{(x-\zeta)^2}{4(t-\tau)}) d\zeta d\tau$$ Are both of these methods correct or one of them is? Any help will me much appreciated as this has confused me for many days! thanks!",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-physics']"
48,Simple second order differential equation of the form $f''(x)+ h(x) = 0$,Simple second order differential equation of the form,f''(x)+ h(x) = 0,"The problem is as follows: The movement equation for a pendulum for some mass hanging in a weightless thread with the length $L$ is as follows: $$\frac{d^2a}{dt^2} + \frac{g}{L}\sin(a) = 0$$ Where $a$ is the starting angle (in radians) and $g$ is the acceleration of gravity. Determine the position of the pendulum after a second if $L = 0.2$m and if the starting angle of the pendulum is $3$ degrees and the velocity is $0$m/s. You may also use the approximation $\sin(a) = a$. My thoughts on it Well, why not just move over the $(g/L)\sin(a)$ term to the right hand side and find the indefinite integral of both sides twice? I'm pretty sure that's not what you're supposed to do however and I'm frankly stumped. All the problems I've done so far has been of the form $f''(x) + f'(x) + f(x) = 0$. The answer provided is: $a = (\pi/60)\cos\sqrt{5g}$ which is approximately $2.26$ degrees.","The problem is as follows: The movement equation for a pendulum for some mass hanging in a weightless thread with the length $L$ is as follows: $$\frac{d^2a}{dt^2} + \frac{g}{L}\sin(a) = 0$$ Where $a$ is the starting angle (in radians) and $g$ is the acceleration of gravity. Determine the position of the pendulum after a second if $L = 0.2$m and if the starting angle of the pendulum is $3$ degrees and the velocity is $0$m/s. You may also use the approximation $\sin(a) = a$. My thoughts on it Well, why not just move over the $(g/L)\sin(a)$ term to the right hand side and find the indefinite integral of both sides twice? I'm pretty sure that's not what you're supposed to do however and I'm frankly stumped. All the problems I've done so far has been of the form $f''(x) + f'(x) + f(x) = 0$. The answer provided is: $a = (\pi/60)\cos\sqrt{5g}$ which is approximately $2.26$ degrees.",,"['calculus', 'ordinary-differential-equations']"
49,Nonlinear Differential Equation question,Nonlinear Differential Equation question,,"I have a nonlinear Diffeq: $$\frac{d^2x}{dt^2}+\beta \frac{dx}{dt}+\epsilon \times e^{- \lambda x} = f(t) $$ where $f(t)$ is a function that is known, and $\beta$ and $\lambda$ are constants that are known. Also, we know that $\epsilon$ is a constant parameter that is small. I first need to obtain the zero order solution $x_0$, before finding the first order solution $x_1$ The first thing that I need to do is to use asymptotic expansions to obtain solutions of order $\epsilon=0$ and (TYPO) Note that general solution for f(t) that will have two unknown constants. UPDATE: After the first order term is solve, it needs to be plugged back in. The exponential needs to be linearized and things should start cancelling out. I am not sure how to do this, I just know this is what needs to be done. UPDATE2: Correction, $\epsilon = 1$ was a typo. It should be $\epsilon^1$ I need to find a solution in the form: $$x(t)=x_0(t)+\epsilon^1x_1(t)+\epsilon^2x_2 (t) + ... $$ So initially, $\epsilon$ needs to be set to 0 in order to obtain $x_0$. To find $x_1$, I need $\epsilon^1$ UPDATE3: I know now that I need to plug: $$x=x_0+\epsilon_1x_1 $$ back into the original equation Thus: $$\frac{d^2}{dt^2}(x_0+\epsilon_1x_1) + \beta\frac{d}{dt}(x_0+\epsilon_1x_1)+\epsilon \times exp(-\lambda(x_0+\epsilon_1x_1))  $$ Then $$\frac{d^2}{dt^2}x_0+\frac{d^2}{dt^2}\epsilon_1x_1+\beta \frac{d}{dt}x_0 +\beta \frac{d}{dt}\epsilon_1 x_1+\epsilon \times exp(-\lambda x_0))+\epsilon \times exp(-\lambda \epsilon_1 x_1)$$ I think then the $x_0$ terms may cancel with f(t) or something like that? It may be some sort of approximation. I still need to linearize the exponential. Any help is appreciated. Update4: Taking the solution a little but further... We know that: $$\frac{d^2x_0}{dt^2}+\beta \frac{dx_0}{dt} = f(t) $$ So, those terms all cancel. And now we have: $$\frac{d^2}{dt^2}\epsilon_1x_1 +\beta \frac{d}{dt}\epsilon_1 x_1+\epsilon \times exp(-\lambda(x_0+\epsilon_1x_1))=0$$ But we dont want $\epsilon^2$ terms, to part of the exponential goes away as well. We are left with: $$\frac{d^2}{dt^2}\epsilon_1x_1 +\beta \frac{d}{dt}\epsilon_1x_1+\epsilon \times exp(-\lambda x_0)=0$$ Where we know $x_0$. This now means that the exponential is no longer a function of arbitrary x. I feel like the solution should be trivial now, but I am having a hard time finding it. Any ideas? Can this be solved with the method of undetermined coefficients? UPDATE5: Well I have updated this problem several times with very little response. As a latch ditch effort, is there anyone who can offer any advice on how to solve: $$\frac{d^2x_1}{dt^2}  +\beta \frac{dx_1}{dt} +  exp{-\lambda x_0}=0$$ where $x_0$ is known","I have a nonlinear Diffeq: $$\frac{d^2x}{dt^2}+\beta \frac{dx}{dt}+\epsilon \times e^{- \lambda x} = f(t) $$ where $f(t)$ is a function that is known, and $\beta$ and $\lambda$ are constants that are known. Also, we know that $\epsilon$ is a constant parameter that is small. I first need to obtain the zero order solution $x_0$, before finding the first order solution $x_1$ The first thing that I need to do is to use asymptotic expansions to obtain solutions of order $\epsilon=0$ and (TYPO) Note that general solution for f(t) that will have two unknown constants. UPDATE: After the first order term is solve, it needs to be plugged back in. The exponential needs to be linearized and things should start cancelling out. I am not sure how to do this, I just know this is what needs to be done. UPDATE2: Correction, $\epsilon = 1$ was a typo. It should be $\epsilon^1$ I need to find a solution in the form: $$x(t)=x_0(t)+\epsilon^1x_1(t)+\epsilon^2x_2 (t) + ... $$ So initially, $\epsilon$ needs to be set to 0 in order to obtain $x_0$. To find $x_1$, I need $\epsilon^1$ UPDATE3: I know now that I need to plug: $$x=x_0+\epsilon_1x_1 $$ back into the original equation Thus: $$\frac{d^2}{dt^2}(x_0+\epsilon_1x_1) + \beta\frac{d}{dt}(x_0+\epsilon_1x_1)+\epsilon \times exp(-\lambda(x_0+\epsilon_1x_1))  $$ Then $$\frac{d^2}{dt^2}x_0+\frac{d^2}{dt^2}\epsilon_1x_1+\beta \frac{d}{dt}x_0 +\beta \frac{d}{dt}\epsilon_1 x_1+\epsilon \times exp(-\lambda x_0))+\epsilon \times exp(-\lambda \epsilon_1 x_1)$$ I think then the $x_0$ terms may cancel with f(t) or something like that? It may be some sort of approximation. I still need to linearize the exponential. Any help is appreciated. Update4: Taking the solution a little but further... We know that: $$\frac{d^2x_0}{dt^2}+\beta \frac{dx_0}{dt} = f(t) $$ So, those terms all cancel. And now we have: $$\frac{d^2}{dt^2}\epsilon_1x_1 +\beta \frac{d}{dt}\epsilon_1 x_1+\epsilon \times exp(-\lambda(x_0+\epsilon_1x_1))=0$$ But we dont want $\epsilon^2$ terms, to part of the exponential goes away as well. We are left with: $$\frac{d^2}{dt^2}\epsilon_1x_1 +\beta \frac{d}{dt}\epsilon_1x_1+\epsilon \times exp(-\lambda x_0)=0$$ Where we know $x_0$. This now means that the exponential is no longer a function of arbitrary x. I feel like the solution should be trivial now, but I am having a hard time finding it. Any ideas? Can this be solved with the method of undetermined coefficients? UPDATE5: Well I have updated this problem several times with very little response. As a latch ditch effort, is there anyone who can offer any advice on how to solve: $$\frac{d^2x_1}{dt^2}  +\beta \frac{dx_1}{dt} +  exp{-\lambda x_0}=0$$ where $x_0$ is known",,['ordinary-differential-equations']
50,Problem involving system of differential equations,Problem involving system of differential equations,,Solve following system of diferential equations$$\begin{cases} \frac{ds}{dt}=y+z\\  \frac{dy}{dt}=s+z\\ \frac{dz}{dt}=z-s. \end{cases}.$$ I tried many tehniques without any success. I would appreciate some help with this problem. One of my tries $$\frac{dz}{dt}=z-s\Rightarrow \frac{e^{-t}dz}{ds}=ze^{-t}-se^{-t}\Rightarrow -\frac{d(e^{-t})}{dt}\frac{dz}{dt}=-z\frac{d(e^{-t})}{dt}+s\frac{d(e^{-t})}{dt}\Rightarrow$$ $$\Rightarrow \frac{d}{dt}\left( \frac{y}{e^t}\right)=-\frac{z}{e^t}\Rightarrow \int d\left(\frac{y}{e^t}\right)=-\int\frac{z}{e^t}dt\Rightarrow y=e^t\left(-\int\frac{z}{e^t}dt+C\right) $$ Inserting this into the first equations doesn't lead to anything pleasant.,Solve following system of diferential equations$$\begin{cases} \frac{ds}{dt}=y+z\\  \frac{dy}{dt}=s+z\\ \frac{dz}{dt}=z-s. \end{cases}.$$ I tried many tehniques without any success. I would appreciate some help with this problem. One of my tries $$\frac{dz}{dt}=z-s\Rightarrow \frac{e^{-t}dz}{ds}=ze^{-t}-se^{-t}\Rightarrow -\frac{d(e^{-t})}{dt}\frac{dz}{dt}=-z\frac{d(e^{-t})}{dt}+s\frac{d(e^{-t})}{dt}\Rightarrow$$ $$\Rightarrow \frac{d}{dt}\left( \frac{y}{e^t}\right)=-\frac{z}{e^t}\Rightarrow \int d\left(\frac{y}{e^t}\right)=-\int\frac{z}{e^t}dt\Rightarrow y=e^t\left(-\int\frac{z}{e^t}dt+C\right) $$ Inserting this into the first equations doesn't lead to anything pleasant.,,['ordinary-differential-equations']
51,"Find $a,b$ to make $V$ a Lyapunov function",Find  to make  a Lyapunov function,"a,b V","Given $V(x,y)=ax^2+by^2$ I'm asked to find $a$ and $b$ to make $V$ a Lyapunov function for the following systems: $(1)$\begin{cases} x'= -x^\color{red}{3}+xy^2 \\ y'= -\color{red}{2}x^2y-y^3\end{cases} Here I have $\dot{V}(x,y)=2ax(-x^2+xy^2)+2b(-y^3)=2ax^3+2ax^2y-4bx^2y^2-2by^4$. The first two termins can take positive and negative values depending on $x$, then here I would set $a=0$; then $b>0$ to make it $\dot{V}(x,y)\leq 0$. $(2)$\begin{cases} x'= -x^\color{red}{3}/2+2xy^2 \\ y'= -y^3\end{cases} In this case is $\dot{V}(x,y)=2ax(-x^2/2+2xy^2)+2b(-y³)=-ax^3-x^2y^2-2by^3$. For the middle term is $-x^2y^2\leq 0$, but $-ax^3-2by^3$ could be less or greater than zero for any $a,b$ chossing an appropiate $x,y$. Then in this case $a=b=0$ is the only possible solution? UPDATE: I copied the problem from my notes instead of the original problem, doing this I messed up some of the exponents. The fixed exponents are those in red, which I before miscopied as 2; I also missed a constant.","Given $V(x,y)=ax^2+by^2$ I'm asked to find $a$ and $b$ to make $V$ a Lyapunov function for the following systems: $(1)$\begin{cases} x'= -x^\color{red}{3}+xy^2 \\ y'= -\color{red}{2}x^2y-y^3\end{cases} Here I have $\dot{V}(x,y)=2ax(-x^2+xy^2)+2b(-y^3)=2ax^3+2ax^2y-4bx^2y^2-2by^4$. The first two termins can take positive and negative values depending on $x$, then here I would set $a=0$; then $b>0$ to make it $\dot{V}(x,y)\leq 0$. $(2)$\begin{cases} x'= -x^\color{red}{3}/2+2xy^2 \\ y'= -y^3\end{cases} In this case is $\dot{V}(x,y)=2ax(-x^2/2+2xy^2)+2b(-y³)=-ax^3-x^2y^2-2by^3$. For the middle term is $-x^2y^2\leq 0$, but $-ax^3-2by^3$ could be less or greater than zero for any $a,b$ chossing an appropiate $x,y$. Then in this case $a=b=0$ is the only possible solution? UPDATE: I copied the problem from my notes instead of the original problem, doing this I messed up some of the exponents. The fixed exponents are those in red, which I before miscopied as 2; I also missed a constant.",,"['ordinary-differential-equations', 'solution-verification']"
52,How does the recursion relation work in the solution to this differential equation (using series)?,How does the recursion relation work in the solution to this differential equation (using series)?,,"Sorry for the vague title but it would not let me post the first step and last step of this equation (too many characters!). How does $$\dfrac{a_0}{3n(3n-1)(3n-3)(3n-4)\cdots 9 \cdot 8 \cdot 6 \cdot 5 \cdot 3 \cdot 2} = \dfrac{a_0\Gamma(\frac{2}{3})}{3^nn!3^n\Gamma(n+\frac{2}{3})}?$$ This is a text book example I'm having trouble following. Solve the differential equation $y'' = xy$ using series. This makes sense in general. But when we get down to solving for the coefficients of the series, it confuses me. Based of the relations $$a_nn(n-1) = 0 \text{ for } n=0,1,2$$ and $$a_nn(n-1) = a_{n-3} \text{ for } n = 3,4,\ldots$$ we have $$\begin{align} a_{3n} &=  \dfrac{a_0}{3n(3n-1)(3n-3)(3n-4)\cdots 9 \cdot 8 \cdot 6 \cdot 5 \cdot 3 \cdot 2} \\ &= \dfrac{a_0}{3^n n! 3^n(n-\frac{1}{3})(n-1-\frac{1}{3})(n-2-\frac{1}{3})\cdots (\frac{5}{3}) (\frac{2}{3})}\\ &= \dfrac{a_0\Gamma(\frac{2}{3})}{3^nn!3^n\Gamma(n+\frac{2}{3})}\\ \end{align}$$ Particularly, that middle step there is throwing me off. The book also provided $$\begin{align} a_{3n+1} &=  \dfrac{a_1}{(3n+1)(3n)(3n-2)(3n-3)\cdots 10 \cdot 9 \cdot 7 \cdot 6 \cdot 4 \cdot 3} \\ &= \dfrac{a_1}{3^n n! 3^n(n+\frac{1}{3})(n-1+\frac{1}{3})(n-2+\frac{1}{3})\cdots (\frac{7}{3}) (\frac{4}{3})}\\ &= \dfrac{a_1\Gamma(\frac{4}{3})}{3^nn!3^n\Gamma(n+\frac{4}{3})}\\ \end{align}$$ and $$a_{3n+2} = 0,$$ but that last one makes sense. As a side note, why do they put $3^n$ twice in the denominator? Why not $3^{2n}$?","Sorry for the vague title but it would not let me post the first step and last step of this equation (too many characters!). How does $$\dfrac{a_0}{3n(3n-1)(3n-3)(3n-4)\cdots 9 \cdot 8 \cdot 6 \cdot 5 \cdot 3 \cdot 2} = \dfrac{a_0\Gamma(\frac{2}{3})}{3^nn!3^n\Gamma(n+\frac{2}{3})}?$$ This is a text book example I'm having trouble following. Solve the differential equation $y'' = xy$ using series. This makes sense in general. But when we get down to solving for the coefficients of the series, it confuses me. Based of the relations $$a_nn(n-1) = 0 \text{ for } n=0,1,2$$ and $$a_nn(n-1) = a_{n-3} \text{ for } n = 3,4,\ldots$$ we have $$\begin{align} a_{3n} &=  \dfrac{a_0}{3n(3n-1)(3n-3)(3n-4)\cdots 9 \cdot 8 \cdot 6 \cdot 5 \cdot 3 \cdot 2} \\ &= \dfrac{a_0}{3^n n! 3^n(n-\frac{1}{3})(n-1-\frac{1}{3})(n-2-\frac{1}{3})\cdots (\frac{5}{3}) (\frac{2}{3})}\\ &= \dfrac{a_0\Gamma(\frac{2}{3})}{3^nn!3^n\Gamma(n+\frac{2}{3})}\\ \end{align}$$ Particularly, that middle step there is throwing me off. The book also provided $$\begin{align} a_{3n+1} &=  \dfrac{a_1}{(3n+1)(3n)(3n-2)(3n-3)\cdots 10 \cdot 9 \cdot 7 \cdot 6 \cdot 4 \cdot 3} \\ &= \dfrac{a_1}{3^n n! 3^n(n+\frac{1}{3})(n-1+\frac{1}{3})(n-2+\frac{1}{3})\cdots (\frac{7}{3}) (\frac{4}{3})}\\ &= \dfrac{a_1\Gamma(\frac{4}{3})}{3^nn!3^n\Gamma(n+\frac{4}{3})}\\ \end{align}$$ and $$a_{3n+2} = 0,$$ but that last one makes sense. As a side note, why do they put $3^n$ twice in the denominator? Why not $3^{2n}$?",,"['sequences-and-series', 'ordinary-differential-equations', 'recurrence-relations', 'closed-form', 'gamma-function']"
53,Laplace Transform...,Laplace Transform...,,Find the Laplace transform of $t^2e^{at}cos(bt)$ My attempt: $\large\mathit{L}\{t^2e^{at}\cos(bt)\}$ = $\large\mathit{L}\{\frac12t^2e^{at}(e^{ibt}+e^{-ibt})\}$ ... ... $=\large\dfrac{1}{(s-a-ib)^3} + \dfrac{1}{(s - a +ib)^3}$ Where $i^2 = -1$ My current answer right now was from using the Laplace table How can I get rid of $i$?,Find the Laplace transform of $t^2e^{at}cos(bt)$ My attempt: $\large\mathit{L}\{t^2e^{at}\cos(bt)\}$ = $\large\mathit{L}\{\frac12t^2e^{at}(e^{ibt}+e^{-ibt})\}$ ... ... $=\large\dfrac{1}{(s-a-ib)^3} + \dfrac{1}{(s - a +ib)^3}$ Where $i^2 = -1$ My current answer right now was from using the Laplace table How can I get rid of $i$?,,"['ordinary-differential-equations', 'laplace-transform']"
54,Exponential growth and decay,Exponential growth and decay,,"There's a cup of coffee made with boiling water standing at room where room temperature is $20ºC$. If $H(t)$ is the temperature of this cup of coffee at the time $t$, in minutes, explain what the differential equation says in everyday terms. What is the sign of $k$? $$\frac{dh}{dt} = -k(H-20)$$ Then solve the differential equation for 90ºC in 2 minutes and how long it will take to cool to 60ºC Observing $dh/dt = 0$ we find that $H=20$ this means that the function stops changing at the room temperature $H=20$. As $t$ is implied to be $H = 20 + Ae^{-kt}$ as $t$ approaches infinity $H=20$.","There's a cup of coffee made with boiling water standing at room where room temperature is $20ºC$. If $H(t)$ is the temperature of this cup of coffee at the time $t$, in minutes, explain what the differential equation says in everyday terms. What is the sign of $k$? $$\frac{dh}{dt} = -k(H-20)$$ Then solve the differential equation for 90ºC in 2 minutes and how long it will take to cool to 60ºC Observing $dh/dt = 0$ we find that $H=20$ this means that the function stops changing at the room temperature $H=20$. As $t$ is implied to be $H = 20 + Ae^{-kt}$ as $t$ approaches infinity $H=20$.",,"['calculus', 'ordinary-differential-equations', 'exponential-function']"
55,Direction Field and Trajectories,Direction Field and Trajectories,,I am wondering how to draw a direction field and trajectories of a system of linear equations: $$ x'= \left[   \begin{array}{ c c }      4 & -2 \\      8 & -4   \end{array} \right] x .$$ I remember how to do them for a equation from the first part of ODE where we would have $$y'=2x-y$$ but I do not really understand how to begin with the system in this form. Would it just be: $$  x'=4x_1 -2x_2 $$ $$ x'=8x_1-4x_2 $$ and we would just plot different values of $x_1$ and $x_2$?,I am wondering how to draw a direction field and trajectories of a system of linear equations: $$ x'= \left[   \begin{array}{ c c }      4 & -2 \\      8 & -4   \end{array} \right] x .$$ I remember how to do them for a equation from the first part of ODE where we would have $$y'=2x-y$$ but I do not really understand how to begin with the system in this form. Would it just be: $$  x'=4x_1 -2x_2 $$ $$ x'=8x_1-4x_2 $$ and we would just plot different values of $x_1$ and $x_2$?,,"['ordinary-differential-equations', 'systems-of-equations']"
56,Differential equation with zero solution of indicial equation?,Differential equation with zero solution of indicial equation?,,"I want to solve this equation $$ y'' + (\frac{1}{x} + 4x)y' + (5+4x^2)y = 0 $$ Where $y''$ is second derivative and so on. This equation has singuar point at $x=0$. And this is regular singular point. So, I used Frobenius method http://en.wikipedia.org/wiki/Frobenius_method . When I find indicial (or characteristic) equation of the solution, $$ p(0)=\lim_{x \to 0} x \bigg (\frac{1}{x} + 4x \bigg) = 1 \\ q(0)=\lim_{x \to 0} x^2 \bigg (5+4x^2\bigg) = 0 \\ $$ Indicial equation $r(r-1) + p(0)r + q(0)=0$ becomes $ r^2=0 \implies r=0,0$. Now, how to proceed with this equation? Any ideas, much appreciated.","I want to solve this equation $$ y'' + (\frac{1}{x} + 4x)y' + (5+4x^2)y = 0 $$ Where $y''$ is second derivative and so on. This equation has singuar point at $x=0$. And this is regular singular point. So, I used Frobenius method http://en.wikipedia.org/wiki/Frobenius_method . When I find indicial (or characteristic) equation of the solution, $$ p(0)=\lim_{x \to 0} x \bigg (\frac{1}{x} + 4x \bigg) = 1 \\ q(0)=\lim_{x \to 0} x^2 \bigg (5+4x^2\bigg) = 0 \\ $$ Indicial equation $r(r-1) + p(0)r + q(0)=0$ becomes $ r^2=0 \implies r=0,0$. Now, how to proceed with this equation? Any ideas, much appreciated.",,"['ordinary-differential-equations', 'special-functions', 'bessel-functions', 'frobenius-groups']"
57,show that all other solutions are bounded,show that all other solutions are bounded,,Suppose $G(x)$ is a solution of the differential equation $$x'(t)\ =\left(\begin{matrix}-5&2\\-4&1\end{matrix}\right) \ x(t)+ \ f(t)$$   where $f(t)$ is a continous function and $Sup\{|G(x)|: t_0\le x <\infty\}<\infty$ show that all other solutions are bounded.,Suppose $G(x)$ is a solution of the differential equation $$x'(t)\ =\left(\begin{matrix}-5&2\\-4&1\end{matrix}\right) \ x(t)+ \ f(t)$$   where $f(t)$ is a continous function and $Sup\{|G(x)|: t_0\le x <\infty\}<\infty$ show that all other solutions are bounded.,,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
58,The volume of a spherical balloon (constant rate),The volume of a spherical balloon (constant rate),,The volume of a spherical balloon is increasing at a rate of $3$ cubic inches per second. After you find the rate of change of the balloon's radius at the time when the radius is $8$ inches explain why this rate is not constant. I found $dr/dt$ to equal $(3/4)\pi r^2$ but why can't $dr/dt$ be constant?,The volume of a spherical balloon is increasing at a rate of $3$ cubic inches per second. After you find the rate of change of the balloon's radius at the time when the radius is $8$ inches explain why this rate is not constant. I found $dr/dt$ to equal $(3/4)\pi r^2$ but why can't $dr/dt$ be constant?,,['ordinary-differential-equations']
59,Classification of pde,Classification of pde,,"I got stuck on the following problem: Determine the subsets of $\mathbb{R}^2$ where the pde $$u_{xx}+2xu_xu_{xy}+yu_{yy}+yu_x=1$$ is elliptic, hyperbolic and parabolic respectively. Now, at first I thought this to be an exceedingly simple task, all we need to do is look at the determinant of $$\begin{pmatrix}1 & xu_x \\xu_x & y\end{pmatrix}.$$ If it is positive, we are elliptic, if it's negative, we are hyperbolic, if it's $0$, we are parabolic. So far, so good. However, what kind of ruined the notion that this was going to be oh so easy was the fact that suddenly I was left with the differential inequalitiy $$y> (xu_x)^2$$ with symmetric versions for $=,<$. Duh. Now, think positively, at least one does get the trivial case that for $y<0$ we are hyperbolic. And for $(x,y)=(0,0)$ we are parabolic. Also, for $x=0,y>0$ we are elliptic. Only about half of the real numbers left, yay. So in the following we could assume that $y\geq0$, you never know, it might help. In order to try and solve this, I figured I could at least try to do something for the parabolic case: $$\frac{y}{x^2}=u_x^2 \quad\Leftrightarrow \\ \frac{\sqrt{y}}{|x|}=|u_x|$$ and now I cannot even say anything definitive about the integral because, well, we have $|u_x|$ instead of $u_x$. Also, even if I could integrate, if I had $\sqrt{y}\ln|x|+c=|u(x,y)|$, I still would have no idea for which sets $(x,y)\in\mathbb{R}^2$ this is true. Same goes for the inequalities, if not worse, which goes to say I've basically not made any progress with this. One other thing I've tried is to at least look at the case $x\neq 0,y=0$, where if $u_x\neq0$ we are also trivially hyperbolic, but inserting that into the original pde doesn't seem to give me any kind of contradiction ($u_{xx}$ does, unfortunately, not need to be $0$, too - also, we do not (well, at least, I don't) know anything about it's sign, since it only appears in our inequality quadratically). Also note that it is explicitly stated that it is not necessary to solve the original pde. In case this is even possible; I'm fairly new to this subject. Hence I assume that I have simply been unable to find anything resembling a ""correct"" approach, and would be quite thankful for anyone shedding light on what to do here.","I got stuck on the following problem: Determine the subsets of $\mathbb{R}^2$ where the pde $$u_{xx}+2xu_xu_{xy}+yu_{yy}+yu_x=1$$ is elliptic, hyperbolic and parabolic respectively. Now, at first I thought this to be an exceedingly simple task, all we need to do is look at the determinant of $$\begin{pmatrix}1 & xu_x \\xu_x & y\end{pmatrix}.$$ If it is positive, we are elliptic, if it's negative, we are hyperbolic, if it's $0$, we are parabolic. So far, so good. However, what kind of ruined the notion that this was going to be oh so easy was the fact that suddenly I was left with the differential inequalitiy $$y> (xu_x)^2$$ with symmetric versions for $=,<$. Duh. Now, think positively, at least one does get the trivial case that for $y<0$ we are hyperbolic. And for $(x,y)=(0,0)$ we are parabolic. Also, for $x=0,y>0$ we are elliptic. Only about half of the real numbers left, yay. So in the following we could assume that $y\geq0$, you never know, it might help. In order to try and solve this, I figured I could at least try to do something for the parabolic case: $$\frac{y}{x^2}=u_x^2 \quad\Leftrightarrow \\ \frac{\sqrt{y}}{|x|}=|u_x|$$ and now I cannot even say anything definitive about the integral because, well, we have $|u_x|$ instead of $u_x$. Also, even if I could integrate, if I had $\sqrt{y}\ln|x|+c=|u(x,y)|$, I still would have no idea for which sets $(x,y)\in\mathbb{R}^2$ this is true. Same goes for the inequalities, if not worse, which goes to say I've basically not made any progress with this. One other thing I've tried is to at least look at the case $x\neq 0,y=0$, where if $u_x\neq0$ we are also trivially hyperbolic, but inserting that into the original pde doesn't seem to give me any kind of contradiction ($u_{xx}$ does, unfortunately, not need to be $0$, too - also, we do not (well, at least, I don't) know anything about it's sign, since it only appears in our inequality quadratically). Also note that it is explicitly stated that it is not necessary to solve the original pde. In case this is even possible; I'm fairly new to this subject. Hence I assume that I have simply been unable to find anything resembling a ""correct"" approach, and would be quite thankful for anyone shedding light on what to do here.",,"['ordinary-differential-equations', 'partial-differential-equations']"
60,How to solve this linear first order differential equation?,How to solve this linear first order differential equation?,,$$\frac{1}{N}\frac{dN}{dt} + 1 = te^{t+2}$$ The equation is separable and so is easily solvable. However doing so gives me the following: $$\int \frac{1}{N}dN = \int(te^{t+2} - 1)dt$$ Simplyifing gives: $$|N| = e^{-t + te^{t+2} - e^{t+2} + c}$$ How do I proceed from here?,$$\frac{1}{N}\frac{dN}{dt} + 1 = te^{t+2}$$ The equation is separable and so is easily solvable. However doing so gives me the following: $$\int \frac{1}{N}dN = \int(te^{t+2} - 1)dt$$ Simplyifing gives: $$|N| = e^{-t + te^{t+2} - e^{t+2} + c}$$ How do I proceed from here?,,"['calculus', 'ordinary-differential-equations']"
61,How to solve the following delay differential equation?,How to solve the following delay differential equation?,,"What is the solution for the following equation? $$\frac\partial{\partial q}f(s,q)= \frac s2 f(s+2,q)$$ Note, it is known that the solution for $$\frac\partial{\partial q}f(s,q)= s f(s+1,q)$$ will be Hurwitz Zeta $\zeta(s,-q)$.","What is the solution for the following equation? $$\frac\partial{\partial q}f(s,q)= \frac s2 f(s+2,q)$$ Note, it is known that the solution for $$\frac\partial{\partial q}f(s,q)= s f(s+1,q)$$ will be Hurwitz Zeta $\zeta(s,-q)$.",,"['ordinary-differential-equations', 'functional-equations', 'zeta-functions', 'delay-differential-equations']"
62,Simple Differential Equation $\displaystyle xy \frac{dy}{dx}=\sqrt{x^2-y^2-x^2y^2-1}$,Simple Differential Equation,\displaystyle xy \frac{dy}{dx}=\sqrt{x^2-y^2-x^2y^2-1},Solve $\displaystyle xy \frac{dy}{dx}=\sqrt{x^2-y^2-x^2y^2-1}$ I started off by assuming $y^2=V$ but could not get far,Solve $\displaystyle xy \frac{dy}{dx}=\sqrt{x^2-y^2-x^2y^2-1}$ I started off by assuming $y^2=V$ but could not get far,,['ordinary-differential-equations']
63,Symmetry group of the vector field $V=x \partial /\partial x + y \partial /\partial y$,Symmetry group of the vector field,V=x \partial /\partial x + y \partial /\partial y,"I was trying to solve an exercise in one of Arnold's book that asks for the symmetry group of the vector field $V=x \partial /\partial x + y \partial /\partial y$, that is the diffeomorphisms $g$ of $\mathbb{R}^2$ such that $g_*V = V$. I can see that the dilations and rotations are in this group and in general all the linear isormorphisms of $\mathbb{R}^2$. For the general solution I guess I have to look for the $g$ such that $Jg|_p V_p = V_{g(p)}$ but this is a PDE and I guess there should be another way to do it.","I was trying to solve an exercise in one of Arnold's book that asks for the symmetry group of the vector field $V=x \partial /\partial x + y \partial /\partial y$, that is the diffeomorphisms $g$ of $\mathbb{R}^2$ such that $g_*V = V$. I can see that the dilations and rotations are in this group and in general all the linear isormorphisms of $\mathbb{R}^2$. For the general solution I guess I have to look for the $g$ such that $Jg|_p V_p = V_{g(p)}$ but this is a PDE and I guess there should be another way to do it.",,"['ordinary-differential-equations', 'differential-geometry', 'vector-fields']"
64,How to use the Mehler kernel to get the solution of the Quantum harmonic oscillator with a given initial condition,How to use the Mehler kernel to get the solution of the Quantum harmonic oscillator with a given initial condition,,"In this wiki-article http://en.wikipedia.org/wiki/Mehler_kernel the fundamental solution of the differential equation for the Quantum harmonic oscillator is provided by the Mehler Kernel: $K(x,y,t)=\frac{\exp(-\coth(2t)(x^2+y^2)/2-\operatorname{cosech}(2t)xy)}{\sqrt{2\pi\sinh(2t)}}$ Now if I have given some initial condition $\Psi(x,t=0)$ I should get the time evolution of $\Psi$ by convoluting $\Psi$ and $K$ (in an analogous matter as was done here for the heat equation: http://en.wikipedia.org/wiki/Heat_equation#Fundamental_solutions ). But I am confused about the two spatial variables in $K$. Can anybody tell me what Integral I actually have to solve to obtain $\Psi(x,t)$?","In this wiki-article http://en.wikipedia.org/wiki/Mehler_kernel the fundamental solution of the differential equation for the Quantum harmonic oscillator is provided by the Mehler Kernel: $K(x,y,t)=\frac{\exp(-\coth(2t)(x^2+y^2)/2-\operatorname{cosech}(2t)xy)}{\sqrt{2\pi\sinh(2t)}}$ Now if I have given some initial condition $\Psi(x,t=0)$ I should get the time evolution of $\Psi$ by convoluting $\Psi$ and $K$ (in an analogous matter as was done here for the heat equation: http://en.wikipedia.org/wiki/Heat_equation#Fundamental_solutions ). But I am confused about the two spatial variables in $K$. Can anybody tell me what Integral I actually have to solve to obtain $\Psi(x,t)$?",,"['ordinary-differential-equations', 'mathematical-physics', 'quantum-mechanics', 'fundamental-solution']"
65,Separation of variables: when to have exponential solution and when sinusoidal?,Separation of variables: when to have exponential solution and when sinusoidal?,,"In separation of variables, one can assume a solution of V(x,y) = X(x)Y(y) and after plugging this into Laplace's equation which is: ${{\partial^2 V} \over {\partial x^2}}$ + ${{\partial^2 V} \over {\partial y^2}}$ = 0 we can get: ${d^2X \over dx^2}$ = ${k^2X}$ which gives a solution ${X(x) = Ae^{kx} - Be^{-kx}}$? and ${d^2Y \over dy^2}$ = ${-k^2Y}$ which gives solution ${Y(y) = C\sin(ky) - D\cos(ky)}$ (where k is some constant) However, I can't understand, why does positive ${k^2}$ give a solution with exponents and ${-k^2}$ has sinusoidal solution? Is it always so? The book that I am referencing this from is Griffith 3rd edition of ""Intro to Electrodynamics"". He does mention this there: ""If X were sinusoidal, we could never arrange for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both zero and a."" But I don't quite see this? Also, the image used is: Ps: I'm sorry if this has been asked before on this site. If it has, could someone please direct me to the answer, since I could not find it?","In separation of variables, one can assume a solution of V(x,y) = X(x)Y(y) and after plugging this into Laplace's equation which is: ${{\partial^2 V} \over {\partial x^2}}$ + ${{\partial^2 V} \over {\partial y^2}}$ = 0 we can get: ${d^2X \over dx^2}$ = ${k^2X}$ which gives a solution ${X(x) = Ae^{kx} - Be^{-kx}}$? and ${d^2Y \over dy^2}$ = ${-k^2Y}$ which gives solution ${Y(y) = C\sin(ky) - D\cos(ky)}$ (where k is some constant) However, I can't understand, why does positive ${k^2}$ give a solution with exponents and ${-k^2}$ has sinusoidal solution? Is it always so? The book that I am referencing this from is Griffith 3rd edition of ""Intro to Electrodynamics"". He does mention this there: ""If X were sinusoidal, we could never arrange for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both zero and a."" But I don't quite see this? Also, the image used is: Ps: I'm sorry if this has been asked before on this site. If it has, could someone please direct me to the answer, since I could not find it?",,"['ordinary-differential-equations', 'partial-differential-equations']"
66,Finding stream function from potential,Finding stream function from potential,,"I have a velocity potential (ie the gradient of this function gives the velocity) given by : $$\phi(x,y) = \frac{1}{\sqrt{(y-1)^2 + x^2}} + \frac{1}{\sqrt{(y+1)^2 + x^2}} + k x$$ where $k$ = constant. I would like to find the corresponding stream function $\psi(x,y)$ so that  $\psi(x,y)$ = constant on streamlines. Another way to state this is to say , find $\psi(x,y)$ such that : $$\psi_x = \phi_y \text{ and } \psi_y = -\phi_x$$ where subscripts denote the partial derivative. Is there a way to find $\psi(x,y)$ Thanks for your help.","I have a velocity potential (ie the gradient of this function gives the velocity) given by : $$\phi(x,y) = \frac{1}{\sqrt{(y-1)^2 + x^2}} + \frac{1}{\sqrt{(y+1)^2 + x^2}} + k x$$ where $k$ = constant. I would like to find the corresponding stream function $\psi(x,y)$ so that  $\psi(x,y)$ = constant on streamlines. Another way to state this is to say , find $\psi(x,y)$ such that : $$\psi_x = \phi_y \text{ and } \psi_y = -\phi_x$$ where subscripts denote the partial derivative. Is there a way to find $\psi(x,y)$ Thanks for your help.",,['ordinary-differential-equations']
67,numerical solution of integral equation,numerical solution of integral equation,,"Consider the basic type of integral equation. In particular, a volterra integral equation of the first kind. That is, we have the following integral equation $$\int_a^xf(s)g(s,x)~ds=h(x)$$ where $h$ and $g$ are known. and we want to obtain function $f(x)$. As I know, It does not have analytic solution except special cases. so numerical solution can be considered. the most basic approach is as follows: $$\sum_j w_jf(t_j)g(s_i,t_j)=h(s_i), i=1,...,n$$ by discretizing variables. Then $n$ equation solves $n$ values of $w_j$. Consequently, we can obtain $f(t_i)$ of dicreted version. What I am wondering is that this numerical solution converges to $f(x)$ as $n$ increases to $\infty$?? If so, what condition is needed? Please let me know releavanet paper or books Thanks in advances","Consider the basic type of integral equation. In particular, a volterra integral equation of the first kind. That is, we have the following integral equation $$\int_a^xf(s)g(s,x)~ds=h(x)$$ where $h$ and $g$ are known. and we want to obtain function $f(x)$. As I know, It does not have analytic solution except special cases. so numerical solution can be considered. the most basic approach is as follows: $$\sum_j w_jf(t_j)g(s_i,t_j)=h(s_i), i=1,...,n$$ by discretizing variables. Then $n$ equation solves $n$ values of $w_j$. Consequently, we can obtain $f(t_i)$ of dicreted version. What I am wondering is that this numerical solution converges to $f(x)$ as $n$ increases to $\infty$?? If so, what condition is needed? Please let me know releavanet paper or books Thanks in advances",,"['calculus', 'integration', 'ordinary-differential-equations', 'convergence-divergence', 'integral-equations']"
68,second degree differential equation,second degree differential equation,,Find all functions $f(x)$ such that $$f''(x)+f(x)=\frac{1}{1+x^2}.$$ I would like to know if it's solvable and the solution/hints. What I got : $$2f(x)f(x)'+2f(x)'f(x)''=2\frac{1}{1+x^2}f'(x)$$   $$(f(x)^2)'+((f(x)')^2)'=2\frac{1}{1+x^2}f'(x)$$   $$f(x)^2+(f(x)')^2=\int 2\frac{1}{1+x^2}f'(x)dx .$$,Find all functions $f(x)$ such that $$f''(x)+f(x)=\frac{1}{1+x^2}.$$ I would like to know if it's solvable and the solution/hints. What I got : $$2f(x)f(x)'+2f(x)'f(x)''=2\frac{1}{1+x^2}f'(x)$$   $$(f(x)^2)'+((f(x)')^2)'=2\frac{1}{1+x^2}f'(x)$$   $$f(x)^2+(f(x)')^2=\int 2\frac{1}{1+x^2}f'(x)dx .$$,,['ordinary-differential-equations']
69,Understanding the Euler operator,Understanding the Euler operator,,"While reading this book I came across a differential equation $$t^5\frac{d^2y}{dt^2}+2t^4\frac{dy}{dt}-y=0$$ that was then rewritten in terms of the Euler operator , $\delta=t\frac{d}{dt}$, with the resulting operator being $$\delta^2-\delta-\frac{1}{t^3}=0.$$ I'm trying to understand why does it seem as if  $$\delta^2 = t^2\frac{d^2}{dt^2} .$$ Shouldn't $$\delta^2=\delta(\delta)=t\frac{d}{dt}\left(t\frac{d}{dt}\right)= t\frac{d}{dt}+t^2\frac{d^2}{dt^2}.$$ Also, I'm looking for a possible generalization of this for $n^{th}$ order polynomials, that is, given a differential operator $$A(x)\frac{d^2}{dt^2}+B(x)\frac{d}{dt}+C(x)=0$$ where $A(x), B(x)$ and $C(x)$ are $n^{th}$ degree polynomials, how can it be expressed in terms of the Euler operator","While reading this book I came across a differential equation $$t^5\frac{d^2y}{dt^2}+2t^4\frac{dy}{dt}-y=0$$ that was then rewritten in terms of the Euler operator , $\delta=t\frac{d}{dt}$, with the resulting operator being $$\delta^2-\delta-\frac{1}{t^3}=0.$$ I'm trying to understand why does it seem as if  $$\delta^2 = t^2\frac{d^2}{dt^2} .$$ Shouldn't $$\delta^2=\delta(\delta)=t\frac{d}{dt}\left(t\frac{d}{dt}\right)= t\frac{d}{dt}+t^2\frac{d^2}{dt^2}.$$ Also, I'm looking for a possible generalization of this for $n^{th}$ order polynomials, that is, given a differential operator $$A(x)\frac{d^2}{dt^2}+B(x)\frac{d}{dt}+C(x)=0$$ where $A(x), B(x)$ and $C(x)$ are $n^{th}$ degree polynomials, how can it be expressed in terms of the Euler operator",,"['functional-analysis', 'ordinary-differential-equations']"
70,Variable substitution in second order PDE,Variable substitution in second order PDE,,"Consider the the PDE $$A(x, y)\partial_{xx} u + B(x, y)\partial_{xy}u + C(x, y)\partial_{yy}u=h(x, y) $$ Now I want to make a variable substitution $\xi=f(x, y), \eta=g(x,y)$, so I can get $u$ as a function of $\xi$ and $\eta$. So $$\partial_{x}u=\partial_{x}\xi\partial_{\xi}u + \partial_{x}\eta\partial_{\eta}u$$ Then $$\partial_{xx}u=\partial_{x}\left(\partial_{x}\xi\partial_{\xi}u + \partial_{x}\eta\partial_{\eta}u\right)\overset{?}{=}\partial_{xx}\xi\partial_{\xi}u + \partial_{x}\xi\partial_{x\xi}u + \partial_{xx}\eta\partial_{\eta}u + \partial_{x}\eta\partial_{x\eta}u$$ I am supposed to find terms like $\partial_{\xi\xi} u$, but how? The question-mark over the equality is that I am unsure if and how the chain rule should be applied.","Consider the the PDE $$A(x, y)\partial_{xx} u + B(x, y)\partial_{xy}u + C(x, y)\partial_{yy}u=h(x, y) $$ Now I want to make a variable substitution $\xi=f(x, y), \eta=g(x,y)$, so I can get $u$ as a function of $\xi$ and $\eta$. So $$\partial_{x}u=\partial_{x}\xi\partial_{\xi}u + \partial_{x}\eta\partial_{\eta}u$$ Then $$\partial_{xx}u=\partial_{x}\left(\partial_{x}\xi\partial_{\xi}u + \partial_{x}\eta\partial_{\eta}u\right)\overset{?}{=}\partial_{xx}\xi\partial_{\xi}u + \partial_{x}\xi\partial_{x\xi}u + \partial_{xx}\eta\partial_{\eta}u + \partial_{x}\eta\partial_{x\eta}u$$ I am supposed to find terms like $\partial_{\xi\xi} u$, but how? The question-mark over the equality is that I am unsure if and how the chain rule should be applied.",,"['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations']"
71,Solving a Riccati equation,Solving a Riccati equation,,"I need help with solving equation $2xy^2 \log(x) - y = x y'$. I used $z=y^{-1}$, but then I got very bad integral. Wolfram says what answer is $\displaystyle\frac {1} {x(c_{1}-\log^{2}(x))}$. What should I do? Thank you.","I need help with solving equation $2xy^2 \log(x) - y = x y'$. I used $z=y^{-1}$, but then I got very bad integral. Wolfram says what answer is $\displaystyle\frac {1} {x(c_{1}-\log^{2}(x))}$. What should I do? Thank you.",,['ordinary-differential-equations']
72,Questions about solutions of $x'=f(x^2)$,Questions about solutions of,x'=f(x^2),"Let $f:\mathbb{R} \rightarrow \mathbb{R}$, class $C^1$ and consider $x'=f(x^2)$ Can all solutions be strictly decreasing? Is every solution either constant or strictly monotonic? I don't know how to answer those questions. How to use the fact that $f$ is $C^1$? Could you help?","Let $f:\mathbb{R} \rightarrow \mathbb{R}$, class $C^1$ and consider $x'=f(x^2)$ Can all solutions be strictly decreasing? Is every solution either constant or strictly monotonic? I don't know how to answer those questions. How to use the fact that $f$ is $C^1$? Could you help?",,['ordinary-differential-equations']
73,Solve $y^{2/3}+(y')^{2/3}=1$ other than the direct method?,Solve  other than the direct method?,y^{2/3}+(y')^{2/3}=1,Is there any way to solve $$y^{2/3}+(y')^{2/3}=1$$ other than just solving for $y'$ and then integrate?,Is there any way to solve $$y^{2/3}+(y')^{2/3}=1$$ other than just solving for $y'$ and then integrate?,,['ordinary-differential-equations']
74,Locate my error for this initial value separable differential equation?,Locate my error for this initial value separable differential equation?,,"The problem is to solve $ sin\,2x\,dx + cos\,3y\,dy = 0, \;\;\;\;y({\pi\over 2}) = {\pi\over 3}$ Here are my steps: $$ cos \,3y \,dy = -sin \,2x \,dx $$ $$\int cos\,3y\,dy = \int -sin\,2x\,dx$$ $$ {1\over 3}\,sin\,3y = {1\over 2}\,cos\,2x\,+C $$ $$ 2\,sin\,3y = 3\,cos\,2x+C$$ Using the initial value then.. $$ 2\,sin({3\pi\over 3}) = 3\,cos({2\pi\over 2}) + C $$ $\qquad\qquad\qquad\qquad\qquad\qquad 0 = -3+C $ and it follows that $ C = 3 $$$$$Then,  $$ 2\,sin\,3y = 3\,cos\,2x+3 $$ $$ sin\,3y = {3\over 2}cos\,2x+{3\over 2} = 3({1\over 2}cos\,2x+{1\over 2})$$ Using the trig. identity, $$ sin\,3y = 3\,cos^2x$$ $$ arcsin(sin\,3y) = arcsin(3\,cos^2x) $$ $$ 3y = arcsin(3\,cos^2x)$$ $$ y = {arcsin(3\,cos^2x)\over 3} $$ However, the solution is stated as: $$ y = {\pi - arcsin(3\,cos^2x)\over 3} $$ Could someone point out what I missed?","The problem is to solve $ sin\,2x\,dx + cos\,3y\,dy = 0, \;\;\;\;y({\pi\over 2}) = {\pi\over 3}$ Here are my steps: $$ cos \,3y \,dy = -sin \,2x \,dx $$ $$\int cos\,3y\,dy = \int -sin\,2x\,dx$$ $$ {1\over 3}\,sin\,3y = {1\over 2}\,cos\,2x\,+C $$ $$ 2\,sin\,3y = 3\,cos\,2x+C$$ Using the initial value then.. $$ 2\,sin({3\pi\over 3}) = 3\,cos({2\pi\over 2}) + C $$ $\qquad\qquad\qquad\qquad\qquad\qquad 0 = -3+C $ and it follows that $ C = 3 $$$$$Then,  $$ 2\,sin\,3y = 3\,cos\,2x+3 $$ $$ sin\,3y = {3\over 2}cos\,2x+{3\over 2} = 3({1\over 2}cos\,2x+{1\over 2})$$ Using the trig. identity, $$ sin\,3y = 3\,cos^2x$$ $$ arcsin(sin\,3y) = arcsin(3\,cos^2x) $$ $$ 3y = arcsin(3\,cos^2x)$$ $$ y = {arcsin(3\,cos^2x)\over 3} $$ However, the solution is stated as: $$ y = {\pi - arcsin(3\,cos^2x)\over 3} $$ Could someone point out what I missed?",,"['ordinary-differential-equations', 'solution-verification']"
75,How did they find this equilibrium condition?,How did they find this equilibrium condition?,,"I'm studying from a book titled ""Mathematical Models in Population Biology and Epidemiology"" and we're dealing with SIS models. In a chapter called ""Infective Periods of Fixed Length"", we get to this differential-difference equation $$I'(t) = \beta I(t) [K - I(t)] - \beta I(t - \tau) [K - I(t - \tau)]$$ We find equilibria by incorporating initial data for $-\tau \leq t \leq 0 $ into the model by writing it in the integrated form, $$I(t) = \int_{t - \tau}^{t} \beta I(x) [K - I(x)] dx$$ The authors get that the equilibrium condition for $I(t)$ is $$I = 0 \text{ or } 1 = \beta \tau (K - I)$$ I met with my professor before he left, and he could not figure out how they came up with this. Perhaps, one you could help me understand how they found this equilibrium condition.","I'm studying from a book titled ""Mathematical Models in Population Biology and Epidemiology"" and we're dealing with SIS models. In a chapter called ""Infective Periods of Fixed Length"", we get to this differential-difference equation $$I'(t) = \beta I(t) [K - I(t)] - \beta I(t - \tau) [K - I(t - \tau)]$$ We find equilibria by incorporating initial data for $-\tau \leq t \leq 0 $ into the model by writing it in the integrated form, $$I(t) = \int_{t - \tau}^{t} \beta I(x) [K - I(x)] dx$$ The authors get that the equilibrium condition for $I(t)$ is $$I = 0 \text{ or } 1 = \beta \tau (K - I)$$ I met with my professor before he left, and he could not figure out how they came up with this. Perhaps, one you could help me understand how they found this equilibrium condition.",,"['ordinary-differential-equations', 'mathematical-modeling']"
76,Provide an example of a function whose inverse is also it's derivative.,Provide an example of a function whose inverse is also it's derivative.,,"This is a question from a mathematics competition. I'm totally stumped with this one. If anyone could give an example, or even better, show working, that would be great.","This is a question from a mathematics competition. I'm totally stumped with this one. If anyone could give an example, or even better, show working, that would be great.",,"['calculus', 'ordinary-differential-equations']"
77,Classify all critical points of Hamiltonian system,Classify all critical points of Hamiltonian system,,"Consider the following system describing pendulum $$\begin{align} & \frac{dx}{dt} = y, \\ & \frac{dy}{dt} = − \sin x. \end{align}$$ I need to classify all critical points of the system. All critical points are of the form $(k\pi,0)$ for any $k \in \mathbb{Z}$ I know the Hamiltonian of the system $\displaystyle H(x,y)= \frac{y^2}2 -\cos x $ , but I'm not sure if this can help in any way.","Consider the following system describing pendulum I need to classify all critical points of the system. All critical points are of the form for any I know the Hamiltonian of the system , but I'm not sure if this can help in any way.","\begin{align} & \frac{dx}{dt} = y, \\ & \frac{dy}{dt} = − \sin x. \end{align} (k\pi,0) k \in \mathbb{Z} \displaystyle H(x,y)= \frac{y^2}2 -\cos x ",['ordinary-differential-equations']
78,How can I calculate the integral??,How can I calculate the integral??,,"I have to solve the following problem: $$u_t=u_{xx}, x \in \mathbb{R}, t>0$$ $$u(x,0)=f(x)=H(x)=\left\{\begin{matrix} 1, x>0\\  0, x<0 \end{matrix}\right.$$ I have done the following: We use the method seperation of variables, $u(x,t)=X(x)T(t)$. I have found that the eigenfunctions are $X_k(x)=e^{ikx}, \lambda=k^2, k \in \mathbb{R}$ $T_k(t)=e^{-k^2t}$ $$u(x,t)=\frac{1}{2\pi} \int_{-\infty}^{+\infty}{\widetilde{f}(k) e^{ikx} e^{-k^2t}}dk$$ $$u(x,0)=H(x) \Rightarrow H(x)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\widetilde{f}(k)e^{ikx}}dk$$ $$\widetilde{f}(k)=\int_0^{\infty}{e^{-ikx}}dx=\frac{1}{ik}$$ $$u(x,t)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx} e^{-k^2t}}dk$$ Is this correct so far?? How can I calculate the last integral?? EDIT: I found that the solution of the problem has the following form: $$u(x,t)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx}e^{-k^2t}}dk$$ Then, I derivated this in respect to $x$: $$u_x=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx}e^{-k^2t}}dk$$ Then, from the formula: $$\int_{-\infty}^{+\infty}{e^{ikb}e^{-k^2a}}dk=\sqrt{\frac{\pi}{a}}e^{-\frac{b^2}{4a}}$$ we get the following: $$u_x=\frac{1}{2 \pi} \sqrt{\frac{\pi}{t}}e^{-\frac{x^2}{4t}}=\frac{1}{\sqrt{4 \pi t}} e^{-\frac{x^2}{4t}}$$ So to find $u(x,t)$, I have to calculate the integral $\displaystyle{\int u_x dx}$ but with what limits??","I have to solve the following problem: $$u_t=u_{xx}, x \in \mathbb{R}, t>0$$ $$u(x,0)=f(x)=H(x)=\left\{\begin{matrix} 1, x>0\\  0, x<0 \end{matrix}\right.$$ I have done the following: We use the method seperation of variables, $u(x,t)=X(x)T(t)$. I have found that the eigenfunctions are $X_k(x)=e^{ikx}, \lambda=k^2, k \in \mathbb{R}$ $T_k(t)=e^{-k^2t}$ $$u(x,t)=\frac{1}{2\pi} \int_{-\infty}^{+\infty}{\widetilde{f}(k) e^{ikx} e^{-k^2t}}dk$$ $$u(x,0)=H(x) \Rightarrow H(x)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\widetilde{f}(k)e^{ikx}}dk$$ $$\widetilde{f}(k)=\int_0^{\infty}{e^{-ikx}}dx=\frac{1}{ik}$$ $$u(x,t)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx} e^{-k^2t}}dk$$ Is this correct so far?? How can I calculate the last integral?? EDIT: I found that the solution of the problem has the following form: $$u(x,t)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx}e^{-k^2t}}dk$$ Then, I derivated this in respect to $x$: $$u_x=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx}e^{-k^2t}}dk$$ Then, from the formula: $$\int_{-\infty}^{+\infty}{e^{ikb}e^{-k^2a}}dk=\sqrt{\frac{\pi}{a}}e^{-\frac{b^2}{4a}}$$ we get the following: $$u_x=\frac{1}{2 \pi} \sqrt{\frac{\pi}{t}}e^{-\frac{x^2}{4t}}=\frac{1}{\sqrt{4 \pi t}} e^{-\frac{x^2}{4t}}$$ So to find $u(x,t)$, I have to calculate the integral $\displaystyle{\int u_x dx}$ but with what limits??",,"['ordinary-differential-equations', 'partial-differential-equations']"
79,Gronwall inequality for $\frac{d}{dt}u(t) \leq C_1u(t) + C_2\sqrt{u(t)}$,Gronwall inequality for,\frac{d}{dt}u(t) \leq C_1u(t) + C_2\sqrt{u(t)},I have the inequality $$\frac{d}{dt}u(t) \leq C_1u(t) + C_2\sqrt{u(t)}$$ for a positive function $u$. Is there a Gronwall inequality that I can use to write $$u(t) \leq C_3u(0)?$$ or something similar. I definitely need something where the right hand side is $u(0) \times \text{something}$.,I have the inequality $$\frac{d}{dt}u(t) \leq C_1u(t) + C_2\sqrt{u(t)}$$ for a positive function $u$. Is there a Gronwall inequality that I can use to write $$u(t) \leq C_3u(0)?$$ or something similar. I definitely need something where the right hand side is $u(0) \times \text{something}$.,,"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
80,Non-Homogenous System,Non-Homogenous System,,Find the general solution of $\vec{x^{'}}=\begin{pmatrix} 1&1\\4&1 \end{pmatrix}x+\begin{pmatrix}2\\-1\end{pmatrix}e^{t}$ I found the eigenvalues to be $\lambda_1=3 \;\; \lambda_2=-1$ Next I calculated the eigenvectors to be $e_1=\begin{pmatrix}1\\2 \end{pmatrix}$ and $e_2=\begin{pmatrix}1\\-2 \end{pmatrix}$ Next I form the fundamental matrix $$\phi(t)= \begin{pmatrix} e^{3t}&e^{-t}\\2e^{3t}&-2e^{-t} \end{pmatrix}$$ Finding the inverse yields $\phi^{-1}(t)=\dfrac{1}{4} \begin{pmatrix}2e^{-t}&e^{-t}\\2e^{3t}&-e^{3t} \end{pmatrix}$ I proceeded to multiply the matrix by $g(t)$ to obtain $$\begin{pmatrix} 4-1\\4e^{4t}+e^{4t} \end{pmatrix} \implies \begin{pmatrix} 3\\5e^{4t} \end{pmatrix}$$ I integrated that and multiplied by $\phi(t)$ to get $\dfrac{1}{4} \begin{pmatrix} 3te^{3t}+\frac{5}{4}e^{3t}\\6te^{3t}-\frac{5}{2}e^{3t} \end{pmatrix}$ I left the $\dfrac{1}{4}$ out from the inverse to avoid working with fractions. Now the book gets $x_h+\dfrac{1}{4}\begin{pmatrix}1\\-8 \end{pmatrix}e^{t}$ where $x_h$ is the solution to the homogenous. Is the book wrong or am I wrong somewhere?,Find the general solution of $\vec{x^{'}}=\begin{pmatrix} 1&1\\4&1 \end{pmatrix}x+\begin{pmatrix}2\\-1\end{pmatrix}e^{t}$ I found the eigenvalues to be $\lambda_1=3 \;\; \lambda_2=-1$ Next I calculated the eigenvectors to be $e_1=\begin{pmatrix}1\\2 \end{pmatrix}$ and $e_2=\begin{pmatrix}1\\-2 \end{pmatrix}$ Next I form the fundamental matrix $$\phi(t)= \begin{pmatrix} e^{3t}&e^{-t}\\2e^{3t}&-2e^{-t} \end{pmatrix}$$ Finding the inverse yields $\phi^{-1}(t)=\dfrac{1}{4} \begin{pmatrix}2e^{-t}&e^{-t}\\2e^{3t}&-e^{3t} \end{pmatrix}$ I proceeded to multiply the matrix by $g(t)$ to obtain $$\begin{pmatrix} 4-1\\4e^{4t}+e^{4t} \end{pmatrix} \implies \begin{pmatrix} 3\\5e^{4t} \end{pmatrix}$$ I integrated that and multiplied by $\phi(t)$ to get $\dfrac{1}{4} \begin{pmatrix} 3te^{3t}+\frac{5}{4}e^{3t}\\6te^{3t}-\frac{5}{2}e^{3t} \end{pmatrix}$ I left the $\dfrac{1}{4}$ out from the inverse to avoid working with fractions. Now the book gets $x_h+\dfrac{1}{4}\begin{pmatrix}1\\-8 \end{pmatrix}e^{t}$ where $x_h$ is the solution to the homogenous. Is the book wrong or am I wrong somewhere?,,['ordinary-differential-equations']
81,Finding the Green's function for $y'' + y' = f(x)$,Finding the Green's function for,y'' + y' = f(x),"I have this ODE: $$y'' + y' = f(x)$$ with $y(0)=0$ and $y'(1) = 0$. I'm trying to find the Green's function. I multiply through by $G$, integrate over the domain and then use integration by parts to find the adjoint and I'm left with: $$ \int\limits_0^1 y(G'' - G')dx + G(1)y(1) = \int\limits_0^1 Gfdx$$. (This is after choosing the boundary conditions for G: $G(0) = 0$ and $G'(1) = 0$ so that most of the boundary terms from the integration by parts are zero. And then I would choose $G'' - G' = \delta (x-\xi)$.) My question is, how do I deal with the $G(1)y(1)$ term? I haven't encountered a problem before where all the boundary terms aren't nicely zero. I've also tried this problem using the variation of parameters method and I see that we get an integral condition, I'm guessing this has something to do with it? Thanks in advance","I have this ODE: $$y'' + y' = f(x)$$ with $y(0)=0$ and $y'(1) = 0$. I'm trying to find the Green's function. I multiply through by $G$, integrate over the domain and then use integration by parts to find the adjoint and I'm left with: $$ \int\limits_0^1 y(G'' - G')dx + G(1)y(1) = \int\limits_0^1 Gfdx$$. (This is after choosing the boundary conditions for G: $G(0) = 0$ and $G'(1) = 0$ so that most of the boundary terms from the integration by parts are zero. And then I would choose $G'' - G' = \delta (x-\xi)$.) My question is, how do I deal with the $G(1)y(1)$ term? I haven't encountered a problem before where all the boundary terms aren't nicely zero. I've also tried this problem using the variation of parameters method and I see that we get an integral condition, I'm guessing this has something to do with it? Thanks in advance",,['ordinary-differential-equations']
82,General solution to a second order nonhomogeneous differential equation.,General solution to a second order nonhomogeneous differential equation.,,"Find the general solution to $y'' - 2y' + 10 = e^x$. I think that first I need to find the solution to $y'' - 2y' + 10 = 0$, but if I assume $y=e^{rx}$ then I'm left with the equation $r^2 -2r + 10 = 0$ which has no real solutions. After that I have that the particular solution is $y_p = \frac{1}{9}e^x$ so unless that's wrong all I'm after is tips on how to solve the homogeneous equation.","Find the general solution to $y'' - 2y' + 10 = e^x$. I think that first I need to find the solution to $y'' - 2y' + 10 = 0$, but if I assume $y=e^{rx}$ then I'm left with the equation $r^2 -2r + 10 = 0$ which has no real solutions. After that I have that the particular solution is $y_p = \frac{1}{9}e^x$ so unless that's wrong all I'm after is tips on how to solve the homogeneous equation.",,['ordinary-differential-equations']
83,Homogeneous second-order differential equation with constant Wronskian,Homogeneous second-order differential equation with constant Wronskian,,"Problem Prove that if the Wronskian of any two solutions of differential equation $y''+p(x)y'+q(x)y=0$ is constant, then $p(x)$ is zero. My attempt. : Let $y_1$ and $y_2$ be two solutions of given differential equation. Note that the Wronskian $W=W[y_1,y_2]$ satisfies $W'+p(x)W=0$ . Since $W$ is constant, we get $p(x)W=0$ . Question. How do I show that $W$ cannot be equal to zero? If there are two solutions $y_1$ , $y_2$ that are linearly independent, then $W[y_1,y_2]\neq 0$ . But I am not certain of the existence of such solutions. My question is : If a second-order ODE has a solution, do there exist two solutions that are linearly independent?","Problem Prove that if the Wronskian of any two solutions of differential equation is constant, then is zero. My attempt. : Let and be two solutions of given differential equation. Note that the Wronskian satisfies . Since is constant, we get . Question. How do I show that cannot be equal to zero? If there are two solutions , that are linearly independent, then . But I am not certain of the existence of such solutions. My question is : If a second-order ODE has a solution, do there exist two solutions that are linearly independent?","y''+p(x)y'+q(x)y=0 p(x) y_1 y_2 W=W[y_1,y_2] W'+p(x)W=0 W p(x)W=0 W y_1 y_2 W[y_1,y_2]\neq 0","['ordinary-differential-equations', 'wronskian']"
84,Forcing function,Forcing function,,"If the forcing function on the right-hand side of a linear $n^{th}$ order differential equation is nonconstant and periodic, can the solution of the equation be a nonperiodic function?","If the forcing function on the right-hand side of a linear $n^{th}$ order differential equation is nonconstant and periodic, can the solution of the equation be a nonperiodic function?",,[]
85,Recovering a frame field from its connection forms,Recovering a frame field from its connection forms,,"I have a faced a research problem where I would need to recover a frame field given its connection forms. More precisely, I begin with an orthonormal frame field (given by data) in $\Re^3$ written as $$ \mathbf F=\begin{pmatrix}\vec f_1\\\vec f_2\\\vec f_3\end{pmatrix} $$ where $\vec f_i:\Re^3\rightarrow\Re^3$ are vectors fields with $\vec f_i\cdot\vec f_j=\delta_{ij}$, and $\delta_{ij}$ is the Kronecker delta. I then obtain the connection forms $$ \omega_{ij}=\text{d}\vec f_i\cdot\vec f_j, $$ which yields $$ \text{d}\mathbf F=\Omega\mathbf F $$ where $\Omega=[\omega_{ij}]\in\Re^{3\times3}$ is the skew-symmetric matrix of connection forms. In my application, I then proceed by computing the interior product of the 1-forms $\omega_{ij}$ onto the frame fields themselves (the Christoffel symbols), i.e., I compute $$ \omega_{ijk}\equiv\omega_{ij}\langle\vec f_k\rangle\in\Re $$ where $\langle\cdot\rangle$ denotes the standard interior product for forms. I thus obtain 9 different measurements at each point, i.e., $\omega_{121},\omega_{122},\omega_{123},\omega_{131},\omega_{132},\omega_{133},\omega_{231},\omega_{232},\omega_{232}$, each of which has a very precise meaning in the application at hand. Now, I would like to do the converse, i.e., I would like to solve for $\mathbf F$ given the list of interior contractions $\omega_{121},\omega_{122},\omega_{123},\omega_{131},\omega_{132},\omega_{133},\omega_{231},\omega_{232},\omega_{232}$. The Frobenius theorem states the unique existence of $\mathbf F$ in the neighborhood of $0$ if we set $\mathbf F(0)=I$ and if the following are satisfied: $$ \Omega=\text{d}(F)F^{-1} $$ $$ \text{d}\Omega-\Omega^2=0. $$ but I'm unsure about where to start. Would anybody have a suggestion on how to approach this problem? Is there a formal name to what I'm trying to do? Perhaps a better way to formulate the problem is to enumerate the frame axis differentials directly: \begin{align} \text{d}\vec f_1&=\omega_{12}\vec f_2+\omega_{13}\vec f_3\\ \text{d}\vec f_2&=-\omega_{12}\vec f_1+\omega_{23}\vec f_3\\ \text{d}\vec f_3&=-\omega_{13}\vec f_1-\omega_{23}\vec f_2. \end{align} Writing the direction of contraction as  $$\vec v=\sum_i^3(\vec v\cdot\vec f_i)\vec f_i=\sum_i^3v_i\vec f_i$$ we obtain \begin{aligned} \text{d}\vec f_1\langle\vec v\rangle&=\left(\sum_i^3v_i\omega_{12i}\right)\vec f_2+\left(\sum_i^3v_i\omega_{13i}\right)\vec f_3\\ \text{d}\vec f_2\langle\vec v\rangle&=-\left(\sum_i^3v_i\omega_{12i}\right)\vec f_1+\left(\sum_i^3v_i\omega_{13i}\right)\vec f_3\\ \text{d}\vec f_3\langle\vec v\rangle&=-\left(\sum_i^3v_i\omega_{13i}\right)\vec f_1-\left(\sum_i^3v_i\omega_{23i}\right)\vec f_2. \end{aligned} Which gives us three coupled differential equations to solve.","I have a faced a research problem where I would need to recover a frame field given its connection forms. More precisely, I begin with an orthonormal frame field (given by data) in $\Re^3$ written as $$ \mathbf F=\begin{pmatrix}\vec f_1\\\vec f_2\\\vec f_3\end{pmatrix} $$ where $\vec f_i:\Re^3\rightarrow\Re^3$ are vectors fields with $\vec f_i\cdot\vec f_j=\delta_{ij}$, and $\delta_{ij}$ is the Kronecker delta. I then obtain the connection forms $$ \omega_{ij}=\text{d}\vec f_i\cdot\vec f_j, $$ which yields $$ \text{d}\mathbf F=\Omega\mathbf F $$ where $\Omega=[\omega_{ij}]\in\Re^{3\times3}$ is the skew-symmetric matrix of connection forms. In my application, I then proceed by computing the interior product of the 1-forms $\omega_{ij}$ onto the frame fields themselves (the Christoffel symbols), i.e., I compute $$ \omega_{ijk}\equiv\omega_{ij}\langle\vec f_k\rangle\in\Re $$ where $\langle\cdot\rangle$ denotes the standard interior product for forms. I thus obtain 9 different measurements at each point, i.e., $\omega_{121},\omega_{122},\omega_{123},\omega_{131},\omega_{132},\omega_{133},\omega_{231},\omega_{232},\omega_{232}$, each of which has a very precise meaning in the application at hand. Now, I would like to do the converse, i.e., I would like to solve for $\mathbf F$ given the list of interior contractions $\omega_{121},\omega_{122},\omega_{123},\omega_{131},\omega_{132},\omega_{133},\omega_{231},\omega_{232},\omega_{232}$. The Frobenius theorem states the unique existence of $\mathbf F$ in the neighborhood of $0$ if we set $\mathbf F(0)=I$ and if the following are satisfied: $$ \Omega=\text{d}(F)F^{-1} $$ $$ \text{d}\Omega-\Omega^2=0. $$ but I'm unsure about where to start. Would anybody have a suggestion on how to approach this problem? Is there a formal name to what I'm trying to do? Perhaps a better way to formulate the problem is to enumerate the frame axis differentials directly: \begin{align} \text{d}\vec f_1&=\omega_{12}\vec f_2+\omega_{13}\vec f_3\\ \text{d}\vec f_2&=-\omega_{12}\vec f_1+\omega_{23}\vec f_3\\ \text{d}\vec f_3&=-\omega_{13}\vec f_1-\omega_{23}\vec f_2. \end{align} Writing the direction of contraction as  $$\vec v=\sum_i^3(\vec v\cdot\vec f_i)\vec f_i=\sum_i^3v_i\vec f_i$$ we obtain \begin{aligned} \text{d}\vec f_1\langle\vec v\rangle&=\left(\sum_i^3v_i\omega_{12i}\right)\vec f_2+\left(\sum_i^3v_i\omega_{13i}\right)\vec f_3\\ \text{d}\vec f_2\langle\vec v\rangle&=-\left(\sum_i^3v_i\omega_{12i}\right)\vec f_1+\left(\sum_i^3v_i\omega_{13i}\right)\vec f_3\\ \text{d}\vec f_3\langle\vec v\rangle&=-\left(\sum_i^3v_i\omega_{13i}\right)\vec f_1-\left(\sum_i^3v_i\omega_{23i}\right)\vec f_2. \end{aligned} Which gives us three coupled differential equations to solve.",,"['ordinary-differential-equations', 'differential-geometry', 'differential-forms', 'differential', 'connections']"
86,Second order differential equation with non constant coefficients,Second order differential equation with non constant coefficients,,"I want to solve the following differential equation: $$ 2f'(x)(2x+1)+\frac{\kappa}{2}f""(x)x(x+1)=f(x)(\frac{-2b}{x+1}+\frac{2c}{x}+2a) $$ where $\kappa,a,b,c$ are arbitrary positive constants. Is there anyway to do this? Thanks.","I want to solve the following differential equation: $$ 2f'(x)(2x+1)+\frac{\kappa}{2}f""(x)x(x+1)=f(x)(\frac{-2b}{x+1}+\frac{2c}{x}+2a) $$ where $\kappa,a,b,c$ are arbitrary positive constants. Is there anyway to do this? Thanks.",,['ordinary-differential-equations']
87,Functional form of a solution to a Differential Equation (Euler-Lagrange),Functional form of a solution to a Differential Equation (Euler-Lagrange),,"Let $f=f(q(t),\dot q(t),t)$, where $q(t)=\{q_1(t),...,q_N(t) \}=\{q_{a}\}_{a=1}^N$ and $\dot q:=\frac{dq}{dt}$. I want to show that if the following equations (Euler-Lagrange) are satisfied identically (independent of $q_a(t)$) $$\frac{d}{dt}\frac{\partial f}{\partial \dot q_{a}}-\frac{\partial f}{\partial q_{a}}=0 \qquad(1)$$ then $f$ must have the following form $$f(q(t),\dot q(t),t)=\frac{d \Lambda (q(t),t)}{dt} \qquad(2)$$ I read a book where they rewrite (1) as $$\sum_{b=1}^N \left[ \frac{\partial^2f}{\partial \dot q_b \partial \dot q_a} \ddot q_b+\frac{\partial^2f}{\partial q_b \partial \dot q_a}\dot q_b \right]+\frac{\partial^2f}{\partial t \partial \dot q_a}=\frac{\partial f}{\partial q_a}\qquad(3)$$ Then the book says that, since we want (1) to be satisfied independent of $q_a$ then the coefficients of $\ddot q_a$ in (3) must be zero. So they find that $f$ must be a linear function of the $q_a$'s: $$f(q,\dot q,t)=\sum_{b=1}^N A_b(q,t)\dot q_b+B(q,t)\qquad(4)$$ My question is: are they forgetting that the coefficients of the $\dot q_a$ must be zero as well? The book inserts (4) into (1) and obtain a set of equations of the form $$\sum_{b=1}^NC_{ab}\dot q_b+\frac{\partial A_a}{\partial t}=\frac{\partial B}{\partial q_a}\qquad(5)$$ Then they say again that since we want (1) to be satisfied independent of $q_a$ then the coefficients of $\dot q_a$ in (5) must be zero. And they find the functional form of $f$. So my question can be restate as: Why do they have to do this step-by-step argument instead of making zero the coefficients of $\ddot q_a$ and $\dot q_a$ in (3) from the beginning?","Let $f=f(q(t),\dot q(t),t)$, where $q(t)=\{q_1(t),...,q_N(t) \}=\{q_{a}\}_{a=1}^N$ and $\dot q:=\frac{dq}{dt}$. I want to show that if the following equations (Euler-Lagrange) are satisfied identically (independent of $q_a(t)$) $$\frac{d}{dt}\frac{\partial f}{\partial \dot q_{a}}-\frac{\partial f}{\partial q_{a}}=0 \qquad(1)$$ then $f$ must have the following form $$f(q(t),\dot q(t),t)=\frac{d \Lambda (q(t),t)}{dt} \qquad(2)$$ I read a book where they rewrite (1) as $$\sum_{b=1}^N \left[ \frac{\partial^2f}{\partial \dot q_b \partial \dot q_a} \ddot q_b+\frac{\partial^2f}{\partial q_b \partial \dot q_a}\dot q_b \right]+\frac{\partial^2f}{\partial t \partial \dot q_a}=\frac{\partial f}{\partial q_a}\qquad(3)$$ Then the book says that, since we want (1) to be satisfied independent of $q_a$ then the coefficients of $\ddot q_a$ in (3) must be zero. So they find that $f$ must be a linear function of the $q_a$'s: $$f(q,\dot q,t)=\sum_{b=1}^N A_b(q,t)\dot q_b+B(q,t)\qquad(4)$$ My question is: are they forgetting that the coefficients of the $\dot q_a$ must be zero as well? The book inserts (4) into (1) and obtain a set of equations of the form $$\sum_{b=1}^NC_{ab}\dot q_b+\frac{\partial A_a}{\partial t}=\frac{\partial B}{\partial q_a}\qquad(5)$$ Then they say again that since we want (1) to be satisfied independent of $q_a$ then the coefficients of $\dot q_a$ in (5) must be zero. And they find the functional form of $f$. So my question can be restate as: Why do they have to do this step-by-step argument instead of making zero the coefficients of $\ddot q_a$ and $\dot q_a$ in (3) from the beginning?",,"['ordinary-differential-equations', 'proof-writing', 'proof-verification']"
88,Finding the derivatives of inverse functions at given point of c,Finding the derivatives of inverse functions at given point of c,,"Hoping someone can help me the understand the steps to solve a problem like this. I'm guessing it involves the formula: $\frac{d}{dx}f^{-1}(f(x))=1/f'(x)$. Am I right in this assumption? I would post some work that I've tried, but I'm not sure where to even start. My professor is bad at explaining things so I'm turning to the site for supplemental instruction. ""For each of the given functions $f(x)$, find the derivative  $\left. \frac{d}{dx}f^{-1}(x) \right|_{x=c}$ at the given point $c$, first finding $a=f^{-1}(c)$. $$ f(x)= 3 x + 9 x^{13}; \ \ c = -12, \ \ \ a=? \ \ \ (f^{-1})'(c)=? $$ $f(x)= x^2 - 13 x + 58$ on the interval $[6.5,\infty);$  $c = 18$, $a=?$ $(f^{-1})'(c)  = ?$"" I've been able to find the inverse of other functions not involving higher powers like the $9x^{13}$, and then finding the derivative of that inverse, but this particular kind of question stumps me.","Hoping someone can help me the understand the steps to solve a problem like this. I'm guessing it involves the formula: $\frac{d}{dx}f^{-1}(f(x))=1/f'(x)$. Am I right in this assumption? I would post some work that I've tried, but I'm not sure where to even start. My professor is bad at explaining things so I'm turning to the site for supplemental instruction. ""For each of the given functions $f(x)$, find the derivative  $\left. \frac{d}{dx}f^{-1}(x) \right|_{x=c}$ at the given point $c$, first finding $a=f^{-1}(c)$. $$ f(x)= 3 x + 9 x^{13}; \ \ c = -12, \ \ \ a=? \ \ \ (f^{-1})'(c)=? $$ $f(x)= x^2 - 13 x + 58$ on the interval $[6.5,\infty);$  $c = 18$, $a=?$ $(f^{-1})'(c)  = ?$"" I've been able to find the inverse of other functions not involving higher powers like the $9x^{13}$, and then finding the derivative of that inverse, but this particular kind of question stumps me.",,"['calculus', 'ordinary-differential-equations', 'derivatives', 'inverse']"
89,Solving differential equation with initial condition,Solving differential equation with initial condition,,"So I've got: $$y''=3y'$$ where $y(3) = 1$ and $y'(3) = 2$. I tried saying: $$y''-3y' = 0 \\ \Rightarrow r^2-3r = 0$$ so $r = 3$ or $0$. The solution is therefore given as: $$y=Ae^{3t} + Be^0 = Ae^{3t} +B$$ According to my book, by auxiliary equations. But I'm not sure if I'm doing this right? If so, how do I finish?","So I've got: $$y''=3y'$$ where $y(3) = 1$ and $y'(3) = 2$. I tried saying: $$y''-3y' = 0 \\ \Rightarrow r^2-3r = 0$$ so $r = 3$ or $0$. The solution is therefore given as: $$y=Ae^{3t} + Be^0 = Ae^{3t} +B$$ According to my book, by auxiliary equations. But I'm not sure if I'm doing this right? If so, how do I finish?",,"['calculus', 'ordinary-differential-equations']"
90,Having trouble integrating for use of energy method to prove uniqueness,Having trouble integrating for use of energy method to prove uniqueness,,"We are given $u_{tt} - c^2u_{xx} + ru_t$.  To prove only one solution exists, I am taking w = $u_1 - u_2$, assuming they are both solutions to the given wave equation. So: $u_{tt} - c^2u_{xx} + ru_t$ which becomes: $w_{tt} - c^2w_{xx} + rw_t$ I know we are suppose to then multiply both sides by $w_t$ and integrate to show that E(t) is decreasing, but I am confused when it comes to the integration.  Please do NOT solve the entire question, I'm simply seeking help for the integration.  Once I understand that part, I will post the rest of the problem and look for opinions on how I reached the final answer. Thanks!","We are given $u_{tt} - c^2u_{xx} + ru_t$.  To prove only one solution exists, I am taking w = $u_1 - u_2$, assuming they are both solutions to the given wave equation. So: $u_{tt} - c^2u_{xx} + ru_t$ which becomes: $w_{tt} - c^2w_{xx} + rw_t$ I know we are suppose to then multiply both sides by $w_t$ and integrate to show that E(t) is decreasing, but I am confused when it comes to the integration.  Please do NOT solve the entire question, I'm simply seeking help for the integration.  Once I understand that part, I will post the rest of the problem and look for opinions on how I reached the final answer. Thanks!",,"['ordinary-differential-equations', 'partial-differential-equations']"
91,Reference Request - Series Solutions to Differential Equations,Reference Request - Series Solutions to Differential Equations,,"I am looking for a text that gives a good exposition of power series solutions to second order equations with variable coefficients . My course I'm guessing focuses mainly on this section. My knowledge of power series and Taylor series is not great. But I'm in the process of rectifying this. In the meantime I need a text that puts emphasis on this section. Most of what I've come across focuses on systems of equations. Any help would be appreciated. This here is my entire syllabus. Differential Equations II (30L, 2C) Syllabus:Ordinary differential equations: Linear equations of the second order where the coefficients are functions of the independent variable; Ordinary points; Singular points; Regular singular points;Solution in series: Stability of the solutions; Solution of Laplace’s equation; Revision of Euler’s homogeneous form of the second order ordinary differential equations; Legendre’s equation; Legendre’s polynomials – their linear independence and recurrence relations; Bessel’s function.Introduction to Difference equations: Complementary functions and particular solutions. Assessment: End of semester examination.","I am looking for a text that gives a good exposition of power series solutions to second order equations with variable coefficients . My course I'm guessing focuses mainly on this section. My knowledge of power series and Taylor series is not great. But I'm in the process of rectifying this. In the meantime I need a text that puts emphasis on this section. Most of what I've come across focuses on systems of equations. Any help would be appreciated. This here is my entire syllabus. Differential Equations II (30L, 2C) Syllabus:Ordinary differential equations: Linear equations of the second order where the coefficients are functions of the independent variable; Ordinary points; Singular points; Regular singular points;Solution in series: Stability of the solutions; Solution of Laplace’s equation; Revision of Euler’s homogeneous form of the second order ordinary differential equations; Legendre’s equation; Legendre’s polynomials – their linear independence and recurrence relations; Bessel’s function.Introduction to Difference equations: Complementary functions and particular solutions. Assessment: End of semester examination.",,"['ordinary-differential-equations', 'reference-request']"
92,Solution to a differential equation going to infinity in finite time,Solution to a differential equation going to infinity in finite time,,"a) Consider the initial value problem $\dot{x} = f(x) = x^{3/2}$ and $x(0) = 2$ . Find the solution and show that the maximum interval of definition is of the form $(-\infty, A)$ , with $A \in \mathbb{R}$ a constant. b) Let $g(x)$ be a $C^1$ -function $\mathbb{R} \to \mathbb{R}$ such that $g(x) > f(x)$ for all $x>0$ . Show that the solution of $\dot{x}=g(x)$ and $x(0)=2$ goes in finite time to infinity. So I did solve part a) and found the solution $x(t) = \frac{4}{(\sqrt{2}-t)^2}$ and the maximum interval of definition $(\infty, \sqrt{2})$ . Now I get the intuitive idea why the solution of the second initial value problem has to go to infinity in finite time, because the solution has to be larger than $x(t)$ and $x(t)$ goes to infinity when $t$ approaches $\sqrt{2}$ . But how can I prove that if $y(t)$ is a solution of the second initial value problem, that $y(t) \geq x(t)$ , so $y(t)$ has to go to infinity in finite time?","a) Consider the initial value problem and . Find the solution and show that the maximum interval of definition is of the form , with a constant. b) Let be a -function such that for all . Show that the solution of and goes in finite time to infinity. So I did solve part a) and found the solution and the maximum interval of definition . Now I get the intuitive idea why the solution of the second initial value problem has to go to infinity in finite time, because the solution has to be larger than and goes to infinity when approaches . But how can I prove that if is a solution of the second initial value problem, that , so has to go to infinity in finite time?","\dot{x} = f(x) = x^{3/2} x(0) = 2 (-\infty, A) A \in \mathbb{R} g(x) C^1 \mathbb{R} \to \mathbb{R} g(x) > f(x) x>0 \dot{x}=g(x) x(0)=2 x(t) = \frac{4}{(\sqrt{2}-t)^2} (\infty, \sqrt{2}) x(t) x(t) t \sqrt{2} y(t) y(t) \geq x(t) y(t)",['ordinary-differential-equations']
93,Calculus of Variations,Calculus of Variations,,"In the Calculus of Variations there is a passage from Euler's characteristic equation: $$      \frac {\partial F}{\partial y} - \frac {d}{dx} \left(\frac {\partial F}{\partial y'} \right)=0     $$ in which $$ F=F(x,y,y') $$ If the function F is made explicitly independent of x then: $$ F=F(y,y') $$ Then by integration of Euler's characteristic equation one finds: $$ F - y'  \frac{\partial F}{\partial y'}=k $$ I'm having great difficulty trying to do that integration, hence I'd appreciate any help!!!","In the Calculus of Variations there is a passage from Euler's characteristic equation: $$      \frac {\partial F}{\partial y} - \frac {d}{dx} \left(\frac {\partial F}{\partial y'} \right)=0     $$ in which $$ F=F(x,y,y') $$ If the function F is made explicitly independent of x then: $$ F=F(y,y') $$ Then by integration of Euler's characteristic equation one finds: $$ F - y'  \frac{\partial F}{\partial y'}=k $$ I'm having great difficulty trying to do that integration, hence I'd appreciate any help!!!",,"['calculus', 'integration', 'ordinary-differential-equations', 'calculus-of-variations']"
94,"Are the following phase portraits drawn (well, at least sketched) correctly, in terms of their behaviour?","Are the following phase portraits drawn (well, at least sketched) correctly, in terms of their behaviour?",,"I have a linear system:  $\frac{dx}{dt}=ax+y; \frac{dy}{dt}=-x+ay$. I'm trying to investigate pictorially what happens for $a$ negative, $0$  and positive. Now, I know that the eigenvalues I eventually get are: $\lambda=a\pm i$  for $a \neq0$; $\lambda=\pm i$  for $a=0$. Here are my sketches. My question is: are the following sketches accurate (in terms of the nature of the respective critical points) and is the behaviour of the system indicated correctly?","I have a linear system:  $\frac{dx}{dt}=ax+y; \frac{dy}{dt}=-x+ay$. I'm trying to investigate pictorially what happens for $a$ negative, $0$  and positive. Now, I know that the eigenvalues I eventually get are: $\lambda=a\pm i$  for $a \neq0$; $\lambda=\pm i$  for $a=0$. Here are my sketches. My question is: are the following sketches accurate (in terms of the nature of the respective critical points) and is the behaviour of the system indicated correctly?",,"['ordinary-differential-equations', 'systems-of-equations']"
95,Finding the general solution to a system of differential equations,Finding the general solution to a system of differential equations,,"How can I solve the following system of differential equations? I am getting confused with the constants of integration... $$\dot{x}=2x-(2+y)e^{y}$$  $$\dot{y}=-y$$ I know that $y=Ce^{-t}$ and the integrating factor method for solving a linear differential equation, but it gets complicated quickly.","How can I solve the following system of differential equations? I am getting confused with the constants of integration... $$\dot{x}=2x-(2+y)e^{y}$$  $$\dot{y}=-y$$ I know that $y=Ce^{-t}$ and the integrating factor method for solving a linear differential equation, but it gets complicated quickly.",,['ordinary-differential-equations']
96,"For $x' = x^2$, and $y' = x + y$, Find all equilibrium points and decide whether they are stable, asymptotically stable, or unstable.","For , and , Find all equilibrium points and decide whether they are stable, asymptotically stable, or unstable.",x' = x^2 y' = x + y,"For $x' = x^2$, and $y' = x + y$, Find all equilibrium points and decide whether they are stable, asymptotically stable, or unstable. I found that the equilibrium points are (0,0). After linearized the system, it become $x' = 0$ and $y' = x+y$ So, one eigenvalue is 0 and the other is 1 right?..Therefore, this is unstable.","For $x' = x^2$, and $y' = x + y$, Find all equilibrium points and decide whether they are stable, asymptotically stable, or unstable. I found that the equilibrium points are (0,0). After linearized the system, it become $x' = 0$ and $y' = x+y$ So, one eigenvalue is 0 and the other is 1 right?..Therefore, this is unstable.",,"['ordinary-differential-equations', 'systems-of-equations']"
97,Arc length paramatrizations satisfy original system of differential equations?,Arc length paramatrizations satisfy original system of differential equations?,,"Say we have a system of differential equations $$ \begin{cases} x'''(t)+f(t)x'(t)=0\\ y'''(t)+f(t)y'(t)=0 \end{cases} $$ on an interval $[a,b]$, along with the restriction that $$ x'(t)^2+y'(t)^2=1 $$ on $[a,b]$. Now, Euler proved that as long as $x(t),y(t)$ are continuous and not both zero at a point, then we can re-parametrize $x,y$ to some functions $\tilde x,\tilde y$ which have the same image as $x,y$ but are arc-length parametrized. Now, say we have a solution $\big(x^*(t),y^*(t)\big)$ to $$ \begin{cases} x'''(t)+f(t)x'(t)=0\\ y'''(t)+f(t)y'(t)=0 \end{cases} $$ that satisfies the hypotheses needed to apply the previously mentioned re-parametrization.  Say $\big(\tilde x^*(t), \tilde y^*(t)\big)$ are this re-parametrization, so they have the same image of $\big(x(t),y(t)\big)$ on $[a,b]$ and satisfy $$ x'(t)^2+y'(t)^2=1. $$ Is it still the case that $$ \begin{cases} (\tilde x^*)'''(t)+f(t)(\tilde x^*)'(t)=0\\ (\tilde y^*)'''(t)+f(t)(\tilde y^*)'(t)=0? \end{cases} $$ Note: I'm not necessarily looking for a proof of this, just some intuition.","Say we have a system of differential equations $$ \begin{cases} x'''(t)+f(t)x'(t)=0\\ y'''(t)+f(t)y'(t)=0 \end{cases} $$ on an interval $[a,b]$, along with the restriction that $$ x'(t)^2+y'(t)^2=1 $$ on $[a,b]$. Now, Euler proved that as long as $x(t),y(t)$ are continuous and not both zero at a point, then we can re-parametrize $x,y$ to some functions $\tilde x,\tilde y$ which have the same image as $x,y$ but are arc-length parametrized. Now, say we have a solution $\big(x^*(t),y^*(t)\big)$ to $$ \begin{cases} x'''(t)+f(t)x'(t)=0\\ y'''(t)+f(t)y'(t)=0 \end{cases} $$ that satisfies the hypotheses needed to apply the previously mentioned re-parametrization.  Say $\big(\tilde x^*(t), \tilde y^*(t)\big)$ are this re-parametrization, so they have the same image of $\big(x(t),y(t)\big)$ on $[a,b]$ and satisfy $$ x'(t)^2+y'(t)^2=1. $$ Is it still the case that $$ \begin{cases} (\tilde x^*)'''(t)+f(t)(\tilde x^*)'(t)=0\\ (\tilde y^*)'''(t)+f(t)(\tilde y^*)'(t)=0? \end{cases} $$ Note: I'm not necessarily looking for a proof of this, just some intuition.",,"['calculus', 'ordinary-differential-equations', 'parametric', 'systems-of-equations']"
98,Solving a partial differential equation using method of characteristics,Solving a partial differential equation using method of characteristics,,"I keep getting stuck and have a hard time understanding my professor, so I'm hoping to get some help here.  The question is: Solve the partial diff/eq: ${\partial u}\over{\partial t}$ + $c {{\partial u}\over{\partial x}}$ = $e^{2x}$ of an unknown function of two variables $u=u(t, x)$ where $c \in \mathbb{R}$ is a fixed parameter, and with the additional initial condition $u(0, x) = f(x)$, where $f$ is a given function. What I have so far: 1st Step: Characteristic Curves $s \to (t(s), x(s))$ ${d \over ds} u(t(s), x(s)) = t'(s){\partial u \over \partial t} + x'(s){\partial u \over \partial x}$ Char. curves:  \begin{cases} t'(s) = 1 \\ x'(s) = c \\ \end{cases} so: \begin{cases} t(s) = s+d_1 \\ x(s) = cs + d_2  \\ \end{cases} the characteristics eq's thus become: \begin{cases} t(s) = s' \\ x(s) = cs + d  \\ \end{cases} 2nd Step: As long as everything above is correct, this is now where I'm getting stuck: Solve for $u$ along the curve: $z(s) = u(t(s), x(s))$ $z'(s) + z(s) = e^{2x}$ and then I'm lost...any help would be great...","I keep getting stuck and have a hard time understanding my professor, so I'm hoping to get some help here.  The question is: Solve the partial diff/eq: ${\partial u}\over{\partial t}$ + $c {{\partial u}\over{\partial x}}$ = $e^{2x}$ of an unknown function of two variables $u=u(t, x)$ where $c \in \mathbb{R}$ is a fixed parameter, and with the additional initial condition $u(0, x) = f(x)$, where $f$ is a given function. What I have so far: 1st Step: Characteristic Curves $s \to (t(s), x(s))$ ${d \over ds} u(t(s), x(s)) = t'(s){\partial u \over \partial t} + x'(s){\partial u \over \partial x}$ Char. curves:  \begin{cases} t'(s) = 1 \\ x'(s) = c \\ \end{cases} so: \begin{cases} t(s) = s+d_1 \\ x(s) = cs + d_2  \\ \end{cases} the characteristics eq's thus become: \begin{cases} t(s) = s' \\ x(s) = cs + d  \\ \end{cases} 2nd Step: As long as everything above is correct, this is now where I'm getting stuck: Solve for $u$ along the curve: $z(s) = u(t(s), x(s))$ $z'(s) + z(s) = e^{2x}$ and then I'm lost...any help would be great...",,"['ordinary-differential-equations', 'partial-differential-equations']"
99,Is it a peculiar solution?,Is it a peculiar solution?,,"Given the differential equation : $y'=\sin(x+y)$ I found for $x+y \neq 2k\pi-\frac{\pi}{2},k\in \mathbb{Z}$: $\frac{-2}{\tan\left(\frac{x+y}{2}\right)+1}=x+c$..But...what's with $y=-x+ 2k\pi-\frac{\pi}{2},k\in \mathbb{Z}$ Is it a peculiar solution?","Given the differential equation : $y'=\sin(x+y)$ I found for $x+y \neq 2k\pi-\frac{\pi}{2},k\in \mathbb{Z}$: $\frac{-2}{\tan\left(\frac{x+y}{2}\right)+1}=x+c$..But...what's with $y=-x+ 2k\pi-\frac{\pi}{2},k\in \mathbb{Z}$ Is it a peculiar solution?",,['ordinary-differential-equations']
