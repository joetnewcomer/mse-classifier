,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Recurrence relation between two series,Recurrence relation between two series,,"The problem asks to prove that $$\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^2(n+1)^2\ldots(n+p-1)^2} = \frac{5p+2}{4(p+1)}\frac{1}{p!^2}-\frac{p(p+1)^3}{4}\cdot\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^2(n+1)^2\ldots(n+p+1)^2}$$ where $p$ is a natural number. How can I show that this equation holds? I think that this follows from a Kummer transformation , which I briefly describe here: usually we want to accelerate the convergence of a given series $a_n$ by means of the following obvious transformation: $$\sum a_n = \gamma C + \sum\left(1-\gamma\frac{c_n}{a_n}\right)a_n$$ where $c_n$ is a convergent series of known sum $C$ such that $a_n/c_n \to \gamma \ne 0 $ (a finite number) as $n\to +\infty$ ; usually $c_n$ is chosen as much close to $a_n$ as possible, to make the convergence more rapid. In this particular example, I tried $c_n = (n+y)a_n-(n+2+y)a_{n+2}$ (which is a telescoping series, thus $C$ is easy to compute) with $y$ to determine such that the sum on the right hand side of the kummer transformation is close to the right hand side of my equation. This choice for $c_n$ is tyipical in other scenarios, but it doesn't really work here. What should be a proper choice? Also, can I approach the problem from another point? Finally let me point out that this equation is a recurrence relation if we let $T_p$ be the sum of the first series. An interesting problem would be to determine the relation between $T_1$ and $T_{2k+1}$ for example, which leads to other potential transformations...","The problem asks to prove that where is a natural number. How can I show that this equation holds? I think that this follows from a Kummer transformation , which I briefly describe here: usually we want to accelerate the convergence of a given series by means of the following obvious transformation: where is a convergent series of known sum such that (a finite number) as ; usually is chosen as much close to as possible, to make the convergence more rapid. In this particular example, I tried (which is a telescoping series, thus is easy to compute) with to determine such that the sum on the right hand side of the kummer transformation is close to the right hand side of my equation. This choice for is tyipical in other scenarios, but it doesn't really work here. What should be a proper choice? Also, can I approach the problem from another point? Finally let me point out that this equation is a recurrence relation if we let be the sum of the first series. An interesting problem would be to determine the relation between and for example, which leads to other potential transformations...",\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^2(n+1)^2\ldots(n+p-1)^2} = \frac{5p+2}{4(p+1)}\frac{1}{p!^2}-\frac{p(p+1)^3}{4}\cdot\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^2(n+1)^2\ldots(n+p+1)^2} p a_n \sum a_n = \gamma C + \sum\left(1-\gamma\frac{c_n}{a_n}\right)a_n c_n C a_n/c_n \to \gamma \ne 0  n\to +\infty c_n a_n c_n = (n+y)a_n-(n+2+y)a_{n+2} C y c_n T_p T_1 T_{2k+1},"['real-analysis', 'sequences-and-series']"
1,Show that function is a diffeomorphism,Show that function is a diffeomorphism,,"Consider two non-empty, bounded and convex domains $A \subset B \subset \mathbb{R}^d$ with $C^2$ -boundaries. Define a function $\phi:\partial A \to \partial B$ by $x\mapsto y$ such that $y-x=\lambda_x\vec{n}_A(x)$ for some $\lambda_x >0$ where $\vec{n}_A(x)$ is the outer normal of $\partial A$ at $x$ . So roughly speaking this function maps any boundary point of $A$ to its ""orthogonal counterpart"" on the boundary of $B$ . I expect that this function is homeomorphic (this should follow from the fact that any boundary of a convex set is homeomorphic to the unit disk) and I also guess that it is a diffeomorphism due to the $C^2$ -boundaries but I don't know how to show this. In particular I am interested in a explicit expression for $|\det (D\phi(x))|$ as I want to use the integration by substitution formula. Can anyone help me with these two problems? I appreciate any help.","Consider two non-empty, bounded and convex domains with -boundaries. Define a function by such that for some where is the outer normal of at . So roughly speaking this function maps any boundary point of to its ""orthogonal counterpart"" on the boundary of . I expect that this function is homeomorphic (this should follow from the fact that any boundary of a convex set is homeomorphic to the unit disk) and I also guess that it is a diffeomorphism due to the -boundaries but I don't know how to show this. In particular I am interested in a explicit expression for as I want to use the integration by substitution formula. Can anyone help me with these two problems? I appreciate any help.",A \subset B \subset \mathbb{R}^d C^2 \phi:\partial A \to \partial B x\mapsto y y-x=\lambda_x\vec{n}_A(x) \lambda_x >0 \vec{n}_A(x) \partial A x A B C^2 |\det (D\phi(x))|,"['real-analysis', 'calculus', 'geometry', 'analysis', 'differential-geometry']"
2,Convergence of $\sum_{n=1}^\infty x_n^k$,Convergence of,\sum_{n=1}^\infty x_n^k,"Let $S\subseteq \mathbb Z^+$ be a set of positive odd numbers. I am asked to prove that there exists a sequence $(x_n)$ such that for any positive integer $k$ , $$ \sum_{n=1}^\infty x_n^k $$ converges iff $k\in S$ . I have no idea where to start. Even in the special case $S=\{1\}$ I don't know if any sequence would work. Any hints? If $S=\{1\}$ , then we have to find a sequence such that $\sum x_n$ converges but $\sum x_n^k$ doesn't for $k\geq 2$ . However, for a positive sequence, if $\sum x_n^k$ converges then $\sum x_n^{k+1}$ converges, so we must not choose $(x_n)$ to be a sequence of positive terms. But what can I do next?","Let be a set of positive odd numbers. I am asked to prove that there exists a sequence such that for any positive integer , converges iff . I have no idea where to start. Even in the special case I don't know if any sequence would work. Any hints? If , then we have to find a sequence such that converges but doesn't for . However, for a positive sequence, if converges then converges, so we must not choose to be a sequence of positive terms. But what can I do next?","S\subseteq \mathbb Z^+ (x_n) k 
\sum_{n=1}^\infty x_n^k
 k\in S S=\{1\} S=\{1\} \sum x_n \sum x_n^k k\geq 2 \sum x_n^k \sum x_n^{k+1} (x_n)","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
3,Beyond the horizon - mathematics with three signs,Beyond the horizon - mathematics with three signs,,"Instead of having two signs $\{+,-\}$ consider mathematics with three $\{r,g,b\}$ . It is $$(r)x + (g)x + (b)x = 0, \ x\in(0,\infty).\tag{1}$$ where $$0= (r)0 = (g)0 = (b)0,$$ is the additive zero-element. It holds $$(r)x + (g)y + (b)z = \text{sign}(\max(x,y,z))[\max(x,y,z) - \text{mid}(x,y,z)].\tag{2}$$ Multiplication is given by the table below. Smaller-than relation still denoted by $<$ is clock-wise $$(r)x < (g)x < (b)x < (r)x < ...$$ giving the following number line. What are the differences to usual mathematics/analysis? Is that approach consistent? What about applicability? I am prob. not the first having this idea... Edit: In contrast to usual analysis, addition is not associative $<$ is not transitive there is no $\max$ or $\min$ among three different signs every number has one unique $n$ -th root $x+y=0$ , $x$ fixed, has two solutions every integer has two successors and two predecessors a successor is also a predecessor of the same number It looks (1) does not work. Better use $$(r)x + (g)x = (g)x + (b)x = (r)x + (b)x= 0.$$ An example for addition:","Instead of having two signs consider mathematics with three . It is where is the additive zero-element. It holds Multiplication is given by the table below. Smaller-than relation still denoted by is clock-wise giving the following number line. What are the differences to usual mathematics/analysis? Is that approach consistent? What about applicability? I am prob. not the first having this idea... Edit: In contrast to usual analysis, addition is not associative is not transitive there is no or among three different signs every number has one unique -th root , fixed, has two solutions every integer has two successors and two predecessors a successor is also a predecessor of the same number It looks (1) does not work. Better use An example for addition:","\{+,-\} \{r,g,b\} (r)x + (g)x + (b)x = 0, \ x\in(0,\infty).\tag{1} 0= (r)0 = (g)0 = (b)0, (r)x + (g)y + (b)z = \text{sign}(\max(x,y,z))[\max(x,y,z) - \text{mid}(x,y,z)].\tag{2} < (r)x < (g)x < (b)x < (r)x < ... < \max \min n x+y=0 x (r)x + (g)x = (g)x + (b)x = (r)x + (b)x= 0.","['real-analysis', 'group-theory', 'logic', 'arithmetic']"
4,Weak derivative under the integral sign,Weak derivative under the integral sign,,"Let $\Omega$ be a bounded and regular open subset $\Omega$ of $\mathbb{R}^N$ and $u:[0,\infty)\times \Omega\to \mathbb{R}$ be a smooth function (for example a smooth solution to a PDE). Thus the function $w=\min(0,u)$ has weak time derivative given by $w_t=u_t.1_{\{u \leq 0 \}}$ . Does the following ""weak differentiation under the integral sign"" holds? $$\frac{d}{dt}\int_\Omega w(t,x)dx=\int_\Omega \frac{\partial}{\partial t}w(t,x)dx,$$ for almost all $t\geq 0$ . We know that differentiation under the integral sign holds for $u$ because it is smooth. But I am wondering if it also holds for a function like $w=\min(0,u)$ which only has a weak derivative. My guess is that since $w$ has a weak derivative in time (real line), then it must be differentiable for almost all $t\geq 0$ (which is not necessarily true in higher dimension, for example differentiation with respect to space $x$ ). So $\int_\Omega w(t,x)dx$ must also be differentiable for almost all $t\geq 0$ and so we can use some kind of dominated convergence theorem.","Let be a bounded and regular open subset of and be a smooth function (for example a smooth solution to a PDE). Thus the function has weak time derivative given by . Does the following ""weak differentiation under the integral sign"" holds? for almost all . We know that differentiation under the integral sign holds for because it is smooth. But I am wondering if it also holds for a function like which only has a weak derivative. My guess is that since has a weak derivative in time (real line), then it must be differentiable for almost all (which is not necessarily true in higher dimension, for example differentiation with respect to space ). So must also be differentiable for almost all and so we can use some kind of dominated convergence theorem.","\Omega \Omega \mathbb{R}^N u:[0,\infty)\times \Omega\to \mathbb{R} w=\min(0,u) w_t=u_t.1_{\{u \leq 0 \}} \frac{d}{dt}\int_\Omega w(t,x)dx=\int_\Omega \frac{\partial}{\partial t}w(t,x)dx, t\geq 0 u w=\min(0,u) w t\geq 0 x \int_\Omega w(t,x)dx t\geq 0","['real-analysis', 'partial-differential-equations', 'lebesgue-integral', 'partial-derivative', 'weak-derivatives']"
5,Proof that the inverse of an analytic function is analytic which uses only real analysis.,Proof that the inverse of an analytic function is analytic which uses only real analysis.,,"I would like to prove the following result: Let $f:R\to R$ be such that $f(x)=\sum\limits_{k=0}^\infty a_k(x-c)^k$ for all $x$ in some open set $O\subset R$ . Suppose that $f'(c)\ne 0$ . Then there is a function $g:T\to R$ such that $g(f(x))=x$ and $g(x)=\sum\limits_{k=0}^\infty b_k(x-f(c))^k$ , where $T$ is an open set of $R$ containing $f(c)$ . I know how to prove that $f$ is locally invertible but not how to prove that its inverse can be written as a power series on some open set. There are many proofs for the inverse function theorem which use complex analysis but I would like to write a proof which uses only real analysis. I have been writing a book of proofs of many of the important theorems in calculus and I don't want to introduce complex analysis for the proof of one theorem. Thanks, Andrew Murdza.","I would like to prove the following result: Let be such that for all in some open set . Suppose that . Then there is a function such that and , where is an open set of containing . I know how to prove that is locally invertible but not how to prove that its inverse can be written as a power series on some open set. There are many proofs for the inverse function theorem which use complex analysis but I would like to write a proof which uses only real analysis. I have been writing a book of proofs of many of the important theorems in calculus and I don't want to introduce complex analysis for the proof of one theorem. Thanks, Andrew Murdza.",f:R\to R f(x)=\sum\limits_{k=0}^\infty a_k(x-c)^k x O\subset R f'(c)\ne 0 g:T\to R g(f(x))=x g(x)=\sum\limits_{k=0}^\infty b_k(x-f(c))^k T R f(c) f,"['real-analysis', 'power-series', 'inverse-function-theorem']"
6,The probability that a Binomial Distribution deviates from its mean by one standard deviation,The probability that a Binomial Distribution deviates from its mean by one standard deviation,,"Let $X$ be a random variable that follows the Binomial Distribution $\text{BIN}(n,p)$ , where $n$ is a positive integer while $p\in(0,1)$ . Its mean is $np$ , and standard deviation is $\sqrt{np(1-p)}$ . Chebyshev's inequality yields that $$\Pr\left(|X-np| > \sqrt{np(1-p)}\right) \le 1,$$ which is trivial. Hoeffding’s inequality seems not helpful to improve the bound if applied in a direct way. Questions: When $p=1/2$ , is it possible to prove for all $n\ge 1$ that $$\Pr\left(|X-np| > \sqrt{np(1-p)}\right) \le \frac{1}{2}?$$ What can we say for a general $p\in(0,1)$ ? Remarks: I found a positive answer to Question 1 (presented below; essentially the same as @Mau314's comment). However, it is not completely satisfactory because we have to verify the inequality for small n (at most 25) numerically. I am still looking forward to an answer that is completely analytical . I am teaching basic probability theory and these questions occur to me when I think about the Central Limit Theorem. When $n\rightarrow \infty$ , asymptotically we have $$\Pr\left(|X-np| > \sqrt{np(1-p)}\right) \sim \Pr(|Y| > 1) < \frac{1}{2},$$ where $Y$ is a random variable that follows the Standard Normal Distribution. Hence I raise the questions by curiousity. Note that my main interest is the non-asymptotic behaviour of the probability because the asymptotic case is characterized by the CLT. One may attack the problem by directly estimate the cumulative distribution function of Binomial Distributions. To this end, bounds for Binomial Coefficients are likely necessary. Results of such kind can be found in, e.g., [Das] , [Stanica] , [Spencer, Chapter 5] , and Wikipedia . Note that non-asymptotic estimations are needed. Thanks.","Let be a random variable that follows the Binomial Distribution , where is a positive integer while . Its mean is , and standard deviation is . Chebyshev's inequality yields that which is trivial. Hoeffding’s inequality seems not helpful to improve the bound if applied in a direct way. Questions: When , is it possible to prove for all that What can we say for a general ? Remarks: I found a positive answer to Question 1 (presented below; essentially the same as @Mau314's comment). However, it is not completely satisfactory because we have to verify the inequality for small n (at most 25) numerically. I am still looking forward to an answer that is completely analytical . I am teaching basic probability theory and these questions occur to me when I think about the Central Limit Theorem. When , asymptotically we have where is a random variable that follows the Standard Normal Distribution. Hence I raise the questions by curiousity. Note that my main interest is the non-asymptotic behaviour of the probability because the asymptotic case is characterized by the CLT. One may attack the problem by directly estimate the cumulative distribution function of Binomial Distributions. To this end, bounds for Binomial Coefficients are likely necessary. Results of such kind can be found in, e.g., [Das] , [Stanica] , [Spencer, Chapter 5] , and Wikipedia . Note that non-asymptotic estimations are needed. Thanks.","X \text{BIN}(n,p) n p\in(0,1) np \sqrt{np(1-p)} \Pr\left(|X-np| > \sqrt{np(1-p)}\right) \le 1, p=1/2 n\ge 1 \Pr\left(|X-np| > \sqrt{np(1-p)}\right) \le \frac{1}{2}? p\in(0,1) n\rightarrow \infty \Pr\left(|X-np| > \sqrt{np(1-p)}\right) \sim \Pr(|Y| > 1) < \frac{1}{2}, Y","['real-analysis', 'probability', 'probability-theory', 'statistics', 'probability-distributions']"
7,"When is One Polynomial ""Similar"" to Another","When is One Polynomial ""Similar"" to Another",,"Let $f$ and $g$ be functions. We will say they are similar (in somewhat of an extension to what it means for linear maps to be similar) if there exists a bijection $p$ such that $$f=p^{-1}\circ g\circ p.$$ Let this relation be denoted as $\sim$ . Under what conditions can polynomials be similar? Specifically, I am trying to find out for which polynomials $f$ can we say that $f\sim T_n$ for some $n$ where $T_n$ is the $n$ -th Chebyshev polynomial. Note that when $f$ , $g$ , and $p$ are all restricted to linear functions from $\mathbb{R}^n\to\mathbb{R}^n$ this aligns with the normal definition of similar matrices. Could a similar equivalence be drawn for polynomial functions? Possibly one could consider matrices over the field $\mathbb{R}[x]$ , and apply a similar theory to get partial results for the above problem.","Let and be functions. We will say they are similar (in somewhat of an extension to what it means for linear maps to be similar) if there exists a bijection such that Let this relation be denoted as . Under what conditions can polynomials be similar? Specifically, I am trying to find out for which polynomials can we say that for some where is the -th Chebyshev polynomial. Note that when , , and are all restricted to linear functions from this aligns with the normal definition of similar matrices. Could a similar equivalence be drawn for polynomial functions? Possibly one could consider matrices over the field , and apply a similar theory to get partial results for the above problem.",f g p f=p^{-1}\circ g\circ p. \sim f f\sim T_n n T_n n f g p \mathbb{R}^n\to\mathbb{R}^n \mathbb{R}[x],"['real-analysis', 'functional-analysis']"
8,"Symmetric monotonic function $f$ on $[0,1]^2$ with $\int_0^1f(x,y)dy=x$",Symmetric monotonic function  on  with,"f [0,1]^2 \int_0^1f(x,y)dy=x","I am searching for an almost-everywhere continuous and monotonic function $f:[0,1]^2\to[0,1]$ with the following properties: $f(x,y)=f(y,x)$ $f(0,y)=0$ and $f(1,y)=1$ (so $f(0,1)$ and $f(1,0)$ are undefined) $f(x,y)=1-f(1-x,1-y)$ therefore $f(0.5,0.5)=0.5$ $\int_0^1f(x,y)dy=x$ Here is what it should look like Are there functions having those properties? And if yes which one? If possible, I would prefer a not too complicated function. Here is a clarification: By ""monotonic function $f:[0,1]^2\to[0,1]$ "" I mean ""function for which, for any $y\in [0,1]$ , the function $f_y: [0,1]\to[0,1]$ so that $f_y(x)=f(x,y)$ is monotonic (and reciprocally by swapping $x$ and $y$ )"". Here are my attempts so far: there is a singular ""discontinuous solution"": the function which is equal to 0 when $x+y\lt1$ and to 1 when $x+y\gt1$ ; it's not really a solution since it's discontinuous, but it can be a starter if you know an invertible and monotonic function $g$ on $[0,1]$ with $g(0)=\infty$ and $g(1)=0$ , you can combine them like this: $f(x,y)=g^{-1}(g(x)g(y))$ and it already have the properties 1 and 2 to enforce the property 4 you can define instead $f(x,y)=g^{-1}(\frac{g(x)g(y)}{g(0.5)})$ to enforce the property 3 (and 4) you can define instead $f(x,y)=h^{-1}(\frac{h(g^{-1}(\frac{g(x)g(y)}{g(0.5}))+h(1-g^{-1}(\frac{g(1-x)g(1-y)}{g(0.5)}))}{2})$ with a generalised $h$ -mean, but it's starting to get ugly (the $\frac{1}{g(0.5)}$ seems to be mandatory to have the right shape but I can't explain why) if we try with $g=ln$ , we get $f(x,y)=exp(\frac{ln(x)ln(y)}{ln(0.5)})$ , but we don't have the property 3, so we need to change it to $f(x,y)=h^{-1}(\frac{h(exp(\frac{ln(x)ln(y)}{ln(0.5)}))+h(1-exp(\frac{ln(1-x)ln(1-y)}{ln(0.5)}))}{2})$ , but I can't find an $h$ that allow to have property 5: using the power means (with $h(x)=x^p$ ), at $y=0.5$ property 5 always holds, but at $y=0.25$ , I always have $\int_0^1f(0.25,y)dy\gt x$ . It seems to decrease with $p$ , but even with $p=-\infty$ (which corresponds to the minimum instead of a mean), I have $\int_0^1f(0.25,y)dy=0.276657\gt x$ idem for $g=arctanh$ : the minimum is $0.2703866$ it's slightly better with $g=x^{-1}-1$ : we get $f(x,y)=((x^{-1}-1)(y^{-1}-1)+1)^{-1}$ with which properties 1, 2, 3 and 4 already hold, but sadly not property 5: $\int_0^1f(x,y)dy=x\frac{2x-1+2(x-1)arctanh(2x-1)}{(2x-1)^2}\neq x$ Here is the background: This question follows this one : I'm trying to find a way to calculate (or approximate) $Pr(S|A B)$ given $Pr(S|A)$ and $Pr(S|B)$ (and possibly $Pr(S)$ ), and knowing that $A$ and $B$ are independent, and that if $Pr(S|A_1)\geq Pr(S|A_2)$ then $Pr(S|A_1 B)\geq Pr(S|A_2 B)$ . The last condition was not present in the initial question but seems useful to me: if the skill categories are well defined, a player that is better at a category of skills than another player should be better at any skill test from this category than the other player. EDIT: Since that question was asked and answered, I realised that the 5th property is not necessary for my problem, see my last comment .","I am searching for an almost-everywhere continuous and monotonic function with the following properties: and (so and are undefined) therefore Here is what it should look like Are there functions having those properties? And if yes which one? If possible, I would prefer a not too complicated function. Here is a clarification: By ""monotonic function "" I mean ""function for which, for any , the function so that is monotonic (and reciprocally by swapping and )"". Here are my attempts so far: there is a singular ""discontinuous solution"": the function which is equal to 0 when and to 1 when ; it's not really a solution since it's discontinuous, but it can be a starter if you know an invertible and monotonic function on with and , you can combine them like this: and it already have the properties 1 and 2 to enforce the property 4 you can define instead to enforce the property 3 (and 4) you can define instead with a generalised -mean, but it's starting to get ugly (the seems to be mandatory to have the right shape but I can't explain why) if we try with , we get , but we don't have the property 3, so we need to change it to , but I can't find an that allow to have property 5: using the power means (with ), at property 5 always holds, but at , I always have . It seems to decrease with , but even with (which corresponds to the minimum instead of a mean), I have idem for : the minimum is it's slightly better with : we get with which properties 1, 2, 3 and 4 already hold, but sadly not property 5: Here is the background: This question follows this one : I'm trying to find a way to calculate (or approximate) given and (and possibly ), and knowing that and are independent, and that if then . The last condition was not present in the initial question but seems useful to me: if the skill categories are well defined, a player that is better at a category of skills than another player should be better at any skill test from this category than the other player. EDIT: Since that question was asked and answered, I realised that the 5th property is not necessary for my problem, see my last comment .","f:[0,1]^2\to[0,1] f(x,y)=f(y,x) f(0,y)=0 f(1,y)=1 f(0,1) f(1,0) f(x,y)=1-f(1-x,1-y) f(0.5,0.5)=0.5 \int_0^1f(x,y)dy=x f:[0,1]^2\to[0,1] y\in [0,1] f_y: [0,1]\to[0,1] f_y(x)=f(x,y) x y x+y\lt1 x+y\gt1 g [0,1] g(0)=\infty g(1)=0 f(x,y)=g^{-1}(g(x)g(y)) f(x,y)=g^{-1}(\frac{g(x)g(y)}{g(0.5)}) f(x,y)=h^{-1}(\frac{h(g^{-1}(\frac{g(x)g(y)}{g(0.5}))+h(1-g^{-1}(\frac{g(1-x)g(1-y)}{g(0.5)}))}{2}) h \frac{1}{g(0.5)} g=ln f(x,y)=exp(\frac{ln(x)ln(y)}{ln(0.5)}) f(x,y)=h^{-1}(\frac{h(exp(\frac{ln(x)ln(y)}{ln(0.5)}))+h(1-exp(\frac{ln(1-x)ln(1-y)}{ln(0.5)}))}{2}) h h(x)=x^p y=0.5 y=0.25 \int_0^1f(0.25,y)dy\gt x p p=-\infty \int_0^1f(0.25,y)dy=0.276657\gt x g=arctanh 0.2703866 g=x^{-1}-1 f(x,y)=((x^{-1}-1)(y^{-1}-1)+1)^{-1} \int_0^1f(x,y)dy=x\frac{2x-1+2(x-1)arctanh(2x-1)}{(2x-1)^2}\neq x Pr(S|A B) Pr(S|A) Pr(S|B) Pr(S) A B Pr(S|A_1)\geq Pr(S|A_2) Pr(S|A_1 B)\geq Pr(S|A_2 B)","['real-analysis', 'integration', 'recreational-mathematics', 'surfaces']"
9,"On the sets of sums $\sum\limits_{n=1}^\infty\frac{a_n}{n^s}$ with $(a_n)$ periodic and integer valued, for different values of $s$ natural number","On the sets of sums  with  periodic and integer valued, for different values of  natural number",\sum\limits_{n=1}^\infty\frac{a_n}{n^s} (a_n) s,"For every positive integer $s$, let $A_s$ denote the set of the sums of the converging series $\sum\limits_{n=1}^\infty\frac{a_n}{n^s}$ for every periodic sequence of integers $(a_n)$. Then each $A_s$ is a countable dense subset of the real numbers, and an additive group. The set $A_1$ is in fact a vector space with scalars drawn from the rationals. I suspect $A_s$ should contain no non-zero rationals (counterexamples are welcome!) but a proof of this would imply that Catalan's number is irrational so attacking that directly should be avoided... Question Can anything interesting be said about the intersections of these sets? For example, is it the case that $A_s\cap A_t=\{0\}$ for every $s\ne t$? This question comes from my own musings and it may be open. I suppose this is a risk one always has when asking questions that flirt with the zeta function. Some Notes: $\zeta(s)\in A_s$, $\eta(s) \in A_s$, $\ln(\mathbb{Q})\subset A_1$. Generalizations that may be worthy of follow up: 1) Is this just the case for positive real numbers $s\neq t$? This has now been answered below. This is not the case. 2) If we define $A_s$ with Gaussian integers do we get the same results? Edit 1 (an effort to spruce this question up): Some Motivations + some cool values This question didn't get the excitement I expected so I will now add some crazy values! Here are a couple of values from Dirichlet series in $A_1$, $A_3$, $A_5$, $A_7$. We can compute specific values in $A_s$ but when we manage to get exact forms of values in these sets (it seems) invariably this is because of their relationship to Dirichlet Series. $$f(s,\vec{a})= \sum_{n=1}^\infty{\frac{a_n}{n^s}} $$ Then  $$ \begin{array}{c|c|c|c|c|c} f(s,\vec{a}) & \vec{a}=(1,-1)  & \vec{a}=(1,0,-1,0)  & \vec{a}=(1,1,0,-1,-1,0)  & \vec{a}=(1,0,1,0,-1,0,-1,0) \\ \hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%% s=1  & \ln(2)  & \frac{\pi}{4}           & \frac{2  \pi}{3\sqrt{3}}  & \frac{\pi}{2\sqrt{2}}  \\ %%%%%%%%%%%%%%%%%%%%%%%%%%%%% s=3     & \frac{3}{4}\zeta(3)        & \frac{\pi^3}{32}  & \frac{5  \pi^3}{81\sqrt{3}}    & \frac{3\pi^3}{64\sqrt{2}} \\ %%%%%%%%%%%%%%%%%%%%%%%%%%%%% s=5     & \frac{15}{16}\zeta(5)        & \frac{5 \pi^5}{1536}    & \frac{17 \pi^5}{2916\sqrt{3}}  & \frac{19 \pi^5}{4096 \sqrt{2}} \\ %%%%%%%%%%%%%%%%%%%%%%%%%%%%% s=7     & \frac{63}{64}\zeta(7)        & \frac{61\pi^7}{184320}  & \frac{91 \pi^7}{157464\sqrt{3}}  & \frac{307 \pi^7}{655360\sqrt{2}} %%%%%%%%%%%%%%%%%%%%%%%%%%%%% \end{array}$$ Column(1) Column(2) Column(3) Column(4) And more So here are just some specific elements in $A_s$ to get a feeling for these sets.","For every positive integer $s$, let $A_s$ denote the set of the sums of the converging series $\sum\limits_{n=1}^\infty\frac{a_n}{n^s}$ for every periodic sequence of integers $(a_n)$. Then each $A_s$ is a countable dense subset of the real numbers, and an additive group. The set $A_1$ is in fact a vector space with scalars drawn from the rationals. I suspect $A_s$ should contain no non-zero rationals (counterexamples are welcome!) but a proof of this would imply that Catalan's number is irrational so attacking that directly should be avoided... Question Can anything interesting be said about the intersections of these sets? For example, is it the case that $A_s\cap A_t=\{0\}$ for every $s\ne t$? This question comes from my own musings and it may be open. I suppose this is a risk one always has when asking questions that flirt with the zeta function. Some Notes: $\zeta(s)\in A_s$, $\eta(s) \in A_s$, $\ln(\mathbb{Q})\subset A_1$. Generalizations that may be worthy of follow up: 1) Is this just the case for positive real numbers $s\neq t$? This has now been answered below. This is not the case. 2) If we define $A_s$ with Gaussian integers do we get the same results? Edit 1 (an effort to spruce this question up): Some Motivations + some cool values This question didn't get the excitement I expected so I will now add some crazy values! Here are a couple of values from Dirichlet series in $A_1$, $A_3$, $A_5$, $A_7$. We can compute specific values in $A_s$ but when we manage to get exact forms of values in these sets (it seems) invariably this is because of their relationship to Dirichlet Series. $$f(s,\vec{a})= \sum_{n=1}^\infty{\frac{a_n}{n^s}} $$ Then  $$ \begin{array}{c|c|c|c|c|c} f(s,\vec{a}) & \vec{a}=(1,-1)  & \vec{a}=(1,0,-1,0)  & \vec{a}=(1,1,0,-1,-1,0)  & \vec{a}=(1,0,1,0,-1,0,-1,0) \\ \hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%% s=1  & \ln(2)  & \frac{\pi}{4}           & \frac{2  \pi}{3\sqrt{3}}  & \frac{\pi}{2\sqrt{2}}  \\ %%%%%%%%%%%%%%%%%%%%%%%%%%%%% s=3     & \frac{3}{4}\zeta(3)        & \frac{\pi^3}{32}  & \frac{5  \pi^3}{81\sqrt{3}}    & \frac{3\pi^3}{64\sqrt{2}} \\ %%%%%%%%%%%%%%%%%%%%%%%%%%%%% s=5     & \frac{15}{16}\zeta(5)        & \frac{5 \pi^5}{1536}    & \frac{17 \pi^5}{2916\sqrt{3}}  & \frac{19 \pi^5}{4096 \sqrt{2}} \\ %%%%%%%%%%%%%%%%%%%%%%%%%%%%% s=7     & \frac{63}{64}\zeta(7)        & \frac{61\pi^7}{184320}  & \frac{91 \pi^7}{157464\sqrt{3}}  & \frac{307 \pi^7}{655360\sqrt{2}} %%%%%%%%%%%%%%%%%%%%%%%%%%%%% \end{array}$$ Column(1) Column(2) Column(3) Column(4) And more So here are just some specific elements in $A_s$ to get a feeling for these sets.",,"['real-analysis', 'sequences-and-series', 'vector-spaces', 'zeta-functions', 'dirichlet-series']"
10,Examine uniform convergence of the series $\sum_{n=1}^\infty \frac{x}{(1+(n-1)x)(1+nx)}$,Examine uniform convergence of the series,\sum_{n=1}^\infty \frac{x}{(1+(n-1)x)(1+nx)},"Examine uniform convergence of the series $$\sum_{n=1}^\infty  \frac{x}{(1+(n-1)x)(1+nx)}$$ on the intervals $[a,b]$ where $(0 < a < b)$ and $[0,b]$ where $(b>0)$ My attempt: (i) On the interval $[a,b]$ Approach (1) Notice that $$\frac{x}{(1+(n-1)x)(1+nx)} = \frac{1}{1+(n-1)x} - \frac{1}{1+nx}$$ Hence: $$\sum_{k=1}^n \frac{x}{(1+(n-1)x)(1+nx)} = 1 - \frac{1}{1+nx}$$ so the series converges pointwise to the constant $1$-function. The convergence is uniform: $$\sup_{x \in [a,b]}\left|1-\frac{1}{1+nx}-1\right| \leq \frac{1}{na} \to 0$$ Approach (2) $$\sup_{x \in [a,b]} \left|\frac{x}{(1+(n-1)x)(1+nx)}\right| \leq \frac{b}{(1+(n-1)a)(1+na)} = \frac{b}{n²}\frac{1}{(1/n + a((n-1)/n))(1/n +a)}$$ and the last expression is a converging sequence (to $a^{-2}$), so it is bounded above by a real number $M$ and hence:$$\sup_{x \in [a,b]} \left|\frac{x}{(1+(n-1)x)(1+nx)}\right| \leq \frac{b}{(1+(n-1)a)(1+na)} \leq \frac{Mb}{n^2}$$ and we can deduce uniform convergence by the Weierstrass' M-test. (ii) On the interval $[0,b]$ Approach (1) Again, using the partial sums, we can see that the series of functions converges pointwise to $f(x) = \begin{cases}1 \quad x \neq 0 \\0 \quad x =0\end{cases}$. This is a function that is not continuous, but the partial sums are. Hence, the convergence can't be uniform. Approach (2) We can also show this directly. Let $n \in \mathbb{N}$. Let $m \geq n$ such that $\frac{1}{m}< b$. Let $x = \frac{1}{m}$. Then: $$\left|1-\frac{1}{1+mx}-1\right| = 1/2$$ Hence, the series does not converge uniformly. Approach (3) Now, I would like to use Cauchy's convergence criterium, as I consider using partial sums a bit cheating, in the sense that it is often impossible to find them. However, I'm struggling with this one. Can someone give a proof that the series does not converge uniformly with the cauchy criterium? I.e., prove that: $$\exists \epsilon > 0: \forall n : \exists p,q > n: \exists x \in [0,b] \left|\sum_k^p - \sum_k^q\right| \geq \epsilon$$ So my questions: How do I complete the proof with the Cauchy criterium? Is what I wrote down correct?","Examine uniform convergence of the series $$\sum_{n=1}^\infty  \frac{x}{(1+(n-1)x)(1+nx)}$$ on the intervals $[a,b]$ where $(0 < a < b)$ and $[0,b]$ where $(b>0)$ My attempt: (i) On the interval $[a,b]$ Approach (1) Notice that $$\frac{x}{(1+(n-1)x)(1+nx)} = \frac{1}{1+(n-1)x} - \frac{1}{1+nx}$$ Hence: $$\sum_{k=1}^n \frac{x}{(1+(n-1)x)(1+nx)} = 1 - \frac{1}{1+nx}$$ so the series converges pointwise to the constant $1$-function. The convergence is uniform: $$\sup_{x \in [a,b]}\left|1-\frac{1}{1+nx}-1\right| \leq \frac{1}{na} \to 0$$ Approach (2) $$\sup_{x \in [a,b]} \left|\frac{x}{(1+(n-1)x)(1+nx)}\right| \leq \frac{b}{(1+(n-1)a)(1+na)} = \frac{b}{n²}\frac{1}{(1/n + a((n-1)/n))(1/n +a)}$$ and the last expression is a converging sequence (to $a^{-2}$), so it is bounded above by a real number $M$ and hence:$$\sup_{x \in [a,b]} \left|\frac{x}{(1+(n-1)x)(1+nx)}\right| \leq \frac{b}{(1+(n-1)a)(1+na)} \leq \frac{Mb}{n^2}$$ and we can deduce uniform convergence by the Weierstrass' M-test. (ii) On the interval $[0,b]$ Approach (1) Again, using the partial sums, we can see that the series of functions converges pointwise to $f(x) = \begin{cases}1 \quad x \neq 0 \\0 \quad x =0\end{cases}$. This is a function that is not continuous, but the partial sums are. Hence, the convergence can't be uniform. Approach (2) We can also show this directly. Let $n \in \mathbb{N}$. Let $m \geq n$ such that $\frac{1}{m}< b$. Let $x = \frac{1}{m}$. Then: $$\left|1-\frac{1}{1+mx}-1\right| = 1/2$$ Hence, the series does not converge uniformly. Approach (3) Now, I would like to use Cauchy's convergence criterium, as I consider using partial sums a bit cheating, in the sense that it is often impossible to find them. However, I'm struggling with this one. Can someone give a proof that the series does not converge uniformly with the cauchy criterium? I.e., prove that: $$\exists \epsilon > 0: \forall n : \exists p,q > n: \exists x \in [0,b] \left|\sum_k^p - \sum_k^q\right| \geq \epsilon$$ So my questions: How do I complete the proof with the Cauchy criterium? Is what I wrote down correct?",,['real-analysis']
11,"Is identity the only function $f$ on real line satisfying $ f(x +f(y) + yf(x)) = y +f(x) + xf(y) ,\forall x,y \in \mathbb R$? [duplicate]",Is identity the only function  on real line satisfying ? [duplicate],"f  f(x +f(y) + yf(x)) = y +f(x) + xf(y) ,\forall x,y \in \mathbb R","This question already has answers here : Find the function equation $f(x+f(y)+yf(x))=y+f(x)+xf(y)$ (2 answers) Closed 5 years ago . Let $f:\mathbb R \to \mathbb R$ be a function such that $ f(x +f(y) + yf(x)) = y +f(x) + xf(y) ,\forall x,y \in \mathbb R$ , then is it true that $f(x)=x,\forall x \in \mathbb R$ ? If not true in general , then what if we also assume $f$ is bijective , or say continuous ? Over $\mathbb C$ , $f(z)=\bar z$ is a bijective continuous , non-identity solution . Over $\mathbb R$ I can show that under the further assumption $\{f(x)/x : x\ne 0\}$ is countable, we must have $f$ is identity . With only the given functional equation , I can show that if $f(x_0)=0$ for some $x_0 $  then $x_0=0$ , and then $f(f(x))=x,\forall x \in \mathbb R$ i.e. $f$ is bijective . But I can't figure out anything else Please help . Thanks in advance","This question already has answers here : Find the function equation $f(x+f(y)+yf(x))=y+f(x)+xf(y)$ (2 answers) Closed 5 years ago . Let $f:\mathbb R \to \mathbb R$ be a function such that $ f(x +f(y) + yf(x)) = y +f(x) + xf(y) ,\forall x,y \in \mathbb R$ , then is it true that $f(x)=x,\forall x \in \mathbb R$ ? If not true in general , then what if we also assume $f$ is bijective , or say continuous ? Over $\mathbb C$ , $f(z)=\bar z$ is a bijective continuous , non-identity solution . Over $\mathbb R$ I can show that under the further assumption $\{f(x)/x : x\ne 0\}$ is countable, we must have $f$ is identity . With only the given functional equation , I can show that if $f(x_0)=0$ for some $x_0 $  then $x_0=0$ , and then $f(f(x))=x,\forall x \in \mathbb R$ i.e. $f$ is bijective . But I can't figure out anything else Please help . Thanks in advance",,"['real-analysis', 'functional-equations']"
12,Prove that an operator (with heat kernel) maps the space of $C^1$ functions with bounded derivatives to itself,Prove that an operator (with heat kernel) maps the space of  functions with bounded derivatives to itself,C^1,"Let $K$ be the heat kernel . Does the operator  $$g \mapsto\int_0^T\int_{\mathbb{R}^N} K(t-\xi,x-\zeta) \ \  f(\xi,\zeta,g(\xi,\zeta),\nabla g(\xi,\zeta)) \, d\xi \, d\zeta$$ map the space of $C^1([0,T]\times \mathbb{R}^N)$ (for any $T>0$) functions bounded and with bounded derivatives to itself if $f: [0,T]\times \mathbb{R}^N \times \mathbb{R} \times \mathbb{R}^N \to \mathbb{R}$ is sufficiently well-behaved (for instance $C^1$)? Does it do the same for the space of $C^2$ or $C^\infty$ functions with bounded derivatives? Here $\nabla$ is used to denote the gradient with respect to the space variable $x \in \mathbb{R}^N$.","Let $K$ be the heat kernel . Does the operator  $$g \mapsto\int_0^T\int_{\mathbb{R}^N} K(t-\xi,x-\zeta) \ \  f(\xi,\zeta,g(\xi,\zeta),\nabla g(\xi,\zeta)) \, d\xi \, d\zeta$$ map the space of $C^1([0,T]\times \mathbb{R}^N)$ (for any $T>0$) functions bounded and with bounded derivatives to itself if $f: [0,T]\times \mathbb{R}^N \times \mathbb{R} \times \mathbb{R}^N \to \mathbb{R}$ is sufficiently well-behaved (for instance $C^1$)? Does it do the same for the space of $C^2$ or $C^\infty$ functions with bounded derivatives? Here $\nabla$ is used to denote the gradient with respect to the space variable $x \in \mathbb{R}^N$.",,"['calculus', 'real-analysis']"
13,Decay of Fourier transform of function composition,Decay of Fourier transform of function composition,,"Given a function $f$ and an invertible matrix $A$, we have the following relation for the Fourier transforms: $$   \widehat{f \circ A}(\xi) = |\det A|^{-1} \widehat f(A^{-T}\xi). $$ In particular, $\widehat f(\xi) = O(|\xi|^{-\alpha})$ at infinity if and only if $\widehat{f \circ A}(\xi) = O(|\xi|^{-\alpha})$. Now, given a $C^\infty$-diffeomorphism $g$ which differs from identity only on some compact set $K$ we can only write (see this answer ) $$    \widehat{f \circ g}(\xi) = \int f(y)|\det g'(y)|^{-1}\exp(ig^{-1}(y)\xi) dy. $$ Is it still possible to say that $\widehat f(\xi) = O(|\xi|^{-\alpha})$ at infinity if and only if $\widehat{f \circ g}(\xi) = O(|\xi|^{-\alpha})$?","Given a function $f$ and an invertible matrix $A$, we have the following relation for the Fourier transforms: $$   \widehat{f \circ A}(\xi) = |\det A|^{-1} \widehat f(A^{-T}\xi). $$ In particular, $\widehat f(\xi) = O(|\xi|^{-\alpha})$ at infinity if and only if $\widehat{f \circ A}(\xi) = O(|\xi|^{-\alpha})$. Now, given a $C^\infty$-diffeomorphism $g$ which differs from identity only on some compact set $K$ we can only write (see this answer ) $$    \widehat{f \circ g}(\xi) = \int f(y)|\det g'(y)|^{-1}\exp(ig^{-1}(y)\xi) dy. $$ Is it still possible to say that $\widehat f(\xi) = O(|\xi|^{-\alpha})$ at infinity if and only if $\widehat{f \circ g}(\xi) = O(|\xi|^{-\alpha})$?",,"['calculus', 'real-analysis', 'fourier-analysis', 'asymptotics']"
14,If $\sum_{n=1}^{\infty}|a_n| < \infty$. then $\sum_{n=1}^{\infty}f(a_nx)$ converges,If . then  converges,\sum_{n=1}^{\infty}|a_n| < \infty \sum_{n=1}^{\infty}f(a_nx),"Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be differentiable at $0$ with $f(0) = 0$. Let $\{a_n\}$ be a real sequence such that $\sum_{n=1}^{\infty}|a_n| < \infty$.  Show that $\sum_{n=1}^{\infty}f(a_nx)$ converges for every $x$ and that the sum of the series is differentiable at $0$. I have included my attempt. Please tell me is that right way to do and what should be next; If not then what can be a better proof. I would welcome alternative proofs for this. Since $$\sum_{k = 1}^\infty |a_k|<\infty$$   Given $0<\epsilon<1$, there exists $N$ such that for all $k\geq N$   $$\sum_{k = N}^\infty |a_k|<\epsilon$$   As a result, we have that $|a_k|<\epsilon$ for all $k\geq N$.  Now, let   $$M = \max\{|a_1|,|a_2|,\dots,|a_N|,\epsilon\}$$   Then, $|a_k|\leq M$ for all $k$. Now, choose $\delta$ such that for all $x$ with $|x|<\delta$:   $$\left|\frac{f(n)}{n}-f'(0)\right|<\frac{\epsilon}{4M}$$   Then, choose $N_1$ such that $\forall n\geq N_1$, $|a_n n|<\delta$ (where $|n|<\frac{\delta}{4}$).   We then choose $N_2$ such that $$\sum_{k = N_2}^\infty |a_k|<\frac{\epsilon}{4}$$   Then, we have that:   $$\sum_{n = 1}^\infty \left(\frac{f(a_n n)}{a_n n}a_n-f'(0)a_n\right)\leq\sum_{n = 1}^{N' = \max\{N_1,N_2\}}\left|\frac{f(a_n n)}{a_n n}a_n-f'(0)a_n\right|+\sum_{n = N'+1}^{\infty}\left|\frac{f(a_n n)}{a_n n}-f'(0)\right||a_n|$$","Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be differentiable at $0$ with $f(0) = 0$. Let $\{a_n\}$ be a real sequence such that $\sum_{n=1}^{\infty}|a_n| < \infty$.  Show that $\sum_{n=1}^{\infty}f(a_nx)$ converges for every $x$ and that the sum of the series is differentiable at $0$. I have included my attempt. Please tell me is that right way to do and what should be next; If not then what can be a better proof. I would welcome alternative proofs for this. Since $$\sum_{k = 1}^\infty |a_k|<\infty$$   Given $0<\epsilon<1$, there exists $N$ such that for all $k\geq N$   $$\sum_{k = N}^\infty |a_k|<\epsilon$$   As a result, we have that $|a_k|<\epsilon$ for all $k\geq N$.  Now, let   $$M = \max\{|a_1|,|a_2|,\dots,|a_N|,\epsilon\}$$   Then, $|a_k|\leq M$ for all $k$. Now, choose $\delta$ such that for all $x$ with $|x|<\delta$:   $$\left|\frac{f(n)}{n}-f'(0)\right|<\frac{\epsilon}{4M}$$   Then, choose $N_1$ such that $\forall n\geq N_1$, $|a_n n|<\delta$ (where $|n|<\frac{\delta}{4}$).   We then choose $N_2$ such that $$\sum_{k = N_2}^\infty |a_k|<\frac{\epsilon}{4}$$   Then, we have that:   $$\sum_{n = 1}^\infty \left(\frac{f(a_n n)}{a_n n}a_n-f'(0)a_n\right)\leq\sum_{n = 1}^{N' = \max\{N_1,N_2\}}\left|\frac{f(a_n n)}{a_n n}a_n-f'(0)a_n\right|+\sum_{n = N'+1}^{\infty}\left|\frac{f(a_n n)}{a_n n}-f'(0)\right||a_n|$$",,"['real-analysis', 'sequences-and-series', 'derivatives', 'convergence-divergence']"
15,"Prove the inequality $a^{2}+b^{2}+c^{2}+abc(a+b+c) \geq 2(ab+bc+ca)$, given $a^{2}b+b^{2}c+c^{2}a+a^{2}b^{2}c^{2}=4$","Prove the inequality , given",a^{2}+b^{2}+c^{2}+abc(a+b+c) \geq 2(ab+bc+ca) a^{2}b+b^{2}c+c^{2}a+a^{2}b^{2}c^{2}=4,"Let $a, b$ and $c$ be positive real numbers such that $a^{2}b+b^{2}c+c^{2}a+a^{2}b^{2}c^{2}=4$. Prove that \begin{equation} a^{2}+b^{2}+c^{2}+abc(a+b+c) \geq 2(ab+bc+ca) \end{equation} Let us consider that  \begin{equation*} \begin{split} \text{$(a+b+c)^{2}$} &=\text{$(a+b+c)(a+b+c)$} \\ &=\text{$a^{2}+ab+ac+ba+b^{2}+bc+ca+cb+c^{2}$} \\ &=\text{$a^{2}+b^{2}+c^{2}+2ac+2ab+2bc$} \\ &=\text{$a^{2}+b^{2}+c^{2}+2(ab+bc+ca)$} \end{split} \end{equation*}  and consider  \begin{equation*} \begin{split} \text{$a^{2}+b^{2}+c^{2}$} &=\text{$(a+b+c)^{2}-2(ab+bc+ca)$}  \end{split} \end{equation*}  The L.H.S of the inequality (1)  \begin{equation} \begin{split} \text{$a^{2}+b^{2}+c^{2}+abc(a+b+c) $}    &=\text{$(a+b+c)^{2}-2(ab+bc+ca)+abc(a+b+c)$} \\ &=\text{$(a+b+c)^{2}+(a+b+c)abc-2(ab+bc+ca)$} \\ &=\text{$(a+b+c)\bigg[(a+b+c)+abc \bigg]-2(ab+bc+ca)$} \\ \end{split} \end{equation} I have not real proof anything yet. I have through out some work, but I have not think how would I use $a^{2}b+b^{2}c+c^{2}a+a^{2}b^{2}c^{2}=4$. Please If you any idea that would help, please write it down.","Let $a, b$ and $c$ be positive real numbers such that $a^{2}b+b^{2}c+c^{2}a+a^{2}b^{2}c^{2}=4$. Prove that \begin{equation} a^{2}+b^{2}+c^{2}+abc(a+b+c) \geq 2(ab+bc+ca) \end{equation} Let us consider that  \begin{equation*} \begin{split} \text{$(a+b+c)^{2}$} &=\text{$(a+b+c)(a+b+c)$} \\ &=\text{$a^{2}+ab+ac+ba+b^{2}+bc+ca+cb+c^{2}$} \\ &=\text{$a^{2}+b^{2}+c^{2}+2ac+2ab+2bc$} \\ &=\text{$a^{2}+b^{2}+c^{2}+2(ab+bc+ca)$} \end{split} \end{equation*}  and consider  \begin{equation*} \begin{split} \text{$a^{2}+b^{2}+c^{2}$} &=\text{$(a+b+c)^{2}-2(ab+bc+ca)$}  \end{split} \end{equation*}  The L.H.S of the inequality (1)  \begin{equation} \begin{split} \text{$a^{2}+b^{2}+c^{2}+abc(a+b+c) $}    &=\text{$(a+b+c)^{2}-2(ab+bc+ca)+abc(a+b+c)$} \\ &=\text{$(a+b+c)^{2}+(a+b+c)abc-2(ab+bc+ca)$} \\ &=\text{$(a+b+c)\bigg[(a+b+c)+abc \bigg]-2(ab+bc+ca)$} \\ \end{split} \end{equation} I have not real proof anything yet. I have through out some work, but I have not think how would I use $a^{2}b+b^{2}c+c^{2}a+a^{2}b^{2}c^{2}=4$. Please If you any idea that would help, please write it down.",,"['calculus', 'real-analysis', 'inequality', 'contest-math']"
16,Solving the geometric series for q,Solving the geometric series for q,,"Is there a general way to find the $q > 0$ solving the equation from the geometric series $$1+q+q^2+q^3+\ldots + q^n = a$$ or $$\frac{1-q^{n+1}}{1-q} = a\quad\text{with } q \neq 1$$ for $a > 1$ and $n\in\mathbb N$? My thoughts: Since polynomials aren't solvable in general for degree 5 or higher, I guess the above equation has no explicit solution for $n\ge 5$. In this case numerical approximations can be used. For $n=5$ also this method can be used.","Is there a general way to find the $q > 0$ solving the equation from the geometric series $$1+q+q^2+q^3+\ldots + q^n = a$$ or $$\frac{1-q^{n+1}}{1-q} = a\quad\text{with } q \neq 1$$ for $a > 1$ and $n\in\mathbb N$? My thoughts: Since polynomials aren't solvable in general for degree 5 or higher, I guess the above equation has no explicit solution for $n\ge 5$. In this case numerical approximations can be used. For $n=5$ also this method can be used.",,"['calculus', 'real-analysis', 'polynomials', 'roots']"
17,"Showing $\sin^n(x)$ does not converge uniformly on $[0,\pi/2]$",Showing  does not converge uniformly on,"\sin^n(x) [0,\pi/2]","I just want to check that I am correct in my argument that $f_n(x) = \sin^n(x)$ does not converge uniformly. When $x = \pi/2$, $\sin^n(x) = 1$ for all $n$, hence $ f_n(\pi/2) \rightarrow 1$, However, for all other $x$, $f_n(x) \rightarrow 0$. Hence the limit function of a sequence of continuous function is not continuous, and so the convergence is not uniform.","I just want to check that I am correct in my argument that $f_n(x) = \sin^n(x)$ does not converge uniformly. When $x = \pi/2$, $\sin^n(x) = 1$ for all $n$, hence $ f_n(\pi/2) \rightarrow 1$, However, for all other $x$, $f_n(x) \rightarrow 0$. Hence the limit function of a sequence of continuous function is not continuous, and so the convergence is not uniform.",,['real-analysis']
18,"Old & cool integral $\int_0^{\pi} \sin^{b-1}(x) \sin(a x) \ dx=\frac{\pi \sin(a \pi/2)}{2^{b-1}b B\left(\frac{b+a+1}{2},\frac{b-a+1}{2}\right)}$",Old & cool integral,"\int_0^{\pi} \sin^{b-1}(x) \sin(a x) \ dx=\frac{\pi \sin(a \pi/2)}{2^{b-1}b B\left(\frac{b+a+1}{2},\frac{b-a+1}{2}\right)}","Here is an integral that appears in the table of integrals by Gradshtein and Ryzhik, it was also studied by Ramanujan (not sure his original solution was found - it seems it doesn't appear in any of the notebooks). $$\int_0^{\pi} \sin^{b-1}(x) \sin(a x) \ dx=\frac{\pi \sin(a \pi/2)}{2^{b-1}b B\left(\frac{b+a+1}{2},\frac{b-a+1}{2}\right)}$$ Now, by complex analysis, one can brifely finish it, I'm not interested in such a solution . But thinking of Ramanujan I'm sure he had  a solution using methods of real analysis (and to avoid possible misunderstandings, I mean not even a touch on complex numbers - to be clear). Do you know such a solution? Post it only if you want to, I'm only curious if such solutions are known, maybe some simple such solutions? Application of the integral above (supplementary question) Prove that $$\int_0^{\pi/2} \frac{\log (\sin (x))+x \csc ^2(x)-x \cot (x)}{x^2+\log ^2(\sin (x))} \, dx=\text{Si}\left(\frac{\pi }{2}\right),$$ http://mathworld.wolfram.com/SineIntegral.html or simply show that $$\int_0^{\pi/2} \frac{x \cot (x)-\log (\sin (x))}{x^2+\log ^2(\sin (x))} \, dx=\frac{\pi}{2}.$$","Here is an integral that appears in the table of integrals by Gradshtein and Ryzhik, it was also studied by Ramanujan (not sure his original solution was found - it seems it doesn't appear in any of the notebooks). $$\int_0^{\pi} \sin^{b-1}(x) \sin(a x) \ dx=\frac{\pi \sin(a \pi/2)}{2^{b-1}b B\left(\frac{b+a+1}{2},\frac{b-a+1}{2}\right)}$$ Now, by complex analysis, one can brifely finish it, I'm not interested in such a solution . But thinking of Ramanujan I'm sure he had  a solution using methods of real analysis (and to avoid possible misunderstandings, I mean not even a touch on complex numbers - to be clear). Do you know such a solution? Post it only if you want to, I'm only curious if such solutions are known, maybe some simple such solutions? Application of the integral above (supplementary question) Prove that $$\int_0^{\pi/2} \frac{\log (\sin (x))+x \csc ^2(x)-x \cot (x)}{x^2+\log ^2(\sin (x))} \, dx=\text{Si}\left(\frac{\pi }{2}\right),$$ http://mathworld.wolfram.com/SineIntegral.html or simply show that $$\int_0^{\pi/2} \frac{x \cot (x)-\log (\sin (x))}{x^2+\log ^2(\sin (x))} \, dx=\frac{\pi}{2}.$$",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'definite-integrals']"
19,Absolutely continuous function on R,Absolutely continuous function on R,,"What is the definition of absolute continuity in whole  $\mathbb{R}$. I know the definition on an interval $[a, b]$. I have a trouble with understanding the definition of absolute continuity in whole $\mathbb{R}$.","What is the definition of absolute continuity in whole  $\mathbb{R}$. I know the definition on an interval $[a, b]$. I have a trouble with understanding the definition of absolute continuity in whole $\mathbb{R}$.",,"['real-analysis', 'analysis', 'measure-theory', 'definition', 'lebesgue-integral']"
20,$L^q(X) \subset L^p(X)$ if $p\leq q$ and $\mu(X) < \infty$.,if  and .,L^q(X) \subset L^p(X) p\leq q \mu(X) < \infty,"Let $(X,\mathfrak{M},\mu)$ be a measure space. Show that if $\mu(X) < \infty$ then $L^q(X) \subset L^p(X)$ for $1\le p \le q \le \infty$. Is it enough to define $$E_0 = \{x \in X \, : \, 0 \leq |f(x)| < 1\}$$ and $E_1 = X \setminus E_0$ so that $\mu(E_0), \, \mu(E_1) < \infty$ and $E_0$ is measurable as $E_0 = \{x \in X : |f(x)| \geq 0 \} \cap \{x \in X : |f(x)| \leq 1 \}$ and $f$ is measurable. Therefore if $f \in L^q$, \begin{eqnarray*} ||f||^p_{L^p} &=& \int_{E_0}|f|^p \, d\mu + \int_{E_1} |f|^p \, d\mu \\ &\leq& \int_{E_0}|f|^p \, d\mu + \int_{E_1} |f|^q \, d\mu \\ &<& \mu(E_0) \,\, + ||f||_{L^q}^q \\ &<& \infty. \end{eqnarray*} which implies $f \in L^p$. Doe this suffice? I've seen a proof using Holder's inequality (that has now been given as an answer for those interested), that is much shorter, but does this suffice?","Let $(X,\mathfrak{M},\mu)$ be a measure space. Show that if $\mu(X) < \infty$ then $L^q(X) \subset L^p(X)$ for $1\le p \le q \le \infty$. Is it enough to define $$E_0 = \{x \in X \, : \, 0 \leq |f(x)| < 1\}$$ and $E_1 = X \setminus E_0$ so that $\mu(E_0), \, \mu(E_1) < \infty$ and $E_0$ is measurable as $E_0 = \{x \in X : |f(x)| \geq 0 \} \cap \{x \in X : |f(x)| \leq 1 \}$ and $f$ is measurable. Therefore if $f \in L^q$, \begin{eqnarray*} ||f||^p_{L^p} &=& \int_{E_0}|f|^p \, d\mu + \int_{E_1} |f|^p \, d\mu \\ &\leq& \int_{E_0}|f|^p \, d\mu + \int_{E_1} |f|^q \, d\mu \\ &<& \mu(E_0) \,\, + ||f||_{L^q}^q \\ &<& \infty. \end{eqnarray*} which implies $f \in L^p$. Doe this suffice? I've seen a proof using Holder's inequality (that has now been given as an answer for those interested), that is much shorter, but does this suffice?",,"['real-analysis', 'analysis', 'measure-theory', 'proof-verification', 'lp-spaces']"
21,Transexponential Functions,Transexponential Functions,,"Recall that $\exp(1,x) = e^x$ and $\exp(n+1,x) = e^{\exp(n,x)}$. Recall that $f(x)$ is transexponential if $f(x)$ is eventually greater than $\exp(n,x)$ $\forall n \in \mathbb{N}$ I am looking for a (general) reference on these types of functions (or any paper about these functions, or maybe even a few pages of a textbook). Note: I have tagged model theory (and now logic) since the only context in which I have encountered transexponential functions is in relation to Wilkie's Conjecture (and so model theorists know about these functions). Please note that I am looking for a reference about transexponential function in general, and not a link to an exposition of Wilkie's Conjecture. Note 2: I have added a bounty to this question. I am trying to get my hands dirty with transexponential functions from $\mathbb{R}^+ \to \mathbb{R}^+$. The most helpful answer would be one where I could ""in some sense"" compute the derivative (locally). Please do not answer with ""piecewise continuous segments"" + bump functions.","Recall that $\exp(1,x) = e^x$ and $\exp(n+1,x) = e^{\exp(n,x)}$. Recall that $f(x)$ is transexponential if $f(x)$ is eventually greater than $\exp(n,x)$ $\forall n \in \mathbb{N}$ I am looking for a (general) reference on these types of functions (or any paper about these functions, or maybe even a few pages of a textbook). Note: I have tagged model theory (and now logic) since the only context in which I have encountered transexponential functions is in relation to Wilkie's Conjecture (and so model theorists know about these functions). Please note that I am looking for a reference about transexponential function in general, and not a link to an exposition of Wilkie's Conjecture. Note 2: I have added a bounty to this question. I am trying to get my hands dirty with transexponential functions from $\mathbb{R}^+ \to \mathbb{R}^+$. The most helpful answer would be one where I could ""in some sense"" compute the derivative (locally). Please do not answer with ""piecewise continuous segments"" + bump functions.",,"['real-analysis', 'reference-request', 'logic', 'model-theory']"
22,"Show $\lim_{m \to \infty ,n \to \infty } f(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}) = f(x,y)$",Show,"\lim_{m \to \infty ,n \to \infty } f(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}) = f(x,y)","Suppose $f(x,y)$ is defined on $[0,1]\times[0,1]$ and continuous on each dimension, i.e. $f(x,y_0)$ is continuous with respect to $x$ when fixing $y=y_0\in [0,1]$ and $f(x_0,y)$ is continuous with respect to $y$ when fixing $x=x_0\in [0,1]$ . Show $$\lim_{m \to \infty ,n \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = f(x,y)$$ My attempt: First, I know $$\lim\limits_{m \to \infty ,n \to \infty } \left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = (x,y)$$ Secondly it looks $$\lim\limits_{m \to \infty }\lim\limits_{n \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = \lim \limits_{m \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},y\right) =  f(x,y)$$ and $$\lim\limits_{n \to \infty } \lim\limits_{m \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = \lim\limits_{n \to \infty } f\left(x,\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = f(x,y)$$ since $f(x,y)$ is continuous on each dimension. However, I am not sure if this can infer $\lim\limits_{m \to \infty ,n \to \infty } f(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}) = f(x,y)$ . Can anyone provide some help? Thank you! Added: I am now sure $\lim\limits_{m \to \infty } \lim\limits_{n \to \infty } {a_{mn}} = \lim\limits_{n \to \infty } \lim\limits_{m \to \infty } {a_{mn}} = L$ does not imply $\lim\limits_{m \to \infty ,n \to \infty } {a_{mn}} =L$ in general. Hope someone can help solve the problem.","Suppose is defined on and continuous on each dimension, i.e. is continuous with respect to when fixing and is continuous with respect to when fixing . Show My attempt: First, I know Secondly it looks and since is continuous on each dimension. However, I am not sure if this can infer . Can anyone provide some help? Thank you! Added: I am now sure does not imply in general. Hope someone can help solve the problem.","f(x,y) [0,1]\times[0,1] f(x,y_0) x y=y_0\in [0,1] f(x_0,y) y x=x_0\in [0,1] \lim_{m \to \infty ,n \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = f(x,y) \lim\limits_{m \to \infty ,n \to \infty } \left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = (x,y) \lim\limits_{m \to \infty }\lim\limits_{n \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = \lim \limits_{m \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},y\right) =  f(x,y) \lim\limits_{n \to \infty } \lim\limits_{m \to \infty } f\left(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = \lim\limits_{n \to \infty } f\left(x,\frac{{\left\lfloor {ny} \right\rfloor }}{n}\right) = f(x,y) f(x,y) \lim\limits_{m \to \infty ,n \to \infty } f(\frac{{\left\lfloor {mx} \right\rfloor }}{m},\frac{{\left\lfloor {ny} \right\rfloor }}{n}) = f(x,y) \lim\limits_{m \to \infty } \lim\limits_{n \to \infty } {a_{mn}} = \lim\limits_{n \to \infty } \lim\limits_{m \to \infty } {a_{mn}} = L \lim\limits_{m \to \infty ,n \to \infty } {a_{mn}} =L","['real-analysis', 'limits', 'continuity']"
23,Semantics of Writing Differential Equations,Semantics of Writing Differential Equations,,"Let $f : \mathbb R \to \mathbb R$, and consider the differential equation $$  f'(t) = f(t) $$ it is easily seen that it has the solutions $f(t) = a\cdot \exp(t)$ for $a \in \mathbb R$. Now another way of writing this $$  \frac{df}{dt} = f $$ (here the argument isn't written anymore, an abstraction step leading to the next notation). Seeing $\frac{df}{dt}$ as a shorthand for $f'(t)$ this is no problem, but I have seen the notation $$  \frac{df}{f} = dt. $$ Suddenly we have two new quantities and $df/dt$ gets handled like a fraction (not just merely a notation for $f'$ anymore), I know this is the Leibniz way of thinking about differentials, and I knew it for example as a shorthand from the substitution rule for integrals. I know how to think about such terms, $dt$ should be interpreted as $t_{i+1} - t_i$ for two time steps or $(t + h) - t$ as the time steps get finer, or $h \to 0$, so $dt \approx h$, similarly $df$ could be understood as $f(t + h) - f(t)$ in the limit, so $df \approx f(t+h) - f(t)$ for $h$ sufficiently small at $t$. But how should $df / f = dt$ be read, as ""the proportion of the change in quantity to the total quantity equals the change in time, i.e. for constant time changes it is constant too, which implies that if the quantity increases, the rate of change has to increase too."" But with substituting $$  df \approx (\exp(t + h) - \exp(t)) \mbox{ and }  dt \approx (t + h) - t $$ we should have $$  \frac{\exp(t + h) - \exp(t)}{\exp(t+h)} = h $$ as $\exp(t+h) - \exp(t) \approx \exp(t+h)h$ (i.e. $df = fdt$) which is not valid, because $(\exp(t + h) - \exp(t))/\exp(t+h) = 1 - \exp(-h) \ne h$? So where is the problem, why does this interpretation does not work?","Let $f : \mathbb R \to \mathbb R$, and consider the differential equation $$  f'(t) = f(t) $$ it is easily seen that it has the solutions $f(t) = a\cdot \exp(t)$ for $a \in \mathbb R$. Now another way of writing this $$  \frac{df}{dt} = f $$ (here the argument isn't written anymore, an abstraction step leading to the next notation). Seeing $\frac{df}{dt}$ as a shorthand for $f'(t)$ this is no problem, but I have seen the notation $$  \frac{df}{f} = dt. $$ Suddenly we have two new quantities and $df/dt$ gets handled like a fraction (not just merely a notation for $f'$ anymore), I know this is the Leibniz way of thinking about differentials, and I knew it for example as a shorthand from the substitution rule for integrals. I know how to think about such terms, $dt$ should be interpreted as $t_{i+1} - t_i$ for two time steps or $(t + h) - t$ as the time steps get finer, or $h \to 0$, so $dt \approx h$, similarly $df$ could be understood as $f(t + h) - f(t)$ in the limit, so $df \approx f(t+h) - f(t)$ for $h$ sufficiently small at $t$. But how should $df / f = dt$ be read, as ""the proportion of the change in quantity to the total quantity equals the change in time, i.e. for constant time changes it is constant too, which implies that if the quantity increases, the rate of change has to increase too."" But with substituting $$  df \approx (\exp(t + h) - \exp(t)) \mbox{ and }  dt \approx (t + h) - t $$ we should have $$  \frac{\exp(t + h) - \exp(t)}{\exp(t+h)} = h $$ as $\exp(t+h) - \exp(t) \approx \exp(t+h)h$ (i.e. $df = fdt$) which is not valid, because $(\exp(t + h) - \exp(t))/\exp(t+h) = 1 - \exp(-h) \ne h$? So where is the problem, why does this interpretation does not work?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'derivatives']"
24,Continuity of normalized displacement vector for a smooth closed curve,Continuity of normalized displacement vector for a smooth closed curve,,"I am currently working on chapter 3.12 of ""Differential Equations and Dynamical Systems"" by Lawrence Perko. I am stuck on the continuity of the function $g$ in Theorem 3. My work (up to the prove of continuity based on Perko with slightly different notation) so far: Suppose that $f\in C^1(E)$ where $E$ is an open subset of $\mathbb{R}^2$ and that $E$ contains a cycle $\Gamma$ of the system $\dot{x}=f(x) \quad(1)$. At any point $x\in\Gamma$, define the unit vector $u(x)=f(x)/|f(x)|$. We can assume that $\Gamma$ lies in the first quadrant and is tangent to the $x$-axis in some point $(x_0,0),x_0>0$ (otherwise translate and rotate the axis). Let $\gamma(t)=(x(t),y(t))$ be the solution of $(1)$ through the point $(x_0,0)$ at time $t=0$. We can assume that $\Gamma$ is positively oriented and that the period of $\Gamma$ is equal to $1$, so \begin{align*}\Gamma=\{\gamma(t)=(x(t),y(t))\in\mathbb{R}^2~|~0\leq t\leq 1\}.\end{align*} Let $T=\{(s,t)\in\mathbb{R}^2~|~0\leq s\leq t\leq 1\}$, then for $(s,t)\in T$ we define the vector field $g$ by \begin{align*}&g(s,s)=u(\gamma(s)), & 0\leq s\leq 1 \\ &g(0,1)=-u(\gamma(0)), & \\ &g(s,t)=\frac{\gamma(t)-\gamma(s)}{|\gamma(t)-\gamma(s)|}, & 0\leq s < t \leq 1\end{align*} Now Perko states, that $g$ is continuous on $T$ and $g\neq 0$ for on $T$. It is clear that $g$ is continuous on $\tilde{T}:=T\setminus\{(s,t)\in T~|~s=t \vee (s,t)=(0,1)\}$. Let $(s_n,t_n)\rightarrow (s,s),0\leq s\leq 1$. One can show that \begin{align*}g(s_n,t_n)=\frac{\gamma(t_n)-\gamma(s_n)}{|\gamma(t_n)-\gamma(s_n)|}=\frac{\frac{\gamma(t_n)-\gamma(s_n)}{t_n-s_n}}{\frac{|\gamma(t_n)-\gamma(s_n)|}{t_n-s_n}}\stackrel{s_n<t_n}{=}\frac{\frac{\gamma(t_n)-\gamma(s_n)}{t_n-s_n}}{\left|\frac{\gamma(t_n)-\gamma(s_n)}{t_n-s_n}\right|}\stackrel{n\to\infty}{\rightarrow} \frac{\dot{\gamma(s)}}{|\dot{\gamma(s)}|}\stackrel{\dot{\gamma}=f(\gamma)}{=}u(\gamma(s)).\end{align*} I now have problems to prove the continuity of $g$ in $(0,1)$. I have uploaded a pdf with figures of the problem (is it possible to attach it directly to this post?), just by ""looking"" one would say that $g(s,t)\rightarrow -u(\gamma(0))$ as $(s,t)\rightarrow (0,1)$, but I don't find a way to really prove it. Any input would be appreciated!","I am currently working on chapter 3.12 of ""Differential Equations and Dynamical Systems"" by Lawrence Perko. I am stuck on the continuity of the function $g$ in Theorem 3. My work (up to the prove of continuity based on Perko with slightly different notation) so far: Suppose that $f\in C^1(E)$ where $E$ is an open subset of $\mathbb{R}^2$ and that $E$ contains a cycle $\Gamma$ of the system $\dot{x}=f(x) \quad(1)$. At any point $x\in\Gamma$, define the unit vector $u(x)=f(x)/|f(x)|$. We can assume that $\Gamma$ lies in the first quadrant and is tangent to the $x$-axis in some point $(x_0,0),x_0>0$ (otherwise translate and rotate the axis). Let $\gamma(t)=(x(t),y(t))$ be the solution of $(1)$ through the point $(x_0,0)$ at time $t=0$. We can assume that $\Gamma$ is positively oriented and that the period of $\Gamma$ is equal to $1$, so \begin{align*}\Gamma=\{\gamma(t)=(x(t),y(t))\in\mathbb{R}^2~|~0\leq t\leq 1\}.\end{align*} Let $T=\{(s,t)\in\mathbb{R}^2~|~0\leq s\leq t\leq 1\}$, then for $(s,t)\in T$ we define the vector field $g$ by \begin{align*}&g(s,s)=u(\gamma(s)), & 0\leq s\leq 1 \\ &g(0,1)=-u(\gamma(0)), & \\ &g(s,t)=\frac{\gamma(t)-\gamma(s)}{|\gamma(t)-\gamma(s)|}, & 0\leq s < t \leq 1\end{align*} Now Perko states, that $g$ is continuous on $T$ and $g\neq 0$ for on $T$. It is clear that $g$ is continuous on $\tilde{T}:=T\setminus\{(s,t)\in T~|~s=t \vee (s,t)=(0,1)\}$. Let $(s_n,t_n)\rightarrow (s,s),0\leq s\leq 1$. One can show that \begin{align*}g(s_n,t_n)=\frac{\gamma(t_n)-\gamma(s_n)}{|\gamma(t_n)-\gamma(s_n)|}=\frac{\frac{\gamma(t_n)-\gamma(s_n)}{t_n-s_n}}{\frac{|\gamma(t_n)-\gamma(s_n)|}{t_n-s_n}}\stackrel{s_n<t_n}{=}\frac{\frac{\gamma(t_n)-\gamma(s_n)}{t_n-s_n}}{\left|\frac{\gamma(t_n)-\gamma(s_n)}{t_n-s_n}\right|}\stackrel{n\to\infty}{\rightarrow} \frac{\dot{\gamma(s)}}{|\dot{\gamma(s)}|}\stackrel{\dot{\gamma}=f(\gamma)}{=}u(\gamma(s)).\end{align*} I now have problems to prove the continuity of $g$ in $(0,1)$. I have uploaded a pdf with figures of the problem (is it possible to attach it directly to this post?), just by ""looking"" one would say that $g(s,t)\rightarrow -u(\gamma(0))$ as $(s,t)\rightarrow (0,1)$, but I don't find a way to really prove it. Any input would be appreciated!",,"['real-analysis', 'multivariable-calculus', 'continuity']"
25,"Real analysis ""theory book"" similar to Andreescu's Problems in Real Analysis: Advanced Calculus on the Real Axis","Real analysis ""theory book"" similar to Andreescu's Problems in Real Analysis: Advanced Calculus on the Real Axis",,"I am going through Andreescu et al. , Problems in Real Analysis: Advanced Calculus on the Real Axis and I am very impressed: the style of the book seems really modern and the material covered includes many theorems, examples, etc. that are rarely or not at all seen in other books (both theory books and problem books) and that are often taken from not well-known sources or from recent issues of math journals. I was wondering if there is a "" theory book "" that resembles the one I   mentioned (and roughly covers the same material)?","I am going through Andreescu et al. , Problems in Real Analysis: Advanced Calculus on the Real Axis and I am very impressed: the style of the book seems really modern and the material covered includes many theorems, examples, etc. that are rarely or not at all seen in other books (both theory books and problem books) and that are often taken from not well-known sources or from recent issues of math journals. I was wondering if there is a "" theory book "" that resembles the one I   mentioned (and roughly covers the same material)?",,"['calculus', 'real-analysis', 'reference-request', 'soft-question', 'book-recommendation']"
26,Arnold ODE Problem,Arnold ODE Problem,,"Problem 1 of Section 1.2.4 of Arnold's ODE book asks Can the integral curves of a smooth (continuously differentiable) equation $\frac{dx}{dt} = v(x)$ approach each other faster than exponentially as $t\rightarrow \infty$ ? It says that the answer is no when one of the curves corresponds to an equilibrium position but is yes otherwise. I interpret this to mean that for two integral curves $x_1, x_2$ defined for all values of $t$ larger than some constant, $\lim_{t\rightarrow \infty} e^t |x_1(t) - x_2(t)| = 0$ . But I can't think of an example of a ODE with solutions like that.  I tried the following ODEs $dx/dt = 1$ .  This had solutions that maintained constant distance from each other $dx/dt = x^2$ . The solutions to this ODE approach 0 for t large but their difference is on the order of $1/t^2$ . $dx/dt = -e^t$ .","Problem 1 of Section 1.2.4 of Arnold's ODE book asks Can the integral curves of a smooth (continuously differentiable) equation approach each other faster than exponentially as ? It says that the answer is no when one of the curves corresponds to an equilibrium position but is yes otherwise. I interpret this to mean that for two integral curves defined for all values of larger than some constant, . But I can't think of an example of a ODE with solutions like that.  I tried the following ODEs .  This had solutions that maintained constant distance from each other . The solutions to this ODE approach 0 for t large but their difference is on the order of . .","\frac{dx}{dt} = v(x) t\rightarrow \infty x_1, x_2 t \lim_{t\rightarrow \infty} e^t |x_1(t) - x_2(t)| = 0 dx/dt = 1 dx/dt = x^2 1/t^2 dx/dt = -e^t","['real-analysis', 'ordinary-differential-equations']"
27,Sets of measure zero and the Lebesgue differentiation theorem.,Sets of measure zero and the Lebesgue differentiation theorem.,,"This is an exercise from Stein-Sharachi Chap. 3 Exx. 25 $\textbf{Problem Statement}$ Let $E$ be a set of measure zero in $\mathbb{R}^d$. Show that there exists a non-negative integrable $f$ in $\mathbb{R}^d$, so that $$\liminf_{m(B) \to 0} \frac{1}{m(B)} \int_{B} f(y) dy = \infty$$ for each $x \in E$ and $B$ is any ball containing $x$. $\textbf{My Attempt:}$. From the hint, let $f(x) = \sum_{n=1}^{\infty} \chi_{\mathcal{O}_n} (x)$, where $\mathcal{O}_n \supset E$ and $m ( \mathcal{O}_n) < \frac{1}{2^n}$. We can do this since $E$ has measure zero. Then $$\int_{\mathbb{R}^d} f(x) dx = \sum_{n=1}^{\infty} \int \chi_{\mathcal{O}_n}(x) = 1$$ and so $f$ is integrable. I know  have left out some details but the above is not where I am having difficulties. Let $B$ be any ball. From above we have that $E \subset \mathcal{O}_n$. Thus for any $x \in E$, there exists a ball $B_n \subset \mathcal{O}_n$ such that $x \in B_n$. $$ \int_B f(y) dy = \sum_{n=1}^{\infty} m ( \mathcal{O}_n \cap B) \ge \sum_{n=1}^{\infty} m ( B_n \cap B)$$ Then $$\frac{1}{m(B)} \int_B f(y) dy \ge \sum_{n=1}^{\infty} \frac{m(B_n \cap B)}{m(B)}$$ Since $x \in B_n$ is a point of Lebesgue density I have that $$\lim_{m(B) \to 0} \frac{m(B_n \cap B)}{m(B)} =1$$ This seems useful since then, $$ \sum_{n=1}^{\infty} \lim_{m(B) \to 0} \frac{m(B_n \cap B)}{m(B)} = \infty$$ and I would have my result. However, how does $\liminf$ ever come in to play? I really do not understand the significance. And how do I back up moving the limit inside of the summation?","This is an exercise from Stein-Sharachi Chap. 3 Exx. 25 $\textbf{Problem Statement}$ Let $E$ be a set of measure zero in $\mathbb{R}^d$. Show that there exists a non-negative integrable $f$ in $\mathbb{R}^d$, so that $$\liminf_{m(B) \to 0} \frac{1}{m(B)} \int_{B} f(y) dy = \infty$$ for each $x \in E$ and $B$ is any ball containing $x$. $\textbf{My Attempt:}$. From the hint, let $f(x) = \sum_{n=1}^{\infty} \chi_{\mathcal{O}_n} (x)$, where $\mathcal{O}_n \supset E$ and $m ( \mathcal{O}_n) < \frac{1}{2^n}$. We can do this since $E$ has measure zero. Then $$\int_{\mathbb{R}^d} f(x) dx = \sum_{n=1}^{\infty} \int \chi_{\mathcal{O}_n}(x) = 1$$ and so $f$ is integrable. I know  have left out some details but the above is not where I am having difficulties. Let $B$ be any ball. From above we have that $E \subset \mathcal{O}_n$. Thus for any $x \in E$, there exists a ball $B_n \subset \mathcal{O}_n$ such that $x \in B_n$. $$ \int_B f(y) dy = \sum_{n=1}^{\infty} m ( \mathcal{O}_n \cap B) \ge \sum_{n=1}^{\infty} m ( B_n \cap B)$$ Then $$\frac{1}{m(B)} \int_B f(y) dy \ge \sum_{n=1}^{\infty} \frac{m(B_n \cap B)}{m(B)}$$ Since $x \in B_n$ is a point of Lebesgue density I have that $$\lim_{m(B) \to 0} \frac{m(B_n \cap B)}{m(B)} =1$$ This seems useful since then, $$ \sum_{n=1}^{\infty} \lim_{m(B) \to 0} \frac{m(B_n \cap B)}{m(B)} = \infty$$ and I would have my result. However, how does $\liminf$ ever come in to play? I really do not understand the significance. And how do I back up moving the limit inside of the summation?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
28,"$\forall x \,\exists k$ s.t. $f^{(k)}(x)=0$, then $f$ is a polynomial","s.t. , then  is a polynomial","\forall x \,\exists k f^{(k)}(x)=0 f","My friend sent me the following problem: Suppose that $f$ is real analytic on $(a,b)$, and that for all $x$ in   $(a,b)$ there exists a non-negative integer $k$ such that   $f^{(k)}(x)=0$.  Show that $f$ is a polynomial. I believe I solved it (you can read my answer below the fold if you are interested). Then my friend posed the question of what happens if $f$ is only $C^{\infty}$. I believe my argument below has shown that $\{x: \exists \,\,\text{a nbd of } x\,\,\text{on which }f \,\,\text{is a polynomial}\}$ is dense in $(a,b)$. But I can't  seem to show that that implies $f$ is a polynomial. Can someone think of a counter-example, or finish the proof? Proposition: Suppose that $f$ is real analytic on $(a,b)$, and that for all $x$ in $(a,b)$ there exists a non-negative integer $k$ such that $f^{(k)}(x)=0$.  Then $f$ is a polynomial. Proof: If $f$ agrees with a polynomial $p$ on an open interval, then the Taylor expansion at any point in that interval is finite, and so has an infinite radius of convergence. Properties of analytic functions then imply that $f=p$ on $(a,b)$. So suppose for contradiction that $f$ does not agree with a polynomial on an open interval. For all $x\in (a,b)$, define $n_x$ to be the smallest integer $k$ for which $f^{(k)}(x)=0$. Let $x\in (a,b)$ be arbitrary. Then by continuity of all derivatives, there exists a neighborhood $(a_1,b_1)\subseteq (a,b)$ such that $n_y\geq n_x$ for all $y\in (a_1,b_1)$. Now if $n_y=n_x$ for all $y\in (a_1,b_1)$, then $f$ agrees with a polynomial on that interval, a contradiction. So there exists $x_1\in (a_1,b_1)$ such that $n_{x_1}>n_x$. Take $[\alpha_1, \beta_1]\subset (a_1,b_1)$ such that $x_1\in [\alpha_1,\beta_1]$. Now repeat the process, starting with $x_1$, to generate $x_2 \in [\alpha_2,\beta_2]$ such that $n_{x_2}>n_{x_1}$. The nested interval theorem now implies that there exists a point $x^* \in \cap_{i}[\alpha_i,\beta_i]$, and since $n_{x_i}\xrightarrow{i\to\infty} \infty$, $f^{k}(x^*) \neq 0\,\,\forall k$, a contradiction. $\blacksquare$","My friend sent me the following problem: Suppose that $f$ is real analytic on $(a,b)$, and that for all $x$ in   $(a,b)$ there exists a non-negative integer $k$ such that   $f^{(k)}(x)=0$.  Show that $f$ is a polynomial. I believe I solved it (you can read my answer below the fold if you are interested). Then my friend posed the question of what happens if $f$ is only $C^{\infty}$. I believe my argument below has shown that $\{x: \exists \,\,\text{a nbd of } x\,\,\text{on which }f \,\,\text{is a polynomial}\}$ is dense in $(a,b)$. But I can't  seem to show that that implies $f$ is a polynomial. Can someone think of a counter-example, or finish the proof? Proposition: Suppose that $f$ is real analytic on $(a,b)$, and that for all $x$ in $(a,b)$ there exists a non-negative integer $k$ such that $f^{(k)}(x)=0$.  Then $f$ is a polynomial. Proof: If $f$ agrees with a polynomial $p$ on an open interval, then the Taylor expansion at any point in that interval is finite, and so has an infinite radius of convergence. Properties of analytic functions then imply that $f=p$ on $(a,b)$. So suppose for contradiction that $f$ does not agree with a polynomial on an open interval. For all $x\in (a,b)$, define $n_x$ to be the smallest integer $k$ for which $f^{(k)}(x)=0$. Let $x\in (a,b)$ be arbitrary. Then by continuity of all derivatives, there exists a neighborhood $(a_1,b_1)\subseteq (a,b)$ such that $n_y\geq n_x$ for all $y\in (a_1,b_1)$. Now if $n_y=n_x$ for all $y\in (a_1,b_1)$, then $f$ agrees with a polynomial on that interval, a contradiction. So there exists $x_1\in (a_1,b_1)$ such that $n_{x_1}>n_x$. Take $[\alpha_1, \beta_1]\subset (a_1,b_1)$ such that $x_1\in [\alpha_1,\beta_1]$. Now repeat the process, starting with $x_1$, to generate $x_2 \in [\alpha_2,\beta_2]$ such that $n_{x_2}>n_{x_1}$. The nested interval theorem now implies that there exists a point $x^* \in \cap_{i}[\alpha_i,\beta_i]$, and since $n_{x_i}\xrightarrow{i\to\infty} \infty$, $f^{k}(x^*) \neq 0\,\,\forall k$, a contradiction. $\blacksquare$",,"['real-analysis', 'polynomials', 'analyticity']"
29,ODE with singular coefficients,ODE with singular coefficients,,"I started with an ODE (first ODE) : $-(1-x^2)y''(x) +x y'(x) -  q(x) y(x) = \lambda y(x).$ Then I got a more sophisticated differential equation ( second one)  and is given by $$-(1-x^2)y''(x) +x y'(x) -  q(x)y(x) + \frac{1}{(1-x^2)}y(x) = \lambda y(x).$$  What I want is to understand the following: Given a solution of equation (1), how can I generalize this one to a solution of equation (2).","I started with an ODE (first ODE) : $-(1-x^2)y''(x) +x y'(x) -  q(x) y(x) = \lambda y(x).$ Then I got a more sophisticated differential equation ( second one)  and is given by $$-(1-x^2)y''(x) +x y'(x) -  q(x)y(x) + \frac{1}{(1-x^2)}y(x) = \lambda y(x).$$  What I want is to understand the following: Given a solution of equation (1), how can I generalize this one to a solution of equation (2).",,"['real-analysis', 'analysis']"
30,Evaluating $\sum_{n=0}^{\infty}\ \int_{\pi/4}^{\pi/3}\sin^{n}x (1-\sin x)^2 dx$ using a convergence theorem,Evaluating  using a convergence theorem,\sum_{n=0}^{\infty}\ \int_{\pi/4}^{\pi/3}\sin^{n}x (1-\sin x)^2 dx,"$$\sum_{n=0}^{\infty}\ \int_{\pi/4}^{\pi/3}\sin^{n}x (1-\sin x)^2 dx$$ Let $g_n = \sin^{n}x (1-\sin x)^2$ $g_n$ is a sequence of measurable functions and $g_n \ge 0$ so applying the Beppo Levi Theorem we get - $$= \int_{\pi/4}^{\pi/3}\sum_{n=0}^{\infty}\sin^{n}x (1-\sin x)^2 dx$$ $$= \int_{\pi/4}^{\pi/3}(1-\sin x)^2 \sum_{n=0}^{\infty}\sin^{n}x dx$$ Now $\sin^{n}x < 1$ for $x \in (\pi/4, \pi/3)$ so we have a geometric series and hence $$\sum_{n=0}^{\infty}\sin^{n}x = \frac{1}{1 - \sin x}$$ This gives us - $$= \int_{\pi/4}^{\pi/3}(1-\sin x)^2 \frac{1}{1 - \sin x} dx$$ $$= \int_{\pi/4}^{\pi/3}(1-\sin x)dx$$ $$= x + \cos  x \mid_{\pi/4}^{\pi/3}$$ $$\frac{\pi}{3} + \cos \frac{\pi}{3} - \frac{\pi}{4} - \cos \frac{\pi}{4}$$ $$\frac{\pi}{12} + \frac{1}{2} - \frac{1}{\sqrt{2}}$$ Is this correct? In particular, have I got the correct conditions to apply the Beppo Levi Theorem?","$$\sum_{n=0}^{\infty}\ \int_{\pi/4}^{\pi/3}\sin^{n}x (1-\sin x)^2 dx$$ Let $g_n = \sin^{n}x (1-\sin x)^2$ $g_n$ is a sequence of measurable functions and $g_n \ge 0$ so applying the Beppo Levi Theorem we get - $$= \int_{\pi/4}^{\pi/3}\sum_{n=0}^{\infty}\sin^{n}x (1-\sin x)^2 dx$$ $$= \int_{\pi/4}^{\pi/3}(1-\sin x)^2 \sum_{n=0}^{\infty}\sin^{n}x dx$$ Now $\sin^{n}x < 1$ for $x \in (\pi/4, \pi/3)$ so we have a geometric series and hence $$\sum_{n=0}^{\infty}\sin^{n}x = \frac{1}{1 - \sin x}$$ This gives us - $$= \int_{\pi/4}^{\pi/3}(1-\sin x)^2 \frac{1}{1 - \sin x} dx$$ $$= \int_{\pi/4}^{\pi/3}(1-\sin x)dx$$ $$= x + \cos  x \mid_{\pi/4}^{\pi/3}$$ $$\frac{\pi}{3} + \cos \frac{\pi}{3} - \frac{\pi}{4} - \cos \frac{\pi}{4}$$ $$\frac{\pi}{12} + \frac{1}{2} - \frac{1}{\sqrt{2}}$$ Is this correct? In particular, have I got the correct conditions to apply the Beppo Levi Theorem?",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral']"
31,Question on questions in Spivak's Calculus?,Question on questions in Spivak's Calculus?,,"I started reading Spivak's Calculus about a month ago and I'm at the end of chapter two, so this is not really calculus yet. However, I find the problems really difficult and the answer keys are not very helpful. I can answer some problems but a lot of them, I've no idea what I should even start with. Also, it takes me a very long time to do each problem. (10-15 minutes on average). I've no prior experience to writing proofs, but I've watched a few youtube videos on it. The problem is generally not the proofs problems but the one where they say ""Find a formula"" or ""derive this equation"". Those, i can never do. Is there some prior knowledge that I should know before reading this besides knowing how to do proofs? Or, are these problems generally difficult to begin with? Or, am I not paying attention to what's written in the chapter? Also, should I look at/try every single problem are is doing like the first page of them sufficient? I had plan to do every problem in the first chapter but called it quits after they started putting epsilons everywhere. Thanks.","I started reading Spivak's Calculus about a month ago and I'm at the end of chapter two, so this is not really calculus yet. However, I find the problems really difficult and the answer keys are not very helpful. I can answer some problems but a lot of them, I've no idea what I should even start with. Also, it takes me a very long time to do each problem. (10-15 minutes on average). I've no prior experience to writing proofs, but I've watched a few youtube videos on it. The problem is generally not the proofs problems but the one where they say ""Find a formula"" or ""derive this equation"". Those, i can never do. Is there some prior knowledge that I should know before reading this besides knowing how to do proofs? Or, are these problems generally difficult to begin with? Or, am I not paying attention to what's written in the chapter? Also, should I look at/try every single problem are is doing like the first page of them sufficient? I had plan to do every problem in the first chapter but called it quits after they started putting epsilons everywhere. Thanks.",,"['real-analysis', 'soft-question']"
32,What is this supremum,What is this supremum,,"For any $10$ points in the unit circle, what is the value of the supremum of the sum of the pairwise distances between the $10$ points, in which the supremum is taken over all configurations of 10 points?","For any $10$ points in the unit circle, what is the value of the supremum of the sum of the pairwise distances between the $10$ points, in which the supremum is taken over all configurations of 10 points?",,"['calculus', 'real-analysis', 'combinatorics', 'geometry']"
33,A property of real sequences,A property of real sequences,,"Show that there is no sequence $x_1, x_2, x_3, ....$  of numbers in $[0,1]$ such that for any $1 \leq k \leq n$ there is $1 \leq i \leq n$ for which $x_i \in [\frac{k-1}n, \frac kn]$. COMMENT: The problem asks for a (infinite) sequence such that the first two numbers must be in different halves, The first three numbers must be in different thirds, the first four numbers must be in different fourths and so on. It is known that no sequence of length more that 17 can satisfy these conditions !!  See here . However, the proof is messy. I was hoping that showing that there is no infinite sequence with this property is much easier. And that's why I am posting this question, to see if there is any simpler proof for the infinite case. If you have (an elementary) solution please let me know.","Show that there is no sequence $x_1, x_2, x_3, ....$  of numbers in $[0,1]$ such that for any $1 \leq k \leq n$ there is $1 \leq i \leq n$ for which $x_i \in [\frac{k-1}n, \frac kn]$. COMMENT: The problem asks for a (infinite) sequence such that the first two numbers must be in different halves, The first three numbers must be in different thirds, the first four numbers must be in different fourths and so on. It is known that no sequence of length more that 17 can satisfy these conditions !!  See here . However, the proof is messy. I was hoping that showing that there is no infinite sequence with this property is much easier. And that's why I am posting this question, to see if there is any simpler proof for the infinite case. If you have (an elementary) solution please let me know.",,"['real-analysis', 'sequences-and-series']"
34,Calculus on the Sobolev space valued function of one real variable $t$?,Calculus on the Sobolev space valued function of one real variable ?,t,"Now I am interested in the calculus on Banach space valued function, especially the function with value in a certain Sobolev space. I want to prove that $$\bigcap_{k=0}^m C^k([0,T];H^{m-k}(\Omega))\subset C^{m-[\frac{n}{2}]-1}(\overline{Q_T}),\tag{1}$$by Sobolev imbedding  theorem. Here $\Omega\subset\mathbb{R}^n$ and $Q_T:=(0,T)\times\Omega$. Since I'm not familiar with the theory of Banach space valued function (only know some basic concepts), I wish to see the detail proof of $(1)$. Any reference which contain the detail proof of $(1)$ is exceedingly welcome! Any answer and reference will be appreciated!","Now I am interested in the calculus on Banach space valued function, especially the function with value in a certain Sobolev space. I want to prove that $$\bigcap_{k=0}^m C^k([0,T];H^{m-k}(\Omega))\subset C^{m-[\frac{n}{2}]-1}(\overline{Q_T}),\tag{1}$$by Sobolev imbedding  theorem. Here $\Omega\subset\mathbb{R}^n$ and $Q_T:=(0,T)\times\Omega$. Since I'm not familiar with the theory of Banach space valued function (only know some basic concepts), I wish to see the detail proof of $(1)$. Any reference which contain the detail proof of $(1)$ is exceedingly welcome! Any answer and reference will be appreciated!",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
35,Killing functions by successive differentiation.,Killing functions by successive differentiation.,,"It is clear that a polynomial function $f(x) = a_0 + a_1 x + \dots + a_n x^n$ has the property that some derivative of $f$ vanishes. (Of course, it's the $(n+1)$-th derivative.) One can also check that the reverse implication is true: if $f^{(n+1)}(x) = 0$ (identically), then $f$ has to be a polynomial of degree at most $n$. (There is even a stronger fact, namely if for any $x$ there is $n$ with $f^{(n)}=0$, then $f$ is a polynomial, but that's beside the point here.) If one takes a polynomial-logarithmic function $f$ made up of terms like $x^a \log^b x$, where $a,b$ may be taken rational, then the similar thing remains true: there is $n$ such that $f^{(n+1)}(x) \to 0$ as $x \to \infty$. Such functions have polynomial growth: they are bounded by $x^N$ for some $N$ (and for sufficiently large $N$). I would like to know to what extent the reverse is true: Suppose that $f$ is defined on $\mathbb{R}_+$ and smooth, and that for some $n$, $f^{(n+1)}(x) \to 0$ as $x \to \infty$. Is it true that $f(x)$ is bounded by $C x^N$ for some $N$, $C$? Can one take $N=n$?","It is clear that a polynomial function $f(x) = a_0 + a_1 x + \dots + a_n x^n$ has the property that some derivative of $f$ vanishes. (Of course, it's the $(n+1)$-th derivative.) One can also check that the reverse implication is true: if $f^{(n+1)}(x) = 0$ (identically), then $f$ has to be a polynomial of degree at most $n$. (There is even a stronger fact, namely if for any $x$ there is $n$ with $f^{(n)}=0$, then $f$ is a polynomial, but that's beside the point here.) If one takes a polynomial-logarithmic function $f$ made up of terms like $x^a \log^b x$, where $a,b$ may be taken rational, then the similar thing remains true: there is $n$ such that $f^{(n+1)}(x) \to 0$ as $x \to \infty$. Such functions have polynomial growth: they are bounded by $x^N$ for some $N$ (and for sufficiently large $N$). I would like to know to what extent the reverse is true: Suppose that $f$ is defined on $\mathbb{R}_+$ and smooth, and that for some $n$, $f^{(n+1)}(x) \to 0$ as $x \to \infty$. Is it true that $f(x)$ is bounded by $C x^N$ for some $N$, $C$? Can one take $N=n$?",,"['real-analysis', 'analysis', 'polynomials']"
36,A problem on intermediate value property and continuity,A problem on intermediate value property and continuity,,"Let $f:\mathbb{R} \to \mathbb{R}$ be a function with the intermediate value property: that is, $f$ maps intervals to intervals. Let $x \in \mathbb{R}$. Suppose to each sequence $ (x_n) $ converging to $x$ there exists a constant $M$ such that $$|f(x) - f(x_n)| ≤M \sup _{n,m}|f(x_n) - f(x_m)|$$ Then show that $f$ is continuous at $x$. How can I solve this problem? Can anyone help me please. I have the basic idea of real analysis but could not crack this problem.","Let $f:\mathbb{R} \to \mathbb{R}$ be a function with the intermediate value property: that is, $f$ maps intervals to intervals. Let $x \in \mathbb{R}$. Suppose to each sequence $ (x_n) $ converging to $x$ there exists a constant $M$ such that $$|f(x) - f(x_n)| ≤M \sup _{n,m}|f(x_n) - f(x_m)|$$ Then show that $f$ is continuous at $x$. How can I solve this problem? Can anyone help me please. I have the basic idea of real analysis but could not crack this problem.",,['real-analysis']
37,The convergence of the Flint Hills series vs the convergence of $\lim_{n\to\infty}\frac{1}{n^3\sin^2(n)}$,The convergence of the Flint Hills series vs the convergence of,\lim_{n\to\infty}\frac{1}{n^3\sin^2(n)},"The Flint Hills series , is the series $$\sum_{n=1}^\infty\frac{1}{n^3\sin^2(n)},$$ and it's an open problem as to whether the series converges. From the proof of Corollary 4 of this paper , it seems that it's also unknown if $$\lim_{n\to\infty}\frac{1}{n^3\sin^2(n)}$$ converges. Generally speaking (at least when it comes to problems one encounters in a typical calculus/analysis course), given a sequence $(a_n)$ it is easier to check the convergence of $$\lim_{n\to\infty}a_n,$$ than the convergence of $$\sum_{n=1}^\infty a_n.$$ Why then is the problem of determining if the series $$\sum_{n=1}^\infty\frac{1}{n^3\sin^2(n)}$$ converges a more pressing matter than determining if $$\lim_{n\to\infty}\frac{1}{n^3\sin^2(n)}$$ converges?","The Flint Hills series , is the series and it's an open problem as to whether the series converges. From the proof of Corollary 4 of this paper , it seems that it's also unknown if converges. Generally speaking (at least when it comes to problems one encounters in a typical calculus/analysis course), given a sequence it is easier to check the convergence of than the convergence of Why then is the problem of determining if the series converges a more pressing matter than determining if converges?","\sum_{n=1}^\infty\frac{1}{n^3\sin^2(n)}, \lim_{n\to\infty}\frac{1}{n^3\sin^2(n)} (a_n) \lim_{n\to\infty}a_n, \sum_{n=1}^\infty a_n. \sum_{n=1}^\infty\frac{1}{n^3\sin^2(n)} \lim_{n\to\infty}\frac{1}{n^3\sin^2(n)}","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'open-problem']"
38,Improvement of Simpson's rule,Improvement of Simpson's rule,,"Let $I_n$ denote the approximation of $$I = \int_a^b f(x)dx$$ obtained by applying Simpson's rule with $2n$ intervals of uniform length. Define a new approximation $$J_n=\frac{16I_{2n}-I_n}{15}.$$ Find the formula for the error $\int_a^b f(x)dx-J_n$ ? We have studied Simpson's rule and proved it's error term $$-\frac{(b-a)^5}{2880n^4}f^{(4)}(c)$$ by applying Cauchy's mean value theorem repeatedly to the quotent $\frac{E(x)}{x^5}$ where $$E(x)=\int_{-x}^x f(t) dt - \frac{x}{3} \left[f(-x)+4f(0)+f(x)\right]$$ I tried apply same method but it was very confused. Let define $$E_*(x)=\int_{-x}^{x} f(x)dx-J_n$$ Choose an interval [-c,c] for two intervals we have Simpson's approximation $$I_n=\frac{c}{3}\left(f(-c)+4f(0)+f(c)\right)$$ The we can check that $E_*(x)$ is quite smooth function, it's first, second and third derivative is zero at $x=0$ By applying Cuachy's mean value theorem repeteadly we get $$\frac{E_*(x)}{x^5}=\frac{E_*^{(1)}(u)}{5u^4}=\frac{E_*^{(2)}(v)}{20v^3}=\frac{E_*^{(3)}(z)}{60z^2}=\frac{E_*^{(4)}(t)}{120t}$$ which $0<t<z<v<u<x$ $${\frac{E_*(x)}{x^5}={\frac{\frac{1}{45}[-7tf^{(4)}(-t)-2tf^{(4)}(-\frac{t}{2})-2tf^{(4)}(\frac{t}{2})-7tf^{(4)}(t)-17f^{(3)}(-t)+16tf^{(3)}(-\frac{t}{2})-16tf^{(3)}(\frac{t}{2})+17f^{(3)}(t)]}{120t}}}$$ Another approach would be $$\int_a^b f(x)dx-I_n =-\frac{(b-a)^5}{2880n^4}f^{(4)}(c)$$ For some $c$ in $[a,b]$ $$\int_a^b f(x)dx-I_{2n} =-\frac{(b-a)^5}{2880n^4*16}f^{(4)}(k)$$ For some $k$ in $[a,b]$ $$J_n=-\frac{(b-a)^5}{2880n^4}(f^{(4)}(c)-f^{(4)}(k))$$ I couldn't proceed whatever these are right approachs. I have looked up some texts which describes Richardson extrapolation, Taylor series and $O(h)$ notation. And nowhere I have seen eroor terror in explicit form. I am not familiar with them. So could anybody help me please.","Let denote the approximation of obtained by applying Simpson's rule with intervals of uniform length. Define a new approximation Find the formula for the error ? We have studied Simpson's rule and proved it's error term by applying Cauchy's mean value theorem repeatedly to the quotent where I tried apply same method but it was very confused. Let define Choose an interval [-c,c] for two intervals we have Simpson's approximation The we can check that is quite smooth function, it's first, second and third derivative is zero at By applying Cuachy's mean value theorem repeteadly we get which Another approach would be For some in For some in I couldn't proceed whatever these are right approachs. I have looked up some texts which describes Richardson extrapolation, Taylor series and notation. And nowhere I have seen eroor terror in explicit form. I am not familiar with them. So could anybody help me please.","I_n I = \int_a^b f(x)dx 2n J_n=\frac{16I_{2n}-I_n}{15}. \int_a^b f(x)dx-J_n -\frac{(b-a)^5}{2880n^4}f^{(4)}(c) \frac{E(x)}{x^5} E(x)=\int_{-x}^x f(t) dt - \frac{x}{3} \left[f(-x)+4f(0)+f(x)\right] E_*(x)=\int_{-x}^{x} f(x)dx-J_n I_n=\frac{c}{3}\left(f(-c)+4f(0)+f(c)\right) E_*(x) x=0 \frac{E_*(x)}{x^5}=\frac{E_*^{(1)}(u)}{5u^4}=\frac{E_*^{(2)}(v)}{20v^3}=\frac{E_*^{(3)}(z)}{60z^2}=\frac{E_*^{(4)}(t)}{120t} 0<t<z<v<u<x {\frac{E_*(x)}{x^5}={\frac{\frac{1}{45}[-7tf^{(4)}(-t)-2tf^{(4)}(-\frac{t}{2})-2tf^{(4)}(\frac{t}{2})-7tf^{(4)}(t)-17f^{(3)}(-t)+16tf^{(3)}(-\frac{t}{2})-16tf^{(3)}(\frac{t}{2})+17f^{(3)}(t)]}{120t}}} \int_a^b f(x)dx-I_n =-\frac{(b-a)^5}{2880n^4}f^{(4)}(c) c [a,b] \int_a^b f(x)dx-I_{2n} =-\frac{(b-a)^5}{2880n^4*16}f^{(4)}(k) k [a,b] J_n=-\frac{(b-a)^5}{2880n^4}(f^{(4)}(c)-f^{(4)}(k)) O(h)","['real-analysis', 'calculus', 'integration', 'numerical-methods', 'simpsons-rule']"
39,Dirichlet's test with unimodal coefficients,Dirichlet's test with unimodal coefficients,,"Briefly: If we modify the hypotheses of Dirichlet's test to require a unimodal sequence of coefficients, not necessarily a monotonic sequence, then do we still get the same quantitative bound on $\sum a_nb_n$ ? And is this a known result? Let $(a_n)$ be a sequence of non-negative real numbers that converges to $0$ , and let $(b_n)$ be a sequence of complex numbers with bounded partial sums: there exists a constant $B$ such that for all $p\leq q$ , we have $\left|\sum_{n=p}^q b_n\right| \leq B$ . If our sequences are supported on the natural numbers, $n\in\mathbb N$ , and $(a_n)$ is monotonic, then Dirichlet's test states that $\sum a_nb_n$ converges. The proof proceeds by summation by parts, and in fact, we get $\left|\sum a_nb_n\right|\leq a_0B$ . But what if we generalize to sequences supported on the integers, $n\in\mathbb Z$ ? Then we should ask for $(a_n)$ to be a unimodal sequence, not a monotonic sequence. That is, $(a_n)$ increases up to some maximum value $a^*$ and then decreases thereafter. So let's do that: $(a_n)$ is now unimodal. But to simplify matters, we'll keep indexing on the natural numbers, $n\in\mathbb N$ , anyway, so that convergence is easy to talk about. Now $(a_n)$ is eventually decreasing, and Dirichlet's test still says $\sum a_nb_n$ converges. What's more interesting is the bound that we can get. The easy approach would be to split the series $\sum a_nb_n$ ""horizontally"" into two series of $a_nb_n$ terms, one with $a_n$ increasing and the other with $a_n$ decreasing. Then we would get $\left|\sum a_nb_n\right|\leq 2a^*B$ . But I believe we can decompose the sum ""vertically"" into a stack of rectangles, and that should give us the better bound $\left|\sum a_nb_n\right|\leq a^*B$ , i.e. without the extra factor of $2$ . Now, I haven't written out a formal proof of the above claim, which might make it hard to evaluate... but I would like to know: Did I make a mistake? Is this a known result? Is there some reference that I can cite for the inequality $\left|\sum a_nb_n\right|\leq a^*B$ ? For comparison, here are some related questions on the site: For Dirichlet's test, is it true that if $\ a_1=1\ $ then $\ \left\vert \sum_{n=1}^{\infty} a_n b_n \right\vert \leq M\ ?$ shows the traditional proof for the bound, in the special case of a monotonic sequence of coefficients. Is this generalization of Dirichlet's test true? is an attempted generalization that goes too far; if we drop all ordering hypotheses, then the resulting series might diverge. No amount of Googling has turned up a reference to the claim, so far. (Incidentally, these course notes contain a claim of the usual Dirichlet's test with an extra factor of $2$ , but that appears to be unrelated to my question; it's merely due to a typo where $k$ and $k+1$ are reversed, causing a series to telescope incorrectly.)","Briefly: If we modify the hypotheses of Dirichlet's test to require a unimodal sequence of coefficients, not necessarily a monotonic sequence, then do we still get the same quantitative bound on ? And is this a known result? Let be a sequence of non-negative real numbers that converges to , and let be a sequence of complex numbers with bounded partial sums: there exists a constant such that for all , we have . If our sequences are supported on the natural numbers, , and is monotonic, then Dirichlet's test states that converges. The proof proceeds by summation by parts, and in fact, we get . But what if we generalize to sequences supported on the integers, ? Then we should ask for to be a unimodal sequence, not a monotonic sequence. That is, increases up to some maximum value and then decreases thereafter. So let's do that: is now unimodal. But to simplify matters, we'll keep indexing on the natural numbers, , anyway, so that convergence is easy to talk about. Now is eventually decreasing, and Dirichlet's test still says converges. What's more interesting is the bound that we can get. The easy approach would be to split the series ""horizontally"" into two series of terms, one with increasing and the other with decreasing. Then we would get . But I believe we can decompose the sum ""vertically"" into a stack of rectangles, and that should give us the better bound , i.e. without the extra factor of . Now, I haven't written out a formal proof of the above claim, which might make it hard to evaluate... but I would like to know: Did I make a mistake? Is this a known result? Is there some reference that I can cite for the inequality ? For comparison, here are some related questions on the site: For Dirichlet's test, is it true that if $\ a_1=1\ $ then $\ \left\vert \sum_{n=1}^{\infty} a_n b_n \right\vert \leq M\ ?$ shows the traditional proof for the bound, in the special case of a monotonic sequence of coefficients. Is this generalization of Dirichlet's test true? is an attempted generalization that goes too far; if we drop all ordering hypotheses, then the resulting series might diverge. No amount of Googling has turned up a reference to the claim, so far. (Incidentally, these course notes contain a claim of the usual Dirichlet's test with an extra factor of , but that appears to be unrelated to my question; it's merely due to a typo where and are reversed, causing a series to telescope incorrectly.)",\sum a_nb_n (a_n) 0 (b_n) B p\leq q \left|\sum_{n=p}^q b_n\right| \leq B n\in\mathbb N (a_n) \sum a_nb_n \left|\sum a_nb_n\right|\leq a_0B n\in\mathbb Z (a_n) (a_n) a^* (a_n) n\in\mathbb N (a_n) \sum a_nb_n \sum a_nb_n a_nb_n a_n a_n \left|\sum a_nb_n\right|\leq 2a^*B \left|\sum a_nb_n\right|\leq a^*B 2 \left|\sum a_nb_n\right|\leq a^*B 2 k k+1,"['real-analysis', 'sequences-and-series', 'reference-request', 'upper-lower-bounds', 'monotone-functions']"
40,Local minimum implies global minimum.,Local minimum implies global minimum.,,"Let $f \in C^1(\mathbb{R}^n, \mathbb{R})$ (or even smoother if that helps) with $n > 1$ . Assume that $f$ has a local minimum in $x_0 \in \mathbb{R}^n$ and no other stationary points. Is $x_0$ then automatically a global minimum? For $n=1$ this is true and easy to show (e.g. with Rolle's theorem)... My intuition says the answer should be yes but I can't prove it.",Let (or even smoother if that helps) with . Assume that has a local minimum in and no other stationary points. Is then automatically a global minimum? For this is true and easy to show (e.g. with Rolle's theorem)... My intuition says the answer should be yes but I can't prove it.,"f \in C^1(\mathbb{R}^n, \mathbb{R}) n > 1 f x_0 \in \mathbb{R}^n x_0 n=1","['real-analysis', 'analysis', 'optimization']"
41,Gronwall lemma with highly oscillatory kernel,Gronwall lemma with highly oscillatory kernel,,"As a toy model for a larger problem, I want to show that if $A,B\geq 0$ and $u(t)$ satisfies $$|u(t)|\leq A+\left|\int_0^t B\cos(s^2)u(s)ds\right|$$ then $u$ satisfies a bound like $$|u(t)|\leq AC$$ where $C$ is a (finite) constant depending on integral(s) involving $B\cos(t^2)$ . The classical Gronwall lemma is of course not applicable here, since the functions involved are not nonnegative. For instance, if we try to repeat the proof of the usual Gronwall inequality, then formally we have $$\frac{d}{dt}\left(A+\left|\int_0^t B\cos(s^2)u(s)ds\right|\right) = B\cos(t^2)u(t)\text{sgn}\left(\int_0^t B\cos(s^2)u(s)ds\right)\leq B|u(t)|$$ After using the bound on $|u(t)|$ we get $$\frac{d}{dt} \ln\left(A+\left|\int_0^t B\cos(s^2)u(s)ds\right|\right)\leq B$$ Applying the fundamental theorem of calculus and exponentiating yields $$|u(t)|\leq A+\left|\int_0^t B\cos(s^2)u(s)ds\right| \leq Ae^{Bt}$$ Which is insufficient for my purposes, as I want to make use of the oscillations in the integral of $\cos(s^2)$ to ensure I don't get growth of $|u(t)|$ in time. $\rule{6cm}{0.4pt}$ Update: I've found a partial counterexample. Technically, this counterexample is for a modified problem, but the ideas are similar. Indeed, for any fixed $A,B$ we can find $u$ satisfying $$|u(t)|\leq A+\left|\int_0^t Be^{is^2}u(s)ds\right|$$ but whose modulus grows exponentially in time (as suggested by the calculation from earlier). Indeed, set $u(t) = Ae^{-it^2}e^{Bt}$ . Then we have $$A+\left|\int_0^t Be^{is^2}u(s)ds\right| = A+(Ae^{Bt} - A) = Ae^{Bt} = |u(t)|$$ It seems harder to come up with an explicit counterexample to the original problem, which suggests the problem may depend on the zeros of the oscillatory kernel.","As a toy model for a larger problem, I want to show that if and satisfies then satisfies a bound like where is a (finite) constant depending on integral(s) involving . The classical Gronwall lemma is of course not applicable here, since the functions involved are not nonnegative. For instance, if we try to repeat the proof of the usual Gronwall inequality, then formally we have After using the bound on we get Applying the fundamental theorem of calculus and exponentiating yields Which is insufficient for my purposes, as I want to make use of the oscillations in the integral of to ensure I don't get growth of in time. Update: I've found a partial counterexample. Technically, this counterexample is for a modified problem, but the ideas are similar. Indeed, for any fixed we can find satisfying but whose modulus grows exponentially in time (as suggested by the calculation from earlier). Indeed, set . Then we have It seems harder to come up with an explicit counterexample to the original problem, which suggests the problem may depend on the zeros of the oscillatory kernel.","A,B\geq 0 u(t) |u(t)|\leq A+\left|\int_0^t B\cos(s^2)u(s)ds\right| u |u(t)|\leq AC C B\cos(t^2) \frac{d}{dt}\left(A+\left|\int_0^t B\cos(s^2)u(s)ds\right|\right) = B\cos(t^2)u(t)\text{sgn}\left(\int_0^t B\cos(s^2)u(s)ds\right)\leq B|u(t)| |u(t)| \frac{d}{dt}
\ln\left(A+\left|\int_0^t B\cos(s^2)u(s)ds\right|\right)\leq B |u(t)|\leq A+\left|\int_0^t B\cos(s^2)u(s)ds\right| \leq Ae^{Bt} \cos(s^2) |u(t)| \rule{6cm}{0.4pt} A,B u |u(t)|\leq A+\left|\int_0^t Be^{is^2}u(s)ds\right| u(t) = Ae^{-it^2}e^{Bt} A+\left|\int_0^t Be^{is^2}u(s)ds\right| = A+(Ae^{Bt} - A) = Ae^{Bt} = |u(t)|","['real-analysis', 'partial-differential-equations', 'harmonic-analysis', 'oscillatory-integral', 'gronwall-type-inequality']"
42,Does the inequality $c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} - x^a - y^a \le C_a$ hold?,Does the inequality  hold?,c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} - x^a - y^a \le C_a,"Update : Posted in MO since it is unanswered in MSE Let $0 \le x,y \le 1$ and $a$ be a real. Consider the function $$ f(x,y,a) = xy^{y^a/x^a} + yx^{x^a/y^a} -  x^a - y^a \tag 1 $$ For a fixed $a$ , the graph of the maximum and the minimum value of $f(x,y,a)$ shown below. The graph shows that for $a \ge 1$ , there is a constant $C_a$ depending only in $a$ such that $xy^{y^a/x^a} + yx^{x^a/y^a} -  y^a - x^a \le C_a$ and similarly for $a \le 2$ , there is a constant $c_a$ such that $c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} -  y^a - x^a$ . Also, $0\ <a < 2$ is the only interval in which $f(x,y,a)$ has non-zero maxima and minima for every $a$ . Some experimentally observed examples of inequalities belonging to this family are $$ -\frac{1}{2} \le xy^{y/x} + yx^{x/y} - x - y < 0 \tag 2 $$ $$ 0 \le xy^{y^2/x^2} + yx^{x^2/y^2} -  x^2 - y^2 \le \frac{1}{4} \tag 3 $$ $$ 0 \le xy^{y^4/x^4} + yx^{x^4/y^4} -  x^4 - y^4 \le \frac{1}{2} \tag 4 $$ Question 1 : Can we prove that for every real $a$ , there exists $c_a$ and $C_a$ such that $$ c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} -  x^a - y^a \le C_a $$ Question 2 : Can we express $c_a$ and $C_a$ in terms of $a$ ?","Update : Posted in MO since it is unanswered in MSE Let and be a real. Consider the function For a fixed , the graph of the maximum and the minimum value of shown below. The graph shows that for , there is a constant depending only in such that and similarly for , there is a constant such that . Also, is the only interval in which has non-zero maxima and minima for every . Some experimentally observed examples of inequalities belonging to this family are Question 1 : Can we prove that for every real , there exists and such that Question 2 : Can we express and in terms of ?","0 \le x,y \le 1 a 
f(x,y,a) = xy^{y^a/x^a} + yx^{x^a/y^a} -  x^a - y^a \tag 1
 a f(x,y,a) a \ge 1 C_a a xy^{y^a/x^a} + yx^{x^a/y^a} -  y^a - x^a \le C_a a \le 2 c_a c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} -  y^a - x^a 0\ <a < 2 f(x,y,a) a 
-\frac{1}{2} \le xy^{y/x} + yx^{x/y} - x - y < 0 \tag 2
 
0 \le xy^{y^2/x^2} + yx^{x^2/y^2} -  x^2 - y^2 \le \frac{1}{4} \tag 3
 
0 \le xy^{y^4/x^4} + yx^{x^4/y^4} -  x^4 - y^4 \le \frac{1}{2} \tag 4
 a c_a C_a 
c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} -  x^a - y^a \le C_a
 c_a C_a a","['real-analysis', 'calculus', 'algebra-precalculus', 'analysis', 'inequality']"
43,"If the Fourier coefficient $\hat{f}(k)$ of $f\in C^1(\mathbb T)$ is zero for all $|k|<N$, then $\|f\|_{L^\infty}\leq \frac CN \|f'\|_{L^\infty}$","If the Fourier coefficient  of  is zero for all , then",\hat{f}(k) f\in C^1(\mathbb T) |k|<N \|f\|_{L^\infty}\leq \frac CN \|f'\|_{L^\infty},"Let $f\in C^1(\mathbb T)=C^1(\mathbb R/\mathbb Z)$ be a function such that $$\hat f(k):=\int_{\mathbb T}f(x)e^{-2\pi ikx}\,dx=0,\qquad \forall k\in\{-N+1,\cdots,-1,0,1,\cdots, N-1\}.$$ Show that $\|f\|_{L^\infty}\leq \frac CN \|f'\|_{L^\infty}$ for some $C>0$ indpendent of $f$ and $N$ . The best constant $C$ is $\frac14$ , as shown by this post , in which the proof uses a very ""hard"" theorem due to Vaaler . I'm trying to prove the decay rate $N^{-1}$ and I don't care about the best constant $C$ . I'm looking for an easier proof. First Try. By Cauchy inequality and the Plancherel identity, \begin{align*} \|f\|_{L^\infty}&\leq\sum_{|k|\geq N}|\hat f(k)|=\sum_{|k|\geq N}\frac{\left|\widehat{f'}(k)\right|}{2\pi |k|}\\ &\leq \frac1{2\pi}\left(\sum_{|k|\geq N}\left|\widehat{f'}(k)\right|^2\right)^{1/2}\left(\sum_{|k|\geq N}\frac1{|k|^2}\right)^{1/2}\\ &\leq \frac1{2\pi}\left\|f'\right\|_{L^2}\left(\frac2N\right)^{1/2}=\frac C{\sqrt N}\left\|f'\right\|_{L^2}\\ &\leq \frac C{\sqrt N}\left\|f'\right\|_{L^\infty}\ \ . \end{align*} The deacy rate on $N$ is not good enough to get the desired result. Second Try. (Also failed.) As Sarvesh Ravichandran Iyer suggested in the comments, using some ideas in the proof that I quoted above, I've made some progress. In this post , let $\psi(x)=x-1/2, x \in [0,1]$ , then $$\psi(x)=-\frac{1}{\pi}\sum_{k \ge 1}\frac{\sin 2\pi kx}{k}, 0<x<1$$ and $\psi*f'=-f$ . If we perturb $\psi$ by any trigonometric polynomial of degree at most $N-1$ and we get an $\eta=\psi-P_{N-1}$ we still have $\eta*f'=-f$ hence $||f||_{\infty}=||\eta*f'||_{\infty} \le ||\eta||_1||f'||_{\infty}$ and in particular if we find $||\eta||_1 \le \frac{C}{N}$ then we are done. A natural candidate is $$\eta(x)=\psi(x)+\frac{1}{\pi}\sum_{1\leq k\leq N-1}\frac{\sin 2\pi kx}{k}=-\frac{1}{\pi}\sum_{k \ge N}\frac{\sin 2\pi kx}{k}.$$ Now, the problem reduces to show that $||\eta||_1 \le \frac{C}{N}$ for this special function $\eta$ , where $C>0$ is independent of $N$ . To explore the oscillation in this summation, we can use the partial sum method. Let $S_M(x)=\sum_{k=N}^M \sin(2\pi kx)$ , then $$S_M(x)=-\frac{\cos((2M+1)\pi x)-\cos((2N-1)\pi x)}{2\sin \pi x}=\frac{\sin((M+N)\pi x)\sin((M-N+1)\pi x)}{\sin\pi x}.$$ The partial sum method applied on $\eta$ gives that $$\eta(x)=-\frac1\pi\sum_{k=N}^\infty \frac1{k(k+1)}S_k(x).$$ Note that near $x=0$ , the denominator is small, hence, naturally we split the integral into two parts: $\int_0^{1/2}|\eta|=\int_0^\delta+\int_\delta^{1/2}$ . By Plancherel we have $$\int_0^\delta |\eta(x)|\,dx\leq \delta^{1/2}\left(\int_0^\delta|\eta(x)|^2\,dx\right)^{1/2}\leq C\delta^{1/2}\left(\sum_{k=N}^\infty\frac1{k^2}\right)^{1/2}\leq C\sqrt{\frac\delta N}.$$ To get the correct decay rate $N^{-1}$ , it seems that $\delta=1/N$ is a good choice. It remains to estimate $\int_\delta^{1/2}|\eta|$ , we have $$\int_\delta^{1/2}|\eta(x)|\,dx\leq C\sum_{k=N}^\infty \frac1{k(k+1)} \int_\delta^{1/2}\frac1{|\sin(\pi x)|}\,dx\leq C\frac {|\log \delta|}{N}=C\frac{\log N}N.$$ Therefore, $\|\eta\|_1\le C\frac{\log N}N$ . This implies that $\|f\|_{L^\infty}\leq C\frac{\log N}N \|f'\|_{L^\infty}$ , which is better than my first try, but not satisfactory either. Any help would be appreciated!","Let be a function such that Show that for some indpendent of and . The best constant is , as shown by this post , in which the proof uses a very ""hard"" theorem due to Vaaler . I'm trying to prove the decay rate and I don't care about the best constant . I'm looking for an easier proof. First Try. By Cauchy inequality and the Plancherel identity, The deacy rate on is not good enough to get the desired result. Second Try. (Also failed.) As Sarvesh Ravichandran Iyer suggested in the comments, using some ideas in the proof that I quoted above, I've made some progress. In this post , let , then and . If we perturb by any trigonometric polynomial of degree at most and we get an we still have hence and in particular if we find then we are done. A natural candidate is Now, the problem reduces to show that for this special function , where is independent of . To explore the oscillation in this summation, we can use the partial sum method. Let , then The partial sum method applied on gives that Note that near , the denominator is small, hence, naturally we split the integral into two parts: . By Plancherel we have To get the correct decay rate , it seems that is a good choice. It remains to estimate , we have Therefore, . This implies that , which is better than my first try, but not satisfactory either. Any help would be appreciated!","f\in C^1(\mathbb T)=C^1(\mathbb R/\mathbb Z) \hat f(k):=\int_{\mathbb T}f(x)e^{-2\pi ikx}\,dx=0,\qquad \forall k\in\{-N+1,\cdots,-1,0,1,\cdots, N-1\}. \|f\|_{L^\infty}\leq \frac CN \|f'\|_{L^\infty} C>0 f N C \frac14 N^{-1} C \begin{align*}
\|f\|_{L^\infty}&\leq\sum_{|k|\geq N}|\hat f(k)|=\sum_{|k|\geq N}\frac{\left|\widehat{f'}(k)\right|}{2\pi |k|}\\
&\leq \frac1{2\pi}\left(\sum_{|k|\geq N}\left|\widehat{f'}(k)\right|^2\right)^{1/2}\left(\sum_{|k|\geq N}\frac1{|k|^2}\right)^{1/2}\\
&\leq \frac1{2\pi}\left\|f'\right\|_{L^2}\left(\frac2N\right)^{1/2}=\frac C{\sqrt N}\left\|f'\right\|_{L^2}\\
&\leq \frac C{\sqrt N}\left\|f'\right\|_{L^\infty}\ \ .
\end{align*} N \psi(x)=x-1/2, x \in [0,1] \psi(x)=-\frac{1}{\pi}\sum_{k \ge 1}\frac{\sin 2\pi kx}{k}, 0<x<1 \psi*f'=-f \psi N-1 \eta=\psi-P_{N-1} \eta*f'=-f ||f||_{\infty}=||\eta*f'||_{\infty} \le ||\eta||_1||f'||_{\infty} ||\eta||_1 \le \frac{C}{N} \eta(x)=\psi(x)+\frac{1}{\pi}\sum_{1\leq k\leq N-1}\frac{\sin 2\pi kx}{k}=-\frac{1}{\pi}\sum_{k \ge N}\frac{\sin 2\pi kx}{k}. ||\eta||_1 \le \frac{C}{N} \eta C>0 N S_M(x)=\sum_{k=N}^M \sin(2\pi kx) S_M(x)=-\frac{\cos((2M+1)\pi x)-\cos((2N-1)\pi x)}{2\sin \pi x}=\frac{\sin((M+N)\pi x)\sin((M-N+1)\pi x)}{\sin\pi x}. \eta \eta(x)=-\frac1\pi\sum_{k=N}^\infty \frac1{k(k+1)}S_k(x). x=0 \int_0^{1/2}|\eta|=\int_0^\delta+\int_\delta^{1/2} \int_0^\delta |\eta(x)|\,dx\leq \delta^{1/2}\left(\int_0^\delta|\eta(x)|^2\,dx\right)^{1/2}\leq C\delta^{1/2}\left(\sum_{k=N}^\infty\frac1{k^2}\right)^{1/2}\leq C\sqrt{\frac\delta N}. N^{-1} \delta=1/N \int_\delta^{1/2}|\eta| \int_\delta^{1/2}|\eta(x)|\,dx\leq C\sum_{k=N}^\infty \frac1{k(k+1)} \int_\delta^{1/2}\frac1{|\sin(\pi x)|}\,dx\leq C\frac {|\log \delta|}{N}=C\frac{\log N}N. \|\eta\|_1\le C\frac{\log N}N \|f\|_{L^\infty}\leq C\frac{\log N}N \|f'\|_{L^\infty}","['real-analysis', 'fourier-analysis', 'fourier-series', 'alternative-proof', 'harmonic-analysis']"
44,Do we need intervals to define the Lebesgue measure?,Do we need intervals to define the Lebesgue measure?,,"The Lebesgue measure is conventionally defined as $$\mu(X) = \inf\{\sum_{n \in \mathbb{N}}(b_n-a_n) | X \subseteq \bigcup_{n \in \mathbb{N}}(a_n,b_n) \}$$ Which can be thought of intuitively as covering the set with translated and stretched copies of the set $(0,1)$ . I am wondering whether we could equally well define it with any other measurable set, and get the same measure, just by dividing by the Lebesgue measure of that basic set. I.e. for some set $B$ $$\mu'(X) = \mu(B) \inf\{\sum_{n \in \mathbb{N}}s_n | X \subseteq \bigcup_{n \in \mathbb{N}}(r_n+s_nB) \}$$ Where $r_n$ are aritrary elements of $\mathbb{R}$ and $s_n$ are arbitrary elements of $\mathbb{R}_{>0}$ . The thing that got me thinking about this was trying to figure out a proof of $$\mu(AX) = |\det A|\mu(X)$$ Where it seems like it would be very helpful to be able to define the measure in terms of sets of the form $AP$ where $P$ is a product of open intervals. I do not know if this is the usual proof. But thinking about this, it seemed like a) this would work and b) this isn't anything particular to sets of the form $AP$ . Other than $0$ and $\infty$ measure sets, it seems like any set could work. Edit: meagre sets also won't work. Moreover, it seems clear that a sets like a disconnected union of a positive measure meagre set and another set wouldn't be able to cover $(0,1)$ without significant overlap, so it seems some kind of 'nowhere meagre' condition will be required. My best guess is that this condition should be that the intersection of any (non-empty) open set with $B$ is non-meagre. E.g. after imposing this restriction, even unbounded sets have to have almost all of their substance in a bounded interval, so the unboundedness shouldn't cause a problem. Highly disconnected sets like fat Cantor sets seem trickier to deal with, but there isn't any obvious proof they couldn't work. @fedja pointed out that this construction won't work for any meagre sets, so fat cantor sets won't work. It is clear that this at least produces a valid outer measure, and Caratheory's criterion turns it into a measure on some $\sigma$ alebra. This measure would also have to be translation invariant, so as long as the Caratheodory criterion produces the same $\sigma$ algebra, it I think the measure has to be the same. I haven't proven this, but I think if stretched and shifted copies of $B$ can cover $(0,1)^n$ so that the set of 'overlap' has Lebesgue measure $0$ , that would be enough to show that $(0,1)$ is in the $\sigma$ algebra generated. And it seems like equality would follow from that. But not being meagre anywhere doesn't seem like a strong enough condition to guarantee the former, and proving either of these things rigorously seems above my ability level. Although the $n$ -dimensional case motivated this, I would be happy with proofs for the $1$ dimensional case. I would ideally like a criterion for which sets this construction works for, though this may be rather optimistic. Failing that, I would like any results about what classes of sets the construction does or doesn't work for, and particularly classes of sets that this does work for.","The Lebesgue measure is conventionally defined as Which can be thought of intuitively as covering the set with translated and stretched copies of the set . I am wondering whether we could equally well define it with any other measurable set, and get the same measure, just by dividing by the Lebesgue measure of that basic set. I.e. for some set Where are aritrary elements of and are arbitrary elements of . The thing that got me thinking about this was trying to figure out a proof of Where it seems like it would be very helpful to be able to define the measure in terms of sets of the form where is a product of open intervals. I do not know if this is the usual proof. But thinking about this, it seemed like a) this would work and b) this isn't anything particular to sets of the form . Other than and measure sets, it seems like any set could work. Edit: meagre sets also won't work. Moreover, it seems clear that a sets like a disconnected union of a positive measure meagre set and another set wouldn't be able to cover without significant overlap, so it seems some kind of 'nowhere meagre' condition will be required. My best guess is that this condition should be that the intersection of any (non-empty) open set with is non-meagre. E.g. after imposing this restriction, even unbounded sets have to have almost all of their substance in a bounded interval, so the unboundedness shouldn't cause a problem. Highly disconnected sets like fat Cantor sets seem trickier to deal with, but there isn't any obvious proof they couldn't work. @fedja pointed out that this construction won't work for any meagre sets, so fat cantor sets won't work. It is clear that this at least produces a valid outer measure, and Caratheory's criterion turns it into a measure on some alebra. This measure would also have to be translation invariant, so as long as the Caratheodory criterion produces the same algebra, it I think the measure has to be the same. I haven't proven this, but I think if stretched and shifted copies of can cover so that the set of 'overlap' has Lebesgue measure , that would be enough to show that is in the algebra generated. And it seems like equality would follow from that. But not being meagre anywhere doesn't seem like a strong enough condition to guarantee the former, and proving either of these things rigorously seems above my ability level. Although the -dimensional case motivated this, I would be happy with proofs for the dimensional case. I would ideally like a criterion for which sets this construction works for, though this may be rather optimistic. Failing that, I would like any results about what classes of sets the construction does or doesn't work for, and particularly classes of sets that this does work for.","\mu(X) = \inf\{\sum_{n \in \mathbb{N}}(b_n-a_n) | X \subseteq \bigcup_{n \in \mathbb{N}}(a_n,b_n) \} (0,1) B \mu'(X) = \mu(B) \inf\{\sum_{n \in \mathbb{N}}s_n | X \subseteq \bigcup_{n \in \mathbb{N}}(r_n+s_nB) \} r_n \mathbb{R} s_n \mathbb{R}_{>0} \mu(AX) = |\det A|\mu(X) AP P AP 0 \infty (0,1) B \sigma \sigma B (0,1)^n 0 (0,1) \sigma n 1","['real-analysis', 'measure-theory', 'lebesgue-measure', 'outer-measure']"
45,Two nice (challenging) binoharmonic series,Two nice (challenging) binoharmonic series,,"I've recently seen a nice binoharmonic series from Ali's book (page 309), but that post, for some reason, vanished ( Update : that post was undeleted and now it may be found here Evaluating $\int_0^{\frac{\pi}{2}}x^2 \cot x\ln(1-\sin x)\mathrm{d}x$ ). It is an excellent addition to MSE, that is, $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}}{n^3}.$$ since finding elegant ways is not easy to do. I would also add the following variant, which seems to be a new one (in case it is known, any reference will be highly appreciated), $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}}{n^4}.$$ Can we find very elegant ways to go for these two series? Lots of other variants we could consider, like $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}^2}{n^2}.$$ More curious binoharmonic structures (only a few examples in the list of possible items alike, keeping the weight $\le5$ ), $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{n^2};$$ $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{n^3};$$ $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{(2n+1)^2};$$ $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{(2n+1)^3}.$$ Let me also make a little update ( February 03, 2023 ): $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}H_{4n}}{n^2};$$ $$ \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}H_{4n}}{(2n+1)^2}.$$ Update : Yes , that's the answer to the question above concerning the first series, and we can get brilliant proofs , and a paper is under work, which will be available soon.","I've recently seen a nice binoharmonic series from Ali's book (page 309), but that post, for some reason, vanished ( Update : that post was undeleted and now it may be found here Evaluating $\int_0^{\frac{\pi}{2}}x^2 \cot x\ln(1-\sin x)\mathrm{d}x$ ). It is an excellent addition to MSE, that is, since finding elegant ways is not easy to do. I would also add the following variant, which seems to be a new one (in case it is known, any reference will be highly appreciated), Can we find very elegant ways to go for these two series? Lots of other variants we could consider, like More curious binoharmonic structures (only a few examples in the list of possible items alike, keeping the weight ), Let me also make a little update ( February 03, 2023 ): Update : Yes , that's the answer to the question above concerning the first series, and we can get brilliant proofs , and a paper is under work, which will be available soon.", \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}}{n^3}.  \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}}{n^4}.  \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_{2n}^2}{n^2}. \le5  \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{n^2};  \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{n^3};  \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{(2n+1)^2};  \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}}{(2n+1)^3}.  \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}H_{4n}}{n^2};  \sum_{n=1}^{\infty} \frac{4^n}{\displaystyle \binom{2n}{n}} \frac{H_n H_{2n}H_{4n}}{(2n+1)^2}.,"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'harmonic-numbers']"
46,"Sharp bounds for power towers $x^x,x^{x^{x^x}},x^{x^{x^{x^{x^x}}}},\cdots$",Sharp bounds for power towers,"x^x,x^{x^{x^x}},x^{x^{x^{x^{x^x}}}},\cdots","I am looking for a reference to results on sharp upper and lower bounds for the $2n$ th power towers $$x^x,x^{x^{x^x}},x^{x^{x^{x^{x^x}}}},x^{x^{x^{x^{x^{x^{x^x}}}}}},\cdots$$ over the intervals $x\in[0,1]$ and/or $[1,\infty)$ for any natural number $n$ , such that the bounds contain exponent terms stacked no higher than the form $a^b$ . An example would be the inequality — valid for $[0,\infty)$ : $$\frac12x^2+\frac12\le x^{x^{x^{x^{x^x}}}}$$ that has been shown on MSE ; however, this is for a specific $n=3$ . I believe the case $x^x$ with $n=1$ is a bit better established, where we can derive lower bounds using Padé approximants, for instance. Are there any papers that investigate this problem for this set of power towers?","I am looking for a reference to results on sharp upper and lower bounds for the th power towers over the intervals and/or for any natural number , such that the bounds contain exponent terms stacked no higher than the form . An example would be the inequality — valid for : that has been shown on MSE ; however, this is for a specific . I believe the case with is a bit better established, where we can derive lower bounds using Padé approximants, for instance. Are there any papers that investigate this problem for this set of power towers?","2n x^x,x^{x^{x^x}},x^{x^{x^{x^{x^x}}}},x^{x^{x^{x^{x^{x^{x^x}}}}}},\cdots x\in[0,1] [1,\infty) n a^b [0,\infty) \frac12x^2+\frac12\le x^{x^{x^{x^{x^x}}}} n=3 x^x n=1","['real-analysis', 'reference-request', 'upper-lower-bounds', 'tetration']"
47,"When can a double limit $\lim_{k\to\infty}\lim_{m\to\infty}f_{k,m}(x)$ be combined into $\lim_{k\to\infty}f_{k,k}(x)$?",When can a double limit  be combined into ?,"\lim_{k\to\infty}\lim_{m\to\infty}f_{k,m}(x) \lim_{k\to\infty}f_{k,k}(x)","Let $f_{k,m}:\mathbb{R}^n\to \mathbb{R}, \forall k, m \in \mathbb{N}$ and suppose that $\lim_{m\to\infty}f_{k,m} = f_{k,-}$ uniformly in $m$ and $\lim_{k\to\infty}f_{k,m} = f_{-,m}$ at least pointwise. Can I then combine the double limit $\lim_{k\to\infty}\lim_{m\to\infty}f_{k,m}$ to $\lim_{k\to\infty}\lim_{m\to\infty}f_{k,m} = \lim_{k\to\infty}f_{k,k}$ ? I already tried to search my analysis notes and books, but as this sort of limit combining is quite rare at least in standard curriculum (I cannot reside any that important theorem that combines limits), so I could not find anything useful. I am aware that under these assumptions conditions one can change the order of the limits of $\lim_{n\to\infty}\lim_{m\to\infty}f_{k,m}$ to $\lim_{m\to\infty}\lim_{k\to\infty}f_{k,m}$ . Edit: A guiding motivation to my question relates to convergence discussed in a problem such as this: Dense subspace of the space of functions vanishing at infinity , where one acceptable approach is this . If I understood that answer correctly, we are showing that we can approximate a continuous function $\tilde{f}_k$ with a compact support in the infinity norm with a sequence of smooth compactly supported functions $g_{k,m}$ . We are taking it as granted that we can approximate a continuous function $f$ vanishing at infinity with such $\tilde{f}_k$ s, so in the end we are approximating $f$ with a sequence that itself is approximated with some other functions. Therefore once wrote out, the entire expression is something like $$\lim_{k\to \infty}||f - \tilde{f}_k||_\infty = \lim_{k\to \infty}||f - \lim_{m\to\infty}g_{k,m}||_\infty$$","Let and suppose that uniformly in and at least pointwise. Can I then combine the double limit to ? I already tried to search my analysis notes and books, but as this sort of limit combining is quite rare at least in standard curriculum (I cannot reside any that important theorem that combines limits), so I could not find anything useful. I am aware that under these assumptions conditions one can change the order of the limits of to . Edit: A guiding motivation to my question relates to convergence discussed in a problem such as this: Dense subspace of the space of functions vanishing at infinity , where one acceptable approach is this . If I understood that answer correctly, we are showing that we can approximate a continuous function with a compact support in the infinity norm with a sequence of smooth compactly supported functions . We are taking it as granted that we can approximate a continuous function vanishing at infinity with such s, so in the end we are approximating with a sequence that itself is approximated with some other functions. Therefore once wrote out, the entire expression is something like","f_{k,m}:\mathbb{R}^n\to \mathbb{R}, \forall k, m \in \mathbb{N} \lim_{m\to\infty}f_{k,m} = f_{k,-} m \lim_{k\to\infty}f_{k,m} = f_{-,m} \lim_{k\to\infty}\lim_{m\to\infty}f_{k,m} \lim_{k\to\infty}\lim_{m\to\infty}f_{k,m} = \lim_{k\to\infty}f_{k,k} \lim_{n\to\infty}\lim_{m\to\infty}f_{k,m} \lim_{m\to\infty}\lim_{k\to\infty}f_{k,m} \tilde{f}_k g_{k,m} f \tilde{f}_k f \lim_{k\to \infty}||f - \tilde{f}_k||_\infty = \lim_{k\to \infty}||f - \lim_{m\to\infty}g_{k,m}||_\infty","['real-analysis', 'sequences-and-series', 'limits', 'functions', 'uniform-convergence']"
48,"Is it possible to evaluate $\int_{-\infty}^{\infty}e^{-x^2}\operatorname{sech}^2(\frac{x}{2})\,dx$?",Is it possible to evaluate ?,"\int_{-\infty}^{\infty}e^{-x^2}\operatorname{sech}^2(\frac{x}{2})\,dx","I am curious to know if it is possible to evaluate the following integral: $$\int_{-\infty}^{\infty}e^{-x^2}\operatorname{sech}^2\left(\frac{x}{2}\right)\,dx$$ Here is the context: Someone had asked me about the error when using the trapezoidal rule on the integral $$I=\int_{0}^{1} \exp\left(-\ln^2\left(\frac{x}{1-x}\right)\right)\,dx= \int_{0}^{1} \exp\left(-\ln^2\left(\frac{x}{x-2}\right)\right)\,dx$$ I was however interested to potentially find a closed-form for this integral. Here are my attempts: After setting $u=\ln \left(\frac{x}{1-x}\right)$ one can show that $$I=\int_{-\infty}^{\infty} \frac{e^{u-u^2}}{(1+e^u)^2}\,du\stackrel{\text{IBP}}{=}-2\int_{-\infty}^{\infty} \frac{u e^{-u^2}}{1+e^u}\,du$$ Here I tried to use contour integration, however, the residue sum diverges rapidly, so I was stuck with this attempt. Splitting the integral at $0$ , and using $$\frac{1}{1+e^u} =\sum_{n=1}^{\infty} (-1)^{n-1} e^{-n u}$$ one can determine $$I=\sum_{n=1}^{\infty} (-1)^{n-1} n \sqrt{\pi} e^{n^2/4} \operatorname{erfc}\left(\frac{n}{2}\right)$$ but I’m not sure how to evaluate this series. Performing some substitutions on $I$ one can arrive at $$I=\int_{-\infty}^{\infty} x e^{-x^2} \tanh\left(\frac{x}{2}\right)\,dx\stackrel{\text{IBP}}{=}\frac{1}{4}\int_{-\infty}^{\infty} e^{-x^2} \operatorname{sech}^2\left(\frac{x}{2}\right)\,dx=\int_{-\infty}^{\infty}\frac{e^{-x^2}}{e^x+e^{-x}+2}\,dx$$ which appears like an integral that potentially contour integration can be applied to but I still arrive at the same rapidly-diverging residue series. I also arrived at the radically different form $$I=1-\frac{4}{\pi} \int_{0}^{\infty} \sin (t) \left(\Im \left(\psi \left(\frac{\sqrt{it}}{2\pi}\right)-2\psi\left(\frac{\sqrt{it}}{\pi}\right)\right)+\frac{\pi}{4}\right)\, dt$$ where $\psi$ is the digamma function. This doesn’t appear too hopeful, however, since integrals of the form $\sin (x) \psi (x)$ generally don’t have closed-forms and can only be expressed in terms of a logarithm series of similar form to my question on $\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2-4}$ due to the results known about the Laplace transform of the digamma function, but perhaps the case $\sin (x) \psi (\sqrt{x})$ works out nicer- I will also accept some logarithmic series of similar form for $I$ . I found this question which asked about a generalised form of this integral.","I am curious to know if it is possible to evaluate the following integral: Here is the context: Someone had asked me about the error when using the trapezoidal rule on the integral I was however interested to potentially find a closed-form for this integral. Here are my attempts: After setting one can show that Here I tried to use contour integration, however, the residue sum diverges rapidly, so I was stuck with this attempt. Splitting the integral at , and using one can determine but I’m not sure how to evaluate this series. Performing some substitutions on one can arrive at which appears like an integral that potentially contour integration can be applied to but I still arrive at the same rapidly-diverging residue series. I also arrived at the radically different form where is the digamma function. This doesn’t appear too hopeful, however, since integrals of the form generally don’t have closed-forms and can only be expressed in terms of a logarithm series of similar form to my question on due to the results known about the Laplace transform of the digamma function, but perhaps the case works out nicer- I will also accept some logarithmic series of similar form for . I found this question which asked about a generalised form of this integral.","\int_{-\infty}^{\infty}e^{-x^2}\operatorname{sech}^2\left(\frac{x}{2}\right)\,dx I=\int_{0}^{1} \exp\left(-\ln^2\left(\frac{x}{1-x}\right)\right)\,dx= \int_{0}^{1} \exp\left(-\ln^2\left(\frac{x}{x-2}\right)\right)\,dx u=\ln \left(\frac{x}{1-x}\right) I=\int_{-\infty}^{\infty} \frac{e^{u-u^2}}{(1+e^u)^2}\,du\stackrel{\text{IBP}}{=}-2\int_{-\infty}^{\infty} \frac{u e^{-u^2}}{1+e^u}\,du 0 \frac{1}{1+e^u} =\sum_{n=1}^{\infty} (-1)^{n-1} e^{-n u} I=\sum_{n=1}^{\infty} (-1)^{n-1} n \sqrt{\pi} e^{n^2/4} \operatorname{erfc}\left(\frac{n}{2}\right) I I=\int_{-\infty}^{\infty} x e^{-x^2} \tanh\left(\frac{x}{2}\right)\,dx\stackrel{\text{IBP}}{=}\frac{1}{4}\int_{-\infty}^{\infty} e^{-x^2} \operatorname{sech}^2\left(\frac{x}{2}\right)\,dx=\int_{-\infty}^{\infty}\frac{e^{-x^2}}{e^x+e^{-x}+2}\,dx I=1-\frac{4}{\pi} \int_{0}^{\infty} \sin (t) \left(\Im \left(\psi \left(\frac{\sqrt{it}}{2\pi}\right)-2\psi\left(\frac{\sqrt{it}}{\pi}\right)\right)+\frac{\pi}{4}\right)\, dt \psi \sin (x) \psi (x) \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2-4} \sin (x) \psi (\sqrt{x}) I","['real-analysis', 'integration', 'sequences-and-series', 'analysis', 'contour-integration']"
49,Inequality with slowly varying functions,Inequality with slowly varying functions,,"Question Let $X$ be a random variable with distribution function $F$ on a probability space $(\Omega, \mathcal F, P)$ . Suppose that there exist $\alpha \in (0,2)$ and a slowly varying function $\ell(\cdot)$ such that $$ \bar F(x) := 1 - F(x) = \frac{C_1(x)}{x^\alpha} \ell(x) \quad \text{and} \quad F(-x) = \frac{C_2(x)}{x^\alpha} \ell(x) \quad \text{for $x > 0$,} $$ where $C_1(\cdot), C_2(\cdot)$ are non-negative functions with $C_i := \lim_{x \to \infty} C_i(x)$ , and $C_1 + C_2 > 0$ . Why then does the following hold? There exist $C, \tilde C > 0$ such that $$ \sum_{n=1}^\infty P\big( |X| > a_n \big) \leq \sum_{n=1}^\infty \frac{C}{nf(n)} \leq \tilde C \int_1^\infty \frac{dt}{t f(t)},  $$ where we define $a_n := [n f(n) \ell(n) ]^{1/\alpha}$ for an arbitrary positive function $f$ with the properties $$ \limsup_{t \to \infty} \sup_{0 \leq t \leq x} \frac{f(t)}{f(x)} < \infty \quad \text{and} \quad \int_1^\infty \frac{dt}{tf(t)}< \infty. $$ If not, what if we also required that $f$ be slowly varying, too? Background and Thoughts This comes from Cai's 2006 paper "" Chover-Type Laws of the Iterated Logarithm for Weighted sums of $\rho^*$ -Mixing Sequences "". Cai writes on page 5 that this is ""easily seen"" based on the representation of $F$ above. I don't see why. What follows below is my attempt so far. $$ \begin{aligned} \sum_{n=1}^\infty P\big( |X| > a_n \big) &= \sum_{n=1}^\infty \Big[ P\big( X > a_n \big) + P\big( X < - a_n \big)  \Big] \\ &\leq \sum_{n=1}^\infty \Big[ \bar F(a_n) + F(-a_n)  \Big] \\ &= \sum_{n=1}^\infty \frac{C_1(a_n) + C_2(a_n)}{a_n^\alpha} \ell(a_n) \\ &\leq \sum_{n=1}^\infty \frac{C}{a_n^\alpha} \ell(a_n) = C \sum_{n=1}^\infty \frac{1}{nf(n)} \cdot \frac{\ell(a_n)}{\ell(n)}, \end{aligned} $$ where the inequality on the last line holds for some $C>0$ , since $C_1(\cdot)$ and $C_2(\cdot)$ are convergent. If $\frac{\ell(a_n)}{\ell(n)} = \ell\Big( \big[ n f(n) \ell(n) \big]^{1/\alpha} \Big) \Big/ \ell(n)$ were bounded, then the first desired inequality would follow. However, it's not clear why this would have to hold. Updates Indeed, I think the author implicitly uses the fact (?) that $\left\{ \frac{\ell(a_n)}{\ell(n)} \right\}$ is a bounded sequence several times in the paper. But I still don't know why that's the case. Although not stated in the paper, maybe we need some more restrictions on $f$ , such that it is also a slowly varying function. If $f$ were slowly varying, then $u(x) := x^{1/\alpha}\cdot[f(x) \ell(x)]^{1/\alpha}$ would be regularly varying with coefficient $1/\alpha$ . And $\frac{\ell(a_n)}{\ell(n)} = \frac{\ell\big(u(n)\big)}{\ell(n)}$ . Since $u(x) \to \infty$ and is regularly varying, $\ell \circ u$ is slowly varying. Hence, $\frac{\ell(a_n)}{\ell(n)} = \frac{ \ell \big(u(n) \big)}{\ell(n)}$ is slowly varying. But, of course, slowly varying functions aren't necessarily bounded.","Question Let be a random variable with distribution function on a probability space . Suppose that there exist and a slowly varying function such that where are non-negative functions with , and . Why then does the following hold? There exist such that where we define for an arbitrary positive function with the properties If not, what if we also required that be slowly varying, too? Background and Thoughts This comes from Cai's 2006 paper "" Chover-Type Laws of the Iterated Logarithm for Weighted sums of -Mixing Sequences "". Cai writes on page 5 that this is ""easily seen"" based on the representation of above. I don't see why. What follows below is my attempt so far. where the inequality on the last line holds for some , since and are convergent. If were bounded, then the first desired inequality would follow. However, it's not clear why this would have to hold. Updates Indeed, I think the author implicitly uses the fact (?) that is a bounded sequence several times in the paper. But I still don't know why that's the case. Although not stated in the paper, maybe we need some more restrictions on , such that it is also a slowly varying function. If were slowly varying, then would be regularly varying with coefficient . And . Since and is regularly varying, is slowly varying. Hence, is slowly varying. But, of course, slowly varying functions aren't necessarily bounded.","X F (\Omega, \mathcal F, P) \alpha \in (0,2) \ell(\cdot) 
\bar F(x) := 1 - F(x) = \frac{C_1(x)}{x^\alpha} \ell(x) \quad \text{and} \quad F(-x) = \frac{C_2(x)}{x^\alpha} \ell(x) \quad \text{for x > 0,}
 C_1(\cdot), C_2(\cdot) C_i := \lim_{x \to \infty} C_i(x) C_1 + C_2 > 0 C, \tilde C > 0  \sum_{n=1}^\infty P\big( |X| > a_n \big) \leq \sum_{n=1}^\infty \frac{C}{nf(n)} \leq \tilde C \int_1^\infty \frac{dt}{t f(t)}, 
 a_n := [n f(n) \ell(n) ]^{1/\alpha} f  \limsup_{t \to \infty} \sup_{0 \leq t \leq x} \frac{f(t)}{f(x)} < \infty \quad \text{and} \quad \int_1^\infty \frac{dt}{tf(t)}< \infty.
 f \rho^* F 
\begin{aligned}
\sum_{n=1}^\infty P\big( |X| > a_n \big) &= \sum_{n=1}^\infty \Big[ P\big( X > a_n \big) + P\big( X < - a_n \big)  \Big] \\
&\leq \sum_{n=1}^\infty \Big[ \bar F(a_n) + F(-a_n)  \Big] \\
&= \sum_{n=1}^\infty \frac{C_1(a_n) + C_2(a_n)}{a_n^\alpha} \ell(a_n) \\
&\leq \sum_{n=1}^\infty \frac{C}{a_n^\alpha} \ell(a_n) = C \sum_{n=1}^\infty \frac{1}{nf(n)} \cdot \frac{\ell(a_n)}{\ell(n)},
\end{aligned}
 C>0 C_1(\cdot) C_2(\cdot) \frac{\ell(a_n)}{\ell(n)} = \ell\Big( \big[ n f(n) \ell(n) \big]^{1/\alpha} \Big) \Big/ \ell(n) \left\{ \frac{\ell(a_n)}{\ell(n)} \right\} f f u(x) := x^{1/\alpha}\cdot[f(x) \ell(x)]^{1/\alpha} 1/\alpha \frac{\ell(a_n)}{\ell(n)} = \frac{\ell\big(u(n)\big)}{\ell(n)} u(x) \to \infty \ell \circ u \frac{\ell(a_n)}{\ell(n)} = \frac{ \ell \big(u(n) \big)}{\ell(n)}","['real-analysis', 'probability-theory', 'asymptotics', 'slowly-varying-functions']"
50,Show convergence of measure of a sequence of sets,Show convergence of measure of a sequence of sets,,"Let $\lambda$ denote the Lebesgue measure on $\mathcal B(\mathbb R)$ and $(E,\mathcal E,\alpha)$ be a measure space. Moreover, let $B\in\mathcal B(\mathbb R)\otimes\mathcal E$ and $$\lambda_{s,\:t}:=(\lambda\otimes\alpha)(B\cap((s,t]\times E))$$ for $t\ge s\ge0$ . Let $T>0$ . Are we able to show that $$\sum_{i=1}^{kT}\left[1-\left(1+\lambda_{\frac{i-1}k,\:\frac ik}\right)e^{-\lambda_{\frac{i-1}k,\:\frac ik}}\right]\xrightarrow{k\to\infty}0?\tag2$$ We've clearly got $$\lim_{x\to0}\frac{1-(1+x)e^{-x}}x=\lim_{x\to0}xe^{-x}=0\tag3,$$ but the situation in $(2)$ is slightly different. Using $(1)$ , we have $$1-\left(1+\lambda_{\frac{i-1}k,\:\frac ik}\right)e^{-\lambda_{\frac{i-1}k,\:\frac ik}}\le1-\left(1+\lambda_{\frac{i-1}k,\:\frac ik}\right)e^{-\frac\alpha k}\tag4,$$ but this is obviously not sharp enough (just consider $B=\emptyset$ ). Note that the claim is easy to establish (using $(3)$ ) when $B$ is of the form $B=\mathbb R\times A$ for some $A\in\mathcal E$ .","Let denote the Lebesgue measure on and be a measure space. Moreover, let and for . Let . Are we able to show that We've clearly got but the situation in is slightly different. Using , we have but this is obviously not sharp enough (just consider ). Note that the claim is easy to establish (using ) when is of the form for some .","\lambda \mathcal B(\mathbb R) (E,\mathcal E,\alpha) B\in\mathcal B(\mathbb R)\otimes\mathcal E \lambda_{s,\:t}:=(\lambda\otimes\alpha)(B\cap((s,t]\times E)) t\ge s\ge0 T>0 \sum_{i=1}^{kT}\left[1-\left(1+\lambda_{\frac{i-1}k,\:\frac ik}\right)e^{-\lambda_{\frac{i-1}k,\:\frac ik}}\right]\xrightarrow{k\to\infty}0?\tag2 \lim_{x\to0}\frac{1-(1+x)e^{-x}}x=\lim_{x\to0}xe^{-x}=0\tag3, (2) (1) 1-\left(1+\lambda_{\frac{i-1}k,\:\frac ik}\right)e^{-\lambda_{\frac{i-1}k,\:\frac ik}}\le1-\left(1+\lambda_{\frac{i-1}k,\:\frac ik}\right)e^{-\frac\alpha k}\tag4, B=\emptyset (3) B B=\mathbb R\times A A\in\mathcal E","['real-analysis', 'measure-theory', 'exponential-function']"
51,"Basic properties of the Glasser function $G(x)=\int_0^x \sin(t\sin t)\,{\rm d}t$",Basic properties of the Glasser function,"G(x)=\int_0^x \sin(t\sin t)\,{\rm d}t","This question brought to my attention the Glasser function $G(x)=\int_0^x \sin(t\sin t)\,\mathrm{d}t$ . Surely there are many variations and generalizations we could look into, but this one seems to have a name already so it's a nice place to start. The MathWorld article says this is problem 785 listed in a 1990 volume of Nieuw Archief voor Wiskunde (New Archive for Mathematics), which is probably only available physically overseas. It's also listed as problem 12767 in Genautica's compilation of 20,000 math problems, with a defunct hyperlink to umr.edu. Anyway, look at the graph. I'd classify the crests/valleys (i.e. local maxima and minima resp.) into two types: major and minor. A major valley always immediately precedes a major crest. There are zero minor crests or valleys between the first and second major crests, one between the second and third major crest, two between the third and fourth major crests, and so on. According to the MathWorld article: This valley-crest pattern holds indefinitely. We have the asymptotic $G(x)\sim 2\sqrt{x/\pi}$ Question . How do we justify these two properties? The extrema occur where $G'(x)=\sin(x\sin x)=0$ . I made a table of all solutions in the interval $[0,20]$ , labelled the extrema (MC = major crest, MV = major valley, mc = minor crest, mv = minor valley), and tagged each $x$ with the integer value of $k$ in $x\sin x=k\pi$ . The first thing that jumped out to me is how the values where $k=0$ are not the major extrema (which was my first guess before I checked out the second's $x$ coordinate), but rather just before the middle of the minor extrema! Indeed, from left to right, the value of $k$ (as a function of $x$ ) bounces down and up and down and up and so on, exactly one more each time. Dunno how to prove any of this, though. I'm not actually sure how to analytically even define what's a major extrema vs. a minor one. Indeed, an asymptotic series would be even more welcome than just the asymptotic. The only situation I've encountered for such asymptotic series involved judicious choices of integration-by-parts, but I have no idea what kind of parts to use for such a strange integrand. Also interesting is the constant $\sqrt{\pi}/2$ is the Gaussian integral $\int_0^\infty e^{-t^2}\,\mathrm{d}t$ . Relevant? From the graph (especially extending out to to $x=60$ or beyond), it seems like we could write $G(x)=g(x)+\phi(x)+\psi(x)$ , where: $g(x)$ is an increasing step function (like the prime counting function) whose jumps occur between adjacent major valley/crests and whose level heights are about the midpoint of the sequence of minor valleys/crests between major ones; $\phi(x)$ is oscillatory, with shrinking amplitude but otherwise about constant frequency, representing the major valleys/crests; and $\psi(x)$ is oscillatory with shrinking amplitude and linearly increasing frequency (so to speak), representing the minor valleys/crests.","This question brought to my attention the Glasser function . Surely there are many variations and generalizations we could look into, but this one seems to have a name already so it's a nice place to start. The MathWorld article says this is problem 785 listed in a 1990 volume of Nieuw Archief voor Wiskunde (New Archive for Mathematics), which is probably only available physically overseas. It's also listed as problem 12767 in Genautica's compilation of 20,000 math problems, with a defunct hyperlink to umr.edu. Anyway, look at the graph. I'd classify the crests/valleys (i.e. local maxima and minima resp.) into two types: major and minor. A major valley always immediately precedes a major crest. There are zero minor crests or valleys between the first and second major crests, one between the second and third major crest, two between the third and fourth major crests, and so on. According to the MathWorld article: This valley-crest pattern holds indefinitely. We have the asymptotic Question . How do we justify these two properties? The extrema occur where . I made a table of all solutions in the interval , labelled the extrema (MC = major crest, MV = major valley, mc = minor crest, mv = minor valley), and tagged each with the integer value of in . The first thing that jumped out to me is how the values where are not the major extrema (which was my first guess before I checked out the second's coordinate), but rather just before the middle of the minor extrema! Indeed, from left to right, the value of (as a function of ) bounces down and up and down and up and so on, exactly one more each time. Dunno how to prove any of this, though. I'm not actually sure how to analytically even define what's a major extrema vs. a minor one. Indeed, an asymptotic series would be even more welcome than just the asymptotic. The only situation I've encountered for such asymptotic series involved judicious choices of integration-by-parts, but I have no idea what kind of parts to use for such a strange integrand. Also interesting is the constant is the Gaussian integral . Relevant? From the graph (especially extending out to to or beyond), it seems like we could write , where: is an increasing step function (like the prime counting function) whose jumps occur between adjacent major valley/crests and whose level heights are about the midpoint of the sequence of minor valleys/crests between major ones; is oscillatory, with shrinking amplitude but otherwise about constant frequency, representing the major valleys/crests; and is oscillatory with shrinking amplitude and linearly increasing frequency (so to speak), representing the minor valleys/crests.","G(x)=\int_0^x \sin(t\sin t)\,\mathrm{d}t G(x)\sim 2\sqrt{x/\pi} G'(x)=\sin(x\sin x)=0 [0,20] x k x\sin x=k\pi k=0 x k x \sqrt{\pi}/2 \int_0^\infty e^{-t^2}\,\mathrm{d}t x=60 G(x)=g(x)+\phi(x)+\psi(x) g(x) \phi(x) \psi(x)","['real-analysis', 'definite-integrals', 'fourier-analysis', 'asymptotics', 'special-functions']"
52,Local max of ratio of elementary symmetric polynomials,Local max of ratio of elementary symmetric polynomials,,"Let $e_k(x_1,\cdots, x_n) := \sum_{1\leq j_1 < \cdots < j_k \leq n}x_{j_1}\cdots x_{j_k}$ be elementary symmetric polynomials (see e.g. https://en.wikipedia.org/wiki/Elementary_symmetric_polynomial ). For any fixed $0<x_1\leq x_2 \leq \cdots \leq x_n \in \mathbb{R}$ let's define the following: \begin{equation} s_k := \frac{1}{k}\left(1 + \frac{e_{n-k+1}(x_1,\cdots,x_n)}{e_{n-k}(x_1,\cdots,x_n)}\right) \end{equation} for $k = 1,\cdots,n$ . I would like to know whether the following is true: There exists no $k = 2,\cdots,n-1$ such that $s_k > s_{k-1}$ and $s_k > s_{k+1}$ . (Prove or Disprove) In other words, the sequence $s_k$ has no 'local maximum', for any given fixed $0<x_1\leq x_2 \leq \cdots \leq x_n$ . I could verify in the simplest case where $x_1 = x_2 = \cdots = x_n =: x$ that this statement holds, we have $e_k(x,\cdots,x) = (n!/k!(n-k)!)x^k$ , and so \begin{equation} s_k = \frac{1}{k}\left(1 + \frac{(n-k)!k!}{(n-k+1)!(k-1)!}x\right) = \frac{1}{k}\left(1 + \frac{k}{n-k+1}x\right) = \frac{1}{k} + \frac{x}{n-k+1}. \end{equation} In this case, for any $k = 2,\cdots,n-1$ , we will either have $s_{k-1} \leq s_k \leq s_{k+1}$ , or $s_{k-1} \geq s_k \geq s_{k+1}$ , or $s_k \leq s_{k-1}, s_{k+1}$ , but not $s_k > s_{k-1}, s_{k+1}$ . I'm not sure how to prove (or disprove) the statement in general, but I numerically compute $s_k$ for randomized $0 < x_1 \leq x_2 \leq \cdots \leq x_n$ many times and the statement appears to be true so far. Below is the plot of an example computed $s_k$ with $n = 20$ for a sample set of $0<x_1\leq x_2\leq \cdots\leq x_n$ : as you can see there is no local maximum. If someone could help or suggest ways to either prove or disprove the statement I would really be appreciated!","Let be elementary symmetric polynomials (see e.g. https://en.wikipedia.org/wiki/Elementary_symmetric_polynomial ). For any fixed let's define the following: for . I would like to know whether the following is true: There exists no such that and . (Prove or Disprove) In other words, the sequence has no 'local maximum', for any given fixed . I could verify in the simplest case where that this statement holds, we have , and so In this case, for any , we will either have , or , or , but not . I'm not sure how to prove (or disprove) the statement in general, but I numerically compute for randomized many times and the statement appears to be true so far. Below is the plot of an example computed with for a sample set of : as you can see there is no local maximum. If someone could help or suggest ways to either prove or disprove the statement I would really be appreciated!","e_k(x_1,\cdots, x_n) := \sum_{1\leq j_1 < \cdots < j_k \leq n}x_{j_1}\cdots x_{j_k} 0<x_1\leq x_2 \leq \cdots \leq x_n \in \mathbb{R} \begin{equation}
s_k := \frac{1}{k}\left(1 + \frac{e_{n-k+1}(x_1,\cdots,x_n)}{e_{n-k}(x_1,\cdots,x_n)}\right)
\end{equation} k = 1,\cdots,n k = 2,\cdots,n-1 s_k > s_{k-1} s_k > s_{k+1} s_k 0<x_1\leq x_2 \leq \cdots \leq x_n x_1 = x_2 = \cdots = x_n =: x e_k(x,\cdots,x) = (n!/k!(n-k)!)x^k \begin{equation}
s_k = \frac{1}{k}\left(1 + \frac{(n-k)!k!}{(n-k+1)!(k-1)!}x\right) = \frac{1}{k}\left(1 + \frac{k}{n-k+1}x\right) = \frac{1}{k} + \frac{x}{n-k+1}.
\end{equation} k = 2,\cdots,n-1 s_{k-1} \leq s_k \leq s_{k+1} s_{k-1} \geq s_k \geq s_{k+1} s_k \leq s_{k-1}, s_{k+1} s_k > s_{k-1}, s_{k+1} s_k 0 < x_1 \leq x_2 \leq \cdots \leq x_n s_k n = 20 0<x_1\leq x_2\leq \cdots\leq x_n","['real-analysis', 'sequences-and-series', 'combinatorics', 'symmetric-polynomials']"
53,Proof that f = g a.e. with Fatou's lemma,Proof that f = g a.e. with Fatou's lemma,,"In the book ""A user friendly introduction to Lebesgue measure and integration"" by Nelson, Exercise 24 in Ch. 2 states: Let $(f_n)$ be a sequence of functions in $\mathcal L [a,b]$ . Suppose $f \in \mathcal L [a,b]$ and $$ \lim_{n\to \infty}\int_a^b|f_n-f| = 0. $$ If the sequence $(f_n)$ converges pointwise a.e. on $[a,b]$ to the function $g$ , show that $f=g$ a.e. on $[a,b]$ . Suggestion: Consider the sequence $(|f-f_n|)$ and Fatou's Lemma. My try: From $\lim_{n\to\infty} f_n  =g$ a.e., we know that $\lim_{n\to\infty}  f_n -f  =  g - f$ a.e. and $\lim_{n\to\infty} | f_n -f | = | g - f|$ a.e.. Integrating, we have \begin{align} 0\leq \int_a^b|g-f|&=\int_a^b\lim_{n\to\infty}|f_n -f|=\int_a^b\liminf_{n\to\infty}|f_n -f|\\ &\leq \liminf_{n\to\infty}\int_a^b|f_n -f| = 0, \end{align} where the second inequality follows from Fatou's lemma and the last step from the assumption that $\lim_{n\to \infty}\int_a^b|f_n-f| = 0$ . We conclude that $|g-f| = 0$ a.e.. Is this the correct direction to approach the exercise? I don't see how to go from the above result to showing that $f=g$ a.e.. I see that $\int_a^b(g-f) \leq \int_a^b|g-f|$ ...","In the book ""A user friendly introduction to Lebesgue measure and integration"" by Nelson, Exercise 24 in Ch. 2 states: Let be a sequence of functions in . Suppose and If the sequence converges pointwise a.e. on to the function , show that a.e. on . Suggestion: Consider the sequence and Fatou's Lemma. My try: From a.e., we know that a.e. and a.e.. Integrating, we have where the second inequality follows from Fatou's lemma and the last step from the assumption that . We conclude that a.e.. Is this the correct direction to approach the exercise? I don't see how to go from the above result to showing that a.e.. I see that ...","(f_n) \mathcal L [a,b] f \in \mathcal L [a,b] 
\lim_{n\to \infty}\int_a^b|f_n-f| = 0.
 (f_n) [a,b] g f=g [a,b] (|f-f_n|) \lim_{n\to\infty} f_n  =g \lim_{n\to\infty}  f_n -f  =  g - f \lim_{n\to\infty} | f_n -f | = | g - f| \begin{align}
0\leq \int_a^b|g-f|&=\int_a^b\lim_{n\to\infty}|f_n -f|=\int_a^b\liminf_{n\to\infty}|f_n -f|\\
&\leq \liminf_{n\to\infty}\int_a^b|f_n -f| = 0,
\end{align} \lim_{n\to \infty}\int_a^b|f_n-f| = 0 |g-f| = 0 f=g \int_a^b(g-f) \leq \int_a^b|g-f|","['real-analysis', 'measure-theory', 'lebesgue-integral']"
54,Direct Proof of Archimedian Property,Direct Proof of Archimedian Property,,"I've written this proof and I'd like to ask if this is a valid proof of the Archimedean Property: $\underline{\text{Claim}:}$ $\forall x \in \mathbb{R}\exists n_x \in \mathbb{N}:x \leq n_x$ . $\underline{\text{Proof}:}$ We know that $1 \in \mathbb{N}$ and $1 \in \mathbb{R}$ . Thus, if $x \leq 1$ then $n_x = 1$ and we are done. Now assume, $x > 1$ . Let M = $\{k \in \mathbb{N} : k < x\}$ . Then $1 \in M$ . Thus, $M\neq \varnothing$ and is bounded above by $x$ . By the completeness property, we can then infer that sup $M$ exists. If we let $u = \text{sup}M -1 $ then we know that $\exists k \in M$ such that $k > u$ . $\therefore\text{sup}M < k + 1$ , which is a natural number by the inductive property of natural numbers. Since, $k + 1 > \text{sup}M$ , we have that $k + 1 \not \in M$ . By setting $n_x = k + 1$ , we have proved the Archimedean Property. $\square$","I've written this proof and I'd like to ask if this is a valid proof of the Archimedean Property: . We know that and . Thus, if then and we are done. Now assume, . Let M = . Then . Thus, and is bounded above by . By the completeness property, we can then infer that sup exists. If we let then we know that such that . , which is a natural number by the inductive property of natural numbers. Since, , we have that . By setting , we have proved the Archimedean Property.",\underline{\text{Claim}:} \forall x \in \mathbb{R}\exists n_x \in \mathbb{N}:x \leq n_x \underline{\text{Proof}:} 1 \in \mathbb{N} 1 \in \mathbb{R} x \leq 1 n_x = 1 x > 1 \{k \in \mathbb{N} : k < x\} 1 \in M M\neq \varnothing x M u = \text{sup}M -1  \exists k \in M k > u \therefore\text{sup}M < k + 1 k + 1 > \text{sup}M k + 1 \not \in M n_x = k + 1 \square,"['real-analysis', 'solution-verification', 'real-numbers']"
55,On the construction of a measure $\mu$ on $\mathbb R^n$ by repeated subdivision,On the construction of a measure  on  by repeated subdivision,\mu \mathbb R^n,"Main Theme of the Question(s): Fractal Geometry by Kenneth Falconer describes the following method to construct a mass distribution $\mu$ (and later, a measure on $\mathbb R^n$ ) on a subset of $\mathbb R^n$ . The author also says that we only concern ourselves with Borel sets. I need help understanding (i) the process of construction of this mass-distribution/measure, and (ii) the proof of a proposition that follows this discussion. Slightly paraphrased from the book: The following method is often used to construct a mass distribution on a subset of $ℝ^n$ . It involves repeated subdivision of a mass between parts of a bounded Borel set $E$ . Let $\mathcal{E}_0$ consist of the single set $E$ . For $k = 1, 2, . . . $ , we let $\mathcal{E}_k$ be a collection of disjoint Borel subsets of $E$ such that each set $U$ in $\mathcal{E}_k$ is contained in one of the sets of $\mathcal{E}_{k−1}$ and contains a finite number of the sets in $\mathcal{E}_{k+1}$ . We assume that the maximum diameter of the sets in $\mathcal{E}_k$ tends to $0$ as $k \to \infty$ . We define a mass distribution on $E$ by repeated subdivision. We let $\mu(E) \in (0,\infty)$ . In general, we assign masses so that $$\sum_i\mu(U_i) = \mu(U)$$ for each set $U$ of $\mathcal{E}_k$ , where $\{U_i\}$ are disjoint sets in $\mathcal{E}_{k+1}$ contained in $U$ . For each $k$ , let $E_k = \bigcup_{A\in \mathcal{E}_k} A$ , and we define $\mu(A) = 0$ for all $A$ with $A\cap E_k = \varnothing$ . Now the author states a proposition, which enables us to determine $\mu(A)$ for any Borel set $A$ . Proposition $1.7.$ Let $𝜇$ be defined on a collection of sets $\mathcal E$ as above. Then the definition of $𝜇$ may be extended to all subsets of $ℝ^n$ so that $𝜇$ becomes a measure. The value of $𝜇(A)$ is uniquely determined if $A$ is a Borel set. The support of $𝜇$ is contained in $E_∞ = \bigcap_{k=1}^\infty \overline{E_k}$ . Note on Proof. If $A \subset \mathbb R^n$ , let $$\mu(A) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ It is not difficult to see that if $A\in\mathcal E$ , then $\mu(A)$ above reduces to the mass $𝜇(A)$ specified in the construction. The complete proof that $𝜇$ satisfies all the conditions of a measure and that its values on the sets of $\mathcal E$ determine its values on the Borel sets is quite involved, and need not concern us here. As $𝜇(ℝ^n\setminus E_k) = 0$ , we have $𝜇(A) = 0$ if $A$ is an open set that does not intersect $E_k$ for some $k$ , so the support of $𝜇$ is in $E_k$ for all $k$ . I have six questions: In the proposition above, why is $\mu(A)$ defined the way it is (to extend $\mu$ to sets other than those in $\mathcal E$ )? How do I show that $\mu$ as defined above is in fact a measure on $\mathbb R^n$ ? The author says the proof is involved, but I want to see it. I need to prove three things: (a) $\mu(\varnothing) = 0$ , (b) $A \subset B \implies \mu(A) \le \mu(B)$ , (c) $\mu\left(\bigcup_{i=1}^\infty A_i\right) \le \sum_{i=1}^\infty \mu(A_i)$ with equality if $A_i$ 's are disjoint Borel sets. Here's what I've tried: (a) $$\begin{align} \mu(\varnothing) &= \inf\left\{\sum_{i=1}^\infty \mu(U_i): \varnothing \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}\\ &= \inf\left\{\sum_{i=1}^\infty \mu(U_i): U_i \in \mathcal{E}\right\} \end{align}$$ Why should $\inf\left\{\sum_{i=1}^\infty \mu(U_i): U_i \in \mathcal{E}\right\}$ be $0$ ? I'm not sure if $\varnothing\in\mathcal E$ . (b) Suppose $A\subset B$ . Then, $A\cap E_\infty \subset B\cap E_\infty$ . We have $$\mu(A) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ and $$\mu(B) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): B \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ and from what I understand - $A\cap E_\infty \subset B\cap E_\infty$ directly implies $\mu(A) \le \mu(B)$ . This is due to the fact that any cover $\bigcup_{i=1}^\infty U_i$ of $B\cap E_\infty$ is also a cover of $A\cap E_\infty$ , but the converse is not true in general. Since $A\cap E_\infty$ can possibly have more covers of the form $\bigcup_{i=1}^\infty U_i$ for $U_i\in\mathcal E$ , the infimum of $\sum_{i=1}^\infty \mu(U_i)$ over covers of $A\cap E_\infty$ is in general $\le$ the infimum of $\sum_{i=1}^\infty \mu(U_i)$ over covers of $B\cap E_\infty$ . Does this make sense? (c)We have $$\mu(A_i) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A_i \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ and $$\mu\left(\bigcup_{i=1}^\infty A_i\right) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): \bigcup_{i=1}^\infty A_i \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ Clearly, $A_i \subset \bigcup_{i=1}^\infty A_i$ , and from the previous part, i.e. (b), we have $\mu(A_i) \le \mu\left(\bigcup_{i=1}^\infty A_i\right)$ . I'm not sure how this helps, and I'm unable to proceed further. Why does $E_∞ = \bigcap_{k=1}^\infty \overline{E_k}$ contain $\operatorname{spt}\mu$ ? $\color{blue}{\text{Answered.}}$ As $\mu(\mathbb R^n \setminus E_k) = 0$ , by monotonicity, we have $\mu(\Bbb R^n\setminus \overline{E_k}) = 0$ . By the definition of support, $\operatorname{spt}\mu \subset \overline{E_k}$ for all $k$ . So, $\operatorname{spt}\mu \subset E_\infty = \bigcap_{k=1}^\infty \overline{E_k}$ . If $A\in\mathcal E$ , how does $\mu(A)$ (given by the wild expression in the proposition) reduce to the construction of $\mu(A)$ (i.e. the mass distribution earlier)? How do we know that the value of $\mu$ on sets of $\mathcal E$ determines $\mu$ uniquely on all Borel sets? What is the point of all this? What have we achieved? I need help understanding the bigger picture! Thanks a lot for reading, and for your help (in advance)! References: Kenneth Falconer. Fractal Geometry.","Main Theme of the Question(s): Fractal Geometry by Kenneth Falconer describes the following method to construct a mass distribution (and later, a measure on ) on a subset of . The author also says that we only concern ourselves with Borel sets. I need help understanding (i) the process of construction of this mass-distribution/measure, and (ii) the proof of a proposition that follows this discussion. Slightly paraphrased from the book: The following method is often used to construct a mass distribution on a subset of . It involves repeated subdivision of a mass between parts of a bounded Borel set . Let consist of the single set . For , we let be a collection of disjoint Borel subsets of such that each set in is contained in one of the sets of and contains a finite number of the sets in . We assume that the maximum diameter of the sets in tends to as . We define a mass distribution on by repeated subdivision. We let . In general, we assign masses so that for each set of , where are disjoint sets in contained in . For each , let , and we define for all with . Now the author states a proposition, which enables us to determine for any Borel set . Proposition Let be defined on a collection of sets as above. Then the definition of may be extended to all subsets of so that becomes a measure. The value of is uniquely determined if is a Borel set. The support of is contained in . Note on Proof. If , let It is not difficult to see that if , then above reduces to the mass specified in the construction. The complete proof that satisfies all the conditions of a measure and that its values on the sets of determine its values on the Borel sets is quite involved, and need not concern us here. As , we have if is an open set that does not intersect for some , so the support of is in for all . I have six questions: In the proposition above, why is defined the way it is (to extend to sets other than those in )? How do I show that as defined above is in fact a measure on ? The author says the proof is involved, but I want to see it. I need to prove three things: (a) , (b) , (c) with equality if 's are disjoint Borel sets. Here's what I've tried: (a) Why should be ? I'm not sure if . (b) Suppose . Then, . We have and and from what I understand - directly implies . This is due to the fact that any cover of is also a cover of , but the converse is not true in general. Since can possibly have more covers of the form for , the infimum of over covers of is in general the infimum of over covers of . Does this make sense? (c)We have and Clearly, , and from the previous part, i.e. (b), we have . I'm not sure how this helps, and I'm unable to proceed further. Why does contain ? As , by monotonicity, we have . By the definition of support, for all . So, . If , how does (given by the wild expression in the proposition) reduce to the construction of (i.e. the mass distribution earlier)? How do we know that the value of on sets of determines uniquely on all Borel sets? What is the point of all this? What have we achieved? I need help understanding the bigger picture! Thanks a lot for reading, and for your help (in advance)! References: Kenneth Falconer. Fractal Geometry.","\mu \mathbb R^n \mathbb R^n ℝ^n E \mathcal{E}_0 E k = 1, 2, . . .  \mathcal{E}_k E U \mathcal{E}_k \mathcal{E}_{k−1} \mathcal{E}_{k+1} \mathcal{E}_k 0 k \to \infty E \mu(E) \in (0,\infty) \sum_i\mu(U_i) = \mu(U) U \mathcal{E}_k \{U_i\} \mathcal{E}_{k+1} U k E_k = \bigcup_{A\in \mathcal{E}_k} A \mu(A) = 0 A A\cap E_k = \varnothing \mu(A) A 1.7. 𝜇 \mathcal E 𝜇 ℝ^n 𝜇 𝜇(A) A 𝜇 E_∞ = \bigcap_{k=1}^\infty \overline{E_k} A \subset \mathbb R^n \mu(A) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\} A\in\mathcal E \mu(A) 𝜇(A) 𝜇 \mathcal E 𝜇(ℝ^n\setminus E_k) = 0 𝜇(A) = 0 A E_k k 𝜇 E_k k \mu(A) \mu \mathcal E \mu \mathbb R^n \mu(\varnothing) = 0 A \subset B \implies \mu(A) \le \mu(B) \mu\left(\bigcup_{i=1}^\infty A_i\right) \le \sum_{i=1}^\infty \mu(A_i) A_i \begin{align}
\mu(\varnothing) &= \inf\left\{\sum_{i=1}^\infty \mu(U_i): \varnothing \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}\\
&= \inf\left\{\sum_{i=1}^\infty \mu(U_i): U_i \in \mathcal{E}\right\}
\end{align} \inf\left\{\sum_{i=1}^\infty \mu(U_i): U_i \in \mathcal{E}\right\} 0 \varnothing\in\mathcal E A\subset B A\cap E_\infty \subset B\cap E_\infty \mu(A) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\} \mu(B) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): B \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\} A\cap E_\infty \subset B\cap E_\infty \mu(A) \le \mu(B) \bigcup_{i=1}^\infty U_i B\cap E_\infty A\cap E_\infty A\cap E_\infty \bigcup_{i=1}^\infty U_i U_i\in\mathcal E \sum_{i=1}^\infty \mu(U_i) A\cap E_\infty \le \sum_{i=1}^\infty \mu(U_i) B\cap E_\infty \mu(A_i) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A_i \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\} \mu\left(\bigcup_{i=1}^\infty A_i\right) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): \bigcup_{i=1}^\infty A_i \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\} A_i \subset \bigcup_{i=1}^\infty A_i \mu(A_i) \le \mu\left(\bigcup_{i=1}^\infty A_i\right) E_∞ = \bigcap_{k=1}^\infty \overline{E_k} \operatorname{spt}\mu \color{blue}{\text{Answered.}} \mu(\mathbb R^n \setminus E_k) = 0 \mu(\Bbb R^n\setminus \overline{E_k}) = 0 \operatorname{spt}\mu \subset \overline{E_k} k \operatorname{spt}\mu \subset E_\infty = \bigcap_{k=1}^\infty \overline{E_k} A\in\mathcal E \mu(A) \mu(A) \mu \mathcal E \mu","['real-analysis', 'analysis', 'measure-theory', 'borel-sets', 'borel-measures']"
56,Is the image of this function open?,Is the image of this function open?,,"Suppose $f: \mathbb{R}^n \to \mathbb{R}^n$ is continuous and for some $\lambda>0$ , we have $\|f(x)-f(y)\|\geq \lambda \|x-y\|$ for all $x,y\in\mathbb{R}^{n}$ . Is $f$ surjective? I can show the image is closed by showing it contains its limit points. So, if I can show it is also open, then I am done (I want to avoid using the Invariance of Domain theorem). Obviously, $f$ is one-to-one. By considering the the inverse map defined from $E=f(\mathbb{R}^n)$ back to $\mathbb{R}^n$ we get the following equivalent formulation of the problem: Suppose $E \subset \mathbb{R}^n$ is closed and $f\colon E \to \mathbb{R}^n$ is Lipschitz, one-to-one, and onto. Is $E$ necessarily equal to all of $\mathbb{R}^n$ ?","Suppose is continuous and for some , we have for all . Is surjective? I can show the image is closed by showing it contains its limit points. So, if I can show it is also open, then I am done (I want to avoid using the Invariance of Domain theorem). Obviously, is one-to-one. By considering the the inverse map defined from back to we get the following equivalent formulation of the problem: Suppose is closed and is Lipschitz, one-to-one, and onto. Is necessarily equal to all of ?","f: \mathbb{R}^n \to \mathbb{R}^n \lambda>0 \|f(x)-f(y)\|\geq \lambda \|x-y\| x,y\in\mathbb{R}^{n} f f E=f(\mathbb{R}^n) \mathbb{R}^n E \subset \mathbb{R}^n f\colon E \to \mathbb{R}^n E \mathbb{R}^n","['real-analysis', 'calculus', 'multivariable-calculus', 'lipschitz-functions']"
57,How to prove this inequality $\left|x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}\right|<2\sqrt{|x-y|}$?,How to prove this inequality ?,\left|x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}\right|<2\sqrt{|x-y|},"For any real numbers $x,y\neq 0$ ,show that $$\left|x\sin{\dfrac{1}{x}}-y\sin{\dfrac{1}{y}}\right|<2\sqrt{|x-y|}$$ I found this problem when I dealt with this problem . But I can't prove it. Maybe the constant $2$ on the right hand side can be replaced by the better constant $\sqrt{2}$ ? Thank you.","For any real numbers ,show that I found this problem when I dealt with this problem . But I can't prove it. Maybe the constant on the right hand side can be replaced by the better constant ? Thank you.","x,y\neq 0 \left|x\sin{\dfrac{1}{x}}-y\sin{\dfrac{1}{y}}\right|<2\sqrt{|x-y|} 2 \sqrt{2}",['inequality']
58,Limits involving nested roots,Limits involving nested roots,,"Let $x>0$ . Let $a_1=\sqrt{x}, a_{n+1}=\sqrt{x+a_n}$ . So $a_2=\sqrt{x+\sqrt{x}}, a_3=\sqrt{x+\sqrt{x+\sqrt{x}}}$ and so on. Let $t:=\frac{1+\sqrt{1+4x}}2$ . It's easy to show that: $$(1) \lim_{n \rightarrow \infty} a_n = t$$ $$(2) \lim_{n \rightarrow \infty} \frac{t-a_n}{t-a_{n+1}}=2t$$ So one might try calculating: $$f(x):=\lim_{n \rightarrow \infty} (2t)^n (t-a_n)$$ I took interest in this kind of limits since $f(2)=\frac{\pi^2}4$ via Viète's formula. I was able to approximate some other limits, but found nothing meaningful so far. Has this function been considered before? Do you have any hints on how to analyze it? EDIT: It may be helpful to look at $f(x)$ as a result of a single recurrency: $$b_1=(2t)(t-\sqrt{x})$$ $$b_{n+1}=(2t)^{n+1}(t-\sqrt{x+t-\frac{b_n}{(2t)^n}})$$ $$f(x)=\lim_{n \rightarrow \infty } b_n$$ EDIT2: Please note that $(t+\sqrt{x+t-\frac{b_n}{(2t)^n}})b_{n+1}=(2t)b_n$ . Since $\sqrt{x+t-\frac{b_n}{(2t)^n}} < t$ , we know that $b_{n+1}>b_n$ . Furthermore, $(t+\sqrt{x+t-\frac{b_n}{(2t)^n}})(b_{n+1}-b_n)=(2t)b_n-(t+\sqrt{x+t-\frac{b_n}{(2t)^n}}) b_n=(t-\sqrt{x+t-\frac{b_n}{(2t)^n}})b_{n}=\frac{b_nb_{n+1}}{(2t)^{n+1}}$ This can be weakened to: $b_{n+1}-b_n \leq \frac{f^2(x)}{(2t)^{n+2}}$ Which means that: $$f(x) \leq b_1 + \sum_{i=1}^n \frac{f^2(x)}{(2t)^{n+2}}=b_1+\frac{f^2(x)}{4t^2-2t}$$ EDIT3: The determinant of this inequality is: $$\Delta:=1-4\frac{b_1}{4t^2-2t}=1-4\frac{2t(t-\sqrt{x})}{4t^2-2t}=1-4\frac{t-\sqrt{x}}{2t-1}$$ $\Delta \leq 0$ for $x \leq \frac{23+8\sqrt{7}}{36}$ . For $\Delta >0$ , this gives us two options: $f(x) \geq (4x+2t)(1+\Delta)$ $f(x) \leq (4x+2t)(1-\Delta)$ We already know that $f(2)=\frac{\pi^2}{4}$ -- this satisfies the second inequality. Since $f(x)$ is quite clearly continuous, we know that: $$f(x) \leq (2x+t)(1-\Delta) \text{  for  } x\geq\frac{23+8\sqrt{7}}{36}$$ $\lim_{x \rightarrow \infty} \frac{b_1}{\sqrt{x}} = \lim_{x \rightarrow \infty} (2x+t)(1-\Delta) = 1$ , so $\lim_{x \rightarrow \infty}\frac{f(x)}{\sqrt{x}}=1$ .","Let . Let . So and so on. Let . It's easy to show that: So one might try calculating: I took interest in this kind of limits since via Viète's formula. I was able to approximate some other limits, but found nothing meaningful so far. Has this function been considered before? Do you have any hints on how to analyze it? EDIT: It may be helpful to look at as a result of a single recurrency: EDIT2: Please note that . Since , we know that . Furthermore, This can be weakened to: Which means that: EDIT3: The determinant of this inequality is: for . For , this gives us two options: We already know that -- this satisfies the second inequality. Since is quite clearly continuous, we know that: , so .","x>0 a_1=\sqrt{x}, a_{n+1}=\sqrt{x+a_n} a_2=\sqrt{x+\sqrt{x}}, a_3=\sqrt{x+\sqrt{x+\sqrt{x}}} t:=\frac{1+\sqrt{1+4x}}2 (1) \lim_{n \rightarrow \infty} a_n = t (2) \lim_{n \rightarrow \infty} \frac{t-a_n}{t-a_{n+1}}=2t f(x):=\lim_{n \rightarrow \infty} (2t)^n (t-a_n) f(2)=\frac{\pi^2}4 f(x) b_1=(2t)(t-\sqrt{x}) b_{n+1}=(2t)^{n+1}(t-\sqrt{x+t-\frac{b_n}{(2t)^n}}) f(x)=\lim_{n \rightarrow \infty } b_n (t+\sqrt{x+t-\frac{b_n}{(2t)^n}})b_{n+1}=(2t)b_n \sqrt{x+t-\frac{b_n}{(2t)^n}} < t b_{n+1}>b_n (t+\sqrt{x+t-\frac{b_n}{(2t)^n}})(b_{n+1}-b_n)=(2t)b_n-(t+\sqrt{x+t-\frac{b_n}{(2t)^n}}) b_n=(t-\sqrt{x+t-\frac{b_n}{(2t)^n}})b_{n}=\frac{b_nb_{n+1}}{(2t)^{n+1}} b_{n+1}-b_n \leq \frac{f^2(x)}{(2t)^{n+2}} f(x) \leq b_1 + \sum_{i=1}^n \frac{f^2(x)}{(2t)^{n+2}}=b_1+\frac{f^2(x)}{4t^2-2t} \Delta:=1-4\frac{b_1}{4t^2-2t}=1-4\frac{2t(t-\sqrt{x})}{4t^2-2t}=1-4\frac{t-\sqrt{x}}{2t-1} \Delta \leq 0 x \leq \frac{23+8\sqrt{7}}{36} \Delta >0 f(x) \geq (4x+2t)(1+\Delta) f(x) \leq (4x+2t)(1-\Delta) f(2)=\frac{\pi^2}{4} f(x) f(x) \leq (2x+t)(1-\Delta) \text{  for  } x\geq\frac{23+8\sqrt{7}}{36} \lim_{x \rightarrow \infty} \frac{b_1}{\sqrt{x}} = \lim_{x \rightarrow \infty} (2x+t)(1-\Delta) = 1 \lim_{x \rightarrow \infty}\frac{f(x)}{\sqrt{x}}=1","['real-analysis', 'sequences-and-series']"
59,The series $\sum_{n=1}^{\infty} \sin(n^4) \sin(4^n)$ converges or not?,The series  converges or not?,\sum_{n=1}^{\infty} \sin(n^4) \sin(4^n),Does the series $$ \sum_{n=1}^{\infty} \sin(n^4) \sin(4^n) $$ converges or diverges? I've been stuck with this problem for a long while and I can't figure out anything. Even I tried to break the $\sin$ into its expansion and still no sign of progress. Really need some help.,Does the series converges or diverges? I've been stuck with this problem for a long while and I can't figure out anything. Even I tried to break the into its expansion and still no sign of progress. Really need some help., \sum_{n=1}^{\infty} \sin(n^4) \sin(4^n)  \sin,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
60,"$f:[0,1]\to[0,1]$ be a continuous function. Let $x_1\in[0,1]$ and define $x_{n+1}={\sum_{i=1}^n f(x_i)\over n}$.Prove, $\{x_n\}$ is convergent","be a continuous function. Let  and define .Prove,  is convergent","f:[0,1]\to[0,1] x_1\in[0,1] x_{n+1}={\sum_{i=1}^n f(x_i)\over n} \{x_n\}","I have tried a little bit which as follows- Since $f(x_n)\in[0,1]$ , $\{f(x_n)\}$ has a convergent subsequence say $y_n=f(x_{r_n})\ \forall n\in\Bbb{N}$ Let, $\lim y_n=l\implies \lim \frac{y_1+y_2+\cdots+y_n}{n}=l\implies \lim \frac{f(x_{r_1})+f(x_{r_2})+\cdots+f(x_{r_n})}{n}=l$ But I am getting no idea to proceed and prove the convergence of $\{x_n\}$ . I have also tried to prove the sequence to be cauchy which goes- $x_{m+1}-x_{n+1}={\sum_{i=1}^m f(x_i)\over m}-{\sum_{i=1}^n f(x_i)\over n}\le {\sum_{i=1}^m f(x_i)\over n}-{\sum_{i=1}^n f(x_i)\over n}$ (since $m\ge n)$ $\implies |x_{m+1}-x_{n+1}|\le \frac{|f(x_m)|+|f(x_{m-1})+\cdots+|f(x_{n+1})|}{n}\le\frac{m-n}{n}$ (since $f([0,1])\subseteq [0,1])$ Now, what will I get if I tend $m,n\to\infty$ ? But I need to do something more in the 2nd case since I have nowhere used the continuity of $f$ . Can anybody give an idea to prove it? Thanks for assistance in advance.","I have tried a little bit which as follows- Since , has a convergent subsequence say Let, But I am getting no idea to proceed and prove the convergence of . I have also tried to prove the sequence to be cauchy which goes- (since (since Now, what will I get if I tend ? But I need to do something more in the 2nd case since I have nowhere used the continuity of . Can anybody give an idea to prove it? Thanks for assistance in advance.","f(x_n)\in[0,1] \{f(x_n)\} y_n=f(x_{r_n})\ \forall n\in\Bbb{N} \lim y_n=l\implies \lim \frac{y_1+y_2+\cdots+y_n}{n}=l\implies \lim \frac{f(x_{r_1})+f(x_{r_2})+\cdots+f(x_{r_n})}{n}=l \{x_n\} x_{m+1}-x_{n+1}={\sum_{i=1}^m f(x_i)\over m}-{\sum_{i=1}^n f(x_i)\over n}\le {\sum_{i=1}^m f(x_i)\over n}-{\sum_{i=1}^n f(x_i)\over n} m\ge n) \implies |x_{m+1}-x_{n+1}|\le \frac{|f(x_m)|+|f(x_{m-1})+\cdots+|f(x_{n+1})|}{n}\le\frac{m-n}{n} f([0,1])\subseteq [0,1]) m,n\to\infty f","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'continuity']"
61,On the convergence of the series $\sum \frac{(-1)^{\lfloor ne\rfloor}}{n}$,On the convergence of the series,\sum \frac{(-1)^{\lfloor ne\rfloor}}{n},The question is pretty simple: It the series $\sum \frac{(-1)^{\lfloor ne\rfloor}}{n}$ convergent or   not ? As usual in this situation we let $S_n:=\sum_{k=0}^n(-1)^{\lfloor ne\rfloor}$ and apply an Abel transform to the original series. Thus the convergence of the original series is equivalent to that of $\sum \frac{S_n}{n^2}$ . We have the obvious bound $S_n = O(n)$ . However in order to conclude one might need to achieve a bound of the form $S_n=O(n^{\alpha})$ for some $\alpha <1$ . Does anyone know a proof of this fact? (Or a proof for the original quesiton).,The question is pretty simple: It the series convergent or   not ? As usual in this situation we let and apply an Abel transform to the original series. Thus the convergence of the original series is equivalent to that of . We have the obvious bound . However in order to conclude one might need to achieve a bound of the form for some . Does anyone know a proof of this fact? (Or a proof for the original quesiton).,\sum \frac{(-1)^{\lfloor ne\rfloor}}{n} S_n:=\sum_{k=0}^n(-1)^{\lfloor ne\rfloor} \sum \frac{S_n}{n^2} S_n = O(n) S_n=O(n^{\alpha}) \alpha <1,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'ceiling-and-floor-functions', 'equidistribution']"
62,Powerful Integral $\int_0^x\frac{t\ln(1-t)}{1+t^2}\ dt$,Powerful Integral,\int_0^x\frac{t\ln(1-t)}{1+t^2}\ dt,"This integral can be found in Cornel's book, (Almost) Impossible Integral, Sums and Series page $97$ where he showed that $$\int_0^x\frac{t\ln(1-t)}{1+t^2}\ dt=\frac14\left(\frac12\ln^2(1+x^2)-2\operatorname{Li}_2(x)+\frac12\operatorname{Li}_2(-x^2)+\operatorname{Li}_2\left(\frac{2x}{1+x^2}\right)\right)$$ by differentiating $\operatorname{Li}_2\left(\frac{2x}{1+x^2}\right)$ then integrating back. How magical is that? My approach: \begin{align} I&=\int_0^x\frac{t\ln(1-t)}{1+t^2}\ dt\overset{IBP}{=}\frac12\ln(1+x^2)\ln(1-x)+\frac12\int_0^x\frac{\ln(1+t^2)}{1-t}\ dt\\ &\overset{1-x=u}{=}\frac12\ln(1+x^2)\ln(1-x)-\frac12\int_{1}^{1-x}\frac{\ln(2+2u+u^2)}{u}\ du\\ &=\frac12\ln(1+x^2)\ln(1-x)-\Re\int_{1}^{1-x}\frac{\ln((1+i)-u)}{u}\ du \end{align} and by using $\displaystyle\int \frac{\ln(a-x)}{x}\ dx=\ln(a)\ln x-\operatorname{Li}_2\left(\frac{x}{a}\right)\ $ , we get \begin{align} I&=\frac12\ln(1+x^2)\ln(1-x)-\Re\left(\ln(1+i)\ln u-\operatorname{Li}_2\left(\frac{u}{1+i}\right)\right)_{u=1}^{u=1-x}\\ &=\frac12\ln(1+x^2)\ln(1-x)-\Re\left(\ln(1+i)\ln(1-x)-\operatorname{Li}_2\left(\frac{1-x}{1+i}\right)+\operatorname{Li}_2\left(\frac{1}{1+i}\right)\right)\\ &=\frac12\ln(1+x^2)\ln(1-x)+\frac12\ln2\ln(1-x)+\Re\operatorname{Li}_2\left(\frac{1-x}{1+i}\right)-\frac{\pi^2}{16}\\ &\boxed{=\frac12\ln\left(2(1+x^2)\right)\ln(1-x)+\Re\operatorname{Li}_2\left(\frac{1-x}{1+i}\right)-\frac{\pi^2}{16}} \end{align} where I used $\ \Re\ln(1+i)=\frac12\ln2\ $ and $\ \Re\operatorname{Li}_2\left(\frac{1}{1+i}\right)=\frac{\pi^2}{16}$ . I tested my solution numerically and all is good but as we can see it does not match Cornel's answer. any idea how to make it match? Thanks, Its worth to mention that by comparing the two results proved above, we find the new interesting identity: $$\Re\operatorname{Li}_2\left(\frac{1-x}{1+i}\right) =\frac14\left(\frac12\ln^2(1+x^2)-2\ln\left(2(1+x^2)\right)\ln(1-x)-2\operatorname{Li}_2(x)+\frac12\operatorname{Li}_2(-x^2)\\+\operatorname{Li}_2\left(\frac{2x}{1+x^2}\right)+\frac{\pi^2}{4}\right)$$","This integral can be found in Cornel's book, (Almost) Impossible Integral, Sums and Series page where he showed that by differentiating then integrating back. How magical is that? My approach: and by using , we get where I used and . I tested my solution numerically and all is good but as we can see it does not match Cornel's answer. any idea how to make it match? Thanks, Its worth to mention that by comparing the two results proved above, we find the new interesting identity:","97 \int_0^x\frac{t\ln(1-t)}{1+t^2}\ dt=\frac14\left(\frac12\ln^2(1+x^2)-2\operatorname{Li}_2(x)+\frac12\operatorname{Li}_2(-x^2)+\operatorname{Li}_2\left(\frac{2x}{1+x^2}\right)\right) \operatorname{Li}_2\left(\frac{2x}{1+x^2}\right) \begin{align}
I&=\int_0^x\frac{t\ln(1-t)}{1+t^2}\ dt\overset{IBP}{=}\frac12\ln(1+x^2)\ln(1-x)+\frac12\int_0^x\frac{\ln(1+t^2)}{1-t}\ dt\\
&\overset{1-x=u}{=}\frac12\ln(1+x^2)\ln(1-x)-\frac12\int_{1}^{1-x}\frac{\ln(2+2u+u^2)}{u}\ du\\
&=\frac12\ln(1+x^2)\ln(1-x)-\Re\int_{1}^{1-x}\frac{\ln((1+i)-u)}{u}\ du
\end{align} \displaystyle\int \frac{\ln(a-x)}{x}\ dx=\ln(a)\ln x-\operatorname{Li}_2\left(\frac{x}{a}\right)\  \begin{align}
I&=\frac12\ln(1+x^2)\ln(1-x)-\Re\left(\ln(1+i)\ln u-\operatorname{Li}_2\left(\frac{u}{1+i}\right)\right)_{u=1}^{u=1-x}\\
&=\frac12\ln(1+x^2)\ln(1-x)-\Re\left(\ln(1+i)\ln(1-x)-\operatorname{Li}_2\left(\frac{1-x}{1+i}\right)+\operatorname{Li}_2\left(\frac{1}{1+i}\right)\right)\\
&=\frac12\ln(1+x^2)\ln(1-x)+\frac12\ln2\ln(1-x)+\Re\operatorname{Li}_2\left(\frac{1-x}{1+i}\right)-\frac{\pi^2}{16}\\
&\boxed{=\frac12\ln\left(2(1+x^2)\right)\ln(1-x)+\Re\operatorname{Li}_2\left(\frac{1-x}{1+i}\right)-\frac{\pi^2}{16}}
\end{align} \ \Re\ln(1+i)=\frac12\ln2\  \ \Re\operatorname{Li}_2\left(\frac{1}{1+i}\right)=\frac{\pi^2}{16} \Re\operatorname{Li}_2\left(\frac{1-x}{1+i}\right)
=\frac14\left(\frac12\ln^2(1+x^2)-2\ln\left(2(1+x^2)\right)\ln(1-x)-2\operatorname{Li}_2(x)+\frac12\operatorname{Li}_2(-x^2)\\+\operatorname{Li}_2\left(\frac{2x}{1+x^2}\right)+\frac{\pi^2}{4}\right)","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'polylogarithm']"
63,Image of a first category set for a typical continuous function,Image of a first category set for a typical continuous function,,"Given a first category set $A \in [0,1]$ , is the set $X = \{f \in C[0,1]:f(A)\text{ is first category}\}$ residual in $C[0,1]$ ? I tried two strategies: First I tried to play the Banach-Mazur game on $X^c$ , but it does not seem clear to me who would have a winning strategy.  Secondly, I tried finding a dense $G_\delta$ set contained in X, but it did not get me very far either. How do I approach this problem?","Given a first category set , is the set residual in ? I tried two strategies: First I tried to play the Banach-Mazur game on , but it does not seem clear to me who would have a winning strategy.  Secondly, I tried finding a dense set contained in X, but it did not get me very far either. How do I approach this problem?","A \in [0,1] X = \{f \in C[0,1]:f(A)\text{ is first category}\} C[0,1] X^c G_\delta","['real-analysis', 'general-topology', 'functional-analysis', 'game-theory', 'descriptive-set-theory']"
64,Is the normalized derivative of a holomorphic function Sobolev?,Is the normalized derivative of a holomorphic function Sobolev?,,"Let $B=\{z\in \mathbb C \,|\,|z|\le 1\}$ be the closed unit disk, and let $f:B \to \mathbb{C}$ be holomorphic. More precisely, I assume that $f$ is holomorphic on the interior $\text{int}(B)$ , and smooth on the closed disk $B$ . Assume also that $f'(z) \neq 0$ almost everywhere on $B$ . Are the derivatives of $\frac{f'(z)}{\|f'(z)\|}$ in $L^1(B)$ ? More precisely, if $f=u+iv$ , then $f'(z)=u_x+iv_x$ , and I consider $$h(x,y)=\frac{u_x}{\sqrt{u_x^2+v_x^2}},g(x,y)=\frac{v_x}{\sqrt{u_x^2+v_x^2}}.$$ The question is whether or not $h_x,h_y,g_x,g_y \in L^1(B)$ . I can show that for any $\epsilon >0$ , the derivatives are integrable on $\{z\in \mathbb C:|z|\le 1-\epsilon\}$ . The ""remaining part"" of the question is to determine what happens near the boundary. I thought that if $f'(z) \neq 0$ a.e., then $f'(z)$ may vanish only at finitely many points, but this is not true in general , since we have a non-empty boundary (where the usual identity theorem doesn't work). So, the case where $f'(z)$ has infinitely many zeros (with an accumulation point on the boundary) is still open. Here is a solution for the case where $f'$ has only finitely many zeroes: (this implies that we are OK on every disk of radius $1-\epsilon$ ). This problem reduces to analyzing what happens around a single zero, say at $z=0$ . We can write $f'(z)=z^ng(z)$ for some holomorphic function $g$ satisfying $g(0) \neq 0$ . Then $\frac{f'(z)}{\|f'(z)\|}=\frac{z^n}{\|z^n\|}\frac{g(z)}{\|g(z)\|}$ . $g$ is smooth and non-zero in a neighbourhood of $z=0$ , so the factor $\frac{g(z)}{\|g(z)\|}$ causes no problems. We are left with $\frac{z^n}{\|z^n\|}=(\frac{z}{\|z\|})^n=e^{in\theta}$ , which reduces the problem to the analysis of the ""argument function"" $\theta$ (i.e. the case of $\frac{z}{\|z\|}$ ). A complete treatment of this can be found here in this previous question of mine .","Let be the closed unit disk, and let be holomorphic. More precisely, I assume that is holomorphic on the interior , and smooth on the closed disk . Assume also that almost everywhere on . Are the derivatives of in ? More precisely, if , then , and I consider The question is whether or not . I can show that for any , the derivatives are integrable on . The ""remaining part"" of the question is to determine what happens near the boundary. I thought that if a.e., then may vanish only at finitely many points, but this is not true in general , since we have a non-empty boundary (where the usual identity theorem doesn't work). So, the case where has infinitely many zeros (with an accumulation point on the boundary) is still open. Here is a solution for the case where has only finitely many zeroes: (this implies that we are OK on every disk of radius ). This problem reduces to analyzing what happens around a single zero, say at . We can write for some holomorphic function satisfying . Then . is smooth and non-zero in a neighbourhood of , so the factor causes no problems. We are left with , which reduces the problem to the analysis of the ""argument function"" (i.e. the case of ). A complete treatment of this can be found here in this previous question of mine .","B=\{z\in \mathbb C \,|\,|z|\le 1\} f:B \to \mathbb{C} f \text{int}(B) B f'(z) \neq 0 B \frac{f'(z)}{\|f'(z)\|} L^1(B) f=u+iv f'(z)=u_x+iv_x h(x,y)=\frac{u_x}{\sqrt{u_x^2+v_x^2}},g(x,y)=\frac{v_x}{\sqrt{u_x^2+v_x^2}}. h_x,h_y,g_x,g_y \in L^1(B) \epsilon >0 \{z\in \mathbb C:|z|\le 1-\epsilon\} f'(z) \neq 0 f'(z) f'(z) f' 1-\epsilon z=0 f'(z)=z^ng(z) g g(0) \neq 0 \frac{f'(z)}{\|f'(z)\|}=\frac{z^n}{\|z^n\|}\frac{g(z)}{\|g(z)\|} g z=0 \frac{g(z)}{\|g(z)\|} \frac{z^n}{\|z^n\|}=(\frac{z}{\|z\|})^n=e^{in\theta} \theta \frac{z}{\|z\|}","['real-analysis', 'complex-analysis', 'sobolev-spaces', 'lp-spaces']"
65,Gagliardo–Nirenberg–Sobolev inequality for weighted Sobolev space with exponential weights,Gagliardo–Nirenberg–Sobolev inequality for weighted Sobolev space with exponential weights,,"Consider the weighted $L^p_\omega(\mathbb{R}^d)$ space on $\mathbb{R}^d$ be the set of Lebesgue measurable functions such that $$\|f\|_{L^p_\omega}=\int_{\mathbb{R}^d}|f|^p\omega_\mu(x)\,dx< \infty,$$ where $\omega_\mu(x)=\exp(-\mu |x|^2)$ or $\omega_\mu(x)=\exp(-\mu\sqrt{1+|x|^2})$ for some $\mu>0$ . Consider the weighted sobolev space $W^{1,p}_\omega(\mathbb{R}^d)$ such that $$W^{1,p}_\omega(\mathbb{R}^d)=\{u\in L^p_\omega\mid  \partial_{x_i}u\in L^p_\omega,\, i=1,\ldots, d \},$$ where $\partial_{x_i}$ denotes the weak derivative in the distribution sense. Similarly we define the high-order sobolev space $W^{2,p}_\omega(\mathbb{R}^d)$ such that $\partial_{x_ix_j}u\in L^p_\omega$ for all $i,j$ . I was wondering whether the Gagliardo–Nirenberg–Sobolev inequality for the classical Sobolev space still holds? In particular, whether for $1\le p<n$ , we have $$ \|u\|_{L^{p^*}_\omega(\mathbf{R}^n)}\leq C \|Du\|_{L^{p}_\omega(\mathbf{R}^n)}.$$ with $1/p* = 1/p - 1/n$ ? Do you have a reference for this? The main motivation is that I would like to find some weighted spaces on $\mathbb{R}^n$ , such that $L^p_\omega(\mathbb{R}^n)\subset L^q_\omega(\mathbb{R}^n)$ if $q\ge p>1$ , and then study the elliptic equation on $\mathbb{R}^n$ , say $$ -\partial_i a^{ij}(x)\partial_j u+ru=f(x), \quad x\in \mathbb{R}^d, $$ where $a^{ij}$ is strictly elliptic. That is why I introduce this decaying weight, such that $\omega (x)dx$ is a finite measure on $\mathbb{R}^n$ . The exponential weight is introduced to involve bounded functions or functions with polynomial growth at infinity.","Consider the weighted space on be the set of Lebesgue measurable functions such that where or for some . Consider the weighted sobolev space such that where denotes the weak derivative in the distribution sense. Similarly we define the high-order sobolev space such that for all . I was wondering whether the Gagliardo–Nirenberg–Sobolev inequality for the classical Sobolev space still holds? In particular, whether for , we have with ? Do you have a reference for this? The main motivation is that I would like to find some weighted spaces on , such that if , and then study the elliptic equation on , say where is strictly elliptic. That is why I introduce this decaying weight, such that is a finite measure on . The exponential weight is introduced to involve bounded functions or functions with polynomial growth at infinity.","L^p_\omega(\mathbb{R}^d) \mathbb{R}^d \|f\|_{L^p_\omega}=\int_{\mathbb{R}^d}|f|^p\omega_\mu(x)\,dx< \infty, \omega_\mu(x)=\exp(-\mu |x|^2) \omega_\mu(x)=\exp(-\mu\sqrt{1+|x|^2}) \mu>0 W^{1,p}_\omega(\mathbb{R}^d) W^{1,p}_\omega(\mathbb{R}^d)=\{u\in L^p_\omega\mid  \partial_{x_i}u\in L^p_\omega,\, i=1,\ldots, d \}, \partial_{x_i} W^{2,p}_\omega(\mathbb{R}^d) \partial_{x_ix_j}u\in L^p_\omega i,j 1\le p<n 
\|u\|_{L^{p^*}_\omega(\mathbf{R}^n)}\leq C \|Du\|_{L^{p}_\omega(\mathbf{R}^n)}. 1/p* = 1/p - 1/n \mathbb{R}^n L^p_\omega(\mathbb{R}^n)\subset L^q_\omega(\mathbb{R}^n) q\ge p>1 \mathbb{R}^n 
-\partial_i a^{ij}(x)\partial_j u+ru=f(x), \quad x\in \mathbb{R}^d,
 a^{ij} \omega (x)dx \mathbb{R}^n","['real-analysis', 'reference-request', 'partial-differential-equations', 'lebesgue-integral', 'sobolev-spaces']"
66,About the product of two Elliptic integrals,About the product of two Elliptic integrals,,"Let $z,x\in\left(0,1\right)$ . It is possible to prove that $$\int_{0}^{1}\int_{0}^{1}\frac{1}{\sqrt{hy\left(1-h\right)\left(1-y\right)}}\frac{dydh}{\sqrt{\left(1+zhy\right)^{2}-4xzhy}}=\frac{4}{\pi^{2}}K\left(\frac{1-\rho-z}{2}\right)K\left(\frac{1-\rho+z}{2}\right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$ where $$K\left(a\right)=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-a\sin^{2}\left(\theta\right)}}=\int_{0}^{1}\frac{du}{2\sqrt{u\left(1-u\right)}\sqrt{1-au}}$$ is the complete elliptic integral of the first kind with the parameter being the elliptic modulus and $$\rho=\sqrt{\left(1+z\right)^{2}-4zx}.$$ It can be done using the generating function of the Legendre polynomials $$\frac{1}{\sqrt{\left(1+z\right)^{2}-4xz}}=\sum_{n\geq0}z^{n}P_{n}\left(2x-1\right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(2)$$ and the Brafman's formula $$\sum_{n\geq0}\frac{\left(s\right)_{n}\left(1-s\right)_{n}}{n!^{2}}z^{n}P_{n}\left(2x-1\right)= \,_{2}F_{1}\left(s,1-s;1;\frac{1-\rho-z}{2}\right){}_{2}F_{1}\left(s,1-s;1;\frac{1-\rho+z}{2}\right)$$ with $ s=1/2$ . Question 1 . Is it possible to prove $(1)$ without the use of the generating function $(2)$ but just with a clever manipulation of the double integral? Question 2 . Is it possible to prove $(1)$ using another strategy than mine (not necessarily with a manipulation of the double integral)? I suspect there is some transformation that allows us to ""separate"" the variables, but I'm not able to find it. Thank you.","Let . It is possible to prove that where is the complete elliptic integral of the first kind with the parameter being the elliptic modulus and It can be done using the generating function of the Legendre polynomials and the Brafman's formula with . Question 1 . Is it possible to prove without the use of the generating function but just with a clever manipulation of the double integral? Question 2 . Is it possible to prove using another strategy than mine (not necessarily with a manipulation of the double integral)? I suspect there is some transformation that allows us to ""separate"" the variables, but I'm not able to find it. Thank you.","z,x\in\left(0,1\right) \int_{0}^{1}\int_{0}^{1}\frac{1}{\sqrt{hy\left(1-h\right)\left(1-y\right)}}\frac{dydh}{\sqrt{\left(1+zhy\right)^{2}-4xzhy}}=\frac{4}{\pi^{2}}K\left(\frac{1-\rho-z}{2}\right)K\left(\frac{1-\rho+z}{2}\right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1) K\left(a\right)=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-a\sin^{2}\left(\theta\right)}}=\int_{0}^{1}\frac{du}{2\sqrt{u\left(1-u\right)}\sqrt{1-au}} \rho=\sqrt{\left(1+z\right)^{2}-4zx}. \frac{1}{\sqrt{\left(1+z\right)^{2}-4xz}}=\sum_{n\geq0}z^{n}P_{n}\left(2x-1\right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(2) \sum_{n\geq0}\frac{\left(s\right)_{n}\left(1-s\right)_{n}}{n!^{2}}z^{n}P_{n}\left(2x-1\right)= \,_{2}F_{1}\left(s,1-s;1;\frac{1-\rho-z}{2}\right){}_{2}F_{1}\left(s,1-s;1;\frac{1-\rho+z}{2}\right)  s=1/2 (1) (2) (1)","['real-analysis', 'integration', 'special-functions', 'hypergeometric-function', 'elliptic-integrals']"
67,$f$ is Riemann integrable if the set of discontinuities is measure zero.,is Riemann integrable if the set of discontinuities is measure zero.,f,"Let $f$ be a bounded function on a compact interval $J$ , and let $I(c,r)$ denote   the open interval centered at $c$ of radius $r>0$ . Let $osc(f,c,r)=\sup|f(x)-f(y)|$ , where   the supremum is taken over all $x,y\in J\cap I(c,r)$ , and define the oscillation of $f$ at $c$ by $osc(f,c)=\underset{r\rightarrow 0}{\lim}osc(f,c,r)$ . Clearly, $f$ is continuous   at $c\in J$ if and only if $osc(f,c)=0$ . (a) For every $\epsilon>0$ , the set of points $c$ in J such that $osc(f,c)\geq \epsilon$ is compact. (b) If the set of discontinuities of $f$ has measure $0$ , then $f$ is Riemann integrable. I've already proved part (a) so this is my attempt at part (b) My proof attempt: Proof. Fix $\epsilon>0$ . Define \begin{equation*} D_\epsilon=\{c\in J: osc(f,c)>\epsilon\} \end{equation*} Then $D_\epsilon$ is the set of discontinuities of $f$ . By hypothesis, $D_\epsilon$ is measure zero.  We will prove that $f$ is Riemann integrable by showing that there exists a partition $P$ such that $|U(P,f)-L(P,f)|<\epsilon$ , where $U(P,f)$ and $L(P,f)$ are the upper and lower sums of $f$ over partition $P$ , respectively. By Observation 3 of the Exterior Measure in Stein, there exists an open set $\mathcal{O}$ such that $D_\epsilon\subset \mathcal{O}$ and $m(\mathcal{O})\leq \epsilon.$ By theorem 1.3, $\mathcal{O}$ can be written as a disjoint countable union of open intervals $(I_j)_j$ . By Part (a), $D_\epsilon$ is compact. Since $(I_j)_j$ covers $D_\epsilon$ , we need only a finite sub-cover to cover $D_\epsilon$ , say $(I_j)^{k}_{1},$ and denote the collection as $\mathcal{F}$ . Then \begin{equation*} m(D_\epsilon)\leq m(\underset{I\in \mathcal{F}}{\bigcup}I)\leq m(\mathcal{O})\leq \epsilon \end{equation*} Hence, we now have a finite cover of $D_\epsilon$ whose ""total length"" is $\leq \epsilon$ . Since $J$ is closed and $I\in \mathcal{F}$ is open, it follows that $J\setminus \underset{I\in \mathcal{F}}{\bigcup}I$ is closed. Since $J\setminus \underset{I\in \mathcal{F}}{\bigcup}I\subset J$ , it is also compact.  Any continuous $f$ over a compact interval is uniformly continuous. Therefore, there exists $\delta>0$ such that for all $x,y\in J\setminus \underset{I\in \mathcal{F}}{\bigcup}I$ satisfying \begin{equation*} |x-y|<\delta \implies |f(x)-f(y)|<\epsilon.  \end{equation*} Let $P_1$ be a partition of $J$ such that every pair of consecutive points is within $\delta$ of each other, e.g. one way of constructing such a set is to partition $J$ into $\frac{|J|}{2^n}$ length intervals for large enough $n\in \mathbb{N}$ . Now define $\hat{P}_1$ to be a partition of $J$ such that \begin{equation*} \hat{P}_1=P_1\setminus \{x\in P_1: x\in \underset{I\in \mathcal{F}}{\bigcup}I\}. \end{equation*} Next, define $P_2$ to be a partition of $J$ such that $P_2$ is the set of the end points of $I_1, I_2,...,I_k$ , and $J$ . Finally, let $P$ be the common refinement of $P_1$ and $P_2$ , i.e. $P=P_1\cup P_2$ . We will now show that $P$ is the desired partition of $J$ . Enumerate the points of $P$ as $x_1,x_2,...,x_N$ .  Denote $\Delta x_i=|x_{i+1}-x_i|$ for $i=1,...,N-1$ . Denote as well the interval $[x_{i+1},x_i]$ as $\nabla x_i.$ If two consecutive points in $P$ are also in $P_2$ , denote the length instead by $\Delta y_i$ . Moreover, for each $\Delta x_i$ , let $m_i$ and $M_i$ be the infimum and supremum of $f$ over $\nabla x_i$ , respectively.  Define \begin{equation} C:=2\cdot \sup\{|f(x)|:x\in J\}<\infty \quad  \text{ Since $f$ is bounded.} \end{equation} Finally, observe \begin{align*} |U(P,f)-L(P,f)|&=|\underset{x_i\in P}{\sum} M_i\Delta x_i- \underset{x_i\in P}{\sum} m_i\Delta x_i| \\ &=|\underset{x_i\in P}{\sum} (M_i-m_i)\Delta x_i| \\ &=|\underset{x_i\in P_1}{\sum} (M_i-m_i)\Delta x_i +  \underset{x_i\in P_2}{\sum} (M_i-m_i)\Delta y_i| \\ &\leq \underset{x_i\in P_1}{\sum} |(M_i-m_i)|\Delta x_i + \underset{x_i\in P_2}{\sum} |(M_i-m_i)|\Delta y_i \\ &\leq \epsilon|J| + C\epsilon \\ &\text{ By Uniform Continuity of $f$ over $J\setminus \underset{I\in \mathcal{F}}{\bigcup}I$} \\ &=\epsilon(|J|+C) \\ \end{align*} As niether $|J|$ nor $C$ depend on $\epsilon$ , this completes our proof. The definition of the Riemann Integrable I use is from Rudin's PMA. Any corrections of the proof or comments on style are highly appreciated. I recognize the proof is really long and I appreciate the time of anybody who parses through it.","Let be a bounded function on a compact interval , and let denote   the open interval centered at of radius . Let , where   the supremum is taken over all , and define the oscillation of at by . Clearly, is continuous   at if and only if . (a) For every , the set of points in J such that is compact. (b) If the set of discontinuities of has measure , then is Riemann integrable. I've already proved part (a) so this is my attempt at part (b) My proof attempt: Proof. Fix . Define Then is the set of discontinuities of . By hypothesis, is measure zero.  We will prove that is Riemann integrable by showing that there exists a partition such that , where and are the upper and lower sums of over partition , respectively. By Observation 3 of the Exterior Measure in Stein, there exists an open set such that and By theorem 1.3, can be written as a disjoint countable union of open intervals . By Part (a), is compact. Since covers , we need only a finite sub-cover to cover , say and denote the collection as . Then Hence, we now have a finite cover of whose ""total length"" is . Since is closed and is open, it follows that is closed. Since , it is also compact.  Any continuous over a compact interval is uniformly continuous. Therefore, there exists such that for all satisfying Let be a partition of such that every pair of consecutive points is within of each other, e.g. one way of constructing such a set is to partition into length intervals for large enough . Now define to be a partition of such that Next, define to be a partition of such that is the set of the end points of , and . Finally, let be the common refinement of and , i.e. . We will now show that is the desired partition of . Enumerate the points of as .  Denote for . Denote as well the interval as If two consecutive points in are also in , denote the length instead by . Moreover, for each , let and be the infimum and supremum of over , respectively.  Define Finally, observe As niether nor depend on , this completes our proof. The definition of the Riemann Integrable I use is from Rudin's PMA. Any corrections of the proof or comments on style are highly appreciated. I recognize the proof is really long and I appreciate the time of anybody who parses through it.","f J I(c,r) c r>0 osc(f,c,r)=\sup|f(x)-f(y)| x,y\in J\cap I(c,r) f c osc(f,c)=\underset{r\rightarrow 0}{\lim}osc(f,c,r) f c\in J osc(f,c)=0 \epsilon>0 c osc(f,c)\geq \epsilon f 0 f \epsilon>0 \begin{equation*}
D_\epsilon=\{c\in J: osc(f,c)>\epsilon\}
\end{equation*} D_\epsilon f D_\epsilon f P |U(P,f)-L(P,f)|<\epsilon U(P,f) L(P,f) f P \mathcal{O} D_\epsilon\subset \mathcal{O} m(\mathcal{O})\leq \epsilon. \mathcal{O} (I_j)_j D_\epsilon (I_j)_j D_\epsilon D_\epsilon (I_j)^{k}_{1}, \mathcal{F} \begin{equation*}
m(D_\epsilon)\leq m(\underset{I\in \mathcal{F}}{\bigcup}I)\leq m(\mathcal{O})\leq \epsilon
\end{equation*} D_\epsilon \leq \epsilon J I\in \mathcal{F} J\setminus \underset{I\in \mathcal{F}}{\bigcup}I J\setminus \underset{I\in \mathcal{F}}{\bigcup}I\subset J f \delta>0 x,y\in J\setminus \underset{I\in \mathcal{F}}{\bigcup}I \begin{equation*}
|x-y|<\delta \implies |f(x)-f(y)|<\epsilon. 
\end{equation*} P_1 J \delta J \frac{|J|}{2^n} n\in \mathbb{N} \hat{P}_1 J \begin{equation*}
\hat{P}_1=P_1\setminus \{x\in P_1: x\in \underset{I\in \mathcal{F}}{\bigcup}I\}.
\end{equation*} P_2 J P_2 I_1, I_2,...,I_k J P P_1 P_2 P=P_1\cup P_2 P J P x_1,x_2,...,x_N \Delta x_i=|x_{i+1}-x_i| i=1,...,N-1 [x_{i+1},x_i] \nabla x_i. P P_2 \Delta y_i \Delta x_i m_i M_i f \nabla x_i \begin{equation}
C:=2\cdot \sup\{|f(x)|:x\in J\}<\infty \quad  \text{ Since f is bounded.}
\end{equation} \begin{align*}
|U(P,f)-L(P,f)|&=|\underset{x_i\in P}{\sum} M_i\Delta x_i-
\underset{x_i\in P}{\sum} m_i\Delta x_i| \\
&=|\underset{x_i\in P}{\sum} (M_i-m_i)\Delta x_i| \\
&=|\underset{x_i\in P_1}{\sum} (M_i-m_i)\Delta x_i + 
\underset{x_i\in P_2}{\sum} (M_i-m_i)\Delta y_i| \\
&\leq \underset{x_i\in P_1}{\sum} |(M_i-m_i)|\Delta x_i +
\underset{x_i\in P_2}{\sum} |(M_i-m_i)|\Delta y_i \\
&\leq \epsilon|J| + C\epsilon \\
&\text{ By Uniform Continuity of f over J\setminus \underset{I\in \mathcal{F}}{\bigcup}I} \\
&=\epsilon(|J|+C) \\
\end{align*} |J| C \epsilon","['real-analysis', 'measure-theory', 'proof-verification']"
68,Some statements about $f''(x)+xf'(x) = \cos(x^3f'(x))$,Some statements about,f''(x)+xf'(x) = \cos(x^3f'(x)),"Consider $f:\mathbb{R}\to\mathbb{R} \in C^2$ such that $f''(x)+xf'(x) = \cos(x^3f'(x)) \;\;\forall x \in \mathbb{R}$ Which of the following are correct? (i) If $f$ has a critic point $x_0$, then it is a local minimum. (ii) $\exists r>0$ such that $f$ is concave upwards in $(-r,r)$. (iii) $f$ can have at most one critic point (iv) $f$ is odd (v) $f(x)=0$ has at most two solutions. Here is my attempt: (i) If $f'(x_0)=0$, then $f''(x_0) = \cos(0) = 1>0$, so it is a local minimum. True. (ii) By continuity, as $f''(0)>0$, then there is a neighborhood $B(0,r)$ where $f''(x)>0 \forall x \in (-r,r)$. True. (iii) Suppose $f'(x_0)=f'(x_1)=0$ for $x_0>x_1$. Then there must be a point $c \in (x_0,x_1)$ such that $f''(c)<0$, since both $x_0$ and $x_1$ are local minima and $f$ is continuous. True. (How can I write this precisely?) (iv) Since $0$ is the only critic point of $f$, then $\lim_{|x|\to\infty} f(x) = \infty$, so it can't be odd. False. (How can I write this precisely?) (v) Since $0$ is the only critic point and is not odd, then it can only cross $x$-axis one time left at $0$ and one time right at $0$, at most. True. (How can I write this precisely?) I think I solved this exercise based on geometric intuition, but I'd like to be rigorous. Please help me formalize these ideas.","Consider $f:\mathbb{R}\to\mathbb{R} \in C^2$ such that $f''(x)+xf'(x) = \cos(x^3f'(x)) \;\;\forall x \in \mathbb{R}$ Which of the following are correct? (i) If $f$ has a critic point $x_0$, then it is a local minimum. (ii) $\exists r>0$ such that $f$ is concave upwards in $(-r,r)$. (iii) $f$ can have at most one critic point (iv) $f$ is odd (v) $f(x)=0$ has at most two solutions. Here is my attempt: (i) If $f'(x_0)=0$, then $f''(x_0) = \cos(0) = 1>0$, so it is a local minimum. True. (ii) By continuity, as $f''(0)>0$, then there is a neighborhood $B(0,r)$ where $f''(x)>0 \forall x \in (-r,r)$. True. (iii) Suppose $f'(x_0)=f'(x_1)=0$ for $x_0>x_1$. Then there must be a point $c \in (x_0,x_1)$ such that $f''(c)<0$, since both $x_0$ and $x_1$ are local minima and $f$ is continuous. True. (How can I write this precisely?) (iv) Since $0$ is the only critic point of $f$, then $\lim_{|x|\to\infty} f(x) = \infty$, so it can't be odd. False. (How can I write this precisely?) (v) Since $0$ is the only critic point and is not odd, then it can only cross $x$-axis one time left at $0$ and one time right at $0$, at most. True. (How can I write this precisely?) I think I solved this exercise based on geometric intuition, but I'd like to be rigorous. Please help me formalize these ideas.",,['real-analysis']
69,Measure of complement of union of nowhere dense set with positive measure,Measure of complement of union of nowhere dense set with positive measure,,"The original question is: Let $A$ be Lebesgue measurable in $\mathbb{R}$ with positive measure. Show that it is not true that there must exist a sequence $\{x_n\}^\infty_{n=1}$ such that the complement of $\bigcup^\infty_{n=1}(A+\{x_n\})$ in $\mathbb{R}$ is measure zero. To disprove the statement, I want to take $A$ to be the fat rational set and argue that for any sequence $\{x_n\}$ in $\mathbb{R}$, the complement of the set stated above must have positive measure. Let $\{q_n\}$ be an enumeration of $\mathbb{Q}$, fix $\epsilon\in(0,1)$ and let \begin{equation} I_n=(q_n-\frac{\epsilon}{2^{n+1}},q_n+\frac{\epsilon}{2^{n+1}}). \end{equation} Let $G=\cup_nI_n$, and $A=[0,1]\setminus G$, then clearly any rationals cannot be in $A$ and it follows that $A$ is nowhere dense. It is easy to show that $A$ is Lebesgue measurable and has measure \begin{equation} m(A)= 1-\sum^\infty_{n=1}\frac{\epsilon}{2^n}=1-\epsilon\in(0,1), \end{equation} so $A$ satisfies the hypothesis of the question. Since $A$ is nowhere dense so $\bigcup^\infty_{n=1}(A+\{x_n\})$ is nowhere dense also for any sequence $\{x_n\}$ in $\mathbb{R}$. However, I found it difficult to prove that the complement of $\bigcup^\infty_{n=1}(A+\{x_n\})$ must have positive measure for arbitrary sequence $\{x_n\}$","The original question is: Let $A$ be Lebesgue measurable in $\mathbb{R}$ with positive measure. Show that it is not true that there must exist a sequence $\{x_n\}^\infty_{n=1}$ such that the complement of $\bigcup^\infty_{n=1}(A+\{x_n\})$ in $\mathbb{R}$ is measure zero. To disprove the statement, I want to take $A$ to be the fat rational set and argue that for any sequence $\{x_n\}$ in $\mathbb{R}$, the complement of the set stated above must have positive measure. Let $\{q_n\}$ be an enumeration of $\mathbb{Q}$, fix $\epsilon\in(0,1)$ and let \begin{equation} I_n=(q_n-\frac{\epsilon}{2^{n+1}},q_n+\frac{\epsilon}{2^{n+1}}). \end{equation} Let $G=\cup_nI_n$, and $A=[0,1]\setminus G$, then clearly any rationals cannot be in $A$ and it follows that $A$ is nowhere dense. It is easy to show that $A$ is Lebesgue measurable and has measure \begin{equation} m(A)= 1-\sum^\infty_{n=1}\frac{\epsilon}{2^n}=1-\epsilon\in(0,1), \end{equation} so $A$ satisfies the hypothesis of the question. Since $A$ is nowhere dense so $\bigcup^\infty_{n=1}(A+\{x_n\})$ is nowhere dense also for any sequence $\{x_n\}$ in $\mathbb{R}$. However, I found it difficult to prove that the complement of $\bigcup^\infty_{n=1}(A+\{x_n\})$ must have positive measure for arbitrary sequence $\{x_n\}$",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'examples-counterexamples', 'baire-category']"
70,Both variables algebraic/ both transcendental?,Both variables algebraic/ both transcendental?,,"Suppose $X\subset \Bbb Z_{<0}$, and $a=\sum_{n\in X} 2^n$ is algebraic, then is $b=\sum_{n\in X}3^n$ also algebraic? (To clarify, what I mean is: does it hold for all such $X$?) We know that $a$ and $b$ must either both be rational or both irrational, simply by noting the fact that under any integer base, rational numbers coincide exactly with recurring decimals. But what about being algebraic/transcendental? Is there anything we can say about it? By the way I'm no algebraist (not even a smart math student) so apologies for not being able to proceed any deeper.","Suppose $X\subset \Bbb Z_{<0}$, and $a=\sum_{n\in X} 2^n$ is algebraic, then is $b=\sum_{n\in X}3^n$ also algebraic? (To clarify, what I mean is: does it hold for all such $X$?) We know that $a$ and $b$ must either both be rational or both irrational, simply by noting the fact that under any integer base, rational numbers coincide exactly with recurring decimals. But what about being algebraic/transcendental? Is there anything we can say about it? By the way I'm no algebraist (not even a smart math student) so apologies for not being able to proceed any deeper.",,"['real-analysis', 'real-numbers', 'transcendental-numbers']"
71,Does there exist a real everywhere differentiable function with the set of critical values of non zero measure?,Does there exist a real everywhere differentiable function with the set of critical values of non zero measure?,,"By Sard's theorem, the measure of the set of critical values of a continuously differentiable real function defined on the real line is zero. Is there a counterexample when one omits the condition of continuity of the derivative (but still demands its existence)? (I have read about the Pompeiu derivative in the answers on this site, whose antiderivative as I understood has a $G_\delta$ dense set of critical values in the unit interval, but I did not find a statement about its measure).","By Sard's theorem, the measure of the set of critical values of a continuously differentiable real function defined on the real line is zero. Is there a counterexample when one omits the condition of continuity of the derivative (but still demands its existence)? (I have read about the Pompeiu derivative in the answers on this site, whose antiderivative as I understood has a $G_\delta$ dense set of critical values in the unit interval, but I did not find a statement about its measure).",,['real-analysis']
72,Is the following series consisting of $\pm 1$ bounded? [closed],Is the following series consisting of  bounded? [closed],\pm 1,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 6 years ago . Improve this question (Link to cross-post to MathOverflow) Let $b=\frac{\sqrt{5}-1}2$ and $a_n:=(-1)^{[nb]}$ where $[\cdot]$ denotes the floor function. Are the partial sums $A_N=\sum\limits_{n=0}^N a_n$ bounded? The sequence $nb\bmod 2$ is equidistributed in $[0,2]$, so, as $n$ gets large, there will be about half of indices in $\{1,2,\cdots,n\}$ that correspond to $1$ and the other half to $-1$. This doesn't help much though because what it says is basically $A_N\in o(N)$. By testing on computer softwares, it seems that $A_N\in O(\log N)$. Also, is there anything special about $b$ besides being an irrational real number? What if one replaces it by, say, $\pi$?","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 6 years ago . Improve this question (Link to cross-post to MathOverflow) Let $b=\frac{\sqrt{5}-1}2$ and $a_n:=(-1)^{[nb]}$ where $[\cdot]$ denotes the floor function. Are the partial sums $A_N=\sum\limits_{n=0}^N a_n$ bounded? The sequence $nb\bmod 2$ is equidistributed in $[0,2]$, so, as $n$ gets large, there will be about half of indices in $\{1,2,\cdots,n\}$ that correspond to $1$ and the other half to $-1$. This doesn't help much though because what it says is basically $A_N\in o(N)$. By testing on computer softwares, it seems that $A_N\in O(\log N)$. Also, is there anything special about $b$ besides being an irrational real number? What if one replaces it by, say, $\pi$?",,"['real-analysis', 'sequences-and-series', 'dynamical-systems', 'random-walk', 'ergodic-theory']"
73,"If $E$ is a totally bounded subset of a metric space $X$. Then, any subset of $E$ is totally bounded.","If  is a totally bounded subset of a metric space . Then, any subset of  is totally bounded.",E X E,"Proof: Let $D \subset E$, where $E \subset X$ and ($X,d$) is a metric space. Suppose that $E$ is totally bounded. That is: for all $\varepsilon > 0$, there exist finitely many points $x_1, \ldots , x_n \in X$ such that: $E \subset \bigcup_{i = 1}^{n} B(x_i, \varepsilon)$. So : given $\varepsilon > 0$, we have $D \subset E$ and $E \subset \bigcup_{i = 1}^{n} B(x_i, \varepsilon)$ with $x_1, \ldots , x_n \in X$. But then, by the transitive property (applied to subsets), it follows that $D \subset \bigcup_{i = 1}^{n} B(x_i, \varepsilon)$, where $x_1, \ldots , x_n \in X$. But this is to say that $D$ is totally bounded, as claimed. QED . Did I do anything wrong or am I correct? Thanks!","Proof: Let $D \subset E$, where $E \subset X$ and ($X,d$) is a metric space. Suppose that $E$ is totally bounded. That is: for all $\varepsilon > 0$, there exist finitely many points $x_1, \ldots , x_n \in X$ such that: $E \subset \bigcup_{i = 1}^{n} B(x_i, \varepsilon)$. So : given $\varepsilon > 0$, we have $D \subset E$ and $E \subset \bigcup_{i = 1}^{n} B(x_i, \varepsilon)$ with $x_1, \ldots , x_n \in X$. But then, by the transitive property (applied to subsets), it follows that $D \subset \bigcup_{i = 1}^{n} B(x_i, \varepsilon)$, where $x_1, \ldots , x_n \in X$. But this is to say that $D$ is totally bounded, as claimed. QED . Did I do anything wrong or am I correct? Thanks!",,"['real-analysis', 'analysis', 'proof-verification', 'metric-spaces']"
74,"Is $\sin(nx)$ equicontinuous on $[0,1]$?",Is  equicontinuous on ?,"\sin(nx) [0,1]","Consider $f_n=\sin(nx),\,x\in[0,1]$. In order to show that this is not an equicontinuous family, take $x=0,y=\frac{1}{N}$ where $N \in \mathbb{N}$ can be arbitrarily large, so $\forall \, \delta>0 \rightarrow y<\delta$. Now, if we consider $|f_N(x)-f_N(y)|=|\sin(N\frac{1}{N})|=|\sin(1)|$, hence there exists $\varepsilon_0 > 0$ such that $\forall \, \delta>0,\, \exists\, x=0,\, y=\frac{1}{N},\, N \in \mathbb{N} \,  \text{ such that } |x-y|=y<\delta, \, \text{ but } |f_N(x)-f_N(y)| \geq \varepsilon_0$ Is there anything not right with that logic?","Consider $f_n=\sin(nx),\,x\in[0,1]$. In order to show that this is not an equicontinuous family, take $x=0,y=\frac{1}{N}$ where $N \in \mathbb{N}$ can be arbitrarily large, so $\forall \, \delta>0 \rightarrow y<\delta$. Now, if we consider $|f_N(x)-f_N(y)|=|\sin(N\frac{1}{N})|=|\sin(1)|$, hence there exists $\varepsilon_0 > 0$ such that $\forall \, \delta>0,\, \exists\, x=0,\, y=\frac{1}{N},\, N \in \mathbb{N} \,  \text{ such that } |x-y|=y<\delta, \, \text{ but } |f_N(x)-f_N(y)| \geq \varepsilon_0$ Is there anything not right with that logic?",,"['real-analysis', 'sequences-and-series', 'equicontinuity']"
75,I conjecture $3(a^ab^bc^c)^{\frac{1}{a+b+c}}\ge (a^ab^b)^{\frac{1}{a+b}}+(b^bc^c)^{\frac{1}{b+c}}+(c^ca^a)^{\frac{1}{c+a}}$,I conjecture,3(a^ab^bc^c)^{\frac{1}{a+b+c}}\ge (a^ab^b)^{\frac{1}{a+b}}+(b^bc^c)^{\frac{1}{b+c}}+(c^ca^a)^{\frac{1}{c+a}},"I Conjecture, for $a, b, c > 0$ , $$3\left(a^ab^bc^c\right)^{\dfrac{1}{a+b+c}}\ge \left(a^ab^b\right)^{\frac{1}{a+b}}+\left(b^bc^c\right)^{\frac{1}{b+c}}+\left(c^ca^a\right)^{\frac{1}{c+a}}.$$ This conjecture is based on the problem (use Jensen inequality can prove it): Prove $a^ab^bc^c\ge (abc)^{\frac{a+b+c}3}$ for positive numbers. Prove that $a^ab^bc^c\ge (abc)^{(a+b+c)/{3}}$ It seem is right,I can't find count-example  so far,because $(a,b,c)=(1,1,1),(1,2,3)$ is hold,and $(a,b,c)=(x,y,\to 0)$ also hold.and jensen inequality Can't use?","I Conjecture, for , This conjecture is based on the problem (use Jensen inequality can prove it): Prove $a^ab^bc^c\ge (abc)^{\frac{a+b+c}3}$ for positive numbers. Prove that $a^ab^bc^c\ge (abc)^{(a+b+c)/{3}}$ It seem is right,I can't find count-example  so far,because is hold,and also hold.and jensen inequality Can't use?","a, b, c > 0 3\left(a^ab^bc^c\right)^{\dfrac{1}{a+b+c}}\ge \left(a^ab^b\right)^{\frac{1}{a+b}}+\left(b^bc^c\right)^{\frac{1}{b+c}}+\left(c^ca^a\right)^{\frac{1}{c+a}}. (a,b,c)=(1,1,1),(1,2,3) (a,b,c)=(x,y,\to 0)","['real-analysis', 'inequality']"
76,"Points of discontinuity of a bijective function $f:\mathbb{R} \to [0,\infty)$",Points of discontinuity of a bijective function,"f:\mathbb{R} \to [0,\infty)","We know that the points of discontinuity of a monotone function on an interval $[a,b]$ are countable. Using this can we prov that: Any bijection $f: \mathbb{R} \to [0,\infty)$ has infinitely many points of discontinuity. If yes, how or otherwise how to prove the above result?","We know that the points of discontinuity of a monotone function on an interval $[a,b]$ are countable. Using this can we prov that: Any bijection $f: \mathbb{R} \to [0,\infty)$ has infinitely many points of discontinuity. If yes, how or otherwise how to prove the above result?",,['real-analysis']
77,Why is the unit circle definition of trig functions not rigorous enough?,Why is the unit circle definition of trig functions not rigorous enough?,,"It has recently come to my attention that the usual unit circle derivation of the elementary trigonometric functions isn't considered rigorous enough. Apparently, this has to do with problems surrounding the definition of arc length. Could somebody explain, to a freshman mathematics major, why this isn't rigorous enough. With emphasis on freshman : relatively simple explanations would be appreciated over highly abstract ones as I'm still only halfway through my ""introduction to analysis"" course. And in contrast - while we're at it - why is deriving the sine function from the integral definition of the arcsine $(*)$ $$\arcsin(x) =\int_0^x \! \frac{1}{\sqrt{1-t^2}} \, \mathrm{d}t \; \; (*)$$ considered to be more rigorous? In order for somebody to know this definition, it would seem to me that you'd have to prove that this is the integral of the arcsine function. And in order to prove this property, you'd have to have a clear definition of an arcsine function that doesn't rely on integrals, which brings you back to the sine function (defining the arcsine as the inverse of a sine function). So you'd end up using property $(*)$ to create a rigorous definition for trigonometric functions even though this property was derived non-rigorously, thus inviting the question ""why wasn't the definition using arc length sufficient if the rigorous approach appears to invoke non-rigorous ideas itself?"" ? Again, please keep it within the bounds of what a freshman could understand, or at least try to do so. Much appreciated!","It has recently come to my attention that the usual unit circle derivation of the elementary trigonometric functions isn't considered rigorous enough. Apparently, this has to do with problems surrounding the definition of arc length. Could somebody explain, to a freshman mathematics major, why this isn't rigorous enough. With emphasis on freshman : relatively simple explanations would be appreciated over highly abstract ones as I'm still only halfway through my ""introduction to analysis"" course. And in contrast - while we're at it - why is deriving the sine function from the integral definition of the arcsine $(*)$ $$\arcsin(x) =\int_0^x \! \frac{1}{\sqrt{1-t^2}} \, \mathrm{d}t \; \; (*)$$ considered to be more rigorous? In order for somebody to know this definition, it would seem to me that you'd have to prove that this is the integral of the arcsine function. And in order to prove this property, you'd have to have a clear definition of an arcsine function that doesn't rely on integrals, which brings you back to the sine function (defining the arcsine as the inverse of a sine function). So you'd end up using property $(*)$ to create a rigorous definition for trigonometric functions even though this property was derived non-rigorously, thus inviting the question ""why wasn't the definition using arc length sufficient if the rigorous approach appears to invoke non-rigorous ideas itself?"" ? Again, please keep it within the bounds of what a freshman could understand, or at least try to do so. Much appreciated!",,"['calculus', 'real-analysis', 'functions', 'trigonometry', 'definition']"
78,Would an integral defined using partitions of an interval into infinitely many intervals make sense?,Would an integral defined using partitions of an interval into infinitely many intervals make sense?,,"In the definition of Riemann integral or Darboux-integral we first study partitions (or tagged partition) of the given interval determined by finitely many points. To each partition and a function $f$ we assign a number, which serves as an ""approximation"" of the integral. And the integral is then defined as some kind of limit of these approximations. Would it be possible to define something similar with partitions consisting of infinitely many points instead of just finitely many? If there is some reasonable definition, would it bring something new, or do we just get another - and more complicated - definition of the same thing? Was something along these lines studied? To make a bit clearer what I have in mind, let me add a few comments and some examples. We want to have partition with infinitely many points. One thing we need is to know how to sum infinitely many values induced by the partition. But another thing is that the order of the points in the partition seems also to be important. (In the usual definition, we take finitely many points in the interval, but their ordering is also important. The Darboux sums and Riemann sums are defined by adding terms of the form $f(x)(a_{i+1}-a_i)$, so it is important to know which point is an immediate successor of $a_i$. This is clear for finitely many points, but less clear if we have an infinite set.) Perhaps the simplest generalization would be to take a partition of the interval $[x,y]$ determined by an increasing sequence $(a_n)$ such that $a_0=x$ and $\lim\limits_{n\to\infty} a_n=y$. Then we could use infinite sums of the form $$\sum_{i=1}^\infty f(x_i)(a_{i+1}-a_i),$$ where $x_i$ belongs to the interval $[a_i,a_{i+1}]$. (My guess would be that this should give the usual Riemann/Darboux integral, at least for bounded functions. However, this definition is ""not nice"", since it treats the right endpoint of the interval differently from the left one. From a reasonable definition we would expect to be symmetric.) We could do something like that for any ordinal $\alpha$. So the partition would be an increasing transfinite sequence $(a_\gamma)_{\gamma<\alpha}$. We probably also want $\lim\limits_{\beta<\gamma} a_\beta=a_\gamma$ for any limit ordinal $\gamma$. Sums of sequences defined by ordinals can be reasonably defined, see Wikipedia . So we could use something like $$\sum_{\gamma<\alpha} f(x_\gamma)(a_{\gamma+1}-a_\gamma).$$ The preceding example would correspond to $\alpha=\omega$. There are also sums over arbitrary index sets . Such sums were discussed here or in some other posts linked there . But since here we simply take limit of sums of finite subsets, I think that this yields the same notion of integral, only described in a more convoluted way. Of course, there may be many other approaches I did not think of. I have tried to search whether similar question was asked in the past. The closest I was able to find was this: Lebesgue integration and countable partitions . This question mentions an approach to Lebesgue integral using countable partitions. Lebesgue integral is defined in this way in the book A Primer of Lebesgue Integration by Herbert Stanley Bear, see page 62 . However, it is slightly different from what I described above. They use partition into countably many measurable sets , not countably many intervals. (But still, this relates to my questions, since it shows that somewhat similar approach can lead to Lebesgue integral .)","In the definition of Riemann integral or Darboux-integral we first study partitions (or tagged partition) of the given interval determined by finitely many points. To each partition and a function $f$ we assign a number, which serves as an ""approximation"" of the integral. And the integral is then defined as some kind of limit of these approximations. Would it be possible to define something similar with partitions consisting of infinitely many points instead of just finitely many? If there is some reasonable definition, would it bring something new, or do we just get another - and more complicated - definition of the same thing? Was something along these lines studied? To make a bit clearer what I have in mind, let me add a few comments and some examples. We want to have partition with infinitely many points. One thing we need is to know how to sum infinitely many values induced by the partition. But another thing is that the order of the points in the partition seems also to be important. (In the usual definition, we take finitely many points in the interval, but their ordering is also important. The Darboux sums and Riemann sums are defined by adding terms of the form $f(x)(a_{i+1}-a_i)$, so it is important to know which point is an immediate successor of $a_i$. This is clear for finitely many points, but less clear if we have an infinite set.) Perhaps the simplest generalization would be to take a partition of the interval $[x,y]$ determined by an increasing sequence $(a_n)$ such that $a_0=x$ and $\lim\limits_{n\to\infty} a_n=y$. Then we could use infinite sums of the form $$\sum_{i=1}^\infty f(x_i)(a_{i+1}-a_i),$$ where $x_i$ belongs to the interval $[a_i,a_{i+1}]$. (My guess would be that this should give the usual Riemann/Darboux integral, at least for bounded functions. However, this definition is ""not nice"", since it treats the right endpoint of the interval differently from the left one. From a reasonable definition we would expect to be symmetric.) We could do something like that for any ordinal $\alpha$. So the partition would be an increasing transfinite sequence $(a_\gamma)_{\gamma<\alpha}$. We probably also want $\lim\limits_{\beta<\gamma} a_\beta=a_\gamma$ for any limit ordinal $\gamma$. Sums of sequences defined by ordinals can be reasonably defined, see Wikipedia . So we could use something like $$\sum_{\gamma<\alpha} f(x_\gamma)(a_{\gamma+1}-a_\gamma).$$ The preceding example would correspond to $\alpha=\omega$. There are also sums over arbitrary index sets . Such sums were discussed here or in some other posts linked there . But since here we simply take limit of sums of finite subsets, I think that this yields the same notion of integral, only described in a more convoluted way. Of course, there may be many other approaches I did not think of. I have tried to search whether similar question was asked in the past. The closest I was able to find was this: Lebesgue integration and countable partitions . This question mentions an approach to Lebesgue integral using countable partitions. Lebesgue integral is defined in this way in the book A Primer of Lebesgue Integration by Herbert Stanley Bear, see page 62 . However, it is slightly different from what I described above. They use partition into countably many measurable sets , not countably many intervals. (But still, this relates to my questions, since it shows that somewhat similar approach can lead to Lebesgue integral .)",,"['real-analysis', 'integration', 'riemann-sum', 'partitions-for-integration']"
79,"Are any tools or techniques available to solve the ""placement of safety points"" problem?","Are any tools or techniques available to solve the ""placement of safety points"" problem?",,"Definition 0. Given a metric space $X$ and subsets $H$ and $S$ thereof, define: $$d(H,S) = \sup_{h \in H} \inf_{s \in S}d(h,s)$$ (This as an asymmetric version of the Hausdorff distance .) Here's some slightly dodgy intuition about why this definition is interesting: imagine that $S$ is a set of ""safety points"" and $H$ is the set of ""homes where people live."" If a disaster strikes and people begin making their way to the nearest safety point, then $d(H,S)$ (read: distance from houses to safety) describes the elapsed time until everyone gets to safety, assuming that everyone leaves immediately and moves with speed $1$ . I'm interested in the following problem: $H$ = ""the homes where people live"" is given, and we need to place $k$ -many safety points in an optimal way, so that the elapsed time before everyone is safe is minimized. More formally: Problem. Given $H$ , a bounded subset of $\mathbb{R}^n$ , and $k$ , a natural number find: a subset $S$ of $\mathbb{R}^n$ satisfying $|S| \leq k$ , such that for all ""competing"" subsets $S'$ of $\mathbb{R}^n$ satisfying $|S'| \leq k,$ we have $d(H,S) \leq d(H,S')$ . Question. Have any tools or techniques been developed to solve this problem for sufficiently nice $H$ ?","Definition 0. Given a metric space and subsets and thereof, define: (This as an asymmetric version of the Hausdorff distance .) Here's some slightly dodgy intuition about why this definition is interesting: imagine that is a set of ""safety points"" and is the set of ""homes where people live."" If a disaster strikes and people begin making their way to the nearest safety point, then (read: distance from houses to safety) describes the elapsed time until everyone gets to safety, assuming that everyone leaves immediately and moves with speed . I'm interested in the following problem: = ""the homes where people live"" is given, and we need to place -many safety points in an optimal way, so that the elapsed time before everyone is safe is minimized. More formally: Problem. Given , a bounded subset of , and , a natural number find: a subset of satisfying , such that for all ""competing"" subsets of satisfying we have . Question. Have any tools or techniques been developed to solve this problem for sufficiently nice ?","X H S d(H,S) = \sup_{h \in H} \inf_{s \in S}d(h,s) S H d(H,S) 1 H k H \mathbb{R}^n k S \mathbb{R}^n |S| \leq k S' \mathbb{R}^n |S'| \leq k, d(H,S) \leq d(H,S') H","['real-analysis', 'optimization', 'operations-research']"
80,Rearrangements that never change the value of a sum,Rearrangements that never change the value of a sum,,"Which bijections $f:\{1,2,3,\ldots\}\to\{1,2,3,\ldots\}$ have the property that for every sequence $\{a_n\}_{n=1}^\infty$, $$ \lim_{n\to\infty} \sum_{k=1}^n a_k = \lim_{n\to\infty} \sum_{k=1}^n a_{f(k)}, $$ where ""$=$"" is construed as meaning that if either limit exists then so does the other and in that case then they are equal? It is clear that there are uncountably many of these. Might it just be that $\{f(n)/n : n=1,2,3,\ldots\}$ is bounded away from both $0$ and $\infty$? These bijections form a group.  Can anything of interest be said about them as a group? PS: Here's another moderately wild guess (the one above appears to be wrong): Might it be just the bijections whose every orbit is finite?","Which bijections $f:\{1,2,3,\ldots\}\to\{1,2,3,\ldots\}$ have the property that for every sequence $\{a_n\}_{n=1}^\infty$, $$ \lim_{n\to\infty} \sum_{k=1}^n a_k = \lim_{n\to\infty} \sum_{k=1}^n a_{f(k)}, $$ where ""$=$"" is construed as meaning that if either limit exists then so does the other and in that case then they are equal? It is clear that there are uncountably many of these. Might it just be that $\{f(n)/n : n=1,2,3,\ldots\}$ is bounded away from both $0$ and $\infty$? These bijections form a group.  Can anything of interest be said about them as a group? PS: Here's another moderately wild guess (the one above appears to be wrong): Might it be just the bijections whose every orbit is finite?",,"['sequences-and-series', 'permutations', 'conditional-convergence']"
81,Capacity vs measure of a set - intuitive understanding,Capacity vs measure of a set - intuitive understanding,,"There is a concept of measure of ""largeness"" of a set, called capacity. The intuition is, instead of physical largeness (measured by Hausdorff or Lebesgue measure), capacity measures how good a given set is in at holding charge. Analytically, the $2$-capacity of a set $\Omega$ sitting inside a Riemannian manifold $M$ is given by $$cap_2(\Omega) = \inf_{u \in C^\infty_0(M), u|_\Omega \equiv 1} \int_M |\nabla u|^2 dM.$$ In the special case $M = \mathbb{R}^3$, I would like to intuitively understand what large capacity actually means. By the heuristic of ability to contain charge, it seems to me that a disc on the surface of the sphere $S^2$ would have larger capacity than that same (isometric) disc if it were rolled and folded upon itself. As another example, consider a book held open so that the angle between the pages is $180^\circ$. Intuitively it seems that the open surface of the pages now have a higher capacity than if the book were partially closed to make the angle, say, $45^\circ$. Is this intuitive understanding correct? How do I prove these rigorously from the definition?","There is a concept of measure of ""largeness"" of a set, called capacity. The intuition is, instead of physical largeness (measured by Hausdorff or Lebesgue measure), capacity measures how good a given set is in at holding charge. Analytically, the $2$-capacity of a set $\Omega$ sitting inside a Riemannian manifold $M$ is given by $$cap_2(\Omega) = \inf_{u \in C^\infty_0(M), u|_\Omega \equiv 1} \int_M |\nabla u|^2 dM.$$ In the special case $M = \mathbb{R}^3$, I would like to intuitively understand what large capacity actually means. By the heuristic of ability to contain charge, it seems to me that a disc on the surface of the sphere $S^2$ would have larger capacity than that same (isometric) disc if it were rolled and folded upon itself. As another example, consider a book held open so that the angle between the pages is $180^\circ$. Intuitively it seems that the open surface of the pages now have a higher capacity than if the book were partially closed to make the angle, say, $45^\circ$. Is this intuitive understanding correct? How do I prove these rigorously from the definition?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'reference-request', 'potential-theory']"
82,"E is measurable, then measure of E is the sum of the inner measure of a subset of E and the outer measure of the complement of the subset in E","E is measurable, then measure of E is the sum of the inner measure of a subset of E and the outer measure of the complement of the subset in E",,"If E is a measurable and A is any subset of E, show that $|E|=|A|_i+|E-A|_e$ where |E| is the measure of of E, $|A|_i$ is the inner measure of A, and $|E-A|_e$ is the outer measure of $E-A$. I have proved $|E|\geq|A|_i+|E-A|_e$, but I can't seem to figure out $\leq$ part of the proof. I just need hints. Please don't solve it for me. Thank you!","If E is a measurable and A is any subset of E, show that $|E|=|A|_i+|E-A|_e$ where |E| is the measure of of E, $|A|_i$ is the inner measure of A, and $|E-A|_e$ is the outer measure of $E-A$. I have proved $|E|\geq|A|_i+|E-A|_e$, but I can't seem to figure out $\leq$ part of the proof. I just need hints. Please don't solve it for me. Thank you!",,"['real-analysis', 'measure-theory']"
83,Half Solved: A problem on the heat operator not being elliptic with a weakened version of elliptic regularity,Half Solved: A problem on the heat operator not being elliptic with a weakened version of elliptic regularity,,"I should first mention this: in my studies of Sobolev spaces I have completed all the questions of chapter 9 from Folland's real analysis with the help of this site and this is my last one, which is also very interesting, but seemingly difficult so I am posting here in the hopes of getting some help, but I should first mention some notation conventions used here. $ \mathcal{S} $ is the Schwartz class of functions and $ \mathcal{S}' $ is the class of distributions (""generalized functions"") on $ \mathcal{S} $ $ \partial ^ {\alpha} $ is the multi-index distribution derivative on $ L^2 $ $ \Lambda _ s f = { [(1+|\xi|^2)^{\frac{s}{2}} \widetilde{f} ]^{\vee} } $ is a continuous linear operator on the Schwartz distributions. The Sobolev space $ H_s = \{ f \in S' | \Lambda_s f \in L^2 \}  $ Now define the following differential operator $ D^{\alpha} = (2i\pi)^{-\alpha}\partial^{\alpha} $ along with its symbol $ P(D) = \sum_{|\alpha|<m} (2\pi i)^{|\alpha|}a_{\alpha} D^{\alpha} $ , this of course has the property that the Fourier transform of $ P(D)[f] $ is equal to $ P\widehat{(f)} $ And now the elliptic regularity theorem as I know it: Suppose that L is a constant-coefficient elliptic differential operator of order m and $ \Omega $ is open in $ R^n $ and $ u \in D'(\Omega) $ . If $ Lu \in H_{s}^{loc}(\Omega) $ for some $ s \in R $ , then $ u \in H_{s+m}^{loc}(\Omega) $ . Also, if $ Lu \in C^{\infty}(\Omega) $ then $ u \in C^{\infty}(\Omega) $ . I can honestly say this theorem makes sense to me. Now for the exercise at hand: The heat operator $ \partial_t - \Delta $ is not elliptic but a weakened version of the elliptic regularity theorem does hold for it. Here we are workign on $ R^{n+1} $ with coordinates $ (x,t) $ and dual coordinates $ (\xi,\tau) $ , and $ \partial_t - \Delta = P(D) $ where $ P(\xi,\tau) = 2i\pi\tau + 4 {\pi}^2 |\xi|^2  $ . a. There exist $ C,R > 0 $ such that $ |\xi| |(\xi,\tau)|^{\frac{1}{2}} \leq C |P(\xi,\tau)| $ for $ |(\xi,\tau)| > R $ (We are to try to consider the region $  |\tau| \leq |\xi|^2 $ and its complement separately). b. If $ f \in H_s $ and $ (\partial_t - \Delta)f \in H_s $ then we are to show $ f \in H_{s+1} $ and $ \partial _ {x_i} f \in H_{s+(1/2)} $ for $ 1 \leq i \leq n $ c. If $ \zeta \in C_C^{\infty}(R^{n+1}) $ , then we are to show: $ [\partial_t - \Delta,\zeta]f = (\partial_t \zeta - \Delta \zeta)f - 2 \sum{(\partial_{x_i}\zeta)(\partial_{x_i}f)} $ d. We are to show that if $ \Omega \in R^{n+1} $ is open and $ u \in \mathcal{D}'(\Omega) $ and if $ (\partial_t-\Delta)u \in H_{s}^{loc}(\Omega) $ then $ u \in H_{s+1}^{loc}(\Omega) $ . We are given this hint for part d: Here is the proof of theorem 9.26 the elliptic regularity theorem: My problem is this is fairly advanced stuff and I cannot seem to find it in me to write this down rigorously despite being able to see the geometric intuition behind it, but still this is hard analysis and I am a novice... I am positng this here following the successes I had and knowing this is the last question I have before I can truly say I completed this chapter's exercises. Thanks in advance to all helpers. ********** Progressed I think I am only truly stuck on parts b and d where for part d I cannot seem to imitate the proof referenced.","I should first mention this: in my studies of Sobolev spaces I have completed all the questions of chapter 9 from Folland's real analysis with the help of this site and this is my last one, which is also very interesting, but seemingly difficult so I am posting here in the hopes of getting some help, but I should first mention some notation conventions used here. is the Schwartz class of functions and is the class of distributions (""generalized functions"") on is the multi-index distribution derivative on is a continuous linear operator on the Schwartz distributions. The Sobolev space Now define the following differential operator along with its symbol , this of course has the property that the Fourier transform of is equal to And now the elliptic regularity theorem as I know it: Suppose that L is a constant-coefficient elliptic differential operator of order m and is open in and . If for some , then . Also, if then . I can honestly say this theorem makes sense to me. Now for the exercise at hand: The heat operator is not elliptic but a weakened version of the elliptic regularity theorem does hold for it. Here we are workign on with coordinates and dual coordinates , and where . a. There exist such that for (We are to try to consider the region and its complement separately). b. If and then we are to show and for c. If , then we are to show: d. We are to show that if is open and and if then . We are given this hint for part d: Here is the proof of theorem 9.26 the elliptic regularity theorem: My problem is this is fairly advanced stuff and I cannot seem to find it in me to write this down rigorously despite being able to see the geometric intuition behind it, but still this is hard analysis and I am a novice... I am positng this here following the successes I had and knowing this is the last question I have before I can truly say I completed this chapter's exercises. Thanks in advance to all helpers. ********** Progressed I think I am only truly stuck on parts b and d where for part d I cannot seem to imitate the proof referenced."," \mathcal{S}   \mathcal{S}'   \mathcal{S}   \partial ^ {\alpha}   L^2   \Lambda _ s f = { [(1+|\xi|^2)^{\frac{s}{2}} \widetilde{f} ]^{\vee} }   H_s = \{ f \in S' | \Lambda_s f \in L^2 \}    D^{\alpha} = (2i\pi)^{-\alpha}\partial^{\alpha}   P(D) = \sum_{|\alpha|<m} (2\pi i)^{|\alpha|}a_{\alpha} D^{\alpha}   P(D)[f]   P\widehat{(f)}   \Omega   R^n   u \in D'(\Omega)   Lu \in H_{s}^{loc}(\Omega)   s \in R   u \in H_{s+m}^{loc}(\Omega)   Lu \in C^{\infty}(\Omega)   u \in C^{\infty}(\Omega)   \partial_t - \Delta   R^{n+1}   (x,t)   (\xi,\tau)   \partial_t - \Delta = P(D)   P(\xi,\tau) = 2i\pi\tau + 4 {\pi}^2 |\xi|^2    C,R > 0   |\xi| |(\xi,\tau)|^{\frac{1}{2}} \leq C |P(\xi,\tau)|   |(\xi,\tau)| > R    |\tau| \leq |\xi|^2   f \in H_s   (\partial_t - \Delta)f \in H_s   f \in H_{s+1}   \partial _ {x_i} f \in H_{s+(1/2)}   1 \leq i \leq n   \zeta \in C_C^{\infty}(R^{n+1})   [\partial_t - \Delta,\zeta]f = (\partial_t \zeta - \Delta \zeta)f - 2 \sum{(\partial_{x_i}\zeta)(\partial_{x_i}f)}   \Omega \in R^{n+1}   u \in \mathcal{D}'(\Omega)   (\partial_t-\Delta)u \in H_{s}^{loc}(\Omega)   u \in H_{s+1}^{loc}(\Omega) ","['real-analysis', 'partial-differential-equations', 'fourier-analysis', 'sobolev-spaces', 'distribution-theory']"
84,"Proof that if $f$ is integrable and $\int_E {f}\,{d\mu} = 0$ for all $E\in\Sigma$ then $f=0$ almost everywhere",Proof that if  is integrable and  for all  then  almost everywhere,"f \int_E {f}\,{d\mu} = 0 E\in\Sigma f=0","Proof that if $f$ is integrable and $\int_E {f}\,{d\mu} = 0$ for all $E\in\Sigma$ then $f=0$ almost everywhere My attempt: I called: $A = \{ x\in X : f(x) \ne 0 \}$ $B_n = \{ x\in X: f(x) \gt \frac{1}{n} \}$ $C_n = \{ x\in X:f(x) \lt -\frac{1}{n} \}$ $B = \bigcup_{n=1}^{\infty}{B_n}$ $C = \bigcup_{n=1}^{\infty}{C_n}$ Then: $A = B\cup C$ (with $B\cap C=\emptyset$),  $B_{n} \subset B_{n+1}$ and $C_{n} \subset C_{n+1}$. Since $\mu$ is continuous  we have: $\mu(B_n)\to \mu(B)$ and $\mu(C_n)\to \mu(C)$ Let's suppose that $\mu(A)>0$. Since $\mu(A) = \mu(B) + \mu(C)$, we have two possibilities: a) $\mu(B)>0$ Since $\mu(B_n)\to \mu(B)$, there is an $k$ such that $\mu(B_{k}) \gt 0$. Thus  $$\int_{B_k} {f}\,{d\mu}  \geq \int_{B_k} {\frac{1}{k}}\,{d\mu} =  \frac{\mu(B_k)}{k} \gt 0$$ which is a contradiction. b) $\mu(C)>0$ Since $\mu(C_n)\to \mu(C)$, there is an $k$ such that $\mu(C_{k}) \gt 0$. Thus  $$\int_{C_k} {(-f)}\,{d\mu}  \geq \int_{C_k} {\frac{1}{k}}\,{d\mu} =  \frac{\mu(C_k)}{k} \gt 0$$ which is a contradiction. Thus $\mu(A)=0$. Am I right? Is there an easier way to solve this problem?","Proof that if $f$ is integrable and $\int_E {f}\,{d\mu} = 0$ for all $E\in\Sigma$ then $f=0$ almost everywhere My attempt: I called: $A = \{ x\in X : f(x) \ne 0 \}$ $B_n = \{ x\in X: f(x) \gt \frac{1}{n} \}$ $C_n = \{ x\in X:f(x) \lt -\frac{1}{n} \}$ $B = \bigcup_{n=1}^{\infty}{B_n}$ $C = \bigcup_{n=1}^{\infty}{C_n}$ Then: $A = B\cup C$ (with $B\cap C=\emptyset$),  $B_{n} \subset B_{n+1}$ and $C_{n} \subset C_{n+1}$. Since $\mu$ is continuous  we have: $\mu(B_n)\to \mu(B)$ and $\mu(C_n)\to \mu(C)$ Let's suppose that $\mu(A)>0$. Since $\mu(A) = \mu(B) + \mu(C)$, we have two possibilities: a) $\mu(B)>0$ Since $\mu(B_n)\to \mu(B)$, there is an $k$ such that $\mu(B_{k}) \gt 0$. Thus  $$\int_{B_k} {f}\,{d\mu}  \geq \int_{B_k} {\frac{1}{k}}\,{d\mu} =  \frac{\mu(B_k)}{k} \gt 0$$ which is a contradiction. b) $\mu(C)>0$ Since $\mu(C_n)\to \mu(C)$, there is an $k$ such that $\mu(C_{k}) \gt 0$. Thus  $$\int_{C_k} {(-f)}\,{d\mu}  \geq \int_{C_k} {\frac{1}{k}}\,{d\mu} =  \frac{\mu(C_k)}{k} \gt 0$$ which is a contradiction. Thus $\mu(A)=0$. Am I right? Is there an easier way to solve this problem?",,"['real-analysis', 'measure-theory']"
85,Another conditionl leading to irrationality of $\sum _{k=1}^ \infty \dfrac 1{n_k}$?,Another conditionl leading to irrationality of ?,\sum _{k=1}^ \infty \dfrac 1{n_k},"If $\{n_k\}$ is a strictly increasing sequence of positive integers such that $\lim \inf _{k \to \infty} n_k ^{1/2^k} >1$ and $\lim _{k \to \infty} n_k^{1/2^k}$ does not exist , then is it true that $\sum _{k=1}^ \infty \dfrac 1{n_k}$ is irrational ?","If $\{n_k\}$ is a strictly increasing sequence of positive integers such that $\lim \inf _{k \to \infty} n_k ^{1/2^k} >1$ and $\lim _{k \to \infty} n_k^{1/2^k}$ does not exist , then is it true that $\sum _{k=1}^ \infty \dfrac 1{n_k}$ is irrational ?",,"['real-analysis', 'sequences-and-series']"
86,How prove this inequality $\left(\int_{0}^{1}f(x)dx\right)^2\le\frac{1}{12}\int_{0}^{1}|f'(x)|^2dx$,How prove this inequality,\left(\int_{0}^{1}f(x)dx\right)^2\le\frac{1}{12}\int_{0}^{1}|f'(x)|^2dx,"Let $f\in C^{1}[0,1]$ such that $f(0)=f(1)=0$. Show that   $$\left(\int_{0}^{1}f(x)dx\right)^2\le\dfrac{1}{12}\int_{0}^{1}|f'(x)|^2dx.$$ I think we must use Cauchy-Schwarz inequality  $$\int_{0}^{1}|f'(x)|^2dx\ge \left(\int_{0}^{1}f(x)dx\right)^2$$ but this maybe is not useful to problem. The coefficient $\dfrac{1}{12}$ is strange. How find it ?","Let $f\in C^{1}[0,1]$ such that $f(0)=f(1)=0$. Show that   $$\left(\int_{0}^{1}f(x)dx\right)^2\le\dfrac{1}{12}\int_{0}^{1}|f'(x)|^2dx.$$ I think we must use Cauchy-Schwarz inequality  $$\int_{0}^{1}|f'(x)|^2dx\ge \left(\int_{0}^{1}f(x)dx\right)^2$$ but this maybe is not useful to problem. The coefficient $\dfrac{1}{12}$ is strange. How find it ?",,['integral-inequality']
87,Two disjoint connected and bounded open sets in the plane that shares the same boundary,Two disjoint connected and bounded open sets in the plane that shares the same boundary,,"In $\mathbb{R}^2$ with std. topology I want to exhibit two open sets that are connected, bounded and disjoint but that have a common boundary. My attempt:  Since both my sets need to be bounded, my thought was to limit them by the unitary ball. Its easy to construct two disjoint open sets that have a common boundary in the interior of the unitary circle, the problem is, of course, the exterior (the boundary of the union). So I considered two closed paths, helix-like curves that in polar coordinates can be expressed by $\gamma(t) = (1-e^{-t},t)$ $\gamma_2(t) = (1-e^{-0.5t},t)$ So, set $U$ consists of the points between the two curves, that is, $U = \{(\rho, \theta) : 1-e^{-0.5\theta}<\rho<1-e^{-\theta}\}$ and $V = B_1(0)\setminus \overline{U}$. It's easy to show that $U$ is open, bounded and since it's path-connected it is connected. Same goes to $V$ and I guess $\partial U = \partial V = Im(\gamma)\cup Im(\gamma_2) \cup D_1$ ($D_1$ being the unitary disk). My questions: Is there an easier construction? Is my construction right?","In $\mathbb{R}^2$ with std. topology I want to exhibit two open sets that are connected, bounded and disjoint but that have a common boundary. My attempt:  Since both my sets need to be bounded, my thought was to limit them by the unitary ball. Its easy to construct two disjoint open sets that have a common boundary in the interior of the unitary circle, the problem is, of course, the exterior (the boundary of the union). So I considered two closed paths, helix-like curves that in polar coordinates can be expressed by $\gamma(t) = (1-e^{-t},t)$ $\gamma_2(t) = (1-e^{-0.5t},t)$ So, set $U$ consists of the points between the two curves, that is, $U = \{(\rho, \theta) : 1-e^{-0.5\theta}<\rho<1-e^{-\theta}\}$ and $V = B_1(0)\setminus \overline{U}$. It's easy to show that $U$ is open, bounded and since it's path-connected it is connected. Same goes to $V$ and I guess $\partial U = \partial V = Im(\gamma)\cup Im(\gamma_2) \cup D_1$ ($D_1$ being the unitary disk). My questions: Is there an easier construction? Is my construction right?",,"['real-analysis', 'general-topology', 'proof-verification', 'examples-counterexamples']"
88,Proving translational invariance of Lebesgue integral,Proving translational invariance of Lebesgue integral,,"I am asked to show that the Lebesgue integral is invariant under translations. Specifically, Let $(\mathbb{R}, \Sigma, \mu)$ be a measure space, and for any $f:\mathbb{R}\rightarrow\mathbb{R}$ define $f_a:\mathbb{R}\rightarrow\mathbb{R}$ by $f_a(x)=f(x-a)$, for all $x\in\mathbb{R}$. Assume that the measure $\mu$ is such that $\mu(A)=\mu(A+a)$, where $A+a=\{x+a:x\in A\}$, for $A\in\Sigma$ and $a\in\mathbb{R}$. Assume also that $A+a\in\Sigma$ for any $A\in\Sigma$ and $a\in\mathbb{R}$. Let $f:\mathbb{R}\rightarrow[0,\infty]$ be a simple, measurable function, $f=\sum_{j=1}^m f_j$, with $f_j=a_j\chi_{A_j}$, $a_j\ge0$ and $A_j\in\Sigma$ for all $1\le j \le m$. The sets $A_j$ are pairwise disjoint. Show that \begin{align} \int_{\mathbb{R}}f\, \mathrm{d}\mu = \int_{\mathbb{R}}f_a\,\mathrm{d}\mu.\end{align} Show that this also holds for any measurable $f:\mathbb{R}\rightarrow[0,\infty]$. Now, I have attempted to solve the first part by writing $f_a$ as  \begin{align} f_a(x) = f(x-a) &= \sum_{j=1}^m f_j(x-a) = \sum_{j=1}^m a_j\chi_{A_j}(x-a) = \sum_{j=1}^m a_j\chi_{A_j+a}(x), \end{align} and then computing  \begin{align} \int_{\mathbb{R}}f(x)\,\mathrm{d}\mu(x) &= \int_{\mathbb{R}}\sum_{j=1}^m f_j(x)\,\mathrm{d}\mu(x)\\ &= \sum_{j=1}^m a_j\mu(A_j) = \sum_{j=1}^m a_j\mu(A_j+a) \\ &= \int_{\mathbb{R}}\sum_{j=1}^m f_{j,a}(x)\,\mathrm{d}\mu(x) = \int_{\mathbb{R}}f_a(x)\,\mathrm{d}\mu(x), \end{align} using the fact that $\mu(A_j)=\mu(A_j+a)$. Now, does this make sense? Also, in order to generalize to arbitrary measurable, positive $f$, can I just use the fact that any measurable function is the pointwise limit of a sequence of simple functions, and apply the monotone convergence theorem? Any help would be greatly appreciated!","I am asked to show that the Lebesgue integral is invariant under translations. Specifically, Let $(\mathbb{R}, \Sigma, \mu)$ be a measure space, and for any $f:\mathbb{R}\rightarrow\mathbb{R}$ define $f_a:\mathbb{R}\rightarrow\mathbb{R}$ by $f_a(x)=f(x-a)$, for all $x\in\mathbb{R}$. Assume that the measure $\mu$ is such that $\mu(A)=\mu(A+a)$, where $A+a=\{x+a:x\in A\}$, for $A\in\Sigma$ and $a\in\mathbb{R}$. Assume also that $A+a\in\Sigma$ for any $A\in\Sigma$ and $a\in\mathbb{R}$. Let $f:\mathbb{R}\rightarrow[0,\infty]$ be a simple, measurable function, $f=\sum_{j=1}^m f_j$, with $f_j=a_j\chi_{A_j}$, $a_j\ge0$ and $A_j\in\Sigma$ for all $1\le j \le m$. The sets $A_j$ are pairwise disjoint. Show that \begin{align} \int_{\mathbb{R}}f\, \mathrm{d}\mu = \int_{\mathbb{R}}f_a\,\mathrm{d}\mu.\end{align} Show that this also holds for any measurable $f:\mathbb{R}\rightarrow[0,\infty]$. Now, I have attempted to solve the first part by writing $f_a$ as  \begin{align} f_a(x) = f(x-a) &= \sum_{j=1}^m f_j(x-a) = \sum_{j=1}^m a_j\chi_{A_j}(x-a) = \sum_{j=1}^m a_j\chi_{A_j+a}(x), \end{align} and then computing  \begin{align} \int_{\mathbb{R}}f(x)\,\mathrm{d}\mu(x) &= \int_{\mathbb{R}}\sum_{j=1}^m f_j(x)\,\mathrm{d}\mu(x)\\ &= \sum_{j=1}^m a_j\mu(A_j) = \sum_{j=1}^m a_j\mu(A_j+a) \\ &= \int_{\mathbb{R}}\sum_{j=1}^m f_{j,a}(x)\,\mathrm{d}\mu(x) = \int_{\mathbb{R}}f_a(x)\,\mathrm{d}\mu(x), \end{align} using the fact that $\mu(A_j)=\mu(A_j+a)$. Now, does this make sense? Also, in order to generalize to arbitrary measurable, positive $f$, can I just use the fact that any measurable function is the pointwise limit of a sequence of simple functions, and apply the monotone convergence theorem? Any help would be greatly appreciated!",,"['real-analysis', 'lebesgue-integral']"
89,"Show that if $F$ is continuous, then it is continuous in each variable separately.","Show that if  is continuous, then it is continuous in each variable separately.",F,"Can someone please verify my proof? I am aware that there may be a similar question posted elsewhere, but I need help with my proof in particular. $\textbf{Note:}$ This is not homework. Let $F: X \times Y \longrightarrow Z$. We say that $F$ is continuous in each variable separately if for each $y_0 \in Y$, the map $h: X \longrightarrow Z$ defined by $h(x) = F(x, y_0)$ is continuous, and for each $x_0 \in X$, the map $k: Y \longrightarrow Z$ defined by $k(y) = F(x_0, y)$ is continuous. Show that if $F$ is continuous, then $F$ is continuous in each variable separately. We show that $h$, as defined above, is continuous. It can similarly be proven that $k$ is continuous. Let $U \subseteq Z$ be open. Then, $$h^{-1}(U) = \{x : (x, y_0) \in F^{-1}(U)\}$$ Let $(X, y_0)$ denote the set $\{(x, y_0): x \in X\}$. Clearly, $F^{-1}(U) \cap (X, y_0)$ is open in $(X, y_0)$ under the subspace topology. Now, note that $(X, y_0)$ is homeomorphic to $X$, with homeomorphism $f:(X, y_0) \longrightarrow X$ defined by $f(x) = x$. So, $$f(F^{-1}(U) \cap (X, y_0))$$ is open in $X$. But this set is precisely $h^{-1}(U)$. Therefore, $h$ is continuous.","Can someone please verify my proof? I am aware that there may be a similar question posted elsewhere, but I need help with my proof in particular. $\textbf{Note:}$ This is not homework. Let $F: X \times Y \longrightarrow Z$. We say that $F$ is continuous in each variable separately if for each $y_0 \in Y$, the map $h: X \longrightarrow Z$ defined by $h(x) = F(x, y_0)$ is continuous, and for each $x_0 \in X$, the map $k: Y \longrightarrow Z$ defined by $k(y) = F(x_0, y)$ is continuous. Show that if $F$ is continuous, then $F$ is continuous in each variable separately. We show that $h$, as defined above, is continuous. It can similarly be proven that $k$ is continuous. Let $U \subseteq Z$ be open. Then, $$h^{-1}(U) = \{x : (x, y_0) \in F^{-1}(U)\}$$ Let $(X, y_0)$ denote the set $\{(x, y_0): x \in X\}$. Clearly, $F^{-1}(U) \cap (X, y_0)$ is open in $(X, y_0)$ under the subspace topology. Now, note that $(X, y_0)$ is homeomorphic to $X$, with homeomorphism $f:(X, y_0) \longrightarrow X$ defined by $f(x) = x$. So, $$f(F^{-1}(U) \cap (X, y_0))$$ is open in $X$. But this set is precisely $h^{-1}(U)$. Therefore, $h$ is continuous.",,"['real-analysis', 'general-topology', 'proof-verification']"
90,locally convex space generated by a countable family of seminorms is metrizable,locally convex space generated by a countable family of seminorms is metrizable,,"Let $X$ be a locally convex Hausdorff topological space, whose topology if generated by the countable family of seminorms $\{p_i:\space i\in\mathbb{N}\}$. I'd like to prove that $X$ is metrizable. So, let $$d(x,y)=\sum_{k=1}^{\infty}\frac{1}{2^k}\frac{p_k(x-y)}{1+p_k(x-y)}.$$ It is easy to see that $d$ is a metric indeed. I'd like to prove that this is a one we need, that generates the original convergence. Is that true by the way? Let $x_n\to x$ in the introduced metric. Then for all $k$ we have $\frac{1}{2^k}\frac{p_k(x_n-x)}{1+p_k(x_n-x)}\le d(x_n,x)$ and thus $p_k(x_n-x)\to 0$ for each $k$. The last means the convergence in the original locally convex topology. How to do the converse? So, suppose $p_k(x_n-x)\to 0$ ($n\to \infty$) for each $k$. How can I prove that $d(x_n,x)\to 0$. Althouhg it seems to be an elementary mathematical analysis, could you tell me wheather the metric I inroduce suits us and how to prove that if yes?","Let $X$ be a locally convex Hausdorff topological space, whose topology if generated by the countable family of seminorms $\{p_i:\space i\in\mathbb{N}\}$. I'd like to prove that $X$ is metrizable. So, let $$d(x,y)=\sum_{k=1}^{\infty}\frac{1}{2^k}\frac{p_k(x-y)}{1+p_k(x-y)}.$$ It is easy to see that $d$ is a metric indeed. I'd like to prove that this is a one we need, that generates the original convergence. Is that true by the way? Let $x_n\to x$ in the introduced metric. Then for all $k$ we have $\frac{1}{2^k}\frac{p_k(x_n-x)}{1+p_k(x_n-x)}\le d(x_n,x)$ and thus $p_k(x_n-x)\to 0$ for each $k$. The last means the convergence in the original locally convex topology. How to do the converse? So, suppose $p_k(x_n-x)\to 0$ ($n\to \infty$) for each $k$. How can I prove that $d(x_n,x)\to 0$. Althouhg it seems to be an elementary mathematical analysis, could you tell me wheather the metric I inroduce suits us and how to prove that if yes?",,"['real-analysis', 'functional-analysis', 'topological-vector-spaces']"
91,"Prove that if $f$ is uniformly continuous on a bounded set $S$, then $f$ is bounded on S","Prove that if  is uniformly continuous on a bounded set , then  is bounded on S",f S f,"Prove that if $f$ is uniformly continuous on a bounded set $S$, then $f$ is bounded on $S$. Here's my proof. Can someone please verify it? Suppose $f$ is not bounded on $S$. Then, $\forall n \in \mathbb{N}$, there corresponds an $f(x_n)$ such that $|f(x_n)|>n$, where $x_n \in S$. Pick a convergent subsequence $(x_{n_k})$ of $(x_n)$. Then, $(x_{n_k})$ is cauchy. Since $f$ is uniformly continuous on $S$, this implies that $f(x_{n_k})$ is cauchy. However, this cannot be the case, since $f(x_{n_k})$ clearly diverges. Therefore, it must be the case that $f$ is bounded on $S$.","Prove that if $f$ is uniformly continuous on a bounded set $S$, then $f$ is bounded on $S$. Here's my proof. Can someone please verify it? Suppose $f$ is not bounded on $S$. Then, $\forall n \in \mathbb{N}$, there corresponds an $f(x_n)$ such that $|f(x_n)|>n$, where $x_n \in S$. Pick a convergent subsequence $(x_{n_k})$ of $(x_n)$. Then, $(x_{n_k})$ is cauchy. Since $f$ is uniformly continuous on $S$, this implies that $f(x_{n_k})$ is cauchy. However, this cannot be the case, since $f(x_{n_k})$ clearly diverges. Therefore, it must be the case that $f$ is bounded on $S$.",,"['real-analysis', 'proof-verification']"
92,Uncountable set has uncountably many limit points. (Proof Checking Request.),Uncountable set has uncountably many limit points. (Proof Checking Request.),,"Show that any uncountable subset of the reals has uncountably many limit points. Let $S\subseteq \mathbb R$  be uncountable and let $L$ be the set of all the limit points of $S$. Assume on the contrary that $L$ is at most countable. Now. For all $x\in S\setminus L$, there exist $a_x,b_x\in \mathbb Q$ be such that the open interval $U_x=(a_x,b_x)$ contains exactly one point of $S$. Note that for all $x,y\in S\setminus L$, we have $x\neq y\iff U_x\neq U_y$. Write $\mathcal C=\{U_x:x\in S\setminus L\}$. Note that $\mathcal C$ is in bijection with $S\setminus L$. But $\mathcal C$ can be viewed as a subset of $\mathbb Q\times \mathbb Q$ and      hence $\mathcal C$ is countable. This forces $S$ to be countable which contradicts the hypothesis. Hence we achieve the required contradiction and the proof is complete.","Show that any uncountable subset of the reals has uncountably many limit points. Let $S\subseteq \mathbb R$  be uncountable and let $L$ be the set of all the limit points of $S$. Assume on the contrary that $L$ is at most countable. Now. For all $x\in S\setminus L$, there exist $a_x,b_x\in \mathbb Q$ be such that the open interval $U_x=(a_x,b_x)$ contains exactly one point of $S$. Note that for all $x,y\in S\setminus L$, we have $x\neq y\iff U_x\neq U_y$. Write $\mathcal C=\{U_x:x\in S\setminus L\}$. Note that $\mathcal C$ is in bijection with $S\setminus L$. But $\mathcal C$ can be viewed as a subset of $\mathbb Q\times \mathbb Q$ and      hence $\mathcal C$ is countable. This forces $S$ to be countable which contradicts the hypothesis. Hence we achieve the required contradiction and the proof is complete.",,"['real-analysis', 'proof-verification']"
93,Equivalent definitions of Lebesgue Measurability (Rudin and Royden),Equivalent definitions of Lebesgue Measurability (Rudin and Royden),,"I'm reading Royden's real analysis 4th edition, and he defines a real set $E$ to be lebesgue measurable if, for all real sets $A$, $m(A)=m(A∩E)+m(A∩E^c)$. Here, $m$ is the outer measure of a set. I believe this is called Caratheodory's criterion. Now, I'm also reading Baby Rudin (Principles of Analysis), and in chapter 11, Rudin defines a real set to be (lebesgue) measurable if it is the union of a countable collection of finitely measurable real sets, and he has another definition for ""finitely measurable"" which is a bit lengthy to type up. I'm having trouble proving that these two definitions are equivalent; that is, I want to show that the set of lebesgue measurable sets constructed under Royden's definition is the same as the set of lebesgue measurable sets constructed under Rudin's definition. Can someone who is familiar with both definitions (especially rudin's definition) help me out by suggesting a proof or a text that may help? So far, I managed to prove that a set measurable under rudin's definition satisfies the caratheodory criterion. I did this by using the following facts: $m(A)=m(A∩E)+m(A∩E^c)$ if and only if for every $\epsilon >0$, there exists an open set $O$ such that $m(O\setminus E) \le \epsilon$. Also: If a set $E$ is measurable under Rudin""s definition then there exists an open set $O$ such that $m(O\setminus E) \le \epsilon$. Can somebody help me out with the converse?","I'm reading Royden's real analysis 4th edition, and he defines a real set $E$ to be lebesgue measurable if, for all real sets $A$, $m(A)=m(A∩E)+m(A∩E^c)$. Here, $m$ is the outer measure of a set. I believe this is called Caratheodory's criterion. Now, I'm also reading Baby Rudin (Principles of Analysis), and in chapter 11, Rudin defines a real set to be (lebesgue) measurable if it is the union of a countable collection of finitely measurable real sets, and he has another definition for ""finitely measurable"" which is a bit lengthy to type up. I'm having trouble proving that these two definitions are equivalent; that is, I want to show that the set of lebesgue measurable sets constructed under Royden's definition is the same as the set of lebesgue measurable sets constructed under Rudin's definition. Can someone who is familiar with both definitions (especially rudin's definition) help me out by suggesting a proof or a text that may help? So far, I managed to prove that a set measurable under rudin's definition satisfies the caratheodory criterion. I did this by using the following facts: $m(A)=m(A∩E)+m(A∩E^c)$ if and only if for every $\epsilon >0$, there exists an open set $O$ such that $m(O\setminus E) \le \epsilon$. Also: If a set $E$ is measurable under Rudin""s definition then there exists an open set $O$ such that $m(O\setminus E) \le \epsilon$. Can somebody help me out with the converse?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
94,"Limits of series, proof of the convergence of two sequences","Limits of series, proof of the convergence of two sequences",,"I have two sequences $x_i$ and $y_i$ defined by their expressions : $$x_i-x_{i+1}=y_i-y_{i+1}=\sqrt{x_{i+1}y_{i+1}}$$ I have to prove that $xy(x-y)=0$. I tried this : I have $$x_i=x^{2^{i-1}}\prod_{j=0}^{j=i-2} (x^{2^j}+y^{2^j})^{-1}$$ And $$y_i=y^{2^{i-1}}\prod_{j=0}^{j=i-2} (x^{2^j}+y^{2^j})^{-1}$$ How to prove that $x=y$ ? I tried this : I suppose $x\neq{y}$ and I define the series. I see that  $$\sqrt{x_iy_i}=y_{i-1}-y_i=x_{i-1}-x_i$$ So $$x_i-x_{i+1}=\sqrt{x_{i+1}y_{i+1}}$$ $$x_{i-1}-x_i=\sqrt{x_iy_i}$$ $$\vdots$$ $$x_1-x_2=x-x_2=\sqrt{x_2y_2}$$ Or $$\sum_{j=2}^{j=i+1}{(\sqrt{x_jy_j})}=x-x_2+x_2-x_3+\cdots+x_i-x_{i+1}=x-x_{i+1}$$ Its limit is $$\sum_{j=2}^{j=\infty}{(\sqrt{x_jy_j})}=\lim_{i\longrightarrow{\infty}}{(x-x_{i+1})}$$ If $x\geq{y}$ $$\sum_{j=2}^{j=\infty}{(\sqrt{x_jy_j})}=\lim_{i\longrightarrow{\infty}}{(x-x_{i+1})}=x-(x-y)=y$$ If $x\leq{y}$ $$\sum_{j=2}^{j=\infty}{(\sqrt{x_jy_j})}=\lim_{i\longrightarrow{\infty}}{(x-x_{i+1})}=x$$ I suppose $x\geq{y}$. Hence $$\sum_{j=2}^{j=i}{((-1)^j\sqrt{x_jy_j})}=x-x_2-(x_2-x_3)+(x_3-x_4)-\cdots+(-1)^i(x_{i-1}-x_i)$$   $$=x-2x_2+2x_3-\cdots+2(-1)^{i-1}x_{i-1}+(-1)^{i+1}x_i$$   $$=2\sum_{j=2}^{j=i-1}{((-1)^{j+1}x_j)}+x+(-1)^{i+1}x_i$$ $$=2\sum_{j=1}^{j=i}{((-1)^{j+1}x_j)}-x-(-1)^{i+1}x_i$$ $$=\sum_{j=2}^{j=i-1}{((-1)^{j+1}x_j)}+\sum_{j=1}^{j=i}{((-1)^{j+1}x_j)}$$ $$=2\sum_{j=2}^{j=i-1}{((-1)^{j+1}y_j)}+y+(-1)^{i+1}y_i$$ $$=2\sum_{j=1}^{j=i}{((-1)^{j+1}y_j)}-y-(-1)^{i+1}y_i$$ $$=\sum_{j=2}^{j=i-1}{((-1)^{j+1}y_j)}+\sum_{j=1}^{j=i}{((-1)^{j+1}y_j)}$$ Or    $$2\sum_{j=1}^{j=i}{((-1)^{j+1}x_j)}=\sum_{j=2}^{j=i}{((-1)^j\sqrt{x_jy_j})}+x+(-1)^{i+1}x_i$$ And $$2\sum_{j=1}^{j=i}{((-1)^{j+1}y_j)}=\sum_{j=2}^{j=i}{((-1)^j\sqrt{x_jy_j})}+y+(-1)^{i+1}y_i$$ I do not know the limit of $(-1)^{i+1}x_i$,    $$\sum_{j=1}^{j=\infty}{((-1)^{j}x_j)}$$ may diverge. But   $$\sum_{j=2}^{j=\infty}{((-1)^j\sqrt{x_jy_j})}$$ is absolutely convergent. As $y_i$ tends to zero in the infinity, then   $$\sum_{j=1}^{j=\infty}{((-1)^{j}y_j)}$$ converge. The limit of   $$\sum_{j=2}^{j=i}{((-1)^j\sqrt{x_jy_j})}=2\sum_{j=1}^{j=i}{((-1)^{j+1}y_j)}-y-(-1)^{i+1}y_i$$   $$=2\sum_{j=1}^{j=i}{((-1)^{j+1}x_j)}-x-(-1)^{i+1}x_i$$ exists and the series are convergent. I will try to prove that   $$\lim_{i\longrightarrow{\infty}}{(x_i)}=x-y=0$$ Let  $$\sum_{k=1}^{k=2m}{((-1)^{k+1}x_{k}e^{-\frac{k}{\sqrt{2m}}})}$$  $$=xe^{-\frac{1}{\sqrt{2m}}}-x_2e^{-\frac{2}{\sqrt{2m}}}+x_3e^{-\frac{3}{\sqrt{2m}}}-\cdots+(-1)^{2m+1}x_{2m}e^{-\frac{2m}{\sqrt{2m}}}$$  $$=xe^{-\frac{2}{\sqrt{2m}}}+x(e^{-\frac{1}{\sqrt{2m}}}-e^{-\frac{2}{\sqrt{2m}}})-x_2e^{-\frac{2}{\sqrt{2m}}}+x_3e^{-\frac{4}{\sqrt{2m}}}+x_3(e^{-\frac{3}{\sqrt{2m}}}-e^{-\frac{4}{\sqrt{2m}}})-x_4e^{-\frac{4}{\sqrt{2m}}}+\cdots-x_{2m}e^{-\frac{2m}{\sqrt{2m}}}$$  $$=xe^{-\frac{2}{\sqrt{2m}}}(e^{\frac{1}{\sqrt{2m}}}-1)+x_3e^{-\frac{4}{\sqrt{2m}}}(e^{\frac{1}{\sqrt{2m}}}-1)+\cdots+x_{2m-1}e^{-\frac{2m}{\sqrt{2m}}}(e^{\frac{1}{\sqrt{2m}}}-1)+$$  $$+(x-x_2)e^{-\frac{2}{\sqrt{2m}}}+(x_3-x_4)e^{-\frac{4}{\sqrt{2m}}}+\cdots+(x_{2m-1}-x_{2m})e^{-\frac{2m}{\sqrt{2m}}}$$  $$=(e^{\frac{1}{\sqrt{2m}}}-1)(xe^{-\frac{2}{\sqrt{2m}}}+x_3e^{-\frac{4}{\sqrt{2m}}}+\cdots+x_{2m-1}e^{-\sqrt{2m}})+(\sqrt{x_2y_2}e^{-\frac{2}{\sqrt{2m}}}+\sqrt{x_4y_4}e^{-\frac{4}{\sqrt{2m}}}+\cdots+\sqrt{x_{2m}y_{2m}}e^{-\frac{2m}{\sqrt{2m}}})$$  $$=(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})}+\sum_{k=1}^{k=m}{(\sqrt{x_{2k}y_{2k}}e^{-\frac{2k}{\sqrt{2m}}})}$$ Also  $$\sum_{k=1}^{k=2m}{((-1)^{k+1}y_{k}e^{-\frac{k}{\sqrt{2m}}})}$$  $$=ye^{-\frac{1}{\sqrt{2m}}}-y_2e^{-\frac{2}{\sqrt{2m}}}+y_3e^{-\frac{3}{\sqrt{2m}}}-\cdots+(-1)^{2m+1}y_{2m}e^{-\frac{2m}{\sqrt{2m}}}$$  $$=(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})}+\sum_{k=1}^{k=m}{(\sqrt{x_{2k}y_{2k}}e^{-\frac{2k}{\sqrt{2m}}})}$$  But  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})}=S$$  And  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k+1}{\sqrt{2m}}})}<S<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k-1}{\sqrt{2m}}})}$$  $$(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{3}{\sqrt{2m}}}\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})}<S<(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{1}{\sqrt{2m}}}\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})}$$  Thus  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)y+(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=2}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=2}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{(S)}$$  And  $$(e^{\frac{1}{\sqrt2m}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p}{\sqrt{2m}}})}=A$$  Or  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p+1}{\sqrt{2m}}})}<A<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}$$  Also  $$(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{2}{\sqrt{2m}}}\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}<A<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}$$  Hence  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)y_{2p-1}e^{-\frac{p-1}{\sqrt{2m}}}+(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p+1}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p+1}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{(A)}=\lim_{m\longrightarrow{\infty}}{(S)}$$  For  $$p+1=m\Rightarrow{\lim_{m\longrightarrow{\infty}}{(A)}=\lim_{m\longrightarrow{\infty}}{(S)}=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)y_{2m-1}e^{-\frac{m}{\sqrt{2m}}})}=0}$$  Consequently  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})})}=0$$  Also   $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})}=S$$  Or  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k+1}{\sqrt{2m}}})}<S<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k-1}{\sqrt{2m}}})}$$  $$(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{3}{\sqrt{2m}}}\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})}<S<(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{1}{\sqrt{2m}}}\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})}$$  And  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)x+(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=2}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=2}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{(S)}$$  Or  $$(e^{\frac{1}{\sqrt2m}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p}{\sqrt{2m}}})}=A$$  And  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p+1}{\sqrt{2m}}})}<A<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}$$  Or  $$(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{2}{\sqrt{2m}}}\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}<A<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}$$  Hence  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)x_{2p-1}e^{-\frac{p-1}{\sqrt{2m}}}+(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p+1}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p+1}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{(A)}=\lim_{m\longrightarrow{\infty}}{(S)}$$  For  $$p+1=m\Rightarrow{\lim_{m\longrightarrow{\infty}}{(A)}=\lim_{m\longrightarrow{\infty}}{(S)}=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)x_{2m-1}e^{-\frac{m}{\sqrt{2m}}})}=0}$$  Consequently  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})})}=0$$ I deduce $$0<\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=2m}{((-1)^{k+1}x_{k}e^{-\frac{k}{\sqrt{2m}}})})}$$ $$=\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=2m}{((-1)^{k+1}y_{k}e^{-\frac{k}{\sqrt{2m}}})})}$$ $$=\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=m}{(\sqrt{x_{2k}y_{2k}}e^{-\frac{2k}{\sqrt{2m}}})})}<\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=m}{(\sqrt{x_{2k}y_{2k}})})}$$ $$<\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=2m}{(\sqrt{x_{k}y_{k}})})}=y$$ Thus $$\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=2m}{((-1)^{k+1}(x_k-y_k)e^{-\frac{k}{\sqrt{2m}}})})}=0$$ $$=\lim_{m\longrightarrow{\infty}}{((x-y)\sum_{k=1}^{k=2m}{(e^{-\frac{k}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{((x-y)e^{-\frac{1}{\sqrt{2m}}}\frac{1-e^{-\sqrt{2m}}}{1+e^{-\frac{1}{\sqrt{2m}}}})}=\frac{x-y}{2}=0$$ And $$x-y=0$$ I recapitulate $$x_i>x_{i+1},\quad{y_i>y_{i+1}}\Rightarrow{x=y}$$ $$x_i=x_{i+1}=x_{i+1}+\sqrt{x_{i+1}y_{i+1}}\Rightarrow{xy=0}$$ $$y_i=y_{i+1}=y_{i+1}+\sqrt{x_{i+1}y_{i+1}}\Rightarrow{xy=0}$$ $$\Rightarrow{xy(x-y)=0}$$ Is this calculus correct ? Thank you.","I have two sequences $x_i$ and $y_i$ defined by their expressions : $$x_i-x_{i+1}=y_i-y_{i+1}=\sqrt{x_{i+1}y_{i+1}}$$ I have to prove that $xy(x-y)=0$. I tried this : I have $$x_i=x^{2^{i-1}}\prod_{j=0}^{j=i-2} (x^{2^j}+y^{2^j})^{-1}$$ And $$y_i=y^{2^{i-1}}\prod_{j=0}^{j=i-2} (x^{2^j}+y^{2^j})^{-1}$$ How to prove that $x=y$ ? I tried this : I suppose $x\neq{y}$ and I define the series. I see that  $$\sqrt{x_iy_i}=y_{i-1}-y_i=x_{i-1}-x_i$$ So $$x_i-x_{i+1}=\sqrt{x_{i+1}y_{i+1}}$$ $$x_{i-1}-x_i=\sqrt{x_iy_i}$$ $$\vdots$$ $$x_1-x_2=x-x_2=\sqrt{x_2y_2}$$ Or $$\sum_{j=2}^{j=i+1}{(\sqrt{x_jy_j})}=x-x_2+x_2-x_3+\cdots+x_i-x_{i+1}=x-x_{i+1}$$ Its limit is $$\sum_{j=2}^{j=\infty}{(\sqrt{x_jy_j})}=\lim_{i\longrightarrow{\infty}}{(x-x_{i+1})}$$ If $x\geq{y}$ $$\sum_{j=2}^{j=\infty}{(\sqrt{x_jy_j})}=\lim_{i\longrightarrow{\infty}}{(x-x_{i+1})}=x-(x-y)=y$$ If $x\leq{y}$ $$\sum_{j=2}^{j=\infty}{(\sqrt{x_jy_j})}=\lim_{i\longrightarrow{\infty}}{(x-x_{i+1})}=x$$ I suppose $x\geq{y}$. Hence $$\sum_{j=2}^{j=i}{((-1)^j\sqrt{x_jy_j})}=x-x_2-(x_2-x_3)+(x_3-x_4)-\cdots+(-1)^i(x_{i-1}-x_i)$$   $$=x-2x_2+2x_3-\cdots+2(-1)^{i-1}x_{i-1}+(-1)^{i+1}x_i$$   $$=2\sum_{j=2}^{j=i-1}{((-1)^{j+1}x_j)}+x+(-1)^{i+1}x_i$$ $$=2\sum_{j=1}^{j=i}{((-1)^{j+1}x_j)}-x-(-1)^{i+1}x_i$$ $$=\sum_{j=2}^{j=i-1}{((-1)^{j+1}x_j)}+\sum_{j=1}^{j=i}{((-1)^{j+1}x_j)}$$ $$=2\sum_{j=2}^{j=i-1}{((-1)^{j+1}y_j)}+y+(-1)^{i+1}y_i$$ $$=2\sum_{j=1}^{j=i}{((-1)^{j+1}y_j)}-y-(-1)^{i+1}y_i$$ $$=\sum_{j=2}^{j=i-1}{((-1)^{j+1}y_j)}+\sum_{j=1}^{j=i}{((-1)^{j+1}y_j)}$$ Or    $$2\sum_{j=1}^{j=i}{((-1)^{j+1}x_j)}=\sum_{j=2}^{j=i}{((-1)^j\sqrt{x_jy_j})}+x+(-1)^{i+1}x_i$$ And $$2\sum_{j=1}^{j=i}{((-1)^{j+1}y_j)}=\sum_{j=2}^{j=i}{((-1)^j\sqrt{x_jy_j})}+y+(-1)^{i+1}y_i$$ I do not know the limit of $(-1)^{i+1}x_i$,    $$\sum_{j=1}^{j=\infty}{((-1)^{j}x_j)}$$ may diverge. But   $$\sum_{j=2}^{j=\infty}{((-1)^j\sqrt{x_jy_j})}$$ is absolutely convergent. As $y_i$ tends to zero in the infinity, then   $$\sum_{j=1}^{j=\infty}{((-1)^{j}y_j)}$$ converge. The limit of   $$\sum_{j=2}^{j=i}{((-1)^j\sqrt{x_jy_j})}=2\sum_{j=1}^{j=i}{((-1)^{j+1}y_j)}-y-(-1)^{i+1}y_i$$   $$=2\sum_{j=1}^{j=i}{((-1)^{j+1}x_j)}-x-(-1)^{i+1}x_i$$ exists and the series are convergent. I will try to prove that   $$\lim_{i\longrightarrow{\infty}}{(x_i)}=x-y=0$$ Let  $$\sum_{k=1}^{k=2m}{((-1)^{k+1}x_{k}e^{-\frac{k}{\sqrt{2m}}})}$$  $$=xe^{-\frac{1}{\sqrt{2m}}}-x_2e^{-\frac{2}{\sqrt{2m}}}+x_3e^{-\frac{3}{\sqrt{2m}}}-\cdots+(-1)^{2m+1}x_{2m}e^{-\frac{2m}{\sqrt{2m}}}$$  $$=xe^{-\frac{2}{\sqrt{2m}}}+x(e^{-\frac{1}{\sqrt{2m}}}-e^{-\frac{2}{\sqrt{2m}}})-x_2e^{-\frac{2}{\sqrt{2m}}}+x_3e^{-\frac{4}{\sqrt{2m}}}+x_3(e^{-\frac{3}{\sqrt{2m}}}-e^{-\frac{4}{\sqrt{2m}}})-x_4e^{-\frac{4}{\sqrt{2m}}}+\cdots-x_{2m}e^{-\frac{2m}{\sqrt{2m}}}$$  $$=xe^{-\frac{2}{\sqrt{2m}}}(e^{\frac{1}{\sqrt{2m}}}-1)+x_3e^{-\frac{4}{\sqrt{2m}}}(e^{\frac{1}{\sqrt{2m}}}-1)+\cdots+x_{2m-1}e^{-\frac{2m}{\sqrt{2m}}}(e^{\frac{1}{\sqrt{2m}}}-1)+$$  $$+(x-x_2)e^{-\frac{2}{\sqrt{2m}}}+(x_3-x_4)e^{-\frac{4}{\sqrt{2m}}}+\cdots+(x_{2m-1}-x_{2m})e^{-\frac{2m}{\sqrt{2m}}}$$  $$=(e^{\frac{1}{\sqrt{2m}}}-1)(xe^{-\frac{2}{\sqrt{2m}}}+x_3e^{-\frac{4}{\sqrt{2m}}}+\cdots+x_{2m-1}e^{-\sqrt{2m}})+(\sqrt{x_2y_2}e^{-\frac{2}{\sqrt{2m}}}+\sqrt{x_4y_4}e^{-\frac{4}{\sqrt{2m}}}+\cdots+\sqrt{x_{2m}y_{2m}}e^{-\frac{2m}{\sqrt{2m}}})$$  $$=(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})}+\sum_{k=1}^{k=m}{(\sqrt{x_{2k}y_{2k}}e^{-\frac{2k}{\sqrt{2m}}})}$$ Also  $$\sum_{k=1}^{k=2m}{((-1)^{k+1}y_{k}e^{-\frac{k}{\sqrt{2m}}})}$$  $$=ye^{-\frac{1}{\sqrt{2m}}}-y_2e^{-\frac{2}{\sqrt{2m}}}+y_3e^{-\frac{3}{\sqrt{2m}}}-\cdots+(-1)^{2m+1}y_{2m}e^{-\frac{2m}{\sqrt{2m}}}$$  $$=(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})}+\sum_{k=1}^{k=m}{(\sqrt{x_{2k}y_{2k}}e^{-\frac{2k}{\sqrt{2m}}})}$$  But  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})}=S$$  And  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k+1}{\sqrt{2m}}})}<S<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k-1}{\sqrt{2m}}})}$$  $$(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{3}{\sqrt{2m}}}\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})}<S<(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{1}{\sqrt{2m}}}\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})}$$  Thus  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)y+(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=2}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=2}^{k=m}{(y_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{(S)}$$  And  $$(e^{\frac{1}{\sqrt2m}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p}{\sqrt{2m}}})}=A$$  Or  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p+1}{\sqrt{2m}}})}<A<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}$$  Also  $$(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{2}{\sqrt{2m}}}\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}<A<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}$$  Hence  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)y_{2p-1}e^{-\frac{p-1}{\sqrt{2m}}}+(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p+1}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p+1}^{k=m}{(y_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{(A)}=\lim_{m\longrightarrow{\infty}}{(S)}$$  For  $$p+1=m\Rightarrow{\lim_{m\longrightarrow{\infty}}{(A)}=\lim_{m\longrightarrow{\infty}}{(S)}=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)y_{2m-1}e^{-\frac{m}{\sqrt{2m}}})}=0}$$  Consequently  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(y_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})})}=0$$  Also   $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})}=S$$  Or  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k+1}{\sqrt{2m}}})}<S<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k-1}{\sqrt{2m}}})}$$  $$(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{3}{\sqrt{2m}}}\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})}<S<(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{1}{\sqrt{2m}}}\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})}$$  And  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)x+(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=2}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=2}^{k=m}{(x_{2k-1}e^{-\frac{2k-2}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{(S)}$$  Or  $$(e^{\frac{1}{\sqrt2m}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p}{\sqrt{2m}}})}=A$$  And  $$(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p+1}{\sqrt{2m}}})}<A<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}$$  Or  $$(e^{\frac{1}{\sqrt{2m}}}-1)e^{-\frac{2}{\sqrt{2m}}}\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}<A<(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})}$$  Hence  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)x_{2p-1}e^{-\frac{p-1}{\sqrt{2m}}}+(e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p+1}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}$$  $$=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=p+1}^{k=m}{(x_{2k-1}e^{-\frac{2k-p-1}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{(A)}=\lim_{m\longrightarrow{\infty}}{(S)}$$  For  $$p+1=m\Rightarrow{\lim_{m\longrightarrow{\infty}}{(A)}=\lim_{m\longrightarrow{\infty}}{(S)}=\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)x_{2m-1}e^{-\frac{m}{\sqrt{2m}}})}=0}$$  Consequently  $$\lim_{m\longrightarrow{\infty}}{((e^{\frac{1}{\sqrt{2m}}}-1)\sum_{k=1}^{k=m}{(x_{2k-1}e^{-\frac{2k}{\sqrt{2m}}})})}=0$$ I deduce $$0<\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=2m}{((-1)^{k+1}x_{k}e^{-\frac{k}{\sqrt{2m}}})})}$$ $$=\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=2m}{((-1)^{k+1}y_{k}e^{-\frac{k}{\sqrt{2m}}})})}$$ $$=\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=m}{(\sqrt{x_{2k}y_{2k}}e^{-\frac{2k}{\sqrt{2m}}})})}<\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=m}{(\sqrt{x_{2k}y_{2k}})})}$$ $$<\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=2m}{(\sqrt{x_{k}y_{k}})})}=y$$ Thus $$\lim_{m\longrightarrow{\infty}}{(\sum_{k=1}^{k=2m}{((-1)^{k+1}(x_k-y_k)e^{-\frac{k}{\sqrt{2m}}})})}=0$$ $$=\lim_{m\longrightarrow{\infty}}{((x-y)\sum_{k=1}^{k=2m}{(e^{-\frac{k}{\sqrt{2m}}})})}=\lim_{m\longrightarrow{\infty}}{((x-y)e^{-\frac{1}{\sqrt{2m}}}\frac{1-e^{-\sqrt{2m}}}{1+e^{-\frac{1}{\sqrt{2m}}}})}=\frac{x-y}{2}=0$$ And $$x-y=0$$ I recapitulate $$x_i>x_{i+1},\quad{y_i>y_{i+1}}\Rightarrow{x=y}$$ $$x_i=x_{i+1}=x_{i+1}+\sqrt{x_{i+1}y_{i+1}}\Rightarrow{xy=0}$$ $$y_i=y_{i+1}=y_{i+1}+\sqrt{x_{i+1}y_{i+1}}\Rightarrow{xy=0}$$ $$\Rightarrow{xy(x-y)=0}$$ Is this calculus correct ? Thank you.",,['real-analysis']
95,The total variation and the integral of the derivative,The total variation and the integral of the derivative,,"I need a hint (not a complete solution) of the following problem: EDIT: when I was posting the question, I found I suddenly got it, so I need some verification. Suppose $F$ is a complex -valued function of bounded variation on $[a,b]$. Then the total variation $T_F(a,b)\ge\int_a^b\lvert F'(x)\rvert\,dx$. If $F$ is real -valued, then the problem is somewhat easier. Suppose the positive and the negative variation of $F$ on $[a,x]$ is $G(x),H(x)$, then $T_F(a,x)=G(x)+H(x)$ and $F(x)=G(x)-H(x)$, hence $$\int_a^b\lvert F'(x)\rvert\,dx\le\int_a^bG'(x)\,dx+\int_a^bH'(x)\,dx\le (G(b)-G(a))+(H(b)-H(a))=T_F(a,b)$$ We have used the following lemma: Suppose $F$ is increasing on $[a,b]$, then $\int_a^bF'(x)\,dx\le F(b)-F(a)$. It could be proved as follows: First, extend $F$ to $[a,+\infty)$, such that $f(x)=f(b),\;\forall x\ge b$. By Fatou's lemma, \begin{align} \int_a^bF'(x)\,dx &=\int_a^b\lim_{n\to\infty}\frac{f(x+1/n)-f(x)}n\,dx\\ &\le\liminf_{n\to\infty}n\left(\int_{a+1/n}^{b+1/n}f-\int_a^bf\right)\\ &=\lim_{n\to\infty}n\left(\int_b^{b+1/n}f-\int_a^{a+1/n}f\right)\\ &=F(b)-F(a^+) \end{align} I cannot apply the same technique to telescope the terms when $F$ is complex-valued. Note that when $F$ is absolutely continuous, then $T_F(a,b)\ge\int_a^b\lvert F'\rvert$ (in fact, $T_F(a,b)=\int_a^b$). The following argument should work: Denote $\lVert f\rVert=\lVert f\rVert_{L^1}=\int_a^b\lvert F'\rvert$. Since $F$ is AC, therefore of BC, hence $F'\in L^1$, fix $\epsilon>0$, we could choose step function $g$ s.t. $\lVert F'-g\rVert\le\epsilon$, and supose $g$ induces the partition $a=t_0<\dotsb<t_N=b$, we have $$\lVert F'\rVert\le\lVert F'-g\rVert+\lVert g\rVert\le\epsilon+\sum_{k=1}^N\left\lvert\int_{t_{k-1}}^{t_k}g\right\rvert\le2\epsilon+\sum_{k=1}^N\left\lvert\int_{t_{k-1}}^{t_k}F'\right\rvert\le2\epsilon+T_F(a,b)$$ It needs some explanation on the last step, i.e.: If $F$ is of BV, then $\lvert\int_a^bF'\rvert\le\lvert F(b)-F(a)\rvert$. By the real case, the real part and the imaginery part satisfy the preceding inequality, and note that $\lvert a\rvert\le\lvert a'\rvert, \lvert b\rvert\le\lvert a'\rvert\implies \lvert a+bi\rvert\le\lvert a'+b'i\rvert$.","I need a hint (not a complete solution) of the following problem: EDIT: when I was posting the question, I found I suddenly got it, so I need some verification. Suppose $F$ is a complex -valued function of bounded variation on $[a,b]$. Then the total variation $T_F(a,b)\ge\int_a^b\lvert F'(x)\rvert\,dx$. If $F$ is real -valued, then the problem is somewhat easier. Suppose the positive and the negative variation of $F$ on $[a,x]$ is $G(x),H(x)$, then $T_F(a,x)=G(x)+H(x)$ and $F(x)=G(x)-H(x)$, hence $$\int_a^b\lvert F'(x)\rvert\,dx\le\int_a^bG'(x)\,dx+\int_a^bH'(x)\,dx\le (G(b)-G(a))+(H(b)-H(a))=T_F(a,b)$$ We have used the following lemma: Suppose $F$ is increasing on $[a,b]$, then $\int_a^bF'(x)\,dx\le F(b)-F(a)$. It could be proved as follows: First, extend $F$ to $[a,+\infty)$, such that $f(x)=f(b),\;\forall x\ge b$. By Fatou's lemma, \begin{align} \int_a^bF'(x)\,dx &=\int_a^b\lim_{n\to\infty}\frac{f(x+1/n)-f(x)}n\,dx\\ &\le\liminf_{n\to\infty}n\left(\int_{a+1/n}^{b+1/n}f-\int_a^bf\right)\\ &=\lim_{n\to\infty}n\left(\int_b^{b+1/n}f-\int_a^{a+1/n}f\right)\\ &=F(b)-F(a^+) \end{align} I cannot apply the same technique to telescope the terms when $F$ is complex-valued. Note that when $F$ is absolutely continuous, then $T_F(a,b)\ge\int_a^b\lvert F'\rvert$ (in fact, $T_F(a,b)=\int_a^b$). The following argument should work: Denote $\lVert f\rVert=\lVert f\rVert_{L^1}=\int_a^b\lvert F'\rvert$. Since $F$ is AC, therefore of BC, hence $F'\in L^1$, fix $\epsilon>0$, we could choose step function $g$ s.t. $\lVert F'-g\rVert\le\epsilon$, and supose $g$ induces the partition $a=t_0<\dotsb<t_N=b$, we have $$\lVert F'\rVert\le\lVert F'-g\rVert+\lVert g\rVert\le\epsilon+\sum_{k=1}^N\left\lvert\int_{t_{k-1}}^{t_k}g\right\rvert\le2\epsilon+\sum_{k=1}^N\left\lvert\int_{t_{k-1}}^{t_k}F'\right\rvert\le2\epsilon+T_F(a,b)$$ It needs some explanation on the last step, i.e.: If $F$ is of BV, then $\lvert\int_a^bF'\rvert\le\lvert F(b)-F(a)\rvert$. By the real case, the real part and the imaginery part satisfy the preceding inequality, and note that $\lvert a\rvert\le\lvert a'\rvert, \lvert b\rvert\le\lvert a'\rvert\implies \lvert a+bi\rvert\le\lvert a'+b'i\rvert$.",,['real-analysis']
96,Could $4+2+4+2+4+2+\cdots = -1 $?,Could ?,4+2+4+2+4+2+\cdots = -1 ,"In physics classes, on this StackExchange and even in blogs the sum $1 + 2 + 3 + 4 + \cdots = - \frac{1}{12} $ has been under the microscope. Why does $1+2+3+\cdots = -\frac{1}{12}$? The Euler-Maclaurin formula, Bernoulli numbers, the zeta function, and real-variable analytic continuation As a consequence of the trapezoid rule, the error to the Riemann sum is bounded by the second derivative. $$ \int_0^N f(x) \; dx = \frac{1}{2}f(0) + f(1) + \dots + f(N-1) + \frac{1}{2}f(N) + O\,(N \| f \|_{\dot{C}^2} ) $$ Letting $f(x) = (1-x/N)_+$ we get $\sum_{i=0}^{N-1} 1 = -\frac{1}{2} + \int_0^N 1 \, dx + O(1)$. I am wondering what happen if we use the Simpson rule : $$ \int_0^N f(x) \; dx = \frac{1}{3} \big( f(0) + 4f(1) + 2f(2) + \dots + 4f(N-1) + f(N)  \big) + O\,(N || f ||_{\dot{C}^4} ) $$ Now we plug in $f(x) = (1-x)_+$ and get $$\frac{1}{3} (4 + 2 + 4 + 2 + \cdots )  =  -\frac{1}{3} + \int_0^N f(x) \; dx +  O(N^{-3}) $$ Is that still consistent with the other types of sums you get from Euler-Macularin type summation methods?","In physics classes, on this StackExchange and even in blogs the sum $1 + 2 + 3 + 4 + \cdots = - \frac{1}{12} $ has been under the microscope. Why does $1+2+3+\cdots = -\frac{1}{12}$? The Euler-Maclaurin formula, Bernoulli numbers, the zeta function, and real-variable analytic continuation As a consequence of the trapezoid rule, the error to the Riemann sum is bounded by the second derivative. $$ \int_0^N f(x) \; dx = \frac{1}{2}f(0) + f(1) + \dots + f(N-1) + \frac{1}{2}f(N) + O\,(N \| f \|_{\dot{C}^2} ) $$ Letting $f(x) = (1-x/N)_+$ we get $\sum_{i=0}^{N-1} 1 = -\frac{1}{2} + \int_0^N 1 \, dx + O(1)$. I am wondering what happen if we use the Simpson rule : $$ \int_0^N f(x) \; dx = \frac{1}{3} \big( f(0) + 4f(1) + 2f(2) + \dots + 4f(N-1) + f(N)  \big) + O\,(N || f ||_{\dot{C}^4} ) $$ Now we plug in $f(x) = (1-x)_+$ and get $$\frac{1}{3} (4 + 2 + 4 + 2 + \cdots )  =  -\frac{1}{3} + \int_0^N f(x) \; dx +  O(N^{-3}) $$ Is that still consistent with the other types of sums you get from Euler-Macularin type summation methods?",,"['calculus', 'real-analysis', 'sequences-and-series', 'interpolation', 'divergent-series']"
97,Existence theorem for antiderivatives by Weierstrass approximation theorem,Existence theorem for antiderivatives by Weierstrass approximation theorem,,Is there a way of proving the existence of antiderivatives (of continuous functions on a compact subset of the real line) without using tools of integration? This is an exercise in: http://www.math.nus.edu.sg/~matngtb/Calculus/MA3110/Chapter%2010%20Weierstrass%20Approximation.pdf last page. Apparently one should somehow be able to use the Weierstrass theorem. A related question is now of course: can we prove the fundamental theorem of calculus using Weierstrass' theorem?,Is there a way of proving the existence of antiderivatives (of continuous functions on a compact subset of the real line) without using tools of integration? This is an exercise in: http://www.math.nus.edu.sg/~matngtb/Calculus/MA3110/Chapter%2010%20Weierstrass%20Approximation.pdf last page. Apparently one should somehow be able to use the Weierstrass theorem. A related question is now of course: can we prove the fundamental theorem of calculus using Weierstrass' theorem?,,['calculus']
98,The Heine-Borel Theorem for the real line,The Heine-Borel Theorem for the real line,,"Hi everyone I'd like to know if the following argument is correct and also I'm very interested in a constructive approach for (2)$\Rightarrow$(1) (a link or a hint it will sufficient for me) I was thinking for a while but I cannot figure out some way to do it. Thanks in advance. Definitions : A subset $X$ of the real line is bounded if we have $X\subset [-M, M]$ for some real number $M>0$. Let $X\subset \mathbb{R}$ and let $x'\in \mathbb{R}$, $x'$ is an adherent point of $X$ iff $\,\forall\varepsilon>0\, ,\exists x\in X \text{ s.t.} \; d(x',x)\le \varepsilon$. We say that $\overline{X}$ is the closure of $X$ if contain all the adherent points of $X$. Theorem (The Heine-Borel Theorem for the line): Let $X$ be a subset of $\mathbb{R}$. Then the following two statements are equivalent: (1) $X$ is closed and bounded (2) Given any sequence $(a_n)$ of real numbers which takes values from $X$ (i.e., $a_n\in X$ for all natural numbers), there exists a subsequence ($a_{n_j}$) which converges to some number $L$ in $X$. Proof: (1)$\Rightarrow$(2) Let $(a_n)_{n=0}^\infty$ be a sequence where $a_n\in X$ for all the natural numbers. Since $X$ is bounded, $|a_n|\le M$  for all $n$, where $M$ is an upper bound for $X$. Then by the Bolzano–Weierstrass theorem there exist a convergent subsequence  $(a_{n_j})$. Let $L=\lim_j a_{n_j},$ then it follows that $L$ is an adherent point of $X$ and since is closed by hypothesis, $L$ is in $X$ as desired. (2)$\Rightarrow$(1) Suppose for the sake of contradiction that either $X$ is not closed or is unbounded. If $X$ is unbounded, let define $X_n=\{y\in X: |y|>n\}$ (each one is non-empty). Then we can find using the AC a sequence $(x_n)$ such that $x_n \in X_n$ for each positive integer. By hypothesis we know that there is a subsequence  $(x_{n_j})$ which converges to some $L$ in $X$. But $|a_n|>L+1$ for all $n\ge L+1$ so, for $j\ge L+1$ we have $|a_{n_j}|>L+1$, a contradiction. Now if $X$ is not closed then it has at least one adherent point $x$ which is not in the set. Since $x$ is adherent there is a sequence $(a_n)$ of terms in $X$ which converges to $x$, i.e., $a_n\rightarrow x$ and $a_n\in X$ for all $n$. Since any subsequence of a convergent sequence converges to the same value, so $x$ must be in $X$, a contradiction. It follows that $X$ must be closed and bounded. $\Box$","Hi everyone I'd like to know if the following argument is correct and also I'm very interested in a constructive approach for (2)$\Rightarrow$(1) (a link or a hint it will sufficient for me) I was thinking for a while but I cannot figure out some way to do it. Thanks in advance. Definitions : A subset $X$ of the real line is bounded if we have $X\subset [-M, M]$ for some real number $M>0$. Let $X\subset \mathbb{R}$ and let $x'\in \mathbb{R}$, $x'$ is an adherent point of $X$ iff $\,\forall\varepsilon>0\, ,\exists x\in X \text{ s.t.} \; d(x',x)\le \varepsilon$. We say that $\overline{X}$ is the closure of $X$ if contain all the adherent points of $X$. Theorem (The Heine-Borel Theorem for the line): Let $X$ be a subset of $\mathbb{R}$. Then the following two statements are equivalent: (1) $X$ is closed and bounded (2) Given any sequence $(a_n)$ of real numbers which takes values from $X$ (i.e., $a_n\in X$ for all natural numbers), there exists a subsequence ($a_{n_j}$) which converges to some number $L$ in $X$. Proof: (1)$\Rightarrow$(2) Let $(a_n)_{n=0}^\infty$ be a sequence where $a_n\in X$ for all the natural numbers. Since $X$ is bounded, $|a_n|\le M$  for all $n$, where $M$ is an upper bound for $X$. Then by the Bolzano–Weierstrass theorem there exist a convergent subsequence  $(a_{n_j})$. Let $L=\lim_j a_{n_j},$ then it follows that $L$ is an adherent point of $X$ and since is closed by hypothesis, $L$ is in $X$ as desired. (2)$\Rightarrow$(1) Suppose for the sake of contradiction that either $X$ is not closed or is unbounded. If $X$ is unbounded, let define $X_n=\{y\in X: |y|>n\}$ (each one is non-empty). Then we can find using the AC a sequence $(x_n)$ such that $x_n \in X_n$ for each positive integer. By hypothesis we know that there is a subsequence  $(x_{n_j})$ which converges to some $L$ in $X$. But $|a_n|>L+1$ for all $n\ge L+1$ so, for $j\ge L+1$ we have $|a_{n_j}|>L+1$, a contradiction. Now if $X$ is not closed then it has at least one adherent point $x$ which is not in the set. Since $x$ is adherent there is a sequence $(a_n)$ of terms in $X$ which converges to $x$, i.e., $a_n\rightarrow x$ and $a_n\in X$ for all $n$. Since any subsequence of a convergent sequence converges to the same value, so $x$ must be in $X$, a contradiction. It follows that $X$ must be closed and bounded. $\Box$",,"['real-analysis', 'proof-verification', 'self-learning', 'alternative-proof']"
99,Fubini's Theorem for Infinite series,Fubini's Theorem for Infinite series,,"In the book what I've read, there is one point where the author suggest to begin the proof of the Fubini's Theorem for infinite sum in the case when is non-negative after this try to generalize. But I really have a bad time trying to understand the entire problem for various reason: number one I'm not absolutely sure if what I've done is correct and second I have no idea of how to pass in the general case. Any suggestion, advice whatever to deal with the general case it would be great. Thanks. Fubini's Theorem for Infinite series : Let $f: \mathbb{N} \times\mathbb{N} \rightarrow \mathbb{R} $ be a function such that $\sum _{(n,m)\in  \mathbb{N} \times\mathbb{N}} f(n,m)  $ is absolutely convergent. Then we have: $\sum _{n=0}^{\infty}\sum _{m=0}^{\infty} f(n,m) =\sum _{(n,m)\in  \mathbb{N} \times\mathbb{N}} f(n,m)=\sum _{(m,n)\in  \mathbb{N} \times\mathbb{N}} f(n,m)= \sum _{m=0}^{\infty}\sum _{n=0}^{\infty} f(n,m)$ Proof: We may assume that for each $n,m$, that the function is non-negative, i.e., $\forall n,m \in\mathbb{N}.f(n,m)\ge0 $. We set $L:=  \sum _{(n,m)\in  \mathbb{N} \times\mathbb{N}} f(n,m)  $, and we want to show that $\sum _{n=0}^{\infty}\sum _{m=0}^{\infty} f(n,m)$ converges to $L$. Let $X\subset \mathbb{N^2}$ and suppose that $X$ is finite. We claim that $\sum _{(n,m)\in  X} f(n,m) \le L$. Let $g: \mathbb{N^2}\rightarrow \mathbb{N}$ be a bijective map. If we restrict the map to $X$, clearly $g[X]$ is finite, hence bounded. Since $L$ is the sup of the  sequences of partial sums $\sum _{(n,m)\in  \{0...N\} \times\{0...M\}} f(n,m)$ which is monotonic increasing. Thus is at most $L$. First we have to show that $\sum _{n=0}^{\infty}\sum _{m=0}^{\infty} f(n,m)$ converges. It will suffice to to show that $\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)\le L$ for any $N$. Since $\sum _{m=0}^{M} f(n,m) \rightarrow \sum _{m=0}^{\infty} f(n,m)$, $\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)$ is the limit of $\sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m)$ as $M \rightarrow \infty$. Then, by the monotonic convergence theorem it will suffice to show that $\sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m)\le L$ for any $M$. But, $\sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m) = \sum _{(n,m)\in  \{0...N\} \times\{0...M\}} f(n,m)$ by the Fubini's Theorem for finite series and since $\{0...N\} \times\{0...M\} \subset \mathbb{N^2}$ is a finite subset. the result follows for what we said above. Now let $\varepsilon >0$ be given. Then, $L -\varepsilon$  cannot be the sup of the sequence of partial sum and we can then find a finite set $ X \subset \mathbb{N^2}$ such that $\sum _{(n,m)\in  X} f(n,m) > L-\varepsilon$. Since $X$ is assumed to be finite then there is  contained in some set $ \{0...N\} \times\{0...M\}$. Thus for any $N'\ge N,M\ge M'$ we have $\sum _{n=0}^{N'}\sum _{m=0}^{M'} f(n,m) = \sum _{(n,m)\in  \{0...N'\} \times\{0...M'\}} f(n,m)\ge L-\varepsilon$, and hence since we already know that the sum exists $\sum _{n=0}^{\infty}\sum _{m=0}^{\infty} f(n,m)\ge\sum _{n=0}^{N'}\sum _{m=0}^{\infty} f(n,m)\ge L-\varepsilon$. Note: I already proved the Fubini's Theorem for finite series using induction. Also I think we can argue as follows. Let $g: \mathbb{N^2}\rightarrow \mathbb{N}$ be any bijective map. Since $ \sum _{(n,m)\in  \mathbb{N} \times\mathbb{N}} f(n,m)  $  converges absolutely then any rearrangement give us the same result. Thus $\sum _{i=0}^{\infty} f(g^{-1}(i)) =  L$. Also the function is non-negative, so the sequence of partial sums converges to its supremum, i.e., $L=\text{sup}_{N\ge 0}\sum _{i=0}^{N} f(g^{-1}(i))$. We claim that if  $X$ is a finite subset $\mathbb{N^2}$. Then $\sum _{(n,m)\in  X} f(n,m) \le L$. The sequence $\big(g(n,m)\big)_{(n,m)\in X} $ is finite and hence bounded. Let $N$ be an upper bound, i.e., $g(n,m)\le N$ for any $(n,m)\in X$. We set $\{ i\in \mathbb{N}: i\le N \}$. Then $g [X] \subset \{ i\in \mathbb{N}: i\le N \}$. We have: $\sum _{(n,m)\in  X} f(n,m)=\sum _{n\in  g[X]} f(g^{-1}(n)) \le \sum _{n\in \{ i\in \mathbb{N}: i\le N \}} f(g^{-1}(n))=\sum _{n=0}^{N} f(g^{-1}(n))\le L$. Thus $\sum _{(n,m)\in  X} f(n,m) \le L$ as desired. Let $n, M\in\mathbb{N} $ be arbitrary. Then $\sum _{(i,j)\in \{n\} \times \{0....M\}} f(i,j)=\sum _{m=0}^{M} f(n,m) \le L$ for each $M\ge 0$. Thus $\sum _{m=0}^{M} f(n,m)$ converges and is at most $L$, i.e.,  $\sum _{m=0}^{\infty} f(n,m)\le L$ for a fixed $n$. We claim that $\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)\le L$ for any $N\in \mathbb{N}$. We argue by contradiction suppose that there is some $N\ge 0$ such that  $\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)> L$. For the sake of simplicity let $S_{N,\infty}:=\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)$, and $S_{N,M}:=\sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m)$. We may assume that $S_{N,\infty}> L$. Let us fix some $\varepsilon>0$ such that $\varepsilon< S_{N,\infty}-L$. Since $S_{N,M}\rightarrow S_{N,\infty}$. Let some sufficient large  $M$ such that $S_{N,\infty}-S_{N,M}\le \varepsilon<S_{N,\infty}-L$. Then $L<S_{N,M} = \sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m) =\sum _{(n,m)\in  \{0...N\} \times\{0...M\}} f(n,m)\le L$ [since $\{0...N\} \times\{0...M\}$ is a finite subset of $\mathbb{N^2}$] a contradiction. This means that $S_{N,\infty}\le L$ for each $N\ge 0$ and hence $S_{\infty,\infty}\le L$. Let $X\subset \mathbb{N^2}$ which is finite. Then $X\subset \{0...N\}\times \{0...N\}$. So $\sum _{(n,m)\in  X} f(n,m)\le \sum _{(n,m)\in  \{0...N\} \times\{0...N\}} f(n,m)=\sum _{n=0}^{N}\sum _{m=0}^{N} f(n,m)\le S_{\infty,\infty}$. Since this hold for any finite set of $\mathbb{N^2}$, then $L\le  S_{\infty,\infty}$. Hence $L = S_{\infty,\infty}$ Is this correct? Any suggestion for the general case (the sign case), please.  What about if we show that $f(n,m)$ can be express as a part which is just positive and other which is negative, with this almost done, right? What do you think about it? Thanks.","In the book what I've read, there is one point where the author suggest to begin the proof of the Fubini's Theorem for infinite sum in the case when is non-negative after this try to generalize. But I really have a bad time trying to understand the entire problem for various reason: number one I'm not absolutely sure if what I've done is correct and second I have no idea of how to pass in the general case. Any suggestion, advice whatever to deal with the general case it would be great. Thanks. Fubini's Theorem for Infinite series : Let $f: \mathbb{N} \times\mathbb{N} \rightarrow \mathbb{R} $ be a function such that $\sum _{(n,m)\in  \mathbb{N} \times\mathbb{N}} f(n,m)  $ is absolutely convergent. Then we have: $\sum _{n=0}^{\infty}\sum _{m=0}^{\infty} f(n,m) =\sum _{(n,m)\in  \mathbb{N} \times\mathbb{N}} f(n,m)=\sum _{(m,n)\in  \mathbb{N} \times\mathbb{N}} f(n,m)= \sum _{m=0}^{\infty}\sum _{n=0}^{\infty} f(n,m)$ Proof: We may assume that for each $n,m$, that the function is non-negative, i.e., $\forall n,m \in\mathbb{N}.f(n,m)\ge0 $. We set $L:=  \sum _{(n,m)\in  \mathbb{N} \times\mathbb{N}} f(n,m)  $, and we want to show that $\sum _{n=0}^{\infty}\sum _{m=0}^{\infty} f(n,m)$ converges to $L$. Let $X\subset \mathbb{N^2}$ and suppose that $X$ is finite. We claim that $\sum _{(n,m)\in  X} f(n,m) \le L$. Let $g: \mathbb{N^2}\rightarrow \mathbb{N}$ be a bijective map. If we restrict the map to $X$, clearly $g[X]$ is finite, hence bounded. Since $L$ is the sup of the  sequences of partial sums $\sum _{(n,m)\in  \{0...N\} \times\{0...M\}} f(n,m)$ which is monotonic increasing. Thus is at most $L$. First we have to show that $\sum _{n=0}^{\infty}\sum _{m=0}^{\infty} f(n,m)$ converges. It will suffice to to show that $\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)\le L$ for any $N$. Since $\sum _{m=0}^{M} f(n,m) \rightarrow \sum _{m=0}^{\infty} f(n,m)$, $\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)$ is the limit of $\sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m)$ as $M \rightarrow \infty$. Then, by the monotonic convergence theorem it will suffice to show that $\sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m)\le L$ for any $M$. But, $\sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m) = \sum _{(n,m)\in  \{0...N\} \times\{0...M\}} f(n,m)$ by the Fubini's Theorem for finite series and since $\{0...N\} \times\{0...M\} \subset \mathbb{N^2}$ is a finite subset. the result follows for what we said above. Now let $\varepsilon >0$ be given. Then, $L -\varepsilon$  cannot be the sup of the sequence of partial sum and we can then find a finite set $ X \subset \mathbb{N^2}$ such that $\sum _{(n,m)\in  X} f(n,m) > L-\varepsilon$. Since $X$ is assumed to be finite then there is  contained in some set $ \{0...N\} \times\{0...M\}$. Thus for any $N'\ge N,M\ge M'$ we have $\sum _{n=0}^{N'}\sum _{m=0}^{M'} f(n,m) = \sum _{(n,m)\in  \{0...N'\} \times\{0...M'\}} f(n,m)\ge L-\varepsilon$, and hence since we already know that the sum exists $\sum _{n=0}^{\infty}\sum _{m=0}^{\infty} f(n,m)\ge\sum _{n=0}^{N'}\sum _{m=0}^{\infty} f(n,m)\ge L-\varepsilon$. Note: I already proved the Fubini's Theorem for finite series using induction. Also I think we can argue as follows. Let $g: \mathbb{N^2}\rightarrow \mathbb{N}$ be any bijective map. Since $ \sum _{(n,m)\in  \mathbb{N} \times\mathbb{N}} f(n,m)  $  converges absolutely then any rearrangement give us the same result. Thus $\sum _{i=0}^{\infty} f(g^{-1}(i)) =  L$. Also the function is non-negative, so the sequence of partial sums converges to its supremum, i.e., $L=\text{sup}_{N\ge 0}\sum _{i=0}^{N} f(g^{-1}(i))$. We claim that if  $X$ is a finite subset $\mathbb{N^2}$. Then $\sum _{(n,m)\in  X} f(n,m) \le L$. The sequence $\big(g(n,m)\big)_{(n,m)\in X} $ is finite and hence bounded. Let $N$ be an upper bound, i.e., $g(n,m)\le N$ for any $(n,m)\in X$. We set $\{ i\in \mathbb{N}: i\le N \}$. Then $g [X] \subset \{ i\in \mathbb{N}: i\le N \}$. We have: $\sum _{(n,m)\in  X} f(n,m)=\sum _{n\in  g[X]} f(g^{-1}(n)) \le \sum _{n\in \{ i\in \mathbb{N}: i\le N \}} f(g^{-1}(n))=\sum _{n=0}^{N} f(g^{-1}(n))\le L$. Thus $\sum _{(n,m)\in  X} f(n,m) \le L$ as desired. Let $n, M\in\mathbb{N} $ be arbitrary. Then $\sum _{(i,j)\in \{n\} \times \{0....M\}} f(i,j)=\sum _{m=0}^{M} f(n,m) \le L$ for each $M\ge 0$. Thus $\sum _{m=0}^{M} f(n,m)$ converges and is at most $L$, i.e.,  $\sum _{m=0}^{\infty} f(n,m)\le L$ for a fixed $n$. We claim that $\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)\le L$ for any $N\in \mathbb{N}$. We argue by contradiction suppose that there is some $N\ge 0$ such that  $\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)> L$. For the sake of simplicity let $S_{N,\infty}:=\sum _{n=0}^{N}\sum _{m=0}^{\infty} f(n,m)$, and $S_{N,M}:=\sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m)$. We may assume that $S_{N,\infty}> L$. Let us fix some $\varepsilon>0$ such that $\varepsilon< S_{N,\infty}-L$. Since $S_{N,M}\rightarrow S_{N,\infty}$. Let some sufficient large  $M$ such that $S_{N,\infty}-S_{N,M}\le \varepsilon<S_{N,\infty}-L$. Then $L<S_{N,M} = \sum _{n=0}^{N}\sum _{m=0}^{M} f(n,m) =\sum _{(n,m)\in  \{0...N\} \times\{0...M\}} f(n,m)\le L$ [since $\{0...N\} \times\{0...M\}$ is a finite subset of $\mathbb{N^2}$] a contradiction. This means that $S_{N,\infty}\le L$ for each $N\ge 0$ and hence $S_{\infty,\infty}\le L$. Let $X\subset \mathbb{N^2}$ which is finite. Then $X\subset \{0...N\}\times \{0...N\}$. So $\sum _{(n,m)\in  X} f(n,m)\le \sum _{(n,m)\in  \{0...N\} \times\{0...N\}} f(n,m)=\sum _{n=0}^{N}\sum _{m=0}^{N} f(n,m)\le S_{\infty,\infty}$. Since this hold for any finite set of $\mathbb{N^2}$, then $L\le  S_{\infty,\infty}$. Hence $L = S_{\infty,\infty}$ Is this correct? Any suggestion for the general case (the sign case), please.  What about if we show that $f(n,m)$ can be express as a part which is just positive and other which is negative, with this almost done, right? What do you think about it? Thanks.",,"['real-analysis', 'sequences-and-series', 'self-learning', 'proof-verification']"
