,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Statement equivalent to Limit of sequence,Statement equivalent to Limit of sequence,,"Def: A sequence $X = (x_n)$ in $\mathbb{R}$ is said to converge to $x ∈ \mathbb{R}$, or $x$ is said to be a limit of $(x_n)$, if for every $\epsilon> 0$, there exists a natural number $K ∈ \mathbb{N}$ such that for all $n ≥ K$, the terms $x_n$ satisfy $\lvert x_n − x\rvert < \epsilon$. Check to see if each of the following statements is equivalent to the definition of limit. If it is, prove it. If it is not, give an example to justify your conclusion. 1) For every $\epsilon > 0$, there exists a natural number $K ∈ \mathbb{N}$ such that for all $n > K$ , the terms $x_n$ satisfy $\lvert x_n − x\rvert < \epsilon$. 2) For every $\epsilon \geq 0$, there exists a natural number $K ∈ \mathbb{N}$ such that for all $n ≥ K$, the terms $x_n$ satisfy $\lvert x_n − x\rvert < \epsilon$.","Def: A sequence $X = (x_n)$ in $\mathbb{R}$ is said to converge to $x ∈ \mathbb{R}$, or $x$ is said to be a limit of $(x_n)$, if for every $\epsilon> 0$, there exists a natural number $K ∈ \mathbb{N}$ such that for all $n ≥ K$, the terms $x_n$ satisfy $\lvert x_n − x\rvert < \epsilon$. Check to see if each of the following statements is equivalent to the definition of limit. If it is, prove it. If it is not, give an example to justify your conclusion. 1) For every $\epsilon > 0$, there exists a natural number $K ∈ \mathbb{N}$ such that for all $n > K$ , the terms $x_n$ satisfy $\lvert x_n − x\rvert < \epsilon$. 2) For every $\epsilon \geq 0$, there exists a natural number $K ∈ \mathbb{N}$ such that for all $n ≥ K$, the terms $x_n$ satisfy $\lvert x_n − x\rvert < \epsilon$.",,['limits']
1,Limit of Lambert $W$ Product Log is the Natural Log?,Limit of Lambert  Product Log is the Natural Log?,W,In solving this equation $\large  y=x^ne^x$ I get the result that  $$n \cdot W\left( \frac{y^{1/n}}{n}\right)=x $$ So now it is apparent to me that when $n=0$ you would simply get $\ln(y)=x$ by normal methods. But is there any way to show that the limit this is true as well? $$\lim_{n \rightarrow 0}  n \cdot W\left( \frac{y^{1/n}}{n}\right)= \ln y$$,In solving this equation $\large  y=x^ne^x$ I get the result that  $$n \cdot W\left( \frac{y^{1/n}}{n}\right)=x $$ So now it is apparent to me that when $n=0$ you would simply get $\ln(y)=x$ by normal methods. But is there any way to show that the limit this is true as well? $$\lim_{n \rightarrow 0}  n \cdot W\left( \frac{y^{1/n}}{n}\right)= \ln y$$,,"['limits', 'lambert-w']"
2,Replacing order of infinite sum and limit with some weaker form of dominated convergence,Replacing order of infinite sum and limit with some weaker form of dominated convergence,,"Let $f(x,n)$ be some positive function of $\mathbb R\times\mathbb N$ Using dominated convergence theorem, with $\mu(A)=|A|$ as the measure, we can prove that if $f(x,n)<g(n)$ for any $x$, and $g(n)$ is integrable in $\mu$ (i.e. absolutly convergent), then $\sum\limits_i\!\lim\limits_{x\rightarrow 0}\!f(x,i)=\lim\limits_{x\rightarrow 0}\!\sum\limits_i\!f(x,i)$. Assume that $\sum\limits_i f(x,i)<c$ for any $x>0$. Say that for any $\delta>0$ there exists some $m$ (we can also assume that for any $m$ there exists some $\delta>0$) and integrable  $g(n)$ such that for any $|x|<\delta$ and any $n<m$ we get $f(x,n)<g(n)$ (note that $g$ is not a function of $m$), and $\sum\limits_i\!g_m(i)<2c$. Is it still true that $\sum\limits_i\!\lim\limits_{x\rightarrow 0}\!f(x,i)=\lim\limits_{x\rightarrow 0}\!\sum\limits_i\!f(x,i)$? Edit: We can assume the following: $f$ is bounded for any $x,i$ $f$ is uniformly continuous for any fixed $i$ $f$ is a function from $(0,1)\mathbb\times{N}\rightarrow \mathbb{R}$ For any $i,x$, $f(x,i)\leq\frac{1}{i}$","Let $f(x,n)$ be some positive function of $\mathbb R\times\mathbb N$ Using dominated convergence theorem, with $\mu(A)=|A|$ as the measure, we can prove that if $f(x,n)<g(n)$ for any $x$, and $g(n)$ is integrable in $\mu$ (i.e. absolutly convergent), then $\sum\limits_i\!\lim\limits_{x\rightarrow 0}\!f(x,i)=\lim\limits_{x\rightarrow 0}\!\sum\limits_i\!f(x,i)$. Assume that $\sum\limits_i f(x,i)<c$ for any $x>0$. Say that for any $\delta>0$ there exists some $m$ (we can also assume that for any $m$ there exists some $\delta>0$) and integrable  $g(n)$ such that for any $|x|<\delta$ and any $n<m$ we get $f(x,n)<g(n)$ (note that $g$ is not a function of $m$), and $\sum\limits_i\!g_m(i)<2c$. Is it still true that $\sum\limits_i\!\lim\limits_{x\rightarrow 0}\!f(x,i)=\lim\limits_{x\rightarrow 0}\!\sum\limits_i\!f(x,i)$? Edit: We can assume the following: $f$ is bounded for any $x,i$ $f$ is uniformly continuous for any fixed $i$ $f$ is a function from $(0,1)\mathbb\times{N}\rightarrow \mathbb{R}$ For any $i,x$, $f(x,i)\leq\frac{1}{i}$",,"['sequences-and-series', 'measure-theory', 'limits']"
3,write this limit based on delta function,write this limit based on delta function,,"If it’s possible, I want to write this limit based on delta function $$ \lim_{t\rightarrow 0}\frac{e^{-u/t}}{t^2},\qquad u>0 $$ would you mind helping me?","If it’s possible, I want to write this limit based on delta function $$ \lim_{t\rightarrow 0}\frac{e^{-u/t}}{t^2},\qquad u>0 $$ would you mind helping me?",,['limits']
4,Is it true that $\lim_{x\to a}f(x)=0$ if and only if $\lim_{x\to a}|f(x)|=0$?,Is it true that  if and only if ?,\lim_{x\to a}f(x)=0 \lim_{x\to a}|f(x)|=0,"Is it true that $\lim_{x\to a}f(x)=0$ if and only if $\lim_{x\to a}|f(x)|=0$? I intuitively think this is true, but really no idea to prove it. Can you give me hints?","Is it true that $\lim_{x\to a}f(x)=0$ if and only if $\lim_{x\to a}|f(x)|=0$? I intuitively think this is true, but really no idea to prove it. Can you give me hints?",,"['calculus', 'real-analysis', 'limits']"
5,Limit of a Rational Trigonometric Function $\lim_{x \to 0} \frac{\sin5x}{\sin4x}$,Limit of a Rational Trigonometric Function,\lim_{x \to 0} \frac{\sin5x}{\sin4x},"When solving a trigonometric limit such as: $$\lim_{x \to 0} \frac{\sin(5x)}{\sin(4x)}$$ we rework the equation to an equivalent for to fit the limit of sine ""rule"": $$\lim_{x \to 0}\frac{\sin(x)}{x}=1$$ so, we move forward in such a manner as follows: $$=\lim_{x \to 0} \frac{\frac{5\sin(5x)}{5x}}{\frac{4\sin(4x)}{4x}}$$ $$=\frac{5}{4}\lim_{x \to 0} \frac{\frac{\sin(5x)}{5x}}{\frac{\sin(4x)}{4x}}$$ $$=\frac{5}{4}\cdot\frac{1}{1}$$ $$L=\frac{5}{4}$$ From that mindset, I am trying to find this trigonometric limit: $$\lim_{x\to 2} \frac{\cos(x-2)-1}{x^{2}+x-6}$$ I know the Limit ""rule"" for cosine is: $$\lim_{x\to 0} \frac{\cos(x)-1}{x}=0$$ If you use direct substitution in the original function you end up with an equation that is same in value to the rule presented. So, from that I am assuming that the answer is $0$. I am just trying to prove that in a step-by-step manner as I did with the sine limit. P.S. I searched through many many pages of questions and didn't find something that helped. So, if I am repeating a question, I apologize that I missed it. Thanks for the help!","When solving a trigonometric limit such as: $$\lim_{x \to 0} \frac{\sin(5x)}{\sin(4x)}$$ we rework the equation to an equivalent for to fit the limit of sine ""rule"": $$\lim_{x \to 0}\frac{\sin(x)}{x}=1$$ so, we move forward in such a manner as follows: $$=\lim_{x \to 0} \frac{\frac{5\sin(5x)}{5x}}{\frac{4\sin(4x)}{4x}}$$ $$=\frac{5}{4}\lim_{x \to 0} \frac{\frac{\sin(5x)}{5x}}{\frac{\sin(4x)}{4x}}$$ $$=\frac{5}{4}\cdot\frac{1}{1}$$ $$L=\frac{5}{4}$$ From that mindset, I am trying to find this trigonometric limit: $$\lim_{x\to 2} \frac{\cos(x-2)-1}{x^{2}+x-6}$$ I know the Limit ""rule"" for cosine is: $$\lim_{x\to 0} \frac{\cos(x)-1}{x}=0$$ If you use direct substitution in the original function you end up with an equation that is same in value to the rule presented. So, from that I am assuming that the answer is $0$. I am just trying to prove that in a step-by-step manner as I did with the sine limit. P.S. I searched through many many pages of questions and didn't find something that helped. So, if I am repeating a question, I apologize that I missed it. Thanks for the help!",,"['calculus', 'limits', 'trigonometry']"
6,"Limit of ratio between a power series and a ""subset"" of the power series","Limit of ratio between a power series and a ""subset"" of the power series",,"$B$ is an infinite power series that converges everywhere, and $A$ is an infinite power series that converges everywhere which is composed only of terms found in $B$ - both have nonnegative real coefficients and are series in a single, nonnegative real variable, $x$, thus both are strictly increasing in $x$. Are there conditions on $A$ or $B$ which determine that $A/B$ has a limit, (i.e. it doesn't oscillate) as $x \to \infty$? It certainly doesn't diverge because $B\ge A$. For my problem, I have a strong feeling, based on some heuristic reasoning and based on strong evidence from a graph, that $A/B$ converges to some limit, but I am having the darnest time proving it. Any help greatly appreciated! EDIT: The actual series' I have in mind are: $$ B = e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!} $$ and, for fixed $n\in\mathbb{Z^+}$ $$ A = \sum_{j=0}^{\infty} \frac{x^{jn}}{(jn)!} $$ In other words, $A$ is composed of every $n$th term of $e^x$, starting with the $0$th term.","$B$ is an infinite power series that converges everywhere, and $A$ is an infinite power series that converges everywhere which is composed only of terms found in $B$ - both have nonnegative real coefficients and are series in a single, nonnegative real variable, $x$, thus both are strictly increasing in $x$. Are there conditions on $A$ or $B$ which determine that $A/B$ has a limit, (i.e. it doesn't oscillate) as $x \to \infty$? It certainly doesn't diverge because $B\ge A$. For my problem, I have a strong feeling, based on some heuristic reasoning and based on strong evidence from a graph, that $A/B$ converges to some limit, but I am having the darnest time proving it. Any help greatly appreciated! EDIT: The actual series' I have in mind are: $$ B = e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!} $$ and, for fixed $n\in\mathbb{Z^+}$ $$ A = \sum_{j=0}^{\infty} \frac{x^{jn}}{(jn)!} $$ In other words, $A$ is composed of every $n$th term of $e^x$, starting with the $0$th term.",,"['real-analysis', 'limits', 'power-series']"
7,"This limit supposed to be $0$ but I get $2$, why?","This limit supposed to be  but I get , why?",0 2,"I am told that $$ \lim_{x\to0}\left(4x^2\sin^2\left(\frac{1}{x}\right)-x\sin\left(\frac{2}{x}\right)\right)=0, $$ but when I calculate this by hand, I get $2$, why? I thought that this limit is $$ 4\lim_{x\to0}\left(\frac{\sin\left(\frac{1}{x}\right)}{\frac{1}{x}}\right)^2-2\lim_{x\to0}\frac{\sin\frac{2}{x}}{\frac{2}{x}}=4-2=2. $$ What I am doing wrong? And in general how can I notice that the method I used is not correct?","I am told that $$ \lim_{x\to0}\left(4x^2\sin^2\left(\frac{1}{x}\right)-x\sin\left(\frac{2}{x}\right)\right)=0, $$ but when I calculate this by hand, I get $2$, why? I thought that this limit is $$ 4\lim_{x\to0}\left(\frac{\sin\left(\frac{1}{x}\right)}{\frac{1}{x}}\right)^2-2\lim_{x\to0}\frac{\sin\frac{2}{x}}{\frac{2}{x}}=4-2=2. $$ What I am doing wrong? And in general how can I notice that the method I used is not correct?",,['calculus']
8,Limit involving logarithm of factorials,Limit involving logarithm of factorials,,Evaluate the limit $$\lim_{n \to \infty} \frac{\log(2n)!! - \log(2n-1)!!}{\log n}.$$ Where $$(2n)!!= 2\cdot 4 \cdot 6  \cdots (2n)$$ $$(2n-1)!!= 1\cdot 3 \cdot 5  \cdots  (2n-1)$$,Evaluate the limit $$\lim_{n \to \infty} \frac{\log(2n)!! - \log(2n-1)!!}{\log n}.$$ Where $$(2n)!!= 2\cdot 4 \cdot 6  \cdots (2n)$$ $$(2n-1)!!= 1\cdot 3 \cdot 5  \cdots  (2n-1)$$,,"['real-analysis', 'sequences-and-series', 'limits']"
9,determining the limit tending to infinity of n times squared roots,determining the limit tending to infinity of n times squared roots,,I'm having troubles showing that $$ \lim_{n\rightarrow \infty} \frac{\sqrt{1}+\sqrt{2}+\sqrt{3}+.....+\sqrt{n}-\frac23n\sqrt{n}}{\sqrt{n}} = \frac12. $$ I have tried this but I am not able to solve this.,I'm having troubles showing that $$ \lim_{n\rightarrow \infty} \frac{\sqrt{1}+\sqrt{2}+\sqrt{3}+.....+\sqrt{n}-\frac23n\sqrt{n}}{\sqrt{n}} = \frac12. $$ I have tried this but I am not able to solve this.,,"['calculus', 'limits']"
10,Limit to infinity of a function involving only gamma functions,Limit to infinity of a function involving only gamma functions,,"I'd like to compute the limit of $$\mathop {\lim }\limits_{x \to +\infty } \frac{{\Gamma \left( {\frac{1}{2} - \frac{x}{2}} \right)\Gamma \left( {1 + \frac{x}{2}} \right)}}{{\Gamma \left( {\frac{1}{2} + \frac{x}{2}} \right)\Gamma \left( {\frac{x}{2}} \right)}}$$ Because of the physical problem involved, I can assume that x is even, i.e. x=2n (n positive non null integer) but... does it make sense to ""approach infinity on even numbers""? Or this apparently oscillating function between +inf and –inf can somehow balance to zero?  I’m little bit confused...  Thanks in advance for any you can provide. Regards","I'd like to compute the limit of $$\mathop {\lim }\limits_{x \to +\infty } \frac{{\Gamma \left( {\frac{1}{2} - \frac{x}{2}} \right)\Gamma \left( {1 + \frac{x}{2}} \right)}}{{\Gamma \left( {\frac{1}{2} + \frac{x}{2}} \right)\Gamma \left( {\frac{x}{2}} \right)}}$$ Because of the physical problem involved, I can assume that x is even, i.e. x=2n (n positive non null integer) but... does it make sense to ""approach infinity on even numbers""? Or this apparently oscillating function between +inf and –inf can somehow balance to zero?  I’m little bit confused...  Thanks in advance for any you can provide. Regards",,"['limits', 'gamma-function']"
11,L'Hospital's rule problem $\lim_{x\to 0^+}(x^{x}-1)\ln(x)$,L'Hospital's rule problem,\lim_{x\to 0^+}(x^{x}-1)\ln(x),"$$\lim_{x\to 0^+}(x^{x}-1)\ln(x)$$ I need to solve this by L´Hopital´s rule: this is an indetermination of the type $0 \cdot \infty$: $$\lim_{x\to 0^+}(x^{x}-1)\ln(x)=\lim_{x\to 0^+}{(x^{x}-1)\over {1\over \ln(x)}} $$ and this is an indetermination of the type $0/0$, so by L'Hospital's rule: $$\lim_{x\to 0^+}{(x^{x}-1)\over {1\over \ln(x)}} =\lim_{x\to 0^+}{(x^{x}(1+\ln(x))\over {-1\over x\ln^{2}(x)}}$$ and this is an indetermination of the type $\infty/\infty$. but if I keep on using L'Hopital's rule, the limit will just get bigger so how can I do to solve this?","$$\lim_{x\to 0^+}(x^{x}-1)\ln(x)$$ I need to solve this by L´Hopital´s rule: this is an indetermination of the type $0 \cdot \infty$: $$\lim_{x\to 0^+}(x^{x}-1)\ln(x)=\lim_{x\to 0^+}{(x^{x}-1)\over {1\over \ln(x)}} $$ and this is an indetermination of the type $0/0$, so by L'Hospital's rule: $$\lim_{x\to 0^+}{(x^{x}-1)\over {1\over \ln(x)}} =\lim_{x\to 0^+}{(x^{x}(1+\ln(x))\over {-1\over x\ln^{2}(x)}}$$ and this is an indetermination of the type $\infty/\infty$. but if I keep on using L'Hopital's rule, the limit will just get bigger so how can I do to solve this?",,"['calculus', 'limits', 'derivatives']"
12,Taking limit of a probability distribution,Taking limit of a probability distribution,,"I have a probability distribution of the form $$ p_{m+1}(s)= \frac {(bs)^m}{b(m!)} e^{-bs}$$ I want to show that under the limit $m \to \infty$, it will becomes a Gaussian. I applied Stirling's formula on the factorial but I can't massage the expression into the form I want. Can someone please help me out?","I have a probability distribution of the form $$ p_{m+1}(s)= \frac {(bs)^m}{b(m!)} e^{-bs}$$ I want to show that under the limit $m \to \infty$, it will becomes a Gaussian. I applied Stirling's formula on the factorial but I can't massage the expression into the form I want. Can someone please help me out?",,"['limits', 'probability-distributions', 'central-limit-theorem']"
13,Finding Limit Function that satisfies the conditions.,Finding Limit Function that satisfies the conditions.,,"I am having some trouble figuring out a few math problems from my Calc 1 class. I am not sure where to start, as all the limits are different. find a function that satisfies the given conditions and then sketch it. sketch a graph of the function y=f(x) that satisfies the given conditions. Just label the coordinate axes and sketch the appropriate graph. For 60, 62, and 64 they are kinda the same thing. 60: would it be a function where if you let X=-1, and the denominator =0, is that what we are looking for? like $ \frac{2}{x+1} \ . $ I would say yes, even though $g(x)$ and $f(x)$ are not discontinuous on their own, that changes when you put them in a function together such as $ \frac{f(x)}{g(x)}.$ not too sure Thank you for all your help.","I am having some trouble figuring out a few math problems from my Calc 1 class. I am not sure where to start, as all the limits are different. find a function that satisfies the given conditions and then sketch it. sketch a graph of the function y=f(x) that satisfies the given conditions. Just label the coordinate axes and sketch the appropriate graph. For 60, 62, and 64 they are kinda the same thing. 60: would it be a function where if you let X=-1, and the denominator =0, is that what we are looking for? like I would say yes, even though and are not discontinuous on their own, that changes when you put them in a function together such as not too sure Thank you for all your help.", \frac{2}{x+1} \ .  g(x) f(x)  \frac{f(x)}{g(x)}.,"['calculus', 'limits']"
14,Question regarding specific limits,Question regarding specific limits,,"I was going through some past final exams for my ""Analysis 1"" class and I came across the following problem, which I've been so far unable to solve. Let $f''$ be continuous in $(-1,1)$ ; with $f(0)=0$ , $f'(0)=3$ and $f''(0)=5$ . $$ \lim_{h\rightarrow 0}\frac{f(h)+f(-h)}{h^{2}} \\ \lim_{h\rightarrow 0}\frac{1}{h^{2}}\int_{0}^{h}f(x)dx $$ Regarding the first limit, I came up with he following, although I'm not sure it's correct: Given that: $$\lim_{h\rightarrow 0}\frac{f(h))}{h^{2}}=\lim_{h\rightarrow 0}\frac{f(-h))}{h^{2}}$$ This gives: $$\lim_{h\rightarrow 0}\frac{f(h)+f(-h)}{h^{2}}=2\lim_{h\rightarrow 0}\frac{f(h)}{h^{2}}=2\lim_{h\rightarrow 0}\frac{f(0+h)-f(0)}{h} \frac{1}{h}= 2 \left [\lim_{h\rightarrow 0}\frac{f(0+h)-f(0)}{h} \ \lim_{h\rightarrow 0}\frac{1}{h}  \right ]$$ Which tends to $+\infty$ As I said, I'm not completely convinced by my reasoning. Confirmation/another solution would be helpful. I haven't been able to solve the 2nd limit. Any ideas? EDIT: I made a typo on the 2nd limit. Its $\frac{1}{h^{2}}$ rather than $h^{2}$ . Sorry...","I was going through some past final exams for my ""Analysis 1"" class and I came across the following problem, which I've been so far unable to solve. Let be continuous in ; with , and . Regarding the first limit, I came up with he following, although I'm not sure it's correct: Given that: This gives: Which tends to As I said, I'm not completely convinced by my reasoning. Confirmation/another solution would be helpful. I haven't been able to solve the 2nd limit. Any ideas? EDIT: I made a typo on the 2nd limit. Its rather than . Sorry...","f'' (-1,1) f(0)=0 f'(0)=3 f''(0)=5 
\lim_{h\rightarrow 0}\frac{f(h)+f(-h)}{h^{2}}
\\
\lim_{h\rightarrow 0}\frac{1}{h^{2}}\int_{0}^{h}f(x)dx
 \lim_{h\rightarrow 0}\frac{f(h))}{h^{2}}=\lim_{h\rightarrow 0}\frac{f(-h))}{h^{2}} \lim_{h\rightarrow 0}\frac{f(h)+f(-h)}{h^{2}}=2\lim_{h\rightarrow 0}\frac{f(h)}{h^{2}}=2\lim_{h\rightarrow 0}\frac{f(0+h)-f(0)}{h} \frac{1}{h}=
2 \left [\lim_{h\rightarrow 0}\frac{f(0+h)-f(0)}{h} \ \lim_{h\rightarrow 0}\frac{1}{h}  \right ] +\infty \frac{1}{h^{2}} h^{2}","['calculus', 'limits']"
15,Questions about continuity and differentiability,Questions about continuity and differentiability,,"$\newcommand{\sgn}{\operatorname{sgn}}$ So i have couple of questions about differentiability and continuity. For example lets consider $f(x)=\sin(\frac1x)$. It is defined and continuous for $x\neq0$ . It's derivative is $\frac{-\cos(\frac1x)}{x^2}$. It looks to me that derivative is also continuous for $x\neq0$. Is that correct? If it is does, this means that $f(x)$ is defferentiable on $(-\infty, 0) \cup (0, + \infty)$? I know that $f(x)$ is not differentiable at $x=0$ because it is not defined there. Let's look now at function $$ f(x) = \begin{cases} |x|^p\sin(\frac1x) & \text{for }x\ne0, \\ 0 & \text{for }x=0.  \end{cases} $$ Here are three questions: For which $p$ is $f(x)$ continuous? For which $p$ is $f(x)$ differentiable?  For which $p$ is $f'(x)$ continuous? Since $|x|^p\sin(\frac1x)$ for $x\neq0$ is product and composition of continous functions that are all defined for $x\neq0$, it follows that $f(x)$ is also continious for $x\neq0$. Let $p=0$. In that case for $x\neq0$ i get $f(x)=\sin(\frac1x)$. In that case $f(x)$ discontinous in 0, because $\lim_{x\to 0}\sin(\frac1x)$ is oscilatinge beetween $[-1, 1]$ and $f(0) = 0$. In other words $\lim_{x\to 0}f(x)$ doesn't exist and it cant be fixed. Let  $p<0$. In that case for $x\neq0$ i get $f(x)=\frac{\sin(\frac1x)}{|x|^p}$. In that case $f(x)$ discontinous in 0, because $f(x)=\frac{\sin(\frac1x)}{|x|^p}$ is oscilatinge beetween $[-\infty,+ \infty]$ and $f(0) = 0$. In other words $\lim_{x\to 0}f(x)$ doesn't exist and it cant be fixed. Let $p>0$. Since $f(x)$ =  $|x|^p\sin(\frac1x)$ for $x\neq0$ and it is product and composition of continious functions for $x\neq0$, this means that $f(x)$ =  $|x|^p\sin(\frac1x)$ is continious. Since $\lim_{x\to 0}|x|^p = 0$ and $\lim_{x\to 0}\sin(\frac1x) $ is oscilating between $[-1, 1]$, this means that  $\lim_{x\to 0}f(x) = 0$. Also $f(0)=0$, this means that f(x) is continious on whole $R$. Thus $f(x)$ is continuous for $p>0$. Next question is: for which $p$ is $f(x)$ differentiable? This is the question i am stuck on. I am assuming that it means differentiable on whole $R$ , if that's the case then answer is for $p>0$. $$f'(x) = \begin{cases} { p|x|^{p-1}\sin(\frac1x)\sgn(x) - \frac{|x|^{p-1}\cos(\frac1x)}{x^2}}& \text{for }x\neq0, \\ 0& \text{for }x=0 \end{cases}$$ Third question is: For which $p$ is $f'(x)$ continuous?. I tried home and it seems to me that for $p\leq1$  $\lim_{x\to 0}{ p|x|^{p-1}\sin(\frac1x)\sgn(x) - \frac{|x|^{p-1}\cos(\frac1x)}{x^2}}$ doesn't exist . For $p>1$ i don't know how to compute the limit. For lets say $p=3/2$ i get $f'(x) =  { p|x|^{\frac12}\sin(\frac1x)\sgn(x) - \frac{|x|^{\frac12}\cos(\frac1x)}{x^2}}$. While $p|x|^{\frac12}\sin(\frac1x)\sgn(x)$ goes to $0$ i dont know how to calculate $\frac{|x|^{\frac12}\cos(\frac1x)}{x^2}$, since it is undefined. Any help would be appreciated.","$\newcommand{\sgn}{\operatorname{sgn}}$ So i have couple of questions about differentiability and continuity. For example lets consider $f(x)=\sin(\frac1x)$. It is defined and continuous for $x\neq0$ . It's derivative is $\frac{-\cos(\frac1x)}{x^2}$. It looks to me that derivative is also continuous for $x\neq0$. Is that correct? If it is does, this means that $f(x)$ is defferentiable on $(-\infty, 0) \cup (0, + \infty)$? I know that $f(x)$ is not differentiable at $x=0$ because it is not defined there. Let's look now at function $$ f(x) = \begin{cases} |x|^p\sin(\frac1x) & \text{for }x\ne0, \\ 0 & \text{for }x=0.  \end{cases} $$ Here are three questions: For which $p$ is $f(x)$ continuous? For which $p$ is $f(x)$ differentiable?  For which $p$ is $f'(x)$ continuous? Since $|x|^p\sin(\frac1x)$ for $x\neq0$ is product and composition of continous functions that are all defined for $x\neq0$, it follows that $f(x)$ is also continious for $x\neq0$. Let $p=0$. In that case for $x\neq0$ i get $f(x)=\sin(\frac1x)$. In that case $f(x)$ discontinous in 0, because $\lim_{x\to 0}\sin(\frac1x)$ is oscilatinge beetween $[-1, 1]$ and $f(0) = 0$. In other words $\lim_{x\to 0}f(x)$ doesn't exist and it cant be fixed. Let  $p<0$. In that case for $x\neq0$ i get $f(x)=\frac{\sin(\frac1x)}{|x|^p}$. In that case $f(x)$ discontinous in 0, because $f(x)=\frac{\sin(\frac1x)}{|x|^p}$ is oscilatinge beetween $[-\infty,+ \infty]$ and $f(0) = 0$. In other words $\lim_{x\to 0}f(x)$ doesn't exist and it cant be fixed. Let $p>0$. Since $f(x)$ =  $|x|^p\sin(\frac1x)$ for $x\neq0$ and it is product and composition of continious functions for $x\neq0$, this means that $f(x)$ =  $|x|^p\sin(\frac1x)$ is continious. Since $\lim_{x\to 0}|x|^p = 0$ and $\lim_{x\to 0}\sin(\frac1x) $ is oscilating between $[-1, 1]$, this means that  $\lim_{x\to 0}f(x) = 0$. Also $f(0)=0$, this means that f(x) is continious on whole $R$. Thus $f(x)$ is continuous for $p>0$. Next question is: for which $p$ is $f(x)$ differentiable? This is the question i am stuck on. I am assuming that it means differentiable on whole $R$ , if that's the case then answer is for $p>0$. $$f'(x) = \begin{cases} { p|x|^{p-1}\sin(\frac1x)\sgn(x) - \frac{|x|^{p-1}\cos(\frac1x)}{x^2}}& \text{for }x\neq0, \\ 0& \text{for }x=0 \end{cases}$$ Third question is: For which $p$ is $f'(x)$ continuous?. I tried home and it seems to me that for $p\leq1$  $\lim_{x\to 0}{ p|x|^{p-1}\sin(\frac1x)\sgn(x) - \frac{|x|^{p-1}\cos(\frac1x)}{x^2}}$ doesn't exist . For $p>1$ i don't know how to compute the limit. For lets say $p=3/2$ i get $f'(x) =  { p|x|^{\frac12}\sin(\frac1x)\sgn(x) - \frac{|x|^{\frac12}\cos(\frac1x)}{x^2}}$. While $p|x|^{\frac12}\sin(\frac1x)\sgn(x)$ goes to $0$ i dont know how to calculate $\frac{|x|^{\frac12}\cos(\frac1x)}{x^2}$, since it is undefined. Any help would be appreciated.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
16,"How prove this $\displaystyle\lim_{\tau\to t}f(t,\tau)=\frac{1}{2\pi}\frac{x'_{1}(t)x''_{2}(t)-x'_{2}(t)x''_{1}(t)}{[x'_{1}(t)]^2+[x'_{2}(t)]^2}$",How prove this,"\displaystyle\lim_{\tau\to t}f(t,\tau)=\frac{1}{2\pi}\frac{x'_{1}(t)x''_{2}(t)-x'_{2}(t)x''_{1}(t)}{[x'_{1}(t)]^2+[x'_{2}(t)]^2}","Question: let  $$j_{1}(t)=\sum_{p=0}^{\infty}\dfrac{(-1)^p}{p!(1+p)!}\left(\dfrac{t}{2}\right)^{1+2p}$$ $$Y_{1}(t)=\dfrac{2}{\pi}\left(\ln{\dfrac{t}{2}}+C\right)j_{1}(t)-\dfrac{1}{\pi}\sum_{p=0}^{\infty}\dfrac{(-1)^p}{p!(1+p)!}\left(\dfrac{t}{2}\right)^{1+2p}\left(\sum_{m=1}^{p+1}\dfrac{1}{m}+\sum_{m=1}^{p}\dfrac{1}{m}\right)\tag{1}$$ let $$f(t,\tau)=\dfrac{ik}{2}\left(x'_{2}(\tau)[x_{1}(\tau)-x_{1}(t)]-x'_{1}(\tau)[x_{2}(\tau)-x_{2}(t)]\right)\dfrac{j_{1}(k|x(t)-x(\tau)|)+iY_{1}(k|x(t)-x(\tau)|)}{|x(t)-x(\tau)|}$$ where $C$ is Euler constant,and $$|x(t)-x(\tau)|=\sqrt{[x_{1}(t)-x_{1}(\tau)]^2+[x_{2}(t)-x_{2}(\tau)]^2}$$ show that:   $$\lim_{\tau\to t}f(t,\tau)=\frac{1}{2\pi}\dfrac{x'_{1}(t)x''_{2}(t)-x'_{2}(t)x''_{1}(t)}{[x'_{1}(t)]^2+[x'_{2}(t)]^2}$$ This problem is from (Inverse Acoustic and Electromagnetic Scattering Theory ) page 77,this author can't post his solution,and I take sometime to prove this,But I can't prove it. Now  my try: since $$J_{1}(t)\approx \dfrac{t}{2}+o(t)$$ $$Y_{1}(t)\approx\dfrac{t}{\pi}\left(\ln{\dfrac{t}{2}}+C\right)-\dfrac{t}{2\pi}\tag{2}$$ and then for $f(t,\tau)$ I want use L'Hôpital's rule Thank you  for you help! Thank you Thank you","Question: let  $$j_{1}(t)=\sum_{p=0}^{\infty}\dfrac{(-1)^p}{p!(1+p)!}\left(\dfrac{t}{2}\right)^{1+2p}$$ $$Y_{1}(t)=\dfrac{2}{\pi}\left(\ln{\dfrac{t}{2}}+C\right)j_{1}(t)-\dfrac{1}{\pi}\sum_{p=0}^{\infty}\dfrac{(-1)^p}{p!(1+p)!}\left(\dfrac{t}{2}\right)^{1+2p}\left(\sum_{m=1}^{p+1}\dfrac{1}{m}+\sum_{m=1}^{p}\dfrac{1}{m}\right)\tag{1}$$ let $$f(t,\tau)=\dfrac{ik}{2}\left(x'_{2}(\tau)[x_{1}(\tau)-x_{1}(t)]-x'_{1}(\tau)[x_{2}(\tau)-x_{2}(t)]\right)\dfrac{j_{1}(k|x(t)-x(\tau)|)+iY_{1}(k|x(t)-x(\tau)|)}{|x(t)-x(\tau)|}$$ where $C$ is Euler constant,and $$|x(t)-x(\tau)|=\sqrt{[x_{1}(t)-x_{1}(\tau)]^2+[x_{2}(t)-x_{2}(\tau)]^2}$$ show that:   $$\lim_{\tau\to t}f(t,\tau)=\frac{1}{2\pi}\dfrac{x'_{1}(t)x''_{2}(t)-x'_{2}(t)x''_{1}(t)}{[x'_{1}(t)]^2+[x'_{2}(t)]^2}$$ This problem is from (Inverse Acoustic and Electromagnetic Scattering Theory ) page 77,this author can't post his solution,and I take sometime to prove this,But I can't prove it. Now  my try: since $$J_{1}(t)\approx \dfrac{t}{2}+o(t)$$ $$Y_{1}(t)\approx\dfrac{t}{\pi}\left(\ln{\dfrac{t}{2}}+C\right)-\dfrac{t}{2\pi}\tag{2}$$ and then for $f(t,\tau)$ I want use L'Hôpital's rule Thank you  for you help! Thank you Thank you",,"['limits', 'functions']"
17,Bounding for convolution convergence,Bounding for convolution convergence,,"Suppose $f\in L^p(\mathbb{R})$ and $K\in L^1(\mathbb{R})$ with $\int_\mathbb{R}K(x)dx=1$. Define $$K_t(x)=\dfrac{1}{t}K\left(\dfrac{x}{t}\right)$$ I'm trying to prove that $\lim_{t\rightarrow 0}\|f\ast K_t-f\|_p=0$. I choose a compactly supported function $g\in C^\infty$ such that $\|f-g\|_p<\epsilon$ (possible because this class is dense in $L^p(\mathbb{R})$). Then I want to bound $$\|f\ast K_t-f\|_p\leq \|f\ast K_t-g\ast K_t\|_p+\|g\ast K_t-g\|_p+\|g-f\|_p$$ We have of course $\|g-f\|_p<\epsilon$. We have $\|(f-g)\ast K_t\|_p\leq \|f-g\|_p\|K_t\|_1<\epsilon\|K_t\|_1=\epsilon\|K\|_1$. Now I need to bound $\|g\ast K_t-g\|_p$. I have $$\left|(g\ast K_t)(x)-g(x)\right|_p=\left|\int_\mathbb{R}(g(x-y)-g(x))K_t(y)dy\right|$$ Since $g$ is continuous and compactly supported, it is uniformly continuous. But what can I do with the $K_t(y)$?","Suppose $f\in L^p(\mathbb{R})$ and $K\in L^1(\mathbb{R})$ with $\int_\mathbb{R}K(x)dx=1$. Define $$K_t(x)=\dfrac{1}{t}K\left(\dfrac{x}{t}\right)$$ I'm trying to prove that $\lim_{t\rightarrow 0}\|f\ast K_t-f\|_p=0$. I choose a compactly supported function $g\in C^\infty$ such that $\|f-g\|_p<\epsilon$ (possible because this class is dense in $L^p(\mathbb{R})$). Then I want to bound $$\|f\ast K_t-f\|_p\leq \|f\ast K_t-g\ast K_t\|_p+\|g\ast K_t-g\|_p+\|g-f\|_p$$ We have of course $\|g-f\|_p<\epsilon$. We have $\|(f-g)\ast K_t\|_p\leq \|f-g\|_p\|K_t\|_1<\epsilon\|K_t\|_1=\epsilon\|K\|_1$. Now I need to bound $\|g\ast K_t-g\|_p$. I have $$\left|(g\ast K_t)(x)-g(x)\right|_p=\left|\int_\mathbb{R}(g(x-y)-g(x))K_t(y)dy\right|$$ Since $g$ is continuous and compactly supported, it is uniformly continuous. But what can I do with the $K_t(y)$?",,"['limits', 'convergence-divergence', 'lebesgue-integral', 'convolution', 'lp-spaces']"
18,Why use $(1+p)^n\geq 1+np$ to prove that the successive powers of a number $q^n$ with $-1<q<1$ approach zero as $n\rightarrow \infty$?,Why use  to prove that the successive powers of a number  with  approach zero as ?,(1+p)^n\geq 1+np q^n -1<q<1 n\rightarrow \infty,"I'm reading Courant's What is Mathematics? In page $64$, he gives the example of limit of the successive powers of $q$. If $-1<q<1$ then the successive powers of $q$ will approach zero as $n$ increases. Then he suggests that to give a rigorous proof of that assertion, one needs to use the inequality proved on page $15$, which is: $$(1+p)^n\geq 1+np$$ But I'm not sure on how these things are connected - And I guess I'm unable to provide more information on my doubt, I just want to understand why that inequality is used to prove that that limit approaches $0$ as $n\rightarrow \infty$ for $-1<q<1$. I have something to add to this question. I guess I'm half the way to understand , but this is not the point. Reading the sentence again: If $-1<q<1$ then the successive powers of $q$ will approach zero as $n$ increases. I have an argument that could work as a proof, and it's a lot simpler: Suppose we take the number $10$, observation the successive powers of 10: $$ \begin{array}{cr}  10 & 10 \\  10^2 & 100 \\  10^3 & 1000 \\  10^4 & 10000 \\ \end{array}$$ As we raise $n$ in $10^n$, we add one zero and the one goes on sliping to the left, this will happen for any number bigger than $1$ and different of $0$, the process will only take a little more time to happen, for $2^n$ we have: $$ \begin{array}{cc}  2 & 2 \\  2^2 & 4 \\  2^3 & 8 \\  2^4 & 16 \\ \end{array}$$ If $a$ is smaller than $1$ in $a^n$, then we'll have for example: $$\begin{array}{cc}  \frac{1}{10} & 0.1 \\  \left(\frac{1}{10}\right)^2 & 0.01 \\  \left(\frac{1}{10}\right)^3 & 0.001 \\  \left(\frac{1}{10}\right)^4 & 0.0001 \\ \end{array}$$ Then if the numbers are different of $1$ and $0$, I know that these numbers are going to walk . If $n<1$, it's going to walk to the right, and if $n>1$, the number will walk to the left. This looks pretty convincing to me so, why do I need to use Bernoulli's inequality? The answers given by John and RobJohn are useful, they tell me how to do it. But it's not really clear why I should use it instead of the method I just pointed.","I'm reading Courant's What is Mathematics? In page $64$, he gives the example of limit of the successive powers of $q$. If $-1<q<1$ then the successive powers of $q$ will approach zero as $n$ increases. Then he suggests that to give a rigorous proof of that assertion, one needs to use the inequality proved on page $15$, which is: $$(1+p)^n\geq 1+np$$ But I'm not sure on how these things are connected - And I guess I'm unable to provide more information on my doubt, I just want to understand why that inequality is used to prove that that limit approaches $0$ as $n\rightarrow \infty$ for $-1<q<1$. I have something to add to this question. I guess I'm half the way to understand , but this is not the point. Reading the sentence again: If $-1<q<1$ then the successive powers of $q$ will approach zero as $n$ increases. I have an argument that could work as a proof, and it's a lot simpler: Suppose we take the number $10$, observation the successive powers of 10: $$ \begin{array}{cr}  10 & 10 \\  10^2 & 100 \\  10^3 & 1000 \\  10^4 & 10000 \\ \end{array}$$ As we raise $n$ in $10^n$, we add one zero and the one goes on sliping to the left, this will happen for any number bigger than $1$ and different of $0$, the process will only take a little more time to happen, for $2^n$ we have: $$ \begin{array}{cc}  2 & 2 \\  2^2 & 4 \\  2^3 & 8 \\  2^4 & 16 \\ \end{array}$$ If $a$ is smaller than $1$ in $a^n$, then we'll have for example: $$\begin{array}{cc}  \frac{1}{10} & 0.1 \\  \left(\frac{1}{10}\right)^2 & 0.01 \\  \left(\frac{1}{10}\right)^3 & 0.001 \\  \left(\frac{1}{10}\right)^4 & 0.0001 \\ \end{array}$$ Then if the numbers are different of $1$ and $0$, I know that these numbers are going to walk . If $n<1$, it's going to walk to the right, and if $n>1$, the number will walk to the left. This looks pretty convincing to me so, why do I need to use Bernoulli's inequality? The answers given by John and RobJohn are useful, they tell me how to do it. But it's not really clear why I should use it instead of the method I just pointed.",,"['calculus', 'sequences-and-series', 'limits', 'inequality']"
19,limit of summation [duplicate],limit of summation [duplicate],,"This question already has answers here : How do you calculate this limit $\lim_{n\to\infty}\sum_{k=1}^{n} \frac{k}{n^2+k^2}$? (3 answers) Closed 4 years ago . Using Riemann integrals of suitably functions, find the following limit $$\lim_{n\to \infty}\sum_{k=1}^n \frac{k}{n^2+k^2}$$ Please help me check my method: Let $$f(x)=\frac{x}{1+x^2}$$ For each n$\in$ $\Bbb N$, let partition $$P_n=({\frac{k}{n}:0\le k\le n})$$ and $$\xi^{(n)}=(\frac{1}{n},\frac{2}{n},...,\frac{n-1}{2n},1)$$ and $||P_n||=\frac{1}{n} \rightarrow 0$ $$\lim_{n\to \infty}\sum_{k=1}^n \frac{k}{n^2+k^2}=\int_0^1 \frac{x}{1+x^2}dx=\frac{1}{2} \ln(1+x^2)|^1_0=\frac{\ln 2}{2}$$ Is there any other methods for this question?","This question already has answers here : How do you calculate this limit $\lim_{n\to\infty}\sum_{k=1}^{n} \frac{k}{n^2+k^2}$? (3 answers) Closed 4 years ago . Using Riemann integrals of suitably functions, find the following limit $$\lim_{n\to \infty}\sum_{k=1}^n \frac{k}{n^2+k^2}$$ Please help me check my method: Let $$f(x)=\frac{x}{1+x^2}$$ For each n$\in$ $\Bbb N$, let partition $$P_n=({\frac{k}{n}:0\le k\le n})$$ and $$\xi^{(n)}=(\frac{1}{n},\frac{2}{n},...,\frac{n-1}{2n},1)$$ and $||P_n||=\frac{1}{n} \rightarrow 0$ $$\lim_{n\to \infty}\sum_{k=1}^n \frac{k}{n^2+k^2}=\int_0^1 \frac{x}{1+x^2}dx=\frac{1}{2} \ln(1+x^2)|^1_0=\frac{\ln 2}{2}$$ Is there any other methods for this question?",,"['limits', 'summation']"
20,Limit with integral or is this function continuous?,Limit with integral or is this function continuous?,,"Hello I need to show one identity and one limit. I am having problems with it. notation: $x_i$ is i-th coordinate of $x$ $B(x,r)$ ball with center $x$ and radius $r$ $S(x,r)$ sphere with center $x$ and radius $r$ $n_y$ in integral it means unit outer normal at point $y$ $dS_y$ standard surface measure with $y$ as integration variable Let $\partial M$ be closed surface in $\mathbb{R}^3$. Than show that this identity hold   $$ x_i = \frac{\int_{\partial M} \frac{y_i}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y}{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} $$   And let $f$ be continuous function defined only on $\partial M$. Than show that $g$ is continuous in $\overline{M}$. Where $g$ is:    $$ g(x) = \frac{\int_{\partial M} \frac{f(y)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y}{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} $$ Reference where I got this problem. I'm reading this paper where they define function $g$ and they just comment that is continuous because $\frac{1}{\| y-x \|}$ goes to infinity as $y$ approaches $x$. Plus they wrote down those integrals in very funny way which I do not completely understand. I am having problems when the surface $\partial M$(they denote it $P$) is not strictly convex. But that is not that important. Ok so the first identity. It is more convinient to write it in this form: $$  \int_{\partial M} \frac{y_i-x_i}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y = 0$$ Intuitively this part: $n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y$ is $dS_y$ projected onto sphere of unit radius and center $x$ and $\frac{y_i-x_i}{||y-x||}$ is just outer normal of that sphere. So the integral is almost this: $\int_{S(x,1)}n_y dS_y$ which is zero. But problem is that in original integral you run over some places multiple times and even in reverse orientation. The only problem with $g$ is to show that it is continuous on the boundary $\partial M$. So we deal with limit: $$ \lim_{x \rightarrow x_0, x\in M} \frac{\int_{\partial M} \frac{f(y)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y}{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} \overset{?}{=} f(x_0) $$ Which we can rewrite as: $$ \lim_{x \rightarrow x_0, x\in M} \frac{\int_{\partial M} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y}{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} \overset{?}{=} 0 $$ What I try: For give $\epsilon$ I take $\delta$ that $|x_0-y|<\delta \Rightarrow |f(x_0)-f(y)|<\epsilon$ Then I split the integral: $$\left| \frac{\int_{\partial M\setminus B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y + \int_{\partial M \cap B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y }{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} \right| \leq $$ $$  \frac{ \left| \int_{\partial M\setminus B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right| +  \left|\int_{\partial M \cap B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right| }{ \left|\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right|} \leq $$ $$ \frac{ \left| \int_{\partial M\setminus B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right| }{ \left|\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right|}  + \epsilon $$ But I can't find any bound for the second part. I'm probably doing it completely wrong. Maybe even this inequality I used is wrong. $$ \frac{  \int_{\partial M \cap B(x_0,\delta)}  \left| \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} \right| dS_y  }{ \left|\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right|} \leq 1  $$ The can maybe go something terribly wrong with the surface $\partial M$ so this inequality fails. I don't know. Any help would be very much appreciated.","Hello I need to show one identity and one limit. I am having problems with it. notation: $x_i$ is i-th coordinate of $x$ $B(x,r)$ ball with center $x$ and radius $r$ $S(x,r)$ sphere with center $x$ and radius $r$ $n_y$ in integral it means unit outer normal at point $y$ $dS_y$ standard surface measure with $y$ as integration variable Let $\partial M$ be closed surface in $\mathbb{R}^3$. Than show that this identity hold   $$ x_i = \frac{\int_{\partial M} \frac{y_i}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y}{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} $$   And let $f$ be continuous function defined only on $\partial M$. Than show that $g$ is continuous in $\overline{M}$. Where $g$ is:    $$ g(x) = \frac{\int_{\partial M} \frac{f(y)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y}{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} $$ Reference where I got this problem. I'm reading this paper where they define function $g$ and they just comment that is continuous because $\frac{1}{\| y-x \|}$ goes to infinity as $y$ approaches $x$. Plus they wrote down those integrals in very funny way which I do not completely understand. I am having problems when the surface $\partial M$(they denote it $P$) is not strictly convex. But that is not that important. Ok so the first identity. It is more convinient to write it in this form: $$  \int_{\partial M} \frac{y_i-x_i}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y = 0$$ Intuitively this part: $n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y$ is $dS_y$ projected onto sphere of unit radius and center $x$ and $\frac{y_i-x_i}{||y-x||}$ is just outer normal of that sphere. So the integral is almost this: $\int_{S(x,1)}n_y dS_y$ which is zero. But problem is that in original integral you run over some places multiple times and even in reverse orientation. The only problem with $g$ is to show that it is continuous on the boundary $\partial M$. So we deal with limit: $$ \lim_{x \rightarrow x_0, x\in M} \frac{\int_{\partial M} \frac{f(y)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y}{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} \overset{?}{=} f(x_0) $$ Which we can rewrite as: $$ \lim_{x \rightarrow x_0, x\in M} \frac{\int_{\partial M} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y}{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} \overset{?}{=} 0 $$ What I try: For give $\epsilon$ I take $\delta$ that $|x_0-y|<\delta \Rightarrow |f(x_0)-f(y)|<\epsilon$ Then I split the integral: $$\left| \frac{\int_{\partial M\setminus B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y + \int_{\partial M \cap B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y }{\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y} \right| \leq $$ $$  \frac{ \left| \int_{\partial M\setminus B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right| +  \left|\int_{\partial M \cap B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right| }{ \left|\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right|} \leq $$ $$ \frac{ \left| \int_{\partial M\setminus B(x_0,\delta)} \frac{f(y)-f(x_0)}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right| }{ \left|\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right|}  + \epsilon $$ But I can't find any bound for the second part. I'm probably doing it completely wrong. Maybe even this inequality I used is wrong. $$ \frac{  \int_{\partial M \cap B(x_0,\delta)}  \left| \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} \right| dS_y  }{ \left|\int_{\partial M} \frac{1}{||y-x||} n_y \cdot \nabla_y \frac{1}{||y-x||} dS_y \right|} \leq 1  $$ The can maybe go something terribly wrong with the surface $\partial M$ so this inequality fails. I don't know. Any help would be very much appreciated.",,"['integration', 'limits', 'multivariable-calculus', 'continuity']"
21,Limit of product of $\sin \frac{k}{n}$,Limit of product of,\sin \frac{k}{n},"Could you help me how to find the limit of $$\left(\sin \frac{1}{n} \cdot \sin \frac{2}{n} \cdots \sin 1\right)^{\frac{1}{n}}?$$ I know that $$\ln \left((\sin \frac{1}{n} \cdot \sin \frac{2}{n} \cdots \sin 1)^{\frac{1}{n}} \right)=\frac{1}{n} \sum_{k=1}^n \ln \left( \sin(\frac{k}{n})\right)$$ and $$\lim _{n \rightarrow \infty}  \frac{1}{n} \sum_{k=1}^n \ln \left( \sin(\frac{k}{n})\right) = \int_0^1 \ln(\sin(x)) \, dx \text{ (Riemann integral)}$$ but I am not sure what to do next, I mean, how do I get back to $$\lim_{n \rightarrow \infty} (\sin \frac{1}{n} \cdot \sin \frac{2}{n} \cdots \sin 1)^{\frac{1}{n}}?$$ Could you help me with that?","Could you help me how to find the limit of $$\left(\sin \frac{1}{n} \cdot \sin \frac{2}{n} \cdots \sin 1\right)^{\frac{1}{n}}?$$ I know that $$\ln \left((\sin \frac{1}{n} \cdot \sin \frac{2}{n} \cdots \sin 1)^{\frac{1}{n}} \right)=\frac{1}{n} \sum_{k=1}^n \ln \left( \sin(\frac{k}{n})\right)$$ and $$\lim _{n \rightarrow \infty}  \frac{1}{n} \sum_{k=1}^n \ln \left( \sin(\frac{k}{n})\right) = \int_0^1 \ln(\sin(x)) \, dx \text{ (Riemann integral)}$$ but I am not sure what to do next, I mean, how do I get back to $$\lim_{n \rightarrow \infty} (\sin \frac{1}{n} \cdot \sin \frac{2}{n} \cdots \sin 1)^{\frac{1}{n}}?$$ Could you help me with that?",,"['sequences-and-series', 'integration', 'limits']"
22,Does uniform continuity imply the interchangeability of the order of the limits?,Does uniform continuity imply the interchangeability of the order of the limits?,,"I am learning Uniform Continuous in the Advanced Calculus class now. Today, teacher taught a very important theorem (he said) as following: Theorem : Let $A\subset M$ where M is a metric space, let $f_k:A\to N$ be a sequence of continuous functions, and suppose that $f_k \to f$ (uniformly on $A$). Then $f$ is continuous on $A$. In order to make us remember more clearly, teacher sketched the proof: $$ \lim_{x\to a}f(x)=\lim_{x\to a}\lim_{n\to\infty}f_n(x){\color{red}=}\lim_{n\to\infty}\lim_{x\to a}f_n(x)=\lim_{n\to\infty}f_n(a)=f(a). $$ Thus, $f$ is continous. But I am wondering that uniform continuity can truly imply the interchangeability of the order of the limits , the process which is marked with red color above. Could anybody please help me? If so, could you please prove it?","I am learning Uniform Continuous in the Advanced Calculus class now. Today, teacher taught a very important theorem (he said) as following: Theorem : Let $A\subset M$ where M is a metric space, let $f_k:A\to N$ be a sequence of continuous functions, and suppose that $f_k \to f$ (uniformly on $A$). Then $f$ is continuous on $A$. In order to make us remember more clearly, teacher sketched the proof: $$ \lim_{x\to a}f(x)=\lim_{x\to a}\lim_{n\to\infty}f_n(x){\color{red}=}\lim_{n\to\infty}\lim_{x\to a}f_n(x)=\lim_{n\to\infty}f_n(a)=f(a). $$ Thus, $f$ is continous. But I am wondering that uniform continuity can truly imply the interchangeability of the order of the limits , the process which is marked with red color above. Could anybody please help me? If so, could you please prove it?",,"['real-analysis', 'limits']"
23,A Limit of a sum related to the exponential series.,A Limit of a sum related to the exponential series.,,"So my question is to evaluate, $\lim_{n\to\infty} n^ne^{-n}\sum_{k=1}^{\infty}\frac{n^k}{(n+k)!}$ What I've done so far is, $\lim_{n\to\infty} n^ne^{-n}\sum_{k=1}^{\infty}\frac{n^k}{(n+k)!} $ $=\lim_{n\to\infty} n^ne^{-n}\frac1{n!}\sum_{k=1}^{\infty}\frac{n^k}{(n+1)(n+2)..(n+k)}$ $=\lim_{n\to\infty} \frac1{\sqrt{2\pi n}} \sum_{k=1}^{\infty}\frac{1}{(1+\frac1n)(1+\frac2n)..(1+\frac k n)}$ $=\lim_{n\to\infty} \frac1{\sqrt{2\pi n}} \sum_{k=1}^{n}\frac{1}{(1+\frac1n)(1+\frac2n)..(1+\frac k n)}$ (The rest of the sum is bounded.) And then, I'm stuck. Any ideas?","So my question is to evaluate, $\lim_{n\to\infty} n^ne^{-n}\sum_{k=1}^{\infty}\frac{n^k}{(n+k)!}$ What I've done so far is, $\lim_{n\to\infty} n^ne^{-n}\sum_{k=1}^{\infty}\frac{n^k}{(n+k)!} $ $=\lim_{n\to\infty} n^ne^{-n}\frac1{n!}\sum_{k=1}^{\infty}\frac{n^k}{(n+1)(n+2)..(n+k)}$ $=\lim_{n\to\infty} \frac1{\sqrt{2\pi n}} \sum_{k=1}^{\infty}\frac{1}{(1+\frac1n)(1+\frac2n)..(1+\frac k n)}$ $=\lim_{n\to\infty} \frac1{\sqrt{2\pi n}} \sum_{k=1}^{n}\frac{1}{(1+\frac1n)(1+\frac2n)..(1+\frac k n)}$ (The rest of the sum is bounded.) And then, I'm stuck. Any ideas?",,"['calculus', 'sequences-and-series', 'limits']"
24,How to determine the sum of $\sum_{k=1}^{\infty} \frac{k^k}{(k!)^2}$,How to determine the sum of,\sum_{k=1}^{\infty} \frac{k^k}{(k!)^2},$$\sum_{k=1}^{\infty} \frac{k^k}{(k!)^2}$$ The series converges by the ratio test but how would I find out the exact sum of the series.,$$\sum_{k=1}^{\infty} \frac{k^k}{(k!)^2}$$ The series converges by the ratio test but how would I find out the exact sum of the series.,,"['sequences-and-series', 'limits']"
25,Question on my proof of unboundedness of sequence.,Question on my proof of unboundedness of sequence.,,"I want to show that the sequence $$  a_{n+1} = \sqrt{ 1 + a_n^2 } $$ is strictly monotone increasing but not bounded. That it is strictly increasing is simple I think, just $a_n = \sqrt{a_n^2} < \sqrt{1 + a_n^2}$ and using that $\sqrt{\cdot}$ is strictly increasing. For unbounded, I used $$  a_{n+1} = \sqrt{1+a_n^2} = \sqrt{1 + (1+a_{n-1})} = \ldots = \sqrt{n + a_1^2}. $$ And it follows because $\sqrt{\cdot}$ is unbounded. Is this right, is there any more formal method to proof this, by showing more directly that there could be no upper bound?","I want to show that the sequence $$  a_{n+1} = \sqrt{ 1 + a_n^2 } $$ is strictly monotone increasing but not bounded. That it is strictly increasing is simple I think, just $a_n = \sqrt{a_n^2} < \sqrt{1 + a_n^2}$ and using that $\sqrt{\cdot}$ is strictly increasing. For unbounded, I used $$  a_{n+1} = \sqrt{1+a_n^2} = \sqrt{1 + (1+a_{n-1})} = \ldots = \sqrt{n + a_1^2}. $$ And it follows because $\sqrt{\cdot}$ is unbounded. Is this right, is there any more formal method to proof this, by showing more directly that there could be no upper bound?",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
26,Find $\lim_{x\to 0} \frac{(1+x)^{1/x} - e}{x}$ [duplicate],Find  [duplicate],\lim_{x\to 0} \frac{(1+x)^{1/x} - e}{x},"This question already has answers here : Limit as $x\to 0$ of $\frac{(1+x)^{1/x}-e}{x}$ (6 answers) Closed 4 years ago . Find $$\lim_{x\to 0} \frac{(1+x)^{1/x} - e}{x}$$. I tried applying L'Hopital's rule but had difficulty with deriving $(1+x)^{1/x}$, and it seemed to me that there is probably a more elegant solution than the horrible derivative WolframAlpha gave (which was also not useful, as the derivative contained $\frac{1}{x}$ as an exponent). Any help would be appreciated.","This question already has answers here : Limit as $x\to 0$ of $\frac{(1+x)^{1/x}-e}{x}$ (6 answers) Closed 4 years ago . Find $$\lim_{x\to 0} \frac{(1+x)^{1/x} - e}{x}$$. I tried applying L'Hopital's rule but had difficulty with deriving $(1+x)^{1/x}$, and it seemed to me that there is probably a more elegant solution than the horrible derivative WolframAlpha gave (which was also not useful, as the derivative contained $\frac{1}{x}$ as an exponent). Any help would be appreciated.",,"['calculus', 'real-analysis', 'limits']"
27,$f$ continous at $x_0$ $⇒\lim_{h→0}∫_{x_0}^{x_0+h}\frac{f(t)}{h}=f(x_0)$,continous at,f x_0 ⇒\lim_{h→0}∫_{x_0}^{x_0+h}\frac{f(t)}{h}=f(x_0),"The function $f:ℝ→ℝ$ is continuous on $x_0\inℝ$.  Prove using the definition of a Darboux Integral that  $$\lim_{h→0}∫_{x_0}^{x_0+h}\frac{f(t)}{h}=f(x_0)$$ I'm a first grade math student following an analysis course. The book that is used is Elementary Analysis by Ross . Definitions The upper Darboux sum $U(f,P)$ of $f$ with respect to a partition $P$ is the sum  $$U(f,P)=∑_{k=1}^nM(f,[t_{k-1},t_k])(t_k-t_{k-1})$$ The lower Darboux sum $L(f,P)$ of $f$ with respect to a partition $P$ is the sum  $$L(f,P)=∑_{k=1}^nm(f,[t_{k-1},t_k])(t_k-t_{k-1})$$ $f$ is continuous at $x_0$ ⇔ $∀ε>0,∃δ>0,(|x-x_0|<δ ⇒ |f(x)-f(x_0)|<ε)$ Let $f$ be a function defined on on $J-\{a\}$ for some interval $J$ containin $a$, and let $L$ be a real number. Then $ \lim_{x→a}f(x)=L$ if and only if $$∀ε>0,∃δ>0,(0<|x-a|<δ⇒|f(x)-L|<ε)$$ Can someone check if this is an correct proof ? Proof Let $ε>0$. Then there exist an $δ>0$, such that: $|x-x_0|<δ ⇒ |f(x)-f(x_0)|<ε$.  Let $0<|h-0|<δ$. If $x\in[x_0,x_0+h]$ then $x\in(x_0-δ,x_0+δ)$, then $|f(x)-f(x_0)|<ε$, then $|\frac{f(x)}{h}-\frac{f(x_0)}{h}|<\frac{ε}{h}$. Therefore:  \begin{equation*}  m ( \frac{f(x)}{h},[x_0,x_0+h]) \cdot (x_0+h - x_0) ≥ \frac{f(x_0)-ε}{h} \cdot h = f(x_0)-ε \end{equation*} \begin{equation*}  M ( \frac{f(x)}{h},[x_0,x_0+h]) \cdot (x_0+h - x_0) ≤ \frac{f(x_0)+ε}{h} \cdot h = f(x_0)+ε \end{equation*} Therefore we can conclude that for a partition $P$ of $[x_o,x_0+h]$ \begin{equation*}  f(x_0)-ε< L(f,P)≤∫_{x_0}^{x_0+h}\frac{f(t)}{h}≤U(f,P)<f(x_0)+ε  \end{equation*} QED","The function $f:ℝ→ℝ$ is continuous on $x_0\inℝ$.  Prove using the definition of a Darboux Integral that  $$\lim_{h→0}∫_{x_0}^{x_0+h}\frac{f(t)}{h}=f(x_0)$$ I'm a first grade math student following an analysis course. The book that is used is Elementary Analysis by Ross . Definitions The upper Darboux sum $U(f,P)$ of $f$ with respect to a partition $P$ is the sum  $$U(f,P)=∑_{k=1}^nM(f,[t_{k-1},t_k])(t_k-t_{k-1})$$ The lower Darboux sum $L(f,P)$ of $f$ with respect to a partition $P$ is the sum  $$L(f,P)=∑_{k=1}^nm(f,[t_{k-1},t_k])(t_k-t_{k-1})$$ $f$ is continuous at $x_0$ ⇔ $∀ε>0,∃δ>0,(|x-x_0|<δ ⇒ |f(x)-f(x_0)|<ε)$ Let $f$ be a function defined on on $J-\{a\}$ for some interval $J$ containin $a$, and let $L$ be a real number. Then $ \lim_{x→a}f(x)=L$ if and only if $$∀ε>0,∃δ>0,(0<|x-a|<δ⇒|f(x)-L|<ε)$$ Can someone check if this is an correct proof ? Proof Let $ε>0$. Then there exist an $δ>0$, such that: $|x-x_0|<δ ⇒ |f(x)-f(x_0)|<ε$.  Let $0<|h-0|<δ$. If $x\in[x_0,x_0+h]$ then $x\in(x_0-δ,x_0+δ)$, then $|f(x)-f(x_0)|<ε$, then $|\frac{f(x)}{h}-\frac{f(x_0)}{h}|<\frac{ε}{h}$. Therefore:  \begin{equation*}  m ( \frac{f(x)}{h},[x_0,x_0+h]) \cdot (x_0+h - x_0) ≥ \frac{f(x_0)-ε}{h} \cdot h = f(x_0)-ε \end{equation*} \begin{equation*}  M ( \frac{f(x)}{h},[x_0,x_0+h]) \cdot (x_0+h - x_0) ≤ \frac{f(x_0)+ε}{h} \cdot h = f(x_0)+ε \end{equation*} Therefore we can conclude that for a partition $P$ of $[x_o,x_0+h]$ \begin{equation*}  f(x_0)-ε< L(f,P)≤∫_{x_0}^{x_0+h}\frac{f(t)}{h}≤U(f,P)<f(x_0)+ε  \end{equation*} QED",,"['real-analysis', 'limits', 'integration', 'continuity']"
28,Monotonicity and Derivatives,Monotonicity and Derivatives,,"It is quite easy to see that there are plenty of functions $f$ for which $$ \frac{d^n}{dx^n} f(x) \geq 0 \;\;\;\forall n \in \mathbb{N}, x \in \mathbb{R}_+$$ For starters, it holds for any polynomial with positive or zero coefficients. The stronger condition, with a strict inequality $$(1) \;\;\;\frac{d^n}{dx^n} f(x) > 0 \;\;\;\forall n \in \mathbb{N}, x \in \mathbb{R}_+$$ holds in less cases. However, sums of exponentials of the form $$ \;\;\; f(x) = \sum_i A_i e^{B_i x} \;\;\; A_i, B_i > 0$$ satisfy this stronger condition. But there are others My questions are: 1) Does (1) imply $O(f(x)) \geq O(\exp(x))$, (in the sense $O(a(x)) >,=,< O(b(x))$ if $\lim_{x\rightarrow\infty} \frac{a(x)}{b(x)} = \infty,\text{constant},0$, respectively) 2) Is there a simple test for (1), and/or, does this condition have an established name?","It is quite easy to see that there are plenty of functions $f$ for which $$ \frac{d^n}{dx^n} f(x) \geq 0 \;\;\;\forall n \in \mathbb{N}, x \in \mathbb{R}_+$$ For starters, it holds for any polynomial with positive or zero coefficients. The stronger condition, with a strict inequality $$(1) \;\;\;\frac{d^n}{dx^n} f(x) > 0 \;\;\;\forall n \in \mathbb{N}, x \in \mathbb{R}_+$$ holds in less cases. However, sums of exponentials of the form $$ \;\;\; f(x) = \sum_i A_i e^{B_i x} \;\;\; A_i, B_i > 0$$ satisfy this stronger condition. But there are others My questions are: 1) Does (1) imply $O(f(x)) \geq O(\exp(x))$, (in the sense $O(a(x)) >,=,< O(b(x))$ if $\lim_{x\rightarrow\infty} \frac{a(x)}{b(x)} = \infty,\text{constant},0$, respectively) 2) Is there a simple test for (1), and/or, does this condition have an established name?",,"['real-analysis', 'limits']"
29,Limit preserving metrics.,Limit preserving metrics.,,"I need to prove that given a sequence of points $\{a_n\}$ in $(\Bbb R^k,d_m)$ that converges to $a$, then it converges to the same limit in both $(\Bbb R^k,d_e)$ and $(\Bbb R^k,d_t)$ (and conversely), where $$ d_m(x,a)=\max_{1\leq i \leq k} \{|x_i-a_i|\} \;;\;\text{ the max metric}$$ $$d_e(x,a)=\sqrt{\sum_{1\leq i \leq k}(x_i-a_i)^2} \;;\;\text{ the Euclidean metric}$$ $$d_t(x,a)={\sum_{1\leq i \leq k}|x_i-a_i|} \;;\;\text{ the so-called taxicab metric}$$ There is a nice characterization of a point $a$ being a limit of a sequence $\{ a_n\}$ which is Given a sequence of points $\{a_n\}$ in a metric space $(X,d)$ then $\lim a_n = a$ if and only if for every neighborhood $V$ of $a$, only but finitely many of the $a_n$ are not contained in $V$, or equivalently $a_n \in V$ for almost all of the $a_n$. More formally, one can say that for every nbhd $V$ of $a$ there is an $N$ such that whenever $n>N$, $a_n \in V$. This said, one can prove: THEOREM Let $\{a_n\}$ be a sequence of points in $\Bbb R^k$. Then if $\lim a_n=a$ under either one of the metrics $d_t,d_e,d_m$, it follows $\lim a_n=a$ for the other two. (NEW) PROOF We have that $$\tag a d_m(a,x) \leq d_e(a,x) \leq  \sqrt{k}\cdot d_m(a,x) $$ $$\tag b d_m(a,x) \leq d_t(a,x) \leq  {k}\cdot d_m(a,x) $$ Let $\lim a_n=a$ in $(\Bbb R^k,d_m)$. Let $T$,$E$ be neighborhoods of $a$ in $(\Bbb R^k,d_t)$ and $(\Bbb R^k,d_e)$ respectively. Then they contain an open $t$-ball and an open $e$-ball about $a$ for a $\delta>0$. $(a)$ and $(b)$ guarantee that each of these balls contain an open $m$-ball about $a$. But this ball $B_m$ is a neighborhood of $a$ in $(\Bbb R^k,d_m)$ so $a_n\in B_m$ whenever $n>N$ for a some $N$. This in turn means that $a_n \in B_t$ and $a_n \in B_e$ whenever $n>N$ so $\lim a_n=a$ in both the two other metrics. The other two cases follow analogously.","I need to prove that given a sequence of points $\{a_n\}$ in $(\Bbb R^k,d_m)$ that converges to $a$, then it converges to the same limit in both $(\Bbb R^k,d_e)$ and $(\Bbb R^k,d_t)$ (and conversely), where $$ d_m(x,a)=\max_{1\leq i \leq k} \{|x_i-a_i|\} \;;\;\text{ the max metric}$$ $$d_e(x,a)=\sqrt{\sum_{1\leq i \leq k}(x_i-a_i)^2} \;;\;\text{ the Euclidean metric}$$ $$d_t(x,a)={\sum_{1\leq i \leq k}|x_i-a_i|} \;;\;\text{ the so-called taxicab metric}$$ There is a nice characterization of a point $a$ being a limit of a sequence $\{ a_n\}$ which is Given a sequence of points $\{a_n\}$ in a metric space $(X,d)$ then $\lim a_n = a$ if and only if for every neighborhood $V$ of $a$, only but finitely many of the $a_n$ are not contained in $V$, or equivalently $a_n \in V$ for almost all of the $a_n$. More formally, one can say that for every nbhd $V$ of $a$ there is an $N$ such that whenever $n>N$, $a_n \in V$. This said, one can prove: THEOREM Let $\{a_n\}$ be a sequence of points in $\Bbb R^k$. Then if $\lim a_n=a$ under either one of the metrics $d_t,d_e,d_m$, it follows $\lim a_n=a$ for the other two. (NEW) PROOF We have that $$\tag a d_m(a,x) \leq d_e(a,x) \leq  \sqrt{k}\cdot d_m(a,x) $$ $$\tag b d_m(a,x) \leq d_t(a,x) \leq  {k}\cdot d_m(a,x) $$ Let $\lim a_n=a$ in $(\Bbb R^k,d_m)$. Let $T$,$E$ be neighborhoods of $a$ in $(\Bbb R^k,d_t)$ and $(\Bbb R^k,d_e)$ respectively. Then they contain an open $t$-ball and an open $e$-ball about $a$ for a $\delta>0$. $(a)$ and $(b)$ guarantee that each of these balls contain an open $m$-ball about $a$. But this ball $B_m$ is a neighborhood of $a$ in $(\Bbb R^k,d_m)$ so $a_n\in B_m$ whenever $n>N$ for a some $N$. This in turn means that $a_n \in B_t$ and $a_n \in B_e$ whenever $n>N$ so $\lim a_n=a$ in both the two other metrics. The other two cases follow analogously.",,"['limits', 'metric-spaces', 'proof-writing']"
30,Exercise: Continuity and Local Boundedness,Exercise: Continuity and Local Boundedness,,"Consider function $f:\mathbb{R}^n \setminus \{0\} \times \mathbb{R}^n \rightarrow \mathbb{R}_{> 0}$ that is: continuous in the first argument; locally bounded in the second argument. Consider a sequence $\{x_i\}_{i=1}^{\infty}$ such that $x_i \in \mathbb{R}^n$, $x_i \rightarrow x$. Prove that $$ \limsup_{i \rightarrow \infty} \ f(x,x_i) - f(x_i,x_i) \ = \ 0. $$","Consider function $f:\mathbb{R}^n \setminus \{0\} \times \mathbb{R}^n \rightarrow \mathbb{R}_{> 0}$ that is: continuous in the first argument; locally bounded in the second argument. Consider a sequence $\{x_i\}_{i=1}^{\infty}$ such that $x_i \in \mathbb{R}^n$, $x_i \rightarrow x$. Prove that $$ \limsup_{i \rightarrow \infty} \ f(x,x_i) - f(x_i,x_i) \ = \ 0. $$",,"['real-analysis', 'limits']"
31,another form of the L'Hospital's rule,another form of the L'Hospital's rule,,"I need some help, any Ideas? in L'Hospital's rule, replace the assumption that $\frac{f}{g}$ tends to $\frac{0}{0}$ with the assumption that it tends to $\frac{\infty}{\infty}$. if $\frac{f'}{g'}$ tends to $L$. prove that $\frac{f}{g}$ tends to $L$ also.","I need some help, any Ideas? in L'Hospital's rule, replace the assumption that $\frac{f}{g}$ tends to $\frac{0}{0}$ with the assumption that it tends to $\frac{\infty}{\infty}$. if $\frac{f'}{g'}$ tends to $L$. prove that $\frac{f}{g}$ tends to $L$ also.",,"['real-analysis', 'limits']"
32,"Am I missing something in the answer to Spivak's Calculus, problem 5-39(vi)?","Am I missing something in the answer to Spivak's Calculus, problem 5-39(vi)?",,"Am I missing something in the answer to Spivak's Calculus (4E), problem 5-39(vi)? Earlier, problem 5-39(v)(p. 113) has established that $$\mathop{\lim}\limits_{x \to \infty}\sqrt{x^{2}+2x}-x=1$$ and then problem 5-39(vi) asks for $$\mathop{\lim}\limits_{x \to \infty}x\left(\sqrt{x+2}-\sqrt{x}\right).$$ Spivak's key ( Combined Answer Book , p.78) determines this answer by going through several algebraic manipulations; but for $x>0$, $$x\left(\sqrt{x+2}-\sqrt{x}\right)=\sqrt{x}\left(\sqrt{x^{2}+2x}-x\right)$$ so that $$\mathop{\lim}\limits_{x \to \infty}x\left(\sqrt{x+2}-\sqrt{x}\right)=\left(\mathop{\lim}\limits_{x \to \infty}\sqrt{x}\right)\cdot1$$ can be determined by simply substituting the limit found in the preceding problem. Does taking this ""shortcut"" miss some required steps? I wouldn't ask, except that where Spivak can refer to the answer to a previous problem to save work and space, he nearly always does. When he goes to the trouble of working out all the steps it is usually to illustrate something that would otherwise have been missed. But it seems that here the thing to observe is that the previously found limit exists and can be used.","Am I missing something in the answer to Spivak's Calculus (4E), problem 5-39(vi)? Earlier, problem 5-39(v)(p. 113) has established that $$\mathop{\lim}\limits_{x \to \infty}\sqrt{x^{2}+2x}-x=1$$ and then problem 5-39(vi) asks for $$\mathop{\lim}\limits_{x \to \infty}x\left(\sqrt{x+2}-\sqrt{x}\right).$$ Spivak's key ( Combined Answer Book , p.78) determines this answer by going through several algebraic manipulations; but for $x>0$, $$x\left(\sqrt{x+2}-\sqrt{x}\right)=\sqrt{x}\left(\sqrt{x^{2}+2x}-x\right)$$ so that $$\mathop{\lim}\limits_{x \to \infty}x\left(\sqrt{x+2}-\sqrt{x}\right)=\left(\mathop{\lim}\limits_{x \to \infty}\sqrt{x}\right)\cdot1$$ can be determined by simply substituting the limit found in the preceding problem. Does taking this ""shortcut"" miss some required steps? I wouldn't ask, except that where Spivak can refer to the answer to a previous problem to save work and space, he nearly always does. When he goes to the trouble of working out all the steps it is usually to illustrate something that would otherwise have been missed. But it seems that here the thing to observe is that the previously found limit exists and can be used.",,"['calculus', 'limits']"
33,Problematic limit $\epsilon \to 0 $ for combination of hypergeometric ${_2}F_2$ functions,Problematic limit  for combination of hypergeometric  functions,\epsilon \to 0  {_2}F_2,"The bounty expires in 5 days . Answers to this question are eligible for a +100 reputation bounty. Semiclassical wants to draw more attention to this question. In an earlier question , the integral $$I_n(c)=\int_0^\infty x^n (1+x)^n e^{-n c x^2} dx$$ was considered with particular focus on its behavior for positive integer $n$ . In trying to analyze this, it appears that Mathematica runs into issues in taking certain limits. At risk of being a CAS-centric question, I will dispense with the original integral and instead focus on a specific example arising from it (namely, that of $n=c=1$ ). After extensive offline manipulation, the discrepancy arises as follows. Let $$f(\epsilon )={_2}F_2 \left(\begin{array}{c}1/2,-\epsilon/2\\ 3/2,-\epsilon \end{array}\Bigg{\vert} -1\right)+\frac16~ {_2}F_2 \left(\begin{array}{c}3/2,\epsilon/2\\ 5/2,2+\epsilon \end{array}\Bigg{\vert} -1\right)$$ (Here I've let $\epsilon=n-1$ , so as to focus on a zero limit.) Empirically (i.e., having Mathematica compute for $\epsilon \approx 0$ ) it appears that $f(0)\to 1$ as $\epsilon \to 0$ . This is also consistent with the original integral, which can be computed by elementary means in the case of $n=1$ (though verifying $f(0)=1$ by this route requires the extensive manipulations alluded to earlier). However, if Mathematica directly evaluates $f(0)$ it instead obtains $$f(0)\underset{?}{=}\frac{3}{2}-\frac{\sqrt{\pi}}{4}\operatorname{erf}(1)\approx 1.12659$$ where $\text{erf}(x)$ is the error function. So for some reason Mathematica behaves as though $f(\epsilon)$ is well-defined but discontinuous at $\epsilon=0$ . What I haven't been able to suss out is if this is a software issue or something more substantive. The limit is not entirely trivial, since when $\epsilon \to 0$ the hypergeometric function picks up integer parameters which can introduce complications. So what I'm looking for is a direct proof that $f(0)=1$ , and hopefully some means of clarifying Mathematica's error.","The bounty expires in 5 days . Answers to this question are eligible for a +100 reputation bounty. Semiclassical wants to draw more attention to this question. In an earlier question , the integral was considered with particular focus on its behavior for positive integer . In trying to analyze this, it appears that Mathematica runs into issues in taking certain limits. At risk of being a CAS-centric question, I will dispense with the original integral and instead focus on a specific example arising from it (namely, that of ). After extensive offline manipulation, the discrepancy arises as follows. Let (Here I've let , so as to focus on a zero limit.) Empirically (i.e., having Mathematica compute for ) it appears that as . This is also consistent with the original integral, which can be computed by elementary means in the case of (though verifying by this route requires the extensive manipulations alluded to earlier). However, if Mathematica directly evaluates it instead obtains where is the error function. So for some reason Mathematica behaves as though is well-defined but discontinuous at . What I haven't been able to suss out is if this is a software issue or something more substantive. The limit is not entirely trivial, since when the hypergeometric function picks up integer parameters which can introduce complications. So what I'm looking for is a direct proof that , and hopefully some means of clarifying Mathematica's error.","I_n(c)=\int_0^\infty x^n (1+x)^n e^{-n c x^2} dx n n=c=1 f(\epsilon )={_2}F_2 \left(\begin{array}{c}1/2,-\epsilon/2\\ 3/2,-\epsilon \end{array}\Bigg{\vert} -1\right)+\frac16~ {_2}F_2 \left(\begin{array}{c}3/2,\epsilon/2\\ 5/2,2+\epsilon \end{array}\Bigg{\vert} -1\right) \epsilon=n-1 \epsilon \approx 0 f(0)\to 1 \epsilon \to 0 n=1 f(0)=1 f(0) f(0)\underset{?}{=}\frac{3}{2}-\frac{\sqrt{\pi}}{4}\operatorname{erf}(1)\approx 1.12659 \text{erf}(x) f(\epsilon) \epsilon=0 \epsilon \to 0 f(0)=1","['calculus', 'limits', 'summation', 'hypergeometric-function', 'pochhammer-symbol']"
34,Is it possible to evaluate $ \int_{0}^{1} \frac{1}{t^4 \sqrt{t^2-25}} dt $,Is it possible to evaluate, \int_{0}^{1} \frac{1}{t^4 \sqrt{t^2-25}} dt ,"This question was in my Calc 2 final and I was stumped with these limits. First, I'm confused since $f(t)$ isn't defined for $-5 \leq t \leq 5$ . My teacher suggested I treat it as an improper integral, but I'm not sure that works out. What I tried was to find the antiderivative first by trig substitution: Consider $\int\frac{1}{t^4 \sqrt{t^2-25}} dt$ If $\sqrt{t^2-25}=5\tan \theta$ , $t=5\sec\theta$ and $dt=\sec \theta \tan \theta d \theta$ We have $\int\frac{5\sec \theta \tan \theta}{(5\sec \theta)^4 5 \sec \theta} d\theta = \frac{1}{5^4}\ \int\frac{1}{\sec^3 \theta} d \theta$ With $u$ -sub and trig identities, we get $=\frac{1}{5^4}\ [\sin \theta - \frac{1}{3}\sin^3 \theta] $ In terms of $t$ : $= \frac{1}{5^4}\ [ \frac{\sqrt{t^2-25}}{t}\ - \frac{1}{3}\ (\frac{\sqrt{t^2-25}}{t}\ )^3 ]$ Back to our integral: $\int_{0}^{1} \frac{1}{t^4 \sqrt{t^2-25}} dt = \frac{1}{5^4}\ [ \frac{\sqrt{t^2-25}}{t}\ - \frac{1}{3}\ (\frac{\sqrt{t^2-25}}{t}\ )^3 ]^1_0 $ And of course I run into the same issue of having a division by zero. If I take the limit, then I find it diverges. Looking forward to comments! Please let me know if I'm making mistakes or conceptual errors. Thanks!","This question was in my Calc 2 final and I was stumped with these limits. First, I'm confused since isn't defined for . My teacher suggested I treat it as an improper integral, but I'm not sure that works out. What I tried was to find the antiderivative first by trig substitution: Consider If , and We have With -sub and trig identities, we get In terms of : Back to our integral: And of course I run into the same issue of having a division by zero. If I take the limit, then I find it diverges. Looking forward to comments! Please let me know if I'm making mistakes or conceptual errors. Thanks!",f(t) -5 \leq t \leq 5 \int\frac{1}{t^4 \sqrt{t^2-25}} dt \sqrt{t^2-25}=5\tan \theta t=5\sec\theta dt=\sec \theta \tan \theta d \theta \int\frac{5\sec \theta \tan \theta}{(5\sec \theta)^4 5 \sec \theta} d\theta = \frac{1}{5^4}\ \int\frac{1}{\sec^3 \theta} d \theta u =\frac{1}{5^4}\ [\sin \theta - \frac{1}{3}\sin^3 \theta]  t = \frac{1}{5^4}\ [ \frac{\sqrt{t^2-25}}{t}\ - \frac{1}{3}\ (\frac{\sqrt{t^2-25}}{t}\ )^3 ] \int_{0}^{1} \frac{1}{t^4 \sqrt{t^2-25}} dt = \frac{1}{5^4}\ [ \frac{\sqrt{t^2-25}}{t}\ - \frac{1}{3}\ (\frac{\sqrt{t^2-25}}{t}\ )^3 ]^1_0 ,"['calculus', 'integration', 'limits', 'definite-integrals', 'trigonometric-integrals']"
35,Finding $\liminf n (\sin n)^2$,Finding,\liminf n (\sin n)^2,"I'm solving an exercise from The elements of Real Analysis by Robert G. Bartle, which asks to find the $\limsup$ and $\liminf$ of the sequence given by $a_n = n (\sin n)^2$ . I have already calculated $\limsup a_n$ as follows: Since $\left\lbrace\sin n : n\in\mathbb{N}\right\rbrace$ is dense in $[-1, 1]$ , and $\sin n \neq 1$ for every $n\in\mathbb{N}$ , there are infinite many numbers $m\in\mathbb{N}$ such that $\sin m \geq \frac{9}{10}$ , and therefore, $m(\sin m)^2\geq \frac{81}{100}m$ for infinite many natural numbers. This implies $a_n$ is not bounded above, so $\limsup a_n = +\infty$ . I was trying to apply something similar to find $\liminf a_n$ , but haven't been able to get any conclusion. Can you help me with that, please?","I'm solving an exercise from The elements of Real Analysis by Robert G. Bartle, which asks to find the and of the sequence given by . I have already calculated as follows: Since is dense in , and for every , there are infinite many numbers such that , and therefore, for infinite many natural numbers. This implies is not bounded above, so . I was trying to apply something similar to find , but haven't been able to get any conclusion. Can you help me with that, please?","\limsup \liminf a_n = n (\sin n)^2 \limsup a_n \left\lbrace\sin n : n\in\mathbb{N}\right\rbrace [-1, 1] \sin n \neq 1 n\in\mathbb{N} m\in\mathbb{N} \sin m \geq \frac{9}{10} m(\sin m)^2\geq \frac{81}{100}m a_n \limsup a_n = +\infty \liminf a_n","['real-analysis', 'limits', 'limsup-and-liminf']"
36,Explain the continuity of the function $f$ defined by $f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} (0\leq x\leq \pi/\sqrt2)$ at $x = 1$. [closed],Explain the continuity of the function  defined by  at . [closed],f f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} (0\leq x\leq \pi/\sqrt2) x = 1,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . Improve this question Explain for continuity the function $f$ defined by $f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} (0\leq x\leq \pi/ \sqrt2)$ at $x = 1$ . Explain why the function $f$ does not vanish anywhere in $[0, \pi/\sqrt2]$ although $f(0)f(\pi/\sqrt2) <0$ . My attemt If $0 \leq x < 1$ then $x^n$ $\to$ 0 therefore $f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} = e^x$ . If $ x > 1$ then $x^n \to ∞$ therefore $f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} = -\sin(nx)$ I don't know how to proceed further please help me",Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . Improve this question Explain for continuity the function defined by at . Explain why the function does not vanish anywhere in although . My attemt If then 0 therefore . If then therefore I don't know how to proceed further please help me,"f f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} (0\leq x\leq \pi/ \sqrt2) x = 1 f [0, \pi/\sqrt2] f(0)f(\pi/\sqrt2) <0 0 \leq x < 1 x^n \to f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} = e^x  x > 1 x^n \to ∞ f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} = -\sin(nx)","['limits', 'continuity']"
37,"Determining limit of multivariable function $ \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-\sin(2y)}{\cos(x)\cos(y)}$",Determining limit of multivariable function," \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-\sin(2y)}{\cos(x)\cos(y)}","Evaluate $ \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-\sin(2y)}{\cos(x)\cos(y)}$ So far my steps have included: $ = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-2\sin(y)\cos(y)}{\cos(x)\cos(y)}$ using the double angle identity $ = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)(1-2\sin(y))}{\cos(x)\cos(y)} = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{1-2\sin(y)}{\cos(x)}$ , which allows us to take the limit at the point since the point exists within the domain, so: $ = \frac{1-2\sin(\frac{\pi}{2})}{\cos(\pi)} = \frac{-1}{-1} = 1$ There is no solution included in my textbook so I was hoping someone might verify. I was also wondering if this solution was sufficient or if there were further steps to evaluate the limit, since other questions require taking different paths to confirm the limit existed from every direction. Would this be required here? Thank you in advance!","Evaluate So far my steps have included: using the double angle identity , which allows us to take the limit at the point since the point exists within the domain, so: There is no solution included in my textbook so I was hoping someone might verify. I was also wondering if this solution was sufficient or if there were further steps to evaluate the limit, since other questions require taking different paths to confirm the limit existed from every direction. Would this be required here? Thank you in advance!"," \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-\sin(2y)}{\cos(x)\cos(y)}  = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-2\sin(y)\cos(y)}{\cos(x)\cos(y)}  = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)(1-2\sin(y))}{\cos(x)\cos(y)} = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{1-2\sin(y)}{\cos(x)}  = \frac{1-2\sin(\frac{\pi}{2})}{\cos(\pi)} = \frac{-1}{-1} = 1","['calculus', 'limits', 'multivariable-calculus', 'epsilon-delta']"
38,How can i evaluate the product? [closed],How can i evaluate the product? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . Improve this question I have to evaluate the following product: $$\prod_{n=3}^{\infty} \frac{(n^3+3n)^2}{n^6-64} $$ But I couldn't develop anything. I thought about breaking the product into smaller parts with some factoring, but I couldn't calculate those products.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . Improve this question I have to evaluate the following product: But I couldn't develop anything. I thought about breaking the product into smaller parts with some factoring, but I couldn't calculate those products.",\prod_{n=3}^{\infty} \frac{(n^3+3n)^2}{n^6-64} ,"['calculus', 'limits', 'infinity', 'products']"
39,Under what conditions is $\lim_{x\to a}\left|\varphi\circ f(x)-\tau \circ g(x)\right|=0$ true?,Under what conditions is  true?,\lim_{x\to a}\left|\varphi\circ f(x)-\tau \circ g(x)\right|=0,"This question is inspired from another much easier problem I was trying to solve which I tried to generalize. The question is essentially as follows (assuming all the limits exist) If $a\in \mathbb R\cup \{\infty\}$ , and $f$ and $g$ are two functions such that $$\lim_{x\to a}\left|f(x)-g(x)\right|=0$$ and $\varphi$ and $\tau$ are functions such that $$\lim_{x\to \lim_{t\to a}f(t)}\left|\varphi (x)-\tau (x)\right|=0$$ then what is the least amount of conditions necessary to impose so that $$\lim_{x\to a}\left|\varphi\circ f(x)-\tau \circ g(x)\right|=0$$ can be concluded? What about the special case $f=\varphi$ and $g=\tau$ ? In the original problem I had, I was in the special case, and dealing with $a=\infty$ which is why I was only considering those cases. I was under the impression that the continuity of $g$ and the extra condition that $$\lim_{x\to \infty} |f(x)|=\infty$$ would be enough. But, I soon arrived at a counterexample: construct $f$ and $g$ such that $$\lim_{x\to \infty} f(x)= \lim_{x\to \infty} g(x) = -\infty$$ but $$\lim_{x\to -\infty} f(x) = 1,\quad \lim_{x\to -\infty} g(x) = 2$$ which shattered my hopes. Also, it seems strange that there is very little (easily available) literature on such questions about composition of functions. So, does anyone know of any such result that deals with such a problem? Link to MO post: https://mathoverflow.net/q/451283/311366","This question is inspired from another much easier problem I was trying to solve which I tried to generalize. The question is essentially as follows (assuming all the limits exist) If , and and are two functions such that and and are functions such that then what is the least amount of conditions necessary to impose so that can be concluded? What about the special case and ? In the original problem I had, I was in the special case, and dealing with which is why I was only considering those cases. I was under the impression that the continuity of and the extra condition that would be enough. But, I soon arrived at a counterexample: construct and such that but which shattered my hopes. Also, it seems strange that there is very little (easily available) literature on such questions about composition of functions. So, does anyone know of any such result that deals with such a problem? Link to MO post: https://mathoverflow.net/q/451283/311366","a\in \mathbb R\cup \{\infty\} f g \lim_{x\to a}\left|f(x)-g(x)\right|=0 \varphi \tau \lim_{x\to \lim_{t\to a}f(t)}\left|\varphi (x)-\tau (x)\right|=0 \lim_{x\to a}\left|\varphi\circ f(x)-\tau \circ g(x)\right|=0 f=\varphi g=\tau a=\infty g \lim_{x\to \infty} |f(x)|=\infty f g \lim_{x\to \infty} f(x)= \lim_{x\to \infty} g(x) = -\infty \lim_{x\to -\infty} f(x) = 1,\quad \lim_{x\to -\infty} g(x) = 2","['calculus', 'limits', 'analysis', 'functions', 'function-and-relation-composition']"
40,Proof of the Riemann curvature endomorphism as parallel transport around an infinitesimal loop(Thm 7.11 of Lee's Introduction to Riemannian Manifolds),Proof of the Riemann curvature endomorphism as parallel transport around an infinitesimal loop(Thm 7.11 of Lee's Introduction to Riemannian Manifolds),,"$\def\bbR{\mathbb{R}}$ I have looked for a while about this in MSE and I couldn't find anything. Here it goes: In J. M. Lee, Introduction to Riemannian Manifolds , 2nd ed., there is this result: I wanted to ask about Lee's proof of Theorem 7.11. For each $\delta,\varepsilon\in\bbR\setminus\{0\}$ and $z\in T_pM$ , denote the limiting term in the RHS of (7.10) as $$ F_{\delta,\varepsilon}(z)=\frac{P_{\delta, 0}^{0,0} \circ P_{\delta, \varepsilon}^{\delta, 0} \circ P_{0, \varepsilon}^{\delta, \varepsilon} \circ P_{0,0}^{0, \varepsilon}(z)-z}{\delta \varepsilon}. $$ What Lee's proof actually does is showing that the following iterated limit formula holds: $$ R(x,y)z= \lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z). $$ My question is: how does one show then that $\displaystyle\lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)=\lim_{\delta,\varepsilon\to 0}F_{\delta,\varepsilon}(z)$ ? Looking ""iterated limits"" in google brought me to this : I am trying to check the hypotheses for our situation. On the one hand, from Lee's proof one has that $\displaystyle\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)$ exists. But on the other hand, I don't know how to show existence of $\displaystyle\lim_{\delta\to 0}F_{\delta,\varepsilon}(z)$ , nor how to argue that of among the limits $\displaystyle\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)$ and $\displaystyle\lim_{\delta\to 0}F_{\delta,\varepsilon}(z)$ , at least one converges uniformly. My questions are: How one would verify the other hypothesis of the theorem on interchange of two limits? Is there any different approach to this auxiliar theorem to show that $\displaystyle\lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)=\lim_{\delta,\varepsilon\to 0}F_{\delta,\varepsilon}(z)$ ? In case it helps, here is the expression from Lee's proof for the limit in $\varepsilon$ : $$ \lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z) = \frac{P_{\delta, 0}^{0,0}\left(D_t Z(\delta, 0)\right)-D_t Z(0,0)}{\delta}, $$ where $D_t$ is the covariant derivative along the curve $t\mapsto\Gamma(s,t)$ (where $s=\delta$ on the first case and $s=0$ in the second case) and $Z$ is the vector field along $\Gamma$ (i.e., a lifting to $TM\to M$ of the map $\Gamma$ ) explained at the beggining of Lee's proof: Define a vector field $Z$ along $\Gamma$ by first parallel transporting $z$ along the curve $t\mapsto\Gamma(0,t)$ , and then for each $t$ , parallel transporting $Z(0,t)$ along the curve $s\mapsto \Gamma(s,t)$ . The resulting vector field along $\Gamma$ is smooth by another application of Theorem A.42 (the fundamental theorem on flows) as in the proof of Lemma 7.8. In the quote I have incorporated the correction to p. 201.","I have looked for a while about this in MSE and I couldn't find anything. Here it goes: In J. M. Lee, Introduction to Riemannian Manifolds , 2nd ed., there is this result: I wanted to ask about Lee's proof of Theorem 7.11. For each and , denote the limiting term in the RHS of (7.10) as What Lee's proof actually does is showing that the following iterated limit formula holds: My question is: how does one show then that ? Looking ""iterated limits"" in google brought me to this : I am trying to check the hypotheses for our situation. On the one hand, from Lee's proof one has that exists. But on the other hand, I don't know how to show existence of , nor how to argue that of among the limits and , at least one converges uniformly. My questions are: How one would verify the other hypothesis of the theorem on interchange of two limits? Is there any different approach to this auxiliar theorem to show that ? In case it helps, here is the expression from Lee's proof for the limit in : where is the covariant derivative along the curve (where on the first case and in the second case) and is the vector field along (i.e., a lifting to of the map ) explained at the beggining of Lee's proof: Define a vector field along by first parallel transporting along the curve , and then for each , parallel transporting along the curve . The resulting vector field along is smooth by another application of Theorem A.42 (the fundamental theorem on flows) as in the proof of Lemma 7.8. In the quote I have incorporated the correction to p. 201.","\def\bbR{\mathbb{R}} \delta,\varepsilon\in\bbR\setminus\{0\} z\in T_pM 
F_{\delta,\varepsilon}(z)=\frac{P_{\delta, 0}^{0,0} \circ P_{\delta, \varepsilon}^{\delta, 0} \circ P_{0, \varepsilon}^{\delta, \varepsilon} \circ P_{0,0}^{0, \varepsilon}(z)-z}{\delta \varepsilon}.
 
R(x,y)z= \lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z).
 \displaystyle\lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)=\lim_{\delta,\varepsilon\to 0}F_{\delta,\varepsilon}(z) \displaystyle\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z) \displaystyle\lim_{\delta\to 0}F_{\delta,\varepsilon}(z) \displaystyle\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z) \displaystyle\lim_{\delta\to 0}F_{\delta,\varepsilon}(z) \displaystyle\lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)=\lim_{\delta,\varepsilon\to 0}F_{\delta,\varepsilon}(z) \varepsilon 
\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)
=
\frac{P_{\delta, 0}^{0,0}\left(D_t Z(\delta, 0)\right)-D_t Z(0,0)}{\delta},
 D_t t\mapsto\Gamma(s,t) s=\delta s=0 Z \Gamma TM\to M \Gamma Z \Gamma z t\mapsto\Gamma(0,t) t Z(0,t) s\mapsto \Gamma(s,t) \Gamma","['limits', 'differential-geometry', 'riemannian-geometry', 'curvature', 'holonomy']"
41,Finding functions that satisfy a condition,Finding functions that satisfy a condition,,"Let $(a_n)_n$ be a positive numbers sequence such that $$ \lim_{n\to\infty} a_n =0 $$ Find the functions $f :R \to R$ that admit primitives such that $$2f(x)=f(x+a_n)+f(x-a_n)$$ for every x $\in R$ and n $\in N$ I've been struggling for a while with this problem. I've noticed that the linear functions satisfy the conditions but I don't know how I might prove that there aren't other functions. My instinct was to first prove that $f$ is monotonous, since  the existence of primitives ensures that the function has Darboux, but I didn't have any success.","Let be a positive numbers sequence such that Find the functions that admit primitives such that for every x and n I've been struggling for a while with this problem. I've noticed that the linear functions satisfy the conditions but I don't know how I might prove that there aren't other functions. My instinct was to first prove that is monotonous, since  the existence of primitives ensures that the function has Darboux, but I didn't have any success.",(a_n)_n  \lim_{n\to\infty} a_n =0  f :R \to R 2f(x)=f(x+a_n)+f(x-a_n) \in R \in N f,"['real-analysis', 'limits', 'indefinite-integrals']"
42,Closed form of an averaging series,Closed form of an averaging series,,"Consider the expression $$ f(\mathbf{x},n)=\sum_{k=0}^{n}  \frac{e^{-k^2\overline{x_k}}-e^{-(k+1)^2\overline{x_k}}}{(2k+1)\overline{x_k}} $$ where $\mathbf{x}=[x_{-n},\cdots,x_{-2},x_{-1},x_0,x_1,x_2,\cdots,x_{n}]^T$ and $$ \overline{x_k}=\frac{1}{2k+1}\sum_{|i|\leq k} x_i $$ with $0<x_i<\infty$ , for all $i\in\{-n,...,-1,0,1,...,n\}$ . What is $\lim_{n\to\infty}f(\mathbf{x},n)$ ? If not a closed formula, is it possible to find an upper bound? My attempt: Following @metamorphy's answer presented here , for the case where $x_i\equiv x,\forall i$ , and given that $\{x_i\}$ is bounded, one would hope there exists $k^*$ such that $$ f(\mathbf{x},\infty)=\sum_{k=0}^{\infty}  \frac{e^{-k^2\overline{x_k}}-e^{-(k+1)^2\overline{x_k}}}{(2k+1)\overline{x_k}}\simeq \frac{1}{\overline{x_{k^*}}} \sum_{k=0}^{\infty}  \frac{e^{-k^2\overline{x_{k^*}}}-e^{-(k+1)^2\overline{x_{k^*}}}}{2k+1} $$ which, following the answer to the linked question, leads to the local approximation $$ f(\mathbf{x},\infty)\simeq \frac12\sqrt{\frac{\pi}{\overline{x_{k^*}}}} $$ However, in simulations, this does not seem to work for numerical attempts on various values of $k^*$ . Nonetheless, I expect an approximation like this to be possible. Just to better visualize the problem, here is the plot of $f(\mathbf{x},n)$ , where the ""data"" vector $\mathbf{x}$ was generated via a random walk with very small increments ( $10^{-5}$ ) and $x_i\in(0,0.1)$ The goal is to approximate the blue curve. Any ideas? Comment on $\mathbf{x}$ : The randomness of $\mathbf{x}$ might difficult finding a potential solution and make this problem way too broad. Therefore, we may take it to behave ""continuously"" enough. For example, it can be taken to be a fine discretisation of a continuous function, as sketched here Naturally the answer should be $\mathbf{x}$ -dependent, but I wonder if it is possible to still make an assertion on an approximation to $f$ , given, for example, a local mean approximation, as seen, for some $k^*$ , Where $\overline{x_n}$ is the overall mean. Note that, for each $k$ , the mean changes little, and we may also assume $$ |\overline{x_k}-\overline{x_{k-1}}|<\epsilon $$ for some small $\epsilon$ .","Consider the expression where and with , for all . What is ? If not a closed formula, is it possible to find an upper bound? My attempt: Following @metamorphy's answer presented here , for the case where , and given that is bounded, one would hope there exists such that which, following the answer to the linked question, leads to the local approximation However, in simulations, this does not seem to work for numerical attempts on various values of . Nonetheless, I expect an approximation like this to be possible. Just to better visualize the problem, here is the plot of , where the ""data"" vector was generated via a random walk with very small increments ( ) and The goal is to approximate the blue curve. Any ideas? Comment on : The randomness of might difficult finding a potential solution and make this problem way too broad. Therefore, we may take it to behave ""continuously"" enough. For example, it can be taken to be a fine discretisation of a continuous function, as sketched here Naturally the answer should be -dependent, but I wonder if it is possible to still make an assertion on an approximation to , given, for example, a local mean approximation, as seen, for some , Where is the overall mean. Note that, for each , the mean changes little, and we may also assume for some small .","
f(\mathbf{x},n)=\sum_{k=0}^{n}  \frac{e^{-k^2\overline{x_k}}-e^{-(k+1)^2\overline{x_k}}}{(2k+1)\overline{x_k}}
 \mathbf{x}=[x_{-n},\cdots,x_{-2},x_{-1},x_0,x_1,x_2,\cdots,x_{n}]^T 
\overline{x_k}=\frac{1}{2k+1}\sum_{|i|\leq k} x_i
 0<x_i<\infty i\in\{-n,...,-1,0,1,...,n\} \lim_{n\to\infty}f(\mathbf{x},n) x_i\equiv x,\forall i \{x_i\} k^* 
f(\mathbf{x},\infty)=\sum_{k=0}^{\infty}  \frac{e^{-k^2\overline{x_k}}-e^{-(k+1)^2\overline{x_k}}}{(2k+1)\overline{x_k}}\simeq \frac{1}{\overline{x_{k^*}}} \sum_{k=0}^{\infty}  \frac{e^{-k^2\overline{x_{k^*}}}-e^{-(k+1)^2\overline{x_{k^*}}}}{2k+1}
 
f(\mathbf{x},\infty)\simeq \frac12\sqrt{\frac{\pi}{\overline{x_{k^*}}}}
 k^* f(\mathbf{x},n) \mathbf{x} 10^{-5} x_i\in(0,0.1) \mathbf{x} \mathbf{x} \mathbf{x} f k^* \overline{x_n} k 
|\overline{x_k}-\overline{x_{k-1}}|<\epsilon
 \epsilon","['sequences-and-series', 'limits', 'logarithms', 'exponential-function', 'upper-lower-bounds']"
43,Find the limit of $x^2 +xc + d$ when $x$ goes to $0^{-}$ using the $\varepsilon-\delta$ definition of a limit,Find the limit of  when  goes to  using the  definition of a limit,x^2 +xc + d x 0^{-} \varepsilon-\delta,"I want to show that $$ \lim\limits_{x\to 0^{-}} (x^2+cx+d) = d $$ Here is my attempt: We want to show that $$ \forall\varepsilon>0,\exists\delta>0 : -\delta\leq x\leq 0\implies\left\lvert x^2+cx + d - d\right\rvert = \left\lvert x^2 +cx\right\rvert \leq\varepsilon \tag{1}\label{1} $$ Fix an $\varepsilon>0$ and put $\delta =  \min\left(1,\frac{\varepsilon}{1+\left\lvert c\right\rvert}\right)$ . By the triangle inequality we have for $x\in[-\delta,0)$ : $$ \left\lvert x^2 +cx\right\rvert\leq \left\lvert x\right\rvert^2 +\left\lvert c\right\rvert\left\lvert x\right\rvert\leq \delta^2 +\left\lvert c\right\rvert\delta\leq\delta\left(1 + \left\lvert c\right\rvert\right) = \frac{\varepsilon(1+\lvert c\rvert)}{1+\lvert c\rvert} = \varepsilon \tag{2}\label{2} $$ The minimum here ensures us that when $\varepsilon$ is ""big"" $($ namely $\varepsilon > 1+\left\lvert c\right\rvert)$ we still have $\eqref{2}$ which is satisfied. EDIT : Many thanks to Surb for the correction et Nerrit for improving the format of my question.","I want to show that Here is my attempt: We want to show that Fix an and put . By the triangle inequality we have for : The minimum here ensures us that when is ""big"" namely we still have which is satisfied. EDIT : Many thanks to Surb for the correction et Nerrit for improving the format of my question.","
\lim\limits_{x\to 0^{-}} (x^2+cx+d) = d
 
\forall\varepsilon>0,\exists\delta>0 : -\delta\leq x\leq 0\implies\left\lvert x^2+cx + d - d\right\rvert = \left\lvert x^2 +cx\right\rvert \leq\varepsilon \tag{1}\label{1}
 \varepsilon>0 \delta =  \min\left(1,\frac{\varepsilon}{1+\left\lvert c\right\rvert}\right) x\in[-\delta,0) 
\left\lvert x^2 +cx\right\rvert\leq \left\lvert x\right\rvert^2 +\left\lvert c\right\rvert\left\lvert x\right\rvert\leq \delta^2 +\left\lvert c\right\rvert\delta\leq\delta\left(1 + \left\lvert c\right\rvert\right) = \frac{\varepsilon(1+\lvert c\rvert)}{1+\lvert c\rvert} = \varepsilon \tag{2}\label{2}
 \varepsilon ( \varepsilon > 1+\left\lvert c\right\rvert) \eqref{2}","['calculus', 'limits', 'analysis', 'solution-verification', 'continuity']"
44,Solve $\lim_{x\to0^+} \frac{(1+\sin x)^{\frac1x} - \exp(-\frac x2)}{(\tan x )^\alpha}$,Solve,\lim_{x\to0^+} \frac{(1+\sin x)^{\frac1x} - \exp(-\frac x2)}{(\tan x )^\alpha},Is this limit solved correctly? $$\lim_{x\to0^+} \frac{(1+\sin x)^{\frac1x} - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+\sin x)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+x+o(x^2)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\left(\left[x+o(x^2)\right]-\frac12\left[x+o(x^2)\right]^2 + o(x^2)\right)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\left[x-\frac{x^2}{2} + o(x^2)\right]\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(1-\frac{x}{2} + o(x)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp(1)\cdot\exp\left(-\frac{x}{2}\right)\cdot\exp(o(x)) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(-\frac{x}{2}\right)\cdot\left(e\cdot\exp(o(x)) - 1\right)}{x^\alpha} = \frac{1\cdot(e\cdot1 -1)}{\lim_{x\to0^+}x^\alpha} = \begin{cases}\frac{e-1}{0^+} = +\infty && \ \text{if   } \alpha >0\\\frac{e-1}{1} = e-1 &&\ \text{if } \alpha =0\\\frac{e-1}{+\infty} = 0 &&\ \text{if } \alpha <0\end{cases}$$ EDIT: A first-order approximation is enough: $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+x+o(x^2)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\left[x+o(x^2)+o(x)\right]\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(1+o(1)\right) - \exp(-\frac x2)}{(\tan x )^\alpha} = \frac{e-1}{\lim_{x\to0^+}x^\alpha} = \begin{cases}\frac{e-1}{0^+} = +\infty && \ \text{if   } \alpha >0\\\frac{e-1}{1} = e-1 &&\ \text{if } \alpha =0\\\frac{e-1}{+\infty} = 0 &&\ \text{if } \alpha <0\end{cases}$$,Is this limit solved correctly? EDIT: A first-order approximation is enough:,\lim_{x\to0^+} \frac{(1+\sin x)^{\frac1x} - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+\sin x)\right) - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+x+o(x^2)\right) - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp\left(\frac1x\left(\left[x+o(x^2)\right]-\frac12\left[x+o(x^2)\right]^2 + o(x^2)\right)\right) - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp\left(\frac1x\left[x-\frac{x^2}{2} + o(x^2)\right]\right) - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp\left(1-\frac{x}{2} + o(x)\right) - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp(1)\cdot\exp\left(-\frac{x}{2}\right)\cdot\exp(o(x)) - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp\left(-\frac{x}{2}\right)\cdot\left(e\cdot\exp(o(x)) - 1\right)}{x^\alpha} = \frac{1\cdot(e\cdot1 -1)}{\lim_{x\to0^+}x^\alpha} = \begin{cases}\frac{e-1}{0^+} = +\infty && \ \text{if   } \alpha >0\\\frac{e-1}{1} = e-1 &&\ \text{if } \alpha =0\\\frac{e-1}{+\infty} = 0 &&\ \text{if } \alpha <0\end{cases} \lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+x+o(x^2)\right) - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp\left(\frac1x\left[x+o(x^2)+o(x)\right]\right) - \exp(-\frac x2)}{(\tan x )^\alpha} \lim_{x\to0^+} \frac{\exp\left(1+o(1)\right) - \exp(-\frac x2)}{(\tan x )^\alpha} = \frac{e-1}{\lim_{x\to0^+}x^\alpha} = \begin{cases}\frac{e-1}{0^+} = +\infty && \ \text{if   } \alpha >0\\\frac{e-1}{1} = e-1 &&\ \text{if } \alpha =0\\\frac{e-1}{+\infty} = 0 &&\ \text{if } \alpha <0\end{cases},"['limits', 'taylor-expansion']"
45,On the explicit series expansion for the inverse exponential/logarithmic integral li$(x)$ and Ei$(x)$,On the explicit series expansion for the inverse exponential/logarithmic integral li and Ei,(x) (x),"Here is a possible explicit series expansion for an inverse of the Exponential Integral function $\text{Ei}(x)$ and logarithmic integral li $(x)$ . It uses the $n$ th derivative formula of Inverse Gamma regularized $\frac{d^n Q^{-1}(a,z)}{dz^n}$ . From Inverse function of li $(x)$ , we know that Ei $\displaystyle\left(-\lim_{a\to0}Q^{-1}(a,-ay)\right)=y<0$ Here is a Taylor series formula about $z=Q(a,x)$ with Gamma Regularized , the Pochhammer Symbol $(u)_v$ , and Kronecker delta $\delta_n$ and $\delta_{n,m}$ : $$Q^{-1}(a,z)=\sum_{n=0}^\infty\frac{(z-Q(a,x))^n}{n!}\left(x\delta_n+\left(-\frac{\Gamma(a)e^x}{x^{a-1}}\right)^n\sum_{j_2=0}^n\cdots\sum_{j_n=0}^n(-1)^{\sum\limits_{m=2}^nj_m}\delta_{\sum\limits_{m=2}^n (m-1)j_m,n-1}\Gamma\left(n+\sum_{m=2}^n j_m\right)\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^x x^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,x)\right)^{j_m}\right)\tag1$$ We have to use the above Taylor series for this branch about $z=\text{Ei}(x)$ : $$\text{Ei}^{-1}(z)=x+xe^{-x}(z-\text{Ei}(x))+\frac14e^{-2x} (x-x^2) (z-\text{Ei}(x))^2+e^{-3x}\left(\frac{x^3}3-\frac{2x^2}3+\frac x6\right)(z-\text{Ei}(x))^3+e^{-4x}\left(\frac x{24}-\frac{x^4}4+\frac{3x^3}4-\frac{11x^2}{24}\right)(z-\text{Ei}(x))^4+…$$ shown by this special case . Now we use the formula: $$\text{Ei}^{-1}(x)=-\lim_{a\to0}Q^{-1}(a,-ax)=-\lim_{a\to0}\sum_{n=0}^\infty\frac{(z-\text{Ei}(x))^n}{n!}\left(-ax\delta_n+\left(-\frac{\Gamma(a)e^{-ax}}{(-ax)^{a-1}}\right)^n\sum_{j_2=0}^n\cdots\sum_{j_n=0}^n(-1)^{\sum\limits_{m=2}^nj_m}\delta_{\sum\limits_{m=2}^2 (m-1)j_m,n-1}\Gamma\left(n+\sum_{m=2}^n j_m\right)\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^{-ax} (-ax)^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,-ax)\right)^{j_m}\right)$$ Clearly $-ax\delta_n\to0,-\frac{\Gamma(a)e^{-ax}}{(-ax)^{a-1}}\to x$ , but what is $$\lim_{a\to0}\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^{-ax} (-ax)^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,-ax)\right)^{j_m}?$$ It may help that the explicit formula looks like a special case of the explicit series reversion formula $$$$ Maybe we substituted $Q^{-1}(a,z)\to -\lim\limits_{a\to0}Q^{-1}(a,-az)$ incorrectly. What is the correct way to use $(1)$ to expand $ \text{Ei}^{-1}(z)=-\lim\limits_{a\to0}Q^{-1}(a,-az)$ and what is its radius of convergence?","Here is a possible explicit series expansion for an inverse of the Exponential Integral function and logarithmic integral li . It uses the th derivative formula of Inverse Gamma regularized . From Inverse function of li , we know that Ei Here is a Taylor series formula about with Gamma Regularized , the Pochhammer Symbol , and Kronecker delta and : We have to use the above Taylor series for this branch about : shown by this special case . Now we use the formula: Clearly , but what is It may help that the explicit formula looks like a special case of the explicit series reversion formula Maybe we substituted incorrectly. What is the correct way to use to expand and what is its radius of convergence?","\text{Ei}(x) (x) n \frac{d^n Q^{-1}(a,z)}{dz^n} (x) \displaystyle\left(-\lim_{a\to0}Q^{-1}(a,-ay)\right)=y<0 z=Q(a,x) (u)_v \delta_n \delta_{n,m} Q^{-1}(a,z)=\sum_{n=0}^\infty\frac{(z-Q(a,x))^n}{n!}\left(x\delta_n+\left(-\frac{\Gamma(a)e^x}{x^{a-1}}\right)^n\sum_{j_2=0}^n\cdots\sum_{j_n=0}^n(-1)^{\sum\limits_{m=2}^nj_m}\delta_{\sum\limits_{m=2}^n (m-1)j_m,n-1}\Gamma\left(n+\sum_{m=2}^n j_m\right)\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^x x^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,x)\right)^{j_m}\right)\tag1 z=\text{Ei}(x) \text{Ei}^{-1}(z)=x+xe^{-x}(z-\text{Ei}(x))+\frac14e^{-2x} (x-x^2) (z-\text{Ei}(x))^2+e^{-3x}\left(\frac{x^3}3-\frac{2x^2}3+\frac x6\right)(z-\text{Ei}(x))^3+e^{-4x}\left(\frac x{24}-\frac{x^4}4+\frac{3x^3}4-\frac{11x^2}{24}\right)(z-\text{Ei}(x))^4+… \text{Ei}^{-1}(x)=-\lim_{a\to0}Q^{-1}(a,-ax)=-\lim_{a\to0}\sum_{n=0}^\infty\frac{(z-\text{Ei}(x))^n}{n!}\left(-ax\delta_n+\left(-\frac{\Gamma(a)e^{-ax}}{(-ax)^{a-1}}\right)^n\sum_{j_2=0}^n\cdots\sum_{j_n=0}^n(-1)^{\sum\limits_{m=2}^nj_m}\delta_{\sum\limits_{m=2}^2 (m-1)j_m,n-1}\Gamma\left(n+\sum_{m=2}^n j_m\right)\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^{-ax} (-ax)^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,-ax)\right)^{j_m}\right) -ax\delta_n\to0,-\frac{\Gamma(a)e^{-ax}}{(-ax)^{a-1}}\to x \lim_{a\to0}\prod_{m=2}^n\frac1{j_m !}\left(\frac{a!e^{-ax} (-ax)^{1-a-m}}{m!}\right)^{j_m} \left(\sum_{k=0}^m(-1)^{m-k}\binom mk (1-a-k)_{m-1}Q(a+k,-ax)\right)^{j_m}?  Q^{-1}(a,z)\to -\lim\limits_{a\to0}Q^{-1}(a,-az) (1)  \text{Ei}^{-1}(z)=-\lim\limits_{a\to0}Q^{-1}(a,-az)","['limits', 'taylor-expansion', 'special-functions', 'gamma-function', 'inverse-function']"
46,Prove the convergence of the sequence given by the recurrence relation,Prove the convergence of the sequence given by the recurrence relation,,"I have the following problem: Let us $v_1^{(0)}, v_2^{(0)}, v_3^{(0)} > 0$ . Define the sequence $$ v_1^{(k+1)} = \dfrac{3}{ \dfrac{2}{v_1^{(k)} + v_3^{(k)}} + \dfrac{4}{v_1^{(k)} + v_2^{(k)}} } $$ $$ v_2^{(k+1)} = \dfrac{4}{ \dfrac{4}{v_1^{(k)} + v_2^{(k)}} + \dfrac{2}{v_2^{(k)} + v_3^{(k)}} } $$ $$ v_3^{(k+1)} = \dfrac{1}{ \dfrac{2}{v_1^{(k)} + v_3^{(k)}} + \dfrac{2}{v_2^{(k)} + v_3^{(k)}} } $$ Prove that the sequence $\{v^{(k)}\}_k = \{(v_1^{(k)}, v_2^{(k)}, v_3^{(k)})\}_k$ converges. Most of the proof methods that I know require knowledge of the limit of the sequence. In this case, it is difficult for me to guess what the expression for the sequence limit should be. Without knowing the limit, it would be possible to prove that the sequence is fundamental, but the calculations look too cumbersome. I tried some numerical experiments and it looks like the sequence converges for any non-zero $v_1^{(0)}, v_2^{(0)}$ and $v_3^{(0)}$ . In this case, for different starting values ​​ $v_1^{(0)}, v_2^{(0)}$ and $v_3^{(0)}$ , the sequence converges to different limit points, however, all limit points lie on a straight line with direction vector $(0.5333965 , 0.8173959 , 0.21760542)$ . But this fact looks clear from the form of the recurrence formula. I tried to calculate the Jacobian of the recurrence mapping at the limit points. Predictably, all eigenvalues ​​are less than 1 except one, which is 1. The eigenvector corresponding to an eigenvalue equal to 1 is $(0.5333965 , 0.8173959 , 0.21760542)$ . This result also looks predictable. Another interesting observation that I got from numerical experiments is that the ratio $$ \dfrac{\|v^{(k+1)} - v^{*}\|_\infty}{\|v^{(k)} - v^{*}\|_\infty}$$ stabilizes fairly quickly as $k$ increases. Here $v^{*}$ denotes the limit of the sequence. An example of $ \dfrac{\|v^{(k+1)} - v^{*}\|_\infty}{\|v^{(k)} - v^{*}\|_\infty}$ ratio values ​​when running from a random point. $$0.08030587438292239$$ $$0.2095128000211839$$ $$0.31076298843195116$$ $$0.35864703173973156$$ $$0.35707344342023817$$ $$0.3565920036480916$$ $$0.3564327245958076$$ $$0.3563787026001393$$ $$0.3563602243686815$$ $$0.35635388477426155$$ However, these are all numerical experiments, I do not have any rigorous evidence. I welcome any advice, ideas, observations, or references to the literature. Thanks in advance for your help!","I have the following problem: Let us . Define the sequence Prove that the sequence converges. Most of the proof methods that I know require knowledge of the limit of the sequence. In this case, it is difficult for me to guess what the expression for the sequence limit should be. Without knowing the limit, it would be possible to prove that the sequence is fundamental, but the calculations look too cumbersome. I tried some numerical experiments and it looks like the sequence converges for any non-zero and . In this case, for different starting values ​​ and , the sequence converges to different limit points, however, all limit points lie on a straight line with direction vector . But this fact looks clear from the form of the recurrence formula. I tried to calculate the Jacobian of the recurrence mapping at the limit points. Predictably, all eigenvalues ​​are less than 1 except one, which is 1. The eigenvector corresponding to an eigenvalue equal to 1 is . This result also looks predictable. Another interesting observation that I got from numerical experiments is that the ratio stabilizes fairly quickly as increases. Here denotes the limit of the sequence. An example of ratio values ​​when running from a random point. However, these are all numerical experiments, I do not have any rigorous evidence. I welcome any advice, ideas, observations, or references to the literature. Thanks in advance for your help!","v_1^{(0)}, v_2^{(0)}, v_3^{(0)} > 0  v_1^{(k+1)} = \dfrac{3}{ \dfrac{2}{v_1^{(k)} + v_3^{(k)}} + \dfrac{4}{v_1^{(k)} + v_2^{(k)}} }   v_2^{(k+1)} = \dfrac{4}{ \dfrac{4}{v_1^{(k)} + v_2^{(k)}} + \dfrac{2}{v_2^{(k)} + v_3^{(k)}} }   v_3^{(k+1)} = \dfrac{1}{ \dfrac{2}{v_1^{(k)} + v_3^{(k)}} + \dfrac{2}{v_2^{(k)} + v_3^{(k)}} }  \{v^{(k)}\}_k = \{(v_1^{(k)}, v_2^{(k)}, v_3^{(k)})\}_k v_1^{(0)}, v_2^{(0)} v_3^{(0)} v_1^{(0)}, v_2^{(0)} v_3^{(0)} (0.5333965 , 0.8173959 , 0.21760542) (0.5333965 , 0.8173959 , 0.21760542)  \dfrac{\|v^{(k+1)} - v^{*}\|_\infty}{\|v^{(k)} - v^{*}\|_\infty} k v^{*}  \dfrac{\|v^{(k+1)} - v^{*}\|_\infty}{\|v^{(k)} - v^{*}\|_\infty} 0.08030587438292239 0.2095128000211839 0.31076298843195116 0.35864703173973156 0.35707344342023817 0.3565920036480916 0.3564327245958076 0.3563787026001393 0.3563602243686815 0.35635388477426155","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'recurrence-relations']"
47,Asymptotics of $G(x)=\sum_{n \in \Bbb N}^x \mathrm{Vol}(\zeta(\mathbf X)).$,Asymptotics of,G(x)=\sum_{n \in \Bbb N}^x \mathrm{Vol}(\zeta(\mathbf X)).,"Consider $ S^n,$ the space of Schur-convex, simply connected, closed topological $n-$ manifolds as subsets of the unit $(n+1)-$ cube, which include $p=(0,0,\cdot\cdot\cdot,0)$ and $q=(1,1,\cdot\cdot\cdot,1).$ Consider the set of all Schur-convexity preserving maps from $S^n$ to itself. Define a class of $S^n,$ denoted $\zeta(\mathbf X),$ in the following way: Let $ L^n_+$ be the set of all $n$ -dimensional nonnegative random vectors $\mathbf X = (X_1, X_2,\cdot\cdot\cdot,X_n)^⊤$ with finite and positive marginal expectations, and let $\mathbf Ψ^{(n)}$ be the class of all measurable functions from $\Bbb R^n_+$ to $[0, 1].$ Then $\zeta(\mathbf X)$ of the random vector $\mathbf X$ with joint CDF $F$ is: $$\zeta(\mathbf X)=\bigg\{\bigg(\int \psi(\mathbf x)dF(\mathbf x), \int \frac{x_1\psi(\mathbf x)}{E(X_1)}dF(\mathbf x),\cdot\cdot\cdot,\int \frac{x_n\psi(\mathbf x)}{E(X_n)}dF(\mathbf x):\psi \in \mathbf Ψ^{(n)}\bigg)\bigg\}, $$ $$ =\bigg\{\bigg(E\psi(\mathbf X), \frac{E(X_1\psi(\mathbf X))}{E(X_1)},\cdot\cdot\cdot,\frac{E(X_n\psi(\mathbf X))}{E(X_n)}:\psi \in \mathbf Ψ^{(n)}\bigg)\bigg\}. $$ Let me provide a concrete example. I have a 3D graph of $\zeta$ for a bivariate II Pareto distribution with parameters $(\mu_1,\mu_2,\sigma_1,\sigma_2)=(0,0,1,1)$ and $\alpha=9$ so I know the qualitative nature of the shape of any given surface given by any distribution. Here is a picture to give some intuition about the definition: I was interested in obtaining the volume for $\zeta(\mathbf X).$ I calculated it to be: $$\mathrm{Vol}(\zeta(\mathbf X))= \frac{1}{(n+1)!}E\big(|\det(Q)|\big). $$ where $Q$ is a $(n+1)\times(n+1)$ matrix whose $i$ th row is $(1, \mathbf{ \bar X}^{(i)}),$ $i=1,2,\cdot\cdot\cdot , n+1.$ Here I'll use a normalized version of $\mathbf X$ denoted by $\mathbf{\bar X}$ such that $\bar X_i= X_i/E(X_i),$ $i=1,2,\cdot\cdot\cdot, n.$ Consider $n+1$ $\mathrm{iid}$ $n$ -dim. random vectors $\mathbf{ \bar X}^{(1)},\cdot\cdot\cdot, \mathbf{ \bar X}^{(n+1)} $ each with the same distribution as $\mathbf{\bar X}.$ I made the following observation: $$\lim_{n \to \infty} \frac{\mathrm{Vol}(\zeta(\mathbf X))}{\mathrm{Vol}(H^{n+1})}= \lim_{n \to \infty} \mathrm{Vol}(\zeta(\mathbf X))=0$$ where $H^{n+1}$ is the unit $(n+1)$ -cube. How do we get precise asymptotics for the following? $$G(x)=\sum_{n \in \Bbb N}^x \mathrm{Vol}(\zeta(\mathbf X)). $$ Let $\mathbf X$ be normally distributed.","Consider the space of Schur-convex, simply connected, closed topological manifolds as subsets of the unit cube, which include and Consider the set of all Schur-convexity preserving maps from to itself. Define a class of denoted in the following way: Let be the set of all -dimensional nonnegative random vectors with finite and positive marginal expectations, and let be the class of all measurable functions from to Then of the random vector with joint CDF is: Let me provide a concrete example. I have a 3D graph of for a bivariate II Pareto distribution with parameters and so I know the qualitative nature of the shape of any given surface given by any distribution. Here is a picture to give some intuition about the definition: I was interested in obtaining the volume for I calculated it to be: where is a matrix whose th row is Here I'll use a normalized version of denoted by such that Consider -dim. random vectors each with the same distribution as I made the following observation: where is the unit -cube. How do we get precise asymptotics for the following? Let be normally distributed."," S^n, n- (n+1)- p=(0,0,\cdot\cdot\cdot,0) q=(1,1,\cdot\cdot\cdot,1). S^n S^n, \zeta(\mathbf X),  L^n_+ n \mathbf X = (X_1, X_2,\cdot\cdot\cdot,X_n)^⊤ \mathbf Ψ^{(n)} \Bbb R^n_+ [0, 1]. \zeta(\mathbf X) \mathbf X F \zeta(\mathbf X)=\bigg\{\bigg(\int \psi(\mathbf x)dF(\mathbf x), \int \frac{x_1\psi(\mathbf x)}{E(X_1)}dF(\mathbf x),\cdot\cdot\cdot,\int \frac{x_n\psi(\mathbf x)}{E(X_n)}dF(\mathbf x):\psi \in \mathbf Ψ^{(n)}\bigg)\bigg\},   =\bigg\{\bigg(E\psi(\mathbf X), \frac{E(X_1\psi(\mathbf X))}{E(X_1)},\cdot\cdot\cdot,\frac{E(X_n\psi(\mathbf X))}{E(X_n)}:\psi \in \mathbf Ψ^{(n)}\bigg)\bigg\}.  \zeta (\mu_1,\mu_2,\sigma_1,\sigma_2)=(0,0,1,1) \alpha=9 \zeta(\mathbf X). \mathrm{Vol}(\zeta(\mathbf X))= \frac{1}{(n+1)!}E\big(|\det(Q)|\big).  Q (n+1)\times(n+1) i (1, \mathbf{ \bar X}^{(i)}), i=1,2,\cdot\cdot\cdot , n+1. \mathbf X \mathbf{\bar X} \bar X_i= X_i/E(X_i), i=1,2,\cdot\cdot\cdot, n. n+1 \mathrm{iid} n \mathbf{ \bar X}^{(1)},\cdot\cdot\cdot, \mathbf{ \bar X}^{(n+1)}  \mathbf{\bar X}. \lim_{n \to \infty} \frac{\mathrm{Vol}(\zeta(\mathbf X))}{\mathrm{Vol}(H^{n+1})}= \lim_{n \to \infty} \mathrm{Vol}(\zeta(\mathbf X))=0 H^{n+1} (n+1) G(x)=\sum_{n \in \Bbb N}^x \mathrm{Vol}(\zeta(\mathbf X)).  \mathbf X","['limits', 'random-variables', 'asymptotics', 'random-matrices', 'discrete-geometry']"
48,Prove or Disprove $\lim_{n\to \infty}\int_{0}^{n}\int_{0}^{n}\frac{1}{(x^n+y^n+1)^n}\mathrm{d}x\mathrm{d}y=1$ [closed],Prove or Disprove  [closed],\lim_{n\to \infty}\int_{0}^{n}\int_{0}^{n}\frac{1}{(x^n+y^n+1)^n}\mathrm{d}x\mathrm{d}y=1,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I was experimenting with the value some basic double integral(Polar) by changing powers of $x$ and $y$ , limits In the end I observed that as we increase $n$ the value of the following limit approaches 1. $$\lim_{n\to \infty}\int_{0}^{n}\int_{0}^{n}\frac{1}{(x^n+y^n+1)^n}\mathrm{d}x\mathrm{d}y=1$$ I don't know where to start evaluating the limit. Can somebody help me?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I was experimenting with the value some basic double integral(Polar) by changing powers of and , limits In the end I observed that as we increase the value of the following limit approaches 1. I don't know where to start evaluating the limit. Can somebody help me?",x y n \lim_{n\to \infty}\int_{0}^{n}\int_{0}^{n}\frac{1}{(x^n+y^n+1)^n}\mathrm{d}x\mathrm{d}y=1,"['calculus', 'integration', 'limits', 'definite-integrals']"
49,Evaluate $\lim\limits_{n \to \infty}\left(\sum\limits_{k=0}^{n}\left(\frac{\left(k-n\right)^k}{k!}\cdot e^{n-k}\right)-2n\right)$,Evaluate,\lim\limits_{n \to \infty}\left(\sum\limits_{k=0}^{n}\left(\frac{\left(k-n\right)^k}{k!}\cdot e^{n-k}\right)-2n\right),"Evaluate $\lim\limits_{n \to \infty}\left(\sum\limits_{k=0}^{n}\left(\frac{\left(k-n\right)^k}{k!}\cdot e^{n-k}\right)-2n\right)$ . By plugging in large values for $n$ , I noticed that the limit is most likely $\frac{2}{3}$ , but I can't prove it. Update Over a year has passed since I asked this question, but recently it was closed out of the blue because it lacks additional context. So let me explain where this limit comes from. I was studying the following problem: Let's consider a sequence of random numbers. Each number is uniformly distributed between $0$ and $1$ . We add up the terms of this sequence one by one until their sum exceeds a certain number $x \in \mathbb{R}$ . Let $E_x$ be the expected number of terms needed for that. The original question asked for the value of $E_1$ , which turns out to be $e$ . After I solved this by using differential equations, I wanted to find a formula for all $E_x$ . Before I had found one, I thought: ""Well, each term of the sequence increases the sum by an expected value of $\frac{1}{2}$ , so the sum should exceed $x$ after roughly $2x$ "" terms. For large $x$ , this approximation should get better and $E_x$ will get closer and closer to $2x$ ."" In other words, I thought that $\lim\limits_{x \to \infty}\left(E_x-2x\right)=0$ . This turned out to be wrong later. Finding a nice formula for $E_x$ is not easy and the only explicit formula I could come up with was a piecewise-defined function with infinitely many pieces. The first piece ranges from $0$ to $1$ , the second from $1$ to $2$ , the third from $2$ to $3$ , and so on. This makes it really hard to evaluate the limit, so I decided to look at only integer values of $x$ . And indeed, you can find a nice for those, namely: $$E_n=\sum\limits_{k=0}^{n}\left(\frac{\left(k-n\right)^k}{k!}\cdot e^{n-k}\right)$$ Plugging this into $\lim\limits_{x \to \infty}\left(E_x-2x\right)$ , we get the limit in the title of this question. And apparently, this limit is equal to $\frac{2}{3}$ and not $0$ . So, there you go. Hopefully, this will be enough to open this question again.","Evaluate . By plugging in large values for , I noticed that the limit is most likely , but I can't prove it. Update Over a year has passed since I asked this question, but recently it was closed out of the blue because it lacks additional context. So let me explain where this limit comes from. I was studying the following problem: Let's consider a sequence of random numbers. Each number is uniformly distributed between and . We add up the terms of this sequence one by one until their sum exceeds a certain number . Let be the expected number of terms needed for that. The original question asked for the value of , which turns out to be . After I solved this by using differential equations, I wanted to find a formula for all . Before I had found one, I thought: ""Well, each term of the sequence increases the sum by an expected value of , so the sum should exceed after roughly "" terms. For large , this approximation should get better and will get closer and closer to ."" In other words, I thought that . This turned out to be wrong later. Finding a nice formula for is not easy and the only explicit formula I could come up with was a piecewise-defined function with infinitely many pieces. The first piece ranges from to , the second from to , the third from to , and so on. This makes it really hard to evaluate the limit, so I decided to look at only integer values of . And indeed, you can find a nice for those, namely: Plugging this into , we get the limit in the title of this question. And apparently, this limit is equal to and not . So, there you go. Hopefully, this will be enough to open this question again.",\lim\limits_{n \to \infty}\left(\sum\limits_{k=0}^{n}\left(\frac{\left(k-n\right)^k}{k!}\cdot e^{n-k}\right)-2n\right) n \frac{2}{3} 0 1 x \in \mathbb{R} E_x E_1 e E_x \frac{1}{2} x 2x x E_x 2x \lim\limits_{x \to \infty}\left(E_x-2x\right)=0 E_x 0 1 1 2 2 3 x E_n=\sum\limits_{k=0}^{n}\left(\frac{\left(k-n\right)^k}{k!}\cdot e^{n-k}\right) \lim\limits_{x \to \infty}\left(E_x-2x\right) \frac{2}{3} 0,"['limits', 'summation', 'expected-value']"
50,Solution containing Riemann Zeta function for an integral involving the EGF of the Bernoulli/Euler polynomials,Solution containing Riemann Zeta function for an integral involving the EGF of the Bernoulli/Euler polynomials,,"In this post , the first of the following integrals is questioned. I added the second one. $$ \begin{align} &2\int_{0}^{\infty}\left(\sum_{k=0}^{n}\frac{\left(-1\right)^{k}B_{k}(1)}{k!}x^{k-n-1}-\frac{1}{x^{n}\left(e^{x}-1\right)}\right)dx\\\\ =&\ \frac{1}{1-2^{n}}\int_{0}^{\infty}\left(\sum_{k=0}^{n-1}\frac{\left(-1\right)^{k}E_k(1)}{k!}x^{k-n}-\frac{2}{x^{n}\left(e^{x}+1\right)}\right)dx\\\\ =&\ \frac{\zeta (n)}{(2\pi)^{n-1}},\quad\text{for odd } n. \end{align} $$ The result is conjectured. How can we prove it? This problem is derived from integrating the EGF of the Bernoulli & Euler polynomials after dividing it by a power of $x$ . See my previous post , which outlines the far more generalized problem (and a generalized conjecture).","In this post , the first of the following integrals is questioned. I added the second one. The result is conjectured. How can we prove it? This problem is derived from integrating the EGF of the Bernoulli & Euler polynomials after dividing it by a power of . See my previous post , which outlines the far more generalized problem (and a generalized conjecture).","
\begin{align}
&2\int_{0}^{\infty}\left(\sum_{k=0}^{n}\frac{\left(-1\right)^{k}B_{k}(1)}{k!}x^{k-n-1}-\frac{1}{x^{n}\left(e^{x}-1\right)}\right)dx\\\\
=&\ \frac{1}{1-2^{n}}\int_{0}^{\infty}\left(\sum_{k=0}^{n-1}\frac{\left(-1\right)^{k}E_k(1)}{k!}x^{k-n}-\frac{2}{x^{n}\left(e^{x}+1\right)}\right)dx\\\\
=&\ \frac{\zeta (n)}{(2\pi)^{n-1}},\quad\text{for odd } n.
\end{align}
 x","['limits', 'definite-integrals', 'generating-functions', 'riemann-zeta', 'bernoulli-polynomials']"
51,Hammack: Example 13.4 -- proof that there is no limit for sin(1/x) as x goes to 0,Hammack: Example 13.4 -- proof that there is no limit for sin(1/x) as x goes to 0,,"In section ""13.3 Limits That Do Not Exist"", of his ""Book of Proof"" (3rd Edition) Hammack gives: Example 13.4 Prove that $ \displaystyle \lim_{x \to 0} \sin\left({\frac{1}{x}}\right)$ does not exist. As $x$ approaches $0$ , the number $\frac{1}{x}$ grows bigger, approaching infinity, so $\sin(\frac{1}{x})$ just bounces up and down, faster and faster the closer $x$ gets to $0$ . His proof appears to be a direct proof that it is not true that there is a limit -- but it seems to start as if it is a proof by contradiction.  This bothers me because a ""proof of the contrary"" does not seem to be the same as ""proof by contradiction"". Here is the proof as given: Proof. Suppose for the sake of contradiction that $ \displaystyle \lim_{x \to 0} \sin\left({\frac{1}{x}}\right) = L$ for $L \in \mathbb{R}$ . Definition 13.2 guarantees a number $\delta$ for which $0 < |x - 0| < \delta)$ implies $|\sin\left({\frac{1}{x}}\right) - L| < \frac{1}{4}$ .  Select $k \in \mathbb{N}$ large enough so that $\frac{1}{k\pi} < \delta$ .  As $0 < \left|\frac{1}{k\pi} - 0\right| < \delta$ , we have $\left|\sin\left(\frac{1}{1/k\pi}\right)-L\right| < \frac{1}{4}$ , and this yields $\left|\sin\left(k\pi\right)-L\right| = \left|0-L\right| = \left|L\right|< \frac{1}{4}$ . Next, take $l \in \mathbb{N}$ large enough so that $\frac{1}{\frac{\pi}{2} + 2l\pi} < \delta$ , so we have $0 < \left|\frac{1}{k\pi} - 0\right| < \delta$ , we have $\left|\sin\left(\frac{1}{\frac{1}{\frac{\pi}{2} + 2l\pi}}\right)-L\right| < \frac{1}{4}$ , which simplifies to $\left|\sin\left(\frac{\pi}{2} + 2l\pi\right)-L\right| = \left|1-L\right| < \frac{1}{4}$ . Above we showed $\left|L\right|< \frac{1}{4}$ and $\left|1-L\right| < \frac{1}{4}$ . Now apply the inequality (13.2) to get the contradiction $1 < \frac{1}{2}$ , as $1 = |L+(1-L)| \le |L| + |1-L| < \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$ . Hammack also gives: ... in symbolic form Definition 13.2 says $\displaystyle \lim_{x \to c} f(x) = L$ if and only if $\forall \epsilon > 0, \exists \delta > 0, (0 < |x - c| < \delta) \implies (|f(x) - L| < \epsilon)$ (13.4) I believe the contrary of that is: $\exists \epsilon > 0, \forall \delta > 0, (0 < |x - c| < \delta) \land \lnot(|f(x) - L| < \epsilon)$ So the above proof appears to prove the contrary.  It chooses an $\epsilon$ and shows that for all $\delta$ it is not true that $|f(x) - L| < \epsilon$ , no matter what value of $L \in \mathbb{R}$ might be.  Proving the last part is by contradiction: it is proved that assuming $|f(x) - L| < \epsilon$ is true (for all $\delta$ and all $L$ ) leads to a contradiction. So my actual questions are: perhaps I have misread Hammack, and all his proofs of the non-existence of limits are indeed (implicitly) direct proofs of not-existence (proof of the contrary), but the last step of that is generally a proof by contradiction ? Or: perhaps I am mistaken in thinking that ""proof of the contrary"" and ""proof by contradiction"" are different ?","In section ""13.3 Limits That Do Not Exist"", of his ""Book of Proof"" (3rd Edition) Hammack gives: Example 13.4 Prove that does not exist. As approaches , the number grows bigger, approaching infinity, so just bounces up and down, faster and faster the closer gets to . His proof appears to be a direct proof that it is not true that there is a limit -- but it seems to start as if it is a proof by contradiction.  This bothers me because a ""proof of the contrary"" does not seem to be the same as ""proof by contradiction"". Here is the proof as given: Proof. Suppose for the sake of contradiction that for . Definition 13.2 guarantees a number for which implies .  Select large enough so that .  As , we have , and this yields . Next, take large enough so that , so we have , we have , which simplifies to . Above we showed and . Now apply the inequality (13.2) to get the contradiction , as . Hammack also gives: ... in symbolic form Definition 13.2 says if and only if (13.4) I believe the contrary of that is: So the above proof appears to prove the contrary.  It chooses an and shows that for all it is not true that , no matter what value of might be.  Proving the last part is by contradiction: it is proved that assuming is true (for all and all ) leads to a contradiction. So my actual questions are: perhaps I have misread Hammack, and all his proofs of the non-existence of limits are indeed (implicitly) direct proofs of not-existence (proof of the contrary), but the last step of that is generally a proof by contradiction ? Or: perhaps I am mistaken in thinking that ""proof of the contrary"" and ""proof by contradiction"" are different ?"," \displaystyle \lim_{x \to 0} \sin\left({\frac{1}{x}}\right) x 0 \frac{1}{x} \sin(\frac{1}{x}) x 0  \displaystyle \lim_{x \to 0} \sin\left({\frac{1}{x}}\right) = L L \in \mathbb{R} \delta 0 < |x - 0| < \delta) |\sin\left({\frac{1}{x}}\right) - L| < \frac{1}{4} k \in \mathbb{N} \frac{1}{k\pi} < \delta 0 < \left|\frac{1}{k\pi} - 0\right| < \delta \left|\sin\left(\frac{1}{1/k\pi}\right)-L\right| < \frac{1}{4} \left|\sin\left(k\pi\right)-L\right| = \left|0-L\right| = \left|L\right|< \frac{1}{4} l \in \mathbb{N} \frac{1}{\frac{\pi}{2} + 2l\pi} < \delta 0 < \left|\frac{1}{k\pi} - 0\right| < \delta \left|\sin\left(\frac{1}{\frac{1}{\frac{\pi}{2} + 2l\pi}}\right)-L\right| < \frac{1}{4} \left|\sin\left(\frac{\pi}{2} + 2l\pi\right)-L\right| = \left|1-L\right| < \frac{1}{4} \left|L\right|< \frac{1}{4} \left|1-L\right| < \frac{1}{4} 1 < \frac{1}{2} 1 = |L+(1-L)| \le |L| + |1-L| < \frac{1}{4} + \frac{1}{4} = \frac{1}{2} \displaystyle \lim_{x \to c} f(x) = L \forall \epsilon > 0, \exists \delta > 0, (0 < |x - c| < \delta) \implies (|f(x) - L| < \epsilon) \exists \epsilon > 0, \forall \delta > 0, (0 < |x - c| < \delta) \land \lnot(|f(x) - L| < \epsilon) \epsilon \delta |f(x) - L| < \epsilon L \in \mathbb{R} |f(x) - L| < \epsilon \delta L","['calculus', 'limits', 'proof-explanation']"
52,Can a function $f$ be continuous at a point $a$ even if $a$ is not a limit point of $f$?,Can a function  be continuous at a point  even if  is not a limit point of ?,f a a f,"Suppose we adopt the following definitions of a limit and continuity: Definition of a Limit Suppose a function $\DeclareMathOperator{\epsilon}{\varepsilon}f:D\mapsto\mathbb{R}$ has a limit point at $a$ , i.e. for every $\delta>0$ there exists $x\in D$ such that $0<|x-a|<\delta$ . We say that $\lim_{x \to a}f(x)=L$ iff For every $\epsilon>0$ there exists $\delta>0$ such that, for all $x$ , if $x\in D$ and $0<|x-a|<\delta$ , then $|f(x)-L|<\epsilon$ . Definition of continuity Suppose a function $f:D\mapsto\mathbb{R}$ is defined at the point $a$ . We say that $f$ is continuous at $a$ iff For every $\epsilon>0$ there exists $\delta>0$ such that, for all $x$ , if $x\in D$ and $|x-a|<\delta$ , then $|f(x)-f(a)|<\epsilon$ . The problem I have is this: the definition of continuity is often abbreviated to $f:D\mapsto\mathbb{R}$ is continuous at $a$ iff $\lim_{x \to a}f(x)=f(a)$ . However, according to the definitions I have presented, it seems that there might be cases where $\lim_{x \to a}f(x)$ does not exist, and yet it is still the case that $f$ is continuous at $a$ . For example, consider the function $\phi=\{(0,0)\}$ . This function is trivially continuous at $0$ , but $\lim_{x \to 0}\phi(x)$ does not exist because $0$ is not a limit point of $\phi$ . So are the two definitions of continuity only equivalent if $a$ is a limit point of $f$ ? I have one further question. The definition of a limit requires that $a$ be a limit point of $f$ . Is this restriction necessary because otherwise we could argue things like $\lim_{x \to 0}\phi(x)=1$ and $\lim_{x \to 0}\phi(x)=2$ are both vacuously true?","Suppose we adopt the following definitions of a limit and continuity: Definition of a Limit Suppose a function has a limit point at , i.e. for every there exists such that . We say that iff For every there exists such that, for all , if and , then . Definition of continuity Suppose a function is defined at the point . We say that is continuous at iff For every there exists such that, for all , if and , then . The problem I have is this: the definition of continuity is often abbreviated to is continuous at iff . However, according to the definitions I have presented, it seems that there might be cases where does not exist, and yet it is still the case that is continuous at . For example, consider the function . This function is trivially continuous at , but does not exist because is not a limit point of . So are the two definitions of continuity only equivalent if is a limit point of ? I have one further question. The definition of a limit requires that be a limit point of . Is this restriction necessary because otherwise we could argue things like and are both vacuously true?","\DeclareMathOperator{\epsilon}{\varepsilon}f:D\mapsto\mathbb{R} a \delta>0 x\in D 0<|x-a|<\delta \lim_{x \to a}f(x)=L \epsilon>0 \delta>0 x x\in D 0<|x-a|<\delta |f(x)-L|<\epsilon f:D\mapsto\mathbb{R} a f a \epsilon>0 \delta>0 x x\in D |x-a|<\delta |f(x)-f(a)|<\epsilon f:D\mapsto\mathbb{R} a \lim_{x \to a}f(x)=f(a) \lim_{x \to a}f(x) f a \phi=\{(0,0)\} 0 \lim_{x \to 0}\phi(x) 0 \phi a f a f \lim_{x \to 0}\phi(x)=1 \lim_{x \to 0}\phi(x)=2","['real-analysis', 'calculus', 'limits', 'definition']"
53,Where is the mistake in my proof of $\lim_{x \to 0} f(|x|)=l \implies \lim_{x \to 0} f(x)=l$?,Where is the mistake in my proof of ?,\lim_{x \to 0} f(|x|)=l \implies \lim_{x \to 0} f(x)=l,"Assume that $f:(-a,a) \setminus{0} \to \mathbb{R}$ . Does the implication $\lim_{x \to 0} f(|x|) =l \implies \lim_{x \to 0} f(x)=l$ hold? My textbook gives a counter-example of this statement using the example $f(x)=[|x|]$ ; before finding out a counter-example myself, I've tried to prove this and made a proof that is of course wrong (so I naively stopped searching for counter-examples; bad behaviour). However, I would like to learn from my mistake and identify where is my wrong reasoning. Can someone help me find where the mistake is? Wrong proof: by hypothesis $\lim_{x \to 0} f(|x|)=l$ , so it is true that for all $\epsilon>0$ there exists $\delta(\epsilon)>0$ such that for all $x\in(-a,a)\setminus{0}$ , $|x|<\delta(\varepsilon)\implies |f(|x|)-l|<\epsilon$ . But $|x|=||x||$ , so the condition $|x|<\delta(\varepsilon)$ is equivalent to $||x||<\delta(\varepsilon)$ ; so it is true that for all $\epsilon>0$ there exists $\delta(\epsilon)>0$ such that for all $x\in(-a,a)\setminus{0}$ , $||x||<\delta(\varepsilon)\implies |f(|x|)-l|<\epsilon$ . This (I believe) mean that $\lim_{|x|\to 0} f(|x|)=l$ ; letting $|x|=r$ , it is $\lim_{r \to 0} f(r)=l$ . Since (I believe) the variable in the limit is dummy, this means $\lim_{x \to 0} f(x)=l$ . I've written ""I believe"" in the steps I'm not sure about. My first impressions in trying to recognize my mistakes is that probably I'm using wrong the definition of limit or the theorem of change of variables in limits. Thanks to anyone who wants to help me.","Assume that . Does the implication hold? My textbook gives a counter-example of this statement using the example ; before finding out a counter-example myself, I've tried to prove this and made a proof that is of course wrong (so I naively stopped searching for counter-examples; bad behaviour). However, I would like to learn from my mistake and identify where is my wrong reasoning. Can someone help me find where the mistake is? Wrong proof: by hypothesis , so it is true that for all there exists such that for all , . But , so the condition is equivalent to ; so it is true that for all there exists such that for all , . This (I believe) mean that ; letting , it is . Since (I believe) the variable in the limit is dummy, this means . I've written ""I believe"" in the steps I'm not sure about. My first impressions in trying to recognize my mistakes is that probably I'm using wrong the definition of limit or the theorem of change of variables in limits. Thanks to anyone who wants to help me.","f:(-a,a) \setminus{0} \to \mathbb{R} \lim_{x \to 0} f(|x|) =l \implies \lim_{x \to 0} f(x)=l f(x)=[|x|] \lim_{x \to 0} f(|x|)=l \epsilon>0 \delta(\epsilon)>0 x\in(-a,a)\setminus{0} |x|<\delta(\varepsilon)\implies |f(|x|)-l|<\epsilon |x|=||x|| |x|<\delta(\varepsilon) ||x||<\delta(\varepsilon) \epsilon>0 \delta(\epsilon)>0 x\in(-a,a)\setminus{0} ||x||<\delta(\varepsilon)\implies |f(|x|)-l|<\epsilon \lim_{|x|\to 0} f(|x|)=l |x|=r \lim_{r \to 0} f(r)=l \lim_{x \to 0} f(x)=l","['limits', 'analysis', 'solution-verification']"
54,Finding $\lim_{n\to\infty} \frac{((1!)(2!)(3!).....(n!))^{1/n^2}}{n^{\alpha}}$ [closed],Finding  [closed],\lim_{n\to\infty} \frac{((1!)(2!)(3!).....(n!))^{1/n^2}}{n^{\alpha}},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question if $\alpha ,\beta$ belong to $\mathbb{R}$ , $\beta$ is not equal to zero, $n$ belong to $\mathbb{N}$ and $$\lim_{n\to\infty} \frac{((1!)(2!)(3!).....(n!))^{1/n^2}}{n^{\alpha}} = \beta$$ then please help me. Any help will be appreciated. Thanks I think this can be solved using Squeeze theorem but i m not able to apply it .... how to apply and solve the problem how to apply and solve the problem i tried using different technique but fail","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question if belong to , is not equal to zero, belong to and then please help me. Any help will be appreciated. Thanks I think this can be solved using Squeeze theorem but i m not able to apply it .... how to apply and solve the problem how to apply and solve the problem i tried using different technique but fail","\alpha ,\beta \mathbb{R} \beta n \mathbb{N} \lim_{n\to\infty} \frac{((1!)(2!)(3!).....(n!))^{1/n^2}}{n^{\alpha}} = \beta",['limits']
55,A question about limit [closed],A question about limit [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Can anyone help this questions? Find the limit of $$1+\sqrt{2+\sqrt[3]{3+\sqrt[4]{4+\sqrt[5]{5+....+\sqrt[n]{n}}}}}$$ I can only solve that this formula is less than 3 but can not find the exact answer for this.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Can anyone help this questions? Find the limit of I can only solve that this formula is less than 3 but can not find the exact answer for this.",1+\sqrt{2+\sqrt[3]{3+\sqrt[4]{4+\sqrt[5]{5+....+\sqrt[n]{n}}}}},"['limits', 'nested-radicals']"
56,How to prove that $\sum(-1)^n\sin \frac{1}{n^\alpha \ln n}$ is divergent for any $\alpha\leq 0$?,How to prove that  is divergent for any ?,\sum(-1)^n\sin \frac{1}{n^\alpha \ln n} \alpha\leq 0,How to prove that $\sum(-1)^n\sin \frac{1}{n^\alpha \ln n}$ is divergent for any $\alpha\leq 0$ ? It is well-known that $\lim \sin n$ does not exist. But that procedure could not be easily adapted to those that $\lim \sin \frac{1}{n^\alpha\ln n}$ does not exists. How to do then for this above problem?,How to prove that is divergent for any ? It is well-known that does not exist. But that procedure could not be easily adapted to those that does not exists. How to do then for this above problem?,\sum(-1)^n\sin \frac{1}{n^\alpha \ln n} \alpha\leq 0 \lim \sin n \lim \sin \frac{1}{n^\alpha\ln n},"['calculus', 'sequences-and-series', 'limits']"
57,A clarification in a limit,A clarification in a limit,,"While reading this answer I do not understand that how can we claim that there exist a polynomial with the properties, even if it exist then how is it same for the function and the inverse.","While reading this answer I do not understand that how can we claim that there exist a polynomial with the properties, even if it exist then how is it same for the function and the inverse.",,"['calculus', 'limits', 'soft-question']"
58,Summing a series having a geometric component,Summing a series having a geometric component,,If $$\alpha=\frac {5}{2!3}+\frac {5\cdot 7}{3!3^2}+\frac {5\cdot 7\cdot 9}{4!3^3}+\dots $$  Find the value of $\alpha^2+4\alpha $. The possible options are: 21 23 25 27 I think $$\alpha=\sum \frac {(2n+4)!}{2^{n+2}(n+1)!(n+2)!3^{n+1}}$$ but cannot think of what else to do.,If $$\alpha=\frac {5}{2!3}+\frac {5\cdot 7}{3!3^2}+\frac {5\cdot 7\cdot 9}{4!3^3}+\dots $$  Find the value of $\alpha^2+4\alpha $. The possible options are: 21 23 25 27 I think $$\alpha=\sum \frac {(2n+4)!}{2^{n+2}(n+1)!(n+2)!3^{n+1}}$$ but cannot think of what else to do.,,['sequences-and-series']
59,Integrating $\ln(x)\times\ln(1-x)$,Integrating,\ln(x)\times\ln(1-x),Is there a way I can derive the value of the integral $ \int_0^1 \ln(x)\ln(1-x)dx$ using the fact that $\displaystyle\sum_{i=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}$ ? (the actual value of the integral is $2-\frac{\pi^2}{6}$) Thanks in advance,Is there a way I can derive the value of the integral $ \int_0^1 \ln(x)\ln(1-x)dx$ using the fact that $\displaystyle\sum_{i=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}$ ? (the actual value of the integral is $2-\frac{\pi^2}{6}$) Thanks in advance,,"['integration', 'logarithms']"
60,Proving a Function which is continous everywhere and differential nowhere.,Proving a Function which is continous everywhere and differential nowhere.,,"Let $f(x)=x-\left \lfloor{x}\right \rfloor$ , $s(x)=\begin{cases} f(x) & \text{when $f(x) \le0.5$,}\\ 1-f(x) & \text{when $f(x)>0.5$} \end{cases}$ , $s_n(x)=\frac{s(2^{n-1}x)}{2^{n-1}}$ , $b(x)=\sum_{n=1}^{n=\infty}s_n(x)=\sum_{n=1}^{n=N} s_n(x)+\frac{b(2^Nx)}{2^N}$ , $L_N(x)=\sum_{n=1}^{n=N}s_n(x)$ which is linear on interval $[\frac{m}{2^N},\frac{m+1}{2^N}]$ for any integer $m$ , and $b(\frac{m}{2^N})=L_N (\frac{m}{2^N})$ , $c$ is the least number in the interval $(a, a+\frac{1}{2^{N-1}}]$ of the form $\frac{m}{2^N}$ . Then $d=\frac{m+1}{2^N}$ and $k=\frac{m+1/3}{2^N}$ which is in the interval so $a<c<k<d\le a+\frac{1}{2^{N-1}}$ , $L$ is linear function which coincides with $L_N$ on the interval $[c,d]$ , By definition $\frac{L(k)-L(a)}{k-a}$ = $\frac{L(c)-L(a)}{c-a}$ so If $b(a)\ge L(a)$ then $\frac{b(k)-b(a)}{k-a}$ - $\frac{b(c)-b(a)}{c-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2$ . And For case $L(a)\ge b(a)$ , $\frac{L(k)-L(a)}{k-a}$ = $\frac{L(d)-L(a)}{d-a}$ so $\frac{b(k)-b(a)}{k-a}$ - $\frac{b(d)-b(a)}{d-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2$ . The main objective here is to prove that limit of $\frac{b(x)-b(a)}{x-a}$ doesn't exist as $x\to a$ . So since We got the inequality for both cases we got $\frac{b(k)-b(a)}{k-a}$ - $\frac{b(d)-b(a)}{d-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2$ and $\frac{b(k)-b(a)}{k-a}$ - $\frac{b(c)-b(a)}{c-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2$ from it. I think we can conclude that the limit doesn't exists since $c,k,d$ are in the interval of the neighbourhood of $a$ . Then I checked book's answer which says: Every neighbourhood of $a$ contains an interval of the form $(a,a+1/2^{N-1}]$ and therefore points corresponding to c,k and d. So there can be no limit $g$ such that $|\frac{b(x)-b(a)}{x-a}-g|<1/5$ , for all $x$ inside any neighbourhood of $a$ . And my question again arised from the answer of book. Why is $|\frac{b(x)-b(a)}{x-a}-g|$ less than 1/5? Where and how did 1/5 come out of thin air? I think I don't understand the answer of book properly. Why did they choose that the $\epsilon=1/5$ and why not $1/2$ ? I think $\epsilon$ can't be chosen randomly there must be a way they got it. But which way? So I want to see how to prove that the $b(x)$ is not differentiable more rigorously? I can intutively see that there are infinite amount of sharp turn in self-similar function $b(x)$ which makes it non differentiable but how to prove it is not differentiable using mathematical analysis is somewhat hard for me to do. (If there is some missing information let me know in the comment)","Let , , , , which is linear on interval for any integer , and , is the least number in the interval of the form . Then and which is in the interval so , is linear function which coincides with on the interval , By definition = so If then - . And For case , = so - . The main objective here is to prove that limit of doesn't exist as . So since We got the inequality for both cases we got - and - from it. I think we can conclude that the limit doesn't exists since are in the interval of the neighbourhood of . Then I checked book's answer which says: Every neighbourhood of contains an interval of the form and therefore points corresponding to c,k and d. So there can be no limit such that , for all inside any neighbourhood of . And my question again arised from the answer of book. Why is less than 1/5? Where and how did 1/5 come out of thin air? I think I don't understand the answer of book properly. Why did they choose that the and why not ? I think can't be chosen randomly there must be a way they got it. But which way? So I want to see how to prove that the is not differentiable more rigorously? I can intutively see that there are infinite amount of sharp turn in self-similar function which makes it non differentiable but how to prove it is not differentiable using mathematical analysis is somewhat hard for me to do. (If there is some missing information let me know in the comment)","f(x)=x-\left \lfloor{x}\right \rfloor s(x)=\begin{cases}
f(x) & \text{when f(x) \le0.5,}\\
1-f(x) & \text{when f(x)>0.5}
\end{cases} s_n(x)=\frac{s(2^{n-1}x)}{2^{n-1}} b(x)=\sum_{n=1}^{n=\infty}s_n(x)=\sum_{n=1}^{n=N} s_n(x)+\frac{b(2^Nx)}{2^N} L_N(x)=\sum_{n=1}^{n=N}s_n(x) [\frac{m}{2^N},\frac{m+1}{2^N}] m b(\frac{m}{2^N})=L_N (\frac{m}{2^N}) c (a, a+\frac{1}{2^{N-1}}] \frac{m}{2^N} d=\frac{m+1}{2^N} k=\frac{m+1/3}{2^N} a<c<k<d\le a+\frac{1}{2^{N-1}} L L_N [c,d] \frac{L(k)-L(a)}{k-a} \frac{L(c)-L(a)}{c-a} b(a)\ge L(a) \frac{b(k)-b(a)}{k-a} \frac{b(c)-b(a)}{c-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2 L(a)\ge b(a) \frac{L(k)-L(a)}{k-a} \frac{L(d)-L(a)}{d-a} \frac{b(k)-b(a)}{k-a} \frac{b(d)-b(a)}{d-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2 \frac{b(x)-b(a)}{x-a} x\to a \frac{b(k)-b(a)}{k-a} \frac{b(d)-b(a)}{d-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2 \frac{b(k)-b(a)}{k-a} \frac{b(c)-b(a)}{c-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2 c,k,d a a (a,a+1/2^{N-1}] g |\frac{b(x)-b(a)}{x-a}-g|<1/5 x a |\frac{b(x)-b(a)}{x-a}-g| \epsilon=1/5 1/2 \epsilon b(x) b(x)","['real-analysis', 'limits']"
61,"show that limit $\lim_{n\to+\infty}f_{n}(x)=x+3$,if $f_{n+1}(x)=\sqrt{6(1+x)+f_{n}(x^2)}$","show that limit ,if",\lim_{n\to+\infty}f_{n}(x)=x+3 f_{n+1}(x)=\sqrt{6(1+x)+f_{n}(x^2)},"let $x$ is give postive real number,if $f_{0}(x)=0,0<x\le\dfrac{1}{2}$ , and such $$f_{n+1}(x)=\sqrt{6(1+x)+f_{n}(x^2)}$$ show that $$\lim_{n\to+\infty}f_{n}(x)=x+3$$ This problem is from AMM 11967(2017),this solution","let is give postive real number,if , and such show that This problem is from AMM 11967(2017),this solution","x f_{0}(x)=0,0<x\le\dfrac{1}{2} f_{n+1}(x)=\sqrt{6(1+x)+f_{n}(x^2)} \lim_{n\to+\infty}f_{n}(x)=x+3",['limits']
62,"Is the $\infty$ symbol used in limits an actual mathematical object, or just notation?","Is the  symbol used in limits an actual mathematical object, or just notation?",\infty,"My calculus textbook uses $$\lim_{x\to c} f(x) = \infty$$ to express an infinite limit. Does this mean that the expression $\lim_{x\to c} f(x)$ is the same as some mathematical entity symbolized by $\infty$ , or is it just notation used to condense meaning? Also, is this notation really correct/proper (is it used in formal contexts) or is my textbook breaking formality for the sake of learning?","My calculus textbook uses to express an infinite limit. Does this mean that the expression is the same as some mathematical entity symbolized by , or is it just notation used to condense meaning? Also, is this notation really correct/proper (is it used in formal contexts) or is my textbook breaking formality for the sake of learning?",\lim_{x\to c} f(x) = \infty \lim_{x\to c} f(x) \infty,"['calculus', 'limits']"
63,Evaluate alternating double series,Evaluate alternating double series,,Does anybody have an idea on how to evaluate the following double series? $$\sum_{n=1}^\infty \sum_{m=1}^\infty\frac{(-1)^{n+m}}{nm(n^2+m^2)}$$ Clearly this is a convergent series but I think it might be hard to find a closed expression for this.,Does anybody have an idea on how to evaluate the following double series? Clearly this is a convergent series but I think it might be hard to find a closed expression for this.,\sum_{n=1}^\infty \sum_{m=1}^\infty\frac{(-1)^{n+m}}{nm(n^2+m^2)},"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
64,Condition for $f(x)=p[x+1]+q[x-1]$ to be continuous at $x=1$,Condition for  to be continuous at,f(x)=p[x+1]+q[x-1] x=1,"The function $f(x)=p[x+1]+q[x-1]$ where $[x]$ is the greatest integer function is continuous at $x=1$ , if ___________________ $f(1)=2p$ $\lim_{x\to 1^+}f(x)=p(2)+q(0)=2p$ $\lim_{x\to 1^-}f(x)=p(1)+q(-1)=p-q$ $p-q=2p\implies p+q=0$ But my reference gives the solution $p=q$ , what is going wrong here ?","The function where is the greatest integer function is continuous at , if ___________________ But my reference gives the solution , what is going wrong here ?",f(x)=p[x+1]+q[x-1] [x] x=1 f(1)=2p \lim_{x\to 1^+}f(x)=p(2)+q(0)=2p \lim_{x\to 1^-}f(x)=p(1)+q(-1)=p-q p-q=2p\implies p+q=0 p=q,"['limits', 'functions', 'continuity', 'limits-without-lhopital']"
65,Euclidean division $n$ and $m$,Euclidean division  and,n m,"$\lim_{n\to \infty}\frac{1}{n^2}\bigl(\sum_{m = 1}^ {n}n\bmod m\bigl)$ Where $n\bmod m $ is the remainder of the Euclidean division of $n$ by $m$ I would like to know if I am right or if something is missing: $n \bmod m=n-m\left \lfloor \frac{n}{m}\right\rfloor$ and so \begin{align} &\lim_{n\to\infty}\frac{1}{n^2}\sum_{m=1}^n(n \bmod m)=\lim_{n\to\infty}\frac{1}{n^2}\sum_{m=1}^n\left(n-m\left \lfloor \frac{n}{m}\right\rfloor\right)=1-\lim_{n\to\infty}\frac{1}{n^2}\sum_{m=1}^nm\left\lfloor\frac{n}{m}\right\rfloor \\ &=1-\lim_{n\to\infty}\frac{1}{n}\sum_{m=1}^n\frac{m}{n}\left \lfloor \frac{n}{m}\right\rfloor=1-\int_0^1x\left\lfloor\frac{1}{x}\right\rfloor dx =1-\int_1^{\infty}\frac{\lfloor x \rfloor}{x^3}\,dx\\&=1-\sum_{n=1}^{\infty}\int_n^{n+1}\frac{n}{x^3}\,dx  =1-\frac{1}{2}\sum_{n=1}^{\infty}n\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) =1-\sum_{n=1}^{\infty}\frac{n+1/2}{n(n+1)^2}\\[1ex] &=1-\sum_{n=1}^{\infty}\frac{1}{(n+1)^2}-\frac{1}{2}\sum_{n=1}^{\infty}\frac{1}{n(n+1)}+\frac{1}{2}\sum_{n=1}^{\infty}\frac{1}{(n+1)^2}=1-\frac{\pi^2}{12}. \end{align}",Where is the remainder of the Euclidean division of by I would like to know if I am right or if something is missing: and so,"\lim_{n\to \infty}\frac{1}{n^2}\bigl(\sum_{m = 1}^ {n}n\bmod m\bigl) n\bmod m  n m n \bmod m=n-m\left \lfloor \frac{n}{m}\right\rfloor \begin{align}
&\lim_{n\to\infty}\frac{1}{n^2}\sum_{m=1}^n(n \bmod m)=\lim_{n\to\infty}\frac{1}{n^2}\sum_{m=1}^n\left(n-m\left \lfloor \frac{n}{m}\right\rfloor\right)=1-\lim_{n\to\infty}\frac{1}{n^2}\sum_{m=1}^nm\left\lfloor\frac{n}{m}\right\rfloor \\
&=1-\lim_{n\to\infty}\frac{1}{n}\sum_{m=1}^n\frac{m}{n}\left \lfloor \frac{n}{m}\right\rfloor=1-\int_0^1x\left\lfloor\frac{1}{x}\right\rfloor dx =1-\int_1^{\infty}\frac{\lfloor x \rfloor}{x^3}\,dx\\&=1-\sum_{n=1}^{\infty}\int_n^{n+1}\frac{n}{x^3}\,dx 
=1-\frac{1}{2}\sum_{n=1}^{\infty}n\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) =1-\sum_{n=1}^{\infty}\frac{n+1/2}{n(n+1)^2}\\[1ex]
&=1-\sum_{n=1}^{\infty}\frac{1}{(n+1)^2}-\frac{1}{2}\sum_{n=1}^{\infty}\frac{1}{n(n+1)}+\frac{1}{2}\sum_{n=1}^{\infty}\frac{1}{(n+1)^2}=1-\frac{\pi^2}{12}.
\end{align}","['calculus', 'limits', 'proof-verification', 'contest-math']"
66,Limit and integral!,Limit and integral!,,"Let $f: [0, +\infty)\rightarrow[0, +\infty)$ if differentiable function and $f'>0$ on all $[0, +\infty)$ . Prove that for all $\epsilon>0$ : $$\lim_{t\rightarrow+\infty} \frac{1}{t^2} \int_0^t \frac{f^{1+\epsilon}(x)}{f'(x)} dx=+\infty$$ My attempt: It is true that $t^2$ and $\int_0^t \frac{f^{1+\epsilon}(x)}{f'(x)}dx$ are to $+\infty$ then $t$ to $+\infty$ , so it is enough to prove that: $$\lim_{t\rightarrow+\infty} \frac{f^{1+\epsilon}(t)}{t\cdot f'(t)}=+\infty$$ But it is not true! So, what I can do another?","Let if differentiable function and on all . Prove that for all : My attempt: It is true that and are to then to , so it is enough to prove that: But it is not true! So, what I can do another?","f: [0, +\infty)\rightarrow[0, +\infty) f'>0 [0, +\infty) \epsilon>0 \lim_{t\rightarrow+\infty} \frac{1}{t^2} \int_0^t \frac{f^{1+\epsilon}(x)}{f'(x)} dx=+\infty t^2 \int_0^t \frac{f^{1+\epsilon}(x)}{f'(x)}dx +\infty t +\infty \lim_{t\rightarrow+\infty} \frac{f^{1+\epsilon}(t)}{t\cdot f'(t)}=+\infty","['real-analysis', 'calculus', 'integration', 'limits', 'monotone-functions']"
67,A limit of a power series,A limit of a power series,,"I was trying to make some undergraduate level analysis problem. The point was interval of convergence of power series. It seems that the students are bored with usual coefficients. So I considered the following 'relatively' new kind of power series. $$ f(x) = \sum_{n=1}^\infty \left( 1-\frac{1}{n}\right)^{n^2} x^n $$ The question I proposed is to find the interval of convergence of $f(x)$ . Soon after, I realized using this series I can make rather interesting questions. Prove that $\lim_{x \rightarrow e^-} f(x) = \infty$ and find the limit $\lim_{x \rightarrow -e^+} f(x)$ . I think this maybe an  interesting question for undergraduate students. However I have failed to make a reasonable solution. Please help me to improve this questions.","I was trying to make some undergraduate level analysis problem. The point was interval of convergence of power series. It seems that the students are bored with usual coefficients. So I considered the following 'relatively' new kind of power series. The question I proposed is to find the interval of convergence of . Soon after, I realized using this series I can make rather interesting questions. Prove that and find the limit . I think this maybe an  interesting question for undergraduate students. However I have failed to make a reasonable solution. Please help me to improve this questions.", f(x) = \sum_{n=1}^\infty \left( 1-\frac{1}{n}\right)^{n^2} x^n  f(x) \lim_{x \rightarrow e^-} f(x) = \infty \lim_{x \rightarrow -e^+} f(x),"['real-analysis', 'calculus', 'limits', 'power-series']"
68,Proving $\lim_{x\to\infty}\frac{x+3}{x^2-3}=0$ using delta-epsilon,Proving  using delta-epsilon,\lim_{x\to\infty}\frac{x+3}{x^2-3}=0,"I'm trying to prove $$\lim_{x\to\infty}\frac{x+3}{x^2-3}=0$$ using delta-epsilon. In the definition of limit $$|f(x)-L|\lt\epsilon$$ $$|\frac{x+3}{x^2-3}-0|\lt\epsilon$$ $$|\frac{x+3}{x^2-3}|\lt\epsilon$$ and since the left side quite complicated I've come up with simple term that always bigger than $$\frac{x+3}{x^2-3}$$ and that is $$\frac{2x}{x^2/1.5}$$ or $$\frac{3}{x}$$ as long I take $x>3$ . so here I take $3$ as $M$ so it will turn $x>M$ into $x>3$ then for second inequality: $$\frac{3}{x}\lt\epsilon$$ $$x\gt\frac{3}{\epsilon}$$ and here I take $\frac{3}{\epsilon}$ as $M$ and wrapping things up $$\frac{x+3}{x^2-3}\lt\frac{3}{x}\lt\epsilon$$ . But in reality how both $M$ affect me to choose $x$ ? I mean in first inequality is demand me with $x>3$ . In other words if I choose $M$ that smaller than $3$ , it will crumble the first inequality (when I try simplifying things).  So if I look at second $M (=\frac{3}{\epsilon})$ should I have the limitation $$\epsilon\lt1$$ so that I only can get $x$ more than $3$ ?","I'm trying to prove using delta-epsilon. In the definition of limit and since the left side quite complicated I've come up with simple term that always bigger than and that is or as long I take . so here I take as so it will turn into then for second inequality: and here I take as and wrapping things up . But in reality how both affect me to choose ? I mean in first inequality is demand me with . In other words if I choose that smaller than , it will crumble the first inequality (when I try simplifying things).  So if I look at second should I have the limitation so that I only can get more than ?",\lim_{x\to\infty}\frac{x+3}{x^2-3}=0 |f(x)-L|\lt\epsilon |\frac{x+3}{x^2-3}-0|\lt\epsilon |\frac{x+3}{x^2-3}|\lt\epsilon \frac{x+3}{x^2-3} \frac{2x}{x^2/1.5} \frac{3}{x} x>3 3 M x>M x>3 \frac{3}{x}\lt\epsilon x\gt\frac{3}{\epsilon} \frac{3}{\epsilon} M \frac{x+3}{x^2-3}\lt\frac{3}{x}\lt\epsilon M x x>3 M 3 M (=\frac{3}{\epsilon}) \epsilon\lt1 x 3,['limits']
69,Find all $A$ such that $f(x) = \sum_{n \in A} \frac{x^n}{n!}$ is bounded for $x<0$,Find all  such that  is bounded for,A f(x) = \sum_{n \in A} \frac{x^n}{n!} x<0,"Let $\mathbb{R}_- = \{ x \in \mathbb{R} \mid x \leq 0 \}$ , $\mathbb{N} = \{0,1, 2, ...\}$ . Find all subsets $A \subset \mathbb{N}$ such that the function $f : \mathbb{R}_- \to \mathbb{R}$ , $$f(x) := \sum_{n \in A} \frac{x^n}{n!}$$ is bounded. Note that the function $f$ is defined on $\mathbb{R}_-$ and not all of $\mathbb{R}$ . Moreover $f$ looks like a lot $x \mapsto e^x$ . So one set for which $f$ is bounded is $A = \mathbb{N}$ , as well as $A=\{0\}$ . Moreover we need to find an equilibrium in $A$ in the following sense: if there are too many even numbers $f$ will not be bounded, and if there are too many odd numbers $f$ also can't be bounded. So I think there is a $K \in \mathbb{R}$ such that if we want $f$ to be bounded then the number of even numbers in $A$ is at most $K \times$ the number of odd numbers in $A$ . Thank you.","Let , . Find all subsets such that the function , is bounded. Note that the function is defined on and not all of . Moreover looks like a lot . So one set for which is bounded is , as well as . Moreover we need to find an equilibrium in in the following sense: if there are too many even numbers will not be bounded, and if there are too many odd numbers also can't be bounded. So I think there is a such that if we want to be bounded then the number of even numbers in is at most the number of odd numbers in . Thank you.","\mathbb{R}_- = \{ x \in \mathbb{R} \mid x \leq 0 \} \mathbb{N} = \{0,1, 2, ...\} A \subset \mathbb{N} f : \mathbb{R}_- \to \mathbb{R} f(x) := \sum_{n \in A} \frac{x^n}{n!} f \mathbb{R}_- \mathbb{R} f x \mapsto e^x f A = \mathbb{N} A=\{0\} A f f K \in \mathbb{R} f A K \times A","['real-analysis', 'sequences-and-series', 'limits', 'power-series']"
70,"About Theorem 5.13 in ""Principles of Mathematical Analysis"" by Walter Rudin L'Hospital's Rule L'Hopital's Rule","About Theorem 5.13 in ""Principles of Mathematical Analysis"" by Walter Rudin L'Hospital's Rule L'Hopital's Rule",,"I am reading ""Principles of Mathematical Analysis"" by Walter Rudin. Thank you Saaqib Mahmood. I copied and pasted your text Theorem 5.13 on p.109: Suppose $f$ and $g$ are real and differentiable in $(a, b)$ , and $g^\prime(x) \neq 0$ for all $x \in (a, b)$ , where $-\infty \leq a < b \leq +\infty$ . Suppose $$ \frac{f^\prime(x)}{g^\prime(x)} \to A \ \mbox{ as } \ x \to a. \tag{13} $$ If $$ f(x) \to 0 \ \mbox{ and } \ g(x) \to 0 \ \mbox{ as } \ x \to a, \tag{14} $$ or if $$ g(x) \to +\infty \ \mbox{ as } \ x \to a, \tag{15} $$ then $$ \frac{f(x)}{g(x)} \to A \ \mbox{ as } \ x \to a. \tag{16}$$ The analogous statement is of course also true if $x \to b$ , or if $g(x) \to -\infty$ in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33. Here is Definition 4.33: Let $f$ be a real function defined on $E \subset \mathbb{R}$ . We say that $$ f(t) \to A \ \mbox{ as } \ t \to x, $$ where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$ , $t \neq x$ . And, here is Rudin's proof: We first consider the case in which $-\infty \leq A < +\infty$ . Choose a real number $q$ such that $A < q$ , and then choose $r$ such that $A < r < q$ . By (13) there is a point $c \in (a, b)$ such that $a < x < c$ implies $$ \frac{ f^\prime(x) }{ g^\prime(x) } < r. \tag{17} $$ If $a < x < y < c$ , then Theorem 5.9 shows that there is a point $t \in (x, y)$ such that $$ \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18} $$ Suppose (14) holds. Letting $x \to a$ in (18), we see that $$ \frac{f(y)}{g(y)} \leq r < q \qquad \qquad \qquad  (a < y < c) \tag{19} $$ Next, suppose (15) holds. Keeping $y$ fixed in (18), we can choos a point $c_1 \in (a, y)$ such that $g(x) > g(y)$ and $g(x) > 0$ if $a < x < c_1$ . Multiplying (18) by $\left[ g(x)- g(y) \right]/g(x)$ , we obtain $$ \frac{ f(x) }{ g(x) } < r - r \frac{ g(y) }{g(x)} + \frac{f(y)}{g(x)} \qquad \qquad \qquad (a < x < c_1). \tag{20}$$ If we let $x \to a$ in (20), (15) shows that there is a point $c_2 \in \left( a, c_1 \right)$ such that $$ \frac{ f(x) }{ g(x) } < q \qquad \qquad \qquad (a < x < c_2 ). \tag{21} $$ Summing up, (19) and (21) show that for any $q$ , subject only to the condition $A < q$ , there is a point $c_2$ such that $f(x)/g(x) < q$ if $a < x < c_2$ . In the same manner, if $-\infty < A \leq +\infty$ , and $p$ is chosen so that $p < A$ , we can find a point $c_3$ such that $$ p < \frac{ f(x) }{ g(x) } \qquad \qquad \qquad ( a< x < c_3), \tag{22} $$ and (16) follows from these two statements. Rudin didn't write $g(x) - g(y) \neq 0$ for any $x, y$ such that $a < x < y < b$ in $$ \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18} $$ Is this fact so obvious? If so, please tell me the reason why this fact is so obvious. I didn't think this fact was so obvious, so I proved: By assumption, $g'(x) \neq 0$ on $(a, b)$ . Let $x, y$ be any real number such that $a < x < y < b$ . Let $x', y'$ be any real number such that $a < x' < x$ and $y < y' < b$ . Then, $g$ is a differentiable function on $[x', y']$ . By the Intermediate Value Theorem for derivatives (Theorem 5.12 on p.108), $g'(x) > 0$ for all $x \in [x', y']$ or $g'(x) < 0$ for all $x \in [x', y']$ . So, $g$ is strictly monotonically increasing on $[x', y']$ or $g$ is strictly monotonically decreasing on $[x', y']$ . So $g(x) < g(y)$ or $g(x) > g(y)$ . So, $g(x) - g(y) \neq 0$ .","I am reading ""Principles of Mathematical Analysis"" by Walter Rudin. Thank you Saaqib Mahmood. I copied and pasted your text Theorem 5.13 on p.109: Suppose and are real and differentiable in , and for all , where . Suppose If or if then The analogous statement is of course also true if , or if in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33. Here is Definition 4.33: Let be a real function defined on . We say that where and are in the extended real number system, if for every neighborhood of there is a neighborhood of such that is not empty, and such that for all , . And, here is Rudin's proof: We first consider the case in which . Choose a real number such that , and then choose such that . By (13) there is a point such that implies If , then Theorem 5.9 shows that there is a point such that Suppose (14) holds. Letting in (18), we see that Next, suppose (15) holds. Keeping fixed in (18), we can choos a point such that and if . Multiplying (18) by , we obtain If we let in (20), (15) shows that there is a point such that Summing up, (19) and (21) show that for any , subject only to the condition , there is a point such that if . In the same manner, if , and is chosen so that , we can find a point such that and (16) follows from these two statements. Rudin didn't write for any such that in Is this fact so obvious? If so, please tell me the reason why this fact is so obvious. I didn't think this fact was so obvious, so I proved: By assumption, on . Let be any real number such that . Let be any real number such that and . Then, is a differentiable function on . By the Intermediate Value Theorem for derivatives (Theorem 5.12 on p.108), for all or for all . So, is strictly monotonically increasing on or is strictly monotonically decreasing on . So or . So, .","f g (a, b) g^\prime(x) \neq 0 x \in (a, b) -\infty \leq a < b \leq +\infty  \frac{f^\prime(x)}{g^\prime(x)} \to A \ \mbox{ as } \ x \to a. \tag{13}   f(x) \to 0 \ \mbox{ and } \ g(x) \to 0 \ \mbox{ as } \ x \to a, \tag{14}   g(x) \to +\infty \ \mbox{ as } \ x \to a, \tag{15}   \frac{f(x)}{g(x)} \to A \ \mbox{ as } \ x \to a. \tag{16} x \to b g(x) \to -\infty f E \subset \mathbb{R}  f(t) \to A \ \mbox{ as } \ t \to x,  A x U A V x V \cap E f(t) \in U t \in V \cap E t \neq x -\infty \leq A < +\infty q A < q r A < r < q c \in (a, b) a < x < c  \frac{ f^\prime(x) }{ g^\prime(x) } < r. \tag{17}  a < x < y < c t \in (x, y)  \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18}  x \to a  \frac{f(y)}{g(y)} \leq r < q \qquad \qquad \qquad  (a < y < c) \tag{19}  y c_1 \in (a, y) g(x) > g(y) g(x) > 0 a < x < c_1 \left[ g(x)- g(y) \right]/g(x)  \frac{ f(x) }{ g(x) } < r - r \frac{ g(y) }{g(x)} + \frac{f(y)}{g(x)} \qquad \qquad \qquad (a < x < c_1). \tag{20} x \to a c_2 \in \left( a, c_1 \right)  \frac{ f(x) }{ g(x) } < q \qquad \qquad \qquad (a < x < c_2 ). \tag{21}  q A < q c_2 f(x)/g(x) < q a < x < c_2 -\infty < A \leq +\infty p p < A c_3  p < \frac{ f(x) }{ g(x) } \qquad \qquad \qquad ( a< x < c_3), \tag{22}  g(x) - g(y) \neq 0 x, y a < x < y < b  \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18}  g'(x) \neq 0 (a, b) x, y a < x < y < b x', y' a < x' < x y < y' < b g [x', y'] g'(x) > 0 x \in [x', y'] g'(x) < 0 x \in [x', y'] g [x', y'] g [x', y'] g(x) < g(y) g(x) > g(y) g(x) - g(y) \neq 0","['calculus', 'limits']"
71,Point-free notation for limits?,Point-free notation for limits?,,"When dealing with functions, there is usually both a point notation and a point-free notation. For instance: Arithmetic operations on functions: $f(x) + g(x)$ vs $f+g$ Function composition: $f(g(x))$ vs $f\circ g$ Derivative: $f'$ vs $\frac{d}{dx} f(x)$ Integration: $\int_{[a,b]} f$ vs $\int_a^b f(x)\, dx$ However, with the limit, except in one place I have always seen $\lim_{x\to a} f(x)$ or $\lim_{x\to a;\, x\in E} f(x)$ instead of (say) $\lim_{a} f$ or $\lim_{a;\, E} f$ . The exception is Spivak's book Calculus , which says (after noting that the limiting variable is a dummy variable): A more logical symbol would be something like $\displaystyle \lim_a f$ , but this notation, despite its brevity, is so infuriatingly rigid that almost no one has seriously tried to use it. The book then goes on to note that point notation allows one to work with anonymous functions and also to work with multiple variables (e.g. $\lim_{x\to a} (x+t^3)$ vs $\lim_{t \to a} (x+t^3)$ ). However, both of these reasons apply to point vs point-free notation in general, not just in the case of limits. Is there some deep (e.g. psychological) reason for limits being an exception in not having a widely-used point-free notation?","When dealing with functions, there is usually both a point notation and a point-free notation. For instance: Arithmetic operations on functions: vs Function composition: vs Derivative: vs Integration: vs However, with the limit, except in one place I have always seen or instead of (say) or . The exception is Spivak's book Calculus , which says (after noting that the limiting variable is a dummy variable): A more logical symbol would be something like , but this notation, despite its brevity, is so infuriatingly rigid that almost no one has seriously tried to use it. The book then goes on to note that point notation allows one to work with anonymous functions and also to work with multiple variables (e.g. vs ). However, both of these reasons apply to point vs point-free notation in general, not just in the case of limits. Is there some deep (e.g. psychological) reason for limits being an exception in not having a widely-used point-free notation?","f(x) + g(x) f+g f(g(x)) f\circ g f' \frac{d}{dx} f(x) \int_{[a,b]} f \int_a^b f(x)\, dx \lim_{x\to a} f(x) \lim_{x\to a;\, x\in E} f(x) \lim_{a} f \lim_{a;\, E} f \displaystyle \lim_a f \lim_{x\to a} (x+t^3) \lim_{t \to a} (x+t^3)","['limits', 'notation']"
72,"Limit of a recursive sequences involving the AM, GM, and HM (arithmetic-geometric-harmonic mean)","Limit of a recursive sequences involving the AM, GM, and HM (arithmetic-geometric-harmonic mean)",,"Let $x,y,z$ be positive real numbers. And let $\text{AM}$ , $\text{GM}$ , $\text{HM}$ respectively be the arithmetic mean, geometric mean, and harmonic mean. Define $$a_n=\text{AM}(a_{n-1},g_{n-1},h_{n-1}),$$ $$g_n=\text{GM}(a_{n-1},g_{n-1},h_{n-1}),$$ $$h_n=\text{HM}(a_{n-1},g_{n-1},h_{n-1})$$ with $a_0=x,$ $g_0=y,$ $h_0=z$ . I found that $$\lim\limits_{n\to\infty} a_n=\lim\limits_{n\to\infty} g_n=\lim\limits_{n\to\infty} h_n$$ This is an analog of other mean combinations such as the arithmetic-geometric mean and geometric-harmonic mean . The arithmetic-harmonic mean is just the geometric mean. Therefore this would be considered the arithmetic-geometric-harmonic mean (AGHM). Indeed, an analogous identity as seen with the GHM seems to hold: $$\text{AGHM}(x,y,z)=\frac{1}{\text{AGHM}(\frac1x,\frac1y,\frac1z)}$$ But how do we prove these things? Maybe unlikely, but can we find a closed form for the AGHM?  Could the AGHM have any application? Interesting: I found from experimentation that $\text{AGHM}(x,y,\sqrt{xy})=\sqrt{xy}$ .","Let be positive real numbers. And let , , respectively be the arithmetic mean, geometric mean, and harmonic mean. Define with . I found that This is an analog of other mean combinations such as the arithmetic-geometric mean and geometric-harmonic mean . The arithmetic-harmonic mean is just the geometric mean. Therefore this would be considered the arithmetic-geometric-harmonic mean (AGHM). Indeed, an analogous identity as seen with the GHM seems to hold: But how do we prove these things? Maybe unlikely, but can we find a closed form for the AGHM?  Could the AGHM have any application? Interesting: I found from experimentation that .","x,y,z \text{AM} \text{GM} \text{HM} a_n=\text{AM}(a_{n-1},g_{n-1},h_{n-1}), g_n=\text{GM}(a_{n-1},g_{n-1},h_{n-1}), h_n=\text{HM}(a_{n-1},g_{n-1},h_{n-1}) a_0=x, g_0=y, h_0=z \lim\limits_{n\to\infty} a_n=\lim\limits_{n\to\infty} g_n=\lim\limits_{n\to\infty} h_n \text{AGHM}(x,y,z)=\frac{1}{\text{AGHM}(\frac1x,\frac1y,\frac1z)} \text{AGHM}(x,y,\sqrt{xy})=\sqrt{xy}","['limits', 'recurrence-relations', 'recursion', 'average', 'means']"
73,Evaluate a limit (and prove it exists),Evaluate a limit (and prove it exists),,"I am unable to evaluate the following limit, nor is my computer algebra system able to produce a float approximation. But the graphical plot it produces shows a pattern that is indicative of convergence to a specific constant value, and I realise that this is by no means a guarantee that indeed the limit exists, but it does allow us to eliminate a few of the commonly known types of divergence.(see figure 1 below): $$\lim_{n \rightarrow \infty}\Biggl(\frac{1}{2n}\Biggl\lfloor\ln(2) \Bigl(\Bigl\lfloor\frac{3^{1/3}n}{3^{1/3}-1}\Bigr\rfloor+1\Bigr)\Biggr\rfloor\Biggr)$$ Also for the sake of a sort of comparative analysis, see the function plotted in figure 2, which likewise is very suggestive of having a limit as $n$ is made infinite. relevant question previously asked","I am unable to evaluate the following limit, nor is my computer algebra system able to produce a float approximation. But the graphical plot it produces shows a pattern that is indicative of convergence to a specific constant value, and I realise that this is by no means a guarantee that indeed the limit exists, but it does allow us to eliminate a few of the commonly known types of divergence.(see figure 1 below): Also for the sake of a sort of comparative analysis, see the function plotted in figure 2, which likewise is very suggestive of having a limit as is made infinite. relevant question previously asked",\lim_{n \rightarrow \infty}\Biggl(\frac{1}{2n}\Biggl\lfloor\ln(2) \Bigl(\Bigl\lfloor\frac{3^{1/3}n}{3^{1/3}-1}\Bigr\rfloor+1\Bigr)\Biggr\rfloor\Biggr) n,"['real-analysis', 'limits']"
74,Estimate on Limit of Recursive Sequence,Estimate on Limit of Recursive Sequence,,"How can I estimate (via a lower bound) the limit of the recursive sequence $$P_{n+1}=P_n-\frac{C(P_n-1)^2}{(2^n+C)(P_n+C2^{-n})}$$ where $0<C<1$ and $1<P_0<2$ . Let $P_{\infty}=\lim_{n\to\infty}P_n$ . The best estimate I can get is $$\begin{aligned} P_{\infty}-P_0 &=-\sum_{n=0}^{\infty}\frac{C(P_n-1)^2}{(2^n+C)(P_n+C2^{-n})} \\ &> -\sum_{n=0}^{\infty}\frac{C(P_n-1)^2}{2^nP_n} \\ &> -\frac{(P_0-1)^2}{P_0}\sum_{n=0}^{\infty}\frac{C}{2^n} \end{aligned}$$ where from line 2 to 3 I use the fact that $(P_n)$ is decreasing. Hence $$P_{\infty}> P_0-2C\frac{(P_0-1)^2}{P_0}$$ This is a fairly decent bound when looking at the absolute difference, but for my application I need as much precision as possible so I'm hoping to find a better lower bound. Special functions are acceptable if they arise. Edit : As per Wolfram, we have that $$\begin{aligned} &\sum_{n=0}^{\infty}\frac{C(P_0-1)^2}{(2^n+C)(P_0+C2^{-n})} \\ =&\frac{P_0-1}{\ln(2)}\Big(\psi_2(\log_2(-1/C))-\psi_2(\log_2(-P_0/C))\Big) \end{aligned}$$ which gives $$P_{\infty}>P_0-\frac{P_0-1}{\ln(2)}\Big(\psi_2(\log_2(-1/C))-\psi_2(\log_2(-P_0/C))\Big)$$","How can I estimate (via a lower bound) the limit of the recursive sequence where and . Let . The best estimate I can get is where from line 2 to 3 I use the fact that is decreasing. Hence This is a fairly decent bound when looking at the absolute difference, but for my application I need as much precision as possible so I'm hoping to find a better lower bound. Special functions are acceptable if they arise. Edit : As per Wolfram, we have that which gives","P_{n+1}=P_n-\frac{C(P_n-1)^2}{(2^n+C)(P_n+C2^{-n})} 0<C<1 1<P_0<2 P_{\infty}=\lim_{n\to\infty}P_n \begin{aligned}
P_{\infty}-P_0 &=-\sum_{n=0}^{\infty}\frac{C(P_n-1)^2}{(2^n+C)(P_n+C2^{-n})} \\
&> -\sum_{n=0}^{\infty}\frac{C(P_n-1)^2}{2^nP_n} \\
&> -\frac{(P_0-1)^2}{P_0}\sum_{n=0}^{\infty}\frac{C}{2^n}
\end{aligned} (P_n) P_{\infty}> P_0-2C\frac{(P_0-1)^2}{P_0} \begin{aligned}
&\sum_{n=0}^{\infty}\frac{C(P_0-1)^2}{(2^n+C)(P_0+C2^{-n})} \\
=&\frac{P_0-1}{\ln(2)}\Big(\psi_2(\log_2(-1/C))-\psi_2(\log_2(-P_0/C))\Big)
\end{aligned} P_{\infty}>P_0-\frac{P_0-1}{\ln(2)}\Big(\psi_2(\log_2(-1/C))-\psi_2(\log_2(-P_0/C))\Big)","['sequences-and-series', 'limits', 'recurrence-relations', 'estimation']"
75,Prove $\lim\limits_{x \to \pm\infty}\dfrac{x^3+1}{x^2+1}=\infty$ by the definition.,Prove  by the definition.,\lim\limits_{x \to \pm\infty}\dfrac{x^3+1}{x^2+1}=\infty,"Problem Prove $\lim\limits_{x \to \pm\infty}\dfrac{x^3+1}{x^2+1}=\infty$ by the definition. Note: The problem asks us to prove that, no matter $x \to +\infty$ or $x \to -\infty$ , the limit is $\infty$ ,which may be $+\infty$ or $-\infty.$ Proof $\forall M>0$ , $\exists X=\max(1,M+1)>0, \forall|x|>X$ ： \begin{align*} \left|\frac{x^3+1}{x^2+1}\right|&=\left|x-\frac{x-1}{x^2+1}\right|\\&\geq |x|-\left|\frac{x-1}{x^2+1}\right|\\&\geq |x|-\frac{|x|+1}{x^2+1}\\&\geq |x|-\frac{x^2+1}{x^2+1}\\&=|x|-1\\&>X-1\\&\geq M. \end{align*} Please verify the proof above.","Problem Prove by the definition. Note: The problem asks us to prove that, no matter or , the limit is ,which may be or Proof , ： Please verify the proof above.","\lim\limits_{x \to \pm\infty}\dfrac{x^3+1}{x^2+1}=\infty x \to +\infty x \to -\infty \infty +\infty -\infty. \forall M>0 \exists X=\max(1,M+1)>0, \forall|x|>X \begin{align*}
\left|\frac{x^3+1}{x^2+1}\right|&=\left|x-\frac{x-1}{x^2+1}\right|\\&\geq |x|-\left|\frac{x-1}{x^2+1}\right|\\&\geq |x|-\frac{|x|+1}{x^2+1}\\&\geq |x|-\frac{x^2+1}{x^2+1}\\&=|x|-1\\&>X-1\\&\geq M.
\end{align*}","['real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
76,"Prob. 7, Sec. 6.1, in Bartle & Sherbert's INTRO TO REAL ANALYSIS: A necessary and sufficient condition for the existence of $\big(|f|\big)^\prime$","Prob. 7, Sec. 6.1, in Bartle & Sherbert's INTRO TO REAL ANALYSIS: A necessary and sufficient condition for the existence of",\big(|f|\big)^\prime,"Here is Prob. 7, Sec. 6.1, in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Suppose that $f \colon \mathbb{R} \to \mathbb{R}$ is differentiable at $c$ and that $f(c) = 0$. Show that $g(x) \colon= \lvert f(x) \rvert$ is differentialbe at $c$ if and only if $f^\prime(c)=0$. My Attempt: Suppose that $f^\prime(c)=0$. Then, given a real number $\varepsilon > 0$, we can find a real number $\delta > 0$ such that    $$ \left\lvert \frac{ f(x) - f(c) }{ x -c } \right\rvert = \left\lvert \frac{ f(x)  }{ x -c } \right\rvert < \varepsilon $$    for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta. $$ Therefore for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta, $$ we find that    $$ \left\lvert \frac{g(x) - g(c) }{x-c} - 0 \right\rvert = \left\lvert \frac{ \lvert f(x) \rvert - \lvert f(c) \rvert }{ x-c} \right\rvert = \left\lvert \frac{ \lvert f(x) \rvert  }{ x-c} \right\rvert = \frac{ \lvert f(x) \rvert  }{ \lvert x-c \rvert } = \left\lvert \frac{ f(x)  }{ x -c } \right\rvert < \varepsilon. $$    Since $\varepsilon > 0$ was arbitrary, it follows that $g$ is differentiable at $c$ and that   $$ g^\prime(c) = 0. $$ Conversely, suppose that $f^\prime(c) \neq 0$. Then either $f^\prime(c) < 0$ or $f^\prime(c) > 0$. Case 1. If $f^\prime(c) > 0$, then for $\varepsilon \colon= f^\prime(c)/2$, we can find a real number $\delta > 0$ such that    $$ \left\lvert \frac{ f(x) - f(c) }{ x -c } - f^\prime(c) \right\rvert < f^\prime(c)/2 $$   or    $$ 0 < \frac{f^\prime(c)}{2} =  f^\prime(c) - f^\prime(c)/2 <  \frac{ f(x) - f(c) }{ x -c } < f^\prime(c) + f^\prime(c)/2 $$   for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta. $$ But $f(c) = 0$.  Thus for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta, $$    we have    $$ \frac{ f(x) }{x-c} > \frac{f^\prime(c)}{2} > 0, \tag{1} $$    which implies that, for all $x \in \mathbb{R}$,    $$ f(x) \ \begin{cases}   > 0 \ \mbox{ if } & c < x < c + \delta, \\  < 0 \ \mbox{ if } & c-\delta < x < c. \end{cases} $$   So from (1) it follows that    $$ \frac{g(x) - g(c) }{x-c} = \frac{ \lvert f(x) \rvert }{ x-c} = \begin{cases} \frac{f(x)}{x-c} & \mbox{ if } \ c < x < c+\delta, \\ -\frac{ f(x)}{x-c} & \mbox{ if } \  c-\delta < x < c. \end{cases} \tag{2}$$   Moreover, from (2) we can also conclude that    $$ \lim_{x \to c+} \frac{g(x) - g(c)}{x-c} = \lim_{x \to c+} \frac{ f(x) }{x-c} = \lim_{x \to c+} \frac{ f(x) - f(c) }{x-c} = f^\prime(c), $$   and    $$ \lim_{x \to c-} \frac{g(x) - g(c)}{x-c} = \lim_{x \to c-}\left(- \frac{ f(x) }{x-c} \right) = \lim_{x \to c-} \left( -  \frac{ f(x) - f(c) }{x-c}\right) = - \lim_{x \to c-}   \frac{ f(x) - f(c) }{x-c}=- f^\prime(c). $$   Thus if $f^\prime(c) > 0$, then   $$ \lim_{x \to c+} \frac{g(x) - g(c)}{x-c} \neq \lim_{x \to c-} \frac{g(x) - g(c)}{x-c}, $$   and so $g^\prime(c)$ does not exist. Therefore if $g$ is differentiable at $c$, then we must have $f^\prime(c) \not> 0$. Case 2. If $f^\prime(c) < 0$, then for $\varepsilon \colon= -f^\prime(c)/2 > 0$, we can find a real number $\delta > 0$ such that    $$ \left\lvert \frac{ f(x) - f(c) }{ x -c } - f^\prime(c) \right\rvert < - f^\prime(c)/2 $$   or    $$ \frac{3f^\prime(c)}{2} =  f^\prime(c) - \frac{-f^\prime(c)}{2} <  \frac{ f(x) - f(c) }{ x -c } < f^\prime(c) + \frac{-f^\prime(c)}{2} = \frac{ f^\prime(c)}{2} < 0 $$   for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta. $$ But $f(c) = 0$.  Thus for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta, $$    we have    $$ \frac{ f(x) }{x-c} < \frac{f^\prime(c)}{2} < 0, \tag{3} $$    which implies that, for all $x \in \mathbb{R}$,    $$ f(x) \ \begin{cases}   < 0 \ \mbox{ if } & c < x < c + \delta, \\  > 0 \ \mbox{ if } & c-\delta < x < c. \end{cases} $$   So from (3) it follows that    $$ \frac{g(x) - g(c) }{x-c} = \frac{ \lvert f(x) \rvert }{ x-c} = \begin{cases} -\frac{f(x)}{x-c} & \mbox{ if } \ c < x < c+\delta, \\ \frac{ f(x)}{x-c} & \mbox{ if } \  c-\delta < x < c. \end{cases} \tag{4}$$   Moreover, from (4) we can also conclude that    $$ \lim_{x \to c-} \frac{g(x) - g(c)}{x-c} = \lim_{x \to c-} \frac{ f(x) }{x-c} = \lim_{x \to c-} \frac{ f(x) - f(c) }{x-c} = f^\prime(c), $$   and    $$ \lim_{x \to c+} \frac{g(x) - g(c)}{x-c} = \lim_{x \to c+}\left(- \frac{ f(x) }{x-c} \right) = \lim_{x \to c+} \left( -  \frac{ f(x) - f(c) }{x-c}\right) = - \lim_{x \to c+}   \frac{ f(x) - f(c) }{x-c}=- f^\prime(c). $$   Thus if $f^\prime(c) < 0$, then   $$ \lim_{x \to c+} \frac{g(x) - g(c)}{x-c} \neq \lim_{x \to c-} \frac{g(x) - g(c)}{x-c}, $$   and so $g^\prime(c)$ does not exist. Therefore if $g$ is differentiable at $c$, then we must have $f^\prime(c) \not< 0$. From the above two cases, we can conclude that if $g$ is differentiable at $c$, then we must have $f^\prime(c)=0$. Is this proof correct? If so, then is the presentation clear and rigorous enough too? If not, then where are the issues as far as accuracy, rigor, or clarity of the argument go?","Here is Prob. 7, Sec. 6.1, in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Suppose that $f \colon \mathbb{R} \to \mathbb{R}$ is differentiable at $c$ and that $f(c) = 0$. Show that $g(x) \colon= \lvert f(x) \rvert$ is differentialbe at $c$ if and only if $f^\prime(c)=0$. My Attempt: Suppose that $f^\prime(c)=0$. Then, given a real number $\varepsilon > 0$, we can find a real number $\delta > 0$ such that    $$ \left\lvert \frac{ f(x) - f(c) }{ x -c } \right\rvert = \left\lvert \frac{ f(x)  }{ x -c } \right\rvert < \varepsilon $$    for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta. $$ Therefore for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta, $$ we find that    $$ \left\lvert \frac{g(x) - g(c) }{x-c} - 0 \right\rvert = \left\lvert \frac{ \lvert f(x) \rvert - \lvert f(c) \rvert }{ x-c} \right\rvert = \left\lvert \frac{ \lvert f(x) \rvert  }{ x-c} \right\rvert = \frac{ \lvert f(x) \rvert  }{ \lvert x-c \rvert } = \left\lvert \frac{ f(x)  }{ x -c } \right\rvert < \varepsilon. $$    Since $\varepsilon > 0$ was arbitrary, it follows that $g$ is differentiable at $c$ and that   $$ g^\prime(c) = 0. $$ Conversely, suppose that $f^\prime(c) \neq 0$. Then either $f^\prime(c) < 0$ or $f^\prime(c) > 0$. Case 1. If $f^\prime(c) > 0$, then for $\varepsilon \colon= f^\prime(c)/2$, we can find a real number $\delta > 0$ such that    $$ \left\lvert \frac{ f(x) - f(c) }{ x -c } - f^\prime(c) \right\rvert < f^\prime(c)/2 $$   or    $$ 0 < \frac{f^\prime(c)}{2} =  f^\prime(c) - f^\prime(c)/2 <  \frac{ f(x) - f(c) }{ x -c } < f^\prime(c) + f^\prime(c)/2 $$   for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta. $$ But $f(c) = 0$.  Thus for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta, $$    we have    $$ \frac{ f(x) }{x-c} > \frac{f^\prime(c)}{2} > 0, \tag{1} $$    which implies that, for all $x \in \mathbb{R}$,    $$ f(x) \ \begin{cases}   > 0 \ \mbox{ if } & c < x < c + \delta, \\  < 0 \ \mbox{ if } & c-\delta < x < c. \end{cases} $$   So from (1) it follows that    $$ \frac{g(x) - g(c) }{x-c} = \frac{ \lvert f(x) \rvert }{ x-c} = \begin{cases} \frac{f(x)}{x-c} & \mbox{ if } \ c < x < c+\delta, \\ -\frac{ f(x)}{x-c} & \mbox{ if } \  c-\delta < x < c. \end{cases} \tag{2}$$   Moreover, from (2) we can also conclude that    $$ \lim_{x \to c+} \frac{g(x) - g(c)}{x-c} = \lim_{x \to c+} \frac{ f(x) }{x-c} = \lim_{x \to c+} \frac{ f(x) - f(c) }{x-c} = f^\prime(c), $$   and    $$ \lim_{x \to c-} \frac{g(x) - g(c)}{x-c} = \lim_{x \to c-}\left(- \frac{ f(x) }{x-c} \right) = \lim_{x \to c-} \left( -  \frac{ f(x) - f(c) }{x-c}\right) = - \lim_{x \to c-}   \frac{ f(x) - f(c) }{x-c}=- f^\prime(c). $$   Thus if $f^\prime(c) > 0$, then   $$ \lim_{x \to c+} \frac{g(x) - g(c)}{x-c} \neq \lim_{x \to c-} \frac{g(x) - g(c)}{x-c}, $$   and so $g^\prime(c)$ does not exist. Therefore if $g$ is differentiable at $c$, then we must have $f^\prime(c) \not> 0$. Case 2. If $f^\prime(c) < 0$, then for $\varepsilon \colon= -f^\prime(c)/2 > 0$, we can find a real number $\delta > 0$ such that    $$ \left\lvert \frac{ f(x) - f(c) }{ x -c } - f^\prime(c) \right\rvert < - f^\prime(c)/2 $$   or    $$ \frac{3f^\prime(c)}{2} =  f^\prime(c) - \frac{-f^\prime(c)}{2} <  \frac{ f(x) - f(c) }{ x -c } < f^\prime(c) + \frac{-f^\prime(c)}{2} = \frac{ f^\prime(c)}{2} < 0 $$   for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta. $$ But $f(c) = 0$.  Thus for all $x \in \mathbb{R}$ which satisfy    $$ 0 < \lvert x-c \rvert < \delta, $$    we have    $$ \frac{ f(x) }{x-c} < \frac{f^\prime(c)}{2} < 0, \tag{3} $$    which implies that, for all $x \in \mathbb{R}$,    $$ f(x) \ \begin{cases}   < 0 \ \mbox{ if } & c < x < c + \delta, \\  > 0 \ \mbox{ if } & c-\delta < x < c. \end{cases} $$   So from (3) it follows that    $$ \frac{g(x) - g(c) }{x-c} = \frac{ \lvert f(x) \rvert }{ x-c} = \begin{cases} -\frac{f(x)}{x-c} & \mbox{ if } \ c < x < c+\delta, \\ \frac{ f(x)}{x-c} & \mbox{ if } \  c-\delta < x < c. \end{cases} \tag{4}$$   Moreover, from (4) we can also conclude that    $$ \lim_{x \to c-} \frac{g(x) - g(c)}{x-c} = \lim_{x \to c-} \frac{ f(x) }{x-c} = \lim_{x \to c-} \frac{ f(x) - f(c) }{x-c} = f^\prime(c), $$   and    $$ \lim_{x \to c+} \frac{g(x) - g(c)}{x-c} = \lim_{x \to c+}\left(- \frac{ f(x) }{x-c} \right) = \lim_{x \to c+} \left( -  \frac{ f(x) - f(c) }{x-c}\right) = - \lim_{x \to c+}   \frac{ f(x) - f(c) }{x-c}=- f^\prime(c). $$   Thus if $f^\prime(c) < 0$, then   $$ \lim_{x \to c+} \frac{g(x) - g(c)}{x-c} \neq \lim_{x \to c-} \frac{g(x) - g(c)}{x-c}, $$   and so $g^\prime(c)$ does not exist. Therefore if $g$ is differentiable at $c$, then we must have $f^\prime(c) \not< 0$. From the above two cases, we can conclude that if $g$ is differentiable at $c$, then we must have $f^\prime(c)=0$. Is this proof correct? If so, then is the presentation clear and rigorous enough too? If not, then where are the issues as far as accuracy, rigor, or clarity of the argument go?",,"['real-analysis', 'limits', 'analysis', 'derivatives', 'proof-verification']"
77,Surface integral enclosing all of space? Improper surface integral convergence?,Surface integral enclosing all of space? Improper surface integral convergence?,,"Consider a surface integral over a closed surface $S$ in $\mathbb{R}^3$ $$ \iint_S V\vec{E} \cdot d\vec{S}$$ (I'm writing out $V$ and $\vec{E}$ to make connection with physics textbooks, and to show the motivation behind this question, where $\vec{E}$ is the electric field of a continuous charge distribution and $V$ is the corresponding potential. Let's think of the integrand as a function $V\vec{E} = \vec{F}: \mathbb{R}^3 \to \mathbb{R}^3$) (What follows isn't even my main question, but it would be useful to setup in order to answer my main question on convergence seen further below) How do you mathematically specify that you want to expand the surface so that it encloses all of space? I would like an 'improper surface integral', in the same way that we have improper 1-dimensional integrals $$ \int_a^\infty f(x) dx = \lim_{R\to \infty} \int_a^R f(x) dx$$ Is there any such notation or is $$ \iint _{\text{surface enclosing all space}} \vec{F}\cdot d\vec{S}$$ the best you can do? Technically such a surface doesn't exist? You need some sort of limit to keep expanding the surface further and further? To phrase this initial notation question in another way, consider a parameterization of the closed surface $g(\theta, \phi): \mathcal{D}\subset \mathbb{R}^2 \to \mathbb{R}^3$ In particular, consider a spherical coordinates parameterization of the surface $g(\theta, \phi) = r(\theta, \phi) \hat{r} + 0\hat{\theta} + 0\hat{\phi}$ where $r$ outputs a radial distance from the origin. How do I show that I want to ""push"" the output $r$, for every given $(\theta, \phi)$ pair, out to infinity so that my surface encloses all of space? There is no unique way to push this surface outwards. It seems like I'd have to give a sequence of functions that take me from the initial enclosing radial function $r_1$ to some final radial function $r_2$ which encloses all of space? Question on convergence Given that we have some sort of notation for a surface enclosing all of space, how do we evaluate if the surface integral converges or not over this limiting sequence? In particular, lets consider an integrand which physics textbooks claim make the integral $0$. They argue (in words, which I will try to translate into math) given $$ \lim_{r\to\infty}\vec{F}(r,\theta,\phi) \sim \frac{1}{r^3}\hat{r}$$ i.e. integrand is asymptotically equivalent to the vector field $(1/r^3) \hat{r}$ at large distances, the surface integral over all space is $0$ since $d\vec{S}$ goes as $r^2$ ($d\vec{S} = r^2\sin\theta\; d\theta \;d\phi \;\hat{r}$). Is there mathematical rigor behind this statement? Considering my notation and thoughts above, it seems like the mathematical problem would be phrased as $$ \lim_{\text{sequence of parameterizations} \;g_i} \iint_{g_i} \vec{F}(g(\theta,\phi)) \cdot |g_{\theta} \times g_{\phi}| \; d\theta \; d\phi $$ where $$ g_{\theta} = \frac{\partial r}{\partial \theta}\hat{r} + r \hat{\theta}$$ $$ g_{\phi} = \frac{\partial r}{\partial \phi}\hat{r} + r \sin\theta\hat{\phi}$$ where I have used that $\partial \hat{r}/ \partial \theta = \hat{\theta}$ and $\partial \hat{r}/\partial \phi = \sin\theta \hat{\phi}$. The cross product yields $$ g_{\theta} \times g_{\phi} = \hat{r}(r^2\sin\theta) + \hat{\theta}(-r\frac{\partial r}{\partial \theta}\sin\theta) + \hat{\phi}(-r\frac{\partial r}{\partial \phi}) $$ Therefore if we could somehow get the limit inside the integral, to turn $\vec{F}$ to $\hat{r}/r^3$, the dot product would result in the integral $$ \lim_{\text{sequence of} g_i}\iint_{g_i} \frac{1}{r^3}r^2\sin\theta d\theta d\phi$$ (Can you make that replacement?) And I guess this now goes to $0$? So in summary, is my notation/thoughts on pushing a surface out to infinity so that it encloses all of $\mathbb{R}^3$ valid/the way to go? What is the mathematical proof behind improper surface integrals converging to $0$? The convergence should be defined so that it is independent of ""path"" right? Meaning, it doesn't matter what sequence of $g_i$'s I choose? I might as well start with a sphere radius $a$ and just let $a$ tend to infinity? Whatever value I get (say it converges), I hope that it's the same value for all other sequences of $g_i$'s.","Consider a surface integral over a closed surface $S$ in $\mathbb{R}^3$ $$ \iint_S V\vec{E} \cdot d\vec{S}$$ (I'm writing out $V$ and $\vec{E}$ to make connection with physics textbooks, and to show the motivation behind this question, where $\vec{E}$ is the electric field of a continuous charge distribution and $V$ is the corresponding potential. Let's think of the integrand as a function $V\vec{E} = \vec{F}: \mathbb{R}^3 \to \mathbb{R}^3$) (What follows isn't even my main question, but it would be useful to setup in order to answer my main question on convergence seen further below) How do you mathematically specify that you want to expand the surface so that it encloses all of space? I would like an 'improper surface integral', in the same way that we have improper 1-dimensional integrals $$ \int_a^\infty f(x) dx = \lim_{R\to \infty} \int_a^R f(x) dx$$ Is there any such notation or is $$ \iint _{\text{surface enclosing all space}} \vec{F}\cdot d\vec{S}$$ the best you can do? Technically such a surface doesn't exist? You need some sort of limit to keep expanding the surface further and further? To phrase this initial notation question in another way, consider a parameterization of the closed surface $g(\theta, \phi): \mathcal{D}\subset \mathbb{R}^2 \to \mathbb{R}^3$ In particular, consider a spherical coordinates parameterization of the surface $g(\theta, \phi) = r(\theta, \phi) \hat{r} + 0\hat{\theta} + 0\hat{\phi}$ where $r$ outputs a radial distance from the origin. How do I show that I want to ""push"" the output $r$, for every given $(\theta, \phi)$ pair, out to infinity so that my surface encloses all of space? There is no unique way to push this surface outwards. It seems like I'd have to give a sequence of functions that take me from the initial enclosing radial function $r_1$ to some final radial function $r_2$ which encloses all of space? Question on convergence Given that we have some sort of notation for a surface enclosing all of space, how do we evaluate if the surface integral converges or not over this limiting sequence? In particular, lets consider an integrand which physics textbooks claim make the integral $0$. They argue (in words, which I will try to translate into math) given $$ \lim_{r\to\infty}\vec{F}(r,\theta,\phi) \sim \frac{1}{r^3}\hat{r}$$ i.e. integrand is asymptotically equivalent to the vector field $(1/r^3) \hat{r}$ at large distances, the surface integral over all space is $0$ since $d\vec{S}$ goes as $r^2$ ($d\vec{S} = r^2\sin\theta\; d\theta \;d\phi \;\hat{r}$). Is there mathematical rigor behind this statement? Considering my notation and thoughts above, it seems like the mathematical problem would be phrased as $$ \lim_{\text{sequence of parameterizations} \;g_i} \iint_{g_i} \vec{F}(g(\theta,\phi)) \cdot |g_{\theta} \times g_{\phi}| \; d\theta \; d\phi $$ where $$ g_{\theta} = \frac{\partial r}{\partial \theta}\hat{r} + r \hat{\theta}$$ $$ g_{\phi} = \frac{\partial r}{\partial \phi}\hat{r} + r \sin\theta\hat{\phi}$$ where I have used that $\partial \hat{r}/ \partial \theta = \hat{\theta}$ and $\partial \hat{r}/\partial \phi = \sin\theta \hat{\phi}$. The cross product yields $$ g_{\theta} \times g_{\phi} = \hat{r}(r^2\sin\theta) + \hat{\theta}(-r\frac{\partial r}{\partial \theta}\sin\theta) + \hat{\phi}(-r\frac{\partial r}{\partial \phi}) $$ Therefore if we could somehow get the limit inside the integral, to turn $\vec{F}$ to $\hat{r}/r^3$, the dot product would result in the integral $$ \lim_{\text{sequence of} g_i}\iint_{g_i} \frac{1}{r^3}r^2\sin\theta d\theta d\phi$$ (Can you make that replacement?) And I guess this now goes to $0$? So in summary, is my notation/thoughts on pushing a surface out to infinity so that it encloses all of $\mathbb{R}^3$ valid/the way to go? What is the mathematical proof behind improper surface integrals converging to $0$? The convergence should be defined so that it is independent of ""path"" right? Meaning, it doesn't matter what sequence of $g_i$'s I choose? I might as well start with a sphere radius $a$ and just let $a$ tend to infinity? Whatever value I get (say it converges), I hope that it's the same value for all other sequences of $g_i$'s.",,"['calculus', 'real-analysis', 'limits', 'notation', 'surface-integrals']"
78,Does a function have to be defined in a neighborhood of a point in order for a limit to exist?,Does a function have to be defined in a neighborhood of a point in order for a limit to exist?,,"I came across the following limit $$\lim_{(x,y)\to(0,0)} \frac{y \sqrt{x}}{\sqrt{x^2 + y^2}}$$ If $x\geq0$ then $$0 \leq \bigg| \frac{y \sqrt{x}}{\sqrt{x^2 + y^2}}\bigg|= \frac{|y| \sqrt{x}}{\sqrt{x^2 + y^2}} \leq \sqrt{x}$$ So using the squeeze theorem the limit is equal to zero. However for any neighborhood $V$ of $(0,0)$ there are points where $x \lt 0$ and therefore $f$ is not defined . Does it mean that the limit doesn't exist, or should we only consider the points where $f$ is defined?","I came across the following limit $$\lim_{(x,y)\to(0,0)} \frac{y \sqrt{x}}{\sqrt{x^2 + y^2}}$$ If $x\geq0$ then $$0 \leq \bigg| \frac{y \sqrt{x}}{\sqrt{x^2 + y^2}}\bigg|= \frac{|y| \sqrt{x}}{\sqrt{x^2 + y^2}} \leq \sqrt{x}$$ So using the squeeze theorem the limit is equal to zero. However for any neighborhood $V$ of $(0,0)$ there are points where $x \lt 0$ and therefore $f$ is not defined . Does it mean that the limit doesn't exist, or should we only consider the points where $f$ is defined?",,"['real-analysis', 'limits', 'multivariable-calculus']"
79,limit of sequence involving the fractional part,limit of sequence involving the fractional part,,"Using the pigeonhole principle, any sequence of the form $(\{\frac{n}{r}\})_{n\geq1}$ where $r$ is an irrational number is dense in the unit interval. Then prove that the following limit does not exit in $[0;\infty]$ $$\lim_{n\to\infty}n\bigg\{\frac{n}{r}\bigg\}$$","Using the pigeonhole principle, any sequence of the form $(\{\frac{n}{r}\})_{n\geq1}$ where $r$ is an irrational number is dense in the unit interval. Then prove that the following limit does not exit in $[0;\infty]$ $$\lim_{n\to\infty}n\bigg\{\frac{n}{r}\bigg\}$$",,"['sequences-and-series', 'limits', 'fractional-part']"
80,A convergent sum of a divergent and convergent sequence?,A convergent sum of a divergent and convergent sequence?,,"Is the following argument correct? Sequences $(x_n)$ and $(y_n)$, where $(x_n)$ converges, $(y_n)$   diverges, and $(x_n+y_n)$ converges. Proof. The request in question is impossible. Assume that we have sequences $(x_n)$ and $(y_n)$ such that $(x_n)\to \alpha$, $(y_n)$ diverges and yet $(x_n+y_n)\to\beta$ where $x,\alpha\in\mathbf{R}$. Then by combining the first two propositions of theorem $\textbf{2.3.3}$ we have $\lim(y_n) = \lim((x_n+y_n)-x_n) = \beta-\alpha$, contradicting our assumption that $(y_n)$ was not convergent. $\blacksquare$ Note: The propositions in question are that Given $(a_n)\to a$ and $(b_n)\to b$ it follows that $\lim ca_n = ca,\forall c\in\mathbf{R}$ and $\lim(a_n+b_n) = \lim a_n+\lim b_n$","Is the following argument correct? Sequences $(x_n)$ and $(y_n)$, where $(x_n)$ converges, $(y_n)$   diverges, and $(x_n+y_n)$ converges. Proof. The request in question is impossible. Assume that we have sequences $(x_n)$ and $(y_n)$ such that $(x_n)\to \alpha$, $(y_n)$ diverges and yet $(x_n+y_n)\to\beta$ where $x,\alpha\in\mathbf{R}$. Then by combining the first two propositions of theorem $\textbf{2.3.3}$ we have $\lim(y_n) = \lim((x_n+y_n)-x_n) = \beta-\alpha$, contradicting our assumption that $(y_n)$ was not convergent. $\blacksquare$ Note: The propositions in question are that Given $(a_n)\to a$ and $(b_n)\to b$ it follows that $\lim ca_n = ca,\forall c\in\mathbf{R}$ and $\lim(a_n+b_n) = \lim a_n+\lim b_n$",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'convergence-divergence']"
81,Is it always possible to calculate the limit of an elementary function.,Is it always possible to calculate the limit of an elementary function.,,"Let be more precise: define an ""strong elementary function"" by only admitting rational for the ""constant function"" in the usual definition of ""elementary"" function (see for example: https://en.wikipedia.org/wiki/Elementary_function ). Let $a$ be an ""elementary real"" if the constant function $f(x)=a$ is a strong elementary function. With this definition some non rational reals are elementary (for example $\pi = 4\cdot \arctan(1)$); but there are reals that are not elementary. Now let $f(x)$ be a strong elementary function defined in an open interval of an elementary real $a$ with the possible exception of $a$. Suppose that $$\lim_{x\rightarrow a} f(x)$$ exists. Is this limit necessarily an elementary real? The idea behind this question is the following: when we learn to calculate limits in elementary calculus, it seems that there is always a method to do it. By calculating a limit we mean to define it by means of the rationals and elementary functions. But is there a general argument that prove that it is always possible?","Let be more precise: define an ""strong elementary function"" by only admitting rational for the ""constant function"" in the usual definition of ""elementary"" function (see for example: https://en.wikipedia.org/wiki/Elementary_function ). Let $a$ be an ""elementary real"" if the constant function $f(x)=a$ is a strong elementary function. With this definition some non rational reals are elementary (for example $\pi = 4\cdot \arctan(1)$); but there are reals that are not elementary. Now let $f(x)$ be a strong elementary function defined in an open interval of an elementary real $a$ with the possible exception of $a$. Suppose that $$\lim_{x\rightarrow a} f(x)$$ exists. Is this limit necessarily an elementary real? The idea behind this question is the following: when we learn to calculate limits in elementary calculus, it seems that there is always a method to do it. By calculating a limit we mean to define it by means of the rationals and elementary functions. But is there a general argument that prove that it is always possible?",,"['calculus', 'limits']"
82,Integral with Riemann Zeta Function. Trying to understand behavior,Integral with Riemann Zeta Function. Trying to understand behavior,,"Hey Math Stack Exchange, I have an integral that I'd like to try and simplify or get bounds on. It involves the Riemann Zeta function which I'm not super familiar with and so I thought I'd look for help here since you all have been helpful in the past. Thank you for taking a look at it and any help would be greatly appreciated! $$f(x) = \Big(\frac{1}{2 \pi i}\Big)*\int_{c-i \infty}^{c + i \infty} \frac{x^w}{w}*\sum_{k=2}^{\infty}\frac{\zeta^k(w)}{k} dw$$ where $c=2\gamma -1$ where $\gamma$ is the Euler Mascheroni Constant. There are a couple of things I'm trying to understand about it: 1) Is there a simplified form or an identity? 2) Is there a lower bound or upper bound behavior to it? (i.e. Big-O or Little-O)? 3) What happens to this as we take the limit of x going to infinity? Thank you guys again for taking a look at it!","Hey Math Stack Exchange, I have an integral that I'd like to try and simplify or get bounds on. It involves the Riemann Zeta function which I'm not super familiar with and so I thought I'd look for help here since you all have been helpful in the past. Thank you for taking a look at it and any help would be greatly appreciated! $$f(x) = \Big(\frac{1}{2 \pi i}\Big)*\int_{c-i \infty}^{c + i \infty} \frac{x^w}{w}*\sum_{k=2}^{\infty}\frac{\zeta^k(w)}{k} dw$$ where $c=2\gamma -1$ where $\gamma$ is the Euler Mascheroni Constant. There are a couple of things I'm trying to understand about it: 1) Is there a simplified form or an identity? 2) Is there a lower bound or upper bound behavior to it? (i.e. Big-O or Little-O)? 3) What happens to this as we take the limit of x going to infinity? Thank you guys again for taking a look at it!",,"['real-analysis', 'sequences-and-series', 'limits', 'riemann-zeta']"
83,Limit of a function given a condition,Limit of a function given a condition,,if $\lim_{x\to\infty}(f(x)+ \frac{1}{f(x)})$ exists.Prove or Disprove that $\lim_{x\to\infty}f(x)$ exists. I am totally clueless about this. I don't know how to start and from where to start. I tried to find some counter examples but could not get any,if $\lim_{x\to\infty}(f(x)+ \frac{1}{f(x)})$ exists.Prove or Disprove that $\lim_{x\to\infty}f(x)$ exists. I am totally clueless about this. I don't know how to start and from where to start. I tried to find some counter examples but could not get any,,['limits']
84,"To prove $\lim_{(x,y)\to (4,\pi)} x^2\sin \frac {y}{8}=\frac {8}{\sqrt 2}$",To prove,"\lim_{(x,y)\to (4,\pi)} x^2\sin \frac {y}{8}=\frac {8}{\sqrt 2}","I have to prove that $$\lim_{(x,y)\to (4,\pi)} x^2\sin \frac {y}{8}=\frac {8}{\sqrt 2}$$ My attempt: I have to prove $|x^2\sin\frac {y}{8}-\frac {8}{\sqrt 2}|\lt \epsilon$ when $|x-4|\lt \delta $ and $|y- \pi|\lt\delta$ So, $$4-\delta\lt x\lt4+\delta$$  $$16+\delta^2-8\delta\lt x^2 \lt16+\delta^2+8\delta$$ $$\pi-\delta\lt y\lt \pi+\delta$$ $$\sin\frac {\pi-\delta}{8}\lt \sin(\frac {y}{8}) \lt \sin\frac {\pi+\delta}{8}$$ But I got stuck after this.Should I multiply the two inequalities?How will I proceed after that?","I have to prove that $$\lim_{(x,y)\to (4,\pi)} x^2\sin \frac {y}{8}=\frac {8}{\sqrt 2}$$ My attempt: I have to prove $|x^2\sin\frac {y}{8}-\frac {8}{\sqrt 2}|\lt \epsilon$ when $|x-4|\lt \delta $ and $|y- \pi|\lt\delta$ So, $$4-\delta\lt x\lt4+\delta$$  $$16+\delta^2-8\delta\lt x^2 \lt16+\delta^2+8\delta$$ $$\pi-\delta\lt y\lt \pi+\delta$$ $$\sin\frac {\pi-\delta}{8}\lt \sin(\frac {y}{8}) \lt \sin\frac {\pi+\delta}{8}$$ But I got stuck after this.Should I multiply the two inequalities?How will I proceed after that?",,"['calculus', 'limits']"
85,$ \lim_{n \to \infty} \left(\frac 1{n^2+1}+\frac 2{n^2+2}+\frac 3{n^2+3}+\cdots +\frac n{n^2+n}\right)$,, \lim_{n \to \infty} \left(\frac 1{n^2+1}+\frac 2{n^2+2}+\frac 3{n^2+3}+\cdots +\frac n{n^2+n}\right),"Evaluate: $$ L=\lim_{n \to \infty} \left(\frac 1{n^2+1}+\frac 2{n^2+2}+\frac 3{n^2+3}+\cdots +\frac n{n^2+n}\right)$$ My approach: Each term can be written as $$ \frac k{n^2+k}=\frac {n^2+k-n^2}{n^2+k}=1-\frac {n^2}{n^2+k}$$ $$ \therefore \lim_{n \to \infty}\frac k{n^2+k}=\lim_{n \to \infty}\left(1-\frac {n^2}{n^2+k}\right)=0$$ hence, $$ L=0$$ Problem: The correct answer is 1/2, please indicate the flaw in my approach or post a new solution. Thank You","Evaluate: $$ L=\lim_{n \to \infty} \left(\frac 1{n^2+1}+\frac 2{n^2+2}+\frac 3{n^2+3}+\cdots +\frac n{n^2+n}\right)$$ My approach: Each term can be written as $$ \frac k{n^2+k}=\frac {n^2+k-n^2}{n^2+k}=1-\frac {n^2}{n^2+k}$$ $$ \therefore \lim_{n \to \infty}\frac k{n^2+k}=\lim_{n \to \infty}\left(1-\frac {n^2}{n^2+k}\right)=0$$ hence, $$ L=0$$ Problem: The correct answer is 1/2, please indicate the flaw in my approach or post a new solution. Thank You",,"['calculus', 'algebra-precalculus', 'proof-verification', 'proof-writing']"
86,Is the squared limit of a function equal to the limit of the squared function?,Is the squared limit of a function equal to the limit of the squared function?,,"If $\lim_{x\to c}\frac{P(x)^2}{Q(x)^2}$ exists and equals $\ell\neq0$, can one say that $$\lim_{x\to c}\frac{P(x)^2}{Q(x)^2}=\left(\lim_{x\to c}\frac{P(x)}{Q(x)}\right)^2?$$ My intuition is that one can, since $$\ell=\lim_{x\to c}\frac{P(x)^2}{Q(x)^2}=\left(\lim_{x\to c}\frac{P(x)}{Q(x)}\right)\left(\lim_{x\to c}\frac{P(x)}{Q(x)}\right)=\left(\lim_{x\to c}\frac{P(x)}{Q(x)}\right)^2$$ which seems quite elementary, but I don't know if I need to demonstrate that $\lim_{x\to c}P(x)/Q(x)$ exists. You never know with limits...","If $\lim_{x\to c}\frac{P(x)^2}{Q(x)^2}$ exists and equals $\ell\neq0$, can one say that $$\lim_{x\to c}\frac{P(x)^2}{Q(x)^2}=\left(\lim_{x\to c}\frac{P(x)}{Q(x)}\right)^2?$$ My intuition is that one can, since $$\ell=\lim_{x\to c}\frac{P(x)^2}{Q(x)^2}=\left(\lim_{x\to c}\frac{P(x)}{Q(x)}\right)\left(\lim_{x\to c}\frac{P(x)}{Q(x)}\right)=\left(\lim_{x\to c}\frac{P(x)}{Q(x)}\right)^2$$ which seems quite elementary, but I don't know if I need to demonstrate that $\lim_{x\to c}P(x)/Q(x)$ exists. You never know with limits...",,"['calculus', 'real-analysis', 'limits']"
87,$\lim_{x\to\infty} x^2\operatorname{cos}\left(\frac{3x+2}{x^2}-1\right)$ without using L’Hôpital.,without using L’Hôpital.,\lim_{x\to\infty} x^2\operatorname{cos}\left(\frac{3x+2}{x^2}-1\right),Can I calculate the following limit without applying L'Hôpital? $$\lim_{x\to\infty} x^2\operatorname{cos}\left(\frac{3x+2}{x^2}-1\right)$$ With L'Hôpital it gives me as a result $\frac{-9}{2}$.,Can I calculate the following limit without applying L'Hôpital? $$\lim_{x\to\infty} x^2\operatorname{cos}\left(\frac{3x+2}{x^2}-1\right)$$ With L'Hôpital it gives me as a result $\frac{-9}{2}$.,,"['calculus', 'limits', 'limits-without-lhopital']"
88,finding limit of multivariate function using squeeze theorem,finding limit of multivariate function using squeeze theorem,,"$\lim \limits_{(x,y) \to (0,1)} \frac{x^2(y-1)^2}{x^2+(y-1)^2}$ trying all lines $y=mx+1$ and $x=0$ yields $0$ So, let's try this $0$ as a candidate of L. $$|\frac{x^2(y-1)^2}{x^2+(y-1)^2}-0| = \frac{x^2(y-1)^2}{x^2+(y-1)^2} \leq \frac{x^2(y-1)^2}{x^2} = (y-1)^2$$ which goes to $0$ as $y \rightarrow 1$ Hence by the squeeze/sandwich theorem $$\lim \limits_{(x,y) \to (0,1)} \frac{x^2(y-1)^2}{x^2+(y-1)^2} = 0$$ Would this be a complete answer?","$\lim \limits_{(x,y) \to (0,1)} \frac{x^2(y-1)^2}{x^2+(y-1)^2}$ trying all lines $y=mx+1$ and $x=0$ yields $0$ So, let's try this $0$ as a candidate of L. $$|\frac{x^2(y-1)^2}{x^2+(y-1)^2}-0| = \frac{x^2(y-1)^2}{x^2+(y-1)^2} \leq \frac{x^2(y-1)^2}{x^2} = (y-1)^2$$ which goes to $0$ as $y \rightarrow 1$ Hence by the squeeze/sandwich theorem $$\lim \limits_{(x,y) \to (0,1)} \frac{x^2(y-1)^2}{x^2+(y-1)^2} = 0$$ Would this be a complete answer?",,"['limits', 'multivariable-calculus']"
89,"Showing the convergence of $a_1+a_2+a_3....$ given $(a_1+a_2)+(a_3+a_4)....=S, a_n \to 0$",Showing the convergence of  given,"a_1+a_2+a_3.... (a_1+a_2)+(a_3+a_4)....=S, a_n \to 0","Given   $$(a_1+a_2)+(a_3+a_4)+(a_5+a_6)+(a_7+a_8)+\cdots= S$$   if $a_n\to 0$, then   $$a_1+a_2+a_3+a_4+a_4+a_5+a_6+a_7+\cdots= S$$ My attempt: The sequence of partial sums of $\sum a_n$ is $$(s_n)=(a_1,\ a_1+a_2,\ a_1+a_2+a_3,\ a_1+a_2+a_3+a_4\ ...)$$ Witch has a subsequence $(s_{2n})=(a_1+a_2,\ a_1+a_2+a_3+a_3,\ ...)$ that is the sequence of partial sums to $\sum(a_n+a_{n+1})$. This implies that it converges to $S$. Also, there is the subsequence $(s_{2n+1})=(a_1,\ a_1+a_2+a_3,\ a_1+a_2+a_3+a_4+a_5\ \cdots)$ that converges to $\lim s_{2n+1}=\lim [ \sum(a_n+a_{n+1})+a_{2n+1}]=\lim\sum(a_n+a_{n+1})+\lim a_{2n+1}=S+0$ Since $\lim s_{2n}=\lim s_{2n+1}=S\implies \lim s_n=S$ Is this correct? Any tips on the solution? Thanks!","Given   $$(a_1+a_2)+(a_3+a_4)+(a_5+a_6)+(a_7+a_8)+\cdots= S$$   if $a_n\to 0$, then   $$a_1+a_2+a_3+a_4+a_4+a_5+a_6+a_7+\cdots= S$$ My attempt: The sequence of partial sums of $\sum a_n$ is $$(s_n)=(a_1,\ a_1+a_2,\ a_1+a_2+a_3,\ a_1+a_2+a_3+a_4\ ...)$$ Witch has a subsequence $(s_{2n})=(a_1+a_2,\ a_1+a_2+a_3+a_3,\ ...)$ that is the sequence of partial sums to $\sum(a_n+a_{n+1})$. This implies that it converges to $S$. Also, there is the subsequence $(s_{2n+1})=(a_1,\ a_1+a_2+a_3,\ a_1+a_2+a_3+a_4+a_5\ \cdots)$ that converges to $\lim s_{2n+1}=\lim [ \sum(a_n+a_{n+1})+a_{2n+1}]=\lim\sum(a_n+a_{n+1})+\lim a_{2n+1}=S+0$ Since $\lim s_{2n}=\lim s_{2n+1}=S\implies \lim s_n=S$ Is this correct? Any tips on the solution? Thanks!",,"['sequences-and-series', 'limits', 'proof-verification', 'convergence-divergence']"
90,Iterated limits and normal limits (over $\mathbb R^2 \to \mathbb R$ (and $\mathbb R^n$)),Iterated limits and normal limits (over  (and )),\mathbb R^2 \to \mathbb R \mathbb R^n,"When one studies multi-variable functions, usually starting with functions from $\mathbb R^2$ to $\mathbb R$, at the very beginning we are taught that iterating limits and ""normal limits"" (i.e by definition) does not behave equally, and that the existance of one does not promise the existence of the other, for example: $$f(x,y) = \begin{cases} \frac{xy}{x^2+y^2} & (x,y)\ne(0,0) \\ 0 & (x,y) = (0,0)\end{cases}$$ for this function we get that: $$\lim_{x\to 0}\lim_{y\to 0}= \lim_{y\to 0}\lim_{x\to 0} = 0$$ but this limit does not exist: $$\lim_{(x,y)\to (0,0)}f(x,y)$$ $$$$ Or the other way, with this function serving as an example: $$f(x,y) = \begin{cases} x+y\sin(\frac{1}{x}) & x\ne0\\0&x=0 \end{cases}$$ here we get that: $$\lim_{(x,y)\to (0,0)}f(x,y)=0$$ but this limit does not exist: $$\lim_{y\to0}\lim_{x\to0} f(x,y)$$ $$$$ My question is this: Is there, under any presumtions or conditions, any connection between the ""normal limit"" and the iterated limits, which makes them ""iff"" statements, or that if one exists, the other also exists (and the other way as well). If so, what are they, and how do we prove them? (I will note that this is not the same question that can be found here: Limits of 2 variable functions , or here: Iterated Limits , since my question does not include any assumption that the all the limits exist and (some of them) are equal, and in fact, I ask if any conditions or presumptions hold (maybe for a group of functions?) we can get the connection). Thanks! Edit: Is the ""Moore-Osgood theorem"" relevant for this? Wikipedia states that ""If ${\displaystyle \lim _{x\to p}f(x,y)}$ exists pointwise for each $y$ different of $q$ and if ${\displaystyle \lim _{y\to q}f(x,y)}$  converges uniformly for $x≠p$ then the double limit and the iterated limits exist and are equal."", also found in this link: http://www.math.unm.edu/~loring/links/analysis_f10/exchange.pdf . If so, I didn't understand it, and would love your explanation on it.","When one studies multi-variable functions, usually starting with functions from $\mathbb R^2$ to $\mathbb R$, at the very beginning we are taught that iterating limits and ""normal limits"" (i.e by definition) does not behave equally, and that the existance of one does not promise the existence of the other, for example: $$f(x,y) = \begin{cases} \frac{xy}{x^2+y^2} & (x,y)\ne(0,0) \\ 0 & (x,y) = (0,0)\end{cases}$$ for this function we get that: $$\lim_{x\to 0}\lim_{y\to 0}= \lim_{y\to 0}\lim_{x\to 0} = 0$$ but this limit does not exist: $$\lim_{(x,y)\to (0,0)}f(x,y)$$ $$$$ Or the other way, with this function serving as an example: $$f(x,y) = \begin{cases} x+y\sin(\frac{1}{x}) & x\ne0\\0&x=0 \end{cases}$$ here we get that: $$\lim_{(x,y)\to (0,0)}f(x,y)=0$$ but this limit does not exist: $$\lim_{y\to0}\lim_{x\to0} f(x,y)$$ $$$$ My question is this: Is there, under any presumtions or conditions, any connection between the ""normal limit"" and the iterated limits, which makes them ""iff"" statements, or that if one exists, the other also exists (and the other way as well). If so, what are they, and how do we prove them? (I will note that this is not the same question that can be found here: Limits of 2 variable functions , or here: Iterated Limits , since my question does not include any assumption that the all the limits exist and (some of them) are equal, and in fact, I ask if any conditions or presumptions hold (maybe for a group of functions?) we can get the connection). Thanks! Edit: Is the ""Moore-Osgood theorem"" relevant for this? Wikipedia states that ""If ${\displaystyle \lim _{x\to p}f(x,y)}$ exists pointwise for each $y$ different of $q$ and if ${\displaystyle \lim _{y\to q}f(x,y)}$  converges uniformly for $x≠p$ then the double limit and the iterated limits exist and are equal."", also found in this link: http://www.math.unm.edu/~loring/links/analysis_f10/exchange.pdf . If so, I didn't understand it, and would love your explanation on it.",,"['calculus', 'general-topology', 'limits', 'multivariable-calculus']"
91,Is it possible to interchange the order of limits in this case?,Is it possible to interchange the order of limits in this case?,,"I'm stuck on this problem. Let $g \in C^{\infty}(\mathbb{R})$ with $|x|^p  |D^{q} g| \rightarrow 0$ as $|x| \rightarrow \infty$ for any nonnegative integers $p$ and $q$. Suppose that $|g(\gamma)| \leq (1+\gamma^2)^{-1}$ for $\gamma \in \mathbb{R}$. Show that $$\lim_{N\rightarrow\infty} \lim_{T\rightarrow\infty}\sum_{|n|\leq NT} \frac{1}{T}g\left(\frac{n}{T}\right) = \lim_{T\rightarrow\infty}\lim_{N\rightarrow\infty}\sum_{|n|\leq NT} \frac{1}{T}g\left(\frac{n}{T}\right).$$ It is just interchange of the order of limits. But how can we guarantee this equality? Any help will be appreciated! **(edit) I've checked that  $$ \lim_{T\rightarrow\infty}\sum_{|n|\leq NT} \frac{1}{T}g\left(\frac{n}{T}\right) = \int_{-N}^N g(\gamma) \, d\gamma,$$ so that $$\lim_{N\rightarrow\infty}\lim_{T\rightarrow\infty}\sum_{|n|\leq NT} \frac{1}{T}g\left(\frac{n}{T}\right) = \int_{-\infty}^{\infty} g(\gamma) \, d\gamma. $$","I'm stuck on this problem. Let $g \in C^{\infty}(\mathbb{R})$ with $|x|^p  |D^{q} g| \rightarrow 0$ as $|x| \rightarrow \infty$ for any nonnegative integers $p$ and $q$. Suppose that $|g(\gamma)| \leq (1+\gamma^2)^{-1}$ for $\gamma \in \mathbb{R}$. Show that $$\lim_{N\rightarrow\infty} \lim_{T\rightarrow\infty}\sum_{|n|\leq NT} \frac{1}{T}g\left(\frac{n}{T}\right) = \lim_{T\rightarrow\infty}\lim_{N\rightarrow\infty}\sum_{|n|\leq NT} \frac{1}{T}g\left(\frac{n}{T}\right).$$ It is just interchange of the order of limits. But how can we guarantee this equality? Any help will be appreciated! **(edit) I've checked that  $$ \lim_{T\rightarrow\infty}\sum_{|n|\leq NT} \frac{1}{T}g\left(\frac{n}{T}\right) = \int_{-N}^N g(\gamma) \, d\gamma,$$ so that $$\lim_{N\rightarrow\infty}\lim_{T\rightarrow\infty}\sum_{|n|\leq NT} \frac{1}{T}g\left(\frac{n}{T}\right) = \int_{-\infty}^{\infty} g(\gamma) \, d\gamma. $$",,"['real-analysis', 'limits', 'summation', 'fourier-analysis', 'fourier-series']"
92,"Prove that $\lim_{t\rightarrow\infty}\int\limits_1^2\frac{\sin(tx)}{x^2\sqrt{x-1}}\,dx=0$",Prove that,"\lim_{t\rightarrow\infty}\int\limits_1^2\frac{\sin(tx)}{x^2\sqrt{x-1}}\,dx=0","Prove that $$\lim_{t\rightarrow\infty}\int\limits_1^2\frac{\sin(tx)}{x^2\sqrt{x-1}}\,dx=0$$ I'm hoping there's some better way to go about this other than bounding the integral by $\int\frac{1}{x^2\sqrt{x-1}}\,dx$, because that integral seems to require two substitutions. Is there?","Prove that $$\lim_{t\rightarrow\infty}\int\limits_1^2\frac{\sin(tx)}{x^2\sqrt{x-1}}\,dx=0$$ I'm hoping there's some better way to go about this other than bounding the integral by $\int\frac{1}{x^2\sqrt{x-1}}\,dx$, because that integral seems to require two substitutions. Is there?",,"['real-analysis', 'integration', 'limits']"
93,A limit question puzzled me,A limit question puzzled me,,$$\frac{1}{\sqrt{k}}＝\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}e^{-kq^{2}}dq$$ so $$\sqrt{x}e^{-x}\sum_{k＝1}^{\infty}\frac{x^{k}}{k!\sqrt{k}}$$ $$＝\frac{2\sqrt{x}e^{-x}}{\sqrt{\pi}}\sum_{k＝1}^{\infty}\frac{x^{k}}{k!}\int_{0}^{\infty}e^{-kq^{2}}dq$$ $$＝\frac{2\sqrt{x}e^{-x}}{\sqrt{\pi}}\int_{0}^{\infty}(e^{xe^{-q^{2}}}-1)dq$$ The question is how to prove $$lim_{x\rightarrow \infty}\frac{2\sqrt{x}e^{-x}}{\sqrt{\pi}}\int_{0}^{\infty}(e^{xe^{-q^{2}}}-1)dq＝1$$ Thanks in advance,$$\frac{1}{\sqrt{k}}＝\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}e^{-kq^{2}}dq$$ so $$\sqrt{x}e^{-x}\sum_{k＝1}^{\infty}\frac{x^{k}}{k!\sqrt{k}}$$ $$＝\frac{2\sqrt{x}e^{-x}}{\sqrt{\pi}}\sum_{k＝1}^{\infty}\frac{x^{k}}{k!}\int_{0}^{\infty}e^{-kq^{2}}dq$$ $$＝\frac{2\sqrt{x}e^{-x}}{\sqrt{\pi}}\int_{0}^{\infty}(e^{xe^{-q^{2}}}-1)dq$$ The question is how to prove $$lim_{x\rightarrow \infty}\frac{2\sqrt{x}e^{-x}}{\sqrt{\pi}}\int_{0}^{\infty}(e^{xe^{-q^{2}}}-1)dq＝1$$ Thanks in advance,,['limits']
94,Showing that the sequence $x^n \sqrt{n}$ converges to 0 when $|x|<1$,Showing that the sequence  converges to 0 when,x^n \sqrt{n} |x|<1,"For a real number $x$ such that $|x|<1$, show that the sequence $(x^n\sqrt{n})_{n \in \mathbb{N}}$ is convergent, with limit 0. Here's what I have so far: Let $\epsilon > 0$. We want to find $N$ such that for all $n > N, |x^n\sqrt{n}| < \epsilon$. Now we have $n\log(x) + \frac{1}{2}\log(n) < n + n = 2n < \log (\epsilon)$. So pick $N$ with $N<\frac{\log(\epsilon)}{2}$. My question is, is using logarithms a valid way to approach a question on convergence, and is there a better alternative method to tackle this question? EDIT: Attempt 2: If $x=0$, then $x^{n}\sqrt{n}=0$ for every $n \in \mathbb{N}$ and therefore $$\lim_{n\to\infty}x^{n}\sqrt{n}=0$$ So we can assume $x \neq 0$. Then, note that $h = \frac{1}{|x|} - 1 > 0$. From Bernoulli's inequality we have: $\frac{1}{|x|^{n}\sqrt{n}} = \frac{(1+h)^{n}}{\sqrt{n}} \geq \frac{1+nh}{\sqrt{n}} > \sqrt{n}h.$ Therefore, we have $0 \leq |x|^{n}\sqrt{n} \leq \frac{1}{\sqrt{n}h}.$ Now, given any $\epsilon > 0$, we take $N \geq (\frac{1}{\epsilon h})^2$. Now for $n>N, |x^n\sqrt{n} - 0| = |x^n\sqrt{n}| \leq \frac{1}{\sqrt{n}h} < \frac{1}{\sqrt{N}h} \leq \epsilon$. Hence $$\lim_{n\to\infty}x^{n}\sqrt{n}=0$$","For a real number $x$ such that $|x|<1$, show that the sequence $(x^n\sqrt{n})_{n \in \mathbb{N}}$ is convergent, with limit 0. Here's what I have so far: Let $\epsilon > 0$. We want to find $N$ such that for all $n > N, |x^n\sqrt{n}| < \epsilon$. Now we have $n\log(x) + \frac{1}{2}\log(n) < n + n = 2n < \log (\epsilon)$. So pick $N$ with $N<\frac{\log(\epsilon)}{2}$. My question is, is using logarithms a valid way to approach a question on convergence, and is there a better alternative method to tackle this question? EDIT: Attempt 2: If $x=0$, then $x^{n}\sqrt{n}=0$ for every $n \in \mathbb{N}$ and therefore $$\lim_{n\to\infty}x^{n}\sqrt{n}=0$$ So we can assume $x \neq 0$. Then, note that $h = \frac{1}{|x|} - 1 > 0$. From Bernoulli's inequality we have: $\frac{1}{|x|^{n}\sqrt{n}} = \frac{(1+h)^{n}}{\sqrt{n}} \geq \frac{1+nh}{\sqrt{n}} > \sqrt{n}h.$ Therefore, we have $0 \leq |x|^{n}\sqrt{n} \leq \frac{1}{\sqrt{n}h}.$ Now, given any $\epsilon > 0$, we take $N \geq (\frac{1}{\epsilon h})^2$. Now for $n>N, |x^n\sqrt{n} - 0| = |x^n\sqrt{n}| \leq \frac{1}{\sqrt{n}h} < \frac{1}{\sqrt{N}h} \leq \epsilon$. Hence $$\lim_{n\to\infty}x^{n}\sqrt{n}=0$$",,"['limits', 'convergence-divergence']"
95,Analysis of a function,Analysis of a function,,"I'm analysing function $$y=x+\frac{\ln x}{x}$$ and I kinda don't get few  parts. I get an oblique asymptote $y=x$, that function is increasing throughout the domain and that it is convex on $(e^\frac{3}{2}, \infty)$ and concave on $(0, e^\frac{3}{2})$ what bugs me is when i try to sketch a graph. It doesn't seem to match one on wolframalpha, particularly, it seems their is not convex on any part. One more thing, it also doesn't say function has an oblique asymptote. Can anyone tell me how is wolfram getting those results?","I'm analysing function $$y=x+\frac{\ln x}{x}$$ and I kinda don't get few  parts. I get an oblique asymptote $y=x$, that function is increasing throughout the domain and that it is convex on $(e^\frac{3}{2}, \infty)$ and concave on $(0, e^\frac{3}{2})$ what bugs me is when i try to sketch a graph. It doesn't seem to match one on wolframalpha, particularly, it seems their is not convex on any part. One more thing, it also doesn't say function has an oblique asymptote. Can anyone tell me how is wolfram getting those results?",,"['calculus', 'limits', 'functions', 'asymptotics']"
96,Linear algebra Geometric question - Brazilian Olympiad,Linear algebra Geometric question - Brazilian Olympiad,,"I've seen this question in the Brazil Undergrad Olympiad this year. There is also a solution provided in the official webpage , but I didn't get it. Please, I'd appreciate it if someone could give a solution. Let $A: \mathbb R^3 \to \mathbb R^2 $ given by $A(x,y,z) = (x+y+z,x+y)$. Prove that there exists an only s $\ge0$ such that the limit $c=\lim\limits_{\epsilon \to 0} \cfrac {vol(\{ v\; \in\;\mathbb R^3 ; \;|v|\; \le\; 1 \; \text{and } |Av| \;\lt \; \epsilon\})  }{\epsilon^s}$ exists, is positive, and compute s and c. Thanks in advance.","I've seen this question in the Brazil Undergrad Olympiad this year. There is also a solution provided in the official webpage , but I didn't get it. Please, I'd appreciate it if someone could give a solution. Let $A: \mathbb R^3 \to \mathbb R^2 $ given by $A(x,y,z) = (x+y+z,x+y)$. Prove that there exists an only s $\ge0$ such that the limit $c=\lim\limits_{\epsilon \to 0} \cfrac {vol(\{ v\; \in\;\mathbb R^3 ; \;|v|\; \le\; 1 \; \text{and } |Av| \;\lt \; \epsilon\})  }{\epsilon^s}$ exists, is positive, and compute s and c. Thanks in advance.",,"['real-analysis', 'linear-algebra', 'limits', 'contest-math', 'hausdorff-measure']"
97,Relationship between linear dependence/convergence,Relationship between linear dependence/convergence,,"Suppose I have two convergent sequences ${\{X_m}\}$ and ${\{Y_m}\}$ with limits $X$ and $Y$ in $\mathbb{R}^n$. How do I go about proving/disproving the following: a) If for every $m$, the two vectors $X_m$ and $Y_m$ are linearly dependent, then $X$ and $Y$ are linearly dependent. Similarly, b) If for every $m$, the two vectors $X_m$ and $Y_m$ are linearly independent, then $X$ and $Y$ are linearly independent. Would the following work for b? Say b is false. Let $X_m = {\{1/m,sin(m)/m}\}$, and $Y_m = {\{sin(m)/m,1/m}\}$, then the vectors $X_m$ and $Y_m$ are always linearly independent and both sequences converge to $(0,0)$, thus they're dependent.","Suppose I have two convergent sequences ${\{X_m}\}$ and ${\{Y_m}\}$ with limits $X$ and $Y$ in $\mathbb{R}^n$. How do I go about proving/disproving the following: a) If for every $m$, the two vectors $X_m$ and $Y_m$ are linearly dependent, then $X$ and $Y$ are linearly dependent. Similarly, b) If for every $m$, the two vectors $X_m$ and $Y_m$ are linearly independent, then $X$ and $Y$ are linearly independent. Would the following work for b? Say b is false. Let $X_m = {\{1/m,sin(m)/m}\}$, and $Y_m = {\{sin(m)/m,1/m}\}$, then the vectors $X_m$ and $Y_m$ are always linearly independent and both sequences converge to $(0,0)$, thus they're dependent.",,"['real-analysis', 'linear-algebra', 'limits', 'convergence-divergence', 'vector-spaces']"
98,Question on a peculiar limit,Question on a peculiar limit,,"Take $f : [0,\infty) \rightarrow \mathbb{R}$ to be a strictly decreasing continuous function such that $\lim_{x \rightarrow \infty} f(x) = 0.$ I would like to prove that $$\int_0^\infty \frac{f(x+1) - f(x)}{f(x)} dx$$ diverges. On initial read, we can see that $$\int_0^\infty \frac{f(x+1)-f(x)}{f(x)}\, dx  =\int_0^\infty \frac{f(x+1)}{f(x)} - \frac{f(x)}{f(x)} \,dx  =\int_0^\infty \frac{f(x+1)}{f(x)}\,dx - \int_0^{\infty} 1 \,dx  =\int_0^\infty \frac{f(x+1)}{f(x)}\,dx - \infty.$$ Hence, it would suffice to show that either  $\int_0^\infty \frac{f(x+1)}{f(x)}\, dx$ is finite or diverges to $-\infty .$ We can see that $$\int_0^\infty \frac{f(x+1)}{f(x)} \,dx$$ $$=\lim_{n \rightarrow \infty} \int_0^n \frac{f(x+1)}{f(x)} \,dx.$$ What's the next step for dealing with this given the strictly monotonically decreasing aspects of $f$?","Take $f : [0,\infty) \rightarrow \mathbb{R}$ to be a strictly decreasing continuous function such that $\lim_{x \rightarrow \infty} f(x) = 0.$ I would like to prove that $$\int_0^\infty \frac{f(x+1) - f(x)}{f(x)} dx$$ diverges. On initial read, we can see that $$\int_0^\infty \frac{f(x+1)-f(x)}{f(x)}\, dx  =\int_0^\infty \frac{f(x+1)}{f(x)} - \frac{f(x)}{f(x)} \,dx  =\int_0^\infty \frac{f(x+1)}{f(x)}\,dx - \int_0^{\infty} 1 \,dx  =\int_0^\infty \frac{f(x+1)}{f(x)}\,dx - \infty.$$ Hence, it would suffice to show that either  $\int_0^\infty \frac{f(x+1)}{f(x)}\, dx$ is finite or diverges to $-\infty .$ We can see that $$\int_0^\infty \frac{f(x+1)}{f(x)} \,dx$$ $$=\lim_{n \rightarrow \infty} \int_0^n \frac{f(x+1)}{f(x)} \,dx.$$ What's the next step for dealing with this given the strictly monotonically decreasing aspects of $f$?",,"['calculus', 'real-analysis', 'limits']"
99,"Proving that, for every $\theta \in \mathbb Q$, $\sin(n!\theta\pi)$ has a limit, using $\epsilon$-$N$ definition of limit","Proving that, for every ,  has a limit, using - definition of limit",\theta \in \mathbb Q \sin(n!\theta\pi) \epsilon N,"If $\theta \in \mathbb Q$, show that $\{\sin(n!\theta\pi)\}$ has a limit How can I rigorously prove that this sequence has a limit using $\epsilon-N$? Is it even possible to do so? I know that we can write $\theta$ as ${p \over q}, p \in \mathbb Z, q \in \mathbb N,$. Hence, as $n\to\infty$, we have that $n!$ is the product of all natural numbers meaning that $q | n!$. But can we choose $N = q > 0$? Then for any $\epsilon > 0$ and for all $n\ge N$, $q|n!$, and we get that $$\begin{align}|\sin(n!\theta\pi) - 0| &= \left|\sin \left(n!{p \over q}\pi\right)\right| \\ &\le |\sin(k\pi)| \tag{Since $q|n!$, then $N!(p/q) = k\in\mathbb Z$} \\ & = 0 < \epsilon\end{align}$$","If $\theta \in \mathbb Q$, show that $\{\sin(n!\theta\pi)\}$ has a limit How can I rigorously prove that this sequence has a limit using $\epsilon-N$? Is it even possible to do so? I know that we can write $\theta$ as ${p \over q}, p \in \mathbb Z, q \in \mathbb N,$. Hence, as $n\to\infty$, we have that $n!$ is the product of all natural numbers meaning that $q | n!$. But can we choose $N = q > 0$? Then for any $\epsilon > 0$ and for all $n\ge N$, $q|n!$, and we get that $$\begin{align}|\sin(n!\theta\pi) - 0| &= \left|\sin \left(n!{p \over q}\pi\right)\right| \\ &\le |\sin(k\pi)| \tag{Since $q|n!$, then $N!(p/q) = k\in\mathbb Z$} \\ & = 0 < \epsilon\end{align}$$",,"['real-analysis', 'limits', 'proof-writing']"
