,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Riemann Mapping Theorem's Consequence,Riemann Mapping Theorem's Consequence,,"It is well known that the Riemann mapping theorem asserts that for any open simply connected $G\subset \mathbb{C}$ and $z_{0}\in G$, there exists a unique bijective analytic function  $f:G\to \mathbb{D}$, such that $f(z_{0})=0$ and $f^{\prime}(z_{0})>0$ (here $\mathbb{D}$ denotes the open unit disk). I would greatly appreciate if you could help me to get an answer to the following. Question. Supposing, in addition, that $\overline{\mathbb{C}}\setminus \overline{G}$ is simply connected and contains the point $\infty$, how can be used the Riemann mapping theorem in order to constructed the unique conformal bijection mapping $\Phi:\overline{\mathbb{C}}\setminus \overline{G}\to \overline{\mathbb{C}}\setminus \overline{\mathbb{D}}$ which satisfies $\Phi(\infty)=\infty$ and $\Phi^{\prime}(\infty)>0$  ? The existence of the above function $\Phi$  appears stated in many books on complex analysis but without to give any explanation to its construction (only mentioned that it  follows from the Riemann mapping theorem). I might presume that the answer is simple, but unfortunately I cannot figure out it. Thus, I could take $\Phi(z)=1/f(z)$, where $f:\overline{\mathbb{C}}\setminus \overline{G}\to\mathbb{D}$ is the mapping from the Riemann's theorem (since $\overline{\mathbb{C}}\setminus \overline{G}$ is open and simply connected). But, the problem is that choosing in the Riemann's theorem $z_{0}=\infty$ and $f$ satisfying $f(\infty)=0$, $f^{\prime}(\infty)>0$, we get $\Phi(\infty)=1/0=+\infty$,  but $\Phi^{\prime}(\infty)=-\frac{f^{\prime}(\infty)}{f^{2}(\infty)}=-\infty$. We were expected to get here $\Phi^{\prime}(\infty)>0$, and the questuon is how could be fixed this thing ? Thank you in advance. George","It is well known that the Riemann mapping theorem asserts that for any open simply connected $G\subset \mathbb{C}$ and $z_{0}\in G$, there exists a unique bijective analytic function  $f:G\to \mathbb{D}$, such that $f(z_{0})=0$ and $f^{\prime}(z_{0})>0$ (here $\mathbb{D}$ denotes the open unit disk). I would greatly appreciate if you could help me to get an answer to the following. Question. Supposing, in addition, that $\overline{\mathbb{C}}\setminus \overline{G}$ is simply connected and contains the point $\infty$, how can be used the Riemann mapping theorem in order to constructed the unique conformal bijection mapping $\Phi:\overline{\mathbb{C}}\setminus \overline{G}\to \overline{\mathbb{C}}\setminus \overline{\mathbb{D}}$ which satisfies $\Phi(\infty)=\infty$ and $\Phi^{\prime}(\infty)>0$  ? The existence of the above function $\Phi$  appears stated in many books on complex analysis but without to give any explanation to its construction (only mentioned that it  follows from the Riemann mapping theorem). I might presume that the answer is simple, but unfortunately I cannot figure out it. Thus, I could take $\Phi(z)=1/f(z)$, where $f:\overline{\mathbb{C}}\setminus \overline{G}\to\mathbb{D}$ is the mapping from the Riemann's theorem (since $\overline{\mathbb{C}}\setminus \overline{G}$ is open and simply connected). But, the problem is that choosing in the Riemann's theorem $z_{0}=\infty$ and $f$ satisfying $f(\infty)=0$, $f^{\prime}(\infty)>0$, we get $\Phi(\infty)=1/0=+\infty$,  but $\Phi^{\prime}(\infty)=-\frac{f^{\prime}(\infty)}{f^{2}(\infty)}=-\infty$. We were expected to get here $\Phi^{\prime}(\infty)>0$, and the questuon is how could be fixed this thing ? Thank you in advance. George",,['complex-analysis']
1,Geometric Interpretation of Antiderivative?,Geometric Interpretation of Antiderivative?,,Could someone please give me a geometric interpretation of the above theorem?,Could someone please give me a geometric interpretation of the above theorem?,,"['calculus', 'complex-analysis', 'multivariable-calculus', 'intuition']"
2,Evaluate $\int_{|z|=r}^{}\frac{\log\ z}{z^2+1}dz$ where $r>0$,Evaluate  where,\int_{|z|=r}^{}\frac{\log\ z}{z^2+1}dz r>0,"I tried to evaluate $$\int_{|z|=r}^{}\frac{\log\ z}{z^2+1}dz$$ when $r>0$. Clearly $\log z$ is not continuous at $z=-r$. So is this integral meaningful then? Since the function is continuous and bounded on  the given path except only at this point, I am thinking it should be possible to actually evaluate this. Hope someone could help me out with this confusion and help to evaluate this integral. Thanks","I tried to evaluate $$\int_{|z|=r}^{}\frac{\log\ z}{z^2+1}dz$$ when $r>0$. Clearly $\log z$ is not continuous at $z=-r$. So is this integral meaningful then? Since the function is continuous and bounded on  the given path except only at this point, I am thinking it should be possible to actually evaluate this. Hope someone could help me out with this confusion and help to evaluate this integral. Thanks",,"['complex-analysis', 'complex-integration']"
3,Are these the correct residues?,Are these the correct residues?,,"$$\int_C \frac{z+1}{z^2-2z} dz$$ for the circle of $\lvert z \rvert = 3 $. Poles are obviously  at $ z = {0,2}$. Can I calculate the residues by viewing the fraction in the integral as either $$\int_C \frac{\frac{z+1}{z}}{z-2} dz $$$$ \int_C \frac{\frac{z+1}{z-2}}{z} dz$$ and plug into 2 and 0 into those numerators respectively? That would yield a final answer of $2\pi i * (\frac{3}{2} + \frac{1}{-2}) = \pi i$. Does this look right? I'm new to residues and want to make sure I'm on the right track.","$$\int_C \frac{z+1}{z^2-2z} dz$$ for the circle of $\lvert z \rvert = 3 $. Poles are obviously  at $ z = {0,2}$. Can I calculate the residues by viewing the fraction in the integral as either $$\int_C \frac{\frac{z+1}{z}}{z-2} dz $$$$ \int_C \frac{\frac{z+1}{z-2}}{z} dz$$ and plug into 2 and 0 into those numerators respectively? That would yield a final answer of $2\pi i * (\frac{3}{2} + \frac{1}{-2}) = \pi i$. Does this look right? I'm new to residues and want to make sure I'm on the right track.",,"['complex-analysis', 'contour-integration', 'solution-verification', 'residue-calculus']"
4,How to prove $\sum_{k=1}^{N} \frac{\sin n\theta}{2^N}=\frac{2^{N+1}\sin \theta + \sin N\theta -2\sin(N+1)\theta}{2^N(5-4\cos \theta)}$,How to prove,\sum_{k=1}^{N} \frac{\sin n\theta}{2^N}=\frac{2^{N+1}\sin \theta + \sin N\theta -2\sin(N+1)\theta}{2^N(5-4\cos \theta)},"Prove This using De Moivre Theorem $$\sum_{n=1}^{N}\frac{\sin n\theta}{2^n}=\frac{2^{N+1}\sin\theta+\sin N\theta-2\sin(N+1)\theta}{2^N(5-4\cos\theta)}$$ Please help me find my mistake, because I am not getting this result. What I did: $$\frac{sin n\theta} {2^N} = \Im \frac{e^{n\theta i}}{2^N}$$ Applied the G.P sum formula : $$\frac{\frac{e^{\theta i}}{2} (1-\frac{e^{N\theta i}}{2^N})}{1-\frac{e^{\theta i}}{2}}$$ $$\Im\frac{e^{\theta i} (\frac{2^N-e^{N\theta i}}{2^N})}{2-{e^{\theta i}}}$$ $$\Im\frac{e^{\theta i} (\frac{2^N-e^{N\theta i}}{2^N})}{2-{e^{\theta i}}}$$ $$\Im\frac{e^{\theta i} ({2^N-e^{N\theta i}})}{2^{N+1}-{2^N e^{\theta i}}}$$ $$\Im\frac{e^{\theta i} ({2^N-e^{N\theta i}})}{2^{N}(2-{e^{\theta i}})}$$ $$\Im\frac{e^{\theta i} ({2^N-e^{N\theta i}})}{2^{N}(2-{e^{\theta i}})} \cdot \frac{2+e^{\theta i}}{2+e^{\theta i}}$$ I got : $$\frac{2^{N+1} \sin \theta + 2^N \sin 2\theta - 2\sin (N+1)\theta- sin (N+2)\theta}{2^N(4-\sin 2\theta)}$$ I have no idea how to brings it to the desired result. They don't look very similar either. What is wrong , or what can I do next ? I also noticed the denominator needs to be in terms of $\cos \theta$ so I also used $sin2\theta=2\sin\theta \cos\theta$ but then I don't know how to get rid of the $\sin\theta$ : $$\frac{2^{N+1} \sin \theta + 2^N \sin 2\theta - 2\sin (N+1)\theta- sin (N+2)\theta}{2^N(4-2\sin\theta\cos \theta)}$$","Prove This using De Moivre Theorem $$\sum_{n=1}^{N}\frac{\sin n\theta}{2^n}=\frac{2^{N+1}\sin\theta+\sin N\theta-2\sin(N+1)\theta}{2^N(5-4\cos\theta)}$$ Please help me find my mistake, because I am not getting this result. What I did: $$\frac{sin n\theta} {2^N} = \Im \frac{e^{n\theta i}}{2^N}$$ Applied the G.P sum formula : $$\frac{\frac{e^{\theta i}}{2} (1-\frac{e^{N\theta i}}{2^N})}{1-\frac{e^{\theta i}}{2}}$$ $$\Im\frac{e^{\theta i} (\frac{2^N-e^{N\theta i}}{2^N})}{2-{e^{\theta i}}}$$ $$\Im\frac{e^{\theta i} (\frac{2^N-e^{N\theta i}}{2^N})}{2-{e^{\theta i}}}$$ $$\Im\frac{e^{\theta i} ({2^N-e^{N\theta i}})}{2^{N+1}-{2^N e^{\theta i}}}$$ $$\Im\frac{e^{\theta i} ({2^N-e^{N\theta i}})}{2^{N}(2-{e^{\theta i}})}$$ $$\Im\frac{e^{\theta i} ({2^N-e^{N\theta i}})}{2^{N}(2-{e^{\theta i}})} \cdot \frac{2+e^{\theta i}}{2+e^{\theta i}}$$ I got : $$\frac{2^{N+1} \sin \theta + 2^N \sin 2\theta - 2\sin (N+1)\theta- sin (N+2)\theta}{2^N(4-\sin 2\theta)}$$ I have no idea how to brings it to the desired result. They don't look very similar either. What is wrong , or what can I do next ? I also noticed the denominator needs to be in terms of $\cos \theta$ so I also used $sin2\theta=2\sin\theta \cos\theta$ but then I don't know how to get rid of the $\sin\theta$ : $$\frac{2^{N+1} \sin \theta + 2^N \sin 2\theta - 2\sin (N+1)\theta- sin (N+2)\theta}{2^N(4-2\sin\theta\cos \theta)}$$",,"['complex-analysis', 'complex-numbers', 'summation']"
5,Möbius Transformation interchanging two preassigned points in the upper half plane,Möbius Transformation interchanging two preassigned points in the upper half plane,,"Is there a Möbius transformation mapping the upper half plane onto itself that interchange two preassigned points in the upper half plane? If so , how many such Möbius transformations are there? Here are steps that I followed: 1.we know we can transform upper half plane onto unit circle. I want  to find a Möbius transformation that maps unit circle to unit circle but interchanging the preassigned points. 3. Finally , if I find a transformation that maps the unit circle to the upper half plane.Then I think we are done. I have problem in finding the Möbius transformation that maps unit circle to unit circle with interchanging property that already mention. Does someone have any idea about this?","Is there a Möbius transformation mapping the upper half plane onto itself that interchange two preassigned points in the upper half plane? If so , how many such Möbius transformations are there? Here are steps that I followed: 1.we know we can transform upper half plane onto unit circle. I want  to find a Möbius transformation that maps unit circle to unit circle but interchanging the preassigned points. 3. Finally , if I find a transformation that maps the unit circle to the upper half plane.Then I think we are done. I have problem in finding the Möbius transformation that maps unit circle to unit circle with interchanging property that already mention. Does someone have any idea about this?",,['complex-analysis']
6,Generating series - Finite groups of order $n$,Generating series - Finite groups of order,n,I am wondering if something of interest can be said about one of the two series $$G_1(x)=\sum_{n=1}^{+\infty}{\mathcal{G}(n)z^n}$$ $$G_2(s)=\sum_{n=1}^{+\infty}{\frac{\mathcal{G}(n)}{n^s}}$$ where $\mathcal{G}(n)$ is the number of finite groups of order $n$. (I don't even know if those series are converging for any $z$ or $s$...) Thanks a lot !,I am wondering if something of interest can be said about one of the two series $$G_1(x)=\sum_{n=1}^{+\infty}{\mathcal{G}(n)z^n}$$ $$G_2(s)=\sum_{n=1}^{+\infty}{\frac{\mathcal{G}(n)}{n^s}}$$ where $\mathcal{G}(n)$ is the number of finite groups of order $n$. (I don't even know if those series are converging for any $z$ or $s$...) Thanks a lot !,,"['complex-analysis', 'finite-groups', 'generating-functions']"
7,Entire function with zeros of even multiplicity is the square of another entire function,Entire function with zeros of even multiplicity is the square of another entire function,,"Let $f: \mathbb{C} \rightarrow \mathbb{C}$ be an entire function such that the multiplicity of each of its zeros is even. Must there exist an entire $g$ such that $f(z) = g(z)^{2}$ ? Progress I thought about  using the Weierstrass Factorization Theorem, but I can't seem to get that to work. In the Weierstrass factorization theorem of $f$ we have an infinite product $\prod_{k = 1}^{\infty}(1 - z/b_{k})^{p_{k}}e^{R_{k}(z)}$ where $p_{k}$ is even, $\{b_{k}\}$ are the zeros of $f$ listed without multiplicity and $R_{k}$ is a polynomial. Then how do I know that $\prod_{k = 1}^{\infty}(1 - z/b_{k})^{p_{k}/2}e^{R_{k}(z)/2}$ is entire?","Let be an entire function such that the multiplicity of each of its zeros is even. Must there exist an entire such that ? Progress I thought about  using the Weierstrass Factorization Theorem, but I can't seem to get that to work. In the Weierstrass factorization theorem of we have an infinite product where is even, are the zeros of listed without multiplicity and is a polynomial. Then how do I know that is entire?",f: \mathbb{C} \rightarrow \mathbb{C} g f(z) = g(z)^{2} f \prod_{k = 1}^{\infty}(1 - z/b_{k})^{p_{k}}e^{R_{k}(z)} p_{k} \{b_{k}\} f R_{k} \prod_{k = 1}^{\infty}(1 - z/b_{k})^{p_{k}/2}e^{R_{k}(z)/2},"['complex-analysis', 'roots', 'factoring']"
8,how to determine the existence of double limit?,how to determine the existence of double limit?,,"Let $f(x,y)$ be a function of two variables. Are there any criterions to determine the existence of double limit $$ \lim_{(x,y)\to(x_0,y_0)} f(x,y)? $$ If for all $y\in(y_0-\delta,y_0+\delta)$, $\lim_{x\to x_0}f(x,y)=A$ uniformly (for any $\epsilon>0$ there exists $\eta>0$ such that for any $y\in(y_0-\delta,y_0+\delta)$ and $|x-x_0|<\eta$, we have $|f(x,y)-f(x_0,y_0)|<\epsilon$), can we claim that double limit $$ \lim_{(x,y)\to(x_0,y_0)} f(x,y) $$ exists? I want some criterions that can be tested...","Let $f(x,y)$ be a function of two variables. Are there any criterions to determine the existence of double limit $$ \lim_{(x,y)\to(x_0,y_0)} f(x,y)? $$ If for all $y\in(y_0-\delta,y_0+\delta)$, $\lim_{x\to x_0}f(x,y)=A$ uniformly (for any $\epsilon>0$ there exists $\eta>0$ such that for any $y\in(y_0-\delta,y_0+\delta)$ and $|x-x_0|<\eta$, we have $|f(x,y)-f(x_0,y_0)|<\epsilon$), can we claim that double limit $$ \lim_{(x,y)\to(x_0,y_0)} f(x,y) $$ exists? I want some criterions that can be tested...",,"['calculus', 'real-analysis', 'complex-analysis', 'analysis', 'convergence-divergence']"
9,Prove that periodic analytic function can be written as $\sum_{-\infty}^{\infty} c_n e^{2\pi inz}$,Prove that periodic analytic function can be written as,\sum_{-\infty}^{\infty} c_n e^{2\pi inz},"This question involves the following homework problem: PROBLEM Suppose $f$ is analytic in the upper half plane and periodic of period 1. Show that $f$ has an extension of the form $$f(z)=\sum_{-\infty}^{\infty} c_n e^{2\pi inz}$$ with $$c_n= \int_0^1 f(x+iy) e^{-2\pi i n (x+iy)}dx$$ for any value $y>0$. HINT Show that there is an analytic function $f^*$ on a disk  from which the origin is deleted such that $$f^*(e^{2\pi i z })=f(z)$$ what is the Laurent series for $f^*$? Abbreviate $q=e^{2\pi i z }$. MY TRY I thought that the extension to the real line should also be analytic: $$g(x):= \lim_{y\text{ to }0}f(x+iy)$$ and then by periodicity and the fact that $f,g$ are analytic this function is equal to its Fourier series everywhere (with possibly complex coefficients). Then by again the fact that   $f,g,\sin$ and cos are analytic this is then equal to the same series using the complex series definition for $\sin$ and $\cos$. I have the idea that I am still missing something, since the hint implies a totally different approach. Thanks!","This question involves the following homework problem: PROBLEM Suppose $f$ is analytic in the upper half plane and periodic of period 1. Show that $f$ has an extension of the form $$f(z)=\sum_{-\infty}^{\infty} c_n e^{2\pi inz}$$ with $$c_n= \int_0^1 f(x+iy) e^{-2\pi i n (x+iy)}dx$$ for any value $y>0$. HINT Show that there is an analytic function $f^*$ on a disk  from which the origin is deleted such that $$f^*(e^{2\pi i z })=f(z)$$ what is the Laurent series for $f^*$? Abbreviate $q=e^{2\pi i z }$. MY TRY I thought that the extension to the real line should also be analytic: $$g(x):= \lim_{y\text{ to }0}f(x+iy)$$ and then by periodicity and the fact that $f,g$ are analytic this function is equal to its Fourier series everywhere (with possibly complex coefficients). Then by again the fact that   $f,g,\sin$ and cos are analytic this is then equal to the same series using the complex series definition for $\sin$ and $\cos$. I have the idea that I am still missing something, since the hint implies a totally different approach. Thanks!",,"['complex-analysis', 'power-series', 'fourier-series', 'laurent-series']"
10,"Show that for a polynomial $P\left(z\right)$ the sum $\sum_{\left\{ y\,:\, P\left(y\right)=z\right\} }P^{'}\left(y\right)$ does not depend on $z$",Show that for a polynomial  the sum  does not depend on,"P\left(z\right) \sum_{\left\{ y\,:\, P\left(y\right)=z\right\} }P^{'}\left(y\right) z","I just came across the following very intriguing question and I'm not sure how to even approach it. Show that for a complex polynomial $P\left(z\right)$ the sum $\sum_{\left\{ y\,:\, P\left(y\right)=z\right\} }P^{'}\left(y\right)   $ does not depend on $z$. Help would be appreciated!","I just came across the following very intriguing question and I'm not sure how to even approach it. Show that for a complex polynomial $P\left(z\right)$ the sum $\sum_{\left\{ y\,:\, P\left(y\right)=z\right\} }P^{'}\left(y\right)   $ does not depend on $z$. Help would be appreciated!",,['complex-analysis']
11,Help with finding the region where the function has an antiderivate...,Help with finding the region where the function has an antiderivate...,,"I'm having trouble in finding the region on wich $f(z) = \exp(1/z)$ has an antiderivative, by making this region as large as i can. And i want to know how that will compare with the real function $f(x) = \exp(1/x)$. Any help would be appreciated. Thanks.","I'm having trouble in finding the region on wich $f(z) = \exp(1/z)$ has an antiderivative, by making this region as large as i can. And i want to know how that will compare with the real function $f(x) = \exp(1/x)$. Any help would be appreciated. Thanks.",,['complex-analysis']
12,How many values does $1^{\alpha}$ have for $\alpha$ irrational?,How many values does  have for  irrational?,1^{\alpha} \alpha,"One such value is $\displaystyle\cos\left(2\pi\alpha\right)+i\sin\left(2\pi\alpha\right)$, by Euler's theorem. On the other hand, we can choose an arbitrary sequence $S=(a_n)_n$ of rational numbers converging to $\alpha$, pick up $a_n=p_n/q_n\in S$ and calculate possible values of $e^{2i\pi p_n/q_n}$. The $q_n$ part will open many branches, but eventually $p_n$ will make some of them become equal. It is not clear whether the number of solutions is growing. Because multivaluedness can cause serious headaches sometimes, let's define some things. Cosine and sine are defined by the usual taylor series and exponentiation by a natural number remains untouched. Roots are calculated by this procedure: $\mathbf{1^{1/q_n}}$ is defined to be the set $$\mathbf{1^{1/q_n}}=\{z:z\in\mathbb{C}\wedge z^{q_n}=1\}.$$ We, then, define $\mathbf{1^{p_n/q_n}}$ to be the set $K_n=\{z^{p_n}:z\in\mathbb{C}\wedge z^{q_n}=1\}$. To avoid even more problems, we let $p_n$ and $q_n$ be coprime. How is the cardinality of $K_n=\mathbf{1^{p_n/q_n}}$ growing over time? I have no intuition in this opening and closing branches game. I want to know the limit $\displaystyle\lim_{n\to\infty}|K_n|$ and a rigorous proof of it.","One such value is $\displaystyle\cos\left(2\pi\alpha\right)+i\sin\left(2\pi\alpha\right)$, by Euler's theorem. On the other hand, we can choose an arbitrary sequence $S=(a_n)_n$ of rational numbers converging to $\alpha$, pick up $a_n=p_n/q_n\in S$ and calculate possible values of $e^{2i\pi p_n/q_n}$. The $q_n$ part will open many branches, but eventually $p_n$ will make some of them become equal. It is not clear whether the number of solutions is growing. Because multivaluedness can cause serious headaches sometimes, let's define some things. Cosine and sine are defined by the usual taylor series and exponentiation by a natural number remains untouched. Roots are calculated by this procedure: $\mathbf{1^{1/q_n}}$ is defined to be the set $$\mathbf{1^{1/q_n}}=\{z:z\in\mathbb{C}\wedge z^{q_n}=1\}.$$ We, then, define $\mathbf{1^{p_n/q_n}}$ to be the set $K_n=\{z^{p_n}:z\in\mathbb{C}\wedge z^{q_n}=1\}$. To avoid even more problems, we let $p_n$ and $q_n$ be coprime. How is the cardinality of $K_n=\mathbf{1^{p_n/q_n}}$ growing over time? I have no intuition in this opening and closing branches game. I want to know the limit $\displaystyle\lim_{n\to\infty}|K_n|$ and a rigorous proof of it.",,"['complex-analysis', 'exponentiation', 'branch-cuts']"
13,$\sum_{n=1}^\infty\frac{z^2}{1+n^2z^2}$ converges to analytic function,converges to analytic function,\sum_{n=1}^\infty\frac{z^2}{1+n^2z^2},"For which $z$ does $\sum_{n=1}^\infty\dfrac{z^2}{1+n^2z^2}$ converge to an analytic function? What are its poles? I think the poles should be $\pm\dfrac{i}{n}$, since those are the values at which one of the denominators disappear. I'm not sure about the converging to analytic function part","For which $z$ does $\sum_{n=1}^\infty\dfrac{z^2}{1+n^2z^2}$ converge to an analytic function? What are its poles? I think the poles should be $\pm\dfrac{i}{n}$, since those are the values at which one of the denominators disappear. I'm not sure about the converging to analytic function part",,"['complex-analysis', 'convergence-divergence']"
14,The existence of a singular point on the circle of convergence (Ahlfors),The existence of a singular point on the circle of convergence (Ahlfors),,"I want to solve the following exercise from Ahlfors' complex analysis text. Below is my attempt followed by questions I have regarding it: If a function element is defined by a power series inside its circle of convergence, supposed to be of finite radius, prove that at least one radius is a singular path for the global analytic function which it determines. (""A power series has at least one singular point on its circle of convergence."") Here is my attempt: Let $f$ be defined and analytic in the disk $\Delta(z_0;R)$ where $R<\infty$ is the radius of convergence around the point $z_0$. Let $\mathbf{f}$ be the global analytic function determined by the function element $(f,\Delta(z_0;R))$, and let $\mathfrak{S}_0(\mathbf{f})$ be its Riemann surface (which is a connected component of the sheaf of germs of analytic functions). For $\theta \in[0,2 \pi)$ denote by $\gamma_\theta:[0,R] \to \mathbb C$ the radius of the circle of convergence parallel to $e^{i \theta}$ (i.e. $\gamma_\theta(t)=z_0+t e^{i \theta}$), and by $\overline{\gamma_\theta}:[0,R] \to \mathfrak{S_0}(\mathbf{f})$ the unique analytic continuation of $\mathbf{f}$ along $\gamma$ (if it exists). By way of contradiction suppose that none of the radii are a singular path. We can extend $f$ to a (univalent) function $F$ which is analytic in a larger concentric disk, and  coincides with $f$ on the original disk $\Delta(z_0;R)$. Clearly, this should be done using analytic continuation along all radii. I would like to define $F$ the following way: For $z \in \Delta(z_0;R)$ set trivially $F(z)=f(z)$ For any germ $\overline{\gamma_\theta}(R)$, consider its power series expansion at $z_0+R e^{i \theta}$. For all points $z$ in the series' disk of convergence, which are not in $\Delta(z_0;R)$, set $F(z)$ to be the value of the series at $z$. The second part seems to be ambiguous, since several different germs may define $F$ at the same point $z \notin \Delta(z_0;R)$. We will now prove that there is no ambiguity. Let $z \notin \Delta(z_0;R)$ be a ""possible ambiguous point"" for $F$. Thus there exist germs $\overline{\gamma_{\theta_1}}(R),\overline{\gamma_{\theta_2}}(R)$ with $\theta_1 \neq \theta_2$ and with power series developments $F_1,F_2$ which converge in the disks $\Delta_1,\Delta_2$ centered at $z_0+Re^{i \theta_1},z_0+Re^{i \theta_2}$ respectively, where both disks contain $z$. Ahlfors has previously proven the equivalence between analytic continuation along arcs and along chains of function elements. Using his construction we can form a chain $\{(f_k,\Omega_k \}_{k=1}^N$ of function elements which follow the radius $\gamma_{\theta_1}$. Combining this with the fact that the analytic continuation along an arc is unique, we find that $$f_{N-1} \big|_{\Delta(z_0;R) \cap \Omega_{N-1}} \equiv f,$$ thus the identity theorem for analytic functions gives $$F_1 \equiv f $$ on $\Delta_1 \cap \Delta(z_0;R)$. Similarly, we find that $$F_2 \equiv f $$ on $\Delta_2 \cap \Delta(z_0;R).$ Now I would like to say that the intersection $\Delta_1 \cap \Delta_2 \cap \Delta(z_0;R)$ is nonempty (by some geometric argument), and from there applying the identity theorem one last time will yield $F_1(z)=F_2(z)$. Hence, $F$ is indeed a univalent analytic function which extends $f$ to an open neighborhood of $\overline{\Delta(z_0;R)}$. By elementary topology, this open neighborhood contains a larger disk $\Delta(z_0,R+\epsilon)$ concentric  with $\Delta(z_0,R)$, contradicting the maximality of the radius of convergence. Questions: Overall, is my solution correct? More specifically, how would one go about proving the ""geometric argument"" that $\Delta(z_0,R) \cap \Delta_1 \cap \Delta_2 \neq \emptyset$? Thanks!","I want to solve the following exercise from Ahlfors' complex analysis text. Below is my attempt followed by questions I have regarding it: If a function element is defined by a power series inside its circle of convergence, supposed to be of finite radius, prove that at least one radius is a singular path for the global analytic function which it determines. (""A power series has at least one singular point on its circle of convergence."") Here is my attempt: Let $f$ be defined and analytic in the disk $\Delta(z_0;R)$ where $R<\infty$ is the radius of convergence around the point $z_0$. Let $\mathbf{f}$ be the global analytic function determined by the function element $(f,\Delta(z_0;R))$, and let $\mathfrak{S}_0(\mathbf{f})$ be its Riemann surface (which is a connected component of the sheaf of germs of analytic functions). For $\theta \in[0,2 \pi)$ denote by $\gamma_\theta:[0,R] \to \mathbb C$ the radius of the circle of convergence parallel to $e^{i \theta}$ (i.e. $\gamma_\theta(t)=z_0+t e^{i \theta}$), and by $\overline{\gamma_\theta}:[0,R] \to \mathfrak{S_0}(\mathbf{f})$ the unique analytic continuation of $\mathbf{f}$ along $\gamma$ (if it exists). By way of contradiction suppose that none of the radii are a singular path. We can extend $f$ to a (univalent) function $F$ which is analytic in a larger concentric disk, and  coincides with $f$ on the original disk $\Delta(z_0;R)$. Clearly, this should be done using analytic continuation along all radii. I would like to define $F$ the following way: For $z \in \Delta(z_0;R)$ set trivially $F(z)=f(z)$ For any germ $\overline{\gamma_\theta}(R)$, consider its power series expansion at $z_0+R e^{i \theta}$. For all points $z$ in the series' disk of convergence, which are not in $\Delta(z_0;R)$, set $F(z)$ to be the value of the series at $z$. The second part seems to be ambiguous, since several different germs may define $F$ at the same point $z \notin \Delta(z_0;R)$. We will now prove that there is no ambiguity. Let $z \notin \Delta(z_0;R)$ be a ""possible ambiguous point"" for $F$. Thus there exist germs $\overline{\gamma_{\theta_1}}(R),\overline{\gamma_{\theta_2}}(R)$ with $\theta_1 \neq \theta_2$ and with power series developments $F_1,F_2$ which converge in the disks $\Delta_1,\Delta_2$ centered at $z_0+Re^{i \theta_1},z_0+Re^{i \theta_2}$ respectively, where both disks contain $z$. Ahlfors has previously proven the equivalence between analytic continuation along arcs and along chains of function elements. Using his construction we can form a chain $\{(f_k,\Omega_k \}_{k=1}^N$ of function elements which follow the radius $\gamma_{\theta_1}$. Combining this with the fact that the analytic continuation along an arc is unique, we find that $$f_{N-1} \big|_{\Delta(z_0;R) \cap \Omega_{N-1}} \equiv f,$$ thus the identity theorem for analytic functions gives $$F_1 \equiv f $$ on $\Delta_1 \cap \Delta(z_0;R)$. Similarly, we find that $$F_2 \equiv f $$ on $\Delta_2 \cap \Delta(z_0;R).$ Now I would like to say that the intersection $\Delta_1 \cap \Delta_2 \cap \Delta(z_0;R)$ is nonempty (by some geometric argument), and from there applying the identity theorem one last time will yield $F_1(z)=F_2(z)$. Hence, $F$ is indeed a univalent analytic function which extends $f$ to an open neighborhood of $\overline{\Delta(z_0;R)}$. By elementary topology, this open neighborhood contains a larger disk $\Delta(z_0,R+\epsilon)$ concentric  with $\Delta(z_0,R)$, contradicting the maximality of the radius of convergence. Questions: Overall, is my solution correct? More specifically, how would one go about proving the ""geometric argument"" that $\Delta(z_0,R) \cap \Delta_1 \cap \Delta_2 \neq \emptyset$? Thanks!",,"['complex-analysis', 'geometry', 'sheaf-theory']"
15,A (possible) error in Ahlfors' text (analytic continuation along arcs),A (possible) error in Ahlfors' text (analytic continuation along arcs),,"In Ahlfors' complex analysis text, page 289 he discusses analytic continuation along arcs. I will start with some background Let $\mathbf{f}$ be a global analytic function, with corresponding Riemann surface $\mathfrak{S_0} (\mathbf{f})$ (which is a connected a component of the sheaf $\mathfrak{S}$ of germs of analytic functions in the complex plane). Ahlfors defines an analytic continuation, along an arc $\gamma:[a,b] \to \mathbb C$ to be a continuous function $\overline{\gamma}:[a,b] \to \mathfrak{S_0}(\mathbf{f})$, such that $\pi \circ \bar{\gamma}=\gamma$, where $\pi$ is the standard projection map. Later in the same page, he investigates the case where analytic continuation is impossible along an arc $\gamma:[a,b] \to \mathbb{C}$, starting at the initial germ $\mathbf{f}_{\zeta(a)}$ (I think that's a typo, and should read $\mathbf{f}_{\gamma(a)}$ instead). He notes that analytic continuation is possible if we restrict ourselves to subarcs $\gamma \big|_{[a,t_0]}$ for small enough $t_0$. Next, he considers the supremum of the set (which I'll call $E$) of all such numbers $t_0$, and denotes it $\tau$. The following claims are made about $\tau$, which I'm having trouble agreeing with : $a<\tau<b$ continuation will be possible for $t_0 < \tau$, impossible for $t_0 \geq \tau$. Regarding the first claim, I agree that $a<\tau$, since $E$ contains numbers greater than $a$ - however, I can't see why $\tau$ is strictly lesser than $b$. Regarding the second claim, I agree that analytic continuation is possible for $t_0<\tau$ since these are members of $E$. I agree also that analytic continuation is impossible for $t_0>\tau$, by the properties of the supremum - however, why is analytic continuation never  possible for the point $t_0=\tau$ itself? Please help me settle this. Thanks!","In Ahlfors' complex analysis text, page 289 he discusses analytic continuation along arcs. I will start with some background Let $\mathbf{f}$ be a global analytic function, with corresponding Riemann surface $\mathfrak{S_0} (\mathbf{f})$ (which is a connected a component of the sheaf $\mathfrak{S}$ of germs of analytic functions in the complex plane). Ahlfors defines an analytic continuation, along an arc $\gamma:[a,b] \to \mathbb C$ to be a continuous function $\overline{\gamma}:[a,b] \to \mathfrak{S_0}(\mathbf{f})$, such that $\pi \circ \bar{\gamma}=\gamma$, where $\pi$ is the standard projection map. Later in the same page, he investigates the case where analytic continuation is impossible along an arc $\gamma:[a,b] \to \mathbb{C}$, starting at the initial germ $\mathbf{f}_{\zeta(a)}$ (I think that's a typo, and should read $\mathbf{f}_{\gamma(a)}$ instead). He notes that analytic continuation is possible if we restrict ourselves to subarcs $\gamma \big|_{[a,t_0]}$ for small enough $t_0$. Next, he considers the supremum of the set (which I'll call $E$) of all such numbers $t_0$, and denotes it $\tau$. The following claims are made about $\tau$, which I'm having trouble agreeing with : $a<\tau<b$ continuation will be possible for $t_0 < \tau$, impossible for $t_0 \geq \tau$. Regarding the first claim, I agree that $a<\tau$, since $E$ contains numbers greater than $a$ - however, I can't see why $\tau$ is strictly lesser than $b$. Regarding the second claim, I agree that analytic continuation is possible for $t_0<\tau$ since these are members of $E$. I agree also that analytic continuation is impossible for $t_0>\tau$, by the properties of the supremum - however, why is analytic continuation never  possible for the point $t_0=\tau$ itself? Please help me settle this. Thanks!",,"['complex-analysis', 'sheaf-theory']"
16,Continuous complex functions.,Continuous complex functions.,,"We are given with a map $g:\bar D\to \Bbb C $, which is continuous on $\bar D$ and analytic on $D$. Where $D$ is a bounded domain and $\bar D=D\cup\partial D$. Then $\partial(g(D))\subseteq g(\partial D).$(I already know, how to prove it). I need two examples: a) First, to show that the above inclusion can be strict, that is: $\partial(g(D))\not= g(\partial D).$ b) Second example, I need to show that conclusion in (1) is not true if $D$ is  not bounded. There is an example, I was working on yesterday. But I couldn't understand it completely. a) If we take $g(z)= z^2$ and $D$ =\begin{cases}z, & \text{where 1<|z|<2} \\\end{cases} This $g$ is not 1-1. Now, we want to prove that $g(\partial D)\not\subset \partial(g(D)) $. Therefore, we need to show that $\exists $ some $z\in g(\partial D)$ but $z \not\in \partial(g(D))$.  How will we show that ??? I want to talk about domain $D$ and its image by map $g$. Please check it: ??","We are given with a map $g:\bar D\to \Bbb C $, which is continuous on $\bar D$ and analytic on $D$. Where $D$ is a bounded domain and $\bar D=D\cup\partial D$. Then $\partial(g(D))\subseteq g(\partial D).$(I already know, how to prove it). I need two examples: a) First, to show that the above inclusion can be strict, that is: $\partial(g(D))\not= g(\partial D).$ b) Second example, I need to show that conclusion in (1) is not true if $D$ is  not bounded. There is an example, I was working on yesterday. But I couldn't understand it completely. a) If we take $g(z)= z^2$ and $D$ =\begin{cases}z, & \text{where 1<|z|<2} \\\end{cases} This $g$ is not 1-1. Now, we want to prove that $g(\partial D)\not\subset \partial(g(D)) $. Therefore, we need to show that $\exists $ some $z\in g(\partial D)$ but $z \not\in \partial(g(D))$.  How will we show that ??? I want to talk about domain $D$ and its image by map $g$. Please check it: ??",,"['complex-analysis', 'analysis']"
17,"If the product of two holomorphic functions is identically zero, then one of the functions is zero","If the product of two holomorphic functions is identically zero, then one of the functions is zero",,"I have to answer the following question: If $f$ and $g$ are holomorphic on some domain $\Omega$ and $f(z)g(z)=0$ for every $z\in \Omega$, then $f(z)=0$ or $g(z)=0$ for every $z\in\Omega$. Is this correct: Let's assume that $f$ is not identically $0$ on the domain. Then there is a point $z_0\in\Omega$ such that $f(z_0)\neq0$. Since $f$ is continuous we find a neighbourhood of $z_0$ on which $f$ is free of zeros, $U$ say (why can I say this?) But since $fg=0$ we have $g=0$ on $U$. From the identity principle we can conclude $g=0$ in $\Omega$.","I have to answer the following question: If $f$ and $g$ are holomorphic on some domain $\Omega$ and $f(z)g(z)=0$ for every $z\in \Omega$, then $f(z)=0$ or $g(z)=0$ for every $z\in\Omega$. Is this correct: Let's assume that $f$ is not identically $0$ on the domain. Then there is a point $z_0\in\Omega$ such that $f(z_0)\neq0$. Since $f$ is continuous we find a neighbourhood of $z_0$ on which $f$ is free of zeros, $U$ say (why can I say this?) But since $fg=0$ we have $g=0$ on $U$. From the identity principle we can conclude $g=0$ in $\Omega$.",,['complex-analysis']
18,Holomorphic function with a fixed point,Holomorphic function with a fixed point,,"Let $f$ be holomorphic on $\overline{N(0,1)}$. Suppose $|f(z)| \leq 1$ for every $|z| = 1$. Without using fixed point theorem, show that there exists $z \in \overline{N(0,1)}$ such that $f(z) = z$. My attempt: Using Rouché's Theorem, I managed to show that if $|f(z)| < 1$ for every $|z| = 1$, then there is a unique $z$ such that $f(z) = z$. However, I am not sure how I should proceed if $|f(z)| = 1$ for some $|z| = 1$.","Let $f$ be holomorphic on $\overline{N(0,1)}$. Suppose $|f(z)| \leq 1$ for every $|z| = 1$. Without using fixed point theorem, show that there exists $z \in \overline{N(0,1)}$ such that $f(z) = z$. My attempt: Using Rouché's Theorem, I managed to show that if $|f(z)| < 1$ for every $|z| = 1$, then there is a unique $z$ such that $f(z) = z$. However, I am not sure how I should proceed if $|f(z)| = 1$ for some $|z| = 1$.",,[]
19,Dirichlet L-series associated to periodic sequence,Dirichlet L-series associated to periodic sequence,,"Let $\{a_n\}$ be a sequence of complex numbers such that $a_n=a_m $ iff $ n\equiv m \mod q$ for some positive integer $q$. Define the Dirichlet L-series associated to $\{a_n\}$ by $$L(s)=\sum_{n=1}^{\infty} \frac{a_n}{n^s} \ \ \  \text{  for   Re}(s)>1. $$ Also define $$Q(x)=\sum_{m=0}^{q-1}a_{q-m} e^{mx}\ \ \ \text{  with   }\ \  a_0=a_q.$$ I showed that $$ L(s)=\frac{1}{\Gamma(s)}\int_{0}^{\infty}\frac{Q(x)x^{s-1}}{e^{qx}-1}dx,  \ \ \text{for   Re}(s)>1  $$ Now I want to show that $L(s)$ is continuable into the complex plane, with the only possible singularity a pole at $s=1$. I follwed the hint in previous asked question , so that $$L(s)=\frac{1}{\Gamma(s)}\int_{0}^{\infty}\frac{(Q(x)-Q(0))x^{s-1}}{e^{qx}-1}dx+\frac{1}{\Gamma(s)}\int_{0}^{\infty}\frac{Q(0)x^{s-1}}{e^{qx}-1}dx$$ I found that the second term equals to $Q(0)\zeta (s)/q^s$, which is meromorphic except a pole at $s=1$ for $Q(0) \neq 0$. So I want to show that the first term is entire in whole plane. But how can I show it?","Let $\{a_n\}$ be a sequence of complex numbers such that $a_n=a_m $ iff $ n\equiv m \mod q$ for some positive integer $q$. Define the Dirichlet L-series associated to $\{a_n\}$ by $$L(s)=\sum_{n=1}^{\infty} \frac{a_n}{n^s} \ \ \  \text{  for   Re}(s)>1. $$ Also define $$Q(x)=\sum_{m=0}^{q-1}a_{q-m} e^{mx}\ \ \ \text{  with   }\ \  a_0=a_q.$$ I showed that $$ L(s)=\frac{1}{\Gamma(s)}\int_{0}^{\infty}\frac{Q(x)x^{s-1}}{e^{qx}-1}dx,  \ \ \text{for   Re}(s)>1  $$ Now I want to show that $L(s)$ is continuable into the complex plane, with the only possible singularity a pole at $s=1$. I follwed the hint in previous asked question , so that $$L(s)=\frac{1}{\Gamma(s)}\int_{0}^{\infty}\frac{(Q(x)-Q(0))x^{s-1}}{e^{qx}-1}dx+\frac{1}{\Gamma(s)}\int_{0}^{\infty}\frac{Q(0)x^{s-1}}{e^{qx}-1}dx$$ I found that the second term equals to $Q(0)\zeta (s)/q^s$, which is meromorphic except a pole at $s=1$ for $Q(0) \neq 0$. So I want to show that the first term is entire in whole plane. But how can I show it?",,['complex-analysis']
20,A problem about contour integral,A problem about contour integral,,"I decide to use contour integral to calculate $I=\int_{-1}^{1}\frac{dx}{(x-2)\sqrt{1-x^{2}}}$ but there's a problem for my result. Following is my process. Denote $f(z)=\frac{1}{(z-2)\sqrt{1-z^{2}}}$, let [-1,1] be the cut and get two analytic branches they are $$f_{0}(z)=\frac{1}{(z-2)\sqrt{\left |1-z^{2}  \right |}e^{i\frac{arg(1-z^{2})}{2}}}$$ and $$f_{1}(z)=\frac{-1}{(z-2)\sqrt{\left |1-z^{2}  \right |}e^{i\frac{arg(1-z^{2})}{2}}}$$ I take the contour as following: contour http://img14.poco.cn/mypoco/myphoto/20130618/11/17398969020130618110820079.png I get a equation that: $$ \lim_{r\rightarrow +\infty ,\varepsilon \rightarrow0^{+} }\int _{\Gamma}f_{0}(z)dz=\int_{-1 }^{1 }f_{0}(x){dx}+\int_{1 }^{-1}f_{1}(x){dx}+\lim_{r\rightarrow +\infty ,\varepsilon \rightarrow0^{+} }\left (\int _{\Gamma_{r}}+\int _{\Gamma_{\varepsilon }}+\int _{{\Gamma_{\varepsilon }}'}  \right )f_{0}(z)dz $$ I work out that $\int _{\Gamma_{r}}f_{0}(z)dz=\int _{\Gamma_{\varepsilon }}f_{0}(z)dz=\int _{{\Gamma_{\varepsilon }}'}f_{0}(z)dz=0$ while $r\rightarrow +\infty $ and $ \varepsilon \rightarrow 0^{+}$ and according to residue theorem I get that  $$ \int _{\Gamma}f_{0}(z)dz=2\pi iRes(f_{0}(z),2)=2\pi i\lim_{z\rightarrow 2}\frac{1}{\sqrt{\left |1-z^{2}  \right |}e^{i\frac{arg(1-z^{2})}{2}}}=2\pi i\frac{1}{\sqrt{3}e^{i\frac{\pi}{2} }}=\frac{2\pi }{\sqrt{3}}$$ otherwise $\int_{-1 }^{1 }f_{0}(x){dx}+\int_{1 }^{-1}f_{1}(x){dx}=2I$ then I have the result that  $I=\frac{1}{2}\frac{2\pi }{\sqrt{3}}=\frac{\pi }{\sqrt{3}}$ but $I$ is explicitly a negative and the true result is $-\frac{\pi }{\sqrt{3}}$ what's wrong?","I decide to use contour integral to calculate $I=\int_{-1}^{1}\frac{dx}{(x-2)\sqrt{1-x^{2}}}$ but there's a problem for my result. Following is my process. Denote $f(z)=\frac{1}{(z-2)\sqrt{1-z^{2}}}$, let [-1,1] be the cut and get two analytic branches they are $$f_{0}(z)=\frac{1}{(z-2)\sqrt{\left |1-z^{2}  \right |}e^{i\frac{arg(1-z^{2})}{2}}}$$ and $$f_{1}(z)=\frac{-1}{(z-2)\sqrt{\left |1-z^{2}  \right |}e^{i\frac{arg(1-z^{2})}{2}}}$$ I take the contour as following: contour http://img14.poco.cn/mypoco/myphoto/20130618/11/17398969020130618110820079.png I get a equation that: $$ \lim_{r\rightarrow +\infty ,\varepsilon \rightarrow0^{+} }\int _{\Gamma}f_{0}(z)dz=\int_{-1 }^{1 }f_{0}(x){dx}+\int_{1 }^{-1}f_{1}(x){dx}+\lim_{r\rightarrow +\infty ,\varepsilon \rightarrow0^{+} }\left (\int _{\Gamma_{r}}+\int _{\Gamma_{\varepsilon }}+\int _{{\Gamma_{\varepsilon }}'}  \right )f_{0}(z)dz $$ I work out that $\int _{\Gamma_{r}}f_{0}(z)dz=\int _{\Gamma_{\varepsilon }}f_{0}(z)dz=\int _{{\Gamma_{\varepsilon }}'}f_{0}(z)dz=0$ while $r\rightarrow +\infty $ and $ \varepsilon \rightarrow 0^{+}$ and according to residue theorem I get that  $$ \int _{\Gamma}f_{0}(z)dz=2\pi iRes(f_{0}(z),2)=2\pi i\lim_{z\rightarrow 2}\frac{1}{\sqrt{\left |1-z^{2}  \right |}e^{i\frac{arg(1-z^{2})}{2}}}=2\pi i\frac{1}{\sqrt{3}e^{i\frac{\pi}{2} }}=\frac{2\pi }{\sqrt{3}}$$ otherwise $\int_{-1 }^{1 }f_{0}(x){dx}+\int_{1 }^{-1}f_{1}(x){dx}=2I$ then I have the result that  $I=\frac{1}{2}\frac{2\pi }{\sqrt{3}}=\frac{\pi }{\sqrt{3}}$ but $I$ is explicitly a negative and the true result is $-\frac{\pi }{\sqrt{3}}$ what's wrong?",,['complex-analysis']
21,Using Morera's theorem to prove analyticity,Using Morera's theorem to prove analyticity,,"I have a function $F(x,y)$ which is continuous and analytic on the complement of a certain function $x(y)$. Is it possible to use Morera's theorem to show that it is analytic everywhere? Clearly, this approach can be used if it fails to be analytic on some linear function $x = a y + b$, for example (as far as I can guess, we could complexify $z = x + iy$ and use Morera's theorem by approximating the integrals over circles passing through the line until we showed analyticity everywhere). If $x(y)$ is also continuous (at least on a certain range of interest), could we use a similar method? If I'm wrong, can anyone think of convincing counterexamples?!","I have a function $F(x,y)$ which is continuous and analytic on the complement of a certain function $x(y)$. Is it possible to use Morera's theorem to show that it is analytic everywhere? Clearly, this approach can be used if it fails to be analytic on some linear function $x = a y + b$, for example (as far as I can guess, we could complexify $z = x + iy$ and use Morera's theorem by approximating the integrals over circles passing through the line until we showed analyticity everywhere). If $x(y)$ is also continuous (at least on a certain range of interest), could we use a similar method? If I'm wrong, can anyone think of convincing counterexamples?!",,['complex-analysis']
22,"Find all entire functions $f$ such that $f^{(n)}(z) = z$ for all $z$, $n$ being a given positive integer","Find all entire functions  such that  for all ,  being a given positive integer",f f^{(n)}(z) = z z n,"Find all entire functions $f$ such that $f^{(n)}(z) = z$ for all $z$, $n$ being a given positive integer. I can not think such a function  exist or not.can somebody help me please","Find all entire functions $f$ such that $f^{(n)}(z) = z$ for all $z$, $n$ being a given positive integer. I can not think such a function  exist or not.can somebody help me please",,['complex-analysis']
23,How to understand $\log{f(z)}$?,How to understand ?,\log{f(z)},"For example, let $\Omega$ be the region $Re(z)>1$ which is simply connected, and let $f(z)=z^9$. I want to find an explicit formula for $\log{f(z)}$ such that $\log{f(z)}$ is holomorphic on $\Omega$ and that $\log{f(z)}$ coincides with $\log{x^9}$ for all real $x>1$. Here is how I did it: Since $f$ is holomorphic and nonvanishing on $\Omega$, there is a holomorphic function $g(z)$ on $\Omega$ such that $e^{g(z)}=f(z)=z^9$ and that $g(x)=9\log{x}$ for real $x>1$. Similarly, there is a holomorphic function $h(z)$ on $\Omega$ such that $e^{h(z)}=z$ and that $h(x)=\log{x}$ for real $x>1$. Since for all real $x>1$ we have $g(x)-9h(x)=0$, we can say that $g(z)-9h(z)=0$ on $\Omega$. On the other hand, we can write $h(z)=\log{|z|}+i\arg{z}$ on $\Omega$ where $\arg{z}\in(-\pi,\pi)$. Thus, since $g(z)=9h(z)$ on $\Omega$, we have $g(z)=9\log{|z|}+9i\arg{z}$ where $\arg{z}\in (-\pi,\pi)$. Is my argument correct or not? If it is, then how can I see the formula for $g(z)$ directly? Moreover, if $f(z)$ is not as simple as a polynomial, how can I give the formula for $\log{f(z)}$ such that $\log{f(z)}$ is holomorphic? Thank you very much.","For example, let $\Omega$ be the region $Re(z)>1$ which is simply connected, and let $f(z)=z^9$. I want to find an explicit formula for $\log{f(z)}$ such that $\log{f(z)}$ is holomorphic on $\Omega$ and that $\log{f(z)}$ coincides with $\log{x^9}$ for all real $x>1$. Here is how I did it: Since $f$ is holomorphic and nonvanishing on $\Omega$, there is a holomorphic function $g(z)$ on $\Omega$ such that $e^{g(z)}=f(z)=z^9$ and that $g(x)=9\log{x}$ for real $x>1$. Similarly, there is a holomorphic function $h(z)$ on $\Omega$ such that $e^{h(z)}=z$ and that $h(x)=\log{x}$ for real $x>1$. Since for all real $x>1$ we have $g(x)-9h(x)=0$, we can say that $g(z)-9h(z)=0$ on $\Omega$. On the other hand, we can write $h(z)=\log{|z|}+i\arg{z}$ on $\Omega$ where $\arg{z}\in(-\pi,\pi)$. Thus, since $g(z)=9h(z)$ on $\Omega$, we have $g(z)=9\log{|z|}+9i\arg{z}$ where $\arg{z}\in (-\pi,\pi)$. Is my argument correct or not? If it is, then how can I see the formula for $g(z)$ directly? Moreover, if $f(z)$ is not as simple as a polynomial, how can I give the formula for $\log{f(z)}$ such that $\log{f(z)}$ is holomorphic? Thank you very much.",,['complex-analysis']
24,An entire function with finite covering group is a polynomial.,An entire function with finite covering group is a polynomial.,,"Let $f$ be an entire function. Think of it as a covering space of $\mathbb{C}$ (perhaps with isolated punctures) to $\mathbb{C}$ (perhaps with isolated punctures). Suppose we know there is only a finite number of covering transformations, $\{\varphi\}_{i=1}^n$: $$ f(\varphi_i(z)) = f(z).$$ How to show $f$ is a polynomial ? Partial answer: The function $\frac{f(z)-f(w)}{z-w}$ is entire in both variables. It is only zero when $z=\varphi_i(w)$ for some $i$, so we may write  $$\frac{f(z)-f(w)}{z-w} = e^{g(z,w)} \prod_{i=1}^n (z-\varphi_i(w)).$$ Now setting $w=0$, and denoting $w_i = \varphi_i(0)$, we obtain: $$\frac{f(z)-f(0)}{z} = e^{g(z,0)} \prod_{i=1}^n (z-w_i)=$$ $$ e^{g(z,0)} (z^n - (\sum w_i) z^{n-1} + \ldots + (-1)^n \prod w_i). $$ To show this is a polynomial amounts to showing $g(z,0)$ is a constant.","Let $f$ be an entire function. Think of it as a covering space of $\mathbb{C}$ (perhaps with isolated punctures) to $\mathbb{C}$ (perhaps with isolated punctures). Suppose we know there is only a finite number of covering transformations, $\{\varphi\}_{i=1}^n$: $$ f(\varphi_i(z)) = f(z).$$ How to show $f$ is a polynomial ? Partial answer: The function $\frac{f(z)-f(w)}{z-w}$ is entire in both variables. It is only zero when $z=\varphi_i(w)$ for some $i$, so we may write  $$\frac{f(z)-f(w)}{z-w} = e^{g(z,w)} \prod_{i=1}^n (z-\varphi_i(w)).$$ Now setting $w=0$, and denoting $w_i = \varphi_i(0)$, we obtain: $$\frac{f(z)-f(0)}{z} = e^{g(z,0)} \prod_{i=1}^n (z-w_i)=$$ $$ e^{g(z,0)} (z^n - (\sum w_i) z^{n-1} + \ldots + (-1)^n \prod w_i). $$ To show this is a polynomial amounts to showing $g(z,0)$ is a constant.",,"['complex-analysis', 'covering-spaces']"
25,Finding a conformal map from unit disk to half-plane,Finding a conformal map from unit disk to half-plane,,"I'm trying to find a conformal map $f$ from the open unit disk to the set $\mathbb{C}-[-1/4,-\infty)$ (I think this means the half-plane Re$(w)>-1/4$ with the properties $f(0)=0$ and $f'(0)>0$. I know that the mapping $$f(z)=\frac{i+z}{i-z}$$ returns the right half-plane Re$(w)>0$ from the open unit disk, but subtracting 1/4 from it doesn't satisfy $f(0)=0$. I can't seem to find a lot of other examples. Are there any other conformal maps that I should try?","I'm trying to find a conformal map $f$ from the open unit disk to the set $\mathbb{C}-[-1/4,-\infty)$ (I think this means the half-plane Re$(w)>-1/4$ with the properties $f(0)=0$ and $f'(0)>0$. I know that the mapping $$f(z)=\frac{i+z}{i-z}$$ returns the right half-plane Re$(w)>0$ from the open unit disk, but subtracting 1/4 from it doesn't satisfy $f(0)=0$. I can't seem to find a lot of other examples. Are there any other conformal maps that I should try?",,"['complex-analysis', 'conformal-geometry']"
26,Average value of a complex valued function on a circle.,Average value of a complex valued function on a circle.,,The following is an exercise from Complex Analysis by Stephen Fisher. Fix a complex number $a$ and a positive real number $R$. Suppose $u$ is a function defined on the circle of radius $R$ centered at $a$. Let $C$ denote this circle. Show that the average value of $u$ on $C$ is given by $\frac{1}{2\pi}\int_{0}^{2\pi} u(a + Re^{it})dt$. Any Hints please.,The following is an exercise from Complex Analysis by Stephen Fisher. Fix a complex number $a$ and a positive real number $R$. Suppose $u$ is a function defined on the circle of radius $R$ centered at $a$. Let $C$ denote this circle. Show that the average value of $u$ on $C$ is given by $\frac{1}{2\pi}\int_{0}^{2\pi} u(a + Re^{it})dt$. Any Hints please.,,['complex-analysis']
27,Fixed Point of a complex dynamical spiral system,Fixed Point of a complex dynamical spiral system,,"Last semester I finished my first class on complex variables and of course we had to show that $i^i$ was real. That got me wondering about quantities like $i^{i^i}$ and similar power towers. For my investigation, I let $f:\mathbb{C} \to \mathbb{C}$ where $f(z)=(ui)^z$ with $u \in \mathbb{R}$ and let $f_n(z)$ denote the quantity $f(f(\cdots f(z)$ where $f$ occurs $n$ times. I then plotted the points generated by $\{f(ui),f_2(ui),f_3(ui),\ldots,f_k(ui)\}$ for various values of $u$. The plots that I obtained are quite interesting! The top one is a plot of the points $\{f(ui),f_2(ui),f_3(ui),\ldots,f_{100}(ui)\}$ with the real axis on the horizontal and imaginary on the vertical, and $u$ going from .05 to 2.05 in increments of .1. Before .05 it blows up and after 2, the points seem to settle into 3 groups near $(0,u),(0,0)$, and $(1,0)$. The second picture is the same as the first, but with lines connecting $f_k(ui)$ to $f_{k+1}(ui)$. Just a note, the dots do spiral inward with successive nestings, so my inkling of convergence is well-founded, and $|f_k(ui)|$ seems to converge only for $0 < u <2$. Does anyone have any insights into the values of $u$ for which this system converges or if this has been written about before? Even reference to a method for determining the existence of a fixed point would be appreciated.","Last semester I finished my first class on complex variables and of course we had to show that $i^i$ was real. That got me wondering about quantities like $i^{i^i}$ and similar power towers. For my investigation, I let $f:\mathbb{C} \to \mathbb{C}$ where $f(z)=(ui)^z$ with $u \in \mathbb{R}$ and let $f_n(z)$ denote the quantity $f(f(\cdots f(z)$ where $f$ occurs $n$ times. I then plotted the points generated by $\{f(ui),f_2(ui),f_3(ui),\ldots,f_k(ui)\}$ for various values of $u$. The plots that I obtained are quite interesting! The top one is a plot of the points $\{f(ui),f_2(ui),f_3(ui),\ldots,f_{100}(ui)\}$ with the real axis on the horizontal and imaginary on the vertical, and $u$ going from .05 to 2.05 in increments of .1. Before .05 it blows up and after 2, the points seem to settle into 3 groups near $(0,u),(0,0)$, and $(1,0)$. The second picture is the same as the first, but with lines connecting $f_k(ui)$ to $f_{k+1}(ui)$. Just a note, the dots do spiral inward with successive nestings, so my inkling of convergence is well-founded, and $|f_k(ui)|$ seems to converge only for $0 < u <2$. Does anyone have any insights into the values of $u$ for which this system converges or if this has been written about before? Even reference to a method for determining the existence of a fixed point would be appreciated.",,"['complex-analysis', 'dynamical-systems', 'fixed-point-theorems', 'complex-dynamics']"
28,A sequence that tell us if a holomorphic function of several variables is identically zero,A sequence that tell us if a holomorphic function of several variables is identically zero,,"Is there any sequence $\{ Z_{\nu} \}_{\nu \in \mathbb{N}}$ in $\mathbb{C}^{n}$, $Z_{\nu} \rightarrow 0$, such that any holomorphic function in $\mathbb{C}^{n}$ which vanishes in $Z_{\nu}$ for all $\nu \in \mathbb{N}$ is identically zero? Thank you!","Is there any sequence $\{ Z_{\nu} \}_{\nu \in \mathbb{N}}$ in $\mathbb{C}^{n}$, $Z_{\nu} \rightarrow 0$, such that any holomorphic function in $\mathbb{C}^{n}$ which vanishes in $Z_{\nu}$ for all $\nu \in \mathbb{N}$ is identically zero? Thank you!",,"['complex-analysis', 'several-complex-variables']"
29,On lower bounds for contour integrals and their divergence,On lower bounds for contour integrals and their divergence,,"I want to find a lower bound for $\left|\int_\gamma f(z)dz\right|$. I know of the estimation lemma and Jordan's lemma for an upper bound, but I don't know of any for a lower bound. The motivation is that I want to prove that a certain integral diverges on a given contour, and I'm looking for ways to do that. I think that for a given smooth contour $\gamma_R$ that depends on a parameter $R$, and a given function $f(z)$ such that $|f(z)|\rightarrow\infty$ as $R\rightarrow\infty$, one can conclude that $\left|\int_{\gamma_R}f(z)dz\right|\rightarrow\infty$ as $R\rightarrow\infty$. In an attempt to prove that, I use a naive approximation for $\left|\int_{\gamma_R}f\right|$: $$\left|\int_{\gamma_R}f\right|=\left|\int_If(\gamma_R(t))\dot{\gamma_R}(t)\right|\geq \left|\sum_{j=1}^nf\left(\gamma_R\left(\frac{j}{n}\right)\right)\dot\gamma_R\left(\frac{j}{n}\right)\right|$$ here I use a lower bound of the Riemann integral after the choice of parameterization for $\gamma_R$, without loss of generality and for ease of writing I assumed that the interval of the parameterization is $[0,1]$ and I used the partition of $I$ to $n$ subintervals of length $\frac{1}{n}$. And then using the triangle inequality and the assumption we have that $\left|\int_{\gamma_R}f(z)dz\right|\rightarrow\infty$ (On second thought, we might also need that the derivative of $\gamma_R$ behaves nice enough). So, to be very specific, the 3 questions I have here: a) is there an ""estimation lemma"" for a lower bound? b) is my reasoning correct in the above proof? c) are there criteria for contour integral divergence or similar tests? Thanks","I want to find a lower bound for $\left|\int_\gamma f(z)dz\right|$. I know of the estimation lemma and Jordan's lemma for an upper bound, but I don't know of any for a lower bound. The motivation is that I want to prove that a certain integral diverges on a given contour, and I'm looking for ways to do that. I think that for a given smooth contour $\gamma_R$ that depends on a parameter $R$, and a given function $f(z)$ such that $|f(z)|\rightarrow\infty$ as $R\rightarrow\infty$, one can conclude that $\left|\int_{\gamma_R}f(z)dz\right|\rightarrow\infty$ as $R\rightarrow\infty$. In an attempt to prove that, I use a naive approximation for $\left|\int_{\gamma_R}f\right|$: $$\left|\int_{\gamma_R}f\right|=\left|\int_If(\gamma_R(t))\dot{\gamma_R}(t)\right|\geq \left|\sum_{j=1}^nf\left(\gamma_R\left(\frac{j}{n}\right)\right)\dot\gamma_R\left(\frac{j}{n}\right)\right|$$ here I use a lower bound of the Riemann integral after the choice of parameterization for $\gamma_R$, without loss of generality and for ease of writing I assumed that the interval of the parameterization is $[0,1]$ and I used the partition of $I$ to $n$ subintervals of length $\frac{1}{n}$. And then using the triangle inequality and the assumption we have that $\left|\int_{\gamma_R}f(z)dz\right|\rightarrow\infty$ (On second thought, we might also need that the derivative of $\gamma_R$ behaves nice enough). So, to be very specific, the 3 questions I have here: a) is there an ""estimation lemma"" for a lower bound? b) is my reasoning correct in the above proof? c) are there criteria for contour integral divergence or similar tests? Thanks",,"['complex-analysis', 'contour-integration']"
30,"conformal but not real differentiable, still holomorphic?","conformal but not real differentiable, still holomorphic?",,"can one prove that a conformal map (one which preserves angles (in sense as well as in size) between arcs with non-zero derivatives at a point z and for which the limit of the absolute value of the difference quotient exists at z) has a complex derivative at z without assuming that it is differentiable at z as a map between IR^2? If so, how? If not can you give me a counterexample? Thanks","can one prove that a conformal map (one which preserves angles (in sense as well as in size) between arcs with non-zero derivatives at a point z and for which the limit of the absolute value of the difference quotient exists at z) has a complex derivative at z without assuming that it is differentiable at z as a map between IR^2? If so, how? If not can you give me a counterexample? Thanks",,['complex-analysis']
31,At which points is this function differentiable/analytic?,At which points is this function differentiable/analytic?,,At which points (if any) is this function differentiable? At which points is it analytic? $f(x+iy) = x^2 + iy^2$ I applied the Cauchy Riemann equations and got the result that $y=-x$. So then am I correct to say that the function is only differentiable on the line $y=-x$ and is analytic nowhere as it is not differentiable at every point in any small disc centred on the line?,At which points (if any) is this function differentiable? At which points is it analytic? $f(x+iy) = x^2 + iy^2$ I applied the Cauchy Riemann equations and got the result that $y=-x$. So then am I correct to say that the function is only differentiable on the line $y=-x$ and is analytic nowhere as it is not differentiable at every point in any small disc centred on the line?,,['complex-analysis']
32,Is there a Möbius transformation that scales disks to the unit disk?,Is there a Möbius transformation that scales disks to the unit disk?,,"When working in the complex plane, often times I would like to scale a disk $|z-z_0|<R$ to the unit disk. I would first translate $z_0$ to the origin, but after that, what can we multiply by to scale the radius down from $R$ to $1$? I'm curious because in reading a proof of Schwarz' lemma, one can map a disk $|z|<r$ to the unit disk with some point $z_0$ mapping to $0$. The transformation is given by $$ \frac{r(z-z_0)}{r^2-\bar{z}_0z} $$ but I don't understand how that formula comes up. How does one come up with it?","When working in the complex plane, often times I would like to scale a disk $|z-z_0|<R$ to the unit disk. I would first translate $z_0$ to the origin, but after that, what can we multiply by to scale the radius down from $R$ to $1$? I'm curious because in reading a proof of Schwarz' lemma, one can map a disk $|z|<r$ to the unit disk with some point $z_0$ mapping to $0$. The transformation is given by $$ \frac{r(z-z_0)}{r^2-\bar{z}_0z} $$ but I don't understand how that formula comes up. How does one come up with it?",,"['geometry', 'complex-analysis']"
33,Looser Conditions on Casorati-Weierstrass,Looser Conditions on Casorati-Weierstrass,,"The Casorati-Weierstrass Theorem presented in Stein and Shakarchi's ""Complex Analysis"" discusses the behavior of the image of a homlomorphic function in a punctured disc about an essential singularity. I show that the function need not be holomorphic (or meromorphic) as long as its infinitely many poles converge to the essential singularity. In this way, I think we can put a ""looser"" condition of Casorati-Weierstrass. I know this is bad form to just post links to read stuff, but I prove it here: http://www.princeton.edu/~rghanta/Casorati-Weierstrass_2.pdf It is a two page write-up, and look on the second page for my proof! I don't think this result is that big of a deal, but I am wondering if anyone has seen this result before cited in another book? If so, where can I look. And now to the primary reason for posting this: Since I have already descended down this route and I find essential singularities very interesting, do you have any suggestions of where I can go from what I have shown? Do you recommend any texts or papers that I can read to better understand behavior near an essential singularity. I am already aware of Picard's Theorem, but I'm also interested if there is anything else we can say about essential singularities. I appreciate your help!","The Casorati-Weierstrass Theorem presented in Stein and Shakarchi's ""Complex Analysis"" discusses the behavior of the image of a homlomorphic function in a punctured disc about an essential singularity. I show that the function need not be holomorphic (or meromorphic) as long as its infinitely many poles converge to the essential singularity. In this way, I think we can put a ""looser"" condition of Casorati-Weierstrass. I know this is bad form to just post links to read stuff, but I prove it here: http://www.princeton.edu/~rghanta/Casorati-Weierstrass_2.pdf It is a two page write-up, and look on the second page for my proof! I don't think this result is that big of a deal, but I am wondering if anyone has seen this result before cited in another book? If so, where can I look. And now to the primary reason for posting this: Since I have already descended down this route and I find essential singularities very interesting, do you have any suggestions of where I can go from what I have shown? Do you recommend any texts or papers that I can read to better understand behavior near an essential singularity. I am already aware of Picard's Theorem, but I'm also interested if there is anything else we can say about essential singularities. I appreciate your help!",,['complex-analysis']
34,Conformal structure of regions of the complex plane and the ring of holomorphic functions,Conformal structure of regions of the complex plane and the ring of holomorphic functions,,How is the conformal structure of regions of the complex plane determined by the integral domain of holomorphic functions defined on those regions? Thanks,How is the conformal structure of regions of the complex plane determined by the integral domain of holomorphic functions defined on those regions? Thanks,,[]
35,Simplifications to a Large (Dottie) Integral,Simplifications to a Large (Dottie) Integral,,"While working on a problem, I encountered this integral. $$ \alpha \in \left(-\frac{3\pi }{2},\frac{\pi }{2}\right),\ \ \ \mathfrak{D}_{\alpha }=\pi+\frac{1}{\pi}\int _0^{\infty }\left(1-\cosh \left(z\right)\right)\left(\frac{\frac{9\pi ^2}{4}+\frac{3\pi }{2}\alpha -z\sinh \left(z\right)+z^2}{\left(\sinh \left(z\right)-z\right)^2+\left(\frac{3\pi }{2}+\alpha \right)^2}-\frac{\frac{\pi ^2}{4}+z^2-z\sinh \left(z\right)-\frac{\pi }{2}\alpha }{\left(\alpha -\frac{\pi }{2}\right)^2+\left(z-\sinh \left(z\right)\right)^2}\right)\mathrm{d}z $$ Derriving inspiration from this paper ( NOTE: THIS LINK DOWNLOADS A PDF!! ) I also found that: $$ \alpha \in \left(-\frac{\pi }{2},\frac{\pi }{2}\right), \ \ \ \mathfrak{D}_{\alpha }=\frac{\pi }{2}-\frac{1}{2\pi }\int _0^{\infty }\ln \left(\frac{\left(t+\sinh \left(t\right)\right)^2+\left(\alpha +\frac{\pi }{2}\right)^2}{\left(t-\sinh \left(t\right)\right)^2+\left(\alpha -\frac{\pi }{2}\right)^2}\right)\mathrm{d}t $$ Are there any simplifications possible to express the first integral more concisely? (If you are curious, $\mathfrak{D}_{\alpha }=\cos \left(\cos \left(\cdots \left(\cos \left(x\right)-\alpha \right)\cdots -\alpha \right)-\alpha \right)-\alpha $ . Which is the solution to $\cos \left(x\right)-x=\alpha $ ).","While working on a problem, I encountered this integral. Derriving inspiration from this paper ( NOTE: THIS LINK DOWNLOADS A PDF!! ) I also found that: Are there any simplifications possible to express the first integral more concisely? (If you are curious, . Which is the solution to ).","
\alpha \in \left(-\frac{3\pi }{2},\frac{\pi }{2}\right),\ \ \ \mathfrak{D}_{\alpha }=\pi+\frac{1}{\pi}\int _0^{\infty }\left(1-\cosh \left(z\right)\right)\left(\frac{\frac{9\pi ^2}{4}+\frac{3\pi }{2}\alpha -z\sinh \left(z\right)+z^2}{\left(\sinh \left(z\right)-z\right)^2+\left(\frac{3\pi }{2}+\alpha \right)^2}-\frac{\frac{\pi ^2}{4}+z^2-z\sinh \left(z\right)-\frac{\pi }{2}\alpha }{\left(\alpha -\frac{\pi }{2}\right)^2+\left(z-\sinh \left(z\right)\right)^2}\right)\mathrm{d}z
 
\alpha \in \left(-\frac{\pi }{2},\frac{\pi }{2}\right), \ \ \ \mathfrak{D}_{\alpha }=\frac{\pi }{2}-\frac{1}{2\pi }\int _0^{\infty }\ln \left(\frac{\left(t+\sinh \left(t\right)\right)^2+\left(\alpha +\frac{\pi }{2}\right)^2}{\left(t-\sinh \left(t\right)\right)^2+\left(\alpha -\frac{\pi }{2}\right)^2}\right)\mathrm{d}t
 \mathfrak{D}_{\alpha }=\cos \left(\cos \left(\cdots \left(\cos \left(x\right)-\alpha \right)\cdots -\alpha \right)-\alpha \right)-\alpha  \cos \left(x\right)-x=\alpha ","['calculus', 'complex-analysis']"
36,Function is decreasing,Function is decreasing,,"I am pushing this calculation a little further and editing it. Perhaps someone could help. The function $s(t)$ defined as $$ s(t) = -\alpha r_i -\alpha r_0 \left( \frac{\sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}{\sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}} \right) $$ with $$ \Gamma_{hi}^{\left(j\right)}=\prod \limits_{l=j, l\neq h}^i \left(\gamma^h-\gamma^l \right), r_i = r_0 \gamma^i $$ where $\gamma > 0, N_0 > 0, r_0 > 0 $ . It can be easily plotted and seen that this function is always decreasing. However, I want to show this analytically. My attempt is as under, but I am stuck on how to proceed further. Any other method is also appreciated. $$ s'(t) = \frac{\alpha^2 r_0}{N_0} \left(\frac{\sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i-1}{\gamma^h \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} - \sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}{\left( \sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \right)^2} \right) $$ If $s(t)$ is always decreasing, then $ s'(t) < 0, \forall t $ , and we must have $$ \sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i-1}{\gamma^h \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} - \sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} < 0 $$ or $$ \frac{\sum \limits_{h=j}^{i-1}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}{\sum \limits_{h=j}^{i-1}{\frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}  < \frac{\sum \limits_{h=j}^i{\gamma^h\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}} {\sum \limits_{h=j}^{i}{ \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}} $$ Obviously, $\Gamma_{hi}^{\left(j\right)}$ has one factor more than $\Gamma_{h,i-1}^{\left(j\right)}$ , and therefore, due to the involvement of the summation index $h$ , these can't be compared. However, taking their absolute values, one can compare them easily. Thus $$ \lvert{\frac{1}{\Gamma_{hi}^{\left(j\right)}}} < \lvert{\frac{1}{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert \; \; \; {\&} \; \; \; \lvert{\frac{\gamma^h}{\Gamma_{hi}^{\left(j\right)}}}\rvert < \lvert{\frac{\gamma^h}{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert, \;\; \text{as $\gamma > 1$ } $$ Now $$    \lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert     \le     \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{hi}^{\left(j\right)}}\rvert}}    \le \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}} $$ $$ \implies \lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert     \le     \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}} $$ Further, by dropping the $i^{th}$ term from the summation, we have $$ \sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}     <     \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}} $$ also, $$ \lvert{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert     \le     \sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}} $$ Since $\gamma > 1$ , therefore, $$\gamma^h > 0 \quad \text{for any} \quad h \in \mathbb{Z}^+$$ . Thus from above, we can write $$  \lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert     <     \sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert}. $$ Now combining the above $$     \frac{\lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}     {\lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}     >     \frac{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}     {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}     \ge     \frac{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}     {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}} $$ By the $\triangle$ ular inequality on the right numerator, we can wirte $$     \frac{\lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}     {\lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}\rvert}}}     >     \frac{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert}     {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}     \ge     \frac{\lvert{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert}     {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}} $$ Now how to proceed further?","I am pushing this calculation a little further and editing it. Perhaps someone could help. The function defined as with where . It can be easily plotted and seen that this function is always decreasing. However, I want to show this analytically. My attempt is as under, but I am stuck on how to proceed further. Any other method is also appreciated. If is always decreasing, then , and we must have or Obviously, has one factor more than , and therefore, due to the involvement of the summation index , these can't be compared. However, taking their absolute values, one can compare them easily. Thus Now Further, by dropping the term from the summation, we have also, Since , therefore, . Thus from above, we can write Now combining the above By the ular inequality on the right numerator, we can wirte Now how to proceed further?","s(t) 
s(t) = -\alpha r_i -\alpha r_0 \left( \frac{\sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}{\sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}} \right)
 
\Gamma_{hi}^{\left(j\right)}=\prod \limits_{l=j, l\neq h}^i \left(\gamma^h-\gamma^l \right), r_i = r_0 \gamma^i
 \gamma > 0, N_0 > 0, r_0 > 0  
s'(t) = \frac{\alpha^2 r_0}{N_0} \left(\frac{\sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i-1}{\gamma^h \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} - \sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}{\left( \sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \right)^2} \right)
 s(t)  s'(t) < 0, \forall t  
\sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i-1}{\gamma^h \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} - \sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} < 0
 
\frac{\sum \limits_{h=j}^{i-1}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}{\sum \limits_{h=j}^{i-1}{\frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}
 < \frac{\sum \limits_{h=j}^i{\gamma^h\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}} {\sum \limits_{h=j}^{i}{ \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}
 \Gamma_{hi}^{\left(j\right)} \Gamma_{h,i-1}^{\left(j\right)} h 
\lvert{\frac{1}{\Gamma_{hi}^{\left(j\right)}}} < \lvert{\frac{1}{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert \; \; \;
{\&} \; \; \;
\lvert{\frac{\gamma^h}{\Gamma_{hi}^{\left(j\right)}}}\rvert < \lvert{\frac{\gamma^h}{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert, \;\; \text{as \gamma > 1 }
 
   \lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert
    \le
    \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{hi}^{\left(j\right)}}\rvert}}
   \le
\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
 
\implies
\lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert
    \le
    \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
 i^{th} 
\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
    <
    \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
 
\lvert{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert
    \le
    \sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
 \gamma > 1 \gamma^h > 0 \quad \text{for any} \quad h \in \mathbb{Z}^+ 
 \lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert
    <
    \sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert}.
 
    \frac{\lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}
    {\lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}
    >
    \frac{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
    {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
    \ge
    \frac{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
    {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
 \triangle 
    \frac{\lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}
    {\lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}\rvert}}}
    >
    \frac{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert}
    {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
    \ge
    \frac{\lvert{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert}
    {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
","['real-analysis', 'complex-analysis', 'analysis']"
37,"How can I prove that if a polynomial has its coefficients specified by continuous functions, its roots will also be continuous functions?","How can I prove that if a polynomial has its coefficients specified by continuous functions, its roots will also be continuous functions?",,"So I want to prove that if we have a polynomial in the form of $f(t,z)=\sum_{i=0}^n a_i(t) z^i$ where the coefficients $a_i(t)$ are continuous w.r.t. $t$ , there then exists a continuous function $r_k(t)$ such that $f(t,r_k(t))=0$ , or in other words such that at least one of the roots of $f(t,z)$ is continuous w.r.t. $t$ . I think I understand proofs of continuity like in: https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://aalexan3.math.ncsu.edu/articles/polyroots.pdf&ved=2ahUKEwjJgfDHj-GDAxW5ITQIHZA5C5wQFnoECBgQAQ&usg=AOvVaw3akZPgEB32DtPskNGBcsF _ which show through Rouche that for a given polynomial which has a minimum magnitude of the difference between its roots as $\tilde{r}$ and for any $0<\epsilon<\frac{\tilde{r}}{2}$ , we will have that there exists a $\delta>0$ s.t. if we perturb the coefficients by any values less than or equal to $\delta$ we will have that the roots of the resulting perturbed polynomial will be within $\epsilon$ of the original roots (magnitude wise). I can see how this is pretty much the definition of continuity, but the upper bound on $\epsilon$ is confusing to me. Isn't it possible that the upper bound on this $\epsilon$ gets arbitrarily small as we perturb the coefficients and roots begin to approach each other? Also, how could we prove an intermediate value theorem for the roots with this upper limit on $\epsilon$ ? Sorry if these are foolish questions. $\textbf{EDIT}$ : oh I just realized that if the above is true for some $\epsilon$ , then it is obviously true for any larger value of $\epsilon$ . Also, looking at the Rouche proof, I think that $\epsilon$ just has to be such that the balls of radius $\epsilon$ about the roots of the original polynomial do not contain other roots on their boundary, so I think I can just consider a larger $\epsilon$ that bounds roots that are becoming arbitrarily close together in order for the resulting $\delta$ to be well defined even as roots potentially become identical as we perturb the coefficients, and I think that pretty much gives me a straightforward $\epsilon-\delta$ continuous relation between the coefficients and resulting roots. I am still not sure though, but I think the above can give me the result for any $\epsilon>0$ where I can consider $0<\epsilon_1<\frac{\tilde{r}_1}{2}$ as in the original proof to get a corresponding $\delta(\epsilon_1)$ , and this $\delta(\epsilon_1)$ also covers the case for $\hat{\epsilon}_1=\frac{\tilde{r}_1}{2} $ . Then I can consider $\frac{\tilde{r}_1}{2}<\epsilon_2<\frac{\tilde{r}_2}{2}$ where $\tilde{r}_2$ is the next minimal difference in the polynomial roots. Since this $\epsilon_2$ is such that all of the balls centered on the roots still do not have any roots on their boundaries, I am pretty sure I can still use the linked Rouche proof to obtain a suitable $\delta(\epsilon_2)$ . Continuing in this manner for all the possible root differences and beyond, I think I can arrive at the familiar ""for all $\epsilon>0$ "" definition of continuity even if multiple roots begin to converge towards being identical.","So I want to prove that if we have a polynomial in the form of where the coefficients are continuous w.r.t. , there then exists a continuous function such that , or in other words such that at least one of the roots of is continuous w.r.t. . I think I understand proofs of continuity like in: https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://aalexan3.math.ncsu.edu/articles/polyroots.pdf&ved=2ahUKEwjJgfDHj-GDAxW5ITQIHZA5C5wQFnoECBgQAQ&usg=AOvVaw3akZPgEB32DtPskNGBcsF _ which show through Rouche that for a given polynomial which has a minimum magnitude of the difference between its roots as and for any , we will have that there exists a s.t. if we perturb the coefficients by any values less than or equal to we will have that the roots of the resulting perturbed polynomial will be within of the original roots (magnitude wise). I can see how this is pretty much the definition of continuity, but the upper bound on is confusing to me. Isn't it possible that the upper bound on this gets arbitrarily small as we perturb the coefficients and roots begin to approach each other? Also, how could we prove an intermediate value theorem for the roots with this upper limit on ? Sorry if these are foolish questions. : oh I just realized that if the above is true for some , then it is obviously true for any larger value of . Also, looking at the Rouche proof, I think that just has to be such that the balls of radius about the roots of the original polynomial do not contain other roots on their boundary, so I think I can just consider a larger that bounds roots that are becoming arbitrarily close together in order for the resulting to be well defined even as roots potentially become identical as we perturb the coefficients, and I think that pretty much gives me a straightforward continuous relation between the coefficients and resulting roots. I am still not sure though, but I think the above can give me the result for any where I can consider as in the original proof to get a corresponding , and this also covers the case for . Then I can consider where is the next minimal difference in the polynomial roots. Since this is such that all of the balls centered on the roots still do not have any roots on their boundaries, I am pretty sure I can still use the linked Rouche proof to obtain a suitable . Continuing in this manner for all the possible root differences and beyond, I think I can arrive at the familiar ""for all "" definition of continuity even if multiple roots begin to converge towards being identical.","f(t,z)=\sum_{i=0}^n a_i(t) z^i a_i(t) t r_k(t) f(t,r_k(t))=0 f(t,z) t \tilde{r} 0<\epsilon<\frac{\tilde{r}}{2} \delta>0 \delta \epsilon \epsilon \epsilon \epsilon \textbf{EDIT} \epsilon \epsilon \epsilon \epsilon \epsilon \delta \epsilon-\delta \epsilon>0 0<\epsilon_1<\frac{\tilde{r}_1}{2} \delta(\epsilon_1) \delta(\epsilon_1) \hat{\epsilon}_1=\frac{\tilde{r}_1}{2}  \frac{\tilde{r}_1}{2}<\epsilon_2<\frac{\tilde{r}_2}{2} \tilde{r}_2 \epsilon_2 \delta(\epsilon_2) \epsilon>0","['real-analysis', 'complex-analysis', 'polynomials', 'roots']"
38,Description of Riemann surface of polynomial inverse,Description of Riemann surface of polynomial inverse,,"My question is about page 4 of the pdf of the following paper , one does $\textbf{not}$ need to read pages 1-3 of the paper to understand my question ( the $\textbf{only}$ part that needs to be read is provided in the image below). The precise part I struggle to understand  is highlighted in the below image: $\textbf{Question}$ : Accepting that $P^{\ast}$ maps the sector $G$ univalently onto the described domain, I fail to see how to apply the Schwarz reflection principle (called the ""Riemann-Schwarz Symmetry principle"" in the paper) to find that the Riemann surface of $(P^{\ast})^{-1}$ has the structure described. $\textbf{Observations}:$ It seems that the description of the Riemann surface of $(P^{\ast})^{-1}$ suggests the following: There must be $n$ possible branches of $(P^{\ast})^{-1}$ definable on $\mathbb{C} \setminus (\bigcup_{k=2}^{n} T_k^{\ast}) = \mathbb{C} \setminus \{ |w| \geq |\alpha_{n}| : \arg(w) = \pi + \frac{2 \pi (k-2) }{n-1} , 2 \leq k \leq n \}$ , because this domain is simply connected and omits all the critical values of $P^{\ast}$ , here $T_k^{\ast} = \{ |w| \geq |\alpha_{n}| : \arg(w) = \pi + \frac{2 \pi (k-2) }{n-1} \}$ . This holds independently of the proposed description of the Riemann surface. Now using the description of the Riemann surface, we should have the following: $(\textbf{1})$ : $n-1$ of these $n$ branches, which we may index as $P_{k},k=2,...,n$ are such that each $P_{k}$ is holomorphic on $\mathbb{C} \setminus T_{k}^{\ast}$ and is discontinuous on the corresponding cut $T_k^{\ast}$ , and another branch $P_1$ is defined on $\mathbb{C} \setminus (\bigcup_{k=2}^{n} T_k^{\ast})$ and is discontinuous on every cut $T_{k}^{\ast}, k=2,...,n$ $(\textbf{2})$ : The branching behaviour is as follows: Crossing the cut $T_{k}^{\ast}$ starting with the branch $P_1$ moves us to the branch $P_k$ , and vice versa (i.e. starting from $P_k$ and crossing the cut $T_k^{\ast}$ moves us back to $P_1$ ), this comes directly from the description of the surface. In other words, proving (1) and (2) using the method proposed by the author (applying the Riemann-Schwarz symmetry principle) or otherwise, one will also be able to obtain the precise description of the Riemann surface as given in the paper. $\textbf{Edit}$ : I will award the bounty to any answer that explains (in detail) why the Riemann surface has the description proposed by the author regardless of whether the explanation involves the Schwarz reflection principle, unless there is also an answer that explains the description of the Riemann surface using the method proposed by the author (using the Schwarz reflection principle) before the bounty closing time.","My question is about page 4 of the pdf of the following paper , one does need to read pages 1-3 of the paper to understand my question ( the part that needs to be read is provided in the image below). The precise part I struggle to understand  is highlighted in the below image: : Accepting that maps the sector univalently onto the described domain, I fail to see how to apply the Schwarz reflection principle (called the ""Riemann-Schwarz Symmetry principle"" in the paper) to find that the Riemann surface of has the structure described. It seems that the description of the Riemann surface of suggests the following: There must be possible branches of definable on , because this domain is simply connected and omits all the critical values of , here . This holds independently of the proposed description of the Riemann surface. Now using the description of the Riemann surface, we should have the following: : of these branches, which we may index as are such that each is holomorphic on and is discontinuous on the corresponding cut , and another branch is defined on and is discontinuous on every cut : The branching behaviour is as follows: Crossing the cut starting with the branch moves us to the branch , and vice versa (i.e. starting from and crossing the cut moves us back to ), this comes directly from the description of the surface. In other words, proving (1) and (2) using the method proposed by the author (applying the Riemann-Schwarz symmetry principle) or otherwise, one will also be able to obtain the precise description of the Riemann surface as given in the paper. : I will award the bounty to any answer that explains (in detail) why the Riemann surface has the description proposed by the author regardless of whether the explanation involves the Schwarz reflection principle, unless there is also an answer that explains the description of the Riemann surface using the method proposed by the author (using the Schwarz reflection principle) before the bounty closing time.","\textbf{not} \textbf{only} \textbf{Question} P^{\ast} G (P^{\ast})^{-1} \textbf{Observations}: (P^{\ast})^{-1} n (P^{\ast})^{-1} \mathbb{C} \setminus (\bigcup_{k=2}^{n} T_k^{\ast}) = \mathbb{C} \setminus \{ |w| \geq |\alpha_{n}| : \arg(w) = \pi + \frac{2 \pi (k-2)
}{n-1} , 2 \leq k \leq n \} P^{\ast} T_k^{\ast} = \{ |w| \geq |\alpha_{n}| : \arg(w) = \pi + \frac{2 \pi (k-2)
}{n-1} \} (\textbf{1}) n-1 n P_{k},k=2,...,n P_{k} \mathbb{C} \setminus T_{k}^{\ast} T_k^{\ast} P_1 \mathbb{C} \setminus (\bigcup_{k=2}^{n} T_k^{\ast}) T_{k}^{\ast}, k=2,...,n (\textbf{2}) T_{k}^{\ast} P_1 P_k P_k T_k^{\ast} P_1 \textbf{Edit}","['complex-analysis', 'polynomials', 'riemann-surfaces', 'analytic-continuation']"
39,What set do you get if you close the rational numbers under exponentiation?,What set do you get if you close the rational numbers under exponentiation?,,"The rational numbers are closed under addition and multiplication, with inverses for both. But what happens if you introduce exponentiation and ask for closure under it? I know there isn't a meaningful inverse, and that you immediately expand into the irrationals and complex numbers, but do you get all the complex numbers? If yes, how can you show this? If not, has this subset of the complex plane been studied? Edit 1: I don't mean only taking rational numbers to rational number powers, I mean you start with rationals to rational powers, but then can use any resulting numbers as base or exponent. Edit 2: Upon reflection I am pretty sure that because you start with a countably infinite set, and you only insist on closure under an operation applied at most a finite number of times, the resulting set will only be countably infinite. So it cannot cover the complex plane. Edit 3: Karl pointed out that exponentiation isn't always single-valued, so in any case where there multiple solutions, we could insist that all are contained in the closure set.","The rational numbers are closed under addition and multiplication, with inverses for both. But what happens if you introduce exponentiation and ask for closure under it? I know there isn't a meaningful inverse, and that you immediately expand into the irrationals and complex numbers, but do you get all the complex numbers? If yes, how can you show this? If not, has this subset of the complex plane been studied? Edit 1: I don't mean only taking rational numbers to rational number powers, I mean you start with rationals to rational powers, but then can use any resulting numbers as base or exponent. Edit 2: Upon reflection I am pretty sure that because you start with a countably infinite set, and you only insist on closure under an operation applied at most a finite number of times, the resulting set will only be countably infinite. So it cannot cover the complex plane. Edit 3: Karl pointed out that exponentiation isn't always single-valued, so in any case where there multiple solutions, we could insist that all are contained in the closure set.",,"['complex-analysis', 'exponentiation']"
40,Bounding a Real Function in Absolute Value,Bounding a Real Function in Absolute Value,,"Show that if $\omega$ is a primitive $2m^\text{th}$ root of unity, where $m\geq 1$ is an integer, then for all $i\geq 0$ and $x\in [0,1]$ , we have the inequality $$\prod_{j=0}^{2m-1}|1-x^{(2i+1)m+j}\omega^j|\leq 1.$$ I am convinced of this result, and numerical experimentation seems to confirm it, although I have yet to find a simple proof of it. I came across this result trying to show that a certain complex analytic function has a natural boundary of the unit circle, but the statement itself seems to be of independent interest (or at least independent musing-ability). I believe the following to be true: the quantity on the left is a decreasing function of $x$ for fixed $i$ and an increasing function of $i$ for fixed $x$ ; if we can prove either of these assertions, then we will be done, but neither seems to be very tractable. Further, no such result is true (even with minor modifications to account for indexing problems) for primitive odd roots of unity. Here's one proof attempt: one could try to compare with the product $$\prod_{j=0}^{2m-1}|1-x^{(2i+1)m}\omega^j|=|1-x^{2(2i+1)m^2}|\leq 1,$$ but this doesn't work because in general it is not true that $$\prod_{j=0}^{2m-1}|1-x^{(2i+1)m+j}\omega^j|\leq \prod_{j=0}^{2m-1}|1-x^{(2i+1)m}\omega^j|.$$ However, this proof attempt works for $m\in \{1,2\}$ (for which this last inequality does hold), and gives some insight to why such a result could be true in general. Note that the limit as $x\to 1^{-}$ of the quantity on the left hand size is $0$ , so the question is really only challenging when $x$ (or more specifically $x^i$ ) is small.","Show that if is a primitive root of unity, where is an integer, then for all and , we have the inequality I am convinced of this result, and numerical experimentation seems to confirm it, although I have yet to find a simple proof of it. I came across this result trying to show that a certain complex analytic function has a natural boundary of the unit circle, but the statement itself seems to be of independent interest (or at least independent musing-ability). I believe the following to be true: the quantity on the left is a decreasing function of for fixed and an increasing function of for fixed ; if we can prove either of these assertions, then we will be done, but neither seems to be very tractable. Further, no such result is true (even with minor modifications to account for indexing problems) for primitive odd roots of unity. Here's one proof attempt: one could try to compare with the product but this doesn't work because in general it is not true that However, this proof attempt works for (for which this last inequality does hold), and gives some insight to why such a result could be true in general. Note that the limit as of the quantity on the left hand size is , so the question is really only challenging when (or more specifically ) is small.","\omega 2m^\text{th} m\geq 1 i\geq 0 x\in [0,1] \prod_{j=0}^{2m-1}|1-x^{(2i+1)m+j}\omega^j|\leq 1. x i i x \prod_{j=0}^{2m-1}|1-x^{(2i+1)m}\omega^j|=|1-x^{2(2i+1)m^2}|\leq 1, \prod_{j=0}^{2m-1}|1-x^{(2i+1)m+j}\omega^j|\leq \prod_{j=0}^{2m-1}|1-x^{(2i+1)m}\omega^j|. m\in \{1,2\} x\to 1^{-} 0 x x^i","['real-analysis', 'complex-analysis', 'upper-lower-bounds', 'roots-of-unity']"
41,Find function $F(s)$ such that $F(s+1) = F(s) - \frac{1}{(s-2)^2}$,Find function  such that,F(s) F(s+1) = F(s) - \frac{1}{(s-2)^2},"I want to find a function $F(s)$ such that $F(s+1) = F(s) - \frac{1}{(s-2)^2}$ . I first consider the function $\psi(s) = \frac{\Gamma'(s)}{\Gamma(s)}$ . Then I have $\psi(s+1) = \frac{\Gamma'(s+1)}{\Gamma(s+1)}$ . Then I use the identity that $\Gamma(s+1) = s\Gamma(s)$ , and the product rule of the derivative. I get $$\psi(s+1) = \frac{\Gamma(s) - s\Gamma'(s)}{s\Gamma(s)}$$ Then I have $$\psi(s+1) - \psi(s) = \frac{\Gamma(s) + s\Gamma'(s) - s\Gamma'(s)}{s\Gamma(s)}$$ which is equal to $\frac{1}{s}$ . But I'm confused about to get the desired function from here. Could anyone help me please? Thanks! UPDATE I take the derivative on the both sides of the equation and I get $$\psi'(s+1) - \psi'(s) = -\frac{1}{s^2}$$ Then I think that my next step would be shifting $s$ to the right by 2 CONCLUSION I choose my $F(s)$ to be $\psi'(s-2)$ . Then I have $$F(s+1) - F(s) = -\frac{1}{(s-2)^2}$$ which is desired","I want to find a function such that . I first consider the function . Then I have . Then I use the identity that , and the product rule of the derivative. I get Then I have which is equal to . But I'm confused about to get the desired function from here. Could anyone help me please? Thanks! UPDATE I take the derivative on the both sides of the equation and I get Then I think that my next step would be shifting to the right by 2 CONCLUSION I choose my to be . Then I have which is desired",F(s) F(s+1) = F(s) - \frac{1}{(s-2)^2} \psi(s) = \frac{\Gamma'(s)}{\Gamma(s)} \psi(s+1) = \frac{\Gamma'(s+1)}{\Gamma(s+1)} \Gamma(s+1) = s\Gamma(s) \psi(s+1) = \frac{\Gamma(s) - s\Gamma'(s)}{s\Gamma(s)} \psi(s+1) - \psi(s) = \frac{\Gamma(s) + s\Gamma'(s) - s\Gamma'(s)}{s\Gamma(s)} \frac{1}{s} \psi'(s+1) - \psi'(s) = -\frac{1}{s^2} s F(s) \psi'(s-2) F(s+1) - F(s) = -\frac{1}{(s-2)^2},"['complex-analysis', 'functional-equations']"
42,Integral form of a telescoping series,Integral form of a telescoping series,,"I want to know if the following relation is true $$ \frac{1}{2\pi i}  \int_{-\infty}^{\infty} dz  \tanh{\pi z} \left[\frac{1}{\sqrt{1-z^2}} - \frac{1}{\sqrt{2-z^2}} \right] = \sum_{n>0} \left[\frac{1}{\sqrt{1+(n+1/2)^2}} - \frac{1}{\sqrt{2+(n+1/2)^2}} \right]. $$ I will name the last series $S$ . I consider a complex contour that begins in the negative part of the real axis, goes around the points $\pm 1, \pm \sqrt{2}$ with half circle, then closes with a half circle in the upper half plane. The small half circles on the real axis won't give any contribution to the full integral because the residues at $z=\pm 1, \pm \sqrt{2}$ will be zero. Same for the half circle in the upper half plane when we took its radius to be large (the integral for the $\frac{1}{\sqrt{1-z^2}}$ term will be canceled by the $\frac{1}{\sqrt{2-z^2}}$ term for radius that tends to infinity). Now the contour enclosed poles of $\tanh{\pi z}$ which are at the points $i(n+1/2)$ , $n>0$ . By the residue theorem I get that the integral written above $I= 2\pi i S$ . I tried to numerically evaluate $I$ in Mathematica to compare it to the numerical value of $S$ . I found significative differences between the two. So I would like to know if my calculation is flawed.","I want to know if the following relation is true I will name the last series . I consider a complex contour that begins in the negative part of the real axis, goes around the points with half circle, then closes with a half circle in the upper half plane. The small half circles on the real axis won't give any contribution to the full integral because the residues at will be zero. Same for the half circle in the upper half plane when we took its radius to be large (the integral for the term will be canceled by the term for radius that tends to infinity). Now the contour enclosed poles of which are at the points , . By the residue theorem I get that the integral written above . I tried to numerically evaluate in Mathematica to compare it to the numerical value of . I found significative differences between the two. So I would like to know if my calculation is flawed."," \frac{1}{2\pi i}  \int_{-\infty}^{\infty} dz  \tanh{\pi z} \left[\frac{1}{\sqrt{1-z^2}} - \frac{1}{\sqrt{2-z^2}} \right] = \sum_{n>0} \left[\frac{1}{\sqrt{1+(n+1/2)^2}} - \frac{1}{\sqrt{2+(n+1/2)^2}} \right].  S \pm 1, \pm \sqrt{2} z=\pm 1, \pm \sqrt{2} \frac{1}{\sqrt{1-z^2}} \frac{1}{\sqrt{2-z^2}} \tanh{\pi z} i(n+1/2) n>0 I= 2\pi i S I S","['complex-analysis', 'complex-integration']"
43,Why do we need to include the factor $a_{n}^{2n-2}$ in the discriminant of a polynomial?,Why do we need to include the factor  in the discriminant of a polynomial?,a_{n}^{2n-2},"Question: Why do we need to include the factor $a_{n}^{2n-2}$ in the discriminant of a polynomial? Here is the definition of the discriminant ( $\Delta$ ) in terms of the roots $r_1,r_2,...$ : $$ \Delta=a_{n}^{2n-2}\prod_{i<j}(r_i-r_j)^2 $$ for the polynomial $a_nx^n+a_{n-1}x^{n-1}+...+a_0$ . If the polynomial equation has non-real coefficients (in particular if $a_n$ is not real), then it is pointless to tell whether $\Delta>0$ and so in general I can't tell the number of real roots using this way for a polynomial equation with non-real coefficients. However, for a polynomial equation with only real coefficients, even if we just consider $\prod_{i<j}(r_i-r_j)^2$ , where $a_{n}^{2n-2}$ is not included, we still have: $$\Delta >0, \text{if the number of complex roots} \equiv 0\mod 4$$ $$\Delta =0, \text{if there is a multiple root}$$ $$\Delta<0, \text{otherwise}$$ So, why isn't this the definition of discriminant? What difference does the factor $a_{n}^{2n-2}$ make in the non-real coefficients case? Any help will be appreciated!","Question: Why do we need to include the factor in the discriminant of a polynomial? Here is the definition of the discriminant ( ) in terms of the roots : for the polynomial . If the polynomial equation has non-real coefficients (in particular if is not real), then it is pointless to tell whether and so in general I can't tell the number of real roots using this way for a polynomial equation with non-real coefficients. However, for a polynomial equation with only real coefficients, even if we just consider , where is not included, we still have: So, why isn't this the definition of discriminant? What difference does the factor make in the non-real coefficients case? Any help will be appreciated!","a_{n}^{2n-2} \Delta r_1,r_2,...  \Delta=a_{n}^{2n-2}\prod_{i<j}(r_i-r_j)^2  a_nx^n+a_{n-1}x^{n-1}+...+a_0 a_n \Delta>0 \prod_{i<j}(r_i-r_j)^2 a_{n}^{2n-2} \Delta >0, \text{if the number of complex roots} \equiv 0\mod 4 \Delta =0, \text{if there is a multiple root} \Delta<0, \text{otherwise} a_{n}^{2n-2}","['real-analysis', 'complex-analysis', 'algebra-precalculus', 'polynomials', 'discriminant']"
44,Confusions when differentiating a Fourier Transform and using the Kramers-Kronig relations,Confusions when differentiating a Fourier Transform and using the Kramers-Kronig relations,,"Let $x(t)$ be a real-valued squared-integrable signal that have a beginning and an ending time (so, is of finite duration). Then I will have that its Fourier Transform: $$X(w) = U(w)+iV(w)$$ with $\{U(w),\ V(w)\}\in\mathbb{R}$ is such that: $X(w)$ is analytic $U(-w) = U(w)$ $V(-w) = -V(w)$ Kramers–Kronig relations : $U(w) = \displaystyle{\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi}$ and $V(w) = \displaystyle{-\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}d\xi}$ Now, since differentiating the Fourier Transform goes at follow: $\displaystyle{\frac{\partial X(w)}{\partial w}=iw X(w)} \equiv iwU(w)-wV(w)\tag{Prop. 1}$ $$\Rightarrow \frac{\partial X(w)}{\partial w}\; \overset{\text{linearity of}\,\frac{\partial}{\partial w}}{=}\; \frac{\partial U(w)}{\partial w}+i\frac{\partial V(w)}{\partial w} \overset{\text{Prop. 1}}{\equiv} iwU(w)-wV(w)$$ so by pairing the real and imaginary parts: $\frac{\partial U(w)}{\partial w} = -wV(w) \tag{Eq. 1}$ $\frac{\partial V(w)}{\partial w} = wU(w) \tag{Eq. 2}$ Then, for example for the first one, I will have: $$V(w) = -\frac{1}{w}\frac{\partial U(w)}{\partial w} = -\frac{1}{w}\frac{\partial}{\partial w}\left(\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right) \tag{Eq. 3}$$ but I don't know how to differentiate $\frac{\partial}{\partial w}\left(\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)$ : if I use Wolfram-Alpha it says that: $$\frac{\partial}{\partial w}\left(\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)= \int\limits_{-\infty}^{\infty} \frac{V(\xi)}{(\xi-w)^2}d\xi$$ bit I am not sure if its right because the Principal Value implies is a complex integral. How to differentiate $\frac{\partial}{\partial w}\left(\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)$ ? Does $\text{Eq. 3}$ set an equation to find a restricted form for $V(w)$ as a function of $w$ ? or it just will end in something like $V(w)=V(w)$ ? It is possible to solve this equation for $V(w)$ ? Does the equations $\text{Eq. 1}$ and $\text{Eq. 2}$ behave as similar restrictions as the Cauchy–Riemann equations ? As example, taking the second derivative of $X(w)$ : $$\frac{\partial^2 X(w)}{\partial w^2} = (iw)^2X(w) = -w^2X(w) = -w^2U(w)-iw^2V(w) \tag{Eq. 4}$$ But if I use $\text{Eq. 1}$ and $\text{Eq. 2}$ , by differentiating them I will have: $$ \frac{\partial^2 U(w)}{\partial w^2} = \frac{\partial}{\partial w}\left(\frac{\partial U(w)}{\partial w}\right) \overset{\text{Eq. 1}}{=}  \frac{\partial}{\partial w}\left(-wV(w)\right) =-V(w)-w\frac{\partial V(w)}{\partial w}  \overset{\text{Eq. 2}}{=} -w^2U(w)-V(w) \tag{Eq. 5} $$ $$\frac{\partial^2 V(w)}{\partial w^2} = \frac{\partial}{\partial w}\left(\frac{\partial V(w)}{\partial w}\right) \overset{\text{Eq. 2}}{=} \frac{\partial}{\partial w}\left(\; wU(w)\right) = \; U(w)+w \frac{\partial U(w)}{\partial w} \overset{\text{Eq. 1}}{=} -w^2V(w) +U(w) \tag{Eq. 6}$$ Now since, $$\begin{array}{r c l} \displaystyle{\frac{\partial^2 X(w)}{\partial w^2}} & = & \displaystyle{\frac{\partial^2 U(w)}{\partial w^2}+i\frac{\partial^2 V(w)}{\partial w^2}} \\ & \overset{\text{Eq. 5 & Eq. 6}}{=} & -w^2U(w)-V(w)+i\left(-w^2V(w) +U(w)\right) \\ & = & -w^2U(w)-iw^2V(w)+i\left(U(w) +iV(w)\right) \\ & = & -w^2X(w)+iX(w) = (i-w^2)X(w) \\ & \neq & \text{Eq. 4} \end{array}$$ Since I found a contradiction, please explain where and why I am making the mistake. Added later___________________ I found in this answer that: $$\frac{\partial}{\partial w}\left(\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)= \text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)-V(w)}{(\xi-w)^2}d\xi$$ so it somehow solves point $(1)$ , but I still don't know if and how to use it to make a differential equation for $V(w)$ , so it still missing an answer  for point $(2)$ . About the mentioned differential equation, I believe that the matching of $\text{Eq. 1}$ and $\text{Eq. 2}$ is an illegal operation, which leads to the contradiction, but I still don't know why and I would like to know were I am making a conceptual mistake... my intuition tells me is related with something about non-uniqueness of the complex derivative when Cauchy-Riemann Equations aren't hold: I believe this is the case of the Fourier Transform where only the imaginary axis is considered. As example, following $\text{Eq. 1}$ and $\text{Eq. 2}$ I can make the following differential equations: $$\begin{array}{l} V''-\frac{V'}{w}+w^2V=0\\ U''-\frac{U'}{w}+w^2U=0\\ \Rightarrow y''-\frac{y'}{w}+w^2y=0 \Rightarrow y(w) = c_1\cos\left(\frac{w^2}{2}\right)+c_2\sin\left(\frac{w^2}{2}\right) \end{array}$$ where not just imply that $U \equiv V$ which is false, but also the solution is an even function which don't fulfill the properties of $V(w)$ which is odd - an also is a fixed solution when the transform can have multiple values. The problem I think is that the transform $X(w)$ which is related to any function (so it could take many forms), cannot be considered as a function in their differentiation property: $$\frac{\partial X(w)}{\partial w} = iw X(w) \overset{\text{as function}}{\Rightarrow} X'(w)-iwX(w)=0 \Rightarrow X(w)=X(0)e^{i\frac{w^2}{2}}=\int\limits_{-\infty}^{\infty}x(t)dt\ e^{i\frac{w^2}{2}}$$ which is a fixed functions differently from the transform, so it is a conceptual mistake, but I would like to know why it don't work: Does it imply is not possible to make a differential equation for $V(w)$ ? As example using the Kramer-Kronig relation an $\text{Eq. 2}$ : $$V(w) = \displaystyle{-\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{1}{\xi(\xi-w)}\cdot\frac{\partial V(\xi)}{\partial \xi}\,d\xi}$$ Is this conceptually wrong? Does having a differential equation fix the solution so is not applicable in the transforms framework? My guess is $\frac{\partial X(w)}{\partial w} \neq \frac{\partial U(w)}{\partial w}+i\frac{\partial V(w)}{\partial w}$ but I don't know why is not plausible, since the derivative is a linear operator.","Let be a real-valued squared-integrable signal that have a beginning and an ending time (so, is of finite duration). Then I will have that its Fourier Transform: with is such that: is analytic Kramers–Kronig relations : and Now, since differentiating the Fourier Transform goes at follow: so by pairing the real and imaginary parts: Then, for example for the first one, I will have: but I don't know how to differentiate : if I use Wolfram-Alpha it says that: bit I am not sure if its right because the Principal Value implies is a complex integral. How to differentiate ? Does set an equation to find a restricted form for as a function of ? or it just will end in something like ? It is possible to solve this equation for ? Does the equations and behave as similar restrictions as the Cauchy–Riemann equations ? As example, taking the second derivative of : But if I use and , by differentiating them I will have: Now since, Since I found a contradiction, please explain where and why I am making the mistake. Added later___________________ I found in this answer that: so it somehow solves point , but I still don't know if and how to use it to make a differential equation for , so it still missing an answer  for point . About the mentioned differential equation, I believe that the matching of and is an illegal operation, which leads to the contradiction, but I still don't know why and I would like to know were I am making a conceptual mistake... my intuition tells me is related with something about non-uniqueness of the complex derivative when Cauchy-Riemann Equations aren't hold: I believe this is the case of the Fourier Transform where only the imaginary axis is considered. As example, following and I can make the following differential equations: where not just imply that which is false, but also the solution is an even function which don't fulfill the properties of which is odd - an also is a fixed solution when the transform can have multiple values. The problem I think is that the transform which is related to any function (so it could take many forms), cannot be considered as a function in their differentiation property: which is a fixed functions differently from the transform, so it is a conceptual mistake, but I would like to know why it don't work: Does it imply is not possible to make a differential equation for ? As example using the Kramer-Kronig relation an : Is this conceptually wrong? Does having a differential equation fix the solution so is not applicable in the transforms framework? My guess is but I don't know why is not plausible, since the derivative is a linear operator.","x(t) X(w) = U(w)+iV(w) \{U(w),\ V(w)\}\in\mathbb{R} X(w) U(-w) = U(w) V(-w) = -V(w) U(w) = \displaystyle{\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi} V(w) = \displaystyle{-\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}d\xi} \displaystyle{\frac{\partial X(w)}{\partial w}=iw X(w)} \equiv iwU(w)-wV(w)\tag{Prop. 1} \Rightarrow \frac{\partial X(w)}{\partial w}\; \overset{\text{linearity of}\,\frac{\partial}{\partial w}}{=}\; \frac{\partial U(w)}{\partial w}+i\frac{\partial V(w)}{\partial w} \overset{\text{Prop. 1}}{\equiv} iwU(w)-wV(w) \frac{\partial U(w)}{\partial w} = -wV(w) \tag{Eq. 1} \frac{\partial V(w)}{\partial w} = wU(w) \tag{Eq. 2} V(w) = -\frac{1}{w}\frac{\partial U(w)}{\partial w} = -\frac{1}{w}\frac{\partial}{\partial w}\left(\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right) \tag{Eq. 3} \frac{\partial}{\partial w}\left(\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right) \frac{\partial}{\partial w}\left(\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)= \int\limits_{-\infty}^{\infty} \frac{V(\xi)}{(\xi-w)^2}d\xi \frac{\partial}{\partial w}\left(\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right) \text{Eq. 3} V(w) w V(w)=V(w) V(w) \text{Eq. 1} \text{Eq. 2} X(w) \frac{\partial^2 X(w)}{\partial w^2} = (iw)^2X(w) = -w^2X(w) = -w^2U(w)-iw^2V(w) \tag{Eq. 4} \text{Eq. 1} \text{Eq. 2} 
\frac{\partial^2 U(w)}{\partial w^2} = \frac{\partial}{\partial w}\left(\frac{\partial U(w)}{\partial w}\right) \overset{\text{Eq. 1}}{=}  \frac{\partial}{\partial w}\left(-wV(w)\right) =-V(w)-w\frac{\partial V(w)}{\partial w}  \overset{\text{Eq. 2}}{=} -w^2U(w)-V(w) \tag{Eq. 5}  \frac{\partial^2 V(w)}{\partial w^2} = \frac{\partial}{\partial w}\left(\frac{\partial V(w)}{\partial w}\right) \overset{\text{Eq. 2}}{=} \frac{\partial}{\partial w}\left(\; wU(w)\right) = \; U(w)+w \frac{\partial U(w)}{\partial w} \overset{\text{Eq. 1}}{=} -w^2V(w) +U(w) \tag{Eq. 6} \begin{array}{r c l}
\displaystyle{\frac{\partial^2 X(w)}{\partial w^2}} & = & \displaystyle{\frac{\partial^2 U(w)}{\partial w^2}+i\frac{\partial^2 V(w)}{\partial w^2}} \\
& \overset{\text{Eq. 5 & Eq. 6}}{=} & -w^2U(w)-V(w)+i\left(-w^2V(w) +U(w)\right) \\
& = & -w^2U(w)-iw^2V(w)+i\left(U(w) +iV(w)\right) \\
& = & -w^2X(w)+iX(w) = (i-w^2)X(w) \\
& \neq & \text{Eq. 4}
\end{array} \frac{\partial}{\partial w}\left(\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)= \text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)-V(w)}{(\xi-w)^2}d\xi (1) V(w) (2) \text{Eq. 1} \text{Eq. 2} \text{Eq. 1} \text{Eq. 2} \begin{array}{l}
V''-\frac{V'}{w}+w^2V=0\\
U''-\frac{U'}{w}+w^2U=0\\
\Rightarrow y''-\frac{y'}{w}+w^2y=0 \Rightarrow y(w) = c_1\cos\left(\frac{w^2}{2}\right)+c_2\sin\left(\frac{w^2}{2}\right)
\end{array} U \equiv V V(w) X(w) \frac{\partial X(w)}{\partial w} = iw X(w) \overset{\text{as function}}{\Rightarrow} X'(w)-iwX(w)=0 \Rightarrow X(w)=X(0)e^{i\frac{w^2}{2}}=\int\limits_{-\infty}^{\infty}x(t)dt\ e^{i\frac{w^2}{2}} V(w) \text{Eq. 2} V(w) = \displaystyle{-\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{1}{\xi(\xi-w)}\cdot\frac{\partial V(\xi)}{\partial \xi}\,d\xi} \frac{\partial X(w)}{\partial w} \neq \frac{\partial U(w)}{\partial w}+i\frac{\partial V(w)}{\partial w}","['real-analysis', 'complex-analysis', 'fourier-analysis', 'fourier-transform', 'distribution-theory']"
45,"If $f$ is holomorphic and injective, then $f'$ is non zero","If  is holomorphic and injective, then  is non zero",f f',"I know there are already many posts about this question, and I've read and understood the proof in Stein and Shakarchi, but I don't understand why my “shortcut” is wrong. Assume $f$ is injective and holomophic in $\Omega$ open set and $f'(z_0) = 0$ for a $z_0 \in \Omega$ . Consider $g(z) = f(z_0+z)-f(z_0)$ , this way $g(0) = g'(0) = 0$ and $g$ is holomorphic and injective in $\Omega-z_0$ . Now, since $g$ and $g'$ are analytic and non-constant, $0$ is an isolated zero for both of them, that is, there exists a disc $D \subseteq \Omega-z_0$ around $0$ such that $g$ and $g'$ have no zeroes in $D \setminus \{0\}$ . Take $w$ such that $0 < |w| < \min_{\partial D} |g|$ , then for Rouché's Theorem $g$ and $g-w$ have the same number of zeroes (counting the multiplicity) in $\mathring{D}$ . Since $0$ is a zero of multiplicity at least $2$ for $g$ , $g-w = 0$ has at least $2$ solutions in $\mathring{D} \setminus \{0\}$ which are different, because otherwise $g'$ would have to be $0$ in that point, leading to a contradiction.","I know there are already many posts about this question, and I've read and understood the proof in Stein and Shakarchi, but I don't understand why my “shortcut” is wrong. Assume is injective and holomophic in open set and for a . Consider , this way and is holomorphic and injective in . Now, since and are analytic and non-constant, is an isolated zero for both of them, that is, there exists a disc around such that and have no zeroes in . Take such that , then for Rouché's Theorem and have the same number of zeroes (counting the multiplicity) in . Since is a zero of multiplicity at least for , has at least solutions in which are different, because otherwise would have to be in that point, leading to a contradiction.",f \Omega f'(z_0) = 0 z_0 \in \Omega g(z) = f(z_0+z)-f(z_0) g(0) = g'(0) = 0 g \Omega-z_0 g g' 0 D \subseteq \Omega-z_0 0 g g' D \setminus \{0\} w 0 < |w| < \min_{\partial D} |g| g g-w \mathring{D} 0 2 g g-w = 0 2 \mathring{D} \setminus \{0\} g' 0,"['complex-analysis', 'solution-verification']"
46,Estimate of the $n$-th derivative of $\frac{x}{(x+1)(x+2)-2e^{-3x}}$,Estimate of the -th derivative of,n \frac{x}{(x+1)(x+2)-2e^{-3x}},"For $x > 0$ let $$ f(x) = \frac{x}{(x+1)(x+2)}. $$ I would like to estimate the $n$ -th derivative of $f$ . To this end I use partial fractions decomposition $$ f(x) = \frac{2}{x+2} - \frac{1}{x+1} $$ and easily obtain $$ |f^{(n)}(x)| \le C n! \frac{1}{x^{n+1}}, $$ for some constant $C > 0$ (independent of $n$ ) and all $x > 0$ . My question is: what happens if we consider $$ g(x) = \frac{x}{(x+1)(x+2) - 2e^{-3x}}? $$ The function $g$ is not rational, hence we do not have partial fractions decomposition. However, numerical analysis suggest that it is possible to obtain similar estimate of the $n$ -th derivative of $g$ as for $f$ . Does there exist $D > 0$ such that for all non-negative integers $n$ and all $x > 0$ we have $$ |g^{(n)}(x)| \le D n! \frac{1}{x^{n+1}}? $$ Edit. I tried the following approach: since $f$ , as a complex function defined on the right-half plane ( $0$ is a removable singularity), is holomorphic, by the Cauchy formula it follows that for every $x > 0$ we have $$ f^{(n)}(x) = \frac{n!}{2\pi i} \int_\gamma \frac{f(z)}{(z-x)^{n+1}} dz, $$ where $\gamma$ is any closed path around $x$ . Hence, if we let $\gamma$ to be a circle with center at $x$ and radius $x$ , then $$ |f^{(n)}(x)| \le \frac{n!}{2\pi} \frac{1}{x^{n+1}} \int_\gamma |f(z)| dz. $$ It looks promising, since $f(z)$ is bounded (say, by $M > 0$ ) for $\mathrm{Re}(z) \ge 0$ . However, the integral is taken over the circle with length $2\pi x$ , which implies $$ |f^{(n)}(x)| \le M \frac{n!}{x^n}, \qquad x > 0. $$ This is nice, but I would like to get the estimate of the form $$ M \frac{n!}{x^{n+1}}. $$ Is it somehow possible to refine this mathod to get $\frac{1}{x^{n+1}}$ instead of $\frac{1}{x^n}$ ?","For let I would like to estimate the -th derivative of . To this end I use partial fractions decomposition and easily obtain for some constant (independent of ) and all . My question is: what happens if we consider The function is not rational, hence we do not have partial fractions decomposition. However, numerical analysis suggest that it is possible to obtain similar estimate of the -th derivative of as for . Does there exist such that for all non-negative integers and all we have Edit. I tried the following approach: since , as a complex function defined on the right-half plane ( is a removable singularity), is holomorphic, by the Cauchy formula it follows that for every we have where is any closed path around . Hence, if we let to be a circle with center at and radius , then It looks promising, since is bounded (say, by ) for . However, the integral is taken over the circle with length , which implies This is nice, but I would like to get the estimate of the form Is it somehow possible to refine this mathod to get instead of ?","x > 0 
f(x) = \frac{x}{(x+1)(x+2)}.
 n f 
f(x) = \frac{2}{x+2} - \frac{1}{x+1}
 
|f^{(n)}(x)| \le C n! \frac{1}{x^{n+1}},
 C > 0 n x > 0 
g(x) = \frac{x}{(x+1)(x+2) - 2e^{-3x}}?
 g n g f D > 0 n x > 0 
|g^{(n)}(x)| \le D n! \frac{1}{x^{n+1}}?
 f 0 x > 0 
f^{(n)}(x) = \frac{n!}{2\pi i} \int_\gamma \frac{f(z)}{(z-x)^{n+1}} dz,
 \gamma x \gamma x x 
|f^{(n)}(x)| \le \frac{n!}{2\pi} \frac{1}{x^{n+1}} \int_\gamma |f(z)| dz.
 f(z) M > 0 \mathrm{Re}(z) \ge 0 2\pi x 
|f^{(n)}(x)| \le M \frac{n!}{x^n}, \qquad x > 0.
 
M \frac{n!}{x^{n+1}}.
 \frac{1}{x^{n+1}} \frac{1}{x^n}","['real-analysis', 'calculus', 'complex-analysis', 'derivatives']"
47,Fresnel Integral Proof (without rigorous complex analysis),Fresnel Integral Proof (without rigorous complex analysis),,"I’d like to present my dodgy proof of the Fresnel Integrals, which I wrote before I knew anything about complex analysis. It takes some… liberties; yet, it still managed to produce the right value for both integrals, so I thought it might be worth sharing. Perhaps, with some input, I could change it into something more rigorous? I also wanted to leave a fun teaser at the end. The Proof So, the goal is to evaluate both $\int_0^\infty \cos{\left(x^2\right)}dx$ and $\int_0^\infty \sin{\left(x^2\right)}dx$ , the Fresnel Integrals. Start by using Euler’s identity: $$e^{ix} = \cos{x} + i\sin{x}$$ Substituting $x$ for $-x^2$ , this leaves: $$e^{-ix^2} = \cos{\left(x^2\right)} - i\sin{\left(x^2\right)}$$ $$\implies \int_0^\infty e^{-ix^2}dx = \int_0^\infty\cos{\left(x^2\right)}\,dx - i\int_0^\infty\sin{\left(x^2\right)}\,dx$$ Notice how the left-hand side looks almost exactly like the Gaussian integral? Using that observation, I used the substitution $ix^2 = u^2 \implies u =\pm\sqrt{i} \cdot x$ . Since this integral is taken within the interval $[0, \infty)$ , we can take only the positive branch from the square root. When $x = 0, u = 0$ ; as $x\to\infty, u\to\infty$ . $dx = \frac{du}{\sqrt{i}} = -i\sqrt{i} \cdot du$ . $$I = -i\sqrt{i}\int_0^\infty e^{-u^2}du$$ $\sqrt{i} = \frac1{\sqrt{2}} + i\frac1{\sqrt{2}}$ (taking only the positive branch), $\implies -i\sqrt{i} = \frac1{\sqrt{2}} - i\frac1{\sqrt{2}}$ . The new integral $\int_0^\infty e^{-u^2}du = \frac{\sqrt{\pi}}{2}$ . All of this leaves: $$I = \left(\frac1{\sqrt{2}} - i\frac1{\sqrt{2}}\right)\frac{\sqrt{\pi}}{2}$$ $$ = \sqrt{\frac{\pi}8} - i\sqrt{\frac{\pi}8} $$ With this, all that’s left to do is compare the real and imaginary parts of $I$ : $$\Re{\left(I\right)} = \sqrt{\frac{\pi}{8}} = \int_0^\infty\cos{\left(x^2\right)}dx$$ $$\Im{\left(I\right)} = -\sqrt{\frac{\pi}{8}} = -\int_0^\infty\sin{\left(x^2\right)}dx$$ $$\therefore  \int_0^\infty\cos{\left(x^2\right)}dx = \int_0^\infty\sin{\left(x^2\right)} = \sqrt{\frac{\pi}{8}}$$ The Problem The most blaring issue here is my assumption that $\lim_\limits{x\to\infty} \sqrt{i} \cdot x = \infty$ . This is nonsense when you look at it this idea of the limit on the complex plane. The complex function $f(x) = \sqrt{i} \cdot x$ can be expressed as $f(x) = \frac{x}{\sqrt2} + i\frac{x}{\sqrt2}$ . To help visualize this, I plotted this on a graphing calculator, which gives the following: As you can see, as this x approaches infinity, this function does approach an infinity; the function approaches the corner of the real-imaginary plane $\infty + i\infty$ . However, this is not the same as $\infty$ , which represents approaching the edge of the horizontal real number line. To put this more rigorously, this picture shows that $\lim_\limits{x\to\infty}\sqrt{i}\cdot x \neq \lim_\limits{x\to\infty}x$ . But why did this proof still work? It clearly returned the right value for both integrals. Is this step really as nonsensical as it seems? And if not, what kind of implications could that carry? The Teaser — The Icing on the Cake For positive n, $$\int_0^\infty e^{-ax^n}dx = a^{-\frac1{n}}\Gamma{\left(1+\frac1{n}\right)}$$ You could derive this yourself with the substitution $u = ax^n$ . But all this thinking about limits to complex infinities has me thinking: why not plug in $a = i$ ? Is that valid? This would yield: $$\int_0^\infty e^{-ix^n}dx = i^{-\frac1{n}}\Gamma{\left(1+\frac1{n}\right)}$$ Using the same logic as before, this would not make sense, since as $x\to\infty$ in the substitution above, $u\to -i\infty$ , which is not the same as $u$ approaching normal infinity. But it managed to work before—could it work now? With $n = 2$ —our original case—it does seem to return the same value as before.","I’d like to present my dodgy proof of the Fresnel Integrals, which I wrote before I knew anything about complex analysis. It takes some… liberties; yet, it still managed to produce the right value for both integrals, so I thought it might be worth sharing. Perhaps, with some input, I could change it into something more rigorous? I also wanted to leave a fun teaser at the end. The Proof So, the goal is to evaluate both and , the Fresnel Integrals. Start by using Euler’s identity: Substituting for , this leaves: Notice how the left-hand side looks almost exactly like the Gaussian integral? Using that observation, I used the substitution . Since this integral is taken within the interval , we can take only the positive branch from the square root. When ; as . . (taking only the positive branch), . The new integral . All of this leaves: With this, all that’s left to do is compare the real and imaginary parts of : The Problem The most blaring issue here is my assumption that . This is nonsense when you look at it this idea of the limit on the complex plane. The complex function can be expressed as . To help visualize this, I plotted this on a graphing calculator, which gives the following: As you can see, as this x approaches infinity, this function does approach an infinity; the function approaches the corner of the real-imaginary plane . However, this is not the same as , which represents approaching the edge of the horizontal real number line. To put this more rigorously, this picture shows that . But why did this proof still work? It clearly returned the right value for both integrals. Is this step really as nonsensical as it seems? And if not, what kind of implications could that carry? The Teaser — The Icing on the Cake For positive n, You could derive this yourself with the substitution . But all this thinking about limits to complex infinities has me thinking: why not plug in ? Is that valid? This would yield: Using the same logic as before, this would not make sense, since as in the substitution above, , which is not the same as approaching normal infinity. But it managed to work before—could it work now? With —our original case—it does seem to return the same value as before.","\int_0^\infty \cos{\left(x^2\right)}dx \int_0^\infty \sin{\left(x^2\right)}dx e^{ix} = \cos{x} + i\sin{x} x -x^2 e^{-ix^2} = \cos{\left(x^2\right)} - i\sin{\left(x^2\right)} \implies \int_0^\infty e^{-ix^2}dx = \int_0^\infty\cos{\left(x^2\right)}\,dx - i\int_0^\infty\sin{\left(x^2\right)}\,dx ix^2 = u^2 \implies u =\pm\sqrt{i} \cdot x [0, \infty) x = 0, u = 0 x\to\infty, u\to\infty dx = \frac{du}{\sqrt{i}} = -i\sqrt{i} \cdot du I = -i\sqrt{i}\int_0^\infty e^{-u^2}du \sqrt{i} = \frac1{\sqrt{2}} + i\frac1{\sqrt{2}} \implies -i\sqrt{i} = \frac1{\sqrt{2}} - i\frac1{\sqrt{2}} \int_0^\infty e^{-u^2}du = \frac{\sqrt{\pi}}{2} I = \left(\frac1{\sqrt{2}} - i\frac1{\sqrt{2}}\right)\frac{\sqrt{\pi}}{2}  = \sqrt{\frac{\pi}8} - i\sqrt{\frac{\pi}8}  I \Re{\left(I\right)} = \sqrt{\frac{\pi}{8}} = \int_0^\infty\cos{\left(x^2\right)}dx \Im{\left(I\right)} = -\sqrt{\frac{\pi}{8}} = -\int_0^\infty\sin{\left(x^2\right)}dx \therefore  \int_0^\infty\cos{\left(x^2\right)}dx = \int_0^\infty\sin{\left(x^2\right)} = \sqrt{\frac{\pi}{8}} \lim_\limits{x\to\infty} \sqrt{i} \cdot x = \infty f(x) = \sqrt{i} \cdot x f(x) = \frac{x}{\sqrt2} + i\frac{x}{\sqrt2} \infty + i\infty \infty \lim_\limits{x\to\infty}\sqrt{i}\cdot x \neq \lim_\limits{x\to\infty}x \int_0^\infty e^{-ax^n}dx = a^{-\frac1{n}}\Gamma{\left(1+\frac1{n}\right)} u = ax^n a = i \int_0^\infty e^{-ix^n}dx = i^{-\frac1{n}}\Gamma{\left(1+\frac1{n}\right)} x\to\infty u\to -i\infty u n = 2","['complex-analysis', 'fresnel-integrals']"
48,Poincaré's Theorem using Schwarz Lemma in $\mathbb C^n$,Poincaré's Theorem using Schwarz Lemma in,\mathbb C^n,"I have been asked to prove Poincaré's theorem using Schwarz Lemma. The statements are as follows: Schwarz Lemma. Let $B^n_1(\mathbf{0}) := \{(z_1, \ldots, z_n)\in \Bbb C^n:  |z_1|^2 + \ldots + |z_n|^2 < 1\}$ and $f: B^n_1(\mathbf{0}) \to \Bbb C$ be holomorphic with $f(\mathbf{0}) = 0$ . Let $|f(z)| \le M$ for all $z\in B^n_1(\mathbf{0})$ for some $M > 0$ , i.e. $f$ is bounded. Then, $|f(z)| \le M\|z\|$ for all $z\in B^n_1(\mathbf{0})$ and $\|f'(\mathbf{0})\|\le M$ . With $B^n_1(\mathbf{0})$ (the unit ball in $\Bbb C^n$ ) as above, let $P^n_1(\mathbf{0}):= \{(z_1, \ldots, z_n)\in \Bbb C^n: |z_i| < 1 \text{ for all }1\le i\le n\}$ the unit polydisc in $\Bbb C^n$ . Poincaré's Theorem. If $n \ge 2$ , there does not exist a biholomorphism between $B^n_1(\mathbf{0})$ and $P^n_1(\mathbf{0})$ . My work. Suppose, for a contradiction, that $n\ge 2$ and there exists a biholomorphism $f: B^n_1(\mathbf{0}) \to P^n_1(\mathbf{0})$ , that is, $f,f^{-1}$ are holomorphic and bijective. Let $f = (f_1, f_2, \ldots, f_n)$ be the coordinate-wise representation of $f$ . Also, denote the derivative map of $f$ at $\mathbf{0}$ by $f'(\mathbf{0}): \Bbb C^n\to \Bbb C^n$ . Without loss of generality, we can assume $f(\mathbf{0}) = \mathbf{0}$ . If $f(\mathbf{0}) \ne \mathbf{0}$ , then we can consider $\widetilde{f} = g \circ f: B^n_1(\mathbf{0}) \to P^n_1(\mathbf{0})$ where $g$ is an automorphism of the polydisc, sending $f(\mathbf{0})$ to $\mathbf{0}$ . Using Schwarz Lemma as stated above, I have proved that if $f(\mathbf{0}) = \mathbf{0}$ then $f'(\mathbf{0})$ maps $B^n_1(\mathbf{0})$ into $P^n_1(\mathbf{0})$ . If we can show (how?) that the image of $B^n_1(\mathbf{0})$ under $f'(\mathbf{0})$ is $P^n_1(\mathbf{0})$ , then, $f'(\mathbf{0})$ maps $\partial B^n_1(\mathbf{0})$ into $\partial P^n_1(\mathbf{0})$ . Also, $f'(\mathbf{0})$ is invertible (use chain rule). Knowing that linear maps send spheres to ellipsoids, $f'(\mathbf{0})(\partial B^n_1(\mathbf{0})) \subset \partial P^n_1(\mathbf{0})$ is a contradiction. Question. Could I get some help with showing that $f'(\mathbf{0})$ maps $B^n_1(\mathbf{0})$ onto $P^n_1(\mathbf{0})$ ? If this is not true, how else can I prove Poincaré's Theorem using Schwarz Lemma? Thanks!","I have been asked to prove Poincaré's theorem using Schwarz Lemma. The statements are as follows: Schwarz Lemma. Let and be holomorphic with . Let for all for some , i.e. is bounded. Then, for all and . With (the unit ball in ) as above, let the unit polydisc in . Poincaré's Theorem. If , there does not exist a biholomorphism between and . My work. Suppose, for a contradiction, that and there exists a biholomorphism , that is, are holomorphic and bijective. Let be the coordinate-wise representation of . Also, denote the derivative map of at by . Without loss of generality, we can assume . If , then we can consider where is an automorphism of the polydisc, sending to . Using Schwarz Lemma as stated above, I have proved that if then maps into . If we can show (how?) that the image of under is , then, maps into . Also, is invertible (use chain rule). Knowing that linear maps send spheres to ellipsoids, is a contradiction. Question. Could I get some help with showing that maps onto ? If this is not true, how else can I prove Poincaré's Theorem using Schwarz Lemma? Thanks!","B^n_1(\mathbf{0}) := \{(z_1, \ldots, z_n)\in \Bbb C^n: 
|z_1|^2 + \ldots + |z_n|^2 < 1\} f: B^n_1(\mathbf{0}) \to \Bbb C f(\mathbf{0}) = 0 |f(z)| \le M z\in B^n_1(\mathbf{0}) M > 0 f |f(z)| \le M\|z\| z\in B^n_1(\mathbf{0}) \|f'(\mathbf{0})\|\le M B^n_1(\mathbf{0}) \Bbb C^n P^n_1(\mathbf{0}):= \{(z_1, \ldots, z_n)\in \Bbb C^n: |z_i| < 1 \text{ for all }1\le i\le n\} \Bbb C^n n \ge 2 B^n_1(\mathbf{0}) P^n_1(\mathbf{0}) n\ge 2 f: B^n_1(\mathbf{0}) \to P^n_1(\mathbf{0}) f,f^{-1} f = (f_1, f_2, \ldots, f_n) f f \mathbf{0} f'(\mathbf{0}): \Bbb C^n\to \Bbb C^n f(\mathbf{0}) = \mathbf{0} f(\mathbf{0}) \ne \mathbf{0} \widetilde{f} = g \circ f: B^n_1(\mathbf{0}) \to P^n_1(\mathbf{0}) g f(\mathbf{0}) \mathbf{0} f(\mathbf{0}) = \mathbf{0} f'(\mathbf{0}) B^n_1(\mathbf{0}) P^n_1(\mathbf{0}) B^n_1(\mathbf{0}) f'(\mathbf{0}) P^n_1(\mathbf{0}) f'(\mathbf{0}) \partial B^n_1(\mathbf{0}) \partial P^n_1(\mathbf{0}) f'(\mathbf{0}) f'(\mathbf{0})(\partial B^n_1(\mathbf{0})) \subset \partial P^n_1(\mathbf{0}) f'(\mathbf{0}) B^n_1(\mathbf{0}) P^n_1(\mathbf{0})","['complex-analysis', 'analysis', 'alternative-proof', 'several-complex-variables']"
49,An algebraic approach to the analyticity of holomorphic functions,An algebraic approach to the analyticity of holomorphic functions,,"Famously, complex analysis is much ""nicer"" (more rigid) than real analysis. The reason for this is that, as I like to put it, complex analysis is just one-dimensional algebraic geometry. This is formalized via GAGA, but the fundamental fact that makes this work is that holomorphic functions are always analytic. As a result, we have a fundamental sequence of inclusions $\mathbb{C}[x_1,\dotsc,x_n]_{(x_1,\dotsc,x_n)}\subset\mathbb{C}\{x_1,\dotsc,x_n\}\subset\mathbb{C}[[x_1,\dotsc,x_n]]$ which allows us to work with holomorphic functions algebraically. We find that holomorphic germs form a regular local ring of dimension $n$ , admit a division algorithm (Weierstrass preparation theorem), and so on. As for why holomorphic functions are analytic, however, the standard proof involves expanding the inside of a path integral. This isn't exactly surprising—after all, $\mathbb{C}$ is an analytic object, so the proof ought to use analysis. In terms of the intuition behind this, though, the best I've been able to find is ""holomorphic functions satisfy the Cauchy-Riemann equations"". My problem with this is that it's based on differential equations over $\mathbb{R}$ , rather than using the basic properties of $\mathbb{C}$ itself (minimal algebraically closed complete Archimedean field). In general, suppose we have a topological field $K$ , or a field with absolute value if you like. We can define a perfectly good theory of differentiability for functions on open subsets of affine space $K^n$ , and we get a theory of smooth functions, analytic functions, and manifolds. From a purely algebreo-geometric point of view, we can always work with varieties over $K$ , and we can do calculus on them algebraically. Moreover, if I'm not mistaken, we should be able to naturally make the set of $K$ -points of any smooth variety into a $K$ -manifold. This is typically done over $\mathbb{R}$ , $\mathbb{C}$ , the $p$ -adic numbers (and their algebraic extensions), and more generally fields with a complete absolute value. So my question is as follows: what conditions on $K$ will make it so that the smooth theory of $K$ -manifolds will be describable, at least approximately, via the algebraic one? What will cause the smooth functions to be analytic? And, in particular, what properties of $\mathbb{C}$ allow its theory of smooth functions to be described algebraically from this perspective?","Famously, complex analysis is much ""nicer"" (more rigid) than real analysis. The reason for this is that, as I like to put it, complex analysis is just one-dimensional algebraic geometry. This is formalized via GAGA, but the fundamental fact that makes this work is that holomorphic functions are always analytic. As a result, we have a fundamental sequence of inclusions which allows us to work with holomorphic functions algebraically. We find that holomorphic germs form a regular local ring of dimension , admit a division algorithm (Weierstrass preparation theorem), and so on. As for why holomorphic functions are analytic, however, the standard proof involves expanding the inside of a path integral. This isn't exactly surprising—after all, is an analytic object, so the proof ought to use analysis. In terms of the intuition behind this, though, the best I've been able to find is ""holomorphic functions satisfy the Cauchy-Riemann equations"". My problem with this is that it's based on differential equations over , rather than using the basic properties of itself (minimal algebraically closed complete Archimedean field). In general, suppose we have a topological field , or a field with absolute value if you like. We can define a perfectly good theory of differentiability for functions on open subsets of affine space , and we get a theory of smooth functions, analytic functions, and manifolds. From a purely algebreo-geometric point of view, we can always work with varieties over , and we can do calculus on them algebraically. Moreover, if I'm not mistaken, we should be able to naturally make the set of -points of any smooth variety into a -manifold. This is typically done over , , the -adic numbers (and their algebraic extensions), and more generally fields with a complete absolute value. So my question is as follows: what conditions on will make it so that the smooth theory of -manifolds will be describable, at least approximately, via the algebraic one? What will cause the smooth functions to be analytic? And, in particular, what properties of allow its theory of smooth functions to be described algebraically from this perspective?","\mathbb{C}[x_1,\dotsc,x_n]_{(x_1,\dotsc,x_n)}\subset\mathbb{C}\{x_1,\dotsc,x_n\}\subset\mathbb{C}[[x_1,\dotsc,x_n]] n \mathbb{C} \mathbb{R} \mathbb{C} K K^n K K K \mathbb{R} \mathbb{C} p K K \mathbb{C}","['complex-analysis', 'algebraic-geometry', 'commutative-algebra', 'complex-geometry']"
50,What is the geometric significance of the definition of the derivative for complex-valued functions?,What is the geometric significance of the definition of the derivative for complex-valued functions?,,"Roughly speaking, a function $f:\mathbf R^2\to\mathbf R^2$ is differentiable at $\mathbf v$ if there is a linear transformation $Df(\mathbf v)$ such that $f(\mathbf v+\mathbf h)\approx f(\mathbf v)+Df(\mathbf v)\mathbf h$ . If, in addition, $f$ is differentiable as a map from $\mathbf C$ to $\mathbf C$ , then we require that this linear transformation is complex multiplication, which rotates and scales the vector $\mathbf h$ . This means that if you consider a tiny disk $D$ centred at $\mathbf v$ , then (in the limit) the image of $D$ under $f$ will also be a disk. My question is: from a geometric perspective, why does the fact that holomorphic functions map disks to disks give them such nice properties—e.g. a function which is differentiable once is infinitely differentiable.","Roughly speaking, a function is differentiable at if there is a linear transformation such that . If, in addition, is differentiable as a map from to , then we require that this linear transformation is complex multiplication, which rotates and scales the vector . This means that if you consider a tiny disk centred at , then (in the limit) the image of under will also be a disk. My question is: from a geometric perspective, why does the fact that holomorphic functions map disks to disks give them such nice properties—e.g. a function which is differentiable once is infinitely differentiable.",f:\mathbf R^2\to\mathbf R^2 \mathbf v Df(\mathbf v) f(\mathbf v+\mathbf h)\approx f(\mathbf v)+Df(\mathbf v)\mathbf h f \mathbf C \mathbf C \mathbf h D \mathbf v D f,"['calculus', 'complex-analysis', 'derivatives', 'analytic-functions']"
51,"upper bound on $L_2$-norm of a power series, in terms of coefficients?","upper bound on -norm of a power series, in terms of coefficients?",L_2,"Is there any upper bound on $L_2$ -norm of a convergent power series (in R), in terms of coefficients? I have $f(x) = a_0+a_1\frac{x}{1!}+a_2\frac{x^2}{2!}+a_3\frac{x^3}{3!}+...$ . I need something like: $$ \|f\|^2_{L_2({\mathbb R})}\leq r(a_0, a_1, a_2,\cdots) $$ where $r(a_0, a_1, a_2,\cdots) = \sum_{ij}c_{ij}a^\ast_i a_j$ is some infinite positive definite quadratic form.","Is there any upper bound on -norm of a convergent power series (in R), in terms of coefficients? I have . I need something like: where is some infinite positive definite quadratic form.","L_2 f(x) = a_0+a_1\frac{x}{1!}+a_2\frac{x^2}{2!}+a_3\frac{x^3}{3!}+... 
\|f\|^2_{L_2({\mathbb R})}\leq r(a_0, a_1, a_2,\cdots)
 r(a_0, a_1, a_2,\cdots) = \sum_{ij}c_{ij}a^\ast_i a_j","['complex-analysis', 'power-series', 'taylor-expansion', 'entire-functions']"
52,"When is a g-dimensional subspace of $H^1(S; \mathbb{C})$ the $H^{1,0}$ of a complex structure on $S$?",When is a g-dimensional subspace of  the  of a complex structure on ?,"H^1(S; \mathbb{C}) H^{1,0} S","Given a closed surface $S$ of genus $g \geq 1$ there are lots of choices of complex structure, and each one singles out a subspace of the surface's first cohomology group with complex coefficients corresponding to the classes of 1-forms which are holomorphic on that Riemann surface.  Given an ""appropriate"" subspace, when does it correspond to the $H^{1,0}$ for some choice of complex structure? To be more precise, let $\mathcal{T}(S)$ be the Teichmüller space of $S$ , whose elements are pairs equivalence classes of pairs $(X, f)$ where $X$ is a Riemann surface and $f : S \to X$ is a diffeomorphism, with $(X, f) \sim (Y, g)$ if there is a holomorphic $\Phi : X \to Y$ such that $g^{-1}\Phi f$ is isotopic to $\mathrm{id}_S$ .  Using the marking $f$ , we can pull $H^{1,0}(X)$ back to a complex $g$ -dimensional subspace of $H^1(S; \mathbb{C})$ .  It is not the case that every $g$ -dimensional $V \subseteq H^1(S; \mathbb{C})$ corresponds to some $H^{1,0}(X)$ for some choice of complex structure on $S$ .  For example, the theorems of Haupt and Kapovich (independently, see https://www.math.ucdavis.edu/~kapovich/EPR/fla2019.pdf ) state that a character $\chi : H_1(S; \mathbb{Z}) \to \mathbb{C}$ will correspond to a holomorphic 1-form on some choice of complex structure if two conditions are satisfied. If I had a $g$ -dimensional $V \subseteq H^1(S; \mathbb{C})$ where each element satisfied the Haupt-Kapovich conditions, is it true this subspace corresponds to some $H^{1,0}(X)$ ?  The issue that comes to mind is if $\chi, \kappa \in V$ they correspond to holomorphic 1-forms on Riemann surfaces, but is there a reason they should be holomorphic 1-forms on the same Riemann surface?","Given a closed surface of genus there are lots of choices of complex structure, and each one singles out a subspace of the surface's first cohomology group with complex coefficients corresponding to the classes of 1-forms which are holomorphic on that Riemann surface.  Given an ""appropriate"" subspace, when does it correspond to the for some choice of complex structure? To be more precise, let be the Teichmüller space of , whose elements are pairs equivalence classes of pairs where is a Riemann surface and is a diffeomorphism, with if there is a holomorphic such that is isotopic to .  Using the marking , we can pull back to a complex -dimensional subspace of .  It is not the case that every -dimensional corresponds to some for some choice of complex structure on .  For example, the theorems of Haupt and Kapovich (independently, see https://www.math.ucdavis.edu/~kapovich/EPR/fla2019.pdf ) state that a character will correspond to a holomorphic 1-form on some choice of complex structure if two conditions are satisfied. If I had a -dimensional where each element satisfied the Haupt-Kapovich conditions, is it true this subspace corresponds to some ?  The issue that comes to mind is if they correspond to holomorphic 1-forms on Riemann surfaces, but is there a reason they should be holomorphic 1-forms on the same Riemann surface?","S g \geq 1 H^{1,0} \mathcal{T}(S) S (X, f) X f : S \to X (X, f) \sim (Y, g) \Phi : X \to Y g^{-1}\Phi f \mathrm{id}_S f H^{1,0}(X) g H^1(S; \mathbb{C}) g V \subseteq H^1(S; \mathbb{C}) H^{1,0}(X) S \chi : H_1(S; \mathbb{Z}) \to \mathbb{C} g V \subseteq H^1(S; \mathbb{C}) H^{1,0}(X) \chi, \kappa \in V","['complex-analysis', 'complex-geometry', 'riemann-surfaces', 'teichmueller-theory']"
53,Decomposing the Fubini-Study metric - Irreducibility of $\mathbb{CP}^n$,Decomposing the Fubini-Study metric - Irreducibility of,\mathbb{CP}^n,"I was wondering whether $(\mathbb{CP}^n, g_{FS})$ , where $g_{FS}$ denotes the Fubini-Study metric, is holonomy irreducible, that is, whether or not the tangent space can be split up into subspaces invariant under the holonomy action. I computed that the holonomy group is $U(n)$ . My line of thinking was the following: If $(\mathbb{CP}^n, g_{FS})$ would not be irreducible, then the de Rham Theorem would imply (because $(\mathbb{CP}^n, g_{FS})$ is simply connected and complete) that $(\mathbb{CP}^n, g_{FS})$ is globally a product, which in turn implies that its holonomy group is a product. So there are two things that might go wrong: Either $(\mathbb{CP}^n, g_{FS})$ can not be a product manifold, that is, either $\mathbb{CP}^n$ can not even topologically be or the metric $g_{FS}$ can not split; or $U(n)$ can not be split into a product. But $U(n)$ is reducible, so the latter might be possible. What I am left with is the first option and I have currently no idea how to show that that's not possible. So my question is: Why can the Fubini-Study metric $g_{FS}$ not be realized as a product metric, $h_1 + h_2 = g$ ? Or: Why can $\mathbb{C}\mathbb{P}^n$ not be written as a product?","I was wondering whether , where denotes the Fubini-Study metric, is holonomy irreducible, that is, whether or not the tangent space can be split up into subspaces invariant under the holonomy action. I computed that the holonomy group is . My line of thinking was the following: If would not be irreducible, then the de Rham Theorem would imply (because is simply connected and complete) that is globally a product, which in turn implies that its holonomy group is a product. So there are two things that might go wrong: Either can not be a product manifold, that is, either can not even topologically be or the metric can not split; or can not be split into a product. But is reducible, so the latter might be possible. What I am left with is the first option and I have currently no idea how to show that that's not possible. So my question is: Why can the Fubini-Study metric not be realized as a product metric, ? Or: Why can not be written as a product?","(\mathbb{CP}^n, g_{FS}) g_{FS} U(n) (\mathbb{CP}^n, g_{FS}) (\mathbb{CP}^n, g_{FS}) (\mathbb{CP}^n, g_{FS}) (\mathbb{CP}^n, g_{FS}) \mathbb{CP}^n g_{FS} U(n) U(n) g_{FS} h_1 + h_2 = g \mathbb{C}\mathbb{P}^n","['complex-analysis', 'riemannian-geometry', 'projective-space', 'holonomy']"
54,A uniform bound on a family of complex integrals,A uniform bound on a family of complex integrals,,"In my research, I have encountered the following integrals, which I want to bound. For $0 < t < 1$ , let $C_t$ denote the portion of the complex unit circle consisting of $z$ for which $\Re (z) \ge t$ and define $$ I_t (n,k) := \int_{C_t} \left(  \frac{1-tz}{|1-tz|} \right)^{1+i+n} \frac{1}{(1-tz)^{1+i}} z^k  dz =  \int_{C_t} \left(  \frac{1-tz}{|1-tz|} \right)^{n} \cdot z^k \cdot \frac{1}{|1-tz|^{1+i}} dz. $$ I want to see that there exists $C>0$ such that $|I_t(n,k)| \leq C$ for all $1/2 < t < 1$ and $(n,k) \in \mathbb{Z}_{\ge 1} \times \mathbb{Z}_{\ge 1}$ . Any ideas will be appreciated.","In my research, I have encountered the following integrals, which I want to bound. For , let denote the portion of the complex unit circle consisting of for which and define I want to see that there exists such that for all and . Any ideas will be appreciated.","0 < t < 1 C_t z \Re (z) \ge t 
I_t (n,k) := \int_{C_t} \left( 
\frac{1-tz}{|1-tz|} \right)^{1+i+n} \frac{1}{(1-tz)^{1+i}} z^k  dz =  \int_{C_t} \left( 
\frac{1-tz}{|1-tz|} \right)^{n} \cdot z^k \cdot \frac{1}{|1-tz|^{1+i}} dz.
 C>0 |I_t(n,k)| \leq C 1/2 < t < 1 (n,k) \in \mathbb{Z}_{\ge 1} \times \mathbb{Z}_{\ge 1}","['complex-analysis', 'fourier-series', 'contour-integration', 'complex-integration']"
55,Sufficient condition for $|1-P(z)|>\left|\frac{(z-a)(1-az)}{b}\right|$ with $|z|=1$,Sufficient condition for  with,|1-P(z)|>\left|\frac{(z-a)(1-az)}{b}\right| |z|=1,"Question Let $P:\mathbb C \to \mathbb C$ be a polynomial with real coefficients, $a\in(0,1)$ and $b\in\mathbb R_+$ . I want to find a   sufficient condition for $$|1-P(z)|>\left|\frac{(z-a)(1-az)}{b}\right|, \quad \forall \;z\in\mathbb C \quad\text{such that}\; |z|=1,$$ that is not too restrictive. I understand ""not too restrictive"" is somewhat subjective, but any improvement on the condition I obtained in my attempt below would be helpful. My Attempt On one hand, it can be shown that $$|z|=1\;\Rightarrow\;\left|\frac{(z-a)(1-az)}{b}\right|\le \frac{(1+a)^2}{b}.$$ On the other hand, by the reverse triangle inequality $$|z|=1\;\Rightarrow\;|1-P(z)|\ge|1-|P(z)||,$$ and since for $|z|=1$ we have $|P(z)|\le||P||_2$ , then if $||P||_2<1$ we have that $$|z|=1\;\Rightarrow\;|1-P(z)|\ge 1-||P||_2.$$ Therefore, a sufficient condition is that $$1-||P||_2>\frac{(1+a)^2}{b}$$ or $$||P||_2<1-\frac{(1+a)^2}{b}.$$ This condition appears to work, but it seems to me to be unnecessarily restrictive. By looking at the two sides of the inequality separately I have not made use of the fact that $z$ is the same on both sides. I suspect, there may be a way to explore the relationship between the sides.","Question Let be a polynomial with real coefficients, and . I want to find a   sufficient condition for that is not too restrictive. I understand ""not too restrictive"" is somewhat subjective, but any improvement on the condition I obtained in my attempt below would be helpful. My Attempt On one hand, it can be shown that On the other hand, by the reverse triangle inequality and since for we have , then if we have that Therefore, a sufficient condition is that or This condition appears to work, but it seems to me to be unnecessarily restrictive. By looking at the two sides of the inequality separately I have not made use of the fact that is the same on both sides. I suspect, there may be a way to explore the relationship between the sides.","P:\mathbb C \to \mathbb C a\in(0,1) b\in\mathbb R_+ |1-P(z)|>\left|\frac{(z-a)(1-az)}{b}\right|, \quad \forall \;z\in\mathbb C \quad\text{such that}\; |z|=1, |z|=1\;\Rightarrow\;\left|\frac{(z-a)(1-az)}{b}\right|\le \frac{(1+a)^2}{b}. |z|=1\;\Rightarrow\;|1-P(z)|\ge|1-|P(z)||, |z|=1 |P(z)|\le||P||_2 ||P||_2<1 |z|=1\;\Rightarrow\;|1-P(z)|\ge 1-||P||_2. 1-||P||_2>\frac{(1+a)^2}{b} ||P||_2<1-\frac{(1+a)^2}{b}. z","['complex-analysis', 'inequality', 'polynomials']"
56,Finding a upper bound for a complex and algebraic function.,Finding a upper bound for a complex and algebraic function.,,"I am stuck in understanding a proof, where I don't really understand one step, which i specify below: (suposse that $f$ is holomorphic, thus analytic) (I have no information about the holomorphism domain of $f$ ). Result. If $\phi(x,y)$ is an algebraic function in both variables, i.e. there are polynomials $P_k(x,y)$ such that \begin{equation*} \sum_{k=0}^{N}P_k(x,y)[\phi(x,y)]^k = 0 \end{equation*} and also let $f$ be a holomorphic function such that \begin{equation*} |f(z)|^2 = \phi(x,y) \end{equation*} then $|f(z)|^2 \leq C|z|^m$ , for some $m$ . EDIT. Based on @MartinR comment, $f(z)=z+1$ would be a counterexample for the inequality above, which makes it invalid. Instead, he suggested (and very well) that: \begin{equation*} |f(z)|^2 \leq C(1+|z|^m), \hspace{.2cm} \text{ for some $m$} \end{equation*} which would also verify the demonstration I am talking about in the beginning. This is a step that appears in a proof, and I tried to write it formally as a proposition/theorem above. I would like to know where this comes from, and even if it's valid, since it's not making much sense to me. Mainly coming here to understand the result and the proof itself, not building it by myself. Thanks for all the help in advance.","I am stuck in understanding a proof, where I don't really understand one step, which i specify below: (suposse that is holomorphic, thus analytic) (I have no information about the holomorphism domain of ). Result. If is an algebraic function in both variables, i.e. there are polynomials such that and also let be a holomorphic function such that then , for some . EDIT. Based on @MartinR comment, would be a counterexample for the inequality above, which makes it invalid. Instead, he suggested (and very well) that: which would also verify the demonstration I am talking about in the beginning. This is a step that appears in a proof, and I tried to write it formally as a proposition/theorem above. I would like to know where this comes from, and even if it's valid, since it's not making much sense to me. Mainly coming here to understand the result and the proof itself, not building it by myself. Thanks for all the help in advance.","f f \phi(x,y) P_k(x,y) \begin{equation*}
\sum_{k=0}^{N}P_k(x,y)[\phi(x,y)]^k = 0
\end{equation*} f \begin{equation*}
|f(z)|^2 = \phi(x,y)
\end{equation*} |f(z)|^2 \leq C|z|^m m f(z)=z+1 \begin{equation*}
|f(z)|^2 \leq C(1+|z|^m), \hspace{.2cm} \text{ for some m}
\end{equation*}","['complex-analysis', 'upper-lower-bounds', 'analytic-functions', 'entire-functions']"
57,Evaluate $\lim_{z\to\infty} \frac{(1 - z^4)^{1/4}}{z}$.,Evaluate .,\lim_{z\to\infty} \frac{(1 - z^4)^{1/4}}{z},"I'm going through my (applied) complex analysis notes from almost a year ago and trying to make sense of something I wrote down. Of course, anybody's own solution is very much welcome. We used the ""+""-shaped branch cut, $\{z ~|~ z \in [-1,1] \subseteq \mathbb{R}\} \cup \{z ~|~ \text{Re}(z)=0,\, \text{Im}(z) \in [-1,1]\}$ , with the specified branch of $(1-z^4)^{1/4}$ corresponding to the first quadrant of the plane evaluating to the positive fourth root. Here is what I have written down: The possible values are $e^{i\pi/4}, -e^{i\pi/4}, ie^{i\pi/4}, -ie^{i\pi/4}$ . Again, relying on continuity of the branch, what is the best path to $\infty$ ? Choose $1 + \epsilon \to \infty$ along the positive real axis. Factoring the numerator, \begin{align*} 	(1 - z^4) &= (-1)(z - 1)(z + 1)(z - i)(z + i) \\ (1 - z^4)^{1/4} &= (-1)^{1/4}(z - 1)^{1/4}(z + 1)^{1/4}(z - i)^{1/4}(z + i)^{1/4}. \end{align*} Let $z = 1 + w$ with $w$ small, i.e. $w = \epsilon e^{i\theta}$ , then $$ f(1 + w) = (-1)^{1/4} (w^{1/4})(2 + w)^{1/4} (2 + 2w + w^2)^{1/4} $$ for $\theta \in (-\pi,\pi)$ . For $\theta = \pi$ , then $$f(z) \approx \sqrt{2} w^{1/4} e^{-i\pi/4} + (\text{smaller terms}).$$ The argument of $f(z)$ for $z = 1 + w$ for small $w$ is $-\pi i/4$ . The limit is therefore $e^{-i\pi/4} = -ie^{i\pi/4}$ . I very much remember not comprehending it at the time as well many other classmates, and also remember that the professor stated that they made an error near the end because he forgot the $\sqrt{2}$ , so I may have added that back in wrong. I apologize for any lack of clarity. My questions are If we're taking a path along the positive real axis as in $1 + \epsilon \to \infty$ , then why are we taking $\epsilon \to 0$ as in "" $w$ small"" in the computation? Why choose $\theta = \pi$ . My guess for the latter is so that the your values ""remain"" in the first quadrant because your dividing the argument of $e^{\pi i}$ by 4. And the major question is that I can't figure out why with $|w|$ small that $\arg f(z) = -\pi i/4$ . Why negative? I'm guessing it's due to how we evaluate $(-1)^{1/4} = (e^{-\pi i})^{1/4} = (e^{\pi i})^{1/4}$ but that confuses me if we're also taking $w = \epsilon e^{i\theta}$ and evaluating $\theta = \pi$ . (Edit:) Another question: Is the statement of the problem, well, problematic? Does this problem even make sense? Does clarifying the limit to mean as $|z| \to +\infty$ help at all?","I'm going through my (applied) complex analysis notes from almost a year ago and trying to make sense of something I wrote down. Of course, anybody's own solution is very much welcome. We used the ""+""-shaped branch cut, , with the specified branch of corresponding to the first quadrant of the plane evaluating to the positive fourth root. Here is what I have written down: The possible values are . Again, relying on continuity of the branch, what is the best path to ? Choose along the positive real axis. Factoring the numerator, Let with small, i.e. , then for . For , then The argument of for for small is . The limit is therefore . I very much remember not comprehending it at the time as well many other classmates, and also remember that the professor stated that they made an error near the end because he forgot the , so I may have added that back in wrong. I apologize for any lack of clarity. My questions are If we're taking a path along the positive real axis as in , then why are we taking as in "" small"" in the computation? Why choose . My guess for the latter is so that the your values ""remain"" in the first quadrant because your dividing the argument of by 4. And the major question is that I can't figure out why with small that . Why negative? I'm guessing it's due to how we evaluate but that confuses me if we're also taking and evaluating . (Edit:) Another question: Is the statement of the problem, well, problematic? Does this problem even make sense? Does clarifying the limit to mean as help at all?","\{z ~|~ z \in [-1,1] \subseteq \mathbb{R}\} \cup \{z ~|~ \text{Re}(z)=0,\, \text{Im}(z) \in [-1,1]\} (1-z^4)^{1/4} e^{i\pi/4}, -e^{i\pi/4}, ie^{i\pi/4}, -ie^{i\pi/4} \infty 1 + \epsilon \to \infty \begin{align*} 	(1 - z^4) &= (-1)(z - 1)(z + 1)(z - i)(z + i) \\ (1 - z^4)^{1/4} &= (-1)^{1/4}(z - 1)^{1/4}(z + 1)^{1/4}(z - i)^{1/4}(z + i)^{1/4}. \end{align*} z = 1 + w w w = \epsilon e^{i\theta}  f(1 + w) = (-1)^{1/4} (w^{1/4})(2 + w)^{1/4} (2 + 2w + w^2)^{1/4}  \theta \in (-\pi,\pi) \theta = \pi f(z) \approx \sqrt{2} w^{1/4} e^{-i\pi/4} + (\text{smaller terms}). f(z) z = 1 + w w -\pi i/4 e^{-i\pi/4} = -ie^{i\pi/4} \sqrt{2} 1 + \epsilon \to \infty \epsilon \to 0 w \theta = \pi e^{\pi i} |w| \arg f(z) = -\pi i/4 (-1)^{1/4} = (e^{-\pi i})^{1/4} = (e^{\pi i})^{1/4} w = \epsilon e^{i\theta} \theta = \pi |z| \to +\infty","['complex-analysis', 'limits', 'branch-cuts']"
58,Analytic continuation of the Kronecker Delta,Analytic continuation of the Kronecker Delta,,"The Kronecker Delta can be written as the integral of the complex function $$f(n,z)=\frac{1}{2\pi i} z^{n-1} \ ,$$ where $n\in \mathbb{Z}$ and $z\in\mathbb{C}$ on a closed path $\mathcal{C}$ enclosing the origin $$ \delta_{n,0}= \oint_\mathcal{C} f(n,z) dz \ .$$ This can be seen as a trivial consequence of the Cauchy integral theorem $$ \delta_{n,0}= \oint_\mathcal{C} dz \frac{1}{2\pi i} z^{n-1}  = \mathrm{Res}_{z=0}(z^{n-1}) \ .$$ The same function computed for $n=i\eta$ with $\eta\in\mathbb{R}$ and integrated on the real axis $$ \int_0^\infty dy \, f(i \eta,y) =\int_0^\infty dy \frac{1}{2\pi i} y^{i\eta-1}  = \int_{-\infty}^\infty dx \frac{1}{2\pi i} e^{i \eta x} = -i \delta(\eta) \ \ .$$ My question is the following. Is there a master function $$ G(n) \equiv \int_\gamma dz f(n,z) $$ defined as the integral on some path $\gamma$ such that $G(n)\propto \delta_{n,0}$ and $G(i \eta)\propto \delta(\eta)$ ? What are the path and the proportionality constants? I do not know if it is useful but I noticed that $z^{n-1}$ has a pole also at infinity.",The Kronecker Delta can be written as the integral of the complex function where and on a closed path enclosing the origin This can be seen as a trivial consequence of the Cauchy integral theorem The same function computed for with and integrated on the real axis My question is the following. Is there a master function defined as the integral on some path such that and ? What are the path and the proportionality constants? I do not know if it is useful but I noticed that has a pole also at infinity.,"f(n,z)=\frac{1}{2\pi i} z^{n-1} \ , n\in \mathbb{Z} z\in\mathbb{C} \mathcal{C}  \delta_{n,0}= \oint_\mathcal{C} f(n,z) dz \ .  \delta_{n,0}= \oint_\mathcal{C} dz \frac{1}{2\pi i} z^{n-1}  = \mathrm{Res}_{z=0}(z^{n-1}) \ . n=i\eta \eta\in\mathbb{R}  \int_0^\infty dy \, f(i \eta,y) =\int_0^\infty dy \frac{1}{2\pi i} y^{i\eta-1}  = \int_{-\infty}^\infty dx \frac{1}{2\pi i} e^{i \eta x} = -i \delta(\eta) \ \ .  G(n) \equiv \int_\gamma dz f(n,z)
 \gamma G(n)\propto \delta_{n,0} G(i \eta)\propto \delta(\eta) z^{n-1}","['complex-analysis', 'dirac-delta', 'analytic-continuation', 'kronecker-delta']"
59,Proof verification: Pringsheim interpolation formula,Proof verification: Pringsheim interpolation formula,,"This is an exercise in Complex Analysis by Stein, Exercise 5.17. Let $\{a_n\}_{k=0}^{\infty}$ be a sequence of distinct complex numbers such that $a_0 =0$ and $\lim_{k \to \infty} |a_k| = \infty$ , and let \begin{align*} E(z)=z\prod_{n=1}^{\infty}{E_n (\frac{z}{a_n})} \end{align*} , where $E_k$ is Weierstrass canonical factors defined by \begin{align*} E_k (z) = (1-z) e^{z+ \frac{z^2}{2} + \cdots + \frac{z^k}{k}} \end{align*} Given complex numbers $\{b_k\}_{k=0}^{\infty}$ , show that there exist integers $m_k \ge 1$ such that the series \begin{align*} F(z) = \frac{b_0}{E'(a_0 )}\frac{E(z)}{z} + \sum_{k=1}^{\infty} \frac{b_k}{E'(a_k)}\frac{E(z)}{z-a_k} \Big ( \frac{z}{a_k} \Big ) ^{m_k}  \end{align*} defines an entire function that satisfies $F(a_k )=b_k $ for all $k \ge 0$ . Here is my proof, which I want to get verified. I assumed that I already know that $E(z)$ is entire, and has simple zeros only at $\{a_k\}$ s. First, let's prove that $E'(a_k) \neq 0$ , so that fractions are well-defined. Since $E$ has simple zero at $a_k$ , there exists entire function $F_k$ where $E(z)=(z-a_k)F_k (z)$ and $F_k (a_k) \neq 0$ . Then, \begin{align*} E'(z)=F_k (z) + (z- a_k ) {F_k}'(z) \end{align*} , thus we have $E'(a_k) \neq 0$ , as desired. Now let $m_k$ be an integer that satisfies \begin{align*} \frac{|b_k|}{|E'(a_k)|} \frac{1}{3} 4^{1-m_k} < 2^{-k} \end{align*} To prove that $F$ is entire, we only need to prove that the infinite sum converges uniformly in the closed disc $|z| \le R$ . Let $M_R$ be the supremum of $E(z)$ in $|z| \le R$ . Let $N$ be a integer such that $n \ge N$ yields $|a_n| \ge \max\{RM_R , 4R\}$ . Then for $N' \ge N$ , we have \begin{align*} \Big | \sum_{k=N'}^{\infty} \frac{b_k}{E'(a_k)} \frac{E(z)}{z-a_k} \Big ( \frac{z}{a_k} \Big )^{m_k} \Big | &\le \sum_{k=N'}^{\infty} \frac{|b_k|}{|E'(a_k)|} \frac{|E(z)|}{|z-a_k|} \Big | \frac{z}{a_k} \Big |^{m_k} \\ &\le \sum_{k=N'}^{\infty} \frac{|b_k|}{|E'(a_k)|} \frac{M_R}{4R-R} \frac{R}{RM_R} \Big ( \frac{R}{4R} \Big ) ^{m_k -1} \\ &= \sum_{k=N'}^{\infty} \frac{|b_k|}{|E'(a_k)|} \frac{1}{3R} \Big ( \frac{1}{4} \Big ) ^{m_k -1} \\ &\le \sum_{k=N'}^{\infty} \frac{2^{-k}}{R} = \frac{2^{1-N'}}{R} \end{align*} Since right side of the inequality converges to $0$ as $N' \to \infty$ , our infinite sum converges uniformly. Thus, $F$ is entire. Now we only need to prove that $F(a_n) = b_n$ . Using that $E(a_n)=0$ and $\lim\limits_{z \to a_n} \frac{E(z)}{z-a_n} = E'(a_n)$ , \begin{align*} F(a_n) = \frac{b_n}{E'(a_n)} \frac{E(z)}{z-a_n} \Big | _ {z=a_n} = b_n \end{align*} Thank you for reading my question. Any kind of feedbacks (both mathematical one, formatting-related one, english-related one) are welcome.","This is an exercise in Complex Analysis by Stein, Exercise 5.17. Let be a sequence of distinct complex numbers such that and , and let , where is Weierstrass canonical factors defined by Given complex numbers , show that there exist integers such that the series defines an entire function that satisfies for all . Here is my proof, which I want to get verified. I assumed that I already know that is entire, and has simple zeros only at s. First, let's prove that , so that fractions are well-defined. Since has simple zero at , there exists entire function where and . Then, , thus we have , as desired. Now let be an integer that satisfies To prove that is entire, we only need to prove that the infinite sum converges uniformly in the closed disc . Let be the supremum of in . Let be a integer such that yields . Then for , we have Since right side of the inequality converges to as , our infinite sum converges uniformly. Thus, is entire. Now we only need to prove that . Using that and , Thank you for reading my question. Any kind of feedbacks (both mathematical one, formatting-related one, english-related one) are welcome.","\{a_n\}_{k=0}^{\infty} a_0 =0 \lim_{k \to \infty} |a_k| = \infty \begin{align*}
E(z)=z\prod_{n=1}^{\infty}{E_n (\frac{z}{a_n})}
\end{align*} E_k \begin{align*}
E_k (z) = (1-z) e^{z+ \frac{z^2}{2} + \cdots + \frac{z^k}{k}}
\end{align*} \{b_k\}_{k=0}^{\infty} m_k \ge 1 \begin{align*}
F(z) = \frac{b_0}{E'(a_0 )}\frac{E(z)}{z} + \sum_{k=1}^{\infty} \frac{b_k}{E'(a_k)}\frac{E(z)}{z-a_k} \Big ( \frac{z}{a_k} \Big ) ^{m_k} 
\end{align*} F(a_k )=b_k  k \ge 0 E(z) \{a_k\} E'(a_k) \neq 0 E a_k F_k E(z)=(z-a_k)F_k (z) F_k (a_k) \neq 0 \begin{align*}
E'(z)=F_k (z) + (z- a_k ) {F_k}'(z)
\end{align*} E'(a_k) \neq 0 m_k \begin{align*}
\frac{|b_k|}{|E'(a_k)|} \frac{1}{3} 4^{1-m_k} < 2^{-k}
\end{align*} F |z| \le R M_R E(z) |z| \le R N n \ge N |a_n| \ge \max\{RM_R , 4R\} N' \ge N \begin{align*}
\Big | \sum_{k=N'}^{\infty} \frac{b_k}{E'(a_k)} \frac{E(z)}{z-a_k} \Big ( \frac{z}{a_k} \Big )^{m_k} \Big | &\le \sum_{k=N'}^{\infty} \frac{|b_k|}{|E'(a_k)|} \frac{|E(z)|}{|z-a_k|} \Big | \frac{z}{a_k} \Big |^{m_k} \\
&\le \sum_{k=N'}^{\infty} \frac{|b_k|}{|E'(a_k)|} \frac{M_R}{4R-R} \frac{R}{RM_R} \Big ( \frac{R}{4R} \Big ) ^{m_k -1} \\
&= \sum_{k=N'}^{\infty} \frac{|b_k|}{|E'(a_k)|} \frac{1}{3R} \Big ( \frac{1}{4} \Big ) ^{m_k -1} \\
&\le \sum_{k=N'}^{\infty} \frac{2^{-k}}{R} = \frac{2^{1-N'}}{R}
\end{align*} 0 N' \to \infty F F(a_n) = b_n E(a_n)=0 \lim\limits_{z \to a_n} \frac{E(z)}{z-a_n} = E'(a_n) \begin{align*}
F(a_n) = \frac{b_n}{E'(a_n)} \frac{E(z)}{z-a_n} \Big | _ {z=a_n} = b_n
\end{align*}","['complex-analysis', 'solution-verification']"
60,Ideas for parameterizing this curve in the complex plane and calculating its length by (numerical) contour integration?,Ideas for parameterizing this curve in the complex plane and calculating its length by (numerical) contour integration?,,"Let $Z (t)$ be the Hardy Z function. Then define \begin{equation}   Y (t) = \tanh (\ln (1 + Z (t)^2)) \end{equation} Let us define the length $L_n$ of the curve which intersects the $n$ -th Zero of $Z (t)$ as a contour integral of the indicator function \begin{equation}   \label{cont} L_n = \oint_{H_n} 1 d t \end{equation} where \begin{equation}   H_n = \{ (t, s) : {Re} (Y (t + i s)) = 0 : H_n \cup \{ y_n \} = \{ y_n   \} \} \end{equation} is the $n$ -th set of points determining the curves where the real part of $Y$ vanishes and $y_n$ is the $n$ -th real zero of the Hardy Z function ordered by ascending magnitude, independently of the Riemann hypothesis; which states that all of the roots lie on the real line. If there are roots of the zeta function off of the line, they will appear as complex roots of $Z (t)$ other than the trivial ones or the points where Z(t)=i*sqrt(2) To calculate the value of the contour integral  , the following algorithm is proposed: a. Choose parameter $n$ , the zero of the index; b. Choose parameter $h$ , the delta with which the integral will be calculated Let $x_0 = y_n$ and ${L_n} = 0$ Determine angle $\theta_m \in \left[ 0, \frac{\pi}{2} \right]$ where \begin{equation}   \label{angle} \theta_m = \left\{ a : {Re} (Y ({circle} (x_{m - 1},   h, a))) = 0, 0 < a < \frac{\pi}{2} \right\} \end{equation} when iterating from $x_{(m - 1)}$ to $x_m$ a step length of $h$ and angle $\theta_m (h)$ given by the circle with radius $h$ centered at $z$ defined by the formula \begin{equation}   {circle} (z, h, \theta) = z + h (\cos (\theta) + i \sin (\theta)) \end{equation} where angle parameter $\theta$ varies from $0$ to $\pi$ and $z = t + i s$ . Set \begin{equation}   x_m = {circle} (x_{m - 1}, h, \theta_m) \end{equation} and \begin{equation}   {L_n} = {L_n} + h \end{equation} then $m$ represents the index of the $m$ -th iterate of the Newton iteration $\theta_m (h) = \lim_{k \rightarrow \infty} N_{\theta_m} (a_k ; h)$ defined below. If $x_m = y_n$ then terminate because the loop has reached its beginning point, and ${L_n}$ will be its length; else set \begin{equation}   m = m + 1 \end{equation} and goto Step 4. In order to find an explicit expression for the angle $\theta_m (h)$ we can use Newton's method \begin{equation}   \label{N} N_{\theta_m} (a_k ; h) = a_{k - 1} - \frac{Y ({circle} (x_m,   h, a_{k - 1}))}{\frac{d}{da} Y ({circle} (x_m, h,   a))_{|_{a = a_{k - 1}}}} \end{equation} so that \begin{equation}   \theta_m = \theta_m (h) = \lim_{k \rightarrow \infty} N_{\theta_m} (a_k ; h) \end{equation} which is dependent on $h$ , but the when the dependence is not written as $\theta_m (h)$ it is still implied. To calculate $\frac{d}{d a} Y ({circle} (x_m, h, a))_{|_{a = a_{k - 1}}}$ let \begin{equation}   \rho_{} (h, a) = h (i \cos (a) - \sin (a)) \end{equation} and differentiate $Y (t)$ to get \begin{equation}   \frac{d}{d t} Y (t) = \frac{d}{d t} \tanh (\ln (1 + Z   (t)^2)) = \frac{8 (1 + Z (t)^2) Z (t) \frac{d}{d t} Z (t)}{(Z   (t)^4 + 2 Z (t)^2 + 2)^2} \end{equation} then it can be seen that $\begin{array}{ll}   \frac{d}{da} Y ({circle} (t, h, a)) & = \frac{d}{   da} Y (t + h e^{i \pi a})\\   & = \frac{d}{d a} \tanh (\ln (1 + Z (t + h e^{i \pi a})^2))\\   & = \frac{8 (1 + Y (t)^2) Y (t) }{(Y (t)^4 + 2 Y (t)^2 + 2)^2}   \frac{d}{d t} Y (t) h e^{i \pi a}\\   & = \frac{8 (1 + Y (t)^2) Y (t) }{(Y (t)^4 + 2 Y (t)^2 + 2)^2} \frac{8 (1 +   Z (t)^2) Z (t)}{(Z (t)^4 + 2 Z (t)^2 + 2)^2} \frac{d}{d t} Z (t) h   e^{i \pi a}\\   & = i 8 \pi h \hspace{0.17em} \mathrm{e}^{i \pi a} Z (t) \frac{ (Z (t) + i)   (Z (t) - i) }{(Z (t)^2 + 1 + i)^2  (Z (t)^2 + 1 - i)^2}   \frac{d}{d t} Z (t) \end{array}$ and therefore the Newton iteration for the angle is expressed as \begin{equation}   \begin{array}{ll}     N_{\theta_m} (t, a_k ; h) & = a_{k - 1} - \frac{Y (t)}{\frac{8 (1 + Y     (t)^2) Y (t) }{(Y (t)^4 + 2 Y (t)^2 + 2)^2} \frac{d}{d t} Y (t)     \rho_{} (h, a_{k - 1}) )}\\     & = a_{k - 1} - \frac{Y (t)}{\frac{8 (1 + Y (t)^2) Y (t) }{(Y (t)^4 + 2 Y     (t)^2 + 2)^2} \frac{8 (1 + Z (t)^2) Z (t) \frac{d}{d t} Z     (t)}{(Z (t)^4 + 2 Z (t)^2 + 2)^2} \rho_{} (h, a_{k - 1}) )}   \end{array} \end{equation} by the change-of-variables \begin{equation}   \begin{array}{ll}     t \rightarrow & {circle} (x_m, h, a_{k - 1})\\     & = x_m + \rho (h, a_{k - 1})\\     & = x_m + h (\cos (a_{k - 1}) + i \sin (a_{k - 1}))   \end{array} \end{equation} The set $H_1$ is black  depicted as the boundary of the 1st hour-glass figure in this plot of the Real part of Y over -12..27 and imaginary part -3..3 . The values of the real part of $Y$ have opposite signs inside and outside of the sets defined by the union of the curves $H_n$ . Here $H_n$ is the Julia set of some dynamical system. My question: Is there a better or different way of calculating the length of these curves? Maple code to calculate the length at https://github.com/crowlogic/Y/blob/master/tracecurve.mpl The first top-half curve length is  4.263.. The second is                       2.982... Real part of Y from Re=12..27 and Im=-3..3 Imag part of Y from Re=12..27 and Im=-3..3 Complex plot of Y from Re=12..27 and Im=-3..3 The sign of the argument of Y which gets very small is shown here: See https://gist.githubusercontent.com/crowlogic/027ea86b58b0fabf8e99d25a685d9196/raw/d55bbc5c866c185b106b44e84d39b6a37f24bb95/maple%2520tracecurve%2520log.txt for the maple script output","Let be the Hardy Z function. Then define Let us define the length of the curve which intersects the -th Zero of as a contour integral of the indicator function where is the -th set of points determining the curves where the real part of vanishes and is the -th real zero of the Hardy Z function ordered by ascending magnitude, independently of the Riemann hypothesis; which states that all of the roots lie on the real line. If there are roots of the zeta function off of the line, they will appear as complex roots of other than the trivial ones or the points where Z(t)=i*sqrt(2) To calculate the value of the contour integral  , the following algorithm is proposed: a. Choose parameter , the zero of the index; b. Choose parameter , the delta with which the integral will be calculated Let and Determine angle where when iterating from to a step length of and angle given by the circle with radius centered at defined by the formula where angle parameter varies from to and . Set and then represents the index of the -th iterate of the Newton iteration defined below. If then terminate because the loop has reached its beginning point, and will be its length; else set and goto Step 4. In order to find an explicit expression for the angle we can use Newton's method so that which is dependent on , but the when the dependence is not written as it is still implied. To calculate let and differentiate to get then it can be seen that and therefore the Newton iteration for the angle is expressed as by the change-of-variables The set is black  depicted as the boundary of the 1st hour-glass figure in this plot of the Real part of Y over -12..27 and imaginary part -3..3 . The values of the real part of have opposite signs inside and outside of the sets defined by the union of the curves . Here is the Julia set of some dynamical system. My question: Is there a better or different way of calculating the length of these curves? Maple code to calculate the length at https://github.com/crowlogic/Y/blob/master/tracecurve.mpl The first top-half curve length is  4.263.. The second is                       2.982... Real part of Y from Re=12..27 and Im=-3..3 Imag part of Y from Re=12..27 and Im=-3..3 Complex plot of Y from Re=12..27 and Im=-3..3 The sign of the argument of Y which gets very small is shown here: See https://gist.githubusercontent.com/crowlogic/027ea86b58b0fabf8e99d25a685d9196/raw/d55bbc5c866c185b106b44e84d39b6a37f24bb95/maple%2520tracecurve%2520log.txt for the maple script output","Z (t) \begin{equation}
  Y (t) = \tanh (\ln (1 + Z (t)^2))
\end{equation} L_n n Z (t) \begin{equation}
  \label{cont} L_n = \oint_{H_n} 1 d t
\end{equation} \begin{equation}
  H_n = \{ (t, s) : {Re} (Y (t + i s)) = 0 : H_n \cup \{ y_n \} = \{ y_n
  \} \}
\end{equation} n Y y_n n Z (t) n h x_0 = y_n {L_n} = 0 \theta_m \in \left[ 0, \frac{\pi}{2} \right] \begin{equation}
  \label{angle} \theta_m = \left\{ a : {Re} (Y ({circle} (x_{m - 1},
  h, a))) = 0, 0 < a < \frac{\pi}{2} \right\}
\end{equation} x_{(m - 1)} x_m h \theta_m (h) h z \begin{equation}
  {circle} (z, h, \theta) = z + h (\cos (\theta) + i \sin (\theta))
\end{equation} \theta 0 \pi z = t + i s \begin{equation}
  x_m = {circle} (x_{m - 1}, h, \theta_m)
\end{equation} \begin{equation}
  {L_n} = {L_n} + h
\end{equation} m m \theta_m (h) = \lim_{k \rightarrow \infty} N_{\theta_m} (a_k ; h) x_m = y_n {L_n} \begin{equation}
  m = m + 1
\end{equation} \theta_m (h) \begin{equation}
  \label{N} N_{\theta_m} (a_k ; h) = a_{k - 1} - \frac{Y ({circle} (x_m,
  h, a_{k - 1}))}{\frac{d}{da} Y ({circle} (x_m, h,
  a))_{|_{a = a_{k - 1}}}}
\end{equation} \begin{equation}
  \theta_m = \theta_m (h) = \lim_{k \rightarrow \infty} N_{\theta_m} (a_k ; h)
\end{equation} h \theta_m (h) \frac{d}{d a}
Y ({circle} (x_m, h, a))_{|_{a = a_{k - 1}}} \begin{equation}
  \rho_{} (h, a) = h (i \cos (a) - \sin (a))
\end{equation} Y (t) \begin{equation}
  \frac{d}{d t} Y (t) = \frac{d}{d t} \tanh (\ln (1 + Z
  (t)^2)) = \frac{8 (1 + Z (t)^2) Z (t) \frac{d}{d t} Z (t)}{(Z
  (t)^4 + 2 Z (t)^2 + 2)^2}
\end{equation} \begin{array}{ll}
  \frac{d}{da} Y ({circle} (t, h, a)) & = \frac{d}{
  da} Y (t + h e^{i \pi a})\\
  & = \frac{d}{d a} \tanh (\ln (1 + Z (t + h e^{i \pi a})^2))\\
  & = \frac{8 (1 + Y (t)^2) Y (t) }{(Y (t)^4 + 2 Y (t)^2 + 2)^2}
  \frac{d}{d t} Y (t) h e^{i \pi a}\\
  & = \frac{8 (1 + Y (t)^2) Y (t) }{(Y (t)^4 + 2 Y (t)^2 + 2)^2} \frac{8 (1 +
  Z (t)^2) Z (t)}{(Z (t)^4 + 2 Z (t)^2 + 2)^2} \frac{d}{d t} Z (t) h
  e^{i \pi a}\\
  & = i 8 \pi h \hspace{0.17em} \mathrm{e}^{i \pi a} Z (t) \frac{ (Z (t) + i)
  (Z (t) - i) }{(Z (t)^2 + 1 + i)^2  (Z (t)^2 + 1 - i)^2}
  \frac{d}{d t} Z (t)
\end{array} \begin{equation}
  \begin{array}{ll}
    N_{\theta_m} (t, a_k ; h) & = a_{k - 1} - \frac{Y (t)}{\frac{8 (1 + Y
    (t)^2) Y (t) }{(Y (t)^4 + 2 Y (t)^2 + 2)^2} \frac{d}{d t} Y (t)
    \rho_{} (h, a_{k - 1}) )}\\
    & = a_{k - 1} - \frac{Y (t)}{\frac{8 (1 + Y (t)^2) Y (t) }{(Y (t)^4 + 2 Y
    (t)^2 + 2)^2} \frac{8 (1 + Z (t)^2) Z (t) \frac{d}{d t} Z
    (t)}{(Z (t)^4 + 2 Z (t)^2 + 2)^2} \rho_{} (h, a_{k - 1}) )}
  \end{array}
\end{equation} \begin{equation}
  \begin{array}{ll}
    t \rightarrow & {circle} (x_m, h, a_{k - 1})\\
    & = x_m + \rho (h, a_{k - 1})\\
    & = x_m + h (\cos (a_{k - 1}) + i \sin (a_{k - 1}))
  \end{array}
\end{equation} H_1 Y H_n H_n","['complex-analysis', 'curves', 'riemann-zeta', 'plane-curves', 'arc-length']"
61,Maximal operator inequality $P^*$,Maximal operator inequality,P^*,"The maximal operator function $P^*$ is defined in this way: DEF: If $f\in L^{p}(\mathbb{T})$ , $P^*f(x)=sup_{0<r<1}|P_{r}*f(x)|$ .Where $P_{r}(t)=\sum_{I=-\infty}^{\infty}r^{|i|}\phi_{i}(t)$ is the kernel of Poisson. But now, with this maximal operator, is possible to demonstrate that, for $1\le p<\infty$ , there is a constant $c_{p}>0$ such that, for all $f\in L^p(\mathbb{T})$ : $||P^*f||_{p}\le c_{p}||f||_{p}$ ? Thanks.","The maximal operator function is defined in this way: DEF: If , .Where is the kernel of Poisson. But now, with this maximal operator, is possible to demonstrate that, for , there is a constant such that, for all : ? Thanks.",P^* f\in L^{p}(\mathbb{T}) P^*f(x)=sup_{0<r<1}|P_{r}*f(x)| P_{r}(t)=\sum_{I=-\infty}^{\infty}r^{|i|}\phi_{i}(t) 1\le p<\infty c_{p}>0 f\in L^p(\mathbb{T}) ||P^*f||_{p}\le c_{p}||f||_{p},"['complex-analysis', 'inequality', 'convolution', 'harmonic-functions']"
62,True or False: Every entire function $f(z)$ which goes to $0$ as $|z|\to \infty$ is bounded?,True or False: Every entire function  which goes to  as  is bounded?,f(z) 0 |z|\to \infty,"I am almost positive I am incorrect, but I was hoping someone could explain why I am wrong because for the life of me I am at a loss. I am not asking for a counterexample or some contra-reasoning as to why my conclusion is incorrect, but rather I am hoping someone can point out the flaw in my actual argument. Suppose $f\in C^{\infty}(\mathbb{C})$ . Suppose $|f(z)|\to 0$ as $|z|\to \infty$ . My claim is that $f\equiv 0$ . Reasoning: If $f$ is smooth on $\mathbb{C}$ , then by definition $f$ is holomorphic, since $f$ is $\mathbb{C}$ -differentiable. So then $f$ is entire. But if $|f(z)|\to 0$ as $|z|\to \infty$ , then there exists some $N$ such that $|z|>N$ implies that $|f(z)|<1$ . So then consider the closed disk of radius $N$ . We know that $f$ is holomorphic on this disc. But such a disc is compact, so $f$ must achieve a maximum and minimum on this disc. So $f$ is bounded on this disc, and $f$ is also bounded outside of the disc by $1$ . So $f$ is bounded and entire, so then by Liouville it is constant. But we know that it goes to $0$ as $|z|\to \infty$ . So then $f\equiv 0$ . I am almost certain this is wrong somewhere, but to me my argument seems sounds. Any insight would be helpful.","I am almost positive I am incorrect, but I was hoping someone could explain why I am wrong because for the life of me I am at a loss. I am not asking for a counterexample or some contra-reasoning as to why my conclusion is incorrect, but rather I am hoping someone can point out the flaw in my actual argument. Suppose . Suppose as . My claim is that . Reasoning: If is smooth on , then by definition is holomorphic, since is -differentiable. So then is entire. But if as , then there exists some such that implies that . So then consider the closed disk of radius . We know that is holomorphic on this disc. But such a disc is compact, so must achieve a maximum and minimum on this disc. So is bounded on this disc, and is also bounded outside of the disc by . So is bounded and entire, so then by Liouville it is constant. But we know that it goes to as . So then . I am almost certain this is wrong somewhere, but to me my argument seems sounds. Any insight would be helpful.",f\in C^{\infty}(\mathbb{C}) |f(z)|\to 0 |z|\to \infty f\equiv 0 f \mathbb{C} f f \mathbb{C} f |f(z)|\to 0 |z|\to \infty N |z|>N |f(z)|<1 N f f f f 1 f 0 |z|\to \infty f\equiv 0,[]
63,Does $\log z$ have a laurent series about $0$?,Does  have a laurent series about ?,\log z 0,I think not because $\log z$ isn't analytic on any neighborhood of 0. Is this correct? thanks,I think not because isn't analytic on any neighborhood of 0. Is this correct? thanks,\log z,"['complex-analysis', 'laurent-series']"
64,"Real, analytic exponential factorial $f(x) = x^{f(x-1)}$","Real, analytic exponential factorial",f(x) = x^{f(x-1)},"I'm wondering if anyone has a reference or a method for construction a real-analytic function interpolating the exponential factorial , i.e. a function such that $f(x) = x^{f(x-1)}$ and $f(1) = 1$ , analogous to the gamma function interpolating the usual factorial. It's easy to construct a complex-valued solution that should be analytic on $\mathbb{C}\setminus\mathbb{R}$ to the clearly related functional equation $f(z) = z^{(z-1)^{(z-2)^{f(z-3)}}}$ (note that satisfying the exponential factorial equation implies this, but not the other way around). If you look at the power tower $$ f_n(z) =z^{(z-1)^{(z-2)^{(z-3)^{\cdots (z-n)}}}} $$ the values have period 3 as $n$ approaches infinity, so it seems reasonable that there are 3 limiting analytic functions, each satisfying $f(z)=z^{(z-1)^{(z-2)^{f(z-3)}}}$ . (I haven't proven any of this formally, since I was mostly trying to find a real-valued solution and these are not real valued on $\mathbb{R}$ ) An example computation, here's a plot of the real and imaginary parts of $f_n(1.5)$ for $n$ from 0 to 100 (real part in black, imaginary part in red): $f_{3n}$ seems to converge pretty rapidly (and similarly for $f_{3n+1}$ and $f_{3n+2}$ ). But these limiting functions won't be analytic at integer values; in general it will only be $n-1$ times differentiable at $n\in\{1,2,3,\dots\}$ . Also, I don't believe they would have the property $f(x)=x^{f(x-1)}$ because of the 3-period nature of the limit. Does anyone know of a way to construct a real-analytic solution to the equation, or at least a solution that is analytic (but possibly complex-valued) on $\mathbb{R}$ ? If it's easier to solve the equation $f(x) = x^{f(x-1)}$ with a different initial value, i.e. $f(1) = c$ for some other value of $c$ , I would also be interested to see that.","I'm wondering if anyone has a reference or a method for construction a real-analytic function interpolating the exponential factorial , i.e. a function such that and , analogous to the gamma function interpolating the usual factorial. It's easy to construct a complex-valued solution that should be analytic on to the clearly related functional equation (note that satisfying the exponential factorial equation implies this, but not the other way around). If you look at the power tower the values have period 3 as approaches infinity, so it seems reasonable that there are 3 limiting analytic functions, each satisfying . (I haven't proven any of this formally, since I was mostly trying to find a real-valued solution and these are not real valued on ) An example computation, here's a plot of the real and imaginary parts of for from 0 to 100 (real part in black, imaginary part in red): seems to converge pretty rapidly (and similarly for and ). But these limiting functions won't be analytic at integer values; in general it will only be times differentiable at . Also, I don't believe they would have the property because of the 3-period nature of the limit. Does anyone know of a way to construct a real-analytic solution to the equation, or at least a solution that is analytic (but possibly complex-valued) on ? If it's easier to solve the equation with a different initial value, i.e. for some other value of , I would also be interested to see that.","f(x) = x^{f(x-1)} f(1) = 1 \mathbb{C}\setminus\mathbb{R} f(z) = z^{(z-1)^{(z-2)^{f(z-3)}}} 
f_n(z) =z^{(z-1)^{(z-2)^{(z-3)^{\cdots (z-n)}}}}
 n f(z)=z^{(z-1)^{(z-2)^{f(z-3)}}} \mathbb{R} f_n(1.5) n f_{3n} f_{3n+1} f_{3n+2} n-1 n\in\{1,2,3,\dots\} f(x)=x^{f(x-1)} \mathbb{R} f(x) = x^{f(x-1)} f(1) = c c","['real-analysis', 'complex-analysis', 'functional-equations', 'analytic-functions']"
65,$\int_a^b\frac{|\gamma'(t)|}{1-|\gamma(t)|^2}dt$ (from a calculation of Poincare distance)?,(from a calculation of Poincare distance)?,\int_a^b\frac{|\gamma'(t)|}{1-|\gamma(t)|^2}dt,"This answer requires only 2 new definitions and elementary complex analysis. I have been reading these notes titled Invariant Pseudodistances and Pseudometrics in Complex Analysis in Several Variables (PDF link via diva-portal.org) . On page 13, section 1.4.3, they define the Poincare distance. Can you help me with the calculations where they compute $L_{{\beta}_\mathbb{D}}$ I know that the portion marked in yellow is a typo. But Can you tell me how $L_{{\beta}_\mathbb{D}}$ Is equal to the expression marked in red with a $dt$ . And also why ${\beta}^i_\mathbb{D}(0, \phi_z(w))$ is equal to ${\beta}^i_\mathbb{D}(0, |\phi_z(w)|)$ ?","This answer requires only 2 new definitions and elementary complex analysis. I have been reading these notes titled Invariant Pseudodistances and Pseudometrics in Complex Analysis in Several Variables (PDF link via diva-portal.org) . On page 13, section 1.4.3, they define the Poincare distance. Can you help me with the calculations where they compute I know that the portion marked in yellow is a typo. But Can you tell me how Is equal to the expression marked in red with a . And also why is equal to ?","L_{{\beta}_\mathbb{D}} L_{{\beta}_\mathbb{D}} dt {\beta}^i_\mathbb{D}(0, \phi_z(w)) {\beta}^i_\mathbb{D}(0, |\phi_z(w)|)","['complex-analysis', 'hyperbolic-geometry', 'mobius-transformation']"
66,Residue of product,Residue of product,,"Suppose $f$ has an $n$ -th order pole at $0$ and $g$ is holomorphic at $0$ .  Can I write the residue of the product ""in terms of"" the residue of $f$ ? Since $$Res(fg)=\frac {1}{(n-1!)}\lim _{s\rightarrow 0}\left (\frac {d^{n-1}}{ds^{n-1}}\left \{ s^nf(s)g(s)\right \}\right )$$ and since the derivative here is $$\sum _{k=0}^{n-1}\left (\begin {array}{l}n-1\\ \hspace {4mm}k\end {array}\right )\frac {d^{n-1-k}}{ds^{n-1-k}}\left \{ s^nf(s)\right \} \frac {d^{k}}{ds^{k}}\left \{ g(s)\right \}$$ I can deduce $$Res(fg)=\frac {1}{(n-1)!}\sum _{k=0}^{n-1}\left (\begin {array}{l}n-1\\ \hspace {4mm}k\end {array}\right )(n-1-k)!g^{(k)}(0)\lim _{s\rightarrow 0}\left (\frac {d^{n-1-k}}{ds^{n-1-k}}\left \{ s^nf(s)\right \} \right )$$ which perhaps isn't too far from what I want?  I'd like it if this last limit was ""something like"" the residue of $f$ .","Suppose has an -th order pole at and is holomorphic at .  Can I write the residue of the product ""in terms of"" the residue of ? Since and since the derivative here is I can deduce which perhaps isn't too far from what I want?  I'd like it if this last limit was ""something like"" the residue of .",f n 0 g 0 f Res(fg)=\frac {1}{(n-1!)}\lim _{s\rightarrow 0}\left (\frac {d^{n-1}}{ds^{n-1}}\left \{ s^nf(s)g(s)\right \}\right ) \sum _{k=0}^{n-1}\left (\begin {array}{l}n-1\\ \hspace {4mm}k\end {array}\right )\frac {d^{n-1-k}}{ds^{n-1-k}}\left \{ s^nf(s)\right \} \frac {d^{k}}{ds^{k}}\left \{ g(s)\right \} Res(fg)=\frac {1}{(n-1)!}\sum _{k=0}^{n-1}\left (\begin {array}{l}n-1\\ \hspace {4mm}k\end {array}\right )(n-1-k)!g^{(k)}(0)\lim _{s\rightarrow 0}\left (\frac {d^{n-1-k}}{ds^{n-1-k}}\left \{ s^nf(s)\right \} \right ) f,['complex-analysis']
67,Plurisubharmonic function composed with nonholomorphic function,Plurisubharmonic function composed with nonholomorphic function,,"Suppose $f:\mathbb{C}^2 \to \mathbb{R}$ is plurisubharmonic, and let $\Omega \subset \mathbb{C}^2$ be a connected, open set. I've got a function $u:\Omega \to \Omega$ that is nonholomorphic, but real-analytic as a function of the real and imaginary parts of its argument; e.g. something like $u(\mathbf{z})= \overline{z_1\!} \, \mathbf{z}$ , where $\mathbf{z} = (z_1,z_2)$ . Is there any literature that specifies conditions on such $u$ so that $f\circ u$ is plurisubharmonic? Currently I can only seem to find results when $u$ is assumed to be holomorphic; e.g., Corollary 2.9.5 in Klimek's Pluripotential theory . What I'm really trying to do is show that the complex Hessian of some specific composition $f\circ u$ is positive-definite, but dealing with this matrix directly is prohibitively complicated. So I am trying to work with an equivalent characterization of this property. Directly checking if my function $f\circ u$ is PLSH also seems intractable via the ""subharmonic on complex lines"" characterization, so I'm running out of ideas. Even results akin to Klimek's Theorem 2.9.19 and its corollaries could be useful to me - basically, I'm interested in general ways of building PLSH functions from ""parts"" that could be non holomorphic. Of course, if there is some strong limit on the extent that what I am asking for is achievable, I would be interested to know that, too.","Suppose is plurisubharmonic, and let be a connected, open set. I've got a function that is nonholomorphic, but real-analytic as a function of the real and imaginary parts of its argument; e.g. something like , where . Is there any literature that specifies conditions on such so that is plurisubharmonic? Currently I can only seem to find results when is assumed to be holomorphic; e.g., Corollary 2.9.5 in Klimek's Pluripotential theory . What I'm really trying to do is show that the complex Hessian of some specific composition is positive-definite, but dealing with this matrix directly is prohibitively complicated. So I am trying to work with an equivalent characterization of this property. Directly checking if my function is PLSH also seems intractable via the ""subharmonic on complex lines"" characterization, so I'm running out of ideas. Even results akin to Klimek's Theorem 2.9.19 and its corollaries could be useful to me - basically, I'm interested in general ways of building PLSH functions from ""parts"" that could be non holomorphic. Of course, if there is some strong limit on the extent that what I am asking for is achievable, I would be interested to know that, too.","f:\mathbb{C}^2 \to \mathbb{R} \Omega \subset \mathbb{C}^2 u:\Omega \to \Omega u(\mathbf{z})= \overline{z_1\!} \, \mathbf{z} \mathbf{z} = (z_1,z_2) u f\circ u u f\circ u f\circ u","['complex-analysis', 'several-complex-variables']"
68,Nevanlinna - Herglotz - Pick - R functions : boundary values on the real axis,Nevanlinna - Herglotz - Pick - R functions : boundary values on the real axis,,"Nevanlinna/Herglotz functions are analytic functions defined on the upper half complex plane and have non-negative imaginary part there. A remarkable theorem asserts that every such function has a unique integral representation of the form $$f(z) = a + bz + \int_{\mathbb{R}} \left( \frac{1}{\lambda-z} -\frac{\lambda}{\lambda^2+1} \right) d\mu(\lambda),$$ where $\mu$ is a positive Borel measure satisfying $\int (\lambda^2+1)^{-1} d\mu(\lambda) < \infty$ , $a \in \mathbb{R}$ and $b \geq 0$ . The measure $\mu$ is recovered from $f$ by the Stieltjes inversion formula $$\mu(( \lambda_1,\lambda_2]) = \lim \limits_{\delta \downarrow 0} \lim \limits_{\epsilon \downarrow 0} \frac{1}{\pi} \int_{\lambda_1+\delta} ^{\lambda_2+\delta} \text{Im} \ (f(\lambda+i\epsilon)) d\lambda.$$ I am interested in understanding the relationship between the support of the measure $\mu$ and the restriction of $f$ to the real line. To this end, I define the Domain of $f(x)$ to be $$\text{Dom}(f) := \{ x \in \mathbb{R} : f(x) \ \text{is defined there and} \ f(x) \ \text{is a finite real number} \}.$$ My question to you : Is it true in general that for all Nevanlinna functions, $\text{Dom} (f) \cup \text{supp} \ \mu = \mathbb{R}$ and $\text{Dom} (f) \cap \text{supp} \ \mu = \emptyset$ ? Intuitively, from the Stieltjes formula $ \text{Dom} (f) \cap \text{supp} \ \mu = \emptyset$ makes sense...but I'd like to hear from the experts. Thanks ! ---------------------------------------------------------------------- Some examples of Nevanlinna functions : 1) $f(z) = z$ . Then $\text{Dom}(f) = \mathbb{R}$ , $\mu = 0$ and $\text{supp} \ \mu = \emptyset$ . 2) $f(z) = -z^{-1}$ . Then $\text{Dom}(f) = \mathbb{R}\setminus \{0\}$ , $\mu = 1_{\{0\}}$ and $\text{supp} \ \mu = \{0\}$ . 3) $f(z) = \log(z)$ . Then $\text{Dom}(f) = (0,+\infty)$ , assuming the standard branch cut, $\mu = 1_{(-\infty,0]}$ and $\text{supp} \ \mu = (-\infty,0]$ . 4) $f(z) = \tan(z)$ . Then, denoting $\Sigma := \{ (n+1/2)\pi : n \in \mathbb{Z}\}$ , we have $\text{Dom}(f) = \mathbb{R} \setminus \Sigma $ , $\mu = 1_{\Sigma}$ and $\text{supp} \ \mu = \Sigma$ .","Nevanlinna/Herglotz functions are analytic functions defined on the upper half complex plane and have non-negative imaginary part there. A remarkable theorem asserts that every such function has a unique integral representation of the form where is a positive Borel measure satisfying , and . The measure is recovered from by the Stieltjes inversion formula I am interested in understanding the relationship between the support of the measure and the restriction of to the real line. To this end, I define the Domain of to be My question to you : Is it true in general that for all Nevanlinna functions, and ? Intuitively, from the Stieltjes formula makes sense...but I'd like to hear from the experts. Thanks ! ---------------------------------------------------------------------- Some examples of Nevanlinna functions : 1) . Then , and . 2) . Then , and . 3) . Then , assuming the standard branch cut, and . 4) . Then, denoting , we have , and .","f(z) = a + bz + \int_{\mathbb{R}} \left( \frac{1}{\lambda-z} -\frac{\lambda}{\lambda^2+1} \right) d\mu(\lambda), \mu \int (\lambda^2+1)^{-1} d\mu(\lambda) < \infty a \in \mathbb{R} b \geq 0 \mu f \mu(( \lambda_1,\lambda_2]) = \lim \limits_{\delta \downarrow 0} \lim \limits_{\epsilon \downarrow 0} \frac{1}{\pi} \int_{\lambda_1+\delta} ^{\lambda_2+\delta} \text{Im} \ (f(\lambda+i\epsilon)) d\lambda. \mu f f(x) \text{Dom}(f) := \{ x \in \mathbb{R} : f(x) \ \text{is defined there and} \ f(x) \ \text{is a finite real number} \}. \text{Dom} (f) \cup \text{supp} \ \mu = \mathbb{R} \text{Dom} (f) \cap \text{supp} \ \mu = \emptyset  \text{Dom} (f) \cap \text{supp} \ \mu = \emptyset f(z) = z \text{Dom}(f) = \mathbb{R} \mu = 0 \text{supp} \ \mu = \emptyset f(z) = -z^{-1} \text{Dom}(f) = \mathbb{R}\setminus \{0\} \mu = 1_{\{0\}} \text{supp} \ \mu = \{0\} f(z) = \log(z) \text{Dom}(f) = (0,+\infty) \mu = 1_{(-\infty,0]} \text{supp} \ \mu = (-\infty,0] f(z) = \tan(z) \Sigma := \{ (n+1/2)\pi : n \in \mathbb{Z}\} \text{Dom}(f) = \mathbb{R} \setminus \Sigma  \mu = 1_{\Sigma} \text{supp} \ \mu = \Sigma","['complex-analysis', 'functions', 'analytic-continuation']"
69,Connection between Residue Theorem in complex analysis and algebraic geometry,Connection between Residue Theorem in complex analysis and algebraic geometry,,"I am currently studying algebraic curves using the Fulton. In chapter 8 we have a proposition called ""Residue Theorem"" RESIDUE THEOREM. Let $C,E$ be as above ( $C$ is a projective plane curve, $E = \sum_{Q\in X} (m_{(f(Q)} - 1)\, Q$ , $X$ is a non singular model of $C$ ). Suppose $D$ and $D'$ are effective divisors on $X$ , with $D'\equiv D$ . Suppose $G$ is an adjoint of degree $m$ such that $\operatorname{div}(G) = D + E + A$ for some effective divisor $A$ . Then there is an adjoint $G'$ of degree $m$ such that $\operatorname{div}(G') = D' + E + A$ . I was wondering if this has anything to do with the Residue Theorem in complex analysis. Can anyone help me?","I am currently studying algebraic curves using the Fulton. In chapter 8 we have a proposition called ""Residue Theorem"" RESIDUE THEOREM. Let be as above ( is a projective plane curve, , is a non singular model of ). Suppose and are effective divisors on , with . Suppose is an adjoint of degree such that for some effective divisor . Then there is an adjoint of degree such that . I was wondering if this has anything to do with the Residue Theorem in complex analysis. Can anyone help me?","C,E C E = \sum_{Q\in X} (m_{(f(Q)} - 1)\, Q X C D D' X D'\equiv D G m \operatorname{div}(G) = D + E + A A G' m \operatorname{div}(G') = D' + E + A","['complex-analysis', 'algebraic-geometry']"
70,Expressing the area of the image of a holomorphic function by the coefficients of its expansion,Expressing the area of the image of a holomorphic function by the coefficients of its expansion,,"I have the following problem. Let $f:D\to \mathbb C$ be a holomorphic function, where $D=\{z:|z|\leq 1\}.$ Let $$f(z)=\sum_{n=0}^\infty c_nz^n.$$ Let $l_2(A)$ denote the Lebesgue measure of a set $A\subseteq \mathbb C$ and $G=f(D).$ Prove that $$l_2(G)=\pi\sum_{n=1}^\infty n|c_n|^2.$$ After a long struggle I managed to come up with the following formula $$l_2(G)=\iint_D |f\,'(z)|^2dxdy.$$ It looks like the right thing to use because $$f\,'(z)=\sum_{n=1}^\infty nc_nz^{n-1},$$ which is similar to what I have to prove. I understand that the $\pi$ will appear when I integrate something over the angle $\phi$ in polar coordinates. But I don't know how to find the square of the absolute value of the right-hand side to even start integrating... EDIT: The function is supposed to be univalent (one-to-one). (I'm not perfectly sure I'm translating the term correctly. The (Polish) word in the statement of the problem was ""jednolistna"". I have not met it before.)","I have the following problem. Let $f:D\to \mathbb C$ be a holomorphic function, where $D=\{z:|z|\leq 1\}.$ Let $$f(z)=\sum_{n=0}^\infty c_nz^n.$$ Let $l_2(A)$ denote the Lebesgue measure of a set $A\subseteq \mathbb C$ and $G=f(D).$ Prove that $$l_2(G)=\pi\sum_{n=1}^\infty n|c_n|^2.$$ After a long struggle I managed to come up with the following formula $$l_2(G)=\iint_D |f\,'(z)|^2dxdy.$$ It looks like the right thing to use because $$f\,'(z)=\sum_{n=1}^\infty nc_nz^{n-1},$$ which is similar to what I have to prove. I understand that the $\pi$ will appear when I integrate something over the angle $\phi$ in polar coordinates. But I don't know how to find the square of the absolute value of the right-hand side to even start integrating... EDIT: The function is supposed to be univalent (one-to-one). (I'm not perfectly sure I'm translating the term correctly. The (Polish) word in the statement of the problem was ""jednolistna"". I have not met it before.)",,"['complex-analysis', 'integration', 'power-series', 'analyticity']"
71,Zeroes of some degree of two elliptic functions,Zeroes of some degree of two elliptic functions,,"Let $\tau \in {\mathbb C}$ with $\mathrm{Im} \tau > 0$ , $a,b \in {\mathbb Q}$ not both integers (it's not clear to me whether assuming only $a,b \in {\mathbb R}$ will make a difference to the question or not), $\Lambda \subset {\mathbb C}$ the lattice generated by $1$ and $\tau$ and $\eta_1$ , $\eta_2$ the quasi-periods of the Weierstrass $\zeta$ function $-\int \wp$ corresponding to $\Lambda$ ( see e.g. https://en.wikipedia.org/wiki/Weierstrass_functions ), i.e. the values of the Weierstrass eta function at $1$ and $\tau$ . While doing some work in algebraic geometry, I've come across the following degree $2$ elliptic function \begin{equation*} \Upsilon(z) = \frac{1}{z(z-a-b\tau)} + \sum_{\omega \in \Lambda \backslash \{0\}} \left[ \frac{1}{(z-\omega)(z-\omega-a-b\tau)} - \frac{1}{\omega^2} \right] - \frac{a \eta_1 + b \eta_2}{a+ b \tau} \end{equation*} Yes, this is quite similar to $\wp$ . Since I know nothing about complex analysis, I'd appreaciate any help ""identifying"" this function -- Does it have a name, can it be expressed in particularly simple way in terms of other functions. Has it appeared anywhere else? It's extremely possible that I'm missing something simple. What I actually need to know about this function is related to its zeroes. Clearly, it has poles at $0$ and $a+b\tau$ , so we know the sum of the zeroes. Can its zeroes be computed? Is it easier than for $\wp$ ? http://people.mpim-bonn.mpg.de/zagier/files/doi/10.1007/BF01453974/fulltext.pdf (I'm under the impression that $\sum_{n \geq 0} \frac{1}{(an+b)^2}$ is harder than the other $\sum_{n \geq 0} \frac{1}{(an+b)(an+c)}$ , so maybe that's not so unlikely.) P.S. If anyone is curious, the algebraic geometry calculation I was doing was taking place on a geometrically ruled surface over an elliptic curve $E$ , specifically, the projectivization of the rank two bundle ${\mathcal E}$ which fits in a nonsplit s.e.s. $0 \to {\mathcal O}_E \to {\mathcal E} \to {\mathcal O}_E \to 0$ .","Let with , not both integers (it's not clear to me whether assuming only will make a difference to the question or not), the lattice generated by and and , the quasi-periods of the Weierstrass function corresponding to ( see e.g. https://en.wikipedia.org/wiki/Weierstrass_functions ), i.e. the values of the Weierstrass eta function at and . While doing some work in algebraic geometry, I've come across the following degree elliptic function Yes, this is quite similar to . Since I know nothing about complex analysis, I'd appreaciate any help ""identifying"" this function -- Does it have a name, can it be expressed in particularly simple way in terms of other functions. Has it appeared anywhere else? It's extremely possible that I'm missing something simple. What I actually need to know about this function is related to its zeroes. Clearly, it has poles at and , so we know the sum of the zeroes. Can its zeroes be computed? Is it easier than for ? http://people.mpim-bonn.mpg.de/zagier/files/doi/10.1007/BF01453974/fulltext.pdf (I'm under the impression that is harder than the other , so maybe that's not so unlikely.) P.S. If anyone is curious, the algebraic geometry calculation I was doing was taking place on a geometrically ruled surface over an elliptic curve , specifically, the projectivization of the rank two bundle which fits in a nonsplit s.e.s. .","\tau \in {\mathbb C} \mathrm{Im} \tau > 0 a,b \in {\mathbb Q} a,b \in {\mathbb R} \Lambda \subset {\mathbb C} 1 \tau \eta_1 \eta_2 \zeta -\int \wp \Lambda 1 \tau 2 \begin{equation*}
\Upsilon(z) = \frac{1}{z(z-a-b\tau)} + \sum_{\omega \in \Lambda \backslash \{0\}} \left[ \frac{1}{(z-\omega)(z-\omega-a-b\tau)} - \frac{1}{\omega^2} \right] - \frac{a \eta_1 + b \eta_2}{a+ b \tau}
\end{equation*} \wp 0 a+b\tau \wp \sum_{n \geq 0} \frac{1}{(an+b)^2} \sum_{n \geq 0} \frac{1}{(an+b)(an+c)} E {\mathcal E} 0 \to {\mathcal O}_E \to {\mathcal E} \to {\mathcal O}_E \to 0","['complex-analysis', 'algebraic-geometry', 'elliptic-functions']"
72,Confusion with the formula for harmonic conjugate,Confusion with the formula for harmonic conjugate,,"According to here , the harmonic conjugate of a harmonic function $u$ is given by $$v(z)=\int_{z_0}^z u_xdy-u_ydx+C$$ where $C$ is a constant, while in here , the harmonic conjugate is given by $$v(z)=\int u_xdy-\int u_ydx-\iint u_{xx}dxdy$$ where the integral is an indefinite integral. The last term above is no way a constant (since it is an indefinite integral). I am wondering whether there is any relation between the line integral and the indefinite integral. In conclusion, my question is: why are the above two formulas equivalent?","According to here , the harmonic conjugate of a harmonic function is given by where is a constant, while in here , the harmonic conjugate is given by where the integral is an indefinite integral. The last term above is no way a constant (since it is an indefinite integral). I am wondering whether there is any relation between the line integral and the indefinite integral. In conclusion, my question is: why are the above two formulas equivalent?",u v(z)=\int_{z_0}^z u_xdy-u_ydx+C C v(z)=\int u_xdy-\int u_ydx-\iint u_{xx}dxdy,"['complex-analysis', 'harmonic-functions']"
73,"Suppose $z = \cos θ + i \sin θ$. If $n$ is an integer, evaluate $z^n + \bar{z}^n$ and $z^n − \bar{z}^n $.","Suppose . If  is an integer, evaluate  and .",z = \cos θ + i \sin θ n z^n + \bar{z}^n z^n − \bar{z}^n ,"Suppose $z = \cos θ + i \sin θ$ . If $n$ is an integer, evaluate $z^n + \bar{z}^n$ and $z^n − \bar{z}^n $ . My attempt: Let $z\in \mathbb{C}$ such that $z=\cos\theta +i\sin\theta$ then $\bar{z}=\cos\theta - i\sin\theta$ Then, using Mouvre Form, we have: $z^n+\bar{z}^n=(\cos\theta +i\sin\theta)^n+(\cos\theta -i\sin\theta)^n=(\cos n\theta+i\sin n\theta)+(\cos n\theta-i\sin n\theta)=2\cos n\theta.$ Analogous: $ z^n-\bar{z}^n=(\cos\theta +i\sin\theta)^n-(\cos\theta -i\sin\theta)^n=(\cos n\theta+i\sin n\theta)-(\cos n\theta-i\sin n\theta)=2i\sin n\theta$ Is this correct?","Suppose . If is an integer, evaluate and . My attempt: Let such that then Then, using Mouvre Form, we have: Analogous: Is this correct?",z = \cos θ + i \sin θ n z^n + \bar{z}^n z^n − \bar{z}^n  z\in \mathbb{C} z=\cos\theta +i\sin\theta \bar{z}=\cos\theta - i\sin\theta z^n+\bar{z}^n=(\cos\theta +i\sin\theta)^n+(\cos\theta -i\sin\theta)^n=(\cos n\theta+i\sin n\theta)+(\cos n\theta-i\sin n\theta)=2\cos n\theta.  z^n-\bar{z}^n=(\cos\theta +i\sin\theta)^n-(\cos\theta -i\sin\theta)^n=(\cos n\theta+i\sin n\theta)-(\cos n\theta-i\sin n\theta)=2i\sin n\theta,['complex-analysis']
74,The ordinary generating function for the square-free kernel: reference request about singularities and its phase plot,The ordinary generating function for the square-free kernel: reference request about singularities and its phase plot,,"Let $n\geq 1$ an integer, in this post I denote the product of distinct prime numbers dividing dividing $n$ as $$\operatorname{rad}(n)=\prod_{\substack{p\mid n\\p\text{ prime}}}p,$$ is the famous arithmetic function that appears in the abc conjecture. See it you want the Wikipedia Radical of an integer Claim. It's easy to prove that the ordinary generating function $$f(z)=\sum_{n=1}^\infty \operatorname{rad}(n)z^n\tag{1}$$ for the square-free kernel or radica of $n$ has radius de convergence $1$. Proof. It's obvious the inequality $1\leq \operatorname{rad}(n)\leq n$ thus $1\leq (\operatorname{rad}(n))^{1/n}\leq n^{1/n}$, and from here squeeze theorem implies $(\operatorname{rad}(n))^{1/n}\to 1$. And the Cauchy–Hadamard theorem that $R=1$.$\square$ Question 1. I'm curious about what standard claims/questions, if any, can be stated about the singularities of a the generating function $f(z)$ over $|z|=1$. That is, imagine that we want to study the singularities of $f(z)$ over $|z|=1$, what questions can be studied? Many thanks. Thus I'm asking what standard questions should can be studied being those potentially interesting. Aren't required deduction, only are required some details about what topics/issues can be studied about the set of singularities of $f(z)$ on the set of complex numbers $|z|=1$. If you know it, or more advanced questions from the literature refer it answering this Question 1 as a reference request, and I try to find and read the theory about previous generating function and its singularities from the literature. I wondered previous and next question searching information about the generating function of the greatest prime factor (that is a different arithmetic function)   in Internet that I found [1]. Question 2 (Optional). I would like to know* the phase plot for previous ordinary generating function $$f(z)=\sum_{n=1}^\infty \operatorname{rad}(n)z^n$$ in the same spirit that is showed in the first plot of the section Greatest Prime Factor from Linas' Mathematical Art Gallery [1]. Can you provide us, or do you know it from the literature, the phase plot for $(1)$ over $|z|\leq 1$? Many thanks. I would like to see the plot (feel free if you want to add some mathematical details about how calculate it) with the purpose to know it. References: [1] Greatest Prime Factor , from Linas' Mathematical Art Gallery, home page of Lina Vepstas (2016).","Let $n\geq 1$ an integer, in this post I denote the product of distinct prime numbers dividing dividing $n$ as $$\operatorname{rad}(n)=\prod_{\substack{p\mid n\\p\text{ prime}}}p,$$ is the famous arithmetic function that appears in the abc conjecture. See it you want the Wikipedia Radical of an integer Claim. It's easy to prove that the ordinary generating function $$f(z)=\sum_{n=1}^\infty \operatorname{rad}(n)z^n\tag{1}$$ for the square-free kernel or radica of $n$ has radius de convergence $1$. Proof. It's obvious the inequality $1\leq \operatorname{rad}(n)\leq n$ thus $1\leq (\operatorname{rad}(n))^{1/n}\leq n^{1/n}$, and from here squeeze theorem implies $(\operatorname{rad}(n))^{1/n}\to 1$. And the Cauchy–Hadamard theorem that $R=1$.$\square$ Question 1. I'm curious about what standard claims/questions, if any, can be stated about the singularities of a the generating function $f(z)$ over $|z|=1$. That is, imagine that we want to study the singularities of $f(z)$ over $|z|=1$, what questions can be studied? Many thanks. Thus I'm asking what standard questions should can be studied being those potentially interesting. Aren't required deduction, only are required some details about what topics/issues can be studied about the set of singularities of $f(z)$ on the set of complex numbers $|z|=1$. If you know it, or more advanced questions from the literature refer it answering this Question 1 as a reference request, and I try to find and read the theory about previous generating function and its singularities from the literature. I wondered previous and next question searching information about the generating function of the greatest prime factor (that is a different arithmetic function)   in Internet that I found [1]. Question 2 (Optional). I would like to know* the phase plot for previous ordinary generating function $$f(z)=\sum_{n=1}^\infty \operatorname{rad}(n)z^n$$ in the same spirit that is showed in the first plot of the section Greatest Prime Factor from Linas' Mathematical Art Gallery [1]. Can you provide us, or do you know it from the literature, the phase plot for $(1)$ over $|z|\leq 1$? Many thanks. I would like to see the plot (feel free if you want to add some mathematical details about how calculate it) with the purpose to know it. References: [1] Greatest Prime Factor , from Linas' Mathematical Art Gallery, home page of Lina Vepstas (2016).",,"['complex-analysis', 'reference-request']"
75,Complex Analysis Book: Conway vs Lang,Complex Analysis Book: Conway vs Lang,,"I want to start stuyding complex analysis on a graduate level on my own (self study). I'm having trouble with diciding which one of the following books to use: Serge Lang's Complex Analysis or John. B. Conway's Functions of One Complex Variable. Here's the first part of the contents of both books: As you can see, Conway has a chapter on Metric Spaces and the Topology of $\Bbb C$, and starts with power series in the third chapter, while Lang does it directly in the second chapter. Which of these two books do you guys recommend me and why?","I want to start stuyding complex analysis on a graduate level on my own (self study). I'm having trouble with diciding which one of the following books to use: Serge Lang's Complex Analysis or John. B. Conway's Functions of One Complex Variable. Here's the first part of the contents of both books: As you can see, Conway has a chapter on Metric Spaces and the Topology of $\Bbb C$, and starts with power series in the third chapter, while Lang does it directly in the second chapter. Which of these two books do you guys recommend me and why?",,"['complex-analysis', 'book-recommendation']"
76,Multiple roots of one polynomial of degree 4,Multiple roots of one polynomial of degree 4,,"I have the following specific polynomial of degree 4: \begin{align}  P(x) &= (x_1^2 + x_2^2)^2 + a(x_1 + i x_2)^2(x_1^2 + x_2^2) + \overline{a}  (x_1-ix_2)^2(x_1^2 + x_2^2)\\ &+b(x_1+ix_2)^4 + \overline{b}(x_1-ix_2)^4, \, x=(x_1,x_2)\in \mathbb{R}^2. \end{align} In addition, I know that coefficients $a,b$ are such that  \begin{equation}  P(x) \geq c(x_1^2+x^2_2)^2 , \, c > 0. \end{equation} I have a conjecture that $P$ must have only simple (complex) roots. For example, if one will look for the similar polynomial of degree 2, i.e.: \begin{align} &Q(x) = (x_1^2 + x_2^2) + a(x_1 + i x_2)^2 + \overline{a}  (x_1-ix_2)^2,\\ &Q(x) \geq c(x_1^2 + x_2^2) \end{align} then one can directly show that the corresponding roots will be only simple. And condition of positivity is used to have that the determinant is not zero.","I have the following specific polynomial of degree 4: \begin{align}  P(x) &= (x_1^2 + x_2^2)^2 + a(x_1 + i x_2)^2(x_1^2 + x_2^2) + \overline{a}  (x_1-ix_2)^2(x_1^2 + x_2^2)\\ &+b(x_1+ix_2)^4 + \overline{b}(x_1-ix_2)^4, \, x=(x_1,x_2)\in \mathbb{R}^2. \end{align} In addition, I know that coefficients $a,b$ are such that  \begin{equation}  P(x) \geq c(x_1^2+x^2_2)^2 , \, c > 0. \end{equation} I have a conjecture that $P$ must have only simple (complex) roots. For example, if one will look for the similar polynomial of degree 2, i.e.: \begin{align} &Q(x) = (x_1^2 + x_2^2) + a(x_1 + i x_2)^2 + \overline{a}  (x_1-ix_2)^2,\\ &Q(x) \geq c(x_1^2 + x_2^2) \end{align} then one can directly show that the corresponding roots will be only simple. And condition of positivity is used to have that the determinant is not zero.",,"['complex-analysis', 'algebraic-geometry', 'polynomials']"
77,Precise definition of a Riemann surface associated to a function,Precise definition of a Riemann surface associated to a function,,"$\newcommand{\C}{\mathbb{C}}$ Let $\Omega \subset \C$ be an open and $f: \Omega \to \C$ be a meromorphic function.  I want a precise definition of THE Riemann surface associated to $f$(and some uniqueness statement). Cause for my confusion:  In every book that I can find, I have seen examples of Riemann surfaces associated to a function but no precise definition. Here is my first stab at a definition. A Riemann surface for $f$ is a pair $(X,F)$, where $X$ is a Riemann surface, $F: X \to \C$ is meromorphic and $F|_U=f$ for some chart and coordinates $U \subset X$. Obviously the Riemann surface for $f$ is not unique in any sense right now. The uniqueness statement that I want is something like Given $(X,F)$, $(Y,G)$ as above, then there is an isomorphism $X \cong Y$ such that the following diagram commutes $\require{AMScd}$ $\begin{CD} X @>F>> \C\\ @| @|\\ Y @>G>> \C \end{CD} $ Is this too much to ask?  And is there a definition of the Riemann surface associated to a function so that I can get such a uniqueness statement?","$\newcommand{\C}{\mathbb{C}}$ Let $\Omega \subset \C$ be an open and $f: \Omega \to \C$ be a meromorphic function.  I want a precise definition of THE Riemann surface associated to $f$(and some uniqueness statement). Cause for my confusion:  In every book that I can find, I have seen examples of Riemann surfaces associated to a function but no precise definition. Here is my first stab at a definition. A Riemann surface for $f$ is a pair $(X,F)$, where $X$ is a Riemann surface, $F: X \to \C$ is meromorphic and $F|_U=f$ for some chart and coordinates $U \subset X$. Obviously the Riemann surface for $f$ is not unique in any sense right now. The uniqueness statement that I want is something like Given $(X,F)$, $(Y,G)$ as above, then there is an isomorphism $X \cong Y$ such that the following diagram commutes $\require{AMScd}$ $\begin{CD} X @>F>> \C\\ @| @|\\ Y @>G>> \C \end{CD} $ Is this too much to ask?  And is there a definition of the Riemann surface associated to a function so that I can get such a uniqueness statement?",,"['complex-analysis', 'complex-geometry']"
78,"Study of a "" flow ""","Study of a "" flow """,,"First, sorry if my english does not correspond to the real/formal worlds that I should use for this domain in which i'm new. (logarithm in complex analysis and potential/flow) I know I can only ask for one question but here it is a part of a whole problem and I also want to know if my reasoning is good so ... I'm asked a small problem to study a potential defined by a function (I dont know what is $a$ ... ) $$ \Phi\left(z\right)=\ln\left(\frac{z-a}{z+a}\right) $$ in which I guess ln is defined on $\mathbb{C} \setminus \mathbb{R}^{-}$ by $$ \ln\left(z\right)=\ln\left(\left|z\right|\right)+i\text{arg}\left(z\right)=\ln\left(\rho\right)+i\theta $$ 1] Find the domain $D$ where $\Psi$ is holomorphic and explicit $\Phi'$. 2] Write the velocity field (  which is $\mathscr{C}^{\infty}$ ) of this flow. Is it possible to define it on an open set which contains strictly $D$ ? 3] Show that the flows lines are given by circular wedges which extremities are $a$ and $-a$. 4] With $\gamma^{+}$ a Jordan contour, discuss of the value of the integral $$ \int_{\gamma^{+}}\overline{V}\left(z\right) \text{d}z $$ where $\overline{V}$ is the conjugate of $V$. My attempt is to find where $\displaystyle z \mapsto \frac{z-a}{z+a}$ is a negative real. With $z=x+iy$ i've found that $x$ and $y$ verify $$ \frac{z-a}{z+a}=\frac{x^2+y^2-a^2+iya}{x^2+2ax+a^2y^2} $$ Hence $y$ needs to be $0$ in order to be a real ( or $a=i \alpha$ ) Hence it is a negative real iif $x^2+y^2 \leq a^2$ then it would be defined on $\mathbb{C}$ minus a circle centered in $(0,0)$ of radius $a$ and  for $z \in D$ $$ \Phi'\left(z\right)=\frac{z+a-z+a}{\left(z+a\right)^2}\frac{z+a}{z-a}=\frac{2z}{\left(z+a\right)\left(z-a\right)} $$ Should I calculate  $$v_{x}=\frac{\partial \Psi}{\partial y}\left(x,y\right) \text{ and }v_{y}=-\frac{\partial \Psi}{\partial x}\left(x,y\right) $$ $$ \frac{z-a}{z+a}=K \Leftrightarrow z=a\frac{1+K}{1-K} $$ What can I conclude ? $z=cst \ \Rightarrow$ $x$ and $y \ \Rightarrow $ $\left|z\right|$ is constant ? No idea of how to proceed. Any ideas ? Thanks !","First, sorry if my english does not correspond to the real/formal worlds that I should use for this domain in which i'm new. (logarithm in complex analysis and potential/flow) I know I can only ask for one question but here it is a part of a whole problem and I also want to know if my reasoning is good so ... I'm asked a small problem to study a potential defined by a function (I dont know what is $a$ ... ) $$ \Phi\left(z\right)=\ln\left(\frac{z-a}{z+a}\right) $$ in which I guess ln is defined on $\mathbb{C} \setminus \mathbb{R}^{-}$ by $$ \ln\left(z\right)=\ln\left(\left|z\right|\right)+i\text{arg}\left(z\right)=\ln\left(\rho\right)+i\theta $$ 1] Find the domain $D$ where $\Psi$ is holomorphic and explicit $\Phi'$. 2] Write the velocity field (  which is $\mathscr{C}^{\infty}$ ) of this flow. Is it possible to define it on an open set which contains strictly $D$ ? 3] Show that the flows lines are given by circular wedges which extremities are $a$ and $-a$. 4] With $\gamma^{+}$ a Jordan contour, discuss of the value of the integral $$ \int_{\gamma^{+}}\overline{V}\left(z\right) \text{d}z $$ where $\overline{V}$ is the conjugate of $V$. My attempt is to find where $\displaystyle z \mapsto \frac{z-a}{z+a}$ is a negative real. With $z=x+iy$ i've found that $x$ and $y$ verify $$ \frac{z-a}{z+a}=\frac{x^2+y^2-a^2+iya}{x^2+2ax+a^2y^2} $$ Hence $y$ needs to be $0$ in order to be a real ( or $a=i \alpha$ ) Hence it is a negative real iif $x^2+y^2 \leq a^2$ then it would be defined on $\mathbb{C}$ minus a circle centered in $(0,0)$ of radius $a$ and  for $z \in D$ $$ \Phi'\left(z\right)=\frac{z+a-z+a}{\left(z+a\right)^2}\frac{z+a}{z-a}=\frac{2z}{\left(z+a\right)\left(z-a\right)} $$ Should I calculate  $$v_{x}=\frac{\partial \Psi}{\partial y}\left(x,y\right) \text{ and }v_{y}=-\frac{\partial \Psi}{\partial x}\left(x,y\right) $$ $$ \frac{z-a}{z+a}=K \Leftrightarrow z=a\frac{1+K}{1-K} $$ What can I conclude ? $z=cst \ \Rightarrow$ $x$ and $y \ \Rightarrow $ $\left|z\right|$ is constant ? No idea of how to proceed. Any ideas ? Thanks !",,"['complex-analysis', 'vector-fields']"
79,function ${\displaystyle \varphi }$ such that ${\displaystyle \varphi (\varphi (u))=\exp(u)}$,function  such that,{\displaystyle \varphi } {\displaystyle \varphi (\varphi (u))=\exp(u)},"Here it's cited: the existence of the holomorphic function ${\displaystyle \varphi }$  such that ${\displaystyle \varphi (\varphi (u))=\exp(u)}$ had been demonstrated in 1950 by Hellmuth Kneser. However I can't find the definition of the function $\varphi(u)$ anywhere. Does this function truly exist? And if so, What is the function?","Here it's cited: the existence of the holomorphic function ${\displaystyle \varphi }$  such that ${\displaystyle \varphi (\varphi (u))=\exp(u)}$ had been demonstrated in 1950 by Hellmuth Kneser. However I can't find the definition of the function $\varphi(u)$ anywhere. Does this function truly exist? And if so, What is the function?",,"['calculus', 'exponential-function', 'chain-rule', 'holomorphic-functions', 'tetration']"
80,Solutions of a complex function with fractional powers,Solutions of a complex function with fractional powers,,"Would anyone know how to calculate the values of the complex variable $s$ such as $(\frac{s}{\omega_b}+1)^n(\frac{s}{\omega_h}+1)^{1-n}=C_0$ with $\omega_b \in \mathbb{R}$, $\omega_h \in \mathbb{R}$, $\omega_b <\omega_h$ $n \in \mathbb{R}$  and  $0<n<1$ $C_0 \in \mathbb{C}$ Thank you very much for your kind help.","Would anyone know how to calculate the values of the complex variable $s$ such as $(\frac{s}{\omega_b}+1)^n(\frac{s}{\omega_h}+1)^{1-n}=C_0$ with $\omega_b \in \mathbb{R}$, $\omega_h \in \mathbb{R}$, $\omega_b <\omega_h$ $n \in \mathbb{R}$  and  $0<n<1$ $C_0 \in \mathbb{C}$ Thank you very much for your kind help.",,['complex-analysis']
81,"an ""algebraic relation"" by any other name...?","an ""algebraic relation"" by any other name...?",,"I am confronted with this definition which is unfamiliar to me: Suppose $F\subset \mathbb C\times \mathbb C$ is a relation. $F$ is said to be algebraic if there exists a matrix $A\in M_{n+1}(\mathbb C)$ such that no row of $A$ is the zero vector, and $$(x,y)\in F\iff \sum_{p=1}^{n+1}\sum_{q=1}^{n+1}A_{pq}y^{p-1}x^{q-1}=0$$ I'd like to find out more about it. I've been trying to find this in other texts without much luck, and hampered by the obvious problem of only having ""algebraic relation"" as a search term. This appeared in the context of a text on analytic functions, at the end of a chapter about analytic relations, analytic continuation, and branch-points. It might be useful to provide the remaining context provided by the book (there isn't much!) The definition above is only used for three following exercises, and the book does not go any further into this thing. The exercises are: If $F$ is an algebraic relation, show $F^{-1}$ is algebraic also. If $g$ is an analytic function and $g\subseteq F$ , then $F$ contains every analytic relation extending $g$ . If $G$ is an analytic relation contained in an algebraic relation, then $G'$ (the derivative of $G$ ) is also contained in an algebraic relation. That's it... I think that's all that is mentioned. Can someone point me to other texts where this idea appears and/or give other terminology that might lead me to see more about this?","I am confronted with this definition which is unfamiliar to me: Suppose is a relation. is said to be algebraic if there exists a matrix such that no row of is the zero vector, and I'd like to find out more about it. I've been trying to find this in other texts without much luck, and hampered by the obvious problem of only having ""algebraic relation"" as a search term. This appeared in the context of a text on analytic functions, at the end of a chapter about analytic relations, analytic continuation, and branch-points. It might be useful to provide the remaining context provided by the book (there isn't much!) The definition above is only used for three following exercises, and the book does not go any further into this thing. The exercises are: If is an algebraic relation, show is algebraic also. If is an analytic function and , then contains every analytic relation extending . If is an analytic relation contained in an algebraic relation, then (the derivative of ) is also contained in an algebraic relation. That's it... I think that's all that is mentioned. Can someone point me to other texts where this idea appears and/or give other terminology that might lead me to see more about this?","F\subset \mathbb C\times \mathbb C F A\in M_{n+1}(\mathbb C) A (x,y)\in F\iff \sum_{p=1}^{n+1}\sum_{q=1}^{n+1}A_{pq}y^{p-1}x^{q-1}=0 F F^{-1} g g\subseteq F F g G G' G","['complex-analysis', 'terminology', 'relations']"
82,Why does this function have an analytic continuation and how can we compute it?,Why does this function have an analytic continuation and how can we compute it?,,"For $q \in \Bbb C$, let $f_q(x) = \frac {3x + q(1-x)}{1 - q(1-x)}$ and for $n \in \Bbb N$, let $F_n(q) = (f_1 \circ f_q \circ f_{q^2} \circ \cdots \circ f_{q^n}) (0)$ For $|q| \notin \{ 1, \frac 13 \}$, the sequence $(F_n(q))$ converges to some $F(q) \in \Bbb P^1(\Bbb C)$, and it looks like $F$ is meromorphic there (should not be too difficult to prove) Here is a picture of $F_{100}$ : (surprisingly, it looks meromorphic on most of the unit circle, save some essential singularities, but it's almost surely deceiving. There should be a necklace of zeros and poles around the circle, but you don't see them because they are so close together) I am more interested in what happens for $|q| < \frac 13$, where $f_{q^n}$ converges too fast to the homothety $x \mapsto 3x$ for the argument $0$ to escape very far. Looking at the picture, it looks very continuable. Thankfully, I picked some coefficients so that the Laurent series of $F_n$ at $0$ converge in $\Bbb Z((q))$, which gives a first way to see past the $|q| = \frac 13$ boundary. Using the series $F(q) = \frac 1q(1-2q+4q^2-6q^3+20q^4-46q^5+\cdots)$ and truncating after a hundred terms, I get another picture The radius of convergence here seems to be around $0.41$, and it doesn't look any less well-behaved except for possibly a pole at $q = -0.41$. Pre and post composing the function with carefully chosen functions can also extend this even further. So the ""natural"" boundary of $|q|= \frac 13 $ isn't really one, and still it looks like it could be extended further, possibly up to the whole open disk of radius $1$. So, what gives ? Is there an alternative way of computing the extension in the annulus $\frac 13 \le |q| < 0.41$ that would explain it and gives a further continuation ?","For $q \in \Bbb C$, let $f_q(x) = \frac {3x + q(1-x)}{1 - q(1-x)}$ and for $n \in \Bbb N$, let $F_n(q) = (f_1 \circ f_q \circ f_{q^2} \circ \cdots \circ f_{q^n}) (0)$ For $|q| \notin \{ 1, \frac 13 \}$, the sequence $(F_n(q))$ converges to some $F(q) \in \Bbb P^1(\Bbb C)$, and it looks like $F$ is meromorphic there (should not be too difficult to prove) Here is a picture of $F_{100}$ : (surprisingly, it looks meromorphic on most of the unit circle, save some essential singularities, but it's almost surely deceiving. There should be a necklace of zeros and poles around the circle, but you don't see them because they are so close together) I am more interested in what happens for $|q| < \frac 13$, where $f_{q^n}$ converges too fast to the homothety $x \mapsto 3x$ for the argument $0$ to escape very far. Looking at the picture, it looks very continuable. Thankfully, I picked some coefficients so that the Laurent series of $F_n$ at $0$ converge in $\Bbb Z((q))$, which gives a first way to see past the $|q| = \frac 13$ boundary. Using the series $F(q) = \frac 1q(1-2q+4q^2-6q^3+20q^4-46q^5+\cdots)$ and truncating after a hundred terms, I get another picture The radius of convergence here seems to be around $0.41$, and it doesn't look any less well-behaved except for possibly a pole at $q = -0.41$. Pre and post composing the function with carefully chosen functions can also extend this even further. So the ""natural"" boundary of $|q|= \frac 13 $ isn't really one, and still it looks like it could be extended further, possibly up to the whole open disk of radius $1$. So, what gives ? Is there an alternative way of computing the extension in the annulus $\frac 13 \le |q| < 0.41$ that would explain it and gives a further continuation ?",,"['complex-analysis', 'modular-forms', 'continued-fractions']"
83,complex-valued continuous function on $\mathbb{D}$.,complex-valued continuous function on .,\mathbb{D},"Would anyone mind providing a hint for the following exercise: Assume $f$ is continuous on the unit disk $\mathbb{D}$  and $\text{Re}(\overline{z}f(z)) > 0$ for all $|z| = 1$. Show that $f(z) = 0$ for some $z$ in the disk. I attempted using convexity of the disk, but this didn't lead me very far.","Would anyone mind providing a hint for the following exercise: Assume $f$ is continuous on the unit disk $\mathbb{D}$  and $\text{Re}(\overline{z}f(z)) > 0$ for all $|z| = 1$. Show that $f(z) = 0$ for some $z$ in the disk. I attempted using convexity of the disk, but this didn't lead me very far.",,"['complex-analysis', 'analysis']"
84,Is Frobenius' method generally useful?,Is Frobenius' method generally useful?,,"Regarding the differential equation $$ y'' + p(z)y' + q(z)y = 0,\quad z\in\mathbb{C}, $$ we can find solutions of the form  $$ \sum_{n=0}^\infty c_n (z-z_0)^n, \quad c_n\in\mathbb{C}, $$ given that $p(z)$ and $q(z)$ are analytic in $z=z_0$. Here $|z-z_0|<R_1$ for some $R_1>0$ must hold. Suppose $p(z)$ and $q(z)$ aren't analytic at $z=z_0$, but $(z-z_0)p(z)$ and $(z-z_0)^2 q(z)$ are, then we can find solutions of the form  $$ \sum_{n=0}^\infty c_n (z-z_0)^{n+r}, \quad c_n\in\mathbb{C}, $$ where $r$ satisfies $r(r-1)+[zp(z)]_{z=z_0}r+[z^2q(z)]_{z=z_0}=0$. Here $|z-z_0|<R_2$ for some $R_2>0$ must hold. The latter is Frobenius' method. Now to illustrate my question suppose $z_0=0$ is a singular point of $p(z)$ but not of $zp(z)$ so we would apply Frobenius' method. Then suppose it turns out that $R_2$ is really small.  In my understanding we could also have looked for solutions around some analytic point. Suppose $z_0=2$ is analytic, then we would've looked for solutions of the form $\sum_{n=0}^\infty c_n (z-2)^n$ and then maybe the corresponding $R_1$ would have turned out to be more satisfactory. So if what I said in the above is right, my question is: can we know beforehand if applying Frobenius' method is useful?","Regarding the differential equation $$ y'' + p(z)y' + q(z)y = 0,\quad z\in\mathbb{C}, $$ we can find solutions of the form  $$ \sum_{n=0}^\infty c_n (z-z_0)^n, \quad c_n\in\mathbb{C}, $$ given that $p(z)$ and $q(z)$ are analytic in $z=z_0$. Here $|z-z_0|<R_1$ for some $R_1>0$ must hold. Suppose $p(z)$ and $q(z)$ aren't analytic at $z=z_0$, but $(z-z_0)p(z)$ and $(z-z_0)^2 q(z)$ are, then we can find solutions of the form  $$ \sum_{n=0}^\infty c_n (z-z_0)^{n+r}, \quad c_n\in\mathbb{C}, $$ where $r$ satisfies $r(r-1)+[zp(z)]_{z=z_0}r+[z^2q(z)]_{z=z_0}=0$. Here $|z-z_0|<R_2$ for some $R_2>0$ must hold. The latter is Frobenius' method. Now to illustrate my question suppose $z_0=0$ is a singular point of $p(z)$ but not of $zp(z)$ so we would apply Frobenius' method. Then suppose it turns out that $R_2$ is really small.  In my understanding we could also have looked for solutions around some analytic point. Suppose $z_0=2$ is analytic, then we would've looked for solutions of the form $\sum_{n=0}^\infty c_n (z-2)^n$ and then maybe the corresponding $R_1$ would have turned out to be more satisfactory. So if what I said in the above is right, my question is: can we know beforehand if applying Frobenius' method is useful?",,"['complex-analysis', 'ordinary-differential-equations', 'frobenius-method']"
85,A holomorphic function with poles at all $(2n)!$,A holomorphic function with poles at all,(2n)!,"During a discussion, I was asking to find a function $f$ which is holomorphic on $$\mathbb{C}\setminus \{(2n)! : n \in \mathbb{N}\}.$$ Naturally, I wanted to set $$f(z) = \prod_{n=0}^{+\infty} \frac{1}{z-(2n)!}$$ but this product does not converge. (Remember that $\prod_n z_n$ is convergent if and only if $\sum_n |z_n-1|$ is convergent.) So finally, I set $$f(z) = \prod_{n=0}^{+\infty} \frac{(2n)!}{(2n)!-z}$$ which is holomorphic on the required domain. First, I'd like to know if someone has a better idea to find a holomorphic function on $\mathbb{C}\setminus \{(2n)! : n \in \mathbb{N}\}.$ (If possible with ""usual"" functions). I would also like to know if one can get interesting informations about the function $f$ I sat. For example, can we obtain its Taylor expansion on $D(0,1)$ ?","During a discussion, I was asking to find a function which is holomorphic on Naturally, I wanted to set but this product does not converge. (Remember that is convergent if and only if is convergent.) So finally, I set which is holomorphic on the required domain. First, I'd like to know if someone has a better idea to find a holomorphic function on (If possible with ""usual"" functions). I would also like to know if one can get interesting informations about the function I sat. For example, can we obtain its Taylor expansion on ?","f \mathbb{C}\setminus \{(2n)! : n \in \mathbb{N}\}. f(z) = \prod_{n=0}^{+\infty} \frac{1}{z-(2n)!} \prod_n z_n \sum_n |z_n-1| f(z) = \prod_{n=0}^{+\infty} \frac{(2n)!}{(2n)!-z} \mathbb{C}\setminus \{(2n)! : n \in \mathbb{N}\}. f D(0,1)","['complex-analysis', 'complex-numbers', 'taylor-expansion', 'holomorphic-functions']"
86,Black magic behind Padé approximation,Black magic behind Padé approximation,,"In condensed matter physics one usually calculates the so-called Matsubara Green's function in the set of discrete points $G (i \omega_n)$, where $$ \omega_n = (2n+1) \pi T $$ Physically significant Green's function, however, is retarded related as follows $$ G_R (\omega) = \lim_{\varepsilon \to 0} G (\omega + i \varepsilon) $$ assuming that the unique analytical continuation $G (z)$ exists (it seems it does) with the property $G (z) \equiv G_R (z)$ in the upper half-plane and $G (z) \equiv G_A (z)$ in the lower half-plane. The task usually is: given set of values $G (i \omega_n)$ (computed in some previous simulation), determine $G (z)$ (and all the other Green's functions, if that's one's desire). The underlying theorem about existence and uniqueness of such continuation is of Baym, Mermin in 1961 paper Determination of Thermodynamic Green's Functions in Journal of Mathematical Physics. The theorem, however useful, does not give the answer to ""how to practically obtain such continuation?"" What physicist do is the Pade approximation. Recipe would be: define the function $P (z)$ as follows $$ P (z) \equiv \frac{p_0 + p_1 z + \cdots + p_n z^n}{q_0 +q_1 z + \cdots + q_m z^m} $$ Now set up appropriate number of equations in the form $$ G (i \omega_0) = P (i \omega_0), \quad G (i \omega_1) = P (i \omega_1), \quad \cdots $$ Solve for the unknown coefficients $\left\{ p, q \right\}$. Plugging the real frequency with a small imaginary part into $P$ should yield the result for the retarded and advanced Green's function $$ G_R (\omega) = P (\omega + i 0^+), \quad G_A (\omega) = P (\omega - i 0^+) $$ And here is where the magic comes: it works. This might sound silly, but recently I've been playing with this in the following way. I made up a very ugly function $A(x)$ (e.g. sum of 20 Gaussians with random parameters). Then I pretended that this is the spectral function for some Green's function, so I calculated the Matsubara Green's function $$ G (i \omega_n) = \int \limits_{-\infty}^\infty \frac{\mathrm{d} \omega \, A (\omega)}{i \omega_n - \omega} $$ Then I calculated the Pade approximation $P (z)$, obtaining $G_R (\omega)$ and finally getting back my original function by the means of Sokhotskij-Plemelj theorem: $$ A (x) = - \frac{1}{\pi} \lim_{\varepsilon \to 0^+} G_R (\omega + i \varepsilon) $$ The ""data"" $G (i \omega_n)$ had to be calculated VERY precisely - at least 20 or 30 significant digits. The number of equations for the Pade approximation had to be at least 40 to reproduce the original function. What baffles and astounds me most is, that for every temperature, i.e. very small like $10^{-5}$, or big, like $1$, this worked, provided the data $G (i \omega_n)$ is precise enough and the number of equations is sufficient. This is quite strange, as for the very low temperatures, what you really approximate is very small portion of the Matsubara Green's function, imagine a function like $1/(1+x^2)$ and now take the first 50 Matsubara points, however at the temperature $10^{-5}$, so what you see is almost a constant. Yet the recipe works. Now imagine some large temperature, like $1$ and do the same. It still works. If someone wants to play with that, I share my Mathematica code with you. Copy it to your notebook and save the notebook somewhere. This generates some funny function $A(x)$ and plots it: n = 20; SetDirectory[NotebookDirectory[]]; sig = Table[RandomReal[] + 0.1, {i, 1, n}]; x0 = 4 Table[RandomReal[] - 0.5, {i, 1, n}]; h = 2 Table[RandomReal[] - 0.5, {i, 1, n}]; A[x_] := Sum[h[[i]] Exp[-((x - x0[[i]])^2/sig[[i]]^2)], {i, 1, n}] Plot[A[x], {x, -5, 5}, PlotRange -> {-5, 5}] The following code generates data for Matsubara Green's function at temperature $T$ T = 1/1000; w[l_] := (2 l +      1) Pi T; wmax = 1; k = 100; acc = 10; Monitor[MatsG =     Table[{w[l],       NIntegrate[A[x]/(       I w[l] - x), {x, -Infinity, Infinity},        WorkingPrecision -> acc, PrecisionGoal -> acc,        AccuracyGoal -> acc, MaxRecursion -> 500]}, {l, 1, 2 k}];, l] This creates the continuation GComplex (I'm using $m = n+1$, because Green's function must approach zero for $|z| \to \infty$ and I've set $q_m = 1$, so I don't make it an eigenvalue problem). Just keep the number acc high enough so Mathematica doesn't complain about the bad conditioned matrix. $2k$ is the number of equations, you can tweak that to see what it does: k = 30; acc = 300; ClearAll[p] f[w_] := Subscript[p, 0] + Sum[Subscript[p, j] w^j, {j, 1, k - 1}]; g[w_] := Subscript[q, 0] + Sum[Subscript[q, j] w^j, {j, 1, k - 1}] +     w^k; vars = Flatten@    Table[{Subscript[p, i], Subscript[q, i]}, {i, 0, k - 1}]; eqns = Table[    SetAccuracy[     f[I MatsG[[i + 1, 1]]] ==       MatsG[[i + 1, 2]] g[I MatsG[[i + 1, 1]]], acc], {i, 0, 2 k - 1}]; sol = Solve[eqns, vars, WorkingPrecision -> acc]; GComplex[w_] := f[w]/g[w] /. First@sol; And finally comparing $A(x)$ with the newly generated function: Plot[{A[x], -(1/Pi) Im[GComplex[x]]}, {x, -6, 6},   PlotRange -> {-5, 5}] I'm very sorry if this is considered an off-topic here, but I'm really surprised this works in every case of the trial function $A(x)$ I did this with and I want to know what theory is behind this black magic box. Does anyone know? Thank you.","In condensed matter physics one usually calculates the so-called Matsubara Green's function in the set of discrete points $G (i \omega_n)$, where $$ \omega_n = (2n+1) \pi T $$ Physically significant Green's function, however, is retarded related as follows $$ G_R (\omega) = \lim_{\varepsilon \to 0} G (\omega + i \varepsilon) $$ assuming that the unique analytical continuation $G (z)$ exists (it seems it does) with the property $G (z) \equiv G_R (z)$ in the upper half-plane and $G (z) \equiv G_A (z)$ in the lower half-plane. The task usually is: given set of values $G (i \omega_n)$ (computed in some previous simulation), determine $G (z)$ (and all the other Green's functions, if that's one's desire). The underlying theorem about existence and uniqueness of such continuation is of Baym, Mermin in 1961 paper Determination of Thermodynamic Green's Functions in Journal of Mathematical Physics. The theorem, however useful, does not give the answer to ""how to practically obtain such continuation?"" What physicist do is the Pade approximation. Recipe would be: define the function $P (z)$ as follows $$ P (z) \equiv \frac{p_0 + p_1 z + \cdots + p_n z^n}{q_0 +q_1 z + \cdots + q_m z^m} $$ Now set up appropriate number of equations in the form $$ G (i \omega_0) = P (i \omega_0), \quad G (i \omega_1) = P (i \omega_1), \quad \cdots $$ Solve for the unknown coefficients $\left\{ p, q \right\}$. Plugging the real frequency with a small imaginary part into $P$ should yield the result for the retarded and advanced Green's function $$ G_R (\omega) = P (\omega + i 0^+), \quad G_A (\omega) = P (\omega - i 0^+) $$ And here is where the magic comes: it works. This might sound silly, but recently I've been playing with this in the following way. I made up a very ugly function $A(x)$ (e.g. sum of 20 Gaussians with random parameters). Then I pretended that this is the spectral function for some Green's function, so I calculated the Matsubara Green's function $$ G (i \omega_n) = \int \limits_{-\infty}^\infty \frac{\mathrm{d} \omega \, A (\omega)}{i \omega_n - \omega} $$ Then I calculated the Pade approximation $P (z)$, obtaining $G_R (\omega)$ and finally getting back my original function by the means of Sokhotskij-Plemelj theorem: $$ A (x) = - \frac{1}{\pi} \lim_{\varepsilon \to 0^+} G_R (\omega + i \varepsilon) $$ The ""data"" $G (i \omega_n)$ had to be calculated VERY precisely - at least 20 or 30 significant digits. The number of equations for the Pade approximation had to be at least 40 to reproduce the original function. What baffles and astounds me most is, that for every temperature, i.e. very small like $10^{-5}$, or big, like $1$, this worked, provided the data $G (i \omega_n)$ is precise enough and the number of equations is sufficient. This is quite strange, as for the very low temperatures, what you really approximate is very small portion of the Matsubara Green's function, imagine a function like $1/(1+x^2)$ and now take the first 50 Matsubara points, however at the temperature $10^{-5}$, so what you see is almost a constant. Yet the recipe works. Now imagine some large temperature, like $1$ and do the same. It still works. If someone wants to play with that, I share my Mathematica code with you. Copy it to your notebook and save the notebook somewhere. This generates some funny function $A(x)$ and plots it: n = 20; SetDirectory[NotebookDirectory[]]; sig = Table[RandomReal[] + 0.1, {i, 1, n}]; x0 = 4 Table[RandomReal[] - 0.5, {i, 1, n}]; h = 2 Table[RandomReal[] - 0.5, {i, 1, n}]; A[x_] := Sum[h[[i]] Exp[-((x - x0[[i]])^2/sig[[i]]^2)], {i, 1, n}] Plot[A[x], {x, -5, 5}, PlotRange -> {-5, 5}] The following code generates data for Matsubara Green's function at temperature $T$ T = 1/1000; w[l_] := (2 l +      1) Pi T; wmax = 1; k = 100; acc = 10; Monitor[MatsG =     Table[{w[l],       NIntegrate[A[x]/(       I w[l] - x), {x, -Infinity, Infinity},        WorkingPrecision -> acc, PrecisionGoal -> acc,        AccuracyGoal -> acc, MaxRecursion -> 500]}, {l, 1, 2 k}];, l] This creates the continuation GComplex (I'm using $m = n+1$, because Green's function must approach zero for $|z| \to \infty$ and I've set $q_m = 1$, so I don't make it an eigenvalue problem). Just keep the number acc high enough so Mathematica doesn't complain about the bad conditioned matrix. $2k$ is the number of equations, you can tweak that to see what it does: k = 30; acc = 300; ClearAll[p] f[w_] := Subscript[p, 0] + Sum[Subscript[p, j] w^j, {j, 1, k - 1}]; g[w_] := Subscript[q, 0] + Sum[Subscript[q, j] w^j, {j, 1, k - 1}] +     w^k; vars = Flatten@    Table[{Subscript[p, i], Subscript[q, i]}, {i, 0, k - 1}]; eqns = Table[    SetAccuracy[     f[I MatsG[[i + 1, 1]]] ==       MatsG[[i + 1, 2]] g[I MatsG[[i + 1, 1]]], acc], {i, 0, 2 k - 1}]; sol = Solve[eqns, vars, WorkingPrecision -> acc]; GComplex[w_] := f[w]/g[w] /. First@sol; And finally comparing $A(x)$ with the newly generated function: Plot[{A[x], -(1/Pi) Im[GComplex[x]]}, {x, -6, 6},   PlotRange -> {-5, 5}] I'm very sorry if this is considered an off-topic here, but I'm really surprised this works in every case of the trial function $A(x)$ I did this with and I want to know what theory is behind this black magic box. Does anyone know? Thank you.",,"['complex-analysis', 'analytic-functions', 'analytic-continuation']"
87,What is the relationship between the asymptotic series outside the radius of convergence and the analytic continuation?,What is the relationship between the asymptotic series outside the radius of convergence and the analytic continuation?,,"In his post about computing the zeta function by smooth cutoff regularization of the series $\sum\frac{1}{n^2}$, Terry Tao shows how $-{1\over 12}$ is the finite part of the asymptotic series for the smooth cutoff regularization of the sum $\sum\frac{1}{n}$. In the last section of his post, he justifies why that value should match the analytic continuation of the function from its convergent domain. While all the steps of that argument make sense, I am somehow missing the larger picture. Can someone attempt another explanation? Let's make my question more concrete. If $f(z)=\sum a_nz^n$ is absolutely convergent for $\lvert z\rvert < R$ with a pole at some $z_0$, $\lvert z_0\rvert = R$, and has analytic continuation $\tilde f(z)$ defined on $\mathbb{C}\setminus z_0$, is there some standard way to break $f$ into an asymptotic series, with a demarcated finite part and infinite part? Like $f(z)\sim\sum_{n=1}^N b_n\epsilon^{-n} +\sum_{n=0}^\infty c_n\epsilon^n$ as $\epsilon\to 0$, and with $\sum_{n=0}^\infty c_n\epsilon^n\to \tilde f(z)$. If we discard the divergent part, the finite part matches the analytic continuation. Why? This is what Tao does, and seems to also match the approach of some shady QFT maneuvers.","In his post about computing the zeta function by smooth cutoff regularization of the series $\sum\frac{1}{n^2}$, Terry Tao shows how $-{1\over 12}$ is the finite part of the asymptotic series for the smooth cutoff regularization of the sum $\sum\frac{1}{n}$. In the last section of his post, he justifies why that value should match the analytic continuation of the function from its convergent domain. While all the steps of that argument make sense, I am somehow missing the larger picture. Can someone attempt another explanation? Let's make my question more concrete. If $f(z)=\sum a_nz^n$ is absolutely convergent for $\lvert z\rvert < R$ with a pole at some $z_0$, $\lvert z_0\rvert = R$, and has analytic continuation $\tilde f(z)$ defined on $\mathbb{C}\setminus z_0$, is there some standard way to break $f$ into an asymptotic series, with a demarcated finite part and infinite part? Like $f(z)\sim\sum_{n=1}^N b_n\epsilon^{-n} +\sum_{n=0}^\infty c_n\epsilon^n$ as $\epsilon\to 0$, and with $\sum_{n=0}^\infty c_n\epsilon^n\to \tilde f(z)$. If we discard the divergent part, the finite part matches the analytic continuation. Why? This is what Tao does, and seems to also match the approach of some shady QFT maneuvers.",,"['real-analysis', 'complex-analysis', 'convergence-divergence', 'asymptotics', 'analytic-continuation']"
88,Peter Walker's $C^{\infty}$ conjectured nowhere analytic slog,Peter Walker's  conjectured nowhere analytic slog,C^{\infty},"Consider the function $h(x)$ which is conjectured to be $c^\infty$ nowhere analytic, and can be used to generate what we call the base change slog, which is the inverse of the basechange sexp.  This is Peter Walker's helper function from his 1991 paper.  In http://eretrandre.org/rb/files/Walker1991_111.pdf Peter Walker proves $h(x)$ is $C^{\infty}\;$ infinitely differentiable.  How can we prove that $h(x)$ nowhere analytic?  $h(x)$ has a surprisingly simple definition, and converges nicely at the real axis. $$l(x) = \ln(x+1)$$ $$h_n (x) =  l^{[n]}\exp^{[n]}(x) $$ $$h(x) = \lim_{n\to \infty} h_n (x) $$ There is an analytic Abel function $\alpha(z)$ for iterating $f(z)=\exp(z)-1,\;\;\alpha(f(z))=\alpha(z)+1$, which has an aymptotic series studied by Ecalle; see https://mathoverflow.net/questions/45608/does-the-formal-power-series-solution-to-ffx-sin-x-converge . Then Peter Walker's slog function, which has also been referred to the basechange slog, is:  $$\text{slog}(x) = \alpha(h(x))\;\;\;\text{slog}(\exp(x))=\text{slog}(x)+1 $$ Peter Walker was aware of the difficulties of defining $h(z)$ in the complex plane, and wrote, ""...we cannot identify our function ... with Kneser's function defined by conformal mappings, without an extension of the domain of the function h to include nonreal values. Until both these difficulties have been overcome, the possibility remains that ... two ... distinct generalized logarithms have been constructed."" In fact, we now have more tools to generate Kneser's analytic sexp(z)/slog(z) solution, and there is a 1-cyclic mapping connecting it to Walker's solution.  For Walker's solution, the issue is that when $\exp^{[n-1]}(z)=(2m-1)\pi i\;$ then $\exp^{[n]}(z)=-1\;$ so $l(\exp^{[1]}(z))\;$ has a singularity in the definition of $h_n(z)$ and these singularities get arbitrarily close to the real axis as n increases. So even though the iteration of $h_n(x)$ converges superbly at the real axis, it fails to converge in any radius in the complex plane, no matter who small that radius is.  As far as I know, it has yet to be proven that Walker's solution doesn't converge to its Taylor series anywhere, so that Walker's solution is nowhere analytic. Here are some steps I took to understand the $h(x)$ function by understanding the $h_n(x)$ sequence. I continue to use the shorthand $l(x)=\ln(x+1)\;$ along with $l^{[n]}(x)$ for the iterated $l(x)$.  I also make use of the shorthand $\chi(x)=\exp(x)\;\;\;\chi^{[n]}(x)=\exp^{[n]}(x);$ and the shorthand $\Delta_n(x)=h_n(x)-h_{n-1}(x)$. $$ h_0(x)=x;\;\;\; h_1(x) = h_0(x) + l\left(\frac{1}{\exp(x)}\right) $$ $$ \Delta_1(x) = l \left(\frac{1}{ \chi^{[1]}(x) }\right)$$ $$ \Delta_2(x) = l \left( l \left(\frac{1}{ \chi^{[2]}(x) }\right)\frac{1}{ (\chi^{[1]}(x))+1 } \right)$$ $$ \Delta_3(x) = l \left(l \left(l \left(\frac{1}{\chi^{[3]}(x)}\right)\frac{1}{ \chi^{[2]}(x)+1 } \right)\frac{1}{ l^{[1]}(\chi^{[2]}(x))+1 }\right)$$ $$ \Delta_4(x) = l \left(l \left(l \left(l \left(\frac{1}{\chi^{[4]}(x)}\right)\frac{1}{ \chi^{[3]}(x)+1 } \right)\frac{1}{ l^{[1]}(\chi^{[3]}(x))+1 }\right)\frac{1}{ l^{[2]}(\chi^{[3]}(x))+1 }\right)$$ $$ \Delta_4(x) \approx \frac{1}{\chi^{[4]}(x)}\cdot\frac{1}{ \chi^{[3]}(x)+1 } \cdot \frac{1}{ l^{[1]}(\chi^{[3]}(x))+1 }\cdot\frac{1}{ l^{[2]}(\chi^{[3]}(x))+1 }\cdot\left(1-\frac{O}{2\chi^{[4]}(x)}\right)$$ The above equation can be extended arbitrarily.  One can also expland these equations using the Tayor series $l(x)=x+\frac{-x^2}{2}+\frac{x^3}{3}+\frac{-x^4}{4}+...$, which is what I think the next step is which leads to an expansion whose first term is the approximation above.  But then ultimately, we need to show that sum of these $\Delta_n(x)$ is nowhere analytic by studying the Taylor series expansion of $\Delta_n(x)$.  Then one hopes to show that as n increases, for low enough terms, the Taylor series for $\Delta_n(x)$ is very small compared with $\Delta_{n-1}(x)$ but for large enough terms, the Taylor terms grow faster than any $r^n$, so that the series radius of convergence gets arbitrarily small.","Consider the function $h(x)$ which is conjectured to be $c^\infty$ nowhere analytic, and can be used to generate what we call the base change slog, which is the inverse of the basechange sexp.  This is Peter Walker's helper function from his 1991 paper.  In http://eretrandre.org/rb/files/Walker1991_111.pdf Peter Walker proves $h(x)$ is $C^{\infty}\;$ infinitely differentiable.  How can we prove that $h(x)$ nowhere analytic?  $h(x)$ has a surprisingly simple definition, and converges nicely at the real axis. $$l(x) = \ln(x+1)$$ $$h_n (x) =  l^{[n]}\exp^{[n]}(x) $$ $$h(x) = \lim_{n\to \infty} h_n (x) $$ There is an analytic Abel function $\alpha(z)$ for iterating $f(z)=\exp(z)-1,\;\;\alpha(f(z))=\alpha(z)+1$, which has an aymptotic series studied by Ecalle; see https://mathoverflow.net/questions/45608/does-the-formal-power-series-solution-to-ffx-sin-x-converge . Then Peter Walker's slog function, which has also been referred to the basechange slog, is:  $$\text{slog}(x) = \alpha(h(x))\;\;\;\text{slog}(\exp(x))=\text{slog}(x)+1 $$ Peter Walker was aware of the difficulties of defining $h(z)$ in the complex plane, and wrote, ""...we cannot identify our function ... with Kneser's function defined by conformal mappings, without an extension of the domain of the function h to include nonreal values. Until both these difficulties have been overcome, the possibility remains that ... two ... distinct generalized logarithms have been constructed."" In fact, we now have more tools to generate Kneser's analytic sexp(z)/slog(z) solution, and there is a 1-cyclic mapping connecting it to Walker's solution.  For Walker's solution, the issue is that when $\exp^{[n-1]}(z)=(2m-1)\pi i\;$ then $\exp^{[n]}(z)=-1\;$ so $l(\exp^{[1]}(z))\;$ has a singularity in the definition of $h_n(z)$ and these singularities get arbitrarily close to the real axis as n increases. So even though the iteration of $h_n(x)$ converges superbly at the real axis, it fails to converge in any radius in the complex plane, no matter who small that radius is.  As far as I know, it has yet to be proven that Walker's solution doesn't converge to its Taylor series anywhere, so that Walker's solution is nowhere analytic. Here are some steps I took to understand the $h(x)$ function by understanding the $h_n(x)$ sequence. I continue to use the shorthand $l(x)=\ln(x+1)\;$ along with $l^{[n]}(x)$ for the iterated $l(x)$.  I also make use of the shorthand $\chi(x)=\exp(x)\;\;\;\chi^{[n]}(x)=\exp^{[n]}(x);$ and the shorthand $\Delta_n(x)=h_n(x)-h_{n-1}(x)$. $$ h_0(x)=x;\;\;\; h_1(x) = h_0(x) + l\left(\frac{1}{\exp(x)}\right) $$ $$ \Delta_1(x) = l \left(\frac{1}{ \chi^{[1]}(x) }\right)$$ $$ \Delta_2(x) = l \left( l \left(\frac{1}{ \chi^{[2]}(x) }\right)\frac{1}{ (\chi^{[1]}(x))+1 } \right)$$ $$ \Delta_3(x) = l \left(l \left(l \left(\frac{1}{\chi^{[3]}(x)}\right)\frac{1}{ \chi^{[2]}(x)+1 } \right)\frac{1}{ l^{[1]}(\chi^{[2]}(x))+1 }\right)$$ $$ \Delta_4(x) = l \left(l \left(l \left(l \left(\frac{1}{\chi^{[4]}(x)}\right)\frac{1}{ \chi^{[3]}(x)+1 } \right)\frac{1}{ l^{[1]}(\chi^{[3]}(x))+1 }\right)\frac{1}{ l^{[2]}(\chi^{[3]}(x))+1 }\right)$$ $$ \Delta_4(x) \approx \frac{1}{\chi^{[4]}(x)}\cdot\frac{1}{ \chi^{[3]}(x)+1 } \cdot \frac{1}{ l^{[1]}(\chi^{[3]}(x))+1 }\cdot\frac{1}{ l^{[2]}(\chi^{[3]}(x))+1 }\cdot\left(1-\frac{O}{2\chi^{[4]}(x)}\right)$$ The above equation can be extended arbitrarily.  One can also expland these equations using the Tayor series $l(x)=x+\frac{-x^2}{2}+\frac{x^3}{3}+\frac{-x^4}{4}+...$, which is what I think the next step is which leads to an expansion whose first term is the approximation above.  But then ultimately, we need to show that sum of these $\Delta_n(x)$ is nowhere analytic by studying the Taylor series expansion of $\Delta_n(x)$.  Then one hopes to show that as n increases, for low enough terms, the Taylor series for $\Delta_n(x)$ is very small compared with $\Delta_{n-1}(x)$ but for large enough terms, the Taylor terms grow faster than any $r^n$, so that the series radius of convergence gets arbitrarily small.",,"['real-analysis', 'complex-analysis', 'analyticity', 'tetration', 'power-towers']"
89,Motivation for the scaling in Tracy Widom distribution,Motivation for the scaling in Tracy Widom distribution,,"For past few months, I am self studying random matrices from the book  - An Introduction to Random Matrices by Anderson et.al. I was going through the result concerning the eigenvalues of the GUE at the edge of the spectrum (Theorem 3.1.4) I am trying to understand how the non intuitive scale factor N^(2/3) comes up. The proof of Lemma 3.7.2 seems to play the key role in the proof of the above theorem. Define $\displaystyle \psi_n(x)=\frac{e^{-x^2/4}H_n(x)}{\sqrt{\sqrt{2\pi}n!}}$ where $H_n(x)$ is the n-th Hermite polynomial and let $\displaystyle \Psi_n(x)=n^{1/12}\psi_n(2\sqrt{n}+\frac{x}{n^{1/6}})$. For this $\Psi_n(x)$ we have I went through the proof of lemma 3.7.2 which uses steepest descent. Why that scaling factor is used is hidden in that proof. The following are first few steps of the proof. Focus on the last paragraph of the above image. A taylor expansion of $F(s) = log(1+s) + s^2/2 - s$ starts with   $s^3/3$ in a neighbourhood of $0$, explains the particular scaling for   u. This line, present in the proof, seems to answer my question. However, I fail to understand how that explains the particular scaling of u . I am not so familiar with complex analysis. I would be highly obliged if anyone can explain the scaling ($n^{1/6}$) and the reasoning mentioned here.","For past few months, I am self studying random matrices from the book  - An Introduction to Random Matrices by Anderson et.al. I was going through the result concerning the eigenvalues of the GUE at the edge of the spectrum (Theorem 3.1.4) I am trying to understand how the non intuitive scale factor N^(2/3) comes up. The proof of Lemma 3.7.2 seems to play the key role in the proof of the above theorem. Define $\displaystyle \psi_n(x)=\frac{e^{-x^2/4}H_n(x)}{\sqrt{\sqrt{2\pi}n!}}$ where $H_n(x)$ is the n-th Hermite polynomial and let $\displaystyle \Psi_n(x)=n^{1/12}\psi_n(2\sqrt{n}+\frac{x}{n^{1/6}})$. For this $\Psi_n(x)$ we have I went through the proof of lemma 3.7.2 which uses steepest descent. Why that scaling factor is used is hidden in that proof. The following are first few steps of the proof. Focus on the last paragraph of the above image. A taylor expansion of $F(s) = log(1+s) + s^2/2 - s$ starts with   $s^3/3$ in a neighbourhood of $0$, explains the particular scaling for   u. This line, present in the proof, seems to answer my question. However, I fail to understand how that explains the particular scaling of u . I am not so familiar with complex analysis. I would be highly obliged if anyone can explain the scaling ($n^{1/6}$) and the reasoning mentioned here.",,"['complex-analysis', 'probability-theory', 'proof-explanation', 'random-matrices']"
90,Zeroes of $z^4+e^z$ in the unit disk,Zeroes of  in the unit disk,z^4+e^z,"How many zeroes does $f(z)=z^4+e^z$ have in the unit disc? ADDED: can you calculate them? Here the same question is asked about the disk of radius $2$. It can be solved easily by Rouché's theorem since when $|z|=2$, if $z=x+iy$ then $|e^z|=e^x\leq e^2$ so $$|f(z)-z^4|=|e^z|\leq e^2\leq 9<16=|z^4|$$ therefore there are $4$ roots in that disc (up to multiplicity). Now when $|z|=1$ we don't have this inequality, nor can we use the same trick by subtracting $e^z$ instead, since when $|z|=1$, $1/e\leq |e^z|\leq e$, one side is less than $1$ and the other greater. This can be used to show that there are no roots in the right half of the disc, where by going around a curve approximating the boundary of the right half of the disc, so always $x>0$,  we get $e^x>1\geq |z|$. A direct calculation also shows no roots are on the $y$ axis. Any ideas on how to deal with the left half?","How many zeroes does $f(z)=z^4+e^z$ have in the unit disc? ADDED: can you calculate them? Here the same question is asked about the disk of radius $2$. It can be solved easily by Rouché's theorem since when $|z|=2$, if $z=x+iy$ then $|e^z|=e^x\leq e^2$ so $$|f(z)-z^4|=|e^z|\leq e^2\leq 9<16=|z^4|$$ therefore there are $4$ roots in that disc (up to multiplicity). Now when $|z|=1$ we don't have this inequality, nor can we use the same trick by subtracting $e^z$ instead, since when $|z|=1$, $1/e\leq |e^z|\leq e$, one side is less than $1$ and the other greater. This can be used to show that there are no roots in the right half of the disc, where by going around a curve approximating the boundary of the right half of the disc, so always $x>0$,  we get $e^x>1\geq |z|$. A direct calculation also shows no roots are on the $y$ axis. Any ideas on how to deal with the left half?",,['complex-analysis']
91,Automorphic forms and the Rankin Selberg method,Automorphic forms and the Rankin Selberg method,,"I was just solving an Exercise, where we looked at an analytic function $\phi : \mathbf{H} \to \mathbf{C}$, which is automorphic and $\phi (z) = \mathcal{O}(y^{-C})$ for all $C > 0$ as $z \to i \infty$ (we write $z = x + iy$). I think that this implies that $\phi$ is constant, since it is a modular form of weight zero. The solution to the Exercise however is three pages long and is about proving that $$\Lambda_\phi (s) := 2 \pi^{-s} \Gamma (s) \zeta(s) \mathcal{M}(\phi)(s - 1)$$ satisfies the functional equation $$\Lambda_\phi (s) = \Lambda_\phi(1 - s)$$ where $\mathcal{M}(\phi)(s)$ is the Mellin-transform $\int_0^\infty \phi(x + iy) y^{s - 1} dy$. This is referred to as ""the simplest case of the Rankin-Selberg-method"". The statement gets rather easy if $\phi$ is constant. So my question is: What was the actual point of the Exercise? I was hoping for someone to recognize a formulation of the Rankin-Selberg-method (which I have only seen stated differently in the sources I considered) or of a general tool that is used in the theory. Thanks! Added: Thinking about the problem again and looking at the specific example of the non-holomorphic Eisenstein series $$E(z, s) = \sum_{(m, n) \in \mathbf{Z}^2, \, (m, n ) \neq (0, 0)} \frac{y^s}{\vert m z + n \vert^{2s}}$$ I suppose that the only weakening on $\phi$ that is necessary is to make $\phi$ a smooth function in $x$ and $y$, instead of a holomorphic function. But my question is still: ""What is the context of this exercise?""","I was just solving an Exercise, where we looked at an analytic function $\phi : \mathbf{H} \to \mathbf{C}$, which is automorphic and $\phi (z) = \mathcal{O}(y^{-C})$ for all $C > 0$ as $z \to i \infty$ (we write $z = x + iy$). I think that this implies that $\phi$ is constant, since it is a modular form of weight zero. The solution to the Exercise however is three pages long and is about proving that $$\Lambda_\phi (s) := 2 \pi^{-s} \Gamma (s) \zeta(s) \mathcal{M}(\phi)(s - 1)$$ satisfies the functional equation $$\Lambda_\phi (s) = \Lambda_\phi(1 - s)$$ where $\mathcal{M}(\phi)(s)$ is the Mellin-transform $\int_0^\infty \phi(x + iy) y^{s - 1} dy$. This is referred to as ""the simplest case of the Rankin-Selberg-method"". The statement gets rather easy if $\phi$ is constant. So my question is: What was the actual point of the Exercise? I was hoping for someone to recognize a formulation of the Rankin-Selberg-method (which I have only seen stated differently in the sources I considered) or of a general tool that is used in the theory. Thanks! Added: Thinking about the problem again and looking at the specific example of the non-holomorphic Eisenstein series $$E(z, s) = \sum_{(m, n) \in \mathbf{Z}^2, \, (m, n ) \neq (0, 0)} \frac{y^s}{\vert m z + n \vert^{2s}}$$ I suppose that the only weakening on $\phi$ that is necessary is to make $\phi$ a smooth function in $x$ and $y$, instead of a holomorphic function. But my question is still: ""What is the context of this exercise?""",,"['complex-analysis', 'number-theory', 'analytic-number-theory', 'modular-forms']"
92,Correcting Gauss's proof of FTA. Need verification,Correcting Gauss's proof of FTA. Need verification,,"In his doctoral thesis, Gauss gave a proof of fundamental theorem of algebra for real polynomials, based on geometric arguents. Later in his life he expanded the proof to complex polynomials. A nice account can be found here . The problem with Gauss's proof was that he assumed some topological arguments, which were not proven rigorously till a century later. A proof is presented here which starts in Gauss's lines, then replaces his topological arguments with least upper bound for real numbers and continuity considerations only. Gauss was aware of the least upper bound property ( LUB ) for real numbers (in fact he co-discovered it). So his topological arguments could have been replaced by the following method, which is why I am calling it ""Correction"". Any help with verification will be greatly appreciated. Theorem: Every complex polynomial has a complex root. Background: We only consider polynomials which are not factorizable into lower degree polynomials of degree >1. For factorizable polynomials, the proof can be applied on the factors. Complex numbers are assumed to exists and it is known that they form a field (So that basic algebraic operations and distributive law are well defined). Modulus function is also defined. Gauss's proof : Rewrite $z=re^{i\theta}$. Then $z^k=r^{k} (cos(k\theta) +i sin(k\theta))$ Then $\sum_{k=0}^{n}c_{k}z^{k}=\sum_{k=0}^{n}  | c_{k} | r^{k}(cos(k\theta+\phi_{n}) +i sin(k\theta+\phi_{k}))$ So real part of $R(r,\theta)=\sum_{k=0}^{n}  | c_{k} | r^{k} cos(k\theta+\phi_{k} )$ and the imaginary part is $I(r,\theta)=\sum_{k=0}^{n}  | c_{k} | r^{k}  sin(k\theta+\phi_{k})$. Taking $c_{n}=1$, Gauss showed that for sufficiently large $r=L$, $R(L,\theta)$ changes sign in the interval $\theta \in (\frac{\pi k'}{n}-cos^{-1}\epsilon,\frac{\pi k'}{n}+cos^{-1}\epsilon), \quad k=0,1,2,...,2(n-1)$. Applying intermediate value theorem , this means that $R(L,\theta)$ has roots in these intervals of $\theta$, for any $r$ large enough. On the other hand, for L sufficiently large, $I(L,\theta)$ changes sign in the interval $\theta \in (\frac{\pi}{2n}+ \frac{\pi k'}{n}-cos^{-1}\epsilon,\frac{\pi}{2n}+\frac{\pi k'}{n}+cos^{-1}\epsilon)$ and thus has a root in every interval. If $\epsilon>0$ is taken sufficiently small, al these intervals have zero overlap. So on the circle $r=L$, we have $2n$ roots of $R$ and $I$ interlacing. All roots are exhausted. Gauss then deduced the existence of a root based on this fact and the following assumption: Lemma 1: If two continuous bivariate function have interlacing roots on a closed curve, they must have a simultaneous roots inside the curve. The proof of this is quite hard, based on topological arguments (connected curves, Jordan curve theorem etc). Gauss speculated that this could be proven (his speculations were often correct and groundbreaking) but he did not have a proof. My proof: Using the same $R$ and $I$ as Gauss, only least upper bound and continuity arguments can be used to prove FTA. $R$ and $I$ can also be written in terms of $(x,y),z=x+iy$ in the following way. $$\left[\begin{array}{l}R(x,y)\\I(x,y)\end{array}\right]= \sum_{k=0}^{n} \left[\begin{array}{l}Re(c_{k})&-Im(c_{k})\\Im(c_{k})&Re(c_{k})\end{array}\right] \left[\begin{array}{l}x&-y\\y&x\end{array}\right]^{k} \left[\begin{array}{l}1\\0\end{array}\right]$$ If we take $\bar{y}$ large enough, $r$ is also large enough for any $x,\bar{y}$. For all  the disjoint $\theta$ intervals Gauss created in this proof, $x=\bar{y} tan(\theta)$ also generate disjoint intervals. Usings intermediate value theorem on $x$, it is starigtforward to deduce that $\exists y_{1}<0$, such that $R(x,\bar{y})$ and $I(x,\bar{y})$  have interlacing roots $\forall \ \bar{y}<y_{1}$. Looking at the matrix formulation of $R$ and $I$, it can be seen directly that the maximum power of $x$ in $R(x,\bar{y})$ and $I(x,\bar{y})$ are $n$ and $n-1$ respectively. So all real roots exist and interlace $\forall \ \bar{y}<y_{1}$. Now comes LUB based 'Correction': 1. $R(x,0)$ does not have any real root. Wrong, see correction at  {*} in the end of this article 2. $\exists y_{1}<0$, such that $R(x,\bar{y})$ and $I(x,\bar{y})$  have maximum possible number of real roots, which interlace $\forall \ \bar{y}<y_{1}$ 3. LUB applied on 1. and 2. says that $\exists y_{sup}<0$ such that both $R(x,\bar{y})$ and $I(x,\bar{y})$ have maximum number of real roots $\forall \bar{y}<y_{sup}$ and at least one of them has less number of real roots for $y_{sup}<\bar{y}<y_{sup}+\delta$, for some $\delta >0$. 4. Using intermediate value theorem and due to the finite number of roots, it can be shown that the roots of both $R(x,\bar{y})$ and $I(x,\bar{y})$ vary continuously with $\bar{y}, \quad \forall \bar{y}<y_{sup}$. Question is what happens at $\bar{y}=y_{sup}$. 5. $R(x,\bar{y})$ and $I(x,\bar{y})$ are both bivariate polynomials of $x$ and $y$. Thus the roots on x are clearly bounded $\forall \bar{y}<y_{sup}<0$. Thus $liminf_{\bar{y} \rightarrow y_{sup}}$ and $limsup_{\bar{y} \rightarrow y_{sup}}$  of the roots exist for any indexed root. If the $limsup$ and $liminf$ are different for any indexed root, $R=0$ or $I=0$ crosses any value between these two infinite times. For fixed $x$, $R$ or $I$ are polynomials in $y$ and cannot have infinite number of roots. So the $liminf=limsup$ (implying that the roots approach limits as $\bar{y} \rightarrow y_{sup}$). Thus $R(x,y_{sup})$ and $I(x,y_{sup})$  have maximum possible real roots. At least one of the roots do not exist for slightly large $\bar{y}$. If all roots of $R(x,y_{sup})$ and $I(x,y_{sup})$ are distinct,  intermediate value theorem can be applied on the larger side of $\bar{y}$ to have a contradiction. Thus the only possibility is that there is a root on $x$ which has same sign on both sides on the line $\bar{y}=y_{sup}$. This is clearly a root with multiplicity, implying that two of the root lines meet. It is easy to show that two consecutive roots meet at $\bar{y}=y_{sup}$ Without loss of generality, let us say two consecutive roots of $R$ meet at $\bar{y}=y_{sup}$. $\forall \bar{y}<y_{1}$, there is a root of $I$ between these two roots. Again, straighforward application of LUB gives that $\exists y_{sup}<y<y_{1}$ and the corresponding root of $I$ mentioned in the last line such that $R(x,y)=I(x,y)$. This proves FTA. I did not want this to be a very long post. So Sorry for being terse in the derivations. {*} Update 24 April 2016 : In my 'Correction', point 1 only works for real polynomials. Slightly more work needs to be done for complex polynomials. If either the real or complex part of the polynomial ($R$ and $I$)has less than $n$ or $n-1$ roots at $\bar{y}=0$, the rest or the arguments go through. If not, since maximum possible real roots exists for $R(x,\bar{y})$ and $I(x,\bar{y})$ for all $\bar{y}$, the roots vary continuity according to point 4.  Then following possibilities exists Case 1: For some $\bar{y}$, the roots of $R(x,\bar{y})$ and $I(x,\bar{y})$ do not interlace. LUB and continuity arguments can be directly applied on this to deduce the existence of a simultaneous solution of $R(x,\bar{y})$ and $I(x,\bar{y})$. Case 2: The interlacing is always preserved. Writing $R(x,\bar{y})$ as a function of $x$ and applying the Fujiwara bound for the upper bound of the roots, it can be seen that there cannot exist a roots of $R(x,\bar{y})$ for large enough $x$ in the region $\bar{y}<mx$, for some $m\in \mathbb{R}$. However, this is impossible since rewriting $R$ as $R(r,\theta)$ gives that for small, say $\theta^{*}=tan^{-1}\frac{m}{2}$, $\exists \bar{r}$ and $\theta(r)$ such that $R(r>\bar{r},\theta(r)))=0$ satisfying $|\theta(r)|<|\theta^{*}|$. If anyone follows up in details to this point and wishes to have a full explanation of this, I will be glad to provide one.","In his doctoral thesis, Gauss gave a proof of fundamental theorem of algebra for real polynomials, based on geometric arguents. Later in his life he expanded the proof to complex polynomials. A nice account can be found here . The problem with Gauss's proof was that he assumed some topological arguments, which were not proven rigorously till a century later. A proof is presented here which starts in Gauss's lines, then replaces his topological arguments with least upper bound for real numbers and continuity considerations only. Gauss was aware of the least upper bound property ( LUB ) for real numbers (in fact he co-discovered it). So his topological arguments could have been replaced by the following method, which is why I am calling it ""Correction"". Any help with verification will be greatly appreciated. Theorem: Every complex polynomial has a complex root. Background: We only consider polynomials which are not factorizable into lower degree polynomials of degree >1. For factorizable polynomials, the proof can be applied on the factors. Complex numbers are assumed to exists and it is known that they form a field (So that basic algebraic operations and distributive law are well defined). Modulus function is also defined. Gauss's proof : Rewrite $z=re^{i\theta}$. Then $z^k=r^{k} (cos(k\theta) +i sin(k\theta))$ Then $\sum_{k=0}^{n}c_{k}z^{k}=\sum_{k=0}^{n}  | c_{k} | r^{k}(cos(k\theta+\phi_{n}) +i sin(k\theta+\phi_{k}))$ So real part of $R(r,\theta)=\sum_{k=0}^{n}  | c_{k} | r^{k} cos(k\theta+\phi_{k} )$ and the imaginary part is $I(r,\theta)=\sum_{k=0}^{n}  | c_{k} | r^{k}  sin(k\theta+\phi_{k})$. Taking $c_{n}=1$, Gauss showed that for sufficiently large $r=L$, $R(L,\theta)$ changes sign in the interval $\theta \in (\frac{\pi k'}{n}-cos^{-1}\epsilon,\frac{\pi k'}{n}+cos^{-1}\epsilon), \quad k=0,1,2,...,2(n-1)$. Applying intermediate value theorem , this means that $R(L,\theta)$ has roots in these intervals of $\theta$, for any $r$ large enough. On the other hand, for L sufficiently large, $I(L,\theta)$ changes sign in the interval $\theta \in (\frac{\pi}{2n}+ \frac{\pi k'}{n}-cos^{-1}\epsilon,\frac{\pi}{2n}+\frac{\pi k'}{n}+cos^{-1}\epsilon)$ and thus has a root in every interval. If $\epsilon>0$ is taken sufficiently small, al these intervals have zero overlap. So on the circle $r=L$, we have $2n$ roots of $R$ and $I$ interlacing. All roots are exhausted. Gauss then deduced the existence of a root based on this fact and the following assumption: Lemma 1: If two continuous bivariate function have interlacing roots on a closed curve, they must have a simultaneous roots inside the curve. The proof of this is quite hard, based on topological arguments (connected curves, Jordan curve theorem etc). Gauss speculated that this could be proven (his speculations were often correct and groundbreaking) but he did not have a proof. My proof: Using the same $R$ and $I$ as Gauss, only least upper bound and continuity arguments can be used to prove FTA. $R$ and $I$ can also be written in terms of $(x,y),z=x+iy$ in the following way. $$\left[\begin{array}{l}R(x,y)\\I(x,y)\end{array}\right]= \sum_{k=0}^{n} \left[\begin{array}{l}Re(c_{k})&-Im(c_{k})\\Im(c_{k})&Re(c_{k})\end{array}\right] \left[\begin{array}{l}x&-y\\y&x\end{array}\right]^{k} \left[\begin{array}{l}1\\0\end{array}\right]$$ If we take $\bar{y}$ large enough, $r$ is also large enough for any $x,\bar{y}$. For all  the disjoint $\theta$ intervals Gauss created in this proof, $x=\bar{y} tan(\theta)$ also generate disjoint intervals. Usings intermediate value theorem on $x$, it is starigtforward to deduce that $\exists y_{1}<0$, such that $R(x,\bar{y})$ and $I(x,\bar{y})$  have interlacing roots $\forall \ \bar{y}<y_{1}$. Looking at the matrix formulation of $R$ and $I$, it can be seen directly that the maximum power of $x$ in $R(x,\bar{y})$ and $I(x,\bar{y})$ are $n$ and $n-1$ respectively. So all real roots exist and interlace $\forall \ \bar{y}<y_{1}$. Now comes LUB based 'Correction': 1. $R(x,0)$ does not have any real root. Wrong, see correction at  {*} in the end of this article 2. $\exists y_{1}<0$, such that $R(x,\bar{y})$ and $I(x,\bar{y})$  have maximum possible number of real roots, which interlace $\forall \ \bar{y}<y_{1}$ 3. LUB applied on 1. and 2. says that $\exists y_{sup}<0$ such that both $R(x,\bar{y})$ and $I(x,\bar{y})$ have maximum number of real roots $\forall \bar{y}<y_{sup}$ and at least one of them has less number of real roots for $y_{sup}<\bar{y}<y_{sup}+\delta$, for some $\delta >0$. 4. Using intermediate value theorem and due to the finite number of roots, it can be shown that the roots of both $R(x,\bar{y})$ and $I(x,\bar{y})$ vary continuously with $\bar{y}, \quad \forall \bar{y}<y_{sup}$. Question is what happens at $\bar{y}=y_{sup}$. 5. $R(x,\bar{y})$ and $I(x,\bar{y})$ are both bivariate polynomials of $x$ and $y$. Thus the roots on x are clearly bounded $\forall \bar{y}<y_{sup}<0$. Thus $liminf_{\bar{y} \rightarrow y_{sup}}$ and $limsup_{\bar{y} \rightarrow y_{sup}}$  of the roots exist for any indexed root. If the $limsup$ and $liminf$ are different for any indexed root, $R=0$ or $I=0$ crosses any value between these two infinite times. For fixed $x$, $R$ or $I$ are polynomials in $y$ and cannot have infinite number of roots. So the $liminf=limsup$ (implying that the roots approach limits as $\bar{y} \rightarrow y_{sup}$). Thus $R(x,y_{sup})$ and $I(x,y_{sup})$  have maximum possible real roots. At least one of the roots do not exist for slightly large $\bar{y}$. If all roots of $R(x,y_{sup})$ and $I(x,y_{sup})$ are distinct,  intermediate value theorem can be applied on the larger side of $\bar{y}$ to have a contradiction. Thus the only possibility is that there is a root on $x$ which has same sign on both sides on the line $\bar{y}=y_{sup}$. This is clearly a root with multiplicity, implying that two of the root lines meet. It is easy to show that two consecutive roots meet at $\bar{y}=y_{sup}$ Without loss of generality, let us say two consecutive roots of $R$ meet at $\bar{y}=y_{sup}$. $\forall \bar{y}<y_{1}$, there is a root of $I$ between these two roots. Again, straighforward application of LUB gives that $\exists y_{sup}<y<y_{1}$ and the corresponding root of $I$ mentioned in the last line such that $R(x,y)=I(x,y)$. This proves FTA. I did not want this to be a very long post. So Sorry for being terse in the derivations. {*} Update 24 April 2016 : In my 'Correction', point 1 only works for real polynomials. Slightly more work needs to be done for complex polynomials. If either the real or complex part of the polynomial ($R$ and $I$)has less than $n$ or $n-1$ roots at $\bar{y}=0$, the rest or the arguments go through. If not, since maximum possible real roots exists for $R(x,\bar{y})$ and $I(x,\bar{y})$ for all $\bar{y}$, the roots vary continuity according to point 4.  Then following possibilities exists Case 1: For some $\bar{y}$, the roots of $R(x,\bar{y})$ and $I(x,\bar{y})$ do not interlace. LUB and continuity arguments can be directly applied on this to deduce the existence of a simultaneous solution of $R(x,\bar{y})$ and $I(x,\bar{y})$. Case 2: The interlacing is always preserved. Writing $R(x,\bar{y})$ as a function of $x$ and applying the Fujiwara bound for the upper bound of the roots, it can be seen that there cannot exist a roots of $R(x,\bar{y})$ for large enough $x$ in the region $\bar{y}<mx$, for some $m\in \mathbb{R}$. However, this is impossible since rewriting $R$ as $R(r,\theta)$ gives that for small, say $\theta^{*}=tan^{-1}\frac{m}{2}$, $\exists \bar{r}$ and $\theta(r)$ such that $R(r>\bar{r},\theta(r)))=0$ satisfying $|\theta(r)|<|\theta^{*}|$. If anyone follows up in details to this point and wishes to have a full explanation of this, I will be glad to provide one.",,"['real-analysis', 'complex-analysis', 'polynomials', 'proof-verification', 'roots']"
93,"(without Phragmén-Lindelöf) $f$ is of exponential type and bounded on the real axis, then there exists $C>0$ such that $|f(x+iy)|\leq Ce^{\tau |y|}$","(without Phragmén-Lindelöf)  is of exponential type and bounded on the real axis, then there exists  such that",f C>0 |f(x+iy)|\leq Ce^{\tau |y|},"Question: Without using Phragmén-Lindelöf, show that if $f$ is of exponential type and uniformly bounded on the real axis, then there exists $C>0$ such that $|f(x+iy)|\leq Ce^{\tau |y|}.$ Definitions: An entire function $f$ is of exponential type if there exist $K>0$ and $\tau>0$ such that $|f(z)|\leq Ke^{\tau z}.$ Relevant information: I'm told to consider $f(z)e^{-\tau z}$ for $x\geq 0$ and $f(z)e^{\tau z}$ for $x\leq 0$ and use the maximum principle on appropriate rectangles. Attempt I am unsure of how to start this problem.","Question: Without using Phragmén-Lindelöf, show that if $f$ is of exponential type and uniformly bounded on the real axis, then there exists $C>0$ such that $|f(x+iy)|\leq Ce^{\tau |y|}.$ Definitions: An entire function $f$ is of exponential type if there exist $K>0$ and $\tau>0$ such that $|f(z)|\leq Ke^{\tau z}.$ Relevant information: I'm told to consider $f(z)e^{-\tau z}$ for $x\geq 0$ and $f(z)e^{\tau z}$ for $x\leq 0$ and use the maximum principle on appropriate rectangles. Attempt I am unsure of how to start this problem.",,"['complex-analysis', 'maximum-principle']"
94,Proof of Sophomore's Dream using Contour Integration,Proof of Sophomore's Dream using Contour Integration,,"Sophomore's dream is a relatively common identity, that states $$ \int _0^1 x^{-x} dx = \sum_{n = 1}^\infty n^{-n}$$ The common proof is found using the series expansion for $ e^{- x \log x} $ and switching the integral and the sum. I have tried to find a proof of the identity using contour integration in the complex plane. I originally thought about the residue theorem, but then realized I couldn't as it only applies if there are finitely many poles. I then decided to transform the original integral as follows. $$ \int_0^1 x^{-x} dx = \int_1^ \infty x^{-\frac 1 x - 2} dx $$  using the transform $ x \rightarrow \frac 1 x $ Then we take the integral over the half annulus with outer radius $R$ and inner radius $1$ of $ z^{-\frac 1 z - 2} $ $$ \int_C z ^ {-\frac 1 z - 2} dz = \int_1^R x ^ {-\frac 1 x - 2} dx + \int_{C_1} z ^ {-\frac 1 z - 2} dz + \int_{-R}^{-1} x ^ {-\frac 1 x - 2} dx + \int_{C_2} z ^ {-\frac 1 z - 2} dz = 0$$ where $C_1$ is the upper semicircle with radius R, and $C_2$ is the upper half of the unit circle. $$\left| \int_{C_1} z ^ {-\frac 1 z - 2} dz \right| \le \int_{C_1} | z ^ {-\frac 1 z - 2} | dz \sim\int_0^{\pi} R ^ {- 2} dz \rightarrow 0$$ From here I am stuck with the other integrals, and I am unsure of how to proceed.","Sophomore's dream is a relatively common identity, that states $$ \int _0^1 x^{-x} dx = \sum_{n = 1}^\infty n^{-n}$$ The common proof is found using the series expansion for $ e^{- x \log x} $ and switching the integral and the sum. I have tried to find a proof of the identity using contour integration in the complex plane. I originally thought about the residue theorem, but then realized I couldn't as it only applies if there are finitely many poles. I then decided to transform the original integral as follows. $$ \int_0^1 x^{-x} dx = \int_1^ \infty x^{-\frac 1 x - 2} dx $$  using the transform $ x \rightarrow \frac 1 x $ Then we take the integral over the half annulus with outer radius $R$ and inner radius $1$ of $ z^{-\frac 1 z - 2} $ $$ \int_C z ^ {-\frac 1 z - 2} dz = \int_1^R x ^ {-\frac 1 x - 2} dx + \int_{C_1} z ^ {-\frac 1 z - 2} dz + \int_{-R}^{-1} x ^ {-\frac 1 x - 2} dx + \int_{C_2} z ^ {-\frac 1 z - 2} dz = 0$$ where $C_1$ is the upper semicircle with radius R, and $C_2$ is the upper half of the unit circle. $$\left| \int_{C_1} z ^ {-\frac 1 z - 2} dz \right| \le \int_{C_1} | z ^ {-\frac 1 z - 2} | dz \sim\int_0^{\pi} R ^ {- 2} dz \rightarrow 0$$ From here I am stuck with the other integrals, and I am unsure of how to proceed.",,"['complex-analysis', 'summation', 'contour-integration', 'alternative-proof']"
95,What is THE domain of analyticity of a holomorphic function?,What is THE domain of analyticity of a holomorphic function?,,"I am self-studying complex analysis, and I am a little bit confused on notations. Suppose that $f:U \to \mathbb C $ is a holomorphic function defined on an open subset of $\mathbb C^n $. I understand that every holomorphic extension of $f$ to a connected open set containing $U$ is uniquely determined by $f$. I understand also that there may be many maximal holomorphic extensions of $f$. However, sometimes I read things like ""find THE domain of holomorphy of the locally-defined holomorphic function ..."" and so on. My question is: what is THE domain of analyticity of a function? If it is meant to be the domain of a maximal holomorphic extension on it, then why use the word ""THE"", since such maximal holomorphic extension is not unique? They refer to some particular maximal holomorphic extension? Thank you.","I am self-studying complex analysis, and I am a little bit confused on notations. Suppose that $f:U \to \mathbb C $ is a holomorphic function defined on an open subset of $\mathbb C^n $. I understand that every holomorphic extension of $f$ to a connected open set containing $U$ is uniquely determined by $f$. I understand also that there may be many maximal holomorphic extensions of $f$. However, sometimes I read things like ""find THE domain of holomorphy of the locally-defined holomorphic function ..."" and so on. My question is: what is THE domain of analyticity of a function? If it is meant to be the domain of a maximal holomorphic extension on it, then why use the word ""THE"", since such maximal holomorphic extension is not unique? They refer to some particular maximal holomorphic extension? Thank you.",,['complex-analysis']
96,Angle between two curves at infinity?,Angle between two curves at infinity?,,"In complex analysis, how would I find the angle between the two curves $f(z)=0$ and $g(z)=0$ at $\infty$? My guess would be that we would let $w=\frac{1}{z}$ and find the angle between the two curves at $w=0$ but if this is right would it work in every case or are there exceptions?","In complex analysis, how would I find the angle between the two curves $f(z)=0$ and $g(z)=0$ at $\infty$? My guess would be that we would let $w=\frac{1}{z}$ and find the angle between the two curves at $w=0$ but if this is right would it work in every case or are there exceptions?",,['complex-analysis']
97,Double contour integral in terms of real integrals,Double contour integral in terms of real integrals,,"Let $\gamma$ be a curve in $\mathbb{C}$, and let $\gamma_0$ be a circle in an open connected set $A \subset \mathbb{C}$ around $z_0 \in A$. Suppose the interior of $\gamma_0$ lies in $A$. Let $z$ be inside $\gamma_0$. Assume $n(\gamma_0; z) = 1$, and $f(z, w)$ is a continuous function of $z$, $w$ for $z$ in $A$ and $w$ on $\gamma$. For each $w$ on $\gamma$ assume that $f$ is analytic in $z$. Consider $$F(z) = \frac{1}{2\pi i} \int_\gamma \int_{\gamma_0} \frac{f(\zeta, w)}{\zeta - z} d\zeta dw.$$ Marsden (Basic Complex Analysis, p. 179) claims that in terms of real integrals $F(z)$ has the form $$\int_a^b \int_\alpha^\beta h(x, y) dx dy + i\int_a^b \int_\alpha^\beta k(x, y) dx dy.$$ Can someone explain step-by-step why it is so?","Let $\gamma$ be a curve in $\mathbb{C}$, and let $\gamma_0$ be a circle in an open connected set $A \subset \mathbb{C}$ around $z_0 \in A$. Suppose the interior of $\gamma_0$ lies in $A$. Let $z$ be inside $\gamma_0$. Assume $n(\gamma_0; z) = 1$, and $f(z, w)$ is a continuous function of $z$, $w$ for $z$ in $A$ and $w$ on $\gamma$. For each $w$ on $\gamma$ assume that $f$ is analytic in $z$. Consider $$F(z) = \frac{1}{2\pi i} \int_\gamma \int_{\gamma_0} \frac{f(\zeta, w)}{\zeta - z} d\zeta dw.$$ Marsden (Basic Complex Analysis, p. 179) claims that in terms of real integrals $F(z)$ has the form $$\int_a^b \int_\alpha^\beta h(x, y) dx dy + i\int_a^b \int_\alpha^\beta k(x, y) dx dy.$$ Can someone explain step-by-step why it is so?",,"['complex-analysis', 'complex-numbers', 'complex-integration']"
98,Finding an explicit entire function $g$ satisfying $g(n \log n) = n^{\pi}$,Finding an explicit entire function  satisfying,g g(n \log n) = n^{\pi},"I encountered the following problem in the lecture note in my complex analysis class: Problem. Find an explicit entire function $g$ satisfying $g(n \log n) = n^{\pi}$ for $n = 1, 2, \cdots$ . Hint. $|(1 − t)|e^t$ is decreasing for $0 < t < 1$ and increasing for $t > 1$ . Use this to compare the derivative of a product vanishing at $n \log n$ to one vanishing at $n$ , for integers $n$ . There is also an easier way using an elementary function that vanishes at the integers. This is an exercise in the chapter where the Mittag-Leffler theorem and the Weierstrass factorization theorem were introduced. I tried to follow the hint and was able to indirectly construct such a function (by mimicking the proof of Mittag-Leffler), but I was unable to find an explicit closed form. Would anyone help me find an explicit example? Thanks in advance!","I encountered the following problem in the lecture note in my complex analysis class: Problem. Find an explicit entire function satisfying for . Hint. is decreasing for and increasing for . Use this to compare the derivative of a product vanishing at to one vanishing at , for integers . There is also an easier way using an elementary function that vanishes at the integers. This is an exercise in the chapter where the Mittag-Leffler theorem and the Weierstrass factorization theorem were introduced. I tried to follow the hint and was able to indirectly construct such a function (by mimicking the proof of Mittag-Leffler), but I was unable to find an explicit closed form. Would anyone help me find an explicit example? Thanks in advance!","g g(n \log n) = n^{\pi} n = 1, 2, \cdots |(1 − t)|e^t 0 < t < 1 t > 1 n \log n n n","['complex-analysis', 'analyticity']"
99,"Morera's theorem, Conway's proof","Morera's theorem, Conway's proof",,"I have a question about the proof of Morera's theorem as presented in Conway's text volume I. Let $G$ be a region and let $ f:G \to \mathbb{C} $ be a continuous function. Fix $ z_o$ in $G$.  Then for any $z \in G$, let $[z_o,z]$ denote a line segment from $ z_o$ to $z$. Then, $$\bigg| \frac{1}{z-z_o} \int_{[z_o,z]} [f(w)-f(z_o)] dw \bigg|\leq |f(z)-f(z_o)|$$ I want to ask how to establish that inequality, my attempt is this, I know that  $$\bigg| \frac{1}{z-z_o} \int_{[z_o,z]} [f(w)-f(z_o)] dw \bigg|\leq \frac{V([z_o,z])}{|z-z_o|}~ \text{sup}\{   ~|f(w)-f(z_o)| : w \in [z_o,z] ~\} $$ where $ V([z_o,z]) $ is the length of $ [z_o,z]$. So, $$\bigg| \frac{1}{z-z_o} \int_{[z_o,z]} [f(w)-f(z_o)] dw \bigg| \leq \text{sup}\{   ~|f(w)-f(z_o)| : w \in [z_o,z] ~\} $$ I can't show that $$\text{sup}\{   ~|f(w)-f(z_o)| : w \in [z_o,z] ~\} \leq |f(z)-f(z_o)| $$ just by the continuity of $f$. I feel that I'm just missing some simple detail to show this.  thank you.","I have a question about the proof of Morera's theorem as presented in Conway's text volume I. Let $G$ be a region and let $ f:G \to \mathbb{C} $ be a continuous function. Fix $ z_o$ in $G$.  Then for any $z \in G$, let $[z_o,z]$ denote a line segment from $ z_o$ to $z$. Then, $$\bigg| \frac{1}{z-z_o} \int_{[z_o,z]} [f(w)-f(z_o)] dw \bigg|\leq |f(z)-f(z_o)|$$ I want to ask how to establish that inequality, my attempt is this, I know that  $$\bigg| \frac{1}{z-z_o} \int_{[z_o,z]} [f(w)-f(z_o)] dw \bigg|\leq \frac{V([z_o,z])}{|z-z_o|}~ \text{sup}\{   ~|f(w)-f(z_o)| : w \in [z_o,z] ~\} $$ where $ V([z_o,z]) $ is the length of $ [z_o,z]$. So, $$\bigg| \frac{1}{z-z_o} \int_{[z_o,z]} [f(w)-f(z_o)] dw \bigg| \leq \text{sup}\{   ~|f(w)-f(z_o)| : w \in [z_o,z] ~\} $$ I can't show that $$\text{sup}\{   ~|f(w)-f(z_o)| : w \in [z_o,z] ~\} \leq |f(z)-f(z_o)| $$ just by the continuity of $f$. I feel that I'm just missing some simple detail to show this.  thank you.",,['complex-analysis']
