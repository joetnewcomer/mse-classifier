,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,I want to show that $\int_{-\infty}^{\infty}{\left(x^2-x+\pi\over x^4-x^2+1\right)^2}dx=\pi+\pi^2+\pi^3$,I want to show that,\int_{-\infty}^{\infty}{\left(x^2-x+\pi\over x^4-x^2+1\right)^2}dx=\pi+\pi^2+\pi^3,"I want to show that $$\int_{-\infty}^{\infty}{\left(x^2-x+\pi\over x^4-x^2+1\right)^2}dx=\pi+\pi^2+\pi^3$$ Expand $(x^4-x+\pi)^2=x^4-2x^3+2x^2-2x\pi+\pi{x^2}+\pi^2$ Let see (substitution of $y=x^2$) $$\int_{-\infty}^{\infty}{x\over (x^4-x^2+1)^2}dx={1\over 2}\int_{-\infty}^{\infty}{1\over (y^2-y+1)^2}dy$$ Substituion of $y=x^3$ $$\int_{-\infty}^{\infty}{x^3\over (x^4-x^2+1)^2}dx={1\over 4}\int_{-\infty}^{\infty}{1\over (y^2-y+1)^2}dy$$ As for $\int_{-\infty}^{\infty}{x^2\over (x^4-x^2+1)^2}dx$ and $\int_{-\infty}^{\infty}{x^4\over (x^4-x^2+1)^2}dx$ are difficult to find a suitable substitution. This is the point where I am shrugged with to find a suitable substitution To lead me to a particular standard integral. Need some help, thank. standard integral of the form $$\int{1\over (ax^2+bx+c)^2}dx={2ax+b\over (4ac-b^2)(ax^2+bx+c)}+{2a\over 4ac-b^2}\int{1\over ax^2+bx+c}dx$$ And $$\int{1\over ax^2+bx+c}dx={2\over \sqrt{4ac-b^2}}\tan^{-1}{2ax+b\over \sqrt{4ac-b^2}}$$","I want to show that $$\int_{-\infty}^{\infty}{\left(x^2-x+\pi\over x^4-x^2+1\right)^2}dx=\pi+\pi^2+\pi^3$$ Expand $(x^4-x+\pi)^2=x^4-2x^3+2x^2-2x\pi+\pi{x^2}+\pi^2$ Let see (substitution of $y=x^2$) $$\int_{-\infty}^{\infty}{x\over (x^4-x^2+1)^2}dx={1\over 2}\int_{-\infty}^{\infty}{1\over (y^2-y+1)^2}dy$$ Substituion of $y=x^3$ $$\int_{-\infty}^{\infty}{x^3\over (x^4-x^2+1)^2}dx={1\over 4}\int_{-\infty}^{\infty}{1\over (y^2-y+1)^2}dy$$ As for $\int_{-\infty}^{\infty}{x^2\over (x^4-x^2+1)^2}dx$ and $\int_{-\infty}^{\infty}{x^4\over (x^4-x^2+1)^2}dx$ are difficult to find a suitable substitution. This is the point where I am shrugged with to find a suitable substitution To lead me to a particular standard integral. Need some help, thank. standard integral of the form $$\int{1\over (ax^2+bx+c)^2}dx={2ax+b\over (4ac-b^2)(ax^2+bx+c)}+{2a\over 4ac-b^2}\int{1\over ax^2+bx+c}dx$$ And $$\int{1\over ax^2+bx+c}dx={2\over \sqrt{4ac-b^2}}\tan^{-1}{2ax+b\over \sqrt{4ac-b^2}}$$",,"['calculus', 'integration']"
1,Good calculus exercises/problems?,Good calculus exercises/problems?,,"I can't enroll in a university this year, so I'm studying calculus at home, but the only exercises about calculus that I find are the easy ones. Do you know a great page where I can find not only calculus exercises, but problems as well? I want to find about: hard limits, derivatives and integrals, and some problems","I can't enroll in a university this year, so I'm studying calculus at home, but the only exercises about calculus that I find are the easy ones. Do you know a great page where I can find not only calculus exercises, but problems as well? I want to find about: hard limits, derivatives and integrals, and some problems",,"['calculus', 'reference-request']"
2,Proof of identity for $\pi$: $\frac{\pi}{3} = \frac{2}{\sqrt{2+\sqrt{3}}}\frac{2}{\sqrt{2+\sqrt{2+\sqrt{3}}}}\cdots$,Proof of identity for :,\pi \frac{\pi}{3} = \frac{2}{\sqrt{2+\sqrt{3}}}\frac{2}{\sqrt{2+\sqrt{2+\sqrt{3}}}}\cdots,"While browsing the internet today, I came across the following picture: (full image can be found here - credit to Цогтгэрэл Гантөмөр) Now, it would naturally seem we can extend this to an infinite product; specifically, I would guess from this post that we have $$\bbox[5px,border:2px solid red]{ \frac{\pi}{3} = \frac{2}{\sqrt{2+\sqrt{3}}}\frac{2}{\sqrt{2+\sqrt{2+\sqrt{3}}}}\frac{2}{\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{3}}}}} \cdots}$$ where we define this product formally by $$\frac{\pi}{3} = \prod_{n=1}^\infty \frac{2}{a_n} $$ where $a_n$ is defined by $a_n = \sqrt{2+a_{n-1}}$ with initial value $a_0 = \sqrt{3}.$ However, I have never seen this expression for $\pi$ in the literature. Given this product comes from a geometric argument bounding the value of $\pi,$ I would fully expect this product to be studied, but I do not know where to look. The most similar product I am aware of is $$\frac{2}{\pi} = \sqrt{\frac{1}{2}}\sqrt{\frac{1}{2} + \frac{1}{2}\sqrt{\frac{1}{2}}}\cdots$$ namely formula $(65)$ on this page . I would imagine that, like this similar looking formula for $\frac{2}{\pi},$ the above product for $\frac{\pi}{3}$ should be the result of evaluating some rational function of trigonometric functions, but how to go about this is not immediately clear to me. Is anyone aware of an existing proof or refutation of this product in the literature, or else can provide one? Edit 1: I have numerically checked this with the following quick and dirty Javascript code, which seems to imply convergence is decently rapid: a = Math.sqrt(3); piEstimate = 1; for (i = 0; i < 100; i++) {   a = Math.sqrt(2 + a);   piEstimate *= 2/a; } console.log(3 * piEstimate)","While browsing the internet today, I came across the following picture: (full image can be found here - credit to Цогтгэрэл Гантөмөр) Now, it would naturally seem we can extend this to an infinite product; specifically, I would guess from this post that we have where we define this product formally by where is defined by with initial value However, I have never seen this expression for in the literature. Given this product comes from a geometric argument bounding the value of I would fully expect this product to be studied, but I do not know where to look. The most similar product I am aware of is namely formula on this page . I would imagine that, like this similar looking formula for the above product for should be the result of evaluating some rational function of trigonometric functions, but how to go about this is not immediately clear to me. Is anyone aware of an existing proof or refutation of this product in the literature, or else can provide one? Edit 1: I have numerically checked this with the following quick and dirty Javascript code, which seems to imply convergence is decently rapid: a = Math.sqrt(3); piEstimate = 1; for (i = 0; i < 100; i++) {   a = Math.sqrt(2 + a);   piEstimate *= 2/a; } console.log(3 * piEstimate)","\bbox[5px,border:2px solid red]{ \frac{\pi}{3} = \frac{2}{\sqrt{2+\sqrt{3}}}\frac{2}{\sqrt{2+\sqrt{2+\sqrt{3}}}}\frac{2}{\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{3}}}}} \cdots} \frac{\pi}{3} = \prod_{n=1}^\infty \frac{2}{a_n}  a_n a_n = \sqrt{2+a_{n-1}} a_0 = \sqrt{3}. \pi \pi, \frac{2}{\pi} = \sqrt{\frac{1}{2}}\sqrt{\frac{1}{2} + \frac{1}{2}\sqrt{\frac{1}{2}}}\cdots (65) \frac{2}{\pi}, \frac{\pi}{3}","['calculus', 'trigonometry', 'pi', 'infinite-product']"
3,Is there a nice general formula for $\int \frac{dx}{x^n-1}$ and/or $\int \frac{dx}{\Phi_n(x)}$?,Is there a nice general formula for  and/or ?,\int \frac{dx}{x^n-1} \int \frac{dx}{\Phi_n(x)},"Question is in the title, where $\Phi_n$ denotes the $n$ -th cyclotomic polynomial . Motivation : I'm just teaching my calculus students basic integration of rational functions with $\log$ and $\arctan$ , so I wondered about this. Observation and more precise question : Pairing complex conjugates, over $\mathbb R$ these polynomials split into factors of the forms $x-1$ , $x+1$ , and $$f_{k,n}(x) = x^2-2\cos(2k\pi/n)x +1,$$ where the reciprocal of the last one, if I'm not mistaken, has antiderivative $$\frac{1}{\sin(2k\pi/n)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right).$$ Now using partial fractions and then integrating, one gets summands of the following types: A) $\log|x-1|$ B) $\log|x+1|$ C) $\frac{1}{\sin(2k\pi/n)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right)$ D) $\log (f_{k,n}(x))$ . (The last two, which come from taking apart something of the form $$\frac{\text{linear}}{f_{k,n}(x)},$$ are of course the only ones in $\int \frac{dx}{\Phi_n(x)}$ for $n\ge 3$ .) These bits I recognise in the example formulae below and I'm happy to have these as ""building blocks"" in a general formula. So I guess all I'm really asking for is a formula which gives the coefficients of each of the terms A)-D) in the integrals. In other words, we have $$\int \frac{dx}{x^n-1} = \color{red}{A_n} \cdot \log|x-1| + \color{red}{B_n} \cdot \log|x+1| + \sum_{1\le k <\frac{n}{2}}  \frac{\color{red}{C_{n,k}}}{\sin(2k\pi/n)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) + \sum_{1\le k <\frac{n}{2}} \color{red}{D_{n,k}} \log(f_{n,k})$$ and for $n \ge 3$ , $$ \int \frac{dx}{\Phi_n(x)} = \sum_{1\le k <\frac{n}{2}\\gcd(k,n)=1}  \frac{\color{red}{E_{n,k}}}{\sin(2k\pi/n)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) + \sum_{1\le k <\frac{n}{2}\\gcd(k,n)=1} \color{red}{F_{n,k}} \log(f_{n,k})$$ and I'm looking for closed formulae depending on $n$ and $k$ for the coefficients in red. Idea : I assume if one goes to $\mathbb C$ one just gets terms of the form $$ \log (x-\zeta)$$ where $\zeta$ runs through the respective roots of unity, and choosing the right branches of $\log$ and grouping complex conjugates together might give a formula for the coefficients. But going through this is beyond me at this point. Edit: From Eric Wofsey's answer one gets, by pairing complex conjugates and using the well-known relation ( 1 , 2 , 3 ) between arctan and complex logarithm (leaving out domain restriction and integration constants): $$\int \frac{dx}{x^n-1} = \color{red}{\frac{1}{n}} \cdot \log|x-1| + \color{blue}{(}(\color{red}{-\frac{1}{n}}) \cdot \log|x+1|\color{blue}{)} + \sum_{1\le k <\frac{n}{2}}  \color{red}{\frac{-2\sin(2k\pi/n)}{n}}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) + \sum_{1\le k <\frac{n}{2}} \color{red}{\frac{\cos(2k\pi/n)}{n}} \log(f_{n,k})$$ (where the term in $\color{blue}{\text{blue}}$ brackets appears iff $n$ is even), i.e my original normalisation of the arctan-term was bad, would give $C_{n,k} = -2\sin^2(2k\pi/n)/n$ ),; and for $n \ge 3$ , $$\int \frac{dx}{\Phi_n(x)} =   \sum_{1\le k <\frac{n}{2}\\gcd(k,n)=1}  \color{red}{-2\cdot\mathfrak{Im}\left(\frac{1}{\Phi'_n(\zeta_k)}\right)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) + \sum_{1\le k <\frac{n}{2}\\gcd(k,n)=1} \color{red}{\mathfrak{Re}\left(\frac{1}{\Phi'_n(\zeta_k)}\right)} \log(f_{n,k})$$ (where $\zeta_{k,n} = \exp(2k\pi i/n)$ ). Remains only the question if there is a nice general formula for the real/imaginary part of $\displaystyle \frac{1}{\Phi'_n(\zeta_k)}$ . I have asked that as a new question here .","Question is in the title, where denotes the -th cyclotomic polynomial . Motivation : I'm just teaching my calculus students basic integration of rational functions with and , so I wondered about this. Observation and more precise question : Pairing complex conjugates, over these polynomials split into factors of the forms , , and where the reciprocal of the last one, if I'm not mistaken, has antiderivative Now using partial fractions and then integrating, one gets summands of the following types: A) B) C) D) . (The last two, which come from taking apart something of the form are of course the only ones in for .) These bits I recognise in the example formulae below and I'm happy to have these as ""building blocks"" in a general formula. So I guess all I'm really asking for is a formula which gives the coefficients of each of the terms A)-D) in the integrals. In other words, we have and for , and I'm looking for closed formulae depending on and for the coefficients in red. Idea : I assume if one goes to one just gets terms of the form where runs through the respective roots of unity, and choosing the right branches of and grouping complex conjugates together might give a formula for the coefficients. But going through this is beyond me at this point. Edit: From Eric Wofsey's answer one gets, by pairing complex conjugates and using the well-known relation ( 1 , 2 , 3 ) between arctan and complex logarithm (leaving out domain restriction and integration constants): (where the term in brackets appears iff is even), i.e my original normalisation of the arctan-term was bad, would give ),; and for , (where ). Remains only the question if there is a nice general formula for the real/imaginary part of . I have asked that as a new question here .","\Phi_n n \log \arctan \mathbb R x-1 x+1 f_{k,n}(x) = x^2-2\cos(2k\pi/n)x +1, \frac{1}{\sin(2k\pi/n)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right). \log|x-1| \log|x+1| \frac{1}{\sin(2k\pi/n)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) \log (f_{k,n}(x)) \frac{\text{linear}}{f_{k,n}(x)}, \int \frac{dx}{\Phi_n(x)} n\ge 3 \int \frac{dx}{x^n-1} = \color{red}{A_n} \cdot \log|x-1| + \color{red}{B_n} \cdot \log|x+1| + \sum_{1\le k <\frac{n}{2}}  \frac{\color{red}{C_{n,k}}}{\sin(2k\pi/n)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) + \sum_{1\le k <\frac{n}{2}} \color{red}{D_{n,k}} \log(f_{n,k}) n \ge 3  \int \frac{dx}{\Phi_n(x)} = \sum_{1\le k <\frac{n}{2}\\gcd(k,n)=1}  \frac{\color{red}{E_{n,k}}}{\sin(2k\pi/n)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) + \sum_{1\le k <\frac{n}{2}\\gcd(k,n)=1} \color{red}{F_{n,k}} \log(f_{n,k}) n k \mathbb C  \log (x-\zeta) \zeta \log \int \frac{dx}{x^n-1} = \color{red}{\frac{1}{n}} \cdot \log|x-1| + \color{blue}{(}(\color{red}{-\frac{1}{n}}) \cdot \log|x+1|\color{blue}{)} + \sum_{1\le k <\frac{n}{2}}  \color{red}{\frac{-2\sin(2k\pi/n)}{n}}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) + \sum_{1\le k <\frac{n}{2}} \color{red}{\frac{\cos(2k\pi/n)}{n}} \log(f_{n,k}) \color{blue}{\text{blue}} n C_{n,k} = -2\sin^2(2k\pi/n)/n n \ge 3 \int \frac{dx}{\Phi_n(x)} = 
 \sum_{1\le k <\frac{n}{2}\\gcd(k,n)=1}  \color{red}{-2\cdot\mathfrak{Im}\left(\frac{1}{\Phi'_n(\zeta_k)}\right)}\arctan\left(\frac{x-\cos(2k\pi/n)}{\sin(2k\pi/n)}\right) + \sum_{1\le k <\frac{n}{2}\\gcd(k,n)=1} \color{red}{\mathfrak{Re}\left(\frac{1}{\Phi'_n(\zeta_k)}\right)} \log(f_{n,k}) \zeta_{k,n} = \exp(2k\pi i/n) \displaystyle \frac{1}{\Phi'_n(\zeta_k)}","['calculus', 'integration', 'complex-analysis', 'cyclotomic-polynomials']"
4,"Closed form of $\int_0^{\pi/2} \frac{\arctan^2 (\sin^2 \theta)}{\sin^2 \theta}\,d\theta$",Closed form of,"\int_0^{\pi/2} \frac{\arctan^2 (\sin^2 \theta)}{\sin^2 \theta}\,d\theta","I'm trying to evaluate the closed form of: $$I =\int_0^{\pi/2} \frac{\arctan^2 (\sin^2 \theta)}{\sin^2 \theta}\,d\theta$$ So far I've tried introducing a parameters, $\displaystyle I(a,b) = \int_0^{\pi/2} \frac{\arctan (a\sin^2 \theta)\arctan (b\sin^2 \theta)}{\sin^2 \theta}\,d\theta$ but that doesn't lead to an integral I can manage. Expanding the series for $\arctan^2 x$ leads to the sum: $$I = \frac{\pi}{2}\sum\limits_{n=0}^{\infty}\frac{(-1)^n}{(n+1)4^{2n+1}}\binom{4n+2}{2n+1}\left(\sum\limits_{k=0}^{n}\frac{1}{2k+1}\right)$$ and using $\displaystyle \int_0^1 x^{n-\frac{1}{2}}\log (1-x)\,dx = \frac{-2\log 2 + 2\sum\limits_{k=0}^{n}\dfrac{1}{2k+1}}{n+\frac{1}{2}}$ leads to an even uglier integral: $\displaystyle \Im \int_{0}^{1} \frac{1}{1+\sqrt{1-i\sqrt{x}}}\frac{\log(1-x)}{x}\,dx$ among others. I got the non-square version, which seems to have a nice closed form $\displaystyle \int_0^{\pi/2} \frac{\arctan (\sin^2 \theta)}{\sin^2 \theta}\,d\theta = \frac{\pi}{\sqrt{2}\sqrt{1+\sqrt{2}}}$ but the squared version seems difficult. Any help is appreciated. (P.S. - I'm not familiar with Hypergeometric identities, so it would be very helpful if a proof or a reference to a proof was provided, should the need arise to use them.)","I'm trying to evaluate the closed form of: $$I =\int_0^{\pi/2} \frac{\arctan^2 (\sin^2 \theta)}{\sin^2 \theta}\,d\theta$$ So far I've tried introducing a parameters, $\displaystyle I(a,b) = \int_0^{\pi/2} \frac{\arctan (a\sin^2 \theta)\arctan (b\sin^2 \theta)}{\sin^2 \theta}\,d\theta$ but that doesn't lead to an integral I can manage. Expanding the series for $\arctan^2 x$ leads to the sum: $$I = \frac{\pi}{2}\sum\limits_{n=0}^{\infty}\frac{(-1)^n}{(n+1)4^{2n+1}}\binom{4n+2}{2n+1}\left(\sum\limits_{k=0}^{n}\frac{1}{2k+1}\right)$$ and using $\displaystyle \int_0^1 x^{n-\frac{1}{2}}\log (1-x)\,dx = \frac{-2\log 2 + 2\sum\limits_{k=0}^{n}\dfrac{1}{2k+1}}{n+\frac{1}{2}}$ leads to an even uglier integral: $\displaystyle \Im \int_{0}^{1} \frac{1}{1+\sqrt{1-i\sqrt{x}}}\frac{\log(1-x)}{x}\,dx$ among others. I got the non-square version, which seems to have a nice closed form $\displaystyle \int_0^{\pi/2} \frac{\arctan (\sin^2 \theta)}{\sin^2 \theta}\,d\theta = \frac{\pi}{\sqrt{2}\sqrt{1+\sqrt{2}}}$ but the squared version seems difficult. Any help is appreciated. (P.S. - I'm not familiar with Hypergeometric identities, so it would be very helpful if a proof or a reference to a proof was provided, should the need arise to use them.)",,"['calculus', 'integration', 'definite-integrals']"
5,Compute polylog of order $3$ at $\frac{1}{2}$,Compute polylog of order  at,3 \frac{1}{2},"How to compute the following series: $$\sum_{n=1}^{\infty}\frac{1}{2^nn^3}$$ I am aware this equals polylog of order  $3$ at $\frac{1}{2}$ or $\operatorname{Li}_3\left(\frac{1}{2}\right)$, but how to prove it using integral or Euler sum only (without using any polylog identities)? I know how to prove $$\sum_{n=1}^{\infty}\frac{1}{2^nn^2}$$ or dilogarithm at $\frac{1}{2}$ like answer provided by Tunk-Fey in my previous OP , but I do not know how to use that fact to compute the polylog of order $3$ at $\frac{1}{2}$. My instructor told me to use geometric series yet I can't figure it out that clue. Any idea how to compute it without using using polylog identity (integral or infinite sum only)? Any help would be appreciated. Thanks in advance.","How to compute the following series: $$\sum_{n=1}^{\infty}\frac{1}{2^nn^3}$$ I am aware this equals polylog of order  $3$ at $\frac{1}{2}$ or $\operatorname{Li}_3\left(\frac{1}{2}\right)$, but how to prove it using integral or Euler sum only (without using any polylog identities)? I know how to prove $$\sum_{n=1}^{\infty}\frac{1}{2^nn^2}$$ or dilogarithm at $\frac{1}{2}$ like answer provided by Tunk-Fey in my previous OP , but I do not know how to use that fact to compute the polylog of order $3$ at $\frac{1}{2}$. My instructor told me to use geometric series yet I can't figure it out that clue. Any idea how to compute it without using using polylog identity (integral or infinite sum only)? Any help would be appreciated. Thanks in advance.",,"['calculus', 'integration', 'sequences-and-series', 'special-functions', 'polylogarithm']"
6,"Need help with $\int_0^\infty\left(\pi\,x+\frac{S(x)\cos\frac{\pi x^2}2-C(x)\sin\frac{\pi x^2}2}{S(x)^2+C(x)^2}\right)dx$",Need help with,"\int_0^\infty\left(\pi\,x+\frac{S(x)\cos\frac{\pi x^2}2-C(x)\sin\frac{\pi x^2}2}{S(x)^2+C(x)^2}\right)dx","Let $$I=\int_0^\infty\left(\pi\,x+\frac{S(x)\cos\frac{\pi x^2}2-C(x)\sin\frac{\pi x^2}2}{S(x)^2+C(x)^2}\right)dx,\tag1$$ where $$S(x)=-\frac12+\int_0^x\sin\frac{\pi t^2}2dt,\tag2$$ $$C(s)=-\frac12+\int_0^x\cos\frac{\pi t^2}2dt\tag3$$ are shifted Fresnel integrals . Mathematica and Maple return the integral $I$ unevaluated. Numerical integration suggests that $$I\stackrel?=-\frac\pi4,\tag4$$ but I was not able to prove it. So, I ask for your help with this problem.","Let $$I=\int_0^\infty\left(\pi\,x+\frac{S(x)\cos\frac{\pi x^2}2-C(x)\sin\frac{\pi x^2}2}{S(x)^2+C(x)^2}\right)dx,\tag1$$ where $$S(x)=-\frac12+\int_0^x\sin\frac{\pi t^2}2dt,\tag2$$ $$C(s)=-\frac12+\int_0^x\cos\frac{\pi t^2}2dt\tag3$$ are shifted Fresnel integrals . Mathematica and Maple return the integral $I$ unevaluated. Numerical integration suggests that $$I\stackrel?=-\frac\pi4,\tag4$$ but I was not able to prove it. So, I ask for your help with this problem.",,"['calculus', 'integration', 'special-functions', 'definite-integrals', 'fresnel-integrals']"
7,Can Sturm-Liouville theory actually solve ODEs?,Can Sturm-Liouville theory actually solve ODEs?,,"My teacher talked about Sturm-Liouville theory, and we learned that any second order differential equation can be put into the self-adjoint form. What is this? Well, the book says is something in the form: $$\frac{d}{dx}\left[p(x)\frac{du(x)}{dx}\right] + q(x)u(x) \tag {1}$$ The book also says that we can put any second order linear ODE into the self-adjoint form: Consider the second order linear ODE: $$Lu(x) = p_0\frac{d^2}{dx^2}u(x) + p_1(x)\frac{d}{dx}u(x) + p_2(x)u(x)$$ Multiply it by: $$\frac{1}{p_0(x)}\exp\left[\int^x\frac{p_1(t)}{p_0(t)}\ dt\right]$$ to obtain: $$\frac{1}{p_0(x)}\exp\left[\int^x\frac{p_1(t)}{p_0(t)}\ dt\right]Lu(x) = \frac{d}{dx}\left\{\exp\left[\int^x\frac{p_1(t)}{p_0(t)}\  dt\right]\frac{du(x)}{dx}\right\} + \frac{p_2(x)}{p_0(x)}\cdot \exp\left[\int^x\frac{p_1(t)}{p_0(t)} \ dt\right]u$$ Which is in the self-adjoint form, whatever that means. We can see by comparing with eq $(1)$ and taking $p(x) = \exp\left[\int^x\frac{p_1(t)}{p_0(t)}\  dt\right]$ and $q(x) = \frac{p_2(x)}{p_0(x)}\cdot \exp\left[\int^x\frac{p_1(t)}{p_0(t)} \ dt\right]$. Then the book, and also my teacher, says that we're interested in finding eigenfunctions and eigenvalues for: $$\frac{d}{dx}\left[p(x)\frac{du(x)}{dx}\right] + q(x)u(x)  + \lambda w(x) u(x) = \\ Lu(x) + \lambda w(x) u(x) $$ for some density function $w(x)$ (what), where $\lambda$ is a eigenvalue related to the eigenfunction $u(x)$, called in the book $u_{\lambda}$ because it depends on $\lambda$. The book and my teacher then talk about completeness of eigenvalues and eigenfunctions, and that's it. Nothing more is said about Sturm-Liouville theory, then we jumped to other things and I'm asking for what all this is used. It also mentions boundary conditions. What are boundary conditions ? Are them simply the initial conditions for some ODE? Because the boundary conditions given in the book are of the form $$a_1 u(a) + a_2 u'(a) = 0\\ b_1 u(a) + b_2 u'(a) = 0$$ but aren't initial conditions things in the form $u'(a) = b, u''(a) = c$? When I see examples on the wikipedia , they all mention that we already know that such eigenfunction $u(x)$ is a solution with eigenvalue $\lambda$, and also in these slides I see no examples of how to actually finding the eigenvalue and eigenfunction... So what is the usefulness of the self-adjoint formula and of the sturm-liouville theory in general?","My teacher talked about Sturm-Liouville theory, and we learned that any second order differential equation can be put into the self-adjoint form. What is this? Well, the book says is something in the form: $$\frac{d}{dx}\left[p(x)\frac{du(x)}{dx}\right] + q(x)u(x) \tag {1}$$ The book also says that we can put any second order linear ODE into the self-adjoint form: Consider the second order linear ODE: $$Lu(x) = p_0\frac{d^2}{dx^2}u(x) + p_1(x)\frac{d}{dx}u(x) + p_2(x)u(x)$$ Multiply it by: $$\frac{1}{p_0(x)}\exp\left[\int^x\frac{p_1(t)}{p_0(t)}\ dt\right]$$ to obtain: $$\frac{1}{p_0(x)}\exp\left[\int^x\frac{p_1(t)}{p_0(t)}\ dt\right]Lu(x) = \frac{d}{dx}\left\{\exp\left[\int^x\frac{p_1(t)}{p_0(t)}\  dt\right]\frac{du(x)}{dx}\right\} + \frac{p_2(x)}{p_0(x)}\cdot \exp\left[\int^x\frac{p_1(t)}{p_0(t)} \ dt\right]u$$ Which is in the self-adjoint form, whatever that means. We can see by comparing with eq $(1)$ and taking $p(x) = \exp\left[\int^x\frac{p_1(t)}{p_0(t)}\  dt\right]$ and $q(x) = \frac{p_2(x)}{p_0(x)}\cdot \exp\left[\int^x\frac{p_1(t)}{p_0(t)} \ dt\right]$. Then the book, and also my teacher, says that we're interested in finding eigenfunctions and eigenvalues for: $$\frac{d}{dx}\left[p(x)\frac{du(x)}{dx}\right] + q(x)u(x)  + \lambda w(x) u(x) = \\ Lu(x) + \lambda w(x) u(x) $$ for some density function $w(x)$ (what), where $\lambda$ is a eigenvalue related to the eigenfunction $u(x)$, called in the book $u_{\lambda}$ because it depends on $\lambda$. The book and my teacher then talk about completeness of eigenvalues and eigenfunctions, and that's it. Nothing more is said about Sturm-Liouville theory, then we jumped to other things and I'm asking for what all this is used. It also mentions boundary conditions. What are boundary conditions ? Are them simply the initial conditions for some ODE? Because the boundary conditions given in the book are of the form $$a_1 u(a) + a_2 u'(a) = 0\\ b_1 u(a) + b_2 u'(a) = 0$$ but aren't initial conditions things in the form $u'(a) = b, u''(a) = c$? When I see examples on the wikipedia , they all mention that we already know that such eigenfunction $u(x)$ is a solution with eigenvalue $\lambda$, and also in these slides I see no examples of how to actually finding the eigenvalue and eigenfunction... So what is the usefulness of the self-adjoint formula and of the sturm-liouville theory in general?",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'physics', 'sturm-liouville']"
8,Maclaurin series expansion for $e^{-1/x^2}$,Maclaurin series expansion for,e^{-1/x^2},"I am currently extremely confused on how to proceed with the Maclaurin series expansion for my current function. I got my derivatives and I got my formula, however, plugging them in gives me a non-possible answers since division by $0$ is not possible.","I am currently extremely confused on how to proceed with the Maclaurin series expansion for my current function. I got my derivatives and I got my formula, however, plugging them in gives me a non-possible answers since division by $0$ is not possible.",,"['calculus', 'taylor-expansion']"
9,Comparison theorem for ODE,Comparison theorem for ODE,,"Here is something I'm trying to prove: Conjecture: Suppose $f'(x) \leq \phi(f(x), x)$ and $f(a)=\alpha$. Suppose $g'(x)=\phi(g(x),x)$ and $g(a)\geq \alpha$. Then $f(x)\leq  g(x)\,\,\forall x$. It definitely seems like it should be true, and I don't think we even need continuity of $\phi$. ( Edit: a user has correctly pointed out that we should require $\phi$ be locally Lipshitz in the first variable, uniformly with respect to the second variable. Let's add that assumption.) I can prove it in the case that the inequality is strict. I'll place my proof below the fold. How can I extend it to weak inequality? If it's wrong, I'd love to see a counterexample. A reference is fine; I have the book by Teschl, for example. A similar question with stronger assumptions was asked here . Proposition: Suppose Suppose $f'(x) < \phi(f(x), x)$ and $f(a)=\alpha$. Suppose $g'(x)=\phi(g(x),x)$ and $g(a)\geq \alpha$. Then $f(x)\leq  g(x)\,\,\forall x$. Proof: Suppose not. Suppose WLOG there is a $b>a$ such that $f(b)>g(b)$. Let $c:=\inf \{x>a:f(x)>g(x)\}$. By definition of the derivative we have $f'(c)\geq g'(c)$, a contradiction, since $f'(c)<\phi(f(c),c)=\phi(g(c),c))=g'(c)$.","Here is something I'm trying to prove: Conjecture: Suppose $f'(x) \leq \phi(f(x), x)$ and $f(a)=\alpha$. Suppose $g'(x)=\phi(g(x),x)$ and $g(a)\geq \alpha$. Then $f(x)\leq  g(x)\,\,\forall x$. It definitely seems like it should be true, and I don't think we even need continuity of $\phi$. ( Edit: a user has correctly pointed out that we should require $\phi$ be locally Lipshitz in the first variable, uniformly with respect to the second variable. Let's add that assumption.) I can prove it in the case that the inequality is strict. I'll place my proof below the fold. How can I extend it to weak inequality? If it's wrong, I'd love to see a counterexample. A reference is fine; I have the book by Teschl, for example. A similar question with stronger assumptions was asked here . Proposition: Suppose Suppose $f'(x) < \phi(f(x), x)$ and $f(a)=\alpha$. Suppose $g'(x)=\phi(g(x),x)$ and $g(a)\geq \alpha$. Then $f(x)\leq  g(x)\,\,\forall x$. Proof: Suppose not. Suppose WLOG there is a $b>a$ such that $f(b)>g(b)$. Let $c:=\inf \{x>a:f(x)>g(x)\}$. By definition of the derivative we have $f'(c)\geq g'(c)$, a contradiction, since $f'(c)<\phi(f(c),c)=\phi(g(c),c))=g'(c)$.",,"['calculus', 'ordinary-differential-equations', 'inequality']"
10,How does one prove $\int_0^\infty \prod_{k=1}^\infty \operatorname{\rm sinc}\left( \frac{t}{2^{k+1}} \right) \mathrm{d} t = 2 \pi$,How does one prove,\int_0^\infty \prod_{k=1}^\infty \operatorname{\rm sinc}\left( \frac{t}{2^{k+1}} \right) \mathrm{d} t = 2 \pi,"Looking into the distribution of a Fabius random variable : $$     X := \sum_{k=1}^\infty 2^{-k} u_k $$ where $u_k$ are i.i.d. uniform variables on a unit interval, I encountered the following expression for its probability density: $$    f_X(x) = \frac{1}{\pi} \int_0^\infty \left( \prod_{k=1}^\infty \operatorname{\rm sinc}\left( \frac{t}{2^{k+1}} \right) \right) \cos \left( t \left( x- \frac{1}{2} \right) \right) \mathrm{d} t $$ It seems, numerically, that $f\left(\frac{1}{2} \right) = 2$, but my several attempts to prove this were not successful. Any ideas how to approach this are much appreciated.","Looking into the distribution of a Fabius random variable : $$     X := \sum_{k=1}^\infty 2^{-k} u_k $$ where $u_k$ are i.i.d. uniform variables on a unit interval, I encountered the following expression for its probability density: $$    f_X(x) = \frac{1}{\pi} \int_0^\infty \left( \prod_{k=1}^\infty \operatorname{\rm sinc}\left( \frac{t}{2^{k+1}} \right) \right) \cos \left( t \left( x- \frac{1}{2} \right) \right) \mathrm{d} t $$ It seems, numerically, that $f\left(\frac{1}{2} \right) = 2$, but my several attempts to prove this were not successful. Any ideas how to approach this are much appreciated.",,"['calculus', 'probability-distributions', 'definite-integrals']"
11,Need help with calculating this sum: $\sum_{n=0}^\infty\frac{1}{2^n}\tan\frac{1}{2^n}$,Need help with calculating this sum:,\sum_{n=0}^\infty\frac{1}{2^n}\tan\frac{1}{2^n},I need help with calculating this sum: $$\sum_{n=0}^\infty\frac{1}{2^n}\tan\frac{1}{2^n}$$,I need help with calculating this sum: $$\sum_{n=0}^\infty\frac{1}{2^n}\tan\frac{1}{2^n}$$,,"['calculus', 'sequences-and-series', 'trigonometry', 'closed-form', 'trigonometric-series']"
12,Prove that $\lim_{a \to 0^{+}} \int_{0}^{a} \frac{1}{\sqrt{\cos(x)-\cos(a)}} \;dx=\frac{\pi}{\sqrt{2}}$,Prove that,\lim_{a \to 0^{+}} \int_{0}^{a} \frac{1}{\sqrt{\cos(x)-\cos(a)}} \;dx=\frac{\pi}{\sqrt{2}},How can I prove that $$\lim_{a \to 0^{+}} \int_{0}^{a} \frac{1}{\sqrt{\cos(x)-\cos(a)}} \;dx=\frac{\pi}{\sqrt{2}}$$,How can I prove that $$\lim_{a \to 0^{+}} \int_{0}^{a} \frac{1}{\sqrt{\cos(x)-\cos(a)}} \;dx=\frac{\pi}{\sqrt{2}}$$,,"['calculus', 'integration', 'limits']"
13,Evaluate $\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}$. [duplicate],Evaluate . [duplicate],\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1},"This question already has answers here : Evaluating the series $\sum\limits_{n=1}^{\infty} \frac{1}{n^{3} \binom{2n}{n}} $ (2 answers) Closed 2 years ago . Evaluate $$\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}.$$ My work so far and background to the problem. This question was inspired by the second page of this paper. The author of the paper doesn't mention how he managed to prove the $4$ identities (shown below) at the top of the second page of the paper, so I tried to find my own method of doing so. \begin{align} \sum\limits_{n=1}^{\infty}\binom{2n}{n}^{-1} &= \frac{1}{3}+\frac{2\pi\sqrt3}{27} \\ \sum\limits_{n=1}^{\infty}\frac{1}{n}\binom{2n}{n}^{-1} &= \frac{\pi\sqrt3}{9} \\ \sum\limits_{n=1}^{\infty}\frac{1}{n^2}\binom{2n}{n}^{-1} &= \frac{\pi^2}{18} \\ \sum\limits_{n=1}^{\infty}\frac{1}{n^4}\binom{2n}{n}^{-1} &= \frac{17\pi^4}{3240} \end{align} The method I used was similar to the method the author used in the rest of his paper: trying to find the generating functions for particular sequences. Using this method, like the author I was successful in proving the first $3$ identities, since I obtained the following: $$\begin{align}\sum\limits_{n=1}^{\infty}\binom{2n}{n}^{-1}x^n&=4\sqrt\frac{x}{(4-x)^3}\arcsin\frac{\sqrt{x}}{2}+\frac{x}{4-x}\\ \sum\limits_{n=1}^{\infty}\frac{1}{n}\binom{2n}{n}^{-1}x^n&=2\sqrt{\frac{x}{4-x}}\arcsin\frac{\sqrt x}{2}\\ \sum\limits_{n=1}^{\infty}\frac{1}{n^2}\binom{2n}{n}^{-1}x^n&=2\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\end{align}$$ I obtained the second and third power series by dividing the power series above each one by $x$ and then integrating. I then attempted to find a closed form for $\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}x^n$ in order to evaluate $\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}$ . The way I tried to do this was finding $$\int\frac{2}{x}\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\mathrm{d}x$$ but according to WA this has no solution in terms of elementary functions and it involves some polylogarithms which I am not familiar with at all really, especially when they have a complex argument. I then realized that $$\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}=\int_0^1\frac{2}{x}\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\mathrm{d}x=\int_0^{1/2}\frac{4}{u}\left(\arcsin u\right)^2\mathrm{d}u$$ (where the second equality is found by using the substitution $u=\frac{\sqrt{x}}{2}$ ) which could be more helpful since definite integrals can often be evaluated in terms of elementary functions even if the indefinite integral cannot. From there however I could not think of a way of continuing. Thank you for your help.","This question already has answers here : Evaluating the series $\sum\limits_{n=1}^{\infty} \frac{1}{n^{3} \binom{2n}{n}} $ (2 answers) Closed 2 years ago . Evaluate My work so far and background to the problem. This question was inspired by the second page of this paper. The author of the paper doesn't mention how he managed to prove the identities (shown below) at the top of the second page of the paper, so I tried to find my own method of doing so. The method I used was similar to the method the author used in the rest of his paper: trying to find the generating functions for particular sequences. Using this method, like the author I was successful in proving the first identities, since I obtained the following: I obtained the second and third power series by dividing the power series above each one by and then integrating. I then attempted to find a closed form for in order to evaluate . The way I tried to do this was finding but according to WA this has no solution in terms of elementary functions and it involves some polylogarithms which I am not familiar with at all really, especially when they have a complex argument. I then realized that (where the second equality is found by using the substitution ) which could be more helpful since definite integrals can often be evaluated in terms of elementary functions even if the indefinite integral cannot. From there however I could not think of a way of continuing. Thank you for your help.","\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}. 4 \begin{align}
\sum\limits_{n=1}^{\infty}\binom{2n}{n}^{-1} &= \frac{1}{3}+\frac{2\pi\sqrt3}{27} \\
\sum\limits_{n=1}^{\infty}\frac{1}{n}\binom{2n}{n}^{-1} &= \frac{\pi\sqrt3}{9} \\
\sum\limits_{n=1}^{\infty}\frac{1}{n^2}\binom{2n}{n}^{-1} &= \frac{\pi^2}{18} \\
\sum\limits_{n=1}^{\infty}\frac{1}{n^4}\binom{2n}{n}^{-1} &= \frac{17\pi^4}{3240}
\end{align} 3 \begin{align}\sum\limits_{n=1}^{\infty}\binom{2n}{n}^{-1}x^n&=4\sqrt\frac{x}{(4-x)^3}\arcsin\frac{\sqrt{x}}{2}+\frac{x}{4-x}\\
\sum\limits_{n=1}^{\infty}\frac{1}{n}\binom{2n}{n}^{-1}x^n&=2\sqrt{\frac{x}{4-x}}\arcsin\frac{\sqrt x}{2}\\
\sum\limits_{n=1}^{\infty}\frac{1}{n^2}\binom{2n}{n}^{-1}x^n&=2\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\end{align} x \sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}x^n \sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1} \int\frac{2}{x}\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\mathrm{d}x \sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}=\int_0^1\frac{2}{x}\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\mathrm{d}x=\int_0^{1/2}\frac{4}{u}\left(\arcsin u\right)^2\mathrm{d}u u=\frac{\sqrt{x}}{2}","['calculus', 'integration', 'sequences-and-series', 'summation', 'closed-form']"
14,"Why $\lim_{\varepsilon \to 0} \varepsilon \int_0^\infty \frac{z\,dz}{(z-1)^2 + \varepsilon^2z^3} = \pi$?",Why ?,"\lim_{\varepsilon \to 0} \varepsilon \int_0^\infty \frac{z\,dz}{(z-1)^2 + \varepsilon^2z^3} = \pi","I need help evaluating the following limit: $$ \lim_{\varepsilon \to 0}\left[2\varepsilon\int_{0}^{\infty} \frac{x^{3}\,\mathrm{d}x} {\left(x^{2} - \varepsilon^{2}\right)^{2} + x^{6}}\right] $$ Making the substitutions $y = x^{2}$ and $z = y/\varepsilon^{2}$ I was able to put the integral in a form that seems more tractable: $$ \lim_{\varepsilon \to 0}\left[\varepsilon \int_{0}^{\infty}\frac{z\,\mathrm{d}z}{\left(z - 1\right)^{2} + \varepsilon^{2}z^{3}}\right] $$ I have two reasons to think that the limit evaluates to $\pi$ : The first one is that it's necessary for the result I'm trying to reach in a physics problem, which obviously isn't a very good justification. The second one ist that I made a numerical calculation that gave me a result very close to $\pi$ : I restricted the integral to the interval $[0,10]$ , because the integrand has a very sharp peak at $z=1$ . Then, I divided that interval in $2 \cdot 10^{6}$ subintervals and used Simpson's method to calculate the integral. With $\varepsilon = 10^{-4}$ I got the result $3.1417$ . I tried to calculate it using the residues method, but I couldn't find the roots of the third degree polynomial in the denominator. Does anyone have an idea on how to evaluate the limit analytically? Any help is appreciated.","I need help evaluating the following limit: Making the substitutions and I was able to put the integral in a form that seems more tractable: I have two reasons to think that the limit evaluates to : The first one is that it's necessary for the result I'm trying to reach in a physics problem, which obviously isn't a very good justification. The second one ist that I made a numerical calculation that gave me a result very close to : I restricted the integral to the interval , because the integrand has a very sharp peak at . Then, I divided that interval in subintervals and used Simpson's method to calculate the integral. With I got the result . I tried to calculate it using the residues method, but I couldn't find the roots of the third degree polynomial in the denominator. Does anyone have an idea on how to evaluate the limit analytically? Any help is appreciated.","
\lim_{\varepsilon \to 0}\left[2\varepsilon\int_{0}^{\infty} \frac{x^{3}\,\mathrm{d}x}
{\left(x^{2} - \varepsilon^{2}\right)^{2} + x^{6}}\right]
 y = x^{2} z = y/\varepsilon^{2} 
\lim_{\varepsilon \to 0}\left[\varepsilon
\int_{0}^{\infty}\frac{z\,\mathrm{d}z}{\left(z - 1\right)^{2} + \varepsilon^{2}z^{3}}\right]
 \pi \pi [0,10] z=1 2 \cdot 10^{6} \varepsilon = 10^{-4} 3.1417","['calculus', 'integration', 'limits']"
15,Is indefinite integration largely a heuristic or it can be mechanical too?,Is indefinite integration largely a heuristic or it can be mechanical too?,,"I am sorry for putting multiple questions in the same post, but I think providing answers here will be better. As far as I know, there is no 'product formula' for integrals, like we have for the derivative. Also, I can be wrong, but I think a general class of functions, differing by a constant, have the same derivative. So, ignoring the constant, one might think that such a product formula might exist. So, my first question: Can it be proved that there is no 'product formula' for integrals, or   is it just that it has not been discovered yet? Let us reduce our case to just rational functions. Partial fractions for integration is a pretty good technique, but I think it can't be used for all rational functions, because not all polynomials have all real roots. So: Is there a technique/algorithm to integrate all rational functions? Another open-ended question, that I think of, is that are there any techniques/formulas for integrals not discovered yet, or are the existence of these techniques/formulas been proven false by some theorem? Answer to any question is suffice. EDIT FROM HERE After reading some of the answers, I felt I need to be more precise. The integration by parts formula, as far as I know, is again a heuristic, and not mechanical. So, there is no scope for integration by parts theorem to be such a product formula. Another user answered that I thought of a formula combining functions in an elementary way, and proposed that it is well known that the sinc function does not have a closed form integral, and so such a formula doesn't exist. But, to add to this, there is also a possibility that such a formula may produce an undefined result, or some weird result, which we can relate to the absence of such a closed-form solution. What I am looking for is a theorem or result which clearly proves the non-existence of such a formula, taking into account all possibilities.","I am sorry for putting multiple questions in the same post, but I think providing answers here will be better. As far as I know, there is no 'product formula' for integrals, like we have for the derivative. Also, I can be wrong, but I think a general class of functions, differing by a constant, have the same derivative. So, ignoring the constant, one might think that such a product formula might exist. So, my first question: Can it be proved that there is no 'product formula' for integrals, or   is it just that it has not been discovered yet? Let us reduce our case to just rational functions. Partial fractions for integration is a pretty good technique, but I think it can't be used for all rational functions, because not all polynomials have all real roots. So: Is there a technique/algorithm to integrate all rational functions? Another open-ended question, that I think of, is that are there any techniques/formulas for integrals not discovered yet, or are the existence of these techniques/formulas been proven false by some theorem? Answer to any question is suffice. EDIT FROM HERE After reading some of the answers, I felt I need to be more precise. The integration by parts formula, as far as I know, is again a heuristic, and not mechanical. So, there is no scope for integration by parts theorem to be such a product formula. Another user answered that I thought of a formula combining functions in an elementary way, and proposed that it is well known that the sinc function does not have a closed form integral, and so such a formula doesn't exist. But, to add to this, there is also a possibility that such a formula may produce an undefined result, or some weird result, which we can relate to the absence of such a closed-form solution. What I am looking for is a theorem or result which clearly proves the non-existence of such a formula, taking into account all possibilities.",,"['calculus', 'integration', 'soft-question']"
16,What does it mean for a function to be holomorphic?,What does it mean for a function to be holomorphic?,,"I am trying to wrap my head around the definition of holomorphic. Wikipedia tells me that: A holomorphic function is a complex-valued function of one or more complex variables that is complex differentiable in a neighborhood of every point in its domain. Can you put this is simpler terms or illustrate it with an example? Also, it seems very similar to the definition of analytic, are they equivalent?","I am trying to wrap my head around the definition of holomorphic. Wikipedia tells me that: A holomorphic function is a complex-valued function of one or more complex variables that is complex differentiable in a neighborhood of every point in its domain. Can you put this is simpler terms or illustrate it with an example? Also, it seems very similar to the definition of analytic, are they equivalent?",,"['calculus', 'complex-analysis', 'complex-numbers', 'definition']"
17,"Integral $\int\!\sqrt{\cot x}\,dx $",Integral,"\int\!\sqrt{\cot x}\,dx ","Find the integral $$\int\!\sqrt{\cot x}\,dx $$ How can one solve this using substitution? Can this be solved by complex methods?","Find the integral $$\int\!\sqrt{\cot x}\,dx $$ How can one solve this using substitution? Can this be solved by complex methods?",,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals']"
18,$\sum_{n=1}^\infty\frac{n}{(2n-1)16^n}\binom{2n}{n}^2\left(\sum_{k=n}^\infty\frac{2^k}{k\binom{2k}{k}}\right)=1-\sqrt2+\log(1+\sqrt2).$,,\sum_{n=1}^\infty\frac{n}{(2n-1)16^n}\binom{2n}{n}^2\left(\sum_{k=n}^\infty\frac{2^k}{k\binom{2k}{k}}\right)=1-\sqrt2+\log(1+\sqrt2).,"Prove: $$\sum_{n=1}^\infty\frac{n}{(2n-1)16^n}\binom{2n}{n}^2\left(\sum_{k=n}^\infty\frac{2^k}{k\binom{2k}{k}}\right)=1-\sqrt2+\log(1+\sqrt2).$$ I'm sorry that I don't even know how to start. I haven't met this kind of series before. I've learnt some simplier methods for calculating the value of some simplier series through complex analysis, for example using the Taylor expansion for some holomorphic functions. But it seems that these methods won't work in this problem. So I want to learn more methods or tricks to handle this kind of problems. Would you please recommend some books about this topic?","Prove: I'm sorry that I don't even know how to start. I haven't met this kind of series before. I've learnt some simplier methods for calculating the value of some simplier series through complex analysis, for example using the Taylor expansion for some holomorphic functions. But it seems that these methods won't work in this problem. So I want to learn more methods or tricks to handle this kind of problems. Would you please recommend some books about this topic?",\sum_{n=1}^\infty\frac{n}{(2n-1)16^n}\binom{2n}{n}^2\left(\sum_{k=n}^\infty\frac{2^k}{k\binom{2k}{k}}\right)=1-\sqrt2+\log(1+\sqrt2).,"['calculus', 'sequences-and-series']"
19,Interpretation of eigenvectors of Hessian in context of local min/max/saddle?,Interpretation of eigenvectors of Hessian in context of local min/max/saddle?,,"Say $f \in C^2$ so we can possibly use its Hessian $H$ to determine whether $f$ has a local max, min, or saddle at a critical point $x_0$. Since $H(x_0)$ is real and symmetric, it is diagonalizable, say with eigenvector-eigenvalue pairs $(v_1,\lambda_1),\ldots,(v_n,\lambda_n)$. The second derivative test asserts that if all the $\lambda_i$ are strictly positive, then $f$ has a local min, if they are all strictly negative, then $f$ has a local max, and if there are at least one strictly positive and one strictly negative, then $f$ has a saddle point. Is there some geometric interpretation to what the $v_i$ are? Are the $v_i$ somehow directions in which the function restricted to that direction has concavity $\lambda_i$?","Say $f \in C^2$ so we can possibly use its Hessian $H$ to determine whether $f$ has a local max, min, or saddle at a critical point $x_0$. Since $H(x_0)$ is real and symmetric, it is diagonalizable, say with eigenvector-eigenvalue pairs $(v_1,\lambda_1),\ldots,(v_n,\lambda_n)$. The second derivative test asserts that if all the $\lambda_i$ are strictly positive, then $f$ has a local min, if they are all strictly negative, then $f$ has a local max, and if there are at least one strictly positive and one strictly negative, then $f$ has a saddle point. Is there some geometric interpretation to what the $v_i$ are? Are the $v_i$ somehow directions in which the function restricted to that direction has concavity $\lambda_i$?",,"['calculus', 'multivariable-calculus', 'optimization', 'stationary-point']"
20,Evaluating $\sum_{n=1}^\infty \frac{n^3}{e^{2\pi n}-1}$ using inverse Mellin transform,Evaluating  using inverse Mellin transform,\sum_{n=1}^\infty \frac{n^3}{e^{2\pi n}-1},inspiration on the post Evaluating $\sum_{n=1}^{\infty} \frac{n}{e^{2 \pi n}-1}$ using the inverse Mellin transform it is possible to calculate in close form  $$\sum _{k=1}^{\infty } -\frac{k^3}{e^{2 \pi  k}-1}=\frac{3840 \pi ^4 \psi _{e^{2 \pi }}^{(1)}(1)+480 \pi ^2 \psi _{e^{2 \pi }}^{(3)}(1)-704 \pi ^6-5760 \pi ^5+3 \Gamma \left(\frac{1}{4}\right)^8}{23040 \pi ^6}$$ using euler sum I appreciatte some comment. I like to give another series it will interesting using elliptic theta function theory to prove it $$\sum _{k=1}^{\infty } \frac{\left(k \left(-\log \left(\frac{\pi }{2}\right)\right)\right)^3}{e^{2 \pi  \left(k \log \left(\frac{\pi }{2}\right)\right)}+1}$$ $$\frac{\log ^4\left(\frac{\pi }{2}\right) \psi _{e^{-\frac{\pi }{\log \left(\frac{\pi }{2}\right)}}}^{(3)}(1)-\log ^4\left(\frac{\pi }{2}\right) \psi _{e^{-\frac{\pi }{\log \left(\frac{\pi }{2}\right)}}}^{(3)}\left(-\frac{\left(i \pi -\frac{\pi }{\log \left(\frac{\pi }{2}\right)}\right) \log \left(\frac{\pi }{2}\right)}{\pi }\right)}{16 \pi ^4 \log \left(\frac{\pi }{2}\right)}-\frac{1}{240} \log ^3\left(\frac{\pi }{2}\right)-\frac{7}{1920 \log \left(\frac{\pi }{2}\right)}$$ sorry for the latex type i do not to improve,inspiration on the post Evaluating $\sum_{n=1}^{\infty} \frac{n}{e^{2 \pi n}-1}$ using the inverse Mellin transform it is possible to calculate in close form  $$\sum _{k=1}^{\infty } -\frac{k^3}{e^{2 \pi  k}-1}=\frac{3840 \pi ^4 \psi _{e^{2 \pi }}^{(1)}(1)+480 \pi ^2 \psi _{e^{2 \pi }}^{(3)}(1)-704 \pi ^6-5760 \pi ^5+3 \Gamma \left(\frac{1}{4}\right)^8}{23040 \pi ^6}$$ using euler sum I appreciatte some comment. I like to give another series it will interesting using elliptic theta function theory to prove it $$\sum _{k=1}^{\infty } \frac{\left(k \left(-\log \left(\frac{\pi }{2}\right)\right)\right)^3}{e^{2 \pi  \left(k \log \left(\frac{\pi }{2}\right)\right)}+1}$$ $$\frac{\log ^4\left(\frac{\pi }{2}\right) \psi _{e^{-\frac{\pi }{\log \left(\frac{\pi }{2}\right)}}}^{(3)}(1)-\log ^4\left(\frac{\pi }{2}\right) \psi _{e^{-\frac{\pi }{\log \left(\frac{\pi }{2}\right)}}}^{(3)}\left(-\frac{\left(i \pi -\frac{\pi }{\log \left(\frac{\pi }{2}\right)}\right) \log \left(\frac{\pi }{2}\right)}{\pi }\right)}{16 \pi ^4 \log \left(\frac{\pi }{2}\right)}-\frac{1}{240} \log ^3\left(\frac{\pi }{2}\right)-\frac{7}{1920 \log \left(\frac{\pi }{2}\right)}$$ sorry for the latex type i do not to improve,,"['calculus', 'sequences-and-series']"
21,Is line element mathematically rigorous?,Is line element mathematically rigorous?,,"I know differentials (in a way of standard analysis) are not very rigorous in mathematics, there are a lot of amazing answers here on the topic. But what about line element? $$ds^2 = dx^2 + dy^2 +dz^2 $$ The way I think about this line element is being geometrically constructed from Pythagoras theorem as: $$\Delta s^2 =\Delta x^2 + \Delta y^2 +\Delta z^2$$ and then we assume that we can 'get' these quantities ($\Delta x$) to be infinitesimally small (as small as we like) and represent as $dx$ instead, right? Now then lets take line element on a sphere: $$ds_2 ^2=r^2sin^2(\theta)d\phi^2 + r^2d\theta ^2$$ It is geometrically constructed again using Pythagoras theorem and assuming that sides of a 'triangle' are small: $$\Delta s_2 ^2 \approx (rsin(\theta)\Delta \phi)^2 + (r\Delta\theta)^2$$ But this approximation never really becomes equality, the smaller the angles the better it works, but still never equality! People just replace $\Delta->d$ and say $ds$ and say it's differential. I guess my question is this: when we write something like $$ds_2 ^2=r^2sin^2(\theta)d\phi^2 + r^2d\theta ^2$$ we actually have in mind that this quantity contains higher order terms, but they will vanish after we parametrise? I think about parametrisation in a way: $$\frac{ds_2^2}{dt^2}=r^2sin^2(\theta)\frac{d\phi^2}{dt^2} + r^2\frac{d\theta^2}{dt^2}$$","I know differentials (in a way of standard analysis) are not very rigorous in mathematics, there are a lot of amazing answers here on the topic. But what about line element? $$ds^2 = dx^2 + dy^2 +dz^2 $$ The way I think about this line element is being geometrically constructed from Pythagoras theorem as: $$\Delta s^2 =\Delta x^2 + \Delta y^2 +\Delta z^2$$ and then we assume that we can 'get' these quantities ($\Delta x$) to be infinitesimally small (as small as we like) and represent as $dx$ instead, right? Now then lets take line element on a sphere: $$ds_2 ^2=r^2sin^2(\theta)d\phi^2 + r^2d\theta ^2$$ It is geometrically constructed again using Pythagoras theorem and assuming that sides of a 'triangle' are small: $$\Delta s_2 ^2 \approx (rsin(\theta)\Delta \phi)^2 + (r\Delta\theta)^2$$ But this approximation never really becomes equality, the smaller the angles the better it works, but still never equality! People just replace $\Delta->d$ and say $ds$ and say it's differential. I guess my question is this: when we write something like $$ds_2 ^2=r^2sin^2(\theta)d\phi^2 + r^2d\theta ^2$$ we actually have in mind that this quantity contains higher order terms, but they will vanish after we parametrise? I think about parametrisation in a way: $$\frac{ds_2^2}{dt^2}=r^2sin^2(\theta)\frac{d\phi^2}{dt^2} + r^2\frac{d\theta^2}{dt^2}$$",,"['calculus', 'multivariable-calculus', 'differential-forms', 'differential', 'infinitesimals']"
22,Approximating $\int_1^{10}x^x\mathrm dx$ to within 5% relative error,Approximating  to within 5% relative error,\int_1^{10}x^x\mathrm dx,"I am working on a few problems from Arnold's trivium because I hate myself. My first and only idea is to try and approximate this by Riemann sums, but  this is of course disgusting. For overestimate (right Riemann sum of increasing function) of $3$ points we have $$ \sum_{n=1}^3 f(1+3n)\frac{1}{3}=\frac{1}{3}(4^4+7^7+10^{10})\approx 10^{9} $$ Which I had to use a calculator to figure out (sorta defeating the point of the exercise, I guess) has relative error of 8%. Is there a more clever and less painful way to do this than bashing out crummy Riemann or trapezoidal approximations or something of the sort?","I am working on a few problems from Arnold's trivium because I hate myself. My first and only idea is to try and approximate this by Riemann sums, but  this is of course disgusting. For overestimate (right Riemann sum of increasing function) of $3$ points we have $$ \sum_{n=1}^3 f(1+3n)\frac{1}{3}=\frac{1}{3}(4^4+7^7+10^{10})\approx 10^{9} $$ Which I had to use a calculator to figure out (sorta defeating the point of the exercise, I guess) has relative error of 8%. Is there a more clever and less painful way to do this than bashing out crummy Riemann or trapezoidal approximations or something of the sort?",,"['calculus', 'integration', 'definite-integrals', 'approximation']"
23,Continuous and discontinuous function problem,Continuous and discontinuous function problem,,"The following problem can be true? Problem: For every positive integer $n>1$ there exists a function $ f\left( x \right)$ on $\mathbb{R}$ which satisfies both following conditions: (i) $f\left( x \right),{\rm{ }}f\left( {f\left( x \right)} \right), \ldots ,{\rm{ }}f\left( { \ldots f\left( x \right) \ldots } \right)$ ( $ (n-1)$ times $ f $) discontinuous at every $x$ belong to $\mathbb{R}$. (ii) $ f\left( { \ldots \left( {f\left( x \right) \ldots } \right)} \right)$ ( $n$ times $f$ ) continuous in $ \mathbb{R}$.","The following problem can be true? Problem: For every positive integer $n>1$ there exists a function $ f\left( x \right)$ on $\mathbb{R}$ which satisfies both following conditions: (i) $f\left( x \right),{\rm{ }}f\left( {f\left( x \right)} \right), \ldots ,{\rm{ }}f\left( { \ldots f\left( x \right) \ldots } \right)$ ( $ (n-1)$ times $ f $) discontinuous at every $x$ belong to $\mathbb{R}$. (ii) $ f\left( { \ldots \left( {f\left( x \right) \ldots } \right)} \right)$ ( $n$ times $f$ ) continuous in $ \mathbb{R}$.",,"['calculus', 'analysis']"
24,Proving that $\lim_{x\to1^-}\left(\sqrt[a]{1-x}\cdot\sum_{n=0}^\infty~x^{n^a}\right)=\Gamma\left(1+\frac1a\right)$,Proving that,\lim_{x\to1^-}\left(\sqrt[a]{1-x}\cdot\sum_{n=0}^\infty~x^{n^a}\right)=\Gamma\left(1+\frac1a\right),"How could we prove that $$\lim_{x\to1^-}~\bigg(\sqrt[a]{1-x}\cdot\sum_{n=0}^\infty~x^{n^a}\bigg)~=~\Gamma\bigg(1+\frac1a\bigg)$$ for $a>0$ ? The inspiration came to me while trying find a solution to this related question , which treats the special case $a=2$. I am pretty confident that the best way to approach this is by some clever manipulation of the well-known identities $~\displaystyle\int_0^\infty e^{-t^a}~dt~=~\Gamma\bigg(1+\frac1a\bigg)~$ and $~e^u~=~\displaystyle\sum_{n=0}^\infty\frac{u^n}{n!}~,$ but unfortunately I haven't been able thus far to capitalize on my own idea. I also tried writing x as $1-\epsilon$, and then expanding $(1-\epsilon)^{n^a}$ into its binomial series, but this endeavor has also been proven worthless, or, at the very least, it appears to be so in my hands. Which is where you, dear reader, come in! Can you help me out of my impasse ? Any ideas or suggestions are welcome!","How could we prove that $$\lim_{x\to1^-}~\bigg(\sqrt[a]{1-x}\cdot\sum_{n=0}^\infty~x^{n^a}\bigg)~=~\Gamma\bigg(1+\frac1a\bigg)$$ for $a>0$ ? The inspiration came to me while trying find a solution to this related question , which treats the special case $a=2$. I am pretty confident that the best way to approach this is by some clever manipulation of the well-known identities $~\displaystyle\int_0^\infty e^{-t^a}~dt~=~\Gamma\bigg(1+\frac1a\bigg)~$ and $~e^u~=~\displaystyle\sum_{n=0}^\infty\frac{u^n}{n!}~,$ but unfortunately I haven't been able thus far to capitalize on my own idea. I also tried writing x as $1-\epsilon$, and then expanding $(1-\epsilon)^{n^a}$ into its binomial series, but this endeavor has also been proven worthless, or, at the very least, it appears to be so in my hands. Which is where you, dear reader, come in! Can you help me out of my impasse ? Any ideas or suggestions are welcome!",,"['calculus', 'sequences-and-series', 'limits', 'power-series', 'gamma-function']"
25,Fast method to find the tangent line to a conic section: why does it work?,Fast method to find the tangent line to a conic section: why does it work?,,"My teacher taught me this fast method to determine the equation of the tangent line to a conic section. In the Netherlands this is called ""eerlijk delen"" or literally translated into English ""fair division"". Here is an example of how it works: Find the equation of the tangent line to the circle $x^2+y^2+6x-8=0$ in $A(1,-1)$. The equation of the tangent line is $x\cdot x_A+y\cdot y_a+3x+3x_A-8=0 \implies x-y+3x+3-8=0 \implies 4x-y-5=0$. This same method works with all conic sections, but it still remains a bit vague to me why this works. Has someone seen this method previously, is it used often, and could someone provide a proof for this?","My teacher taught me this fast method to determine the equation of the tangent line to a conic section. In the Netherlands this is called ""eerlijk delen"" or literally translated into English ""fair division"". Here is an example of how it works: Find the equation of the tangent line to the circle $x^2+y^2+6x-8=0$ in $A(1,-1)$. The equation of the tangent line is $x\cdot x_A+y\cdot y_a+3x+3x_A-8=0 \implies x-y+3x+3-8=0 \implies 4x-y-5=0$. This same method works with all conic sections, but it still remains a bit vague to me why this works. Has someone seen this method previously, is it used often, and could someone provide a proof for this?",,"['calculus', 'conic-sections']"
26,"Improper integral with log and absolute value: $\int^{\infty}_{0} \frac{\log |\tan x|}{1+x^{2}} \, dx$",Improper integral with log and absolute value:,"\int^{\infty}_{0} \frac{\log |\tan x|}{1+x^{2}} \, dx","How do you show that $$ \int^{\infty}_{0} \frac{\log |\tan x|}{1+x^{2}} \, dx = \frac{\pi}{2} \log (\tanh 1)\, ?$$ I know that the integral converges since $\log |\tan x| = \frac{1}{2} \log(\tan^{2}x)$, and $\log (\tan^{2} x)$ behaves like $2 (-1)^{n} \log \left(x-\frac{n \pi}{2} \right)$ near $x= \frac{n \pi}{2}$. Thus the singularities at $x= \frac{n \pi}{2}$ are integrable.","How do you show that $$ \int^{\infty}_{0} \frac{\log |\tan x|}{1+x^{2}} \, dx = \frac{\pi}{2} \log (\tanh 1)\, ?$$ I know that the integral converges since $\log |\tan x| = \frac{1}{2} \log(\tan^{2}x)$, and $\log (\tan^{2} x)$ behaves like $2 (-1)^{n} \log \left(x-\frac{n \pi}{2} \right)$ near $x= \frac{n \pi}{2}$. Thus the singularities at $x= \frac{n \pi}{2}$ are integrable.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
27,Convert integral $\int_0^{\frac{\sqrt3}2} {\frac{1}{{{x^{3/4}}{{\left( {1 + x} \right)}^{3/4}}}}dx}$ to elliptic integral,Convert integral  to elliptic integral,\int_0^{\frac{\sqrt3}2} {\frac{1}{{{x^{3/4}}{{\left( {1 + x} \right)}^{3/4}}}}dx},"I found this integral from a friend of mine $$I = \int_0^{\frac{\sqrt3}2} {\frac{1}{{{x^{3/4}}{{\left( {1 + x} \right)}^{3/4}}}}dx} $$ Which its closed-form is : $$\frac{{2\sqrt 2 {\pi ^{3/2}}}}{{3{\Gamma ^2}( {\frac{3}{4}} )}}$$ Look at this closed form I have a feeling that the given integral can be involved with the complete elliptic integral of the first kind $K( {\frac{1}{2}} )$ . But I don't know how to convert this integral to get that result. I tried to use sub: $$\eqalign{   & x = \frac{{1 - t}}{{1 + t}} \Rightarrow t = \frac{{1 - x}}{{1 + x}} \Rightarrow dx =  - \frac{2}{{{{\left( {1 + t} \right)}^2}}}dt  \cr    &  \Rightarrow I = \frac{2}{{{2^{3/4}}}}\int\limits_{7 - 4\sqrt 3 }^1 {\frac{1}{{\sqrt {1 + t} {{\left( {1 - t} \right)}^{3/4}}}}} dt,{\text{ since }}7 - 4\sqrt 3  = {\left( {2 - \sqrt 3 } \right)^2}{\text{ then let}}:t = {u^2}  \cr    &  \Rightarrow I = \frac{4}{{{2^{3/4}}}}\int\limits_{2 - \sqrt 3 }^1 {\frac{u}{{\sqrt {1 + {u^2}} {{\left( {1 - {u^2}} \right)}^{3/4}}}}} du \cr} $$ So I get stuck here. May I ask for help? Or give me a hint about substitution. Thank you very much. Edit #1: After using generalized binomial theorem and changing order of summation and integration, with some manipulation with the last sum, I arrived at: $$I=2\ 2^{3/4} \sqrt[8]{3} \, _2F_1\left(\frac{1}{4},\frac{3}{4};\frac{5}{4};-\frac{\sqrt{3}}{2}\right)$$ May be, there is a transformation with $_2F_1$ can link this result with the elliptic integral. I am still trying to figure out.","I found this integral from a friend of mine Which its closed-form is : Look at this closed form I have a feeling that the given integral can be involved with the complete elliptic integral of the first kind . But I don't know how to convert this integral to get that result. I tried to use sub: So I get stuck here. May I ask for help? Or give me a hint about substitution. Thank you very much. Edit #1: After using generalized binomial theorem and changing order of summation and integration, with some manipulation with the last sum, I arrived at: May be, there is a transformation with can link this result with the elliptic integral. I am still trying to figure out.","I = \int_0^{\frac{\sqrt3}2} {\frac{1}{{{x^{3/4}}{{\left( {1 + x} \right)}^{3/4}}}}dx}  \frac{{2\sqrt 2 {\pi ^{3/2}}}}{{3{\Gamma ^2}( {\frac{3}{4}} )}} K( {\frac{1}{2}} ) \eqalign{
  & x = \frac{{1 - t}}{{1 + t}} \Rightarrow t = \frac{{1 - x}}{{1 + x}} \Rightarrow dx =  - \frac{2}{{{{\left( {1 + t} \right)}^2}}}dt  \cr 
  &  \Rightarrow I = \frac{2}{{{2^{3/4}}}}\int\limits_{7 - 4\sqrt 3 }^1 {\frac{1}{{\sqrt {1 + t} {{\left( {1 - t} \right)}^{3/4}}}}} dt,{\text{ since }}7 - 4\sqrt 3  = {\left( {2 - \sqrt 3 } \right)^2}{\text{ then let}}:t = {u^2}  \cr 
  &  \Rightarrow I = \frac{4}{{{2^{3/4}}}}\int\limits_{2 - \sqrt 3 }^1 {\frac{u}{{\sqrt {1 + {u^2}} {{\left( {1 - {u^2}} \right)}^{3/4}}}}} du \cr}  I=2\ 2^{3/4} \sqrt[8]{3} \, _2F_1\left(\frac{1}{4},\frac{3}{4};\frac{5}{4};-\frac{\sqrt{3}}{2}\right) _2F_1","['calculus', 'integration', 'analysis', 'definite-integrals', 'elliptic-integrals']"
28,How can I evaluate the following integral? $\int(\sqrt{x}-x)(e^{\arctan\sqrt{x}})^2dx$,How can I evaluate the following integral?,\int(\sqrt{x}-x)(e^{\arctan\sqrt{x}})^2dx,"How can I evaluate the following integral? $$\int(\sqrt{x}-x)(e^{\arctan\sqrt{x}})^2dx$$ I'd like the whole solution if possible. I tried using the substitution: $\sqrt{x}=t$, followed by: $2\arctan{t}=m$, to get: $$\int e^m\tan^2{\frac{m}{2}}\sec^2{\frac{m}{2}}\left[1-\tan{\frac{m}{2}}\right]dm$$ But it doesn't get me anywhere. A complete solution will be sincerely appreciated.","How can I evaluate the following integral? $$\int(\sqrt{x}-x)(e^{\arctan\sqrt{x}})^2dx$$ I'd like the whole solution if possible. I tried using the substitution: $\sqrt{x}=t$, followed by: $2\arctan{t}=m$, to get: $$\int e^m\tan^2{\frac{m}{2}}\sec^2{\frac{m}{2}}\left[1-\tan{\frac{m}{2}}\right]dm$$ But it doesn't get me anywhere. A complete solution will be sincerely appreciated.",,"['calculus', 'integration', 'indefinite-integrals']"
29,"Does there exist a continuous $g(x,t)$ such that every continuous$ f(x)$ equals $g(x,t)$ for some $t$ and all $x$??",Does there exist a continuous  such that every continuous equals  for some  and all ??,"g(x,t)  f(x) g(x,t) t x","Is there a continuous $g(x,t)$ such that every continuous $f(x)$ equals $g(x,t)$ for some $t$ and all $x$? $f$ is from $[0,1]$ to itself with $f(0)=0$ and $f(1)=1$ and is either smooth or continuous (your choice) $g$ is defined on $[0,1]\times(0,1)$ or $[0,1]\times[0,1]$ (your choice) What if we require there to be a unique $t$ for each $f$? Is there an simple explicit such $g$?","Is there a continuous $g(x,t)$ such that every continuous $f(x)$ equals $g(x,t)$ for some $t$ and all $x$? $f$ is from $[0,1]$ to itself with $f(0)=0$ and $f(1)=1$ and is either smooth or continuous (your choice) $g$ is defined on $[0,1]\times(0,1)$ or $[0,1]\times[0,1]$ (your choice) What if we require there to be a unique $t$ for each $f$? Is there an simple explicit such $g$?",,['calculus']
30,A couple of definite integrals related to Stieltjes constants,A couple of definite integrals related to Stieltjes constants,,"In a (great) paper ""A theorem for the closed-form evaluation of the first generalized Stieltjes constant at rational arguments and some related summations"" by Iaroslav V. Blagouchine, the following integral representation of the first Stieltjes constant $\gamma_1$ is given (on page 539): $$\gamma_1=-\left[\gamma-\frac{\ln2}2\right]\ln2+i\int_0^\infty\frac{dx}{e^{\pi x}+1}\left\{\frac{\ln(1-ix)}{1-ix}-\frac{\ln(1+ix)}{1+ix}\right\}.\tag1$$ It's possible to get rid of imaginary numbers in this formula, and rewrite it in terms of only real-valued functions: $$2\int_0^\infty\frac{\arctan x}{1+x^2}\frac{dx}{e^{\pi x}+1}-\int_0^\infty\frac{x\ln(1+x^2)}{1+x^2}\frac{dx}{e^{\pi x}+1}=\gamma_1+\left[\gamma-\frac{\ln2}2\right]\ln2.\tag2$$ Question: Is it possible to find closed forms separately for each integral on the left-hand side of $(2)$?","In a (great) paper ""A theorem for the closed-form evaluation of the first generalized Stieltjes constant at rational arguments and some related summations"" by Iaroslav V. Blagouchine, the following integral representation of the first Stieltjes constant $\gamma_1$ is given (on page 539): $$\gamma_1=-\left[\gamma-\frac{\ln2}2\right]\ln2+i\int_0^\infty\frac{dx}{e^{\pi x}+1}\left\{\frac{\ln(1-ix)}{1-ix}-\frac{\ln(1+ix)}{1+ix}\right\}.\tag1$$ It's possible to get rid of imaginary numbers in this formula, and rewrite it in terms of only real-valued functions: $$2\int_0^\infty\frac{\arctan x}{1+x^2}\frac{dx}{e^{\pi x}+1}-\int_0^\infty\frac{x\ln(1+x^2)}{1+x^2}\frac{dx}{e^{\pi x}+1}=\gamma_1+\left[\gamma-\frac{\ln2}2\right]\ln2.\tag2$$ Question: Is it possible to find closed forms separately for each integral on the left-hand side of $(2)$?",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'stieltjes-constants']"
31,Show $\sum_{k=1}^{\infty}\left(\frac{1+\sin(k)}{2}\right)^k$ diverges,Show  diverges,\sum_{k=1}^{\infty}\left(\frac{1+\sin(k)}{2}\right)^k,"Show$$\sum_{k=1}^{\infty}\left(\frac{1+\sin(k)}{2}\right)^k$$diverges. Just going down the list, the following tests don't work (or I failed at using them correctly) because: $\lim \limits_{k\to\infty}a_k\neq0$ — The limit is hard to evaluate. $\lim \limits_{k\to\infty}\left|\dfrac{a_{k+1}}{a_k}\right|>1$—Limit does not converge; inconclusive. $\lim \limits_{k\to\infty}\sqrt[k]{|a_k|}>1$—Limit does not converge; inconclusive. $\int \limits_{1}^{\infty}a_k\,dk$—How do I even do this. There exists a $|b_k|\geq|a_k|$ and $\sum b_k$ converges—Cannot think of any such a $b_k$. $\lim \limits_{k\to\infty}\cfrac{a_k}{b_k}$ exists and $\sum b_k$ converges — Cannot think of any such $b_k$. Can I have some help? EDIT We will show that there is no $\delta$ or corresponding $\epsilon$ so that $k>\delta$ implies$$\left|\left(\frac{1+\sin(k)}{2}\right)^k-\lim_{n\to\infty}\left(\frac{1+\sin(n)}{2}\right)^n\right|<\epsilon$$ Let the first term be denoted $a_k$ and the second $L$ (for limit). Pick the the smaller number of $|a_{\frac{3\pi}2}-L|$ or $|a_{\frac\pi2}-L|$. Whichever was chosen (denote choice as $\epsilon$), it is clear that setting $k:=k+\pi$, which is greater than $\delta$, will produce a result such that $|a_k-L|\geq\epsilon$. Therefore, the limit does not exist, and the sum diverges. EDIT AGAIN The proof above is no good. More help pls. I can't figure this one out.","Show$$\sum_{k=1}^{\infty}\left(\frac{1+\sin(k)}{2}\right)^k$$diverges. Just going down the list, the following tests don't work (or I failed at using them correctly) because: $\lim \limits_{k\to\infty}a_k\neq0$ — The limit is hard to evaluate. $\lim \limits_{k\to\infty}\left|\dfrac{a_{k+1}}{a_k}\right|>1$—Limit does not converge; inconclusive. $\lim \limits_{k\to\infty}\sqrt[k]{|a_k|}>1$—Limit does not converge; inconclusive. $\int \limits_{1}^{\infty}a_k\,dk$—How do I even do this. There exists a $|b_k|\geq|a_k|$ and $\sum b_k$ converges—Cannot think of any such a $b_k$. $\lim \limits_{k\to\infty}\cfrac{a_k}{b_k}$ exists and $\sum b_k$ converges — Cannot think of any such $b_k$. Can I have some help? EDIT We will show that there is no $\delta$ or corresponding $\epsilon$ so that $k>\delta$ implies$$\left|\left(\frac{1+\sin(k)}{2}\right)^k-\lim_{n\to\infty}\left(\frac{1+\sin(n)}{2}\right)^n\right|<\epsilon$$ Let the first term be denoted $a_k$ and the second $L$ (for limit). Pick the the smaller number of $|a_{\frac{3\pi}2}-L|$ or $|a_{\frac\pi2}-L|$. Whichever was chosen (denote choice as $\epsilon$), it is clear that setting $k:=k+\pi$, which is greater than $\delta$, will produce a result such that $|a_k-L|\geq\epsilon$. Therefore, the limit does not exist, and the sum diverges. EDIT AGAIN The proof above is no good. More help pls. I can't figure this one out.",,"['calculus', 'sequences-and-series', 'limits', 'summation']"
32,Why can you mix Partial Derivatives with Ordinary Derivatives in the Chain Rule?,Why can you mix Partial Derivatives with Ordinary Derivatives in the Chain Rule?,,"This question is a simplified version of this previous question asked by myself . The following is a short extract from a book I am reading: If $u=(x^2+2y)^2 + 4$ and $p=x^2 + 2y$ $\space$ then $u=p^2 + 4=f(p)$ therefore $$\frac{\partial u}{\partial x}=\frac{\rm d f(p)}{\rm d p}\times \frac{\partial p}{\partial x}=2xf^{\prime}(p)\tag{1}$$ and $$\frac{\partial u}{\partial y}=\frac{\rm d f(p)}{\rm d p}\times \frac{\partial p}{\partial y}=2f^{\prime}(p)\tag{2}$$ I know that the chain rule for a function of one variable $y=f(x)$ is $$\begin{align}\color{red}{\fbox{$\frac{{\rm d}}{{\rm d}x}=\frac{{\rm d}}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}x}$}}\color{red}{\tag{A}}\end{align}$$ I also know that if $u=f(x,y)$ then the differential is $$\begin{align}\color{blue}{\fbox{${{\rm d}u=\frac{\partial u}{\partial x}\cdot{\rm d} x+\frac{\partial u}{\partial y}\cdot{\rm d}y}$}}\color{blue}{\tag{B}}\end{align}$$ I'm aware that if $u=u(x,y)$ and $x=x(t)$ and $y=y(t)$ then the chain rule is $$\begin{align}\color{#180}{\fbox{$\frac{\rm d u}{\rm d t}=\frac{\partial u}{\partial x}\times \frac{\rm d x}{\rm d t}+\frac{\partial u}{\partial y}\times \frac{\rm d y}{\rm d t}$}}\color{#180}{\tag{C}}\end{align}$$ Finally, I also know that if $u=u(x,y)$ and $x=x(s,t)$ and $y=y(s,t)$ then the chain rule is $$\begin{align}\color{#F80}{\fbox{$\frac{\partial u}{\partial t}=\frac{\partial u}{\partial x}\times \frac{\partial x}{\partial t}+\frac{\partial u}{\partial y}\times \frac{\partial y}{\partial t}$}}\color{#F80}{\tag{D}}\end{align}$$ Could someone please explain the origin or meaning of equations $(1)$ and $(2)$? The reason I ask is because I am only familiar with equations $\color{red}{\rm (A)}$, $\color{blue}{\rm (B)}$, $\color{#180}{\rm (C)}$ and $\color{#F80}{\rm (D)}$ so I am not used to seeing partial derivatives mixed up with ordinary ones in they way they were in $(1)$ and $(2)$. Many thanks, BLAZE.","This question is a simplified version of this previous question asked by myself . The following is a short extract from a book I am reading: If $u=(x^2+2y)^2 + 4$ and $p=x^2 + 2y$ $\space$ then $u=p^2 + 4=f(p)$ therefore $$\frac{\partial u}{\partial x}=\frac{\rm d f(p)}{\rm d p}\times \frac{\partial p}{\partial x}=2xf^{\prime}(p)\tag{1}$$ and $$\frac{\partial u}{\partial y}=\frac{\rm d f(p)}{\rm d p}\times \frac{\partial p}{\partial y}=2f^{\prime}(p)\tag{2}$$ I know that the chain rule for a function of one variable $y=f(x)$ is $$\begin{align}\color{red}{\fbox{$\frac{{\rm d}}{{\rm d}x}=\frac{{\rm d}}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}x}$}}\color{red}{\tag{A}}\end{align}$$ I also know that if $u=f(x,y)$ then the differential is $$\begin{align}\color{blue}{\fbox{${{\rm d}u=\frac{\partial u}{\partial x}\cdot{\rm d} x+\frac{\partial u}{\partial y}\cdot{\rm d}y}$}}\color{blue}{\tag{B}}\end{align}$$ I'm aware that if $u=u(x,y)$ and $x=x(t)$ and $y=y(t)$ then the chain rule is $$\begin{align}\color{#180}{\fbox{$\frac{\rm d u}{\rm d t}=\frac{\partial u}{\partial x}\times \frac{\rm d x}{\rm d t}+\frac{\partial u}{\partial y}\times \frac{\rm d y}{\rm d t}$}}\color{#180}{\tag{C}}\end{align}$$ Finally, I also know that if $u=u(x,y)$ and $x=x(s,t)$ and $y=y(s,t)$ then the chain rule is $$\begin{align}\color{#F80}{\fbox{$\frac{\partial u}{\partial t}=\frac{\partial u}{\partial x}\times \frac{\partial x}{\partial t}+\frac{\partial u}{\partial y}\times \frac{\partial y}{\partial t}$}}\color{#F80}{\tag{D}}\end{align}$$ Could someone please explain the origin or meaning of equations $(1)$ and $(2)$? The reason I ask is because I am only familiar with equations $\color{red}{\rm (A)}$, $\color{blue}{\rm (B)}$, $\color{#180}{\rm (C)}$ and $\color{#F80}{\rm (D)}$ so I am not used to seeing partial derivatives mixed up with ordinary ones in they way they were in $(1)$ and $(2)$. Many thanks, BLAZE.",,"['calculus', 'derivatives', 'partial-derivative', 'chain-rule']"
33,How does one evaluate $\int \frac{\sin(x)}{\sin(5x)} \ dx$,How does one evaluate,\int \frac{\sin(x)}{\sin(5x)} \ dx,The below problem is taken from Joseph Edwards book Integral Calculus for beginners . How does one show: $$5 \int \frac{\sin(x)}{\sin(5x)} \ dx= \sin\left(\frac{2\pi}{5}\right) \cdot \log\left\{\frac{\sin\left(x-\frac{2\pi}{5}\right)}{\sin\left(x+\frac{2\pi}{5}\right)}\right\} -\sin\left(\frac{\pi}{5}\right) \cdot \log\left\{\frac{\sin\left(x-\frac{\pi}{5}\right)}{\sin\left(x+\frac{\pi}{5}\right)}\right\} $$ Splitting $\sin{(5x)}$ as $\sin{(4x+x)}$ doesn't seem to be of much help since then we have a big term in the denominator after expansion.,The below problem is taken from Joseph Edwards book Integral Calculus for beginners . How does one show: $$5 \int \frac{\sin(x)}{\sin(5x)} \ dx= \sin\left(\frac{2\pi}{5}\right) \cdot \log\left\{\frac{\sin\left(x-\frac{2\pi}{5}\right)}{\sin\left(x+\frac{2\pi}{5}\right)}\right\} -\sin\left(\frac{\pi}{5}\right) \cdot \log\left\{\frac{\sin\left(x-\frac{\pi}{5}\right)}{\sin\left(x+\frac{\pi}{5}\right)}\right\} $$ Splitting $\sin{(5x)}$ as $\sin{(4x+x)}$ doesn't seem to be of much help since then we have a big term in the denominator after expansion.,,"['calculus', 'integration', 'indefinite-integrals', 'trigonometric-integrals']"
34,Why does the order of summation of the terms of an infinite series influence its value?,Why does the order of summation of the terms of an infinite series influence its value?,,I was looking through my lecture notes and got puzzled by the following fact: if we want to find the value of some infinite series we are allowed to rearrange only the finite number of its terms. To visualize this consider the alternating harmonic series: $$\sum_{n=1}^\infty(-1)^{k-1}\frac1k=1-\frac12+\frac13-\frac14+\frac15-+\dots=0.693147...$$ But if we rearrange the terms as follows the value of the series gets influenced by this action: $$1+\frac13-\frac12+\frac15+\frac17-\frac14+\dots=1.03972...$$ So commutativity of addition isn't true on infinity? How was it obtained and how can it be proved?,I was looking through my lecture notes and got puzzled by the following fact: if we want to find the value of some infinite series we are allowed to rearrange only the finite number of its terms. To visualize this consider the alternating harmonic series: $$\sum_{n=1}^\infty(-1)^{k-1}\frac1k=1-\frac12+\frac13-\frac14+\frac15-+\dots=0.693147...$$ But if we rearrange the terms as follows the value of the series gets influenced by this action: $$1+\frac13-\frac12+\frac15+\frac17-\frac14+\dots=1.03972...$$ So commutativity of addition isn't true on infinity? How was it obtained and how can it be proved?,,"['calculus', 'sequences-and-series']"
35,Is there a shape with infinite volume but finite surface area?,Is there a shape with infinite volume but finite surface area?,,"Is there any pathological shape that has a finite surface area but an infinite volume, sort of like the opposite of a Gabriel's horn ?","Is there any pathological shape that has a finite surface area but an infinite volume, sort of like the opposite of a Gabriel's horn ?",,"['calculus', 'geometry', 'area', '3d', 'volume']"
36,Simplification of a trilogarithm of a complex argument,Simplification of a trilogarithm of a complex argument,,"Is it possible to simplify the following expression? $$\large\Im\,\operatorname{Li}_3\left(-e^{\xi\,\left(\sqrt3-\sqrt{-1}\right)-\frac{\pi^2}{12\,\xi}\left(\sqrt3+\sqrt{-1}\right)}\right)$$ where $$\large\xi=\frac{\sqrt[3]3}6\sqrt[3]{27+\sqrt3\,\sqrt{243-\pi^6}}$$ and $\Im\,\operatorname{Li}_3(z)$ denotes the imaginary part of the trilogarithm .","Is it possible to simplify the following expression? $$\large\Im\,\operatorname{Li}_3\left(-e^{\xi\,\left(\sqrt3-\sqrt{-1}\right)-\frac{\pi^2}{12\,\xi}\left(\sqrt3+\sqrt{-1}\right)}\right)$$ where $$\large\xi=\frac{\sqrt[3]3}6\sqrt[3]{27+\sqrt3\,\sqrt{243-\pi^6}}$$ and $\Im\,\operatorname{Li}_3(z)$ denotes the imaginary part of the trilogarithm .",,"['calculus', 'complex-analysis', 'complex-numbers', 'special-functions', 'polylogarithm']"
37,Who gave you the epsilon?,Who gave you the epsilon?,,"Who gave you the epsilon? is the title of an article by J. Grabiner on Cauchy from the 1980s, and the implied answer is ""Cauchy"".  On the other hand, historian I. Grattan-Guinness points out in his book that ""Weierstrass undoubtedly saw himself as Cauchy's heir in analysis and so helped to create the belief that Cauchy's achievements included ideas that were actually his own"" ( The development of the foundations of mathematical analysis from Euler to Riemann , page 120). In fact, Cauchy apparently never gave an epsilon-delta definition of either limit or continuity .  His infinitesimal definition of continuity (""infinitesimal $x$-increment always produces infinitesimal $y$-increment"") is reproduced in a number of secondary studies, for example in J. Gray ( Plato's Ghost , page 64). The question is then, who in fact gave you epsilon-delta: Cauchy or Weierstrass? Editors are requested to refrain from answers based purely on opinion, but rather to base themselves on published sources supporting either of the two views. In particular, a primary source in Cauchy giving an epsilon-delta definition of continuity would be welcome. Note 1. Grabiner's example cited by @Brian  illustrates the issue well. Cauchy infrequently uses preliminary forms of epsilon, delta arguments (notice the opening ""let delta, epsilon be very small numbers"" which would certainly not make it into calculus texts today) in some of his proofs, but he never gave an epsilon, delta definition . This example suggests that Cauchy never gave such a definition, for if he did, Grabiner would have likely cited it.  Cauchy's definition of continuity is ""infinitesimal $x$-increment always produces an infinitesimal change in $y$"".  This is closer to the modern infinitesimal definition of continuity than to the modern epsilon, delta definition; in fact it looks identical to the modern hyperreal definition at the syntactic level.","Who gave you the epsilon? is the title of an article by J. Grabiner on Cauchy from the 1980s, and the implied answer is ""Cauchy"".  On the other hand, historian I. Grattan-Guinness points out in his book that ""Weierstrass undoubtedly saw himself as Cauchy's heir in analysis and so helped to create the belief that Cauchy's achievements included ideas that were actually his own"" ( The development of the foundations of mathematical analysis from Euler to Riemann , page 120). In fact, Cauchy apparently never gave an epsilon-delta definition of either limit or continuity .  His infinitesimal definition of continuity (""infinitesimal $x$-increment always produces infinitesimal $y$-increment"") is reproduced in a number of secondary studies, for example in J. Gray ( Plato's Ghost , page 64). The question is then, who in fact gave you epsilon-delta: Cauchy or Weierstrass? Editors are requested to refrain from answers based purely on opinion, but rather to base themselves on published sources supporting either of the two views. In particular, a primary source in Cauchy giving an epsilon-delta definition of continuity would be welcome. Note 1. Grabiner's example cited by @Brian  illustrates the issue well. Cauchy infrequently uses preliminary forms of epsilon, delta arguments (notice the opening ""let delta, epsilon be very small numbers"" which would certainly not make it into calculus texts today) in some of his proofs, but he never gave an epsilon, delta definition . This example suggests that Cauchy never gave such a definition, for if he did, Grabiner would have likely cited it.  Cauchy's definition of continuity is ""infinitesimal $x$-increment always produces an infinitesimal change in $y$"".  This is closer to the modern infinitesimal definition of continuity than to the modern epsilon, delta definition; in fact it looks identical to the modern hyperreal definition at the syntactic level.",,"['calculus', 'continuity', 'math-history', 'nonstandard-analysis', 'infinitesimals']"
38,"Evaluate $\int\frac{1+x+\sqrt{1+x^2}}{\sqrt{x+1}+\sqrt{x}}\,dx$",Evaluate,"\int\frac{1+x+\sqrt{1+x^2}}{\sqrt{x+1}+\sqrt{x}}\,dx","Question: Solve the integral   $$ \int\frac{1+x+\sqrt{1+x^2}}{\sqrt{x+1}+\sqrt{x}}\,dx $$ My solution: Multiply both the numerator and denominator by $\sqrt{x+1}-\sqrt{x}$. This changes the integral to $$ \begin{align} \int&\left(1+x+\sqrt{1+x^2}\right)\left(\sqrt{x+1}-\sqrt{x}\right)\,dx\\ &= \int{(1+x)^{3/2}}\,dx-\int{\sqrt{(1+x)(1+x^2)}}\,dx-\int{\sqrt{x}(1+x)}\,dx-\int{\sqrt{x}\sqrt{1+x^2}}\,dx\\ & = \frac{2}{5}(1+x)^{5/2}-I-\frac{2}{3}x^{3/2}-\frac{2}{5}x^{5/2}-J \end{align} $$ where $I = \int{(1+x)^{1/2}(1+x^2)^{1/2}}\,dx$ and $J = \int{x^{1/2}(1+x^2)^{1/2}}\,dx$. How can I solve the integrals $I$ and $J$?","Question: Solve the integral   $$ \int\frac{1+x+\sqrt{1+x^2}}{\sqrt{x+1}+\sqrt{x}}\,dx $$ My solution: Multiply both the numerator and denominator by $\sqrt{x+1}-\sqrt{x}$. This changes the integral to $$ \begin{align} \int&\left(1+x+\sqrt{1+x^2}\right)\left(\sqrt{x+1}-\sqrt{x}\right)\,dx\\ &= \int{(1+x)^{3/2}}\,dx-\int{\sqrt{(1+x)(1+x^2)}}\,dx-\int{\sqrt{x}(1+x)}\,dx-\int{\sqrt{x}\sqrt{1+x^2}}\,dx\\ & = \frac{2}{5}(1+x)^{5/2}-I-\frac{2}{3}x^{3/2}-\frac{2}{5}x^{5/2}-J \end{align} $$ where $I = \int{(1+x)^{1/2}(1+x^2)^{1/2}}\,dx$ and $J = \int{x^{1/2}(1+x^2)^{1/2}}\,dx$. How can I solve the integrals $I$ and $J$?",,"['calculus', 'integration', 'indefinite-integrals']"
39,Integral of $\sqrt{1 + \sqrt{x}}$,Integral of,\sqrt{1 + \sqrt{x}},"My professor wants us to do this problem to refresh ourselves with substitution. We have to solve: $$\int\sqrt{1 + \sqrt{x}}\,\mathrm dx$$ $$\int\sqrt{1 + \sqrt{1 + \sqrt{x}}}\,\mathrm dx$$ ...etc... If someone could point me in the right direction with the first one, I think I can handle the others. Thanks for the help guys!","My professor wants us to do this problem to refresh ourselves with substitution. We have to solve: $$\int\sqrt{1 + \sqrt{x}}\,\mathrm dx$$ $$\int\sqrt{1 + \sqrt{1 + \sqrt{x}}}\,\mathrm dx$$ ...etc... If someone could point me in the right direction with the first one, I think I can handle the others. Thanks for the help guys!",,"['calculus', 'integration', 'indefinite-integrals', 'radicals']"
40,"Closed form for $\int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx$",Closed form for,"\int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx","I am trying to find the closed form for the following integral: $$\int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx, \quad \forall m,n \in \mathbb{N}$$ where $\{x\}$ denotes the fractional part of $x$ . This integral is a generalization of some of the fractional part integrals found in Ovidiu Furdui's book Limits, Series, and Fractional Part Integrals: Problems in Mathematical Analysis . My attempt: $$I(m,n):= \int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx, \quad \forall m,n \in \mathbb{N}.$$ \begin{align*} I(m,n) &\stackrel{x \mapsto \frac{m}{x}}{=} m\int_m^\infty \frac{\{x\}^n}{x^2}\,dx\\ &= m\sum_{k = m}^\infty \int_k^{k+1} \frac{(x-k)^n}{x^2}\,dx\\ &= m\sum_{k = m}^\infty \sum_{\ell=0}^n (-1)^\ell \binom{n}{\ell} k^n\int_k^{k+1} \frac{x^{n - \ell}}{x^2}\,dx\\ &= m\sum_{k = m}^\infty\left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\int_k^{k+1} \frac{x^{n - \ell}}{x^2}\,dx\right] + (-1)^{n-1}n k^{n-1} \int_0^1 \frac{1}{x}\,dx + (-1)^n k^n\int_k^{k+1} \frac{1}{x^2}\,dx\right]\\ &= m\sum_{k = m}^\infty\left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1}\right] + (-1)^{n-1}n k^{n-1} \log\left(\frac{k+1}{k} \right) + (-1)^n \frac{k^{n-1}}{k+1}\right]\\ &= \lim_{N \to \infty} m\sum_{k = m}^N \left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1} \right] + (-1)^{n-1}n k^{n-1} \log\left(\frac{k+1}{k} \right) + (-1)^n \frac{k^{n-1}}{k+1}\right]\\ &= \lim_{N \to \infty} m\left[ \sum_{\ell=0}^{n-2}\sum_{k = m}^N\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1} \right] + (-1)^{n-1}n \sum_{k = m}^N \log\left[\left(\frac{k+1}{k} \right)^{k^{n-1}}\right] + (-1)^n \sum_{k = m}^N \frac{k^{n-1}}{k+1}\right]. \end{align*} From here I began to struggle. I believe I I was able to make some good progress with the sum with the logarithm. To do so I defined something I will call the generalized hyperfactorials (I do not know if such a function exists in literature): $$\mathrm{H}(k,n) := \prod_{j = 1}^n j^{j^k}.$$ We see that if $k = 0$ , we get the regular factorial. If we let $k = 1$ , we get the hyperfactorial. With this function, I believe I the partial sum with the logarithms has a closed form: \begin{align*} \sum_{k = m}^N \log\left[\left(\frac{k+1}{k} \right)^{k^{n-1}}\right] &= \log\left[\prod_{k = m}^N\left(\frac{k+1}{k} \right)^{k^{n-1}}\right]\\ &= \log\left[\frac{(m+1)^{m^{n-1}} \times (m+2)^{(m+1)^{n-1}} \times \ldots \times N^{(N-1)^{n-1}} \times (N+1)^{N^{n-1}}}{m^{m^{n-1}} \times (m+1)^{(m+1)^{n-1}} \times (m+2)^{(m+2)^{n-1}} \times \ldots \times N^{N^{n-1}}}\right]\\ &= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{m^{n-1} - (m+1)^{n-1}} \times (m+2)^{(m+1)^{n-1} - (m+2)^{n-1}} \times N^{(N - 1)^{n-1} - N^{n-1}}\right)\right]\\ &= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{((m+1) - 1)^{n-1} - (m+1)^{n-1}} \times (m+2)^{((m+2) - 1)^{n-1} - (m+2)^{n-1}} \times N^{(N - 1)^{n-1} - N^{n-1}}\right)\right]\\ &= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p}(m+1)^{n - p -1}} \times (m+2)^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p}(m+2)^{n - p -1}} \times \ldots \times N^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p} N^{n - p -1}}\right)\right]\\ &= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \prod_{p=1}^{n-1}\left((m+1)^{(m+1)^{n - p -1}} \times (m+2)^{(m+2)^{n - p -1}} \times \ldots \times N^{ N^{n - p -1}}\right)^{(-1)^p \binom{n-1}{p}}\right]\\ &= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \prod_{p=1}^{n-1}\left(\frac{\mathrm{H}(n -p-1,N)}{\mathrm{H}(n -p-1,m)}\right)^{(-1)^p \binom{n-1}{p}}\right]. \end{align*} Hopefully I did not make any typos. I believe the integrals has a strong connection to the Bendersky-Adamchik constants. Here are some papers on the constants Some new quicker approximations of Glaisher–Kinkelin's and Bendersky–Adamchik's constants Closed-form calculation of infinite products of Glaisher-type related to Dirichlet series From these papers, we can use the limit definition of the Bendersky-Adamchik constants to derive Stirling-like asymptotic formulas for the generalized hyperfactorials, which I conjecture will be handy when evaluating the limit. I also believe that the other sums left to be simplified could be simplified via the Euler-Maclaurin summation formula or even Faulhaber's formula somewhere down the line. Possibly the Bernoulli numbers from the definition of the limit definition of the Bendersky-Adamchik constants interact with Bernoulli numbers from the Euler-Maclaurin summation formula. Is it possible that I am approaching this integral the wrong way? If so, what would be the best way to tackle this integral?","I am trying to find the closed form for the following integral: where denotes the fractional part of . This integral is a generalization of some of the fractional part integrals found in Ovidiu Furdui's book Limits, Series, and Fractional Part Integrals: Problems in Mathematical Analysis . My attempt: From here I began to struggle. I believe I I was able to make some good progress with the sum with the logarithm. To do so I defined something I will call the generalized hyperfactorials (I do not know if such a function exists in literature): We see that if , we get the regular factorial. If we let , we get the hyperfactorial. With this function, I believe I the partial sum with the logarithms has a closed form: Hopefully I did not make any typos. I believe the integrals has a strong connection to the Bendersky-Adamchik constants. Here are some papers on the constants Some new quicker approximations of Glaisher–Kinkelin's and Bendersky–Adamchik's constants Closed-form calculation of infinite products of Glaisher-type related to Dirichlet series From these papers, we can use the limit definition of the Bendersky-Adamchik constants to derive Stirling-like asymptotic formulas for the generalized hyperfactorials, which I conjecture will be handy when evaluating the limit. I also believe that the other sums left to be simplified could be simplified via the Euler-Maclaurin summation formula or even Faulhaber's formula somewhere down the line. Possibly the Bernoulli numbers from the definition of the limit definition of the Bendersky-Adamchik constants interact with Bernoulli numbers from the Euler-Maclaurin summation formula. Is it possible that I am approaching this integral the wrong way? If so, what would be the best way to tackle this integral?","\int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx, \quad \forall m,n \in \mathbb{N} \{x\} x I(m,n):= \int_0^1 \left\{ \frac{m}{x}\right\}^n\,dx, \quad \forall m,n \in \mathbb{N}. \begin{align*}
I(m,n)
&\stackrel{x \mapsto \frac{m}{x}}{=} m\int_m^\infty \frac{\{x\}^n}{x^2}\,dx\\
&= m\sum_{k = m}^\infty \int_k^{k+1} \frac{(x-k)^n}{x^2}\,dx\\
&= m\sum_{k = m}^\infty \sum_{\ell=0}^n (-1)^\ell \binom{n}{\ell} k^n\int_k^{k+1} \frac{x^{n - \ell}}{x^2}\,dx\\
&= m\sum_{k = m}^\infty\left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\int_k^{k+1} \frac{x^{n - \ell}}{x^2}\,dx\right] + (-1)^{n-1}n k^{n-1} \int_0^1 \frac{1}{x}\,dx + (-1)^n k^n\int_k^{k+1} \frac{1}{x^2}\,dx\right]\\
&= m\sum_{k = m}^\infty\left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1}\right] + (-1)^{n-1}n k^{n-1} \log\left(\frac{k+1}{k} \right) + (-1)^n \frac{k^{n-1}}{k+1}\right]\\
&= \lim_{N \to \infty} m\sum_{k = m}^N \left[ \sum_{\ell=0}^{n-2}\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1} \right] + (-1)^{n-1}n k^{n-1} \log\left(\frac{k+1}{k} \right) + (-1)^n \frac{k^{n-1}}{k+1}\right]\\
&= \lim_{N \to \infty} m\left[ \sum_{\ell=0}^{n-2}\sum_{k = m}^N\left[ (-1)^\ell \binom{n}{\ell} k^n\frac{(k+1)^{n-\ell-1}-k^{n-\ell-1}}{n-\ell-1} \right] + (-1)^{n-1}n \sum_{k = m}^N \log\left[\left(\frac{k+1}{k} \right)^{k^{n-1}}\right] + (-1)^n \sum_{k = m}^N \frac{k^{n-1}}{k+1}\right].
\end{align*} \mathrm{H}(k,n) := \prod_{j = 1}^n j^{j^k}. k = 0 k = 1 \begin{align*}
\sum_{k = m}^N \log\left[\left(\frac{k+1}{k} \right)^{k^{n-1}}\right]
&= \log\left[\prod_{k = m}^N\left(\frac{k+1}{k} \right)^{k^{n-1}}\right]\\
&= \log\left[\frac{(m+1)^{m^{n-1}} \times (m+2)^{(m+1)^{n-1}} \times \ldots \times N^{(N-1)^{n-1}} \times (N+1)^{N^{n-1}}}{m^{m^{n-1}} \times (m+1)^{(m+1)^{n-1}} \times (m+2)^{(m+2)^{n-1}} \times \ldots \times N^{N^{n-1}}}\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{m^{n-1} - (m+1)^{n-1}} \times (m+2)^{(m+1)^{n-1} - (m+2)^{n-1}} \times N^{(N - 1)^{n-1} - N^{n-1}}\right)\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{((m+1) - 1)^{n-1} - (m+1)^{n-1}} \times (m+2)^{((m+2) - 1)^{n-1} - (m+2)^{n-1}} \times N^{(N - 1)^{n-1} - N^{n-1}}\right)\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \left((m+1)^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p}(m+1)^{n - p -1}} \times (m+2)^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p}(m+2)^{n - p -1}} \times \ldots \times N^{\sum_{p=1}^{n-1} (-1)^p \binom{n-1}{p} N^{n - p -1}}\right)\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \prod_{p=1}^{n-1}\left((m+1)^{(m+1)^{n - p -1}} \times (m+2)^{(m+2)^{n - p -1}} \times \ldots \times N^{ N^{n - p -1}}\right)^{(-1)^p \binom{n-1}{p}}\right]\\
&= \log\left[\frac{(N+1)^{N^{n-1}}}{m^{m^{n-1}}} \prod_{p=1}^{n-1}\left(\frac{\mathrm{H}(n -p-1,N)}{\mathrm{H}(n -p-1,m)}\right)^{(-1)^p \binom{n-1}{p}}\right].
\end{align*}","['calculus', 'integration', 'sequences-and-series', 'summation', 'improper-integrals']"
41,Proof verification: $\lim\limits_{s\to\infty}\zeta(s)=1$,Proof verification:,\lim\limits_{s\to\infty}\zeta(s)=1,"Note : I'm aware that there are much simpler proofs for this result. I decided to go with this approach because $(1)$ it was a nice challenge, and $(2)$ it makes use of a nice identity shown below. I've recently been spending time evaluating integrals involving the floor function. One of them was $$\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx$$ for integers $p\geq 0$ . By splitting the integral $$\int_\frac{1}{k}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx$$ into a sum of integrals indexed by the intervals $[1/(i+1),1/i]$ for $i=1,2,3,...,k-1$ , evaluating each of the integrals, and manipulating the resulting sum, I found that $$\int_\frac{1}{k}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx=\frac{1}{(p+1)k}-1+\frac{1}{p+1}\left(\sum_{n=1}^{k-1}\frac{1}{n^{p+2}}+\sum_{m=2}^{p+1}\sum_{n=1}^{k}\frac{1}{n^m}\right)$$ which, after letting $k\to\infty$ , yields \begin{align} \int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx &= -1+\frac{1}{p+1}\left(\zeta(p+2)+\sum_{m=2}^{p+1}\zeta(m)\right)\\ &= -1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m) \end{align} Pretty neat! I believe I can use this equation to give an overkill proof of $\lim_{s\to\infty}\zeta(s)=1$ ( $s$ is a real parameter). Here is my attempt: Beginning We first note that the sequence $\{\zeta(n)\}_{n=2}^\infty$ certainly has a limit, since $\zeta$ is strictly decreasing and bounded below by $1$ (both of these facts easily follow from the series $\sum_{n=1}^\infty 1/n^s$ ), so \begin{align} \lim_{n\to\infty}\zeta(n) &= L & (1) \end{align} for some real number $L\geq 1$ . Fixing an arbitrary $\varepsilon>0$ , we infer that for some $N\in\mathbb{N}$ , $$L-\varepsilon<\zeta(n)<L+\varepsilon\text{ for every }n>N$$ $$\implies \sum_{n=2}^{p+2}(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<\sum_{n=2}^{p+2}(L+\varepsilon)\text{ for every }p>N$$ $$\implies (p+2-1)(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<(p+2-1)(L+\varepsilon)\text{ for every }p>N$$ $$\implies (p+1)(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<(p+1)(L+\varepsilon)\text{ for every }p>N$$ $$\implies L-\varepsilon<\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)<L+\varepsilon\text{ for every }p>N$$ Since $\varepsilon>0$ was fixed arbitrarily, we can apply the prior sequence of implications to any positive real number. This shows that \begin{align} \lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n) &= L & (2) \end{align} Now consider the identity $$\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx=-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)$$ We can write \begin{align} 0<\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx &= \int_0^\frac{1}{2} \frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\int_\frac{1}{2}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx\\ &\leq \int_0^\frac{1}{2}\frac{\left(\frac{1}{2}\right)^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\int_\frac{1}{2}^1 \frac{x^p}{1}dx\\ &= \left(\frac{1}{2}\right)^p\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{\left(\frac{1}{2}\right)^{p+1}}{p+1} \end{align} and thus $$0<-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)\leq\frac{1}{2^p}\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{1}{(p+1)\cdot 2^{p+1}}$$ Since $$\lim_{p\to\infty}\left(\frac{1}{2^p}\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{1}{(p+1)\cdot 2^{p+1}}\right)=0$$ the Squeeze Theorem gives $$\lim_{p\to\infty}\left(-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)\right)=0$$ which is equivalent to $\lim_{p\to\infty}\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)=1$ . Combining this with $(2)$ , we see that we must have $L=1$ . We deduce from $(1)$ that $$\lim_{n\to\infty}\zeta(n)=1$$ We now do the final push and get $\lim_{s\to\infty}\zeta(s)=1$ . Fix an arbitrary $\varepsilon>0$ . Since the sequence $\{\zeta(n)\}$ converges to $1$ and $\zeta(t)>1$ for every real $t>1$ , there is an $N\in\mathbb{N}$ such that $$0<\zeta(n)-1<\varepsilon\text{ for every }n>N$$ We know that $\zeta(s)<\zeta(n)$ for every real $s>n$ , so $$0<\zeta(s)-1<\zeta(n)-1<\varepsilon\text{ for every real }s>n>N$$ Since $\varepsilon$ was fixed arbitrarily, we can apply the preceding argument to every positive real number, so we are done. $\blacksquare$ Let me know what you think! If you identify any errors or optimizations, feel free to share them with me. Edit : as @stochasticboy321 kindly pointed out, my proof has a small error. You see, I can't deduce the inequality $$\sum_{n=2}^{p+2}(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<\sum_{n=2}^{p+2}(L+\varepsilon)$$ from the fact that $L-\varepsilon<\zeta(n)<L+\varepsilon$ for every $n>N$ , since this assumes that the latter inequality is also true for $2,3,...,N$ . A correct approach would $(1)$ use the integral to prove that $$\lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)=1$$ $(2)$ write $$\sum_{n=2}^{p+2}\zeta(n)=\sum_{n=2}^{N}\zeta(n)+\sum_{n=N+1}^{p+2}\zeta(n)$$ for an arbitrary $p\geq N-1$ , $(3)$ apply the inequality $L-\varepsilon<\zeta(n)<L+\varepsilon$ to get $$(L-\varepsilon)(p+2-N)+\sum_{n=2}^{N}\zeta(n)<\sum_{n=2}^{p+2}\zeta(n)<(L+\varepsilon)(p+2-N)+\sum_{n=2}^{N}\zeta(n)$$ $$\implies (L-\varepsilon)\left(\frac{p+2}{p+1}-\frac{N}{p+1}\right)+\frac{1}{p+1}\sum_{n=2}^{N}\zeta(n)<\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)$$ $$\text{ and }$$ $$\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)<(L+\varepsilon)\left(\frac{p+2}{p+1}-\frac{N}{p+1}\right)+\frac{1}{p+1}\sum_{n=2}^{N}\zeta(n)$$ and $(4)$ let $p\to\infty$ to yield $$L-\varepsilon\leq\lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)=1\leq L+\varepsilon$$ from which $L=1$ follows because $\varepsilon$ was fixed arbitrarily, so the inequality $|L-1|\leq\varepsilon$ will hold for any $\varepsilon>0$ . For the sake of honesty, I won't edit the original proof.","Note : I'm aware that there are much simpler proofs for this result. I decided to go with this approach because it was a nice challenge, and it makes use of a nice identity shown below. I've recently been spending time evaluating integrals involving the floor function. One of them was for integers . By splitting the integral into a sum of integrals indexed by the intervals for , evaluating each of the integrals, and manipulating the resulting sum, I found that which, after letting , yields Pretty neat! I believe I can use this equation to give an overkill proof of ( is a real parameter). Here is my attempt: Beginning We first note that the sequence certainly has a limit, since is strictly decreasing and bounded below by (both of these facts easily follow from the series ), so for some real number . Fixing an arbitrary , we infer that for some , Since was fixed arbitrarily, we can apply the prior sequence of implications to any positive real number. This shows that Now consider the identity We can write and thus Since the Squeeze Theorem gives which is equivalent to . Combining this with , we see that we must have . We deduce from that We now do the final push and get . Fix an arbitrary . Since the sequence converges to and for every real , there is an such that We know that for every real , so Since was fixed arbitrarily, we can apply the preceding argument to every positive real number, so we are done. Let me know what you think! If you identify any errors or optimizations, feel free to share them with me. Edit : as @stochasticboy321 kindly pointed out, my proof has a small error. You see, I can't deduce the inequality from the fact that for every , since this assumes that the latter inequality is also true for . A correct approach would use the integral to prove that write for an arbitrary , apply the inequality to get and let to yield from which follows because was fixed arbitrarily, so the inequality will hold for any . For the sake of honesty, I won't edit the original proof.","(1) (2) \int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx p\geq 0 \int_\frac{1}{k}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx [1/(i+1),1/i] i=1,2,3,...,k-1 \int_\frac{1}{k}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx=\frac{1}{(p+1)k}-1+\frac{1}{p+1}\left(\sum_{n=1}^{k-1}\frac{1}{n^{p+2}}+\sum_{m=2}^{p+1}\sum_{n=1}^{k}\frac{1}{n^m}\right) k\to\infty \begin{align}
\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx &= -1+\frac{1}{p+1}\left(\zeta(p+2)+\sum_{m=2}^{p+1}\zeta(m)\right)\\
&= -1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)
\end{align} \lim_{s\to\infty}\zeta(s)=1 s \{\zeta(n)\}_{n=2}^\infty \zeta 1 \sum_{n=1}^\infty 1/n^s \begin{align}
\lim_{n\to\infty}\zeta(n) &= L & (1)
\end{align} L\geq 1 \varepsilon>0 N\in\mathbb{N} L-\varepsilon<\zeta(n)<L+\varepsilon\text{ for every }n>N \implies \sum_{n=2}^{p+2}(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<\sum_{n=2}^{p+2}(L+\varepsilon)\text{ for every }p>N \implies (p+2-1)(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<(p+2-1)(L+\varepsilon)\text{ for every }p>N \implies (p+1)(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<(p+1)(L+\varepsilon)\text{ for every }p>N \implies L-\varepsilon<\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)<L+\varepsilon\text{ for every }p>N \varepsilon>0 \begin{align}
\lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n) &= L & (2)
\end{align} \int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx=-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m) \begin{align}
0<\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx &= \int_0^\frac{1}{2} \frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\int_\frac{1}{2}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx\\
&\leq \int_0^\frac{1}{2}\frac{\left(\frac{1}{2}\right)^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\int_\frac{1}{2}^1 \frac{x^p}{1}dx\\
&= \left(\frac{1}{2}\right)^p\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{\left(\frac{1}{2}\right)^{p+1}}{p+1}
\end{align} 0<-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)\leq\frac{1}{2^p}\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{1}{(p+1)\cdot 2^{p+1}} \lim_{p\to\infty}\left(\frac{1}{2^p}\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{1}{(p+1)\cdot 2^{p+1}}\right)=0 \lim_{p\to\infty}\left(-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)\right)=0 \lim_{p\to\infty}\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)=1 (2) L=1 (1) \lim_{n\to\infty}\zeta(n)=1 \lim_{s\to\infty}\zeta(s)=1 \varepsilon>0 \{\zeta(n)\} 1 \zeta(t)>1 t>1 N\in\mathbb{N} 0<\zeta(n)-1<\varepsilon\text{ for every }n>N \zeta(s)<\zeta(n) s>n 0<\zeta(s)-1<\zeta(n)-1<\varepsilon\text{ for every real }s>n>N \varepsilon \blacksquare \sum_{n=2}^{p+2}(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<\sum_{n=2}^{p+2}(L+\varepsilon) L-\varepsilon<\zeta(n)<L+\varepsilon n>N 2,3,...,N (1) \lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)=1 (2) \sum_{n=2}^{p+2}\zeta(n)=\sum_{n=2}^{N}\zeta(n)+\sum_{n=N+1}^{p+2}\zeta(n) p\geq N-1 (3) L-\varepsilon<\zeta(n)<L+\varepsilon (L-\varepsilon)(p+2-N)+\sum_{n=2}^{N}\zeta(n)<\sum_{n=2}^{p+2}\zeta(n)<(L+\varepsilon)(p+2-N)+\sum_{n=2}^{N}\zeta(n) \implies (L-\varepsilon)\left(\frac{p+2}{p+1}-\frac{N}{p+1}\right)+\frac{1}{p+1}\sum_{n=2}^{N}\zeta(n)<\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n) \text{ and } \frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)<(L+\varepsilon)\left(\frac{p+2}{p+1}-\frac{N}{p+1}\right)+\frac{1}{p+1}\sum_{n=2}^{N}\zeta(n) (4) p\to\infty L-\varepsilon\leq\lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)=1\leq L+\varepsilon L=1 \varepsilon |L-1|\leq\varepsilon \varepsilon>0","['calculus', 'sequences-and-series', 'solution-verification', 'riemann-zeta']"
42,"If $\,a_n\searrow 0\,$ and $\,\sum_{n=1}^\infty a_n<\infty,\,$ does this imply that $\,n\log n\, a_n\to 0$?",If  and  does this imply that ?,"\,a_n\searrow 0\, \,\sum_{n=1}^\infty a_n<\infty,\, \,n\log n\, a_n\to 0","A quite elegant and classic exercise of Calculus (in infinite series) is the following: If the non-negative sequence $\{a_n\}$ is decreasing and $\sum_{n=1}^\infty a_n<\infty$, then $na_n\to 0$. To show this observe that, if $\,\sum_{n\ge n_0}a_n<\varepsilon/2$, then for for $n\ge 2n_0+1$,   $$\frac{\varepsilon}{2}>a_{\lfloor n/2\rfloor}+\cdots+a_n\ge \frac{1}{2}na_n\ge 0.$$ This, in a sense, is related to the fact that $\sum\frac{1}{n}=\infty$. To go one step further, since $\sum\frac{1}{n\log n}=\infty$, can we obtain, with the same assumptions on $\{a_n\}$, that $\,n\log n\, a_n\to 0$? This conjecture holds with the additional assumption that $b_n=na_n$ is also decreasing. To see this, let $n_0\in\mathbb N$, such that $\sum_{n\ge n_0}a_n<\varepsilon$. Then for $n\ge n_0^2$, we have $$ \varepsilon>\sum_{\sqrt{n}\le k\le n}a_n\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor}\sum_{k=\lfloor2^{\ell-1}\log_2\sqrt{n}\rfloor+1}^{ \lfloor2^\ell\log_2\sqrt{n}\rfloor}a_k\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor} (2^{\ell-1}\log_2\sqrt{n}-1)a_{\lfloor2^\ell\log_2\sqrt{n}\rfloor} \\ \ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor} \Big(\frac{1}{2}{\lfloor2^\ell\log_2\sqrt{n}\rfloor}-1\Big) a_{{\lfloor2^\ell\log_2\sqrt{n}\rfloor}}\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor}\Big(\frac{1}{2}n-1\Big)a_n\ge (\log_2 n-1)\Big(\frac{1}{2}n-1\Big)a_n. $$  Hence, $(\log_2 n-1)\big(\frac{1}{2}n-1\big)a_n\to 0$, which implies that $\,\,n\log n\,a_n\to 0$. One step further: If $a_n>0$, $\sum_{n=1^\infty}a_n<0$ $a_n$, and the sequences $na_n$ and $n\log n a_n$ are decreasing, then $$ n\log n \log\log n\,a_n\to 0. $$","A quite elegant and classic exercise of Calculus (in infinite series) is the following: If the non-negative sequence $\{a_n\}$ is decreasing and $\sum_{n=1}^\infty a_n<\infty$, then $na_n\to 0$. To show this observe that, if $\,\sum_{n\ge n_0}a_n<\varepsilon/2$, then for for $n\ge 2n_0+1$,   $$\frac{\varepsilon}{2}>a_{\lfloor n/2\rfloor}+\cdots+a_n\ge \frac{1}{2}na_n\ge 0.$$ This, in a sense, is related to the fact that $\sum\frac{1}{n}=\infty$. To go one step further, since $\sum\frac{1}{n\log n}=\infty$, can we obtain, with the same assumptions on $\{a_n\}$, that $\,n\log n\, a_n\to 0$? This conjecture holds with the additional assumption that $b_n=na_n$ is also decreasing. To see this, let $n_0\in\mathbb N$, such that $\sum_{n\ge n_0}a_n<\varepsilon$. Then for $n\ge n_0^2$, we have $$ \varepsilon>\sum_{\sqrt{n}\le k\le n}a_n\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor}\sum_{k=\lfloor2^{\ell-1}\log_2\sqrt{n}\rfloor+1}^{ \lfloor2^\ell\log_2\sqrt{n}\rfloor}a_k\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor} (2^{\ell-1}\log_2\sqrt{n}-1)a_{\lfloor2^\ell\log_2\sqrt{n}\rfloor} \\ \ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor} \Big(\frac{1}{2}{\lfloor2^\ell\log_2\sqrt{n}\rfloor}-1\Big) a_{{\lfloor2^\ell\log_2\sqrt{n}\rfloor}}\ge \sum_{\ell=1}^{\lfloor\log_2 \sqrt{n}\rfloor}\Big(\frac{1}{2}n-1\Big)a_n\ge (\log_2 n-1)\Big(\frac{1}{2}n-1\Big)a_n. $$  Hence, $(\log_2 n-1)\big(\frac{1}{2}n-1\big)a_n\to 0$, which implies that $\,\,n\log n\,a_n\to 0$. One step further: If $a_n>0$, $\sum_{n=1^\infty}a_n<0$ $a_n$, and the sequences $na_n$ and $n\log n a_n$ are decreasing, then $$ n\log n \log\log n\,a_n\to 0. $$",,"['calculus', 'sequences-and-series', 'convergence-divergence', 'summation', 'logarithms']"
43,How to prove that $x^y - y^x = x + y$ has only one solution in positive integers?,How to prove that  has only one solution in positive integers?,x^y - y^x = x + y,"Prompted by this question , I tried to show that $(2,5)$ is the only solution in positive integers of $x^y - y^x = x+y$ (which would show, a fortiori , that it's the only solution in primes). It's convenient to rewrite the equation as $f(x,y) = x^y - y^x - x - y = 0$. With the aid of some trial calculations, I reasoned informally as follows: If $x=1$ then $f(x,y) = -2y=0$, implying $y=0$. If $x=2$ and $y \leq 5$, then a case-by-case check shows that only $(2,5)$ is a solution. If $(x,y) = (2,6)$ then $f(x,y) = 20$, and as $y$ increases above $6$, $f(x,y)$ increases. If $x \geq 3$ and $y \leq x$ then $f(x,y) < 0$. If $x \geq 3$ and $y=x+1$, then $f(x,y)>0$, and as $y$ increases above $x+1$, $f(x,y)$ increases. How can the above be made into a rigorous proof?  I've included calculus as a tag since it could be useful in showing under what conditions $f(x,y)$ is an increasing function of $y$ (viewing it as a real variable).","Prompted by this question , I tried to show that $(2,5)$ is the only solution in positive integers of $x^y - y^x = x+y$ (which would show, a fortiori , that it's the only solution in primes). It's convenient to rewrite the equation as $f(x,y) = x^y - y^x - x - y = 0$. With the aid of some trial calculations, I reasoned informally as follows: If $x=1$ then $f(x,y) = -2y=0$, implying $y=0$. If $x=2$ and $y \leq 5$, then a case-by-case check shows that only $(2,5)$ is a solution. If $(x,y) = (2,6)$ then $f(x,y) = 20$, and as $y$ increases above $6$, $f(x,y)$ increases. If $x \geq 3$ and $y \leq x$ then $f(x,y) < 0$. If $x \geq 3$ and $y=x+1$, then $f(x,y)>0$, and as $y$ increases above $x+1$, $f(x,y)$ increases. How can the above be made into a rigorous proof?  I've included calculus as a tag since it could be useful in showing under what conditions $f(x,y)$ is an increasing function of $y$ (viewing it as a real variable).",,"['calculus', 'elementary-number-theory', 'diophantine-equations']"
44,Why is the area under $\frac1{\sqrt{x}}$ finite and the area under $\frac1x$ infinite?,Why is the area under  finite and the area under  infinite?,\frac1{\sqrt{x}} \frac1x,"If this integral is calculated analytically, $$\int_0^1 \frac1{\sqrt{x}} dx = 2\sqrt{1}-2\sqrt{0}=2.$$ However, the graph of $\dfrac1{\sqrt{x}}\to \infty$ as $x\to 0$, so the area under the  graph should approach infinity. In contrast, the integral: $$\int_0^1 \frac1{x} dx = \ln1-\ln0 =\infty.$$ Indeed, the graph of $\dfrac1x\to \infty $as $x\to 0$, so the area under the  graph should approach infinity. My question: The red line represents $\dfrac1{\sqrt{x}}$ and the blue line represents $\dfrac1x$. How is it that the area under the red line is finite and the area under the blue line is infinite?","If this integral is calculated analytically, $$\int_0^1 \frac1{\sqrt{x}} dx = 2\sqrt{1}-2\sqrt{0}=2.$$ However, the graph of $\dfrac1{\sqrt{x}}\to \infty$ as $x\to 0$, so the area under the  graph should approach infinity. In contrast, the integral: $$\int_0^1 \frac1{x} dx = \ln1-\ln0 =\infty.$$ Indeed, the graph of $\dfrac1x\to \infty $as $x\to 0$, so the area under the  graph should approach infinity. My question: The red line represents $\dfrac1{\sqrt{x}}$ and the blue line represents $\dfrac1x$. How is it that the area under the red line is finite and the area under the blue line is infinite?",,"['calculus', 'integration', 'definite-integrals']"
45,little-o and its properties,little-o and its properties,,"I know that $f(x) = o(g(x))$ for $x \to \infty $ if (and only if) $\lim_{x \to \infty}\frac{f(x)}{g(x)}=0$ Which means than $f(x)$ has a order of growth less than that of $g(x)$. 1) I'm still confused if $x \to 0$. Because in this case $x^5 = o(x^2)$ 2) Can someone list me the properties of little-o? For now, I know the following: $f(x)*o(g(x) = o(f(x)*g(x))$ $o(f(x)) \pm o(f(x)) = o(f(x))$ Thank you!","I know that $f(x) = o(g(x))$ for $x \to \infty $ if (and only if) $\lim_{x \to \infty}\frac{f(x)}{g(x)}=0$ Which means than $f(x)$ has a order of growth less than that of $g(x)$. 1) I'm still confused if $x \to 0$. Because in this case $x^5 = o(x^2)$ 2) Can someone list me the properties of little-o? For now, I know the following: $f(x)*o(g(x) = o(f(x)*g(x))$ $o(f(x)) \pm o(f(x)) = o(f(x))$ Thank you!",,"['calculus', 'limits', 'asymptotics', 'infinitesimals']"
46,Euler-Maclaurin Summation Formula for Multiple Sums,Euler-Maclaurin Summation Formula for Multiple Sums,,"The Euler-Maclaurin summation formula is \begin{eqnarray} \sum_{k = a}^{b} f(k)  = \int_{a}^{b} f(t) \, dt  + B_1 (f(a) + f(b)) + \sum_{n = 1}^{N} \frac{B_{2n}}{(2n)!} ( f^{(2n-1)}(b) -  f^{(2n-1)}(a) )  + R_{N}, \end{eqnarray} where $B_{n}$ is the $n^{\text{th}}$-Bernoulli number taking $B_{1} = \tfrac{1}{2}$, and the remainder term is bounded by the following \begin{align} |R_{N}| \leq    \frac{|B_{2N} |}{(2n)!}  \int_{a}^{b} | f^{(2N)}(t) | \, dt. \end{align} for any arbitrary positive integer $N$. Is there a similar formula for nested sums of the form, \begin{eqnarray} \sum_{k_1 = a_1}^{b_1} \cdots \sum_{k_n = a_n}^{b_n} f(k_1, \dots, k_n). \end{eqnarray} Thanks!","The Euler-Maclaurin summation formula is \begin{eqnarray} \sum_{k = a}^{b} f(k)  = \int_{a}^{b} f(t) \, dt  + B_1 (f(a) + f(b)) + \sum_{n = 1}^{N} \frac{B_{2n}}{(2n)!} ( f^{(2n-1)}(b) -  f^{(2n-1)}(a) )  + R_{N}, \end{eqnarray} where $B_{n}$ is the $n^{\text{th}}$-Bernoulli number taking $B_{1} = \tfrac{1}{2}$, and the remainder term is bounded by the following \begin{align} |R_{N}| \leq    \frac{|B_{2N} |}{(2n)!}  \int_{a}^{b} | f^{(2N)}(t) | \, dt. \end{align} for any arbitrary positive integer $N$. Is there a similar formula for nested sums of the form, \begin{eqnarray} \sum_{k_1 = a_1}^{b_1} \cdots \sum_{k_n = a_n}^{b_n} f(k_1, \dots, k_n). \end{eqnarray} Thanks!",,['calculus']
47,"Conjecture:$\int_0^{\pi/2} \left(\frac{\sin(2n+1)x}{\sin x}\right)^\beta dx$ is integer multiple of $\pi/2$,for integer $\beta>2$","Conjecture: is integer multiple of ,for integer",\int_0^{\pi/2} \left(\frac{\sin(2n+1)x}{\sin x}\right)^\beta dx \pi/2 \beta>2,"I was solving the integral $$I_n=\int_0^{\frac {\pi}{2}} \left(\frac {\sin ((2n+1)x)}{\sin x}\right)^2 dx$$ With $n\ge 0$ And $n\in \mathbb{N}$ On solving, I got $$I_n =\frac {(2n+1)\pi}{2}$$ But, due to curiosity, I started investigating the family of integrals as $$I_n(\beta) =\int_0^{\frac {\pi}{2}} \left(\frac {\sin (2n+1)x}{\sin x}\right)^{\beta} dx$$ On trying various values of $\beta\gt 2$ and $\beta\in \mathbb{N}$ , I conjectured that $$I_n(\beta) =c_{\beta} \frac{\pi}{2}$$ where $c_{\beta}$ denotes ""Number of arrays of $\beta$ integers in $-n$ to $n$ with sum $0$ "" But, on trying a lot, I couldn't prove this statement. Also, I suppose that the statement could be proved with help of Dirichlet kernel, but I couldn't get the way out through it. Any help and hints to prove/disprove the conjecture are greatly appreciated.","I was solving the integral With And On solving, I got But, due to curiosity, I started investigating the family of integrals as On trying various values of and , I conjectured that where denotes ""Number of arrays of integers in to with sum "" But, on trying a lot, I couldn't prove this statement. Also, I suppose that the statement could be proved with help of Dirichlet kernel, but I couldn't get the way out through it. Any help and hints to prove/disprove the conjecture are greatly appreciated.",I_n=\int_0^{\frac {\pi}{2}} \left(\frac {\sin ((2n+1)x)}{\sin x}\right)^2 dx n\ge 0 n\in \mathbb{N} I_n =\frac {(2n+1)\pi}{2} I_n(\beta) =\int_0^{\frac {\pi}{2}} \left(\frac {\sin (2n+1)x}{\sin x}\right)^{\beta} dx \beta\gt 2 \beta\in \mathbb{N} I_n(\beta) =c_{\beta} \frac{\pi}{2} c_{\beta} \beta -n n 0,"['calculus', 'integration', 'trigonometry', 'definite-integrals']"
48,Infinite Geometric Series Formula Derivation,Infinite Geometric Series Formula Derivation,,"We know that the formula for computing a geometric series is: $$\sum_{i=1}^{\infty}{a_0r^{i-1}} = \frac{a_0}{1-r}$$ Out of curiosity, I would like ask: Is there any ways the formula can be derived other than the following two ways? Method 1 (The way I found on my own): $$\sum_{i=1}^{\infty}{a_0r^{i-1}} \equiv S$$ $$S = a_0r^0+a_0r^1+a_0r^2+\cdots$$ $$S = r\left(a_0r^{-1} + a_0r^{0} + a_0r^1+\cdots\right)$$ $$S = r\left(a_0r^{-1} + S\right)$$ $$S = a_0 + rS$$ $$(1-r)S = a_0$$ $$S = \frac{a_0}{(1-r)}$$ Note that for this to work, you must first confirm this: $$\lim_{n\to\infty} a_n = 0$$ Method 2 (The way I found on the web): $$\sum_{i=1}^{n}{a_0r^{i-1}} \equiv S_n$$ $$S_n = a_0r^0+a_0r^1+a_0r^2+\cdots + a_0 r^{n-2} + a_0 r^{n-1}$$ $$rS_n = r\left(a_0r^0+a_0r^1+a_0r^2+\cdots + a_0 r^{n-2} + a_0 r^{n-1}\right)$$ $$rS_n = a_0r^1 + a_0r^2 + a_0r^3 + \cdots + a_0 r^{n-1} + a_0 r^{n}$$ $$S_n-rS_n = a_0r^0 - a_0r^n$$ $$(1-r)S_n = a_0 - a_0 r^n$$ $$S_n = \frac{a_0(1 - r^n)}{1-r}$$ Given: $$\left|r\right| < 1,$$ $$\lim_{n\to \infty} S_n = \lim_{n\to \infty}\frac{a_0(1 - r^n)}{1-r} = \frac{a_0}{1-r}$$ I personally prefer Method 1 because it is faster and more intuitive, as we don't have to multiply by $r$ . Method 1 for formula of partial sums: $$\sum_{i=1}^{n}{a_0r^{i-1}} \equiv S_n$$ $$S_n = a_0r^0+a_0r^1+a_0r^2+\cdots+a_0r^{n-2}+a_0r^{n-1}$$ $$S_n = r\left(a_0r^{-1} + a_0r^{0} + a_0r^1+\cdots+a_0r^{n-3}+a_0r^{n-2}\right)$$ $$S_n = r\left(a_0r^{-1} + S_n - a_0r^{n-1}\right)$$ $$S_n = a_0 + rS_n - a_0r^{n}$$ $$(1-r)S_n = a_0 - a_0r^n$$ $$S_n = \frac{a_0(1 - r^n)}{(1-r)}$$","We know that the formula for computing a geometric series is: Out of curiosity, I would like ask: Is there any ways the formula can be derived other than the following two ways? Method 1 (The way I found on my own): Note that for this to work, you must first confirm this: Method 2 (The way I found on the web): Given: I personally prefer Method 1 because it is faster and more intuitive, as we don't have to multiply by . Method 1 for formula of partial sums:","\sum_{i=1}^{\infty}{a_0r^{i-1}} = \frac{a_0}{1-r} \sum_{i=1}^{\infty}{a_0r^{i-1}} \equiv S S = a_0r^0+a_0r^1+a_0r^2+\cdots S = r\left(a_0r^{-1} + a_0r^{0} + a_0r^1+\cdots\right) S = r\left(a_0r^{-1} + S\right) S = a_0 + rS (1-r)S = a_0 S = \frac{a_0}{(1-r)} \lim_{n\to\infty} a_n = 0 \sum_{i=1}^{n}{a_0r^{i-1}} \equiv S_n S_n = a_0r^0+a_0r^1+a_0r^2+\cdots + a_0 r^{n-2} + a_0 r^{n-1} rS_n = r\left(a_0r^0+a_0r^1+a_0r^2+\cdots + a_0 r^{n-2} + a_0 r^{n-1}\right) rS_n = a_0r^1 + a_0r^2 + a_0r^3 + \cdots + a_0 r^{n-1} + a_0 r^{n} S_n-rS_n = a_0r^0 - a_0r^n (1-r)S_n = a_0 - a_0 r^n S_n = \frac{a_0(1 - r^n)}{1-r} \left|r\right| < 1, \lim_{n\to \infty} S_n = \lim_{n\to \infty}\frac{a_0(1 - r^n)}{1-r} = \frac{a_0}{1-r} r \sum_{i=1}^{n}{a_0r^{i-1}} \equiv S_n S_n = a_0r^0+a_0r^1+a_0r^2+\cdots+a_0r^{n-2}+a_0r^{n-1} S_n = r\left(a_0r^{-1} + a_0r^{0} + a_0r^1+\cdots+a_0r^{n-3}+a_0r^{n-2}\right) S_n = r\left(a_0r^{-1} + S_n - a_0r^{n-1}\right) S_n = a_0 + rS_n - a_0r^{n} (1-r)S_n = a_0 - a_0r^n S_n = \frac{a_0(1 - r^n)}{(1-r)}","['calculus', 'sequences-and-series']"
49,"About $I(a,b)=\int_{a}^{b}\sqrt{1+x+x^2+x^3+x^4}\text{ d}x$",About,"I(a,b)=\int_{a}^{b}\sqrt{1+x+x^2+x^3+x^4}\text{ d}x","The following is an MCQ question, one should answer it without a calculator, within $3$ minutes. $\text{Consider the expression}$ $$I(a,b)=\int_{a}^{b}\sqrt{1+x+x^2+x^3+x^4}\text{ d}x.$$ $\text{Which of the following is true?}$ $\space\space\space\space\space\text{I.}$ $I(2,3)+I(3,4)=I(4,5)$ $\space\space\space\space\space\text{II.}$ $I(2,3)+I(3,4)<I(4,5)$ $\space\space\space\space\space\text{III.}$ $16<I(4,5)<25$ $\text{(A) }$ $\text{I only}$ $\text{(B) }$ $\text{II only}$ $\text{(C) }$ $\text{III only}$ $\text{(D) }$ $\text{I and III}$ $\text{(E) }$ $\text{II and III}$ My Attempt (which, I think, is not a feasible way since it is not that accurate and also time consuming): $\sqrt{1+x+x^2+x^3+x^4}\bigg|_{x=2}=\sqrt{1+2+2^2+2^3+2^4}=\sqrt{1+2+4+8+16}=\sqrt{31}\approx 6$ and $\sqrt{1+x+x^2+x^3+x^4}\bigg|_{x=5}=\sqrt{1+5+5^2+5^3+5^4}=\sqrt{1+5+25+125+625}=\sqrt{781}\approx 28$ Now considering the integrand, approximately, as the straight line joining the points $(2,6)$ and $(5,28)$ . The equation of the line can be found by first determining the slope, $m$ : $$m=\frac{28-6}{5-2}=\frac{22}{3}\approx 7$$ The equation of the line is therefore $$y-6=7(x-2) \implies y=7x-8$$ now $I(a,b) \approx \int_{a}^{b} (7x-8)\text{ d}x=\frac{7}{2}x^2-8x \bigg|_{x=a}^{x=b} = \dots$ and $I(2,3) \approx \int_{2}^{3} (7x-8)\text{ d}x=9.5$ and $I(3,4) \approx \int_{3}^{4} (7x-8)\text{ d}x=16.5$ and $I(4,5) \approx \int_{4}^{5} (7x-8)\text{ d}x=23.5$ From these approximations, we can say, with low confidence, that $\text{(C)}$ is the correct option, which is not. In fact $\text{(E)}$ is the correct option. Noting that we can do better approximations, but that will cost time. Attempt $\text{#}2$ (Not sure if it is a correct way): In the interval $[2,5]$ , function $f(x)=\sqrt{1+x+x^2+x^3+x^4}$ is always (positive), is always (increasing), and is always (concaving up). In that case, the inequalities will be the same when we do not consider the radical. So simply we evaluate: $$\int_{2}^{3}(1+x+x^2+x^3+x^4)\text{ d}x$$ which gives $(x+x^2/2+x^3/3+x^4/4+x^5/5) \bigg|_{x=2}^{x=3}\approx 68$ Similarly, we evaluate: $$\int_{3}^{4}(1+x+x^2+x^3+x^4)\text{ d}x \text{ and } \int_{4}^{5}(1+x+x^2+x^3+x^4)\text{ d}x$$ which give $\approx 217$ and $\approx 538$ , respectively. Clearly $68+217 < 538$ . Hence II is true. AND $16^2 = 256 < 538 < 625 = 25^2$ . Hence III is true. Well, this attempt may also require time, but still it would be nice to know if it is a correct way. So, the approach (which might be wrong) is to observe that; between the limits of integration, the integrand is positive, increasing, and concaving up, the we deal with it without the radical. If this is wrong, please support by giving a counter-example. Your help would be appreciated. Thanks!","The following is an MCQ question, one should answer it without a calculator, within minutes. My Attempt (which, I think, is not a feasible way since it is not that accurate and also time consuming): and Now considering the integrand, approximately, as the straight line joining the points and . The equation of the line can be found by first determining the slope, : The equation of the line is therefore now and and and From these approximations, we can say, with low confidence, that is the correct option, which is not. In fact is the correct option. Noting that we can do better approximations, but that will cost time. Attempt (Not sure if it is a correct way): In the interval , function is always (positive), is always (increasing), and is always (concaving up). In that case, the inequalities will be the same when we do not consider the radical. So simply we evaluate: which gives Similarly, we evaluate: which give and , respectively. Clearly . Hence II is true. AND . Hence III is true. Well, this attempt may also require time, but still it would be nice to know if it is a correct way. So, the approach (which might be wrong) is to observe that; between the limits of integration, the integrand is positive, increasing, and concaving up, the we deal with it without the radical. If this is wrong, please support by giving a counter-example. Your help would be appreciated. Thanks!","3 \text{Consider the expression} I(a,b)=\int_{a}^{b}\sqrt{1+x+x^2+x^3+x^4}\text{ d}x. \text{Which of the following is true?} \space\space\space\space\space\text{I.} I(2,3)+I(3,4)=I(4,5) \space\space\space\space\space\text{II.} I(2,3)+I(3,4)<I(4,5) \space\space\space\space\space\text{III.} 16<I(4,5)<25 \text{(A) } \text{I only} \text{(B) } \text{II only} \text{(C) } \text{III only} \text{(D) } \text{I and III} \text{(E) } \text{II and III} \sqrt{1+x+x^2+x^3+x^4}\bigg|_{x=2}=\sqrt{1+2+2^2+2^3+2^4}=\sqrt{1+2+4+8+16}=\sqrt{31}\approx 6 \sqrt{1+x+x^2+x^3+x^4}\bigg|_{x=5}=\sqrt{1+5+5^2+5^3+5^4}=\sqrt{1+5+25+125+625}=\sqrt{781}\approx 28 (2,6) (5,28) m m=\frac{28-6}{5-2}=\frac{22}{3}\approx 7 y-6=7(x-2) \implies y=7x-8 I(a,b) \approx \int_{a}^{b} (7x-8)\text{ d}x=\frac{7}{2}x^2-8x \bigg|_{x=a}^{x=b} = \dots I(2,3) \approx \int_{2}^{3} (7x-8)\text{ d}x=9.5 I(3,4) \approx \int_{3}^{4} (7x-8)\text{ d}x=16.5 I(4,5) \approx \int_{4}^{5} (7x-8)\text{ d}x=23.5 \text{(C)} \text{(E)} \text{#}2 [2,5] f(x)=\sqrt{1+x+x^2+x^3+x^4} \int_{2}^{3}(1+x+x^2+x^3+x^4)\text{ d}x (x+x^2/2+x^3/3+x^4/4+x^5/5) \bigg|_{x=2}^{x=3}\approx 68 \int_{3}^{4}(1+x+x^2+x^3+x^4)\text{ d}x \text{ and } \int_{4}^{5}(1+x+x^2+x^3+x^4)\text{ d}x \approx 217 \approx 538 68+217 < 538 16^2 = 256 < 538 < 625 = 25^2","['calculus', 'integration', 'definite-integrals', 'numerical-calculus', 'gre-exam']"
50,Differentiating The Law of Cosines,Differentiating The Law of Cosines,,"Alright, I've got a stupid question (maybe). I took the equation given by the law of cosines and I differentiated it with respect to some other parameter. Right, so we begin be considering a triangle with vertices $A,B,$ and $C$ . In $\triangle ABC,$ we have $|AB|=a,|BC|=b,$ and $|CA|=c.$ The angle opposite to $a$ is $\alpha,$ the angle opposite to $b$ is $\beta,$ and the angle opposite to $c$ is $\gamma.$ I begin by considering the cosine rule. We have that $a^2+b^2=c^2+2ab\cos{(\gamma)}.$ Now, this is the weird-ish part. I differentiated with respect to some other parameter, say $t$ . So, we get $2[a\frac{da}{dt}+b\frac{db}{dt}]=2[c\frac{dc}{dt}-ab\sin{(\gamma)}\frac{d\gamma}{dt}+a\cos{(\gamma)}\frac{db}{dt}+b\cos{(\gamma)}\frac{da}{dt}].$ Cumbersome, I know. But, consider what happens when the sides $a,b,$ and $c$ don't change as $t$ changes. Most of the derivatives vanish, and we get $-ab\sin{(\gamma)}\frac{d\gamma}{dt}=0.$ Now, for a triangle, $a,b,$ and $\sin{(\gamma)}$ cannot be $0.$ This means that the angle $\gamma$ remains constant as $t$ varies. Similar arguments can be used for $\alpha,$ and $\beta.$ I know that I simply could've used $SSS$ congruency or something like that to prove that if all $3$ sides of a triangle remain constant, then so do the angles, but is this line of reasoning correct? Further, if we do something similar with the law of sines, we don't get a neat argument that shows that the angles remain constant as the sides do. Is there any way to rectify that? In particular, we get $\tan{(\beta)}\frac{d\alpha}{dt}=\tan{(\alpha)}\frac{d\beta}{dt},$ when the sine rule is applied to $a,b,\alpha,$ and $\beta,$ and differentiated. Nothing about this tells me that either of the derivatives are $0$ .","Alright, I've got a stupid question (maybe). I took the equation given by the law of cosines and I differentiated it with respect to some other parameter. Right, so we begin be considering a triangle with vertices and . In we have and The angle opposite to is the angle opposite to is and the angle opposite to is I begin by considering the cosine rule. We have that Now, this is the weird-ish part. I differentiated with respect to some other parameter, say . So, we get Cumbersome, I know. But, consider what happens when the sides and don't change as changes. Most of the derivatives vanish, and we get Now, for a triangle, and cannot be This means that the angle remains constant as varies. Similar arguments can be used for and I know that I simply could've used congruency or something like that to prove that if all sides of a triangle remain constant, then so do the angles, but is this line of reasoning correct? Further, if we do something similar with the law of sines, we don't get a neat argument that shows that the angles remain constant as the sides do. Is there any way to rectify that? In particular, we get when the sine rule is applied to and and differentiated. Nothing about this tells me that either of the derivatives are .","A,B, C \triangle ABC, |AB|=a,|BC|=b, |CA|=c. a \alpha, b \beta, c \gamma. a^2+b^2=c^2+2ab\cos{(\gamma)}. t 2[a\frac{da}{dt}+b\frac{db}{dt}]=2[c\frac{dc}{dt}-ab\sin{(\gamma)}\frac{d\gamma}{dt}+a\cos{(\gamma)}\frac{db}{dt}+b\cos{(\gamma)}\frac{da}{dt}]. a,b, c t -ab\sin{(\gamma)}\frac{d\gamma}{dt}=0. a,b, \sin{(\gamma)} 0. \gamma t \alpha, \beta. SSS 3 \tan{(\beta)}\frac{d\alpha}{dt}=\tan{(\alpha)}\frac{d\beta}{dt}, a,b,\alpha, \beta, 0","['calculus', 'geometry']"
51,Integral (Tanh and Normal),Integral (Tanh and Normal),,"I am trying to evaluate the following: The expectation of the hyperbolic tangent of an arbitrary normal random variable. $\mathbb{E}[\mathrm{tanh}(\phi)]; \phi \sim N(\mu, \sigma^2)$ Equivalently: $\int_{-\infty}^{\infty} \mathrm{tanh}(\phi)\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(\phi-\mu)^2) d\phi$ I've resorted to Wolfram Alpha, and I can sometimes (!) get it to evaluate the integral for $\sigma^2 = 1$. It gives: $\exp(\mu) - 1$ for negative $\mu$ and $1-\exp(-\mu)$ for positive mu. I have no idea how it got this, but it seems plausible as I've done some simulations (i.e. lots of draws from the normal distribution and then the mean of the tanh of the draws) and the formula it gives for the $\sigma^2 = 1$ case seems pretty close. I cannot get it to evalute anything with a different variance. Mathematica also seems unwilling to compute the integral or the Fourier transform (which I thought might be a way forward) but this is tricker than the ones I know how to deal with. Thanks!","I am trying to evaluate the following: The expectation of the hyperbolic tangent of an arbitrary normal random variable. $\mathbb{E}[\mathrm{tanh}(\phi)]; \phi \sim N(\mu, \sigma^2)$ Equivalently: $\int_{-\infty}^{\infty} \mathrm{tanh}(\phi)\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(\phi-\mu)^2) d\phi$ I've resorted to Wolfram Alpha, and I can sometimes (!) get it to evaluate the integral for $\sigma^2 = 1$. It gives: $\exp(\mu) - 1$ for negative $\mu$ and $1-\exp(-\mu)$ for positive mu. I have no idea how it got this, but it seems plausible as I've done some simulations (i.e. lots of draws from the normal distribution and then the mean of the tanh of the draws) and the formula it gives for the $\sigma^2 = 1$ case seems pretty close. I cannot get it to evalute anything with a different variance. Mathematica also seems unwilling to compute the integral or the Fourier transform (which I thought might be a way forward) but this is tricker than the ones I know how to deal with. Thanks!",,"['calculus', 'normal-distribution', 'fourier-transform', 'hyperbolic-functions']"
52,"Closed-forms of the integrals $\int_0^1 K(\sqrt{k})^2 \, dk$, $\int_0^1 E(\sqrt{k})^2 \, dk$ and $\int_0^1 K(\sqrt{k}) E(\sqrt{k}) \, dk$","Closed-forms of the integrals ,  and","\int_0^1 K(\sqrt{k})^2 \, dk \int_0^1 E(\sqrt{k})^2 \, dk \int_0^1 K(\sqrt{k}) E(\sqrt{k}) \, dk","Let denote $K$ and $E$ the complete elliptic integral of the first and second kind . The integrand $K(\sqrt{k})$ and $E(\sqrt{k})$ has a closed-form antiderivative in term of $K(\sqrt{k})$ and $E(\sqrt{k})$, so we know that $$ \int_0^1 K\left(\sqrt{k}\right) \, dk = 2, $$ and $$ \int_0^1 E\left(\sqrt{k}\right) \, dk = \frac{4}{3}. $$ I couldn't find closed-form antiderivatives to the integrals $\int K(\sqrt{k})^2 \, dk$, $\int E(\sqrt{k})^2 \, dk$, $\int E(\sqrt{k})K(\sqrt{k}) \, dk$, but I've conjectured, that $$\begin{align} \int_0^1 K\left(\sqrt{k}\right)^2 \, dk &\stackrel{?}{=} \frac{7}{2}\zeta(3),\\ \int_0^1 E\left(\sqrt{k}\right)^2 \, dk &\stackrel{?}{=} \frac{7}{8}\zeta(3)+\frac{3}{4},\\ \int_0^1 K\left(\sqrt{k}\right)E\left(\sqrt{k}\right) \, dk &\stackrel{?}{=} \frac{7}{4}\zeta(3)+\frac{1}{2}. \end{align}$$ How could we prove this closed-forms? It would be nice to see some references to these integrals.","Let denote $K$ and $E$ the complete elliptic integral of the first and second kind . The integrand $K(\sqrt{k})$ and $E(\sqrt{k})$ has a closed-form antiderivative in term of $K(\sqrt{k})$ and $E(\sqrt{k})$, so we know that $$ \int_0^1 K\left(\sqrt{k}\right) \, dk = 2, $$ and $$ \int_0^1 E\left(\sqrt{k}\right) \, dk = \frac{4}{3}. $$ I couldn't find closed-form antiderivatives to the integrals $\int K(\sqrt{k})^2 \, dk$, $\int E(\sqrt{k})^2 \, dk$, $\int E(\sqrt{k})K(\sqrt{k}) \, dk$, but I've conjectured, that $$\begin{align} \int_0^1 K\left(\sqrt{k}\right)^2 \, dk &\stackrel{?}{=} \frac{7}{2}\zeta(3),\\ \int_0^1 E\left(\sqrt{k}\right)^2 \, dk &\stackrel{?}{=} \frac{7}{8}\zeta(3)+\frac{3}{4},\\ \int_0^1 K\left(\sqrt{k}\right)E\left(\sqrt{k}\right) \, dk &\stackrel{?}{=} \frac{7}{4}\zeta(3)+\frac{1}{2}. \end{align}$$ How could we prove this closed-forms? It would be nice to see some references to these integrals.",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'elliptic-integrals']"
53,The limit of a product of functions equals the product of the limits: Is this proof rigorous?,The limit of a product of functions equals the product of the limits: Is this proof rigorous?,,"All the proofs I've seen so for the limit of a product of functions equaling the product of the limits are based on the following : Let $f$ and $g$ be real or complex functions having the limits $$\lim_{x\to x_0}f(x) = F \quad \mbox{and} \quad \lim_{x\to x_0}g(x) = G.$$ Then also the limit $\displaystyle\lim_{x\to x_0}f(x)g(x)$ exists and equals $FG$ . Let $\varepsilon$ be any positive number. The assumptions imply the existence of the positive numbers $\delta_1,\,\delta_2,\,\delta_3$ such that \begin{align} |f(x)-F| < \frac{\varepsilon}{2(1+|G|)}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_1\tag{1} \end{align} \begin{align} |g(x)-G| < \frac{\varepsilon}{2(1+|F|)}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_2\tag{2} \end{align} \begin{align} |g(x)-G| < 1\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_3\tag{3} \end{align} According to the condition (3) we see that $$|g(x)| = |g(x)\!-\!G\!+\!G| \leqq |g(x)\!-\!G|+|G| < 1\!+\!|G|\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_3.$$ Supposing then that, $0 < |x-x_0| < \min\{\delta_1,\,\delta_2,\,\delta_3\}$ , and using (1) and (2) we obtain \begin{align*} |f(x)g(x)-FG|\;& = |f(x)g(x)-Fg(x)+Fg(x)-FG|\\                & \leqq |f(x)g(x)\!-\!Fg(x)|+|Fg(x)\!-\!FG|\\                & = |g(x)|\cdot|f(x)\!-\!F|+|F|\cdot|g(x)\!-\!G|\\                & < (1\!+\!|G|)\frac{\varepsilon}{2(1\!+\!|G|)}+(1\!+\!|F|)\frac{\varepsilon}{2(1\!+\!|F|)}\\                 & = \varepsilon \end{align*} This settles the proof. But after having a go myself, I came up with the following: Let $\varepsilon$ be any positive number. The assumptions imply the existence of the positive numbers $\delta_1,\,\delta_2$ such that \begin{align} |f(x)-F| < {\varepsilon}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_1\tag{1} \end{align} \begin{align} |g(x)-G| < {\varepsilon};\;\mbox{when}\;\;0 < |x-x_0| < \delta_2\tag{2} \end{align} Supposing then that, $0 < |x-x_0| < \min\{\delta_1,\,\delta_2\}$ , and using (1) and (2) we obtain \begin{align*} |f(x)g(x)-FG|\;& = |(f(x) - F)(g(x) - G) + G(f(x) - F) + F(g(x) - G)|\\                & \leq |f(x) - F||g(x) - G| + |G||f(x) - F| + |F||g(x) - G|\\                & < \varepsilon^2 + \varepsilon(|F| + |G|)\\                & = \varepsilon' \end{align*} where $\varepsilon'$ is any postive number, giving $\varepsilon = -1/2(|F|+|G|) +1/2\sqrt{(|F| + |G|)^2 + 4\varepsilon'}$ This settles the proof. Is this(my proof) rigorous enough?","All the proofs I've seen so for the limit of a product of functions equaling the product of the limits are based on the following : Let and be real or complex functions having the limits Then also the limit exists and equals . Let be any positive number. The assumptions imply the existence of the positive numbers such that According to the condition (3) we see that Supposing then that, , and using (1) and (2) we obtain This settles the proof. But after having a go myself, I came up with the following: Let be any positive number. The assumptions imply the existence of the positive numbers such that Supposing then that, , and using (1) and (2) we obtain where is any postive number, giving This settles the proof. Is this(my proof) rigorous enough?","f g \lim_{x\to x_0}f(x) = F \quad \mbox{and} \quad \lim_{x\to x_0}g(x) = G. \displaystyle\lim_{x\to x_0}f(x)g(x) FG \varepsilon \delta_1,\,\delta_2,\,\delta_3 \begin{align}
|f(x)-F| < \frac{\varepsilon}{2(1+|G|)}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_1\tag{1}
\end{align} \begin{align}
|g(x)-G| < \frac{\varepsilon}{2(1+|F|)}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_2\tag{2}
\end{align} \begin{align}
|g(x)-G| < 1\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_3\tag{3}
\end{align} |g(x)| = |g(x)\!-\!G\!+\!G| \leqq |g(x)\!-\!G|+|G| < 1\!+\!|G|\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_3. 0 < |x-x_0| < \min\{\delta_1,\,\delta_2,\,\delta_3\} \begin{align*}
|f(x)g(x)-FG|\;& = |f(x)g(x)-Fg(x)+Fg(x)-FG|\\
               & \leqq |f(x)g(x)\!-\!Fg(x)|+|Fg(x)\!-\!FG|\\
               & = |g(x)|\cdot|f(x)\!-\!F|+|F|\cdot|g(x)\!-\!G|\\
               & < (1\!+\!|G|)\frac{\varepsilon}{2(1\!+\!|G|)}+(1\!+\!|F|)\frac{\varepsilon}{2(1\!+\!|F|)}\\ 
               & = \varepsilon
\end{align*} \varepsilon \delta_1,\,\delta_2 \begin{align}
|f(x)-F| < {\varepsilon}\;\;\mbox{when}\;\;0 < |x-x_0| < \delta_1\tag{1}
\end{align} \begin{align}
|g(x)-G| < {\varepsilon};\;\mbox{when}\;\;0 < |x-x_0| < \delta_2\tag{2}
\end{align} 0 < |x-x_0| < \min\{\delta_1,\,\delta_2\} \begin{align*}
|f(x)g(x)-FG|\;& = |(f(x) - F)(g(x) - G) + G(f(x) - F) + F(g(x) - G)|\\
               & \leq |f(x) - F||g(x) - G| + |G||f(x) - F| + |F||g(x) - G|\\
               & < \varepsilon^2 + \varepsilon(|F| + |G|)\\
               & = \varepsilon'
\end{align*} \varepsilon' \varepsilon = -1/2(|F|+|G|) +1/2\sqrt{(|F| + |G|)^2 + 4\varepsilon'}","['calculus', 'limits']"
54,"Limit of the sequence $\{n^n/n!\}$, is this sequence bounded, convergent and eventually monotonic?","Limit of the sequence , is this sequence bounded, convergent and eventually monotonic?",\{n^n/n!\},"I am trying to check whether or not the sequence $$a_{n} =\left\{\frac{n^n}{n!}\right\}_{n=1}^{\infty}$$ is bounded, convergent and ultimately monotonic (there exists an $N$ such that for all $n\geq N$ the sequence is monotonically increasing or decreasing). However, I'm having a lot of trouble finding a solution that sufficiently satisfies me. My best argument so far is as follows, $$a_{n} = \frac{n\cdot n\cdot n\cdot \ldots\cdot n}{n(n-1)(n-2)(n-3)\dots(2)(1)} = \frac{n}{n}\cdot \frac{n}{n-1}\cdot  \ldots \cdot \frac{n}2\cdot n$$ so $\lim a_{n}\rightarrow \infty$ since $n<a_{n}$ for all $n>1$. Since the sequence is divergent, it follows that the function must be ultimately monotonic. This feels a little dubious to me, I feel like I can form a much better argument than that, or at the very least a more elegant one. I've tried to assume $\{a_{n}\}$ approaches some limit $L$ so there exists some $N$ such that $|a_{n} - L| < \epsilon$ whenever $n>N$ and derive a contradiction, but this approach got me nowhere. Finally, I've also tried to use the fact that $\frac{a_{n+1}}{a_n}\rightarrow e$ to help me, but I couldn't find an argument where that fact would be useful.","I am trying to check whether or not the sequence $$a_{n} =\left\{\frac{n^n}{n!}\right\}_{n=1}^{\infty}$$ is bounded, convergent and ultimately monotonic (there exists an $N$ such that for all $n\geq N$ the sequence is monotonically increasing or decreasing). However, I'm having a lot of trouble finding a solution that sufficiently satisfies me. My best argument so far is as follows, $$a_{n} = \frac{n\cdot n\cdot n\cdot \ldots\cdot n}{n(n-1)(n-2)(n-3)\dots(2)(1)} = \frac{n}{n}\cdot \frac{n}{n-1}\cdot  \ldots \cdot \frac{n}2\cdot n$$ so $\lim a_{n}\rightarrow \infty$ since $n<a_{n}$ for all $n>1$. Since the sequence is divergent, it follows that the function must be ultimately monotonic. This feels a little dubious to me, I feel like I can form a much better argument than that, or at the very least a more elegant one. I've tried to assume $\{a_{n}\}$ approaches some limit $L$ so there exists some $N$ such that $|a_{n} - L| < \epsilon$ whenever $n>N$ and derive a contradiction, but this approach got me nowhere. Finally, I've also tried to use the fact that $\frac{a_{n+1}}{a_n}\rightarrow e$ to help me, but I couldn't find an argument where that fact would be useful.",,"['calculus', 'limits', 'factorial']"
55,Reconciling measure theory change of variables with u-substitution,Reconciling measure theory change of variables with u-substitution,,"In measure theory we learn that $$ \int_\Omega g \circ f d\mu = \int_{f(\Omega)}g d(\mu \circ f^{-1}) $$ where $(\Omega, \mathscr F, \mu)$ is a measure space and $f$ and $g$ are measurable. Now in calculus we have that $$ \int_{\phi(a)}^{\phi(b)} h(x)dx = \int_a^b h(\phi(t))\phi'(t)dt $$ for a suitable substitution $x = \phi(t)$. From this substitution we have $dx = \phi'(t)dt$ with $\phi'(t)$ being the Jacobian, but I want to understand this in terms of the measure theoretic formulation. Clearly I'll have $g = h$ and $f = \phi$, so this ought to be equivalent to $$ \int_{[a,b]} h \circ \phi d\mu \stackrel ?= \int_{\phi([a,b])} h \ d(\mu \circ \phi^{-1}). $$ Let $\lambda$ be the Lebesgue measure. It seems fair to assume that $\mu \ll \lambda$ so the Radon-Nikodym derivative $\frac{d\mu}{d\lambda}$ exists. This means that  $$ \int_{[a,b]}h \circ \phi d\mu = \int_{[a,b]} h \circ \phi \frac{d\mu}{d\lambda}d\lambda $$ so we have $\phi' = d\mu / d\lambda$? Now how do we reconcile this with the $\mu \circ \phi^{-1}?$","In measure theory we learn that $$ \int_\Omega g \circ f d\mu = \int_{f(\Omega)}g d(\mu \circ f^{-1}) $$ where $(\Omega, \mathscr F, \mu)$ is a measure space and $f$ and $g$ are measurable. Now in calculus we have that $$ \int_{\phi(a)}^{\phi(b)} h(x)dx = \int_a^b h(\phi(t))\phi'(t)dt $$ for a suitable substitution $x = \phi(t)$. From this substitution we have $dx = \phi'(t)dt$ with $\phi'(t)$ being the Jacobian, but I want to understand this in terms of the measure theoretic formulation. Clearly I'll have $g = h$ and $f = \phi$, so this ought to be equivalent to $$ \int_{[a,b]} h \circ \phi d\mu \stackrel ?= \int_{\phi([a,b])} h \ d(\mu \circ \phi^{-1}). $$ Let $\lambda$ be the Lebesgue measure. It seems fair to assume that $\mu \ll \lambda$ so the Radon-Nikodym derivative $\frac{d\mu}{d\lambda}$ exists. This means that  $$ \int_{[a,b]}h \circ \phi d\mu = \int_{[a,b]} h \circ \phi \frac{d\mu}{d\lambda}d\lambda $$ so we have $\phi' = d\mu / d\lambda$? Now how do we reconcile this with the $\mu \circ \phi^{-1}?$",,"['calculus', 'integration', 'measure-theory', 'substitution']"
56,Solutions to this fractional differential equation,Solutions to this fractional differential equation,,"So we all know that $\frac d{dx}e^x=e^x$ and that the $n$th derivative of $e^x$ is still $e^x$, but upon entering fractional calculus, this is ruined.  Let $D^\alpha$ be the $\alpha$th derivative with respect to $x$.  Then, as we can see , when $\alpha\in[0,1)$, $$D^\alpha e^x=-\frac1{\Gamma(1-\alpha)}e^x\gamma(\alpha,x)\ne e^x$$ where we use the lower incomplete gamma function. Which raises the interesting question: What are the solutions to the following fractional differential equation? $$D^\alpha f(x)=f(x)$$ where we have $$D^\alpha f(x)=\frac1{\Gamma(n-\alpha)}\int_0^x\frac{f^{(n)}(t)}{(x-t)^{\alpha+1-n}}\ dt$$ with $n=\lfloor\alpha+1\rfloor$. $f$ may be a function of $\alpha$.","So we all know that $\frac d{dx}e^x=e^x$ and that the $n$th derivative of $e^x$ is still $e^x$, but upon entering fractional calculus, this is ruined.  Let $D^\alpha$ be the $\alpha$th derivative with respect to $x$.  Then, as we can see , when $\alpha\in[0,1)$, $$D^\alpha e^x=-\frac1{\Gamma(1-\alpha)}e^x\gamma(\alpha,x)\ne e^x$$ where we use the lower incomplete gamma function. Which raises the interesting question: What are the solutions to the following fractional differential equation? $$D^\alpha f(x)=f(x)$$ where we have $$D^\alpha f(x)=\frac1{\Gamma(n-\alpha)}\int_0^x\frac{f^{(n)}(t)}{(x-t)^{\alpha+1-n}}\ dt$$ with $n=\lfloor\alpha+1\rfloor$. $f$ may be a function of $\alpha$.",,"['calculus', 'ordinary-differential-equations', 'definite-integrals', 'fractional-calculus', 'fractional-differential-equations']"
57,evans book pde estimate question,evans book pde estimate question,,"I'm reading Evans' book on PDE and I'm having troubles understanding one estimate. He defines the fundamental solution to Laplace' equation as $$ \Phi(x) = \begin{cases} -\frac{1}{2\pi} \, \log(|x|), \, & n=2, \\ \frac{1}{n \, (n-2) \, \omega_n} \, \frac{1}{|x|^{n-2}}, \, & n\geq 3, \end{cases} $$ where $\omega_n$ is the volume of the $n$-ball. For the solution of Poisson's equation $ -\Delta u = f$ he computes the Laplace acting on the convolution of $f$ and $\Phi$, involving this estimate: $$ \bigg|\int_{B(0,\varepsilon)} \Phi(y) \, \Delta_y f(x-y) \, dy \bigg| \leq C \, \lVert D^2f \rVert_{L^\infty} \int_{B(0,\varepsilon)} |\Phi(y)| \, dy \leq \begin{cases} C \, \varepsilon^2 \, |\log(\varepsilon)|, & n=2, \\ C \, \varepsilon^2, & n\geq 3. \end{cases} $$ How do you obtain the last inequality?","I'm reading Evans' book on PDE and I'm having troubles understanding one estimate. He defines the fundamental solution to Laplace' equation as $$ \Phi(x) = \begin{cases} -\frac{1}{2\pi} \, \log(|x|), \, & n=2, \\ \frac{1}{n \, (n-2) \, \omega_n} \, \frac{1}{|x|^{n-2}}, \, & n\geq 3, \end{cases} $$ where $\omega_n$ is the volume of the $n$-ball. For the solution of Poisson's equation $ -\Delta u = f$ he computes the Laplace acting on the convolution of $f$ and $\Phi$, involving this estimate: $$ \bigg|\int_{B(0,\varepsilon)} \Phi(y) \, \Delta_y f(x-y) \, dy \bigg| \leq C \, \lVert D^2f \rVert_{L^\infty} \int_{B(0,\varepsilon)} |\Phi(y)| \, dy \leq \begin{cases} C \, \varepsilon^2 \, |\log(\varepsilon)|, & n=2, \\ C \, \varepsilon^2, & n\geq 3. \end{cases} $$ How do you obtain the last inequality?",,"['calculus', 'multivariable-calculus', 'partial-differential-equations']"
58,How to show the divergence of $\sum\limits_{n=1}^\infty\frac{\sin(\sqrt{n})}{\sqrt{n}}$,How to show the divergence of,\sum\limits_{n=1}^\infty\frac{\sin(\sqrt{n})}{\sqrt{n}},The 10 standard tests taught in class are: 1) $n^{th}$ term test for divergence.(Not applicable: $\lim =0$). 2) Geometric Series(Not applicable). 3) Telescoping Series(Not applicable) 4) Integral Test(Not applicable: $f<0$ sometimes) 5) $p$-series(Not applicable) 6) Direct Comparison(maybe) 7) Limit Comparison(Not applicable $a_n<0$ sometimes) 8) Alternating Series Test(Not Alternating) 9) Ratio Test fails 10) Root Test fails I did find a hint online that states we should show that for $k^2+1\leq n\leq k^2+k$ we have $\sum\limits_{n=k^2+1}^{k^2+k}\frac{\sin(\sqrt{n})}{\sqrt{n}}>\frac{1}{8}$. Is there an easier way and if not how should we go about showing this?,The 10 standard tests taught in class are: 1) $n^{th}$ term test for divergence.(Not applicable: $\lim =0$). 2) Geometric Series(Not applicable). 3) Telescoping Series(Not applicable) 4) Integral Test(Not applicable: $f<0$ sometimes) 5) $p$-series(Not applicable) 6) Direct Comparison(maybe) 7) Limit Comparison(Not applicable $a_n<0$ sometimes) 8) Alternating Series Test(Not Alternating) 9) Ratio Test fails 10) Root Test fails I did find a hint online that states we should show that for $k^2+1\leq n\leq k^2+k$ we have $\sum\limits_{n=k^2+1}^{k^2+k}\frac{\sin(\sqrt{n})}{\sqrt{n}}>\frac{1}{8}$. Is there an easier way and if not how should we go about showing this?,,"['calculus', 'sequences-and-series']"
59,Prove $f(x)=ax+b$,Prove,f(x)=ax+b,"Let $f(x)$ be a continuous function in $\mathbb R$ that for all $x\in(-\infty,+\infty)$, satisfies $$ \lim_{h\rightarrow+\infty}{[f(x+h)-2f(x)+f(x-h)]}=0. $$ Prove that $f(x)=ax+b$ for some $a,b\in\mathbb R$. This is a problem from my exercise book, but I can't figure out the solution of it, I think the solution in my book is wrong. :( Any idea and proof of it are welcome! Thank you in advance.","Let $f(x)$ be a continuous function in $\mathbb R$ that for all $x\in(-\infty,+\infty)$, satisfies $$ \lim_{h\rightarrow+\infty}{[f(x+h)-2f(x)+f(x-h)]}=0. $$ Prove that $f(x)=ax+b$ for some $a,b\in\mathbb R$. This is a problem from my exercise book, but I can't figure out the solution of it, I think the solution in my book is wrong. :( Any idea and proof of it are welcome! Thank you in advance.",,['calculus']
60,Does the limit of the cubic formula approach the quadratic one as the cubic coefficient goes to $0$?,Does the limit of the cubic formula approach the quadratic one as the cubic coefficient goes to ?,0,"The formula for solving a cubic equation of the form $ax^3+bx^2+cx+d=0$ does not seem to yield the quadratic formula for the limit $\lim _{a \rightarrow 0} \text{(cubic formula)}$ . But, if one tries the same thing with the quadratic formula the limit exists for the right choice of the square root sign. My question is, is there a way to take the limit $\lim _{a \rightarrow 0} \text{(cubic formula)}$ and produce the quadratic formula? or does the limit simply not exist? Finally, if the limit does not exist, is there a technical reason for that? Any input is very much appreciated. Thanks Edit: I was trying to tackle the simple case of $b=0$ . Doing so, the cubic formula reduces to $$\left(-\frac{d}{2a} - \left(\frac{4c^3 + 27ad^2}{108a^3}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}} + \left(-\frac{d}{2a} + \left(\frac{4c^3 + 27ad^2}{108a^3}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}}$$ If one only considers the first term cubed, then it can be written as $$\frac{1}{a}\left(-\frac{d}{2} - \left(4c^3+27ad^2\right)^{\frac{1}{2}}\right)$$ and I don't see how is it possible to find a finite limit as $a\rightarrow 0$ . Am I missing something trivial?","The formula for solving a cubic equation of the form does not seem to yield the quadratic formula for the limit . But, if one tries the same thing with the quadratic formula the limit exists for the right choice of the square root sign. My question is, is there a way to take the limit and produce the quadratic formula? or does the limit simply not exist? Finally, if the limit does not exist, is there a technical reason for that? Any input is very much appreciated. Thanks Edit: I was trying to tackle the simple case of . Doing so, the cubic formula reduces to If one only considers the first term cubed, then it can be written as and I don't see how is it possible to find a finite limit as . Am I missing something trivial?",ax^3+bx^2+cx+d=0 \lim _{a \rightarrow 0} \text{(cubic formula)} \lim _{a \rightarrow 0} \text{(cubic formula)} b=0 \left(-\frac{d}{2a} - \left(\frac{4c^3 + 27ad^2}{108a^3}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}} + \left(-\frac{d}{2a} + \left(\frac{4c^3 + 27ad^2}{108a^3}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}} \frac{1}{a}\left(-\frac{d}{2} - \left(4c^3+27ad^2\right)^{\frac{1}{2}}\right) a\rightarrow 0,"['calculus', 'limits', 'polynomials', 'quadratics', 'cubics']"
61,"Evaluating $\int_0^{2\pi}\frac{dt}{\sqrt[4]{P(\cos t,\sin t)}}$",Evaluating,"\int_0^{2\pi}\frac{dt}{\sqrt[4]{P(\cos t,\sin t)}}","$${\LARGE\int}_0^{2\pi}\frac{dt}{\sqrt[{\LARGE 4}]{A\Big(\sin^8t+\cos^8t\Big)+B\Big(\sin^6t\cos^2t+\sin^2t\cos^6t\Big)+C~\sin^4t\cos^4t}}~=~?$$ where $A=0.3$, $B=-3.3$, and $C=10$. Its numerical value is about $12.0165220075768590.$ The Inverse Symbolic Calculator seems baffled. Maple and Mathematica are both unable to return a closed form. $\bigg($Feel free to choose $A=\dfrac13$ and $B=-\dfrac{10}3$ , if it helps$\bigg)$. Motivation: $\qquad\qquad\qquad\quad$ The above shape is given by the implicit polynomial equation $$A\Big(x^8+y^8\Big)+B\Big(x^6y^2+x^2y^6\Big)+C~x^4y^4=R^8,$$ where $A=0.3$, $B=-3.3$, $C=10$, and $R=2$. Letting $x=r\sin t$ and $y=r\cos t$, we are finally able to express it in polar coordinates, since the Cartesian ones seem somewhat inadequate, given its extremely concave shape, which render it quite resistant to being parsed in terms of single-value functions, despite various sectionings and rotations. Thus the afore-mentioned integral is born. But whether it possesses a closed form, even one in terms of special functions, such as elliptic integrals, is beyond me. I don't really see the tangent half-angle substitution going anywhere. Perhaps some complex integration methods are in order ?","$${\LARGE\int}_0^{2\pi}\frac{dt}{\sqrt[{\LARGE 4}]{A\Big(\sin^8t+\cos^8t\Big)+B\Big(\sin^6t\cos^2t+\sin^2t\cos^6t\Big)+C~\sin^4t\cos^4t}}~=~?$$ where $A=0.3$, $B=-3.3$, and $C=10$. Its numerical value is about $12.0165220075768590.$ The Inverse Symbolic Calculator seems baffled. Maple and Mathematica are both unable to return a closed form. $\bigg($Feel free to choose $A=\dfrac13$ and $B=-\dfrac{10}3$ , if it helps$\bigg)$. Motivation: $\qquad\qquad\qquad\quad$ The above shape is given by the implicit polynomial equation $$A\Big(x^8+y^8\Big)+B\Big(x^6y^2+x^2y^6\Big)+C~x^4y^4=R^8,$$ where $A=0.3$, $B=-3.3$, $C=10$, and $R=2$. Letting $x=r\sin t$ and $y=r\cos t$, we are finally able to express it in polar coordinates, since the Cartesian ones seem somewhat inadequate, given its extremely concave shape, which render it quite resistant to being parsed in terms of single-value functions, despite various sectionings and rotations. Thus the afore-mentioned integral is born. But whether it possesses a closed form, even one in terms of special functions, such as elliptic integrals, is beyond me. I don't really see the tangent half-angle substitution going anywhere. Perhaps some complex integration methods are in order ?",,"['calculus', 'integration', 'definite-integrals', 'closed-form']"
62,Is there a definite integral for which the Riemann sum can be calculated but for which there is no closed-form antiderivative?,Is there a definite integral for which the Riemann sum can be calculated but for which there is no closed-form antiderivative?,,"Some definite integrals, such as $\int_0^\infty e^{-x^2}\,dx$, are known despite the fact that there is no closed-form antiderivative. However, the method I know of calculating this particular integral (square it, and integrate over the first quadrant in polar coordinates) is not dependent on the Riemann sum definition. What I thought might be interesting is a definite integral $\int_a^bf(x)\,dx$ for which the limit of the Riemann sums happens to be calculable, but for which no closed-form antiderivative of $f$ exists. Of course there are some obvious uninteresting examples, like integrating odd functions over symmetric intervals, but one doesn't need Riemann sums to calculate these uninteresting examples. Edit: To make this a bit clearer, it would be nice to have a ""natural"" continuous function $f(x)$ where by some miracle $\lim_{n\to\infty} \sum_{i=1}^nf(x_i)\Delta x$ is computable (for some interval $[a,b]$) using series trickery, but for which no antiderivative exists composed of elementary functions.","Some definite integrals, such as $\int_0^\infty e^{-x^2}\,dx$, are known despite the fact that there is no closed-form antiderivative. However, the method I know of calculating this particular integral (square it, and integrate over the first quadrant in polar coordinates) is not dependent on the Riemann sum definition. What I thought might be interesting is a definite integral $\int_a^bf(x)\,dx$ for which the limit of the Riemann sums happens to be calculable, but for which no closed-form antiderivative of $f$ exists. Of course there are some obvious uninteresting examples, like integrating odd functions over symmetric intervals, but one doesn't need Riemann sums to calculate these uninteresting examples. Edit: To make this a bit clearer, it would be nice to have a ""natural"" continuous function $f(x)$ where by some miracle $\lim_{n\to\infty} \sum_{i=1}^nf(x_i)\Delta x$ is computable (for some interval $[a,b]$) using series trickery, but for which no antiderivative exists composed of elementary functions.",,['calculus']
63,Some basic doubts about partial differentiation - PART I,Some basic doubts about partial differentiation - PART I,,"EDIT : I had earlier asked four doubts together and I thought that maybe I should ask some of them in a separate question. I have some very basics doubts about partial differentials .... Doubt #1 : What is the correct mathematical interpretation of the statement "" $x$ and $y$ are independent variables"" ? Does it mean $\large \frac {\partial x}{\partial y}=\frac{\partial y}{\partial x}=0$ ? According to my understanding of independent variables, if $x$ and $y$ are given to be independent, then any change in $y$ and $y$ only should not cause any change in $x$ . So $ \large \frac{\partial x}{\partial y}$ must be zero. Similar logic for $\large \frac {\partial y}{\partial x} =0$ . Is it okay to use this logic ? Doubt #2 : : Under what circumstances is $\large\frac{\partial x }{\partial y} = \Large \frac{1}{\frac{\partial y}{\partial x}}$ valid? These doubts have become a real hinderance to my understanding of partial derivatives. I need some insightful examples so that I can understand the subtleties that I am currently overlooking. Rest of the doubts can be found here : Some basic doubts about partial differentiation - PART II","EDIT : I had earlier asked four doubts together and I thought that maybe I should ask some of them in a separate question. I have some very basics doubts about partial differentials .... Doubt #1 : What is the correct mathematical interpretation of the statement "" and are independent variables"" ? Does it mean ? According to my understanding of independent variables, if and are given to be independent, then any change in and only should not cause any change in . So must be zero. Similar logic for . Is it okay to use this logic ? Doubt #2 : : Under what circumstances is valid? These doubts have become a real hinderance to my understanding of partial derivatives. I need some insightful examples so that I can understand the subtleties that I am currently overlooking. Rest of the doubts can be found here : Some basic doubts about partial differentiation - PART II",x y \large \frac {\partial x}{\partial y}=\frac{\partial y}{\partial x}=0 x y y y x  \large \frac{\partial x}{\partial y} \large \frac {\partial y}{\partial x} =0 \large\frac{\partial x }{\partial y} = \Large \frac{1}{\frac{\partial y}{\partial x}},"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
64,Stochastic Calculus : Dangers of Incorrectly Calculating Derivatives,Stochastic Calculus : Dangers of Incorrectly Calculating Derivatives,,"I am trying to understand the importance of Ito's Lemma in Stochastic Calculus. When I learn about some mathematical technique for the first time, I always like to ask questions such as : "" Is this complicated approach truly necessary - and what happens if I were to incorrectly persist with a simpler approach? How much trouble can I land myself into by persisting within the incorrect and simpler approach? Are there some situations where this is more of a problem compared to other situations where this might be less of a problem? "" Part 1: For example, consider the following equation: $$f(t, B_t) = X_t = \mu t + \sigma \log(B_t)$$ Where: $X_t$ is the stochastic process $\mu$ is the drift term $\sigma$ is the volatility term $B_t$ is a geometric Brownian motion. When it comes to taking the derivative of this equation, there are 3 approaches that come to mind: Approach 1: Basic Differencing We can do this by simulating $X_t$ and evaluates consecutive differences: $$df(t, B_t) = f(t, B_t) - f(t_{-1}, B_{t-1}) $$ or $$dX_t = X_t - X_{t-1} $$ Approach 2: Basic Calculus (Incorrect): If this was a basic calculus derivative, for some generic function $f(x,y)$ , I could use chain rule to determine: $$\frac{df}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}$$ $$df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy$$ Thus, applying the logic of basic calculus incorrectly to stochastic calculus, I would incorrectly determine that: $$f(t, B_t) = \mu t + \sigma \log(B_t)$$ $$df(t, B_t) = \frac{\partial}{\partial t} f(t,B_t) dt + \frac{\partial}{\partial B_t} f(t, B_t) dB_t$$ $$df(t, B_t) = \mu dt + \sigma \left(\frac{1}{B_t}\right) dB_t$$ Approach 3: Ito's Calculus (Correct): Just to recap, in a Taylor Expansion of a non-stochastic function, we can show that a first order Taylor Expansion of a function equals to its first derivative (in limit), since all other higher order terms are negligible: $$f(x + \Delta x) = f(x) + (\Delta x) f'(x) + \frac{(\Delta x)^2}{2} f''(x) + ... $$ $$(\Delta x) f'(x) = f(x + \Delta x) - f(x) - 0.5(\Delta x)^2 f''(x) + ... $$ $$f'(x) = \frac{f(x + \Delta x) - f(x)}{\Delta x} - 0.5 \Delta x f''(x) + ...$$ $$\lim_{\Delta x \to 0} f'(x) = \lim_{\Delta x \to 0} \left( \frac{f(x + \Delta x) - f(x)}{\Delta x} \right) - \lim_{\Delta x \to 0} \left(0.5 \Delta x f''(x)\right) + \lim_{\Delta x \to 0} (....) $$ $$f'(x) = f'(x) - 0$$ $$f'(x) = f'(x) $$ However, this is not true in the Taylor Expansion of a stochastic function: $$df = \left(\frac{dB_t}{dt} f'(B_t)\right) dt$$ Since the Brownian Motion is not smooth and not differentiable in any interval (i.e. if you zoom into a very small part, there is still ""more Brownian Motion"" happening). Thus, $dB_t$ is defined, but $\frac{dB_t}{dt}$ is not defined. In the non-stochastic case, we could have written the Taylor Series this way with all higher order terms dropping off: $$\Delta f = f(x + \Delta x)  - f(x) =  (\Delta x) f'(x) + \frac{(\Delta x)^2}{2} f''(x) + ... $$ But, for some stochastic function of a Brownian Motion $f(B_t)$ , if we were to write the same Taylor Series: $$\Delta f = f(B_t + \Delta B_t) - f(B_t) = (\Delta B_t) f'(B_t) + \frac{(\Delta B_t)^2}{2} f''(B_t) + ...$$ We know that $\Delta B_t$ (i.e. two differences in a Brownian Motion) is equal to a Weiner Process, i.e. $$B_{t+s} - B_s = W_t \sim N(0,  t)$$ $$\Delta B_t = B_{t+\Delta t} - B_t = W_t \sim N(0, \Delta t)$$ $$\text{Var}(\Delta B_t^2) = E(\Delta B_t^2) - E(\Delta B_t)^2 = \Delta t - 0 = \Delta t $$ $$E(\Delta B_t^2) = \Delta t $$ Going back to the Taylor Series of the stochastic function, we can now make this replacement for $\Delta t$ using the Expected Value: $$\Delta f = f(B_t + \Delta B_t) - f(B_t) = (\Delta B_t) f'(B_t) + \frac{(\Delta t)^2}{2} f''(B_t) + ...$$ Now (for some reason that I don't understand), the second order term is not negligible anymore. Using this information, we can now formally write Ito's Lemma as : $$df(t, B_t) = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial B_t} dB_t + \frac{1}{2} \frac{\partial^2 f}{\partial B_t^2} (dB_t)^2$$ Now, going back to our original problem, for the function $f(t, B_t) = X_t = \mu t + \sigma \log(B_t)$ , we can write: $\frac{\partial f}{\partial t} = \mu$ $\frac{\partial f}{\partial B_t} = \frac{\sigma}{B_t}$ $\frac{\partial^2 f}{\partial B_t^2} = -\frac{\sigma}{B_t^2}$ Thus, the final answer is: $$df(t, B_t) = \mu dt + \frac{\sigma}{B_t} dB_t - \frac{1}{2} \frac{\sigma}{B_t ^2} dt$$ Note that $ Var(B_t) = E(B_t^2) - E(B_t)^2 = t - (0)^2 = t$ Part 2: Looking at Part 1 (provided I have done everything correctly), it is not immediately clear to me what are the ""dangers"" of incorrectly calculating the derivative (i.e. how much more ""wrong"" would the incorrect approach be compared to the correct approach). To get a better understanding of this, I tried to make a computer simulation to look at this (using the R programming language): library(ggplot2)  set.seed(123) # parameters n <- 1000 dt <- 0.01 mu <- 0.05 sigma <- 0.2 t <- seq(0, (n-1)*dt, dt) Bt <- exp(cumsum(rnorm(n, 0, sqrt(dt)))) Xt <- mu * t + sigma * log(Bt)  # Original stochastic process: Xt = mu*t + sigma * log(Bt) p1 <- ggplot(data.frame(Time = t, Xt = Xt), aes(Time, Xt)) +     geom_line() +     labs(title = paste(""Original Equation: Xt = mu*t + sigma * log(Bt)\nmu ="", mu, "", sigma ="", sigma)) +     theme_bw()  # Approach 1: Basic Differencing # dXt_1 = Xt[i+1] - Xt[i] dXt_1 <- c(0, diff(Xt)) p2 <- ggplot(data.frame(Time = t, dXt_1 = dXt_1), aes(Time, dXt_1)) +     geom_line() +     labs(title = paste(""Approach 1: Basic Differencing\nXt[i+1] - Xt[i]\nmu ="", mu, "", sigma ="", sigma)) +     theme_bw()  # Approach 2: Incorrect Derivative using basic calculus # dXt_2 = mu*dt + sigma*(1/Bt)*dBt dBt <- c(0, diff(Bt)) dXt_2 <- mu * dt + sigma * (1 / Bt) * dBt p3 <- ggplot(data.frame(Time = t, dXt_2 = dXt_2), aes(Time, dXt_2)) +     geom_line() +     labs(title = paste(""Approach 2: Incorrect Derivative\nmu*dt + sigma*(1/Bt)*dBt\nmu ="", mu, "", sigma ="", sigma)) +     theme_bw()  # Approach 3: Correct Derivative using Ito's Lemma # dXt_3 = mu*dt + (sigma/Bt)*dBt - 0.5*(sigma/(Bt^2))*dt dXt_3 <- mu * dt + (sigma / Bt) * dBt - 0.5 * (sigma / (t)) * dt p4 <- ggplot(data.frame(Time = t, dXt_3 = dXt_3), aes(Time, dXt_3)) +     geom_line() +     labs(title = paste(""Approach 3: Correct Derivative using Ito's Lemma\nmu*dt + (sigma/Bt)*dBt - 0.5*(sigma/(t))*dt\nmu ="", mu, "", sigma ="", sigma)) +     theme_bw() In the above graphs, the results of all 3 approaches look quite similar to one another. I then compared the absolute differences between Approach 1 and Approach 3, and between Approach 2 and Approach 3: abs_diff_1_3 <- abs(dXt_1 - dXt_3) abs_diff_2_3 <- abs(dXt_2 - dXt_3)  p5 <- ggplot(data.frame(Time = t, AbsDiff = abs_diff_1_3), aes(Time, AbsDiff)) +     geom_line() +     labs(title = ""Absolute Difference between Approach 1 and 3"") +     theme_bw()  p6 <- ggplot(data.frame(Time = t, AbsDiff = abs_diff_2_3), aes(Time, AbsDiff)) +     geom_line() +     labs(title = ""Absolute Difference between Approach 2 and 3"") +     theme_bw() Again, all results look quite similar to one another. My Question: Based on this exercise, it seems like whether you take the correct derivative (via Ito's Lemma) or the incorrect derivative (basic calculus), the final answer looks very similar. Thus, is Ito's Lemma more of a theoretical consideration with little added value compared to the incorrect derivative? Or perhaps there are much bigger differences for the derivatives of other functions (and perhaps for stochastic integrals)? Thus, for stochastic functions, is there any ""real danger"" in incorrectly calculating their derivatives and integrals using basic calculus methods? Thanks! Note: The one thing that comes to mind is perhaps Ito Calculus is really needed when you need to take the derivative or integral of a stochastic function in an intermediate step for some math problem (e.g. first passage time). In such cases, perhaps the ""danger"" of propagating an incorrect result is much higher compared to these basic simulations. References: https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/ef2c66c8079ba656210ad1fd4a5e2fa8_MIT18_S096F13_lecnote18.pdf Distribution of Brownian Motion squared https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_%28Siegrist%29/18%3A_Brownian_Motion/18.01%3A_Standard_Brownian_Motion","I am trying to understand the importance of Ito's Lemma in Stochastic Calculus. When I learn about some mathematical technique for the first time, I always like to ask questions such as : "" Is this complicated approach truly necessary - and what happens if I were to incorrectly persist with a simpler approach? How much trouble can I land myself into by persisting within the incorrect and simpler approach? Are there some situations where this is more of a problem compared to other situations where this might be less of a problem? "" Part 1: For example, consider the following equation: Where: is the stochastic process is the drift term is the volatility term is a geometric Brownian motion. When it comes to taking the derivative of this equation, there are 3 approaches that come to mind: Approach 1: Basic Differencing We can do this by simulating and evaluates consecutive differences: or Approach 2: Basic Calculus (Incorrect): If this was a basic calculus derivative, for some generic function , I could use chain rule to determine: Thus, applying the logic of basic calculus incorrectly to stochastic calculus, I would incorrectly determine that: Approach 3: Ito's Calculus (Correct): Just to recap, in a Taylor Expansion of a non-stochastic function, we can show that a first order Taylor Expansion of a function equals to its first derivative (in limit), since all other higher order terms are negligible: However, this is not true in the Taylor Expansion of a stochastic function: Since the Brownian Motion is not smooth and not differentiable in any interval (i.e. if you zoom into a very small part, there is still ""more Brownian Motion"" happening). Thus, is defined, but is not defined. In the non-stochastic case, we could have written the Taylor Series this way with all higher order terms dropping off: But, for some stochastic function of a Brownian Motion , if we were to write the same Taylor Series: We know that (i.e. two differences in a Brownian Motion) is equal to a Weiner Process, i.e. Going back to the Taylor Series of the stochastic function, we can now make this replacement for using the Expected Value: Now (for some reason that I don't understand), the second order term is not negligible anymore. Using this information, we can now formally write Ito's Lemma as : Now, going back to our original problem, for the function , we can write: Thus, the final answer is: Note that Part 2: Looking at Part 1 (provided I have done everything correctly), it is not immediately clear to me what are the ""dangers"" of incorrectly calculating the derivative (i.e. how much more ""wrong"" would the incorrect approach be compared to the correct approach). To get a better understanding of this, I tried to make a computer simulation to look at this (using the R programming language): library(ggplot2)  set.seed(123) # parameters n <- 1000 dt <- 0.01 mu <- 0.05 sigma <- 0.2 t <- seq(0, (n-1)*dt, dt) Bt <- exp(cumsum(rnorm(n, 0, sqrt(dt)))) Xt <- mu * t + sigma * log(Bt)  # Original stochastic process: Xt = mu*t + sigma * log(Bt) p1 <- ggplot(data.frame(Time = t, Xt = Xt), aes(Time, Xt)) +     geom_line() +     labs(title = paste(""Original Equation: Xt = mu*t + sigma * log(Bt)\nmu ="", mu, "", sigma ="", sigma)) +     theme_bw()  # Approach 1: Basic Differencing # dXt_1 = Xt[i+1] - Xt[i] dXt_1 <- c(0, diff(Xt)) p2 <- ggplot(data.frame(Time = t, dXt_1 = dXt_1), aes(Time, dXt_1)) +     geom_line() +     labs(title = paste(""Approach 1: Basic Differencing\nXt[i+1] - Xt[i]\nmu ="", mu, "", sigma ="", sigma)) +     theme_bw()  # Approach 2: Incorrect Derivative using basic calculus # dXt_2 = mu*dt + sigma*(1/Bt)*dBt dBt <- c(0, diff(Bt)) dXt_2 <- mu * dt + sigma * (1 / Bt) * dBt p3 <- ggplot(data.frame(Time = t, dXt_2 = dXt_2), aes(Time, dXt_2)) +     geom_line() +     labs(title = paste(""Approach 2: Incorrect Derivative\nmu*dt + sigma*(1/Bt)*dBt\nmu ="", mu, "", sigma ="", sigma)) +     theme_bw()  # Approach 3: Correct Derivative using Ito's Lemma # dXt_3 = mu*dt + (sigma/Bt)*dBt - 0.5*(sigma/(Bt^2))*dt dXt_3 <- mu * dt + (sigma / Bt) * dBt - 0.5 * (sigma / (t)) * dt p4 <- ggplot(data.frame(Time = t, dXt_3 = dXt_3), aes(Time, dXt_3)) +     geom_line() +     labs(title = paste(""Approach 3: Correct Derivative using Ito's Lemma\nmu*dt + (sigma/Bt)*dBt - 0.5*(sigma/(t))*dt\nmu ="", mu, "", sigma ="", sigma)) +     theme_bw() In the above graphs, the results of all 3 approaches look quite similar to one another. I then compared the absolute differences between Approach 1 and Approach 3, and between Approach 2 and Approach 3: abs_diff_1_3 <- abs(dXt_1 - dXt_3) abs_diff_2_3 <- abs(dXt_2 - dXt_3)  p5 <- ggplot(data.frame(Time = t, AbsDiff = abs_diff_1_3), aes(Time, AbsDiff)) +     geom_line() +     labs(title = ""Absolute Difference between Approach 1 and 3"") +     theme_bw()  p6 <- ggplot(data.frame(Time = t, AbsDiff = abs_diff_2_3), aes(Time, AbsDiff)) +     geom_line() +     labs(title = ""Absolute Difference between Approach 2 and 3"") +     theme_bw() Again, all results look quite similar to one another. My Question: Based on this exercise, it seems like whether you take the correct derivative (via Ito's Lemma) or the incorrect derivative (basic calculus), the final answer looks very similar. Thus, is Ito's Lemma more of a theoretical consideration with little added value compared to the incorrect derivative? Or perhaps there are much bigger differences for the derivatives of other functions (and perhaps for stochastic integrals)? Thus, for stochastic functions, is there any ""real danger"" in incorrectly calculating their derivatives and integrals using basic calculus methods? Thanks! Note: The one thing that comes to mind is perhaps Ito Calculus is really needed when you need to take the derivative or integral of a stochastic function in an intermediate step for some math problem (e.g. first passage time). In such cases, perhaps the ""danger"" of propagating an incorrect result is much higher compared to these basic simulations. References: https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/ef2c66c8079ba656210ad1fd4a5e2fa8_MIT18_S096F13_lecnote18.pdf Distribution of Brownian Motion squared https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_%28Siegrist%29/18%3A_Brownian_Motion/18.01%3A_Standard_Brownian_Motion","f(t, B_t) = X_t = \mu t + \sigma \log(B_t) X_t \mu \sigma B_t X_t df(t, B_t) = f(t, B_t) - f(t_{-1}, B_{t-1})  dX_t = X_t - X_{t-1}  f(x,y) \frac{df}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt} df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy f(t, B_t) = \mu t + \sigma \log(B_t) df(t, B_t) = \frac{\partial}{\partial t} f(t,B_t) dt + \frac{\partial}{\partial B_t} f(t, B_t) dB_t df(t, B_t) = \mu dt + \sigma \left(\frac{1}{B_t}\right) dB_t f(x + \Delta x) = f(x) + (\Delta x) f'(x) + \frac{(\Delta x)^2}{2} f''(x) + ...  (\Delta x) f'(x) = f(x + \Delta x) - f(x) - 0.5(\Delta x)^2 f''(x) + ...  f'(x) = \frac{f(x + \Delta x) - f(x)}{\Delta x} - 0.5 \Delta x f''(x) + ... \lim_{\Delta x \to 0} f'(x) = \lim_{\Delta x \to 0} \left( \frac{f(x + \Delta x) - f(x)}{\Delta x} \right) - \lim_{\Delta x \to 0} \left(0.5 \Delta x f''(x)\right) + \lim_{\Delta x \to 0} (....)  f'(x) = f'(x) - 0 f'(x) = f'(x)  df = \left(\frac{dB_t}{dt} f'(B_t)\right) dt dB_t \frac{dB_t}{dt} \Delta f = f(x + \Delta x)  - f(x) =  (\Delta x) f'(x) + \frac{(\Delta x)^2}{2} f''(x) + ...  f(B_t) \Delta f = f(B_t + \Delta B_t) - f(B_t) = (\Delta B_t) f'(B_t) + \frac{(\Delta B_t)^2}{2} f''(B_t) + ... \Delta B_t B_{t+s} - B_s = W_t \sim N(0,  t) \Delta B_t = B_{t+\Delta t} - B_t = W_t \sim N(0, \Delta t) \text{Var}(\Delta B_t^2) = E(\Delta B_t^2) - E(\Delta B_t)^2 = \Delta t - 0 = \Delta t  E(\Delta B_t^2) = \Delta t  \Delta t \Delta f = f(B_t + \Delta B_t) - f(B_t) = (\Delta B_t) f'(B_t) + \frac{(\Delta t)^2}{2} f''(B_t) + ... df(t, B_t) = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial B_t} dB_t + \frac{1}{2} \frac{\partial^2 f}{\partial B_t^2} (dB_t)^2 f(t, B_t) = X_t = \mu t + \sigma \log(B_t) \frac{\partial f}{\partial t} = \mu \frac{\partial f}{\partial B_t} = \frac{\sigma}{B_t} \frac{\partial^2 f}{\partial B_t^2} = -\frac{\sigma}{B_t^2} df(t, B_t) = \mu dt + \frac{\sigma}{B_t} dB_t - \frac{1}{2} \frac{\sigma}{B_t ^2} dt  Var(B_t) = E(B_t^2) - E(B_t)^2 = t - (0)^2 = t","['calculus', 'stochastic-calculus', 'brownian-motion', 'motivation']"
65,A tricky integral - $\int_0^1 \sqrt{\frac{1}{(1-t^2)^2}-\frac{(n+1)^2t^{2n}}{(1-t^{2n+2})^2}}dt $,A tricky integral -,\int_0^1 \sqrt{\frac{1}{(1-t^2)^2}-\frac{(n+1)^2t^{2n}}{(1-t^{2n+2})^2}}dt ,"$$ \mathbf{\mbox{Evaluate:}}\qquad \int_{0}^{1} \sqrt{\frac{1}{\left(1 - t^{2}\right)^2} - \frac{\left(n + 1\right)^{2}\,t^{2n}}{\left(\, 1 - t^{2n+2}\,\,\right)^{2}}} \,\,\mathrm{d}t $$   where $n$ is any positive integer. Introduction : This integral came up while studying the distribution of the roots of random polynomials - and I can't crack it. It seems impervious to methods of integration I know. Neither Mathematica nor Wolfram-Alpha could find a closed form, not only for this general integral, but any special case of $n>1$. My attempt: For $n=1$, the integral is pretty trivial to compute - expanding the integrand gives:   $$\int_0^1 \sqrt{\frac{1}{t^4-2 t^2+1}-\frac{4 t^2}{t^8-2 t^4+1}}$$   Which simplifies quite easily to:   $$\int_0^1 \frac{1}{t^2+1}$$   The antiderivative of the integrand is $\tan^{-1}{t}$. Evaluating at the limits gives:   $$\int_0^1 \sqrt{\frac{1}{t^4-2 t^2+1}-\frac{4 t^2}{t^8-2 t^4+1}}=\frac{\pi}{4}-0=\frac{\pi}{4}$$   However, this method does not work for $n>1$, and niether does any method I know of. Numerical values: Listed below are the approximate numerical values for this integral. Neither Wolfram Alpha nor the Inverse Symbolic calculator were able to find closed forms for these numbers. $$n=2 \qquad 1.01868$$   $$n=3 \qquad 1.17241$$   $$n=4 \qquad 1.28844$$   $$n=5 \qquad 1.38198$$   $$n=6 \qquad 1.46049$$ Any help on this integral would be greatly appreciated. Thank you!","$$ \mathbf{\mbox{Evaluate:}}\qquad \int_{0}^{1} \sqrt{\frac{1}{\left(1 - t^{2}\right)^2} - \frac{\left(n + 1\right)^{2}\,t^{2n}}{\left(\, 1 - t^{2n+2}\,\,\right)^{2}}} \,\,\mathrm{d}t $$   where $n$ is any positive integer. Introduction : This integral came up while studying the distribution of the roots of random polynomials - and I can't crack it. It seems impervious to methods of integration I know. Neither Mathematica nor Wolfram-Alpha could find a closed form, not only for this general integral, but any special case of $n>1$. My attempt: For $n=1$, the integral is pretty trivial to compute - expanding the integrand gives:   $$\int_0^1 \sqrt{\frac{1}{t^4-2 t^2+1}-\frac{4 t^2}{t^8-2 t^4+1}}$$   Which simplifies quite easily to:   $$\int_0^1 \frac{1}{t^2+1}$$   The antiderivative of the integrand is $\tan^{-1}{t}$. Evaluating at the limits gives:   $$\int_0^1 \sqrt{\frac{1}{t^4-2 t^2+1}-\frac{4 t^2}{t^8-2 t^4+1}}=\frac{\pi}{4}-0=\frac{\pi}{4}$$   However, this method does not work for $n>1$, and niether does any method I know of. Numerical values: Listed below are the approximate numerical values for this integral. Neither Wolfram Alpha nor the Inverse Symbolic calculator were able to find closed forms for these numbers. $$n=2 \qquad 1.01868$$   $$n=3 \qquad 1.17241$$   $$n=4 \qquad 1.28844$$   $$n=5 \qquad 1.38198$$   $$n=6 \qquad 1.46049$$ Any help on this integral would be greatly appreciated. Thank you!",,"['calculus', 'integration', 'definite-integrals']"
66,"On Reshetnikov's integral $\int_0^1\frac{dx}{\sqrt[3]x\ \sqrt[6]{1-x}\ \sqrt{1-x\,\alpha^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{3}\,|\alpha|}$",On Reshetnikov's integral,"\int_0^1\frac{dx}{\sqrt[3]x\ \sqrt[6]{1-x}\ \sqrt{1-x\,\alpha^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{3}\,|\alpha|}","V. Reshetnikov gave the remarkable integral , $$\int_0^1\frac{dx}{\sqrt[3]x\,\sqrt[6]{1-x}\,\sqrt{1-x\left(\sqrt{6}\sqrt{12+7\sqrt3}-3\sqrt3-6\right)^2}}=\frac\pi9(3+\sqrt2\sqrt[4]{27})\tag1$$ More generally, given some integer/rational $N$, we are to find an algebraic number $\alpha$ that solves, $$\int_0^1\frac{dx}{\sqrt[3]x\ \sqrt[6]{1-x}\ \sqrt{1-x\,\alpha^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{3}\,|\alpha|}\tag2$$ and absolute value $|\alpha|$. ( Compare to the similar integral in this post .) Equivalently, to find $\alpha$ such that, $$\begin{aligned} \frac{1}{N} &=I\left(\alpha^2;\ \tfrac12,\tfrac13\right)\\[1.8mm] &= \frac{B\left(\alpha^2;\ \tfrac12,\tfrac13\right)}{B\left(\tfrac12,\tfrac13\right)}\\ &=B\left(\alpha^2;\ \tfrac12,\tfrac13\right)\frac{\Gamma\left(\frac56\right)}{\sqrt{\pi}\,\Gamma\left(\frac13\right)}\end{aligned} \tag3$$ with beta function $\beta(a,b)$, incomplete beta $\beta(z;a,b)$ and regularized beta $I(z;a,b)$.  Solutions $\alpha$ for $N=2,3,4,5,7$ are known. Let, $$\alpha=\frac{-3^{1/2}+v^{1/2}}{3^{-1/2}+v^{1/2}}\tag4$$ Then, $$ - 3 + 6 v + v^2 = 0, \quad N = 2\\  - 3 + 27 v - 33v^2 + v^3 = 0, \quad N = 3\\ 3^2 - 150 v^2 + 120 v^3 + 5 v^4 = 0, \quad N = 5\\  - 3^3 - 54 v + 1719 v^2 - 3492v^3 - 957 v^4 + 186 v^5 + v^6 = 0, \quad N = 7$$ and ( added later ), $$3^4 - 648 v + 1836 v^2 + 1512 v^3 - 13770 v^4 + 12168 v^5 - 7476 v^6 + 408 v^7 + v^8 = 0,\quad N=4$$ using the largest positive root, respectively. The example was just $N=2$, while $N=4$ leads to, $$I\left(\tfrac{1-\alpha}{2};\tfrac{1}{3},\tfrac{1}{3}\right)=\tfrac{3}{8},\quad\quad I\left(\tfrac{1+\alpha}{2};\tfrac{1}{3},\tfrac{1}{3}\right)=\tfrac{5}{8}$$ I found these using Mathematica 's FindRoot command, and some hints from Reshetnikov's and other's works, but as much as I tried, I couldn't find prime $N=11$. Q: Is it true one can find algebraic number $\alpha$ for all $N$? What is it for $N=11$?","V. Reshetnikov gave the remarkable integral , $$\int_0^1\frac{dx}{\sqrt[3]x\,\sqrt[6]{1-x}\,\sqrt{1-x\left(\sqrt{6}\sqrt{12+7\sqrt3}-3\sqrt3-6\right)^2}}=\frac\pi9(3+\sqrt2\sqrt[4]{27})\tag1$$ More generally, given some integer/rational $N$, we are to find an algebraic number $\alpha$ that solves, $$\int_0^1\frac{dx}{\sqrt[3]x\ \sqrt[6]{1-x}\ \sqrt{1-x\,\alpha^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{3}\,|\alpha|}\tag2$$ and absolute value $|\alpha|$. ( Compare to the similar integral in this post .) Equivalently, to find $\alpha$ such that, $$\begin{aligned} \frac{1}{N} &=I\left(\alpha^2;\ \tfrac12,\tfrac13\right)\\[1.8mm] &= \frac{B\left(\alpha^2;\ \tfrac12,\tfrac13\right)}{B\left(\tfrac12,\tfrac13\right)}\\ &=B\left(\alpha^2;\ \tfrac12,\tfrac13\right)\frac{\Gamma\left(\frac56\right)}{\sqrt{\pi}\,\Gamma\left(\frac13\right)}\end{aligned} \tag3$$ with beta function $\beta(a,b)$, incomplete beta $\beta(z;a,b)$ and regularized beta $I(z;a,b)$.  Solutions $\alpha$ for $N=2,3,4,5,7$ are known. Let, $$\alpha=\frac{-3^{1/2}+v^{1/2}}{3^{-1/2}+v^{1/2}}\tag4$$ Then, $$ - 3 + 6 v + v^2 = 0, \quad N = 2\\  - 3 + 27 v - 33v^2 + v^3 = 0, \quad N = 3\\ 3^2 - 150 v^2 + 120 v^3 + 5 v^4 = 0, \quad N = 5\\  - 3^3 - 54 v + 1719 v^2 - 3492v^3 - 957 v^4 + 186 v^5 + v^6 = 0, \quad N = 7$$ and ( added later ), $$3^4 - 648 v + 1836 v^2 + 1512 v^3 - 13770 v^4 + 12168 v^5 - 7476 v^6 + 408 v^7 + v^8 = 0,\quad N=4$$ using the largest positive root, respectively. The example was just $N=2$, while $N=4$ leads to, $$I\left(\tfrac{1-\alpha}{2};\tfrac{1}{3},\tfrac{1}{3}\right)=\tfrac{3}{8},\quad\quad I\left(\tfrac{1+\alpha}{2};\tfrac{1}{3},\tfrac{1}{3}\right)=\tfrac{5}{8}$$ I found these using Mathematica 's FindRoot command, and some hints from Reshetnikov's and other's works, but as much as I tried, I couldn't find prime $N=11$. Q: Is it true one can find algebraic number $\alpha$ for all $N$? What is it for $N=11$?",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'experimental-mathematics']"
67,To solve $ \frac {dy}{dx}=\frac 1{\sqrt{x^2+y^2}}$,To solve, \frac {dy}{dx}=\frac 1{\sqrt{x^2+y^2}},How do we solve the differential equation $ \dfrac {dy}{dx}=\dfrac 1{\sqrt{x^2+y^2}}$ ?,How do we solve the differential equation $ \dfrac {dy}{dx}=\dfrac 1{\sqrt{x^2+y^2}}$ ?,,['calculus']
68,How to prove that $n^{-2}[x+g(x)+g\circ g(x)+\cdots +g^{\circ n}(x)]$ converges when $n\to\infty$ [duplicate],How to prove that  converges when  [duplicate],n^{-2}[x+g(x)+g\circ g(x)+\cdots +g^{\circ n}(x)] n\to\infty,"This question already has an answer here : Show that $\lim\limits_{n\to\infty}\frac{a_1+a_2+\dots+a_n}{n^2}$ exists and is independent of the choice of $a$ (1 answer) Closed 10 years ago . Let $f:\mathbb R\to\mathbb R$ be a periodic function with period $1$. We assume that $f$ is Lipschitz continuous, and in particular, we assume that there exists an $L\in (0,1)$, such that $$ |f(x)-f(y)| \le L|x-y|, \quad \text{for all $x,y\in\mathbb R$.} $$ Let also $g(x)=x+f(x).$ Show that the limit $$\lim_{n\to +\infty}\frac1{n^2}[x+g(x)+g(g(x))+\cdots +g^{\circ n}(x)]$$ exists and is independent of $x$, where, for every $n\ge1$, $g^{\circ n}$ is $g$ composed with itself $n$ times, thus $g^{\circ 1}=g$ and, for every $n\ge1$, $g^{\circ n+1}=g\circ g^{\circ n}$. My attempt: Since $$f(x+1)=f(x),\forall x\in R$$ then $$g(g(x))=g(x+f(x))=x+f(x)+f(x+f(x))$$ $$g(g(g(x)))=g(x+f(x)+f(x+f(x)))=x+f(x)+f(x+f(x))+f(x+f(x)+f(x+f(x)))$$ $$\cdots\cdots $$ and  I have $$|g(x)-g(y)|=|x-y+f(x)-f(y)|\le |x-y|+|f(x)-f(y)|<(L+1)|x-y|$$ where $L+1>1$. So $g$ is also Lipschitz continuous, and that's all I can do. Thank you for you help.","This question already has an answer here : Show that $\lim\limits_{n\to\infty}\frac{a_1+a_2+\dots+a_n}{n^2}$ exists and is independent of the choice of $a$ (1 answer) Closed 10 years ago . Let $f:\mathbb R\to\mathbb R$ be a periodic function with period $1$. We assume that $f$ is Lipschitz continuous, and in particular, we assume that there exists an $L\in (0,1)$, such that $$ |f(x)-f(y)| \le L|x-y|, \quad \text{for all $x,y\in\mathbb R$.} $$ Let also $g(x)=x+f(x).$ Show that the limit $$\lim_{n\to +\infty}\frac1{n^2}[x+g(x)+g(g(x))+\cdots +g^{\circ n}(x)]$$ exists and is independent of $x$, where, for every $n\ge1$, $g^{\circ n}$ is $g$ composed with itself $n$ times, thus $g^{\circ 1}=g$ and, for every $n\ge1$, $g^{\circ n+1}=g\circ g^{\circ n}$. My attempt: Since $$f(x+1)=f(x),\forall x\in R$$ then $$g(g(x))=g(x+f(x))=x+f(x)+f(x+f(x))$$ $$g(g(g(x)))=g(x+f(x)+f(x+f(x)))=x+f(x)+f(x+f(x))+f(x+f(x)+f(x+f(x)))$$ $$\cdots\cdots $$ and  I have $$|g(x)-g(y)|=|x-y+f(x)-f(y)|\le |x-y|+|f(x)-f(y)|<(L+1)|x-y|$$ where $L+1>1$. So $g$ is also Lipschitz continuous, and that's all I can do. Thank you for you help.",,"['calculus', 'analysis', 'limits']"
69,Are there parts of Integral Calculus that just *have* to be memorized?,Are there parts of Integral Calculus that just *have* to be memorized?,,"Note : In this question I speak more from a calculation/operational point of view,  as opposed to a more theoretical (Analysis) point of view. When studying Differential Calculus, I found that there was very little that I had to memorize . Virtually all calculation aspects, such as finding derivatives etc., and some theorems, could all be derived on the spot through basic methods. As examples, through basic implicit differentiation, one could prove the inverse function theorem, within a few lines. $$\text{Inverse Function Theorem}\ \ \ \ (f^{-1})'(x) = \frac{1}{f'(f^{-1}(x))}$$ Or if I wanted to find $\dfrac{d}{dx}\ \tan^{-1}(x)$, I could use the inverse function theorem and with the help of a trigonometric identity find the derivative quite easily. I didn't have to memorize $\dfrac{d}{dx}\ \tan^{-1}(x) = \dfrac{1}{1+x^2}$. In fact, apart from the derivatives of $\sin(x)$, $\sinh(x)$ and $\cos(x)$, $\cosh(x)$, I didn't memorize any of the other derivatives for trigonometric functions, I would just re-derive them using basic differentiation rules each time. However I noticed that when studying Integral Calculus, there tends to be a lot more that one just has to commit to memory . For example if I wanted to evaluate the following integral $$\int \dfrac{1}{1+x^2}\ dx$$ The only way I could ever evaluate the integral, would be if I knew $\dfrac{d}{dx}\ \tan^{-1}(x) = \dfrac{1}{1+x^2}$, which would require that I had memorized the derivative (something I tried my best not to do when studying differential calculus). When studying Mathematics, for the most part (and within reason of course) I try my best never to memorize what I can re-derive/prove. I've found that this approach helps improve my skills, and pushes me to search for the deepest possible understanding. But it seems that there are some things, that just have to be committed to memory to be able to make any sort of progress, and this troubles me quite a bit, as I'm not sure as to what I should be just memorizing , and what I should really be working to get the best understanding on. Furthermore Integration is a very heuristic process, whereas Differentiation is a more algorithmic process. Generally we try to get integrals into forms we know of already so that we can evaluate them (with the exception of the Risch algorithm), or it would be impossible to evaluate them by any other means. Wouldn't that require one to memorize the various types of possible integrals? First off, am I looking at this wrong? Are there ways one can reprove results, or evaluate integrals, in a manner that doesn't require one to just memorize and recall a list of formula's like a parrot? What aspects of Integral Calculus would you say, just have to be memorized , i.e. what results in Integral Calculus are close to impossible to re-derive or prove on the spot? Where does one draw the line, between what should be looked at long and hard for the deepest possible understanding, and what should just be memorized ? Lastly, correct me if I'm wrong, but as one makes the transition into higher mathematics (analysis and beyond), that there are some things that you just have to commit to memory , to be able to make any sort of progress?","Note : In this question I speak more from a calculation/operational point of view,  as opposed to a more theoretical (Analysis) point of view. When studying Differential Calculus, I found that there was very little that I had to memorize . Virtually all calculation aspects, such as finding derivatives etc., and some theorems, could all be derived on the spot through basic methods. As examples, through basic implicit differentiation, one could prove the inverse function theorem, within a few lines. $$\text{Inverse Function Theorem}\ \ \ \ (f^{-1})'(x) = \frac{1}{f'(f^{-1}(x))}$$ Or if I wanted to find $\dfrac{d}{dx}\ \tan^{-1}(x)$, I could use the inverse function theorem and with the help of a trigonometric identity find the derivative quite easily. I didn't have to memorize $\dfrac{d}{dx}\ \tan^{-1}(x) = \dfrac{1}{1+x^2}$. In fact, apart from the derivatives of $\sin(x)$, $\sinh(x)$ and $\cos(x)$, $\cosh(x)$, I didn't memorize any of the other derivatives for trigonometric functions, I would just re-derive them using basic differentiation rules each time. However I noticed that when studying Integral Calculus, there tends to be a lot more that one just has to commit to memory . For example if I wanted to evaluate the following integral $$\int \dfrac{1}{1+x^2}\ dx$$ The only way I could ever evaluate the integral, would be if I knew $\dfrac{d}{dx}\ \tan^{-1}(x) = \dfrac{1}{1+x^2}$, which would require that I had memorized the derivative (something I tried my best not to do when studying differential calculus). When studying Mathematics, for the most part (and within reason of course) I try my best never to memorize what I can re-derive/prove. I've found that this approach helps improve my skills, and pushes me to search for the deepest possible understanding. But it seems that there are some things, that just have to be committed to memory to be able to make any sort of progress, and this troubles me quite a bit, as I'm not sure as to what I should be just memorizing , and what I should really be working to get the best understanding on. Furthermore Integration is a very heuristic process, whereas Differentiation is a more algorithmic process. Generally we try to get integrals into forms we know of already so that we can evaluate them (with the exception of the Risch algorithm), or it would be impossible to evaluate them by any other means. Wouldn't that require one to memorize the various types of possible integrals? First off, am I looking at this wrong? Are there ways one can reprove results, or evaluate integrals, in a manner that doesn't require one to just memorize and recall a list of formula's like a parrot? What aspects of Integral Calculus would you say, just have to be memorized , i.e. what results in Integral Calculus are close to impossible to re-derive or prove on the spot? Where does one draw the line, between what should be looked at long and hard for the deepest possible understanding, and what should just be memorized ? Lastly, correct me if I'm wrong, but as one makes the transition into higher mathematics (analysis and beyond), that there are some things that you just have to commit to memory , to be able to make any sort of progress?",,"['calculus', 'integration', 'derivatives', 'soft-question']"
70,How to interpret Newton's 6th Lemma?,How to interpret Newton's 6th Lemma?,,"In Newton's ""Principia Mathematica"" Book 1, Section 1 (""Of the Motion of Bodies"") there is the following Lemma 6: ""LEMMA VI. If any arc ACB, given in position, is subtended by its chord AB, and in any point A, in the middle of the continued curvature, is touched   by a right line AD, produced both ways; then if the points A and B   approach one another and meet, I say, the angle BAD, contained between   the chord and the tangent, will be diminished in infinitum, and   ultimately will vanish. For if that angle does not vanish, the arc ACB will contain with the   tangent AD an angle equal to a rectilinear angle; and therefore the   curvature at the point A will not be continued, which is against the   supposition."" ...this text is accompanied by the following diagram: Noting, that the line rbd is parallel to the line RBD and the arc Acb appears to have a smaller curvature than the arc ACB on Newton's diagram, which of the following animated diagrams correctly depicts his Lemma ? This diagram of mine?: ...or this diagram of mine?: Note, that in the latter diagram, I have added the red colored depictions of the angles BAD and ABD .  These angle depictions do not appear on Newton's diagram (...but he writes about the angle BAD in the text of his 6 th Lemma).","In Newton's ""Principia Mathematica"" Book 1, Section 1 (""Of the Motion of Bodies"") there is the following Lemma 6: ""LEMMA VI. If any arc ACB, given in position, is subtended by its chord AB, and in any point A, in the middle of the continued curvature, is touched   by a right line AD, produced both ways; then if the points A and B   approach one another and meet, I say, the angle BAD, contained between   the chord and the tangent, will be diminished in infinitum, and   ultimately will vanish. For if that angle does not vanish, the arc ACB will contain with the   tangent AD an angle equal to a rectilinear angle; and therefore the   curvature at the point A will not be continued, which is against the   supposition."" ...this text is accompanied by the following diagram: Noting, that the line rbd is parallel to the line RBD and the arc Acb appears to have a smaller curvature than the arc ACB on Newton's diagram, which of the following animated diagrams correctly depicts his Lemma ? This diagram of mine?: ...or this diagram of mine?: Note, that in the latter diagram, I have added the red colored depictions of the angles BAD and ABD .  These angle depictions do not appear on Newton's diagram (...but he writes about the angle BAD in the text of his 6 th Lemma).",,"['calculus', 'geometry']"
71,How to evaluate $\int_{0}^{+\infty }\frac{x^{3}\sin\left ( (1/2)\pi x \right )}{e^{2\pi \sqrt{x}}-1}\mathrm{d}x$,How to evaluate,\int_{0}^{+\infty }\frac{x^{3}\sin\left ( (1/2)\pi x \right )}{e^{2\pi \sqrt{x}}-1}\mathrm{d}x,I got an answer below \begin{align*} \int_{0}^{\infty }\frac{x^{3}\sin\left ( \frac{1}{2}\pi x \right )}{e^{2\pi \sqrt{x}}-1}\mathrm{d}x&=\frac{17}{16}-\frac{8}{3\pi }-\frac{7}{\pi ^{2}}+\frac{35}{2\pi ^{3}}-\frac{105}{16\pi ^{4}} \\&\approx 0.00145669538148559\cdots \cdots  \end{align*} which agree with mathematica.But I have no idea how to prove it.,I got an answer below \begin{align*} \int_{0}^{\infty }\frac{x^{3}\sin\left ( \frac{1}{2}\pi x \right )}{e^{2\pi \sqrt{x}}-1}\mathrm{d}x&=\frac{17}{16}-\frac{8}{3\pi }-\frac{7}{\pi ^{2}}+\frac{35}{2\pi ^{3}}-\frac{105}{16\pi ^{4}} \\&\approx 0.00145669538148559\cdots \cdots  \end{align*} which agree with mathematica.But I have no idea how to prove it.,,"['calculus', 'integration', 'analysis']"
72,"Closed-form of $\int_0^1 \left(\ln \Gamma(x)\right)^3\,dx$",Closed-form of,"\int_0^1 \left(\ln \Gamma(x)\right)^3\,dx","From the amazing result by Raabe we know that $$LG_1=\int_0^1 \ln \Gamma(x)\,dx = \frac{1}{2}\ln(2\pi) = -\zeta'(0).$$ We also know that $$LG_2 = \int_0^1 \left(\ln \Gamma(x)\right)^2\,dx = \frac{\gamma^2}{12}+\frac{\pi^2}{48}+\frac{1}{3}\gamma\ln\sqrt{2\pi}+\frac{4}{3}\left(\ln\sqrt{2\pi}\right)^2-\left(\gamma+2\ln\sqrt{2\pi}\right)\frac{\zeta'(2)}{\pi^2}+\frac{\zeta''(2)}{2\pi^2}.$$ Decimal expansion of $LG_1$ is at $\text{A075700}$ and of $LG_2$ at $\text{A102887}$ . According to several authors there is not a known closed-form of $$\begin{align} LG_3 & =  \int_0^1 \left(\ln \Gamma(x)\right)^3\,dx \\ & \approx 5.740388807229474280019571688102461462961\dots \end{align}$$ Any idea to get a closed-form of $LG_3$? A conjectured one also would be nice.","From the amazing result by Raabe we know that $$LG_1=\int_0^1 \ln \Gamma(x)\,dx = \frac{1}{2}\ln(2\pi) = -\zeta'(0).$$ We also know that $$LG_2 = \int_0^1 \left(\ln \Gamma(x)\right)^2\,dx = \frac{\gamma^2}{12}+\frac{\pi^2}{48}+\frac{1}{3}\gamma\ln\sqrt{2\pi}+\frac{4}{3}\left(\ln\sqrt{2\pi}\right)^2-\left(\gamma+2\ln\sqrt{2\pi}\right)\frac{\zeta'(2)}{\pi^2}+\frac{\zeta''(2)}{2\pi^2}.$$ Decimal expansion of $LG_1$ is at $\text{A075700}$ and of $LG_2$ at $\text{A102887}$ . According to several authors there is not a known closed-form of $$\begin{align} LG_3 & =  \int_0^1 \left(\ln \Gamma(x)\right)^3\,dx \\ & \approx 5.740388807229474280019571688102461462961\dots \end{align}$$ Any idea to get a closed-form of $LG_3$? A conjectured one also would be nice.",,"['calculus', 'definite-integrals', 'special-functions', 'closed-form', 'gamma-function']"
73,An integration to first order,An integration to first order,,"I am having some trouble evaluating an integral -- involving taking an approximation. It would be great if someone could help me. I wish to evaluate $$\int_0^\pi {\cos\theta\cos \left[\omega t-{\omega \over c}(a^2+r^2-2rb\cos \theta)^{1\over 2}\right]\over (a^2+r^2-2rb\cos \theta)^{1\over 2}}d\theta$$ for the case where $a\gg r$ and $b=a\sin \phi$, to first order in $1\over a$. The answer is supposed to be  $$\pi r^2\omega(\sin \phi)[\sin(\omega t-{\omega\over c}a)]\over 2a$$but I can't see how to get there... There must be some Taylor expansion required. I suppose we could write $$(a^2+r^2-2rb\cos \theta)^{1\over 2}\approx a\left(1-{rb\over a^2}\cos\theta\right)$$ but when next? Ok, if we ignore the given answer -- assuming it is wrong, what should I be getting? For the argument of the $\cos \left[\omega t-{\omega \over c}(a^2+r^2-2rb\cos \theta)^{1\over 2}\right]$, suppose I Taylor expand $\cos$. It would go something like $$1-{1\over 2}\left[\omega t-{\omega \over c}(a^2+r^2-2rb\cos \theta)^{1\over 2}\right]^2+...$$ However, note that the higher order terms involve terms of $\left(1\over a\right)^n$, with values of $n$ of the non-negligible magnitude. So that can't be the way to do it? Please help! Thanks.","I am having some trouble evaluating an integral -- involving taking an approximation. It would be great if someone could help me. I wish to evaluate $$\int_0^\pi {\cos\theta\cos \left[\omega t-{\omega \over c}(a^2+r^2-2rb\cos \theta)^{1\over 2}\right]\over (a^2+r^2-2rb\cos \theta)^{1\over 2}}d\theta$$ for the case where $a\gg r$ and $b=a\sin \phi$, to first order in $1\over a$. The answer is supposed to be  $$\pi r^2\omega(\sin \phi)[\sin(\omega t-{\omega\over c}a)]\over 2a$$but I can't see how to get there... There must be some Taylor expansion required. I suppose we could write $$(a^2+r^2-2rb\cos \theta)^{1\over 2}\approx a\left(1-{rb\over a^2}\cos\theta\right)$$ but when next? Ok, if we ignore the given answer -- assuming it is wrong, what should I be getting? For the argument of the $\cos \left[\omega t-{\omega \over c}(a^2+r^2-2rb\cos \theta)^{1\over 2}\right]$, suppose I Taylor expand $\cos$. It would go something like $$1-{1\over 2}\left[\omega t-{\omega \over c}(a^2+r^2-2rb\cos \theta)^{1\over 2}\right]^2+...$$ However, note that the higher order terms involve terms of $\left(1\over a\right)^n$, with values of $n$ of the non-negligible magnitude. So that can't be the way to do it? Please help! Thanks.",,"['calculus', 'integration', 'asymptotics']"
74,Trying to prove a proposition about the nth order derivative of a polynomial by induction - is this correct?,Trying to prove a proposition about the nth order derivative of a polynomial by induction - is this correct?,,"Recently, I decided to try and create a formula for the $n$ th order derivative of a polynomial, and I believe I succeeded! I tried to do a proof by induction to confirm this for myself, but since I haven't done one before, I'm not really sure if this is correct. Any insight would be greatly appreciated! So, here's my (attempted) proof: Let $P(x)$ be a polynomial of degree $s$ with integer coefficients $c_0,c_1,c_2,...c_s$ of the form $$P(x)=c_0+c_1x+c_2x^2+c_3x^3+\cdots+c_sx^s$$ Then, I propose that $$P^{(n)}(x)=\sum_{i=0}^{s-n}c_{n+i}\frac{(n+i)!}{i!}x^i$$ Base case for $n=0$ : $$P^{(0)}(x)=\sum_{i=0}^{s}c_{i}x^i$$ $$=c_0+c_1x^1+c_2x^2+c_3x^3+\cdots+c_sx^s$$ $$=P(x)$$ Inductive step: Assume that for a particular natural number $k$ , the case $n=k$ is true (induction hypothesis): $$P^{(k)}(x)=\sum_{i=0}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i$$ Expanding the first term gives: $$P^{(k)}(x)=c_kk!+\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i$$ Differentiating both sides with respect to $x$ gives: $$\frac{d}{dx}P^{(k)}(x)=\frac{d}{dx}(c_kk!+\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i)$$ $$P^{(k+1)}(x)=\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{(i-1)!}x^{i-1}$$ The right side can be simplified as: $$\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{(i-1)!}x^{i-1}=\sum_{i=0}^{s-k-1}c_{k+i+1}\frac{(k+i+1)!}{i!}x^i$$ $$=\sum_{i=0}^{s-(k+1)}c_{(k+1)+i}\frac{((k+1)+i)!}{i!}x^i$$ Equating both sides once more gives: $$P^{(k+1)}(x)=\sum_{i=0}^{s-(k+1)}c_{(k+1)+i}\frac{((k+1)+i)!}{i!}x^i$$ Therefore, the statement is true for $n=k+1$ , proving the inductive step. Since the base case and the inductive step have been proven true, the statement holds for all positive integers $n$ . Hopefully that was all correct! Let me know if I messed up somewhere, it'd be appreciated! Edit: As Aman Kushwaha described, the inductive step has an assumption that $n\leq s$ , and that for $n>s$ , we can just define $P^{(n)}(x)=0$ . Edit 2: Ryszard Szwarc showed me that there is actually a formula on Wikipedia analogous to the power rule for regular derivatives, but for fractional derivatives instead. Whereas the normal power rule for the nth order derivative is $$f^{(n)}(x)=\frac{d^n}{dx^n}cx^k=\frac{k!}{k-n}cx^{k-n}$$ the generalized power rule for derivatives of a real number order (excluding the negative integers) is $$f^{(n)}(x)=\frac{d^n}{dx^n}cx^k=\frac{\Gamma (k+1)}{\Gamma (k-n+1)}cx^{k-n}$$ Using this, a new formula that encompasses fractional derivatives can be made that avoids the ambiguity associated with the non integer subscript polynomial coefficients. The only remaining issue is the ambiguity with the upper bound of the summation, however, when analyzing the generalized power rule, it becomes clear that constant terms in the polynomial only evaluate to 0 after each whole number iteration of the differential operator. Therefore the issue can actually be avoided entirely by using the floor function! So, overall, this is what I came up with: $$P^{(n)}(x)=\sum_{i=0}^{s-\lfloor n \rfloor}c_{\lfloor n \rfloor +i}\frac{\Gamma (\lfloor n \rfloor +i+1)}{\Gamma (\lfloor n \rfloor -n+i+1)}x^{\lfloor n \rfloor -n+i},\: n \neq \mathbb{Z}^-$$ As far as I can tell, it seems to work, but I have absolutely no idea how to prove it! The formula isn't exactly pretty though, so if anyone has any idea how to simplify it, or how to prove it of course, I'd love to hear about it.","Recently, I decided to try and create a formula for the th order derivative of a polynomial, and I believe I succeeded! I tried to do a proof by induction to confirm this for myself, but since I haven't done one before, I'm not really sure if this is correct. Any insight would be greatly appreciated! So, here's my (attempted) proof: Let be a polynomial of degree with integer coefficients of the form Then, I propose that Base case for : Inductive step: Assume that for a particular natural number , the case is true (induction hypothesis): Expanding the first term gives: Differentiating both sides with respect to gives: The right side can be simplified as: Equating both sides once more gives: Therefore, the statement is true for , proving the inductive step. Since the base case and the inductive step have been proven true, the statement holds for all positive integers . Hopefully that was all correct! Let me know if I messed up somewhere, it'd be appreciated! Edit: As Aman Kushwaha described, the inductive step has an assumption that , and that for , we can just define . Edit 2: Ryszard Szwarc showed me that there is actually a formula on Wikipedia analogous to the power rule for regular derivatives, but for fractional derivatives instead. Whereas the normal power rule for the nth order derivative is the generalized power rule for derivatives of a real number order (excluding the negative integers) is Using this, a new formula that encompasses fractional derivatives can be made that avoids the ambiguity associated with the non integer subscript polynomial coefficients. The only remaining issue is the ambiguity with the upper bound of the summation, however, when analyzing the generalized power rule, it becomes clear that constant terms in the polynomial only evaluate to 0 after each whole number iteration of the differential operator. Therefore the issue can actually be avoided entirely by using the floor function! So, overall, this is what I came up with: As far as I can tell, it seems to work, but I have absolutely no idea how to prove it! The formula isn't exactly pretty though, so if anyone has any idea how to simplify it, or how to prove it of course, I'd love to hear about it.","n P(x) s c_0,c_1,c_2,...c_s P(x)=c_0+c_1x+c_2x^2+c_3x^3+\cdots+c_sx^s P^{(n)}(x)=\sum_{i=0}^{s-n}c_{n+i}\frac{(n+i)!}{i!}x^i n=0 P^{(0)}(x)=\sum_{i=0}^{s}c_{i}x^i =c_0+c_1x^1+c_2x^2+c_3x^3+\cdots+c_sx^s =P(x) k n=k P^{(k)}(x)=\sum_{i=0}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i P^{(k)}(x)=c_kk!+\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i x \frac{d}{dx}P^{(k)}(x)=\frac{d}{dx}(c_kk!+\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i) P^{(k+1)}(x)=\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{(i-1)!}x^{i-1} \sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{(i-1)!}x^{i-1}=\sum_{i=0}^{s-k-1}c_{k+i+1}\frac{(k+i+1)!}{i!}x^i =\sum_{i=0}^{s-(k+1)}c_{(k+1)+i}\frac{((k+1)+i)!}{i!}x^i P^{(k+1)}(x)=\sum_{i=0}^{s-(k+1)}c_{(k+1)+i}\frac{((k+1)+i)!}{i!}x^i n=k+1 n n\leq s n>s P^{(n)}(x)=0 f^{(n)}(x)=\frac{d^n}{dx^n}cx^k=\frac{k!}{k-n}cx^{k-n} f^{(n)}(x)=\frac{d^n}{dx^n}cx^k=\frac{\Gamma (k+1)}{\Gamma (k-n+1)}cx^{k-n} P^{(n)}(x)=\sum_{i=0}^{s-\lfloor n \rfloor}c_{\lfloor n \rfloor +i}\frac{\Gamma (\lfloor n \rfloor +i+1)}{\Gamma (\lfloor n \rfloor -n+i+1)}x^{\lfloor n \rfloor -n+i},\: n \neq \mathbb{Z}^-","['calculus', 'solution-verification', 'proof-writing', 'induction']"
75,"Which Fourier series are ""legal""?","Which Fourier series are ""legal""?",,"Let's consider a continuous function $f(x)$ and real numbers $\lambda_n=(\alpha+\beta n)\pi$ where both $\alpha$ and $\beta$ are integers.  In any interval $I$, is it true that  $$ f(x)=\sum_{n\geq 0}a_n\sin(\lambda_n x) \quad\text{and}\quad a_n=\frac{\int_I f(x)\sin (\lambda_n x)\,\mathrm{d}x}{\int_I \sin^2(\lambda_n x)\,\mathrm{d}x}? $$ If not, can I modify the hypothesis so that it becomes true?","Let's consider a continuous function $f(x)$ and real numbers $\lambda_n=(\alpha+\beta n)\pi$ where both $\alpha$ and $\beta$ are integers.  In any interval $I$, is it true that  $$ f(x)=\sum_{n\geq 0}a_n\sin(\lambda_n x) \quad\text{and}\quad a_n=\frac{\int_I f(x)\sin (\lambda_n x)\,\mathrm{d}x}{\int_I \sin^2(\lambda_n x)\,\mathrm{d}x}? $$ If not, can I modify the hypothesis so that it becomes true?",,"['calculus', 'sequences-and-series', 'fourier-analysis', 'fourier-series', 'trigonometric-series']"
76,"Question on the paper Donal F. Connon, ""Some integrals involving the Stieltjes constants""","Question on the paper Donal F. Connon, ""Some integrals involving the Stieltjes constants""",,"I'm reading Donal F. Connon, Some integrals involving the Stieltjes constants . It gives a definition of the generalized Stieltjes constants $\gamma_n(u)$ as coefficients in the Laurent series expansion of the Hurwitz zeta function (formula $(2.3)$ on page $7$): $$\zeta(s,u)=\sum_{n=0}^\infty\frac1{(n+u)^s}=\frac1{s-1}+\sum_{n=0}^\infty\frac{(-1)^n}{n!}\gamma_n(u)\,(s-1)^n$$ It also gives the following integral representation (formula $(2.4)$ on page $7$): $$\int_0^1\left[\frac1{1-y}+\frac1{\log y}\right]y^{u-1}\log\left|\log y\right|dy=-2\gamma_1(u)-\gamma\,\gamma_0(u)-\log^2u-\gamma\log u$$ Recall that $\gamma_0(u)$ in the second term is just a negation of the digamma function : $\gamma_0(u)=-\psi(u)$. The given integral representation does not check numerically for me, e.g. for $u=1$ the integral on the left is $$-0.26036207832404194945778976048034290752168080191529...$$ but the numeric value of the right hand side is $$-0.18754623284036522459720338460544158838394446358095...$$ Further numeric calculations suggest that the right hand side should instead be $$-\gamma_1(u)-\gamma\,\gamma_0(u)-\frac{\log^2u}2-\gamma\log u$$ Is it an error in the paper, or I just misunderstand something?","I'm reading Donal F. Connon, Some integrals involving the Stieltjes constants . It gives a definition of the generalized Stieltjes constants $\gamma_n(u)$ as coefficients in the Laurent series expansion of the Hurwitz zeta function (formula $(2.3)$ on page $7$): $$\zeta(s,u)=\sum_{n=0}^\infty\frac1{(n+u)^s}=\frac1{s-1}+\sum_{n=0}^\infty\frac{(-1)^n}{n!}\gamma_n(u)\,(s-1)^n$$ It also gives the following integral representation (formula $(2.4)$ on page $7$): $$\int_0^1\left[\frac1{1-y}+\frac1{\log y}\right]y^{u-1}\log\left|\log y\right|dy=-2\gamma_1(u)-\gamma\,\gamma_0(u)-\log^2u-\gamma\log u$$ Recall that $\gamma_0(u)$ in the second term is just a negation of the digamma function : $\gamma_0(u)=-\psi(u)$. The given integral representation does not check numerically for me, e.g. for $u=1$ the integral on the left is $$-0.26036207832404194945778976048034290752168080191529...$$ but the numeric value of the right hand side is $$-0.18754623284036522459720338460544158838394446358095...$$ Further numeric calculations suggest that the right hand side should instead be $$-\gamma_1(u)-\gamma\,\gamma_0(u)-\frac{\log^2u}2-\gamma\log u$$ Is it an error in the paper, or I just misunderstand something?",,"['calculus', 'definite-integrals', 'special-functions', 'zeta-functions', 'stieltjes-constants']"
77,Conflicting limit answers using calculator and wolfram alpha,Conflicting limit answers using calculator and wolfram alpha,,"I want to evaluate $\lim\limits_{x\to0} \dfrac{\tan(x) - \sin(x)}{(\sin(x))^3}$, Calculator says it's 0 when substituted with 0.0000000001. Wolfram Alpha says it's 1/2. The Problem Set says the answer is 1/2. I think I believe Wolfram Alpha more but I've been using the calculator method so I can answer stuff really fast (because it's for a board exam, shouldn't spend too much time deriving) is there a way for me to know?","I want to evaluate $\lim\limits_{x\to0} \dfrac{\tan(x) - \sin(x)}{(\sin(x))^3}$, Calculator says it's 0 when substituted with 0.0000000001. Wolfram Alpha says it's 1/2. The Problem Set says the answer is 1/2. I think I believe Wolfram Alpha more but I've been using the calculator method so I can answer stuff really fast (because it's for a board exam, shouldn't spend too much time deriving) is there a way for me to know?",,"['calculus', 'wolfram-alpha', 'calculator']"
78,Help to identify every equation in this meme? [closed],Help to identify every equation in this meme? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 5 years ago . Improve this question A couple of the equations in this meme aren’t easy to read, and I probably don’t know them so I couldn’t tell what they are. Can you identify all the equations, and help me feel smart on twitter?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 5 years ago . Improve this question A couple of the equations in this meme aren’t easy to read, and I probably don’t know them so I couldn’t tell what they are. Can you identify all the equations, and help me feel smart on twitter?",,"['calculus', 'geometry', 'physics', 'popular-math']"
79,Does $\sum\limits_{n=1}^\infty\frac{1}{\sqrt{n}+\sqrt{n+1}}$ converge?,Does  converge?,\sum\limits_{n=1}^\infty\frac{1}{\sqrt{n}+\sqrt{n+1}},"Does the following series converge or diverge? $$ \sum\limits_{n=1}^\infty\frac{1}{\sqrt{n}+\sqrt{n+1}} $$ The methods I have at my disposal are geometric and harmonic series, comparison test, limit comparison test, and the ratio test.","Does the following series converge or diverge? $$ \sum\limits_{n=1}^\infty\frac{1}{\sqrt{n}+\sqrt{n+1}} $$ The methods I have at my disposal are geometric and harmonic series, comparison test, limit comparison test, and the ratio test.",,"['calculus', 'sequences-and-series', 'convergence-divergence']"
80,Why is $\frac{\ln\infty}{\infty}$ equal to $\frac\infty\infty$?,Why is  equal to ?,\frac{\ln\infty}{\infty} \frac\infty\infty,"For me there is no intuitive explanation for this. Yes, i get that if you want to find the natural logarithm of a very high number (infinity) that the ln would be high too. But it does not grow nearly as fast as infinity itself, for example the natural logarithm of $\ln{100} \approx 4.60$ that number is 21 times as small as the number we are taking ln off. But anyhow, here comes the equation making my head jittery. $$ \lim_{x\to\infty} \frac{\ln{\left(x\right)}}{x}=\frac{\infty}{\infty}$$ How, is this possible. Doesn't $\ln$ infinity grow at a much slower speed than infinity itself? I know how to do derivatives. I'm asking for an intuitive explanation. Of why we use that specific convention. In other words, why do we use an indeterminate form, and is there an intuitive explanation for why we might use infinity over infinity, or zero over zero? Khans example. Showing that $\frac{ln\left(x\right)}{x} = \frac{\infty}{\infty}$","For me there is no intuitive explanation for this. Yes, i get that if you want to find the natural logarithm of a very high number (infinity) that the ln would be high too. But it does not grow nearly as fast as infinity itself, for example the natural logarithm of $\ln{100} \approx 4.60$ that number is 21 times as small as the number we are taking ln off. But anyhow, here comes the equation making my head jittery. $$ \lim_{x\to\infty} \frac{\ln{\left(x\right)}}{x}=\frac{\infty}{\infty}$$ How, is this possible. Doesn't $\ln$ infinity grow at a much slower speed than infinity itself? I know how to do derivatives. I'm asking for an intuitive explanation. Of why we use that specific convention. In other words, why do we use an indeterminate form, and is there an intuitive explanation for why we might use infinity over infinity, or zero over zero? Khans example. Showing that $\frac{ln\left(x\right)}{x} = \frac{\infty}{\infty}$",,"['calculus', 'limits', 'infinity', 'indeterminate-forms']"
81,Can a cubic function have two tangents at a single point?,Can a cubic function have two tangents at a single point?,,"I have a question regarding this question . The question posed is if from a point ($h,3−h$) exactly two distinct tangents are drawn to $f(x)=x^3−9x^2−px+q$ find $p$ and $q$ I've been waiting all week for someone smarter than me to answer this question, but no one has, so I have to ask my question. I have forgotten most of my calculus, but I can't see how there could be two distinct tangents at a given point. I know the first derivative gives the slope of the tangent line at a given point. If two tangents are distinct, then they must have different slopes (if they are at the same point), otherwise the lines will be parallel. But the first derivative only gives one slope. For two lines with the same slope to be distinct, they must be parallel. Thus the lines do not go through the same point. But the problem says they do. In my mind, then, there can't be two tangents at a single point because of this apparent contradiction. I have searched the web and nowhere could I find an example (even a strange, unique condition) where there are two tangents at a point. A function is either differentiable at a point, which means there is one tangent line, or the function is not differentiable, which means there can be any number of tangents. Further confusing me is this business with $(h, 3-h)$. Does this present some special situation where dual tangents are possible? I appreciate any light that you can shed on the subject.","I have a question regarding this question . The question posed is if from a point ($h,3−h$) exactly two distinct tangents are drawn to $f(x)=x^3−9x^2−px+q$ find $p$ and $q$ I've been waiting all week for someone smarter than me to answer this question, but no one has, so I have to ask my question. I have forgotten most of my calculus, but I can't see how there could be two distinct tangents at a given point. I know the first derivative gives the slope of the tangent line at a given point. If two tangents are distinct, then they must have different slopes (if they are at the same point), otherwise the lines will be parallel. But the first derivative only gives one slope. For two lines with the same slope to be distinct, they must be parallel. Thus the lines do not go through the same point. But the problem says they do. In my mind, then, there can't be two tangents at a single point because of this apparent contradiction. I have searched the web and nowhere could I find an example (even a strange, unique condition) where there are two tangents at a point. A function is either differentiable at a point, which means there is one tangent line, or the function is not differentiable, which means there can be any number of tangents. Further confusing me is this business with $(h, 3-h)$. Does this present some special situation where dual tangents are possible? I appreciate any light that you can shed on the subject.",,"['calculus', 'derivatives', 'tangent-line']"
82,Evaluating $\int_0^1 \frac{3x}{\sqrt{4-3x}} dx$,Evaluating,\int_0^1 \frac{3x}{\sqrt{4-3x}} dx,"So this is the integral I must evaluate: $$\int_0^1 \frac{3x}{\sqrt{4-3x}} dx$$ I have this already evaluated but I don't understand one of the steps in its transformation. I understand how integrals are evaluated, but I don't understand some of the steps when it is being broken down and integrated. The steps are as follows: $$ - \left( \frac {-3x} {\sqrt{4-3x}}\right)$$ $$ - \left( \frac {4-3x-4}{\sqrt{4-3x}}\right) $$ $$ - \sqrt {4-3x} + \frac {4}{\sqrt{4-3x}}$$ $\mathbf {Question1} $ In these three steps the first thing I don't understand how it got broken down into two terms in the third step. If I add together the terms of the third step I get back the original one but I don't understnad how the author reached this point in the first place, like how can I know how to break it down into which and which terms? After this it is put back into the original equation: $$ \int_0^1 \frac {3x}{\sqrt{4-3x}} dx = - \int_0^1 (4-3x)^\frac {1}{2} dx + 4 \int_0^1 (4-3x)^\frac{-1}{2} dx $$ $$ =  \frac {1}{3} \int_0^1 (4-3x)^\frac {1}{2} (-3 dx) - \frac{4}{3} \int_0^1 (4-3x)^\frac{-1}{2} (-3dx) $$ After this it is integrated as usual with $-3dx$ term disappearing in both and $(4-3x)^\frac{1}{2}$ and $(4-3x)^\frac{-1}{2}$ get integrated with n+1 formula. $\mathbf{Question 2}$ Why was $-3$ multiplied and divided in second step. The dx doesn't change to du so it clearly isn't substitution. So what exactly is happening here?","So this is the integral I must evaluate: I have this already evaluated but I don't understand one of the steps in its transformation. I understand how integrals are evaluated, but I don't understand some of the steps when it is being broken down and integrated. The steps are as follows: In these three steps the first thing I don't understand how it got broken down into two terms in the third step. If I add together the terms of the third step I get back the original one but I don't understnad how the author reached this point in the first place, like how can I know how to break it down into which and which terms? After this it is put back into the original equation: After this it is integrated as usual with term disappearing in both and and get integrated with n+1 formula. Why was multiplied and divided in second step. The dx doesn't change to du so it clearly isn't substitution. So what exactly is happening here?",\int_0^1 \frac{3x}{\sqrt{4-3x}} dx  - \left( \frac {-3x} {\sqrt{4-3x}}\right)  - \left( \frac {4-3x-4}{\sqrt{4-3x}}\right)   - \sqrt {4-3x} + \frac {4}{\sqrt{4-3x}} \mathbf {Question1}   \int_0^1 \frac {3x}{\sqrt{4-3x}} dx = - \int_0^1 (4-3x)^\frac {1}{2} dx + 4 \int_0^1 (4-3x)^\frac{-1}{2} dx   =  \frac {1}{3} \int_0^1 (4-3x)^\frac {1}{2} (-3 dx) - \frac{4}{3} \int_0^1 (4-3x)^\frac{-1}{2} (-3dx)  -3dx (4-3x)^\frac{1}{2} (4-3x)^\frac{-1}{2} \mathbf{Question 2} -3,"['calculus', 'integration', 'definite-integrals', 'substitution']"
83,Best approximation of log 3?,Best approximation of log 3?,,"For $0 \lt x \leq 2$, I can approximate $\log{x}$ using Taylor series. How to do so when  $2\lt x$? My attempt is that: approximate $\log{c}$, where $1\lt c\lt 2$. find $n \in \mathbb{N}$ such that $c^{n} \leq x \lt c^{n+1}$. then, $n\log{c} \leq \log{x}\lt (n+1)\log{c}$ !!! However, for accuracy at 3, $c$ should be small, but if $c$ is small, $n$ is large and can't calculate without a computer.","For $0 \lt x \leq 2$, I can approximate $\log{x}$ using Taylor series. How to do so when  $2\lt x$? My attempt is that: approximate $\log{c}$, where $1\lt c\lt 2$. find $n \in \mathbb{N}$ such that $c^{n} \leq x \lt c^{n+1}$. then, $n\log{c} \leq \log{x}\lt (n+1)\log{c}$ !!! However, for accuracy at 3, $c$ should be small, but if $c$ is small, $n$ is large and can't calculate without a computer.",,['calculus']
84,How is the area of a circle calculated using basic mathematics?,How is the area of a circle calculated using basic mathematics?,,Area of a circle is addition of circumference of layers of a onion. If n is radius of a onion then area is $$ A = 2 \pi \cdot 1 + 2 \pi \cdot 2 + 2\pi \cdot 3 + \ldots + 2  \pi \cdot n $$ which $$ = 2 \pi \cdot \frac{n(n+1)}{2} = \pi (n^2 + n) $$ But the answer is wrong. Please tell me where am I wrong?,Area of a circle is addition of circumference of layers of a onion. If n is radius of a onion then area is $$ A = 2 \pi \cdot 1 + 2 \pi \cdot 2 + 2\pi \cdot 3 + \ldots + 2  \pi \cdot n $$ which $$ = 2 \pi \cdot \frac{n(n+1)}{2} = \pi (n^2 + n) $$ But the answer is wrong. Please tell me where am I wrong?,,"['calculus', 'geometry', 'circles', 'area']"
85,An example that a 2D shape with its centre of mass on its boundary [closed],An example that a 2D shape with its centre of mass on its boundary [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question The object has constant density. Could any body suggest one for me?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question The object has constant density. Could any body suggest one for me?",,"['calculus', 'geometry']"
86,Compute $\lim\limits_{n\to \infty} \int\limits_0^1 x^{2019} \{nx\} dx$,Compute,\lim\limits_{n\to \infty} \int\limits_0^1 x^{2019} \{nx\} dx,"Compute $$\lim\limits_{n\to \infty} \int\limits_0^1 x^{2019} \{nx\} dx,$$ where $\{a\}$ denotes the fractional part of the real number $a$ . I firstly tried to apply the substitution $nx=t$ , but the computations didn't look nice, so I couldn't make any further progress. I also tried to use the mean value theorem for integrals, but it was also a dead end.","Compute where denotes the fractional part of the real number . I firstly tried to apply the substitution , but the computations didn't look nice, so I couldn't make any further progress. I also tried to use the mean value theorem for integrals, but it was also a dead end.","\lim\limits_{n\to \infty} \int\limits_0^1 x^{2019} \{nx\} dx, \{a\} a nx=t","['calculus', 'integration', 'limits', 'definite-integrals']"
87,Are all continuous one one functions differentiable?,Are all continuous one one functions differentiable?,,"I was reading about one one functions and found out that they cannot have maxima or minima except at endpoints of domain. So their derivative , if it exists,  must not change it sign , i.e. , the function should be either strictly increasing or strictly decreasing. From this I've a feeling that all continuous one one functions must be differentiable . Is this true?","I was reading about one one functions and found out that they cannot have maxima or minima except at endpoints of domain. So their derivative , if it exists,  must not change it sign , i.e. , the function should be either strictly increasing or strictly decreasing. From this I've a feeling that all continuous one one functions must be differentiable . Is this true?",,"['calculus', 'functions', 'derivatives']"
88,Integral involving logarithm: $\int_0^\infty \frac{ \ln x}{(x+a)(x+b)} dx$,Integral involving logarithm:,\int_0^\infty \frac{ \ln x}{(x+a)(x+b)} dx,"How to solve the following integral $$\int_{0}^{\infty} \frac{ \ln x}{(x+a)(x+b)} dx,$$ where $a,b>0$ and $a \neq b$. I was looking for some kind of substitution. However, I don't see an obvious one. Thanks!","How to solve the following integral $$\int_{0}^{\infty} \frac{ \ln x}{(x+a)(x+b)} dx,$$ where $a,b>0$ and $a \neq b$. I was looking for some kind of substitution. However, I don't see an obvious one. Thanks!",,"['calculus', 'integration', 'logarithms', 'improper-integrals']"
89,Solve $\int_0^\infty\frac{\ln(2e^x-1)}{e^x-1}dx$,Solve,\int_0^\infty\frac{\ln(2e^x-1)}{e^x-1}dx,"In one of the final problems of the MIT integration bee for this year, $$I=\int_0^\infty\frac{\ln(2e^x-1)}{e^x-1}dx$$ was one of the given problems. My try was to let $u=e^x-1$ to get $$I=\int_0^\infty\frac{\ln(2u+1)}{u(u+1)}du=\int_0^\infty\frac{\ln(x+1)}{x(x+2)}dx$$ I don't know whether I would be right but I had a feeling this was a dead end. Turning the original integral into a geometric series doesn't seem promising either. How should I solve this? Note: These problems are solved in 5 minutes so please come up with a solution that can be done in such a time limit.","In one of the final problems of the MIT integration bee for this year, was one of the given problems. My try was to let to get I don't know whether I would be right but I had a feeling this was a dead end. Turning the original integral into a geometric series doesn't seem promising either. How should I solve this? Note: These problems are solved in 5 minutes so please come up with a solution that can be done in such a time limit.",I=\int_0^\infty\frac{\ln(2e^x-1)}{e^x-1}dx u=e^x-1 I=\int_0^\infty\frac{\ln(2u+1)}{u(u+1)}du=\int_0^\infty\frac{\ln(x+1)}{x(x+2)}dx,"['calculus', 'integration', 'contest-math']"
90,A proof of the product rule using the single variable chain rule?,A proof of the product rule using the single variable chain rule?,,"A while back I saw someone claim that you could prove the product rule in calculus with the single variable chain rule. He provided a proof, but it was utterly incomprehensible. It is easy to prove from the multi variable chain rule, or from logarithmic differentiation, or even from first principles. Is there an actual proof using just the single variable chain rule?","A while back I saw someone claim that you could prove the product rule in calculus with the single variable chain rule. He provided a proof, but it was utterly incomprehensible. It is easy to prove from the multi variable chain rule, or from logarithmic differentiation, or even from first principles. Is there an actual proof using just the single variable chain rule?",,"['calculus', 'alternative-proof']"
91,Why can a derivative be non-linear?,Why can a derivative be non-linear?,,"A definition of the derivative is that it is the slope of the tangent line. For example, $x^3$ has a quadratic derivative. How could the slope of the tangent line be non-linear?","A definition of the derivative is that it is the slope of the tangent line. For example, $x^3$ has a quadratic derivative. How could the slope of the tangent line be non-linear?",,"['calculus', 'derivatives']"
92,show that $\int_{0}^{\infty} \frac {\sin^3(x)}{x^3}dx=\frac{3\pi}{8}$,show that,\int_{0}^{\infty} \frac {\sin^3(x)}{x^3}dx=\frac{3\pi}{8},show that $$\int_{0}^{\infty} \frac {\sin^3(x)}{x^3}dx=\frac{3\pi}{8}$$ using different ways thanks for all,show that $$\int_{0}^{\infty} \frac {\sin^3(x)}{x^3}dx=\frac{3\pi}{8}$$ using different ways thanks for all,,"['calculus', 'integration', 'fourier-analysis', 'contour-integration']"
93,How to find the limit of series? (What should I know?),How to find the limit of series? (What should I know?),,"There is a couple of limits that I failed to find: $$\lim_{n\to\infty}\frac 1 2 + \frac 1 4 + \frac 1 {8} + \cdots + \frac {1}{2^{n}}$$ and $$\lim_{n\to\infty}1 - \frac 1 3 + \frac 1 9 - \frac 1 {27} + \cdots + \frac {(-1)^{n-1}}{3^{n-1}}$$ There is no problem to calculate a limit using Wolfram Alfa or something like that. But what I am interested in is the method, not just a concrete result. So my questions are: What should I do when I need a limit of infinite sum? (are there any rules of thumb?) What theorems or topics from calculus should I know to solve these problems better? I am new to math and will appreciate any help. Thank you!","There is a couple of limits that I failed to find: $$\lim_{n\to\infty}\frac 1 2 + \frac 1 4 + \frac 1 {8} + \cdots + \frac {1}{2^{n}}$$ and $$\lim_{n\to\infty}1 - \frac 1 3 + \frac 1 9 - \frac 1 {27} + \cdots + \frac {(-1)^{n-1}}{3^{n-1}}$$ There is no problem to calculate a limit using Wolfram Alfa or something like that. But what I am interested in is the method, not just a concrete result. So my questions are: What should I do when I need a limit of infinite sum? (are there any rules of thumb?) What theorems or topics from calculus should I know to solve these problems better? I am new to math and will appreciate any help. Thank you!",,"['calculus', 'sequences-and-series', 'limits', 'summation', 'geometric-progressions']"
94,"Is it really true that ""if a function is discontinuous, automatically, it's not differentiable""? [duplicate]","Is it really true that ""if a function is discontinuous, automatically, it's not differentiable""? [duplicate]",,"This question already has answers here : Why can a discontinuous function not be differentiable? (2 answers) Closed 8 years ago . I while back, my calculus teacher said something that I find very bothersome. I didn't have time to clarify, but he said: If a function is discontinuous, automatically, it's not differentiable. I find this bothersome because I can think of many discontinuous piecewise functions like this: $$f(x) = \begin{cases} x^2, & \text{$x≤3$} \\ x^2+3, & \text{$x>3$} \end{cases}$$ Where $f'(x)$ would have two parts of the same function, and give: $$\begin{align} f'(x) = && \begin{cases} 2x, & \text{$x≤3$} \\ 2x, & \text{$x>3$} \end{cases} \\ = && 2x \end{align}$$ So I'm wondering, what exactly is wrong with this? Is there something I'm missing about what it means to be ""continuous""? Or maybe, are there special rules for how to deal with the derivatives of piecewise functions, that I don't know about.","This question already has answers here : Why can a discontinuous function not be differentiable? (2 answers) Closed 8 years ago . I while back, my calculus teacher said something that I find very bothersome. I didn't have time to clarify, but he said: If a function is discontinuous, automatically, it's not differentiable. I find this bothersome because I can think of many discontinuous piecewise functions like this: $$f(x) = \begin{cases} x^2, & \text{$x≤3$} \\ x^2+3, & \text{$x>3$} \end{cases}$$ Where $f'(x)$ would have two parts of the same function, and give: $$\begin{align} f'(x) = && \begin{cases} 2x, & \text{$x≤3$} \\ 2x, & \text{$x>3$} \end{cases} \\ = && 2x \end{align}$$ So I'm wondering, what exactly is wrong with this? Is there something I'm missing about what it means to be ""continuous""? Or maybe, are there special rules for how to deal with the derivatives of piecewise functions, that I don't know about.",,['calculus']
95,"Compute: $\int_{0}^{1}\frac{x^4+1}{x^6+1}\, dx$",Compute:,"\int_{0}^{1}\frac{x^4+1}{x^6+1}\, dx","I'm trying to compute: $$\int_{0}^{1}\dfrac{x^4+1}{x^6+1}\,dx.$$ I tried to change $x^{4}$ into $t^{2}$ or $t$ , but it didn't work for me. Any suggestions? Thanks!","I'm trying to compute: I tried to change into or , but it didn't work for me. Any suggestions? Thanks!","\int_{0}^{1}\dfrac{x^4+1}{x^6+1}\,dx. x^{4} t^{2} t","['calculus', 'integration', 'definite-integrals']"
96,"Will inverse functions, and functions always meet at the line $y=x$?","Will inverse functions, and functions always meet at the line ?",y=x,"If I have a function, the inverse function, by definition will be a reflection of the original function in the line $y=x$ , so if I wanted to find the point of intersection, instead of solving it with equating both of the functions to equaling each other, could I assume that the point of intersection, between the two functions, will always be at the point $y=x$ ? This would enable me to solve problems much easier, instead of having to solve quartic equations, for example.","If I have a function, the inverse function, by definition will be a reflection of the original function in the line , so if I wanted to find the point of intersection, instead of solving it with equating both of the functions to equaling each other, could I assume that the point of intersection, between the two functions, will always be at the point ? This would enable me to solve problems much easier, instead of having to solve quartic equations, for example.",y=x y=x,"['calculus', 'algebra-precalculus', 'analysis', 'inverse']"
97,Evaluating $\lim_{n \to \infty}\frac{n}{2}\sqrt{2-2\cos\left(\frac{360^\circ}{n}\right)}$,Evaluating,\lim_{n \to \infty}\frac{n}{2}\sqrt{2-2\cos\left(\frac{360^\circ}{n}\right)},"I was thinking about different ways of finding $\pi$ and stumbled upon what I'm sure is a very old method: dividing a circle of radius $r$ up into $n$ isosceles triangles each with radial side length $r$ and central angle $$\theta=\frac{360^\circ}{n}$$ Use $s$ for the side opposite to $\theta$. Then we can approximate the circumference as $sn$. By the law of cosines: $$s=\sqrt{2r^2-2r^2 \cos{\theta}}=r\sqrt{2-2\cos{\left(\frac{360^\circ}{n}\right)}}$$ We know $\pi=\text{circumference}/\text{diameter} \approx \frac{sn}{2r}=\frac{n}{2}\sqrt{2-2\cos\left(\frac{360^\circ}{n}\right)}$. This becomes exact at the limit: $$\pi=\lim_{n \to \infty}\frac{n}{2}\sqrt{2-2\cos\left(\frac{360^\circ}{n}\right)}$$ Now for my question: How would you solve the opposite problem? To make my meaning more clear, above I used the definition of $\pi$ to determine a limit that gives its value. But if I had just been given the limit, what technique(s) could I have used to determine that it evaluates to $\pi$?","I was thinking about different ways of finding $\pi$ and stumbled upon what I'm sure is a very old method: dividing a circle of radius $r$ up into $n$ isosceles triangles each with radial side length $r$ and central angle $$\theta=\frac{360^\circ}{n}$$ Use $s$ for the side opposite to $\theta$. Then we can approximate the circumference as $sn$. By the law of cosines: $$s=\sqrt{2r^2-2r^2 \cos{\theta}}=r\sqrt{2-2\cos{\left(\frac{360^\circ}{n}\right)}}$$ We know $\pi=\text{circumference}/\text{diameter} \approx \frac{sn}{2r}=\frac{n}{2}\sqrt{2-2\cos\left(\frac{360^\circ}{n}\right)}$. This becomes exact at the limit: $$\pi=\lim_{n \to \infty}\frac{n}{2}\sqrt{2-2\cos\left(\frac{360^\circ}{n}\right)}$$ Now for my question: How would you solve the opposite problem? To make my meaning more clear, above I used the definition of $\pi$ to determine a limit that gives its value. But if I had just been given the limit, what technique(s) could I have used to determine that it evaluates to $\pi$?",,"['calculus', 'geometry', 'limits', 'trigonometry', 'pi']"
98,What is wrong with this integral reasoning?,What is wrong with this integral reasoning?,,"$$\int\frac{x^{2}+1}{x\sqrt{x^{4}+1}}dx$$ We start by multiplying by $1=\frac{x}{x}$. $$\int\frac{x^{2}+1}{x^{2}\sqrt{x^{4}+1}}xdx$$ Next, we use the substitution $u=x^{2}$;$\frac{du}{2}=xdx$. $$\frac{1}{2}\int\frac{u+1}{u\sqrt{u^{2}+1}}du=\frac{1}{2}\int\frac{1}{\sqrt{u^{2}+1}}du+\frac{1}{2}\int\frac{1}{u\sqrt{u^{2}+1}}du=$$ $$=\frac{1}{2}\ln\left(u+\sqrt{u^{2}+1}\right)+\frac{1}{2}\int\frac{1}{u\sqrt{u^{2}+1}}du$$ The problem is now reduced to computing the integral $\frac{1}{2}\int\frac{1}{u\sqrt{u^{2}+1}}du$. $$\frac{1}{2}\int\frac{1}{u\sqrt{u^{2}+1}}du=\frac{1}{2}\int\frac{1}{u^2\sqrt{1+\frac{1}{u^{2}}}}du$$ We use substitution again $v=\frac{1}{u}$;$-dv=\frac{1}{u^{2}}du$, then we have the next integral. $$-\frac{1}{2}\int\frac{1}{\sqrt{1+v^{2}}}dv=-\frac{1}{2}\ln\left(v+\sqrt{1+v^{2}}\right)=-\frac{1}{2}\ln\left(\frac{1}{u}+\sqrt{1+\frac{1}{u^{2}}}\right)=$$ $$=-\frac{1}{2}\ln\left(\frac{1+\sqrt{u^{2}+1}}{u}\right)$$ Finally the solution is: $$\int\frac{x^{2}+1}{x\sqrt{x^{4}+1}}dx=\frac{1}{2}\ln\left(u+\sqrt{1+u^{2}}\right)-\frac{1}{2}\ln\left(\frac{1+\sqrt{u^{2}+1}}{u}\right)=\frac{1}{2}\ln\left(\frac{x^{4}+x^{2}\sqrt{x^{4}+1}}{1+\sqrt{x^{4}+1}}\right).$$ But the problem is, the book has the following solution: $$\int\frac{x^{2}+1}{x\sqrt{x^{4}+1}}dx=\ln\left(\frac{x^{2}-1+\sqrt{x^{4}+1}}{x}\right)$$ which is obviously different.","$$\int\frac{x^{2}+1}{x\sqrt{x^{4}+1}}dx$$ We start by multiplying by $1=\frac{x}{x}$. $$\int\frac{x^{2}+1}{x^{2}\sqrt{x^{4}+1}}xdx$$ Next, we use the substitution $u=x^{2}$;$\frac{du}{2}=xdx$. $$\frac{1}{2}\int\frac{u+1}{u\sqrt{u^{2}+1}}du=\frac{1}{2}\int\frac{1}{\sqrt{u^{2}+1}}du+\frac{1}{2}\int\frac{1}{u\sqrt{u^{2}+1}}du=$$ $$=\frac{1}{2}\ln\left(u+\sqrt{u^{2}+1}\right)+\frac{1}{2}\int\frac{1}{u\sqrt{u^{2}+1}}du$$ The problem is now reduced to computing the integral $\frac{1}{2}\int\frac{1}{u\sqrt{u^{2}+1}}du$. $$\frac{1}{2}\int\frac{1}{u\sqrt{u^{2}+1}}du=\frac{1}{2}\int\frac{1}{u^2\sqrt{1+\frac{1}{u^{2}}}}du$$ We use substitution again $v=\frac{1}{u}$;$-dv=\frac{1}{u^{2}}du$, then we have the next integral. $$-\frac{1}{2}\int\frac{1}{\sqrt{1+v^{2}}}dv=-\frac{1}{2}\ln\left(v+\sqrt{1+v^{2}}\right)=-\frac{1}{2}\ln\left(\frac{1}{u}+\sqrt{1+\frac{1}{u^{2}}}\right)=$$ $$=-\frac{1}{2}\ln\left(\frac{1+\sqrt{u^{2}+1}}{u}\right)$$ Finally the solution is: $$\int\frac{x^{2}+1}{x\sqrt{x^{4}+1}}dx=\frac{1}{2}\ln\left(u+\sqrt{1+u^{2}}\right)-\frac{1}{2}\ln\left(\frac{1+\sqrt{u^{2}+1}}{u}\right)=\frac{1}{2}\ln\left(\frac{x^{4}+x^{2}\sqrt{x^{4}+1}}{1+\sqrt{x^{4}+1}}\right).$$ But the problem is, the book has the following solution: $$\int\frac{x^{2}+1}{x\sqrt{x^{4}+1}}dx=\ln\left(\frac{x^{2}-1+\sqrt{x^{4}+1}}{x}\right)$$ which is obviously different.",,['calculus']
99,"How to evaluate $\int_0^{\frac{\pi}{2}} \frac{\ln(\cos(x))}{1+\sin^2(x)} \, dx$",How to evaluate,"\int_0^{\frac{\pi}{2}} \frac{\ln(\cos(x))}{1+\sin^2(x)} \, dx","I saw this problem $$\int_0^{\frac{\pi}{2}}  \frac{\ln(\cos(x))}{1+\sin^2(x)} \, dx$$ on my problem book but I have no idea how to evaluate it. I reduced the integral like this by king's rule $$I_1= \int_0^{\frac{\pi}{2}}  \frac{\ln(\cos(x))}{1+\sin^2(x)}dx =\int_0^{\frac{\pi}{2}}  \frac{\ln(\sin(x))}{1+\cos^2(x)}dx $$ by using the double angle rule $$I_1=\int_0^{\frac{\pi}{2}}  \frac{\ln(2\sin(x/2) \cos(x/2))}{2\cos(x/2)^2}dx$$ we let $x/2=x$ then $dx=2dx$ $$I_1=\int_0^{\frac{\pi}{4}}  \frac{\ln(2)}{\cos(x)^2}dx+\int_0^{\frac{\pi}{4}}  \frac{\ln(\sin(x))}{\cos(x)^2}dx+\int_0^{\frac{\pi}{4}}  \frac{\ln(\cos(x))}{\cos(x)^2}dx$$ now I will denote $\int_0^{\frac{\pi}{4}}\frac{\ln(2)}{\cos(x)^2}dx$ by $I_2$ , and I will denote $\int_0^{\frac{\pi}{4}}\frac{\ln(\sin(x))}{\cos(x)^2}dx$ by $I_3$ , and I will denote $\int_0^{\frac{\pi}{4}}  \frac{\ln(\cos(x))}{\cos(x)^2}dx$ by $I_4$ $I_2$ is easy to do $$I_3=\frac{\ln(\tan(x))}{\cos(x)^2}dx +I_4$$ I will denote $\int_0^{\frac{\pi}{4}}\frac{\ln(\tan(x))}{\cos(x)^2}dx$ by $I_5$ which is easy to do. the problem was to evaluate $2I_4$ which I couldn't do","I saw this problem on my problem book but I have no idea how to evaluate it. I reduced the integral like this by king's rule by using the double angle rule we let then now I will denote by , and I will denote by , and I will denote by is easy to do I will denote by which is easy to do. the problem was to evaluate which I couldn't do","\int_0^{\frac{\pi}{2}}  \frac{\ln(\cos(x))}{1+\sin^2(x)} \, dx I_1= \int_0^{\frac{\pi}{2}}  \frac{\ln(\cos(x))}{1+\sin^2(x)}dx =\int_0^{\frac{\pi}{2}}  \frac{\ln(\sin(x))}{1+\cos^2(x)}dx  I_1=\int_0^{\frac{\pi}{2}}  \frac{\ln(2\sin(x/2) \cos(x/2))}{2\cos(x/2)^2}dx x/2=x dx=2dx I_1=\int_0^{\frac{\pi}{4}}  \frac{\ln(2)}{\cos(x)^2}dx+\int_0^{\frac{\pi}{4}}  \frac{\ln(\sin(x))}{\cos(x)^2}dx+\int_0^{\frac{\pi}{4}}  \frac{\ln(\cos(x))}{\cos(x)^2}dx \int_0^{\frac{\pi}{4}}\frac{\ln(2)}{\cos(x)^2}dx I_2 \int_0^{\frac{\pi}{4}}\frac{\ln(\sin(x))}{\cos(x)^2}dx I_3 \int_0^{\frac{\pi}{4}}  \frac{\ln(\cos(x))}{\cos(x)^2}dx I_4 I_2 I_3=\frac{\ln(\tan(x))}{\cos(x)^2}dx +I_4 \int_0^{\frac{\pi}{4}}\frac{\ln(\tan(x))}{\cos(x)^2}dx I_5 2I_4","['calculus', 'integration', 'definite-integrals']"
