,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Linear independence of vectors over larger fields,Linear independence of vectors over larger fields,,"I was just wondering whether anyone knows an answer to the following: Suppose that ${\mathbb F}$ is a subfield of a field ${\mathbb G}$ and that $v_1,\ldots ,v_k$ are linearly independent vectors in ${\mathbb F}^n$ (over $\mathbb F$). Is it necessarily true that $v_1,\ldots ,v_k$ are also linearly independent when considered as vectors in ${\mathbb G}^n$ (over $\mathbb G$})? Any help would be much appreciated. Thanks!","I was just wondering whether anyone knows an answer to the following: Suppose that ${\mathbb F}$ is a subfield of a field ${\mathbb G}$ and that $v_1,\ldots ,v_k$ are linearly independent vectors in ${\mathbb F}^n$ (over $\mathbb F$). Is it necessarily true that $v_1,\ldots ,v_k$ are also linearly independent when considered as vectors in ${\mathbb G}^n$ (over $\mathbb G$})? Any help would be much appreciated. Thanks!",,['linear-algebra']
1,Prove that matrix is non-negative,Prove that matrix is non-negative,,"Problem: Given $A_{1}, A_{2}, ..., A_{n}$ - finite sets and $a_{ij} = |A_{i}\cap A_{j}|$ - number of elements in intersection of sets. Prove, that matrix  $(a_{ij})_{i=1,2,..,n}^{j=1,2,.., n}$ is non-negative. I've cleared out that this matrix is symmetric and the largest elements are on its diagonal. I need to prove that $x^{T}Ax \ge 0$ or $$\sum_{i=1}^n\sum_{j=1}^na_{ij}x_{i}x_{j} \ge 0$$ or its eigenvalues are non-negative. I don't know, what need I do next? Any hint will be useful!","Problem: Given $A_{1}, A_{2}, ..., A_{n}$ - finite sets and $a_{ij} = |A_{i}\cap A_{j}|$ - number of elements in intersection of sets. Prove, that matrix  $(a_{ij})_{i=1,2,..,n}^{j=1,2,.., n}$ is non-negative. I've cleared out that this matrix is symmetric and the largest elements are on its diagonal. I need to prove that $x^{T}Ax \ge 0$ or $$\sum_{i=1}^n\sum_{j=1}^na_{ij}x_{i}x_{j} \ge 0$$ or its eigenvalues are non-negative. I don't know, what need I do next? Any hint will be useful!",,['linear-algebra']
2,Advice: Modern vs. Classics,Advice: Modern vs. Classics,,"First of all, my apologies if (well, I know I am but I don't know where to put it) I am posting this in the wrong place. So please feel free to move it to someplace else or to tag it differently if that is possible. Anyways, I've been reading Lang's Linear Algebra and Lang's Undergraduate Algebra and I feel that they might not be the books I was looking for. They don't seem to be thorough enough and even though I have really been enjoying Lang's style I've started to look for other texts to use instead. I've narrowed it down to Hoffman & Kunze vs. Friedberg, Insel & Spence for Linear Algebra and for Abstract Algebra to Artin vs. Dummit & Foote , but I'm having troubles deciding which ones to go with. I've looked at reviews for all of them and they all seem to be great books and exactly what I'm looking for, but I don't know which ones to take as the main ones for reading. Can anyone who is familiar with these help me decide? What are the differences between each book in each set? These are both for self-study and abstraction isn't a real issue. Thank-you.","First of all, my apologies if (well, I know I am but I don't know where to put it) I am posting this in the wrong place. So please feel free to move it to someplace else or to tag it differently if that is possible. Anyways, I've been reading Lang's Linear Algebra and Lang's Undergraduate Algebra and I feel that they might not be the books I was looking for. They don't seem to be thorough enough and even though I have really been enjoying Lang's style I've started to look for other texts to use instead. I've narrowed it down to Hoffman & Kunze vs. Friedberg, Insel & Spence for Linear Algebra and for Abstract Algebra to Artin vs. Dummit & Foote , but I'm having troubles deciding which ones to go with. I've looked at reviews for all of them and they all seem to be great books and exactly what I'm looking for, but I don't know which ones to take as the main ones for reading. Can anyone who is familiar with these help me decide? What are the differences between each book in each set? These are both for self-study and abstraction isn't a real issue. Thank-you.",,"['linear-algebra', 'abstract-algebra']"
3,"Without choosing bases, how to show that the determinant is multiplicative in this sense?","Without choosing bases, how to show that the determinant is multiplicative in this sense?",,"I was recently considering this statement: Let $V$ be a finite-dimensional $k$-vector space, and let $\phi:V\to V$ be an endomorphism. Suppose that $W\subseteq V$ is a subspace that is stable under $\phi$, i.e. such that $\phi(W)\subseteq W$. Let $\psi:W\to W$ be the restriction of $\phi$ to $W$, and let $\rho:(V/W)\to(V/W)$ be the induced map on the quotient space $V/W$. Then   $$\det(\phi)=\det(\psi)\det(\rho).$$ I came up with a proof, but it required choosing bases (ick!): if $\{w_1,\ldots,w_r\}$ is a basis for $W$, and $\{v_1,\ldots,v_s\}$ a set in $V$ that maps down to a basis of $V/W$, then their union $\{w_1,\ldots,w_r,v_1,\ldots,v_s\}$ is a basis for $V$. Expressing $\phi$ as a matrix in this basis, it is a block matrix of the form $$\begin{bmatrix} A & B\\ 0 & C \end{bmatrix}$$ because $\phi(W)\subseteq W$. But $A$ is the $r\times r$ matrix representing the action of $\psi$ on $W$, and $C$ is the $s\times s$ matrix representing the action of $\rho$ on $V/W$, and by properties of block matrices, we have $$\det(\phi)=\det\left(\begin{bmatrix} A & B\\ 0 & C \end{bmatrix}\right)=\det(A)\det(C)=\det(\psi)\det(\rho).$$ All well and good, but can someone tell me how to prove this statement the ""right"" way (via the exterior power functor, exact sequences, etc.?)","I was recently considering this statement: Let $V$ be a finite-dimensional $k$-vector space, and let $\phi:V\to V$ be an endomorphism. Suppose that $W\subseteq V$ is a subspace that is stable under $\phi$, i.e. such that $\phi(W)\subseteq W$. Let $\psi:W\to W$ be the restriction of $\phi$ to $W$, and let $\rho:(V/W)\to(V/W)$ be the induced map on the quotient space $V/W$. Then   $$\det(\phi)=\det(\psi)\det(\rho).$$ I came up with a proof, but it required choosing bases (ick!): if $\{w_1,\ldots,w_r\}$ is a basis for $W$, and $\{v_1,\ldots,v_s\}$ a set in $V$ that maps down to a basis of $V/W$, then their union $\{w_1,\ldots,w_r,v_1,\ldots,v_s\}$ is a basis for $V$. Expressing $\phi$ as a matrix in this basis, it is a block matrix of the form $$\begin{bmatrix} A & B\\ 0 & C \end{bmatrix}$$ because $\phi(W)\subseteq W$. But $A$ is the $r\times r$ matrix representing the action of $\psi$ on $W$, and $C$ is the $s\times s$ matrix representing the action of $\rho$ on $V/W$, and by properties of block matrices, we have $$\det(\phi)=\det\left(\begin{bmatrix} A & B\\ 0 & C \end{bmatrix}\right)=\det(A)\det(C)=\det(\psi)\det(\rho).$$ All well and good, but can someone tell me how to prove this statement the ""right"" way (via the exterior power functor, exact sequences, etc.?)",,"['linear-algebra', 'determinant', 'exterior-algebra']"
4,Inverse of a diagonal matrix plus a Kronecker product?,Inverse of a diagonal matrix plus a Kronecker product?,,"Given two matrices $X$ and $Y$, it's easy to take the inverse of their Kronecker product: $(X\otimes Y)^{-1} = X^{-1}\otimes Y^{-1}$ Now, suppose we have some diagonal matrix $\Lambda$ (or more generally an easily inverted matrix, or one for which we already know the inverse).  Is there a closed-form expression or efficient algorithm for computing $(\Lambda + (X\otimes Y))^{-1}$?","Given two matrices $X$ and $Y$, it's easy to take the inverse of their Kronecker product: $(X\otimes Y)^{-1} = X^{-1}\otimes Y^{-1}$ Now, suppose we have some diagonal matrix $\Lambda$ (or more generally an easily inverted matrix, or one for which we already know the inverse).  Is there a closed-form expression or efficient algorithm for computing $(\Lambda + (X\otimes Y))^{-1}$?",,"['linear-algebra', 'tensor-products', 'inverse']"
5,Sum of n squares $(x_1^2+x_2^2 + \dots + x_n^2)^2 (y_1^2+y_2^2 + \dots + y_n^2) = z_1^2+z_2^2 + \dots + z_n^2$,Sum of n squares,(x_1^2+x_2^2 + \dots + x_n^2)^2 (y_1^2+y_2^2 + \dots + y_n^2) = z_1^2+z_2^2 + \dots + z_n^2,"Consider this 5-Square Identity, $(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2)^2 (y_1^2+y_2^2+y_3^2+y_4^2+y_5^2) = z_1^2+z_2^2+z_3^2+z_4^2+z_5^2$ where, $\begin{align} z_1 &= (-x_1^2+x_2^2+x_3^2+x_4^2+x_5^2)y_1 - 2x_1(0x_1 y_1+x_2 y_2+x_3 y_3+x_4 y_4 + x_5 y_5)\\ z_2 &= (x_1^2-x_2^2+x_3^2+x_4^2+x_5^2)y_2 - 2x_2(x_1 y_1+0x_2 y_2+x_3 y_3+x_4 y_4 + x_5 y_5)\\ z_3 &= (x_1^2+x_2^2-x_3^2+x_4^2+x_5^2)y_3 - 2x_3(x_1 y_1+x_2 y_2+0x_3 y_3+x_4 y_4 + x_5 y_5)\\ z_4 &= (x_1^2+x_2^2+x_3^2-x_4^2+x_5^2)y_4 - 2x_4(x_1 y_1+x_2 y_2+x_3 y_3+0x_4 y_4 + x_5 y_5)\\ z_5 &= (x_1^2+x_2^2+x_3^2+x_4^2-x_5^2)y_5 - 2x_5(x_1 y_1+x_2 y_2+x_3 y_3+x_4 y_4 + 0x_5 y_5) \end{align}$ The pattern is easily seen for, $(x_1^2+x_2^2 + \dots + x_n^2)^2 (y_1^2+y_2^2 + \dots + y_n^2) = z_1^2+z_2^2 + \dots + z_n^2$ The case n = 4 is used in Pfister’s 8-square Identity .  How to prove the pattern indeed holds true for ALL positive integer n ?","Consider this 5-Square Identity, $(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2)^2 (y_1^2+y_2^2+y_3^2+y_4^2+y_5^2) = z_1^2+z_2^2+z_3^2+z_4^2+z_5^2$ where, $\begin{align} z_1 &= (-x_1^2+x_2^2+x_3^2+x_4^2+x_5^2)y_1 - 2x_1(0x_1 y_1+x_2 y_2+x_3 y_3+x_4 y_4 + x_5 y_5)\\ z_2 &= (x_1^2-x_2^2+x_3^2+x_4^2+x_5^2)y_2 - 2x_2(x_1 y_1+0x_2 y_2+x_3 y_3+x_4 y_4 + x_5 y_5)\\ z_3 &= (x_1^2+x_2^2-x_3^2+x_4^2+x_5^2)y_3 - 2x_3(x_1 y_1+x_2 y_2+0x_3 y_3+x_4 y_4 + x_5 y_5)\\ z_4 &= (x_1^2+x_2^2+x_3^2-x_4^2+x_5^2)y_4 - 2x_4(x_1 y_1+x_2 y_2+x_3 y_3+0x_4 y_4 + x_5 y_5)\\ z_5 &= (x_1^2+x_2^2+x_3^2+x_4^2-x_5^2)y_5 - 2x_5(x_1 y_1+x_2 y_2+x_3 y_3+x_4 y_4 + 0x_5 y_5) \end{align}$ The pattern is easily seen for, $(x_1^2+x_2^2 + \dots + x_n^2)^2 (y_1^2+y_2^2 + \dots + y_n^2) = z_1^2+z_2^2 + \dots + z_n^2$ The case n = 4 is used in Pfister’s 8-square Identity .  How to prove the pattern indeed holds true for ALL positive integer n ?",,"['linear-algebra', 'abstract-algebra']"
6,"Automorphisms of $SO(n,\mathbb R)$",Automorphisms of,"SO(n,\mathbb R)","Sorry, I'm not a specialist, I want to ask about automorphisms of the group $SO(n,\mathbb{R})$ ($\mathbb{R}$ - field of reals). It is easy  that a function of the form $f_C(A)=CAC^{-1}$ for $A \in SO(n, \mathbb{R})$, where $C\in O(n,\mathbb{R})$, is an automorphism. But, is it true that each automorphism of $SO(n,\mathbb{R})$ is of the form $f_C$ with $C \in O(n,\mathbb{R})$ or maybe with $C \in SO(n,\mathbb{R})$? Thanks.","Sorry, I'm not a specialist, I want to ask about automorphisms of the group $SO(n,\mathbb{R})$ ($\mathbb{R}$ - field of reals). It is easy  that a function of the form $f_C(A)=CAC^{-1}$ for $A \in SO(n, \mathbb{R})$, where $C\in O(n,\mathbb{R})$, is an automorphism. But, is it true that each automorphism of $SO(n,\mathbb{R})$ is of the form $f_C$ with $C \in O(n,\mathbb{R})$ or maybe with $C \in SO(n,\mathbb{R})$? Thanks.",,"['linear-algebra', 'abstract-algebra', 'lie-groups']"
7,Elementary Row/Column Operations and Change of Basis,Elementary Row/Column Operations and Change of Basis,,"Let $V$ and $W$ be finite-dimensional vector spaces and let $T:V \rightarrow W$ be a linear transformation between them. I have read that Performing an elementary row operation on the matrix that represents $T$ is equivalent to performing a corresponding change of basis in the range of $T$, and Performing an elementary column operation on the matrix that represents $T$ is equivalent to performing a corresponding change of basis in the domain of $T$ Admittedly, this is a rather vague formulation but it's all I have. My question is: Can anyone either explain, or provide a reference to, a precise statement of the relationship between change of basis operations and elementary matrices as described above?","Let $V$ and $W$ be finite-dimensional vector spaces and let $T:V \rightarrow W$ be a linear transformation between them. I have read that Performing an elementary row operation on the matrix that represents $T$ is equivalent to performing a corresponding change of basis in the range of $T$, and Performing an elementary column operation on the matrix that represents $T$ is equivalent to performing a corresponding change of basis in the domain of $T$ Admittedly, this is a rather vague formulation but it's all I have. My question is: Can anyone either explain, or provide a reference to, a precise statement of the relationship between change of basis operations and elementary matrices as described above?",,['linear-algebra']
8,Morphism of Exterior Algebras,Morphism of Exterior Algebras,,"Let $k$ be a field, let $V$ and $W$ be $k$-vector spaces of dimensions $n$ and $m$ respectively, and let $f:V\to W$ be a $k$-linear transformation. Let $\Lambda(V)$ and $\Lambda(W)$ denote the exterior algebras of $V$ and $W$ respectively. So we have $$\Lambda(V) = \Lambda^0(V)\oplus\Lambda^1(V)\oplus\cdots\oplus\Lambda^n(V)$$ and $$\Lambda(W) = \Lambda^0(W)\oplus\Lambda^1(W)\oplus\cdots\oplus\Lambda^m(W).$$ The wikipedia page on exterior algebras states that there is a unique function $\Lambda(f):\Lambda(V)\to\Lambda(W)$ such that $\Lambda(f)|_{\Lambda^1(V)}:\Lambda^1(V)\to\Lambda^1(W)$ is defined by $\Lambda(f)(v)=f(v)$. In fact, $\Lambda(f)$ preserves grading (i.e. it can be written as a sum of maps $\Lambda^k(f):=\Lambda(f)|_{\Lambda^k(V)}:\Lambda^k(V)\to\Lambda^k(W)$). If $1\leq k \leq n$, then $\Lambda(f)$ is given by $$\Lambda^k(f)(v_1\wedge\cdots\wedge v_k) = f(v_1)\wedge\cdots\wedge f(v_k).$$ I do not understand how this function acts on $\Lambda^0(V)=k$. I know that we have a map $$\Lambda^0(f):\Lambda^0(V)\to\Lambda^0(W)$$ which is really the same as $$\Lambda^0(f):k\to k.$$ My question has two parts: what is $\Lambda^0(f)$ and how is it determined from the universal mapping property for exterior algebras?","Let $k$ be a field, let $V$ and $W$ be $k$-vector spaces of dimensions $n$ and $m$ respectively, and let $f:V\to W$ be a $k$-linear transformation. Let $\Lambda(V)$ and $\Lambda(W)$ denote the exterior algebras of $V$ and $W$ respectively. So we have $$\Lambda(V) = \Lambda^0(V)\oplus\Lambda^1(V)\oplus\cdots\oplus\Lambda^n(V)$$ and $$\Lambda(W) = \Lambda^0(W)\oplus\Lambda^1(W)\oplus\cdots\oplus\Lambda^m(W).$$ The wikipedia page on exterior algebras states that there is a unique function $\Lambda(f):\Lambda(V)\to\Lambda(W)$ such that $\Lambda(f)|_{\Lambda^1(V)}:\Lambda^1(V)\to\Lambda^1(W)$ is defined by $\Lambda(f)(v)=f(v)$. In fact, $\Lambda(f)$ preserves grading (i.e. it can be written as a sum of maps $\Lambda^k(f):=\Lambda(f)|_{\Lambda^k(V)}:\Lambda^k(V)\to\Lambda^k(W)$). If $1\leq k \leq n$, then $\Lambda(f)$ is given by $$\Lambda^k(f)(v_1\wedge\cdots\wedge v_k) = f(v_1)\wedge\cdots\wedge f(v_k).$$ I do not understand how this function acts on $\Lambda^0(V)=k$. I know that we have a map $$\Lambda^0(f):\Lambda^0(V)\to\Lambda^0(W)$$ which is really the same as $$\Lambda^0(f):k\to k.$$ My question has two parts: what is $\Lambda^0(f)$ and how is it determined from the universal mapping property for exterior algebras?",,"['linear-algebra', 'abstract-algebra', 'exterior-algebra']"
9,Matrices which determinant is obvious.,Matrices which determinant is obvious.,,"At least in my current Linear Algebra course, exercises concerning the determinant of a matrix of dimension $n\geq 5$ more often than not require one to notice some property of the matrix so that its determinant becomes obvious and one needs not compute it in the usual way. I'm trying to find a simple way to realize that the determinant of $$\begin{pmatrix} 0 & -1 & -1 & -1 & -1 \\ 1 & 0 & -1 & -1 & -1 \\ 1  & 1 & a & -1 & -1 \\ 1 & 1 & 1 & 0 & -1 \\ 1 & 1 & 1 & 1 & 0\end{pmatrix}$$ is equal to $a$ , and I would appreciate anyone who points me in the right direction. Also, since these are the types of questions that might go to my (as well as other students') final, I'm open to users giving other examples of matrices which determinants are obvious without the need to compute them, here are a few: $$\det \begin{pmatrix}  -4 & 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & -4 & 1 \\ 1  & 1 & -4 & 1 & 1 \\ 1 & -4 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 & -4\end{pmatrix} = \det\begin{pmatrix}  -4 & 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & -4 & 1 \\ 0  & 0 & 0 & 0 & 0 \\ 1 & -4 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 & -4 \end{pmatrix}=0 $$ by adding rows $1,2,4,5$ to row $3$ . $$\det \begin{pmatrix}  1 & 2 & 3 & 4 & 5 \\ 2 & 3 & 4 & 5 & 6 \\ 3  & 4 & 5 & 6 & 7 \\ 4 & 5 & 6 & 7 & 8 \\ 5 & 6 & 7 & 8 & 9\end{pmatrix} =\det \begin{pmatrix}  1 & 2 & 3 & 4 & 5 \\ 1 & 1 & 1 & 1 & 1 \\ 2  & 2 & 2 & 2 & 2 \\ 4 & 5 & 6 & 7 & 8 \\ 5 & 6 & 7 & 8 & 9\end{pmatrix}=0$$ by subtracting row $1$ from row $2$ & $3$ . $$\det \begin{pmatrix}  0 & -a & -b & -d & -g \\ a & 0 & -c & -e & -h \\ b  & c & 0 & -f & -i \\ d & e & f & 0 & -j \\ g & h & i & j & 0\end{pmatrix}=0$$ since $(-1)^5\det (A)=\det(A^T)=\det(A)$ . All of these can be generalized to other dimensions, with the exception of the last one for which the dimension of the matrix must be odd.","At least in my current Linear Algebra course, exercises concerning the determinant of a matrix of dimension more often than not require one to notice some property of the matrix so that its determinant becomes obvious and one needs not compute it in the usual way. I'm trying to find a simple way to realize that the determinant of is equal to , and I would appreciate anyone who points me in the right direction. Also, since these are the types of questions that might go to my (as well as other students') final, I'm open to users giving other examples of matrices which determinants are obvious without the need to compute them, here are a few: by adding rows to row . by subtracting row from row & . since . All of these can be generalized to other dimensions, with the exception of the last one for which the dimension of the matrix must be odd.","n\geq 5 \begin{pmatrix} 0 & -1 & -1 & -1 & -1
\\
1 & 0 & -1 & -1 & -1
\\
1  & 1 & a & -1 & -1
\\
1 & 1 & 1 & 0 & -1
\\
1 & 1 & 1 & 1 & 0\end{pmatrix} a \det \begin{pmatrix} 
-4 & 1 & 1 & 1 & 1
\\
1 & 1 & 1 & -4 & 1
\\
1  & 1 & -4 & 1 & 1
\\
1 & -4 & 1 & 1 & 1
\\
1 & 1 & 1 & 1 & -4\end{pmatrix}
=
\det\begin{pmatrix} 
-4 & 1 & 1 & 1 & 1
\\
1 & 1 & 1 & -4 & 1
\\
0  & 0 & 0 & 0 & 0
\\
1 & -4 & 1 & 1 & 1
\\
1 & 1 & 1 & 1 & -4
\end{pmatrix}=0
 1,2,4,5 3 \det \begin{pmatrix} 
1 & 2 & 3 & 4 & 5
\\
2 & 3 & 4 & 5 & 6
\\
3  & 4 & 5 & 6 & 7
\\
4 & 5 & 6 & 7 & 8
\\
5 & 6 & 7 & 8 & 9\end{pmatrix}
=\det \begin{pmatrix} 
1 & 2 & 3 & 4 & 5
\\
1 & 1 & 1 & 1 & 1
\\
2  & 2 & 2 & 2 & 2
\\
4 & 5 & 6 & 7 & 8
\\
5 & 6 & 7 & 8 & 9\end{pmatrix}=0 1 2 3 \det \begin{pmatrix} 
0 & -a & -b & -d & -g
\\
a & 0 & -c & -e & -h
\\
b  & c & 0 & -f & -i
\\
d & e & f & 0 & -j
\\
g & h & i & j & 0\end{pmatrix}=0 (-1)^5\det (A)=\det(A^T)=\det(A)","['linear-algebra', 'matrices', 'linear-transformations', 'determinant', 'big-list']"
10,A linear algebra problem regarding $AB-BA=A$,A linear algebra problem regarding,AB-BA=A,"Let $A$ be a $n \times n$ complex matrix. Show (1) is equivalent to (2) (1) There exists $B$ such that $AB-BA = A$ (2) $A^n=0$ Furthermore, prove $B^n\neq 0$ in (1) if $A \neq 0$ . Attmept for (1) $\implies$ (2) $A^2 = A^2B - ABA = ABA-BA^2$ . Therefore $A^2 = \frac12 (A^2B - BA^2)$ . Inductively $A^n = \frac 1 n (A^nB- BA^n)$ . Take submultiplicative norms, we obtain $\| A^n \|\leq \frac 2n \|A^n \| \|B\| $ . But here I don't know how to preceed.","Let be a complex matrix. Show (1) is equivalent to (2) (1) There exists such that (2) Furthermore, prove in (1) if . Attmept for (1) (2) . Therefore . Inductively . Take submultiplicative norms, we obtain . But here I don't know how to preceed.",A n \times n B AB-BA = A A^n=0 B^n\neq 0 A \neq 0 \implies A^2 = A^2B - ABA = ABA-BA^2 A^2 = \frac12 (A^2B - BA^2) A^n = \frac 1 n (A^nB- BA^n) \| A^n \|\leq \frac 2n \|A^n \| \|B\| ,"['linear-algebra', 'matrices', 'matrix-equations']"
11,"If $f: \mathbb{C} \to \mathbb{C}$ is analytic and not linear, then $z, f(z), f(f(z)), \dots$ are linearly independent functions over $\mathbb{C}.$","If  is analytic and not linear, then  are linearly independent functions over","f: \mathbb{C} \to \mathbb{C} z, f(z), f(f(z)), \dots \mathbb{C}.","I wish to prove that if $f: \mathbb{C} \to \mathbb{C}$ is analytic and not of the form $az+b,$ then $z, f(z), f(f(z)), \dots$ are linearly independent functions over $\mathbb{C}.$ The cases $n=0, 1$ are trivial. To solve this problem, I'm starting on the first non-trivial scenario: assume $c_1 z + c_2 f(z) + c_3 f(f(z)) = 0$ for some $c_1, c_2, c_3.$ If I can solve this, I'll probably know how to solve the general case. But I'm getting nowhere. The equations $z-f(z)-f(f(z))+f^3(z)=g(z)-g(g(z)) = 0$ for $f(z) = z+1, g(z) = |z|$ suggest we should impose analycity and non-linearity. Any hints or ideas? Edit: I've obtained a major breakthrough, but one step is still missing. Define a function to be $n$ -independent if $z, f(z), f(f(z)), \dots, f^{(n)}(z)$ are linearly independent over $\mathbb{C}$ on some non-empty open subset of $\mathbb{C}.$ Unproven Lemma: The set of $n$ -dependent functions forms a vector space under addition. $f$ being $n$ -dependent easily implies $cf$ is $n$ -dependent, so additivity is the only thing that stands in the way of proving this lemma. We need to figure out some way to handle terms like $(f+g) \circ (f+g) = f(f(z)+g(z))+g(f(z)+g(z))$ without messing up the rest of the terms. First note that $h(z)=az+b$ is $2$ -dependent since $z, h(z), h(h(z))$ are $3$ vectors in the $2$ dimensional vector space of polynomials with degree $\le 1.$ Suppose $f$ is analytic and non-linear. Let $n$ be minimal such that $f$ is $n$ -dependent. Let $g(z) = f(z)-(f(1)-f(0))z-f(0).$ Since $f$ isn't linear, $n \ge 2,$ implying $g$ is $n$ -dependepnt. Suppose $c_0 z + c_1 g(z) + \dots + c_n g^n(z) = 0.$ Setting $z=1,$ we get $c_0 = 0,$ so $c_1 z + \dots + c_n g^{n-1}(z) = 0$ on $\mathcal{O} = g(\mathbb{C}),$ which is open by the open mapping theorem since $g$ is analytic and non-constant. Thus, $g$ is $n-1$ dependent. If $n \ge 3,$ this means $f$ is $n-1$ dependent, contradiction. If $n=2,$ then $g$ is $1$ -dependent, so $g(z)=cz$ for some $c,$ which means $f$ is linear, contradiction.","I wish to prove that if is analytic and not of the form then are linearly independent functions over The cases are trivial. To solve this problem, I'm starting on the first non-trivial scenario: assume for some If I can solve this, I'll probably know how to solve the general case. But I'm getting nowhere. The equations for suggest we should impose analycity and non-linearity. Any hints or ideas? Edit: I've obtained a major breakthrough, but one step is still missing. Define a function to be -independent if are linearly independent over on some non-empty open subset of Unproven Lemma: The set of -dependent functions forms a vector space under addition. being -dependent easily implies is -dependent, so additivity is the only thing that stands in the way of proving this lemma. We need to figure out some way to handle terms like without messing up the rest of the terms. First note that is -dependent since are vectors in the dimensional vector space of polynomials with degree Suppose is analytic and non-linear. Let be minimal such that is -dependent. Let Since isn't linear, implying is -dependepnt. Suppose Setting we get so on which is open by the open mapping theorem since is analytic and non-constant. Thus, is dependent. If this means is dependent, contradiction. If then is -dependent, so for some which means is linear, contradiction.","f: \mathbb{C} \to \mathbb{C} az+b, z, f(z), f(f(z)), \dots \mathbb{C}. n=0, 1 c_1 z + c_2 f(z) + c_3 f(f(z)) = 0 c_1, c_2, c_3. z-f(z)-f(f(z))+f^3(z)=g(z)-g(g(z)) = 0 f(z) = z+1, g(z) = |z| n z, f(z), f(f(z)), \dots, f^{(n)}(z) \mathbb{C} \mathbb{C}. n f n cf n (f+g) \circ (f+g) = f(f(z)+g(z))+g(f(z)+g(z)) h(z)=az+b 2 z, h(z), h(h(z)) 3 2 \le 1. f n f n g(z) = f(z)-(f(1)-f(0))z-f(0). f n \ge 2, g n c_0 z + c_1 g(z) + \dots + c_n g^n(z) = 0. z=1, c_0 = 0, c_1 z + \dots + c_n g^{n-1}(z) = 0 \mathcal{O} = g(\mathbb{C}), g g n-1 n \ge 3, f n-1 n=2, g 1 g(z)=cz c, f","['linear-algebra', 'complex-analysis']"
12,"If $|A|, |B|$ are sufficiently small, does $e^{A+B} = e^A e^B$ imply that $AB = BA$?","If  are sufficiently small, does  imply that ?","|A|, |B| e^{A+B} = e^A e^B AB = BA","For  complex matrices $A, B$ of the same size, there are simple counterexamples to the question, ""Does $e^{A+B} = e^A e^B$ imply that $AB = BA$ ?''; but they do not involve matrices that have very small (Euclidean) norm. See for example, one of the responses to this question , in which $e^{A+B} = e^A = e^B = I$ , where $I$ is the identity matrix, but $A$ and $B$ do not commute. If there are counterexamples with $|A|, |B|$ being arbitrarily small then that settles the question.  In the alternative, here is the small progress that I have made, in case the following line of reasoning can be continued to a solution. First, by ""very small'', I mean that the Campbell-Baker-Hausdorff (CBH) series expansion is valid for both $\log(e^A e^B)$ and $\log(e^B e^A)$ . The CBH series expansion for $\log(e^X e^Y)$ is $e^X e^Y = \exp( \sum_{m=1}^{\infty} F_m(X, Y) )$ , where $F_m(X, Y)$ is a homogeneous polynomial of degree $m$ in the (generally, non-commuting) variables $X, Y$ (complex matrices of equal size, $n$ , say);  and $F_m(X, Y)$ contains neither the term $X^m$ nor the term $Y^m$ , except for $m=1$ : $F_1(X, Y) = X + Y$ . Each $F_m(X, Y)$ is moreover a linear combination of nested Lie brackets of $X$ and $Y$ . Also, we have the elementary property, $F_m(Y, X) = (-1)^{m+1} F_m(X, Y)$ . Next, it suffices to show that $e^{A+B} = e^A e^B$ implies that $e^{A+B} = e^B e^A$ , for then $e^B e^A = e^B e^A$ , which does imply that $AB = BA$ , under the assumption that $|A|, |B|$ are sufficiently small. Indeed, setting $X = e^A$ and $Y = e^B$ , so that $A = \log X$ and $B = \log Y$ , then $AB = BA$ because $\qquad A = (X−I) − \frac{1}{2}(X−I)^2 + \frac{1}{3}(X−I)^3 − \cdots$ , $\qquad B = (Y−I) − \frac{1}{2}(Y−I)^2 + \frac{1}{3}(Y−I)^3 − \cdots$ , and the series commute since $X$ and $Y$ do. The assumed equality, $e^A e^B = e^{A+B}$ , implies that $\sum_{m=2}^{\infty} F_m(A, B) = 0$ ; but $A$ and $B$ are specific matrices, so we can't (immediately) conclude that each $F_m(A, B) = 0$ , for $m \geq 2$ .  Using the property, $F_m(B, A) = (-1)^{m+1} F_m(A, B)$ , we have $$  e^B e^A = \exp\left( \sum_{m=1}^{\infty} (-1)^{m+1} F_m(A, B) \right) = \exp\left( X + Y + \sum_{m=2}^{\infty} (-1)^{m+1} F_m(A, B) \right). $$ Based only on the equality, $\sum_{m=2}^{\infty} F_m(A, B) = 0$ , I don't see why $\sum_{m=2}^{\infty} (-1)^{m+1} F_m(A, B)$ should equal $0$ . Another approach might be to consider $e^A e^B e^{-A} e^{-B}$ .","For  complex matrices of the same size, there are simple counterexamples to the question, ""Does imply that ?''; but they do not involve matrices that have very small (Euclidean) norm. See for example, one of the responses to this question , in which , where is the identity matrix, but and do not commute. If there are counterexamples with being arbitrarily small then that settles the question.  In the alternative, here is the small progress that I have made, in case the following line of reasoning can be continued to a solution. First, by ""very small'', I mean that the Campbell-Baker-Hausdorff (CBH) series expansion is valid for both and . The CBH series expansion for is , where is a homogeneous polynomial of degree in the (generally, non-commuting) variables (complex matrices of equal size, , say);  and contains neither the term nor the term , except for : . Each is moreover a linear combination of nested Lie brackets of and . Also, we have the elementary property, . Next, it suffices to show that implies that , for then , which does imply that , under the assumption that are sufficiently small. Indeed, setting and , so that and , then because , , and the series commute since and do. The assumed equality, , implies that ; but and are specific matrices, so we can't (immediately) conclude that each , for .  Using the property, , we have Based only on the equality, , I don't see why should equal . Another approach might be to consider .","A, B e^{A+B} = e^A e^B AB = BA e^{A+B} = e^A = e^B = I I A B |A|, |B| \log(e^A e^B) \log(e^B e^A) \log(e^X e^Y) e^X e^Y = \exp( \sum_{m=1}^{\infty} F_m(X, Y) ) F_m(X, Y) m X, Y n F_m(X, Y) X^m Y^m m=1 F_1(X, Y) = X + Y F_m(X, Y) X Y F_m(Y, X) = (-1)^{m+1} F_m(X, Y) e^{A+B} = e^A e^B e^{A+B} = e^B e^A e^B e^A = e^B e^A AB = BA |A|, |B| X = e^A Y = e^B A = \log X B = \log Y AB = BA \qquad A = (X−I) − \frac{1}{2}(X−I)^2 + \frac{1}{3}(X−I)^3 − \cdots \qquad B = (Y−I) − \frac{1}{2}(Y−I)^2 + \frac{1}{3}(Y−I)^3 − \cdots X Y e^A e^B = e^{A+B} \sum_{m=2}^{\infty} F_m(A, B) = 0 A B F_m(A, B) = 0 m \geq 2 F_m(B, A) = (-1)^{m+1} F_m(A, B) 
 e^B e^A = \exp\left( \sum_{m=1}^{\infty} (-1)^{m+1} F_m(A, B) \right) = \exp\left( X + Y + \sum_{m=2}^{\infty} (-1)^{m+1} F_m(A, B) \right).
 \sum_{m=2}^{\infty} F_m(A, B) = 0 \sum_{m=2}^{\infty} (-1)^{m+1} F_m(A, B) 0 e^A e^B e^{-A} e^{-B}","['linear-algebra', 'exponential-function', 'lie-groups']"
13,Proof of the CS (cosine-sine) matrix decomposition,Proof of the CS (cosine-sine) matrix decomposition,,"The CS decomposition is a way to write the singular value decomposition of a matrix with orthonormal columns. More specifically, taking the notation from these notes (pdf alert), consider a $(n_1+n_2)\times p$ matrix $Q$ , with $$Q=\begin{bmatrix}Q_1 \\ Q_2\end{bmatrix},$$ where $Q_1$ has dimensions $n_1\times p$ and $Q_2$ has dimensions $n_2\times p$ . Assume $Q$ has orthonormal columns, that is, $Q_1^\dagger Q_1+Q_2^\dagger Q_2=I$ . Then the CS decomposition essentially tells us that the SVDs of $Q_1$ and $Q_2$ are related. More specifically, there are unitaries $V, U_1, U_2$ such that \begin{aligned} U_1^\dagger Q_1 V=\operatorname{diag}(c_1,...,c_p), \\ U_2^\dagger Q_2 V=\operatorname{diag}(s_1,...,s_q), \end{aligned} with $c_i^2+s_i^2=1$ (from which the name of the decomposition comes). As far as I understand, this means that there is a set of orthonormal vectors $\{v_k\}_k$ such that both $\{Q_1 v_k\}_k$ and $\{Q_2 v_k\}$ are orthogonal sets of vectors (with some relations between their norms). To prove that this is the case, I start by writing down the SVDs of $Q_1$ and $Q_2$ , which tell us that there are unitaries $U_1, U_2, V_1, V_2$ , and diagonal positive matrices $D_1, D_2$ , such that \begin{aligned} Q_1= U_1 D_1 V_1^\dagger, \\ Q_2= U_2 D_2 V_2^\dagger. \end{aligned} The condition $Q_1^\dagger Q_1+Q_2^\dagger Q_2=I$ then translates into $$V_1 D_1^2 V_1^\dagger + V_2 D_2^2 V_2^\dagger=I.$$ Denoting with $v^{(i)}_k$ the $k$ -th column of $V_i$ , and $P^{(i)}_k\equiv v^{(i)}_k v^{(i)*}_k$ the associated projector, this condition can be seen to be equivalent to $$\sum_k (d^{(1)}_k)^2 P_k^{(1)}+\sum_k (d^{(2)}_k)^2 P_k^{(2)}=I,\tag A$$ where $d^{(i)}_k\equiv (D_i)_{kk}$ . Now, however, I'm a bit stuck into how to proceed from (A). It seems a generalisation of the things proved in this post and links therein, which show that if a sum of projectors gives the identity then the projectors must be orthogonal, but I'm not sure how to prove this in this case.","The CS decomposition is a way to write the singular value decomposition of a matrix with orthonormal columns. More specifically, taking the notation from these notes (pdf alert), consider a matrix , with where has dimensions and has dimensions . Assume has orthonormal columns, that is, . Then the CS decomposition essentially tells us that the SVDs of and are related. More specifically, there are unitaries such that with (from which the name of the decomposition comes). As far as I understand, this means that there is a set of orthonormal vectors such that both and are orthogonal sets of vectors (with some relations between their norms). To prove that this is the case, I start by writing down the SVDs of and , which tell us that there are unitaries , and diagonal positive matrices , such that The condition then translates into Denoting with the -th column of , and the associated projector, this condition can be seen to be equivalent to where . Now, however, I'm a bit stuck into how to proceed from (A). It seems a generalisation of the things proved in this post and links therein, which show that if a sum of projectors gives the identity then the projectors must be orthogonal, but I'm not sure how to prove this in this case.","(n_1+n_2)\times p Q Q=\begin{bmatrix}Q_1 \\ Q_2\end{bmatrix}, Q_1 n_1\times p Q_2 n_2\times p Q Q_1^\dagger Q_1+Q_2^\dagger Q_2=I Q_1 Q_2 V, U_1, U_2 \begin{aligned}
U_1^\dagger Q_1 V=\operatorname{diag}(c_1,...,c_p), \\
U_2^\dagger Q_2 V=\operatorname{diag}(s_1,...,s_q),
\end{aligned} c_i^2+s_i^2=1 \{v_k\}_k \{Q_1 v_k\}_k \{Q_2 v_k\} Q_1 Q_2 U_1, U_2, V_1, V_2 D_1, D_2 \begin{aligned}
Q_1= U_1 D_1 V_1^\dagger, \\
Q_2= U_2 D_2 V_2^\dagger.
\end{aligned} Q_1^\dagger Q_1+Q_2^\dagger Q_2=I V_1 D_1^2 V_1^\dagger + V_2 D_2^2 V_2^\dagger=I. v^{(i)}_k k V_i P^{(i)}_k\equiv v^{(i)}_k v^{(i)*}_k \sum_k (d^{(1)}_k)^2 P_k^{(1)}+\sum_k (d^{(2)}_k)^2 P_k^{(2)}=I,\tag A d^{(i)}_k\equiv (D_i)_{kk}","['linear-algebra', 'matrices', 'matrix-decomposition', 'projection-matrices']"
14,Eigenvalues versus algebraic closedness,Eigenvalues versus algebraic closedness,,"Suppose $k$ is a field, and every square matrix over $k$ has an eigenvalue (in $k$ ). Does it follow that $k$ is algebraically closed? (Context: see here . It follows from the result proved there that it's enough to show that every polynomial is a factor of the characteriestic polynomial of some matrix.)","Suppose is a field, and every square matrix over has an eigenvalue (in ). Does it follow that is algebraically closed? (Context: see here . It follows from the result proved there that it's enough to show that every polynomial is a factor of the characteriestic polynomial of some matrix.)",k k k k,"['linear-algebra', 'abstract-algebra']"
15,Are matrices which yield a given characteristic polynomial and have specified structure connected?,Are matrices which yield a given characteristic polynomial and have specified structure connected?,,"Let us consider a subset $S$ of $M_4(\mathbb R)$ which has following form \begin{align*} \begin{pmatrix} 0 & * & 0 & * \\ 1 & * & 0 & * \\ 0 & * & 0 & * \\ 0 & * & 1 & * \end{pmatrix}, \end{align*} where $*$ can assume any real number. It is also clear for any monic $4^{th}$ degree real polynomial, we can at least find one realization in $S$ since the upper left block and lower right block can be considered as in companion form. Let $f: S \to \mathbb R^n$ be the map sending the coefficients of characteristic polynomial to $\mathbb R^n$. My question is: suppose we have a square-free polynomial $p(t) = t^4 + a_3 t^3 + a_2 t^2 + a_1 t + a_0$, is $f^{-1}(a)$ a connected set in $M_n(\mathbb R)$ where $a=(a_3, a_2, a_1, a_0)$? If we let $C$ denote the companion form of $p(t)$, which is an element of $S$, then $$f^{-1}(a) = \{V C V^{-1}: V \in GL_4(\mathbb R), V C V^{-1} \text{ is in above form }\}.$$ I have a feeling we can first choose a realization in block form, \begin{align*} \begin{pmatrix} 0 & * & 0 & 0 \\ 1 & * & 0 & 0 \\ 0 & 0 & 0 & * \\ 0 & 0 & 1 & * \end{pmatrix}, \end{align*} and then continuously change everything in $f^{-1}(a)$ into this form without changing the characteristic polynomial. Indeed, if we consider a matrix \begin{align*} A = \begin{pmatrix} 0 & -b_1 &  0 & -c_1 \\ 1 & -b_2 &  0 & -c_2 \\ 0 & -b_3 & 0 & -c_3 \\ 0 & -b_4 & 1  & -c_4 \\ \end{pmatrix}, \end{align*} then the charateristic polynomial is \begin{align*} t^4 + (c_4+b_2)t^3 + (c_3 + b_1 + b_2c_4 - b_4 c_3)t^2 + (b_1 c_4 - b_3 c_3 + b_2 c_3 - b_4 c_1) t + (b_1 c_3 - b_3 c_1). \end{align*} I am thinking we should be able to continuously decrease $b_3, b_4, c_1, c_2$ to $0$ while keeping the polynomial unchanged by changing $b_1, b_2, c_3, c_4$ accordingly.","Let us consider a subset $S$ of $M_4(\mathbb R)$ which has following form \begin{align*} \begin{pmatrix} 0 & * & 0 & * \\ 1 & * & 0 & * \\ 0 & * & 0 & * \\ 0 & * & 1 & * \end{pmatrix}, \end{align*} where $*$ can assume any real number. It is also clear for any monic $4^{th}$ degree real polynomial, we can at least find one realization in $S$ since the upper left block and lower right block can be considered as in companion form. Let $f: S \to \mathbb R^n$ be the map sending the coefficients of characteristic polynomial to $\mathbb R^n$. My question is: suppose we have a square-free polynomial $p(t) = t^4 + a_3 t^3 + a_2 t^2 + a_1 t + a_0$, is $f^{-1}(a)$ a connected set in $M_n(\mathbb R)$ where $a=(a_3, a_2, a_1, a_0)$? If we let $C$ denote the companion form of $p(t)$, which is an element of $S$, then $$f^{-1}(a) = \{V C V^{-1}: V \in GL_4(\mathbb R), V C V^{-1} \text{ is in above form }\}.$$ I have a feeling we can first choose a realization in block form, \begin{align*} \begin{pmatrix} 0 & * & 0 & 0 \\ 1 & * & 0 & 0 \\ 0 & 0 & 0 & * \\ 0 & 0 & 1 & * \end{pmatrix}, \end{align*} and then continuously change everything in $f^{-1}(a)$ into this form without changing the characteristic polynomial. Indeed, if we consider a matrix \begin{align*} A = \begin{pmatrix} 0 & -b_1 &  0 & -c_1 \\ 1 & -b_2 &  0 & -c_2 \\ 0 & -b_3 & 0 & -c_3 \\ 0 & -b_4 & 1  & -c_4 \\ \end{pmatrix}, \end{align*} then the charateristic polynomial is \begin{align*} t^4 + (c_4+b_2)t^3 + (c_3 + b_1 + b_2c_4 - b_4 c_3)t^2 + (b_1 c_4 - b_3 c_3 + b_2 c_3 - b_4 c_1) t + (b_1 c_3 - b_3 c_1). \end{align*} I am thinking we should be able to continuously decrease $b_3, b_4, c_1, c_2$ to $0$ while keeping the polynomial unchanged by changing $b_1, b_2, c_3, c_4$ accordingly.",,"['linear-algebra', 'general-topology', 'matrices', 'polynomials', 'connectedness']"
16,Prove that $GL_n(\mathbb{F}_p)$ contains an element of order $p^n-1$,Prove that  contains an element of order,GL_n(\mathbb{F}_p) p^n-1,"If $p$ is prime and $n$ is a positive integer, prove that GL$_n(\mathbb{F}_p)$ contains an element of order $p^n-1$. I am a bit stuck on how to get started on this problem. I found it in the Galois theory section on a list of past qualifying exams, so I'm guessing some kind of Galois theory should be involved, but I can't seem to get anywhere with it. I know that $\mathbb{F}_{p^n}$ and $\mathbb{F}_p^n$ are isomorphic as $\mathbb{F}_p$ vector spaces, so $Gal(\mathbb{F}_{p^n}/\mathbb{F}_p)$ is isomorphic to a cyclic subgroup of GL$_n(\mathbb{F}_p)$ of order $n$, though this doesn't seem too useful (but maybe I'm wrong!). My only other idea at this point is to try to find some $\mathbb{F}_p$-linear transformation $T$ of $\mathbb{F}_{p^n}$ with characteristic polynomial $x^{p^n-1}-1$ and then use Cayley-Hamilton to get that $T$ has order dividing $p^n-1$, and hopefully the definition of $T$ (whatever it may be) will show it's order can't be lower than $p^n-1$. Can anyone see a more clever way to attack this problem?","If $p$ is prime and $n$ is a positive integer, prove that GL$_n(\mathbb{F}_p)$ contains an element of order $p^n-1$. I am a bit stuck on how to get started on this problem. I found it in the Galois theory section on a list of past qualifying exams, so I'm guessing some kind of Galois theory should be involved, but I can't seem to get anywhere with it. I know that $\mathbb{F}_{p^n}$ and $\mathbb{F}_p^n$ are isomorphic as $\mathbb{F}_p$ vector spaces, so $Gal(\mathbb{F}_{p^n}/\mathbb{F}_p)$ is isomorphic to a cyclic subgroup of GL$_n(\mathbb{F}_p)$ of order $n$, though this doesn't seem too useful (but maybe I'm wrong!). My only other idea at this point is to try to find some $\mathbb{F}_p$-linear transformation $T$ of $\mathbb{F}_{p^n}$ with characteristic polynomial $x^{p^n-1}-1$ and then use Cayley-Hamilton to get that $T$ has order dividing $p^n-1$, and hopefully the definition of $T$ (whatever it may be) will show it's order can't be lower than $p^n-1$. Can anyone see a more clever way to attack this problem?",,"['linear-algebra', 'group-theory', 'galois-theory', 'finite-fields']"
17,What is the largest possible number of moves that can be taken to color the whole grid?,What is the largest possible number of moves that can be taken to color the whole grid?,,"Consider a $10\times 10$ grid. On every move, we color $4$ unit squares that lie in the intersection of some two rows and two columns. A move is allowed if at least one of the $4$ squares is previously uncolored. What is the largest possible number of moves that can be taken to color the whole grid? Attempt: It is easy to get $81$ moves: By always choosing the first line, the first column and a square of the remaining $9\times 9$ grid as the lower right square, the whole grid can be colored in $81$ moves. But is this a maximum number of moves?","Consider a grid. On every move, we color unit squares that lie in the intersection of some two rows and two columns. A move is allowed if at least one of the squares is previously uncolored. What is the largest possible number of moves that can be taken to color the whole grid? Attempt: It is easy to get moves: By always choosing the first line, the first column and a square of the remaining grid as the lower right square, the whole grid can be colored in moves. But is this a maximum number of moves?",10\times 10 4 4 81 9\times 9 81,"['linear-algebra', 'combinatorics', 'algorithms', 'contest-math']"
18,Convergence of a sequence of eigenvectors (nonnegative matrix),Convergence of a sequence of eigenvectors (nonnegative matrix),,"Let $A$ be a $n\times n$ matrix with coefficients in $ [0,1] $ . Let $ B $ be the matrix filled up only with the value $ \frac{1}{2} $ : $$B = \begin{pmatrix} \frac{1}{2} & \dots  & \frac{1}{2} \\  \vdots & \ddots  & \vdots \\  \frac{1}{2} & \dots  & \frac{1}{2}  \end{pmatrix}\,.$$ For all $ t \in ]0,1] $ let $ A(t) = tB + (1-t)A $ . The matrix $ A(t) $ is primitive for any fixed $ t $ . Hence, from the Perron-Frobenius theorem, we have that $ \rho(t) $ (i.e. the spectral radius of the matrix $ A(t) $ ) is a simple eigenvalue, with the relative eigenvector that can be taken positive (i.e. every component is strictly positive). Let me call it $ x(t) $ , choosing it such that $ \|x(t)\|_1 = \rho(t) $ (i.e. the sum of all the components is equal to the spectral radius of the matrix). In this way, I obtain the following properties: \begin{matrix} A(t)x(t) = \rho(t)x(t) \\ \|x(t)\|_1 = \rho(t)\\  x(t) > 0 \end{matrix} My question is: does there exist $ \lim_{t \to 0^+}x(t) $ ? I have read that spectral radius is continuous with respect to any matrix norm, and then I would have: $ \lim_{t \to 0^+}A(t) = A \Rightarrow \lim_{t \to 0^+}\rho(t) = \rho(0) $ . Is it correct? Anyway, passing to the sequences with $ t = \frac{1}{n} $ , I did not succeed in proving that the generated sequence $ x_n = x(\frac{1}{n}) $ is a Cauchy sequence (this fact would imply that $ x_n $ is convergent, because of the sequentially compactness of $ [0,1]^n $ ). I think that the better way in order to prove the existence of the limit above is to prove the monotonicity of the components of $ x_n $ using the monotonicity of the coefficients $ a_{ij}(t) $ of $ A(t) $ . It is only an idea but I do not know if it works. I really thank you in advance.","Let be a matrix with coefficients in . Let be the matrix filled up only with the value : For all let . The matrix is primitive for any fixed . Hence, from the Perron-Frobenius theorem, we have that (i.e. the spectral radius of the matrix ) is a simple eigenvalue, with the relative eigenvector that can be taken positive (i.e. every component is strictly positive). Let me call it , choosing it such that (i.e. the sum of all the components is equal to the spectral radius of the matrix). In this way, I obtain the following properties: My question is: does there exist ? I have read that spectral radius is continuous with respect to any matrix norm, and then I would have: . Is it correct? Anyway, passing to the sequences with , I did not succeed in proving that the generated sequence is a Cauchy sequence (this fact would imply that is convergent, because of the sequentially compactness of ). I think that the better way in order to prove the existence of the limit above is to prove the monotonicity of the components of using the monotonicity of the coefficients of . It is only an idea but I do not know if it works. I really thank you in advance.","A n\times n  [0,1]   B   \frac{1}{2}  B = \begin{pmatrix}
\frac{1}{2} & \dots  & \frac{1}{2} \\ 
\vdots & \ddots  & \vdots \\ 
\frac{1}{2} & \dots  & \frac{1}{2} 
\end{pmatrix}\,.  t \in ]0,1]   A(t) = tB + (1-t)A   A(t)   t   \rho(t)   A(t)   x(t)   \|x(t)\|_1 = \rho(t)  \begin{matrix}
A(t)x(t) = \rho(t)x(t) \\
\|x(t)\|_1 = \rho(t)\\ 
x(t) > 0
\end{matrix}  \lim_{t \to 0^+}x(t)   \lim_{t \to 0^+}A(t) = A \Rightarrow \lim_{t \to 0^+}\rho(t) = \rho(0)   t = \frac{1}{n}   x_n = x(\frac{1}{n})   x_n   [0,1]^n   x_n   a_{ij}(t)   A(t) ","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
19,"What if the basis not countable, then what?","What if the basis not countable, then what?",,"I'm second year physics & mathematics student, and self-studying Abstract linear algebra from the book Linear Algebra by Werner Greub. In the mean time I have come across several times to the notion of countable basis.I know what can or can not do if a some set is countable/uncountable, but while studying linear algebra I do not exactly know what I couldn't do if the basis of space is not countable ? I generally worked either on abstract (finite / infinite) spaces or finite abstract spaces, and while in the abstract case, we have never assumed that the basis is countable, so I'm not sure what I would lose if I have a space of infinite dimension whose basis is uncountable. tl;dr If our vector space has a basis that is not countable, then which properties would be lost or gained compared to the case where we have space having countable basis (finite or infinite).","I'm second year physics & mathematics student, and self-studying Abstract linear algebra from the book Linear Algebra by Werner Greub. In the mean time I have come across several times to the notion of countable basis.I know what can or can not do if a some set is countable/uncountable, but while studying linear algebra I do not exactly know what I couldn't do if the basis of space is not countable ? I generally worked either on abstract (finite / infinite) spaces or finite abstract spaces, and while in the abstract case, we have never assumed that the basis is countable, so I'm not sure what I would lose if I have a space of infinite dimension whose basis is uncountable. tl;dr If our vector space has a basis that is not countable, then which properties would be lost or gained compared to the case where we have space having countable basis (finite or infinite).",,"['linear-algebra', 'hamel-basis']"
20,Help understand an inequality in a proof,Help understand an inequality in a proof,,"Assume standard inner product and 2-norm. $A$ is any symmetric real-valued $n\times n$ square matrix. $V_k$ is an $n\times k$ matrix whose columns are orthonormal. $T_k$ is a matrix such that it has the following relation with $A$ and $V_k$: $AV_k=V_kT_k+{\hat v}_{k+1}e_k^T$ where ${\hat v}_{k+1}$ is another vector orthogonal to every column of $V_k$, and $e_k^T = (0,...,0,1)$ is a vector with only the $k$th component being 1. I need to understand the following proof, but got stuck at an inequality. This is a proof about the error bound of Lanczos iteration. Above proof uses In order to apply Cauchy-Schwarz, which is $<x,y>\le \|x\|\|y\|$, we need to place absolute value operator on both the LHS and RHS of (10.26), but I have trouble understanding how the LHS of (10.26) becomes the LHS of (10.27). Many thanks! PS: the whole thing for those who are interested. the theorem in question is the last one.","Assume standard inner product and 2-norm. $A$ is any symmetric real-valued $n\times n$ square matrix. $V_k$ is an $n\times k$ matrix whose columns are orthonormal. $T_k$ is a matrix such that it has the following relation with $A$ and $V_k$: $AV_k=V_kT_k+{\hat v}_{k+1}e_k^T$ where ${\hat v}_{k+1}$ is another vector orthogonal to every column of $V_k$, and $e_k^T = (0,...,0,1)$ is a vector with only the $k$th component being 1. I need to understand the following proof, but got stuck at an inequality. This is a proof about the error bound of Lanczos iteration. Above proof uses In order to apply Cauchy-Schwarz, which is $<x,y>\le \|x\|\|y\|$, we need to place absolute value operator on both the LHS and RHS of (10.26), but I have trouble understanding how the LHS of (10.26) becomes the LHS of (10.27). Many thanks! PS: the whole thing for those who are interested. the theorem in question is the last one.",,"['linear-algebra', 'matrices', 'numerical-methods']"
21,Show that $|A|$ is an integer multiple of 125,Show that  is an integer multiple of 125,|A|,"Let A=$(a_{ij})\in M_{4\times4}(\mathbb Q)$ be a matrix each entry of which is either -2 or 3. Show that $|A|$ is an integer multiple of 125. Would it be best to look at the $2\times 2$, minor determinants, or is there some better way?","Let A=$(a_{ij})\in M_{4\times4}(\mathbb Q)$ be a matrix each entry of which is either -2 or 3. Show that $|A|$ is an integer multiple of 125. Would it be best to look at the $2\times 2$, minor determinants, or is there some better way?",,['linear-algebra']
22,Solve the vector cross product equation,Solve the vector cross product equation,,"Consider the vector equation $a \times x = b$ in $\Bbb R^3$ , where $a\ne0$. Show that $$x = \frac{b\times a}{|a|^2} + ka$$ is the solution to the equation for any scalar $k$ i really dont know where to start , i really appreciate any help","Consider the vector equation $a \times x = b$ in $\Bbb R^3$ , where $a\ne0$. Show that $$x = \frac{b\times a}{|a|^2} + ka$$ is the solution to the equation for any scalar $k$ i really dont know where to start , i really appreciate any help",,"['linear-algebra', 'vector-spaces', 'cross-product']"
23,Prove that matrix $A$ diagonalizable if $A^2=I$ using characteristic polynomial,Prove that matrix  diagonalizable if  using characteristic polynomial,A A^2=I,Prove that the matrix $A$ is diagonalizable if $A^2=I$ using characteristic   polynomial I saw an answer that used the minimal polynomial of $A$. Can that be proven without using minimal polynomial? In my university we didn't learn minimal polynomial and therefore we cannot use it in our tasks and exams. We didn't get this particular question for homework but I just wanted to know if I can use this claim in my tasks and exams.,Prove that the matrix $A$ is diagonalizable if $A^2=I$ using characteristic   polynomial I saw an answer that used the minimal polynomial of $A$. Can that be proven without using minimal polynomial? In my university we didn't learn minimal polynomial and therefore we cannot use it in our tasks and exams. We didn't get this particular question for homework but I just wanted to know if I can use this claim in my tasks and exams.,,"['linear-algebra', 'matrices', 'diagonalization']"
24,Change in eigenvalues by changing only one entry of a square matrix,Change in eigenvalues by changing only one entry of a square matrix,,"Consider following square matrix $A$ of order $n$ $A=\begin{bmatrix} 0 & a_{12} & a_{13} & a_{14} & \cdots  & a_{1n}  \\  a_{21} & 0 & a_{23} & a_{24} & \cdots & a_{2n} \\  a_{31} & a_{32} & 0 & a_{34} & \cdots & a_{3n}\\ a_{41} & a_{42} & a_{43} & 0 & \cdots & a_{4n}\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & a_{n3} & a_{n4} & \cdots & 0\\ \end{bmatrix}_n$ Let $A$ is symmetric matrix with eigenvalues $\lambda_i,i=1,2\cdots,n$. Now consider two matrices $B$ and $C$ with eigenvalues $\mu_i$ and $\gamma_i,i=1,2,\cdots,n$,which are obtained from matrix $A$ by replacing $a_{11}$ entry to $1$ and $-1$ respectively i.e $B=\begin{bmatrix} 1 & a_{12} & a_{13} & a_{14} & \cdots  & a_{1n}  \\  a_{21} & 0 & a_{23} & a_{24} & \cdots & a_{2n} \\  a_{31} & a_{32} & 0 & a_{34} & \cdots & a_{3n}\\ a_{41} & a_{42} & a_{43} & 0 & \cdots & a_{4n}\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & a_{n3} & a_{n4} & \cdots & 0\\ \end{bmatrix}_n$ $C=\begin{bmatrix} -1 & a_{12} & a_{13} & a_{14} & \cdots  & a_{1n}  \\  a_{21} & 0 & a_{23} & a_{24} & \cdots & a_{2n} \\  a_{31} & a_{32} & 0 & a_{34} & \cdots & a_{3n}\\ a_{41} & a_{42} & a_{43} & 0 & \cdots & a_{4n}\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & a_{n3} & a_{n4} & \cdots & 0\\ \end{bmatrix}_n$ My questions are $1.$Can we find any relation between $\lambda_i$ and $\mu_i$? $2.$Can we find any relation between  $\lambda_i$ and $\gamma_i$? $3.$Can we find any $\sum_{i=1}^{n} \left|\mu_i\right|$ in terms of $\lambda_i$? $4.$Can we find any $\sum_{i=1}^{n} \left|\gamma_i\right|$ in terms of $\lambda_i$?","Consider following square matrix $A$ of order $n$ $A=\begin{bmatrix} 0 & a_{12} & a_{13} & a_{14} & \cdots  & a_{1n}  \\  a_{21} & 0 & a_{23} & a_{24} & \cdots & a_{2n} \\  a_{31} & a_{32} & 0 & a_{34} & \cdots & a_{3n}\\ a_{41} & a_{42} & a_{43} & 0 & \cdots & a_{4n}\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & a_{n3} & a_{n4} & \cdots & 0\\ \end{bmatrix}_n$ Let $A$ is symmetric matrix with eigenvalues $\lambda_i,i=1,2\cdots,n$. Now consider two matrices $B$ and $C$ with eigenvalues $\mu_i$ and $\gamma_i,i=1,2,\cdots,n$,which are obtained from matrix $A$ by replacing $a_{11}$ entry to $1$ and $-1$ respectively i.e $B=\begin{bmatrix} 1 & a_{12} & a_{13} & a_{14} & \cdots  & a_{1n}  \\  a_{21} & 0 & a_{23} & a_{24} & \cdots & a_{2n} \\  a_{31} & a_{32} & 0 & a_{34} & \cdots & a_{3n}\\ a_{41} & a_{42} & a_{43} & 0 & \cdots & a_{4n}\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & a_{n3} & a_{n4} & \cdots & 0\\ \end{bmatrix}_n$ $C=\begin{bmatrix} -1 & a_{12} & a_{13} & a_{14} & \cdots  & a_{1n}  \\  a_{21} & 0 & a_{23} & a_{24} & \cdots & a_{2n} \\  a_{31} & a_{32} & 0 & a_{34} & \cdots & a_{3n}\\ a_{41} & a_{42} & a_{43} & 0 & \cdots & a_{4n}\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & a_{n3} & a_{n4} & \cdots & 0\\ \end{bmatrix}_n$ My questions are $1.$Can we find any relation between $\lambda_i$ and $\mu_i$? $2.$Can we find any relation between  $\lambda_i$ and $\gamma_i$? $3.$Can we find any $\sum_{i=1}^{n} \left|\mu_i\right|$ in terms of $\lambda_i$? $4.$Can we find any $\sum_{i=1}^{n} \left|\gamma_i\right|$ in terms of $\lambda_i$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
25,Possible method to prove infinite twin prime conjecture,Possible method to prove infinite twin prime conjecture,,"I have an idea looking more and more promising that may lead to proving the infinite twin prime conjecture. My idea would set up a correspondence between primes and twin prime pairs. Since primes have been proven infinite, twin primes would be shown infinite as well. Here it is: For every prime $p>7$ there exists at least one unique twin prime pair $(p_t,p_t+2)$ created using only primes less than $p$ as follows: $$(p_t,p_t+2)=(3\times5\times P_p\times p-4,\ \ 3\times 5\times p\times P_p-2)$$ or $$(p_t,p_t+2)=(3\times5\times P_p\times p+2,\ \ 3\times5\times p\times P_p+4)$$ where $P_p$ is some product of individual primes ($p_n$) and their powers ( although recent developments indicate powers may be unnecessary! ) such that each fits the following condition: $$5<p_n<p$$ Here's a few examples: $(3\times5\times43-4,\ \ 3\times5\times43-2)=(641,643)$ $(3\times5\times7^2\times11\times47+2,\ \ 3\times5\times7^2\times11\times47+4)=(379997,379999)$ My request is for one of the following: Someone to refine our program for a brute force testing method trying to find a counter-example to disprove my conjecture. Here's the code: Original Wolfram Notebook NEW AND IMPROVED Wolfram Notebook Someone to develop a proof of my conjecture; perhaps something related to the fact that the multitude of combinations/permutations etc. of primes ($5<p_p<p$) and their powers requires that there be at least one twin prime pair created. Perhaps a proof by contradiction? I.e. $p$ exists such that no twin prime is created is proven absurd, thus each $p$ maps to a unique twin prime, and as primes are infinite, so are twin primes? Need some help here! Maybe someone with rep to spare set a bounty? EDIT (2/15/16) Thanks to @dbanet, I now have code needing some refinements. Still, what's astonishing is that we've checked the first $10,000$ primes and each has its own unique twin prime pair... and it didn't even require powers of primes; everything is to the 1st power! This fact alone should lend high credence to the conjecture that each prime may be mapped to (at least one) unique twin prime pair. I'm considering perhaps removing powers of primes from the original question. Here's the list up to 109 for verification. You can check each by adding $4$ or subtracting $2$ from the first in the pair and looking at the prime factors. All will include $3$, $5$ , $p$, and primes between $5$ and $p$ all to the $1$st power (prime#, prime, twin prime): 4, 7, {101},{103} 5, 11, {1151},{1153} 6, 13, {191},{193} 7, 17, {4337},{4339} 8, 19, {281},{283} 9, 23, {347},{349} 10, 29, {431},{433} 11, 31, {461},{463} 12, 37, {17207},{17209} 13, 41, {617},{619} 14, 43, {641},{643} 15, 47, {1225997},{1225999} 16, 53, {37361},{37363} 17, 59, {881},{883} 18, 61, {55817},{55819} 19, 67, {3616997},{3616999} 20, 71, {1061},{1063} 21, 73, {1091},{1093} 22, 79, {6141857},{6141859} 23, 83, {5922461},{5922463} 24, 89, {546625097},{546625099} 25, 97, {1451},{1453} 26, 101, {134837},{134839} 27, 103, {13888001},{13888003} 28, 107, {1607},{1609} 29, 109, {16969661},{16969663} EDIT (2/15/16)PM Got a new list of twin primes because of https://mathematica.stackexchange.com/questions/107417/memory-limit-hit-optimize-code-for-finding-twin-primes . Here's the list of primes $2000-10000$ with the corresponding twin prime pairs! https://dl.dropboxusercontent.com/u/76769933/8000%20twin%20primes.txt And just for kicks here's the $100,000,000$th prime with the first found (probably not only) twin prime unique to it: $2038074743$ -- $(126984732620985857058143952617,$ $126984732620985857058143952619)$","I have an idea looking more and more promising that may lead to proving the infinite twin prime conjecture. My idea would set up a correspondence between primes and twin prime pairs. Since primes have been proven infinite, twin primes would be shown infinite as well. Here it is: For every prime $p>7$ there exists at least one unique twin prime pair $(p_t,p_t+2)$ created using only primes less than $p$ as follows: $$(p_t,p_t+2)=(3\times5\times P_p\times p-4,\ \ 3\times 5\times p\times P_p-2)$$ or $$(p_t,p_t+2)=(3\times5\times P_p\times p+2,\ \ 3\times5\times p\times P_p+4)$$ where $P_p$ is some product of individual primes ($p_n$) and their powers ( although recent developments indicate powers may be unnecessary! ) such that each fits the following condition: $$5<p_n<p$$ Here's a few examples: $(3\times5\times43-4,\ \ 3\times5\times43-2)=(641,643)$ $(3\times5\times7^2\times11\times47+2,\ \ 3\times5\times7^2\times11\times47+4)=(379997,379999)$ My request is for one of the following: Someone to refine our program for a brute force testing method trying to find a counter-example to disprove my conjecture. Here's the code: Original Wolfram Notebook NEW AND IMPROVED Wolfram Notebook Someone to develop a proof of my conjecture; perhaps something related to the fact that the multitude of combinations/permutations etc. of primes ($5<p_p<p$) and their powers requires that there be at least one twin prime pair created. Perhaps a proof by contradiction? I.e. $p$ exists such that no twin prime is created is proven absurd, thus each $p$ maps to a unique twin prime, and as primes are infinite, so are twin primes? Need some help here! Maybe someone with rep to spare set a bounty? EDIT (2/15/16) Thanks to @dbanet, I now have code needing some refinements. Still, what's astonishing is that we've checked the first $10,000$ primes and each has its own unique twin prime pair... and it didn't even require powers of primes; everything is to the 1st power! This fact alone should lend high credence to the conjecture that each prime may be mapped to (at least one) unique twin prime pair. I'm considering perhaps removing powers of primes from the original question. Here's the list up to 109 for verification. You can check each by adding $4$ or subtracting $2$ from the first in the pair and looking at the prime factors. All will include $3$, $5$ , $p$, and primes between $5$ and $p$ all to the $1$st power (prime#, prime, twin prime): 4, 7, {101},{103} 5, 11, {1151},{1153} 6, 13, {191},{193} 7, 17, {4337},{4339} 8, 19, {281},{283} 9, 23, {347},{349} 10, 29, {431},{433} 11, 31, {461},{463} 12, 37, {17207},{17209} 13, 41, {617},{619} 14, 43, {641},{643} 15, 47, {1225997},{1225999} 16, 53, {37361},{37363} 17, 59, {881},{883} 18, 61, {55817},{55819} 19, 67, {3616997},{3616999} 20, 71, {1061},{1063} 21, 73, {1091},{1093} 22, 79, {6141857},{6141859} 23, 83, {5922461},{5922463} 24, 89, {546625097},{546625099} 25, 97, {1451},{1453} 26, 101, {134837},{134839} 27, 103, {13888001},{13888003} 28, 107, {1607},{1609} 29, 109, {16969661},{16969663} EDIT (2/15/16)PM Got a new list of twin primes because of https://mathematica.stackexchange.com/questions/107417/memory-limit-hit-optimize-code-for-finding-twin-primes . Here's the list of primes $2000-10000$ with the corresponding twin prime pairs! https://dl.dropboxusercontent.com/u/76769933/8000%20twin%20primes.txt And just for kicks here's the $100,000,000$th prime with the first found (probably not only) twin prime unique to it: $2038074743$ -- $(126984732620985857058143952617,$ $126984732620985857058143952619)$",,"['linear-algebra', 'proof-verification', 'prime-numbers']"
26,Hamel basis for $\mathbb R$ over the field $\mathbb Q$,Hamel basis for  over the field,\mathbb R \mathbb Q,"A set $S\subseteq\mathbb R$ is said to be linearly independent if for distinct $x_1,\ldots,x_k$ ($k\in\mathbb N$) and for integers $n_1,\ldots,n_k$, $$n_1x_1+\ldots+ n_kx_k=0$$ implies that $$n_1=\ldots=n_k=0.$$ It is not difficult to see that this definition is equivalent to the one in which $n_1,\ldots,n_k$ are allowed to be rationals. By Zorn’s lemma, there exists a maximal such linearly independent $S$, which is a Hamel basis for $\mathbb R$ over $\mathbb Q$. Now, Problem 14.7 in Billingsley’s Probability and Measure (1995) claims that [E]ach real $x$ can be written uniquely as $x=n_1x_1+\cdots+n_kx_k$ for distinct points $x_i$ in $S$ and integers $n_i$. [emphasis added] I think this is not right: (1) In the above definition of linear independence, integers and rationals are interchangeable, but... (2) ...in the Hamel-basis representation, rationals must be allowed. Integer coordinates are not enough to represent all of $\mathbb R$ for any maximal linear independent $S$. I wonder if someone could confirm this is a typo. Thank you. Here is a proof sketch for why integers are not sufficient. Let $x\in S$. Now, if Billingsley’s claim were true, it would be the case that $$\frac{x}{2}=\sum_{i=1}^kn_ix_i$$ for distinct $x_i\in S$ and integers $n_i$. But then $$x-2n_1x_1-\ldots-2n_kx_k=0.$$ Because of linear independence, $x$ must coincide with some other $x_i$. Since the $x_i$’s are distinct, there must be precisely one such $x_i$. Then, by matching the coefficients, it must be the case that $$1-2n_i=0,$$ or $n_i=1/2$, which is not an integer.","A set $S\subseteq\mathbb R$ is said to be linearly independent if for distinct $x_1,\ldots,x_k$ ($k\in\mathbb N$) and for integers $n_1,\ldots,n_k$, $$n_1x_1+\ldots+ n_kx_k=0$$ implies that $$n_1=\ldots=n_k=0.$$ It is not difficult to see that this definition is equivalent to the one in which $n_1,\ldots,n_k$ are allowed to be rationals. By Zorn’s lemma, there exists a maximal such linearly independent $S$, which is a Hamel basis for $\mathbb R$ over $\mathbb Q$. Now, Problem 14.7 in Billingsley’s Probability and Measure (1995) claims that [E]ach real $x$ can be written uniquely as $x=n_1x_1+\cdots+n_kx_k$ for distinct points $x_i$ in $S$ and integers $n_i$. [emphasis added] I think this is not right: (1) In the above definition of linear independence, integers and rationals are interchangeable, but... (2) ...in the Hamel-basis representation, rationals must be allowed. Integer coordinates are not enough to represent all of $\mathbb R$ for any maximal linear independent $S$. I wonder if someone could confirm this is a typo. Thank you. Here is a proof sketch for why integers are not sufficient. Let $x\in S$. Now, if Billingsley’s claim were true, it would be the case that $$\frac{x}{2}=\sum_{i=1}^kn_ix_i$$ for distinct $x_i\in S$ and integers $n_i$. But then $$x-2n_1x_1-\ldots-2n_kx_k=0.$$ Because of linear independence, $x$ must coincide with some other $x_i$. Since the $x_i$’s are distinct, there must be precisely one such $x_i$. Then, by matching the coefficients, it must be the case that $$1-2n_i=0,$$ or $n_i=1/2$, which is not an integer.",,"['linear-algebra', 'abstract-algebra', 'number-theory']"
27,Prove $AB=I$ implies $BA=I$ using Fitting's Lemma,Prove  implies  using Fitting's Lemma,AB=I BA=I,"I know this question has already been asked, but I need a proof that for $A,B \in M_n(K)$, $$AB=I_n \Rightarrow BA=I_n$$ using Fitting's lemma . I thought of using the fact that $K^n$ is a simple $M_n(K)$-module, but I don't see what endomorphism of $K^n$ I can use Fitting's lemma on, since multiplying by $BA$ is not $M_n(K)$-linear.","I know this question has already been asked, but I need a proof that for $A,B \in M_n(K)$, $$AB=I_n \Rightarrow BA=I_n$$ using Fitting's lemma . I thought of using the fact that $K^n$ is a simple $M_n(K)$-module, but I don't see what endomorphism of $K^n$ I can use Fitting's lemma on, since multiplying by $BA$ is not $M_n(K)$-linear.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces', 'modules']"
28,Singularity of a conic combination of rank-$1$ matrices,Singularity of a conic combination of rank- matrices,1,"Given rank- $1$ square matrices $A_1, A_2, \dots, A_n$ , determine if there exists $x \in \mathbb R_{>0}^n$ such that $$ \sum_{i=1}^n x_i A_i = x_1 A_1 + \cdots + x_n A_n $$ is singular, or decide whether none exists (i.e., the positive linear combination is nonsingular for all positive $x$ ). Is there a well-known method or algorithm to do this?","Given rank- square matrices , determine if there exists such that is singular, or decide whether none exists (i.e., the positive linear combination is nonsingular for all positive ). Is there a well-known method or algorithm to do this?","1 A_1, A_2, \dots, A_n x \in \mathbb R_{>0}^n  \sum_{i=1}^n x_i A_i = x_1 A_1 + \cdots + x_n A_n  x","['linear-algebra', 'matrices', 'decision-problems', 'rank-1-matrices']"
29,"zeros of $x^*Ax$, a quadratic form","zeros of , a quadratic form",x^*Ax,"The question hopefully says it all! We have a Hermitian matrix $A=A^* \in \mathbb{C}^n$ and a quadratic form: $f(x)=x^*Ax,~x\in \mathbb{C}^n$ We want to find the solution of $f(x) = x^*Ax = 0$ When the matrix is positive semi definite (p.s.d.), the solution seems to be null-space of the matrix of $A$. This I found by diagonalising $A$. But suddenly I became helpless when $A$ has negative Eigen values too! Please note that when the matrix is not p.s.d., the solution space contains the null space. (I.e. null space is always a solution)","The question hopefully says it all! We have a Hermitian matrix $A=A^* \in \mathbb{C}^n$ and a quadratic form: $f(x)=x^*Ax,~x\in \mathbb{C}^n$ We want to find the solution of $f(x) = x^*Ax = 0$ When the matrix is positive semi definite (p.s.d.), the solution seems to be null-space of the matrix of $A$. This I found by diagonalising $A$. But suddenly I became helpless when $A$ has negative Eigen values too! Please note that when the matrix is not p.s.d., the solution space contains the null space. (I.e. null space is always a solution)",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'quadratic-forms', 'matrix-decomposition']"
30,Singular value decomposition of column & row vectors,Singular value decomposition of column & row vectors,,"(a) Given the column vector $a$ as an $n\times 1$ matrix. Write out its singular value decomposition, showing the matrices $U, \sum$, and $V$ explicitly. (b) Given the row vector $a^T$ as an $1\times n$ matrix. Write out its singular value decomposition, showing the matrices $U, \sum$, and $V$ explicitly. I currently get stucked on this question, since I couldn't find the eigenvalues of the $n\times n$ matrix $aa^T$ in part (a), and $a^Ta$ in part (b) to find the columns of $U$ and $V$, respectively. Can someone please help me on this problem?","(a) Given the column vector $a$ as an $n\times 1$ matrix. Write out its singular value decomposition, showing the matrices $U, \sum$, and $V$ explicitly. (b) Given the row vector $a^T$ as an $1\times n$ matrix. Write out its singular value decomposition, showing the matrices $U, \sum$, and $V$ explicitly. I currently get stucked on this question, since I couldn't find the eigenvalues of the $n\times n$ matrix $aa^T$ in part (a), and $a^Ta$ in part (b) to find the columns of $U$ and $V$, respectively. Can someone please help me on this problem?",,"['linear-algebra', 'numerical-methods']"
31,How to find this determinant of the binomial coefficient $\det{(A)}$,How to find this determinant of the binomial coefficient,\det{(A)},"Let matrix   $$A=\begin{bmatrix} \binom{m}{k}&\binom{m}{k+1}&\cdots&\binom{m}{k+n-1}\\ \binom{m+1}{k}&\binom{m+1}{k+1}&\cdots&\binom{m+1}{k+n-1}\\ \cdots&\cdots&\cdots&\cdots\\ \binom{m+n-1}{k}&\binom{m+n-1}{k+1}&\cdots&\binom{m+n-1}{k+n-1} \end{bmatrix}$$   Find $\det{(A)}$ I know maybe use this well-known identity: $$\binom{m+i}{j-1}+\binom{m+i}{j}=\binom{m+i+1}{j}\Longrightarrow \binom{m+i+1}{j}-\binom{m+i}{j}=\binom{m+i}{j-1} $$ $$A\to\begin{bmatrix} \binom{m}{k}&\binom{m}{k+1}&\cdots&\binom{m}{k+n-1}\\ \binom{m}{k-1}&\binom{m}{k}&\cdots&\binom{m}{k+n-2}\\ \cdots&\cdots&\cdots&\cdots\\ \binom{m+n-2}{k-1}&\binom{m+n-2}{k}&\cdots&\binom{m+n-2}{k+n-2} \end{bmatrix}$$ I have used this identity many times(because if $k=0$, I can turn int $A$ into a diagonal matrix, and easily find this value $\det{(A)}=1$). But I can't turn into this matrix $A$ diagonal matrix. PS: this problem is from   book problem excise.see Linear algebra and matrix theory","Let matrix   $$A=\begin{bmatrix} \binom{m}{k}&\binom{m}{k+1}&\cdots&\binom{m}{k+n-1}\\ \binom{m+1}{k}&\binom{m+1}{k+1}&\cdots&\binom{m+1}{k+n-1}\\ \cdots&\cdots&\cdots&\cdots\\ \binom{m+n-1}{k}&\binom{m+n-1}{k+1}&\cdots&\binom{m+n-1}{k+n-1} \end{bmatrix}$$   Find $\det{(A)}$ I know maybe use this well-known identity: $$\binom{m+i}{j-1}+\binom{m+i}{j}=\binom{m+i+1}{j}\Longrightarrow \binom{m+i+1}{j}-\binom{m+i}{j}=\binom{m+i}{j-1} $$ $$A\to\begin{bmatrix} \binom{m}{k}&\binom{m}{k+1}&\cdots&\binom{m}{k+n-1}\\ \binom{m}{k-1}&\binom{m}{k}&\cdots&\binom{m}{k+n-2}\\ \cdots&\cdots&\cdots&\cdots\\ \binom{m+n-2}{k-1}&\binom{m+n-2}{k}&\cdots&\binom{m+n-2}{k+n-2} \end{bmatrix}$$ I have used this identity many times(because if $k=0$, I can turn int $A$ into a diagonal matrix, and easily find this value $\det{(A)}=1$). But I can't turn into this matrix $A$ diagonal matrix. PS: this problem is from   book problem excise.see Linear algebra and matrix theory",,"['linear-algebra', 'determinant']"
32,the rank of a matrix and its inverse are always equal,the rank of a matrix and its inverse are always equal,,"I had a true or false quiz in a linear algebra course, one of the statements read the rank of a matrix and its inverse are always equal I answered true but the professor said it is false , he said that not all matrices have an inverse, but, I tough that since the statement says ""and its inverse"" it had an inverse since the statement say it; so my question would be, is the statement fine by being false or is it an error in the thinking of the solution of my professor, thanks","I had a true or false quiz in a linear algebra course, one of the statements read the rank of a matrix and its inverse are always equal I answered true but the professor said it is false , he said that not all matrices have an inverse, but, I tough that since the statement says ""and its inverse"" it had an inverse since the statement say it; so my question would be, is the statement fine by being false or is it an error in the thinking of the solution of my professor, thanks",,"['linear-algebra', 'matrices', 'inverse']"
33,Difference between coordinate space and vector space,Difference between coordinate space and vector space,,"I am new to linear algebra and was going through this link, and it made me wonder: Is there a technical difference between coordinate space (e.g., $R^n$) and vector space? If not, then why are there 2 terms for the same concept?","I am new to linear algebra and was going through this link, and it made me wonder: Is there a technical difference between coordinate space (e.g., $R^n$) and vector space? If not, then why are there 2 terms for the same concept?",,"['linear-algebra', 'vector-spaces']"
34,Cholesky for non-positive definite matrices,Cholesky for non-positive definite matrices,,"I am trying to approximate a NPD matrix with the nearest PD form and compute its Cholesky decomposition. I know that the usual method is to perform an eigenvalue decomposition, zero out the negative eigenvalues, and recompute the eigenvalue decomposition. However, I don't want to do any eigenvalue decompositions, and I want to deal with this issue within the Cholesky algorithm. For those of you who are familiar with the Cholesky algorithm, there is a step where you compute the diagonal of the lower diagonal matrix as the square root of a value (let's say x). If x<0 then, this means the matrix is NPD. A simple way to deal with this could be to set x = 0 or x = 10^-10, just to work around this problem. However, it lacks the mathematical rigor. My question is, has any of you heard such a method, or something similar to the above method, which is either published or somewhat proven?","I am trying to approximate a NPD matrix with the nearest PD form and compute its Cholesky decomposition. I know that the usual method is to perform an eigenvalue decomposition, zero out the negative eigenvalues, and recompute the eigenvalue decomposition. However, I don't want to do any eigenvalue decompositions, and I want to deal with this issue within the Cholesky algorithm. For those of you who are familiar with the Cholesky algorithm, there is a step where you compute the diagonal of the lower diagonal matrix as the square root of a value (let's say x). If x<0 then, this means the matrix is NPD. A simple way to deal with this could be to set x = 0 or x = 10^-10, just to work around this problem. However, it lacks the mathematical rigor. My question is, has any of you heard such a method, or something similar to the above method, which is either published or somewhat proven?",,"['linear-algebra', 'matrices', 'reference-request', 'diagonalization', 'cholesky-decomposition']"
35,The path of complex structure.,The path of complex structure.,,"$J = \left( {\begin{array}{*{20}{c}} 0&{ - {I_n}}\\{{I_n}}&0\end{array}} \right)$. If $I(t)$ is a path in $\rm M(2n, \mathbb R)$ ($2n \times 2n $ real matrix) such that $I(0)=0$ and $I(t)J+JI(t)=0$, then for sufficiently small $t$ can we find a path $J(t) \in \rm M(2n, \mathbb R)$ such that (1) $J(0)=J$, (2) $J^2(t)+I^2(t)=-I_{2n}$, (3) $I(t)J(t)+J(t)I(t)=0$?","$J = \left( {\begin{array}{*{20}{c}} 0&{ - {I_n}}\\{{I_n}}&0\end{array}} \right)$. If $I(t)$ is a path in $\rm M(2n, \mathbb R)$ ($2n \times 2n $ real matrix) such that $I(0)=0$ and $I(t)J+JI(t)=0$, then for sufficiently small $t$ can we find a path $J(t) \in \rm M(2n, \mathbb R)$ such that (1) $J(0)=J$, (2) $J^2(t)+I^2(t)=-I_{2n}$, (3) $I(t)J(t)+J(t)I(t)=0$?",,['linear-algebra']
36,Construction of Free Vector Spaces,Construction of Free Vector Spaces,,"I've been studying the construction of Free Vector Spaces and I want to confirm if my conclusions are correct. Given a set $A$ we wish to construct a vector space $F(A)$ which intuitively is the vector space whose elements are linear combination of $A$ (in other words, we think of $F(A)$ as the vector space that has $A$ as it's base). Well, the problem here is that $A$ is just a set, without additional operations defined on it's elements. So what we do is to consider indicator functions $\delta_a : A \to \left\{0,1\right\}$ such that $\delta_a(x) = 1$ if $x = a$ and $\delta_a(x) = 0$ if not. Then we consider the set $F(A)$ of all functions with finite support on $A$. Well, this seems very natural: the indicator functions themselves are in $F(A)$ since they have finite support. In this case, $\delta_a\in F(A)$ is the ""representative"" of $a \in A$ at $F(A)$. Now, for those functions we can define their sum and multiplication by scalar in the natural pointwise fashion. Now, each $\delta_a$ represents some element of $A$ and the linear combinations of those functions represents the linear combinations of the elements of $A$ that we wanted to define. Since all functions with finite support can be expressed as linear combination of those indicator functions, and since they are linearly independent, we can say that they indeed form a base for the space. And then we can check that the set $F(A)$ with the operations defined really is a vector space. With all of this, given a set $A$ we've succesfully constructed a vector space $F(A)$ that intuitively has all linear combinations from the elements of $A$. And since $\delta_a$ represents the element $a \in A$ we usually forget the delta, and just write $a \in F(A)$. Is all of this correct ? My understanding of the construction is indeed correct ? Thanks very much for your help.","I've been studying the construction of Free Vector Spaces and I want to confirm if my conclusions are correct. Given a set $A$ we wish to construct a vector space $F(A)$ which intuitively is the vector space whose elements are linear combination of $A$ (in other words, we think of $F(A)$ as the vector space that has $A$ as it's base). Well, the problem here is that $A$ is just a set, without additional operations defined on it's elements. So what we do is to consider indicator functions $\delta_a : A \to \left\{0,1\right\}$ such that $\delta_a(x) = 1$ if $x = a$ and $\delta_a(x) = 0$ if not. Then we consider the set $F(A)$ of all functions with finite support on $A$. Well, this seems very natural: the indicator functions themselves are in $F(A)$ since they have finite support. In this case, $\delta_a\in F(A)$ is the ""representative"" of $a \in A$ at $F(A)$. Now, for those functions we can define their sum and multiplication by scalar in the natural pointwise fashion. Now, each $\delta_a$ represents some element of $A$ and the linear combinations of those functions represents the linear combinations of the elements of $A$ that we wanted to define. Since all functions with finite support can be expressed as linear combination of those indicator functions, and since they are linearly independent, we can say that they indeed form a base for the space. And then we can check that the set $F(A)$ with the operations defined really is a vector space. With all of this, given a set $A$ we've succesfully constructed a vector space $F(A)$ that intuitively has all linear combinations from the elements of $A$. And since $\delta_a$ represents the element $a \in A$ we usually forget the delta, and just write $a \in F(A)$. Is all of this correct ? My understanding of the construction is indeed correct ? Thanks very much for your help.",,['linear-algebra']
37,about closed linear subspace,about closed linear subspace,,"Can you help me, plese, with the notion of closed linear subspace . What means, examples of closed linear subspace, how can I prove that a subspace is a closed linear subspace. Thanks :-)","Can you help me, plese, with the notion of closed linear subspace . What means, examples of closed linear subspace, how can I prove that a subspace is a closed linear subspace. Thanks :-)",,"['linear-algebra', 'reference-request', 'hilbert-spaces']"
38,From a vector to a skew symmetric matrix,From a vector to a skew symmetric matrix,,"Is there an existing linear mapping that maps a 3-dimensional vector: $$\mathbf{v}=\begin{pmatrix} v_1\\v_2\\v_3 \end{pmatrix}$$ to a corresponding skew-symmetric matrix: $$\mathbf{V}=\begin{pmatrix} 0 & -v_3 & v_2 \\ v_3 & 0 & -v_1 \\ -v_2 & v_1 & 0\end{pmatrix}$$  A tensor of order 3 should probably be defined. Edit The question is related to the following one: knowing that there exists a matrix $\mathbf{V}\in\mathbb{R}^{3,3}$ such that for a given vector $\mathbf{v}\in\mathbb{R}^3$: $$\forall\mathbf{x}\in\mathbb{R}^3,\quad\mathbf{V}\mathbf{x}=\mathbf{v}\times \mathbf{x}\quad\Leftrightarrow\quad \mathbf{V}=\mathrm{CPM}(\mathbf{v})$$ where CPM means cross-product matrix, can we express in a frame-invariant fashion the quantity: $$ \mathbf{V}_\mathrm{A}=\mathrm{CPM}(\mathbf{Av})$$ where $\mathbf{A}$ is any $3\times 3$ real matrix? (the result is $\mathbf{V}_\mathrm{A}=(\mathbf{VA})^T-\mathbf{VA}+\mathrm{tr}(\mathbf{A})\mathbf{V}$ but was obtained by calculating each coordinate of the left-hand and right-hand side matrices and subsequently identifying each term)","Is there an existing linear mapping that maps a 3-dimensional vector: $$\mathbf{v}=\begin{pmatrix} v_1\\v_2\\v_3 \end{pmatrix}$$ to a corresponding skew-symmetric matrix: $$\mathbf{V}=\begin{pmatrix} 0 & -v_3 & v_2 \\ v_3 & 0 & -v_1 \\ -v_2 & v_1 & 0\end{pmatrix}$$  A tensor of order 3 should probably be defined. Edit The question is related to the following one: knowing that there exists a matrix $\mathbf{V}\in\mathbb{R}^{3,3}$ such that for a given vector $\mathbf{v}\in\mathbb{R}^3$: $$\forall\mathbf{x}\in\mathbb{R}^3,\quad\mathbf{V}\mathbf{x}=\mathbf{v}\times \mathbf{x}\quad\Leftrightarrow\quad \mathbf{V}=\mathrm{CPM}(\mathbf{v})$$ where CPM means cross-product matrix, can we express in a frame-invariant fashion the quantity: $$ \mathbf{V}_\mathrm{A}=\mathrm{CPM}(\mathbf{Av})$$ where $\mathbf{A}$ is any $3\times 3$ real matrix? (the result is $\mathbf{V}_\mathrm{A}=(\mathbf{VA})^T-\mathbf{VA}+\mathrm{tr}(\mathbf{A})\mathbf{V}$ but was obtained by calculating each coordinate of the left-hand and right-hand side matrices and subsequently identifying each term)",,"['linear-algebra', 'matrices']"
39,proof of Zassenhaus formula for exponentials of linear operators,proof of Zassenhaus formula for exponentials of linear operators,,"The Zassenhaus formula is $$e^{t(X+Y)}= e^{tX}~  e^{tY} ~e^{-\frac{t^2}{2} [X,Y]} ~ e^{\frac{t^3}{6}(2[Y,[X,Y]]+ [X,[X,Y]] )} ~ e^{\frac{-t^4}{24}([[[X,Y],X],X] + 3[[[X,Y],X],Y] + 3[[[X,Y],Y],Y]) } \cdots$$ from this Wikipedia page . $X$ and $Y$ are linear operators, and $[X,Y]$ is their commutator. I mostly want to prove it for the case where the commutator of $X$ and $Y$ is a constant, or simply the general proof. What I'm looking for is either a reference to a place where it's proven or at least some shove in the right direction. So far I've tried to just expand the exponential to see if I see anything but have no ideas so far.","The Zassenhaus formula is $$e^{t(X+Y)}= e^{tX}~  e^{tY} ~e^{-\frac{t^2}{2} [X,Y]} ~ e^{\frac{t^3}{6}(2[Y,[X,Y]]+ [X,[X,Y]] )} ~ e^{\frac{-t^4}{24}([[[X,Y],X],X] + 3[[[X,Y],X],Y] + 3[[[X,Y],Y],Y]) } \cdots$$ from this Wikipedia page . $X$ and $Y$ are linear operators, and $[X,Y]$ is their commutator. I mostly want to prove it for the case where the commutator of $X$ and $Y$ is a constant, or simply the general proof. What I'm looking for is either a reference to a place where it's proven or at least some shove in the right direction. So far I've tried to just expand the exponential to see if I see anything but have no ideas so far.",,"['linear-algebra', 'noncommutative-algebra']"
40,Inner product space over $\mathbb{R}$,Inner product space over,\mathbb{R},"Definition of the problem I have to prove the following statement: Let $\left(E,\left\langle \cdot,\cdot\right\rangle \right)$ be an inner product space over $\mathbb{R}$. prove that for all $x,y\in E$ we have  $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)\left|\left\langle x,y\right\rangle \right|\leq\left\Vert x+y\right\Vert \cdot\left\Vert x\right\Vert \left\Vert y\right\Vert . $$ My efforts I tried two different ways to prove that, both unsuccessfull.. First: First, by squaring the whole inequality: $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\left|\left\langle x,y\right\rangle \right|^{2}\leq\left\Vert x+y\right\Vert ^{2}\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}. $$ We have from Cauchy-Schwarz that $$ \left|\left\langle x,y\right\rangle \right|\leq\left\Vert x\right\Vert \cdot\left\Vert y\right\Vert  $$ So we obtain  $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\left|\left\langle x,y\right\rangle \right|^{2}\leq\left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}=\left(\left\Vert x\right\Vert ^{2}+\left\Vert y\right\Vert ^{2}+2\left\Vert x\right\Vert \left\Vert y\right\Vert \right)\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}. $$ By Pythagorean theorem, we obtain $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\left|\left\langle x,y\right\rangle \right|^{2}\leq\left(\left\Vert x+y\right\Vert ^{2}+2\left\Vert x\right\Vert \left\Vert y\right\Vert \right)\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}. $$ We're almost there, except an extra term very annoying: $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\left|\left\langle x,y\right\rangle \right|^{2}\leq\left\Vert x+y\right\Vert ^{2}\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}+2\left\Vert x\right\Vert ^{3}\left\Vert y\right\Vert ^{3}. $$ Second I tried after to use only the Cauchy-Schwarz inequality, not squared: $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)\left|\left\langle x,y\right\rangle \right|\leq\left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)\cdot\left\Vert x\right\Vert \left\Vert y\right\Vert . $$ My question Could you give me a hint/idea on how to solve this problem? which Lemma/Theorem should I use? Thank you Franck","Definition of the problem I have to prove the following statement: Let $\left(E,\left\langle \cdot,\cdot\right\rangle \right)$ be an inner product space over $\mathbb{R}$. prove that for all $x,y\in E$ we have  $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)\left|\left\langle x,y\right\rangle \right|\leq\left\Vert x+y\right\Vert \cdot\left\Vert x\right\Vert \left\Vert y\right\Vert . $$ My efforts I tried two different ways to prove that, both unsuccessfull.. First: First, by squaring the whole inequality: $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\left|\left\langle x,y\right\rangle \right|^{2}\leq\left\Vert x+y\right\Vert ^{2}\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}. $$ We have from Cauchy-Schwarz that $$ \left|\left\langle x,y\right\rangle \right|\leq\left\Vert x\right\Vert \cdot\left\Vert y\right\Vert  $$ So we obtain  $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\left|\left\langle x,y\right\rangle \right|^{2}\leq\left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}=\left(\left\Vert x\right\Vert ^{2}+\left\Vert y\right\Vert ^{2}+2\left\Vert x\right\Vert \left\Vert y\right\Vert \right)\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}. $$ By Pythagorean theorem, we obtain $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\left|\left\langle x,y\right\rangle \right|^{2}\leq\left(\left\Vert x+y\right\Vert ^{2}+2\left\Vert x\right\Vert \left\Vert y\right\Vert \right)\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}. $$ We're almost there, except an extra term very annoying: $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)^{2}\left|\left\langle x,y\right\rangle \right|^{2}\leq\left\Vert x+y\right\Vert ^{2}\cdot\left\Vert x\right\Vert ^{2}\left\Vert y\right\Vert ^{2}+2\left\Vert x\right\Vert ^{3}\left\Vert y\right\Vert ^{3}. $$ Second I tried after to use only the Cauchy-Schwarz inequality, not squared: $$ \left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)\left|\left\langle x,y\right\rangle \right|\leq\left(\left\Vert x\right\Vert +\left\Vert y\right\Vert \right)\cdot\left\Vert x\right\Vert \left\Vert y\right\Vert . $$ My question Could you give me a hint/idea on how to solve this problem? which Lemma/Theorem should I use? Thank you Franck",,"['linear-algebra', 'normed-spaces', 'inner-products']"
41,Largest eigenvalue of a positive semi-definite matrix is less than or equal to sum of eigenvalues of its diagonal blocks,Largest eigenvalue of a positive semi-definite matrix is less than or equal to sum of eigenvalues of its diagonal blocks,,"This question is very similar to this one . Let $$B = \begin{bmatrix} B_{11} & B_{12} \\ B_{12}' & B_{22} \end{bmatrix}$$ be a positive semidefinite matrix, where block $B_{11}$ is $p \times p$ . Then $$\lambda_1(B) \le \lambda_1(B_{11}) + \lambda_1(B_{22})$$ where $\lambda_1$ is the largest eigenvalue of the matrix in the argument. How can I show it using extremal representation of the maximum eigenvalue of a symmetric matrix? $$ \lambda_1(B) = \max_{||x||=1} x'Bx \\ = \max_{||x||=1} [x_1' x_2']\begin{bmatrix} B_{11} & B_{12} \\ B_{12}' & B_{22}\end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \\ = \max_{||x||=1} [x_1' x_2']\begin{bmatrix} B_{11}x_1 + B_{12}x_2 \\ B_{12}'x_1 + B_{22}x_2\end{bmatrix} \\ = \max_{||x||=1} { x_1'B_{11}x_1 + x_1'B_{12}x_2 + x_2'B_{12}'x_1 + x_2'B_{22}x_2 } $$ If $x_1'B_{12}x_2 + x_2'B_{12}'x_1 $ can go to zero then I am done. But this is certainly not the case. Where am I making a mistake?","This question is very similar to this one . Let be a positive semidefinite matrix, where block is . Then where is the largest eigenvalue of the matrix in the argument. How can I show it using extremal representation of the maximum eigenvalue of a symmetric matrix? If can go to zero then I am done. But this is certainly not the case. Where am I making a mistake?","B = \begin{bmatrix} B_{11} & B_{12} \\ B_{12}' & B_{22} \end{bmatrix} B_{11} p \times p \lambda_1(B) \le \lambda_1(B_{11}) + \lambda_1(B_{22}) \lambda_1  \lambda_1(B) = \max_{||x||=1} x'Bx \\
= \max_{||x||=1} [x_1' x_2']\begin{bmatrix} B_{11} & B_{12} \\ B_{12}' & B_{22}\end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \\
= \max_{||x||=1} [x_1' x_2']\begin{bmatrix} B_{11}x_1 + B_{12}x_2 \\ B_{12}'x_1 + B_{22}x_2\end{bmatrix} \\
= \max_{||x||=1} { x_1'B_{11}x_1 + x_1'B_{12}x_2 + x_2'B_{12}'x_1 + x_2'B_{22}x_2 }
 x_1'B_{12}x_2 + x_2'B_{12}'x_1 ","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-semidefinite', 'block-matrices']"
42,eigenvalues of the subtraction of a PSD matrix and a rank 1 matrix,eigenvalues of the subtraction of a PSD matrix and a rank 1 matrix,,"Let $A$ be a positive semi-definite matrix, and let $C$ be a rank 1 matrix. Prove that $A-C$ has at most one negative eigenvalue. PS: It's easy to show that if $A$ and $C$ commute, then the statement is true, but most of the time they do not commute.","Let $A$ be a positive semi-definite matrix, and let $C$ be a rank 1 matrix. Prove that $A-C$ has at most one negative eigenvalue. PS: It's easy to show that if $A$ and $C$ commute, then the statement is true, but most of the time they do not commute.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
43,Show that the matrix $A$ with integer entries is injective on the reals to $\mathbb{R}^m$ iff it is injective on the integer lattice.,Show that the matrix  with integer entries is injective on the reals to  iff it is injective on the integer lattice.,A \mathbb{R}^m,"Show that the $m \times n$ matrix $A$ with integer entries is an injective linear map from $\mathbb{R}^n$ to $\mathbb{R}^m$ iff  it is injective as a linear map from $\mathbb{Z}^n$ to $\mathbb{Z}^m$. One direction is quite obvious, it is injective on the reals, the kernel is empty and intersecting that with $\mathbb{Z}^n$ is still empty so the map restricted to the lattice has trivial kernel and is therefore injective. In the other direction I can't seem to make progress. I have been trying to think about it in two different ways. The first, consider the columns of the matrix. For it to be injective as a map between lattices, these columns need to be linearly independent, i.e. they satisfy no non trivial relation ($\sum r_iv_i =0$ implies $r_i = 0$). However, the key is that they don't satisfy an non trivial relation over the integers, but this may not be the case over the reals; there may be some non-zero real numbers $r'_i$ that cause $\sum r_iv_i =0$ and thus the map is not injective as a map between real spaces. This is where we need to use the fact that entries in the column vectors are also integers. Clearly injectivity over integers implies it for the rationals because if we could find special rationals $r'_i$ that cause $\sum r_iv_i =0$ we could just clear denominators and derive a contradiction. The other way I've been trying to think about it is by proving the converse. Suppose the map wasn't injective we have to show that it can't be both composed of integer entries and injective as map between lattices. For it to be injective as a lattice map though would mean that the none of standard basis elements of $\mathbb{R}$ are in the kernel, and that no integer (or rational and just clear denominators) combination of them are in the kernel either, thus for something to be in the kernel it has to be linear combination of the basis vectors with at least one coefficient irrational. I think this proves it, but I feel unsatisfied, so I am not sure if something is wrong.","Show that the $m \times n$ matrix $A$ with integer entries is an injective linear map from $\mathbb{R}^n$ to $\mathbb{R}^m$ iff  it is injective as a linear map from $\mathbb{Z}^n$ to $\mathbb{Z}^m$. One direction is quite obvious, it is injective on the reals, the kernel is empty and intersecting that with $\mathbb{Z}^n$ is still empty so the map restricted to the lattice has trivial kernel and is therefore injective. In the other direction I can't seem to make progress. I have been trying to think about it in two different ways. The first, consider the columns of the matrix. For it to be injective as a map between lattices, these columns need to be linearly independent, i.e. they satisfy no non trivial relation ($\sum r_iv_i =0$ implies $r_i = 0$). However, the key is that they don't satisfy an non trivial relation over the integers, but this may not be the case over the reals; there may be some non-zero real numbers $r'_i$ that cause $\sum r_iv_i =0$ and thus the map is not injective as a map between real spaces. This is where we need to use the fact that entries in the column vectors are also integers. Clearly injectivity over integers implies it for the rationals because if we could find special rationals $r'_i$ that cause $\sum r_iv_i =0$ we could just clear denominators and derive a contradiction. The other way I've been trying to think about it is by proving the converse. Suppose the map wasn't injective we have to show that it can't be both composed of integer entries and injective as map between lattices. For it to be injective as a lattice map though would mean that the none of standard basis elements of $\mathbb{R}$ are in the kernel, and that no integer (or rational and just clear denominators) combination of them are in the kernel either, thus for something to be in the kernel it has to be linear combination of the basis vectors with at least one coefficient irrational. I think this proves it, but I feel unsatisfied, so I am not sure if something is wrong.",,"['linear-algebra', 'modules', 'integer-lattices']"
44,How many nilpotent matrices are there in $M_n(\mathbb R)$ up to similarity?,How many nilpotent matrices are there in  up to similarity?,M_n(\mathbb R),"I am trying to count all nilpotent matrices in $M_n(\mathbb R)$ up to similarity. I did the same exercise for idempotent matrices and it was quite simple. I realised that rank of an idempotent matrix is a non-negative integer and two idempotent matrices are similar iff they have the same rank. So I got the answer $n+1$ . However, it doesn't seem to be simple for nilpotent matrices. Things which are clear to me: If two nilpotent matrices are similar, they must have the same order of nilpotence. There is exactly one class of similarity for nilpotent matrices of order $n$ . The question I would like to ask: How many nilpotent matrices of order $k$ are there up to similarity? The answer is $1$ if $k=n$ . The answer is $1$ again if $k=1$ . (The null matrix is the only matrix which has nilpotence of order $1$ and it is its own similarity class.) What about other values of $k$ ? By looking at Jordan normal form, here's what I found about $n=2$ and $n=3$ . How to generalize? For $n=2$ , order of nilpotence vs number of matrices up to similarity. $\begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 1\end{array}\tag*{}$ For $n=3$ , $\begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 1\\ 3&1\end{array}\tag*{}$ The answer is $1$ for $k=2$ because there is only one unique way to create blocks along the diagonal so that one of the blocks is nilpotent of order $2$ . The arrangement if $2+1$ . $\begin{pmatrix}\color{red}{0}&\color{red}1&0\\ \color{red}0&\color{red}0&0\\0&0&\color{blue}0\end{pmatrix}\tag*{}$ Note that each block should be nilpotent in itself and hence, the choice of zero and non-zero entries. For $n=4$ , $\quad k=2$ : From $2+2$ and $2+1+1$ , I get one matrix each. If I consider $3+1$ , I would overcount. $\displaystyle \begin{pmatrix} 0 & 1 &  & \\ 0 & 0 &  & \\  &  & 0 & 1\\  &  & 0 & 0 \end{pmatrix} \ \begin{pmatrix} 0 & 1 &  & \\ 0 & 0 &  & \\  &  & 0 & \\  &  &   & 0 \end{pmatrix}\tag*{}$ $\quad k=3$ : From $3+1$ , I get one matrix. $\displaystyle \begin{pmatrix} 0 & 1 & 0 & \\ 0 & 0 & 1 & \\ 0 & 0 & 0 & \\  &  &  & 0 \end{pmatrix}\tag*{}$ Thus, $\begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 2\\ 3&1\\ 4 & 1\end{array}\tag*{}$ I am not sure how to generalize. It seems to me that there should be a recursive formula.","I am trying to count all nilpotent matrices in up to similarity. I did the same exercise for idempotent matrices and it was quite simple. I realised that rank of an idempotent matrix is a non-negative integer and two idempotent matrices are similar iff they have the same rank. So I got the answer . However, it doesn't seem to be simple for nilpotent matrices. Things which are clear to me: If two nilpotent matrices are similar, they must have the same order of nilpotence. There is exactly one class of similarity for nilpotent matrices of order . The question I would like to ask: How many nilpotent matrices of order are there up to similarity? The answer is if . The answer is again if . (The null matrix is the only matrix which has nilpotence of order and it is its own similarity class.) What about other values of ? By looking at Jordan normal form, here's what I found about and . How to generalize? For , order of nilpotence vs number of matrices up to similarity. For , The answer is for because there is only one unique way to create blocks along the diagonal so that one of the blocks is nilpotent of order . The arrangement if . Note that each block should be nilpotent in itself and hence, the choice of zero and non-zero entries. For , : From and , I get one matrix each. If I consider , I would overcount. : From , I get one matrix. Thus, I am not sure how to generalize. It seems to me that there should be a recursive formula.","M_n(\mathbb R) n+1 n k 1 k=n 1 k=1 1 k n=2 n=3 n=2 \begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 1\end{array}\tag*{} n=3 \begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 1\\ 3&1\end{array}\tag*{} 1 k=2 2 2+1 \begin{pmatrix}\color{red}{0}&\color{red}1&0\\ \color{red}0&\color{red}0&0\\0&0&\color{blue}0\end{pmatrix}\tag*{} n=4 \quad k=2 2+2 2+1+1 3+1 \displaystyle \begin{pmatrix}
0 & 1 &  & \\
0 & 0 &  & \\
 &  & 0 & 1\\
 &  & 0 & 0
\end{pmatrix} \ \begin{pmatrix}
0 & 1 &  & \\
0 & 0 &  & \\
 &  & 0 & \\
 &  &   & 0
\end{pmatrix}\tag*{} \quad k=3 3+1 \displaystyle \begin{pmatrix}
0 & 1 & 0 & \\
0 & 0 & 1 & \\
0 & 0 & 0 & \\
 &  &  & 0
\end{pmatrix}\tag*{} \begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 2\\ 3&1\\ 4 & 1\end{array}\tag*{}","['linear-algebra', 'combinatorics', 'matrices']"
45,Find a vector NOT perpendicular to a given set of vectors,Find a vector NOT perpendicular to a given set of vectors,,"Let us suppose to have a finite set of vectors $S=\{v_1,\ldots,v_m\}$ in $\mathbb{R}^n$ (with $m \gg n$ in general). I need to find a vector $x \in \mathbb{R}^n$ that is NOT perpendicular to any vector in $S$ . The existence of such a vector $x$ is guaranteed, moreover almost every vector in $\mathbb{R}^n$ satisfies this property. But I need to find an algorithm to determine a vector with these properties, I cannot close my eyes and choose. Any ideas? EDIT1: in my case, the vectors in $S$ have some ""symmetries"" in the sense that they are generated by permutation and change of signs of a few vectors in $S$ EDIT2: $v_1+\ldots+v_m=0 \in \mathbb{R}^n$ EDIT3: I simplified the solution proposed by Tom Collinge. We define a sequence $\{x_i\}_{i=1}^m$ such that $x_m=x$ is what we are looking for. First define $x_1=v_1$ , so $x_1 \cdot v_1 \ne 0$ and they are not perpendicular (obviously). Then, recursively, define for $k \in \{2,\ldots,m\}$ $$ x_k=\begin{cases} x_{k-1}+2v_k & \text{if }x_{k-1}=-v_k\\ x_{k-1}+v_k & \text{otherwise} \end{cases}. $$ By construction we have that $x_k \cdot v_i \ne 0$ for $i \le k$ , then $x_m \cdot v_i \ne 0$ for all $v_i \in S$ , so $x=x_m$ is not perpendicular to every element in $S$ . Can it works? EDIT4: As pointed out, the algorithm proposed in EDIT3 does not work","Let us suppose to have a finite set of vectors in (with in general). I need to find a vector that is NOT perpendicular to any vector in . The existence of such a vector is guaranteed, moreover almost every vector in satisfies this property. But I need to find an algorithm to determine a vector with these properties, I cannot close my eyes and choose. Any ideas? EDIT1: in my case, the vectors in have some ""symmetries"" in the sense that they are generated by permutation and change of signs of a few vectors in EDIT2: EDIT3: I simplified the solution proposed by Tom Collinge. We define a sequence such that is what we are looking for. First define , so and they are not perpendicular (obviously). Then, recursively, define for By construction we have that for , then for all , so is not perpendicular to every element in . Can it works? EDIT4: As pointed out, the algorithm proposed in EDIT3 does not work","S=\{v_1,\ldots,v_m\} \mathbb{R}^n m \gg n x \in \mathbb{R}^n S x \mathbb{R}^n S S v_1+\ldots+v_m=0 \in \mathbb{R}^n \{x_i\}_{i=1}^m x_m=x x_1=v_1 x_1 \cdot v_1 \ne 0 k \in \{2,\ldots,m\} 
x_k=\begin{cases}
x_{k-1}+2v_k & \text{if }x_{k-1}=-v_k\\
x_{k-1}+v_k & \text{otherwise}
\end{cases}.
 x_k \cdot v_i \ne 0 i \le k x_m \cdot v_i \ne 0 v_i \in S x=x_m S","['linear-algebra', 'orthogonality']"
46,"A linear algebra ""game"" with binary matrices","A linear algebra ""game"" with binary matrices",,"Let $n\in\mathbb N$ and an arbitrary binary matrix $M\in\{0,1\}^{n\times n}$ of full rank be given such that the last row of $M$ equals $(1,1,\ldots,1)$ . The following problem arose from a research question I have been working on recently. The core mechanic of the ""game"" is about taking any two rows $m_j,m_k$ of $M$ and partitioning them into $\min\{m_j,m_k\},\max\{m_j,m_k\}$ where $\min,\max$ operate entrywise. Just as an example to make things clear, if $m_j=\begin{pmatrix}1&0&1&0  \end{pmatrix}$ and $m_k=\begin{pmatrix} 1&1&0&0 \end{pmatrix}$ then $$ \min\{m_j,m_k\}=\begin{pmatrix}1&0&0&0\end{pmatrix}\qquad \max\{m_j,m_k\}=\begin{pmatrix}  1&1&1&0\end{pmatrix} $$ and, obviously, $m_j+m_k=\min\{m_j,m_k\}+\max\{m_j,m_k\}$ .  Starting from $M_0:=M$ there are three possible scenarios: Neither $\min\{m_j,m_k\}$ nor $\max\{m_j,m_k\}$ are a row of $M_0$ . Then do nothing. Either $\min\{m_j,m_k\}$ or $\max\{m_j,m_k\}$ are a row of $M_0$ (but not both). Then extend $M$ by the other row vector. (Here we use the convention that the $0$ vector is a row of $M$ so if $\min\{m_j,m_k\}=0$ then extend $M_0$ by $\max\{m_j,m_k\}$ to get a new matrix $M_1$ .) Both $\min\{m_j,m_k\}$ and $\max\{m_j,m_k\}$ are a row of $M_0$ . Then do nothing. Repeat this process (where $M_0$ becomes $M_l$ and $M_1$ becomes $M_{l+1}$ in step $l\in\mathbb N_0$ ) until for every two rows of $M_l$ scenario 2 does not occur anymore, i.e. it is maximal in this sense and no other row is added via this scheme (then denoted by $M_\text{max}$ ). Now the ""game"" ends with a victory if there exist rows $m_{i_1},\ldots,m_{i_n}$ of $M_\text{max}$ such that $$ \begin{pmatrix}m_{i_1}\\\vdots\\m_{i_n}\end{pmatrix}=\begin{pmatrix} 1&0&\cdots&0\\ \vdots&\ddots&\ddots&\vdots\\ \vdots&&\ddots&0\\ 1&\cdots&\cdots&1 \end{pmatrix}\underline{\sigma}\tag{1} $$ for some permutation $\sigma\in S_n$ (where $\underline{\sigma}$ is the corresponding permutation matrix. This is equivalent to saying that $m_{i_{l+1}}$ arises from $m_{i_l}$ by turning a $0$ into a $1$ for all $l=1,\ldots,n-1$ . This concept should become a lot clearer again by considering an example. Example 1. Let $$ M=M_0=\begin{pmatrix} 1&0&0&0\\ 0&1&1&0\\ 1&0&1&1\\ 1&1&1&1 \end{pmatrix}=\begin{pmatrix}m_1\\m_2\\m_3\\m_4\end{pmatrix}\tag{2} $$ which is of full rank so we may start the game using this matrix. Now take for example $m_2$ and $m_3$ so $$ \min\{m_2,m_3\}=\begin{pmatrix} 0&0&1&0\end{pmatrix}\qquad\max\{m_2,m_3\}=\begin{pmatrix} 1&1&1&1\end{pmatrix}\,. $$ Because $\max\{m_2,m_3\}$ is a row of $M$ but $\min\{m_2,m_3\}$ is not, apply Step 2 to get $$ M_1=\begin{pmatrix} 1&0&0&0\\ 0&0&1&0\\ 0&1&1&0\\ 1&0&1&1\\ 1&1&1&1 \end{pmatrix} $$ Doing the same for $m_1$ and (now) $m_3$ yields $\max\{m_1,m_3\}=(1\ 1\ 1\ 0)$ and $\min\{m_1,m_3\}=(0\ 0\ 0\ 0)$ . By our convention this again is Scenario 2 so we extend $M_1$ by $(1\ 1\ 1\ 0)$ to get $$M_2=\begin{pmatrix} 1&0&0&0\\ 0&0&1&0\\ 0&1&1&0\\ 1&0&1&1\\ 1&1&1&0\\ 1&1&1&1 \end{pmatrix}$$ Once more for $m_4,m_5$ and we arrive at $$ M_3=\begin{pmatrix} 1&0&0&0\\ 0&0&1&0\\ 0&1&1&0\\ 1&0&1&0\\ 1&0&1&1\\ 1&1&1&0\\ 1&1&1&1 \end{pmatrix}=M_\text{max}\,. $$ One readily verifies that for any two rows of $M_3$ the above rule ends in Scenario 3 (so in particular not in Scenario 2, nothing new can be generated anymore) which shows maximality.    The submatrices of $M_\text{max}$ which satisfy the winning condition read $$ \begin{pmatrix} 1&0&0&0\\ 1&0&1&0\\ 1&0&1&1\\ 1&1&1&1	 \end{pmatrix}, \begin{pmatrix} 1&0&0&0\\ 1&0&1&0\\ 1&1&1&0\\ 1&1&1&1 \end{pmatrix}, \begin{pmatrix} 0&0&1&0\\ 1&0&1&0\\ 1&0&1&1\\ 1&1&1&1 \end{pmatrix}, \begin{pmatrix} 0&0&1&0\\ 1&0&1&0\\ 1&1&1&0\\ 1&1&1&1 \end{pmatrix}, \begin{pmatrix} 0&0&1&0\\ 0&1&1&0\\ 1&1&1&0\\ 1&1&1&1 \end{pmatrix}\tag{3} $$ One can also reformulate the problem in terms of directed graphs, which may (or may not) be a good alternative approach here but it certainly visualizes this problem in a nice way. Turn row vectors $\{m_1,\ldots,m_l\}$ into vertices $\{v_1,\ldots,v_l\}$ and draw an arrow from $v_j$ to $v_k$ whenever $v_k-v_j$ is a standard basis vector (i.e. $v_k-v_j$ consists of $n-1$ zeros and one $1$ ). Again by convention, $(0\ \ldots\ 0)$ is turned into a vertex as well. The victory condition then is met if and only if there exists a path from $(0\ \ldots\ 0)$ to $(1\ \ldots\ 1)$ which is equivalent to $A_\text{max}^n\neq 0$ with $A_\text{max}$ the adjacency matrix of the graph corresponding to $M_\text{max}$ . Example 1 (directed graphs-version). The graph of the original $M$ from (2) is with adjacency matrix $$A_0=\begin{pmatrix} 0&1&0&0&0\\0&0&0&0&0\\0&0&0&0&0\\0&0&0&0&1\\0&0&0&0&0 \end{pmatrix}\,.$$ Then $M_1$ converts to with adjacency matrix $$A_1=\begin{pmatrix} 0&1&\color{blue}1&0&0&0\\0&0&\color{blue}0&0&0&0\\\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1&\color{blue}0&\color{blue}0\\0&0&\color{blue}0&0&0&0\\0&0&\color{blue}0&0&0&1 \\0&0&\color{blue}0&0&0&0\end{pmatrix}$$ followed by $$A_2=\begin{pmatrix} 0&1&1&0&0&\color{blue}0&0\\ 0&0&0&0&0&\color{blue}0&0\\ 0&0&0&1&0&\color{blue}0&0\\ 0&0&0&0&0&\color{blue}1&0\\ 0&0&0&0&0&\color{blue}0&1\\ \color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1\\ 0&0&0&0&0&\color{blue}0&0 \end{pmatrix}$$ and $$ A_3=A_\text{max}=\begin{pmatrix}0&1&1&0&\color{blue}0&0&0&0\\ 0&0&0&0&\color{blue}1&0&0&0\\ 0&0&0&1&\color{blue}1&0&0&0\\ 0&0&0&0&\color{blue}0&0&1&0\\ \color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1&\color{blue}1&\color{blue}0\\ 0&0&0&0&\color{blue}0&0&0&1\\ 0&0&0&0&\color{blue}0&0&0&1\\ 0&0&0&0&\color{blue}0&0&0&0  \end{pmatrix}\,.$$ The blue numbers are the ones that are added in the corresponding step.   Also note that the adjacency matrices of the individual steps are necessarily triangular.   Now the victory condition is met if and only if $A_\text{max}^4\neq 0$ . Indeed $A_\text{max}^4=5 e_1e_n^T$ so there exist 5 different paths (corresponding to eq.(3)) that lead from $(0\ \ldots\ 0)$ to $(1\ \ldots\ 1)$ . Be aware that $A_2^4=e_1e_n^T\neq 0$ becasue there is one path from $\vec 0$ to $\vec 1$ so we already won after the second step. The whole time I put the word ""game"" in quotation marks for the following reason: Conjecture 1. For every $M$ satisfying the above conditions the game ends in a victory, i.e. $M_\text{max}$ contains at least one submatrix of form (1). Equivalently the directed graph corresponding to $M_\text{max}$ contains at least one path leading from from $(0\ \ldots\ 0)$ to $(1\ \ldots\ 1)$ , i.e. $A_\text{max}^n\neq 0$ . A useful intermediate step might be the following: Conjecture 2. Let $M$ satisfying the above conditions be given. For any two rows $m_j,m_k$ of $M_\text{max}$ only scenario 3 can occur. These conjectures are supported by a plethora of examples I have computed (up to $n=6$ ) which makes me rather confident that this is true. Two short comments to make here. The full rank condition on $M$ cannot be waived or replaced by something like ""all rows of $M$ have to be pairwise distinct"". For this consider $$ M=\begin{pmatrix} 1&1&0\\0&1&1\\1&1&1 \end{pmatrix}=M_\text{max} $$ which does obviously not satisfy the winning condition (although for any two rows, Scenario 3 occurs). Having one row equal to $(1\ \ldots\ 1)$ is necessary. Consider $$ M=\begin{pmatrix} 1&1&0\\1&0&1\\0&1&1\end{pmatrix}=M_\text{max}\,, $$ cf. also mjacobse's answer. I am currently lacking ideas on how to approach this problem. Working with the adjacency matrices seems like a possibility, although I neither see how the rank of $M$ is reflected in $A_\text{max}$ nor do I see a general pattern there yet. The only thing I see is that if $M_1,M_2\in\{0,1\}^{n\times n}$ of full rank satisfy $M_1=M_2\underline{\tau}$ for some permutation $\tau\in S_n$ then $M_1$ leads to victory if and only if $M_2$ does---so we can reduce this problem to equivalence classes of initial matrices but this should at best be a simplification, not an idea for a proof. Thank you in advance for any comments and or ideas!","Let and an arbitrary binary matrix of full rank be given such that the last row of equals . The following problem arose from a research question I have been working on recently. The core mechanic of the ""game"" is about taking any two rows of and partitioning them into where operate entrywise. Just as an example to make things clear, if and then and, obviously, .  Starting from there are three possible scenarios: Neither nor are a row of . Then do nothing. Either or are a row of (but not both). Then extend by the other row vector. (Here we use the convention that the vector is a row of so if then extend by to get a new matrix .) Both and are a row of . Then do nothing. Repeat this process (where becomes and becomes in step ) until for every two rows of scenario 2 does not occur anymore, i.e. it is maximal in this sense and no other row is added via this scheme (then denoted by ). Now the ""game"" ends with a victory if there exist rows of such that for some permutation (where is the corresponding permutation matrix. This is equivalent to saying that arises from by turning a into a for all . This concept should become a lot clearer again by considering an example. Example 1. Let which is of full rank so we may start the game using this matrix. Now take for example and so Because is a row of but is not, apply Step 2 to get Doing the same for and (now) yields and . By our convention this again is Scenario 2 so we extend by to get Once more for and we arrive at One readily verifies that for any two rows of the above rule ends in Scenario 3 (so in particular not in Scenario 2, nothing new can be generated anymore) which shows maximality.    The submatrices of which satisfy the winning condition read One can also reformulate the problem in terms of directed graphs, which may (or may not) be a good alternative approach here but it certainly visualizes this problem in a nice way. Turn row vectors into vertices and draw an arrow from to whenever is a standard basis vector (i.e. consists of zeros and one ). Again by convention, is turned into a vertex as well. The victory condition then is met if and only if there exists a path from to which is equivalent to with the adjacency matrix of the graph corresponding to . Example 1 (directed graphs-version). The graph of the original from (2) is with adjacency matrix Then converts to with adjacency matrix followed by and The blue numbers are the ones that are added in the corresponding step.   Also note that the adjacency matrices of the individual steps are necessarily triangular.   Now the victory condition is met if and only if . Indeed so there exist 5 different paths (corresponding to eq.(3)) that lead from to . Be aware that becasue there is one path from to so we already won after the second step. The whole time I put the word ""game"" in quotation marks for the following reason: Conjecture 1. For every satisfying the above conditions the game ends in a victory, i.e. contains at least one submatrix of form (1). Equivalently the directed graph corresponding to contains at least one path leading from from to , i.e. . A useful intermediate step might be the following: Conjecture 2. Let satisfying the above conditions be given. For any two rows of only scenario 3 can occur. These conjectures are supported by a plethora of examples I have computed (up to ) which makes me rather confident that this is true. Two short comments to make here. The full rank condition on cannot be waived or replaced by something like ""all rows of have to be pairwise distinct"". For this consider which does obviously not satisfy the winning condition (although for any two rows, Scenario 3 occurs). Having one row equal to is necessary. Consider cf. also mjacobse's answer. I am currently lacking ideas on how to approach this problem. Working with the adjacency matrices seems like a possibility, although I neither see how the rank of is reflected in nor do I see a general pattern there yet. The only thing I see is that if of full rank satisfy for some permutation then leads to victory if and only if does---so we can reduce this problem to equivalence classes of initial matrices but this should at best be a simplification, not an idea for a proof. Thank you in advance for any comments and or ideas!","n\in\mathbb N M\in\{0,1\}^{n\times n} M (1,1,\ldots,1) m_j,m_k M \min\{m_j,m_k\},\max\{m_j,m_k\} \min,\max m_j=\begin{pmatrix}1&0&1&0  \end{pmatrix} m_k=\begin{pmatrix} 1&1&0&0 \end{pmatrix} 
\min\{m_j,m_k\}=\begin{pmatrix}1&0&0&0\end{pmatrix}\qquad
\max\{m_j,m_k\}=\begin{pmatrix}  1&1&1&0\end{pmatrix}
 m_j+m_k=\min\{m_j,m_k\}+\max\{m_j,m_k\} M_0:=M \min\{m_j,m_k\} \max\{m_j,m_k\} M_0 \min\{m_j,m_k\} \max\{m_j,m_k\} M_0 M 0 M \min\{m_j,m_k\}=0 M_0 \max\{m_j,m_k\} M_1 \min\{m_j,m_k\} \max\{m_j,m_k\} M_0 M_0 M_l M_1 M_{l+1} l\in\mathbb N_0 M_l M_\text{max} m_{i_1},\ldots,m_{i_n} M_\text{max} 
\begin{pmatrix}m_{i_1}\\\vdots\\m_{i_n}\end{pmatrix}=\begin{pmatrix}
1&0&\cdots&0\\
\vdots&\ddots&\ddots&\vdots\\
\vdots&&\ddots&0\\
1&\cdots&\cdots&1
\end{pmatrix}\underline{\sigma}\tag{1}
 \sigma\in S_n \underline{\sigma} m_{i_{l+1}} m_{i_l} 0 1 l=1,\ldots,n-1 
M=M_0=\begin{pmatrix}
1&0&0&0\\
0&1&1&0\\
1&0&1&1\\
1&1&1&1
\end{pmatrix}=\begin{pmatrix}m_1\\m_2\\m_3\\m_4\end{pmatrix}\tag{2}
 m_2 m_3 
\min\{m_2,m_3\}=\begin{pmatrix} 0&0&1&0\end{pmatrix}\qquad\max\{m_2,m_3\}=\begin{pmatrix} 1&1&1&1\end{pmatrix}\,.
 \max\{m_2,m_3\} M \min\{m_2,m_3\} 
M_1=\begin{pmatrix}
1&0&0&0\\
0&0&1&0\\
0&1&1&0\\
1&0&1&1\\
1&1&1&1
\end{pmatrix}
 m_1 m_3 \max\{m_1,m_3\}=(1\ 1\ 1\ 0) \min\{m_1,m_3\}=(0\ 0\ 0\ 0) M_1 (1\ 1\ 1\ 0) M_2=\begin{pmatrix} 1&0&0&0\\
0&0&1&0\\
0&1&1&0\\
1&0&1&1\\
1&1&1&0\\
1&1&1&1 \end{pmatrix} m_4,m_5 
M_3=\begin{pmatrix}
1&0&0&0\\
0&0&1&0\\
0&1&1&0\\
1&0&1&0\\
1&0&1&1\\
1&1&1&0\\
1&1&1&1
\end{pmatrix}=M_\text{max}\,.
 M_3 M_\text{max} 
\begin{pmatrix}
1&0&0&0\\
1&0&1&0\\
1&0&1&1\\
1&1&1&1	
\end{pmatrix},
\begin{pmatrix}
1&0&0&0\\
1&0&1&0\\
1&1&1&0\\
1&1&1&1
\end{pmatrix},
\begin{pmatrix}
0&0&1&0\\
1&0&1&0\\
1&0&1&1\\
1&1&1&1
\end{pmatrix},
\begin{pmatrix}
0&0&1&0\\
1&0&1&0\\
1&1&1&0\\
1&1&1&1
\end{pmatrix},
\begin{pmatrix}
0&0&1&0\\
0&1&1&0\\
1&1&1&0\\
1&1&1&1
\end{pmatrix}\tag{3}
 \{m_1,\ldots,m_l\} \{v_1,\ldots,v_l\} v_j v_k v_k-v_j v_k-v_j n-1 1 (0\ \ldots\ 0) (0\ \ldots\ 0) (1\ \ldots\ 1) A_\text{max}^n\neq 0 A_\text{max} M_\text{max} M A_0=\begin{pmatrix} 0&1&0&0&0\\0&0&0&0&0\\0&0&0&0&0\\0&0&0&0&1\\0&0&0&0&0 \end{pmatrix}\,. M_1 A_1=\begin{pmatrix} 0&1&\color{blue}1&0&0&0\\0&0&\color{blue}0&0&0&0\\\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1&\color{blue}0&\color{blue}0\\0&0&\color{blue}0&0&0&0\\0&0&\color{blue}0&0&0&1 \\0&0&\color{blue}0&0&0&0\end{pmatrix} A_2=\begin{pmatrix} 0&1&1&0&0&\color{blue}0&0\\
0&0&0&0&0&\color{blue}0&0\\
0&0&0&1&0&\color{blue}0&0\\
0&0&0&0&0&\color{blue}1&0\\
0&0&0&0&0&\color{blue}0&1\\
\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1\\
0&0&0&0&0&\color{blue}0&0 \end{pmatrix} 
A_3=A_\text{max}=\begin{pmatrix}0&1&1&0&\color{blue}0&0&0&0\\
0&0&0&0&\color{blue}1&0&0&0\\
0&0&0&1&\color{blue}1&0&0&0\\
0&0&0&0&\color{blue}0&0&1&0\\
\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1&\color{blue}1&\color{blue}0\\
0&0&0&0&\color{blue}0&0&0&1\\
0&0&0&0&\color{blue}0&0&0&1\\
0&0&0&0&\color{blue}0&0&0&0  \end{pmatrix}\,. A_\text{max}^4\neq 0 A_\text{max}^4=5 e_1e_n^T (0\ \ldots\ 0) (1\ \ldots\ 1) A_2^4=e_1e_n^T\neq 0 \vec 0 \vec 1 M M_\text{max} M_\text{max} (0\ \ldots\ 0) (1\ \ldots\ 1) A_\text{max}^n\neq 0 M m_j,m_k M_\text{max} n=6 M M 
M=\begin{pmatrix} 1&1&0\\0&1&1\\1&1&1 \end{pmatrix}=M_\text{max}
 (1\ \ldots\ 1) 
M=\begin{pmatrix} 1&1&0\\1&0&1\\0&1&1\end{pmatrix}=M_\text{max}\,,
 M A_\text{max} M_1,M_2\in\{0,1\}^{n\times n} M_1=M_2\underline{\tau} \tau\in S_n M_1 M_2","['linear-algebra', 'matrices', 'graph-theory']"
47,Tensor product commutes with associated graded,Tensor product commutes with associated graded,,"Let $V,W$ be vector spaces over a field $k$ , not necessarily finite-dimensional, and $V_{\bullet}=(V=V_0\supseteq V_1\supseteq\cdots\supseteq V_n=0)$ and $W_{\bullet}=(W=W_0\supseteq W_1\supseteq\cdots\supseteq W_m=0)$ be finite filtrations of each. Then $V\otimes_k W$ admits a natural finite filtration $(V\otimes W)_\bullet$ given by $$ (V\otimes W)_k=\sum\limits_{i+j=k}V_i\otimes W_j.$$ I want to know whether $\operatorname{gr}(V\otimes W)_\bullet\cong\operatorname{gr}V_\bullet\otimes\operatorname{gr}W_\bullet$ as graded vector spaces, where $\operatorname{gr}$ is the functor which takes a filtered vector space to its associated graded. More specifically, for each $k$ we have a natural map $$\phi_k:\bigoplus\limits_{i+j=k}(V_i/V_{i+1})\otimes(W_j/W_{j+1})\to(V\otimes W)_k/(V\otimes W)_{k+1}$$ coming from the natural surjective map $$\bigoplus\limits_{i+j=k}V_i\otimes W_j\to(V\otimes W)_k/(V\otimes W)_{k+1}$$ coming from the inclusions $V_i\otimes W_j\subseteq(V\otimes W)_k$ when $i+j=k$ .  Thus $\phi_k$ is surjective for all $k$ .  I would like to know whether $\phi_k$ is always an isomorphism.  I've tried writing out a basis of some splitting of this filtration to show it, and I can't quite complete the argument that way. If you have a proof, know a counterexample, or have a reference for this I would really appreciate it!!","Let be vector spaces over a field , not necessarily finite-dimensional, and and be finite filtrations of each. Then admits a natural finite filtration given by I want to know whether as graded vector spaces, where is the functor which takes a filtered vector space to its associated graded. More specifically, for each we have a natural map coming from the natural surjective map coming from the inclusions when .  Thus is surjective for all .  I would like to know whether is always an isomorphism.  I've tried writing out a basis of some splitting of this filtration to show it, and I can't quite complete the argument that way. If you have a proof, know a counterexample, or have a reference for this I would really appreciate it!!","V,W k V_{\bullet}=(V=V_0\supseteq V_1\supseteq\cdots\supseteq V_n=0) W_{\bullet}=(W=W_0\supseteq W_1\supseteq\cdots\supseteq W_m=0) V\otimes_k W (V\otimes W)_\bullet  (V\otimes W)_k=\sum\limits_{i+j=k}V_i\otimes W_j. \operatorname{gr}(V\otimes W)_\bullet\cong\operatorname{gr}V_\bullet\otimes\operatorname{gr}W_\bullet \operatorname{gr} k \phi_k:\bigoplus\limits_{i+j=k}(V_i/V_{i+1})\otimes(W_j/W_{j+1})\to(V\otimes W)_k/(V\otimes W)_{k+1} \bigoplus\limits_{i+j=k}V_i\otimes W_j\to(V\otimes W)_k/(V\otimes W)_{k+1} V_i\otimes W_j\subseteq(V\otimes W)_k i+j=k \phi_k k \phi_k","['linear-algebra', 'vector-spaces', 'graded-modules', 'filtrations']"
48,Are linear transformations precisely those that keep lines straight and the origin fixed?,Are linear transformations precisely those that keep lines straight and the origin fixed?,,"It's easy to show that given a linear transformation $T:\mathbb{R}^n \rightarrow \mathbb{R}^m$ lines are mapped to lines and the origin stays fixed (as long as its rank $=n$ ). Yet is the converse true? More precisely, if $T:\mathbb{R}^n \rightarrow \mathbb{R}^m$ is a function that maps lines to lines in the sense that for any pair of vectors $a, b$ there exists vectors $c, d$ such that $T(a+tb)=c+td$ & $T(0)=0$ can we deduce that $T(x+y)=T(x)+T(y)$ for all vectors $x, y$ ? Would appreciate any help.","It's easy to show that given a linear transformation lines are mapped to lines and the origin stays fixed (as long as its rank ). Yet is the converse true? More precisely, if is a function that maps lines to lines in the sense that for any pair of vectors there exists vectors such that & can we deduce that for all vectors ? Would appreciate any help.","T:\mathbb{R}^n \rightarrow \mathbb{R}^m =n T:\mathbb{R}^n \rightarrow \mathbb{R}^m a, b c, d T(a+tb)=c+td T(0)=0 T(x+y)=T(x)+T(y) x, y","['linear-algebra', 'proof-writing', 'linear-transformations']"
49,"If $A$ is invertible and $A^n$ is diagonalizable, then $A$ is diagonalizable.","If  is invertible and  is diagonalizable, then  is diagonalizable.",A A^n A,"I'm attempting to understand a proof presented on this Wikipedia page: https://en.wikipedia.org/wiki/Diagonalizable_matrix The claim is as follows: Let $A$ be a matrix over $F$ . If $A$ is diagonalizable, then so is any power of it. Conversely, if $A$ is invertible, $F$ is algebraically closed, and $A^n$ is diagonalizable for some $n$ that is not an integer multiple of the characteristic of $F$ , then $A$ is diagonalizable. This is the provided proof: If $A^n$ is diagonalizable, then $A$ is annihilated by some polynomial ${\displaystyle \left(x^{n}-\lambda _{1}\right)\cdots \left(x^{n}-\lambda _{k}\right)}$ , which has no multiple root (since ${\displaystyle \lambda _{j}\neq 0})$ and is divided by the minimal polynomial of $A$ . Here are my questions: (a) Why is it clear that $A$ is annihilated by that polynomial? $A$ certainly is annihilated by $(x-\lambda_1)\cdots(x-\lambda_k)$ and $A^n$ is annihilated by $(x-\lambda_1^n)\cdots(x-\lambda_k^n)$ (as powers of matrices have powers of eigenvalues as their eigenvalues), but I don't see the connection to Wikipedia's claim. (b) What tells us that there is no multiple root? It's clear to me that $\lambda_j\neq0$ ( $A$ is invertible implies $A^n$ is invertible implies $A^n$ does not have $0$ as an eigenvalue), but why can't two of the $\lambda_j$ 's be equal? What says that we have distinct eigenvalues? (c) Any polynomial that annihilates a matrix certainly is a multiple of the minimal polynomial...but why does this tell us that $A$ is diagonalizable? (d) Earlier in the article, the following is claimed: A matrix or linear map is diagonalizable over the field $F$ if and only if its minimal polynomial is a product of distinct linear factors over $F$ . If questions (a) and (b) are resolved, I can see how this would imply (c), but why is this claim true? Here is another approach to this problem, but this one seems to be more complicated than what is presented on Wikipedia. Positive power of an invertible matrix with complex entries is diagonalizable only if the matrix itself is diagonalizable.","I'm attempting to understand a proof presented on this Wikipedia page: https://en.wikipedia.org/wiki/Diagonalizable_matrix The claim is as follows: Let be a matrix over . If is diagonalizable, then so is any power of it. Conversely, if is invertible, is algebraically closed, and is diagonalizable for some that is not an integer multiple of the characteristic of , then is diagonalizable. This is the provided proof: If is diagonalizable, then is annihilated by some polynomial , which has no multiple root (since and is divided by the minimal polynomial of . Here are my questions: (a) Why is it clear that is annihilated by that polynomial? certainly is annihilated by and is annihilated by (as powers of matrices have powers of eigenvalues as their eigenvalues), but I don't see the connection to Wikipedia's claim. (b) What tells us that there is no multiple root? It's clear to me that ( is invertible implies is invertible implies does not have as an eigenvalue), but why can't two of the 's be equal? What says that we have distinct eigenvalues? (c) Any polynomial that annihilates a matrix certainly is a multiple of the minimal polynomial...but why does this tell us that is diagonalizable? (d) Earlier in the article, the following is claimed: A matrix or linear map is diagonalizable over the field if and only if its minimal polynomial is a product of distinct linear factors over . If questions (a) and (b) are resolved, I can see how this would imply (c), but why is this claim true? Here is another approach to this problem, but this one seems to be more complicated than what is presented on Wikipedia. Positive power of an invertible matrix with complex entries is diagonalizable only if the matrix itself is diagonalizable.",A F A A F A^n n F A A^n A {\displaystyle \left(x^{n}-\lambda _{1}\right)\cdots \left(x^{n}-\lambda _{k}\right)} {\displaystyle \lambda _{j}\neq 0}) A A A (x-\lambda_1)\cdots(x-\lambda_k) A^n (x-\lambda_1^n)\cdots(x-\lambda_k^n) \lambda_j\neq0 A A^n A^n 0 \lambda_j A F F,"['linear-algebra', 'proof-explanation', 'diagonalization']"
50,Generalized convex combination using matrices,Generalized convex combination using matrices,,"A convex combination between two points $x_1$ and $x_2$ in $\mathbb{R}^N$ is defined as: $$ x(\lambda) = \lambda x_1 + (1-\lambda)x_2, \qquad \lambda \in [0,1].$$ Here $x(0) = x_2$ , $x(1) = x_1$ , and the set $x(\lambda)$ (when $\lambda$ varies) can be interpreted as the line segment between $x_1$ and $x_2$ , which is itself a convex set. This could be generalized to matrix combinations, for example: $$ x(A) = A x_1 + (I-A)x_2, \qquad  0 \preceq A \preceq I,$$ where $I$ is the identity matrix, $A \preceq B \Leftrightarrow B-A$ positive semi-definite, and similarly here $x(0) = x_2$ and $x(I) = x_1$ . Question : what can be said about the set $S = \{A x_1 + (I-A)x_2 | 0 \preceq A \preceq I\}$ ? In particular, is there any geometrical interpretation of S? More generally, for $M$ point $\{x_i\}_{1,...,M}$ , one could consider: $$\left\{\sum_{i=1}^M A_ix_i \;\Bigg| \;\sum_{i=1}^M A_i = I, \, \quad 0 \preceq A_i \preceq I\right\},$$ which simplifies to the convex hull of $\{x_i\}_{1,...,M}$ when $A_i = \lambda_i I$ , $\sum \lambda_i = 1$","A convex combination between two points and in is defined as: Here , , and the set (when varies) can be interpreted as the line segment between and , which is itself a convex set. This could be generalized to matrix combinations, for example: where is the identity matrix, positive semi-definite, and similarly here and . Question : what can be said about the set ? In particular, is there any geometrical interpretation of S? More generally, for point , one could consider: which simplifies to the convex hull of when ,","x_1 x_2 \mathbb{R}^N  x(\lambda) = \lambda x_1 + (1-\lambda)x_2, \qquad \lambda \in [0,1]. x(0) = x_2 x(1) = x_1 x(\lambda) \lambda x_1 x_2  x(A) = A x_1 + (I-A)x_2, \qquad  0 \preceq A \preceq I, I A \preceq B \Leftrightarrow B-A x(0) = x_2 x(I) = x_1 S = \{A x_1 + (I-A)x_2 | 0 \preceq A \preceq I\} M \{x_i\}_{1,...,M} \left\{\sum_{i=1}^M A_ix_i \;\Bigg| \;\sum_{i=1}^M A_i = I, \, \quad 0 \preceq A_i \preceq I\right\}, \{x_i\}_{1,...,M} A_i = \lambda_i I \sum \lambda_i = 1","['linear-algebra', 'convex-geometry']"
51,Conditioning of the linear systems in the inverse or Rayleigh quotient iteration algorithms,Conditioning of the linear systems in the inverse or Rayleigh quotient iteration algorithms,,"I'm working through the book Numerical Linear Algebra by Trefethen and Bau.  In Lecture 27 (and exercise 27.5), the following claim is made about the inverse iteration algorithm: Let $ A $ be a real, symmetric matrix.  Solving the system $ (A - \mu I) w = v^{(k-1)} $ at step $ k $ is poorly conditioned if $ \mu $ is approximately an eigenvalue of $ A $.  However, this does not cause an issue for the inverse iteration algorithm if it is solved with a backward stable algorithm which outputs $ \tilde{w} $ such that $ (A - \mu I + \delta M) \tilde{w} = v^{(k-1)}$ where $ \frac{\|\delta M\|}{\|M\|} = O(\epsilon_\text{machine}) $.  The reason is that even though $ w $ and $ \tilde{w} $ are not close, $ \frac{w}{\|w\|} $ and $ \frac{\tilde{w}}{\|\tilde{w}\|} $ are. The same issue occurs in the Rayleigh quotient iteration where at each step $ \mu $ is updated with a more accurate estimate of an eigenvalue of $ A $. I completely understand why the system is poorly conditioned when $ \mu $ is approximately an eigenvalue of $ A $.  I am attempting to prove the remainder of the claim or at least understand why it should be true.  Applying the definitions of backward stability and the condition of the problem don't lead anywhere beyond the usual bound for the accuracy: $ \frac{\|w - \tilde{w} \|}{\|w\|} = O(\kappa(A - \mu I) \epsilon_\text{machine}) = O(1) $ for $ \mu $ near an eigenvalue of $ A $.  I suspect that I need to use the fact that $ A $ is normal to move forward, but I don't see how. Any help is appreciated.  Thanks! Links to Wikipedia articles on 1. Inverse iteration 2. Rayleigh quotient iteration","I'm working through the book Numerical Linear Algebra by Trefethen and Bau.  In Lecture 27 (and exercise 27.5), the following claim is made about the inverse iteration algorithm: Let $ A $ be a real, symmetric matrix.  Solving the system $ (A - \mu I) w = v^{(k-1)} $ at step $ k $ is poorly conditioned if $ \mu $ is approximately an eigenvalue of $ A $.  However, this does not cause an issue for the inverse iteration algorithm if it is solved with a backward stable algorithm which outputs $ \tilde{w} $ such that $ (A - \mu I + \delta M) \tilde{w} = v^{(k-1)}$ where $ \frac{\|\delta M\|}{\|M\|} = O(\epsilon_\text{machine}) $.  The reason is that even though $ w $ and $ \tilde{w} $ are not close, $ \frac{w}{\|w\|} $ and $ \frac{\tilde{w}}{\|\tilde{w}\|} $ are. The same issue occurs in the Rayleigh quotient iteration where at each step $ \mu $ is updated with a more accurate estimate of an eigenvalue of $ A $. I completely understand why the system is poorly conditioned when $ \mu $ is approximately an eigenvalue of $ A $.  I am attempting to prove the remainder of the claim or at least understand why it should be true.  Applying the definitions of backward stability and the condition of the problem don't lead anywhere beyond the usual bound for the accuracy: $ \frac{\|w - \tilde{w} \|}{\|w\|} = O(\kappa(A - \mu I) \epsilon_\text{machine}) = O(1) $ for $ \mu $ near an eigenvalue of $ A $.  I suspect that I need to use the fact that $ A $ is normal to move forward, but I don't see how. Any help is appreciated.  Thanks! Links to Wikipedia articles on 1. Inverse iteration 2. Rayleigh quotient iteration",,"['linear-algebra', 'numerical-linear-algebra', 'floating-point']"
52,How to convert a random matrix to Unitary Matrix?,How to convert a random matrix to Unitary Matrix?,,"I know that a complex matrix $n \times n$ is said to be unitary if $AA^*=A^*A=I$ or equivalently if $A^*=A^{-1}$. But I asked what if there is a random matrix and we want to turn it into an unitary matrix, please also give an example.","I know that a complex matrix $n \times n$ is said to be unitary if $AA^*=A^*A=I$ or equivalently if $A^*=A^{-1}$. But I asked what if there is a random matrix and we want to turn it into an unitary matrix, please also give an example.",,['linear-algebra']
53,Efficiently computing Schur complement,Efficiently computing Schur complement,,"I would like to compute the Schur complement $A-B^TC^{-1}B$ , where $C = I+VV^T$ (diagonal plus low rank). The matrix $A$ has $10^3$ rows/columns, while $C$ has $10^6$ rows/columns. The Woodbury formula yields the following expression for the Schur complement: $$A-B^TB+B^TV(I+V^TV)^{-1}V^TB.$$ This expression can be evaluated without storing large matrices, but the result suffers from numerical inaccuracies. Is there a numerically more stable way of computing the Schur complement, without high memory requirements?","I would like to compute the Schur complement , where (diagonal plus low rank). The matrix has rows/columns, while has rows/columns. The Woodbury formula yields the following expression for the Schur complement: This expression can be evaluated without storing large matrices, but the result suffers from numerical inaccuracies. Is there a numerically more stable way of computing the Schur complement, without high memory requirements?",A-B^TC^{-1}B C = I+VV^T A 10^3 C 10^6 A-B^TB+B^TV(I+V^TV)^{-1}V^TB.,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'schur-complement']"
54,Find the rank of the tensor,Find the rank of the tensor,,"I need to find the rank of the tensor $t = a \otimes a \otimes b + a \otimes b \otimes a + b \otimes a \otimes a$. For simplicity let $a = (1, 0)^T$ and $b = (0, 1)^T$. I know the answer, rank equals 3. And moreover as I know rank equals 3 for every pair of linear independent vectors $a$ and $b$. But I do not know how to prove that rank equals 3 even in this particular case. I tried to write an assumption that $t = u_1 \otimes v_1 \otimes w_1 + u_2 \otimes v_2 \otimes w_2$ and write elementwise equations. But I got eight equations, so it looks difficult to solve: \begin{cases} t_{111} = u_{11}v_{11}w_{11} + u_{21}v_{21}w_{21} = 0\\ t_{112} = u_{11}v_{11}w_{12} + u_{21}v_{21}w_{22} = 1\\ t_{121} = u_{11}v_{12}w_{11} + u_{21}v_{22}w_{21} = 1\\ t_{122} = u_{11}v_{12}w_{12} + u_{21}v_{22}w_{22} = 0\\ t_{211} = u_{12}v_{11}w_{11} + u_{22}v_{21}w_{21} = 1\\ t_{212} = u_{12}v_{11}w_{12} + u_{22}v_{21}w_{22} = 0\\ t_{221} = u_{12}v_{12}w_{11} + u_{22}v_{22}w_{21} = 0\\ t_{222} = u_{12}v_{12}w_{12} + u_{22}v_{22}w_{22} = 0\\ \end{cases} Thanks for the help, do not judge strictly it is my first experience with tensors.","I need to find the rank of the tensor $t = a \otimes a \otimes b + a \otimes b \otimes a + b \otimes a \otimes a$. For simplicity let $a = (1, 0)^T$ and $b = (0, 1)^T$. I know the answer, rank equals 3. And moreover as I know rank equals 3 for every pair of linear independent vectors $a$ and $b$. But I do not know how to prove that rank equals 3 even in this particular case. I tried to write an assumption that $t = u_1 \otimes v_1 \otimes w_1 + u_2 \otimes v_2 \otimes w_2$ and write elementwise equations. But I got eight equations, so it looks difficult to solve: \begin{cases} t_{111} = u_{11}v_{11}w_{11} + u_{21}v_{21}w_{21} = 0\\ t_{112} = u_{11}v_{11}w_{12} + u_{21}v_{21}w_{22} = 1\\ t_{121} = u_{11}v_{12}w_{11} + u_{21}v_{22}w_{21} = 1\\ t_{122} = u_{11}v_{12}w_{12} + u_{21}v_{22}w_{22} = 0\\ t_{211} = u_{12}v_{11}w_{11} + u_{22}v_{21}w_{21} = 1\\ t_{212} = u_{12}v_{11}w_{12} + u_{22}v_{21}w_{22} = 0\\ t_{221} = u_{12}v_{12}w_{11} + u_{22}v_{22}w_{21} = 0\\ t_{222} = u_{12}v_{12}w_{12} + u_{22}v_{22}w_{22} = 0\\ \end{cases} Thanks for the help, do not judge strictly it is my first experience with tensors.",,"['linear-algebra', 'tensors']"
55,"What does the word ""norm"" stands for in linear algebra?","What does the word ""norm"" stands for in linear algebra?",,"I know that ""norm"" is the formal name for length, but where did this name came from? or from what language is came from? Thank you in advance.","I know that ""norm"" is the formal name for length, but where did this name came from? or from what language is came from? Thank you in advance.",,"['linear-algebra', 'terminology']"
56,How to generalize the determinant as function,How to generalize the determinant as function,,"Hi I was asked to show that for any vector space $V$ over a field $\mathbb{F}$ of arbitrary dimension $n$ that if we fix some basis $\beta=\{w_1,\ldots,w_n\}$ that there is a unique function $D_\beta : V \times \cdots \times V \to \mathbb{F}$ that will satisfy the three properties, $D_\beta(v_1,\ldots,v_n)=0$ if some $v_i=v_j, i \neq j$ $D_\beta$ is linear in each factor and $D_\beta(w_1,\ldots,w_n)=1$ Moreover, I want to be able to prove some things using this. For example that Applying D to some vectors v will be the same as taking the determinant of the coordinate representation of those v, that applying D and getting something non zero to a set of vectors implys those vectors are not a basis, etc. I know that in the case of two dimensional, we can do this by using the idea that the determinant gives the area of a parallelepiped, etc. And if we define the function to be that that satisfies the above properties then it can be easily shown to be unique. But in cases when it is not dim 2, since I dont have a general formula for the n vectors , how can this be done? Is it safe to assume that such a function exists and then just show it is unique, or must it be shown that even such a function exists at all? And say we could just define the function that satisfies those properties. Then how would we show it is unique, etc? Is it possible to maybe do this problem using matrices? Ie, we can define the determinant on the nxn matrices as our inputs, and somehow use that to prove things? I have not learned about things called alternating maps. I am still trying to do this , using advice in answers, is this how it should be approached? Define a mapping $f: L(V^{n}, \mathbb{F}) \to \mathbb{F}$ by $f \to f(e_1,...,e_n)$, or should it be as $f \to f(x_1,...x_n)$? Then $f$ is injective as if $f_{1}(e_1,...,e_n)=f_{2}(e_1,...,e_n)$ then would this imply they are equal as how a function acts on a basis completely determines it? I a just really confused. and then If I could show that it is subjective and thus an isomorphism I could say choose that unique f which is the one that gives $f(x_1,\ldots,x_n)=0$ and claim this is the function $D_\beta$ I wanted. But I am having lots of trouble putting it all together. I still dont understand. It seems like the top answer is getting many vote, but I dont understand it. This is only a first course in linear algebra by the way, so I do not know about many advanced results. Maybe the answer could rely on sgn function and permutations. Then I could have a general formula I dont know how in some of the answers we can just simply say suppose D is alternating etc, when this is one of the things I want to prove. That is what is also confusing me. I dont know what we can even start with. Or if we must start from scratch completely. Looking for advice. If this cannot be done so easily, I would also be happy to see that if we assume such a function does exist, then at least prove it is unique. Without just saying the determinant is that function and the determinant is unique. Thank you","Hi I was asked to show that for any vector space $V$ over a field $\mathbb{F}$ of arbitrary dimension $n$ that if we fix some basis $\beta=\{w_1,\ldots,w_n\}$ that there is a unique function $D_\beta : V \times \cdots \times V \to \mathbb{F}$ that will satisfy the three properties, $D_\beta(v_1,\ldots,v_n)=0$ if some $v_i=v_j, i \neq j$ $D_\beta$ is linear in each factor and $D_\beta(w_1,\ldots,w_n)=1$ Moreover, I want to be able to prove some things using this. For example that Applying D to some vectors v will be the same as taking the determinant of the coordinate representation of those v, that applying D and getting something non zero to a set of vectors implys those vectors are not a basis, etc. I know that in the case of two dimensional, we can do this by using the idea that the determinant gives the area of a parallelepiped, etc. And if we define the function to be that that satisfies the above properties then it can be easily shown to be unique. But in cases when it is not dim 2, since I dont have a general formula for the n vectors , how can this be done? Is it safe to assume that such a function exists and then just show it is unique, or must it be shown that even such a function exists at all? And say we could just define the function that satisfies those properties. Then how would we show it is unique, etc? Is it possible to maybe do this problem using matrices? Ie, we can define the determinant on the nxn matrices as our inputs, and somehow use that to prove things? I have not learned about things called alternating maps. I am still trying to do this , using advice in answers, is this how it should be approached? Define a mapping $f: L(V^{n}, \mathbb{F}) \to \mathbb{F}$ by $f \to f(e_1,...,e_n)$, or should it be as $f \to f(x_1,...x_n)$? Then $f$ is injective as if $f_{1}(e_1,...,e_n)=f_{2}(e_1,...,e_n)$ then would this imply they are equal as how a function acts on a basis completely determines it? I a just really confused. and then If I could show that it is subjective and thus an isomorphism I could say choose that unique f which is the one that gives $f(x_1,\ldots,x_n)=0$ and claim this is the function $D_\beta$ I wanted. But I am having lots of trouble putting it all together. I still dont understand. It seems like the top answer is getting many vote, but I dont understand it. This is only a first course in linear algebra by the way, so I do not know about many advanced results. Maybe the answer could rely on sgn function and permutations. Then I could have a general formula I dont know how in some of the answers we can just simply say suppose D is alternating etc, when this is one of the things I want to prove. That is what is also confusing me. I dont know what we can even start with. Or if we must start from scratch completely. Looking for advice. If this cannot be done so easily, I would also be happy to see that if we assume such a function does exist, then at least prove it is unique. Without just saying the determinant is that function and the determinant is unique. Thank you",,"['linear-algebra', 'vector-spaces', 'determinant']"
57,"A tricky problem about matrices with no $\{-1,0,1\}$ vector in their kernel",A tricky problem about matrices with no  vector in their kernel,"\{-1,0,1\}","A Hankel matrix is a square matrix in which each ascending skew-diagonal from left to right is constant. Let us call a matrix partial Hankel if it is the first $m<n$ rows of some $n$ by $n$ Hankel matrix. I was posed this question by a mathematician a few years ago and I have been completely stuck on it ever since.   Let us assume all our partial Hankel matrices have elements which are chosen from $\{-1,1\}$. Does there exist an $m$ by $n$ partial Hankel matrix $M$ such that it has $m < n/2$ rows and there is no non-zero vector $v\in\{-1,0,1\}^n$ in its kernel? Such matrices certainly do exist for non-Hankel $m$ by $n$ $\{-1,1\}$-matrices. Other that writing computer code to exhaustively explore small matrices I am not sure how to approach this question. I would be very grateful for any ideas at all.","A Hankel matrix is a square matrix in which each ascending skew-diagonal from left to right is constant. Let us call a matrix partial Hankel if it is the first $m<n$ rows of some $n$ by $n$ Hankel matrix. I was posed this question by a mathematician a few years ago and I have been completely stuck on it ever since.   Let us assume all our partial Hankel matrices have elements which are chosen from $\{-1,1\}$. Does there exist an $m$ by $n$ partial Hankel matrix $M$ such that it has $m < n/2$ rows and there is no non-zero vector $v\in\{-1,0,1\}^n$ in its kernel? Such matrices certainly do exist for non-Hankel $m$ by $n$ $\{-1,1\}$-matrices. Other that writing computer code to exhaustively explore small matrices I am not sure how to approach this question. I would be very grateful for any ideas at all.",,['linear-algebra']
58,Proving Things About Rings Using Things About Vector Spaces,Proving Things About Rings Using Things About Vector Spaces,,"All rings below are assumed to be commutative and having an identity. $\newcommand{\bw}{\bigwedge}\newcommand{\R}{\mathbf R}\newcommand{\mc}{\mathcal}$ Consider the following problem: Problem 1. Let $M$ and $N$ be $n\times n$ matrices with entries from a field $F$ of characteristic $0$. Then   $$\det(MN)=\det M\det N\tag{1}$$ The above can be neatly proved using exterior algebras (I have hidden the details of this since this is not important here. Doing this seems to reduce the length of the post. Long posts tend to decrease the number of readers.): We know that given given a linear operator $T$ on an $n$-dimensional vector space based on the field $F$, we have a induced map $\bw^k T:\bw^kV\to \bw^k V$. If $k=n$, then since $\dim(\bw^n V)=1$, we kwow that there is a unique constant $c$ such that $\bw^n T=c I$, where $I$ is the identity map on $\bw^n V$. It is easily shown that $c$ is same as the determinant of the matrix of $T$ which respect to any basis of $V$. Now if $T, S\in \mathcal L(V)$, then $$ \det(ST)(v_1\wedge \cdots \wedge v_n)=\bigwedge^n(TS)(v_1\wedge \cdots \wedge v_n) \\=TS(v_1)\wedge\cdots TS(v_n)=\bw^n T(Sv_1\wedge \cdots \wedge Sv_n) =\det S\ \bw^n T(v_1\wedge\cdots\wedge v_n) \\=\det S\det T(v_1\wedge\cdots\wedge v_n) $$ Of course, equation (1) remains true even if we have the entries of $M$ and $N$ coming from a ring. I am not aware of any analog of exterior algebras for arbitrary rings. So I have a workaround. Consider the polynomial ring $A=\R[x_{ij},y_{ij}:\ 1\leq i,j\leq n]$ in $2n^2$ indeteminates $x_{ij}$ and $y_{ij}$. Let $\mc M$ be the $n\times n$ matrix whose $i,j$-th entry is $x_{ij}$ and $\mc N$ be the matrix whose $i, j$-th entry is $y_{ij}$. Consider the polynomials $P, Q$ and $R$ in $A$ defined as $P=\det(\mc M\mc N)$, $Q=\det \mc M$, and $R=\det \mc N$. Now let $S\in A$ be defined as $P-QR$. We know from (1) that $S$ evaluates to $0$ identically and thus $S$ must be the zero polynomial in $A$. Note that $S$ can be regarded as a polynomial in $Z:=\mathbb Z[x_{ij}, y_{ij}:\ 1\leq i, j\leq n]$. Now suppose we have matrices $M$ and $N$ with entries from a ring $B$. By the universal property of polynomial rings, we have a unique ring homomorphism $f:Z\to B$ which carries $x_{ij}$ to the $i,j$-th entry of $M$ and $y_{ij}$ to the $i,j$-th entry of $N$. Now since $S$ is the zero polynomial, we have $f(S)=0$. But $f(S)=\det(MN)-\det M\det N$. And thus we have proved (1) in the general setting of rings without doing much symbolic computation. Does the above make sense? It seems a bit artificial. Is there a more direct way (of course, apart from playing with symbols). This approach (provided all is well) can be used to prove other similar results. Thanks.","All rings below are assumed to be commutative and having an identity. $\newcommand{\bw}{\bigwedge}\newcommand{\R}{\mathbf R}\newcommand{\mc}{\mathcal}$ Consider the following problem: Problem 1. Let $M$ and $N$ be $n\times n$ matrices with entries from a field $F$ of characteristic $0$. Then   $$\det(MN)=\det M\det N\tag{1}$$ The above can be neatly proved using exterior algebras (I have hidden the details of this since this is not important here. Doing this seems to reduce the length of the post. Long posts tend to decrease the number of readers.): We know that given given a linear operator $T$ on an $n$-dimensional vector space based on the field $F$, we have a induced map $\bw^k T:\bw^kV\to \bw^k V$. If $k=n$, then since $\dim(\bw^n V)=1$, we kwow that there is a unique constant $c$ such that $\bw^n T=c I$, where $I$ is the identity map on $\bw^n V$. It is easily shown that $c$ is same as the determinant of the matrix of $T$ which respect to any basis of $V$. Now if $T, S\in \mathcal L(V)$, then $$ \det(ST)(v_1\wedge \cdots \wedge v_n)=\bigwedge^n(TS)(v_1\wedge \cdots \wedge v_n) \\=TS(v_1)\wedge\cdots TS(v_n)=\bw^n T(Sv_1\wedge \cdots \wedge Sv_n) =\det S\ \bw^n T(v_1\wedge\cdots\wedge v_n) \\=\det S\det T(v_1\wedge\cdots\wedge v_n) $$ Of course, equation (1) remains true even if we have the entries of $M$ and $N$ coming from a ring. I am not aware of any analog of exterior algebras for arbitrary rings. So I have a workaround. Consider the polynomial ring $A=\R[x_{ij},y_{ij}:\ 1\leq i,j\leq n]$ in $2n^2$ indeteminates $x_{ij}$ and $y_{ij}$. Let $\mc M$ be the $n\times n$ matrix whose $i,j$-th entry is $x_{ij}$ and $\mc N$ be the matrix whose $i, j$-th entry is $y_{ij}$. Consider the polynomials $P, Q$ and $R$ in $A$ defined as $P=\det(\mc M\mc N)$, $Q=\det \mc M$, and $R=\det \mc N$. Now let $S\in A$ be defined as $P-QR$. We know from (1) that $S$ evaluates to $0$ identically and thus $S$ must be the zero polynomial in $A$. Note that $S$ can be regarded as a polynomial in $Z:=\mathbb Z[x_{ij}, y_{ij}:\ 1\leq i, j\leq n]$. Now suppose we have matrices $M$ and $N$ with entries from a ring $B$. By the universal property of polynomial rings, we have a unique ring homomorphism $f:Z\to B$ which carries $x_{ij}$ to the $i,j$-th entry of $M$ and $y_{ij}$ to the $i,j$-th entry of $N$. Now since $S$ is the zero polynomial, we have $f(S)=0$. But $f(S)=\det(MN)-\det M\det N$. And thus we have proved (1) in the general setting of rings without doing much symbolic computation. Does the above make sense? It seems a bit artificial. Is there a more direct way (of course, apart from playing with symbols). This approach (provided all is well) can be used to prove other similar results. Thanks.",,"['linear-algebra', 'polynomials', 'ring-theory', 'determinant', 'multilinear-algebra']"
59,Determinant of a special $4\times 4$ matrix,Determinant of a special  matrix,4\times 4,"Let $f(x)=\sum_{k=1}^{4}a_{k}x^{k},\varepsilon =\cos\frac{\pi}{2}+i\sin\frac{\pi}{2}.$ $\qquad\qquad 4\times 4$ matrix $$T=\begin{bmatrix}  1&  a_{2}&  a_{3}& a_{4}\\   1&  a_{1}&  a_{2}& a_{3}\\   1&  a_{4}&  a_{1}& a_{2}\\   0&  \varepsilon^{2}&  \varepsilon&  1\end{bmatrix}$$ Show that $$\det(T)=f(\varepsilon^{2})f(\varepsilon^{3})$$ Further I can generalize this question : Let $f(x)=\sum_{k=1}^{n}a_{k}x^{k},\varepsilon =\cos\frac{2\pi}{n}+i\sin\frac{2\pi}{n}.$ $\qquad\qquad n\times n$ matrix $$T=\begin{bmatrix}  1&  a_{2}&  a_{3} & \cdots & a_{n}\\   1&  a_{1}&  a_{2}&\cdots & a_{n-1}\\   \cdots&   \cdots&   \cdots&\cdots &  \cdots\\  1&  a_{4}&  a_{5}& \cdots &a_{2}\\   0&  \varepsilon^{n-2}&  \varepsilon^{n-3}& \cdots& 1\end{bmatrix}$$ Show that $$\det(T)=f(\varepsilon^{2})f(\varepsilon^{3}) \cdots f(\varepsilon^{n-1})$$ Let $$A=\begin{bmatrix}   1& 0&  a_{3}&  a_{4}& a_{1}\\  0& 1&  a_{2}&  a_{3}& a_{4}\\   0& 1&  a_{1}&  a_{2}& a_{3}\\   0& 1&  a_{4}&  a_{1}& a_{2}\\   0& 0&  \varepsilon^{2}&  \varepsilon&  1\end{bmatrix}$$ then $\det(T)=\det(A)$.  Now add $\varepsilon^{2}$ of row 4 to row 1, add $\varepsilon^{4}$ of row 3 to row 1, add $\varepsilon^{6}$ of row 2 to row 1, we get $$A=\begin{bmatrix}   1& 0&  a_{3}&  a_{4}& a_{1}\\  0& 1&  a_{2}&  a_{3}& a_{4}\\   0& 1&  a_{1}&  a_{2}& a_{3}\\   0& 1&  a_{4}&  a_{1}& a_{2}\\   0& 0&  \varepsilon^{2}&  \varepsilon&  1\end{bmatrix}\longrightarrow \begin{bmatrix}   1& 0&  \varepsilon^{4}f(\varepsilon^{2})&  \varepsilon^{2}f(\varepsilon^{2})& f(\varepsilon^{2})\\  0& 1&  a_{2}&  a_{3}& a_{4}\\   0& 1&  a_{1}&  a_{2}& a_{3}\\   0& 1&  a_{4}&  a_{1}& a_{2}\\   0& 0&  \varepsilon^{2}&  \varepsilon&  1\end{bmatrix}=A_{1}$$ How can I separate $f(\varepsilon^{2})$ from $\det(A_{1})$? If you have another proof to my question,please give me some hints. Any help would be appreciated","Let $f(x)=\sum_{k=1}^{4}a_{k}x^{k},\varepsilon =\cos\frac{\pi}{2}+i\sin\frac{\pi}{2}.$ $\qquad\qquad 4\times 4$ matrix $$T=\begin{bmatrix}  1&  a_{2}&  a_{3}& a_{4}\\   1&  a_{1}&  a_{2}& a_{3}\\   1&  a_{4}&  a_{1}& a_{2}\\   0&  \varepsilon^{2}&  \varepsilon&  1\end{bmatrix}$$ Show that $$\det(T)=f(\varepsilon^{2})f(\varepsilon^{3})$$ Further I can generalize this question : Let $f(x)=\sum_{k=1}^{n}a_{k}x^{k},\varepsilon =\cos\frac{2\pi}{n}+i\sin\frac{2\pi}{n}.$ $\qquad\qquad n\times n$ matrix $$T=\begin{bmatrix}  1&  a_{2}&  a_{3} & \cdots & a_{n}\\   1&  a_{1}&  a_{2}&\cdots & a_{n-1}\\   \cdots&   \cdots&   \cdots&\cdots &  \cdots\\  1&  a_{4}&  a_{5}& \cdots &a_{2}\\   0&  \varepsilon^{n-2}&  \varepsilon^{n-3}& \cdots& 1\end{bmatrix}$$ Show that $$\det(T)=f(\varepsilon^{2})f(\varepsilon^{3}) \cdots f(\varepsilon^{n-1})$$ Let $$A=\begin{bmatrix}   1& 0&  a_{3}&  a_{4}& a_{1}\\  0& 1&  a_{2}&  a_{3}& a_{4}\\   0& 1&  a_{1}&  a_{2}& a_{3}\\   0& 1&  a_{4}&  a_{1}& a_{2}\\   0& 0&  \varepsilon^{2}&  \varepsilon&  1\end{bmatrix}$$ then $\det(T)=\det(A)$.  Now add $\varepsilon^{2}$ of row 4 to row 1, add $\varepsilon^{4}$ of row 3 to row 1, add $\varepsilon^{6}$ of row 2 to row 1, we get $$A=\begin{bmatrix}   1& 0&  a_{3}&  a_{4}& a_{1}\\  0& 1&  a_{2}&  a_{3}& a_{4}\\   0& 1&  a_{1}&  a_{2}& a_{3}\\   0& 1&  a_{4}&  a_{1}& a_{2}\\   0& 0&  \varepsilon^{2}&  \varepsilon&  1\end{bmatrix}\longrightarrow \begin{bmatrix}   1& 0&  \varepsilon^{4}f(\varepsilon^{2})&  \varepsilon^{2}f(\varepsilon^{2})& f(\varepsilon^{2})\\  0& 1&  a_{2}&  a_{3}& a_{4}\\   0& 1&  a_{1}&  a_{2}& a_{3}\\   0& 1&  a_{4}&  a_{1}& a_{2}\\   0& 0&  \varepsilon^{2}&  \varepsilon&  1\end{bmatrix}=A_{1}$$ How can I separate $f(\varepsilon^{2})$ from $\det(A_{1})$? If you have another proof to my question,please give me some hints. Any help would be appreciated",,['linear-algebra']
60,Finding a basis for subspace of polynomials,Finding a basis for subspace of polynomials,,"Let $V=\mathscr{P}_{3}$ be the vector space of polynomials of degree 3. Let W be the subspace of polynomials p(x) such that p(0)= 0 and p(1)= 0. Find a basis for W. Extend the basis to a basis of V. Here is what I've done so far. $$p(x) = ax^3 + bx^2 + cx + d$$ $$p(0) = 0 = ax^3 + bx^2 + cx + d\\\text{d = 0}\\ p(1) = 0 = ax^3 + bx^2 + cx + 0 => a + b + c = 0\\ c = -a - b\\ p(x)= ax^3 + bx^2 + (-a-b)x = 0\\ = a(x^3-x) + b(x^2-x)\\ \text{Basis is {(x^3-x),(x^2-x)}} $$ Would this be a correct basis for W, and how would I extend it to the vector space V?","Let $V=\mathscr{P}_{3}$ be the vector space of polynomials of degree 3. Let W be the subspace of polynomials p(x) such that p(0)= 0 and p(1)= 0. Find a basis for W. Extend the basis to a basis of V. Here is what I've done so far. $$p(x) = ax^3 + bx^2 + cx + d$$ $$p(0) = 0 = ax^3 + bx^2 + cx + d\\\text{d = 0}\\ p(1) = 0 = ax^3 + bx^2 + cx + 0 => a + b + c = 0\\ c = -a - b\\ p(x)= ax^3 + bx^2 + (-a-b)x = 0\\ = a(x^3-x) + b(x^2-x)\\ \text{Basis is {(x^3-x),(x^2-x)}} $$ Would this be a correct basis for W, and how would I extend it to the vector space V?",,['linear-algebra']
61,Where can I find linear algebra described in a pointfree manner?,Where can I find linear algebra described in a pointfree manner?,,"Clearly, some of linear algebra can be described in a pointfree fashion. For example, if $X$ is an $R$-module and $A : X \leftarrow X$ is an endomorphism of $X$, then we can define that the ""eigenspace function of $A$"" is the map $\mathrm{Eig}_A : \mathrm{Sub}(X) \leftarrow R$ described by the following equalizer. $$\mathrm{Eig}_A(\lambda) = \mathrm{Eq}(A,\lambda \cdot \mathrm{id}_X)$$ In fact, this makes sense in any $R$-$\mathbf{Mod}$ enriched category with equalizers. Anyway, I did some googling for ""pointfree linear algebra"" and ""pointless linear algebra"" etc., and nothing really came up, except for an article called ""Point-Free, Set-Free Concrete Linear Algebra"" which really isn't what I'm looking for. So anyway... Question. Where can I find linear algebra described in a pointfree manner?","Clearly, some of linear algebra can be described in a pointfree fashion. For example, if $X$ is an $R$-module and $A : X \leftarrow X$ is an endomorphism of $X$, then we can define that the ""eigenspace function of $A$"" is the map $\mathrm{Eig}_A : \mathrm{Sub}(X) \leftarrow R$ described by the following equalizer. $$\mathrm{Eig}_A(\lambda) = \mathrm{Eq}(A,\lambda \cdot \mathrm{id}_X)$$ In fact, this makes sense in any $R$-$\mathbf{Mod}$ enriched category with equalizers. Anyway, I did some googling for ""pointfree linear algebra"" and ""pointless linear algebra"" etc., and nothing really came up, except for an article called ""Point-Free, Set-Free Concrete Linear Algebra"" which really isn't what I'm looking for. So anyway... Question. Where can I find linear algebra described in a pointfree manner?",,"['linear-algebra', 'reference-request', 'category-theory']"
62,$L^2$ Bounds for Markov Chains.,Bounds for Markov Chains.,L^2,"Consider a non-negative, square stochastic $n \times n$ matrix $P$ (rows sum to one, $P$ is ergodic). We are interested in characterizing the set of $n \times n$ invertible matrices $A$ such that we have: $\| APA^{-1} \|_2 \leq 1$ Here $ \| \cdot  \|_2$ means a matrix norm induced by the vector norm $L_2$. One example is $\operatorname{diag}(\xi)^{1/2}$, where $\xi$ is the vector representing the stationary distribution and $\operatorname{diag}$ is an operator constructing a diagonal matrix with zeros off the diagonal. I am interested in a rule defining all (or at least a large class) of valid choices for $A$.","Consider a non-negative, square stochastic $n \times n$ matrix $P$ (rows sum to one, $P$ is ergodic). We are interested in characterizing the set of $n \times n$ invertible matrices $A$ such that we have: $\| APA^{-1} \|_2 \leq 1$ Here $ \| \cdot  \|_2$ means a matrix norm induced by the vector norm $L_2$. One example is $\operatorname{diag}(\xi)^{1/2}$, where $\xi$ is the vector representing the stationary distribution and $\operatorname{diag}$ is an operator constructing a diagonal matrix with zeros off the diagonal. I am interested in a rule defining all (or at least a large class) of valid choices for $A$.",,"['linear-algebra', 'matrices', 'normed-spaces', 'markov-chains', 'control-theory']"
63,How do I close the gap between intuitively knowing something is true vs being able to prove it?,How do I close the gap between intuitively knowing something is true vs being able to prove it?,,"For example, one of my review problems is: Let $S_k$ be the kernel of $T^k$. Show there is a $K$ such that $S_K = S_{K+1} = \cdots$ Somewhere in the back of my brain there's an intuition that told me, ""Well duh , $K = \dim(T)$."" Obviously (to me anyway) once $K$ gets bigger than $\dim(T)$, you're either cycling around in some $T$-invariant subspace, or you're in the null space. My brain got there by envisioning a matrix in my head and thinking about what would happen to each vector in the domain until it was satisfied that $K=\dim(T)$ satisfies the prompt. At this point that part of my brain was content that a solution exists and moved on to something more interesting. But if I sit down and try to prove it with only the properties of transformations and subspaces, I don't even know where to start. Do I take a basis? Do I count dimensions? Nothing I try seems to get me anywhere. I think my brain is thinking about it the wrong way. Lower division math is about manipulating formulas and calculating. I got pretty good at doing that, and now my brain seems to attack every problem that way. I get the sense from talking to other (smarter) people that proofs are different. When my instructors come up with proofs they seem to be doing something completely different in their minds than I'm doing. To me it seems more akin to solving a puzzle than to manipulating equations. I don't see what they're doing that makes it so clear to them, in the way that lower division stuff is clear to me. I've heard many tips, including ""Write the first line and the last line of your proof, and then try to fill in the gaps."" And also, ""Write statements for everything you know is true in one place."" And also, ""Write as many statements as you can until you see something that can help you make the conclusion you need."" And so on. Those are good tips that help simplify the problem, but I feel like the real solution is rewiring my brain to think in a different way. Sitting here and looking at and doing dozens of proofs hasn't gotten me anywhere, so I'm hoping for some insight from some people smarter than I am.","For example, one of my review problems is: Let $S_k$ be the kernel of $T^k$. Show there is a $K$ such that $S_K = S_{K+1} = \cdots$ Somewhere in the back of my brain there's an intuition that told me, ""Well duh , $K = \dim(T)$."" Obviously (to me anyway) once $K$ gets bigger than $\dim(T)$, you're either cycling around in some $T$-invariant subspace, or you're in the null space. My brain got there by envisioning a matrix in my head and thinking about what would happen to each vector in the domain until it was satisfied that $K=\dim(T)$ satisfies the prompt. At this point that part of my brain was content that a solution exists and moved on to something more interesting. But if I sit down and try to prove it with only the properties of transformations and subspaces, I don't even know where to start. Do I take a basis? Do I count dimensions? Nothing I try seems to get me anywhere. I think my brain is thinking about it the wrong way. Lower division math is about manipulating formulas and calculating. I got pretty good at doing that, and now my brain seems to attack every problem that way. I get the sense from talking to other (smarter) people that proofs are different. When my instructors come up with proofs they seem to be doing something completely different in their minds than I'm doing. To me it seems more akin to solving a puzzle than to manipulating equations. I don't see what they're doing that makes it so clear to them, in the way that lower division stuff is clear to me. I've heard many tips, including ""Write the first line and the last line of your proof, and then try to fill in the gaps."" And also, ""Write statements for everything you know is true in one place."" And also, ""Write as many statements as you can until you see something that can help you make the conclusion you need."" And so on. Those are good tips that help simplify the problem, but I feel like the real solution is rewiring my brain to think in a different way. Sitting here and looking at and doing dozens of proofs hasn't gotten me anywhere, so I'm hoping for some insight from some people smarter than I am.",,"['linear-algebra', 'proof-writing', 'intuition']"
64,"Show that $A\in\mathbb{C}_n$ is normal $\iff$ $tr(A*A) = \sum_{i = 1}^n|\lambda_i|^2$, where $\lambda_1,...,\lambda_n$ are the eigenvalues of $A$.","Show that  is normal  , where  are the eigenvalues of .","A\in\mathbb{C}_n \iff tr(A*A) = \sum_{i = 1}^n|\lambda_i|^2 \lambda_1,...,\lambda_n A","Title restated: Show that $A\in\mathbb{C}_n$ is normal $\iff$ $tr(A^*A) = \sum_{i = 1}^n|\lambda_i|^2$, where $\lambda_1,...,\lambda_n$ are the eigenvalues of $A$. This question comes from ""Matrices and Linear Transformations"" by Charles Cullen. I'm studying for an exam and am trying to do some problems but I'm having trouble with this one. Any help would be appreciated. Thank you.","Title restated: Show that $A\in\mathbb{C}_n$ is normal $\iff$ $tr(A^*A) = \sum_{i = 1}^n|\lambda_i|^2$, where $\lambda_1,...,\lambda_n$ are the eigenvalues of $A$. This question comes from ""Matrices and Linear Transformations"" by Charles Cullen. I'm studying for an exam and am trying to do some problems but I'm having trouble with this one. Any help would be appreciated. Thank you.",,"['linear-algebra', 'matrices']"
65,Show that the inverse of a strictly diagonally dominant matrix is monotone,Show that the inverse of a strictly diagonally dominant matrix is monotone,,"I have been struggling with this problem for awhile. Given that $A$ is a strictly diagonally dominant matrix with positive diagonal entries and non-positive off-diagonal entries, show that $A$ is monotone, i.e. $A^{-1} \geq 0$, meaning $a^{-1}_{ij} \geq 0$ for all $i,j$. I have looked extensively for some help on this problem but have not come up with anything. Any help or a link to the right resource would help me out immensely!","I have been struggling with this problem for awhile. Given that $A$ is a strictly diagonally dominant matrix with positive diagonal entries and non-positive off-diagonal entries, show that $A$ is monotone, i.e. $A^{-1} \geq 0$, meaning $a^{-1}_{ij} \geq 0$ for all $i,j$. I have looked extensively for some help on this problem but have not come up with anything. Any help or a link to the right resource would help me out immensely!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
66,Let $A$ and $B$ in $O_n(\mathbb{R})$. Show that $A$ and $B$ commute.,Let  and  in . Show that  and  commute.,A B O_n(\mathbb{R}) A B,"Let $A$ and $B$ in $O_n(\mathbb{R})$ (orthogonal matrices) such that $|||B-I_n|||<\sqrt{2}$ (subordinate norm) and $A$ commute with $BAB^{-1}$ . Show that $A$ and $B$ commute. My 'attempt': I know that $B^{-1}=B^{T}.$ We have $$ABAB^{T}=BAB^{T}A.$$ Since $A,B \in On(\mathbb{R})$ then $AA^{T}=I_n$ and $BB^{T}=I_n$ . Unfortunately I do not see how can I use the fact that $|||B-I_n|||<\sqrt{2}$ . Thank you in advance for your help.",Let and in (orthogonal matrices) such that (subordinate norm) and commute with . Show that and commute. My 'attempt': I know that We have Since then and . Unfortunately I do not see how can I use the fact that . Thank you in advance for your help.,"A B O_n(\mathbb{R}) |||B-I_n|||<\sqrt{2} A BAB^{-1} A B B^{-1}=B^{T}. ABAB^{T}=BAB^{T}A. A,B \in On(\mathbb{R}) AA^{T}=I_n BB^{T}=I_n |||B-I_n|||<\sqrt{2}",['linear-algebra']
67,A question about linear subspace and subfield,A question about linear subspace and subfield,,"If we see the complex numbers $\mathbb{C}$ as a linear space on field $\mathbb{Q}$, then $\mathbb{C}$ is infinite dimensional over $\mathbb{Q}$, $2$-dimensional over $\mathbb{R}$, $1$-dimensional over $\mathbb{C}$ itself. My question: Is there a field $F\subset\mathbb{R}$, $\mathbb{R}$ is $2$-dimensional linear space over $F$?","If we see the complex numbers $\mathbb{C}$ as a linear space on field $\mathbb{Q}$, then $\mathbb{C}$ is infinite dimensional over $\mathbb{Q}$, $2$-dimensional over $\mathbb{R}$, $1$-dimensional over $\mathbb{C}$ itself. My question: Is there a field $F\subset\mathbb{R}$, $\mathbb{R}$ is $2$-dimensional linear space over $F$?",,"['linear-algebra', 'extension-field']"
68,Minimal polynomial of $T(A) = A^\top - A$,Minimal polynomial of,T(A) = A^\top - A,"As said in the title , I need to find the minimal polynomial of the linear transformation $$T(A)=A^\top-A.$$ The matrices are $M_n(\mathbb{C})$ . I've figured out that $T^2 = 2A - 2A^t$ , so a polynomial $p(t) = t^2 + 2t$ works so $p(T) = 0$ . Now $p(t)$ breaks to $t(t+2)$ but non of them kills T. Therefore $p(t)$ is the minimal polynomial. I'm having trouble with this, because I guessed $p(t)$ , and Im not sure on how to actually find the polynomial. For example, I have no idea how to find a matrix, because of that transpose. Is there another way to do this?","As said in the title , I need to find the minimal polynomial of the linear transformation The matrices are . I've figured out that , so a polynomial works so . Now breaks to but non of them kills T. Therefore is the minimal polynomial. I'm having trouble with this, because I guessed , and Im not sure on how to actually find the polynomial. For example, I have no idea how to find a matrix, because of that transpose. Is there another way to do this?",T(A)=A^\top-A. M_n(\mathbb{C}) T^2 = 2A - 2A^t p(t) = t^2 + 2t p(T) = 0 p(t) t(t+2) p(t) p(t),"['linear-algebra', 'matrices', 'linear-transformations', 'minimal-polynomials', 'transpose']"
69,Can we classify commuting pairs of matrices up to conjugacy?,Can we classify commuting pairs of matrices up to conjugacy?,,"Recall that two $n\times n$ matrices over $\mathbb{C}$ are conjugate if and only if they have the same Jordan canonical form . Question. Is there a similar classification for commuting pairs of matrices? To be precise, a commuting pair of matrices is an ordered pair $(A,B)$ of $n\times n$ matrices over $\mathbb{C}$ for which $AB=BA$. Two commuting pairs $(A,B)$ and $(A',B\,')$ are conjugate if there exists a nonsingular matrix $P$ so that $$ PAP^{-1} = A'\qquad\text{and}\qquad PBP^{-1} = B\,'. $$ What are the equivalence classes under this relation?  Can we enumerate them?  Can we check whether two pairs are in the same class? A few notes: If $A$ and $B$ are diagonalizable, they can be simultaneously diagonalized, and the resulting pair of diagonal matrices determines the conjugacy class. According to this answer , a commuting pair of matrices cannot in general be  simultaneously Jordanized. According to this Wikipedia article , a commuting pair of matrices can be simultaneously triangularized, but of course the pair of triangular matrices is not uniquely determined. By the way, this question is equivalent to asking for a classification of indecomposable modules over $\mathbb{C}[x,y]$ with finite dimension over $\mathbb{C}$.","Recall that two $n\times n$ matrices over $\mathbb{C}$ are conjugate if and only if they have the same Jordan canonical form . Question. Is there a similar classification for commuting pairs of matrices? To be precise, a commuting pair of matrices is an ordered pair $(A,B)$ of $n\times n$ matrices over $\mathbb{C}$ for which $AB=BA$. Two commuting pairs $(A,B)$ and $(A',B\,')$ are conjugate if there exists a nonsingular matrix $P$ so that $$ PAP^{-1} = A'\qquad\text{and}\qquad PBP^{-1} = B\,'. $$ What are the equivalence classes under this relation?  Can we enumerate them?  Can we check whether two pairs are in the same class? A few notes: If $A$ and $B$ are diagonalizable, they can be simultaneously diagonalized, and the resulting pair of diagonal matrices determines the conjugacy class. According to this answer , a commuting pair of matrices cannot in general be  simultaneously Jordanized. According to this Wikipedia article , a commuting pair of matrices can be simultaneously triangularized, but of course the pair of triangular matrices is not uniquely determined. By the way, this question is equivalent to asking for a classification of indecomposable modules over $\mathbb{C}[x,y]$ with finite dimension over $\mathbb{C}$.",,"['linear-algebra', 'matrices', 'modules']"
70,Positive semi-definite matrix problem,Positive semi-definite matrix problem,,"If $A，B$ and $M$ are positive semi-definite matrices, and we have $$ A+B \succeq M .$$ Do there always exist two positive semi-definite matrices $ M_{1}, M_{2}  , $ such that $$   A \succeq M_{1},    \quad B \succeq M_{2},   \quad M_{1}+M_{2}=M  ?$$","If $A，B$ and $M$ are positive semi-definite matrices, and we have $$ A+B \succeq M .$$ Do there always exist two positive semi-definite matrices $ M_{1}, M_{2}  , $ such that $$   A \succeq M_{1},    \quad B \succeq M_{2},   \quad M_{1}+M_{2}=M  ?$$",,"['linear-algebra', 'matrices']"
71,Prove that if $A^2x=x$ then $Ax=x$,Prove that if  then,A^2x=x Ax=x,I feel this should be easy but I cant solve this problem: Prove that if $A$ is a $n\times n$ matrix and $x$ a vector in $\mathbb R^n$ both with real positive entries and $A^2x=x$ then $Ax=x$. I looking at the terms of the sum that defines the product $AAx$ and comparing with the entries of $x$ but I get nowhere. Can you give me any hints?,I feel this should be easy but I cant solve this problem: Prove that if $A$ is a $n\times n$ matrix and $x$ a vector in $\mathbb R^n$ both with real positive entries and $A^2x=x$ then $Ax=x$. I looking at the terms of the sum that defines the product $AAx$ and comparing with the entries of $x$ but I get nowhere. Can you give me any hints?,,"['linear-algebra', 'matrices']"
72,Advice on Understanding Vector Spaces and Subspaces,Advice on Understanding Vector Spaces and Subspaces,,"currently I am studying Vector spaces and sub spaces. I enjoyed working with matrices and using the Gaussian-Jordon elimination and I also had no problems with cofactor expansion and determinants in general. But for some reason I lost track when it came to vectors. I understand the geometrical representation of $\mathbb R^2$ and $\mathbb R^3$ and how to solve for angles and areas of a parallelogram. I have a hard time thinking abstractly and I think that this is currently the problem why I don't grasp vector spaces. Do you have any advice on studying this material. I know I am not brilliant in math yet, but I want to study it and take more advanced topics because I see the beauty in math and how it applies to the real world. When I work through the proofs I am unable to see the turning point or the ""a-ha"" effect. The proofs are not in numbers so I can't even check my results if I am doing it right. Is there actually a method to train abstract thinking? I really appreciate any advice on this matter even though it is not the usually question asked here. Thank you for your time reading this and your effort in possible answers. -Daniel","currently I am studying Vector spaces and sub spaces. I enjoyed working with matrices and using the Gaussian-Jordon elimination and I also had no problems with cofactor expansion and determinants in general. But for some reason I lost track when it came to vectors. I understand the geometrical representation of $\mathbb R^2$ and $\mathbb R^3$ and how to solve for angles and areas of a parallelogram. I have a hard time thinking abstractly and I think that this is currently the problem why I don't grasp vector spaces. Do you have any advice on studying this material. I know I am not brilliant in math yet, but I want to study it and take more advanced topics because I see the beauty in math and how it applies to the real world. When I work through the proofs I am unable to see the turning point or the ""a-ha"" effect. The proofs are not in numbers so I can't even check my results if I am doing it right. Is there actually a method to train abstract thinking? I really appreciate any advice on this matter even though it is not the usually question asked here. Thank you for your time reading this and your effort in possible answers. -Daniel",,"['linear-algebra', 'soft-question', 'vector-spaces', 'learning']"
73,Question about 8.4 in Humphreys,Question about 8.4 in Humphreys,,"I am reading section 8.4 in Humphreys' book Introduction to Lie Algebras and Representation Theory. He is showing that the only scalar multiples of a root are 1 and -1, but I have trouble understanding his reasoning: He considers the direct sum $M=\bigoplus_{c\in\mathbb{F}} L_{c\alpha}$ for some fixed root $\alpha$. He shows that the only even weights of $h_\alpha$ on $M$ are 0, 2 and -2. Then, he says that this proves that twice a root is never a root. Why is that true? The function $\frac 12\alpha$ could still be a root.","I am reading section 8.4 in Humphreys' book Introduction to Lie Algebras and Representation Theory. He is showing that the only scalar multiples of a root are 1 and -1, but I have trouble understanding his reasoning: He considers the direct sum $M=\bigoplus_{c\in\mathbb{F}} L_{c\alpha}$ for some fixed root $\alpha$. He shows that the only even weights of $h_\alpha$ on $M$ are 0, 2 and -2. Then, he says that this proves that twice a root is never a root. Why is that true? The function $\frac 12\alpha$ could still be a root.",,['linear-algebra']
74,Angle eigenvector makes with $x$-axis,Angle eigenvector makes with -axis,x,"Say I have a symmetric matrix that lives in $xy$ space. I will write it as $$A = \begin{pmatrix} a & b \\ b & c \end{pmatrix}.$$ I am interested in the covariance matrix, so let me also say $a,c>0$ , and $A$ is positive-definite, which means $|b|\leq\sqrt{ac}$ . The eigenvalues of $A$ are therefore $\geq 0$ , and the eigenvectors are orthogonal. The eigenvalue problem $A\mathbf{x}=\lambda\mathbf{x}$ reads $$\begin{pmatrix} a & b \\ b & c \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} = \lambda \begin{pmatrix} x \\ y \end{pmatrix}.$$ The system of equations can be written as \begin{cases} y = x(\lambda-a)/b \\  x = y(\lambda-c)/b, \end{cases} and combined to yield $$ \frac{c-a}{b}=\frac{y}{x}-\frac{x}{y}.$$ In the figure above, the red and blue vectors are the eigenvectors with the larger and smaller eigenvalue respectively. Working with the largest one, I can write the equation above the figure as \begin{align}     \frac{c-a}{b}  &= \tan{\theta}-\tan{\beta} \\                                 &= (1+\tan{\theta}\tan{\beta})\tan(\theta-\beta) \\                                 &= 2\tan(2\theta-\pi/2) \\                                 &=-2\cot(2\theta), \end{align} where to write the second equality I have used the trigonometric identity for the tangent of a difference. The third line follows from writing $\tan \theta \tan \beta = 1$ and $\beta = \pi/2 - \theta$ . Lastly, I used the trigonometric relations between complementary angles to write the last equality, which can be recasted as \begin{equation}     \tan(2\theta) = \frac{2b}{a -c}. \end{equation} The following equation $$\theta = \frac{1}{2}\textrm{atan2}(2b,a-c),$$ where $\textrm{atan2}(y,x)=\textrm{Arg}(x+iy)$ , can be used to compute the angle the eigenvector with largest eigenvalue, $\mathbf{x}$ , makes with the $x$ -direction. Choosing the principal branch, the range of $\theta$ is $(-90,90]^\circ$ , i.e. the right half of the plane. PROBLEM As you may have noticed, I absorbed the eigenvalue $\lambda$ in the equation right before the figure. This means the rest of the derivation should hold for both eigenvectors of $A$ . However, I have tested the calculation with multiple cases and magically the angle which $\theta$ measures is always that between the $x$ -direction and the eigenvector with largest eigenvalue. As an example, we can look at the special case $$A = \begin{pmatrix} a & 0 \\ 0 & c \end{pmatrix},$$ where the eigenvectors are aligned with the coordinate axes, and $a$ and $c$ are the eigenvalues. If $a>c$ , we get $\theta=0^\circ$ . However, if $a < c$ , we get $\theta = 90^\circ$ . Can anybody provide me with an explanation of why the ambiguity in the eigenvector that $\theta$ is describing seems to fade away somehow? I'd really appreciate it! NEW OBSERVATION I have realised something interesting. If I multiply $$\tan(2\theta) = \frac{2b}{a -c}$$ by $-1$ twice, getting $$ \tan(2\theta) = \frac{-2b}{c-a},$$ and I proceed as before and write $$\theta = \frac{1}{2}\textrm{atan2}(-2b,c-a),$$ now the angle is calculated between the $x$ -axis and the eigenvector with the smaller eigenvalue! In the wikipedia article on $\textrm{atan2}$ , specifically the section called "" East-counterclockwise, north-clockwise and south-clockwise conventions, etc. "", it says: Apparently, changing the sign of the x- and/or y-arguments and swapping their positions can create 8 possible variations of the $\mathrm{atan2}$ function and they, interestingly, correspond to 8 possible definitions of the angle, namely, clockwise or counterclockwise starting from each of the 4 cardinal directions, north, east, south and west. I think this brings me closer to the answer to my question but I need some help putting everything together. VISUAL AID Let me call the eigenvectors with larger and smaller eigenvalue $\mathbf{L}$ and $\mathbf{S}$ respectively. Let's look at how a few eigenvectors might look like in the $(a-c,b)$ plane. Consider the sketch that follows. The eigenvectors $\mathbf{L}$ and $\mathbf{S}$ are drawn in red and blue respectively, and the angle they make with the $x$ axis is called $\theta_l$ and $\theta_s$ respectively in the top subplot. As you can see, traversing the $(a-c,b)$ plane clockwise leads to the direction of the eigenvectors in the $xy$ plane rotating clockwise too. The arrow heads indicate the side of the vectors which falls in the range $(-90,90]^\circ$ . I have used Mathematica to produce the following surface plots of the angles in the $(a-c,b)$ plane. This is how the angle between the $x$ axis and $\mathbf{L}$ , given by $$\theta_l=\frac{1}{2}\textrm{atan2}(2b,a-c),$$ looks like: And this is how the angle the $x$ axis makes with $\mathbf{S}$ , $$\theta_s=\frac{1}{2}\textrm{atan2}(-2b,c-a),$$ looks like: If you check the surface values you can see they match what my sketch described. Let's come back to what was described in the wikipedia link. We can see $\textrm{atan2}(y,x)$ follows the "" East-anticlockwise "" convention. We have $\theta_l=0^\circ$ when $\mathbf{L}$ is pointing East, and the angle grows as $\mathbf{L}$ rotates anticlockwise. Now, if we try to make sense of the $\theta_s$ values as if they described $\mathbf{L}$ too, we can see $\theta_s=0^\circ$ when $\mathbf{L}$ is pointing North, and grows as $\mathbf{L}$ rotates anticlockwise. Hence, the $\textrm{atan2}(-y,-x)$ convention might be "" North-anticlockwise "". But again, I don't know what is special about $\mathbf{L}$ . From the derivation of the angles, either equation could have corresponded to either eigenvector. There is still a missing piece of the puzzle which I believe must lie in the derivation of the equation for $\theta_l$ . Can anybody give me a hand? Any insights would be greatly appreciated. SUMMARY The equation $$\tan(2\theta) = \frac{2b}{a -c}$$ holds for the azimuths of both eigenvectors, L and S , of the matrix $$A = \begin{pmatrix} a & b \\ b & c \end{pmatrix}.$$ Let me define 2 new vectors: $\mathbf{p}=(a-c,2b)$ , with azimuth $\gamma_p=\textrm{atan2}(p_y,p_x)$ . $\mathbf{n}=-\mathbf{p}$ , with azimuth $\gamma_n=\textrm{atan2}(-p_y,-p_x)$ . We can write $$\tan(2\theta) = \tan(\gamma_p) = \tan(\gamma_n).$$ It turns out using the first equality we get $$2\theta_l=\gamma_p=\textrm{atan2}(2b,a-c),$$ where $\theta_l$ is the angle between the $x$ axis and the eigenvector with larger eigenvalue, $\mathbf{L}$ . The second equality $$2\theta_s = \gamma_n = \textrm{atan2}(-2b,c-a)$$ gives us $\theta_s$ , the angle between the $x$ axis and the eigenvector with smaller eigenvalue, $\mathbf{S}$ . Now the question remaining is why does that happen? How could I have predicted that $\mathbf{p}$ and $\mathbf{n}$ would always have an azimuth that is twice that of $\mathbf{L}$ and $\mathbf{S}$ respectively? DIAGONALISATION I have found this derivation from Howard E. Haber from his Physics 116A class of Winter 2011. He obtains the same equation for $\tan(2\theta)$ by diagonalising the matrix $A$ (note the difference in notation: he uses $b$ in $A_{22}$ , and $c$ in the off-diagonal terms). He then proceeds by setting constraints in the angle $\theta$ . When he plugs his eq. 1 in his eq. 8 he makes it explicit that $\theta$ is measuring the angle between the positive $x$ axis and the eigenvector with largest eigenvalue. The conclusions drawn are the same as mine, but I somehow bypassed all that when I decided to use the function atan2 (unjustifiably, but it works). The question remains: why does my approach of using atan2 work?","Say I have a symmetric matrix that lives in space. I will write it as I am interested in the covariance matrix, so let me also say , and is positive-definite, which means . The eigenvalues of are therefore , and the eigenvectors are orthogonal. The eigenvalue problem reads The system of equations can be written as and combined to yield In the figure above, the red and blue vectors are the eigenvectors with the larger and smaller eigenvalue respectively. Working with the largest one, I can write the equation above the figure as where to write the second equality I have used the trigonometric identity for the tangent of a difference. The third line follows from writing and . Lastly, I used the trigonometric relations between complementary angles to write the last equality, which can be recasted as The following equation where , can be used to compute the angle the eigenvector with largest eigenvalue, , makes with the -direction. Choosing the principal branch, the range of is , i.e. the right half of the plane. PROBLEM As you may have noticed, I absorbed the eigenvalue in the equation right before the figure. This means the rest of the derivation should hold for both eigenvectors of . However, I have tested the calculation with multiple cases and magically the angle which measures is always that between the -direction and the eigenvector with largest eigenvalue. As an example, we can look at the special case where the eigenvectors are aligned with the coordinate axes, and and are the eigenvalues. If , we get . However, if , we get . Can anybody provide me with an explanation of why the ambiguity in the eigenvector that is describing seems to fade away somehow? I'd really appreciate it! NEW OBSERVATION I have realised something interesting. If I multiply by twice, getting and I proceed as before and write now the angle is calculated between the -axis and the eigenvector with the smaller eigenvalue! In the wikipedia article on , specifically the section called "" East-counterclockwise, north-clockwise and south-clockwise conventions, etc. "", it says: Apparently, changing the sign of the x- and/or y-arguments and swapping their positions can create 8 possible variations of the function and they, interestingly, correspond to 8 possible definitions of the angle, namely, clockwise or counterclockwise starting from each of the 4 cardinal directions, north, east, south and west. I think this brings me closer to the answer to my question but I need some help putting everything together. VISUAL AID Let me call the eigenvectors with larger and smaller eigenvalue and respectively. Let's look at how a few eigenvectors might look like in the plane. Consider the sketch that follows. The eigenvectors and are drawn in red and blue respectively, and the angle they make with the axis is called and respectively in the top subplot. As you can see, traversing the plane clockwise leads to the direction of the eigenvectors in the plane rotating clockwise too. The arrow heads indicate the side of the vectors which falls in the range . I have used Mathematica to produce the following surface plots of the angles in the plane. This is how the angle between the axis and , given by looks like: And this is how the angle the axis makes with , looks like: If you check the surface values you can see they match what my sketch described. Let's come back to what was described in the wikipedia link. We can see follows the "" East-anticlockwise "" convention. We have when is pointing East, and the angle grows as rotates anticlockwise. Now, if we try to make sense of the values as if they described too, we can see when is pointing North, and grows as rotates anticlockwise. Hence, the convention might be "" North-anticlockwise "". But again, I don't know what is special about . From the derivation of the angles, either equation could have corresponded to either eigenvector. There is still a missing piece of the puzzle which I believe must lie in the derivation of the equation for . Can anybody give me a hand? Any insights would be greatly appreciated. SUMMARY The equation holds for the azimuths of both eigenvectors, L and S , of the matrix Let me define 2 new vectors: , with azimuth . , with azimuth . We can write It turns out using the first equality we get where is the angle between the axis and the eigenvector with larger eigenvalue, . The second equality gives us , the angle between the axis and the eigenvector with smaller eigenvalue, . Now the question remaining is why does that happen? How could I have predicted that and would always have an azimuth that is twice that of and respectively? DIAGONALISATION I have found this derivation from Howard E. Haber from his Physics 116A class of Winter 2011. He obtains the same equation for by diagonalising the matrix (note the difference in notation: he uses in , and in the off-diagonal terms). He then proceeds by setting constraints in the angle . When he plugs his eq. 1 in his eq. 8 he makes it explicit that is measuring the angle between the positive axis and the eigenvector with largest eigenvalue. The conclusions drawn are the same as mine, but I somehow bypassed all that when I decided to use the function atan2 (unjustifiably, but it works). The question remains: why does my approach of using atan2 work?","xy A = \begin{pmatrix} a & b \\ b & c \end{pmatrix}. a,c>0 A |b|\leq\sqrt{ac} A \geq 0 A\mathbf{x}=\lambda\mathbf{x} \begin{pmatrix} a & b \\ b & c \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} = \lambda \begin{pmatrix} x \\ y \end{pmatrix}. \begin{cases}
y = x(\lambda-a)/b \\ 
x = y(\lambda-c)/b,
\end{cases}  \frac{c-a}{b}=\frac{y}{x}-\frac{x}{y}. \begin{align}
    \frac{c-a}{b}  &= \tan{\theta}-\tan{\beta} \\
                                &= (1+\tan{\theta}\tan{\beta})\tan(\theta-\beta) \\
                                &= 2\tan(2\theta-\pi/2) \\
                                &=-2\cot(2\theta),
\end{align} \tan \theta \tan \beta = 1 \beta = \pi/2 - \theta \begin{equation}
    \tan(2\theta) = \frac{2b}{a -c}.
\end{equation} \theta = \frac{1}{2}\textrm{atan2}(2b,a-c), \textrm{atan2}(y,x)=\textrm{Arg}(x+iy) \mathbf{x} x \theta (-90,90]^\circ \lambda A \theta x A = \begin{pmatrix} a & 0 \\ 0 & c \end{pmatrix}, a c a>c \theta=0^\circ a < c \theta = 90^\circ \theta \tan(2\theta) = \frac{2b}{a -c} -1  \tan(2\theta) = \frac{-2b}{c-a}, \theta = \frac{1}{2}\textrm{atan2}(-2b,c-a), x \textrm{atan2} \mathrm{atan2} \mathbf{L} \mathbf{S} (a-c,b) \mathbf{L} \mathbf{S} x \theta_l \theta_s (a-c,b) xy (-90,90]^\circ (a-c,b) x \mathbf{L} \theta_l=\frac{1}{2}\textrm{atan2}(2b,a-c), x \mathbf{S} \theta_s=\frac{1}{2}\textrm{atan2}(-2b,c-a), \textrm{atan2}(y,x) \theta_l=0^\circ \mathbf{L} \mathbf{L} \theta_s \mathbf{L} \theta_s=0^\circ \mathbf{L} \mathbf{L} \textrm{atan2}(-y,-x) \mathbf{L} \theta_l \tan(2\theta) = \frac{2b}{a -c} A = \begin{pmatrix} a & b \\ b & c \end{pmatrix}. \mathbf{p}=(a-c,2b) \gamma_p=\textrm{atan2}(p_y,p_x) \mathbf{n}=-\mathbf{p} \gamma_n=\textrm{atan2}(-p_y,-p_x) \tan(2\theta) = \tan(\gamma_p) = \tan(\gamma_n). 2\theta_l=\gamma_p=\textrm{atan2}(2b,a-c), \theta_l x \mathbf{L} 2\theta_s = \gamma_n = \textrm{atan2}(-2b,c-a) \theta_s x \mathbf{S} \mathbf{p} \mathbf{n} \mathbf{L} \mathbf{S} \tan(2\theta) A b A_{22} c \theta \theta x","['linear-algebra', 'eigenvalues-eigenvectors', 'angle']"
75,Harmonic numbers as ratio of two Determinants,Harmonic numbers as ratio of two Determinants,,"Provide a proof to this interesting identity: $$\frac{\begin{vmatrix} 1^0 & 1^2 & 1^3 & \cdots & 1^n \\ 2^0 & 2^2 & 2^3 & \cdots & 2^n \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ n^0 & n^2 & n^3 & \cdots & n^n \end{vmatrix}}{\begin{vmatrix} 1^1 & 1^2 & 1^3 & \cdots & 1^n \\ 2^1 & 2^2 & 2^3 & \cdots & 2^n \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ n^1 & n^2 & n^3 & \cdots & n^n \end{vmatrix}} = H_n  \tag{1}\label{harmonic}$$ where $H_n$ is the nth Harmonic Number. I accidently stumbled upon this observation while Programming. The inverse of a $(n+1)\times (n+1)$ Vandermonde Matrix (say $V_n$ ) with $ [\alpha_1 , \alpha_2, \alpha_3, \cdots, \alpha_n , \alpha_{n+1} ] = [0,1,2,\cdots,n-1,n] $ , where we treat $0^0$ as $1$ , $$V_n = \begin{bmatrix} 1 & 0 & 0 & \cdots & 0 \\ 1^0 & 1^1 & 1^2 & \cdots & 1^n \\ 2^0 & 2^1 & 2^2 & \cdots & 2^n  \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ n^0 & n^1 & n^2 & \cdots & n^n \end{bmatrix} $$ has some interesting properties. The first row of $V_n^{-1}$ is $ [ 1 \ 0 \ 0 \cdots \ 0]$ . We also have this recurrence relation: $$V_n^{-1}[m][0] = - \sum_{k=1}^n \frac{V_n^{-1}[m-1][0]}{k}  \tag{2}\label{recurrence}$$ Using \eqref{recurrence} we find: $V_n^{-1}[1][0] = -H_n$ . We can also find $V_n^{-1}[1][0]$ using $ \ adj(V_n)/det(V_n)$ which gives $V_n^{-1}[1][0] = C_{01}/det(V_n)$ where $C_{01}$ is the Cofactor of $V_n[0][1]$ . But it is same as the -ve of LHS of \eqref{harmonic}. QCD We can prove the same using Cramer's rule by Reframing the Arguments here a little. But None of these proofs are "" nice "" in a way of not using inverse of Vandermonde Matrix but only elementary arguments like Expansion of the determinants or Mathematical Inductions or something else. I've put exhaustive efforts but haven't succeded yet. Beside an elementary proof, any other perspective on the Result would be helpful and appreciated.","Provide a proof to this interesting identity: where is the nth Harmonic Number. I accidently stumbled upon this observation while Programming. The inverse of a Vandermonde Matrix (say ) with , where we treat as , has some interesting properties. The first row of is . We also have this recurrence relation: Using \eqref{recurrence} we find: . We can also find using which gives where is the Cofactor of . But it is same as the -ve of LHS of \eqref{harmonic}. QCD We can prove the same using Cramer's rule by Reframing the Arguments here a little. But None of these proofs are "" nice "" in a way of not using inverse of Vandermonde Matrix but only elementary arguments like Expansion of the determinants or Mathematical Inductions or something else. I've put exhaustive efforts but haven't succeded yet. Beside an elementary proof, any other perspective on the Result would be helpful and appreciated.","\frac{\begin{vmatrix} 1^0 & 1^2 & 1^3 & \cdots & 1^n \\ 2^0 & 2^2 & 2^3 & \cdots & 2^n \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ n^0 & n^2 & n^3 & \cdots & n^n \end{vmatrix}}{\begin{vmatrix} 1^1 & 1^2 & 1^3 & \cdots & 1^n \\ 2^1 & 2^2 & 2^3 & \cdots & 2^n \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ n^1 & n^2 & n^3 & \cdots & n^n \end{vmatrix}} = H_n  \tag{1}\label{harmonic} H_n (n+1)\times (n+1) V_n  [\alpha_1 , \alpha_2, \alpha_3, \cdots, \alpha_n , \alpha_{n+1} ] = [0,1,2,\cdots,n-1,n]  0^0 1 V_n = \begin{bmatrix} 1 & 0 & 0 & \cdots & 0 \\ 1^0 & 1^1 & 1^2 & \cdots & 1^n \\ 2^0 & 2^1 & 2^2 & \cdots & 2^n  \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ n^0 & n^1 & n^2 & \cdots & n^n \end{bmatrix}  V_n^{-1}  [ 1 \ 0 \ 0 \cdots \ 0] V_n^{-1}[m][0] = - \sum_{k=1}^n \frac{V_n^{-1}[m-1][0]}{k}  \tag{2}\label{recurrence} V_n^{-1}[1][0] = -H_n V_n^{-1}[1][0]  \ adj(V_n)/det(V_n) V_n^{-1}[1][0] = C_{01}/det(V_n) C_{01} V_n[0][1]","['linear-algebra', 'recurrence-relations', 'determinant', 'numerical-linear-algebra', 'harmonic-numbers']"
76,Eigendecomposition and Diagonalization of a matrix.,Eigendecomposition and Diagonalization of a matrix.,,"Is a matrix diagonalizable if and only if it has an eigendecomposition? If not, can you give an example of a diagonalizable matrix which doesn't have an eigendecomposition, or vice-versa?","Is a matrix diagonalizable if and only if it has an eigendecomposition? If not, can you give an example of a diagonalizable matrix which doesn't have an eigendecomposition, or vice-versa?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
77,5 points determine uniquely a conic,5 points determine uniquely a conic,,"I'm reading ""Multiple View Geometry"" by Richard Hartley and Andrew Zisserman. On pages 30-31 they have a section which proves that 5 points determine a conic. now, I know this question has been asked before but I would like an answer for the specific presentation which I describe below. First, they describe a conic by the equation: $$ax^2+bxy+cy^2+dx+ey+f=0$$ So by aggregating 5 points $(x_i,y_i)$ for $i\in\{1,\ldots,5\}$ which hold the above equation we get the matrix equation: $$\begin{bmatrix}x_1^2&x_1y_1&y_1^2&x_1&y_1&1\\x_2^2&x_2y_2&y_2^2&x_2&y_2&1\\x_3^2&x_3y_3&y_3^2&x_3&y_3&1\\x_4^2&x_4y_4&y_4^2&x_4&y_4&1\\x_5^2&x_5y_5&y_5^2&x_5&y_5&1\end{bmatrix}\mathbf{c}=0$$ where $\mathbf{c}=(a,b,c,d,e,f)^T$ . Finally, they claim the following: ""...the conic is the null vector of this $5\times$ 6 matrix . This shows that a conic is determined uniquely (up to scale) by five points in general position."" I have 2 questions about their derivation: 1) I know from other sources that 5 points where no 3 of them are collinear determine uniquely a conic so I guess that here the authors meant general linear position (which in this case translate to the condition that no 3 of the 5 points are collinear) and not some other kind of notion of general position. Am I right? 2) From a Linear algebraic point of view I would say that the immediate conclusion of the above matrix equation is that if the rank of such matrix is 5 then those 5 points determine uniquely a conic so I guess that this condition (of rank=5) can be translated to the condition for the 5 points to be in general position but I can't see why it is the same.","I'm reading ""Multiple View Geometry"" by Richard Hartley and Andrew Zisserman. On pages 30-31 they have a section which proves that 5 points determine a conic. now, I know this question has been asked before but I would like an answer for the specific presentation which I describe below. First, they describe a conic by the equation: So by aggregating 5 points for which hold the above equation we get the matrix equation: where . Finally, they claim the following: ""...the conic is the null vector of this 6 matrix . This shows that a conic is determined uniquely (up to scale) by five points in general position."" I have 2 questions about their derivation: 1) I know from other sources that 5 points where no 3 of them are collinear determine uniquely a conic so I guess that here the authors meant general linear position (which in this case translate to the condition that no 3 of the 5 points are collinear) and not some other kind of notion of general position. Am I right? 2) From a Linear algebraic point of view I would say that the immediate conclusion of the above matrix equation is that if the rank of such matrix is 5 then those 5 points determine uniquely a conic so I guess that this condition (of rank=5) can be translated to the condition for the 5 points to be in general position but I can't see why it is the same.","ax^2+bxy+cy^2+dx+ey+f=0 (x_i,y_i) i\in\{1,\ldots,5\} \begin{bmatrix}x_1^2&x_1y_1&y_1^2&x_1&y_1&1\\x_2^2&x_2y_2&y_2^2&x_2&y_2&1\\x_3^2&x_3y_3&y_3^2&x_3&y_3&1\\x_4^2&x_4y_4&y_4^2&x_4&y_4&1\\x_5^2&x_5y_5&y_5^2&x_5&y_5&1\end{bmatrix}\mathbf{c}=0 \mathbf{c}=(a,b,c,d,e,f)^T 5\times","['linear-algebra', 'geometry', 'conic-sections', 'projective-geometry']"
78,A system of equations Olympiad question,A system of equations Olympiad question,,"Find all non-zero real numbers $x,y,z$ which satisfy the system of equations: \begin{align} (x^2+xy+y^2)(y^2+yz+z^2)(z^2+zx+x^2)&=xyz,\\(x^4+x^2y^2+y^4)(y^4+y^2z^2+z^4)(z^4+z^2x^2+x^4)&=x^3y^3z^3 \end{align} It's an indian olympiad question. Can you guys help me out in solving it please ? edit: I have tried to write the first equation as: $(\frac{x^3-y^3}{x-y})(\frac{y^3-z^3}{y-z})(\frac{z^3-x^3}{z-x})$ = xyz And second equation as: $(\frac{x^6-y^6}{x^2-y^2})(\frac{y^6-z^6}{y^2-z^2})(\frac{z^6-x^6}{z^2-x^2}) = x^3y^3z^3$ Then divided the two to get: $(\frac{x^3+y^3}{x+y})(\frac{y^3+z^3}{y+z})(\frac{z^3+x^3}{z+x}) = x^2y^2z^2$ After that I have no sign what to do??",Find all non-zero real numbers which satisfy the system of equations: It's an indian olympiad question. Can you guys help me out in solving it please ? edit: I have tried to write the first equation as: = xyz And second equation as: Then divided the two to get: After that I have no sign what to do??,"x,y,z \begin{align}
(x^2+xy+y^2)(y^2+yz+z^2)(z^2+zx+x^2)&=xyz,\\(x^4+x^2y^2+y^4)(y^4+y^2z^2+z^4)(z^4+z^2x^2+x^4)&=x^3y^3z^3
\end{align} (\frac{x^3-y^3}{x-y})(\frac{y^3-z^3}{y-z})(\frac{z^3-x^3}{z-x}) (\frac{x^6-y^6}{x^2-y^2})(\frac{y^6-z^6}{y^2-z^2})(\frac{z^6-x^6}{z^2-x^2}) = x^3y^3z^3 (\frac{x^3+y^3}{x+y})(\frac{y^3+z^3}{y+z})(\frac{z^3+x^3}{z+x}) = x^2y^2z^2","['linear-algebra', 'polynomials', 'systems-of-equations', 'contest-math']"
79,"Name for ""the kernel lemma""?","Name for ""the kernel lemma""?",,"Lately I've been fascinated by the result that one might state slightly informally Lemma. (In the context of linear algebra over a field.) If $p$ and $q$ are relatively prime polynomials and $T$ is a linear operator then $\ker(pq(T))=\ker(p(T))\oplus\ker(q(T))$ . Follows easily from the fact that $F[x]$ is a PID; you can use it to start a proof of the existence of the Jordan Canonical Form, also for a proof that the solution to a constant-coefficient linear homogeneous DE is what it is. Q: Does this result have a standard name? Or do we know who proved it?","Lately I've been fascinated by the result that one might state slightly informally Lemma. (In the context of linear algebra over a field.) If and are relatively prime polynomials and is a linear operator then . Follows easily from the fact that is a PID; you can use it to start a proof of the existence of the Jordan Canonical Form, also for a proof that the solution to a constant-coefficient linear homogeneous DE is what it is. Q: Does this result have a standard name? Or do we know who proved it?",p q T \ker(pq(T))=\ker(p(T))\oplus\ker(q(T)) F[x],"['linear-algebra', 'abstract-algebra', 'reference-request', 'math-history']"
80,Finding irrational entries such that the determinant will never be zero,Finding irrational entries such that the determinant will never be zero,,"Context. The main goal is to find whether or not a subspace of $\mathbb R^5$ of dimension $3$ intersects a rational subspace of dimension $2$ . By rational subspace, we mean a subspace of $\mathbb R^5$ which admits a rational basis ( i.e. a basis formed with vectors with rational entries). This is what motivates this question. The question. Let $Y_1,Y_2,Y_3$ be three vectors of $\mathbb R^5$ such that all the coordinates of the $Y_i$ are in $$\mathbb Q(\sqrt 2,\sqrt 3,\sqrt 6),$$ i.e. the coordinates of the $Y_i$ are of the form $$a+b\sqrt 2+c\sqrt 3+d\sqrt 6,\qquad a,b,c,d\in\mathbb Q.$$ Let $X_1,X_2\in\mathbb Q^5$ be two vectors with rational entries such that $(X_1,X_2)$ is free over $\mathbb R$ . Does there exist such $(Y_1,Y_2,Y_3)$ such that for all such $(X_1,X_2)$ , the matrix $M\in\mathrm M_5(\mathbb R)$ with columns $Y_1,Y_2,Y_3,X_1,X_2$ , i.e. $$M:=(Y_1\vert Y_2\vert Y_3\vert X_1\vert X_2),$$ satisfies $$\det M\ne 0\quad ?$$ Remarks. I have tried many choices of vectors $Y_1,Y_2,Y_3$ , but it always result in a system of four rational equations that I can not solve. The goal would be to show that the system has no rational solution. Any ideas or references which would be related to this matter would be of great help.","Context. The main goal is to find whether or not a subspace of of dimension intersects a rational subspace of dimension . By rational subspace, we mean a subspace of which admits a rational basis ( i.e. a basis formed with vectors with rational entries). This is what motivates this question. The question. Let be three vectors of such that all the coordinates of the are in i.e. the coordinates of the are of the form Let be two vectors with rational entries such that is free over . Does there exist such such that for all such , the matrix with columns , i.e. satisfies Remarks. I have tried many choices of vectors , but it always result in a system of four rational equations that I can not solve. The goal would be to show that the system has no rational solution. Any ideas or references which would be related to this matter would be of great help.","\mathbb R^5 3 2 \mathbb R^5 Y_1,Y_2,Y_3 \mathbb R^5 Y_i \mathbb Q(\sqrt 2,\sqrt 3,\sqrt 6), Y_i a+b\sqrt 2+c\sqrt 3+d\sqrt 6,\qquad a,b,c,d\in\mathbb Q. X_1,X_2\in\mathbb Q^5 (X_1,X_2) \mathbb R (Y_1,Y_2,Y_3) (X_1,X_2) M\in\mathrm M_5(\mathbb R) Y_1,Y_2,Y_3,X_1,X_2 M:=(Y_1\vert Y_2\vert Y_3\vert X_1\vert X_2), \det M\ne 0\quad ? Y_1,Y_2,Y_3","['linear-algebra', 'vector-spaces', 'determinant', 'diophantine-equations', 'rational-numbers']"
81,Operator norm (induced $2$-norm) of a Kronecker tensor,Operator norm (induced -norm) of a Kronecker tensor,2,"Let $A \in \mathcal M(n \times n; \mathbb R)$ with $\rho(A) < 1$ . Then we know $I \otimes I - A^T \otimes A^T$ is invertible where $\otimes$ denotes kronecker product. Let $\text{vec}$ denote the vectorization operation and $\mathcal T = (I \otimes I - A^T \otimes A^T)^{-1} : \mathbb R^{n^2} \to \mathbb R^{n^2}$ . The operator norm of $\mathcal T$ is given by \begin{align*}   \|\mathcal T\|_2 = \sup_{\|x\|_2=1} \|\mathcal Tx\|_2, \end{align*} where $x \in \mathbb R^{n^2}$ . Let $\text{mat}$ denote the inverse operation of $\text{vec}$ , i.e., stacking the elements into a matrix. Let $X = \text{mat}(x)$ where $X \in \mathcal M(n \times n)$ . We note $\mathcal Tx$ is exactly the vectorization of $Y$ where $Y$ is the unique solution of \begin{align} \label{eq:1} \tag{$\star$} A^T Y A + X = Y. \end{align} The assumption on $A$ guarantees a unique solution. My question is whether we can take $X$ to be symmetric to determine the operator norm of $\mathcal T$ . That is, does the following hold \begin{align*} \|\mathcal T\|_2 = \sup \{\|Y\|_F: X \in \mathbb S_n, \|X\|_F=1\}, \end{align*} where $Y$ is the solution to \eqref{eq:1} and $\mathbb S_n$ denotes the set of symmetric matrices. If we let $c = \sup \{\|Y\|_F: X \in \mathbb S_n, \|X\|_F=1\}$ , clearly $c \le \|\mathcal T\|_2$ . Will the other way hold? $\mathcal T$ seems to have nice structure and it makes me wonder whether we can just consider supremum over this proper subset. BOUNTY EDIT: This question has been a while. Loup Blanc has an excellent answer. I thought I understood his edit 2 before but actually it was misunderstanding. Now I am starting a bounty hoping someone (maybe loup himeself) could elaborate his Edit 2.","Let with . Then we know is invertible where denotes kronecker product. Let denote the vectorization operation and . The operator norm of is given by where . Let denote the inverse operation of , i.e., stacking the elements into a matrix. Let where . We note is exactly the vectorization of where is the unique solution of The assumption on guarantees a unique solution. My question is whether we can take to be symmetric to determine the operator norm of . That is, does the following hold where is the solution to \eqref{eq:1} and denotes the set of symmetric matrices. If we let , clearly . Will the other way hold? seems to have nice structure and it makes me wonder whether we can just consider supremum over this proper subset. BOUNTY EDIT: This question has been a while. Loup Blanc has an excellent answer. I thought I understood his edit 2 before but actually it was misunderstanding. Now I am starting a bounty hoping someone (maybe loup himeself) could elaborate his Edit 2.","A \in \mathcal M(n \times n; \mathbb R) \rho(A) < 1 I \otimes I - A^T \otimes A^T \otimes \text{vec} \mathcal T = (I \otimes I - A^T \otimes A^T)^{-1} : \mathbb R^{n^2} \to \mathbb R^{n^2} \mathcal T \begin{align*}
  \|\mathcal T\|_2 = \sup_{\|x\|_2=1} \|\mathcal Tx\|_2,
\end{align*} x \in \mathbb R^{n^2} \text{mat} \text{vec} X = \text{mat}(x) X \in \mathcal M(n \times n) \mathcal Tx Y Y \begin{align}
\label{eq:1}
\tag{\star}
A^T Y A + X = Y.
\end{align} A X \mathcal T \begin{align*}
\|\mathcal T\|_2 = \sup \{\|Y\|_F: X \in \mathbb S_n, \|X\|_F=1\},
\end{align*} Y \mathbb S_n c = \sup \{\|Y\|_F: X \in \mathbb S_n, \|X\|_F=1\} c \le \|\mathcal T\|_2 \mathcal T","['linear-algebra', 'matrices', 'kronecker-product', 'matrix-norms', 'matrix-analysis']"
82,If $2AB = (BA)^2+I_n$ then $1$ is an eigenvalue for $AB$,If  then  is an eigenvalue for,2AB = (BA)^2+I_n 1 AB,"Let $n$ be an odd integer, $A,B$ be $n\times n$ real matrices such that $2AB = (BA)^2+I_n$. Prove that $1$ is an eigenvalue of $AB$. This was asked at an oral exam. I've been pondering this question for a while without success. $1$ is an eigenvalue of $AB$ iff $AB-I_n$ is singular, that is $\det(AB-I_n)=0$ which is equivalent to  $\det ((BA)^2-I_n)=0$. That's all I have noted... I'd appreciate any hint. I know that $AB$ and $BA$ have the same characteristic polynomials, hence the same eigenvalues. So it may suffice to prove that $1$ is eigenvalue of $BA$, or $\det(BA-I_n)=0$.","Let $n$ be an odd integer, $A,B$ be $n\times n$ real matrices such that $2AB = (BA)^2+I_n$. Prove that $1$ is an eigenvalue of $AB$. This was asked at an oral exam. I've been pondering this question for a while without success. $1$ is an eigenvalue of $AB$ iff $AB-I_n$ is singular, that is $\det(AB-I_n)=0$ which is equivalent to  $\det ((BA)^2-I_n)=0$. That's all I have noted... I'd appreciate any hint. I know that $AB$ and $BA$ have the same characteristic polynomials, hence the same eigenvalues. So it may suffice to prove that $1$ is eigenvalue of $BA$, or $\det(BA-I_n)=0$.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
83,Probability that a random binary matrix is positive semi-definite,Probability that a random binary matrix is positive semi-definite,,"Let $A$ be a random $n \times n$ matrix such that $A_{ij}\in\{0,1\}$. Assume that each element $A_{ij}$ equals 1 with some probability $p>0$ and that all the draws are independent across elements. What is the probability that the symmetric part of $A$, the matrix $\frac{1}{2}(A+A^T)$, is positive semi-definite? Any results pertaining to the symmetric part being positive definite would also be welcome.","Let $A$ be a random $n \times n$ matrix such that $A_{ij}\in\{0,1\}$. Assume that each element $A_{ij}$ equals 1 with some probability $p>0$ and that all the draws are independent across elements. What is the probability that the symmetric part of $A$, the matrix $\frac{1}{2}(A+A^T)$, is positive semi-definite? Any results pertaining to the symmetric part being positive definite would also be welcome.",,"['linear-algebra', 'matrices', 'positive-definite', 'random-matrices', 'positive-semidefinite']"
84,Odds of ellipse from five random points,Odds of ellipse from five random points,,"It's known that five points determine a conic section .  Five random points can go right into the $6\times6$ matrix, and then the $A x^2 + B xy + C y^2$ part can be looked at.  If $B^2-4AC<0$, it's an ellipse.  Five random points will almost never produce circles or parabolas, so the results will be ellipses and hyperbolas.  What are the odds of an ellipse? In a random run of 100000 trials, I got 27974 ellipses. ""It's less than $e/10$,"" seems like a solid answer.  Anyone have anything more specific? EDIT:  As Oscar points out, I should have said ""It's more than $e/10$.""  In my trial, real-values points were randomly picked from a unit square. Square Triangle Picking methods might be applicable. EDIT2: Aretino points out that odds of a convex pentagon are $49/144≈0.34$. So how can points making a convex pentagon give a non-ellipse?  Here's a picture. With the red points fixed, the black points are outside of the convex hull yet still yield a non-ellipse. EDIT3: That spray of points above goes back to Newton, Philosophiae naturalis principia mathematica, 1687, where he solved the 4 point parabola ( another version ). If a point is between one of the two parabolas and the degenerate lines, then it gives a hyperbola.","It's known that five points determine a conic section .  Five random points can go right into the $6\times6$ matrix, and then the $A x^2 + B xy + C y^2$ part can be looked at.  If $B^2-4AC<0$, it's an ellipse.  Five random points will almost never produce circles or parabolas, so the results will be ellipses and hyperbolas.  What are the odds of an ellipse? In a random run of 100000 trials, I got 27974 ellipses. ""It's less than $e/10$,"" seems like a solid answer.  Anyone have anything more specific? EDIT:  As Oscar points out, I should have said ""It's more than $e/10$.""  In my trial, real-values points were randomly picked from a unit square. Square Triangle Picking methods might be applicable. EDIT2: Aretino points out that odds of a convex pentagon are $49/144≈0.34$. So how can points making a convex pentagon give a non-ellipse?  Here's a picture. With the red points fixed, the black points are outside of the convex hull yet still yield a non-ellipse. EDIT3: That spray of points above goes back to Newton, Philosophiae naturalis principia mathematica, 1687, where he solved the 4 point parabola ( another version ). If a point is between one of the two parabolas and the degenerate lines, then it gives a hyperbola.",,"['linear-algebra', 'geometry', 'conic-sections']"
85,Computing determinant without expansion,Computing determinant without expansion,,$$\begin{align}\mathrm D &= \left|\begin{matrix} (b+c)^2 & a^2 & a^2 \\ b^2 & (a+c)^2 & b^2 \\ c^2 & c^2 & (a+b)^2 \end{matrix}\right|\\ &= (a+b+c)\left|\begin{matrix} b+c - a & a^2 & a^2 \\ b - a -c & (a+c)^2 & b^2 \\ 0 & c^2 & (a+b)^2 \end{matrix}\right| \\ &= (a+b+c)^2\left|\begin{matrix} b+c - a & 0 & a^2 \\ b - a -c & a+c - b & b^2 \\ 0 & c - a-b & (a+b)^2 \end{matrix}\right|\\ &= (a+b+c)^2\left|\begin{matrix} b+c - a & 0 & a^2 \\ 0 & a+c - b & b^2 \\ c - a-b & c - a-b & (a+b)^2 \end{matrix}\right|\end{align}$$ Can $\rm D$ be further simplified without expanding ?  I feel it should be because this was competition question.,$$\begin{align}\mathrm D &= \left|\begin{matrix} (b+c)^2 & a^2 & a^2 \\ b^2 & (a+c)^2 & b^2 \\ c^2 & c^2 & (a+b)^2 \end{matrix}\right|\\ &= (a+b+c)\left|\begin{matrix} b+c - a & a^2 & a^2 \\ b - a -c & (a+c)^2 & b^2 \\ 0 & c^2 & (a+b)^2 \end{matrix}\right| \\ &= (a+b+c)^2\left|\begin{matrix} b+c - a & 0 & a^2 \\ b - a -c & a+c - b & b^2 \\ 0 & c - a-b & (a+b)^2 \end{matrix}\right|\\ &= (a+b+c)^2\left|\begin{matrix} b+c - a & 0 & a^2 \\ 0 & a+c - b & b^2 \\ c - a-b & c - a-b & (a+b)^2 \end{matrix}\right|\end{align}$$ Can $\rm D$ be further simplified without expanding ?  I feel it should be because this was competition question.,,"['linear-algebra', 'matrices', 'contest-math', 'determinant']"
86,Left ideals of matrix ring over a field,Left ideals of matrix ring over a field,,"The claim is made here that for $k$ a field The left ideals of $M_n(k)$ are all of the form  $$\{A \in M_n(k) \mid  \operatorname{ker}A > S \}, \rlap{ \qquad \text{for some subspace  $S$.}}$$ I was trying to think through that claim. I understand why all the left ideals of $M_n(k)$ are of the form $$\{A \in M_n(k) \mid \operatorname{Rowspace}(A) < S \}, \rlap{ \qquad \text{for some subspace $S$.}}$$ In the presence of an inner product, we have that $\operatorname{ker}A^T = \operatorname{Range}(A)^{\bot}$, and therefore we get the characterization we want. But what about if there isn't an inner product? Does the same hold for $M_n(\Delta)$, where $\Delta$ is a division ring?","The claim is made here that for $k$ a field The left ideals of $M_n(k)$ are all of the form  $$\{A \in M_n(k) \mid  \operatorname{ker}A > S \}, \rlap{ \qquad \text{for some subspace  $S$.}}$$ I was trying to think through that claim. I understand why all the left ideals of $M_n(k)$ are of the form $$\{A \in M_n(k) \mid \operatorname{Rowspace}(A) < S \}, \rlap{ \qquad \text{for some subspace $S$.}}$$ In the presence of an inner product, we have that $\operatorname{ker}A^T = \operatorname{Range}(A)^{\bot}$, and therefore we get the characterization we want. But what about if there isn't an inner product? Does the same hold for $M_n(\Delta)$, where $\Delta$ is a division ring?",,"['linear-algebra', 'matrices', 'ideals', 'noncommutative-algebra']"
87,How to find a symmetric matrix that transforms one ellipsoid to another?,How to find a symmetric matrix that transforms one ellipsoid to another?,,"Given two origin-centered ellipsoids $E_0$ and $E_1$ in $\mathbb{R}^n$, I'd like to find an SPD (symmetric positive definite) transformation matrix $M$ that transforms $E_0$ into $E_1$. Let's say $E_0$ and $E_1$ are specified by SPD matrices that take the unit sphere to the respective ellipsoid: $$     M_0 = R_0 D_0 {R_0}^{-1} \mathrm{\ takes\ unit\ sphere\ to\ }E_0\\     M_1 = R_1 D_1 {R_1}^{-1} \mathrm{\ takes\ unit\ sphere\ to\ }E_1 $$ where each $R_i$ is a rotation matrix and $D_i$ is a positive diagonal matrix whose diagonal entries are the respective ellipsoid's principal radii. Then of course the matrix $M_1 {M_0}^{-1}$ will take $E_0$ to $E_1$, but it is in general not a symmetric matrix (even though each $M_i$ is symmetric), so that's not a solution. I strongly suspect there is a unique SPD matrix that takes $E_0$ to $E_1$.  How can it be computed? More generally, if $R$ is any rotation matrix, then $M_1 R {M_0}^{-1}$ will take $E_0$ to $E_1$. In fact I suspect the matrices that take $E_0$ to $E_1$ are precisely the matrices of this form.  So perhaps the question boils down to ""Given SPD $M_0$, $M_1$, find a rotation $R$ such that $M_1 R {M_0}^{-1}$ is symmetric"". This comes from a statistics question: given a point cloud in $\mathbb{R}^n$ and a target covariance matrix, I want to find an SPD transformation such that the transformed point cloud has the target covariance.  Intuitively, it seems like what's needed is an SPD transformation that transforms the point cloud's error ellipsoid (found by taking the square root of its covariance matrix) to the error ellipsoid of the target covariance matrix; so that's why I'm asking this question.  However, even if I found such a transform, I'm not certain the transformed point cloud will have exactly the desired target covariance;  if it turns out that it doesn't, I'll ask a different question for the underlying statistics problem.","Given two origin-centered ellipsoids $E_0$ and $E_1$ in $\mathbb{R}^n$, I'd like to find an SPD (symmetric positive definite) transformation matrix $M$ that transforms $E_0$ into $E_1$. Let's say $E_0$ and $E_1$ are specified by SPD matrices that take the unit sphere to the respective ellipsoid: $$     M_0 = R_0 D_0 {R_0}^{-1} \mathrm{\ takes\ unit\ sphere\ to\ }E_0\\     M_1 = R_1 D_1 {R_1}^{-1} \mathrm{\ takes\ unit\ sphere\ to\ }E_1 $$ where each $R_i$ is a rotation matrix and $D_i$ is a positive diagonal matrix whose diagonal entries are the respective ellipsoid's principal radii. Then of course the matrix $M_1 {M_0}^{-1}$ will take $E_0$ to $E_1$, but it is in general not a symmetric matrix (even though each $M_i$ is symmetric), so that's not a solution. I strongly suspect there is a unique SPD matrix that takes $E_0$ to $E_1$.  How can it be computed? More generally, if $R$ is any rotation matrix, then $M_1 R {M_0}^{-1}$ will take $E_0$ to $E_1$. In fact I suspect the matrices that take $E_0$ to $E_1$ are precisely the matrices of this form.  So perhaps the question boils down to ""Given SPD $M_0$, $M_1$, find a rotation $R$ such that $M_1 R {M_0}^{-1}$ is symmetric"". This comes from a statistics question: given a point cloud in $\mathbb{R}^n$ and a target covariance matrix, I want to find an SPD transformation such that the transformed point cloud has the target covariance.  Intuitively, it seems like what's needed is an SPD transformation that transforms the point cloud's error ellipsoid (found by taking the square root of its covariance matrix) to the error ellipsoid of the target covariance matrix; so that's why I'm asking this question.  However, even if I found such a transform, I'm not certain the transformed point cloud will have exactly the desired target covariance;  if it turns out that it doesn't, I'll ask a different question for the underlying statistics problem.",,"['linear-algebra', 'statistics', 'linear-transformations', 'symmetric-matrices']"
88,Counting the number of rank $r$ binary $n \times k$ matrices that has unique columns,Counting the number of rank  binary  matrices that has unique columns,r n \times k,"I'm trying to figure out how many ways there are to construct a $k \times n$ binary matrix such that it has rank $r$ and no column is repeating. I've tried a bunch of different approaches. The attempt that I assign the highest confidence is the following: We want to enumerate all possible $B_r = [\textbf{b}_1,\dots \textbf{b}_n]$ such that $\textbf{b}_i \in \mathbb{F}_2^k$, $\textbf{b}_i \neq \textbf{b}_j$ and Rank$(B_r) = r$. The number of ways to pick a basis of dimension $r$ from a subspace of dimension $k$ is given by the Gaussian binomial ${k \brack r}$. Since we have $n$ column slots and this basis will occupy $r$ of these slots, we can insert the basis in $\binom{n}{r}$ different ways. To fill the remaining $n-r$ columns, we must not choose a vector outside of the span of this basis, which means we must fill the remaining $n-r$ rows from a set of $2^r-r$ candidates. There are $2^r-r$ ways of choosing $\textbf{b}_{r+1}$ and $2^r-r-i$ ways of choosing $\textbf{b}_{r+i}$, where $i= 1,\dots,n-r$, so the number of possible rank $r$ matrices of dimension $k \times n$ with no repeating columns is given by $$|\{M \in \mathbb{F}_2^{n \times k}: \text{Rank}(M)=r, \textbf{m}_i \neq \textbf{m}_j\}| = \binom{n}{r}{k \brack r}\prod_{i=1}^{n-r} (2^r - r-i).$$ EDIT: After comparing joriki's expression with simulations in Matlab I just didn't get the right results. Let me try to reformulate my thought process as clearly as possible. Begin by considering the number of possible $r$-dimensional subspaces of $\mathbb{F}_2^k$, given by ${k \brack r}$. Enumerate all ways of building a $k \times n$ matrix only using elements from such a subspace, which is $\frac{2^r!}{(2^r-n)!}$. The problem is that all of these ${k \brack r}\frac{2^r!}{(2^r-n)!}$ matrices will not have rank $r$ (but none will have rank \emph{more} than $r$). To begin with, we must subtract all $k \times n$ rank $r-1$ submatrices. How many of these are there? Well the columns are eminating from a $r$-dimensional binary space. So we count the number of subspaces of dimension $r-1$ to an $r$-dimensional space and multiply with the number of ways of choosing $n$ vectors from this space. We get ${r \brack r-1}\frac{2^{r-1}!}{(2^{r-1}-n)!}$. But once again we have the problem that not all these have rank $r-1$. So from this we must subtract ${r-1 \brack r-2}\frac{2^{r-2}!}{(2^{r-2}-n)!}$ and so forth. \begin{align} |\mathcal{N}_{r,k}| = &{k \brack r}\frac{2^r!}{(2^r-n)!} - \sum_{i=1}^r \Big({r \brack r-i}\frac{2^{r-i}!}{(2^{r-i}-n)!} \\ &- \sum_{j=1}^{r-i}\Big({r-i \brack r-i-j}\frac{2^{r-i-j}!}{(2^{r-i-j}-n)!} - \sum_{\ell=1}^{r-i-j} \Big(\dots   \end{align} So how do we define this recursively? Define \begin{align} a_0 &:= {k \brack r}\frac{2^r!}{(2^r-n)!} \\ a_{p,q} &:= {p \brack q}\frac{2^q!}{(2^q-n)!} - \sum_{i=1}^p a_{q,q-i}. \end{align} Then we should have \begin{align} |\mathcal{N}_{r,k}| = a_0 - \sum_{i=1}^r a_{r,r-i}. \end{align} Tested this in Matlab and it does not work - it doesn't subtract enough matrices. The effect is greatest when $n,r < k$ and all parameters are not tiny. It is kind of weird how $k$ never enters into the enumeration of rank $r-i$ matrices of size $k\times n$. I guess it COULD make sense that the enumeration is conditionally independent on $k$ given that we know this new $r-i$ dimesional subspace is embedded in an $r<k$ dimensional space. But clearly undercounting is done somewhere in the recursion and this seems like the likeliest candidate. I guess the question is: Is counting the ways we can build a $k \times n$ matrix by only using elements from an $r-i$ dimensional subspace really the same as counting $r-i$ dimensional subspaces of $r<k$-dimensional space times the number of ways we can choose $n$ vectors out of this space? It does not seem right. EDIT 2: Back to where I started, essentially can't see the problem of the original attempt $$|\mathcal{N}_{r,k}| = {k \brack r}\frac{2^r!}{(2^r-n)!} - \sum_{i=1}^r |\mathcal{N}_{r-i,k}|.$$ Just count the number of $k \times n$ matrices with rank $r$ or less, then subtract all $k \times n$ matrices with rank less than $r$. This expression should essentially hold by definition. Really frustrating. EDIT 3: So the fault is ${k \brack r}\frac{2^r!}{(2^r-n)!}$ as a formula for counting all $k \times n$ matrices with rank at most $r$. It is overcounting by quite a bit. This is because the same matrix could be formed by choosing $n$ different components from two different subspaces. Consider subspaces $A = \{x_1,\dots, x_n, a_{n+1},\dots, a_{2^r}\}$ and $B = \{x_1,\dots,x_n, b_{n+1},\dots,b_{2^r}\}$. They're two distinct subspaces but we are counting the matrix formed from taking the first $n$ vectors from $A$ and the matrix formed from taking the first $n$ vectors from $B$ as two different matrices. Clearly they're not. Which makes one wonder if this can be solved with some kind of additional inclusion-exclusion trick, or if it's just one of those really tricky problems.","I'm trying to figure out how many ways there are to construct a $k \times n$ binary matrix such that it has rank $r$ and no column is repeating. I've tried a bunch of different approaches. The attempt that I assign the highest confidence is the following: We want to enumerate all possible $B_r = [\textbf{b}_1,\dots \textbf{b}_n]$ such that $\textbf{b}_i \in \mathbb{F}_2^k$, $\textbf{b}_i \neq \textbf{b}_j$ and Rank$(B_r) = r$. The number of ways to pick a basis of dimension $r$ from a subspace of dimension $k$ is given by the Gaussian binomial ${k \brack r}$. Since we have $n$ column slots and this basis will occupy $r$ of these slots, we can insert the basis in $\binom{n}{r}$ different ways. To fill the remaining $n-r$ columns, we must not choose a vector outside of the span of this basis, which means we must fill the remaining $n-r$ rows from a set of $2^r-r$ candidates. There are $2^r-r$ ways of choosing $\textbf{b}_{r+1}$ and $2^r-r-i$ ways of choosing $\textbf{b}_{r+i}$, where $i= 1,\dots,n-r$, so the number of possible rank $r$ matrices of dimension $k \times n$ with no repeating columns is given by $$|\{M \in \mathbb{F}_2^{n \times k}: \text{Rank}(M)=r, \textbf{m}_i \neq \textbf{m}_j\}| = \binom{n}{r}{k \brack r}\prod_{i=1}^{n-r} (2^r - r-i).$$ EDIT: After comparing joriki's expression with simulations in Matlab I just didn't get the right results. Let me try to reformulate my thought process as clearly as possible. Begin by considering the number of possible $r$-dimensional subspaces of $\mathbb{F}_2^k$, given by ${k \brack r}$. Enumerate all ways of building a $k \times n$ matrix only using elements from such a subspace, which is $\frac{2^r!}{(2^r-n)!}$. The problem is that all of these ${k \brack r}\frac{2^r!}{(2^r-n)!}$ matrices will not have rank $r$ (but none will have rank \emph{more} than $r$). To begin with, we must subtract all $k \times n$ rank $r-1$ submatrices. How many of these are there? Well the columns are eminating from a $r$-dimensional binary space. So we count the number of subspaces of dimension $r-1$ to an $r$-dimensional space and multiply with the number of ways of choosing $n$ vectors from this space. We get ${r \brack r-1}\frac{2^{r-1}!}{(2^{r-1}-n)!}$. But once again we have the problem that not all these have rank $r-1$. So from this we must subtract ${r-1 \brack r-2}\frac{2^{r-2}!}{(2^{r-2}-n)!}$ and so forth. \begin{align} |\mathcal{N}_{r,k}| = &{k \brack r}\frac{2^r!}{(2^r-n)!} - \sum_{i=1}^r \Big({r \brack r-i}\frac{2^{r-i}!}{(2^{r-i}-n)!} \\ &- \sum_{j=1}^{r-i}\Big({r-i \brack r-i-j}\frac{2^{r-i-j}!}{(2^{r-i-j}-n)!} - \sum_{\ell=1}^{r-i-j} \Big(\dots   \end{align} So how do we define this recursively? Define \begin{align} a_0 &:= {k \brack r}\frac{2^r!}{(2^r-n)!} \\ a_{p,q} &:= {p \brack q}\frac{2^q!}{(2^q-n)!} - \sum_{i=1}^p a_{q,q-i}. \end{align} Then we should have \begin{align} |\mathcal{N}_{r,k}| = a_0 - \sum_{i=1}^r a_{r,r-i}. \end{align} Tested this in Matlab and it does not work - it doesn't subtract enough matrices. The effect is greatest when $n,r < k$ and all parameters are not tiny. It is kind of weird how $k$ never enters into the enumeration of rank $r-i$ matrices of size $k\times n$. I guess it COULD make sense that the enumeration is conditionally independent on $k$ given that we know this new $r-i$ dimesional subspace is embedded in an $r<k$ dimensional space. But clearly undercounting is done somewhere in the recursion and this seems like the likeliest candidate. I guess the question is: Is counting the ways we can build a $k \times n$ matrix by only using elements from an $r-i$ dimensional subspace really the same as counting $r-i$ dimensional subspaces of $r<k$-dimensional space times the number of ways we can choose $n$ vectors out of this space? It does not seem right. EDIT 2: Back to where I started, essentially can't see the problem of the original attempt $$|\mathcal{N}_{r,k}| = {k \brack r}\frac{2^r!}{(2^r-n)!} - \sum_{i=1}^r |\mathcal{N}_{r-i,k}|.$$ Just count the number of $k \times n$ matrices with rank $r$ or less, then subtract all $k \times n$ matrices with rank less than $r$. This expression should essentially hold by definition. Really frustrating. EDIT 3: So the fault is ${k \brack r}\frac{2^r!}{(2^r-n)!}$ as a formula for counting all $k \times n$ matrices with rank at most $r$. It is overcounting by quite a bit. This is because the same matrix could be formed by choosing $n$ different components from two different subspaces. Consider subspaces $A = \{x_1,\dots, x_n, a_{n+1},\dots, a_{2^r}\}$ and $B = \{x_1,\dots,x_n, b_{n+1},\dots,b_{2^r}\}$. They're two distinct subspaces but we are counting the matrix formed from taking the first $n$ vectors from $A$ and the matrix formed from taking the first $n$ vectors from $B$ as two different matrices. Clearly they're not. Which makes one wonder if this can be solved with some kind of additional inclusion-exclusion trick, or if it's just one of those really tricky problems.",,"['linear-algebra', 'combinatorics', 'matrix-rank', 'inclusion-exclusion']"
89,Prove that the Pfaffian satisfies $\text{Pf}(MAM^T)=\det(M)\text{Pf}(A)$,Prove that the Pfaffian satisfies,\text{Pf}(MAM^T)=\det(M)\text{Pf}(A),"Show that $$\text{Pf} MAM^T = \text{det}M \cdot \text{Pf} A$$ for any matrix $M$ and antisymmetric $A$. Attempt: $$\text{Pf} MAM^T = \frac{1}{2^N N!} \epsilon_{\alpha_1 \dots \alpha_{2N}} (MAM^T)_{\alpha_1 \alpha_2} \dots (MAM^T)_{\alpha_{2N-1} \alpha_{2N}} = \frac{1}{2^N N!}\epsilon_{\alpha_1 \dots \alpha_{2N}} M_{\alpha_1 \sigma_1} A_{\sigma_1 \delta_1} (M^T)_{\delta_1 \alpha_2} \dots M_{\alpha_{2N-1} \sigma_{2N-1}} A_{\sigma_{2N-1} \delta_{2N-1}} (M^T)_{\delta_{2N-1} \alpha_{2N}}  $$ while $$\text{det} M = \epsilon_{\beta_1 \dots \beta_{2N}} M_{1, \beta_1} \dots M_{2N, \beta_{2N}}$$ and $$\text{Pf}A = \frac{1}{2^N N!} \epsilon_{\gamma_1 \dots \gamma_{2N}} (A)_{\gamma_1 \gamma_2} \dots (A)_{\gamma_{2N-1} \gamma_{2N}}$$ Working with the terms on the r.hs I see that $$\text{Pf}A \cdot \det M = \frac{1}{2^N N!} \epsilon_{\beta_1 \dots \beta_{2N}} \epsilon_{\gamma_1 \dots \gamma_{2N}} M_{1, \beta_1} \dots M_{2N, \beta_{2N}}(A)_{\gamma_1 \gamma_2} \dots (A)_{\gamma_{2N-1} \gamma_{2N}}$$ I don't see a way to proceed - is there perhaps another definition of $det$ I should use or can I argue based on these diagrammatic forms below?","Show that $$\text{Pf} MAM^T = \text{det}M \cdot \text{Pf} A$$ for any matrix $M$ and antisymmetric $A$. Attempt: $$\text{Pf} MAM^T = \frac{1}{2^N N!} \epsilon_{\alpha_1 \dots \alpha_{2N}} (MAM^T)_{\alpha_1 \alpha_2} \dots (MAM^T)_{\alpha_{2N-1} \alpha_{2N}} = \frac{1}{2^N N!}\epsilon_{\alpha_1 \dots \alpha_{2N}} M_{\alpha_1 \sigma_1} A_{\sigma_1 \delta_1} (M^T)_{\delta_1 \alpha_2} \dots M_{\alpha_{2N-1} \sigma_{2N-1}} A_{\sigma_{2N-1} \delta_{2N-1}} (M^T)_{\delta_{2N-1} \alpha_{2N}}  $$ while $$\text{det} M = \epsilon_{\beta_1 \dots \beta_{2N}} M_{1, \beta_1} \dots M_{2N, \beta_{2N}}$$ and $$\text{Pf}A = \frac{1}{2^N N!} \epsilon_{\gamma_1 \dots \gamma_{2N}} (A)_{\gamma_1 \gamma_2} \dots (A)_{\gamma_{2N-1} \gamma_{2N}}$$ Working with the terms on the r.hs I see that $$\text{Pf}A \cdot \det M = \frac{1}{2^N N!} \epsilon_{\beta_1 \dots \beta_{2N}} \epsilon_{\gamma_1 \dots \gamma_{2N}} M_{1, \beta_1} \dots M_{2N, \beta_{2N}}(A)_{\gamma_1 \gamma_2} \dots (A)_{\gamma_{2N-1} \gamma_{2N}}$$ I don't see a way to proceed - is there perhaps another definition of $det$ I should use or can I argue based on these diagrammatic forms below?",,"['linear-algebra', 'matrices', 'determinant', 'pfaffian']"
90,Functions satisfying 4 out of 5 inner product properties,Functions satisfying 4 out of 5 inner product properties,,"Let us consider function $s:K^m \times K^m \mapsto K$ (here $K = \mathbb{R}$ or $K = \mathbb{C}$ ). If $\forall x, y, z \in K^m, \forall \lambda \in K$ $s(x + y, z) = s(x, z) + s(y, z)$ $s(\lambda x, y) = \lambda s(x, y)$ $s(y, x) = \overline{s(x, y)}$ $s(x, x) \geq 0$ $s(x, x) = 0 \implies x = 0$ then $s$ is called inner product. Problem. For each $n = 1, 2, 3, 4, 5$ find a function $s$ that doesn't satisfy the $n$ -th property and satisfies the remaining four. First consider $K = \mathbb{R}$ . I found the following: $n = 3, s(x, y) = xy^3$ $n = 4, s(x, y) = -xy$ $n = 5, s(x, y) \equiv 0$ How can I approach $n = 1, 2$ ? Perhaps I need to choose $K = \mathbb{C}$ for those? Edit : I changed the domain of $s$ from $\mathbb{R} \times \mathbb{R}$ to $K^m \times K^m$ because if $\lambda \in \mathbb{C}$ then $\mathbb{R}$ is not closed w.r.t. scalar multiplication and if $s: K \times K \mapsto K$ and 2-5 hold then 1 must hold.",Let us consider function (here or ). If then is called inner product. Problem. For each find a function that doesn't satisfy the -th property and satisfies the remaining four. First consider . I found the following: How can I approach ? Perhaps I need to choose for those? Edit : I changed the domain of from to because if then is not closed w.r.t. scalar multiplication and if and 2-5 hold then 1 must hold.,"s:K^m \times K^m \mapsto K K = \mathbb{R} K = \mathbb{C} \forall x, y, z \in K^m, \forall \lambda \in K s(x + y, z) = s(x, z) + s(y, z) s(\lambda x, y) = \lambda s(x, y) s(y, x) = \overline{s(x, y)} s(x, x) \geq 0 s(x, x) = 0 \implies x = 0 s n = 1, 2, 3, 4, 5 s n K = \mathbb{R} n = 3, s(x, y) = xy^3 n = 4, s(x, y) = -xy n = 5, s(x, y) \equiv 0 n = 1, 2 K = \mathbb{C} s \mathbb{R} \times \mathbb{R} K^m \times K^m \lambda \in \mathbb{C} \mathbb{R} s: K \times K \mapsto K","['linear-algebra', 'inner-products', 'functional-equations']"
91,Cayley-Hamilton Theorem - Trace of Exterior Power Form,Cayley-Hamilton Theorem - Trace of Exterior Power Form,,"Let $V$ be an $n$-dimensional vector space over a field $F$ (the characteristic of which, for the purpose of this post, may be taken as $0$). Let $T$ be a linear operator on $V$ and $\lambda\in F$. Sometime ago, somewhere (I can't recall where) I read that Formula. $\det(T-\lambda I)= \sum_{k=0}^n (-1)^k \text{trace}(\Lambda^k T)\lambda^{n-k}$ I considered a special case to test this out by taking $n=3$. And here is what I got: Let $\{e_1, e_2, e_3\}$ form a basis for $V$. Then  $$\det(T-\lambda I)(e_1\wedge e_2\wedge e_3)=(Te_1-\lambda e_1)\wedge (Te_2-\lambda e_2)\wedge (Te_3- \lambda e_3)$$ whence expanding the RHS we get $$(\det T)(e_1\wedge e_2\wedge e_3) - \lambda(Te_2\wedge Te_2 \wedge e_3+ Te_1\wedge e_2\wedge Te_3 + e_1\wedge Te_2\wedge Te_3)+\lambda^2(Te_1\wedge e_2\wedge e_3+ e_1\wedge Te_2\wedge e_3+e_1\wedge e_2 \wedge Te_3) - \lambda^3(e_1\wedge e_2\wedge e_3)$$ Since $\det T=\text{trace}(\Lambda^3 T)$, the coefficient of $\lambda^0$ matches with that in the formula. Also, the coefficient of $\lambda^2$ is just $\text{trace}(T)$ by definition so this is also fine. The Problem. What I am not able to see is how the coefficient of $\lambda$ eaul to $\text{trace}(\Lambda^2T)$. Can somebody please help. Thanks.","Let $V$ be an $n$-dimensional vector space over a field $F$ (the characteristic of which, for the purpose of this post, may be taken as $0$). Let $T$ be a linear operator on $V$ and $\lambda\in F$. Sometime ago, somewhere (I can't recall where) I read that Formula. $\det(T-\lambda I)= \sum_{k=0}^n (-1)^k \text{trace}(\Lambda^k T)\lambda^{n-k}$ I considered a special case to test this out by taking $n=3$. And here is what I got: Let $\{e_1, e_2, e_3\}$ form a basis for $V$. Then  $$\det(T-\lambda I)(e_1\wedge e_2\wedge e_3)=(Te_1-\lambda e_1)\wedge (Te_2-\lambda e_2)\wedge (Te_3- \lambda e_3)$$ whence expanding the RHS we get $$(\det T)(e_1\wedge e_2\wedge e_3) - \lambda(Te_2\wedge Te_2 \wedge e_3+ Te_1\wedge e_2\wedge Te_3 + e_1\wedge Te_2\wedge Te_3)+\lambda^2(Te_1\wedge e_2\wedge e_3+ e_1\wedge Te_2\wedge e_3+e_1\wedge e_2 \wedge Te_3) - \lambda^3(e_1\wedge e_2\wedge e_3)$$ Since $\det T=\text{trace}(\Lambda^3 T)$, the coefficient of $\lambda^0$ matches with that in the formula. Also, the coefficient of $\lambda^2$ is just $\text{trace}(T)$ by definition so this is also fine. The Problem. What I am not able to see is how the coefficient of $\lambda$ eaul to $\text{trace}(\Lambda^2T)$. Can somebody please help. Thanks.",,"['linear-algebra', 'determinant', 'multilinear-algebra', 'trace', 'exterior-algebra']"
92,Can epsilon be a matrix?,Can epsilon be a matrix?,,"Question In the following expression can $\epsilon$ be a matrix? $$ (H + \epsilon H_1) ( |m\rangle +\epsilon|m_1\rangle + \epsilon^2 |m_2\rangle + \dots) = (E |m\rangle + \epsilon E|m_1\rangle + \epsilon^2 E_2 |m_2\rangle + \dots) ( |m\rangle +\epsilon|m_1\rangle + \epsilon^2 |m_2\rangle + \dots) $$ Background So in quantum mechanics we generally have a solution $|m\rangle$ to a Hamiltonian: $$ H | m\rangle = E |m\rangle $$ Now using perturbation theory: $$ (H + \epsilon H_1) ( |m\rangle +\epsilon|m_1\rangle + \epsilon^2 |m_2\rangle + \dots) = (E |m\rangle + \epsilon E|m_1\rangle + \epsilon^2 E_2 |m_2\rangle + \dots) ( |m\rangle +\epsilon|m_1\rangle + \epsilon^2 |m_2\rangle + \dots) $$ I was curious and substituted $\epsilon$ as a matrix: $$ \epsilon = \left( \begin{array}{cc} 0 & 0 \\ 1 & 0 \end{array} \right) $$ where $\epsilon$ now, is the nilpotent matrix, we get: $$ \left( \begin{array}{cc} H | m \rangle & 0 \\ H_1 |m_1 \rangle + H | m\rangle & H |m_1 \rangle \end{array} \right) = \left( \begin{array}{cc} E | m \rangle & 0 \\ E_1 |m_1 \rangle + E | m\rangle & E |m_1 \rangle \end{array} \right)$$ Which is what we'd expect if we compared powers of $\epsilon$'s. All this made me wonder if $\epsilon$ could be a matrix? Say something like $| m_k\rangle \langle m_k |$ ? Say we chose $\epsilon \to \hat I \epsilon$ then there exists a radius of convergence. What is the radius of convergence in a general case of any matrix?","Question In the following expression can $\epsilon$ be a matrix? $$ (H + \epsilon H_1) ( |m\rangle +\epsilon|m_1\rangle + \epsilon^2 |m_2\rangle + \dots) = (E |m\rangle + \epsilon E|m_1\rangle + \epsilon^2 E_2 |m_2\rangle + \dots) ( |m\rangle +\epsilon|m_1\rangle + \epsilon^2 |m_2\rangle + \dots) $$ Background So in quantum mechanics we generally have a solution $|m\rangle$ to a Hamiltonian: $$ H | m\rangle = E |m\rangle $$ Now using perturbation theory: $$ (H + \epsilon H_1) ( |m\rangle +\epsilon|m_1\rangle + \epsilon^2 |m_2\rangle + \dots) = (E |m\rangle + \epsilon E|m_1\rangle + \epsilon^2 E_2 |m_2\rangle + \dots) ( |m\rangle +\epsilon|m_1\rangle + \epsilon^2 |m_2\rangle + \dots) $$ I was curious and substituted $\epsilon$ as a matrix: $$ \epsilon = \left( \begin{array}{cc} 0 & 0 \\ 1 & 0 \end{array} \right) $$ where $\epsilon$ now, is the nilpotent matrix, we get: $$ \left( \begin{array}{cc} H | m \rangle & 0 \\ H_1 |m_1 \rangle + H | m\rangle & H |m_1 \rangle \end{array} \right) = \left( \begin{array}{cc} E | m \rangle & 0 \\ E_1 |m_1 \rangle + E | m\rangle & E |m_1 \rangle \end{array} \right)$$ Which is what we'd expect if we compared powers of $\epsilon$'s. All this made me wonder if $\epsilon$ could be a matrix? Say something like $| m_k\rangle \langle m_k |$ ? Say we chose $\epsilon \to \hat I \epsilon$ then there exists a radius of convergence. What is the radius of convergence in a general case of any matrix?",,"['linear-algebra', 'matrices', 'convergence-divergence', 'quantum-mechanics', 'perturbation-theory']"
93,$3\times 3$ matrix always has determinant $0$. Must $7$ of the elements be $0$?,matrix always has determinant . Must  of the elements be ?,3\times 3 0 7 0,"Let $M$ be a $3\times 3$ real matrix with at least $3$ distinct elements and the property that any permutation of it's elements gives a matrix with determinant $0$ . Must $M$ contain exactly seven $0$ s? This question is a special case of my previous question: Exactly $n-1$ nonzero elements if $\det(A)=0$ for every arrangement Thanks to user Holonomia, we know a counterexample to the analogous question for $2\times 2$ matrices: Any $2\times 2$ matrix with two $1$ s and two $-1$ s has determinant $0$ . However, I have not been able to make much progress on the $3\times 3$ case. Theoretically the property gives us a system of $9!$ equations (with some symmetries and repeats), but I haven't found a good way to compute with this idea.","Let be a real matrix with at least distinct elements and the property that any permutation of it's elements gives a matrix with determinant . Must contain exactly seven s? This question is a special case of my previous question: Exactly $n-1$ nonzero elements if $\det(A)=0$ for every arrangement Thanks to user Holonomia, we know a counterexample to the analogous question for matrices: Any matrix with two s and two s has determinant . However, I have not been able to make much progress on the case. Theoretically the property gives us a system of equations (with some symmetries and repeats), but I haven't found a good way to compute with this idea.",M 3\times 3 3 0 M 0 2\times 2 2\times 2 1 -1 0 3\times 3 9!,"['linear-algebra', 'matrices']"
94,Determinant of block tridiagonal Toeplitz matrices,Determinant of block tridiagonal Toeplitz matrices,,"Is there a formula to compute the determinant of block tridiagonal matrices, when the determinants of the involved matrices are known? In particular, I am interested in the case $$A = \begin{pmatrix} J_n & I_n & 0 & \cdots & \cdots & 0 \\ I_n & J_n & I_n & 0 & \cdots & 0 \\ 0 & I_n & J_n  & I_n & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots &  \ddots & 0 \\ 0 & \cdots & \cdots & I_n & J_n & I_n \\ 0 & \cdots & \cdots & \cdots & I_n & J_n \end{pmatrix}$$ where $J_n$ is the $n \times n$ tridiagonal matrix whose entries on the sub-, super- and main diagonals are all equal to $1$ and $I_n$ is identity matrix of size $n$ .","Is there a formula to compute the determinant of block tridiagonal matrices, when the determinants of the involved matrices are known? In particular, I am interested in the case where is the tridiagonal matrix whose entries on the sub-, super- and main diagonals are all equal to and is identity matrix of size .",A = \begin{pmatrix} J_n & I_n & 0 & \cdots & \cdots & 0 \\ I_n & J_n & I_n & 0 & \cdots & 0 \\ 0 & I_n & J_n  & I_n & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots &  \ddots & 0 \\ 0 & \cdots & \cdots & I_n & J_n & I_n \\ 0 & \cdots & \cdots & \cdots & I_n & J_n \end{pmatrix} J_n n \times n 1 I_n n,"['linear-algebra', 'matrices', 'determinant', 'block-matrices', 'toeplitz-matrices']"
95,"Explicit homotopy equivalence of homogeneous spaces $O(2n)/U(n)$ and $GL(2n,\mathbb{R})/GL(n,\mathbb{C})$",Explicit homotopy equivalence of homogeneous spaces  and,"O(2n)/U(n) GL(2n,\mathbb{R})/GL(n,\mathbb{C})","Exercise 2.25 of symplectic topology by McDuff and Salamon asks me to prove that $O(2n)/U(n)$ is homotopy equivalent to $GL(2n,\mathbb{R})/GL(n,\mathbb{C})$. They suggest to use the polar decomposition. I guess they mean something like this: if $A=PQ$, with $P$ positive definite, $Q$ orthogonal, then $P=\sqrt{AA^T}$, and $Q=(\sqrt{AA^T})^{-1}A$, so this decomposition is continuous in $A$. The obvious thing to try is then $$\Psi_t(A)=(AA^T)^{-t/2}A$$ This retracts $GL(2n,\mathbb{R})$ to $O(2n)$. However, if I multiply $A$ by a matrix $B$, I have $$ \Psi_t(AB)=(ABB^TA^T)^{t/2}AB $$ Now if $B$ is in $U(n)$ then I see that $\Psi_t(AB)=\Psi_t(A)B$ as $BB^T=1$, so it passes to a homotopy equivalence of the quotients $O(2n)/U(n)$ and $GL(2n,\mathbb{R})/U(n)$. This argument does not work for $B\in GL(n,\mathbb{C})$. What did McDuff and Salamon have in mind? ps: Note that I am not looking for another proof of the statement (see also Inclusion $O(2n)/U(n)\to GL(2n,\mathbb{R})/GL(n,\mathbb{C}) $ ). I think the following argument works: We have a diagram of fibrations $$\begin{array}{ccc} U(n) & \rightarrow & O(2n) &\rightarrow&O(2n)/U(n)\\ \downarrow &  & \downarrow &  & \downarrow & \\ GL(n,\mathbb{C}) & \rightarrow & GL(2n,\mathbb{R}) & \rightarrow & GL(2n,\mathbb{R})/GL(n,\mathbb{C}) \end{array}$$ The first two vertical maps are homotopy equivalences. Now the long exact sequence of homotopy groups, the five lemma, and Whitehead will finish the job. But I am looking for an explicit homotopy equivalence.","Exercise 2.25 of symplectic topology by McDuff and Salamon asks me to prove that $O(2n)/U(n)$ is homotopy equivalent to $GL(2n,\mathbb{R})/GL(n,\mathbb{C})$. They suggest to use the polar decomposition. I guess they mean something like this: if $A=PQ$, with $P$ positive definite, $Q$ orthogonal, then $P=\sqrt{AA^T}$, and $Q=(\sqrt{AA^T})^{-1}A$, so this decomposition is continuous in $A$. The obvious thing to try is then $$\Psi_t(A)=(AA^T)^{-t/2}A$$ This retracts $GL(2n,\mathbb{R})$ to $O(2n)$. However, if I multiply $A$ by a matrix $B$, I have $$ \Psi_t(AB)=(ABB^TA^T)^{t/2}AB $$ Now if $B$ is in $U(n)$ then I see that $\Psi_t(AB)=\Psi_t(A)B$ as $BB^T=1$, so it passes to a homotopy equivalence of the quotients $O(2n)/U(n)$ and $GL(2n,\mathbb{R})/U(n)$. This argument does not work for $B\in GL(n,\mathbb{C})$. What did McDuff and Salamon have in mind? ps: Note that I am not looking for another proof of the statement (see also Inclusion $O(2n)/U(n)\to GL(2n,\mathbb{R})/GL(n,\mathbb{C}) $ ). I think the following argument works: We have a diagram of fibrations $$\begin{array}{ccc} U(n) & \rightarrow & O(2n) &\rightarrow&O(2n)/U(n)\\ \downarrow &  & \downarrow &  & \downarrow & \\ GL(n,\mathbb{C}) & \rightarrow & GL(2n,\mathbb{R}) & \rightarrow & GL(2n,\mathbb{R})/GL(n,\mathbb{C}) \end{array}$$ The first two vertical maps are homotopy equivalences. Now the long exact sequence of homotopy groups, the five lemma, and Whitehead will finish the job. But I am looking for an explicit homotopy equivalence.",,"['linear-algebra', 'lie-groups', 'symplectic-geometry', 'symplectic-linear-algebra']"
96,"If G is finite group with even number of elements and has identity e, there is a in G such that a*a=e","If G is finite group with even number of elements and has identity e, there is a in G such that a*a=e",,"My approach is ; I subtract e from G then G-{e} has odd number of elements. For any element in G-{e}, there must be an inverse of that element in G-{e}. Take any element in G-{e}, say b,  If b*b=e, then proof is done. If inverse of b is not itself, then there must be one element in G-{e}, say c, such that b*c=e. Because there are odd number of elements, I keep doing this until I have left with one element that does not have a pair, say d. any other elements in G-{e} can not be inverse of d because they all have pairs, and e can not be an inverse of d. Therefore d has to be inverse of itself. Is my proof valid? or Can anyone modify it please?","My approach is ; I subtract e from G then G-{e} has odd number of elements. For any element in G-{e}, there must be an inverse of that element in G-{e}. Take any element in G-{e}, say b,  If b*b=e, then proof is done. If inverse of b is not itself, then there must be one element in G-{e}, say c, such that b*c=e. Because there are odd number of elements, I keep doing this until I have left with one element that does not have a pair, say d. any other elements in G-{e} can not be inverse of d because they all have pairs, and e can not be an inverse of d. Therefore d has to be inverse of itself. Is my proof valid? or Can anyone modify it please?",,"['linear-algebra', 'abstract-algebra', 'group-theory']"
97,Prove that Either $T$ Is Diagonalizable or $T$ Is Nilpotent.,Prove that Either  Is Diagonalizable or  Is Nilpotent.,T T,"(Linear Algebra - Hoffman, Kunze, 2nd Ed., Sec 6.8, Q6) Let $V$ be a finite-dimensional vector space over the field $\mathbb{F}$ , and let $T$ be a linear operator on $V$ such that $\textrm{rank}  (T) = 1$ . Prove that either $T$ is diagonalizable or $T$ is nilpotent, not both. Here's how I proceeded: Let $\textrm{dim} V = n$ . By the $\textrm{rank}(T) + \textrm{null}(T) = \textrm{dim} V$ theorem, $\textrm{null} (T) = n-1$ . Now let $B = \{a_1, a_2,... , a_{n-1}, a_n\}$ be a basis for $V$ , with $B' = \{a_2,...,a_n\}$ a basis for $\ker(T)$ . Now, let $$A = [T]_B= \left(\begin{array}[cccc]   (c_1 & 0 & \cdots & 0\\ c_2 & 0 & \cdots & 0\\  \vdots & \vdots & \ddots & \vdots\\ c_n & 0 & \cdots & 0 \end{array}\right).$$ Where $T(a_1) = c_1(a_1) + c_2(a_2) + ... + c_n(a_n)$ Now I argue as follows: if $c_1 = 0$ , then $A$ (and $T$ ) is nilpotent with $A^2 = 0$ . Since the minimal polynomial is not a linear factor, $A$ (and $T$ ) is not diagonilazable. On the other hand, if $c_1\neq 0$ , then $A$ (and $T$ ) is diagonalizable, since the minimal polynomial = $x(x-c_1)$ . Is my solution correct?","(Linear Algebra - Hoffman, Kunze, 2nd Ed., Sec 6.8, Q6) Let be a finite-dimensional vector space over the field , and let be a linear operator on such that . Prove that either is diagonalizable or is nilpotent, not both. Here's how I proceeded: Let . By the theorem, . Now let be a basis for , with a basis for . Now, let Where Now I argue as follows: if , then (and ) is nilpotent with . Since the minimal polynomial is not a linear factor, (and ) is not diagonilazable. On the other hand, if , then (and ) is diagonalizable, since the minimal polynomial = . Is my solution correct?","V \mathbb{F} T V \textrm{rank}  (T) = 1 T T \textrm{dim} V = n \textrm{rank}(T) + \textrm{null}(T) = \textrm{dim} V \textrm{null} (T) = n-1 B = \{a_1, a_2,... , a_{n-1}, a_n\} V B' = \{a_2,...,a_n\} \ker(T) A = [T]_B= \left(\begin{array}[cccc]  
(c_1 & 0 & \cdots & 0\\
c_2 & 0 & \cdots & 0\\ 
\vdots & \vdots & \ddots & \vdots\\
c_n & 0 & \cdots & 0 \end{array}\right). T(a_1) = c_1(a_1) + c_2(a_2) + ... + c_n(a_n) c_1 = 0 A T A^2 = 0 A T c_1\neq 0 A T x(x-c_1)",['linear-algebra']
98,Matrices which commute with all the matrices commuting with a given matrix [duplicate],Matrices which commute with all the matrices commuting with a given matrix [duplicate],,"This question already has an answer here : Matrices $B$ that commute with every matrix commuting with $A$ (1 answer) Closed 9 years ago . Let $A$ be an $n \times n$ matrix with entries from an arbitrary field $F$ and let $C(A)$ denote the set of all matrices which commute with $A$. Is it true that $C(C(A))= \{ \alpha_1 + \alpha_2 A + \cdots + \alpha_{n-1}A^{n-1}\mid\alpha_i \in F  \}$ ? This problem is a generalized version of Problems 6.3.13, 6.3.14 of Herstein's Topics in Algebra which I am unable to do. Herstein asks to prove that this holds for the cases $n=2,3$. Of course, one way to go about it is by brute force calculation (which I have not tried), but I guess there is a more conceptual way to do this which I am unable to find. Clearly all the matrices of the type in R.H.S. are in $C(C(A))$ so the part I am unable to do is that these are the only matrices. I can see that $C(A)$ and $C(C(A))$ are themselves vector spaces but what to do further ?","This question already has an answer here : Matrices $B$ that commute with every matrix commuting with $A$ (1 answer) Closed 9 years ago . Let $A$ be an $n \times n$ matrix with entries from an arbitrary field $F$ and let $C(A)$ denote the set of all matrices which commute with $A$. Is it true that $C(C(A))= \{ \alpha_1 + \alpha_2 A + \cdots + \alpha_{n-1}A^{n-1}\mid\alpha_i \in F  \}$ ? This problem is a generalized version of Problems 6.3.13, 6.3.14 of Herstein's Topics in Algebra which I am unable to do. Herstein asks to prove that this holds for the cases $n=2,3$. Of course, one way to go about it is by brute force calculation (which I have not tried), but I guess there is a more conceptual way to do this which I am unable to find. Clearly all the matrices of the type in R.H.S. are in $C(C(A))$ so the part I am unable to do is that these are the only matrices. I can see that $C(A)$ and $C(C(A))$ are themselves vector spaces but what to do further ?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'noncommutative-algebra']"
99,Simple to state yet tricky question,Simple to state yet tricky question,,"Define $$A=\left[\mathrm I+\sum_{k=1}^{m_1}v_k v_k^T+\sum_{k=1}^{m_2}u_k u_k^T\right]^{-1},$$ where each $u_k$ and $v_k$ is a $0$-$1$ column vector, and for each $1\leq i \leq n$, the $i$th component is $1$ in exactly one of $u$ vectors and in exactly one of $v$ vectors. Prove that for every $1\leq k \leq m$ if the $i'th$ element of $u_k$ is $1$ then $i$th  component of $Au_{k}$ is positive: $$u_{k,i}=1 \implies (Au_{k})_{i}>0 $$ and same thing for $v$ vectors: $$v_{k,i}=1 \implies (Av_k)_i>0$$ I checked this with computer and it seems correct.","Define $$A=\left[\mathrm I+\sum_{k=1}^{m_1}v_k v_k^T+\sum_{k=1}^{m_2}u_k u_k^T\right]^{-1},$$ where each $u_k$ and $v_k$ is a $0$-$1$ column vector, and for each $1\leq i \leq n$, the $i$th component is $1$ in exactly one of $u$ vectors and in exactly one of $v$ vectors. Prove that for every $1\leq k \leq m$ if the $i'th$ element of $u_k$ is $1$ then $i$th  component of $Au_{k}$ is positive: $$u_{k,i}=1 \implies (Au_{k})_{i}>0 $$ and same thing for $v$ vectors: $$v_{k,i}=1 \implies (Av_k)_i>0$$ I checked this with computer and it seems correct.",,"['linear-algebra', 'matrices', 'inverse', 'block-matrices']"
