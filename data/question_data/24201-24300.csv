,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,The derivative of the determinant of a Kronecker product,The derivative of the determinant of a Kronecker product,,"For an invertible matrix $A$, we have the identity \begin{align} \dfrac{\partial \det A}{\partial A} = \det A (A^{-1})^T \end{align} where the $T$ denotes the transpose operation. How does this formula change when considering the Kronecker product of $A$ with some other (invertible) matrix $B$? For example, how does one compute the following: \begin{align} \dfrac{\partial \det \left[A\otimes B\right]}{\partial A} \end{align}","For an invertible matrix $A$, we have the identity \begin{align} \dfrac{\partial \det A}{\partial A} = \det A (A^{-1})^T \end{align} where the $T$ denotes the transpose operation. How does this formula change when considering the Kronecker product of $A$ with some other (invertible) matrix $B$? For example, how does one compute the following: \begin{align} \dfrac{\partial \det \left[A\otimes B\right]}{\partial A} \end{align}",,"['linear-algebra', 'tensor-products', 'tensors', 'matrix-equations', 'matrix-calculus']"
1,"Is there a way to find $B,C$ such that $A=[B,C]$?",Is there a way to find  such that ?,"B,C A=[B,C]","The fact that $\mathfrak{sl}_2(\mathbb{C})$ is a simple Lie algebra implies that every $2 \times 2$-matrix $A \in \mathbb{C}^{2\times 2}$ with $\mathrm{tr}(A) = 0$ can be expressed as a commutator of two matrices. Is there any way to find two appropriate matrices? If I give you $A = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$, for example, is there any algorithm that can find matrices $B,C$ with $A = BC - CB$?","The fact that $\mathfrak{sl}_2(\mathbb{C})$ is a simple Lie algebra implies that every $2 \times 2$-matrix $A \in \mathbb{C}^{2\times 2}$ with $\mathrm{tr}(A) = 0$ can be expressed as a commutator of two matrices. Is there any way to find two appropriate matrices? If I give you $A = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$, for example, is there any algorithm that can find matrices $B,C$ with $A = BC - CB$?",,"['linear-algebra', 'matrices', 'lie-algebras']"
2,"Show that if $A$ is invertible and $AB = AC$, then $B = C$.","Show that if  is invertible and , then .",A AB = AC B = C,"Question: Show that if $A$ is invertible and $AB = AC$, then $B = C$. My work: My thought process: If I can find the inverse of $A$, then I can show A is invertible. I will prove by example. $A$ is a $2 \times 2$ matrix. first row: 2 3. second row: 4 5. Where $a = 2$, $b = 3$, $c = 4$, $d = 5$ such that $ad - bc$ is not zero. $ad-bc$ or the determinant is $-2$ which is not equal to zero so I know matrix $A$ is invertible. $A$ inverse would be...first row is -5/2 3/2. second row is 2 -1. Matrix $C$ is equal to $A$ inverse or matrix $B$. $A$ inverse and matrix $B$ are the same thing.","Question: Show that if $A$ is invertible and $AB = AC$, then $B = C$. My work: My thought process: If I can find the inverse of $A$, then I can show A is invertible. I will prove by example. $A$ is a $2 \times 2$ matrix. first row: 2 3. second row: 4 5. Where $a = 2$, $b = 3$, $c = 4$, $d = 5$ such that $ad - bc$ is not zero. $ad-bc$ or the determinant is $-2$ which is not equal to zero so I know matrix $A$ is invertible. $A$ inverse would be...first row is -5/2 3/2. second row is 2 -1. Matrix $C$ is equal to $A$ inverse or matrix $B$. $A$ inverse and matrix $B$ are the same thing.",,"['linear-algebra', 'matrices']"
3,"Bilinear form with symmetric ""perpendicular"" relation is either symmetric or skew-symmetric","Bilinear form with symmetric ""perpendicular"" relation is either symmetric or skew-symmetric",,"Let $b$ be a bilinear form on a finite-dimension vector space $V$ (over a field with char $\neq$ 2) such that for each $x,y\in V$ one has $b(x,y)=0\Leftrightarrow b(y,x)=0$. Prove that $b$ is symmetric or skew-symmetric. The condition is equal to this: for every vector $x$ co-vectors $b(\cdot,x)$ and $b(x,\cdot)$ have the same kernels, so for some non-zero  constant $c:b(\cdot,x)=cb(x,\cdot)$. But I didn't manage to prove that $c$ is equal to $1$ or $-1$ for every $x$.","Let $b$ be a bilinear form on a finite-dimension vector space $V$ (over a field with char $\neq$ 2) such that for each $x,y\in V$ one has $b(x,y)=0\Leftrightarrow b(y,x)=0$. Prove that $b$ is symmetric or skew-symmetric. The condition is equal to this: for every vector $x$ co-vectors $b(\cdot,x)$ and $b(x,\cdot)$ have the same kernels, so for some non-zero  constant $c:b(\cdot,x)=cb(x,\cdot)$. But I didn't manage to prove that $c$ is equal to $1$ or $-1$ for every $x$.",,"['linear-algebra', 'bilinear-form']"
4,$n^{th}$ root of a matrix.,root of a matrix.,n^{th},What conditions do I need on a matrix $A$ in order to know an $n^{th}$ root exists. In other words there is a matrix $B$ such that $B^n=A$ for $n \in \mathbb{Z}^+$.,What conditions do I need on a matrix $A$ in order to know an $n^{th}$ root exists. In other words there is a matrix $B$ such that $B^n=A$ for $n \in \mathbb{Z}^+$.,,"['linear-algebra', 'matrices']"
5,How to compute (and check) this transform matrix?,How to compute (and check) this transform matrix?,,"Background: This is a homework exercise which asks to compute a transform matrix. The answer has been published by our teacher. However, my approach goes a different way and gets a different solution. I checked over and over, but failed to identify the error. The Exercise Problem: Suppose $X \in R^{2 \times 2}$, define a linear transformation over $R^{2 \times 2}$ as: $ \mathbf{T(X)} = \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix} X$. Please compute its transform matrix under the following basis: $\mathbf{E_1} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ \end{bmatrix}$, $\mathbf{E_2} = \begin{bmatrix} 1 & 1 \\ 0 & 0 \\ \end{bmatrix}$, $\mathbf{E_3} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ \end{bmatrix}$, $\mathbf{E_4} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \\ \end{bmatrix}$. My solution: My approach is simply to transform each of the vectors of the basis by $T$, then insert the result into the columns of a matrix.       \begin{cases}         T(E_1) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix}         \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ \end{bmatrix}         = \begin{bmatrix} 1 & 0 \\ 2 & 0 \\ \end{bmatrix} = E_1 - 2E_3 + 2E_4.         \\[15pt]         T(E_2) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix}         \begin{bmatrix} 1 & 1 \\ 0 & 0 \\ \end{bmatrix}         = \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix} = -E_2 + 2E_4.         \\[15pt]         T(E_3) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix} 		\begin{bmatrix} 1 & 1 \\ 0 & 1 \\ \end{bmatrix} 		= \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ \end{bmatrix} = -E_1 - 2E_2 + 2E_3 + 2E_4. 		\\[15pt] 		T(E_4) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix} 		\begin{bmatrix} 1 & 1 \\ 1 & 1 \\\end{bmatrix} 		= \begin{bmatrix} 2 & 2 \\ 4 & 4 \\ \end{bmatrix} = -2E_2 + E_4.		       \end{cases}   Thus, the transform matrix of $T$ is:     $B = \begin{bmatrix}       1 & 0 & -1 & 0 \\       0 & -2 & -2 & -2 \\       -2 & 0 & 2 & 0 \\       2 & 2 & 2 & 1 \\     \end{bmatrix}$. My teacher's solution: This approach first computes the transform matrix of $T$ under the natural basis of $R^{2 \times 2}$, that is, $\mathbf{E_{11}} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ \end{bmatrix}$, $\mathbf{E_{12}} = \begin{bmatrix} 0 & 1 \\ 0 & 0 \\ \end{bmatrix}$, $\mathbf{E_{21}} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \\ \end{bmatrix}$, $\mathbf{E_{22}} = \begin{bmatrix} 0 & 0 \\ 0 & 1 \\ \end{bmatrix}$.        \begin{cases}         T(E_{11}) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix}         \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ \end{bmatrix}         = \begin{bmatrix} 1 & 0 \\ 2 & 0 \\ \end{bmatrix} = E_{11} + 0 E_{12} + 2E_{21} + 0E_{22}.         \\[15pt]         T(E_{12}) &= 0 E_{11} + E_{12} + 0 E_{21} + 2E_{22}.         \\[15pt]         T(E_{21}) &= E_{11} + 0 E_{12} + 2E_{21} + 0 E_{22}.         \\[15pt]         T(E_4) &= 0 E_{11} + 1 E_{12} + 0 E_{21} + 2E_{22}.		       \end{cases} Therefore,         $T(E_{11},E_{12},E_{21},E_{22}) = (E_{11},E_{12},E_{21},E_{22}) \begin{bmatrix} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \end{bmatrix} = (E_{11},E_{12},E_{21},E_{22}) A$. It then takes advantage of the transform matrix $C$ from the natural basis to the target basis: $C = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix}$. So, the transform matrix of $T$ under the target basis is: $B = C^{-1}AC = \begin{bmatrix} 1 & -1 & 0 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 1 & -1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \cdot \begin{bmatrix} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \end{bmatrix} \cdot \begin{bmatrix} 1 & 1 & 1 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 1 & 0 \\ -2 & -1 & -3 & -2 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 4 \end{bmatrix}$. My Question: As you can see, the two answers are different. Then, what is wrong with my solution? How to check whether a transform matrix has been correctly computed without the teacher's answer?","Background: This is a homework exercise which asks to compute a transform matrix. The answer has been published by our teacher. However, my approach goes a different way and gets a different solution. I checked over and over, but failed to identify the error. The Exercise Problem: Suppose $X \in R^{2 \times 2}$, define a linear transformation over $R^{2 \times 2}$ as: $ \mathbf{T(X)} = \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix} X$. Please compute its transform matrix under the following basis: $\mathbf{E_1} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ \end{bmatrix}$, $\mathbf{E_2} = \begin{bmatrix} 1 & 1 \\ 0 & 0 \\ \end{bmatrix}$, $\mathbf{E_3} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ \end{bmatrix}$, $\mathbf{E_4} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \\ \end{bmatrix}$. My solution: My approach is simply to transform each of the vectors of the basis by $T$, then insert the result into the columns of a matrix.       \begin{cases}         T(E_1) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix}         \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ \end{bmatrix}         = \begin{bmatrix} 1 & 0 \\ 2 & 0 \\ \end{bmatrix} = E_1 - 2E_3 + 2E_4.         \\[15pt]         T(E_2) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix}         \begin{bmatrix} 1 & 1 \\ 0 & 0 \\ \end{bmatrix}         = \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix} = -E_2 + 2E_4.         \\[15pt]         T(E_3) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix} 		\begin{bmatrix} 1 & 1 \\ 0 & 1 \\ \end{bmatrix} 		= \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ \end{bmatrix} = -E_1 - 2E_2 + 2E_3 + 2E_4. 		\\[15pt] 		T(E_4) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix} 		\begin{bmatrix} 1 & 1 \\ 1 & 1 \\\end{bmatrix} 		= \begin{bmatrix} 2 & 2 \\ 4 & 4 \\ \end{bmatrix} = -2E_2 + E_4.		       \end{cases}   Thus, the transform matrix of $T$ is:     $B = \begin{bmatrix}       1 & 0 & -1 & 0 \\       0 & -2 & -2 & -2 \\       -2 & 0 & 2 & 0 \\       2 & 2 & 2 & 1 \\     \end{bmatrix}$. My teacher's solution: This approach first computes the transform matrix of $T$ under the natural basis of $R^{2 \times 2}$, that is, $\mathbf{E_{11}} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ \end{bmatrix}$, $\mathbf{E_{12}} = \begin{bmatrix} 0 & 1 \\ 0 & 0 \\ \end{bmatrix}$, $\mathbf{E_{21}} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \\ \end{bmatrix}$, $\mathbf{E_{22}} = \begin{bmatrix} 0 & 0 \\ 0 & 1 \\ \end{bmatrix}$.        \begin{cases}         T(E_{11}) &= \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ \end{bmatrix}         \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ \end{bmatrix}         = \begin{bmatrix} 1 & 0 \\ 2 & 0 \\ \end{bmatrix} = E_{11} + 0 E_{12} + 2E_{21} + 0E_{22}.         \\[15pt]         T(E_{12}) &= 0 E_{11} + E_{12} + 0 E_{21} + 2E_{22}.         \\[15pt]         T(E_{21}) &= E_{11} + 0 E_{12} + 2E_{21} + 0 E_{22}.         \\[15pt]         T(E_4) &= 0 E_{11} + 1 E_{12} + 0 E_{21} + 2E_{22}.		       \end{cases} Therefore,         $T(E_{11},E_{12},E_{21},E_{22}) = (E_{11},E_{12},E_{21},E_{22}) \begin{bmatrix} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \end{bmatrix} = (E_{11},E_{12},E_{21},E_{22}) A$. It then takes advantage of the transform matrix $C$ from the natural basis to the target basis: $C = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix}$. So, the transform matrix of $T$ under the target basis is: $B = C^{-1}AC = \begin{bmatrix} 1 & -1 & 0 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 1 & -1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \cdot \begin{bmatrix} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \end{bmatrix} \cdot \begin{bmatrix} 1 & 1 & 1 & 1 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 1 & 0 \\ -2 & -1 & -3 & -2 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 4 \end{bmatrix}$. My Question: As you can see, the two answers are different. Then, what is wrong with my solution? How to check whether a transform matrix has been correctly computed without the teacher's answer?",,"['linear-algebra', 'matrices', 'transformation']"
6,Cramer's rule: Geometric Interpretation,Cramer's rule: Geometric Interpretation,,"I have a question concerning Cramer's rule: Let $A$ be a matrix and $A \cdot \vec x = \vec b$ a lineare equation. $A_i$ is the matrix $A$ where the i'th column is replaced by $\vec b$ if $det(A) \neq 0$, then we have a unique solution if $det(A)=0$ and at least one $det(A_i) \neq 0$, we have no solution if $det(A)=0$ and all $det(A_i)=0$ we have infinitely many solutions [false!] I'm looking for a geometric interpretation of the rule. I know that $det(A)=$area of parallelepiped, but I'm not able draw a picture for Cramer's rule. Anyone can help me here? Thanks a lot in advance,","I have a question concerning Cramer's rule: Let $A$ be a matrix and $A \cdot \vec x = \vec b$ a lineare equation. $A_i$ is the matrix $A$ where the i'th column is replaced by $\vec b$ if $det(A) \neq 0$, then we have a unique solution if $det(A)=0$ and at least one $det(A_i) \neq 0$, we have no solution if $det(A)=0$ and all $det(A_i)=0$ we have infinitely many solutions [false!] I'm looking for a geometric interpretation of the rule. I know that $det(A)=$area of parallelepiped, but I'm not able draw a picture for Cramer's rule. Anyone can help me here? Thanks a lot in advance,",,"['linear-algebra', 'geometry', 'determinant']"
7,LU factorization of a non-symmetric matrix,LU factorization of a non-symmetric matrix,,"I want to find $L$ and $U$ for the following non-symmetric matrix $A$: $  A = \begin{bmatrix} a & r & r & r \\ a & b & s & s \\  a & b & c & t \\ a & b & c & d\\      \end{bmatrix} $ I'm not sure how the matrix being non-symmetric helps me. Does $A$ have to be a square matrix which is symmetric and invertible? Do I need to calculate its determinant to be nonzero in the first place? Since I don't know the answers to the above questions, I calculated $U$ anyway: $  U = \begin{bmatrix} a&r&r&r\\ 0&b-r&s-r&s-r\\  0&0&c-s&b-s\\ 0&0&0&d-b      \end{bmatrix} $ But I don't know how to proceed about $L$. I'd also know what are the four conditions to get $A=LU$ with four pivots. I think the diagonal elements of both $L$ and $U$ must be nonzero to get four pivot elements, right? So $a \neq 0, b \neq r, c \neq s, d \neq b$, am I right?","I want to find $L$ and $U$ for the following non-symmetric matrix $A$: $  A = \begin{bmatrix} a & r & r & r \\ a & b & s & s \\  a & b & c & t \\ a & b & c & d\\      \end{bmatrix} $ I'm not sure how the matrix being non-symmetric helps me. Does $A$ have to be a square matrix which is symmetric and invertible? Do I need to calculate its determinant to be nonzero in the first place? Since I don't know the answers to the above questions, I calculated $U$ anyway: $  U = \begin{bmatrix} a&r&r&r\\ 0&b-r&s-r&s-r\\  0&0&c-s&b-s\\ 0&0&0&d-b      \end{bmatrix} $ But I don't know how to proceed about $L$. I'd also know what are the four conditions to get $A=LU$ with four pivots. I think the diagonal elements of both $L$ and $U$ must be nonzero to get four pivot elements, right? So $a \neq 0, b \neq r, c \neq s, d \neq b$, am I right?",,"['linear-algebra', 'matrices']"
8,Determinant of symmetric tridiagonal matrices,Determinant of symmetric tridiagonal matrices,,"Given an $n\times n$ tridiagonal matrix $$A =\left(\begin{array}{ccccccc} d_1&a_1\\c_2&d_2&a_2\\&c_3&d_3&a_3\\&&&\ddots\\&&&&\ddots\\&&&&c_{n-1}&d_{n-1}&a_{n-1}\\&&&&&c_n&d_n \end{array}\right)$$ and $$f_i =\left|\begin{array}{ccccccc} d_1&a_1\\c_2&d_2&a_2\\&c_3&d_3&a_3\\&&&\ddots\\&&&&\ddots\\&&&&c_{i-1}&d_{i-1}&a_{i-1}\\&&&&&c_i&d_i \end{array}\right|$$ then it is true that the determinants $f_i$ satisfy a three-term recurrence: $$f_i = d_if_{i-1} - c_ia_{i-1}f_{i-2}$$ for $$f_0=1\text{ and }f_{-1}=0$$ If we are given a symmetric tridiagonal matrix $$A =\left(\begin{array}{ccccccc} d_1&a_1\\a_1&d_2&a_2\\&a_2&d_3&a_3\\&&&\ddots\\&&&&\ddots\\&&&&a_{n-2}&d_{n-1}&a_{n-1}\\&&&&&a_{n-1}&d_n \end{array}\right)$$ is it possible to calculate the determinant more efficiently than using the recurrence relation? In other words, can we write a function that theoretically performs quicker than the following code (barring any language/programming-specific optimizations): double det(double* d, int count, double* a) {   double f0 = 1;   double f1 = d[0];   double ftemp;         for (int i = 1; i < count; i++) {     ftemp = f1;     f1 = d[i]*f1 - a[i-1]*a[i-1]*f0;     f0 = ftemp;   }   return f1; }","Given an $n\times n$ tridiagonal matrix $$A =\left(\begin{array}{ccccccc} d_1&a_1\\c_2&d_2&a_2\\&c_3&d_3&a_3\\&&&\ddots\\&&&&\ddots\\&&&&c_{n-1}&d_{n-1}&a_{n-1}\\&&&&&c_n&d_n \end{array}\right)$$ and $$f_i =\left|\begin{array}{ccccccc} d_1&a_1\\c_2&d_2&a_2\\&c_3&d_3&a_3\\&&&\ddots\\&&&&\ddots\\&&&&c_{i-1}&d_{i-1}&a_{i-1}\\&&&&&c_i&d_i \end{array}\right|$$ then it is true that the determinants $f_i$ satisfy a three-term recurrence: $$f_i = d_if_{i-1} - c_ia_{i-1}f_{i-2}$$ for $$f_0=1\text{ and }f_{-1}=0$$ If we are given a symmetric tridiagonal matrix $$A =\left(\begin{array}{ccccccc} d_1&a_1\\a_1&d_2&a_2\\&a_2&d_3&a_3\\&&&\ddots\\&&&&\ddots\\&&&&a_{n-2}&d_{n-1}&a_{n-1}\\&&&&&a_{n-1}&d_n \end{array}\right)$$ is it possible to calculate the determinant more efficiently than using the recurrence relation? In other words, can we write a function that theoretically performs quicker than the following code (barring any language/programming-specific optimizations): double det(double* d, int count, double* a) {   double f0 = 1;   double f1 = d[0];   double ftemp;         for (int i = 1; i < count; i++) {     ftemp = f1;     f1 = d[i]*f1 - a[i-1]*a[i-1]*f0;     f0 = ftemp;   }   return f1; }",,"['linear-algebra', 'matrices', 'determinant', 'symmetric-matrices', 'tridiagonal-matrices']"
9,"Find characteristic polynomial of $\,A^2$ if the characteristic polynomial of $\,A$ is $\,t^4 -t$",Find characteristic polynomial of  if the characteristic polynomial of  is,"\,A^2 \,A \,t^4 -t","$A \in M_{4\times4}(\mathbb{R})$ . The characteristic polynomial of $A$ is $P_A(t)=t^4-t$ . I have to find the characteristic polynomial of $A^2$ and $A^4$ . So I know that due to the Cayley–Hamilton theorem that $P_A(A) = A^4-A=0$ , therefore $A^4 = A$ , Therefore $P_{A^4}(t)=P_A(t)=t^4-t$ . But what do I do with $A^2$ ? I also know that the eigenvalues of A are $0,1$ ... how does this help? Thank you all.",". The characteristic polynomial of is . I have to find the characteristic polynomial of and . So I know that due to the Cayley–Hamilton theorem that , therefore , Therefore . But what do I do with ? I also know that the eigenvalues of A are ... how does this help? Thank you all.","A \in M_{4\times4}(\mathbb{R}) A P_A(t)=t^4-t A^2 A^4 P_A(A) = A^4-A=0 A^4 = A P_{A^4}(t)=P_A(t)=t^4-t A^2 0,1","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
10,Should isometries be linear?,Should isometries be linear?,,"Question Suppose $V$ is a (finite-dimensional) vector space over $F$ ($\operatorname{char }F\neq2$, due to user1551) equipped with a non-degenerate quadratic form $Q$, and $T$ is a distance-preserving operator on $V$, viz. $Q(Tu-Tv)=Q(u-v)$ for each $u,v\in V$. Is it true that $T$ is linear affine (due to user1551)? Background I'm thinking about the mathematical derivation of Lorentz transformation from the principles of special relativity. In the context, $F=\mathbb R$, $V=F^4$ and $Q$ is the Lorentz quadratic form. The original problem might be with condition that $T$ acts on the space $\mathbb R^3$ as a translate (since they are inertial frames of reference), but $Q(Tu-Tv)=Q(u-v)$ only when $Q(u-v)=0$, which means that the operator preserves light cones. On condition that $F=\mathbb R$ and $Q$ is positive definite the answer is true. It follows from a standard derivation: Suppose $\langle x,y\rangle=(Q(x+y)-Q(x)-Q(y))/2$, then by definition $\langle\circ,\circ\rangle$ is a positive definite bilinear form. Note that $\langle Tu,Tv\rangle=\frac12(Q(Tu)+Q(Tv)-Q(Tu-Tv))=\frac12(Q(u)+Q(v)-Q(u-v))=\langle u,v\rangle$, we have $Q(Tcv-cTv)=\langle Tcv-cTv,Tcv-cTv\rangle=0$ and $Q(T(u+v)-Tu-Tv)=\langle T(u+v)-Tu-Tv,T(u+v)-Tu-Tv\rangle=0$ follows, which implies that $T(u+v)=Tu+Tv$ and $Tcv=cTv$. From the preceding argument, $T(u+v)-Tu-Tv$ and $Tcv-cTv$ are generally isotropic, but I don't know whether they must be zero. Any idea? Thanks!","Question Suppose $V$ is a (finite-dimensional) vector space over $F$ ($\operatorname{char }F\neq2$, due to user1551) equipped with a non-degenerate quadratic form $Q$, and $T$ is a distance-preserving operator on $V$, viz. $Q(Tu-Tv)=Q(u-v)$ for each $u,v\in V$. Is it true that $T$ is linear affine (due to user1551)? Background I'm thinking about the mathematical derivation of Lorentz transformation from the principles of special relativity. In the context, $F=\mathbb R$, $V=F^4$ and $Q$ is the Lorentz quadratic form. The original problem might be with condition that $T$ acts on the space $\mathbb R^3$ as a translate (since they are inertial frames of reference), but $Q(Tu-Tv)=Q(u-v)$ only when $Q(u-v)=0$, which means that the operator preserves light cones. On condition that $F=\mathbb R$ and $Q$ is positive definite the answer is true. It follows from a standard derivation: Suppose $\langle x,y\rangle=(Q(x+y)-Q(x)-Q(y))/2$, then by definition $\langle\circ,\circ\rangle$ is a positive definite bilinear form. Note that $\langle Tu,Tv\rangle=\frac12(Q(Tu)+Q(Tv)-Q(Tu-Tv))=\frac12(Q(u)+Q(v)-Q(u-v))=\langle u,v\rangle$, we have $Q(Tcv-cTv)=\langle Tcv-cTv,Tcv-cTv\rangle=0$ and $Q(T(u+v)-Tu-Tv)=\langle T(u+v)-Tu-Tv,T(u+v)-Tu-Tv\rangle=0$ follows, which implies that $T(u+v)=Tu+Tv$ and $Tcv=cTv$. From the preceding argument, $T(u+v)-Tu-Tv$ and $Tcv-cTv$ are generally isotropic, but I don't know whether they must be zero. Any idea? Thanks!",,"['linear-algebra', 'physics', 'quadratic-forms', 'bilinear-form']"
11,It is not always possible to express two diagonal matrices as polynomials of some matrix,It is not always possible to express two diagonal matrices as polynomials of some matrix,,"I am looking for an illuminating proof of the following statement: There are not any polynomials $p,q$ and matrix $X$ over $GF(2)$ such that $p(X)=A:=\operatorname{diag}(1,0,0)$ and $q(X)=B:=\operatorname{diag}(0,1,0)$. (Note that the underlying field is $GF(2)$, not $\mathbb{R}$ or $\mathbb{C}$.) The statement originated from a remark I made to MSE/562509 and it is also related to MSE/326293 and MO/34314 . I have verified by computer that the statement is indeed true. Here is a proof that I find unsatisfactory: Suppose $p(X)=A$ and $q(X)=B$. As $A,B\neq 0,I$, by Cayley-Hamilton theorem, there are only six possible choices of $p$ or $q$, namely,   $$ p(x),q(x)\in\{x,\ x+1\}\cup\{x^2,\ x^2+1\}\cup\{x^2+x,\ x^2+x+1\}=S_1\cup S_2\cup S_3 \textrm{ (say)}. $$   Since $A$ differs from $B$ and $B+I$, we must have $p\in S_m$ and $q\in S_n$ for some $m\ne n$. Therefore, either $X$ or $X+I$ lies inside $\operatorname{span}\{p(X),q(X)\}=\operatorname{span}\{A,B\}$. So, $X$ must be a diagonal matrix. As some two diagonal entries $x_{ii}$ and $x_{jj}$ of $X$ are equal to each other (because $X$ has three diagonal entries but $GF(2)$ has size two), it follows that $a_{ii}=a_{jj}$ and $b_{ii}=b_{jj}$, which is a contradiction. While the above proof is not long, my feeling is that it is just a bunch of technical details but not a revealing proof. Does anyone have a better idea?","I am looking for an illuminating proof of the following statement: There are not any polynomials $p,q$ and matrix $X$ over $GF(2)$ such that $p(X)=A:=\operatorname{diag}(1,0,0)$ and $q(X)=B:=\operatorname{diag}(0,1,0)$. (Note that the underlying field is $GF(2)$, not $\mathbb{R}$ or $\mathbb{C}$.) The statement originated from a remark I made to MSE/562509 and it is also related to MSE/326293 and MO/34314 . I have verified by computer that the statement is indeed true. Here is a proof that I find unsatisfactory: Suppose $p(X)=A$ and $q(X)=B$. As $A,B\neq 0,I$, by Cayley-Hamilton theorem, there are only six possible choices of $p$ or $q$, namely,   $$ p(x),q(x)\in\{x,\ x+1\}\cup\{x^2,\ x^2+1\}\cup\{x^2+x,\ x^2+x+1\}=S_1\cup S_2\cup S_3 \textrm{ (say)}. $$   Since $A$ differs from $B$ and $B+I$, we must have $p\in S_m$ and $q\in S_n$ for some $m\ne n$. Therefore, either $X$ or $X+I$ lies inside $\operatorname{span}\{p(X),q(X)\}=\operatorname{span}\{A,B\}$. So, $X$ must be a diagonal matrix. As some two diagonal entries $x_{ii}$ and $x_{jj}$ of $X$ are equal to each other (because $X$ has three diagonal entries but $GF(2)$ has size two), it follows that $a_{ii}=a_{jj}$ and $b_{ii}=b_{jj}$, which is a contradiction. While the above proof is not long, my feeling is that it is just a bunch of technical details but not a revealing proof. Does anyone have a better idea?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
12,Are self-inverse operators normal?,Are self-inverse operators normal?,,"Let $\mathcal{H}$ be an Hilbert space. Consider a bounded Operator $T:\mathcal{H}\to \mathcal{H}$. Suppose $TT=1$, does it hold, that $T^{*}T=TT^{*}$? If so, how does one show this? If not, what kind of counterexamples are there?","Let $\mathcal{H}$ be an Hilbert space. Consider a bounded Operator $T:\mathcal{H}\to \mathcal{H}$. Suppose $TT=1$, does it hold, that $T^{*}T=TT^{*}$? If so, how does one show this? If not, what kind of counterexamples are there?",,"['linear-algebra', 'operator-algebras']"
13,How to translate between differential forms and tensor index notation,How to translate between differential forms and tensor index notation,,"The books on Manifold theory & geometry that I studied introduce connection and curvature in the language of differential forms. But Physics books on the other hand like (General Relativity by Wald) prefer to do things entirely using tensor index notation (for example introduce Ricci & Riemann tensors). I do not always find it easy to translate between the two notations especially when exterior derivative and the totally antisymmetric levi-civita tensor are involved. Is there a good reference where I can learn how to translate between two notations ? Also I feel that I am comfortable with using forms, but not so much with using tensor algebra & especially in the index notation. Is there a good place where geometry is done using tensors alone ? References for learning tensors independently of geometry will also be useful.","The books on Manifold theory & geometry that I studied introduce connection and curvature in the language of differential forms. But Physics books on the other hand like (General Relativity by Wald) prefer to do things entirely using tensor index notation (for example introduce Ricci & Riemann tensors). I do not always find it easy to translate between the two notations especially when exterior derivative and the totally antisymmetric levi-civita tensor are involved. Is there a good reference where I can learn how to translate between two notations ? Also I feel that I am comfortable with using forms, but not so much with using tensor algebra & especially in the index notation. Is there a good place where geometry is done using tensors alone ? References for learning tensors independently of geometry will also be useful.",,"['linear-algebra', 'abstract-algebra', 'reference-request', 'differential-geometry', 'tensors']"
14,Find the tangent space of $\mathrm{Aff}(n)$,Find the tangent space of,\mathrm{Aff}(n),"Find the tangent space of $\mathrm{Aff}(n)$. see Proof: Tangent space of the general linear group is the set of all squared matrices $\mathrm{Aff}(n)$ is the set of all matrices of the form   $$     \begin{pmatrix}         A & \ b \\         0 & 1 \\         \end{pmatrix}$$ where A $\in GL_n(\mathbb{R})$, b in $\mathbb{R}^n$, and $0$ is $1 \times n$ zero vector Let   $$    1 =  \begin{pmatrix}         I & \ 0 \\         0 & 1 \\         \end{pmatrix}$$ where I is identity matrix and 0 is a 0 column vector. So my path is Q(t)  $$     \begin{pmatrix}         A(t) & \ bt \\         0 & 1 \\         \end{pmatrix}$$ where $A(t)$ is a path in $ GL_n(\mathbb{R})$ s.t $A(0) = I$.","Find the tangent space of $\mathrm{Aff}(n)$. see Proof: Tangent space of the general linear group is the set of all squared matrices $\mathrm{Aff}(n)$ is the set of all matrices of the form   $$     \begin{pmatrix}         A & \ b \\         0 & 1 \\         \end{pmatrix}$$ where A $\in GL_n(\mathbb{R})$, b in $\mathbb{R}^n$, and $0$ is $1 \times n$ zero vector Let   $$    1 =  \begin{pmatrix}         I & \ 0 \\         0 & 1 \\         \end{pmatrix}$$ where I is identity matrix and 0 is a 0 column vector. So my path is Q(t)  $$     \begin{pmatrix}         A(t) & \ bt \\         0 & 1 \\         \end{pmatrix}$$ where $A(t)$ is a path in $ GL_n(\mathbb{R})$ s.t $A(0) = I$.",,"['linear-algebra', 'abstract-algebra', 'lie-groups', 'affine-geometry']"
15,Geometric meaning of block-diagonalization of a matrix,Geometric meaning of block-diagonalization of a matrix,,"some times we need to do block-diagonalization in favor of easy computation. For instance, for a matrix like this $$ \begin{bmatrix} A_{11} & A_{12} & A_{13} & 0 & 0 & 0\\ A_{21} & A_{22} & A_{23} & 0 & 0 & c\\ A_{31} & A_{32} & A_{33} & 0 & c & 0\\ 0 & 0 & 0 & B_{11} & B_{12} & B_{13}\\ 0 & 0 & c & B_{21} & B_{22} & B_{23}\\  0 & c & 0 & B_{31} & B_{32} & B_{33} \end{bmatrix} $$ we want to block-diagonalize it, i.e. eliminate 'c' by merging it into the block-diagonal part, we can do this by using some transformation matrix(unitary operator here) which only involves some phase terms $me^{i\theta}$, I would like to know what is the geometric meaning of this operation?(seems not like a usual rotation)","some times we need to do block-diagonalization in favor of easy computation. For instance, for a matrix like this $$ \begin{bmatrix} A_{11} & A_{12} & A_{13} & 0 & 0 & 0\\ A_{21} & A_{22} & A_{23} & 0 & 0 & c\\ A_{31} & A_{32} & A_{33} & 0 & c & 0\\ 0 & 0 & 0 & B_{11} & B_{12} & B_{13}\\ 0 & 0 & c & B_{21} & B_{22} & B_{23}\\  0 & c & 0 & B_{31} & B_{32} & B_{33} \end{bmatrix} $$ we want to block-diagonalize it, i.e. eliminate 'c' by merging it into the block-diagonal part, we can do this by using some transformation matrix(unitary operator here) which only involves some phase terms $me^{i\theta}$, I would like to know what is the geometric meaning of this operation?(seems not like a usual rotation)",,"['quantum-mechanics', 'quantum-field-theory', 'group-theory', 'geometry', 'linear-algebra']"
16,Simultaneous diagonalization of two positive semi-definite matrices,Simultaneous diagonalization of two positive semi-definite matrices,,"Let matrices $A, B$ be two $n \times n $ positive semi-definite matrices ; they can be represented in the following form   $$A=\sum_{i=1}^{n} \psi_{i}p_{i}p_{i}^{T}=P\Psi P^{T}, \quad B=\sum_{i=1}^{n} \phi_{i}q_{i}q_{i}^{T}=Q \Phi Q^{T} $$ where $\{p_{i}, \cdots, p_{n} \}$ and $\{q_{i}, \cdots, q_{n} \}$ are standard orthogonal sets in $\mathcal{R}^{n}$. There always exist a matrix $U$  diagonalizing simultaneously $A,B$, i.e., $$U^{T}AU=\Lambda_{1}, \quad U^{T}BU=\Lambda_{2}$$, where $\Lambda_{1}, \Lambda_{2}$ are diagonal matrices.  My question is how to find the matrices $\Lambda_{1}, \Lambda_{2}$ form the eigenvalues and eigenvectors of $A, B$, that is to say, find the relationship between $(U, \Lambda_{1}, \Lambda_{2})$ and $(P,\Psi, Q, \Phi)$.","Let matrices $A, B$ be two $n \times n $ positive semi-definite matrices ; they can be represented in the following form   $$A=\sum_{i=1}^{n} \psi_{i}p_{i}p_{i}^{T}=P\Psi P^{T}, \quad B=\sum_{i=1}^{n} \phi_{i}q_{i}q_{i}^{T}=Q \Phi Q^{T} $$ where $\{p_{i}, \cdots, p_{n} \}$ and $\{q_{i}, \cdots, q_{n} \}$ are standard orthogonal sets in $\mathcal{R}^{n}$. There always exist a matrix $U$  diagonalizing simultaneously $A,B$, i.e., $$U^{T}AU=\Lambda_{1}, \quad U^{T}BU=\Lambda_{2}$$, where $\Lambda_{1}, \Lambda_{2}$ are diagonal matrices.  My question is how to find the matrices $\Lambda_{1}, \Lambda_{2}$ form the eigenvalues and eigenvectors of $A, B$, that is to say, find the relationship between $(U, \Lambda_{1}, \Lambda_{2})$ and $(P,\Psi, Q, \Phi)$.",,"['linear-algebra', 'matrices']"
17,Determinant of the product equal to the product of determinants?,Determinant of the product equal to the product of determinants?,,Let $X$ be an $n\times p$ matrix and $A$ be a $n\times n$ matrix. When is it true that $$\det (X^{\top}AX) = \det(A)\det(X^{\top}X)?$$,Let $X$ be an $n\times p$ matrix and $A$ be a $n\times n$ matrix. When is it true that $$\det (X^{\top}AX) = \det(A)\det(X^{\top}X)?$$,,['linear-algebra']
18,reflection representation of isometry,reflection representation of isometry,,"I am reading the book Naive Lie Theory It proves that any isometry of $R^n$ that fixed the origin  O is the product of at most n reflections in hyperplanes through O. The proof is elementary and by induction. However, I cannot understand the arguments. 'Now suppose that $f$ is an isometry that fixes O and the result is true for $n=k-1$. If $f$ is not the identity, suppose $v \in R^k$ is such that $f(v)=w \neq v$. Then the reflection $r_u$ in the hyperplane orthogonal to $u=v-w$ maps the subspace $Ru$ of real multiples of $u$ onto itself and the map $r_uf$ is the identity on the subspace $Ru$.' Can anyone explain in detail to me why the map $r_uf$ is the identity on the subspace $Ru$? Why  do we have $$r_uf(u)=u$$ Can anyone explain why the last equality holds?","I am reading the book Naive Lie Theory It proves that any isometry of $R^n$ that fixed the origin  O is the product of at most n reflections in hyperplanes through O. The proof is elementary and by induction. However, I cannot understand the arguments. 'Now suppose that $f$ is an isometry that fixes O and the result is true for $n=k-1$. If $f$ is not the identity, suppose $v \in R^k$ is such that $f(v)=w \neq v$. Then the reflection $r_u$ in the hyperplane orthogonal to $u=v-w$ maps the subspace $Ru$ of real multiples of $u$ onto itself and the map $r_uf$ is the identity on the subspace $Ru$.' Can anyone explain in detail to me why the map $r_uf$ is the identity on the subspace $Ru$? Why  do we have $$r_uf(u)=u$$ Can anyone explain why the last equality holds?",,['linear-algebra']
19,Which is easier to work out: determinant or inverse?,Which is easier to work out: determinant or inverse?,,"Suppose $A\in M_n(R)$ be a $n\times n$ matrix over some ring $R$. Which of the following two tasks is easier? to work out $\det(A)$; to work out $A^{-1}$. More specifically, I want to know the answers according to the following different settings of $R$: $R$ is commutative; $R$ is non-commutative. $R$ is ring group $\mathbb{Z}_n[\mathbb{G}]$ for (1) commutative group $\mathbb{G}$, (2) non-commutative group $\mathbb{G}$.","Suppose $A\in M_n(R)$ be a $n\times n$ matrix over some ring $R$. Which of the following two tasks is easier? to work out $\det(A)$; to work out $A^{-1}$. More specifically, I want to know the answers according to the following different settings of $R$: $R$ is commutative; $R$ is non-commutative. $R$ is ring group $\mathbb{Z}_n[\mathbb{G}]$ for (1) commutative group $\mathbb{G}$, (2) non-commutative group $\mathbb{G}$.",,"['linear-algebra', 'ring-theory', 'computational-complexity', 'determinant', 'inverse']"
20,Matrix similar to its inverse,Matrix similar to its inverse,,"I have this problem: $A$ is an $n \times n$-matrix, its characteristic polynomial is $P(X)=(X-1)^n$. Prove that $A$ is similar to its inverse. How do you solve it? I really don't know.","I have this problem: $A$ is an $n \times n$-matrix, its characteristic polynomial is $P(X)=(X-1)^n$. Prove that $A$ is similar to its inverse. How do you solve it? I really don't know.",,"['linear-algebra', 'matrices', 'inverse']"
21,"Calculating the average of a possibly infinite ""compound"" length","Calculating the average of a possibly infinite ""compound"" length",,"Sorry for the ambiguous title, I couldn't find a good word to describe my problem. So here is my problem: You are a player, and you have a dice. You have N number of throws available then you can't throw any more. However, every time you get a 6 in a throw. You get M more throws available. Example: N = 5 M = 1 1st throw: 1 (remains 4 throws) 2nd throw: 2 (remains 3 throws) 3rd throw: 3 (remains 2 throws) 4th throw: 6 (remains 2 throws) 5th throw: 4 (remains 1 throws) 6th throw: 6 (remains 1 throws) 7th throw: 1 (remains 0 throws, STOP) As you can see, it's theoretically possible to get infinite number of throws. Also you can see why I said ""compound"", because every time you get extra throws, within those extra throws you can get again more extra throws and so on, which is why it can be infinite. Now how do I calculate the average number of throws you can get based on the three inputs: Initial throws available = N Awarded throws when outcome is X = M Probability of getting X in one throw = 1/6 (in this case of a dice) One note however: I can already solve this using Markov Chains but I want an answer using Algebra or something other than Markov Chains (I know it exists because someone showed me before but I cannot remember any more how he did it, should have taken notes) For the interested, here is how I do it using Markov Chains: By constructing a transition matrix, where the ""state"" is the number of throws remaining. Of course, you can have infinite number of throws so you should only make the matrix big enough to provide a good approximation. Now, you can get the chance of stopping within X number of throws by checking the matrix power X number of throws, row N (initial state) and column 0 (end state: stop). Since we can calculate the chance of stopping within X number of throws, we can get the chance of stopping exactly after X number of throws by calculating for X and X-1 then doing (chance of stopping within X - chance of stopping within (X-1)) = Chance of stopping exactly after X throws You can then do this for X = N .... Infinity Then you can sum the product of the two arrays of probabilities of stopping and X number of throws and you get the average number of throws that the player gets before stopping. Thanks, Space Monkey.","Sorry for the ambiguous title, I couldn't find a good word to describe my problem. So here is my problem: You are a player, and you have a dice. You have N number of throws available then you can't throw any more. However, every time you get a 6 in a throw. You get M more throws available. Example: N = 5 M = 1 1st throw: 1 (remains 4 throws) 2nd throw: 2 (remains 3 throws) 3rd throw: 3 (remains 2 throws) 4th throw: 6 (remains 2 throws) 5th throw: 4 (remains 1 throws) 6th throw: 6 (remains 1 throws) 7th throw: 1 (remains 0 throws, STOP) As you can see, it's theoretically possible to get infinite number of throws. Also you can see why I said ""compound"", because every time you get extra throws, within those extra throws you can get again more extra throws and so on, which is why it can be infinite. Now how do I calculate the average number of throws you can get based on the three inputs: Initial throws available = N Awarded throws when outcome is X = M Probability of getting X in one throw = 1/6 (in this case of a dice) One note however: I can already solve this using Markov Chains but I want an answer using Algebra or something other than Markov Chains (I know it exists because someone showed me before but I cannot remember any more how he did it, should have taken notes) For the interested, here is how I do it using Markov Chains: By constructing a transition matrix, where the ""state"" is the number of throws remaining. Of course, you can have infinite number of throws so you should only make the matrix big enough to provide a good approximation. Now, you can get the chance of stopping within X number of throws by checking the matrix power X number of throws, row N (initial state) and column 0 (end state: stop). Since we can calculate the chance of stopping within X number of throws, we can get the chance of stopping exactly after X number of throws by calculating for X and X-1 then doing (chance of stopping within X - chance of stopping within (X-1)) = Chance of stopping exactly after X throws You can then do this for X = N .... Infinity Then you can sum the product of the two arrays of probabilities of stopping and X number of throws and you get the average number of throws that the player gets before stopping. Thanks, Space Monkey.",,"['linear-algebra', 'probability', 'statistics']"
22,Proving when Gram's determinant is equal to zero [duplicate],Proving when Gram's determinant is equal to zero [duplicate],,"This question already has answers here : Proof: $\det\pmatrix{\langle v_i , v_j \rangle}\neq0$ $\iff \{v_1,\dots,v_n\}~\text{l.i.}$ (4 answers) Closed 11 years ago . Prove that Gram's determinant $G(x_1,\dots, x_n)=0$ if and only if   $x_1, \dots, x_k$ are linearly dependent. So I know that $G(x_1,\dots, x_n)=\det \begin{vmatrix} \xi( x_1,x_1) & \xi( x_1,x_2) &\dots & \xi( x_1,x_n)\\  \xi( x_2,x_1) & \xi( x_2,x_2) &\dots & \xi( x_2,x_n)\\ \vdots&\vdots&\ddots&\vdots\\  \xi( x_n,x_1) & \xi( x_n,x_2) &\dots & \xi( x_n,x_n)\end{vmatrix}$ (where $\xi$ denotes an inner product) which means that if $G(x_1,\dots, x_n)=0$, then the determinant has to be equal to 0 as well. Why does it happen only with $x_1,\dots, x_k$ being linearly dependent, though?","This question already has answers here : Proof: $\det\pmatrix{\langle v_i , v_j \rangle}\neq0$ $\iff \{v_1,\dots,v_n\}~\text{l.i.}$ (4 answers) Closed 11 years ago . Prove that Gram's determinant $G(x_1,\dots, x_n)=0$ if and only if   $x_1, \dots, x_k$ are linearly dependent. So I know that $G(x_1,\dots, x_n)=\det \begin{vmatrix} \xi( x_1,x_1) & \xi( x_1,x_2) &\dots & \xi( x_1,x_n)\\  \xi( x_2,x_1) & \xi( x_2,x_2) &\dots & \xi( x_2,x_n)\\ \vdots&\vdots&\ddots&\vdots\\  \xi( x_n,x_1) & \xi( x_n,x_2) &\dots & \xi( x_n,x_n)\end{vmatrix}$ (where $\xi$ denotes an inner product) which means that if $G(x_1,\dots, x_n)=0$, then the determinant has to be equal to 0 as well. Why does it happen only with $x_1,\dots, x_k$ being linearly dependent, though?",,"['linear-algebra', 'abstract-algebra', 'inner-products']"
23,Jordan block in a matrix with a complex eigenvalue,Jordan block in a matrix with a complex eigenvalue,,"Could you tell me, in general, how many Jordan blocks there are in the Jordan form of a matrix whose eigenvalue is complex, for example the matrix is $6 \times 6 $ and the eigenvalue is $3i+2$? What can we say about Jordan blocks of a matrix with two eigenvalues? I know that if we have $f: X \rightarrow X$ an endomorphism of a real vector space, then its complexification $f^{\mathbb{C}} : X^{\mathbb{C}} \rightarrow X^{\mathbb{C}}$ has the following Jordan decomposition: $ X^{\mathbb{C}} = V_1 \bigoplus ... \bigoplus V_p \bigoplus W_1 ... \bigoplus W_p \bigoplus \overline{W_1} \bigoplus ... \overline{W_p} $, where $V_i$ is are Jordan subspace with real eigenvalues and $W_i$ are subspaces with complex eigenvalues with positive imaginary part. I also know that the numbers of Jordan blocks corresponding to $\lambda$ and $\overline{\lambda}$ are equal. For example, if $M \in \mathcal{M}(6 \times 6, \mathbb{C})$ and $\lambda = a + bi$, then $\left[\begin{array}{ccc}a+bi&0&0&0&0&0\\1&a+bi&0&0&0&0\\0&1&a+bi&0&0&0\\0&0&1&a-bi&0&0\\0&0&0&1&a-bi&0\\0&0&0&0&1&a-bi\end{array}\right]$ (one block $3 \times 3$ with $\lambda$ and one block with $\overline{\lambda}$). We can also have $1$ block $1 \times 1$ with $\lambda$, $1$ block $2 \times 2$ with $\lambda$ and the same with $\overline{\lambda}$, and so on, until we exhaust all possible partitions of $3$. Is that reasoning corect? My problem is how can we express this in terms of a matrix with real entries, I mean, with blocks $\left[\begin{array}{ccc}1&0\\0&1\end{array}\right]$ and $\left[\begin{array}{ccc}a&b\\-b&a\end{array}\right]$? Could you help me?","Could you tell me, in general, how many Jordan blocks there are in the Jordan form of a matrix whose eigenvalue is complex, for example the matrix is $6 \times 6 $ and the eigenvalue is $3i+2$? What can we say about Jordan blocks of a matrix with two eigenvalues? I know that if we have $f: X \rightarrow X$ an endomorphism of a real vector space, then its complexification $f^{\mathbb{C}} : X^{\mathbb{C}} \rightarrow X^{\mathbb{C}}$ has the following Jordan decomposition: $ X^{\mathbb{C}} = V_1 \bigoplus ... \bigoplus V_p \bigoplus W_1 ... \bigoplus W_p \bigoplus \overline{W_1} \bigoplus ... \overline{W_p} $, where $V_i$ is are Jordan subspace with real eigenvalues and $W_i$ are subspaces with complex eigenvalues with positive imaginary part. I also know that the numbers of Jordan blocks corresponding to $\lambda$ and $\overline{\lambda}$ are equal. For example, if $M \in \mathcal{M}(6 \times 6, \mathbb{C})$ and $\lambda = a + bi$, then $\left[\begin{array}{ccc}a+bi&0&0&0&0&0\\1&a+bi&0&0&0&0\\0&1&a+bi&0&0&0\\0&0&1&a-bi&0&0\\0&0&0&1&a-bi&0\\0&0&0&0&1&a-bi\end{array}\right]$ (one block $3 \times 3$ with $\lambda$ and one block with $\overline{\lambda}$). We can also have $1$ block $1 \times 1$ with $\lambda$, $1$ block $2 \times 2$ with $\lambda$ and the same with $\overline{\lambda}$, and so on, until we exhaust all possible partitions of $3$. Is that reasoning corect? My problem is how can we express this in terms of a matrix with real entries, I mean, with blocks $\left[\begin{array}{ccc}1&0\\0&1\end{array}\right]$ and $\left[\begin{array}{ccc}a&b\\-b&a\end{array}\right]$? Could you help me?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
24,Geometric intuition behind the Uniform Boundedness Principle,Geometric intuition behind the Uniform Boundedness Principle,,Is there a way to visualize why the Uniform Boundedness Principle should be true? I understand the statement of the theorem but I'm having a hard time seeing a picture of it in my head.,Is there a way to visualize why the Uniform Boundedness Principle should be true? I understand the statement of the theorem but I'm having a hard time seeing a picture of it in my head.,,"['linear-algebra', 'analysis', 'functional-analysis', 'intuition']"
25,3D rotation group,3D rotation group,,"It is known that the group $\text{SO}(3)$ of rotation-matrices (matrices $A$ with $\det(A)=1$) are generated from three parameters. This can be expressed by the fact, that any rotation matrix is a composition of axis rotations $$ \begin{pmatrix} \cos(\phi)&-\sin(\phi)&0\\ \sin(\phi)&\cos(\phi)&0\\ 0&0&1\\ \end{pmatrix}, \begin{pmatrix} \cos(\phi)&0&\sin(\phi)\\ 0&1&0\\ -\sin(\phi)&0&\cos(\phi)\\ \end{pmatrix}, \begin{pmatrix} 1&0&0\\ 0&\cos(\phi)&-\sin(\phi)\\ 0&\sin(\phi)&\cos(\phi)\\ \end{pmatrix} $$ The question is: Why is the second matrix (Usually called rotation around the $y$-axis ) in almost any textbook written like this? Related to the other two matrices, I would say that the negative  $$ \begin{pmatrix} \cos(\phi)&0&-\sin(\phi)\\ 0&1&0\\ \sin(\phi)&0&\cos(\phi)\\ \end{pmatrix}, $$ is conceptual more straight forward. Any help or guidance will be appreciated.","It is known that the group $\text{SO}(3)$ of rotation-matrices (matrices $A$ with $\det(A)=1$) are generated from three parameters. This can be expressed by the fact, that any rotation matrix is a composition of axis rotations $$ \begin{pmatrix} \cos(\phi)&-\sin(\phi)&0\\ \sin(\phi)&\cos(\phi)&0\\ 0&0&1\\ \end{pmatrix}, \begin{pmatrix} \cos(\phi)&0&\sin(\phi)\\ 0&1&0\\ -\sin(\phi)&0&\cos(\phi)\\ \end{pmatrix}, \begin{pmatrix} 1&0&0\\ 0&\cos(\phi)&-\sin(\phi)\\ 0&\sin(\phi)&\cos(\phi)\\ \end{pmatrix} $$ The question is: Why is the second matrix (Usually called rotation around the $y$-axis ) in almost any textbook written like this? Related to the other two matrices, I would say that the negative  $$ \begin{pmatrix} \cos(\phi)&0&-\sin(\phi)\\ 0&1&0\\ \sin(\phi)&0&\cos(\phi)\\ \end{pmatrix}, $$ is conceptual more straight forward. Any help or guidance will be appreciated.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'rotations']"
26,Limit of matrix powers.,Limit of matrix powers.,,"Consider an arbitrary matrix $A$ with eigenvalues within the unit circle. Is there a nice formula for $A^\infty = \lim_{n \rightarrow \infty} A^n$? In particular, maybe there is a formula which involves the three matrices from the SVD of A? I am asking this from an algorithmic standpoint, i.e. is there a better way to compute $A^\infty$ than simply squaring the matrix $A$ many times?","Consider an arbitrary matrix $A$ with eigenvalues within the unit circle. Is there a nice formula for $A^\infty = \lim_{n \rightarrow \infty} A^n$? In particular, maybe there is a formula which involves the three matrices from the SVD of A? I am asking this from an algorithmic standpoint, i.e. is there a better way to compute $A^\infty$ than simply squaring the matrix $A$ many times?",,"['linear-algebra', 'matrices']"
27,"If $U$ is finite dimensional, then operator norm is finite","If  is finite dimensional, then operator norm is finite",U,"Let $M:U\to V$ be a linear map between normed vector space $U$ and $V$. We know $U$ is finite dimensional (but don't know about $V$). Define $\|M\| = \sup \{\|Mv\|\;:\;\|v\| = 1\}$. I want to show that $M$ is continuous and that $\|M\|$ is bounded. There are two difficulties, first is that I know the proof for $M:\mathbb{R}^m\to\mathbb{R}^n$. Continuity of $M$ is due to each entry of operator matrix of $M$ is linear thus continuous. I show unit sphere is compact and therefore continuous function obtains max. But now we are not in $\mathbb{R}$ any more, I don't know if unit sphere is compact or not. The second difficulty is that $V$ might not be finite dimensional any more, so I can't represent $M$ in a matrix of finite dimension any more. Please help. Further, not knowing if $U$ and $V$ are finite dimensional or not, I want to prove that $M$ continuous then $\|M\|$ finite.","Let $M:U\to V$ be a linear map between normed vector space $U$ and $V$. We know $U$ is finite dimensional (but don't know about $V$). Define $\|M\| = \sup \{\|Mv\|\;:\;\|v\| = 1\}$. I want to show that $M$ is continuous and that $\|M\|$ is bounded. There are two difficulties, first is that I know the proof for $M:\mathbb{R}^m\to\mathbb{R}^n$. Continuity of $M$ is due to each entry of operator matrix of $M$ is linear thus continuous. I show unit sphere is compact and therefore continuous function obtains max. But now we are not in $\mathbb{R}$ any more, I don't know if unit sphere is compact or not. The second difficulty is that $V$ might not be finite dimensional any more, so I can't represent $M$ in a matrix of finite dimension any more. Please help. Further, not knowing if $U$ and $V$ are finite dimensional or not, I want to prove that $M$ continuous then $\|M\|$ finite.",,"['linear-algebra', 'normed-spaces']"
28,Laplacian is the only 2nd order operator translation and rotation invariant,Laplacian is the only 2nd order operator translation and rotation invariant,,How to show that the Laplacian is the only 2nd order operator that is translation and rotation invariant such that $L0=0$? I have shown that it is rotation and translation invariant but I could not show the uniqueness.,How to show that the Laplacian is the only 2nd order operator that is translation and rotation invariant such that $L0=0$? I have shown that it is rotation and translation invariant but I could not show the uniqueness.,,"['linear-algebra', 'partial-differential-equations']"
29,Non-degenerate bilinear forms,Non-degenerate bilinear forms,,"Let $b$ be a non-degenerate bilinear form on a finite dimensional vector space $V$. Let $b'$ be any bilinear form on $V$. Show that $\exists$ $T \in L(V,V)$ such that $b'(v,w)=b(Tv,w) \,  \forall v,w \in V$.","Let $b$ be a non-degenerate bilinear form on a finite dimensional vector space $V$. Let $b'$ be any bilinear form on $V$. Show that $\exists$ $T \in L(V,V)$ such that $b'(v,w)=b(Tv,w) \,  \forall v,w \in V$.",,"['linear-algebra', 'bilinear-form']"
30,Question from Artin's algebra book,Question from Artin's algebra book,,"Let $A$ be an $n\times n$ matrix such that $A^r =I$ and $A$ has exactly one eigenvalue ,then $A= \lambda I$. My answe: As $A$ is a $n\times n$ matrix then characteristic polynomial has degree n and also exactly one root so $p(x) = (x-a)^n$ ($p(x)$ is the  char. polynomial.) Now the minimal polynomial $m_A(x)|p(x) $ also $m_A(x) | (x^r-1) =(x-\zeta_1)...(x-\zeta_r)$ (where $\zeta_i $ are the rth roots of unity) hence $m_A(x)$ is $(x-\zeta_i)$ for some $i$. As a result $A$ is diagonalizable  and $A$ is of the form $A = \lambda I$ Is this correct?","Let $A$ be an $n\times n$ matrix such that $A^r =I$ and $A$ has exactly one eigenvalue ,then $A= \lambda I$. My answe: As $A$ is a $n\times n$ matrix then characteristic polynomial has degree n and also exactly one root so $p(x) = (x-a)^n$ ($p(x)$ is the  char. polynomial.) Now the minimal polynomial $m_A(x)|p(x) $ also $m_A(x) | (x^r-1) =(x-\zeta_1)...(x-\zeta_r)$ (where $\zeta_i $ are the rth roots of unity) hence $m_A(x)$ is $(x-\zeta_i)$ for some $i$. As a result $A$ is diagonalizable  and $A$ is of the form $A = \lambda I$ Is this correct?",,['linear-algebra']
31,For which values of $\alpha \in \mathbb R$ is the following system of linear equations solvable?,For which values of  is the following system of linear equations solvable?,\alpha \in \mathbb R,"The problem I was given: Calculate the value of the following determinant: $\left| \begin{array}{ccc} \alpha & 1 & \alpha^2 & -\alpha\\ 1 & \alpha & 1 & 1\\ 1 & \alpha^2 & 2\alpha & 2\alpha\\ 1 & 1 & \alpha^2 & -\alpha \end{array} \right|$ For which values of $\alpha \in \mathbb R$ is the following system of linear equations solvable? $\begin{array}{lcl}  \alpha x_1 & + & x_2 & + & \alpha^2 x_3 & = & -\alpha\\  x_1 & + & \alpha x_2 & + & x_3 & = & 1\\ x_1 & + & \alpha^2 x_2 & + & 2\alpha x_3 & = & 2\alpha\\ x_1 & + & x_2 & + & \alpha^2 x_3 & = & -\alpha\\ \end{array}$ I got as far as finding the determinant, and then I got stuck. So I solve the determinant like this: $\left| \begin{array}{ccc} \alpha & 1 & \alpha^2 & -\alpha\\ 1 & \alpha & 1 & 1\\ 1 & \alpha^2 & 2\alpha & 2\alpha\\ 1 & 1 & \alpha^2 & -\alpha \end{array} \right|$ =  $\left| \begin{array}{ccc} \alpha - 1 & 0 & 0 & 0\\ 1 & \alpha & 1 & 1\\ 1 & \alpha^2 & 2\alpha & 2\alpha\\ 1 & 1 & \alpha^2 & -\alpha \end{array} \right|$ =  $(\alpha - 1)\left| \begin{array}{ccc} \alpha & 1 & 1\\ \alpha^2 & 2\alpha & 2\alpha \\ 1 & \alpha^2 & -\alpha \end{array} \right|$ = $(\alpha - 1)\left| \begin{array}{ccc} \alpha & 1 & 0\\ \alpha^2 & 2\alpha & 0 \\ 1 & \alpha^2 & -\alpha - \alpha^2 \end{array} \right|$  = $-\alpha^3(\alpha - 1) (1 + \alpha)$ However, now I haven't got a clue on solving the system of linear equations... It's got to do with the fact that the equations look like the determinant I calculated before, but I don't know how to connect those two. Thanks in advance for any help. (:","The problem I was given: Calculate the value of the following determinant: $\left| \begin{array}{ccc} \alpha & 1 & \alpha^2 & -\alpha\\ 1 & \alpha & 1 & 1\\ 1 & \alpha^2 & 2\alpha & 2\alpha\\ 1 & 1 & \alpha^2 & -\alpha \end{array} \right|$ For which values of $\alpha \in \mathbb R$ is the following system of linear equations solvable? $\begin{array}{lcl}  \alpha x_1 & + & x_2 & + & \alpha^2 x_3 & = & -\alpha\\  x_1 & + & \alpha x_2 & + & x_3 & = & 1\\ x_1 & + & \alpha^2 x_2 & + & 2\alpha x_3 & = & 2\alpha\\ x_1 & + & x_2 & + & \alpha^2 x_3 & = & -\alpha\\ \end{array}$ I got as far as finding the determinant, and then I got stuck. So I solve the determinant like this: $\left| \begin{array}{ccc} \alpha & 1 & \alpha^2 & -\alpha\\ 1 & \alpha & 1 & 1\\ 1 & \alpha^2 & 2\alpha & 2\alpha\\ 1 & 1 & \alpha^2 & -\alpha \end{array} \right|$ =  $\left| \begin{array}{ccc} \alpha - 1 & 0 & 0 & 0\\ 1 & \alpha & 1 & 1\\ 1 & \alpha^2 & 2\alpha & 2\alpha\\ 1 & 1 & \alpha^2 & -\alpha \end{array} \right|$ =  $(\alpha - 1)\left| \begin{array}{ccc} \alpha & 1 & 1\\ \alpha^2 & 2\alpha & 2\alpha \\ 1 & \alpha^2 & -\alpha \end{array} \right|$ = $(\alpha - 1)\left| \begin{array}{ccc} \alpha & 1 & 0\\ \alpha^2 & 2\alpha & 0 \\ 1 & \alpha^2 & -\alpha - \alpha^2 \end{array} \right|$  = $-\alpha^3(\alpha - 1) (1 + \alpha)$ However, now I haven't got a clue on solving the system of linear equations... It's got to do with the fact that the equations look like the determinant I calculated before, but I don't know how to connect those two. Thanks in advance for any help. (:",,"['linear-algebra', 'matrices']"
32,Linear Algebra: Projection Maps,Linear Algebra: Projection Maps,,"I would like to check if my understanding of projection maps is correct. I have been given the following subset of $\mathbb{R}^3$: $$A=\left\{\begin{pmatrix} x \\ y \\ -x+2y \end{pmatrix} \middle| x,y,z\in\mathbb{R}\right\}$$ A basis for this subset is $\mathscr{B}=\left\{ \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix},\begin{pmatrix} 0 \\ 1 \\ 2 \end{pmatrix} \right\}$, and to extend this basis to one for the vector space $\mathbb{R^3}$ we simply add to the basis the vector: $$\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$$ To obtain $\mathscr{C} = \left\{ \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix},\begin{pmatrix} 0 \\ 1 \\ 2 \end{pmatrix},\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \right\}$, a basis for $\mathbb{R}^3$. We can call $B = Span\left\{\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\right\}$, and then we can say $\mathbb{R}^3=A\bigoplus B$. What I want to know is if I am correct in interpreting the definition of projection map. Let $P:\mathbb{R}\to\mathbb{R}$ be the projection map onto A. The question asks me to calculate $P(e_1)$, $P(e_2)$ and $P(e_3)$ then write down the matrix of $P$ with respect to the standard basis of $\mathbb{R}^3$. Without explicitly giving my answer (I want to check my method, not my answers), this is my method: Write each vector $e_1$, $e_2$ and $e_3$ as a linear combination of the vectors in $\mathscr{C}$, so, for example, $e_1 = \alpha\begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix}+\beta\begin{pmatrix} 0 \\ 1 \\ 2 \end{pmatrix} \gamma\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$. For the projection map onto $A$ we take only the first two terms as the first two terms are in the basis $\mathscr{B}$. So, for the combination in step 1, $P(e_1)=\begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix} = \alpha e_1+\beta e_2+\gamma e_3$ To form the matrix P we write down the columns of the matrix the coefficients describe in the last step, so we get: $P=\begin{pmatrix} \alpha & . & . \\ \beta & . & . \\ \gamma & . & . \end{pmatrix}$, and fill in the missing columns as we did for the first column above. Am I correct in my method? If I have any of this wrong, please guide me in the right direction.","I would like to check if my understanding of projection maps is correct. I have been given the following subset of $\mathbb{R}^3$: $$A=\left\{\begin{pmatrix} x \\ y \\ -x+2y \end{pmatrix} \middle| x,y,z\in\mathbb{R}\right\}$$ A basis for this subset is $\mathscr{B}=\left\{ \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix},\begin{pmatrix} 0 \\ 1 \\ 2 \end{pmatrix} \right\}$, and to extend this basis to one for the vector space $\mathbb{R^3}$ we simply add to the basis the vector: $$\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$$ To obtain $\mathscr{C} = \left\{ \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix},\begin{pmatrix} 0 \\ 1 \\ 2 \end{pmatrix},\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \right\}$, a basis for $\mathbb{R}^3$. We can call $B = Span\left\{\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\right\}$, and then we can say $\mathbb{R}^3=A\bigoplus B$. What I want to know is if I am correct in interpreting the definition of projection map. Let $P:\mathbb{R}\to\mathbb{R}$ be the projection map onto A. The question asks me to calculate $P(e_1)$, $P(e_2)$ and $P(e_3)$ then write down the matrix of $P$ with respect to the standard basis of $\mathbb{R}^3$. Without explicitly giving my answer (I want to check my method, not my answers), this is my method: Write each vector $e_1$, $e_2$ and $e_3$ as a linear combination of the vectors in $\mathscr{C}$, so, for example, $e_1 = \alpha\begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix}+\beta\begin{pmatrix} 0 \\ 1 \\ 2 \end{pmatrix} \gamma\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$. For the projection map onto $A$ we take only the first two terms as the first two terms are in the basis $\mathscr{B}$. So, for the combination in step 1, $P(e_1)=\begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix} = \alpha e_1+\beta e_2+\gamma e_3$ To form the matrix P we write down the columns of the matrix the coefficients describe in the last step, so we get: $P=\begin{pmatrix} \alpha & . & . \\ \beta & . & . \\ \gamma & . & . \end{pmatrix}$, and fill in the missing columns as we did for the first column above. Am I correct in my method? If I have any of this wrong, please guide me in the right direction.",,['linear-algebra']
33,Application of rank of a matrix,Application of rank of a matrix,,Are there any real life applications of the rank of a matrix? It need to have a real impact which motivates students why they should learn about rank.,Are there any real life applications of the rank of a matrix? It need to have a real impact which motivates students why they should learn about rank.,,['linear-algebra']
34,"A unitary matrix taking a real matrix to another real matrix, is it an orthogonal matrix?","A unitary matrix taking a real matrix to another real matrix, is it an orthogonal matrix?",,"I tried to prove that a real antisymmetric matrix can be taken by an orthogonal tranformation to a form: where the eigenvalues are $\pm i\lambda_1, \pm i\lambda_2 ...  $ which is a statement I saw on wikipedia in http://en.wikipedia.org/wiki/Antisymmetric_matrix I also know an antisymmetric matrix can be diagonalized by a unitary transformation, and I found a unitary transformation taking the diagonal matrix to the required form. So by composing the two transformations (diagonalization, then taking the diagonal matrix to the required form), I'll get a unitary transformation taking the real antisymmetric matrix to another real matrix. My question is if this transformation must be a real matrix? if so I can deduce that the unitary transformation is in fact an orthogonal transformation. So is this true? Is a unitary transformation taking a real matrix to another real matrix necessarily an orthogonal transformation? EDIT: After receiving in the comment here a counterexample, I'm adding: Alternatively, if it is not necessarily orthogonal, does there necessarily exist an orthogonal transformation taking the two matrices to each other?","I tried to prove that a real antisymmetric matrix can be taken by an orthogonal tranformation to a form: where the eigenvalues are $\pm i\lambda_1, \pm i\lambda_2 ...  $ which is a statement I saw on wikipedia in http://en.wikipedia.org/wiki/Antisymmetric_matrix I also know an antisymmetric matrix can be diagonalized by a unitary transformation, and I found a unitary transformation taking the diagonal matrix to the required form. So by composing the two transformations (diagonalization, then taking the diagonal matrix to the required form), I'll get a unitary transformation taking the real antisymmetric matrix to another real matrix. My question is if this transformation must be a real matrix? if so I can deduce that the unitary transformation is in fact an orthogonal transformation. So is this true? Is a unitary transformation taking a real matrix to another real matrix necessarily an orthogonal transformation? EDIT: After receiving in the comment here a counterexample, I'm adding: Alternatively, if it is not necessarily orthogonal, does there necessarily exist an orthogonal transformation taking the two matrices to each other?",,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'unitary-matrices']"
35,Smallest possible rank of an $n×n$ matrix that has zeros along the main diagonal and strictly positive real numbers off the main diagonal,Smallest possible rank of an  matrix that has zeros along the main diagonal and strictly positive real numbers off the main diagonal,n×n,A problem in IMC 2012 in which i'm interested but I have no answer. Can you help me? Many thanks. Problem : Let $n$ be a fixed positive integer. Determine the smallest possible rank of an $n\times n$ matrix that has zeros along the main diagonal and strictly positive real numbers off the main diagonal.,A problem in IMC 2012 in which i'm interested but I have no answer. Can you help me? Many thanks. Problem : Let $n$ be a fixed positive integer. Determine the smallest possible rank of an $n\times n$ matrix that has zeros along the main diagonal and strictly positive real numbers off the main diagonal.,,"['linear-algebra', 'matrices']"
36,Convention on non-negative singular values?,Convention on non-negative singular values?,,"In the literature I have on disposal it is stated that singular values are non-negative values, and that, for a symmetric matrix $A$, the SVD and EVD coincide. This would mean that singular values of $A$ are the eigenvalues of $A$, but the eigenvalues of $A$ can be negative, regardless of $A$ being symmetric. So, I wonder if the choice of singular values being exclusively positive is some kind of convention? If so, how degenerate that is given the above observation the equivalence of SVD and EVD for symmetric matrices?","In the literature I have on disposal it is stated that singular values are non-negative values, and that, for a symmetric matrix $A$, the SVD and EVD coincide. This would mean that singular values of $A$ are the eigenvalues of $A$, but the eigenvalues of $A$ can be negative, regardless of $A$ being symmetric. So, I wonder if the choice of singular values being exclusively positive is some kind of convention? If so, how degenerate that is given the above observation the equivalence of SVD and EVD for symmetric matrices?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
37,Is this action of $\mathbb F[[x]]$ on $\bigoplus_{i=0}^{\infty}\mathbb F$ natural?,Is this action of  on  natural?,\mathbb F[[x]] \bigoplus_{i=0}^{\infty}\mathbb F,"The title of my question has a field $\mathbb F$ in it, but to make sure I'm not losing anything, I would like to introduce my question in full generality. But still, I will be happy with an answer in fields, which is why I chose this title. Please feel free to consider $R$ a field and all modules to be vector spaces. Notation I'm assuming $\mathbb N=\{0,1,\ldots\}$ in this question. Let $R$ be any unital ring. $R[x]$ denotes the ring of polynomials over $R$. $R[[x]]$ denotes the ring of formal power series over $R$. (The linked article requires that $R$ be commutative. I don't.) $M_n(R)$ will denote the ring of all square matrices over $R$ indexed by the set $\{0,1,\ldots,n-1\}\times\{0,1,\ldots,n-1\}.$ $M_{\infty}(R)$ denotes the set of all $\mathbb N\times\mathbb N$-matrices over $R$ whose columns have finitely many non-zero coefficients. This restriction allows the usual multiplication of such matrices, and so they form a ring. I will also be using the ring of endomorphisms $E(R)$ of the free $R$-module $V(R)=\bigoplus_{i=0}^{\infty}R,$ with a fixed basis $e_0,e_1,\ldots$ Background The rings $M_\infty (R)$ and $E(R)$ are isomorphic. Just like for finite matrices over fields, the $i$-th column of a matrix $A$ represents the element $y$ of $V(R)$ to which $e_i$ is mapped by a corresponding endomorphisms $\alpha.$ To be precise, the column contains the coefficients of $y$ in the basis $e_0,e_1,\ldots$ This explains the requirement that there be only finitely many non-zero elements in the columns, because every $y\in V(R)$ can be written uniquely as a linear combination of the basis, and linear combinations are finite sums. Let now $\phi:R[[x]]\longrightarrow M_\infty(R)$ such that $$\sum_{i=0}^\infty a_ix^i\mapsto  \begin{pmatrix} a_0    & a_1    & a_2    & a_3    & \ldots\\ 0      & a_0    & a_1    & a_2    & \ldots\\ 0      & 0      & a_0    & a_1    & \ldots\\ 0      & 0      & 0      & a_0    & \ldots\\ \vdots & \vdots & \vdots & \vdots & \ddots \end{pmatrix} $$ $\phi$ is a ring isomorpisms onto its image $M_\triangledown(R).$ I found it very interesting that formal power series (and therefore also polynomials) can be embedded in matrices. It turns out to be useful. I'm reading a 1974 paper by Jan Krempa which uses it. The title is On the Jacobson Radical of Polynomial rings . Very similarly, we can define the map $\psi:R[x]/\langle x^n\rangle\longrightarrow M_n(R)$ such that $$ \left(\sum_{i=0}^{n-1}a_ix^i\right)+\langle x^n\rangle\mapsto \begin{pmatrix} a_0    & a_1    & a_2    & \ldots & a_{n-1}\\ 0      & a_0    & a_1    & \ldots & a_{n-2}\\ 0      & 0      & a_0    & \ldots & a_{n-3}\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ 0      & 0      & 0      & \ldots & a_0 \end{pmatrix}$$ Question Let $f=\sum_{i=0}^\infty a_ix^i\in R[[x]].$ Now $\phi(f)$ is a matrix over $R$ and as such can be identified with an endomorphism $\alpha_f$ of $V(R).$ As an endomorphism, $\alpha_f$ can take arguments from $V(R)$ and send them to some other elements of $V(R).$ However, the original power series $f$ is not usually thought to have such a capability. We can define for any $f\in R[[x]]$ and $y\in V(R)$ $$fy:=\alpha_f(y).$$ I would like to know if thinking of formal power series acting in this way on $V(R)$ is natural. I was trying to see some natural way in which a power series can be thought of as an endomorphism of a free module or vector space but I failed. It's easy to write what $fy$ actually is. Let $y\neq 0$ for simplicity. Let $\{y_i\}_{i=1}^\infty$ be the coefficients of $y$ in the basis $e_0,e_1,\ldots$ Let $n$ be the greatest index such that $y_n=0$ but $y_{n-1}\neq 0$. Now calculating $fy$ is essentially the same as calculating $$ \begin{pmatrix} a_0    & a_1    & a_2    & \ldots & a_{n-1} & \ldots\\ 0      & a_0    & a_1    & \ldots & a_{n-2} & \ldots\\ 0      & 0      & a_0    & \ldots & a_{n-3} & \ldots\\ \vdots & \vdots & \vdots & \ddots & \vdots  & \ldots\\ 0      & 0      & 0      & \ldots & a_0     & \ldots\\ \vdots & \vdots & \vdots & \vdots & \vdots  & \ddots \end{pmatrix} \begin{pmatrix} y_0 \\ y_1 \\ y_2 \\ \vdots \\ y_{n-1} \\ \vdots \end{pmatrix} = \begin{pmatrix} a_0y_0+a_1y_1+a_2y_2 +\cdots +a_{n-1}y_{n-1}\\ a_0y_1+a_1y_2+\cdots +a_{n-2}y_{n-1}\\ a_0y_2+\cdots +a_{n-3}y_{n-1}\\ \vdots\\ a_0y_{n-1}\\ \vdots \end{pmatrix} $$ I've looked at these formulas for quite some time now and they still tell me nothing. Could please tell me if you recognize them? An analogous question can be asked about the finite-dimensional case of $R[x]/\langle x^n\rangle$ and the map $\psi$. I hereby ask it then, as this post is already so long and latex-laden that I have to wait one or two seconds for a reaction to my typing on the screen.","The title of my question has a field $\mathbb F$ in it, but to make sure I'm not losing anything, I would like to introduce my question in full generality. But still, I will be happy with an answer in fields, which is why I chose this title. Please feel free to consider $R$ a field and all modules to be vector spaces. Notation I'm assuming $\mathbb N=\{0,1,\ldots\}$ in this question. Let $R$ be any unital ring. $R[x]$ denotes the ring of polynomials over $R$. $R[[x]]$ denotes the ring of formal power series over $R$. (The linked article requires that $R$ be commutative. I don't.) $M_n(R)$ will denote the ring of all square matrices over $R$ indexed by the set $\{0,1,\ldots,n-1\}\times\{0,1,\ldots,n-1\}.$ $M_{\infty}(R)$ denotes the set of all $\mathbb N\times\mathbb N$-matrices over $R$ whose columns have finitely many non-zero coefficients. This restriction allows the usual multiplication of such matrices, and so they form a ring. I will also be using the ring of endomorphisms $E(R)$ of the free $R$-module $V(R)=\bigoplus_{i=0}^{\infty}R,$ with a fixed basis $e_0,e_1,\ldots$ Background The rings $M_\infty (R)$ and $E(R)$ are isomorphic. Just like for finite matrices over fields, the $i$-th column of a matrix $A$ represents the element $y$ of $V(R)$ to which $e_i$ is mapped by a corresponding endomorphisms $\alpha.$ To be precise, the column contains the coefficients of $y$ in the basis $e_0,e_1,\ldots$ This explains the requirement that there be only finitely many non-zero elements in the columns, because every $y\in V(R)$ can be written uniquely as a linear combination of the basis, and linear combinations are finite sums. Let now $\phi:R[[x]]\longrightarrow M_\infty(R)$ such that $$\sum_{i=0}^\infty a_ix^i\mapsto  \begin{pmatrix} a_0    & a_1    & a_2    & a_3    & \ldots\\ 0      & a_0    & a_1    & a_2    & \ldots\\ 0      & 0      & a_0    & a_1    & \ldots\\ 0      & 0      & 0      & a_0    & \ldots\\ \vdots & \vdots & \vdots & \vdots & \ddots \end{pmatrix} $$ $\phi$ is a ring isomorpisms onto its image $M_\triangledown(R).$ I found it very interesting that formal power series (and therefore also polynomials) can be embedded in matrices. It turns out to be useful. I'm reading a 1974 paper by Jan Krempa which uses it. The title is On the Jacobson Radical of Polynomial rings . Very similarly, we can define the map $\psi:R[x]/\langle x^n\rangle\longrightarrow M_n(R)$ such that $$ \left(\sum_{i=0}^{n-1}a_ix^i\right)+\langle x^n\rangle\mapsto \begin{pmatrix} a_0    & a_1    & a_2    & \ldots & a_{n-1}\\ 0      & a_0    & a_1    & \ldots & a_{n-2}\\ 0      & 0      & a_0    & \ldots & a_{n-3}\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ 0      & 0      & 0      & \ldots & a_0 \end{pmatrix}$$ Question Let $f=\sum_{i=0}^\infty a_ix^i\in R[[x]].$ Now $\phi(f)$ is a matrix over $R$ and as such can be identified with an endomorphism $\alpha_f$ of $V(R).$ As an endomorphism, $\alpha_f$ can take arguments from $V(R)$ and send them to some other elements of $V(R).$ However, the original power series $f$ is not usually thought to have such a capability. We can define for any $f\in R[[x]]$ and $y\in V(R)$ $$fy:=\alpha_f(y).$$ I would like to know if thinking of formal power series acting in this way on $V(R)$ is natural. I was trying to see some natural way in which a power series can be thought of as an endomorphism of a free module or vector space but I failed. It's easy to write what $fy$ actually is. Let $y\neq 0$ for simplicity. Let $\{y_i\}_{i=1}^\infty$ be the coefficients of $y$ in the basis $e_0,e_1,\ldots$ Let $n$ be the greatest index such that $y_n=0$ but $y_{n-1}\neq 0$. Now calculating $fy$ is essentially the same as calculating $$ \begin{pmatrix} a_0    & a_1    & a_2    & \ldots & a_{n-1} & \ldots\\ 0      & a_0    & a_1    & \ldots & a_{n-2} & \ldots\\ 0      & 0      & a_0    & \ldots & a_{n-3} & \ldots\\ \vdots & \vdots & \vdots & \ddots & \vdots  & \ldots\\ 0      & 0      & 0      & \ldots & a_0     & \ldots\\ \vdots & \vdots & \vdots & \vdots & \vdots  & \ddots \end{pmatrix} \begin{pmatrix} y_0 \\ y_1 \\ y_2 \\ \vdots \\ y_{n-1} \\ \vdots \end{pmatrix} = \begin{pmatrix} a_0y_0+a_1y_1+a_2y_2 +\cdots +a_{n-1}y_{n-1}\\ a_0y_1+a_1y_2+\cdots +a_{n-2}y_{n-1}\\ a_0y_2+\cdots +a_{n-3}y_{n-1}\\ \vdots\\ a_0y_{n-1}\\ \vdots \end{pmatrix} $$ I've looked at these formulas for quite some time now and they still tell me nothing. Could please tell me if you recognize them? An analogous question can be asked about the finite-dimensional case of $R[x]/\langle x^n\rangle$ and the map $\psi$. I hereby ask it then, as this post is already so long and latex-laden that I have to wait one or two seconds for a reaction to my typing on the screen.",,"['linear-algebra', 'matrices']"
38,Dimension of commutator space of matrices (again),Dimension of commutator space of matrices (again),,"For a square matrix $A\in F^{n\times n}$ over a field $F$, define the commutator subspace $C_A = \{ B\in F^{n\times n} \vert AB = BA\}$ of matrices which commute with $A$. This other question by RiaD asks for a proof that $\dim C_A\geq n$ for any $A$.  The answer by Johannes Kloos uses Jordan Normal Form, and so yields the same result over any algebraically closed field $F$ in place of $\mathbb{C}$. Now let's write $C_{F,A}$ in place of $C_A$ to keep track of the field.  Note that $C_{F,A}$ is the kernel of the linear map $B\mapsto AB-BA$.  The dimension of a kernel doesn't change in a field extension because row reduction proceeds in the same way regardless of field, so if $K$ is a field extension of $F$ then $\dim_F C_{F,A} = \dim_K C_{K,A}$.  Since any field $F$ is contained in an algebraically closed field $K$ we can combine these facts to get $\dim_F C_{F,A} = \dim_K C_{K,A}\geq n$, so the original result holds for all fields $F$, not just algebraically closed fields. The question: is there a purely linear algebraic proof of this?  By this I mean one which works for all fields and does not involve passing to a field extension.","For a square matrix $A\in F^{n\times n}$ over a field $F$, define the commutator subspace $C_A = \{ B\in F^{n\times n} \vert AB = BA\}$ of matrices which commute with $A$. This other question by RiaD asks for a proof that $\dim C_A\geq n$ for any $A$.  The answer by Johannes Kloos uses Jordan Normal Form, and so yields the same result over any algebraically closed field $F$ in place of $\mathbb{C}$. Now let's write $C_{F,A}$ in place of $C_A$ to keep track of the field.  Note that $C_{F,A}$ is the kernel of the linear map $B\mapsto AB-BA$.  The dimension of a kernel doesn't change in a field extension because row reduction proceeds in the same way regardless of field, so if $K$ is a field extension of $F$ then $\dim_F C_{F,A} = \dim_K C_{K,A}$.  Since any field $F$ is contained in an algebraically closed field $K$ we can combine these facts to get $\dim_F C_{F,A} = \dim_K C_{K,A}\geq n$, so the original result holds for all fields $F$, not just algebraically closed fields. The question: is there a purely linear algebraic proof of this?  By this I mean one which works for all fields and does not involve passing to a field extension.",,['linear-algebra']
39,Sparseness of a Vector,Sparseness of a Vector,,The sparseness of a vector is defined a follows: $$\psi(\textbf{x}) = \frac{\sqrt{n}-\frac{\left(\sum_{i} x_i \right)}{\sqrt{\sum_{i} x_{i}^{2}}}}{\sqrt{n}-1}$$ So $\psi(\textbf{x}) =0 $ if $\sqrt{n} = \frac{\left(\sum_{i}x_i \right)}{\sqrt{\sum_{i} x_{i}^{2}}}$ or if $$\sqrt{n} \sqrt{\sum_{i} x_{i}^{2}} = \sum_{i} x_i$$ How does imply that all the $x_i$ are equal? Cauchy-Schwarz? Likewise $\psi(\textbf{x}) = 1$ iff $\textbf{x}$ contains a single non-zero element. How does this follow? We know that $$\sqrt{n}-\frac{\left(\sum_{i} x_i \right)}{\sqrt{\sum_{i} x_{i}^{2}}} = \sqrt{n}-1$$,The sparseness of a vector is defined a follows: $$\psi(\textbf{x}) = \frac{\sqrt{n}-\frac{\left(\sum_{i} x_i \right)}{\sqrt{\sum_{i} x_{i}^{2}}}}{\sqrt{n}-1}$$ So $\psi(\textbf{x}) =0 $ if $\sqrt{n} = \frac{\left(\sum_{i}x_i \right)}{\sqrt{\sum_{i} x_{i}^{2}}}$ or if $$\sqrt{n} \sqrt{\sum_{i} x_{i}^{2}} = \sum_{i} x_i$$ How does imply that all the $x_i$ are equal? Cauchy-Schwarz? Likewise $\psi(\textbf{x}) = 1$ iff $\textbf{x}$ contains a single non-zero element. How does this follow? We know that $$\sqrt{n}-\frac{\left(\sum_{i} x_i \right)}{\sqrt{\sum_{i} x_{i}^{2}}} = \sqrt{n}-1$$,,['linear-algebra']
40,"If a $3 \times 3$ matrix $A$ is similar to $A^2$, find all possible Jordan Canonical Forms of $A$","If a  matrix  is similar to , find all possible Jordan Canonical Forms of",3 \times 3 A A^2 A,"I want to find all possible Jordan Canonical Forms (JCF) of $3\times 3$ matrix $A$, if $A$ is similar to $A^2$. I have only $4$, and I think there is something wrong with my solution. Let $J$ be JCF of $A$, then $J^2$ is JCF of $A^2$, since $A$ and $A^2$ are similar, $J=J^2$. Then the minimal polynomial of $J$ divides $x(x-1)$, so $m(x)$ is one of $x, x-1, x(x-1)$. So:  $$J=[0]+[0]+[0], [1]+[1]+[1], [0]+[0]+[1] or [0]+[1]+[1]$$ But, $J$ could also be $[0]+J2[1]$, which is not in the list above. Am I leaving out some cases? Thanks,","I want to find all possible Jordan Canonical Forms (JCF) of $3\times 3$ matrix $A$, if $A$ is similar to $A^2$. I have only $4$, and I think there is something wrong with my solution. Let $J$ be JCF of $A$, then $J^2$ is JCF of $A^2$, since $A$ and $A^2$ are similar, $J=J^2$. Then the minimal polynomial of $J$ divides $x(x-1)$, so $m(x)$ is one of $x, x-1, x(x-1)$. So:  $$J=[0]+[0]+[0], [1]+[1]+[1], [0]+[0]+[1] or [0]+[1]+[1]$$ But, $J$ could also be $[0]+J2[1]$, which is not in the list above. Am I leaving out some cases? Thanks,",,['linear-algebra']
41,Infinite sum of geometric series of a matrix,Infinite sum of geometric series of a matrix,,"Can anybody show how to prove or show where to find a proof of the following statement: Given a matrix $$T = \begin{pmatrix} t_{11} & 0 & 0 & \dotsm & 0 & 0& \dotsm & 0& 0\\ t_{21} & t_{22} & 0 & \dotsm & 0 & 0& \dotsm & 0 & 0 \\ 0 & t_{32} & t_{33} & \dotsm & 0& 0& \dotsm & 0 & 0 \\ \vdots \\0 & 0 & 0 & \dotsm & t_{i,i-1} & t_{ii} & \dotsm &0&0 \\ \vdots \\0&0&0& \dotsm &0& 0 & \dotsm &t_{m,m-1}&t_{mm} \end{pmatrix}$$ where $0 \leqslant t_{ij} \leqslant 1$  ( for $ 1 \leqslant i,j \leqslant m , i \neq j), 0<t_{ii}<1$ (for $1\leqslant i \leqslant m$) and $\sum_{i=1}^m{t_{ij}} \leqslant 1$ for each $j$  Then  \begin{equation} \label{trans_mat_geom}  (I-T)^{-1} = \sum\limits_{k=0}^{\infty}{T^k}.  \end{equation}  Thanks in advance.","Can anybody show how to prove or show where to find a proof of the following statement: Given a matrix $$T = \begin{pmatrix} t_{11} & 0 & 0 & \dotsm & 0 & 0& \dotsm & 0& 0\\ t_{21} & t_{22} & 0 & \dotsm & 0 & 0& \dotsm & 0 & 0 \\ 0 & t_{32} & t_{33} & \dotsm & 0& 0& \dotsm & 0 & 0 \\ \vdots \\0 & 0 & 0 & \dotsm & t_{i,i-1} & t_{ii} & \dotsm &0&0 \\ \vdots \\0&0&0& \dotsm &0& 0 & \dotsm &t_{m,m-1}&t_{mm} \end{pmatrix}$$ where $0 \leqslant t_{ij} \leqslant 1$  ( for $ 1 \leqslant i,j \leqslant m , i \neq j), 0<t_{ii}<1$ (for $1\leqslant i \leqslant m$) and $\sum_{i=1}^m{t_{ij}} \leqslant 1$ for each $j$  Then  \begin{equation} \label{trans_mat_geom}  (I-T)^{-1} = \sum\limits_{k=0}^{\infty}{T^k}.  \end{equation}  Thanks in advance.",,"['linear-algebra', 'analysis', 'matrices']"
42,"Homogeneous polynomials on a vector space $V$, $\operatorname{Sym}^d(V^*)$ and naturality","Homogeneous polynomials on a vector space ,  and naturality",V \operatorname{Sym}^d(V^*),"My three questions all relate to Harris' ""Algebraic Geometry - A First Course"". Unless mentioned otherwise, $V$ is an n-dimensional vector space over an arbitrary field $k$. 1) At first, I'd like to make sure that I have the right motivation for this construction. According to my humble understanding, the main advantage of $\operatorname{Sym}^d(V^*)$ is having a framework to talk about homogeneous polynomials of a fixed degree on V and being able to evaluate them without the need to choose a basis. More precisely, there exists an evaluation map $\operatorname{Sym}^d(V^*) \times V \rightarrow k$ via $(\sum\limits_i a_{i_1}a_{i_2}\cdots a_{i_d}\hat{v_{i_1}}\cdot \hat{v_{i_2}} \cdots \hat{v_{i_d}}, v) \mapsto \sum\limits_i a_{i_1}a_{i_2}\cdots a_{i_d}\hat{v_{i_1}}(v) \hat{v_{i_2}}(v) \cdots \hat{v_{i_d}}(v)$ Is that correct? 2) On page 4, it is written that homogeneous polynomials on $\mathbb{P}V$ can be naturally identified with the vector space $\operatorname{Sym}^d(V^*)$. I was able to find the obvious isomorphism from $\operatorname{Sym}^d(V^*)$ to $k[X_0, X_1, \ldots, X_n]_d$ after choosing a basis for $V^*$, but I don't see how such an identification can be made naturally. 3) Let $V$ be two-dimensional, $char\ k \neq 2$ In chapter 10, 10.8 we deal with the action of $PGL_2(k)$ on $\mathbb{P}^2$. Now $PGL_2(k)$ obviously acts on $\mathbb{P}^1$ and Harris mentions that this naturally induces an action on $\mathbb{P}(\operatorname{Sym}^2(V^*))$. However, I fail to realize what this action is supposed to look like, let alone how it is obtained naturally (I DO see how $PGL_2(k)$ acts on $\mathbb{P}(\operatorname{Sym}^2(V))\cong \mathbb{P^2}$, though). On a side note, where exactly comes the assumption on the characteristic of $k$ into play? When I verified that the set of squares $v\cdot v$ is isomorphic to the image of the quadratic Veronese, I had to divide by two, but I suspect that this is not the only reason to make this assumption. Thanks in advance for any help. I tried to keep this as brief as possible, but to no avail. Sorry for that.","My three questions all relate to Harris' ""Algebraic Geometry - A First Course"". Unless mentioned otherwise, $V$ is an n-dimensional vector space over an arbitrary field $k$. 1) At first, I'd like to make sure that I have the right motivation for this construction. According to my humble understanding, the main advantage of $\operatorname{Sym}^d(V^*)$ is having a framework to talk about homogeneous polynomials of a fixed degree on V and being able to evaluate them without the need to choose a basis. More precisely, there exists an evaluation map $\operatorname{Sym}^d(V^*) \times V \rightarrow k$ via $(\sum\limits_i a_{i_1}a_{i_2}\cdots a_{i_d}\hat{v_{i_1}}\cdot \hat{v_{i_2}} \cdots \hat{v_{i_d}}, v) \mapsto \sum\limits_i a_{i_1}a_{i_2}\cdots a_{i_d}\hat{v_{i_1}}(v) \hat{v_{i_2}}(v) \cdots \hat{v_{i_d}}(v)$ Is that correct? 2) On page 4, it is written that homogeneous polynomials on $\mathbb{P}V$ can be naturally identified with the vector space $\operatorname{Sym}^d(V^*)$. I was able to find the obvious isomorphism from $\operatorname{Sym}^d(V^*)$ to $k[X_0, X_1, \ldots, X_n]_d$ after choosing a basis for $V^*$, but I don't see how such an identification can be made naturally. 3) Let $V$ be two-dimensional, $char\ k \neq 2$ In chapter 10, 10.8 we deal with the action of $PGL_2(k)$ on $\mathbb{P}^2$. Now $PGL_2(k)$ obviously acts on $\mathbb{P}^1$ and Harris mentions that this naturally induces an action on $\mathbb{P}(\operatorname{Sym}^2(V^*))$. However, I fail to realize what this action is supposed to look like, let alone how it is obtained naturally (I DO see how $PGL_2(k)$ acts on $\mathbb{P}(\operatorname{Sym}^2(V))\cong \mathbb{P^2}$, though). On a side note, where exactly comes the assumption on the characteristic of $k$ into play? When I verified that the set of squares $v\cdot v$ is isomorphic to the image of the quadratic Veronese, I had to divide by two, but I suspect that this is not the only reason to make this assumption. Thanks in advance for any help. I tried to keep this as brief as possible, but to no avail. Sorry for that.",,"['linear-algebra', 'algebraic-geometry']"
43,Creating a matrix of rank r from r number of rank 1 matrices?,Creating a matrix of rank r from r number of rank 1 matrices?,,"I am told that all matrices of Rank $r$ can be formed out of the combinations of $r$ number of Rank 1 matrices. So that's the original matrix can be broken down into $r$ number of rank 1 matrices. But I don't understand and see how this is possible. Say for a matrix of this form: $$ A=\begin{bmatrix} 1 & 3 & 2 & 6\\  3 & 0 & 1 & 4\\  2 & 1 & 1 & 4 \end{bmatrix} $$ The $rank(A)= 3$. So if the claim was right, then I can form back the same matrix $A$ with the combination of 3 of Rank 1 matrices. I tried to ""emulate"" that idea but I just don't totally get how I could do it. Thanks for any help on this!","I am told that all matrices of Rank $r$ can be formed out of the combinations of $r$ number of Rank 1 matrices. So that's the original matrix can be broken down into $r$ number of rank 1 matrices. But I don't understand and see how this is possible. Say for a matrix of this form: $$ A=\begin{bmatrix} 1 & 3 & 2 & 6\\  3 & 0 & 1 & 4\\  2 & 1 & 1 & 4 \end{bmatrix} $$ The $rank(A)= 3$. So if the claim was right, then I can form back the same matrix $A$ with the combination of 3 of Rank 1 matrices. I tried to ""emulate"" that idea but I just don't totally get how I could do it. Thanks for any help on this!",,"['linear-algebra', 'matrices']"
44,Will any value of a free variable satisfy a system of equation?,Will any value of a free variable satisfy a system of equation?,,"Say I have a reduced row echelon form matrix like this: $$A=\begin{bmatrix} 1 & 0 & \frac{1}{2} & 0\\  0 & 1 & -\frac{1}{3} & 0\\  0 & 0 & 0 & 1 \end{bmatrix}$$ The number of unknowns is more than the number of known equations. So I can expect an infinite number of solutions. And $Ax=b$ is like this: $$ \begin{bmatrix} 1 & 0 & \frac{1}{2} & 0\\  0 & 1 & -\frac{1}{3} & 0\\  0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x_{1}\\  x_{2}\\  x_{3}\\  x_{4} \end{bmatrix} = \begin{bmatrix} b_{1}\\  b_{2}\\  b_{3} \end{bmatrix} $$ Then I can say that my $x$ is like this with $x_{3}$ being a free variable in the equation: $$\begin{bmatrix} x_{1}\\  x_{2}\\  x_{3}\\  x_{4} \end{bmatrix} = x_{3}\begin{bmatrix} \frac{1}{2}\\  \frac{1}{3}\\  1\\  0 \end{bmatrix}+ \begin{bmatrix} b_{1}\\  b_{2}\\  0\\  b_{3} \end{bmatrix} $$ Now, if I let $\; \begin{bmatrix} b_{1}\\  b_{2}\\  b_{3} \end{bmatrix}=\begin{bmatrix} 5\\  2\\  7 \end{bmatrix}$, then... $$\begin{bmatrix} x_{1}\\  x_{2}\\  x_{3}\\  x_{4} \end{bmatrix} = x_{3}\begin{bmatrix} \frac{1}{2}\\  \frac{1}{3}\\  1\\  0 \end{bmatrix}+ \begin{bmatrix} 5\\  2\\  0\\  7 \end{bmatrix} $$ At this stage, I can say that for any value that I put into the variable $x_{3}$, I would get an answer that is equals to $b$, right? So assume I just randomly throw a value into $x_{3}=2$, then... $$\begin{bmatrix} x_{1}\\  x_{2}\\  x_{3}\\  x_{4} \end{bmatrix} = \begin{bmatrix} 1\\  \frac{2}{3}\\  2\\  0 \end{bmatrix}+ \begin{bmatrix} 5\\  2\\  0\\  7 \end{bmatrix}= \begin{bmatrix} 6\\  \frac{8}{3}\\  2\\  7 \end{bmatrix} $$ From here, I need to tally if the equation really gets back my intended values of $b$, which is $\begin{bmatrix} b_{1}\\  b_{2}\\  b_{3} \end{bmatrix}=\begin{bmatrix} 5\\  2\\  7 \end{bmatrix}$. So, $x_{4} = b_{3} =7$ This is correct. $b_{2}=x_{2}-\frac{1}{3}x_{3}$ $b_{2}=\frac{8}{3}-\frac{2}{3}=2$ This is also correct. BUT NOW, $b_{1}=x_{1}+\frac{1}{2}x_{3}$ $b_{1}=6+1=7$ $b_{1}$ suppose to be 6 but somehow, why am I getting 7 instead? This is weird. Where did I go wrong? Thanks for any help!","Say I have a reduced row echelon form matrix like this: $$A=\begin{bmatrix} 1 & 0 & \frac{1}{2} & 0\\  0 & 1 & -\frac{1}{3} & 0\\  0 & 0 & 0 & 1 \end{bmatrix}$$ The number of unknowns is more than the number of known equations. So I can expect an infinite number of solutions. And $Ax=b$ is like this: $$ \begin{bmatrix} 1 & 0 & \frac{1}{2} & 0\\  0 & 1 & -\frac{1}{3} & 0\\  0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x_{1}\\  x_{2}\\  x_{3}\\  x_{4} \end{bmatrix} = \begin{bmatrix} b_{1}\\  b_{2}\\  b_{3} \end{bmatrix} $$ Then I can say that my $x$ is like this with $x_{3}$ being a free variable in the equation: $$\begin{bmatrix} x_{1}\\  x_{2}\\  x_{3}\\  x_{4} \end{bmatrix} = x_{3}\begin{bmatrix} \frac{1}{2}\\  \frac{1}{3}\\  1\\  0 \end{bmatrix}+ \begin{bmatrix} b_{1}\\  b_{2}\\  0\\  b_{3} \end{bmatrix} $$ Now, if I let $\; \begin{bmatrix} b_{1}\\  b_{2}\\  b_{3} \end{bmatrix}=\begin{bmatrix} 5\\  2\\  7 \end{bmatrix}$, then... $$\begin{bmatrix} x_{1}\\  x_{2}\\  x_{3}\\  x_{4} \end{bmatrix} = x_{3}\begin{bmatrix} \frac{1}{2}\\  \frac{1}{3}\\  1\\  0 \end{bmatrix}+ \begin{bmatrix} 5\\  2\\  0\\  7 \end{bmatrix} $$ At this stage, I can say that for any value that I put into the variable $x_{3}$, I would get an answer that is equals to $b$, right? So assume I just randomly throw a value into $x_{3}=2$, then... $$\begin{bmatrix} x_{1}\\  x_{2}\\  x_{3}\\  x_{4} \end{bmatrix} = \begin{bmatrix} 1\\  \frac{2}{3}\\  2\\  0 \end{bmatrix}+ \begin{bmatrix} 5\\  2\\  0\\  7 \end{bmatrix}= \begin{bmatrix} 6\\  \frac{8}{3}\\  2\\  7 \end{bmatrix} $$ From here, I need to tally if the equation really gets back my intended values of $b$, which is $\begin{bmatrix} b_{1}\\  b_{2}\\  b_{3} \end{bmatrix}=\begin{bmatrix} 5\\  2\\  7 \end{bmatrix}$. So, $x_{4} = b_{3} =7$ This is correct. $b_{2}=x_{2}-\frac{1}{3}x_{3}$ $b_{2}=\frac{8}{3}-\frac{2}{3}=2$ This is also correct. BUT NOW, $b_{1}=x_{1}+\frac{1}{2}x_{3}$ $b_{1}=6+1=7$ $b_{1}$ suppose to be 6 but somehow, why am I getting 7 instead? This is weird. Where did I go wrong? Thanks for any help!",,"['linear-algebra', 'matrices']"
45,Convergence from spectrum of the Choi matrix?,Convergence from spectrum of the Choi matrix?,,"Suppose I have a linear operation of the form $T(C)=\frac{1}{m}\sum_i^m A_i C A_i$ for a set of $m$ symmetric $d \times d$ matrices $A_i$ . I construct Choi matrix below with $e_i$ referring to standard basis $$ M=\left(\begin{array}{cccc} T(e_1 e_1^T) & T(e_1 e_2^T )& \ldots& T(e_1 e_d^T)\\ \ldots & \ldots & \ldots&\ldots\\ T(e_d e_1^T) & T(e_d e_2^T) & \ldots & T(e_d e_d^T) \end{array} \right) $$ Is it possible to tell if $T$ is contractive or convergent by looking at eigenvalues of $M$ ? Note that entries of $M$ are rearrangement of entries of $T$ viewed as linear matrix acting on $\operatorname{vec}C$ ( Choi-Jamiołkowski isomorphism ) $$ T=\left(\begin{array}{c} \operatorname{vec}T(e_1 e_1^T)^T\\ \operatorname{vec}T(e_2 e_1^T)^T\\ \ldots \\ \operatorname{vec}T(e_d e_1^T)^T\\ \operatorname{vec}T(e_1 e_2^T)^T\\ \ldots \\ \operatorname{vec}T(e_d e_d^T)^T\\ \end{array} \right) $$ In my application, most eigenvalues of $M$ appear zero, while most eigenvalues of $T$ not-zero, so it seems that $M$ spectrum is easier to analyze. Example: $$A=\left( \begin{array}{cc}  0 & 0 \\  0 & 1 \\ \end{array} \right),\left( \begin{array}{cc}  0 & -1 \\  -1 & 0 \\ \end{array} \right) $$ $$M=\left( \begin{array}{cccc}  0 & 0 & 0 & 0 \\  0 & \frac{1}{2} & \frac{1}{2} & 0 \\  0 & \frac{1}{2} & \frac{1}{2} & 0 \\  0 & 0 & 0 & \frac{1}{2} \\ \end{array} \right)$$ $$T=\left( \begin{array}{cccc}  0 & 0 & 0 & \frac{1}{2} \\  0 & 0 & \frac{1}{2} & 0 \\  0 & \frac{1}{2} & 0 & 0 \\  \frac{1}{2} & 0 & 0 & \frac{1}{2} \\ \end{array} \right)$$ Eigenvalues of $M$ are ${1, 1/2, 0, 0}$ , while eigenvalues of $T$ are $\left\{\frac{1}{4} \left(\sqrt{5}+1\right),-\frac{1}{2},\frac{1}{2},\frac{1}{4} \left(1-\sqrt{5}\right)\right\}$ Notebook","Suppose I have a linear operation of the form for a set of symmetric matrices . I construct Choi matrix below with referring to standard basis Is it possible to tell if is contractive or convergent by looking at eigenvalues of ? Note that entries of are rearrangement of entries of viewed as linear matrix acting on ( Choi-Jamiołkowski isomorphism ) In my application, most eigenvalues of appear zero, while most eigenvalues of not-zero, so it seems that spectrum is easier to analyze. Example: Eigenvalues of are , while eigenvalues of are Notebook","T(C)=\frac{1}{m}\sum_i^m A_i C A_i m d \times d A_i e_i 
M=\left(\begin{array}{cccc}
T(e_1 e_1^T) & T(e_1 e_2^T )& \ldots& T(e_1 e_d^T)\\
\ldots & \ldots & \ldots&\ldots\\
T(e_d e_1^T) & T(e_d e_2^T) & \ldots & T(e_d e_d^T)
\end{array}
\right)
 T M M T \operatorname{vec}C 
T=\left(\begin{array}{c}
\operatorname{vec}T(e_1 e_1^T)^T\\
\operatorname{vec}T(e_2 e_1^T)^T\\
\ldots \\
\operatorname{vec}T(e_d e_1^T)^T\\
\operatorname{vec}T(e_1 e_2^T)^T\\
\ldots \\
\operatorname{vec}T(e_d e_d^T)^T\\
\end{array}
\right)
 M T M A=\left(
\begin{array}{cc}
 0 & 0 \\
 0 & 1 \\
\end{array}
\right),\left(
\begin{array}{cc}
 0 & -1 \\
 -1 & 0 \\
\end{array}
\right)
 M=\left(
\begin{array}{cccc}
 0 & 0 & 0 & 0 \\
 0 & \frac{1}{2} & \frac{1}{2} & 0 \\
 0 & \frac{1}{2} & \frac{1}{2} & 0 \\
 0 & 0 & 0 & \frac{1}{2} \\
\end{array}
\right) T=\left(
\begin{array}{cccc}
 0 & 0 & 0 & \frac{1}{2} \\
 0 & 0 & \frac{1}{2} & 0 \\
 0 & \frac{1}{2} & 0 & 0 \\
 \frac{1}{2} & 0 & 0 & \frac{1}{2} \\
\end{array}
\right) M {1, 1/2, 0, 0} T \left\{\frac{1}{4} \left(\sqrt{5}+1\right),-\frac{1}{2},\frac{1}{2},\frac{1}{4} \left(1-\sqrt{5}\right)\right\}","['linear-algebra', 'operator-theory', 'positive-definite', 'quantum-mechanics']"
46,Different Approaches for Proving Kantorovich Inequality,Different Approaches for Proving Kantorovich Inequality,,"Here is a statement of the famous Kantorovich inequality. Thoerem (Kantorovich). Let $A$ be a $n\times n$ symmetric and positive matrix. Furthermore, assume that its eigenvalues are $0 < \lambda_1 \leq \dots \leq \lambda_n$ . Then, the following inequality holds for all $\mathbf{x}\in\mathbb{R}^n$ \begin{equation} \frac{(\mathbf{x}^{\top}A\mathbf{x})(\mathbf{x}^{\top}A^{-1}\mathbf{x})}{(\mathbf{x}^{\top}\mathbf{x})^2}  \leq \frac{1}{4}\frac{(\lambda_1+\lambda_n)^2}{\lambda_1\lambda_n} = \frac{1}{4}\Bigg(\sqrt{\frac{\lambda_1}{\lambda_n}}+\sqrt{\frac{\lambda_n}{\lambda_1}}\Bigg)^2. \end{equation} There are a variety of proofs for this inequality. My aim for asking this question is three fold. First, to gather a list of all nice proofs about this inequality. Second, to see if a proof with constrained optimization techniques is possible. Third, to know how Kantorovich thought about the problem. Here are the main questions. Questions What are different approaches (excluding those mentioned below) for proving Kantorovich inequality? Can it be proved via constrained optimization techniques, continuing what I described below? How did Kantorovich prove it himself? Different Approaches This is an elegant and beautiful proof based on probability techniques. This is another proof by simple and clever algebra. A Constrained Optimization Way However, I am wondering if it can be proved via the most naive idea that comes to mind. Indeed, by maximizing the left hand side of the inequality! For this purpose, we can rewrite the left hand side by introducing $\mathbf{y} = \frac{\mathbf{x}}{\lVert\mathbf{x}\rVert}$ as below \begin{equation} f(\mathbf{y}) = (\mathbf{y}^{\top}A\mathbf{y})(\mathbf{y}^{\top}A^{-1}\mathbf{y}). \end{equation} Now, it seems natural to maximize $\phi(\mathbf{y})$ subject to the constraint $\mathbf{y}^{\top}\mathbf{y} = 1$ . To make the problem even simpler, one can use the spectral decomposition $A=Q^{\top}\Lambda Q$ to write $\phi(\mathbf{y})$ as \begin{equation} g(\mathbf{z}) = \big(\sum_{i=1}^{n} \lambda_i z_i^2\big) \big(\sum_{i=1}^{n} \frac{1}{\lambda_i} z_i^2\big), \end{equation} where $\mathbf{z} = Q \mathbf{y}$ . Finally, let $\xi_i = z_i^2$ to arrive at \begin{equation} \phi(\boldsymbol{\xi}) = \big(\sum_{i=1}^{n} \lambda_i \xi_i\big) \big(\sum_{i=1}^{n} \frac{1}{\lambda_i} \xi_i\big) = \sum_{i=1}^{n}\sum_{j=1}^{n} \frac{\lambda_i}{\lambda_j}\xi_i\xi_j = \boldsymbol{\xi}B\boldsymbol{\xi}, \end{equation} with the constraints \begin{equation} \sum_{i=1}^{n}\xi_i = 1, \qquad \xi_i \ge 0. \end{equation} As we are usually fond of symmetric matrices we can replace $B$ by $\frac{1}{2}(B + B^{\top})$ because we know that $B = \frac{1}{2} (B + B^{\top}) + \frac{1}{2}(B - B^{\top})$ and $\frac{1}{2}\boldsymbol{\xi}(B - B^{\top})\boldsymbol{\xi} = 0.$ Consequently, $f$ can be rewritten as \begin{equation} \phi(\boldsymbol{\xi}) = \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n} \Bigg(\frac{\lambda_i}{\lambda_j} + \frac{\lambda_j}{\lambda_i}\Bigg)\xi_i\xi_j = \frac{1}{2}\boldsymbol{\xi}H\boldsymbol{\xi}. \end{equation} Can we find the maximizer of $\phi(\boldsymbol{\xi})$ subject to the aforementioned constraints via constrained optimization techniques?","Here is a statement of the famous Kantorovich inequality. Thoerem (Kantorovich). Let be a symmetric and positive matrix. Furthermore, assume that its eigenvalues are . Then, the following inequality holds for all There are a variety of proofs for this inequality. My aim for asking this question is three fold. First, to gather a list of all nice proofs about this inequality. Second, to see if a proof with constrained optimization techniques is possible. Third, to know how Kantorovich thought about the problem. Here are the main questions. Questions What are different approaches (excluding those mentioned below) for proving Kantorovich inequality? Can it be proved via constrained optimization techniques, continuing what I described below? How did Kantorovich prove it himself? Different Approaches This is an elegant and beautiful proof based on probability techniques. This is another proof by simple and clever algebra. A Constrained Optimization Way However, I am wondering if it can be proved via the most naive idea that comes to mind. Indeed, by maximizing the left hand side of the inequality! For this purpose, we can rewrite the left hand side by introducing as below Now, it seems natural to maximize subject to the constraint . To make the problem even simpler, one can use the spectral decomposition to write as where . Finally, let to arrive at with the constraints As we are usually fond of symmetric matrices we can replace by because we know that and Consequently, can be rewritten as Can we find the maximizer of subject to the aforementioned constraints via constrained optimization techniques?","A n\times n 0 < \lambda_1 \leq \dots \leq \lambda_n \mathbf{x}\in\mathbb{R}^n \begin{equation}
\frac{(\mathbf{x}^{\top}A\mathbf{x})(\mathbf{x}^{\top}A^{-1}\mathbf{x})}{(\mathbf{x}^{\top}\mathbf{x})^2} 
\leq \frac{1}{4}\frac{(\lambda_1+\lambda_n)^2}{\lambda_1\lambda_n}
= \frac{1}{4}\Bigg(\sqrt{\frac{\lambda_1}{\lambda_n}}+\sqrt{\frac{\lambda_n}{\lambda_1}}\Bigg)^2.
\end{equation} \mathbf{y} = \frac{\mathbf{x}}{\lVert\mathbf{x}\rVert} \begin{equation}
f(\mathbf{y}) = (\mathbf{y}^{\top}A\mathbf{y})(\mathbf{y}^{\top}A^{-1}\mathbf{y}).
\end{equation} \phi(\mathbf{y}) \mathbf{y}^{\top}\mathbf{y} = 1 A=Q^{\top}\Lambda Q \phi(\mathbf{y}) \begin{equation}
g(\mathbf{z}) = \big(\sum_{i=1}^{n} \lambda_i z_i^2\big) \big(\sum_{i=1}^{n} \frac{1}{\lambda_i} z_i^2\big),
\end{equation} \mathbf{z} = Q \mathbf{y} \xi_i = z_i^2 \begin{equation}
\phi(\boldsymbol{\xi}) = \big(\sum_{i=1}^{n} \lambda_i \xi_i\big) \big(\sum_{i=1}^{n} \frac{1}{\lambda_i} \xi_i\big)
= \sum_{i=1}^{n}\sum_{j=1}^{n} \frac{\lambda_i}{\lambda_j}\xi_i\xi_j
= \boldsymbol{\xi}B\boldsymbol{\xi},
\end{equation} \begin{equation}
\sum_{i=1}^{n}\xi_i = 1, \qquad \xi_i \ge 0.
\end{equation} B \frac{1}{2}(B + B^{\top}) B = \frac{1}{2} (B + B^{\top}) + \frac{1}{2}(B - B^{\top}) \frac{1}{2}\boldsymbol{\xi}(B - B^{\top})\boldsymbol{\xi} = 0. f \begin{equation}
\phi(\boldsymbol{\xi})
= \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n} \Bigg(\frac{\lambda_i}{\lambda_j} + \frac{\lambda_j}{\lambda_i}\Bigg)\xi_i\xi_j
= \frac{1}{2}\boldsymbol{\xi}H\boldsymbol{\xi}.
\end{equation} \phi(\boldsymbol{\xi})","['linear-algebra', 'multivariable-calculus', 'inequality', 'optimization', 'constraints']"
47,Eigenvalue of the 'norm' matrix,Eigenvalue of the 'norm' matrix,,"A week ago, I asked a question about the matrix composed with absolute difference of distinct number must have exactly only one positive eigenvalue. But I have no idea how to prove it with 'norm' matrix $A=(a_{ij})_{n\times n}$ , where $$ a_{ij}=\|\xi_{i}-\xi_{j}\|_2. $$ Here $\xi_i\in\mathbb{R}^m$ are distinct vector and $\|\cdot\|_2$ is the Euclidean norm. Also I have verified it with MATLAB for $n=20$ and $m=5$ , but the method used for $m=1$ failed. Any advice is welcome!","A week ago, I asked a question about the matrix composed with absolute difference of distinct number must have exactly only one positive eigenvalue. But I have no idea how to prove it with 'norm' matrix , where Here are distinct vector and is the Euclidean norm. Also I have verified it with MATLAB for and , but the method used for failed. Any advice is welcome!","A=(a_{ij})_{n\times n} 
a_{ij}=\|\xi_{i}-\xi_{j}\|_2.
 \xi_i\in\mathbb{R}^m \|\cdot\|_2 n=20 m=5 m=1","['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-norms']"
48,Confusion about Theorem 11 Chapter 2 of Hoffman and Kunze,Confusion about Theorem 11 Chapter 2 of Hoffman and Kunze,,"This question has been asked before but I'm afraid I didn't follow the answer (as I mention in my (1a) way below, I'm not even sure the answer is complete), and neither OP nor the answerer are active. I thus ask again, with some extra details around my confusion. Note that HK = Hoffman and Kunze. The theorem statement: Theorem 11. Let $m$ and $n$ be positive integers and let $\mathbb{F}$ be a field. Suppose $W$ is a subspace of $\mathbb{F}^n$ and $\dim(W)\leq m$ . Then there is precisely one $m\times n$ row-reduced echelon matrix over $\mathbb{F}$ which has $W$ as its row space. Before giving the proof, I'll emphasize that I understand that this is an existence and uniqueness claim. That is, given some subspace $W$ of $\mathbb{F}^n$ (1) there exists a (2) unique $m \times n$ matrix in reduced-row echelon form (RREF). (Indeed, as we'll see by the construction, the matrices with different $m$ are trivially related by adding or removing zero rows). Now for the proof, which comes in what feels to me like a convoluted order. I label the various ""stanzas"" with letters so as to refer to them later. Proof. Existence: Proof omitted as I follow this. I will call the matrix  with the requisite properties proved constructively to exist in this step as $R'$ [see 1a below]. Uniqueness: (a) Now let $R$ be any row-reduced echelon matrix which has $W$ as its row space. Let $\rho_1,\dots,\rho_r$ be the non-zero row vectors of $R$ and suppose that the leading non-zero entry of $\rho_i$ occurs in column $k_i$ , $i=1,\dots,r$ . The vectors $\rho_1,\dots,\rho_r$ form a basis for $W$ ( Theorem 10 ). In the proof of Theorem 10 , we observed that if $\beta=(b_1, \dots , b_n)$ is in $W$ , then the unique expression of $\beta$ as a linear combination of $\rho_1,\dots,\rho_r$ is $$\beta=\sum_{i=1}^{r}b_{k_i}\rho_i.$$ Thus any vector $\beta$ is determined if one knows the coordinates $b_{k_i}$ , $i=1,\dots,r$ . (b) Suppose $\beta\in W$ and $\beta\ne0$ . We claim the first non-zero coordinate of $\beta$ occurs in one of the columns $k_s$ . Since $$\beta=\sum_{i=1}^{r}b_{k_i}\rho_i$$ and $\beta\ne0$ , we can write $$\beta=\sum_{i=s}^{r}b_{k_i}\rho_i\;,\;b_{k_s}\ne0.$$ From the fact that $R$ is a row-reduced echelon matrix one has $R_{ij}=0$ if $i>s$ and $j\leq k_s$ . Thus $$\beta=(0,\dots,0,b_{k_s},\dots,b_n)\;,\;b_{k_s}\ne0$$ and the first non-zero coordinate of $\beta$ occurs in one of the columns $k_s$ . (c) It is now clear that $R$ is uniquely determined by $W$ . The description of $R$ in terms of $W$ is as follows. We consider all vectors $\beta = (b_1, \dots , b_n)$ in $W$ . If $\beta\ne0$ , then the first non-zero coordinate of $\beta$ must occur in some column $t$ : $$\beta=(0,\dots,0,b_t,\dots,b_n)\;,\;b_t\ne0.$$ Let $k_1, \dots , k_r$ be those positive integers $t$ such that there is some $\beta\ne0$ in $W$ , the first non-zero coordinate of which occurs in column $t$ . Arrange $k_1, \dots , k_r$ in the order $k_1 < k_2 < \dots < k_r$ . For each of the positive integers $k_s$ there will be one and only one vector $\rho_s$ in $W$ such that the $k_s$ th coordinate of $\rho_s$ is 1 and the $k_i$ th coordinate of $\rho_s$ is 0 for $i\ne s$ . Then $R$ is the $m\times n$ matrix which has row vectors $\rho_1, \dots , \rho_r, 0, \dots , 0$ . Now it's clear that stanza (c) is where the actual uniqueness claim is proved, and that stanzas (a) and (b) are lemmas used in (c). But I can't quite follow how. In particular, (1a) ""For each of the positive integers $k_s$ there will be one and only one vector $\rho_s$ in $W$ such that the $k_s$ th coordinate of $\rho_s$ is 1 and the $k_i$ th coordinate of $\rho_s$ is 0."" All we have by construction of the $k_i$ is that there is at least one vector in $W$ with a nonzero entry in $k_s$ , but that doesn't say that it's zero in the other columns $k_i$ . So where does the ""there will be one"" come from? The answer linked seems to suggest that the existence is guaranteed by the existence of $R'$ with the required properties. But why should the columns which are nonzero in the $R'$ be related a priori to the $k_i$ which we have constructed here? (1b) Further, why should this be unique? I think (but am not sure, so can someone confirm?) that this is guaranteed by stanza (a), in that fixing the $b_{k_i}$ (in particular, we've chosen to fix $b_{k_i} = \delta_{is}$ ) determines the expansion coefficients of the vector in terms of the basis given by the rows of whatever $R$ (which has row space $W$ ) we are considering in this uniqueness proof, and basis expansion are unique. (2) Accepting that (1a,b) have been solved, I still cannot see why ""Then $R$ is the $m\times n$ matrix which has row vectors $\rho_1, \dots , \rho_r, 0, \dots , 0$ ."" Given that we haven't used it, clearly this must follow from stanza (b). But I can't see how it does the trick for us?","This question has been asked before but I'm afraid I didn't follow the answer (as I mention in my (1a) way below, I'm not even sure the answer is complete), and neither OP nor the answerer are active. I thus ask again, with some extra details around my confusion. Note that HK = Hoffman and Kunze. The theorem statement: Theorem 11. Let and be positive integers and let be a field. Suppose is a subspace of and . Then there is precisely one row-reduced echelon matrix over which has as its row space. Before giving the proof, I'll emphasize that I understand that this is an existence and uniqueness claim. That is, given some subspace of (1) there exists a (2) unique matrix in reduced-row echelon form (RREF). (Indeed, as we'll see by the construction, the matrices with different are trivially related by adding or removing zero rows). Now for the proof, which comes in what feels to me like a convoluted order. I label the various ""stanzas"" with letters so as to refer to them later. Proof. Existence: Proof omitted as I follow this. I will call the matrix  with the requisite properties proved constructively to exist in this step as [see 1a below]. Uniqueness: (a) Now let be any row-reduced echelon matrix which has as its row space. Let be the non-zero row vectors of and suppose that the leading non-zero entry of occurs in column , . The vectors form a basis for ( Theorem 10 ). In the proof of Theorem 10 , we observed that if is in , then the unique expression of as a linear combination of is Thus any vector is determined if one knows the coordinates , . (b) Suppose and . We claim the first non-zero coordinate of occurs in one of the columns . Since and , we can write From the fact that is a row-reduced echelon matrix one has if and . Thus and the first non-zero coordinate of occurs in one of the columns . (c) It is now clear that is uniquely determined by . The description of in terms of is as follows. We consider all vectors in . If , then the first non-zero coordinate of must occur in some column : Let be those positive integers such that there is some in , the first non-zero coordinate of which occurs in column . Arrange in the order . For each of the positive integers there will be one and only one vector in such that the th coordinate of is 1 and the th coordinate of is 0 for . Then is the matrix which has row vectors . Now it's clear that stanza (c) is where the actual uniqueness claim is proved, and that stanzas (a) and (b) are lemmas used in (c). But I can't quite follow how. In particular, (1a) ""For each of the positive integers there will be one and only one vector in such that the th coordinate of is 1 and the th coordinate of is 0."" All we have by construction of the is that there is at least one vector in with a nonzero entry in , but that doesn't say that it's zero in the other columns . So where does the ""there will be one"" come from? The answer linked seems to suggest that the existence is guaranteed by the existence of with the required properties. But why should the columns which are nonzero in the be related a priori to the which we have constructed here? (1b) Further, why should this be unique? I think (but am not sure, so can someone confirm?) that this is guaranteed by stanza (a), in that fixing the (in particular, we've chosen to fix ) determines the expansion coefficients of the vector in terms of the basis given by the rows of whatever (which has row space ) we are considering in this uniqueness proof, and basis expansion are unique. (2) Accepting that (1a,b) have been solved, I still cannot see why ""Then is the matrix which has row vectors ."" Given that we haven't used it, clearly this must follow from stanza (b). But I can't see how it does the trick for us?","m n \mathbb{F} W \mathbb{F}^n \dim(W)\leq m m\times n \mathbb{F} W W \mathbb{F}^n m \times n m R' R W \rho_1,\dots,\rho_r R \rho_i k_i i=1,\dots,r \rho_1,\dots,\rho_r W \beta=(b_1, \dots , b_n) W \beta \rho_1,\dots,\rho_r \beta=\sum_{i=1}^{r}b_{k_i}\rho_i. \beta b_{k_i} i=1,\dots,r \beta\in W \beta\ne0 \beta k_s \beta=\sum_{i=1}^{r}b_{k_i}\rho_i \beta\ne0 \beta=\sum_{i=s}^{r}b_{k_i}\rho_i\;,\;b_{k_s}\ne0. R R_{ij}=0 i>s j\leq k_s \beta=(0,\dots,0,b_{k_s},\dots,b_n)\;,\;b_{k_s}\ne0 \beta k_s R W R W \beta = (b_1, \dots , b_n) W \beta\ne0 \beta t \beta=(0,\dots,0,b_t,\dots,b_n)\;,\;b_t\ne0. k_1, \dots , k_r t \beta\ne0 W t k_1, \dots , k_r k_1 < k_2 < \dots < k_r k_s \rho_s W k_s \rho_s k_i \rho_s i\ne s R m\times n \rho_1, \dots , \rho_r, 0, \dots , 0 k_s \rho_s W k_s \rho_s k_i \rho_s k_i W k_s k_i R' R' k_i b_{k_i} b_{k_i} = \delta_{is} R W R m\times n \rho_1, \dots , \rho_r, 0, \dots , 0","['linear-algebra', 'matrices', 'vector-spaces']"
49,"Given a matrix $A$ and vector $x$, is it possible to find the permutation of $x$ that minimizes $\lVert APx \rVert$?","Given a matrix  and vector , is it possible to find the permutation of  that minimizes ?",A x x \lVert APx \rVert,"Let $A$ be some $n\times n$ matrix, and $x$ be a column vector of length $n$ . I am looking to find the permutation matrix $P$ of size $n\times n$ that minimizes the vector norm of $APx$ . There are $n!$ such permutation matrices, so to exhaustively search for the minimizing $P$ quickly becomes intractable. I'm wondering if it is possible to show analytically what this $P$ should be, but am having trouble proceeding on this question. Any help or leads would be much appreciated! EDIT: While interested in the general case, which seems to be NP-complete, I would be happy to hear if there are special results concerning the special case that $A = QQ^\dagger-I$ , where $Q^\dagger$ is the Moore-Penrose inverse of $Q$ .","Let be some matrix, and be a column vector of length . I am looking to find the permutation matrix of size that minimizes the vector norm of . There are such permutation matrices, so to exhaustively search for the minimizing quickly becomes intractable. I'm wondering if it is possible to show analytically what this should be, but am having trouble proceeding on this question. Any help or leads would be much appreciated! EDIT: While interested in the general case, which seems to be NP-complete, I would be happy to hear if there are special results concerning the special case that , where is the Moore-Penrose inverse of .",A n\times n x n P n\times n APx n! P P A = QQ^\dagger-I Q^\dagger Q,"['linear-algebra', 'optimization', 'permutations', 'least-squares']"
50,"Dual space of $\mathcal{P}(M)$, when regarded as an $\mathbb{F}_2$-vector space.","Dual space of , when regarded as an -vector space.",\mathcal{P}(M) \mathbb{F}_2,"This question stems from the well-known observation that the power set $\mathcal{P}(M)$ of any set $M$ can be given an $\mathbb{F}_2$ -vector space structure by the symmetric difference operation. Any powerset is then naturally a dual space, namely of the subspace of the finite-element subsets, as can be seen from the pairing $$ \mathcal{P}(M)\times \mathcal{P}^{\mathrm{fin}}(M) \to \mathbb{F}_2, (A,B)\mapsto |A\cap B| \mod 2 $$ The question now is: Can we find a similarly explicit description for $\mathcal{P}(M)^*$ ? More ambitiously: Does the following construction encompass the whole space? For any ultrafilter $U$ on $M$ , we obtain an element in this space via ""integration against $U$ "", i.e. as the map $$\phi_{U}:\mathcal{P}(M) \to \mathbb{F}_2, A \mapsto \begin{cases} 1 & \text{if $A\in U$ and} \newline 0 & \text{otherwise.}\end{cases}$$ It is not difficult to show that the $\phi_U$ are linearly independent in $\mathcal{P}(M)^*$ : For different ultrafilters $U_1,...,U_n$ , we may always find subsets $A_1,...,A_{n-1}$ of $M$ such that $A_i\in U_n $ but $A_i\notin U_i$ for each $i$ . Then the intersection $A_1\cap...\cap A_{n-1}$ still lies in $U_n$ but in no other $U_i$ - consequently $\phi_{U_n}$ is not a linear combination of the other $\phi_{U_i}$ . Since $M$ supports $2^{2^{|M|}}$ different ultrafilters (assuming AC), the space spanned by the $\phi_U$ at least has the correct dimension (again assuming AC). (Note also that it contains the image of $\mathcal{P}^{\mathrm{fin}}(M)$ under the double dual embedding: Every element gets identified with the principal ultrafilter generated by its singleton.) I have no clue how to show that these maps generate the whole space though. Do you know an argument? Alternatively, can you give a description of a functional that is not of this form?","This question stems from the well-known observation that the power set of any set can be given an -vector space structure by the symmetric difference operation. Any powerset is then naturally a dual space, namely of the subspace of the finite-element subsets, as can be seen from the pairing The question now is: Can we find a similarly explicit description for ? More ambitiously: Does the following construction encompass the whole space? For any ultrafilter on , we obtain an element in this space via ""integration against "", i.e. as the map It is not difficult to show that the are linearly independent in : For different ultrafilters , we may always find subsets of such that but for each . Then the intersection still lies in but in no other - consequently is not a linear combination of the other . Since supports different ultrafilters (assuming AC), the space spanned by the at least has the correct dimension (again assuming AC). (Note also that it contains the image of under the double dual embedding: Every element gets identified with the principal ultrafilter generated by its singleton.) I have no clue how to show that these maps generate the whole space though. Do you know an argument? Alternatively, can you give a description of a functional that is not of this form?","\mathcal{P}(M) M \mathbb{F}_2 
\mathcal{P}(M)\times \mathcal{P}^{\mathrm{fin}}(M) \to \mathbb{F}_2, (A,B)\mapsto |A\cap B| \mod 2
 \mathcal{P}(M)^* U M U \phi_{U}:\mathcal{P}(M) \to \mathbb{F}_2, A \mapsto \begin{cases} 1 & \text{if A\in U and} \newline
0 & \text{otherwise.}\end{cases} \phi_U \mathcal{P}(M)^* U_1,...,U_n A_1,...,A_{n-1} M A_i\in U_n  A_i\notin U_i i A_1\cap...\cap A_{n-1} U_n U_i \phi_{U_n} \phi_{U_i} M 2^{2^{|M|}} \phi_U \mathcal{P}^{\mathrm{fin}}(M)","['linear-algebra', 'set-theory', 'filters']"
51,Is there any more efficient way to find the basis of the intersection of two subspaces?,Is there any more efficient way to find the basis of the intersection of two subspaces?,,"Let $V = \mathbb R^6.$ Let $W_1$ be the subspace of $V$ spanned by $$\left ( 1,2,3,4,5,6 \right ) ,\space \left ( 3,4,6,7,9,10 \right ) ,\space \left ( 0,1,0,2,0,3 \right ),\space\left ( 1,-2,3,-4,5,-6 \right ).  $$ Let $W_2$ be the subspace of $V$ spanned by $$\left ( 1,1,1,2,2,3 \right ) ,\space \left ( -2,0,-1,0,1,2 \right ) ,\space \left ( 1,0,1,0,2,0 \right ),\space\left ( 0,0,1,0,-2,-2 \right ).$$ Find the dimension of the subspace $W_1\cap W_2$ and find a basis for this subspace. I know I could take $\forall x=\left ( a,b,c,d,e,f \right ) \in W_1\cap W_2$ $$x=\alpha _1\left ( 1,2,3,4,5,6 \right ) +\space \alpha _2\left ( 3,4,6,7,9,10 \right ) +\space \alpha _3\left ( 0,1,0,2,0,3 \right )+\space\alpha_4 \left ( 1,-2,3,-4,5,-6 \right ) $$ and $$x=\beta_1 \left ( 1,1,1,2,2,3 \right ) +\space \beta _2\left ( -2,0,-1,0,1,2 \right ) +\space  \beta_3\left ( 1,0,1,0,2,0 \right )+\space\beta _4\left ( 0,0,1,0,-2,-2 \right ).$$ Then $$\begin{bmatrix}  1&  3&  0&  1& a\\  2&  4&  1&  -2& b\\  3&  6&  0&  3& c\\  4&  7&  2&  -4& d\\  5&  9&  0&  5& e\\  6&  10&  3&  -6&f\end{bmatrix} \text{and} \begin{bmatrix}  1&  -2&  1&  0& a\\  1&  0&  0&  0& b\\  1&  -1&  1&  1& c\\  2&  0&  0&  0& d\\  2&  1&  2&  -2& e\\  3&  2&  0&  -2&f\end{bmatrix}.$$ Then takes so much (tedious) effort to get $$\begin{bmatrix}  1&  3&  0&  1& a\\  0&  0&  1&  -4& b-\frac{2}{3}c \\  0&  1&  0&  0& a-\frac{1}{3}c \\  0&  0&  0&  0& a-2b-\frac{1}{3}c+d \\  0&  0&  0&  0&  a-2c+e\\  0&  0&  0&  0&2a-3b-\frac{2}{3}c+f\end{bmatrix} \text{and} \begin{bmatrix}  1&  -2&  1&  0& a\\  0&  0&  -1&  -2& b+a-2c \\  0&  1&  0&  1& c-a \\  0&  0&  0&  0& d-2b \\  0&  0&  0&  0&  4e-2a-6c-7f+21b\\  0&  0&  0&  1&\frac{-1}{4}f-\frac{1}{2}a+\frac{1}{2}c+\frac{3}{4}b    \end{bmatrix}.$$ So $\forall x=\left ( a,b,c,d,e,f \right ) \in W_1\cap W_2$ , we have \begin{align} a -2b-\frac{1}{3}c+d &=0 \\ a-2c+e &= 0 \\ 2a-3b-\frac{2}{3}c +f &=0 \\ 4e-2a-6c-7f+21b &=0 \\ d-2b &=0 \end{align} Finally, we get $$x=\left ( e,f,3e,2f,5e,3f \right ) $$ so $$\left \{ \left  (1,0,3,0,5,0 \right ) ,\space \left (0,1,0,2,0,3  \right )   \right \} $$ is a basis for $W_1\cap W_2$ . Is there any efficient way to accomplish this?","Let Let be the subspace of spanned by Let be the subspace of spanned by Find the dimension of the subspace and find a basis for this subspace. I know I could take and Then Then takes so much (tedious) effort to get So , we have Finally, we get so is a basis for . Is there any efficient way to accomplish this?","V = \mathbb R^6. W_1 V \left ( 1,2,3,4,5,6 \right ) ,\space \left ( 3,4,6,7,9,10 \right ) ,\space \left ( 0,1,0,2,0,3 \right ),\space\left ( 1,-2,3,-4,5,-6 \right ).   W_2 V \left ( 1,1,1,2,2,3 \right ) ,\space \left ( -2,0,-1,0,1,2 \right ) ,\space \left ( 1,0,1,0,2,0 \right ),\space\left ( 0,0,1,0,-2,-2 \right ). W_1\cap W_2 \forall x=\left ( a,b,c,d,e,f \right ) \in W_1\cap W_2 x=\alpha _1\left ( 1,2,3,4,5,6 \right ) +\space \alpha _2\left ( 3,4,6,7,9,10 \right ) +\space \alpha _3\left ( 0,1,0,2,0,3 \right )+\space\alpha_4 \left ( 1,-2,3,-4,5,-6 \right )  x=\beta_1 \left ( 1,1,1,2,2,3 \right ) +\space \beta _2\left ( -2,0,-1,0,1,2 \right ) +\space  \beta_3\left ( 1,0,1,0,2,0 \right )+\space\beta _4\left ( 0,0,1,0,-2,-2 \right ). \begin{bmatrix}  1&  3&  0&  1& a\\  2&  4&  1&  -2& b\\  3&  6&  0&  3& c\\  4&  7&  2&  -4& d\\  5&  9&  0&  5& e\\  6&  10&  3&  -6&f\end{bmatrix} \text{and} \begin{bmatrix}  1&  -2&  1&  0& a\\  1&  0&  0&  0& b\\  1&  -1&  1&  1& c\\  2&  0&  0&  0& d\\  2&  1&  2&  -2& e\\  3&  2&  0&  -2&f\end{bmatrix}. \begin{bmatrix}  1&  3&  0&  1& a\\  0&  0&  1&  -4& b-\frac{2}{3}c \\  0&  1&  0&  0& a-\frac{1}{3}c \\  0&  0&  0&  0& a-2b-\frac{1}{3}c+d \\  0&  0&  0&  0&  a-2c+e\\  0&  0&  0&  0&2a-3b-\frac{2}{3}c+f\end{bmatrix} \text{and} \begin{bmatrix}  1&  -2&  1&  0& a\\  0&  0&  -1&  -2& b+a-2c \\  0&  1&  0&  1& c-a \\  0&  0&  0&  0& d-2b \\  0&  0&  0&  0&  4e-2a-6c-7f+21b\\  0&  0&  0&  1&\frac{-1}{4}f-\frac{1}{2}a+\frac{1}{2}c+\frac{3}{4}b    \end{bmatrix}. \forall x=\left ( a,b,c,d,e,f \right ) \in W_1\cap W_2 \begin{align}
a -2b-\frac{1}{3}c+d &=0 \\
a-2c+e &= 0 \\
2a-3b-\frac{2}{3}c +f &=0 \\
4e-2a-6c-7f+21b &=0 \\
d-2b &=0
\end{align} x=\left ( e,f,3e,2f,5e,3f \right )  \left \{ \left  (1,0,3,0,5,0 \right ) ,\space \left (0,1,0,2,0,3  \right )   \right \}  W_1\cap W_2","['linear-algebra', 'dimensional-analysis']"
52,Surprising determinant/trace relation,Surprising determinant/trace relation,,"For any $N$ , we can take $N$ diagonal $N\times N$ matrices $C_i$ which form an orthonormal set: Tr $(C_i,C_j)=\delta_{ij}$ . Now take any diagonal $N\times N$ matrix $D$ , and form the matrix $M_{ij} = Tr(D C_i C_j)$ . To my surprise, I find empirically that the determinant of $M$ is the determinant of $D$ ; in fact, the eigenvalues of $M$ are the eigenvalues of $D$ . For instance: if $N=2$ , then $C_1 = diag\{1,1\}/\sqrt{2}$ and $C_2 = diag\{1,-1\}/\sqrt{2}$ ; then if $D=diag\{a,b\}$ , $$M = \frac{1}{2}\left[\matrix{a+b & a-b \\ a-b & a+b}\right]$$ whose determinant is $ab$ and whose trace is $(a+b)$ , showing it has eigenvalues $a,b$ . I suppose this must follow from some elementary facts about matrices, but so far I can't seem to prove it myself.  Is it obvious? What argument or theorem does it follow from? [Note added: since someone answered as though this were an obvious triviality, let me point out how it works for $N=3$ .  The $C_i$ are proportional to diag $\{1,1,1\}$ , diag $\{1,-1,0\}$ , diag $\{1,1,-2\}$ , properly normalized for orthnormality; in this case if $D$ = diag $\{a,b,c\}$ , the matrix $M$ is $$\left[ \begin{array}{ccc}  \frac{a}{2}+\frac{b}{2} & \frac{a}{2 \sqrt{3}}-\frac{b}{2 \sqrt{3}} & \frac{a}{\sqrt{6}}-\frac{b}{\sqrt{6}} \\  \frac{a}{2 \sqrt{3}}-\frac{b}{2 \sqrt{3}} & \frac{a}{6}+\frac{b}{6}+\frac{2 c}{3} & \frac{a}{3 \sqrt{2}}+\frac{b}{3 \sqrt{2}}-\frac{\sqrt{2} c}{3} \\  \frac{a}{\sqrt{6}}-\frac{b}{\sqrt{6}} & \frac{a}{3 \sqrt{2}}+\frac{b}{3 \sqrt{2}}-\frac{\sqrt{2} c}{3} & \frac{a}{3}+\frac{b}{3}+\frac{c}{3} \\ \end{array} \right] $$ and it is not instantly obvious that the eigenvalues of this matrix are $a,b,c$ .]","For any , we can take diagonal matrices which form an orthonormal set: Tr . Now take any diagonal matrix , and form the matrix . To my surprise, I find empirically that the determinant of is the determinant of ; in fact, the eigenvalues of are the eigenvalues of . For instance: if , then and ; then if , whose determinant is and whose trace is , showing it has eigenvalues . I suppose this must follow from some elementary facts about matrices, but so far I can't seem to prove it myself.  Is it obvious? What argument or theorem does it follow from? [Note added: since someone answered as though this were an obvious triviality, let me point out how it works for .  The are proportional to diag , diag , diag , properly normalized for orthnormality; in this case if = diag , the matrix is and it is not instantly obvious that the eigenvalues of this matrix are .]","N N N\times N C_i (C_i,C_j)=\delta_{ij} N\times N D M_{ij} = Tr(D C_i C_j) M D M D N=2 C_1 = diag\{1,1\}/\sqrt{2} C_2 = diag\{1,-1\}/\sqrt{2} D=diag\{a,b\} M = \frac{1}{2}\left[\matrix{a+b & a-b \\ a-b & a+b}\right] ab (a+b) a,b N=3 C_i \{1,1,1\} \{1,-1,0\} \{1,1,-2\} D \{a,b,c\} M \left[
\begin{array}{ccc}
 \frac{a}{2}+\frac{b}{2} & \frac{a}{2 \sqrt{3}}-\frac{b}{2 \sqrt{3}} & \frac{a}{\sqrt{6}}-\frac{b}{\sqrt{6}} \\
 \frac{a}{2 \sqrt{3}}-\frac{b}{2 \sqrt{3}} & \frac{a}{6}+\frac{b}{6}+\frac{2 c}{3} & \frac{a}{3 \sqrt{2}}+\frac{b}{3 \sqrt{2}}-\frac{\sqrt{2} c}{3} \\
 \frac{a}{\sqrt{6}}-\frac{b}{\sqrt{6}} & \frac{a}{3 \sqrt{2}}+\frac{b}{3 \sqrt{2}}-\frac{\sqrt{2} c}{3} & \frac{a}{3}+\frac{b}{3}+\frac{c}{3} \\
\end{array}
\right]
 a,b,c","['linear-algebra', 'matrices', 'determinant', 'trace']"
53,What's the maximum order of an element in $SL_2(\mathbb{Z} /p\mathbb{Z})$ for $p>2$ prime?,What's the maximum order of an element in  for  prime?,SL_2(\mathbb{Z} /p\mathbb{Z}) p>2,"I know the answer is $2p$ as I've checked it for $p=3,5$ and $71$ . The characteristic polynomial of a matrix $A\in$ $SL_2(\mathbb{Z} /p\mathbb{Z})$ is $P_A(x)=x^2-tr(A)x+1$ , so if this polynomial has a solution, the matrix can be diagonalized. I've studied all of the cases when a matrix is diagonalizable: $A$ has double eigenvalues $1$ or $-1$ of multiplicity $2$ , so it's similar to a matrix $\left(\begin{matrix} \pm 1 & 0 \\ 0 & \pm 1 \end{matrix}\right)$ , which have respectively orders $1$ or $2$ . $A$ two distinct eigenvalues $\omega$ and $\omega ^{-1}$ , with $\omega \in (\mathbb{Z}/p\mathbb{Z})^*$ , so it's similar to $\left(\begin{matrix} \omega & 0 \\ 0 & \omega ^{-1} \end{matrix}\right)$ , with order $p-1$ . $A$ has a double eigenvalue $1$ of multiplicity $1$ , so it's similar to $\left(\begin{matrix}  1 & 1 \\ 0 & 1 \end{matrix}\right)$ , of order $p$ . $A$ has a double eigenvalue $-1$ of multiplicity $1$ , so it's similar to $\left(\begin{matrix}  -1 & 1 \\ 0 & -1 \end{matrix}\right)$ , of order $2p$ . This covers every case except when $P_A(x)$ has no solution. So, I'd just need to prove that, in that case, the order is $\le 2p$ .","I know the answer is as I've checked it for and . The characteristic polynomial of a matrix is , so if this polynomial has a solution, the matrix can be diagonalized. I've studied all of the cases when a matrix is diagonalizable: has double eigenvalues or of multiplicity , so it's similar to a matrix , which have respectively orders or . two distinct eigenvalues and , with , so it's similar to , with order . has a double eigenvalue of multiplicity , so it's similar to , of order . has a double eigenvalue of multiplicity , so it's similar to , of order . This covers every case except when has no solution. So, I'd just need to prove that, in that case, the order is .","2p p=3,5 71 A\in SL_2(\mathbb{Z} /p\mathbb{Z}) P_A(x)=x^2-tr(A)x+1 A 1 -1 2 \left(\begin{matrix} \pm 1 & 0 \\ 0 & \pm 1 \end{matrix}\right) 1 2 A \omega \omega ^{-1} \omega \in (\mathbb{Z}/p\mathbb{Z})^* \left(\begin{matrix} \omega & 0 \\ 0 & \omega ^{-1} \end{matrix}\right) p-1 A 1 1 \left(\begin{matrix}  1 & 1 \\ 0 & 1 \end{matrix}\right) p A -1 1 \left(\begin{matrix}  -1 & 1 \\ 0 & -1 \end{matrix}\right) 2p P_A(x) \le 2p","['linear-algebra', 'group-theory', 'diagonalization', 'general-linear-group', 'modular-group']"
54,Spot it! and geometry over finite fields.,Spot it! and geometry over finite fields.,,"Problem: The party game “Spot It!” features 55 cards, each of which has eight symbols printed on it, in such a way that any two cards have exactly one symbol in common. (In the game, each player looks at a pair of cards and tries to find their common symbol.) The “Junior” version of the game has 30 cards with six symbols each. How do you use geometry over finite fields, as in the parts above, to build decks of cards with the required property? (Note: Spot It decks don’t quite have the optimal number of cards.) My work: This question is the third part of a problem. I already solved the two parts before where I prove the following formulas: (I) The number of $m$ -dimensional subspaces of an $n$ -dimensional vector space over a field with order $q$ is $$\frac{(q^n - 1)(q^n - q)\ldots(q^n - q^{m-1})}{(q^m - 1)(q^m - q)\ldots(q^m - q^{m-1})}$$ (II) The number of two-dimensional spaces that contain a given 1-dimensional subspace is $$\frac{q^n - q}{q^2 - q} = \frac{q^{n-1} - 1}{q - 1}$$ So I am supposed to use those formulas or a similar method to solve the problem. I already read this but none of the answers talks about the approach I am looking for, the most similar thing I was able to find is Example 4 of this answer but still doesn't solve my doubts. I tried thinking of a field $F$ whose order $q$ is the number of symbols, then the set of all cards are all elements of $F^8$ where any two vectors only share a single coordinate or equivalently, a symbol. Since any two cards must share a single symbol or equivalently, a one-dimensional subspace of $F^8$ $$\frac{q^{7} - 1}{q - 1} = 2 \implies q^7 - 2q + 1= 0$$ but then I realised that the argument was wrong and didn't make much sense and that is why the equation yields wrong answers for the number of total symbols. This section of the Wikipedia article about projective planes seems useful but I don't know how to apply it to the problem. I would appreciate any hints on how to get started. Thanks in advance.","Problem: The party game “Spot It!” features 55 cards, each of which has eight symbols printed on it, in such a way that any two cards have exactly one symbol in common. (In the game, each player looks at a pair of cards and tries to find their common symbol.) The “Junior” version of the game has 30 cards with six symbols each. How do you use geometry over finite fields, as in the parts above, to build decks of cards with the required property? (Note: Spot It decks don’t quite have the optimal number of cards.) My work: This question is the third part of a problem. I already solved the two parts before where I prove the following formulas: (I) The number of -dimensional subspaces of an -dimensional vector space over a field with order is (II) The number of two-dimensional spaces that contain a given 1-dimensional subspace is So I am supposed to use those formulas or a similar method to solve the problem. I already read this but none of the answers talks about the approach I am looking for, the most similar thing I was able to find is Example 4 of this answer but still doesn't solve my doubts. I tried thinking of a field whose order is the number of symbols, then the set of all cards are all elements of where any two vectors only share a single coordinate or equivalently, a symbol. Since any two cards must share a single symbol or equivalently, a one-dimensional subspace of but then I realised that the argument was wrong and didn't make much sense and that is why the equation yields wrong answers for the number of total symbols. This section of the Wikipedia article about projective planes seems useful but I don't know how to apply it to the problem. I would appreciate any hints on how to get started. Thanks in advance.",m n q \frac{(q^n - 1)(q^n - q)\ldots(q^n - q^{m-1})}{(q^m - 1)(q^m - q)\ldots(q^m - q^{m-1})} \frac{q^n - q}{q^2 - q} = \frac{q^{n-1} - 1}{q - 1} F q F^8 F^8 \frac{q^{7} - 1}{q - 1} = 2 \implies q^7 - 2q + 1= 0,"['linear-algebra', 'combinatorics', 'card-games']"
55,Minimize $\mathrm{tr}(R^{-1}B^TXB)$ over $R$ subject to $X=A^TXA-A^TXB(R+B^TXB)^{-1}B^TXA$,Minimize  over  subject to,\mathrm{tr}(R^{-1}B^TXB) R X=A^TXA-A^TXB(R+B^TXB)^{-1}B^TXA,"Given $A\in\mathbb{R}^{n\times n}$ and $B\in\mathbb{R}^{n\times 2}$ , I am interested in solving the following problem: \begin{array}{ll} \underset{R \in \mathbb{R}^{2\times 2}}{\text{minimize}} & \mathrm{tr} \left( R^{-1}B^T X B \right)\\ \text{subject to} & X=A^TXA-A^TXB(R+B^TXB)^{-1}B^TXA.\end{array} Here $X$ is unique stabilizing solution to DARE , thus X is positive definite. It is required that $R>0$ (i.e. positive definite) and $B^TXB$ to be full rank. EDIT: For a fixed $A,B,R$ , we can get unique $X$ by solving DARE, for example by using matlab ""idare"" or ""dare"" command. However, here $R$ is not fixed, it is a variable, thus for each $R$ , there is corresponding $X$ . My attempt: I wanted to start with simpler case when we put additional constraints on $R$ . Assume that $R$ is diagonal and positive definite. WLOG we can assume that $R=\mathrm{diag}\{r_1,r_2\}$ , such  that $r_1+r_2=1$ and $1>r_i>0$ for $i=1,2.$ A=[3 0 0 0; 0 2 1 0; 0 0 2 0; 0 0 0 2]; B=rand(4,2); Q=zeros(4,4);     r1=linspace(0.01, 0.99); for i=1:100       R=[r1(i) 0; 0 1-r1(i)];     [X,~,~] = idare(A,B,Q,R);     T(i)=trace(inv(R)*B'*X*B); end  plot(T) It looks like that as we increase $r_1$ from $0$ to $1$ , then $\mathrm{tr} \left( R^{-1}B^T X B \right)$ is continuous, moreover, it is convex. However, I am unable to prove it.","Given and , I am interested in solving the following problem: Here is unique stabilizing solution to DARE , thus X is positive definite. It is required that (i.e. positive definite) and to be full rank. EDIT: For a fixed , we can get unique by solving DARE, for example by using matlab ""idare"" or ""dare"" command. However, here is not fixed, it is a variable, thus for each , there is corresponding . My attempt: I wanted to start with simpler case when we put additional constraints on . Assume that is diagonal and positive definite. WLOG we can assume that , such  that and for A=[3 0 0 0; 0 2 1 0; 0 0 2 0; 0 0 0 2]; B=rand(4,2); Q=zeros(4,4);     r1=linspace(0.01, 0.99); for i=1:100       R=[r1(i) 0; 0 1-r1(i)];     [X,~,~] = idare(A,B,Q,R);     T(i)=trace(inv(R)*B'*X*B); end  plot(T) It looks like that as we increase from to , then is continuous, moreover, it is convex. However, I am unable to prove it.","A\in\mathbb{R}^{n\times n} B\in\mathbb{R}^{n\times 2} \begin{array}{ll} \underset{R \in \mathbb{R}^{2\times 2}}{\text{minimize}} & \mathrm{tr} \left( R^{-1}B^T X B \right)\\ \text{subject to} & X=A^TXA-A^TXB(R+B^TXB)^{-1}B^TXA.\end{array} X R>0 B^TXB A,B,R X R R X R R R=\mathrm{diag}\{r_1,r_2\} r_1+r_2=1 1>r_i>0 i=1,2. r_1 0 1 \mathrm{tr} \left( R^{-1}B^T X B \right)","['linear-algebra', 'optimization', 'convex-optimization', 'trace']"
56,Looking for an alternative and easier way to prove Sylvester's theorem,Looking for an alternative and easier way to prove Sylvester's theorem,,"My professor provided two enunciates of Sylvester's Theorem, the first is the following: Two symmetric n × n matrices B and C are congruent if and only if the diagonal representations for B and C have the same rank, index and signature . He didn't give a proof of this enunciate but he did give a proof of the following version of Sylvester's Theorem: Given a scalar product $\langle \cdot \rangle$ on the euclidean space V, there exists a basis { $v_1,...,v_n$ } such that the matrix of V with respect to this basis is \begin{pmatrix} I_p & 0 & 0 \\ 0 & -I_q & 0 \\ 0 & 0 & 0 \\ \end{pmatrix} However, the proof is a bit ""overwhelming"" and with too many passages, making it quite hard (at least for me) to be able to do it on my own since I get often stuck, so I wondered if any of you knew a more comprehensible way to prove this second enunciate? I tried looking online but this enunciate is apparently very rare to find in books and whatnot...","My professor provided two enunciates of Sylvester's Theorem, the first is the following: Two symmetric n × n matrices B and C are congruent if and only if the diagonal representations for B and C have the same rank, index and signature . He didn't give a proof of this enunciate but he did give a proof of the following version of Sylvester's Theorem: Given a scalar product on the euclidean space V, there exists a basis { } such that the matrix of V with respect to this basis is However, the proof is a bit ""overwhelming"" and with too many passages, making it quite hard (at least for me) to be able to do it on my own since I get often stuck, so I wondered if any of you knew a more comprehensible way to prove this second enunciate? I tried looking online but this enunciate is apparently very rare to find in books and whatnot...","\langle \cdot \rangle v_1,...,v_n \begin{pmatrix}
I_p & 0 & 0 \\
0 & -I_q & 0 \\
0 & 0 & 0 \\
\end{pmatrix}","['linear-algebra', 'matrices', 'inner-products', 'symmetric-matrices']"
57,eigenvalues of adjoint operator of a diagonizable complex matrix,eigenvalues of adjoint operator of a diagonizable complex matrix,,"Let $A \in M_{n}(\mathbb{C})$ with eigenvalues $\lambda_{1}, \ldots, \lambda_{n}$ . Define a linear $\operatorname{map} \operatorname{ad}(A): M_{n}(\mathbb{C}) \longrightarrow M_{n}(\mathbb{C})$ by $\operatorname{ad}(A)(B)=A B-B A$ for $B \in M_{n}(\mathbb{C})$ . Determine the eigenvalues of $\operatorname{ad}(A)$ in terms of $\lambda_{1}, \ldots, \lambda_{n}$ . Here, we assume that $A$ is diagonizable. Previously, I worked on the following question: Let $W$ be the subspace of $M_2(\mathbb R)$ consisting of $2 \times 2$ matrices with trace $0$ . Let $V$ be the space of linear maps from $W$ to $W$ . Define a linear map $T : W \rightarrow V$ by $A \mapsto \operatorname{ad}(A)$ , where $\operatorname{ad}(A)(B) = AB - BA$ for $B \in W$ . Choose bases for $W$ and $V$ and compute the matrix of $T$ with respect to the bases. and solved it like this: First, we introduce a base for $W$ as follows: \begin{equation*} \mathcal{B}_W= \bigg \{ \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix},  \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix},  \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} \bigg \}. \end{equation*} Now, for $V$ we have the following set as a base with respect to $\mathcal{B}_W$ : \begin{equation*} \begin{aligned}     \mathcal{B}_V=\{T_{ij}:= &\text{The $3 \times 3$ matrix that the only nonzero element} \\  &\text{of it is the $ij$-th element, that equals $1$} \mid 1 \leq i, j \leq 3  \} \end{aligned} \end{equation*} Now, to compute the matrix of $T$ with respect to these bases, first note that for any $1 \leq i \leq 3$ , we have $\alpha_i . \alpha _i=0$ . So, without loss of generality, writing the matrix $A$ as $(\alpha_1, \alpha_2, \alpha_3)$ , and $B=(a\alpha_1, b \alpha_2, c\alpha_3)$ , we have: \begin{equation*} \begin{aligned}     T\alpha_1 (a, b, c) &= (2c, 0, -b), \\     T\alpha_2 (a, b, c) &= (0, -2c, a), \\     T\alpha_3 (a, b, c) &= (-2a, 2b, 0). \end{aligned} \end{equation*} Now, writing this in forms of matrices, we'll have: \begin{equation*} \begin{aligned}     T\alpha_1&= \begin{bmatrix}0 & 0 & 2 \\ 0 & 0 & 0 \\ 0 & -1 & 0 \end{bmatrix},\\     T\alpha_2&= \begin{bmatrix}0 & 0 & 0 \\ 0 & 0 & -2 \\ 1 & 0 & 0 \end{bmatrix},\\     T\alpha_3&= \begin{bmatrix}-2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 \end{bmatrix}.\\ \end{aligned} \end{equation*} So, the matrix of $T$ is going to be a $9 \times 3$ matrix as following: \begin{equation*}     \begin{bmatrix}      0 & 0 & -2\\      0 & 0 & 0 \\     2 & 0 & 0 \\     0 & 0 & 0 \\     0 & 0 & 2 \\     0 & -2 & 0 \\     0 & 1 & 0 \\     -1 & 0 & 0 \\     0 & 0 & 0 \\     \end{bmatrix}. \end{equation*} Now, I think to solve the eigenvalues question, I should be able to use the second question I mentioned, however I don't know how and where to start. any help's appreciated.","Let with eigenvalues . Define a linear by for . Determine the eigenvalues of in terms of . Here, we assume that is diagonizable. Previously, I worked on the following question: Let be the subspace of consisting of matrices with trace . Let be the space of linear maps from to . Define a linear map by , where for . Choose bases for and and compute the matrix of with respect to the bases. and solved it like this: First, we introduce a base for as follows: Now, for we have the following set as a base with respect to : Now, to compute the matrix of with respect to these bases, first note that for any , we have . So, without loss of generality, writing the matrix as , and , we have: Now, writing this in forms of matrices, we'll have: So, the matrix of is going to be a matrix as following: Now, I think to solve the eigenvalues question, I should be able to use the second question I mentioned, however I don't know how and where to start. any help's appreciated.","A \in M_{n}(\mathbb{C}) \lambda_{1}, \ldots, \lambda_{n} \operatorname{map} \operatorname{ad}(A): M_{n}(\mathbb{C}) \longrightarrow M_{n}(\mathbb{C}) \operatorname{ad}(A)(B)=A B-B A B \in M_{n}(\mathbb{C}) \operatorname{ad}(A) \lambda_{1}, \ldots, \lambda_{n} A W M_2(\mathbb R) 2 \times 2 0 V W W T : W \rightarrow V A \mapsto \operatorname{ad}(A) \operatorname{ad}(A)(B) = AB - BA B \in W W V T W \begin{equation*}
\mathcal{B}_W= \bigg \{ \begin{pmatrix}
0 & 0 \\ 1 & 0
\end{pmatrix}, 
\begin{pmatrix}
0 & 1 \\ 0 & 0
\end{pmatrix}, 
\begin{pmatrix}
1 & 0 \\ 0 & -1
\end{pmatrix}
\bigg \}.
\end{equation*} V \mathcal{B}_W \begin{equation*}
\begin{aligned}
    \mathcal{B}_V=\{T_{ij}:= &\text{The 3 \times 3 matrix that the only nonzero element} \\  &\text{of it is the ij-th element, that equals 1} \mid 1 \leq i, j \leq 3  \}
\end{aligned}
\end{equation*} T 1 \leq i \leq 3 \alpha_i . \alpha _i=0 A (\alpha_1, \alpha_2, \alpha_3) B=(a\alpha_1, b \alpha_2, c\alpha_3) \begin{equation*}
\begin{aligned}
    T\alpha_1 (a, b, c) &= (2c, 0, -b), \\
    T\alpha_2 (a, b, c) &= (0, -2c, a), \\
    T\alpha_3 (a, b, c) &= (-2a, 2b, 0).
\end{aligned}
\end{equation*} \begin{equation*}
\begin{aligned}
    T\alpha_1&= \begin{bmatrix}0 & 0 & 2 \\ 0 & 0 & 0 \\ 0 & -1 & 0 \end{bmatrix},\\
    T\alpha_2&= \begin{bmatrix}0 & 0 & 0 \\ 0 & 0 & -2 \\ 1 & 0 & 0 \end{bmatrix},\\
    T\alpha_3&= \begin{bmatrix}-2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 \end{bmatrix}.\\
\end{aligned}
\end{equation*} T 9 \times 3 \begin{equation*}
    \begin{bmatrix} 
    0 & 0 & -2\\ 
    0 & 0 & 0 \\
    2 & 0 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 2 \\
    0 & -2 & 0 \\
    0 & 1 & 0 \\
    -1 & 0 & 0 \\
    0 & 0 & 0 \\
    \end{bmatrix}.
\end{equation*}","['linear-algebra', 'abstract-algebra']"
58,"Let $M\in \text{SO}(3,\mathbb{R})$, prove that $\det(M-I_3)=0$.","Let , prove that .","M\in \text{SO}(3,\mathbb{R}) \det(M-I_3)=0","Let $M\in \text{SO}(3,\mathbb{R})$ , prove that $\det(M-I_3)=0$ . My attempt: $$ \begin{align}  \det(M-I_3)&=\det(M-M^TM)\\&=\det((I_3-M^T)M)\\&=\underbrace{\det(M)}_{=1}\det(I_3-M^T) \end{align}  $$ Hence $$ \begin{align}  \det(M-I_3)&=\det(I_3-M^T)\\&=\det((I_3-M)^T)\\&=\det(I_3-M)\\&=\det(-(M-I_3))\\&=\underbrace{(-1)^3}_{=-1}\det(M-I_3), \end{align}  $$ and thus $\det(M-I_3)=0.$ Is this proof correct or did I miss out on something?","Let , prove that . My attempt: Hence and thus Is this proof correct or did I miss out on something?","M\in \text{SO}(3,\mathbb{R}) \det(M-I_3)=0 
\begin{align} 
\det(M-I_3)&=\det(M-M^TM)\\&=\det((I_3-M^T)M)\\&=\underbrace{\det(M)}_{=1}\det(I_3-M^T)
\end{align} 
 
\begin{align} 
\det(M-I_3)&=\det(I_3-M^T)\\&=\det((I_3-M)^T)\\&=\det(I_3-M)\\&=\det(-(M-I_3))\\&=\underbrace{(-1)^3}_{=-1}\det(M-I_3),
\end{align} 
 \det(M-I_3)=0.","['linear-algebra', 'matrices', 'orthogonal-matrices']"
59,"Determinant of ""quasi-circulant"" matrices","Determinant of ""quasi-circulant"" matrices",,"Several weeks ago I asked a question regarding the determinant of a $5 \times 5$ matrix: Determinant of a matrix when its diagonal elements have a certain property Here I would like to consider a more general problem:the non-diagonal element will still share a circulant fashion, whereas some of them can be $0$ instead of all being $-1$ . To be more specific, it follows a pattern that in each row, the first $i^{th}$ elements that follow the diagonal element are $-1$ while the rest are $0$ . For instance, in a $5\times5$ case, things can be $A=\begin{pmatrix} l_1&-1 &-1 &-1&0 \\ 0&l_2 &-1 & -1 & -1\\ -1 & 0 &l_3 & -1 &-1\\ -1 & -1 & 0 & l_4 & -1\\ -1 & -1 &-1 & 0 &l_5 \end{pmatrix}$ or $A=\begin{pmatrix} l_1&-1 &-1 &0 &0 \\ 0&l_2 & -1 & -1 & 0\\ 0 & 0 &l_3 & -1 &-1\\ -1 & 0 & 0 & l_4 & -1\\ -1 & -1 & 0 & 0 &l_5 \end{pmatrix}$ Again, I would like to know if there is any delicate way to show that det $(A)<0$ when one of the $l_i$ equals to $0$ whereas others are positive. Some of my thoughts: Consider $l_1=l_2=...=l_n=l$ ( $n$ is the dimension of the matrix). In this case, $A$ is a circulant matrix, and det( $A$ ) can be demonstrated as a polynomial of $l$ . It suffices to show that $l^n$ is the only term with positive coefficient, yet for me this is something too tedious to show.(with all those roots of unity to deal with) I have also tried the matrix determinant lemma, which is helpful when all the non-diagonal elements are $-1$ but not that helpful when some of the non-diagonal elements are $0$ . Any hints or ideas will be greatly appreciated. Thanks! Sorry for not making this clear in the first place, but this is just a conjecture (though I believe this is correct). A counterexample will also be greatly appreciated!","Several weeks ago I asked a question regarding the determinant of a matrix: Determinant of a matrix when its diagonal elements have a certain property Here I would like to consider a more general problem:the non-diagonal element will still share a circulant fashion, whereas some of them can be instead of all being . To be more specific, it follows a pattern that in each row, the first elements that follow the diagonal element are while the rest are . For instance, in a case, things can be or Again, I would like to know if there is any delicate way to show that det when one of the equals to whereas others are positive. Some of my thoughts: Consider ( is the dimension of the matrix). In this case, is a circulant matrix, and det( ) can be demonstrated as a polynomial of . It suffices to show that is the only term with positive coefficient, yet for me this is something too tedious to show.(with all those roots of unity to deal with) I have also tried the matrix determinant lemma, which is helpful when all the non-diagonal elements are but not that helpful when some of the non-diagonal elements are . Any hints or ideas will be greatly appreciated. Thanks! Sorry for not making this clear in the first place, but this is just a conjecture (though I believe this is correct). A counterexample will also be greatly appreciated!","5 \times 5 0 -1 i^{th} -1 0 5\times5 A=\begin{pmatrix}
l_1&-1 &-1 &-1&0 \\
0&l_2 &-1 & -1 & -1\\
-1 & 0 &l_3 & -1 &-1\\
-1 & -1 & 0 & l_4 & -1\\
-1 & -1 &-1 & 0 &l_5
\end{pmatrix} A=\begin{pmatrix}
l_1&-1 &-1 &0 &0 \\
0&l_2 & -1 & -1 & 0\\
0 & 0 &l_3 & -1 &-1\\
-1 & 0 & 0 & l_4 & -1\\
-1 & -1 & 0 & 0 &l_5
\end{pmatrix} (A)<0 l_i 0 l_1=l_2=...=l_n=l n A A l l^n -1 0","['linear-algebra', 'matrices', 'determinant']"
60,"For $u,v,x \in \mathbb{R}^m$, let $T(u,v)(x)=x - \frac{\langle(u+v),x\rangle}{1+\langle u,v\rangle}(u+v)+2\langle u,x\rangle v$","For , let","u,v,x \in \mathbb{R}^m T(u,v)(x)=x - \frac{\langle(u+v),x\rangle}{1+\langle u,v\rangle}(u+v)+2\langle u,x\rangle v","The function in question appears in lemma $6.3$ of Milnors characteristic classes. Let $\langle \cdot, \cdot\rangle$ denote the dot product on $\mathbb{R}^m$ Let $u,v \in \mathbb{R}^m$ be unit vectors with $u \neq -v$ . Let $T(u,v)$ denote the unique rotation of $\mathbb{R}^m$ which carries vector $u$ to vector $v$ and leaves everything orthogonal to $u$ and $v$ fixed. Alternatively, $T(u,v)$ can be defined by: $$T(u,v)(x)=x - \frac{\langle(u+v),x\rangle}{1+\langle u,v\rangle}(u+v)+2\langle u,x\rangle v$$ Can somebody help me understand why this formula above works? I'm having troubles understanding why this formula gives the desired rotation. Thank you.","The function in question appears in lemma of Milnors characteristic classes. Let denote the dot product on Let be unit vectors with . Let denote the unique rotation of which carries vector to vector and leaves everything orthogonal to and fixed. Alternatively, can be defined by: Can somebody help me understand why this formula above works? I'm having troubles understanding why this formula gives the desired rotation. Thank you.","6.3 \langle \cdot, \cdot\rangle \mathbb{R}^m u,v \in \mathbb{R}^m u \neq -v T(u,v) \mathbb{R}^m u v u v T(u,v) T(u,v)(x)=x - \frac{\langle(u+v),x\rangle}{1+\langle u,v\rangle}(u+v)+2\langle u,x\rangle v",['linear-algebra']
61,Understanding the Basic Mathematics of a Kalman Filter,Understanding the Basic Mathematics of a Kalman Filter,,"In Introduction to Linear Algebra, Gilbert Strang, it says that for a Kalman filter $$ \hat{x}_1=\hat{x}_0+K_1(b_1-A_1\hat{x}_0) $$ where the Kalman gain matrix $K_1=W_1A_1^TV_1^{-1}$ and covariance of errors in $\hat{x}_1$ is $W_1^{-1}=W_0^{-1}+A_1^{T}V_1^{-1}A_1$ . I think I understand what Kalman filter is trying to do, when new data is coming instead of computing the least square solution for the whole data we are trying to make use of the least square solution for old data to find that for the whole data. And I think here we are assuming the data error has a normal distribution and that is why the covariance matrix comes into picture. I also understand weighted least square solution for $Ax=b$ is $\hat{x}=(A^TV^{-1}A)^{-1}A^TV^{-1}b$ , and how we arrive at this expression. But how do we arrive at the term $K_1(b_1-A_1\hat{x}_0)$ ? I am having difficulty following the mathematical form of the equation as it is ? Reference: Page 560,Chapter 12-Linear Algebra in Probability & Statistics, Introdcution to Linear Algebra, Gilbert Strang","In Introduction to Linear Algebra, Gilbert Strang, it says that for a Kalman filter where the Kalman gain matrix and covariance of errors in is . I think I understand what Kalman filter is trying to do, when new data is coming instead of computing the least square solution for the whole data we are trying to make use of the least square solution for old data to find that for the whole data. And I think here we are assuming the data error has a normal distribution and that is why the covariance matrix comes into picture. I also understand weighted least square solution for is , and how we arrive at this expression. But how do we arrive at the term ? I am having difficulty following the mathematical form of the equation as it is ? Reference: Page 560,Chapter 12-Linear Algebra in Probability & Statistics, Introdcution to Linear Algebra, Gilbert Strang","
\hat{x}_1=\hat{x}_0+K_1(b_1-A_1\hat{x}_0)
 K_1=W_1A_1^TV_1^{-1} \hat{x}_1 W_1^{-1}=W_0^{-1}+A_1^{T}V_1^{-1}A_1 Ax=b \hat{x}=(A^TV^{-1}A)^{-1}A^TV^{-1}b K_1(b_1-A_1\hat{x}_0)","['linear-algebra', 'statistics', 'least-squares', 'kalman-filter']"
62,Proof by induction with an nxn-matrix,Proof by induction with an nxn-matrix,,"The matrix A $ \in \mathbb{R}^{n\times n}$ is of the form \begin{equation*} A = \begin{pmatrix} 0 & 1 & 0 & \cdots & 0 \\ \vdots & \ddots & \ddots & \ddots & \vdots \\  &  & \ddots & \ddots & 0  \\   \vdots &  &  & \ddots & 1  \\ 0 & \cdots &  & \cdots & 0  \\  \end{pmatrix} \end{equation*} Now I want to compute $e^{tA}$ and $e^{tA} = \sum_{k=0}^{\infty} \frac{1}{k!}\cdot (tA)^{k}$ . I observed that $A^{2}$ is equal to the matrix A only with de ""diagonal"" of ones moved 1 up. \begin{equation*} A^{2} = \begin{pmatrix} 0 & 0 & 1 & \cdots & 0 \\ \vdots & \ddots & \ddots & \ddots & \vdots \\  &  & \ddots & \ddots & 1  \\   \vdots &  &  & \ddots & 0  \\ 0 & \cdots &  & \cdots & 0  \\  \end{pmatrix} \end{equation*} And $A^{3}$ is equal to the matrix A with the ""diagonal"" of ones moved 2 up. So to compute $e^{tA}$ I want to proof that $A^{n}$ is equal to the matrix A with the ""diagonal"" of ones moved up n-1 times, which results in $A^{n} = 0$ . I have tried to proof it with induction. So claim: If A $ \in \mathbb{R}^{n\times n}$ is of the above form, then $A^{n} = 0$ . For n = 2, the 2 x 2 matrix is equal to: \begin{equation*} A =  \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \end{equation*} And $A^{2} = 0$ . So the claim holds for n = 2. But I don't know how to do the induction step in this case. Other ways to compute $e^{tA}$ with this matrix A are also welcome.","The matrix A is of the form Now I want to compute and . I observed that is equal to the matrix A only with de ""diagonal"" of ones moved 1 up. And is equal to the matrix A with the ""diagonal"" of ones moved 2 up. So to compute I want to proof that is equal to the matrix A with the ""diagonal"" of ones moved up n-1 times, which results in . I have tried to proof it with induction. So claim: If A is of the above form, then . For n = 2, the 2 x 2 matrix is equal to: And . So the claim holds for n = 2. But I don't know how to do the induction step in this case. Other ways to compute with this matrix A are also welcome."," \in \mathbb{R}^{n\times n} \begin{equation*}
A = \begin{pmatrix}
0 & 1 & 0 & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
 &  & \ddots & \ddots & 0  \\  
\vdots &  &  & \ddots & 1  \\
0 & \cdots &  & \cdots & 0  \\ 
\end{pmatrix}
\end{equation*} e^{tA} e^{tA} = \sum_{k=0}^{\infty} \frac{1}{k!}\cdot (tA)^{k} A^{2} \begin{equation*}
A^{2} = \begin{pmatrix}
0 & 0 & 1 & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
 &  & \ddots & \ddots & 1  \\  
\vdots &  &  & \ddots & 0  \\
0 & \cdots &  & \cdots & 0  \\ 
\end{pmatrix}
\end{equation*} A^{3} e^{tA} A^{n} A^{n} = 0  \in \mathbb{R}^{n\times n} A^{n} = 0 \begin{equation*}
A = 
\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
\end{equation*} A^{2} = 0 e^{tA}","['linear-algebra', 'matrices', 'induction']"
63,Numerical Analysis or Numerical Linear Algebra Book with Available Solutions?,Numerical Analysis or Numerical Linear Algebra Book with Available Solutions?,,"I am a beginning graduate student in Mathematics soon and I am planning to self-study Numerical Analysis and Numerical Linear Algebra. I know there are already reference-request questions about this, but I am looking for some more specific books. I am looking for books in the following categories: Books with difficult problems, and solutions in the back (ideal) Books with difficult problems Books with solutions in the back Books with separate solution manuals Problem books (ideally with solutions in the back, or solution manuals) Could you please recommend me some textbooks, and tell me in which category they are? Thank you very much, any recommendations are immensely appreciated!","I am a beginning graduate student in Mathematics soon and I am planning to self-study Numerical Analysis and Numerical Linear Algebra. I know there are already reference-request questions about this, but I am looking for some more specific books. I am looking for books in the following categories: Books with difficult problems, and solutions in the back (ideal) Books with difficult problems Books with solutions in the back Books with separate solution manuals Problem books (ideally with solutions in the back, or solution manuals) Could you please recommend me some textbooks, and tell me in which category they are? Thank you very much, any recommendations are immensely appreciated!",,"['linear-algebra', 'optimization', 'reference-request', 'numerical-methods', 'numerical-linear-algebra']"
64,Why do simplex codes have constant weight?,Why do simplex codes have constant weight?,,"I was reading a proof in Algorithms and Computation in Mathematics, Vol 18 by Anton Betton, where in Chapter 2. Bounds and Modifications, page 84, Theorem 2.1.7 , they've explained why in a simplex code, every word has constant weight $q^{m-1}$ . Consider the matrix ∆ from the proof of 2.1.6. This time, regard ∆ as a generator matrix. ... For this, we consider the encoding map $v \rightarrow \Delta.v$ . Write $$\Delta = (u^{(0)^T}| ... | u^{(n−1)^T})$$ with $u^{(i)} \in \mathbb{F}^m_q$ . Then, using the standard bilinear form, we have for $v \in \mathbb{F}^m_q$ , $v·\Delta= (〈v,u^{(0)}〉,...,〈v,u^{(n−1)}〉)$ . Fix an element $v \in \mathbb{F}^m_q \setminus \{0\}$ . The mapping $u \mapsto〈v,u$ 〉for $u \in \mathbb{F}^m_q$ is a surjective linear form, as already pointed out in the proof of 1.6.8. It takes on each value of $\mathbb{F}_q$ exactly $q^{m−1}$ times. Thus, for exactly $q^{m−1}(q−1)$ vectors $u\in \mathbb{F}^m_q$ the value of $〈v,u〉$ is nonzero. By linearity, we have $\langle v, \lambda u \rangle = \lambda \langle v,u \rangle$ for all $\lambda \in \mathbb{F}_q$ . In particular, the value of $\langle v,u \rangle$ is either always zero or always nonzero for elements of the form $w= \lambda u$ , where $\lambda \in \mathbb{F}^*_q$ . This means that the fact that $\langle v,u \rangle$ is zero or nonzero only depends on the one-dimensional subspace containing $u \neq 0$ . Now recall that the $u^{(i)}$ form a transversal of the one-dimensional subspaces (disregarding the zero vector, which is in every subspace). This means that the products $\lambda\cdot u^{(i)}$ where $\lambda \in \mathbb{F}^*_q$ and $0≤i< \frac{q^m−1}{q−1}$ take on every nonzero vector $u \in \mathbb{F}^m_q$ exactly once. The previous remark implies that the $q^{m-1}(q−1)$ vectors $u \in \mathbb{F}^m_q$ with $\langle v,u \rangle =0$ ( $u=0$ is not one of them!)  are contained in exactly $q^{m-1}$ one-dimensional subspaces. For clarity, they have defined the matrix $\Delta$ to be the following: Let $\Delta$ be any matrix whose columns form a system of nonzero representatives of the one-dimensional sub-spaces of $\mathbb{F}^m_q$ . The dual code of a Hamming-code, i.e.the code which is generated by the matrix $\Delta$ is called an $m-th$ order q-ary simplex-code I understand that why for every $q^{m-1}(q-1)$ vectors $u \in \mathbb{F}^m_q$ , $\langle v,u \rangle$ is nonzero, and that it being zero or non zero only depends on what $u$ is. However, I can't understand why $\lambda\cdot u^{(i)}$ takes every non zero vector $u \in \mathbb{F}^m_q$ exactly once. If someone could help me, I'd really be grateful!","I was reading a proof in Algorithms and Computation in Mathematics, Vol 18 by Anton Betton, where in Chapter 2. Bounds and Modifications, page 84, Theorem 2.1.7 , they've explained why in a simplex code, every word has constant weight . Consider the matrix ∆ from the proof of 2.1.6. This time, regard ∆ as a generator matrix. ... For this, we consider the encoding map . Write with . Then, using the standard bilinear form, we have for , . Fix an element . The mapping 〉for is a surjective linear form, as already pointed out in the proof of 1.6.8. It takes on each value of exactly times. Thus, for exactly vectors the value of is nonzero. By linearity, we have for all . In particular, the value of is either always zero or always nonzero for elements of the form , where . This means that the fact that is zero or nonzero only depends on the one-dimensional subspace containing . Now recall that the form a transversal of the one-dimensional subspaces (disregarding the zero vector, which is in every subspace). This means that the products where and take on every nonzero vector exactly once. The previous remark implies that the vectors with ( is not one of them!)  are contained in exactly one-dimensional subspaces. For clarity, they have defined the matrix to be the following: Let be any matrix whose columns form a system of nonzero representatives of the one-dimensional sub-spaces of . The dual code of a Hamming-code, i.e.the code which is generated by the matrix is called an order q-ary simplex-code I understand that why for every vectors , is nonzero, and that it being zero or non zero only depends on what is. However, I can't understand why takes every non zero vector exactly once. If someone could help me, I'd really be grateful!","q^{m-1} v \rightarrow \Delta.v \Delta = (u^{(0)^T}| ... | u^{(n−1)^T}) u^{(i)} \in \mathbb{F}^m_q v \in \mathbb{F}^m_q v·\Delta= (〈v,u^{(0)}〉,...,〈v,u^{(n−1)}〉) v \in \mathbb{F}^m_q \setminus \{0\} u \mapsto〈v,u u \in \mathbb{F}^m_q \mathbb{F}_q q^{m−1} q^{m−1}(q−1) u\in \mathbb{F}^m_q 〈v,u〉 \langle v, \lambda u \rangle = \lambda \langle v,u \rangle \lambda \in \mathbb{F}_q \langle v,u \rangle w= \lambda u \lambda \in \mathbb{F}^*_q \langle v,u \rangle u \neq 0 u^{(i)} \lambda\cdot u^{(i)} \lambda \in \mathbb{F}^*_q 0≤i< \frac{q^m−1}{q−1} u \in \mathbb{F}^m_q q^{m-1}(q−1) u \in \mathbb{F}^m_q \langle v,u \rangle =0 u=0 q^{m-1} \Delta \Delta \mathbb{F}^m_q \Delta m-th q^{m-1}(q-1) u \in \mathbb{F}^m_q \langle v,u \rangle u \lambda\cdot u^{(i)} u \in \mathbb{F}^m_q","['linear-algebra', 'discrete-mathematics', 'finite-fields', 'coding-theory']"
65,"Distance between two lines $L_1:\> x+y+z=6,\> x-2z=-5$ and $L_2:\> x+2y=3,\> y+2z=3 $",Distance between two lines  and,"L_1:\> x+y+z=6,\> x-2z=-5 L_2:\> x+2y=3,\> y+2z=3 ","Find the distance between the two lines defined by : $$\mathbb L_{1}=   \begin{cases}                                    x+y+z=6 & \\                                    x-2z=-5 &  \\    \end{cases}$$ $$\mathbb L_{2}=   \begin{cases}                                    x+2y=3 & \\                                    y+2z=3 &  \\    \end{cases}$$ I know that if we have two lines : $\mathbb L_{1}=P_1+tv_1$ and $\mathbb L_{2}=P_2+tv_2$ ,then the distance is given by : $$d(\mathbb L_{1},\mathbb L_{2})=\frac{\left|\left(P_{2}-P_{1}\right)\cdot\left(v_{1}\times v_{2}\right)\right|}{\left|v_{1}\times v_{2}\right|}$$ But the problem is that thegiven equations are not in the mentioned form,and I 'm not sure even if they are line (the equations seems to be plane). So how to start?","Find the distance between the two lines defined by : I know that if we have two lines : and ,then the distance is given by : But the problem is that thegiven equations are not in the mentioned form,and I 'm not sure even if they are line (the equations seems to be plane). So how to start?","\mathbb L_{1}=
  \begin{cases}
                                   x+y+z=6 & \\
                                   x-2z=-5 &  \\ 
  \end{cases} \mathbb L_{2}=
  \begin{cases}
                                   x+2y=3 & \\
                                   y+2z=3 &  \\ 
  \end{cases} \mathbb L_{1}=P_1+tv_1 \mathbb L_{2}=P_2+tv_2 d(\mathbb L_{1},\mathbb L_{2})=\frac{\left|\left(P_{2}-P_{1}\right)\cdot\left(v_{1}\times v_{2}\right)\right|}{\left|v_{1}\times v_{2}\right|}","['linear-algebra', 'vectors']"
66,"Proving:$\operatorname{Proj}_{U^\perp}(x)=-\frac1{\det(A^TA)} X(u_1,\ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x))$",Proving:,"\operatorname{Proj}_{U^\perp}(x)=-\frac1{\det(A^TA)} X(u_1,\ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x))","The problem I'm trying to solve is as follows, which was posed to me by my professor as an exercise: Let $x, u_i \in \Bbb R^n$ , $ A = (u_1, u_2, \ldots, u_{n-2})$ and $\{u_1, u_2, \ldots, u_{n-2}\}$ is linearly independent. Let $U = \text{Col}(A)$ . Then, show $\operatorname{Proj}_{U^\perp}(x) =- \frac1{\det(A^{T}A)} X(u_1,\ldots, u_{n-2}, X(u_1, \ldots, u_{n-2}, x))$ . Here is my proof so far: We want to show that $$x - \operatorname{Proj}_U(x) = -\frac1{\det(A^TA)} X(u_1,\ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x)).$$ Equivalently, $\begin{aligned}x - A(A^TA)^{-1}A^Tx &= -\frac1{\det(A^{T}A)} X(u_1, \ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x)) \\\iff A\text{ adj}(A^TA)A^Tx - x\det(A^TA) &= X(u_1,\ldots, u_{n-2}, X(u_1, \ldots, u_{n-2}, x)).\end{aligned}$ Now, I've used a fact proven in class that $$\begin{aligned}&\ X(u_1, \ldots,u_{n-2}, X(u_1,\ldots, u_{n-2}, x))\\&=\left(\sum\limits_{i=1}^{n-2} (-1)^{n+i} \det((B^TA)^{(i)})u_i\right) - \det((B^TA)^{(n-1)})x\end{aligned}$$ where $B = (u_1, u_2, \ldots, u_{n-2}, x)$ and $(B^TA)^{(i)}$ is obtained by removing the $i-\text{th}$ row of $B^TA$ . Observing that $(B^TA)^{(n-1)} = A^TA$ , we can rewrite the goal, so now we need to show that $$A\text{ adj}(A^TA)A^Tx = \left(\sum\limits_{i=1}^{n-2} (-1)^{n+i}\det((B^TA)^{(i)})u_i\right).$$ This is about where I am out of ideas on how to proceed. I think I am onto something, but I am not sure how to prove this last goal. Any observations, hints, or solutions would be very much appreciated!","The problem I'm trying to solve is as follows, which was posed to me by my professor as an exercise: Let , and is linearly independent. Let . Then, show . Here is my proof so far: We want to show that Equivalently, Now, I've used a fact proven in class that where and is obtained by removing the row of . Observing that , we can rewrite the goal, so now we need to show that This is about where I am out of ideas on how to proceed. I think I am onto something, but I am not sure how to prove this last goal. Any observations, hints, or solutions would be very much appreciated!","x, u_i \in \Bbb R^n  A = (u_1, u_2, \ldots, u_{n-2}) \{u_1, u_2, \ldots, u_{n-2}\} U = \text{Col}(A) \operatorname{Proj}_{U^\perp}(x) =- \frac1{\det(A^{T}A)} X(u_1,\ldots, u_{n-2}, X(u_1, \ldots, u_{n-2}, x)) x - \operatorname{Proj}_U(x) = -\frac1{\det(A^TA)} X(u_1,\ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x)). \begin{aligned}x - A(A^TA)^{-1}A^Tx &= -\frac1{\det(A^{T}A)} X(u_1, \ldots, u_{n-2}, X(u_1,\ldots, u_{n-2}, x)) \\\iff A\text{ adj}(A^TA)A^Tx - x\det(A^TA) &= X(u_1,\ldots, u_{n-2}, X(u_1, \ldots, u_{n-2}, x)).\end{aligned} \begin{aligned}&\ X(u_1, \ldots,u_{n-2}, X(u_1,\ldots, u_{n-2}, x))\\&=\left(\sum\limits_{i=1}^{n-2} (-1)^{n+i} \det((B^TA)^{(i)})u_i\right) - \det((B^TA)^{(n-1)})x\end{aligned} B = (u_1, u_2, \ldots, u_{n-2}, x) (B^TA)^{(i)} i-\text{th} B^TA (B^TA)^{(n-1)} = A^TA A\text{ adj}(A^TA)A^Tx = \left(\sum\limits_{i=1}^{n-2} (-1)^{n+i}\det((B^TA)^{(i)})u_i\right).","['linear-algebra', 'determinant', 'transpose', 'laplace-expansion']"
67,Indecomposable elements in a lattice,Indecomposable elements in a lattice,,"Let $L$ be an discrete lattice in $\mathbb R^n$ . We say that a nonzero $a\in L$ is indecomposable if and only if $a$ cannot be written as $a=b+c$ with $b,c$ nonzero and $b^T c>0$ . I was initially trying to prove that the indecomposable elements generate the Voronoi cell (also called Dirichlet domain) $V=\{x\in\mathbb R^n:|x|<|x-v| \mbox{ for all } 0\ne v\in L\}$ , in the sense that if we define $H_v=\{x\in\mathbb R^n:|x|<|x-v|\}$ then $V=\cap H_a$ where the intersection runs over the indecomposable elements. Now, I have managed to show the above by establishing that $u^Tv\ge 0$ implies $H_u\cap H_v\subset H_{u+v}$ . Further I wish to show that this intersection is minimal in the sense that we cannot remove any indecomposable element and still get $V(0)$ . Also it is the unique minimal such set. How is that possible? What I am possibly thinking of is to prove that if $a,b$ are both indecomposable and distinct then we cannot have $H_a\subset H_b$ . But how to prove that? I am not getting an intuition of what is an indecomposable vector. Update: I think the last two paragraphs on Pg 57 of these notes contain the answer. But I am unable to understand them almost entirely. Can someone explain?","Let be an discrete lattice in . We say that a nonzero is indecomposable if and only if cannot be written as with nonzero and . I was initially trying to prove that the indecomposable elements generate the Voronoi cell (also called Dirichlet domain) , in the sense that if we define then where the intersection runs over the indecomposable elements. Now, I have managed to show the above by establishing that implies . Further I wish to show that this intersection is minimal in the sense that we cannot remove any indecomposable element and still get . Also it is the unique minimal such set. How is that possible? What I am possibly thinking of is to prove that if are both indecomposable and distinct then we cannot have . But how to prove that? I am not getting an intuition of what is an indecomposable vector. Update: I think the last two paragraphs on Pg 57 of these notes contain the answer. But I am unable to understand them almost entirely. Can someone explain?","L \mathbb R^n a\in L a a=b+c b,c b^T c>0 V=\{x\in\mathbb R^n:|x|<|x-v| \mbox{ for all } 0\ne v\in L\} H_v=\{x\in\mathbb R^n:|x|<|x-v|\} V=\cap H_a u^Tv\ge 0 H_u\cap H_v\subset H_{u+v} V(0) a,b H_a\subset H_b",['linear-algebra']
68,How should I restrict the points considered in each hexagonal lattice to correctly count all unique near-coincident lattices?,How should I restrict the points considered in each hexagonal lattice to correctly count all unique near-coincident lattices?,,"Background: If a hexagonal lattice is defined by integers $i, j$ where $x = a_1 \left(i + \frac{1}{2} j\right)$ and $y = a_1 \frac{\sqrt{3}}{2} j$ , the distance to the origin for each point $r(a_1, i, j)$ will be $a_1 \sqrt{i^2 + j^2 + ij}$ . See this answer to my earlier question. If I have a second hexagonal lattice with constant $a_2$ it will form a coincident if there's some supercell of one that matches a supercell of the other, and since both are periodic it's sufficient to show that these lengths are equal: $$a_1^2 (i^2 + j^2 + ij) = a_2^2(k^2 + l^2 + kl)$$ The example in the plot below is for $(i, j), (k, l) = (5, 4), (2, 3)$ and $a_1=1$ , which makes $a_2 = \sqrt{\frac{61}{19}}$ To visualize the coincidence it's necessary to rotate the second lattice by $$\theta = \text{arctan2} \left(\frac{\sqrt{3}}{2}j, \ \ i+\frac{1}{2}j \right) - \text{arctan2} \left(\frac{\sqrt{3}}{2}l, \ \ k+\frac{1}{2}l \right)$$ or about -10.26°. We can know by symmetry that the negative of this angle or +10.26° will generate a second coincident lattice, and in fact every $\theta$ such that $\mod(\theta, \ 30°) \ne 0$ will have a complementary lattice at $-\theta$ . At integer multiples of 30 degrees the pair will be degenerate and we'll count it as only a single coincident lattice. Question: I am writing an algorithm to find near-coincident lattices, where the lengths differ by some small fraction $\delta$ , perhaps 1 percent: $$\left|\frac{a_1^2 (i^2 + j^2 + ij) }{ a_2^2(k^2 + l^2 + kl)} - 1\right| <= \delta$$ I want to count the number of unique near-coincident configurations. The algorithm will be used in a python script. My problem is that I don't want to miss any near-coincidences and at the same time don't want to double-count. Question: How should I restrict the points considered in each hexagonal lattice to correctly count all unique near-coincident lattices? I know I should restrict my search to a pie-shaped segment of all points in each of the two lattices, perhaps a 30° slice of one against a 60° slice of the other, but I haven't been able to convince myself that this guess is mathematically sound. Example of a proper coincident lattice: $(i, j), (k, l) = (5, 4), (2, 3)$ with $\frac{a_2}{a_1} = \sqrt{\frac{61}{19}}$","Background: If a hexagonal lattice is defined by integers where and , the distance to the origin for each point will be . See this answer to my earlier question. If I have a second hexagonal lattice with constant it will form a coincident if there's some supercell of one that matches a supercell of the other, and since both are periodic it's sufficient to show that these lengths are equal: The example in the plot below is for and , which makes To visualize the coincidence it's necessary to rotate the second lattice by or about -10.26°. We can know by symmetry that the negative of this angle or +10.26° will generate a second coincident lattice, and in fact every such that will have a complementary lattice at . At integer multiples of 30 degrees the pair will be degenerate and we'll count it as only a single coincident lattice. Question: I am writing an algorithm to find near-coincident lattices, where the lengths differ by some small fraction , perhaps 1 percent: I want to count the number of unique near-coincident configurations. The algorithm will be used in a python script. My problem is that I don't want to miss any near-coincidences and at the same time don't want to double-count. Question: How should I restrict the points considered in each hexagonal lattice to correctly count all unique near-coincident lattices? I know I should restrict my search to a pie-shaped segment of all points in each of the two lattices, perhaps a 30° slice of one against a 60° slice of the other, but I haven't been able to convince myself that this guess is mathematically sound. Example of a proper coincident lattice: with","i, j x = a_1 \left(i + \frac{1}{2} j\right) y = a_1 \frac{\sqrt{3}}{2} j r(a_1, i, j) a_1 \sqrt{i^2 + j^2 + ij} a_2 a_1^2 (i^2 + j^2 + ij) = a_2^2(k^2 + l^2 + kl) (i, j), (k, l) = (5, 4), (2, 3) a_1=1 a_2 = \sqrt{\frac{61}{19}} \theta = \text{arctan2} \left(\frac{\sqrt{3}}{2}j, \ \ i+\frac{1}{2}j \right) - \text{arctan2} \left(\frac{\sqrt{3}}{2}l, \ \ k+\frac{1}{2}l \right) \theta \mod(\theta, \ 30°) \ne 0 -\theta \delta \left|\frac{a_1^2 (i^2 + j^2 + ij) }{ a_2^2(k^2 + l^2 + kl)} - 1\right| <= \delta (i, j), (k, l) = (5, 4), (2, 3) \frac{a_2}{a_1} = \sqrt{\frac{61}{19}}","['linear-algebra', 'geometry']"
69,Why does an orthogonal matrix have to be square?,Why does an orthogonal matrix have to be square?,,"I understand intuitively why this has to be the case (otherwise you could lose a dimension / gain a dimension which changes the length), but what is the formal proof that an orthogonal matrix has to be square?","I understand intuitively why this has to be the case (otherwise you could lose a dimension / gain a dimension which changes the length), but what is the formal proof that an orthogonal matrix has to be square?",,"['linear-algebra', 'orthogonality', 'orthogonal-matrices']"
70,Generalization of scalar product to multiple vectors,Generalization of scalar product to multiple vectors,,"Let's look at the scalar product of two vectors $\mathbf{a}$ and $\mathbf{b}$ : $$ \mathbf{a} \cdot \mathbf{b} = \sum_ka_k \cdot b_k $$ I'm working my way through some physics problems where I have terms of the following form popping up: $$ \sum_k a_k \cdot b_k \cdot c_k \cdot \ldots $$ This looks a little like a ""generalization of the scalar product"" to an arbitrary numbers of vectors to me. At the moment I'm making up my own notation by borrowing the "" $\circ$ "" symbol from the Hadamard product to write $$ \sum_ka_k \cdot b_k \cdot c_k \cdot \ldots = [ \mathbf{a} \circ \mathbf{b} \circ \mathbf{c} \circ \ldots ] $$ where I'm using square brackets to imply summation over all elements. Since I also have to deal with powers of the elements in this sum, I mimick Hadamard powers that I've seen written with the same symbol: $$ \sum_k a^\alpha_k \cdot b^\beta_k \cdot c^\gamma_k \cdot \ldots = [ \mathbf{a}^{\circ\alpha} \circ \mathbf{b}^{\circ\beta} \circ \mathbf{c}^{\circ\gamma} \circ \ldots ] $$ However I would like to know if there is already an accepted way to represent a sum like the one above so I wouldn't have to reinvent the wheel. In addition, so far I have only seen the Hadamard product defined for matrices, not vectors, so it's my working assumption that this would be acceptable use for the symbol as well. (I guess I could write this as a series of multiplications of diagonal matrices, however it feels a bit like overkill to introduce matrices if I know that none of my terms will ever have two indices to them. That's more personal taste though.)","Let's look at the scalar product of two vectors and : I'm working my way through some physics problems where I have terms of the following form popping up: This looks a little like a ""generalization of the scalar product"" to an arbitrary numbers of vectors to me. At the moment I'm making up my own notation by borrowing the "" "" symbol from the Hadamard product to write where I'm using square brackets to imply summation over all elements. Since I also have to deal with powers of the elements in this sum, I mimick Hadamard powers that I've seen written with the same symbol: However I would like to know if there is already an accepted way to represent a sum like the one above so I wouldn't have to reinvent the wheel. In addition, so far I have only seen the Hadamard product defined for matrices, not vectors, so it's my working assumption that this would be acceptable use for the symbol as well. (I guess I could write this as a series of multiplications of diagonal matrices, however it feels a bit like overkill to introduce matrices if I know that none of my terms will ever have two indices to them. That's more personal taste though.)","\mathbf{a} \mathbf{b} 
\mathbf{a} \cdot \mathbf{b} = \sum_ka_k \cdot b_k
 
\sum_k a_k \cdot b_k \cdot c_k \cdot \ldots
 \circ 
\sum_ka_k \cdot b_k \cdot c_k \cdot \ldots = [ \mathbf{a} \circ \mathbf{b} \circ \mathbf{c} \circ \ldots ]
 
\sum_k a^\alpha_k \cdot b^\beta_k \cdot c^\gamma_k \cdot \ldots = [ \mathbf{a}^{\circ\alpha} \circ \mathbf{b}^{\circ\beta} \circ \mathbf{c}^{\circ\gamma} \circ \ldots ]
","['linear-algebra', 'products']"
71,"Existence of a decomposition of an arbitrary rotation into three rotations about the $x,y,z$ axis respectively.",Existence of a decomposition of an arbitrary rotation into three rotations about the  axis respectively.,"x,y,z","In reading about Euler Angles from various sources on the internet, it seems the treatment of this subject usually assumes that for an arbitrary rotation $3 \times 3$ rotation matrix $R$ with real entries, that there exists various decompositions of $R=ABC$ where $A,B,C$ are rotations of three angles respective to three co-ordinate axes, and then proceeds to show how to find the angles. Examples include, for three angles in radians, say $\psi, \theta, \phi$ , a decomposition $R=R_x(\psi)R_y(\theta)R_z(\phi)$ , i.e. rotations around the $x,y,z$ axis respectively. Wikipedia also includes in their description here , Proper Euler Angles a decomposition using these rotation axis': Proper Euler angles (z-x-z, x-y-x, y-z-y, z-y-z, x-z-x, y-x-y), where the first decomposition reuses the z axis. In the case where we wish to express $R=R_z(\psi)R_y(\theta)R_x(\phi)$ , I am trying to write an existence proof for such a decomposition. If we assume this is true, then we can solve $\small R = \begin{bmatrix} R_{11} & R_{12} & R_{13} \\ R_{21} & R_{22} & R_{33}\\R_{31} & R_{32} & R_{33}\end{bmatrix} = \begin{bmatrix} \cos \psi & -\sin \psi & 0\\ \sin \psi & \cos \psi & 0 \\ 0 & 0 & 1  \end{bmatrix} \begin{bmatrix} \cos \theta & 0 & \sin \theta \\ 0 & 1 & 0 \\ -\sin \theta & 0 & \cos \theta\\ \end{bmatrix} \cdot \begin{bmatrix}1 & 0 & 0 \\ 0 & \cos \phi & -\sin \phi \\ 0 &  \sin \phi & \cos \phi \end{bmatrix}$ Giving $R = \begin{bmatrix} \cos \theta \cos \phi & \sin \psi \sin \theta \cos \phi - \cos \psi \sin\phi & \cos \phi \sin \theta \cos \phi + \sin \psi \sin \phi\\ \cos\theta \sin\phi & \sin \psi \sin\theta \sin \phi + \cos \psi \cos \phi & \cos \psi \sin \theta \sin \phi - \sin \psi\cos \phi & \\-\sin \theta & \sin \psi \cos \theta & \cos \psi \cos \theta \end{bmatrix}$ . Then assuming existence, we can solve for each angle, where for example $\theta = - \sin ^{-1}(R_{31})$ . I am not sure why such a decomposition exists, a priori. Any insights appreciated.","In reading about Euler Angles from various sources on the internet, it seems the treatment of this subject usually assumes that for an arbitrary rotation rotation matrix with real entries, that there exists various decompositions of where are rotations of three angles respective to three co-ordinate axes, and then proceeds to show how to find the angles. Examples include, for three angles in radians, say , a decomposition , i.e. rotations around the axis respectively. Wikipedia also includes in their description here , Proper Euler Angles a decomposition using these rotation axis': Proper Euler angles (z-x-z, x-y-x, y-z-y, z-y-z, x-z-x, y-x-y), where the first decomposition reuses the z axis. In the case where we wish to express , I am trying to write an existence proof for such a decomposition. If we assume this is true, then we can solve Giving . Then assuming existence, we can solve for each angle, where for example . I am not sure why such a decomposition exists, a priori. Any insights appreciated.","3 \times 3 R R=ABC A,B,C \psi, \theta, \phi R=R_x(\psi)R_y(\theta)R_z(\phi) x,y,z R=R_z(\psi)R_y(\theta)R_x(\phi) \small R = \begin{bmatrix} R_{11} & R_{12} & R_{13} \\ R_{21} & R_{22} & R_{33}\\R_{31} & R_{32} & R_{33}\end{bmatrix} = \begin{bmatrix} \cos \psi & -\sin \psi & 0\\ \sin \psi & \cos \psi & 0 \\ 0 & 0 & 1  \end{bmatrix} \begin{bmatrix} \cos \theta & 0 & \sin \theta \\ 0 & 1 & 0 \\ -\sin \theta & 0 & \cos \theta\\ \end{bmatrix} \cdot \begin{bmatrix}1 & 0 & 0 \\ 0 & \cos \phi & -\sin \phi \\ 0 &  \sin \phi & \cos \phi \end{bmatrix} R = \begin{bmatrix} \cos \theta \cos \phi & \sin \psi \sin \theta \cos \phi - \cos \psi \sin\phi & \cos \phi \sin \theta \cos \phi + \sin \psi \sin \phi\\ \cos\theta \sin\phi & \sin \psi \sin\theta \sin \phi + \cos \psi \cos \phi & \cos \psi \sin \theta \sin \phi - \sin \psi\cos \phi & \\-\sin \theta & \sin \psi \cos \theta & \cos \psi \cos \theta \end{bmatrix} \theta = - \sin ^{-1}(R_{31})","['linear-algebra', 'geometry']"
72,"Proof of ""$\| I-A\|<1$ implies $A$ is invertible"" using contraction mapping","Proof of "" implies  is invertible"" using contraction mapping",\| I-A\|<1 A,"For a linear operator $A:\mathbb{R}^n\rightarrow \mathbb{R}^n$ consider its norm defined by $$\| A\| = \sup\{ |Ax| \,\,|\,\, x\in\mathbb{R}^n, \,\, \|x\|\le 1\}.$$ From the definition it is clear that $\| Ax\| \le \|A\| \|x\|$ for all $x\in\mathbb{R}^n$ . With this notation, we come to the problem. Theorem: If $A$ is a linear operator with $\| I-A\|<1$ then $A$ is invertible. Proof: Let $\| I-A\| = c<1$ . We can assume that $A\neq I$ (o.w. we are done), hence $c>0$ . Let us denote the operator $I-A$ by $T$ . Notice that if $\|x\| \le 1$ then $\| Tx\| \le \| T\| \|x\| <1$ . Hence, $T$ takes closed unit ball of $\mathbb{R}^n$ into itself. Moreover, $\|Tx-Ty\|  \le \|T\| \| x-y\| \le c\| x-y\|$ where $0<c<1$ . This means that $T$ is a contraction mapping on closed unit ball; it must have unique fixed point. But $0$ is already a fixed point of $T$ ; thus for $x\neq 0$ we have $T(x)\neq x$ i.e. $(I-A)(x)\neq x$ i.e. $A(x)\neq 0$ , i.e. $A$ is injective on closed unit ball (and hence on $\mathbb{R}^n$ ). This forces that $A$ must be invertible. Q. Is this proof correct? (I tried to use property of contraction of mapping, which we got from hypothesis).","For a linear operator consider its norm defined by From the definition it is clear that for all . With this notation, we come to the problem. Theorem: If is a linear operator with then is invertible. Proof: Let . We can assume that (o.w. we are done), hence . Let us denote the operator by . Notice that if then . Hence, takes closed unit ball of into itself. Moreover, where . This means that is a contraction mapping on closed unit ball; it must have unique fixed point. But is already a fixed point of ; thus for we have i.e. i.e. , i.e. is injective on closed unit ball (and hence on ). This forces that must be invertible. Q. Is this proof correct? (I tried to use property of contraction of mapping, which we got from hypothesis).","A:\mathbb{R}^n\rightarrow \mathbb{R}^n \| A\| = \sup\{ |Ax| \,\,|\,\, x\in\mathbb{R}^n, \,\, \|x\|\le 1\}. \| Ax\| \le \|A\| \|x\| x\in\mathbb{R}^n A \| I-A\|<1 A \| I-A\| = c<1 A\neq I c>0 I-A T \|x\| \le 1 \| Tx\| \le \| T\| \|x\| <1 T \mathbb{R}^n \|Tx-Ty\|  \le \|T\| \| x-y\| \le c\| x-y\| 0<c<1 T 0 T x\neq 0 T(x)\neq x (I-A)(x)\neq x A(x)\neq 0 A \mathbb{R}^n A","['linear-algebra', 'multivariable-calculus']"
73,Representing the determinant of a Hermitian matrix as an integral,Representing the determinant of a Hermitian matrix as an integral,,"Let $M=\left (\omega\mathbb{I}-A\right )\left(\omega^{*}\mathbb{I}-A^{\dagger}\right)$ be a Hermitian matrix of size $n\times n$ where $A$ is a real non symmetric matrix and $\omega=a+\mathrm{i}b$ . $A^{\dagger}$ represents the  conjugate transpose of $A$ . I want to compute $\det[M]^{-\frac{1}{2}}$ . I know that for a real symmetric matrix $\Sigma$ we can represent its determinant as a gaussian integral with real variables $x_i$ : $$ \frac{1}{|\Sigma|^{1 / 2}}=\int \frac{1}{(2 \pi)^{n / 2}} \exp \left(-\frac{1}{2}\mathbf{x}^{T} \Sigma\mathbf{x}\right)\mathrm{d}\mathbf{x}.$$ However in my case $M$ has complex values. I was wondering if we could extend this integral representation to Hermitian matrices. Among the feedback I got, these are the candidates: \begin{equation}     \det[M]^{-\frac{1}{2}}=\int \left ( \prod_{i} \frac{\mathrm{d} x_i}{\sqrt{2 \pi / i}}\right ) \exp \left\{-\frac{\mathrm{i}}{2} \sum_{i j }x_i\left (\sum_k\left(\omega \delta_{i k}-A_{i k}\right)\left(\omega^* \delta_{k j}-A_{k j}^T\right)\right ) x_j\right\}. \end{equation} \begin{equation}  \det[M]^{-\frac{1}{2}}=\int\left(\prod_i \frac{d^{2} z_{i}}{\pi}\right) \exp \left\{-\sum_{i, j, k} z_{i}^{*}\left(\omega^{*} \delta_{i k}-J_{i k}^{T}\right)\left(\omega \delta_{k j}-J_{k j}\right) z_{j}\right\} \end{equation} The second one involving complex variables seems intuitively the best suited. However I do not know whether this is correct, and I could use a simpler integral then I would prefer very much so. Why would not this work: $$     \det[M]^{-\frac{1}{2}}=\int \left ( \prod_{i} \frac{\mathrm{d} x_i}{\sqrt{2 \pi }}\right ) \exp \left\{-\frac{1}{2} \sum_{i j }x_i\left (\sum_k\left(\omega \delta_{i k}-A_{i k}\right)\left(\omega^* \delta_{k j}-A_{k j}^T\right)\right ) x_j\right\}. $$ I am very curious on what the correct way would be. Any remark or advice would be greatly appreciated! edit: I consider the case where $A$ is real, and does not have complex entries anymore. Second edit: I was told that I had to integrate over complex $z_i$ rather than real $x_i$ . If this is true I would like to know why I can't use real integration.","Let be a Hermitian matrix of size where is a real non symmetric matrix and . represents the  conjugate transpose of . I want to compute . I know that for a real symmetric matrix we can represent its determinant as a gaussian integral with real variables : However in my case has complex values. I was wondering if we could extend this integral representation to Hermitian matrices. Among the feedback I got, these are the candidates: The second one involving complex variables seems intuitively the best suited. However I do not know whether this is correct, and I could use a simpler integral then I would prefer very much so. Why would not this work: I am very curious on what the correct way would be. Any remark or advice would be greatly appreciated! edit: I consider the case where is real, and does not have complex entries anymore. Second edit: I was told that I had to integrate over complex rather than real . If this is true I would like to know why I can't use real integration.","M=\left (\omega\mathbb{I}-A\right )\left(\omega^{*}\mathbb{I}-A^{\dagger}\right) n\times n A \omega=a+\mathrm{i}b A^{\dagger} A \det[M]^{-\frac{1}{2}} \Sigma x_i 
\frac{1}{|\Sigma|^{1 / 2}}=\int \frac{1}{(2 \pi)^{n / 2}} \exp \left(-\frac{1}{2}\mathbf{x}^{T} \Sigma\mathbf{x}\right)\mathrm{d}\mathbf{x}. M \begin{equation}
    \det[M]^{-\frac{1}{2}}=\int \left ( \prod_{i} \frac{\mathrm{d} x_i}{\sqrt{2 \pi / i}}\right ) \exp \left\{-\frac{\mathrm{i}}{2} \sum_{i j }x_i\left (\sum_k\left(\omega \delta_{i k}-A_{i k}\right)\left(\omega^* \delta_{k j}-A_{k j}^T\right)\right ) x_j\right\}.
\end{equation} \begin{equation}
 \det[M]^{-\frac{1}{2}}=\int\left(\prod_i \frac{d^{2} z_{i}}{\pi}\right) \exp \left\{-\sum_{i, j, k} z_{i}^{*}\left(\omega^{*} \delta_{i k}-J_{i k}^{T}\right)\left(\omega \delta_{k j}-J_{k j}\right) z_{j}\right\}
\end{equation} 
    \det[M]^{-\frac{1}{2}}=\int \left ( \prod_{i} \frac{\mathrm{d} x_i}{\sqrt{2 \pi }}\right ) \exp \left\{-\frac{1}{2} \sum_{i j }x_i\left (\sum_k\left(\omega \delta_{i k}-A_{i k}\right)\left(\omega^* \delta_{k j}-A_{k j}^T\right)\right ) x_j\right\}.
 A z_i x_i","['linear-algebra', 'integration', 'matrices', 'random-matrices', 'gaussian-integral']"
74,"Is it possible to upper bound ${\bf tr}(ABXBA)$ in terms of ${\bf tr}(AXA)$ for positive definite $A, B, X$?",Is it possible to upper bound  in terms of  for positive definite ?,"{\bf tr}(ABXBA) {\bf tr}(AXA) A, B, X","Suppose $A, B, X$ are all real, symmetric and positive definite matrices. I want to upper bound ${\bf tr}(ABXBA)$ in terms of ${\bf tr}(AXA)$ ? Is it possble? My guess would be ${\bf tr}(ABXBA) \le \lambda_{\max}(BB) {\bf tr}(AXA)$ ? This is obviously true if $A, B$ commute. I am wondering whether it still holds if $A, B$ does not commute. I don't have any insight to prove this. But I tried a numerical simulation and it seems to be true. I would not trust numerical simulations as often they cannot capture good counterexamples. The bound I guessed is wrong as pointed out by user293121. But actually there is a very crude bound, i.e. \begin{align*} {\bf tr}(ABXBA) \le \lambda_{\max}(A^2) {\bf tr}(BXB) \le \lambda_{\max}^2 (A) \lambda_{\max}^2 (B) {\bf tr}(X) \le \frac{\lambda_{\max}^2 (A) \lambda_{\max}^2 (B)}{\lambda_{\min}^2(A)} {\bf tr}(AXA). \end{align*}","Suppose are all real, symmetric and positive definite matrices. I want to upper bound in terms of ? Is it possble? My guess would be ? This is obviously true if commute. I am wondering whether it still holds if does not commute. I don't have any insight to prove this. But I tried a numerical simulation and it seems to be true. I would not trust numerical simulations as often they cannot capture good counterexamples. The bound I guessed is wrong as pointed out by user293121. But actually there is a very crude bound, i.e.","A, B, X {\bf tr}(ABXBA) {\bf tr}(AXA) {\bf tr}(ABXBA) \le \lambda_{\max}(BB) {\bf tr}(AXA) A, B A, B \begin{align*}
{\bf tr}(ABXBA) \le \lambda_{\max}(A^2) {\bf tr}(BXB) \le \lambda_{\max}^2 (A) \lambda_{\max}^2 (B) {\bf tr}(X) \le \frac{\lambda_{\max}^2 (A) \lambda_{\max}^2 (B)}{\lambda_{\min}^2(A)} {\bf tr}(AXA).
\end{align*}","['linear-algebra', 'matrices', 'inequality', 'trace']"
75,Estimate parameter $a$ such that $tr \left[ A (B- (I-aC)B(I-aC) ) \right] > 0$.,Estimate parameter  such that .,a tr \left[ A (B- (I-aC)B(I-aC) ) \right] > 0,"Suppose $A, B, C$ are all real symmetric and positive definite matrices. Consider the function $f: \mathbb R \to \mathbb R$ given by $$ a \mapsto {\bf tr}\left[ A (B- (I-aC)B(I-aC) ) \right],$$ where $I$ is identity matrix. It is clear $f(0) = 0$ and further assume there exists some $\tau > 0$ such that $f(x) > 0$ for every $x \in (0, \tau)$ . We may as well assume the maximal interval such that $f(x) > 0$ to be $(0, \tau)$ . That is, $f(x) > 0$ for $x \in (0, \tau)$ and $f(0) = f(\tau) = 0$ . I am wondering with these information, is it possible to deduce $\tau \ge \frac{1}{\lambda_{\max}(C)}$ ? Essentially I am in the situation that I know for small $a$ , the trace is positive and by continuity there should be some maximal interval the trace is always positive. I want to estimate this interval. I tried to use a crude bound \begin{align*} {\bf tr}\left[ A (B- (I-aC)B(I-aC) ) \right] \ge \lambda_{\min}(A) {\bf tr}(B) - \lambda_{\max}^2(I-aC)\lambda_{\max}(A) {\bf tr}(B). \end{align*} But this gives us meaningless bound since if we set above bound to be greater tha $0$ , $a$ could be possibly unsolvable. On the other hand, I feel that $a$ must be related to $\lambda_{\max}(I-aC)$ so we can choose $a$ to minimize this quantity and this would give us $a'=\frac{2}{\lambda_{\min}(C) + \lambda_{\max}(C)}$ . Intuitively, I would imagine over $[0, a']$ , $f$ should be positive.","Suppose are all real symmetric and positive definite matrices. Consider the function given by where is identity matrix. It is clear and further assume there exists some such that for every . We may as well assume the maximal interval such that to be . That is, for and . I am wondering with these information, is it possible to deduce ? Essentially I am in the situation that I know for small , the trace is positive and by continuity there should be some maximal interval the trace is always positive. I want to estimate this interval. I tried to use a crude bound But this gives us meaningless bound since if we set above bound to be greater tha , could be possibly unsolvable. On the other hand, I feel that must be related to so we can choose to minimize this quantity and this would give us . Intuitively, I would imagine over , should be positive.","A, B, C f: \mathbb R \to \mathbb R  a \mapsto {\bf tr}\left[ A (B- (I-aC)B(I-aC) ) \right], I f(0) = 0 \tau > 0 f(x) > 0 x \in (0, \tau) f(x) > 0 (0, \tau) f(x) > 0 x \in (0, \tau) f(0) = f(\tau) = 0 \tau \ge \frac{1}{\lambda_{\max}(C)} a \begin{align*}
{\bf tr}\left[ A (B- (I-aC)B(I-aC) ) \right] \ge \lambda_{\min}(A) {\bf tr}(B) - \lambda_{\max}^2(I-aC)\lambda_{\max}(A) {\bf tr}(B).
\end{align*} 0 a a \lambda_{\max}(I-aC) a a'=\frac{2}{\lambda_{\min}(C) + \lambda_{\max}(C)} [0, a'] f","['linear-algebra', 'matrices', 'matrix-analysis']"
76,$T \in \mathcal L (V)$ has no real eigenvalues. Prove that every subspace of $V$ invariant under $T$ has even dimension.,has no real eigenvalues. Prove that every subspace of  invariant under  has even dimension.,T \in \mathcal L (V) V T,"Suppose $V$ is a real vector space and $T \in \mathcal L (V)$ has no real eigenvalues. Prove that every subspace of $V$ invariant under $T$ has even dimension. Solution : Suppose $U$ is a subspace of $V$ that is invariant under $T$ . If $\dim U$ were odd, then $T|_{U}$ would have an eigenvalue $\lambda \in \Bbb R$ . $\exists v \neq 0, v\in U$ such that $T|_{U} u = \lambda u$ .Then $\lambda$ is an eigenvalue of $T$ .But $T$ has no eigenvalues, so $\dim U $ must be even. Why that happened when $T|_{U}$ has odd dimension?","Suppose is a real vector space and has no real eigenvalues. Prove that every subspace of invariant under has even dimension. Solution : Suppose is a subspace of that is invariant under . If were odd, then would have an eigenvalue . such that .Then is an eigenvalue of .But has no eigenvalues, so must be even. Why that happened when has odd dimension?","V T \in \mathcal L (V) V T U V T \dim U T|_{U} \lambda \in \Bbb R \exists v \neq 0, v\in U T|_{U} u = \lambda u \lambda T T \dim U  T|_{U}","['linear-algebra', 'eigenvalues-eigenvectors', 'invariant-subspace']"
77,Show that $(I − P)^2 = I − P$ if $P=P^2$,Show that  if,(I − P)^2 = I − P P=P^2,Let $P $ be an $n \times n$ matrix and $I$ be the $n \times n$ identity matrix. Show that $$ (I − P)^2 = I − P $$ is valid if $P = P^2$ . I did the following. $$(I - P)^2 = I^2 - IP - PI + P^2 = I - P$$ where $I^2 = I $ because it is the identity matrix. Is this enough to show or did I miss something?,Let be an matrix and be the identity matrix. Show that is valid if . I did the following. where because it is the identity matrix. Is this enough to show or did I miss something?,P  n \times n I n \times n  (I − P)^2 = I − P  P = P^2 (I - P)^2 = I^2 - IP - PI + P^2 = I - P I^2 = I ,"['linear-algebra', 'matrices', 'idempotents', 'projection-matrices']"
78,"Determining eigenvalues of sum of 2 matrices, and then evaluating whether the limit exists","Determining eigenvalues of sum of 2 matrices, and then evaluating whether the limit exists",,"I'm studying for an exam on Tuesday and have been stumped on this question for a little while. Any hints or help at all would be appreciate! I'm given 2 matrices, $A$ and $R$ as shown below. I'm also given that the eigenvalues for matrix $A$ are: 4, 3, -3, & -2. I am then told to determine the eigenvalues of $C(\alpha, \beta) = \alpha A + \beta R$ , and thus determine when the limit $\lim \limits_{n \to \infty} C(\alpha, \beta)^n$ exists, given that $\alpha$ and $\beta$ are both greater than 0. For the second part (determining when the limit exists), I think I have to use the rule that $\lim \limits_{n \to \infty} C(\alpha, \beta)^n$ will only exist if the eigenvalues $\lvert \lambda\rvert < 1$ , so I imagine I'll have to set up some inequalities to do so. Any help is appreciated, since I've been stuck on this for over a day now! I've attempted to use the fact that the rank of matrix $R$ is 1, but I'm unsure how. $A$ : $$     \begin{pmatrix}     2 & 0 & 2 & 0 \\     2 & -1 & 3 & 0 \\     2 & -1 & 2 & 1 \\     -16 & 8 & 13 & -1 \\     \end{pmatrix} $$ $R$ : $$     \begin{pmatrix}     1 & 1 & 1 & 1 \\     1 & 1 & 1 & 1 \\     1 & 1 & 1 & 1 \\     1 & 1 & 1 & 1 \\     \end{pmatrix} $$ EDIT: I realise that given the eigenvalues of A, the eigenvalues of $\alpha A$ will simply be the eigenvalues of A multiplied by $\alpha$ . Is this property true of all matrices? Or is it just because they have the same row sums? EDIT #2: I also realise that the eigenvalues of $A + R$ are 8, 3, -3, 2 i.e. the highest eigenvalue was multiplied by 2 while the rest remained the same. Similarly, eigenvalues of $A + 2R$ are 12, 3, -3, 2 and so on. I again fail to see why this is the case - would appreciate any pointers. EDIT #3: Based on the previous 2 edits, the eigenvalues for $C(\alpha, \beta) = \alpha A + \beta R$ will be: $4\alpha + 4\beta,  3\alpha,  -3\alpha,  -2\alpha$ . I figured this out by manually calculating the eigenvalues for the first couple of $\alpha's$ and $\beta's$ , however is there a trick I'm missing to simplifying this problem, since on the exam I don't think I'll be asked to compute the eigenvalues of a 4X4 matrix.","I'm studying for an exam on Tuesday and have been stumped on this question for a little while. Any hints or help at all would be appreciate! I'm given 2 matrices, and as shown below. I'm also given that the eigenvalues for matrix are: 4, 3, -3, & -2. I am then told to determine the eigenvalues of , and thus determine when the limit exists, given that and are both greater than 0. For the second part (determining when the limit exists), I think I have to use the rule that will only exist if the eigenvalues , so I imagine I'll have to set up some inequalities to do so. Any help is appreciated, since I've been stuck on this for over a day now! I've attempted to use the fact that the rank of matrix is 1, but I'm unsure how. : : EDIT: I realise that given the eigenvalues of A, the eigenvalues of will simply be the eigenvalues of A multiplied by . Is this property true of all matrices? Or is it just because they have the same row sums? EDIT #2: I also realise that the eigenvalues of are 8, 3, -3, 2 i.e. the highest eigenvalue was multiplied by 2 while the rest remained the same. Similarly, eigenvalues of are 12, 3, -3, 2 and so on. I again fail to see why this is the case - would appreciate any pointers. EDIT #3: Based on the previous 2 edits, the eigenvalues for will be: . I figured this out by manually calculating the eigenvalues for the first couple of and , however is there a trick I'm missing to simplifying this problem, since on the exam I don't think I'll be asked to compute the eigenvalues of a 4X4 matrix.","A R A C(\alpha, \beta) = \alpha A + \beta R \lim \limits_{n \to \infty} C(\alpha, \beta)^n \alpha \beta \lim \limits_{n \to \infty} C(\alpha, \beta)^n \lvert \lambda\rvert < 1 R A 
    \begin{pmatrix}
    2 & 0 & 2 & 0 \\
    2 & -1 & 3 & 0 \\
    2 & -1 & 2 & 1 \\
    -16 & 8 & 13 & -1 \\
    \end{pmatrix}
 R 
    \begin{pmatrix}
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    \end{pmatrix}
 \alpha A \alpha A + R A + 2R C(\alpha, \beta) = \alpha A + \beta R 4\alpha + 4\beta,  3\alpha,  -3\alpha,  -2\alpha \alpha's \beta's","['linear-algebra', 'limits', 'convergence-divergence', 'eigenvalues-eigenvectors']"
79,"Is the differential at a regular point, a vector space isomorphism of tangent spaces, also a diffeomorphism of tangent spaces as manifolds?","Is the differential at a regular point, a vector space isomorphism of tangent spaces, also a diffeomorphism of tangent spaces as manifolds?",,"Note: My question is not ""If $f$ is a diffeomorphism, then is the differential $D_qf$ an isomorphism?"" My book is From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave. I didn't study much of the definitions or theorems in the book, if they were already found in An Introduction to Manifolds by Loring W. Tu. I mostly assume they're the same until there is evidence otherwise. In Chapter 11, Madsen and Tornehave define ""local index"", which looks to me like just a different way to say sign of the determinant of the Jacobian matrix that represents the differential (See Tu Proposition 8.11 ; Tu Section 23.3 ; Madsen and Tornehave Lemma 10.1 ; Madsen and Tornehave Lemma 10.3 ; Wikipedia Degree of a continuous mapping , specifically this ). Now, for a regular point $q \in f^{-1}(p)$ for a regular value $p$ that is in the image of $f$ (For a regular value $p$ that isn't in the image of $f$ , I'm sure there are neat vacuous arguments that I'm gonna skip), it says the local index is defined as $1$ if $D_qf$ preserves orientation and $-1$ otherwise. I was surprised to see orientation-preserving as an adjective for an isomorphism of vector spaces because I'm used to seeing orientation-preserving as an adjective for diffeomorphisms of manifolds. However, $T_pN^n \cong \mathbb R^n$ (vector space isomorphic), so I guess tangent spaces of manifolds are manifolds as well, assuming the image of an oriented manifold under a vector space isomorphism is also an oriented manifold or something. ( This question seems to confirm that tangent spaces of manifolds are manifolds, although I think the definition in the question is the same as the one in Madsen and Tornehave but different from the one in Tu). Actually, upon a second reading of the answer of Alex Mathers to that question, I think I have an answer to my question: Any vector space isomorphism, of tangent spaces of manifolds or any other vector spaces, turns out to be a homeomorphism. While my question is diffeomorphism, it turns out John M. Lee's Example 1.24 , which was pointed out by Alex Mathers, shows that any isomorphism of finite real vector spaces is a diffeomorphism as well. Rather than analyzing the example, I'm going to try a different proof.) I think that $D_qf$ , or $f_{*, q}$ in Tu's notation, is a diffeomorphism of the tangent spaces as manifolds because: $D_qf$ is surjective either by definition of $q$ being a regular point (Tu Definition 8.22 ) or by $q \in f^{-1}(p)$ and definition of $p$ being regular value of $f$ that is in the image of $f$ (Madsen and Tornehave Chapter 11 ). $D_qf$ is a homomorphism of tangent spaces (almost immediately from definition, but anyway, this follows from Tu Exercise 8.3 ). $D_qf$ is injective, by this , because of (1), (2) and that the dimensions of $T_qN$ and $TpM$ are finite and equal. $D_qf$ is a local diffeomorphism of manifolds if and only if for each $X_q \in T_qN$ , the (double) differential $D_{X_q}(D_qf): T_{X_q}(T_qN) \to T_{D_qf(X_q)}(T_pM)$ is an isomorphism of (double) tangent spaces, by the Inverse Function Theorem for manifolds (specifically by Tu Remark 8.12 , which gives a ""coordinate-free description"" for Tu Inverse Function Theorem for manifolds (Tu Theorem 6.26) ) $D_qf$ is a diffeomorphism of manifolds if and only if $D_qf$ is a bijective local diffeomorphism of manifolds (at each $X_q \in T_qN$ ) by this . $D_qf$ is an isomorphism of tangent spaces by (1), (2) and (3). Every $D_{X_q}(D_qf)$ is identical to $D_qf$ itself, by Tu Problem 8.2 (also found in this question and this question ), because of (2). Every $D_{X_q}(D_qf)$ is an isomorphism of tangent spaces because of (6) and (7). $D_qf$ is a local diffeomorphism of manifolds (at each $X_q \in T_qN$ ) by (4) and (8). $D_qf$ is a diffeomorphism of manifolds by (1), (3), (5), and (9).","Note: My question is not ""If is a diffeomorphism, then is the differential an isomorphism?"" My book is From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave. I didn't study much of the definitions or theorems in the book, if they were already found in An Introduction to Manifolds by Loring W. Tu. I mostly assume they're the same until there is evidence otherwise. In Chapter 11, Madsen and Tornehave define ""local index"", which looks to me like just a different way to say sign of the determinant of the Jacobian matrix that represents the differential (See Tu Proposition 8.11 ; Tu Section 23.3 ; Madsen and Tornehave Lemma 10.1 ; Madsen and Tornehave Lemma 10.3 ; Wikipedia Degree of a continuous mapping , specifically this ). Now, for a regular point for a regular value that is in the image of (For a regular value that isn't in the image of , I'm sure there are neat vacuous arguments that I'm gonna skip), it says the local index is defined as if preserves orientation and otherwise. I was surprised to see orientation-preserving as an adjective for an isomorphism of vector spaces because I'm used to seeing orientation-preserving as an adjective for diffeomorphisms of manifolds. However, (vector space isomorphic), so I guess tangent spaces of manifolds are manifolds as well, assuming the image of an oriented manifold under a vector space isomorphism is also an oriented manifold or something. ( This question seems to confirm that tangent spaces of manifolds are manifolds, although I think the definition in the question is the same as the one in Madsen and Tornehave but different from the one in Tu). Actually, upon a second reading of the answer of Alex Mathers to that question, I think I have an answer to my question: Any vector space isomorphism, of tangent spaces of manifolds or any other vector spaces, turns out to be a homeomorphism. While my question is diffeomorphism, it turns out John M. Lee's Example 1.24 , which was pointed out by Alex Mathers, shows that any isomorphism of finite real vector spaces is a diffeomorphism as well. Rather than analyzing the example, I'm going to try a different proof.) I think that , or in Tu's notation, is a diffeomorphism of the tangent spaces as manifolds because: is surjective either by definition of being a regular point (Tu Definition 8.22 ) or by and definition of being regular value of that is in the image of (Madsen and Tornehave Chapter 11 ). is a homomorphism of tangent spaces (almost immediately from definition, but anyway, this follows from Tu Exercise 8.3 ). is injective, by this , because of (1), (2) and that the dimensions of and are finite and equal. is a local diffeomorphism of manifolds if and only if for each , the (double) differential is an isomorphism of (double) tangent spaces, by the Inverse Function Theorem for manifolds (specifically by Tu Remark 8.12 , which gives a ""coordinate-free description"" for Tu Inverse Function Theorem for manifolds (Tu Theorem 6.26) ) is a diffeomorphism of manifolds if and only if is a bijective local diffeomorphism of manifolds (at each ) by this . is an isomorphism of tangent spaces by (1), (2) and (3). Every is identical to itself, by Tu Problem 8.2 (also found in this question and this question ), because of (2). Every is an isomorphism of tangent spaces because of (6) and (7). is a local diffeomorphism of manifolds (at each ) by (4) and (8). is a diffeomorphism of manifolds by (1), (3), (5), and (9).","f D_qf q \in f^{-1}(p) p f p f 1 D_qf -1 T_pN^n \cong \mathbb R^n D_qf f_{*, q} D_qf q q \in f^{-1}(p) p f f D_qf D_qf T_qN TpM D_qf X_q \in T_qN D_{X_q}(D_qf): T_{X_q}(T_qN) \to T_{D_qf(X_q)}(T_pM) D_qf D_qf X_q \in T_qN D_qf D_{X_q}(D_qf) D_qf D_{X_q}(D_qf) D_qf X_q \in T_qN D_qf","['linear-algebra', 'proof-verification']"
80,Find Jordan normal form and basis,Find Jordan normal form and basis,,"Let $$A=M(\varphi)^{st}_{st}={\begin{bmatrix}0&1&1\\-4&-4&-2\\0&0&-2\end{bmatrix}}$$ and $ \varphi: \mathbb R^{3} \rightarrow \mathbb R^{3}$ . Find the Jordan normal form $J_{A}$ for the matrix $A$ and a basis $X$ for the endomorphism $\varphi$ such that $J_{A}=M(\varphi)^{X}_{X}$ . I did this task but unfortunately my basis is not correct and I do not know where I'm making a mistake. My try: The appointment of $ J_ {A} $ is clear to me, so I will write only the result: $$J_{A}={\begin{bmatrix}-2&1&0\\0&-2&0\\0&0&-2\end{bmatrix}}$$ Then I am trying to find basis: $$(A+2I)^{2}={\begin{bmatrix}0&0&0\\0&0&0\\0&0&0\end{bmatrix}}$$ $$\alpha_{2} \in \ker(\varphi+2id)^{2}- \ker(\varphi +2id)$$ $$\ker(\varphi+2id)^{2}=\mathbb R^{3}$$ $$\ker(\varphi+2id)=lin\left\{(1,-2,0),(0,-1,1)\right\}$$ From the above conclusions I think that I can take: $$\alpha_{2}=(0,1,0)$$ $$\alpha_{1}=(\varphi+2id)(\alpha_{2})=(1,-2,0)$$ As $\alpha_{3}$ I choose a linearly independent vector and I have for example: $$\alpha_{3}=(0,0,1)$$ So my basis is: $$X=\left\{(1,-2,0),(0,1,0),(0,0,1)\right\}$$ I know that basis can be different. That's why I checked my answer: $$J_{A}=M(\varphi)_{X}^{X}=M(id)^{X}_{st}M(\varphi)^{st}_{st}M(id)^{st}_{X}$$ From my basis I have: $$M(id)^{st}_{X}={\begin{bmatrix}1&0&0\\-2&0&1\\0&1&0\end{bmatrix}}$$ Then I made a multiplication: $$M(id)^{X}_{st}M(\varphi)^{st}_{st}M(id)^{st}_{X}$$ and I get: $${\begin{bmatrix}-2&1&1\\0&-2&0\\0&0&-2\end{bmatrix}}$$ Of course this is not equal $J_{A}$ so I know that I have a mistake. I would like to add that it is very strange for me that when I was curiosity I changed the vector $\alpha_{1}$ and $\alpha_{2}$ then I get $J_{A}$ . However by my calculation I can't do it. Can you help me and say where I am doing something wrong? I suspect that I make some mistake in determining $ \alpha_{2} $ but I still do not realize what.","Let and . Find the Jordan normal form for the matrix and a basis for the endomorphism such that . I did this task but unfortunately my basis is not correct and I do not know where I'm making a mistake. My try: The appointment of is clear to me, so I will write only the result: Then I am trying to find basis: From the above conclusions I think that I can take: As I choose a linearly independent vector and I have for example: So my basis is: I know that basis can be different. That's why I checked my answer: From my basis I have: Then I made a multiplication: and I get: Of course this is not equal so I know that I have a mistake. I would like to add that it is very strange for me that when I was curiosity I changed the vector and then I get . However by my calculation I can't do it. Can you help me and say where I am doing something wrong? I suspect that I make some mistake in determining but I still do not realize what.","A=M(\varphi)^{st}_{st}={\begin{bmatrix}0&1&1\\-4&-4&-2\\0&0&-2\end{bmatrix}}  \varphi: \mathbb R^{3} \rightarrow \mathbb R^{3} J_{A} A X \varphi J_{A}=M(\varphi)^{X}_{X}  J_ {A}  J_{A}={\begin{bmatrix}-2&1&0\\0&-2&0\\0&0&-2\end{bmatrix}} (A+2I)^{2}={\begin{bmatrix}0&0&0\\0&0&0\\0&0&0\end{bmatrix}} \alpha_{2} \in \ker(\varphi+2id)^{2}- \ker(\varphi +2id) \ker(\varphi+2id)^{2}=\mathbb R^{3} \ker(\varphi+2id)=lin\left\{(1,-2,0),(0,-1,1)\right\} \alpha_{2}=(0,1,0) \alpha_{1}=(\varphi+2id)(\alpha_{2})=(1,-2,0) \alpha_{3} \alpha_{3}=(0,0,1) X=\left\{(1,-2,0),(0,1,0),(0,0,1)\right\} J_{A}=M(\varphi)_{X}^{X}=M(id)^{X}_{st}M(\varphi)^{st}_{st}M(id)^{st}_{X} M(id)^{st}_{X}={\begin{bmatrix}1&0&0\\-2&0&1\\0&1&0\end{bmatrix}} M(id)^{X}_{st}M(\varphi)^{st}_{st}M(id)^{st}_{X} {\begin{bmatrix}-2&1&1\\0&-2&0\\0&0&-2\end{bmatrix}} J_{A} \alpha_{1} \alpha_{2} J_{A}  \alpha_{2} ","['linear-algebra', 'matrices', 'jordan-normal-form']"
81,A Counter Examples in Linear Algebra (Vector Space),A Counter Examples in Linear Algebra (Vector Space),,"I have been studying linear algebra for a few years now and in fact, also teaching it. What makes learning (and teaching) mathematics more interesting is to find examples and/or counterexamples of what we learn. As a process, I am trying to find counterexamples of sets along with two operations $+$ and $\cdot$ , which we will call for the time being ""addition"" and ""scalar multiplication"" which do not form a vector space because they fail to satisfy exactly one of the axioms of a vector space. If I am to list out the axioms and hence the definition, it proceeds as follows:- A set $V$ along with two operations $+: V \times V \rightarrow V$ and $\cdot: \mathbb{F} \times V \rightarrow V$ , where $\mathbb{F}$ is a field, is called a ""vector space"" over the field $\mathbb{F}$ if: $\forall x, y, z \in V$ we have $\left( x + y \right) + z = x + \left( y + z \right)$ $\forall x, y, \in V$ we have $x + y = y + x$ $\exists 0 \in V$ such that $\forall x \in V$ , we have $x + 0 = x$ $\forall x \in V$ , $\exists y \in V$ such that $x + y = 0$ $\forall x, y \in V$ and $\forall \alpha \in \mathbb{F}$ , we have $\alpha \cdot \left( x + y \right) = \alpha \cdot x + \alpha \cdot y$ $\forall x \in V$ and $\forall \alpha, \beta \in \mathbb{F}$ , we have $\left( \alpha + \beta \right) \cdot x = \alpha \cdot x + \beta \cdot y$ $\forall x \in V$ and $\forall \alpha, \beta \in \mathbb{F}$ , we have $\alpha \cdot \left( \beta \cdot x \right) = \left( \alpha \beta \right) \cdot x$ , where $\alpha \beta$ denotes the multiplication of $\alpha$ with $\beta$ in the field $\mathbb{F}$ $\forall x \in V$ , we have $1 \cdot v = v$ , where $1 \in \mathbb{F}$ is the unity. It is not so difficult (if not easy) to find counterexamples of sets, fields and operations which satisfy all but one property from $1$ through $7$ . However, I have not yet been able to find an example which satisfy all properties except $8$ and hence fails to be a vector space. I would like some help in constructing such a counter example.","I have been studying linear algebra for a few years now and in fact, also teaching it. What makes learning (and teaching) mathematics more interesting is to find examples and/or counterexamples of what we learn. As a process, I am trying to find counterexamples of sets along with two operations and , which we will call for the time being ""addition"" and ""scalar multiplication"" which do not form a vector space because they fail to satisfy exactly one of the axioms of a vector space. If I am to list out the axioms and hence the definition, it proceeds as follows:- A set along with two operations and , where is a field, is called a ""vector space"" over the field if: we have we have such that , we have , such that and , we have and , we have and , we have , where denotes the multiplication of with in the field , we have , where is the unity. It is not so difficult (if not easy) to find counterexamples of sets, fields and operations which satisfy all but one property from through . However, I have not yet been able to find an example which satisfy all properties except and hence fails to be a vector space. I would like some help in constructing such a counter example.","+ \cdot V +: V \times V \rightarrow V \cdot: \mathbb{F} \times V \rightarrow V \mathbb{F} \mathbb{F} \forall x, y, z \in V \left( x + y \right) + z = x + \left( y + z \right) \forall x, y, \in V x + y = y + x \exists 0 \in V \forall x \in V x + 0 = x \forall x \in V \exists y \in V x + y = 0 \forall x, y \in V \forall \alpha \in \mathbb{F} \alpha \cdot \left( x + y \right) = \alpha \cdot x + \alpha \cdot y \forall x \in V \forall \alpha, \beta \in \mathbb{F} \left( \alpha + \beta \right) \cdot x = \alpha \cdot x + \beta \cdot y \forall x \in V \forall \alpha, \beta \in \mathbb{F} \alpha \cdot \left( \beta \cdot x \right) = \left( \alpha \beta \right) \cdot x \alpha \beta \alpha \beta \mathbb{F} \forall x \in V 1 \cdot v = v 1 \in \mathbb{F} 1 7 8","['linear-algebra', 'vector-spaces', 'examples-counterexamples']"
82,Showing $ 2 $ matrices are similar [duplicate],Showing  matrices are similar [duplicate], 2 ,This question already has answers here : How do I tell if matrices are similar? (6 answers) Closed 5 years ago . I gotta show if or if not those $ 2 $ matrices are similar: $$     \left(\begin{matrix}     3 & 2 & -2 \\     1 & 4 & 0 \\     -2 & 1 & -1 \\     \end{matrix}\right) $$ $$     \left(\begin{matrix}     1 & 3 & -1 \\     3 & 3 & 1 \\     -2 & 1 & 2 \\     \end{matrix}\right) $$ I already calculated their determinant but it's equal so nothing there. Is it possible to show it based on their polynominal?,This question already has answers here : How do I tell if matrices are similar? (6 answers) Closed 5 years ago . I gotta show if or if not those matrices are similar: I already calculated their determinant but it's equal so nothing there. Is it possible to show it based on their polynominal?," 2  
    \left(\begin{matrix}
    3 & 2 & -2 \\
    1 & 4 & 0 \\
    -2 & 1 & -1 \\
    \end{matrix}\right)
 
    \left(\begin{matrix}
    1 & 3 & -1 \\
    3 & 3 & 1 \\
    -2 & 1 & 2 \\
    \end{matrix}\right)
","['linear-algebra', 'matrices']"
83,Geometric understanding of subtracting lambda from diagonals,Geometric understanding of subtracting lambda from diagonals,,"Given the definition of eigenvalues/eigenvectors: $Av = \lambda v $ you could rearrange the terms to be: $(A - \lambda I)v = 0$ Geometrically, the first equation says that multiplying by $A$ is the same as scaling the vector $v$ by $\lambda$ . However, in the second equation, how do you visualize the effect of subtracting the matrix $\lambda I$ from matrix $A$ and how does that induce a linearly dependent set of basis vectors? TL;DR: I understand that the new matrix $(A-\lambda I)$ collapses the span of $v$ into a lower dimension but I don't understand how $A$ relates to $(A-\lambda I)$ geometrically.","Given the definition of eigenvalues/eigenvectors: you could rearrange the terms to be: Geometrically, the first equation says that multiplying by is the same as scaling the vector by . However, in the second equation, how do you visualize the effect of subtracting the matrix from matrix and how does that induce a linearly dependent set of basis vectors? TL;DR: I understand that the new matrix collapses the span of into a lower dimension but I don't understand how relates to geometrically.",Av = \lambda v  (A - \lambda I)v = 0 A v \lambda \lambda I A (A-\lambda I) v A (A-\lambda I),"['linear-algebra', 'geometry', 'eigenvalues-eigenvectors', 'visualization']"
84,3x3 integer matrix,3x3 integer matrix,,"As far as I know, a real matrix $M$ has a real square root if $M$ is positive semidefinite, that is, if all eigenvalues are nonnegative. In fact, its square root is unique. I have read some research papers on how to solve for the square root of a $3 \times 3$ positive definite matrix using the Cayley-Hamilton theorem, the minimal polynomial, and diagonalization. When does $3 \times 3$ matrix $M$ with integer entries have a square root with integer entries? Trivially, $M$ must be positive definite to make sure its square root exists and is real. And $\det(M)$ must be a a perfect square. Other than that, I'm stuck.","As far as I know, a real matrix has a real square root if is positive semidefinite, that is, if all eigenvalues are nonnegative. In fact, its square root is unique. I have read some research papers on how to solve for the square root of a positive definite matrix using the Cayley-Hamilton theorem, the minimal polynomial, and diagonalization. When does matrix with integer entries have a square root with integer entries? Trivially, must be positive definite to make sure its square root exists and is real. And must be a a perfect square. Other than that, I'm stuck.",M M 3 \times 3 3 \times 3 M M \det(M),"['linear-algebra', 'matrices', 'matrix-exponential']"
85,When does $AA^T$ commute with $A^T$?,When does  commute with ?,AA^T A^T,"Suppose $A \in M_n(\mathbb R)$ is a real matrix. I am wondering what conditions would guarantee $AA^T$ commute with $A^T$, i.e., \begin{align*} AA^T A^T = A^T A A^T. \end{align*} If $A$ is normal, I think the relation holds since then $A^T$ is a polynomial in $A$. On the other hand, the commutativity is exactly $A(AA^T-A^TA) = 0$ which is columns of $(AA^T-A^TA)$ are in $ \text{ker}(A)$. Is this a property of some class of matrices?","Suppose $A \in M_n(\mathbb R)$ is a real matrix. I am wondering what conditions would guarantee $AA^T$ commute with $A^T$, i.e., \begin{align*} AA^T A^T = A^T A A^T. \end{align*} If $A$ is normal, I think the relation holds since then $A^T$ is a polynomial in $A$. On the other hand, the commutativity is exactly $A(AA^T-A^TA) = 0$ which is columns of $(AA^T-A^TA)$ are in $ \text{ker}(A)$. Is this a property of some class of matrices?",,"['linear-algebra', 'matrices']"
86,Change the trace of a Matrix,Change the trace of a Matrix,,"I want to know if I have a matrix $A \in \mathbb{M}_{n\times n}(\mathbb{K}) $ and I want to change the trace multiplying it by a number $\beta \in \mathbb{K}$: $$ A=\left( \begin{array}{ccc} \alpha_{11} & ... & \alpha_{1n} \\ \vdots &  & \vdots \\ \alpha_{n1} & ... & \alpha_{nn} \end{array} \right) \to  B=\left( \begin{array}{cccc} \beta \alpha_{11} & \alpha_{12} & ... & \alpha_{1n}  \\ \alpha_{21} &\beta\alpha_{22}  & & \vdots \\ \vdots &  & & \vdots \\ \alpha_{n1} & ... & ... & \beta\alpha_{nn} \end{array} \right)$$ There exist some $X \in \mathbb{M}_{n\times n}(\mathbb{K})$ s.t $A\times X = B$ and if it exist, what form does it have. Thanks","I want to know if I have a matrix $A \in \mathbb{M}_{n\times n}(\mathbb{K}) $ and I want to change the trace multiplying it by a number $\beta \in \mathbb{K}$: $$ A=\left( \begin{array}{ccc} \alpha_{11} & ... & \alpha_{1n} \\ \vdots &  & \vdots \\ \alpha_{n1} & ... & \alpha_{nn} \end{array} \right) \to  B=\left( \begin{array}{cccc} \beta \alpha_{11} & \alpha_{12} & ... & \alpha_{1n}  \\ \alpha_{21} &\beta\alpha_{22}  & & \vdots \\ \vdots &  & & \vdots \\ \alpha_{n1} & ... & ... & \beta\alpha_{nn} \end{array} \right)$$ There exist some $X \in \mathbb{M}_{n\times n}(\mathbb{K})$ s.t $A\times X = B$ and if it exist, what form does it have. Thanks",,"['linear-algebra', 'matrices', 'trace']"
87,Expected number of steps for reaching a specific absorbing state in an absorbing Markov chain,Expected number of steps for reaching a specific absorbing state in an absorbing Markov chain,,"For a finite Markov chain with one or more absorbing states there are well-known methods for determining either the probability of ending up in a specific absorbing state or the expected number of steps to end up in any absorbing state. However, I've not found a way to determine the expected number of steps to reach a specific state. For instance, in the classic Drunkard's Walk, the drunkard is absorbed by both home and the bar. If I were looking at a Drunkard's Walk, what I would want to determine is how many steps it takes, on average, a drunkard who gets home to actually get home. Ideally so that the answer can be presented in a form like ""There's an X% chance the drunkard will end up at home, taking on average Y steps, and a Z% chance the drunkard will end up at the bar after an average of W steps."" or equivalent. I've not yet found a good way to find the solution, I'm at a bit of a loss where to start, and if the answer exists out there I've not found the correct Google search terms to land me on it. Thanks!","For a finite Markov chain with one or more absorbing states there are well-known methods for determining either the probability of ending up in a specific absorbing state or the expected number of steps to end up in any absorbing state. However, I've not found a way to determine the expected number of steps to reach a specific state. For instance, in the classic Drunkard's Walk, the drunkard is absorbed by both home and the bar. If I were looking at a Drunkard's Walk, what I would want to determine is how many steps it takes, on average, a drunkard who gets home to actually get home. Ideally so that the answer can be presented in a form like ""There's an X% chance the drunkard will end up at home, taking on average Y steps, and a Z% chance the drunkard will end up at the bar after an average of W steps."" or equivalent. I've not yet found a good way to find the solution, I'm at a bit of a loss where to start, and if the answer exists out there I've not found the correct Google search terms to land me on it. Thanks!",,"['linear-algebra', 'markov-chains']"
88,Loewner order in terms of eigenvalues,Loewner order in terms of eigenvalues,,"Suppose that $A \succeq B$, where $A$ and $B$ are real symmetric matrices, so that $A - B$ is positive semidefinite, equivalently, $A - B$ has nonnegative eigenvalues. Is it always true that $\lambda_i(A) \geq \lambda_i(B)$ (assuming that eigenvalues are ordered)?","Suppose that $A \succeq B$, where $A$ and $B$ are real symmetric matrices, so that $A - B$ is positive semidefinite, equivalently, $A - B$ has nonnegative eigenvalues. Is it always true that $\lambda_i(A) \geq \lambda_i(B)$ (assuming that eigenvalues are ordered)?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'order-theory', 'symmetric-matrices']"
89,"Probably of $A^k[i, j] \geq 1$ for a random matrix",Probably of  for a random matrix,"A^k[i, j] \geq 1","Suppose there is a square matrix $A$, with random elements (say  $A[i, j] \geq 1$ with probability $p$). This can be thought of as the adjacency matrix of a (directed) graph and $A^k[i, j]$ as the number of path from $i$ to $j$ with length $k$. Two question: What is the probability that Prob($A^k[i, j] \geq 1$). I.e. what is the probability that there is a path of length $k$ between two given nodes, in a random graph. How would the above probability change, if I tell you: (a) $A^l[i, j]\geq 1, \text{for some }l,  l > k$ (b) $A^l[i, j]\geq 1, \text{for some }l, l < k$. I am not sure how hard are these questions. I did a little bit of thinking, didn't get anywhere. I hope I am not asking trivial questions.","Suppose there is a square matrix $A$, with random elements (say  $A[i, j] \geq 1$ with probability $p$). This can be thought of as the adjacency matrix of a (directed) graph and $A^k[i, j]$ as the number of path from $i$ to $j$ with length $k$. Two question: What is the probability that Prob($A^k[i, j] \geq 1$). I.e. what is the probability that there is a path of length $k$ between two given nodes, in a random graph. How would the above probability change, if I tell you: (a) $A^l[i, j]\geq 1, \text{for some }l,  l > k$ (b) $A^l[i, j]\geq 1, \text{for some }l, l < k$. I am not sure how hard are these questions. I did a little bit of thinking, didn't get anywhere. I hope I am not asking trivial questions.",,"['linear-algebra', 'graph-theory', 'random-graphs']"
90,geometric interpretation of element-wise vector multiplication,geometric interpretation of element-wise vector multiplication,,"Geometrically speaking, vector dot product is interpreted as a projection of one vector on the other, while vector cross product gives another vector that is orthogonal to the multiplied vectors(right?). I am wondering if element-wise vector multiplication has any geometric intuition. I am not a mathematician, so I would appreciate a lay-person friendly explanation.","Geometrically speaking, vector dot product is interpreted as a projection of one vector on the other, while vector cross product gives another vector that is orthogonal to the multiplied vectors(right?). I am wondering if element-wise vector multiplication has any geometric intuition. I am not a mathematician, so I would appreciate a lay-person friendly explanation.",,"['linear-algebra', 'algebraic-geometry']"
91,"If $A,B$ Hermitian and $A-B$ has only nonnegative eigenvalues, show that $\lambda_i(A)\geq\lambda_i(B)$","If  Hermitian and  has only nonnegative eigenvalues, show that","A,B A-B \lambda_i(A)\geq\lambda_i(B)","Denote the sorted eigenvalues of an $n\times n$ matrix $C$ as $\lambda_1(C)\leq\dots\leq\lambda_n(C)$. I want to show that the following statement holds for all $i=1,\dots,n$ If $A,B$ Hermitian and $A-B$ has only nonnegative eigenvalues, show that $\lambda_i(A)\geq\lambda_i(B)$ Let me state Weyl's Theorem (Thm. 4.3.1 in Horn and Johnson's Matrix Analysis book) Let $A,B\in\mathbb{C}^{n\times n}$ be Hermitian and let the respective eigenvalues of $A$, $B$, and $A+B$ be $\{\lambda_i(A)\}_{i=1}^n$, $\{\lambda_i(B)\}_{i=1}^n$, and $\{\lambda_i(A+B)\}_{i=1}^n$, each algebraicly ordered as explained above (i.e. $\lambda_1(\dots)\leq\dots\leq\lambda_n(\dots)$). Then   $$\lambda_i(A+B)\leq\lambda_{i+j}(A)+\lambda_{n-j}(B),\qquad j=0,\dots,n-i$$   for each $i=1,\dots,n$.   Also,   $$\lambda_{i-j+1}(A)+\lambda_{j}(B)\leq\lambda_i(A+B),\qquad j=1,\dots,i$$   for each $i=1,\dots,n$. Let $\lambda_1'(B')\leq\dots\leq\lambda_n'(B')$ denote the sorted eigenvalues of $B'=-B$. First, since $B$ Hermitian, then $B'$ is also Hermitian and if $(\lambda, \mathbf{x})$ is an eigenpair of $B$, then $(-\lambda, \mathbf{x})$ is an eigenpair of $B'$. Hence, if $\lambda_1(B)\leq\dots\leq\lambda_n(B)$ are the sorted eigenvalues of $B$, then $\lambda_1'(B')=-\lambda_n(B)\leq\dots\leq\lambda_n'(B')=-\lambda_1(B)$ are the sorted eigenvalues of $B'$. Let's apply the first part of the theorem for matrices $A,B'$ for $j=0$ \begin{eqnarray*} \lambda_i(A+B')\leq\lambda_{i+j}(A)+\lambda_{n-j}'(B')&\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(B')\geq\lambda_i(A+B')\\ &\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(B')\geq0\qquad (A-B\text{ has nonnegative eigenvalues})\\ &\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(-B)\geq0\\ &\Rightarrow&\lambda_{i}(A)-\lambda_{1}(B)\geq0\\ &\Rightarrow&\lambda_{i}(A)\geq\lambda_{1}(B) \end{eqnarray*} But, this only shows that $\lambda_{1}(A)\geq\lambda_{1}(B)$ and I need $\lambda_{i}(A)\geq\lambda_{i}(B)$ for all values of $i$. What else can I do? I think I need to show that $\lambda_{1}(A)\geq\lambda_{n}(B)$ which would be enough.","Denote the sorted eigenvalues of an $n\times n$ matrix $C$ as $\lambda_1(C)\leq\dots\leq\lambda_n(C)$. I want to show that the following statement holds for all $i=1,\dots,n$ If $A,B$ Hermitian and $A-B$ has only nonnegative eigenvalues, show that $\lambda_i(A)\geq\lambda_i(B)$ Let me state Weyl's Theorem (Thm. 4.3.1 in Horn and Johnson's Matrix Analysis book) Let $A,B\in\mathbb{C}^{n\times n}$ be Hermitian and let the respective eigenvalues of $A$, $B$, and $A+B$ be $\{\lambda_i(A)\}_{i=1}^n$, $\{\lambda_i(B)\}_{i=1}^n$, and $\{\lambda_i(A+B)\}_{i=1}^n$, each algebraicly ordered as explained above (i.e. $\lambda_1(\dots)\leq\dots\leq\lambda_n(\dots)$). Then   $$\lambda_i(A+B)\leq\lambda_{i+j}(A)+\lambda_{n-j}(B),\qquad j=0,\dots,n-i$$   for each $i=1,\dots,n$.   Also,   $$\lambda_{i-j+1}(A)+\lambda_{j}(B)\leq\lambda_i(A+B),\qquad j=1,\dots,i$$   for each $i=1,\dots,n$. Let $\lambda_1'(B')\leq\dots\leq\lambda_n'(B')$ denote the sorted eigenvalues of $B'=-B$. First, since $B$ Hermitian, then $B'$ is also Hermitian and if $(\lambda, \mathbf{x})$ is an eigenpair of $B$, then $(-\lambda, \mathbf{x})$ is an eigenpair of $B'$. Hence, if $\lambda_1(B)\leq\dots\leq\lambda_n(B)$ are the sorted eigenvalues of $B$, then $\lambda_1'(B')=-\lambda_n(B)\leq\dots\leq\lambda_n'(B')=-\lambda_1(B)$ are the sorted eigenvalues of $B'$. Let's apply the first part of the theorem for matrices $A,B'$ for $j=0$ \begin{eqnarray*} \lambda_i(A+B')\leq\lambda_{i+j}(A)+\lambda_{n-j}'(B')&\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(B')\geq\lambda_i(A+B')\\ &\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(B')\geq0\qquad (A-B\text{ has nonnegative eigenvalues})\\ &\Rightarrow&\lambda_{i}(A)+\lambda_{n}'(-B)\geq0\\ &\Rightarrow&\lambda_{i}(A)-\lambda_{1}(B)\geq0\\ &\Rightarrow&\lambda_{i}(A)\geq\lambda_{1}(B) \end{eqnarray*} But, this only shows that $\lambda_{1}(A)\geq\lambda_{1}(B)$ and I need $\lambda_{i}(A)\geq\lambda_{i}(B)$ for all values of $i$. What else can I do? I think I need to show that $\lambda_{1}(A)\geq\lambda_{n}(B)$ which would be enough.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
92,Measuring 'parallelness' of vectors,Measuring 'parallelness' of vectors,,"I wish to construct some sort of 'measure' (not in the formal sense) of the 'parallelness' of a finite set of $m$ vectors $S = \{v_1 , \ldots , v_m\} \subset \mathbb{R}^n$. This parallelness $p$ should have the following properties: If $v \in \mathbb{R}^n$ and $\{\lambda_1 , \ldots , \lambda_m \} \subset \mathbb{R}^+$ then $$ p(\lambda_1 v , \ldots , \lambda_m v) = 1 $$ since, with our set of positive $\lambda$'s, all vectors of the form $\lambda_iv$ point in the 'same diection' In any other case $$ p < 1 $$ to indicate that these vectors are not totally parallel An easy way to construct such a thing for $m=2$ is using the dot product. Denoting unit vectors by a hat then $$ p(v_1,v_2) = \hat{v}_1 \cdot \hat{v}_2 $$ Note that $p(v_1,v_2) \leq 1$ because $\hat{v}_1$ and $\hat{v}_2$ are unit vectors. For more vectors it gets trickier. I currently have the approach $$ p(v_1, \ldots , v_m) = \left\lVert \frac {\hat{v}_1 + \ldots + \hat{v}_m}{m} \right\rVert $$ which has the nice property that if the vectors are uniformly distributed over a sphere then $p=0$, no parallelness. This version is inspired by the mean of circular quantities Is there a general approach and theory behind what I'm trying to do? Is there a 'better' way to measure how parallel a set of vectors are? Motivation: This question is inspired by numerical computations, where I get a number of vector fields and I need to know if the vector fields are parallel. Of course there will be some error in the computation, and so I need to check if the vector fields exceed some level of parallelness. However, I am interested in whether there is some general approach to get something like a 'standard deviation of direction' or similar in high dimensional space, an analytical tool to approach this kind of problem. Edit (13/Nov/2017): After considering Raskolnikov's answer, it turns out that I want to first determine whether the vectors are 'parallel' without caring whether they are aligned or anti-aligned, so at this stage $v$ and $-v$ are considered the same, this gives me a 'region' in my vector field. I then want to identify the type of region by comparing all vectors to the first and determining whether they are parallel or anti-parallel. This second step is trivial, it is the first step I am addressing in this question. I therefore update my required properties to be: If $v \in \mathbb{R}^n$ and $\{\lambda_1 , \ldots , \lambda_m \} \subset \mathbb{R}$ then $$ p(\lambda_1 v , \ldots , \lambda_m v) = 1 $$ since all vectors of the form $\lambda_iv$ are parallel/antiparallel In any other case $$ p < 1 $$ to indicate that these vectors are not totally parallel/antiparallel","I wish to construct some sort of 'measure' (not in the formal sense) of the 'parallelness' of a finite set of $m$ vectors $S = \{v_1 , \ldots , v_m\} \subset \mathbb{R}^n$. This parallelness $p$ should have the following properties: If $v \in \mathbb{R}^n$ and $\{\lambda_1 , \ldots , \lambda_m \} \subset \mathbb{R}^+$ then $$ p(\lambda_1 v , \ldots , \lambda_m v) = 1 $$ since, with our set of positive $\lambda$'s, all vectors of the form $\lambda_iv$ point in the 'same diection' In any other case $$ p < 1 $$ to indicate that these vectors are not totally parallel An easy way to construct such a thing for $m=2$ is using the dot product. Denoting unit vectors by a hat then $$ p(v_1,v_2) = \hat{v}_1 \cdot \hat{v}_2 $$ Note that $p(v_1,v_2) \leq 1$ because $\hat{v}_1$ and $\hat{v}_2$ are unit vectors. For more vectors it gets trickier. I currently have the approach $$ p(v_1, \ldots , v_m) = \left\lVert \frac {\hat{v}_1 + \ldots + \hat{v}_m}{m} \right\rVert $$ which has the nice property that if the vectors are uniformly distributed over a sphere then $p=0$, no parallelness. This version is inspired by the mean of circular quantities Is there a general approach and theory behind what I'm trying to do? Is there a 'better' way to measure how parallel a set of vectors are? Motivation: This question is inspired by numerical computations, where I get a number of vector fields and I need to know if the vector fields are parallel. Of course there will be some error in the computation, and so I need to check if the vector fields exceed some level of parallelness. However, I am interested in whether there is some general approach to get something like a 'standard deviation of direction' or similar in high dimensional space, an analytical tool to approach this kind of problem. Edit (13/Nov/2017): After considering Raskolnikov's answer, it turns out that I want to first determine whether the vectors are 'parallel' without caring whether they are aligned or anti-aligned, so at this stage $v$ and $-v$ are considered the same, this gives me a 'region' in my vector field. I then want to identify the type of region by comparing all vectors to the first and determining whether they are parallel or anti-parallel. This second step is trivial, it is the first step I am addressing in this question. I therefore update my required properties to be: If $v \in \mathbb{R}^n$ and $\{\lambda_1 , \ldots , \lambda_m \} \subset \mathbb{R}$ then $$ p(\lambda_1 v , \ldots , \lambda_m v) = 1 $$ since all vectors of the form $\lambda_iv$ are parallel/antiparallel In any other case $$ p < 1 $$ to indicate that these vectors are not totally parallel/antiparallel",,"['linear-algebra', 'statistics', 'vector-spaces', 'normed-spaces']"
93,Number of all subspaces of a vector space over a finite field,Number of all subspaces of a vector space over a finite field,,"Let $V$ be an $n$-dimensional vector space over a finite field $\mathbb{F}_q$. We know that the number of $k$-dimensional subspaces of $V$ is given by the $q$-binomial coefficient $$\binom{n}{k}_q = \frac{(q^n-1) \cdots (q^n-q^{k-1})}{(q^k-1) \cdots (q^k-q^{k-1})}.$$ What can we say about the number of all subspaces of $V$, which is thus given by $$s(n,q) := \sum_{k=0}^{n} \binom{n}{k}_q~?$$ This is a polynomial in  $q$ with coefficients in $\mathbb{N}$. Is there are more concrete description? I already know that the coefficients count certain partitions, but I am not so much interested in the coefficients than in a closed or computable form. Can we write $s(n,q)$ as a sort of $q$-analog of a power of $2$? (In the limit case $q \to 1$, we will get $2^n$.) Is there a recurrence relation? Do these numbers have a name? It is tempting to use the $q$-binomial theorem, which states $$(x+y)^n = \sum_{k=0}^{n} \binom{n}{k}_q x^k y^{n-k}$$ in the ring $\mathbb{Z}\langle x,y : yx=q xy \rangle$. But we cannot simply plug in $x=y=1$ here, since this does not define a ring homomorphism. Here are some examples: $s(0,q)=1\\ s(1,q)=2\\ s(2,q)=q+3\\ s(3,q)=2 q^2+2 q+4\\ s(4,q)=q^4+3 q^3+4 q^2+3 q+5\\ s(5,q)=2 q^6+2 q^5+6 q^4+6 q^3+6 q^2+4 q+6\\ s(6,q)=q^9+3 q^8+4 q^7+7 q^6+9 q^5+11 q^4+9 q^3+8 q^2+5 q+7\\ s(7,q)=2 q^{12}{+}2 q^{11}{+}6 q^{10}{+}8 q^9{+}12 q^8{+}12 q^7{+}18 q^6{+}16 q^5{+}16 q^4{+}12 q^3{+}10 q^2{+}6 q{+}8$","Let $V$ be an $n$-dimensional vector space over a finite field $\mathbb{F}_q$. We know that the number of $k$-dimensional subspaces of $V$ is given by the $q$-binomial coefficient $$\binom{n}{k}_q = \frac{(q^n-1) \cdots (q^n-q^{k-1})}{(q^k-1) \cdots (q^k-q^{k-1})}.$$ What can we say about the number of all subspaces of $V$, which is thus given by $$s(n,q) := \sum_{k=0}^{n} \binom{n}{k}_q~?$$ This is a polynomial in  $q$ with coefficients in $\mathbb{N}$. Is there are more concrete description? I already know that the coefficients count certain partitions, but I am not so much interested in the coefficients than in a closed or computable form. Can we write $s(n,q)$ as a sort of $q$-analog of a power of $2$? (In the limit case $q \to 1$, we will get $2^n$.) Is there a recurrence relation? Do these numbers have a name? It is tempting to use the $q$-binomial theorem, which states $$(x+y)^n = \sum_{k=0}^{n} \binom{n}{k}_q x^k y^{n-k}$$ in the ring $\mathbb{Z}\langle x,y : yx=q xy \rangle$. But we cannot simply plug in $x=y=1$ here, since this does not define a ring homomorphism. Here are some examples: $s(0,q)=1\\ s(1,q)=2\\ s(2,q)=q+3\\ s(3,q)=2 q^2+2 q+4\\ s(4,q)=q^4+3 q^3+4 q^2+3 q+5\\ s(5,q)=2 q^6+2 q^5+6 q^4+6 q^3+6 q^2+4 q+6\\ s(6,q)=q^9+3 q^8+4 q^7+7 q^6+9 q^5+11 q^4+9 q^3+8 q^2+5 q+7\\ s(7,q)=2 q^{12}{+}2 q^{11}{+}6 q^{10}{+}8 q^9{+}12 q^8{+}12 q^7{+}18 q^6{+}16 q^5{+}16 q^4{+}12 q^3{+}10 q^2{+}6 q{+}8$",,"['linear-algebra', 'combinatorics', 'binomial-coefficients', 'finite-fields', 'q-analogs']"
94,Satisfying the following determinant inequality,Satisfying the following determinant inequality,,"I would like to find least restrictive conditions on $W = W^T \succ 0, \ V = V^T \succ 0$ (which are $\mathbb{R}^{n \times n}$ positive definite matrices) such that the following inequality is satisfied: $$ \text{det} \bigg( W^{-1} \Gamma W^{-1} + A^T V A \bigg) \geq 1  \tag{*}  $$ where $\Gamma = \Gamma^T \triangleq W - B P B^T \in \mathbb{R}^{n \times n} $, but is not necessarily a positive definite matrix, with $P = P^T \succ 0$ being a $\mathbb{R}^{n \times n}$ positive definite matrix. Further, the matrices $A, B \in \mathbb{R}^{n \times n}$ are arbitrary. I was hoping that (*) can be simplified by using appropriate determinant inequalities, for example using Minkowski's determinant inequality (here: https://mathoverflow.net/questions/251646/reverse-minkowski-and-related-determinant-inequalities ), but this requires that $\Gamma$ be positive definite--though it is symmetric $\Gamma = \Gamma^T$--so not sure. Further, the second term would need to be positive definite as well. I would not want to impose conditions on $A,B$ though.","I would like to find least restrictive conditions on $W = W^T \succ 0, \ V = V^T \succ 0$ (which are $\mathbb{R}^{n \times n}$ positive definite matrices) such that the following inequality is satisfied: $$ \text{det} \bigg( W^{-1} \Gamma W^{-1} + A^T V A \bigg) \geq 1  \tag{*}  $$ where $\Gamma = \Gamma^T \triangleq W - B P B^T \in \mathbb{R}^{n \times n} $, but is not necessarily a positive definite matrix, with $P = P^T \succ 0$ being a $\mathbb{R}^{n \times n}$ positive definite matrix. Further, the matrices $A, B \in \mathbb{R}^{n \times n}$ are arbitrary. I was hoping that (*) can be simplified by using appropriate determinant inequalities, for example using Minkowski's determinant inequality (here: https://mathoverflow.net/questions/251646/reverse-minkowski-and-related-determinant-inequalities ), but this requires that $\Gamma$ be positive definite--though it is symmetric $\Gamma = \Gamma^T$--so not sure. Further, the second term would need to be positive definite as well. I would not want to impose conditions on $A,B$ though.",,"['linear-algebra', 'matrices', 'determinant', 'normed-spaces']"
95,Rank of a Submatrix,Rank of a Submatrix,,"Let    $$B=\left[\begin{array}{c |cc}   1 & \begin{array}{ccc}0 & \cdots & 0\end{array} \\ \hline   \begin{array}{c}0\\ \vdots\\0\end{array} & B' \\ \end{array}\right],$$   where $B'$ is an $m \times n$ submatrix of $B$. Prove that if $rank(B)=r$, then $rank(B')=r-1$. Here is an attempt at a proof. As you will notice, I am not sure how to finish it, so hopefully someone might help me with finishing it. First note that $f : F^n \to F^{n+1}$ defined by $$f(v) = f(\begin{bmatrix} v_1 \\ \vdots \\ v_n \\ \end{bmatrix}) = \begin{bmatrix} 0 \\ v_1 \\ \vdots \\ v_n \\ \end{bmatrix}$$ is a injective linear operator and therefore maps linearly independent sets to linearly independent sets. For simplicity, we may write $f(v) = \begin{bmatrix} 0 \\ v \\ \end{bmatrix}$ Let $B = [e_1 ~|~ b_1 ~|~ b_2 ~|~ \dots ~|~ b_n]$, where $e_1$ is the first column of $B$, $b_2$ the second, etc; similarly, let $B' = [b_1' ~|~ b_2' ~|~ \dots ~|~ b_n']$. Then it can easily be seen that $b_i = f(b_i')$. Moreover, let $S$ denote the maximal set of linearly independent columns of $B$, and let $S'$ denote the same thing of $B'$; then we have $rank(B)=|S|$ and $rank(B')=|S'|$. Since the columns of $B$ have zeros in their first entry, clearly the first column $e_1$ cannot be in the span of the remaining columns and is therefore included in $S$. Here is the part I am having trouble with. I am trying to argue that $S = \{e_1\} \cup f(S')$. The sets involved in the union are clearly disjoint, since $e_1 \notin f(F^n)$, so $|S| = 1 + |f(S')|$. Since injective maps preserve cardinality, we have $|f(S')| = |S'| = rank(B')$, and the problem follows immediately. I am being a knucklehead; I can't figure out how to finish this last step. I could use some help. EDIT: When you read this post, if you could please check that the dimensions of the matrix are consistent with the spaces over which $f$ acts, I would appreciate that, since I often mess up those details.","Let    $$B=\left[\begin{array}{c |cc}   1 & \begin{array}{ccc}0 & \cdots & 0\end{array} \\ \hline   \begin{array}{c}0\\ \vdots\\0\end{array} & B' \\ \end{array}\right],$$   where $B'$ is an $m \times n$ submatrix of $B$. Prove that if $rank(B)=r$, then $rank(B')=r-1$. Here is an attempt at a proof. As you will notice, I am not sure how to finish it, so hopefully someone might help me with finishing it. First note that $f : F^n \to F^{n+1}$ defined by $$f(v) = f(\begin{bmatrix} v_1 \\ \vdots \\ v_n \\ \end{bmatrix}) = \begin{bmatrix} 0 \\ v_1 \\ \vdots \\ v_n \\ \end{bmatrix}$$ is a injective linear operator and therefore maps linearly independent sets to linearly independent sets. For simplicity, we may write $f(v) = \begin{bmatrix} 0 \\ v \\ \end{bmatrix}$ Let $B = [e_1 ~|~ b_1 ~|~ b_2 ~|~ \dots ~|~ b_n]$, where $e_1$ is the first column of $B$, $b_2$ the second, etc; similarly, let $B' = [b_1' ~|~ b_2' ~|~ \dots ~|~ b_n']$. Then it can easily be seen that $b_i = f(b_i')$. Moreover, let $S$ denote the maximal set of linearly independent columns of $B$, and let $S'$ denote the same thing of $B'$; then we have $rank(B)=|S|$ and $rank(B')=|S'|$. Since the columns of $B$ have zeros in their first entry, clearly the first column $e_1$ cannot be in the span of the remaining columns and is therefore included in $S$. Here is the part I am having trouble with. I am trying to argue that $S = \{e_1\} \cup f(S')$. The sets involved in the union are clearly disjoint, since $e_1 \notin f(F^n)$, so $|S| = 1 + |f(S')|$. Since injective maps preserve cardinality, we have $|f(S')| = |S'| = rank(B')$, and the problem follows immediately. I am being a knucklehead; I can't figure out how to finish this last step. I could use some help. EDIT: When you read this post, if you could please check that the dimensions of the matrix are consistent with the spaces over which $f$ acts, I would appreciate that, since I often mess up those details.",,"['linear-algebra', 'vector-spaces', 'matrix-rank']"
96,Existence of a positive semidefinite matrix that satisfies a set of equality constraints,Existence of a positive semidefinite matrix that satisfies a set of equality constraints,,"Given vectors $a_1, b_2, a_2, b_2 \in \mathcal{R}^{n\times 1}$, I am interested in finding a positive semi-definite matrix $M \in \mathcal{R}^{n\times n}$, $M \succeq 0$, such that $M\cdot a_1 = b_1$, $M\cdot a_2 = b_2$. Here $n \gg 2$, say $n = 1000 $. $a_1, a_2$ are not parallel and are non-zero. To write it in equations, I want to solve the following semidefinite program \begin{equation*} \begin{aligned} & \underset{M}{\text{minimize}} & & 0 \\ & \text{subject to} & & M\cdot a_1 = b_1 \\ && & M\cdot a_2 = b_2 \\ &&& M \succeq 0. \end{aligned} \end{equation*} Depending on the value of $a_1, b_2, a_2, b_2$, sometimes a numerical solver will report this program is infeasible (no such $M$ exists). I have experimented with multiple solvers with identical result. I can further impose that $a_1^T\cdot b_1>0, a_2^T\cdot b_2>0$, but the result is the same. An observation: If $a_1, a_2$ are orthogonal, it appears the problem is always feasible. My intuition is that the number of free variables in $M$ is $(n(n+1)/2 -n)$, because a symmetric matrix has $n(n+1)/2$ free variables, and positive semidefiniteness requires all principal minors to be positive, adding $n$ constraints. It appears this intuition is not correct. What is the requirement of $a_1, b_2, a_2, b_2$ for $M$ to exist?","Given vectors $a_1, b_2, a_2, b_2 \in \mathcal{R}^{n\times 1}$, I am interested in finding a positive semi-definite matrix $M \in \mathcal{R}^{n\times n}$, $M \succeq 0$, such that $M\cdot a_1 = b_1$, $M\cdot a_2 = b_2$. Here $n \gg 2$, say $n = 1000 $. $a_1, a_2$ are not parallel and are non-zero. To write it in equations, I want to solve the following semidefinite program \begin{equation*} \begin{aligned} & \underset{M}{\text{minimize}} & & 0 \\ & \text{subject to} & & M\cdot a_1 = b_1 \\ && & M\cdot a_2 = b_2 \\ &&& M \succeq 0. \end{aligned} \end{equation*} Depending on the value of $a_1, b_2, a_2, b_2$, sometimes a numerical solver will report this program is infeasible (no such $M$ exists). I have experimented with multiple solvers with identical result. I can further impose that $a_1^T\cdot b_1>0, a_2^T\cdot b_2>0$, but the result is the same. An observation: If $a_1, a_2$ are orthogonal, it appears the problem is always feasible. My intuition is that the number of free variables in $M$ is $(n(n+1)/2 -n)$, because a symmetric matrix has $n(n+1)/2$ free variables, and positive semidefiniteness requires all principal minors to be positive, adding $n$ constraints. It appears this intuition is not correct. What is the requirement of $a_1, b_2, a_2, b_2$ for $M$ to exist?",,"['linear-algebra', 'convex-optimization', 'numerical-optimization', 'positive-semidefinite', 'semidefinite-programming']"
97,Linear maps of polynomial roots over finite fields,Linear maps of polynomial roots over finite fields,,"Take an arbitrary degree $n$ monic irreducible polynomial $f(x)$ over $\mathbb{F}_p$ with roots $$\alpha, \alpha^p,\alpha^{p^2},\dots,\alpha^{p^{n-1}}$$ in $\mathbb{F}_p[x] / \langle f(x) \rangle$. Now, consider another degree $n$ monic irreducible $g(x)$ over $\mathbb{F}_p$. Is there a linear map which sends the roots of $f$ to the roots of $g$ in $\mathbb{F}_{p^n}\cong \mathbb{F}_p[x] / \langle f(x) \rangle$ (viewed as a vector space)? Edit: More specifically, if $\alpha,\dots,\alpha^{p^{n-1}}$ are not a normal basis for $\mathbb{F}_{p^n}$, what (if anything) can be said?","Take an arbitrary degree $n$ monic irreducible polynomial $f(x)$ over $\mathbb{F}_p$ with roots $$\alpha, \alpha^p,\alpha^{p^2},\dots,\alpha^{p^{n-1}}$$ in $\mathbb{F}_p[x] / \langle f(x) \rangle$. Now, consider another degree $n$ monic irreducible $g(x)$ over $\mathbb{F}_p$. Is there a linear map which sends the roots of $f$ to the roots of $g$ in $\mathbb{F}_{p^n}\cong \mathbb{F}_p[x] / \langle f(x) \rangle$ (viewed as a vector space)? Edit: More specifically, if $\alpha,\dots,\alpha^{p^{n-1}}$ are not a normal basis for $\mathbb{F}_{p^n}$, what (if anything) can be said?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'field-theory', 'finite-fields']"
98,Relation between exclusive-OR and modular addition in a specific function,Relation between exclusive-OR and modular addition in a specific function,,"I am trying to understand the relation between exclusive-OR (XOR) and modular addition in the function $f(x,a,b,c,d) = \bigg(\Big(\big((x \oplus a) \boxplus b\big) \oplus c\Big) \boxplus d\bigg)$ over $\mathbb{Z}_{2^n}$, where $\oplus$ denotes XOR and $\boxplus$ denotes addition modulo $2^n$. It seems that when $n=2$, for any value of $(a,b,c,d)$, I can find $31$ different $(a',b',c',d')$ such that $f(x,a,b,c,d) = f(x,a',b',c',d')$. I can explain it in the following way: bits of different operands can be flipped to get the same result at the end. For example, let's consider $(a,b,c,d) = (3,3,1,0)$. If we flip the most significant bit of $a$ and the most significant bit of $b$ we get $(a',b',c',d') = (1,1,1,0)$ and $f(x,a,b,c,d) = f(x,a',b',c',d')$ for any $x$. There are four other combinations, which explains the number of solutions $2^5 = 32$. I get $96$ results for $n=3$, $256$ for $n=4$, etc... My problem is the following: whereas some combinations remain the same regardless of the parameter $n$, it seems that operands' size impact the number of combinations. So my question is:  given $n$, and $(a,b,c,d)$ how can I find the set $\mathcal{S}$ such that $\mathcal{S} = \{(a',b',c',d') \in (\mathbb{Z}_{2^n})^4\space | \space f(x,a,b,c,d) = f(x,a',b',c',d') \space \forall x \in \mathbb{Z}_{2^n}\}$?","I am trying to understand the relation between exclusive-OR (XOR) and modular addition in the function $f(x,a,b,c,d) = \bigg(\Big(\big((x \oplus a) \boxplus b\big) \oplus c\Big) \boxplus d\bigg)$ over $\mathbb{Z}_{2^n}$, where $\oplus$ denotes XOR and $\boxplus$ denotes addition modulo $2^n$. It seems that when $n=2$, for any value of $(a,b,c,d)$, I can find $31$ different $(a',b',c',d')$ such that $f(x,a,b,c,d) = f(x,a',b',c',d')$. I can explain it in the following way: bits of different operands can be flipped to get the same result at the end. For example, let's consider $(a,b,c,d) = (3,3,1,0)$. If we flip the most significant bit of $a$ and the most significant bit of $b$ we get $(a',b',c',d') = (1,1,1,0)$ and $f(x,a,b,c,d) = f(x,a',b',c',d')$ for any $x$. There are four other combinations, which explains the number of solutions $2^5 = 32$. I get $96$ results for $n=3$, $256$ for $n=4$, etc... My problem is the following: whereas some combinations remain the same regardless of the parameter $n$, it seems that operands' size impact the number of combinations. So my question is:  given $n$, and $(a,b,c,d)$ how can I find the set $\mathcal{S}$ such that $\mathcal{S} = \{(a',b',c',d') \in (\mathbb{Z}_{2^n})^4\space | \space f(x,a,b,c,d) = f(x,a',b',c',d') \space \forall x \in \mathbb{Z}_{2^n}\}$?",,"['linear-algebra', 'combinatorics', 'finite-fields', 'binary', 'binary-operations']"
99,Are there Complex eigenvalues in a given matrix?,Are there Complex eigenvalues in a given matrix?,,"If a matrix is Hermitian, then its eigenvalues are all real. But given any real matrix that is not Hermitian , how to determine whether there are complex eigenvalues or not? Or the question can be re-formulated in this way: rather than Hermitian, are there any more general rules which can be used to determine whether the eigenvalues of a matrix are all real numbers? EDIT: I could think of another solution to this (it is only an indirect way compared with the Hermitian matrix): If a matrix is similar to a Hermitian matrix, then its eigenvalues are all real.","If a matrix is Hermitian, then its eigenvalues are all real. But given any real matrix that is not Hermitian , how to determine whether there are complex eigenvalues or not? Or the question can be re-formulated in this way: rather than Hermitian, are there any more general rules which can be used to determine whether the eigenvalues of a matrix are all real numbers? EDIT: I could think of another solution to this (it is only an indirect way compared with the Hermitian matrix): If a matrix is similar to a Hermitian matrix, then its eigenvalues are all real.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
