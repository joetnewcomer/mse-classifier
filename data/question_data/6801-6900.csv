,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving that existence of bounded partial derivatives implies continuity of a function.,Proving that existence of bounded partial derivatives implies continuity of a function.,,"Let $E \subset \mathbb{R^n}$ be open and let $f:E \to \mathbb{R}$. Suppose that ${\partial f \over \partial x_1}, ..., {\partial f \over \partial x_n}$ exist and are bounded in E. Prove that $f$ is continuous in $E$ How in the world do I prove this? I thought that the existence of partial derivatives did not imply continuity of the function. How does the condition that each partial is bounded affect that? Any hints or possibly a sketch of the proof would be welcome. Thanks. The subscript on the latter partial is an n by the way. I know it's hard to read.","Let $E \subset \mathbb{R^n}$ be open and let $f:E \to \mathbb{R}$. Suppose that ${\partial f \over \partial x_1}, ..., {\partial f \over \partial x_n}$ exist and are bounded in E. Prove that $f$ is continuous in $E$ How in the world do I prove this? I thought that the existence of partial derivatives did not imply continuity of the function. How does the condition that each partial is bounded affect that? Any hints or possibly a sketch of the proof would be welcome. Thanks. The subscript on the latter partial is an n by the way. I know it's hard to read.",,"['calculus', 'real-analysis']"
1,True/False: Self-adjoint compact operator,True/False: Self-adjoint compact operator,,"Let $H$ be a hilbert space and $T$ a compact self-adjoint operator on it. T is also injective on a dense subspace $U \subset H$ and we also have that $T(H) \subset U$. Now I am asked whether it is true that $T$ has to be injective on the whole Hilbert space? Until now, I don't know whether this is true or false. Maybe I could add that the only compact self-adjoint operator we treated in class was a Fredholm operator with a symmetric kernel function.(maybe this could serve as a counterexample).","Let $H$ be a hilbert space and $T$ a compact self-adjoint operator on it. T is also injective on a dense subspace $U \subset H$ and we also have that $T(H) \subset U$. Now I am asked whether it is true that $T$ has to be injective on the whole Hilbert space? Until now, I don't know whether this is true or false. Maybe I could add that the only compact self-adjoint operator we treated in class was a Fredholm operator with a symmetric kernel function.(maybe this could serve as a counterexample).",,"['real-analysis', 'analysis']"
2,continuity of $L^p$ norms with respect to $p$,continuity of  norms with respect to,L^p p,"Let $0<p_0<p<p<p_1\leq \infty$. Then I have proved $L^{p_0}(\mu)\cap L^{p_1}(\mu)\subseteq L^{p}(\mu)$. In particular, when $p_0=1$, $p_1=\infty$, I have proved further $||f||_\infty=lim_{p\rightarrow\infty}||f||_p$ for any $f\in L^{1}(\mu)\cap L^{\infty}(\mu)$. Does this equality hold for all $p_0,p_1$? How to prove? Does the following map continuous? \begin{eqnarray*} \Phi: L^{p_0}(\mu)\cap L^{p_1}(\mu)\times [p_0,p_1]\longrightarrow \mathbb{R},\\ (f,p)\longmapsto ||f||_p  \end{eqnarray*} ? I have no way to get any solutions. Thanks for help.","Let $0<p_0<p<p<p_1\leq \infty$. Then I have proved $L^{p_0}(\mu)\cap L^{p_1}(\mu)\subseteq L^{p}(\mu)$. In particular, when $p_0=1$, $p_1=\infty$, I have proved further $||f||_\infty=lim_{p\rightarrow\infty}||f||_p$ for any $f\in L^{1}(\mu)\cap L^{\infty}(\mu)$. Does this equality hold for all $p_0,p_1$? How to prove? Does the following map continuous? \begin{eqnarray*} \Phi: L^{p_0}(\mu)\cap L^{p_1}(\mu)\times [p_0,p_1]\longrightarrow \mathbb{R},\\ (f,p)\longmapsto ||f||_p  \end{eqnarray*} ? I have no way to get any solutions. Thanks for help.",,"['real-analysis', 'functional-analysis', 'functions', 'normed-spaces']"
3,When we can change $\int$ and $\sum$ for indefinite integral?,When we can change  and  for indefinite integral?,\int \sum,"I know, for example, that if the series $\displaystyle\sum_{n=1}^{\infty}f_n(x)$ consisting of integrable functions on a closed interval $[a, b] \subset \mathbb{R}$ converges uniformly on that closed interval, then its sum is also integrable on $[a, b]$ and $$\int_a^b \left(\sum_{n=1}^{\infty}f(x)\right)\, dx=\sum_{n=1}^{\infty} \int_a^b f_n(x)\, dx.$$ Or, Beppo-Levi lemma (we are working on $(X, \mathfrak{M}, \mu))$: Let $f_n:X \to [0,+\infty]$ be sequence of measurable functions on $X$. Then, for every measurable set $A \subset X,$ $$\int_A \sum_{n=1}^{\infty} f_n \, d\mu =\sum_{n=1}^{\infty} \int_A f_n d\mu.$$ And similar proposition. But, we are working here with definite integrals. My question is: What we can say if we are working with indefinite integrals. Is this always true, or we have some restrictions?","I know, for example, that if the series $\displaystyle\sum_{n=1}^{\infty}f_n(x)$ consisting of integrable functions on a closed interval $[a, b] \subset \mathbb{R}$ converges uniformly on that closed interval, then its sum is also integrable on $[a, b]$ and $$\int_a^b \left(\sum_{n=1}^{\infty}f(x)\right)\, dx=\sum_{n=1}^{\infty} \int_a^b f_n(x)\, dx.$$ Or, Beppo-Levi lemma (we are working on $(X, \mathfrak{M}, \mu))$: Let $f_n:X \to [0,+\infty]$ be sequence of measurable functions on $X$. Then, for every measurable set $A \subset X,$ $$\int_A \sum_{n=1}^{\infty} f_n \, d\mu =\sum_{n=1}^{\infty} \int_A f_n d\mu.$$ And similar proposition. But, we are working here with definite integrals. My question is: What we can say if we are working with indefinite integrals. Is this always true, or we have some restrictions?",,"['real-analysis', 'indefinite-integrals']"
4,Convergence of infinite sum in Schwartz class,Convergence of infinite sum in Schwartz class,,"Suppose I have a function $g$ in the Schwartz class. Consider the sum $$\dfrac{1}{L}\sum_{n=-\infty}^{\infty}g\left(\dfrac{n\pi}{L}\right)$$ In other words, just evaluate the value of $g$ at intervals of length $\pi/L$. As $L\rightarrow\infty$, does this sum always converge to $$\dfrac{1}{\pi}\int_{-\infty}^\infty g(x)dx?$$","Suppose I have a function $g$ in the Schwartz class. Consider the sum $$\dfrac{1}{L}\sum_{n=-\infty}^{\infty}g\left(\dfrac{n\pi}{L}\right)$$ In other words, just evaluate the value of $g$ at intervals of length $\pi/L$. As $L\rightarrow\infty$, does this sum always converge to $$\dfrac{1}{\pi}\int_{-\infty}^\infty g(x)dx?$$",,"['real-analysis', 'convergence-divergence']"
5,Uniform convergence of $f_n = (n^a x^2)/(n^2 +x^3)$,Uniform convergence of,f_n = (n^a x^2)/(n^2 +x^3),"My question is, if you have the sequence $$f_n = \frac{n^\alpha x^2}{n^2 +x^3}$$ on $[0, \infty)$, for values of a for $0<\alpha<2$ does the sequence uniformly converge? I guess another way to think about it is what values can $a$ take such that $$\lim_{n \rightarrow \infty} \left(\sup_{x \in [0,\infty)}\left[\frac{n^a x^2}{n^2 +x^3}\right]\right) = 0. $$ I'm having trouble proving this.. but I think that intuitively, $\alpha$ can be less than or equal to 1. Thanks for the help.","My question is, if you have the sequence $$f_n = \frac{n^\alpha x^2}{n^2 +x^3}$$ on $[0, \infty)$, for values of a for $0<\alpha<2$ does the sequence uniformly converge? I guess another way to think about it is what values can $a$ take such that $$\lim_{n \rightarrow \infty} \left(\sup_{x \in [0,\infty)}\left[\frac{n^a x^2}{n^2 +x^3}\right]\right) = 0. $$ I'm having trouble proving this.. but I think that intuitively, $\alpha$ can be less than or equal to 1. Thanks for the help.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'uniform-convergence']"
6,$n$th derivative of $(x^2-1)^n$,th derivative of,n (x^2-1)^n,"Define $R_n(x)=\dfrac{d^n}{dx^n}(x^2-1)^n$. Show that $R_n(x)$ is orthogonal to $1,x,\ldots,x^{n-1}$ in $L^2([-1,1])$. Also, what is the value of $R_n(1)$? By definition we have to show that $$\int_{-1}^1R_n(x)x^k=0$$ for $k=0,1,\ldots,n-1$. This looks a lot like integration by parts, so suppose $S_k$ is the $k$-th derivative of $(x^2-1)^n$. Then $$\int_{-1}^1R_n(x)x^k=x^kS_{n-1}(x)\mid_{-1}^1-k\int_{-1}^1x^{k-1}S_{n-1}(x)$$. It looks like the second term can be integrated by parts again, but what is the first term? Also, how would it be possible to compute $R_n(1)$?","Define $R_n(x)=\dfrac{d^n}{dx^n}(x^2-1)^n$. Show that $R_n(x)$ is orthogonal to $1,x,\ldots,x^{n-1}$ in $L^2([-1,1])$. Also, what is the value of $R_n(1)$? By definition we have to show that $$\int_{-1}^1R_n(x)x^k=0$$ for $k=0,1,\ldots,n-1$. This looks a lot like integration by parts, so suppose $S_k$ is the $k$-th derivative of $(x^2-1)^n$. Then $$\int_{-1}^1R_n(x)x^k=x^kS_{n-1}(x)\mid_{-1}^1-k\int_{-1}^1x^{k-1}S_{n-1}(x)$$. It looks like the second term can be integrated by parts again, but what is the first term? Also, how would it be possible to compute $R_n(1)$?",,"['calculus', 'real-analysis', 'measure-theory', 'orthogonal-polynomials']"
7,Counterexample to second derivative test when f''(x) is not continuously differentiable,Counterexample to second derivative test when f''(x) is not continuously differentiable,,"When looking over true/false questions on previous midterms, one of my conscientious students said: ""If f is defined on an open interval containing c, f'(c)=0, and f''(c)>0, then c is a local min of f"" was false because one of the hypotheses for the second derivative test (at least in Stewart) is that the second derivative is continuous in a neighborhood of c. Can anyone think of a counterexample for this statement (in one real variable)? It's apparently been too long since I've taken an analysis class to come up with something clever.","When looking over true/false questions on previous midterms, one of my conscientious students said: ""If f is defined on an open interval containing c, f'(c)=0, and f''(c)>0, then c is a local min of f"" was false because one of the hypotheses for the second derivative test (at least in Stewart) is that the second derivative is continuous in a neighborhood of c. Can anyone think of a counterexample for this statement (in one real variable)? It's apparently been too long since I've taken an analysis class to come up with something clever.",,['calculus']
8,"A continuous function that is uniformly continuous on two sets, but not uniformly continuous on the union of these two sets?","A continuous function that is uniformly continuous on two sets, but not uniformly continuous on the union of these two sets?",,"This homework problem has just cost me 3 hours... But I still have no clue what it can be... Let $A, B \subseteq \mathbb{R}$. Find a continuous function $f:A\cup B \to \mathbb{R}$ where $f$ is uniformly continuous on $A$ and on $B$, but $f$ is not uniformly continuous on $A\cup B$.","This homework problem has just cost me 3 hours... But I still have no clue what it can be... Let $A, B \subseteq \mathbb{R}$. Find a continuous function $f:A\cup B \to \mathbb{R}$ where $f$ is uniformly continuous on $A$ and on $B$, but $f$ is not uniformly continuous on $A\cup B$.",,['real-analysis']
9,proving that the quotient linear map of a continuous linear map is also continuous (normed spaces),proving that the quotient linear map of a continuous linear map is also continuous (normed spaces),,"Let $X,Y$ be a normed vector spaces over $\mathbb K $ , $T:X\to Y$ a $\mathbb K$ -linear continuous map ( $\mathbb K$ could be $\mathbb R$ or $\mathbb C$ ). Let's consider $ \hat T: X/\ker T \to Y$ the induced linear map ( $\hat T (\bar x)=T(x)$ . This map it's well defined and it's clearly injective. We consider the usual quotient norm. I want to prove that $\hat T$ it's also continuous and also $\|\hat T\|=\|T\|$ . I only proved that $\|T\|\le \|\hat T\|$ if I prove the other inequality, then I'm done with the continuity but I don't know how.","Let be a normed vector spaces over , a -linear continuous map ( could be or ). Let's consider the induced linear map ( . This map it's well defined and it's clearly injective. We consider the usual quotient norm. I want to prove that it's also continuous and also . I only proved that if I prove the other inequality, then I'm done with the continuity but I don't know how.","X,Y \mathbb K  T:X\to Y \mathbb K \mathbb K \mathbb R \mathbb C  \hat T: X/\ker T \to Y \hat T (\bar x)=T(x) \hat T \|\hat T\|=\|T\| \|T\|\le \|\hat T\|","['real-analysis', 'functional-analysis', 'normed-spaces']"
10,"If $f$ is a continuous periodic function with irrational period and if $\sum_n\frac{|f(n)|}{n}<\infty$, then $f$ is identically zero.","If  is a continuous periodic function with irrational period and if , then  is identically zero.",f \sum_n\frac{|f(n)|}{n}<\infty f,"Please show that if $f:\mathbb{R}\to\mathbb{R}$ is a continuous periodic function with irrational period and if $\sum_n\frac{|f(n)|}{n}<\infty$, then $f$ is identically zero. (For example, using this we know $\sum_{n\ge1}\frac{|\sin n|}{n}$ diverges.) The book mentions the ""so-called equidistribution criterion""; I'm not sure if it's referring to this: http://en.wikipedia.org/wiki/Equidistribution_theorem )","Please show that if $f:\mathbb{R}\to\mathbb{R}$ is a continuous periodic function with irrational period and if $\sum_n\frac{|f(n)|}{n}<\infty$, then $f$ is identically zero. (For example, using this we know $\sum_{n\ge1}\frac{|\sin n|}{n}$ diverges.) The book mentions the ""so-called equidistribution criterion""; I'm not sure if it's referring to this: http://en.wikipedia.org/wiki/Equidistribution_theorem )",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'periodic-functions']"
11,Prove $(f+g)'(x) = f'(x) + g'(x)$,Prove,(f+g)'(x) = f'(x) + g'(x),"In Rudin's textbook, ""Principles of Mathematical Analysis"", theorem 5.3 says: If $f$ and $g$ are defined on $[a, b]$ and are differentiable at a point $x \in [a,b]$, then   $$(f+g)'(x) = f'(x) + g'(x)$$ Rudin said this statement is clear by theorem 4.4, but I tried to prove it by myself. Could you tell me if my way is correct? My proof: \begin{align} (f+g)'(x) &= \lim_{t\to x} \frac{(f+g)(t) - (f+g)(x)}{(t-x)}\\ &= \lim _{t\to x} \frac{f(t) + g(t) - f(x) - g(x)}{(t-x)}\\ &= \lim _{t\to x} \frac{f(t)-f(x)}{t-x} + \lim _{t\to x} \frac{g(t)- g(x)}{t-x}\\ &= f'(x) + g'(x) \end{align}","In Rudin's textbook, ""Principles of Mathematical Analysis"", theorem 5.3 says: If $f$ and $g$ are defined on $[a, b]$ and are differentiable at a point $x \in [a,b]$, then   $$(f+g)'(x) = f'(x) + g'(x)$$ Rudin said this statement is clear by theorem 4.4, but I tried to prove it by myself. Could you tell me if my way is correct? My proof: \begin{align} (f+g)'(x) &= \lim_{t\to x} \frac{(f+g)(t) - (f+g)(x)}{(t-x)}\\ &= \lim _{t\to x} \frac{f(t) + g(t) - f(x) - g(x)}{(t-x)}\\ &= \lim _{t\to x} \frac{f(t)-f(x)}{t-x} + \lim _{t\to x} \frac{g(t)- g(x)}{t-x}\\ &= f'(x) + g'(x) \end{align}",,['real-analysis']
12,Help to prove bijection between subset of $S^2$ and $\mathbb{R}^2$,Help to prove bijection between subset of  and,S^2 \mathbb{R}^2,"Well, I've been solving some exercises about bijections between subsets of $\mathbb{R}^n$ and $\mathbb{R}^m$ to get practice with this and I'm stuck with this one. Let $S^2 \subset \mathbb{R}^3$ be the unit sphere, and let $N = (0,0,1)$. I want to show that the map $f : S^2 \setminus \{N\} \to \mathbb{R}^2$ given by: $$f(x,y,z)=\left(\frac{2x}{1-z},\frac{2y}{1-z}\right)$$ is a bijection. Well, I've tried going direct from the definition. First injectivity: I have to show that $f(x_1,y_1,z_1)=f(x_2,y_2,z_2)$ implies $x_1=x_2$, $y_1=y_2$ and $z_1=z_2$. The hypothesis gives $$\begin{cases}\frac{x_1}{1-z_1}=\frac{x_2}{1-z_2}\\ \frac{y_1}{1-z_1}=\frac{y_2}{1-z_2}\end{cases}$$ From this, substituting $1-z_1$ on the second equation by $x_1(1-z_2)/x_2$ I've found that this implies that $x_1y_2=x_2y_1$, but I couldn't proceed any further from this. To show that $f$ is surjective, I have to show that given $(a,b) \in \mathbb{R}^2$ there's some $(x,y,z)\in S^2\setminus \{N\}$ such that $(a,b)=f(x,y,z)$, in other words, such that: I've tried proving surjectivity looking at the expression of $f$ and thinking ""what $(x,y,z)$ I must put in there in terms of $(a,b)$ to get $f(x,y,z)=(a,b)$""? I've found then that $(a/2,b/2,0)$ does the job, but this point is not necessarily in $S^2\setminus\{N\}$. I should find $(x,y,z)$ such that $f(x,y,z)=(a,b)$ and $x^2+y^2+z^2=1$. But, just looking a long time to these two conditions, I find very difficult to take a guess. Is there a better way to procede? Thanks very much in advance!","Well, I've been solving some exercises about bijections between subsets of $\mathbb{R}^n$ and $\mathbb{R}^m$ to get practice with this and I'm stuck with this one. Let $S^2 \subset \mathbb{R}^3$ be the unit sphere, and let $N = (0,0,1)$. I want to show that the map $f : S^2 \setminus \{N\} \to \mathbb{R}^2$ given by: $$f(x,y,z)=\left(\frac{2x}{1-z},\frac{2y}{1-z}\right)$$ is a bijection. Well, I've tried going direct from the definition. First injectivity: I have to show that $f(x_1,y_1,z_1)=f(x_2,y_2,z_2)$ implies $x_1=x_2$, $y_1=y_2$ and $z_1=z_2$. The hypothesis gives $$\begin{cases}\frac{x_1}{1-z_1}=\frac{x_2}{1-z_2}\\ \frac{y_1}{1-z_1}=\frac{y_2}{1-z_2}\end{cases}$$ From this, substituting $1-z_1$ on the second equation by $x_1(1-z_2)/x_2$ I've found that this implies that $x_1y_2=x_2y_1$, but I couldn't proceed any further from this. To show that $f$ is surjective, I have to show that given $(a,b) \in \mathbb{R}^2$ there's some $(x,y,z)\in S^2\setminus \{N\}$ such that $(a,b)=f(x,y,z)$, in other words, such that: I've tried proving surjectivity looking at the expression of $f$ and thinking ""what $(x,y,z)$ I must put in there in terms of $(a,b)$ to get $f(x,y,z)=(a,b)$""? I've found then that $(a/2,b/2,0)$ does the job, but this point is not necessarily in $S^2\setminus\{N\}$. I should find $(x,y,z)$ such that $f(x,y,z)=(a,b)$ and $x^2+y^2+z^2=1$. But, just looking a long time to these two conditions, I find very difficult to take a guess. Is there a better way to procede? Thanks very much in advance!",,"['real-analysis', 'elementary-set-theory']"
13,"If $f$ is differentiable at $a$, then all first order partial derivatives of $f$ exist at $a$","If  is differentiable at , then all first order partial derivatives of  exist at",f a f a,"Let $f$ be a vector function. If $f$ is differentiable at $a$, then all first order partial derivatives of $f$ exist at $a$ Moreover, the total derivative of $f$ at $a$ is unique and can be compute by the following: $$ Df(a) = \left[\frac{\partial f_i}{\partial x_j}(a) \right]_{m \times n} := \begin{bmatrix}   \frac{\partial f_1}{\partial x_1}(a)  &\ldots &\frac{\partial f_1}{\partial x_n}(a) \\ \vdots &\ddots &\vdots \\ \frac{\partial f_m}{\partial x_1}(a) &\ldots &\frac{\partial f_m}{\partial x_n}(a) \end{bmatrix} $$ Please help me to prove? This is in fact a theorem. But I need to learn its proof. Please can somebody prove tags step by step. Thank you so much. I Will be happy if one can teachs me its proof.","Let $f$ be a vector function. If $f$ is differentiable at $a$, then all first order partial derivatives of $f$ exist at $a$ Moreover, the total derivative of $f$ at $a$ is unique and can be compute by the following: $$ Df(a) = \left[\frac{\partial f_i}{\partial x_j}(a) \right]_{m \times n} := \begin{bmatrix}   \frac{\partial f_1}{\partial x_1}(a)  &\ldots &\frac{\partial f_1}{\partial x_n}(a) \\ \vdots &\ddots &\vdots \\ \frac{\partial f_m}{\partial x_1}(a) &\ldots &\frac{\partial f_m}{\partial x_n}(a) \end{bmatrix} $$ Please help me to prove? This is in fact a theorem. But I need to learn its proof. Please can somebody prove tags step by step. Thank you so much. I Will be happy if one can teachs me its proof.",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'partial-derivative']"
14,Check my answer: $\Bbb Q$ is neither open nor closed,Check my answer:  is neither open nor closed,\Bbb Q,"I have a so easy question. I have done it's answer by myself. I want you to only check my answer please. Does there exist any mistake or the missing? The set of irrational numbers - $\Bbb Q$ is neither open nor closed. Proof: Assume $\Bbb Q$ were open. There would be a neighborhood of $0$, and so an interval containing $0$ lying entirely within $\Bbb Q$. However, each such interval contains irrational numbers, which is a contradiction. suppose $\Bbb Q$ were closed. $\Bbb R-\Bbb Q$ is open. There is a neighborhood of $\pi$ and therefore an interval containing $\pi$ lying completely within $\Bbb R-\Bbb Q$ . however again each such interval contains rational numbers, which is a contradiction.","I have a so easy question. I have done it's answer by myself. I want you to only check my answer please. Does there exist any mistake or the missing? The set of irrational numbers - $\Bbb Q$ is neither open nor closed. Proof: Assume $\Bbb Q$ were open. There would be a neighborhood of $0$, and so an interval containing $0$ lying entirely within $\Bbb Q$. However, each such interval contains irrational numbers, which is a contradiction. suppose $\Bbb Q$ were closed. $\Bbb R-\Bbb Q$ is open. There is a neighborhood of $\pi$ and therefore an interval containing $\pi$ lying completely within $\Bbb R-\Bbb Q$ . however again each such interval contains rational numbers, which is a contradiction.",,"['real-analysis', 'general-topology', 'irrational-numbers']"
15,How can I prove this monster inequality?,How can I prove this monster inequality?,,"Given fixed values $\alpha \in (0,1)$ and $ n \in \Bbb N$. For $k \in \{1,...,n-1\}$, consider the function $$f(k)=\left(1-\alpha\sqrt{\frac{n-k}{k(n-1)}}\right)^k\left(1+\alpha\sqrt{\frac{k}{(n-k)(n-1)}}\right)^{n-k}$$. How can I show that, for all $k \in \{1,...,n-1\}$, $$f(k) \ge \left(1-\alpha\right)\left(1+\frac{\alpha}{n-1}\right)^{n-1}$$ (where equality occurs exactly at $k=1$)? EDIT 1: I tried showing that $f$ is monotoniacally increasing in $k$: $$\frac{d}{dk} \log (f(k))= \log\left(1-\alpha\sqrt{\frac{n-k}{k(n-1)}}\right)-\log\left(1+\alpha\sqrt{\frac{k}{(n-k)(n-1)}}\right)+\frac{\alpha n}{2(\sqrt{k(n-k)(n-1)}-\alpha (n-k))}+\frac{\alpha n}{2(\sqrt{k(n-k)(n-1)}+\alpha k)}$$ which should be $\ge 0$, but I don't see how to make the right estimates. EDIT 2: To the background of this inequality: This is what remains to be shown when you apply Lagrange multipliers to this inequality . If anyone has an idea how to procede in that inequality without using Lagrange multipliers, I would be equally content.","Given fixed values $\alpha \in (0,1)$ and $ n \in \Bbb N$. For $k \in \{1,...,n-1\}$, consider the function $$f(k)=\left(1-\alpha\sqrt{\frac{n-k}{k(n-1)}}\right)^k\left(1+\alpha\sqrt{\frac{k}{(n-k)(n-1)}}\right)^{n-k}$$. How can I show that, for all $k \in \{1,...,n-1\}$, $$f(k) \ge \left(1-\alpha\right)\left(1+\frac{\alpha}{n-1}\right)^{n-1}$$ (where equality occurs exactly at $k=1$)? EDIT 1: I tried showing that $f$ is monotoniacally increasing in $k$: $$\frac{d}{dk} \log (f(k))= \log\left(1-\alpha\sqrt{\frac{n-k}{k(n-1)}}\right)-\log\left(1+\alpha\sqrt{\frac{k}{(n-k)(n-1)}}\right)+\frac{\alpha n}{2(\sqrt{k(n-k)(n-1)}-\alpha (n-k))}+\frac{\alpha n}{2(\sqrt{k(n-k)(n-1)}+\alpha k)}$$ which should be $\ge 0$, but I don't see how to make the right estimates. EDIT 2: To the background of this inequality: This is what remains to be shown when you apply Lagrange multipliers to this inequality . If anyone has an idea how to procede in that inequality without using Lagrange multipliers, I would be equally content.",,"['real-analysis', 'inequality']"
16,A convex function that is bounded on a neighborhood is Lipschitz,A convex function that is bounded on a neighborhood is Lipschitz,,"Let consider a normed vector space $V$. I want to prove that If $f:V\to \mathbb R$ is a convex function and if for some $x_0 \in V$ the function is bounded on a neighborhood $W$ of $x_0$, then there exists a neighborhood $U$ of $x_0$ such that $f$ is Lipschitz on    $U$. When I say neighborhood of $x_0$ I mean an open set that contains $x_0$ WLOG the neighborhood clearly can be considered to be a ball with center $x_0$. I don't know what can I do, because I can compute directly $f(x)-f(y)$ because the convexity only works with positive scalars. So I'm a little confused, please help me!","Let consider a normed vector space $V$. I want to prove that If $f:V\to \mathbb R$ is a convex function and if for some $x_0 \in V$ the function is bounded on a neighborhood $W$ of $x_0$, then there exists a neighborhood $U$ of $x_0$ such that $f$ is Lipschitz on    $U$. When I say neighborhood of $x_0$ I mean an open set that contains $x_0$ WLOG the neighborhood clearly can be considered to be a ball with center $x_0$. I don't know what can I do, because I can compute directly $f(x)-f(y)$ because the convexity only works with positive scalars. So I'm a little confused, please help me!",,"['real-analysis', 'functional-analysis', 'convex-analysis']"
17,Prove that a standard torus is diffeomorphic to $ \mathbb S^1\times \mathbb S^1$,Prove that a standard torus is diffeomorphic to, \mathbb S^1\times \mathbb S^1,"I was asked to prove that a standard torus(which means we don't consider those pathological cases where it intersects with itself, e.g horn torus) is diffeomorphic to $ \mathbb S^1\times \mathbb S^1$. I was thinking if we could prove it this way: Since every point on the torus can be uniquely defined with a pair of angles $(\theta_1, \theta_2)$. Then we construct a diffeomorphism $\phi(\theta_1, \theta_2)=(\tilde{\theta}_1 ,\tilde{\theta}_2)$ which maps every point on the torus to every point on $\mathbb S^1 \times \mathbb S^1$. Since the map is definitely bijective and smooth with a smooth inverse. We're basically done... THERE MUST BE SOMETHING WRONG I THINK. Thanks a lot for everyone's help!","I was asked to prove that a standard torus(which means we don't consider those pathological cases where it intersects with itself, e.g horn torus) is diffeomorphic to $ \mathbb S^1\times \mathbb S^1$. I was thinking if we could prove it this way: Since every point on the torus can be uniquely defined with a pair of angles $(\theta_1, \theta_2)$. Then we construct a diffeomorphism $\phi(\theta_1, \theta_2)=(\tilde{\theta}_1 ,\tilde{\theta}_2)$ which maps every point on the torus to every point on $\mathbb S^1 \times \mathbb S^1$. Since the map is definitely bijective and smooth with a smooth inverse. We're basically done... THERE MUST BE SOMETHING WRONG I THINK. Thanks a lot for everyone's help!",,"['real-analysis', 'general-topology', 'analysis', 'differential-geometry', 'differential-topology']"
18,Continuity of function given as a maximum,Continuity of function given as a maximum,,"Let $f(x,y)$ is continuous in $[a,b]\times[c,d]$, and we define the function $g(y)$ as follows $$g(y):=\max_{x\in[a,b]}f(x,y),\quad\forall y\in[c,d].\tag{1}$$ The question is when we can conclude that $g\in C[c,d]$, or provide a counterexample to show $g$ is not a continuous function again? Any answer will be appreciated!","Let $f(x,y)$ is continuous in $[a,b]\times[c,d]$, and we define the function $g(y)$ as follows $$g(y):=\max_{x\in[a,b]}f(x,y),\quad\forall y\in[c,d].\tag{1}$$ The question is when we can conclude that $g\in C[c,d]$, or provide a counterexample to show $g$ is not a continuous function again? Any answer will be appreciated!",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
19,How can I show whether the series $\sum\limits_{n=1}^\infty \frac{(-1)^n}{n(2+(-1)^n)} $ converges or diverges?,How can I show whether the series  converges or diverges?,\sum\limits_{n=1}^\infty \frac{(-1)^n}{n(2+(-1)^n)} ,"While revising for exams, I came across a question where at the end of it, we had to determine whether the below series was convergent or divergent: $$\sum_{n=1}^\infty \frac{(-1)^n}{n(2+(-1)^n)} $$ Unfortunately, other than knowing that as the series $ \sum_{n=1}^\infty \frac{(-1)^n}{n} $ converges, this series will most likely converge as the $(2 + (-1)^n)$ terms are bounded, I can't see a way of using this to determine if it converges or not. Here's a list of some of the other ideas I've tried, which don't seem to get me anywhere: Generalising the series - By this, I mean replacing the $-1$ for $z$ and then try and use the ratio test to see whether $-1$ is inside or outside or on the boundary of the circle of convergence - however, when I did so, I believe that the ratio test is inconclusive for all values of $|z|$, which while disappointing, is an interesting feature of the series. Summation by parts - While this is usually a nice tool to use for when dealing with awkward sums, I can't see any good choice of sequences $ (a_n) $ and $(b_n)$ to choose to use. Summing consecutive terms - By this, I mean evaluating the series by looking at the sum of $$\sum_{k=1}^\infty \frac{(-1)^{2k}}{2k(2+(-1)^{2k})} + \frac{(-1)^{2k-1}}{(2k-1)(2+(-1)^{2k-1})}$$ $$ \Rightarrow \sum_{k=1}^\infty \frac{1}{2k} - \frac{1}{3(2k-1)}$$ $\quad$ but then as we are left with parts of the harmonic series this doesn't seem like a nice way to  $\quad$evaluate it. Other than that, I'm completely out of ideas, so any new ones (or ways to make my old ones work) would be much appreciated!","While revising for exams, I came across a question where at the end of it, we had to determine whether the below series was convergent or divergent: $$\sum_{n=1}^\infty \frac{(-1)^n}{n(2+(-1)^n)} $$ Unfortunately, other than knowing that as the series $ \sum_{n=1}^\infty \frac{(-1)^n}{n} $ converges, this series will most likely converge as the $(2 + (-1)^n)$ terms are bounded, I can't see a way of using this to determine if it converges or not. Here's a list of some of the other ideas I've tried, which don't seem to get me anywhere: Generalising the series - By this, I mean replacing the $-1$ for $z$ and then try and use the ratio test to see whether $-1$ is inside or outside or on the boundary of the circle of convergence - however, when I did so, I believe that the ratio test is inconclusive for all values of $|z|$, which while disappointing, is an interesting feature of the series. Summation by parts - While this is usually a nice tool to use for when dealing with awkward sums, I can't see any good choice of sequences $ (a_n) $ and $(b_n)$ to choose to use. Summing consecutive terms - By this, I mean evaluating the series by looking at the sum of $$\sum_{k=1}^\infty \frac{(-1)^{2k}}{2k(2+(-1)^{2k})} + \frac{(-1)^{2k-1}}{(2k-1)(2+(-1)^{2k-1})}$$ $$ \Rightarrow \sum_{k=1}^\infty \frac{1}{2k} - \frac{1}{3(2k-1)}$$ $\quad$ but then as we are left with parts of the harmonic series this doesn't seem like a nice way to  $\quad$evaluate it. Other than that, I'm completely out of ideas, so any new ones (or ways to make my old ones work) would be much appreciated!",,"['real-analysis', 'sequences-and-series']"
20,"Continuous function differentiable on $[0,1]\setminus\mathbb{Q}$, but nondifferentiable on all of $\mathbb{Q}\cap[0,1]$?","Continuous function differentiable on , but nondifferentiable on all of ?","[0,1]\setminus\mathbb{Q} \mathbb{Q}\cap[0,1]","I'm trying to work out an example of a continuous function which is differentiable at all irrationals but nondifferentiable at all rationals in $[0,1]$. Since $\mathbb{Q}$ is countable, list it as $\mathbb{Q}=\{q_0,q_1,q_2,\dots\}$. Define a map $g\colon\mathbb{Q}\to\mathbb{R}$ by $q_n\mapsto 2^{-n}$. Since $\sum_{n=0}^\infty\frac{1}{2^n}$ is absolutely convergent, $\sum_{r\in\mathbb{Q}}g(r)$ is too. Then define $f\colon [0,1]\to\mathbb{R}$ by $$ f(x)=\sum_{r\in\mathbb{Q};r<x}g(r) $$ which is well defined. It is not hard to see that $f$ is monotonically increasing on $[0,1]$, and thus Riemann integrable on $[0,1]$. I've been able to show that $f$ is continuous at all irrationals, but discontinuous at all rationals. I can add this if needed. By the fundamental theorem of calculus, the function $F\colon [0,1]\to\mathbb{R}$ defined by $$ F(x)=\int_0^x f $$is continuous, and differentiable at all irrationals since $f$ is continuous at all irrationals. The example on page 7 of these notes, Math 131AH Winter 2003, Prof. Terry Tao remarks that $F$ is actually nondifferentiable at every rational, by use of the mean value theorem. I'm confused because I don't see how I can apply the mean value theorem. I don't think I'm intended to apply it to $f$, since $f$ is not differentiable on any nondegenerate interval. Also, although $F$ is continuous, I don't think I can apply the mean value theorem to it without assuming that $F$ is differentiable on all the rationals in some interval, which seems like a large assumption. How can the mean value theorem (if necessary), show that $F$ is discontinuous at all rationals? Thanks.","I'm trying to work out an example of a continuous function which is differentiable at all irrationals but nondifferentiable at all rationals in $[0,1]$. Since $\mathbb{Q}$ is countable, list it as $\mathbb{Q}=\{q_0,q_1,q_2,\dots\}$. Define a map $g\colon\mathbb{Q}\to\mathbb{R}$ by $q_n\mapsto 2^{-n}$. Since $\sum_{n=0}^\infty\frac{1}{2^n}$ is absolutely convergent, $\sum_{r\in\mathbb{Q}}g(r)$ is too. Then define $f\colon [0,1]\to\mathbb{R}$ by $$ f(x)=\sum_{r\in\mathbb{Q};r<x}g(r) $$ which is well defined. It is not hard to see that $f$ is monotonically increasing on $[0,1]$, and thus Riemann integrable on $[0,1]$. I've been able to show that $f$ is continuous at all irrationals, but discontinuous at all rationals. I can add this if needed. By the fundamental theorem of calculus, the function $F\colon [0,1]\to\mathbb{R}$ defined by $$ F(x)=\int_0^x f $$is continuous, and differentiable at all irrationals since $f$ is continuous at all irrationals. The example on page 7 of these notes, Math 131AH Winter 2003, Prof. Terry Tao remarks that $F$ is actually nondifferentiable at every rational, by use of the mean value theorem. I'm confused because I don't see how I can apply the mean value theorem. I don't think I'm intended to apply it to $f$, since $f$ is not differentiable on any nondegenerate interval. Also, although $F$ is continuous, I don't think I can apply the mean value theorem to it without assuming that $F$ is differentiable on all the rationals in some interval, which seems like a large assumption. How can the mean value theorem (if necessary), show that $F$ is discontinuous at all rationals? Thanks.",,"['real-analysis', 'analysis']"
21,Does there exist a smooth function which is nowhere analytic? [duplicate],Does there exist a smooth function which is nowhere analytic? [duplicate],,"This question already has an answer here : A smooth function's domain of being non-analytic (1 answer) Closed 11 years ago . Smooth means has derivatives of all order, and analytic means can be given as a convergence of power series.","This question already has an answer here : A smooth function's domain of being non-analytic (1 answer) Closed 11 years ago . Smooth means has derivatives of all order, and analytic means can be given as a convergence of power series.",,['real-analysis']
22,$n$th derivative of $\ln f(x)$,th derivative of,n \ln f(x),"I've been trying to find the $n$th derivative of the function $h(x)=\ln f(x)$. Wolfram|Alpha and Gradshteyn's Table of Integrals, Series and Products give   $$     \frac{d^n}{dx^n}\ln f(x)=-\sum_{k=1}^n\frac{(-1)^k\left[\binom{n}{k}\frac{d^n}{dx^n}\left(f(x)^k\right)\right]}{kf(x)^k}.  $$ Notice, however, that the $\frac{d^n}{dx^n}\left(f(x)^k\right)$ is not explicited.  Again, Gradshteyn says    $$     \frac{d^n}{dx^n}\left(f(x)^k\right) =        k\binom{n-k}{n}\left[\sum_{\ell=0}^n\frac{f(x)^{p-n}}{p-n}\frac{d^n(f(x)^\ell)}{dx^n}\right]   $$ which is implicit. Knowing that $k\leq n$, is there a way to write the $n$th derivative of $\ln f(x)$ in a way that makes clear the dependance on   $$  \frac{d^m}{dx^m}f(x)\qquad\text{and}\qquad \left(\frac{df(x)}{dx}\right)^\ell?  $$ I've looked at Leibniz formula for the $n$th derivative of a product with product $f'/f$, the logarithmic derivative and Faà di Bruno's formula, but gotten nowhere.","I've been trying to find the $n$th derivative of the function $h(x)=\ln f(x)$. Wolfram|Alpha and Gradshteyn's Table of Integrals, Series and Products give   $$     \frac{d^n}{dx^n}\ln f(x)=-\sum_{k=1}^n\frac{(-1)^k\left[\binom{n}{k}\frac{d^n}{dx^n}\left(f(x)^k\right)\right]}{kf(x)^k}.  $$ Notice, however, that the $\frac{d^n}{dx^n}\left(f(x)^k\right)$ is not explicited.  Again, Gradshteyn says    $$     \frac{d^n}{dx^n}\left(f(x)^k\right) =        k\binom{n-k}{n}\left[\sum_{\ell=0}^n\frac{f(x)^{p-n}}{p-n}\frac{d^n(f(x)^\ell)}{dx^n}\right]   $$ which is implicit. Knowing that $k\leq n$, is there a way to write the $n$th derivative of $\ln f(x)$ in a way that makes clear the dependance on   $$  \frac{d^m}{dx^m}f(x)\qquad\text{and}\qquad \left(\frac{df(x)}{dx}\right)^\ell?  $$ I've looked at Leibniz formula for the $n$th derivative of a product with product $f'/f$, the logarithmic derivative and Faà di Bruno's formula, but gotten nowhere.",,"['real-analysis', 'derivatives']"
23,Relevance of smoothness.,Relevance of smoothness.,,"What is the real relevance of a function being smooth, other than this being necessary for analyticity. What is the real problem if the 4323rd derivative has a discontinuity?","What is the real relevance of a function being smooth, other than this being necessary for analyticity. What is the real problem if the 4323rd derivative has a discontinuity?",,"['real-analysis', 'analysis']"
24,Prove that $\int_{0}^{\infty }\frac{x^{a-3/2}dx}{[ x^2+( b^2-2)x+1]^a}=b^{1-2a}\frac{\Gamma(1/2)\Gamma(a-1/2)}{\Gamma(a)}$,Prove that,\int_{0}^{\infty }\frac{x^{a-3/2}dx}{[ x^2+( b^2-2)x+1]^a}=b^{1-2a}\frac{\Gamma(1/2)\Gamma(a-1/2)}{\Gamma(a)},"How can one prove that $$I\left( a,b \right)= \int_{0}^{\infty }\frac{x^{a-\frac{3}{2}}dx}{\left[ x^2+\left( b^2-2 \right)x+1 \right]^a}=b^{1-2a}\frac{\Gamma \left( \frac{1}{2} \right)\Gamma \left( a-\frac{1}{2} \right)}{\Gamma \left( a \right)},\ $$ where $a>\frac12,\ b\in \mathbb{R}^+$?","How can one prove that $$I\left( a,b \right)= \int_{0}^{\infty }\frac{x^{a-\frac{3}{2}}dx}{\left[ x^2+\left( b^2-2 \right)x+1 \right]^a}=b^{1-2a}\frac{\Gamma \left( \frac{1}{2} \right)\Gamma \left( a-\frac{1}{2} \right)}{\Gamma \left( a \right)},\ $$ where $a>\frac12,\ b\in \mathbb{R}^+$?",,"['calculus', 'real-analysis', 'integration', 'improper-integrals', 'gamma-function']"
25,"Assume that:$f(0) \geq 0$ and $f^\prime(x) \geq f(x)$ show $f(x)\geq 0 \forall x \in (0,\infty)$",Assume that: and  show,"f(0) \geq 0 f^\prime(x) \geq f(x) f(x)\geq 0 \forall x \in (0,\infty)","$f$  is continuous on $[0,\infty)$ and differentiable on $(0,\infty)$ and $f(0) \geq 0$ and $f^\prime(x) \geq f(x)$ to show $f(x)\geq 0 \forall x \in (0,\infty)$ my answer: if $\exists x_0 \in (0,\infty)$ s.t.  $f(x_0)<0$ then $\frac{f(x_0)-f(0)}{x_0} = f^\prime(a_1) \leq 0$ now $a_1 \in (0,x_0)  $ $\frac{f(a_1)-f(0)}{a_1}=f^\prime(a_2) \leq 0$ continuing this way we get a sequence $(a_i) \rightarrow 0$ s.t.$f^\prime(a_i)\leq 0 \leq f(a_i)$ which is a contradiction is this correct or not?","$f$  is continuous on $[0,\infty)$ and differentiable on $(0,\infty)$ and $f(0) \geq 0$ and $f^\prime(x) \geq f(x)$ to show $f(x)\geq 0 \forall x \in (0,\infty)$ my answer: if $\exists x_0 \in (0,\infty)$ s.t.  $f(x_0)<0$ then $\frac{f(x_0)-f(0)}{x_0} = f^\prime(a_1) \leq 0$ now $a_1 \in (0,x_0)  $ $\frac{f(a_1)-f(0)}{a_1}=f^\prime(a_2) \leq 0$ continuing this way we get a sequence $(a_i) \rightarrow 0$ s.t.$f^\prime(a_i)\leq 0 \leq f(a_i)$ which is a contradiction is this correct or not?",,"['calculus', 'real-analysis', 'derivatives', 'inequality']"
26,Proving the equivalence of norms is an equivalence relation,Proving the equivalence of norms is an equivalence relation,,"Two norms $\Vert -\Vert _1 $ , $\Vert -\Vert _2$ are equivalent if: for two constants $a,b$ and $x$ from $V$ a vector space over a field it holds that: $$a\Vert x\Vert _1\leqslant \Vert x\Vert _2\leqslant  b\Vert x\Vert _1.$$ This is a equivalence relation because: $$a\Vert x\Vert _1\leqslant \Vert x\Vert _2\leqslant  b\Vert x\Vert _1$$ and $$c\Vert x\Vert _2\leqslant  \Vert x\Vert _3 \leqslant  d\Vert x\Vert _2$$ it follows that there are constants such that (transitivity): $$e\Vert x\Vert _1 \leqslant  \Vert x\Vert _3 \leqslant  f\Vert x\Vert _1$$ Reflexivity: $$a\Vert x\Vert _1\leqslant  \Vert x\Vert _1\leqslant  b\Vert x\Vert _1$$ with $a,b = 1$ ; this is true. Symmetry: $$a\Vert x\Vert _1 \leqslant  \Vert x\Vert _2 \leqslant  b\Vert x\Vert _1$$ if we take: $$-\frac{1}{b}\Vert x\Vert _2\leqslant  \Vert x \Vert_1 \leqslant  \frac{-1}{a} \Vert x\Vert _2.$$ Is this a valid proof  that the equivalence of two norms is truly a equivalence relationship? Attempt 2 : Symmetry: $$a\Vert x\Vert _1 \leqslant  \Vert x\Vert _2 \leqslant  b\Vert x\Vert _1$$ $\Rightarrow $ : $$\frac{1}{b} \Vert x\Vert _2 \leqslant  \Vert x\Vert _1 \leqslant  \frac{1}{a}\Vert x\Vert _2.$$ Transitivity: given $$a\Vert x\Vert _1\leqslant \Vert x\Vert _2\leqslant  b\Vert x\Vert _1$$ and $$c\Vert x\Vert _2\leqslant  \Vert x\Vert _3 \leqslant  d\Vert x\Vert _2$$ $\Rightarrow$ : $$ac \Vert x\Vert _1 \leqslant  c\Vert x\Vert _2\leqslant  \Vert x\Vert _3 \leqslant d\Vert x\Vert _2\leqslant  db\Vert x\Vert _1.$$","Two norms , are equivalent if: for two constants and from a vector space over a field it holds that: This is a equivalence relation because: and it follows that there are constants such that (transitivity): Reflexivity: with ; this is true. Symmetry: if we take: Is this a valid proof  that the equivalence of two norms is truly a equivalence relationship? Attempt 2 : Symmetry: : Transitivity: given and :","\Vert -\Vert _1  \Vert -\Vert _2 a,b x V a\Vert x\Vert _1\leqslant \Vert x\Vert _2\leqslant  b\Vert x\Vert _1. a\Vert x\Vert _1\leqslant \Vert x\Vert _2\leqslant  b\Vert x\Vert _1 c\Vert x\Vert _2\leqslant  \Vert x\Vert _3 \leqslant  d\Vert x\Vert _2 e\Vert x\Vert _1 \leqslant  \Vert x\Vert _3 \leqslant  f\Vert x\Vert _1 a\Vert x\Vert _1\leqslant  \Vert x\Vert _1\leqslant  b\Vert x\Vert _1 a,b = 1 a\Vert x\Vert _1 \leqslant  \Vert x\Vert _2 \leqslant  b\Vert x\Vert _1 -\frac{1}{b}\Vert x\Vert _2\leqslant  \Vert x \Vert_1 \leqslant  \frac{-1}{a} \Vert x\Vert _2. a\Vert x\Vert _1 \leqslant  \Vert x\Vert _2 \leqslant  b\Vert x\Vert _1 \Rightarrow  \frac{1}{b} \Vert x\Vert _2 \leqslant  \Vert x\Vert _1 \leqslant  \frac{1}{a}\Vert x\Vert _2. a\Vert x\Vert _1\leqslant \Vert x\Vert _2\leqslant  b\Vert x\Vert _1 c\Vert x\Vert _2\leqslant  \Vert x\Vert _3 \leqslant  d\Vert x\Vert _2 \Rightarrow ac \Vert x\Vert _1 \leqslant  c\Vert x\Vert _2\leqslant  \Vert x\Vert _3 \leqslant d\Vert x\Vert _2\leqslant  db\Vert x\Vert _1.","['real-analysis', 'solution-verification', 'normed-spaces', 'relations', 'equivalence-relations']"
27,Orthonormal basis for Sobolev Spaces,Orthonormal basis for Sobolev Spaces,,Sobolev spaces of order 2 are known to form a Hilbert space. Consider such a Sobolev space of (order 2) functions on the domain $f:\mathbb{R}\rightarrow \mathbb{R}$. What is an example for the basis of such a Sobolev space.,Sobolev spaces of order 2 are known to form a Hilbert space. Consider such a Sobolev space of (order 2) functions on the domain $f:\mathbb{R}\rightarrow \mathbb{R}$. What is an example for the basis of such a Sobolev space.,,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'sobolev-spaces', 'orthonormal']"
28,Strictly increasing function,Strictly increasing function,,"If $f(x)$ is a continuous function on $\mathbb R$, and $|f(-x)|< |f(x)|$ for all $x>0$. Does it imply that $|f(x)|$ is strictly increasing on $(0,\infty)$? I tried to use the definition: let $a,b \in (0,\infty)$ with $a<b$, we need to show that $|f(a)|<|f(b)|$. We have  $|f(-a)|< |f(a)|$ and $|f(-b)|< |f(b)|$, and I don't know how to proceed!","If $f(x)$ is a continuous function on $\mathbb R$, and $|f(-x)|< |f(x)|$ for all $x>0$. Does it imply that $|f(x)|$ is strictly increasing on $(0,\infty)$? I tried to use the definition: let $a,b \in (0,\infty)$ with $a<b$, we need to show that $|f(a)|<|f(b)|$. We have  $|f(-a)|< |f(a)|$ and $|f(-b)|< |f(b)|$, and I don't know how to proceed!",,"['calculus', 'real-analysis']"
29,Can anyone tell me why the arclength integral is a lower semicontinuous function on the set of continuously differentiable real-valued functions?,Can anyone tell me why the arclength integral is a lower semicontinuous function on the set of continuously differentiable real-valued functions?,,"I posted the question stating that it was upper semicontinuous , but that was definitely wrong.  I am trying to prove lower semicontinuity.","I posted the question stating that it was upper semicontinuous , but that was definitely wrong.  I am trying to prove lower semicontinuity.",,"['real-analysis', 'general-topology', 'arc-length']"
30,Proving that if $\cos{x} = \cos{y}$ and $\sin{x} = \sin{y}$ then $x-y = 2\pi n$ for some $n\in \mathbb{Z}$,Proving that if  and  then  for some,\cos{x} = \cos{y} \sin{x} = \sin{y} x-y = 2\pi n n\in \mathbb{Z},"I was solving some exercises in complex analysis in preparation for a qualifying exam, and I came across a problem which asked me to prove that if $x, y \in \mathbb{R}$ then $$     e^{ix} = e^{iy} \iff x - y = 2 \pi n \quad \text{for some} \  n \in \mathbb{Z}     $$ At first I thought it was quite easy and what I thought is that it was kind of obvious if I used Euler's formula and equated real parts and imaginary parts, so that $$ e^{ix} = e^{iy} \iff \cos{x} = \cos{y} \quad \text{and} \quad \sin{x} = \sin{y} $$ and thought of this in terms of the corresponding points in the unit circle. But that doesn't seem very rigorous to me. So I would like to ask the following. How can this be proved using another definition for the trigonometric functions like the series definition or the definition as solutions of the differential equation $y'' = -y$ with some initial conditions? And what properties of the trigonometric functions have to be used to prove this? Note I thought of doing the following but still can't complete the argument. $$ e^{ix} = e^{iy} \iff e^{i(x-y)} = 1 \iff \cos{(x-y)} = 1 \quad \text{and} \quad \sin{(x-y)} = 0 $$ and now I would like to conclude that this happens if and only if $x-y = 2 \pi n$ but don't know how to justify this. Any help would be very appreciated.  Thank you.","I was solving some exercises in complex analysis in preparation for a qualifying exam, and I came across a problem which asked me to prove that if $x, y \in \mathbb{R}$ then $$     e^{ix} = e^{iy} \iff x - y = 2 \pi n \quad \text{for some} \  n \in \mathbb{Z}     $$ At first I thought it was quite easy and what I thought is that it was kind of obvious if I used Euler's formula and equated real parts and imaginary parts, so that $$ e^{ix} = e^{iy} \iff \cos{x} = \cos{y} \quad \text{and} \quad \sin{x} = \sin{y} $$ and thought of this in terms of the corresponding points in the unit circle. But that doesn't seem very rigorous to me. So I would like to ask the following. How can this be proved using another definition for the trigonometric functions like the series definition or the definition as solutions of the differential equation $y'' = -y$ with some initial conditions? And what properties of the trigonometric functions have to be used to prove this? Note I thought of doing the following but still can't complete the argument. $$ e^{ix} = e^{iy} \iff e^{i(x-y)} = 1 \iff \cos{(x-y)} = 1 \quad \text{and} \quad \sin{(x-y)} = 0 $$ and now I would like to conclude that this happens if and only if $x-y = 2 \pi n$ but don't know how to justify this. Any help would be very appreciated.  Thank you.",,"['real-analysis', 'complex-analysis', 'trigonometry']"
31,Understanding of nowhere dense sets,Understanding of nowhere dense sets,,"The following is the definition of nowhere dense sets in Wikipedia. In topology, a subset $A$ of a topological space $X$ is called nowhere dense (in $X$ ) if there is no neighborhood in $X$ on which $A$ is dense. It is also said in this lecture note that a nowhere dense set is a set $E$ which is not dense in any ball. I don't quite understand what ""not dense in any ball"" means unless I can find several essentially different counterexamples. Considering ${\mathbb R}$ with the standard topology, I am thinking about examples which is NOT a nowhere dense set. Here are my questions : What can be an example such that the set $E$ is dense in any ball? What can be an example such that the set $E$ is dense in some balls while it is not in others? I'm sure what "" $E$ is dense in a ball"" means. Does it mean $\overline{E}$ is a ball or $\overline{E}$ contains a ball or $\overline{E}$ is contained in a ball?","The following is the definition of nowhere dense sets in Wikipedia. In topology, a subset of a topological space is called nowhere dense (in ) if there is no neighborhood in on which is dense. It is also said in this lecture note that a nowhere dense set is a set which is not dense in any ball. I don't quite understand what ""not dense in any ball"" means unless I can find several essentially different counterexamples. Considering with the standard topology, I am thinking about examples which is NOT a nowhere dense set. Here are my questions : What can be an example such that the set is dense in any ball? What can be an example such that the set is dense in some balls while it is not in others? I'm sure what "" is dense in a ball"" means. Does it mean is a ball or contains a ball or is contained in a ball?",A X X X A E {\mathbb R} E E E \overline{E} \overline{E} \overline{E},['real-analysis']
32,Sequences and Intervals,Sequences and Intervals,,"I came across another real analysis problem in my self study: Let $[a,b]$ be a closed interval in $\mathbb{R}$ and let $(x_n)$ be any sequence in $\mathbb{R}$. Prove that $[a,b]$ contains a real number not equal to any term of the sequence. I think I need to use the nested interval theorem: Theorem. If $(I_n)$ is a nested sequence of closed intervals, then the intersection of the $I_n$ is nonempty. In other words, if $I_n = [a_n, b_n]$, where $a_n \leq b_n$ and $I_1 \supset I_2 \supset I_3 \supset \dots$ and $a = \sup \{a_n: n \in \mathbb{Z}^{+} \}$, $b = \inf \{b_n: n \in \mathbb{Z}^{+} \}$ then $a \leq b$ and $\bigcap_{n=1}^{\infty} [a_n, b_n] = [a,b]$. It seems obvious if we know that the interval is uncountable and the sequence is countable. Or could you do the following: Pick an arbitrary element $x_0$ of $(x_n)$ in $[a,b]$ (if there is none then we are done). By denseness, there is a real number $\alpha$ between $a$ and $x_0$. If $\alpha$ is in the sequence pick another number $\alpha_1$ between $a$ and $\alpha$. Keep doing this until you find a number not in the sequence. Would this idea work?","I came across another real analysis problem in my self study: Let $[a,b]$ be a closed interval in $\mathbb{R}$ and let $(x_n)$ be any sequence in $\mathbb{R}$. Prove that $[a,b]$ contains a real number not equal to any term of the sequence. I think I need to use the nested interval theorem: Theorem. If $(I_n)$ is a nested sequence of closed intervals, then the intersection of the $I_n$ is nonempty. In other words, if $I_n = [a_n, b_n]$, where $a_n \leq b_n$ and $I_1 \supset I_2 \supset I_3 \supset \dots$ and $a = \sup \{a_n: n \in \mathbb{Z}^{+} \}$, $b = \inf \{b_n: n \in \mathbb{Z}^{+} \}$ then $a \leq b$ and $\bigcap_{n=1}^{\infty} [a_n, b_n] = [a,b]$. It seems obvious if we know that the interval is uncountable and the sequence is countable. Or could you do the following: Pick an arbitrary element $x_0$ of $(x_n)$ in $[a,b]$ (if there is none then we are done). By denseness, there is a real number $\alpha$ between $a$ and $x_0$. If $\alpha$ is in the sequence pick another number $\alpha_1$ between $a$ and $\alpha$. Keep doing this until you find a number not in the sequence. Would this idea work?",,['real-analysis']
33,Harmonic mean and logarithmic mean,Harmonic mean and logarithmic mean,,"The harmonic mean of a finite set of positive real numbers $\{x_1, x_2, \ldots, x_n\}$ is defined to be $$H(\{x_1, x_2, \ldots, x_n\}) = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + \cdots + \frac{1}{x_n}}.$$ The logarithmic mean of two distinct positive real numbers $a$ and $b$ is defined to be $$L(a,b) = \frac{b - a}{\ln b - \ln a}.$$ One of the first applications of integration that students often see is the extension of the arithmetic mean of a finite set of real numbers to the arithmetic mean of a function $f(x)$ on a continuous interval $[a,b]$ via $\frac{1}{b-a} \int_a^b f(x) dx.$ In the same way, you can extend the harmonic mean so that it applies to a positive function $f(x)$ over a continuous interval $[a,b]$.  You get  $$\frac{b-a}{\int_a^b \frac{dx}{f(x)}} .$$ Thus, if you take $f(x) = x$ you obtain the harmonic mean of the continuous interval $[a,b]$.  This is  $$H([a,b]) = \frac{b-a}{\int_a^b \frac{1}{x}  dx} = \frac{b-a}{\ln b - \ln a} = L(a,b).$$ My question is this: Is there an intuitive reason why $H([a,b]) = L(a,b)$? For comparison purposes, note that if $A$ denotes the arithmetic mean, then $A([a,b]) = A(a,b) = \frac{a+b}{2}$.","The harmonic mean of a finite set of positive real numbers $\{x_1, x_2, \ldots, x_n\}$ is defined to be $$H(\{x_1, x_2, \ldots, x_n\}) = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + \cdots + \frac{1}{x_n}}.$$ The logarithmic mean of two distinct positive real numbers $a$ and $b$ is defined to be $$L(a,b) = \frac{b - a}{\ln b - \ln a}.$$ One of the first applications of integration that students often see is the extension of the arithmetic mean of a finite set of real numbers to the arithmetic mean of a function $f(x)$ on a continuous interval $[a,b]$ via $\frac{1}{b-a} \int_a^b f(x) dx.$ In the same way, you can extend the harmonic mean so that it applies to a positive function $f(x)$ over a continuous interval $[a,b]$.  You get  $$\frac{b-a}{\int_a^b \frac{dx}{f(x)}} .$$ Thus, if you take $f(x) = x$ you obtain the harmonic mean of the continuous interval $[a,b]$.  This is  $$H([a,b]) = \frac{b-a}{\int_a^b \frac{1}{x}  dx} = \frac{b-a}{\ln b - \ln a} = L(a,b).$$ My question is this: Is there an intuitive reason why $H([a,b]) = L(a,b)$? For comparison purposes, note that if $A$ denotes the arithmetic mean, then $A([a,b]) = A(a,b) = \frac{a+b}{2}$.",,"['real-analysis', 'intuition', 'average']"
34,Probability measures on the hypersphere $\Bbb S^{n-1}$ and the $O(n)$ isomorphisms,Probability measures on the hypersphere  and the  isomorphisms,\Bbb S^{n-1} O(n),"Let $\Bbb S^{n-1}$ denotes the unit sphere in $\Bbb R^n$ and let $O(n)$ be the group of all orthogonal linear transformations on $\Bbb R^n$ . For any (outer) measure $\mu$ on $\Bbb R^n$ , the pushforward of $\mu$ by $T\in O(n)$ is defined as $T_{\#}\mu(E) := \mu(T^{-1}(E))$ . It is clear that the pushforward under $T\in O(n)$ maps a probability measure on $\Bbb S^{n-1}$ to another probability measure on $\Bbb S^{n-1}$ . Suppose that $f\in C(\Bbb S^{n-1})$ satisfies the condition $$ \int_{\Bbb S^{n-1}} f \,d\mu = \int_{\Bbb S^{n-1}} f \,dT_{\#}\mu \quad \text{for all } \ T\in O(n), \tag{*}\label{*} $$ is it true that $$ \int_{\Bbb S^{n-1}} f \,d\mu = \frac{1}{\mathcal H^{n-1}(\Bbb S^{n-1})} \int_{\Bbb S^{n-1}} f \,d\mathcal H^{n-1} =: \bar f, \tag{**}\label{**} $$ where $\mathcal H^{n-1}$ is the $n-1$ dimensional Hausdorff measure? Of course, the case where $\mu$ is a Dirac mass $\delta_x$ for some $x\in\Bbb S^{n-1}$ is clear, and, in fact, we can trivially conclude that $f$ must be a constant function. One might be tempted to conjecture that the condition $\eqref{*}$ implies that $f$ must be a constant function, but that is false . We can already find a counterexample to the stronger conjecture by considering $\mu$ to be a sum of Dirac masses: $\mu = \frac1n (\delta_{e_1} + \cdots + \delta_{e_n})$ and $f(x) = x\cdot Ax$ for any $n\times n$ matrix $A$ . Indeed, in this case we have $$ \int_{\Bbb S^{n-1}} f \,d\mu = \frac 1n \sum_{i=1}^n e_i\cdot Ae_i = \frac{\text{Tr}(A)}{n}, $$ and the statement that $\eqref{*}$ holds follows from the well-known fact that the trace of a matrix $A$ is invariant under a change of an orthogonal basis. Clearly, $f$ is not a constant function, but it is not hard to show that $$ \frac{\text{Tr}(A)}{n} = \frac{1}{\mathcal H^{n-1}(\Bbb S^{n-1})} \int_{\Bbb S^{n-1}} x\cdot Ax \,d\mathcal H^{n-1}(x) $$ using the divergence theorem and that fact that $\text{Tr}(A) = \text{div}\, Ax$ . This question is largely inspired by the example regarding the trace that I mentioned above. I restrict $f$ in this question to be a continuous function since I feel like there might be a beautiful proof using the duality between continuous functions and Radon measures on $\Bbb S^{n-1}$ . I am aware of the existence of the things called measures on homogeneous spaces that generalize Haar measures, but aside from that, I know next to nothing about it. I welcome a proof that uses ideas from that theory, but would like to read a proof that doesn't use it as well, even if it might be longer and less elegant.","Let denotes the unit sphere in and let be the group of all orthogonal linear transformations on . For any (outer) measure on , the pushforward of by is defined as . It is clear that the pushforward under maps a probability measure on to another probability measure on . Suppose that satisfies the condition is it true that where is the dimensional Hausdorff measure? Of course, the case where is a Dirac mass for some is clear, and, in fact, we can trivially conclude that must be a constant function. One might be tempted to conjecture that the condition implies that must be a constant function, but that is false . We can already find a counterexample to the stronger conjecture by considering to be a sum of Dirac masses: and for any matrix . Indeed, in this case we have and the statement that holds follows from the well-known fact that the trace of a matrix is invariant under a change of an orthogonal basis. Clearly, is not a constant function, but it is not hard to show that using the divergence theorem and that fact that . This question is largely inspired by the example regarding the trace that I mentioned above. I restrict in this question to be a continuous function since I feel like there might be a beautiful proof using the duality between continuous functions and Radon measures on . I am aware of the existence of the things called measures on homogeneous spaces that generalize Haar measures, but aside from that, I know next to nothing about it. I welcome a proof that uses ideas from that theory, but would like to read a proof that doesn't use it as well, even if it might be longer and less elegant.","\Bbb S^{n-1} \Bbb R^n O(n) \Bbb R^n \mu \Bbb R^n \mu T\in O(n) T_{\#}\mu(E) := \mu(T^{-1}(E)) T\in O(n) \Bbb S^{n-1} \Bbb S^{n-1} f\in C(\Bbb S^{n-1}) 
\int_{\Bbb S^{n-1}} f \,d\mu = \int_{\Bbb S^{n-1}} f \,dT_{\#}\mu \quad \text{for all } \ T\in O(n), \tag{*}\label{*}
 
\int_{\Bbb S^{n-1}} f \,d\mu = \frac{1}{\mathcal H^{n-1}(\Bbb S^{n-1})} \int_{\Bbb S^{n-1}} f \,d\mathcal H^{n-1} =: \bar f, \tag{**}\label{**}
 \mathcal H^{n-1} n-1 \mu \delta_x x\in\Bbb S^{n-1} f \eqref{*} f \mu \mu = \frac1n (\delta_{e_1} + \cdots + \delta_{e_n}) f(x) = x\cdot Ax n\times n A 
\int_{\Bbb S^{n-1}} f \,d\mu = \frac 1n \sum_{i=1}^n e_i\cdot Ae_i = \frac{\text{Tr}(A)}{n},
 \eqref{*} A f 
\frac{\text{Tr}(A)}{n} = \frac{1}{\mathcal H^{n-1}(\Bbb S^{n-1})} \int_{\Bbb S^{n-1}} x\cdot Ax \,d\mathcal H^{n-1}(x)
 \text{Tr}(A) = \text{div}\, Ax f \Bbb S^{n-1}","['real-analysis', 'probability', 'measure-theory', 'fourier-analysis', 'lie-algebras']"
35,Matrix and Taylor Expansion of a Finite Continued Function,Matrix and Taylor Expansion of a Finite Continued Function,,"I noticed a pattern between matrix and Taylor series of a finite continued fraction function. However, I don't know how to prove it or why they are related. Let $$ f_{1}(z)=\frac{1}{-z-1} $$ $$ f_{2}(z)=\frac{1}{-z-\frac{1}{-z-1}} $$ Similarly, we define $f_{3}(z),f_{4}(z)$ , etc. That is, $f_{l}(z)$ has $l$ layers. Let's consider the Taylor expansion of $f_{l}(z)$ at $z=0$ , and write $$ f_{l}(z)=a_{l}(0)+a_{l}(1)z+a_{l}(2)z^{2}+\cdots $$ Now, let's consider the following $l \times l$ matrix $$ A_{l}=\begin{pmatrix}     1 & 1 & \cdots & 1 & 1 \\     1 & 1 & \cdots & 1 & 0\\     &\vdots \\     1 & 0 & \cdots & 0 & 0   \end{pmatrix} $$ Here is the amazing thing: $$ (A_{l}^{n})_{1,1}= \mid a_{l}(n) \mid $$ where $(A_{l}^{n})_{1,1}$ stands for the $(1,1)$ -entry of the matrix $A_{l}^{n}$ . I don't have a proof, but I checked with many $l$ and $n$ and they all hold. My questions are the following: How to prove this equality? (I tried to prove it directly, but taking $n$ -th derivative of a continued fraction is a nightmare) What is the intuition behind it? Is there any deeper connection between this matrix and this sequence of finite continued fractions? Is it a special case of some deeper result? Thank you very much!","I noticed a pattern between matrix and Taylor series of a finite continued fraction function. However, I don't know how to prove it or why they are related. Let Similarly, we define , etc. That is, has layers. Let's consider the Taylor expansion of at , and write Now, let's consider the following matrix Here is the amazing thing: where stands for the -entry of the matrix . I don't have a proof, but I checked with many and and they all hold. My questions are the following: How to prove this equality? (I tried to prove it directly, but taking -th derivative of a continued fraction is a nightmare) What is the intuition behind it? Is there any deeper connection between this matrix and this sequence of finite continued fractions? Is it a special case of some deeper result? Thank you very much!","
f_{1}(z)=\frac{1}{-z-1}
 
f_{2}(z)=\frac{1}{-z-\frac{1}{-z-1}}
 f_{3}(z),f_{4}(z) f_{l}(z) l f_{l}(z) z=0 
f_{l}(z)=a_{l}(0)+a_{l}(1)z+a_{l}(2)z^{2}+\cdots
 l \times l 
A_{l}=\begin{pmatrix}
    1 & 1 & \cdots & 1 & 1 \\
    1 & 1 & \cdots & 1 & 0\\
    &\vdots \\
    1 & 0 & \cdots & 0 & 0
  \end{pmatrix}
 
(A_{l}^{n})_{1,1}= \mid a_{l}(n) \mid
 (A_{l}^{n})_{1,1} (1,1) A_{l}^{n} l n n","['real-analysis', 'linear-algebra', 'taylor-expansion', 'continued-fractions']"
36,Number of real roots of equation,Number of real roots of equation,,"Given $\cos(x)=ax$ , express the number of real roots as a function of $a$ . I've found that the sign of $a$ doesn't matter since $cos(x)$ is an even function, and that the number of roots increases as the value of $a$ decreases, for infinitely many roots as $a$ approaches zero. But I have no idea where to go from there. I know that $\cos(x)=x$ is unsolvable analytically, so is this question also only solvable numerically?","Given , express the number of real roots as a function of . I've found that the sign of doesn't matter since is an even function, and that the number of roots increases as the value of decreases, for infinitely many roots as approaches zero. But I have no idea where to go from there. I know that is unsolvable analytically, so is this question also only solvable numerically?",\cos(x)=ax a a cos(x) a a \cos(x)=x,"['real-analysis', 'roots']"
37,"Is there a book on mathematical logic with a similar style to mathematician Terence Tao's book ""Analysis 1""?","Is there a book on mathematical logic with a similar style to mathematician Terence Tao's book ""Analysis 1""?",,"I am looking for a mathematical logic textbook that I can study on my own. However, I am not very proficient in English. To give you an idea of my English level, I can read children's books such as ""Matilda"" and math textbooks in English. In particular, I really like Terence Tao's book ""Analysis 1"". This book provides very detailed explanations of proofs. However, the problem is that books with a lot of everyday English language are difficult for me to read. This includes books in the philosophy category. I found that books with titles like ""A Friendly Introduction to Logic"" were actually more difficult for me to read. The more explanations there were, the more difficult it was for me as a non-native English speaker. So, I was wondering if there are any mathematical logic textbooks that are easy for me to read? I've searched Amazon and Google extensively, but I haven't found a book that's perfect for me. I considered purchasing Enderton's ""Mathematical Introduction to Logic,"" which seems to be highly recommended, but the proofs aren't detailed enough. I'm hoping to find a book that's perfect for me, but since Terence Tao's book was so good, I'm looking for something similar. Thank you very much for your help.","I am looking for a mathematical logic textbook that I can study on my own. However, I am not very proficient in English. To give you an idea of my English level, I can read children's books such as ""Matilda"" and math textbooks in English. In particular, I really like Terence Tao's book ""Analysis 1"". This book provides very detailed explanations of proofs. However, the problem is that books with a lot of everyday English language are difficult for me to read. This includes books in the philosophy category. I found that books with titles like ""A Friendly Introduction to Logic"" were actually more difficult for me to read. The more explanations there were, the more difficult it was for me as a non-native English speaker. So, I was wondering if there are any mathematical logic textbooks that are easy for me to read? I've searched Amazon and Google extensively, but I haven't found a book that's perfect for me. I considered purchasing Enderton's ""Mathematical Introduction to Logic,"" which seems to be highly recommended, but the proofs aren't detailed enough. I'm hoping to find a book that's perfect for me, but since Terence Tao's book was so good, I'm looking for something similar. Thank you very much for your help.",,"['real-analysis', 'logic', 'reference-request', 'first-order-logic', 'book-recommendation']"
38,Can a Lipschitz-continuous function take on every value more than $L$ times?,Can a Lipschitz-continuous function take on every value more than  times?,L,"Here's an interesting question I stumbled upon recently: can a Lipschitz-continuous function $f:[0,1]\to[0,1]$ with Lipschitz bound $L$ take on every value $y\in[0,1]$ more than $L$ times? Intuitively, I'd expect the answer to be no, but it's not at all obvious to me how that could be proven. It seems to me like there might necessarily be some measure theory involved, but I'm curious whether a more elementary proof exists as well.","Here's an interesting question I stumbled upon recently: can a Lipschitz-continuous function with Lipschitz bound take on every value more than times? Intuitively, I'd expect the answer to be no, but it's not at all obvious to me how that could be proven. It seems to me like there might necessarily be some measure theory involved, but I'm curious whether a more elementary proof exists as well.","f:[0,1]\to[0,1] L y\in[0,1] L","['real-analysis', 'lipschitz-functions']"
39,"An inequality equivalent to Hörmander's condition $\sup_{y\in\mathbb R^n}\int_{\{x: |x|>2|y|\}}|K(x-y)-K(x)|\,dx<\infty$",An inequality equivalent to Hörmander's condition,"\sup_{y\in\mathbb R^n}\int_{\{x: |x|>2|y|\}}|K(x-y)-K(x)|\,dx<\infty","Let $K\in L_{\text{loc}}^1(\mathbb R^n\setminus\{0\})$ . Prove that $$\sup_{y\in\mathbb R^n}\int_{\{x: |x|>2|y|\}}|K(x-y)-K(x)|\,dx<\infty\label{1}\tag{1}$$ if and only if $$\sup_{r>0}\frac1{r^n}\int_{B(0,r)}\int_{\{x: |x|>2r\}}|K(x-y)-K(x)|\,dx\,dy<\infty.\label{2}\tag{2}$$ This is an old exam problem on Harmonic Analysis. Formula \eqref{1} is called the Hörmander's condition for singular integrals. The proof of \eqref{1} $\Rightarrow$ \eqref{2} is quite easy: assume $$\int_{\{x: |x|>2|y|\}}|K(x-y)-K(x)|\,dx\leq M,\qquad \forall y\in\mathbb R^n,$$ then for $r>0$ and $y\in B(0,r)$ we have $\{x: |x|>2r\}\subset \{x: |x|>2|y|\}$ , so $$\int_{\{x: |x|>2r\}}|K(x-y)-K(x)|\,dx\leq \int_{\{x: |x|>2|y|\}}|K(x-y)-K(x)|\,dx\leq M,$$ hence $$\frac1{r^n}\int_{B(0,r)}\int_{\{x: |x|>2r\}}|K(x-y)-K(x)|\,dx\,dy\leq \frac1{r^n}\int_{B(0,r)}M\,dy=M|B(0,1)|,\ \ \ \forall r>0.$$ This completes the proof of \eqref{1} $\Rightarrow$ \eqref{2}. However, for \eqref{2} $\Rightarrow$ \eqref{1}, I don't know how to start. Any help would be appreciated!","Let . Prove that if and only if This is an old exam problem on Harmonic Analysis. Formula \eqref{1} is called the Hörmander's condition for singular integrals. The proof of \eqref{1} \eqref{2} is quite easy: assume then for and we have , so hence This completes the proof of \eqref{1} \eqref{2}. However, for \eqref{2} \eqref{1}, I don't know how to start. Any help would be appreciated!","K\in L_{\text{loc}}^1(\mathbb R^n\setminus\{0\}) \sup_{y\in\mathbb R^n}\int_{\{x: |x|>2|y|\}}|K(x-y)-K(x)|\,dx<\infty\label{1}\tag{1} \sup_{r>0}\frac1{r^n}\int_{B(0,r)}\int_{\{x: |x|>2r\}}|K(x-y)-K(x)|\,dx\,dy<\infty.\label{2}\tag{2} \Rightarrow \int_{\{x: |x|>2|y|\}}|K(x-y)-K(x)|\,dx\leq M,\qquad \forall y\in\mathbb R^n, r>0 y\in B(0,r) \{x: |x|>2r\}\subset \{x: |x|>2|y|\} \int_{\{x: |x|>2r\}}|K(x-y)-K(x)|\,dx\leq \int_{\{x: |x|>2|y|\}}|K(x-y)-K(x)|\,dx\leq M, \frac1{r^n}\int_{B(0,r)}\int_{\{x: |x|>2r\}}|K(x-y)-K(x)|\,dx\,dy\leq \frac1{r^n}\int_{B(0,r)}M\,dy=M|B(0,1)|,\ \ \ \forall r>0. \Rightarrow \Rightarrow","['real-analysis', 'harmonic-analysis', 'singular-integrals']"
40,"Let $\alpha$ be an increasing function on $[a,b]$. Show that $\int^a_b\alpha d \alpha = \frac{1}{2}[\alpha (b)^2 - \alpha(a)^2]$",Let  be an increasing function on . Show that,"\alpha [a,b] \int^a_b\alpha d \alpha = \frac{1}{2}[\alpha (b)^2 - \alpha(a)^2]","I am wanting to try to prove the question below, but there is a step that I can't get pass. I know that the proof is worthless if I assume incorrectly, and should have stopped proving from there, but I feel that I am close and possibly just missing a theorem or something that might be able to salvage the proof. But if ther is no way then I will just try a different attempt altogether. Book: here page: 166 I would really appreciate any help\insight you can offer. Question Let $\alpha$ be an increasing function on $[a,b]$ and suppose $\alpha \in R(\alpha )$ on $[a,b]$ . Show that $\int^a_b\alpha d \alpha = \frac{1}{2}[\alpha (b)^2 - \alpha(a)^2]$ Note: $\alpha \in R(\alpha )$ this is showing that $\alpha$ is Riemann-integrable My attempt Let $P$ be a partition on $[a,b]$ Let as $\alpha$ is increasing, thus $\alpha(x) \leq \alpha(y)$ where $x<y$ for $x,y\in[a,b]$ Let $M_k = sup\{\alpha(x) | x_{k-1} \leq x \leq x_k\} = \alpha(x_k)$ Let $m_k = inf\{\alpha(x) | x_{k-1} \leq x \leq x_k\} = \alpha(x_{k-1})$ let $\Delta\alpha_k =  \alpha(x_k) - \alpha(x_{k-1})$ Now the upper Stieltjies integral: $U(P,\alpha,\alpha) = \sum\limits_{k=1}^n M_k\Delta\alpha_k = \sum\limits_{k=1}^n\alpha(x_k)\Delta\alpha_k$ and the lower Stieltjies integral: $L(P,\alpha,\alpha) = \sum\limits_{k=1}^n m_k\Delta\alpha_k = \sum\limits_{k=1}^n\alpha(x_{k-1})\Delta\alpha_k$ As $\alpha$ is Riemann-integrable thus the upper Stieltjies integral $=$ lower Stieltjies integral, thus $\inf\{U(P,\alpha,\alpha)|$ where is P is a partition on $[a,b]\}$ $ = \sup\{L(P,\alpha,\alpha)|$ where is P is a partition on $[a,b]\}$ $ = \int^a_b\alpha d \alpha$ Let $\int^a_b\alpha d \alpha = \frac{1}{2}[U(P,\alpha,\alpha) + L(P,\alpha,\alpha)]$ <<< this is the problem step $= \frac{1}{2}[\sum\limits_{k=1}^n\alpha(x_k)\Delta\alpha_k + \sum\limits_{k=1}^n\alpha(x_{k-1})\Delta\alpha_k]$ $= \frac{1}{2}\sum\limits_{k=1}^n[\alpha(x_k)+\alpha(x_{k-1})]\Delta\alpha_k$ $= \frac{1}{2}[(\alpha(x_1) + \alpha(x_0))(\alpha(x_1)- \alpha(x_0))+ (\alpha(x_2) + \alpha(x_1))(\alpha(x_2)- \alpha(x_1))+\cdots]$ $= \frac{1}{2}[\alpha(x_1)^2 - \alpha(x_0)^2+ \alpha(x_2)^2 - \alpha(x_1)^2+\cdots]$ $= \frac{1}{2}[\alpha(x_{last})^2 - \alpha(x_0)^2] = \frac{1}{2}[\alpha(b)^2 - \alpha(a)^2]$ the problem this $\int^a_b\alpha d \alpha = \frac{1}{2}[U(P,\alpha,\alpha) + L(P,\alpha,\alpha)]$ should be $\int^a_b\alpha d \alpha = \frac{1}{2}[\inf\{U(P,\alpha,\alpha)|$ for $P$ on $[a,b]\} + \sup\{L(P,\alpha,\alpha)| $ for $P$ on $[a,b]\}]$ but I can't get rid on the $\inf$ and $\sup$ . Is there away to do this?","I am wanting to try to prove the question below, but there is a step that I can't get pass. I know that the proof is worthless if I assume incorrectly, and should have stopped proving from there, but I feel that I am close and possibly just missing a theorem or something that might be able to salvage the proof. But if ther is no way then I will just try a different attempt altogether. Book: here page: 166 I would really appreciate any help\insight you can offer. Question Let be an increasing function on and suppose on . Show that Note: this is showing that is Riemann-integrable My attempt Let be a partition on Let as is increasing, thus where for Let Let let Now the upper Stieltjies integral: and the lower Stieltjies integral: As is Riemann-integrable thus the upper Stieltjies integral lower Stieltjies integral, thus where is P is a partition on where is P is a partition on Let <<< this is the problem step the problem this should be for on for on but I can't get rid on the and . Is there away to do this?","\alpha [a,b] \alpha \in R(\alpha ) [a,b] \int^a_b\alpha d \alpha = \frac{1}{2}[\alpha (b)^2 - \alpha(a)^2] \alpha \in R(\alpha ) \alpha P [a,b] \alpha \alpha(x) \leq \alpha(y) x<y x,y\in[a,b] M_k = sup\{\alpha(x) | x_{k-1} \leq x \leq x_k\} = \alpha(x_k) m_k = inf\{\alpha(x) | x_{k-1} \leq x \leq x_k\} = \alpha(x_{k-1}) \Delta\alpha_k =  \alpha(x_k) - \alpha(x_{k-1}) U(P,\alpha,\alpha) = \sum\limits_{k=1}^n M_k\Delta\alpha_k = \sum\limits_{k=1}^n\alpha(x_k)\Delta\alpha_k L(P,\alpha,\alpha) = \sum\limits_{k=1}^n m_k\Delta\alpha_k = \sum\limits_{k=1}^n\alpha(x_{k-1})\Delta\alpha_k \alpha = \inf\{U(P,\alpha,\alpha)| [a,b]\}  = \sup\{L(P,\alpha,\alpha)| [a,b]\}  = \int^a_b\alpha d \alpha \int^a_b\alpha d \alpha = \frac{1}{2}[U(P,\alpha,\alpha) + L(P,\alpha,\alpha)] = \frac{1}{2}[\sum\limits_{k=1}^n\alpha(x_k)\Delta\alpha_k + \sum\limits_{k=1}^n\alpha(x_{k-1})\Delta\alpha_k] = \frac{1}{2}\sum\limits_{k=1}^n[\alpha(x_k)+\alpha(x_{k-1})]\Delta\alpha_k = \frac{1}{2}[(\alpha(x_1) + \alpha(x_0))(\alpha(x_1)- \alpha(x_0))+ (\alpha(x_2) + \alpha(x_1))(\alpha(x_2)- \alpha(x_1))+\cdots] = \frac{1}{2}[\alpha(x_1)^2 - \alpha(x_0)^2+ \alpha(x_2)^2 - \alpha(x_1)^2+\cdots] = \frac{1}{2}[\alpha(x_{last})^2 - \alpha(x_0)^2] = \frac{1}{2}[\alpha(b)^2 - \alpha(a)^2] \int^a_b\alpha d \alpha = \frac{1}{2}[U(P,\alpha,\alpha) + L(P,\alpha,\alpha)] \int^a_b\alpha d \alpha = \frac{1}{2}[\inf\{U(P,\alpha,\alpha)| P [a,b]\} + \sup\{L(P,\alpha,\alpha)|  P [a,b]\}] \inf \sup","['real-analysis', 'solution-verification', 'riemann-integration', 'stieltjes-integral']"
41,"Let $f$ be a real-valued function on $[0,1]$ with a continuous second derivative. Assume that $f(0)=0, f'(0)= 1, f''(0)\neq 0$, and $0<f'(x)< $ ....","Let  be a real-valued function on  with a continuous second derivative. Assume that , and  ....","f [0,1] f(0)=0, f'(0)= 1, f''(0)\neq 0 0<f'(x)< ","Let $f$ be a real-valued function on $[0,1]$ with a continuous second derivative. Assume that $f(0)=0, f'(0)= 1, f''(0)\neq 0$ , and $0<f'(x)<1 $ for all $x\in(0, 1]$ . Let $x_1, x_2,\ldots$ be a sequence with $0< x_1 \leq 1$ and with $$x_n = f\left(\dfrac{x_1+x_2+\cdots+x_{n-1}}{n-1}\right)$$ for $n \geq 2$ . Prove that $\displaystyle\lim_{n\to\infty}x_n\ln n = −2/f'' (0)$ . I tried to use fixed point theorem by converting it to form $x_n=\phi(x_{n+1})$ but it is not coming. Please help regarding this.","Let be a real-valued function on with a continuous second derivative. Assume that , and for all . Let be a sequence with and with for . Prove that . I tried to use fixed point theorem by converting it to form but it is not coming. Please help regarding this.","f [0,1] f(0)=0, f'(0)= 1, f''(0)\neq 0 0<f'(x)<1  x\in(0, 1] x_1, x_2,\ldots 0< x_1 \leq 1 x_n = f\left(\dfrac{x_1+x_2+\cdots+x_{n-1}}{n-1}\right) n \geq 2 \displaystyle\lim_{n\to\infty}x_n\ln n = −2/f'' (0) x_n=\phi(x_{n+1})",['real-analysis']
42,"$f : \mathbb{R}\rightarrow\mathbb{R}$ be a continuous function such that for every fixed $y\in\mathbb{R}$, $f(x + y)-f (x)$ is a polynomial in $x$ [closed]","be a continuous function such that for every fixed ,  is a polynomial in  [closed]",f : \mathbb{R}\rightarrow\mathbb{R} y\in\mathbb{R} f(x + y)-f (x) x,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $f : \mathbb{R}\rightarrow\mathbb{R}$ be a continuous function such that, for every fixed $y\in\mathbb{R}$ , $f(x + y) - f (x)$ is a polynomial in $ x$ . Prove that $f$ is a polynomial function. please give some hints for the problem. I tried to use the idea of differentiation.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let be a continuous function such that, for every fixed , is a polynomial in . Prove that is a polynomial function. please give some hints for the problem. I tried to use the idea of differentiation.",f : \mathbb{R}\rightarrow\mathbb{R} y\in\mathbb{R} f(x + y) - f (x)  x f,['real-analysis']
43,"Ramanujan: If $\psi(p,n)=\int_0^a\phi(p,x)\cos(nx)dx$, then $\frac\pi2\int_0^a\phi(p,x)\phi(q,nx)dx=\int_0^\infty\psi(q,x)\psi(p,nx)dx$.","Ramanujan: If , then .","\psi(p,n)=\int_0^a\phi(p,x)\cos(nx)dx \frac\pi2\int_0^a\phi(p,x)\phi(q,nx)dx=\int_0^\infty\psi(q,x)\psi(p,nx)dx","Apparently, the following appeared in Ramanujan's first letter to GH Hardy. If $$\psi(p,n)=\int_0^a\phi(p,x)\cos(nx)dx,$$ then $$\frac{\pi}{2}\int_0^a\phi(p,x)\phi(q,nx)dx=\int_0^\infty \psi(q,x)\psi(p,nx)dx$$ I tried proving this, but with not much success. First, I wrote $$\psi(q,x)\psi(p,nx)=\int_0^a\int_0^a\phi(q,u)\phi(p,v)\cos(xu)\cos(nxv)dudv$$ Then I guess we have $$\int_0^\infty\psi(q,x)\psi(p,nx)dx=\int_0^\infty\int_0^a\int_0^a\phi(q,u)\phi(p,v)\cos(xu)\cos(nxv)dudvdx,$$ but this looks just awful. It's hard to believe that expression would somehow reduce to the desired one. If there is a way to go about it like this perhaps it is via some clever substitution or change of variables, but seeing as so little is known about $\phi$ , I don't imagine there'd be much to work with. Does anyone know how to prove Ramanujan's result?","Apparently, the following appeared in Ramanujan's first letter to GH Hardy. If then I tried proving this, but with not much success. First, I wrote Then I guess we have but this looks just awful. It's hard to believe that expression would somehow reduce to the desired one. If there is a way to go about it like this perhaps it is via some clever substitution or change of variables, but seeing as so little is known about , I don't imagine there'd be much to work with. Does anyone know how to prove Ramanujan's result?","\psi(p,n)=\int_0^a\phi(p,x)\cos(nx)dx, \frac{\pi}{2}\int_0^a\phi(p,x)\phi(q,nx)dx=\int_0^\infty \psi(q,x)\psi(p,nx)dx \psi(q,x)\psi(p,nx)=\int_0^a\int_0^a\phi(q,u)\phi(p,v)\cos(xu)\cos(nxv)dudv \int_0^\infty\psi(q,x)\psi(p,nx)dx=\int_0^\infty\int_0^a\int_0^a\phi(q,u)\phi(p,v)\cos(xu)\cos(nxv)dudvdx, \phi","['real-analysis', 'integration', 'multivariable-calculus', 'iterated-integrals']"
44,"$A\subset (b,c)\subset \mathbb{R}$ is Lebesgue measurable if and only if $|A|+ |(b, c)\setminus A|= c −b$",is Lebesgue measurable if and only if,"A\subset (b,c)\subset \mathbb{R} |A|+ |(b, c)\setminus A|= c −b","I am trying to prove the following statement and I am having difficulties proving the leftward implication so I would appreciate an hint about how to do it: ""Suppose $b < c$ and $A\subset (b, c)$ . Prove that $A$ is Lebesgue measurable if and only if $|A|+ |(b, c)\setminus A|= c −b$ ."" What I have done: $\fbox{$\Rightarrow$}$ : $(b, c)\setminus A$ and $A$ are disjoint subsets of $\mathbb{R}$ of which $A$ is Lebesgue measurable: since we know that if $A$ and $B$ are disjoint subsets of $\mathbb{R}$ and $B$ is Lebesgue measurable then $|A\cup B|=|A|+|B|$ it follows that $c-b=|(b,c)|=|((b,c)\setminus A)\cup A|=|(b,c)\setminus A|+|A|$ , as desired. $\fbox{$\Leftarrow$}$ : For the leftward implication I have tried to argue by contradiction by using the fact that if $A$ is not Lebesgue measurable then there exists some $\bar{\varepsilon}>0$ such that $|G\setminus A|\geq\bar{\varepsilon}$ for every $G\supset A$ open so in particular $|(b,c)\setminus A|\geq\bar{\varepsilon}$ but I haven't been able to obtain a contradiction and I am now stuck so an hint about how to better understand this problem is welcome. EDIT: I guess that another way to see the leftward implication is to note that $|A|+ |(b, c)\setminus A|=c−b=|(b,c)|=|A\cup (b,c)\setminus A|$ so outer measure is additive and it wouldn't be if $A$ were not Lebesgue measurable (cfr Vitali set). Now, while compelling, this is not a rigorous explanation: how to make it so? Thanks DEF. (Lebesgue measurable set): A set $A\subset\mathbb{R}$ is called Lebesgue measurable if there exists a Borel set $B\subset A$ such that $|A\setminus B|= 0$ . $|\cdot|$ denotes outer measure","I am trying to prove the following statement and I am having difficulties proving the leftward implication so I would appreciate an hint about how to do it: ""Suppose and . Prove that is Lebesgue measurable if and only if ."" What I have done: : and are disjoint subsets of of which is Lebesgue measurable: since we know that if and are disjoint subsets of and is Lebesgue measurable then it follows that , as desired. : For the leftward implication I have tried to argue by contradiction by using the fact that if is not Lebesgue measurable then there exists some such that for every open so in particular but I haven't been able to obtain a contradiction and I am now stuck so an hint about how to better understand this problem is welcome. EDIT: I guess that another way to see the leftward implication is to note that so outer measure is additive and it wouldn't be if were not Lebesgue measurable (cfr Vitali set). Now, while compelling, this is not a rigorous explanation: how to make it so? Thanks DEF. (Lebesgue measurable set): A set is called Lebesgue measurable if there exists a Borel set such that . denotes outer measure","b < c A\subset (b, c) A |A|+ |(b, c)\setminus A|= c −b \fbox{\Rightarrow} (b, c)\setminus A A \mathbb{R} A A B \mathbb{R} B |A\cup B|=|A|+|B| c-b=|(b,c)|=|((b,c)\setminus A)\cup A|=|(b,c)\setminus A|+|A| \fbox{\Leftarrow} A \bar{\varepsilon}>0 |G\setminus A|\geq\bar{\varepsilon} G\supset A |(b,c)\setminus A|\geq\bar{\varepsilon} |A|+ |(b, c)\setminus A|=c−b=|(b,c)|=|A\cup (b,c)\setminus A| A A\subset\mathbb{R} B\subset A |A\setminus B|= 0 |\cdot|","['real-analysis', 'measure-theory', 'lebesgue-measure']"
45,"Prove that $a^a+b^b\ge a^b+b^a>1$ if $a,b> 0$. [duplicate]",Prove that  if . [duplicate],"a^a+b^b\ge a^b+b^a>1 a,b> 0","This question already has answers here : Does $|x|^p$ with $0<p<1$ satisfy the triangle inequality on $\mathbb{R}$? (4 answers) Closed 2 years ago . I succeeded in proving the second part of the inequality by showing that for every real number $x,y ∈ (0, 1)$ , we have $x^y≥ \frac x{x + y − xy}.$ By Bernoulli’s inequality we have $x^{1−y}= (1 + x − 1) ^{1−y}     ≤ 1 + (x − 1)(1 − y) = x + y − xy,$ $\implies$ $x^y ≥ \frac x{x + y − xy}.$ If $a ≥ 1$ or $b ≥ 1$ then the given inequality clearly holds. So let $0 <a,b< 1.$ By the previous inequality we have $a^b + b^a≥ \frac a{a + b− ab} + \frac b{a + b− ab}= \frac {a+b} {a + b− ab} > \frac {a+b}{a+b}= 1.$ But, I found some difficulty on how to prove the first part. Thanks in advance","This question already has answers here : Does $|x|^p$ with $0<p<1$ satisfy the triangle inequality on $\mathbb{R}$? (4 answers) Closed 2 years ago . I succeeded in proving the second part of the inequality by showing that for every real number , we have By Bernoulli’s inequality we have If or then the given inequality clearly holds. So let By the previous inequality we have But, I found some difficulty on how to prove the first part. Thanks in advance","x,y ∈ (0, 1) x^y≥ \frac x{x + y − xy}. x^{1−y}= (1 + x − 1)
^{1−y} 
   ≤ 1 + (x − 1)(1 − y) = x + y − xy, \implies x^y ≥ \frac x{x + y − xy}. a ≥ 1 b ≥ 1 0 <a,b< 1. a^b + b^a≥ \frac a{a + b− ab}
+ \frac b{a + b− ab}= \frac {a+b}
{a + b− ab}
> \frac {a+b}{a+b}= 1.","['real-analysis', 'calculus']"
46,Infinite series $\sum_{n=1}^{\infty}{\log}^2(1+\frac{1}{n})$,Infinite series,\sum_{n=1}^{\infty}{\log}^2(1+\frac{1}{n}),"Does anybody help me obtain a closed form for the following series (via a bit elementary ways)? $$\sum_{n=1}^{\infty}{\log}^2\left(1+\frac{1}{n}\right)$$ Seemingly, the answer would be $$2\zeta(2)-\sum_{n=1}^{\infty} \frac {\zeta (2n)}{n^2}.$$ There are some questions with answers (e.g. this one ) on Stack Exchange relating to this series and pointing out the answer, but they possess advanced approaches in this regard. Thanks in advance!","Does anybody help me obtain a closed form for the following series (via a bit elementary ways)? Seemingly, the answer would be There are some questions with answers (e.g. this one ) on Stack Exchange relating to this series and pointing out the answer, but they possess advanced approaches in this regard. Thanks in advance!",\sum_{n=1}^{\infty}{\log}^2\left(1+\frac{1}{n}\right) 2\zeta(2)-\sum_{n=1}^{\infty} \frac {\zeta (2n)}{n^2}.,"['real-analysis', 'riemann-zeta']"
47,"Problem $2.18$, Rudin's RCA - Painfully Set Theoretic","Problem , Rudin's RCA - Painfully Set Theoretic",2.18,"Problem $2.18$ : This exercise requires more set-theoretic skill than the preceding ones. Let $X$ be a well-ordered uncountable set which has a last element $\omega_1$ such that every predecessor of $\omega_1$ has at most countably many predecessors. (""Construction"": Take any well-ordered set which has elements with uncountably many predecessors, and let $\omega_1$ be the first of these; $\omega_1$ is called the first uncountable ordinal.) For $\alpha\in X$ , let $P_\alpha[S_\alpha]$ be the set of all predecessors (successors) of $\alpha$ , and call a subset of $X$ open if it is a $P_\alpha$ or an $S_\beta$ or a $P_\alpha \cap S_\beta$ or a union of such sets. Prove that $X$ is then a compact Hausdorff space. (Hint: No well-ordered set contains an infinite decreasing sequence.) Prove that the complement of the point $\omega_1$ is an open set which is not $\sigma$ -compact. Prove that to every $f \in C(X)$ there corresponds an $\alpha\ne\omega_1$ such that $f$ is constant on $S_\alpha$ . Prove that the intersection of every countable collection $\{K_n\}$ of uncountable compact subsets of $X$ is uncountable. (Hint: Consider limits of increasing countable sequences in $X$ which intersect each $K_n$ in infinitely many points.) Let $\mathfrak M$ be the collection of all $E \subset X$ such that either $E \cup \{\omega_1\}$ or $E^c \cup \{\omega_1\}$ contains an uncountable compact set; in the first case, define $\lambda(E) = 1$ ; in the second case, define $\lambda(E) = O$ . Prove that $\mathfrak M$ is a $\sigma$ -algebra which contains all Borel sets in $X$ , that, $\lambda$ is a measure on $\mathfrak M$ which is not regular (every neighborhood of $\omega_1$ has measure $1$ ), and that $$f(\omega_1) = \int_X f\ d\lambda$$ for every $f \in C(X)$ . Describe the regular $\mu$ which Theorem 2.14 associates with this linear functional. The problem certainly demands more set-theoretic skill than what I possess. My goal is to solve the problem in full, and that would start by understanding what it's saying. I request you to kindly help me with explanations/hints, and I will keep updating this post with my progress on this problem as I'm able to understand and do more parts of it. Thoughts: First, they take $X$ to be a well-ordered (every non-empty subset has a least element) uncountable set with last element $\omega_1$ . What do they mean by last ? What is meant by ""let $\omega_1$ be the first of these""? They have defined exactly the open sets (i.e. the topology) in $X$ . We must prove that $X$ is compact and Hausdorff . Hausdorff-ness is clear from Oliver's answer. For compactness, note that the topology on $X$ is the order topology, and by Theorem 27.1 of Munkres, we are done. Update: For (1): Note that $P_{\omega_1} = X\setminus \{\omega_1\}$ , since $\omega_1$ is the last element. So the complement of $\omega_1$ is open. We want to show that it is not $\sigma$ -compact. How do I do that? For (2): Suppose $f\in C(X)$ , i.e. $f$ is continuous on $X$ . I'm not sure what the codomain of the function is, so I'm assuming $\mathbb C$ . It looks like we want to find $z\in\mathbb C$ so that $f^{-1}(\{z\})$ is an $S_\alpha$ set for $\alpha < \omega_1$ . For (3): I found this answer , which seems to work. Lastly, I could use hints on how to go about the final part of the problem about measures. Thanks!","Problem : This exercise requires more set-theoretic skill than the preceding ones. Let be a well-ordered uncountable set which has a last element such that every predecessor of has at most countably many predecessors. (""Construction"": Take any well-ordered set which has elements with uncountably many predecessors, and let be the first of these; is called the first uncountable ordinal.) For , let be the set of all predecessors (successors) of , and call a subset of open if it is a or an or a or a union of such sets. Prove that is then a compact Hausdorff space. (Hint: No well-ordered set contains an infinite decreasing sequence.) Prove that the complement of the point is an open set which is not -compact. Prove that to every there corresponds an such that is constant on . Prove that the intersection of every countable collection of uncountable compact subsets of is uncountable. (Hint: Consider limits of increasing countable sequences in which intersect each in infinitely many points.) Let be the collection of all such that either or contains an uncountable compact set; in the first case, define ; in the second case, define . Prove that is a -algebra which contains all Borel sets in , that, is a measure on which is not regular (every neighborhood of has measure ), and that for every . Describe the regular which Theorem 2.14 associates with this linear functional. The problem certainly demands more set-theoretic skill than what I possess. My goal is to solve the problem in full, and that would start by understanding what it's saying. I request you to kindly help me with explanations/hints, and I will keep updating this post with my progress on this problem as I'm able to understand and do more parts of it. Thoughts: First, they take to be a well-ordered (every non-empty subset has a least element) uncountable set with last element . What do they mean by last ? What is meant by ""let be the first of these""? They have defined exactly the open sets (i.e. the topology) in . We must prove that is compact and Hausdorff . Hausdorff-ness is clear from Oliver's answer. For compactness, note that the topology on is the order topology, and by Theorem 27.1 of Munkres, we are done. Update: For (1): Note that , since is the last element. So the complement of is open. We want to show that it is not -compact. How do I do that? For (2): Suppose , i.e. is continuous on . I'm not sure what the codomain of the function is, so I'm assuming . It looks like we want to find so that is an set for . For (3): I found this answer , which seems to work. Lastly, I could use hints on how to go about the final part of the problem about measures. Thanks!",2.18 X \omega_1 \omega_1 \omega_1 \omega_1 \alpha\in X P_\alpha[S_\alpha] \alpha X P_\alpha S_\beta P_\alpha \cap S_\beta X \omega_1 \sigma f \in C(X) \alpha\ne\omega_1 f S_\alpha \{K_n\} X X K_n \mathfrak M E \subset X E \cup \{\omega_1\} E^c \cup \{\omega_1\} \lambda(E) = 1 \lambda(E) = O \mathfrak M \sigma X \lambda \mathfrak M \omega_1 1 f(\omega_1) = \int_X f\ d\lambda f \in C(X) \mu X \omega_1 \omega_1 X X X P_{\omega_1} = X\setminus \{\omega_1\} \omega_1 \omega_1 \sigma f\in C(X) f X \mathbb C z\in\mathbb C f^{-1}(\{z\}) S_\alpha \alpha < \omega_1,"['real-analysis', 'general-topology', 'measure-theory']"
48,$f$ is uniformly continuous $\implies |f(x)| < C + D|x|$,is uniformly continuous,f \implies |f(x)| < C + D|x|,"Prove $f$ is uniformly continuous $\implies$ there exist $C, D$ such that $|f(x)| < C + D|x|$ . Proof below.  Please verify or critique. By definition of uniform continuity, there exists $\delta > 0$ such that $|x_a - x_b| \leq \delta \implies |f(x_a)- f(x_b)| < 1$ .  Choose $D > 1/\delta$ and $C > |f(0)| + D + 1$ . For any $x$ , $|f(x)| - |f(0)| \leq  |f(x) - f(0)| \leq \sum_{0 < j \leq |x/\delta|+1}|f(j\delta) - f((j-1)\delta)| \leq |x/\delta|+1$ , so $|f(x)| \leq |x/\delta| + 1 + |f(0)| < C + D|x|$ .","Prove is uniformly continuous there exist such that . Proof below.  Please verify or critique. By definition of uniform continuity, there exists such that .  Choose and . For any , , so .","f \implies C, D |f(x)| < C + D|x| \delta > 0 |x_a - x_b| \leq \delta \implies |f(x_a)- f(x_b)| < 1 D > 1/\delta C > |f(0)| + D + 1 x |f(x)| - |f(0)| \leq  |f(x) - f(0)| \leq \sum_{0 < j \leq |x/\delta|+1}|f(j\delta) - f((j-1)\delta)| \leq |x/\delta|+1 |f(x)| \leq |x/\delta| + 1 + |f(0)| < C + D|x|","['real-analysis', 'solution-verification', 'uniform-continuity']"
49,Conditions for weak convergence and generalized distribution functions,Conditions for weak convergence and generalized distribution functions,,"I am having some trouble proving Corollary 6.3.2 in Borovkov's Probability Theory (for reference, this material is on pages 147 to 149 in the book). For convenience, I provide some definitions and related theorems. Skip to the end for a TLDR. Definition: We say a function $G$ is a generalized distribution function if it satisfies monotonicity and right-continuity. We denote the class of generalized distribution functions to be $\mathcal{G}$ and the class of distribution functions to be $\mathcal{F}$ . Of course, $\mathcal{F}\subseteq \mathcal{G}$ and the only difference is that distribution functions have $\lim_{x\to\infty}F(x) = 1$ and $\lim_{x\to-\infty}F(x) = 0$ . Definition: We say that a sequence of generalized distribution functions $\{G_n\}\subseteq\mathcal{G}$ converges weakly to some $G\in \mathcal{G}$ if, for all points of continuity $x\in\mathbb{R}$ of $G$ , we have $G_n(x)\to G(x)$ . Note that the above definition is not as ""nice"" as weak convergence in distribution functions. Recall that, for $\{F_n\}\subseteq \mathcal{F}$ and $F\in\mathcal{F}$ such that $F_n(x)\to F(x)$ for each point of continuity of $F$ , an equivalent condition is to say that, for all bounded continuous functions $f$ , we have $$\int f\ \mathrm{d}F_n \to \int f\ \mathrm{d}F.$$ However, this equivalence is not true for the case where $G_n,G\in\mathcal{G}$ . Some extra definitions: Definition: A sequence of probability measures $\{\mathbb{F}_n\}_{n=1}^\infty$ is said to be tight if, for all $\varepsilon>0$ , there exists some $N\in\mathbb{N}$ such that $$\inf_n \mathbb{F}_n([-N,N]) > 1- \varepsilon.$$ Definition: A class of bounded continuous functions $\mathcal{L}$ is said to be distribution determining if, for $F\in \mathcal{F}$ and $G\in \mathcal{G}$ , $$\int f\ \mathrm{d}F = \int f\ \mathrm{d}G \qquad (\forall f\in \mathcal{L})$$ implies that $F=G$ . The book then presents a variation of Helly's selection theorem and a corollary: Theorem (Helly's Selection Theorem): Let $\{G_n\}_{n=1}^\infty\subseteq \mathcal{G}$ be a sequence of generalized distribution  functions, then there exists some subsequence $\{G_{n_k}\}_{k=1}^\infty$ and $G\in \mathcal{G}$ such that $G_{n_k}$ converges weakly to $G$ for some $G\in \mathcal{G}$ . That is, the space $\mathcal{G}$ is sequentially compact with respect to weak convergence. Corollary: If every convergent subsequence of a sequence $G_n$ converges weakly to the same $G\in\mathcal{G}$ then the entire sequence $G_n$ converges weakly to $G$ . The following theorem is true: Theorem: Let $\mathcal{L}$ be a distribution determining class and $\{\mathbb{F}_n\}_{n=1}^\infty$ be a sequence of probability measures. Then $\mathbb{F}_n$ converges weakly to some probability measure $\mathbb{F}$ if and only if the sequence $\{\mathbb{F}_n\}_{n=1}^\infty$ is tight and $\lim_n \int f\ \mathrm{d}\mathbb{F}_n$ exists for all $f\in \mathcal{L}$ . I will provide a proof of the converse that basically outlines Borovkov's proof: Proof . By Helly's selection theorem, there exists a subsequence of distribution functions (that correspond to the probability measures) $F_{n_k}$ that converges weakly to some $F\in \mathcal{G}$ . Now, let $\varepsilon>0$ . By the tightness of $\mathbb{F}_{n_k}$ , can find some $M$ such that, for all $x\geq M$ , we have $$\inf_k F_{n_k}(x) \geq \inf_k \mathbb{F}_{n_k}([-M,M]) > 1- \varepsilon.$$ Let $x$ be a point of continuity of $F$ with $x\geq M$ (this must exist as $F$ only has countably many points of discontinuity). We then have $$F(x) = \lim_k F_{n_k}(x) \geq \inf_k F_{n_k}(x) >1- \varepsilon.$$ Further, for all $y\geq x$ , we would have $F(y) > 1- \varepsilon$ . Therefore, we have $\lim_{x\to\infty}F(x) = 1$ . A similar argument shows that $\lim_{x\to-\infty}F(x) = 0$ . Hence, we have $F\in \mathcal{F}$ is actually a distribution function. It remains to show that the entire sequence $\mathbb{F}_n$ converges weakly to the $\mathbb{F}$ (given by the distribution function $F$ ) above. By Corollary to Helly's selection theorem, it suffices to show, for arbitrary $\{F_{n_j}\}_{j=1}^\infty$ subsequence that converges weakly to some $G\in \mathcal{G}$ , we have $F=G$ . First, notice that the above argument ensures the limit function is actually a distribution function, so we actually have $G\in\mathcal{F}$ . Let $\mathbb{F}$ and $\mathbb{G}$ denote the probability measures induced by distribution functions $F$ and $G$ , respectively. Since $F_{n_j}$ converges weakly to $G \in\mathcal{F}$ , we have, for all $f\in \mathcal{L}$ , $$\lim_j \int f\ \mathrm{d}\mathbb{F}_{n_j} = \int f\ \mathrm{d}\mathbb{G}.$$ Further, the weak convergence of our original sequence $\{\mathbb{F}_{n_k}\}_{k=1}^\infty$ to $\mathbb{F}$ gives us that, for all $f\in \mathcal{L}$ , $$\lim_k \int f\ \mathrm{d}\mathbb{F}_{n_k} = \int f\ \mathrm{d}\mathbb{F}.$$ Since $\lim_n \int f\ \mathrm{d}\mathbb{F}_n$ exists for all $f\in \mathcal{L}$ , all its subsequences must converge to the same limit, which gives us $$\int f\ \mathrm{d}\mathbb{F} = \lim_k \int f\ \mathrm{d}\mathbb{F}_{n_k} = \lim_j \int f\ \mathrm{d}\mathbb{F}_{n_j} = \int f\ \mathrm{d}\mathbb{G}$$ As the above equality holds for all $f\in \mathcal{L}$ , a distribution determining class, we have $\mathbb{F} = \mathbb{F}'$ and thus $F=F'$ , as desired. However, I am having trouble proving the following corollary (I copied and paste straight from the text): I am having trouble showing that (2) is sufficient. The argument they gave appears to be down the lines of the following (just basing off the proof of the previous theorem) Consider the sequence $F_n$ , by Helly's selection theorem, we have $F_{n_k}$ converges weakly to some $G\in \mathcal{G}$ . Further, we have, for each of those subsequences that converge weakly, $$\lim_{k\to\infty}\int f\ \mathrm{d}F_{n_k} = \int f\ \mathrm{d}F.$$ Now, it would be nice if we also write (I think the author just assumes this, but this does not appear to be true for me) $$\lim_{k\to\infty}\int f\ \mathrm{d}F_{n_k} = \int f\ \mathrm{d}G$$ If that's true, then we're done because that would imply $G=F$ and every subsequence that converges in $F_n$ converges to the same distribution function. But we cannot do that , because $G\in \mathcal{G}$ and we know that weak convergence (as defined by pointwise convergence to all points of continuity in the limit function) is no longer equivalent to convergence in integrals. TLDR: I have having trouble proving (2) is sufficient in the above corollary and the hint the author gave is confusing.","I am having some trouble proving Corollary 6.3.2 in Borovkov's Probability Theory (for reference, this material is on pages 147 to 149 in the book). For convenience, I provide some definitions and related theorems. Skip to the end for a TLDR. Definition: We say a function is a generalized distribution function if it satisfies monotonicity and right-continuity. We denote the class of generalized distribution functions to be and the class of distribution functions to be . Of course, and the only difference is that distribution functions have and . Definition: We say that a sequence of generalized distribution functions converges weakly to some if, for all points of continuity of , we have . Note that the above definition is not as ""nice"" as weak convergence in distribution functions. Recall that, for and such that for each point of continuity of , an equivalent condition is to say that, for all bounded continuous functions , we have However, this equivalence is not true for the case where . Some extra definitions: Definition: A sequence of probability measures is said to be tight if, for all , there exists some such that Definition: A class of bounded continuous functions is said to be distribution determining if, for and , implies that . The book then presents a variation of Helly's selection theorem and a corollary: Theorem (Helly's Selection Theorem): Let be a sequence of generalized distribution  functions, then there exists some subsequence and such that converges weakly to for some . That is, the space is sequentially compact with respect to weak convergence. Corollary: If every convergent subsequence of a sequence converges weakly to the same then the entire sequence converges weakly to . The following theorem is true: Theorem: Let be a distribution determining class and be a sequence of probability measures. Then converges weakly to some probability measure if and only if the sequence is tight and exists for all . I will provide a proof of the converse that basically outlines Borovkov's proof: Proof . By Helly's selection theorem, there exists a subsequence of distribution functions (that correspond to the probability measures) that converges weakly to some . Now, let . By the tightness of , can find some such that, for all , we have Let be a point of continuity of with (this must exist as only has countably many points of discontinuity). We then have Further, for all , we would have . Therefore, we have . A similar argument shows that . Hence, we have is actually a distribution function. It remains to show that the entire sequence converges weakly to the (given by the distribution function ) above. By Corollary to Helly's selection theorem, it suffices to show, for arbitrary subsequence that converges weakly to some , we have . First, notice that the above argument ensures the limit function is actually a distribution function, so we actually have . Let and denote the probability measures induced by distribution functions and , respectively. Since converges weakly to , we have, for all , Further, the weak convergence of our original sequence to gives us that, for all , Since exists for all , all its subsequences must converge to the same limit, which gives us As the above equality holds for all , a distribution determining class, we have and thus , as desired. However, I am having trouble proving the following corollary (I copied and paste straight from the text): I am having trouble showing that (2) is sufficient. The argument they gave appears to be down the lines of the following (just basing off the proof of the previous theorem) Consider the sequence , by Helly's selection theorem, we have converges weakly to some . Further, we have, for each of those subsequences that converge weakly, Now, it would be nice if we also write (I think the author just assumes this, but this does not appear to be true for me) If that's true, then we're done because that would imply and every subsequence that converges in converges to the same distribution function. But we cannot do that , because and we know that weak convergence (as defined by pointwise convergence to all points of continuity in the limit function) is no longer equivalent to convergence in integrals. TLDR: I have having trouble proving (2) is sufficient in the above corollary and the hint the author gave is confusing.","G \mathcal{G} \mathcal{F} \mathcal{F}\subseteq \mathcal{G} \lim_{x\to\infty}F(x) = 1 \lim_{x\to-\infty}F(x) = 0 \{G_n\}\subseteq\mathcal{G} G\in \mathcal{G} x\in\mathbb{R} G G_n(x)\to G(x) \{F_n\}\subseteq \mathcal{F} F\in\mathcal{F} F_n(x)\to F(x) F f \int f\ \mathrm{d}F_n \to \int f\ \mathrm{d}F. G_n,G\in\mathcal{G} \{\mathbb{F}_n\}_{n=1}^\infty \varepsilon>0 N\in\mathbb{N} \inf_n \mathbb{F}_n([-N,N]) > 1- \varepsilon. \mathcal{L} F\in \mathcal{F} G\in \mathcal{G} \int f\ \mathrm{d}F = \int f\ \mathrm{d}G \qquad (\forall f\in \mathcal{L}) F=G \{G_n\}_{n=1}^\infty\subseteq \mathcal{G} \{G_{n_k}\}_{k=1}^\infty G\in \mathcal{G} G_{n_k} G G\in \mathcal{G} \mathcal{G} G_n G\in\mathcal{G} G_n G \mathcal{L} \{\mathbb{F}_n\}_{n=1}^\infty \mathbb{F}_n \mathbb{F} \{\mathbb{F}_n\}_{n=1}^\infty \lim_n \int f\ \mathrm{d}\mathbb{F}_n f\in \mathcal{L} F_{n_k} F\in \mathcal{G} \varepsilon>0 \mathbb{F}_{n_k} M x\geq M \inf_k F_{n_k}(x) \geq \inf_k \mathbb{F}_{n_k}([-M,M]) > 1- \varepsilon. x F x\geq M F F(x) = \lim_k F_{n_k}(x) \geq \inf_k F_{n_k}(x) >1- \varepsilon. y\geq x F(y) > 1- \varepsilon \lim_{x\to\infty}F(x) = 1 \lim_{x\to-\infty}F(x) = 0 F\in \mathcal{F} \mathbb{F}_n \mathbb{F} F \{F_{n_j}\}_{j=1}^\infty G\in \mathcal{G} F=G G\in\mathcal{F} \mathbb{F} \mathbb{G} F G F_{n_j} G \in\mathcal{F} f\in \mathcal{L} \lim_j \int f\ \mathrm{d}\mathbb{F}_{n_j} = \int f\ \mathrm{d}\mathbb{G}. \{\mathbb{F}_{n_k}\}_{k=1}^\infty \mathbb{F} f\in \mathcal{L} \lim_k \int f\ \mathrm{d}\mathbb{F}_{n_k} = \int f\ \mathrm{d}\mathbb{F}. \lim_n \int f\ \mathrm{d}\mathbb{F}_n f\in \mathcal{L} \int f\ \mathrm{d}\mathbb{F} = \lim_k \int f\ \mathrm{d}\mathbb{F}_{n_k} = \lim_j \int f\ \mathrm{d}\mathbb{F}_{n_j} = \int f\ \mathrm{d}\mathbb{G} f\in \mathcal{L} \mathbb{F} = \mathbb{F}' F=F' F_n F_{n_k} G\in \mathcal{G} \lim_{k\to\infty}\int f\ \mathrm{d}F_{n_k} = \int f\ \mathrm{d}F. \lim_{k\to\infty}\int f\ \mathrm{d}F_{n_k} = \int f\ \mathrm{d}G G=F F_n G\in \mathcal{G}","['real-analysis', 'probability-theory', 'measure-theory', 'weak-convergence', 'cumulative-distribution-functions']"
50,Show that $\prod_{k=0}^n |x-k| \le (n-1)!/2$ for $1 \le x \le n-1$.,Show that  for .,\prod_{k=0}^n |x-k| \le (n-1)!/2 1 \le x \le n-1,"Let $n \ge 3$ , $x \in \Bbb R$ such that $1 \le x \le n-1$ . Show that $\prod_{k=0}^n |x-k| \le (n-1)!/2$ . For $n=3$ , $1 \le x \le 2$ we want to show $|x(x-1)(x-2)(x-3)|=x(x-1)(2-x)(3-x) \le 1$ . We see that some large bounds are easy to guess: $$ x(x-1)(2-x)(3-x) \le 2 \times 1 \times 1 \times 2=4 $$ but not precise enough. Another try: $$ x(x-1)(2-x)(3-x) \le x(3-x) \le 9/4 $$ the last inequality is from the study of $f(x)=x(3-x)$ . Same problem. For $n=3$ and $1 \le x \le 2$ let $0 \le y=x-1 \le 1$ . We get using AM-GM \begin{align} |x(x-1)(x-2)(x-3)| &= |(y+1)(y)(y-1)(y-2)|\\ &=y(1-y^2)(2-y)\\ &\le (\frac{3-y^2}{3})^3\\ &\le  1 \end{align} For the general case I tried to use the same approach: let $1 \le x \le n-1$ ie. $0 \le y = (x-1)/(n-2) \le 1$ . We have \begin{align} \prod_{k=0}^n |x-k| &= \prod_{k=0}^n |(n-1)y +1-k|\\ &= \prod_{k=0}^l |(n-1)y +1-k| \prod_{k=l+1}^n |(n-1)y +1-k|\\ &= \prod_{k=0}^l ((n-1)y +1-k) \prod_{k=l+1}^n (k-1-(n-1)y)\\ \end{align} where $l = \lfloor (n-1)y+1\rfloor$ . Applying AM-GM doesn't really help here. Suppose $y<1$ ie. $l<n$ , we see that $$ \prod_{k=0}^l ((n-1)y +1-k) \le \prod_{k=0}^l (n-k)=\frac{n!}{(n-l-1)!} $$ and $$ \prod_{k=l+1}^n (k-1-(n-1)y) \le \prod_{k=l}^{n-1} (k) = \frac{(n-1)!}{(l-1)!} $$ So $$ \prod_{k=0}^n |x-k|\le \frac{(n-1)!}{(l-1)!}\frac{n!}{(n-l-1)!} $$ The bound is too big... I also tried this approach: let $f(x)= \prod_{k=0}^n (x-k)$ , $f'(x)=f(x)(\sum_{k=0}^n \frac{1}{x-k})$ . I'm trying to find informations about $x$ such that $f'(x)=0$ . Let $x \in \Bbb R - [|0,n|]$ . So $\sum_{k=0}^n \frac{1}{x-k}=0$ . But I'm stuck here. My questions are: Does someone have a hint or a proof? Can someone give me some advice on how to handle such problems during an exam? Thanks :)","Let , such that . Show that . For , we want to show . We see that some large bounds are easy to guess: but not precise enough. Another try: the last inequality is from the study of . Same problem. For and let . We get using AM-GM For the general case I tried to use the same approach: let ie. . We have where . Applying AM-GM doesn't really help here. Suppose ie. , we see that and So The bound is too big... I also tried this approach: let , . I'm trying to find informations about such that . Let . So . But I'm stuck here. My questions are: Does someone have a hint or a proof? Can someone give me some advice on how to handle such problems during an exam? Thanks :)","n \ge 3 x \in \Bbb R 1 \le x \le n-1 \prod_{k=0}^n |x-k| \le (n-1)!/2 n=3 1 \le x \le 2 |x(x-1)(x-2)(x-3)|=x(x-1)(2-x)(3-x) \le 1 
x(x-1)(2-x)(3-x) \le 2 \times 1 \times 1 \times 2=4
 
x(x-1)(2-x)(3-x) \le x(3-x) \le 9/4
 f(x)=x(3-x) n=3 1 \le x \le 2 0 \le y=x-1 \le 1 \begin{align}
|x(x-1)(x-2)(x-3)| &= |(y+1)(y)(y-1)(y-2)|\\
&=y(1-y^2)(2-y)\\
&\le (\frac{3-y^2}{3})^3\\
&\le  1
\end{align} 1 \le x \le n-1 0 \le y = (x-1)/(n-2) \le 1 \begin{align}
\prod_{k=0}^n |x-k|
&=
\prod_{k=0}^n |(n-1)y +1-k|\\
&=
\prod_{k=0}^l |(n-1)y +1-k| \prod_{k=l+1}^n |(n-1)y +1-k|\\
&=
\prod_{k=0}^l ((n-1)y +1-k) \prod_{k=l+1}^n (k-1-(n-1)y)\\
\end{align} l = \lfloor (n-1)y+1\rfloor y<1 l<n 
\prod_{k=0}^l ((n-1)y +1-k) \le \prod_{k=0}^l (n-k)=\frac{n!}{(n-l-1)!}
 
\prod_{k=l+1}^n (k-1-(n-1)y) \le \prod_{k=l}^{n-1} (k) = \frac{(n-1)!}{(l-1)!}
 
\prod_{k=0}^n |x-k|\le
\frac{(n-1)!}{(l-1)!}\frac{n!}{(n-l-1)!}
 f(x)= \prod_{k=0}^n (x-k) f'(x)=f(x)(\sum_{k=0}^n \frac{1}{x-k}) x f'(x)=0 x \in \Bbb R - [|0,n|] \sum_{k=0}^n \frac{1}{x-k}=0","['real-analysis', 'calculus', 'inequality', 'polynomials', 'factorial']"
51,"Prove $(x-1)(y-1)>(e-1)^2$ where $x^y=y^x$, $y>x>0$.","Prove  where , .",(x-1)(y-1)>(e-1)^2 x^y=y^x y>x>0,"Let $x,y$ be different positive real numbers satisfying $x^y=y^x$ . Prove $(x-1)(y-1)>(e-1)^2$ . We may suppose $a=\frac{y}{x}>1$ . Then we obtain $x=a^{\frac{1}{a-1}},y=a^{\frac{a}{a-1}}.$ But how to go on?",Let be different positive real numbers satisfying . Prove . We may suppose . Then we obtain But how to go on?,"x,y x^y=y^x (x-1)(y-1)>(e-1)^2 a=\frac{y}{x}>1 x=a^{\frac{1}{a-1}},y=a^{\frac{a}{a-1}}.","['real-analysis', 'calculus', 'inequality']"
52,Series of characteristic polynomials,Series of characteristic polynomials,,"Consider the sequence of symmetric matrices with diagonal 2 and second-diagonal s $-1$ , e.g. $$ M_4= \begin{pmatrix}                2 & -1 & 0 & 0 \\                -1 & 2 & -1 & 0\\                0 & -1 & 2 & -1\\                0 & 0 & -1 & 2\\               \end{pmatrix}  $$ I've found out that the characteristic polynomials are $$ \begin{cases} P_1(x)=2-x\\ P_2(x)=(2-x)^2-1\\ P_n(x) = (2-x)P_{n-1}(x)-P_{n-2}(x) \end{cases} $$ Or with a variable change $$ \begin{cases} Q_1(y)=y\\ Q_2(y)=y^2-1\\ Q_n(y) = y Q_{n-1}(y)-Q_{n-2}(y) \end{cases} $$ Looking at the first 8 $P_n$ I see that all eigenvalues are real (as for any symmetric matrix), they are between 0 and 4. How can I prove that all eigenvalues are between 0 and 4? Are these polynomials known (have a name)? How can I prove that the polynomial are sandwitched between $$ \frac{1}{x}+\frac{1}{4-x}\quad\text{and}\quad -\frac{1}{x}-\frac{1}{4-x} $$","Consider the sequence of symmetric matrices with diagonal 2 and second-diagonal s , e.g. I've found out that the characteristic polynomials are Or with a variable change Looking at the first 8 I see that all eigenvalues are real (as for any symmetric matrix), they are between 0 and 4. How can I prove that all eigenvalues are between 0 and 4? Are these polynomials known (have a name)? How can I prove that the polynomial are sandwitched between","-1 
M_4= \begin{pmatrix}
               2 & -1 & 0 & 0 \\
               -1 & 2 & -1 & 0\\
               0 & -1 & 2 & -1\\
               0 & 0 & -1 & 2\\
              \end{pmatrix} 
 
\begin{cases}
P_1(x)=2-x\\
P_2(x)=(2-x)^2-1\\
P_n(x) = (2-x)P_{n-1}(x)-P_{n-2}(x)
\end{cases}
 
\begin{cases}
Q_1(y)=y\\
Q_2(y)=y^2-1\\
Q_n(y) = y Q_{n-1}(y)-Q_{n-2}(y)
\end{cases}
 P_n 
\frac{1}{x}+\frac{1}{4-x}\quad\text{and}\quad
-\frac{1}{x}-\frac{1}{4-x}
","['real-analysis', 'linear-algebra', 'polynomials', 'eigenvalues-eigenvectors', 'characteristic-polynomial']"
53,Are there other important measure spaces which are not obtained directly from an outer measure or from the Caratheodory extension theorem?,Are there other important measure spaces which are not obtained directly from an outer measure or from the Caratheodory extension theorem?,,"I have just started to study measure theory and I have a question. But before presenting it, I will provide the context from which it comes. Given a nonempty set $\Omega$ , we say that a set function $\mu$ defined on a algebra $\mathcal{F}\subseteq\mathcal{P}(\Omega)$ is a measure if $\mu(A)\geq 0$ for all $A\in\mathcal{F}$ , $\mu(\varnothing) = 0$ , $\mu$ satisfies the countable additivity property. We say the set function $\mu^{*}:\mathcal{P}(\Omega)\to[0,+\infty)$ is an outer measure if $\mu^{*}(\varnothing) = 0$ , it satisfies the monotonicity property and the countable subadditivity property. We also say that $A\subseteq\Omega$ is $\mu^{*}$ -measurable if, for any set $E\subseteq\Omega$ , one has that \begin{align*} \mu^{*}(E) = \mu^{*}(E\cap A) + \mu^{*}(E\cap A^{c}) \end{align*} Then we have the following theorem: Let $\mu^{*}$ be an outer measure on $\mathcal{P}(\Omega)$ . Let $\mathcal{M} := \mathcal{M}_{\mu^{*}} := \{A:A\,\text{is}\,\mu^{*}\text{-measurable}\}$ . Then $\mathcal{M}$ is a $\sigma$ -algebra $\mu^{*}$ restriced to $\mathcal{M}$ is a measure, and $\mu^{*}(A) = 0$ implies that $\mathcal{P}(A)\subset\mathcal{M}$ . This result makes $(\Omega,\mathcal{M}_{\mu^{*}},\mu^{*})$ a complete measure space. Moreover, it gives an inexhaustible source method to construct measure spaces (as far as I have understood). We may now state the Caratheodory's extension theorem, which says: Let $\mu$ be a measure on a semi-algebra $\mathcal{C}$ and let $\mu^{*}$ be the set function induced by $\mu$ defined on $\mathcal{P}(\Omega)$ s.t. \begin{align*} \mu^{*}(A) = \inf\left\{\sum_{i=1}^{\infty}\mu(A_{i}):\{A_{n}\}_{n\geq1}\subset\mathcal{C},\,A\subset\bigcup_{i=1}^{\infty}A_{n}\right\} \end{align*} Then we have that $\mu^{*}$ is an outer measure, $\mathcal{C}\subset\mathcal{M}_{\mu^{*}}$ , and $\mu^{*} = \mu$ on $\mathcal{C}$ Now let us consider the semialgebra \begin{align*} \mathcal{C} = \{(a,b]:-\infty\leq a\leq b<\infty\}\cup\{(a,\infty):-\infty\leq a < \infty\} \end{align*} as well as the nondecreasing functions $F:\textbf{R}\to\textbf{R}$ which induces the following measure on $\mathcal{C}$ : \begin{align*} \begin{cases} \mu_{F}((a,b]) = F(b+) - F(a+)\\\\ \mu_{F}((a,\infty)) = F(\infty) - F(a+) \end{cases} \end{align*} Let $(\textbf{R},\mathcal{M}_{\mu^{*}_{F}},\mu^{*}_{F})$ be the Caratheodory extesion of $\mu_{F}$ . Then the book defines such measure space as the Lebesgue-Stieltjes measure space and $\mu^{*}_{F}$ is the Lebesgue-Stieltjes measure generated by $F$ . My question is: are there other important measure spaces which are not obtained directly from an outer measure or from the Caratheodory extension theorem? I am new to this so any contribution is appreciated.","I have just started to study measure theory and I have a question. But before presenting it, I will provide the context from which it comes. Given a nonempty set , we say that a set function defined on a algebra is a measure if for all , , satisfies the countable additivity property. We say the set function is an outer measure if , it satisfies the monotonicity property and the countable subadditivity property. We also say that is -measurable if, for any set , one has that Then we have the following theorem: Let be an outer measure on . Let . Then is a -algebra restriced to is a measure, and implies that . This result makes a complete measure space. Moreover, it gives an inexhaustible source method to construct measure spaces (as far as I have understood). We may now state the Caratheodory's extension theorem, which says: Let be a measure on a semi-algebra and let be the set function induced by defined on s.t. Then we have that is an outer measure, , and on Now let us consider the semialgebra as well as the nondecreasing functions which induces the following measure on : Let be the Caratheodory extesion of . Then the book defines such measure space as the Lebesgue-Stieltjes measure space and is the Lebesgue-Stieltjes measure generated by . My question is: are there other important measure spaces which are not obtained directly from an outer measure or from the Caratheodory extension theorem? I am new to this so any contribution is appreciated.","\Omega \mu \mathcal{F}\subseteq\mathcal{P}(\Omega) \mu(A)\geq 0 A\in\mathcal{F} \mu(\varnothing) = 0 \mu \mu^{*}:\mathcal{P}(\Omega)\to[0,+\infty) \mu^{*}(\varnothing) = 0 A\subseteq\Omega \mu^{*} E\subseteq\Omega \begin{align*}
\mu^{*}(E) = \mu^{*}(E\cap A) + \mu^{*}(E\cap A^{c})
\end{align*} \mu^{*} \mathcal{P}(\Omega) \mathcal{M} := \mathcal{M}_{\mu^{*}} := \{A:A\,\text{is}\,\mu^{*}\text{-measurable}\} \mathcal{M} \sigma \mu^{*} \mathcal{M} \mu^{*}(A) = 0 \mathcal{P}(A)\subset\mathcal{M} (\Omega,\mathcal{M}_{\mu^{*}},\mu^{*}) \mu \mathcal{C} \mu^{*} \mu \mathcal{P}(\Omega) \begin{align*}
\mu^{*}(A) = \inf\left\{\sum_{i=1}^{\infty}\mu(A_{i}):\{A_{n}\}_{n\geq1}\subset\mathcal{C},\,A\subset\bigcup_{i=1}^{\infty}A_{n}\right\}
\end{align*} \mu^{*} \mathcal{C}\subset\mathcal{M}_{\mu^{*}} \mu^{*} = \mu \mathcal{C} \begin{align*}
\mathcal{C} = \{(a,b]:-\infty\leq a\leq b<\infty\}\cup\{(a,\infty):-\infty\leq a < \infty\}
\end{align*} F:\textbf{R}\to\textbf{R} \mathcal{C} \begin{align*}
\begin{cases}
\mu_{F}((a,b]) = F(b+) - F(a+)\\\\
\mu_{F}((a,\infty)) = F(\infty) - F(a+)
\end{cases}
\end{align*} (\textbf{R},\mathcal{M}_{\mu^{*}_{F}},\mu^{*}_{F}) \mu_{F} \mu^{*}_{F} F","['real-analysis', 'measure-theory', 'outer-measure']"
54,Does this partial midpoint-convexity imply full convexity?,Does this partial midpoint-convexity imply full convexity?,,"Let $f:(-\infty,0] \to \mathbb [0,\infty)$ be a $C^1$ strictly decreasing function. Definiton: Given $c \in (-\infty,0]$ , we say that $f$ is midpoint-convex at the point $c$ if $$ f((x+y)/2) \le (f(x) + f(y))/2, $$ whenever $(x+y)/2=c$ , $x,y \in (-\infty,0]$ . Question: Let $r<0$ be fixed, and suppose that for every $x \in [r,0]$ , $f$ is midpoint-convex at $x/2$ . Is $f|_{[r,0]}$ convex? I know that $f|_{[r/2,0]}$ is convex. Indeed, the assumption implies that $f|_{[r/2,0]}$ is midpoint-convex in the usual sense, i.e. $$ f((x+y)/2) \le (f(x) + f(y))/2, $$ whenever $x,y \in [r/2,0]$ , and midpoint-convexity plus continuity implies full convexity. Clarification: The point is that on the one hand, we are given assumptions which are stronger than midpoint-convexity at $[r/2,0]$ (since we are given ""convexity-information"" on how values of $f$ on $[r/2,0]$ relate to its values on $[r,0]$ as well. On the other hand, we are assuming something weaker then midpoint-convexity on all $[r,0]$ .","Let be a strictly decreasing function. Definiton: Given , we say that is midpoint-convex at the point if whenever , . Question: Let be fixed, and suppose that for every , is midpoint-convex at . Is convex? I know that is convex. Indeed, the assumption implies that is midpoint-convex in the usual sense, i.e. whenever , and midpoint-convexity plus continuity implies full convexity. Clarification: The point is that on the one hand, we are given assumptions which are stronger than midpoint-convexity at (since we are given ""convexity-information"" on how values of on relate to its values on as well. On the other hand, we are assuming something weaker then midpoint-convexity on all .","f:(-\infty,0] \to \mathbb [0,\infty) C^1 c \in (-\infty,0] f c 
f((x+y)/2) \le (f(x) + f(y))/2,
 (x+y)/2=c x,y \in (-\infty,0] r<0 x \in [r,0] f x/2 f|_{[r,0]} f|_{[r/2,0]} f|_{[r/2,0]} 
f((x+y)/2) \le (f(x) + f(y))/2,
 x,y \in [r/2,0] [r/2,0] f [r/2,0] [r,0] [r,0]","['real-analysis', 'calculus', 'convex-analysis', 'examples-counterexamples']"
55,Exchanging derivative and integral,Exchanging derivative and integral,,"Define the function $g:[1,2]\rightarrow \mathbb{R}$ to be $$ g(t):= \int_0^{\frac{\pi}4} \arctan\frac{\sin 2x}{t-2\sin^2 x}\,dx. $$ I would like to differentiate obtaining \begin{align*} g'(t) &= \frac{\rm d}{{\rm d}t}\int_0^{\frac{\pi}4} \arctan\frac{\sin 2x}{t-2\sin^2 x}\,dx  =\int_0^{\frac{\pi}4} \frac{\partial}{\partial t}\left(\arctan\frac{\sin 2x}{t-2\sin^2 x}\right)\,dx=\\ &= -\int_0^{\frac{\pi}4} \frac{\sin 2x}{2(t-1)\cos 2x + t(t-2)+2} \,dx. \end{align*} and I would like to be sure to be allowed to do that. Call $f(x,t)$ the integrand function, defined over $\left[0,\frac{\pi}4\right]\times [1,2]$ . For all $t\in [1,2]$ we have $|f(x,t)|\leq \frac{\pi}2$ , which is integrable over $\left[0,\frac{\pi}4\right]$ , and for all $x\in \left[0,\frac{\pi}4\right]$ we have that $\frac{\partial f}{\partial t}(x,t)$ exists and satisfies $\left|\frac{\partial f}{\partial t}(x,t)\right|\leq \sin 2x$ , which again is summable over $\left[0,\frac{\pi}4\right]$ . Are this domination conditions enough to be allowed to derive under   the integral sign?","Define the function to be I would like to differentiate obtaining and I would like to be sure to be allowed to do that. Call the integrand function, defined over . For all we have , which is integrable over , and for all we have that exists and satisfies , which again is summable over . Are this domination conditions enough to be allowed to derive under   the integral sign?","g:[1,2]\rightarrow \mathbb{R} 
g(t):= \int_0^{\frac{\pi}4} \arctan\frac{\sin 2x}{t-2\sin^2 x}\,dx.
 \begin{align*}
g'(t) &= \frac{\rm d}{{\rm d}t}\int_0^{\frac{\pi}4} \arctan\frac{\sin 2x}{t-2\sin^2 x}\,dx 
=\int_0^{\frac{\pi}4} \frac{\partial}{\partial t}\left(\arctan\frac{\sin 2x}{t-2\sin^2 x}\right)\,dx=\\
&= -\int_0^{\frac{\pi}4} \frac{\sin 2x}{2(t-1)\cos 2x + t(t-2)+2} \,dx.
\end{align*} f(x,t) \left[0,\frac{\pi}4\right]\times [1,2] t\in [1,2] |f(x,t)|\leq \frac{\pi}2 \left[0,\frac{\pi}4\right] x\in \left[0,\frac{\pi}4\right] \frac{\partial f}{\partial t}(x,t) \left|\frac{\partial f}{\partial t}(x,t)\right|\leq \sin 2x \left[0,\frac{\pi}4\right]",['real-analysis']
56,Can a function $f$ have an antiderivative even though its indefinite integral $F(x) = \int_{a}^{x} f(t)\ dt$ is not one?,Can a function  have an antiderivative even though its indefinite integral  is not one?,f F(x) = \int_{a}^{x} f(t)\ dt,"The fundamental theorem of calculus states that if $f:[a,b] \to \mathbb{R}$ is integrable and $F(x) = \int_{a}^{x} f(t)\ dt$ , then $F'(x) = f(x)$ at every point $x$ at which $f$ is continuous. This means that if $f$ is integrable, $F'(x) = f(x)$ almost everywhere. If $f$ is continuous on $[a,b]$ , then $F$ is an antiderivative of $f$ , since $F'(x) = f(x)$ holds for all $x \in [a,b]$ . But what if $f$ has a discontinuity at some $x \in [a,b]$ ? In this case, it is not necessarily true that $F'(x) = f(x)$ , and so we cannot necessarily conclude that $F$ is an antiderivative of $f$ . Does this mean that there is no antiderivative of $f$ ? Is it possible for $f$ to have an antiderivative but the indefinite integral $F$ is not an antiderivative of $f$ ? I know that if $f$ has a jump discontinuity, then $f$ can have no antiderivative (since the derivative of a function must satisfy the intermediate value property), but what if we have some other type of discontinuity?","The fundamental theorem of calculus states that if is integrable and , then at every point at which is continuous. This means that if is integrable, almost everywhere. If is continuous on , then is an antiderivative of , since holds for all . But what if has a discontinuity at some ? In this case, it is not necessarily true that , and so we cannot necessarily conclude that is an antiderivative of . Does this mean that there is no antiderivative of ? Is it possible for to have an antiderivative but the indefinite integral is not an antiderivative of ? I know that if has a jump discontinuity, then can have no antiderivative (since the derivative of a function must satisfy the intermediate value property), but what if we have some other type of discontinuity?","f:[a,b] \to \mathbb{R} F(x) = \int_{a}^{x} f(t)\ dt F'(x) = f(x) x f f F'(x) = f(x) f [a,b] F f F'(x) = f(x) x \in [a,b] f x \in [a,b] F'(x) = f(x) F f f f F f f f","['real-analysis', 'calculus', 'integration']"
57,Is this monotonicity property equivalent to convexity?,Is this monotonicity property equivalent to convexity?,,"This is a follow-up of this question . Let $\psi:[0,\infty) \to [0,\infty)$ be a strictly increasing $C^2$ (or $C^{\infty}$ ) function, satisfying $\psi(0)=0$ . Suppose that the function $f(r)=\psi'(r)+\frac{\psi(r)}{r}$ is non-increasing. Must $\psi$ be concave? The converse statement is true, i.e. $\psi$ concave implies $f$ non-increasing: Indeed, $$ f'(r)=\psi''(r)+\frac{1}{r}(\psi'(r)-\frac{\psi(r)}{r}), $$ $\psi'' \le 0$ by concavity, and since $\psi(r)=\int_0^r \psi'(t)dt \ge \int_0^r \psi'(r)dt=r\psi'(r)$ , the term $\frac{1}{r}(\psi'(r)-\frac{\psi(r)}{r})$ is also non-positive. Edit: Here is a partial result-I can prove that $\psi''(0) \le 0$ . By our assumption $$ 0 \ge f'(r)=\psi''(r)+\frac{1}{r}(\psi'(r)-\frac{\psi(r)}{r}), $$ for every $r>0$ . Using the mean value theorem (twice), we can rewrite this as $$ f'(r)=\psi''(r)+\psi''(s(r)) \le 0, \tag{1} $$ where $s(r)$ is some point in $(0,r)$ . In particular, taking the limit when $r \to 0$ , we deduce that $\psi''(0) \le 0$ .","This is a follow-up of this question . Let be a strictly increasing (or ) function, satisfying . Suppose that the function is non-increasing. Must be concave? The converse statement is true, i.e. concave implies non-increasing: Indeed, by concavity, and since , the term is also non-positive. Edit: Here is a partial result-I can prove that . By our assumption for every . Using the mean value theorem (twice), we can rewrite this as where is some point in . In particular, taking the limit when , we deduce that .","\psi:[0,\infty) \to [0,\infty) C^2 C^{\infty} \psi(0)=0 f(r)=\psi'(r)+\frac{\psi(r)}{r} \psi \psi f 
f'(r)=\psi''(r)+\frac{1}{r}(\psi'(r)-\frac{\psi(r)}{r}),
 \psi'' \le 0 \psi(r)=\int_0^r \psi'(t)dt \ge \int_0^r \psi'(r)dt=r\psi'(r) \frac{1}{r}(\psi'(r)-\frac{\psi(r)}{r}) \psi''(0) \le 0 
0 \ge f'(r)=\psi''(r)+\frac{1}{r}(\psi'(r)-\frac{\psi(r)}{r}),
 r>0 
f'(r)=\psi''(r)+\psi''(s(r)) \le 0, \tag{1}
 s(r) (0,r) r \to 0 \psi''(0) \le 0","['real-analysis', 'calculus', 'convex-analysis', 'functional-inequalities', 'convexity-inequality']"
58,Behavior of the Fourier Transform at infinity,Behavior of the Fourier Transform at infinity,,"I have a problem in the proof of the following result: Given $f \in L^1(\mathbb{R}^n)$ , we have that $$|\hat{f}(\xi)| \rightarrow 0, \;\;\; as |\xi| \rightarrow \infty.$$ This result is known as the Riemann-Lebesgue Lemma (Proposition 2.2.17 from Grafakos's book Classical Fourier Analysis, third edition). In the proof of this proposition, one considers the function $$g := \prod_{j=1}^n \chi_{[a_j,b_j]},$$ that I suppose is the characteristic function of the cube $\prod_{j=1}^n[a_j,b_j] \subset \mathbb{R}^n$ , and whose Fourier transform is $$\hat{g}(\xi) = \prod_{j=1}^n \frac{e^{-2\pi i \xi_ja_j} - e^{-2\pi i \xi_jb_j} }{2\pi i \xi_j},$$ in the meaning that if some $\xi_j = 0$ , the correspondent  factor is equal $b_j-a_j$ .  Now, if $\xi =(\xi_1, ..., \xi_n) \neq 0$ , choose $j_0$ such that $|\xi_{j_0}| \geq |\xi|/\sqrt{n}$ . So that $$\left| \prod_{j=1}^n  \frac{e^{-2\pi i \xi_ja_j} - e^{-2\pi i \xi_jb_j} }{2\pi i \xi_j} \right| \leq  \frac{2\sqrt{n}}{2\pi|\xi|}\sup_{1\leq j_0\leq n}\prod_{j\neq j_0}(b_j-a_j).$$ This inequality is what I'm not being able to prove. Once it's proved, I have the desired result.","I have a problem in the proof of the following result: Given , we have that This result is known as the Riemann-Lebesgue Lemma (Proposition 2.2.17 from Grafakos's book Classical Fourier Analysis, third edition). In the proof of this proposition, one considers the function that I suppose is the characteristic function of the cube , and whose Fourier transform is in the meaning that if some , the correspondent  factor is equal .  Now, if , choose such that . So that This inequality is what I'm not being able to prove. Once it's proved, I have the desired result.","f \in L^1(\mathbb{R}^n) |\hat{f}(\xi)| \rightarrow 0, \;\;\; as |\xi| \rightarrow \infty. g := \prod_{j=1}^n \chi_{[a_j,b_j]}, \prod_{j=1}^n[a_j,b_j] \subset \mathbb{R}^n \hat{g}(\xi) = \prod_{j=1}^n \frac{e^{-2\pi i \xi_ja_j} - e^{-2\pi i \xi_jb_j} }{2\pi i \xi_j}, \xi_j = 0 b_j-a_j \xi =(\xi_1, ..., \xi_n) \neq 0 j_0 |\xi_{j_0}| \geq |\xi|/\sqrt{n} \left| \prod_{j=1}^n  \frac{e^{-2\pi i \xi_ja_j} - e^{-2\pi i \xi_jb_j} }{2\pi i \xi_j} \right| \leq 
\frac{2\sqrt{n}}{2\pi|\xi|}\sup_{1\leq j_0\leq n}\prod_{j\neq j_0}(b_j-a_j).","['real-analysis', 'analysis', 'fourier-analysis', 'fourier-transform']"
59,Finding conditions for $\lim\limits_{n\to\infty}\sum\limits_{k=0}^{\lfloor\frac1{a_n}\rfloor}(-1)^k\binom nk(1-ka_n)^{n-1}=1$,Finding conditions for,\lim\limits_{n\to\infty}\sum\limits_{k=0}^{\lfloor\frac1{a_n}\rfloor}(-1)^k\binom nk(1-ka_n)^{n-1}=1,Question: Find necessary and sufficient condition on the sequence $(a_n)_{n=1}^∞$ so that $$\lim_{n→∞}\sum_{k=0}^{\lfloor\frac1{a_n}\rfloor}(-1)^k\binom nk(1-ka_n)^{n-1}=1\tag 1$$ given that $\lim\limits_{n\to\infty}a_n=0$ and $a_n\gt 0$ for all $n\in\Bbb{N}$ . After some guesswork I got to a condition that if $\sum\limits_{n\ge 1} a_n=\infty$ then eq.(1) holds. But I was not able to prove it neither could I find a counterexample for the conjecture. Searching on internet I found that this sum is very closely related to a special case of Dvoretzky covering problem but still couldn't find the necessary and sufficient condition. Until now I have tried using approximations for the Binomial Coefficient and binomial approximation to tackle the sum to no avail. I would be glad if someone could help. Edit: I have got a counterexample for my conjecture i.e. $\sum\limits_{n\ge 1} a_n=\infty$ is alone not sufficient for eq.(1) to hold. So what should be the necessary and sufficient condition?,Question: Find necessary and sufficient condition on the sequence so that given that and for all . After some guesswork I got to a condition that if then eq.(1) holds. But I was not able to prove it neither could I find a counterexample for the conjecture. Searching on internet I found that this sum is very closely related to a special case of Dvoretzky covering problem but still couldn't find the necessary and sufficient condition. Until now I have tried using approximations for the Binomial Coefficient and binomial approximation to tackle the sum to no avail. I would be glad if someone could help. Edit: I have got a counterexample for my conjecture i.e. is alone not sufficient for eq.(1) to hold. So what should be the necessary and sufficient condition?,(a_n)_{n=1}^∞ \lim_{n→∞}\sum_{k=0}^{\lfloor\frac1{a_n}\rfloor}(-1)^k\binom nk(1-ka_n)^{n-1}=1\tag 1 \lim\limits_{n\to\infty}a_n=0 a_n\gt 0 n\in\Bbb{N} \sum\limits_{n\ge 1} a_n=\infty \sum\limits_{n\ge 1} a_n=\infty,"['real-analysis', 'probability', 'sequences-and-series', 'limits']"
60,Looking for a Real Analysis textbook with solutions for self study,Looking for a Real Analysis textbook with solutions for self study,,I am wondering if there are any good texts out there that also have a solution easily available. I've found solutions to Mathematical Analysis by Apostol and Principles of Mathematical Analysis by Rudin but found them a bit too dense for my background (single-variable calculus with an emphasis on theory/proofs instead of application). I also came across Understanding Analysis by Abbott but I'd prefer something that covers a more extensive range of topics (like that of the first two). I might be asking for too much but is there a book that: Has a good exposition of the content (I'd prefer more hand-holding over less) Has solutions either online (not necessarily by the auther of the book) or in the back of the book Covers more content like Lebesgue Integrals or perhaps Multivariable Calculus My initial plan was to go with Understanding Analysis since it covers 1) and 2) but then I'd face the issue with finding the same characteristics in a book that picks up where Understanding Analysis left off as most textbooks don't have easily accessible solutions which I find incredibly useful especially since I'll be self-studying.,I am wondering if there are any good texts out there that also have a solution easily available. I've found solutions to Mathematical Analysis by Apostol and Principles of Mathematical Analysis by Rudin but found them a bit too dense for my background (single-variable calculus with an emphasis on theory/proofs instead of application). I also came across Understanding Analysis by Abbott but I'd prefer something that covers a more extensive range of topics (like that of the first two). I might be asking for too much but is there a book that: Has a good exposition of the content (I'd prefer more hand-holding over less) Has solutions either online (not necessarily by the auther of the book) or in the back of the book Covers more content like Lebesgue Integrals or perhaps Multivariable Calculus My initial plan was to go with Understanding Analysis since it covers 1) and 2) but then I'd face the issue with finding the same characteristics in a book that picks up where Understanding Analysis left off as most textbooks don't have easily accessible solutions which I find incredibly useful especially since I'll be self-studying.,,"['real-analysis', 'analysis', 'reference-request', 'self-learning', 'book-recommendation']"
61,Fourier Transform of compactly supported Distribution is actually a Function,Fourier Transform of compactly supported Distribution is actually a Function,,"If $u$ is a compactly-supported distribution on $\mathbb{R}^n$ , how can we prove that its Fourier transform $\mathcal{F}u$ is the tempered distribution given by the function $\xi\mapsto u(e^{-ix\xi})$ ? Here, the Fourier transform is defined on Schwartz functions as $\mathcal{F}\phi(\xi)=\int e^{-ix\xi}\phi(x) dx $ , and on distributions as $\mathcal{F}u(\phi)=u(\mathcal{F}\phi)$ . If $u$ is compactly-supported, then $u=\chi u$ for some compactly-supported smooth $\chi$ , and so $u(e^{-ix\xi}) = u(\chi(x)e^{-ix\xi})$ is well-defined for any $\xi$ . Intuitively, one has $$ \int_{\xi\in\mathbb{R}^n} u(\chi(x)e^{-ix\xi}) \phi(\xi) d\xi = u\left(\int_{\xi\in\mathbb{R}^n}\chi(x)e^{-ix\xi}\phi(\xi) d\xi \right) $$ which is what we want to show (the LHS is $u$ applied to that function on $x$ ), but I'm not sure how rigorous it is to pull the integral sign inside the distribution.","If is a compactly-supported distribution on , how can we prove that its Fourier transform is the tempered distribution given by the function ? Here, the Fourier transform is defined on Schwartz functions as , and on distributions as . If is compactly-supported, then for some compactly-supported smooth , and so is well-defined for any . Intuitively, one has which is what we want to show (the LHS is applied to that function on ), but I'm not sure how rigorous it is to pull the integral sign inside the distribution.",u \mathbb{R}^n \mathcal{F}u \xi\mapsto u(e^{-ix\xi}) \mathcal{F}\phi(\xi)=\int e^{-ix\xi}\phi(x) dx  \mathcal{F}u(\phi)=u(\mathcal{F}\phi) u u=\chi u \chi u(e^{-ix\xi}) = u(\chi(x)e^{-ix\xi}) \xi  \int_{\xi\in\mathbb{R}^n} u(\chi(x)e^{-ix\xi}) \phi(\xi) d\xi = u\left(\int_{\xi\in\mathbb{R}^n}\chi(x)e^{-ix\xi}\phi(\xi) d\xi \right)  u x,"['real-analysis', 'fourier-analysis', 'fourier-transform', 'distribution-theory', 'schwartz-space']"
62,"Does $\sum_{n=0}^{\infty} \frac{x^2}{(1+x^2)^n}$ converge uniformly on $(-\infty,\infty)$?",Does  converge uniformly on ?,"\sum_{n=0}^{\infty} \frac{x^2}{(1+x^2)^n} (-\infty,\infty)","Does $\displaystyle \sum_{n=0}^{\infty} \frac{x^2}{(1+x^2)^n}$ converge uniformly on $(-\infty,\infty)$ ? My attempt: No. Consider the case where $x=0$ , then $\displaystyle \sum_{n=0}^{\infty} \frac{x^2}{(1+x^2)^n} = 0$ . For $x \neq 0$ , observe $\displaystyle 0 \lt \frac{1}{(1+x^2)^n} \lt 1$ , so by geometric series formula $\displaystyle \sum_{n=0}^{\infty} \frac{x^2}{(1+x^2)^n}$ $\displaystyle = \frac{x^2}{1 - \frac{1}{1+x^2}} = 1+x^2$ (1) So clearly the series doesn't even converge for all $x$ , let alone converge uniformly. Now, my question is about the case where $x \neq 0$ . Does it converge uniformly to $1 + x^2$ ? (2) I think, ""yes"". By Dini's theorem for series the convergence of the series to $1 + x^2$ must be uniform since $1+x^2$ is continuous and $(-\infty,0) \cup (0,\infty)$ is compact. Is my reasoning for (1) and (2) correct?","Does converge uniformly on ? My attempt: No. Consider the case where , then . For , observe , so by geometric series formula (1) So clearly the series doesn't even converge for all , let alone converge uniformly. Now, my question is about the case where . Does it converge uniformly to ? (2) I think, ""yes"". By Dini's theorem for series the convergence of the series to must be uniform since is continuous and is compact. Is my reasoning for (1) and (2) correct?","\displaystyle \sum_{n=0}^{\infty} \frac{x^2}{(1+x^2)^n} (-\infty,\infty) x=0 \displaystyle \sum_{n=0}^{\infty} \frac{x^2}{(1+x^2)^n} = 0 x \neq 0 \displaystyle 0 \lt \frac{1}{(1+x^2)^n} \lt 1 \displaystyle \sum_{n=0}^{\infty} \frac{x^2}{(1+x^2)^n} \displaystyle = \frac{x^2}{1 - \frac{1}{1+x^2}} = 1+x^2 x x \neq 0 1 + x^2 1 + x^2 1+x^2 (-\infty,0) \cup (0,\infty)","['real-analysis', 'sequences-and-series', 'proof-verification', 'uniform-convergence']"
63,"Prove inequality $|y \ln{y} - x \ln{x}| < 2 |\ln{\frac{1}{|y-x|}}|$ when $x,y \in (0,1]$, $x \neq y$.","Prove inequality  when , .","|y \ln{y} - x \ln{x}| < 2 |\ln{\frac{1}{|y-x|}}| x,y \in (0,1] x \neq y","EDIT: Counter-example found. Statement is FALSE. However, I think the argument still has value. It is true if you restrict the domain to $[0.223,0.716]$ . Maybe $[\frac{3}{10},\frac{7}{10}]$ so it’s less “obvious”? Prove inequality $|y \ln{y} - x \ln{x}| < 2 |\ln{\frac{1}{|y-x|}}|$ , when $x,y \in (0,1]$ , $x \neq y$ . Assume WLOG $y > x$ . I have tried Mean Value Theorem. For some $c \in (0,1)$ : \begin{align*} |y \ln{y} - x \ln{x}| &= (1+\ln{c})(y-x) = (\ln(e)+\ln(c))(y-x)\\ & \leq 2 \ln{\frac{1}{(y-x)}} \text{ if and only if } \frac{(y-x)^2}{e} \leq c \leq \frac{1}{e(y-x)^2} \end{align*} So the inequality is true when $$ \frac{(y-x)^2}{e} \leq x \leq y \leq \frac{1}{e(y-x)^2} $$ in which $c$ is between $x$ and $y$ . Now we need to prove for the following cases in a different way: \begin{align*} x < \frac{(y-x)^2}{e} &\implies x^2 - (2+e)x + 1 > 0 : \text{when 0<x<0.222ish}\\ \frac{1}{e(y-x)^2} < y &\implies y > e^{-1/3} : \text{when y > e^-1/3 = 0.716ish } \end{align*} as $1-x>y-x>y$ . I think this statement should be true because some of the cases can be taken care by MVT, and cases where it can’t be solved by MVT are when $x$ and $y$ are near the endpoints. I tried using taylor $\ln(1+x)$ but I can’t get anything. Any help would be appreciated.","EDIT: Counter-example found. Statement is FALSE. However, I think the argument still has value. It is true if you restrict the domain to . Maybe so it’s less “obvious”? Prove inequality , when , . Assume WLOG . I have tried Mean Value Theorem. For some : So the inequality is true when in which is between and . Now we need to prove for the following cases in a different way: as . I think this statement should be true because some of the cases can be taken care by MVT, and cases where it can’t be solved by MVT are when and are near the endpoints. I tried using taylor but I can’t get anything. Any help would be appreciated.","[0.223,0.716] [\frac{3}{10},\frac{7}{10}] |y \ln{y} - x \ln{x}| < 2 |\ln{\frac{1}{|y-x|}}| x,y \in (0,1] x \neq y y > x c \in (0,1) \begin{align*}
|y \ln{y} - x \ln{x}| &= (1+\ln{c})(y-x) = (\ln(e)+\ln(c))(y-x)\\
& \leq 2 \ln{\frac{1}{(y-x)}} \text{ if and only if } \frac{(y-x)^2}{e} \leq c \leq \frac{1}{e(y-x)^2}
\end{align*} 
\frac{(y-x)^2}{e} \leq x \leq y \leq \frac{1}{e(y-x)^2}
 c x y \begin{align*}
x < \frac{(y-x)^2}{e} &\implies x^2 - (2+e)x + 1 > 0 : \text{when 0<x<0.222ish}\\
\frac{1}{e(y-x)^2} < y &\implies y > e^{-1/3} : \text{when y > e^-1/3 = 0.716ish }
\end{align*} 1-x>y-x>y x y \ln(1+x)","['real-analysis', 'inequality', 'logarithms', 'examples-counterexamples']"
64,Is $\lim\limits_{n\to\infty}\Bigl(\frac{2+\sin n}{3}\Bigr)^n =0 $; $n \in \mathbb{N}$?,Is ; ?,\lim\limits_{n\to\infty}\Bigl(\frac{2+\sin n}{3}\Bigr)^n =0  n \in \mathbb{N},$$\lim_{n\to\infty}\Bigl(\frac{2+\sin n}{3}\Bigr)^n $$ where $n$ is a natural number. The problem is if the numerator is less than 3. I think it is because $\sin  n$ is never $1$ otherwise $\pi$ would be rational. But i am not sure if the limit exists because $\sin$ oscillates. How to solve this ?,where is a natural number. The problem is if the numerator is less than 3. I think it is because is never otherwise would be rational. But i am not sure if the limit exists because oscillates. How to solve this ?,\lim_{n\to\infty}\Bigl(\frac{2+\sin n}{3}\Bigr)^n  n \sin  n 1 \pi \sin,"['real-analysis', 'sequences-and-series', 'limits']"
65,"What does ""isomorphism"" exactly refer to in topology?","What does ""isomorphism"" exactly refer to in topology?",,"I am reading an article and I came across the sentence that states as below: ""A linear separating isomorphism from $C(T)$ onto $C(S)$ is continuous, in which $C(S)$ and $C(T)$ denote sup-normed Banach spaces of real or complex-valued continuous functions on the compact Hausdorff spaces $S$ and $T$ , respectively."" The writer has used only the injectivity and surjectivity properties to prove the theorem, so it has made me think that the isomorphism mentioned above is the same as a bijection map . On the other hand, I have been used to seeing isomorphisms in Algebra. Is there anything more than this with the word isomorphism in the sentence above? Any help would be highly appreciated.","I am reading an article and I came across the sentence that states as below: ""A linear separating isomorphism from onto is continuous, in which and denote sup-normed Banach spaces of real or complex-valued continuous functions on the compact Hausdorff spaces and , respectively."" The writer has used only the injectivity and surjectivity properties to prove the theorem, so it has made me think that the isomorphism mentioned above is the same as a bijection map . On the other hand, I have been used to seeing isomorphisms in Algebra. Is there anything more than this with the word isomorphism in the sentence above? Any help would be highly appreciated.",C(T) C(S) C(S) C(T) S T,"['real-analysis', 'general-topology']"
66,Find the limit of $\frac{n^{x}}{(1 + x)^{n}}$,Find the limit of,\frac{n^{x}}{(1 + x)^{n}},"Let $x>0$ . Find $$\lim_{n\to\infty}\frac{n^{x}}{(1 + x)^{n}}.$$ What I was thinking involved an inequality, by reason of considering the fact that $$(1 + x)^{n} > C(n, k) x^{k}$$ for some $k$ such that $n>k$ . Expanding the binomial coefficient and taking the $n$ common from each bracket, we have something like $$(1 + x)^{n} > C(n, k) x^{n} > \frac {x^{k}n^{k}}{k! 2^{k}}.$$ But now, I am totally clueless. By intuition, I know that the limit has to be zero, so I just want to prove using the concept of inequalities that what I thought was the right approach (being the sandwich theorem). Any help would be appreciated!","Let . Find What I was thinking involved an inequality, by reason of considering the fact that for some such that . Expanding the binomial coefficient and taking the common from each bracket, we have something like But now, I am totally clueless. By intuition, I know that the limit has to be zero, so I just want to prove using the concept of inequalities that what I thought was the right approach (being the sandwich theorem). Any help would be appreciated!","x>0 \lim_{n\to\infty}\frac{n^{x}}{(1 + x)^{n}}. (1 + x)^{n} > C(n, k) x^{k} k n>k n (1 + x)^{n} > C(n, k) x^{n} > \frac {x^{k}n^{k}}{k! 2^{k}}.","['real-analysis', 'sequences-and-series', 'limits']"
67,"Given $a,b,c$ positive numbers, is there a function $h$ such that $h(ax+b) = c \cdot h(x)$ for all $x>0$?","Given  positive numbers, is there a function  such that  for all ?","a,b,c h h(ax+b) = c \cdot h(x) x>0","Given $a,b,c$ positive numbers, is there a function $h$ (no trivial) such that $$h(ax+b) = c\, h(x)$$ for all $x>0$ ?. I know that taking $h(x)= c^{\frac{x}{b}}$ makes $h(x+b) = c \, h(x)$ , and that taking $h(x)= c^{\frac{\ln(x)}{\ln(a)}}$ makes $h(ax)=c\,h(x)$ but i don't see how to combine both conditions. Any help will be appreciated","Given positive numbers, is there a function (no trivial) such that for all ?. I know that taking makes , and that taking makes but i don't see how to combine both conditions. Any help will be appreciated","a,b,c h h(ax+b) = c\, h(x) x>0 h(x)= c^{\frac{x}{b}} h(x+b) = c \, h(x) h(x)= c^{\frac{\ln(x)}{\ln(a)}} h(ax)=c\,h(x)","['real-analysis', 'functional-equations']"
68,"Prove that every continuous map $f: [0,1] \rightarrow [0,1]$ has a fixed point.",Prove that every continuous map  has a fixed point.,"f: [0,1] \rightarrow [0,1]","Prove that every continuous map $f: [0,1] \rightarrow [0,1]$ has a fixed point. Suppose $f$ does not have a fixed point, then $\forall x \in [0,1], f(x) \neq x$ . Thus we have a well defined function $g(x) = \frac{1}{f(x)-x}$ . Note that as $g(x)$ is the composition or continuous functions, it must be continuous. However, $g(0) > 0$ and $g(1) < 0$ , so by the intermediate value theorem $\exists x \in [0,1]$ such that $g(x) = 0$ . This is clearly impossible. Thus $f$ has a fixed point.","Prove that every continuous map has a fixed point. Suppose does not have a fixed point, then . Thus we have a well defined function . Note that as is the composition or continuous functions, it must be continuous. However, and , so by the intermediate value theorem such that . This is clearly impossible. Thus has a fixed point.","f: [0,1] \rightarrow [0,1] f \forall x \in [0,1], f(x) \neq x g(x) = \frac{1}{f(x)-x} g(x) g(0) > 0 g(1) < 0 \exists x \in [0,1] g(x) = 0 f",['real-analysis']
69,Convergence of $(\sin x)^x$,Convergence of,(\sin x)^x,This showed up on a forum and had no clear answer so I'm asking this here to see if anyone can give some light. Define the sequence $$a_n := (\sin n)^n \ \forall n \in \mathbb{N}$$ How do you proof the existence of $\lim_{n\to \infty} a_n$ ? Thanks.,This showed up on a forum and had no clear answer so I'm asking this here to see if anyone can give some light. Define the sequence How do you proof the existence of ? Thanks.,a_n := (\sin n)^n \ \forall n \in \mathbb{N} \lim_{n\to \infty} a_n,"['real-analysis', 'number-theory']"
70,Existence of convex defining functions for convex domains,Existence of convex defining functions for convex domains,,"I have a question regarding the construction of a barrier frequently used in PDE. The barrier used is the following: Let $\Omega$ be a uniformly convex domain in $\mathbb{R}^n$ with $C^2$ boundary. Here uniformly convex means there exists some $r>0$ such that every point in $\partial \Omega$ satisfies an interior sphere condition for a sphere with radius $r$ . Then there exists a uniformly convex defining function $h \in C^2(\Omega)$ , that is a function satisfying $$ h < 0 \text{ in } \Omega, \quad h = 0 \text{ on }\partial\Omega$$ and $$ |Dh| = 1 \text{ on }\partial\Omega, \quad D^2h \geq \delta I \text{ in } \Omega,$$ for some $\delta >0$ . My question is the following: Whilst I understand how such a function can be constructed in some neighborhood of the boundary by taking for example $h(x) = -\text{dist}(x,\partial \Omega)+\text{dist}(x,\partial \Omega)^2$ (as outlined in Gilbarg and Trudinger $\S$ 14.6 or the footnote on page 40 of Figalli's Monge–Ampère book), how does one extend this function to the entire domain? That is how does one explicitly construct $h$ ?","I have a question regarding the construction of a barrier frequently used in PDE. The barrier used is the following: Let be a uniformly convex domain in with boundary. Here uniformly convex means there exists some such that every point in satisfies an interior sphere condition for a sphere with radius . Then there exists a uniformly convex defining function , that is a function satisfying and for some . My question is the following: Whilst I understand how such a function can be constructed in some neighborhood of the boundary by taking for example (as outlined in Gilbarg and Trudinger 14.6 or the footnote on page 40 of Figalli's Monge–Ampère book), how does one extend this function to the entire domain? That is how does one explicitly construct ?","\Omega \mathbb{R}^n C^2 r>0 \partial \Omega r h \in C^2(\Omega)  h < 0 \text{ in } \Omega, \quad h = 0 \text{ on }\partial\Omega  |Dh| = 1 \text{ on }\partial\Omega, \quad D^2h \geq \delta I \text{ in } \Omega, \delta >0 h(x) = -\text{dist}(x,\partial \Omega)+\text{dist}(x,\partial \Omega)^2 \S h","['real-analysis', 'differential-geometry', 'partial-differential-equations', 'convex-analysis', 'elliptic-operators']"
71,Proving a specific set in $\mathbb R^2$ is closed.,Proving a specific set in  is closed.,\mathbb R^2,"I am trying to prove that the set $A=\{(x,y)|x^2\leq y\}$ is closed in $\mathbb R^2$ . I wrote a proof, but I think the end is wrong. My proof is: Consider the set $A=\{(x,y)|x^2\leq y\}$ in $\mathbb R^2$ . Let $(x_n,y_n)_{n \in \mathbb N}$ be a sequence in $A$ that converges to $(x,y) \in \mathbb R^2$ . By the componentwise convergence criterion, $(x_n,y_n) \to (x,y)$ iff $x_n \to x$ and $y_n \to y$ as $n \to \infty$ . (From here on out, I think it is wrong as I really didn't know how to proceed). So, $x_n^2 \to x^2$ and $y_n \to y$ as $n \to \infty$ . Since $\forall n \in \mathbb N \ \ (x_n,y_n)\in A$ , $x_n^2\leq y$ $\forall n\in \mathbb N$ . Hence by taking the limit as $n \to \infty$ of both sides, $x^2\leq y$ . Therefore, $(x,y)\in A$ and so $A$ is closed.","I am trying to prove that the set is closed in . I wrote a proof, but I think the end is wrong. My proof is: Consider the set in . Let be a sequence in that converges to . By the componentwise convergence criterion, iff and as . (From here on out, I think it is wrong as I really didn't know how to proceed). So, and as . Since , . Hence by taking the limit as of both sides, . Therefore, and so is closed.","A=\{(x,y)|x^2\leq y\} \mathbb R^2 A=\{(x,y)|x^2\leq y\} \mathbb R^2 (x_n,y_n)_{n \in \mathbb N} A (x,y) \in \mathbb R^2 (x_n,y_n) \to (x,y) x_n \to x y_n \to y n \to \infty x_n^2 \to x^2 y_n \to y n \to \infty \forall n \in \mathbb N \ \ (x_n,y_n)\in A x_n^2\leq y \forall n\in \mathbb N n \to \infty x^2\leq y (x,y)\in A A",['real-analysis']
72,Stolz-Cesàro $0/0$ case: is $\limsup \frac{a_n}{b_n}\le \limsup\frac{a_{n+1}-a_n}{b_{n+1}-b_n}$?,Stolz-Cesàro  case: is ?,0/0 \limsup \frac{a_n}{b_n}\le \limsup\frac{a_{n+1}-a_n}{b_{n+1}-b_n},"The general form of Stolz-Cesaro $\infty/\infty$ case states that any two real two sequences $a_n$ and $b_n$ , with the latter being monotone and unbounded, satisfy $$\liminf\frac{a_{n+1}-a_n}{b_{n+1}-b_n}\le\liminf\frac{a_n}{b_n}\le\limsup \frac{a_n}{b_n}\le \limsup\frac{a_{n+1}-a_n}{b_{n+1}-b_n}.$$ Does the same hold for the $0/0$ case? That is, is it true that if $\lim a_n=\lim b_n=0$ and $b_n$ is strictly monotone, then $$\liminf\frac{a_{n+1}-a_n}{b_{n+1}-b_n}\le\liminf\frac{a_n}{b_n}\le\limsup \frac{a_n}{b_n}\le \limsup\frac{a_{n+1}-a_n}{b_{n+1}-b_n}$$ ? EDIT : Here's my attempt, please any feedback is appreciated. I tried with the $\limsup$ , assuming $0<b_{n+1}<b_n$ for all $n$ . Suppose $\alpha>\limsup_{n\to\infty}\frac{a_{n+1}-a_n}{b_{n+1}-b_n}$ . Then there exist infinitely many $N$ such that for all $k\ge0$ , $$\alpha>\frac{a_{N+k}-a_{N+k-1}}{b_{N+k}-b_{N+k-1}}.$$ Since $b_{n+1}<b_n$ , we have for $k\ge0$ that $\alpha(b_{N+k}-b_{n+K-1})<a_{N+k}-a_{N+k-1}$ . Thus for any $m\ge0$ , \begin{align} \alpha\sum_{k=0}^m(b_{N+k}-b_{N+k-1}) &< \sum_{k=0}^m(a_{N+k}-a_{N+k-1}) \\ \alpha(b_{N+m}-b_{N-1})&<a_{N+m}-a_{n-1}\end{align} and taking $m\to\infty$ , \begin{align} -\alpha b_{N-1}&<-a_{N-1}  \\ \alpha&>\frac{a_{N-1}}{b_{N-1}}.\end{align} Taking finally $N\to\infty$ , we must have $\alpha\ge\limsup_{n\to\infty}\frac{a_n}{b_n}$ . Thus we can conclude $$\limsup_{n\to\infty}\frac{a_{n+1}-a_n}{b_{n+1}-b_n}\ge\limsup_{n\to\infty}\frac{a_n}{b_n}.$$","The general form of Stolz-Cesaro case states that any two real two sequences and , with the latter being monotone and unbounded, satisfy Does the same hold for the case? That is, is it true that if and is strictly monotone, then ? EDIT : Here's my attempt, please any feedback is appreciated. I tried with the , assuming for all . Suppose . Then there exist infinitely many such that for all , Since , we have for that . Thus for any , and taking , Taking finally , we must have . Thus we can conclude",\infty/\infty a_n b_n \liminf\frac{a_{n+1}-a_n}{b_{n+1}-b_n}\le\liminf\frac{a_n}{b_n}\le\limsup \frac{a_n}{b_n}\le \limsup\frac{a_{n+1}-a_n}{b_{n+1}-b_n}. 0/0 \lim a_n=\lim b_n=0 b_n \liminf\frac{a_{n+1}-a_n}{b_{n+1}-b_n}\le\liminf\frac{a_n}{b_n}\le\limsup \frac{a_n}{b_n}\le \limsup\frac{a_{n+1}-a_n}{b_{n+1}-b_n} \limsup 0<b_{n+1}<b_n n \alpha>\limsup_{n\to\infty}\frac{a_{n+1}-a_n}{b_{n+1}-b_n} N k\ge0 \alpha>\frac{a_{N+k}-a_{N+k-1}}{b_{N+k}-b_{N+k-1}}. b_{n+1}<b_n k\ge0 \alpha(b_{N+k}-b_{n+K-1})<a_{N+k}-a_{N+k-1} m\ge0 \begin{align} \alpha\sum_{k=0}^m(b_{N+k}-b_{N+k-1}) &< \sum_{k=0}^m(a_{N+k}-a_{N+k-1}) \\ \alpha(b_{N+m}-b_{N-1})&<a_{N+m}-a_{n-1}\end{align} m\to\infty \begin{align} -\alpha b_{N-1}&<-a_{N-1}  \\ \alpha&>\frac{a_{N-1}}{b_{N-1}}.\end{align} N\to\infty \alpha\ge\limsup_{n\to\infty}\frac{a_n}{b_n} \limsup_{n\to\infty}\frac{a_{n+1}-a_n}{b_{n+1}-b_n}\ge\limsup_{n\to\infty}\frac{a_n}{b_n}.,"['real-analysis', 'calculus', 'sequences-and-series', 'proof-verification', 'limsup-and-liminf']"
73,There exists a unique extension $\hat{T}$ of a bounded linear operator $T$.,There exists a unique extension  of a bounded linear operator .,\hat{T} T,"I am trying to prove the following theorem : Theorem : Let $X,Z$ be Banach (normed) spaces and $Y$ be a dense subspace of $X$ . Let $T:Y \to Z$ be a bounded linear operator. Then, there exists a unique extension $\hat{T}$ of $T$ which is also a bounder linear operator with $\|\hat{T}\| = \|T\|$ . Attempted proof : If $x \in X$ then since $Y$ is dense over $X$ , there will exist a sequence $(x_n)$ of $Y$ with $x_n \to x$ . This means that for another sequence $(x_m)$ of $Y$ : $$\|x_n - x \| \to 0 \implies \|x_n - x_m \| < \varepsilon$$ But since $T$ is a bounded linear operator and $(x_n),(x_m) \in Y$ , it is : $$\|Tx_n-Tx_m\|\leq \|T\|\cdot\|x_n-x_m\| < \|T\|\cdot \varepsilon$$ which implies that the sequence $(Tx_n)$ is Cauchy. But the space $Z$ is Banach, so this means that $Tx_n \to z$ for some $z \in Z$ . Now, if we let $\hat{T}x \equiv z$ , we beed to prove that $\hat{T}|_Y = T$ while $\hat{T}$ is linear, bounded and also $\|\hat{T}\| = \|T\|$ . Question : I seem to be stuck and out of ideas on how to proceed to proving the final terms needed for my proof. Any hints or elaborations on how to prove the final statements -- $\hat{T}|_Y = T$ while $\hat{T}$ is linear, bounded and also $\|\hat{T}\| = \|T\|$ -- will be very appreciated !","I am trying to prove the following theorem : Theorem : Let be Banach (normed) spaces and be a dense subspace of . Let be a bounded linear operator. Then, there exists a unique extension of which is also a bounder linear operator with . Attempted proof : If then since is dense over , there will exist a sequence of with . This means that for another sequence of : But since is a bounded linear operator and , it is : which implies that the sequence is Cauchy. But the space is Banach, so this means that for some . Now, if we let , we beed to prove that while is linear, bounded and also . Question : I seem to be stuck and out of ideas on how to proceed to proving the final terms needed for my proof. Any hints or elaborations on how to prove the final statements -- while is linear, bounded and also -- will be very appreciated !","X,Z Y X T:Y \to Z \hat{T} T \|\hat{T}\| = \|T\| x \in X Y X (x_n) Y x_n \to x (x_m) Y \|x_n - x \| \to 0 \implies \|x_n - x_m \| < \varepsilon T (x_n),(x_m) \in Y \|Tx_n-Tx_m\|\leq \|T\|\cdot\|x_n-x_m\| < \|T\|\cdot \varepsilon (Tx_n) Z Tx_n \to z z \in Z \hat{T}x \equiv z \hat{T}|_Y = T \hat{T} \|\hat{T}\| = \|T\| \hat{T}|_Y = T \hat{T} \|\hat{T}\| = \|T\|","['real-analysis', 'functional-analysis', 'operator-theory', 'banach-spaces', 'normed-spaces']"
74,How to analyze $(-1)^{\left \lfloor n\theta \right \rfloor}$ (in which $\theta$ is an irrational number)?,How to analyze  (in which  is an irrational number)?,(-1)^{\left \lfloor n\theta \right \rfloor} \theta,"Let $\theta = \frac{\sqrt{5}-1}{2}$ . Define $a_{n}=(-1)^{\left \lfloor n\theta \right \rfloor}$ . Please judge whether $S_{n}=\sum_{k=1}^{n} a_{k}$ is unbounded. I tried to relate $\left\lfloor n\theta \right\rfloor$ to $\left\lfloor\theta^n\right\rfloor$ , because $\theta^n$ can be written in the form "" $x_{n}\theta + y_{n}$ "" in which $x_{n}$ and $y_{n}$ are related to Fibonacci sequence and $\left \lfloor \theta^n \right \rfloor$ is $0$ . But in this why I can only analyze some of the $a_{n}$ . Any ideas for help?","Let . Define . Please judge whether is unbounded. I tried to relate to , because can be written in the form "" "" in which and are related to Fibonacci sequence and is . But in this why I can only analyze some of the . Any ideas for help?",\theta = \frac{\sqrt{5}-1}{2} a_{n}=(-1)^{\left \lfloor n\theta \right \rfloor} S_{n}=\sum_{k=1}^{n} a_{k} \left\lfloor n\theta \right\rfloor \left\lfloor\theta^n\right\rfloor \theta^n x_{n}\theta + y_{n} x_{n} y_{n} \left \lfloor \theta^n \right \rfloor 0 a_{n},"['real-analysis', 'sequences-and-series', 'number-theory', 'irrational-numbers']"
75,Is the image of a $G_\delta$ set under a continuous mapping of $\mathbb R^n$ a Borel set?,Is the image of a  set under a continuous mapping of  a Borel set?,G_\delta \mathbb R^n,"If $A\subset \mathbb R^n$ is a $G_\delta$ set and $f\colon \mathbb R^n\to \mathbb R^n$ is continuous, does it follow that $f(A)$ is a Borel set? It is well-known that the image of a Borel set in $\mathbb R^n$ under a continuous map (even a projection) need not be Borel in general. However, for low levels of the Borel hierarchy the situation might be different: if $A\subset \mathbb R^n$ is $F_\sigma$ , then (by writing $A$ as a countable union of compact sets) one finds that $f(A)$ is also $F_\sigma$ , hence Borel. The image of a $G_\delta$ set need not be $G_\delta$ (e.g., take a piecewise linear function  such that $f(\mathbb{N})=\mathbb{Q}$ ) but I did not find any discussion of whether it must be Borel. The discussion in Continuous images of open sets are Borel? involves spaces that are not $\sigma$ -compact.","If is a set and is continuous, does it follow that is a Borel set? It is well-known that the image of a Borel set in under a continuous map (even a projection) need not be Borel in general. However, for low levels of the Borel hierarchy the situation might be different: if is , then (by writing as a countable union of compact sets) one finds that is also , hence Borel. The image of a set need not be (e.g., take a piecewise linear function  such that ) but I did not find any discussion of whether it must be Borel. The discussion in Continuous images of open sets are Borel? involves spaces that are not -compact.",A\subset \mathbb R^n G_\delta f\colon \mathbb R^n\to \mathbb R^n f(A) \mathbb R^n A\subset \mathbb R^n F_\sigma A f(A) F_\sigma G_\delta G_\delta f(\mathbb{N})=\mathbb{Q} \sigma,['real-analysis']
76,Can you prove the power rule for irrational exponents without invoking $e$?,Can you prove the power rule for irrational exponents without invoking ?,e,"The power rule states that for any real number $r$, $$\frac{d}{dx}x^r=rx^{r-1}$$ Now one common way to prove this is to use the definition $x^r=e^{r\ln x}$, where $e^x$ is defined as the inverse function of $\ln x$, which is in turn defined as $\int_1^x\frac{dt}{t}$. But this puts the cart before the horse, because students typically learn differential calculus before integral calculus.  And there is a perfectly good definition of exponentiation of real numbers that does not rely on integral calculus: $$x^r=\lim_{q\rightarrow r} x^q$$ where $q$ is a variable that ranges over the rational numbers. So my question is, if we use this definition, and we take it for granted that $\frac{d}{dx}x^q=qx^{q-1}$ holds true for rational numbers (which can be easily proven without invoking $e$), then can we prove the power rule for real exponents without invoking $e$? EDIT: Here’s a more precise formulation of the definition above.  If $r$ is a real number, we say that $x^r = L$ if for any $\epsilon>0$ there exists a $\delta>0$ such that for any rational number $q$ such that $|q-x| < \delta$, we have $|x^q-L|<\epsilon$.","The power rule states that for any real number $r$, $$\frac{d}{dx}x^r=rx^{r-1}$$ Now one common way to prove this is to use the definition $x^r=e^{r\ln x}$, where $e^x$ is defined as the inverse function of $\ln x$, which is in turn defined as $\int_1^x\frac{dt}{t}$. But this puts the cart before the horse, because students typically learn differential calculus before integral calculus.  And there is a perfectly good definition of exponentiation of real numbers that does not rely on integral calculus: $$x^r=\lim_{q\rightarrow r} x^q$$ where $q$ is a variable that ranges over the rational numbers. So my question is, if we use this definition, and we take it for granted that $\frac{d}{dx}x^q=qx^{q-1}$ holds true for rational numbers (which can be easily proven without invoking $e$), then can we prove the power rule for real exponents without invoking $e$? EDIT: Here’s a more precise formulation of the definition above.  If $r$ is a real number, we say that $x^r = L$ if for any $\epsilon>0$ there exists a $\delta>0$ such that for any rational number $q$ such that $|q-x| < \delta$, we have $|x^q-L|<\epsilon$.",,"['calculus', 'real-analysis', 'limits', 'derivatives', 'exponentiation']"
77,proof of the surjectivity of a function that satisfies certain properties [duplicate],proof of the surjectivity of a function that satisfies certain properties [duplicate],,"This question already has an answer here : $|f(x)-f(y)|\geq k|x-y|$.Then $f$ is bijective and its inverse is continuous. (1 answer) Closed 5 years ago . Let $f\colon \mathbb R^2 \to \mathbb R^2$ be a continuous function such that $|f(p)-f(q)|\geq a|p-q|,\quad \forall p,q\in\mathbb R^2$ and $a>0$. Show that $f$ is injective and surjective (therefore has inverse) and that its inverse is continuous. This is a problem from a metric space topology test that I did. The most important contents of the test were uniform convergence, equicontinuity, Arzelà-Ascoli Theorem, iterated functions, Stone-Weierstrass Theorem etc. The exercise is very simple and I believe it is possible to solve it by more elementary concepts. I already showed that $f$ is injective so my problem is surjectivity. For surjectivity i've tried something like this: since $f$ is injective it follows that $\exists g \colon \mathbb R^2 \to \mathbb R^2$ such that $\left(g\circ f\right)(x)=x$. On the other hand $f$ is a right inverse for $g$ which implies that $g$ is surjective. I was trying to show that $g$ is injective but i did not get anything. I have also shown that $f^{-1}$ is continuous (assuming $ f $ is surjective). Thank you very much!","This question already has an answer here : $|f(x)-f(y)|\geq k|x-y|$.Then $f$ is bijective and its inverse is continuous. (1 answer) Closed 5 years ago . Let $f\colon \mathbb R^2 \to \mathbb R^2$ be a continuous function such that $|f(p)-f(q)|\geq a|p-q|,\quad \forall p,q\in\mathbb R^2$ and $a>0$. Show that $f$ is injective and surjective (therefore has inverse) and that its inverse is continuous. This is a problem from a metric space topology test that I did. The most important contents of the test were uniform convergence, equicontinuity, Arzelà-Ascoli Theorem, iterated functions, Stone-Weierstrass Theorem etc. The exercise is very simple and I believe it is possible to solve it by more elementary concepts. I already showed that $f$ is injective so my problem is surjectivity. For surjectivity i've tried something like this: since $f$ is injective it follows that $\exists g \colon \mathbb R^2 \to \mathbb R^2$ such that $\left(g\circ f\right)(x)=x$. On the other hand $f$ is a right inverse for $g$ which implies that $g$ is surjective. I was trying to show that $g$ is injective but i did not get anything. I have also shown that $f^{-1}$ is continuous (assuming $ f $ is surjective). Thank you very much!",,['real-analysis']
78,Is the following Sum inequality true?,Is the following Sum inequality true?,,"Ι got a feeling that $$\sum_{m=1}^{N}\Big\lvert \sum_{k=0}^{\infty} \frac{m^{2k}}{(2k+1)!}(-1)^{k}\Big\rvert \geq C \sum_{m=1}^{N} \frac{1}{m} $$ Does it exist a $n_o$ such that for every $N\geq n_0$ the above is true? $$\sum_{m=1}^{N} \Big\lvert 1-\frac{m^2}{3!}+\frac{m^4}{5!}... \Big\rvert \geq C+\frac{C}{2}+\frac{C}{3}... + \frac{C}{N}$$ i feel that somehow terms will get canceled for big enough N, but i cant prove it!!  ( $ m \in N$) and $0< C<1$ constant. this came up as a part of problem i was solving . I got no idea if the above inequallity is true got no clue how to approach it!","Ι got a feeling that $$\sum_{m=1}^{N}\Big\lvert \sum_{k=0}^{\infty} \frac{m^{2k}}{(2k+1)!}(-1)^{k}\Big\rvert \geq C \sum_{m=1}^{N} \frac{1}{m} $$ Does it exist a $n_o$ such that for every $N\geq n_0$ the above is true? $$\sum_{m=1}^{N} \Big\lvert 1-\frac{m^2}{3!}+\frac{m^4}{5!}... \Big\rvert \geq C+\frac{C}{2}+\frac{C}{3}... + \frac{C}{N}$$ i feel that somehow terms will get canceled for big enough N, but i cant prove it!!  ( $ m \in N$) and $0< C<1$ constant. this came up as a part of problem i was solving . I got no idea if the above inequallity is true got no clue how to approach it!",,"['calculus', 'real-analysis', 'analysis', 'numerical-methods']"
79,"If $f$ is an entire function such that for each $\theta$, $|f(re^{i\theta})|\rightarrow \infty$ as $r\rightarrow \infty$","If  is an entire function such that for each ,  as",f \theta |f(re^{i\theta})|\rightarrow \infty r\rightarrow \infty,"Let $f: \mathbb{C}\mapsto \mathbb{C}$ be an entire function such that for each $\theta$, $|f(re^{i\theta})|\rightarrow \infty$ as $r\rightarrow \infty$. a) Does this imply that $|f(z)|\rightarrow \infty$ as $|z|\rightarrow\infty$? b) Does this imply that $f(z)$ is a polynomial? I know that if a) is true, then $b)$ is true since a) implies that $\infty$ is a pole for $f(z)$ which is equivalent to $f(z)$ is a polynomial. However, I don't know how to show that a) is true or construct a counter example. Edited 5/11: I am also considering a closely related question:  Let $f: \mathbb{R}^2\mapsto \mathbb{R}$ be a continuous function. Suppose for all $k\in \mathbb{R}$, we have $|f(x,kx)|\rightarrow \infty$ as $x\rightarrow \infty$. a) Does this imply that $|f(x,y)|\rightarrow \infty$ as $|(x,y)|\rightarrow\infty$ b) If we require $f$ to be differentiable, does this imply that $|f(x,y)|\rightarrow \infty$ as $|(x,y)|\rightarrow\infty$","Let $f: \mathbb{C}\mapsto \mathbb{C}$ be an entire function such that for each $\theta$, $|f(re^{i\theta})|\rightarrow \infty$ as $r\rightarrow \infty$. a) Does this imply that $|f(z)|\rightarrow \infty$ as $|z|\rightarrow\infty$? b) Does this imply that $f(z)$ is a polynomial? I know that if a) is true, then $b)$ is true since a) implies that $\infty$ is a pole for $f(z)$ which is equivalent to $f(z)$ is a polynomial. However, I don't know how to show that a) is true or construct a counter example. Edited 5/11: I am also considering a closely related question:  Let $f: \mathbb{R}^2\mapsto \mathbb{R}$ be a continuous function. Suppose for all $k\in \mathbb{R}$, we have $|f(x,kx)|\rightarrow \infty$ as $x\rightarrow \infty$. a) Does this imply that $|f(x,y)|\rightarrow \infty$ as $|(x,y)|\rightarrow\infty$ b) If we require $f$ to be differentiable, does this imply that $|f(x,y)|\rightarrow \infty$ as $|(x,y)|\rightarrow\infty$",,"['real-analysis', 'complex-analysis']"
80,Does the amount of positive integers $x \le n$ without middle divisors divided by $n$ converge to $\frac{3}{4}$?,Does the amount of positive integers  without middle divisors divided by  converge to ?,x \le n n \frac{3}{4},"I recently came across an interesting conjecture that stated that the amount of positive integers $x$ on the interval $[1,n]$ such that $x$ does not have a $2$-middle divisor divided by $n$ approaches $\frac{3}{4}$ as $n$ approaches infinity. First, for clarification, $d|x$ is a $\lambda$-middle divisor if and only if $$\sqrt{\frac{x}{\lambda}} < d \le \sqrt{\lambda x}$$ More information here . The conjecture can be expressed in mathematical notation by first defining a set $A_n$ where $n\in\mathbb{Z^+}$. $$A_n := \left\{x\in\mathbb{Z^+}, x \leq n : (\nexists d|x)\left[\sqrt{\frac{x}{2}} < d \leq \sqrt{2x}\right]\right\}$$ The conjecture may now be stated as $$\lim_{n\to\infty} \frac{|A_n|}{n} = \frac{3}{4}$$ At this, I noticed that the set contains all odd primes up to $n$, so I defined a new set, $$B_n := \{x\in A_n : x \notin \mathbb{P}\}$$ and added its cardinality to the prime counting function $\pi(n)$ minus $1$ for the exclusion of the single even prime. $$\lim_{n\to\infty} \frac{|B_n| + \pi(n) - 1}{n} = \frac{3}{4}$$ I then separated everything except $|B_n|$ from the numerator and attempted to solve it independently. $$\lim_{n\to\infty} \frac{\pi(n) - 1}{n} = \lim_{n\to\infty} \frac{{\rm {li}}(n) - 1 + O\left(ne^{-\sqrt{\ln n}/15}\right)}{n} = \lim_{n\to\infty} \left(\frac{{\rm {li}}(n) - 1}{n} + O\left(e^{-\sqrt{\ln n}/15}\right)\right)$$ Since $e^{-\sqrt{\ln n}/15}$ converges to $0$, we may remove the big O notation portion of the limit. $$\lim_{n\to\infty} \frac{{\rm {li}}(n) - 1}{n} \overset{LH}{=} \lim_{n\to\infty} \frac{1}{\ln(n)} = 0 \implies \lim_{n\to\infty} \frac{|B_n|}{n} = \frac{3}{4}$$ I wasn't expecting this result. If the conjecture is true, then the non-prime elements of $A_n$ are infinite as $n$ approaches infinity. This is where I am not sure how to continue. Is this conjecture true? Is there a way to prove or disprove that the amount of elements in $B_n$ is infinite as $n$ approaches infinity without proving the conjecture? Edit: I have now found that the product of any two primes $p_1$ and $p_2$ such that $p_2 > 2p_1$ is also in this set. If these two primes do not satisfy the inequality, then the product is guaranteed not to be in the set. This means we can define a new set, $$C_n := \{x \in B_n : (\nexists p_1, p_2 \in \mathbb{P})[x = p_1 p_2]\}$$ This allows us to simplify the limit to $$\lim_{n\to\infty} \left[\frac{|C_n|}{n} - \sum_{p \text{ prime}}^{\sqrt{n/2}} \frac{\pi\left(\frac{n}{p}\right) - \pi\left(2p\right)}{n} \right] = \frac{3}{4}$$ Let $p_k$ represent the $k$-th prime. $$\lim_{n\to\infty} \left[\frac{|C_n|}{n} - \sum_{k=1}^{\pi\left(\sqrt{n/2}\right)} \frac{\pi\left(\frac{n}{p_k}\right) - \pi\left(2p_k\right)}{n} \right] = \frac{3}{4}$$ I am unsure how to simplify this further or if it is of use to do so.","I recently came across an interesting conjecture that stated that the amount of positive integers $x$ on the interval $[1,n]$ such that $x$ does not have a $2$-middle divisor divided by $n$ approaches $\frac{3}{4}$ as $n$ approaches infinity. First, for clarification, $d|x$ is a $\lambda$-middle divisor if and only if $$\sqrt{\frac{x}{\lambda}} < d \le \sqrt{\lambda x}$$ More information here . The conjecture can be expressed in mathematical notation by first defining a set $A_n$ where $n\in\mathbb{Z^+}$. $$A_n := \left\{x\in\mathbb{Z^+}, x \leq n : (\nexists d|x)\left[\sqrt{\frac{x}{2}} < d \leq \sqrt{2x}\right]\right\}$$ The conjecture may now be stated as $$\lim_{n\to\infty} \frac{|A_n|}{n} = \frac{3}{4}$$ At this, I noticed that the set contains all odd primes up to $n$, so I defined a new set, $$B_n := \{x\in A_n : x \notin \mathbb{P}\}$$ and added its cardinality to the prime counting function $\pi(n)$ minus $1$ for the exclusion of the single even prime. $$\lim_{n\to\infty} \frac{|B_n| + \pi(n) - 1}{n} = \frac{3}{4}$$ I then separated everything except $|B_n|$ from the numerator and attempted to solve it independently. $$\lim_{n\to\infty} \frac{\pi(n) - 1}{n} = \lim_{n\to\infty} \frac{{\rm {li}}(n) - 1 + O\left(ne^{-\sqrt{\ln n}/15}\right)}{n} = \lim_{n\to\infty} \left(\frac{{\rm {li}}(n) - 1}{n} + O\left(e^{-\sqrt{\ln n}/15}\right)\right)$$ Since $e^{-\sqrt{\ln n}/15}$ converges to $0$, we may remove the big O notation portion of the limit. $$\lim_{n\to\infty} \frac{{\rm {li}}(n) - 1}{n} \overset{LH}{=} \lim_{n\to\infty} \frac{1}{\ln(n)} = 0 \implies \lim_{n\to\infty} \frac{|B_n|}{n} = \frac{3}{4}$$ I wasn't expecting this result. If the conjecture is true, then the non-prime elements of $A_n$ are infinite as $n$ approaches infinity. This is where I am not sure how to continue. Is this conjecture true? Is there a way to prove or disprove that the amount of elements in $B_n$ is infinite as $n$ approaches infinity without proving the conjecture? Edit: I have now found that the product of any two primes $p_1$ and $p_2$ such that $p_2 > 2p_1$ is also in this set. If these two primes do not satisfy the inequality, then the product is guaranteed not to be in the set. This means we can define a new set, $$C_n := \{x \in B_n : (\nexists p_1, p_2 \in \mathbb{P})[x = p_1 p_2]\}$$ This allows us to simplify the limit to $$\lim_{n\to\infty} \left[\frac{|C_n|}{n} - \sum_{p \text{ prime}}^{\sqrt{n/2}} \frac{\pi\left(\frac{n}{p}\right) - \pi\left(2p\right)}{n} \right] = \frac{3}{4}$$ Let $p_k$ represent the $k$-th prime. $$\lim_{n\to\infty} \left[\frac{|C_n|}{n} - \sum_{k=1}^{\pi\left(\sqrt{n/2}\right)} \frac{\pi\left(\frac{n}{p_k}\right) - \pi\left(2p_k\right)}{n} \right] = \frac{3}{4}$$ I am unsure how to simplify this further or if it is of use to do so.",,"['calculus', 'real-analysis', 'limits', 'prime-numbers', 'divisibility']"
81,"If $x\mapsto \lim_{h\to 0} \frac{f(x+h) - 2f(x) +f(x-h)}{h^2}$ is continuous, then $f\in C^2$","If  is continuous, then",x\mapsto \lim_{h\to 0} \frac{f(x+h) - 2f(x) +f(x-h)}{h^2} f\in C^2,"Let $a<b$ and $f:(a,b)\to \mathbb R$ be continuous and such that for all $x\in (a,b)$,  $\lim_{h\to 0} \frac{f(x+h) - 2f(x) +f(x-h)}{h^2}$ exists. Let $\beta:x\mapsto \lim_{h\to 0} \frac{f(x+h) - 2f(x) +f(x-h)}{h^2}$. If $\beta$ is continuous over $(a,b)$, prove that $f\in C^2((a,b))$ (i.e. $f$ is twice continuously differentiable) I'm aware that if $f$ is twice differentiable, then $\beta = f''$. I'm also aware that the mere existence of $\beta$ does not guarantee that $f$ is twice differentiable. The strong assumption in this problem is that $\beta$ be continuous. The continuity assumption on $f$ should also come into play somewhere... I've seen this problem asked at an oral exam and I find it quite interesting. I've been thinking about it for a few days but I have made zero progress towards a solution ...","Let $a<b$ and $f:(a,b)\to \mathbb R$ be continuous and such that for all $x\in (a,b)$,  $\lim_{h\to 0} \frac{f(x+h) - 2f(x) +f(x-h)}{h^2}$ exists. Let $\beta:x\mapsto \lim_{h\to 0} \frac{f(x+h) - 2f(x) +f(x-h)}{h^2}$. If $\beta$ is continuous over $(a,b)$, prove that $f\in C^2((a,b))$ (i.e. $f$ is twice continuously differentiable) I'm aware that if $f$ is twice differentiable, then $\beta = f''$. I'm also aware that the mere existence of $\beta$ does not guarantee that $f$ is twice differentiable. The strong assumption in this problem is that $\beta$ be continuous. The continuity assumption on $f$ should also come into play somewhere... I've seen this problem asked at an oral exam and I find it quite interesting. I've been thinking about it for a few days but I have made zero progress towards a solution ...",,"['real-analysis', 'derivatives']"
82,Prove that $f \in L^2(\mathbb{R})$ and $||f||_2 \leq 1$.,Prove that  and .,f \in L^2(\mathbb{R}) ||f||_2 \leq 1,"The entire question reads: Let $f$ be a Lebesgue measurable function on $\mathbb{R}$ with the property that $\sup_{\{g \in L^2(\mathbb{R}):||g||_2 \leq 1\}}\int_{\mathbb{R}}|fg|d \lambda \leq 1$. Prove that $f \in L^2(\mathbb{R})$ and $||f||_2 \leq 1$. I'm not quite sure where to begin on this problem. I'm trying to get more practice with $L^p$ spaces, but it's clear I'm still lacking in skill and know-how. Any tips or tricks are greatly appreciated.","The entire question reads: Let $f$ be a Lebesgue measurable function on $\mathbb{R}$ with the property that $\sup_{\{g \in L^2(\mathbb{R}):||g||_2 \leq 1\}}\int_{\mathbb{R}}|fg|d \lambda \leq 1$. Prove that $f \in L^2(\mathbb{R})$ and $||f||_2 \leq 1$. I'm not quite sure where to begin on this problem. I'm trying to get more practice with $L^p$ spaces, but it's clear I'm still lacking in skill and know-how. Any tips or tricks are greatly appreciated.",,"['real-analysis', 'lp-spaces']"
83,Swapping an improper integral and series,Swapping an improper integral and series,,"What are the hypothesis that allow me to write the identity $$\int_{x_0}^{+\infty}\left(\sum_{n=1}^{\infty}f_n(x)\right)\mathbf{d}x=\sum_{n=1}^{+\infty}\left(\int_{x_0}^{+\infty}f_n(x)\mathbf{d}x\right)$$ , where $x_0\in\mathbb{R}$ and $\forall n\in\mathbb{N}_{>0}\ \ f_n:[x_0,+\infty)\to\mathbb{R}$ is a continuous function? I know that exist many theorems about a similar argument, but in the hypothesis the set of integration is a bounded closed interval. Maybe I can use the definition of improper integral? I mean. Fix $M>x_0$, and suppose that $\sum_{n=1}^{+\infty}f_n(x)$ is uniformly convergent over $[x_0,M]$ for all $M>x_0$, then: $$\lim_{M\to+\infty}\int_{x_0}^{M}\left(\sum_{n=1}^{\infty}f_n(x)\right)\mathbf{d}x=\lim_{M\to+\infty}\sum_{n=1}^{+\infty}\int_{x_0}^{M}f_n(x)\mathbf{d}x$$ but it's not the same thing...","What are the hypothesis that allow me to write the identity $$\int_{x_0}^{+\infty}\left(\sum_{n=1}^{\infty}f_n(x)\right)\mathbf{d}x=\sum_{n=1}^{+\infty}\left(\int_{x_0}^{+\infty}f_n(x)\mathbf{d}x\right)$$ , where $x_0\in\mathbb{R}$ and $\forall n\in\mathbb{N}_{>0}\ \ f_n:[x_0,+\infty)\to\mathbb{R}$ is a continuous function? I know that exist many theorems about a similar argument, but in the hypothesis the set of integration is a bounded closed interval. Maybe I can use the definition of improper integral? I mean. Fix $M>x_0$, and suppose that $\sum_{n=1}^{+\infty}f_n(x)$ is uniformly convergent over $[x_0,M]$ for all $M>x_0$, then: $$\lim_{M\to+\infty}\int_{x_0}^{M}\left(\sum_{n=1}^{\infty}f_n(x)\right)\mathbf{d}x=\lim_{M\to+\infty}\sum_{n=1}^{+\infty}\int_{x_0}^{M}f_n(x)\mathbf{d}x$$ but it's not the same thing...",,"['real-analysis', 'sequences-and-series', 'improper-integrals']"
84,Try to generalize First Mean Value Theorem For Integrals,Try to generalize First Mean Value Theorem For Integrals,,"1. Suppose  that $f(x)$ is (Riemann) integrable on $[a,b]$ and $F^{'}(x)=f(x)$ for all $x\in[a,b]$,then there is a number $\color{red}{\xi\in(a,b)}$ such that  $$\int_{a}^{b}f(x)dx=f(\xi)(b-a)$$ 2. I generalize the above statement which is ture to the following version: Suppose  that $f(x)$ and $g(x)$ are (Riemann)integrable on $[a,b]$ with $g(x)\geq 0$ for all $x\in[a,b]$ and $F$is  a primitive function of $f$ on $[a,b]$ ,then there is a number $\color{red}{\xi\in(a,b)}$ such that  $$\int_{a}^{b}f(x)g(x)dx=f(\xi)\int_{a}^{b}g(x)dx$$ 3. I need to proof the generalisation is ture,or give some counterexamples to disprove it . From the First mean value theorem for definite integrals ,we have   $$\int_{a}^{b}f(x)g(x)dx=\mu\int_{a}^{b}g(x)dx,\quad \inf_{x\in[a,b]}\{f(x)\}\leq \mu\leq \sup_{x\in[a,b]}\{f(x)\}.$$ If $$\inf_{x\in[a,b]}\{f(x)\}< \mu <\sup_{x\in[a,b]}\{f(x)\},$$ Darboux's theorem tells us there is a $\xi\in(a,b)$ such that $f(\xi)=\mu.$ If $$\mu=\inf_{x\in[a,b]}\{f(x)\}\quad  \text{or}\quad  \mu=\sup_{x\in[a,b]}\{f(x)\},$$ is there also  a  $\xi\in(a,b)$ such that $f(\xi)=\mu ?$","1. Suppose  that $f(x)$ is (Riemann) integrable on $[a,b]$ and $F^{'}(x)=f(x)$ for all $x\in[a,b]$,then there is a number $\color{red}{\xi\in(a,b)}$ such that  $$\int_{a}^{b}f(x)dx=f(\xi)(b-a)$$ 2. I generalize the above statement which is ture to the following version: Suppose  that $f(x)$ and $g(x)$ are (Riemann)integrable on $[a,b]$ with $g(x)\geq 0$ for all $x\in[a,b]$ and $F$is  a primitive function of $f$ on $[a,b]$ ,then there is a number $\color{red}{\xi\in(a,b)}$ such that  $$\int_{a}^{b}f(x)g(x)dx=f(\xi)\int_{a}^{b}g(x)dx$$ 3. I need to proof the generalisation is ture,or give some counterexamples to disprove it . From the First mean value theorem for definite integrals ,we have   $$\int_{a}^{b}f(x)g(x)dx=\mu\int_{a}^{b}g(x)dx,\quad \inf_{x\in[a,b]}\{f(x)\}\leq \mu\leq \sup_{x\in[a,b]}\{f(x)\}.$$ If $$\inf_{x\in[a,b]}\{f(x)\}< \mu <\sup_{x\in[a,b]}\{f(x)\},$$ Darboux's theorem tells us there is a $\xi\in(a,b)$ such that $f(\xi)=\mu.$ If $$\mu=\inf_{x\in[a,b]}\{f(x)\}\quad  \text{or}\quad  \mu=\sup_{x\in[a,b]}\{f(x)\},$$ is there also  a  $\xi\in(a,b)$ such that $f(\xi)=\mu ?$",,"['real-analysis', 'integration', 'definite-integrals', 'riemann-integration']"
85,"Is it true that $ \limsup_{T\to0+}F(0,t)=\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]$?",Is it true that ?," \limsup_{T\to0+}F(0,t)=\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]","Consider a continuous function $F:\Bbb R_{\ge0}\times\Bbb R_{\ge0}\setminus\Delta_{\Bbb R_{\ge0}^2}\to\Bbb R_{\ge0}$ where $\Delta_{\Bbb R_{\ge0}^2}:=\{(x,x):x\ge0\}$ is the diagonal of $\Bbb R_{\ge0}^2$. Suppose moreover $F$ symmetric, that is $F(t,s)=F(s,t)\;\;\forall (s,t)\in\Bbb R_{\ge0}\times\Bbb R_{\ge0}\setminus\Delta_{\Bbb R_{\ge0}^2}$ and $F(0,t)>0$ for all $t>0$. Is it true that $$ \lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right] =\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]\;\;? $$ It seems clear that RHS is $\ge$ than LHS; is the converse true? I have a proof, but in the comments there is a counterexample: where is the problem? PROOF: We distinguish two cases for the value of RHS. Let us suppose first $\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=c\in\Bbb R_{\ge0}$. If $c=0$ then the conclusion is trivial. Suppose thus $c>0$. Now for every $n\in\Bbb N$ there exists $\delta_n>0$ such that $$ \left|\sup_{0\le s<t\le\delta_n}F(s,t)-c\right|<\frac1n $$ and for every $n\in\Bbb N$, fixed $\delta_n$, there exists $\{(s_m^{(\delta_n)},t_m^{(\delta_n)})\}_{m\ge1}\subset\Bbb R_{\ge0}^2\setminus\Delta_{\Bbb R_{\ge0}^2}$ such that $$ F(s_m^{(\delta_n)},t_m^{(\delta_n)})\stackrel{m\to+\infty}{\longrightarrow} \sup_{0\le s<t\le\delta_n}F(s,t). $$ from which we get $$ \lim_{n\to+\infty}F(s_n^{(\delta_n)},t_n^{(\delta_n)}) =\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=c\; $$ (moreover it is clear that $s_n^{(\delta_n)},t_n^{(\delta_n)},\delta_n\to0$). Then, by continuity, $\forall\varepsilon>0\;\;\exists N_{\varepsilon}\ge1$ such that $$ \left|F(0,t_n^{(\delta_n)})-F(s_n^{(\delta_n)},t_n^{(\delta_n)})\right|<\frac{\varepsilon}2\;\;\;\;\;\forall n\ge N_{\varepsilon} $$ and $$ \left|F(s_n^{(\delta_n)},t_n^{(\delta_n)})-c\right|<\frac{\varepsilon}2\;\;\;\;\;\forall n\ge N_{\varepsilon}, $$ from which we get $$ \left|F(0,t_n^{(\delta_n)})-c\right|<\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon}, $$ and thus $$ \lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right] =\lim_{n\to+\infty}\left[\sup_{0<t\le t_n^{(\delta_n)}}F(0,t)\right] \ge \lim_{n\to+\infty}F(0,t_n^{(\delta_n)})=c $$ which allows to conclude, since clearly $\lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right]\le c$. The case $\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=+\infty$ is only slightly different: since $$ T\mapsto\sup_{0\le s<t\le T}F(s,t) $$ is not decresing, it is clear that $$ \sup_{0\le s<t\le T}F(s,t)=+\infty\;\;\;\forall T>0. $$ In particular fixing $n\ge1$ we have that $\sup_{0\le s<t\le \frac1n}F(s,t)=+\infty$, thus there exists $\{(s_m^{(n)},t_m^{(n)})\}_{m\ge1}\subset\Bbb R_{\ge0}^2\setminus\Delta_{\Bbb R_{\ge0}^2}$ such that $$ \lim_{m\to+\infty}F (s_m^{(n)},t_m^{(n)})=+\infty, $$ and since this holds true for every $n\ge1$ we can write \begin{equation} \lim_{n\to+\infty}F (s_n^{(n)},t_n^{(n)})=+\infty. \end{equation} Now, being clearly $s_n^{(n)},t_n^{(n)}\to0$ as $n\to+\infty$ and since $F$ is continuous, $\forall\varepsilon>0$ $\exists N_{\varepsilon}\ge1$ such that $$ \left|F(0,t_n^{(n)})-F(s_n^{(n)},t_n^{(n)})\right|<\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon}; $$ in particular we get $$ F(0,t_n^{(n)})>F(s_n^{(n)},t_n^{(n)})-\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon}, $$ from which  $$ \lim_{n\to+\infty}F (0,t_n^{(n)})=+\infty. $$ Then we can conclude as in the previous case: $$ \lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right] =\lim_{n\to+\infty}\left[\sup_{0<t\le t_n^{(n)}}F(0,t)\right] \ge \lim_{n\to+\infty}F(0,t_n^{(n)})=+\infty. $$ FINALLY: I wanted to treat my problem in general, but originally it was $$ F(s,t)=\frac{|f(t)-f(s)|}{|t-s|^{\lambda}} $$ for some $0<\lambda\le1$ and for a function $f$ which is $\alpha$-Holder continuous for some $\alpha$. THE MOST IMPORTANT THING: If my proof is wrong, what kind of hypothesis we can put in order to obtain the result? THE GENESIS OF THIS POST: $f$ originally was the difference of two solutions of the following scalar SDE: $$ x_t=\underbrace{\xi_0+\int_0^tb(s,x_s)\,ds+\int_0^t\sigma(s,x_s)\,dW_s^H}_{=:z_t}+y_t\; $$ where $$ y_t=\sup_{s\in[0,t]}\left(z_s\right)^{-}. $$ and $$ b:\Bbb R_{\ge0}\times\Bbb R\to\Bbb R $$ and $$ \sigma:\Bbb R_{\ge0}\times\Bbb R\to\Bbb R $$ are bounded measurable functions, called drift and diffusion coefficient respectively. We will assume the following hypotesis on them: $$ |b(t,x)-b(t,y)|\le K_0|x-y|\;\;\forall x,y\in\Bbb R,\;\forall t\in[0,T] $$ $$ |\sigma(t,x)-\sigma(t,y)|\le K_0|x-y|\;\;\forall x,y\in\Bbb R,\;\forall t\in[0,T] $$ $$ |\sigma(t,x)-\sigma(s,x)|\le K_0|t-s|^{\nu}\;\;\forall x\in\Bbb R,\;\forall s,t\in[0,T] $$ and $W^H$ is the fractional Brownian motion of Hurst index $1/2<H<1$; moreover the integral is the YOUNG INTEGRAL , which is defined as the limit of the usual Riemann Sums, or it can be equivalently viewed as follows (here we deal with deterministic case: we think to $W^H$ as a fixed path of the fBM): if $h$ is a $\lambda$-Holder real function, for $\lambda>1-H$, then it can be proved that $$ \int_s^th_u\,dW_u^H=h_s(W_t^H-W_s^H)+\Lambda_{st}((h_c-h_a)(W_b^H-W_c^H)) $$ where $a,c,b$ are mute variables and $\Lambda:\mathcal{ZC}_3^{\mu}\to\mathcal{C}_2^{\mu}$ is the inverse of $\delta:\mathcal{C}_2^{\mu}\to\mathcal{ZC}_3^{\mu}$ which is defined as $$ (\delta h)_{sut}:=-h_{ut}+h_{st}-h_{su} $$ and $$ \mathscr C_2^{\mu}:=\{h\in\mathscr{C}_{2}\;:\;\|h\|_{\mu}<+\infty\} $$ where $\mathscr{C}_{2}$ is the $\Bbb R$-vector space of all functions $h:[0,T]^2\to\Bbb R$ continuous and such that they vanish on diagonal. Moreover $$ \|h\|_{\mu}:=\sup_{\substack{s,t\in[0,T]\\s\neq t}}\frac{|h_{ts}|}{|t-s|^{\mu}} $$ and $$ \mathscr C_3^{\mu}:=\{h\in\mathscr{C}_{3}\;:\;\|h\|_{\mu}<+\infty\} $$ where $\mathscr{C}_{3}$ is the $\Bbb R$-vector space of all functions $h:[0,T]^3\to\Bbb R$ continuous such that $h_{sut}=0$ whenever $s=u$ or $u=t$ and $\|\cdot\|_{\mu}$ is a suitable norm. Thus \begin{align*} f_t &=x_t^{(1)}-x_t^{(2)}\\ &=\int_0^t(b(u,x_u^{(1)})-b(u,x_u^{(2)}))\,du+ \int_0^t(\sigma(u,x_u^{(1)})-\sigma(u,x_u^{(2)}))\,dW_u^H +y_t^{(1)}-y_t^{(2)} \end{align*} and $$ F(0,t)=\frac{|f_t|}{t^H} $$ IMPORTANT since the original exponent is $\rho>1$, then it would be enough to prove that this last limit is nonzero, in fact $$ \frac{|f_t|}{t^{\rho}}=\frac{|f_t|}{t^{H}}\frac1{t^{\rho-H}} $$","Consider a continuous function $F:\Bbb R_{\ge0}\times\Bbb R_{\ge0}\setminus\Delta_{\Bbb R_{\ge0}^2}\to\Bbb R_{\ge0}$ where $\Delta_{\Bbb R_{\ge0}^2}:=\{(x,x):x\ge0\}$ is the diagonal of $\Bbb R_{\ge0}^2$. Suppose moreover $F$ symmetric, that is $F(t,s)=F(s,t)\;\;\forall (s,t)\in\Bbb R_{\ge0}\times\Bbb R_{\ge0}\setminus\Delta_{\Bbb R_{\ge0}^2}$ and $F(0,t)>0$ for all $t>0$. Is it true that $$ \lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right] =\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]\;\;? $$ It seems clear that RHS is $\ge$ than LHS; is the converse true? I have a proof, but in the comments there is a counterexample: where is the problem? PROOF: We distinguish two cases for the value of RHS. Let us suppose first $\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=c\in\Bbb R_{\ge0}$. If $c=0$ then the conclusion is trivial. Suppose thus $c>0$. Now for every $n\in\Bbb N$ there exists $\delta_n>0$ such that $$ \left|\sup_{0\le s<t\le\delta_n}F(s,t)-c\right|<\frac1n $$ and for every $n\in\Bbb N$, fixed $\delta_n$, there exists $\{(s_m^{(\delta_n)},t_m^{(\delta_n)})\}_{m\ge1}\subset\Bbb R_{\ge0}^2\setminus\Delta_{\Bbb R_{\ge0}^2}$ such that $$ F(s_m^{(\delta_n)},t_m^{(\delta_n)})\stackrel{m\to+\infty}{\longrightarrow} \sup_{0\le s<t\le\delta_n}F(s,t). $$ from which we get $$ \lim_{n\to+\infty}F(s_n^{(\delta_n)},t_n^{(\delta_n)}) =\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=c\; $$ (moreover it is clear that $s_n^{(\delta_n)},t_n^{(\delta_n)},\delta_n\to0$). Then, by continuity, $\forall\varepsilon>0\;\;\exists N_{\varepsilon}\ge1$ such that $$ \left|F(0,t_n^{(\delta_n)})-F(s_n^{(\delta_n)},t_n^{(\delta_n)})\right|<\frac{\varepsilon}2\;\;\;\;\;\forall n\ge N_{\varepsilon} $$ and $$ \left|F(s_n^{(\delta_n)},t_n^{(\delta_n)})-c\right|<\frac{\varepsilon}2\;\;\;\;\;\forall n\ge N_{\varepsilon}, $$ from which we get $$ \left|F(0,t_n^{(\delta_n)})-c\right|<\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon}, $$ and thus $$ \lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right] =\lim_{n\to+\infty}\left[\sup_{0<t\le t_n^{(\delta_n)}}F(0,t)\right] \ge \lim_{n\to+\infty}F(0,t_n^{(\delta_n)})=c $$ which allows to conclude, since clearly $\lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right]\le c$. The case $\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=+\infty$ is only slightly different: since $$ T\mapsto\sup_{0\le s<t\le T}F(s,t) $$ is not decresing, it is clear that $$ \sup_{0\le s<t\le T}F(s,t)=+\infty\;\;\;\forall T>0. $$ In particular fixing $n\ge1$ we have that $\sup_{0\le s<t\le \frac1n}F(s,t)=+\infty$, thus there exists $\{(s_m^{(n)},t_m^{(n)})\}_{m\ge1}\subset\Bbb R_{\ge0}^2\setminus\Delta_{\Bbb R_{\ge0}^2}$ such that $$ \lim_{m\to+\infty}F (s_m^{(n)},t_m^{(n)})=+\infty, $$ and since this holds true for every $n\ge1$ we can write \begin{equation} \lim_{n\to+\infty}F (s_n^{(n)},t_n^{(n)})=+\infty. \end{equation} Now, being clearly $s_n^{(n)},t_n^{(n)}\to0$ as $n\to+\infty$ and since $F$ is continuous, $\forall\varepsilon>0$ $\exists N_{\varepsilon}\ge1$ such that $$ \left|F(0,t_n^{(n)})-F(s_n^{(n)},t_n^{(n)})\right|<\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon}; $$ in particular we get $$ F(0,t_n^{(n)})>F(s_n^{(n)},t_n^{(n)})-\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon}, $$ from which  $$ \lim_{n\to+\infty}F (0,t_n^{(n)})=+\infty. $$ Then we can conclude as in the previous case: $$ \lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right] =\lim_{n\to+\infty}\left[\sup_{0<t\le t_n^{(n)}}F(0,t)\right] \ge \lim_{n\to+\infty}F(0,t_n^{(n)})=+\infty. $$ FINALLY: I wanted to treat my problem in general, but originally it was $$ F(s,t)=\frac{|f(t)-f(s)|}{|t-s|^{\lambda}} $$ for some $0<\lambda\le1$ and for a function $f$ which is $\alpha$-Holder continuous for some $\alpha$. THE MOST IMPORTANT THING: If my proof is wrong, what kind of hypothesis we can put in order to obtain the result? THE GENESIS OF THIS POST: $f$ originally was the difference of two solutions of the following scalar SDE: $$ x_t=\underbrace{\xi_0+\int_0^tb(s,x_s)\,ds+\int_0^t\sigma(s,x_s)\,dW_s^H}_{=:z_t}+y_t\; $$ where $$ y_t=\sup_{s\in[0,t]}\left(z_s\right)^{-}. $$ and $$ b:\Bbb R_{\ge0}\times\Bbb R\to\Bbb R $$ and $$ \sigma:\Bbb R_{\ge0}\times\Bbb R\to\Bbb R $$ are bounded measurable functions, called drift and diffusion coefficient respectively. We will assume the following hypotesis on them: $$ |b(t,x)-b(t,y)|\le K_0|x-y|\;\;\forall x,y\in\Bbb R,\;\forall t\in[0,T] $$ $$ |\sigma(t,x)-\sigma(t,y)|\le K_0|x-y|\;\;\forall x,y\in\Bbb R,\;\forall t\in[0,T] $$ $$ |\sigma(t,x)-\sigma(s,x)|\le K_0|t-s|^{\nu}\;\;\forall x\in\Bbb R,\;\forall s,t\in[0,T] $$ and $W^H$ is the fractional Brownian motion of Hurst index $1/2<H<1$; moreover the integral is the YOUNG INTEGRAL , which is defined as the limit of the usual Riemann Sums, or it can be equivalently viewed as follows (here we deal with deterministic case: we think to $W^H$ as a fixed path of the fBM): if $h$ is a $\lambda$-Holder real function, for $\lambda>1-H$, then it can be proved that $$ \int_s^th_u\,dW_u^H=h_s(W_t^H-W_s^H)+\Lambda_{st}((h_c-h_a)(W_b^H-W_c^H)) $$ where $a,c,b$ are mute variables and $\Lambda:\mathcal{ZC}_3^{\mu}\to\mathcal{C}_2^{\mu}$ is the inverse of $\delta:\mathcal{C}_2^{\mu}\to\mathcal{ZC}_3^{\mu}$ which is defined as $$ (\delta h)_{sut}:=-h_{ut}+h_{st}-h_{su} $$ and $$ \mathscr C_2^{\mu}:=\{h\in\mathscr{C}_{2}\;:\;\|h\|_{\mu}<+\infty\} $$ where $\mathscr{C}_{2}$ is the $\Bbb R$-vector space of all functions $h:[0,T]^2\to\Bbb R$ continuous and such that they vanish on diagonal. Moreover $$ \|h\|_{\mu}:=\sup_{\substack{s,t\in[0,T]\\s\neq t}}\frac{|h_{ts}|}{|t-s|^{\mu}} $$ and $$ \mathscr C_3^{\mu}:=\{h\in\mathscr{C}_{3}\;:\;\|h\|_{\mu}<+\infty\} $$ where $\mathscr{C}_{3}$ is the $\Bbb R$-vector space of all functions $h:[0,T]^3\to\Bbb R$ continuous such that $h_{sut}=0$ whenever $s=u$ or $u=t$ and $\|\cdot\|_{\mu}$ is a suitable norm. Thus \begin{align*} f_t &=x_t^{(1)}-x_t^{(2)}\\ &=\int_0^t(b(u,x_u^{(1)})-b(u,x_u^{(2)}))\,du+ \int_0^t(\sigma(u,x_u^{(1)})-\sigma(u,x_u^{(2)}))\,dW_u^H +y_t^{(1)}-y_t^{(2)} \end{align*} and $$ F(0,t)=\frac{|f_t|}{t^H} $$ IMPORTANT since the original exponent is $\rho>1$, then it would be enough to prove that this last limit is nonzero, in fact $$ \frac{|f_t|}{t^{\rho}}=\frac{|f_t|}{t^{H}}\frac1{t^{\rho-H}} $$",,"['real-analysis', 'continuity', 'limsup-and-liminf']"
86,Partitioning the irrational numbers in a special way,Partitioning the irrational numbers in a special way,,"Can the set of  irrational numbers be partitioned into two nonempty subsets , each of which is closed under addition ? I can show that if $A \subseteq \mathbb R$ has the property that $A$ is closed under addition and there is at least two elements in $A$ that can be extended to a Hamel basis of $\mathbb R$ over $\mathbb Q$ then $A$ can be partitioned into two nonempty subsets , each of which is closed under addition . Unfortunately for me , the set of  irrational numbers is not closed under addition . Please help . Thanks in advance","Can the set of  irrational numbers be partitioned into two nonempty subsets , each of which is closed under addition ? I can show that if $A \subseteq \mathbb R$ has the property that $A$ is closed under addition and there is at least two elements in $A$ that can be extended to a Hamel basis of $\mathbb R$ over $\mathbb Q$ then $A$ can be partitioned into two nonempty subsets , each of which is closed under addition . Unfortunately for me , the set of  irrational numbers is not closed under addition . Please help . Thanks in advance",,"['real-analysis', 'set-theory', 'irrational-numbers']"
87,Showing that a metric space is discrete if and only if any function from it to another metric space is continuous,Showing that a metric space is discrete if and only if any function from it to another metric space is continuous,,"Let $(X, d)$ be a metric space and $(Y, p)$ is another metric space that has at least two distinct elements. Show that $(X, d)$ is a discrete metric space (a metric space is defined to be discrete if every subset is open) if and only if any function from $X$ to $Y$ is continuous. I'm not too sure how to prove this, I'm guessing we need to use the open set characterization of continuity, i.e., $f:X \rightarrow Y$ is continuous if and only if for every open set $A \subset Y$, the set $f^{-1}(A) = \{x \in X : f(x) \in A\} \subseteq X$ is open. Can anyone provide a proof?","Let $(X, d)$ be a metric space and $(Y, p)$ is another metric space that has at least two distinct elements. Show that $(X, d)$ is a discrete metric space (a metric space is defined to be discrete if every subset is open) if and only if any function from $X$ to $Y$ is continuous. I'm not too sure how to prove this, I'm guessing we need to use the open set characterization of continuity, i.e., $f:X \rightarrow Y$ is continuous if and only if for every open set $A \subset Y$, the set $f^{-1}(A) = \{x \in X : f(x) \in A\} \subseteq X$ is open. Can anyone provide a proof?",,"['real-analysis', 'metric-spaces']"
88,A clarification regarding the Borel $\sigma$-algebra on $\mathbb{R}^{2}$,A clarification regarding the Borel -algebra on,\sigma \mathbb{R}^{2},"Let $B_{2}$ be the Borel-$\sigma$ algebra on $\mathbb{R}^{2}$, that is, the smallest $\sigma-algebra$ that contains all open subsets of $\mathbb{R}^{2}$. Let $B_{1}$ be the usual Borel-$\sigma$ algebra on $\mathbb{R}$. (1) Is $B_{2}$ equal to $B_{1}\times B_{1}$? Prove or Disprove. (2) What about the completion of $B_{2}$ and $B_{1}\times B_{1}$? Are they same? Prove or Disprove. I have been trying this for quiet sometime, but have been unable to do it. I have read this product measure from the book ""Real analysis for Graduate Students-R. Bass"" and this is a problem from the same book. What I know is given two measure space $(X,A,\mu)$ and $(Y,B,\nu)$, the product sigma algebra denoted by $A\times B$ on the space $X\times Y$, is defined to the smallest sigma algebra containing the measurable rectangles, where the measurable rectangles are subsets of $X\times Y$ of the form $a\times b$(where $a\in A, b\in B)$. Now I know that the problem with this product measure is that even if $\mu$ and $\nu$ are complete, the product measure may not be complete. The book has given a very common example taking $\mathbb{R}^{2}$ and the lebesgue measure $m$ on $\mathbb{R}$. So $m\times m$ is the product measure and he shows that this measure is not complete by displaying a null set that is not in the product sigma algebra. And he remarks that the two dimensional lebesgue measure can easily be constructed by taking the completion of this product measure $m\times m$. Also I know that Lebesgue sigma algebra in $\mathbb{R}$ is basically the completion of the Borel $\sigma$-algebra. For the first part I have no idea. But for the second part I think that the completion of $m\times m$ is same as $B_{1}\times B_{1}$ and $B_{2}$. But then I have to prove it which I have no idea about. I hope to get a elaborate explanation so that I can understand this concept clearly. Thanks in advance.","Let $B_{2}$ be the Borel-$\sigma$ algebra on $\mathbb{R}^{2}$, that is, the smallest $\sigma-algebra$ that contains all open subsets of $\mathbb{R}^{2}$. Let $B_{1}$ be the usual Borel-$\sigma$ algebra on $\mathbb{R}$. (1) Is $B_{2}$ equal to $B_{1}\times B_{1}$? Prove or Disprove. (2) What about the completion of $B_{2}$ and $B_{1}\times B_{1}$? Are they same? Prove or Disprove. I have been trying this for quiet sometime, but have been unable to do it. I have read this product measure from the book ""Real analysis for Graduate Students-R. Bass"" and this is a problem from the same book. What I know is given two measure space $(X,A,\mu)$ and $(Y,B,\nu)$, the product sigma algebra denoted by $A\times B$ on the space $X\times Y$, is defined to the smallest sigma algebra containing the measurable rectangles, where the measurable rectangles are subsets of $X\times Y$ of the form $a\times b$(where $a\in A, b\in B)$. Now I know that the problem with this product measure is that even if $\mu$ and $\nu$ are complete, the product measure may not be complete. The book has given a very common example taking $\mathbb{R}^{2}$ and the lebesgue measure $m$ on $\mathbb{R}$. So $m\times m$ is the product measure and he shows that this measure is not complete by displaying a null set that is not in the product sigma algebra. And he remarks that the two dimensional lebesgue measure can easily be constructed by taking the completion of this product measure $m\times m$. Also I know that Lebesgue sigma algebra in $\mathbb{R}$ is basically the completion of the Borel $\sigma$-algebra. For the first part I have no idea. But for the second part I think that the completion of $m\times m$ is same as $B_{1}\times B_{1}$ and $B_{2}$. But then I have to prove it which I have no idea about. I hope to get a elaborate explanation so that I can understand this concept clearly. Thanks in advance.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
89,"Convergence/divergence of $\int_{0}^{\infty}{x^{3}\,\mathrm{d}x \over 1 + x^{a} \sin^2{x}}$",Convergence/divergence of,"\int_{0}^{\infty}{x^{3}\,\mathrm{d}x \over 1 + x^{a} \sin^2{x}}","I am trying to conclude about the convergence/divergence of $$ \int_{0}^{\infty}{x^3 \over 1+x^a \sin^{2}\left(\,x\,\right)}\,\mathrm{d}x \qquad\mbox{for}\quad a \in \mathbb{R}. $$ First, we notice $${x^3 \over 1+x^a} \leq{x^3 \over 1+x^a \sin^2{x}}$$ $${x^3 \over 1+x^a} \approx {x^3 \over x^a}={1 \over x^{a-3}}$$ So for $a \leq 4$ the integral diverges by the comparison test. How do I approach this for $a > 4$ ?. We do seem to run into trouble for $x = k\pi$.","I am trying to conclude about the convergence/divergence of $$ \int_{0}^{\infty}{x^3 \over 1+x^a \sin^{2}\left(\,x\,\right)}\,\mathrm{d}x \qquad\mbox{for}\quad a \in \mathbb{R}. $$ First, we notice $${x^3 \over 1+x^a} \leq{x^3 \over 1+x^a \sin^2{x}}$$ $${x^3 \over 1+x^a} \approx {x^3 \over x^a}={1 \over x^{a-3}}$$ So for $a \leq 4$ the integral diverges by the comparison test. How do I approach this for $a > 4$ ?. We do seem to run into trouble for $x = k\pi$.",,"['real-analysis', 'integration', 'improper-integrals']"
90,Relationship between the Wronskian and the Gramian,Relationship between the Wronskian and the Gramian,,"Is it possible to draw some parallels between the Wronskian and the Gram matrix? Could they be used for solving the same problem? What is the principal difference between them? The Gram matrix of a set of vectors $v_{1},\cdots ,v_{n}$ in an inner product space is the Hermitian matrix of inner products, whose entries are given by $G_{ij}=\langle v_{i},v_{j}\rangle$ . A set of vectors is linearly independent if and only if the Gram determinant (the determinant of the Gram matrix) is non-zero. For $n-1$ times differentiable functions $f_1, \cdots , f_n$ the Wronskian $W(f_1, \cdots , f_n)$ as a function is defined by $$ {W(f_{1},\ldots ,f_{n})(x)={\begin{vmatrix}f_{1}(x)&f_{2}(x)&\cdots &f_{n}(x)\\f_{1}'(x)&f_{2}'(x)&\cdots &f_{n}'(x)\\\vdots &\vdots &\ddots &\vdots \\f_{1}^{(n-1)}(x)&f_{2}^{(n-1)}(x)&\cdots &f_{n}^{(n-1)}(x)\end{vmatrix}}}  $$ If the functions $f_i$ are linearly dependent, then so are the columns of the Wronskian as differentiation is a linear operation, so the Wronskian vanishes.","Is it possible to draw some parallels between the Wronskian and the Gram matrix? Could they be used for solving the same problem? What is the principal difference between them? The Gram matrix of a set of vectors in an inner product space is the Hermitian matrix of inner products, whose entries are given by . A set of vectors is linearly independent if and only if the Gram determinant (the determinant of the Gram matrix) is non-zero. For times differentiable functions the Wronskian as a function is defined by If the functions are linearly dependent, then so are the columns of the Wronskian as differentiation is a linear operation, so the Wronskian vanishes.","v_{1},\cdots ,v_{n} G_{ij}=\langle v_{i},v_{j}\rangle n-1 f_1, \cdots , f_n W(f_1, \cdots , f_n)  {W(f_{1},\ldots ,f_{n})(x)={\begin{vmatrix}f_{1}(x)&f_{2}(x)&\cdots
&f_{n}(x)\\f_{1}'(x)&f_{2}'(x)&\cdots &f_{n}'(x)\\\vdots &\vdots
&\ddots &\vdots \\f_{1}^{(n-1)}(x)&f_{2}^{(n-1)}(x)&\cdots
&f_{n}^{(n-1)}(x)\end{vmatrix}}} 
 f_i","['real-analysis', 'harmonic-analysis', 'wronskian']"
91,Proving Plancherel's identity.,Proving Plancherel's identity.,,"I want to prove that $$\Arrowvert \hat{f} \Arrowvert_{l^2(\mathbb{Z})} = \Arrowvert f \Arrowvert _{L^2([-L,L])},$$ Could anyone give me a hint please?","I want to prove that $$\Arrowvert \hat{f} \Arrowvert_{l^2(\mathbb{Z})} = \Arrowvert f \Arrowvert _{L^2([-L,L])},$$ Could anyone give me a hint please?",,"['real-analysis', 'measure-theory', 'fourier-analysis', 'fourier-series', 'fourier-transform']"
92,Is there a metric space on $\mathbb{R}$ such that open sets of Euclidean metric space are not open?,Is there a metric space on  such that open sets of Euclidean metric space are not open?,\mathbb{R},"If $X$ is metric space together with a metric function $d$, we call $U \subseteq X$ open if for all $u \in U$ there is some $\epsilon > 0$ with $B_{\epsilon}(u) \subseteq U$ where $B_{\epsilon}(u) = \{v \in X|d(u,v) < \epsilon \}$. So, my question is that can we have a metric on $\mathbb{R}$ such that open intervals (of Euclidean metric) are not open in this particular metric we provide? Or, can we prove that open intervals are always open in any other metric? In latter case, of course open intervals can be closed and open.","If $X$ is metric space together with a metric function $d$, we call $U \subseteq X$ open if for all $u \in U$ there is some $\epsilon > 0$ with $B_{\epsilon}(u) \subseteq U$ where $B_{\epsilon}(u) = \{v \in X|d(u,v) < \epsilon \}$. So, my question is that can we have a metric on $\mathbb{R}$ such that open intervals (of Euclidean metric) are not open in this particular metric we provide? Or, can we prove that open intervals are always open in any other metric? In latter case, of course open intervals can be closed and open.",,"['real-analysis', 'metric-spaces']"
93,A Question on Borel Measurable Functions,A Question on Borel Measurable Functions,,"Define $A, B:[a,b] \to \mathbb R$ by $$ A(x) = \lim_{\epsilon \to 0} \sup_{y\in [a,b]\\|x-y|<\epsilon} f(y), \quad B(x) = \lim_{\epsilon \to 0} \inf_{y\in [a,b]\\|x-y|<\epsilon} f(y), $$ where $f:[a,b]\to\mathbb R$ is bounded function on $[a,b]$. I want to show that (i) $A, B$ are borel measurable functions. (ii) If $f$ is continuous at $x_0$, then $A(x_0) = f(x_0) = B(x_0)$. (iii) If $A(x_0) = B(x_0)$ then $f$ is continuous at $x_0$. Actually I have no idea for (i) because I could not express $A,B$ as some limit of measurable functions. For (ii) I obtained that for any $x \in [a,b]$, $B(x) \le f(x) \le A(x)$. Please help me.","Define $A, B:[a,b] \to \mathbb R$ by $$ A(x) = \lim_{\epsilon \to 0} \sup_{y\in [a,b]\\|x-y|<\epsilon} f(y), \quad B(x) = \lim_{\epsilon \to 0} \inf_{y\in [a,b]\\|x-y|<\epsilon} f(y), $$ where $f:[a,b]\to\mathbb R$ is bounded function on $[a,b]$. I want to show that (i) $A, B$ are borel measurable functions. (ii) If $f$ is continuous at $x_0$, then $A(x_0) = f(x_0) = B(x_0)$. (iii) If $A(x_0) = B(x_0)$ then $f$ is continuous at $x_0$. Actually I have no idea for (i) because I could not express $A,B$ as some limit of measurable functions. For (ii) I obtained that for any $x \in [a,b]$, $B(x) \le f(x) \le A(x)$. Please help me.",,"['real-analysis', 'analysis', 'measure-theory', 'borel-sets']"
94,Derivative of $ f(x) = \sum\limits_{n=1}^{\infty} \frac{1}{n^2}e^{-n^2x^2}$ at 0,Derivative of  at 0, f(x) = \sum\limits_{n=1}^{\infty} \frac{1}{n^2}e^{-n^2x^2},"Let $ f(x) = \sum_{n=1}^{\infty} \frac{1}{n^2}e^{-n^2x^2}$ I know that the serie of the derivatives $g(x) = \sum_{n=1}^{\infty} -2xe^{-n^2x^2}$ is uniformely convergent on every set $]-\infty;-b] \cup [b;+\infty[ , b>0 $. This implies that $f$ is differentiable on $\mathbb{R} \setminus \{0\}$. My question is, is it differentiable on $0$ ? I tried to apply L'hopistal rule but I cannot figure out if $g(x)$ has a limit when $x \to 0^+$ or $x \to 0^{-}$. I also tried to compute the derivative at $0$ but I don't see how to carry the computations. Many thx for any help","Let $ f(x) = \sum_{n=1}^{\infty} \frac{1}{n^2}e^{-n^2x^2}$ I know that the serie of the derivatives $g(x) = \sum_{n=1}^{\infty} -2xe^{-n^2x^2}$ is uniformely convergent on every set $]-\infty;-b] \cup [b;+\infty[ , b>0 $. This implies that $f$ is differentiable on $\mathbb{R} \setminus \{0\}$. My question is, is it differentiable on $0$ ? I tried to apply L'hopistal rule but I cannot figure out if $g(x)$ has a limit when $x \to 0^+$ or $x \to 0^{-}$. I also tried to compute the derivative at $0$ but I don't see how to carry the computations. Many thx for any help",,"['real-analysis', 'sequences-and-series', 'derivatives', 'convergence-divergence']"
95,Probability that Random Harmonic Type Series Converges,Probability that Random Harmonic Type Series Converges,,"Recently I have come across a variety of truly wonderful results that deal with series that look like harmonic series: All of the series $$\sum_{n = 1}^{\infty} \frac{1}{n^{1+1/n}}, \ \sum_{n = 1}^{\infty} \frac{|\sin(n)|}{n}, \ \sum_{n = 1}^{\infty} \frac{1}{n^{2 - \epsilon + \sin(n)}}, \ \sum_{n = 1}^{\infty} \frac{1}{n^{1 + |\sin(n)|}}   $$ diverge. The proof for the latter three mostly hinges on the fact that the fractional parts of $\{\sin(n)\}_{n \in \mathbb{N}}$ are equidistributed in the unit interval. This leads me to ask the following question: Let $u_i \sim Unif([0,1])$. What is the probability that $$\sum_{n = 1}^{\infty} \frac{1}{n^{1+ u_n}}$$ diverges ? Similarly, we can ask an analogue question where $u_i$ are drawn from an arbitrary distribution $X$. I do not much knowledge in this area so any helpful comments and directions are welcomed.","Recently I have come across a variety of truly wonderful results that deal with series that look like harmonic series: All of the series $$\sum_{n = 1}^{\infty} \frac{1}{n^{1+1/n}}, \ \sum_{n = 1}^{\infty} \frac{|\sin(n)|}{n}, \ \sum_{n = 1}^{\infty} \frac{1}{n^{2 - \epsilon + \sin(n)}}, \ \sum_{n = 1}^{\infty} \frac{1}{n^{1 + |\sin(n)|}}   $$ diverge. The proof for the latter three mostly hinges on the fact that the fractional parts of $\{\sin(n)\}_{n \in \mathbb{N}}$ are equidistributed in the unit interval. This leads me to ask the following question: Let $u_i \sim Unif([0,1])$. What is the probability that $$\sum_{n = 1}^{\infty} \frac{1}{n^{1+ u_n}}$$ diverges ? Similarly, we can ask an analogue question where $u_i$ are drawn from an arbitrary distribution $X$. I do not much knowledge in this area so any helpful comments and directions are welcomed.",,"['real-analysis', 'sequences-and-series', 'probability-theory', 'stochastic-processes']"
96,Generalization of intermediate value theorem for derivative,Generalization of intermediate value theorem for derivative,,"Let $f: \Bbb R^n \to \Bbb R $ be a differentiable function (not necessarily  smooth), and $C$ is a connected set. Is $\nabla f(C)$  connected subset of $\Bbb R^n?$","Let $f: \Bbb R^n \to \Bbb R $ be a differentiable function (not necessarily  smooth), and $C$ is a connected set. Is $\nabla f(C)$  connected subset of $\Bbb R^n?$",,"['real-analysis', 'general-topology', 'multivariable-calculus']"
97,Prove that there cannot be a continuous onto function $f :\Bbb R \to \Bbb R \setminus \Bbb Q$,Prove that there cannot be a continuous onto function,f :\Bbb R \to \Bbb R \setminus \Bbb Q,"Prove that there cannot be a continuous onto function $f :\Bbb  R \to \Bbb R \setminus \Bbb Q$ Please let me know if you agree with my proof: Suppose, toward a contradiction, that there exists such a continuous onto function $f :\Bbb  R \to \Bbb R \setminus \Bbb Q$ . Since $\Bbb R$ is connected, $f(\Bbb R) \subset \Bbb R\setminus \Bbb Q$ must also be connected. Pick any $q \in \Bbb Q$ and let $U=(\infty, q)$ and $V=(q, \infty)$. Note that $U$ and $V$ are non-empty, disjoint and open. Then $U\cup V$ is a disconnection of $\Bbb R\setminus \Bbb Q$, a contradiction. Furthermore, ${\Bbb R}=f^{-1}(U \cup V)$ is also a contradiction since $\Bbb R$ is connected.","Prove that there cannot be a continuous onto function $f :\Bbb  R \to \Bbb R \setminus \Bbb Q$ Please let me know if you agree with my proof: Suppose, toward a contradiction, that there exists such a continuous onto function $f :\Bbb  R \to \Bbb R \setminus \Bbb Q$ . Since $\Bbb R$ is connected, $f(\Bbb R) \subset \Bbb R\setminus \Bbb Q$ must also be connected. Pick any $q \in \Bbb Q$ and let $U=(\infty, q)$ and $V=(q, \infty)$. Note that $U$ and $V$ are non-empty, disjoint and open. Then $U\cup V$ is a disconnection of $\Bbb R\setminus \Bbb Q$, a contradiction. Furthermore, ${\Bbb R}=f^{-1}(U \cup V)$ is also a contradiction since $\Bbb R$ is connected.",,"['real-analysis', 'proof-verification', 'continuity', 'functional-equations']"
98,Proof Verification About Sequence Limits,Proof Verification About Sequence Limits,,"I had work to hand in which included the following question Let $(a_n), (b_n)$ satisfy $\lim_{n\rightarrow\infty} a_nb_n=1$. Proof that if for all $n$, $0\leq a_n,b_n \leq 1$ then    $\lim_{n\rightarrow\infty} a_n=\lim_{n\rightarrow\infty}b_n=1$. My solution which I handed in was pretty straightfoward- showing easily that $a_nb_n\leq a_n\leq 1$ and the same for $b_n$. Then I used the squeeze theorem to show their limits are $1$. However, people (also the teachers who assisted some students) started talking about partial limits and Bolzano–Weierstrass which really makes my dbout my solution's validness, which I seek here from you at the moment. Thanks in advance. EDIT See my comment below","I had work to hand in which included the following question Let $(a_n), (b_n)$ satisfy $\lim_{n\rightarrow\infty} a_nb_n=1$. Proof that if for all $n$, $0\leq a_n,b_n \leq 1$ then    $\lim_{n\rightarrow\infty} a_n=\lim_{n\rightarrow\infty}b_n=1$. My solution which I handed in was pretty straightfoward- showing easily that $a_nb_n\leq a_n\leq 1$ and the same for $b_n$. Then I used the squeeze theorem to show their limits are $1$. However, people (also the teachers who assisted some students) started talking about partial limits and Bolzano–Weierstrass which really makes my dbout my solution's validness, which I seek here from you at the moment. Thanks in advance. EDIT See my comment below",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
99,Prove that $\cos(x+y)=\cos(x)\cos(y)-\sin(x)\sin(y)$ using Cauchy product.,Prove that  using Cauchy product.,\cos(x+y)=\cos(x)\cos(y)-\sin(x)\sin(y),Show the correctness of the equality by making use of the Cauchy product of the series $\cos(x)=\sum_{n=0}^{\infty}(-1)^n.\frac{x^{2n}}{(2n)!}$ and $\sin(x)=\sum_{n=0}^{\infty}(-1)^n.\frac{x^{2n+1}}{(2n+1)!}$. I've reached $$\cos(x).\cos(y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{n}\frac{1}{(2k)!(2(n-k))!}.x^{2k}.y^{2(n-k)}$$ and $$\sin(x).\sin(y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{n}\frac{1}{(2k+1)!(2(n-k)+1)!}.x^{2k+1}.y^{2(n-k)+1}$$ and $$\cos(x+y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{2n}\frac{1}{k!(2n-k)!}.x^{k}.y^{2n-k}$$ but I guess I'm missing some steps or got something wrong.,Show the correctness of the equality by making use of the Cauchy product of the series $\cos(x)=\sum_{n=0}^{\infty}(-1)^n.\frac{x^{2n}}{(2n)!}$ and $\sin(x)=\sum_{n=0}^{\infty}(-1)^n.\frac{x^{2n+1}}{(2n+1)!}$. I've reached $$\cos(x).\cos(y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{n}\frac{1}{(2k)!(2(n-k))!}.x^{2k}.y^{2(n-k)}$$ and $$\sin(x).\sin(y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{n}\frac{1}{(2k+1)!(2(n-k)+1)!}.x^{2k+1}.y^{2(n-k)+1}$$ and $$\cos(x+y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{2n}\frac{1}{k!(2n-k)!}.x^{k}.y^{2n-k}$$ but I guess I'm missing some steps or got something wrong.,,"['real-analysis', 'analysis', 'trigonometry']"
