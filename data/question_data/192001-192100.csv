,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Did Anton, Bivens, and Davis make a mistake here (in Calculus)?","Did Anton, Bivens, and Davis make a mistake here (in Calculus)?",,"I am having this book for Calculus. I am reviewing stuffs on abs. Max and min. Here is what I think is a mistake by the three authors: a question asks to find the relative extrema, if at all, for $f(x)=\frac{1}{x^2-x}$ on $(0,1)$. The first derivative came out to be $f'(x)=-\frac{2x-1}{(x^2-x)^2}$. Here my book says ""although $f$ is not differentiable at $x=0,1$, these values are doubly disqualified since they are neither in the domain of $f$ nor in the interval $(0,1)$."" But aren't we supposed to include points of non-differentiability when finding the values of abs. Extrema?","I am having this book for Calculus. I am reviewing stuffs on abs. Max and min. Here is what I think is a mistake by the three authors: a question asks to find the relative extrema, if at all, for $f(x)=\frac{1}{x^2-x}$ on $(0,1)$. The first derivative came out to be $f'(x)=-\frac{2x-1}{(x^2-x)^2}$. Here my book says ""although $f$ is not differentiable at $x=0,1$, these values are doubly disqualified since they are neither in the domain of $f$ nor in the interval $(0,1)$."" But aren't we supposed to include points of non-differentiability when finding the values of abs. Extrema?",,"['calculus', 'derivatives']"
1,Is $(\nabla \times \nabla)$ an operator?,Is  an operator?,(\nabla \times \nabla),Is $(\nabla \times \nabla)$ an operator? I am wondering if its possible to compute \begin{align} \vec{f} \cdot (\nabla \times \nabla) \end{align} Where $f$ is a vector. I would be interested in both cartesian and polar expressions.,Is $(\nabla \times \nabla)$ an operator? I am wondering if its possible to compute \begin{align} \vec{f} \cdot (\nabla \times \nabla) \end{align} Where $f$ is a vector. I would be interested in both cartesian and polar expressions.,,"['derivatives', 'vectors', 'partial-derivative', 'vector-analysis']"
2,How can I differentiate the following using Matrix Calculus?,How can I differentiate the following using Matrix Calculus?,,"The mathematical expression is like this: $f(\mu_{q(\beta)}, \Sigma_{q(\beta)}) = \mathbf{1}_{n}^{T} \exp \left \{ X \mu_{q(\beta)} + \frac{1}{2} \operatorname{diagonal}(X \Sigma_{q(\beta)} X^{T}) \right\}$ where $\exp$ indicates element-wise exponentiation and $\operatorname{diagonal}$ is a vector with the diagonal entries of the matrix as its components. Additionally, $\mathbf{1}_{n}^{T}$ is a vector in $\mathbb{R}^{n}$ with only ones as its entries and for notational ease, I used lower-case for vectors whereas matrices are in upper-case. $\Sigma_{q(\beta)}$ is symmetric whereas $X$ is not. And I want to differentiate this by $\mu_{q(\beta)}$ and $\Sigma_{q(\beta)}$ each. So $\frac{\partial f}{\partial \mu_{q(\beta)}}$ and $\frac{\partial f}{\partial \Sigma_{q(\beta)}}$ are what I need.","The mathematical expression is like this: $f(\mu_{q(\beta)}, \Sigma_{q(\beta)}) = \mathbf{1}_{n}^{T} \exp \left \{ X \mu_{q(\beta)} + \frac{1}{2} \operatorname{diagonal}(X \Sigma_{q(\beta)} X^{T}) \right\}$ where $\exp$ indicates element-wise exponentiation and $\operatorname{diagonal}$ is a vector with the diagonal entries of the matrix as its components. Additionally, $\mathbf{1}_{n}^{T}$ is a vector in $\mathbb{R}^{n}$ with only ones as its entries and for notational ease, I used lower-case for vectors whereas matrices are in upper-case. $\Sigma_{q(\beta)}$ is symmetric whereas $X$ is not. And I want to differentiate this by $\mu_{q(\beta)}$ and $\Sigma_{q(\beta)}$ each. So $\frac{\partial f}{\partial \mu_{q(\beta)}}$ and $\frac{\partial f}{\partial \Sigma_{q(\beta)}}$ are what I need.",,"['calculus', 'linear-algebra', 'derivatives', 'matrix-calculus']"
3,Finding extrema of $\frac{\sin (x) \sin (y)}{x y}$,Finding extrema of,\frac{\sin (x) \sin (y)}{x y},"I need to find the extrema of the following function in the range $-2\pi$ and $2\pi$ for both $x$ and $y$, but I don't know how to go about doing it since it's a bit weird and not similar to other functions I've seen: $$f(x,y)=\frac{\sin (x) \sin (y)}{xy}$$ I've evaluated the gradient function as below: $\nabla f = <\frac{\sin (y) (x \cos (x)-\sin (x))}{x^2 y}, \frac{\sin (x) (y \cos (y)-\sin (y))}{y^2 x}>$ but setting it to zero gives a few answers for $x$ and $y$, none of which seem to be the right answer. The following is the sketch of the graph in the aforementioned range. According to WolframAlpha, it should have it's local extrema at $\{0, 4.49\}$, $\{0, -4.49\}$, $\{4.49, 0\}$, $\{-4.49, 0\}$.","I need to find the extrema of the following function in the range $-2\pi$ and $2\pi$ for both $x$ and $y$, but I don't know how to go about doing it since it's a bit weird and not similar to other functions I've seen: $$f(x,y)=\frac{\sin (x) \sin (y)}{xy}$$ I've evaluated the gradient function as below: $\nabla f = <\frac{\sin (y) (x \cos (x)-\sin (x))}{x^2 y}, \frac{\sin (x) (y \cos (y)-\sin (y))}{y^2 x}>$ but setting it to zero gives a few answers for $x$ and $y$, none of which seem to be the right answer. The following is the sketch of the graph in the aforementioned range. According to WolframAlpha, it should have it's local extrema at $\{0, 4.49\}$, $\{0, -4.49\}$, $\{4.49, 0\}$, $\{-4.49, 0\}$.",,"['calculus', 'derivatives', 'optimization', 'graphing-functions']"
4,Nonhomgeneous Linear Differential Equation: Harmonic Oscillator,Nonhomgeneous Linear Differential Equation: Harmonic Oscillator,,"Consider frictionless harmonic oscillator (w/ m = 1) driven by an external force $f(t) = A\sin{\omega t} $, so that $$\frac{d^2 x}{dt^2} + \omega_0^2x = A\sin{\omega t}. $$ Show that the particular solution for $\omega \neq \omega_0 $ is $$x_p(t) = \frac{A}{\omega_0^2 - \omega^2} \sin{\omega t} $$ Method of undetermined coefficients looks like the best choice. I Acknowledged this is a second order linear nonhomegeneous equation.","Consider frictionless harmonic oscillator (w/ m = 1) driven by an external force $f(t) = A\sin{\omega t} $, so that $$\frac{d^2 x}{dt^2} + \omega_0^2x = A\sin{\omega t}. $$ Show that the particular solution for $\omega \neq \omega_0 $ is $$x_p(t) = \frac{A}{\omega_0^2 - \omega^2} \sin{\omega t} $$ Method of undetermined coefficients looks like the best choice. I Acknowledged this is a second order linear nonhomegeneous equation.",,"['calculus', 'integration', 'derivatives', 'partial-derivative', 'mathematical-physics']"
5,Frechet Derivatives of a nonlinear integral operator,Frechet Derivatives of a nonlinear integral operator,,"The nonlinear integral operator $P:C[0,1]\to C[0,1]$ is defined as follow: $$P(f)(x)=1+kxf(x)\int_0^1\frac{f(s)}{x+s}ds$$ In order to obtain the Frechet derivative of the operator, I start with: $$P(f+h)(x)-P(f)(x)=kxf(x)\int_0^1\frac{h(s)}{x+s}ds+kxh(x)\int_0^1\frac{f(s)}{x+s}ds+kxh(x)\int_0^1\frac{h(s)}{s+x}ds$$ It seems the last term is of order $\lVert h(x)\lVert _\infty ^2$ but how to prove it? Note that the function inside the integral is no longer continuous.","The nonlinear integral operator $P:C[0,1]\to C[0,1]$ is defined as follow: $$P(f)(x)=1+kxf(x)\int_0^1\frac{f(s)}{x+s}ds$$ In order to obtain the Frechet derivative of the operator, I start with: $$P(f+h)(x)-P(f)(x)=kxf(x)\int_0^1\frac{h(s)}{x+s}ds+kxh(x)\int_0^1\frac{f(s)}{x+s}ds+kxh(x)\int_0^1\frac{h(s)}{s+x}ds$$ It seems the last term is of order $\lVert h(x)\lVert _\infty ^2$ but how to prove it? Note that the function inside the integral is no longer continuous.",,"['derivatives', 'operator-theory', 'normed-spaces', 'integral-operators']"
6,Finding values of a piecewise function such that it is differentiable at $x=1$,Finding values of a piecewise function such that it is differentiable at,x=1,"Let $$ f(x)= \begin{cases} a-x & x \leq 1, \\ \frac{1}{bx} & x>1. \end{cases} $$ Considering this piecewise defined function find values of $a,b$ such that the function is differentiable at $x=1$. Give the value of $f'(1)$. I don't know how to go about this question without having any actual numerical values. Any help would be greatly appreciated.","Let $$ f(x)= \begin{cases} a-x & x \leq 1, \\ \frac{1}{bx} & x>1. \end{cases} $$ Considering this piecewise defined function find values of $a,b$ such that the function is differentiable at $x=1$. Give the value of $f'(1)$. I don't know how to go about this question without having any actual numerical values. Any help would be greatly appreciated.",,"['derivatives', 'continuity']"
7,"Prove , there exists $\theta , \phi \in (\frac{\pi}{6},\frac{\pi}{3})$ such that $f'(\theta) = 0$ and $f'(\phi)\neq 0$","Prove , there exists  such that  and","\theta , \phi \in (\frac{\pi}{6},\frac{\pi}{3}) f'(\theta) = 0 f'(\phi)\neq 0","Let the function $$f(\theta) = \begin{vmatrix} \sin\theta & \cos\theta & \tan\theta  \\ \sin(\frac{\pi}{6}) & \cos(\frac{\pi}{6}) & \tan(\frac{\pi}{6}) & \\ \sin(\frac{\pi}{3}) & \cos(\frac{\pi}{3}) & \tan(\frac{\pi}{3})   \end{vmatrix} $$ where  $\theta \in \left[ \frac{\pi}{6},\frac{\pi}{3} \right]$ and $f'(\theta)$     denote the derivative of $f$ with respect to $\theta$. Which of the following statements is/are TRUE? (I) There exists $\theta \in (\frac{\pi}{6},\frac{\pi}{3})$ such that $f'(\theta) = 0$ (II) There exists $\theta \in (\frac{\pi}{6},\frac{\pi}{3})$ such that $f'(\theta)\neq  0$ I only II only Both I and II Neither I Nor II I try to explain $:$ $$f(\theta) = \begin{vmatrix} \sin\theta & \cos\theta & \tan\theta  \\ \frac{1}{2} & \frac{\sqrt{3}}{2} & \frac{1}{\sqrt{3}} & \\ \frac{\sqrt{3}}{2} & \frac{1}{2} & \sqrt{3}   \end{vmatrix} $$ $f(\theta) =$$\left(\frac{3}{2}-\frac{1}{2\sqrt{3}}\right)sin\left(\theta\right)-\left(\frac{1}{2\sqrt{3}}-\frac{1}{2}\right)cos\left(\theta\right)+\left(\frac{1}{4}-\frac{3}{4}\right)tan\left(\theta\right)$ Since , $f'(\theta) =$$\frac{1}{6}\left(-3\sec ^2\left(\theta\right)-\left(\sqrt{3}-9\right)\cos \left(\theta\right)+\left(\sqrt{3}-3\right)\sin \left(\theta\right)\right)$ I know option (3) is true , since it is possible for both statements . Can you prove in formal way ?","Let the function $$f(\theta) = \begin{vmatrix} \sin\theta & \cos\theta & \tan\theta  \\ \sin(\frac{\pi}{6}) & \cos(\frac{\pi}{6}) & \tan(\frac{\pi}{6}) & \\ \sin(\frac{\pi}{3}) & \cos(\frac{\pi}{3}) & \tan(\frac{\pi}{3})   \end{vmatrix} $$ where  $\theta \in \left[ \frac{\pi}{6},\frac{\pi}{3} \right]$ and $f'(\theta)$     denote the derivative of $f$ with respect to $\theta$. Which of the following statements is/are TRUE? (I) There exists $\theta \in (\frac{\pi}{6},\frac{\pi}{3})$ such that $f'(\theta) = 0$ (II) There exists $\theta \in (\frac{\pi}{6},\frac{\pi}{3})$ such that $f'(\theta)\neq  0$ I only II only Both I and II Neither I Nor II I try to explain $:$ $$f(\theta) = \begin{vmatrix} \sin\theta & \cos\theta & \tan\theta  \\ \frac{1}{2} & \frac{\sqrt{3}}{2} & \frac{1}{\sqrt{3}} & \\ \frac{\sqrt{3}}{2} & \frac{1}{2} & \sqrt{3}   \end{vmatrix} $$ $f(\theta) =$$\left(\frac{3}{2}-\frac{1}{2\sqrt{3}}\right)sin\left(\theta\right)-\left(\frac{1}{2\sqrt{3}}-\frac{1}{2}\right)cos\left(\theta\right)+\left(\frac{1}{4}-\frac{3}{4}\right)tan\left(\theta\right)$ Since , $f'(\theta) =$$\frac{1}{6}\left(-3\sec ^2\left(\theta\right)-\left(\sqrt{3}-9\right)\cos \left(\theta\right)+\left(\sqrt{3}-3\right)\sin \left(\theta\right)\right)$ I know option (3) is true , since it is possible for both statements . Can you prove in formal way ?",,"['calculus', 'derivatives', 'continuity']"
8,How to calculate radius of the surface of water at different points in time when it is poured into a spherical container?,How to calculate radius of the surface of water at different points in time when it is poured into a spherical container?,,"So I have a sphere with radius $1\ cm$, and I'm pouring in water at $0.5\ cm^3/s$. How would I find a the radius of the surface of the water at any given time? So in the end this would look like a quadratic equation relating time and radius, where the maxima would be where radius $= 1\ cm$. How would I go about using differentiation in solving for this equation?","So I have a sphere with radius $1\ cm$, and I'm pouring in water at $0.5\ cm^3/s$. How would I find a the radius of the surface of the water at any given time? So in the end this would look like a quadratic equation relating time and radius, where the maxima would be where radius $= 1\ cm$. How would I go about using differentiation in solving for this equation?",,"['calculus', 'derivatives', 'quadratics', 'volume']"
9,Differentiating composition of functions proof help,Differentiating composition of functions proof help,,"Theorem: Let $X, Y, Z$ be normed spaces and $U\subset X$, $V \subset Y$ open sets. If the function $f:U \to V$ is differentiable in $x \in U$ and function $g: V \to Z$ differentiable in $f(x)\in V$, then the function $g \circ f: U \to Z$ is differentiable in $x \in U,$ and : $$(g \circ f)'(x)=g'(f(x))\circ f'(x)$$ Proof:(I will highlight from which point on is unclear to me..) -since the functions are differentiable then: $f(x+h)-f(x)=f'(x)h+R_1(h), \\g(y+h)-g(y)=g'(x)h+R_2(k) $ where $\frac{\|R_1(h)\|}{\|h\|}\to 0 $ when $h\to 0, \frac{\|R_2(k)\|}{\|k\|} \to 0$ when $k \to 0$. Let's give $f(x)$ the notation $y$, when we have : $$(g \circ f)(x+h)-(g  \circ f )(x)= g(f(x+h))-g(f(x))\\ g(y+f'(x)h+R_1(h))-g(y)=$$ $$g'(y)(f'(x)h)+g'(y)(R_1(h))+R_2(f'(x)h+R_1(h))= \\ (g'(y) \circ f'(x))h+ R(h) \text{ where } R(h)=g'(y)R_1(h)+R_2(f'(x))h + R_1(h).\text { Lets give the vector  } f'(x)h+R_1(h) \text{ the notation } H. \\ \|g'(y)R_1(h)\|\leq \|g'(y)\| \|R_1(h)\|, \\ \|H\| \leq \|f'(x) \|\|h\|+ \| R_1(h)\|, \text{Since }R_1(h)=o(h), R_2(H)=o(H), \text{ from (1),(2),(3) we have:}\\ g'(y)R_1(h)=o(h) \\  H\to 0 \text{ when } h \to 0 \text{ where } \frac{\|H\|}{\|h\|} \text{ is bounded (the professor puts a lot of attention on this part. )}$$ My definition of differentiability: Let $X$ and $Y$ be normed vector spaces upon the same field $\mathbb R$ or $\mathbb C$ and $U$ an open set in $X$. For a function $f:U \to Y$ it is said to be differentiable in point $x \in U$ if there exists a continuous linear map $A_x:X \to Y$ such that: $$f(x+h)-f(x)=A_xh+R(h)$$ where $$\lim_{h \to 0}\frac{R(h)}{\|h\|}=0. \text{ or } R(h)=o(h)$$","Theorem: Let $X, Y, Z$ be normed spaces and $U\subset X$, $V \subset Y$ open sets. If the function $f:U \to V$ is differentiable in $x \in U$ and function $g: V \to Z$ differentiable in $f(x)\in V$, then the function $g \circ f: U \to Z$ is differentiable in $x \in U,$ and : $$(g \circ f)'(x)=g'(f(x))\circ f'(x)$$ Proof:(I will highlight from which point on is unclear to me..) -since the functions are differentiable then: $f(x+h)-f(x)=f'(x)h+R_1(h), \\g(y+h)-g(y)=g'(x)h+R_2(k) $ where $\frac{\|R_1(h)\|}{\|h\|}\to 0 $ when $h\to 0, \frac{\|R_2(k)\|}{\|k\|} \to 0$ when $k \to 0$. Let's give $f(x)$ the notation $y$, when we have : $$(g \circ f)(x+h)-(g  \circ f )(x)= g(f(x+h))-g(f(x))\\ g(y+f'(x)h+R_1(h))-g(y)=$$ $$g'(y)(f'(x)h)+g'(y)(R_1(h))+R_2(f'(x)h+R_1(h))= \\ (g'(y) \circ f'(x))h+ R(h) \text{ where } R(h)=g'(y)R_1(h)+R_2(f'(x))h + R_1(h).\text { Lets give the vector  } f'(x)h+R_1(h) \text{ the notation } H. \\ \|g'(y)R_1(h)\|\leq \|g'(y)\| \|R_1(h)\|, \\ \|H\| \leq \|f'(x) \|\|h\|+ \| R_1(h)\|, \text{Since }R_1(h)=o(h), R_2(H)=o(H), \text{ from (1),(2),(3) we have:}\\ g'(y)R_1(h)=o(h) \\  H\to 0 \text{ when } h \to 0 \text{ where } \frac{\|H\|}{\|h\|} \text{ is bounded (the professor puts a lot of attention on this part. )}$$ My definition of differentiability: Let $X$ and $Y$ be normed vector spaces upon the same field $\mathbb R$ or $\mathbb C$ and $U$ an open set in $X$. For a function $f:U \to Y$ it is said to be differentiable in point $x \in U$ if there exists a continuous linear map $A_x:X \to Y$ such that: $$f(x+h)-f(x)=A_xh+R(h)$$ where $$\lim_{h \to 0}\frac{R(h)}{\|h\|}=0. \text{ or } R(h)=o(h)$$",,"['calculus', 'derivatives']"
10,Function $f(x)=2e^x/(1+e^x)$ and its critical point,Function  and its critical point,f(x)=2e^x/(1+e^x),"Looking at the graph of $$\frac{2e^{x}}{1+e^{x}},$$ there is a critical point at $x=0$ with an undefined derivative. The problem that I have is to find the critical point algebraically: $$f'(x)=\frac{2e^{x}}{(1+e^{x})^{2}} = 0 \implies e^{x}=-1 \text{ or } e^{x}=0$$ To my knowledge there no values of $x$ that would make those two equations true. Am I mistaken in my approach/calculations, or is there something that i do not know yet?","Looking at the graph of $$\frac{2e^{x}}{1+e^{x}},$$ there is a critical point at $x=0$ with an undefined derivative. The problem that I have is to find the critical point algebraically: $$f'(x)=\frac{2e^{x}}{(1+e^{x})^{2}} = 0 \implies e^{x}=-1 \text{ or } e^{x}=0$$ To my knowledge there no values of $x$ that would make those two equations true. Am I mistaken in my approach/calculations, or is there something that i do not know yet?",,"['calculus', 'real-analysis', 'derivatives']"
11,Why should I use derivatives and calculus?,Why should I use derivatives and calculus?,,"I know that this question maybe sounds pretty generic, but it's a curiosity that I have and I didn't found any answer yet. I recently started studying calculus using this material where is said that ""The fundamental idea of calculus is to study change by studying ""instantaneous"" change"". So, its said that when you're trying to find a instantaneous speed you should use the derivative of delta Distance by delta Time and not the ""common"" equation (without the derivate). My questions are: (1) why derivaties represents the instantaneous change in the system, (2) why the equation not-derived don't represent correctly that instantaneous change (considering that that still receives the instantaneous parameters for time and space) and (3) why should I use calculus? Thanks.","I know that this question maybe sounds pretty generic, but it's a curiosity that I have and I didn't found any answer yet. I recently started studying calculus using this material where is said that ""The fundamental idea of calculus is to study change by studying ""instantaneous"" change"". So, its said that when you're trying to find a instantaneous speed you should use the derivative of delta Distance by delta Time and not the ""common"" equation (without the derivate). My questions are: (1) why derivaties represents the instantaneous change in the system, (2) why the equation not-derived don't represent correctly that instantaneous change (considering that that still receives the instantaneous parameters for time and space) and (3) why should I use calculus? Thanks.",,"['calculus', 'derivatives']"
12,Integral Inequality Proof Using Hölder's inequality,Integral Inequality Proof Using Hölder's inequality,,"I'm working on the extra credit for my Calculus 1 class and the last problem is a proof. We have done proofs before, but I'm unsure of how to approach this problem. Any help would be much appreciated, I'm just not entirely sure how to start. Specifically, showing how the continuous function defined by the problem is to taken into account. I understand that it defines the bounds for the integrals, but beyond that I am unsure of how they are affected. My intuition tells me that it implies the use of a specific theorem, but no specific theorem is apparent to me from simple observation. I've found evidence that Holder's Inequality would be helpful, but I'm not certain as to how to apply this. The problem is as follows: Show that for any continuous function $f:[-1,1]\to\mathbb R$ we have that: $$2\int_{-1}^{1} f^2(x) dx \ge \left(\int_{-1}^{1}f(x)dx\right)^2+3\left(\int_{-1}^{1}xf(x)dx\right)^2$$ I've tried multiple approaches essentially yielding nothing though. My understanding of how to manipulate integrals in a general manner is weak at best. I did get a hint from my TA that said the proof is almost a page long and uses results beyond Calculus 1, so technically I shouldn't even be able to do this proof, but it is worth a significant amount of my grade and I could really use all the help I can get. We actually only started integrals about 3 weeks ago. Not to mention it has now started to bother me that I can't solve it despite spending so much time. The extra credit is due Friday, but my Final is tomorrow, so I won't be able to spend much more time on it. Thanks for any help you guys can provide.","I'm working on the extra credit for my Calculus 1 class and the last problem is a proof. We have done proofs before, but I'm unsure of how to approach this problem. Any help would be much appreciated, I'm just not entirely sure how to start. Specifically, showing how the continuous function defined by the problem is to taken into account. I understand that it defines the bounds for the integrals, but beyond that I am unsure of how they are affected. My intuition tells me that it implies the use of a specific theorem, but no specific theorem is apparent to me from simple observation. I've found evidence that Holder's Inequality would be helpful, but I'm not certain as to how to apply this. The problem is as follows: Show that for any continuous function $f:[-1,1]\to\mathbb R$ we have that: $$2\int_{-1}^{1} f^2(x) dx \ge \left(\int_{-1}^{1}f(x)dx\right)^2+3\left(\int_{-1}^{1}xf(x)dx\right)^2$$ I've tried multiple approaches essentially yielding nothing though. My understanding of how to manipulate integrals in a general manner is weak at best. I did get a hint from my TA that said the proof is almost a page long and uses results beyond Calculus 1, so technically I shouldn't even be able to do this proof, but it is worth a significant amount of my grade and I could really use all the help I can get. We actually only started integrals about 3 weeks ago. Not to mention it has now started to bother me that I can't solve it despite spending so much time. The extra credit is due Friday, but my Final is tomorrow, so I won't be able to spend much more time on it. Thanks for any help you guys can provide.",,"['calculus', 'integration', 'derivatives', 'integral-inequality']"
13,"What does the notation $\frac{\partial(x,y)}{\partial(u,v)}$ mean?",What does the notation  mean?,"\frac{\partial(x,y)}{\partial(u,v)}","Suppose G $(u,v) = (x, y, z)$ In terms of derivatives, what does $\frac{\partial(x,y)}{\partial(u,v)}$ mean?","Suppose G $(u,v) = (x, y, z)$ In terms of derivatives, what does $\frac{\partial(x,y)}{\partial(u,v)}$ mean?",,"['derivatives', 'notation', 'partial-derivative']"
14,Behavior of a periodic function,Behavior of a periodic function,,"Can a periodic function satisfy $f''(x)f(x)>0, x\in \mathbb{R}$ My intuition says no. Any thoughts on how to approach this?","Can a periodic function satisfy $f''(x)f(x)>0, x\in \mathbb{R}$ My intuition says no. Any thoughts on how to approach this?",,"['derivatives', 'periodic-functions']"
15,Is this proof of an elementary formula for the second derivative correct?,Is this proof of an elementary formula for the second derivative correct?,,"Let $f : (a,b) \rightarrow \mathbb{R}$  be twice continuously differentiable. I want to rigorously show the well-known formula $$f''(t) = \lim_{h \rightarrow 0} \frac{1}{h^2} ( f(t+h) + f(t-h) - 2f(t)).$$ So define the functions $\epsilon(t,h), \xi(t,h)$ by the following formulas: $$ f'(t) = \frac{f(t+h)-f(t-h)}{2h} + \epsilon(t,h)$$ and $$ f''(t) = \frac{f'(t+h)-f'(t-h)}{2h} + \xi(t,h).$$ By the properties of $f$, $\xi, \epsilon$ are continuous in $t$ for fixed nonzero $h$, and for fixed $t$ tend to $0$ as $h$ tends to $0$. Applying these formulas we get $$f''(t) = \frac{1}{2h} \left( \frac{f(t+2h)- f(t)}{2h} - \frac{f(t)- f(t-2h)}{2h}\right) + \frac{\epsilon(t+h,h) - \epsilon(t-h,h)}{2h},$$ and it's enough to show that the second term goes to $0$ with $h$. But the second term equals $\epsilon'(t_0(h),h)=\xi(t_0(h),h)$ for some $t_0(h) \in [t-h,t+h]$ (the derivative is with respect to $t$ for fixed $h$) and looking at the definition of $\xi$ we see that $ \xi(t_0,h) = f''(t_0) - f''(t_1)$ for some $t_1(h) \in [t-2h,t+2h]$. Finally we see that $$ |f''(t_0)-f''(t_1)| \leq \sup_{x,y \in [t-2h,t+2h]} |f''(x) - f''(y)| \rightarrow 0 \quad \textrm{as} \quad h \rightarrow 0. $$ I use the Mean Value theorem repeatedly above. Is this argument rigorous? Many thanks for helping me.","Let $f : (a,b) \rightarrow \mathbb{R}$  be twice continuously differentiable. I want to rigorously show the well-known formula $$f''(t) = \lim_{h \rightarrow 0} \frac{1}{h^2} ( f(t+h) + f(t-h) - 2f(t)).$$ So define the functions $\epsilon(t,h), \xi(t,h)$ by the following formulas: $$ f'(t) = \frac{f(t+h)-f(t-h)}{2h} + \epsilon(t,h)$$ and $$ f''(t) = \frac{f'(t+h)-f'(t-h)}{2h} + \xi(t,h).$$ By the properties of $f$, $\xi, \epsilon$ are continuous in $t$ for fixed nonzero $h$, and for fixed $t$ tend to $0$ as $h$ tends to $0$. Applying these formulas we get $$f''(t) = \frac{1}{2h} \left( \frac{f(t+2h)- f(t)}{2h} - \frac{f(t)- f(t-2h)}{2h}\right) + \frac{\epsilon(t+h,h) - \epsilon(t-h,h)}{2h},$$ and it's enough to show that the second term goes to $0$ with $h$. But the second term equals $\epsilon'(t_0(h),h)=\xi(t_0(h),h)$ for some $t_0(h) \in [t-h,t+h]$ (the derivative is with respect to $t$ for fixed $h$) and looking at the definition of $\xi$ we see that $ \xi(t_0,h) = f''(t_0) - f''(t_1)$ for some $t_1(h) \in [t-2h,t+2h]$. Finally we see that $$ |f''(t_0)-f''(t_1)| \leq \sup_{x,y \in [t-2h,t+2h]} |f''(x) - f''(y)| \rightarrow 0 \quad \textrm{as} \quad h \rightarrow 0. $$ I use the Mean Value theorem repeatedly above. Is this argument rigorous? Many thanks for helping me.",,"['calculus', 'real-analysis', 'derivatives', 'proof-verification']"
16,Taylor series for $\frac{1}{(1+x)^t}$,Taylor series for,\frac{1}{(1+x)^t},I'm having some trouble finding the Taylor series for the following function at zero (Maclaurin series). \begin{equation} \frac{1}{(1+x)^t} \end{equation} Where $t$ is a constant that is greater than zero.,I'm having some trouble finding the Taylor series for the following function at zero (Maclaurin series). \begin{equation} \frac{1}{(1+x)^t} \end{equation} Where $t$ is a constant that is greater than zero.,,"['calculus', 'real-analysis', 'sequences-and-series', 'derivatives', 'taylor-expansion']"
17,How to approach learning differentiation?,How to approach learning differentiation?,,"I currently in my first semester of Single Variable Calculus. I did reasonably well in Algebra (A), Trigonometry (B+), and Pre-Calculus (B+). However, I'm having some difficulty learning to differentiate more complex functions. I remember formulas reasonably well, so I remember the various rules of differentiation: the power rule, the rule for finding derivatives of a sum/difference or constant multiple of a function, as well as the product, and quotient rules. I'm having more difficulty with the chain rule, though - I understand the basic formula: $$ \frac{df}{dx} = \frac{df}{du}\frac{du}{dx} $$ But in practice, such as when trying to differentiate a complex function (like the one below), which involves using some or all of the differentiation rules in combination, I get bogged down in applying the rules correctly: i.e. in breaking the complex function into its constituent parts and applying the appropriate differentiation rules to them, in the context of the chain rule. $$ f(x) = \frac{3x^7+x^4\sqrt{2x^5+15x^\frac{4}{3}-23x+9}}{6x^2-4} $$ With problems like this one (especially any differentiation problem that involves use of the quotient rule, and especially any problem requiring the use of a combination of the product, quotient, and/or chain rules), I have a lot of trouble keeping all the small, constituent functions straight and then combining the values of their derivatives correctly to arrive at the correct answer. Surely, I can't be the first person to have this trouble. So, my question is: is there any intuition, technique, or approach, aside from ""practice, practice, practice"" (which I am doing) that might help me here? I've tried using the Wolfram Alpha Calculus course assistant app to show me the differentiation steps when I get stuck, but it can be a bit hard to follow, for me. Sometimes it seems to use notation that is different from the notation my instructor uses in class.","I currently in my first semester of Single Variable Calculus. I did reasonably well in Algebra (A), Trigonometry (B+), and Pre-Calculus (B+). However, I'm having some difficulty learning to differentiate more complex functions. I remember formulas reasonably well, so I remember the various rules of differentiation: the power rule, the rule for finding derivatives of a sum/difference or constant multiple of a function, as well as the product, and quotient rules. I'm having more difficulty with the chain rule, though - I understand the basic formula: $$ \frac{df}{dx} = \frac{df}{du}\frac{du}{dx} $$ But in practice, such as when trying to differentiate a complex function (like the one below), which involves using some or all of the differentiation rules in combination, I get bogged down in applying the rules correctly: i.e. in breaking the complex function into its constituent parts and applying the appropriate differentiation rules to them, in the context of the chain rule. $$ f(x) = \frac{3x^7+x^4\sqrt{2x^5+15x^\frac{4}{3}-23x+9}}{6x^2-4} $$ With problems like this one (especially any differentiation problem that involves use of the quotient rule, and especially any problem requiring the use of a combination of the product, quotient, and/or chain rules), I have a lot of trouble keeping all the small, constituent functions straight and then combining the values of their derivatives correctly to arrive at the correct answer. Surely, I can't be the first person to have this trouble. So, my question is: is there any intuition, technique, or approach, aside from ""practice, practice, practice"" (which I am doing) that might help me here? I've tried using the Wolfram Alpha Calculus course assistant app to show me the differentiation steps when I get stuck, but it can be a bit hard to follow, for me. Sometimes it seems to use notation that is different from the notation my instructor uses in class.",,"['calculus', 'derivatives']"
18,derivative of sqrt(5/(x+7)),derivative of sqrt(5/(x+7)),,Why is it that: $$\frac{d}{dx}\sqrt{\frac{5}{x+7}} = -\frac{\sqrt{5}}2\frac{1}{(x+7)^{3/2}}$$ (image) ??? My attempt: It seems that somehow you end up adding 1 to 1/2 to get 3/2 in the exponent. But why?,Why is it that: $$\frac{d}{dx}\sqrt{\frac{5}{x+7}} = -\frac{\sqrt{5}}2\frac{1}{(x+7)^{3/2}}$$ (image) ??? My attempt: It seems that somehow you end up adding 1 to 1/2 to get 3/2 in the exponent. But why?,,"['calculus', 'derivatives']"
19,Verification of product rule for covariant derivatives. Stuck on one step involving simplifying terms to yield zero.,Verification of product rule for covariant derivatives. Stuck on one step involving simplifying terms to yield zero.,,"I am trying to learn more about covariant differentiation. I'm specifically interested in physics applications, but I found this nice exercise in Misner, Thorne, and Wheeler's book Gravitation that I thought would help me get more familiar with the concept. It's purely math. Specifically, the task is to prove the product rule for covariant differentiation. Let $\text{S}^{\alpha\beta}_{\gamma}$ be a $(2,1)$ tensor field, and let $\text{M}^{\gamma}_{\beta}$ be a $(1,1)$ tensor field. By contracting these tensor fields, one obtains the vector field $\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^\gamma$. The divergence of this vector field reads   \begin{equation} (\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^\gamma)_{;\alpha}=\text{S}^{\alpha\beta}_{\gamma;\alpha}\text{M}_{\beta}^\gamma+\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta;\alpha}^\gamma \end{equation}   Verify the validity of this product rule by expressing both sides of the equation in terms of directional derivatives plus connection-coefficient corrections. Here's what I did, I started with the left hand side of the equation. Note, I use the Einstein summation convention. LHS:    \begin{align*} (\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^\gamma)_{;\alpha}&=\partial_{\alpha}(\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^\gamma)+\Gamma^{\alpha}_{\lambda\alpha}\text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma}\\&=(\partial_{\alpha}\text{S}^{\alpha\beta}_{\gamma})\text{M}_{\beta}^\gamma + \text{S}^{\alpha\beta}_{\gamma}(\partial_{\alpha}\text{M}_{\beta}^\gamma) + \Gamma^{\alpha}_{\lambda\alpha}\text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma} \end{align*}   I then decided to do some work on the right-hand side. RHS:   \begin{align*} \text{S}^{\alpha\beta}_{\gamma;\alpha}\text{M}_{\beta}^\gamma+\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta;\alpha}^\gamma&=\left((\partial_{\alpha}\text{S}^{\alpha\beta}_{\gamma})\text{M}_{\beta}^\gamma+\Gamma_{\alpha\lambda}^{\alpha}\text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma}+\Gamma^{\beta}_{\alpha\lambda}\text{S}^{\alpha\lambda}_{\gamma}\text{M}_{\beta}^{\gamma}-\Gamma^{\lambda}_{\alpha\gamma}\text{S}^{\alpha\beta}_{\lambda}\text{M}_{\beta}^{\gamma}\right)\\&\phantom{x}+\left(\text{S}^{\alpha\beta}_{\gamma}(\partial_{\alpha}\text{M}_{\beta}^\gamma)+\Gamma^{\gamma}_{\alpha\lambda}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^{\lambda}-\Gamma^{\lambda}_{\alpha\beta}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\lambda}^{\gamma}\right)\\&=\left((\partial_{\alpha}\text{S}^{\alpha\beta}_{\gamma})\text{M}_{\beta}^\gamma + \text{S}^{\alpha\beta}_{\gamma}(\partial_{\alpha}\text{M}_{\beta}^\gamma) + \Gamma^{\alpha}_{\lambda\alpha} \text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma}\right)\\&\phantom{x}+ \left(\Gamma^{\beta}_{\alpha\lambda}\text{S}^{\alpha\lambda}_{\gamma}\text{M}_{\beta}^{\gamma}-\Gamma^{\lambda}_{\alpha\gamma}\text{S}^{\alpha\beta}_{\lambda}\text{M}_{\beta}^{\gamma}+\Gamma^{\gamma}_{\alpha\lambda}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^{\lambda}-\Gamma^{\lambda}_{\alpha\beta}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\lambda}^{\gamma}\right)\\&=\left((\partial_{\alpha}\text{S}^{\alpha\beta}_{\gamma})\text{M}_{\beta}^\gamma + \text{S}^{\alpha\beta}_{\gamma}(\partial_{\alpha}\text{M}_{\beta}^\gamma) + \Gamma^{\alpha}_{\lambda\alpha} \text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma}\right)+(0) \end{align*} It's with the right hand side I got stuck. I should be able to somehow simplify \begin{equation} \left(\Gamma^{\beta}_{\alpha\lambda}\text{S}^{\alpha\lambda}_{\gamma}\text{M}_{\beta}^{\gamma}-\Gamma^{\lambda}_{\alpha\gamma}\text{S}^{\alpha\beta}_{\lambda}\text{M}_{\beta}^{\gamma}+\Gamma^{\gamma}_{\alpha\lambda}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^{\lambda}-\Gamma^{\lambda}_{\alpha\beta}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\lambda}^{\gamma}\right) \end{equation} to yield $0$, but I can't figure out how to do it. This would leave my right-hand and left-hand sides equal and thus completing the proof. But I am stuck on this one step. Any ideas? My Thoughts: I'm not sure what kind of manipulation I need to do. Do I need to manipulate indices? That seemed like a possibility. I have seen sometimes you can relabel dummy indices in a way that allows you to rearrange terms. But I can't tell if I can do that here. Another possibility is maybe I need to do some kind of factoring by multiplying by identity and then rearranging to somehow simplify so things cancel. I'm not sure if that's the right idea or even how I would do that, but that seemed like another possibility. Apart from that, I am stumped.","I am trying to learn more about covariant differentiation. I'm specifically interested in physics applications, but I found this nice exercise in Misner, Thorne, and Wheeler's book Gravitation that I thought would help me get more familiar with the concept. It's purely math. Specifically, the task is to prove the product rule for covariant differentiation. Let $\text{S}^{\alpha\beta}_{\gamma}$ be a $(2,1)$ tensor field, and let $\text{M}^{\gamma}_{\beta}$ be a $(1,1)$ tensor field. By contracting these tensor fields, one obtains the vector field $\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^\gamma$. The divergence of this vector field reads   \begin{equation} (\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^\gamma)_{;\alpha}=\text{S}^{\alpha\beta}_{\gamma;\alpha}\text{M}_{\beta}^\gamma+\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta;\alpha}^\gamma \end{equation}   Verify the validity of this product rule by expressing both sides of the equation in terms of directional derivatives plus connection-coefficient corrections. Here's what I did, I started with the left hand side of the equation. Note, I use the Einstein summation convention. LHS:    \begin{align*} (\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^\gamma)_{;\alpha}&=\partial_{\alpha}(\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^\gamma)+\Gamma^{\alpha}_{\lambda\alpha}\text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma}\\&=(\partial_{\alpha}\text{S}^{\alpha\beta}_{\gamma})\text{M}_{\beta}^\gamma + \text{S}^{\alpha\beta}_{\gamma}(\partial_{\alpha}\text{M}_{\beta}^\gamma) + \Gamma^{\alpha}_{\lambda\alpha}\text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma} \end{align*}   I then decided to do some work on the right-hand side. RHS:   \begin{align*} \text{S}^{\alpha\beta}_{\gamma;\alpha}\text{M}_{\beta}^\gamma+\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta;\alpha}^\gamma&=\left((\partial_{\alpha}\text{S}^{\alpha\beta}_{\gamma})\text{M}_{\beta}^\gamma+\Gamma_{\alpha\lambda}^{\alpha}\text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma}+\Gamma^{\beta}_{\alpha\lambda}\text{S}^{\alpha\lambda}_{\gamma}\text{M}_{\beta}^{\gamma}-\Gamma^{\lambda}_{\alpha\gamma}\text{S}^{\alpha\beta}_{\lambda}\text{M}_{\beta}^{\gamma}\right)\\&\phantom{x}+\left(\text{S}^{\alpha\beta}_{\gamma}(\partial_{\alpha}\text{M}_{\beta}^\gamma)+\Gamma^{\gamma}_{\alpha\lambda}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^{\lambda}-\Gamma^{\lambda}_{\alpha\beta}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\lambda}^{\gamma}\right)\\&=\left((\partial_{\alpha}\text{S}^{\alpha\beta}_{\gamma})\text{M}_{\beta}^\gamma + \text{S}^{\alpha\beta}_{\gamma}(\partial_{\alpha}\text{M}_{\beta}^\gamma) + \Gamma^{\alpha}_{\lambda\alpha} \text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma}\right)\\&\phantom{x}+ \left(\Gamma^{\beta}_{\alpha\lambda}\text{S}^{\alpha\lambda}_{\gamma}\text{M}_{\beta}^{\gamma}-\Gamma^{\lambda}_{\alpha\gamma}\text{S}^{\alpha\beta}_{\lambda}\text{M}_{\beta}^{\gamma}+\Gamma^{\gamma}_{\alpha\lambda}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^{\lambda}-\Gamma^{\lambda}_{\alpha\beta}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\lambda}^{\gamma}\right)\\&=\left((\partial_{\alpha}\text{S}^{\alpha\beta}_{\gamma})\text{M}_{\beta}^\gamma + \text{S}^{\alpha\beta}_{\gamma}(\partial_{\alpha}\text{M}_{\beta}^\gamma) + \Gamma^{\alpha}_{\lambda\alpha} \text{S}^{\lambda\beta}_{\gamma}\text{M}_{\beta}^{\gamma}\right)+(0) \end{align*} It's with the right hand side I got stuck. I should be able to somehow simplify \begin{equation} \left(\Gamma^{\beta}_{\alpha\lambda}\text{S}^{\alpha\lambda}_{\gamma}\text{M}_{\beta}^{\gamma}-\Gamma^{\lambda}_{\alpha\gamma}\text{S}^{\alpha\beta}_{\lambda}\text{M}_{\beta}^{\gamma}+\Gamma^{\gamma}_{\alpha\lambda}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\beta}^{\lambda}-\Gamma^{\lambda}_{\alpha\beta}\text{S}^{\alpha\beta}_{\gamma}\text{M}_{\lambda}^{\gamma}\right) \end{equation} to yield $0$, but I can't figure out how to do it. This would leave my right-hand and left-hand sides equal and thus completing the proof. But I am stuck on this one step. Any ideas? My Thoughts: I'm not sure what kind of manipulation I need to do. Do I need to manipulate indices? That seemed like a possibility. I have seen sometimes you can relabel dummy indices in a way that allows you to rearrange terms. But I can't tell if I can do that here. Another possibility is maybe I need to do some kind of factoring by multiplying by identity and then rearranging to somehow simplify so things cancel. I'm not sure if that's the right idea or even how I would do that, but that seemed like another possibility. Apart from that, I am stumped.",,"['differential-geometry', 'derivatives', 'riemannian-geometry']"
20,Derivative with respect to another function,Derivative with respect to another function,,"I stumbled on this calculus problem here: Let $f(x) = \ln|\sec x + \tan x|$ and $g(x) = \sec x + \tan x.$ Find the fourth derivative of $g(x)$ taken with respect to $f(x)$ A) $\\$ $f'(x)$ B) $\\$ $f'(x)g(x)$ C) $\\$ $g(x)$ D) $\\$ $g'(x)$ E) $\\$ NOTA The answer to the question was C I understand how to find a function taken with respect with a derivative. For example, if I was trying to find the first derivative: $$\frac{dg(x)}{df(x)} = \frac{dg(x)}{dx}*\frac{dx}{df(x)}$$ I would simply use the chain rule. However, for subsequent derivatives, the two functions tend to get more messy, so I decided to look at the solution provided to see if there was an easier method. Interestingly enough, the solution is: The first derivative simplifies as follows: $$\frac{d(g(x))}{d(f(x))} = \frac{d(g(x))}{dx}*\frac{dx}{d(f(x))} = \frac{\sec^2(x) + \sec(x)\tan(x)}{\sec(x)} = g(x)$$ Thus all higher order derivatives will produce the same result, and the answer is $g(x) =$ C. Is this conclusion true for any function taken with respect with another function? If so, if anyone could point me to a proof that would be also appreciated.","I stumbled on this calculus problem here: Let and Find the fourth derivative of taken with respect to A) B) C) D) E) NOTA The answer to the question was C I understand how to find a function taken with respect with a derivative. For example, if I was trying to find the first derivative: I would simply use the chain rule. However, for subsequent derivatives, the two functions tend to get more messy, so I decided to look at the solution provided to see if there was an easier method. Interestingly enough, the solution is: The first derivative simplifies as follows: Thus all higher order derivatives will produce the same result, and the answer is C. Is this conclusion true for any function taken with respect with another function? If so, if anyone could point me to a proof that would be also appreciated.",f(x) = \ln|\sec x + \tan x| g(x) = \sec x + \tan x. g(x) f(x) \\ f'(x) \\ f'(x)g(x) \\ g(x) \\ g'(x) \\ \frac{dg(x)}{df(x)} = \frac{dg(x)}{dx}*\frac{dx}{df(x)} \frac{d(g(x))}{d(f(x))} = \frac{d(g(x))}{dx}*\frac{dx}{d(f(x))} = \frac{\sec^2(x) + \sec(x)\tan(x)}{\sec(x)} = g(x) g(x) =,"['calculus', 'reference-request', 'derivatives']"
21,Integral inequality given the bounds for derivative,Integral inequality given the bounds for derivative,,"Let $f:[0,1]\rightarrow \mathbb{R}$ be a continuously differentiable function such that $$\int_0^1 f(x) \, dx=0$$ and $m \leq f'(x) \leq M$ on $(0,1)$. Prove that $$\frac{m}{12} \leq \int_0^1 xf(x) \, dx \leq \frac{M}{12}.$$ This is just the last bonus question in our test yesterday. I wasn't able to answer of course. Though I did verify it by letting $\displaystyle f(x)= \left( x−\frac{1}{2} \right)^3$ for which I found $0\le f′(x)\le 3/4$ on $(0,1)$ and $$0 \le \int^1_0 xf(x) \, dx=\frac{1}{80} \le \frac{3}{48}.$$ I do not know how to prove this.","Let $f:[0,1]\rightarrow \mathbb{R}$ be a continuously differentiable function such that $$\int_0^1 f(x) \, dx=0$$ and $m \leq f'(x) \leq M$ on $(0,1)$. Prove that $$\frac{m}{12} \leq \int_0^1 xf(x) \, dx \leq \frac{M}{12}.$$ This is just the last bonus question in our test yesterday. I wasn't able to answer of course. Though I did verify it by letting $\displaystyle f(x)= \left( x−\frac{1}{2} \right)^3$ for which I found $0\le f′(x)\le 3/4$ on $(0,1)$ and $$0 \le \int^1_0 xf(x) \, dx=\frac{1}{80} \le \frac{3}{48}.$$ I do not know how to prove this.",,"['real-analysis', 'inequality', 'derivatives', 'definite-integrals']"
22,"Derivative, the tangent line $y = \sqrt{9-4x}$ at point (-4,5)","Derivative, the tangent line  at point (-4,5)",y = \sqrt{9-4x},"The function is given, $$y=\sqrt{9-4x}$$ Now I have to find the tangent line at point (-4,5). This is how i did it previously... Derivative; The Tangent line $y=x^2 -4x -5 ; (-2,7)$ Now, I think I'm getting mixed or something... This is what I did so far: $$$$ $$f(x)=\sqrt{9-4x}\\f(x_1)=\sqrt{9-4x_1}\\f(x_1+\Delta x) = \sqrt{9-4(x_1+\Delta x})$$ $$$$ So, $$\lim_{\Delta x\to 0} {f(x_1+\Delta x) - f(x_1) \over \Delta x} \\=\lim_{\Delta x\to 0} {\sqrt{9-4(x_1 +\Delta x)} - \sqrt{9-4x_1}\over \Delta x} \\=\lim_{\Delta x\to 0}{\sqrt{9-4x_1-4\Delta x -9 +4x_1}\over \Delta x} \\=\lim_{\Delta x\to 0}{\sqrt{-4\Delta x} \over \Delta x}$$ Now, I tried doing, $$\lim_{\Delta x \to 0} {\sqrt{-4\Delta x}\over\sqrt {\Delta x^2}}$$ I'm not sure if it's right, but I can't get it to work. Any help?? Thanks","The function is given, $$y=\sqrt{9-4x}$$ Now I have to find the tangent line at point (-4,5). This is how i did it previously... Derivative; The Tangent line $y=x^2 -4x -5 ; (-2,7)$ Now, I think I'm getting mixed or something... This is what I did so far: $$$$ $$f(x)=\sqrt{9-4x}\\f(x_1)=\sqrt{9-4x_1}\\f(x_1+\Delta x) = \sqrt{9-4(x_1+\Delta x})$$ $$$$ So, $$\lim_{\Delta x\to 0} {f(x_1+\Delta x) - f(x_1) \over \Delta x} \\=\lim_{\Delta x\to 0} {\sqrt{9-4(x_1 +\Delta x)} - \sqrt{9-4x_1}\over \Delta x} \\=\lim_{\Delta x\to 0}{\sqrt{9-4x_1-4\Delta x -9 +4x_1}\over \Delta x} \\=\lim_{\Delta x\to 0}{\sqrt{-4\Delta x} \over \Delta x}$$ Now, I tried doing, $$\lim_{\Delta x \to 0} {\sqrt{-4\Delta x}\over\sqrt {\Delta x^2}}$$ I'm not sure if it's right, but I can't get it to work. Any help?? Thanks",,"['calculus', 'derivatives']"
23,"Faulty application of the Fundamental Theorem of Calculus to $f(x) = 0$ for $x\ne 0$, $f(0)=1$","Faulty application of the Fundamental Theorem of Calculus to  for ,",f(x) = 0 x\ne 0 f(0)=1,"I think I have given a fallacious proof but I can't seem to find what is wrong with it. Suppose $f : \mathbb{R} \rightarrow \mathbb{R}$ has the property that $\forall a,b \in \mathbb{R}. \int_a^b f(t)dt = b - a$. Consider the following ""proof"" that f(x) = 1. Define $g(x) = \int_0^x f(t)dt$. Then from above we know g(x) = x. Further by the fundamental theorem of calculus $g'(x) = f(x) = 1$. But then consider $$f(x) = \begin{cases} 0 & \text{if $x=0$} \\ 1 & \text{if $ x \not = 0$}\end{cases}$$ Doesn't this function have the above property?","I think I have given a fallacious proof but I can't seem to find what is wrong with it. Suppose $f : \mathbb{R} \rightarrow \mathbb{R}$ has the property that $\forall a,b \in \mathbb{R}. \int_a^b f(t)dt = b - a$. Consider the following ""proof"" that f(x) = 1. Define $g(x) = \int_0^x f(t)dt$. Then from above we know g(x) = x. Further by the fundamental theorem of calculus $g'(x) = f(x) = 1$. But then consider $$f(x) = \begin{cases} 0 & \text{if $x=0$} \\ 1 & \text{if $ x \not = 0$}\end{cases}$$ Doesn't this function have the above property?",,"['real-analysis', 'integration', 'derivatives']"
24,Why do we need to assume continuity in the proof of the chain-rule?,Why do we need to assume continuity in the proof of the chain-rule?,,"Look at this proof: If $f$ is differentiable at $x$, then it must be continuous there too? Does he then need in the hypothesis that $f$ need to be continuous in the entire interval? What if he just assumes that it is differentiable at $x$? He also does not mention anything about the continuity of $g$?","Look at this proof: If $f$ is differentiable at $x$, then it must be continuous there too? Does he then need in the hypothesis that $f$ need to be continuous in the entire interval? What if he just assumes that it is differentiable at $x$? He also does not mention anything about the continuity of $g$?",,"['calculus', 'real-analysis', 'derivatives', 'continuity']"
25,"Prove that if $f(0)=f(1)=0$ and $\{x:f'(x)=0\}\subset \{x:f(x)=0\}$, then $f\equiv 0$","Prove that if  and , then",f(0)=f(1)=0 \{x:f'(x)=0\}\subset \{x:f(x)=0\} f\equiv 0,"Suppose $f: \mathbb{R}\rightarrow \mathbb{R}$ is differentiable with $f(0)=f(1)=0$ and $\{x:f'(x)=0\}\subset \{x:f(x)=0\}$. Show that $f(x)=0$ for all $x\in [0,1]$. My Work: By Rolle's Theorem intituily I see that this is true. Or by Rolle's Theorem there are local minima and maxima, so $f$ is $0$ at those local minima and maxima. Hence, $f$ must be $0$ for all $x\in [0,1]$. But how can I write a legible proof for this? Can anyone please help?","Suppose $f: \mathbb{R}\rightarrow \mathbb{R}$ is differentiable with $f(0)=f(1)=0$ and $\{x:f'(x)=0\}\subset \{x:f(x)=0\}$. Show that $f(x)=0$ for all $x\in [0,1]$. My Work: By Rolle's Theorem intituily I see that this is true. Or by Rolle's Theorem there are local minima and maxima, so $f$ is $0$ at those local minima and maxima. Hence, $f$ must be $0$ for all $x\in [0,1]$. But how can I write a legible proof for this? Can anyone please help?",,"['calculus', 'real-analysis', 'derivatives', 'continuity']"
26,Why this approach to differentiate $\log_{10}(x+1)^x$ does not work?,Why this approach to differentiate  does not work?,\log_{10}(x+1)^x,"I am trying to differentiate $\log_{10}(x+1)^x$ but I don't get the correct answer, could you please help me? I know that one correct solution is the following: \begin{align} \frac{d}{dx}\log_{10}(x+1)^x &= \frac{d}{dx}e^{x\ln(\log_{10}(x+1))}\\  &= e^{x\ln(\log_{10}(x+1))}(\ln(\log_{10}(x+1))+\frac{x}{(x+1)\log_{10}(x+1)\ln(10)})\\  &= \log_{10}(x+1)^x(\ln(\log_{10}(x+1))+\frac{x}{(x+1)\log_{10}(x+1)\ln(10)}) \end{align} However, I also tried this other approach but I don't know why it doesn't work: $$\frac{d}{dx}\log_{10}(x+1)^x = (\log_{10}(1+x)^x)(\ln(\log_{10}(1+x)))(\frac{1}{(x+1)\ln(10)}),$$ using these rules: $$\frac{d}{dx}a^x = \ln(a)a^x$$ $$\frac{d}{dx}\log_a(x) = \frac{1}{x\ln(a)}.$$ Could you explain me why it doesn't work? What I am doing wrong? Thanks!","I am trying to differentiate $\log_{10}(x+1)^x$ but I don't get the correct answer, could you please help me? I know that one correct solution is the following: \begin{align} \frac{d}{dx}\log_{10}(x+1)^x &= \frac{d}{dx}e^{x\ln(\log_{10}(x+1))}\\  &= e^{x\ln(\log_{10}(x+1))}(\ln(\log_{10}(x+1))+\frac{x}{(x+1)\log_{10}(x+1)\ln(10)})\\  &= \log_{10}(x+1)^x(\ln(\log_{10}(x+1))+\frac{x}{(x+1)\log_{10}(x+1)\ln(10)}) \end{align} However, I also tried this other approach but I don't know why it doesn't work: $$\frac{d}{dx}\log_{10}(x+1)^x = (\log_{10}(1+x)^x)(\ln(\log_{10}(1+x)))(\frac{1}{(x+1)\ln(10)}),$$ using these rules: $$\frac{d}{dx}a^x = \ln(a)a^x$$ $$\frac{d}{dx}\log_a(x) = \frac{1}{x\ln(a)}.$$ Could you explain me why it doesn't work? What I am doing wrong? Thanks!",,"['algebra-precalculus', 'derivatives']"
27,Find $a$ such that $x^3 +3x^2-9x+a = 0$ has only one real root,Find  such that  has only one real root,a x^3 +3x^2-9x+a = 0,"I have the function $$x^3 +3x^2-9x+a$$ If I take the derivative, I have $$3x^2+6x-9$$ This is a parabola with a negative part. So my function isn't always increasing, and therefore can have more than one root. I have to find $a$ such that, even when the function decreases, it can't reach the $x$ axis. More precisely, I have to move my function up by a amount of $a$ such that the local minimum of the function is greater than zero. How would you guys solve it? I can't think of a better way.","I have the function $$x^3 +3x^2-9x+a$$ If I take the derivative, I have $$3x^2+6x-9$$ This is a parabola with a negative part. So my function isn't always increasing, and therefore can have more than one root. I have to find $a$ such that, even when the function decreases, it can't reach the $x$ axis. More precisely, I have to move my function up by a amount of $a$ such that the local minimum of the function is greater than zero. How would you guys solve it? I can't think of a better way.",,"['calculus', 'derivatives']"
28,Looking for differentiable functions $f$ such that the set of points at which $|f|$ is not differentiable has some particular properties,Looking for differentiable functions  such that the set of points at which  is not differentiable has some particular properties,f |f|,"I know that $\sin x$ is differentiable at all points but $|\sin x|$ is not differentiable at countably many points namely at integer multiples of $\pi$ . So I am asking the following questions i) Give example of a differentiable function $f:\mathbb R \to \mathbb R$ such that the set of points at which $|f|$ is  not differentiable is countable and dense in $\mathbb R$ ii) Give example of a differentiable function $f:\mathbb R \to \mathbb R$ such that the set of points at which $|f|$ is  not differentiable is uncountable and not dense in $\mathbb R$ iii) Give example of a differentiable function $f:\mathbb R \to \mathbb R$ such that the set of points at which $|f|$ is  not differentiable is uncountable and  dense in $\mathbb R$ $ UPDATE$:- As mentioned in the comments if $f$ is differentiable and and at some point non-zero then since by continuity $f$ will have same sign in a  neighborhood of that point , $|f|$ will be differentiable ; thus $|f|$ is not differentiable $c$ only when $f(c)=0$ but then as  John pointed out , if such points are dense then this leads to a contradiction , resolving i) and iii) in the negative ; this leaves us with (ii) only","I know that $\sin x$ is differentiable at all points but $|\sin x|$ is not differentiable at countably many points namely at integer multiples of $\pi$ . So I am asking the following questions i) Give example of a differentiable function $f:\mathbb R \to \mathbb R$ such that the set of points at which $|f|$ is  not differentiable is countable and dense in $\mathbb R$ ii) Give example of a differentiable function $f:\mathbb R \to \mathbb R$ such that the set of points at which $|f|$ is  not differentiable is uncountable and not dense in $\mathbb R$ iii) Give example of a differentiable function $f:\mathbb R \to \mathbb R$ such that the set of points at which $|f|$ is  not differentiable is uncountable and  dense in $\mathbb R$ $ UPDATE$:- As mentioned in the comments if $f$ is differentiable and and at some point non-zero then since by continuity $f$ will have same sign in a  neighborhood of that point , $|f|$ will be differentiable ; thus $|f|$ is not differentiable $c$ only when $f(c)=0$ but then as  John pointed out , if such points are dense then this leads to a contradiction , resolving i) and iii) in the negative ; this leaves us with (ii) only",,['real-analysis']
29,Calculating derivative by keeping all but one $x$ constant and adding the results together for different $x$,Calculating derivative by keeping all but one  constant and adding the results together for different,x x,In this answer there's a comment which says That's not wrong; that's a perfectly valid method. You get the derivative of any expression with respect to $x$ as the sums of all the derivatives with respect to the individual instances of $x$ while holding all other instances constant.  – joriki This blew my mind. How do you prove this?,In this answer there's a comment which says That's not wrong; that's a perfectly valid method. You get the derivative of any expression with respect to $x$ as the sums of all the derivatives with respect to the individual instances of $x$ while holding all other instances constant.  – joriki This blew my mind. How do you prove this?,,"['calculus', 'derivatives']"
30,The derivative of something with respect to $3x+5$?,The derivative of something with respect to ?,3x+5,"If you take $(3x+5)^2$ and differentiate it with respect to $3x+5$ it's just $2(3x+5)$. Can someone explain to me how this would actually work out? I understand normal derivatives with respect to say, $x$, where at some point $x$, $f '(x)$ is the slope at that $x$ value. But how would this work out in this situation?","If you take $(3x+5)^2$ and differentiate it with respect to $3x+5$ it's just $2(3x+5)$. Can someone explain to me how this would actually work out? I understand normal derivatives with respect to say, $x$, where at some point $x$, $f '(x)$ is the slope at that $x$ value. But how would this work out in this situation?",,"['calculus', 'derivatives']"
31,differentiating a function of a function $w=\sqrt{u^2+v^2}$,differentiating a function of a function,w=\sqrt{u^2+v^2},"I want to find the total differential of $w$ from a given function $w=\sqrt{u^2+v^2}\:with\:u\:=\:cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)\:and\:v\:=\:sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\right)$ to solving this problem I'm using $\left(a\right)\:\:\:\:\frac{dw}{dp}=\frac{∂w}{∂u}\cdot \frac{du}{dp}+\frac{∂w}{∂v}\cdot \frac{dv}{dp}$ first I'm solving for the partial derivatives of $w$ with respect to $u$ and $v$ on RHS $\left(b\right)\:\:\:\:\frac{∂w}{∂u}=\frac{u}{\sqrt{u^2+v^2}}\:\:\:\:and\:\:\:\:\frac{∂w}{∂v}=\frac{v}{\sqrt{u^2+v^2}}$ and then I'm solving for the total derivatives of $u$ and $v$ with respect to $p$ on RHS using chain rule. $\frac{du}{dp}=\frac{d}{dx}cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)=\frac{du}{dx}\cdot \frac{dx}{dy}\cdot \frac{dy}{dp}$ let $x=ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\:$ and $y\:=tan\left(p+\frac{1}{2}\pi \right)$ $\left(1\right)\:\frac{du}{dx}=\frac{dcos\left(x\right)}{dx}=-sin\left(x\right)=-sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)$ $\left(2\right)\:\frac{dx}{dy}=\frac{dln\left(y\right)}{dy}=\frac{1}{y}=\frac{1}{tan\left(p+\frac{1}{2}\pi \right)}$ $\left(3\right)\:\frac{dy}{dp}=\frac{dtan\left(p+\frac{1}{2}\pi \right)}{dp}=sec^2\left(p+\frac{1}{2}\pi \right)$ substitute back $(1)$ , $(2)$ , and $(3)$ into $\frac{du}{dp}$ i get $\left(c\right)\:\:\:\:\frac{du}{dp}=-\frac{sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{tan\left(p+\frac{1}{2}\pi \:\right)cos^2\left(p+\frac{1}{2}\pi \:\right)}$ now i want to find $\frac{dv}{dp}$ $\frac{dv}{dp}=\frac{dsin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{dp}=\frac{dv}{dx}\cdot \frac{dx}{dy}\cdot \frac{dy}{dp}$ since $\frac{dx}{dy}$ and $\frac{dy}{dp}$ have the same result as $(2)$ and $(3)$ , now i only have to find $\frac{dv}{dx}$ with $v\:=\:sin\left(x\right)$ i get $\frac{dv}{dx}=cos\left(x\right)=cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)$ and then substitute them back into $\frac{dv}{dp}$ yields $\left(d\right)\:\:\:\:\frac{dv}{dp}=\frac{cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{tan\left(p+\frac{1}{2}\pi \:\right)cos^2\left(p+\frac{1}{2}\pi \:\right)}$ finally, substitute $(b)$ , $(c)$ , and $(d)$ into $(a)$ i get $\frac{dw}{dp}=\frac{cot\left(p+\frac{1}{2}\pi \:\right)\cdot sec^2\left(p+\frac{1}{2}\pi \:\right)}{\sqrt{u^2+v^2}}\left(v\cdot cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\right)-u\cdot sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\:\right)\right)\right)\right)$ this is my answer, but i'm not sure is there an error in my calculation? and how to know if my answer is correct? thank you for your time and advice, it has nothing to do with homework or school, i just have no one to ask to correct my answer.","I want to find the total differential of from a given function to solving this problem I'm using first I'm solving for the partial derivatives of with respect to and on RHS and then I'm solving for the total derivatives of and with respect to on RHS using chain rule. let and substitute back , , and into i get now i want to find since and have the same result as and , now i only have to find with i get and then substitute them back into yields finally, substitute , , and into i get this is my answer, but i'm not sure is there an error in my calculation? and how to know if my answer is correct? thank you for your time and advice, it has nothing to do with homework or school, i just have no one to ask to correct my answer.",w w=\sqrt{u^2+v^2}\:with\:u\:=\:cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)\:and\:v\:=\:sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\right) \left(a\right)\:\:\:\:\frac{dw}{dp}=\frac{∂w}{∂u}\cdot \frac{du}{dp}+\frac{∂w}{∂v}\cdot \frac{dv}{dp} w u v \left(b\right)\:\:\:\:\frac{∂w}{∂u}=\frac{u}{\sqrt{u^2+v^2}}\:\:\:\:and\:\:\:\:\frac{∂w}{∂v}=\frac{v}{\sqrt{u^2+v^2}} u v p \frac{du}{dp}=\frac{d}{dx}cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)=\frac{du}{dx}\cdot \frac{dx}{dy}\cdot \frac{dy}{dp} x=ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\: y\:=tan\left(p+\frac{1}{2}\pi \right) \left(1\right)\:\frac{du}{dx}=\frac{dcos\left(x\right)}{dx}=-sin\left(x\right)=-sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right) \left(2\right)\:\frac{dx}{dy}=\frac{dln\left(y\right)}{dy}=\frac{1}{y}=\frac{1}{tan\left(p+\frac{1}{2}\pi \right)} \left(3\right)\:\frac{dy}{dp}=\frac{dtan\left(p+\frac{1}{2}\pi \right)}{dp}=sec^2\left(p+\frac{1}{2}\pi \right) (1) (2) (3) \frac{du}{dp} \left(c\right)\:\:\:\:\frac{du}{dp}=-\frac{sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{tan\left(p+\frac{1}{2}\pi \:\right)cos^2\left(p+\frac{1}{2}\pi \:\right)} \frac{dv}{dp} \frac{dv}{dp}=\frac{dsin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{dp}=\frac{dv}{dx}\cdot \frac{dx}{dy}\cdot \frac{dy}{dp} \frac{dx}{dy} \frac{dy}{dp} (2) (3) \frac{dv}{dx} v\:=\:sin\left(x\right) \frac{dv}{dx}=cos\left(x\right)=cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right) \frac{dv}{dp} \left(d\right)\:\:\:\:\frac{dv}{dp}=\frac{cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{tan\left(p+\frac{1}{2}\pi \:\right)cos^2\left(p+\frac{1}{2}\pi \:\right)} (b) (c) (d) (a) \frac{dw}{dp}=\frac{cot\left(p+\frac{1}{2}\pi \:\right)\cdot sec^2\left(p+\frac{1}{2}\pi \:\right)}{\sqrt{u^2+v^2}}\left(v\cdot cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\right)-u\cdot sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\:\right)\right)\right)\right),"['calculus', 'derivatives', 'chain-rule']"
32,Finding $\int_{0}^{1} \frac{\log(1+x)}{1+x^2} {\rm d}x$ by differentiating under the integral sign.,Finding  by differentiating under the integral sign.,\int_{0}^{1} \frac{\log(1+x)}{1+x^2} {\rm d}x,"I've tried to find this integral by the method already outlined in the title. I decided to let $$ \displaystyle  I(\alpha) = \int_{0}^{1} \dfrac{\log(1+\alpha x)}{1+x^2} \text{ d}x. $$ From this integral, I differentiated the equality with respect to $\alpha$ and performed a partial fraction decomposition on the resulting integral. $$\begin{aligned} I'(\alpha) \ \ & = \int_{0}^{1} \dfrac{\partial}{\partial \alpha} \left( \dfrac{\log(1+\alpha x)}{1+x^2} \right) \text{ d}x \\ & = \int_{0}^{1} \dfrac{x}{(1+\alpha x)(1+x^2)} \text{ d}x \\ & = \dfrac{-\alpha}{1+\alpha^2} \int_{0}^{1} \dfrac{1}{1+\alpha x} \text{ d}x + \dfrac{1}{1+\alpha^2} \int_{0}^{1} \dfrac{x+\alpha}{1+x^2} \text{ d}x \\ & = \dfrac{-1}{1+\alpha^2} \log \left| 1+\alpha x\right| \Big|_{\ x=0}^{\ x=1} + \dfrac{1}{2(1+\alpha^2)} \log \left( 1+x^2 \right) \Big|_{\ x=0}^{\ x=1} + \dfrac{\alpha}{1+\alpha^2} \arctan x \Big|_{\ x=0}^{\ x=1} \\ & = \dfrac{-\log \left| 1 + \alpha \right|}{1+\alpha^2} + \dfrac{\log(2)}{2(1+\alpha^2)} + \dfrac{\alpha \pi}{4(1+\alpha^2)} \end{aligned} $$ Integrating this equality with respect to $\alpha$, I get something along the lines of. $$\begin{aligned} I(\alpha) \ \ & = - \int \dfrac{-\log \left| 1 + \alpha \right|}{1+\alpha^2} \text{ d}\alpha + \dfrac{\log(2)}{2} \arctan \alpha + \dfrac{\pi}{8} \log(1+\alpha^2) + \mathcal{C} \end{aligned}$$ Where $\mathcal{C}$ is a constant of integration to be found. Now from here on in, I'm unsure of how to perform integration by parts on the integral in the final line, with $u=\log|1+\alpha|$ so that I can finish the integral off. Any help would be greatly appreciated!","I've tried to find this integral by the method already outlined in the title. I decided to let $$ \displaystyle  I(\alpha) = \int_{0}^{1} \dfrac{\log(1+\alpha x)}{1+x^2} \text{ d}x. $$ From this integral, I differentiated the equality with respect to $\alpha$ and performed a partial fraction decomposition on the resulting integral. $$\begin{aligned} I'(\alpha) \ \ & = \int_{0}^{1} \dfrac{\partial}{\partial \alpha} \left( \dfrac{\log(1+\alpha x)}{1+x^2} \right) \text{ d}x \\ & = \int_{0}^{1} \dfrac{x}{(1+\alpha x)(1+x^2)} \text{ d}x \\ & = \dfrac{-\alpha}{1+\alpha^2} \int_{0}^{1} \dfrac{1}{1+\alpha x} \text{ d}x + \dfrac{1}{1+\alpha^2} \int_{0}^{1} \dfrac{x+\alpha}{1+x^2} \text{ d}x \\ & = \dfrac{-1}{1+\alpha^2} \log \left| 1+\alpha x\right| \Big|_{\ x=0}^{\ x=1} + \dfrac{1}{2(1+\alpha^2)} \log \left( 1+x^2 \right) \Big|_{\ x=0}^{\ x=1} + \dfrac{\alpha}{1+\alpha^2} \arctan x \Big|_{\ x=0}^{\ x=1} \\ & = \dfrac{-\log \left| 1 + \alpha \right|}{1+\alpha^2} + \dfrac{\log(2)}{2(1+\alpha^2)} + \dfrac{\alpha \pi}{4(1+\alpha^2)} \end{aligned} $$ Integrating this equality with respect to $\alpha$, I get something along the lines of. $$\begin{aligned} I(\alpha) \ \ & = - \int \dfrac{-\log \left| 1 + \alpha \right|}{1+\alpha^2} \text{ d}\alpha + \dfrac{\log(2)}{2} \arctan \alpha + \dfrac{\pi}{8} \log(1+\alpha^2) + \mathcal{C} \end{aligned}$$ Where $\mathcal{C}$ is a constant of integration to be found. Now from here on in, I'm unsure of how to perform integration by parts on the integral in the final line, with $u=\log|1+\alpha|$ so that I can finish the integral off. Any help would be greatly appreciated!",,"['integration', 'trigonometry', 'derivatives', 'logarithms', 'partial-derivative']"
33,Find arc length of curve on the given interval,Find arc length of curve on the given interval,,"I was asked to find the arc length of the curve of the following curve: $24xy = x^4 + 48$ from $x = 2$ to $x = 4$ This has turned out to be a very difficult problem, I get stuck using the arc length formula with the derivative I have calculated.","I was asked to find the arc length of the curve of the following curve: $24xy = x^4 + 48$ from $x = 2$ to $x = 4$ This has turned out to be a very difficult problem, I get stuck using the arc length formula with the derivative I have calculated.",,"['calculus', 'integration', 'derivatives']"
34,Can all null-homotopy be made differentiable on arbitrary metric space?,Can all null-homotopy be made differentiable on arbitrary metric space?,,"Let $M$ be a metric, and assume that it is simply connected. For a closed curve $f$, we define it to be differentiable iff for any $x$ then $\lim\limits_{h\rightarrow 0}\frac{d(f(x),f(x+h))}{h}$ exist, where we take $f$ to be extended into a periodic function with period $1$ for the sake of having both side when taking the derivative at the boundary. The question is: Given a closed curve $f$ that is differentiable. Does there always exist a null-homotopy of $f$ such that all curves along the homotopy process is differentiable. I believe the answer to be no, but I am still unsure either way. Any helps would be welcome. Thank you.","Let $M$ be a metric, and assume that it is simply connected. For a closed curve $f$, we define it to be differentiable iff for any $x$ then $\lim\limits_{h\rightarrow 0}\frac{d(f(x),f(x+h))}{h}$ exist, where we take $f$ to be extended into a periodic function with period $1$ for the sake of having both side when taking the derivative at the boundary. The question is: Given a closed curve $f$ that is differentiable. Does there always exist a null-homotopy of $f$ such that all curves along the homotopy process is differentiable. I believe the answer to be no, but I am still unsure either way. Any helps would be welcome. Thank you.",,"['derivatives', 'metric-spaces', 'homotopy-theory']"
35,Calculus Implicit Differentiation and Concavity,Calculus Implicit Differentiation and Concavity,,Consider the relation $4x^2 - y^2 = -2$ (a) Use implicit differentiation to calculate $dy/dx$ and find all critical points of the curve. (b) Calculate the second derivative and determine the function's concavity at each critical point. (c) Graph and clearly label the relation using (a) and (b). What type of curve is described by the relation? So this seems like an easy question but there are somethings that confuse me: I did part (a) getting the derivative as $\large\frac{4x}{y}$  and to find the critical points I guess its just at $x = 0$. For part (b): the second derivative I calculated to be $(4y^2 - 16x^2)/y^3$ but I don't know how to determine the functions concavity at the critical point cause when I plug in $x = 0$ I still have another variable... so help me with that please... After doing that I wouldnt mind some help on how to draw the graph :p,Consider the relation $4x^2 - y^2 = -2$ (a) Use implicit differentiation to calculate $dy/dx$ and find all critical points of the curve. (b) Calculate the second derivative and determine the function's concavity at each critical point. (c) Graph and clearly label the relation using (a) and (b). What type of curve is described by the relation? So this seems like an easy question but there are somethings that confuse me: I did part (a) getting the derivative as $\large\frac{4x}{y}$  and to find the critical points I guess its just at $x = 0$. For part (b): the second derivative I calculated to be $(4y^2 - 16x^2)/y^3$ but I don't know how to determine the functions concavity at the critical point cause when I plug in $x = 0$ I still have another variable... so help me with that please... After doing that I wouldnt mind some help on how to draw the graph :p,,"['calculus', 'derivatives', 'implicit-differentiation']"
36,Find the inflection points in the graph,Find the inflection points in the graph,,"The question is which of the $x$-values of the given points are inflection points of the function $f(x)$ itself? I chose $C,F$ and $H$ because at this point the $f'(x)$ is zero. But my answer was wrong. Why ? is it only $C$ & $F$ ?","The question is which of the $x$-values of the given points are inflection points of the function $f(x)$ itself? I chose $C,F$ and $H$ because at this point the $f'(x)$ is zero. But my answer was wrong. Why ? is it only $C$ & $F$ ?",,"['calculus', 'derivatives']"
37,Inverse Function Theroem in $R^1$,Inverse Function Theroem in,R^1,"I have a question about the inverse function theorem in R1. The version of the theorem that I know says: Let $y = f(x)$ be a continuously differentiable function defined on an open interval $I$  in $R$. If $f'(x_0) \neq 0$ at some point $x_0$ in $I$, then there exists a function $f^{-1}(x) $ defined on some neighbourhood $N$ of $f(x_0)$ such that $f(f^{-1}(y)) = y$ for each $y \in N$. I want to relax the hypotheses of the theorem a little bit, so the following questions came to my mind. Instead of $f(x)$ being continuously differentiable on the whole interval $I$, if I only know that $f(x)$ is differentiable at the point $x_0$ and $f'(x_0) \neq 0$, would I still be able to find an inverse function $f^{-1}(x)$? In other words, can I guarantee the existence of an (local) inverse function just from the differentiability of the function at the single point and its derivative there not equal to zero?","I have a question about the inverse function theorem in R1. The version of the theorem that I know says: Let $y = f(x)$ be a continuously differentiable function defined on an open interval $I$  in $R$. If $f'(x_0) \neq 0$ at some point $x_0$ in $I$, then there exists a function $f^{-1}(x) $ defined on some neighbourhood $N$ of $f(x_0)$ such that $f(f^{-1}(y)) = y$ for each $y \in N$. I want to relax the hypotheses of the theorem a little bit, so the following questions came to my mind. Instead of $f(x)$ being continuously differentiable on the whole interval $I$, if I only know that $f(x)$ is differentiable at the point $x_0$ and $f'(x_0) \neq 0$, would I still be able to find an inverse function $f^{-1}(x)$? In other words, can I guarantee the existence of an (local) inverse function just from the differentiability of the function at the single point and its derivative there not equal to zero?",,"['real-analysis', 'derivatives']"
38,Length of Curve $6xy=3+x^4$,Length of Curve,6xy=3+x^4,Question : Find the length of the curve $6xy=3+x^4$ between $x=1$ and $x=2$. Answer = 17/12 I have tried this: I obtain a different answer. Where did I do wrongly? Thank you for your help.,Question : Find the length of the curve $6xy=3+x^4$ between $x=1$ and $x=2$. Answer = 17/12 I have tried this: I obtain a different answer. Where did I do wrongly? Thank you for your help.,,"['calculus', 'derivatives']"
39,Proving that nth derivate of $x e^{-x}$ is $(-1)^n (e^{-x})(x-n)$ by induction.,Proving that nth derivate of  is  by induction.,x e^{-x} (-1)^n (e^{-x})(x-n),I'm quite stuck on this. How would you prove that the $n^{th}$ derivative of $x e^{-x}$ if the $(-1)^n (e^{-x})(x-n)$ by induction? I did: $\frac{d}{dx}(x e^{-x})=(e^{-x}) - x(e^{-x})$ Now I have no idea how to proceed and prove this by induction. I would really appreciate if anyone could guide me. A clear answer would be highly appreciated. Thanks,I'm quite stuck on this. How would you prove that the $n^{th}$ derivative of $x e^{-x}$ if the $(-1)^n (e^{-x})(x-n)$ by induction? I did: $\frac{d}{dx}(x e^{-x})=(e^{-x}) - x(e^{-x})$ Now I have no idea how to proceed and prove this by induction. I would really appreciate if anyone could guide me. A clear answer would be highly appreciated. Thanks,,"['calculus', 'derivatives', 'induction']"
40,Is this a valid proof for eulers formula?,Is this a valid proof for eulers formula?,,"I am wondering whether this proof is a valid proof of Eulers formula: $e^{ix}=i\sin(x)+\cos(x)$ $$\frac{d}{dx}e^{ix} = i(e^{ix})$$ $$\frac{d}{dx}(i\sin(x)+\cos(x)) = i\cos(x)-\sin(x) = i(i\sin(x)+cos(x))$$ Therefore it follows that for both $f(x)=e^{ix}$ and $f(x)=i\sin(x)+\cos(x)$, the following statement is true:  $$\frac{d}{dx}f(x)=i(f(x))$$ And therefore since they both follow this property, they are equivalent. I am very well aware of the actual proof involving taylor series, and my instinct is telling me that there is no justification for that final statement thus making it wrong, so now I am wondering, is there anything that can be done to this to make it a valid proof of Euler's formula?","I am wondering whether this proof is a valid proof of Eulers formula: $e^{ix}=i\sin(x)+\cos(x)$ $$\frac{d}{dx}e^{ix} = i(e^{ix})$$ $$\frac{d}{dx}(i\sin(x)+\cos(x)) = i\cos(x)-\sin(x) = i(i\sin(x)+cos(x))$$ Therefore it follows that for both $f(x)=e^{ix}$ and $f(x)=i\sin(x)+\cos(x)$, the following statement is true:  $$\frac{d}{dx}f(x)=i(f(x))$$ And therefore since they both follow this property, they are equivalent. I am very well aware of the actual proof involving taylor series, and my instinct is telling me that there is no justification for that final statement thus making it wrong, so now I am wondering, is there anything that can be done to this to make it a valid proof of Euler's formula?",,"['derivatives', 'complex-numbers', 'exponential-function']"
41,Finding a tangent to an ellipse parallel to a given line,Finding a tangent to an ellipse parallel to a given line,,"Problem: Find the lines that are tangent to the ellipse $x^2 + 4y^2 = 8$ and parallel to $x +2y = 6$. I tried to find the derivative of $x^2 + 4y^2 = 8$ and I got: $$\frac{dx}{dy} = -\frac{x}{2y}.$$ Not quite sure if it's right, but I tried to equate it with the slope of $x +2y = 6$, which is $-\frac{1}{2}$. So I get, $$-\frac{1}{2} = -\frac{x}{2y}$$  Now, I'm stuck and I don't know what to do. How do I get the tangent lines? I have no knowledge of any point of that tangent line, only the slope. I appreciate any help. Thanks.","Problem: Find the lines that are tangent to the ellipse $x^2 + 4y^2 = 8$ and parallel to $x +2y = 6$. I tried to find the derivative of $x^2 + 4y^2 = 8$ and I got: $$\frac{dx}{dy} = -\frac{x}{2y}.$$ Not quite sure if it's right, but I tried to equate it with the slope of $x +2y = 6$, which is $-\frac{1}{2}$. So I get, $$-\frac{1}{2} = -\frac{x}{2y}$$  Now, I'm stuck and I don't know what to do. How do I get the tangent lines? I have no knowledge of any point of that tangent line, only the slope. I appreciate any help. Thanks.",,"['calculus', 'derivatives']"
42,calculus question attempt,calculus question attempt,,"find the maximum value of the function $$y = 15 \sin x -8 \cos x $$ attempt at  a solution:   deriving: $y' = 15\cos x +8\sin x  $  equating to zero and doubling by $ 1/\cos x$  (Im not sure this is allowed since if $x$ is $\pi/2 $ we might find ourselves in a bit of a trouble)  : $$0 = 15 +8\tan x $$ a few steps forward; $x=-0.344\pi$. by checking near by points we can determine its a maximum... so final solution is $(-0.344\pi,0) max$. I didn't get the answers right though, so any help would be appreciated.","find the maximum value of the function $$y = 15 \sin x -8 \cos x $$ attempt at  a solution:   deriving: $y' = 15\cos x +8\sin x  $  equating to zero and doubling by $ 1/\cos x$  (Im not sure this is allowed since if $x$ is $\pi/2 $ we might find ourselves in a bit of a trouble)  : $$0 = 15 +8\tan x $$ a few steps forward; $x=-0.344\pi$. by checking near by points we can determine its a maximum... so final solution is $(-0.344\pi,0) max$. I didn't get the answers right though, so any help would be appreciated.",,"['calculus', 'derivatives']"
43,Continuous differentiation for polynomials?,Continuous differentiation for polynomials?,,"Has this concept been explored & if so what name does it go by? Taking a simple polynomial & its derivatives: $$y = x^3 + x ^ 2 + x + 1$$ $$\frac{dy}{dx} = 3x^2 + 2x + 1$$ $$\frac{d^2y}{dx^2} = 6x + 2$$ $$\frac{d^3y}{dx^3} = 6$$ At a glance, differentiation seems like a discrete operation. However it looks like it cuold be continuous. Let's say q is the degree of differentiation, and for normal highschool differentiation $q = 1$, and we now want to try differentiating where $0 < q < 1$. The process of differentiating a component in a polynomial becomes something like: $vx^u$ becomes $(v+qu(v-1))x^(u-q)$ When q = 1 we still get the first descrete derivative as $v+1u(v-1) = vu$, while when q = 0 we get the original formula as $v+0u(v-1) = v$, so we have a q representing a continuous variable between the two formulae. Dragging q as a slider on a graph confirms it toggles between the two: https://www.desmos.com/calculator/eqitd8v37x I'm interested to know: What name does this sort of operation go by? Are there smoother or more elegant ways to make make continuous paths between a function and its derivative? Is there a more general way which allows q > 1 to represent higher derivatives? (Factorials?) Are there more general operations allowing us to make continuous paths between other formulae? (Beyond just weighting & adding their outputs)","Has this concept been explored & if so what name does it go by? Taking a simple polynomial & its derivatives: $$y = x^3 + x ^ 2 + x + 1$$ $$\frac{dy}{dx} = 3x^2 + 2x + 1$$ $$\frac{d^2y}{dx^2} = 6x + 2$$ $$\frac{d^3y}{dx^3} = 6$$ At a glance, differentiation seems like a discrete operation. However it looks like it cuold be continuous. Let's say q is the degree of differentiation, and for normal highschool differentiation $q = 1$, and we now want to try differentiating where $0 < q < 1$. The process of differentiating a component in a polynomial becomes something like: $vx^u$ becomes $(v+qu(v-1))x^(u-q)$ When q = 1 we still get the first descrete derivative as $v+1u(v-1) = vu$, while when q = 0 we get the original formula as $v+0u(v-1) = v$, so we have a q representing a continuous variable between the two formulae. Dragging q as a slider on a graph confirms it toggles between the two: https://www.desmos.com/calculator/eqitd8v37x I'm interested to know: What name does this sort of operation go by? Are there smoother or more elegant ways to make make continuous paths between a function and its derivative? Is there a more general way which allows q > 1 to represent higher derivatives? (Factorials?) Are there more general operations allowing us to make continuous paths between other formulae? (Beyond just weighting & adding their outputs)",,"['calculus', 'derivatives', 'discrete-calculus']"
44,Directional derivatives of $f \mapsto \max f$,Directional derivatives of,f \mapsto \max f,"Consider the functional $\Psi \colon C^{0}([0,1]) \to \mathbb R$ defined by  $$ \Psi(f):=\max_{x \in [0,1]} f(x) $$ Find the directional derivative (if it exists) in the generic point $f$ in the generic direction $g$. I should find the limit $$ \frac{\partial \Psi}{\partial g}(f):=\lim_{t \to 0} \frac{\max(f+tg)-\max f}{t} $$ but I do not know how to estimate $\max(f+tg)$. Thanks.","Consider the functional $\Psi \colon C^{0}([0,1]) \to \mathbb R$ defined by  $$ \Psi(f):=\max_{x \in [0,1]} f(x) $$ Find the directional derivative (if it exists) in the generic point $f$ in the generic direction $g$. I should find the limit $$ \frac{\partial \Psi}{\partial g}(f):=\lim_{t \to 0} \frac{\max(f+tg)-\max f}{t} $$ but I do not know how to estimate $\max(f+tg)$. Thanks.",,['derivatives']
45,How do you differentiate a function with respect to the negative of its variable?,How do you differentiate a function with respect to the negative of its variable?,,"How do you differentiate a function with respect to the negative of its variable. For example, is it true that df(-x)/dx = - df(x)/dx? If so, why is it?","How do you differentiate a function with respect to the negative of its variable. For example, is it true that df(-x)/dx = - df(x)/dx? If so, why is it?",,['derivatives']
46,Simple optimization of cylindrical radius for volume,Simple optimization of cylindrical radius for volume,,"I'm having trouble solving this simple optimization problem, can't work out where I'm going wrong. A brewery wants to make a cylindrical aluminium beer can which will hold 375ml. (This means the volume of the can is 375 cm3 ). (Assume that any aluminium used for the joins and tab are not  included in the calculations.) Set up an appropriate mathematical model which can be used to calculate the radius  of the base of the can if the amount of aluminium used in its construction is to be  minimised. So I think: write an equation for the volume (V) and area (A) based on the radius & height: $$375 = V = \pi h r^2$$ $$A = 2\pi r^2 + \pi h r^2$$ Let's use the constant V to simply to a single variable: $$h=\frac{V}{\pi r^2}$$ $$A = 2\pi r^2 + \frac{\pi r^2 V}{\pi r^2}$$ $$A = 2\pi r^2 + V$$ To find the optimal value of r which minimizes A, we take the first derivative and solve for 0: $$\frac{dA}{dr} = 4\pi r$$ $$4\pi r = 0$$ Therefore the optimal value for r to minimize A is: $$r=0$$ This is clearly wrong, where have I messed up??","I'm having trouble solving this simple optimization problem, can't work out where I'm going wrong. A brewery wants to make a cylindrical aluminium beer can which will hold 375ml. (This means the volume of the can is 375 cm3 ). (Assume that any aluminium used for the joins and tab are not  included in the calculations.) Set up an appropriate mathematical model which can be used to calculate the radius  of the base of the can if the amount of aluminium used in its construction is to be  minimised. So I think: write an equation for the volume (V) and area (A) based on the radius & height: $$375 = V = \pi h r^2$$ $$A = 2\pi r^2 + \pi h r^2$$ Let's use the constant V to simply to a single variable: $$h=\frac{V}{\pi r^2}$$ $$A = 2\pi r^2 + \frac{\pi r^2 V}{\pi r^2}$$ $$A = 2\pi r^2 + V$$ To find the optimal value of r which minimizes A, we take the first derivative and solve for 0: $$\frac{dA}{dr} = 4\pi r$$ $$4\pi r = 0$$ Therefore the optimal value for r to minimize A is: $$r=0$$ This is clearly wrong, where have I messed up??",,"['calculus', 'derivatives', 'optimization']"
47,Find the derivative of $F(x) = \int_0^x xf(t) dt$,Find the derivative of,F(x) = \int_0^x xf(t) dt,this was given as an exercise in my first year honours math class. I can't seem to wrap my head around why this is not equal to $xf(t)$. Any help is appreciated! heres the question: Find the derivative of $F(x) = \int \limits_0^x xf(t) dt$,this was given as an exercise in my first year honours math class. I can't seem to wrap my head around why this is not equal to $xf(t)$. Any help is appreciated! heres the question: Find the derivative of $F(x) = \int \limits_0^x xf(t) dt$,,"['calculus', 'real-analysis', 'integration', 'derivatives', 'definite-integrals']"
48,How to derive this second derivative using the quotient rule?,How to derive this second derivative using the quotient rule?,,If a given first derivative is: $\ {dy \over dx} = {-48x \over (x^2+12)^2} $ What are the steps using the quotient rule to derive the second derivative: $\ {d^2y \over dx^2} = {-144(4-x^2) \over (x^2+12)^3} $ My Steps: \begin{align*} {d^2y \over dx^2} &= {-48(x^2 +12)^2 - 2(x^2+12)(2x)(-48x)\over (x^2+12)^4} \\ &=-48{(x^2 +12)^2 - 4x^2(x^2+12)\over (x^2+12)^4} \\ &= -48{(x^2 +12)( - 4x^2 +(x^2+12))\over (x^2+12)^4} \\ &=  -48{(x^2 +12)( - 4x^2)\over (x^2+12)^3} \\ &= {???} \end{align*},If a given first derivative is: $\ {dy \over dx} = {-48x \over (x^2+12)^2} $ What are the steps using the quotient rule to derive the second derivative: $\ {d^2y \over dx^2} = {-144(4-x^2) \over (x^2+12)^3} $ My Steps: \begin{align*} {d^2y \over dx^2} &= {-48(x^2 +12)^2 - 2(x^2+12)(2x)(-48x)\over (x^2+12)^4} \\ &=-48{(x^2 +12)^2 - 4x^2(x^2+12)\over (x^2+12)^4} \\ &= -48{(x^2 +12)( - 4x^2 +(x^2+12))\over (x^2+12)^4} \\ &=  -48{(x^2 +12)( - 4x^2)\over (x^2+12)^3} \\ &= {???} \end{align*},,"['calculus', 'derivatives']"
49,Derivatives of $\frac{\csc x}{e^{-x}}$ and $\ln\left(\frac{3x^2}{\sqrt{3+x^2}}\right)$,Derivatives of  and,\frac{\csc x}{e^{-x}} \ln\left(\frac{3x^2}{\sqrt{3+x^2}}\right),"I have tried to mainly ask thoughtful conceptual questions here, but now I am reduced to asking for help on a specific problem that I have been wrestling with for over an hour. Disclaimer: I am not a lazy student trying to get free internet homework help. I am an adult who is learning Calculus from a textbook.  I am deeply grateful to the members of this community for their time. $$f(x)= \frac{\csc(x)}{e^{-x}}$$ $$f'(x)= ?$$ Answer key says choice (1): $$e^x\csc2x(1-2\cot2x)$$ 2) My answer does not match the answer key.   Is there a typo? Any guidance is enjoyed...it's driving me nuts! $$\frac{d}{dx}\ln\left(\frac{3x^2}{\sqrt{3+x^2}}\right)$$ Answer key says choice (2): $$\frac{x^2+6}{x^3+3x}$$","I have tried to mainly ask thoughtful conceptual questions here, but now I am reduced to asking for help on a specific problem that I have been wrestling with for over an hour. Disclaimer: I am not a lazy student trying to get free internet homework help. I am an adult who is learning Calculus from a textbook.  I am deeply grateful to the members of this community for their time. $$f(x)= \frac{\csc(x)}{e^{-x}}$$ $$f'(x)= ?$$ Answer key says choice (1): $$e^x\csc2x(1-2\cot2x)$$ 2) My answer does not match the answer key.   Is there a typo? Any guidance is enjoyed...it's driving me nuts! $$\frac{d}{dx}\ln\left(\frac{3x^2}{\sqrt{3+x^2}}\right)$$ Answer key says choice (2): $$\frac{x^2+6}{x^3+3x}$$",,"['calculus', 'derivatives']"
50,Finding critical points of $x^{(2/3)}(5-x)$,Finding critical points of,x^{(2/3)}(5-x),"So I tried this out and got stuck with this: $$0 = 3x^{(7/6)} + 2x - 10$$ I didn't think I could use a quadratic for this since its to the power of $7/6$ Here is the working I did: We know its a critical point when f'(a) = 0 So I found the derivative of f(x) which is $$2*(5-x)/3x^{1/2} - x ^{2/3}$$ So I set this equal to 0 $$2*(5-x)/3x^{1/2} - x ^{2/3} = 0$$  $$2*(5-x)/3x^{1/2}=x ^{2/3}$$  $$2*(5-x)=x ^{2/3}\times3x^{1/2}$$  $$10-2x=3x ^{2/3 +1/2}$$  $$10=3x ^{7/6} + 2x$$ But this would be such a messy answer, so I think I have done something wrong with my working. Do you have any ideas?","So I tried this out and got stuck with this: $$0 = 3x^{(7/6)} + 2x - 10$$ I didn't think I could use a quadratic for this since its to the power of $7/6$ Here is the working I did: We know its a critical point when f'(a) = 0 So I found the derivative of f(x) which is $$2*(5-x)/3x^{1/2} - x ^{2/3}$$ So I set this equal to 0 $$2*(5-x)/3x^{1/2} - x ^{2/3} = 0$$  $$2*(5-x)/3x^{1/2}=x ^{2/3}$$  $$2*(5-x)=x ^{2/3}\times3x^{1/2}$$  $$10-2x=3x ^{2/3 +1/2}$$  $$10=3x ^{7/6} + 2x$$ But this would be such a messy answer, so I think I have done something wrong with my working. Do you have any ideas?",,"['calculus', 'derivatives']"
51,Piecewise interpolation with derivatives that is also twice differentiable,Piecewise interpolation with derivatives that is also twice differentiable,,"This question regards the issue of interpolation of one dimension real functions. If one has a finite set of function values and its corresponding derivatives, one could find unique continuous piecewise cubic function that interpolates those points and has the desired derivatives. This function is at least once-differentiable and in the general case is not twice-differentiable as the second derivative is discontinuous. On the other hand a cspline-interpolation (with some boundary conditions) of those same points gives a three twice-differentiable function (but does not respect the values of the derivatives at the points). In other words, the cspline is the only cubic spline that is twice differentiable, which makes me think that if I want a twice-differentiable interpolation with derivatives I will need a higher order interpolation. For example quartic or quintic. The question is, does interpolation with derivatives that is also twice-differentialy exists already? In practice, should it be quartic or quintic? Is there any reference or numerical implementation for it? (I experimented with quartic interpolation --for values only-- in the past, but as it is well-know, even-number order interpolations produce unstable oscillations making it unsuitable for numerical approximation, which discourages me from trying quartic order interpolation) Here it is an illustration of function interpolation of values (on the right) and derivative values (on the left) (in this example the value of the derivative is choosen to be 1 in the sample data for illustration only)","This question regards the issue of interpolation of one dimension real functions. If one has a finite set of function values and its corresponding derivatives, one could find unique continuous piecewise cubic function that interpolates those points and has the desired derivatives. This function is at least once-differentiable and in the general case is not twice-differentiable as the second derivative is discontinuous. On the other hand a cspline-interpolation (with some boundary conditions) of those same points gives a three twice-differentiable function (but does not respect the values of the derivatives at the points). In other words, the cspline is the only cubic spline that is twice differentiable, which makes me think that if I want a twice-differentiable interpolation with derivatives I will need a higher order interpolation. For example quartic or quintic. The question is, does interpolation with derivatives that is also twice-differentialy exists already? In practice, should it be quartic or quintic? Is there any reference or numerical implementation for it? (I experimented with quartic interpolation --for values only-- in the past, but as it is well-know, even-number order interpolations produce unstable oscillations making it unsuitable for numerical approximation, which discourages me from trying quartic order interpolation) Here it is an illustration of function interpolation of values (on the right) and derivative values (on the left) (in this example the value of the derivative is choosen to be 1 in the sample data for illustration only)",,"['derivatives', 'continuity', 'interpolation', 'spline']"
52,Not Riemann integrable derivative,Not Riemann integrable derivative,,"Is there a function $F:[a,b]\to\mathbb{R}$ differentiable with $F' = f$ , but $f$ is not Riemann integrable on $[a,b]$ . $[a,b]$ is a bounded interval? Motivation Rudin page 152 Theorem 7.17: Suppose $\{f_n\}$ a sequence of functions, differentiable on $[a,b]$ and such that $\{f_n(x_0)\}$ converges for some point $x_0\in [a,b]$ . If $\{f_n'\}$ converges uniformly on $[a,b]$ to $g$ , then $\{f_n\}$ converges uniformly on $[a,b]$ to $f$ where $f' = g$ . In the remark, it says if the continuity of the function $f'_n$ is assumed in addition to the above hypothesis, then a much shorter proof can be based on fundamental theorem of calculus and the theorem that $\int_a^b f_n d\alpha\to\int_a^b f d\alpha$ if we have $f_n\to f$ uniformly. I was thinking that we don't really need $f_n'$ to be continuous, we just need it to be Riemann integrable so we can apply Fundamental Thm of Calculus. From the remark I guess that there should be a derivative that is not integrable. _ EDIT Dominic Michaelis answered my question immediately. A further thought yields a more difficult question, is there a function as said above with $f'$ bounded on $[a,b]$ ?","Is there a function differentiable with , but is not Riemann integrable on . is a bounded interval? Motivation Rudin page 152 Theorem 7.17: Suppose a sequence of functions, differentiable on and such that converges for some point . If converges uniformly on to , then converges uniformly on to where . In the remark, it says if the continuity of the function is assumed in addition to the above hypothesis, then a much shorter proof can be based on fundamental theorem of calculus and the theorem that if we have uniformly. I was thinking that we don't really need to be continuous, we just need it to be Riemann integrable so we can apply Fundamental Thm of Calculus. From the remark I guess that there should be a derivative that is not integrable. _ EDIT Dominic Michaelis answered my question immediately. A further thought yields a more difficult question, is there a function as said above with bounded on ?","F:[a,b]\to\mathbb{R} F' = f f [a,b] [a,b] \{f_n\} [a,b] \{f_n(x_0)\} x_0\in [a,b] \{f_n'\} [a,b] g \{f_n\} [a,b] f f' = g f'_n \int_a^b f_n d\alpha\to\int_a^b f d\alpha f_n\to f f_n' f' [a,b]","['integration', 'derivatives']"
53,Differentiating with Euler's constant,Differentiating with Euler's constant,,"Simplified to a very basic problem, is there a standard procedure for these types of differentiations? $$\displaylines{   y = {x^{{e^x}}} \cr    {{d} \over {dx}}\left( y \right) = {{d} \over {dx}}\left( {{x^{{e^x}}}} \right) \cr     = {x^{{e^x}}}\left( {{e^x}\ln \left( x \right) + {{{e^x}} \over x}} \right) \cr} $$","Simplified to a very basic problem, is there a standard procedure for these types of differentiations? $$\displaylines{   y = {x^{{e^x}}} \cr    {{d} \over {dx}}\left( y \right) = {{d} \over {dx}}\left( {{x^{{e^x}}}} \right) \cr     = {x^{{e^x}}}\left( {{e^x}\ln \left( x \right) + {{{e^x}} \over x}} \right) \cr} $$",,"['calculus', 'derivatives']"
54,Find the gradient of a graph at the y axis,Find the gradient of a graph at the y axis,,The question gave the equation: $y^3+2y = \sin x+\cos x-1+3x^{2}$ and then asked to find the gradient of the curve at the points when the graph cuts the y axis. I am not sure how to approach this. Maybe let $x = 0$? If we do then we get $y(y^{2}+2)= 0$ so $y = 0$? Then maybe find the derivative using implicit differentiation but not sure how far that would get us? Thanks.,The question gave the equation: $y^3+2y = \sin x+\cos x-1+3x^{2}$ and then asked to find the gradient of the curve at the points when the graph cuts the y axis. I am not sure how to approach this. Maybe let $x = 0$? If we do then we get $y(y^{2}+2)= 0$ so $y = 0$? Then maybe find the derivative using implicit differentiation but not sure how far that would get us? Thanks.,,"['calculus', 'derivatives']"
55,Min max word problem?,Min max word problem?,,How would I solve the following problem. Problem: A rectangular warehouse will hold 8000 square feet of floor space and will be separated into two rectangular rooms by an interior wall. The cost of the exterior wall is 120 per linear foot and the cost of the interior wall is 100 per linear foot. Find the dimensions that will minimize the cost of building the warehouse. This is what I did $xy=8000$ $y=\frac{8000}{x}$ exterior wall $2(x+y)(120)$ interior wall $(x)(100)$ $C=2(x+y)(120)+100x$ $240(x+y)+100x$ $340x+240(8000)x^{-1}$ $340x+\frac{1920000}{x}$ $c'(x)=340-\frac{1920000}{x^2}$ $\frac{340x^2-1920000}{x^2}$ $340x^2-1920000=0$ $x=75.15$ The minimum cost is approximatly $x=75.15$,How would I solve the following problem. Problem: A rectangular warehouse will hold 8000 square feet of floor space and will be separated into two rectangular rooms by an interior wall. The cost of the exterior wall is 120 per linear foot and the cost of the interior wall is 100 per linear foot. Find the dimensions that will minimize the cost of building the warehouse. This is what I did $xy=8000$ $y=\frac{8000}{x}$ exterior wall $2(x+y)(120)$ interior wall $(x)(100)$ $C=2(x+y)(120)+100x$ $240(x+y)+100x$ $340x+240(8000)x^{-1}$ $340x+\frac{1920000}{x}$ $c'(x)=340-\frac{1920000}{x^2}$ $\frac{340x^2-1920000}{x^2}$ $340x^2-1920000=0$ $x=75.15$ The minimum cost is approximatly $x=75.15$,,"['calculus', 'derivatives']"
56,Derivative of a metric tensor along a curve,Derivative of a metric tensor along a curve,,"Let $M$ be a Riemannian manifold with metric tensor $g$ and Levi-Civita connection $\nabla$. Also, let $u: \mathbb{R}\to T_pM$ be a smooth curve in $T_pM$. In a proof, my course notes assure that $$\frac{d}{dt} \left(g_p(u(t),u(t))\right)=2g_p\left(\frac{d}{dt} u(t),u(t)\right)\text{ (1).}$$ This is giving me a few problems. First of all, $\frac{d}{dt} u(t)$ is an element of $T(T_pM)$, but we identify it with $T_pM$, is that right ? But then, for me, by definition of the Levi-Civita connection, $$\frac{d}{dt} \left(g_p(u(t),u(t))\right)=2g_p\left(\frac{D}{dt}u(t),u(t)\right)\text{ (2)},$$ where $\frac{D}{dt}$ is the covariant derivative, giving here $$\frac{D}{dt} u(t)=\dot u_i(t)\frac{\partial}{\partial x_i}+u_i(t)\nabla_{\dot u(t)}\frac{\partial}{\partial x_i}$$ with $\frac{\partial}{\partial x_i}$ a basis of $T_pM$. Are the two expressions equal (which doesn't seem because of the second terme of the last equation) or does one of them contain a mistake ?","Let $M$ be a Riemannian manifold with metric tensor $g$ and Levi-Civita connection $\nabla$. Also, let $u: \mathbb{R}\to T_pM$ be a smooth curve in $T_pM$. In a proof, my course notes assure that $$\frac{d}{dt} \left(g_p(u(t),u(t))\right)=2g_p\left(\frac{d}{dt} u(t),u(t)\right)\text{ (1).}$$ This is giving me a few problems. First of all, $\frac{d}{dt} u(t)$ is an element of $T(T_pM)$, but we identify it with $T_pM$, is that right ? But then, for me, by definition of the Levi-Civita connection, $$\frac{d}{dt} \left(g_p(u(t),u(t))\right)=2g_p\left(\frac{D}{dt}u(t),u(t)\right)\text{ (2)},$$ where $\frac{D}{dt}$ is the covariant derivative, giving here $$\frac{D}{dt} u(t)=\dot u_i(t)\frac{\partial}{\partial x_i}+u_i(t)\nabla_{\dot u(t)}\frac{\partial}{\partial x_i}$$ with $\frac{\partial}{\partial x_i}$ a basis of $T_pM$. Are the two expressions equal (which doesn't seem because of the second terme of the last equation) or does one of them contain a mistake ?",,"['derivatives', 'riemannian-geometry']"
57,Finding coefficients of quadratic given one tangent and point on the curve,Finding coefficients of quadratic given one tangent and point on the curve,,"I am given a quadratic equation:  $$ y = Ax^2 + Bx + C  $$ that passes through $(1,3)$ and $(2,3)$, and a tangent to the curve is $x - y + 1 = 0$ at $(2,3)$. How do I find $A$, $B$, and $C$? The derivative of $\frac{\mathrm dy}{\mathrm dx} = 2AX + B$, so at $x=2$, the slope of the tangent is $4A + B$, and from the givens we know that $4A + B = 1$. We also know that  $$ 3 = A + B + C,\qquad 3 = 4A + 2B + C. $$ From there, how does one find $A$, $B$, and $C$? (I can't seem to get the answers that make any sense from here).","I am given a quadratic equation:  $$ y = Ax^2 + Bx + C  $$ that passes through $(1,3)$ and $(2,3)$, and a tangent to the curve is $x - y + 1 = 0$ at $(2,3)$. How do I find $A$, $B$, and $C$? The derivative of $\frac{\mathrm dy}{\mathrm dx} = 2AX + B$, so at $x=2$, the slope of the tangent is $4A + B$, and from the givens we know that $4A + B = 1$. We also know that  $$ 3 = A + B + C,\qquad 3 = 4A + 2B + C. $$ From there, how does one find $A$, $B$, and $C$? (I can't seem to get the answers that make any sense from here).",,"['calculus', 'derivatives', 'quadratics']"
58,Derivative of $\frac{1}{\sqrt{x+5}}$,Derivative of,\frac{1}{\sqrt{x+5}},"I'm trying to find the derivative of $\dfrac{1}{\sqrt{x+5}}$   using $\displaystyle \lim_{h\to 0} \frac {f(x+h)-f(x)}{h}$ So, $$\begin{align*} \lim_{h\to 0} \frac{\dfrac{1}{\sqrt{x+h+5}}-\dfrac{1}{\sqrt{x+5}}}{h} &= \frac{\dfrac{\sqrt{x+5}-\sqrt{x+h+5}}{(\sqrt{x+h+5})(\sqrt{x+5})}}{h}\\\\ &= \frac{\dfrac{x+5-x-h-5}{(\sqrt{x+h+5})(\sqrt{x+5})}}{\dfrac{h}{\sqrt{x+5}}+\dfrac{h}{\sqrt{x+h+5}}} \end{align*}$$ I do not know if this is correct or not.. please help. I'm stuck.","I'm trying to find the derivative of $\dfrac{1}{\sqrt{x+5}}$   using $\displaystyle \lim_{h\to 0} \frac {f(x+h)-f(x)}{h}$ So, $$\begin{align*} \lim_{h\to 0} \frac{\dfrac{1}{\sqrt{x+h+5}}-\dfrac{1}{\sqrt{x+5}}}{h} &= \frac{\dfrac{\sqrt{x+5}-\sqrt{x+h+5}}{(\sqrt{x+h+5})(\sqrt{x+5})}}{h}\\\\ &= \frac{\dfrac{x+5-x-h-5}{(\sqrt{x+h+5})(\sqrt{x+5})}}{\dfrac{h}{\sqrt{x+5}}+\dfrac{h}{\sqrt{x+h+5}}} \end{align*}$$ I do not know if this is correct or not.. please help. I'm stuck.",,"['calculus', 'derivatives']"
59,Calculus the n'th derivative of $y_{n}=x^{n-1}e^{1/x}$,Calculus the n'th derivative of,y_{n}=x^{n-1}e^{1/x},"Without using Mathematical Induction to calculus the n'th derivative of the following function. $y_{n}=x^{n-1}e^{1/x}$ , $n\in\mathbb{N}$ Find : $\frac{d^n}{dx^n}y_n$ I tried to finish the question from the given answer : $ y^{(n)}_{n}=\frac{(-1)^{n}e^{1/x}}{x^{n+1}} $ So, I want to know other method(s) to get the answer. Thanks for your help!! :)","Without using Mathematical Induction to calculus the n'th derivative of the following function. $y_{n}=x^{n-1}e^{1/x}$ , $n\in\mathbb{N}$ Find : $\frac{d^n}{dx^n}y_n$ I tried to finish the question from the given answer : $ y^{(n)}_{n}=\frac{(-1)^{n}e^{1/x}}{x^{n+1}} $ So, I want to know other method(s) to get the answer. Thanks for your help!! :)",,"['calculus', 'derivatives', 'induction']"
60,Derivative of a function defined by the divided difference of another function.,Derivative of a function defined by the divided difference of another function.,,"Given a function $f$ of class $C$ $^{n+2}$ in an interval $[a,b]$ and $x_{0}=a<x_1<x_2 ... <x_n = b$ a subdivision of $[a,b]$ into $n+1$ points. Given another function $g$ defined in the same interval $[a,b]$ by the divided difference such that $g(x) = f[x_0, x_1, ... , x_n, x]$. Prove that $g'(x)=f[x_0, x_1, ... , x_n, x, x]$.","Given a function $f$ of class $C$ $^{n+2}$ in an interval $[a,b]$ and $x_{0}=a<x_1<x_2 ... <x_n = b$ a subdivision of $[a,b]$ into $n+1$ points. Given another function $g$ defined in the same interval $[a,b]$ by the divided difference such that $g(x) = f[x_0, x_1, ... , x_n, x]$. Prove that $g'(x)=f[x_0, x_1, ... , x_n, x, x]$.",,"['calculus', 'numerical-methods']"
61,Strengthening a result on the growth of continuously differentiable functions.,Strengthening a result on the growth of continuously differentiable functions.,,"Given two continuously differentiable functions $f,\  g:\mathbb{R}\rightarrow \mathbb{R}$ such that $f(x_0) = g(x_0)$ and    $f'(x_0) < g'(x_0)$, there exists $\epsilon > 0$ such that $f(x) <  g(x)$ for $x \in (x_0, x_0 + \epsilon)$. The above result is not too difficult to prove, but I was wondering if the condition that the functions be continuously differentiable is absolutely necessary. I have not taken much analysis, but I'm wondering if the fact that the derivative exists at $x_0$ would be enough to prove this result without needing the derivative to be continuous. The reason I ask this is because it seems that the existence of a derivative function already implies it must satisfy some quite stringent requirements (i.e. Darboux's Theorem), perhaps these are enough?","Given two continuously differentiable functions $f,\  g:\mathbb{R}\rightarrow \mathbb{R}$ such that $f(x_0) = g(x_0)$ and    $f'(x_0) < g'(x_0)$, there exists $\epsilon > 0$ such that $f(x) <  g(x)$ for $x \in (x_0, x_0 + \epsilon)$. The above result is not too difficult to prove, but I was wondering if the condition that the functions be continuously differentiable is absolutely necessary. I have not taken much analysis, but I'm wondering if the fact that the derivative exists at $x_0$ would be enough to prove this result without needing the derivative to be continuous. The reason I ask this is because it seems that the existence of a derivative function already implies it must satisfy some quite stringent requirements (i.e. Darboux's Theorem), perhaps these are enough?",,"['calculus', 'real-analysis', 'derivatives']"
62,Product Chain Power Rule： Either it's the book or I am wrong.,Product Chain Power Rule： Either it's the book or I am wrong.,,"The problem:　$x^3\sqrt{2x+4}$ $f(x):= x^3$,  $g(x):= \sqrt{2x+4}$ $(f\times g)' = f^{\prime}g+fg^{\prime}$  thus it should be $3x^2\sqrt{2x+4} + (x^3)[\frac{1}{2}(2x+4)^{\frac{-1}{2}}(2)]$ which is $3x^2\sqrt{2x+4}+\frac{x^3}{\sqrt{2x+4}}$ The book gives: $3x^2\sqrt{2x+4}+\frac{x^3}{2\sqrt{2x+4}}$ I'm correct? I always get worry when my answers don't match the book. $\frac{d}{dx}[f\times g(h(x))] = f^{\prime} \times g(h(x))+ f\times g^{\prime}(h(x))h^{\prime}$ right?","The problem:　$x^3\sqrt{2x+4}$ $f(x):= x^3$,  $g(x):= \sqrt{2x+4}$ $(f\times g)' = f^{\prime}g+fg^{\prime}$  thus it should be $3x^2\sqrt{2x+4} + (x^3)[\frac{1}{2}(2x+4)^{\frac{-1}{2}}(2)]$ which is $3x^2\sqrt{2x+4}+\frac{x^3}{\sqrt{2x+4}}$ The book gives: $3x^2\sqrt{2x+4}+\frac{x^3}{2\sqrt{2x+4}}$ I'm correct? I always get worry when my answers don't match the book. $\frac{d}{dx}[f\times g(h(x))] = f^{\prime} \times g(h(x))+ f\times g^{\prime}(h(x))h^{\prime}$ right?",,['derivatives']
63,how calculate integral $\int \dot{x} \; dx$,how calculate integral,\int \dot{x} \; dx,Let $x$ depend on $t$. $\dot{x}$ is derivative $x$ over $t$. I want to calculate the integral $\int \dot{x} \; dx$. I asked similar question about differentiation here . Any thoughts and ideas are appreciated. Thank you!,Let $x$ depend on $t$. $\dot{x}$ is derivative $x$ over $t$. I want to calculate the integral $\int \dot{x} \; dx$. I asked similar question about differentiation here . Any thoughts and ideas are appreciated. Thank you!,,"['integration', 'derivatives']"
64,The $\omega$ function that proves that a function is differentiable at a point $a$,The  function that proves that a function is differentiable at a point,\omega a,"I'll start by listing a definition: ""The function $f$ is differentiable in a $\in$ $D^0$ if there is a number $A_a \in R$ and a function $\omega_a$ continuous and null in $a$ so that, for any $x \in D$ we have: $$f(x)-f(a)=A_a(x-a)+\omega_a(x)(x-a)$$ where $\lim_{a\to\infty}\omega_a(x)=\omega_a(a)=0$ Immediately we see that $A_a=f'(a)$"" Now, I've found an example but I don't see very well how it's related to the formula above. The problem asks to prove that for a=1 $f$ is not differentiable in $(0,0)$ and for $a>1 \ f$ is differentiable in the origin. $$f(x,y)=\frac{x^2y}{\sqrt{x^2+y^2}}, \ x^2+y^2\not=0$$ $$f(x,y)=0, \ x^2+y^2=0$$ $$f(x,y)=\frac{\partial f}{\partial x}(0,0)(x-0)+\frac{\partial f}{\partial y}(0,0)(y-0)+\omega(x,y)\sqrt{x^2+y^2}$$ I guess the last row applies the formula listed in the definition (right?). But in the last part in the definition formula says $\omega_a(x)(x-a)$, shouldn't then be? $$f(x,y)=\frac{\partial f}{\partial x}(0,0)(x-0)+\frac{\partial f}{\partial y}(0,0)(y-0)+\omega(x,y)(x-0)$$","I'll start by listing a definition: ""The function $f$ is differentiable in a $\in$ $D^0$ if there is a number $A_a \in R$ and a function $\omega_a$ continuous and null in $a$ so that, for any $x \in D$ we have: $$f(x)-f(a)=A_a(x-a)+\omega_a(x)(x-a)$$ where $\lim_{a\to\infty}\omega_a(x)=\omega_a(a)=0$ Immediately we see that $A_a=f'(a)$"" Now, I've found an example but I don't see very well how it's related to the formula above. The problem asks to prove that for a=1 $f$ is not differentiable in $(0,0)$ and for $a>1 \ f$ is differentiable in the origin. $$f(x,y)=\frac{x^2y}{\sqrt{x^2+y^2}}, \ x^2+y^2\not=0$$ $$f(x,y)=0, \ x^2+y^2=0$$ $$f(x,y)=\frac{\partial f}{\partial x}(0,0)(x-0)+\frac{\partial f}{\partial y}(0,0)(y-0)+\omega(x,y)\sqrt{x^2+y^2}$$ I guess the last row applies the formula listed in the definition (right?). But in the last part in the definition formula says $\omega_a(x)(x-a)$, shouldn't then be? $$f(x,y)=\frac{\partial f}{\partial x}(0,0)(x-0)+\frac{\partial f}{\partial y}(0,0)(y-0)+\omega(x,y)(x-0)$$",,"['calculus', 'derivatives']"
65,Geometric interpretation of implicit differentiation,Geometric interpretation of implicit differentiation,,"It is well known that, given a function $f:\mathbb{R} \to \mathbb{R} $ , $f'(x_0)$ can be interpreted as the slope of the tangent line to $f$ in $x_0$ . What about curves of the form $F(x, y, c)=0$ , which cannot be written in the form $y=f(x)$ , like the circumference $x^2+y^2-c^2=0$ ? I know that in this example, to find the tangent in the point $(x_0, y_0)$ , one may distinguish the two functions $y=\sqrt{c^2-x^2}$ and $y=-\sqrt{c^2-x^2}$ and then evaluate the derivative in $x_0$ (now ignoring $y_0$ , since those are functions) , but this procedure doesn't really convince me, as in general one should actually know how to ""split up"" the curve into two (or, I guess, more) functions. Thus, I have asked my Professor for a general working rule and he answered to use implicit differentiation. For example, suppose we want to find the slope of the tangent to $x^2+y^2-25=0$ in the point $(3,-4)$ . Then, differentiating both sides we get $$2x+2y \frac{dy}{dx}=0 \implies \frac{dy}{dx}=-\frac{x}{y}$$ Hence, the slope in $(3,-4)$ is simply $-\frac{3}{(-4)} = \frac{3}{4}$ . This is indeed the correct answer (in fact, the same I would get if I computed the derivative of $-\sqrt{25-x^2}$ and evaluated it in $x=3$ ). However, I am very confused about the meaning of $\frac{dy}{dx}$ here, since again this is not a function, and I do not understand the geometric interpretation of implicit differentiation either (that is, how/why this trick works). In the same way, my Professor showed that for every $(a,b) \in \mathbb{R^2}$ the curves $$x^2-y^2=a$$ and $$xy=b$$ are orthogonal, since $$x^2-y^2-a=0 \implies \frac{dy}{dx}=\frac{x}{y}$$ and since $$xy-b=0 \implies \frac{dy}{dx}=-\frac{y}{x}$$ Could you please clarify what I have asked before? Thanks in advance for your precious time and kindness.","It is well known that, given a function , can be interpreted as the slope of the tangent line to in . What about curves of the form , which cannot be written in the form , like the circumference ? I know that in this example, to find the tangent in the point , one may distinguish the two functions and and then evaluate the derivative in (now ignoring , since those are functions) , but this procedure doesn't really convince me, as in general one should actually know how to ""split up"" the curve into two (or, I guess, more) functions. Thus, I have asked my Professor for a general working rule and he answered to use implicit differentiation. For example, suppose we want to find the slope of the tangent to in the point . Then, differentiating both sides we get Hence, the slope in is simply . This is indeed the correct answer (in fact, the same I would get if I computed the derivative of and evaluated it in ). However, I am very confused about the meaning of here, since again this is not a function, and I do not understand the geometric interpretation of implicit differentiation either (that is, how/why this trick works). In the same way, my Professor showed that for every the curves and are orthogonal, since and since Could you please clarify what I have asked before? Thanks in advance for your precious time and kindness.","f:\mathbb{R} \to \mathbb{R}  f'(x_0) f x_0 F(x, y, c)=0 y=f(x) x^2+y^2-c^2=0 (x_0, y_0) y=\sqrt{c^2-x^2} y=-\sqrt{c^2-x^2} x_0 y_0 x^2+y^2-25=0 (3,-4) 2x+2y \frac{dy}{dx}=0 \implies \frac{dy}{dx}=-\frac{x}{y} (3,-4) -\frac{3}{(-4)} = \frac{3}{4} -\sqrt{25-x^2} x=3 \frac{dy}{dx} (a,b) \in \mathbb{R^2} x^2-y^2=a xy=b x^2-y^2-a=0 \implies \frac{dy}{dx}=\frac{x}{y} xy-b=0 \implies \frac{dy}{dx}=-\frac{y}{x}","['real-analysis', 'derivatives', 'implicit-differentiation', 'geometric-interpretation']"
66,Taking derivative to determine differentiability,Taking derivative to determine differentiability,,"The ""correct"" way to determine differentiability is to use the definition of the derivative as a limit. However, sometimes we have a function with a piecewise definition. Say for example (just an example, the question itself is more general) $f(x)=\begin{cases}  e^x & x\leq 0\\ {\sin x} & x>0 \end{cases}$ A ""shortcut"" is to take the derivative of both pieces of the function at the ""breaking point"" $x=c$ and take the right hand and left hand limits of the derivatives as x approaches c. However, their equality does not always imply that the function is differentiable. In the above case this would not work: the limits of the derivatives of both parts are 1, but the function itself is not differentiable at $x=0$ because it is not continuous at the point. So it is only differentiable in the neighbourhood of x=0 but not at 0 itself. However, if the function above were continuous, say we had $(\sin x)+1$ instead, it would be differentiable at x=0. It seems that continuity of f at x=0 was the ""missing piece"" to making it differentiable. So my question is, do the equality of the limits of the derivatives of $f$ from both sides of $x=c$ , AND the continuity of $f$ at $c$ , together imply that the function is differentiable at x=c? If yes can we prove it?","The ""correct"" way to determine differentiability is to use the definition of the derivative as a limit. However, sometimes we have a function with a piecewise definition. Say for example (just an example, the question itself is more general) A ""shortcut"" is to take the derivative of both pieces of the function at the ""breaking point"" and take the right hand and left hand limits of the derivatives as x approaches c. However, their equality does not always imply that the function is differentiable. In the above case this would not work: the limits of the derivatives of both parts are 1, but the function itself is not differentiable at because it is not continuous at the point. So it is only differentiable in the neighbourhood of x=0 but not at 0 itself. However, if the function above were continuous, say we had instead, it would be differentiable at x=0. It seems that continuity of f at x=0 was the ""missing piece"" to making it differentiable. So my question is, do the equality of the limits of the derivatives of from both sides of , AND the continuity of at , together imply that the function is differentiable at x=c? If yes can we prove it?","f(x)=\begin{cases} 
e^x & x\leq 0\\
{\sin x} & x>0 \end{cases} x=c x=0 (\sin x)+1 f x=c f c","['calculus', 'derivatives']"
67,Second derivatives of 1/r,Second derivatives of 1/r,,"Let $r = \sqrt{x^2 + y^2 + z^2}$ . From the fact that $\nabla^2 r^{-1} = -4\pi \delta^{(3)}(\vec{r})$ , is it correct to say that $$ \frac{\partial^2}{\partial x^2}(r^{-1}) = \frac{3x^2 - r^2}{r^5} - \frac{4\pi}{3} \delta^{(3)}(\vec{r}) \\ \frac{\partial^2}{\partial x \partial y}(r^{-1}) = \frac{3xy}{r^5} $$ The question is, is it justified to split the Dirac delta evenly in all three directions? This seems like the most straightforward way. $$ \delta = \frac{\delta}{3} + \frac{\delta}{3} + \frac{\delta}{3} $$ Or maybe there are other ways to distribute, while still respecting symmetry, like $$ \delta = \frac{x^2 \delta}{r^2} + \frac{y^2 \delta}{r^2} + \frac{z^2 \delta}{r^2} $$","Let . From the fact that , is it correct to say that The question is, is it justified to split the Dirac delta evenly in all three directions? This seems like the most straightforward way. Or maybe there are other ways to distribute, while still respecting symmetry, like","r = \sqrt{x^2 + y^2 + z^2} \nabla^2 r^{-1} = -4\pi \delta^{(3)}(\vec{r}) 
\frac{\partial^2}{\partial x^2}(r^{-1}) = \frac{3x^2 - r^2}{r^5} - \frac{4\pi}{3} \delta^{(3)}(\vec{r}) \\
\frac{\partial^2}{\partial x \partial y}(r^{-1}) = \frac{3xy}{r^5}
 
\delta = \frac{\delta}{3} + \frac{\delta}{3} + \frac{\delta}{3}
 
\delta = \frac{x^2 \delta}{r^2} + \frac{y^2 \delta}{r^2} + \frac{z^2 \delta}{r^2}
","['derivatives', 'dirac-delta']"
68,Right Differentiability of $\|f(x)\|$,Right Differentiability of,\|f(x)\|,"Problem: Let $E$ be a normed space, and $f:[a,b]\to E$ be continuous. Define $g:[a,b]\to\mathbb R$ by $g(x)=\|f(x)\|$ . Prove that if $f'_{+}(t_0)$ exists for some $t\in[a,b)$ , then so does $g'_{+}(t_0)$ , and moreover $$|g'_{+}(t_0)|\leq \|f'_{+}(t_0)\|.$$ What I have done: I couldn't prove the existence of $g'_{+}(x)$ , however assume it exists I can prove the inequality as follows: For $h>0$ , we have \begin{align*} g(t_0+h)-g(t_0) &=\|f(t_0+h)-f(t_0)+f(t_0)\|-\|f(t_0)\|\\ &\leq \|f(t_0+h)-f(t_0)\| \end{align*} Since $\|\cdot\|$ is continuous, and by assumption $g'_{+}(t_0)$ exists, we obtain that $g'_+(t_0)\leq \|f'_+(t_0)\|.$ Similar argument yields $-g'_+(t_0)\leq \|f'_+(t_0)\|$ . Thus we obtain the desired inequality. Need Help: How to prove $g'_+(t_0)$ exist?","Problem: Let be a normed space, and be continuous. Define by . Prove that if exists for some , then so does , and moreover What I have done: I couldn't prove the existence of , however assume it exists I can prove the inequality as follows: For , we have Since is continuous, and by assumption exists, we obtain that Similar argument yields . Thus we obtain the desired inequality. Need Help: How to prove exist?","E f:[a,b]\to E g:[a,b]\to\mathbb R g(x)=\|f(x)\| f'_{+}(t_0) t\in[a,b) g'_{+}(t_0) |g'_{+}(t_0)|\leq \|f'_{+}(t_0)\|. g'_{+}(x) h>0 \begin{align*}
g(t_0+h)-g(t_0)
&=\|f(t_0+h)-f(t_0)+f(t_0)\|-\|f(t_0)\|\\
&\leq \|f(t_0+h)-f(t_0)\|
\end{align*} \|\cdot\| g'_{+}(t_0) g'_+(t_0)\leq \|f'_+(t_0)\|. -g'_+(t_0)\leq \|f'_+(t_0)\| g'_+(t_0)","['calculus', 'derivatives', 'differential']"
69,$2\csc^2(1-2x) = 2\cot(1-2x)\csc(1-2x)$ when using two different methods on same derivative.,when using two different methods on same derivative.,2\csc^2(1-2x) = 2\cot(1-2x)\csc(1-2x),"I was playing around with the following derivative: $$ \frac{d}{dx} \left(\frac{1}{\sin(1-2x)}\right) $$ The way our teacher approached it was to transform the sine into a cosecant: $$ \begin{align}  \frac{d}{dx}( \csc(1-2x))   &= -\csc^2(1-2x)\frac{d}{dx} (1-2x) \\\\  &= 2\csc^2(1-2x)  \end{align} $$ However, I also tried combining the chain and power rules: $$ \begin{align} \frac{d}{dx} \left(\frac{1}{\sin(1-2x)}\right)   &= \frac{-1}{\sin^2(1-2x)}\frac{d}{dx}(\sin(1-2x)) \\\\  &= \frac{-1}{\sin^2(1-2x)}(\cos(1-2x))\frac{d}{dx}(1-2x) \\\\  &= \frac{2\cos(1-2x)}{\sin^2(1-2x)} \\\\  &= 2\cot(1-2x)\csc(1-2x)  \end{align}$$ I also tried graphing them using desmos (blue, green and purple overlap): I want to know if both methods were correct, and if so, why are the graphs different and is there another way to demonstrate that these two terms are equivalent.","I was playing around with the following derivative: The way our teacher approached it was to transform the sine into a cosecant: However, I also tried combining the chain and power rules: I also tried graphing them using desmos (blue, green and purple overlap): I want to know if both methods were correct, and if so, why are the graphs different and is there another way to demonstrate that these two terms are equivalent."," \frac{d}{dx} \left(\frac{1}{\sin(1-2x)}\right)   \begin{align} 
\frac{d}{dx}( \csc(1-2x)) 
 &= -\csc^2(1-2x)\frac{d}{dx} (1-2x) \\\\
 &= 2\csc^2(1-2x) 
\end{align}   \begin{align}
\frac{d}{dx} \left(\frac{1}{\sin(1-2x)}\right) 
 &= \frac{-1}{\sin^2(1-2x)}\frac{d}{dx}(\sin(1-2x)) \\\\
 &= \frac{-1}{\sin^2(1-2x)}(\cos(1-2x))\frac{d}{dx}(1-2x) \\\\
 &= \frac{2\cos(1-2x)}{\sin^2(1-2x)} \\\\
 &= 2\cot(1-2x)\csc(1-2x) 
\end{align}","['derivatives', 'trigonometry']"
70,Differentiation. Why does differentiating a function allows us to find a tangent of a point of that specific function?,Differentiation. Why does differentiating a function allows us to find a tangent of a point of that specific function?,,I know that the purpose of differentiation is to find the rate of change of a function. But i don't really understand how this is concept can be used to find the tangent of a specific point. It would be greatly appreciated if you can explain this to me.,I know that the purpose of differentiation is to find the rate of change of a function. But i don't really understand how this is concept can be used to find the tangent of a specific point. It would be greatly appreciated if you can explain this to me.,,"['calculus', 'derivatives']"
71,Find $x$ for which $\int_1^2 \frac{e^{-t}}{1+xt} dt$ is continuous and differentiable.,Find  for which  is continuous and differentiable.,x \int_1^2 \frac{e^{-t}}{1+xt} dt,"Find $x \in \mathbb{R}$ for which $$f(x)=\int_1^2 \frac{e^{-t}}{1+xt} dt$$ is continuous and differentiable. And if possible, find this function and the derivative of this function in the closed form. For $x \ge 0$ , we have $\frac{e^{-t}}{1+xt} \le e^{-t}$ and $\int_1^2 e^{-t} dt$ converges. So $f(x)$ is continuous on $[0, \infty)$ . In the case of the derivative, $$\frac{\partial}{\partial{x}}\frac{e^{-t}}{1+xt}=-\frac{te^{-t}}{(1+xt)^2}$$ and again, for $x \ge 0$ , $$\Biggr|-\frac{te^{-t}}{(1+xt)^2}\Biggl| \le te^{-t}$$ $$\int_1^2 te^{-t} dt = \frac{2}{e} - \frac{3}{e^2}$$ So $$f'(x)= \int_1^2 -\frac{t e^{-t}}{(1+xt)^2} dt$$ is differentiable on $x \in [0, \infty)$ . Also, if $x \in [-1, -\frac{1}{2}]$ , then $$\lim_{t \to -\frac{1}{x}} \frac{e^{-t}}{1+xt} = \text{NOT defined}.$$ So $f(x)$ cannot be defined for $x \in [-1, -\frac{1}{2}]$ . But I'm not sure whether this interval is only exception or not. Here are my questions regarding the above function. I'm not sure how to deal with $x <0$ , is this function continous and differentiable for all $x \in \mathbb{R} \setminus [-1, -\frac{1}{2}]$ ? Also, I'd like to calculate the function or the derivative of the function in closed forms, if possible. Thank you, in advance.","Find for which is continuous and differentiable. And if possible, find this function and the derivative of this function in the closed form. For , we have and converges. So is continuous on . In the case of the derivative, and again, for , So is differentiable on . Also, if , then So cannot be defined for . But I'm not sure whether this interval is only exception or not. Here are my questions regarding the above function. I'm not sure how to deal with , is this function continous and differentiable for all ? Also, I'd like to calculate the function or the derivative of the function in closed forms, if possible. Thank you, in advance.","x \in \mathbb{R} f(x)=\int_1^2 \frac{e^{-t}}{1+xt} dt x \ge 0 \frac{e^{-t}}{1+xt} \le e^{-t} \int_1^2 e^{-t} dt f(x) [0, \infty) \frac{\partial}{\partial{x}}\frac{e^{-t}}{1+xt}=-\frac{te^{-t}}{(1+xt)^2} x \ge 0 \Biggr|-\frac{te^{-t}}{(1+xt)^2}\Biggl| \le te^{-t} \int_1^2 te^{-t} dt = \frac{2}{e} - \frac{3}{e^2} f'(x)= \int_1^2 -\frac{t e^{-t}}{(1+xt)^2} dt x \in [0, \infty) x \in [-1, -\frac{1}{2}] \lim_{t \to -\frac{1}{x}} \frac{e^{-t}}{1+xt} = \text{NOT defined}. f(x) x \in [-1, -\frac{1}{2}] x <0 x \in \mathbb{R} \setminus [-1, -\frac{1}{2}]","['real-analysis', 'derivatives', 'continuity']"
72,Question on Increasing function,Question on Increasing function,,"Let $ f(x) = \frac{x^5}{5} + \frac{x^4}{4} + x^3 + \frac{kx^2}{2} + x$ be a real valued function. Then the greatest value of $k^2$ for which $f(x)$ is an increasing function $∀$ $x \in \mathbb{R}$ My approach was that : $f'(x) = x^4+x^3+3x^2+kx+1$ $f''(x) = 4x^3+3x^2+6x+k$ $f'''(x) = 12x^2+6x+6 = 6(2x^2+x+1)$ ie, $f'''(x)$ has no real roots, and is always greater than zero So, 1. the cubic $f''(x)$ , will have only one root (ie. $f'(x)$ will have a minima or a maxima) $f'(x)$ will always be greater than zero as we want the function to be increasing Net, we can say that the graph of the $f'(x)$ will be upward opening, and will touch touch the $x-$ axis only at 1 point or always stay above it with one minima. Can you please carry forward this method, or give some other solution, if possible, please try to refrain from giving solution completely based on graph making websites?","Let be a real valued function. Then the greatest value of for which is an increasing function My approach was that : ie, has no real roots, and is always greater than zero So, 1. the cubic , will have only one root (ie. will have a minima or a maxima) will always be greater than zero as we want the function to be increasing Net, we can say that the graph of the will be upward opening, and will touch touch the axis only at 1 point or always stay above it with one minima. Can you please carry forward this method, or give some other solution, if possible, please try to refrain from giving solution completely based on graph making websites?", f(x) = \frac{x^5}{5} + \frac{x^4}{4} + x^3 + \frac{kx^2}{2} + x k^2 f(x) ∀ x \in \mathbb{R} f'(x) = x^4+x^3+3x^2+kx+1 f''(x) = 4x^3+3x^2+6x+k f'''(x) = 12x^2+6x+6 = 6(2x^2+x+1) f'''(x) f''(x) f'(x) f'(x) f'(x) x-,"['calculus', 'derivatives', 'monotone-functions']"
73,"How to get the derivative of $\log_4 x$ using the change of base, but without assuming we have the derivative of $\ln x$?","How to get the derivative of  using the change of base, but without assuming we have the derivative of ?",\log_4 x \ln x,"I can compute the derivative of $\log_4 x$ using implicit differentiation.  I can also compute it by writing it as $\log_4 x = (\ln x)/(\ln 4)$ and using the fact that the derivative of $\ln x$ is $1/x$ .  But I'm investigating why I can't seem to get it in the following approach, however bad an approach it might be. Let $y = \log_4 x = \frac{\ln x}{\ln 4}$ .  Now I raise $e$ to the equation, getting $$\exp(y) = \exp\left(\frac{\ln x}{\ln 4}\right).$$ Using implicit differentiation, I get \begin{align*} \exp(y)y' &= \exp\left(\frac{\ln x}{\ln 4}\right) \left(\frac{\ln x}{\ln 4}\right)' = \exp\left(y\right) y'\\ \exp(y)y' - \exp(y)y' &= 0\\ (\exp(y) - \exp(y))y' &= 0 \end{align*} In other words, it leads me nowhere.  Is it possible at all do this with the conditions imposed in the question?  It seems that if the exercise asks me to use a change of base, then I must really use the derivative of $\ln x$ to solve the problem.","I can compute the derivative of using implicit differentiation.  I can also compute it by writing it as and using the fact that the derivative of is .  But I'm investigating why I can't seem to get it in the following approach, however bad an approach it might be. Let .  Now I raise to the equation, getting Using implicit differentiation, I get In other words, it leads me nowhere.  Is it possible at all do this with the conditions imposed in the question?  It seems that if the exercise asks me to use a change of base, then I must really use the derivative of to solve the problem.","\log_4 x \log_4 x = (\ln x)/(\ln 4) \ln x 1/x y = \log_4 x = \frac{\ln x}{\ln 4} e \exp(y) = \exp\left(\frac{\ln x}{\ln 4}\right). \begin{align*}
\exp(y)y' &= \exp\left(\frac{\ln x}{\ln 4}\right) \left(\frac{\ln x}{\ln 4}\right)' = \exp\left(y\right) y'\\
\exp(y)y' - \exp(y)y' &= 0\\
(\exp(y) - \exp(y))y' &= 0
\end{align*} \ln x","['calculus', 'derivatives', 'logarithms']"
74,Steps in the proof of the corollary of fundamental theorem of calculus in Wikipedia,Steps in the proof of the corollary of fundamental theorem of calculus in Wikipedia,,"I am unsure on a specific step in the proof of the corollary of FTC from Wikipedia . Suppose $F$ is an antiderivative of $f$ on the interval $[a,b]$ . Wiki used the fact that $G$ define as $$G(x) = \int_a^x f(x) \;dx$$ is an antiderivative of $f$ to show that $$F(a)+c = G(a) \tag{1}$$ for some constant $c$ . However, in wiki's proof of Fundamental Theorem of Calculus, we only concluded that $G$ as defined above is an antiderivative of $f$ on the interval $(a,b)$ . Since $G$ might not be an antiderivative of $f$ on the point $a$ , how can we use the property of antiderivative differ from each other by a constant to conclude equation (1)? Is there any other proof of FTC that shows $G$ is an antiderivative of $f$ on point $a$ and $b$ ? Thank you so much in advance for answering this! It has bother me for a while now and I can't figure it out.","I am unsure on a specific step in the proof of the corollary of FTC from Wikipedia . Suppose is an antiderivative of on the interval . Wiki used the fact that define as is an antiderivative of to show that for some constant . However, in wiki's proof of Fundamental Theorem of Calculus, we only concluded that as defined above is an antiderivative of on the interval . Since might not be an antiderivative of on the point , how can we use the property of antiderivative differ from each other by a constant to conclude equation (1)? Is there any other proof of FTC that shows is an antiderivative of on point and ? Thank you so much in advance for answering this! It has bother me for a while now and I can't figure it out.","F f [a,b] G G(x) = \int_a^x f(x) \;dx f F(a)+c = G(a) \tag{1} c G f (a,b) G f a G f a b","['real-analysis', 'calculus', 'integration', 'derivatives']"
75,Derivative-ish of $f^2(x)$,Derivative-ish of,f^2(x),"The problem is as follows: $f$ is differentiable at $x$ . Show that $\lim_{h\rightarrow 0}\frac{f^2(x+3h)-f^2(x-h)}{h}$ exists and find its value. Note that $f^2(a)$ just means $[f(a)]^2$ . Well, basis calculus tells me the answer should be $2f'(x)f(x)$ . Here's my attempt: using the fact that $a^2-b^2=(a-b)(a+b)$ , I get that the fraction turns into $\frac{f(x+3h)-f(x-h)}{h}\cdot (f(x+3h)+f(x-h))$ . I know since $f$ is differentiable at $x$ , it's continuous in a neighborhood around $x$ , and so the right product just turns into $2f(x)$ as $h\rightarrow 0$ . The left product is basically $f'(x)$ , but I'm worried abound $f$ being evaluated at $x+3h$ and $x-h$ , rather than strictly $x+h$ and $x$ . How do I deal with this? Or can I just say that the left product is $f'(x)$ has $h\rightarrow 0$ ? Thanks.","The problem is as follows: is differentiable at . Show that exists and find its value. Note that just means . Well, basis calculus tells me the answer should be . Here's my attempt: using the fact that , I get that the fraction turns into . I know since is differentiable at , it's continuous in a neighborhood around , and so the right product just turns into as . The left product is basically , but I'm worried abound being evaluated at and , rather than strictly and . How do I deal with this? Or can I just say that the left product is has ? Thanks.",f x \lim_{h\rightarrow 0}\frac{f^2(x+3h)-f^2(x-h)}{h} f^2(a) [f(a)]^2 2f'(x)f(x) a^2-b^2=(a-b)(a+b) \frac{f(x+3h)-f(x-h)}{h}\cdot (f(x+3h)+f(x-h)) f x x 2f(x) h\rightarrow 0 f'(x) f x+3h x-h x+h x f'(x) h\rightarrow 0,['derivatives']
76,Show that this function is $C^1$ class.,Show that this function is  class.,C^1,"Let $f:[-1,1]\to\mathbb R$ be $C^1$ class. Define $g:[-1,1]\to\mathbb R$ by $$g(x)=\begin{cases}f(0) &\mathrm{if}\ x=0 \\ \frac{1}{x}\int_0^x f(t)dt &\mathrm{otherwise} \end{cases}$$ Then, show that $g$ is $C^1$ . If $x\neq 0$ , $g$ is differentiable and $g'(x)=-\frac{1}{x^2}\int_0^x f(t)dt+\frac{1}{x}f(x)$ and this is continuous by Fundamental theorem of Calculus. Thus what I have to show is $g'(0)$ exists and $\displaystyle\lim_{x\to 0}g'(x)=g'(0)$ . Expectation Before calculating $g'(0)$ , I expect $g'(0)$ is $\frac{f'(0)}{2}$ because if I assume $g'$ is continuous at $0$ , i.e., $\displaystyle\lim_{x\to 0}g'(x)=g'(0)$ , I have \begin{align} \lim_{x\to 0}g'(x) &=\lim_{x\to 0}\left[-\frac{1}{x^2}\int_0^x f(t)dt+\frac{1}{x}f(x)\right]\\ &\underset{\mathrm{l'Hôpital}}=\dfrac{f'(0)}{2}, \end{align} and thus $g'(0)=\dfrac{f'(0)}{2}$ . Anyway, I calculate $g'(0)$ by definition. I have to evaluate $$\lim_{h\to 0}\frac{g(h)-g(0)}{h}.$$ For $h>0$ , I have $$\dfrac{g(h)-g(0)}{h}=\dfrac{\tfrac{1}{h}\int_0^h [f(t)-f(0)] dt}{h}$$ From the mean value theorem for integral, there is $c_h\in(0,h)$ s.t. $$\frac{1}{h}\int_0^h[f(t)-f(0)]dt=f(c_h)-f(0)$$ Thus $$\dfrac{g(h)-g(0)}{h}=\dfrac{f(c_h)-f(0)}{h}=\dfrac{c_h}{h}\dfrac{f(c_h)-f(0)}{c_h}$$ I have $\displaystyle\lim_{h\to 0^+}\dfrac{f(c_h)-f(0)}{c_h}=f'(0)$ , but what is $\displaystyle\lim_{h\to 0^+}\dfrac{c_h}{h}$ ? By the expectation above, $\displaystyle\lim_{h\to 0^+}\dfrac{c_h}{h}$ seems to be $\frac{1}{2}$ , but I don't know how I should conclude that. Perhaps my solution doesn't work. Thanks for the help.","Let be class. Define by Then, show that is . If , is differentiable and and this is continuous by Fundamental theorem of Calculus. Thus what I have to show is exists and . Expectation Before calculating , I expect is because if I assume is continuous at , i.e., , I have and thus . Anyway, I calculate by definition. I have to evaluate For , I have From the mean value theorem for integral, there is s.t. Thus I have , but what is ? By the expectation above, seems to be , but I don't know how I should conclude that. Perhaps my solution doesn't work. Thanks for the help.","f:[-1,1]\to\mathbb R C^1 g:[-1,1]\to\mathbb R g(x)=\begin{cases}f(0) &\mathrm{if}\ x=0 \\ \frac{1}{x}\int_0^x f(t)dt &\mathrm{otherwise} \end{cases} g C^1 x\neq 0 g g'(x)=-\frac{1}{x^2}\int_0^x f(t)dt+\frac{1}{x}f(x) g'(0) \displaystyle\lim_{x\to 0}g'(x)=g'(0) g'(0) g'(0) \frac{f'(0)}{2} g' 0 \displaystyle\lim_{x\to 0}g'(x)=g'(0) \begin{align}
\lim_{x\to 0}g'(x)
&=\lim_{x\to 0}\left[-\frac{1}{x^2}\int_0^x f(t)dt+\frac{1}{x}f(x)\right]\\
&\underset{\mathrm{l'Hôpital}}=\dfrac{f'(0)}{2},
\end{align} g'(0)=\dfrac{f'(0)}{2} g'(0) \lim_{h\to 0}\frac{g(h)-g(0)}{h}. h>0 \dfrac{g(h)-g(0)}{h}=\dfrac{\tfrac{1}{h}\int_0^h [f(t)-f(0)] dt}{h} c_h\in(0,h) \frac{1}{h}\int_0^h[f(t)-f(0)]dt=f(c_h)-f(0) \dfrac{g(h)-g(0)}{h}=\dfrac{f(c_h)-f(0)}{h}=\dfrac{c_h}{h}\dfrac{f(c_h)-f(0)}{c_h} \displaystyle\lim_{h\to 0^+}\dfrac{f(c_h)-f(0)}{c_h}=f'(0) \displaystyle\lim_{h\to 0^+}\dfrac{c_h}{h} \displaystyle\lim_{h\to 0^+}\dfrac{c_h}{h} \frac{1}{2}","['real-analysis', 'calculus', 'derivatives', 'solution-verification']"
77,Find second derivative of $\gamma (t) = A^{-1}(t)$ where $A: \mathbb{R} \to GL(\mathbb{R}^l)$ is smooth curve.,Find second derivative of  where  is smooth curve.,\gamma (t) = A^{-1}(t) A: \mathbb{R} \to GL(\mathbb{R}^l),"Find second derivative of $\gamma (t) = A^{-1}(t)$ where $A: \mathbb{R} \to GL(\mathbb{R}^l)$ is smooth curve. What I should get is following $$\gamma '' = 2 \gamma ' A \gamma ' - A^{-1} \frac{d^2 A}{dt^2} A^{-1}.$$ I have proven that if $f:GL(X) \to GL(X)$ given as $f(L) = L^{-1} $ , where $GL(X) := \{ L \in \mathcal{L} (X; X) \mid \exists L^{-1} \in \mathcal{L} (X; X) \} $ , first derivative of such a map is $$Df(A)h = -A^{-1} h A^{-1}.$$ I've tried to do something with that but I was unable to get exactly what is required.","Find second derivative of where is smooth curve. What I should get is following I have proven that if given as , where , first derivative of such a map is I've tried to do something with that but I was unable to get exactly what is required.",\gamma (t) = A^{-1}(t) A: \mathbb{R} \to GL(\mathbb{R}^l) \gamma '' = 2 \gamma ' A \gamma ' - A^{-1} \frac{d^2 A}{dt^2} A^{-1}. f:GL(X) \to GL(X) f(L) = L^{-1}  GL(X) := \{ L \in \mathcal{L} (X; X) \mid \exists L^{-1} \in \mathcal{L} (X; X) \}  Df(A)h = -A^{-1} h A^{-1}.,"['real-analysis', 'derivatives', 'linear-transformations']"
78,Differentiate $|\sin x|$,Differentiate,|\sin x|,"Could you differentiate $|\sin x|$ and give the points where the derivative doesn't exist? I have only so far tried to analyze it using the graph, from 0 to $\pi$ is $\cos x$ . But from $\pi$ to $2\pi$ $\sin x$ will be on the positive side and it'd be given by the derivative of $\cos x$ too. Anyways I am confused and need help. Is there a better way of doing this?","Could you differentiate and give the points where the derivative doesn't exist? I have only so far tried to analyze it using the graph, from 0 to is . But from to will be on the positive side and it'd be given by the derivative of too. Anyways I am confused and need help. Is there a better way of doing this?",|\sin x| \pi \cos x \pi 2\pi \sin x \cos x,"['calculus', 'algebra-precalculus', 'derivatives', 'absolute-value']"
79,What happens when you find the derivative of $f(x) =|x^2-1|$.,What happens when you find the derivative of .,f(x) =|x^2-1|,"Let $f(x) =|x^2-1|$ . I'm trying to see if this function has a point of inflection and even though looking at the graph tells me the answer already, I was just curious. What happens to the modulus part when you differentiate it? Would you just take the piecewise version of the function and differentiate each function?","Let . I'm trying to see if this function has a point of inflection and even though looking at the graph tells me the answer already, I was just curious. What happens to the modulus part when you differentiate it? Would you just take the piecewise version of the function and differentiate each function?",f(x) =|x^2-1|,"['calculus', 'derivatives']"
80,Computing integration $\int_0^{\infty}\frac{\log(x)\sin(x)}{x} dx$,Computing integration,\int_0^{\infty}\frac{\log(x)\sin(x)}{x} dx,"I can't seem to find this problem on Math.SE, so sorry if it actually exists. I want to evaluate $I(a) = \int_0^{\infty} \frac{\log(ax)\sin x}{x} dx$ . Then I separated it into two integrals $I(a) = \int_0^{\infty} \frac{\log(a)\sin x}{x} + \int_0^{\infty} \frac{\log(x)\sin x}{x}$ . The first term is constant and evaluates to $\frac{\pi}{2}\log a$ , and the second term is where I am struggling with, it's equal to $I(1)$ . I also tried using differentiation under the integral sign, but it gives the same result, as I get $I'(a) = \frac{\pi}{2a}$ , and I can't find the constant. I also tried putting the variable inside $\sin(x)$ , so $J(a) = \int_0^{\infty} \frac{\log x \sin(ax)}{x}$ , but $I'(a) = \int_0^{\infty} \log x \cos(ax) dx$ is divergent. WolframAlpha gives something about $\pi\gamma$ , so I am wondering where that comes from. Thanks!","I can't seem to find this problem on Math.SE, so sorry if it actually exists. I want to evaluate . Then I separated it into two integrals . The first term is constant and evaluates to , and the second term is where I am struggling with, it's equal to . I also tried using differentiation under the integral sign, but it gives the same result, as I get , and I can't find the constant. I also tried putting the variable inside , so , but is divergent. WolframAlpha gives something about , so I am wondering where that comes from. Thanks!",I(a) = \int_0^{\infty} \frac{\log(ax)\sin x}{x} dx I(a) = \int_0^{\infty} \frac{\log(a)\sin x}{x} + \int_0^{\infty} \frac{\log(x)\sin x}{x} \frac{\pi}{2}\log a I(1) I'(a) = \frac{\pi}{2a} \sin(x) J(a) = \int_0^{\infty} \frac{\log x \sin(ax)}{x} I'(a) = \int_0^{\infty} \log x \cos(ax) dx \pi\gamma,"['integration', 'derivatives']"
81,Unique solution of the differential equation with the initial value,Unique solution of the differential equation with the initial value,,"exercise: Let $g$ be a continuous function on the domain $\mathbb{R}$ and suppose that for each $c$ the differential equation $y^{\prime}=g(y)$ has a unique solution, that satisfies the initial condition $y(0)=c$ , and is defined on the domain $\mathbb{R}$ . We can define the function $\phi(x, c)$ of the two variables $x$ and $c$ , so that, as a function of $x$ , it is the solution that satisfies the condition $y(0)=c$ . (a) Show that the translate of a solution of $y^{\prime}=g(y)$ is again a solution; that is, if $y=f(x)$ is a solution and $\alpha$ a number then $f(x-\alpha)$ is also a solution. (b) Show that there is a unique solution that satisfies $y^{\prime}(\alpha)=c$ , and that it is $\phi(x-\alpha, c)$ My draft work: a) Let $y=f(x)$ it means $f'(x)=g(f(x))\Rightarrow f'(x-a)=g(f(x-a))\Rightarrow h'(s)=g(h(s))$ and $h(0)=c$ b)  for simplicity denote $h(s)=t$ . $\,$ $h'(s)=g(h(s))\Rightarrow [h'(s)]'=g'(t)h'(s)$ from here if we get the derivative of $q(s)=\dfrac{h'(s)}{e^{g(h(s))}}\Rightarrow q'(s)=\dfrac{[h'(s)]'e^{g(h(s))}-[e^{g(h(s))}]'h'(s)}{(e^{g(h(s))})^2}=0$ we conclude that $h'(s)=le^{g(h(s))}$ for some number l. However, I couldn't to show the uniqueness. Can anybody check my work if I am on the right track?? Please. Thanks to @Martin R comments I am very suspicious about this may be typo. It should be $y(\alpha)=c$ not $y'(\alpha)=c$","exercise: Let be a continuous function on the domain and suppose that for each the differential equation has a unique solution, that satisfies the initial condition , and is defined on the domain . We can define the function of the two variables and , so that, as a function of , it is the solution that satisfies the condition . (a) Show that the translate of a solution of is again a solution; that is, if is a solution and a number then is also a solution. (b) Show that there is a unique solution that satisfies , and that it is My draft work: a) Let it means and b)  for simplicity denote . from here if we get the derivative of we conclude that for some number l. However, I couldn't to show the uniqueness. Can anybody check my work if I am on the right track?? Please. Thanks to @Martin R comments I am very suspicious about this may be typo. It should be not","g \mathbb{R} c y^{\prime}=g(y) y(0)=c \mathbb{R} \phi(x, c) x c x y(0)=c y^{\prime}=g(y) y=f(x) \alpha f(x-\alpha) y^{\prime}(\alpha)=c \phi(x-\alpha, c) y=f(x) f'(x)=g(f(x))\Rightarrow f'(x-a)=g(f(x-a))\Rightarrow h'(s)=g(h(s)) h(0)=c h(s)=t \, h'(s)=g(h(s))\Rightarrow [h'(s)]'=g'(t)h'(s) q(s)=\dfrac{h'(s)}{e^{g(h(s))}}\Rightarrow q'(s)=\dfrac{[h'(s)]'e^{g(h(s))}-[e^{g(h(s))}]'h'(s)}{(e^{g(h(s))})^2}=0 h'(s)=le^{g(h(s))} y(\alpha)=c y'(\alpha)=c","['real-analysis', 'calculus', 'derivatives', 'logarithms']"
82,"prove that there exist $x_1\neq x_2, x_1,x_2\in [0,1]$ so that $\int_{x_1}^{x_2} f(x)dx = (x_1-x_2)^{2002}$",prove that there exist  so that,"x_1\neq x_2, x_1,x_2\in [0,1] \int_{x_1}^{x_2} f(x)dx = (x_1-x_2)^{2002}","Let $f : [0,1]\to\mathbb{R}$ be an integrable function so that $0 < |\int_0^1 f(x)dx| \leq 1$ . Prove that there exist $x_1\neq x_2, x_1,x_2\in [0,1]$ so that $\int_{x_1}^{x_2} f(x)dx = (x_1-x_2)^{2002}$ . Let $F : [0,1]\to \mathbb{R}$ be given by $F(x) = \int_0^x f(t)dt.$ Note that if we have $|F(b)-F(a)| \ge |b-a|^{2002}$ for all $a\neq b\in [0,1],$ then we can just choose $x_1 = a, x_2 = b$ or vice versa (so that the sign of $F(x_2)-F(x_1)$ is positive) so that $1\ge |F(1)-F(0)| \ge 1.$ If $|F(b)-F(a)|\leq |b-a|^{2002}$ for all $b\neq a$ in $[0,1]$ , then for an arbitrary $a\in [0,1],$ letting $b\to a,$ we get that $|F'(a)| \leq \lim\limits_{b\to a} |b-a|^{2001} = 0$ and hence $F'(a) = 0$ for all a. Hence $F$ is constant, contradicting $0 < |F(1)-F(0)|.$ Thus there exist $a,b,c,d\in [0,1]$ with $|F(b)-F(a)| < |b-a|^{2002}$ and $|F(d)-F(c)| > |d-c|^{2002}$ . But I'm not sure how to proceed from here. Perhaps using the Intermediate Value Theorem might be useful?","Let be an integrable function so that . Prove that there exist so that . Let be given by Note that if we have for all then we can just choose or vice versa (so that the sign of is positive) so that If for all in , then for an arbitrary letting we get that and hence for all a. Hence is constant, contradicting Thus there exist with and . But I'm not sure how to proceed from here. Perhaps using the Intermediate Value Theorem might be useful?","f : [0,1]\to\mathbb{R} 0 < |\int_0^1 f(x)dx| \leq 1 x_1\neq x_2, x_1,x_2\in [0,1] \int_{x_1}^{x_2} f(x)dx = (x_1-x_2)^{2002} F : [0,1]\to \mathbb{R} F(x) = \int_0^x f(t)dt. |F(b)-F(a)| \ge |b-a|^{2002} a\neq b\in [0,1], x_1 = a, x_2 = b F(x_2)-F(x_1) 1\ge |F(1)-F(0)| \ge 1. |F(b)-F(a)|\leq |b-a|^{2002} b\neq a [0,1] a\in [0,1], b\to a, |F'(a)| \leq \lim\limits_{b\to a} |b-a|^{2001} = 0 F'(a) = 0 F 0 < |F(1)-F(0)|. a,b,c,d\in [0,1] |F(b)-F(a)| < |b-a|^{2002} |F(d)-F(c)| > |d-c|^{2002}","['real-analysis', 'calculus', 'integration', 'derivatives', 'inequality']"
83,How to compute the gradient of any loss function,How to compute the gradient of any loss function,,"I have the following loss function to minimize : $\hat{\mathbf{A}} = \arg \min_{\mathbf{A}} \frac{1}{2}{\parallel{\mathbf{Y} - \mathbf{K}  \left(\left(  \mathbf{D}\mathbf{A}\right)\odot\mathbf{M}\right)}\parallel}_{F}^{2} + \frac{1}{2}{\parallel{\mathbf{W} - \mathbf{L}\left(\left(  \mathbf{D}\mathbf{A}\right)\odot\mathbf{H}\right) \mathbf{S}}\parallel}_{F}^{2}$ Where ${\parallel\mathbf{X}\parallel}_{F}^{2} =Tr(\mathbf{XX^T})$ is the Frobenius norm, and $\odot$ denote Hadamard product (element-wise product). For the first term : $\mathbf{Y}\in\mathbb{C}^{a\times l}$ , $\mathbf{K}\in\mathbb{C}^{a\times b}$ , $\mathbf{D}\in\mathbb{C}^{b\times d}$ , $\mathbf{A}\in\mathbb{C}^{d\times l}$ and $\mathbf{M}\in\mathbb{C}^{b\times l}$ . For the second term : $\mathbf{W}\in\mathbb{C}^{b\times c}$ , $\mathbf{L}\in\mathbb{C}^{b\times b}$ , $\mathbf{H}\in\mathbb{C}^{b\times l}$ and $\mathbf{S}\in\mathbb{C}^{l\times c}$ . I know when the loss function is on the form : $J(X) = \frac{1}{2}{\parallel{\mathbf{Y} - \mathbf{K}  \left(  \mathbf{X}\odot\mathbf{M}\right)}\parallel}_{F}^{2}$ then its derivative is : $\nabla J(X) = K^{H}\left( \mathbf{Y} - \mathbf{K}  \left(  \mathbf{X}\odot\mathbf{M}\right)\right)\odot \mathbf{M}$ But in this case it is so difficult. Is there any way to compute its gradiant ?","I have the following loss function to minimize : Where is the Frobenius norm, and denote Hadamard product (element-wise product). For the first term : , , , and . For the second term : , , and . I know when the loss function is on the form : then its derivative is : But in this case it is so difficult. Is there any way to compute its gradiant ?",\hat{\mathbf{A}} = \arg \min_{\mathbf{A}} \frac{1}{2}{\parallel{\mathbf{Y} - \mathbf{K}  \left(\left(  \mathbf{D}\mathbf{A}\right)\odot\mathbf{M}\right)}\parallel}_{F}^{2} + \frac{1}{2}{\parallel{\mathbf{W} - \mathbf{L}\left(\left(  \mathbf{D}\mathbf{A}\right)\odot\mathbf{H}\right) \mathbf{S}}\parallel}_{F}^{2} {\parallel\mathbf{X}\parallel}_{F}^{2} =Tr(\mathbf{XX^T}) \odot \mathbf{Y}\in\mathbb{C}^{a\times l} \mathbf{K}\in\mathbb{C}^{a\times b} \mathbf{D}\in\mathbb{C}^{b\times d} \mathbf{A}\in\mathbb{C}^{d\times l} \mathbf{M}\in\mathbb{C}^{b\times l} \mathbf{W}\in\mathbb{C}^{b\times c} \mathbf{L}\in\mathbb{C}^{b\times b} \mathbf{H}\in\mathbb{C}^{b\times l} \mathbf{S}\in\mathbb{C}^{l\times c} J(X) = \frac{1}{2}{\parallel{\mathbf{Y} - \mathbf{K}  \left(  \mathbf{X}\odot\mathbf{M}\right)}\parallel}_{F}^{2} \nabla J(X) = K^{H}\left( \mathbf{Y} - \mathbf{K}  \left(  \mathbf{X}\odot\mathbf{M}\right)\right)\odot \mathbf{M},"['derivatives', 'optimization', 'matrix-calculus', 'gradient-descent']"
84,Finding the range of $\frac {2x^2+x-3}{x^2+4x-5}$,Finding the range of,\frac {2x^2+x-3}{x^2+4x-5},"I solved for the range as follows: Setting $f(x)=\frac {2x^2+x-3}{x^2+4x-5} =y$ , I rearranged it to get a quadratic in x. $$(y-2)x^{2}+ (4y-1)x +(3-5y)=0$$ Next, using $\Delta \ge 0$ , I got $$(4y-1)^2-4(y-2)(3-5y) \ge 0$$ Which boiled down to $$(6y-5)^2 \ge 0$$ And this gave me $$ y \in R$$ Next, the value of $x$ for which $y=2$ is $x=1$ , for which the function isn't defined, so that gives me $$y \in R - \{2\}$$ However, the solution is $$y \in R- \{\frac{5}{6}, 2\}$$ I understand that the original function is identical to $$g(x) =\frac{2x+3}{x+5}    \forall x \in R - \{1\}$$ and that $\frac{5}{6} =g(1)$ , but in the original function $f(x)$ I found $y=2$ corresponded to $x=1$ and therefore excluded it, but if $x= 1$ also corresponds to $y=\frac{5}{6}$ then wouldn't this $not$ be a function, as $x=1$ would then be associated with two different $y$ values? I've read the answers in Why D≥0 while finding the range of rational functions and Finding the range of $y =\frac{x^2+2x+4}{2x^2+4x+9}$ (and $y=\frac{\text{quadratic}}{\text{quadratic}}$ in general) but I'm still unsure of how to apply the information from those to figure out what values of $y$ need to be excluded when dealing with such questions. While I am familiar with derivatives and limits to a certain degree, we were assumed to $\underline {not}$ know calculus when we were taught this and solved such problems. I apologise if there are any issues with formatting, this is my first time using Latex.","I solved for the range as follows: Setting , I rearranged it to get a quadratic in x. Next, using , I got Which boiled down to And this gave me Next, the value of for which is , for which the function isn't defined, so that gives me However, the solution is I understand that the original function is identical to and that , but in the original function I found corresponded to and therefore excluded it, but if also corresponds to then wouldn't this be a function, as would then be associated with two different values? I've read the answers in Why D≥0 while finding the range of rational functions and Finding the range of $y =\frac{x^2+2x+4}{2x^2+4x+9}$ (and $y=\frac{\text{quadratic}}{\text{quadratic}}$ in general) but I'm still unsure of how to apply the information from those to figure out what values of need to be excluded when dealing with such questions. While I am familiar with derivatives and limits to a certain degree, we were assumed to know calculus when we were taught this and solved such problems. I apologise if there are any issues with formatting, this is my first time using Latex.","f(x)=\frac {2x^2+x-3}{x^2+4x-5} =y (y-2)x^{2}+ (4y-1)x +(3-5y)=0 \Delta \ge 0 (4y-1)^2-4(y-2)(3-5y) \ge 0 (6y-5)^2 \ge 0  y \in R x y=2 x=1 y \in R - \{2\} y \in R- \{\frac{5}{6}, 2\} g(x) =\frac{2x+3}{x+5} 
  \forall x \in R - \{1\} \frac{5}{6} =g(1) f(x) y=2 x=1 x= 1 y=\frac{5}{6} not x=1 y y \underline {not}","['derivatives', 'quadratics', 'rational-functions']"
85,"Problem on continuously differentiable function on (0, ∞)","Problem on continuously differentiable function on (0, ∞)",,"Q. Let $f$ be continuously differentiable on $(0, \infty)$ and let $f(0)=1$ . Show that if $|f(x)| \leq e^{-x}$ for $x \geq 0$ , then there is $x_{0}>0$ such that $f^{\prime}\left(x_{0}\right)=-e^{-x_{0}}$ . I think Setting $g(x)=f(x)-e^{-x}, x \geq 0$ . Then $g(0)=0, g(x) \leq 0$ and $\lim _{x \rightarrow \infty} g(x)=0$ . If $g(x) \equiv 0$ , then $f^{\prime}(x)=-e^{-x}$ for $x \in(0, \infty)$ . So, suppose that there is $a>0$ such that $g(a)<0$ . Then for sufficiently large $x$ , say $x>M$ , we have $g(x)>\frac{1}{2} g(a)$ . Consequently, $g$ attains its minimum value at some $x_{0}$ in $(0, M )$ . Thus $g^{\prime}\left(x_{0}\right)=0$ . Is this okay? Also give me another approach.","Q. Let be continuously differentiable on and let . Show that if for , then there is such that . I think Setting . Then and . If , then for . So, suppose that there is such that . Then for sufficiently large , say , we have . Consequently, attains its minimum value at some in . Thus . Is this okay? Also give me another approach.","f (0, \infty) f(0)=1 |f(x)| \leq e^{-x} x \geq 0 x_{0}>0 f^{\prime}\left(x_{0}\right)=-e^{-x_{0}} g(x)=f(x)-e^{-x}, x \geq 0 g(0)=0, g(x) \leq 0 \lim _{x \rightarrow \infty} g(x)=0 g(x) \equiv 0 f^{\prime}(x)=-e^{-x} x \in(0, \infty) a>0 g(a)<0 x x>M g(x)>\frac{1}{2} g(a) g x_{0} (0, M ) g^{\prime}\left(x_{0}\right)=0","['real-analysis', 'derivatives', 'continuity', 'solution-verification']"
86,Disconnected image of a derivative,Disconnected image of a derivative,,"I would like to know if there exists a differentiable vector-valued function $f:[a,b]\rightarrow \Bbb R^2$ such that the image $f'([a,b])$ of its derivative is disconnected. My Attempt First of all I asked for $f$ to be vector-valued because, if it were real-valued, then Darboux's Theorem would imply that $f'([a,b])$ be connected. For a similar reason, if we let $x(t)$ and $y(t)$ denote the component functions of $f$ , then their derivatives can not both be continuous, for otherwise $f'$ would also be continuous hence preserving connectedness. Therefore, WLOG, $x'$ has to be discontinuous; however, Darboux's Theorem imply that it can not have discontinuities of the I kind or discontinuities of the II kind with infinite directional limits on $]a, b[$ . Now, all the functions I came up for $x$ are not enough to construct the desired $f$ and I am starting to believe that such a function doesn't actually exist but I am not able to prove it. Any help or hint, as always, is highly appreciated!","I would like to know if there exists a differentiable vector-valued function such that the image of its derivative is disconnected. My Attempt First of all I asked for to be vector-valued because, if it were real-valued, then Darboux's Theorem would imply that be connected. For a similar reason, if we let and denote the component functions of , then their derivatives can not both be continuous, for otherwise would also be continuous hence preserving connectedness. Therefore, WLOG, has to be discontinuous; however, Darboux's Theorem imply that it can not have discontinuities of the I kind or discontinuities of the II kind with infinite directional limits on . Now, all the functions I came up for are not enough to construct the desired and I am starting to believe that such a function doesn't actually exist but I am not able to prove it. Any help or hint, as always, is highly appreciated!","f:[a,b]\rightarrow \Bbb R^2 f'([a,b]) f f'([a,b]) x(t) y(t) f f' x' ]a, b[ x f","['real-analysis', 'general-topology', 'derivatives', 'vectors', 'connectedness']"
87,"Spivak, Ch 10 Differentiation, Problem *33: Very tricky proof that $f'(0),...,f^{(n)}(0)$ exist if $f(x)=x^{2n}\sin(1/x)$.","Spivak, Ch 10 Differentiation, Problem *33: Very tricky proof that  exist if .","f'(0),...,f^{(n)}(0) f(x)=x^{2n}\sin(1/x)","Let $f(x)=x^{2n}\sin(1/x)$ if $x \neq 0$ , and let $f(0)=0$ . Prove that $f'(0),...,f^{(n)}(0)$ exist, and that $f^{(n)}$ is not continuous at $0$ . The strategy to prove this can be split into the following steps: prove a conjecture about what the terms of $f^{(k)}$ look like, using induction on $k$ using another induction, prove that for $k<n$ , each of the terms of $f^{(k)}$ are a product of x raised to at least the second power, times a sinusoid, which we know is bounded. use yet another induction to show that $f^{(k)}(0)=0$ , for $k \leq n$ , by calculating the limit that defines each derivative. These calculations are quite easy given step 2. in the case of $f^{(n)}$ not all terms will have an $x$ raised to at least the second power, so when we calculate the limit it will not be 0. Therefore, $f^{(n)}$ is not continuous at $0$ . My question is about step 1. If we write out, say, the first, second, and third derivatives of $f$ we start to notice a pattern, and so we conjecture that $f^{(k)}(x)$ is composed only of the following types of terms: $$a \cdot \sin(1/x)x^{2n-k}\tag{1}$$ $$\pm \sin(1/x)x^{2n-2k}, \text{ if k even}\tag{2}$$ $$\pm \cos(1/x)x^{2n-2k}, \text{ if k odd}\tag{3}$$ $$\sum_{i=k+1}^{2k-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)]\tag{4}$$ We can try to prove that this is true by using induction on $k$ . $$f'(x)=\sin(1/x) \cdot 2nx^{2n-1}+\cos(1/x)(-x^{2n-2})$$ The first term is a term like in $(1)$ , and the second term is like in $(4)$ . Now assume that the conjecture is true for some $k$ . Then we should only see the following terms in $f^{(k+1)}(x)$ $$\sin(1/x)x^{2n-(k+1)}\tag{5}$$ $$\pm \sin(1/x)x^{2n-2(k+1)}, \text{ if k is even}\tag{6}$$ $$\pm \cos(1/x) x^{2n-2(k+1)}, \text{ if k is odd}\tag{7}$$ $$\sum_{i=(k+1)+1}^{2(k+1)-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)]\tag{8}$$ To check this, we can differentiate $(1)$ , $(2)$ , $(3)$ , and $(4)$ and check if the results only contain terms as in $(5)$ , $(6)$ , $(7)$ , and $(8)$ . Differentiation of $(1)$ , $(2)$ , and $(3)$ produces the correct results. My question regards the differentiation of $(4)$ . $$\frac{d}{dx}(\sum_{i=k+1}^{2k-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)])$$ $$=\sum_{i=k+1}^{2k-1} [a_i(2n-1)x^{2n-i-1}\sin(1/x)-a_i x^{2n-i-2}\cos(1/x)+b_i(2n-1)x^{2n-i-1}\cos(1/x)+b_ix^{2n-i-2}\sin(1/x)]$$ $$=\sum_{i=k+1}^{2k-1} [a_i(2n-1)x^{2n-(i+1)}\sin(1/x)+b_i(2n-1)x^{2n-(i+1)}\cos(1/x)-a_i x^{2n-(i+1)-1}\cos(1/x)+b_ix^{2n-(i+1)-1}\sin(1/x)]$$ $$=\sum_{i=(k+1)+1}^{2(k+1)-1} [a_i(2n-1)x^{2n-i}\sin(1/x)+b_i(2n-1)x^{2n-i}\cos(1/x)-a_i x^{2n-i-1}\cos(1/x)+b_ix^{2n-i-1}\sin(1/x)]\tag{9}$$ The first two terms in the sum are like the terms in $(8)$ , but the last two terms in the sum are problematic. They don't seem to fit. This problem is clearly quite tricky and I've spent a couple hours on it. I believe the reasoning is correct (and it is in the solution manual as well), but the solution manual does not go through every step of every differentiation (and specifically the differentiation of $(4)$ as I showed above). I am wondering if there is a mistake or if there is some algebraic manipulation of $(9)$ that I am missing.","Let if , and let . Prove that exist, and that is not continuous at . The strategy to prove this can be split into the following steps: prove a conjecture about what the terms of look like, using induction on using another induction, prove that for , each of the terms of are a product of x raised to at least the second power, times a sinusoid, which we know is bounded. use yet another induction to show that , for , by calculating the limit that defines each derivative. These calculations are quite easy given step 2. in the case of not all terms will have an raised to at least the second power, so when we calculate the limit it will not be 0. Therefore, is not continuous at . My question is about step 1. If we write out, say, the first, second, and third derivatives of we start to notice a pattern, and so we conjecture that is composed only of the following types of terms: We can try to prove that this is true by using induction on . The first term is a term like in , and the second term is like in . Now assume that the conjecture is true for some . Then we should only see the following terms in To check this, we can differentiate , , , and and check if the results only contain terms as in , , , and . Differentiation of , , and produces the correct results. My question regards the differentiation of . The first two terms in the sum are like the terms in , but the last two terms in the sum are problematic. They don't seem to fit. This problem is clearly quite tricky and I've spent a couple hours on it. I believe the reasoning is correct (and it is in the solution manual as well), but the solution manual does not go through every step of every differentiation (and specifically the differentiation of as I showed above). I am wondering if there is a mistake or if there is some algebraic manipulation of that I am missing.","f(x)=x^{2n}\sin(1/x) x \neq 0 f(0)=0 f'(0),...,f^{(n)}(0) f^{(n)} 0 f^{(k)} k k<n f^{(k)} f^{(k)}(0)=0 k \leq n f^{(n)} x f^{(n)} 0 f f^{(k)}(x) a \cdot \sin(1/x)x^{2n-k}\tag{1} \pm \sin(1/x)x^{2n-2k}, \text{ if k even}\tag{2} \pm \cos(1/x)x^{2n-2k}, \text{ if k odd}\tag{3} \sum_{i=k+1}^{2k-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)]\tag{4} k f'(x)=\sin(1/x) \cdot 2nx^{2n-1}+\cos(1/x)(-x^{2n-2}) (1) (4) k f^{(k+1)}(x) \sin(1/x)x^{2n-(k+1)}\tag{5} \pm \sin(1/x)x^{2n-2(k+1)}, \text{ if k is even}\tag{6} \pm \cos(1/x) x^{2n-2(k+1)}, \text{ if k is odd}\tag{7} \sum_{i=(k+1)+1}^{2(k+1)-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)]\tag{8} (1) (2) (3) (4) (5) (6) (7) (8) (1) (2) (3) (4) \frac{d}{dx}(\sum_{i=k+1}^{2k-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)]) =\sum_{i=k+1}^{2k-1} [a_i(2n-1)x^{2n-i-1}\sin(1/x)-a_i x^{2n-i-2}\cos(1/x)+b_i(2n-1)x^{2n-i-1}\cos(1/x)+b_ix^{2n-i-2}\sin(1/x)] =\sum_{i=k+1}^{2k-1} [a_i(2n-1)x^{2n-(i+1)}\sin(1/x)+b_i(2n-1)x^{2n-(i+1)}\cos(1/x)-a_i x^{2n-(i+1)-1}\cos(1/x)+b_ix^{2n-(i+1)-1}\sin(1/x)] =\sum_{i=(k+1)+1}^{2(k+1)-1} [a_i(2n-1)x^{2n-i}\sin(1/x)+b_i(2n-1)x^{2n-i}\cos(1/x)-a_i x^{2n-i-1}\cos(1/x)+b_ix^{2n-i-1}\sin(1/x)]\tag{9} (8) (4) (9)","['calculus', 'derivatives', 'solution-verification']"
88,"If normal to curve $x=3\cos\theta-\cos^3{\theta}$ and $y=3\sin\theta-\sin^3{\theta}$ at point $P(\theta)$ passes through the origin, then $\theta=$","If normal to curve  and  at point  passes through the origin, then",x=3\cos\theta-\cos^3{\theta} y=3\sin\theta-\sin^3{\theta} P(\theta) \theta=,"If normal to curve $x=3\cos\theta-\cos^3{\theta}$ and $y=3\sin\theta-\sin^3{\theta}$ at point $P(\theta)$ passes through the origin, then $\theta=$ (A) $0$ (B) $\dfrac{\pi}{4}$ (C) $\dfrac{\pi}{2}$ (D) $\dfrac{\pi}{6}$ My Approach: I got the slope of normal as $\tan^3{\theta}.$ So equation of normal will be $$y-3\sin\theta +\sin^3{\theta}=\tan^3{\theta \cdot(x-3\cos\theta+3\cos^3{\theta})}$$ Because it passes through origin so I put $x=0,y=0$ , which lead me to the equation, $$3\sin\theta \cdot \cos{\theta} \cdot\cos{2\theta}=0$$ So value of $\theta$ will be $\theta=0, \dfrac{\pi}{2},\dfrac{\pi}{4}.$ But answer given is only $\theta = \dfrac{\pi}{4}$ My doubt: Why option (A) and (C) are wrong.","If normal to curve and at point passes through the origin, then (A) (B) (C) (D) My Approach: I got the slope of normal as So equation of normal will be Because it passes through origin so I put , which lead me to the equation, So value of will be But answer given is only My doubt: Why option (A) and (C) are wrong.","x=3\cos\theta-\cos^3{\theta} y=3\sin\theta-\sin^3{\theta} P(\theta) \theta= 0 \dfrac{\pi}{4} \dfrac{\pi}{2} \dfrac{\pi}{6} \tan^3{\theta}. y-3\sin\theta +\sin^3{\theta}=\tan^3{\theta \cdot(x-3\cos\theta+3\cos^3{\theta})} x=0,y=0 3\sin\theta \cdot \cos{\theta} \cdot\cos{2\theta}=0 \theta \theta=0, \dfrac{\pi}{2},\dfrac{\pi}{4}. \theta = \dfrac{\pi}{4}","['derivatives', 'trigonometry', 'polar-coordinates']"
89,Exact Solution form Fundamental Solution PDE,Exact Solution form Fundamental Solution PDE,,"Let $f$ be a function on $\mathbb{R}_+\times\mathbb{R}^d$ . Let $L$ be some differential operator, like $L=\frac{\partial}{\partial t}+\frac{\partial^2}{\partial x^2}$ . Consider for some function $g$ the PDE $$ L[f]=g$$ equipped with the initial condition that $f(0,x)=h(x)$ . If we are told the fundamental solution ( i.e the solution when $f(0,x)=\delta(x_0)$ ) how do I from this get the solution for when $f(0,x)=h(x)$ ? As an example I am thinking of eq (12.27) the Langevin equation from the book Elements of Nonequilibrium Statistical Mechanics by V. Balakrishnan. Im confused when comparing it to the wikipedia https://en.wikipedia.org/wiki/Fundamental_solution#Proof_that_the_convolution_is_a_solution because one is relating fundamental solutions about the RHS of the PDE and the other is relating them to initial conditions.","Let be a function on . Let be some differential operator, like . Consider for some function the PDE equipped with the initial condition that . If we are told the fundamental solution ( i.e the solution when ) how do I from this get the solution for when ? As an example I am thinking of eq (12.27) the Langevin equation from the book Elements of Nonequilibrium Statistical Mechanics by V. Balakrishnan. Im confused when comparing it to the wikipedia https://en.wikipedia.org/wiki/Fundamental_solution#Proof_that_the_convolution_is_a_solution because one is relating fundamental solutions about the RHS of the PDE and the other is relating them to initial conditions.","f \mathbb{R}_+\times\mathbb{R}^d L L=\frac{\partial}{\partial t}+\frac{\partial^2}{\partial x^2} g  L[f]=g f(0,x)=h(x) f(0,x)=\delta(x_0) f(0,x)=h(x)","['derivatives', 'partial-differential-equations', 'initial-value-problems', 'parabolic-pde', 'fundamental-solution']"
90,Differential of transition map.,Differential of transition map.,,"I'm a beginner in the calculation of differentials in smooth manifolds. I was thoughtful about the case of the transition map differential. For example, in the complex projective line $\mathbb C P_1$ , the transition map $\phi_1 \circ \phi_2^{-1}$ is given by $\frac{1}{z}$ . To calculate the differential of this map at an arbitrary point $x$ just perform the conventional derivative calculation on $\mathbb C$ ? That is, $d_x (\phi_1 \circ \phi_2^{-1}) = \frac{-1}{x^2}$ , it seems to me that this makes sense, since $\phi_1 \circ \phi_2^ {-1}$ is a map of $\mathbb C$ to $\mathbb C$ , and tangent spaces can be identified with $\mathbb C$ . Is my reasoning correct?","I'm a beginner in the calculation of differentials in smooth manifolds. I was thoughtful about the case of the transition map differential. For example, in the complex projective line , the transition map is given by . To calculate the differential of this map at an arbitrary point just perform the conventional derivative calculation on ? That is, , it seems to me that this makes sense, since is a map of to , and tangent spaces can be identified with . Is my reasoning correct?",\mathbb C P_1 \phi_1 \circ \phi_2^{-1} \frac{1}{z} x \mathbb C d_x (\phi_1 \circ \phi_2^{-1}) = \frac{-1}{x^2} \phi_1 \circ \phi_2^ {-1} \mathbb C \mathbb C \mathbb C,"['derivatives', 'differential-geometry', 'smooth-manifolds']"
91,Problem on Mean Value Theorem,Problem on Mean Value Theorem,,"The Question: Let $f$ be a function defined on an interval $[a,b]$ . What conditions could you place on $f$ to guarantee that $$ \min f'\leq \frac{f(b)-f(a)}{b-a} \leq \max f' $$ My Answer We must require a $\min f'$ and a $\max f'$ before we can proceed further. That means $f'$ must be defined in $[a,b]$ , that is, $f$ should be differentiable in $[a,b]$ . So that's one condition. Now, if $f$ is continuous in $[a,b]$ and differentiable in $(a,b)$ then, per the mean value theorem $$ \exists \text{ } c\in(a,b) \text{ | } f'(c)=\frac{f(b)-f(a)}{b-a} $$ and obviously $$ \min f'\leq f'(c) \leq \max f' $$ So the only condition would be for $f$ to be differentiable in $[a,b]$ . Now, I'll tell what's bothering me. The textbook I'm reading currently has the following definition for absolute extrema. Let $f$ be a function with domain $D$ . Then $f$ has an absolute maximum value on $D$ at a point $c$ if $$ f(x) \leq f(c) \text{ }  \forall \text{ } x \in D $$ and an absolute minimum value on $D$ at $c$ if $$ f(x) \geq f(c) \text{ }  \forall \text{ } x \in D  $$ The reason, I've considered $f$ to be differentiable in $[a,b]$ and not just $(a,b)$ is because of the definition of absolute extrema. Did I miss anything?","The Question: Let be a function defined on an interval . What conditions could you place on to guarantee that My Answer We must require a and a before we can proceed further. That means must be defined in , that is, should be differentiable in . So that's one condition. Now, if is continuous in and differentiable in then, per the mean value theorem and obviously So the only condition would be for to be differentiable in . Now, I'll tell what's bothering me. The textbook I'm reading currently has the following definition for absolute extrema. Let be a function with domain . Then has an absolute maximum value on at a point if and an absolute minimum value on at if The reason, I've considered to be differentiable in and not just is because of the definition of absolute extrema. Did I miss anything?","f [a,b] f 
\min f'\leq \frac{f(b)-f(a)}{b-a} \leq \max f'
 \min f' \max f' f' [a,b] f [a,b] f [a,b] (a,b) 
\exists \text{ } c\in(a,b) \text{ | } f'(c)=\frac{f(b)-f(a)}{b-a}
 
\min f'\leq f'(c) \leq \max f'
 f [a,b] f D f D c  f(x) \leq f(c) \text{ }
 \forall \text{ } x \in D  D c  f(x) \geq f(c) \text{ }
 \forall \text{ } x \in D   f [a,b] (a,b)",['derivatives']
92,Show the function is concave function but not strictly concave,Show the function is concave function but not strictly concave,,"For an optimisation problem, it is required to prove: is the utility function $$f(\lambda X)=−\frac{1}{\alpha}\log\left(\frac{1}{n} \sum_{i=1}^n \exp\{−\alpha \lambda 𝑋^{(𝑖)}\}\right),$$ strictly concave (strictly convex-up) function  of the $\lambda$ on an open set? Here $0\leq \alpha \leq 1$ , $n$ is a sample size, $𝑋^{(𝑖)}$ 's are random samples from leaner space $X$ . The notations are borrowed from (Miyahara, 2010) , $g_X(\lambda):=f(\lambda X)$ is a scaled value measure and $\lambda$ is a scale parameter. From first principles: A twice differentiable function of one variable is convex-up (strictly convex-up) on an interval if and only if its second derivative is non-negative (negative) there. In my case I should test: $f''_{\lambda \lambda}(\lambda X)<0$ . My solution is: $$f'(\lambda X)=−\frac{1}{\alpha}\left(\log\left(\frac{1}{n}(\exp\{−\alpha \lambda 𝑋^{(1)}\}+\exp\{−\alpha \lambda 𝑋^{(2)}\}+\ldots + \exp\{−\alpha \lambda 𝑋^{(n)}\})\right)\right)'=\\ =−\frac{n}{\alpha}\cdot \frac{\left(\exp\{−\alpha \lambda 𝑋^{(1)}\}+\exp\{−\alpha \lambda 𝑋^{(2)}\}+\ldots + \exp\{−\alpha \lambda 𝑋^{(n)}\}\right)'}{\exp\{−\alpha \lambda 𝑋^{(1)}\}+\exp\{−\alpha \lambda 𝑋^{(2)}\}+\ldots + \exp\{−\alpha \lambda 𝑋^{(n)}\}}=\\ =−\frac{n}{\alpha}\cdot \frac{{\sum_{i=1}^n (\exp\{−\alpha \lambda 𝑋^{(i)}\}})'}{\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\}}=\\ =−\frac{n}{\alpha}\cdot \frac{-\alpha{\sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}}}{\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\}}=\\ =n\cdot\frac{{\sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}}}{\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\}}.$$ $$f''(\lambda X) = \left(n \cdot \frac{{\sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}}}{\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\}}\right)'= \left(\frac{u(\lambda)}{v(\lambda)}\right)'=\\ =n \cdot\frac{({\sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}})'\cdot \sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\} - \sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}\cdot(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})'}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=\\ =n \cdot \frac{{\sum_{i=1}^n (X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}})'\cdot \exp\{−\alpha \lambda 𝑋^{(i)}\} - \sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}\cdot(\exp\{−\alpha \lambda 𝑋^{(i)}\})'}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=\\ =n \cdot \frac{{\sum_{i=1}^n \exp\{−\alpha \lambda 𝑋^{(i)}\}((X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}})'  -  X^{(i)}\cdot(\exp\{−\alpha \lambda 𝑋^{(i)}\})')}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=\\ =n \cdot \frac{{\sum_{i=1}^n \exp\{−\alpha \lambda 𝑋^{(i)}\}((X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}})'  -  X^{(i)}\cdot(\exp\{−\alpha \lambda 𝑋^{(i)}\})')}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=\\ =n \cdot \frac{{\sum_{i=1}^n X^{(i)} \cdot \exp\{−\alpha \lambda 𝑋^{(i)}\}\cdot  (\color{red}{(\exp\{−\alpha \lambda X^{(i)}\})'-(\exp\{−\alpha \lambda X^{(i)}\})'})}}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=0. $$ I have found that $f''_{\lambda \lambda}(\lambda X)=0$ because in the numerator we have $((\exp\{−\alpha \lambda 𝑋^{(i)}\})'  -  (\exp\{−\alpha \lambda 𝑋^{(i)}\})')=0$ and therefore $f(\lambda X)$ is not strictly concave , it is the concave function only. Question. I need the solution verification or critical comments. My be this Q&A is usefull for verification. Literature Miyahara Y. (2010). “Risk-Sensitive Value Measure Method for Projects Evaluation,” J. Option and Strategy, 2, 185-204.","For an optimisation problem, it is required to prove: is the utility function strictly concave (strictly convex-up) function  of the on an open set? Here , is a sample size, 's are random samples from leaner space . The notations are borrowed from (Miyahara, 2010) , is a scaled value measure and is a scale parameter. From first principles: A twice differentiable function of one variable is convex-up (strictly convex-up) on an interval if and only if its second derivative is non-negative (negative) there. In my case I should test: . My solution is: I have found that because in the numerator we have and therefore is not strictly concave , it is the concave function only. Question. I need the solution verification or critical comments. My be this Q&A is usefull for verification. Literature Miyahara Y. (2010). “Risk-Sensitive Value Measure Method for Projects Evaluation,” J. Option and Strategy, 2, 185-204.","f(\lambda X)=−\frac{1}{\alpha}\log\left(\frac{1}{n}
\sum_{i=1}^n \exp\{−\alpha \lambda 𝑋^{(𝑖)}\}\right), \lambda 0\leq \alpha \leq 1 n 𝑋^{(𝑖)} X g_X(\lambda):=f(\lambda X) \lambda f''_{\lambda \lambda}(\lambda X)<0 f'(\lambda X)=−\frac{1}{\alpha}\left(\log\left(\frac{1}{n}(\exp\{−\alpha \lambda 𝑋^{(1)}\}+\exp\{−\alpha \lambda 𝑋^{(2)}\}+\ldots + \exp\{−\alpha \lambda 𝑋^{(n)}\})\right)\right)'=\\
=−\frac{n}{\alpha}\cdot \frac{\left(\exp\{−\alpha \lambda 𝑋^{(1)}\}+\exp\{−\alpha \lambda 𝑋^{(2)}\}+\ldots + \exp\{−\alpha \lambda 𝑋^{(n)}\}\right)'}{\exp\{−\alpha \lambda 𝑋^{(1)}\}+\exp\{−\alpha \lambda 𝑋^{(2)}\}+\ldots + \exp\{−\alpha \lambda 𝑋^{(n)}\}}=\\
=−\frac{n}{\alpha}\cdot \frac{{\sum_{i=1}^n (\exp\{−\alpha \lambda 𝑋^{(i)}\}})'}{\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\}}=\\
=−\frac{n}{\alpha}\cdot \frac{-\alpha{\sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}}}{\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\}}=\\
=n\cdot\frac{{\sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}}}{\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\}}. f''(\lambda X) = \left(n \cdot \frac{{\sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}}}{\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\}}\right)'= \left(\frac{u(\lambda)}{v(\lambda)}\right)'=\\
=n \cdot\frac{({\sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}})'\cdot \sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\} - \sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}\cdot(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})'}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=\\
=n \cdot \frac{{\sum_{i=1}^n (X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}})'\cdot \exp\{−\alpha \lambda 𝑋^{(i)}\} - \sum_{i=1}^n X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}\cdot(\exp\{−\alpha \lambda 𝑋^{(i)}\})'}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=\\
=n \cdot \frac{{\sum_{i=1}^n \exp\{−\alpha \lambda 𝑋^{(i)}\}((X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}})'  -  X^{(i)}\cdot(\exp\{−\alpha \lambda 𝑋^{(i)}\})')}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=\\
=n \cdot \frac{{\sum_{i=1}^n \exp\{−\alpha \lambda 𝑋^{(i)}\}((X^{(i)}\exp\{−\alpha \lambda 𝑋^{(i)}\}})'  -  X^{(i)}\cdot(\exp\{−\alpha \lambda 𝑋^{(i)}\})')}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=\\
=n \cdot \frac{{\sum_{i=1}^n X^{(i)} \cdot \exp\{−\alpha \lambda 𝑋^{(i)}\}\cdot  (\color{red}{(\exp\{−\alpha \lambda X^{(i)}\})'-(\exp\{−\alpha \lambda X^{(i)}\})'})}}{(\sum_{i=1}^n\exp\{−\alpha \lambda 𝑋^{(i)}\})^2}=0.
 f''_{\lambda \lambda}(\lambda X)=0 ((\exp\{−\alpha \lambda 𝑋^{(i)}\})'  -  (\exp\{−\alpha \lambda 𝑋^{(i)}\})')=0 f(\lambda X)","['calculus', 'derivatives', 'solution-verification', 'convex-optimization']"
93,Extrema points of a non-differentiable function.,Extrema points of a non-differentiable function.,,"Say I have a function $f\left(x\right)=\left|\frac{x^{2}-2}{x^{2}-1}\right|$ . I have to find the local minima/maxima of the function. The point is that this function is non-differentiable at $x=\pm\sqrt{2},\pm1$ . When i try to break free of the modulus, I end up getting a piece-wise function. How do I find the extrema points in this case ( I do not want to go by graph method) ? Because, sure I can find the critical points, which in this case is $x=\pm\sqrt{2},\pm1,\ 0$ . But how do I use the first derivative test to check whether these points are actually extrema or not. So basically my question is how do we go about finding local minima/maxima without any graphical method? Any hints or approaches are welcome. I don't want answer of this particular question, I just want to know the concept/method.","Say I have a function . I have to find the local minima/maxima of the function. The point is that this function is non-differentiable at . When i try to break free of the modulus, I end up getting a piece-wise function. How do I find the extrema points in this case ( I do not want to go by graph method) ? Because, sure I can find the critical points, which in this case is . But how do I use the first derivative test to check whether these points are actually extrema or not. So basically my question is how do we go about finding local minima/maxima without any graphical method? Any hints or approaches are welcome. I don't want answer of this particular question, I just want to know the concept/method.","f\left(x\right)=\left|\frac{x^{2}-2}{x^{2}-1}\right| x=\pm\sqrt{2},\pm1 x=\pm\sqrt{2},\pm1,\ 0","['calculus', 'derivatives']"
94,We have $(x^2+y^2)^2-3(x^2+y^2)+1=0$. What is the value of $\frac{d^2y}{dx^2}$?,We have . What is the value of ?,(x^2+y^2)^2-3(x^2+y^2)+1=0 \frac{d^2y}{dx^2},"We have $(x^2+y^2)^2-3(x^2+y^2)+1=0$ . What is the value of $\frac{d^2y}{dx^2}$ ? $1)-\frac{x^2+y^2}{y^2}\qquad\qquad2)-\frac{x^2+y^2}{y^3}\qquad\qquad3)\frac{x+y}{x^2+y^2}\qquad\qquad4)\frac{xy}{x^2+y^2}$ Here is my approach: We have a quadratic equation in $x^2+y^2$ . So $x^2+y^2=\frac{3\pm\sqrt5}{2}$ (RHS is a constant). Taking differentiate with respect to $x$ , $$2x+2yy'=0\Rightarrow\quad y'=\frac{-x}{y}$$ $$2+2y'^2+2yy''=0\Rightarrow \quad y''=\frac{-y'^2}{y}\Rightarrow y''=\frac{-x^2}{y^3}$$ Edit: The second option can be written as $-(\frac{x^2}{y^3}+\frac{1}y)$ . It seems it is the correct answer. But why I missed $-\frac1y$ in this approach?","We have . What is the value of ? Here is my approach: We have a quadratic equation in . So (RHS is a constant). Taking differentiate with respect to , Edit: The second option can be written as . It seems it is the correct answer. But why I missed in this approach?",(x^2+y^2)^2-3(x^2+y^2)+1=0 \frac{d^2y}{dx^2} 1)-\frac{x^2+y^2}{y^2}\qquad\qquad2)-\frac{x^2+y^2}{y^3}\qquad\qquad3)\frac{x+y}{x^2+y^2}\qquad\qquad4)\frac{xy}{x^2+y^2} x^2+y^2 x^2+y^2=\frac{3\pm\sqrt5}{2} x 2x+2yy'=0\Rightarrow\quad y'=\frac{-x}{y} 2+2y'^2+2yy''=0\Rightarrow \quad y''=\frac{-y'^2}{y}\Rightarrow y''=\frac{-x^2}{y^3} -(\frac{x^2}{y^3}+\frac{1}y) -\frac1y,"['calculus', 'derivatives']"
95,Proving $\frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}} = \frac{1}{(1+x)^2}$ with $x\sqrt{1+y}+y\sqrt{1+x}=0$,Proving  with,\frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}} = \frac{1}{(1+x)^2} x\sqrt{1+y}+y\sqrt{1+x}=0,"I want to find $\frac{dy}{dx}$ in $x\sqrt{1+y}+y\sqrt{1+x}=0$ and I proceed through 2 different ways expecting the same answer. Method:-1 $x\sqrt{1+y}=-y\sqrt{1+x}$ $\implies x^2(1+y)=y^2(1+x)$ $\vdots$ $\implies y=-1+\frac{1}{1+x}$ $$\frac{dy}{dx}=\frac{-1}{(1+x)^2}$$ Method:-2 $\phi(x,y)=x\sqrt{1+y}+y\sqrt{1+x}=0$ $$\frac{dy}{dx}=-\frac{\frac{\partial {\phi}}{\partial x}}{ \frac{\partial {\phi}}{\partial y}  }$$ $$\frac{dy}{dx}=-\frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}}$$ But I could not find any way to convert $\frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}}$ to $\frac{1}{(1+x)^2}$ . Thank you for your help!",I want to find in and I proceed through 2 different ways expecting the same answer. Method:-1 Method:-2 But I could not find any way to convert to . Thank you for your help!,"\frac{dy}{dx} x\sqrt{1+y}+y\sqrt{1+x}=0 x\sqrt{1+y}=-y\sqrt{1+x} \implies x^2(1+y)=y^2(1+x) \vdots \implies y=-1+\frac{1}{1+x} \frac{dy}{dx}=\frac{-1}{(1+x)^2} \phi(x,y)=x\sqrt{1+y}+y\sqrt{1+x}=0 \frac{dy}{dx}=-\frac{\frac{\partial {\phi}}{\partial x}}{ \frac{\partial {\phi}}{\partial y}  } \frac{dy}{dx}=-\frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}} \frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}} \frac{1}{(1+x)^2}","['calculus', 'derivatives', 'arithmetic']"
96,Does convergence of integral series and second derivative series imply convergence?,Does convergence of integral series and second derivative series imply convergence?,,"Let $f_n : \mathbb R \to \mathbb [0,\infty)$ be a sequence of smooth functions such that $$\sum_{n=1}^\infty \int_{\mathbb R} f_n(x) dx$$ is convergent. Further, we have that $$\sum_{n=1}^\infty f_n''(x)$$ converges for every $x \in \mathbb R$ . Does this imply that $$\sum_{n=1}^\infty f_n(x)$$ converges for all $x\in\mathbb R$ ?","Let be a sequence of smooth functions such that is convergent. Further, we have that converges for every . Does this imply that converges for all ?","f_n : \mathbb R \to \mathbb [0,\infty) \sum_{n=1}^\infty \int_{\mathbb R} f_n(x) dx \sum_{n=1}^\infty f_n''(x) x \in \mathbb R \sum_{n=1}^\infty f_n(x) x\in\mathbb R","['real-analysis', 'integration', 'derivatives', 'convergence-divergence']"
97,Connection between the Jacobian of a vector function and exterior derivative of associated differential form.,Connection between the Jacobian of a vector function and exterior derivative of associated differential form.,,"Let $A\subseteq\mathbb{R}^n$ be an open set and \begin{equation} F:A\rightarrow R^n \qquad F\in C^1(A) \end{equation} a vector field. The Jacobian matrix is defined as: \begin{equation} J(\boldsymbol{x})=\left(\frac{\partial F_i}{\partial x_j}\right)  \quad \forall\boldsymbol{x}\in A \quad i,j=1...n \end{equation} Since we are not dealing with $\mathbb{R^3}$ , I shall talk about exterior derivatives rather than curl. Now, the exterior derivative of a 0-form (scalar function) is the differential, whose representative vector is the gradient of the function. The exterior derivative of a differential 1-form $\omega$ is the 2-form $d\omega$ that can be represented through a proper matrix. On the other hand, the Jacobian matrix plays for vector field the same role that the gradient plays for scalar functions, so by analogy with 0-forms, does the Jacobian matrix of the vector field associated with $\omega$ represent $d\omega$ ? If it doesn't, which of these two is the differential $dF$ of the  vector field? Does this happen only for 0-forms?","Let be an open set and a vector field. The Jacobian matrix is defined as: Since we are not dealing with , I shall talk about exterior derivatives rather than curl. Now, the exterior derivative of a 0-form (scalar function) is the differential, whose representative vector is the gradient of the function. The exterior derivative of a differential 1-form is the 2-form that can be represented through a proper matrix. On the other hand, the Jacobian matrix plays for vector field the same role that the gradient plays for scalar functions, so by analogy with 0-forms, does the Jacobian matrix of the vector field associated with represent ? If it doesn't, which of these two is the differential of the  vector field? Does this happen only for 0-forms?","A\subseteq\mathbb{R}^n \begin{equation}
F:A\rightarrow R^n \qquad F\in C^1(A)
\end{equation} \begin{equation}
J(\boldsymbol{x})=\left(\frac{\partial F_i}{\partial x_j}\right)  \quad \forall\boldsymbol{x}\in A
\quad i,j=1...n
\end{equation} \mathbb{R^3} \omega d\omega \omega d\omega dF","['derivatives', 'differential-geometry', 'vector-fields', 'exterior-algebra', 'jacobian']"
98,Is $ \int \frac{f'(x)}{f(x)} \ dx = \log |f(x)| + C$ true for all differentiable functions $f$?,Is  true for all differentiable functions ?, \int \frac{f'(x)}{f(x)} \ dx = \log |f(x)| + C f,"Let $f$ be a differentiable function. Is the following identity true for all such $f$ ? $$ \int \frac{f'(x)}{f(x)} \ dx = \log |f(x)| +  C $$ I ask because there exist differentiable functions whose derivatives are not Riemann integrable (see here for instance). On the other hand, if we use the substitution $u = f(x)$ for $f$ on $[a,b]$ , $$ \int_a^b \frac{f'(x)}{f(x)} \ dx  = \int_{f(a)}^{f(b)} \frac{1}{u} \ du $$ and the RHS appears to be integrable. How can we reconcile this? Any comments, help and explanations are welcome.","Let be a differentiable function. Is the following identity true for all such ? I ask because there exist differentiable functions whose derivatives are not Riemann integrable (see here for instance). On the other hand, if we use the substitution for on , and the RHS appears to be integrable. How can we reconcile this? Any comments, help and explanations are welcome.","f f 
\int \frac{f'(x)}{f(x)} \ dx = \log |f(x)| +  C
 u = f(x) f [a,b] 
\int_a^b \frac{f'(x)}{f(x)} \ dx  = \int_{f(a)}^{f(b)} \frac{1}{u} \ du
","['calculus', 'integration', 'derivatives', 'riemann-integration']"
99,"The derivative of the cumulative distribution of $\min(X_1, a_1) + \min(X_2, a_2)$",The derivative of the cumulative distribution of,"\min(X_1, a_1) + \min(X_2, a_2)","Assume $X_1, X_2$ are continuous random variables in $L^{\infty}$ . Let $a_1, a_2$ be real numbers and $$Y := \min(X_1, a_1)+ \min(X_2, a_2).$$ The dependency structure between $X_1$ and $ X_2$ is not known, but we assume that the joint and marginal distributions are well-defined. Is there a way to calculate \begin{align} \frac{d}{da_i} F_{Y}(t),  \end{align} where $F_{Y}(t) := P(\min(X_1, a_1)+ \min(X_2, a_2) \leq t)$ ? I have tried to find an explicit expression for $Y$ , with the idea to express the cumulative distribution function as a double integral, with the hope of appealing to the fundamental theorem of calculus, but I cannot seem to find an expression that makes sense.","Assume are continuous random variables in . Let be real numbers and The dependency structure between and is not known, but we assume that the joint and marginal distributions are well-defined. Is there a way to calculate where ? I have tried to find an explicit expression for , with the idea to express the cumulative distribution function as a double integral, with the hope of appealing to the fundamental theorem of calculus, but I cannot seem to find an expression that makes sense.","X_1, X_2 L^{\infty} a_1, a_2 Y := \min(X_1, a_1)+ \min(X_2, a_2). X_1  X_2 \begin{align}
\frac{d}{da_i} F_{Y}(t), 
\end{align} F_{Y}(t) := P(\min(X_1, a_1)+ \min(X_2, a_2) \leq t) Y","['probability', 'integration', 'derivatives']"
