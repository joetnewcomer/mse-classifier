,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,evaluating $\int_0^\infty \int_y^\infty y^2e^{-x^4} \ dx \ dy$,evaluating,\int_0^\infty \int_y^\infty y^2e^{-x^4} \ dx \ dy,evaluating $\int_0^\infty \int_y^\infty y^2e^{-x^4} \ dx \ dy$ my book states  $$\int_0^\infty \int_y^\infty y^2e^{-x^4} \ dx \ dy = \int_0^\infty \int_0^x y^2e^{-x^4} \ dy \ dx$$ Could someone explain how the domains have changed?,evaluating $\int_0^\infty \int_y^\infty y^2e^{-x^4} \ dx \ dy$ my book states  $$\int_0^\infty \int_y^\infty y^2e^{-x^4} \ dx \ dy = \int_0^\infty \int_0^x y^2e^{-x^4} \ dy \ dx$$ Could someone explain how the domains have changed?,,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
1,"Langrange Multiplier, to find maximum volume of a cone","Langrange Multiplier, to find maximum volume of a cone",,"Question: A right-angled triangle is rotated about one of its sides that form the right angle to a cone. Given that the sum of the lengths of two sides of the triangle that form the right angle is $P$, find these lengths that would maximize the volume of the cone. My attempt: I know there must be a use of langrange multipliers but I am not too sure how to implement it.","Question: A right-angled triangle is rotated about one of its sides that form the right angle to a cone. Given that the sum of the lengths of two sides of the triangle that form the right angle is $P$, find these lengths that would maximize the volume of the cone. My attempt: I know there must be a use of langrange multipliers but I am not too sure how to implement it.",,"['calculus', 'multivariable-calculus']"
2,Does anyone know a book on sketching surfaces?,Does anyone know a book on sketching surfaces?,,"Is anyone aware of books or sources dedicated to sketching surfaces? Sort of like Forst's An Elementary Treatise on Curve Tracing, but on surfaces; or a book that has a fair few chapters dedicated to this matter (it could be on anything - multivariable calculus, solid geometry etc). I've found books on analytical geometry in three dimensions but there isn't much sketching there.","Is anyone aware of books or sources dedicated to sketching surfaces? Sort of like Forst's An Elementary Treatise on Curve Tracing, but on surfaces; or a book that has a fair few chapters dedicated to this matter (it could be on anything - multivariable calculus, solid geometry etc). I've found books on analytical geometry in three dimensions but there isn't much sketching there.",,"['multivariable-calculus', 'differential-geometry', 'reference-request', 'book-recommendation', 'solid-geometry']"
3,Embedding of $2$-torus minus one point into $\mathbb{R}^2$ as an open set?,Embedding of -torus minus one point into  as an open set?,2 \mathbb{R}^2,Let $M^2$ be the $2$-torus minus one point. Is there an embedding of $M^2$ into $\mathbb{R}^2$ as an open set?,Let $M^2$ be the $2$-torus minus one point. Is there an embedding of $M^2$ into $\mathbb{R}^2$ as an open set?,,"['multivariable-calculus', 'differential-geometry']"
4,Existence of a positively homogeneous function of degree $k$,Existence of a positively homogeneous function of degree,k,"Question: For every $k \in \mathbb R$ show that there exists a function $f: \mathbb R^m - \{0\} \to \mathbb R$ of class $C^{\infty}$, positively homogeneous of degree $k$, that is, $f(tx) = t^k f(x)$ , for every $x \in \mathbb R^m - \{0\}$, with $t > 0$, such that $f(x) > 0$ for all $x$ and $f$ is not a polynomial. Attempt: I have tried to work with $$f(x) = \frac{1}{k\displaystyle\sum_{j=1}^{m}\frac{1}{x_j^k}}$$ Then $f(tx) = t^k f(x)$. But I have failed to show that $f(x) > 0$. From $$f(tx) - f(x) = f'(x) \cdot (t-1)x + \rho(x) (t-1) |x|\tag{*}$$ and $$f'(x) \cdot x = \frac{f(x)}{k} $$ I took $t \to 1^+$ and got $k^2 = 1$. Any thoughts are welcome.","Question: For every $k \in \mathbb R$ show that there exists a function $f: \mathbb R^m - \{0\} \to \mathbb R$ of class $C^{\infty}$, positively homogeneous of degree $k$, that is, $f(tx) = t^k f(x)$ , for every $x \in \mathbb R^m - \{0\}$, with $t > 0$, such that $f(x) > 0$ for all $x$ and $f$ is not a polynomial. Attempt: I have tried to work with $$f(x) = \frac{1}{k\displaystyle\sum_{j=1}^{m}\frac{1}{x_j^k}}$$ Then $f(tx) = t^k f(x)$. But I have failed to show that $f(x) > 0$. From $$f(tx) - f(x) = f'(x) \cdot (t-1)x + \rho(x) (t-1) |x|\tag{*}$$ and $$f'(x) \cdot x = \frac{f(x)}{k} $$ I took $t \to 1^+$ and got $k^2 = 1$. Any thoughts are welcome.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
5,When is the rank of Jacobian constant?,When is the rank of Jacobian constant?,,"Suppose I've got a function $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ which I know is bijective. Considering $\mathcal{J}$, the Jacobian of $\ f$, I want to understand what can be said about the rank of $\mathcal{J}(\mathbf{x})$. Let's say I evaluate $\mathcal{J}(\mathbf{0})$, and find that the rank of $\mathcal{J}(\mathbf{0})$ is $k$. Does this mean that the rank of $\mathcal{J}(\mathbf{x})$ is $k$ for all $\mathbf{x} \in \mathbb{R}^{n}$? Is there a theorem regarding this? EDIT: If this is not enough that $f$ is a bijection, what if $f$ is a homeomorphism? Or of class $C^{\infty}$?","Suppose I've got a function $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ which I know is bijective. Considering $\mathcal{J}$, the Jacobian of $\ f$, I want to understand what can be said about the rank of $\mathcal{J}(\mathbf{x})$. Let's say I evaluate $\mathcal{J}(\mathbf{0})$, and find that the rank of $\mathcal{J}(\mathbf{0})$ is $k$. Does this mean that the rank of $\mathcal{J}(\mathbf{x})$ is $k$ for all $\mathbf{x} \in \mathbb{R}^{n}$? Is there a theorem regarding this? EDIT: If this is not enough that $f$ is a bijection, what if $f$ is a homeomorphism? Or of class $C^{\infty}$?",,"['multivariable-calculus', 'matrix-rank']"
6,How to show $\operatorname{curl}\operatorname{curl}(e_r) = 0$,How to show,\operatorname{curl}\operatorname{curl}(e_r) = 0,"$\DeclareMathOperator{curl}{curl}$ I want to figure out how to calculate $\text{curl}(e_r$ ). Where $e_r$ is a base vector for the Spherical co-ordinate system. Taking $e_r = (\sin\theta \cos\phi)i+(\sin\theta \sin\phi)j+(\cos\theta)k$ and I tried taking the $\text{curl}(e_r)$ as follows, $\text{curl }e_r=\begin{vmatrix}i & j & k\\ \frac{\partial}{\partial r} & \frac{\partial}{\partial \theta} & \frac{\partial}{\partial \phi}  \\ (\sin\theta \cos\phi) & (\sin\theta \sin\phi) & (\cos\theta)\end{vmatrix}$ And then took the $\operatorname{curl}$ again for the output of the above. But this gives a non-zero value. I would like to know whether the above steps and the values I am using for $e_r$ are correct. If not can you please tell me what I am doing wrong.","I want to figure out how to calculate ). Where is a base vector for the Spherical co-ordinate system. Taking and I tried taking the as follows, And then took the again for the output of the above. But this gives a non-zero value. I would like to know whether the above steps and the values I am using for are correct. If not can you please tell me what I am doing wrong.",\DeclareMathOperator{curl}{curl} \text{curl}(e_r e_r e_r = (\sin\theta \cos\phi)i+(\sin\theta \sin\phi)j+(\cos\theta)k \text{curl}(e_r) \text{curl }e_r=\begin{vmatrix}i & j & k\\ \frac{\partial}{\partial r} & \frac{\partial}{\partial \theta} & \frac{\partial}{\partial \phi}  \\ (\sin\theta \cos\phi) & (\sin\theta \sin\phi) & (\cos\theta)\end{vmatrix} \operatorname{curl} e_r,"['multivariable-calculus', 'vector-analysis', 'spherical-coordinates', 'curl']"
7,"Why do we need both partial derivatives for $f(x, y)$ to guarantee that we can find min & max points?",Why do we need both partial derivatives for  to guarantee that we can find min & max points?,"f(x, y)","Given for example a function of 2 variables $z = f(x,y) = x^2 + xy + y^2$, I believe there are infinitely many directions on the $xy$ plane in which we could take partial derivatives (this could already be wrong...). If we differentiate $z$ w.r.t $x$, we get $f_x = 2x+y$. If we set this partial derivative equal to zero, we get infinitely many solutions; namely it seems that for any value of $y$, we could find a value of $x$ such that at the point $(x,y)$ the slope in the direction of the $x$ axis would be zero. However, only one of these infinitely many points is actually the minimum of the function. But if we take the other partial derivative $f_y = 2y + x$ and set both of them equal to zero as a system of equations and solve that system, we will get our minimum point at $(0,0)$. Why is that? Why is it that we need both partial derivatives and not fewer, or especially more ? EDIT: Just to make it clear, I'm wondering both why two PD's are the minimum, and also why they are guaranteed to be enough (as in, why don't we need 20 PD's?).","Given for example a function of 2 variables $z = f(x,y) = x^2 + xy + y^2$, I believe there are infinitely many directions on the $xy$ plane in which we could take partial derivatives (this could already be wrong...). If we differentiate $z$ w.r.t $x$, we get $f_x = 2x+y$. If we set this partial derivative equal to zero, we get infinitely many solutions; namely it seems that for any value of $y$, we could find a value of $x$ such that at the point $(x,y)$ the slope in the direction of the $x$ axis would be zero. However, only one of these infinitely many points is actually the minimum of the function. But if we take the other partial derivative $f_y = 2y + x$ and set both of them equal to zero as a system of equations and solve that system, we will get our minimum point at $(0,0)$. Why is that? Why is it that we need both partial derivatives and not fewer, or especially more ? EDIT: Just to make it clear, I'm wondering both why two PD's are the minimum, and also why they are guaranteed to be enough (as in, why don't we need 20 PD's?).",,"['multivariable-calculus', 'partial-derivative']"
8,On a step of a proof of the mean value theorem in several variables.,On a step of a proof of the mean value theorem in several variables.,,"I am following this proof of the mean value theorem And I am a bit stuck on the last line, to be precise, why is $$f(a +t[b-a] + h[b-a]) - f(a + t[b-a]) = \bigtriangledown f(a+t[b-a]) (h[b-a]) + o(h [b-a]) ?$$ I am familiar with little oh notation but I need a reminder on what happened here, does someone mind to lend a hand?","I am following this proof of the mean value theorem And I am a bit stuck on the last line, to be precise, why is I am familiar with little oh notation but I need a reminder on what happened here, does someone mind to lend a hand?",f(a +t[b-a] + h[b-a]) - f(a + t[b-a]) = \bigtriangledown f(a+t[b-a]) (h[b-a]) + o(h [b-a]) ?,"['real-analysis', 'multivariable-calculus']"
9,"Switch to polar coordinates and Then change order of integral $\int_{0}^{1} \int_{0}^{x^2} f(x,y) dy dx$",Switch to polar coordinates and Then change order of integral,"\int_{0}^{1} \int_{0}^{x^2} f(x,y) dy dx","Substitute to polar coordinates and change order of integral $$\int_{0}^{1} \int_{0}^{x^2} f(x,y) dy dx$$ I could substitute to polar coordinates, but failed to change the order of integral $x=r\cos\theta$, $y=r\sin\theta$. $$\int_{0}^{1} \int_{0}^{x^2} f(x,y) dy dx = \int_{0}^{\frac{\pi}{4}}d\theta \int_{\frac{\sin\theta}{{\cos^2\theta}}}^{\frac{1}{\cos\theta}} r f(r\cos\theta,r\sin\theta) dr$$. ADDED $r=\frac{1}{cos\theta}$, $\theta = \arccos\frac{1}{r}$ $r=\frac{\sin\theta}{{\cos^2\theta}}$, $\sin\theta= r(1-sin^2\theta)$, $sin\theta = \frac{-1\pm \sqrt{1+4r^2}}{2r}$, $\theta = \arcsin \frac{-1\pm \sqrt{1+4r^2}}{2r}$ when $\theta = 0$, $r=0$ to $r=1$, when $\theta = \frac{\pi}{4}$, $r=\sqrt{2}$ My problem is, that when $r \rightarrow 0$, I am not able to see what is happening. I need $r$, $\theta$ dependency graph.","Substitute to polar coordinates and change order of integral $$\int_{0}^{1} \int_{0}^{x^2} f(x,y) dy dx$$ I could substitute to polar coordinates, but failed to change the order of integral $x=r\cos\theta$, $y=r\sin\theta$. $$\int_{0}^{1} \int_{0}^{x^2} f(x,y) dy dx = \int_{0}^{\frac{\pi}{4}}d\theta \int_{\frac{\sin\theta}{{\cos^2\theta}}}^{\frac{1}{\cos\theta}} r f(r\cos\theta,r\sin\theta) dr$$. ADDED $r=\frac{1}{cos\theta}$, $\theta = \arccos\frac{1}{r}$ $r=\frac{\sin\theta}{{\cos^2\theta}}$, $\sin\theta= r(1-sin^2\theta)$, $sin\theta = \frac{-1\pm \sqrt{1+4r^2}}{2r}$, $\theta = \arcsin \frac{-1\pm \sqrt{1+4r^2}}{2r}$ when $\theta = 0$, $r=0$ to $r=1$, when $\theta = \frac{\pi}{4}$, $r=\sqrt{2}$ My problem is, that when $r \rightarrow 0$, I am not able to see what is happening. I need $r$, $\theta$ dependency graph.",,['multivariable-calculus']
10,Is it possible for this function on $R^2$ to be defined continuously at $0$?,Is it possible for this function on  to be defined continuously at ?,R^2 0,"I wont post all the functions.I am practicing on continuity.So i wanna use definitions but i am not really sure how? Suppose i have the function $$f(x)= \frac{x^4-y^4}{(x^2+y^2)^2}$$ When $(x,y)\neq (0,0)$ and $f(x)=b $ when $(x,y)=(0,0)$ Proof:To be defined continouesly at $0$ the limit at zero must exist.So there must exist the $$\lim_{(x,y)\to(0,0)}\frac{x^4-y^4}{(x^2+y^2)^2} $$ Or if i do a variables change to polar coordinates  $x=rcosθ $  $y=rsinθ$  then$$\lim_{(r,θ)\to(0,0)} cos^2θ-sin^2θ $$ but that function does not depend on $r$  and so will give the same limit for All $r's$ since it must be unique?. So i said the function cannot be defined continuously .Is it right?Does not seem that rigorous to me i think i need a better explanation as to why it might not be defined and why when it is not depended on both variables it doesnt work. Also i would like a proof using other  definitions(open sets of the preimage or  the ε,δ,) because i do not know how to use the other definitions except that a function is continuous to a point iff when x-->xo then f(x)-->f(xo).","I wont post all the functions.I am practicing on continuity.So i wanna use definitions but i am not really sure how? Suppose i have the function $$f(x)= \frac{x^4-y^4}{(x^2+y^2)^2}$$ When $(x,y)\neq (0,0)$ and $f(x)=b $ when $(x,y)=(0,0)$ Proof:To be defined continouesly at $0$ the limit at zero must exist.So there must exist the $$\lim_{(x,y)\to(0,0)}\frac{x^4-y^4}{(x^2+y^2)^2} $$ Or if i do a variables change to polar coordinates  $x=rcosθ $  $y=rsinθ$  then$$\lim_{(r,θ)\to(0,0)} cos^2θ-sin^2θ $$ but that function does not depend on $r$  and so will give the same limit for All $r's$ since it must be unique?. So i said the function cannot be defined continuously .Is it right?Does not seem that rigorous to me i think i need a better explanation as to why it might not be defined and why when it is not depended on both variables it doesnt work. Also i would like a proof using other  definitions(open sets of the preimage or  the ε,δ,) because i do not know how to use the other definitions except that a function is continuous to a point iff when x-->xo then f(x)-->f(xo).",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'continuity']"
11,Calculating derivative of a special matrix exponential,Calculating derivative of a special matrix exponential,,"I am trying to calculate the following derivative: Let $V,W$ be fixed $n \times n$ real matrices. Define $A(t)=e^{V+tW}$. What is $A'(0)$? I do not assume $V,W$ commute. (If they commute, it becomes trivial since then $A(t)=e^Ve^{tW}$). Here is the problem as I see it: Define $A(t)=e^{\alpha(t)}$ where $\alpha(t)$ is a (smooth) path inside $Mat(n,\mathbb{R})$. $e^{\alpha(t)}=\sum_{k=0}^\infty \frac{\alpha(t)^k}{k!} \Rightarrow (e^{\alpha(t)})'=\sum_{k=0}^\infty \frac{(\alpha(t)^k)'}{k!}$ But the formula $(\alpha^k)'=k(\alpha^{k-1})\alpha'$ is not necessarily valid if $\alpha,\alpha'$ do not commute. Here is what happens when we try to calculate $(\alpha^2)'$: By the Leibniz rule: $(\alpha^2)'=(\alpha \cdot \alpha)'=\alpha' \cdot \alpha + \alpha \cdot \alpha' \neq 2\alpha \cdot \alpha'$ if $\alpha,\alpha'$ do not commute. The general formula is* $(\alpha(t)^n)'=\sum_{k=1}^n\alpha(t)^{k-1}\alpha'(t)\alpha(t)^{n-k}$, so plugging this into the series formula of matrix exponential does not give you something managable, at least not immediately. Is there a way to claculate (or at least give an estimate of) the derivative $A'(0)$? Can we deduce if it's zero? Another approach is maybe to use the Baker–Campbell–Hausdorff formula but so far it didn't work for me. *Note that while in general it seems that there is no simpler closed expression for $(\alpha^n)'$, maybe there is till hope that the specific ""exponential series"" with this term plugged in is tractable in some sense.","I am trying to calculate the following derivative: Let $V,W$ be fixed $n \times n$ real matrices. Define $A(t)=e^{V+tW}$. What is $A'(0)$? I do not assume $V,W$ commute. (If they commute, it becomes trivial since then $A(t)=e^Ve^{tW}$). Here is the problem as I see it: Define $A(t)=e^{\alpha(t)}$ where $\alpha(t)$ is a (smooth) path inside $Mat(n,\mathbb{R})$. $e^{\alpha(t)}=\sum_{k=0}^\infty \frac{\alpha(t)^k}{k!} \Rightarrow (e^{\alpha(t)})'=\sum_{k=0}^\infty \frac{(\alpha(t)^k)'}{k!}$ But the formula $(\alpha^k)'=k(\alpha^{k-1})\alpha'$ is not necessarily valid if $\alpha,\alpha'$ do not commute. Here is what happens when we try to calculate $(\alpha^2)'$: By the Leibniz rule: $(\alpha^2)'=(\alpha \cdot \alpha)'=\alpha' \cdot \alpha + \alpha \cdot \alpha' \neq 2\alpha \cdot \alpha'$ if $\alpha,\alpha'$ do not commute. The general formula is* $(\alpha(t)^n)'=\sum_{k=1}^n\alpha(t)^{k-1}\alpha'(t)\alpha(t)^{n-k}$, so plugging this into the series formula of matrix exponential does not give you something managable, at least not immediately. Is there a way to claculate (or at least give an estimate of) the derivative $A'(0)$? Can we deduce if it's zero? Another approach is maybe to use the Baker–Campbell–Hausdorff formula but so far it didn't work for me. *Note that while in general it seems that there is no simpler closed expression for $(\alpha^n)'$, maybe there is till hope that the specific ""exponential series"" with this term plugged in is tractable in some sense.",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'lie-groups', 'matrix-calculus']"
12,Showing that the set is disconnected,Showing that the set is disconnected,,"I need to prove directly from the definition that $$(x,y)\in \Bbb R^2, x^2-y^2=1$$ is disconnected. Can someone please give a methodological answer? I cannot handle this type of problems Thanks in advance!","I need to prove directly from the definition that $$(x,y)\in \Bbb R^2, x^2-y^2=1$$ is disconnected. Can someone please give a methodological answer? I cannot handle this type of problems Thanks in advance!",,"['calculus', 'multivariable-calculus']"
13,"Surface area of $x^2+y^2+z^2=9$, where $1\leq x^2+y^2\leq4$ and $z\geq0$","Surface area of , where  and",x^2+y^2+z^2=9 1\leq x^2+y^2\leq4 z\geq0,"Let $S$ be the portion of the sphere $x^2+y^2+z^2=9$, where $1\leq x^2+y^2\leq4$ and $z\geq0$. Calculate the surface area of $S$ Ok i'm really confused with this one. I know i have to apply the surface area formula but and possibly spherical coordinates but i can't seem how to get the integral out. The shape. I thought of using spherical system but after doing i ended up with a 3 coordinate system. I'm not even sure how to begin with this one.","Let $S$ be the portion of the sphere $x^2+y^2+z^2=9$, where $1\leq x^2+y^2\leq4$ and $z\geq0$. Calculate the surface area of $S$ Ok i'm really confused with this one. I know i have to apply the surface area formula but and possibly spherical coordinates but i can't seem how to get the integral out. The shape. I thought of using spherical system but after doing i ended up with a 3 coordinate system. I'm not even sure how to begin with this one.",,"['integration', 'multivariable-calculus']"
14,Calculate the limit of a function of two variables,Calculate the limit of a function of two variables,,"How to calculate this limit? $$\lim_{(x,y) \rightarrow (0,0)}\frac{x^2+y^2}{\sqrt{x^2+y^2+9}-3}$$","How to calculate this limit? $$\lim_{(x,y) \rightarrow (0,0)}\frac{x^2+y^2}{\sqrt{x^2+y^2+9}-3}$$",,"['limits', 'multivariable-calculus']"
15,"Find an equation of the sphere with center $(3,−2,1)$ and that goes through the point $(4,2,5)$",Find an equation of the sphere with center  and that goes through the point,"(3,−2,1) (4,2,5)","Find an equation of the sphere with center $(3,−2,1)$ and that goes through the point $(4,2,5)$ I did the following: $r^2=(x-h)^2+(y-k)^2+(z-l)^2$ Since that is the equation of a sphere I simply plugged in the center and the points supplied to get: $r^2=(4-3)^2+(2+2)^2+(5-1)^2 \Rightarrow r=\sqrt{33}$ This solution just provides the radius, so how do I represent this as an equation?","Find an equation of the sphere with center $(3,−2,1)$ and that goes through the point $(4,2,5)$ I did the following: $r^2=(x-h)^2+(y-k)^2+(z-l)^2$ Since that is the equation of a sphere I simply plugged in the center and the points supplied to get: $r^2=(4-3)^2+(2+2)^2+(5-1)^2 \Rightarrow r=\sqrt{33}$ This solution just provides the radius, so how do I represent this as an equation?",,"['calculus', 'geometry', 'multivariable-calculus']"
16,"Prove $\nabla u \times \nabla v =0$ is a necessary and sufficient condition that $u$ and $v$ are functionally related by the equation $F(u,v) = 0$",Prove  is a necessary and sufficient condition that  and  are functionally related by the equation,"\nabla u \times \nabla v =0 u v F(u,v) = 0","Let $u$ and $v$ be differentiable functions of $x$,  $y$ and $z$. Show   that a necessary and sufficient condition that $u$ and $v$ are   functionally related by the equation $F(u,v) = 0$ is that $$\nabla u  \times \nabla v =0$$","Let $u$ and $v$ be differentiable functions of $x$,  $y$ and $z$. Show   that a necessary and sufficient condition that $u$ and $v$ are   functionally related by the equation $F(u,v) = 0$ is that $$\nabla u  \times \nabla v =0$$",,"['multivariable-calculus', 'vector-analysis']"
17,"Prove that $\{(x,y,z) \in \mathbb{R}^3 \mid z^2-x^2-y^2-1>0 \}$ is an open set in $3$-space.",Prove that  is an open set in -space.,"\{(x,y,z) \in \mathbb{R}^3 \mid z^2-x^2-y^2-1>0 \} 3","Prove that $\{(x,y,z) \in \mathbb{R}^3 \mid z^2-x^2-y^2-1>0 \}$ is an open set in $3$-space. I'm getting no clue as to how to proceed in order to prove this formally.","Prove that $\{(x,y,z) \in \mathbb{R}^3 \mid z^2-x^2-y^2-1>0 \}$ is an open set in $3$-space. I'm getting no clue as to how to proceed in order to prove this formally.",,['multivariable-calculus']
18,The gradient is everywhere perpendicular to the contour lines of a function,The gradient is everywhere perpendicular to the contour lines of a function,,"In this article, there's something saying: The gradient is everywhere perpendicular to the contour lines of a function It justifies it by saying: Since along contour lines the change in height is zero, this means the directional derivative along the contour is zero But what is exactly a directional derivative along a contour? I only know about directional derivatives in directions of vectors. I'm asking this because I'm studying Lagrange multipliers, and my book justifies it geometrically only. It seeks to find the max or min of a function $f$ with the restriction on the domain $\{(x,y) | g(x,y) = 0\}$ It says that when $(x_0,y_0)$ is a critical point (either max or min), it means that if we trace the contour lines near this point, we'll observe a contour line of $f$ that is 'tangent' to the restriction $g$ . Here's the image: I can't see why this is true geometrically, could somebody give me a reason? I want a proof but I also want to understand it geometrically. My book then ends by saying that the gradients of $f$ and $g$ must be perpendicular. But I can't see this.","In this article, there's something saying: The gradient is everywhere perpendicular to the contour lines of a function It justifies it by saying: Since along contour lines the change in height is zero, this means the directional derivative along the contour is zero But what is exactly a directional derivative along a contour? I only know about directional derivatives in directions of vectors. I'm asking this because I'm studying Lagrange multipliers, and my book justifies it geometrically only. It seeks to find the max or min of a function with the restriction on the domain It says that when is a critical point (either max or min), it means that if we trace the contour lines near this point, we'll observe a contour line of that is 'tangent' to the restriction . Here's the image: I can't see why this is true geometrically, could somebody give me a reason? I want a proof but I also want to understand it geometrically. My book then ends by saying that the gradients of and must be perpendicular. But I can't see this.","f \{(x,y) | g(x,y) = 0\} (x_0,y_0) f g f g","['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
19,Using Partial Derivatives to find the equation of a tangent plane,Using Partial Derivatives to find the equation of a tangent plane,,"$z = 4(x-1)^2 + 5(y+3)^2 +1$ at the point $(2,-2,10)$ I'm not sure how exactly how to proceed through the problem. I know to find the derivative with respect to $x$ , $y$, and $z$, which I did. For $x$, I got $8(x-1)$ and for $y$, I got $10(y+3)$. When I found $z$, I got was pretty much the sum of the derivatives of $x$ and $y$ . I know I'm supposed to plug in the point $(2,-2,10)$ somewhere, but I'm not sure if it is the original question or the derived formulas.","$z = 4(x-1)^2 + 5(y+3)^2 +1$ at the point $(2,-2,10)$ I'm not sure how exactly how to proceed through the problem. I know to find the derivative with respect to $x$ , $y$, and $z$, which I did. For $x$, I got $8(x-1)$ and for $y$, I got $10(y+3)$. When I found $z$, I got was pretty much the sum of the derivatives of $x$ and $y$ . I know I'm supposed to plug in the point $(2,-2,10)$ somewhere, but I'm not sure if it is the original question or the derived formulas.",,"['multivariable-calculus', 'partial-derivative']"
20,"Finding partial derivative of $f(x,y)=\frac{xy}{\sqrt{x^2+y^2}}$",Finding partial derivative of,"f(x,y)=\frac{xy}{\sqrt{x^2+y^2}}","I'm studying Mathematical Analysis II for a university course. There is a training exercise that asks me to: Find the partial derivatives at $(1,0)$ of $f(x,y)$ , where: $f(x,y)=\frac{xy}{\sqrt{x^2+y^2}}$ when $(x,y)\neq (0,0)$ and $f(x,y) = 0$ when $(x,y)= (0,0)$ So far, I've used the definition as per the other examples. For example, let's start with $f_x(a,b)=g'(a)=\lim_{h\to 0}\frac{f(a+h,b) - f(a,b)}{h}$ Applied to my problem, I've got $f_x(1,0)=\lim_{h\to 0}\frac{f(1+h,0) - f(1,0)}{h}$ And this is where I stall. According to other examples, this is supposed to equal $\frac{0}{h}=0$ , but HOW ? And then I'm supposed to get the limit to +inf? Am I using the wrong methodology?","I'm studying Mathematical Analysis II for a university course. There is a training exercise that asks me to: Find the partial derivatives at of , where: when and when So far, I've used the definition as per the other examples. For example, let's start with Applied to my problem, I've got And this is where I stall. According to other examples, this is supposed to equal , but HOW ? And then I'm supposed to get the limit to +inf? Am I using the wrong methodology?","(1,0) f(x,y) f(x,y)=\frac{xy}{\sqrt{x^2+y^2}} (x,y)\neq (0,0) f(x,y) = 0 (x,y)= (0,0) f_x(a,b)=g'(a)=\lim_{h\to 0}\frac{f(a+h,b) - f(a,b)}{h} f_x(1,0)=\lim_{h\to 0}\frac{f(1+h,0) - f(1,0)}{h} \frac{0}{h}=0","['analysis', 'multivariable-calculus', 'partial-derivative']"
21,Why does $\lim_{x\to 0} \frac {\sin (xy)}{x} \to y $?,Why does ?,\lim_{x\to 0} \frac {\sin (xy)}{x} \to y ,"Let $f(x,y) = \frac{\sin (xy)}{x}$ for $x\neq 0$. How should you define $f(0,y)$ for $y\in \mathbb{R}$ so as to make $f$ a continuous function on all of $\mathbb{R}^2$? So in order for a function to be continuous the limit of the function approaching the point has to equal the value of the function at the point. In this case the trouble point is $(0,\,y)$. Now i looked at some other posts and took the suggestion of using L'Hopital's rule. Using it I get the right solution, but I don't understand why doing L'Hopital does provide the right solution? If i am in a multivariate environment, how am i allowed to use something like L'Hopital?","Let $f(x,y) = \frac{\sin (xy)}{x}$ for $x\neq 0$. How should you define $f(0,y)$ for $y\in \mathbb{R}$ so as to make $f$ a continuous function on all of $\mathbb{R}^2$? So in order for a function to be continuous the limit of the function approaching the point has to equal the value of the function at the point. In this case the trouble point is $(0,\,y)$. Now i looked at some other posts and took the suggestion of using L'Hopital's rule. Using it I get the right solution, but I don't understand why doing L'Hopital does provide the right solution? If i am in a multivariate environment, how am i allowed to use something like L'Hopital?",,"['multivariable-calculus', 'continuity']"
22,Doesn't $x^3+2y^3+3z^3=0$ give a surface in $R^3$?,Doesn't  give a surface in ?,x^3+2y^3+3z^3=0 R^3,"In my last exam on Advanced Calculus (following Spivak's Calculus on Manifolds), I couldn't solve the following question. True or false: the set $S$ in $R^3$ given by $x^3+2y^3+3z^3=0$ is a surface. Thanks in advance for any ideas! What I know: $S$ minus $0$ is a surface, because it is the inverse image of a regular value; so $0$ is the only possible problem. Now, there is a neighbourhood of any point in a surface which is the graph of a differentiable function in two variables. Moreover, the functions $-(1/3)(x^3+2y^3)^{1/3}$, $-(1/2)(x^3+3y^3)^{1/3}$, and $-(2y^3+3z^3)^{1/3}$ are not differentiable at $(0,0)$. So, right  now my guess is: false.","In my last exam on Advanced Calculus (following Spivak's Calculus on Manifolds), I couldn't solve the following question. True or false: the set $S$ in $R^3$ given by $x^3+2y^3+3z^3=0$ is a surface. Thanks in advance for any ideas! What I know: $S$ minus $0$ is a surface, because it is the inverse image of a regular value; so $0$ is the only possible problem. Now, there is a neighbourhood of any point in a surface which is the graph of a differentiable function in two variables. Moreover, the functions $-(1/3)(x^3+2y^3)^{1/3}$, $-(1/2)(x^3+3y^3)^{1/3}$, and $-(2y^3+3z^3)^{1/3}$ are not differentiable at $(0,0)$. So, right  now my guess is: false.",,"['real-analysis', 'multivariable-calculus', 'differential-geometry', 'differential-topology']"
23,Find the jacobian,Find the jacobian,,"I'm been struggling with the problem for a quite some time now. I need to find the jacobian for the following : $$u=x-y$$ $$v=xy$$ What I did : $$x=y+u\\x=\frac{v}{y}\\y=x-u\\y=\frac{v}{x}$$ \begin{vmatrix} dx/du & dx/dv \\  dy/du & dy/dv  \end{vmatrix} \begin{vmatrix} 1 & \frac{1}{y} \\  -1 & \frac{1}{x}  \end{vmatrix} $$\frac{1}{x}+\frac{1}{y}$$ But for some reason the answer say I should get : $$\frac{1}{x+y}$$ The method they used in the book is to calculate $J^{-1}$ which is $x+y$ and use the law $J(x,y)*J(u,v)=1$ in order to get $J(u,v)=\frac{1}{x+y}$ I don't know, why I get different answer. Any idea? Any help will be appreciated, Thanks in advance!","I'm been struggling with the problem for a quite some time now. I need to find the jacobian for the following : $$u=x-y$$ $$v=xy$$ What I did : $$x=y+u\\x=\frac{v}{y}\\y=x-u\\y=\frac{v}{x}$$ \begin{vmatrix} dx/du & dx/dv \\  dy/du & dy/dv  \end{vmatrix} \begin{vmatrix} 1 & \frac{1}{y} \\  -1 & \frac{1}{x}  \end{vmatrix} $$\frac{1}{x}+\frac{1}{y}$$ But for some reason the answer say I should get : $$\frac{1}{x+y}$$ The method they used in the book is to calculate $J^{-1}$ which is $x+y$ and use the law $J(x,y)*J(u,v)=1$ in order to get $J(u,v)=\frac{1}{x+y}$ I don't know, why I get different answer. Any idea? Any help will be appreciated, Thanks in advance!",,"['integration', 'multivariable-calculus', 'differential-geometry', 'determinant']"
24,What is the mean value theorem for the Fréchet (total) derivative?,What is the mean value theorem for the Fréchet (total) derivative?,,"What is the mean value theorem for the Fréchet (total) derivative?  Off the top of my head, it's something like $$ \|F(x+h)-F(x)\|\leq \sup_{c\in[0,1]} \|F^\prime(x+ch)\|\|h\| $$ but the double direction of $h$ on the right hand side feels odd to me and I'm not entirely sure what all of the assumptions behind the statement should be.","What is the mean value theorem for the Fréchet (total) derivative?  Off the top of my head, it's something like $$ \|F(x+h)-F(x)\|\leq \sup_{c\in[0,1]} \|F^\prime(x+ch)\|\|h\| $$ but the double direction of $h$ on the right hand side feels odd to me and I'm not entirely sure what all of the assumptions behind the statement should be.",,"['real-analysis', 'functional-analysis', 'multivariable-calculus', 'derivatives']"
25,How to evaluate $\sum_{j=0}^\infty\;\sum_{\substack{k=0 \\ k \neq j}}^\infty \frac{1}{j^2-k^2}$,How to evaluate,\sum_{j=0}^\infty\;\sum_{\substack{k=0 \\ k \neq j}}^\infty \frac{1}{j^2-k^2},"I was reading an introductory text on multiple integrals and I have encountered a problem asking me to explain why  $$ \sum_{j=0}^\infty\;\sum_{\substack{k=0 \\ k \neq j}}^\infty \frac{1}{j^2-k^2}\neq\sum_{k=0}^\infty\;\sum_{\substack{j=0 \\ j\neq k}}^\infty \frac{1}{j^2-k^2} $$ I felt that the infinite sum would diverge because the two terms corresponding to the pair of points $(j,k)$ and $(k,j)$ are always included in the sum and the sum of the two terms $\frac{1}{j^2-k^2}+\frac{1}{k^2-j^2}$are zero. However, I have no idea how to show that by evaluating the sum in any way. I would appreciate any help.","I was reading an introductory text on multiple integrals and I have encountered a problem asking me to explain why  $$ \sum_{j=0}^\infty\;\sum_{\substack{k=0 \\ k \neq j}}^\infty \frac{1}{j^2-k^2}\neq\sum_{k=0}^\infty\;\sum_{\substack{j=0 \\ j\neq k}}^\infty \frac{1}{j^2-k^2} $$ I felt that the infinite sum would diverge because the two terms corresponding to the pair of points $(j,k)$ and $(k,j)$ are always included in the sum and the sum of the two terms $\frac{1}{j^2-k^2}+\frac{1}{k^2-j^2}$are zero. However, I have no idea how to show that by evaluating the sum in any way. I would appreciate any help.",,"['sequences-and-series', 'multivariable-calculus']"
26,"Suppose $\{v_1,v_2,v_3\}$ is a basis for some subspace $V$ of $\mathbb R^m$.",Suppose  is a basis for some subspace  of .,"\{v_1,v_2,v_3\} V \mathbb R^m","Let $b$ be a vector in that subspace. Prove that if $b$ is orthogonal to all three basis vectors, then b has to be a zero vector. Hint: What is $\|b\|$ I do not know how to start this proof. Thanks in advance for any help i get.","Let $b$ be a vector in that subspace. Prove that if $b$ is orthogonal to all three basis vectors, then b has to be a zero vector. Hint: What is $\|b\|$ I do not know how to start this proof. Thanks in advance for any help i get.",,"['linear-algebra', 'multivariable-calculus', 'proof-writing', 'vectors']"
27,"Find if the limit of $\lim_{ (x,y) \to (0,0)} \frac{xy}{\sqrt{x^2+y^2}}$",Find if the limit of,"\lim_{ (x,y) \to (0,0)} \frac{xy}{\sqrt{x^2+y^2}}","If we approach $f(x,y)=\frac{xy}{\sqrt{x^2+y^2}}$ on $y=mx^n, n \in R, n>2$ Then we get $\lim_{ (x,y) \to (0,0)} \frac{x(mx^n)}{\sqrt{x^2+m^2x^{2n}}}$ $\lim_{ (x,y) \to (0,0)} \frac{(mx^n)}{\sqrt{1+m^2x^{2n-2}}} =0$ This suggests that the limit does exists and is 0, but how do I show this formally? Thanks.","If we approach $f(x,y)=\frac{xy}{\sqrt{x^2+y^2}}$ on $y=mx^n, n \in R, n>2$ Then we get $\lim_{ (x,y) \to (0,0)} \frac{x(mx^n)}{\sqrt{x^2+m^2x^{2n}}}$ $\lim_{ (x,y) \to (0,0)} \frac{(mx^n)}{\sqrt{1+m^2x^{2n-2}}} =0$ This suggests that the limit does exists and is 0, but how do I show this formally? Thanks.",,"['calculus', 'limits', 'multivariable-calculus']"
28,What will be value of $\vec{r} \cdot \nabla$,What will be value of,\vec{r} \cdot \nabla,"I was studying on Nabla Operator and saw that $\nabla \cdot \vec{r} \neq \vec{r} \cdot \nabla$ So, if I were to find $\vec{r} \cdot \nabla$ how would I calculate it? I know that $\vec{r} \cdot \nabla$ = $r_1 \cdot \frac{\partial}{\partial x} + r_2 \cdot \frac{\partial}{\partial y} + r_3 \cdot \frac{\partial}{\partial z}$ but what I'm confused is on what am i suppose to differentiate? Thanks.","I was studying on Nabla Operator and saw that $\nabla \cdot \vec{r} \neq \vec{r} \cdot \nabla$ So, if I were to find $\vec{r} \cdot \nabla$ how would I calculate it? I know that $\vec{r} \cdot \nabla$ = $r_1 \cdot \frac{\partial}{\partial x} + r_2 \cdot \frac{\partial}{\partial y} + r_3 \cdot \frac{\partial}{\partial z}$ but what I'm confused is on what am i suppose to differentiate? Thanks.",,"['multivariable-calculus', 'vector-analysis']"
29,Prove that a set is closed,Prove that a set is closed,,Let $f : \mathbb{R}^m \rightarrow \mathbb{R}^n$ be a $C^1$ function. Prove or disprove that $\{x \in \mathbb{R}^m : f(x) = 0 \}$ is a closed set. How would you prove this?? I do not even understant what $f(x)=0$ represnets. I assume that it represents a surface in $\mathbb{R}^n$ but I am not sure.,Let $f : \mathbb{R}^m \rightarrow \mathbb{R}^n$ be a $C^1$ function. Prove or disprove that $\{x \in \mathbb{R}^m : f(x) = 0 \}$ is a closed set. How would you prove this?? I do not even understant what $f(x)=0$ represnets. I assume that it represents a surface in $\mathbb{R}^n$ but I am not sure.,,['multivariable-calculus']
30,Give an argument for $\int_{0}^{n} x^p dx \leq 1 +2^{p} + 3^{p} + \cdots+ n^{p}\leq \int_{0}^{n+1} x^p dx$,Give an argument for,\int_{0}^{n} x^p dx \leq 1 +2^{p} + 3^{p} + \cdots+ n^{p}\leq \int_{0}^{n+1} x^p dx,"For any $n$ and $p\geq 0$ give an argument that the following is true: $$\int_{0}^{n} x^p dx \leq 1 +2^{p} + 3^{p} + \cdots+ n^{p}\leq \int_{0}^{n+1} x^p dx$$ I'm having trouble even beginning this question. My first thought it to somehow meld this with the squeeze theorem, but, again, am not sure how to begin and show any real work. Any insight is very much appreciated.","For any $n$ and $p\geq 0$ give an argument that the following is true: $$\int_{0}^{n} x^p dx \leq 1 +2^{p} + 3^{p} + \cdots+ n^{p}\leq \int_{0}^{n+1} x^p dx$$ I'm having trouble even beginning this question. My first thought it to somehow meld this with the squeeze theorem, but, again, am not sure how to begin and show any real work. Any insight is very much appreciated.",,['multivariable-calculus']
31,Area enclosed by cardioid using Green's theorem,Area enclosed by cardioid using Green's theorem,,"Let $$\gamma(t) = \begin{pmatrix} (1+\cos t)\cos t \\ (1+ \cos t) \sin t \end{pmatrix}, \qquad t \in [0,2\pi].$$ Find the area enclosed by $\gamma$ using Green's theorem. So the area enclosed by $\gamma$ is a cardioid, let's denote it as $B$. By Green's theorem we have for $f=(f_1, f_2) \in C^1(\mathbb{R}^2, \mathbb{R}^2):$ $$\int_B \text{div} \begin{pmatrix} f_2 \\ -f_1 \end{pmatrix} d(x,y) = \int_{\partial B} f \cdot ds$$ So if we choose $f(x,y) = \begin{pmatrix} -y \\ 0 \end{pmatrix}$ for example, we get $$\begin{eqnarray} \text{Area of $B$} &=& \int_{\partial B} f \cdot ds \\&=& \int_{\gamma} f(\gamma(t)) \cdot \gamma'(t) dt \\&=& \int_0^{2\pi} \begin{pmatrix} -(1+ \cos t) \sin t \\ 0 \end{pmatrix} \cdot\begin{pmatrix} \sin t ( 1 - 2 \cos t) \\ \cos^2t - \sin^2t + \cos t \end{pmatrix} dt ,\end{eqnarray}$$ which I guess we can evaluate but if I keep going, this will become very nasty and tedious. There must be a nicer way to do this. Please help me see it.","Let $$\gamma(t) = \begin{pmatrix} (1+\cos t)\cos t \\ (1+ \cos t) \sin t \end{pmatrix}, \qquad t \in [0,2\pi].$$ Find the area enclosed by $\gamma$ using Green's theorem. So the area enclosed by $\gamma$ is a cardioid, let's denote it as $B$. By Green's theorem we have for $f=(f_1, f_2) \in C^1(\mathbb{R}^2, \mathbb{R}^2):$ $$\int_B \text{div} \begin{pmatrix} f_2 \\ -f_1 \end{pmatrix} d(x,y) = \int_{\partial B} f \cdot ds$$ So if we choose $f(x,y) = \begin{pmatrix} -y \\ 0 \end{pmatrix}$ for example, we get $$\begin{eqnarray} \text{Area of $B$} &=& \int_{\partial B} f \cdot ds \\&=& \int_{\gamma} f(\gamma(t)) \cdot \gamma'(t) dt \\&=& \int_0^{2\pi} \begin{pmatrix} -(1+ \cos t) \sin t \\ 0 \end{pmatrix} \cdot\begin{pmatrix} \sin t ( 1 - 2 \cos t) \\ \cos^2t - \sin^2t + \cos t \end{pmatrix} dt ,\end{eqnarray}$$ which I guess we can evaluate but if I keep going, this will become very nasty and tedious. There must be a nicer way to do this. Please help me see it.",,"['integration', 'multivariable-calculus', 'definite-integrals']"
32,Find the volume of the region of a sphere bounded by two planes,Find the volume of the region of a sphere bounded by two planes,,"Calculate the volume of a sphere $x^2+y^2+z^2=R^2$ which is bounded by $z=a$ and $z=b$, where $0\leq a<b<R$ using double integral. I can imagine the picture but I don't know how to set it up.","Calculate the volume of a sphere $x^2+y^2+z^2=R^2$ which is bounded by $z=a$ and $z=b$, where $0\leq a<b<R$ using double integral. I can imagine the picture but I don't know how to set it up.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
33,(Geometric algebra) Acceleration of a particle with constant speed as a bivector-vector inner product,(Geometric algebra) Acceleration of a particle with constant speed as a bivector-vector inner product,,"I've been working on (self-studying) Geometric Algebra for Physicists which, sadly, has no solutions manual. This is not a problem in general, but I feel like one of my solutions for a question asked in the textbook is incomplete. The question is: A particle in three dimensions moves along a curve $x(t)$ such that $|v|$ is constant. Show that there exists a bivector $Ω$ such that   $$ \dot v = Ω\cdot v $$   and give an explicit formula for $Ω$. Is this bivector unique? My solution: Since we're in three dimensions, we can construct a vector $\dot v$ with the following property:   $$ v^2=v_0^2 \implies \dot v v+v \dot v = 0 \implies v\cdot \dot v = 0 $$   (This is obvious from elementary multivariable calculus) As $\dot v$ must always be perpendicular to $v$, we can always come up with such a vector by forming a plane with it and some arbitrary vector and then take this resulting bivector's dual. That is,   $$ \dot v = I(v\wedge b) $$   Where $I=e_1e_2e_3$ is the unit pseudoscalar. We can re-write this in the following form:   $$ I(v\wedge b) = v\cdot (Ib)=-(Ib)\cdot v $$If we allow $b$ to absorb the constant, then we can claim:   $$ \Omega = Ib(t) $$   Where $b(t)$ is any vector-valued function of $t$. Clearly, then, the bivector is not unique. Is this all? It seems like the question implies there should be a more restrictive condition on $\Omega$, but I haven't been able to find any (and, intuitively, it doesn't seem like it could be made more restrictive). Thank you.","I've been working on (self-studying) Geometric Algebra for Physicists which, sadly, has no solutions manual. This is not a problem in general, but I feel like one of my solutions for a question asked in the textbook is incomplete. The question is: A particle in three dimensions moves along a curve $x(t)$ such that $|v|$ is constant. Show that there exists a bivector $Ω$ such that   $$ \dot v = Ω\cdot v $$   and give an explicit formula for $Ω$. Is this bivector unique? My solution: Since we're in three dimensions, we can construct a vector $\dot v$ with the following property:   $$ v^2=v_0^2 \implies \dot v v+v \dot v = 0 \implies v\cdot \dot v = 0 $$   (This is obvious from elementary multivariable calculus) As $\dot v$ must always be perpendicular to $v$, we can always come up with such a vector by forming a plane with it and some arbitrary vector and then take this resulting bivector's dual. That is,   $$ \dot v = I(v\wedge b) $$   Where $I=e_1e_2e_3$ is the unit pseudoscalar. We can re-write this in the following form:   $$ I(v\wedge b) = v\cdot (Ib)=-(Ib)\cdot v $$If we allow $b$ to absorb the constant, then we can claim:   $$ \Omega = Ib(t) $$   Where $b(t)$ is any vector-valued function of $t$. Clearly, then, the bivector is not unique. Is this all? It seems like the question implies there should be a more restrictive condition on $\Omega$, but I haven't been able to find any (and, intuitively, it doesn't seem like it could be made more restrictive). Thank you.",,"['calculus', 'multivariable-calculus', 'self-learning', 'clifford-algebras', 'geometric-algebras']"
34,Strange double integral,Strange double integral,,"What is wrong with this computation of $\int_0^1\int_{-y}^y \sqrt[3]{x} \, dx \, dy$? I'm considering real functions only. Since $x^{4/3}$ is an antiderivative of the integrand, we will get $\frac{3}{4}[x^{4/3}]_{-y}^y =\frac{3}{4}(y^{4/3}-(-y)^{4/3})=\frac{3}{4}(y^{4/3}-y^{4/3})=0$. Thus $\int_0^1 \int_{-y}^y \sqrt[3]{x} \, dx \, dy=0$. However, maple is giving  me a complex (nonzero) number as the answer. Why is that? Any hint?","What is wrong with this computation of $\int_0^1\int_{-y}^y \sqrt[3]{x} \, dx \, dy$? I'm considering real functions only. Since $x^{4/3}$ is an antiderivative of the integrand, we will get $\frac{3}{4}[x^{4/3}]_{-y}^y =\frac{3}{4}(y^{4/3}-(-y)^{4/3})=\frac{3}{4}(y^{4/3}-y^{4/3})=0$. Thus $\int_0^1 \int_{-y}^y \sqrt[3]{x} \, dx \, dy=0$. However, maple is giving  me a complex (nonzero) number as the answer. Why is that? Any hint?",,['multivariable-calculus']
35,Can you switch the order of the determinants when changing variables using the Jacobian?,Can you switch the order of the determinants when changing variables using the Jacobian?,,"Let say we're changing the variables and we use the Jacobian to do this. Lets say we integrate in respect to $u$ and $v$, does it matter if we set up the integral like $\int\int\,\mathrm{d}u\mathrm{d}v$ or $\int\int\,\mathrm{d}v\mathrm{d}u$ after calculating the Jacobian?","Let say we're changing the variables and we use the Jacobian to do this. Lets say we integrate in respect to $u$ and $v$, does it matter if we set up the integral like $\int\int\,\mathrm{d}u\mathrm{d}v$ or $\int\int\,\mathrm{d}v\mathrm{d}u$ after calculating the Jacobian?",,"['calculus', 'multivariable-calculus', 'differential-forms']"
36,"Evaluating $\lim_{(x,y)\rightarrow (0,0)} \frac{(xy)^3}{x^2+y^6}$",Evaluating,"\lim_{(x,y)\rightarrow (0,0)} \frac{(xy)^3}{x^2+y^6}","$$\lim_{(x,y)\rightarrow (0,0)} \frac{(xy)^3}{x^2+y^6}$$ I don't really know how to do, but I was trying to do like that: $a=x$,  $b=y^2$ then I was trying to do this $$\lim_{(x,y)\rightarrow (0,0)} \frac{ab}{a^2+b^2}$$ then I don't know no more how to do...","$$\lim_{(x,y)\rightarrow (0,0)} \frac{(xy)^3}{x^2+y^6}$$ I don't really know how to do, but I was trying to do like that: $a=x$,  $b=y^2$ then I was trying to do this $$\lim_{(x,y)\rightarrow (0,0)} \frac{ab}{a^2+b^2}$$ then I don't know no more how to do...",,"['limits', 'multivariable-calculus']"
37,"Can $f(x,y) = |x|^y$ be be made continuous?",Can  be be made continuous?,"f(x,y) = |x|^y","Can  $f(x,y) =  |x|^y$  be appropriately defined at (0,0) in order to be continous there . if we approach from path y=mx then f(x) becomes |x|^mx with x approaches to 0 .then by taking logs and we get infinity by infinity form . on solving with regular l'hop method ans comes cout to be e^-mx .which is dependent to m . am i right ?","Can  $f(x,y) =  |x|^y$  be appropriately defined at (0,0) in order to be continous there . if we approach from path y=mx then f(x) becomes |x|^mx with x approaches to 0 .then by taking logs and we get infinity by infinity form . on solving with regular l'hop method ans comes cout to be e^-mx .which is dependent to m . am i right ?",,"['limits', 'multivariable-calculus']"
38,What is the easiest way to evaluate this integral?,What is the easiest way to evaluate this integral?,,"\begin{equation*} \int_{0}^{64}\int_{\frac{1}{2}\sqrt[3]{y}}^{2} \frac{y^2}{\sqrt{x^{10} +1}} dxdy \end{equation*} I'm probably doing something really wrong, because I'm stuck. Any help will be appreciated. Thanks.","\begin{equation*} \int_{0}^{64}\int_{\frac{1}{2}\sqrt[3]{y}}^{2} \frac{y^2}{\sqrt{x^{10} +1}} dxdy \end{equation*} I'm probably doing something really wrong, because I'm stuck. Any help will be appreciated. Thanks.",,['multivariable-calculus']
39,"What is meant with ""$df_x(h)$ is a linear function of $h$""?","What is meant with "" is a linear function of ""?",df_x(h) h,"In Milnor's Topology From the Differentiable Viewpoint , the derivative of a smooth map $f: U\to V$  is defined as  $$ \mathrm{d}f_x: \mathbb{R}^k \to \mathbb{R}^l $$ $$ h\mapsto \lim_{t\to0} \frac{f(x+th)-f(x)}{t} $$ On the fifth page he remarks Clearly $\mathrm{d}f_x (h)$  is a linear function of $h$. What does he mean here? This would require that $\mathrm{d} f_x (h_1+h_2)= \mathrm{d} f_x(h_1)+\mathrm{d} f_x(h_2)$, but this is does not seem to the case in general.","In Milnor's Topology From the Differentiable Viewpoint , the derivative of a smooth map $f: U\to V$  is defined as  $$ \mathrm{d}f_x: \mathbb{R}^k \to \mathbb{R}^l $$ $$ h\mapsto \lim_{t\to0} \frac{f(x+th)-f(x)}{t} $$ On the fifth page he remarks Clearly $\mathrm{d}f_x (h)$  is a linear function of $h$. What does he mean here? This would require that $\mathrm{d} f_x (h_1+h_2)= \mathrm{d} f_x(h_1)+\mathrm{d} f_x(h_2)$, but this is does not seem to the case in general.",,"['multivariable-calculus', 'derivatives', 'differential-topology']"
40,"3-Variable Limit $\lim\limits_{(x,y,z,) \to (0,0,0)} \frac{xy+yz^2+xz^2}{x^2+y^2+z^4}$",3-Variable Limit,"\lim\limits_{(x,y,z,) \to (0,0,0)} \frac{xy+yz^2+xz^2}{x^2+y^2+z^4}","$$\lim_{(x,y,z,) \to (0,0,0)} \frac{xy+yz^2+xz^2}{x^2+y^2+z^4}$$ Background: I think that converting the formula into parametric variables won't work since I only know that's useful for converting $\sin^2(x) + \cos^2(x)$ into $1$. I've actually never done a limit with three variables before so I'm a little confused about how we can go about it. Any suggestions as to how I should begin to approach this?","$$\lim_{(x,y,z,) \to (0,0,0)} \frac{xy+yz^2+xz^2}{x^2+y^2+z^4}$$ Background: I think that converting the formula into parametric variables won't work since I only know that's useful for converting $\sin^2(x) + \cos^2(x)$ into $1$. I've actually never done a limit with three variables before so I'm a little confused about how we can go about it. Any suggestions as to how I should begin to approach this?",,"['limits', 'multivariable-calculus']"
41,Sketch curve $r(t)=\cos t\hat{\bf i}+\sin t\hat{\bf j}+t\hat{\bf k}$,Sketch curve,r(t)=\cos t\hat{\bf i}+\sin t\hat{\bf j}+t\hat{\bf k},"This is from ""Multivariable Calculus, Concepts and Contexts"" by Stewart. He says ""The parametric equations for this curve are: $x=\cos t$ , $y=\sin t$ , $z=t$ . This makes sense. However, he then goes on to state that ""Since $x^2+y^2=\cos^2t+\sin^2t=1$ , the curve must lie on the circular cylinder $x^2+y^2=1$ . I understand the trigonometric identity, I just don't understand how we get to even consider $x^2+y^2=1$ since the vector function is defined as $r(t)=\cos t\hat{\bf i}+\sin t\hat{\bf j}+t\hat{\bf k}$ . Where does $x^2+y^2=1$ come from? How do we get to use that fact when $x=\cos t$ and $y=\sin t$ ?","This is from ""Multivariable Calculus, Concepts and Contexts"" by Stewart. He says ""The parametric equations for this curve are: , , . This makes sense. However, he then goes on to state that ""Since , the curve must lie on the circular cylinder . I understand the trigonometric identity, I just don't understand how we get to even consider since the vector function is defined as . Where does come from? How do we get to use that fact when and ?",x=\cos t y=\sin t z=t x^2+y^2=\cos^2t+\sin^2t=1 x^2+y^2=1 x^2+y^2=1 r(t)=\cos t\hat{\bf i}+\sin t\hat{\bf j}+t\hat{\bf k} x^2+y^2=1 x=\cos t y=\sin t,"['multivariable-calculus', 'parametric']"
42,Find the volume below $\sqrt{x}+\sqrt{y}+\sqrt{z}=1$ in the first quadrant,Find the volume below  in the first quadrant,\sqrt{x}+\sqrt{y}+\sqrt{z}=1,"I understand that we have to use transformation $$x = u^2, y = v^2, z = w^2$$ but I cannot figure out the limits. I just need a rough sketch of how to approach this. Could anyone give me some ideas?","I understand that we have to use transformation $$x = u^2, y = v^2, z = w^2$$ but I cannot figure out the limits. I just need a rough sketch of how to approach this. Could anyone give me some ideas?",,"['calculus', 'integration', 'multivariable-calculus', 'volume']"
43,Find the work done by the force field in moving the particle from one point to another,Find the work done by the force field in moving the particle from one point to another,,"Find work done by the force field F in moving  the particle from $(-1, 1)$ to $(3, 2)$ This sounds good till we are given that $\textbf{F} = \dfrac{2x}{y}\textbf{ i }- \dfrac{x^2}{y^2}\textbf{ j }$ Can someone explain how to understand this problem, does the word conservative field have a meaning if the field is not continuous everywhere","Find work done by the force field F in moving  the particle from $(-1, 1)$ to $(3, 2)$ This sounds good till we are given that $\textbf{F} = \dfrac{2x}{y}\textbf{ i }- \dfrac{x^2}{y^2}\textbf{ j }$ Can someone explain how to understand this problem, does the word conservative field have a meaning if the field is not continuous everywhere",,"['calculus', 'integration', 'multivariable-calculus', 'physics']"
44,Double integral where limits are the first quadrant,Double integral where limits are the first quadrant,,"Evaluate the integral $$\iint\limits_D  \frac{1}{(x+y+1)^3} \, dA$$ where $D$ is the first quadrant. In this case, what would the limits of integration be? I'm having trouble moving to polar coordinates. Obviously the unit circle for the first quadrant is the area from $0$ to $\pi/2$, but I'm not sure how to break this up for $x$ and $y$.","Evaluate the integral $$\iint\limits_D  \frac{1}{(x+y+1)^3} \, dA$$ where $D$ is the first quadrant. In this case, what would the limits of integration be? I'm having trouble moving to polar coordinates. Obviously the unit circle for the first quadrant is the area from $0$ to $\pi/2$, but I'm not sure how to break this up for $x$ and $y$.",,"['integration', 'multivariable-calculus', 'definite-integrals']"
45,How do I find a point on the surface of a sphere,How do I find a point on the surface of a sphere,,"How do I find a point on a sphere knowing its radius and center point ? I have a sphere:  $$x^2+(y-1)^2+(z+3)^2=16$$ Obviously its center point is $(0,1,-3)$ and its radius is $4$. I am asked to find the minimum and maximum distance to point $(1,1,1)$ So nearest point would be the touch point with the surface, and farthest point would be the touchpoint + distance of diameter. Can you help me solve this?","How do I find a point on a sphere knowing its radius and center point ? I have a sphere:  $$x^2+(y-1)^2+(z+3)^2=16$$ Obviously its center point is $(0,1,-3)$ and its radius is $4$. I am asked to find the minimum and maximum distance to point $(1,1,1)$ So nearest point would be the touch point with the surface, and farthest point would be the touchpoint + distance of diameter. Can you help me solve this?",,"['calculus', 'multivariable-calculus']"
46,"Clairauts ""equality of mixed partial derivatives"" theorem (interpretation)","Clairauts ""equality of mixed partial derivatives"" theorem (interpretation)",,"So I know how to prove this theorem via limits or whatever and I'm okay with that. What I'm not okay with is the interpretation. I just can't visualise how this is true in 3d space, any ideas? How do you guys interpret this theorem? My guess is that this theorem is implying that change looks the same from all directions. But even if this is true, I would like to know how this can be visualised. Or tell me if this is really not important and I should just trust what the theorem says? Thanks","So I know how to prove this theorem via limits or whatever and I'm okay with that. What I'm not okay with is the interpretation. I just can't visualise how this is true in 3d space, any ideas? How do you guys interpret this theorem? My guess is that this theorem is implying that change looks the same from all directions. But even if this is true, I would like to know how this can be visualised. Or tell me if this is really not important and I should just trust what the theorem says? Thanks",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
47,What is meant by the continuity of the Hessian matrix,What is meant by the continuity of the Hessian matrix,,"I have a simple and short question: ""What is meant by the continuity of the Hessian matrix?"" I guess it means that all the second partial derivatives of a function $f$ are continuous functions? is that the meaning? =) Thank you for any help! =)","I have a simple and short question: ""What is meant by the continuity of the Hessian matrix?"" I guess it means that all the second partial derivatives of a function $f$ are continuous functions? is that the meaning? =) Thank you for any help! =)",,"['multivariable-calculus', 'continuity']"
48,Find the directional derivative of the scalar field,Find the directional derivative of the scalar field,,"Find the directional derivative of the scalar field: $f(x,y,z)=\log(x^2+y^2+z^2)$ at $P_0(1,1,1)$ in the direction of the straight line $\ P_0P $  where $P=(3,2,1)$ What I have done: $\nabla(f)=(2/3,2/3,2/3)$ at $P_0$ and I know the eqation of $P_0P$ is $(x-1)/2=(y-1)/1=(z-1)/0=t(say)$ Now the required unit vector is $\frac{(3-1,2-1,1-1)}{\sqrt 5}$ taking dot product with $\nabla f$ my final result comes out to be $\frac{2}{\sqrt{5}}$ . But the answer does not match. Also my confusion arises seeing this article . According to the article, answer comes out as $\frac{9}{\sqrt{14}}$ . But the answer given in my exercisebook is $\frac{8}{3\sqrt5}$. Please help.","Find the directional derivative of the scalar field: $f(x,y,z)=\log(x^2+y^2+z^2)$ at $P_0(1,1,1)$ in the direction of the straight line $\ P_0P $  where $P=(3,2,1)$ What I have done: $\nabla(f)=(2/3,2/3,2/3)$ at $P_0$ and I know the eqation of $P_0P$ is $(x-1)/2=(y-1)/1=(z-1)/0=t(say)$ Now the required unit vector is $\frac{(3-1,2-1,1-1)}{\sqrt 5}$ taking dot product with $\nabla f$ my final result comes out to be $\frac{2}{\sqrt{5}}$ . But the answer does not match. Also my confusion arises seeing this article . According to the article, answer comes out as $\frac{9}{\sqrt{14}}$ . But the answer given in my exercisebook is $\frac{8}{3\sqrt5}$. Please help.",,"['real-analysis', 'multivariable-calculus']"
49,"$\lim \limits_{\left(x,\:y,\:z\right)\to \left(0,\:0,\:0\right)}\left(\frac{xyz}{\sqrt{x^2+y^2+z^2}}\right)$",,"\lim \limits_{\left(x,\:y,\:z\right)\to \left(0,\:0,\:0\right)}\left(\frac{xyz}{\sqrt{x^2+y^2+z^2}}\right)","Calculate the limit if it exists or prove it doesn´t. a) $\lim \limits_{\left(x,y\right)\to \left(0,0\right)}\left(\frac{8x^2y^2}{x^4+y^4}\right)$ b) $\lim \limits_{\left(x,\:y,\:z\right)\to \left(0,\:0,\:0\right)}\left(\frac{xyz}{\sqrt{x^2+y^2+z^2}}\right)$ a) was easy to prove it doesn't exist making $y=mx$ in b) i think it is $0$ but i can't prove it","Calculate the limit if it exists or prove it doesn´t. a) $\lim \limits_{\left(x,y\right)\to \left(0,0\right)}\left(\frac{8x^2y^2}{x^4+y^4}\right)$ b) $\lim \limits_{\left(x,\:y,\:z\right)\to \left(0,\:0,\:0\right)}\left(\frac{xyz}{\sqrt{x^2+y^2+z^2}}\right)$ a) was easy to prove it doesn't exist making $y=mx$ in b) i think it is $0$ but i can't prove it",,"['limits', 'multivariable-calculus']"
50,Extrema of $x+y+z$ subject to $x^2 - y^2 = 1$ and $2x + z = 1$ using Lagrange Multipliers,Extrema of  subject to  and  using Lagrange Multipliers,x+y+z x^2 - y^2 = 1 2x + z = 1,Find the extrema of $x+y+z$ subject to $x^2 - y^2 = 1$ and $2x + z = 1$ using Lagrange multipliers. So I set it up: $$ 1 = 2x\lambda_1 + 2\lambda_2 \\ 1 = -2y\lambda_1 \\ 1 = \lambda_2 $$ Plug in for $\lambda_2$: $$ 1 = 2x\lambda_1 + 2 \\ 1 = -2y\lambda_1 \\ $$ So we work with: $$ 1 = 2x\lambda_1 + 2 \\ 1 = -2y\lambda_1 \\ 1 = x^2 - y^2 \\ 1 = 2x + z $$ After some algebra I got $x = y$ as a solution but that's impossible because of the constraint $1 = x^2 - y^2$. What am I missing?,Find the extrema of $x+y+z$ subject to $x^2 - y^2 = 1$ and $2x + z = 1$ using Lagrange multipliers. So I set it up: $$ 1 = 2x\lambda_1 + 2\lambda_2 \\ 1 = -2y\lambda_1 \\ 1 = \lambda_2 $$ Plug in for $\lambda_2$: $$ 1 = 2x\lambda_1 + 2 \\ 1 = -2y\lambda_1 \\ $$ So we work with: $$ 1 = 2x\lambda_1 + 2 \\ 1 = -2y\lambda_1 \\ 1 = x^2 - y^2 \\ 1 = 2x + z $$ After some algebra I got $x = y$ as a solution but that's impossible because of the constraint $1 = x^2 - y^2$. What am I missing?,,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
51,Question about limit definition of partial derivative,Question about limit definition of partial derivative,,"I've seen it written two different ways: $$\frac{\partial f}{\partial x} = \lim\limits_{h \rightarrow 0} \frac{f(x + h, y) - f(x,y)}{h}$$ and $$\frac{\partial f}{\partial x} = \lim\limits_{h \rightarrow 0} \frac{f(x_0 + h, y_0) - f(x_0,y_0)}{h}$$ where the latter evaluates the function at the respective point before plugging it into the definition of the limit. For example, the function $f(x,y) = \begin{cases} \frac{x^2 y^4}{x^4 + 6y^8}, & \text{if }(x,y) \neq (0,0) \\ 0, & \text{if }(x,y) = (0,0) \end{cases}$ I want to determine if $\frac{\partial f}{\partial x}$ exists at $(0,0)$. Using the second limit definition would make showing the existence of $\frac{\partial f}{\partial x}$ considerably easier, since $y_0$ makes the first term in the limit $0$, and $f(x_0,y_0)$ is defined to be $0$. But using the first definition, we have to evaluate: $$\frac{(x+h)^2 y^4}{(x+h)^4 + 6y^8} - \frac{x^2 y^4}{x^4 + 6y^8}$$ I'm hoping the ""real"" or at least usable definition is the second one, but which one is the one we're supposed to use in practice to be technically/mathematically correct?","I've seen it written two different ways: $$\frac{\partial f}{\partial x} = \lim\limits_{h \rightarrow 0} \frac{f(x + h, y) - f(x,y)}{h}$$ and $$\frac{\partial f}{\partial x} = \lim\limits_{h \rightarrow 0} \frac{f(x_0 + h, y_0) - f(x_0,y_0)}{h}$$ where the latter evaluates the function at the respective point before plugging it into the definition of the limit. For example, the function $f(x,y) = \begin{cases} \frac{x^2 y^4}{x^4 + 6y^8}, & \text{if }(x,y) \neq (0,0) \\ 0, & \text{if }(x,y) = (0,0) \end{cases}$ I want to determine if $\frac{\partial f}{\partial x}$ exists at $(0,0)$. Using the second limit definition would make showing the existence of $\frac{\partial f}{\partial x}$ considerably easier, since $y_0$ makes the first term in the limit $0$, and $f(x_0,y_0)$ is defined to be $0$. But using the first definition, we have to evaluate: $$\frac{(x+h)^2 y^4}{(x+h)^4 + 6y^8} - \frac{x^2 y^4}{x^4 + 6y^8}$$ I'm hoping the ""real"" or at least usable definition is the second one, but which one is the one we're supposed to use in practice to be technically/mathematically correct?",,"['limits', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
52,Prove the following Injective and Surjective function,Prove the following Injective and Surjective function,,"Prove that the following function is bijective (both injective and surjective): $$f(x,y) = (x^2+y+x-2, x+3)$$ Usually I know how to do these, but the fact that the first x is squared, throws me off course. How can I tackle this problem? Thanks","Prove that the following function is bijective (both injective and surjective): $$f(x,y) = (x^2+y+x-2, x+3)$$ Usually I know how to do these, but the fact that the first x is squared, throws me off course. How can I tackle this problem? Thanks",,"['multivariable-calculus', 'functions']"
53,Determining the values of $k$ for which the matrix $A$ has an inverse,Determining the values of  for which the matrix  has an inverse,k A,"I've been given this question in class, with the 3x3 matrix: $$\begin{bmatrix}     2&   1&   0\\     1  & 2 &  1\\     0 & -3  & k\end{bmatrix}.$$ My job here is to find the values of $k$ for which this matrix has an inverse. Now in class the only method I've been taught how to do this is Gaussian Elimination, placing this matrix alongside the identity matrix (all entries are zero, except for ones along the diagonal) and performing row operations on both in order to find the inverse. My attempts at this usually result in the last row being $k$ plus or minus some number, like $k+2$. Is not assigning a number to $k$ until I've ""isolated"" it in it's row correct? Any advice would be appreciated.","I've been given this question in class, with the 3x3 matrix: $$\begin{bmatrix}     2&   1&   0\\     1  & 2 &  1\\     0 & -3  & k\end{bmatrix}.$$ My job here is to find the values of $k$ for which this matrix has an inverse. Now in class the only method I've been taught how to do this is Gaussian Elimination, placing this matrix alongside the identity matrix (all entries are zero, except for ones along the diagonal) and performing row operations on both in order to find the inverse. My attempts at this usually result in the last row being $k$ plus or minus some number, like $k+2$. Is not assigning a number to $k$ until I've ""isolated"" it in it's row correct? Any advice would be appreciated.",,"['matrices', 'multivariable-calculus', 'inverse']"
54,Is there an easier way to prove a multivariate function is differentiable?,Is there an easier way to prove a multivariate function is differentiable?,,"$f\colon U \rightarrow \mathbb{R}, (x,y) \mapsto \sqrt{1 - x^2 - y^2}$ where $U = \{(x,y) \mid x^2 + y^2 < 1\}$. So the definition of differentiability I have is: $$\lim \limits_{(x,y) \rightarrow (x_0,y_0)} \dfrac{\|f(x,y) - f(x_0,y_0) - [\frac{\partial{f}}{\partial{x}}(x_0,y_0)]\cdot(x - x_0) - [\frac{\partial{f}}{\partial{y}}(x_0,y_0)]\cdot(y - y_0)\|}{\|(x,y) - (x_0,y_0)\|} = 0$$ So I get: $$\lim \limits_{(x,y) \rightarrow (x_0,y_0)} \dfrac{\|\sqrt{1 - x^2 - y^2} - \sqrt{1 - {x_0}^2 - {y_0}^2} + \frac{x_0(x - x_0)}{\sqrt{1 - {x_0}^2 - {y_0}^2}} + \frac{y_0(y - y_0)}{\sqrt{1 - {x_0}^2 - {y_0}^2}}\|}{\sqrt{(x - x_0)^2 + (y - y_0)^2}} = 0$$ So far it looks to be getting very hairy and I am wondering now whether there is an easier/simpler way than using the definition of the derivative directly, or if I may have missed some algebraic trick from the beginning.","$f\colon U \rightarrow \mathbb{R}, (x,y) \mapsto \sqrt{1 - x^2 - y^2}$ where $U = \{(x,y) \mid x^2 + y^2 < 1\}$. So the definition of differentiability I have is: $$\lim \limits_{(x,y) \rightarrow (x_0,y_0)} \dfrac{\|f(x,y) - f(x_0,y_0) - [\frac{\partial{f}}{\partial{x}}(x_0,y_0)]\cdot(x - x_0) - [\frac{\partial{f}}{\partial{y}}(x_0,y_0)]\cdot(y - y_0)\|}{\|(x,y) - (x_0,y_0)\|} = 0$$ So I get: $$\lim \limits_{(x,y) \rightarrow (x_0,y_0)} \dfrac{\|\sqrt{1 - x^2 - y^2} - \sqrt{1 - {x_0}^2 - {y_0}^2} + \frac{x_0(x - x_0)}{\sqrt{1 - {x_0}^2 - {y_0}^2}} + \frac{y_0(y - y_0)}{\sqrt{1 - {x_0}^2 - {y_0}^2}}\|}{\sqrt{(x - x_0)^2 + (y - y_0)^2}} = 0$$ So far it looks to be getting very hairy and I am wondering now whether there is an easier/simpler way than using the definition of the derivative directly, or if I may have missed some algebraic trick from the beginning.",,"['multivariable-calculus', 'derivatives']"
55,Tangent plane passes through origin,Tangent plane passes through origin,,"This is from a section in my course book on elementary differential geometry: Since the tangent plane $T_p S$ of a surface $S$ at a point $p \in S$ passes through the origin of $\mathbb{R}^3$, it is completely determined by giving a unit vector perpendicular to it... There are plenty of surfaces with points whose tangent plane doesn't pass through the origin, so why does it say so here?","This is from a section in my course book on elementary differential geometry: Since the tangent plane $T_p S$ of a surface $S$ at a point $p \in S$ passes through the origin of $\mathbb{R}^3$, it is completely determined by giving a unit vector perpendicular to it... There are plenty of surfaces with points whose tangent plane doesn't pass through the origin, so why does it say so here?",,"['multivariable-calculus', 'differential-geometry']"
56,A model for the spruce budworm population,A model for the spruce budworm population,,"A model for the spruce budworm population $u(t)$ is governed by $$\frac{du}{dt}=ru\left(1-\frac{u}{q}\right)-\frac{u^2}{1+u^2}$$ where $r,q$ are positive dimensionless parameters. The nonzero stedy states are thus given by the intersection of the two curve:  $$U(u)=r\left(1-\frac{u}{q}\right), \ \ \ V(u)=\frac{u}{1+u^2}$$ Show, using the conditions for a double root, that the curve in $r,q$ space which divides it into regions where there are $1$ or $3$ positive steady states is given parametrically by: $$r=\frac{2a^3}{(1+a^2)^2}, \ \ \ q=\frac{2a^3}{a^2-1}$$ What mean  ""conditions of double roots""? How can I find the parametrization? Thank you for your help.","A model for the spruce budworm population $u(t)$ is governed by $$\frac{du}{dt}=ru\left(1-\frac{u}{q}\right)-\frac{u^2}{1+u^2}$$ where $r,q$ are positive dimensionless parameters. The nonzero stedy states are thus given by the intersection of the two curve:  $$U(u)=r\left(1-\frac{u}{q}\right), \ \ \ V(u)=\frac{u}{1+u^2}$$ Show, using the conditions for a double root, that the curve in $r,q$ space which divides it into regions where there are $1$ or $3$ positive steady states is given parametrically by: $$r=\frac{2a^3}{(1+a^2)^2}, \ \ \ q=\frac{2a^3}{a^2-1}$$ What mean  ""conditions of double roots""? How can I find the parametrization? Thank you for your help.",,"['calculus', 'multivariable-calculus', 'mathematical-modeling', 'applications']"
57,"Does $\lim_{y \to 0} [\lim_{x \to 0} f(x, y)] \ne \lim_{x \to 0} [\lim_{y \to 0} f(x, y)] \implies \text{ no limit }$ works in the opposite direction?",Does  works in the opposite direction?,"\lim_{y \to 0} [\lim_{x \to 0} f(x, y)] \ne \lim_{x \to 0} [\lim_{y \to 0} f(x, y)] \implies \text{ no limit }","There's a thorem that say: Let $f(x, y)$ be a function.  if: $\lim_{y \to 0} [\lim_{x \to 0} f(x, y)] \ne \lim_{x \to 0} [\lim_{y \to 0} f(x, y)] \implies \text{ no limit }$ Is that theorem works in the opposite direction? I mean, Let $L = \lim_{y \to 0} [\lim_{x \to 0} f(x, y)] = \lim_{x \to 0} [\lim_{y \to 0} f(x, y)]$. It implies that $\lim_{(x, y) \to (0, 0)} f(x, y) = L$? Thanks in advance!","There's a thorem that say: Let $f(x, y)$ be a function.  if: $\lim_{y \to 0} [\lim_{x \to 0} f(x, y)] \ne \lim_{x \to 0} [\lim_{y \to 0} f(x, y)] \implies \text{ no limit }$ Is that theorem works in the opposite direction? I mean, Let $L = \lim_{y \to 0} [\lim_{x \to 0} f(x, y)] = \lim_{x \to 0} [\lim_{y \to 0} f(x, y)]$. It implies that $\lim_{(x, y) \to (0, 0)} f(x, y) = L$? Thanks in advance!",,"['limits', 'multivariable-calculus']"
58,$\int_0^\pi\int_0^\infty e^{-xy}\sin kx~dy~dx=$ ?,?,\int_0^\pi\int_0^\infty e^{-xy}\sin kx~dy~dx=,$$\int_0^\pi\int_0^\infty e^{-xy}\sin kx~dy~dx=~?$$ My computation is $$\int_0^\infty e^{-xy}\sin kx~dx=\frac{1}{k+y^2}$$ so $$\int_0^\pi\frac{1}{k+y^2}dy=\frac{\sqrt{k}}{k}\arctan\pi$$ $$\int_0^\pi\int_0^\infty e^{-xy}\sin kx~dy~dx=\frac{\sqrt{k}}{k}\arctan\pi$$ Is my result wrong?,$$\int_0^\pi\int_0^\infty e^{-xy}\sin kx~dy~dx=~?$$ My computation is $$\int_0^\infty e^{-xy}\sin kx~dx=\frac{1}{k+y^2}$$ so $$\int_0^\pi\frac{1}{k+y^2}dy=\frac{\sqrt{k}}{k}\arctan\pi$$ $$\int_0^\pi\int_0^\infty e^{-xy}\sin kx~dy~dx=\frac{\sqrt{k}}{k}\arctan\pi$$ Is my result wrong?,,"['integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
59,Minimum area of the parallelepiped surface,Minimum area of the parallelepiped surface,,"Among all the retangular parallelpipeds of volume $V$, find one whose total surface área is minimum Using the Lagrange Multipliers method, I've found that it is a cube with dimensions $ \sqrt[3]{V} $. But I don't know how to prove that it is, indeed, a cube with those dimensions, since I couldn't prove that the function $S_A(x,y,z)=2xy + 2xz + 2yz$ (surface total area) have a minimum. Can you help me with it? Thanks in advance","Among all the retangular parallelpipeds of volume $V$, find one whose total surface área is minimum Using the Lagrange Multipliers method, I've found that it is a cube with dimensions $ \sqrt[3]{V} $. But I don't know how to prove that it is, indeed, a cube with those dimensions, since I couldn't prove that the function $S_A(x,y,z)=2xy + 2xz + 2yz$ (surface total area) have a minimum. Can you help me with it? Thanks in advance",,['multivariable-calculus']
60,Is this equivalent to continuity?,Is this equivalent to continuity?,,I played around a little bit with the definition of continuity and I think I got the following relations that may be equivalent to continuity. Maybe there is somebody who can check this:  $$ f(\overline{M}) \subset \overline{f(M)} \Leftrightarrow \overline{f^{-1}(A)} \subset f^{-1}(\overline{A}) \Leftrightarrow f^{-1}(A^°)\subset (f^{-1}(A))^°.$$,I played around a little bit with the definition of continuity and I think I got the following relations that may be equivalent to continuity. Maybe there is somebody who can check this:  $$ f(\overline{M}) \subset \overline{f(M)} \Leftrightarrow \overline{f^{-1}(A)} \subset f^{-1}(\overline{A}) \Leftrightarrow f^{-1}(A^°)\subset (f^{-1}(A))^°.$$,,"['real-analysis', 'general-topology']"
61,J-measurable sets and functions of class $C^1$,J-measurable sets and functions of class,C^1,"If $f:\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is $C^1$ class and $det f^{\prime}(0)=0$ show that, when $r\rightarrow 0$ $$\dfrac{Vol(f(B[0,r]))}{ Vol(B[0,r])} \rightarrow 0$$ where $Vol(X)$ is n-dimensional volume to set $J$-measurable $X$. Any suggestions are welcome, thanks","If $f:\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is $C^1$ class and $det f^{\prime}(0)=0$ show that, when $r\rightarrow 0$ $$\dfrac{Vol(f(B[0,r]))}{ Vol(B[0,r])} \rightarrow 0$$ where $Vol(X)$ is n-dimensional volume to set $J$-measurable $X$. Any suggestions are welcome, thanks",,"['real-analysis', 'analysis', 'multivariable-calculus', 'functions']"
62,Why am I getting half the correct answer by using Green's Theorem?,Why am I getting half the correct answer by using Green's Theorem?,,"I had this homework problem that asked me to use Green's Theorem to solve it, so I did. Unfortunately, my answer was wrong. I looked for an error in my reasoning, but did not find it. I eventually solved by way of the line integral, which is usually harder than using Green's Theorem, so now I know that the correct answer is $-64\pi$. Yet, the answer I get from trying to use Green's Theorem is $-32\pi$, exactly half. Below is my attempt, and I'd appreciate a pointer as to where my error is. $\vec{F} = 4y\hat{i} + 5xy\hat{j}$ $C =$ circle of radius $4$ centered on the origin, oriented counter-clockwise. $$\begin{align*} \int_C \vec{F}\cdot\text{d}\vec{r} &= \int^{2\pi}_0\int^4_0 (5r\sin(\theta)-4) \text{ d}r\text{ d}\theta \\ &= \int^{2\pi}_0 \left.\left( \frac{5}{2}r^2\sin(\theta) - 4r \right)\right|^{r=4}_{r=0} \text{ d}\theta \\ &= \int^{2\pi}_0 (40\sin(\theta)-16) \text{ d}\theta \\ &= \left. (-40\cos(\theta)-16\theta) \right|^{\theta=2\pi}_{\theta=0} \\ &= -32\pi \\ &\ne -64\pi \end{align*}$$","I had this homework problem that asked me to use Green's Theorem to solve it, so I did. Unfortunately, my answer was wrong. I looked for an error in my reasoning, but did not find it. I eventually solved by way of the line integral, which is usually harder than using Green's Theorem, so now I know that the correct answer is $-64\pi$. Yet, the answer I get from trying to use Green's Theorem is $-32\pi$, exactly half. Below is my attempt, and I'd appreciate a pointer as to where my error is. $\vec{F} = 4y\hat{i} + 5xy\hat{j}$ $C =$ circle of radius $4$ centered on the origin, oriented counter-clockwise. $$\begin{align*} \int_C \vec{F}\cdot\text{d}\vec{r} &= \int^{2\pi}_0\int^4_0 (5r\sin(\theta)-4) \text{ d}r\text{ d}\theta \\ &= \int^{2\pi}_0 \left.\left( \frac{5}{2}r^2\sin(\theta) - 4r \right)\right|^{r=4}_{r=0} \text{ d}\theta \\ &= \int^{2\pi}_0 (40\sin(\theta)-16) \text{ d}\theta \\ &= \left. (-40\cos(\theta)-16\theta) \right|^{\theta=2\pi}_{\theta=0} \\ &= -32\pi \\ &\ne -64\pi \end{align*}$$",,['multivariable-calculus']
63,"Nonexistence of the limit $\lim_{(x,y)\rightarrow (0,0)} \frac{x^2y^2}{x^3+y^3}$ [duplicate]",Nonexistence of the limit  [duplicate],"\lim_{(x,y)\rightarrow (0,0)} \frac{x^2y^2}{x^3+y^3}","This question already has answers here : Limit of $\lim_{(x,y)\rightarrow(0,0)}\frac{x^2y^2}{x^3+y^3}$ (3 answers) Closed 7 years ago . How can we prove that the limit $\lim_{(x,y)\rightarrow (0,0)} \dfrac{x^2y^2}{x^3+y^3}$ doesn't exist? I have tried a lot of different paths and all of them lead to zero. I have only tried paths belonging to the domain and so I thank you for the lights that you've thrown here for me. When I've plotted the graphic of the function, it was clear that this limit doesn't exist, but could not prove it. For instance, this is an exercise of the book of Louis Leithold.","This question already has answers here : Limit of $\lim_{(x,y)\rightarrow(0,0)}\frac{x^2y^2}{x^3+y^3}$ (3 answers) Closed 7 years ago . How can we prove that the limit $\lim_{(x,y)\rightarrow (0,0)} \dfrac{x^2y^2}{x^3+y^3}$ doesn't exist? I have tried a lot of different paths and all of them lead to zero. I have only tried paths belonging to the domain and so I thank you for the lights that you've thrown here for me. When I've plotted the graphic of the function, it was clear that this limit doesn't exist, but could not prove it. For instance, this is an exercise of the book of Louis Leithold.",,"['limits', 'multivariable-calculus']"
64,Find the intersection of the surface $z=2x^2+y^2$ with the $x-y$ plane,Find the intersection of the surface  with the  plane,z=2x^2+y^2 x-y,I was asked to find the partial derivatives of $z=2x^2+y^2$ and the intersection of this surface with the $x-y$ plane. Is finding it relates to the partial derivatives? Here are my solutions for the partial derivatives: $z_x=4x$ $z_y=2y$ $z_{xx}=4$ $z_{xy}=0$ $z_{yy}=2$ $z_{yx}=0$,I was asked to find the partial derivatives of $z=2x^2+y^2$ and the intersection of this surface with the $x-y$ plane. Is finding it relates to the partial derivatives? Here are my solutions for the partial derivatives: $z_x=4x$ $z_y=2y$ $z_{xx}=4$ $z_{xy}=0$ $z_{yy}=2$ $z_{yx}=0$,,['multivariable-calculus']
65,Showing limit does not exist using two-path test,Showing limit does not exist using two-path test,,"I am new to using two-path test and my textbook only discusses it without showing any examples. I attempted to do this question below but I am not sure if I am correct. The question says to show the limit doesn't exist as $(x,y) \to (0,0)$: $$f(x,y)=\frac{xy}{|xy|}.$$ First I set $y=0$ and let $x \to 0$ and got the limit to be undefined  Second I set $x=0$ and let $y \to 0$ and got the limit to be undefined Is this how you do this test? Since limits are undefined they don't exist at this point $(0,0)$.","I am new to using two-path test and my textbook only discusses it without showing any examples. I attempted to do this question below but I am not sure if I am correct. The question says to show the limit doesn't exist as $(x,y) \to (0,0)$: $$f(x,y)=\frac{xy}{|xy|}.$$ First I set $y=0$ and let $x \to 0$ and got the limit to be undefined  Second I set $x=0$ and let $y \to 0$ and got the limit to be undefined Is this how you do this test? Since limits are undefined they don't exist at this point $(0,0)$.",,"['limits', 'multivariable-calculus']"
66,Rotate a function around an axis (not a volume of revolution question),Rotate a function around an axis (not a volume of revolution question),,"I am trying to understand this problem a little better. Rotate the curve $y=x^4-2x^2$ around the $y$-axis, find an equation for the resulting surface. If I write the curve as a vector-valued function (parameterically), as $\mathbf{x} = [x(t),y(t),z(t)]^T$, where \begin{align*} x(t) &= t, \\ y(t) &= t^4-2t^2, \\ z(t) &= 0, \end{align*} and write the rotation matrix out, \begin{equation} R_{\theta} = \begin{bmatrix} \cos\theta & 0 & -\sin\theta \\ 0 & 1& 0 \\ \sin\theta &0 & \cos\theta \end{bmatrix} \end{equation} Then the equations of the surface paramterised by $t$ and $\theta$ are $$ R_{\theta} \mathbf{x} = \begin{bmatrix} \hat{x} \\\hat{y}\\ \hat{z}\end{bmatrix}=\begin{bmatrix} t\cos\theta \\t^4-2t^2\\ t\sin\theta  \end{bmatrix}. $$ Trying to solve for the parameters, I get $t ^{\color{#C00}{2}}=1\pm\sqrt{1-y} \, $ $\color{#C00}{\mbox{(this was the mistake)}}$, and $\theta = \arctan\left(\displaystyle\frac{z}{x}\right)$. Solving would give something like $$ z= x\tan\arccos\left(\frac{x}{1\pm\sqrt{1+y}}\right), $$ however neither the parameterised version nor this expression look right when plotted. Where did I go wrong?","I am trying to understand this problem a little better. Rotate the curve $y=x^4-2x^2$ around the $y$-axis, find an equation for the resulting surface. If I write the curve as a vector-valued function (parameterically), as $\mathbf{x} = [x(t),y(t),z(t)]^T$, where \begin{align*} x(t) &= t, \\ y(t) &= t^4-2t^2, \\ z(t) &= 0, \end{align*} and write the rotation matrix out, \begin{equation} R_{\theta} = \begin{bmatrix} \cos\theta & 0 & -\sin\theta \\ 0 & 1& 0 \\ \sin\theta &0 & \cos\theta \end{bmatrix} \end{equation} Then the equations of the surface paramterised by $t$ and $\theta$ are $$ R_{\theta} \mathbf{x} = \begin{bmatrix} \hat{x} \\\hat{y}\\ \hat{z}\end{bmatrix}=\begin{bmatrix} t\cos\theta \\t^4-2t^2\\ t\sin\theta  \end{bmatrix}. $$ Trying to solve for the parameters, I get $t ^{\color{#C00}{2}}=1\pm\sqrt{1-y} \, $ $\color{#C00}{\mbox{(this was the mistake)}}$, and $\theta = \arctan\left(\displaystyle\frac{z}{x}\right)$. Solving would give something like $$ z= x\tan\arccos\left(\frac{x}{1\pm\sqrt{1+y}}\right), $$ however neither the parameterised version nor this expression look right when plotted. Where did I go wrong?",,"['multivariable-calculus', 'differential-geometry']"
67,Will the rules of calculus stay the same when a real-valued function is defined over infinite number of variables?,Will the rules of calculus stay the same when a real-valued function is defined over infinite number of variables?,,So the question would be: Can we ever talk about a real-valued function that is defined over infinite number of variables? Will the rules of calculus remain the same for such functions described in 1.?,So the question would be: Can we ever talk about a real-valued function that is defined over infinite number of variables? Will the rules of calculus remain the same for such functions described in 1.?,,"['functional-analysis', 'multivariable-calculus', 'functions']"
68,Existence of a global minimum,Existence of a global minimum,,"Let $S = \{(x, y, z) \in \mathbb{R}^{3}: x > 0, y >0, z > 0\}$ and consider $f(x, y, z) = xyz + \frac{1}{xyz}$. Why must $f$ attain a global minimum at some $p \in S$?","Let $S = \{(x, y, z) \in \mathbb{R}^{3}: x > 0, y >0, z > 0\}$ and consider $f(x, y, z) = xyz + \frac{1}{xyz}$. Why must $f$ attain a global minimum at some $p \in S$?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
69,Why does Stokes' Theorem allow any surface to be used when calculating a line integral.,Why does Stokes' Theorem allow any surface to be used when calculating a line integral.,,"I'm trying to understand Stokes' Theorem, what I don't get is how it allows you to pick any surface as long as the boundary is the same. Let's say that the vector field is increasing in strength along the z-axis, wouldn't then the curl be stronger if you chose a parabola shaped surface with a non-zero z-value compared to the unit disk where z would be zero?","I'm trying to understand Stokes' Theorem, what I don't get is how it allows you to pick any surface as long as the boundary is the same. Let's say that the vector field is increasing in strength along the z-axis, wouldn't then the curl be stronger if you chose a parabola shaped surface with a non-zero z-value compared to the unit disk where z would be zero?",,"['calculus', 'integration', 'multivariable-calculus']"
70,"Gauss & Stokes, direction of normal vector","Gauss & Stokes, direction of normal vector",,"I've decided to finish my education through completing my last exam (I've been working for 5 years). The exam is in multivariable calculus and I took the classes 6 years ago so I am very rusty. Will ask a bunch of questions over the following weeks and I love you all for helping me. Translating the question from Swedish, sorry if I'm not using the right terminology. Question: Let $S$ be the surface $x^2+y^2=1 ; 0 \le z \le 2;$ oriented so that the normal points from the z-axis. Calculate: $$\iint\limits_S F\cdot dS;F(x,y,z)=xy^2i+x^2yj+zk;$$ Answer: The teacher starts out by adding a bottom $B$ and top $T$ to the cylinder, closing the surface. $$\iint\limits_{S+B+T} F\cdot dS = \iiint\limits_V \nabla F dV=\iint\limits_{x^2+y^2\le1}\int_0^2(y^2+x^2+1) dzdxdy=[polar]=...=3\pi$$ Now it's time to subtract $B$ and $T$ again. The teacher puts up the following two equations: $$\iint\limits_B F\cdot dS=\iint\limits_{x^2+y^2\le1}F(x,y,0)\cdot (0,0,-1)dxdy= 0$$ and $$\iint\limits_T F\cdot dS=\iint\limits_{x^2+y^2\le1}F(x,y,2)\cdot (0,0,1)dxdy= 2\pi$$ I guess he's using the general Stokes (edit: this is wrong) $$\int\limits_CF\cdot dS=\iint\limits_S \nabla F\cdot NdS$$ My questions are: If he is using that formula, where's the $\nabla$ disappearing? Why are the normal vectors chosen in that particular way? How do I know which way is which? Thank you for your time.","I've decided to finish my education through completing my last exam (I've been working for 5 years). The exam is in multivariable calculus and I took the classes 6 years ago so I am very rusty. Will ask a bunch of questions over the following weeks and I love you all for helping me. Translating the question from Swedish, sorry if I'm not using the right terminology. Question: Let $S$ be the surface $x^2+y^2=1 ; 0 \le z \le 2;$ oriented so that the normal points from the z-axis. Calculate: $$\iint\limits_S F\cdot dS;F(x,y,z)=xy^2i+x^2yj+zk;$$ Answer: The teacher starts out by adding a bottom $B$ and top $T$ to the cylinder, closing the surface. $$\iint\limits_{S+B+T} F\cdot dS = \iiint\limits_V \nabla F dV=\iint\limits_{x^2+y^2\le1}\int_0^2(y^2+x^2+1) dzdxdy=[polar]=...=3\pi$$ Now it's time to subtract $B$ and $T$ again. The teacher puts up the following two equations: $$\iint\limits_B F\cdot dS=\iint\limits_{x^2+y^2\le1}F(x,y,0)\cdot (0,0,-1)dxdy= 0$$ and $$\iint\limits_T F\cdot dS=\iint\limits_{x^2+y^2\le1}F(x,y,2)\cdot (0,0,1)dxdy= 2\pi$$ I guess he's using the general Stokes (edit: this is wrong) $$\int\limits_CF\cdot dS=\iint\limits_S \nabla F\cdot NdS$$ My questions are: If he is using that formula, where's the $\nabla$ disappearing? Why are the normal vectors chosen in that particular way? How do I know which way is which? Thank you for your time.",,"['multivariable-calculus', 'integration']"
71,A Distinction Between Different Types of Partial Derivatives,A Distinction Between Different Types of Partial Derivatives,,"I recently noticed a subtle distinction between different types of partial derivatives: those that involve differentiation with respect to a parameter (that is, an   independent coordinate) those that involve differentiation with respect to a variable that is a function of a parameter or another variable. I don't recall this distinction being made in my calculus courses, and I'd like to check if my reasoning is valid and if there is a name for this observation. Consider two functions $f(x,y)$ and $g(z,t)$, where $x$, $y$, and $t$ are independent parameters but $z(t)$.  For example, $f(x,y)$ could represent that magnitude of a $2D$ magnetic field while $g(z,t)$ might represent the energy of a particle moving in a time-dependent potential. In the first case, the partial derivative $\partial{f}/\partial{x}$ is also a total derivative since $$ \frac{df}{dx} = \frac{\partial{f}}{\partial{x}} + \frac{\partial{f}}{\partial{y}}\frac{dy}{dx} = \frac{\partial{f}}{\partial{x}} $$ since $y$ is independent of $x$. In the second case, the partial derivative $\partial{g}/\partial{t}$ is more of a ""pure"" partial derivative since it only describes the explicit dependence of $g(z,t)$ on $t$, with the complete dependence being described by the total derivative $$ \frac{dg}{dt} = \frac{\partial{g}}{\partial{t}} + \frac{\partial{g}}{\partial{z}}\frac{dz}{dt} $$ Clearly, the first case is just a special instance of the more general second case, but the first case exhibits some useful properties that are not found in general in the second case.  Most importantly in my mind, since $\partial{f}/\partial{x}$ is essentially a total derivative, any partial differential equation for $f(x,y)$ involving only derivatives in $x$ is in fact just an ordinary differential equation.  The same is not true for a partial differential equation for $g(z,t)$ involving only derivatives in $t$, unless the explicit form of $z(t)$ is known a priori. As a simple example, suppose $f(x,y)$ is unknown but the form of $f_x(x,y)$ is given.  We then have $$ \frac{\partial{f}}{\partial{x}} = \frac{df}{dx} = f_x(x,y) \rightarrow f(x,y) = \int f_x(x,y) dx $$ I don't believe that this simple integration of a differential equation is possible in general in the second case.  This is certainly a substantive difference between the two type of partial derivatives, but I don't believe it was discussed in my undergraduate curriculum.","I recently noticed a subtle distinction between different types of partial derivatives: those that involve differentiation with respect to a parameter (that is, an   independent coordinate) those that involve differentiation with respect to a variable that is a function of a parameter or another variable. I don't recall this distinction being made in my calculus courses, and I'd like to check if my reasoning is valid and if there is a name for this observation. Consider two functions $f(x,y)$ and $g(z,t)$, where $x$, $y$, and $t$ are independent parameters but $z(t)$.  For example, $f(x,y)$ could represent that magnitude of a $2D$ magnetic field while $g(z,t)$ might represent the energy of a particle moving in a time-dependent potential. In the first case, the partial derivative $\partial{f}/\partial{x}$ is also a total derivative since $$ \frac{df}{dx} = \frac{\partial{f}}{\partial{x}} + \frac{\partial{f}}{\partial{y}}\frac{dy}{dx} = \frac{\partial{f}}{\partial{x}} $$ since $y$ is independent of $x$. In the second case, the partial derivative $\partial{g}/\partial{t}$ is more of a ""pure"" partial derivative since it only describes the explicit dependence of $g(z,t)$ on $t$, with the complete dependence being described by the total derivative $$ \frac{dg}{dt} = \frac{\partial{g}}{\partial{t}} + \frac{\partial{g}}{\partial{z}}\frac{dz}{dt} $$ Clearly, the first case is just a special instance of the more general second case, but the first case exhibits some useful properties that are not found in general in the second case.  Most importantly in my mind, since $\partial{f}/\partial{x}$ is essentially a total derivative, any partial differential equation for $f(x,y)$ involving only derivatives in $x$ is in fact just an ordinary differential equation.  The same is not true for a partial differential equation for $g(z,t)$ involving only derivatives in $t$, unless the explicit form of $z(t)$ is known a priori. As a simple example, suppose $f(x,y)$ is unknown but the form of $f_x(x,y)$ is given.  We then have $$ \frac{\partial{f}}{\partial{x}} = \frac{df}{dx} = f_x(x,y) \rightarrow f(x,y) = \int f_x(x,y) dx $$ I don't believe that this simple integration of a differential equation is possible in general in the second case.  This is certainly a substantive difference between the two type of partial derivatives, but I don't believe it was discussed in my undergraduate curriculum.",,"['calculus', 'multivariable-calculus']"
72,What is a smooth surface?,What is a smooth surface?,,"What is a smooth surface in terms of tangents and normals? I read in a book that surfaces are smooth if its surface normals depend continuously on the points of that surface. I did not understand this definition, could somebody simplify it for me?","What is a smooth surface in terms of tangents and normals? I read in a book that surfaces are smooth if its surface normals depend continuously on the points of that surface. I did not understand this definition, could somebody simplify it for me?",,"['multivariable-calculus', 'surfaces']"
73,"How to show that $f(x,y)$ is continuous.",How to show that  is continuous.,"f(x,y)","How to show that $f(x,y)$ is continuous. $$f(x,y)=\frac{4y^3(x^2+y^2)-(x^4+y^4)2x\alpha}{(x^2+y^2)^{\alpha +1}}$$  for $\alpha <3/2$. Please show me   Thanks :)","How to show that $f(x,y)$ is continuous. $$f(x,y)=\frac{4y^3(x^2+y^2)-(x^4+y^4)2x\alpha}{(x^2+y^2)^{\alpha +1}}$$  for $\alpha <3/2$. Please show me   Thanks :)",,"['calculus', 'analysis', 'multivariable-calculus', 'continuity']"
74,Differentiability of projection,Differentiability of projection,,"Where $\pi_i:\Bbb R^n\rightarrow\Bbb R$ is projection onto the $i$th coordinate, the differentiability of $\pi_i$ at $X$ is given by: $$\pi_i(X+H)-\pi_i(X)=\textrm{grad}\ \pi_i(X)\cdot H+||H||g(H)$$ Where $g$ tends to $0$ as its argument does. We deduce: $$g(H)=\frac{h_i}{||H||}(1-x_i)$$ Where $h_i$ and $x_i$ are the $i$th coordinates of $H$ and $X$ respectively. But how can this tend to zero? What if $H$ is approaching along the $i$th axis? Then $||H||=|h_i|$ and the limit either doesn't exist or is equal to $1-x_i$.","Where $\pi_i:\Bbb R^n\rightarrow\Bbb R$ is projection onto the $i$th coordinate, the differentiability of $\pi_i$ at $X$ is given by: $$\pi_i(X+H)-\pi_i(X)=\textrm{grad}\ \pi_i(X)\cdot H+||H||g(H)$$ Where $g$ tends to $0$ as its argument does. We deduce: $$g(H)=\frac{h_i}{||H||}(1-x_i)$$ Where $h_i$ and $x_i$ are the $i$th coordinates of $H$ and $X$ respectively. But how can this tend to zero? What if $H$ is approaching along the $i$th axis? Then $||H||=|h_i|$ and the limit either doesn't exist or is equal to $1-x_i$.",,"['calculus', 'multivariable-calculus']"
75,What does surface integral define?,What does surface integral define?,,"What does a surface integral define.... what is the difference between $dS$ and $dA$ if any..? I know that $dA$ give the volume of an function with 2 variables, but what about $dS$ does it do the same or what..? Trying to study using this material","What does a surface integral define.... what is the difference between $dS$ and $dA$ if any..? I know that $dA$ give the volume of an function with 2 variables, but what about $dS$ does it do the same or what..? Trying to study using this material",,"['calculus', 'integration', 'multivariable-calculus']"
76,Integrating a form and using Gauss' theorem.,Integrating a form and using Gauss' theorem.,,"Given the 2-form   $$ \varphi = \frac{1}{(x^2+y^2+z^2)^{3/2}}\left( x\,dy\wedge dz +y\,dz\wedge dx + z\,dx\wedge dy\right) \ . $$   (a) Compute the exterior derivative $\textbf{d}\varphi$ of $\varphi$. (b) Compute the integral of $\varphi$ over the unit sphere oriented by the outward normal. (c) Compute the integral of $\varphi$ over the boundary of the cube of side 4, centered at the origin, and oriented by the outward normal. (d) Can $\varphi$ be written $\textbf{d}\psi$ for some 1-form $\psi$ on $\mathbb{R}^3-\{0\}$? This is problem 6.23 from Hubbard's Vector Calculus text. This is not homework, I am just studying for my final. The only part I am having any trouble with is part (c). For part (a), note that $\varphi = \Phi_{\vec{F}}$ where $$ \vec{F} = \frac{1}{(x^2+y^2+z^2)^{3/2}}\begin{bmatrix} x\\y\\z \end{bmatrix} \ . $$ Then $\textbf{d}\varphi = M_{\nabla\cdot\vec{F}}=0.$ For part (b), we pick an orientation-preserving parametrization of the sphere, call it $\partial S$, and use it to evaluate the integral. Namely, we use $$ \gamma : \begin{pmatrix} \alpha \\ \beta \end{pmatrix} \mapsto \begin{pmatrix} \sin\alpha\cos\beta \\ \sin\alpha\sin\beta \\ \cos\alpha \end{pmatrix} $$ where $\alpha\in[0,\pi]$ and $\beta \in [0,2\pi)$. Then the pullback is $$ \gamma^*\varphi = (\sin^3\alpha+\sin\alpha\cos^2\alpha)\,d\alpha\,d\beta = \sin\alpha\,d\alpha\,d\beta $$ so that $$ \int\limits_{\partial S}\varphi = \int\limits_0^{2\pi}\int\limits_0^{\pi} \sin\alpha\,d\alpha\,d\beta = 4\pi. $$ Part (c) is where I am a bit stuck. Consider the region $R$ bounded between the described cube, say $C$ with boundary $\partial C$, and the unit ball at the origin, say $S$. We can use Guass' theorem since $\varphi$ is well defined there. We have $$ \int\limits_{\partial C} \varphi = \int\limits_C \textbf{d}\varphi = \int\limits_R \textbf{d}\varphi \, + \int\limits_S \textbf{d}\varphi = \int\limits_S \textbf{d}\varphi \stackrel{?}{=} 4\pi $$ by part (a). How do I go about obtaining the last equality since $\varphi$ is not defined at the origin? I'm not sure how to justify appealing to my result in part (b).","Given the 2-form   $$ \varphi = \frac{1}{(x^2+y^2+z^2)^{3/2}}\left( x\,dy\wedge dz +y\,dz\wedge dx + z\,dx\wedge dy\right) \ . $$   (a) Compute the exterior derivative $\textbf{d}\varphi$ of $\varphi$. (b) Compute the integral of $\varphi$ over the unit sphere oriented by the outward normal. (c) Compute the integral of $\varphi$ over the boundary of the cube of side 4, centered at the origin, and oriented by the outward normal. (d) Can $\varphi$ be written $\textbf{d}\psi$ for some 1-form $\psi$ on $\mathbb{R}^3-\{0\}$? This is problem 6.23 from Hubbard's Vector Calculus text. This is not homework, I am just studying for my final. The only part I am having any trouble with is part (c). For part (a), note that $\varphi = \Phi_{\vec{F}}$ where $$ \vec{F} = \frac{1}{(x^2+y^2+z^2)^{3/2}}\begin{bmatrix} x\\y\\z \end{bmatrix} \ . $$ Then $\textbf{d}\varphi = M_{\nabla\cdot\vec{F}}=0.$ For part (b), we pick an orientation-preserving parametrization of the sphere, call it $\partial S$, and use it to evaluate the integral. Namely, we use $$ \gamma : \begin{pmatrix} \alpha \\ \beta \end{pmatrix} \mapsto \begin{pmatrix} \sin\alpha\cos\beta \\ \sin\alpha\sin\beta \\ \cos\alpha \end{pmatrix} $$ where $\alpha\in[0,\pi]$ and $\beta \in [0,2\pi)$. Then the pullback is $$ \gamma^*\varphi = (\sin^3\alpha+\sin\alpha\cos^2\alpha)\,d\alpha\,d\beta = \sin\alpha\,d\alpha\,d\beta $$ so that $$ \int\limits_{\partial S}\varphi = \int\limits_0^{2\pi}\int\limits_0^{\pi} \sin\alpha\,d\alpha\,d\beta = 4\pi. $$ Part (c) is where I am a bit stuck. Consider the region $R$ bounded between the described cube, say $C$ with boundary $\partial C$, and the unit ball at the origin, say $S$. We can use Guass' theorem since $\varphi$ is well defined there. We have $$ \int\limits_{\partial C} \varphi = \int\limits_C \textbf{d}\varphi = \int\limits_R \textbf{d}\varphi \, + \int\limits_S \textbf{d}\varphi = \int\limits_S \textbf{d}\varphi \stackrel{?}{=} 4\pi $$ by part (a). How do I go about obtaining the last equality since $\varphi$ is not defined at the origin? I'm not sure how to justify appealing to my result in part (b).",,['differential-geometry']
77,What is the (parametric) intersection of a plane and a sphere?,What is the (parametric) intersection of a plane and a sphere?,,"Can someone please show me how to prove that the intersection of the plane $$x+y+z=0$$  and the sphere $$x^2+y^2+z^2=1$$ can be expressed as $$x(t)=\frac{\cos t-\sqrt3 \cdot\sin t}{\sqrt6}$$ $$y(t)=\frac{\cos t+\sqrt3 \cdot\sin t}{\sqrt6}$$ $$z(t)=\frac{-2\cos t}{\sqrt6}$$ Ps: Also, why am I not getting the correct notation? I am using a macbook pro (safari) if that is a concern?","Can someone please show me how to prove that the intersection of the plane $$x+y+z=0$$  and the sphere $$x^2+y^2+z^2=1$$ can be expressed as $$x(t)=\frac{\cos t-\sqrt3 \cdot\sin t}{\sqrt6}$$ $$y(t)=\frac{\cos t+\sqrt3 \cdot\sin t}{\sqrt6}$$ $$z(t)=\frac{-2\cos t}{\sqrt6}$$ Ps: Also, why am I not getting the correct notation? I am using a macbook pro (safari) if that is a concern?",,['multivariable-calculus']
78,Find expression for $dy/dx $ + state where it is valid,Find expression for  + state where it is valid,dy/dx ,"hopefully you guys can shed some insight into this question I'm working on. Given $xy+y^{2}-e^{x^{2}} = 6$ find an expression for $dy/dx$ and state where it is valid. So, what I did was differentiate it, which resulted in: $x+3y-2xe^{x^{2}} = 0 $ Although I am unsure whether this is correct, and I do not understand the state where it is valid. Any help or direction is much appreciated!","hopefully you guys can shed some insight into this question I'm working on. Given $xy+y^{2}-e^{x^{2}} = 6$ find an expression for $dy/dx$ and state where it is valid. So, what I did was differentiate it, which resulted in: $x+3y-2xe^{x^{2}} = 0 $ Although I am unsure whether this is correct, and I do not understand the state where it is valid. Any help or direction is much appreciated!",,"['multivariable-calculus', 'derivatives']"
79,Double integral of polar coordinates?,Double integral of polar coordinates?,,Compute $\int_C (8-\sqrt{x^2 +y^2}) ~ds$ where $C$ is the circle $x^2 + y^2 =4$. Answer: $24\pi$ How is the answer $24\pi$? I converted the integral into a double integral of polar coordinates and got $\frac{80}{3}\pi$ as my answer. Can someone please help me? I converted the integral into $\int_0^{2\pi}\int_0^2 (8-r)r~drd\theta$. Is this correct?,Compute $\int_C (8-\sqrt{x^2 +y^2}) ~ds$ where $C$ is the circle $x^2 + y^2 =4$. Answer: $24\pi$ How is the answer $24\pi$? I converted the integral into a double integral of polar coordinates and got $\frac{80}{3}\pi$ as my answer. Can someone please help me? I converted the integral into $\int_0^{2\pi}\int_0^2 (8-r)r~drd\theta$. Is this correct?,,"['integration', 'multivariable-calculus']"
80,Really Confused on a surface area integral can't seem to finish the integral off.,Really Confused on a surface area integral can't seem to finish the integral off.,,"Basically the question asks to compute $\int \int_{S} ( x^{2}+y^{2}) dA$ where S is the portion of the sphere  $x^{2}  + y^{2}+  z^{2}= 4$ and $z \in [1,2]$ we start with a chnage of variables $x=x  $ $y=y$ $ z= 2 \cdot(4-(x^{2}  + y^{2}))^{1/2}$ $Det(u,v)= \begin{bmatrix} i & j& k \\ 1 & 0 & \frac {-x}{(4-(x^{2}  + y^{2}))^{1/2}} \\ 0 & 1 & \frac {-y}{(4-(x^{2}  + y^{2}))^{1/2}} \\ \end{bmatrix}=(\frac {x}{(4-(x^{2}  + y^{2}))^{1/2}})i + (\frac {y}{(4-(x^{2}  + y^{2}))^{1/2}})j + k$ $dA=(\frac {x^{2}+y^{2}}{(4-(x^{2}  + y^{2}))} +1)^{1/2}$ $\int \int_{S} (\frac {(x^{2}+y^{2})^{3}}{(4-(x^{2}  + y^{2}))}+(x^{2}+y^{2})^{2})^{1/2}$ Projecting when z=1 and z=2 we have $x^{2}  + y^{2}= 4-1$ $\to r= 0,(3)^{1/2}$ going to polar we have: $(\frac {r^{6}}{(4-r^{2})}+r^{4})^{1/2}rdrd\theta=(\frac {4r^{4}}{(4-r^{2})})^{1/2}rdrd\theta$ my problem is $2\int^{2\pi}_{0} \int^{(3)^{1/2}}_{0} (\frac {4r^{4}}{(4-r^{2})})^{1/2}rdrd\theta$ there is no nice way i can think of to integrate this. it can also be written as: $2\int^{2\pi}_{0} \int^{(3)^{1/2}}_{0} \frac {2r^{2}}{((4-r^{2}))^{1/2}}rdrd\theta$","Basically the question asks to compute $\int \int_{S} ( x^{2}+y^{2}) dA$ where S is the portion of the sphere  $x^{2}  + y^{2}+  z^{2}= 4$ and $z \in [1,2]$ we start with a chnage of variables $x=x  $ $y=y$ $ z= 2 \cdot(4-(x^{2}  + y^{2}))^{1/2}$ $Det(u,v)= \begin{bmatrix} i & j& k \\ 1 & 0 & \frac {-x}{(4-(x^{2}  + y^{2}))^{1/2}} \\ 0 & 1 & \frac {-y}{(4-(x^{2}  + y^{2}))^{1/2}} \\ \end{bmatrix}=(\frac {x}{(4-(x^{2}  + y^{2}))^{1/2}})i + (\frac {y}{(4-(x^{2}  + y^{2}))^{1/2}})j + k$ $dA=(\frac {x^{2}+y^{2}}{(4-(x^{2}  + y^{2}))} +1)^{1/2}$ $\int \int_{S} (\frac {(x^{2}+y^{2})^{3}}{(4-(x^{2}  + y^{2}))}+(x^{2}+y^{2})^{2})^{1/2}$ Projecting when z=1 and z=2 we have $x^{2}  + y^{2}= 4-1$ $\to r= 0,(3)^{1/2}$ going to polar we have: $(\frac {r^{6}}{(4-r^{2})}+r^{4})^{1/2}rdrd\theta=(\frac {4r^{4}}{(4-r^{2})})^{1/2}rdrd\theta$ my problem is $2\int^{2\pi}_{0} \int^{(3)^{1/2}}_{0} (\frac {4r^{4}}{(4-r^{2})})^{1/2}rdrd\theta$ there is no nice way i can think of to integrate this. it can also be written as: $2\int^{2\pi}_{0} \int^{(3)^{1/2}}_{0} \frac {2r^{2}}{((4-r^{2}))^{1/2}}rdrd\theta$",,"['calculus', 'integration', 'multivariable-calculus']"
81,Flow lines of a gradient field,Flow lines of a gradient field,,"Let $\mathbf c(t)$ be a flow line of a gradient field $\mathbf F = - \nabla V$. Prove that $V(\mathbf c(t))$ is a decreasing function of t. Not sure where to begin here, although it might have to do with the gradient chain rule? My attempt: $$\mathbf c'(t) = \mathbf F(\mathbf c(t)) = -\nabla V(\mathbf c(t))$$ So for $\mathbf c'(t) > 0, \nabla V(\mathbf c(t)) < 0$ indicating that $V$ is decreasing. Is that right?","Let $\mathbf c(t)$ be a flow line of a gradient field $\mathbf F = - \nabla V$. Prove that $V(\mathbf c(t))$ is a decreasing function of t. Not sure where to begin here, although it might have to do with the gradient chain rule? My attempt: $$\mathbf c'(t) = \mathbf F(\mathbf c(t)) = -\nabla V(\mathbf c(t))$$ So for $\mathbf c'(t) > 0, \nabla V(\mathbf c(t)) < 0$ indicating that $V$ is decreasing. Is that right?",,['multivariable-calculus']
82,Does a 3-Dimensional coordinate transformation exist such that its scale factors are equal?,Does a 3-Dimensional coordinate transformation exist such that its scale factors are equal?,,"Let $\vec r=(x,y,z) $ be the position vector expressed in Cartesian coordinates. Let us define the coordinate transformation as $\vec r(u,v,w)=(x(u,v,w),y(u,v,w),z(u,v,w)) $ The scale factors are defined by $h_u=\vert \partial \vec r/\partial u \vert, h_v=\vert \partial \vec r/\partial v \vert, h_w=\vert \partial \vec r/\partial w \vert$ I wonder if a transformation can be defined such that $h_u=h_v=h_w$ Now a pair of examples in the two dimentional case. The transformation between elliptic and cartesian coordinates: $\vec r(u,v)=(cosh(u)cos(v)/2,sinh(u)sin(v)/2) $ $h_u=h_v=\sqrt{cosh^2(u)-cos^2(v)}/2$ The transformation between parabolic and cartesian coordinates. $\vec r(u,v)=((u^2-v^2)/2,u v) $ $h_u=h_v=\sqrt{u^2+v^2}$","Let $\vec r=(x,y,z) $ be the position vector expressed in Cartesian coordinates. Let us define the coordinate transformation as $\vec r(u,v,w)=(x(u,v,w),y(u,v,w),z(u,v,w)) $ The scale factors are defined by $h_u=\vert \partial \vec r/\partial u \vert, h_v=\vert \partial \vec r/\partial v \vert, h_w=\vert \partial \vec r/\partial w \vert$ I wonder if a transformation can be defined such that $h_u=h_v=h_w$ Now a pair of examples in the two dimentional case. The transformation between elliptic and cartesian coordinates: $\vec r(u,v)=(cosh(u)cos(v)/2,sinh(u)sin(v)/2) $ $h_u=h_v=\sqrt{cosh^2(u)-cos^2(v)}/2$ The transformation between parabolic and cartesian coordinates. $\vec r(u,v)=((u^2-v^2)/2,u v) $ $h_u=h_v=\sqrt{u^2+v^2}$",,"['differential-geometry', 'multivariable-calculus']"
83,"fubini's theorem, multivariable integration","fubini's theorem, multivariable integration",,"Evaluate the integral $$ \int_{[0,1]^n} \max{(x_1,x_2,x_3, \cdots,x_n)} \,dx_1dx_2\cdots dx_n $$ Any hints comments are appreciated. Thanks","Evaluate the integral $$ \int_{[0,1]^n} \max{(x_1,x_2,x_3, \cdots,x_n)} \,dx_1dx_2\cdots dx_n $$ Any hints comments are appreciated. Thanks",,"['multivariable-calculus', 'integration']"
84,A manifold being orientable vs oriented.,A manifold being orientable vs oriented.,,"I read that an oriented manifold is a manifold with a choice of orientations for each tangent space so that for $p \in M$, there is an open set $U$ and a collection of vector fields $X_1,...,X_n$ so that for all $q \in U$ $X_1(q),...,X_n(q)$, is a basis of $T_qM$ belonging to the orientation of $T_qM$.  What does it mean for manifold to be ""orientable""?  Is it the same thing as oriented?","I read that an oriented manifold is a manifold with a choice of orientations for each tangent space so that for $p \in M$, there is an open set $U$ and a collection of vector fields $X_1,...,X_n$ so that for all $q \in U$ $X_1(q),...,X_n(q)$, is a basis of $T_qM$ belonging to the orientation of $T_qM$.  What does it mean for manifold to be ""orientable""?  Is it the same thing as oriented?",,"['differential-geometry', 'multivariable-calculus', 'differential-topology']"
85,Vector Calculus - Curl of Vector,Vector Calculus - Curl of Vector,,"I'm asked to prove the following identity, using index notation: $(\nabla\times A)\times A=A \cdot\nabla A - \nabla(A \cdot A)$ However, when I work it out, I find that the actual solution should be: $(\nabla\times A)\times A=A \cdot\nabla A - \frac{1}{2}\nabla(A \cdot A)$ Am I missing something, or is the book wrong?","I'm asked to prove the following identity, using index notation: $(\nabla\times A)\times A=A \cdot\nabla A - \nabla(A \cdot A)$ However, when I work it out, I find that the actual solution should be: $(\nabla\times A)\times A=A \cdot\nabla A - \frac{1}{2}\nabla(A \cdot A)$ Am I missing something, or is the book wrong?",,"['multivariable-calculus', 'vector-analysis', 'tensors']"
86,"Is $\int_{\mathbb{R}^D}\exp(\mathbf{y}^\top\mathbf{S}\mathbf{y})  \mathbf{y}\,d\mathbf{y}$ equal to $\mathbf{0}$?",Is  equal to ?,"\int_{\mathbb{R}^D}\exp(\mathbf{y}^\top\mathbf{S}\mathbf{y})  \mathbf{y}\,d\mathbf{y} \mathbf{0}","I want to show that $\int_{\mathbb{R}^D}\exp(\mathbf{y}^\top\mathbf{S}\mathbf{y})\mathbf{y} \, d\mathbf{y}$, where $\mathbf{S}$ is a symmetric real matrix, is equal to $\mathbf{0}$ . My intuition (although very immature) hints that it is true, but I can't show that — actually, it looks like the opposite is true. $\int_{\mathbb{R}^D}\exp(\mathbf{y}^\top\mathbf{S}\mathbf{y})\mathbf{y}d\mathbf{y} = \int_{\mathbb{R}^D}\prod_{i,j}\exp(y_i y_j s_{ij})\mathbf{y} \, d\mathbf{y}=\mathbf{v}$; $v_i=\int_{\mathbb{R}^D}\prod_{i,j}\exp(y_i y_j s_{ij})y_i \, d\mathbf{y} = \int_{-\infty}^\infty dy_{k\neq i,j}(\int_{-\infty}^\infty(\int_{-\infty}^\infty{\exp(y_i^2+2y_iy_j)}dy_i)\exp(y_j^2)dy_j)$. $v_i=0\Leftarrow\forall{a}\;\int_{-\infty}^\infty\exp(x^2+2ax)x \, dx=0$. Seems like the only hope would be for the function under the last integral to be antisymmetric with regard to some $x=b$, but it's clear that it is not for any $a\neq 0$. Where's my mistake? I want to prove to myself that the title integral is zero... [UPDATE] I apologize to the comminuty for not clarifying what is meant by ""$\mathbf{y}\;d\mathbf{y}$"", which caused some misunderstanding between the commenters, the reason being my apparent ignorance. This question comes from me trying to derive multivariate Gaussian as the probability distribution with maximum entropy (with the given mean and variance); this is given as an exercise in the textbook I'm working through, and the mean is given by the integral $\int{p(\mathbf{x})\mathbf{x}d\mathbf{x}} = \mathbf{\mu}$ ($\mathbf{\mu}$ is a vector with $D$ elements.) I have a vague understanding of this notion, I supposed it means that the $i$-th component of this vector is given by the integral $\int{p(\mathbf{x})x_idx_1dx_2...dx_n}$. Seems like it's more a volume unit integral, not a dot product. Am I right here? Also, there was some confusion on the Riemann integral vs. v.p. I don't think v.p. is what I need, after some consideration; please excuse me for misleading you.","I want to show that $\int_{\mathbb{R}^D}\exp(\mathbf{y}^\top\mathbf{S}\mathbf{y})\mathbf{y} \, d\mathbf{y}$, where $\mathbf{S}$ is a symmetric real matrix, is equal to $\mathbf{0}$ . My intuition (although very immature) hints that it is true, but I can't show that — actually, it looks like the opposite is true. $\int_{\mathbb{R}^D}\exp(\mathbf{y}^\top\mathbf{S}\mathbf{y})\mathbf{y}d\mathbf{y} = \int_{\mathbb{R}^D}\prod_{i,j}\exp(y_i y_j s_{ij})\mathbf{y} \, d\mathbf{y}=\mathbf{v}$; $v_i=\int_{\mathbb{R}^D}\prod_{i,j}\exp(y_i y_j s_{ij})y_i \, d\mathbf{y} = \int_{-\infty}^\infty dy_{k\neq i,j}(\int_{-\infty}^\infty(\int_{-\infty}^\infty{\exp(y_i^2+2y_iy_j)}dy_i)\exp(y_j^2)dy_j)$. $v_i=0\Leftarrow\forall{a}\;\int_{-\infty}^\infty\exp(x^2+2ax)x \, dx=0$. Seems like the only hope would be for the function under the last integral to be antisymmetric with regard to some $x=b$, but it's clear that it is not for any $a\neq 0$. Where's my mistake? I want to prove to myself that the title integral is zero... [UPDATE] I apologize to the comminuty for not clarifying what is meant by ""$\mathbf{y}\;d\mathbf{y}$"", which caused some misunderstanding between the commenters, the reason being my apparent ignorance. This question comes from me trying to derive multivariate Gaussian as the probability distribution with maximum entropy (with the given mean and variance); this is given as an exercise in the textbook I'm working through, and the mean is given by the integral $\int{p(\mathbf{x})\mathbf{x}d\mathbf{x}} = \mathbf{\mu}$ ($\mathbf{\mu}$ is a vector with $D$ elements.) I have a vague understanding of this notion, I supposed it means that the $i$-th component of this vector is given by the integral $\int{p(\mathbf{x})x_idx_1dx_2...dx_n}$. Seems like it's more a volume unit integral, not a dot product. Am I right here? Also, there was some confusion on the Riemann integral vs. v.p. I don't think v.p. is what I need, after some consideration; please excuse me for misleading you.",,"['calculus', 'multivariable-calculus']"
87,How do I find the derivative of a definite integral in which the variable of differentiation is a limit in the integral?,How do I find the derivative of a definite integral in which the variable of differentiation is a limit in the integral?,,"The derivative I would like to find is $$\frac{\partial}{\partial z} \int_{-z_0}^z (z - z^{\prime}) \; f(z^{\prime}) \; dz^{\prime}$$ where $f(z^{\prime})$ is some arbitrary function of $z^{\prime}$.  So $z$ actually appears twice: once in the upper limit of integration, and once in the integrand.","The derivative I would like to find is $$\frac{\partial}{\partial z} \int_{-z_0}^z (z - z^{\prime}) \; f(z^{\prime}) \; dz^{\prime}$$ where $f(z^{\prime})$ is some arbitrary function of $z^{\prime}$.  So $z$ actually appears twice: once in the upper limit of integration, and once in the integrand.",,"['calculus', 'multivariable-calculus']"
88,Calculating a Multivariable derivative.,Calculating a Multivariable derivative.,,"I'm trying to work through Spivak's Calculus on Manifolds and I've arrived at Differentiation.  While I can usually follow his steps, I find myself lost or stuck when I try to do something on my own.  So I decided to work through one of his first examples using $Df$ notation instead of $f'$ notation. My main point, I have confused myself.  My question is clearly asked only at the very bottom of this post. As for the example, I need to calculate the derivative of $f:\mathbb{R}^{2}\to \mathbb{R}$, where $$f(x,y) = \sin(xy^2).$$ The following rules are available to me: 1) For a point $a$ in the domain of $f$ such that $f(a)$ is in the domain of $g$, $$D(g\circ f)(a) = Dg(f(a))\circ Df(a).$$ 2) For two functions $f,g:\mathbb{R}^{n}\to \mathbb{R}$,  $$D(fg)(a) = g(a)Df(a) + f(a)Dg(a)$$ and $$D(f+g)(a) = Df(a) + Dg(a).$$ If I have stated either of these rules even slightly incorrectly please be brutally in my face about it. I'm trying to carefully apply this rules to my function. If I let $p,s:\mathbb{R}^{2}\to \mathbb{R}$ denote the product function and $s:\mathbb{R}\to \mathbb{R}$ represent the squaring function, I can write: $f = \sin\circ p\circ (\pi_{1}, s\circ \pi_{2})$, where $\pi_{1}$ and $\pi_{2}$ are the coordinate functions. Now my derivative of $f$, denoted $Df$, should be a map from $\mathbb{R}^{2}\to \mathbb{R}$, just like $f$ is. So at a point $(a,b)\in \mathbb{R}^{2}$, I can write \begin{align*} Df(a,b) &= D\left(\sin\circ p\circ (\pi_{1}, s\circ \pi_{2})\right)(a,b)\\         &= D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))\circ Dp((\pi_{1}, s\circ \pi_{2})(a,b))\circ D(\pi_{1}, s\circ \pi_{2})(a,b) \end{align*} So I try to calculate this in separate blocks: \begin{align*} D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b)) &=  \cos(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))\\ &= \cos(p\circ (\pi_{1}(a,b), [s\circ \pi_{2}](a,b)))\\ &= \cos(p\circ (a, s(b)))\\ &= \cos(p\circ (a, b^2)))\\ &= \cos(ab^2). \end{align*} But this brings me to my first (among several) points of confusion. In the equation: $$Df(a,b) = D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))\circ Dp((\pi_{1}, s\circ \pi_{2})(a,b))\circ D(\pi_{1}, s\circ \pi_{2})(a,b)$$ it appears $D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))$ should be a function, not a number.  Can someone point out what my error in thinking is? (answered below) Continuing on to compute the 3rd block, \begin{align*} D(\pi_{1}, s\circ \pi_{2})(a,b) &= (D\pi_{1}(a,b), D(s\circ \pi_{2})(a,b))\\ &= (\pi_{1}(a,b), Ds(\pi_{2}(a,b))\circ D\pi_{2}(a,b))\\ &= (a, Ds(b)\circ \pi_{2}(a,b))\\ &= (a, 2b\circ b)\\ &= (a, 2b^2) \end{align*} Now the middle one: \begin{align*} Dp((\pi_{1}, s\circ\pi_{2})(a,b)) &= Dp((\pi_{1}(a,b), (s\circ \pi_{2})(a,b))\\ &= Dp(a, b^2) \end{align*} Now substituting these smaller calculations, the whole thing simplifies down to: \begin{align*} Df(a,b) &= D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))\circ Dp((\pi_{1}, s\circ \pi_{2})(a,b))\circ D(\pi_{1}, s\circ \pi_{2})(a,b)\\ &= \cos(ab^2)\circ \underbrace{Dp(a, b^2)\circ (a, 2b^2)}_{= a\cdot 2b^2 + b^2\cdot a}\\ &= \cos(ab^2)(3ab^2) \end{align*} Now I will insist that I have something wrong.  $Df(a,b)$ should be a map from $\mathbb{R}^{2}\to \mathbb{R}$. But it has collapsed into a single real number. Spivak calculates the derivative using Jacobian notation, arriving at the conclusion that $f'(a,b) = (b^2\cdot\cos(ab^2), 2ab\cdot \cos(ab^2))$, which naturally is the transformation matrix for a map $\mathbb{R}^{2}\to \mathbb{R}$. Sorry this problem is so long winded, but I wanted to show all my steps so as to be able to identify the one that went awry.","I'm trying to work through Spivak's Calculus on Manifolds and I've arrived at Differentiation.  While I can usually follow his steps, I find myself lost or stuck when I try to do something on my own.  So I decided to work through one of his first examples using $Df$ notation instead of $f'$ notation. My main point, I have confused myself.  My question is clearly asked only at the very bottom of this post. As for the example, I need to calculate the derivative of $f:\mathbb{R}^{2}\to \mathbb{R}$, where $$f(x,y) = \sin(xy^2).$$ The following rules are available to me: 1) For a point $a$ in the domain of $f$ such that $f(a)$ is in the domain of $g$, $$D(g\circ f)(a) = Dg(f(a))\circ Df(a).$$ 2) For two functions $f,g:\mathbb{R}^{n}\to \mathbb{R}$,  $$D(fg)(a) = g(a)Df(a) + f(a)Dg(a)$$ and $$D(f+g)(a) = Df(a) + Dg(a).$$ If I have stated either of these rules even slightly incorrectly please be brutally in my face about it. I'm trying to carefully apply this rules to my function. If I let $p,s:\mathbb{R}^{2}\to \mathbb{R}$ denote the product function and $s:\mathbb{R}\to \mathbb{R}$ represent the squaring function, I can write: $f = \sin\circ p\circ (\pi_{1}, s\circ \pi_{2})$, where $\pi_{1}$ and $\pi_{2}$ are the coordinate functions. Now my derivative of $f$, denoted $Df$, should be a map from $\mathbb{R}^{2}\to \mathbb{R}$, just like $f$ is. So at a point $(a,b)\in \mathbb{R}^{2}$, I can write \begin{align*} Df(a,b) &= D\left(\sin\circ p\circ (\pi_{1}, s\circ \pi_{2})\right)(a,b)\\         &= D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))\circ Dp((\pi_{1}, s\circ \pi_{2})(a,b))\circ D(\pi_{1}, s\circ \pi_{2})(a,b) \end{align*} So I try to calculate this in separate blocks: \begin{align*} D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b)) &=  \cos(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))\\ &= \cos(p\circ (\pi_{1}(a,b), [s\circ \pi_{2}](a,b)))\\ &= \cos(p\circ (a, s(b)))\\ &= \cos(p\circ (a, b^2)))\\ &= \cos(ab^2). \end{align*} But this brings me to my first (among several) points of confusion. In the equation: $$Df(a,b) = D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))\circ Dp((\pi_{1}, s\circ \pi_{2})(a,b))\circ D(\pi_{1}, s\circ \pi_{2})(a,b)$$ it appears $D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))$ should be a function, not a number.  Can someone point out what my error in thinking is? (answered below) Continuing on to compute the 3rd block, \begin{align*} D(\pi_{1}, s\circ \pi_{2})(a,b) &= (D\pi_{1}(a,b), D(s\circ \pi_{2})(a,b))\\ &= (\pi_{1}(a,b), Ds(\pi_{2}(a,b))\circ D\pi_{2}(a,b))\\ &= (a, Ds(b)\circ \pi_{2}(a,b))\\ &= (a, 2b\circ b)\\ &= (a, 2b^2) \end{align*} Now the middle one: \begin{align*} Dp((\pi_{1}, s\circ\pi_{2})(a,b)) &= Dp((\pi_{1}(a,b), (s\circ \pi_{2})(a,b))\\ &= Dp(a, b^2) \end{align*} Now substituting these smaller calculations, the whole thing simplifies down to: \begin{align*} Df(a,b) &= D(\sin)(p\circ (\pi_{1}, s\circ \pi_{2})(a,b))\circ Dp((\pi_{1}, s\circ \pi_{2})(a,b))\circ D(\pi_{1}, s\circ \pi_{2})(a,b)\\ &= \cos(ab^2)\circ \underbrace{Dp(a, b^2)\circ (a, 2b^2)}_{= a\cdot 2b^2 + b^2\cdot a}\\ &= \cos(ab^2)(3ab^2) \end{align*} Now I will insist that I have something wrong.  $Df(a,b)$ should be a map from $\mathbb{R}^{2}\to \mathbb{R}$. But it has collapsed into a single real number. Spivak calculates the derivative using Jacobian notation, arriving at the conclusion that $f'(a,b) = (b^2\cdot\cos(ab^2), 2ab\cdot \cos(ab^2))$, which naturally is the transformation matrix for a map $\mathbb{R}^{2}\to \mathbb{R}$. Sorry this problem is so long winded, but I wanted to show all my steps so as to be able to identify the one that went awry.",,"['differential-geometry', 'multivariable-calculus', 'derivatives']"
89,Calculating the surface area of sphere above a plane,Calculating the surface area of sphere above a plane,,"How do I calculate the surface area of the unit sphere above the plane $z=\frac12$? EDIT: I have been attempting things and I am thinking about parameterizing this... While I know that surface area is given by the double integral of the cross products of partial derivatives of the new parameters, I don't know what to set them to.. (sorry I'm not good with the fancy notation)","How do I calculate the surface area of the unit sphere above the plane $z=\frac12$? EDIT: I have been attempting things and I am thinking about parameterizing this... While I know that surface area is given by the double integral of the cross products of partial derivatives of the new parameters, I don't know what to set them to.. (sorry I'm not good with the fancy notation)",,['multivariable-calculus']
90,Area of circle cut by line,Area of circle cut by line,,"Say I have a circle $x^2+y^2=R^2$ and a line $2ax+2by=R^2$ ($a,b>0$). How might I go about measuring the area of the smaller part of the circle cut off by the line? (This question is relevant to Calculate the volume between $z=x^2+y^2$ and $z=2ax+2by$ ) Thanks! P.S. I'm not sure if this question is correctly tagged. Please correct me if I tagged it wrongly.","Say I have a circle $x^2+y^2=R^2$ and a line $2ax+2by=R^2$ ($a,b>0$). How might I go about measuring the area of the smaller part of the circle cut off by the line? (This question is relevant to Calculate the volume between $z=x^2+y^2$ and $z=2ax+2by$ ) Thanks! P.S. I'm not sure if this question is correctly tagged. Please correct me if I tagged it wrongly.",,['multivariable-calculus']
91,Finding derivative of dot-product of two vectors,Finding derivative of dot-product of two vectors,,"I have to find the derivative of the dot-product of two vectors using the product rule. It took me an hour, checked every component and double checked, and then when I check it on Wolfram, of course it is wrong. I have two vectors: $u(t) = \langle-\sqrt{2}\sin(t), t, t^{2/3}\rangle$ and $v(t) = \langle-\sqrt{2}\sin(t), \cos^{2}(t), t^{-1/3}\rangle$ Since by product rule $\frac{d}{dt}[u(t) \cdot v(t)] = u'(t) \cdot v(t) + u(t) \cdot v'(t)$, I need to differentiate each vector before finding the sum-of-products by finding the component of each derivative. For $\vec{u'(t)}$ I found: $$\Bigg\langle\frac{-\sin(t) + 2t\cos(t)}{2\sqrt{t}}, 1, \frac{2}{3t^{1/3}}\Bigg\rangle$$ For $\vec{v'(t)}$ I found: $$\Bigg\langle\frac{-\sin(t) + 2t\cos(t)}{2\sqrt{t}}, -2\sin(t)\cos(t), \frac{-1}{3t^{4/3}}\Bigg\rangle$$ Finding $\vec{u'(t)} \cdot v(t)$: $$\Bigg\langle\frac{\sin^{2}(t)}{2} + t\sin(t)\cos(t), \cos^{2}(t), \frac{-2}{3t^{2/3}}\Bigg\rangle$$ Finding $\vec{u(t)} \cdot v'(t)$: $$\Bigg\langle\frac{\sin^{2}(t)}{2} + t\sin(t)\cos(t), -tsin(2t), \frac{-1}{3t^{2/3}}\Bigg\rangle$$ Finally, after finding the products, this is what I calculated for the sum: $$\Bigg\langle\sin^{2}(t) + 2t\sin(t)\cos(t), \cos^{2}(t)-t\sin(2t), \frac{-1}{t^{2/3}}\Bigg\rangle$$ But apparently, according to Wolfram, this is wrong. Where did I go astray here?","I have to find the derivative of the dot-product of two vectors using the product rule. It took me an hour, checked every component and double checked, and then when I check it on Wolfram, of course it is wrong. I have two vectors: $u(t) = \langle-\sqrt{2}\sin(t), t, t^{2/3}\rangle$ and $v(t) = \langle-\sqrt{2}\sin(t), \cos^{2}(t), t^{-1/3}\rangle$ Since by product rule $\frac{d}{dt}[u(t) \cdot v(t)] = u'(t) \cdot v(t) + u(t) \cdot v'(t)$, I need to differentiate each vector before finding the sum-of-products by finding the component of each derivative. For $\vec{u'(t)}$ I found: $$\Bigg\langle\frac{-\sin(t) + 2t\cos(t)}{2\sqrt{t}}, 1, \frac{2}{3t^{1/3}}\Bigg\rangle$$ For $\vec{v'(t)}$ I found: $$\Bigg\langle\frac{-\sin(t) + 2t\cos(t)}{2\sqrt{t}}, -2\sin(t)\cos(t), \frac{-1}{3t^{4/3}}\Bigg\rangle$$ Finding $\vec{u'(t)} \cdot v(t)$: $$\Bigg\langle\frac{\sin^{2}(t)}{2} + t\sin(t)\cos(t), \cos^{2}(t), \frac{-2}{3t^{2/3}}\Bigg\rangle$$ Finding $\vec{u(t)} \cdot v'(t)$: $$\Bigg\langle\frac{\sin^{2}(t)}{2} + t\sin(t)\cos(t), -tsin(2t), \frac{-1}{3t^{2/3}}\Bigg\rangle$$ Finally, after finding the products, this is what I calculated for the sum: $$\Bigg\langle\sin^{2}(t) + 2t\sin(t)\cos(t), \cos^{2}(t)-t\sin(2t), \frac{-1}{t^{2/3}}\Bigg\rangle$$ But apparently, according to Wolfram, this is wrong. Where did I go astray here?",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
92,pointwise multiplication in function spaces,pointwise multiplication in function spaces,,"It is well - known that a point - wise multiplication operation can be defined on some function spaces. For example, $C([0,1])$, the vector space of a real - valued (say) continuous functions $f$ defined on the unit - interval $[0,1] \subset \mathbb{R}$, admits such a multiplication, defined as \begin{equation} (fg)(x) := f(x)g(x), \quad f,g \in C([0,1]) \end{equation} Now, for function spaces whose elements are defined on a countable set, say \begin{equation} \mathbb{R}^k = \{ x: \mathbb{N}_k \to \mathbb{R}, \quad n \mapsto x_n, \quad n = 1, \dots, k \} \quad (k \in \mathbb{N}) \end{equation} or \begin{equation} \mathbb{R}^\infty = \{ x: \mathbb{N} \to \mathbb{R}, \quad n \mapsto x_n, \quad n = 1,2 \dots \,\} \quad \end{equation} I wonder why this is not done analogously. So, define pointwise multiplication by \begin{equation} (xy)(n) := x_ny_n \end{equation} For example, in the ""column - notation"", two elements of $\mathbb{R}^2$, $x = (x_1,x_2)$ and $y = (y_1,y_2)$ would then have the product $ xy = (x_1y_1, x_2y_2)$. I realize there must be something that makes this obvious operation utterly useless, for otherwise, it would be used commonly. Any hints as to why this attempt to define pointwise multiplication on these functions spaces is uninteresting would be great ! From what I understand so far (unfortunately far to little), I wonder whether the cardinality of the domain (uncountable in the case $[0,1]$, countable in the cases $\mathbb{N}_k = \{1,\dots k\}$ and $\mathbb{N}$) makes a crucial difference, whether there is some algebraic property that breaks down, or whether it is something completely different ? Thanks for your feedback and help!","It is well - known that a point - wise multiplication operation can be defined on some function spaces. For example, $C([0,1])$, the vector space of a real - valued (say) continuous functions $f$ defined on the unit - interval $[0,1] \subset \mathbb{R}$, admits such a multiplication, defined as \begin{equation} (fg)(x) := f(x)g(x), \quad f,g \in C([0,1]) \end{equation} Now, for function spaces whose elements are defined on a countable set, say \begin{equation} \mathbb{R}^k = \{ x: \mathbb{N}_k \to \mathbb{R}, \quad n \mapsto x_n, \quad n = 1, \dots, k \} \quad (k \in \mathbb{N}) \end{equation} or \begin{equation} \mathbb{R}^\infty = \{ x: \mathbb{N} \to \mathbb{R}, \quad n \mapsto x_n, \quad n = 1,2 \dots \,\} \quad \end{equation} I wonder why this is not done analogously. So, define pointwise multiplication by \begin{equation} (xy)(n) := x_ny_n \end{equation} For example, in the ""column - notation"", two elements of $\mathbb{R}^2$, $x = (x_1,x_2)$ and $y = (y_1,y_2)$ would then have the product $ xy = (x_1y_1, x_2y_2)$. I realize there must be something that makes this obvious operation utterly useless, for otherwise, it would be used commonly. Any hints as to why this attempt to define pointwise multiplication on these functions spaces is uninteresting would be great ! From what I understand so far (unfortunately far to little), I wonder whether the cardinality of the domain (uncountable in the case $[0,1]$, countable in the cases $\mathbb{N}_k = \{1,\dots k\}$ and $\mathbb{N}$) makes a crucial difference, whether there is some algebraic property that breaks down, or whether it is something completely different ? Thanks for your feedback and help!",,"['abstract-algebra', 'functional-analysis', 'multivariable-calculus']"
93,polynomial-torus,polynomial-torus,,"I am wondering if it is possible to prove (or come up with an explicit example) that there is a polynomial $f ( x,y,z  )$ of degree 8 such that the set $f ( x,y,z  )=0$ is a union of two torri? Any specific proof or example of such polynomial? Thanks","I am wondering if it is possible to prove (or come up with an explicit example) that there is a polynomial $f ( x,y,z  )$ of degree 8 such that the set $f ( x,y,z  )=0$ is a union of two torri? Any specific proof or example of such polynomial? Thanks",,"['general-topology', 'analysis', 'differential-geometry', 'multivariable-calculus']"
94,example of Diffeomorphism,example of Diffeomorphism,,"I am trying to come up with a diffeomorphism of the upper half plane $y> 0$ onto the first quadrant $x> 0 , y> 0$ Can anyone come up with such a diffeomorphism?","I am trying to come up with a diffeomorphism of the upper half plane $y> 0$ onto the first quadrant $x> 0 , y> 0$ Can anyone come up with such a diffeomorphism?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
95,"Computing $\lim_{(x,y)\to (0,0)}\frac{x+y}{\sqrt{x^2+y^2}}$",Computing,"\lim_{(x,y)\to (0,0)}\frac{x+y}{\sqrt{x^2+y^2}}","What is the result of  $\lim_{(x,y)\to (0,0)}\frac{x+y}{\sqrt{x^2+y^2}}$ . I tried to do couple of algebraic manipulations, but I didn't reach to any conclusion. Thanks a lot.","What is the result of  $\lim_{(x,y)\to (0,0)}\frac{x+y}{\sqrt{x^2+y^2}}$ . I tried to do couple of algebraic manipulations, but I didn't reach to any conclusion. Thanks a lot.",,"['calculus', 'limits', 'multivariable-calculus']"
96,union(new: intersection) of any number of open sets is also open [closed],union(new: intersection) of any number of open sets is also open [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 9 years ago . Improve this question I've just begun reading Spivak's Calculus on Manifold and attempted to proof this simple result. -I've updated my proof- My proof are as follows, img http://dl.dropbox.com/u/5681270/open-set%20proofs.png My proof for the intersection case still looks kinda dubious though. @Devan Ware, the notation $N_{\epsilon}(x)$ looks very useful to me but i haven't seen it anywhere, which branch of math is it found in and where can i learn more about it?","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 9 years ago . Improve this question I've just begun reading Spivak's Calculus on Manifold and attempted to proof this simple result. -I've updated my proof- My proof are as follows, img http://dl.dropbox.com/u/5681270/open-set%20proofs.png My proof for the intersection case still looks kinda dubious though. @Devan Ware, the notation $N_{\epsilon}(x)$ looks very useful to me but i haven't seen it anywhere, which branch of math is it found in and where can i learn more about it?",,['multivariable-calculus']
97,What is the integral of three orthonormal basis functions?,What is the integral of three orthonormal basis functions?,,"The hyperspherical harmonics, given by: $Z_{l,m}^n(\omega,\theta,\phi)=(-i)^l\frac{2^{l+1/2}l!}{2\pi}\sqrt{(2l+1)\frac{(l-m)!}{(l+m)!}\frac{(n+1)(n-l)!}{(n+l+1)!}}\sin^l(\omega/2)C_{n-l}^{l+1}(\cos(\omega/2))P_l^m(\cos\theta)\exp(im\phi)$ (where $C_{n-l}^{l+1}$ is a Gegenbauer polynomial, and $P_l^m$ is an associated Legendre function), form an orthonormal basis for an expansion of functions on the hypersphere. As such, the following identity is true: $\int_\Omega Z_{l,m}^n(\omega,\theta,\phi)Z_{l',m'}^{n'}(\omega,\theta,\phi)d\Omega=\delta_{n,n'}\delta_{l,l'}\delta_{m,m'}$ My question is whether or not the following identity is also true: $\int_\Omega Z_{l,m}^n(\omega,\theta,\phi)Z_{l',m'}^{n'}(\omega,\theta,\phi)Z_{l'',m''}^{n''}(\omega,\theta,\phi)d\Omega=\delta_{n,n',n''}\delta_{l,l',l''}\delta_{m,m',m''}$ Note: $\Omega$ is the entire domain, i.e., $\omega\in[0,\pi], \theta\in[0,\pi], \phi\in[0,2\pi]$, and $d\Omega=\frac{1}{2}\sin^2(\omega/2)\sin(\theta)d\omega d\theta d\phi$","The hyperspherical harmonics, given by: $Z_{l,m}^n(\omega,\theta,\phi)=(-i)^l\frac{2^{l+1/2}l!}{2\pi}\sqrt{(2l+1)\frac{(l-m)!}{(l+m)!}\frac{(n+1)(n-l)!}{(n+l+1)!}}\sin^l(\omega/2)C_{n-l}^{l+1}(\cos(\omega/2))P_l^m(\cos\theta)\exp(im\phi)$ (where $C_{n-l}^{l+1}$ is a Gegenbauer polynomial, and $P_l^m$ is an associated Legendre function), form an orthonormal basis for an expansion of functions on the hypersphere. As such, the following identity is true: $\int_\Omega Z_{l,m}^n(\omega,\theta,\phi)Z_{l',m'}^{n'}(\omega,\theta,\phi)d\Omega=\delta_{n,n'}\delta_{l,l'}\delta_{m,m'}$ My question is whether or not the following identity is also true: $\int_\Omega Z_{l,m}^n(\omega,\theta,\phi)Z_{l',m'}^{n'}(\omega,\theta,\phi)Z_{l'',m''}^{n''}(\omega,\theta,\phi)d\Omega=\delta_{n,n',n''}\delta_{l,l',l''}\delta_{m,m',m''}$ Note: $\Omega$ is the entire domain, i.e., $\omega\in[0,\pi], \theta\in[0,\pi], \phi\in[0,2\pi]$, and $d\Omega=\frac{1}{2}\sin^2(\omega/2)\sin(\theta)d\omega d\theta d\phi$",,"['special-functions', 'multivariable-calculus', 'inner-products', 'spherical-harmonics']"
98,Problem with Repeated Integrals,Problem with Repeated Integrals,,"I havent had the time to familiarize myself with Latex quite yet, so please excuse my formatting.  I have attempted the following problem four times and got four completely different answers. $$\int_0^1\int_1^2\int_0^{x+y}12(4x+y+3z)^2 dz dy dx$$ to my understanding, the first integral should equal: $$\frac{4}{3}(7x+4y)^3$$ The second would be: $$\frac{1}{12}(7x+8)^4-\frac{1}{12}(7x+4)^4$$ And the final integral: $$\frac{1}{420}(7+8)^5-\frac{1}{420}(7+4)^5$$ or 1424.59 Again I've tried several different methods receiving different answers, each marked as wrong on the homework website. I think I'm missing something basic here, but I dont know what.","I havent had the time to familiarize myself with Latex quite yet, so please excuse my formatting.  I have attempted the following problem four times and got four completely different answers. $$\int_0^1\int_1^2\int_0^{x+y}12(4x+y+3z)^2 dz dy dx$$ to my understanding, the first integral should equal: $$\frac{4}{3}(7x+4y)^3$$ The second would be: $$\frac{1}{12}(7x+8)^4-\frac{1}{12}(7x+4)^4$$ And the final integral: $$\frac{1}{420}(7+8)^5-\frac{1}{420}(7+4)^5$$ or 1424.59 Again I've tried several different methods receiving different answers, each marked as wrong on the homework website. I think I'm missing something basic here, but I dont know what.",,"['integration', 'multivariable-calculus']"
99,Question about total derivative,Question about total derivative,,"If $z=f(x,y)$, then total derivative is $\mathrm{d}z=\frac{\partial f}{\partial x}\mathrm{d}x+\frac{\partial f}{\partial y}\mathrm{d}y$. If $\mathrm{d} z=0$, how do you show that $z$ is a constant?","If $z=f(x,y)$, then total derivative is $\mathrm{d}z=\frac{\partial f}{\partial x}\mathrm{d}x+\frac{\partial f}{\partial y}\mathrm{d}y$. If $\mathrm{d} z=0$, how do you show that $z$ is a constant?",,['multivariable-calculus']
