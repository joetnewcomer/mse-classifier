,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Computing Fourier transform for $L^2$ function,Computing Fourier transform for  function,L^2,"For a function $f\in L^1(\mathbb{R})$, its Fourier transform is defined as $$\hat{f}(y)=\int_{-\infty}^\infty f(x)e^{-ixy}dx$$ For a function $f\in L^2(\mathbb{R})$, its Fourier transform is defined as the unique continuous mapping $g:L^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ that extends the mapping $h:S\rightarrow L^2(\mathbb{R})$, where $S$ is the Schwartz class, and the Fourier transform of a function in the Schwartz class is defined as in the first paragraph. (We may assume that this continuous mapping $g$ exists and is unique.) Suppose $f\in L^2(\mathbb{R})$, and let $c>0$.  Show that $$\lim_{c\rightarrow\infty}\int_{-c}^cf(x)e^{-ixy}dx$$ exists in the $L^2$ sense and is equal to $\hat{f}$ defined above. Define $f_c(x)$ to be $f(x)$ when $|x|\leq c$ and $0$ when $|x|>c$. Then the limit in question is $$\lim_{c\rightarrow\infty}\int_{-\infty}^\infty f_c(x)e^{-ixy}dx$$ The questions are: 1) Why does this limit exist? 2) Why does it equal $\hat{f}$ defined as the unique extension from the Schwartz class? We know by the dominated convergence theorem that $\|f_c-f\|_2\rightarrow 0$ as $c\rightarrow\infty$. Might that help?","For a function $f\in L^1(\mathbb{R})$, its Fourier transform is defined as $$\hat{f}(y)=\int_{-\infty}^\infty f(x)e^{-ixy}dx$$ For a function $f\in L^2(\mathbb{R})$, its Fourier transform is defined as the unique continuous mapping $g:L^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ that extends the mapping $h:S\rightarrow L^2(\mathbb{R})$, where $S$ is the Schwartz class, and the Fourier transform of a function in the Schwartz class is defined as in the first paragraph. (We may assume that this continuous mapping $g$ exists and is unique.) Suppose $f\in L^2(\mathbb{R})$, and let $c>0$.  Show that $$\lim_{c\rightarrow\infty}\int_{-c}^cf(x)e^{-ixy}dx$$ exists in the $L^2$ sense and is equal to $\hat{f}$ defined above. Define $f_c(x)$ to be $f(x)$ when $|x|\leq c$ and $0$ when $|x|>c$. Then the limit in question is $$\lim_{c\rightarrow\infty}\int_{-\infty}^\infty f_c(x)e^{-ixy}dx$$ The questions are: 1) Why does this limit exist? 2) Why does it equal $\hat{f}$ defined as the unique extension from the Schwartz class? We know by the dominated convergence theorem that $\|f_c-f\|_2\rightarrow 0$ as $c\rightarrow\infty$. Might that help?",,"['real-analysis', 'integration', 'fourier-analysis', 'fourier-series']"
1,A discontinuous function such that $f(x + y) = f(x) + f(y)$ [duplicate],A discontinuous function such that  [duplicate],f(x + y) = f(x) + f(y),This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 10 years ago . Is it possible to construct a function $f \colon \mathbb{R} \to \mathbb{R}$ such that $$f(x + y) = f(x) + f(y)$$ and $f$ is not continuous?,This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 10 years ago . Is it possible to construct a function $f \colon \mathbb{R} \to \mathbb{R}$ such that $$f(x + y) = f(x) + f(y)$$ and $f$ is not continuous?,,['real-analysis']
2,How to write well in analysis (calculus)?,How to write well in analysis (calculus)?,,"This is kind of a subjective question, I know; often I find myself failing exams and homeworks because of the way i write down proofs. Either I don't know how to start, or somehow the main point of the proof is lost. I've noticed that in many books there's an ""style"" but doesn't matter how much i try to mimic it, i can't seem to do it right. I'm more of an algebra person, I love it, but Analysis, well... isn't my strong suit. If you have any tips I'll appreciate it. After a while of waiting, I couldn't get the writting that I wanted to post as an example, so here it is a ""fresh"" example of an excerisize I didn't finish: If $S$ is the set of all the sequences of real numbers, for $\bar x=(x_i), \bar y=(y_i) \in S$ we define: $$d(\bar x, \bar y)= \sum_{i=0}^\infty \frac {|x_i -y_i|}{2^i(1+|x_i-y_i|)}$$ (a) Prove that $d(\bar x, \bar y)$ is a metric in $S$. (b) Let $\bar x^k=(x_i^k),\bar x=(x_i)\in S$. Prove that:    $$\lim_{k\to \infty} d(\bar x^k,\bar x)=0 \Leftrightarrow \lim_{k\to \infty}x^k_i=x_i \quad \forall \; i\in \mathbb N$$ So I proved (a) , and I don't feel there's much to say there. However with (b) I got in trouble very easyly. ($\Leftarrow$) We know that $x_i^k \to x_i$ if $k\to \infty$, that means that $\forall \; \varepsilon>0 \; \exists \; m\in \mathbb N$ such that $\forall k>m \;\; |x_i^k -x_i|<\varepsilon $. On the other hand, what we want to prove is $\forall \; \varepsilon>0 \; \exists \; N\in \mathbb N$ such that $\forall k>N$ $$|\sum_{i=0}^\infty \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}|<\varepsilon$$ So what I tough is that, since we already have that $|x_i^k -x_i|<\varepsilon$ for any $\varepsilon >0$ so I did some ""reverse engineering"" , I took the absolut value that I want to prove and started to operate:  $$\mathbf {(1)}\;\;|\sum_{i=0}^\infty \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}| = \sum_{i=0}^\infty \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\varepsilon $$ $$\Rightarrow \sum_{i=0}^ n \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\varepsilon ,\;\; if\;\; n\to \infty $$ $$\Rightarrow \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\varepsilon $$ $$\Rightarrow \frac {|x_i^k -x_i|}{1+|x_i^k-x_i|}<2^i \varepsilon, \;\;where\;\; 2^i\;\;is\;\;constant\;\;with\;\;respect\;\;to\;\; k$$ From here, I'm pretty much frozen. What I wanted was to get to what we already had: $|x_i^k -x_i|<\varepsilon$, but I couldn't, somehow it seems futile. In class we already saw how to do it, and it is completly different from what I tried.","This is kind of a subjective question, I know; often I find myself failing exams and homeworks because of the way i write down proofs. Either I don't know how to start, or somehow the main point of the proof is lost. I've noticed that in many books there's an ""style"" but doesn't matter how much i try to mimic it, i can't seem to do it right. I'm more of an algebra person, I love it, but Analysis, well... isn't my strong suit. If you have any tips I'll appreciate it. After a while of waiting, I couldn't get the writting that I wanted to post as an example, so here it is a ""fresh"" example of an excerisize I didn't finish: If $S$ is the set of all the sequences of real numbers, for $\bar x=(x_i), \bar y=(y_i) \in S$ we define: $$d(\bar x, \bar y)= \sum_{i=0}^\infty \frac {|x_i -y_i|}{2^i(1+|x_i-y_i|)}$$ (a) Prove that $d(\bar x, \bar y)$ is a metric in $S$. (b) Let $\bar x^k=(x_i^k),\bar x=(x_i)\in S$. Prove that:    $$\lim_{k\to \infty} d(\bar x^k,\bar x)=0 \Leftrightarrow \lim_{k\to \infty}x^k_i=x_i \quad \forall \; i\in \mathbb N$$ So I proved (a) , and I don't feel there's much to say there. However with (b) I got in trouble very easyly. ($\Leftarrow$) We know that $x_i^k \to x_i$ if $k\to \infty$, that means that $\forall \; \varepsilon>0 \; \exists \; m\in \mathbb N$ such that $\forall k>m \;\; |x_i^k -x_i|<\varepsilon $. On the other hand, what we want to prove is $\forall \; \varepsilon>0 \; \exists \; N\in \mathbb N$ such that $\forall k>N$ $$|\sum_{i=0}^\infty \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}|<\varepsilon$$ So what I tough is that, since we already have that $|x_i^k -x_i|<\varepsilon$ for any $\varepsilon >0$ so I did some ""reverse engineering"" , I took the absolut value that I want to prove and started to operate:  $$\mathbf {(1)}\;\;|\sum_{i=0}^\infty \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}| = \sum_{i=0}^\infty \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\varepsilon $$ $$\Rightarrow \sum_{i=0}^ n \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\varepsilon ,\;\; if\;\; n\to \infty $$ $$\Rightarrow \frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\varepsilon $$ $$\Rightarrow \frac {|x_i^k -x_i|}{1+|x_i^k-x_i|}<2^i \varepsilon, \;\;where\;\; 2^i\;\;is\;\;constant\;\;with\;\;respect\;\;to\;\; k$$ From here, I'm pretty much frozen. What I wanted was to get to what we already had: $|x_i^k -x_i|<\varepsilon$, but I couldn't, somehow it seems futile. In class we already saw how to do it, and it is completly different from what I tried.",,"['real-analysis', 'soft-question', 'proof-writing']"
3,Averages of a Function,Averages of a Function,,"Let $(X,M,\mu)$ a measure space and $f:X \rightarrow \mathbb{C}$ a function in $L^{\infty}(\mu)$. Define $A_f$ as the set of all averages \begin{equation} \frac{1}{\mu(E)} \int_{E} f d \mu \end{equation} where $E \in M$ with $ 0 < \mu(E) < \infty$. An exercise in Rudin, Real and Complex Analysis, asks whether there exists a measure space such that the set $A_f$ is convex for all $f \in L^{\infty}(\mu)$. Obviously, a space $X$ containing only one point gives an affirmative answer. But does there exists a non trivial example? That is, a measure space $X$ in which $A_f$ contains more than one point for some $f$? Even in the special case in which $X=\mathbb{R}$ and $\mu$ is the Lebesgue measure, I don't know whether $A_f$ is convex for all $f \in L^{\infty}(\mu)$ or not. Do you have some idea? Thank you very very much for your help. My Best Regards, Maurizio Barbato PS If we define the essential range of $f \in L^{\infty}(\mu)$ as the set of all complex numbers $w$ such that \begin{equation} \mu({x: |f(x) - w| < \epsilon}) > 0 \end{equation} for all $\epsilon > 0$, then it is easy to prove that $R_f$ is compact. Moreover, the closure of $A_f$ is contained in the convex hull of $R_f$, and in the special case in which $X=\mathbb{R}$ and $\mu$ is the Lebesgue measure, we have  $cl(A_f)=conv(R_f)$. This was proved by prof. David Ullrich. See the posts http://mathforum.org/kb/thread.jspa?forumID=13&threadID=1465722&messageID=5234768 and http://mathforum.org/kb/thread.jspa?forumID=253&threadID=559133&messageID=1677532","Let $(X,M,\mu)$ a measure space and $f:X \rightarrow \mathbb{C}$ a function in $L^{\infty}(\mu)$. Define $A_f$ as the set of all averages \begin{equation} \frac{1}{\mu(E)} \int_{E} f d \mu \end{equation} where $E \in M$ with $ 0 < \mu(E) < \infty$. An exercise in Rudin, Real and Complex Analysis, asks whether there exists a measure space such that the set $A_f$ is convex for all $f \in L^{\infty}(\mu)$. Obviously, a space $X$ containing only one point gives an affirmative answer. But does there exists a non trivial example? That is, a measure space $X$ in which $A_f$ contains more than one point for some $f$? Even in the special case in which $X=\mathbb{R}$ and $\mu$ is the Lebesgue measure, I don't know whether $A_f$ is convex for all $f \in L^{\infty}(\mu)$ or not. Do you have some idea? Thank you very very much for your help. My Best Regards, Maurizio Barbato PS If we define the essential range of $f \in L^{\infty}(\mu)$ as the set of all complex numbers $w$ such that \begin{equation} \mu({x: |f(x) - w| < \epsilon}) > 0 \end{equation} for all $\epsilon > 0$, then it is easy to prove that $R_f$ is compact. Moreover, the closure of $A_f$ is contained in the convex hull of $R_f$, and in the special case in which $X=\mathbb{R}$ and $\mu$ is the Lebesgue measure, we have  $cl(A_f)=conv(R_f)$. This was proved by prof. David Ullrich. See the posts http://mathforum.org/kb/thread.jspa?forumID=13&threadID=1465722&messageID=5234768 and http://mathforum.org/kb/thread.jspa?forumID=253&threadID=559133&messageID=1677532",,"['real-analysis', 'analysis', 'measure-theory', 'convex-analysis']"
4,Every continuous function $f: \mathbb{R} \to \mathbb{R}$ is uniformly continuous on every bounded set.,Every continuous function  is uniformly continuous on every bounded set.,f: \mathbb{R} \to \mathbb{R},"Every continuous function $f: \mathbb{R} \to \mathbb{R}$ is uniformly continuous on every bounded set. Here's what I did so far.  Let $U \subset \mathbb{R}$ be bounded set.  Let $\epsilon> 0$. For every $x \in U$, there is a $\delta_x$ such that if $|a - x| < \delta_x$, then $|f(x) - f(a)| < \epsilon$.  Now here's, where I'm not sure if my reasoning is correct.  We have $$U \subseteq \bigcup_{x \in U} (x - \delta_x, x + \delta_x) $$ It seems intuitively clear that since $U$ is bounded, the union of only finitely many such intervals contains $U$, in which case we can simply let $\delta = \min\{\delta_x : x \in I\}$, where $I$ is a finite indexing set. Help please?","Every continuous function $f: \mathbb{R} \to \mathbb{R}$ is uniformly continuous on every bounded set. Here's what I did so far.  Let $U \subset \mathbb{R}$ be bounded set.  Let $\epsilon> 0$. For every $x \in U$, there is a $\delta_x$ such that if $|a - x| < \delta_x$, then $|f(x) - f(a)| < \epsilon$.  Now here's, where I'm not sure if my reasoning is correct.  We have $$U \subseteq \bigcup_{x \in U} (x - \delta_x, x + \delta_x) $$ It seems intuitively clear that since $U$ is bounded, the union of only finitely many such intervals contains $U$, in which case we can simply let $\delta = \min\{\delta_x : x \in I\}$, where $I$ is a finite indexing set. Help please?",,"['real-analysis', 'continuity', 'uniform-continuity']"
5,using fixed point theorem,using fixed point theorem,,"Hi I want to use the fixed point theorem to show that for  $G: \mathbb{R}^n \rightarrow \mathbb{R}^n$ $G(x)= \epsilon M x + \max(x,y)$, here $y$ is given and $\max(x,y)$ is the vector with component $\max(x_i,y_i)$, $M$ is negative definite $n \times n$ matrix, $G(x)=x$ has a solution. (Note this is for any $\epsilon >0 $). First, I showed that $G$ is a non expansive map for $\epsilon < \dfrac{1}{\|M\|}$. So the fixed point for contraction map wouldn't work so I changed direction. I know that $G$ is a continuous map, so if I can find a closed set $B$ (ball) in $\mathbb R^n$ such that $G$ maps from $B$ to $B$ then the Brower's fixed point theorem says that $G$ has a fixed point. I can not figure out $B$. I had difficulty to deal with $|\max(x,y)|$. Any thought on this would be very much appreciated!","Hi I want to use the fixed point theorem to show that for  $G: \mathbb{R}^n \rightarrow \mathbb{R}^n$ $G(x)= \epsilon M x + \max(x,y)$, here $y$ is given and $\max(x,y)$ is the vector with component $\max(x_i,y_i)$, $M$ is negative definite $n \times n$ matrix, $G(x)=x$ has a solution. (Note this is for any $\epsilon >0 $). First, I showed that $G$ is a non expansive map for $\epsilon < \dfrac{1}{\|M\|}$. So the fixed point for contraction map wouldn't work so I changed direction. I know that $G$ is a continuous map, so if I can find a closed set $B$ (ball) in $\mathbb R^n$ such that $G$ maps from $B$ to $B$ then the Brower's fixed point theorem says that $G$ has a fixed point. I can not figure out $B$. I had difficulty to deal with $|\max(x,y)|$. Any thought on this would be very much appreciated!",,"['real-analysis', 'linear-algebra', 'complex-analysis']"
6,Equivalence of definitions for upper semicontinuity,Equivalence of definitions for upper semicontinuity,,"I am trying to show that a function is upper semicontinuous if and only if the preimage of any open ray $(-\infty, a)$ is open. The definition given for upper semicontinuity is that $\lim\limits_{k \to \infty} x_k = x \implies \limsup\limits_{k\to \infty} f(x_k) \leq f(x)$. I find this definition hard to work with, as I have never been comfortable with $\limsup$ and $\liminf$. Can anyone give me a hint as to how to approach this? Thank you!","I am trying to show that a function is upper semicontinuous if and only if the preimage of any open ray $(-\infty, a)$ is open. The definition given for upper semicontinuity is that $\lim\limits_{k \to \infty} x_k = x \implies \limsup\limits_{k\to \infty} f(x_k) \leq f(x)$. I find this definition hard to work with, as I have never been comfortable with $\limsup$ and $\liminf$. Can anyone give me a hint as to how to approach this? Thank you!",,"['real-analysis', 'semicontinuous-functions']"
7,"$ W_0^{1,p}$ norm bounded by norm of Laplacian",norm bounded by norm of Laplacian," W_0^{1,p}","Let $f\in W_0^{1,p}(U)$, for $U$ a bounded domain and $p < n/(n-1)$. I am trying to prove that there is an inequality of the form $$\|f\|_{W^{1,p}} \leq C \int_{\Omega} |\Delta f| $$ where $\Delta$ is the Laplacian. I tried applying the Sobolev embedding theorem, followed by the inequality $$\|D^2 f\|_p \leq C \|\Delta f\|_p$$ but this does not hold for $p = 1$. I also tried using elliptic estimates, also to no avail. Does anyone have any suggestions? EDIT: Perhaps it is possible to deduce this from the $L^p$ inequality $$\|D^2 f\|_{L^p} \leq C \|\Delta f\|_{L^p}$$ for $u\in W_0^{2,p}(\Omega)$, $1 < p < \infty$?","Let $f\in W_0^{1,p}(U)$, for $U$ a bounded domain and $p < n/(n-1)$. I am trying to prove that there is an inequality of the form $$\|f\|_{W^{1,p}} \leq C \int_{\Omega} |\Delta f| $$ where $\Delta$ is the Laplacian. I tried applying the Sobolev embedding theorem, followed by the inequality $$\|D^2 f\|_p \leq C \|\Delta f\|_p$$ but this does not hold for $p = 1$. I also tried using elliptic estimates, also to no avail. Does anyone have any suggestions? EDIT: Perhaps it is possible to deduce this from the $L^p$ inequality $$\|D^2 f\|_{L^p} \leq C \|\Delta f\|_{L^p}$$ for $u\in W_0^{2,p}(\Omega)$, $1 < p < \infty$?",,"['real-analysis', 'inequality', 'partial-differential-equations']"
8,Proving Injectivity,Proving Injectivity,,"The problem is to show the function $f:\mathbb{R}^2\rightarrow\mathbb{R}^2$ given by $$f(x,y)=(\tfrac{1}{2}x^2+y^2+2y,\,x^2-2x+y^3)$$ is injective on the set $$M=\{(x,y)\in\mathbb{R}^2:|x-1|+|y+1|<\tfrac{1}{6}\}.$$ My idea is to consider the following map (here, $u,v\in\mathbb{R}^2$): $$\phi_v(u)=u-f(u)+v,\quad v\in M$$ If I manage to show that $\phi_v:D\rightarrow D$ is well-defined for some closed sets $\phi_v$ is a contraction (Lipschitz constant $<1$) on $D$ then by the Contraction Mapping Theorem, $\phi_v$ has a unique fixed point. Hence, $v$ has a unique preimage $u$ for each $v$. i.e. $f$ is injective as desired. But I ran into troubles when I attempted to find a suitable closed set $D$. Obviously it depends on the domain $M$. $M$ given here is really weird so I am not too sure how to proceed.","The problem is to show the function $f:\mathbb{R}^2\rightarrow\mathbb{R}^2$ given by $$f(x,y)=(\tfrac{1}{2}x^2+y^2+2y,\,x^2-2x+y^3)$$ is injective on the set $$M=\{(x,y)\in\mathbb{R}^2:|x-1|+|y+1|<\tfrac{1}{6}\}.$$ My idea is to consider the following map (here, $u,v\in\mathbb{R}^2$): $$\phi_v(u)=u-f(u)+v,\quad v\in M$$ If I manage to show that $\phi_v:D\rightarrow D$ is well-defined for some closed sets $\phi_v$ is a contraction (Lipschitz constant $<1$) on $D$ then by the Contraction Mapping Theorem, $\phi_v$ has a unique fixed point. Hence, $v$ has a unique preimage $u$ for each $v$. i.e. $f$ is injective as desired. But I ran into troubles when I attempted to find a suitable closed set $D$. Obviously it depends on the domain $M$. $M$ given here is really weird so I am not too sure how to proceed.",,"['real-analysis', 'functions']"
9,"Dominated convergence theorem for $\lim\int_{0}^{n}(1-x/n)^{n}x^sdx$,  $s<-1$.","Dominated convergence theorem for ,  .",\lim\int_{0}^{n}(1-x/n)^{n}x^sdx s<-1,"When I was studying the dominated convergence theorem, I was glad to come across this problem: Use the dominated convergence theorem (DCT) to show that $$\lim \int_{0}^{n} \left(1-\frac{x}{n} \right)^{n}x^sdx=\int_{0}^{\infty}e^{-x}x^sdx$$ where $s<-1$. I let $\displaystyle f_n(x)= \left(1-\frac{x}{n} \right)^{n}$, clearly $\lim f_n(x)=e^{-x}$. And $e^{-x}$ is integrable. The book I am using stated the dominated convergence theorem as follows: Theorem : Let $f_n$ be a sequence of measurable functions such that $f_n \to f$ almost everywhere. If there exist a real valued function $g$ define on a measure space such that for each $n$, $|f_n| \le g$ almost everywhere then $f$ is integrable and $$\int f=\lim\int f_n$$ To be able to use the DCT it necessary to find any real valued function $g$ that dominate $f_n$. That is where I get stuck, can somebody help me out? EDIT: The $\lim$ here means limit as n tend to infinity.","When I was studying the dominated convergence theorem, I was glad to come across this problem: Use the dominated convergence theorem (DCT) to show that $$\lim \int_{0}^{n} \left(1-\frac{x}{n} \right)^{n}x^sdx=\int_{0}^{\infty}e^{-x}x^sdx$$ where $s<-1$. I let $\displaystyle f_n(x)= \left(1-\frac{x}{n} \right)^{n}$, clearly $\lim f_n(x)=e^{-x}$. And $e^{-x}$ is integrable. The book I am using stated the dominated convergence theorem as follows: Theorem : Let $f_n$ be a sequence of measurable functions such that $f_n \to f$ almost everywhere. If there exist a real valued function $g$ define on a measure space such that for each $n$, $|f_n| \le g$ almost everywhere then $f$ is integrable and $$\int f=\lim\int f_n$$ To be able to use the DCT it necessary to find any real valued function $g$ that dominate $f_n$. That is where I get stuck, can somebody help me out? EDIT: The $\lim$ here means limit as n tend to infinity.",,"['real-analysis', 'measure-theory']"
10,Do the two limits coincide?,Do the two limits coincide?,,"Let $a$ be a non negative (positive almost everywhere) weight  in $L_{loc}^1(\Omega)$, $\Omega\subseteq\mathbb{R}^n$ is open. For $\varphi\in C_c^{\infty}(\Omega)$ define $$ \Vert\varphi\Vert_a^2=\int_{\Omega} a(x)|\nabla\varphi(x)|^2\,\mathrm{d}x. $$ Then $\Vert\cdot\Vert_a^2$ is a norm on $C_c^{\infty}(\Omega)$. Let $D_0^1(\Omega;a)$ denotes the closure of $C_c^{\infty}(\Omega)$ with respect to the $\Vert\cdot\Vert_a^2$-norm. Let $\{\varphi_n\}\in C_c^{\infty}(\Omega)$ and $\varphi_n\to \varphi$ in $D_0^1(\Omega;a)$  and also we have $\varphi_n\to \psi$ in some space $L^p(\Omega),\,1\leqslant p\leqslant\infty$. Can we say that $\varphi=\psi$? And why? A similar situation happens in the proof of Sobolev embedding theorem. Where we have a smooth sequence $\{\varphi_n\}$ tends to a $u\in W^{1,p}(\mathbb{R}^n)$ and then we show $\varphi_n$ tends to some $v\in L^{p^*}(\mathbb{R}^n)$ and then we say $u=v$ a.e. In this situation, my understanding is : since $\Vert\varphi_n-u\Vert_p\to 0$ and $\Vert\varphi_n-v\Vert_{p^*}\to 0$ there is a subsequence $\{\varphi_{n_{k_i}}\}$ converges almost everywhere to both $u$ and $v$, then $u=v$ a.e. But in the $D_0^1(\Omega;a)$ case, does there also a subsequence converges to $\varphi$ a.e.? Or some other approachs? Any advice will be appreciated! Edit Here is some of my thought: For $u_k\in L^p(\Omega), 1\leqslant p\leqslant\infty$, $u_k\to u$ in $L^p(\Omega)$, define  $$ \langle u,\varphi\rangle=\int_{\Omega}u\varphi $$  for all $\varphi\in C_c^{\infty}(\Omega)$. We see this integral makes sense since $\varphi$ has compact support and $u$ is locally integrable. And it is a continuous linear functional on $C_c^{\infty}(\Omega)$. Furthermore $$ |\langle u_k,\varphi\rangle-\langle u,\varphi\rangle|\leqslant \int_{\Omega}|u_k-u||\varphi|\to 0 $$ by Holder's inequality. Thus we have $u_k\to u$ in $\mathcal{D}'(\Omega)$. In Sobolev case we have $u_k$ tends to both $u$ and $v$ in $\mathcal{D}'(\Omega)$, so $u=v$ a.e. For $u_k\in D_0^1(\Omega;a)$, $u_k\to v$ in $D_0^1(\Omega;a)$, define  $$ \langle u,\varphi\rangle=\int_{\Omega}a^{1/2}(x)|\nabla u|\varphi. $$ for all $\varphi\in C_c^{\infty}(\Omega)$. Then the integral makes sense by applying Holder's inequality. And it is also a linear continuous functional on $\mathcal{D}'(\Omega)$. Furthermore $$ |\langle u_k,\varphi\rangle-\langle v,\varphi\rangle|\leqslant \int_{\Omega}a^{1/2}(x)|\nabla u_k-\nabla u||\varphi|\to 0 $$ by Holder's inequality. So we have $u_k \to v$ in $\mathcal{D}'(\Omega)$, which implies $u=v$ a.e. And this answers my question why $\psi=\varphi$ a.e. Is there something wrong in my ""proof""? Or some better approaches? Thanks! Edit $$ |\langle u_k,\varphi\rangle-\langle v,\varphi\rangle|\leqslant \int_{\Omega}a^{1/2}(x)|\nabla u_k-\nabla u||\varphi|\to 0 $$ seems only mean $a^{1/2}(x)|\nabla u|\to a^{1/2}(x)|\nabla v|$ in $\mathcal{D}'(\Omega)$, and this says nothing about my question I think! Could any one help me?","Let $a$ be a non negative (positive almost everywhere) weight  in $L_{loc}^1(\Omega)$, $\Omega\subseteq\mathbb{R}^n$ is open. For $\varphi\in C_c^{\infty}(\Omega)$ define $$ \Vert\varphi\Vert_a^2=\int_{\Omega} a(x)|\nabla\varphi(x)|^2\,\mathrm{d}x. $$ Then $\Vert\cdot\Vert_a^2$ is a norm on $C_c^{\infty}(\Omega)$. Let $D_0^1(\Omega;a)$ denotes the closure of $C_c^{\infty}(\Omega)$ with respect to the $\Vert\cdot\Vert_a^2$-norm. Let $\{\varphi_n\}\in C_c^{\infty}(\Omega)$ and $\varphi_n\to \varphi$ in $D_0^1(\Omega;a)$  and also we have $\varphi_n\to \psi$ in some space $L^p(\Omega),\,1\leqslant p\leqslant\infty$. Can we say that $\varphi=\psi$? And why? A similar situation happens in the proof of Sobolev embedding theorem. Where we have a smooth sequence $\{\varphi_n\}$ tends to a $u\in W^{1,p}(\mathbb{R}^n)$ and then we show $\varphi_n$ tends to some $v\in L^{p^*}(\mathbb{R}^n)$ and then we say $u=v$ a.e. In this situation, my understanding is : since $\Vert\varphi_n-u\Vert_p\to 0$ and $\Vert\varphi_n-v\Vert_{p^*}\to 0$ there is a subsequence $\{\varphi_{n_{k_i}}\}$ converges almost everywhere to both $u$ and $v$, then $u=v$ a.e. But in the $D_0^1(\Omega;a)$ case, does there also a subsequence converges to $\varphi$ a.e.? Or some other approachs? Any advice will be appreciated! Edit Here is some of my thought: For $u_k\in L^p(\Omega), 1\leqslant p\leqslant\infty$, $u_k\to u$ in $L^p(\Omega)$, define  $$ \langle u,\varphi\rangle=\int_{\Omega}u\varphi $$  for all $\varphi\in C_c^{\infty}(\Omega)$. We see this integral makes sense since $\varphi$ has compact support and $u$ is locally integrable. And it is a continuous linear functional on $C_c^{\infty}(\Omega)$. Furthermore $$ |\langle u_k,\varphi\rangle-\langle u,\varphi\rangle|\leqslant \int_{\Omega}|u_k-u||\varphi|\to 0 $$ by Holder's inequality. Thus we have $u_k\to u$ in $\mathcal{D}'(\Omega)$. In Sobolev case we have $u_k$ tends to both $u$ and $v$ in $\mathcal{D}'(\Omega)$, so $u=v$ a.e. For $u_k\in D_0^1(\Omega;a)$, $u_k\to v$ in $D_0^1(\Omega;a)$, define  $$ \langle u,\varphi\rangle=\int_{\Omega}a^{1/2}(x)|\nabla u|\varphi. $$ for all $\varphi\in C_c^{\infty}(\Omega)$. Then the integral makes sense by applying Holder's inequality. And it is also a linear continuous functional on $\mathcal{D}'(\Omega)$. Furthermore $$ |\langle u_k,\varphi\rangle-\langle v,\varphi\rangle|\leqslant \int_{\Omega}a^{1/2}(x)|\nabla u_k-\nabla u||\varphi|\to 0 $$ by Holder's inequality. So we have $u_k \to v$ in $\mathcal{D}'(\Omega)$, which implies $u=v$ a.e. And this answers my question why $\psi=\varphi$ a.e. Is there something wrong in my ""proof""? Or some better approaches? Thanks! Edit $$ |\langle u_k,\varphi\rangle-\langle v,\varphi\rangle|\leqslant \int_{\Omega}a^{1/2}(x)|\nabla u_k-\nabla u||\varphi|\to 0 $$ seems only mean $a^{1/2}(x)|\nabla u|\to a^{1/2}(x)|\nabla v|$ in $\mathcal{D}'(\Omega)$, and this says nothing about my question I think! Could any one help me?",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
11,Approximating measures by open sets and compact sets.,Approximating measures by open sets and compact sets.,,"I'm having trouble starting two similar proofs: Let $\epsilon > 0$. And let $E$ be a measurable set of finite measure. Prove that there is an open set $U$ containing $E$ such that $m(U \setminus E) < \epsilon$. Similarly, prove there is a compact set $K$ contained in $E$ such that $m(E \setminus K) < \epsilon$. Any hints are much appreciated. NOTE: $m$ is the Lebesgue outer measure. And $E \subseteq \mathbb{R}$ is measurable if $m(A) \geq m(A \cap E) + m(A \setminus E)$.","I'm having trouble starting two similar proofs: Let $\epsilon > 0$. And let $E$ be a measurable set of finite measure. Prove that there is an open set $U$ containing $E$ such that $m(U \setminus E) < \epsilon$. Similarly, prove there is a compact set $K$ contained in $E$ such that $m(E \setminus K) < \epsilon$. Any hints are much appreciated. NOTE: $m$ is the Lebesgue outer measure. And $E \subseteq \mathbb{R}$ is measurable if $m(A) \geq m(A \cap E) + m(A \setminus E)$.",,"['real-analysis', 'measure-theory']"
12,"If $f,g$ are simple measurable functions, show that $f+g$ and $f\,g$ are too","If  are simple measurable functions, show that  and  are too","f,g f+g f\,g","This homework problem involves showing that if $f,g$ are measurable simple functions, then so is $f+g$ and $f\,g$ - without using: 1) $\{x \in A: (f+g)(x) < t\} = \bigcup_{r\in\mathbb{Q}} \left[ \{x\in A: f(x) < r\} \cap \{x \in A: g(x) < t-r\} \right]$, for $(X,\mathcal{A})$, $X \supseteq A \in \mathcal{A}$, $f,g$ are $[0,{}^+\infty]$-valued measurable functions on $A$. 2) using $f-g$ as $f+(-1)\,g$, or $f^2$ proofs whereby one uses the set $\{x\in A: f^2(x) < t \} = \{ x\in A: -\sqrt{t} < f(x) < \sqrt{t} \}$ (I hope this is clear!) So I believe (think) that a simple function is: $f := \sum_{k=1}^n\,a_k\,\chi_{E_k}$, where $E_k = f^{-1}(\{a_k\}) \in \mathcal{A}$ (though I don't think it has to belong to the algebra). I am supposed to show these claims without using the two enumerations above in two different ways, one using simple functions, and the other using: 3) $f \vee g := \max(f(x),g(x)) \leftrightarrow \{x \in A: (f \vee g)(x) \leq t\} = \{x \in A: f(x) \leq t\} \cap \{x \in A: g(x) \leq t \}$ 4) $f \wedge g \cdots \{ \} = \{ \} \cup \{ \}$ 5) This theorem claims: $(X,\mathcal{A})$ - measurable space, $X \supseteq A \in \mathcal{A}$, $\{f_n\}$ be a sequence of $[{}^-\infty,{}^+\infty]$-valued measurable functions on $A$. Then (a) functions $\sup_n f_n$ and $\inf_n f_n$ are measurable, (b) $\limsup_n f_n$ and $\liminf_n f_n$ are measurable, and (c) $\lim_n f_n$ is measurable, with a domain where $\limsup_n f_n = \liminf_n f_n$ 6) The sets $A_{n,k} = \{x \in A: \frac{k-1}{2^n} \leq f(x) < \frac{k}{2^n} \}$, which is like a scaled (to $n$) `largest integer function' that is more and more refined with increasing $n$ These are utilized in the claim: $(X,\mathcal{A})$ - measurable space, $X \supseteq A \in \mathcal{A}$, and $f$ be a $[0,+\infty]$-valued measurable function on $A$. Then there is a sequence $\{f_n\}$ of $[0,+\infty]$-valued simple measurable functions on $A$ that satisfy (1) $f_1(x) \leq f_2(x) \leq \cdots$ and (2) $f(x) = \lim_n f_n(x)$, at each $x \in A$. I am pretty clear on 3,4,5, and 6; as well as how 1 and 2 work and why the question doesn't want them used (it would be too easy to quote a theorem). Unfortunately, I just don't know how to start - in either direction. For the simple way, I was thinking of something like $$ (f+g)(x) = \sum_{n=1}^k\,(a_k^f+a_k^g)\,\chi_{E_k} = \cdots, $$  $$ (f\,g)(x) = \sum_{n=1}^k\,(a_k^f\,a_k^g)\,\chi_{E_k} = \cdots  .$$ As for the way utilizing 3-6 I have no clue! This is question 5 out of Donald Cohn's book Measure Theory , Chapter 2 section 1 - page 57 - if interested. I usually ask math.stackexchange because the answers are not given and I eventually get to understand the question - that is what I am hoping for now too! Thanks much!","This homework problem involves showing that if $f,g$ are measurable simple functions, then so is $f+g$ and $f\,g$ - without using: 1) $\{x \in A: (f+g)(x) < t\} = \bigcup_{r\in\mathbb{Q}} \left[ \{x\in A: f(x) < r\} \cap \{x \in A: g(x) < t-r\} \right]$, for $(X,\mathcal{A})$, $X \supseteq A \in \mathcal{A}$, $f,g$ are $[0,{}^+\infty]$-valued measurable functions on $A$. 2) using $f-g$ as $f+(-1)\,g$, or $f^2$ proofs whereby one uses the set $\{x\in A: f^2(x) < t \} = \{ x\in A: -\sqrt{t} < f(x) < \sqrt{t} \}$ (I hope this is clear!) So I believe (think) that a simple function is: $f := \sum_{k=1}^n\,a_k\,\chi_{E_k}$, where $E_k = f^{-1}(\{a_k\}) \in \mathcal{A}$ (though I don't think it has to belong to the algebra). I am supposed to show these claims without using the two enumerations above in two different ways, one using simple functions, and the other using: 3) $f \vee g := \max(f(x),g(x)) \leftrightarrow \{x \in A: (f \vee g)(x) \leq t\} = \{x \in A: f(x) \leq t\} \cap \{x \in A: g(x) \leq t \}$ 4) $f \wedge g \cdots \{ \} = \{ \} \cup \{ \}$ 5) This theorem claims: $(X,\mathcal{A})$ - measurable space, $X \supseteq A \in \mathcal{A}$, $\{f_n\}$ be a sequence of $[{}^-\infty,{}^+\infty]$-valued measurable functions on $A$. Then (a) functions $\sup_n f_n$ and $\inf_n f_n$ are measurable, (b) $\limsup_n f_n$ and $\liminf_n f_n$ are measurable, and (c) $\lim_n f_n$ is measurable, with a domain where $\limsup_n f_n = \liminf_n f_n$ 6) The sets $A_{n,k} = \{x \in A: \frac{k-1}{2^n} \leq f(x) < \frac{k}{2^n} \}$, which is like a scaled (to $n$) `largest integer function' that is more and more refined with increasing $n$ These are utilized in the claim: $(X,\mathcal{A})$ - measurable space, $X \supseteq A \in \mathcal{A}$, and $f$ be a $[0,+\infty]$-valued measurable function on $A$. Then there is a sequence $\{f_n\}$ of $[0,+\infty]$-valued simple measurable functions on $A$ that satisfy (1) $f_1(x) \leq f_2(x) \leq \cdots$ and (2) $f(x) = \lim_n f_n(x)$, at each $x \in A$. I am pretty clear on 3,4,5, and 6; as well as how 1 and 2 work and why the question doesn't want them used (it would be too easy to quote a theorem). Unfortunately, I just don't know how to start - in either direction. For the simple way, I was thinking of something like $$ (f+g)(x) = \sum_{n=1}^k\,(a_k^f+a_k^g)\,\chi_{E_k} = \cdots, $$  $$ (f\,g)(x) = \sum_{n=1}^k\,(a_k^f\,a_k^g)\,\chi_{E_k} = \cdots  .$$ As for the way utilizing 3-6 I have no clue! This is question 5 out of Donald Cohn's book Measure Theory , Chapter 2 section 1 - page 57 - if interested. I usually ask math.stackexchange because the answers are not given and I eventually get to understand the question - that is what I am hoping for now too! Thanks much!",,['real-analysis']
13,Convergence of Integral Implies Uniform convergence of Equicontinuous Family,Convergence of Integral Implies Uniform convergence of Equicontinuous Family,,"Let $\{f_n\}$ be an equicontinuous family of functions on $[0,1]$ such that each $f_n$ is pointwise bounded and $\int_{[a,b]} f_n(x)dx \rightarrow 0$ as $n\rightarrow \infty$, for every $  0\leq a \leq b \leq 1$. Show $f_n$ converges to $0$ uniformly. For this question I know that there exists a uniformly convergent subsequence $f_{n_k}$ by Arzela-Ascoli Theorem. For this uniformly convergent sequence I know $$\lim_{n\rightarrow \infty} \int_a^bf_{n_k}(x)dx = \int_a^b \lim_{n\rightarrow \infty}f_{n_k}(x)dx$$ Since the left side is zero If we assume $\lim_{n\rightarrow \infty}f_{n_k}(x)dx \neq \ 0$ for some $x\in [0,1]$ Then uniform continuity of the limit implies that it is $\neq 0$ on some interval which implies the integral is $\neq 0$ Which is a contradiction . Thus $f_{n_k}$ must converge uniformly to $0$. I don't see how to get to $f_n$ converges uniformly to $0$ though.","Let $\{f_n\}$ be an equicontinuous family of functions on $[0,1]$ such that each $f_n$ is pointwise bounded and $\int_{[a,b]} f_n(x)dx \rightarrow 0$ as $n\rightarrow \infty$, for every $  0\leq a \leq b \leq 1$. Show $f_n$ converges to $0$ uniformly. For this question I know that there exists a uniformly convergent subsequence $f_{n_k}$ by Arzela-Ascoli Theorem. For this uniformly convergent sequence I know $$\lim_{n\rightarrow \infty} \int_a^bf_{n_k}(x)dx = \int_a^b \lim_{n\rightarrow \infty}f_{n_k}(x)dx$$ Since the left side is zero If we assume $\lim_{n\rightarrow \infty}f_{n_k}(x)dx \neq \ 0$ for some $x\in [0,1]$ Then uniform continuity of the limit implies that it is $\neq 0$ on some interval which implies the integral is $\neq 0$ Which is a contradiction . Thus $f_{n_k}$ must converge uniformly to $0$. I don't see how to get to $f_n$ converges uniformly to $0$ though.",,"['real-analysis', 'analysis', 'integration', 'convergence-divergence']"
14,proving derivative in real analysis,proving derivative in real analysis,,"I have proved the following problem, can you help me check if there is any loopholes in my proof? Let I be an open interval in R, let $c \in I$, and let $f, g\colon I\to \mathbb{R}$ be functions. Suppose that $f(c) = g(c)$, and that $f(x) \leq g(x)$ for all $x \in I$. Prove that if f and g are differentiable at c, then f'(c) = g'(c). So the following is my proof: Let $x \in I$, if $x \lt c$, then $\lim\limits_{x\rightarrow c-}\frac{f(x)          - f(c)}{x-c}$ = $\lim\limits_{x\rightarrow c-}\frac{f(c)          - f(x)}{c-x} \geq \lim\limits_{x\rightarrow c-}\frac{g(c)          - g(x)}{c-x} = \lim\limits_{x\rightarrow c-}\frac{g(x)          - g(c)}{x-c}$ if $x \gt c$, then $\lim\limits_{x\rightarrow c+}\frac{f(x)          - f(c)}{x-c} \leq \lim\limits_{x\rightarrow c+}\frac{g(x)          - g(c)}{x-c}$ Since $f$ and $g$ are differentiable at $c$, we know that $\lim\limits_{x\rightarrow c-}\frac{f(x)          - f(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{f(x)          - f(c)}{x-c}=f'(c)$ and $\lim\limits_{x\rightarrow c-}\frac{g(x)          - g(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{g(x)          - g(c)}{x-c} = g'(c)$ Thus $\lim\limits_{x\rightarrow c-}\frac{f(x)          - f(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{f(x)          - f(c)}{x-c} = \lim\limits_{x\rightarrow c-}\frac{g(x)          - g(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{g(x)          - g(c)}{x-c}$ Therefore $f'(c) = g'(c)$. I can't find any problem in my proof, but for some reason I'm not feeling comfortable with it. However, if it's totally correct, just tell me :) Thanks!!","I have proved the following problem, can you help me check if there is any loopholes in my proof? Let I be an open interval in R, let $c \in I$, and let $f, g\colon I\to \mathbb{R}$ be functions. Suppose that $f(c) = g(c)$, and that $f(x) \leq g(x)$ for all $x \in I$. Prove that if f and g are differentiable at c, then f'(c) = g'(c). So the following is my proof: Let $x \in I$, if $x \lt c$, then $\lim\limits_{x\rightarrow c-}\frac{f(x)          - f(c)}{x-c}$ = $\lim\limits_{x\rightarrow c-}\frac{f(c)          - f(x)}{c-x} \geq \lim\limits_{x\rightarrow c-}\frac{g(c)          - g(x)}{c-x} = \lim\limits_{x\rightarrow c-}\frac{g(x)          - g(c)}{x-c}$ if $x \gt c$, then $\lim\limits_{x\rightarrow c+}\frac{f(x)          - f(c)}{x-c} \leq \lim\limits_{x\rightarrow c+}\frac{g(x)          - g(c)}{x-c}$ Since $f$ and $g$ are differentiable at $c$, we know that $\lim\limits_{x\rightarrow c-}\frac{f(x)          - f(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{f(x)          - f(c)}{x-c}=f'(c)$ and $\lim\limits_{x\rightarrow c-}\frac{g(x)          - g(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{g(x)          - g(c)}{x-c} = g'(c)$ Thus $\lim\limits_{x\rightarrow c-}\frac{f(x)          - f(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{f(x)          - f(c)}{x-c} = \lim\limits_{x\rightarrow c-}\frac{g(x)          - g(c)}{x-c} = \lim\limits_{x\rightarrow c+}\frac{g(x)          - g(c)}{x-c}$ Therefore $f'(c) = g'(c)$. I can't find any problem in my proof, but for some reason I'm not feeling comfortable with it. However, if it's totally correct, just tell me :) Thanks!!",,['real-analysis']
15,Help understanding game version of Baire category theorem,Help understanding game version of Baire category theorem,,"I got this from Thomson et al.'s freely available "" Elementary Real Analysis "" p.356. They introduce Baire's category theorem through a game where, given two players (A) and (B) Player (A) is given a subset $A$ of $\mathbb{R}$, and player (B) is given the complementary set $B = \mathbb{R} \backslash A$. Player (A) first selects a closed interval $I_1 \subset \mathbb{R}$; then player (B) chooses a closed interval $I_2 \subset I_1$. The players alternate moves, a move consisting of selecting a closed interval inside the previously chosen interval. The play of the game thus determines a descending sequence of closed intervals   \begin{align}    I_1 \supset I_2 \supset \ldots I_n \supset    \ldots\end{align}   where player (A) chooses those with odd index and player (B) those with even index. If   \begin{align}   A \: \bigcap_n^{\infty} I_n \neq \emptyset   \end{align}   then player (A) wins; otherwise player (B) wins. Then they argue that if player (A) is dealt the irrational set and (B) is dealt the rational set, (A) always has a strategy to win. I'm confused in several ways by this argument. One confusion is about the term ""closed interval"". Does, for example, the closed interval [1,1] count as an interval? Because if that's the case, can't whoever has the first turn just end the game then and there without regard to whether he has the rationals or irrationals? Say, if (A) received the rationals, he can just pick [0.5,0.5]. Game over. (But I'm guessing probably not, because the game wouldn't happen) If that is not the case, and an interval has to be defined $[a,b]$ s.t. $a < b$, isn't it always true that for any $I_{2n+1} = [a_{2n+1}, b_{2n+1}]$ for the odd numbered turns of (A), and where (A) has the irrationals, there exists some $q \in [a_{2n+1},b_{2n+1}]$ s.t. $q$ is rational? Because $\mathbb{Q}$ is also dense on the real line. Or is this argument relying on the fact that a countable intersection of closed sets is always closed? And that it can converge to a single point. But if it converges to a single point, it might be a closed set, but is it still a closed interval? And does that still count as winning? And can't (A) still play this same game even if he were dealt the rationals if all that is needed is that his choice of intervals converge to a single point? I've been thinking about this for a few days now and none of the ways I've approached it convince me that their discussion of the game is true (though I do trust that it is true, since the authors are mathematicians and I'm not), so any help would be appreciated.","I got this from Thomson et al.'s freely available "" Elementary Real Analysis "" p.356. They introduce Baire's category theorem through a game where, given two players (A) and (B) Player (A) is given a subset $A$ of $\mathbb{R}$, and player (B) is given the complementary set $B = \mathbb{R} \backslash A$. Player (A) first selects a closed interval $I_1 \subset \mathbb{R}$; then player (B) chooses a closed interval $I_2 \subset I_1$. The players alternate moves, a move consisting of selecting a closed interval inside the previously chosen interval. The play of the game thus determines a descending sequence of closed intervals   \begin{align}    I_1 \supset I_2 \supset \ldots I_n \supset    \ldots\end{align}   where player (A) chooses those with odd index and player (B) those with even index. If   \begin{align}   A \: \bigcap_n^{\infty} I_n \neq \emptyset   \end{align}   then player (A) wins; otherwise player (B) wins. Then they argue that if player (A) is dealt the irrational set and (B) is dealt the rational set, (A) always has a strategy to win. I'm confused in several ways by this argument. One confusion is about the term ""closed interval"". Does, for example, the closed interval [1,1] count as an interval? Because if that's the case, can't whoever has the first turn just end the game then and there without regard to whether he has the rationals or irrationals? Say, if (A) received the rationals, he can just pick [0.5,0.5]. Game over. (But I'm guessing probably not, because the game wouldn't happen) If that is not the case, and an interval has to be defined $[a,b]$ s.t. $a < b$, isn't it always true that for any $I_{2n+1} = [a_{2n+1}, b_{2n+1}]$ for the odd numbered turns of (A), and where (A) has the irrationals, there exists some $q \in [a_{2n+1},b_{2n+1}]$ s.t. $q$ is rational? Because $\mathbb{Q}$ is also dense on the real line. Or is this argument relying on the fact that a countable intersection of closed sets is always closed? And that it can converge to a single point. But if it converges to a single point, it might be a closed set, but is it still a closed interval? And does that still count as winning? And can't (A) still play this same game even if he were dealt the rationals if all that is needed is that his choice of intervals converge to a single point? I've been thinking about this for a few days now and none of the ways I've approached it convince me that their discussion of the game is true (though I do trust that it is true, since the authors are mathematicians and I'm not), so any help would be appreciated.",,"['real-analysis', 'baire-category']"
16,Why are signed and complex measures typically not allowed to assume infinite values?,Why are signed and complex measures typically not allowed to assume infinite values?,,"In a number of real analysis texts (I am thinking of Folland in particular), three different kinds of measures are defined. Positive measures: Take values in $[0, +\infty]$ Signed measures: Take values in either $(-\infty, \infty]$ or $[-\infty, \infty)$, but cannot assume both $+\infty$ and $-\infty$. Complex measures: Take values in $\mathbb{C}$.  Any kind of infinity is not allowed. My question is: why is this?  Is this because of how we set up integrals with respect to these measures, or does it have to do with adding and subtracting measures to make new ones?","In a number of real analysis texts (I am thinking of Folland in particular), three different kinds of measures are defined. Positive measures: Take values in $[0, +\infty]$ Signed measures: Take values in either $(-\infty, \infty]$ or $[-\infty, \infty)$, but cannot assume both $+\infty$ and $-\infty$. Complex measures: Take values in $\mathbb{C}$.  Any kind of infinity is not allowed. My question is: why is this?  Is this because of how we set up integrals with respect to these measures, or does it have to do with adding and subtracting measures to make new ones?",,"['real-analysis', 'measure-theory']"
17,When is it possible to transform limits using substitution?,When is it possible to transform limits using substitution?,,"In calculus, if I have to calculate $\lim_{x \to x_0}f(x)$ , we can usually do substitutions of the from $x=\phi(y)$ and then our limit would be $\lim_{y \to \phi(x_0)}f(\phi(y))$ . However, we cannot just choose some arbitrary $\phi$ . As a counterexample, take $\lim_{x \to 2}x$ . You cannot take $x=\phi(y)=0$ , so my intuition says that $\phi$ needs to be bijective in a small enough neighbourhood of $x_0$ . Also, I would assume that $\phi$ needs to be continuous at $x_0$ . The substitution $\phi(x)=\text{floor}(x)$ would also fail, firstly because of non-injectivity, but also because of non-continuity. Do we need any more conditions on $\phi$ ? For context, the reason why I am asking this is because in my university coursed, our profesor said that if we want to calculate $\lim_{(x, y) \to (0, 0)}f(x, y)$ , we can just change to polar coordinates and calculate and it would be easier. I am currently studying engineering, so I would be looking for some theoretical, pure maths proof on the restrictions of the substitution $(x, y)=\phi(u, v)$ . To rephrase my question. Given two topological spaces $(T, \tau_T)$ , $(S, \tau_S)$ and $(U, \tau_U)$ , $D \subseteq T$ , a function $f\colon D \to S$ and $x_0$ an accumulation point of $D$ . Now let $V$ be a neighbourhood of $x_0$ and $\phi\colon U \to D \cap V$ . What would be the minimum conditions that $\phi$ needs to obey such that $\lim_{x \to x_0}f(x)=\lim_{y \to \phi(x_0)}f(\phi(y))$ ? Is the bijectivity and continuity of $\phi$ equivalent to the equality of limits in the general case? My intuition says yes, however from working a lot in abstract algebra these types of substitutions required $\phi$ to be an isomorphism, in our case shouldn't $\phi$ be a homeomorphism? I can't think of a counterexample where if $\phi$ is continuous and bijective would fail if $\phi^-1$ is not continuous? And one more thing, the polar coordinates substitution ""fails"", if we work on the direct product of the extended real number line, at points that have infinity on one component, since I don't think polar coordinates are defined just like how the angle isn't defined for the origin.","In calculus, if I have to calculate , we can usually do substitutions of the from and then our limit would be . However, we cannot just choose some arbitrary . As a counterexample, take . You cannot take , so my intuition says that needs to be bijective in a small enough neighbourhood of . Also, I would assume that needs to be continuous at . The substitution would also fail, firstly because of non-injectivity, but also because of non-continuity. Do we need any more conditions on ? For context, the reason why I am asking this is because in my university coursed, our profesor said that if we want to calculate , we can just change to polar coordinates and calculate and it would be easier. I am currently studying engineering, so I would be looking for some theoretical, pure maths proof on the restrictions of the substitution . To rephrase my question. Given two topological spaces , and , , a function and an accumulation point of . Now let be a neighbourhood of and . What would be the minimum conditions that needs to obey such that ? Is the bijectivity and continuity of equivalent to the equality of limits in the general case? My intuition says yes, however from working a lot in abstract algebra these types of substitutions required to be an isomorphism, in our case shouldn't be a homeomorphism? I can't think of a counterexample where if is continuous and bijective would fail if is not continuous? And one more thing, the polar coordinates substitution ""fails"", if we work on the direct product of the extended real number line, at points that have infinity on one component, since I don't think polar coordinates are defined just like how the angle isn't defined for the origin.","\lim_{x \to x_0}f(x) x=\phi(y) \lim_{y \to \phi(x_0)}f(\phi(y)) \phi \lim_{x \to 2}x x=\phi(y)=0 \phi x_0 \phi x_0 \phi(x)=\text{floor}(x) \phi \lim_{(x, y) \to (0, 0)}f(x, y) (x, y)=\phi(u, v) (T, \tau_T) (S, \tau_S) (U, \tau_U) D \subseteq T f\colon D \to S x_0 D V x_0 \phi\colon U \to D \cap V \phi \lim_{x \to x_0}f(x)=\lim_{y \to \phi(x_0)}f(\phi(y)) \phi \phi \phi \phi \phi^-1","['real-analysis', 'calculus', 'general-topology', 'limits', 'multivariable-calculus']"
18,Why inner regularity of measure is defined as to be approximable from within by compact sets and not by closed sets?,Why inner regularity of measure is defined as to be approximable from within by compact sets and not by closed sets?,,"This question has been asked here , but I don't believe that we have had a satisfactory answer, so I would like to reformulate that question. Please forgive me if this question turns out to be rather silly. Let $(X,\mathcal{T})$ be a topological space, $(X,\Sigma,\mu)$ be a measurable space such that $\Sigma$ contains the borel algebra. Usually, $\mu$ is called outer regular if $$ \mu(A)=\inf\{\mu(G):G\supset A,G\text{ open}\},\quad\forall A\text{ measurable}. $$ Personally, I would be very tempted to defind a measure to be inner regular if $$ \mu(A)=\sup\{\mu(F):F\subset A,F\text{ closed}\},\quad\forall A\text{ measurable}.\tag{1} $$ But it turns out that the definition in reality is $$ \mu(A)=\sup\{\mu(F):F\subset A,F\text{ compact}\},\quad\forall A\text{ measurable}.\tag{2} $$ And I'm wondering why. Since that a measure satisfying $(2)$ is called inner regular, let's call a measure satisfying $(1)$ one that is approximable by closed sets . The two notions can be very different. For a $KC$ space (where every compact set is closed), every inner regular measure is approximable by closed sets, but not vice versa: consider the lower limit topology and the Lebesgue measure $\mu$ over $\mathbb{R}$ . Since this topology contains more closed sets than the usual one, $\mu$ is clearly approximable by closed sets, but $\mu$ is not inner regular since every compact set in the lower limit topology is countable. If we remove the $KC$ assumption, then an inner regular measure is not necessarily approximable by closed sets: consider the trivial topology where every set is compact. If $\mu$ is approximable by closed sets, then every measurable proper subset of $X$ has measure $0$ , so either $\Sigma=\{\emptyset,X\}$ , either $\mu(X)=0$ . So I would like to ask: why do we choose to break the symmetry here in the definition of outer and inner regularity? In other words, why is using closed sets less desirable, while using open sets is acceptable? Or are we generally working in the setting where the two definitions become equivalent?","This question has been asked here , but I don't believe that we have had a satisfactory answer, so I would like to reformulate that question. Please forgive me if this question turns out to be rather silly. Let be a topological space, be a measurable space such that contains the borel algebra. Usually, is called outer regular if Personally, I would be very tempted to defind a measure to be inner regular if But it turns out that the definition in reality is And I'm wondering why. Since that a measure satisfying is called inner regular, let's call a measure satisfying one that is approximable by closed sets . The two notions can be very different. For a space (where every compact set is closed), every inner regular measure is approximable by closed sets, but not vice versa: consider the lower limit topology and the Lebesgue measure over . Since this topology contains more closed sets than the usual one, is clearly approximable by closed sets, but is not inner regular since every compact set in the lower limit topology is countable. If we remove the assumption, then an inner regular measure is not necessarily approximable by closed sets: consider the trivial topology where every set is compact. If is approximable by closed sets, then every measurable proper subset of has measure , so either , either . So I would like to ask: why do we choose to break the symmetry here in the definition of outer and inner regularity? In other words, why is using closed sets less desirable, while using open sets is acceptable? Or are we generally working in the setting where the two definitions become equivalent?","(X,\mathcal{T}) (X,\Sigma,\mu) \Sigma \mu 
\mu(A)=\inf\{\mu(G):G\supset A,G\text{ open}\},\quad\forall A\text{ measurable}.
 
\mu(A)=\sup\{\mu(F):F\subset A,F\text{ closed}\},\quad\forall A\text{ measurable}.\tag{1}
 
\mu(A)=\sup\{\mu(F):F\subset A,F\text{ compact}\},\quad\forall A\text{ measurable}.\tag{2}
 (2) (1) KC \mu \mathbb{R} \mu \mu KC \mu X 0 \Sigma=\{\emptyset,X\} \mu(X)=0","['real-analysis', 'measure-theory', 'borel-measures']"
19,Double Limits When Defining Improper Integral,Double Limits When Defining Improper Integral,,"In my textbook , an improper integral is defined as shown below, where $F$ is a primitive function of a function $f$ . $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{b\to+\infty}F(b)-\lim_{a\to-\infty}F(a) \tag{1}$$ Could I rewrite it in one of the following ways? $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{a\to+\infty}\int_{-a}^af(x)\mathrm{\ d}x \tag{2}$$ $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{a\to-\infty}\lim_{b\to+\infty}\int_a^bf(x)\mathrm{\ d}x \tag{3}$$ $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{b\to+\infty}\lim_{a\to-\infty}\int_a^bf(x)\mathrm{\ d}x \tag{4}$$ $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\underset{b\to+\infty}{\lim_{a\to-\infty}}\int_a^bf(x)\mathrm{\ d}x \tag{5}$$ $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{(a,b)\to(-\infty,+\infty)}\int_a^bf(x)\mathrm{\ d}x \tag{6}$$ What would be the differences between each one of these notations (talking about the limits)? Actually, are any of them even valid?","In my textbook , an improper integral is defined as shown below, where is a primitive function of a function . Could I rewrite it in one of the following ways? What would be the differences between each one of these notations (talking about the limits)? Actually, are any of them even valid?","F f \int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{b\to+\infty}F(b)-\lim_{a\to-\infty}F(a) \tag{1} \int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{a\to+\infty}\int_{-a}^af(x)\mathrm{\ d}x \tag{2} \int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{a\to-\infty}\lim_{b\to+\infty}\int_a^bf(x)\mathrm{\ d}x \tag{3} \int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{b\to+\infty}\lim_{a\to-\infty}\int_a^bf(x)\mathrm{\ d}x \tag{4} \int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\underset{b\to+\infty}{\lim_{a\to-\infty}}\int_a^bf(x)\mathrm{\ d}x \tag{5} \int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{(a,b)\to(-\infty,+\infty)}\int_a^bf(x)\mathrm{\ d}x \tag{6}","['real-analysis', 'limits', 'measure-theory', 'notation', 'improper-integrals']"
20,Prove / Disprove / Complete the proof That $f$ is Infinitely Differentiable,Prove / Disprove / Complete the proof That  is Infinitely Differentiable,f,"THE NEW (MUCH SHORTER) POST My original post perhaps was too long for most people, and understandably not a lot of people tried to go over it. In this new post my question is much shorter please prove or disprove that following function is infinitely differentiable on $\mathbb{R}$ , or alternatively answer my original post (it is below here), which basically means helping me completing my proposed proof. Even if you don't want to answer my original post because it 's too long, it might be worth at least looking at some of it, because it could be useful. Let $E$ be some closed set of real numbers. Then, we define $f$ by 3 cases: $\quad$ (I) If $x \in E$ , $f(x) = 0$ . $\quad$ (II) If $x \in (a,b)$ , where $a,b \in E$ and $(a,b) \subseteq \mathbb{R} \setminus E$ , define $$F_{a,b}(x) = \frac{\pi}{b-a}\left(x + \frac{3b-5a}{2}\right),$$ $\quad$ and then $$f(x) = X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right),$$ $\quad$ where $X_{a,b}$ is some constant real based on $a, b$ , which you need to define appropriately (probably different for every $(a,b)$ ), to prove that $f$ is infinitely differentiable, or if you want to prove that $f$ is not infinitely differentiable, you need to show that this is true for every choice for $X_{a,b}$ . $\quad$ (III) If $E$ is bounded above, $M=\sup E$ , then $f(x)=e^{-\frac{1}{(x-M)^2}}$ for $x > M$ . Similarly, $f(x)=e^{-\frac{1}{(x-N)^2}}$ if $E$ is bounded below and $\quad N=\inf E$ . MY ORIGINAL POST: Baby Rudin Ex. 5.21 - Assessment of a Proposed Partial Solution and Help Completing It This exercise is driving me crazy. I'm honestly ashamed to tell you how long I actually stack with this problem not continuing to study mathematics until I solve it. In many points a long the way I was very close to giving up and in many points I thought that I finally managed to solve the problem but then realized there is some mistake or a missing piece in the proof. The proposed solution here is the closest I managed to get at this point. At this point I decided it's time to ask for help on the last part missing from this proof and get some people to critic this partial proof in general. The Problem: Ex. 21, Chap. 5 Let $E$ be a closed subset of $\mathbb{R}$ . We saw in Exercise 22, Chap. 4, that there is a real continuous function $f$ on $\mathbb{R}$ whose zero set is $E$ . Is it possible, for each closed set $E$ , to find such an $f$ which is differentiable on $\mathbb{R}$ , or one which is $n$ times differentiable, or even one which has derivatives of all orders on $\mathbb{R}$ ? My Idea (Informally) I think such an infinitely differentable function does exist. I want to give you a visual explanation of how I thought of constructing this function before going to the formal solution. We obviously set $f$ to be 0 for all points of $E$ . For every open interval of points of $\mathbb{R} \setminus E$ , where the end points are points of $E$ , $f$ is a ""wave"" that smoothly approches 0 in the end points so that the derivative of any order on those end points is always 0. The shorter this open interval is, the smaller the maximum point of this ""wave"". If $E$ is bounded above, $f$ is just some increasing function after $\sup E$ , so that it smoothly approches 0 at $\sup E$ (the same goes if $E$ is bounded below). I drew an illustration of this idea: The red parts are points of $f(E)$ , the blue parts are points of $f(\mathbb{R} \setminus E)$ . $P$ is a limit point of $E$ . My Partial Solution My proof uses two previous results from the book and two additional lemmas. I shall present them here (and prove the lemmas) before going to the main proof. Ex. 5.9. Let $f$ be a continuous real function on $\mathbb{R}$ , of which it is known that $f'(x)$ exists for all $x \neq 0$ and that $f'(x) \to 3$ as $x \to 0$ . Does it follow that $f'(0)$ exists? In the solution to this exercise we saw that $f'(0)$ does in fact exists and is 3. In general to any real continuous function $f$ on $\mathbb{R}$ for which the derivative is known to exist except for a specific point $a$ and $f'(x) \to D$ as $x \to a$ , we can use the same proof we use in this exercise to show that $f'(a)$ exists and $f'(a)=D$ . Theorem 4.15. If $f$ is a continuous mapping of a compact metric space $X$ into $\mathbb{R}^k$ , then $f(X)$ is closed and bounded. Thus, $f$ is bounded. Lemma 1. Let $f, g: \mathbb{R} \to \mathbb{R}$ differentiable functions. Then, $$\left(f(x)^{g(x)}\right)' = f(x)^{g(x)}\left[g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}\right]$$ Proof. Let $h(x)=f(x)^{g(x)}$ , then $$\ln(h(x)) = \ln\left(f(x)^{g(x)}\right) = g(x)\ln(f(x)).$$ Differentiating both sides we get: $$\frac{h'(x)}{h(x)} = g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}.$$ Thus, $$h'(x) = f(x)^{g(x)}\left[g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}\right].$$ Lemma 2. Let $f,g : \mathbb{R} \to \mathbb{R}$ infinitely differentiable functions. Then, for every $n \in \mathbb{N}$ : $\quad$ (a) $f+g$ is infinitely differentiable and $$(f+g)^{(n)}(x)=f^{(n)}(x)+g^{(n)}(x)$$ . $\quad$ (b) $f\cdot g$ is infinitely differentiable and $$(f\cdot g)^{(n)}(x)=\sum_{i=1}^{2^n} f^{\left(\alpha_{n_i}\right)}(x)\cdot g^{\left(\beta_{n_i}\right)}(x),$$ $\quad$ where $\{\alpha_{n_i}\}$ , $\{\beta_{n_i}\}$ are sequences of non-negative integers. $\quad$ (c) $\frac{f}{g}$ is infinitely differentiable whenever $g(x) \neq 0$ and $$\left(\frac{f}{g}\right)^{(n)}(x)=\frac{F_n(x)}{g^{2^n}(x)},$$ $\quad$ where $F_n$ is some infinitely differentiable function. $\quad$ (d) $f\circ g$ is infinitely differentiable and $$(f\circ g)^{(n)}(x)=\sum_{j=1}^{z_n}\prod_{i=1}^{m_{n_j}}t_{n_{j_i}},$$ $\quad$ for some $z_n \in \mathbb{N}$ , $m_{n_1} and \dots, m_{n_{z_n}} \in \mathbb{N}$ and where each term $t_{n_{j_i}}$ is either of the form $f^{(m)}(g(x))$ or $g^{(m)}(x)$ , $m$ a non-negative $\quad$ integer. Proof. We use induction for all 4 of the sections of this lemma. $\quad$ (a) For $n=1$ , $(f+g)'(x) = f'(x) + g'(x)$ . Suppose that $(f+g)^{(k)}(x)=f^{(k)}(x)+g^{(k)}(x)$ for some $k \in \mathbb{N}$ . Then, $$(f+g)^{(k+1)}(x)=\left(f^{(k)}(x)\right)' + \left(g^{(k)}(x)\right)' = f^{(k+1)}(x) + g^{(k+1)}(x).$$ $\quad$ (b) For $n=1$ , $(f \cdot g)'(x) = f'(x)g(x) + f(x)g'(x)$ . Suppose that for some $k \in \mathbb{N}$ , there are sequences $\{\alpha_{k_i}\}$ , $\{\beta_{k_i}\}$ of non-negative integers such that $(f\cdot g)^{(k)}(x)=\sum_{i=1}^{2^k} f^{\left(\alpha_{k_i}\right)}(x)\cdot g^{\left(\beta_{k_i}\right)}(x)$ . Then, $$(f\cdot g)^{(k+1)}(x)=\sum_{i=1}^{2^k} f^{\left(\alpha_{k_i}+1\right)}(x)\cdot g^{\left(\beta_{k_i}\right)}(x) + f^{\left(\alpha_{k_i}\right)}(x)\cdot g^{\left(\beta_{k_i}+1\right)}(x).$$ Now, let $$\alpha_{\left(k+1\right)_i} = \begin{cases} \alpha_{k_i+1}, \quad 1 \leq i \leq 2^k\\ \alpha_{k_{\left(i-2^k\right)}}, \quad 2^k + 1 \leq i \leq 2^{k+1}, \end{cases}$$ $$\beta_{\left(k+1\right)_i} = \begin{cases} \beta_{k_i}, \quad 1 \leq i \leq 2^k\\ \beta_{k_{\left(i-2^k\right)}+1}, \quad 2^k + 1 \leq i \leq 2^{k+1}. \end{cases}$$ Thus, $$(f\cdot g)^{(k+1)}(x) = \sum_{i=1}^{2^{k+1}}f^{\left(\alpha_{(k+1)_i}\right)}(x)\cdot g^{\left(\beta_{(k+1)_i}\right)}(x).$$ $\quad$ (c) For $n=1$ , $\left(\frac{f}{g}\right)'(x) = \frac{f'(x)g(x)+f(x)g'(x)}{g^2(x)}$ . Suppose that for some $k \in \mathbb{N}$ , $\left(\frac{f}{g}\right)^{(k)}(x) = \frac{F_k(x)}{g^{2^k}(x)}$ , where $F_k$ is some infinitely differentiable function. Then, if we put $F_{k+1}(x) = F_k(x)g^{2^k}(x)-2^kg^{2^k-1}(x)g'(x)F_k(x)$ , we get the desired result that $$\left(\frac{f}{g}\right)^{(k+1)}(x) = \frac{F_{k+1}(x)}{g^{2^{k+1}}(x)}.$$ $\quad$ (d) For $n=1$ , $(f \circ g)'(x) = f'(g(x))g'(x)$ . Suppose that for some $k \in \mathbb{N}$ , $(f\circ g)^{(k)}(x)=\sum_{j=1}^{z_k}\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}$ for some $z_k \in \mathbb{N}$ , $m_{k_1} and \dots, m_{k_{z_k}} \in \mathbb{N}$ and where each term $t_{k_{j_i}}$ is either of the form $f^{(m)}(g(x))$ or $g^{(m)}(x)$ , $m$ a non-negative integer. Then, $(f\circ g)^{(k+1)}(x)=\sum_{j=1}^{z_k}\left(\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right)'$ . We then prove by induction $$\bullet \quad \left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = \sum_{i=1}^{m}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m}t_{k_{j_i}}\left(\prod_{i=m+1}^{m_{k_j}}t_{k_{j_i}}\right)',$$ for every $m,j \in \mathbb{N}$ such that $j \leq z_k$ and $m < m_{k_j}$ . For $m=1$ , $\left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = t_{k_{j_1}}'\prod_{i=2}^{m_{k_j}}t_{k_{j_i}} + t_{k_{j_1}}\left(\prod_{i=2}^{m_{k_j}}t_{k_{j_i}}\right)'$ . Suppose that $\bullet$ is true for some natural $m < m_{k_j}-1$ . Then, $$\begin{align*} \left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)'  & = \sum_{i=1}^{m}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m}t_{k_{j_i}}\left[t_{k_{j_{m+1}}}'\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right) + t_{k_{j_{m+1}}}\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right)'\right]\\ & = \sum_{i=1}^{m+1}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m+1}t_{k_{j_i}}\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right)'. \end{align*}$$ Then, if we put $m=m_{k_j}-1$ we get that $$\left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = \sum_{i=1}^{m_{k_j}}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right).$$ Since $t_{k_{j_i}}'$ is either of the form $f^{(u)}(g(x))g'(x)$ or $g^{(u)}(x)$ , $u \in \mathbb{N}$ , $$ (f \circ g)^{(k+1)}(x) = \sum_{j=1}^{z_k}\sum_{i=1}^{m_{k_j}}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) $$ can clearly be arranged in our desired form. The main proof. Yes, such a function which is infinitely differentiable exists. We shall construct an example. Let $E \subseteq \mathbb{R}$ be closed. Defining $f$ For the trivial case that $E=\emptyset$ take $f(x)=1$ . Otherwise, we define $f$ by 3 cases: $\quad$ (I) If $x \in E$ , $f(x) = 0$ . $\quad$ (II) If $x \in (a,b)$ , where $a,b \in E$ and $(a,b) \subseteq \mathbb{R} \setminus E$ , define $$F_{a,b}(x) = \frac{\pi}{b-a}\left(x + \frac{3b-5a}{2}\right),$$ $\quad$ and then $$f(x) = X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right),$$ $\quad$ where $X_{a,b}$ is some constant real based on $a, b$ , which for now is irrelevant. We shall define $X_{a,b}$ later in the proof when its value will be $\quad$ relevant. $\quad$ (III) If $E$ is bounded above, $M=\sup E$ , then $f(x)=e^{-\frac{1}{(x-M)^2}}$ for $x > M$ . Similarly, $f(x)=e^{-\frac{1}{(x-N)^2}}$ if $E$ is bounded below and $\quad N=\inf E$ . Clearly $f$ is defined for every $x \in E$ . $f$ is also defined for every $x \in \mathbb{R} \setminus E$ because such $x$ have some neighborhood in $\mathbb{R} \setminus E$ , otherwise $x$ would be a limit point of $E$ and therefore contained in $E$ . $E$ is the zero-set of $f$ To show that $E=Z(f)$ , we need to show that $x \not\in E$ implies $f(x) \neq 0$ . For $x$ in case (II) $$* \quad \frac{3}{2}\pi < F_{a,b}(x) < \frac{5}{2}\pi, $$ therefore $f(x)>0$ . For $x$ in case (III) clearly $f(x)>0$ as well. $f$ is infinitely differentiable we treat points of $E$ and points of $\mathbb{R} \setminus E$ separately. Points of $\mathbb{R} \setminus E$ : $\quad$ For $x$ in case (II), $f(x)=X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right)$ . To derive $f'(x)$ , we first note that $$F_{a,b}'(x)=\frac{\pi}{b-a}$$ and using lemma 1 $$\left(\cos^x(x)\right)' = \cos^x(x)\left(\ln\left(\cos x\right)-x\tan x\right).$$ We now prove by induction that for every $n \in \mathbb{N}$ , $f^{(n)}(x)$ exists and $f^{(n)}(x) = f(x)\cdot f_n(x)$ , where $f_n$ is some infinitely differentiable function. For $n=1$ , $$f'(x) = f(x)\left[\ln\left(\cos F_{a,b}(x)\right) - F_{a,b}(x)\tan F_{a,b}(x)\right]\frac{\pi}{b-a},$$ and we denote $f_1(x) = \frac{\pi}{b-a}\left[\ln\left(\cos F_{a,b}(x)\right) - F_{a,b}(x)\tan F_{a,b}(x)\right]$ . From $*$ it follows that $\cos F_{a,b}(x) > 0$ , hence $\ln\cos F_{a,b}(x)$ is well defined and is infinitely differentiable by lemma 2(d). Then, additionally using lemmas 2(a),(b),(d) we see that $f_1$ is infinitely differentiable. Suppose that for some $k \in \mathbb{N}$ $f^{(k)}(x)=f(x)f_k(x)$ , where $f_k$ is some infinitely differentiable function. Then, $$\begin{align*} f^{(k+1)}(x) &= f'(x)f_k(x) + f(x)f_k'(x)\\ &= f(x)(f_1(x)f_k(x) + f_k'(x)). \end{align*}$$ Denoting $f_{k+1}(x) = f_1(x)f_k(x)+f_k'(x)$ , it's clear from lemmas 2(b),(c) that $f_{k+1}$ is infinitely differentiable. $\quad$ For $x$ in case (III), suppose without loss of generality that $E$ is bounded above and $x > M$ . Then, since $e^x$ and $-\frac{1}{(x-M)^2}$ are infinitely differentiable (for $x \neq M$ ) we can use lemma 2(d) to get that $f(x)$ is infinitely differentiable. Points of $E$ : We shall describe the area ""to the left"" and ""to the right"" of $x$ . Given $d > 0$ , we denote $l_d=(x-d,x)$ and $r_d=(x,x+d)$ . Then, we claim that there is some $\varepsilon_0>0$ , such that at least one of the following must be true: $\quad$ (1) $\quad$ $l_{\varepsilon_0} \subseteq E$ . $\quad$ (2) $\quad$ $l_{\varepsilon_0} \subseteq \mathbb{R} \setminus E$ . $\quad$ (3) $\quad$ For all $0<\varepsilon\leq \varepsilon_0$ , there's some $t \in l_\varepsilon$ such that $t \in E$ . And the same goes for ""the right side"" of $x$ . To show that one of these options must be true, suppose by contradiction that they're all false. Then, since (3) is false, (2) is true and that's a contradiction. $\quad$ We shall prove that for every non-negative integer $n$ , $f^{(n)}(t) \to 0$ as $t \to x$ . Then we use it to prove by induction that $f^{(n)}(x)=0$ and therefore $f^{(n)}(x)$ exists. Here is this induction proof: For $n=0$ we already know that $f(x)=0$ . Suppose that for some non-negative integer $k$ , $f^{(k)}(x)=0$ . Then, since $f^{(k)}(t) \to 0$ as $t \to x$ it follows that $f^{(k)}$ is continuous at $x$ and since we know that $f^{(k+1)}$ exists for points of $\mathbb{R} \setminus E$ , $f^{(k)}$ is continuous. Then, since $f^{(k+1)}(t) \to 0$ as $t \to x$ , using Ex.9 it follows that $f^{(k+1)}(x)=0$ . $\quad$ If (1) is true, then $f$ is $0$ constant in $l_{\varepsilon_0}$ , therefore for every $n \in \mathbb{N}$ $f^{(n)}$ is 0 constant as well. Thus, for every non-negative integer $n$ , $f^{(n)}(t) \to 0$ as $t \to x$ for $x \in l_{\varepsilon_0}$ . This argument of course can be applied to $r_{\varepsilon_0}$ as well. $\quad$ If (2) is true, then either $x=N=\inf E$ or $x=b$ for some $(a,b)$ as in case (II). If $x=b$ , denote $w(z)=X_{a,b}\cos^{F_{a,b}(z)}\left(F_{a,b}(z)\right)$ , for $z \in [a,b]$ (note that $w=f$ on $[a,b]$ ). Then, since $w$ is continuous on $[a,b]$ (composition of continuous functions is continuous, $\lim_{t \to x}f(t) = 0$ , for $t \in l_{\varepsilon_0}$ . Since $w^{(n)}(t) = w(t)\cdot f_n(t)$ and $w^{(n+1)}(t)$ exists for all non-negative integer $n$ and $t \in [a,b]$ , it follows that $w^{(n)}$ is continuous on $[a,b]$ , therefore $f^{(n)}(t) \to w^{(n)}(x) = 0$ as $t \to x$ for $t \in l_{\varepsilon_0}$ . If $x=N$ , then $f(t)=e^{-\frac{1}{(t-N)^2}}$ , for $t \in l_{\varepsilon_0}$ . Then, we show that $f^{(n)}(t) = f(t)\cdot p_n(t)$ , where $p_n(t)$ is some polynomial of $\frac{1}{t-N}$ . We use induction: For $n=1$ , $f'(t) = f(t)\cdot 2\left(\frac{1}{t-N}\right)^3$ . If for some $k \in \mathbb{N}$ , $f^{(n)} = f(t)p_k(t)$ , where $p_k(t) = \sum_{j=0}^{i}c_j\left(\frac{1}{t-N}\right)^j$ , $i \in \mathbb{N}$ and $c_0, \dots, c_i \in \mathbb{R}$ . Then, $$\begin{align*} f^{(k+1)}(t)  &= f(t)2\left(\frac{1}{t-N}\right)^3\sum_{j=0}^{i}c_j\left(\frac{1}{t-N}\right)^j + f(t)\sum_{j=0}^{i}jc_j\left(\frac{1}{t-N}\right)^{j-1}\\ &= f(t)\underbrace{\left[\sum_{j=0}^{i}2c_j\left(\frac{1}{t-N}\right)^{j+3} + \sum_{j=0}^{i}jc_j\left(\frac{1}{t-N}\right)^{j-1}\right]}_{\text{clearly a polynomial of } \frac{1}{t-N}} \end{align*}$$ Now, if we prove that $\lim_{t \to N}\frac{f(t)}{|t-N|^m} = 0$ , for every $m \in \mathbb{N}$ , then clearly $f^{(n)}(t) \to 0$ as $t \to x$ for $t \in l_{\varepsilon_0}$ . $$ \lim_{t \to N}\frac{f(t)}{|t-N|^m} = \lim_{t \to N}\frac{e^{-\frac{1}{|t-N|^2}}}{|t-N|^m} = \lim_{h \to 0}\frac{e^{-\frac{1}{h^2}}}{h^m} = \lim_{n \to \infty}\frac{n^{\frac{m}{2}}}{e^n}. $$ If $m=2z$ , for some $z \in \mathbb{N}$ , we apply l'Hpital's rule $z$ times and get that $$\lim_{t \to N}\frac{f(t)}{|t-N|^m} = \lim_{n \to \infty}\frac{z!}{e^n} = 0.$$ If $m=2z-1$ , for some $z \in \mathbb{N}$ , we apply l'Hpital's rule $z$ times and get that $$\lim_{t \to N}\frac{f(t)}{|t-N|^m} =  \lim_{n \to \infty}\frac{\left(\frac{m}{2}\right)\left(\frac{m}{2}-1\right)\cdots\frac{1}{2}}{\sqrt{n}e^n} = 0.$$ In a very similar way we can prove this for $r_{\varepsilon_0}$ when either $x=a$ or $x=M=\sup E$ . The missing piece from the proof The last thing left to prove which I didn't mange to prove is that $f^{(n)}(t) \to 0$ as $t \to x$ for points $x \in E$ for which (3) is true. I should show how I tried to do it and what I did manage to show, that perhaps could be useful: $\quad$ Suppose (3) is true (but (1) is false). We should now define $X_{a,b}$ . Let $n_{a,b} \in \mathbb{N}$ be the smallest integer such that $\frac{1}{n_{a,b}} < b-a$ . Since $f_n$ is differentiable on $[a,b]$ , $f_n$ is continuous on $[a,b]$ , therefore by Theorem 4.15, there's some $M_n \in \mathbb{R}$ such that $\left|f_n(t)\right| \leq M_n$ for all $t \in [a,b]$ (notice that $f_n$ is independent of the value of $X_{a,b}$ ). Similarly, there is some $M_* \in \mathbb{R}$ such that $\left|\cos^{F_{a,b}(t)}\left(F_{a,b}(t)\right)\right| \leq M_*$ for all $t \in [a,b]$ . Put $M_{a,b} = \max \{M_n|n \leq n_{a,b}\}$ . Finally, we define $$X_{a,b} = \frac{1}{n_{a,b}M_*M_{a,b}}.$$ We shall now use this definition of $X_{a,b}$ to demonstrate that for every $d > 0$ , there is some $\delta > 0$ such that if $t \in l_\delta$ and $t \not\in E$ , then $|f^{(n)}(t)|< d$ . Given $d > 0$ , there is some $x_d \in l_d$ , such that $x_d \in E$ . Put $\delta = x - x_d$ . If $t \in l_\delta$ and $t \not\in E$ , then $t \in (a,b)$ as in (II), where $b-a \leq \delta$ . Thus $$ |f^{(n)}(t)|= X_{a,b}|f_n(t)|\left|\cos^{F_{a,b}(t)}\left(F_{a,b}(t)\right)\right| \leq \frac{1}{n_{a,b}M_*M_{a,b}}M_*M_{a,b} = \frac{1}{n_{a,b}} < b-a \leq \delta < d .$$ For points $t$ for which (1) is true we know that $f^{(n)}(t) = 0$ , so there's no problem there. But the problem is points for which only (3) is true. We don't know that they are 0 (we only think they suppose to be 0), but how can we prove this? It seems a bit circular. What I'm Asking For I have a few questions / things I want to get in an answer to this question: General assessment / critique of the incomplete proof (ignore the missing piece): I would like to get your opinion about every aspect of this proof - soundness, rigor, style, clarity and any other possible aspect you can think of. And most importantly is there some big unfixable mistake in this proof? Do you have a proposal to solve the missing piece? If needed feel free to change the definition of the coefficient $X_{a,b}$ as you see fit. If you think there's no way to solve this missing piece and this construction simply doesn't work, please don't suggest me a completely different solution to this exercise, I can look for solutions online, that's not a problem. Instead, please prove why this construction can't work for any choice of $X_{a,b}$ . How long does such a question should take to solve for a first time analysis student? Does it make sense that a single exercise would take so much time and effort, even when we talk about Rudin?","THE NEW (MUCH SHORTER) POST My original post perhaps was too long for most people, and understandably not a lot of people tried to go over it. In this new post my question is much shorter please prove or disprove that following function is infinitely differentiable on , or alternatively answer my original post (it is below here), which basically means helping me completing my proposed proof. Even if you don't want to answer my original post because it 's too long, it might be worth at least looking at some of it, because it could be useful. Let be some closed set of real numbers. Then, we define by 3 cases: (I) If , . (II) If , where and , define and then where is some constant real based on , which you need to define appropriately (probably different for every ), to prove that is infinitely differentiable, or if you want to prove that is not infinitely differentiable, you need to show that this is true for every choice for . (III) If is bounded above, , then for . Similarly, if is bounded below and . MY ORIGINAL POST: Baby Rudin Ex. 5.21 - Assessment of a Proposed Partial Solution and Help Completing It This exercise is driving me crazy. I'm honestly ashamed to tell you how long I actually stack with this problem not continuing to study mathematics until I solve it. In many points a long the way I was very close to giving up and in many points I thought that I finally managed to solve the problem but then realized there is some mistake or a missing piece in the proof. The proposed solution here is the closest I managed to get at this point. At this point I decided it's time to ask for help on the last part missing from this proof and get some people to critic this partial proof in general. The Problem: Ex. 21, Chap. 5 Let be a closed subset of . We saw in Exercise 22, Chap. 4, that there is a real continuous function on whose zero set is . Is it possible, for each closed set , to find such an which is differentiable on , or one which is times differentiable, or even one which has derivatives of all orders on ? My Idea (Informally) I think such an infinitely differentable function does exist. I want to give you a visual explanation of how I thought of constructing this function before going to the formal solution. We obviously set to be 0 for all points of . For every open interval of points of , where the end points are points of , is a ""wave"" that smoothly approches 0 in the end points so that the derivative of any order on those end points is always 0. The shorter this open interval is, the smaller the maximum point of this ""wave"". If is bounded above, is just some increasing function after , so that it smoothly approches 0 at (the same goes if is bounded below). I drew an illustration of this idea: The red parts are points of , the blue parts are points of . is a limit point of . My Partial Solution My proof uses two previous results from the book and two additional lemmas. I shall present them here (and prove the lemmas) before going to the main proof. Ex. 5.9. Let be a continuous real function on , of which it is known that exists for all and that as . Does it follow that exists? In the solution to this exercise we saw that does in fact exists and is 3. In general to any real continuous function on for which the derivative is known to exist except for a specific point and as , we can use the same proof we use in this exercise to show that exists and . Theorem 4.15. If is a continuous mapping of a compact metric space into , then is closed and bounded. Thus, is bounded. Lemma 1. Let differentiable functions. Then, Proof. Let , then Differentiating both sides we get: Thus, Lemma 2. Let infinitely differentiable functions. Then, for every : (a) is infinitely differentiable and . (b) is infinitely differentiable and where , are sequences of non-negative integers. (c) is infinitely differentiable whenever and where is some infinitely differentiable function. (d) is infinitely differentiable and for some , and where each term is either of the form or , a non-negative integer. Proof. We use induction for all 4 of the sections of this lemma. (a) For , . Suppose that for some . Then, (b) For , . Suppose that for some , there are sequences , of non-negative integers such that . Then, Now, let Thus, (c) For , . Suppose that for some , , where is some infinitely differentiable function. Then, if we put , we get the desired result that (d) For , . Suppose that for some , for some , and where each term is either of the form or , a non-negative integer. Then, . We then prove by induction for every such that and . For , . Suppose that is true for some natural . Then, Then, if we put we get that Since is either of the form or , , can clearly be arranged in our desired form. The main proof. Yes, such a function which is infinitely differentiable exists. We shall construct an example. Let be closed. Defining For the trivial case that take . Otherwise, we define by 3 cases: (I) If , . (II) If , where and , define and then where is some constant real based on , which for now is irrelevant. We shall define later in the proof when its value will be relevant. (III) If is bounded above, , then for . Similarly, if is bounded below and . Clearly is defined for every . is also defined for every because such have some neighborhood in , otherwise would be a limit point of and therefore contained in . is the zero-set of To show that , we need to show that implies . For in case (II) therefore . For in case (III) clearly as well. is infinitely differentiable we treat points of and points of separately. Points of : For in case (II), . To derive , we first note that and using lemma 1 We now prove by induction that for every , exists and , where is some infinitely differentiable function. For , and we denote . From it follows that , hence is well defined and is infinitely differentiable by lemma 2(d). Then, additionally using lemmas 2(a),(b),(d) we see that is infinitely differentiable. Suppose that for some , where is some infinitely differentiable function. Then, Denoting , it's clear from lemmas 2(b),(c) that is infinitely differentiable. For in case (III), suppose without loss of generality that is bounded above and . Then, since and are infinitely differentiable (for ) we can use lemma 2(d) to get that is infinitely differentiable. Points of : We shall describe the area ""to the left"" and ""to the right"" of . Given , we denote and . Then, we claim that there is some , such that at least one of the following must be true: (1) . (2) . (3) For all , there's some such that . And the same goes for ""the right side"" of . To show that one of these options must be true, suppose by contradiction that they're all false. Then, since (3) is false, (2) is true and that's a contradiction. We shall prove that for every non-negative integer , as . Then we use it to prove by induction that and therefore exists. Here is this induction proof: For we already know that . Suppose that for some non-negative integer , . Then, since as it follows that is continuous at and since we know that exists for points of , is continuous. Then, since as , using Ex.9 it follows that . If (1) is true, then is constant in , therefore for every is 0 constant as well. Thus, for every non-negative integer , as for . This argument of course can be applied to as well. If (2) is true, then either or for some as in case (II). If , denote , for (note that on ). Then, since is continuous on (composition of continuous functions is continuous, , for . Since and exists for all non-negative integer and , it follows that is continuous on , therefore as for . If , then , for . Then, we show that , where is some polynomial of . We use induction: For , . If for some , , where , and . Then, Now, if we prove that , for every , then clearly as for . If , for some , we apply l'Hpital's rule times and get that If , for some , we apply l'Hpital's rule times and get that In a very similar way we can prove this for when either or . The missing piece from the proof The last thing left to prove which I didn't mange to prove is that as for points for which (3) is true. I should show how I tried to do it and what I did manage to show, that perhaps could be useful: Suppose (3) is true (but (1) is false). We should now define . Let be the smallest integer such that . Since is differentiable on , is continuous on , therefore by Theorem 4.15, there's some such that for all (notice that is independent of the value of ). Similarly, there is some such that for all . Put . Finally, we define We shall now use this definition of to demonstrate that for every , there is some such that if and , then . Given , there is some , such that . Put . If and , then as in (II), where . Thus For points for which (1) is true we know that , so there's no problem there. But the problem is points for which only (3) is true. We don't know that they are 0 (we only think they suppose to be 0), but how can we prove this? It seems a bit circular. What I'm Asking For I have a few questions / things I want to get in an answer to this question: General assessment / critique of the incomplete proof (ignore the missing piece): I would like to get your opinion about every aspect of this proof - soundness, rigor, style, clarity and any other possible aspect you can think of. And most importantly is there some big unfixable mistake in this proof? Do you have a proposal to solve the missing piece? If needed feel free to change the definition of the coefficient as you see fit. If you think there's no way to solve this missing piece and this construction simply doesn't work, please don't suggest me a completely different solution to this exercise, I can look for solutions online, that's not a problem. Instead, please prove why this construction can't work for any choice of . How long does such a question should take to solve for a first time analysis student? Does it make sense that a single exercise would take so much time and effort, even when we talk about Rudin?","\mathbb{R} E f \quad x \in E f(x) = 0 \quad x \in (a,b) a,b \in E (a,b) \subseteq \mathbb{R} \setminus E F_{a,b}(x) = \frac{\pi}{b-a}\left(x + \frac{3b-5a}{2}\right), \quad f(x) = X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right), \quad X_{a,b} a, b (a,b) f f X_{a,b} \quad E M=\sup E f(x)=e^{-\frac{1}{(x-M)^2}} x > M f(x)=e^{-\frac{1}{(x-N)^2}} E \quad N=\inf E E \mathbb{R} f \mathbb{R} E E f \mathbb{R} n \mathbb{R} f E \mathbb{R} \setminus E E f E f \sup E \sup E E f(E) f(\mathbb{R} \setminus E) P E f \mathbb{R} f'(x) x \neq 0 f'(x) \to 3 x \to 0 f'(0) f'(0) f \mathbb{R} a f'(x) \to D x \to a f'(a) f'(a)=D f X \mathbb{R}^k f(X) f f, g: \mathbb{R} \to \mathbb{R} \left(f(x)^{g(x)}\right)' = f(x)^{g(x)}\left[g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}\right] h(x)=f(x)^{g(x)} \ln(h(x)) = \ln\left(f(x)^{g(x)}\right) = g(x)\ln(f(x)). \frac{h'(x)}{h(x)} = g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}. h'(x) = f(x)^{g(x)}\left[g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}\right]. f,g : \mathbb{R} \to \mathbb{R} n \in \mathbb{N} \quad f+g (f+g)^{(n)}(x)=f^{(n)}(x)+g^{(n)}(x) \quad f\cdot g (f\cdot g)^{(n)}(x)=\sum_{i=1}^{2^n} f^{\left(\alpha_{n_i}\right)}(x)\cdot g^{\left(\beta_{n_i}\right)}(x), \quad \{\alpha_{n_i}\} \{\beta_{n_i}\} \quad \frac{f}{g} g(x) \neq 0 \left(\frac{f}{g}\right)^{(n)}(x)=\frac{F_n(x)}{g^{2^n}(x)}, \quad F_n \quad f\circ g (f\circ g)^{(n)}(x)=\sum_{j=1}^{z_n}\prod_{i=1}^{m_{n_j}}t_{n_{j_i}}, \quad z_n \in \mathbb{N} m_{n_1} and \dots, m_{n_{z_n}} \in \mathbb{N} t_{n_{j_i}} f^{(m)}(g(x)) g^{(m)}(x) m \quad \quad n=1 (f+g)'(x) = f'(x) + g'(x) (f+g)^{(k)}(x)=f^{(k)}(x)+g^{(k)}(x) k \in \mathbb{N} (f+g)^{(k+1)}(x)=\left(f^{(k)}(x)\right)' + \left(g^{(k)}(x)\right)' = f^{(k+1)}(x) + g^{(k+1)}(x). \quad n=1 (f \cdot g)'(x) = f'(x)g(x) + f(x)g'(x) k \in \mathbb{N} \{\alpha_{k_i}\} \{\beta_{k_i}\} (f\cdot g)^{(k)}(x)=\sum_{i=1}^{2^k} f^{\left(\alpha_{k_i}\right)}(x)\cdot g^{\left(\beta_{k_i}\right)}(x) (f\cdot g)^{(k+1)}(x)=\sum_{i=1}^{2^k} f^{\left(\alpha_{k_i}+1\right)}(x)\cdot g^{\left(\beta_{k_i}\right)}(x) + f^{\left(\alpha_{k_i}\right)}(x)\cdot g^{\left(\beta_{k_i}+1\right)}(x). \alpha_{\left(k+1\right)_i} = \begin{cases}
\alpha_{k_i+1}, \quad 1 \leq i \leq 2^k\\
\alpha_{k_{\left(i-2^k\right)}}, \quad 2^k + 1 \leq i \leq 2^{k+1},
\end{cases} \beta_{\left(k+1\right)_i} = \begin{cases}
\beta_{k_i}, \quad 1 \leq i \leq 2^k\\
\beta_{k_{\left(i-2^k\right)}+1}, \quad 2^k + 1 \leq i \leq 2^{k+1}.
\end{cases} (f\cdot g)^{(k+1)}(x) = \sum_{i=1}^{2^{k+1}}f^{\left(\alpha_{(k+1)_i}\right)}(x)\cdot g^{\left(\beta_{(k+1)_i}\right)}(x). \quad n=1 \left(\frac{f}{g}\right)'(x) = \frac{f'(x)g(x)+f(x)g'(x)}{g^2(x)} k \in \mathbb{N} \left(\frac{f}{g}\right)^{(k)}(x) = \frac{F_k(x)}{g^{2^k}(x)} F_k F_{k+1}(x) = F_k(x)g^{2^k}(x)-2^kg^{2^k-1}(x)g'(x)F_k(x) \left(\frac{f}{g}\right)^{(k+1)}(x) = \frac{F_{k+1}(x)}{g^{2^{k+1}}(x)}. \quad n=1 (f \circ g)'(x) = f'(g(x))g'(x) k \in \mathbb{N} (f\circ g)^{(k)}(x)=\sum_{j=1}^{z_k}\prod_{i=1}^{m_{k_j}}t_{k_{j_i}} z_k \in \mathbb{N} m_{k_1} and \dots, m_{k_{z_k}} \in \mathbb{N} t_{k_{j_i}} f^{(m)}(g(x)) g^{(m)}(x) m (f\circ g)^{(k+1)}(x)=\sum_{j=1}^{z_k}\left(\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right)' \bullet \quad \left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = \sum_{i=1}^{m}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m}t_{k_{j_i}}\left(\prod_{i=m+1}^{m_{k_j}}t_{k_{j_i}}\right)', m,j \in \mathbb{N} j \leq z_k m < m_{k_j} m=1 \left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = t_{k_{j_1}}'\prod_{i=2}^{m_{k_j}}t_{k_{j_i}} + t_{k_{j_1}}\left(\prod_{i=2}^{m_{k_j}}t_{k_{j_i}}\right)' \bullet m < m_{k_j}-1 \begin{align*}
\left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' 
& = \sum_{i=1}^{m}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m}t_{k_{j_i}}\left[t_{k_{j_{m+1}}}'\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right) + t_{k_{j_{m+1}}}\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right)'\right]\\
& = \sum_{i=1}^{m+1}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m+1}t_{k_{j_i}}\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right)'.
\end{align*} m=m_{k_j}-1 \left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = \sum_{i=1}^{m_{k_j}}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right). t_{k_{j_i}}' f^{(u)}(g(x))g'(x) g^{(u)}(x) u \in \mathbb{N} 
(f \circ g)^{(k+1)}(x) = \sum_{j=1}^{z_k}\sum_{i=1}^{m_{k_j}}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right)
 E \subseteq \mathbb{R} f E=\emptyset f(x)=1 f \quad x \in E f(x) = 0 \quad x \in (a,b) a,b \in E (a,b) \subseteq \mathbb{R} \setminus E F_{a,b}(x) = \frac{\pi}{b-a}\left(x + \frac{3b-5a}{2}\right), \quad f(x) = X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right), \quad X_{a,b} a, b X_{a,b} \quad \quad E M=\sup E f(x)=e^{-\frac{1}{(x-M)^2}} x > M f(x)=e^{-\frac{1}{(x-N)^2}} E \quad N=\inf E f x \in E f x \in \mathbb{R} \setminus E x \mathbb{R} \setminus E x E E E f E=Z(f) x \not\in E f(x) \neq 0 x * \quad \frac{3}{2}\pi < F_{a,b}(x) < \frac{5}{2}\pi,  f(x)>0 x f(x)>0 f E \mathbb{R} \setminus E \mathbb{R} \setminus E \quad x f(x)=X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right) f'(x) F_{a,b}'(x)=\frac{\pi}{b-a} \left(\cos^x(x)\right)' = \cos^x(x)\left(\ln\left(\cos x\right)-x\tan x\right). n \in \mathbb{N} f^{(n)}(x) f^{(n)}(x) = f(x)\cdot f_n(x) f_n n=1 f'(x) = f(x)\left[\ln\left(\cos F_{a,b}(x)\right) - F_{a,b}(x)\tan F_{a,b}(x)\right]\frac{\pi}{b-a}, f_1(x) = \frac{\pi}{b-a}\left[\ln\left(\cos F_{a,b}(x)\right) - F_{a,b}(x)\tan F_{a,b}(x)\right] * \cos F_{a,b}(x) > 0 \ln\cos F_{a,b}(x) f_1 k \in \mathbb{N} f^{(k)}(x)=f(x)f_k(x) f_k \begin{align*}
f^{(k+1)}(x)
&= f'(x)f_k(x) + f(x)f_k'(x)\\
&= f(x)(f_1(x)f_k(x) + f_k'(x)).
\end{align*} f_{k+1}(x) = f_1(x)f_k(x)+f_k'(x) f_{k+1} \quad x E x > M e^x -\frac{1}{(x-M)^2} x \neq M f(x) E x d > 0 l_d=(x-d,x) r_d=(x,x+d) \varepsilon_0>0 \quad \quad l_{\varepsilon_0} \subseteq E \quad \quad l_{\varepsilon_0} \subseteq \mathbb{R} \setminus E \quad \quad 0<\varepsilon\leq \varepsilon_0 t \in l_\varepsilon t \in E x \quad n f^{(n)}(t) \to 0 t \to x f^{(n)}(x)=0 f^{(n)}(x) n=0 f(x)=0 k f^{(k)}(x)=0 f^{(k)}(t) \to 0 t \to x f^{(k)} x f^{(k+1)} \mathbb{R} \setminus E f^{(k)} f^{(k+1)}(t) \to 0 t \to x f^{(k+1)}(x)=0 \quad f 0 l_{\varepsilon_0} n \in \mathbb{N} f^{(n)} n f^{(n)}(t) \to 0 t \to x x \in l_{\varepsilon_0} r_{\varepsilon_0} \quad x=N=\inf E x=b (a,b) x=b w(z)=X_{a,b}\cos^{F_{a,b}(z)}\left(F_{a,b}(z)\right) z \in [a,b] w=f [a,b] w [a,b] \lim_{t \to x}f(t) = 0 t \in l_{\varepsilon_0} w^{(n)}(t) = w(t)\cdot f_n(t) w^{(n+1)}(t) n t \in [a,b] w^{(n)} [a,b] f^{(n)}(t) \to w^{(n)}(x) = 0 t \to x t \in l_{\varepsilon_0} x=N f(t)=e^{-\frac{1}{(t-N)^2}} t \in l_{\varepsilon_0} f^{(n)}(t) = f(t)\cdot p_n(t) p_n(t) \frac{1}{t-N} n=1 f'(t) = f(t)\cdot 2\left(\frac{1}{t-N}\right)^3 k \in \mathbb{N} f^{(n)} = f(t)p_k(t) p_k(t) = \sum_{j=0}^{i}c_j\left(\frac{1}{t-N}\right)^j i \in \mathbb{N} c_0, \dots, c_i \in \mathbb{R} \begin{align*}
f^{(k+1)}(t) 
&= f(t)2\left(\frac{1}{t-N}\right)^3\sum_{j=0}^{i}c_j\left(\frac{1}{t-N}\right)^j + f(t)\sum_{j=0}^{i}jc_j\left(\frac{1}{t-N}\right)^{j-1}\\
&= f(t)\underbrace{\left[\sum_{j=0}^{i}2c_j\left(\frac{1}{t-N}\right)^{j+3} + \sum_{j=0}^{i}jc_j\left(\frac{1}{t-N}\right)^{j-1}\right]}_{\text{clearly a polynomial of } \frac{1}{t-N}}
\end{align*} \lim_{t \to N}\frac{f(t)}{|t-N|^m} = 0 m \in \mathbb{N} f^{(n)}(t) \to 0 t \to x t \in l_{\varepsilon_0} 
\lim_{t \to N}\frac{f(t)}{|t-N|^m} = \lim_{t \to N}\frac{e^{-\frac{1}{|t-N|^2}}}{|t-N|^m} = \lim_{h \to 0}\frac{e^{-\frac{1}{h^2}}}{h^m} = \lim_{n \to \infty}\frac{n^{\frac{m}{2}}}{e^n}.
 m=2z z \in \mathbb{N} z \lim_{t \to N}\frac{f(t)}{|t-N|^m} = \lim_{n \to \infty}\frac{z!}{e^n} = 0. m=2z-1 z \in \mathbb{N} z \lim_{t \to N}\frac{f(t)}{|t-N|^m} = 
\lim_{n \to \infty}\frac{\left(\frac{m}{2}\right)\left(\frac{m}{2}-1\right)\cdots\frac{1}{2}}{\sqrt{n}e^n} = 0. r_{\varepsilon_0} x=a x=M=\sup E f^{(n)}(t) \to 0 t \to x x \in E \quad X_{a,b} n_{a,b} \in \mathbb{N} \frac{1}{n_{a,b}} < b-a f_n [a,b] f_n [a,b] M_n \in \mathbb{R} \left|f_n(t)\right| \leq M_n t \in [a,b] f_n X_{a,b} M_* \in \mathbb{R} \left|\cos^{F_{a,b}(t)}\left(F_{a,b}(t)\right)\right| \leq M_* t \in [a,b] M_{a,b} = \max \{M_n|n \leq n_{a,b}\} X_{a,b} = \frac{1}{n_{a,b}M_*M_{a,b}}. X_{a,b} d > 0 \delta > 0 t \in l_\delta t \not\in E |f^{(n)}(t)|< d d > 0 x_d \in l_d x_d \in E \delta = x - x_d t \in l_\delta t \not\in E t \in (a,b) b-a \leq \delta 
|f^{(n)}(t)|=
X_{a,b}|f_n(t)|\left|\cos^{F_{a,b}(t)}\left(F_{a,b}(t)\right)\right|
\leq \frac{1}{n_{a,b}M_*M_{a,b}}M_*M_{a,b}
= \frac{1}{n_{a,b}}
< b-a \leq \delta
< d
. t f^{(n)}(t) = 0 X_{a,b} X_{a,b}","['real-analysis', 'derivatives', 'solution-verification', 'proof-writing', 'smooth-functions']"
21,"Does convergence in $L^1$ of $\{f_n(\cdot,\ell)\}_{n \in \mathbb{N}}$ for each $\ell \in \mathbb{Q}$ implies convergence for $\ell \in \mathbb{R}$?",Does convergence in  of  for each  implies convergence for ?,"L^1 \{f_n(\cdot,\ell)\}_{n \in \mathbb{N}} \ell \in \mathbb{Q} \ell \in \mathbb{R}","On a measure space $(X,\mathcal{A},\mu)$ , consider a sequence of functions $\{f_n:X \times \mathbb{R} \longrightarrow \mathbb{R}\}_{n \in \mathbb{N}}$ with $\ell \in \mathbb{R} \longmapsto f_n(x,\ell)\in \mathbb{R}$ is increasing and continuous for each $x \in X$ , $n \in \mathbb{N}$ , such that $$     \lim_{n \to \infty}\int_X|f_n(x,\ell) - f_0(x,\ell)|\mu(dx) = 0,    ~\mbox{for all}~    \ell \in \mathbb{Q}, $$ then does it hold that $$     \lim_{n \to \infty}\int_X|f_n(x,\ell) - f_0(x,\ell)|\mu(dx) = 0,    ~\mbox{for all}~    \ell \in \mathbb{R}. $$ My attempt: if $\mu$ is a finite measure, then for $\ell \in \mathbb{Q}$ , $f_n(\cdot,\ell)$ converges to $f_0(\cdot,\ell)$ in measure $\mu$ , then it is equivalent to that for any subsequence $\{n_k\}_{k \in \mathbb{N}}$ of $\{n\}_{n \in \mathbb{N}}$ , there exists a subsubsequence $\{n_{k_m}\}_{m \in \mathbb{N}}$ such that for $\ell \in \mathbb{Q}$ , $f_{n_{k_m}}(\cdot,\ell)$ converges to $f_0(\cdot,\ell)$ $\mu$ -a.e. Then we can easily deduce that for $\ell \in \mathbb{R}$ , $f_{n_{k_m}}(\cdot,\ell)$ converges to $f_0(\cdot,\ell)$ $\mu$ -a.e., which is equivalent to that for $\ell \in \mathbb{R}$ , $f_n(\cdot,\ell)$ converges to $f_0(\cdot,\ell)$ in measure $\mu$ . Then it remains to prove that for all $\ell \in \mathbb{R}$ , $\{f_n(\cdot,\ell)\}_{n \in \mathbb{N}}$ is uniformly integrable. From another point of view, if we define a projection \begin{align}     \pi: (L^1(\mu))^{\mathbb{R}} & \longrightarrow (L^1(\mu))^{\mathbb{Q}}, \\       (f(\cdot,\ell))_{\ell \in \mathbb{R}}  & \longmapsto  (f(\cdot,\ell))_{\ell \in \mathbb{Q}}, \end{align} and let $E \subset (L^1(\mu))^{\mathbb{R}}$ defined as $$         E := \{(f(\cdot,\ell))_{\ell \in \mathbb{R}}: \ell \in \mathbb{R} \longmapsto f(x,\ell)\in \mathbb{R}~\mbox{is increasing and continuous for each}~ x \in X \}, $$ All product spaces are equipped with the product topology and then I can show that $\pi|_{E}$ is a continuous injective function, the question falls into that is $\pi^{-1}|_{\pi(E)}$ continuous, i.e. is $\pi|_{E}$ a homeomorphism?","On a measure space , consider a sequence of functions with is increasing and continuous for each , , such that then does it hold that My attempt: if is a finite measure, then for , converges to in measure , then it is equivalent to that for any subsequence of , there exists a subsubsequence such that for , converges to -a.e. Then we can easily deduce that for , converges to -a.e., which is equivalent to that for , converges to in measure . Then it remains to prove that for all , is uniformly integrable. From another point of view, if we define a projection and let defined as All product spaces are equipped with the product topology and then I can show that is a continuous injective function, the question falls into that is continuous, i.e. is a homeomorphism?","(X,\mathcal{A},\mu) \{f_n:X \times \mathbb{R} \longrightarrow \mathbb{R}\}_{n \in \mathbb{N}} \ell \in \mathbb{R} \longmapsto f_n(x,\ell)\in \mathbb{R} x \in X n \in \mathbb{N} 
    \lim_{n \to \infty}\int_X|f_n(x,\ell) - f_0(x,\ell)|\mu(dx) = 0,
   ~\mbox{for all}~
   \ell \in \mathbb{Q},
 
    \lim_{n \to \infty}\int_X|f_n(x,\ell) - f_0(x,\ell)|\mu(dx) = 0,
   ~\mbox{for all}~
   \ell \in \mathbb{R}.
 \mu \ell \in \mathbb{Q} f_n(\cdot,\ell) f_0(\cdot,\ell) \mu \{n_k\}_{k \in \mathbb{N}} \{n\}_{n \in \mathbb{N}} \{n_{k_m}\}_{m \in \mathbb{N}} \ell \in \mathbb{Q} f_{n_{k_m}}(\cdot,\ell) f_0(\cdot,\ell) \mu \ell \in \mathbb{R} f_{n_{k_m}}(\cdot,\ell) f_0(\cdot,\ell) \mu \ell \in \mathbb{R} f_n(\cdot,\ell) f_0(\cdot,\ell) \mu \ell \in \mathbb{R} \{f_n(\cdot,\ell)\}_{n \in \mathbb{N}} \begin{align}
    \pi: (L^1(\mu))^{\mathbb{R}} & \longrightarrow (L^1(\mu))^{\mathbb{Q}},
\\       (f(\cdot,\ell))_{\ell \in \mathbb{R}}  & \longmapsto  (f(\cdot,\ell))_{\ell \in \mathbb{Q}},
\end{align} E \subset (L^1(\mu))^{\mathbb{R}} 
        E := \{(f(\cdot,\ell))_{\ell \in \mathbb{R}}: \ell \in \mathbb{R} \longmapsto f(x,\ell)\in \mathbb{R}~\mbox{is increasing and continuous for each}~ x \in X \},
 \pi|_{E} \pi^{-1}|_{\pi(E)} \pi|_{E}","['real-analysis', 'calculus', 'probability', 'convergence-divergence']"
22,Diffeomorphism between two norms,Diffeomorphism between two norms,,"If I consider the $L^2$ and $L^4$ norms on $\mathbb R^n$ , i.e. $||x||_2 = (\sum_{i=1}^n x_i^2)^{1/2}$ and $||x||_4 = (\sum_{i=1}^n x_i^4)^{1/4}$ , I know these two norms are equivalent in the sense we can find constants $c, C$ such that $c||x||_2 \leq ||x||_4 \leq C||x||_2$ , but are they equivalent as smooth (away from zero) functions in the sense I can find a change of coordinates/diffeomorphism sending one norm to the other? I can definitely find a composition which sends one norm to the other: if $$\varphi(x_1, \cdots, x_n) = (x_1^2, \cdots, x_n^2), \quad \psi(x) = \sqrt{x}$$ then $\psi \circ ||x||_2 \circ \varphi = ||x||_4$ . I would like to know whether I can find a single function $\Phi: \mathbb R^n \rightarrow \mathbb R^n$ such that $||x||_2 \circ \Phi = ||x||_4$ and whether $\Phi$ can have bounded Jacobian. Also, in general, can I do this for any pair of norms on $\mathbb R^n$ and not necessarily $L^p$ ? If not, can we do it with two functions $\varphi, \psi$ instead? I have some intuition: I think I ""want to send a $L^2$ -ball to an $L^4$ -ball"" in order for $||x||_2\circ\Phi=||x||_4$ to hold. And if I have a map $\Phi_0$ sending one ball to another $B_{L^2}(1) \rightarrow B_{L^4}(1)$ I think I can extend this to a map $\Phi$ on the entire space by taking a point, scaling it back to the unit ball, mapping it under $\varphi$ , then rescaling. The only downside of this is that I imagine is the scaling: I think I would end up needing to divide (and multiply) by a norm and I can't imagine this having a nice derivative. This makes me think maybe I'm hoping too much. The idea of this application might be the integration of radial functions - is there a nice change of coordinates (i.e. bounded Jacobian) from one norm to another?","If I consider the and norms on , i.e. and , I know these two norms are equivalent in the sense we can find constants such that , but are they equivalent as smooth (away from zero) functions in the sense I can find a change of coordinates/diffeomorphism sending one norm to the other? I can definitely find a composition which sends one norm to the other: if then . I would like to know whether I can find a single function such that and whether can have bounded Jacobian. Also, in general, can I do this for any pair of norms on and not necessarily ? If not, can we do it with two functions instead? I have some intuition: I think I ""want to send a -ball to an -ball"" in order for to hold. And if I have a map sending one ball to another I think I can extend this to a map on the entire space by taking a point, scaling it back to the unit ball, mapping it under , then rescaling. The only downside of this is that I imagine is the scaling: I think I would end up needing to divide (and multiply) by a norm and I can't imagine this having a nice derivative. This makes me think maybe I'm hoping too much. The idea of this application might be the integration of radial functions - is there a nice change of coordinates (i.e. bounded Jacobian) from one norm to another?","L^2 L^4 \mathbb R^n ||x||_2 = (\sum_{i=1}^n x_i^2)^{1/2} ||x||_4 = (\sum_{i=1}^n x_i^4)^{1/4} c, C c||x||_2 \leq ||x||_4 \leq C||x||_2 \varphi(x_1, \cdots, x_n) = (x_1^2, \cdots, x_n^2), \quad \psi(x) = \sqrt{x} \psi \circ ||x||_2 \circ \varphi = ||x||_4 \Phi: \mathbb R^n \rightarrow \mathbb R^n ||x||_2 \circ \Phi = ||x||_4 \Phi \mathbb R^n L^p \varphi, \psi L^2 L^4 ||x||_2\circ\Phi=||x||_4 \Phi_0 B_{L^2}(1) \rightarrow B_{L^4}(1) \Phi \varphi","['real-analysis', 'functional-analysis', 'normed-spaces']"
23,Is the continuous image of a Borel subset Lebesgue measurable?,Is the continuous image of a Borel subset Lebesgue measurable?,,"Let $f:\mathbb R^n \to \mathbb R^n$ be a continuous map and $B \subset \mathbb R^n$ a Borel subset. It is well known that $f(B)$ may not be a Borel subset. My question is, can we prove that $f(B)$ must be measurable for Lebesgue measure?","Let be a continuous map and a Borel subset. It is well known that may not be a Borel subset. My question is, can we prove that must be measurable for Lebesgue measure?",f:\mathbb R^n \to \mathbb R^n B \subset \mathbb R^n f(B) f(B),"['real-analysis', 'measure-theory', 'lebesgue-measure', 'borel-sets', 'measurable-sets']"
24,How to solve $\left(\sqrt{\frac{x-1}{x}}\right)^{x^2}=\left(\frac{1}{x}\right)^{x+1}$?,How to solve ?,\left(\sqrt{\frac{x-1}{x}}\right)^{x^2}=\left(\frac{1}{x}\right)^{x+1},"I need help to solve this equation, please. $$\left(\sqrt{\frac{x-1}{x}}\right)^{x^2}=\left(\frac{1}{x}\right)^{x+1}$$ I know that the solution is $x=\varphi$ (the golden ratio). I got this result by equating the bases and the exponents. I want to know if there are another way to obtain this. Thank you.","I need help to solve this equation, please. I know that the solution is (the golden ratio). I got this result by equating the bases and the exponents. I want to know if there are another way to obtain this. Thank you.",\left(\sqrt{\frac{x-1}{x}}\right)^{x^2}=\left(\frac{1}{x}\right)^{x+1} x=\varphi,"['real-analysis', 'calculus', 'algebra-precalculus', 'radicals', 'golden-ratio']"
25,"Solving overdetermined, well posed, linear system of PDEs","Solving overdetermined, well posed, linear system of PDEs",,"Let $f=f(u,v)$ be a (given) solution of the following PDE, $$ \begin{equation} \frac{\partial^2 f}{\partial u\partial v}=f,\label{1}\tag{$*$} \end{equation} $$ and consider the overdetermined system(s) of PDEs $$ \begin{cases} \dfrac{\partial x}{\partial u}=f\cos\left(u-v\right)\\ \\ \dfrac{\partial x}{\partial v}=\dfrac{\partial f}{\partial v}\sin\left(u-v\right)\\ \\ \dfrac{\partial y}{\partial u}=f\sin\left(u-v\right)\\ \\ \dfrac{\partial y}{\partial v}=-\dfrac{\partial f}{\partial v}\cos\left(u-v\right). \end{cases} $$ Since $f$ solves \eqref{1} it is guaranteed that $$ \frac{\partial^2 x}{\partial u\partial v}=\frac{\partial^2 x}{\partial v\partial u} $$ and similarly for $y$ , so the system is well posed. The question is, is it possible to write down a general solution for $x(u,v)$ and $y(u,v)$ in terms of integrals of $f$ ? If I try, for instance, to integrate the first equation to get $$ x=\int \mathrm{d}u\;f\cos(u-v)+g(v), $$ with $g(v)$ some unknown function of $v$ , and then plug in the second equation for $x$ , I cannot solve for $g(v)$ , so I need to try some other ansatz.","Let be a (given) solution of the following PDE, and consider the overdetermined system(s) of PDEs Since solves \eqref{1} it is guaranteed that and similarly for , so the system is well posed. The question is, is it possible to write down a general solution for and in terms of integrals of ? If I try, for instance, to integrate the first equation to get with some unknown function of , and then plug in the second equation for , I cannot solve for , so I need to try some other ansatz.","f=f(u,v) 
\begin{equation}
\frac{\partial^2 f}{\partial u\partial v}=f,\label{1}\tag{*}
\end{equation}
 
\begin{cases}
\dfrac{\partial x}{\partial u}=f\cos\left(u-v\right)\\
\\
\dfrac{\partial x}{\partial v}=\dfrac{\partial f}{\partial v}\sin\left(u-v\right)\\
\\
\dfrac{\partial y}{\partial u}=f\sin\left(u-v\right)\\
\\
\dfrac{\partial y}{\partial v}=-\dfrac{\partial f}{\partial v}\cos\left(u-v\right).
\end{cases}
 f 
\frac{\partial^2 x}{\partial u\partial v}=\frac{\partial^2 x}{\partial v\partial u}
 y x(u,v) y(u,v) f 
x=\int \mathrm{d}u\;f\cos(u-v)+g(v),
 g(v) v x g(v)","['real-analysis', 'analysis', 'multivariable-calculus', 'partial-differential-equations', 'systems-of-equations']"
26,Picture of l'Hopital's rule at infinity,Picture of l'Hopital's rule at infinity,,"I know that when L'Hopital's rule is taken for the $\frac{0}{0}$ case, we get that the functions start looking at lines, like in this figure from Stewart's Calculus: But what is the visual when the limit is the $\frac{\infty}{\infty}$ case? The only thing that comes to mind that is easily visualized is a rational function's asymptotes (and that comes from dividing numerator and denominator by the largest exponent in the denominator)","I know that when L'Hopital's rule is taken for the case, we get that the functions start looking at lines, like in this figure from Stewart's Calculus: But what is the visual when the limit is the case? The only thing that comes to mind that is easily visualized is a rational function's asymptotes (and that comes from dividing numerator and denominator by the largest exponent in the denominator)",\frac{0}{0} \frac{\infty}{\infty},['real-analysis']
27,Does the derivative of a differentiable function have to be Lebesgue integrable in some interval?,Does the derivative of a differentiable function have to be Lebesgue integrable in some interval?,,"I know that the derivative of a differentiable function doesn't have to be continuous. How discontinuous can a derivative be? . Inspired by Limits and continuity of a derivative , I was thinking of defining the notion of pseudo-continuous: $f:(a,b) \to \mathbb R$ is pseudo-continuous at $x \in (a,b)$ if $$ f(x) = \lim_{y\to x} \frac1{y-x} \int_x^y f(t) \, dt .$$ And then I wanted to show that a function is the derivative of a differentiable function if and only if it is pseudo-continuous. But then I realized that the derivative doesn't have to be Lebesgue integrable, for example $$ f(x) = \frac x{\log|x|} \sin\left(\frac1x\right) , \quad x \in (-\tfrac12,\tfrac12) ,$$ or $$ f(x) = x^2 \sin\left(\frac1{x^2}\right)  ,$$ Does there exist a differentiable function $f:(0,1) \to \mathbb R$ such that its derivative restricted to any subinterval of $(0,1)$ fails to be in $L^1$ ?","I know that the derivative of a differentiable function doesn't have to be continuous. How discontinuous can a derivative be? . Inspired by Limits and continuity of a derivative , I was thinking of defining the notion of pseudo-continuous: is pseudo-continuous at if And then I wanted to show that a function is the derivative of a differentiable function if and only if it is pseudo-continuous. But then I realized that the derivative doesn't have to be Lebesgue integrable, for example or Does there exist a differentiable function such that its derivative restricted to any subinterval of fails to be in ?","f:(a,b) \to \mathbb R x \in (a,b)  f(x) = \lim_{y\to x} \frac1{y-x} \int_x^y f(t) \, dt .  f(x) = \frac x{\log|x|} \sin\left(\frac1x\right) , \quad x \in (-\tfrac12,\tfrac12) ,  f(x) = x^2 \sin\left(\frac1{x^2}\right)  , f:(0,1) \to \mathbb R (0,1) L^1","['real-analysis', 'derivatives', 'lebesgue-integral']"
28,The set of Lipschitz continuous bounded functions is dense in the set of bounded uniformly continuous functions.,The set of Lipschitz continuous bounded functions is dense in the set of bounded uniformly continuous functions.,,"Let $(X,d)$ be a metric space. Let $f$ : $X$ $\rightarrow$ $\mathbb{R}$ be uniformly continuous and bounded. The method of attack is to consider the sequence of functions { ${f_n}$ } defined by $$ f_n(x) = \inf_{y\in X}\{f(y)+n\text{d}(x,y)\}.$$ It's quite easy to show that these are bounded and Lipschitz continuous. However, I cannot prove that $f_n \rightarrow f$ uniformly. I've looked at proving it by contradiction: Assuming $f_n$ doesn't converge to $f$ uniformly, there is an $\epsilon > 0$ such that, for all $N \in \mathbb{N}$ , there is a $t \in X$ and a $n \in \mathbb{N}$ with $n \geq N$ such that $$f(t)-f_n(t) \geq \epsilon.$$ Then we can construct a sequence of $\{t_k\}$ in $X$ such that for all $N \in \mathbb{N}$ ,  there exists a $n_k \in \mathbb{N}$ with $n_k \geq N$ where $f(t_k) - f_{n_k}(t_k) \geq \epsilon.$ I don't see a way to proceed now. Is there something here? I appreciate any help that you can provide.","Let be a metric space. Let : be uniformly continuous and bounded. The method of attack is to consider the sequence of functions { } defined by It's quite easy to show that these are bounded and Lipschitz continuous. However, I cannot prove that uniformly. I've looked at proving it by contradiction: Assuming doesn't converge to uniformly, there is an such that, for all , there is a and a with such that Then we can construct a sequence of in such that for all ,  there exists a with where I don't see a way to proceed now. Is there something here? I appreciate any help that you can provide.","(X,d) f X \rightarrow \mathbb{R} {f_n}  f_n(x) = \inf_{y\in X}\{f(y)+n\text{d}(x,y)\}. f_n \rightarrow f f_n f \epsilon > 0 N \in \mathbb{N} t \in X n \in \mathbb{N} n \geq N f(t)-f_n(t) \geq \epsilon. \{t_k\} X N \in \mathbb{N} n_k \in \mathbb{N} n_k \geq N f(t_k) - f_{n_k}(t_k) \geq \epsilon.",['real-analysis']
29,"Does there exist a continuous, open, and surjective map from $f\colon\mathbb{R}^n\to\mathbb{R}^m$ for $m>n$?","Does there exist a continuous, open, and surjective map from  for ?",f\colon\mathbb{R}^n\to\mathbb{R}^m m>n,"My question is that from above. Here are my approaches so far: I know that there is no homeomorphism between an open set of $\mathbb R^n$ and an open set of $\mathbb R^m$ . So if there is an open set where $ f $ is injective we get a contradiction. Furthermore, since $ f $ is surjective there are right-inverses of $ f $ . If there was a continuous right-inverse, we would also get a contradiction since this right-inverse would be a continuous injection from $ \mathbb R^m $ to $ \mathbb R^n $ which cannot exist by Borsuk-Ulam. Unfortunately, I was not able to use one of these two approaches to give an answer to my question. If the answer is yes, I would also be interested in stronger assumptions on $ f $ to make the answer no. I wonder if uniform continuity does the job, since for Hoelder continuity and large enough $ m $ the answer is no even if we drop the openness of $ f $ (This one can prove using Hausdorff-Dimension and how Hoelder continuous maps preserve them.) Thanks for your help!","My question is that from above. Here are my approaches so far: I know that there is no homeomorphism between an open set of and an open set of . So if there is an open set where is injective we get a contradiction. Furthermore, since is surjective there are right-inverses of . If there was a continuous right-inverse, we would also get a contradiction since this right-inverse would be a continuous injection from to which cannot exist by Borsuk-Ulam. Unfortunately, I was not able to use one of these two approaches to give an answer to my question. If the answer is yes, I would also be interested in stronger assumptions on to make the answer no. I wonder if uniform continuity does the job, since for Hoelder continuity and large enough the answer is no even if we drop the openness of (This one can prove using Hausdorff-Dimension and how Hoelder continuous maps preserve them.) Thanks for your help!",\mathbb R^n \mathbb R^m  f   f   f   \mathbb R^m   \mathbb R^n   f   m   f ,"['real-analysis', 'general-topology', 'continuity', 'open-map']"
30,Is there a continuous waveform that sounds the same as a square wave?,Is there a continuous waveform that sounds the same as a square wave?,,"The fourier series $$f(t)=\sum_{n\in\mathbb N\\n\text{ odd}}\frac1n\,\sin(nt)$$ converges to a square wave. Square waves are discontinuous functions. I'm wondering if there's a continuous function that ""sounds the same as"" a square wave, in the sense that it has components with the same amplitude and frequency as the series above, but with different phases. Do there exist $a_n\in\mathbb R$ such that $$\sum_{n\in\mathbb N\\n\text{ odd}}\frac1n\,\sin(nt+a_n)$$ converges (pointwise everywhere) to a continuous function?","The fourier series converges to a square wave. Square waves are discontinuous functions. I'm wondering if there's a continuous function that ""sounds the same as"" a square wave, in the sense that it has components with the same amplitude and frequency as the series above, but with different phases. Do there exist such that converges (pointwise everywhere) to a continuous function?","f(t)=\sum_{n\in\mathbb N\\n\text{ odd}}\frac1n\,\sin(nt) a_n\in\mathbb R \sum_{n\in\mathbb N\\n\text{ odd}}\frac1n\,\sin(nt+a_n)","['real-analysis', 'analysis', 'fourier-analysis', 'fourier-series']"
31,Evaluate $\lim_{n \to \infty} (n!)^{\frac{1}{n^2}}$,Evaluate,\lim_{n \to \infty} (n!)^{\frac{1}{n^2}},"Let $a_n = (n!)^{\frac{1}{n^2}}$ . Now, $$n! \geq1 \implies (n!)^{\frac{1}{n^2}} \geq 1$$ and $$n! \leq n^n \implies (n!)^{\frac{1}{n^2}} \leq n^{\frac1n}$$ But $$\lim_{n \to \infty} n^{\frac1n} = 1 = \lim_{n \to \infty} 1$$ Thus by Sandwich Theorem $$\lim_{n \to \infty} (n!)^{\frac{1}{n^2}} =1$$ Is the solution correct?","Let . Now, and But Thus by Sandwich Theorem Is the solution correct?",a_n = (n!)^{\frac{1}{n^2}} n! \geq1 \implies (n!)^{\frac{1}{n^2}} \geq 1 n! \leq n^n \implies (n!)^{\frac{1}{n^2}} \leq n^{\frac1n} \lim_{n \to \infty} n^{\frac1n} = 1 = \lim_{n \to \infty} 1 \lim_{n \to \infty} (n!)^{\frac{1}{n^2}} =1,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
32,Problem 30 from Shakarchi Stein's book,Problem 30 from Shakarchi Stein's book,,"If $E$ and $F$ are measurable sets with $m(E)>0$ and $m(F)>0$ . Prove that $E+F$ contains some interval. I know that this problem is very popular in MSE and I found many topics but most of the solutions use Fourier transform, Lebesgue density theorem which I am not familiar yet. I know the following two facts are true: If $\alpha\in(0,1)$ and $m_*(E)>0$ then exists an open interval $I$ such that $m_*(E\cap I)\geq \alpha m_*(I)$ . 2)If $E\subset  \mathbb{R}$ with $m(E)>0$ then $E-E$ contains some neighborhood of zero. My professor said that above problem could be solved via these problems. I was thinking on this problem about a week but no results. However, I have ideas: Let's $\alpha=\frac{9}{10}$ then exists open intervals $I$ and $J$ such that $m(E\cap I)\geq \frac{9}{10}m(I)$ and $m(F\cap J)\geq \frac{9}{10}m(J)$ . I had idea to shift one of the intervals, say $I+a\subset J$ WLOG. But this did not give any good results. Anyway, the idea if shifting may not work if $I=(-\infty,a)$ and $J=(b,+\infty)$ . I would be very grateful if somebody will show how to solve this problems using my ideas. And please do not close this topic because other topic on this problem have quite advanced solutions and I would like to see more simpler using the ideas which I provide.","If and are measurable sets with and . Prove that contains some interval. I know that this problem is very popular in MSE and I found many topics but most of the solutions use Fourier transform, Lebesgue density theorem which I am not familiar yet. I know the following two facts are true: If and then exists an open interval such that . 2)If with then contains some neighborhood of zero. My professor said that above problem could be solved via these problems. I was thinking on this problem about a week but no results. However, I have ideas: Let's then exists open intervals and such that and . I had idea to shift one of the intervals, say WLOG. But this did not give any good results. Anyway, the idea if shifting may not work if and . I would be very grateful if somebody will show how to solve this problems using my ideas. And please do not close this topic because other topic on this problem have quite advanced solutions and I would like to see more simpler using the ideas which I provide.","E F m(E)>0 m(F)>0 E+F \alpha\in(0,1) m_*(E)>0 I m_*(E\cap I)\geq \alpha m_*(I) E\subset  \mathbb{R} m(E)>0 E-E \alpha=\frac{9}{10} I J m(E\cap I)\geq \frac{9}{10}m(I) m(F\cap J)\geq \frac{9}{10}m(J) I+a\subset J I=(-\infty,a) J=(b,+\infty)","['real-analysis', 'measure-theory']"
33,Showing the existence of a strictly positive probability vector using a separation argument,Showing the existence of a strictly positive probability vector using a separation argument,,"Let $s = (s_1,...,s_n) \in (0,1)^n$. Suppose that $x$ is a real number and that $x \in (\min_j s_j, \max_j s_j)$. It's not difficult to show, by a direct construction, that There exists a probability vector $p = (p_1,...,p_n) \in (0,1)^n$ such that $s \cdot p = x$, where $s \cdot p$ is inner product, and $p$ is a probability vector if $\sum_j p_j = 1$. I'm wondering if this result can be demonstrated using a separating hyperplane argument. Here's the idea. Suppose for contradiction that no such $p \in (0,1)^n$ exists. Then, with $X = \{p \in (0,1)^n: p \ \text{a probability vector} \}$ and $Y = \{p: s \cdot p = x\}$, we have $X \cap Y = \emptyset$. Both sets $X$ and $Y$ are convex. So, by the separation theorem, there is a vector $q \in \mathbb{R}^n$ and $b \in \mathbb{R}$ such that $q \cdot p \leq b$ for all $p \in X$ and $q \cdot p \geq b$ for all $p \in Y$. I don't see a way to derive a contradiction from this, however, so any help here would be appreciated. I would also like to generalize this argument, if it can be made to work, to the countably infinite case, i.e. replacing $n$ with $\mathbb{N}$. Any suggestions about how to achieve that would also be appreciated.","Let $s = (s_1,...,s_n) \in (0,1)^n$. Suppose that $x$ is a real number and that $x \in (\min_j s_j, \max_j s_j)$. It's not difficult to show, by a direct construction, that There exists a probability vector $p = (p_1,...,p_n) \in (0,1)^n$ such that $s \cdot p = x$, where $s \cdot p$ is inner product, and $p$ is a probability vector if $\sum_j p_j = 1$. I'm wondering if this result can be demonstrated using a separating hyperplane argument. Here's the idea. Suppose for contradiction that no such $p \in (0,1)^n$ exists. Then, with $X = \{p \in (0,1)^n: p \ \text{a probability vector} \}$ and $Y = \{p: s \cdot p = x\}$, we have $X \cap Y = \emptyset$. Both sets $X$ and $Y$ are convex. So, by the separation theorem, there is a vector $q \in \mathbb{R}^n$ and $b \in \mathbb{R}$ such that $q \cdot p \leq b$ for all $p \in X$ and $q \cdot p \geq b$ for all $p \in Y$. I don't see a way to derive a contradiction from this, however, so any help here would be appreciated. I would also like to generalize this argument, if it can be made to work, to the countably infinite case, i.e. replacing $n$ with $\mathbb{N}$. Any suggestions about how to achieve that would also be appreciated.",,"['real-analysis', 'probability', 'convex-analysis']"
34,Schwartz functions dense in weighted $L^p$ space?,Schwartz functions dense in weighted  space?,L^p,"Upon learning about Schwartz functions, one result that is usually presented to students is that not only are Schwartz functions dense in $L^2(\mathbb{R}^n)$ but in $L^p(\mathbb{R}^n)$ for all $1 \le p < \infty$. Here, $L^p(\mathbb{R}^n)$ is the standard $L^p$ space with respect to Lebesgue measure. With how nicely Schwartz functions behave (with their smooth rapid decay), does this result extend to weighted $L^p$ spaces ? For instance, suppose we have a weight $w \in A_p$ ($1 < p < \infty$) where $A_p$ is the class of Muckenhoupt weights of parameter $p$. Let $L^p(w)$ be the space of Lebesgue measurable functions over $\mathbb{R}^n$ such that $$ \| f\|_{L^p(w)} = \left( \int_{\mathbb{R}^n} |f(x)|^p w(x) \,dx \right)^{1/p} < \infty$$ (so by taking $w=1$, we get back to standard $L^p$ space.) Can we say Schwartz functions are still dense in $L^p(w)$?","Upon learning about Schwartz functions, one result that is usually presented to students is that not only are Schwartz functions dense in $L^2(\mathbb{R}^n)$ but in $L^p(\mathbb{R}^n)$ for all $1 \le p < \infty$. Here, $L^p(\mathbb{R}^n)$ is the standard $L^p$ space with respect to Lebesgue measure. With how nicely Schwartz functions behave (with their smooth rapid decay), does this result extend to weighted $L^p$ spaces ? For instance, suppose we have a weight $w \in A_p$ ($1 < p < \infty$) where $A_p$ is the class of Muckenhoupt weights of parameter $p$. Let $L^p(w)$ be the space of Lebesgue measurable functions over $\mathbb{R}^n$ such that $$ \| f\|_{L^p(w)} = \left( \int_{\mathbb{R}^n} |f(x)|^p w(x) \,dx \right)^{1/p} < \infty$$ (so by taking $w=1$, we get back to standard $L^p$ space.) Can we say Schwartz functions are still dense in $L^p(w)$?",,"['real-analysis', 'harmonic-analysis']"
35,Uniform convergence and improper Riemann integral of the second kind,Uniform convergence and improper Riemann integral of the second kind,,"It is well  known that the property: If $f_n:A\to \mathbb{R}$ is Riemann integrable for all $n$ and $(f_n)$ converges uniformly to $f:A\to\mathbb{R}$, then $f$ is Riemann integrable. is false in general  when $A$ is of the form $[a,\infty)$. For example : $$ f_n(x) = \left\{     \begin{array}{ll}         1/x & \mbox{if } 1\le x\le n\\\         n/x^2 & \mbox{if }  x\ge  n     \end{array} \right. $$ What about the case $A=(a,b)$?  Can we still  construct a counter-example?","It is well  known that the property: If $f_n:A\to \mathbb{R}$ is Riemann integrable for all $n$ and $(f_n)$ converges uniformly to $f:A\to\mathbb{R}$, then $f$ is Riemann integrable. is false in general  when $A$ is of the form $[a,\infty)$. For example : $$ f_n(x) = \left\{     \begin{array}{ll}         1/x & \mbox{if } 1\le x\le n\\\         n/x^2 & \mbox{if }  x\ge  n     \end{array} \right. $$ What about the case $A=(a,b)$?  Can we still  construct a counter-example?",,"['real-analysis', 'improper-integrals', 'examples-counterexamples', 'uniform-convergence', 'riemann-integration']"
36,Behavior of a function defined by two infinite integrals around the singularity point,Behavior of a function defined by two infinite integrals around the singularity point,,"Bounty ending tomorrow: Consider the following function defined by two infinite integrals: $$ F(\epsilon) = \int_0^\infty  	\frac{q^3 (1+q)e^{-2q}}{q^4+\epsilon^4} \, \mathrm{d} q 	+  \int_0^\infty \frac{\, G_q \, e^{-q}}{q^2(q^4+\epsilon^4)} \, \mathrm{d} q  $$ where $$ G_q := G \left( \left[ \left[\frac{1}{2} \right] , [\,]\right] ,  		\left[ \left[ \frac{9}{2},\frac{5}{2} \right], \left[\frac{3}{2}\right] \right] ,\frac{q^2}{4}  \right) \, ,  $$ with $G$ being the Meijer G-function. The goal is to study the behavior of $F$ around the singularity point, i.e. as $\epsilon\to 0$. Numerically, it can clearly be observed that $F$ scales logarithmically with $\epsilon$. I am wondering whether this behavior can be shown analytically via a rigorous analysis, e.g. using perturbation techniques. Your help or hints are highly appreciated and are most welcome. Thanks H","Bounty ending tomorrow: Consider the following function defined by two infinite integrals: $$ F(\epsilon) = \int_0^\infty  	\frac{q^3 (1+q)e^{-2q}}{q^4+\epsilon^4} \, \mathrm{d} q 	+  \int_0^\infty \frac{\, G_q \, e^{-q}}{q^2(q^4+\epsilon^4)} \, \mathrm{d} q  $$ where $$ G_q := G \left( \left[ \left[\frac{1}{2} \right] , [\,]\right] ,  		\left[ \left[ \frac{9}{2},\frac{5}{2} \right], \left[\frac{3}{2}\right] \right] ,\frac{q^2}{4}  \right) \, ,  $$ with $G$ being the Meijer G-function. The goal is to study the behavior of $F$ around the singularity point, i.e. as $\epsilon\to 0$. Numerically, it can clearly be observed that $F$ scales logarithmically with $\epsilon$. I am wondering whether this behavior can be shown analytically via a rigorous analysis, e.g. using perturbation techniques. Your help or hints are highly appreciated and are most welcome. Thanks H",,"['real-analysis', 'integration', 'indefinite-integrals', 'contour-integration', 'complex-integration']"
37,Proof of inequality in Weierstrass approximation theorem proof through probability,Proof of inequality in Weierstrass approximation theorem proof through probability,,"I found this exercise in my probability theory book. The problem guides you through a proof of the Weierstrass Approximation Theorem through probability theory. My question is only about part b, so any help or hint is much appreciated. I can make the exercise, except for part $b$, and I show all my workings below. I understand that you might want to see my workings for $b$, but I have really no idea. First I quote Chebyshev's inequality, because apparently we need this. Thus: my question is only about part b, my work for the other parts can be found below. $\bf{Chebyshev's \ inequality}$ If $Y$ is a random variable and $\mathbb{E}(Y^2)<\infty$, then $$\mathbb{P}(|Y|\geq t)\leq \frac{\mathbb{E}(Y^2)}{t^2}, \quad \mathrm{for \ } t>0.$$ I quote the exercise: Let ($X_i$) be a sequence of i.i.d. Bernoulli$(p)$ variables, thus $\mathbb{P}(X_i=0)=1-p$ and $\mathbb{P}(X_i=1)=p$, for all $i\in\mathbb{N}$. a) Let $f$ be a continuous function of $[0,1]$ and prove that $$B_n(p)=\mathbb{E}\left(f\left(\frac{\sum_{i=1}^nX_i}{n}\right)\right)$$ is a polynomial in $p$ of degree at most $n$. b) Use Chebyshev's inequality to prove that for all $p$ such that $0\leq p\leq1$ and for any $\epsilon>0$, $$\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}\leq\frac{1}{4n\epsilon^2},$$ where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$. c) Using this and the fact that $f$ is bounded and uniformly continuous on $[0,1]$, prove the following version of the Weierstrass approximation theorem: $$\lim_{n\to\infty}\sup_{0\leq p\leq1}|f(p)-B_n(p)|=0.$$ For part $a$, if we let $Y_n:=\sum_{i=1}^nX_i$, then $Y_n\sim \mathrm{Bin}(n,p)$. Thus $$B_n(p)=\mathbb{E}\left(f\left(\frac{\sum_{i=1}^nX_i}{n}\right)\right)=\mathbb{E}\left(f\left(\frac{Y_n}{n}\right)\right)=\sum_{k=0}^n f\left(\frac{k}{n}\right)\binom{n}{k}p^k(1-p)^k$$ is a polynomial in $p$ of degree at most $n$. This follows directly from just writing out the expectation of the binomial distribution. For part $c$, $f$ is continuous on the closed interval $[0,1]$, so $f$ is uniformly continuous and bounded on $[0,1]$, and assumes it's minimum and maximum on this interval. These are all basic real analysis results. Since $f$ is bounded on $[0,1]$, there exists an $M\in\mathbb{R}$ such that $|f(p)|\leq M$, for all $p\in[0,1]$. From part $b$, we know that $0\leq p\leq1$ and for any $\epsilon>0$, $$\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}=\mathbb{P}(|k/n-p|\geq \epsilon)\leq\frac{1}{4n\epsilon^2},$$ where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$. Thus we obtain the following: In equation 1, we take the limit of the result of part b; equation 2 follows because of uniform continuity of $f$; equation 4 follows because of linearity of expectation, part $a$ and because $f(p)$ is fixed; equation 5 follows from the partition theorem for expectations:  $$\begin{align}  &\lim_{n\to\infty}\mathbb{P}(|\frac{k}{n}-p|\geq\epsilon) = 0 & (1)\\  &\lim_{n\to\infty}\mathbb{P}\left(|f\left(\frac{k}{n}\right)-f(p)|\geq\epsilon\right) = 0 & (2)\\  &\lim_{n\to\infty}\mathbb{E}\left(|f\left(\frac{k}{n}\right)-f(p)|\right)  = & (3)\\ &\lim_{n\to\infty}|B_n(p)-f(p)|=& (4)\\   &\lim_{n\to\infty}\mathbb{E}\left(|f\left(k/n\right)-f(p)|\Bigg|k/n-p|\geq\epsilon\right)\cdot\mathbb{P}(|k/n-p|\geq\epsilon)+& \\ &\lim_{n\to\infty}\mathbb{E}\left(|f\left(k/n\right)-f(p)|\Bigg|\frac{k}{n}-p|<\epsilon\right)\cdot\mathbb{P}(|k/n-p|<\epsilon)=& (5)\\  & 0+0=0.& (6)\\ \end{align}$$ The first term is zero because $\mathbb{P}(|\frac{k}{n}-p|\geq\epsilon)$ goes to zero as $n$ goes to $\infty$, as shown in part $b$, and because the boundedness of $f$. The second term goes to zero because: (i) $f(k/n)-f(p)$ goes to zero as $n$ goes to infinity because $f$ is uniformly continuous, because $\lim_{n\to\infty}k/n=p$ because of the law of large numbers (remember that $k$ denotes the number of successes in a $\mathrm{Bin}(n,p)$ experiment); and (ii) because $\mathbb{P}(|k/n-p|<\epsilon)$ is bounded by definition of a probability measure. Thus $\lim_{n\to\infty}|B_n(p)-f(p)|=0$, for any $p\in[0,1]$, which is the interval where $f$ is uniformly continuous, and thus $\lim_{n\to\infty}\sup_{0\leq p\leq1}|f(p)-B_n(p)|=0$. Thus any continuous function on $[0,1]$ can be uniformly approximated by polyinomials on $[0,1]$. Obvious rescaling proves the Weierstrass Approximation Theorem. EDIT : Should I use for part $b$ that $\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}$, where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$ equals $$\begin{eqnarray}\mathbb{P}(|k/n-p|\geq \epsilon) & = & \mathbb{P}(|k-np|\geq \epsilon n)\\&=& \mathbb{P}(|k-\mathbb{E}(Y_n)\tag{with $Y_n\sim\mathrm{Bin}(n,p)$}|\geq \epsilon n)\\ & \leq & \frac{1}{\epsilon^2n^2}\mathbb{E}((k-np)^2)\tag{Chebyshev's inequality}\\ &\leq& \frac{1}{\epsilon^2n^2}(\frac12\cdot\frac12\cdot n) \tag{variance is maximized if $p=\frac12$}\\&=& \frac{1}{4n\epsilon^2}\end{eqnarray}$$ I think this is correct, after posting I came a lot further through reading on wikipedia and the like.","I found this exercise in my probability theory book. The problem guides you through a proof of the Weierstrass Approximation Theorem through probability theory. My question is only about part b, so any help or hint is much appreciated. I can make the exercise, except for part $b$, and I show all my workings below. I understand that you might want to see my workings for $b$, but I have really no idea. First I quote Chebyshev's inequality, because apparently we need this. Thus: my question is only about part b, my work for the other parts can be found below. $\bf{Chebyshev's \ inequality}$ If $Y$ is a random variable and $\mathbb{E}(Y^2)<\infty$, then $$\mathbb{P}(|Y|\geq t)\leq \frac{\mathbb{E}(Y^2)}{t^2}, \quad \mathrm{for \ } t>0.$$ I quote the exercise: Let ($X_i$) be a sequence of i.i.d. Bernoulli$(p)$ variables, thus $\mathbb{P}(X_i=0)=1-p$ and $\mathbb{P}(X_i=1)=p$, for all $i\in\mathbb{N}$. a) Let $f$ be a continuous function of $[0,1]$ and prove that $$B_n(p)=\mathbb{E}\left(f\left(\frac{\sum_{i=1}^nX_i}{n}\right)\right)$$ is a polynomial in $p$ of degree at most $n$. b) Use Chebyshev's inequality to prove that for all $p$ such that $0\leq p\leq1$ and for any $\epsilon>0$, $$\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}\leq\frac{1}{4n\epsilon^2},$$ where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$. c) Using this and the fact that $f$ is bounded and uniformly continuous on $[0,1]$, prove the following version of the Weierstrass approximation theorem: $$\lim_{n\to\infty}\sup_{0\leq p\leq1}|f(p)-B_n(p)|=0.$$ For part $a$, if we let $Y_n:=\sum_{i=1}^nX_i$, then $Y_n\sim \mathrm{Bin}(n,p)$. Thus $$B_n(p)=\mathbb{E}\left(f\left(\frac{\sum_{i=1}^nX_i}{n}\right)\right)=\mathbb{E}\left(f\left(\frac{Y_n}{n}\right)\right)=\sum_{k=0}^n f\left(\frac{k}{n}\right)\binom{n}{k}p^k(1-p)^k$$ is a polynomial in $p$ of degree at most $n$. This follows directly from just writing out the expectation of the binomial distribution. For part $c$, $f$ is continuous on the closed interval $[0,1]$, so $f$ is uniformly continuous and bounded on $[0,1]$, and assumes it's minimum and maximum on this interval. These are all basic real analysis results. Since $f$ is bounded on $[0,1]$, there exists an $M\in\mathbb{R}$ such that $|f(p)|\leq M$, for all $p\in[0,1]$. From part $b$, we know that $0\leq p\leq1$ and for any $\epsilon>0$, $$\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}=\mathbb{P}(|k/n-p|\geq \epsilon)\leq\frac{1}{4n\epsilon^2},$$ where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$. Thus we obtain the following: In equation 1, we take the limit of the result of part b; equation 2 follows because of uniform continuity of $f$; equation 4 follows because of linearity of expectation, part $a$ and because $f(p)$ is fixed; equation 5 follows from the partition theorem for expectations:  $$\begin{align}  &\lim_{n\to\infty}\mathbb{P}(|\frac{k}{n}-p|\geq\epsilon) = 0 & (1)\\  &\lim_{n\to\infty}\mathbb{P}\left(|f\left(\frac{k}{n}\right)-f(p)|\geq\epsilon\right) = 0 & (2)\\  &\lim_{n\to\infty}\mathbb{E}\left(|f\left(\frac{k}{n}\right)-f(p)|\right)  = & (3)\\ &\lim_{n\to\infty}|B_n(p)-f(p)|=& (4)\\   &\lim_{n\to\infty}\mathbb{E}\left(|f\left(k/n\right)-f(p)|\Bigg|k/n-p|\geq\epsilon\right)\cdot\mathbb{P}(|k/n-p|\geq\epsilon)+& \\ &\lim_{n\to\infty}\mathbb{E}\left(|f\left(k/n\right)-f(p)|\Bigg|\frac{k}{n}-p|<\epsilon\right)\cdot\mathbb{P}(|k/n-p|<\epsilon)=& (5)\\  & 0+0=0.& (6)\\ \end{align}$$ The first term is zero because $\mathbb{P}(|\frac{k}{n}-p|\geq\epsilon)$ goes to zero as $n$ goes to $\infty$, as shown in part $b$, and because the boundedness of $f$. The second term goes to zero because: (i) $f(k/n)-f(p)$ goes to zero as $n$ goes to infinity because $f$ is uniformly continuous, because $\lim_{n\to\infty}k/n=p$ because of the law of large numbers (remember that $k$ denotes the number of successes in a $\mathrm{Bin}(n,p)$ experiment); and (ii) because $\mathbb{P}(|k/n-p|<\epsilon)$ is bounded by definition of a probability measure. Thus $\lim_{n\to\infty}|B_n(p)-f(p)|=0$, for any $p\in[0,1]$, which is the interval where $f$ is uniformly continuous, and thus $\lim_{n\to\infty}\sup_{0\leq p\leq1}|f(p)-B_n(p)|=0$. Thus any continuous function on $[0,1]$ can be uniformly approximated by polyinomials on $[0,1]$. Obvious rescaling proves the Weierstrass Approximation Theorem. EDIT : Should I use for part $b$ that $\sum_{k\in K}\binom{n}{k}p^k(1-p)^{n-k}$, where $K=\{k:0\leq k\leq n, |k/n-p|>\epsilon\}$ equals $$\begin{eqnarray}\mathbb{P}(|k/n-p|\geq \epsilon) & = & \mathbb{P}(|k-np|\geq \epsilon n)\\&=& \mathbb{P}(|k-\mathbb{E}(Y_n)\tag{with $Y_n\sim\mathrm{Bin}(n,p)$}|\geq \epsilon n)\\ & \leq & \frac{1}{\epsilon^2n^2}\mathbb{E}((k-np)^2)\tag{Chebyshev's inequality}\\ &\leq& \frac{1}{\epsilon^2n^2}(\frac12\cdot\frac12\cdot n) \tag{variance is maximized if $p=\frac12$}\\&=& \frac{1}{4n\epsilon^2}\end{eqnarray}$$ I think this is correct, after posting I came a lot further through reading on wikipedia and the like.",,"['real-analysis', 'probability', 'weierstrass-approximation']"
38,Do we really need axioms to define order in $\mathbb R?$,Do we really need axioms to define order in,\mathbb R?,"I apologize if this question appears to be a dumb one. However given my preliminary knowledge in real analysis, I am unable to resolve the issue; its about the order axiom (of reals). It can be shown that algebraic properties of reals follows from field axioms that work as a premise for further reasoning. Since order properties of reals cannot (?) be shown from algebraic properties, we assume the existence of a positive set based on which we can define order of reals. Thats the way Bartle-Sherbert text introduced the concepts of order (of reals). Image: Bartle-Sherbert Introduction to Order But why do we just assume the existence of a positive set the way we assume an axiom? Cant we really define a positive set $P$ based on algebraic properties as: $P=\{y^2:y\in\mathbb R, y\ne 0\}$. For then we could define order relation $<$ as: $x<y$ iff $y  x\in P$. Based on this observation, do we really need extra (order) axioms, whereas we could define order based on algebraic properties?","I apologize if this question appears to be a dumb one. However given my preliminary knowledge in real analysis, I am unable to resolve the issue; its about the order axiom (of reals). It can be shown that algebraic properties of reals follows from field axioms that work as a premise for further reasoning. Since order properties of reals cannot (?) be shown from algebraic properties, we assume the existence of a positive set based on which we can define order of reals. Thats the way Bartle-Sherbert text introduced the concepts of order (of reals). Image: Bartle-Sherbert Introduction to Order But why do we just assume the existence of a positive set the way we assume an axiom? Cant we really define a positive set $P$ based on algebraic properties as: $P=\{y^2:y\in\mathbb R, y\ne 0\}$. For then we could define order relation $<$ as: $x<y$ iff $y  x\in P$. Based on this observation, do we really need extra (order) axioms, whereas we could define order based on algebraic properties?",,['real-analysis']
39,Equivalent definitions of $C^1-$boundary,Equivalent definitions of boundary,C^1-,"I am studying PDE, and I have two definition of $C^1$ open set as follow: Definition 1. (Evans' PDE book) An open set $\Omega \subset \mathbb{R}^N$ is $C^1$ if for each point $x_0 \in \partial \Omega$, there exist $r > 0$ and a $C^1$ function $\gamma: \mathbb{R}^{N-1} \to \mathbb{R}$ such that - upon relabeling and reorienting the coordinates axes if necessary - we have $$\Omega \cap B(x_0,r) = \left\{ x \in B(x_0,r): x_N > \gamma(x_1,...,x_{N-1}) \right\}.$$ Definition 2. (I rewrote it partly based on Brezis' book and Trudinger's book) An open set $\Omega \subset \mathbb{R}^N$ is $C^1$ if for every $x_0 \in \partial \Omega$, there is an neighborhood $U \subset \mathbb{R}^N$ of $x_0$ and a $C^1-$diffeomorphism $\varphi: U \to B(0,1)$ such that $$\varphi(U \cap \Omega) = B(0,1) \cap \{ y_n > 0\}, \varphi(U\cap \partial \Omega) = B(0,1) \cap \{ y_n = 0 \}.$$ The question is: Are two definitions actually equivalent?","I am studying PDE, and I have two definition of $C^1$ open set as follow: Definition 1. (Evans' PDE book) An open set $\Omega \subset \mathbb{R}^N$ is $C^1$ if for each point $x_0 \in \partial \Omega$, there exist $r > 0$ and a $C^1$ function $\gamma: \mathbb{R}^{N-1} \to \mathbb{R}$ such that - upon relabeling and reorienting the coordinates axes if necessary - we have $$\Omega \cap B(x_0,r) = \left\{ x \in B(x_0,r): x_N > \gamma(x_1,...,x_{N-1}) \right\}.$$ Definition 2. (I rewrote it partly based on Brezis' book and Trudinger's book) An open set $\Omega \subset \mathbb{R}^N$ is $C^1$ if for every $x_0 \in \partial \Omega$, there is an neighborhood $U \subset \mathbb{R}^N$ of $x_0$ and a $C^1-$diffeomorphism $\varphi: U \to B(0,1)$ such that $$\varphi(U \cap \Omega) = B(0,1) \cap \{ y_n > 0\}, \varphi(U\cap \partial \Omega) = B(0,1) \cap \{ y_n = 0 \}.$$ The question is: Are two definitions actually equivalent?",,"['real-analysis', 'functional-analysis', 'partial-differential-equations']"
40,Why is this a safe neighborhood for uniform convergence of the sequence implicitly defining a function?,Why is this a safe neighborhood for uniform convergence of the sequence implicitly defining a function?,,"This discussion pertains to Theorem III-1.4 in C.H. Edwards, Jr.'s Advanced Calculus of Several Variables. I am trying to understand why the neighborhood described in the proof of the implicit function theorem is ""safe"" for convergence of the contraction mapping with the inductive hypothesis for the sequence : $f_{n+1}[x]=f_{n}[x]-\frac{G[x,f_{n}[x]]}{D_{2}G[a,b]}$, $f_{0}[x]=b$. The neighborhood is given as follows: $\varepsilon\ge\lvert x-a\rvert\bigwedge\varepsilon\ge\lvert y-b\rvert\implies\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert\le\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ $\varepsilon>\delta\ge\lvert x-a\rvert\implies\lvert G[x,b]\rvert\le\frac{1}{2}\varepsilon\lvert D_{2}G[a,b]\rvert$ The graph depicts the trace on the surface $\{x,y,G[x,y]\}$ of the square $[a-\varepsilon,a+\varepsilon]\times[b-\varepsilon,b+\varepsilon]$ in green. The further $\delta$ restriction is indicated in magenta, as is the trace of the line $y=b$. The trace of $x=a$ is blue. The white angle represents the projection of the point $\{a,b+\frac{\varepsilon}{2},\frac{\varepsilon}{2}D_{2}G[a,b]\}$. $\frac{\varepsilon}{2}D_{2}G[a,b]$ also gives the height of the upper plane. The actual solution curve $\{x,f[x],0\}$ is the intersection of the lower plane and the surface $\{x,y,G[x,y]\}$. The red parallelogram represents the condition $G[a,y]=2(y-b)D_{2}G[a,y]$ which leads to an infinite loop. The choice of $\varepsilon$ prevents that from happening on the blue curve. This is explained in my answer to a previous question: Neighborhoods necessary for convergence of a sequence. What I am trying to determine is why the rectangle $[a-\delta,a+\delta]\times[b-\varepsilon,b+\varepsilon]$ is safe for convergence for all $x\in[a-\delta,a+\delta]$. In other words, how it avoids the infinite loop condition. A second diagram shows a simpler depiction of the problem using a different $G[x,y]$. Edit to add: I failed to mention a potentially significant assertion regarding the second image.  The rectangle in that image was chosen so that the trace of the line segment $G[{c_1,d_1},{c_2,d_1}]$ is completely below the plane $z=0$, and $G[{c_1,d_2},{c_2,d_2}]$ is completely above the surface of $z=0$. $D_2[x,y]>0$ holds everywhere in the rectangle $[c_1,c_2]\times[d_1,d_2]$. The objective is to show that $f_{n+1}[x_*]=f_{n}[x_*]-\frac{G[x_*,f_{n}[x_*]]}{D_{2}G[a,b]}$ converges to a point on the solution curve for all $x_*$ in some neighborhood within the rectangle $[c_1,c_2]\times[d_1,d_2]$.  The neighborhood described above would lie within that rectangle. Edit to add: Edit to modify: I observe that the portions of the green curve with constant $x$ never intersect the plane $z=0$ in the neighborhood, so it is right to exclude them.  Though the magenta curves do intersect the solution curve, that is not a guarantee that they would do so with a different $G[x,y]$. Edit to add: I also note that the hypotheses of the theorem only stipulate $G$ is $\mathscr{C}^1$. That makes talking about inflection points difficult.  But they seem relevant. It seems proper to examine what happens at the extreme conditions such as: $D_2G[x_*,y]=\frac{1}{2}D_2G[a,b]$, $D_2G[x_*,y]=\frac{3}{2}D_2G[a,b]$ and $\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert=\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ I've removed my own speculations about those conditions because I believe they were too restrictive (IOW, wrong). Edit to add: Another observation that seems significant is that all sequences begin with the base state $f_0[x]=b$. I'm pretty sure that means all subsequent $f_n[x]$ will be on the same side of the plane $y=b$ as $f_1[x]$. Unfortunately I don't have a good exact vocabulary, nor theory of this problem domain.  IOW, I'm hand-waving it.  All I'm seeking at this point is an intuitively satisfying geometric explanation. Edit to add: A third graphic showing a ""side view"" of the situation, with the $x$ dimension suppressed.  By collapsing the problem into the consideration of a family of curves in two dimensions which adhere to the stated conditions, the problem might reduce to one of real analysis. The yellow lines depict the range of allowable slopes for any curve in the neighborhood, determined by $\varepsilon\ge\lvert x-a\rvert\bigwedge\varepsilon\ge\lvert y-b\rvert\implies\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert\le\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ The blue curve and tangent line represent a safe state.  The red parallelogram and chord depict a loop state and the slope of twice the loop tangent, respectively.  The green vertical lines represent $y=\pm\varepsilon$.  The vertical magenta line segment represents the restriction: $\varepsilon>\delta\ge\lvert x-a\rvert\implies\lvert G[x,b]\rvert\le\frac{1}{2}\varepsilon\lvert D_{2}G[a,b]\rvert$. That is, (I believe) every 2-dimensional projection of a curve must intersect the magenta line segment.  Furthermore, the base term of every sequence is $f_0[x]=b$.  The first iteration is therefore:  $f_{1}[x]=b-\frac{G[x,b]}{D_{2}G[a,b]}$.","This discussion pertains to Theorem III-1.4 in C.H. Edwards, Jr.'s Advanced Calculus of Several Variables. I am trying to understand why the neighborhood described in the proof of the implicit function theorem is ""safe"" for convergence of the contraction mapping with the inductive hypothesis for the sequence : $f_{n+1}[x]=f_{n}[x]-\frac{G[x,f_{n}[x]]}{D_{2}G[a,b]}$, $f_{0}[x]=b$. The neighborhood is given as follows: $\varepsilon\ge\lvert x-a\rvert\bigwedge\varepsilon\ge\lvert y-b\rvert\implies\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert\le\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ $\varepsilon>\delta\ge\lvert x-a\rvert\implies\lvert G[x,b]\rvert\le\frac{1}{2}\varepsilon\lvert D_{2}G[a,b]\rvert$ The graph depicts the trace on the surface $\{x,y,G[x,y]\}$ of the square $[a-\varepsilon,a+\varepsilon]\times[b-\varepsilon,b+\varepsilon]$ in green. The further $\delta$ restriction is indicated in magenta, as is the trace of the line $y=b$. The trace of $x=a$ is blue. The white angle represents the projection of the point $\{a,b+\frac{\varepsilon}{2},\frac{\varepsilon}{2}D_{2}G[a,b]\}$. $\frac{\varepsilon}{2}D_{2}G[a,b]$ also gives the height of the upper plane. The actual solution curve $\{x,f[x],0\}$ is the intersection of the lower plane and the surface $\{x,y,G[x,y]\}$. The red parallelogram represents the condition $G[a,y]=2(y-b)D_{2}G[a,y]$ which leads to an infinite loop. The choice of $\varepsilon$ prevents that from happening on the blue curve. This is explained in my answer to a previous question: Neighborhoods necessary for convergence of a sequence. What I am trying to determine is why the rectangle $[a-\delta,a+\delta]\times[b-\varepsilon,b+\varepsilon]$ is safe for convergence for all $x\in[a-\delta,a+\delta]$. In other words, how it avoids the infinite loop condition. A second diagram shows a simpler depiction of the problem using a different $G[x,y]$. Edit to add: I failed to mention a potentially significant assertion regarding the second image.  The rectangle in that image was chosen so that the trace of the line segment $G[{c_1,d_1},{c_2,d_1}]$ is completely below the plane $z=0$, and $G[{c_1,d_2},{c_2,d_2}]$ is completely above the surface of $z=0$. $D_2[x,y]>0$ holds everywhere in the rectangle $[c_1,c_2]\times[d_1,d_2]$. The objective is to show that $f_{n+1}[x_*]=f_{n}[x_*]-\frac{G[x_*,f_{n}[x_*]]}{D_{2}G[a,b]}$ converges to a point on the solution curve for all $x_*$ in some neighborhood within the rectangle $[c_1,c_2]\times[d_1,d_2]$.  The neighborhood described above would lie within that rectangle. Edit to add: Edit to modify: I observe that the portions of the green curve with constant $x$ never intersect the plane $z=0$ in the neighborhood, so it is right to exclude them.  Though the magenta curves do intersect the solution curve, that is not a guarantee that they would do so with a different $G[x,y]$. Edit to add: I also note that the hypotheses of the theorem only stipulate $G$ is $\mathscr{C}^1$. That makes talking about inflection points difficult.  But they seem relevant. It seems proper to examine what happens at the extreme conditions such as: $D_2G[x_*,y]=\frac{1}{2}D_2G[a,b]$, $D_2G[x_*,y]=\frac{3}{2}D_2G[a,b]$ and $\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert=\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ I've removed my own speculations about those conditions because I believe they were too restrictive (IOW, wrong). Edit to add: Another observation that seems significant is that all sequences begin with the base state $f_0[x]=b$. I'm pretty sure that means all subsequent $f_n[x]$ will be on the same side of the plane $y=b$ as $f_1[x]$. Unfortunately I don't have a good exact vocabulary, nor theory of this problem domain.  IOW, I'm hand-waving it.  All I'm seeking at this point is an intuitively satisfying geometric explanation. Edit to add: A third graphic showing a ""side view"" of the situation, with the $x$ dimension suppressed.  By collapsing the problem into the consideration of a family of curves in two dimensions which adhere to the stated conditions, the problem might reduce to one of real analysis. The yellow lines depict the range of allowable slopes for any curve in the neighborhood, determined by $\varepsilon\ge\lvert x-a\rvert\bigwedge\varepsilon\ge\lvert y-b\rvert\implies\lvert D_{2}G[x,y]-D_{2}G[a,b]\rvert\le\frac{1}{2}\lvert D_{2}G[a,b]\rvert$ The blue curve and tangent line represent a safe state.  The red parallelogram and chord depict a loop state and the slope of twice the loop tangent, respectively.  The green vertical lines represent $y=\pm\varepsilon$.  The vertical magenta line segment represents the restriction: $\varepsilon>\delta\ge\lvert x-a\rvert\implies\lvert G[x,b]\rvert\le\frac{1}{2}\varepsilon\lvert D_{2}G[a,b]\rvert$. That is, (I believe) every 2-dimensional projection of a curve must intersect the magenta line segment.  Furthermore, the base term of every sequence is $f_0[x]=b$.  The first iteration is therefore:  $f_{1}[x]=b-\frac{G[x,b]}{D_{2}G[a,b]}$.",,"['real-analysis', 'sequences-and-series', 'multivariable-calculus', 'differential-topology', 'implicit-function-theorem']"
41,Distributional derivative of $\frac{e^{i|x|}}{|x|}$,Distributional derivative of,\frac{e^{i|x|}}{|x|},For $x\in\mathbb{R}^3$ we have $\frac{1}{|x|} \in L^1_{loc}(\mathbb{R}^3)$ and $\frac{1}{|x|} \to 0$ as $|x| \to \infty$. Hence we can conclude that $$ f(x)=\frac{e^{i|x|}}{|x|} $$ defines a (tempered) distribution $$ \phi \mapsto \int_{\mathbb{R}^3}f\phi. $$ For a distribution $T \in \mathcal{S}'(\mathbb{R}^3)$ its derivative is defined as $(\partial x_i T)\phi = -T(\partial x_i \phi)$. Now I want to calculate the partial distributional derivative of $f$. Starting with the definition yields $$ -\int_{\mathbb{R}^3}\frac{e^{i|x|}}{|x|}\partial x_i \phi(x) dx $$ But how do I proceed? Since $f$ has a singularitie in $0$ there could somewhere appear a delta function.,For $x\in\mathbb{R}^3$ we have $\frac{1}{|x|} \in L^1_{loc}(\mathbb{R}^3)$ and $\frac{1}{|x|} \to 0$ as $|x| \to \infty$. Hence we can conclude that $$ f(x)=\frac{e^{i|x|}}{|x|} $$ defines a (tempered) distribution $$ \phi \mapsto \int_{\mathbb{R}^3}f\phi. $$ For a distribution $T \in \mathcal{S}'(\mathbb{R}^3)$ its derivative is defined as $(\partial x_i T)\phi = -T(\partial x_i \phi)$. Now I want to calculate the partial distributional derivative of $f$. Starting with the definition yields $$ -\int_{\mathbb{R}^3}\frac{e^{i|x|}}{|x|}\partial x_i \phi(x) dx $$ But how do I proceed? Since $f$ has a singularitie in $0$ there could somewhere appear a delta function.,,"['real-analysis', 'integration']"
42,$\phi \mapsto \lim_{\epsilon \to 0} \int_{\mathbb{R}^n} \frac{\phi (x)}{\left \Vert x \right \Vert-1 + i\epsilon}$ is a distribution,is a distribution,\phi \mapsto \lim_{\epsilon \to 0} \int_{\mathbb{R}^n} \frac{\phi (x)}{\left \Vert x \right \Vert-1 + i\epsilon},"I want to show that $$\phi \mapsto \lim_{\epsilon \to 0} \int_{\mathbb{R}^n} \frac{\phi (x)}{\left \Vert x \right \Vert-1 + i\epsilon}$$ is a distribution for $x \in \mathbb{R}^n$, $\phi \in \mathcal{S}(\mathbb{R}^n)$ in the Schwartz space and $\left \Vert \cdot \right \Vert$ the euclidean norm. I think that for the one dimensional case this should follow from the theory of Cauchy principle values but I'm not sure.","I want to show that $$\phi \mapsto \lim_{\epsilon \to 0} \int_{\mathbb{R}^n} \frac{\phi (x)}{\left \Vert x \right \Vert-1 + i\epsilon}$$ is a distribution for $x \in \mathbb{R}^n$, $\phi \in \mathcal{S}(\mathbb{R}^n)$ in the Schwartz space and $\left \Vert \cdot \right \Vert$ the euclidean norm. I think that for the one dimensional case this should follow from the theory of Cauchy principle values but I'm not sure.",,"['real-analysis', 'functional-analysis']"
43,Sign of integral of $\frac{ 2 ^{\frac{it}{2/3}} \Gamma ( \frac{it +1}{2/3}) }{ 2 ^{\frac{it}{1.5}} \Gamma ( \frac{it +1}{1.5}) } \frac{1}{(a+it)^k}$,Sign of integral of,\frac{ 2 ^{\frac{it}{2/3}} \Gamma ( \frac{it +1}{2/3}) }{ 2 ^{\frac{it}{1.5}} \Gamma ( \frac{it +1}{1.5}) } \frac{1}{(a+it)^k},"Can we determine the sign of the following function \begin{align} f(a,k)=\int_{-\infty}^\infty  \frac{ 2 ^{\frac{it}{2/3}} \Gamma \left( \frac{it +1}{2/3}\right)  }{  2 ^{\frac{it}{3/2}}  \Gamma \left( \frac{it +1}{3/2}\right) }   \frac{1}{(a+it)^k} dt, \end{align}  where $a\neq 0$ and $k \ge 1$ is some positive integer. The conjecture is that the sign of the integral is equal to  \begin{align} {\rm sign } (f(a,k))={\rm sign}(a)^k. \end{align} Perhpas the following limit can be usefull. By using a  method  in this question it is not difficult to see that \begin{align} \left |  \frac{ 2 ^{\frac{it}{2/3}} \Gamma \left( \frac{it +1}{2/3}\right)  }{  2 ^{\frac{it}{3/2}}  \Gamma \left( \frac{it +1}{3/2}\right) }  \right| \to O( e^{- (\frac{3}{2}-\frac{2}{3}) t}) \text{ as } t \to \infty. \end{align} Thanks","Can we determine the sign of the following function \begin{align} f(a,k)=\int_{-\infty}^\infty  \frac{ 2 ^{\frac{it}{2/3}} \Gamma \left( \frac{it +1}{2/3}\right)  }{  2 ^{\frac{it}{3/2}}  \Gamma \left( \frac{it +1}{3/2}\right) }   \frac{1}{(a+it)^k} dt, \end{align}  where $a\neq 0$ and $k \ge 1$ is some positive integer. The conjecture is that the sign of the integral is equal to  \begin{align} {\rm sign } (f(a,k))={\rm sign}(a)^k. \end{align} Perhpas the following limit can be usefull. By using a  method  in this question it is not difficult to see that \begin{align} \left |  \frac{ 2 ^{\frac{it}{2/3}} \Gamma \left( \frac{it +1}{2/3}\right)  }{  2 ^{\frac{it}{3/2}}  \Gamma \left( \frac{it +1}{3/2}\right) }  \right| \to O( e^{- (\frac{3}{2}-\frac{2}{3}) t}) \text{ as } t \to \infty. \end{align} Thanks",,"['real-analysis', 'integration', 'complex-analysis', 'gamma-function']"
44,How do you prove this very different method for evaluating $\sum_{k=1}^nk^p$,How do you prove this very different method for evaluating,\sum_{k=1}^nk^p,"I found the following formula in my previous question .  This differs from my previous question in that I want an alternative proof of the below recursive formula for calculating $\displaystyle\sum_{k=1}^nk^p$. Suppose I had a function recursively defined as $$f(x,p)=a_px+p\int_0^xf(t,p-1)dt$$ $$a_p=1-p\int_0^1f(t,p-1)dt$$ For $p\in\mathbb N$.  For $p=0$, we trivially get $f(x,0)=x$, which shall be our initial condition. It can then be noticed that $$a_1=1-\int_0^1tdt=\frac12$$ $$f(x,1)=\frac12x+\int_0^xtdt=\frac12x+\frac12x^2$$ $$a_2=1-2\int_0^1\frac12t+\frac12t^2dt=\frac16$$ $$f(x,2)=\frac16x+\int_0^x\frac12t+\frac12t^2dt=\frac16x+\frac14x^2+\frac16x^3$$ And the general pattern is $f(x,p)=\sum_{k=1}^xk^p$ whenever $x\in\mathbb N\quad(?)$ How do I prove that whenever $x\in\mathbb N$ $$f(x,p)=\sum_{k=1}^xk^p$$ without applying the methods mentioned in the link above?","I found the following formula in my previous question .  This differs from my previous question in that I want an alternative proof of the below recursive formula for calculating $\displaystyle\sum_{k=1}^nk^p$. Suppose I had a function recursively defined as $$f(x,p)=a_px+p\int_0^xf(t,p-1)dt$$ $$a_p=1-p\int_0^1f(t,p-1)dt$$ For $p\in\mathbb N$.  For $p=0$, we trivially get $f(x,0)=x$, which shall be our initial condition. It can then be noticed that $$a_1=1-\int_0^1tdt=\frac12$$ $$f(x,1)=\frac12x+\int_0^xtdt=\frac12x+\frac12x^2$$ $$a_2=1-2\int_0^1\frac12t+\frac12t^2dt=\frac16$$ $$f(x,2)=\frac16x+\int_0^x\frac12t+\frac12t^2dt=\frac16x+\frac14x^2+\frac16x^3$$ And the general pattern is $f(x,p)=\sum_{k=1}^xk^p$ whenever $x\in\mathbb N\quad(?)$ How do I prove that whenever $x\in\mathbb N$ $$f(x,p)=\sum_{k=1}^xk^p$$ without applying the methods mentioned in the link above?",,"['real-analysis', 'definite-integrals', 'summation', 'recurrence-relations', 'alternative-proof']"
45,"How do i show that : for $x, y >0 ,\log (x+y)=\log{x}\cdot \log{y}$ has no integer solutions?",How do i show that : for  has no integer solutions?,"x, y >0 ,\log (x+y)=\log{x}\cdot \log{y}","I have tried to solve this equation for $x, y >0$, $\log(x+y)=\log{x}\cdot \log{y}$ by writing it as this $x+y={x}^{\log{y}}$. But the last expression is complicated to me to get any solutions, I think it has no solution in integers !!! My question here is : How do I show that for $x, y >0$,  $\log(x+y)=\log{x} \cdot \log{y}$ has no solution in integers? Note: $x, y$ are integer numbers. Edit 01: I edited the question because I have a wrong meaning, I meant in integers. Edit 02: I edited the question to show my key idea for the proof. Attempt : This is just an Idea for the proof What I think is  : $\log(x+y)\leq\log(xy)=\log{x}+\log{y}$ which is less than $\log{x} \cdot \log{y}$ for $ x, y \geq 8$, hence: $2\leq \min(x,y)\leq 7$ , which  it's easy to check. Thank you for any help !!!!","I have tried to solve this equation for $x, y >0$, $\log(x+y)=\log{x}\cdot \log{y}$ by writing it as this $x+y={x}^{\log{y}}$. But the last expression is complicated to me to get any solutions, I think it has no solution in integers !!! My question here is : How do I show that for $x, y >0$,  $\log(x+y)=\log{x} \cdot \log{y}$ has no solution in integers? Note: $x, y$ are integer numbers. Edit 01: I edited the question because I have a wrong meaning, I meant in integers. Edit 02: I edited the question to show my key idea for the proof. Attempt : This is just an Idea for the proof What I think is  : $\log(x+y)\leq\log(xy)=\log{x}+\log{y}$ which is less than $\log{x} \cdot \log{y}$ for $ x, y \geq 8$, hence: $2\leq \min(x,y)\leq 7$ , which  it's easy to check. Thank you for any help !!!!",,"['real-analysis', 'inequality', 'logarithms']"
46,Prove that there is a subsequence satisfying Three Properties (Lebesgue Integration),Prove that there is a subsequence satisfying Three Properties (Lebesgue Integration),,"Suppose $\{f_n\}$ are Lebesgue measurable functions on $[0,1]$, such that $\int_0^1 |f_n|\,d\mu=1$ for all $n$, and $f_n\to 0$ almost everywhere. I have proved: given $\epsilon>0$, there exists a Lebesgue meausurable $E\subseteq [0,1]$ such that $\mu(E)<\epsilon$ and $$\lim_{n\to\infty}\int_E |f_n|\,d\mu=1$$ (using Egorov's Theorem, where $E$ turns out to be $[0,1]\setminus F$ for some closed $F$ in which the convergence is uniform.) Hence, or otherwise, how do we prove that there exists a subsequence $f_{n_k}$ of $f_n$ and sequences of measurable functions $g_k$ and $h_k$ such that (i) $f_{n_k}=g_k+h_k$ for all $k$ (ii) $g_kg_j=0$ a.e. for $k\neq j$ (iii) $\lim_{k\to\infty}\int_0^1|h_k|\,d\mu=0$ The hardest condition in my opinion is (ii). I managed to find a candidate that satisfies both (i) and (iii), but not (ii). Let $f_{n_k}$ be a subsequence such that $\int_E |f_{n_k}|\,d\mu>1-\frac 1k$. Let $g_k=f_{n_k}\chi_E$ and $h_k=f_{n_k}\chi_F$, where $E=[0,1]\setminus F$. Then (i) $f_{n_k}=g_k+h_k$ is satisfied. $\lim_{k\to\infty}\int_0^1 |h_k|\,d\mu=\lim_{k\to\infty}\int_F |f_{n_k}|\,d\mu=1-\lim_{k\to\infty}\int_E |f_{n_k}|\,d\mu=1-1=0$. So (iii) is satisfied. However, condition (ii) remains unsatisfied. Thanks for any help.","Suppose $\{f_n\}$ are Lebesgue measurable functions on $[0,1]$, such that $\int_0^1 |f_n|\,d\mu=1$ for all $n$, and $f_n\to 0$ almost everywhere. I have proved: given $\epsilon>0$, there exists a Lebesgue meausurable $E\subseteq [0,1]$ such that $\mu(E)<\epsilon$ and $$\lim_{n\to\infty}\int_E |f_n|\,d\mu=1$$ (using Egorov's Theorem, where $E$ turns out to be $[0,1]\setminus F$ for some closed $F$ in which the convergence is uniform.) Hence, or otherwise, how do we prove that there exists a subsequence $f_{n_k}$ of $f_n$ and sequences of measurable functions $g_k$ and $h_k$ such that (i) $f_{n_k}=g_k+h_k$ for all $k$ (ii) $g_kg_j=0$ a.e. for $k\neq j$ (iii) $\lim_{k\to\infty}\int_0^1|h_k|\,d\mu=0$ The hardest condition in my opinion is (ii). I managed to find a candidate that satisfies both (i) and (iii), but not (ii). Let $f_{n_k}$ be a subsequence such that $\int_E |f_{n_k}|\,d\mu>1-\frac 1k$. Let $g_k=f_{n_k}\chi_E$ and $h_k=f_{n_k}\chi_F$, where $E=[0,1]\setminus F$. Then (i) $f_{n_k}=g_k+h_k$ is satisfied. $\lim_{k\to\infty}\int_0^1 |h_k|\,d\mu=\lim_{k\to\infty}\int_F |f_{n_k}|\,d\mu=1-\lim_{k\to\infty}\int_E |f_{n_k}|\,d\mu=1-1=0$. So (iii) is satisfied. However, condition (ii) remains unsatisfied. Thanks for any help.",,"['real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
47,"If a collection of disjoint disks covers the unit square, then the circumferences add up to infinitude.","If a collection of disjoint disks covers the unit square, then the circumferences add up to infinitude.",,"A question from Makarov & Podykorytov, Real analysis: Measures, Integrals and Applications (Can't recall what page though, but it's in the chapter about product measure). Assume a collection of disjoint disks cover the unit square, $[0,1]^2$ up to a (Lebesgue) null set.   Then, the sum of the lengths of their boundaries is infinitude. My attempt: We denote the disks $\{D_n\}_{n=1}^\infty $, their corresponding radii with $r_n$, the union $\bigcup_{n=1}^\infty D_n = C$ and Lebesgue measure on $\mathbb{R}^2$  with $m^2$, $\sum_{n=1}^\infty m^2(D_n) =\sum_{n=1}^\infty \pi r_n^2 =1 $ But this does not (and i think, cannot) produce a good bound on the sum of lengths of circumferences. Almost all (vertical and horizontal) cross-sections must have 1-dimensional measure $1$. I speculate this implies that up to a null-set, every cross-section intersects infinitely many disks, but did not manage to show this. If the last remark is true, then maybe we can show that almost-all cross-sections of the circumferences have positive measure, (as the fact that the cross-section intersects infintely many disks is encouraging).","A question from Makarov & Podykorytov, Real analysis: Measures, Integrals and Applications (Can't recall what page though, but it's in the chapter about product measure). Assume a collection of disjoint disks cover the unit square, $[0,1]^2$ up to a (Lebesgue) null set.   Then, the sum of the lengths of their boundaries is infinitude. My attempt: We denote the disks $\{D_n\}_{n=1}^\infty $, their corresponding radii with $r_n$, the union $\bigcup_{n=1}^\infty D_n = C$ and Lebesgue measure on $\mathbb{R}^2$  with $m^2$, $\sum_{n=1}^\infty m^2(D_n) =\sum_{n=1}^\infty \pi r_n^2 =1 $ But this does not (and i think, cannot) produce a good bound on the sum of lengths of circumferences. Almost all (vertical and horizontal) cross-sections must have 1-dimensional measure $1$. I speculate this implies that up to a null-set, every cross-section intersects infinitely many disks, but did not manage to show this. If the last remark is true, then maybe we can show that almost-all cross-sections of the circumferences have positive measure, (as the fact that the cross-section intersects infintely many disks is encouraging).",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
48,"Let $p(x)$ be a monic cubic polynomial with three distinct real roots. How many real roots does $\big(p'(x)\big)^2 - 2\,p(x)\,p''(x)$ have?",Let  be a monic cubic polynomial with three distinct real roots. How many real roots does  have?,"p(x) \big(p'(x)\big)^2 - 2\,p(x)\,p''(x)","So I came across this problem Let $p(x)$ be a monic polynomial of degree $3$ with three distinct real roots. How many real roots does the polynomial $\big(p'(x)\big)^2 - 2\,p(x)\,p''(x)$ have? If you let the given expression equal $f(x)$ and take its derivative, you get that $f'(x)=-12\,p(x)$ This means that the roots of $p(x)$ are where the extrema of $f(x)$ are. You can also figure out that the leading coefficient of $p(x)$ is $-3$ so for positively large and negatively large values of $x$, $f(x)$ is negative. From here, how do I figure out what is going on in between and how many real roots there are? For those who want to know, this problem is from the Swedish Mathematical Olympiad in 1988.","So I came across this problem Let $p(x)$ be a monic polynomial of degree $3$ with three distinct real roots. How many real roots does the polynomial $\big(p'(x)\big)^2 - 2\,p(x)\,p''(x)$ have? If you let the given expression equal $f(x)$ and take its derivative, you get that $f'(x)=-12\,p(x)$ This means that the roots of $p(x)$ are where the extrema of $f(x)$ are. You can also figure out that the leading coefficient of $p(x)$ is $-3$ so for positively large and negatively large values of $x$, $f(x)$ is negative. From here, how do I figure out what is going on in between and how many real roots there are? For those who want to know, this problem is from the Swedish Mathematical Olympiad in 1988.",,"['calculus', 'real-analysis', 'derivatives', 'polynomials', 'contest-math']"
49,"How can it be proved that the limit definition and series definition of $e$ are equivalent, how do they model continuous growth?","How can it be proved that the limit definition and series definition of  are equivalent, how do they model continuous growth?",e,"The limit definition of $e$ gives the best intuition toward how $e$ can model continuous growth. $$\lim\limits_{n \to \infty} \left( 1 + \frac1n\right)^n$$ Let n represent n moments of growth that scale the initial value by $ 1 + \frac1n $ . The key to me seems to be that the $\Delta x $ approaches 1 but does not reach it. This allows for the best approximation $\Delta x = 1 $ at each moment. In other words, at each moment of growth the value x experiences $\Delta x = 1 + \frac1n $ , or as $n \to \infty$ ... $$\lim\limits_{n \to \infty} 1 + \frac1n = 1 $$ Growth with n = 2 moments: $1 \times 1\frac12 \times 1\frac12 $ = 2.25 insight: If 2 moments of growth occur to x where $\Delta x = 1\frac12$ , this amounts to discrete growth by $\Delta x = 1\frac12$ twice. Growth with three moments: $1\times 1\frac13 \times 1\frac13 \times 1\frac13 $ = 2.35 insight: If 3 moments of growth occur to x in where $\Delta x = 1\frac13$ , this amounts to discrete growth by $\Delta x = 1\frac13$ three times. Growth at seven moments: $1\times \frac17 \times 1\frac17  \times 1\frac17 \times 1\frac17 \times 1\frac17 \times 1\frac17 \times 1\frac17 $ = 2.55 insight: If 7 moments of growth occur to x in this period where $\Delta x = 1\frac17$ , this amounts to discrete growth by $\Delta x = 1\frac17$ seven times. $\lim\limits_{n \to \infty} \left( 1 + \frac1n\right)^n $ is a definition of $e$ as it models n moments of growth occurring where at each instant the initial value is scaled by a factor $\lim\limits_{n \to \infty} 1 + \frac1n = 1 $ . To me this seems intuitive to a small extent. If we want to grow continuously we scale by an infinitely small increase an infinite number of times and this converges onto the number $e$ . Why it must converge is mysterious to me, and besides seeing a proof I can't intuit to something that is satisfying. Now there is also the Taylor series representation of $e$ $$ \sum_{n=0}^\infty \frac1{n!} = 1 + 1 + \frac12 + \frac1{3\times2\times1} + \frac1{4\times3\times2\times1} ... $$ Speaking loosely it seems to get $e$ you can sum a whole and itself, with $ \frac12$ that whole, and $ \frac13$ of $ \frac12$ of that whole and so on...and I have no intuition for this. Again, the Taylor Series converges to $e$ but I don't see the same intuition for how this can model continuous growth. Furthermore, with my limited math skills I wouldn't know how to prove: $$\lim\limits_{n \to \infty} \left( 1 + \frac1n\right)^n = \sum_{n=0}^\infty \frac1{n!}$$ My 3 questions I am seeking to answer are 1. By what application or intuition can it be seen that $ \sum_{n=0}^\infty \frac1{n!} $ models continuous growth? 2. What are some proofs that: $$\lim\limits_{n \to \infty} \left( 1 + \frac1n\right)^n = \sum_{n=0}^\infty \frac1{n!}$$ 3. Is euler's number inextricably linked to time as $\pi$ is to the circle? When discussing $e$ it seems necessary to talk of periods of continuous growth $ p = kt $ being represented by $ e = e^p $ where $ p = 1 $ in the fundamental case. I do not know how to think of $e$ without seeing it as one period of continuous growth proven to be $e$ by the convergence of the intuitive definition I gave . I cannot see this same inextricable link to a period in the Taylor Series nor in other definitions of $e$ . Thanks for any comments, I have been trying to wrap my head around $e$ at the most fundamental level for a long time so that I can more deeply understand differential equations. How was it arrived upon that e best modeled continuous growth? Was it an experimental observation or was it a proof that convinced mathematicians and scientists that $e$ embodies the idea of continuous growth i.e. growth at all instances? I have so many questions about Taylor Series and $e^{i\theta}$ I'm trying to answer as well.. Best Wishes,","The limit definition of gives the best intuition toward how can model continuous growth. Let n represent n moments of growth that scale the initial value by . The key to me seems to be that the approaches 1 but does not reach it. This allows for the best approximation at each moment. In other words, at each moment of growth the value x experiences , or as ... Growth with n = 2 moments: = 2.25 insight: If 2 moments of growth occur to x where , this amounts to discrete growth by twice. Growth with three moments: = 2.35 insight: If 3 moments of growth occur to x in where , this amounts to discrete growth by three times. Growth at seven moments: = 2.55 insight: If 7 moments of growth occur to x in this period where , this amounts to discrete growth by seven times. is a definition of as it models n moments of growth occurring where at each instant the initial value is scaled by a factor . To me this seems intuitive to a small extent. If we want to grow continuously we scale by an infinitely small increase an infinite number of times and this converges onto the number . Why it must converge is mysterious to me, and besides seeing a proof I can't intuit to something that is satisfying. Now there is also the Taylor series representation of Speaking loosely it seems to get you can sum a whole and itself, with that whole, and of of that whole and so on...and I have no intuition for this. Again, the Taylor Series converges to but I don't see the same intuition for how this can model continuous growth. Furthermore, with my limited math skills I wouldn't know how to prove: My 3 questions I am seeking to answer are 1. By what application or intuition can it be seen that models continuous growth? 2. What are some proofs that: 3. Is euler's number inextricably linked to time as is to the circle? When discussing it seems necessary to talk of periods of continuous growth being represented by where in the fundamental case. I do not know how to think of without seeing it as one period of continuous growth proven to be by the convergence of the intuitive definition I gave . I cannot see this same inextricable link to a period in the Taylor Series nor in other definitions of . Thanks for any comments, I have been trying to wrap my head around at the most fundamental level for a long time so that I can more deeply understand differential equations. How was it arrived upon that e best modeled continuous growth? Was it an experimental observation or was it a proof that convinced mathematicians and scientists that embodies the idea of continuous growth i.e. growth at all instances? I have so many questions about Taylor Series and I'm trying to answer as well.. Best Wishes,",e e \lim\limits_{n \to \infty} \left( 1 + \frac1n\right)^n  1 + \frac1n  \Delta x  \Delta x = 1  \Delta x = 1 + \frac1n  n \to \infty \lim\limits_{n \to \infty} 1 + \frac1n = 1  1 \times 1\frac12 \times 1\frac12  \Delta x = 1\frac12 \Delta x = 1\frac12 1\times 1\frac13 \times 1\frac13 \times 1\frac13  \Delta x = 1\frac13 \Delta x = 1\frac13 1\times \frac17 \times 1\frac17  \times 1\frac17 \times 1\frac17 \times 1\frac17 \times 1\frac17 \times 1\frac17  \Delta x = 1\frac17 \Delta x = 1\frac17 \lim\limits_{n \to \infty} \left( 1 + \frac1n\right)^n  e \lim\limits_{n \to \infty} 1 + \frac1n = 1  e e  \sum_{n=0}^\infty \frac1{n!} = 1 + 1 + \frac12 + \frac1{3\times2\times1} + \frac1{4\times3\times2\times1} ...  e  \frac12  \frac13  \frac12 e \lim\limits_{n \to \infty} \left( 1 + \frac1n\right)^n = \sum_{n=0}^\infty \frac1{n!}  \sum_{n=0}^\infty \frac1{n!}  \lim\limits_{n \to \infty} \left( 1 + \frac1n\right)^n = \sum_{n=0}^\infty \frac1{n!} \pi e  p = kt   e = e^p   p = 1  e e e e e e^{i\theta},"['calculus', 'real-analysis', 'ordinary-differential-equations']"
50,Differentiation under the integral sign for volumes in higher dimensions,Differentiation under the integral sign for volumes in higher dimensions,,"Consider a smooth convex/compact domain $D\subset \mathbb{R}^n$ and a smooth, concave function $F:D\to \mathbb{R}$.  Then we can define the function that simply takes the volume of the upper contour sets determined by the argument: $$G(t) = \int_{\{x\in D \; : \; F(x) \ge t\}} d\lambda$$ where $\lambda$ denotes the Lebesgue measure.  I'm trying to figure out an expression for $\frac{d}{dt}G(t)$. This seems like nothing more than a special case of a higher-dimensional Leibniz Integral Rule , but wikipedia gives me a substantially more general formula than I suspect I need for this case (for definitions of terms see the link): $$\frac{d}{dt} \int_{\Omega(t)} \omega = \int_{\Omega(t)} i_{\vec{v}}(d_x \omega) + \int_{\partial \Omega(t)} i_{\vec{v}}\omega + \int_{\Omega(t)} \dot{\omega}.$$ I have almost no background in differential forms, but immediately I know, for starters, the volume form I'm integrating is time invariant so the last term drops out here.  Moreover, given I'm just concerned with a uniform density, I'd imagine the first term should be zero too? (This corresponding to the intuition that all that really matters here is how much 'volume bleeds out of the bag $\Omega(t)$' as I cinch it shut by increasing $t$, and hence I need only be concerned with the incremental flow of volume across the boundary.)  But that may be wildly incorrect. Ideally if someone could help guide me (ideally both intuitively and analytically) to be able to understand and describe this derivative I'd be very grateful!  In particular an expression for what the Leibniz rule reduces to in this case would be most welcome.","Consider a smooth convex/compact domain $D\subset \mathbb{R}^n$ and a smooth, concave function $F:D\to \mathbb{R}$.  Then we can define the function that simply takes the volume of the upper contour sets determined by the argument: $$G(t) = \int_{\{x\in D \; : \; F(x) \ge t\}} d\lambda$$ where $\lambda$ denotes the Lebesgue measure.  I'm trying to figure out an expression for $\frac{d}{dt}G(t)$. This seems like nothing more than a special case of a higher-dimensional Leibniz Integral Rule , but wikipedia gives me a substantially more general formula than I suspect I need for this case (for definitions of terms see the link): $$\frac{d}{dt} \int_{\Omega(t)} \omega = \int_{\Omega(t)} i_{\vec{v}}(d_x \omega) + \int_{\partial \Omega(t)} i_{\vec{v}}\omega + \int_{\Omega(t)} \dot{\omega}.$$ I have almost no background in differential forms, but immediately I know, for starters, the volume form I'm integrating is time invariant so the last term drops out here.  Moreover, given I'm just concerned with a uniform density, I'd imagine the first term should be zero too? (This corresponding to the intuition that all that really matters here is how much 'volume bleeds out of the bag $\Omega(t)$' as I cinch it shut by increasing $t$, and hence I need only be concerned with the incremental flow of volume across the boundary.)  But that may be wildly incorrect. Ideally if someone could help guide me (ideally both intuitively and analytically) to be able to understand and describe this derivative I'd be very grateful!  In particular an expression for what the Leibniz rule reduces to in this case would be most welcome.",,"['real-analysis', 'integration', 'measure-theory', 'differential-geometry', 'derivatives']"
51,Schwartz functions dense?,Schwartz functions dense?,,"I want to show that the Schwartz functions are dense in $$\left\{f \in L^2; \int |x|^2 \left|f(x)\right|^2 dx + \int |\xi|^2 \left|\hat{f}(\xi)\right|^2 d \xi < \infty\right\}$$ where the norm is given by $$\left\lVert f\right\rVert_{L^2}^2 = \int (1+|x|^2) |f(x)|^2 dx + \int \left(1+|\xi|^2\right) \left|\hat{f}(\xi)\right|^2 d \xi.$$ If we would only have one summand, then this would be a $H^1-$ Sobolev summand norm and for this space I know that this is true, but how can I show this for this more general norm?","I want to show that the Schwartz functions are dense in $$\left\{f \in L^2; \int |x|^2 \left|f(x)\right|^2 dx + \int |\xi|^2 \left|\hat{f}(\xi)\right|^2 d \xi < \infty\right\}$$ where the norm is given by $$\left\lVert f\right\rVert_{L^2}^2 = \int (1+|x|^2) |f(x)|^2 dx + \int \left(1+|\xi|^2\right) \left|\hat{f}(\xi)\right|^2 d \xi.$$ If we would only have one summand, then this would be a $H^1-$ Sobolev summand norm and for this space I know that this is true, but how can I show this for this more general norm?",,"['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations', 'sobolev-spaces']"
52,"$\mu(X) \lt \infty$. Then $f_k \to f$ in measure iff for any subsequence $k_l$, there is a subsequence $k_{l_n}$ such that $f_{k_{l_n}}\to f$ a.e.",". Then  in measure iff for any subsequence , there is a subsequence  such that  a.e.",\mu(X) \lt \infty f_k \to f k_l k_{l_n} f_{k_{l_n}}\to f,"Let $\mu(X) \lt \infty$. Then $f_k \to f$ in measure iff for any subsequence $k_l$, there is a subsequence $k_{l_n}$ such that $f_{k_{l_n}}\to f$ a.e. I can show the only if part by using the theorem that if $f_k \to f$ in measure then there is a subsequence converging to $f$ a.e. However, I can't show the other part. I'm trying to use the finiteness of the measure, but I am out of ideas. How can I show this? I would greatly appreciate any help.","Let $\mu(X) \lt \infty$. Then $f_k \to f$ in measure iff for any subsequence $k_l$, there is a subsequence $k_{l_n}$ such that $f_{k_{l_n}}\to f$ a.e. I can show the only if part by using the theorem that if $f_k \to f$ in measure then there is a subsequence converging to $f$ a.e. However, I can't show the other part. I'm trying to use the finiteness of the measure, but I am out of ideas. How can I show this? I would greatly appreciate any help.",,"['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'convergence-divergence']"
53,f(x) Changing Sign Implies f'(x) Not Changing Sign,f(x) Changing Sign Implies f'(x) Not Changing Sign,,"Let $f(x)$ be a real-valued function with $c$ in its domain. Given 1. $f(c)=0, $ 2. $f$ is not identically zero in any interval about $c$, and 3. $f$ is differentiable at $c$. Claim: a. If $f(x)$ changes sign at $c$, then $f(x)$ does not change sign at $c$. b. If $f(x)$ does not change sign at $c$, then $f(x)$ changes sign at $c$. How would I go about proving this claim? Thoughts: The main difficulty I have with this question is that I cannot express ""changing sign"" in a way involving derivatives that makes this result amenable to a proof I can come up with. The one precise meaning of ""changing sign"" I thought of is: $f(x)$ changes sign at $c$ if there exists an open set $S$ containing $c$ such that for all $y\in S$  and $y<c$, $f(y)$ is positive (negative) and for all $y\in S$ and $y>c$, $f(y)$ is negative (positive). It's also clear that: If $f(x)$ changes sign at $c$, $f(c)=0$, but the converse is not true. Suggestions would be appreciated.","Let $f(x)$ be a real-valued function with $c$ in its domain. Given 1. $f(c)=0, $ 2. $f$ is not identically zero in any interval about $c$, and 3. $f$ is differentiable at $c$. Claim: a. If $f(x)$ changes sign at $c$, then $f(x)$ does not change sign at $c$. b. If $f(x)$ does not change sign at $c$, then $f(x)$ changes sign at $c$. How would I go about proving this claim? Thoughts: The main difficulty I have with this question is that I cannot express ""changing sign"" in a way involving derivatives that makes this result amenable to a proof I can come up with. The one precise meaning of ""changing sign"" I thought of is: $f(x)$ changes sign at $c$ if there exists an open set $S$ containing $c$ such that for all $y\in S$  and $y<c$, $f(y)$ is positive (negative) and for all $y\in S$ and $y>c$, $f(y)$ is negative (positive). It's also clear that: If $f(x)$ changes sign at $c$, $f(c)=0$, but the converse is not true. Suggestions would be appreciated.",,"['calculus', 'real-analysis']"
54,Embedding $ \mathbb{R} \to \mathbb{R}^2$,Embedding, \mathbb{R} \to \mathbb{R}^2,"The question I'm looking at is as follows: Prove that there is an embedding of the line as a closed subset of the plane, and there is an embedding of the line as a bounded subset of the plane, but there is no embedding of the line as a closed and bounded subset of the plane. My understanding of embedding is that it needs to be a homomorphism from $ \mathbb{R} \to f(x) \in \mathbb{R}^2$. I.e. the entire number line needs to be in $ \mathbb{R}^2$ in some shape or form after the transformation. My thoughts for the closed subset are simply $ f(x) : x \to (1,x) $ as this is is effectively the identity function plus one dimension. It is closed as all 0 limits are contained, and unbounded as the Cauchy sequences do not converge as x approaches $ - \infty $ and $ \infty $. For bounded $ f(x) : x \to (arctanh(x),x) $ on $ (-1,1) $ which encodes the entire number line, has Cauchy sequences converging at limits, but does not contain $ x = -1 $ or $ x = 1 $. Are these intuitions correct for these parts of the question, or am I misinterpreting embedding as a concept? Are there significantly simpler answers? I feel like I'm missing something.","The question I'm looking at is as follows: Prove that there is an embedding of the line as a closed subset of the plane, and there is an embedding of the line as a bounded subset of the plane, but there is no embedding of the line as a closed and bounded subset of the plane. My understanding of embedding is that it needs to be a homomorphism from $ \mathbb{R} \to f(x) \in \mathbb{R}^2$. I.e. the entire number line needs to be in $ \mathbb{R}^2$ in some shape or form after the transformation. My thoughts for the closed subset are simply $ f(x) : x \to (1,x) $ as this is is effectively the identity function plus one dimension. It is closed as all 0 limits are contained, and unbounded as the Cauchy sequences do not converge as x approaches $ - \infty $ and $ \infty $. For bounded $ f(x) : x \to (arctanh(x),x) $ on $ (-1,1) $ which encodes the entire number line, has Cauchy sequences converging at limits, but does not contain $ x = -1 $ or $ x = 1 $. Are these intuitions correct for these parts of the question, or am I misinterpreting embedding as a concept? Are there significantly simpler answers? I feel like I'm missing something.",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
55,"Lebesgue integral, differentiation under the integral sign","Lebesgue integral, differentiation under the integral sign",,"Let $f:[0,1] \times [0,1]  \to \mathbb R$ be a function such that: (a) for each $x \in [0,1]$, the function $y \to f(x,y)$ is Lebesgue integrable on $[0,1]$. (b) $\dfrac{\partial f}{\partial x}(x,y)$ is a bounded function of $(x,y)$. Show that for each $x$, the function $y \to \dfrac{\partial f}{\partial x}(x,y)$ is measurable and $\dfrac{d}{dx} \int_0^1 f(x,y)dy=\int_0^1 \dfrac{\partial f}{\partial x}(x,y)dy$. I am a bit stuck on the exercise, to prove that $g_x(y)=\dfrac{\partial f}{\partial x}(x,y)$ I did the following: If for each $x$, $f_x(y)=f(x,y)$ is integrable, by definition, it is measurable. So given $x$ in [0,1), the function $f_{x+\frac{1}{n}}$ is measurable for each $n>\dfrac{1}{x+1}$ and the function $f_{1-\frac{1}{n}}$ is measurable for $x=1$, but then for $0 \leq x <1$ we have $$\dfrac{\partial f}{\partial x}(x,y)=\lim_{n \to \infty} \dfrac{f(x+\frac{1}{n},y)-f(x,y)}{\frac{1}{n}}$$ and for $x=1$, $$\dfrac{\partial f}{\partial x}(x,y)=\lim_{n \to \infty} \dfrac{f(x-\frac{1}{n},y)-f(x,y)}{-\frac{1}{n}}$$ Since each function is a pointwise limit of measurable functions, then for each $0 \leq x \leq 1$, $\dfrac{\partial f}{\partial x}(x,y)$ is a measurable function. I am not sure if my reasoning is correct and I don't know what to do for the last part of the exercise. Any help would be greatly appreciated. Thanks in advance","Let $f:[0,1] \times [0,1]  \to \mathbb R$ be a function such that: (a) for each $x \in [0,1]$, the function $y \to f(x,y)$ is Lebesgue integrable on $[0,1]$. (b) $\dfrac{\partial f}{\partial x}(x,y)$ is a bounded function of $(x,y)$. Show that for each $x$, the function $y \to \dfrac{\partial f}{\partial x}(x,y)$ is measurable and $\dfrac{d}{dx} \int_0^1 f(x,y)dy=\int_0^1 \dfrac{\partial f}{\partial x}(x,y)dy$. I am a bit stuck on the exercise, to prove that $g_x(y)=\dfrac{\partial f}{\partial x}(x,y)$ I did the following: If for each $x$, $f_x(y)=f(x,y)$ is integrable, by definition, it is measurable. So given $x$ in [0,1), the function $f_{x+\frac{1}{n}}$ is measurable for each $n>\dfrac{1}{x+1}$ and the function $f_{1-\frac{1}{n}}$ is measurable for $x=1$, but then for $0 \leq x <1$ we have $$\dfrac{\partial f}{\partial x}(x,y)=\lim_{n \to \infty} \dfrac{f(x+\frac{1}{n},y)-f(x,y)}{\frac{1}{n}}$$ and for $x=1$, $$\dfrac{\partial f}{\partial x}(x,y)=\lim_{n \to \infty} \dfrac{f(x-\frac{1}{n},y)-f(x,y)}{-\frac{1}{n}}$$ Since each function is a pointwise limit of measurable functions, then for each $0 \leq x \leq 1$, $\dfrac{\partial f}{\partial x}(x,y)$ is a measurable function. I am not sure if my reasoning is correct and I don't know what to do for the last part of the exercise. Any help would be greatly appreciated. Thanks in advance",,"['real-analysis', 'lebesgue-integral']"
56,When is a continuous function piecewise monotone?,When is a continuous function piecewise monotone?,,"Given a continuous function $f:[a,b]\mapsto \mathbb{R}$, are there known additional conditions that ensure $f$ is piecewise monotone? Like this question , my motivation is to decompose the interval $[a,b]$ as disjoint union of countable subintervals such that $f$ is monotone over each of these subintervals. However, I want to understand what condition on $f$, in addition to given continuity , is required for piecewise monotonicity. So far I understand that continuity is not enough since it does not exclude the possibility of nowhere differentiable functions such as this. Also, requiring $f$ to be differentiable is not enough as it can be everywhere differentiable but nowhere monotonic. What, then, is required to make a continuous function piecewise monotonic?","Given a continuous function $f:[a,b]\mapsto \mathbb{R}$, are there known additional conditions that ensure $f$ is piecewise monotone? Like this question , my motivation is to decompose the interval $[a,b]$ as disjoint union of countable subintervals such that $f$ is monotone over each of these subintervals. However, I want to understand what condition on $f$, in addition to given continuity , is required for piecewise monotonicity. So far I understand that continuity is not enough since it does not exclude the possibility of nowhere differentiable functions such as this. Also, requiring $f$ to be differentiable is not enough as it can be everywhere differentiable but nowhere monotonic. What, then, is required to make a continuous function piecewise monotonic?",,"['real-analysis', 'analysis']"
57,Show that $ \sum_{n\in \mathbb {S}} \frac{1}{n} $ is convergent [duplicate],Show that  is convergent [duplicate], \sum_{n\in \mathbb {S}} \frac{1}{n} ,"This question already has an answer here : Sum over all non-evil numbers (1 answer) Closed 9 years ago . Let $\mathbb {S} =\left \{ 1,2,3,...,9,11,12,...,19,21,...99,111,112,113... \right \} $ i.e, the positive integers set which contain zero digit is omitted. Now show that $ \sum_{n\in \mathbb {S}}  \frac{1}{n} $ is convergent . I really don't have no idea about how to prove this","This question already has an answer here : Sum over all non-evil numbers (1 answer) Closed 9 years ago . Let $\mathbb {S} =\left \{ 1,2,3,...,9,11,12,...,19,21,...99,111,112,113... \right \} $ i.e, the positive integers set which contain zero digit is omitted. Now show that $ \sum_{n\in \mathbb {S}}  \frac{1}{n} $ is convergent . I really don't have no idea about how to prove this",,['real-analysis']
58,"If $f$ is a continuous function such that $|f(x+y)-f(x)-f(y)|$ is bounded and $f(n)=o(n)$, then $f$ is bounded","If  is a continuous function such that  is bounded and , then  is bounded",f |f(x+y)-f(x)-f(y)| f(n)=o(n) f,"Suppose that $f:\mathbf{R}\to\mathbf{R}$ is a continuous function such that $$\sup_{x,y\in\mathbf{R}}|f(x+y)-f(x)-f(y)|<\infty \quad (*)$$ and $\lim_{n\to\infty}\frac{f(n)}{n}=0$, prove that $\sup_{x\in\mathbf{R}} |f(x)|<\infty$ I don't know how to start, what can I get from $(*)$ ?","Suppose that $f:\mathbf{R}\to\mathbf{R}$ is a continuous function such that $$\sup_{x,y\in\mathbf{R}}|f(x+y)-f(x)-f(y)|<\infty \quad (*)$$ and $\lim_{n\to\infty}\frac{f(n)}{n}=0$, prove that $\sup_{x\in\mathbf{R}} |f(x)|<\infty$ I don't know how to start, what can I get from $(*)$ ?",,"['calculus', 'real-analysis']"
59,"Calculate $\int_0^{\pi/2}\frac{\sin(x)\log{\sin{(x)}}}{x}\,dx$",Calculate,"\int_0^{\pi/2}\frac{\sin(x)\log{\sin{(x)}}}{x}\,dx","Inspired by a question I saw these days, I try to calculate in closed form $$\int_0^{\pi/2}\frac{\sin(x)\log{\sin{(x)}}}{x}\,dx$$ So far no fruitful idea that is worth sharing. What way would you propose? Note I prefer ways suggested,  not necessarily solutions, but I have nothing against any of the options you prefer.","Inspired by a question I saw these days, I try to calculate in closed form $$\int_0^{\pi/2}\frac{\sin(x)\log{\sin{(x)}}}{x}\,dx$$ So far no fruitful idea that is worth sharing. What way would you propose? Note I prefer ways suggested,  not necessarily solutions, but I have nothing against any of the options you prefer.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
60,Any hint for this measure theory problem from Halmos?,Any hint for this measure theory problem from Halmos?,,"I was reading Halmos book 'Measure Theory' and I'm really stuck with this one. Could anyone please give me a hint? Let $A\in\mathbb{R}$ be a Lebesgue measurable set and $D$ a dense subset of $A$.  If $\mu(A\,\triangle\, A+d) = 0\;\;\forall d\in D$ where $\triangle$ denotes symmetric difference and $A+d$ is the set resulting of adding $d$ to each $a\in A$,  it must be that $\mu(A) = 0$ or $\mu(A^c) = 0$. I've tried thinking of A as an open interval and then using properties of Lebesgue measure, also tried to apply Lebesgue Density Theorem, with no success.","I was reading Halmos book 'Measure Theory' and I'm really stuck with this one. Could anyone please give me a hint? Let $A\in\mathbb{R}$ be a Lebesgue measurable set and $D$ a dense subset of $A$.  If $\mu(A\,\triangle\, A+d) = 0\;\;\forall d\in D$ where $\triangle$ denotes symmetric difference and $A+d$ is the set resulting of adding $d$ to each $a\in A$,  it must be that $\mu(A) = 0$ or $\mu(A^c) = 0$. I've tried thinking of A as an open interval and then using properties of Lebesgue measure, also tried to apply Lebesgue Density Theorem, with no success.",,['real-analysis']
61,Question about differentiating under the integral sign,Question about differentiating under the integral sign,,"Let $0<m(E)<\infty$ for a Lebesgue measurable subset $E \subset \mathbb{R}$. Consider two positive functions $f$ and $g$ in $L^p(E)$, $p>1$, and define the function $F:[0,\infty) \to [0, \infty)$ as $$F(t)= \int_E [f(x)+tg(x)]^pd x.$$ Show $F$ is differentiable on $(0, \infty)$ and compute its derivative $F'(t)$. Make sure to show $F'(t)$ is finite. This came from an old qualifying exam at my university. Here is my attempt (letting $h(x,t)$ stand for the function in the integral): Since $p-1>0$ and since $f$ and $g$ are positive, the partial $$\frac{\partial h}{\partial t}=pg(x)[f(x)+tg(x)]^{p-1}$$ exists for each $(x,t), x\in E, t\in [0,\infty)$. Also note for each $(x,t)$ again since $g$ and $f$ are positive, \begin{align*} \bigg|\frac{\partial h}{\partial t}\bigg|&=\big|pg(x)[f(x)+tg(x)]^{p-1}\big| \\ & \leq p 2^{p-1}\big(f(x)^{p-1}g(x)+t^{p-1}g(x)^p\big) \end{align*} The finiteness of $m(E)$ ensures that the latter function is integrable. Hence, $$\frac{\partial F}{\partial t}=\frac {\partial}{\partial t} \int_E h(x,t)\:\mathrm{d}x=\int_E \frac{\partial h}{\partial t}\:\mathrm{d}x=p\int_Eg(x)[f(x)+tg(x)]^{p-1}\:\mathrm{d}x < \infty$$ for all $(x,t)$ as previously described. I feel uneasy with this solution, particularly with the inequalities, and whether I correctly justified differentiation under the integral sign.","Let $0<m(E)<\infty$ for a Lebesgue measurable subset $E \subset \mathbb{R}$. Consider two positive functions $f$ and $g$ in $L^p(E)$, $p>1$, and define the function $F:[0,\infty) \to [0, \infty)$ as $$F(t)= \int_E [f(x)+tg(x)]^pd x.$$ Show $F$ is differentiable on $(0, \infty)$ and compute its derivative $F'(t)$. Make sure to show $F'(t)$ is finite. This came from an old qualifying exam at my university. Here is my attempt (letting $h(x,t)$ stand for the function in the integral): Since $p-1>0$ and since $f$ and $g$ are positive, the partial $$\frac{\partial h}{\partial t}=pg(x)[f(x)+tg(x)]^{p-1}$$ exists for each $(x,t), x\in E, t\in [0,\infty)$. Also note for each $(x,t)$ again since $g$ and $f$ are positive, \begin{align*} \bigg|\frac{\partial h}{\partial t}\bigg|&=\big|pg(x)[f(x)+tg(x)]^{p-1}\big| \\ & \leq p 2^{p-1}\big(f(x)^{p-1}g(x)+t^{p-1}g(x)^p\big) \end{align*} The finiteness of $m(E)$ ensures that the latter function is integrable. Hence, $$\frac{\partial F}{\partial t}=\frac {\partial}{\partial t} \int_E h(x,t)\:\mathrm{d}x=\int_E \frac{\partial h}{\partial t}\:\mathrm{d}x=p\int_Eg(x)[f(x)+tg(x)]^{p-1}\:\mathrm{d}x < \infty$$ for all $(x,t)$ as previously described. I feel uneasy with this solution, particularly with the inequalities, and whether I correctly justified differentiation under the integral sign.",,['real-analysis']
62,"Finitely additive function on an infinite set, s.t., $m(A)=0$ for any finite set and $m(X)=1$ (constructive approach)","Finitely additive function on an infinite set, s.t.,  for any finite set and  (constructive approach)",m(A)=0 m(X)=1,"Other exercise which I found in Dudley's Analysis book: Show that there is a measure on a infinite set $X$, defined on $2^X$ s.t. is finitely additive, $m(A)=0$ for any finite set and $m(X)=1$. The solution is very simple using the Frechet filter $\mathcal{F}:=\{A: X\setminus A \text{ finite}\}$ and defining the measure on the ultrafilter $\mathfrak{U}$ containing $\mathcal{F}$, as follows $$m(A)=\begin{cases} 1& A\in \mathfrak{U}\\ 0&A\notin \mathfrak{U} \end{cases}$$ For the following Lemma : Let $\mathfrak{U}$ be an ultrafilter of subsets of $X$ and let $m$ defined as above. Then $m$ is finitely additive on $2^X$. PF: It's clear that $\varnothing\notin \mathfrak{U}$, so $m(\varnothing)=0$. Let $\{A_n\}_{n=1}^N\subset 2^{X}$ and disjoint, and let $A$ be their union. We consider two cases: If all are not in $ \mathfrak{U}$, i.e, $X\setminus A_n\in \mathfrak{U}$ for $n\le N$. Thus $X\setminus A=\bigcap_{n\le N}X\setminus A_n\in \mathfrak{U}$, so $A\notin \mathfrak{U}$. Hence $m(A)=\sum_{n\le N}m(A_n)=0$ Now suppose that at least one is in $\mathfrak{U}$. Let $A_1\in \mathfrak{U}$, so all the other elements are not in $\mathfrak{U}$ since otherwise $\varnothing=A\cap A_i\in \mathfrak{U}$ for $i\not=1$. Since $A_1\subset A$ and $A_1\in \mathfrak{U}$,  $A\in\mathfrak{U}$. Thus $m(A)=\sum_{n\le N}m(A_n)=1$. $\Box$ Does someone know if there is hope of a constructive approach? I believe the answer is negative...","Other exercise which I found in Dudley's Analysis book: Show that there is a measure on a infinite set $X$, defined on $2^X$ s.t. is finitely additive, $m(A)=0$ for any finite set and $m(X)=1$. The solution is very simple using the Frechet filter $\mathcal{F}:=\{A: X\setminus A \text{ finite}\}$ and defining the measure on the ultrafilter $\mathfrak{U}$ containing $\mathcal{F}$, as follows $$m(A)=\begin{cases} 1& A\in \mathfrak{U}\\ 0&A\notin \mathfrak{U} \end{cases}$$ For the following Lemma : Let $\mathfrak{U}$ be an ultrafilter of subsets of $X$ and let $m$ defined as above. Then $m$ is finitely additive on $2^X$. PF: It's clear that $\varnothing\notin \mathfrak{U}$, so $m(\varnothing)=0$. Let $\{A_n\}_{n=1}^N\subset 2^{X}$ and disjoint, and let $A$ be their union. We consider two cases: If all are not in $ \mathfrak{U}$, i.e, $X\setminus A_n\in \mathfrak{U}$ for $n\le N$. Thus $X\setminus A=\bigcap_{n\le N}X\setminus A_n\in \mathfrak{U}$, so $A\notin \mathfrak{U}$. Hence $m(A)=\sum_{n\le N}m(A_n)=0$ Now suppose that at least one is in $\mathfrak{U}$. Let $A_1\in \mathfrak{U}$, so all the other elements are not in $\mathfrak{U}$ since otherwise $\varnothing=A\cap A_i\in \mathfrak{U}$ for $i\not=1$. Since $A_1\subset A$ and $A_1\in \mathfrak{U}$,  $A\in\mathfrak{U}$. Thus $m(A)=\sum_{n\le N}m(A_n)=1$. $\Box$ Does someone know if there is hope of a constructive approach? I believe the answer is negative...",,"['real-analysis', 'measure-theory', 'proof-verification', 'alternative-proof', 'filters']"
63,"Exercise on Radon measures, constructing a convergent sequence","Exercise on Radon measures, constructing a convergent sequence",,"Let $\mu$ be a Radon measure on $\mathbb{R}^n$ such that $\mu(B(0, s)) > 0$ for all $s > 0$ and suppose that $$C = \limsup_{s\ \downarrow\ 0}\frac{\mu(B(0, 2s))}{\mu(B(0, s))} < \infty.$$ For all $r > 0$ , let $\mu_r$ be the Radon measure on $\mathbb{R}^n$ defined by the rule $$\mu_r(E) = \frac{\mu(rE)}{\mu(B(0, r))}\ \text{for all}\ E \subset \mathbb{R}^n.$$ Prove that there exists $r_i\ \downarrow\ 0$ and a Radon measure $\mu_0$ on $\mathbb{R}^n$ such that $\mu_{r_i} \rightharpoonup \mu_0$ . I actually have very little idea of how to approach this problem. I was hoping to make use of the constant $C$ by doing some manipulations such as $$\mu_r(E) = \frac{\mu(rE)}{\mu(B(0, r))} = \frac{\mu(rE)}{\mu(B(0, 2r))}\frac{\mu(B(0, 2r))}{\mu(B(0, r))}$$ but no such luck. Any hints, or recommendations for Radon measure references, would be greatly appreciated.","Let be a Radon measure on such that for all and suppose that For all , let be the Radon measure on defined by the rule Prove that there exists and a Radon measure on such that . I actually have very little idea of how to approach this problem. I was hoping to make use of the constant by doing some manipulations such as but no such luck. Any hints, or recommendations for Radon measure references, would be greatly appreciated.","\mu \mathbb{R}^n \mu(B(0, s)) > 0 s > 0 C = \limsup_{s\ \downarrow\ 0}\frac{\mu(B(0, 2s))}{\mu(B(0, s))} < \infty. r > 0 \mu_r \mathbb{R}^n \mu_r(E) = \frac{\mu(rE)}{\mu(B(0, r))}\ \text{for all}\ E \subset \mathbb{R}^n. r_i\ \downarrow\ 0 \mu_0 \mathbb{R}^n \mu_{r_i} \rightharpoonup \mu_0 C \mu_r(E) = \frac{\mu(rE)}{\mu(B(0, r))} = \frac{\mu(rE)}{\mu(B(0, 2r))}\frac{\mu(B(0, 2r))}{\mu(B(0, r))}","['real-analysis', 'measure-theory']"
64,Prove that the $p$-mean is an increasing function of $p$,Prove that the -mean is an increasing function of,p p,"Let $p\neq0$ and $j=1,2,\cdots,n$ and $x_j>0$ and $$\chi(p)=\left(\frac{1}{n}\sum_{j=1}^nx_j^p\right)^\frac{1}{p}.$$ Prove that $\chi$ is strictly increasing and the following statements hold $\lim\limits_{p\to0}\chi(p)=(x_1x_2\cdots x_n)^\frac{1}{n}$ $\lim\limits_{p\to+\infty}\chi(p)=\max\{x_1,x_2,\cdots, x_n\}$ $\lim\limits_{p\to-\infty}\chi(p)=\min\{x_1,x_2,\cdots, x_n\}$ I don't have any idea to prove them!","Let $p\neq0$ and $j=1,2,\cdots,n$ and $x_j>0$ and $$\chi(p)=\left(\frac{1}{n}\sum_{j=1}^nx_j^p\right)^\frac{1}{p}.$$ Prove that $\chi$ is strictly increasing and the following statements hold $\lim\limits_{p\to0}\chi(p)=(x_1x_2\cdots x_n)^\frac{1}{n}$ $\lim\limits_{p\to+\infty}\chi(p)=\max\{x_1,x_2,\cdots, x_n\}$ $\lim\limits_{p\to-\infty}\chi(p)=\min\{x_1,x_2,\cdots, x_n\}$ I don't have any idea to prove them!",,"['real-analysis', 'limits', 'inequality', 'lp-spaces', 'means']"
65,Find compressed form for cumbersome calculation,Find compressed form for cumbersome calculation,,"Given the three functions $u^{\mathrm{(I)}}(t)\;=t \left(t^2\right)^{k}\,e^{2\beta t^2},\\ u^{\mathrm{(II)}}(t)=\sqrt{\left(t^2\right)^{2k}-\left(t^2\right)^{2k+1}}\,e^{2\beta t^2},\\ u^{\mathrm{(III)}}(t)\,=\sqrt{\left(t^2\right)^{2k+1}-\left(t^2\right)^{2k+2}}\,e^{2\beta t^2}$ for $k \in \{0,...,n\}$ and $C,\beta \in \mathbb{R}$ fixed. such that $$u'''(t)^{\mathrm{(x)}}-\frac{t}{(1-t)}u'(t)^{\mathrm{(x)}}+4\left(A_n^{\mathrm{(x)}}\frac{(2t)}{(1-t^2)}+\ \beta \frac{(2t)^2}{(1-t^2)}+ \frac{C}{(1-t^2)}\right)u(t)^{\mathrm{(x)}}=0$$ where $A_n^{\mathrm{(I)}} = n\\$ $A_n^{\mathrm{(II)}} = 2n\\$ $A_n^{\mathrm{(III)}} = 3n\\$ Now, I was wondering whether it is possible to find an explicit representation for this, such that for every $k \in \{0,...,n\}$ and $(I,II,III)$ you end up with an equation that looks like $c_0 (t^2)^{k-1} + c_1 (t^2)^{k} +c_2 (t^2)^{k+1} = 0$?","Given the three functions $u^{\mathrm{(I)}}(t)\;=t \left(t^2\right)^{k}\,e^{2\beta t^2},\\ u^{\mathrm{(II)}}(t)=\sqrt{\left(t^2\right)^{2k}-\left(t^2\right)^{2k+1}}\,e^{2\beta t^2},\\ u^{\mathrm{(III)}}(t)\,=\sqrt{\left(t^2\right)^{2k+1}-\left(t^2\right)^{2k+2}}\,e^{2\beta t^2}$ for $k \in \{0,...,n\}$ and $C,\beta \in \mathbb{R}$ fixed. such that $$u'''(t)^{\mathrm{(x)}}-\frac{t}{(1-t)}u'(t)^{\mathrm{(x)}}+4\left(A_n^{\mathrm{(x)}}\frac{(2t)}{(1-t^2)}+\ \beta \frac{(2t)^2}{(1-t^2)}+ \frac{C}{(1-t^2)}\right)u(t)^{\mathrm{(x)}}=0$$ where $A_n^{\mathrm{(I)}} = n\\$ $A_n^{\mathrm{(II)}} = 2n\\$ $A_n^{\mathrm{(III)}} = 3n\\$ Now, I was wondering whether it is possible to find an explicit representation for this, such that for every $k \in \{0,...,n\}$ and $(I,II,III)$ you end up with an equation that looks like $c_0 (t^2)^{k-1} + c_1 (t^2)^{k} +c_2 (t^2)^{k+1} = 0$?",,"['calculus', 'real-analysis']"
66,Sum of divergent and convergent sequence proof.,Sum of divergent and convergent sequence proof.,,"Suppose $(s_n)$ is a convergent sequence and $(t_n)$ diverges to $\infty$. Prove that lim $s_n+t_n = \infty$. Proof (can someone verify it?): Pick $N_1$ such that $\forall n > N_1$, $|s_n-s|<1$. Then, $\forall n > N_1$, $s_n > s-1$. Now, let $M > 0$. Pick $N_2$ such that $\forall n > N_2$, $t_n > M -s+1$. Let $N = $ max$\{N_1, N_2\}$. Then $\forall n > N$, $t_n+s-1>M$. So, $t_n+s_n > M$ Therefore, lim $s_n + t_n = \infty$","Suppose $(s_n)$ is a convergent sequence and $(t_n)$ diverges to $\infty$. Prove that lim $s_n+t_n = \infty$. Proof (can someone verify it?): Pick $N_1$ such that $\forall n > N_1$, $|s_n-s|<1$. Then, $\forall n > N_1$, $s_n > s-1$. Now, let $M > 0$. Pick $N_2$ such that $\forall n > N_2$, $t_n > M -s+1$. Let $N = $ max$\{N_1, N_2\}$. Then $\forall n > N$, $t_n+s-1>M$. So, $t_n+s_n > M$ Therefore, lim $s_n + t_n = \infty$",,"['real-analysis', 'proof-verification']"
67,On derivatives that are not Riemann integrable,On derivatives that are not Riemann integrable,,"Let $f:\;[a,b]\to\mathbb{R}$ be differentiable on $[a,b]$. It is not a mystery that $f'$ need not be Riemann integrable. In fact even if we require $f'$ to be bounded the implication is still false. This had me wondering: What is the weakest condition on $f$ that will guarantee a Riemann integrable derivative?","Let $f:\;[a,b]\to\mathbb{R}$ be differentiable on $[a,b]$. It is not a mystery that $f'$ need not be Riemann integrable. In fact even if we require $f'$ to be bounded the implication is still false. This had me wondering: What is the weakest condition on $f$ that will guarantee a Riemann integrable derivative?",,"['real-analysis', 'integration', 'analysis', 'derivatives']"
68,Show that the interior of a set $S^0$ is open.,Show that the interior of a set  is open.,S^0,"$S^0$  is the interior of a set $S$. Let $x\in S^0$ be given. We want to find $\delta>0$ such that $(x-\delta,x+\delta)\subset S^0$. $S^0$ is the interior of $S$, then $S^0\subset S$, then $x\in S$ and $\exists \delta_1>0$ such that $(x-\delta_1,x+\delta_1)\subset S$. Can we say choose $\delta<\delta_1$  such that $(x-\delta,x+\delta)\subset(x-\delta_1,x+\delta_1)$, then $(x-\delta,x+\delta)\subset S^0$, therefore the interior of $S$ is open? Does it make sense? Please guide me.","$S^0$  is the interior of a set $S$. Let $x\in S^0$ be given. We want to find $\delta>0$ such that $(x-\delta,x+\delta)\subset S^0$. $S^0$ is the interior of $S$, then $S^0\subset S$, then $x\in S$ and $\exists \delta_1>0$ such that $(x-\delta_1,x+\delta_1)\subset S$. Can we say choose $\delta<\delta_1$  such that $(x-\delta,x+\delta)\subset(x-\delta_1,x+\delta_1)$, then $(x-\delta,x+\delta)\subset S^0$, therefore the interior of $S$ is open? Does it make sense? Please guide me.",,"['real-analysis', 'general-topology']"
69,Integral with respect to greatest integer function,Integral with respect to greatest integer function,,"Assume $f$ is continuous on $[1,n]$. How would you go about taking the integral  $$\int_1^n f(x)\,d\lfloor x\rfloor$$ where $\lfloor x\rfloor$ represents the greatest integer function?","Assume $f$ is continuous on $[1,n]$. How would you go about taking the integral  $$\int_1^n f(x)\,d\lfloor x\rfloor$$ where $\lfloor x\rfloor$ represents the greatest integer function?",,['real-analysis']
70,"Find $\lim_{n\to\infty}\int_{-\pi}^{\pi}f(t)\cos^2(nt) \,dt$",Find,"\lim_{n\to\infty}\int_{-\pi}^{\pi}f(t)\cos^2(nt) \,dt","Let $f \in C[-\pi,\pi]$. Find the following limit: $$\lim_{n\to\infty}\int_{-\pi}^{\pi}f(t)\cos^2(nt)\,dt$$","Let $f \in C[-\pi,\pi]$. Find the following limit: $$\lim_{n\to\infty}\int_{-\pi}^{\pi}f(t)\cos^2(nt)\,dt$$",,"['real-analysis', 'analysis', 'limits', 'fourier-series']"
71,"Show that $\max_{x\in[a,b]}|f'(x)|\geq\frac{4}{(b-a)^2}\int_a^b|f(x)|dx$ [duplicate]",Show that  [duplicate],"\max_{x\in[a,b]}|f'(x)|\geq\frac{4}{(b-a)^2}\int_a^b|f(x)|dx","This question already has answers here : Prove that $|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2}$ (2 answers) Closed 10 years ago . Let $f:[a,b]\to\mathbb{R}$ be differentiable and $f'$ is continuous. Suppose $f(a)=f(b)=0$, show that $$\max_{x\in[a,b]}|f'(x)|\geq\frac{4}{(b-a)^2}\int_a^b|f(x)|dx$$ My approach. For any $x$, by Taylor's theorem there is $\xi\in(a,x)$ s.t. $$|f(x)|=|f(a)+f'(\xi)(x-a)|\leq\max_{x\in[a,b]}|f'(x)|(x-a)$$ Hence $$\int_a^b |f(x)|dx\leq\max_{x\in[a,b]}|f'(x)|\int_a^bx-adx=\frac{(b-a)^2}{2}\max_{x\in[a,b]}|f'(x)|$$ I can only get this, and I don't know how to obtain $\frac{(b-a)^2}{4}$.","This question already has answers here : Prove that $|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2}$ (2 answers) Closed 10 years ago . Let $f:[a,b]\to\mathbb{R}$ be differentiable and $f'$ is continuous. Suppose $f(a)=f(b)=0$, show that $$\max_{x\in[a,b]}|f'(x)|\geq\frac{4}{(b-a)^2}\int_a^b|f(x)|dx$$ My approach. For any $x$, by Taylor's theorem there is $\xi\in(a,x)$ s.t. $$|f(x)|=|f(a)+f'(\xi)(x-a)|\leq\max_{x\in[a,b]}|f'(x)|(x-a)$$ Hence $$\int_a^b |f(x)|dx\leq\max_{x\in[a,b]}|f'(x)|\int_a^bx-adx=\frac{(b-a)^2}{2}\max_{x\in[a,b]}|f'(x)|$$ I can only get this, and I don't know how to obtain $\frac{(b-a)^2}{4}$.",,"['calculus', 'real-analysis']"
72,Intuition for a function that belongs to a $L^p$ space,Intuition for a function that belongs to a  space,L^p,"Does a function $f(x):[0,\infty)\rightarrow R$ with $f\in L^p$ for $p<\infty$ have to die down to $0$ as $x\rightarrow \infty$? I some how feel that the $L^p$ norm exist only when the function dies down to $0$ and the rate at which it dies down to $0$ depends on $p$. Is this right?","Does a function $f(x):[0,\infty)\rightarrow R$ with $f\in L^p$ for $p<\infty$ have to die down to $0$ as $x\rightarrow \infty$? I some how feel that the $L^p$ norm exist only when the function dies down to $0$ and the rate at which it dies down to $0$ depends on $p$. Is this right?",,"['real-analysis', 'lp-spaces']"
73,$F(xy) = F(x)+F(y)$ Proof,Proof,F(xy) = F(x)+F(y),"Suppose $F$ is differentiable $\forall x>0$ and $F(xy) = F(x)+F(y)$ , $ \forall x,y>0$ . Prove that if $F$ is not the zero function, then $\exists$ $  a>0$ such that: $F(x)=\log_a(x)$ , $\forall x>0$ . I seem to be doing fine except on getting the base $a$ for the log. So far I have that $F'(x)=\frac{F'(1)}{x}$ . I know from calculus that $\int\frac{1}{t}dt=\ln(t)$ . How can I get the base to be $F'(1)$ instead of $e$ ?","Suppose is differentiable and , . Prove that if is not the zero function, then such that: , . I seem to be doing fine except on getting the base for the log. So far I have that . I know from calculus that . How can I get the base to be instead of ?","F \forall x>0 F(xy) = F(x)+F(y)  \forall x,y>0 F \exists   a>0 F(x)=\log_a(x) \forall x>0 a F'(x)=\frac{F'(1)}{x} \int\frac{1}{t}dt=\ln(t) F'(1) e","['real-analysis', 'derivatives', 'functional-equations']"
74,Question on Riemann sums,Question on Riemann sums,,"Question is : What I have read in Riemann sum definition is something like $$\sum_{i=1}^n f(y) .|x_i-x_{i-1}|$$ So, at first sight i am afraid this is not even related to Riemann integration of $f$ and then I got something like : $|f(x_i)-f(x_{i-1})|$ being seen as  $$\frac{|f(x_i)-f(x_{i-1})|}{|(x_i-x_{i-1})|}.|(x_i-x_{i-1})|$$ and this is same as $$f'(y_i) |x_i-x_{i-1}|\text{for some $y_i \in (x_{i-1}, x_i)$}$$ So, I would now be left with $$S(P)=\sum_{i=1}^n |f(x_i)-f(x_{i-1})|=\sum_{i=1}^n\frac{|f(x_i)-f(x_{i-1})|}{|(x_i-x_{i-1})|}.|(x_i-x_{i-1})|=\sum_{i=1}^nf'(y_i) |x_i-x_{i-1}|$$ and i see this is Riemann sum for $f'(x)$ on $[0,1]$ So, I would like to say that $$S(P)= \int_{0}^1 |f'(x)|$$ I am brand new for this Riemann integration problems (this might be third of fourth problem i have tried in this topic). So, I would be thankful if some one can assure my reasons for this problem are correct and precise. Thank you","Question is : What I have read in Riemann sum definition is something like $$\sum_{i=1}^n f(y) .|x_i-x_{i-1}|$$ So, at first sight i am afraid this is not even related to Riemann integration of $f$ and then I got something like : $|f(x_i)-f(x_{i-1})|$ being seen as  $$\frac{|f(x_i)-f(x_{i-1})|}{|(x_i-x_{i-1})|}.|(x_i-x_{i-1})|$$ and this is same as $$f'(y_i) |x_i-x_{i-1}|\text{for some $y_i \in (x_{i-1}, x_i)$}$$ So, I would now be left with $$S(P)=\sum_{i=1}^n |f(x_i)-f(x_{i-1})|=\sum_{i=1}^n\frac{|f(x_i)-f(x_{i-1})|}{|(x_i-x_{i-1})|}.|(x_i-x_{i-1})|=\sum_{i=1}^nf'(y_i) |x_i-x_{i-1}|$$ and i see this is Riemann sum for $f'(x)$ on $[0,1]$ So, I would like to say that $$S(P)= \int_{0}^1 |f'(x)|$$ I am brand new for this Riemann integration problems (this might be third of fourth problem i have tried in this topic). So, I would be thankful if some one can assure my reasons for this problem are correct and precise. Thank you",,"['real-analysis', 'integration']"
75,How to best understand Euclid's definition of equal ratios? How does it relate to Dedekind cuts?,How to best understand Euclid's definition of equal ratios? How does it relate to Dedekind cuts?,,"This is something I've been wondering about. When I think of ""ratios"" $x/y$ and $z/w$ as being ""equal"", with $x$, $y$, $z$, and $w$ being real numbers, this means the results of dividing the real numbers $x$ by $y$ and $z$ by $w$ are equal. Or that $xw = yz$, from manipulation of the fractions. Intuitively, we may say this means the ""scale factor"" going from $y$ to $x$ is the same as that going from $w$ to $z$, or that $x$ has as many ""units"" of size $y$ (allowing for non-integral numbers of units) as $z$ has of size $w$. Yet, Euclid (~300 BCE) did not have real-number arithmetic to work with. Instead he had various kinds of ""magnitudes"", like line segments and shapes with areas and other things that had a kind of ""size"" to them. So he had to do something else, and this I don't get. If we have ""magnitudes"" $x$, $y$, $z$, and $w$, which for modern purposes could be taken as nonnegative real numbers, then we say $x/y = z/w$ iff for every pair of nonzero natural numbers $m$ and $n$, $mx < ny \rightarrow mz < nw$, $mx = ny \rightarrow mz = nw$, and $mx > ny \rightarrow mz > nw$. But how does one intuitively grasp this definition? How does it relate to our modern one? On Wikipedia, it says also ""There is a remarkable similarity between this definition and the theory of Dedekind cuts used in the modern definition of irrational numbers."" How exactly does this relate to Dedekind cuts? (this last bit is why I also file this under real analysis)","This is something I've been wondering about. When I think of ""ratios"" $x/y$ and $z/w$ as being ""equal"", with $x$, $y$, $z$, and $w$ being real numbers, this means the results of dividing the real numbers $x$ by $y$ and $z$ by $w$ are equal. Or that $xw = yz$, from manipulation of the fractions. Intuitively, we may say this means the ""scale factor"" going from $y$ to $x$ is the same as that going from $w$ to $z$, or that $x$ has as many ""units"" of size $y$ (allowing for non-integral numbers of units) as $z$ has of size $w$. Yet, Euclid (~300 BCE) did not have real-number arithmetic to work with. Instead he had various kinds of ""magnitudes"", like line segments and shapes with areas and other things that had a kind of ""size"" to them. So he had to do something else, and this I don't get. If we have ""magnitudes"" $x$, $y$, $z$, and $w$, which for modern purposes could be taken as nonnegative real numbers, then we say $x/y = z/w$ iff for every pair of nonzero natural numbers $m$ and $n$, $mx < ny \rightarrow mz < nw$, $mx = ny \rightarrow mz = nw$, and $mx > ny \rightarrow mz > nw$. But how does one intuitively grasp this definition? How does it relate to our modern one? On Wikipedia, it says also ""There is a remarkable similarity between this definition and the theory of Dedekind cuts used in the modern definition of irrational numbers."" How exactly does this relate to Dedekind cuts? (this last bit is why I also file this under real analysis)",,"['real-analysis', 'arithmetic', 'fractions']"
76,Proof that all Cauchy sequences converge,Proof that all Cauchy sequences converge,,"I am supposed to prove that all Cauchy sequences converge, using the fact that a sequence converges iff its lim sup equals its lim inf. I think the proof I have works, but the one given in the solutions is different, so I'd like to make sure. Also, this is for a calculus class, not real analysis, so I don't know about metric spaces or any of that. I believe the problem pertains solely to sequences in $\mathbb{R}$. We have the Cauchy sequence $\{a_n\}$. By the definition of the Cauchy sequence, we have $$\left|a_m-a_n\right|<\frac{\epsilon}{3}$$ for a given $\epsilon>0$ and $m,n>N$. There exist the numbers $\inf_{n>N}\{a_n\}$ and $\sup_{n>N}\{a_n\}$, that is to say, the infimum and supremum of the sequence with the first $N$ terms truncated. By the definition of the infimum, we must have $$\left| a_{low}-\inf_{n>N}\{a_n\}\right|<\frac{\epsilon}{3}$$ for some $a_{low} \in \{a_n|n>N\}$, because otherwise $\inf_{n>N}\{a_n\}+\frac{\epsilon}{3}$ would be a greater lower bound for $\{a_n|n>N\}$ than the infimum, a contradiction. By the same reasoning, we also have $$\left|\sup_{n>N}\{a_n\}-a_{high}\right|<\frac{\epsilon}{3}$$ for some $a_{high} \in \{a_n|n>N\}$. Now, by the definition of the Cauchy sequence, we must have $$\left|a_{high}-a_{low}\right|<\frac{\epsilon}{3}$$ Expanding the absolute value terms gives $$-\frac{\epsilon}{3}<a_{low}-\inf_{n>N}\{a_n\}<\frac{\epsilon}{3}$$ $$-\frac{\epsilon}{3}<\sup_{n>N}\{a_n\}-a_{high}<\frac{\epsilon}{3}$$ $$-\frac{\epsilon}{3}<a_{high}-a_{low}<\frac{\epsilon}{3}$$ Adding these together gives $$-\epsilon<\sup_{n>N}\{a_n\}-\inf_{n>N}\{a_n\}<\epsilon$$ So $$\left|\sup_{n>N}\{a_n\}-\inf_{n>N}\{a_n\}\right|<\epsilon$$ which, we remember, holds for $n>N$. By the definition of the infinite limit, then, because we can make the difference of the infimum and the supremum arbitrarily close to zero by increasing $N$, we have $$\lim_{N \rightarrow \infty}\left( \sup_{n>N}\{a_n\}-\inf_{n>N}\{a_n\} \right)=\lim_{N \rightarrow \infty}\sup_{n>N}\{a_n\}-\lim_{N \rightarrow \infty}\inf_{n>N}\{a_n\}=0$$ So the lim inf of any Cauchy sequence equals its lim sup, and therefore it converges.","I am supposed to prove that all Cauchy sequences converge, using the fact that a sequence converges iff its lim sup equals its lim inf. I think the proof I have works, but the one given in the solutions is different, so I'd like to make sure. Also, this is for a calculus class, not real analysis, so I don't know about metric spaces or any of that. I believe the problem pertains solely to sequences in $\mathbb{R}$. We have the Cauchy sequence $\{a_n\}$. By the definition of the Cauchy sequence, we have $$\left|a_m-a_n\right|<\frac{\epsilon}{3}$$ for a given $\epsilon>0$ and $m,n>N$. There exist the numbers $\inf_{n>N}\{a_n\}$ and $\sup_{n>N}\{a_n\}$, that is to say, the infimum and supremum of the sequence with the first $N$ terms truncated. By the definition of the infimum, we must have $$\left| a_{low}-\inf_{n>N}\{a_n\}\right|<\frac{\epsilon}{3}$$ for some $a_{low} \in \{a_n|n>N\}$, because otherwise $\inf_{n>N}\{a_n\}+\frac{\epsilon}{3}$ would be a greater lower bound for $\{a_n|n>N\}$ than the infimum, a contradiction. By the same reasoning, we also have $$\left|\sup_{n>N}\{a_n\}-a_{high}\right|<\frac{\epsilon}{3}$$ for some $a_{high} \in \{a_n|n>N\}$. Now, by the definition of the Cauchy sequence, we must have $$\left|a_{high}-a_{low}\right|<\frac{\epsilon}{3}$$ Expanding the absolute value terms gives $$-\frac{\epsilon}{3}<a_{low}-\inf_{n>N}\{a_n\}<\frac{\epsilon}{3}$$ $$-\frac{\epsilon}{3}<\sup_{n>N}\{a_n\}-a_{high}<\frac{\epsilon}{3}$$ $$-\frac{\epsilon}{3}<a_{high}-a_{low}<\frac{\epsilon}{3}$$ Adding these together gives $$-\epsilon<\sup_{n>N}\{a_n\}-\inf_{n>N}\{a_n\}<\epsilon$$ So $$\left|\sup_{n>N}\{a_n\}-\inf_{n>N}\{a_n\}\right|<\epsilon$$ which, we remember, holds for $n>N$. By the definition of the infinite limit, then, because we can make the difference of the infimum and the supremum arbitrarily close to zero by increasing $N$, we have $$\lim_{N \rightarrow \infty}\left( \sup_{n>N}\{a_n\}-\inf_{n>N}\{a_n\} \right)=\lim_{N \rightarrow \infty}\sup_{n>N}\{a_n\}-\lim_{N \rightarrow \infty}\inf_{n>N}\{a_n\}=0$$ So the lim inf of any Cauchy sequence equals its lim sup, and therefore it converges.",,['real-analysis']
77,Decomposition of $C_0^{\infty}(\mathbb{R}^n)$ -function,Decomposition of  -function,C_0^{\infty}(\mathbb{R}^n),"I got the following question as part of a fourier-analysis course.. Consider $\phi\in C_0^{\infty}(\mathbb{R}^n)$ with $\phi(0)=0$. Apparantly then we can write $$\phi =\sum_{j=1}^nx_j\psi_j $$ for functions $\psi_j$ in the same space, and I would like to prove this. However I'm little stuck on this. The steps would involve that we start integrating $$\int_0^{x_1}D_1\phi(t,x_2,\cdots, x_n)dt+\phi(0,x_2,\cdots, x_n) $$ and continue doing so w.r.t. to the other variables, and change the interval of integration to [0,1]. Then get everything in $C_0^{\infty}(\mathbb{R}^n)$ by writing $$\phi = \sum \frac{x_i^2\phi(x)}{\left\|x\right\|^2 } $$  and patch everything together with a partition of unity. I just dont quite see what they mean...A partition of unity is a sequence of functions that sums up to 1 for all $x\in \mathbb{R}^n$.","I got the following question as part of a fourier-analysis course.. Consider $\phi\in C_0^{\infty}(\mathbb{R}^n)$ with $\phi(0)=0$. Apparantly then we can write $$\phi =\sum_{j=1}^nx_j\psi_j $$ for functions $\psi_j$ in the same space, and I would like to prove this. However I'm little stuck on this. The steps would involve that we start integrating $$\int_0^{x_1}D_1\phi(t,x_2,\cdots, x_n)dt+\phi(0,x_2,\cdots, x_n) $$ and continue doing so w.r.t. to the other variables, and change the interval of integration to [0,1]. Then get everything in $C_0^{\infty}(\mathbb{R}^n)$ by writing $$\phi = \sum \frac{x_i^2\phi(x)}{\left\|x\right\|^2 } $$  and patch everything together with a partition of unity. I just dont quite see what they mean...A partition of unity is a sequence of functions that sums up to 1 for all $x\in \mathbb{R}^n$.",,"['real-analysis', 'fourier-analysis']"
78,How to find a measurable but not integrable function or a positive integrable function?,How to find a measurable but not integrable function or a positive integrable function?,,"For an arbitrary interval $I$, how can we find a positive on $I$ integrable function? And how does one construct a measurable but not integrable function. If not all measurable functions are integrable, what is the best way to determine if a function is L-Integrable? thx in advance","For an arbitrary interval $I$, how can we find a positive on $I$ integrable function? And how does one construct a measurable but not integrable function. If not all measurable functions are integrable, what is the best way to determine if a function is L-Integrable? thx in advance",,"['real-analysis', 'analysis', 'lebesgue-integral']"
79,Continuity and limit,Continuity and limit,,"$f: (0,\infty) \rightarrow \mathbb{R}$ is continuous and $f(x) \le f(nx) \  \ \forall{{x>0},{n\in\mathbb{N}}} $ Prove that f has limit (may be infinity) Don't know what to do maybe something with Darboux property.","$f: (0,\infty) \rightarrow \mathbb{R}$ is continuous and $f(x) \le f(nx) \  \ \forall{{x>0},{n\in\mathbb{N}}} $ Prove that f has limit (may be infinity) Don't know what to do maybe something with Darboux property.",,"['real-analysis', 'limits']"
80,When is Hardy's inequality a strict inequality?,When is Hardy's inequality a strict inequality?,,"Let $1<p<\infty$, $f\in L^{p}(0,\infty)$ and $$F(x)=\frac{1}{x}\int^{x}_{0}f(t)dt$$ Hardy's inequality states $$|F|_{p}\leq \frac{p}{p-1}|f|_{p}$$ To show the bound is sharp Rudin suggested to use $f(x)=x^{-1/p}$ on $[1,A]$ and $0$ otherwise. Then let $A$ be arbitrally large. But I am at loss how to show this rigorously. So we have $F(x)=\frac{1}{x}\int^{x}_{1}\frac{1}{t^{1/p}}dt$ for $A\ge x\ge 1$, for $x<1$ this is $0$. For $x> A$ we have $$F(x)=\frac{1}{x}\int^{A}_{1}\frac{1}{t^{1/p}}dt$$ To fixed the formula we have $$ \int^{x}_{1}\frac{1}{t^{1/p}}dt=\frac{p}{p-1}t^{\frac{p-1}{p}}|^{x}_{1}=\frac{p}{p-1}(x^{\frac{p-1}{p}}-1) $$ Putting together we have $$|F|_{p}=(\int_{1}^{A}F^{p}+\int_{A}^{\infty}F^{p})^{1/p}$$ and the first half is $$C\int^{A}_{0}\frac{1}{x^{p}}(x^{\frac{p-1}{p}}-1)^{p}=C\int^{A}_{1}(x^{-1/p}-1/x)^{p}$$ here $C=(\frac{p}{p-1})^{p}$. The second half is $$C \cdot D^{p} \cdot \int_A^\infty \frac{1}{x^p} \, dx  = C \cdot \frac{1}{p-1} \cdot A \cdot \left(A^{-\frac{1}{p}}- \frac{1}{A} \right)^p$$ where $D=(A^{\frac{p-1}{p}}-1)$. But now I do not know how to evaluate $$\int^{A}_{1} (x^{-1/p}-1/x)^{p}$$ though it is clear that $x^{-1/p}\ge \frac{1}{x}$ and hence this can be bound from above by $\log[A]$. If we ignore the second part, then first part of the integral is less than $$\frac{p}{p-1}\log[A]^{1/p}$$ whereas the right hand side $$\frac{p}{p-1}|f|_{p}=\frac{p}{p-1}(\int^{A}_{1}\frac{1}{x})^{1/p}=\frac{p}{p-1}\log[A]^{1/p}$$ But this ignored the second part. So I am wondering how to fix it. What I want to show after fixing all the constants is $$[\int^{A}_{1}(x^{-1/p}-1/x)^{p}]+\frac{1}{p-1} \cdot A \cdot \left(A^{-\frac{1}{p}}- \frac{1}{A} \right)^p)\le \log[A]$$  Now since $A$ is very large the $\frac{1}{A}$ factor is small. So the left hand side become $$[\int^{A}_{1}(x^{-1/p}-1/x)^{p}]+\frac{K}{p-1}\le \log[A]$$ where $K$ is some constant that can be arbitrally close to 1 as we select $A$. So now all we need is to prove $$\int^{A}_{1}(x^{-1/p}-1/x)^{p}\le Log[A]-\frac{1}{p-1}$$ where $A$ is a large enough constant. This problem address the identical question following a different hint.","Let $1<p<\infty$, $f\in L^{p}(0,\infty)$ and $$F(x)=\frac{1}{x}\int^{x}_{0}f(t)dt$$ Hardy's inequality states $$|F|_{p}\leq \frac{p}{p-1}|f|_{p}$$ To show the bound is sharp Rudin suggested to use $f(x)=x^{-1/p}$ on $[1,A]$ and $0$ otherwise. Then let $A$ be arbitrally large. But I am at loss how to show this rigorously. So we have $F(x)=\frac{1}{x}\int^{x}_{1}\frac{1}{t^{1/p}}dt$ for $A\ge x\ge 1$, for $x<1$ this is $0$. For $x> A$ we have $$F(x)=\frac{1}{x}\int^{A}_{1}\frac{1}{t^{1/p}}dt$$ To fixed the formula we have $$ \int^{x}_{1}\frac{1}{t^{1/p}}dt=\frac{p}{p-1}t^{\frac{p-1}{p}}|^{x}_{1}=\frac{p}{p-1}(x^{\frac{p-1}{p}}-1) $$ Putting together we have $$|F|_{p}=(\int_{1}^{A}F^{p}+\int_{A}^{\infty}F^{p})^{1/p}$$ and the first half is $$C\int^{A}_{0}\frac{1}{x^{p}}(x^{\frac{p-1}{p}}-1)^{p}=C\int^{A}_{1}(x^{-1/p}-1/x)^{p}$$ here $C=(\frac{p}{p-1})^{p}$. The second half is $$C \cdot D^{p} \cdot \int_A^\infty \frac{1}{x^p} \, dx  = C \cdot \frac{1}{p-1} \cdot A \cdot \left(A^{-\frac{1}{p}}- \frac{1}{A} \right)^p$$ where $D=(A^{\frac{p-1}{p}}-1)$. But now I do not know how to evaluate $$\int^{A}_{1} (x^{-1/p}-1/x)^{p}$$ though it is clear that $x^{-1/p}\ge \frac{1}{x}$ and hence this can be bound from above by $\log[A]$. If we ignore the second part, then first part of the integral is less than $$\frac{p}{p-1}\log[A]^{1/p}$$ whereas the right hand side $$\frac{p}{p-1}|f|_{p}=\frac{p}{p-1}(\int^{A}_{1}\frac{1}{x})^{1/p}=\frac{p}{p-1}\log[A]^{1/p}$$ But this ignored the second part. So I am wondering how to fix it. What I want to show after fixing all the constants is $$[\int^{A}_{1}(x^{-1/p}-1/x)^{p}]+\frac{1}{p-1} \cdot A \cdot \left(A^{-\frac{1}{p}}- \frac{1}{A} \right)^p)\le \log[A]$$  Now since $A$ is very large the $\frac{1}{A}$ factor is small. So the left hand side become $$[\int^{A}_{1}(x^{-1/p}-1/x)^{p}]+\frac{K}{p-1}\le \log[A]$$ where $K$ is some constant that can be arbitrally close to 1 as we select $A$. So now all we need is to prove $$\int^{A}_{1}(x^{-1/p}-1/x)^{p}\le Log[A]-\frac{1}{p-1}$$ where $A$ is a large enough constant. This problem address the identical question following a different hint.",,"['real-analysis', 'integral-inequality']"
81,"Consider $f_n(x) = \sum_{k=0}^{n} {x^k}$. Does $f_n$ converge pointwise on $[0,1]$?",Consider . Does  converge pointwise on ?,"f_n(x) = \sum_{k=0}^{n} {x^k} f_n [0,1]","Consider $f_n(x) = \sum_{k=0}^{n} {x^k}$. Does $f_n$ converge pointwise on $[0,1]$? Does it converge uniformly on $[0,1]$? Well, my approach would be, first of all to notice that if $x \neq 1$, then by simple induction we get $$f_n(x) = \frac{1 - x^{n+1}}{1-x}$$ So, $(f_n) \rightarrow \frac{1}{1-x}$. But at $x = 1$, the $f_n \rightarrow \infty$, hence $(f_n)$ does not converge pointwise. Therefore, it does not converge uniformly. Is this correct? Hope to get feedback thanks,","Consider $f_n(x) = \sum_{k=0}^{n} {x^k}$. Does $f_n$ converge pointwise on $[0,1]$? Does it converge uniformly on $[0,1]$? Well, my approach would be, first of all to notice that if $x \neq 1$, then by simple induction we get $$f_n(x) = \frac{1 - x^{n+1}}{1-x}$$ So, $(f_n) \rightarrow \frac{1}{1-x}$. But at $x = 1$, the $f_n \rightarrow \infty$, hence $(f_n)$ does not converge pointwise. Therefore, it does not converge uniformly. Is this correct? Hope to get feedback thanks,",,"['calculus', 'real-analysis']"
82,question about derivative of exponential function,question about derivative of exponential function,,When I proved derivation the exponential function expose with problem that have to use derivative of $e^x$ $$\frac{de^x}{dx} = \lim_{h\to 0}\frac{e^{x+h} -e^x}h=\lim_{h\to 0} e^x  \frac{e^h-1}h =e^x \cdot \lim_{h\to 0}  \frac{e^h-1}h$$ Calculate  $\displaystyle\lim_{h\to 0}  \frac{e^h-1}h$ but cant use lhopital theorem and Taylors theorem because use derivative of $e^x$ . Please help me to solve it.,When I proved derivation the exponential function expose with problem that have to use derivative of $e^x$ $$\frac{de^x}{dx} = \lim_{h\to 0}\frac{e^{x+h} -e^x}h=\lim_{h\to 0} e^x  \frac{e^h-1}h =e^x \cdot \lim_{h\to 0}  \frac{e^h-1}h$$ Calculate  $\displaystyle\lim_{h\to 0}  \frac{e^h-1}h$ but cant use lhopital theorem and Taylors theorem because use derivative of $e^x$ . Please help me to solve it.,,['real-analysis']
83,Bounding Error term in Taylor Expansion of $\sqrt{1+x}$,Bounding Error term in Taylor Expansion of,\sqrt{1+x},"I am attempting to justify the expansion $$ \sqrt{1+x}= 1 + \frac{x}{2} + \sum_{n=2}^{\infty}{(-1)^n \frac{1}{2n}\frac{(1-\frac{1}{2}) \cdots ((n-1)-\frac{1}{2})}{(n-1)!}x^n} $$ for $-1&ltx\leq 1$ I've got the expansion, but I cannot prove that the error term tends to zero. $$ E_n = \frac{(-1)^{n-1}(2n-3)(2n-1) \cdots (1)}{2^n n!}(1+\theta x)^{-\frac{2n-1}{2}}x^n $$ where $\theta \in (0,1)$ The question suggests using the Constancy Lemma (if the differential is zero, the function is constant), but I can't make that work either. Any help gratefully appreciated. (While technically this is homework I'm the tutor so I allowed to cheat! Also it is extra embarrassing that I cannot do it)","I am attempting to justify the expansion $$ \sqrt{1+x}= 1 + \frac{x}{2} + \sum_{n=2}^{\infty}{(-1)^n \frac{1}{2n}\frac{(1-\frac{1}{2}) \cdots ((n-1)-\frac{1}{2})}{(n-1)!}x^n} $$ for $-1&ltx\leq 1$ I've got the expansion, but I cannot prove that the error term tends to zero. $$ E_n = \frac{(-1)^{n-1}(2n-3)(2n-1) \cdots (1)}{2^n n!}(1+\theta x)^{-\frac{2n-1}{2}}x^n $$ where $\theta \in (0,1)$ The question suggests using the Constancy Lemma (if the differential is zero, the function is constant), but I can't make that work either. Any help gratefully appreciated. (While technically this is homework I'm the tutor so I allowed to cheat! Also it is extra embarrassing that I cannot do it)",,"['real-analysis', 'sequences-and-series', 'taylor-expansion']"
84,Limit Summation interchanging,Limit Summation interchanging,,"Is there a theorem which says when we can interchange the limit and sum as follow: $$\lim_{x\to \infty} \sum_{n=1}^{\infty}f(x,n)= \sum_{n=1}^{\infty}\lim_{x\to \infty}f(x,n)$$ Note: In my case the sum  $\sum_{n=1}^{\infty}f(x,n)$ is finite at each finite $x\in \mathbb R$.","Is there a theorem which says when we can interchange the limit and sum as follow: $$\lim_{x\to \infty} \sum_{n=1}^{\infty}f(x,n)= \sum_{n=1}^{\infty}\lim_{x\to \infty}f(x,n)$$ Note: In my case the sum  $\sum_{n=1}^{\infty}f(x,n)$ is finite at each finite $x\in \mathbb R$.",,['real-analysis']
85,Help to finish my proof: inequality with norm and Schwarz ineq,Help to finish my proof: inequality with norm and Schwarz ineq,,"Let $A=\left [ a_{ij} \right ]$ be the matrix of a linear mapping $A\in L\left ( \mathbb{R}^{n},\mathbb{R}^{m} \right )$. -Prove that: $\left \| A \right \|\leq \left ( \sum_{i=1}^{m}\sum_{j=1}^{n}a{_{ij}}^{2} \right )^{^{\frac{1}{2}}}$ Note that: $\left \| A \right \|=sup_{\left \| x \right \|=1} \left \| Ax \right \|$ My tentative: $A$ is an $m*n$ matrix. Let $A_{i}$ be the $i$th row of $A$. Then the $ith$ row of the vector $Ax$ is the dot product $\left \langle A_{i},x \right \rangle$. Then: $\left \| Ax \right \|=\sqrt{\left \langle  A_{1},x\right \rangle^{2}+\left \langle A_{2},x \right \rangle^{2}+...+\left \langle A_{m},x \right \rangle^{2}}$ and using Schwarz inequality:$\left | \left \langle A_{i},x \right \rangle  \right |< \left \| A_{i} \right \|.\left \| x \right \|$ we get that the left hand side is less than or equal to $\sqrt{\left ( \left \| A_{1} \right \|^{2}+...+\left \| A_{n} \right \|^{2} \right ).\left \| x \right \|^{2}}=\sqrt{\left ( \left \| A_{1} \right \|^{2}+...+\left \| A_{n} \right \|^{2} \right )}$ because $\left \| x \right \|=1$ Is there anything wrong with my proof? Please let me know if there are any details I should add to the proof to finish it off. Also, if you guys have any alternative proofs, please share. Thanks","Let $A=\left [ a_{ij} \right ]$ be the matrix of a linear mapping $A\in L\left ( \mathbb{R}^{n},\mathbb{R}^{m} \right )$. -Prove that: $\left \| A \right \|\leq \left ( \sum_{i=1}^{m}\sum_{j=1}^{n}a{_{ij}}^{2} \right )^{^{\frac{1}{2}}}$ Note that: $\left \| A \right \|=sup_{\left \| x \right \|=1} \left \| Ax \right \|$ My tentative: $A$ is an $m*n$ matrix. Let $A_{i}$ be the $i$th row of $A$. Then the $ith$ row of the vector $Ax$ is the dot product $\left \langle A_{i},x \right \rangle$. Then: $\left \| Ax \right \|=\sqrt{\left \langle  A_{1},x\right \rangle^{2}+\left \langle A_{2},x \right \rangle^{2}+...+\left \langle A_{m},x \right \rangle^{2}}$ and using Schwarz inequality:$\left | \left \langle A_{i},x \right \rangle  \right |< \left \| A_{i} \right \|.\left \| x \right \|$ we get that the left hand side is less than or equal to $\sqrt{\left ( \left \| A_{1} \right \|^{2}+...+\left \| A_{n} \right \|^{2} \right ).\left \| x \right \|^{2}}=\sqrt{\left ( \left \| A_{1} \right \|^{2}+...+\left \| A_{n} \right \|^{2} \right )}$ because $\left \| x \right \|=1$ Is there anything wrong with my proof? Please let me know if there are any details I should add to the proof to finish it off. Also, if you guys have any alternative proofs, please share. Thanks",,"['linear-algebra', 'real-analysis', 'analysis', 'multivariable-calculus']"
86,Is this class of periodic functions closed under the (circular) convolution operation ? Help in proving.,Is this class of periodic functions closed under the (circular) convolution operation ? Help in proving.,,"I am studying the properties of a particular class of functions, and I'd appreciate some help in proving a property of that class. I started with a class of functions and made some modifications to show that it forms a vector space and to show that it is closed under convolution. I have given the entire description below, and require help in proving that the set is closed under the operation of convolution (circular). I have also mentioned what methods I have used to prove that it forms a vector space, so that i can get cleared of the difficulty i am facing in arriving at the result of closure under convolution. ( in case you want to avoid reading the entire post linearly, the property needs to be proved on the set $S_{pc}$ and it is described in the section ""closure under addition""). PS : Please do let me know if you want to know the motivation for such a study. I do not have any concrete reasons which can be expressed in mathematically precise terms, behind the motivation, but I have an intuition behind it. PS 2 : A construction method and proof of non-emptiness of this class of functions is given in this Q&A . Definition Let $f \colon \mathbb{R} \to \mathbb{R}$ be a function, consider a point $x \in \mathbb{R}$ where $f$ is continuous, then the following two statements are equivalent. $Cf(x) = k$, where $k \in \{0\}\bigcup\mathbb{N}\bigcup\{\infty\}$. The function is continuous at $x$ and the maximum number of times $f$ is differentiable at $x$ is $k$. Clarification (Added) $Cf(x)=k$, where we know that $f$ is a function and $x$ is the point in its domain, then it means that the maximum number of times $f$ is differentiable at $x$ is $k$. This is not a good notation as someone could think that $Cf$ itself is one function different from f and not associated with it, but here I intended that $C$ tells about a proprty of the function at a point in its domain.....not a convincing notation......but i'd like know any ideas to make a good notation for it. Definition of a class of functions The set $S$ consists of functions $f \colon (0,1) \to \mathbb{R}$, which satisfy the following properties. Given any $f \in S$ there exist a countable dense subset $D \subset (0,1)$ and maps $k,ck$ defined as $k \colon D \to \mathbb{N}$ and $ck \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ which satisfy the following properties. $\forall n \in \mathbb{N}$, the pre-image $k^{-1}(\{n\})$ is a finite set. $\forall x \in $D$, ck(x) \ge k(x)$ but finite and $\forall x \in (0,1)$\D$, ck(x) = \infty$. $\forall x \in (0,1), Cf(x) = ck(x)$ Whenever $Cf(x) = k$ is finite, for the $(k+1)^{nth}$ derivative of $f$ at $x$, the left and right limits exist and are not equal. (the left and right limits do not diverge). Let $S_p$ be the set of all functions $f_p$ which are periodic versions of the functions $f \in S$. The periodic version $f_p$ of the function $f \in S$ is defined as $f_p(x) = f(x) \forall x \in (0,1)$, $f_p(0) = f(0+)$ and $f_p(1) = f_p(0)$ and $ \forall x \in \mathbb{R}, f_p(x) = f_p(x+1)$. Closure property under addition EDIT : (This argument is false and the set $S_{pc}$ is not closed under addition) (see comment by Andrew) Let $f_1,f_2 \in S_p$ such that $f_1$ is not same as $-f_2$. I am able to prove that if $f_3 = f_1 + f_2$, then $f_3 \in S_p$, by making the following considerations. Let $D_1,k_1,ck_1$ be the required set and maps respectively for the function $f_1$ as per the definitions given above. Let $D_2,k_2,ck_2$ be the required set and maps of $f_2$. Let $D_3,k_3,ck_3$ be defined as below. $D_3 = D_1 \bigcup D_2$. Let $l_1 \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ and $l_2 \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ are defined as $l_1(x) = k_1(x)$ if $x \in D_1$ otherwise $\infty$. $l_2(x) = k_2(x)$ if $x \in D_2$ otherwise $\infty$. $k_3(x)$ is assigned as $k_3(x) = \min\{l_1(x),l_2(x)\}$ and Let $m_1 \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ and $m_2 \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ are defined as $m_1(x) = ck_1(x)$ if $x \in D_1$ otherwise $\infty$. $m_2(x) = ck_2(x)$ if $x \in D_2$ otherwise $\infty$. $ck_3(x)$ is assigned as $ck_3(x) = \min\{m_1(x),m_2(x)\}$. By assigning $D_3,k_3,ck_3$ as mentioned above I am able to prove that $f_3 \in S_p$. To get the closure property under addition, we can add the set of all constant functions $K$ to the set $S_p$ to form a new set $S_{pc}$. Hence I am able to prove that the set $S_{pc}$ as defined above is closed under the addition operation (and it easily follows that the set $S_{pc}$ is closed under the multiplication operation as well). There by I am able to show that the set $S_{pc}$ is indeed a vector space. (as it cab be easily seen that the set $S_{pc}$ is closed under scalar multiplication). The Question Where I need some help is to show that the set $S_{pc}$ is closed under the binary operation of circular convolution. Here by circular convolution, i mean the convolution operation with the convolution integral summed only over one period i.e., on $[0,1]$. Specifically How should i make the choice of $D_3,k_3,ck_3$, the required set and maps for the resultant function of a convolution, to show that it belongs to $S_{pc}$. What I know Let $f_1$ is periodic with period $1$ and is smooth (within one period) except at $x = a_1 \in [0,1]$, where it is only $n_1$ times differentiable and Let $f_2$ is periodic with period $1$ and is smooth (within one period) except at $x = a_2 \in [0,1]$, where it is only $n_2$ times differentiable. Now the function $f_3 = f_1 \star f_2$ is smooth (within one period) except at $x = a_1 - a_2$ where it is only $n_1 + n_2$ times differentiable. But I am confused as to how to use this fact to arrive at the desired result.","I am studying the properties of a particular class of functions, and I'd appreciate some help in proving a property of that class. I started with a class of functions and made some modifications to show that it forms a vector space and to show that it is closed under convolution. I have given the entire description below, and require help in proving that the set is closed under the operation of convolution (circular). I have also mentioned what methods I have used to prove that it forms a vector space, so that i can get cleared of the difficulty i am facing in arriving at the result of closure under convolution. ( in case you want to avoid reading the entire post linearly, the property needs to be proved on the set $S_{pc}$ and it is described in the section ""closure under addition""). PS : Please do let me know if you want to know the motivation for such a study. I do not have any concrete reasons which can be expressed in mathematically precise terms, behind the motivation, but I have an intuition behind it. PS 2 : A construction method and proof of non-emptiness of this class of functions is given in this Q&A . Definition Let $f \colon \mathbb{R} \to \mathbb{R}$ be a function, consider a point $x \in \mathbb{R}$ where $f$ is continuous, then the following two statements are equivalent. $Cf(x) = k$, where $k \in \{0\}\bigcup\mathbb{N}\bigcup\{\infty\}$. The function is continuous at $x$ and the maximum number of times $f$ is differentiable at $x$ is $k$. Clarification (Added) $Cf(x)=k$, where we know that $f$ is a function and $x$ is the point in its domain, then it means that the maximum number of times $f$ is differentiable at $x$ is $k$. This is not a good notation as someone could think that $Cf$ itself is one function different from f and not associated with it, but here I intended that $C$ tells about a proprty of the function at a point in its domain.....not a convincing notation......but i'd like know any ideas to make a good notation for it. Definition of a class of functions The set $S$ consists of functions $f \colon (0,1) \to \mathbb{R}$, which satisfy the following properties. Given any $f \in S$ there exist a countable dense subset $D \subset (0,1)$ and maps $k,ck$ defined as $k \colon D \to \mathbb{N}$ and $ck \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ which satisfy the following properties. $\forall n \in \mathbb{N}$, the pre-image $k^{-1}(\{n\})$ is a finite set. $\forall x \in $D$, ck(x) \ge k(x)$ but finite and $\forall x \in (0,1)$\D$, ck(x) = \infty$. $\forall x \in (0,1), Cf(x) = ck(x)$ Whenever $Cf(x) = k$ is finite, for the $(k+1)^{nth}$ derivative of $f$ at $x$, the left and right limits exist and are not equal. (the left and right limits do not diverge). Let $S_p$ be the set of all functions $f_p$ which are periodic versions of the functions $f \in S$. The periodic version $f_p$ of the function $f \in S$ is defined as $f_p(x) = f(x) \forall x \in (0,1)$, $f_p(0) = f(0+)$ and $f_p(1) = f_p(0)$ and $ \forall x \in \mathbb{R}, f_p(x) = f_p(x+1)$. Closure property under addition EDIT : (This argument is false and the set $S_{pc}$ is not closed under addition) (see comment by Andrew) Let $f_1,f_2 \in S_p$ such that $f_1$ is not same as $-f_2$. I am able to prove that if $f_3 = f_1 + f_2$, then $f_3 \in S_p$, by making the following considerations. Let $D_1,k_1,ck_1$ be the required set and maps respectively for the function $f_1$ as per the definitions given above. Let $D_2,k_2,ck_2$ be the required set and maps of $f_2$. Let $D_3,k_3,ck_3$ be defined as below. $D_3 = D_1 \bigcup D_2$. Let $l_1 \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ and $l_2 \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ are defined as $l_1(x) = k_1(x)$ if $x \in D_1$ otherwise $\infty$. $l_2(x) = k_2(x)$ if $x \in D_2$ otherwise $\infty$. $k_3(x)$ is assigned as $k_3(x) = \min\{l_1(x),l_2(x)\}$ and Let $m_1 \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ and $m_2 \colon (0,1) \to \mathbb{N}\bigcup\{\infty\}$ are defined as $m_1(x) = ck_1(x)$ if $x \in D_1$ otherwise $\infty$. $m_2(x) = ck_2(x)$ if $x \in D_2$ otherwise $\infty$. $ck_3(x)$ is assigned as $ck_3(x) = \min\{m_1(x),m_2(x)\}$. By assigning $D_3,k_3,ck_3$ as mentioned above I am able to prove that $f_3 \in S_p$. To get the closure property under addition, we can add the set of all constant functions $K$ to the set $S_p$ to form a new set $S_{pc}$. Hence I am able to prove that the set $S_{pc}$ as defined above is closed under the addition operation (and it easily follows that the set $S_{pc}$ is closed under the multiplication operation as well). There by I am able to show that the set $S_{pc}$ is indeed a vector space. (as it cab be easily seen that the set $S_{pc}$ is closed under scalar multiplication). The Question Where I need some help is to show that the set $S_{pc}$ is closed under the binary operation of circular convolution. Here by circular convolution, i mean the convolution operation with the convolution integral summed only over one period i.e., on $[0,1]$. Specifically How should i make the choice of $D_3,k_3,ck_3$, the required set and maps for the resultant function of a convolution, to show that it belongs to $S_{pc}$. What I know Let $f_1$ is periodic with period $1$ and is smooth (within one period) except at $x = a_1 \in [0,1]$, where it is only $n_1$ times differentiable and Let $f_2$ is periodic with period $1$ and is smooth (within one period) except at $x = a_2 \in [0,1]$, where it is only $n_2$ times differentiable. Now the function $f_3 = f_1 \star f_2$ is smooth (within one period) except at $x = a_1 - a_2$ where it is only $n_1 + n_2$ times differentiable. But I am confused as to how to use this fact to arrive at the desired result.",,"['real-analysis', 'vector-spaces', 'convolution']"
87,Does the p-norm converge to the max-norm in some norm,Does the p-norm converge to the max-norm in some norm,,"Does the $p$-norm on $\mathbf{R}^n$ converge to the max-norm on $\mathbf{R}^n$ as elements in the space of real valued continuous functions on $\mathbf{R}^n$ endowed with some norm? More precisely, consider $V = C(\mathbf{R}^n, \mathbf{R})$. Does there exist a norm $\Vert \cdot \Vert$ on $V$ such that the sequence $(\Vert \cdot \Vert_p)_p$ converges to the maximum norm $\Vert \cdot \Vert_\infty$ with respect  to $\Vert \cdot \Vert$? Here's the motivation for this question. In some sense, I though the max-norm should be the limit of the $p$-norms as $p$ goes to infinity. ""Taking an $\infty$-th root of the sum of the infinite powers"" in some sense should be the maximum norm. I just thought that this could be made precise.","Does the $p$-norm on $\mathbf{R}^n$ converge to the max-norm on $\mathbf{R}^n$ as elements in the space of real valued continuous functions on $\mathbf{R}^n$ endowed with some norm? More precisely, consider $V = C(\mathbf{R}^n, \mathbf{R})$. Does there exist a norm $\Vert \cdot \Vert$ on $V$ such that the sequence $(\Vert \cdot \Vert_p)_p$ converges to the maximum norm $\Vert \cdot \Vert_\infty$ with respect  to $\Vert \cdot \Vert$? Here's the motivation for this question. In some sense, I though the max-norm should be the limit of the $p$-norms as $p$ goes to infinity. ""Taking an $\infty$-th root of the sum of the infinite powers"" in some sense should be the maximum norm. I just thought that this could be made precise.",,"['real-analysis', 'general-topology', 'convergence-divergence', 'normed-spaces']"
88,"For what $x\in[0,1]$ is $y = \sum\limits_{k = 1}^\infty\frac{\sin( k!^2x )}{k!}$ differentiable?",For what  is  differentiable?,"x\in[0,1] y = \sum\limits_{k = 1}^\infty\frac{\sin( k!^2x )}{k!}","For what $x\in[0,1]$ will the function $y = \sum\limits_{k = 1}^\infty\frac{\sin( k!^2x )}{k!}$ be differentiable? How do you know? Here is the equation expressed more clearly on Wolfram Alpha. The only difference is that 10 should be Infinity (Wolfram apparently can't handle that yet). I'm trying to understand for what $x\in[0,1]$ this function is differentiable. I've used a computer to plot the graph of $y'$ (the derivative of the function) with the upper limit (top number of sigma) as 10, and then with the upper limit as 11, 12... it looks like these ""zig-zags"" continue to exist as you ""go deeper"" into the function. ...so I'm thinking the values of $x\in[0,1]$ that make the function differentiable are all of them... But is my line of thinking correct? How can I validate?","For what $x\in[0,1]$ will the function $y = \sum\limits_{k = 1}^\infty\frac{\sin( k!^2x )}{k!}$ be differentiable? How do you know? Here is the equation expressed more clearly on Wolfram Alpha. The only difference is that 10 should be Infinity (Wolfram apparently can't handle that yet). I'm trying to understand for what $x\in[0,1]$ this function is differentiable. I've used a computer to plot the graph of $y'$ (the derivative of the function) with the upper limit (top number of sigma) as 10, and then with the upper limit as 11, 12... it looks like these ""zig-zags"" continue to exist as you ""go deeper"" into the function. ...so I'm thinking the values of $x\in[0,1]$ that make the function differentiable are all of them... But is my line of thinking correct? How can I validate?",,"['calculus', 'real-analysis']"
89,Integer translates of a scaling function,Integer translates of a scaling function,,"I think this is asked as a standard exercise in books about wavelets (e.g. exercise 7.2 in Mallat's book ), but I couldn't find a proof.  Let $\phi$ be a scaling function (see definition below).  I would like to learn why  $$\sum_{k\in\mathbb Z} \phi(x-k) = 1 $$ almost everywhere. Definition. A sequence of subspaces $\{V_j: j\in \mathbb{Z}\}$ of $L^2(\mathbb R)$ is called a multiresolution analysis if it satisfies the following: $V_j \subset V_{j+1}$ $\bigcap_{j}V_j = \{0\}$ $\overline{\bigcup_jV_j} = L^2(\mathbb R)$ $f(x)\in V_j$ if and only if $f(2x) \in V_{j+1}$ There exists a function $\phi \in V_0$ such that $\{\phi(x-k)\}_{k\in\mathbb Z}$ is an orthogonal basis for $V_0$ The function $\phi$ here is called as a scaling function .","I think this is asked as a standard exercise in books about wavelets (e.g. exercise 7.2 in Mallat's book ), but I couldn't find a proof.  Let $\phi$ be a scaling function (see definition below).  I would like to learn why  $$\sum_{k\in\mathbb Z} \phi(x-k) = 1 $$ almost everywhere. Definition. A sequence of subspaces $\{V_j: j\in \mathbb{Z}\}$ of $L^2(\mathbb R)$ is called a multiresolution analysis if it satisfies the following: $V_j \subset V_{j+1}$ $\bigcap_{j}V_j = \{0\}$ $\overline{\bigcup_jV_j} = L^2(\mathbb R)$ $f(x)\in V_j$ if and only if $f(2x) \in V_{j+1}$ There exists a function $\phi \in V_0$ such that $\{\phi(x-k)\}_{k\in\mathbb Z}$ is an orthogonal basis for $V_0$ The function $\phi$ here is called as a scaling function .",,"['real-analysis', 'functional-analysis']"
90,Has L'Hopital's Rule been studied as an operator?,Has L'Hopital's Rule been studied as an operator?,,"I discovered while teaching Calc 2 that if you apply L'Hopital's rule to $\frac{x}{\sqrt{x^2+1}}$ you get $\frac{\sqrt{x^2+1}}{x}$ , and if you apply L'Hopital again you get $\frac{x}{\sqrt{x^2+1}}$ back.  In other words the L'Hopital operator has a cycle of order two. EDIT (Thanks KennyTM): ""I suppose the L'Hopital operator should be defined on equivalence classes of pairs $(f(x),g(x))$ of differentiable functions with the fractional equivalence: $(f(x),g(x))\equiv(h(x),k(x))$ if and only if $fk=gh$ ."" This does not work .  But it doesn't really take pairs of functions to pairs of function, either.  So the first problem is to find out how it is an operator. Has anyone ever studied this operator?  Wikipedia tells me nothing. Yes, I know it is easier to find the limit by dividing through by $x$ , but some students want to apply L'Hopital to everything .","I discovered while teaching Calc 2 that if you apply L'Hopital's rule to you get , and if you apply L'Hopital again you get back.  In other words the L'Hopital operator has a cycle of order two. EDIT (Thanks KennyTM): ""I suppose the L'Hopital operator should be defined on equivalence classes of pairs of differentiable functions with the fractional equivalence: if and only if ."" This does not work .  But it doesn't really take pairs of functions to pairs of function, either.  So the first problem is to find out how it is an operator. Has anyone ever studied this operator?  Wikipedia tells me nothing. Yes, I know it is easier to find the limit by dividing through by , but some students want to apply L'Hopital to everything .","\frac{x}{\sqrt{x^2+1}} \frac{\sqrt{x^2+1}}{x} \frac{x}{\sqrt{x^2+1}} (f(x),g(x)) (f(x),g(x))\equiv(h(x),k(x)) fk=gh x","['calculus', 'real-analysis']"
91,A Fourier series failing to converge on the Cantor Set,A Fourier series failing to converge on the Cantor Set,,"This is a strengthening of Chandru's question: Example of a function whose Fourier Series fails to converge at One point Is there a nice and concrete example of a Fourier series that fails to converge on some ""big"" set of measure zero, for instance on the Cantor ternary set?","This is a strengthening of Chandru's question: Example of a function whose Fourier Series fails to converge at One point Is there a nice and concrete example of a Fourier series that fails to converge on some ""big"" set of measure zero, for instance on the Cantor ternary set?",,['real-analysis']
92,"Rudin's RCA, Theorem $7.16$: The Fundamental Theorem of Calculus.","Rudin's RCA, Theorem : The Fundamental Theorem of Calculus.",7.16,"There is the equality: $$ f(x) - f(a) = \int_a^x f'(t)dt \ \ (a \leq x \leq b). \tag{1}$$ There is assumption by Rudin: Suppose $f$ is continuous on $[a,b], f$ is differentiable at almost every point of $[a,b]$ and $f' \in L^{1}$ on $[a,b]$ . Do these assumptions imply that $(1)$ holds? Answer: No. Choose $\{\delta_n\}$ so that $1 = \delta_0 \gt \delta_1 \gt \delta_2 \gt ..., \delta_n \to 0$ . Put $E_0 = [0,1]$ . Suppose $n \geq 0$ and $E_n$ is constructed so that $E_n$ is the union $2^n$ disjoint closed intervals, each of length $2^{-n}\delta_n$ . Delete a segment in the center of each of these $2^n$ intervals, so that each of the remaining $2^{n+1}$ intervals has length $2^{-n-1}\delta_{n+1}$ (this is possible since $\delta_{n+1} \lt \delta_n$ ), and let $E_{n+1}$ be the union of these $2^{n+1}$ intervals. Then $E_1 \supset E_2 \supset ..., m(E_n) = \delta_n$ , and if $$ E = \bigcap_{n=1}^{\infty} E_n, $$ then $E$ is compact and $m(E) = 0$ . (In fact, $E$ is perfect). Put $g_n = {\delta_n}^{-1} \chi_{E_n}$ and $f_n(x) = \int_0^x g_n(t) dt \ \ \ (n = 0,1,2,...).$ Then $f_n(0) = 0, f_n(1) = 1,$ and each $f_n$ is a monotonic function which is constant on each segment in the complement of $E_n$ . If $I$ is one of the $2^n$ intervals whose union is $E_n$ then $$ \int_{I} g_n(t) dt = \int_{I} g_{n+1}(t) dt = 2^{-n}.$$ It follows from the latest equality that $f_{n+1}(x) = f_n(x) \ \ (x \notin E_n)$ and that $$|f_n(x) - f_{n+1}(x)| \leq \int_I |g_n - g_{n+1}| \lt 2^{-n+1} \  (x \in E_n).$$ I  don't understand how do we get that $f_{n+1}(x) = f_n(x) \ \  (x \not\in E_n)$ and how do we get that $|f_n(x) - f_{n+1}(x)| \leq \int_I |g_n - g_{n+1}|$ of which I am not certain why is it less than $2^{-n+1} \ \ (x \in E_n).$ Any help would be appreciated.","There is the equality: There is assumption by Rudin: Suppose is continuous on is differentiable at almost every point of and on . Do these assumptions imply that holds? Answer: No. Choose so that . Put . Suppose and is constructed so that is the union disjoint closed intervals, each of length . Delete a segment in the center of each of these intervals, so that each of the remaining intervals has length (this is possible since ), and let be the union of these intervals. Then , and if then is compact and . (In fact, is perfect). Put and Then and each is a monotonic function which is constant on each segment in the complement of . If is one of the intervals whose union is then It follows from the latest equality that and that I  don't understand how do we get that and how do we get that of which I am not certain why is it less than Any help would be appreciated."," f(x) - f(a) = \int_a^x f'(t)dt \ \ (a \leq x \leq b). \tag{1} f [a,b], f [a,b] f' \in L^{1} [a,b] (1) \{\delta_n\} 1 = \delta_0 \gt \delta_1 \gt \delta_2 \gt ..., \delta_n \to 0 E_0 = [0,1] n \geq 0 E_n E_n 2^n 2^{-n}\delta_n 2^n 2^{n+1} 2^{-n-1}\delta_{n+1} \delta_{n+1} \lt \delta_n E_{n+1} 2^{n+1} E_1 \supset E_2 \supset ..., m(E_n) = \delta_n  E = \bigcap_{n=1}^{\infty} E_n,  E m(E) = 0 E g_n = {\delta_n}^{-1} \chi_{E_n} f_n(x) = \int_0^x g_n(t) dt \ \ \ (n = 0,1,2,...). f_n(0) = 0, f_n(1) = 1, f_n E_n I 2^n E_n  \int_{I} g_n(t) dt = \int_{I} g_{n+1}(t) dt = 2^{-n}. f_{n+1}(x) = f_n(x) \ \ (x \notin E_n) |f_n(x) - f_{n+1}(x)| \leq \int_I |g_n - g_{n+1}| \lt 2^{-n+1} \  (x \in E_n). f_{n+1}(x) = f_n(x) \ \  (x \not\in E_n) |f_n(x) - f_{n+1}(x)| \leq \int_I |g_n - g_{n+1}| 2^{-n+1} \ \ (x \in E_n).","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
93,"Evaluate $I(a,b)=\int_{0}^{1}k^a(1-k^2)^bK(k)\text{d}k$",Evaluate,"I(a,b)=\int_{0}^{1}k^a(1-k^2)^bK(k)\text{d}k","With the interests of $$ I(a,b)=\int_{0}^{1}k^a(1-k^2)^bK(k)\text{d}k, $$ where $K(k)$ represents the complete elliptic integral with modulus $k$ and $K^\prime(k)$ its complementary, many $I(a,b)$ for rational $a,b$ are evaluated. The methodologies are as follows: $\textbf{1.Hypergeometric Representations}$ Expanding $K(k)$ , we obtain(conditions omitted) $$I(a,b)=\frac{\pi}{4} \frac{\Gamma(b+1)\Gamma\left ( \frac{a+1}{2}  \right ) }{ \Gamma\left ( \frac{a+3}{2}+b  \right ) } {}_3F_2\left ( \frac{1}{2},\frac12,\frac{a+1}{2};\frac{a+3}{2}+b,1;1    \right ).$$ Note that $I(a,b+1)=I(a,b)-I(a+2,b)$ . Apparently, supposing $\frac{a+3}{2}+b=\frac12$ i.e. $b=-1-\frac{a}{2}$ , it simplifies to $$ I\left ( a,-1-\frac{a}{2}  \right )  =\frac{\cos\left ( \frac{\pi a}{2}  \right ) }{4\pi}  \Gamma\left ( -\frac{a}{2}  \right )^2\Gamma\left ( \frac{a+1}{2}  \right )^2. $$ Also, $$ I(1,b)=\frac{\pi}{4} \frac{\Gamma\left ( b+1 \right )^2 }{ \Gamma\left ( b+\frac32 \right )^2}. $$ Some hypergeometric transformations are applicable to the given ${}_3F_2$ . For instances, from here we have $$I(s,0)+I(-s-1,0) =-\frac{\pi}{4}\tan\left ( \frac{\pi s}{2}  \right )  \frac{\Gamma\left ( -\frac{s}{2} \right )^2 }{ \Gamma\left ( \frac{1-s}{2}  \right )^2 }.$$ (add on May 20th, 23 ) Applying Dixon's ${}_3F_2$ Theorem, that allowed us to proceed further. Explicitly, as $a+b=-1/2$ , $$ I(a,b)=\frac{\Gamma\left ( \frac14 \right )^2}{8\sqrt{2\pi} }  \frac{\Gamma\left ( \frac{1+a}{2}  \right ) \Gamma\left ( \frac12-a \right ) \Gamma\left ( \frac14-\frac{a}{2} \right )  }{\Gamma\left ( \frac34-\frac{a}{2}  \right ) \Gamma\left ( \frac{1-a}{2}  \right )  }. $$ If $a-2b=1,\Re(a)>0$ , $$ I(a,b)=\frac{\Gamma\left ( \frac14 \right )^2}{8\sqrt{\pi} } \frac{\Gamma\left ( 1+\frac{a}{2}  \right )\Gamma\left ( \frac{1+a}{2}  \right )^3}{ \Gamma(1+a)\Gamma\left ( \frac{3}{4}+\frac{a}{2}   \right )^2 }, $$ which gives the evaluation $$ I\left ( -\frac{5}{6},-\frac{11}{12}   \right ) =\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{4\pi\cdot2^{2/3}}. $$ $\textbf{2.Contour Integration}$ We have $$ I(a,b)+\cos(b\pi)I(-a-2b-1,b) +\sin(b\pi)I\left ( 2b+1,-\frac{a}{2}-b-1  \right )  +\sin\left ( \frac{\pi a}{2}  \right )I\left ( a,-\frac{a}{2}-b-1  \right )=0 ,$$ and $$ \cos(b\pi)I\left ( 2b+1,-\frac{a}{2}-b-1  \right )  =\cos\left ( \frac{\pi a}{2}  \right ) I\left ( a,-\frac{a}{2}-b-1 \right )+\sin\left ( \pi b \right ) I(-a-2b-1,b). $$ Setting $a=-5/6,b=-11/12$ , producing \begin{aligned} I\left ( \frac{5}{3},-\frac{11}{12}   \right )  & = \sqrt{3}\left ( \sqrt{3}+1  \right ) I\left ( -\frac56,\frac13 \right )\\ & =\frac{3^{1/4}\left ( \sqrt{3}+1  \right )\Gamma\left ( \frac14 \right )^4  }{4\pi\cdot 2^{1/6}}, \end{aligned} where the second equality is owing to the former identity for $a+b=-1/2$ . Another less obvious one is, $$ \int_{0}^{1} \frac{k^{2/3}K(k)}{\left ( 1-k^2 \right )^{2/3} } \text{d} k=\frac{3^{3/4}\Gamma\left ( \frac14 \right )^4 }{24\pi\cdot2^{1/6}  }. $$ $\textbf{3.Modular Forms(1)}$ The basic idea is to construct a modular form expressed by Jacobi $\vartheta$ functions and compute its $L$ -value. For example, setting $q=\exp(-\pi K^\prime(k)/K(k))$ $$ f(q)=\sum_{m,n\in\mathbb{Z}} (-1)^m\left [ 3\left ( m+\frac16 \right )^2-n^2 \right ] q^{3\left ( m+\frac16 \right )^2+n^2 }.$$ We have $$f(q)=\frac{2^{2/3}k^{1/6}(1-k^2)^{1/12}(1-2k^2)}{ 3\pi^3}K(k)^3.$$ And $$\int_{0}^{\infty}xf(q)\text{d}x =\frac{1}{\pi^2} \sum_{m,n\in\mathbb{Z}} \frac{(-1)^m}{\left ( \sqrt{3}\left ( m+\frac16 \right )+ni   \right )^2 }.$$ Therefore it's sufficient to show that $$\int_{0}^{1} \frac{(2k^2-1)K(k)}{k^{5/6}(1-k^2)^{11/12}}\text{d}k =\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{ 16\pi^2\cdot2^{1/3}  },$$ and therefore, $$ I\left ( \frac{7}{6},-\frac{11}{12}   \right ) =\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot2^{2/3}} +\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{ 32\pi^2\cdot2^{1/3}  }. $$ Which also gives $$ I\left ( -\frac{5}{6},\frac{1}{12}   \right ) =\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot2^{2/3}} -\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{ 32\pi^2\cdot2^{1/3}  }, $$ with $$ \int_{0}^{1}\left ( 1-k^{12} \right )^{\frac{1}{12}} K\left ( k^6 \right )\text{d}k =\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{48\pi\cdot2^{2/3}} -\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{64\pi^2\cdot2^{1/3}  }. $$ However, this usually only gives linear equation among two $I(a,b)$ . Apart from $$I\left(-\frac34,0\right)=\frac{\left ( 3+\sqrt{2}  \right ) \Gamma\left ( \frac18 \right )^2\Gamma\left ( \frac38 \right )^2  }{ 48\pi\sqrt{2} },$$ $$ I\left(-\frac14,0\right)=\frac{\left ( 3-\sqrt{2}  \right ) \Gamma\left ( \frac18 \right )^2\Gamma\left ( \frac38 \right )^2  }{ 48\pi\sqrt{2} },$$ they can be: $$ 2I\left ( -\frac13,-\frac{11}{12} \right ) -I\left ( \frac53,-\frac{11}{12} \right )  =\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{ 8\cdot2^{5/6}\pi^2  }, $$ $$ 4I\left ( -\frac23,-\frac{17}{24} \right ) +I\left ( \frac43,-\frac{17}{24} \right )  =\frac{\left ( \sqrt{2}-1  \right )^{\frac32} \left ( \sqrt{3} +\sqrt{2}  \right )^{\frac32}\cdot3^{\frac14} \left ( 1+\left ( 2-\sqrt{3}  \right ) \left ( \sqrt{3} -\sqrt{2}  \right )   \right )^2 }{8\cdot2^{\frac13}\pi} \Gamma\left ( \frac{1}{24} \right ) \Gamma\left ( \frac{5}{24} \right ) \Gamma\left ( \frac{7}{24} \right ) \Gamma\left ( \frac{11}{24} \right ). $$ Gathering all information we know about $I(a,b)$ , we obtain two additional values: \begin{aligned} &I\left ( -\frac13,-\frac{11}{12} \right ) =\frac{3^{1/4}\left ( 1+\sqrt{3} \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot 2^{1/6}} +\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{ 16\cdot2^{5/6}\pi^2  },\\ &I\left ( -\frac13,\frac{1}{12}  \right ) =-\frac{3^{1/4}\left ( 1+\sqrt{3} \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot 2^{1/6}} +\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{ 16\cdot2^{5/6}\pi^2  }. \end{aligned} $\textbf{4.Modular Forms(2)}$ It's also possible to prove that \begin{aligned} &\color{purple}{\left(\frac2\pi\right)^2\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1} \frac{(1-2k^2)K(k)}{\sqrt{k}(1-k^2)^{3/4}}\text{d}k =2^{2s}\pi^{-s}\Gamma(s)[L_8(s)L_{-8}(s-2)+L_{-8}(s)L_{8}(s-2)]},\\ &\color{purple}{\left(\frac2\pi\right)^2\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1} \frac{\sqrt{k}K(k)}{(1-k^2)^{1/4}}\text{d}k =2^{2s-1}\pi^{-s}\Gamma(s)[L_8(s)L_{-8}(s-2)-L_{-8}(s)L_{8}(s-2)]}. \end{aligned} Also note that $a=-\frac12,b=-\frac34=-1-\frac{a}{2}$ , we derive \begin{aligned} &I\left ( \frac32,-\frac34 \right )  =\frac{\pi^2}{4\sqrt{2} }+\frac{\Gamma\left ( \frac14 \right )^4 }{8\pi\sqrt{2} },\\ &I\left ( \frac12,-\frac14 \right )  =\frac{\pi^2}{4\sqrt{2} }. \end{aligned} $\textbf{5.Fourier-Legendre Expansions}$ Similarly to here . But most results aren't newly-created. The questions come here: Whether we can find more closed-forms for single $I(a,b)$ ? Can we come up with more ways to cope with $I(a,b)$ ? Remark : The remained question is $I\left(\frac34,-\frac38\right)$ , which seems to be unable to prove in ways listed above.","With the interests of where represents the complete elliptic integral with modulus and its complementary, many for rational are evaluated. The methodologies are as follows: Expanding , we obtain(conditions omitted) Note that . Apparently, supposing i.e. , it simplifies to Also, Some hypergeometric transformations are applicable to the given . For instances, from here we have (add on May 20th, 23 ) Applying Dixon's Theorem, that allowed us to proceed further. Explicitly, as , If , which gives the evaluation We have and Setting , producing where the second equality is owing to the former identity for . Another less obvious one is, The basic idea is to construct a modular form expressed by Jacobi functions and compute its -value. For example, setting We have And Therefore it's sufficient to show that and therefore, Which also gives with However, this usually only gives linear equation among two . Apart from they can be: Gathering all information we know about , we obtain two additional values: It's also possible to prove that Also note that , we derive Similarly to here . But most results aren't newly-created. The questions come here: Whether we can find more closed-forms for single ? Can we come up with more ways to cope with ? Remark : The remained question is , which seems to be unable to prove in ways listed above.","
I(a,b)=\int_{0}^{1}k^a(1-k^2)^bK(k)\text{d}k,
 K(k) k K^\prime(k) I(a,b) a,b \textbf{1.Hypergeometric Representations} K(k) I(a,b)=\frac{\pi}{4} \frac{\Gamma(b+1)\Gamma\left ( \frac{a+1}{2}  \right ) }{
\Gamma\left ( \frac{a+3}{2}+b  \right ) }
{}_3F_2\left ( \frac{1}{2},\frac12,\frac{a+1}{2};\frac{a+3}{2}+b,1;1    \right ). I(a,b+1)=I(a,b)-I(a+2,b) \frac{a+3}{2}+b=\frac12 b=-1-\frac{a}{2} 
I\left ( a,-1-\frac{a}{2}  \right ) 
=\frac{\cos\left ( \frac{\pi a}{2}  \right ) }{4\pi} 
\Gamma\left ( -\frac{a}{2}  \right )^2\Gamma\left ( \frac{a+1}{2}  \right )^2.
 
I(1,b)=\frac{\pi}{4} \frac{\Gamma\left ( b+1 \right )^2 }{
\Gamma\left ( b+\frac32 \right )^2}.
 {}_3F_2 I(s,0)+I(-s-1,0)
=-\frac{\pi}{4}\tan\left ( \frac{\pi s}{2}  \right ) 
\frac{\Gamma\left ( -\frac{s}{2} \right )^2 }{
\Gamma\left ( \frac{1-s}{2}  \right )^2 }. {}_3F_2 a+b=-1/2 
I(a,b)=\frac{\Gamma\left ( \frac14 \right )^2}{8\sqrt{2\pi} } 
\frac{\Gamma\left ( \frac{1+a}{2}  \right ) \Gamma\left ( \frac12-a \right )
\Gamma\left ( \frac14-\frac{a}{2} \right )  }{\Gamma\left ( \frac34-\frac{a}{2}  \right )
\Gamma\left ( \frac{1-a}{2}  \right )  }.
 a-2b=1,\Re(a)>0 
I(a,b)=\frac{\Gamma\left ( \frac14 \right )^2}{8\sqrt{\pi} }
\frac{\Gamma\left ( 1+\frac{a}{2}  \right )\Gamma\left ( \frac{1+a}{2}  \right )^3}{
\Gamma(1+a)\Gamma\left ( \frac{3}{4}+\frac{a}{2}   \right )^2 },
 
I\left ( -\frac{5}{6},-\frac{11}{12}   \right )
=\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{4\pi\cdot2^{2/3}}.
 \textbf{2.Contour Integration} 
I(a,b)+\cos(b\pi)I(-a-2b-1,b)
+\sin(b\pi)I\left ( 2b+1,-\frac{a}{2}-b-1  \right ) 
+\sin\left ( \frac{\pi a}{2}  \right )I\left ( a,-\frac{a}{2}-b-1  \right )=0
, 
\cos(b\pi)I\left ( 2b+1,-\frac{a}{2}-b-1  \right ) 
=\cos\left ( \frac{\pi a}{2}  \right )
I\left ( a,-\frac{a}{2}-b-1 \right )+\sin\left ( \pi b \right )
I(-a-2b-1,b).
 a=-5/6,b=-11/12 \begin{aligned}
I\left ( \frac{5}{3},-\frac{11}{12}   \right ) 
& = \sqrt{3}\left ( \sqrt{3}+1  \right ) I\left ( -\frac56,\frac13 \right )\\
& =\frac{3^{1/4}\left ( \sqrt{3}+1  \right )\Gamma\left ( \frac14 \right )^4  }{4\pi\cdot 2^{1/6}},
\end{aligned} a+b=-1/2 
\int_{0}^{1} \frac{k^{2/3}K(k)}{\left ( 1-k^2 \right )^{2/3} }
\text{d} k=\frac{3^{3/4}\Gamma\left ( \frac14 \right )^4 }{24\pi\cdot2^{1/6}  }.
 \textbf{3.Modular Forms(1)} \vartheta L q=\exp(-\pi K^\prime(k)/K(k)) 
f(q)=\sum_{m,n\in\mathbb{Z}}
(-1)^m\left [ 3\left ( m+\frac16 \right )^2-n^2 \right ]
q^{3\left ( m+\frac16 \right )^2+n^2 }. f(q)=\frac{2^{2/3}k^{1/6}(1-k^2)^{1/12}(1-2k^2)}{
3\pi^3}K(k)^3. \int_{0}^{\infty}xf(q)\text{d}x
=\frac{1}{\pi^2} \sum_{m,n\in\mathbb{Z}}
\frac{(-1)^m}{\left ( \sqrt{3}\left ( m+\frac16 \right )+ni   \right )^2 }. \int_{0}^{1} \frac{(2k^2-1)K(k)}{k^{5/6}(1-k^2)^{11/12}}\text{d}k
=\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
16\pi^2\cdot2^{1/3}  }, 
I\left ( \frac{7}{6},-\frac{11}{12}   \right )
=\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot2^{2/3}}
+\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
32\pi^2\cdot2^{1/3}  }.
 
I\left ( -\frac{5}{6},\frac{1}{12}   \right )
=\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot2^{2/3}}
-\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
32\pi^2\cdot2^{1/3}  },
 
\int_{0}^{1}\left ( 1-k^{12} \right )^{\frac{1}{12}}
K\left ( k^6 \right )\text{d}k =\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{48\pi\cdot2^{2/3}}
-\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{64\pi^2\cdot2^{1/3}  }.
 I(a,b) I\left(-\frac34,0\right)=\frac{\left ( 3+\sqrt{2}  \right )
\Gamma\left ( \frac18 \right )^2\Gamma\left ( \frac38 \right )^2  }{
48\pi\sqrt{2} }, 
I\left(-\frac14,0\right)=\frac{\left ( 3-\sqrt{2}  \right )
\Gamma\left ( \frac18 \right )^2\Gamma\left ( \frac38 \right )^2  }{
48\pi\sqrt{2} }, 
2I\left ( -\frac13,-\frac{11}{12} \right )
-I\left ( \frac53,-\frac{11}{12} \right ) 
=\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
8\cdot2^{5/6}\pi^2  },
 
4I\left ( -\frac23,-\frac{17}{24} \right )
+I\left ( \frac43,-\frac{17}{24} \right ) 
=\frac{\left ( \sqrt{2}-1  \right )^{\frac32}
\left ( \sqrt{3} +\sqrt{2}  \right )^{\frac32}\cdot3^{\frac14}
\left ( 1+\left ( 2-\sqrt{3}  \right )
\left ( \sqrt{3} -\sqrt{2}  \right )   \right )^2 }{8\cdot2^{\frac13}\pi}
\Gamma\left ( \frac{1}{24} \right )
\Gamma\left ( \frac{5}{24} \right )
\Gamma\left ( \frac{7}{24} \right )
\Gamma\left ( \frac{11}{24} \right ).
 I(a,b) \begin{aligned}
&I\left ( -\frac13,-\frac{11}{12} \right )
=\frac{3^{1/4}\left ( 1+\sqrt{3} \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot 2^{1/6}}
+\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
16\cdot2^{5/6}\pi^2  },\\
&I\left ( -\frac13,\frac{1}{12}  \right )
=-\frac{3^{1/4}\left ( 1+\sqrt{3} \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot 2^{1/6}}
+\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
16\cdot2^{5/6}\pi^2  }.
\end{aligned} \textbf{4.Modular Forms(2)} \begin{aligned}
&\color{purple}{\left(\frac2\pi\right)^2\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{(1-2k^2)K(k)}{\sqrt{k}(1-k^2)^{3/4}}\text{d}k
=2^{2s}\pi^{-s}\Gamma(s)[L_8(s)L_{-8}(s-2)+L_{-8}(s)L_{8}(s-2)]},\\
&\color{purple}{\left(\frac2\pi\right)^2\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{\sqrt{k}K(k)}{(1-k^2)^{1/4}}\text{d}k
=2^{2s-1}\pi^{-s}\Gamma(s)[L_8(s)L_{-8}(s-2)-L_{-8}(s)L_{8}(s-2)]}.
\end{aligned} a=-\frac12,b=-\frac34=-1-\frac{a}{2} \begin{aligned}
&I\left ( \frac32,-\frac34 \right ) 
=\frac{\pi^2}{4\sqrt{2} }+\frac{\Gamma\left ( \frac14 \right )^4 }{8\pi\sqrt{2} },\\
&I\left ( \frac12,-\frac14 \right ) 
=\frac{\pi^2}{4\sqrt{2} }.
\end{aligned} \textbf{5.Fourier-Legendre Expansions} I(a,b) I(a,b) I\left(\frac34,-\frac38\right)","['real-analysis', 'integration', 'definite-integrals', 'hypergeometric-function', 'elliptic-integrals']"
94,When is the projection $\mathrm{P}:x\longmapsto \mathrm{J}(x)^\top(\mathrm{J}(x)\mathrm{J}(x)^\top)^{-1}\mathrm{J}(x)$ Lipschitz continuous?,When is the projection  Lipschitz continuous?,\mathrm{P}:x\longmapsto \mathrm{J}(x)^\top(\mathrm{J}(x)\mathrm{J}(x)^\top)^{-1}\mathrm{J}(x),"$f:\mathbb{R}^n\to\mathbb{R}^m$ is a smooth function with $n > m$ . $\mathrm{J}(x)$ is the Jacobian matrix of $f$ and it is full row-rank at every $x\in\mathbb{R}^n$ . For any $x\in\mathbb{R}^n$ define the projection matrix as the $n\times n$ symmetric idempotent matrix $$ \mathrm{P}(x) = \mathrm{J}(x)^\top(\mathrm{J}(x)\mathrm{J}(x)^\top)^{-1}\mathrm{J}(x). $$ Can I say that $\mathrm{P}: x\longmapsto \mathrm{P}(x)$ is Lipschitz continuous? If not, what conditions do I need to say this?","is a smooth function with . is the Jacobian matrix of and it is full row-rank at every . For any define the projection matrix as the symmetric idempotent matrix Can I say that is Lipschitz continuous? If not, what conditions do I need to say this?","f:\mathbb{R}^n\to\mathbb{R}^m n > m \mathrm{J}(x) f x\in\mathbb{R}^n x\in\mathbb{R}^n n\times n 
\mathrm{P}(x) = \mathrm{J}(x)^\top(\mathrm{J}(x)\mathrm{J}(x)^\top)^{-1}\mathrm{J}(x).
 \mathrm{P}: x\longmapsto \mathrm{P}(x)","['real-analysis', 'calculus', 'linear-algebra', 'multivariable-calculus', 'differential-geometry']"
95,"How homeomorphic are noninjective images of $[0,1]$ to $[0,1]$?",How homeomorphic are noninjective images of  to ?,"[0,1] [0,1]","I have a continuous function $f:[0,1]\rightarrow \mathbb{R^2}$ . If $f$ we injective, we'd know the image is homeomorphic to $[0,1]$ . Lets consider $S:=\{f(t) : f \text{ injective at }   t\}$ (i.e $t$ such that $f^{-1}\big(\{f(t)\}\big)=\{t\}$ .) What can we say about the homeomorphism type of S as a subspace of $\mathbb{R^2}$ ? Is $S$ homeomorphic to a subset of $[0,1]$ ? In particular, if $S$ is dense in the image of $f$ , must $S$ be homeomorphic to a dense subset of $[0,1]$ ? Clearly the same theorem doesnt apply since $S$ need not be compact. We could choose big compact subsets of $S$ and show that almost all of $S$ is homeomorphic to some union of compact intervals, therefore embeds in $[0,1]$ nicely.","I have a continuous function . If we injective, we'd know the image is homeomorphic to . Lets consider (i.e such that .) What can we say about the homeomorphism type of S as a subspace of ? Is homeomorphic to a subset of ? In particular, if is dense in the image of , must be homeomorphic to a dense subset of ? Clearly the same theorem doesnt apply since need not be compact. We could choose big compact subsets of and show that almost all of is homeomorphic to some union of compact intervals, therefore embeds in nicely.","f:[0,1]\rightarrow \mathbb{R^2} f [0,1] S:=\{f(t) : f \text{ injective at }  
t\} t f^{-1}\big(\{f(t)\}\big)=\{t\} \mathbb{R^2} S [0,1] S f S [0,1] S S S [0,1]","['real-analysis', 'general-topology', 'analysis']"
96,What's so special about square root cancellation?,What's so special about square root cancellation?,,"I am reading a lecture about analytic number theory and I found a statement I do not quite understand. We were studying a sum and got the result $$\sum_{n\leq x} n^{-it} = O(x^{1/2}\vert t\vert^\epsilon)$$ for $\vert t\vert \geq x^{1/2}$ . The author concluded, that because of this square root cancellation, ""the functions $n^{-it}$ , $1\leq n\leq x$ behave like independent identically distributed variables provided $\vert t\vert \geq x^{1/2}$ "". In another context with the Riemann Hypothesis I have read, that the hypothesis is equivalent to $$\pi(x)=\int_2^x\frac{dt}{\log t} + O(x^{1/2+\epsilon})$$ for all $\epsilon>0$ and that this error term would imply that the primes would be distributed ""as uniformly as possible"" (this is just a literal translation since the book is not in english, but I think it is clear what I mean). However, I don't understand how these $x^{1/2}$ error terms (namely the square root cancellation) implies these statistical statements. What is so special with square root cancellation? The statement about the prime number theorem was just an example and this is not a questing about analytic number theory, but a question about these statistical conclusion. I'd appreciate any explanation and any help on this!","I am reading a lecture about analytic number theory and I found a statement I do not quite understand. We were studying a sum and got the result for . The author concluded, that because of this square root cancellation, ""the functions , behave like independent identically distributed variables provided "". In another context with the Riemann Hypothesis I have read, that the hypothesis is equivalent to for all and that this error term would imply that the primes would be distributed ""as uniformly as possible"" (this is just a literal translation since the book is not in english, but I think it is clear what I mean). However, I don't understand how these error terms (namely the square root cancellation) implies these statistical statements. What is so special with square root cancellation? The statement about the prime number theorem was just an example and this is not a questing about analytic number theory, but a question about these statistical conclusion. I'd appreciate any explanation and any help on this!",\sum_{n\leq x} n^{-it} = O(x^{1/2}\vert t\vert^\epsilon) \vert t\vert \geq x^{1/2} n^{-it} 1\leq n\leq x \vert t\vert \geq x^{1/2} \pi(x)=\int_2^x\frac{dt}{\log t} + O(x^{1/2+\epsilon}) \epsilon>0 x^{1/2},"['real-analysis', 'statistics', 'analytic-number-theory']"
97,"Proving that $f(t)=\frac{n^2}{2}\cdot t^{n-4}(1-t^2)\left(t^2-\frac{n-3}{n}\right)$ is bounded above by $1$, for $n\geq6$ and $t\in[0,1]$","Proving that  is bounded above by , for  and","f(t)=\frac{n^2}{2}\cdot t^{n-4}(1-t^2)\left(t^2-\frac{n-3}{n}\right) 1 n\geq6 t\in[0,1]","I have a problem that looks like a typical problem of maximizing functions in a compact interval. However, I am not being able to prove the bound I need. Let $n\geq 6$ be an integer number. Consider the function: $$f(t) = \frac{n^2}{2} \cdot t^{n-4}(1-t^2) \left(t^2 - \frac{n-3}{n}\right) $$ Prove that for all $t\in [0,1]$ it holds $f(t) \leq 1$ . The points where the derivative $f'$ is zero are very ugly expressions. By maximizing the factor $t^{n-4}(1-t^2)$ and using that the last factor is at most $\frac{3}{n}$ it is possible to deduce that for $n\geq 6$ it is $f(t) \leq \frac{3}{2}$ (in fact, it is possible to bound it in the limit by $\frac{3}{e}\approx 1.1036...$ but that is far from $1$ . I have checked that the claim is true for several random values of $n$ (in fact, I think that the bound can be reduced to something like $0.61...$ for $n>20$ say).","I have a problem that looks like a typical problem of maximizing functions in a compact interval. However, I am not being able to prove the bound I need. Let be an integer number. Consider the function: Prove that for all it holds . The points where the derivative is zero are very ugly expressions. By maximizing the factor and using that the last factor is at most it is possible to deduce that for it is (in fact, it is possible to bound it in the limit by but that is far from . I have checked that the claim is true for several random values of (in fact, I think that the bound can be reduced to something like for say).","n\geq 6 f(t) = \frac{n^2}{2} \cdot t^{n-4}(1-t^2) \left(t^2 - \frac{n-3}{n}\right)  t\in [0,1] f(t) \leq 1 f' t^{n-4}(1-t^2) \frac{3}{n} n\geq 6 f(t) \leq \frac{3}{2} \frac{3}{e}\approx 1.1036... 1 n 0.61... n>20","['real-analysis', 'calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
98,"Sequence such that $x_{n+m} \geq x_n+x_m$, show that $(\frac{n}{x_n})$ converges (or not)","Sequence such that , show that  converges (or not)",x_{n+m} \geq x_n+x_m (\frac{n}{x_n}),"Let $(x_n)$ be a sequence such that $x_n \geq  0$ $\forall n \in \mathbb N$ and that $$x_{n+m} \geq x_n+x_m \quad \forall n,m \in \mathbb N^* $$ I have the feeling that $(\frac{n}{x_n})$ converges but I have a little problem with the proof's end. Can someone help me (the statement can also be wrong but I do not find counterexamples) ? We have $x_{n+m} \geq x_n+x_m \quad \forall n,m \in \mathbb N^*$ , we want to show that $(\frac{n}{x_n})$ converges : Let's show that $ \lim_{n\to\infty} \frac{n}{x_n} = a$ where $a = inf\{\frac{n}{x_n} | n \in \mathbb N^*\}$ : We have that $x_{pq+r} \geq x_{pq}+x_r \geq px_q+x_r$ with $p,q,r \in \mathbb N^*$ that gives $\frac{1}{x_{pq+r}} \leq \frac{1}{px_q+x_r}$ . Let $\epsilon > 0$ , as $a$ is an inf, we have that $\exists q \in \mathbb N^*$ such that $ a \leq \frac{q}{x_q} \leq a + \frac{\epsilon}{2} $ Let $N>q$ , so we have for $n > N$ and by Euclidean division on $n$ , $$a \leq \frac{n}{x_n} \leq \frac{pq+r}{px_q+x_r} = \frac{pq}{px_q+x_r}+\frac{r}{px_q+x_r} \leq \frac{q}{x_q} + \frac{q}{px_q+x_r} \leq \frac{q}{x_q} + \frac{q}{px_q} = (1+\frac{1}{p})\frac{q}{x_q} \leq 2 \frac{q}{x_q} $$ Now I can write $a \leq \frac{n}{x_n}\leq 2a+\epsilon$ but we want $a \leq \frac{n}{x_n}\leq a+\epsilon$ , is there a way to fix it ?","Let be a sequence such that and that I have the feeling that converges but I have a little problem with the proof's end. Can someone help me (the statement can also be wrong but I do not find counterexamples) ? We have , we want to show that converges : Let's show that where : We have that with that gives . Let , as is an inf, we have that such that Let , so we have for and by Euclidean division on , Now I can write but we want , is there a way to fix it ?","(x_n) x_n \geq  0 \forall n \in \mathbb N x_{n+m} \geq x_n+x_m \quad \forall n,m \in \mathbb N^*  (\frac{n}{x_n}) x_{n+m} \geq x_n+x_m \quad \forall n,m \in \mathbb N^* (\frac{n}{x_n})  \lim_{n\to\infty} \frac{n}{x_n} = a a = inf\{\frac{n}{x_n} | n \in \mathbb N^*\} x_{pq+r} \geq x_{pq}+x_r \geq px_q+x_r p,q,r \in \mathbb N^* \frac{1}{x_{pq+r}} \leq \frac{1}{px_q+x_r} \epsilon > 0 a \exists q \in \mathbb N^*  a \leq \frac{q}{x_q} \leq a + \frac{\epsilon}{2}  N>q n > N n a \leq \frac{n}{x_n} \leq \frac{pq+r}{px_q+x_r} = \frac{pq}{px_q+x_r}+\frac{r}{px_q+x_r} \leq \frac{q}{x_q} + \frac{q}{px_q+x_r} \leq \frac{q}{x_q} + \frac{q}{px_q} = (1+\frac{1}{p})\frac{q}{x_q} \leq 2 \frac{q}{x_q}  a \leq \frac{n}{x_n}\leq 2a+\epsilon a \leq \frac{n}{x_n}\leq a+\epsilon","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'cauchy-sequences']"
99,Density of Lebesgue measurable set,Density of Lebesgue measurable set,,"I am looking to show that if we have a Lebesgue measurable set $E \subseteq \mathbb{R}$ with a density  1 at every element in $E$ and a density of 0 at every element of $\mathbb{R} \backslash E$ . Then it must be that $E = \mathbb{R}$ or $E = \emptyset$ . I am working through Axlers book on measure theory and we have defined the density of $E$ at a number $b \in \mathbb{R}$ to be $\lim _{t \downarrow 0} \frac{|E \cap(b-t, b+t)|}{2 t}$ . From the Lebesgue Density Theorem I know that for a Lebesgue measurable set $E \subset \mathbb{R}$ , the density of $E$ is 1 at almost every element of $E$ and is 0 at almost every element of $\mathbb{R} \backslash E$ . So the difference to this case is that we are saying its true everywhere as opposed to almost everywhere. So so far I have that: For all $b \in E$ we have $\lim _{t \downarrow 0} \frac{|E \cap(b-t, b+t)|}{2 t} = 1$ For all $b \in \mathbb{R} \backslash E$ we have $\lim _{t \downarrow 0} \frac{|E \cap(b-t, b+t)|}{2 t}=0$ Intuitively it makes sense why only $\mathbb{R}$ and $\emptyset$ work but I am having trouble putting it into a complete proof. Any help is greatly appreciated. Thanks in advance!","I am looking to show that if we have a Lebesgue measurable set with a density  1 at every element in and a density of 0 at every element of . Then it must be that or . I am working through Axlers book on measure theory and we have defined the density of at a number to be . From the Lebesgue Density Theorem I know that for a Lebesgue measurable set , the density of is 1 at almost every element of and is 0 at almost every element of . So the difference to this case is that we are saying its true everywhere as opposed to almost everywhere. So so far I have that: For all we have For all we have Intuitively it makes sense why only and work but I am having trouble putting it into a complete proof. Any help is greatly appreciated. Thanks in advance!","E \subseteq \mathbb{R} E \mathbb{R} \backslash E E = \mathbb{R} E = \emptyset E b \in \mathbb{R} \lim _{t \downarrow 0} \frac{|E \cap(b-t, b+t)|}{2 t} E \subset \mathbb{R} E E \mathbb{R} \backslash E b \in E \lim _{t \downarrow 0} \frac{|E \cap(b-t, b+t)|}{2 t} = 1 b \in \mathbb{R} \backslash E \lim _{t \downarrow 0} \frac{|E \cap(b-t, b+t)|}{2 t}=0 \mathbb{R} \emptyset","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
