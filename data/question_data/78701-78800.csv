,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Dividing matrices,Dividing matrices,,HI Im learning about matrices in school and am curious about how to divide them so i can find out different identity matrices for refecting shapes over linear functions,HI Im learning about matrices in school and am curious about how to divide them so i can find out different identity matrices for refecting shapes over linear functions,,"['linear-algebra', 'matrices']"
1,Finding inverse of a $3\times3$ matrix,Finding inverse of a  matrix,3\times3,"I am given a $3 \times3$ matrix and am asked to find the inverse using elementary row operations. I know how they work, but have no idea of which steps to apply first, followed by which steps. First, the matrices: $$\begin{pmatrix} 1 & 1 & -3\\ 2 & 1 & -3\\ 2 & 2 & 1 \end{pmatrix}$$ All I know thus far is that, if there is a series of operations (pre-multipliers) $E_nE_{n-1}...E_2E_1A$ that reduces to the identity matrix, the same sequence $ E_nE_{n-1}...E_2E_1I$ reduces to the inverse of $A$, $A^{-1}$. Any help? If not, I will use another method already because this is not working thus far. UPDATE Thanks to the community, I got the final answer: $$\begin{pmatrix} -1 & 1 & 0\\ \frac8 7 & -1 & \frac 3 7\\ \frac{-2}{7} & 0 & \frac 1 7 \end{pmatrix}$$","I am given a $3 \times3$ matrix and am asked to find the inverse using elementary row operations. I know how they work, but have no idea of which steps to apply first, followed by which steps. First, the matrices: $$\begin{pmatrix} 1 & 1 & -3\\ 2 & 1 & -3\\ 2 & 2 & 1 \end{pmatrix}$$ All I know thus far is that, if there is a series of operations (pre-multipliers) $E_nE_{n-1}...E_2E_1A$ that reduces to the identity matrix, the same sequence $ E_nE_{n-1}...E_2E_1I$ reduces to the inverse of $A$, $A^{-1}$. Any help? If not, I will use another method already because this is not working thus far. UPDATE Thanks to the community, I got the final answer: $$\begin{pmatrix} -1 & 1 & 0\\ \frac8 7 & -1 & \frac 3 7\\ \frac{-2}{7} & 0 & \frac 1 7 \end{pmatrix}$$",,['matrices']
2,Need help understanding matrix norm notation,Need help understanding matrix norm notation,,"I've been trying to understand matrix norms (full disclosure: school assignment, not looking for answers, just clarity!), and how they follow from vector norms - been a while since I did much linear algebra, so I'm struggling a bit with the notation, in particular, I'm solving in the general case that for matrix A (and any nonzero vector x) $$\frac{\|Ax\|_1}{\|x\|_1} \le C,$$ for $C$ = maximum column sum of $A$ . The part I don't think I understand is what ${||Ax||_1}$ actually means, relative to matrix A. Could someone help me to understand a bit better the notation, and to apply the matrix norm?","I've been trying to understand matrix norms (full disclosure: school assignment, not looking for answers, just clarity!), and how they follow from vector norms - been a while since I did much linear algebra, so I'm struggling a bit with the notation, in particular, I'm solving in the general case that for matrix A (and any nonzero vector x) for = maximum column sum of . The part I don't think I understand is what actually means, relative to matrix A. Could someone help me to understand a bit better the notation, and to apply the matrix norm?","\frac{\|Ax\|_1}{\|x\|_1} \le C, C A {||Ax||_1}","['linear-algebra', 'matrices', 'normed-spaces']"
3,Finding dimension of a vector space,Finding dimension of a vector space,,"Let $H_n$ be the space of all $n\times n$ matrices $A = (a_{i,j})$ with entries in $\mathbb{R}$ satisfying $a_{i,j} = a_{r,s}$ whenever $i+j = r+s$ $(i, j , r , s = 1, 2, \ldots, n)$. What would be dimension of $H_n$ as a vector space over $\mathbb{R}$? i have options for the dimension 1 -  $n^2$ 2-   $n^2-n+1$ 3 - $2n+1$ 4- $2n-1$ I am finding difficulty in identifying the matrix $A$ . thanks for support","Let $H_n$ be the space of all $n\times n$ matrices $A = (a_{i,j})$ with entries in $\mathbb{R}$ satisfying $a_{i,j} = a_{r,s}$ whenever $i+j = r+s$ $(i, j , r , s = 1, 2, \ldots, n)$. What would be dimension of $H_n$ as a vector space over $\mathbb{R}$? i have options for the dimension 1 -  $n^2$ 2-   $n^2-n+1$ 3 - $2n+1$ 4- $2n-1$ I am finding difficulty in identifying the matrix $A$ . thanks for support",,"['linear-algebra', 'matrices']"
4,What is the expected root mean square determinant of an $n\times n$ matrix?,What is the expected root mean square determinant of an  matrix?,n\times n,The expected mean determinant of random $n\times n$ matrices of $0$'s and $1$'s is $0$. What is the expected root mean square determinant? e.g. $\frac{\sqrt{3}}{2\sqrt{2}}$ for a $2\times 2$,The expected mean determinant of random $n\times n$ matrices of $0$'s and $1$'s is $0$. What is the expected root mean square determinant? e.g. $\frac{\sqrt{3}}{2\sqrt{2}}$ for a $2\times 2$,,['matrices']
5,How to compute the characteristic polynomial of $A$,How to compute the characteristic polynomial of,A,"The matrix associated with $f$ is: $$ \left(\begin{array}{rrr}      3 & -1 & -1 \\     -1 & 3 & -1 \\     -1 & -1 & 3   \end{array}\right) . $$ First, I am going to find the characteristic equation of $\det(A- \lambda I)$. Please correct me if I'm wrong. $$= (3-\lambda)(3-\lambda)(3-\lambda)-1-1-(3-\lambda)-(3-\lambda)-(3-\lambda) =-\lambda^3+9\lambda^2-24\lambda+16 .$$ How to factor this? I know the $\lambda$'s should be $1$, $4$ and $4$. But how am I supposed to find these values?","The matrix associated with $f$ is: $$ \left(\begin{array}{rrr}      3 & -1 & -1 \\     -1 & 3 & -1 \\     -1 & -1 & 3   \end{array}\right) . $$ First, I am going to find the characteristic equation of $\det(A- \lambda I)$. Please correct me if I'm wrong. $$= (3-\lambda)(3-\lambda)(3-\lambda)-1-1-(3-\lambda)-(3-\lambda)-(3-\lambda) =-\lambda^3+9\lambda^2-24\lambda+16 .$$ How to factor this? I know the $\lambda$'s should be $1$, $4$ and $4$. But how am I supposed to find these values?",,"['matrices', 'polynomials', 'eigenvalues-eigenvectors', 'roots']"
6,"""Change in rotation"" matrix","""Change in rotation"" matrix",,"Given two rotation matrices A and B (rotated from the same initial frame), how can I find the rotation matrix that represents the change in rotation from A to B ? (I actually want to find the Euler axis and angle that represents that change in rotation.) My one idea seems too involved: Convert A and B into their equivalent Euler angles, find the difference in Euler angles, convert the difference back into a rotation matrix, and then convert the new rotation matrix into Euler axis and angle. Thanks for the help!","Given two rotation matrices A and B (rotated from the same initial frame), how can I find the rotation matrix that represents the change in rotation from A to B ? (I actually want to find the Euler axis and angle that represents that change in rotation.) My one idea seems too involved: Convert A and B into their equivalent Euler angles, find the difference in Euler angles, convert the difference back into a rotation matrix, and then convert the new rotation matrix into Euler axis and angle. Thanks for the help!",,"['matrices', 'rotations']"
7,How come the columns of a matrix can form its nullspace?,How come the columns of a matrix can form its nullspace?,,"I am trying to work out some problems and then I realise something funny. Say I have a matrix : $$ A=\begin{bmatrix} -2 & 1 & 3\\  -1.5 & 1 & 2\\  -1.5 & 1 & 2 \end{bmatrix}$$ The $\operatorname{rank}(A)=2$. I want to find the null space of the matrix $A$: $$ \begin{bmatrix} -2 & 1 & 3\\  -1.5 & 1 & 2\\  -1.5 & 1 & 2 \end{bmatrix} \begin{bmatrix} x_1\\  x_2\\  x_3 \end{bmatrix}=0$$ And I got it through the usual steps: $ N(A)=t\begin{bmatrix} 2\\  1\\  1 \end{bmatrix}, t \in \mathbb{R}$ But what surprises me is that the null space could be a combination of the columns of $A$! $$ \begin{bmatrix} 2\\  1\\  1 \end{bmatrix}=-2\begin{bmatrix} -2 \\  -1.5 \\  -1.5  \end{bmatrix}-2 \begin{bmatrix}  1 \\  1 \\   1  \end{bmatrix}$$ How is this possible? I thought if it in the null space, then it wouldn't be in the column space of $A$.","I am trying to work out some problems and then I realise something funny. Say I have a matrix : $$ A=\begin{bmatrix} -2 & 1 & 3\\  -1.5 & 1 & 2\\  -1.5 & 1 & 2 \end{bmatrix}$$ The $\operatorname{rank}(A)=2$. I want to find the null space of the matrix $A$: $$ \begin{bmatrix} -2 & 1 & 3\\  -1.5 & 1 & 2\\  -1.5 & 1 & 2 \end{bmatrix} \begin{bmatrix} x_1\\  x_2\\  x_3 \end{bmatrix}=0$$ And I got it through the usual steps: $ N(A)=t\begin{bmatrix} 2\\  1\\  1 \end{bmatrix}, t \in \mathbb{R}$ But what surprises me is that the null space could be a combination of the columns of $A$! $$ \begin{bmatrix} 2\\  1\\  1 \end{bmatrix}=-2\begin{bmatrix} -2 \\  -1.5 \\  -1.5  \end{bmatrix}-2 \begin{bmatrix}  1 \\  1 \\   1  \end{bmatrix}$$ How is this possible? I thought if it in the null space, then it wouldn't be in the column space of $A$.",,"['linear-algebra', 'matrices']"
8,Why is the determinant of the following matrix zero,Why is the determinant of the following matrix zero,,"Let $a_1,\ldots,a_n$ be complex numbers and let $b_1,\ldots,b_n$ be complex numbers. Let $A$ be the matrix whose $(i,j)$-th entry is $A_{ij} = a_i b_j$. Then I think $\det A = 0$ when $n>1$. This is easy to compute when $n=2$. Question. Why is $\det A =0$?","Let $a_1,\ldots,a_n$ be complex numbers and let $b_1,\ldots,b_n$ be complex numbers. Let $A$ be the matrix whose $(i,j)$-th entry is $A_{ij} = a_i b_j$. Then I think $\det A = 0$ when $n>1$. This is easy to compute when $n=2$. Question. Why is $\det A =0$?",,"['linear-algebra', 'matrices']"
9,Why does this method for solving matrix equations work?,Why does this method for solving matrix equations work?,,"I have this assignment: Given: $A = \begin{pmatrix} 2 & 4 \\ 0 & 3 \end{pmatrix}$ $C = \begin {pmatrix} -1 & 2 \\ -6 & 3 \end{pmatrix}$ Find all B that satisfy $AB = C$. I know that one option is to say $B = \left( \begin{smallmatrix} a & b \\ c & d \end{smallmatrix} \right) $ and multiply it with $A$. By making each member equal to the one in $C$, I have a system of linear equations which I can solve. However, I also know that I can set up a system like this: $$ \left( \begin{array} {cc|cc} 2 & 4 & -1 & 2 \\                                 0 & 3 & -6 & 3 \end{array} \right)$$ If I manipulate it like I would a system of linear equations (for example, by swapping rows, or adding a multiple of a row to another) to get the identity matrix $\left( \begin{smallmatrix} 1 & 0 \\ 0 & 1 \end{smallmatrix} \right)$, then what I'm looking for (matrix $B$) will appear in the right hand side, like this: $$ \left( \begin{array} {cc|cc} 1 & 0 & 7/2 & -1 \\                                 0 & 1 & -2 & 1 \end{array} \right)$$ In this case, $B = \left( \begin{smallmatrix} 7/2 & -1 \\ -2 & 1\end{smallmatrix} \right)$. My question is, quite simply, how does this work? It looks like magic to me right now.","I have this assignment: Given: $A = \begin{pmatrix} 2 & 4 \\ 0 & 3 \end{pmatrix}$ $C = \begin {pmatrix} -1 & 2 \\ -6 & 3 \end{pmatrix}$ Find all B that satisfy $AB = C$. I know that one option is to say $B = \left( \begin{smallmatrix} a & b \\ c & d \end{smallmatrix} \right) $ and multiply it with $A$. By making each member equal to the one in $C$, I have a system of linear equations which I can solve. However, I also know that I can set up a system like this: $$ \left( \begin{array} {cc|cc} 2 & 4 & -1 & 2 \\                                 0 & 3 & -6 & 3 \end{array} \right)$$ If I manipulate it like I would a system of linear equations (for example, by swapping rows, or adding a multiple of a row to another) to get the identity matrix $\left( \begin{smallmatrix} 1 & 0 \\ 0 & 1 \end{smallmatrix} \right)$, then what I'm looking for (matrix $B$) will appear in the right hand side, like this: $$ \left( \begin{array} {cc|cc} 1 & 0 & 7/2 & -1 \\                                 0 & 1 & -2 & 1 \end{array} \right)$$ In this case, $B = \left( \begin{smallmatrix} 7/2 & -1 \\ -2 & 1\end{smallmatrix} \right)$. My question is, quite simply, how does this work? It looks like magic to me right now.",,"['linear-algebra', 'matrices']"
10,Is this matrix diagonalisable?,Is this matrix diagonalisable?,,"Let $A$ be the following matrix: $$A = \left(\begin{array}{rrr} -1 & \hphantom{-}3 & \hphantom{-}0\\ 0  & 2 & 0\\ -3 & 3 & 2 \end{array}\right).$$ I've found that the eigenvalues are -1 and 2 (multiplicity 2). However, when I try to find the eigenvectors for the eigenvalue 2, I can only find one, as the augmented matrix $A-2I$ reduces to  $$\left(\begin{array}{rrr} 1 & -1 & 0\\ 0 & 0 & 0\\ 0 & 0 & 0 \end{array}\right).$$ So is there any other way to diagonalise this matrix, or have I made a mistake somewhere?","Let $A$ be the following matrix: $$A = \left(\begin{array}{rrr} -1 & \hphantom{-}3 & \hphantom{-}0\\ 0  & 2 & 0\\ -3 & 3 & 2 \end{array}\right).$$ I've found that the eigenvalues are -1 and 2 (multiplicity 2). However, when I try to find the eigenvectors for the eigenvalue 2, I can only find one, as the augmented matrix $A-2I$ reduces to  $$\left(\begin{array}{rrr} 1 & -1 & 0\\ 0 & 0 & 0\\ 0 & 0 & 0 \end{array}\right).$$ So is there any other way to diagonalise this matrix, or have I made a mistake somewhere?",,[]
11,Finding Smith normal form of a $\mathbb C[\lambda]$-matrix,Finding Smith normal form of a -matrix,\mathbb C[\lambda],"Let $J_n(\lambda)$ denote the Jordan block of size $n$ with eigenvalue $\lambda$ , i.e. $$J_n(\lambda)=\begin{pmatrix} \lambda & 1 & & \\ & \lambda & \ddots & \\ & & \ddots & 1\\ & & & \lambda \end{pmatrix},$$ Then $J_n(\lambda)$ could be viewed as a matrix over the ring $\mathbb C[\lambda]$ . What is the Smith normal form of $J_n^n(\lambda)=(J_n^n(\lambda))^n$ ? For example, the matrix $J_3^3(\lambda)$ is $\begin{pmatrix} \lambda^3 & 3\lambda^2 & 3\lambda\\ 0 & \lambda^3 & 3\lambda^2\\ 0 & 0 & \lambda^3 \end{pmatrix}$ , and the Smith form is $\text {diag}\{\lambda, \lambda^3, \lambda^5\}$ . Some brute force calculations show that the Smith form of general $J_n^n(\lambda)$ is very likely to be $$\text {diag}\{\lambda, \lambda^3, \dots, \lambda^{2n-1}\}  (\ast)$$ Is there any way to prove it? Moreover, if possible, can we do the more general case $J_n^m(\lambda)$ where $m\ne n$ ? Thanks for any help in advance. (Motivation: I ended up with this $\lambda$ -matrix while trying to determine the Jordan form of $I_n\otimes J_n(0)+ J_n(0)\otimes I_n$ . I have already shown in other approaches that the largest Jordan block of $I_n\otimes J_n(0)+ J_n(0)\otimes I_n$ has size $2n-1$ and it has exactly $n$ Jordan blocks (which matches with the conjecture $(\ast)$ ). But I can't prove that the explicit Jordan form is $\text{diag}\{J_1(0),J_3(0),\dots, J_{2n-1}(0)\}$ . It would also be appreciated if any of u solve this more original problem:) Edit: Jyrki's answer probably does the work, but as I don't have any knowledge of Lie algebra, I need a more elementary answer","Let denote the Jordan block of size with eigenvalue , i.e. Then could be viewed as a matrix over the ring . What is the Smith normal form of ? For example, the matrix is , and the Smith form is . Some brute force calculations show that the Smith form of general is very likely to be Is there any way to prove it? Moreover, if possible, can we do the more general case where ? Thanks for any help in advance. (Motivation: I ended up with this -matrix while trying to determine the Jordan form of . I have already shown in other approaches that the largest Jordan block of has size and it has exactly Jordan blocks (which matches with the conjecture ). But I can't prove that the explicit Jordan form is . It would also be appreciated if any of u solve this more original problem:) Edit: Jyrki's answer probably does the work, but as I don't have any knowledge of Lie algebra, I need a more elementary answer","J_n(\lambda) n \lambda J_n(\lambda)=\begin{pmatrix}
\lambda & 1 & & \\
& \lambda & \ddots & \\
& & \ddots & 1\\
& & & \lambda
\end{pmatrix}, J_n(\lambda) \mathbb C[\lambda] J_n^n(\lambda)=(J_n^n(\lambda))^n J_3^3(\lambda) \begin{pmatrix}
\lambda^3 & 3\lambda^2 & 3\lambda\\
0 & \lambda^3 & 3\lambda^2\\
0 & 0 & \lambda^3
\end{pmatrix} \text {diag}\{\lambda, \lambda^3, \lambda^5\} J_n^n(\lambda) \text {diag}\{\lambda, \lambda^3, \dots, \lambda^{2n-1}\}  (\ast) J_n^m(\lambda) m\ne n \lambda I_n\otimes J_n(0)+ J_n(0)\otimes I_n I_n\otimes J_n(0)+ J_n(0)\otimes I_n 2n-1 n (\ast) \text{diag}\{J_1(0),J_3(0),\dots, J_{2n-1}(0)\}","['linear-algebra', 'matrices', 'jordan-normal-form', 'smith-normal-form']"
12,What is the maximum number of non-zero entries of a matrix $A$ with non-negative entries that fulfills $A^2 = 0$,What is the maximum number of non-zero entries of a matrix  with non-negative entries that fulfills,A A^2 = 0,"A question I found and that I could not answer so far. Assuming that $A$ is a $n \times n$ matrix with non-negative entries, that fulfills the equation $A^2= 0$ , where $0$ is the zero matrix. What is the maximal number of positive entries for $A$ ? So far I have tried going through the specific entries, namely to solve $0 = \sum_{k=1}^n a_{i,k} \cdot a_{k,j}$ for all $i,j$ , but I was hoping for a simpler way. An example for $n=5$ has been $ \begin{bmatrix} 0 & 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 0 \\ \end{bmatrix} $ which fulfills the equation, but I do not know if there are better possibilities","A question I found and that I could not answer so far. Assuming that is a matrix with non-negative entries, that fulfills the equation , where is the zero matrix. What is the maximal number of positive entries for ? So far I have tried going through the specific entries, namely to solve for all , but I was hoping for a simpler way. An example for has been which fulfills the equation, but I do not know if there are better possibilities","A n \times n A^2= 0 0 A 0 = \sum_{k=1}^n a_{i,k} \cdot a_{k,j} i,j n=5 
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
","['linear-algebra', 'matrices', 'graph-theory']"
13,Evaluate binary determinant,Evaluate binary determinant,,"Let $\alpha=(a_1, \dots, a_n), \beta=(b_1, \dots, b_n) \in \mathbb{F}_2^n$ . Let $E$ denote the identity matrix. I am trying to evaluate determinant $\det(E + \alpha^T\beta) $ . Some experiments show that it will be equal to $1+\alpha\beta^T$ . Can anybody help with proof or may be give some hints how to apply an induction principle?",Let . Let denote the identity matrix. I am trying to evaluate determinant . Some experiments show that it will be equal to . Can anybody help with proof or may be give some hints how to apply an induction principle?,"\alpha=(a_1, \dots, a_n), \beta=(b_1, \dots, b_n) \in \mathbb{F}_2^n E \det(E + \alpha^T\beta)  1+\alpha\beta^T","['linear-algebra', 'matrices', 'determinant', 'finite-fields']"
14,"If $ A $ and $ A^T $ commute, do they share the same eigenvectors when they are not diagonalizable?","If  and  commute, do they share the same eigenvectors when they are not diagonalizable?", A   A^T ,"I saw this on Strong's book Introduction to Linear Algebra , but it didn't prove it: If $ A $ and $ A^T $ commute, then they share the same eigenvectors. And now I am wondering whether it is still true even if they may not be diagonalizable. There are many similar questions like "" $A$ and $B$ are both diagonalizable and commute if and only if they share the same eigenvectors"", but I am still wondering whether it is still true when $B=A^T$ and $A$ is not diagonalizable.","I saw this on Strong's book Introduction to Linear Algebra , but it didn't prove it: If and commute, then they share the same eigenvectors. And now I am wondering whether it is still true even if they may not be diagonalizable. There are many similar questions like "" and are both diagonalizable and commute if and only if they share the same eigenvectors"", but I am still wondering whether it is still true when and is not diagonalizable.", A   A^T  A B B=A^T A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'transpose']"
15,"An example of a matrix in $\mathrm{SL}(4, \mathbb{Z})$ with the following properties",An example of a matrix in  with the following properties,"\mathrm{SL}(4, \mathbb{Z})","I am looking for a matrix $M\in \mathrm{SL}(4, \mathbb{Z})$ , with all eigenvalues equal to $1$ , and with the following properties: Write $M=\begin{bmatrix} A_1&A_2\\ A_3&A_4  \end{bmatrix},  $ where the $A_i$ are $2$ by $2$ sumbatrix of $M$ . Let $d_i$ be the dot product of two rows of $A_i$ , i.e. if $A_i = \begin{bmatrix} a&b\\ c&d  \end{bmatrix}$ , then $d_i = ac +bd$ . Let $a_i = \mathrm{det}(A_i) - d_i$ . For example if $A_1 =\begin{bmatrix} 1&2\\ 3&4 \end{bmatrix}$ ,  then $d_1 = 11$ and $a_1 = - 2 - 11 = -13$ . Consider the matrix $A =  \begin{bmatrix} a_1&a_2\\ a_3&a_4  \end{bmatrix}$ ,  I would like to find $M$ such that $A$ is in $\mathrm{GL}(2, \mathbb{Z})$ , and has one eigenvalue with absolute value not equal to $1$ . The matrices I have tried so far : since $M$ needs to have eigenvalues all equal to $1$ , I tried the matrices consisting of just Jordan blocks, unfortunately, they didn't work. I also tried the matrix in the form $M=\begin{bmatrix} A_1&A_2\\ 0&A_4  \end{bmatrix},  $ where $A_1$ and $A_4$ have all eigenvalues all equal to $1$ , but in this case $A$ will have all eigenvalues with absolute values $1$ . I am a bit stuck as I can't think of other $4$ by $4$ matrices that have all eigenvalues equal to $1$ . Any idea to construct such matrices will be really appreciated.","I am looking for a matrix , with all eigenvalues equal to , and with the following properties: Write where the are by sumbatrix of . Let be the dot product of two rows of , i.e. if , then . Let . For example if ,  then and . Consider the matrix ,  I would like to find such that is in , and has one eigenvalue with absolute value not equal to . The matrices I have tried so far : since needs to have eigenvalues all equal to , I tried the matrices consisting of just Jordan blocks, unfortunately, they didn't work. I also tried the matrix in the form where and have all eigenvalues all equal to , but in this case will have all eigenvalues with absolute values . I am a bit stuck as I can't think of other by matrices that have all eigenvalues equal to . Any idea to construct such matrices will be really appreciated.","M\in \mathrm{SL}(4, \mathbb{Z}) 1 M=\begin{bmatrix}
A_1&A_2\\
A_3&A_4 
\end{bmatrix}, 
 A_i 2 2 M d_i A_i A_i = \begin{bmatrix}
a&b\\
c&d 
\end{bmatrix} d_i = ac +bd a_i = \mathrm{det}(A_i) - d_i A_1 =\begin{bmatrix}
1&2\\
3&4
\end{bmatrix} d_1 = 11 a_1 = - 2 - 11 = -13 A =  \begin{bmatrix}
a_1&a_2\\
a_3&a_4 
\end{bmatrix} M A \mathrm{GL}(2, \mathbb{Z}) 1 M 1 M=\begin{bmatrix}
A_1&A_2\\
0&A_4 
\end{bmatrix}, 
 A_1 A_4 1 A 1 4 4 1","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'eigenvalues-eigenvectors']"
16,"An example of a non-diagonalisable matrix in $\mathrm{SL}(n, \mathbb{Z})$ whose eigenvalues don't all have absolute value $1$",An example of a non-diagonalisable matrix in  whose eigenvalues don't all have absolute value,"\mathrm{SL}(n, \mathbb{Z}) 1","I was wondering if there exists a matrix $M \in \mathrm{SL}(n, \mathbb{Z})$ , such that: $M$ is not diagonalisable; $M$ does not have all eigenvalues with absolute value $1$ . Thoughts : The only non-diagonalisable matrix in $\mathrm{SL}(n, \mathbb{Z})$ I can think of are the ones consisting of Jordan block with $\pm 1$ on the diagonal. Any hint on how to construct such a matrix would be really appreciated.","I was wondering if there exists a matrix , such that: is not diagonalisable; does not have all eigenvalues with absolute value . Thoughts : The only non-diagonalisable matrix in I can think of are the ones consisting of Jordan block with on the diagonal. Any hint on how to construct such a matrix would be really appreciated.","M \in \mathrm{SL}(n, \mathbb{Z}) M M 1 \mathrm{SL}(n, \mathbb{Z}) \pm 1","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'eigenvalues-eigenvectors']"
17,"Are the inverse of ""nearly"" diagonal nonsingular matrices also ""nearly"" diagonal?","Are the inverse of ""nearly"" diagonal nonsingular matrices also ""nearly"" diagonal?",,"Assumption: Given a sequence of square matrices $P^k, k=1,2, ..., n$ with $\lim_{k \to \infty} P^k_{ij} = 0$ for $i \neq j$ (but the diagonal element of $P^k$ may not converge). Moreover, the determinant of the sequence is a constant, suppose $\det P_k = 1, k=1,2, ... n$ . Question: Let $U^k = (P^k)^{-1}$ . I am wondering if $\lim_{k \to \infty} U^k_{ij} = 0$ , for $i \neq j$ ? Currently, I think perhaps we can start from the equaiton: $$ U^k_{ij} = \frac{C^k_{ji}}{\det P^k} $$ , where $C^k_{ji}$ is the $(j, i)$ cofactor of $P^k$ . But I can not go through further. Also, it might by easier if we suppose the diagonal elements of $P^k, k=1,2, ..., n$ are uniformly bounded so that $P^k$ has a convergent subsequence.","Assumption: Given a sequence of square matrices with for (but the diagonal element of may not converge). Moreover, the determinant of the sequence is a constant, suppose . Question: Let . I am wondering if , for ? Currently, I think perhaps we can start from the equaiton: , where is the cofactor of . But I can not go through further. Also, it might by easier if we suppose the diagonal elements of are uniformly bounded so that has a convergent subsequence.","P^k, k=1,2, ..., n \lim_{k \to \infty} P^k_{ij} = 0 i \neq j P^k \det P_k = 1, k=1,2, ... n U^k = (P^k)^{-1} \lim_{k \to \infty} U^k_{ij} = 0 i \neq j 
U^k_{ij} = \frac{C^k_{ji}}{\det P^k}
 C^k_{ji} (j, i) P^k P^k, k=1,2, ..., n P^k","['calculus', 'linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
18,Solving a simple matrix multiplication/evaluation problem in a shorter/simpler way,Solving a simple matrix multiplication/evaluation problem in a shorter/simpler way,,"I've solved the following problem by calculating matrix A's entries as variables and then evaluating them using the system of linear equations. As I'm a bit new to matrices, I wanted to know whether there are shorter ways to solving this problem cause solving it from the way I did seems so long... If $$ 	\begin{bmatrix}  	2 & 1 \\ 	\end{bmatrix} . A =\begin{bmatrix}  	3 & 5 \\ 	\end{bmatrix} $$ and $$ 	\begin{bmatrix}  	3 & 4 \\ 	\end{bmatrix} . A =\begin{bmatrix}  	-1 & 2 \\ 	\end{bmatrix} $$ what's the value of $$ 	\begin{bmatrix}  	8 & 9 \\ 	\end{bmatrix} . A $$ $A$ is a matrix","I've solved the following problem by calculating matrix A's entries as variables and then evaluating them using the system of linear equations. As I'm a bit new to matrices, I wanted to know whether there are shorter ways to solving this problem cause solving it from the way I did seems so long... If and what's the value of is a matrix","
	\begin{bmatrix} 
	2 & 1 \\
	\end{bmatrix} . A =\begin{bmatrix} 
	3 & 5 \\
	\end{bmatrix}
 
	\begin{bmatrix} 
	3 & 4 \\
	\end{bmatrix} . A =\begin{bmatrix} 
	-1 & 2 \\
	\end{bmatrix}
 
	\begin{bmatrix} 
	8 & 9 \\
	\end{bmatrix} . A
 A",['matrices']
19,A JEE Exam problem on determinants and matrices,A JEE Exam problem on determinants and matrices,,"I am first stating the question: Let $A=\{a_{ij}\}$ be a $3\times 3$ matrix, where $$a_{ij}=\begin{cases}  (-1)^{j-i}&\text{if $i<j$,}\\ 2&\text{if $i=j$,}\\ (-1)^{i-j}&\text{if $i>j$,} \end{cases}$$ then $\det(3\,\text{adj}(2A^{-1}))$ is equal to __________ I solved this in the following manner: $$ A=\left[\begin{array}{lcc} 2 & (-1)^{2-1} & (-1)^{3-1} \\ (-1)^{2+1} & 2 & (-1)^{3-2} \\ (-1)^{3+1} & (-1)^{3 + 2} & 2 \end{array}\right]=\left[\begin{array}{ccc} 2 & -1 & 1 \\ -1 & 2 & -1 \\ 1 & -1 & 2 \end{array}\right] $$ $$\begin{aligned}|A| &=2(4-1)+(-2+1)+(1-2) \\ &=6-1-1=4 \end{aligned}$$ $$ \begin{aligned} & \operatorname{det}\left(3 \operatorname{adj}\left(2 A^{-1}\right)\right) \\ =& 3^{3}\left|\operatorname{adj}\left(2 A^{-1}\right)\right| \\ =& 3^{3}\left|2^{3} \operatorname{adj}\left(A^{-1}\right)\right| \\ =&(3 \times 2)^{3} \times\left(\left|A^{-1}\right|\right)^{2}\\=&6^3\times\Big(\frac14\Big)\\=&13.5 \end{aligned} $$ Original image Is my solution correct? Note: The problem came in the JEE Main Exam of India, on the 20th of July. The answer given for this question in the Answer Key is 108.","I am first stating the question: Let be a matrix, where then is equal to __________ I solved this in the following manner: Original image Is my solution correct? Note: The problem came in the JEE Main Exam of India, on the 20th of July. The answer given for this question in the Answer Key is 108.","A=\{a_{ij}\} 3\times 3 a_{ij}=\begin{cases} 
(-1)^{j-i}&\text{if i<j,}\\
2&\text{if i=j,}\\
(-1)^{i-j}&\text{if i>j,}
\end{cases} \det(3\,\text{adj}(2A^{-1})) 
A=\left[\begin{array}{lcc}
2 & (-1)^{2-1} & (-1)^{3-1} \\
(-1)^{2+1} & 2 & (-1)^{3-2} \\
(-1)^{3+1} & (-1)^{3 + 2} & 2
\end{array}\right]=\left[\begin{array}{ccc}
2 & -1 & 1 \\
-1 & 2 & -1 \\
1 & -1 & 2
\end{array}\right]
 \begin{aligned}|A| &=2(4-1)+(-2+1)+(1-2) \\ &=6-1-1=4 \end{aligned} 
\begin{aligned}
& \operatorname{det}\left(3 \operatorname{adj}\left(2 A^{-1}\right)\right) \\
=& 3^{3}\left|\operatorname{adj}\left(2 A^{-1}\right)\right| \\
=& 3^{3}\left|2^{3} \operatorname{adj}\left(A^{-1}\right)\right| \\
=&(3 \times 2)^{3} \times\left(\left|A^{-1}\right|\right)^{2}\\=&6^3\times\Big(\frac14\Big)\\=&13.5
\end{aligned}
","['matrices', 'determinant']"
20,Computing trace of matrix,Computing trace of matrix,,Let A be a n×n matrix such that $$[a_{ij}]_{n×n}=\frac{((-1)^i)(2i^2+1)}{4j^4+1}$$ then what is $$1+ \lim_{n\to {\infty}}\left(tr(A^n)^{1/n}\right)$$ I cannot figure out how to calculate trace of $$A^n$$,Let A be a n×n matrix such that then what is I cannot figure out how to calculate trace of,[a_{ij}]_{n×n}=\frac{((-1)^i)(2i^2+1)}{4j^4+1} 1+ \lim_{n\to {\infty}}\left(tr(A^n)^{1/n}\right) A^n,"['matrices', 'limits', 'matrix-calculus', 'trace']"
21,Derivative involving inverse matrix,Derivative involving inverse matrix,,"Let $\alpha_i\in\mathbb{R}$ , $x_i\in\mathbb{R}^d$ for all $i\in[k]$ , with $k \geq d$ . I am looking for this derivative $$ \frac{\partial}{\partial\alpha_i} \left(\sum_{i=1}^k\alpha_ix_ix_i^T\right)^{-1} = \frac{\partial}{\partial\alpha_i} \left(X\Lambda X^\top\right)^{-1}, $$ where we define $X: \text{col(X)}= \{x_i\}_{i\in[k]}$ , $\Lambda = \text{diag}(\alpha)$ , and we assume $\left(X\Lambda X^\top\right)$ is invertible.","Let , for all , with . I am looking for this derivative where we define , , and we assume is invertible.","\alpha_i\in\mathbb{R} x_i\in\mathbb{R}^d i\in[k] k \geq d 
\frac{\partial}{\partial\alpha_i} \left(\sum_{i=1}^k\alpha_ix_ix_i^T\right)^{-1} = \frac{\partial}{\partial\alpha_i} \left(X\Lambda X^\top\right)^{-1},
 X: \text{col(X)}= \{x_i\}_{i\in[k]} \Lambda = \text{diag}(\alpha) \left(X\Lambda X^\top\right)","['matrices', 'derivatives']"
22,Inverse of a matrix and matrix relation,Inverse of a matrix and matrix relation,,"Let $A$ be the matrix $$ A= \begin{pmatrix} 2 & -1 & -1\\ 0 & -2 & -1 \\ 0 & 3 & 2 \\ \end{pmatrix} $$ I am trying to find $A^{-1}$ as a relation of $I_{3}, A$ and $A^{2}$ and also to prove that $A^{2006}-2A^{2005}=A^{2}-2A$ .  For the first one I noticed that $$A^{n}= \begin{pmatrix} 2^{n} & -(2^{n}-1) & -(2^{n}-1)\\ 0 & -2 & -1 \\ 0 & 3 & 2 \\ \end{pmatrix} , A^{k}= \begin{pmatrix} 2^{k} & -(2^{k}-1) & -(2^{k}-1)\\ 0 & 1 & 0 \\ 0 & 0 &  1\\ \end{pmatrix}$$ for $n$ odd and $k$ even but I don't know how to proceed from here. Also the the equality $A^{2006}-2A^{2005}=A^{2}-2A$ leads to $A^{2004}(A-2I_{3})=A-2I_{3}$ but again I don't know what to do next since $(A-2I_{3})$ is not invertible. Any help?",Let be the matrix I am trying to find as a relation of and and also to prove that .  For the first one I noticed that for odd and even but I don't know how to proceed from here. Also the the equality leads to but again I don't know what to do next since is not invertible. Any help?,"A  A= \begin{pmatrix}
2 & -1 & -1\\
0 & -2 & -1 \\
0 & 3 & 2 \\
\end{pmatrix}  A^{-1} I_{3}, A A^{2} A^{2006}-2A^{2005}=A^{2}-2A A^{n}= \begin{pmatrix}
2^{n} & -(2^{n}-1) & -(2^{n}-1)\\
0 & -2 & -1 \\
0 & 3 & 2 \\ \end{pmatrix} , A^{k}= \begin{pmatrix}
2^{k} & -(2^{k}-1) & -(2^{k}-1)\\
0 & 1 & 0 \\
0 & 0 &  1\\ \end{pmatrix} n k A^{2006}-2A^{2005}=A^{2}-2A A^{2004}(A-2I_{3})=A-2I_{3} (A-2I_{3})","['linear-algebra', 'matrices', 'inverse']"
23,Is $\left(\begin{smallmatrix}0&0&1\\1&0&0\\ 0&1&0\end{smallmatrix}\right)$ diagonalizable over $\mathbb{Z}_2$?,Is  diagonalizable over ?,\left(\begin{smallmatrix}0&0&1\\1&0&0\\ 0&1&0\end{smallmatrix}\right) \mathbb{Z}_2,"Is $A= \begin{pmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0 \end{pmatrix}$ diagonalizable over $\mathbb{Z}_2$ ? I tried two approaches and got two different answers so I was hoping someone could point me to a flaw in my reasoning: First approach: The minimal polynomial for $A$ is easily found to be $m(x) =x^3-1$ which is the same as $x-1$ over $\mathbb{Z}_2$ . Since the minimal polynomial decomposes into distinct linear factors it must be that $A$ is diagonalizable over $\mathbb{Z}_2$ . Second approach: It follows from the minimal polynomial that $1$ is the only eigenvalue of $A$ . The eigenvector equation is $\begin{pmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} x \\ y\\ z \end{pmatrix} = \begin{pmatrix} z \\ x\\ y \end{pmatrix} = 1 \times \begin{pmatrix} x \\ y\\ z \end{pmatrix}  $ and the only solution is $\begin{pmatrix} 1 \\ 1\\ 1 \end{pmatrix}$ . But $\mathbb{Z}_2^3$ has dimension $3$ , so there is no basis for $\mathbb{Z}_2^3$ consisting of eigenvectors for $A$ . $A$ can't be diagonalized over $\mathbb{Z}_2$ . What went wrong? Many thanks!","Is diagonalizable over ? I tried two approaches and got two different answers so I was hoping someone could point me to a flaw in my reasoning: First approach: The minimal polynomial for is easily found to be which is the same as over . Since the minimal polynomial decomposes into distinct linear factors it must be that is diagonalizable over . Second approach: It follows from the minimal polynomial that is the only eigenvalue of . The eigenvector equation is and the only solution is . But has dimension , so there is no basis for consisting of eigenvectors for . can't be diagonalized over . What went wrong? Many thanks!",A= \begin{pmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0 \end{pmatrix} \mathbb{Z}_2 A m(x) =x^3-1 x-1 \mathbb{Z}_2 A \mathbb{Z}_2 1 A \begin{pmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} x \\ y\\ z \end{pmatrix} = \begin{pmatrix} z \\ x\\ y \end{pmatrix} = 1 \times \begin{pmatrix} x \\ y\\ z \end{pmatrix}   \begin{pmatrix} 1 \\ 1\\ 1 \end{pmatrix} \mathbb{Z}_2^3 3 \mathbb{Z}_2^3 A A \mathbb{Z}_2,"['linear-algebra', 'matrices']"
24,Is matrix exponential injective and surjective?,Is matrix exponential injective and surjective?,,"Exponential of real numbers exp: $\mathbb{R}\rightarrow\mathbb{R}$ is injective. Does the same hold for exponential of real matrices exp: $\mathbb{R}^{2\times2}\rightarrow\mathbb{R}^{2\times2}$ ? What about surjectivity, can every regular real matrix be written as $X=$ exp $A$ ?","Exponential of real numbers exp: is injective. Does the same hold for exponential of real matrices exp: ? What about surjectivity, can every regular real matrix be written as exp ?",\mathbb{R}\rightarrow\mathbb{R} \mathbb{R}^{2\times2}\rightarrow\mathbb{R}^{2\times2} X= A,"['linear-algebra', 'matrices', 'matrix-exponential']"
25,"Any element $g$ of $GL(2,p)$ of order $p$, $p$ prime, is conjugate to $\begin{bmatrix}1&1\\0&1\end{bmatrix}$","Any element  of  of order ,  prime, is conjugate to","g GL(2,p) p p \begin{bmatrix}1&1\\0&1\end{bmatrix}","Any element $g$ of $GL(2,p)$ of order $p$ , $p$ prime, is conjugate to $\begin{bmatrix}1&1\\0&1\end{bmatrix}.$ I showed that $\langle g\rangle $ acts on the set $X$ of vectors with entries in $ F_p$ and hence that $g$ fixes some non-zero element of $X$ (By Orbit-Stabiliser, since $|X| = p^2$ and $|\langle g\rangle|=p$ ). In an exercise, I am then asked to deduce from this the statement above, which I am stuck on.","Any element of of order , prime, is conjugate to I showed that acts on the set of vectors with entries in and hence that fixes some non-zero element of (By Orbit-Stabiliser, since and ). In an exercise, I am then asked to deduce from this the statement above, which I am stuck on.","g GL(2,p) p p \begin{bmatrix}1&1\\0&1\end{bmatrix}. \langle g\rangle  X  F_p g X |X| = p^2 |\langle g\rangle|=p","['matrices', 'group-theory', 'finite-fields']"
26,How to determine all $2 \times 2$ normal matrices?,How to determine all  normal matrices?,2 \times 2,"Determine all $2 \times 2$ normal matrices. In particular, how would I show that there are normal matrices which are neither unitary, Hermitian, skew-Hermitian, symmetric, nor skew-symmetric. The only thing I do know is $AA^* = A^*A$ , but I'm not sure how to proceed. I tried writing in $a,b,c,d$ as entries of $2 \times 2$ matrix, but it seemed to lead nowhere.","Determine all normal matrices. In particular, how would I show that there are normal matrices which are neither unitary, Hermitian, skew-Hermitian, symmetric, nor skew-symmetric. The only thing I do know is , but I'm not sure how to proceed. I tried writing in as entries of matrix, but it seemed to lead nowhere.","2 \times 2 AA^* = A^*A a,b,c,d 2 \times 2","['linear-algebra', 'matrices', 'hermitian-matrices']"
27,Inequality for trace of product of matrices,Inequality for trace of product of matrices,,"Assume that $A \in \mathbb{R}^{n \times n}$ is a symmetric matrix and $B \in \mathbb{R}^{n \times n}$ is a symmetric positive definite matrix. Is the following statement true $$ \lambda_{\mathrm{min}} \operatorname{tr} A \le \operatorname{tr} (AB) \le \lambda_{\mathrm{max}} \operatorname{tr} A \, ? $$ Here, $\lambda_{\mathrm{min}}$ denotes the smallest eigenvalue of $B$ and $\lambda_{\mathrm{max}}$ denotes the largest eigenvalue of $B$ .","Assume that is a symmetric matrix and is a symmetric positive definite matrix. Is the following statement true Here, denotes the smallest eigenvalue of and denotes the largest eigenvalue of .","A \in \mathbb{R}^{n \times n} B \in \mathbb{R}^{n \times n} 
\lambda_{\mathrm{min}} \operatorname{tr} A \le \operatorname{tr} (AB) \le \lambda_{\mathrm{max}} \operatorname{tr} A \, ?
 \lambda_{\mathrm{min}} B \lambda_{\mathrm{max}} B","['linear-algebra', 'matrices', 'trace', 'positive-definite']"
28,Show that a certain set of invertible matrices is a normal subgroup of another set of invertible matrices (triangular),Show that a certain set of invertible matrices is a normal subgroup of another set of invertible matrices (triangular),,$U_n=$$\left( \begin{array}{rrrr} 1 & * & \cdots & * \\ 0 & \ddots & * & \vdots \\ \vdots & 0 & \ddots & * \\ 0 & \cdots & 0 & 1 \\ \end{array}\right) $$T_n=$$\left( \begin{array}{rrrr} * & * & \cdots & * \\ 0 & \ddots & * & \vdots \\ \vdots & 0 & \ddots & * \\ 0 & \cdots & 0 & * \\ \end{array}\right) $ I have to Show that $U_n$ is a normal subgroup of $T_n$ To this end I have to show (after I have verified that $U_n$ is a subgroup of $T_n$ ): $$\forall_{x\in T_n}\forall_{a\in U_n}\exists_{b\in U_n}xa=ba$$ I have looked at the case n=2 the results are $\begin{pmatrix} a & b \\ 0 & c \\ \end{pmatrix}$ $\begin{pmatrix} 1 & d \\ 0 & 1 \\ \end{pmatrix}$ $=\begin{pmatrix} a & ad+b \\ 0 & c \\ \end{pmatrix}$ $=\begin{pmatrix} 1 & adc^{-1} \\ 0 & 1 \\ \end{pmatrix}$ $\begin{pmatrix} a & b \\ 0 & c \\ \end{pmatrix}$ Now the Question is how can I choose the coefficients for my Matrix $b$ in Dependance to $a$ ? I suspect that the coefficients of $b$ can be determined this way (Note that I use the notation $b_{kj}$ which translates to: I look at the coefficient in the $k$ -th row at the $j$ -th column of the Matrix $b$ ) $$b_{kj}\begin{cases}1 &\mbox{if } j=k \\ 0 & \mbox{if } j>k\\a_{(k-1)j}a_{kj}a_{k(j+1)} & \mbox{otherwise}  \end{cases}$$ How can I prove this idea?,I have to Show that is a normal subgroup of To this end I have to show (after I have verified that is a subgroup of ): I have looked at the case n=2 the results are Now the Question is how can I choose the coefficients for my Matrix in Dependance to ? I suspect that the coefficients of can be determined this way (Note that I use the notation which translates to: I look at the coefficient in the -th row at the -th column of the Matrix ) How can I prove this idea?,"U_n=\left( \begin{array}{rrrr}
1 & * & \cdots & * \\
0 & \ddots & * & \vdots \\
\vdots & 0 & \ddots & * \\
0 & \cdots & 0 & 1 \\
\end{array}\right) T_n=\left( \begin{array}{rrrr}
* & * & \cdots & * \\
0 & \ddots & * & \vdots \\
\vdots & 0 & \ddots & * \\
0 & \cdots & 0 & * \\
\end{array}\right)  U_n T_n U_n T_n \forall_{x\in T_n}\forall_{a\in U_n}\exists_{b\in U_n}xa=ba \begin{pmatrix}
a & b \\
0 & c \\
\end{pmatrix} \begin{pmatrix}
1 & d \\
0 & 1 \\
\end{pmatrix} =\begin{pmatrix}
a & ad+b \\
0 & c \\
\end{pmatrix} =\begin{pmatrix}
1 & adc^{-1} \\
0 & 1 \\
\end{pmatrix} \begin{pmatrix}
a & b \\
0 & c \\
\end{pmatrix} b a b b_{kj} k j b b_{kj}\begin{cases}1 &\mbox{if } j=k \\ 0 & \mbox{if } j>k\\a_{(k-1)j}a_{kj}a_{k(j+1)} & \mbox{otherwise}  \end{cases}","['linear-algebra', 'matrices', 'proof-writing']"
29,"if $A \in C^{2015,2015}$ and $rank(A) < 1000$ proof that $\dim(\ker(A+A^T)) > 15$",if  and  proof that,"A \in C^{2015,2015} rank(A) < 1000 \dim(\ker(A+A^T)) > 15","I want to solve that thesis: if $A \in C^{2015,2015}$ and $rank(A) < 1000$ proof that $\dim(\ker(A+A^T)) > 15 $ from the fact that $$\dim(im(A)) = \dim(im(A^T))$$ and $$ \dim(\ker(A))+\dim(im(A))=n $$ it follows that $$\dim(\ker(A)) = \dim(\ker(A^T))$$ We know that $rank(A) < 1000$ . Hence $\dim(\ker(A))>1015$ But there I have stuck... thanks for your time",I want to solve that thesis: if and proof that from the fact that and it follows that We know that . Hence But there I have stuck... thanks for your time,"A \in C^{2015,2015} rank(A) < 1000 \dim(\ker(A+A^T)) > 15  \dim(im(A)) = \dim(im(A^T))  \dim(\ker(A))+\dim(im(A))=n  \dim(\ker(A)) = \dim(\ker(A^T)) rank(A) < 1000 \dim(\ker(A))>1015",['linear-algebra']
30,Stuck on a proof about rotating a matrix,Stuck on a proof about rotating a matrix,,"Given a matrix $A \in M^{n×n}(F)$ , let $A^{\rho}$ denote the matrix obtained from $A$ by ‘rotating’ it $90^{\circ}$ clockwise. For example, $$\begin{bmatrix} 1&2\\ 3&4 \end{bmatrix}^\rho =\begin{bmatrix} 3&1\\ 4&2 \end{bmatrix}.$$ Find (with proof) an expression for $A^\rho$ in terms of $A$ . Through analyzing $2\times 2, 3\times 3, 4\times 4$ , and $5\times 5$ cases, it seems that the rotation is equivalent to transposing A and then swapping the first column with the $n^{th}$ column, the second column with the $(n-1)^{th}$ column, and so on. Performing these column operations is equivalent to right multiplication by a permutation matrix. So I've concluded that $A^\rho$ is equivalent to $A^TP$ where $P$ is the the ""mirror image"" of $I_n$ . For example in the $5 \times 5$ case, $P= \begin{bmatrix}  0&0&0&0&1\\  0&0&0&1&0\\  0&0&1&0&0\\  0&1&0&0&0\\  1&0&0&0&0 \end{bmatrix}.$ I'm struggling with how to prove this in the general case, or how to express $P$ for any $n$ . Any help is appreciated!","Given a matrix , let denote the matrix obtained from by ‘rotating’ it clockwise. For example, Find (with proof) an expression for in terms of . Through analyzing , and cases, it seems that the rotation is equivalent to transposing A and then swapping the first column with the column, the second column with the column, and so on. Performing these column operations is equivalent to right multiplication by a permutation matrix. So I've concluded that is equivalent to where is the the ""mirror image"" of . For example in the case, I'm struggling with how to prove this in the general case, or how to express for any . Any help is appreciated!","A \in M^{n×n}(F) A^{\rho} A 90^{\circ} \begin{bmatrix} 1&2\\ 3&4 \end{bmatrix}^\rho =\begin{bmatrix} 3&1\\ 4&2 \end{bmatrix}. A^\rho A 2\times 2, 3\times 3, 4\times 4 5\times 5 n^{th} (n-1)^{th} A^\rho A^TP P I_n 5 \times 5 P= \begin{bmatrix}
 0&0&0&0&1\\
 0&0&0&1&0\\
 0&0&1&0&0\\
 0&1&0&0&0\\
 1&0&0&0&0
\end{bmatrix}. P n",['linear-algebra']
31,How to swap rows in square a matrix algebraically,How to swap rows in square a matrix algebraically,,"Is there some way to achieve swapping of rows of a 3x3 square matrix(for example exchanging rows 0 and 2) by using matrix algebra? Or is it something that cannot be done with algebra? What about row operations in general like multiplying a row with a constant, can it be expressed in terms of matrix algebra?","Is there some way to achieve swapping of rows of a 3x3 square matrix(for example exchanging rows 0 and 2) by using matrix algebra? Or is it something that cannot be done with algebra? What about row operations in general like multiplying a row with a constant, can it be expressed in terms of matrix algebra?",,"['linear-algebra', 'matrices']"
32,"Using basis $e=[x^3,x^2,x,1]$ instead of $e=[1,x,x^2,x^3]$",Using basis  instead of,"e=[x^3,x^2,x,1] e=[1,x,x^2,x^3]","So on an exam I've got zero points on the question (and sub-questions) to find matrix of linear operator $L:\Bbb{R}^4[x]\to \Bbb{R}^4[x]$ given by $L(p(x)) = p(x)+xp(2)$ with respect to canonical basis $e$ I've said I'm using notation $(a,b,c,d)$ to mean $(ax^3,bx^2,cx,d)$ I've found the matrix for $L$ lets say $A$ which is $$A=\begin{bmatrix}1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\8 & 4 & 3 & 1\\0 & 0 & 0 & 1\end{bmatrix}$$ Now $$\begin{bmatrix}1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\8 & 4 & 3 & 1\\0 & 0 & 0 & 1\end{bmatrix}\begin{bmatrix}a \\ b \\ c \\ d\end{bmatrix}=\begin{bmatrix}a \\ b \\ 8a+4b+3c+d \\ d\end{bmatrix}$$ Which is the right result (using my notation), however they've got a different matrix by using $(a,b,c,d) = (a,bx,cx^2,dx^3)$ They found $$A=\begin{bmatrix}1 & 0 & 0 & 0\\1 & 3 & 4 & 8\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1\end{bmatrix}$$ Which is again the right answer (using their notation), so I'm looking for references to use of the first notation or references/reasons to why my notation is wrong.","So on an exam I've got zero points on the question (and sub-questions) to find matrix of linear operator $L:\Bbb{R}^4[x]\to \Bbb{R}^4[x]$ given by $L(p(x)) = p(x)+xp(2)$ with respect to canonical basis $e$ I've said I'm using notation $(a,b,c,d)$ to mean $(ax^3,bx^2,cx,d)$ I've found the matrix for $L$ lets say $A$ which is $$A=\begin{bmatrix}1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\8 & 4 & 3 & 1\\0 & 0 & 0 & 1\end{bmatrix}$$ Now $$\begin{bmatrix}1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\8 & 4 & 3 & 1\\0 & 0 & 0 & 1\end{bmatrix}\begin{bmatrix}a \\ b \\ c \\ d\end{bmatrix}=\begin{bmatrix}a \\ b \\ 8a+4b+3c+d \\ d\end{bmatrix}$$ Which is the right result (using my notation), however they've got a different matrix by using $(a,b,c,d) = (a,bx,cx^2,dx^3)$ They found $$A=\begin{bmatrix}1 & 0 & 0 & 0\\1 & 3 & 4 & 8\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1\end{bmatrix}$$ Which is again the right answer (using their notation), so I'm looking for references to use of the first notation or references/reasons to why my notation is wrong.",,"['linear-algebra', 'matrices', 'reference-request', 'vector-spaces', 'hamel-basis']"
33,Maximum and minimum value of Determinant of $3 \times 3$ Matrix with entries $\pm 1$,Maximum and minimum value of Determinant of  Matrix with entries,3 \times 3 \pm 1,"Find Maximum value of Determinant of $3 \times 3$ Matrix  with  entries $\pm 1$ My try: I considered a matrix as : $$A=\begin{bmatrix} 1 &-1  &-1 \\  -1 &1  &-1 \\   -1&-1  &1  \end{bmatrix}$$ we have $$Det(A)=-4$$ and maximum is $4$, but how can we show that these are max and min values? I also tried as follows: By definition Determinant of a matrix is dot product of elements of any row with corresponding Cofactors hence $$Det(A)=a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13}$$ By cauchy Scwartz Inequality we have $$a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13} \le \left(\sqrt{a_{11}^2+a_{12}^2+a_{13}^2}\right)\left(\sqrt{C_{11}^2+C_{12}^2+C_{13}^2}\right)$$ any way to proceed here?","Find Maximum value of Determinant of $3 \times 3$ Matrix  with  entries $\pm 1$ My try: I considered a matrix as : $$A=\begin{bmatrix} 1 &-1  &-1 \\  -1 &1  &-1 \\   -1&-1  &1  \end{bmatrix}$$ we have $$Det(A)=-4$$ and maximum is $4$, but how can we show that these are max and min values? I also tried as follows: By definition Determinant of a matrix is dot product of elements of any row with corresponding Cofactors hence $$Det(A)=a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13}$$ By cauchy Scwartz Inequality we have $$a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13} \le \left(\sqrt{a_{11}^2+a_{12}^2+a_{13}^2}\right)\left(\sqrt{C_{11}^2+C_{12}^2+C_{13}^2}\right)$$ any way to proceed here?",,"['linear-algebra', 'matrices', 'algebra-precalculus', 'determinant']"
34,Frobenius and operator norms of rank 1 matrices,Frobenius and operator norms of rank 1 matrices,,"$\newcommand{\opnorm}[1]{\left\| #1 \right\|_{\mathrm{op}}}  \newcommand{\norm}[1]{\left\| #1 \right\|}$ Suppose we have $X = x_1 x_2^\top \in \mathbb{R}^{n \times d}$ a rank-1 matrix which is non-symmetric in general. I want to either prove that $\| X\|_F = \opnorm{X}$ or give an upper bound $\norm{X}_F \leq c \opnorm{X}$, where ideally $c$ does not depend on the dimensions of $X$. My attempt: $$ \opnorm{X} = \sup_{\norm{v} = 1} \norm{Xv}_2 = \sqrt{\sup_{\norm{v} = 1} \norm{Xv}_2^2} = \sqrt{\sup_{\norm{v} = 1} v^\top X^\top X v} = \sqrt{\lambda_{\max}(X^\top X)} $$ For the Frobenius norm, use $X$'s singular value decomposition to write $X = USV^\top$, and obtain $$ \norm{X}_F = \sqrt{\mathrm{tr}(X^\top X)} = \sqrt{\mathrm{tr}(V S U^\top U S V^\top)} = \sqrt{\mathrm{tr}( S^2)} = \sqrt{\sum_{i=1}^{\mathrm{rank}(X)} \sigma_i^2} = \sqrt{\sigma_1^2} $$ However, we know that the singular values of $A$ are the square roots of the eigenvalues of $X^\top X$, therefore $\sigma_1^2 = \lambda_{\max}(X^\top X)$. Hence $\norm{X}_F = \opnorm{X}$. Is the above proof correct?","$\newcommand{\opnorm}[1]{\left\| #1 \right\|_{\mathrm{op}}}  \newcommand{\norm}[1]{\left\| #1 \right\|}$ Suppose we have $X = x_1 x_2^\top \in \mathbb{R}^{n \times d}$ a rank-1 matrix which is non-symmetric in general. I want to either prove that $\| X\|_F = \opnorm{X}$ or give an upper bound $\norm{X}_F \leq c \opnorm{X}$, where ideally $c$ does not depend on the dimensions of $X$. My attempt: $$ \opnorm{X} = \sup_{\norm{v} = 1} \norm{Xv}_2 = \sqrt{\sup_{\norm{v} = 1} \norm{Xv}_2^2} = \sqrt{\sup_{\norm{v} = 1} v^\top X^\top X v} = \sqrt{\lambda_{\max}(X^\top X)} $$ For the Frobenius norm, use $X$'s singular value decomposition to write $X = USV^\top$, and obtain $$ \norm{X}_F = \sqrt{\mathrm{tr}(X^\top X)} = \sqrt{\mathrm{tr}(V S U^\top U S V^\top)} = \sqrt{\mathrm{tr}( S^2)} = \sqrt{\sum_{i=1}^{\mathrm{rank}(X)} \sigma_i^2} = \sqrt{\sigma_1^2} $$ However, we know that the singular values of $A$ are the square roots of the eigenvalues of $X^\top X$, therefore $\sigma_1^2 = \lambda_{\max}(X^\top X)$. Hence $\norm{X}_F = \opnorm{X}$. Is the above proof correct?",,"['linear-algebra', 'matrices', 'matrix-norms', 'singular-values', 'rank-1-matrices']"
35,Dual basis with non-degenerate bilinear form,Dual basis with non-degenerate bilinear form,,"Let $V$ be $n$-dimensional vector space and $\{x_1,\cdots,x_n\}$ a basis. Let $\beta:V\times V\rightarrow F$ be a non-degenerate (symmetric) bilinear form. This implies that there exists a dual basis $\{y_1,\cdots, y_n\}$ of $V$ w.r.t. $\beta$, i.e.  $$\beta(x_i,y_j)=\delta_{ij}.$$ Can we write expression for basis elements $y_i$'s in terms of $x_i$'s and matrix of $\beta$ w.r.t. basis $\{x_1,\cdots, x_n\}$? This is an obvious computational question and may be very trivial, but I didn't find in any book mentioning about computation of dual basis. For example, as a concrete example, let $V$ be the space of column vectors of length $n$ over field $\mathbb{R}$. Consider dot product on this column space.  Then given any invertible $n\times n$ matrix $A$ over $\mathbb{R}$, its $n$ columns define a basis of $V$; what is dual basis w.r.t. dot product? Perhaps it is columns of $(A^{-1})^t$, am I right? Then next question comes more general, which I put above.","Let $V$ be $n$-dimensional vector space and $\{x_1,\cdots,x_n\}$ a basis. Let $\beta:V\times V\rightarrow F$ be a non-degenerate (symmetric) bilinear form. This implies that there exists a dual basis $\{y_1,\cdots, y_n\}$ of $V$ w.r.t. $\beta$, i.e.  $$\beta(x_i,y_j)=\delta_{ij}.$$ Can we write expression for basis elements $y_i$'s in terms of $x_i$'s and matrix of $\beta$ w.r.t. basis $\{x_1,\cdots, x_n\}$? This is an obvious computational question and may be very trivial, but I didn't find in any book mentioning about computation of dual basis. For example, as a concrete example, let $V$ be the space of column vectors of length $n$ over field $\mathbb{R}$. Consider dot product on this column space.  Then given any invertible $n\times n$ matrix $A$ over $\mathbb{R}$, its $n$ columns define a basis of $V$; what is dual basis w.r.t. dot product? Perhaps it is columns of $(A^{-1})^t$, am I right? Then next question comes more general, which I put above.",,"['linear-algebra', 'matrices', 'bilinear-form']"
36,Calculating the Inverse of a matrix.,Calculating the Inverse of a matrix.,,"$A)$ Solve: $x_1-x_2=1$ $-x_1+2x_2-x_3=0$ $\vdots$ $-x_{99}+2x_{100}=0$ $B$)Deduce the inverse of A=\begin{pmatrix}         1 & -1 & 0 & \cdots& \cdots & 0   \\         -1 & 2 & -1 & \ddots& \cdots  &\vdots\\         0 & -1 & 2 &-1& \ddots& \vdots\\         \vdots & \ddots & \ddots & \ddots &\ddots&0\\         \vdots & \cdots & \ddots & \ddots &2&-1\\         0 & \cdots & \cdots & 0 &-1&2\\ \end{pmatrix} I've solved the first part and got: $x_1=100$ $x_2=99$ $\vdots$ $x_{100}=1$ (Correct me if I'm wrong) For the part B , I'm not sure how to approach it although I'm quite sure the first column of $A^{-1}$ should contain $x_1$ till $x_{100}$ in this order , however I have no idea on how to fill the other columns. If anyone could help me or give me hints , I would be grateful. Thanks in advance.","$A)$ Solve: $x_1-x_2=1$ $-x_1+2x_2-x_3=0$ $\vdots$ $-x_{99}+2x_{100}=0$ $B$)Deduce the inverse of A=\begin{pmatrix}         1 & -1 & 0 & \cdots& \cdots & 0   \\         -1 & 2 & -1 & \ddots& \cdots  &\vdots\\         0 & -1 & 2 &-1& \ddots& \vdots\\         \vdots & \ddots & \ddots & \ddots &\ddots&0\\         \vdots & \cdots & \ddots & \ddots &2&-1\\         0 & \cdots & \cdots & 0 &-1&2\\ \end{pmatrix} I've solved the first part and got: $x_1=100$ $x_2=99$ $\vdots$ $x_{100}=1$ (Correct me if I'm wrong) For the part B , I'm not sure how to approach it although I'm quite sure the first column of $A^{-1}$ should contain $x_1$ till $x_{100}$ in this order , however I have no idea on how to fill the other columns. If anyone could help me or give me hints , I would be grateful. Thanks in advance.",,"['linear-algebra', 'matrices', 'systems-of-equations']"
37,Find the general solution of linear system of equations (homework assignement),Find the general solution of linear system of equations (homework assignement),,"Assuming that $\lambda$ is any number, does below set of equations have a solution? If it has, find the general solution for this system. $$\lambda x_1+x_2+x_3=1$$ $$x_1+\lambda x_2+x_3=\lambda$$ $$x_1+x_2+\lambda x_3=\lambda^2$$ I understand how this problem should be solved, however when transform this set to a matrix and then row reduce it to an echolon form I get: $$B=\left(\begin{array}{ccc|c} 1 & 1 & \lambda & \lambda\\ 0 & \lambda-1 & 1-\lambda & \lambda-\lambda^2\\ 0 & 0 & (\lambda-1)(\lambda+2) & \lambda^3+\lambda^2-\lambda-1\end{array}\right)$$ But from this point I start to struggle, don't really know where I'm making mistake.","Assuming that $\lambda$ is any number, does below set of equations have a solution? If it has, find the general solution for this system. $$\lambda x_1+x_2+x_3=1$$ $$x_1+\lambda x_2+x_3=\lambda$$ $$x_1+x_2+\lambda x_3=\lambda^2$$ I understand how this problem should be solved, however when transform this set to a matrix and then row reduce it to an echolon form I get: $$B=\left(\begin{array}{ccc|c} 1 & 1 & \lambda & \lambda\\ 0 & \lambda-1 & 1-\lambda & \lambda-\lambda^2\\ 0 & 0 & (\lambda-1)(\lambda+2) & \lambda^3+\lambda^2-\lambda-1\end{array}\right)$$ But from this point I start to struggle, don't really know where I'm making mistake.",,"['linear-algebra', 'matrices', 'vectors', 'systems-of-equations']"
38,Why can any orthogonal matrix be written as $O=e^A$?,Why can any orthogonal matrix be written as ?,O=e^A,"I read in the book Quantum Field Theory in a Nutshell that any orthogonal real matrix $O$, can be written as $O=e^A$. I should clarify that  $O \in \textrm{SO}(N)$, so $O^TO=1$ and $\det (O)=1$ I wonder why this is true and if there's a simple proof?","I read in the book Quantum Field Theory in a Nutshell that any orthogonal real matrix $O$, can be written as $O=e^A$. I should clarify that  $O \in \textrm{SO}(N)$, so $O^TO=1$ and $\det (O)=1$ I wonder why this is true and if there's a simple proof?",,"['matrices', 'rotations', 'orthogonal-matrices', 'matrix-exponential']"
39,How to measure how far a matrix is from being singular?,How to measure how far a matrix is from being singular?,,What would be the best mathematical tool/concept to measure how far a matrix is from being singular? Could it be the condition number?,What would be the best mathematical tool/concept to measure how far a matrix is from being singular? Could it be the condition number?,,"['linear-algebra', 'matrices', 'condition-number']"
40,Derivative of matrix-valued function with respect to matrix input,Derivative of matrix-valued function with respect to matrix input,,"I have the expression $$\bf \phi  = \bf X W$$ where $\bf X$ is a $20 \times 10$ matrix, $\bf W$ is a $10 \times 5$ matrix. How can I calculate $\frac{d\phi}{d\bf W}$? What is the dimension of the result?","I have the expression $$\bf \phi  = \bf X W$$ where $\bf X$ is a $20 \times 10$ matrix, $\bf W$ is a $10 \times 5$ matrix. How can I calculate $\frac{d\phi}{d\bf W}$? What is the dimension of the result?",,"['matrices', 'derivatives', 'matrix-calculus']"
41,Characteristic polynomial of a $7 \times 7$ matrix whose entries are $5$ [duplicate],Characteristic polynomial of a  matrix whose entries are  [duplicate],7 \times 7 5,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 6 years ago . Avoiding too many steps, what is the characteristic polynomial of the following $7 \times 7$ matrix? And why? \begin{pmatrix}5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\end{pmatrix}","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 6 years ago . Avoiding too many steps, what is the characteristic polynomial of the following matrix? And why?",7 \times 7 \begin{pmatrix}5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\end{pmatrix},"['linear-algebra', 'matrices', 'characteristic-polynomial']"
42,Find a $3\times 3$ matrix $A\not = I_3$ such that $A^3 = I_{3}$,Find a  matrix  such that,3\times 3 A\not = I_3 A^3 = I_{3},Use the correspondence between matrices and linear transformation to find find a $3\times 3$ matrix $A$ such that $A^3 = I_{3}$ and find an $A$ matrix that is not $I_{3}$ Where $I_{3}$ is the identity matrix: $$I_{3}=   \left[ {\begin{array}{ccc}    1 & 0 & 0\\    0 & 1 & 0\\    0 & 0 & 1\\   \end{array} } \right]$$ I was tried with the following $A$ matrix: $$A=   \left[ {\begin{array}{ccc}    1 & 0 & 0\\    0 & 1 & 0\\    0 & 0 & 1\\   \end{array} } \right]$$ and when I multiply $A \times A \times A$ I got the same matrix as $I_{3}$. And to find a matrix $A$ that is not equal to $I_{3}$ I can take any $A$ matrix that is not:  $$A=   \left[ {\begin{array}{ccc}    1 & 0 & 0\\    0 & 1 & 0\\    0 & 0 & 1\\   \end{array} } \right]$$ but I think that the exercise is expecting something else using linear transformations. Sorry I realised that $A$ cannot be equal to $I_{3}$,Use the correspondence between matrices and linear transformation to find find a $3\times 3$ matrix $A$ such that $A^3 = I_{3}$ and find an $A$ matrix that is not $I_{3}$ Where $I_{3}$ is the identity matrix: $$I_{3}=   \left[ {\begin{array}{ccc}    1 & 0 & 0\\    0 & 1 & 0\\    0 & 0 & 1\\   \end{array} } \right]$$ I was tried with the following $A$ matrix: $$A=   \left[ {\begin{array}{ccc}    1 & 0 & 0\\    0 & 1 & 0\\    0 & 0 & 1\\   \end{array} } \right]$$ and when I multiply $A \times A \times A$ I got the same matrix as $I_{3}$. And to find a matrix $A$ that is not equal to $I_{3}$ I can take any $A$ matrix that is not:  $$A=   \left[ {\begin{array}{ccc}    1 & 0 & 0\\    0 & 1 & 0\\    0 & 0 & 1\\   \end{array} } \right]$$ but I think that the exercise is expecting something else using linear transformations. Sorry I realised that $A$ cannot be equal to $I_{3}$,,"['abstract-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
43,"In linear algebra, why is it that linear transformation is orthogonal if it preserves the length of vectors?","In linear algebra, why is it that linear transformation is orthogonal if it preserves the length of vectors?",,How are orthogonality and preserving length of a linear transformation even relating? A linear transformation can also be orthogonal even if it doesn't preserve the length of vector.,How are orthogonality and preserving length of a linear transformation even relating? A linear transformation can also be orthogonal even if it doesn't preserve the length of vector.,,"['linear-algebra', 'matrices', 'linear-transformations', 'orthogonality']"
44,"Find a positive definite matrix with $a_{1,1}<0$",Find a positive definite matrix with,"a_{1,1}<0","Find a matrix $A=[a_{i,j}]$ such that $A$ is positive definite and $a_{1,1}<0$. It is not easy. In my opinion, there is no such matrix if the matrix dimension is even.","Find a matrix $A=[a_{i,j}]$ such that $A$ is positive definite and $a_{1,1}<0$. It is not easy. In my opinion, there is no such matrix if the matrix dimension is even.",,"['matrices', 'examples-counterexamples', 'positive-definite']"
45,How to solve an empty sudoku?,How to solve an empty sudoku?,,I am making a computer game sudoku. I have a simple algorithm(more like a rule) : check rows and columns before placing a number. But solving like that sometimes get me stuck and I want to avoid correction algorithm unless there is no choice. here is a case : Fill any diagonal matrices with random numbers 0-9 : Now pick one matrix and by the rule fill the numbers randomly : But this step has a problem as shown in picture. How to overcome that problem ? P.S : I need a solved sudoku in order to turn it into a question.,I am making a computer game sudoku. I have a simple algorithm(more like a rule) : check rows and columns before placing a number. But solving like that sometimes get me stuck and I want to avoid correction algorithm unless there is no choice. here is a case : Fill any diagonal matrices with random numbers 0-9 : Now pick one matrix and by the rule fill the numbers randomly : But this step has a problem as shown in picture. How to overcome that problem ? P.S : I need a solved sudoku in order to turn it into a question.,,"['matrices', 'algorithms', 'sudoku']"
46,How do row operations affect $\det(U)$?,How do row operations affect ?,\det(U),"'We can do row operations without changing $\det(A)$' - A quote from Introduction to Linear Algebra by G. Strang But let's say I have an arbitrary upper triangular matrix $U$ $$U = \begin{bmatrix} a & a & a \\ 0 & b & b \\ 0 & 0 & c \\ \end{bmatrix}$$ And I perform the following row operations on $U$ to bring it to $U'$ $\frac{1}{a}R_1 \rightarrow R_1$ $\frac{1}{b}R_2 \rightarrow R_2$ $\frac{1}{c}R_3 \rightarrow R_3$ Then $U'$ is: $$U' = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \\ \end{bmatrix}$$ But now $\det(U) = abc$ and $\det(U') = 1$, thus $$\det(U) \neq \det(U')$$ All I've done is perform row operations on $U$ to bring it to $U'$, but by performing those row operations, their determinants lose equality. How can that be possible? So how is this seeming contradiction is resolved. I'm assuming that I must have some misconception either on row operations or on determinants. Furthermore on a deeper level, what geometric interpretation/meaning does scaling the rows as I've done bringing $U$ to $U'$, have on the determinant? Since the determinants of $U$ and $U'$ are obviously no longer equal, geometrically what is this scaling doing to the determinant?","'We can do row operations without changing $\det(A)$' - A quote from Introduction to Linear Algebra by G. Strang But let's say I have an arbitrary upper triangular matrix $U$ $$U = \begin{bmatrix} a & a & a \\ 0 & b & b \\ 0 & 0 & c \\ \end{bmatrix}$$ And I perform the following row operations on $U$ to bring it to $U'$ $\frac{1}{a}R_1 \rightarrow R_1$ $\frac{1}{b}R_2 \rightarrow R_2$ $\frac{1}{c}R_3 \rightarrow R_3$ Then $U'$ is: $$U' = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \\ \end{bmatrix}$$ But now $\det(U) = abc$ and $\det(U') = 1$, thus $$\det(U) \neq \det(U')$$ All I've done is perform row operations on $U$ to bring it to $U'$, but by performing those row operations, their determinants lose equality. How can that be possible? So how is this seeming contradiction is resolved. I'm assuming that I must have some misconception either on row operations or on determinants. Furthermore on a deeper level, what geometric interpretation/meaning does scaling the rows as I've done bringing $U$ to $U'$, have on the determinant? Since the determinants of $U$ and $U'$ are obviously no longer equal, geometrically what is this scaling doing to the determinant?",,"['linear-algebra', 'matrices', 'determinant']"
47,Let $A$ be an $n\times n$ matrices over $\mathbb{C}$ then which of the following are true?,Let  be an  matrices over  then which of the following are true?,A n\times n \mathbb{C},"Let $A$ be an $n\times n$ matrix over $\mathbb{C}$ such that every non zero vector of $\mathbb{C^n}$ is an eigenvector of $A$ . Then, All eigen values of $A$ are equal. All eigen values of $A$ are distinct. $A=\lambda I$ for $\lambda \in \mathbb{C}$ where $I$ is an $n\times n$ identity matrix. If $\chi_A$ and $m_A$ denote the characteristic polynomial and minimal polynomial respectively, then $\chi_A=m_A$ . Now for $4$ , I know that $\chi_A=m_A$ implies the eigen space has dimension $1$ , that is all the eigen-values are different. But how can I conclude if every non-zero vectors are eigen-vectors or not if this happens. Again if $A=\lambda I$ then all the eigen values are same and every non-zero vector is an eigen vector. But I am confused to analyze it properly. Any help please.","Let be an matrix over such that every non zero vector of is an eigenvector of . Then, All eigen values of are equal. All eigen values of are distinct. for where is an identity matrix. If and denote the characteristic polynomial and minimal polynomial respectively, then . Now for , I know that implies the eigen space has dimension , that is all the eigen-values are different. But how can I conclude if every non-zero vectors are eigen-vectors or not if this happens. Again if then all the eigen values are same and every non-zero vector is an eigen vector. But I am confused to analyze it properly. Any help please.",A n\times n \mathbb{C} \mathbb{C^n} A A A A=\lambda I \lambda \in \mathbb{C} I n\times n \chi_A m_A \chi_A=m_A 4 \chi_A=m_A 1 A=\lambda I,"['matrices', 'eigenvalues-eigenvectors']"
48,Identity with inverse matrix multiplication $(A_1+A_2)^{-1}$,Identity with inverse matrix multiplication,(A_1+A_2)^{-1},"If we have that $A_1$ and $A_2$ are matrices, how can I prove that $$I-A_1(A_1+A_2)^{-1} = A_2(A_1+A_2)^{-1}$$ if $(A_1+A_2)^{-1}$ exists? The case where we have numbers makes intuitive sense but here I am having trouble explicitly coming up with a reason why. Any help would be appreciated, thanks!","If we have that $A_1$ and $A_2$ are matrices, how can I prove that $$I-A_1(A_1+A_2)^{-1} = A_2(A_1+A_2)^{-1}$$ if $(A_1+A_2)^{-1}$ exists? The case where we have numbers makes intuitive sense but here I am having trouble explicitly coming up with a reason why. Any help would be appreciated, thanks!",,"['linear-algebra', 'matrices']"
49,Degree of Characteristic Polynomial of a Square Matrix,Degree of Characteristic Polynomial of a Square Matrix,,"Let $A$ be a Square Matrix where $A \in M^C_{n \times n}$ and $P(x)$ be its characteristic polynomial. $A$ is also nilpotent. Is it true if $A$ is a matrix $A_{n, n} $ in the complex matrices field, which is nilpotent, then its characteristic polynomial has a degree $n^2$ ? If it's not true, when does it happen? What are the conditions  to such case? Thanks, Alan","Let be a Square Matrix where and be its characteristic polynomial. is also nilpotent. Is it true if is a matrix in the complex matrices field, which is nilpotent, then its characteristic polynomial has a degree ? If it's not true, when does it happen? What are the conditions  to such case? Thanks, Alan","A A \in M^C_{n \times n} P(x) A A A_{n, n}  n^2","['linear-algebra', 'matrices', 'polynomials', 'characteristic-polynomial']"
50,Determinant of $ n \times n$ matrix and its characteristic polynomial.,Determinant of  matrix and its characteristic polynomial., n \times n,"Suppose, $M_4, M_5,..M_n$ is as follows then determinant and characteristic polynomial of $M_n$. $M_4=\left( \begin{array}{cccc} 	0 & 0 & 1 & 0 \\ 	0 & 0 & 0 & 1 \\ 	1 & 0 & 0 & 0 \\ 	0 & 1 & 0 & 0 \\ \end{array} \right),M_5=\left( \begin{array}{ccccc} 	0 & 0 & 1 & 1 & 0 \\ 	0 & 0 & 0 & 1 & 1 \\ 	1 & 0 & 0 & 0 & 1 \\ 	1 & 1 & 0 & 0 & 0 \\ 	0 & 1 & 1 & 0 & 0 \\ \end{array} \right),M_6=\left( \begin{array}{cccccc} 	0 & 0 & 1 & 1 & 1 & 0 \\ 	0 & 0 & 0 & 1 & 1 & 1 \\ 	1 & 0 & 0 & 0 & 1 & 1 \\ 	1 & 1 & 0 & 0 & 0 & 1 \\ 	1 & 1 & 1 & 0 & 0 & 0 \\ 	0 & 1 & 1 & 1 & 0 & 0 \\ \end{array} \right), \quad M_8=\left( \begin{array}{cccccccc} 	0 & 0 & 1 & 1 & 1 & 1 & 1 & 0 \\ 	0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \\ 	1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\ 	1 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\ 	1 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\ 	1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 \\ 	1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\ 	0 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\ \end{array} \right)$","Suppose, $M_4, M_5,..M_n$ is as follows then determinant and characteristic polynomial of $M_n$. $M_4=\left( \begin{array}{cccc} 	0 & 0 & 1 & 0 \\ 	0 & 0 & 0 & 1 \\ 	1 & 0 & 0 & 0 \\ 	0 & 1 & 0 & 0 \\ \end{array} \right),M_5=\left( \begin{array}{ccccc} 	0 & 0 & 1 & 1 & 0 \\ 	0 & 0 & 0 & 1 & 1 \\ 	1 & 0 & 0 & 0 & 1 \\ 	1 & 1 & 0 & 0 & 0 \\ 	0 & 1 & 1 & 0 & 0 \\ \end{array} \right),M_6=\left( \begin{array}{cccccc} 	0 & 0 & 1 & 1 & 1 & 0 \\ 	0 & 0 & 0 & 1 & 1 & 1 \\ 	1 & 0 & 0 & 0 & 1 & 1 \\ 	1 & 1 & 0 & 0 & 0 & 1 \\ 	1 & 1 & 1 & 0 & 0 & 0 \\ 	0 & 1 & 1 & 1 & 0 & 0 \\ \end{array} \right), \quad M_8=\left( \begin{array}{cccccccc} 	0 & 0 & 1 & 1 & 1 & 1 & 1 & 0 \\ 	0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \\ 	1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\ 	1 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\ 	1 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\ 	1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 \\ 	1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\ 	0 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\ \end{array} \right)$",,"['linear-algebra', 'matrices', 'determinant', 'spectral-graph-theory']"
51,Does the lowest diagonal element of a real symmetric matrix form an upper bound to the lowest eigenvalue?,Does the lowest diagonal element of a real symmetric matrix form an upper bound to the lowest eigenvalue?,,"If I have a real symmetric matrix, is it possible to look at the lowest diagonal element and then claim that the lowest eigenvalue of the matrix must be less than or equal to that diagonal element? I think this may be the case, so if it is, is this a well known theorem? Gerschgorin's disk theorem doesn't appear useful to me, as the disk radius extends both ways in both the positive and negative direction from the diagonal. FWIW, in quantum chemistry we have the Hyleraas-Undheim-MacDonald theorem, which bounds eigenvalues this way, but I'm not sure if this applies to any real symmetric matrix.","If I have a real symmetric matrix, is it possible to look at the lowest diagonal element and then claim that the lowest eigenvalue of the matrix must be less than or equal to that diagonal element? I think this may be the case, so if it is, is this a well known theorem? Gerschgorin's disk theorem doesn't appear useful to me, as the disk radius extends both ways in both the positive and negative direction from the diagonal. FWIW, in quantum chemistry we have the Hyleraas-Undheim-MacDonald theorem, which bounds eigenvalues this way, but I'm not sure if this applies to any real symmetric matrix.",,"['matrices', 'eigenvalues-eigenvectors']"
52,Is this operator $A = \pmatrix{1&1\\0&1}$ self-adjoint?,Is this operator  self-adjoint?,A = \pmatrix{1&1\\0&1},"Is this operator $$A = \pmatrix{1&1\\0&1}$$ self-adjoint? I think not, because $$\pmatrix{1&1\\0&1}^T\neq A$$ where $T$ is the transposition of the matrix. What do you all think?","Is this operator $$A = \pmatrix{1&1\\0&1}$$ self-adjoint? I think not, because $$\pmatrix{1&1\\0&1}^T\neq A$$ where $T$ is the transposition of the matrix. What do you all think?",,"['matrices', 'operator-theory', 'adjoint-operators']"
53,Rotate a unit sphere such as to align it two orthogonal unit vectors,Rotate a unit sphere such as to align it two orthogonal unit vectors,,"I have two orthogonal vectors $a$, $b$, which lie on a unit sphere (i.e. unit vectors). I want to apply one or more rotations to the sphere such that $a$ is transformed to $c$, and $b$ is transformed to $d$, where $c$ and $d$ are two other orthogonal unit vectors. It feels a very similar problem to this question: , but I'm not quite seeing how to obtain a rotation matrix for the sphere from the answer given there (which I'm sure is due to my limited mathematical abilities!).","I have two orthogonal vectors $a$, $b$, which lie on a unit sphere (i.e. unit vectors). I want to apply one or more rotations to the sphere such that $a$ is transformed to $c$, and $b$ is transformed to $d$, where $c$ and $d$ are two other orthogonal unit vectors. It feels a very similar problem to this question: , but I'm not quite seeing how to obtain a rotation matrix for the sphere from the answer given there (which I'm sure is due to my limited mathematical abilities!).",,"['matrices', 'vectors', '3d', 'spherical-coordinates']"
54,"Do $J$, the all-ones matrix of even order, always have eigenvectors consisting of entries $-1, 1$ only?","Do , the all-ones matrix of even order, always have eigenvectors consisting of entries  only?","J -1, 1","Do $J$ , the all-ones matrix of even order, always have eigenvectors consisting of entries $-1, 1$ only? It seems so, vector having all its entries $1$ is one eigenvector for larest eigenvalue $n$ and any vector having half its entries as $1$ and half as $-1$ is eigenvector for ev $0$ . For $n=4k$ we have Hadamard matrices there, so those vector will work. But I don't know how to show existence of such $n$ linearly independent vectors.","Do , the all-ones matrix of even order, always have eigenvectors consisting of entries only? It seems so, vector having all its entries is one eigenvector for larest eigenvalue and any vector having half its entries as and half as is eigenvector for ev . For we have Hadamard matrices there, so those vector will work. But I don't know how to show existence of such linearly independent vectors.","J -1, 1 1 n 1 -1 0 n=4k n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
55,Under what conditions does $(I-N)^{-1}$ exist?,Under what conditions does  exist?,(I-N)^{-1},"Given an nxn matrix N and $I=I_n$, under what conditions does $(I-N)^{-1}$ exist? On one hand $(I-N)(I + N + N^2 + ...) = (I + N + N^2 + ...) - (N + N^2 + ...) = I?$ On the other hand, $(I-N)(I + N + N^2 + ...) = \lim_{m \to \infty} (I-N) \sum_{i=0}^{m} N^i = \lim_{m \to \infty} \sum_{i=0}^{m} (N^i - N^{i+1}) = \lim_{m \to \infty} (I - N^{m+1}) = I - \lim_{m \to \infty} (N^{m+1}) = I?$ I think $I - \lim_{m \to \infty} (N^m) = I$ if (and only if?) the entries of N are between -1 and 1? Iirc, one of my professors stated it without specifying why it is true in that case if it was true in that case but not true in other cases or why it is true in all cases. This arose in the context of Markov chains which iirc deal with matrices whose entries are between -1 and 1. So, does $(I-N)^{-1}$ always exist? If so, why? If not, under what conditions?","Given an nxn matrix N and $I=I_n$, under what conditions does $(I-N)^{-1}$ exist? On one hand $(I-N)(I + N + N^2 + ...) = (I + N + N^2 + ...) - (N + N^2 + ...) = I?$ On the other hand, $(I-N)(I + N + N^2 + ...) = \lim_{m \to \infty} (I-N) \sum_{i=0}^{m} N^i = \lim_{m \to \infty} \sum_{i=0}^{m} (N^i - N^{i+1}) = \lim_{m \to \infty} (I - N^{m+1}) = I - \lim_{m \to \infty} (N^{m+1}) = I?$ I think $I - \lim_{m \to \infty} (N^m) = I$ if (and only if?) the entries of N are between -1 and 1? Iirc, one of my professors stated it without specifying why it is true in that case if it was true in that case but not true in other cases or why it is true in all cases. This arose in the context of Markov chains which iirc deal with matrices whose entries are between -1 and 1. So, does $(I-N)^{-1}$ always exist? If so, why? If not, under what conditions?",,"['linear-algebra', 'matrices', 'markov-chains', 'markov-process', 'operations-research']"
56,Priority vector and eigenvectors - AHP method,Priority vector and eigenvectors - AHP method,,"I'm reading about the AHP method (Analytic Hierarchy Process). On page 2 of this document , it says: Given the priorities of the alternatives and given the matrix of   preferences for each alternative over every other alternative, what   meaning do we attach to the vector obtained by weighting the   preferences by the corresponding priorities of the alternatives and   adding? It is another priority vector for the alternatives. We can use   it again to derive another priority vector ad infinitum. Even then   what is the limit priority and what is the real priority vector to be   associated with the alternatives? It all comes down to this: What   condition must a priority vector satisfy to remain invariant under the   hierarchic composition principle? A priority vector must reproduce   itself on a ratio scale because it is ratios that preserve the   strength of preferences. Thus a necessary condition that the priority   vector should satisfy is not only that it should belong to a ratio   scale, which means that it should remain invariant under   multiplication by a positive constant c, but also that it should be   invariant under hierarchic composition for its own judgment matrix so   that one does not keep getting new priority vectors from that matrix.   In sum, a priority vector x must satisfy the relation Ax = cx, c > 0 To let you quickly grasp what AHP is all about, check this simple tutorial . The matrix of preferences for each alternative over every other alternative is obvious to me. Ideally, such a matrix should satisfy a property $a_{ij}=a_{ik}a_{kj}$ (because if I say I prefer A to B two times, and B to C three times, then I should prefer A to C six times (it makes sense I guess, but it's a very informal rule). OK, but in the quote I gave, it says: what meaning do we attach to the vector obtained by weighting the   preferences by the corresponding priorities of the alternatives and   adding? It is another priority vector for the alternatives. I'm not quite sure what it means. Alternatives can be apple, banana, cherry. Preferences are just numbers in matrix of preferences, just like here But what are 'corresponding priorities of the alternatives'? I'd say to obtain a priority vector (i.e. to find out which fruit is preferred the most) one could just 1) divide every element in a given column by the sum of elements in that column (normalization) 2) calculate average of elements in each row of the matrix obtained in step 1). The obtained vector is the priority vector, I belive. But in the quoted text, it gets worse - the author describes raising the matrix to consecutive powers. Why do we multiply priority matrix by itself? It says the result of this multiplication is 'another priority vector of alternatives'. Why? Haven't we just lost some information by doing this? I mean, we always can multiply matrices, but it should be justified. In case of priority matrix I can't see the justification. Later in the document I've quoted, the author uses the Perron-Frobenius theorem and other sophisticated methods. I'd be grateful for a intuitive, clear explanation of what's going on here. And finally: WHY the eigenvector $w$ matching the maximum eigenvalue $\lambda_{max}$ of the pairwise comparison matrix $A$ is the final expression of the preferences between the investigated elements? More articles on AHP method that might help you with answering my questions: http://books.google.com/books?id=wct10TlbbIUC&printsec=frontcover&hl=eng&redir_esc=y#v=onepage&q&f=false http://www.booksites.net/download/coyle/student_files/AHP_Technique.pdf http://www.isahp.org/2001Proceedings/Papers/065-P.pdf For example, what's the relationship between Perron-Frobenius theorem and this method?","I'm reading about the AHP method (Analytic Hierarchy Process). On page 2 of this document , it says: Given the priorities of the alternatives and given the matrix of   preferences for each alternative over every other alternative, what   meaning do we attach to the vector obtained by weighting the   preferences by the corresponding priorities of the alternatives and   adding? It is another priority vector for the alternatives. We can use   it again to derive another priority vector ad infinitum. Even then   what is the limit priority and what is the real priority vector to be   associated with the alternatives? It all comes down to this: What   condition must a priority vector satisfy to remain invariant under the   hierarchic composition principle? A priority vector must reproduce   itself on a ratio scale because it is ratios that preserve the   strength of preferences. Thus a necessary condition that the priority   vector should satisfy is not only that it should belong to a ratio   scale, which means that it should remain invariant under   multiplication by a positive constant c, but also that it should be   invariant under hierarchic composition for its own judgment matrix so   that one does not keep getting new priority vectors from that matrix.   In sum, a priority vector x must satisfy the relation Ax = cx, c > 0 To let you quickly grasp what AHP is all about, check this simple tutorial . The matrix of preferences for each alternative over every other alternative is obvious to me. Ideally, such a matrix should satisfy a property $a_{ij}=a_{ik}a_{kj}$ (because if I say I prefer A to B two times, and B to C three times, then I should prefer A to C six times (it makes sense I guess, but it's a very informal rule). OK, but in the quote I gave, it says: what meaning do we attach to the vector obtained by weighting the   preferences by the corresponding priorities of the alternatives and   adding? It is another priority vector for the alternatives. I'm not quite sure what it means. Alternatives can be apple, banana, cherry. Preferences are just numbers in matrix of preferences, just like here But what are 'corresponding priorities of the alternatives'? I'd say to obtain a priority vector (i.e. to find out which fruit is preferred the most) one could just 1) divide every element in a given column by the sum of elements in that column (normalization) 2) calculate average of elements in each row of the matrix obtained in step 1). The obtained vector is the priority vector, I belive. But in the quoted text, it gets worse - the author describes raising the matrix to consecutive powers. Why do we multiply priority matrix by itself? It says the result of this multiplication is 'another priority vector of alternatives'. Why? Haven't we just lost some information by doing this? I mean, we always can multiply matrices, but it should be justified. In case of priority matrix I can't see the justification. Later in the document I've quoted, the author uses the Perron-Frobenius theorem and other sophisticated methods. I'd be grateful for a intuitive, clear explanation of what's going on here. And finally: WHY the eigenvector $w$ matching the maximum eigenvalue $\lambda_{max}$ of the pairwise comparison matrix $A$ is the final expression of the preferences between the investigated elements? More articles on AHP method that might help you with answering my questions: http://books.google.com/books?id=wct10TlbbIUC&printsec=frontcover&hl=eng&redir_esc=y#v=onepage&q&f=false http://www.booksites.net/download/coyle/student_files/AHP_Technique.pdf http://www.isahp.org/2001Proceedings/Papers/065-P.pdf For example, what's the relationship between Perron-Frobenius theorem and this method?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
57,Prove that an upper triangular matrix is invertible if and only if every diagonal entry is non-zero.,Prove that an upper triangular matrix is invertible if and only if every diagonal entry is non-zero.,,"Prove that an upper triangular matrix is invertible if and only if every diagonal entry is non-zero. I have proved that if every diagonal entry is non-zero, then the matrix is invertible by showing we can row reduce the matrix to an identity matrix. But how do I prove the only if part?","Prove that an upper triangular matrix is invertible if and only if every diagonal entry is non-zero. I have proved that if every diagonal entry is non-zero, then the matrix is invertible by showing we can row reduce the matrix to an identity matrix. But how do I prove the only if part?",,"['linear-algebra', 'matrices']"
58,Find the matrix given the determinant,Find the matrix given the determinant,,"Is there a general method to find a 3x3, or 2x2 matrices, given the determinant? I want to do a project with my students when we start to study Systems of Equations. It would be interesting if the determinant is a prime number and then work backwards. For example, I got this by playing around: if I let $$A = \begin{bmatrix}        10 & -17\\        31 & 2      \end{bmatrix}$$ then the $\det(A)=547$, a prime number. Not sure if this would be a good mini project or not. Thanks for any help.","Is there a general method to find a 3x3, or 2x2 matrices, given the determinant? I want to do a project with my students when we start to study Systems of Equations. It would be interesting if the determinant is a prime number and then work backwards. For example, I got this by playing around: if I let $$A = \begin{bmatrix}        10 & -17\\        31 & 2      \end{bmatrix}$$ then the $\det(A)=547$, a prime number. Not sure if this would be a good mini project or not. Thanks for any help.",,"['matrices', 'determinant']"
59,"Given $A$ is $6×6 $ real symetric matrix of rank $5$ , then to determine rank of $A^{2}+ A+I $","Given  is  real symetric matrix of rank  , then to determine rank of",A 6×6  5 A^{2}+ A+I ,"Given $A$ is $6×6 $matrix of rank $5$ , then to determine rank of $A^{2}+ A+I $. I knowthat rank of matrix doesnot change when we square it , but how to proceed in this question.Any hints ? Thanks","Given $A$ is $6×6 $matrix of rank $5$ , then to determine rank of $A^{2}+ A+I $. I knowthat rank of matrix doesnot change when we square it , but how to proceed in this question.Any hints ? Thanks",,"['linear-algebra', 'matrices']"
60,Rank of a matrix $A^2$ without calculating the square,Rank of a matrix  without calculating the square,A^2,"I have a matrix $A=\begin{bmatrix} 2 & 0 & 4\\  1 & -1 & 3\\  2 & 1 & 3 \end{bmatrix} $ with rank 2. How do I prove that the matrix $A^2$ has also rank 2 without actually calculating $A^2$. I know that $rank(AB)\leq min(rank(A), rank(B))$, and so $rank(A^2) \leq 2$, but I still don't have enough information. Thanks!","I have a matrix $A=\begin{bmatrix} 2 & 0 & 4\\  1 & -1 & 3\\  2 & 1 & 3 \end{bmatrix} $ with rank 2. How do I prove that the matrix $A^2$ has also rank 2 without actually calculating $A^2$. I know that $rank(AB)\leq min(rank(A), rank(B))$, and so $rank(A^2) \leq 2$, but I still don't have enough information. Thanks!",,"['linear-algebra', 'matrices']"
61,Linearly dependent eigenvectors of a matrix,Linearly dependent eigenvectors of a matrix,,"I read a theorem that says squared matrix $A_{n\times n}$ is diagonalizable iff there is a set of $n$ linearly independent vectors ,each of which is an eigen-vector of $A_{n\times n}$ . I understand from the theorem that if $A_{n\times n}$ has at least two dependent eigen-vectors then $A_{n\times n}$ is not diagonalizable. Is it true?and according to this I want to understand when a matrix has linearly dependent eigenvectors ? (eigenvectors is a set of vectors) In other words if there is'nt any set of linearly independent set of eigenvectors for a matrix, then does that mean that the matrix is not diagonalizable?","I read a theorem that says squared matrix $A_{n\times n}$ is diagonalizable iff there is a set of $n$ linearly independent vectors ,each of which is an eigen-vector of $A_{n\times n}$ . I understand from the theorem that if $A_{n\times n}$ has at least two dependent eigen-vectors then $A_{n\times n}$ is not diagonalizable. Is it true?and according to this I want to understand when a matrix has linearly dependent eigenvectors ? (eigenvectors is a set of vectors) In other words if there is'nt any set of linearly independent set of eigenvectors for a matrix, then does that mean that the matrix is not diagonalizable?",,"['linear-algebra', 'matrices']"
62,"$f(AB)=f(A)f(B)$, show that $f$ is or injective or zero",", show that  is or injective or zero",f(AB)=f(A)f(B) f,"Let $f\in\mathcal{L}(\mathcal{M}_n(\mathbb{R}))$ such that: $\forall(A,B)\in\mathcal{M}_n(\mathbb{R}),f(AB)=f(A)f(B)$ How can I show that $f$ is or injective or the null function ? What I have tried so far haven't worked at all. Notations : $\mathcal{L}(E)$ : set of the endomorphisms $E\rightarrow E$","Let $f\in\mathcal{L}(\mathcal{M}_n(\mathbb{R}))$ such that: $\forall(A,B)\in\mathcal{M}_n(\mathbb{R}),f(AB)=f(A)f(B)$ How can I show that $f$ is or injective or the null function ? What I have tried so far haven't worked at all. Notations : $\mathcal{L}(E)$ : set of the endomorphisms $E\rightarrow E$",,"['linear-algebra', 'matrices', 'functions']"
63,Inverse of a sum of symmetric matrices,Inverse of a sum of symmetric matrices,,"How can I proof this result? $$ X(X+Y)^{-1}Y=(X^{-1} +Y^{-1})^{-1} $$ where $X$, $ Y$, $(X+Y)$, and $(X^{-1} +Y^{-1})$  are symmetric and invertible matrices,each one with dimensions $p\times p$.","How can I proof this result? $$ X(X+Y)^{-1}Y=(X^{-1} +Y^{-1})^{-1} $$ where $X$, $ Y$, $(X+Y)$, and $(X^{-1} +Y^{-1})$  are symmetric and invertible matrices,each one with dimensions $p\times p$.",,['matrices']
64,Proof $p(A)=0$ without Cayley-Hamilton theorem when $A$ is upper triangular,Proof  without Cayley-Hamilton theorem when  is upper triangular,p(A)=0 A,"I need help proving $p(A)=0$ without Cayley-Hamilton theorem when $A$ is upper triangular, as part of the proof of the Cayley-Hamilton theorem The result makes a lot of sense but I can't prove it properly If $A \in \Bbb C^{n \times n }$ is upper triangular then its characteristic polynomial is $p= (x-c_1)^{r _1}...(x-c_k)^{r_k}$ then $p(A)$ will be the product of $k$ upper triangular matrices with interspersed zeros on the diagonal...","I need help proving $p(A)=0$ without Cayley-Hamilton theorem when $A$ is upper triangular, as part of the proof of the Cayley-Hamilton theorem The result makes a lot of sense but I can't prove it properly If $A \in \Bbb C^{n \times n }$ is upper triangular then its characteristic polynomial is $p= (x-c_1)^{r _1}...(x-c_k)^{r_k}$ then $p(A)$ will be the product of $k$ upper triangular matrices with interspersed zeros on the diagonal...",,"['linear-algebra', 'matrices']"
65,Eigenvalues and Eigenspace Question,Eigenvalues and Eigenspace Question,,"Thank you ahead of time for the help, I am having a problem with part $4$. I understand parts $1$ and $2$ and $3$ and have solved them but I cant seem to understand $4$. If someone could help me out, that would be amazing. Let $A = \begin{pmatrix}0 & -4 & -6\\-1 & 0 & -3\\1 & 2 & 5\end{pmatrix}$ Find the characteristic polynomial and the eigenvalues of $A$. Find a basis for each eigenspace. Is $A$ diagonalizable? If yes, diagonalize $A$. Find $A^{10}$","Thank you ahead of time for the help, I am having a problem with part $4$. I understand parts $1$ and $2$ and $3$ and have solved them but I cant seem to understand $4$. If someone could help me out, that would be amazing. Let $A = \begin{pmatrix}0 & -4 & -6\\-1 & 0 & -3\\1 & 2 & 5\end{pmatrix}$ Find the characteristic polynomial and the eigenvalues of $A$. Find a basis for each eigenspace. Is $A$ diagonalizable? If yes, diagonalize $A$. Find $A^{10}$",,"['linear-algebra', 'matrices']"
66,Finding determinant of 4*4 Matrix via LU Decomposition?,Finding determinant of 4*4 Matrix via LU Decomposition?,,"What is the shortcut way of finding the determinant of a 4 by 4 matrix (and I assume this applies to any n by n square matrix greater than 2) once you have found an LU or PLU decomposition? Given that det(A)=det(P)det(L)det(U) or det(A)=det(L)det(U) (if no permutation matrix was necessary). Here is a PLU decomposition example: http://www.wolframalpha.com/input/?i=LU+decomposition+of+%7B%7B4%2C3%2C2%2C1%7D%2C%7B1%2C10%2C3%2C4%7D%2C%7B5%2C3%2C2%2C-4%7D%2C%7B4%2C8%2C7%2C9%7D%7D The determinant of that matrix is 602. If I recall, one is supposed to multiply the diagonal entries of the Upper Matrix starting from row 1, column 1 (which gets me -602) and then...    magic? What happens from there?","What is the shortcut way of finding the determinant of a 4 by 4 matrix (and I assume this applies to any n by n square matrix greater than 2) once you have found an LU or PLU decomposition? Given that det(A)=det(P)det(L)det(U) or det(A)=det(L)det(U) (if no permutation matrix was necessary). Here is a PLU decomposition example: http://www.wolframalpha.com/input/?i=LU+decomposition+of+%7B%7B4%2C3%2C2%2C1%7D%2C%7B1%2C10%2C3%2C4%7D%2C%7B5%2C3%2C2%2C-4%7D%2C%7B4%2C8%2C7%2C9%7D%7D The determinant of that matrix is 602. If I recall, one is supposed to multiply the diagonal entries of the Upper Matrix starting from row 1, column 1 (which gets me -602) and then...    magic? What happens from there?",,"['calculus', 'matrices']"
67,Cramer's Rule Question,Cramer's Rule Question,,"Use Cramer's rule to solve this system for z: $$2x+y+z=1$$ $$3x+z=4$$ $$x-y-z=2$$ so my work is: $$\frac{\left|\begin{matrix} 2 & 1 & 1\\ 3 & 0 & 4\\ 1 & -1 & 2 \end{matrix}\right|}{\left|\begin{matrix} 2 & 1 & 1\\ 3 & 0 & 1\\ 1 & -1 & 1 \end{matrix}\right|}$$ which gives $\frac{3}{-3}$ or $-1$. The solution is $1$, can someone tell me what I am doing wrong?","Use Cramer's rule to solve this system for z: $$2x+y+z=1$$ $$3x+z=4$$ $$x-y-z=2$$ so my work is: $$\frac{\left|\begin{matrix} 2 & 1 & 1\\ 3 & 0 & 4\\ 1 & -1 & 2 \end{matrix}\right|}{\left|\begin{matrix} 2 & 1 & 1\\ 3 & 0 & 1\\ 1 & -1 & 1 \end{matrix}\right|}$$ which gives $\frac{3}{-3}$ or $-1$. The solution is $1$, can someone tell me what I am doing wrong?",,"['linear-algebra', 'matrices', 'determinant']"
68,Product of symmetric positive definite matrices,Product of symmetric positive definite matrices,,"I was asked the following question: Let $A,B \in \mbox{Mat}_n(\mathbb R)$ be symmetric positive definite matrices. Show that $AB$ does not have negative eigenvalues. My answer I'm getting something very weird. According to my answer, $AB$ has to be positive definite. which is weird because we don't know that $AB$ is symmetric. $B$ is symmetric so there is a basis $v_1,...,v_n$ of eigenvectors such that $Bv_i=\lambda_i$ and $\lambda_i>0$ for all $i$, and $<v_i,v_j>=0$ when $i\neq j$ let's look at $<ABx,x>$ where $x$ is some vector in $\mathbb R^n$ $<ABx,x>=<AB\sum_{i=1}^n \alpha_i v_i,\sum_{i=1}^n \alpha_i v_i> = <AB\alpha_1 v_1,\alpha_1 v_1>+<AB\alpha_2 v_2,\alpha_2 v_2>+...+<AB\alpha_n v_n,\alpha_n v_n> = \alpha_1 ^2 \lambda_1<Av_1,v_1>+\alpha_2 ^2\lambda_2<Av_2,v_2>+...+\alpha_n ^2 \lambda_n<Av_n,v_n>$ obviously for all $i$ $\alpha_i ^2$ is positive. Because $B$ is positive definite, for all $i : \lambda_i >0$. Because $A$ is positive definite, for all $i: <Av_i,v_i>$ is positive. so overall, for all $i: \alpha_i ^2 \lambda_i <Av_i,v_i>$ is positive. So we got that $<ABx,x>$ is larger than zero for any vector $x$, and so I infer that $AB$ is positive definite. And so it doesn't have negative eigenvalues. But how can we say that it's positive definite if it's not symmetric? I did something wrong I'm sure.","I was asked the following question: Let $A,B \in \mbox{Mat}_n(\mathbb R)$ be symmetric positive definite matrices. Show that $AB$ does not have negative eigenvalues. My answer I'm getting something very weird. According to my answer, $AB$ has to be positive definite. which is weird because we don't know that $AB$ is symmetric. $B$ is symmetric so there is a basis $v_1,...,v_n$ of eigenvectors such that $Bv_i=\lambda_i$ and $\lambda_i>0$ for all $i$, and $<v_i,v_j>=0$ when $i\neq j$ let's look at $<ABx,x>$ where $x$ is some vector in $\mathbb R^n$ $<ABx,x>=<AB\sum_{i=1}^n \alpha_i v_i,\sum_{i=1}^n \alpha_i v_i> = <AB\alpha_1 v_1,\alpha_1 v_1>+<AB\alpha_2 v_2,\alpha_2 v_2>+...+<AB\alpha_n v_n,\alpha_n v_n> = \alpha_1 ^2 \lambda_1<Av_1,v_1>+\alpha_2 ^2\lambda_2<Av_2,v_2>+...+\alpha_n ^2 \lambda_n<Av_n,v_n>$ obviously for all $i$ $\alpha_i ^2$ is positive. Because $B$ is positive definite, for all $i : \lambda_i >0$. Because $A$ is positive definite, for all $i: <Av_i,v_i>$ is positive. so overall, for all $i: \alpha_i ^2 \lambda_i <Av_i,v_i>$ is positive. So we got that $<ABx,x>$ is larger than zero for any vector $x$, and so I infer that $AB$ is positive definite. And so it doesn't have negative eigenvalues. But how can we say that it's positive definite if it's not symmetric? I did something wrong I'm sure.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite', 'positive-semidefinite']"
69,Weird matrix identity,Weird matrix identity,,"I stumbled upon a weird equality that I can't seem to explain: For a $N\times N$ (symmetric) matrix $\mathbf{X}$ that satisfies the eigenvalue problem $$\mathbf{X} \mathbf{v}_k = \lambda_k \mathbf{v}_k$$ and a $N\times1$ matrix $\mathbf{u}$ that is split in 2 (elements in the first half are equal to 1, equal to -1 for the second half), i.e. $$\mathbf{u}^T := \frac{1}{\sqrt{N}}\bigl[\underbrace{1 ...\ 1}_{\times N/2}\ \underbrace{-1\ ....-1}_{\times N/2}\bigr]$$ the following (apparantly) holds $$ \mathbf{u}^T(z \mathbf{I} - \mathbf{X})^{-1} \mathbf{u} = \sum_{k=1}^N \frac{(\mathbf{u}^T \mathbf{v}_k)^2}{z-\lambda_k}.$$ $z$ is complex but vanishingly close to the real axis. I'm guessing it has something to do with the resolvent $$ (z \mathbf{I} - \mathbf{X})^{-1} \equiv \sum_{k=0}^\infty \frac{\mathbf{X}^k}{z^{k+1}}$$ but I can't figure it...","I stumbled upon a weird equality that I can't seem to explain: For a $N\times N$ (symmetric) matrix $\mathbf{X}$ that satisfies the eigenvalue problem $$\mathbf{X} \mathbf{v}_k = \lambda_k \mathbf{v}_k$$ and a $N\times1$ matrix $\mathbf{u}$ that is split in 2 (elements in the first half are equal to 1, equal to -1 for the second half), i.e. $$\mathbf{u}^T := \frac{1}{\sqrt{N}}\bigl[\underbrace{1 ...\ 1}_{\times N/2}\ \underbrace{-1\ ....-1}_{\times N/2}\bigr]$$ the following (apparantly) holds $$ \mathbf{u}^T(z \mathbf{I} - \mathbf{X})^{-1} \mathbf{u} = \sum_{k=1}^N \frac{(\mathbf{u}^T \mathbf{v}_k)^2}{z-\lambda_k}.$$ $z$ is complex but vanishingly close to the real axis. I'm guessing it has something to do with the resolvent $$ (z \mathbf{I} - \mathbf{X})^{-1} \equiv \sum_{k=0}^\infty \frac{\mathbf{X}^k}{z^{k+1}}$$ but I can't figure it...",,"['matrices', 'eigenvalues-eigenvectors']"
70,Name of this matrix product?,Name of this matrix product?,,"Suppose $A$ and $B$ have columns as follows, $$A = \begin{bmatrix} a_1 & a_2 & \dots & a_n \end{bmatrix},$$ $$B = \begin{bmatrix} b_1 & b_2 & \dots & b_n \end{bmatrix}.$$ Is there any name for the following matrix ""product"", or simple way of expressing in terms of standard matrix operations: $$A \times B := \begin{bmatrix} a_1 b_1^T & a_2 b_1^T & \dots & a_n b_1^T \\ a_1 b_2^T & a_2 b_2^T & \dots & a_n b_2^T \\ \vdots & \vdots & \ddots & \vdots \\ a_1 b_n^T & a_2 b_n^T & \dots & a_n b_n^T \end{bmatrix}?$$ It is almost like the product of the vectorizations , $$\mathrm{vec}(B)\mathrm{vec}(A)^T = \begin{bmatrix} b_1 a_1^T & b_1 a_2^T & \dots & b_1 a_n^T \\ b_2 a_1^T & b_2 a_2^T & \dots & b_2 a_n^T \\ \vdots & \vdots & \ddots & \vdots \\ b_n a_1^T & b_n a_2^T & \dots & b_n a_n^T \end{bmatrix}$$ except every sub block is individually transposed.","Suppose $A$ and $B$ have columns as follows, $$A = \begin{bmatrix} a_1 & a_2 & \dots & a_n \end{bmatrix},$$ $$B = \begin{bmatrix} b_1 & b_2 & \dots & b_n \end{bmatrix}.$$ Is there any name for the following matrix ""product"", or simple way of expressing in terms of standard matrix operations: $$A \times B := \begin{bmatrix} a_1 b_1^T & a_2 b_1^T & \dots & a_n b_1^T \\ a_1 b_2^T & a_2 b_2^T & \dots & a_n b_2^T \\ \vdots & \vdots & \ddots & \vdots \\ a_1 b_n^T & a_2 b_n^T & \dots & a_n b_n^T \end{bmatrix}?$$ It is almost like the product of the vectorizations , $$\mathrm{vec}(B)\mathrm{vec}(A)^T = \begin{bmatrix} b_1 a_1^T & b_1 a_2^T & \dots & b_1 a_n^T \\ b_2 a_1^T & b_2 a_2^T & \dots & b_2 a_n^T \\ \vdots & \vdots & \ddots & \vdots \\ b_n a_1^T & b_n a_2^T & \dots & b_n a_n^T \end{bmatrix}$$ except every sub block is individually transposed.",,"['linear-algebra', 'matrices', 'block-matrices']"
71,"If A, B, C, D are non-invertible $n \times n$ matrices, is it true that their $2n \times 2n$ block matrix is non-invertible?","If A, B, C, D are non-invertible  matrices, is it true that their  block matrix is non-invertible?",n \times n 2n \times 2n,Is it true that $ \left( \begin{array}{ccc} A & B \\ C & D \\\end{array} \right)$ is non-invertible? Assume that the matrix is over a field.,Is it true that $ \left( \begin{array}{ccc} A & B \\ C & D \\\end{array} \right)$ is non-invertible? Assume that the matrix is over a field.,,"['linear-algebra', 'matrices']"
72,Matrix Differentiation of $x^TAx$ [duplicate],Matrix Differentiation of  [duplicate],x^TAx,This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . I know the matrix differentiation of $x^TAx$ is if A is symmetric is 2Ax. I saw that some people write as $2x^TA$. Are these two results the same? I am not sure how they are same. Can anyone explain to me..,This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . I know the matrix differentiation of $x^TAx$ is if A is symmetric is 2Ax. I saw that some people write as $2x^TA$. Are these two results the same? I am not sure how they are same. Can anyone explain to me..,,"['linear-algebra', 'matrices']"
73,Trace zero matrix that can't be written as $AB - BA$?,Trace zero matrix that can't be written as ?,AB - BA,"According to this paper , every trace zero matrix over a field can be written in the form $AB - BA$.  However, here's a basic counterexample: Let $A = diag(a, -a)$ for some nonzero number a.  Then $A  = BC - CB$ if and only if a system of equations involving entries of $B,C,A$ is true.  Then the entries of $A$ are: $$\begin{align} a = b_{1,1}c_{1,1} + b_{1,2}c_{2,1} - b_{1,1}c_{1,1} - b_{2,1}c_{1,2} \\ 0 = b_{1,1}c_{1,2} + b_{1,2}c_{2,2} - b_{1,1}c_{2,1} - b_{2,1}c_{2,2} \\ -a = b_{2,1}c_{1,1} + b_{2,2}c_{2,1} - b_{1,2}c_{1,1} - b_{2,2}c_{1,2} \\ 0 = b_{2,1}c_{1,2} + b_{2,2}c_{2,2} - b_{1,2}c_{2,1} - b_{2,2}c_{2,2} \end{align} $$ For $a$ nonzero the first and the last equations can't both be true.  So where did I mess up?","According to this paper , every trace zero matrix over a field can be written in the form $AB - BA$.  However, here's a basic counterexample: Let $A = diag(a, -a)$ for some nonzero number a.  Then $A  = BC - CB$ if and only if a system of equations involving entries of $B,C,A$ is true.  Then the entries of $A$ are: $$\begin{align} a = b_{1,1}c_{1,1} + b_{1,2}c_{2,1} - b_{1,1}c_{1,1} - b_{2,1}c_{1,2} \\ 0 = b_{1,1}c_{1,2} + b_{1,2}c_{2,2} - b_{1,1}c_{2,1} - b_{2,1}c_{2,2} \\ -a = b_{2,1}c_{1,1} + b_{2,2}c_{2,1} - b_{1,2}c_{1,1} - b_{2,2}c_{1,2} \\ 0 = b_{2,1}c_{1,2} + b_{2,2}c_{2,2} - b_{1,2}c_{2,1} - b_{2,2}c_{2,2} \end{align} $$ For $a$ nonzero the first and the last equations can't both be true.  So where did I mess up?",,"['linear-algebra', 'matrices', 'examples-counterexamples']"
74,If AB is a projection then BA is a projection,If AB is a projection then BA is a projection,,"Given two complex matrices $A$ and $B$, and knowing $AB$ is a projection, prove or provide a counterexample that $BA$ is a projection. Stuck with this question, need help. According to the matrix projection properties, $BA$ should be idempotent to be projection, so $(BA)(BA)$ = $BA$. We can rewrite it, if $B$ is invertible, as $$(BA)(BA)=B(AB)(AB)B^{-1} \Rightarrow (BA)(BA)=BA.$$ However it seems impossible in given conditions to prove that $B$ is invertible, so I am most likely moving in the wrong direction. Any help or reference much is appreciated, thank you.","Given two complex matrices $A$ and $B$, and knowing $AB$ is a projection, prove or provide a counterexample that $BA$ is a projection. Stuck with this question, need help. According to the matrix projection properties, $BA$ should be idempotent to be projection, so $(BA)(BA)$ = $BA$. We can rewrite it, if $B$ is invertible, as $$(BA)(BA)=B(AB)(AB)B^{-1} \Rightarrow (BA)(BA)=BA.$$ However it seems impossible in given conditions to prove that $B$ is invertible, so I am most likely moving in the wrong direction. Any help or reference much is appreciated, thank you.",,"['linear-algebra', 'matrices']"
75,von Neumann entropy and change of basis,von Neumann entropy and change of basis,,"The von Neumann entropy is defined as $S(\rho)=-Tr({\rho \ln \rho})$, where $\rho$ is density matrix. http://en.wikipedia.org/wiki/Von_Neumann_entropy In the above article it says: S(ρ) is invariant under changes in the basis of ρ, that is, S(ρ) =   S(UρU†), with U a unitary transformation. How can we prove this statement? We have that the trace is independent of the choice of basis in which the matrix $\rho$ is expressed:  $$Tr(\rho)=Tr(U \rho U^{\dagger})$$ But in the case of the von Neumann entropy we have the $\ln \rho$, so a change of basis for $\rho$ gives: $$Tr[U \rho U^{\dagger}\ln (U \rho U^{\dagger})]$$ How is this equal to $Tr(\rho \ln\rho)$?","The von Neumann entropy is defined as $S(\rho)=-Tr({\rho \ln \rho})$, where $\rho$ is density matrix. http://en.wikipedia.org/wiki/Von_Neumann_entropy In the above article it says: S(ρ) is invariant under changes in the basis of ρ, that is, S(ρ) =   S(UρU†), with U a unitary transformation. How can we prove this statement? We have that the trace is independent of the choice of basis in which the matrix $\rho$ is expressed:  $$Tr(\rho)=Tr(U \rho U^{\dagger})$$ But in the case of the von Neumann entropy we have the $\ln \rho$, so a change of basis for $\rho$ gives: $$Tr[U \rho U^{\dagger}\ln (U \rho U^{\dagger})]$$ How is this equal to $Tr(\rho \ln\rho)$?",,"['matrices', 'mathematical-physics', 'entropy']"
76,Eigenvectors of a $2 \times 2$ matrix when the eigenvalues are not integers,Eigenvectors of a  matrix when the eigenvalues are not integers,2 \times 2,"How can I calculate the eigenvectors of the following matrix? $$\begin{bmatrix}1& 3\\3& 2\end{bmatrix}$$ I calculated the eigenvalues. I got $$\lambda_1 =  4.541381265149109$$ $$\lambda_2 = -1.5413812651491097$$ But, now I don't know how to get the eigenvectors. When I create a new matrix after I subtracted Lambda value from all the members of the matrix on the main diagonal and tried to solve the homogeneous system of equations, I get only null vector for both $\lambda_1$ and $\lambda_2$ .... When I used this website for calculating eigenvalues and eigenvectors. I got these eigenvectors $(0.6463748961301958, 0.7630199824727257)$ for $\lambda_1$ $(-0.7630199824727257, 0.6463748961301957)$ for $\lambda_2$ .... but have no idea how to calculate them by myself... Is it even possible? ....or it's possible to calculate it numerically?","How can I calculate the eigenvectors of the following matrix? I calculated the eigenvalues. I got But, now I don't know how to get the eigenvectors. When I create a new matrix after I subtracted Lambda value from all the members of the matrix on the main diagonal and tried to solve the homogeneous system of equations, I get only null vector for both and .... When I used this website for calculating eigenvalues and eigenvectors. I got these eigenvectors for for .... but have no idea how to calculate them by myself... Is it even possible? ....or it's possible to calculate it numerically?","\begin{bmatrix}1& 3\\3& 2\end{bmatrix} \lambda_1 =  4.541381265149109 \lambda_2 = -1.5413812651491097 \lambda_1 \lambda_2 (0.6463748961301958, 0.7630199824727257) \lambda_1 (-0.7630199824727257, 0.6463748961301957) \lambda_2","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
77,determinant divisible by 13,determinant divisible by 13,,"Question: Given: $195,403$ and $247$ are divisible by 13. Prove (without actually calculating the determinant) that $$\det \begin{bmatrix} 1 & 9 & 5 \\ 2 & 4 & 7 \\ 4 & 0 & 3 \end{bmatrix}$$ is divisible by 13. What I did: Apart from calculating the determinant and seeing that it's true, I couldn't really think of anything else... Thanks","Question: Given: $195,403$ and $247$ are divisible by 13. Prove (without actually calculating the determinant) that $$\det \begin{bmatrix} 1 & 9 & 5 \\ 2 & 4 & 7 \\ 4 & 0 & 3 \end{bmatrix}$$ is divisible by 13. What I did: Apart from calculating the determinant and seeing that it's true, I couldn't really think of anything else... Thanks",,"['linear-algebra', 'matrices']"
78,Jordan normal form theorem - a question about the proof I've found,Jordan normal form theorem - a question about the proof I've found,,"I've been reading this proof of Jordan's theorem: http://www.cs.uleth.ca/~holzmann/notes/jordan.pdf and there are a few questions I hope you could answer for me. Firstly, why $(A_{\lambda})_{\mu _i} = A _{\lambda + \mu _{i}} =: A _{\lambda_{ \ i}}$ and secondly, how do we know that $\dim (W \cap Ker A_{\lambda}) \ge 1$? The problem in the first question is that I'm not sure what $(A_{\lambda})_{\mu _i}$ means. Thank you. I understand that we pick a vector $v\neq 0$ with an eigenvalue $\lambda$. $A$ is $n \times n$ $A_{\lambda} = A - \lambda I \ \ \ \ \ \ \ \ \ A_{\lambda}v=0,$ so we have $r = \dim \ker A_{\lambda}>0$ $\dim \ range A_{\lambda} = n-r<n $ , $W := range \ A_{\lambda} $ I undestand that because $W := range \ A_{\lambda} $, then $ A_{\lambda} (W) \subset W$, so  $A_{\lambda} $ induces $T: W \rightarrow W$ and we can apply inductive hipothesis to its matrix. What I don't understand is why all of a sudden there are $\lambda_i$ over the arrows and it is probably because I don't know what $(A_{\lambda})_{\mu _i}$ means","I've been reading this proof of Jordan's theorem: http://www.cs.uleth.ca/~holzmann/notes/jordan.pdf and there are a few questions I hope you could answer for me. Firstly, why $(A_{\lambda})_{\mu _i} = A _{\lambda + \mu _{i}} =: A _{\lambda_{ \ i}}$ and secondly, how do we know that $\dim (W \cap Ker A_{\lambda}) \ge 1$? The problem in the first question is that I'm not sure what $(A_{\lambda})_{\mu _i}$ means. Thank you. I understand that we pick a vector $v\neq 0$ with an eigenvalue $\lambda$. $A$ is $n \times n$ $A_{\lambda} = A - \lambda I \ \ \ \ \ \ \ \ \ A_{\lambda}v=0,$ so we have $r = \dim \ker A_{\lambda}>0$ $\dim \ range A_{\lambda} = n-r<n $ , $W := range \ A_{\lambda} $ I undestand that because $W := range \ A_{\lambda} $, then $ A_{\lambda} (W) \subset W$, so  $A_{\lambda} $ induces $T: W \rightarrow W$ and we can apply inductive hipothesis to its matrix. What I don't understand is why all of a sudden there are $\lambda_i$ over the arrows and it is probably because I don't know what $(A_{\lambda})_{\mu _i}$ means",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
79,Rank of a matrix.,Rank of a matrix.,,Let a non-zero column matrix $A_{m\times 1}$ be multiplied with a non-zero row matrix $B_{1\times n}$ to get a matrix $X_{m\times n}$ . Then how to find rank of $X$?,Let a non-zero column matrix $A_{m\times 1}$ be multiplied with a non-zero row matrix $B_{1\times n}$ to get a matrix $X_{m\times n}$ . Then how to find rank of $X$?,,['matrices']
80,$4 \times 4$ matrix and its inverse. Is my method ok?,matrix and its inverse. Is my method ok?,4 \times 4,"I have been struggling to derive inverse matrix of a $4 \times 4$  Lorenz matrix $\Lambda$.  $$ \Lambda = \begin{bmatrix} \gamma&0&0&-\beta \gamma \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ -\beta \gamma & 0 & 0 & \gamma  \end{bmatrix} $$ My professor says that inverse is: $$ \Lambda^{-1} = \begin{bmatrix} \gamma&0&0&\beta \gamma \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ \beta \gamma & 0 & 0 & \gamma  \end{bmatrix} $$ Soo i tried to derive inverse matrix $\Lambda^{-1}$ using adjugate formula: $$\Lambda^{-1} = \frac{1}{|\Lambda|} \textrm{adj}(\Lambda)$$ I did it quite metodically by first calculating the determinant of $\Lambda$, then matrix of minors, matrix of cofactors,  adjugate matrix and in the end using the above formula to find the inverse ( here is my whole derivation ). Long story short, at the end I end up with this: $$ \Lambda^{-1} = \frac{1}{|\Lambda|} \textrm{adj}(\Lambda) = \frac{1}{\gamma^2 (1 - \beta^2)} \begin{bmatrix} \gamma & 0 & 0 &\beta \gamma\\ 0 & \gamma^2(1-\beta^2) & 0 & 0\\ 0 & 0 & \gamma^2(1-\beta^2) & 0\\ \beta \gamma & 0 & 0 & \gamma\\ \end{bmatrix} $$ The result is not what my professor says i should get. In my adjugate matrix parts with $\gamma$ and $\beta \gamma$ seem wrong. Is it possible my professor wrote down wrong inverse matrix? Could anyone point me to the right direction?  I am kind of lost here, but i am sure i have done a lot of work and am near the solution.","I have been struggling to derive inverse matrix of a $4 \times 4$  Lorenz matrix $\Lambda$.  $$ \Lambda = \begin{bmatrix} \gamma&0&0&-\beta \gamma \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ -\beta \gamma & 0 & 0 & \gamma  \end{bmatrix} $$ My professor says that inverse is: $$ \Lambda^{-1} = \begin{bmatrix} \gamma&0&0&\beta \gamma \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ \beta \gamma & 0 & 0 & \gamma  \end{bmatrix} $$ Soo i tried to derive inverse matrix $\Lambda^{-1}$ using adjugate formula: $$\Lambda^{-1} = \frac{1}{|\Lambda|} \textrm{adj}(\Lambda)$$ I did it quite metodically by first calculating the determinant of $\Lambda$, then matrix of minors, matrix of cofactors,  adjugate matrix and in the end using the above formula to find the inverse ( here is my whole derivation ). Long story short, at the end I end up with this: $$ \Lambda^{-1} = \frac{1}{|\Lambda|} \textrm{adj}(\Lambda) = \frac{1}{\gamma^2 (1 - \beta^2)} \begin{bmatrix} \gamma & 0 & 0 &\beta \gamma\\ 0 & \gamma^2(1-\beta^2) & 0 & 0\\ 0 & 0 & \gamma^2(1-\beta^2) & 0\\ \beta \gamma & 0 & 0 & \gamma\\ \end{bmatrix} $$ The result is not what my professor says i should get. In my adjugate matrix parts with $\gamma$ and $\beta \gamma$ seem wrong. Is it possible my professor wrote down wrong inverse matrix? Could anyone point me to the right direction?  I am kind of lost here, but i am sure i have done a lot of work and am near the solution.",,['matrices']
81,If $A \in M_2(\mathbb R)$ non identical with $A^3=I $ then $\text{tr}(A)=-1$,If  non identical with  then,A \in M_2(\mathbb R) A^3=I  \text{tr}(A)=-1,"Let $A \in M_2(\mathbb R)$ a $2\times 2$ matrix with real coefficient, such that $A \ne I$ and   $$ A^3=I $$   Then $\text{tr}(A)=-1$. What if we consider $M_n(\mathbb R)$? Is the statement still true? I didn't manage to solve it, but I have a question: can we say $A$ is non singular? Indeed,  $$ A^3=I \Rightarrow AA^2=A^2A=I\Leftrightarrow A^{-1} = A^2. $$ Is it right? How can we prove the statement?","Let $A \in M_2(\mathbb R)$ a $2\times 2$ matrix with real coefficient, such that $A \ne I$ and   $$ A^3=I $$   Then $\text{tr}(A)=-1$. What if we consider $M_n(\mathbb R)$? Is the statement still true? I didn't manage to solve it, but I have a question: can we say $A$ is non singular? Indeed,  $$ A^3=I \Rightarrow AA^2=A^2A=I\Leftrightarrow A^{-1} = A^2. $$ Is it right? How can we prove the statement?",,['matrices']
82,find the eigenvalues of a linear operator T,find the eigenvalues of a linear operator T,,"Let $A$ be $m*m$ and B be $n*n$ complex matrices, and consider the linear operator $T$ on the space $C^{m*n}$ of all  $m*n$ complex matrices defined by $T(M) = AMB$. -Show how to construct an eigenvector for $T$ out of a pair of column vectors $X, Y$, where $X$ is an eigenvector for $A$ and $Y$ is an eigenvector for $B^t$. -Determine the eigenvalues of $T$ in terms of those of $A$ and $B$ -Determine the trace of this operator","Let $A$ be $m*m$ and B be $n*n$ complex matrices, and consider the linear operator $T$ on the space $C^{m*n}$ of all  $m*n$ complex matrices defined by $T(M) = AMB$. -Show how to construct an eigenvector for $T$ out of a pair of column vectors $X, Y$, where $X$ is an eigenvector for $A$ and $Y$ is an eigenvector for $B^t$. -Determine the eigenvalues of $T$ in terms of those of $A$ and $B$ -Determine the trace of this operator",,"['linear-algebra', 'matrices']"
83,the rank of an interesting matrix,the rank of an interesting matrix,,"Let $A$ be a square matrix whose off-diagonal entries $a_{i,j} \in (0,1)$ when $i \neq j$. The diagonal entries of $A$ are all 1s. I am wondering whether $A$ has a full rank.","Let $A$ be a square matrix whose off-diagonal entries $a_{i,j} \in (0,1)$ when $i \neq j$. The diagonal entries of $A$ are all 1s. I am wondering whether $A$ has a full rank.",,"['linear-algebra', 'matrices']"
84,How to find the tangent space to a matrix space,How to find the tangent space to a matrix space,,"I have a hard time approaching these types of problems. In an article it had claimed that the tangent space to all symmetric matrices with the same signature as $M$ at a matrix $M$ is the set of all the matrices of the form $WM+MW^T$. So, I started looking for similar and maybe simpler problems of this type, like finding the tangent space to the set of all invertible matrices at $I$, or the tangent space to the set of all matrices with determinant equal to $1$ at $I$. Without giving a solution could you give me a hint on how do you start solving such problems? What I know is that I have to find a path on these manifolds which pass through that certain matrix and find the derivative of that path. I've also heard of using the exponential map, but I'm not sure that I've understood it. What is not clear to me is that how do you think of a path for such a manifold, and more importantly, how can one show that this is a manifold? To make the question clear here is what I want to show: Let $SL_n\mathbb(R)$ be the set of all real $n\times n$ matrices with determinant equal to 1. Show this set is a manifold, and find the tangent space to this manifold at identity.","I have a hard time approaching these types of problems. In an article it had claimed that the tangent space to all symmetric matrices with the same signature as $M$ at a matrix $M$ is the set of all the matrices of the form $WM+MW^T$. So, I started looking for similar and maybe simpler problems of this type, like finding the tangent space to the set of all invertible matrices at $I$, or the tangent space to the set of all matrices with determinant equal to $1$ at $I$. Without giving a solution could you give me a hint on how do you start solving such problems? What I know is that I have to find a path on these manifolds which pass through that certain matrix and find the derivative of that path. I've also heard of using the exponential map, but I'm not sure that I've understood it. What is not clear to me is that how do you think of a path for such a manifold, and more importantly, how can one show that this is a manifold? To make the question clear here is what I want to show: Let $SL_n\mathbb(R)$ be the set of all real $n\times n$ matrices with determinant equal to 1. Show this set is a manifold, and find the tangent space to this manifold at identity.",,"['matrices', 'differential-geometry']"
85,Inverse of a 4x4 Matrix,Inverse of a 4x4 Matrix,,"I'm trying to take the inverse of this matrix: $\left[\begin{array}{rrrr} 2/7 &-6/7  &3/7  &1 \\   6/7& 3/7 &2/7  &2 \\  -3/7 &2/7  &6/7  &3 \\  0\ \ &0\ \ &0\ \  &\>\>\>1  \end{array}\right]$ I have been using this as a guide and I've seen people talk about row elimination (I don't know how to use it) to solve it but I am unsure which strategy would be the best way to attack this problem. I know its going to be a large amount of work either way.","I'm trying to take the inverse of this matrix: $\left[\begin{array}{rrrr} 2/7 &-6/7  &3/7  &1 \\   6/7& 3/7 &2/7  &2 \\  -3/7 &2/7  &6/7  &3 \\  0\ \ &0\ \ &0\ \  &\>\>\>1  \end{array}\right]$ I have been using this as a guide and I've seen people talk about row elimination (I don't know how to use it) to solve it but I am unsure which strategy would be the best way to attack this problem. I know its going to be a large amount of work either way.",,['matrices']
86,Name the matrix (not the game show),Name the matrix (not the game show),,"I have a matrix of the following form: $ \begin{matrix} a_1 & 0 & \ldots & 0 \\ a_2 & a_1 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ a_n & a_{n-1} & \ldots & a_1 \end{matrix} $ I'd like to know if this matrix has a special name (or even if it's a sum/product of special matrices). The thing is, I need to build this matrix in matlab, and I could use some for loops, but I'd rather use some clean builtin functions. Thanks Edit: I just had a thought, maybe it can be a Vandermonde matrix times some other (unknown) matrix to nullify the terms above the diagonal...","I have a matrix of the following form: $ \begin{matrix} a_1 & 0 & \ldots & 0 \\ a_2 & a_1 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ a_n & a_{n-1} & \ldots & a_1 \end{matrix} $ I'd like to know if this matrix has a special name (or even if it's a sum/product of special matrices). The thing is, I need to build this matrix in matlab, and I could use some for loops, but I'd rather use some clean builtin functions. Thanks Edit: I just had a thought, maybe it can be a Vandermonde matrix times some other (unknown) matrix to nullify the terms above the diagonal...",,['matrices']
87,Can we think of matrices as functions of sorts?,Can we think of matrices as functions of sorts?,,"Can we think of matrices as surjective functions where; $i,j$ (the indices) represent the inputs and $z$ (the element at that particular position)? Further, ( $i,j,z$ ) as an ordered triplet... The reason why I ask, is because, then that could possibly allow us to think of matrix multiplication as a composition of sorts, right? I think that this approach works, but I'm new to the matrices, so I wanted to check here. I've heard of linear maps, but I don't think that they're what I'm aiming for. Are they?","Can we think of matrices as surjective functions where; (the indices) represent the inputs and (the element at that particular position)? Further, ( ) as an ordered triplet... The reason why I ask, is because, then that could possibly allow us to think of matrix multiplication as a composition of sorts, right? I think that this approach works, but I'm new to the matrices, so I wanted to check here. I've heard of linear maps, but I don't think that they're what I'm aiming for. Are they?","i,j z i,j,z","['linear-algebra', 'matrices', 'soft-question']"
88,"Given $A^2+B^2=\left(\begin{smallmatrix}1402&2022\\2022&1402\end{smallmatrix}\right)$ for which $A,B\in M_2(\mathbb{R})$, show that $AB\neq BA$","Given  for which , show that","A^2+B^2=\left(\begin{smallmatrix}1402&2022\\2022&1402\end{smallmatrix}\right) A,B\in M_2(\mathbb{R}) AB\neq BA","$M_2(\mathbb{R})$ is the set of all $2\times2$ matrices that their entries are in $\mathbb{R}$ . Now consider $A,B\in M_2(\mathbb{R})$ . We have $$A^2+B^2= \begin{bmatrix}1402&&2022\\ 2022 && 1402\\ \end{bmatrix} $$ Show that $AB\neq BA$ . I tried writing $(A+B)^2=A^2+AB+BA+B^2$ and we know if we assume $AB=BA$ , then $(A+B)^2=A^2+2AB+B^2$ . Then I tried to find the form of a squared matrix and compare these to reach a contradiction but I couldn't. I even don't know if it helps or not. Any help is appreciated!","is the set of all matrices that their entries are in . Now consider . We have Show that . I tried writing and we know if we assume , then . Then I tried to find the form of a squared matrix and compare these to reach a contradiction but I couldn't. I even don't know if it helps or not. Any help is appreciated!","M_2(\mathbb{R}) 2\times2 \mathbb{R} A,B\in M_2(\mathbb{R}) A^2+B^2=
\begin{bmatrix}1402&&2022\\
2022 && 1402\\
\end{bmatrix}
 AB\neq BA (A+B)^2=A^2+AB+BA+B^2 AB=BA (A+B)^2=A^2+2AB+B^2","['linear-algebra', 'matrices', 'matrix-equations']"
89,Does this nonlinear equation give eigenvectors for matrix $A$?,Does this nonlinear equation give eigenvectors for matrix ?,A,"Let $A$ be a matrix with nonnegative entries and $y$ be a vector satisfying $$\sum_{k = 1}^\infty y^k =  Ay $$ where $[y^k]_i = y_i^k$ is the pointwise exponential and $0 < y_i < 1$ . Is $y$ an eigenvector of $A$ ? Then proof shown in the text I am reading now argues as follows. Let $y = \epsilon x $ where $\epsilon > 0$ is arbitrarily small. Dividing the above equation by $\epsilon$ gives $$Ax = x + \epsilon x^2 + O(\epsilon^2)$$ For sufficiently small $\epsilon > 0$ , it reduces to $Ax = x$ . My question is how can we take $\epsilon \to 0$ ? The vector $x$ clearly depends on $\epsilon$ .","Let be a matrix with nonnegative entries and be a vector satisfying where is the pointwise exponential and . Is an eigenvector of ? Then proof shown in the text I am reading now argues as follows. Let where is arbitrarily small. Dividing the above equation by gives For sufficiently small , it reduces to . My question is how can we take ? The vector clearly depends on .",A y \sum_{k = 1}^\infty y^k =  Ay  [y^k]_i = y_i^k 0 < y_i < 1 y A y = \epsilon x  \epsilon > 0 \epsilon Ax = x + \epsilon x^2 + O(\epsilon^2) \epsilon > 0 Ax = x \epsilon \to 0 x \epsilon,"['linear-algebra', 'matrices']"
90,Non-mathematician looking for a complete example of representing points in one 3D coordinate system as points in another,Non-mathematician looking for a complete example of representing points in one 3D coordinate system as points in another,,"I am going crazy researching trigonometry, vectors, matrices, quaternions, Euler angles, etc. etc. in pursuit of an efficient, ""best practice"" solution to the problem of representing points in one 3D coordinate system as points in another 3D coordinate system. There are plenty of answers to this question already, but none that I can use because they are too high level, assuming the reader will know how to flesh out the described methods. So I'm looking for a specific, step by step, actual calculation that demonstrates how the job is done. I hope this approach will also help others who may be as confused by their research as I am. Let's say we have object A in its own ""right hand rule"" 3D XYZ coordinate system, ""floating in space"" at no particular location or coordinates, with one point P defined in this object, at X=2, Y=4, Z=7. Let's say we have object B with its own 3D XYZ coordinate system, and this object initially was in the same position and the same orientation as A, their coordinate systems matched (and therefore P would have the same XYZ values in B as in A). But now B is separated from A, in A's coordinates, 10 units on X, 5 units on Y, and 4 on Z. B is additionally rotated, in order and in its own coordinates, 45 degrees on the Z axis, 225 degrees on the Y, and 3 on the X. What are the XYZ values of P in B's coordinates? Please provide the complete math that the reader may reproduce either by hand or programming. I will be grateful for an answer based on any method, but it would be super cool if a number of answers are posted using different methods.","I am going crazy researching trigonometry, vectors, matrices, quaternions, Euler angles, etc. etc. in pursuit of an efficient, ""best practice"" solution to the problem of representing points in one 3D coordinate system as points in another 3D coordinate system. There are plenty of answers to this question already, but none that I can use because they are too high level, assuming the reader will know how to flesh out the described methods. So I'm looking for a specific, step by step, actual calculation that demonstrates how the job is done. I hope this approach will also help others who may be as confused by their research as I am. Let's say we have object A in its own ""right hand rule"" 3D XYZ coordinate system, ""floating in space"" at no particular location or coordinates, with one point P defined in this object, at X=2, Y=4, Z=7. Let's say we have object B with its own 3D XYZ coordinate system, and this object initially was in the same position and the same orientation as A, their coordinate systems matched (and therefore P would have the same XYZ values in B as in A). But now B is separated from A, in A's coordinates, 10 units on X, 5 units on Y, and 4 on Z. B is additionally rotated, in order and in its own coordinates, 45 degrees on the Z axis, 225 degrees on the Y, and 3 on the X. What are the XYZ values of P in B's coordinates? Please provide the complete math that the reader may reproduce either by hand or programming. I will be grateful for an answer based on any method, but it would be super cool if a number of answers are posted using different methods.",,"['matrices', 'vectors', 'linear-transformations', '3d', 'rotations']"
91,"Let $A, B, M ∈ M_n(C)$ such that $AM = MB$ . If $M ≠0$, Show that $A$ and $B$ must have at least one eigenvalue in common. [duplicate]","Let  such that  . If , Show that  and  must have at least one eigenvalue in common. [duplicate]","A, B, M ∈ M_n(C) AM = MB M ≠0 A B",This question already has answers here : There exists $C\neq0$ with $CA=BC$ iff $A$ and $B$ have a common eigenvalue (5 answers) Closed 1 year ago . I supose that if $t$ is an eigenvalue of $B$ and hence there is an eigenvector $x$ associated such that $Bx=tx$ $MBx=M(tx)$ $AMx=A(Mx)=t(Mx)$ How can I ensure that $Mx$ is different from $0$ so that $t$ is an eigenvalue of $A$ ?,This question already has answers here : There exists $C\neq0$ with $CA=BC$ iff $A$ and $B$ have a common eigenvalue (5 answers) Closed 1 year ago . I supose that if is an eigenvalue of and hence there is an eigenvector associated such that How can I ensure that is different from so that is an eigenvalue of ?,t B x Bx=tx MBx=M(tx) AMx=A(Mx)=t(Mx) Mx 0 t A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
92,"If $A + tB$ is similar to $A$ for infinitely many values of $t$, where $A$ is diagonalizable, is $B$ necessarily equal to $0$?","If  is similar to  for infinitely many values of , where  is diagonalizable, is  necessarily equal to ?",A + tB A t A B 0,"I came across a similar question here , which should guarantee that $B$ is nilpotent, but I wonder if knowing that the matrices are similar (not just having the same eigenvalues) is enough to conclude that $B = 0$ . I'm particularly interested in the case where $A$ is diagonalizable, but I would not be surprised if that isn't relevant here. I considered using the Jordan-Chevalley decomposition to show that $B = 0$ since if $A = P(A + tB)P^{-1}$ for $t \neq 0$ , we have $A = A + 0$ as well as $A = PAP^{-1} + tPBP^{-1}$ , where $A$ and $PAP^{-1}$ are diagonalizable and $0$ and $tPBP^{-1}$ are nilpotent. However, I don't know that $A$ and $B$ commute, so this decomposition is not necessarily unique. Any help would be appreciated.","I came across a similar question here , which should guarantee that is nilpotent, but I wonder if knowing that the matrices are similar (not just having the same eigenvalues) is enough to conclude that . I'm particularly interested in the case where is diagonalizable, but I would not be surprised if that isn't relevant here. I considered using the Jordan-Chevalley decomposition to show that since if for , we have as well as , where and are diagonalizable and and are nilpotent. However, I don't know that and commute, so this decomposition is not necessarily unique. Any help would be appreciated.",B B = 0 A B = 0 A = P(A + tB)P^{-1} t \neq 0 A = A + 0 A = PAP^{-1} + tPBP^{-1} A PAP^{-1} 0 tPBP^{-1} A B,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-decomposition']"
93,Calculate: $\Delta=\left|\begin{array}{ccc} b c & c a & a b \\ a(b+c) & b(c+a) & c(a+b) \\ a^{2} & b^{2} & c^{2} \end{array}\right|$,Calculate:,\Delta=\left|\begin{array}{ccc} b c & c a & a b \\ a(b+c) & b(c+a) & c(a+b) \\ a^{2} & b^{2} & c^{2} \end{array}\right|,Calculate: $$\Delta=\left|\begin{array}{ccc} b c & c a & a b \\ a(b+c) & b(c+a) & c(a+b) \\ a^{2} & b^{2} & c^{2} \end{array}\right|$$ Does anyone know any easy way to calculate this determinant? I tried the classic way but I was wondering if anyone knows an easier method.,Calculate: Does anyone know any easy way to calculate this determinant? I tried the classic way but I was wondering if anyone knows an easier method.,"\Delta=\left|\begin{array}{ccc}
b c & c a & a b \\
a(b+c) & b(c+a) & c(a+b) \\
a^{2} & b^{2} & c^{2}
\end{array}\right|","['matrices', 'determinant']"
94,A quick proof for $\mbox{rank}(A + B) \leq \mbox{rank}(A) + \mbox{rank}(B)$,A quick proof for,\mbox{rank}(A + B) \leq \mbox{rank}(A) + \mbox{rank}(B),"A recent question in this forum led me to recall the linear algebra result: $$ \mbox{rank}(A + B) \leq \mbox{rank}(A) + \mbox{rank}(B), \ \ \left( A, B \in \mathbf{R}^{n \times n} \right) $$ (I corrected my first posting after Ben Grossmann corrected it and cited an important inequality in linear algebra. I like to thank him first!) Define $U = \mbox{Range}(A)$ , $V = \mbox{Range}(B)$ . Then $U$ and $V$ are subspaces of $\mathbf{R}^n$ . Clearly, $\mbox{rank}(A) = \mbox{dim}(U)$ , $\mbox{rank}(B) = \mbox{dim}(V)$ . Also, $\mbox{rank}(A + B) \leq \mbox{dim}(U + V)$ . (Thanks to Ben Grossmann for correcting my original statement!) We know the theorem from linear algebra: $$ \mbox{dim}(U + V) = \mbox{dim}(U) + \mbox{dim}(V) - \mbox{dim}(U \cap V) $$ which implies that $$ \mbox{dim}(U + V) \leq \mbox{dim}(U) + \mbox{dim}(V). $$ Thus, $\mbox{rank}(A + B) \leq \mbox{dim}(U + V) \leq \mbox{rank}(A) + \mbox{rank}(B)$ .","A recent question in this forum led me to recall the linear algebra result: (I corrected my first posting after Ben Grossmann corrected it and cited an important inequality in linear algebra. I like to thank him first!) Define , . Then and are subspaces of . Clearly, , . Also, . (Thanks to Ben Grossmann for correcting my original statement!) We know the theorem from linear algebra: which implies that Thus, .","
\mbox{rank}(A + B) \leq \mbox{rank}(A) + \mbox{rank}(B), \ \ \left( A, B \in \mathbf{R}^{n \times n} \right)
 U = \mbox{Range}(A) V = \mbox{Range}(B) U V \mathbf{R}^n \mbox{rank}(A) = \mbox{dim}(U) \mbox{rank}(B) = \mbox{dim}(V) \mbox{rank}(A + B) \leq \mbox{dim}(U + V) 
\mbox{dim}(U + V) = \mbox{dim}(U) + \mbox{dim}(V) - \mbox{dim}(U \cap V)
 
\mbox{dim}(U + V) \leq \mbox{dim}(U) + \mbox{dim}(V).
 \mbox{rank}(A + B) \leq \mbox{dim}(U + V) \leq \mbox{rank}(A) + \mbox{rank}(B)","['linear-algebra', 'matrices', 'solution-verification']"
95,Area calculation for transformed rectangle seems too small.,Area calculation for transformed rectangle seems too small.,,"I came upon this problem in Khan Academy precalculus, in the unit on matrices.  In the video , he shows this image of a rectangle and asks you to determine its area after being transformed by a matrix.  The determinant of the transformation, multiplied times the original area, should equal the transformed area. I followed the instructions and got his answer. $Area = 7 * 5 = 35$ $det=(5*8)-(9*4) = 4$ $Area' = 35 * 4 = 140$ But I didn't believe the answer, because the matrix seems to transform the unit vectors by a large factor.  So I reframed the problem to imagine the rectangle beginning with its corner on the origin. I defined a new vector for this rectangle, $A$ .  If I then apply the same transformation matrix ... $$ \begin{pmatrix}5 & 9\\\ 4 & 8\end{pmatrix}\begin{pmatrix}7\\\ 5\end{pmatrix}$$ $$7\begin{pmatrix}5 \\\ 4 \end{pmatrix}+5\begin{pmatrix}9 \\\ 8 \end{pmatrix}=\begin{pmatrix}35 \\\ 28 \end{pmatrix}+\begin{pmatrix}45 \\\ 40 \end{pmatrix}=\begin{pmatrix}80 \\\ 68 \end{pmatrix}$$ I get a rectangle of the dimensions $80*68=5440$ I'm certain that Khan is correct and I am wrong, but I don't understand why.  How can the area of the transformed image be so small when I think it should be very large?","I came upon this problem in Khan Academy precalculus, in the unit on matrices.  In the video , he shows this image of a rectangle and asks you to determine its area after being transformed by a matrix.  The determinant of the transformation, multiplied times the original area, should equal the transformed area. I followed the instructions and got his answer. But I didn't believe the answer, because the matrix seems to transform the unit vectors by a large factor.  So I reframed the problem to imagine the rectangle beginning with its corner on the origin. I defined a new vector for this rectangle, .  If I then apply the same transformation matrix ... I get a rectangle of the dimensions I'm certain that Khan is correct and I am wrong, but I don't understand why.  How can the area of the transformed image be so small when I think it should be very large?",Area = 7 * 5 = 35 det=(5*8)-(9*4) = 4 Area' = 35 * 4 = 140 A  \begin{pmatrix}5 & 9\\\ 4 & 8\end{pmatrix}\begin{pmatrix}7\\\ 5\end{pmatrix} 7\begin{pmatrix}5 \\\ 4 \end{pmatrix}+5\begin{pmatrix}9 \\\ 8 \end{pmatrix}=\begin{pmatrix}35 \\\ 28 \end{pmatrix}+\begin{pmatrix}45 \\\ 40 \end{pmatrix}=\begin{pmatrix}80 \\\ 68 \end{pmatrix} 80*68=5440,"['matrices', 'vectors', 'determinant', 'area']"
96,Why do all eighteen of these rotations have the same angle of rotation?,Why do all eighteen of these rotations have the same angle of rotation?,,"In 3 dimensions, a rotation can be characterized by a roll (an $\alpha$ degree rotation around the $x$ axis), pitch (a $\beta$ degree rotation around the $y$ axis), and a yaw (a $\gamma$ degree rotation around the $z$ axis), say in that order. (Let's call roll, pitch, and yaw the different 'modes' of rotation.) Therefore we can express the total rotation $R =  R_\alpha * P_\beta * Y_\gamma$ where each is a rotation matrix that I don't want to write out here. We can bash out what this matrix product evaluates to (it's not that difficult), and we find that its trace is $tr(R) = \cos{\alpha} \cos{\beta} + \cos{\beta} \cos{\gamma} + \cos{\gamma}\cos{\alpha} + \sin{\alpha} \sin{\beta} \sin{\gamma}$ . Alternatively, we can characterize $R$ by an axis-of-rotation which it leaves fixed and the angle that everything else gets rotated by (around the axis-of-rotation). This is Euler's Rotation Theorem. Therefore, if we let $T$ be any rotation (there are many) that brings the axis-of-rotation to the x-y axis, and then $S_\theta$ a rotation around the x-y axis by the angle theta (ie. the matrix $\begin{pmatrix}\cos{\theta} & -\sin{\theta} & 0 \\ \sin{\theta} & \cos{\theta} & 0 \\ 0 & 0 & 1\end{pmatrix}$ ), we can write $R = T S_\theta T^{-1}$ . Taking the trace, we get $tr(R) = tr(T S_\theta T^{-1}) = tr(T^{-1} T S_\theta) = tr(S_\theta) = 1 + 2\cos{\theta}$ where we use the following trace property: Cyclic Permutation Property of Trace (or just the ""Trace Property""). For three square matrices $A,B,C$ , we have $tr(ABC) = tr(CAB) = tr(BCA) := x$ . (Thus we also have $tr(BAC) = tr(ACB) = tr(CBA) :=y$ ; but $x \ne y$ in general.) Note that $CAB, BCA$ are cyclic permutations of $ABC$ , hence the name of the property. Therefore, we have $1 + 2\cos\theta = \cos{\alpha} \cos{\beta} + \cos{\beta} \cos{\gamma} + \cos{\gamma}\cos{\alpha} + \sin{\alpha} \sin{\beta} \sin{\gamma}$ . It is interesting that this expression is symmetric in the variables $\alpha, \beta, \gamma$ . This means that if we exchange the angles of in any of the 3!=6 ways (while keeping the modes in the same order), we obtain rotations that still have the same angle of rotation (but possibly different axes). Call this the ""Angle Permutation Property"". The Trace Property above also implies the same with the 3 cyclic permutations of the modes of rotation - call that the ""Mode Cyclic Permutation Property"". Combining the 6 permutations of the angles ( $\alpha, \beta, \gamma$ ) with the 3 cyclic permutations of the modes, we have found a set of 18 related rotations all of which have the same angle of rotation (but possibly different axes). Is there a geometric argument for the truth of the Angle Permutation Property? of the Mode Cyclic Permutation Property (for which a geometric argument for the Trace Property suffices)? Are their axes related in a similar way as well? What is the geometric significance of this equivalence of these 18 rotations? By the way, assuming $\alpha, \beta, \gamma$ are all small, we obtain $\theta^2 \approx \alpha^2 + \beta^2 + \gamma^2 (+\alpha\beta\gamma)$ , which perhaps makes sense because the three rotations are in some vague sense ""orthogonal"" (which sense?) but is perhaps surprising given that the surface of the sphere has only two dimensions, not three!","In 3 dimensions, a rotation can be characterized by a roll (an degree rotation around the axis), pitch (a degree rotation around the axis), and a yaw (a degree rotation around the axis), say in that order. (Let's call roll, pitch, and yaw the different 'modes' of rotation.) Therefore we can express the total rotation where each is a rotation matrix that I don't want to write out here. We can bash out what this matrix product evaluates to (it's not that difficult), and we find that its trace is . Alternatively, we can characterize by an axis-of-rotation which it leaves fixed and the angle that everything else gets rotated by (around the axis-of-rotation). This is Euler's Rotation Theorem. Therefore, if we let be any rotation (there are many) that brings the axis-of-rotation to the x-y axis, and then a rotation around the x-y axis by the angle theta (ie. the matrix ), we can write . Taking the trace, we get where we use the following trace property: Cyclic Permutation Property of Trace (or just the ""Trace Property""). For three square matrices , we have . (Thus we also have ; but in general.) Note that are cyclic permutations of , hence the name of the property. Therefore, we have . It is interesting that this expression is symmetric in the variables . This means that if we exchange the angles of in any of the 3!=6 ways (while keeping the modes in the same order), we obtain rotations that still have the same angle of rotation (but possibly different axes). Call this the ""Angle Permutation Property"". The Trace Property above also implies the same with the 3 cyclic permutations of the modes of rotation - call that the ""Mode Cyclic Permutation Property"". Combining the 6 permutations of the angles ( ) with the 3 cyclic permutations of the modes, we have found a set of 18 related rotations all of which have the same angle of rotation (but possibly different axes). Is there a geometric argument for the truth of the Angle Permutation Property? of the Mode Cyclic Permutation Property (for which a geometric argument for the Trace Property suffices)? Are their axes related in a similar way as well? What is the geometric significance of this equivalence of these 18 rotations? By the way, assuming are all small, we obtain , which perhaps makes sense because the three rotations are in some vague sense ""orthogonal"" (which sense?) but is perhaps surprising given that the surface of the sphere has only two dimensions, not three!","\alpha x \beta y \gamma z R =  R_\alpha * P_\beta * Y_\gamma tr(R) = \cos{\alpha} \cos{\beta} + \cos{\beta} \cos{\gamma} + \cos{\gamma}\cos{\alpha} + \sin{\alpha} \sin{\beta} \sin{\gamma} R T S_\theta \begin{pmatrix}\cos{\theta} & -\sin{\theta} & 0 \\ \sin{\theta} & \cos{\theta} & 0 \\ 0 & 0 & 1\end{pmatrix} R = T S_\theta T^{-1} tr(R) = tr(T S_\theta T^{-1}) = tr(T^{-1} T S_\theta) = tr(S_\theta) = 1 + 2\cos{\theta} A,B,C tr(ABC) = tr(CAB) = tr(BCA) := x tr(BAC) = tr(ACB) = tr(CBA) :=y x \ne y CAB, BCA ABC 1 + 2\cos\theta = \cos{\alpha} \cos{\beta} + \cos{\beta} \cos{\gamma} + \cos{\gamma}\cos{\alpha} + \sin{\alpha} \sin{\beta} \sin{\gamma} \alpha, \beta, \gamma \alpha, \beta, \gamma \alpha, \beta, \gamma \theta^2 \approx \alpha^2 + \beta^2 + \gamma^2 (+\alpha\beta\gamma)","['matrices', 'rotations']"
97,What is $\det (W^T A W)$ for $W$ not square?,What is  for  not square?,\det (W^T A W) W,"I am sure the answer to this question is out there, but I cannot find it, maybe because I don't know the correct torm for 'matrix multiplication from left and right'. Consider a matrix $W \in \mathbb{R}^{m \times n}$ and a square matrix $A \in \mathbb{R}^{m \times m}$ . What can we say about the determinant $$\det (W^TAW)?$$ In the case that $W$ is also square, since the determinant then commutes, we have $$\det (W^TAW) = \det (A WW^T) = \det (A) \det (WW^T) = \det(A) \det(W^TW).$$ Do we have a similar result when $W$ is not square? We can not expect the two last equalities on the left generally hold, since one of $\det (WW^T)$ and $ \det(W^TW)$ will be equal to $0$ , but maybe it holds for the respective one that might have full rank? Edit: Maybe this works indeed, using singular value decomposition: Let $W = U \Sigma V$ and assume $W$ has full rank, and assume that $m<n$ , such that $U$ is an orthogonal $m\times m$ matrix, $V$ is an orthogonal $n \times n$ matrix, and $\Sigma \in \mathbb{R}^{m\times n}$ is of the form $\begin{pmatrix}D & 0\end{pmatrix}$ with $D$ diagonal. Then $$\det (W^T AW) = \det (V^T \Sigma^T U^T A U \Sigma V) = \det ( \Sigma^T U^T A U \Sigma VV^T) = \det ( \Sigma^T U^T A U\Sigma)$$ Bot now if we denote $U^TAU=B$ , then $$\Sigma^T B \Sigma = \begin{pmatrix}D \\ 0\end{pmatrix} B \begin{pmatrix}D & 0\end{pmatrix} = DBD $$ . But then $$\det(\Sigma^T U^T A U\Sigma) = \det (\Sigma^TB\Sigma) = \det (\Sigma^T\Sigma B) = \det(D^2) \det(B) = \det(D^2) \det(U^TAU) = \det(D^2) \det(A) =\det(WW^T)\det(A).$$ Is this correct? second edit : the 'solution' above must contain an error, since we obviously have that the determinant is zero in the case that $m<n$ ...","I am sure the answer to this question is out there, but I cannot find it, maybe because I don't know the correct torm for 'matrix multiplication from left and right'. Consider a matrix and a square matrix . What can we say about the determinant In the case that is also square, since the determinant then commutes, we have Do we have a similar result when is not square? We can not expect the two last equalities on the left generally hold, since one of and will be equal to , but maybe it holds for the respective one that might have full rank? Edit: Maybe this works indeed, using singular value decomposition: Let and assume has full rank, and assume that , such that is an orthogonal matrix, is an orthogonal matrix, and is of the form with diagonal. Then Bot now if we denote , then . But then Is this correct? second edit : the 'solution' above must contain an error, since we obviously have that the determinant is zero in the case that ...",W \in \mathbb{R}^{m \times n} A \in \mathbb{R}^{m \times m} \det (W^TAW)? W \det (W^TAW) = \det (A WW^T) = \det (A) \det (WW^T) = \det(A) \det(W^TW). W \det (WW^T)  \det(W^TW) 0 W = U \Sigma V W m<n U m\times m V n \times n \Sigma \in \mathbb{R}^{m\times n} \begin{pmatrix}D & 0\end{pmatrix} D \det (W^T AW) = \det (V^T \Sigma^T U^T A U \Sigma V) = \det ( \Sigma^T U^T A U \Sigma VV^T) = \det ( \Sigma^T U^T A U\Sigma) U^TAU=B \Sigma^T B \Sigma = \begin{pmatrix}D \\ 0\end{pmatrix} B \begin{pmatrix}D & 0\end{pmatrix} = DBD  \det(\Sigma^T U^T A U\Sigma) = \det (\Sigma^TB\Sigma) = \det (\Sigma^T\Sigma B) = \det(D^2) \det(B) = \det(D^2) \det(U^TAU) = \det(D^2) \det(A) =\det(WW^T)\det(A). m<n,"['linear-algebra', 'matrices', 'determinant']"
98,"Perron Frobenius theorem for matrices with entries from $\{-1,0,1\}$",Perron Frobenius theorem for matrices with entries from,"\{-1,0,1\}","The Perron-Frobenius theorem is a well known theorem for positive symmetric matrices and irreducible non-negative matrices (it gives information about the largest eigenvalue and the existence of a positive/non-negative eigenvector corresponding to it). I am looking for some reading material on the Perron-Frobenius theory for matrices with negative entries also. Particularly, is there anything known for symmetric matrices having entries $\{-1,0,1\}$ ? EDIT: specific results on the existence of Perron like eigenvector, or if there is any nice characterization of the eigenvector(s) corresponding to the eigen value of maximum modulus, would be useful.","The Perron-Frobenius theorem is a well known theorem for positive symmetric matrices and irreducible non-negative matrices (it gives information about the largest eigenvalue and the existence of a positive/non-negative eigenvector corresponding to it). I am looking for some reading material on the Perron-Frobenius theory for matrices with negative entries also. Particularly, is there anything known for symmetric matrices having entries ? EDIT: specific results on the existence of Perron like eigenvector, or if there is any nice characterization of the eigenvector(s) corresponding to the eigen value of maximum modulus, would be useful.","\{-1,0,1\}","['linear-algebra', 'matrices', 'reference-request', 'eigenvalues-eigenvectors']"
99,best way to show tr(AB) = tr(BA) for non square A and b matrices,best way to show tr(AB) = tr(BA) for non square A and b matrices,,"I have matrices $A \in \mathbb{K}^{n \times m}$ and $B \in \mathbb{K}^{m \times n}$ What is the best way to prove that tr(AB) = tr(BA). I found a prove in Matrix Analysis by Horn and Johnson, but they only prove it if $n \leq m$ .","I have matrices and What is the best way to prove that tr(AB) = tr(BA). I found a prove in Matrix Analysis by Horn and Johnson, but they only prove it if .",A \in \mathbb{K}^{n \times m} B \in \mathbb{K}^{m \times n} n \leq m,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace']"
