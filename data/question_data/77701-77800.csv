,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Significance of the following Matrix?,Significance of the following Matrix?,,"I am unfamiliar with advanced Matrix theory (nor am I a mathematician), so please bear with me. Is there anything significant about the following Matrix structure? Are there any special symmetries or conserved quantities that can be extracted from this? $$ \pmatrix{-u_2 & 0 & -\sqrt{2} u_1 & 0 \\ 0 & u_2 & 0 & -\sqrt{2} u_1 \\ \sqrt{2} u_1 & 0 & 0 & 0 \\ 0 & \sqrt{2} u_1 & 0 & 0} $$ where $u_1,u_2$ are real and have a maximum value of $1$. Edit: I was reading something about the correspondence between the group of traceless matrices and the SU(2) group (are they isomorphic?). Am I on the right track? Anything about the blocks that stand out? I was also reading about trace conserving ""volume"", but I am not sure what that means.","I am unfamiliar with advanced Matrix theory (nor am I a mathematician), so please bear with me. Is there anything significant about the following Matrix structure? Are there any special symmetries or conserved quantities that can be extracted from this? $$ \pmatrix{-u_2 & 0 & -\sqrt{2} u_1 & 0 \\ 0 & u_2 & 0 & -\sqrt{2} u_1 \\ \sqrt{2} u_1 & 0 & 0 & 0 \\ 0 & \sqrt{2} u_1 & 0 & 0} $$ where $u_1,u_2$ are real and have a maximum value of $1$. Edit: I was reading something about the correspondence between the group of traceless matrices and the SU(2) group (are they isomorphic?). Am I on the right track? Anything about the blocks that stand out? I was also reading about trace conserving ""volume"", but I am not sure what that means.",,"['matrices', 'lie-algebras']"
1,How many geometrical interpretations does matrix multiplication have?,How many geometrical interpretations does matrix multiplication have?,,"I am wondering that what is the geometrical interpertation of matrix multiplication. And how many different ways it could be interpreted? The one obvious use is the transformations... I understand a bit in 3D but how about the n-dimensional multiplication --- what kind of transformations would be there? So, what I want to know is: 1. How can we interpret matrix multiplication? 2. In how many ways do we interpret? 3. How actually coefficients play role in transformations 4. How to interpret more than 3D matrix multiplications Thanks a lot.","I am wondering that what is the geometrical interpertation of matrix multiplication. And how many different ways it could be interpreted? The one obvious use is the transformations... I understand a bit in 3D but how about the n-dimensional multiplication --- what kind of transformations would be there? So, what I want to know is: 1. How can we interpret matrix multiplication? 2. In how many ways do we interpret? 3. How actually coefficients play role in transformations 4. How to interpret more than 3D matrix multiplications Thanks a lot.",,['matrices']
2,On matrix norm equivalence: estimates on $A \|\mathbf M\|_2 \leq \|\mathbf M \|_F \leq B\|\mathbf M\|_2$,On matrix norm equivalence: estimates on,A \|\mathbf M\|_2 \leq \|\mathbf M \|_F \leq B\|\mathbf M\|_2,"For finite dimensional spaces, all norms are equivalent, i.e. there exist constants say $A,B$ such that for all matrices from the $\mathbf M \in R^{d\times d}$ (let $d$ be a fixed positive integer), $$A \|\mathbf M\|_2 \leq \|\mathbf M \|_F \leq B\|\mathbf M\|_2\text{,} $$ where $\|\cdot\|_2$ denotes the spectral norm and $\|\cdot\|_F$ denotes the Frobenius norm. My question is now, whether you know anything specific about $A$ and $B$ , for example whether there exists an analytical expression for let's say $B$ .","For finite dimensional spaces, all norms are equivalent, i.e. there exist constants say such that for all matrices from the (let be a fixed positive integer), where denotes the spectral norm and denotes the Frobenius norm. My question is now, whether you know anything specific about and , for example whether there exists an analytical expression for let's say .","A,B \mathbf M \in R^{d\times d} d A \|\mathbf M\|_2 \leq \|\mathbf M \|_F \leq B\|\mathbf M\|_2\text{,}  \|\cdot\|_2 \|\cdot\|_F A B B","['matrices', 'normed-spaces', 'matrix-norms']"
3,Eigenvalues of anti-circulant matrix,Eigenvalues of anti-circulant matrix,,"Is there any theorem to find the eigenvalues of any anti-circulant matrix with real entries?  By anti-circulant matrix, I mean any $n\times n$ matrix  of the form : $$ \begin{pmatrix}a & b & c & d & e & f \\ b & c & d & e & f & a \\ c & d & e & f &  a & b \\ d & e & f & a & b & c \\ e & f & a & b & c & d  \\ f & a & b & c & d & e  \end{pmatrix}$$","Is there any theorem to find the eigenvalues of any anti-circulant matrix with real entries?  By anti-circulant matrix, I mean any $n\times n$ matrix  of the form : $$ \begin{pmatrix}a & b & c & d & e & f \\ b & c & d & e & f & a \\ c & d & e & f &  a & b \\ d & e & f & a & b & c \\ e & f & a & b & c & d  \\ f & a & b & c & d & e  \end{pmatrix}$$",,"['matrices', 'eigenvalues-eigenvectors']"
4,Eigenvalues of product of a matrix with a diagonal matrix,Eigenvalues of product of a matrix with a diagonal matrix,,"I have got a question and I would appreciate if one could help with it. Assume $S$ is a diagonal matrix (with diagonal entries $s_1, s_2, \cdots$) and $M$ is a positive symmetric matrix with eigen value decomposition as follows: $$\mathrm{eig}(M) = ULU^T$$ where $U^T$ means the transpose of $U$. I am trying to find out about the eigenvalues of $SM$. In other words, is there any relation between the eigenvalues of the matrix $S$, the matrix $M$ and their product $SM?$ any help/hint is appreciated","I have got a question and I would appreciate if one could help with it. Assume $S$ is a diagonal matrix (with diagonal entries $s_1, s_2, \cdots$) and $M$ is a positive symmetric matrix with eigen value decomposition as follows: $$\mathrm{eig}(M) = ULU^T$$ where $U^T$ means the transpose of $U$. I am trying to find out about the eigenvalues of $SM$. In other words, is there any relation between the eigenvalues of the matrix $S$, the matrix $M$ and their product $SM?$ any help/hint is appreciated",,"['matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition']"
5,Standard Notation for diagonal matrices,Standard Notation for diagonal matrices,,"Is there standard notation for the set of diagonal matrices? Specifically if the elements must be nonnegative, i.e. the matrix is positive semi-definite?","Is there standard notation for the set of diagonal matrices? Specifically if the elements must be nonnegative, i.e. the matrix is positive semi-definite?",,"['matrices', 'notation']"
6,Is every symmetric matrix diagonalizable?,Is every symmetric matrix diagonalizable?,,"I know that Hermitian matrices are always diagonalizable and real symmetric matrices are real Hermitian matrices and therefore diagonalizable. But, it is always not the case that a symmetric matrix is a Hermitian matrix. So my question is I think every real symmetric matrix is diagonalizable, but is it true for every symmetric matrix? Also, $1$ and $-1$ are only possible eigenvalues for real orthogonal matrix?","I know that Hermitian matrices are always diagonalizable and real symmetric matrices are real Hermitian matrices and therefore diagonalizable. But, it is always not the case that a symmetric matrix is a Hermitian matrix. So my question is I think every real symmetric matrix is diagonalizable, but is it true for every symmetric matrix? Also, $1$ and $-1$ are only possible eigenvalues for real orthogonal matrix?",,"['matrices', 'transformation']"
7,Complex function and Jacobian matrix,Complex function and Jacobian matrix,,"Given some complex-differentiable function $f:\mathbb{C}\rightarrow\mathbb{C}$ defined $f(x,y)=u(x,y)+iv(x,y)$, we know the Cauchy-Riemann equations hold, so: $$\dfrac{\partial u}{\partial x}=\dfrac{\partial v}{\partial y}\quad\textrm{and}\quad\dfrac{\partial u}{\partial y}=-\dfrac{\partial v}{\partial x}$$ Then, we can write the Jacobian for the function: $$\begin{bmatrix}\dfrac{\partial u}{\partial x}&\dfrac{\partial u}{\partial y}\\-\dfrac{\partial u}{\partial y}&\dfrac{\partial u}{\partial x}\end{bmatrix}$$ At this point, my textbook claims that this matrix has the same effect on $\mathbb{C}$ as multiplication by the complex number $a=\dfrac{\partial u}{\partial x}-i\dfrac{\partial u}{\partial y}$ (therefore, $a$ is the derivative of $f$), but I'm having a hard time seeing why that's the case, and how this value of $a$ was reached in the first place. Any suggestions?","Given some complex-differentiable function $f:\mathbb{C}\rightarrow\mathbb{C}$ defined $f(x,y)=u(x,y)+iv(x,y)$, we know the Cauchy-Riemann equations hold, so: $$\dfrac{\partial u}{\partial x}=\dfrac{\partial v}{\partial y}\quad\textrm{and}\quad\dfrac{\partial u}{\partial y}=-\dfrac{\partial v}{\partial x}$$ Then, we can write the Jacobian for the function: $$\begin{bmatrix}\dfrac{\partial u}{\partial x}&\dfrac{\partial u}{\partial y}\\-\dfrac{\partial u}{\partial y}&\dfrac{\partial u}{\partial x}\end{bmatrix}$$ At this point, my textbook claims that this matrix has the same effect on $\mathbb{C}$ as multiplication by the complex number $a=\dfrac{\partial u}{\partial x}-i\dfrac{\partial u}{\partial y}$ (therefore, $a$ is the derivative of $f$), but I'm having a hard time seeing why that's the case, and how this value of $a$ was reached in the first place. Any suggestions?",,"['matrices', 'derivatives', 'complex-numbers']"
8,Second order approximation of $\log \det X$,Second order approximation of,\log \det X,"I'm trying to follow the derivation of second order approximation of $\log \det X$ from page 658 of Boyd & Vandenberghe's Convex Optimization . How is the last step derived? I.e., where does the trace expression come from?","I'm trying to follow the derivation of second order approximation of from page 658 of Boyd & Vandenberghe's Convex Optimization . How is the last step derived? I.e., where does the trace expression come from?",\log \det X,"['matrices', 'matrix-calculus', 'hessian-matrix', 'scalar-fields']"
9,Nearest signed permutation matrix to a given matrix $A$,Nearest signed permutation matrix to a given matrix,A,"Given $A \in \mathbb{R}^{n \times n}$ , let $Q \in O(n)$ be the orthogonal matrix nearest to $A$ in the Frobenius norm, i.e., $$Q := \text{arg}\min_{M \in O(n)} \| A - M \|_{F}^2$$ It's well known that $Q = U V^{T}$ , where $A = U\Sigma V^{T}$ is the SVD of $A$ (see Orthogonal_Procrustes , Nearest orthogonal matrix ). I'm trying to solve a similar problem: $$S := \text{arg}\min_{M \in \mbox{SP}(n)} \| A - M \|_{F}^2$$ where $\mbox{SP}(n)$ is a group of signed permutation matrices . I know that in the case of permutation matrices, the problem reduces to linear sum assignment and can be solved using the Hungarian algorithm. I suspect in the signed permutation case it will reduce to some linear program. Is it possible to somehow solve this problem using SVD or the Hungarian algorithm? I would really like to avoid general LP solvers, if possible.","Given , let be the orthogonal matrix nearest to in the Frobenius norm, i.e., It's well known that , where is the SVD of (see Orthogonal_Procrustes , Nearest orthogonal matrix ). I'm trying to solve a similar problem: where is a group of signed permutation matrices . I know that in the case of permutation matrices, the problem reduces to linear sum assignment and can be solved using the Hungarian algorithm. I suspect in the signed permutation case it will reduce to some linear program. Is it possible to somehow solve this problem using SVD or the Hungarian algorithm? I would really like to avoid general LP solvers, if possible.",A \in \mathbb{R}^{n \times n} Q \in O(n) A Q := \text{arg}\min_{M \in O(n)} \| A - M \|_{F}^2 Q = U V^{T} A = U\Sigma V^{T} A S := \text{arg}\min_{M \in \mbox{SP}(n)} \| A - M \|_{F}^2 \mbox{SP}(n),"['matrices', 'optimization', 'discrete-optimization', 'permutation-matrices']"
10,Trace of a differential operator,Trace of a differential operator,,Given the differential operator: $$A=\exp(-\beta H)$$ where $$H=\frac{1}{2}\left( -\frac{d^2}{dx^2}+x^2 \right)$$ and $\beta\gt 0$ How can I get the trace of this operator? Thanks in advance.,Given the differential operator: $$A=\exp(-\beta H)$$ where $$H=\frac{1}{2}\left( -\frac{d^2}{dx^2}+x^2 \right)$$ and $\beta\gt 0$ How can I get the trace of this operator? Thanks in advance.,,"['matrices', 'operator-theory']"
11,Intersection of Algebraic Varieties.,Intersection of Algebraic Varieties.,,"Let $\mathbb K$ be  an algebraically closed field. Consider the set $M_n(\mathbb K)$ of all matrices of order $n$. Identify the set $M_n(\mathbb K)$ with the affine space $\mathbb A^{n^2}_{\mathbb K}$. The set $V_1=\{A\in M_n(\mathbb K):A=-A^T\}$ of anti-symetric matrices   is an algebraic variety of dimension $\frac{n^2-n}{2}$. The set $V_2=\{A\in M_n(\mathbb K):\det(A)=0\}$ of singular matrices is an algebraic variety of dimension $n^2-1$. My question is: The set $V_1\cap V_2$ is algebraic variety, that is, it is  irreducible as an algebraic set? If the answer is yes, what is its dimension?","Let $\mathbb K$ be  an algebraically closed field. Consider the set $M_n(\mathbb K)$ of all matrices of order $n$. Identify the set $M_n(\mathbb K)$ with the affine space $\mathbb A^{n^2}_{\mathbb K}$. The set $V_1=\{A\in M_n(\mathbb K):A=-A^T\}$ of anti-symetric matrices   is an algebraic variety of dimension $\frac{n^2-n}{2}$. The set $V_2=\{A\in M_n(\mathbb K):\det(A)=0\}$ of singular matrices is an algebraic variety of dimension $n^2-1$. My question is: The set $V_1\cap V_2$ is algebraic variety, that is, it is  irreducible as an algebraic set? If the answer is yes, what is its dimension?",,"['matrices', 'algebraic-geometry', 'commutative-algebra']"
12,Is there a basis-independent proof of Abel's identity?,Is there a basis-independent proof of Abel's identity?,,"Abel's identity states that if $X(t)$ and $A(t)$ are $n\times n$ matrix-valued functions such that $X'(t)=A(t)X(t)$, then $\frac{d}{dt}(\det X(t)) = \mathrm{tr}\,A(t) \cdot \det X(t)$. The question is whether there's a nice high-brow basis-independent way to see this. Given that you can state the problem without ever referring to matrix entries, I want prove it without ever referring to matrix entries. I'd expect the identity $\det e^A = e^{\mathrm{tr}(A)}$ to play a central role in the proof, but I have yet to come up with such an argument. You can prove this without too much trouble in a bare-hands way, but I don't see how to turn this into a basis-free proof. Suppose $X_i(t)$ is the matrix you get from $X(t)$ by taking the derivative of every entry in the $i$-th row. Then $\frac{d}{dt}(\det X(t)) = \sum_{i=1}^n \det(X_i(t))$. Using the relation $X'(t)=A(t)X(t)$ you can express the derivative of the $i$-th row of $X'(t)$ in terms of the entries of $A$ and $X$. When you do this, you find that $\det X_i(t)$ is simply $\det X(t)$ multiplied by the $(i,i)$-th entry of $A(t)$.","Abel's identity states that if $X(t)$ and $A(t)$ are $n\times n$ matrix-valued functions such that $X'(t)=A(t)X(t)$, then $\frac{d}{dt}(\det X(t)) = \mathrm{tr}\,A(t) \cdot \det X(t)$. The question is whether there's a nice high-brow basis-independent way to see this. Given that you can state the problem without ever referring to matrix entries, I want prove it without ever referring to matrix entries. I'd expect the identity $\det e^A = e^{\mathrm{tr}(A)}$ to play a central role in the proof, but I have yet to come up with such an argument. You can prove this without too much trouble in a bare-hands way, but I don't see how to turn this into a basis-free proof. Suppose $X_i(t)$ is the matrix you get from $X(t)$ by taking the derivative of every entry in the $i$-th row. Then $\frac{d}{dt}(\det X(t)) = \sum_{i=1}^n \det(X_i(t))$. Using the relation $X'(t)=A(t)X(t)$ you can express the derivative of the $i$-th row of $X'(t)$ in terms of the entries of $A$ and $X$. When you do this, you find that $\det X_i(t)$ is simply $\det X(t)$ multiplied by the $(i,i)$-th entry of $A(t)$.",,"['ordinary-differential-equations', 'matrices']"
13,Matrices with continuous indices,Matrices with continuous indices,,"I've recently come across the concept of thinking about two-variable functions as ""continuous"" matrices. Such that matrix multiplication is defined as: $$f(x,y)\times g(x,y) =\int_Df(x,u)\cdot g(u,y)\text du$$ Does this concept actually have a name? I can't seem to find many references to it. Thank you very much.","I've recently come across the concept of thinking about two-variable functions as ""continuous"" matrices. Such that matrix multiplication is defined as: $$f(x,y)\times g(x,y) =\int_Df(x,u)\cdot g(u,y)\text du$$ Does this concept actually have a name? I can't seem to find many references to it. Thank you very much.",,['matrices']
14,Matrix irreducibility. Strongly connected graph,Matrix irreducibility. Strongly connected graph,,"I have this theorem from Combinatorial Matrix Theory written by Richard A. Brualdi and others. Let $A$ be a matrix of order $n$. Then $A$ is irreducible if and only   if its digraph $D$ is strongly connected. However, a friend showed me the following example $\left[\begin{array}{ccc} 0 &1& 0\\ 0 &0 &1\\ 1 &0& 0 \end{array}\right] $ and its associated graph goes as follows: $1\to2\to3\to1$ which is strongly connected. However, the matrix turns out to be reducible (in particular I can not have a strictly positive matrix power) Any ideas whats going wrong here?","I have this theorem from Combinatorial Matrix Theory written by Richard A. Brualdi and others. Let $A$ be a matrix of order $n$. Then $A$ is irreducible if and only   if its digraph $D$ is strongly connected. However, a friend showed me the following example $\left[\begin{array}{ccc} 0 &1& 0\\ 0 &0 &1\\ 1 &0& 0 \end{array}\right] $ and its associated graph goes as follows: $1\to2\to3\to1$ which is strongly connected. However, the matrix turns out to be reducible (in particular I can not have a strictly positive matrix power) Any ideas whats going wrong here?",,"['matrices', 'graph-theory', 'algebraic-graph-theory']"
15,Inverse of an infinitely large matrix?,Inverse of an infinitely large matrix?,,"This is probably a trivial problem for some people, but I've spent quite some time on it: What is the inverse of the infinite matrix $$ \left[\begin{matrix} 0^0 & 0^1 & 0^2 & 0^3 & \ldots\\ 1^0 & 1^1 & 1^2 & 1^3 & \ldots\\ 2^0 & 2^1 & 2^2 & 2^3 & \ldots\\ 3^0 & 3^1 & 3^2 & 3^3 & \ldots\\ \vdots & \vdots & \vdots & \vdots &\ddots \end{matrix}\right] $$ (Assume that $0^0=1$ for this problem). I'm not sure if this problem has a solution or is well-defined, but if it has a solution, it would help greatly in a ton of stuff I'm doing (mostly related to generating functions and polynomial approximations). I began by taking the inverse of progressively larger square matricies, but I didn't see any clear pattern.","This is probably a trivial problem for some people, but I've spent quite some time on it: What is the inverse of the infinite matrix $$ \left[\begin{matrix} 0^0 & 0^1 & 0^2 & 0^3 & \ldots\\ 1^0 & 1^1 & 1^2 & 1^3 & \ldots\\ 2^0 & 2^1 & 2^2 & 2^3 & \ldots\\ 3^0 & 3^1 & 3^2 & 3^3 & \ldots\\ \vdots & \vdots & \vdots & \vdots &\ddots \end{matrix}\right] $$ (Assume that $0^0=1$ for this problem). I'm not sure if this problem has a solution or is well-defined, but if it has a solution, it would help greatly in a ton of stuff I'm doing (mostly related to generating functions and polynomial approximations). I began by taking the inverse of progressively larger square matricies, but I didn't see any clear pattern.",,"['analysis', 'matrices', 'functional-analysis', 'infinity']"
16,"Find $M$, where $M^7=I$ and $M\neq I$, $M$ has only 0's and 1's.","Find , where  and ,  has only 0's and 1's.",M M^7=I M\neq I M,Find a $3 \times 3 $ matrix $M$ with entries 0 and 1 only such that $M^7=I$ and $M\neq I$. This was a short question in a recent exam. I tried with permutation matrices but couldn't find $M^{odd}=I$ except for 3.,Find a $3 \times 3 $ matrix $M$ with entries 0 and 1 only such that $M^7=I$ and $M\neq I$. This was a short question in a recent exam. I tried with permutation matrices but couldn't find $M^{odd}=I$ except for 3.,,[]
17,Square root of the product of a matrix with a diagonal matrix,Square root of the product of a matrix with a diagonal matrix,,"If I have a matrix M which can be decomposed as: $M = DH$ where $D$ is a diagonal matrix and $H$ is another matrix with known positive semi-definite square root $H^{1/2}$, does this give a formula or method for finding $M^{1/2}$? Some additional details for the specific problem that I'm looking at, both D and H are positive semi-definite, Hermitian and have trace 1. In general they are non-commuting.","If I have a matrix M which can be decomposed as: $M = DH$ where $D$ is a diagonal matrix and $H$ is another matrix with known positive semi-definite square root $H^{1/2}$, does this give a formula or method for finding $M^{1/2}$? Some additional details for the specific problem that I'm looking at, both D and H are positive semi-definite, Hermitian and have trace 1. In general they are non-commuting.",,['matrices']
18,Why can powers of the adjacency matrix determine connections in the graph?,Why can powers of the adjacency matrix determine connections in the graph?,,"I am studying graph theory and I am not sure about how the power of the adjacency works. I know that the $k$ -th powers of $A$ tell us about connections in the graph, and I can read lengths between a vertex to vertex after taking powers of the adjacency matrix. My question is: why can powers of the adjacency matrix determine connections in the graph, and how many powers need to determine it?","I am studying graph theory and I am not sure about how the power of the adjacency works. I know that the -th powers of tell us about connections in the graph, and I can read lengths between a vertex to vertex after taking powers of the adjacency matrix. My question is: why can powers of the adjacency matrix determine connections in the graph, and how many powers need to determine it?",k A,"['matrices', 'discrete-mathematics', 'graph-theory', 'adjacency-matrix']"
19,Cross product of vectors as a determinant: valid matrix operation?,Cross product of vectors as a determinant: valid matrix operation?,,"""The definition of the cross product can also be represented by the   determinant of a formal matrix.""   — Wikipedia This seems like a hack to me—something of much practical use but not mathematically exact. Unless I am mistaken, It only works for vectors in 3-space A matrix with mixed scalars and vectors is not strictly legal Again, Wikipedia says, ""Matrices can be considered with much more general types of entries   than real or complex numbers. As a first step of generalization, any   field, i.e., a set where addition, subtraction, multiplication and   division operations are defined and well-behaved, may be used instead   of $\mathbb{R}$ or $\mathbb{C}$, for example rational numbers or   finite fields."" Are there fields can consist of a mix of vectors and scalars? Is there a more generalized form of vector cross products that would demonstrate why the determinant solution for 3-space has pragmatic value? Is this indeed a hack?","""The definition of the cross product can also be represented by the   determinant of a formal matrix.""   — Wikipedia This seems like a hack to me—something of much practical use but not mathematically exact. Unless I am mistaken, It only works for vectors in 3-space A matrix with mixed scalars and vectors is not strictly legal Again, Wikipedia says, ""Matrices can be considered with much more general types of entries   than real or complex numbers. As a first step of generalization, any   field, i.e., a set where addition, subtraction, multiplication and   division operations are defined and well-behaved, may be used instead   of $\mathbb{R}$ or $\mathbb{C}$, for example rational numbers or   finite fields."" Are there fields can consist of a mix of vectors and scalars? Is there a more generalized form of vector cross products that would demonstrate why the determinant solution for 3-space has pragmatic value? Is this indeed a hack?",,"['matrices', 'vector-spaces', 'field-theory', 'determinant', 'cross-product']"
20,"Have symmetric matrices for which the $(i,j)$ entry is $\min(i,j)$ earned a name?",Have symmetric matrices for which the  entry is  earned a name?,"(i,j) \min(i,j)","I am interested in symmetric matrices where the $(i,j)$ entry is $\min(i,j)$ , i.e., $$\begin{pmatrix} 1      & 1      & 1      & \dots  & 1      & 1     \\ 1      & 2      & 2      & \dots  & 2      & 2     \\ 1      & 2      & 3      & \dots  & 3      & 3     \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ 1      & 2      & 3      & \dots  & n-1    & n-1   \\ 1      & 2      & 3      & \dots  & n-1    & n    \end{pmatrix}$$ Note that these symmetric matrices are related to Lehmer matrices . Have these symmetric matrices earned a name? Motivation These matrices are interesting because they are the Gramians of binary triangular matrices, e.g., $$ \begin{bmatrix}1 & 1 & 1 & 1 & 1\\ 0 & 1 & 1 & 1 & 1\\ 0 & 0 & 1 & 1 & 1\\ 0 & 0 & 0 & 1 & 1\\ 0 & 0 & 0 & 0 & 1 \end{bmatrix}^\top \begin{bmatrix}1 & 1 & 1 & 1 & 1\\ 0 & 1 & 1 & 1 & 1\\ 0 & 0 & 1 & 1 & 1\\ 0 & 0 & 0 & 1 & 1\\ 0 & 0 & 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 & 1 & 1 & 1\\ 1 & 2 & 2 & 2 & 2\\ 1 & 2 & 3 & 3 & 3\\ 1 & 2 & 3 & 4 & 4\\ 1 & 2 & 3 & 4 & 5\end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 & 0\\ 1 & 1 & 0 & 0 & 0\\ 1 & 1 & 1 & 0 & 0\\ 1 & 1 & 1 & 1 & 0\\ 1 & 1 & 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 & 0 & 0\\ 1 & 1 & 0 & 0 & 0\\ 1 & 1 & 1 & 0 & 0\\ 1 & 1 & 1 & 1 & 0\\ 1 & 1 & 1 & 1 & 1 \end{bmatrix}^\top$$ Thus, the determinant of these symmetric matrices is $1$ . Some findings In 2002, Mario Catalani studied these matrices In 2006, Rajendra Bhatia called these $\min$ matrices In 2016, Mika Mattila & Pentti Haukkanen called the $n \times n$ matrix of this form the MIN matrix of the set $\{1,2,\dots,n\}$ In 2023, Darij Grinberg studied a generalization of these matrices. Related About positive semidefiniteness of one matrix Evaluation of a specific determinant. Form matrix and calculate its determinant Determinant of matrix with $A_{ij} = \min (i, j)$ Determinant of matrix whose $(i,j)$ entry is $\min(i,j)$ Find rank of $A$ with entries $a_{ij}=\min\{i,j\} , 1\leq i,j \leq n $. Inverse of a symmetric tridiagonal filter matrix Properties of square array of numbers min (i,j) Name of a particular matrix with $M_{ij}=t_{\min(i,j)}$? Does this type of matrix with the same diagonals have a name?","I am interested in symmetric matrices where the entry is , i.e., Note that these symmetric matrices are related to Lehmer matrices . Have these symmetric matrices earned a name? Motivation These matrices are interesting because they are the Gramians of binary triangular matrices, e.g., Thus, the determinant of these symmetric matrices is . Some findings In 2002, Mario Catalani studied these matrices In 2006, Rajendra Bhatia called these matrices In 2016, Mika Mattila & Pentti Haukkanen called the matrix of this form the MIN matrix of the set In 2023, Darij Grinberg studied a generalization of these matrices. Related About positive semidefiniteness of one matrix Evaluation of a specific determinant. Form matrix and calculate its determinant Determinant of matrix with $A_{ij} = \min (i, j)$ Determinant of matrix whose $(i,j)$ entry is $\min(i,j)$ Find rank of $A$ with entries $a_{ij}=\min\{i,j\} , 1\leq i,j \leq n $. Inverse of a symmetric tridiagonal filter matrix Properties of square array of numbers min (i,j) Name of a particular matrix with $M_{ij}=t_{\min(i,j)}$? Does this type of matrix with the same diagonals have a name?","(i,j) \min(i,j) \begin{pmatrix}
1      & 1      & 1      & \dots  & 1      & 1     \\
1      & 2      & 2      & \dots  & 2      & 2     \\
1      & 2      & 3      & \dots  & 3      & 3     \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
1      & 2      & 3      & \dots  & n-1    & n-1   \\
1      & 2      & 3      & \dots  & n-1    & n   
\end{pmatrix}  \begin{bmatrix}1 & 1 & 1 & 1 & 1\\ 0 & 1 & 1 & 1 & 1\\ 0 & 0 & 1 & 1 & 1\\ 0 & 0 & 0 & 1 & 1\\ 0 & 0 & 0 & 0 & 1 \end{bmatrix}^\top \begin{bmatrix}1 & 1 & 1 & 1 & 1\\ 0 & 1 & 1 & 1 & 1\\ 0 & 0 & 1 & 1 & 1\\ 0 & 0 & 0 & 1 & 1\\ 0 & 0 & 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 & 1 & 1 & 1\\ 1 & 2 & 2 & 2 & 2\\ 1 & 2 & 3 & 3 & 3\\ 1 & 2 & 3 & 4 & 4\\ 1 & 2 & 3 & 4 & 5\end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 & 0\\ 1 & 1 & 0 & 0 & 0\\ 1 & 1 & 1 & 0 & 0\\ 1 & 1 & 1 & 1 & 0\\ 1 & 1 & 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 & 0 & 0\\ 1 & 1 & 0 & 0 & 0\\ 1 & 1 & 1 & 0 & 0\\ 1 & 1 & 1 & 1 & 0\\ 1 & 1 & 1 & 1 & 1 \end{bmatrix}^\top 1 \min n \times n \{1,2,\dots,n\}","['matrices', 'terminology']"
21,Show that eigenvalues are symmetric with respect to the origin,Show that eigenvalues are symmetric with respect to the origin,,"The matrices that I am considering are $$ M = \begin{bmatrix} A & B \\ C & -A^\top \end{bmatrix}, $$ with $A,B,C\in\mathbb{C}^{n\times n}$ , $C = C^\top$ and $B = B^\top$ . I noticed from numerical calculations that the eigenvalues are symmetric with respect to the origin, i.e., if $\lambda$ is an eigenvalue of $M$ then $-\lambda$ is as well. I have not been able to shown that this has to be the case. Initially I tried to see if I could come up with a similarity transformation which would make it block diagonal, with one block the negative of the other, but with no success. Next I tried to use some tricks to manipulate the determinant, namely I showed that \begin{align} \det(M - \lambda\,I) &= \det(B) \det((-A^\top - \lambda\,I) B^{-1} (A - \lambda\,I) - C) \\ &= \det(C) \det((A - \lambda\,I) C^{-1} (-A^\top - \lambda\,I) - B) \\ \end{align} assuming that either $\det(B)\neq0$ or $\det(C)\neq0$ . However, this did not seem to bring me any closer to showing that all eigenvalues are mirrored around the imaginary axis.","The matrices that I am considering are with , and . I noticed from numerical calculations that the eigenvalues are symmetric with respect to the origin, i.e., if is an eigenvalue of then is as well. I have not been able to shown that this has to be the case. Initially I tried to see if I could come up with a similarity transformation which would make it block diagonal, with one block the negative of the other, but with no success. Next I tried to use some tricks to manipulate the determinant, namely I showed that assuming that either or . However, this did not seem to bring me any closer to showing that all eigenvalues are mirrored around the imaginary axis.","
M =
\begin{bmatrix}
A & B \\
C & -A^\top
\end{bmatrix},
 A,B,C\in\mathbb{C}^{n\times n} C = C^\top B = B^\top \lambda M -\lambda \begin{align}
\det(M - \lambda\,I) &= \det(B) \det((-A^\top - \lambda\,I) B^{-1} (A - \lambda\,I) - C) \\
&= \det(C) \det((A - \lambda\,I) C^{-1} (-A^\top - \lambda\,I) - B) \\
\end{align} \det(B)\neq0 \det(C)\neq0","['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
22,Is Schur complement better conditioned than the original matrix?,Is Schur complement better conditioned than the original matrix?,,"Consider the following linear system (in block form) with s.p.d. matrix: $$ \begin{pmatrix} A & B\\ B^\top & C \end{pmatrix} \begin{pmatrix} x\\y \end{pmatrix} = \begin{pmatrix} f\\g \end{pmatrix} $$ I'm wondering if elimination of some variables does improve conditioning of the system matrix. If we eliminate $y$ first, by substituting $y = C^{-1}g - C^{-1}B^\top x$ the following system is obtained: $$ (A - BC^{-1}B^\top) x = f - BC^{-1}g. $$ The matrix of the new system is simply the Schur complement of the block $C$. The question is whether the conditioning number of the resulting system is less than the conditioning number of the original one? The case $C = I$ is particularly interesting. I've tried using formula $$ 0 = \det \begin{pmatrix} A - \lambda I& B\\ B^\top & C - \lambda I \end{pmatrix} =  \det(C - \lambda I) \det(A - \lambda I - B (C - \lambda I)^{-1}B^\top), $$ but with no luck, though $A - B (C - \lambda I)^{-1}B^\top$ seems to be quite close to $A - BC^{-1}B^\top$. Numerical experiments show that the Schur complment is always better conditioned than the original matrix, here's my code . Experiments also show that not only s.p.d, but also diagonally dominant M-matrices share this property.","Consider the following linear system (in block form) with s.p.d. matrix: $$ \begin{pmatrix} A & B\\ B^\top & C \end{pmatrix} \begin{pmatrix} x\\y \end{pmatrix} = \begin{pmatrix} f\\g \end{pmatrix} $$ I'm wondering if elimination of some variables does improve conditioning of the system matrix. If we eliminate $y$ first, by substituting $y = C^{-1}g - C^{-1}B^\top x$ the following system is obtained: $$ (A - BC^{-1}B^\top) x = f - BC^{-1}g. $$ The matrix of the new system is simply the Schur complement of the block $C$. The question is whether the conditioning number of the resulting system is less than the conditioning number of the original one? The case $C = I$ is particularly interesting. I've tried using formula $$ 0 = \det \begin{pmatrix} A - \lambda I& B\\ B^\top & C - \lambda I \end{pmatrix} =  \det(C - \lambda I) \det(A - \lambda I - B (C - \lambda I)^{-1}B^\top), $$ but with no luck, though $A - B (C - \lambda I)^{-1}B^\top$ seems to be quite close to $A - BC^{-1}B^\top$. Numerical experiments show that the Schur complment is always better conditioned than the original matrix, here's my code . Experiments also show that not only s.p.d, but also diagonally dominant M-matrices share this property.",,"['matrices', 'block-matrices', 'condition-number', 'schur-complement']"
23,A bound on $e^{A+B}$ for noncommutative $A$ and $B$,A bound on  for noncommutative  and,e^{A+B} A B,"It is folklore that when two complex $n\times n$ matrices $A$ and $B$ commute, the equality $e^{A+B}=e^Ae^B$ holds.  An elementary consequence of this is that for any submultiplicative norm, commuting matrices satisfy the inequality $$||e^{A+B}||\leq ||e^A||\cdot||e^B||.$$ Earlier this week, I was trying to prove a result (with $n=2$) that would have benefitted from the above inequality. However, my matrices did not commute. Testing a few thousand pairs of matrices in GNU Octave, not a single one failed to pass the test, which is reasonable evidence to suggest that the above inequality might hold generally or, if it fails, one might need to be clever in finding a counterexample. I could not find a counterexample and did not get very far in proving it. While I have moved on a proved the result without the inequality, it's still bothering me. Could somebody shed some light on this?","It is folklore that when two complex $n\times n$ matrices $A$ and $B$ commute, the equality $e^{A+B}=e^Ae^B$ holds.  An elementary consequence of this is that for any submultiplicative norm, commuting matrices satisfy the inequality $$||e^{A+B}||\leq ||e^A||\cdot||e^B||.$$ Earlier this week, I was trying to prove a result (with $n=2$) that would have benefitted from the above inequality. However, my matrices did not commute. Testing a few thousand pairs of matrices in GNU Octave, not a single one failed to pass the test, which is reasonable evidence to suggest that the above inequality might hold generally or, if it fails, one might need to be clever in finding a counterexample. I could not find a counterexample and did not get very far in proving it. While I have moved on a proved the result without the inequality, it's still bothering me. Could somebody shed some light on this?",,"['matrices', 'analysis']"
24,Rank of a lower triangular block matrix,Rank of a lower triangular block matrix,,"For $$A= \begin{bmatrix}B&0\\C&D\end{bmatrix}$$ where $B, C, D$ are matrices that may be rectangular, is it true or false that $$\text{rank}(A)=\text{rank}(B)+\text{rank}(D)$$ I think that if $C=0$ this is true since the rank of A is the number of linear independent columns, which is the number of linear independent columns of B and of D, but does C affect this relationship? Or is it still true that $\text{rank}(A)=\text{rank}(B)+\text{rank}(D)$ when C is nonzero?","For $$A= \begin{bmatrix}B&0\\C&D\end{bmatrix}$$ where $B, C, D$ are matrices that may be rectangular, is it true or false that $$\text{rank}(A)=\text{rank}(B)+\text{rank}(D)$$ I think that if $C=0$ this is true since the rank of A is the number of linear independent columns, which is the number of linear independent columns of B and of D, but does C affect this relationship? Or is it still true that $\text{rank}(A)=\text{rank}(B)+\text{rank}(D)$ when C is nonzero?",,"['matrices', 'matrix-rank']"
25,Geometric interpretation of complex eigenvalues,Geometric interpretation of complex eigenvalues,,What is the geometric interpretation of complex eigenvalues? For me it is clear that real eigenvalues of a matrix $A$ are associates to eigenvectors along which the matrix $A$ contracts or expands. Complex eigenvalues are associated intuitively (but not clearly) to me to eigenvectors along which the matrix $A$ rotates the space.,What is the geometric interpretation of complex eigenvalues? For me it is clear that real eigenvalues of a matrix $A$ are associates to eigenvectors along which the matrix $A$ contracts or expands. Complex eigenvalues are associated intuitively (but not clearly) to me to eigenvectors along which the matrix $A$ rotates the space.,,['matrices']
26,"If $A^3=I$ for a real matrix, is $A$ normal/orthogonal?","If  for a real matrix, is  normal/orthogonal?",A^3=I A,"I read earlier that if $A$ is a real $3\times 3$ matrix satsifying $A^3=I$, then $A$ is similar to a matrix of form $$ \begin{bmatrix} 1 & 0 & 0\\ 0 & \cos\theta & -\sin\theta\\ 0 & \sin\theta & \cos\theta \end{bmatrix} $$ From the structure theorems for normal operators, I know that among the real normal operators, the othogonal operators are exactly those matrices which are block diagonal with their eigenvalues and $2\times 2$ blocks of the above form. So I suppose $A$ must be normal/orthogonal? Is there a way to deduce that at all from just knowing that $A^3=I$ without appealing to the structure theorem in hindsight?","I read earlier that if $A$ is a real $3\times 3$ matrix satsifying $A^3=I$, then $A$ is similar to a matrix of form $$ \begin{bmatrix} 1 & 0 & 0\\ 0 & \cos\theta & -\sin\theta\\ 0 & \sin\theta & \cos\theta \end{bmatrix} $$ From the structure theorems for normal operators, I know that among the real normal operators, the othogonal operators are exactly those matrices which are block diagonal with their eigenvalues and $2\times 2$ blocks of the above form. So I suppose $A$ must be normal/orthogonal? Is there a way to deduce that at all from just knowing that $A^3=I$ without appealing to the structure theorem in hindsight?",,['matrices']
27,Name of a special matrix,Name of a special matrix,,"I have a matrix which is kind of symmetrical with the other diagonal, i.e., something like $$A = \left[ \begin{array}{c c c c}    a & b & c & d \\    e & f & g & c \\    h & i & f & b \\    j & h & e & a  \end{array} \right]$$ Does this matrix have a special name in literature? What are it's properties? And a matrix that is symmetrical by both diagonals $$A = \left[ \begin{array}{c c c c}    a & b & c & d \\    b & e & f & c \\    c & f & e & b \\    d & c & b & a  \end{array} \right]$$ What's the name of it? Any interesting properties?","I have a matrix which is kind of symmetrical with the other diagonal, i.e., something like $$A = \left[ \begin{array}{c c c c}    a & b & c & d \\    e & f & g & c \\    h & i & f & b \\    j & h & e & a  \end{array} \right]$$ Does this matrix have a special name in literature? What are it's properties? And a matrix that is symmetrical by both diagonals $$A = \left[ \begin{array}{c c c c}    a & b & c & d \\    b & e & f & c \\    c & f & e & b \\    d & c & b & a  \end{array} \right]$$ What's the name of it? Any interesting properties?",,"['matrices', 'terminology']"
28,Projection of a matrix onto the spectral norm ball,Projection of a matrix onto the spectral norm ball,,"Given a matrix $X$ with at least one singular value greater than $1$ , $$\begin{array}{ll} \underset{Z}{\text{minimize}} & \langle X - Z, X - Z \rangle\\ \text{subject to} & \| Z \| := \max_i \sigma_i (Z) \le 1\end{array}$$ where $\langle A,B \rangle := \mathrm{tr}(AB^\top)$ . I'm tempted to just replace the singular values of $X$ that are greater than $1$ with exactly one, but I can't prove that that's correct.  It's certainly right for the equivalent vector case of projection onto the $\ell_\infty$ -ball.","Given a matrix with at least one singular value greater than , where . I'm tempted to just replace the singular values of that are greater than with exactly one, but I can't prove that that's correct.  It's certainly right for the equivalent vector case of projection onto the -ball.","X 1 \begin{array}{ll} \underset{Z}{\text{minimize}} & \langle X - Z, X - Z \rangle\\ \text{subject to} & \| Z \| := \max_i \sigma_i (Z) \le 1\end{array} \langle A,B \rangle := \mathrm{tr}(AB^\top) X 1 \ell_\infty","['matrices', 'optimization', 'convex-optimization', 'projection', 'spectral-norm']"
29,Show that the representation $\mathbb Z\ni a\mapsto\begin{pmatrix}1& a\\0&1\end{pmatrix}$ is not completely reducible,Show that the representation  is not completely reducible,\mathbb Z\ni a\mapsto\begin{pmatrix}1& a\\0&1\end{pmatrix},"Let $\rho : \mathbb Z \to \mathrm{GL}_2(\mathbb C)$ be the representation defined by $\rho(1) = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$ . I'd like to show that $\rho$ is not completely reducible. I have one preliminary question (which is probably a silly one) - for what vector space $V$ is $\mathrm{GL}(V) \cong \mathrm{GL}_2(\mathbb C)$ ? Firstly, I noted that $\rho(1)$ has an eigenvector, so the representation is not irreducible. So if it were completely reducible, it would have to break up as a direct sum of two $1$ -dimensional sub representations. But a 1-dimensional subrep is given by an eigenvector - but $\rho$ only has one eigenvalue, which has a $1$ -dimensional eigenspace. So this can't happen. Is this reasoning OK? Once I've shown that the representation isn't irreducible, the problem is equivalent to showing that $\rho(1)$ cannot be diagonalised (which I've done by showing that the sum of the dimensions of the eigenspaces is $1$ , not 2). Depending on the answer to question (1), I could have reduced (excuse the pun) the amount of work by considering Jordan Normal Form ( $\rho(1)$ is in JNF but isn't diagonal, so isn't diagonalisable).","Let be the representation defined by . I'd like to show that is not completely reducible. I have one preliminary question (which is probably a silly one) - for what vector space is ? Firstly, I noted that has an eigenvector, so the representation is not irreducible. So if it were completely reducible, it would have to break up as a direct sum of two -dimensional sub representations. But a 1-dimensional subrep is given by an eigenvector - but only has one eigenvalue, which has a -dimensional eigenspace. So this can't happen. Is this reasoning OK? Once I've shown that the representation isn't irreducible, the problem is equivalent to showing that cannot be diagonalised (which I've done by showing that the sum of the dimensions of the eigenspaces is , not 2). Depending on the answer to question (1), I could have reduced (excuse the pun) the amount of work by considering Jordan Normal Form ( is in JNF but isn't diagonal, so isn't diagonalisable).",\rho : \mathbb Z \to \mathrm{GL}_2(\mathbb C) \rho(1) = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix} \rho V \mathrm{GL}(V) \cong \mathrm{GL}_2(\mathbb C) \rho(1) 1 \rho 1 \rho(1) 1 \rho(1),"['matrices', 'finite-groups', 'representation-theory']"
30,"If $Ax = Bx$ for all $x \in C^{n}$, then $A = B$.","If  for all , then .",Ax = Bx x \in C^{n} A = B,"Let $A$ and $B$ are $n\times n$ matrices and $x \in C^{n}$ . If $Ax = Bx$ for all $x$ then $A = B$ . To prove this I have selected $x$ from Euclidean basis B = { $e_{1},e_{2},...,e_{n}$ }. Then $Ae_{i} = Be_{i}$ implies $i^{th}$ column of A = $i^{th}$ column of B for all $1 \leq i \leq n$ . Hence $A = B.$ Is this proof complete?",Let and are matrices and . If for all then . To prove this I have selected from Euclidean basis B = { }. Then implies column of A = column of B for all . Hence Is this proof complete?,"A B n\times n x \in C^{n} Ax = Bx x A = B x e_{1},e_{2},...,e_{n} Ae_{i} = Be_{i} i^{th} i^{th} 1 \leq i \leq n A = B.",['matrices']
31,"If number of zeros is increased in below diagonal entries of $A$, the largest eigenvalue of $B$ decreases. Is it right?","If number of zeros is increased in below diagonal entries of , the largest eigenvalue of  decreases. Is it right?",A B,"Let $A_i$ be a $n\times n$ lower triangular matrix with diagonal entries $1$ and below diagonal entries are $0$ or $1$. Let O be the zero matrix of order $n$. Consider $$B_i=\left[ {\begin{array}{cc} \text{O} & A_i \\ A_i^T & \text{O} \end{array} } \right] \text{    };i=1,2$$  Suppose $A_1$ has $a_1$ number of zero entries below diagonal and $A_2$ has $a_2$ number of zero entries below diagonal. I observed that when $a_2>a_1$, the largest eigenvalue of $B_2$ is less than that of $B_1$. Is it right? if so, any hint to prove it?","Let $A_i$ be a $n\times n$ lower triangular matrix with diagonal entries $1$ and below diagonal entries are $0$ or $1$. Let O be the zero matrix of order $n$. Consider $$B_i=\left[ {\begin{array}{cc} \text{O} & A_i \\ A_i^T & \text{O} \end{array} } \right] \text{    };i=1,2$$  Suppose $A_1$ has $a_1$ number of zero entries below diagonal and $A_2$ has $a_2$ number of zero entries below diagonal. I observed that when $a_2>a_1$, the largest eigenvalue of $B_2$ is less than that of $B_1$. Is it right? if so, any hint to prove it?",,['matrices']
32,Sinkhorn theorem for doubly stochastic matrices,Sinkhorn theorem for doubly stochastic matrices,,"I was reading something about doubly stochastic matrices and got stuck while reading the original proof of the uniqueness part of the Sinkhorn theorem . I'm not able to understand the logic. Could someone help me in figuring it out? I state below the theorem and the proof as written in the original paper. Theorem: To a given strictly positive $N \times N$ matrix $A$ corresponds exactly one doubly stochastic matrix $T=D_1AD_2$ where $D_1$ and $D_2$ are diagonal matrices with positive diagonals. The matrices $D_1$ and $D_2$ are themselves unique up to a scalar factor. Uniqueness proof: if there exist two different pairs of diagonal matrices $C_1, C_2$ and $D_1, D_2$ such that both $C_1 A C_2$ and $D_1 A D_2$ are doubly stochastic, then this means that there exist a positive doubly stochastic matrix $P$ and matrices $$B_1 = \mbox{diag}[b_{11}, b_{12},\dots, b_{1N}]$$ and $$B_2 = \mbox{diag}[b_{21}, b_{22}, \dots, b_{2N}]$$ which are not multiple of identity matrix, for which $B_1 P B_2$ is also doubly stochastic. But this is impossible since by convexity one obtains: $$\min_jb_{2j} \le \frac{1}{b_{1i}} \le \max_j b_{2j}$$ and $$\min_ib_{1j}\le \frac{1}{b_{2j}} \le \max_jb_{1i}$$ which leads to a contradiction if $b_{1i} b_{2j} \ne 1$ for some $i$ and $j$. It follows that $C_1 = p D_1$, $C_2 = p^{-1} D_2$ for some $p > 0$. What I do not understand: why from the existence of $C_1,C_2$ and $D_1,D_2$ it will exist $P$, $B_1$ and $B_2$? What is the convexity argument? And why there is a contradiction? And how does it imply the thesis? Thanks.","I was reading something about doubly stochastic matrices and got stuck while reading the original proof of the uniqueness part of the Sinkhorn theorem . I'm not able to understand the logic. Could someone help me in figuring it out? I state below the theorem and the proof as written in the original paper. Theorem: To a given strictly positive $N \times N$ matrix $A$ corresponds exactly one doubly stochastic matrix $T=D_1AD_2$ where $D_1$ and $D_2$ are diagonal matrices with positive diagonals. The matrices $D_1$ and $D_2$ are themselves unique up to a scalar factor. Uniqueness proof: if there exist two different pairs of diagonal matrices $C_1, C_2$ and $D_1, D_2$ such that both $C_1 A C_2$ and $D_1 A D_2$ are doubly stochastic, then this means that there exist a positive doubly stochastic matrix $P$ and matrices $$B_1 = \mbox{diag}[b_{11}, b_{12},\dots, b_{1N}]$$ and $$B_2 = \mbox{diag}[b_{21}, b_{22}, \dots, b_{2N}]$$ which are not multiple of identity matrix, for which $B_1 P B_2$ is also doubly stochastic. But this is impossible since by convexity one obtains: $$\min_jb_{2j} \le \frac{1}{b_{1i}} \le \max_j b_{2j}$$ and $$\min_ib_{1j}\le \frac{1}{b_{2j}} \le \max_jb_{1i}$$ which leads to a contradiction if $b_{1i} b_{2j} \ne 1$ for some $i$ and $j$. It follows that $C_1 = p D_1$, $C_2 = p^{-1} D_2$ for some $p > 0$. What I do not understand: why from the existence of $C_1,C_2$ and $D_1,D_2$ it will exist $P$, $B_1$ and $B_2$? What is the convexity argument? And why there is a contradiction? And how does it imply the thesis? Thanks.",,"['matrices', 'stochastic-matrices']"
33,Can the determinant of an integer matrix with a given row be any multiple of the gcd of that row?,Can the determinant of an integer matrix with a given row be any multiple of the gcd of that row?,,"Let $n\geq2$ be an integer and let $a_1,\ldots,a_n\in\mathbb Z$ with $\gcd(a_1,\ldots,a_n)=1$. Does the equation   $$\begin{vmatrix}a_1&\cdots&a_n\\x_{11}&\cdots&x_{1n}\\x_{21}&\cdots&x_{2n}\\\vdots&\ddots&\vdots\\x_{n-1,1}&\cdots&x_{n-1,n}\end{vmatrix}=1$$   always have an integer solution for the $x_{kl}$'s? Motivation Consider the following question: Let $(0,0)\neq(a,b)\in\mathbb Z^2$. What is the minimal area of a triangle whose vertices have coordinates $(0,0)$, $(a,b)$ and $(x,y)\in\mathbb Z^2$? It is easily seen to be $\frac12\gcd(a,b)$, by noting that the area is given by $$\frac12\left|\begin{vmatrix}a&b\\x&y\end{vmatrix}\right|=\frac12|ay-bx|$$ and Bézout's theorem . Note that this is the case $n=2$. We also have: Let $(0,0,0)\neq(a_1,a_2,a_3)\in\mathbb Z^3$. The minimal volume of a tetrahedron whose vertices have integer coordinates $(0,0,0)$, $(a_1,a_2,a_3)$, $(x_1,x_2,x_3)$ and $(y_1,y_2,y_3)$ is $\frac16\gcd(a_1,a_2,a_3)$. The proof is a bit more tedious: ( source (in Dutch) ) The volume is given by $\frac16\left|\begin{vmatrix}a_1&a_2&a_3\\x_1&x_2&x_3\\y_1&y_2&y_3\end{vmatrix}\right|=\frac16\left|a_1\begin{vmatrix}x_2&x_3\\y_2&y_3\end{vmatrix}+a_2\begin{vmatrix}x_1&x_3\\y_1&y_3\end{vmatrix}+a_3\begin{vmatrix}x_1&x_2\\y_1&y_2\end{vmatrix}\right|$. So it suffices to show that $\left(\begin{vmatrix}x_2&x_3\\y_2&y_3\end{vmatrix},\begin{vmatrix}x_1&x_3\\y_1&y_3\end{vmatrix},\begin{vmatrix}x_1&x_2\\y_1&y_2\end{vmatrix}\right)$ can take any value in $\mathbb Z^3$. Let $(u,v,w)\in\mathbb Z^3$. If $\gcd(v,w)=d>1$, we can solve for $(u,\frac vd,\frac wd)$ and multiply $x_1,y_1$ by $d$ to get a solution for $(u,v,w)$. Assume $\gcd(v,w)=1$. Now choose $x_1=y_1=1$, $x_2,x_3$ such that $x_2v-x_3w=u$ and $y_2=x_2+w$, $y_3=x_3+v$ to get $\left(\begin{vmatrix}x_2&x_3\\y_2&y_3\end{vmatrix},\begin{vmatrix}x_1&x_3\\y_1&y_3\end{vmatrix},\begin{vmatrix}x_1&x_2\\y_1&y_2\end{vmatrix}\right)=(u,v,w)$. This is what made me think that these observations might generalise to higher dimensions...","Let $n\geq2$ be an integer and let $a_1,\ldots,a_n\in\mathbb Z$ with $\gcd(a_1,\ldots,a_n)=1$. Does the equation   $$\begin{vmatrix}a_1&\cdots&a_n\\x_{11}&\cdots&x_{1n}\\x_{21}&\cdots&x_{2n}\\\vdots&\ddots&\vdots\\x_{n-1,1}&\cdots&x_{n-1,n}\end{vmatrix}=1$$   always have an integer solution for the $x_{kl}$'s? Motivation Consider the following question: Let $(0,0)\neq(a,b)\in\mathbb Z^2$. What is the minimal area of a triangle whose vertices have coordinates $(0,0)$, $(a,b)$ and $(x,y)\in\mathbb Z^2$? It is easily seen to be $\frac12\gcd(a,b)$, by noting that the area is given by $$\frac12\left|\begin{vmatrix}a&b\\x&y\end{vmatrix}\right|=\frac12|ay-bx|$$ and Bézout's theorem . Note that this is the case $n=2$. We also have: Let $(0,0,0)\neq(a_1,a_2,a_3)\in\mathbb Z^3$. The minimal volume of a tetrahedron whose vertices have integer coordinates $(0,0,0)$, $(a_1,a_2,a_3)$, $(x_1,x_2,x_3)$ and $(y_1,y_2,y_3)$ is $\frac16\gcd(a_1,a_2,a_3)$. The proof is a bit more tedious: ( source (in Dutch) ) The volume is given by $\frac16\left|\begin{vmatrix}a_1&a_2&a_3\\x_1&x_2&x_3\\y_1&y_2&y_3\end{vmatrix}\right|=\frac16\left|a_1\begin{vmatrix}x_2&x_3\\y_2&y_3\end{vmatrix}+a_2\begin{vmatrix}x_1&x_3\\y_1&y_3\end{vmatrix}+a_3\begin{vmatrix}x_1&x_2\\y_1&y_2\end{vmatrix}\right|$. So it suffices to show that $\left(\begin{vmatrix}x_2&x_3\\y_2&y_3\end{vmatrix},\begin{vmatrix}x_1&x_3\\y_1&y_3\end{vmatrix},\begin{vmatrix}x_1&x_2\\y_1&y_2\end{vmatrix}\right)$ can take any value in $\mathbb Z^3$. Let $(u,v,w)\in\mathbb Z^3$. If $\gcd(v,w)=d>1$, we can solve for $(u,\frac vd,\frac wd)$ and multiply $x_1,y_1$ by $d$ to get a solution for $(u,v,w)$. Assume $\gcd(v,w)=1$. Now choose $x_1=y_1=1$, $x_2,x_3$ such that $x_2v-x_3w=u$ and $y_2=x_2+w$, $y_3=x_3+v$ to get $\left(\begin{vmatrix}x_2&x_3\\y_2&y_3\end{vmatrix},\begin{vmatrix}x_1&x_3\\y_1&y_3\end{vmatrix},\begin{vmatrix}x_1&x_2\\y_1&y_2\end{vmatrix}\right)=(u,v,w)$. This is what made me think that these observations might generalise to higher dimensions...",,"['matrices', 'number-theory', 'diophantine-equations', 'determinant', 'gcd-and-lcm']"
34,Is $GL(n;\mathbb{C})$ algebraic or not?,Is  algebraic or not?,GL(n;\mathbb{C}),"The set of $n\times n$ matrices can be identified with $\mathbb{C}^{n^2}$. 1) Consider the subset $V$ of the affine space $\mathbb{A}^{n^2+1}$ (note plus one) given by $$V:=\{(x_{ij},t): \det(x_{ij})\cdot t-1=0\}$$ This is an algebraic subset, indeed $V=V(f)$ where $f=\det(x_{ij})\cdot t-1\in\mathbb{C}[x_{ij},t]$. Then consider the map $\phi:GL(n;\mathbb{C})\longrightarrow V$ sending the matrix $(a_{ij})$ to the $(n^2+1)$-tuple $(a_{ij},\frac{1}{\det(a_{ij})})$. This is a bijection, making $GL(n;\mathbb{C})$ also algebraic . 2) $GL(n;\mathbb{C})$ is the complement of the algebraic variety in $\mathbb{C}^{n^2}$ defined by the vanishing of the determinant polynomial, and so is open in the Euclidean topology. Since every affine algebraic variety in $\mathbb{C}^n$ is closed in the Euclidean topology, we can conclude that the general linear group is not an algebraic variety. So what should I argue from the above reasoning? 1) $GL(n;\mathbb{C})$ is algebraic 2) $GL(n;\mathbb{C})$ is not algebraic 3) It is a common fact that some subsets can be algebraic in one affine space and not algebraic in an affine space of different dimension, so both 1) and 2) can be correct.","The set of $n\times n$ matrices can be identified with $\mathbb{C}^{n^2}$. 1) Consider the subset $V$ of the affine space $\mathbb{A}^{n^2+1}$ (note plus one) given by $$V:=\{(x_{ij},t): \det(x_{ij})\cdot t-1=0\}$$ This is an algebraic subset, indeed $V=V(f)$ where $f=\det(x_{ij})\cdot t-1\in\mathbb{C}[x_{ij},t]$. Then consider the map $\phi:GL(n;\mathbb{C})\longrightarrow V$ sending the matrix $(a_{ij})$ to the $(n^2+1)$-tuple $(a_{ij},\frac{1}{\det(a_{ij})})$. This is a bijection, making $GL(n;\mathbb{C})$ also algebraic . 2) $GL(n;\mathbb{C})$ is the complement of the algebraic variety in $\mathbb{C}^{n^2}$ defined by the vanishing of the determinant polynomial, and so is open in the Euclidean topology. Since every affine algebraic variety in $\mathbb{C}^n$ is closed in the Euclidean topology, we can conclude that the general linear group is not an algebraic variety. So what should I argue from the above reasoning? 1) $GL(n;\mathbb{C})$ is algebraic 2) $GL(n;\mathbb{C})$ is not algebraic 3) It is a common fact that some subsets can be algebraic in one affine space and not algebraic in an affine space of different dimension, so both 1) and 2) can be correct.",,"['matrices', 'algebraic-geometry']"
35,Obtaining the Airy kernel from the Christoffel-Darboux formula with asymptotic Hermite polynomials,Obtaining the Airy kernel from the Christoffel-Darboux formula with asymptotic Hermite polynomials,,"Let the Kernel associated to a family of orthogonal polynomial $p_n(x)$ with weight $w(x)$ be defined as $$K_N(x,y):=\frac{\sqrt{w(x)w(y)}}{\int w(x) p_{N-1}(x)p_{N-1}(x)dx} \frac{p_N(x)p_{N-1}(y)-p_{N-1}(x) p_N(y)}{x-y} $$ (this is known as the Christoffel-Darboux formula) I'm trying to reproduce the result of Forrester (p.723 eq.3.6) . Basically, using the polynomials $$p_n(x):=2^{-n}H_n(x)\\w(x):=e^{-x^2}$$ and the asymptotic expansion $$ e^{-x^2/2}H_n(x) \sim\pi^{1/4}2^{n/2+1/4} \Gamma(n-1)^{1/2} n^{-1/12} \left(\pi \text{Ai}(t)+\mathcal{O}(n^{-2/3})\right),$$ with $t=\sqrt{2}n^{1/6}(x-\sqrt{2n})$ one must find an expression for $K_N(x,y)$, and evaluate the limit $$K(X,Y):=\lim_{N\to\infty} \frac{1}{2^{1/2}N^{1/6}} K_N\left(\sqrt{2N}+\frac{X}{2^{1/2}N^{1/6}},\sqrt{2N}+\frac{Y}{2^{1/2}N^{1/6}}\right)$$ to prove that $$K(X,Y)=\frac{Ai(X)Ai'(Y) - Ai(Y)Ai'(X)}{X-Y}.$$ So far I can easily show that the norm of the $N-1$ polynomial is  $2^{-N-1}\Gamma(N)\sqrt{\pi}$ and thus that   $$ K_N(x,y) = \frac{e^{-x^2/2}e^{-y^2/2}}{2^N\Gamma(N)\sqrt{\pi}}\left(\frac{H_N(x)H_{N-1}(y)-H_{N-1}(X)H_N(Y)}{x-y}\right)$$   which can then be related to the asymptotic extension. However I have no idea of how to relate $H_n$ and $H_{n-1}$ in a way that yields a derivative of the Airy function... even in the limiting case $N\to\infty$. Any hint appreciated.","Let the Kernel associated to a family of orthogonal polynomial $p_n(x)$ with weight $w(x)$ be defined as $$K_N(x,y):=\frac{\sqrt{w(x)w(y)}}{\int w(x) p_{N-1}(x)p_{N-1}(x)dx} \frac{p_N(x)p_{N-1}(y)-p_{N-1}(x) p_N(y)}{x-y} $$ (this is known as the Christoffel-Darboux formula) I'm trying to reproduce the result of Forrester (p.723 eq.3.6) . Basically, using the polynomials $$p_n(x):=2^{-n}H_n(x)\\w(x):=e^{-x^2}$$ and the asymptotic expansion $$ e^{-x^2/2}H_n(x) \sim\pi^{1/4}2^{n/2+1/4} \Gamma(n-1)^{1/2} n^{-1/12} \left(\pi \text{Ai}(t)+\mathcal{O}(n^{-2/3})\right),$$ with $t=\sqrt{2}n^{1/6}(x-\sqrt{2n})$ one must find an expression for $K_N(x,y)$, and evaluate the limit $$K(X,Y):=\lim_{N\to\infty} \frac{1}{2^{1/2}N^{1/6}} K_N\left(\sqrt{2N}+\frac{X}{2^{1/2}N^{1/6}},\sqrt{2N}+\frac{Y}{2^{1/2}N^{1/6}}\right)$$ to prove that $$K(X,Y)=\frac{Ai(X)Ai'(Y) - Ai(Y)Ai'(X)}{X-Y}.$$ So far I can easily show that the norm of the $N-1$ polynomial is  $2^{-N-1}\Gamma(N)\sqrt{\pi}$ and thus that   $$ K_N(x,y) = \frac{e^{-x^2/2}e^{-y^2/2}}{2^N\Gamma(N)\sqrt{\pi}}\left(\frac{H_N(x)H_{N-1}(y)-H_{N-1}(X)H_N(Y)}{x-y}\right)$$   which can then be related to the asymptotic extension. However I have no idea of how to relate $H_n$ and $H_{n-1}$ in a way that yields a derivative of the Airy function... even in the limiting case $N\to\infty$. Any hint appreciated.",,"['matrices', 'asymptotics', 'orthogonal-polynomials', 'random-matrices']"
36,Questions on fractional Laplacian graph spectra,Questions on fractional Laplacian graph spectra,,"Both the signed ($D-A$) and unsigned ($D+A$) Laplacian are of interest in spectral graph theory, see eg Cvetkovic: Bibliography on the signless Laplacian eigenvalues: first one hundred references . Considering the spectral function $D+\rho A$ over the interval $\rho \in [-1,1]$ rather than just the extreme values $\rho={-1,1}$, results in the curves $\lambda_i(\rho)$. For example, the following fractional spectra are are shown as point interpolations (as opposed to curve following, as in bifurcation theory) corresponding to some random Bernoulli graphs with increasing connectivity: These experimental results raise basic questions: If graphs are $M$-cospectral on $\rho = {-1,1}$ are they cospectral for $\rho \in [-1,1]$? Are there no intersections of $\lambda_i(\rho)$ outside $\rho \in [-1,1]$? Does $\lim_{\lambda_i \to \infty}\frac{d\lambda_i}{d\rho} = const?$ What's the combinatorial interpretation of intersections in $\rho \in [-1,1]$? Feel free to add your own conjectures.","Both the signed ($D-A$) and unsigned ($D+A$) Laplacian are of interest in spectral graph theory, see eg Cvetkovic: Bibliography on the signless Laplacian eigenvalues: first one hundred references . Considering the spectral function $D+\rho A$ over the interval $\rho \in [-1,1]$ rather than just the extreme values $\rho={-1,1}$, results in the curves $\lambda_i(\rho)$. For example, the following fractional spectra are are shown as point interpolations (as opposed to curve following, as in bifurcation theory) corresponding to some random Bernoulli graphs with increasing connectivity: These experimental results raise basic questions: If graphs are $M$-cospectral on $\rho = {-1,1}$ are they cospectral for $\rho \in [-1,1]$? Are there no intersections of $\lambda_i(\rho)$ outside $\rho \in [-1,1]$? Does $\lim_{\lambda_i \to \infty}\frac{d\lambda_i}{d\rho} = const?$ What's the combinatorial interpretation of intersections in $\rho \in [-1,1]$? Feel free to add your own conjectures.",,"['matrices', 'graph-theory', 'spectral-graph-theory', 'graph-laplacian']"
37,How to find a 2D basis within a 3D plane - direct matrix method?,How to find a 2D basis within a 3D plane - direct matrix method?,,"I have a plane equation in 3D, in the form $Ax+By+Cz+D=0$ (or equivalently, $\textbf{x}\cdot\textbf{n} = \textbf{a}\cdot\textbf{n}$), where $\textbf{n}=\left[A\:B\:C\right]^T$ is the plane normal, and $D=-\textbf{a}\cdot\textbf{n}$ with $\textbf{a}$ as a point in the plane, hence $D$ is negative perpendicular distance from the plane to the origin. I have some points in 3D space that I can project onto the plane, and I wish to express these points as 2D vectors within a local 2D coordinate system in the plane. The 2D coordinate system should have orthogonal axes, so I guess this is a case of finding a 2D orthonormal basis within the plane? There are obviously an infinity of choices for the origin of the coordinate system (within the plane), and the in-plane $x$ axis $\textbf{i}$ may be chosen to be any vector perpendicular to the plane normal. The 3D unit vector of the in-plane $y$ axis $\textbf{j}$ could then be computed as the cross product of the $x$ axis 3D unit vector and the plane normal. One algorithm for choosing these could be: Set origin as projection of $\left[0\:0\:0\right]^T$ on plane Compute 2D $x$ axis unit vector $\textbf{i}$ direction as $\left[1\:0\:0\right]^T\times\textbf{n}$ (then normalize if nonzero) If this is zero (i.e. $\textbf{n}$ is also $\left[1\:0\:0\right]^T$) then use $\textbf{i}$ direction as $\left[0\:1\:0\right]^T\times\textbf{n}$ instead (then normalize) Compute $\textbf{j}=\textbf{i}\times\textbf{n}$ However, this all seems a bit hacky (especially the testing for normal along the 3D $x$ axis, which would need to deal with the near-parallel case on a fixed-precision computer, which is where I'll be doing these sums). I'm sure there should be a nice, numerically stable, matrix-based method to find a suitable $\textbf{i}$ and $\textbf{j}$ basis within the plane. My question is: what might this matrix method be (in terms of my plane equation), and could you explain why it works? Thanks, Eric","I have a plane equation in 3D, in the form $Ax+By+Cz+D=0$ (or equivalently, $\textbf{x}\cdot\textbf{n} = \textbf{a}\cdot\textbf{n}$), where $\textbf{n}=\left[A\:B\:C\right]^T$ is the plane normal, and $D=-\textbf{a}\cdot\textbf{n}$ with $\textbf{a}$ as a point in the plane, hence $D$ is negative perpendicular distance from the plane to the origin. I have some points in 3D space that I can project onto the plane, and I wish to express these points as 2D vectors within a local 2D coordinate system in the plane. The 2D coordinate system should have orthogonal axes, so I guess this is a case of finding a 2D orthonormal basis within the plane? There are obviously an infinity of choices for the origin of the coordinate system (within the plane), and the in-plane $x$ axis $\textbf{i}$ may be chosen to be any vector perpendicular to the plane normal. The 3D unit vector of the in-plane $y$ axis $\textbf{j}$ could then be computed as the cross product of the $x$ axis 3D unit vector and the plane normal. One algorithm for choosing these could be: Set origin as projection of $\left[0\:0\:0\right]^T$ on plane Compute 2D $x$ axis unit vector $\textbf{i}$ direction as $\left[1\:0\:0\right]^T\times\textbf{n}$ (then normalize if nonzero) If this is zero (i.e. $\textbf{n}$ is also $\left[1\:0\:0\right]^T$) then use $\textbf{i}$ direction as $\left[0\:1\:0\right]^T\times\textbf{n}$ instead (then normalize) Compute $\textbf{j}=\textbf{i}\times\textbf{n}$ However, this all seems a bit hacky (especially the testing for normal along the 3D $x$ axis, which would need to deal with the near-parallel case on a fixed-precision computer, which is where I'll be doing these sums). I'm sure there should be a nice, numerically stable, matrix-based method to find a suitable $\textbf{i}$ and $\textbf{j}$ basis within the plane. My question is: what might this matrix method be (in terms of my plane equation), and could you explain why it works? Thanks, Eric",,"['geometry', 'matrices', 'coordinate-systems']"
38,Understanding an algorithm for computing a matrix polynomial,Understanding an algorithm for computing a matrix polynomial,,"I'm trying to understand this algorithm by Charles Van Loan for evaluating a matrix polynomial $p(\mathbf A)=\sum\limits_{k=0}^q b_k \mathbf A^k=b_0\mathbf I+b_1\mathbf A+\cdots$ (where $\mathbf A$ is $n\times n$): $\displaystyle s=\lfloor q\rfloor, \quad r=\lfloor q/s\rfloor$ $\displaystyle \mathbf Y=\mathbf A^s$ $\displaystyle \text{for}\quad j=1,\dots,n$ $\displaystyle\quad\quad \mathbf y_0^{(j)}=\mathbf e_j$ ($\mathbf e_j$ is the $j$th column of the identity matrix) $\displaystyle\quad\quad \text{for}\quad k=1,\dots,s-1$ $\displaystyle\quad\quad\quad \mathbf y_k^{(j)}=\mathbf A\mathbf y_{k-1}^{(j)}$ $\displaystyle\quad\quad \mathbf f_0^{(j)}=\sum_{k=q}^{rs}b_k\mathbf y_{k-rs}^{(j)}$ $\displaystyle\quad\quad \text{for}\quad k=1,\dots,r$ $\displaystyle\quad\quad\quad \mathbf f_k^{(j)}=\mathbf Y\mathbf p_{k-1}^{(j)}+\sum_{i=0}^{s-1}b_{s(r-k)+i}\mathbf y_{i}^{(j)}$ I've tried going through it a number of times, but it seems that the quantity (vector?) $\mathbf p_{k-1}^{(j)}$ was never defined anywhere in the paper. What might it be? Also, the paper claims (or more probably it's how I (mis)understood the paper) that only three matrices need to be stored for the evaluation: $\mathbf A$, $\mathbf Y$, and the final $p(\mathbf A)$. However, I can't see how to properly organize things so that an extra array for storing the $\mathbf y_k^{(j)}$ is not needed (otherwise, four matrices are now needed instead of the three claimed in the paper). Could anyone enlighten me on how to implement this algorithm properly? Better yet, is there any code available anywhere that implements this algorithm (I'm language-agnostic, so whatever code you post, I can likely translate into the language I'm currently using). Thanks for any help!","I'm trying to understand this algorithm by Charles Van Loan for evaluating a matrix polynomial $p(\mathbf A)=\sum\limits_{k=0}^q b_k \mathbf A^k=b_0\mathbf I+b_1\mathbf A+\cdots$ (where $\mathbf A$ is $n\times n$): $\displaystyle s=\lfloor q\rfloor, \quad r=\lfloor q/s\rfloor$ $\displaystyle \mathbf Y=\mathbf A^s$ $\displaystyle \text{for}\quad j=1,\dots,n$ $\displaystyle\quad\quad \mathbf y_0^{(j)}=\mathbf e_j$ ($\mathbf e_j$ is the $j$th column of the identity matrix) $\displaystyle\quad\quad \text{for}\quad k=1,\dots,s-1$ $\displaystyle\quad\quad\quad \mathbf y_k^{(j)}=\mathbf A\mathbf y_{k-1}^{(j)}$ $\displaystyle\quad\quad \mathbf f_0^{(j)}=\sum_{k=q}^{rs}b_k\mathbf y_{k-rs}^{(j)}$ $\displaystyle\quad\quad \text{for}\quad k=1,\dots,r$ $\displaystyle\quad\quad\quad \mathbf f_k^{(j)}=\mathbf Y\mathbf p_{k-1}^{(j)}+\sum_{i=0}^{s-1}b_{s(r-k)+i}\mathbf y_{i}^{(j)}$ I've tried going through it a number of times, but it seems that the quantity (vector?) $\mathbf p_{k-1}^{(j)}$ was never defined anywhere in the paper. What might it be? Also, the paper claims (or more probably it's how I (mis)understood the paper) that only three matrices need to be stored for the evaluation: $\mathbf A$, $\mathbf Y$, and the final $p(\mathbf A)$. However, I can't see how to properly organize things so that an extra array for storing the $\mathbf y_k^{(j)}$ is not needed (otherwise, four matrices are now needed instead of the three claimed in the paper). Could anyone enlighten me on how to implement this algorithm properly? Better yet, is there any code available anywhere that implements this algorithm (I'm language-agnostic, so whatever code you post, I can likely translate into the language I'm currently using). Thanks for any help!",,"['matrices', 'algorithms', 'polynomials']"
39,Products of Determinantal Ideals,Products of Determinantal Ideals,,"Let $X=(x_{ij})_{1\leq i\leq m, 1\leq j\leq n}$ where the $x_{ij}$ are variables. Now, for all $d$ , we consider the ideal $I_d$ of the polynomial ring $S=\mathbb{C}[x_{ij}:\, 1\leq i\leq m, 1\leq j\leq n]$ that is generated by all $d\times d$ minors of $X$ . After doing some experiments, I conjecture that we have $$I_{d-1}\cdot I_{d+1}\subset I_d^2$$ for all $d$ . I suspect that this might simply follow from some well-known determinantal identities but I didn't make a find. Does anybody have a smart proof or a reference? Edit: In order to resolve Darij's concern, let me give an example here. Let $m=n=4$ , so we consider the matrix $$X=\begin{pmatrix} x_0& x_4& x_8& x_{12}\\x_1& x_5& x_9& x_{13}\\x_2& x_6& x_{10}&       x_{14}\\x_3& x_7& x_{11}& x_{15} \end{pmatrix}.$$ The top left $3\times 3$ minor is $$h=-x_2 x_5 x_8+x_1 x_6 x_8+x_2 x_4 x_9-x_0 x_6 x_9-x_1 x_4 x_{10}+x_0 x_5 x_{10}.$$ Darij had some doubts that $x_{15}\cdot h$ is in the ideal $I_2^2$ . Here is such a representation: $$x_{15}\cdot h=\frac{1}{2}((x _{11} x _{14}-x _{10} x _{15}) (x_1 x_4-x_0 x_5)-(x _{11} x _{13}-x_9 x _{15}) (x_2 x_4-x_0 x_6)-(x _{10} x _{13}-x_9 x _{14}) (x_3 x_4-x_0 x_7)-(x_7 x _{14}-x_6 x _{15}) (x_1 x_8-x_0 x_9)+(x_7 x _{13}-x_5 x _{15}) (x_2 x_8-x_0 x _{10})+(x_6 x _{13}-x_5 x _{14}) (x_3 x_8-x_0 x _{11})+(x_3 x _{14}-x_2 x _{15}) (x_5 x_8-x_4 x_9)-(x_3 x _{13}-x_1 x _{15}) (x_6 x_8-x_4 x _{10})-(x_2 x _{13}-x_1 x _{14}) (x_7 x_8-x_4 x _{11})).$$ I hope that no typo happened.","Let where the are variables. Now, for all , we consider the ideal of the polynomial ring that is generated by all minors of . After doing some experiments, I conjecture that we have for all . I suspect that this might simply follow from some well-known determinantal identities but I didn't make a find. Does anybody have a smart proof or a reference? Edit: In order to resolve Darij's concern, let me give an example here. Let , so we consider the matrix The top left minor is Darij had some doubts that is in the ideal . Here is such a representation: I hope that no typo happened.","X=(x_{ij})_{1\leq i\leq m, 1\leq j\leq n} x_{ij} d I_d S=\mathbb{C}[x_{ij}:\, 1\leq i\leq m, 1\leq j\leq n] d\times d X I_{d-1}\cdot I_{d+1}\subset I_d^2 d m=n=4 X=\begin{pmatrix} x_0& x_4& x_8& x_{12}\\x_1& x_5& x_9& x_{13}\\x_2& x_6& x_{10}&
      x_{14}\\x_3& x_7& x_{11}& x_{15} \end{pmatrix}. 3\times 3 h=-x_2 x_5 x_8+x_1 x_6 x_8+x_2 x_4 x_9-x_0 x_6 x_9-x_1 x_4 x_{10}+x_0 x_5 x_{10}. x_{15}\cdot h I_2^2 x_{15}\cdot h=\frac{1}{2}((x _{11} x _{14}-x _{10} x _{15}) (x_1 x_4-x_0 x_5)-(x _{11} x _{13}-x_9 x _{15}) (x_2 x_4-x_0 x_6)-(x _{10} x _{13}-x_9 x _{14}) (x_3 x_4-x_0 x_7)-(x_7 x _{14}-x_6 x _{15}) (x_1 x_8-x_0 x_9)+(x_7 x _{13}-x_5 x _{15}) (x_2 x_8-x_0 x _{10})+(x_6 x _{13}-x_5 x _{14}) (x_3 x_8-x_0 x _{11})+(x_3 x _{14}-x_2 x _{15}) (x_5 x_8-x_4 x_9)-(x_3 x _{13}-x_1 x _{15}) (x_6 x_8-x_4 x _{10})-(x_2 x _{13}-x_1 x _{14}) (x_7 x_8-x_4 x _{11})).","['matrices', 'algebraic-geometry', 'commutative-algebra', 'determinant']"
40,Lower bound on absolute value of determinant of sum of matrices,Lower bound on absolute value of determinant of sum of matrices,,"I needed to find a lower bound on $|\det(A+B)|$ where $|.|$ is the absolute value operator. Because I was unable to get such a bound so I was trying to guess a bound and prove it. But $||\det(A)|-|\det(B)||$ does not seem to work. If $\sigma_1$, $\sigma_2$, ..., $\sigma_n$ are singular values of $A$, and $\mu_1$, $\mu_2$, ..., $\mu_n$ are singular values of $B$, then $|(\sigma_1-\mu_1)(\sigma_2-\mu_2)\cdots(\sigma_n-\mu_n)|$ does not seem to work as the lower bound either. By ""does not seem to work"" I mean it is not the lower bound of $|\det(A+B)|$. Any pointers on how to proceed will be helpful. A bound in case there is a significant difference between the singular values of $A$ and $B$ may also work. The only case that I can solve for is when both $A$ and $B$ are positive semi definite, where $|\det(A+B)|=\det(A+B)\geq \det(A)+\det(B)=|\det(A)|+|\det(B)|.$","I needed to find a lower bound on $|\det(A+B)|$ where $|.|$ is the absolute value operator. Because I was unable to get such a bound so I was trying to guess a bound and prove it. But $||\det(A)|-|\det(B)||$ does not seem to work. If $\sigma_1$, $\sigma_2$, ..., $\sigma_n$ are singular values of $A$, and $\mu_1$, $\mu_2$, ..., $\mu_n$ are singular values of $B$, then $|(\sigma_1-\mu_1)(\sigma_2-\mu_2)\cdots(\sigma_n-\mu_n)|$ does not seem to work as the lower bound either. By ""does not seem to work"" I mean it is not the lower bound of $|\det(A+B)|$. Any pointers on how to proceed will be helpful. A bound in case there is a significant difference between the singular values of $A$ and $B$ may also work. The only case that I can solve for is when both $A$ and $B$ are positive semi definite, where $|\det(A+B)|=\det(A+B)\geq \det(A)+\det(B)=|\det(A)|+|\det(B)|.$",,"['matrices', 'inequality', 'determinant', 'absolute-value']"
41,How to prove a function is a matrix exponential?,How to prove a function is a matrix exponential?,,"If $F(x) = \exp(x A) = \sum_{i = 0}^\infty \frac{1}{i!} x^i A^i$ where $F(x), A \in \mathbb{R}^{n \times n}$, then \begin{equation} F(x + y) = F(x) F(y) = F(y) F(x) \end{equation} holds. When is the converse true, i.e. if $F(x + y) = F(x) F(y) = F(y) F(x)$, then there exists a matrix $A$ such that $F(x) = \exp(x A)$?","If $F(x) = \exp(x A) = \sum_{i = 0}^\infty \frac{1}{i!} x^i A^i$ where $F(x), A \in \mathbb{R}^{n \times n}$, then \begin{equation} F(x + y) = F(x) F(y) = F(y) F(x) \end{equation} holds. When is the converse true, i.e. if $F(x + y) = F(x) F(y) = F(y) F(x)$, then there exists a matrix $A$ such that $F(x) = \exp(x A)$?",,"['matrices', 'matrix-calculus', 'matrix-exponential']"
42,How to solve a non-linear matrix equation over integer numbers?,How to solve a non-linear matrix equation over integer numbers?,,"The equations I want to solve looks like this: $$ X^n = 6 I $$ where $I$ is the identity matrix, $n$ is $1,2,3,...""$ and the square-matrix  $$ X=\begin{pmatrix}         x_{11} & x_{12} & \cdots  \\         x_{21} & x_{22} & \cdots \\         \vdots & \vdots & \ddots \\         \end{pmatrix} $$  with the $x_{ij} \in \Bbb Z$. If the entries of the matrix $X$ would be allowed to be reals, the problem would be easy to solve. The solution would be $$ X=\begin{pmatrix}         \sqrt[n]{6} & 0 & \cdots  \\         0 & \sqrt[n]{6} & \cdots \\         \vdots & \vdots & \ddots \\         \end{pmatrix} $$ However for $x_{ij}$ being integer the solution seems to be much harder. With guessing and some computer help I was able to find solutions for $n=2$  $$ X=\begin{pmatrix}         0 & 3 \\         2 & 0 \\         \end{pmatrix}  $$ and for $n=3$ $$ X=\begin{pmatrix}         2 & 3 & 1 \\         -1 & -1 & -2 \\         -2 & -1 & -1          \end{pmatrix} $$ I noticed that it didn't seem to be possible to find a two-dimensional matrix $X$ to solve the equation $X^3 = 6 I$. Could it be a general rule, that if you are searching for the $n$th root of a square matrix with integer component, the solution matrix needs to have $n$ dimensions? And I would like to know if there is a better method than just guessing to solve these kind of nonlinear matrix equations over integer numbers?","The equations I want to solve looks like this: $$ X^n = 6 I $$ where $I$ is the identity matrix, $n$ is $1,2,3,...""$ and the square-matrix  $$ X=\begin{pmatrix}         x_{11} & x_{12} & \cdots  \\         x_{21} & x_{22} & \cdots \\         \vdots & \vdots & \ddots \\         \end{pmatrix} $$  with the $x_{ij} \in \Bbb Z$. If the entries of the matrix $X$ would be allowed to be reals, the problem would be easy to solve. The solution would be $$ X=\begin{pmatrix}         \sqrt[n]{6} & 0 & \cdots  \\         0 & \sqrt[n]{6} & \cdots \\         \vdots & \vdots & \ddots \\         \end{pmatrix} $$ However for $x_{ij}$ being integer the solution seems to be much harder. With guessing and some computer help I was able to find solutions for $n=2$  $$ X=\begin{pmatrix}         0 & 3 \\         2 & 0 \\         \end{pmatrix}  $$ and for $n=3$ $$ X=\begin{pmatrix}         2 & 3 & 1 \\         -1 & -1 & -2 \\         -2 & -1 & -1          \end{pmatrix} $$ I noticed that it didn't seem to be possible to find a two-dimensional matrix $X$ to solve the equation $X^3 = 6 I$. Could it be a general rule, that if you are searching for the $n$th root of a square matrix with integer component, the solution matrix needs to have $n$ dimensions? And I would like to know if there is a better method than just guessing to solve these kind of nonlinear matrix equations over integer numbers?",,"['matrices', 'matrix-equations', 'integers']"
43,"Proof of the Isomorphism between: $SL(2,\mathbb R) \times SL(2, \mathbb R) \cong SO^+(2,2)$",Proof of the Isomorphism between:,"SL(2,\mathbb R) \times SL(2, \mathbb R) \cong SO^+(2,2)","I want to do a proof that $SL(2,\mathbb R)\times SL(2, \mathbb R) \cong SO^+(2,2)$. My idea was to use the same Argument as in this Question . So I wanted to begin with the Basis of the Lie algebra $\mathfrak{sl(2,\mathbb R)} \times \mathfrak{sl(2, \mathbb R)}$, but i am not sure how to do it. I know that the basis of $\mathfrak{sl(2, \mathbb R)}$ is $$ \mathfrak{sl(2,\mathbb R)} = span \left\{ \left(\begin{matrix} 0 & 1\\ 0 & 0 \end{matrix}\right), \left(\begin{matrix} 0 & 0\\ 1 & 0 \end{matrix}\right), \left(\begin{matrix} 1 & 0\\ 0 & -1 \end{matrix}\right)  \right\}. $$ But i dont know what to do with this result. I read , that  $$ \left(\begin{matrix} 1 & 0\\ 0 & 1 \end{matrix}\right),  \left(\begin{matrix} 1 & 0\\ 0 & -1 \end{matrix}\right),  \left(\begin{matrix} 0 & 1\\ -1 & 0 \end{matrix}\right),  \left(\begin{matrix} 0 & 1\\ 1 & 0 \end{matrix}\right) (*) $$ Is a basis of $\mathfrak{sl(2,\mathbb R)} \times \mathfrak{sl(2, \mathbb R)}$ wich gives for the following bilinear form: $$ \langle x, y \rangle := tr(x\cdot wy^Tw^{-1}) \qquad \qquad w := \left(\begin{matrix} 0 & -1\\ 1 & 0 \end{matrix} \right) $$ the desired signature $2,-2,2,-2$.  On the other hand, this question, $\dim_{\mathbb{R}}(\mathfrak{so(2,2)}) = 6$. Therefore $\dim_{\mathbb R}(\mathfrak{sl(2, \mathbb R) \times sl(2, \mathbb R)})$ should also be $6$. Was my conclusion in this question wrong, and the statement, $\dim_{\mathbb{R}}(\mathfrak{so(2,2)}) = 6$ is wrong, or is $(*)$ not a basis of $\mathfrak{sl(\mathbb R, 2)\times sl(\mathbb R,2)}$? What am i missing, and can someone help me, to write this proof in a more explicit way?","I want to do a proof that $SL(2,\mathbb R)\times SL(2, \mathbb R) \cong SO^+(2,2)$. My idea was to use the same Argument as in this Question . So I wanted to begin with the Basis of the Lie algebra $\mathfrak{sl(2,\mathbb R)} \times \mathfrak{sl(2, \mathbb R)}$, but i am not sure how to do it. I know that the basis of $\mathfrak{sl(2, \mathbb R)}$ is $$ \mathfrak{sl(2,\mathbb R)} = span \left\{ \left(\begin{matrix} 0 & 1\\ 0 & 0 \end{matrix}\right), \left(\begin{matrix} 0 & 0\\ 1 & 0 \end{matrix}\right), \left(\begin{matrix} 1 & 0\\ 0 & -1 \end{matrix}\right)  \right\}. $$ But i dont know what to do with this result. I read , that  $$ \left(\begin{matrix} 1 & 0\\ 0 & 1 \end{matrix}\right),  \left(\begin{matrix} 1 & 0\\ 0 & -1 \end{matrix}\right),  \left(\begin{matrix} 0 & 1\\ -1 & 0 \end{matrix}\right),  \left(\begin{matrix} 0 & 1\\ 1 & 0 \end{matrix}\right) (*) $$ Is a basis of $\mathfrak{sl(2,\mathbb R)} \times \mathfrak{sl(2, \mathbb R)}$ wich gives for the following bilinear form: $$ \langle x, y \rangle := tr(x\cdot wy^Tw^{-1}) \qquad \qquad w := \left(\begin{matrix} 0 & -1\\ 1 & 0 \end{matrix} \right) $$ the desired signature $2,-2,2,-2$.  On the other hand, this question, $\dim_{\mathbb{R}}(\mathfrak{so(2,2)}) = 6$. Therefore $\dim_{\mathbb R}(\mathfrak{sl(2, \mathbb R) \times sl(2, \mathbb R)})$ should also be $6$. Was my conclusion in this question wrong, and the statement, $\dim_{\mathbb{R}}(\mathfrak{so(2,2)}) = 6$ is wrong, or is $(*)$ not a basis of $\mathfrak{sl(\mathbb R, 2)\times sl(\mathbb R,2)}$? What am i missing, and can someone help me, to write this proof in a more explicit way?",,"['matrices', 'group-theory', 'representation-theory', 'lie-groups', 'lie-algebras']"
44,How to find x so that $\|A x\| = \|A\| \|x\|$ holds,How to find x so that  holds,\|A x\| = \|A\| \|x\|,"The subbordinance property of matrix-vector multiplication states that $\|A x\| \le \|A\| \|x\|$ where $\|x\|$ is the norm of vector $x$ and $\|A\|$ is the induced norm of matrix $A$. Many textbooks provide the proof of this result in theory, but none gives a specific way to find such an $x$ that satisfies the equality $\|Ax\| = \|A\|\|x\|$. For example, given a simple 2x2 matrix $A$, how would one find a 2-D vector $x$ that satisfies the equality? I am actually interested in the significance of such vector $x$, in addition to its existence in theory. The Cauchy-Schwarz inequality $|\langle x, y \rangle| \le \|x\|\|y\|$ becomes an equality if vectors $x$ and $y$ are linearly dependent, i.e., $y=cx$, or the angle between $x$ and $y$ is zero. Is there some similar interpretation for $A$ and $x$ that satisfy the equality $\|Ax\|=\|A\|\|x\|$ ?","The subbordinance property of matrix-vector multiplication states that $\|A x\| \le \|A\| \|x\|$ where $\|x\|$ is the norm of vector $x$ and $\|A\|$ is the induced norm of matrix $A$. Many textbooks provide the proof of this result in theory, but none gives a specific way to find such an $x$ that satisfies the equality $\|Ax\| = \|A\|\|x\|$. For example, given a simple 2x2 matrix $A$, how would one find a 2-D vector $x$ that satisfies the equality? I am actually interested in the significance of such vector $x$, in addition to its existence in theory. The Cauchy-Schwarz inequality $|\langle x, y \rangle| \le \|x\|\|y\|$ becomes an equality if vectors $x$ and $y$ are linearly dependent, i.e., $y=cx$, or the angle between $x$ and $y$ is zero. Is there some similar interpretation for $A$ and $x$ that satisfy the equality $\|Ax\|=\|A\|\|x\|$ ?",,"['matrices', 'normed-spaces']"
45,Matrix permutation-similarity invariants,Matrix permutation-similarity invariants,,"https://en.wikipedia.org/wiki/Matrix_similarity https://en.wikipedia.org/wiki/Permutation_matrix The determinant and trace (and characteristic polynomial coefficients) are well-known similarity invariants of a matrix. There are more if we only allow permutation similarities (swapping a pair of rows, and swapping the corresponding pair of columns). How many independent invariants does an $n \times n$ matrix have? I assume it would be $n^2$. How many invariants (polynomials of the matrix's elements) must be known to determine the matrix up to permutation-similarity? I don't know if it's too much to ask for a general formula for all the polynomials. (This is related to graph isomorphism , by the adjacency matrix.) A $2 \times 2$ matrix has only one other matrix that is permutation-similar: $$\begin{bmatrix} a & b \\ c & d \end{bmatrix} \sim \begin{bmatrix} d & c \\ b & a \end{bmatrix}$$ The following quantities are invariant with respect to this similarity: $$p_1 = a + d$$ $$p_2 = ad$$ $$q_1 = b + c$$ $$q_2 = bc$$ $$r_2 = (a-d)(b-c)$$ Knowing the $p$'s and $q$'s almost determines the matrix; it's either one of the two shown above, or one of their transposes. The transposes are eliminated if we are also given $r_2$. There are 5 equations here, but only 4 unknowns, so the system is over-determined. It has a solution (in Complex Numbers) if and only if $$(p_1^2 - 4p_2)(q_1^2 - 4q_2) = r_2^2$$ (If the system is restricted to Real Numbers, we also need $(p_1^2 - 4p_2) \ge 0$, and $(q_1^2 - 4q_2) \ge 0$ .) This equation cannot be used, in general, to reduce the system to 4 equations, because of the possibility that one factor on the left is zero, putting the other factor out of reach. (This is similar to the equation of a line, $Ax + By = C$; there are 3 parameters, but only 2 degrees of freedom. This is fixed by $x \cos\alpha + y \sin\alpha = D$, but I don't want to use trig functions in the matrix context.) So all 5 invariants are necessary to determine the matrix. For a $3 \times 3$ matrix $A$, I've found at least a dozen invariants, such as $$p_3 = A_{11}A_{22}A_{33}$$ $$q_3 = A_{12}A_{23}A_{31} + A_{13}A_{21}A_{32}$$ $$r_3 = A_{11}A_{23}A_{32} + A_{12}A_{21}A_{33} + A_{13}A_{22}A_{31}$$ $$p_5 = (A_{12}A_{13}A_{21}A_{23}A_{31} + A_{12}A_{13}A_{21}A_{23}A_{32} + A_{12}A_{13}A_{21}A_{31}A_{32} + A_{12}A_{13}A_{23}A_{31}A_{32} + A_{12}A_{21}A_{23}A_{31}A_{32} + A_{13}A_{21}A_{23}A_{31}A_{32})$$ In the $n \times n$ case, $n$ invariants will be the elementary symmetric polynomials of the matrix's diagonal elements, others will depend only on the off-diagonal elements, and others will be mixed. This is one way of classifying the invariants. We could refine this classification by specifying the number of on-diagonal and off-diagonal factors in each term. So, what invariants will guarantee that two matrices are permutation-similar? (There may be a better type of invariant to tell whether they're similar, such as a canonical/normal form of the matrix. But that's not what this question is about.)","https://en.wikipedia.org/wiki/Matrix_similarity https://en.wikipedia.org/wiki/Permutation_matrix The determinant and trace (and characteristic polynomial coefficients) are well-known similarity invariants of a matrix. There are more if we only allow permutation similarities (swapping a pair of rows, and swapping the corresponding pair of columns). How many independent invariants does an $n \times n$ matrix have? I assume it would be $n^2$. How many invariants (polynomials of the matrix's elements) must be known to determine the matrix up to permutation-similarity? I don't know if it's too much to ask for a general formula for all the polynomials. (This is related to graph isomorphism , by the adjacency matrix.) A $2 \times 2$ matrix has only one other matrix that is permutation-similar: $$\begin{bmatrix} a & b \\ c & d \end{bmatrix} \sim \begin{bmatrix} d & c \\ b & a \end{bmatrix}$$ The following quantities are invariant with respect to this similarity: $$p_1 = a + d$$ $$p_2 = ad$$ $$q_1 = b + c$$ $$q_2 = bc$$ $$r_2 = (a-d)(b-c)$$ Knowing the $p$'s and $q$'s almost determines the matrix; it's either one of the two shown above, or one of their transposes. The transposes are eliminated if we are also given $r_2$. There are 5 equations here, but only 4 unknowns, so the system is over-determined. It has a solution (in Complex Numbers) if and only if $$(p_1^2 - 4p_2)(q_1^2 - 4q_2) = r_2^2$$ (If the system is restricted to Real Numbers, we also need $(p_1^2 - 4p_2) \ge 0$, and $(q_1^2 - 4q_2) \ge 0$ .) This equation cannot be used, in general, to reduce the system to 4 equations, because of the possibility that one factor on the left is zero, putting the other factor out of reach. (This is similar to the equation of a line, $Ax + By = C$; there are 3 parameters, but only 2 degrees of freedom. This is fixed by $x \cos\alpha + y \sin\alpha = D$, but I don't want to use trig functions in the matrix context.) So all 5 invariants are necessary to determine the matrix. For a $3 \times 3$ matrix $A$, I've found at least a dozen invariants, such as $$p_3 = A_{11}A_{22}A_{33}$$ $$q_3 = A_{12}A_{23}A_{31} + A_{13}A_{21}A_{32}$$ $$r_3 = A_{11}A_{23}A_{32} + A_{12}A_{21}A_{33} + A_{13}A_{22}A_{31}$$ $$p_5 = (A_{12}A_{13}A_{21}A_{23}A_{31} + A_{12}A_{13}A_{21}A_{23}A_{32} + A_{12}A_{13}A_{21}A_{31}A_{32} + A_{12}A_{13}A_{23}A_{31}A_{32} + A_{12}A_{21}A_{23}A_{31}A_{32} + A_{13}A_{21}A_{23}A_{31}A_{32})$$ In the $n \times n$ case, $n$ invariants will be the elementary symmetric polynomials of the matrix's diagonal elements, others will depend only on the off-diagonal elements, and others will be mixed. This is one way of classifying the invariants. We could refine this classification by specifying the number of on-diagonal and off-diagonal factors in each term. So, what invariants will guarantee that two matrices are permutation-similar? (There may be a better type of invariant to tell whether they're similar, such as a canonical/normal form of the matrix. But that's not what this question is about.)",,"['matrices', 'permutations', 'systems-of-equations', 'invariance']"
46,Von Neumann's Trace Inequality for Multiple Matrices,Von Neumann's Trace Inequality for Multiple Matrices,,"Von Neumann's trace inequality states that $|tr(AB)| \le \sum_{i=1}^{n} \sigma_i(A) \sigma_i(B)$ where $A, B$ are general square matrices with singular values $(\sigma_i(A)), (\sigma_i(B))$, respectively. Question : does the analogous inequality hold for 3 (or more) matrices? Related : If it were true, I would expect to find this generalization elsewhere. However, it seems to be proven in a slightly different form in Theorem 2 of [Kristof 69] . When checking the reference, note (1) that $\lambda_i(A A^*) = \sigma_i(A)^2$ and (2) that $tr(ABC) \le tr(Z_1 diag(\sigma(A)) Z_2 diag(\sigma(B)) Z_3 diag(\sigma(C)))$ where $Z_i$ are unitary. The second claim can be easily argued via SVD and trace cycling. I would like somebody more knowledgable to point out my mistake to me. I haven't checked the proof in the paper.","Von Neumann's trace inequality states that $|tr(AB)| \le \sum_{i=1}^{n} \sigma_i(A) \sigma_i(B)$ where $A, B$ are general square matrices with singular values $(\sigma_i(A)), (\sigma_i(B))$, respectively. Question : does the analogous inequality hold for 3 (or more) matrices? Related : If it were true, I would expect to find this generalization elsewhere. However, it seems to be proven in a slightly different form in Theorem 2 of [Kristof 69] . When checking the reference, note (1) that $\lambda_i(A A^*) = \sigma_i(A)^2$ and (2) that $tr(ABC) \le tr(Z_1 diag(\sigma(A)) Z_2 diag(\sigma(B)) Z_3 diag(\sigma(C)))$ where $Z_i$ are unitary. The second claim can be easily argued via SVD and trace cycling. I would like somebody more knowledgable to point out my mistake to me. I haven't checked the proof in the paper.",,"['matrices', 'inequality', 'trace']"
47,Triangularization of matrix over PID,Triangularization of matrix over PID,,Let $R$ be a PID and let  $A \in Mat_n(R)$ with all of its eigenvalues in R. Is it true that I can always find $P \in GL_n(R)$ such that $P^{-1}AP$ is uppertriangular?  If so can I have a reference for this. Otherwise what additional hypotheses on $A$ do I need in order for it to be true. If it helps the case I am interested in is when $R$ is the ring of integers of some extension of $\mathbb{Q}_p$ or even just $\mathcal{O}_{\mathbb{C}_p}$. Thank you.,Let $R$ be a PID and let  $A \in Mat_n(R)$ with all of its eigenvalues in R. Is it true that I can always find $P \in GL_n(R)$ such that $P^{-1}AP$ is uppertriangular?  If so can I have a reference for this. Otherwise what additional hypotheses on $A$ do I need in order for it to be true. If it helps the case I am interested in is when $R$ is the ring of integers of some extension of $\mathbb{Q}_p$ or even just $\mathcal{O}_{\mathbb{C}_p}$. Thank you.,,"['matrices', 'matrix-decomposition', 'p-adic-number-theory', 'principal-ideal-domains', 'triangularization']"
48,Potential uses for viewing discrete wavelets constructed by filter banks as hierarchical random walks.,Potential uses for viewing discrete wavelets constructed by filter banks as hierarchical random walks.,,"I have some weak memory that some sources I have encountered a long time ago make some connection between random walks and wavelets, but I am quite sure it is not in the same sense. What I was thinking is to in the matrices representing filtering operations replace every $a\in \mathbb{R}$: $$\cases {\phantom{-}a\to \begin{bmatrix}a&0\\0&a\end{bmatrix}\\-a\to \begin{bmatrix}0&a\\a&0\end{bmatrix}}$$ Assuming filter taps are real numbers to start with, we will then have made our matrix positive in the sense that each element $\geq 0$, furthermore we can choose to normalize so that the matrix becomes stochastic. All operations will then preserve the positivity because they will be permutations and subsamplings concatenated with new filterings which are in turn constructed in the same way. And more generally : not necessarily filter banks, but any sequence of convolutions (assuming filters and signals consisting of real numbers) could be translated in the same way. What would the implications of such a construction be? What could it be useful for? What would ""happen"" to the signal in this new probabilistic sense and how to make use of it? Own work / example : The Haar system is usually considered one of the easiest to learn when introduced to the subject. It has a lowpass and a highpass filter of two taps each ( sum and difference ): $$\left[\begin{array}{rr}1&1\\1&-1\end{array}\right] \to \frac{1}{2}\cdot \left[\begin{array}{rr|rr}1&0&1&0\\0&1&0&1\\\hline 1&0&0&1\\0&1&1&0\end{array}\right]$$ where the factor $\frac 1 2$ is normalization factor to get a stochastic matrix and the horizontal/vertical lines are just to make the block structure clearer. The stationary distribution (eigenvalue 1) has 1/4 in each bin.","I have some weak memory that some sources I have encountered a long time ago make some connection between random walks and wavelets, but I am quite sure it is not in the same sense. What I was thinking is to in the matrices representing filtering operations replace every $a\in \mathbb{R}$: $$\cases {\phantom{-}a\to \begin{bmatrix}a&0\\0&a\end{bmatrix}\\-a\to \begin{bmatrix}0&a\\a&0\end{bmatrix}}$$ Assuming filter taps are real numbers to start with, we will then have made our matrix positive in the sense that each element $\geq 0$, furthermore we can choose to normalize so that the matrix becomes stochastic. All operations will then preserve the positivity because they will be permutations and subsamplings concatenated with new filterings which are in turn constructed in the same way. And more generally : not necessarily filter banks, but any sequence of convolutions (assuming filters and signals consisting of real numbers) could be translated in the same way. What would the implications of such a construction be? What could it be useful for? What would ""happen"" to the signal in this new probabilistic sense and how to make use of it? Own work / example : The Haar system is usually considered one of the easiest to learn when introduced to the subject. It has a lowpass and a highpass filter of two taps each ( sum and difference ): $$\left[\begin{array}{rr}1&1\\1&-1\end{array}\right] \to \frac{1}{2}\cdot \left[\begin{array}{rr|rr}1&0&1&0\\0&1&0&1\\\hline 1&0&0&1\\0&1&1&0\end{array}\right]$$ where the factor $\frac 1 2$ is normalization factor to get a stochastic matrix and the horizontal/vertical lines are just to make the block structure clearer. The stationary distribution (eigenvalue 1) has 1/4 in each bin.",,"['matrices', 'convolution', 'signal-processing', 'random-walk', 'wavelets']"
49,Why matrix representation of convolution cannot explain the convolution theorem?,Why matrix representation of convolution cannot explain the convolution theorem?,,"A record saying that Convolution Theorem is trivial since it is identical to the statement that convolution, as Toeplitz operator, has fourier eigenbasis and, therefore, is diagonal in it, has disappeared from Wikipedia. The Convolution Theorem states that convolution of functions, h(t) and x(t), in time domain is equivalent to their multiplication in the frequency domain. That is, you convolve them, h*x, and take result into frequency domain. Result F(h*x) must be the same as multiplying their Fourier images, H and X: $F(h*x) = H \cdot X$, where H and X are fourier images, $H = F(h)$ and $X = F(x)$. This was the definition where I used the letters h and x for the functions, instead of conventional f and g, because convolution h*x is often represented by h(x), where h is a convolution matrix derived from the first function, h, which is applied to the second function, x. Being Toeplitz, matrix h has eigenbasis F -- the same as apply to vector to take it into Fourier domain (see change of basis to see why base matrix product for base transform). Therefore, matrix h, translated into its eigenbasis, happens to be diagonal and H. That is, according to the Convolution Theorem, we must prove that $$F(h \vec x) = H \vec X$$ But, there is nothing to prove. We just can show that $$H \vec X = (FhF^{-1})(F\vec x) = F(F^{-1}F)h(F^{-1}F)\vec x = F(h \vec x)$$ or, the other way around $$F(h \vec x) = F(F^{-1}F)h(F^{-1}F)\vec x = (FhF^{-1})(F\vec x) = H \vec X$$ just to exercise the beautiful algebra of relationships and diagonalization in F. We just need to keep in mind that $H = FhF^{-1}$ is diagonal ( multiplication operator ) because F is eigenbasis of h. This discussion was classified as nonsense. But why? I find it amazing that Toeplitz (or convolution) has Fourier eigenbasis. Should this precious fact be hidden from the general population? Why should general population appreciate the integral-based proof rather than enjoy this fact? Can I say that Toeplitz operator = Convolution (operator)? Can I say that operator is a (continuous-case) generalization of matrix? Is convolution theorem related with the Fourier eigenbasis? Can it be explained simpler, based on this fact?","A record saying that Convolution Theorem is trivial since it is identical to the statement that convolution, as Toeplitz operator, has fourier eigenbasis and, therefore, is diagonal in it, has disappeared from Wikipedia. The Convolution Theorem states that convolution of functions, h(t) and x(t), in time domain is equivalent to their multiplication in the frequency domain. That is, you convolve them, h*x, and take result into frequency domain. Result F(h*x) must be the same as multiplying their Fourier images, H and X: $F(h*x) = H \cdot X$, where H and X are fourier images, $H = F(h)$ and $X = F(x)$. This was the definition where I used the letters h and x for the functions, instead of conventional f and g, because convolution h*x is often represented by h(x), where h is a convolution matrix derived from the first function, h, which is applied to the second function, x. Being Toeplitz, matrix h has eigenbasis F -- the same as apply to vector to take it into Fourier domain (see change of basis to see why base matrix product for base transform). Therefore, matrix h, translated into its eigenbasis, happens to be diagonal and H. That is, according to the Convolution Theorem, we must prove that $$F(h \vec x) = H \vec X$$ But, there is nothing to prove. We just can show that $$H \vec X = (FhF^{-1})(F\vec x) = F(F^{-1}F)h(F^{-1}F)\vec x = F(h \vec x)$$ or, the other way around $$F(h \vec x) = F(F^{-1}F)h(F^{-1}F)\vec x = (FhF^{-1})(F\vec x) = H \vec X$$ just to exercise the beautiful algebra of relationships and diagonalization in F. We just need to keep in mind that $H = FhF^{-1}$ is diagonal ( multiplication operator ) because F is eigenbasis of h. This discussion was classified as nonsense. But why? I find it amazing that Toeplitz (or convolution) has Fourier eigenbasis. Should this precious fact be hidden from the general population? Why should general population appreciate the integral-based proof rather than enjoy this fact? Can I say that Toeplitz operator = Convolution (operator)? Can I say that operator is a (continuous-case) generalization of matrix? Is convolution theorem related with the Fourier eigenbasis? Can it be explained simpler, based on this fact?",,"['matrices', 'operator-theory', 'convolution', 'alternative-proof', 'proof-without-words']"
50,"Fastest way of multiplying two $n×n$ matrices for fixed $n=1,2,3,\ldots$?",Fastest way of multiplying two  matrices for fixed ?,"n×n n=1,2,3,\ldots","While the question of what is the asymptotically fastest matrix multiplication algorithm is still open, and tremendous improvements were made between 1968 and 1990 (Strassen, Coppersmith-Winograd), I was wondering in the opposite direction, whether it is known what the fastest matrix multiplication algorithm is for matrices of fixed size $n=1,2,3,\ldots$ More specifically, given fixed costs for basic scalar operations, is there some table out there on the web containing this data? How do these algorithms look like for, say, $n=1\ldots 20$ ? What is smallest $n$ for which the answer is unknown? (I suspect it will be rather small due to combinatorial explosion issues.)","While the question of what is the asymptotically fastest matrix multiplication algorithm is still open, and tremendous improvements were made between 1968 and 1990 (Strassen, Coppersmith-Winograd), I was wondering in the opposite direction, whether it is known what the fastest matrix multiplication algorithm is for matrices of fixed size More specifically, given fixed costs for basic scalar operations, is there some table out there on the web containing this data? How do these algorithms look like for, say, ? What is smallest for which the answer is unknown? (I suspect it will be rather small due to combinatorial explosion issues.)","n=1,2,3,\ldots n=1\ldots 20 n","['matrices', 'computational-complexity']"
51,Prove that two matrices commute iff the square of the matrices commute,Prove that two matrices commute iff the square of the matrices commute,,In my textbook there is a task in which I have to prove the relation \begin{equation} AB=BA\Leftrightarrow A^2B^2=B^2A^2. \end{equation} For ($\Rightarrow$) it is easy \begin{equation} AB=BA\Rightarrow (AB)^2=(BA)^2\Rightarrow ABAB=BABA\Rightarrow BBAA=AABB. \end{equation} But how do I prove ($\Leftarrow$)?,In my textbook there is a task in which I have to prove the relation \begin{equation} AB=BA\Leftrightarrow A^2B^2=B^2A^2. \end{equation} For ($\Rightarrow$) it is easy \begin{equation} AB=BA\Rightarrow (AB)^2=(BA)^2\Rightarrow ABAB=BABA\Rightarrow BBAA=AABB. \end{equation} But how do I prove ($\Leftarrow$)?,,['matrices']
52,Are all vectors matrices?,Are all vectors matrices?,,"It's pretty clear to me that all vectors are matrices (either 1 x n or n x 1).  But there is some discussion about this at work.  We're wondering whether the expression ""Vectors/Matrices"" in a software GUI can be shortened to ""Matrices"".  (The shorter the expression, the better, because of space restrictions in the GUI.) So, is every vector a matrix or is there some subtlety in the definitions that I don't know about?","It's pretty clear to me that all vectors are matrices (either 1 x n or n x 1).  But there is some discussion about this at work.  We're wondering whether the expression ""Vectors/Matrices"" in a software GUI can be shortened to ""Matrices"".  (The shorter the expression, the better, because of space restrictions in the GUI.) So, is every vector a matrix or is there some subtlety in the definitions that I don't know about?",,"['matrices', 'vector-spaces']"
53,Examples of matrices that are both skew-symmetric and orthogonal,Examples of matrices that are both skew-symmetric and orthogonal,,"Are there matrices that satisfy these two conditions? That is, a matrix $A$ such that $$A^T=A^{-1}=-A$$ What I know is that a skew-symmetric matrix with $n$ dimensions is singular when $n$ is odd.","Are there matrices that satisfy these two conditions? That is, a matrix $A$ such that $$A^T=A^{-1}=-A$$ What I know is that a skew-symmetric matrix with $n$ dimensions is singular when $n$ is odd.",,"['matrices', 'examples-counterexamples', 'orthogonal-matrices']"
54,Hessian matrix of a quadratic form,Hessian matrix of a quadratic form,,Prove that the Hessian matrix of a quadratic form $f(x)=x^TAx$ is $f^{\prime\prime}(x) = A + A^T$ . I am not even sure what the Jacobian looks like (I never did one for $x \in \Bbb R^n$ ). Please help.,Prove that the Hessian matrix of a quadratic form is . I am not even sure what the Jacobian looks like (I never did one for ). Please help.,f(x)=x^TAx f^{\prime\prime}(x) = A + A^T x \in \Bbb R^n,"['matrices', 'multivariable-calculus', 'derivatives', 'quadratic-forms', 'hessian-matrix']"
55,"In what sense can the Lie algebra of $GL_n(\mathbb{C})$ be ""identified"" with $M_n(\mathbb C)$?","In what sense can the Lie algebra of  be ""identified"" with ?",GL_n(\mathbb{C}) M_n(\mathbb C),"I would like to do Tao's exercise 6 (i) but before I can even attempt it I need to be clear about his terminology. Exercise 6 Show that the Lie algebra $gl_n(\mathbb{C})$ of the general linear group $GL_n(\mathbb{C})$ can be identified with the space $M_n(\mathbb{C})$ of $n \times n$ complex matrices, with the Lie bracket $[A,B] := AB - BA$. When he writes ""identified"", does he just mean there is an isomorphism? So the exercise would be to find an explicit isomorphism from $M_n(\mathbb{C})$ to $gl_n(\mathbb{C})$? Is that correct? Many thanks for your help.","I would like to do Tao's exercise 6 (i) but before I can even attempt it I need to be clear about his terminology. Exercise 6 Show that the Lie algebra $gl_n(\mathbb{C})$ of the general linear group $GL_n(\mathbb{C})$ can be identified with the space $M_n(\mathbb{C})$ of $n \times n$ complex matrices, with the Lie bracket $[A,B] := AB - BA$. When he writes ""identified"", does he just mean there is an isomorphism? So the exercise would be to find an explicit isomorphism from $M_n(\mathbb{C})$ to $gl_n(\mathbb{C})$? Is that correct? Many thanks for your help.",,"['matrices', 'lie-groups', 'lie-algebras']"
56,Find $2\times 2$ matrices $A$ and $B$ such that $AB=0$ but $BA$ does not equals to $0$ [closed],Find  matrices  and  such that  but  does not equals to  [closed],2\times 2 A B AB=0 BA 0,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find $2\times 2$ matrices $A$ and $B$ such that $AB=0$ but $BA$ does not equals to $0$ (please show working and the concepts used) Thanks :)","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find $2\times 2$ matrices $A$ and $B$ such that $AB=0$ but $BA$ does not equals to $0$ (please show working and the concepts used) Thanks :)",,['matrices']
57,Calculating the matrix $M^{2006}$,Calculating the matrix,M^{2006},"Say you have the matrix $M$: $$\begin{bmatrix}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\-\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\end{bmatrix}$$ How do you find $M^{2006}$?  My thinking was that you can find that $M^8 = I$, so if $\frac{2006}{8} = 250\frac{3}{4}$, then $M^{2003} = I$, so if you multiply this by $M$, $3$ times, you would get $M^{2006}$.  Though, there seems to be something wrong with my arithmetic or else you cannot do this with matrix powers, as this is the incorrect answer. The correct answer is: $$\begin{bmatrix}0&-1\\1&0\end{bmatrix}$$ How do I get there?","Say you have the matrix $M$: $$\begin{bmatrix}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\-\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\end{bmatrix}$$ How do you find $M^{2006}$?  My thinking was that you can find that $M^8 = I$, so if $\frac{2006}{8} = 250\frac{3}{4}$, then $M^{2003} = I$, so if you multiply this by $M$, $3$ times, you would get $M^{2006}$.  Though, there seems to be something wrong with my arithmetic or else you cannot do this with matrix powers, as this is the incorrect answer. The correct answer is: $$\begin{bmatrix}0&-1\\1&0\end{bmatrix}$$ How do I get there?",,"['matrices', 'matrix-equations']"
58,Matrix Representation of Octonions,Matrix Representation of Octonions,,"Since quaternions $\mathbb{H}$ have a matrix representation as elements of $\text{SU}(2,\mathbb{C})$ as the following $$   1 \mapsto \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix},\quad \mathrm i \mapsto \begin{pmatrix} \mathrm i_{\mathbb C} & 0 \\ 0 & -\mathrm i_{\mathbb C} \end{pmatrix},\quad \mathrm j \mapsto \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix},\quad \mathrm k \mapsto \begin{pmatrix} 0 & \mathrm i_{\mathbb C} \\ \mathrm i_{\mathbb C} & 0 \end{pmatrix},  $$ I always wondered if there is also matrix representation of the octonions? How is the non-associativity realised with matrices?","Since quaternions $\mathbb{H}$ have a matrix representation as elements of $\text{SU}(2,\mathbb{C})$ as the following $$   1 \mapsto \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix},\quad \mathrm i \mapsto \begin{pmatrix} \mathrm i_{\mathbb C} & 0 \\ 0 & -\mathrm i_{\mathbb C} \end{pmatrix},\quad \mathrm j \mapsto \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix},\quad \mathrm k \mapsto \begin{pmatrix} 0 & \mathrm i_{\mathbb C} \\ \mathrm i_{\mathbb C} & 0 \end{pmatrix},  $$ I always wondered if there is also matrix representation of the octonions? How is the non-associativity realised with matrices?",,"['matrices', 'representation-theory', 'quaternions', 'octonions']"
59,Understanding matrices.,Understanding matrices.,,"I'm trying to understand matrices. As far as I can understand, a matrix is a way to represent data (?) or some sort of function on data (?). However apart from the fact that they're a way to represent data (?), I really don't understand why we need them and what they're for. I'll give you an example: I'm a game developer, and in my games I often use the well known rotation matrix. When I want to rotate a shape by some angle, I do this for every vertex: newX = cos(angle) - sin(angle) newY = sin(angle) + cos(angle) This works great. However, I don't understand why we need to represent it as a matrix (a table). The formula written above works great, what do I gain by thinking of it as a matrix? Please explain to me what I gain by using matrices (tables) and what exactly they're used for. And please go easy on the math.","I'm trying to understand matrices. As far as I can understand, a matrix is a way to represent data (?) or some sort of function on data (?). However apart from the fact that they're a way to represent data (?), I really don't understand why we need them and what they're for. I'll give you an example: I'm a game developer, and in my games I often use the well known rotation matrix. When I want to rotate a shape by some angle, I do this for every vertex: newX = cos(angle) - sin(angle) newY = sin(angle) + cos(angle) This works great. However, I don't understand why we need to represent it as a matrix (a table). The formula written above works great, what do I gain by thinking of it as a matrix? Please explain to me what I gain by using matrices (tables) and what exactly they're used for. And please go easy on the math.",,['matrices']
60,difference between linear transformation and its matrix representation .,difference between linear transformation and its matrix representation .,,"I can't understand this: Given a matrix T =$\small \{T_{ij}\} \in \mathbb M_{mn}$,define a transformation $\small T:\mathbb R^n \rightarrow \mathbb R^m$ as follows: If   $~~\small T(x_1,\ldots \ldots,x_n)=(y_1,\ldots \ldots ,y_m)$ ,     then $~\small y_i=\sum_{j=1}^{n}T_{ij}~x_j~~$ for all $\small i=1,.....,m$ I can't differentiate between a linear mapping and the matrix of transformation.Aren't they representing the same thing.Please help me understanding the difference between them...","I can't understand this: Given a matrix T =$\small \{T_{ij}\} \in \mathbb M_{mn}$,define a transformation $\small T:\mathbb R^n \rightarrow \mathbb R^m$ as follows: If   $~~\small T(x_1,\ldots \ldots,x_n)=(y_1,\ldots \ldots ,y_m)$ ,     then $~\small y_i=\sum_{j=1}^{n}T_{ij}~x_j~~$ for all $\small i=1,.....,m$ I can't differentiate between a linear mapping and the matrix of transformation.Aren't they representing the same thing.Please help me understanding the difference between them...",,"['matrices', 'linear-transformations']"
61,Find all matrices $A$ of order $2 \times 2$ that satisfy the equation $A^2-5A+6I = O$,Find all matrices  of order  that satisfy the equation,A 2 \times 2 A^2-5A+6I = O,"Find all matrices $A$ of order $2 \times 2$ that satisfy the equation $$ A^2-5A+6I = O $$ My Attempt: We can separate the $A$ term of the given equality: $$ \begin{align} A^2-5A+6I &= O\\ A^2-3A-2A+6I^2 &= O \end{align} $$ This implies that $A\in\{3I,2I\} = \left\{\begin{pmatrix} 3 & 0\\  0 & 3 \end{pmatrix}, \begin{pmatrix} 2 & 0\\  0 & 2 \end{pmatrix}\right\}$. Are these the only two possible values for $A$, or are there other solutions?If there are other solutions, how can I find them?","Find all matrices $A$ of order $2 \times 2$ that satisfy the equation $$ A^2-5A+6I = O $$ My Attempt: We can separate the $A$ term of the given equality: $$ \begin{align} A^2-5A+6I &= O\\ A^2-3A-2A+6I^2 &= O \end{align} $$ This implies that $A\in\{3I,2I\} = \left\{\begin{pmatrix} 3 & 0\\  0 & 3 \end{pmatrix}, \begin{pmatrix} 2 & 0\\  0 & 2 \end{pmatrix}\right\}$. Are these the only two possible values for $A$, or are there other solutions?If there are other solutions, how can I find them?",,['matrices']
62,A book for self-study of matrix decompositions,A book for self-study of matrix decompositions,,"I am a third year math student and I noticed that there are many uses for decomposing a matrix (I mean decompositions like SVD, LU etc'). Is there a good book for self-study of the subject ? Note that I don't want to read about different decompositions but rather understand the proof for their existence and if there is an explanation for ""where the decomposition came from"" it will be fantastic. Any suggestions ?","I am a third year math student and I noticed that there are many uses for decomposing a matrix (I mean decompositions like SVD, LU etc'). Is there a good book for self-study of the subject ? Note that I don't want to read about different decompositions but rather understand the proof for their existence and if there is an explanation for ""where the decomposition came from"" it will be fantastic. Any suggestions ?",,"['matrices', 'reference-request', 'soft-question', 'numerical-linear-algebra']"
63,How to count the number of free parameters in an orthogonal transformation matrix?,How to count the number of free parameters in an orthogonal transformation matrix?,,"Prove that the number of free parameters in an $n\times n$ orthogonal transformation matrix is equal to $\frac{n(n-1)}{2}$. For example parametrization of $2 \times 2$ orthogonal matrix requires only one parameter, ie $\theta$. And the parametric form is $$ M_2 = \pm\left(\begin{array}{cc} \cos\theta & \sin\theta\\ -\sin\theta & \cos\theta \end{array}\right) .$$","Prove that the number of free parameters in an $n\times n$ orthogonal transformation matrix is equal to $\frac{n(n-1)}{2}$. For example parametrization of $2 \times 2$ orthogonal matrix requires only one parameter, ie $\theta$. And the parametric form is $$ M_2 = \pm\left(\begin{array}{cc} \cos\theta & \sin\theta\\ -\sin\theta & \cos\theta \end{array}\right) .$$",,['matrices']
64,Finding a matrix from its product with its transpose,Finding a matrix from its product with its transpose,,"Suppose $A$ is a $3 \times 3$ matrix. If $A A^T = B$ and $A^T A = C$, where $B$ and $C$ are known and $B \neq C$, can I uniquely determine A? $A$ has 9 independent elements. Since $B$ and $C$ are symmetric, they have 6 independent entries each. Thus I have an overdetermined nonlinear system of equations with 9 variables and 12 equations. Does a unique solution exist for this system? How can I prove that it does or doesn't?","Suppose $A$ is a $3 \times 3$ matrix. If $A A^T = B$ and $A^T A = C$, where $B$ and $C$ are known and $B \neq C$, can I uniquely determine A? $A$ has 9 independent elements. Since $B$ and $C$ are symmetric, they have 6 independent entries each. Thus I have an overdetermined nonlinear system of equations with 9 variables and 12 equations. Does a unique solution exist for this system? How can I prove that it does or doesn't?",,['matrices']
65,Prove that $\det(A+B)\det(A-B)=0$,Prove that,\det(A+B)\det(A-B)=0,"Let $A,B$ be two $n\times n$ matrices with real entries, where $n$ is odd, such that $A\cdot A^{t}=I_n$ and $B\cdot B^{t}=I_n$. Prove that $$\det(A+B)\det(A-B)=0$$ It is obvious that $A^{-1}=A^{t}$ and $B^{-1}=B^{t}$, so $\det A, \det B = \pm 1$. Then I tried to write $\det(A+B)\det(A-B)=\det(A^2-AB+BA-B^2)$, but I didn't get anything useful.","Let $A,B$ be two $n\times n$ matrices with real entries, where $n$ is odd, such that $A\cdot A^{t}=I_n$ and $B\cdot B^{t}=I_n$. Prove that $$\det(A+B)\det(A-B)=0$$ It is obvious that $A^{-1}=A^{t}$ and $B^{-1}=B^{t}$, so $\det A, \det B = \pm 1$. Then I tried to write $\det(A+B)\det(A-B)=\det(A^2-AB+BA-B^2)$, but I didn't get anything useful.",,"['matrices', 'determinant', 'transpose']"
66,Matrix-Square Root,Matrix-Square Root,,I was wondering about matrix square roots. What is the procedure to evaluate $(X^{T}X)^{-1/2}$? Is it by a spectral decomposition of $(X^{T}X)^{-1}$ as $U\lambda U^{T}$ followed by the square root $S$ obtained as $U\lambda^{1/2}$? I would like to hear more.,I was wondering about matrix square roots. What is the procedure to evaluate $(X^{T}X)^{-1/2}$? Is it by a spectral decomposition of $(X^{T}X)^{-1}$ as $U\lambda U^{T}$ followed by the square root $S$ obtained as $U\lambda^{1/2}$? I would like to hear more.,,['matrices']
67,Finding $A^n$ for a matrix,Finding  for a matrix,A^n,"I have a matrix $$ A = \left[ {\begin{array}{cc}  1 & c  \\  0 & d  \\  \end{array} } \right] $$ with $c$ and $d$ constant. I need to find $A^n$ ($n$ positive) and then need to prove that formula using induction. I would like to check that the formula I derived is correct: $$ A^n = \left[ {\begin{array}{cc}  1 & c^{n-2}(dc + c)  \\  0 & d^n  \\  \end{array} } \right] $$ If this is correct, how can I prove this? I suppose I can write $A^{n+1} = A^n A$, which would be $$ \left[ {\begin{array}{cc}  1 & c^{n-2}(dc + c)  \\  0 & d^n  \\  \end{array} } \right] \left[ {\begin{array}{cc}  1 & c  \\  0 & d  \\  \end{array} } \right]  $$ But then what would I do? Thanks.","I have a matrix $$ A = \left[ {\begin{array}{cc}  1 & c  \\  0 & d  \\  \end{array} } \right] $$ with $c$ and $d$ constant. I need to find $A^n$ ($n$ positive) and then need to prove that formula using induction. I would like to check that the formula I derived is correct: $$ A^n = \left[ {\begin{array}{cc}  1 & c^{n-2}(dc + c)  \\  0 & d^n  \\  \end{array} } \right] $$ If this is correct, how can I prove this? I suppose I can write $A^{n+1} = A^n A$, which would be $$ \left[ {\begin{array}{cc}  1 & c^{n-2}(dc + c)  \\  0 & d^n  \\  \end{array} } \right] \left[ {\begin{array}{cc}  1 & c  \\  0 & d  \\  \end{array} } \right]  $$ But then what would I do? Thanks.",,"['matrices', 'induction']"
68,Are all square roots of diagonalizable matrices diagonalizable?,Are all square roots of diagonalizable matrices diagonalizable?,,"If a matrix is normal/unitarily diagonalizable, then its square roots are readily computed by taking the square roots of its eigenvalues (in the complex plane if needed). Any square root computed in this way is clearly going to also be normal. However, are all square roots of a normal matrix $A$ also going to be normal, or more generally, diagonalisable? If this is the case, how can it be proven? How can the non-diagonal square roots be computed? (The question could probably be generalized to refer to any inverse of a matrix function, but I am not sure about that). Some related questions: Determine all $2\times2$ matrices $A$ such that $A^2=0$. Families of Square Roots of Identity Matrices Are there sometimes only finitely many square roots of a positive matrix?","If a matrix is normal/unitarily diagonalizable, then its square roots are readily computed by taking the square roots of its eigenvalues (in the complex plane if needed). Any square root computed in this way is clearly going to also be normal. However, are all square roots of a normal matrix also going to be normal, or more generally, diagonalisable? If this is the case, how can it be proven? How can the non-diagonal square roots be computed? (The question could probably be generalized to refer to any inverse of a matrix function, but I am not sure about that). Some related questions: Determine all $2\times2$ matrices $A$ such that $A^2=0$. Families of Square Roots of Identity Matrices Are there sometimes only finitely many square roots of a positive matrix?",A,"['matrices', 'eigenvalues-eigenvectors', 'inverse-function']"
69,Rotate the parabola $y=x^2$ clockwise $45^\circ$.,Rotate the parabola  clockwise .,y=x^2 45^\circ,"I used the rotation matrix to do this and I ended up with the equation: $$x^2+y^2+2xy+\sqrt{2}x-\sqrt{2}y=0$$ I tried to plot this but none of the graphing softwares that I use would allow it. Is the above the correct equation for a parabola with vertex (0,0) and axis of symmetry $y=x$ ? $$\left( \begin{array}{cc} \cos\theta&-\sin\theta\\ \sin\theta&\cos\theta\\ \end{array} \right)\left( \begin{array}{cc} x\\ y\\ \end{array} \right)=\left( \begin{array}{cc} X\\ Y\\ \end{array} \right)$$ For a clockwise rotation of $\frac{\pi}{4}$, $\sin{-\frac{\pi}{4}}=\frac{-1}{\sqrt{2}}$ and $\cos{-\frac{\pi}{4}}=\frac{1}{\sqrt{2}}$ $$\left( \begin{array}{cc} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\ \end{array} \right)\left( \begin{array}{cc} x\\ y\\ \end{array} \right)=\left( \begin{array}{cc} X\\ Y\\ \end{array} \right)$$ $$X=\frac{x}{\sqrt{2}}+\frac{y}{\sqrt{2}}$$ $$Y=\frac{-x}{\sqrt{2}}+\frac{y}{\sqrt{2}}$$ $$y=x^2$$ $$\left(\frac{-x}{\sqrt{2}}+\frac{y}{\sqrt{2}}\right)=\left(\frac{x}{\sqrt{2}}+\frac{y}{\sqrt{2}}\right)^2$$ $$\frac{-x}{\sqrt{2}}+\frac{y}{\sqrt{2}}=\frac{x^2}{2}+\frac{2xy}{2}+\frac{y^2}{2}$$ $$-\sqrt{2}x+\sqrt{2}y=x^2+2xy+y^2$$ $$x^2+2xy+y^2+\sqrt{2}x-\sqrt{2}y=0$$ Have I made a mistake somewhere?","I used the rotation matrix to do this and I ended up with the equation: $$x^2+y^2+2xy+\sqrt{2}x-\sqrt{2}y=0$$ I tried to plot this but none of the graphing softwares that I use would allow it. Is the above the correct equation for a parabola with vertex (0,0) and axis of symmetry $y=x$ ? $$\left( \begin{array}{cc} \cos\theta&-\sin\theta\\ \sin\theta&\cos\theta\\ \end{array} \right)\left( \begin{array}{cc} x\\ y\\ \end{array} \right)=\left( \begin{array}{cc} X\\ Y\\ \end{array} \right)$$ For a clockwise rotation of $\frac{\pi}{4}$, $\sin{-\frac{\pi}{4}}=\frac{-1}{\sqrt{2}}$ and $\cos{-\frac{\pi}{4}}=\frac{1}{\sqrt{2}}$ $$\left( \begin{array}{cc} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\ \end{array} \right)\left( \begin{array}{cc} x\\ y\\ \end{array} \right)=\left( \begin{array}{cc} X\\ Y\\ \end{array} \right)$$ $$X=\frac{x}{\sqrt{2}}+\frac{y}{\sqrt{2}}$$ $$Y=\frac{-x}{\sqrt{2}}+\frac{y}{\sqrt{2}}$$ $$y=x^2$$ $$\left(\frac{-x}{\sqrt{2}}+\frac{y}{\sqrt{2}}\right)=\left(\frac{x}{\sqrt{2}}+\frac{y}{\sqrt{2}}\right)^2$$ $$\frac{-x}{\sqrt{2}}+\frac{y}{\sqrt{2}}=\frac{x^2}{2}+\frac{2xy}{2}+\frac{y^2}{2}$$ $$-\sqrt{2}x+\sqrt{2}y=x^2+2xy+y^2$$ $$x^2+2xy+y^2+\sqrt{2}x-\sqrt{2}y=0$$ Have I made a mistake somewhere?",,"['matrices', 'conic-sections', 'rotations']"
70,"When A and B are of different order given the $\det(AB)$,then calculate $\det(BA)$","When A and B are of different order given the ,then calculate",\det(AB) \det(BA),"Let 'A' be a $2 \times 3$ matrix where as B be a $3 \times 2$ matrix if $\det(AB) = 4$ the find value of the $\det(BA)$ My attempt: I took A =  $$         \begin{bmatrix}         2 & 0  &0\\         0 & 0  &2\\         \end{bmatrix} $$ B= $$         \begin{bmatrix}         1 & 0  \\         0 & 0  \\         0 & 1  \\         \end{bmatrix} $$ It satisfies given condition and I get $\det(BA)=0$ But I have not proved it How do I prove that it is always zero (background)I am 12th grader and I know about adjoint,inverse,determinant,rank of a matrix and the other basics.  However I do NOT know about eigenvalues and eigenvectors.","Let 'A' be a $2 \times 3$ matrix where as B be a $3 \times 2$ matrix if $\det(AB) = 4$ the find value of the $\det(BA)$ My attempt: I took A =  $$         \begin{bmatrix}         2 & 0  &0\\         0 & 0  &2\\         \end{bmatrix} $$ B= $$         \begin{bmatrix}         1 & 0  \\         0 & 0  \\         0 & 1  \\         \end{bmatrix} $$ It satisfies given condition and I get $\det(BA)=0$ But I have not proved it How do I prove that it is always zero (background)I am 12th grader and I know about adjoint,inverse,determinant,rank of a matrix and the other basics.  However I do NOT know about eigenvalues and eigenvectors.",,"['matrices', 'determinant', 'inverse']"
71,Rings with isomorphic proper subrings,Rings with isomorphic proper subrings,,"Rings will be unital here but I don't require that subrings share the identity elements with superrings. I just accidentally came up with an example of a ring $R$ with a proper subring $S$ such that $R\cong S$ as rings. I may be mistaken, but I think this is the first such example I've ever come across. The example is this. $R$ is the ring of $\mathbb Z\times \mathbb Z$-matrices over $\mathbb R$ with finite-support columns, and $S$ is the subring of matrices with zeroes in all places indexed by at least one negative number. Then $S$ is isomorphic to the ring of $\mathbb N\times\mathbb N$-matrices over $\mathbb R$. But then $R$ is isomorphic to the endomorphism ring of the direct sum of $\aleph_0$ copies of $\mathbb R$ and $S$ is too. I would like to know if there are other examples of such rings and if it has anything to do with the rings being Dedekind-infinite. I suspect it might because Dedekind-infinite sets are those which are equinumerous with some of their proper subsets.","Rings will be unital here but I don't require that subrings share the identity elements with superrings. I just accidentally came up with an example of a ring $R$ with a proper subring $S$ such that $R\cong S$ as rings. I may be mistaken, but I think this is the first such example I've ever come across. The example is this. $R$ is the ring of $\mathbb Z\times \mathbb Z$-matrices over $\mathbb R$ with finite-support columns, and $S$ is the subring of matrices with zeroes in all places indexed by at least one negative number. Then $S$ is isomorphic to the ring of $\mathbb N\times\mathbb N$-matrices over $\mathbb R$. But then $R$ is isomorphic to the endomorphism ring of the direct sum of $\aleph_0$ copies of $\mathbb R$ and $S$ is too. I would like to know if there are other examples of such rings and if it has anything to do with the rings being Dedekind-infinite. I suspect it might because Dedekind-infinite sets are those which are equinumerous with some of their proper subsets.",,['matrices']
72,Summing Matrix Series,Summing Matrix Series,,"I need to sum the series $$I + A + A^2 + \ldots$$ for the matrix $$A = \left(\begin{array}{rr} 0 & \epsilon \\ -\epsilon & 0 \end{array}\right)$$ and $\epsilon$ small. The goal is to invert the matrix $I - A$. The text says to use a geometric series but I had a hard time finding it. I'm studying on my own so I can't ask my teacher. The way I did it follows. I know it isn't quite rigorous (I assume the series in question converge) so I'd like to see how I'm supposed to do it. We see that $$\left(\begin{array}{rr} 0 & \epsilon \\ -\epsilon & 0 \end{array}\right) \left(\begin{array}{rr} a_{00} & a_{01} \\ a_{10} & a_{11} \end{array}\right) =  \left(\begin{array}{rr} \epsilon a_{10} & \epsilon a_{11} \\ -\epsilon a_{00} & \epsilon a_{01} \end{array}\right)$$ so if we let $a(i, j, k)$ be entry $a_{ij}$ in the $k$'th power of $A$ then we see that $$ a(0, 0, k) = \epsilon a(1, 0, k-1) $$ $$ a(1, 0, k) = -\epsilon a(0, 0, k-1) $$ Then, letting $\alpha$'s denote the entries in the sum without $I$ added in, we see that $$ \begin{eqnarray*} \alpha_{00} &=& \sum_{k=0}^{\infty}a(0, 0, k) \\ &=& \sum_{k=0}^{\infty}a(0, 0, 2k + 1) \\ &=& \epsilon\sum_{k=0}^{\infty}a(1, 0, 2k) \\ &=& \epsilon\alpha_{10} \end{eqnarray*}  $$ and $$ \begin{eqnarray*} \alpha_{10} &=& \sum_{k=0}^{\infty}a(1,0,k) \\ &=& \sum_{k=0}^{\infty}a(1,0,2k) \\ &=& -\epsilon + \sum_{k=1}^{\infty}a(1,0,2k) \\ &=& -\epsilon - \epsilon\sum_{k=1}^{\infty}a(0,0,2k-1) \\ &=& -\epsilon\left(1 + \sum_{k=0}^{\infty}a(0,0,2k+1) \right) \\ &=& -\epsilon\left(1 + \alpha_{00}\right) \end{eqnarray*} $$ so $\alpha_{00} = \epsilon\alpha_{10}$ and $\alpha_{10} = -\epsilon(1 + \alpha_{00})$ which we can solve for the $\alpha$'s. It's pretty much the same for the other two. I feel like there has got to be a better way to do this.","I need to sum the series $$I + A + A^2 + \ldots$$ for the matrix $$A = \left(\begin{array}{rr} 0 & \epsilon \\ -\epsilon & 0 \end{array}\right)$$ and $\epsilon$ small. The goal is to invert the matrix $I - A$. The text says to use a geometric series but I had a hard time finding it. I'm studying on my own so I can't ask my teacher. The way I did it follows. I know it isn't quite rigorous (I assume the series in question converge) so I'd like to see how I'm supposed to do it. We see that $$\left(\begin{array}{rr} 0 & \epsilon \\ -\epsilon & 0 \end{array}\right) \left(\begin{array}{rr} a_{00} & a_{01} \\ a_{10} & a_{11} \end{array}\right) =  \left(\begin{array}{rr} \epsilon a_{10} & \epsilon a_{11} \\ -\epsilon a_{00} & \epsilon a_{01} \end{array}\right)$$ so if we let $a(i, j, k)$ be entry $a_{ij}$ in the $k$'th power of $A$ then we see that $$ a(0, 0, k) = \epsilon a(1, 0, k-1) $$ $$ a(1, 0, k) = -\epsilon a(0, 0, k-1) $$ Then, letting $\alpha$'s denote the entries in the sum without $I$ added in, we see that $$ \begin{eqnarray*} \alpha_{00} &=& \sum_{k=0}^{\infty}a(0, 0, k) \\ &=& \sum_{k=0}^{\infty}a(0, 0, 2k + 1) \\ &=& \epsilon\sum_{k=0}^{\infty}a(1, 0, 2k) \\ &=& \epsilon\alpha_{10} \end{eqnarray*}  $$ and $$ \begin{eqnarray*} \alpha_{10} &=& \sum_{k=0}^{\infty}a(1,0,k) \\ &=& \sum_{k=0}^{\infty}a(1,0,2k) \\ &=& -\epsilon + \sum_{k=1}^{\infty}a(1,0,2k) \\ &=& -\epsilon - \epsilon\sum_{k=1}^{\infty}a(0,0,2k-1) \\ &=& -\epsilon\left(1 + \sum_{k=0}^{\infty}a(0,0,2k+1) \right) \\ &=& -\epsilon\left(1 + \alpha_{00}\right) \end{eqnarray*} $$ so $\alpha_{00} = \epsilon\alpha_{10}$ and $\alpha_{10} = -\epsilon(1 + \alpha_{00})$ which we can solve for the $\alpha$'s. It's pretty much the same for the other two. I feel like there has got to be a better way to do this.",,"['matrices', 'power-series']"
73,Is $A$ nilpotent matrix?,Is  nilpotent matrix?,A,"Let $A$ be a $n \times n$ matrix over $\mathbb{C}$ and $I+At$ is invertible for all t, does that imply $A$ is nilpotent? I know the converse is true, but is it also true?","Let $A$ be a $n \times n$ matrix over $\mathbb{C}$ and $I+At$ is invertible for all t, does that imply $A$ is nilpotent? I know the converse is true, but is it also true?",,"['matrices', 'nilpotence']"
74,Determinant of complex matrix,Determinant of complex matrix,,"How is the determinant of a complex matrix calculated? Is it the same algorithm as for real matrices, but the determinant itself is complex instead of real? (I was unable to find any hints with google...)","How is the determinant of a complex matrix calculated? Is it the same algorithm as for real matrices, but the determinant itself is complex instead of real? (I was unable to find any hints with google...)",,"['matrices', 'algorithms', 'determinant']"
75,Square root of nilpotent matrix,Square root of nilpotent matrix,,How could I show that $\forall n \ge 2$ if $A^n=0$ and $A^{n-1} \ne 0$ then $A$ has no square root? That is there is no $B$ such that $B^2=A$. Both matrices are $n \times n$. Thank you.,How could I show that $\forall n \ge 2$ if $A^n=0$ and $A^{n-1} \ne 0$ then $A$ has no square root? That is there is no $B$ such that $B^2=A$. Both matrices are $n \times n$. Thank you.,,['matrices']
76,Formula for determinant of block matrix with commuting blocks,Formula for determinant of block matrix with commuting blocks,,"On Wikipedia , I saw the following formula $$\det\begin{bmatrix}A & B\\ C & D\end{bmatrix} = \det(AD-BC)$$ if $C$ and $D$ commute. Is this always true? Or is there a good counter example for each $2 \times 2$ block matrices?","On Wikipedia , I saw the following formula $$\det\begin{bmatrix}A & B\\ C & D\end{bmatrix} = \det(AD-BC)$$ if $C$ and $D$ commute. Is this always true? Or is there a good counter example for each $2 \times 2$ block matrices?",,"['matrices', 'determinant', 'block-matrices']"
77,Differential and derivative of the trace of a matrix,Differential and derivative of the trace of a matrix,,"If $X$ is a square matrix, obtain the differential and the derivative of the functions: $f(X) = \operatorname{tr}(X)$ , $f(X) = \operatorname{tr}(X^2)$ , $f(X) = \operatorname{tr}(X^p)$ ( $p$ is a natural number). To find the differential I thought I could just find the differential of the compostion function first and then take the trace of that differential. Am I right in saying so? As for the derivative, I have no idea how I should do it for traces. Could anyone please help me out? wj32's answer makes sense to me, however, I wonder if it is also possible to solve this question by using the ordinary way of finding differentials and derivatives, namely f(x+dx)-f(x). Is there someone who could maybe show me how this would be done (if possible)?","If is a square matrix, obtain the differential and the derivative of the functions: , , ( is a natural number). To find the differential I thought I could just find the differential of the compostion function first and then take the trace of that differential. Am I right in saying so? As for the derivative, I have no idea how I should do it for traces. Could anyone please help me out? wj32's answer makes sense to me, however, I wonder if it is also possible to solve this question by using the ordinary way of finding differentials and derivatives, namely f(x+dx)-f(x). Is there someone who could maybe show me how this would be done (if possible)?",X f(X) = \operatorname{tr}(X) f(X) = \operatorname{tr}(X^2) f(X) = \operatorname{tr}(X^p) p,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
78,"If $A$ and$ I+AB$ are invertible, show $I+BA$ is also invertible","If  and are invertible, show  is also invertible",A  I+AB I+BA,"Show that if $A$ and $I+AB$ are invertible, then $I+BA$ is also invertible with $$(I+BA)^{-1} = A^{-1}(I+AB)^{-1}A$$","Show that if $A$ and $I+AB$ are invertible, then $I+BA$ is also invertible with $$(I+BA)^{-1} = A^{-1}(I+AB)^{-1}A$$",,"['matrices', 'inverse']"
79,What is the rotation axis and rotation angle of the composition of two rotation matrix in $\mathbb{R}^{3}$,What is the rotation axis and rotation angle of the composition of two rotation matrix in,\mathbb{R}^{3},"I was told in class that a rotation matrix is defined by a rotation angle and rotation axis, if we call the rotation axis $v$ and take a basis of $\mathbb{R}^{3}=\{v\}\bigoplus\{v\}^{\perp}$ then the matrix is similar by an orthogonal matrix to a matrix of the form $$\begin{pmatrix}\cos\theta & -\sin\theta\\ \sin\theta & \cos\theta\\  &  & 1 \end{pmatrix}$$ I asked my self the following question: If I rotate in the $xy$ plain (i.e. rotation axis is $z$) in angle $\theta$, and then rotate in the $yz$ plain  (i.e. rotation axis is $x$) in angle $\varphi$ , what rotation matrix I get ? I tried multiplying the corresponding matrices but that did not produce anything useful, I can't also thing of a vector $v\in\mathbb{R}^{3}$that is invariant under the composition... What is the rotation axis, and the rotation angle of these two compositions ? Help is appreciated!","I was told in class that a rotation matrix is defined by a rotation angle and rotation axis, if we call the rotation axis $v$ and take a basis of $\mathbb{R}^{3}=\{v\}\bigoplus\{v\}^{\perp}$ then the matrix is similar by an orthogonal matrix to a matrix of the form $$\begin{pmatrix}\cos\theta & -\sin\theta\\ \sin\theta & \cos\theta\\  &  & 1 \end{pmatrix}$$ I asked my self the following question: If I rotate in the $xy$ plain (i.e. rotation axis is $z$) in angle $\theta$, and then rotate in the $yz$ plain  (i.e. rotation axis is $x$) in angle $\varphi$ , what rotation matrix I get ? I tried multiplying the corresponding matrices but that did not produce anything useful, I can't also thing of a vector $v\in\mathbb{R}^{3}$that is invariant under the composition... What is the rotation axis, and the rotation angle of these two compositions ? Help is appreciated!",,"['matrices', 'geometry', 'rotations']"
80,How can I prove $A − A(A + B)^{−1}A = B − B(A + B)^{−1}B$ for matrices $A$ and $B$?,How can I prove  for matrices  and ?,A − A(A + B)^{−1}A = B − B(A + B)^{−1}B A B,The matrix cookbook (page 16) offers this amazing result: $$A − A(A + B)^{−1}A = B − B(A + B)^{−1}B$$ This seems to be too unbelievable to be true and I can't seem to prove it. Can anyone verify this equation/offer proof?,The matrix cookbook (page 16) offers this amazing result: This seems to be too unbelievable to be true and I can't seem to prove it. Can anyone verify this equation/offer proof?,A − A(A + B)^{−1}A = B − B(A + B)^{−1}B,"['matrices', 'matrix-equations']"
81,Prove that the rank of a block diagonal matrix equals the sum of the ranks of the matrices that are the main diagonal blocks.,Prove that the rank of a block diagonal matrix equals the sum of the ranks of the matrices that are the main diagonal blocks.,,"If $A$ and $B$ are matrices, $0$ is a zero matrix, and \begin{equation*} X= \begin{pmatrix} A& 0 \newline 0& B \end{pmatrix}, \end{equation*} prove that $\mathrm{rank}(X)=\mathrm{rank}(A)+\mathrm{rank}B)$ . Also, if the upper right zero matrix would be replaced with matrix $C$ , that is, \begin{equation*} X= \begin{pmatrix} A& C \newline 0& B \end{pmatrix} \end{equation*} would it still be true that $\mathrm{rank}(X)=\mathrm{rank}(A)+\mathrm{rank}B)$ ?","If and are matrices, is a zero matrix, and prove that . Also, if the upper right zero matrix would be replaced with matrix , that is, would it still be true that ?","A B 0 \begin{equation*}
X=
\begin{pmatrix}
A& 0
\newline
0& B
\end{pmatrix},
\end{equation*} \mathrm{rank}(X)=\mathrm{rank}(A)+\mathrm{rank}B) C \begin{equation*}
X=
\begin{pmatrix}
A& C
\newline
0& B
\end{pmatrix}
\end{equation*} \mathrm{rank}(X)=\mathrm{rank}(A)+\mathrm{rank}B)","['matrices', 'matrix-rank', 'block-matrices']"
82,Positive definite matrix over a non-Archimedean field,Positive definite matrix over a non-Archimedean field,,"Suppose $K$ is a non-Archimedean ordered field, and $A$ is an $n\times n$ square matrix over $K$ such that (1) all diagonal entries of $A$ are $1$, and (2) all off-diagonal entries of $A$ are infinitesimal (i.e. between $-\frac1k$ and $\frac1k$ for all nonzero natural numbers $k$).  For instance, if $K=\mathbb{R}(x)$ is the field of rational functions, ordered with $x$ infinitesimal, then the off-diagonal entries of $A$ could be polynomials in $x$ with zero constant term. Does it follow that $A$ is positive definite, i.e. that $V^{T} A V > 0$ (in the ordering of $K$) for all nonzero $V\in K^n$?  This seems intuitive to me because it is an ""infinitesimal deformation of the identity matrix"", but I don't immediately see how to prove it.","Suppose $K$ is a non-Archimedean ordered field, and $A$ is an $n\times n$ square matrix over $K$ such that (1) all diagonal entries of $A$ are $1$, and (2) all off-diagonal entries of $A$ are infinitesimal (i.e. between $-\frac1k$ and $\frac1k$ for all nonzero natural numbers $k$).  For instance, if $K=\mathbb{R}(x)$ is the field of rational functions, ordered with $x$ infinitesimal, then the off-diagonal entries of $A$ could be polynomials in $x$ with zero constant term. Does it follow that $A$ is positive definite, i.e. that $V^{T} A V > 0$ (in the ordering of $K$) for all nonzero $V\in K^n$?  This seems intuitive to me because it is an ""infinitesimal deformation of the identity matrix"", but I don't immediately see how to prove it.",,"['matrices', 'field-theory', 'positive-definite', 'infinitesimals']"
83,positive definite and transpose,positive definite and transpose,,"When a matrix A has m rows and n columns (m>n), explain why $AA^{T}$ can't be positive definite. For the same matrix A, is $A^{T}A$ always   positive definite? If so, explain. If not, what is the condition for A   so that $A^{T}A$ is positive definite? Now $A^{T}$ is the transpose of A. This means the columns of $A^{T}$ are formed with the corresponding rows of A. Positive definite means that $x^{T}Ax$ >0 for all$x\neq 0$. Also with square symmetric matrices, the quadratic form $x^{T}Ax$ is positive definite if and only if the eigenvalues of A are all positive. But how does this show that $AA^{T}$ is not positive definite?","When a matrix A has m rows and n columns (m>n), explain why $AA^{T}$ can't be positive definite. For the same matrix A, is $A^{T}A$ always   positive definite? If so, explain. If not, what is the condition for A   so that $A^{T}A$ is positive definite? Now $A^{T}$ is the transpose of A. This means the columns of $A^{T}$ are formed with the corresponding rows of A. Positive definite means that $x^{T}Ax$ >0 for all$x\neq 0$. Also with square symmetric matrices, the quadratic form $x^{T}Ax$ is positive definite if and only if the eigenvalues of A are all positive. But how does this show that $AA^{T}$ is not positive definite?",,['matrices']
84,Are there different conventions for representing rotations as quaternions?,Are there different conventions for representing rotations as quaternions?,,"I am trying to understand how quaternions are represented as rotations, in particular how to convert from a quaternion representation to a rotation matrix. The following paper by Diebel gives an overview of many ways to represent rotations, including quaternions: http://www.swarthmore.edu/NatSci/mzucker1/e27/diebel2006attitude.pdf According to Diebel, a quaternion $a+bi+cj+dk$ corresponds to the following rotation matrix: $\left( \begin{array}{ccc} a^2+b^2-c^2-d^2 & 2bc+2ad & 2bd-2ac \\ 2bc-2ad & a^2-b^2+c^2-d^2 & 2cd+2ab \\ 2bd+2ac & 2cd-2ab & a^2-b^2-c^2+d^2 \end{array} \right)$ (This is equation 125 on page 15, just with the $q_0,q_1,q_2,q_3$ replaced with $a,b,c,d$ respectively.) In contrast, on the Wikipedia page: http://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation the following matrix is given: $\left( \begin{array}{ccc} a^2+b^2-c^2-d^2 & 2bc-2ad & 2bd+2ac \\ 2bc+2ad & a^2-b^2+c^2-d^2 & 2cd-2ab \\ 2bd-2ac & 2cd+2ab & a^2-b^2-c^2+d^2 \end{array} \right)$ Note that this is not the same matrix as the one above (it is the transpose). I have looked at some other references and I can find some that give the first matrix, and some that give the second matrix. For example, the following references agree with the matrix given by Diebel: http://www.cs.princeton.edu/~gewang/projects/darth/stuff/quat_faq.html (question 54) http://www.ladispe.polito.it/corsi/Meccatronica/02JHCOR/2011-12/Slides/Shuster_Pub_1993h_J_Repsurv_scan.pdf (equation 158) (Note that some of these references use different names for the four quaternion components or put them with the scalar as the last instead of the first element; you can easily tell the difference in the two forms by seeing if the item in the first row, second column has a ""+"" or a ""-"" sign.) But the following references agree with the matrix given by the Wikipedia article: http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/FORTRAN/spicelib/q2m.html (under ""Associating SPICE Quaternions with Rotation Matrices"") http://books.google.com/books?id=hiBFUv_FT0wC&printsec=frontcover&dq=game+programming+gems+volume+1&hl=en&sa=X&ei=LQ-IUZ7CLKmpyAG66IGIDA&ved=0CEAQ6AEwAQ#v=onepage&q=convert%20quaternion%20to%20rotation%20matrix&f=false (page 202) What is the cause of this difference? Since there are several references that use each of these two matrices, it seems unlikely that it is simply an error in one of the references. Are there different conventions for how to represent rotations by quaternions that would lead to this difference?","I am trying to understand how quaternions are represented as rotations, in particular how to convert from a quaternion representation to a rotation matrix. The following paper by Diebel gives an overview of many ways to represent rotations, including quaternions: http://www.swarthmore.edu/NatSci/mzucker1/e27/diebel2006attitude.pdf According to Diebel, a quaternion $a+bi+cj+dk$ corresponds to the following rotation matrix: $\left( \begin{array}{ccc} a^2+b^2-c^2-d^2 & 2bc+2ad & 2bd-2ac \\ 2bc-2ad & a^2-b^2+c^2-d^2 & 2cd+2ab \\ 2bd+2ac & 2cd-2ab & a^2-b^2-c^2+d^2 \end{array} \right)$ (This is equation 125 on page 15, just with the $q_0,q_1,q_2,q_3$ replaced with $a,b,c,d$ respectively.) In contrast, on the Wikipedia page: http://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation the following matrix is given: $\left( \begin{array}{ccc} a^2+b^2-c^2-d^2 & 2bc-2ad & 2bd+2ac \\ 2bc+2ad & a^2-b^2+c^2-d^2 & 2cd-2ab \\ 2bd-2ac & 2cd+2ab & a^2-b^2-c^2+d^2 \end{array} \right)$ Note that this is not the same matrix as the one above (it is the transpose). I have looked at some other references and I can find some that give the first matrix, and some that give the second matrix. For example, the following references agree with the matrix given by Diebel: http://www.cs.princeton.edu/~gewang/projects/darth/stuff/quat_faq.html (question 54) http://www.ladispe.polito.it/corsi/Meccatronica/02JHCOR/2011-12/Slides/Shuster_Pub_1993h_J_Repsurv_scan.pdf (equation 158) (Note that some of these references use different names for the four quaternion components or put them with the scalar as the last instead of the first element; you can easily tell the difference in the two forms by seeing if the item in the first row, second column has a ""+"" or a ""-"" sign.) But the following references agree with the matrix given by the Wikipedia article: http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/FORTRAN/spicelib/q2m.html (under ""Associating SPICE Quaternions with Rotation Matrices"") http://books.google.com/books?id=hiBFUv_FT0wC&printsec=frontcover&dq=game+programming+gems+volume+1&hl=en&sa=X&ei=LQ-IUZ7CLKmpyAG66IGIDA&ved=0CEAQ6AEwAQ#v=onepage&q=convert%20quaternion%20to%20rotation%20matrix&f=false (page 202) What is the cause of this difference? Since there are several references that use each of these two matrices, it seems unlikely that it is simply an error in one of the references. Are there different conventions for how to represent rotations by quaternions that would lead to this difference?",,"['matrices', 'rotations', 'quaternions']"
85,Eigenvalues of product of matrices,Eigenvalues of product of matrices,,"If $\mathbf{A}_{n\times n}$ is a positive semi-definite matrix with eigenvalues $\{\alpha_k\},\ k\in\{1,...,n\}$, and $\mathbf{B}_{m\times n}$ is an arbitrary matrix with singular values $\{\beta_k\},\ k\in\{1,...,\min(m,n)\}$, can anything be said about the singular values $\{\gamma_k\},\ k\in\{1,...,\min(m,n)\}$ of the matrix $\mathbf{\Gamma}=\mathbf{BA}$? Is there a way I can relate $\gamma_k$ to $\alpha_k$ and $\beta_k$?","If $\mathbf{A}_{n\times n}$ is a positive semi-definite matrix with eigenvalues $\{\alpha_k\},\ k\in\{1,...,n\}$, and $\mathbf{B}_{m\times n}$ is an arbitrary matrix with singular values $\{\beta_k\},\ k\in\{1,...,\min(m,n)\}$, can anything be said about the singular values $\{\gamma_k\},\ k\in\{1,...,\min(m,n)\}$ of the matrix $\mathbf{\Gamma}=\mathbf{BA}$? Is there a way I can relate $\gamma_k$ to $\alpha_k$ and $\beta_k$?",,['matrices']
86,What does $x^TAx$?,What does ?,x^TAx,What does the following mean? I know that $x^Tx$ is the magnitude of $x$. What does the following formula represent intuitively? $x$ is a vector and $A$ is some scaling matrix. The given is $x^T A x i$ (Ignore the i) I'm learning about positive definiteness.,What does the following mean? I know that $x^Tx$ is the magnitude of $x$. What does the following formula represent intuitively? $x$ is a vector and $A$ is some scaling matrix. The given is $x^T A x i$ (Ignore the i) I'm learning about positive definiteness.,,['matrices']
87,"The average determinant of all integer matrices with coefficients $0,1,2$ [closed]",The average determinant of all integer matrices with coefficients  [closed],"0,1,2","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $S$ denote the set of $A \in M(n,\mathbb R)$ such that every entry of $A$ is either of $0$, $1$ or $2$, then is it true that $$\sum_{A \in S} \det A \ge 3^{n^2}\ ?$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $S$ denote the set of $A \in M(n,\mathbb R)$ such that every entry of $A$ is either of $0$, $1$ or $2$, then is it true that $$\sum_{A \in S} \det A \ge 3^{n^2}\ ?$$",,['matrices']
88,For which classes of matrix can the matrix exponential be easily computed?,For which classes of matrix can the matrix exponential be easily computed?,,"We have diagonal matrices $A = \mbox{diag} (\lambda_1, \ldots, \lambda_n)$ for which matrix exponential has simple form $e^A = \mbox{diag} (e^{\lambda_1}, \ldots, e^{\lambda_n})$, and it can be computed with $\mathcal{O}(n)$ time complexity. There are general algorithms for computing matrix exponential for general matrix, such as Pade Approximation, but they work cubic time in size of a square matrix. I'm interested in finding classes of square matrices for which matrix exponential can be computed not harder than $\mathcal{O}(n^2)$ complexity. Is there any structured matrices that has this property? Any literature/articles suggestions will be appreciated.","We have diagonal matrices $A = \mbox{diag} (\lambda_1, \ldots, \lambda_n)$ for which matrix exponential has simple form $e^A = \mbox{diag} (e^{\lambda_1}, \ldots, e^{\lambda_n})$, and it can be computed with $\mathcal{O}(n)$ time complexity. There are general algorithms for computing matrix exponential for general matrix, such as Pade Approximation, but they work cubic time in size of a square matrix. I'm interested in finding classes of square matrices for which matrix exponential can be computed not harder than $\mathcal{O}(n^2)$ complexity. Is there any structured matrices that has this property? Any literature/articles suggestions will be appreciated.",,"['matrices', 'reference-request', 'exponential-function', 'computational-complexity', 'matrix-exponential']"
89,Is the function $\mbox{tr}(XAX')$ convex?,Is the function  convex?,\mbox{tr}(XAX'),"Let matrix $A$ be symmetric and positive semidefinite (PSD). Is the function $X \mapsto \mbox{tr}(XAX')$ convex? I know that, for a general $A$ , the above trace function is not convex. But for a PSD $A$ , is the function convex?","Let matrix be symmetric and positive semidefinite (PSD). Is the function convex? I know that, for a general , the above trace function is not convex. But for a PSD , is the function convex?",A X \mapsto \mbox{tr}(XAX') A A,"['matrices', 'convex-analysis', 'quadratic-forms', 'trace', 'positive-semidefinite']"
90,How to identify an orthogonal(orthonormal matrix)?,How to identify an orthogonal(orthonormal matrix)?,,"This question was asked in an examination a while back. I was able to solve this question but the computation required was too much. The solution said that the trick to solving this lies in the fact that the product of $P$ with its Transpose is an Identity matrix.(I've come to learn that this is known as an orthogonal matrix). My question is, how can I know something like this while solving the question. Given any Matrix $A$ , is it possible to identify whether it is an orthogonal matrix? PS:I am still in high school, so please use simple words to explain. Thank you.","This question was asked in an examination a while back. I was able to solve this question but the computation required was too much. The solution said that the trick to solving this lies in the fact that the product of $P$ with its Transpose is an Identity matrix.(I've come to learn that this is known as an orthogonal matrix). My question is, how can I know something like this while solving the question. Given any Matrix $A$ , is it possible to identify whether it is an orthogonal matrix? PS:I am still in high school, so please use simple words to explain. Thank you.",,['matrices']
91,Prove that determinant of an odd dimension anti-symmetric matrix is zero,Prove that determinant of an odd dimension anti-symmetric matrix is zero,,"Suppose $A$ is an $(2n+1) \times (2n+1)$ anti-symmetric matrix $(A=-A^T)$. Show that $\det(A)=0$ using Pfaffian formula. Well, in the wiki page, the formula is only defined for matrix with even dimension. So I'm not sure how to proceed. Any help is greatly appreciated.","Suppose $A$ is an $(2n+1) \times (2n+1)$ anti-symmetric matrix $(A=-A^T)$. Show that $\det(A)=0$ using Pfaffian formula. Well, in the wiki page, the formula is only defined for matrix with even dimension. So I'm not sure how to proceed. Any help is greatly appreciated.",,['matrices']
92,How to get Euler angles with respect to initial Euler angle,How to get Euler angles with respect to initial Euler angle,,"I have a sensor which gives me Euler angles (roll,pitch,yaw). There is a baseline value of Euler angle (assume it is $5,10,15$) at the beginning.I want to calibrate from this baseline values all subsequent value. How can I get those values? Is it just subtract $5,10,15$ from all values Or is there any rotational matrix for doing so? As an example if any time the value is $5,10,15$ then it should show $0,0,0$ and on the same way show other angle values with respect to the baseline values.I don't know how to do this. Please advise.","I have a sensor which gives me Euler angles (roll,pitch,yaw). There is a baseline value of Euler angle (assume it is $5,10,15$) at the beginning.I want to calibrate from this baseline values all subsequent value. How can I get those values? Is it just subtract $5,10,15$ from all values Or is there any rotational matrix for doing so? As an example if any time the value is $5,10,15$ then it should show $0,0,0$ and on the same way show other angle values with respect to the baseline values.I don't know how to do this. Please advise.",,"['matrices', 'coordinate-systems', 'rotations', 'project-euler']"
93,"How to multiply vector 3 with 4by4 matrix, more precisely position * transformation matrix","How to multiply vector 3 with 4by4 matrix, more precisely position * transformation matrix",,"All geometry in computer graphics are transformed by position * transform matrix; The issue is the fact that position is a vector with 3 components (x,y,z); And transform matrix is a 4 by 4 with one column that can be dumped(at least in my case). So my transform matrix is now a 3 by 4 matrix: axis x          { x, y, z } axis y          { x, y, z } axis z          { x, y, z } position axis   { x, y, z } multiplied with position vector { x, y ,z } If I dump position axis this can be done with standard formula of matrix multiplication.  But it does not transform it.  I can make the position vector with 4 components { x, y, z, w } but don't know what to do with the w? My only solution is a slow one, put position vector in a new transform matrix in position axis and multiply them. But it is computationally expensive.  How to approach such problem?","All geometry in computer graphics are transformed by position * transform matrix; The issue is the fact that position is a vector with 3 components (x,y,z); And transform matrix is a 4 by 4 with one column that can be dumped(at least in my case). So my transform matrix is now a 3 by 4 matrix: axis x          { x, y, z } axis y          { x, y, z } axis z          { x, y, z } position axis   { x, y, z } multiplied with position vector { x, y ,z } If I dump position axis this can be done with standard formula of matrix multiplication.  But it does not transform it.  I can make the position vector with 4 components { x, y, z, w } but don't know what to do with the w? My only solution is a slow one, put position vector in a new transform matrix in position axis and multiply them. But it is computationally expensive.  How to approach such problem?",,"['geometry', 'matrices', 'computational-geometry', 'transformation', '3d']"
94,Eigenvalues of companion matrix of $4x^3 - 3x^2 + 9x - 1$,Eigenvalues of companion matrix of,4x^3 - 3x^2 + 9x - 1,"I want to find all the roots of a polynomial and decided to compute the eigenvalues of its companion matrix. How do I do that? For example, if I have this polynomial: $4x^3 - 3x^2 + 9x - 1$ , I compute the companion matrix: $$\begin{bmatrix} 0&0&\frac{3}{4} \\ 1&0&-\frac{9}{4} \\ 0&1&\frac{1}{4} \end{bmatrix}$$ Now how can I find the eigenvalues of this matrix? Thanks in advance.","I want to find all the roots of a polynomial and decided to compute the eigenvalues of its companion matrix. How do I do that? For example, if I have this polynomial: , I compute the companion matrix: Now how can I find the eigenvalues of this matrix? Thanks in advance.",4x^3 - 3x^2 + 9x - 1 \begin{bmatrix} 0&0&\frac{3}{4} \\ 1&0&-\frac{9}{4} \\ 0&1&\frac{1}{4} \end{bmatrix},"['matrices', 'polynomials', 'numerical-methods', 'roots', 'companion-matrices']"
95,"Conjugacy classes in $SL(2,q)$: where to find them in a particular book.",Conjugacy classes in : where to find them in a particular book.,"SL(2,q)","NB: This question is tagged with reference-request . It does not require the usual type of context. I am interested in what the conjugacy classes of $SL_2(q)$ are. They can be found in Remark 3 of Harris et al. 's, ""On Conjugacy Classes of $SL_2(q)$ ."" Here is said remark (letting $\mathcal{F}=\Bbb F_q$ ): We can describe the matrix representatives of conjugacy classes in $\mathcal{S}=SL(2,\mathcal{F})$ by four families of types ([6]): $\begin{pmatrix} r & 0 \\ 0 & r\end{pmatrix}$ , where $r\in\mathcal{F}$ and $r^2=1$ . $\begin{pmatrix} r & 0 \\ 0 & s\end{pmatrix}$ , where $r,s\in\mathcal{F}$ and $rs=1$ . $\begin{pmatrix} s & u \\ 0 & s\end{pmatrix}$ , where $s\in\mathcal{F}, s^2=1$ and $u$ is either $1$ or a non-square element of $\mathcal{F}$ , i.e. $u\in \mathcal{F}\setminus\{ x^2: x\in \mathcal{F}\}$ . $\begin{pmatrix} 0 & 1 \\ -1 & w\end{pmatrix}$ , where $w=r+r^q$ and $1=r^{1+q}$ for some $r\in \mathcal{E}\setminus\mathcal{F}$ , where $\mathcal{E}$ is a quadratic extension of $\mathcal{F}$ . That is, any conjugacy class $A^\mathcal{S}$ of $\mathcal S$ must contain one of the above matrices. Here [6] is G. James and M. Liebeck's, Representations and Characters of Groups , Cambridge mathematical textbooks, Cambridge University Press, 2001. My university library can lend me a physical copy of that book within a week (borrowing from a different library, which I believe might cost me a couple of quid); however, I only need this bit; further: it can get me a pdf of any given chapter of the second edition, that doesn't expire. Therefore, my question is: In which chapter of the second edition of said book can I find a proof of the conjugacy classes of $SL(2,q)$ ? This will save me time and money. Of course, you could provide the proof here, but it wouldn't be right asking for that without more context. The closest I could find is this MO post . EDIT: Now that I think about it, some alternatives of where to find a proof would be welcome too!","NB: This question is tagged with reference-request . It does not require the usual type of context. I am interested in what the conjugacy classes of are. They can be found in Remark 3 of Harris et al. 's, ""On Conjugacy Classes of ."" Here is said remark (letting ): We can describe the matrix representatives of conjugacy classes in by four families of types ([6]): , where and . , where and . , where and is either or a non-square element of , i.e. . , where and for some , where is a quadratic extension of . That is, any conjugacy class of must contain one of the above matrices. Here [6] is G. James and M. Liebeck's, Representations and Characters of Groups , Cambridge mathematical textbooks, Cambridge University Press, 2001. My university library can lend me a physical copy of that book within a week (borrowing from a different library, which I believe might cost me a couple of quid); however, I only need this bit; further: it can get me a pdf of any given chapter of the second edition, that doesn't expire. Therefore, my question is: In which chapter of the second edition of said book can I find a proof of the conjugacy classes of ? This will save me time and money. Of course, you could provide the proof here, but it wouldn't be right asking for that without more context. The closest I could find is this MO post . EDIT: Now that I think about it, some alternatives of where to find a proof would be welcome too!","SL_2(q) SL_2(q) \mathcal{F}=\Bbb F_q \mathcal{S}=SL(2,\mathcal{F}) \begin{pmatrix} r & 0 \\ 0 & r\end{pmatrix} r\in\mathcal{F} r^2=1 \begin{pmatrix} r & 0 \\ 0 & s\end{pmatrix} r,s\in\mathcal{F} rs=1 \begin{pmatrix} s & u \\ 0 & s\end{pmatrix} s\in\mathcal{F}, s^2=1 u 1 \mathcal{F} u\in \mathcal{F}\setminus\{ x^2: x\in \mathcal{F}\} \begin{pmatrix} 0 & 1 \\ -1 & w\end{pmatrix} w=r+r^q 1=r^{1+q} r\in \mathcal{E}\setminus\mathcal{F} \mathcal{E} \mathcal{F} A^\mathcal{S} \mathcal S SL(2,q)","['matrices', 'group-theory', 'reference-request']"
96,A = UL factorization [duplicate],A = UL factorization [duplicate],,This question already has an answer here : Using permutation matrix to get LU-Factorization with $A=UL$ (1 answer) Closed 8 years ago . How do I calculate $A=UL$ factorization where $U$ is upper triangular matrix with 1's along the diagonal and $L$ is lower triangular matrix? How is this similar to the $LU$ factorization?,This question already has an answer here : Using permutation matrix to get LU-Factorization with $A=UL$ (1 answer) Closed 8 years ago . How do I calculate $A=UL$ factorization where $U$ is upper triangular matrix with 1's along the diagonal and $L$ is lower triangular matrix? How is this similar to the $LU$ factorization?,,['matrices']
97,Matrix inversion via Levi-Civita symbols,Matrix inversion via Levi-Civita symbols,,"Using Cramer's formula for the inverse of a matrix $M_{ij}$, is it possible to express the entries $(M^{-1})_{ij}$ in terms of the entries $M_{ij}$ using the Levi-Civita symbol and Kronecker deltas? It seems like this should be possible but I can't find such a formula written out anywhere, and if the formula is known I'd rather not have to work out all the algebra by hand.","Using Cramer's formula for the inverse of a matrix $M_{ij}$, is it possible to express the entries $(M^{-1})_{ij}$ in terms of the entries $M_{ij}$ using the Levi-Civita symbol and Kronecker deltas? It seems like this should be possible but I can't find such a formula written out anywhere, and if the formula is known I'd rather not have to work out all the algebra by hand.",,"['matrices', 'inverse', 'tensors']"
98,Does the Schur complement preserve the partial order?,Does the Schur complement preserve the partial order?,,"Let $$\begin{bmatrix} A_{1} &B_1  \\ B_1'  &C_1 \end{bmatrix} \quad \text{and} \quad \begin{bmatrix} A_2 &B_2 \\ B_2'  &C_2 \end{bmatrix}$$ be symmetric positive definite and conformably partitioned matrices. If $$\begin{bmatrix} A_{1} &B_1  \\ B_1'  &C_1 \end{bmatrix}-\begin{bmatrix} A_2 &B_2 \\ B_2'  &C_2 \end{bmatrix}$$ is positive semidefinite, is it true $$(A_1-B_1C^{-1}_1B_1')-(A_2-B_2C^{-1}_2B_2')$$ also positive semidefinite? Here, $X'$ means the transpose of $X$.","Let $$\begin{bmatrix} A_{1} &B_1  \\ B_1'  &C_1 \end{bmatrix} \quad \text{and} \quad \begin{bmatrix} A_2 &B_2 \\ B_2'  &C_2 \end{bmatrix}$$ be symmetric positive definite and conformably partitioned matrices. If $$\begin{bmatrix} A_{1} &B_1  \\ B_1'  &C_1 \end{bmatrix}-\begin{bmatrix} A_2 &B_2 \\ B_2'  &C_2 \end{bmatrix}$$ is positive semidefinite, is it true $$(A_1-B_1C^{-1}_1B_1')-(A_2-B_2C^{-1}_2B_2')$$ also positive semidefinite? Here, $X'$ means the transpose of $X$.",,"['matrices', 'inequality', 'order-theory', 'positive-semidefinite', 'schur-complement']"
99,Derivative of the trace of matrix product $(X^TX)^p$,Derivative of the trace of matrix product,(X^TX)^p,"Let $X$ be a squared matrix, We know that $\frac {\partial tr(X^TX)}{\partial X}$ is $2X$ But how about the case of  $\frac {\partial tr((X^TX)^2)}{\partial X}$ or even  $\frac {\partial tr((X^TX)^p)}{\partial X}$ Is there any generalization? Note that here $(X^TX)^2 = X^TXX^TX$ and similar case applies to $(X^TX)^p$","Let $X$ be a squared matrix, We know that $\frac {\partial tr(X^TX)}{\partial X}$ is $2X$ But how about the case of  $\frac {\partial tr((X^TX)^2)}{\partial X}$ or even  $\frac {\partial tr((X^TX)^p)}{\partial X}$ Is there any generalization? Note that here $(X^TX)^2 = X^TXX^TX$ and similar case applies to $(X^TX)^p$",,"['matrices', 'multivariable-calculus', 'matrix-calculus']"
