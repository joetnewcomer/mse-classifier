,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Vector Calculus Proof,Vector Calculus Proof,,"Preamble: dr/ds≡T where T is a unit vector tangent to a curve, C,  with arc length κ is known as the curvature. It is the proportionality constant defined by T′=κN, where the prime denotes the derivative with respect to arc length. J. (statement) If C is not a straight line, as one moves along C one changes direction, which may be estimated as ΔT∼κNΔs. Also Δθ∼Δs/ρ, ∣ΔT/T∣=|ΔT|∼Δθ must also be true. It follows that ρ=1/κ, I am having trouble figuring out if the above statement J. is true or false. Specifically I know the first two lines of the statement do hold true, however, I am a little unsure of the third line |ΔT / T| = |ΔT|~Δθ. How does |ΔT / T| just reduce down to |ΔT| and why is that approximately equal to Δθ? Thank you so much to whoever can help me with this in advance.","Preamble: dr/ds≡T where T is a unit vector tangent to a curve, C,  with arc length κ is known as the curvature. It is the proportionality constant defined by T′=κN, where the prime denotes the derivative with respect to arc length. J. (statement) If C is not a straight line, as one moves along C one changes direction, which may be estimated as ΔT∼κNΔs. Also Δθ∼Δs/ρ, ∣ΔT/T∣=|ΔT|∼Δθ must also be true. It follows that ρ=1/κ, I am having trouble figuring out if the above statement J. is true or false. Specifically I know the first two lines of the statement do hold true, however, I am a little unsure of the third line |ΔT / T| = |ΔT|~Δθ. How does |ΔT / T| just reduce down to |ΔT| and why is that approximately equal to Δθ? Thank you so much to whoever can help me with this in advance.",,['multivariable-calculus']
1,area under parametric curve,area under parametric curve,,"I have difficulty on how to eliminate parameter especially the equation involved trigonometry equation. The question is asking for the area bounded by the curve , the 2 axes and the line $y=1$ . $x=4 \sin t$ $y=\cot t$ where $t$ is in the range $(0, \pi)$ . I have tried to use the trigonometric formula $1+ \cot^2 t =\csc^2 t$ but I got the wrong answer.The answer given is 4ln[(square root 2)+1].","I have difficulty on how to eliminate parameter especially the equation involved trigonometry equation. The question is asking for the area bounded by the curve , the 2 axes and the line . where is in the range . I have tried to use the trigonometric formula but I got the wrong answer.The answer given is 4ln[(square root 2)+1].","y=1 x=4 \sin t y=\cot t t (0, \pi) 1+ \cot^2 t =\csc^2 t","['integration', 'multivariable-calculus', 'area', 'parametric']"
2,Fundamental Theorem of Calculus from Leibniz Rule Applied to Velocity,Fundamental Theorem of Calculus from Leibniz Rule Applied to Velocity,,"I am trying to simplify Leibniz Rule to the (first) Fundamental Theorem of Calculus (FTC) but believe I am doing so incorrectly. Leibniz rule can be written as: $$\frac{d}{dt} \int_{f(t)}^{g(t)} A(t,\sigma) d\sigma = A(t,g(t))\dot g(t) - A(t,f(t))\dot f(t) + \int_{f(t)}^{g(t)} \frac{\partial}{\partial t} A(t,\sigma) d\sigma \qquad (1)$$ If I set $f(t)=c=const$ and $g(t)=t$ this simplifies to $$\frac{d}{dt} \int_{c}^{t} A(t,\sigma) d\sigma = A(t) + \int_{c}^{t} \frac{\partial}{\partial t} A(t,\sigma) d\sigma \qquad (2)$$ Now if I assume $A$ does not depend on $t$ s.t. $A=A(\sigma)$ then $$\frac{d}{dt} \int_{c}^{t} A(\sigma) d\sigma = A(t) + \int_{c}^{t} \frac{\partial}{\partial t} A(\sigma) d\sigma \qquad (3)$$ which simplifies to $$\frac{d}{dt} \int_{c}^{t} A(\sigma) d\sigma = A(t) \qquad (4)$$ which is the (first) FTC. But what happens if instead of assuming $A$ does not depend on $t$ , we assumed $\sigma=t$ ? We get $$\frac{d}{dt} \int_{c}^{t} A(t) dt= A(t) + \int_{c}^{t} \frac{\partial}{\partial t} A(t) dt \qquad (5)$$ which can be proven incorrect by setting $A(t) = t^2$ yeilding $$t^2=t^2+t^2-c^2=2t^2-c^2 \qquad (6)$$ I don't understand where my error in logic is. Can anyone please help? I'm trying to understand how the (first) FTC applies to functions of time like velocity; i.e. the following should be true $$\frac{d}{dt} \int_{c}^{t} v(t) dt= v(t) \qquad (7)$$ Please let me know if I need to be more specific or clarify anything. Many thanks.","I am trying to simplify Leibniz Rule to the (first) Fundamental Theorem of Calculus (FTC) but believe I am doing so incorrectly. Leibniz rule can be written as: If I set and this simplifies to Now if I assume does not depend on s.t. then which simplifies to which is the (first) FTC. But what happens if instead of assuming does not depend on , we assumed ? We get which can be proven incorrect by setting yeilding I don't understand where my error in logic is. Can anyone please help? I'm trying to understand how the (first) FTC applies to functions of time like velocity; i.e. the following should be true Please let me know if I need to be more specific or clarify anything. Many thanks.","\frac{d}{dt} \int_{f(t)}^{g(t)} A(t,\sigma) d\sigma = A(t,g(t))\dot g(t) - A(t,f(t))\dot f(t) + \int_{f(t)}^{g(t)} \frac{\partial}{\partial t} A(t,\sigma) d\sigma \qquad (1) f(t)=c=const g(t)=t \frac{d}{dt} \int_{c}^{t} A(t,\sigma) d\sigma = A(t) + \int_{c}^{t} \frac{\partial}{\partial t} A(t,\sigma) d\sigma \qquad (2) A t A=A(\sigma) \frac{d}{dt} \int_{c}^{t} A(\sigma) d\sigma = A(t) + \int_{c}^{t} \frac{\partial}{\partial t} A(\sigma) d\sigma \qquad (3) \frac{d}{dt} \int_{c}^{t} A(\sigma) d\sigma = A(t) \qquad (4) A t \sigma=t \frac{d}{dt} \int_{c}^{t} A(t) dt= A(t) + \int_{c}^{t} \frac{\partial}{\partial t} A(t) dt \qquad (5) A(t) = t^2 t^2=t^2+t^2-c^2=2t^2-c^2 \qquad (6) \frac{d}{dt} \int_{c}^{t} v(t) dt= v(t) \qquad (7)","['calculus', 'multivariable-calculus', 'derivatives', 'definite-integrals']"
3,Can we perturb a low rank map to a full rank map in a smooth way?,Can we perturb a low rank map to a full rank map in a smooth way?,,"Let $f:\mathbb{R}^n \to \mathbb{R}^n$ be smooth. Can we find, for every $\epsilon>0$ , a $C^1$ map $\tilde f:\mathbb{R}^n \to \mathbb{R}^n$ of full rank such that $\|df-d\tilde f\|_{C^0}<\epsilon$ ? Note that what I require is weaker than $\|f-\tilde f\|_{C^1}<\epsilon$ , I only want the differentials to be uniformly close. The point is that pointwise, for every real $n \times n$ matrix $A$ , and every $\epsilon>0$ , there exist an invertible matrix $\tilde A$ such that $\|A-\tilde A\|<\epsilon$ . The question is whether we can always create a smooth and exact perturbation. (i.e. I want $d\tilde f$ to be everywhere invertible). I am fine if this perturbation cannot be done on the whole space, but only on a fixed ""part of it""- e.g. I am OK with restricting the domain to be the unit disk. (but I don't want to be forced to shrink the domain to a disk with a radius which depends on $\epsilon$ ).","Let be smooth. Can we find, for every , a map of full rank such that ? Note that what I require is weaker than , I only want the differentials to be uniformly close. The point is that pointwise, for every real matrix , and every , there exist an invertible matrix such that . The question is whether we can always create a smooth and exact perturbation. (i.e. I want to be everywhere invertible). I am fine if this perturbation cannot be done on the whole space, but only on a fixed ""part of it""- e.g. I am OK with restricting the domain to be the unit disk. (but I don't want to be forced to shrink the domain to a disk with a radius which depends on ).",f:\mathbb{R}^n \to \mathbb{R}^n \epsilon>0 C^1 \tilde f:\mathbb{R}^n \to \mathbb{R}^n \|df-d\tilde f\|_{C^0}<\epsilon \|f-\tilde f\|_{C^1}<\epsilon n \times n A \epsilon>0 \tilde A \|A-\tilde A\|<\epsilon d\tilde f \epsilon,"['real-analysis', 'multivariable-calculus', 'matrix-rank', 'perturbation-theory', 'stability-theory']"
4,"For what values of $a$ and $b$ is the function $\frac{x^ay^b}{x^2+y^2}$ continuous at $(0,0)$?",For what values of  and  is the function  continuous at ?,"a b \frac{x^ay^b}{x^2+y^2} (0,0)","I have the function $$f(x,y)=\begin{cases}\dfrac{x^ay^b}{x^2+y^2} &(x,y)\neq(0,0)\\ 0 &(x,y)=(0,0) \end{cases}$$ I am trying to figure out what constants $a$ and $b$ will make the function continuous at $(0,0)$ . I know that the limit has to be $0$ as $(x,y)\to(0,0)$ for it to be continuous. Using polar coordinates, I think that $a+b \geq 3$ since $x^2+y^2= r^2$ and the limit of any polar function $r^x$ with $x > 0$ as $r\to0$ is $0$ . Am I on the right track or am I missing something?","I have the function I am trying to figure out what constants and will make the function continuous at . I know that the limit has to be as for it to be continuous. Using polar coordinates, I think that since and the limit of any polar function with as is . Am I on the right track or am I missing something?","f(x,y)=\begin{cases}\dfrac{x^ay^b}{x^2+y^2} &(x,y)\neq(0,0)\\ 0 &(x,y)=(0,0) \end{cases} a b (0,0) 0 (x,y)\to(0,0) a+b \geq 3 x^2+y^2= r^2 r^x x > 0 r\to0 0","['calculus', 'limits', 'multivariable-calculus', 'continuity']"
5,Easy Integration by Parts in Spherical Coordinates,Easy Integration by Parts in Spherical Coordinates,,"I am trying to use the integration by parts formula in spherical coordinates $$\int_\Omega \frac{\partial u}{\partial x_i} v d \Omega = \int_\Gamma u v \nu_i d\Gamma-\int_\Omega u \frac{\partial v}{\partial x_i} d \Omega, $$ with $\nu$ a unit vector normal to the boundary $\Gamma$ , but I must be misunderstanding something. Toy problem of integrating over unit ball: $$\int_0^{2 \pi} \int_0^\pi \int_0^1 r \cdot 1 (r^2 \sin \theta dr d\theta d\phi)=\pi. $$ Try integration by parts for the above with $\frac{\partial u}{\partial r}=1$ and $v=r$ . The unit normal to the disk is $\nu=\frac{\partial}{\partial r}$ so $\nu_i=1$ . At the boundary, $r=1$ so I obtain $$2\int_0^{2 \pi} \int_0^\pi \int_0^1 r (r^2 \sin \theta dr d\theta d\phi)=\int_{S^2} dS=4\pi.$$ This does not match what the answer should be! Where did I go wrong in trying to use the formula?","I am trying to use the integration by parts formula in spherical coordinates with a unit vector normal to the boundary , but I must be misunderstanding something. Toy problem of integrating over unit ball: Try integration by parts for the above with and . The unit normal to the disk is so . At the boundary, so I obtain This does not match what the answer should be! Where did I go wrong in trying to use the formula?","\int_\Omega \frac{\partial u}{\partial x_i} v d \Omega = \int_\Gamma u v \nu_i d\Gamma-\int_\Omega u \frac{\partial v}{\partial x_i} d \Omega,  \nu \Gamma \int_0^{2 \pi} \int_0^\pi \int_0^1 r \cdot 1 (r^2 \sin \theta dr d\theta d\phi)=\pi.  \frac{\partial u}{\partial r}=1 v=r \nu=\frac{\partial}{\partial r} \nu_i=1 r=1 2\int_0^{2 \pi} \int_0^\pi \int_0^1 r (r^2 \sin \theta dr d\theta d\phi)=\int_{S^2} dS=4\pi.","['multivariable-calculus', 'vector-analysis']"
6,"How to visualize the partial derivation of $f(x,y,z)$ with respect to only one of the axes?",How to visualize the partial derivation of  with respect to only one of the axes?,"f(x,y,z)","given a function $f(x,y)$ , we can easily visualize the partial derivation of $f(x,y)$ with respect to $x$ or $y$ . 1. The output of the function $f(x,y)$ is in the z direction. Just like the output of $f(x)$ is in the y direction. 2. $\frac{\partial }{\partial x }f(x,y)$ can be visualized by thinking that $y$ is constant and $x$ is changing. And with the change of $x$ the output is also changing. (we can think it as the change in height of the graph or simply the change in z direction). then slight change in $z$ direction divided by slight change in $x$ direction is $\frac{\partial }{\partial x }f(x,y)$ . But what about $f(x,y,z)$ ? 1. If I continue to follow the previous examples about visualizing the output on a different dimension (like in $f(x,y)$ the output was in z direction) then I need to think of a fourth dimension. Which in my level, is not possible instantly, and also I don't think it's necessary to solve my problem. 2. Now to think about the partial derivation process, $\frac{\partial }{\partial z }f(x,y,z)$ means $x$ and $y$ both are to be thought as constants. Now with the change of z the output changes. But as I cannot even visualize the change of outputs as mentioned in (no 1), I cannot think of any curve forming like it did in case of $f(x,y)$ (If we pick a certainly value of $y$ , say $y = 1$ , and move to $x = 0$ (say) to $x = 7$ (say) and pinpoint the outputs in the $z$ direction, then adding all those points will give us a curve, which can be thought of a simple two dimensional curve and the rate of change of this very curve is what we call the partial derivative of $f(x,y)$ with respect to x.) Now my question is simply, how do I visualize taking the partial derivative of functions that include all of the $ x, y, z $ variables?","given a function , we can easily visualize the partial derivation of with respect to or . 1. The output of the function is in the z direction. Just like the output of is in the y direction. 2. can be visualized by thinking that is constant and is changing. And with the change of the output is also changing. (we can think it as the change in height of the graph or simply the change in z direction). then slight change in direction divided by slight change in direction is . But what about ? 1. If I continue to follow the previous examples about visualizing the output on a different dimension (like in the output was in z direction) then I need to think of a fourth dimension. Which in my level, is not possible instantly, and also I don't think it's necessary to solve my problem. 2. Now to think about the partial derivation process, means and both are to be thought as constants. Now with the change of z the output changes. But as I cannot even visualize the change of outputs as mentioned in (no 1), I cannot think of any curve forming like it did in case of (If we pick a certainly value of , say , and move to (say) to (say) and pinpoint the outputs in the direction, then adding all those points will give us a curve, which can be thought of a simple two dimensional curve and the rate of change of this very curve is what we call the partial derivative of with respect to x.) Now my question is simply, how do I visualize taking the partial derivative of functions that include all of the variables?","f(x,y) f(x,y) x y f(x,y) f(x) \frac{\partial }{\partial x }f(x,y) y x x z x \frac{\partial }{\partial x }f(x,y) f(x,y,z) f(x,y) \frac{\partial }{\partial z }f(x,y,z) x y f(x,y) y y = 1 x = 0 x = 7 z f(x,y)  x, y, z ",['multivariable-calculus']
7,Why is the volume element unique in the way Spivak develops it?,Why is the volume element unique in the way Spivak develops it?,,"In Spivak's Calculus on Manifolds, he develops the volume element in the following way: The fact that $\dim \Lambda^n(\mathbb{R}^n) = 1$ is probably not new to you, since det is often defined as the unique element $\omega \in \Lambda^n(\mathbb{R}^n)$ such that $\omega (e_1, \ldots, e_n) = 1$ . For a general vector space $V$ there is no extra criterion of this sort to distinguish a particular $\omega \in \Lambda^n(V)$ . Suppose, however, that an inner product $T$ for $V$ is given. If $v_1, \ldots, v_n$ and $w_1, \ldots, w_n$ are two bases which are orthonormal with respect to $T$ , and the matrix $A = (a_{ij})$ is defined by $w_i = \sum_{j=1} ^n a_{ij}v_j,$ then $$\delta_{ij} = T(w_i, w_j) = \sum_{k,l = 1} ^n a_{ik}a_{jl}T(v_k,v_l) =\sum_{k=1} ^na_{ik}a_{jk}.$$ In other words, if $A^T$ denotes the transpose of the matrix $A$ , then we have $A \cdot A^T = I$ , so $\det A = \pm 1$ . It follows from Theorem 4-6 (stated below) that if $\omega \in \Lambda^n(V)$ satisfies $\omega (v_1, \ldots, v_n) = \pm 1$ , then $\omega (w_1, \ldots, w_n) = \pm 1$ . If an orientation $\mu$ for $V$ has also been given, it follows that there is a unique $\omega \in \Lambda^n(V)$ such that $\omega (v_1,\ldots, v_n) = 1$ whenever $v_1, \ldots,v_n$ is an orthonormal basis such that $[v_1, \ldots, v_n] = \mu.$ This unique $\omega$ is called the volume element of $V$ , determined by the inner product $T$ and orientation $\mu$ . Theorem 4-6: Let $v_1, \ldots, v_n$ be a basis for $V$ , and let $\omega \in \Lambda^n (V)$ . If $w_i = \sum_{j = 1}^na_{ij}v_j$ are $n$ vectors in $V$ , then $$\omega (w_1, \ldots, w_n) = det (a_{ij}) \cdot \omega (v_1, \ldots, v_n)$$ Two questions: Why is $\omega$ unique? Is it because $\dim \Lambda^n(V) = 1$ so every other element is $c \cdot \omega$ for some $c$ ? What exactly is the role of the inner product in determining the volume element? Is it so that an orthonormal basis can be determined? Why are orthonormal bases important in this development?","In Spivak's Calculus on Manifolds, he develops the volume element in the following way: The fact that is probably not new to you, since det is often defined as the unique element such that . For a general vector space there is no extra criterion of this sort to distinguish a particular . Suppose, however, that an inner product for is given. If and are two bases which are orthonormal with respect to , and the matrix is defined by then In other words, if denotes the transpose of the matrix , then we have , so . It follows from Theorem 4-6 (stated below) that if satisfies , then . If an orientation for has also been given, it follows that there is a unique such that whenever is an orthonormal basis such that This unique is called the volume element of , determined by the inner product and orientation . Theorem 4-6: Let be a basis for , and let . If are vectors in , then Two questions: Why is unique? Is it because so every other element is for some ? What exactly is the role of the inner product in determining the volume element? Is it so that an orthonormal basis can be determined? Why are orthonormal bases important in this development?","\dim \Lambda^n(\mathbb{R}^n) = 1 \omega \in \Lambda^n(\mathbb{R}^n) \omega (e_1, \ldots, e_n) = 1 V \omega \in \Lambda^n(V) T V v_1, \ldots, v_n w_1, \ldots, w_n T A = (a_{ij}) w_i = \sum_{j=1} ^n a_{ij}v_j, \delta_{ij} = T(w_i, w_j) = \sum_{k,l = 1} ^n a_{ik}a_{jl}T(v_k,v_l) =\sum_{k=1} ^na_{ik}a_{jk}. A^T A A \cdot A^T = I \det A = \pm 1 \omega \in \Lambda^n(V) \omega (v_1, \ldots, v_n) = \pm 1 \omega (w_1, \ldots, w_n) = \pm 1 \mu V \omega \in \Lambda^n(V) \omega (v_1,\ldots, v_n) = 1 v_1, \ldots,v_n [v_1, \ldots, v_n] = \mu. \omega V T \mu v_1, \ldots, v_n V \omega \in \Lambda^n (V) w_i = \sum_{j = 1}^na_{ij}v_j n V \omega (w_1, \ldots, w_n) = det (a_{ij}) \cdot \omega (v_1, \ldots, v_n) \omega \dim \Lambda^n(V) = 1 c \cdot \omega c","['linear-algebra', 'multivariable-calculus', 'tensors', 'volume', 'exterior-algebra']"
8,Flux through region : paraboloid and sphere,Flux through region : paraboloid and sphere,,"I have this region : $D=\{(x,y,z)\mid y^2+z^2\le 3|x|,(x-2)^2+y^2+z^2 \le 4\}$ with vector field $\mathbf F=(-2x,-2y,xy)$ I can use the divergence theorem : $\mathrm{div}(\mathbf F)=-4$ Attempt : Let's see where they intersect : $(x-2)^2+3x=4 \implies x^2-x=0 \implies x=0,1$ In order to find the radius of integration I can plug $x=1$ into $y^2+z^2\le 3x$ . so radius is $r=\sqrt{3}$ Now It's time to get the $x$ range : $(x-2)^2+y^2+z^2 = 4 \implies x^2-4x+y^2+z^2=0 \implies x_{1,2}=\frac{4\pm\sqrt{16-4(y^2+z^2)}}{2}$ I take the positive one. $x\in\left[\frac{y^2+z^2}{3},2+2\sqrt{4-y^2-z^2}\right]$ Integration cylindrical coordinates : $x\in\left[\frac{r^2}{3},2+2\sqrt{4-r^2}\right]$ $\theta\in[0,2\pi]$ $r\in[0,\sqrt{3}]$ The final integral is : $$\text{Flux} = -4\int_{0}^{2\pi}\int_{0}^{\sqrt{3}}\int_{\frac{r^2}{3}}^{2+2\sqrt{4-r^2}}r\,\mathrm dx\,\mathrm dr\,\mathrm d\theta=...=-\frac{166}{3}\pi$$ Is my Integral set-up right ? UPDATE : I think I can also split the region into two parts : the first one is the paraboloid with $x \in [0,1]$ and the second one in the sphere with $x \in [1,4]$ $\int_{0}^{2\pi}\int_{0}^{\sqrt{3}}\int_{\frac{r^2}{3}}^{1}rdxdrd\theta=\frac{3}{2}\pi$ $\int_{1}^{4}(\sqrt{4x-4x^2})^2\pi dx=9\pi$ so Flux = $-4(\frac{3}{2}\pi+9\pi)=-42\pi$ ?",I have this region : with vector field I can use the divergence theorem : Attempt : Let's see where they intersect : In order to find the radius of integration I can plug into . so radius is Now It's time to get the range : I take the positive one. Integration cylindrical coordinates : The final integral is : Is my Integral set-up right ? UPDATE : I think I can also split the region into two parts : the first one is the paraboloid with and the second one in the sphere with so Flux = ?,"D=\{(x,y,z)\mid y^2+z^2\le 3|x|,(x-2)^2+y^2+z^2 \le 4\} \mathbf F=(-2x,-2y,xy) \mathrm{div}(\mathbf F)=-4 (x-2)^2+3x=4 \implies x^2-x=0 \implies x=0,1 x=1 y^2+z^2\le 3x r=\sqrt{3} x (x-2)^2+y^2+z^2 = 4 \implies x^2-4x+y^2+z^2=0 \implies x_{1,2}=\frac{4\pm\sqrt{16-4(y^2+z^2)}}{2} x\in\left[\frac{y^2+z^2}{3},2+2\sqrt{4-y^2-z^2}\right] x\in\left[\frac{r^2}{3},2+2\sqrt{4-r^2}\right] \theta\in[0,2\pi] r\in[0,\sqrt{3}] \text{Flux} = -4\int_{0}^{2\pi}\int_{0}^{\sqrt{3}}\int_{\frac{r^2}{3}}^{2+2\sqrt{4-r^2}}r\,\mathrm dx\,\mathrm dr\,\mathrm d\theta=...=-\frac{166}{3}\pi x \in [0,1] x \in [1,4] \int_{0}^{2\pi}\int_{0}^{\sqrt{3}}\int_{\frac{r^2}{3}}^{1}rdxdrd\theta=\frac{3}{2}\pi \int_{1}^{4}(\sqrt{4x-4x^2})^2\pi dx=9\pi -4(\frac{3}{2}\pi+9\pi)=-42\pi","['integration', 'multivariable-calculus', 'proof-verification']"
9,show this inequality to with $n$ variables inequality,show this inequality to with  variables inequality,n,"Let $a_{i}\in R(i=1,2,\cdots,n),n\ge 3$ . such that $a_{i}\ge \dfrac{n}{2-n},\forall i=1,2,\cdots,n$ ,and $$a_{1}+a_{2}+\cdots+a_{n}=n$$ show that $$\sum_{i=1}^{n}\dfrac{1}{a^2_{i}}\ge\sum_{i=1}^{n}\dfrac{1}{a_{i}}$$ my attempt if $n=3$ ,then $a_{1}+a_{2}+a_{3}=3,a_{i}>-3$ ,so we must prove $$\sum (a_{1}a_{2})^2\ge \sum (a_{1}a_{2})$$ since $$\sum(a_{1}a_{2})^2\ge a_{1}a_{2}a_{3}(a_{1}+a_{2}+a_{3})=3a_{1}a_{2}a_{3}$$","Let . such that ,and show that my attempt if ,then ,so we must prove since","a_{i}\in R(i=1,2,\cdots,n),n\ge 3 a_{i}\ge \dfrac{n}{2-n},\forall i=1,2,\cdots,n a_{1}+a_{2}+\cdots+a_{n}=n \sum_{i=1}^{n}\dfrac{1}{a^2_{i}}\ge\sum_{i=1}^{n}\dfrac{1}{a_{i}} n=3 a_{1}+a_{2}+a_{3}=3,a_{i}>-3 \sum (a_{1}a_{2})^2\ge \sum (a_{1}a_{2}) \sum(a_{1}a_{2})^2\ge a_{1}a_{2}a_{3}(a_{1}+a_{2}+a_{3})=3a_{1}a_{2}a_{3}","['real-analysis', 'multivariable-calculus', 'inequality']"
10,General quadratic functions in two variables,General quadratic functions in two variables,,"Consider the quadratic function $2x^2-4xy+y^2-3x+4y$ . This can be expressed as $2(x-5/4)^2+(y-1/2)^2-4(x-5/4)(y-1/2)-7/8$ Is there any advantage of expressing in the latter form? Are there some features of the function that become apparent by looking at the second expression? In other words, why would we ever want to write the function in the other manner?","Consider the quadratic function . This can be expressed as Is there any advantage of expressing in the latter form? Are there some features of the function that become apparent by looking at the second expression? In other words, why would we ever want to write the function in the other manner?",2x^2-4xy+y^2-3x+4y 2(x-5/4)^2+(y-1/2)^2-4(x-5/4)(y-1/2)-7/8,['multivariable-calculus']
11,Show that taylor expansion,Show that taylor expansion,,"Let $M$ be a spherically symmetric $C^2$ manifold. Consider the open ball $B_{R}$ , centered in $x\in M$ .  Let $y\in B_{R}$ , then we define by $\rho$ the geodesic distance, beetwen $x$ and $y$ . I don't see the term $-\frac{1}{2}h_{p}$ . Sorry if this is very imprecise, but my point is that I do not understand the indexes of the derivatives of $(*)$ , but for me it is not correct that in the derivative the same argument of the function appears and even more multiplying (for instance $h_{p}(p,\theta)p$ ). Thanks!!!!","Let be a spherically symmetric manifold. Consider the open ball , centered in .  Let , then we define by the geodesic distance, beetwen and . I don't see the term . Sorry if this is very imprecise, but my point is that I do not understand the indexes of the derivatives of , but for me it is not correct that in the derivative the same argument of the function appears and even more multiplying (for instance ). Thanks!!!!","M C^2 B_{R} x\in M y\in B_{R} \rho x y -\frac{1}{2}h_{p} (*) h_{p}(p,\theta)p","['multivariable-calculus', 'taylor-expansion']"
12,Tangent plane to a surface that is parallel to another plane,Tangent plane to a surface that is parallel to another plane,,"Given: surface S: $x^2+y^2+4z^2=16$ plane $\pi$ : $x+y+2\sqrt2z=10$ Find all points (a,b,c) such that tangent plane to S at (a,b,c) is parallel to $\pi$ Find all tangent planes to S parallel to $\pi$ My knowledge: I realise that the given surface is an ellipsoid and the given surface is flat, so I'm expecting 2 such points. I know that 2 surfaces are parallel if their gradients are parallel. I know that 2 vectors are parallel if they are scalar multiples of each other. My attempted solution: $grad(\pi) = [1,1,2\sqrt2]$ $grad(S) = [2x,2y,8z]$ How to proceed?","Given: surface S: plane : Find all points (a,b,c) such that tangent plane to S at (a,b,c) is parallel to Find all tangent planes to S parallel to My knowledge: I realise that the given surface is an ellipsoid and the given surface is flat, so I'm expecting 2 such points. I know that 2 surfaces are parallel if their gradients are parallel. I know that 2 vectors are parallel if they are scalar multiples of each other. My attempted solution: How to proceed?","x^2+y^2+4z^2=16 \pi x+y+2\sqrt2z=10 \pi \pi grad(\pi) = [1,1,2\sqrt2] grad(S) = [2x,2y,8z]","['geometry', 'multivariable-calculus']"
13,Understanding what ij mean in a Double Riemann Sum (Double Integral),Understanding what ij mean in a Double Riemann Sum (Double Integral),,"I am having trouble understanding what the ( $x^*_{ij} $ , $y^*_{ij}$ ) in this diagram (circled in blue) is explaining. What I do know is that $i$ is the iteration of the $x$ Riemann Sum and the $j$ is the iteration for the $y$ Riemann Sum. What I am having an issue with is understanding why $x$ needs the $j$ and $y$ needs the $i$ . Couldn't the $P$ simply be represented as ( $x^*_i$ , $y^*_j$ )?","I am having trouble understanding what the ( , ) in this diagram (circled in blue) is explaining. What I do know is that is the iteration of the Riemann Sum and the is the iteration for the Riemann Sum. What I am having an issue with is understanding why needs the and needs the . Couldn't the simply be represented as ( , )?",x^*_{ij}  y^*_{ij} i x j y x j y i P x^*_i y^*_j,"['integration', 'multivariable-calculus', 'definite-integrals', 'riemann-sum']"
14,changing the order of Integration weirdly,changing the order of Integration weirdly,,"So far what I understood is if your limits are some constant, you can find the volume under a function f(x,y) is $\begin{equation} \int_{c}^{d} \int_{a}^{b}f(x,y) \,dx\,dy \end{equation}$ and it is equal to $\begin{equation} \int_{a}^{b} \int_{c}^{d}f(x,y) \,dy\,dx \end{equation}$ And if you have a variable limit its not always equal. But I came across a problem which looks like this: $\begin{equation} \int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dx\,dy \end{equation}$ And here I am completely lost. I thought the question doesn't make any sense. As you can see the x limit is from x to x+2. why the x limit is not in y? How can I even draw the limit of x and x+2 in x-axis? But in textbook they simply solved it by changing the order of Integration. $\begin{equation} \int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dx\,dy \end{equation}$ = $\begin{equation} \int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dy\,dx \end{equation}$ And now the question looks normal. But can we just change the dx and dy anytime we want? I am completely lost here. Please help me! Here is the actual question and the answer in a handwritten note that my teacher sent me.","So far what I understood is if your limits are some constant, you can find the volume under a function f(x,y) is and it is equal to And if you have a variable limit its not always equal. But I came across a problem which looks like this: And here I am completely lost. I thought the question doesn't make any sense. As you can see the x limit is from x to x+2. why the x limit is not in y? How can I even draw the limit of x and x+2 in x-axis? But in textbook they simply solved it by changing the order of Integration. = And now the question looks normal. But can we just change the dx and dy anytime we want? I am completely lost here. Please help me! Here is the actual question and the answer in a handwritten note that my teacher sent me.","\begin{equation}
\int_{c}^{d} \int_{a}^{b}f(x,y) \,dx\,dy
\end{equation} \begin{equation}
\int_{a}^{b} \int_{c}^{d}f(x,y) \,dy\,dx
\end{equation} \begin{equation}
\int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dx\,dy
\end{equation} \begin{equation}
\int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dx\,dy
\end{equation} \begin{equation}
\int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dy\,dx
\end{equation}","['integration', 'multivariable-calculus']"
15,Help understanding a little Jacobians lemma,Help understanding a little Jacobians lemma,,"This was used as lemma in a bigger proof. Let $A ⊆ ℝ^m$ be an open set, $f = (f_1,…,f_m) : A → ℝ^m$ a $C^1$ function, and $p ∈ A$ . If $det[Df(p)] ≠ 0$ , then there is an open ball $B ⊆ A$ of center $p$ such that $f|_B$ is injective. proof: The function $x ↦ det[Df(x)]$ is continuous, so there is an open ball $B ⊆ A$ , of center $p$ , such that $x ∈ B ⇒ det[Df(x)] ≠ 0$ . Let $a$ and $b$ be two points of $B$ . By the mean value theorem we have $f_i(b) - f_i(a) = Df_i(c_i)⋅(b-a)$ for some $c_i ∈ \{ta + (1-t)b : t \in [0,1]\}$ , this means that $$f(b) - f(a) = \begin{bmatrix}Df_1(c_1)\\\vdots\\Df_m(c_m)\end{bmatrix}⋅(b-a).$$ But $det[Df(c_i)] ≠ 0$ so $f(b) = f(a) ⇒ b = a$ . The last step is where I have troubles, it seems to me that we need $$det\begin{bmatrix}Df_1(c_1)\\\vdots\\Df_m(c_m)\end{bmatrix} ≠ 0$$ and it's not clear how this follows from $det[Df(c_i)] ≠ 0$ .","This was used as lemma in a bigger proof. Let be an open set, a function, and . If , then there is an open ball of center such that is injective. proof: The function is continuous, so there is an open ball , of center , such that . Let and be two points of . By the mean value theorem we have for some , this means that But so . The last step is where I have troubles, it seems to me that we need and it's not clear how this follows from .","A ⊆ ℝ^m f = (f_1,…,f_m) : A → ℝ^m C^1 p ∈ A det[Df(p)] ≠ 0 B ⊆ A p f|_B x ↦ det[Df(x)] B ⊆ A p x ∈ B ⇒ det[Df(x)] ≠ 0 a b B f_i(b) - f_i(a) = Df_i(c_i)⋅(b-a) c_i ∈ \{ta + (1-t)b : t \in [0,1]\} f(b) - f(a) = \begin{bmatrix}Df_1(c_1)\\\vdots\\Df_m(c_m)\end{bmatrix}⋅(b-a). det[Df(c_i)] ≠ 0 f(b) = f(a) ⇒ b = a det\begin{bmatrix}Df_1(c_1)\\\vdots\\Df_m(c_m)\end{bmatrix} ≠ 0 det[Df(c_i)] ≠ 0","['calculus', 'linear-algebra', 'multivariable-calculus']"
16,Partial derivatives are 0 iff the function is constant,Partial derivatives are 0 iff the function is constant,,"Let $f:\mathbb R^2\rightarrow \mathbb R$ has first partial derivatives and $\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}=0, \text{for all } (x,y)\in\mathbb R^2$ . Show that $f$ is constant. (Hint: Show that the restriction of $f$ to a line parallel to one of the coordinate axes is constant). My attempt: Following the hint, consider the restriction of $f$ to a line $y=c$ , then $\frac{\partial f}{\partial x} (x,c) = f_x(x,c)=0$ . Pick 2 points a and b, then by mean value theorem there is $x_0$ , such that $f_x(x,c) = \frac{f(b,c)-f(a,c)}{b-a}=0 \implies f(b,c)-f(a,c)=0 \implies f(b,c)=f(a,c)\implies f(x,c)=const.$ We can show the same way that $f(c,y)=const$ . Am I thinking in the right direction? If so, can I just combine $f(x,c)=const$ and $f(c,y)=const$ to get $f(x,y)=const$ ? Did I miss something?","Let has first partial derivatives and . Show that is constant. (Hint: Show that the restriction of to a line parallel to one of the coordinate axes is constant). My attempt: Following the hint, consider the restriction of to a line , then . Pick 2 points a and b, then by mean value theorem there is , such that We can show the same way that . Am I thinking in the right direction? If so, can I just combine and to get ? Did I miss something?","f:\mathbb R^2\rightarrow \mathbb R \frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}=0, \text{for all } (x,y)\in\mathbb R^2 f f f y=c \frac{\partial f}{\partial x} (x,c) = f_x(x,c)=0 x_0 f_x(x,c) = \frac{f(b,c)-f(a,c)}{b-a}=0 \implies f(b,c)-f(a,c)=0 \implies f(b,c)=f(a,c)\implies f(x,c)=const. f(c,y)=const f(x,c)=const f(c,y)=const f(x,y)=const","['real-analysis', 'multivariable-calculus', 'partial-derivative']"
17,"Continuity of partial derivatives at (0,0)","Continuity of partial derivatives at (0,0)",,"For the function $$f = \begin {cases} \frac{xy}{x^2+y^2}, \text{if } (x,y) \neq (0,0) \\ 0, \text{if } (x,y) = (0,0) \end {cases}$$ show that $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are not continuous at (0,0). My attempt: I found $$\frac{\partial f}{\partial x}(x,y)=\frac{y^3-x^2y}{(x^2+y^2)^2}$$ At $(x,y)=(0,0)$ $$\frac{\partial f}{\partial x}(0,0)=\lim_{t\rightarrow 0}\frac{f(0+t,0)-f(0,0)}{t}=0.$$ Similarly $\frac{\partial f}{\partial y}(0,0)=0$ Then $$\lim_{(x,y)\rightarrow(0,0)}\frac{\partial f}{\partial x}(x,y)=\lim_{(x,y)\rightarrow(0,0)}\frac{y^3-x^2y}{(x^2+y^2)^2}= \text{ (consider restriction to the line } x = my) = \lim_{y\rightarrow 0}\frac{y^3-m^2y^3}{(m^2y^2+y^2)^2}=\lim_{y\rightarrow 0}\frac{y^3(1-m^2)}{(m^2+1)^2y^4}=\infty \neq \frac{\partial f}{\partial x}(0,0).$$ Thus $\frac{\partial f}{\partial x}$ is not continuous at (0,0). Similarly, for $\frac{\partial f}{\partial y}.$ Is it correct? Also is it sufficient to show the inequality of the limit and the value of the partial, considering only one restriction to the line (in this case $x=my$ )? Or should we consider other restrictions as well?","For the function show that and are not continuous at (0,0). My attempt: I found At Similarly Then Thus is not continuous at (0,0). Similarly, for Is it correct? Also is it sufficient to show the inequality of the limit and the value of the partial, considering only one restriction to the line (in this case )? Or should we consider other restrictions as well?","f = \begin {cases} \frac{xy}{x^2+y^2}, \text{if } (x,y) \neq (0,0) \\ 0, \text{if } (x,y) = (0,0) \end {cases} \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} \frac{\partial f}{\partial x}(x,y)=\frac{y^3-x^2y}{(x^2+y^2)^2} (x,y)=(0,0) \frac{\partial f}{\partial x}(0,0)=\lim_{t\rightarrow 0}\frac{f(0+t,0)-f(0,0)}{t}=0. \frac{\partial f}{\partial y}(0,0)=0 \lim_{(x,y)\rightarrow(0,0)}\frac{\partial f}{\partial x}(x,y)=\lim_{(x,y)\rightarrow(0,0)}\frac{y^3-x^2y}{(x^2+y^2)^2}= \text{ (consider restriction to the line } x = my) = \lim_{y\rightarrow 0}\frac{y^3-m^2y^3}{(m^2y^2+y^2)^2}=\lim_{y\rightarrow 0}\frac{y^3(1-m^2)}{(m^2+1)^2y^4}=\infty \neq \frac{\partial f}{\partial x}(0,0). \frac{\partial f}{\partial x} \frac{\partial f}{\partial y}. x=my","['real-analysis', 'multivariable-calculus', 'partial-derivative']"
18,Understanding submersions in differential topology,Understanding submersions in differential topology,,"I'm having trouble understanding an example in Guillemin and Pollack. If $f:R^k\rightarrow R$ be defined by $f(x) = |x|^2 = x_1^2+...+x_k^2$ The derivative $df_x$ at the point $a = (a_1,..,a_k)$ has matrix $(2a_1,..,2a_k)$ . Thus $df_a:R^k\rightarrow R$ is surjective unless $f(a)=0$ , so every nonzero real number is a regular value of $f$ . I can't understand how to check $df_a$ is surjective and why it is obvious that every nonzero real number is a regular value of $f$ ? A hint is appreciated. Thanks.","I'm having trouble understanding an example in Guillemin and Pollack. If be defined by The derivative at the point has matrix . Thus is surjective unless , so every nonzero real number is a regular value of . I can't understand how to check is surjective and why it is obvious that every nonzero real number is a regular value of ? A hint is appreciated. Thanks.","f:R^k\rightarrow R f(x) = |x|^2 = x_1^2+...+x_k^2 df_x a = (a_1,..,a_k) (2a_1,..,2a_k) df_a:R^k\rightarrow R f(a)=0 f df_a f","['multivariable-calculus', 'differential-geometry']"
19,Finding an $\epsilon-\delta$ proof for a multivariable limit.,Finding an  proof for a multivariable limit.,\epsilon-\delta,"Suppose $f:\mathbb{R}^2\rightarrow \mathbb{R}$ is defined as $$(x,y) \longmapsto \left\{ \begin{array}{cl}       \dfrac{4x^2y^3 +x^4y - y^5}{(x^2+y^2)^2} & \mbox {if } (x,y) \neq (0,0) \\ \\       0 & \mbox {if } (x,y) = (0,0)    \end{array} \right.  $$ I have shown that $f$ is continuous at $(0,0)$ using polar coordinates, but I am really trying to improve my $\epsilon-\delta$ proofs for such limits. I always end up getting confused with the inequalities. (Sorry if the question is a bit repetitive here) So for $\epsilon > 0$ I need to find a $\delta$ such that $\|(x,y)\| =\sqrt{x^2 + y^2} < \delta$ implies $|f(x,y)| < \epsilon$ I have tried to work backwards using the inequality $(x^2+y^2)^2 \geq 4x^2y^2$ $$\begin{align*} \left|\frac{4x^2y^3 +x^4y - y^5}{(x^2+y^2)^2}\right|&=\left|\frac{4x^2y^3}{(x^2+y^2)^2} + \frac{x^4 y}{(x^2+y^2)^2} - \frac{y^5}{(x^2+y^2)^2}\right|\\ &\leq\left|\frac{4x^2y^3}{(x^2+y^2)^2}\right| + \left|\frac{x^4 y}{(x^2+y^2)^2}\right| + \left|\frac{y^5}{(x^2+y^2)^2}\right|\\ &\leq\left|\frac{4x^2y^3}{4x^2y^2}\right| + \left|\frac{x^4 y}{4x^2y^2}\right| + \left|\frac{y^5}{4x^2y^2}\right|\\ &= \left|y\right| + \left|\frac{x^2}{4y}\right| + \left|\frac{y^3}{4x^2}\right| \end{align*}$$ Got stuck here and not sure if my approach is correct.","Suppose is defined as I have shown that is continuous at using polar coordinates, but I am really trying to improve my proofs for such limits. I always end up getting confused with the inequalities. (Sorry if the question is a bit repetitive here) So for I need to find a such that implies I have tried to work backwards using the inequality Got stuck here and not sure if my approach is correct.","f:\mathbb{R}^2\rightarrow \mathbb{R} (x,y) \longmapsto
\left\{
\begin{array}{cl}
      \dfrac{4x^2y^3 +x^4y - y^5}{(x^2+y^2)^2} & \mbox {if } (x,y) \neq (0,0) \\
\\
      0 & \mbox {if } (x,y) = (0,0)
   \end{array}
\right.
  f (0,0) \epsilon-\delta \epsilon > 0 \delta \|(x,y)\| =\sqrt{x^2 + y^2} < \delta |f(x,y)| < \epsilon (x^2+y^2)^2 \geq 4x^2y^2 \begin{align*}
\left|\frac{4x^2y^3 +x^4y - y^5}{(x^2+y^2)^2}\right|&=\left|\frac{4x^2y^3}{(x^2+y^2)^2} + \frac{x^4 y}{(x^2+y^2)^2} - \frac{y^5}{(x^2+y^2)^2}\right|\\
&\leq\left|\frac{4x^2y^3}{(x^2+y^2)^2}\right| + \left|\frac{x^4 y}{(x^2+y^2)^2}\right| + \left|\frac{y^5}{(x^2+y^2)^2}\right|\\
&\leq\left|\frac{4x^2y^3}{4x^2y^2}\right| + \left|\frac{x^4 y}{4x^2y^2}\right| + \left|\frac{y^5}{4x^2y^2}\right|\\
&= \left|y\right| + \left|\frac{x^2}{4y}\right| + \left|\frac{y^3}{4x^2}\right|
\end{align*}","['real-analysis', 'limits', 'multivariable-calculus', 'epsilon-delta']"
20,Does a change of variable affect the function,Does a change of variable affect the function,,"If I have a function $f: \mathbb{R}^2 \to \mathbb{R}$ then I can get $f$ in polar coordinates by doing : $f \circ g(r, \theta)$ where $g(r, \theta) = (r \cos \theta, r \sin \theta)$ . Now my question is the following : Does the shape of the graph of $f$ in the cartesian plane will look exactly the same as the shape of $f \circ g(r, \theta)$ drawn in the polar coordinates plane ? I am saking this question because for example if I take a function $h : \mathbb{R} \to \mathbb{R}$ then the shape of $h(x)$ doesn't look like the sape of $h(2x)$ . So for me it's not clear that if I transform $f$ to get the function in polar coordinates then in the cartesian plane we calculate : $f \circ g(\sqrt{x^2+y^2}, Arctan(y/x))$ or we calculate $f \circ g (x, y)$ . Because in the second interpretation the functions aren't the same. For example $h(x)$ and $h(2x)$ aren't the same functions. Thank you !",If I have a function then I can get in polar coordinates by doing : where . Now my question is the following : Does the shape of the graph of in the cartesian plane will look exactly the same as the shape of drawn in the polar coordinates plane ? I am saking this question because for example if I take a function then the shape of doesn't look like the sape of . So for me it's not clear that if I transform to get the function in polar coordinates then in the cartesian plane we calculate : or we calculate . Because in the second interpretation the functions aren't the same. For example and aren't the same functions. Thank you !,"f: \mathbb{R}^2 \to \mathbb{R} f f \circ g(r, \theta) g(r, \theta) = (r \cos \theta, r \sin \theta) f f \circ g(r, \theta) h : \mathbb{R} \to \mathbb{R} h(x) h(2x) f f \circ g(\sqrt{x^2+y^2}, Arctan(y/x)) f \circ g (x, y) h(x) h(2x)","['real-analysis', 'calculus', 'multivariable-calculus', 'polar-coordinates']"
21,Existence of the Limit of a Two-Variable Function,Existence of the Limit of a Two-Variable Function,,"This problem is an example in my calculus textbook. Let: $$ f(x,y)=\frac{y^2\sin^2(x)}{x^4+y^4} $$ My textbook says that $$ \lim_{(x,y)\to(0,0)}f(x,y) \text{ does not exist.} $$ Questions: How do we know the limit does not exist? In general, suppose the limit of a function exists but we do not know the value of such limit, how do we find it?","This problem is an example in my calculus textbook. Let: My textbook says that Questions: How do we know the limit does not exist? In general, suppose the limit of a function exists but we do not know the value of such limit, how do we find it?","
f(x,y)=\frac{y^2\sin^2(x)}{x^4+y^4}
 
\lim_{(x,y)\to(0,0)}f(x,y) \text{ does not exist.}
","['limits', 'multivariable-calculus']"
22,"Find distance to the origin of the tangent plane of $x^2-y^2+2z^2=5$ in the point $(2,-1,1)$",Find distance to the origin of the tangent plane of  in the point,"x^2-y^2+2z^2=5 (2,-1,1)","I'm asked to find the distance to the origin of the tangent plane of the surface $x^2-y^2+2z^2=5$ in the point $(2,-1,1)$ . This seems to be an optimization problem with constraint, which I think can be solved with Lagrange multiplier. The tangent plane is of the form $f_x(p_0)(x-x_0)+f_y(p_0)(y-y_0)+f_z(p_0)(z-z_0)$ where $f_x=2x, f_y=-2y, f_z=4z$ . So in that point, we get $4(x-2)+2(y+1)+4(z-1)$ . This will be our constraint $g(x,y,z)$ The function we want to optimize is the Euclidian distance squared $(x-0)^2+(y-0)^2+(z-0)^2:=f(x,y,z)$ So with lagrange : $(2x,2y,2z)=\lambda(4,2,4)$ so $(x,y,z)=\lambda(2,1,2)$ . So $\lambda=\frac{x}{2}=y=\frac{z}{2}$ So $x=z=2y$ My questions are : 1) Is what I did until now correct ? 2) If yes, how am I supposed to find the given point(s) now ? I have everything expressed in term of one variable but how do I find the value of this variable ? I thought about plugging $x$ and $z$ in terms of $y$ into the original surface equation $x^2-y^2+2z^2=5$ but we're asked to find the distance of the tangent plane of that equation at a point, so the tangent plane of that point isn't necessarily member of that surface, or am I wrong ? Thanks for your help !","I'm asked to find the distance to the origin of the tangent plane of the surface in the point . This seems to be an optimization problem with constraint, which I think can be solved with Lagrange multiplier. The tangent plane is of the form where . So in that point, we get . This will be our constraint The function we want to optimize is the Euclidian distance squared So with lagrange : so . So So My questions are : 1) Is what I did until now correct ? 2) If yes, how am I supposed to find the given point(s) now ? I have everything expressed in term of one variable but how do I find the value of this variable ? I thought about plugging and in terms of into the original surface equation but we're asked to find the distance of the tangent plane of that equation at a point, so the tangent plane of that point isn't necessarily member of that surface, or am I wrong ? Thanks for your help !","x^2-y^2+2z^2=5 (2,-1,1) f_x(p_0)(x-x_0)+f_y(p_0)(y-y_0)+f_z(p_0)(z-z_0) f_x=2x, f_y=-2y, f_z=4z 4(x-2)+2(y+1)+4(z-1) g(x,y,z) (x-0)^2+(y-0)^2+(z-0)^2:=f(x,y,z) (2x,2y,2z)=\lambda(4,2,4) (x,y,z)=\lambda(2,1,2) \lambda=\frac{x}{2}=y=\frac{z}{2} x=z=2y x z y x^2-y^2+2z^2=5","['real-analysis', 'calculus']"
23,Partial derivative of coordinates with respect to function,Partial derivative of coordinates with respect to function,,Let $f : \mathbb{R}^n \rightarrow \mathbb{R}^n$ . Then $$\frac{\partial f^i}{\partial x^j} = (\nabla f)^i_j$$ where $\nabla f$ is the Jacobian matrix of $f$ . When reading this paper I came across the expression $$\frac{\partial x^i}{\partial f^j}$$ Should I interpret this as $$\frac{\partial x^i}{\partial f^j} = ((\nabla f)^{-1})^i_j$$,Let . Then where is the Jacobian matrix of . When reading this paper I came across the expression Should I interpret this as,f : \mathbb{R}^n \rightarrow \mathbb{R}^n \frac{\partial f^i}{\partial x^j} = (\nabla f)^i_j \nabla f f \frac{\partial x^i}{\partial f^j} \frac{\partial x^i}{\partial f^j} = ((\nabla f)^{-1})^i_j,"['matrices', 'multivariable-calculus', 'differential-geometry', 'vector-analysis', 'jacobian']"
24,Compute volume of parallelepiped using triple vector,Compute volume of parallelepiped using triple vector,,"Before anyone claims that I am not studying, my university recently changed their coursework that does not require linear algebra as a pre-requisite to multi-variable calculus. My professor gave us these questions even though we have no experience with matrix, he did not even teach us about it and I had to study them online by myself. I only got as far as I could and I only want one question help so I can work on the other similar questions by myself. I would not be here if I can't find any other help.","Before anyone claims that I am not studying, my university recently changed their coursework that does not require linear algebra as a pre-requisite to multi-variable calculus. My professor gave us these questions even though we have no experience with matrix, he did not even teach us about it and I had to study them online by myself. I only got as far as I could and I only want one question help so I can work on the other similar questions by myself. I would not be here if I can't find any other help.",,"['linear-algebra', 'multivariable-calculus']"
25,"Have I correctly proved that $\lim_{||(x,y)||\to\infty}\frac1{y-x}\int_x^y\exp(-1/|t|)dt$ equals $1$?",Have I correctly proved that  equals ?,"\lim_{||(x,y)||\to\infty}\frac1{y-x}\int_x^y\exp(-1/|t|)dt 1","I should prove that, as long as $y\ne x$ , $$f(x,y)=\frac1{y-x}\int_x^y\exp(-1/|t|)dt\longrightarrow1 \ \text{as $||(x,y)||\to\infty$}$$ and I would like to do it without $\varepsilon-\delta$ reasoning. My idea was to let $h=y-x$ , and then separately consider the cases $|h|\to\infty$ and $|h|\not\to\infty$ , as $||(x,y)||\to\infty$ . We have $$f(x,y)=f_h(x)=\frac1{h}\int_x^{x+h}\exp(-1/|t|)dt=\frac1h\int_0^h\exp(-1/|t+x|)dt.$$ If $|h|\to\infty$ then at least one of $|x|$ and $|y|$ must also go to $\infty$ . In the former case, by the dominated convergence theorem, $$\lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\frac1h\int_0^hdt=1;$$ in the latter case, by De L'Hospital, $$\lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\exp(-1/|h+x|)=\lim_{|y|\to\infty}\exp(-1/|y|)=1. $$ Finally, suppose $|h|\not\to\infty$ ; then $|x|\to\infty$ , as a consequence of \begin{align}\sqrt{x^2+y^2}=\sqrt{x^2+(x+h)^2}=\sqrt{2x^2+2hx+h^2}&\le\sqrt{4x^2+4|hx|+h^2} \\ &\le\sqrt{(2|x|+|h|)^2} \\ &=2|x|+|h|.\end{align} So we get once again $$\lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\frac1h\int_0^hdt=\lim_{||(x,y)||\to\infty}\frac{h}h=1.$$ Is my proof correct? Otherwise, how can I amend it?","I should prove that, as long as , and I would like to do it without reasoning. My idea was to let , and then separately consider the cases and , as . We have If then at least one of and must also go to . In the former case, by the dominated convergence theorem, in the latter case, by De L'Hospital, Finally, suppose ; then , as a consequence of So we get once again Is my proof correct? Otherwise, how can I amend it?","y\ne x f(x,y)=\frac1{y-x}\int_x^y\exp(-1/|t|)dt\longrightarrow1 \ \text{as ||(x,y)||\to\infty} \varepsilon-\delta h=y-x |h|\to\infty |h|\not\to\infty ||(x,y)||\to\infty f(x,y)=f_h(x)=\frac1{h}\int_x^{x+h}\exp(-1/|t|)dt=\frac1h\int_0^h\exp(-1/|t+x|)dt. |h|\to\infty |x| |y| \infty \lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\frac1h\int_0^hdt=1; \lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\exp(-1/|h+x|)=\lim_{|y|\to\infty}\exp(-1/|y|)=1.  |h|\not\to\infty |x|\to\infty \begin{align}\sqrt{x^2+y^2}=\sqrt{x^2+(x+h)^2}=\sqrt{2x^2+2hx+h^2}&\le\sqrt{4x^2+4|hx|+h^2} \\ &\le\sqrt{(2|x|+|h|)^2} \\ &=2|x|+|h|.\end{align} \lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\frac1h\int_0^hdt=\lim_{||(x,y)||\to\infty}\frac{h}h=1.","['real-analysis', 'integration', 'multivariable-calculus', 'proof-verification', 'alternative-proof']"
26,"What condition on $f$ to be differentiable on $(0,0)$?",What condition on  to be differentiable on ?,"f (0,0)","Let $a > 0$ , $b > 0$ and $f_{a,b} :\mathbb{R}^2 \rightarrow \mathbb{R} $ defined as: $$f_{a,b} (x,y) = x +y + |x|^a |y|^b$$ Give a necessary and sufficient condition on $ (a,b) $ for $f_{a,b}$ to be differentiable at (0,0). Using the definition of the differentiation on the point $(0,0)$ , I need to get : $$\lim_{(x,y) \to (0,0)} \frac{|f_{a,b}(x,y) - f_{a,b}(0,0)|}{\sqrt{x^2 + y^2}} = 0$$ We have: $$ \frac{|f_{a,b}(x,y) - f_{a,b}(0,0)|}{\sqrt{x^2 + y^2}} = \frac{|x + y + |x|^a|y|^b|}{\sqrt{x^2 + y^2}} \leq \frac{|x|^a|y|^b}{\sqrt{x^2 + y^2}} $$ As a sufficient condition, the last term needs to tend to zero to get the differentiability, for that to happen, I do not how to derive the condition on $a$ and $b$ . I don't know how to get the necessary condition. Thank you.","Let , and defined as: Give a necessary and sufficient condition on for to be differentiable at (0,0). Using the definition of the differentiation on the point , I need to get : We have: As a sufficient condition, the last term needs to tend to zero to get the differentiability, for that to happen, I do not how to derive the condition on and . I don't know how to get the necessary condition. Thank you.","a > 0 b > 0 f_{a,b} :\mathbb{R}^2 \rightarrow \mathbb{R}  f_{a,b} (x,y) = x +y + |x|^a |y|^b  (a,b)  f_{a,b} (0,0) \lim_{(x,y) \to (0,0)} \frac{|f_{a,b}(x,y) - f_{a,b}(0,0)|}{\sqrt{x^2 + y^2}} = 0  \frac{|f_{a,b}(x,y) - f_{a,b}(0,0)|}{\sqrt{x^2 + y^2}} = \frac{|x + y + |x|^a|y|^b|}{\sqrt{x^2 + y^2}} \leq \frac{|x|^a|y|^b}{\sqrt{x^2 + y^2}}  a b","['limits', 'analysis', 'multivariable-calculus', 'derivatives']"
27,Proving that a function derived from $\arctan(x/y)$ is continuous on $y\ne0$,Proving that a function derived from  is continuous on,\arctan(x/y) y\ne0,"$\Omega_1 = \{y > 0\} $ $\Omega_2 = \{y < 0\} $ $\Omega_3 = \mathbb R^2 \backslash \{x \leq 0 \ \ ; \  \ y = 0 \} $ \begin{equation}   f(x,y) = \left \{   \begin{aligned}     &- \arctan \frac x y + \pi, && \text{if}\ (x,y) \in \Omega_1 \\     & \frac {\pi} 2, && \text{if}  \ y = 0 \ ; \ x > 0 \\     &- \arctan \frac x y, &&  \text{if}\ (x,y) \in \Omega_2   \end{aligned} \right. \end{equation} Prove that : $$ f \in C^1( \Omega_3) $$ So the obviously only problem is on the boundary of $\Omega_1$ and $\Omega_2$ . I don't know how to prove neither continuity nor differentiability. Can you help me ?",Prove that : So the obviously only problem is on the boundary of and . I don't know how to prove neither continuity nor differentiability. Can you help me ?,"\Omega_1 = \{y > 0\}  \Omega_2 = \{y < 0\}  \Omega_3 = \mathbb R^2 \backslash \{x \leq 0 \ \ ; \  \ y = 0 \}  \begin{equation}
  f(x,y) = \left \{
  \begin{aligned}
    &- \arctan \frac x y + \pi, && \text{if}\ (x,y) \in \Omega_1 \\
    & \frac {\pi} 2, && \text{if}  \ y = 0 \ ; \ x > 0 \\
    &- \arctan \frac x y, &&  \text{if}\ (x,y) \in \Omega_2
  \end{aligned} \right.
\end{equation}  f \in C^1( \Omega_3)  \Omega_1 \Omega_2",['multivariable-calculus']
28,Computing the differential of a certain smooth map,Computing the differential of a certain smooth map,,"Let $M \subseteq \mathbb{R}^k$ be an embedded submanifold of $\mathbb{R}^k$ , with dim $M=n$ . Let $v$ be in $\mathbb{S}^{k-1}$ , and let $P_v:\mathbb{R}^k\to(\mathbb{R}v)^{\bot}$ defined by $P_v(x)=x-<x,v>v$ where $<x,v>=x^1v^1+\dots+x^kv^k$ is the Euclidean dot product and $(\mathbb{R}v)^{\bot}$ is the vector space of the vectors in $\mathbb{R}^k$ orthogonal to $v$ . We know that $(\mathbb{R}v)^{\bot}$ has dimension $k-1$ as a real vector space, so it is isomorphic to $\mathbb{R}^{k-1}$ . Question 1) If I want to consider $(\mathbb{R}v)^{\bot}$ as a smooth manifold, I can choose an isomorphism between $(\mathbb{R}v)^{\bot}$ and $\mathbb{R}^{k-1}$ and declare this to be a diffeomorphism, right? Or, better, is there a canonical identification between $(\mathbb{R}v)^{\bot}$ and $\mathbb{R}^{k-1}$ ? Now, (assuming the answer to Question 1 is affirmative), I have that $P_v:\mathbb{R}^k\to(\mathbb{R}v)^{\bot}\simeq \mathbb{R}^{k-1}$ is a map between two smooth manifolds. Question 2) How can I show that this map is smooth? Do I have to calculate it in local coordinates? So do I have to explictly choose an isomorphism between $(\mathbb{R}v)^{\bot}$ and $\mathbb{R}^{k-1}$ and then calculate the map in local coordinates? Let $\Phi_v:M\to (\mathbb{R}v)^{\bot}$ be $P_v\circ \iota_M$ with $\iota_M$ the inclusion map of $M$ in $\mathbb{R}^k$ . So, since $P_v$ is smooth, then also $\Phi_v$ is smooth. Let $p\in M$ and $X\in T_pM$ . Then $d(\iota_M)_p(X)=\sum_iw^i \partial_i|_p\in T_p\mathbb{R}^k$ for some $w\in \mathbb{R}^k$ . My notes say that $$d(\Phi_v)_p(X)=\sum_iP_v(w)^i\partial_i|_p$$ Question 3) How can I prove the above equation? How should I imagine $T_{\Phi_v(p)}((\mathbb{R}v)^{\bot})$ ? Who is a (canonical) basis for $T_{\Phi_v(p)}((\mathbb{R}v)^{\bot})$ ? All I can see is that $$d(\Phi_v)_p(X)=d(P_v)_p(\sum_iw^i\partial_i|_p)=\sum_iw^id(P_v)_p(\partial_i|_p)$$ where the last $=$ is by the linearity of the differential. Please use simple language since I'm a beginner in this subject, do full calculation if needed, and also, if you think I lack some knowledge of some topic of smooth manifold theory (useful to better understand your answer to my question), please let me know.","Let be an embedded submanifold of , with dim . Let be in , and let defined by where is the Euclidean dot product and is the vector space of the vectors in orthogonal to . We know that has dimension as a real vector space, so it is isomorphic to . Question 1) If I want to consider as a smooth manifold, I can choose an isomorphism between and and declare this to be a diffeomorphism, right? Or, better, is there a canonical identification between and ? Now, (assuming the answer to Question 1 is affirmative), I have that is a map between two smooth manifolds. Question 2) How can I show that this map is smooth? Do I have to calculate it in local coordinates? So do I have to explictly choose an isomorphism between and and then calculate the map in local coordinates? Let be with the inclusion map of in . So, since is smooth, then also is smooth. Let and . Then for some . My notes say that Question 3) How can I prove the above equation? How should I imagine ? Who is a (canonical) basis for ? All I can see is that where the last is by the linearity of the differential. Please use simple language since I'm a beginner in this subject, do full calculation if needed, and also, if you think I lack some knowledge of some topic of smooth manifold theory (useful to better understand your answer to my question), please let me know.","M \subseteq \mathbb{R}^k \mathbb{R}^k M=n v \mathbb{S}^{k-1} P_v:\mathbb{R}^k\to(\mathbb{R}v)^{\bot} P_v(x)=x-<x,v>v <x,v>=x^1v^1+\dots+x^kv^k (\mathbb{R}v)^{\bot} \mathbb{R}^k v (\mathbb{R}v)^{\bot} k-1 \mathbb{R}^{k-1} (\mathbb{R}v)^{\bot} (\mathbb{R}v)^{\bot} \mathbb{R}^{k-1} (\mathbb{R}v)^{\bot} \mathbb{R}^{k-1} P_v:\mathbb{R}^k\to(\mathbb{R}v)^{\bot}\simeq \mathbb{R}^{k-1} (\mathbb{R}v)^{\bot} \mathbb{R}^{k-1} \Phi_v:M\to (\mathbb{R}v)^{\bot} P_v\circ \iota_M \iota_M M \mathbb{R}^k P_v \Phi_v p\in M X\in T_pM d(\iota_M)_p(X)=\sum_iw^i \partial_i|_p\in T_p\mathbb{R}^k w\in \mathbb{R}^k d(\Phi_v)_p(X)=\sum_iP_v(w)^i\partial_i|_p T_{\Phi_v(p)}((\mathbb{R}v)^{\bot}) T_{\Phi_v(p)}((\mathbb{R}v)^{\bot}) d(\Phi_v)_p(X)=d(P_v)_p(\sum_iw^i\partial_i|_p)=\sum_iw^id(P_v)_p(\partial_i|_p) =","['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'smooth-functions']"
29,Switching order of integration on unbounded domain,Switching order of integration on unbounded domain,,"Let's say ${f\left( {x,y} \right)}$ is a continuous function and assume that: $\int\limits_{ - \infty }^\infty  {\left| {f\left( {x,y} \right)} \right|dx} $ converges for all $y  $ 2) $\int\limits_{ - \infty }^\infty  {\left| {f\left( {x,y} \right)} \right|dy} $ converges for all $x $ Is the following statement correct? $$\int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {f\left( {x,y} \right)dxdy} }  = \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {f\left( {x,y} \right)dydx} } $$ This is supposed to be some version of Fubini's or Tonelli's theorem but I wasn't able to find the exact version for this case.",Let's say is a continuous function and assume that: converges for all 2) converges for all Is the following statement correct? This is supposed to be some version of Fubini's or Tonelli's theorem but I wasn't able to find the exact version for this case.,"{f\left( {x,y} \right)} \int\limits_{ - \infty }^\infty  {\left| {f\left( {x,y} \right)} \right|dx}  y   \int\limits_{ - \infty }^\infty  {\left| {f\left( {x,y} \right)} \right|dy}  x  \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {f\left( {x,y} \right)dxdy} }  = \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {f\left( {x,y} \right)dydx} } ",['multivariable-calculus']
30,Calculating integral over unit ball using co area formula,Calculating integral over unit ball using co area formula,,"Calculate $\int_V\frac{1}{2+(x^2+y^2+z^2)^{\frac{3}{2}}}dxdydz$ , where $V=\{(x,y,z)|x^2+y^2+z^2\lt 1\}$ using the co area formula. So I know that the formula is: $\int_V f dx$ = $\int_a^b\int_{M_c}\frac{f(x)}{|\nabla \phi(x)|}dS dc$ , where $\phi:V\rightarrow\mathbb{R}\in C^1,\forall x: \nabla \phi(x)\neq 0,\phi(V)=(a,b)$ and $M_c=\{x=(x_1,...,x_n)\in V|\phi(x)=c,c\in(a,b)\}$ . My attempt - So the difficult part here (for me) is to find the correct $\phi$ . I thought about $\phi = x^2+y^2+z^2$ which is $\phi \in(0,1)$ but $\nabla \phi(0,0,0) =0$ so I cannot use it. I also thought about $\phi = (x^2+y^2+z^2)^{\frac{3}{2}}$ , but also here $\nabla \phi(0,0,0) =0$ so I cannot use it. Other attempt was to use the function that we integrate - $\phi = \frac{1}{2+(x^2+y^2+z^2)^{\frac{3}{2}}}$ and here I know that $\nabla \phi \neq 0 \forall x$ , but when I lower the dimension of the integral, which variable should be gone?(i.e., instead of $3$ dimensional, $dxdydz$ , what should it be?) Or perhaps, is there any better $\phi$ to use in this case?","Calculate , where using the co area formula. So I know that the formula is: = , where and . My attempt - So the difficult part here (for me) is to find the correct . I thought about which is but so I cannot use it. I also thought about , but also here so I cannot use it. Other attempt was to use the function that we integrate - and here I know that , but when I lower the dimension of the integral, which variable should be gone?(i.e., instead of dimensional, , what should it be?) Or perhaps, is there any better to use in this case?","\int_V\frac{1}{2+(x^2+y^2+z^2)^{\frac{3}{2}}}dxdydz V=\{(x,y,z)|x^2+y^2+z^2\lt 1\} \int_V f dx \int_a^b\int_{M_c}\frac{f(x)}{|\nabla \phi(x)|}dS dc \phi:V\rightarrow\mathbb{R}\in C^1,\forall x: \nabla \phi(x)\neq 0,\phi(V)=(a,b) M_c=\{x=(x_1,...,x_n)\in V|\phi(x)=c,c\in(a,b)\} \phi \phi = x^2+y^2+z^2 \phi \in(0,1) \nabla \phi(0,0,0) =0 \phi = (x^2+y^2+z^2)^{\frac{3}{2}} \nabla \phi(0,0,0) =0 \phi = \frac{1}{2+(x^2+y^2+z^2)^{\frac{3}{2}}} \nabla \phi \neq 0 \forall x 3 dxdydz \phi","['calculus', 'integration', 'multivariable-calculus', 'area', 'multiple-integral']"
31,Method of Undermined coefficients,Method of Undermined coefficients,,"The following equation: $y''+y=\cos(x)$ Solving $y''+y=0$ gives me $c_1\cos(x)+c_2\sin(x)$ Using the particular integral yp form of $a\cos(x)+b\sin(x)$ and substituting this back into the ODE $y = a\cos(x)+b\sin(x)$ , $y'=-a\sin(x)+b\cos(x)$ , $y''=-a\cos(x)-b\sin(x)$ Plugging this back into the ODE leaves me with $0=\cos(x)$ So how is the particular solution $y=1/2x\sin(x)$ with the coefficients value of $c_1=0$ and $c_2=1/2$ , and where does the $x$ come from?","The following equation: Solving gives me Using the particular integral yp form of and substituting this back into the ODE , , Plugging this back into the ODE leaves me with So how is the particular solution with the coefficients value of and , and where does the come from?",y''+y=\cos(x) y''+y=0 c_1\cos(x)+c_2\sin(x) a\cos(x)+b\sin(x) y = a\cos(x)+b\sin(x) y'=-a\sin(x)+b\cos(x) y''=-a\cos(x)-b\sin(x) 0=\cos(x) y=1/2x\sin(x) c_1=0 c_2=1/2 x,"['calculus', 'multivariable-calculus', 'numerical-methods']"
32,"Evaluating $\int_{1/4}^1 \int_{\sqrt{x-x^2}}^{\sqrt x}\left(\frac{x^2-y^2}{x^2}\right)\,dy\,dx$",Evaluating,"\int_{1/4}^1 \int_{\sqrt{x-x^2}}^{\sqrt x}\left(\frac{x^2-y^2}{x^2}\right)\,dy\,dx","I am looking for an efficient way to evaluate $$\int_{1/4}^1 \int_{\sqrt{x-x^2}}^{\sqrt x}\left(\frac{x^2-y^2}{x^2}\right)\,\mathrm{d}y\,\mathrm{d}x$$ I have \begin{align} I&=\int_{1/4}^1\left(\int_{\sqrt{x-x^2}}^{\sqrt x}\,dy-\frac{1}{x^2}\int_{\sqrt{x-x^2}}^{\sqrt x}y^2\,dy\right)\,dx \\&=\int_{1/4}^1\left[\sqrt x-\sqrt{x-x^2}-\frac{(\sqrt{x})^3-(\sqrt{x-x^2})^3}{3x^2}\right]\,dx \\&=\int_{1/4}^1\sqrt x\,dx-\int_{1/4}^1 \sqrt{x-x^2}\,dx-\frac{1}{3}\int_{1/4}^1 \frac{1}{\sqrt x}\,dx+\frac{1}{3}\int_{1/4}^1\frac{(x-x^2)^{3/2}}{x^2}\,dx \\&=\frac{2}{3}\left(1-\frac{1}{4^{3/2}}\right)-\frac{1}{3}-\color{darkred}{\int_{1/4}^1 \sqrt{x-x^2}\,dx}+\frac{1}{3}\color{green}{\int_{1/4}^1\frac{(x-x^2)^{3/2}}{x^2}\,dx} \end{align} Using this answer, $$\color{darkred}{\int_{1/4}^1 \sqrt{x-x^2}\,dx}=\frac{1}{4}\int_{-\pi/6}^{\pi/2}\cos^2\,dt=\frac{1}{8}\int_{-\pi/6}^{\pi/2}(1+\cos 2t)\,dt=\frac{1}{96}(3\sqrt 3+8\pi)$$ For $\color{green}{\int_{1/4}^1\frac{(x-x^2)^{3/2}}{x^2}\,dx}$ or even the indefinite integral, nothing comes to mind off the top of my head. In a different approach, if I try to change the order of integration right at the start, it complicates matters for me to rewrite the region $$\sqrt{x-x^2}<y<\sqrt x\,,\,1/4<x<1$$ keeping a separate range of $y$ free of $x$ and bounding $x$ with $y$ . Any suggestion regarding specific substitution or change of variables would also be helpful.","I am looking for an efficient way to evaluate I have Using this answer, For or even the indefinite integral, nothing comes to mind off the top of my head. In a different approach, if I try to change the order of integration right at the start, it complicates matters for me to rewrite the region keeping a separate range of free of and bounding with . Any suggestion regarding specific substitution or change of variables would also be helpful.","\int_{1/4}^1 \int_{\sqrt{x-x^2}}^{\sqrt x}\left(\frac{x^2-y^2}{x^2}\right)\,\mathrm{d}y\,\mathrm{d}x \begin{align}
I&=\int_{1/4}^1\left(\int_{\sqrt{x-x^2}}^{\sqrt x}\,dy-\frac{1}{x^2}\int_{\sqrt{x-x^2}}^{\sqrt x}y^2\,dy\right)\,dx
\\&=\int_{1/4}^1\left[\sqrt x-\sqrt{x-x^2}-\frac{(\sqrt{x})^3-(\sqrt{x-x^2})^3}{3x^2}\right]\,dx
\\&=\int_{1/4}^1\sqrt x\,dx-\int_{1/4}^1 \sqrt{x-x^2}\,dx-\frac{1}{3}\int_{1/4}^1 \frac{1}{\sqrt x}\,dx+\frac{1}{3}\int_{1/4}^1\frac{(x-x^2)^{3/2}}{x^2}\,dx
\\&=\frac{2}{3}\left(1-\frac{1}{4^{3/2}}\right)-\frac{1}{3}-\color{darkred}{\int_{1/4}^1 \sqrt{x-x^2}\,dx}+\frac{1}{3}\color{green}{\int_{1/4}^1\frac{(x-x^2)^{3/2}}{x^2}\,dx}
\end{align} \color{darkred}{\int_{1/4}^1 \sqrt{x-x^2}\,dx}=\frac{1}{4}\int_{-\pi/6}^{\pi/2}\cos^2\,dt=\frac{1}{8}\int_{-\pi/6}^{\pi/2}(1+\cos 2t)\,dt=\frac{1}{96}(3\sqrt 3+8\pi) \color{green}{\int_{1/4}^1\frac{(x-x^2)^{3/2}}{x^2}\,dx} \sqrt{x-x^2}<y<\sqrt x\,,\,1/4<x<1 y x x y","['integration', 'multivariable-calculus', 'multiple-integral']"
33,Double Integral unequal after switching $dy$ and $dx$. Exact reasoning required,Double Integral unequal after switching  and . Exact reasoning required,dy dx,"I have worked out $\int_{[0,1]}(\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2} d\lambda(x)) d\lambda (y)=-\frac{\pi}{4}$ and $\int_{[0,1]}(\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2} d\lambda(y)) d\lambda (x)=\frac{\pi}{4}$ What should my exact reasoning be as to why the double integral does not exist according to $\lambda^2$ ? Should I argue along the lines of $\int_{[0,1]\times[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}d\lambda^2(x,y)$ not being well-defined as $\int_{[0,1]\times[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}d\lambda^2(x,y)=\int_{[0,1]}(\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2} d\lambda(x)) d\lambda (y)=-\frac{\pi}{4}$ and $\int_{[0,1]\times[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}d\lambda^2(x,y)=\int_{[0,1]}(\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2} d\lambda(y)) d\lambda (x)=\frac{\pi}{4} $ But I am not sure on this argument.",I have worked out and What should my exact reasoning be as to why the double integral does not exist according to ? Should I argue along the lines of not being well-defined as and But I am not sure on this argument.,"\int_{[0,1]}(\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2} d\lambda(x)) d\lambda (y)=-\frac{\pi}{4} \int_{[0,1]}(\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2} d\lambda(y)) d\lambda (x)=\frac{\pi}{4} \lambda^2 \int_{[0,1]\times[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}d\lambda^2(x,y) \int_{[0,1]\times[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}d\lambda^2(x,y)=\int_{[0,1]}(\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2} d\lambda(x)) d\lambda (y)=-\frac{\pi}{4} \int_{[0,1]\times[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}d\lambda^2(x,y)=\int_{[0,1]}(\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2} d\lambda(y)) d\lambda (x)=\frac{\pi}{4}
","['real-analysis', 'integration', 'measure-theory', 'multivariable-calculus']"
34,Continuity for a two-variable function from baby Rudin,Continuity for a two-variable function from baby Rudin,,"While I'm studying Baby Rudin's Exercise, I got some problems. The Exercise $9.28$ says I need to check the continuity of the function $$\varphi(x,t)=\begin{cases}x&0\leq x\leq \sqrt{t} \\ -x+2\sqrt{t}& \sqrt{t}\leq x\leq 2\sqrt{t}\\ 0&\text{otherwise}\end{cases}$$ and $\varphi(x,-t)=-\varphi(x,t)$ for $t\geq0$ . Intuitively, I know the function is continuous on $\mathbb{R}^2$ , but I can't show it easily. Even I tried to show the continuity by $\varepsilon$ - $\delta$ argument, but the formula got complicated; there are too many cases. Is there any simple way to show its continuity? I heard that by using pasting lemma it can be proved easily, but it is out of the scope of the class.","While I'm studying Baby Rudin's Exercise, I got some problems. The Exercise says I need to check the continuity of the function and for . Intuitively, I know the function is continuous on , but I can't show it easily. Even I tried to show the continuity by - argument, but the formula got complicated; there are too many cases. Is there any simple way to show its continuity? I heard that by using pasting lemma it can be proved easily, but it is out of the scope of the class.","9.28 \varphi(x,t)=\begin{cases}x&0\leq x\leq \sqrt{t} \\ -x+2\sqrt{t}& \sqrt{t}\leq x\leq 2\sqrt{t}\\ 0&\text{otherwise}\end{cases} \varphi(x,-t)=-\varphi(x,t) t\geq0 \mathbb{R}^2 \varepsilon \delta","['analysis', 'multivariable-calculus', 'vector-analysis']"
35,Intuition on Stokes's Theorem,Intuition on Stokes's Theorem,,"Stokes's Theorem says that $$\int_C\vec{F} \cdot d\vec{r} = \int \int_S (curl \space\vec{F}) \space\cdot \vec{n} \space dS$$ I understand that $curl \space\vec{F}$ is the ""spin"" or ""circulation"" on a given surface. I also understand that the integral is essentially a summation of a quantity. However, why is $curl \space \vec{F}$ dotted with $\vec{n}$ ? Setting aside the fact that integrals require scalar functions (i.e. a dot with some vector is necessary), the dot product says that vector $\vec{a} \cdot \vec{b} = 0$ if $\vec{a}$ and $\vec{b}$ are orthogonal. If you visualize some surface $S$ with a curl vector $\vec{a}$ , assuming it is measuring the circulation of particles actually on the surface, and a normal vector $\vec{b}$ , dotting $\vec{a}$ and $\vec{b}$ would yield $0$ , or, in other words, not the circulation. What am I missing? Thanks.","Stokes's Theorem says that I understand that is the ""spin"" or ""circulation"" on a given surface. I also understand that the integral is essentially a summation of a quantity. However, why is dotted with ? Setting aside the fact that integrals require scalar functions (i.e. a dot with some vector is necessary), the dot product says that vector if and are orthogonal. If you visualize some surface with a curl vector , assuming it is measuring the circulation of particles actually on the surface, and a normal vector , dotting and would yield , or, in other words, not the circulation. What am I missing? Thanks.","\int_C\vec{F} \cdot d\vec{r} = \int
\int_S (curl \space\vec{F}) \space\cdot \vec{n} \space dS curl \space\vec{F} curl \space \vec{F} \vec{n} \vec{a} \cdot \vec{b} = 0 \vec{a} \vec{b} S \vec{a} \vec{b} \vec{a} \vec{b} 0","['multivariable-calculus', 'vectors', 'intuition', 'vector-fields', 'stokes-theorem']"
36,Changing variables or coordinates?,Changing variables or coordinates?,,"While posing another question I got stuck on the distinction of the following two concepts; The components of a vector is usually referred to as it's coordinates, while trying to understand what ""change of variables"" means when it comes to systems of ODE's, I saw several sourse talked about change of variables as ""a change of coordiinates"". Sure a function has a graph $(x,f(x))$ where if we do some kind of transformation we get a new graph. But this should be the same kind of coordinates as in a linear space. Is this the thing that one refers to when one says ""change of coordinates"" as in change to polar coordiantes?","While posing another question I got stuck on the distinction of the following two concepts; The components of a vector is usually referred to as it's coordinates, while trying to understand what ""change of variables"" means when it comes to systems of ODE's, I saw several sourse talked about change of variables as ""a change of coordiinates"". Sure a function has a graph where if we do some kind of transformation we get a new graph. But this should be the same kind of coordinates as in a linear space. Is this the thing that one refers to when one says ""change of coordinates"" as in change to polar coordiantes?","(x,f(x))","['analysis', 'multivariable-calculus']"
37,Volume between cone and sphere of radius $\sqrt2$ with surface integral,Volume between cone and sphere of radius  with surface integral,\sqrt2,"Consider the cone $z^2=x^2+y^2$ between $z=0$ and $z=1$ . Find the volume of the region above this cone and inside the sphere of radius $\sqrt2$ centered at the origin that encloses the cone. The straightforward approach to this problem would have been a triple integral in spherical coordinates, but for practice I tried using the cone as a surface and using a surface integral to find the volume between the cone and the sphere. I first parameterized the surface using cylindrical coordinates $x=r\cos\theta$ , $y=r\sin\theta$ , $z=r$ (since $z^2=x^2+y^2$ ), and then found the intersection points using the equations $z=r$ and $r^2+z^2=2$ : $$2z^2=2\Rightarrow z=1\Rightarrow r=1$$ Now that the bounds of integration have been found I used the function $z=\sqrt{2-r^2}$ as the function to integrate and set up my surface integral as: $$\iint_S \sqrt{2-r^2}\Vert\vec{r_r}\times\vec{r_\theta}\Vert\,\text{d}S$$ I then computed the cross product, for $\vec{r}(r,\theta)=\left<r\cos\theta,r\sin\theta,r\right>$ : $$\begin{vmatrix} \hat{\imath}&\hat{\jmath}&\hat{k}\\ \cos\theta&\sin\theta&1\\ -r\sin\theta&r\cos\theta&0 \end{vmatrix}=\hat{\imath}(-r\cos\theta)-\hat{\jmath}(-r\sin\theta)+\hat{k}(r\cos^2\theta+r\sin^2\theta)$$ So the surface integral becomes $$\int_0^{2\pi}\int_0^1\sqrt{\left(2-r^2\right)\left(r^2\cos^2\theta+r^2\sin^2\theta+r^2\right)}\,\text dr\,\text d\theta$$ $$=\int_0^{2\pi}\int_0^1\sqrt{\left(2-r^2\right)r^2\left(\cos^2\theta+\sin^2\theta+1\right)}\,\text dr\,\text d\theta$$ $$=\int_0^{2\pi}\int_0^1\sqrt{2r^2(2-r^2)}\,\text dr\text d\theta$$ $$=2\sqrt2\pi\int_0^1r\sqrt{2-r^2}\,\text dr$$ Using a $u$ -substitution of $u=2-r^2$ : $$\sqrt2\pi\int_1^2 \sqrt{u}\,\text du=\sqrt2\pi\left.\left(\frac{2u\sqrt u}{3}\right)\right|_1^2=\sqrt2\pi\left(\frac{4\sqrt2-2}{3}\right)$$ $$=\frac{2\pi}{3}\left(4-\sqrt2\right)$$ However, the answer given in the answer key says that the volume is $\frac{4\pi}{3}(\sqrt2-1)$ , so I would like to know why my answer is incorrect, whether that be a computational mistake or faulty reasoning as to why a surface integral would work here to find the volume. EDIT: Here is the posted solution:","Consider the cone between and . Find the volume of the region above this cone and inside the sphere of radius centered at the origin that encloses the cone. The straightforward approach to this problem would have been a triple integral in spherical coordinates, but for practice I tried using the cone as a surface and using a surface integral to find the volume between the cone and the sphere. I first parameterized the surface using cylindrical coordinates , , (since ), and then found the intersection points using the equations and : Now that the bounds of integration have been found I used the function as the function to integrate and set up my surface integral as: I then computed the cross product, for : So the surface integral becomes Using a -substitution of : However, the answer given in the answer key says that the volume is , so I would like to know why my answer is incorrect, whether that be a computational mistake or faulty reasoning as to why a surface integral would work here to find the volume. EDIT: Here is the posted solution:","z^2=x^2+y^2 z=0 z=1 \sqrt2 x=r\cos\theta y=r\sin\theta z=r z^2=x^2+y^2 z=r r^2+z^2=2 2z^2=2\Rightarrow z=1\Rightarrow r=1 z=\sqrt{2-r^2} \iint_S \sqrt{2-r^2}\Vert\vec{r_r}\times\vec{r_\theta}\Vert\,\text{d}S \vec{r}(r,\theta)=\left<r\cos\theta,r\sin\theta,r\right> \begin{vmatrix}
\hat{\imath}&\hat{\jmath}&\hat{k}\\
\cos\theta&\sin\theta&1\\
-r\sin\theta&r\cos\theta&0
\end{vmatrix}=\hat{\imath}(-r\cos\theta)-\hat{\jmath}(-r\sin\theta)+\hat{k}(r\cos^2\theta+r\sin^2\theta) \int_0^{2\pi}\int_0^1\sqrt{\left(2-r^2\right)\left(r^2\cos^2\theta+r^2\sin^2\theta+r^2\right)}\,\text dr\,\text d\theta =\int_0^{2\pi}\int_0^1\sqrt{\left(2-r^2\right)r^2\left(\cos^2\theta+\sin^2\theta+1\right)}\,\text dr\,\text d\theta =\int_0^{2\pi}\int_0^1\sqrt{2r^2(2-r^2)}\,\text dr\text d\theta =2\sqrt2\pi\int_0^1r\sqrt{2-r^2}\,\text dr u u=2-r^2 \sqrt2\pi\int_1^2
\sqrt{u}\,\text du=\sqrt2\pi\left.\left(\frac{2u\sqrt u}{3}\right)\right|_1^2=\sqrt2\pi\left(\frac{4\sqrt2-2}{3}\right) =\frac{2\pi}{3}\left(4-\sqrt2\right) \frac{4\pi}{3}(\sqrt2-1)","['calculus', 'multivariable-calculus', 'surface-integrals']"
38,Why is this the second derivative here?,Why is this the second derivative here?,,"Let $F\colon\mathbb{R}^{2}\to\mathbb{R}$ and $f\colon\mathbb{R}\to\mathbb{R}$ be two $\mathcal{C}^{1}$ functions. Lets say we have a neighborhood of a point $\left(x_{0},y_{0}\right)$ such that for every $\left(x,y\right)$ we have $F\left(x,y\right)=0$ if and only if $y=f\left(x\right).$ So we can say that in this neighborhood we have $F\left(x,f\left(x\right)\right)=0$ and if we let $g\colon\mathbb{R}\to\mathbb{R}^{2}$ be defined by $g\left(x\right)=\left(x,f\left(x\right)\right)$ then $F\left(g\left(x\right)\right)=0$ and therefore \begin{align*} 0 & =\left[F\left(g\left(x\right)\right)\right]'=D_{F\circ g}\left(x\right)=D_{F}\left(g\left(x\right)\right)\cdot D_{g}\left(x\right)=\\  & =\left(\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right),\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)\right)\cdot\begin{pmatrix}1\\ f'\left(x\right) \end{pmatrix}=\\  & =\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right)+\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)\cdot f'\left(x\right) \end{align*} $$ \Rightarrow\qquad f'\left(x\right)=-\frac{\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right)}{\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)} $$ Im trying to compute $f''\left(x\right)$ . I was thinking it would be something like $$ f''\left(x\right)=-\frac{\frac{\partial^{2}F}{\partial x^{2}}\left(x,f\left(x\right)\right)\cdot\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)-\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right)\cdot\frac{\partial^{2}F}{\partial x\partial y}\left(x,f\left(x\right)\right)}{\left[\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)\right]^{2}} $$ but I know it is wrong. The correct answer is $$ f''\left(x\right)=-\frac{\left[\frac{\partial^{2}F}{\partial x^{2}}\left(x,f\left(x\right)\right)+\frac{\partial^{2}F}{\partial x\partial y}\left(x,f\left(x\right)\right)\cdot f'\left(x\right)\right]\cdot\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)-\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right)\cdot\left[\frac{\partial^{2}F}{\partial x\partial y}\left(x,f\left(x\right)\right)+\frac{\partial^{2}F}{\partial y^{2}}\left(x,f\left(x\right)\right)\cdot f'\left(x\right)\right]}{\left[\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)\right]^{2}} $$ and I don't understand why. Any help please?",Let and be two functions. Lets say we have a neighborhood of a point such that for every we have if and only if So we can say that in this neighborhood we have and if we let be defined by then and therefore Im trying to compute . I was thinking it would be something like but I know it is wrong. The correct answer is and I don't understand why. Any help please?,"F\colon\mathbb{R}^{2}\to\mathbb{R} f\colon\mathbb{R}\to\mathbb{R} \mathcal{C}^{1} \left(x_{0},y_{0}\right) \left(x,y\right) F\left(x,y\right)=0 y=f\left(x\right). F\left(x,f\left(x\right)\right)=0 g\colon\mathbb{R}\to\mathbb{R}^{2} g\left(x\right)=\left(x,f\left(x\right)\right) F\left(g\left(x\right)\right)=0 \begin{align*}
0 & =\left[F\left(g\left(x\right)\right)\right]'=D_{F\circ g}\left(x\right)=D_{F}\left(g\left(x\right)\right)\cdot D_{g}\left(x\right)=\\
 & =\left(\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right),\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)\right)\cdot\begin{pmatrix}1\\
f'\left(x\right)
\end{pmatrix}=\\
 & =\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right)+\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)\cdot f'\left(x\right)
\end{align*} 
\Rightarrow\qquad f'\left(x\right)=-\frac{\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right)}{\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)}
 f''\left(x\right) 
f''\left(x\right)=-\frac{\frac{\partial^{2}F}{\partial x^{2}}\left(x,f\left(x\right)\right)\cdot\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)-\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right)\cdot\frac{\partial^{2}F}{\partial x\partial y}\left(x,f\left(x\right)\right)}{\left[\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)\right]^{2}}
 
f''\left(x\right)=-\frac{\left[\frac{\partial^{2}F}{\partial x^{2}}\left(x,f\left(x\right)\right)+\frac{\partial^{2}F}{\partial x\partial y}\left(x,f\left(x\right)\right)\cdot f'\left(x\right)\right]\cdot\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)-\frac{\partial F}{\partial x}\left(x,f\left(x\right)\right)\cdot\left[\frac{\partial^{2}F}{\partial x\partial y}\left(x,f\left(x\right)\right)+\frac{\partial^{2}F}{\partial y^{2}}\left(x,f\left(x\right)\right)\cdot f'\left(x\right)\right]}{\left[\frac{\partial F}{\partial y}\left(x,f\left(x\right)\right)\right]^{2}}
","['multivariable-calculus', 'derivatives']"
39,Area of triangle using double integrals,Area of triangle using double integrals,,"I have one (rather simple) problem, but I'm stuck and can't figure out what I'm constantly doing wrong. I need to calculate area of triangle with points at $(0,0)$ , $(t,0)$ , $(t,\frac{t}{2})$ . In other words triangle under function $y=\frac{x}{2}$ , for $x\in [0,t]$ I thought it is calculated with $$ \int_0^t \int_0^\frac{t}{2} dudv$$ But it turns out that this equals to $\frac{t^2}{2}$ , when obviously this area is $\frac{t\times\frac{t}{2}}{2} = \frac{t^2}{4}$ . What am I doing wrong here? I need to calculate it this way, not with single integral, or geometrically.","I have one (rather simple) problem, but I'm stuck and can't figure out what I'm constantly doing wrong. I need to calculate area of triangle with points at , , . In other words triangle under function , for I thought it is calculated with But it turns out that this equals to , when obviously this area is . What am I doing wrong here? I need to calculate it this way, not with single integral, or geometrically.","(0,0) (t,0) (t,\frac{t}{2}) y=\frac{x}{2} x\in [0,t]  \int_0^t \int_0^\frac{t}{2} dudv \frac{t^2}{2} \frac{t\times\frac{t}{2}}{2} = \frac{t^2}{4}","['integration', 'multivariable-calculus', 'area', 'multiple-integral']"
40,Higher Order Multivariable Taylor Expansions,Higher Order Multivariable Taylor Expansions,,"The quadratic multivariable Taylor approximation of a function $f(x, y)$ around a point $(a, b)$ is given by $f(a, b) + f_x(a, b)(x - a) + f_y(a, b)(y - b) + \frac{1}{2}f_{xx}(a, b)(x - a)^2 + f_{xy}(a, b)(x - a)(y - b) + \frac{1}{2}f_{yy}(a, b)(y - b)^2$ There is a vectorized version of this expression which works for functions and points with an arbitrary number of variables, $f(\vec{a})\ + ∇f(\vec{a})\cdot(\vec{x} - \vec{a}) + \frac{1}{2}(\vec{x} - \vec{a})^TH(\vec{a})(\vec{x} - \vec{a})$ , where the elements of $\vec{a}$ are the coordinates of the point around which we're approximating, the elements of $\vec{x}$ are the inputs of $f$ , and $H$ is the Hessian matrix of $f$ . Following the pattern of single variable Taylor approximations, the cubic multivariable Taylor approximation should be $f(a, b) + f_x(a, b)(x - a) + f_y(a, b)(y - b) + \frac{1}{2}f_{xx}(a, b)(x - a)^2 + f_{xy}(a, b)(x - a)(y - b) + \frac{1}{2}f_{yy}(a, b)(y - b)^2 + \frac{1}{3!}f_{xxx}(a, b)(x - a)^3 + \frac{3}{3!}f_{xxy}(a, b)(x - a)^2(y - b) + \frac{3}{3!}f_{xyy}(a, b)(x - a)(y - b)^2 + \frac{1}{3!}f_{yyy}(a, b)(y - b)^3$ The amount of simplification in my coefficients is inconsistent, and I'm intentionally not generalizing the approximation order by introducing additional variables or sigma notation, as this expression already requires more than enough mental unpacking in my opinion, and it can only get worse in vector notation.  Instead I'll use an image and words to describe the pattern I have in mind. Every unique partial derivative up to the order of the approximation constitutes a term (third order approximation includes all zeroth, first, second, and third partial derivatives), and the numerator of the fraction coefficient for each term is the number of equivalent permutations of that partial derivative ( $f_{xyy} = f_{yxy} = f_{yyx}$ , so the numberator is $3$ ). Is this the correct extension of the quadratic multivariable Taylor approximation to cubic approximations, and a correct pattern for writing quartic and higher approximations, and how can we put this into vector notation?  An optimal answer, in order to avoid layering on too much abstraction, will likely include the expression for a cubic approximation in vector notation (possibly also quartic, if helpful) and an explanation in words of how to continue the pattern into higher order approximations (i.e., do we require an increasingly higher order ""tensor,"" a ""Hyperhessian"" of sorts, as we continue to add more terms?).","The quadratic multivariable Taylor approximation of a function around a point is given by There is a vectorized version of this expression which works for functions and points with an arbitrary number of variables, , where the elements of are the coordinates of the point around which we're approximating, the elements of are the inputs of , and is the Hessian matrix of . Following the pattern of single variable Taylor approximations, the cubic multivariable Taylor approximation should be The amount of simplification in my coefficients is inconsistent, and I'm intentionally not generalizing the approximation order by introducing additional variables or sigma notation, as this expression already requires more than enough mental unpacking in my opinion, and it can only get worse in vector notation.  Instead I'll use an image and words to describe the pattern I have in mind. Every unique partial derivative up to the order of the approximation constitutes a term (third order approximation includes all zeroth, first, second, and third partial derivatives), and the numerator of the fraction coefficient for each term is the number of equivalent permutations of that partial derivative ( , so the numberator is ). Is this the correct extension of the quadratic multivariable Taylor approximation to cubic approximations, and a correct pattern for writing quartic and higher approximations, and how can we put this into vector notation?  An optimal answer, in order to avoid layering on too much abstraction, will likely include the expression for a cubic approximation in vector notation (possibly also quartic, if helpful) and an explanation in words of how to continue the pattern into higher order approximations (i.e., do we require an increasingly higher order ""tensor,"" a ""Hyperhessian"" of sorts, as we continue to add more terms?).","f(x, y) (a, b) f(a, b) + f_x(a, b)(x - a) + f_y(a, b)(y - b) + \frac{1}{2}f_{xx}(a, b)(x - a)^2 + f_{xy}(a, b)(x - a)(y - b) + \frac{1}{2}f_{yy}(a, b)(y - b)^2 f(\vec{a})\ + ∇f(\vec{a})\cdot(\vec{x} - \vec{a}) + \frac{1}{2}(\vec{x} - \vec{a})^TH(\vec{a})(\vec{x} - \vec{a}) \vec{a} \vec{x} f H f f(a, b) + f_x(a, b)(x - a) + f_y(a, b)(y - b) + \frac{1}{2}f_{xx}(a, b)(x - a)^2 + f_{xy}(a, b)(x - a)(y - b) + \frac{1}{2}f_{yy}(a, b)(y - b)^2 + \frac{1}{3!}f_{xxx}(a, b)(x - a)^3 + \frac{3}{3!}f_{xxy}(a, b)(x - a)^2(y - b) + \frac{3}{3!}f_{xyy}(a, b)(x - a)(y - b)^2 + \frac{1}{3!}f_{yyy}(a, b)(y - b)^3 f_{xyy} = f_{yxy} = f_{yyx} 3","['multivariable-calculus', 'permutations', 'taylor-expansion', 'partial-derivative', 'tensors']"
41,Finding the maximum of a function on a triangle,Finding the maximum of a function on a triangle,,"I want to find the maximum of $f(x,y) = x^ae^{-x}y^be^{-y}$ on the triangle given by $x\geq0$ , $y\geq0$ , and $x+y\leq1$ in terms of $a$ and $b$ such that $a,b>0$ . I can see that the vertices of the triangle are $(0,0)$ , $(1,0)$ , and $(0,1)$ . To check whether there is a maximum in the interior of the triangle, would I just check to find points where both partials vanish, or do I need to apply Lagrange multipliers? If so, what would I use for the constraint function? On the boundaries, I know to check the values at the three vertices, but what about all the other points on the boundaries?","I want to find the maximum of on the triangle given by , , and in terms of and such that . I can see that the vertices of the triangle are , , and . To check whether there is a maximum in the interior of the triangle, would I just check to find points where both partials vanish, or do I need to apply Lagrange multipliers? If so, what would I use for the constraint function? On the boundaries, I know to check the values at the three vertices, but what about all the other points on the boundaries?","f(x,y) = x^ae^{-x}y^be^{-y} x\geq0 y\geq0 x+y\leq1 a b a,b>0 (0,0) (1,0) (0,1)",['multivariable-calculus']
42,Double integral over a region with two bounds,Double integral over a region with two bounds,,"Calculate the integral of $f(x, y)$ $=$ $3x$ over the region bounded above by $y=x(2-x)$ and below by $x=y(2-y)$ . What I tried to do here was first expand the two equations out to get $y=x^2-x$ and $x=y^2-y$ . I then rearranged the second equation to get $y=1-\sqrt{1-x}$ . But when i tried to graph this region, I ran into a problem as the second equation is invalid for values of $x$ larger than $1$ . But I do feel that the first equation will be the lower bound of the double integral and the second equation will be upper bound of the integral. But because of the above problem, I can not find any intersection points so I am a little confused on how to proceed further. Any help would be highly appreciated!","Calculate the integral of over the region bounded above by and below by . What I tried to do here was first expand the two equations out to get and . I then rearranged the second equation to get . But when i tried to graph this region, I ran into a problem as the second equation is invalid for values of larger than . But I do feel that the first equation will be the lower bound of the double integral and the second equation will be upper bound of the integral. But because of the above problem, I can not find any intersection points so I am a little confused on how to proceed further. Any help would be highly appreciated!","f(x, y) = 3x y=x(2-x) x=y(2-y) y=x^2-x x=y^2-y y=1-\sqrt{1-x} x 1","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
43,Help:For what $ \alpha $ do the integrals exist?,Help:For what  do the integrals exist?, \alpha ,"Let $ K_R (y) \subset \mathbb{R}^n $ be  an n-dimensional Ball with radius $R>0$ and midpoint $y \in \mathbb{R}^n $ Far what $ \alpha \in \mathbb{R} $ and $ n \in \mathbb{N}\{0\} $ do following integrals exist: $ \int_{K{_R(y)}} |x-y|^{\alpha}dx $ $ \int_{ \mathbb{R}^n \setminus K{_R(y)}} |x-y|^{\alpha}dx $ $ \int_{ \mathbb{R}^n } |x-y|^{\alpha}dx $ with what criteria can i solve this task? I don't know how to begin. Thanks for any help!! Let's take $ \int_{K{_R(y)}} |x-y|^{\alpha}dx $ , $y=0, n=2 $ If I replace by polarcoordinates $ x= rsin \phi $ : $ \int_0^{2 \pi} \int_0^R | r|^{\alpha} r dr d\phi $ ..is this the right way?","Let be  an n-dimensional Ball with radius and midpoint Far what and do following integrals exist: with what criteria can i solve this task? I don't know how to begin. Thanks for any help!! Let's take , If I replace by polarcoordinates : ..is this the right way?"," K_R (y) \subset \mathbb{R}^n  R>0 y \in \mathbb{R}^n   \alpha \in \mathbb{R}   n \in \mathbb{N}\{0\}   \int_{K{_R(y)}} |x-y|^{\alpha}dx   \int_{ \mathbb{R}^n \setminus K{_R(y)}} |x-y|^{\alpha}dx   \int_{ \mathbb{R}^n } |x-y|^{\alpha}dx   \int_{K{_R(y)}} |x-y|^{\alpha}dx  y=0, n=2   x= rsin \phi   \int_0^{2 \pi} \int_0^R | r|^{\alpha} r dr d\phi ","['real-analysis', 'integration', 'multivariable-calculus']"
44,"If $F(x,y)=\int_{f(x,y)}^{g(x,y)}h(x,y,t)\, dt$, then find $\partial F/\partial x$.","If , then find .","F(x,y)=\int_{f(x,y)}^{g(x,y)}h(x,y,t)\, dt \partial F/\partial x","Let $f,g:\mathbb{R}^2\to \mathbb{R}$ and $h:\mathbb{R}^3\to\mathbb{R}$ be $C^1$ , define $F(x,y)=\int_{f(x,y)}^{g(x,y)}h(x,y,t)\, dt$ . Find $\frac{\partial F}{\partial x}$ . So I split $$F(x,y)=\int_0^{g(x,y)}h(x,y,t)\, dt-\int_0^{f(x,y)}h(x,y,t)\, dt.$$ And then using FTC I believe I have $$F(x,y)=H(x,y,g(x,y))-H(x,y,0)-H(x,y,f(x,y))+H(x,y,0)=H(x,y,g(x,y))-H(x,y,f(x,y))$$ Then differentiating this new thing with respect to x. $$\frac{\partial F}{\partial x}=\frac{\partial H}{\partial x}+\frac{\partial H}{\partial y}\frac{\partial y}{\partial x}+\frac{\partial H}{\partial g(x,y)}\frac{\partial g(x,y)}{\partial x}-\frac{\partial H}{\partial x}-\frac{\partial H}{\partial y}\frac{\partial y}{\partial x}-\frac{\partial H}{\partial f(x,y)}\frac{\partial f(x,y)}{\partial x}$$ Is this correct? From here I would simplify the expression and expand out the partials. But I'm not sure that the FTC works in this way for $H$ , since it's a function of 3 variables.","Let and be , define . Find . So I split And then using FTC I believe I have Then differentiating this new thing with respect to x. Is this correct? From here I would simplify the expression and expand out the partials. But I'm not sure that the FTC works in this way for , since it's a function of 3 variables.","f,g:\mathbb{R}^2\to \mathbb{R} h:\mathbb{R}^3\to\mathbb{R} C^1 F(x,y)=\int_{f(x,y)}^{g(x,y)}h(x,y,t)\, dt \frac{\partial F}{\partial x} F(x,y)=\int_0^{g(x,y)}h(x,y,t)\, dt-\int_0^{f(x,y)}h(x,y,t)\, dt. F(x,y)=H(x,y,g(x,y))-H(x,y,0)-H(x,y,f(x,y))+H(x,y,0)=H(x,y,g(x,y))-H(x,y,f(x,y)) \frac{\partial F}{\partial x}=\frac{\partial H}{\partial x}+\frac{\partial H}{\partial y}\frac{\partial y}{\partial x}+\frac{\partial H}{\partial g(x,y)}\frac{\partial g(x,y)}{\partial x}-\frac{\partial H}{\partial x}-\frac{\partial H}{\partial y}\frac{\partial y}{\partial x}-\frac{\partial H}{\partial f(x,y)}\frac{\partial f(x,y)}{\partial x} H","['multivariable-calculus', 'proof-verification']"
45,Calculus of variations confusion,Calculus of variations confusion,,"Let's say I have the lagrangian $$L(x,u,u')=u'$$ If apply the Euler-Lagrange equation for $L$ , I have: $$\frac{\partial L}{\partial u}-\frac{d}{dx}\frac{\partial L}{\partial u'}=0-\frac{d}{dx}(1)=0$$ If I choose to apply to $L^2$ , I have: \begin{equation} \frac{\partial L^2}{\partial u}-\frac{d}{dx}\frac{\partial L^2}{\partial u'}=0-\frac{d}{dx}(2u')=-2u'' \tag{1} \end{equation} If I expand the Euler-Langrange equation for $L^2$ , I get: \begin{equation} \frac{\partial L^2}{\partial u}-\frac{d}{dx}\frac{\partial L^2}{\partial u'}=2L\frac{\partial L}{\partial u}-\frac{d}{dx}(2L\frac{\partial L}{\partial u'})=2L(\frac{\partial L}{\partial u}-\frac{d}{dx}\frac{\partial L}{\partial u'})-2\frac{dL}{dx}\frac{\partial L}{\partial u'} \end{equation} $L$ does not explicitly depend on x, so: \begin{equation} \frac{\partial L^2}{\partial u}-\frac{d}{dx}\frac{\partial L^2}{\partial u'}=2L(\frac{\partial L}{\partial u}-\frac{d}{dx}\frac{\partial L}{\partial u'})=2L(0)=0?? \tag{2} \end{equation} Why does $(1)$ differ from $(2)$ ? What am I doing wrong?","Let's say I have the lagrangian If apply the Euler-Lagrange equation for , I have: If I choose to apply to , I have: If I expand the Euler-Langrange equation for , I get: does not explicitly depend on x, so: Why does differ from ? What am I doing wrong?","L(x,u,u')=u' L \frac{\partial L}{\partial u}-\frac{d}{dx}\frac{\partial L}{\partial u'}=0-\frac{d}{dx}(1)=0 L^2 \begin{equation}
\frac{\partial L^2}{\partial u}-\frac{d}{dx}\frac{\partial L^2}{\partial u'}=0-\frac{d}{dx}(2u')=-2u'' \tag{1}
\end{equation} L^2 \begin{equation}
\frac{\partial L^2}{\partial u}-\frac{d}{dx}\frac{\partial L^2}{\partial u'}=2L\frac{\partial L}{\partial u}-\frac{d}{dx}(2L\frac{\partial L}{\partial u'})=2L(\frac{\partial L}{\partial u}-\frac{d}{dx}\frac{\partial L}{\partial u'})-2\frac{dL}{dx}\frac{\partial L}{\partial u'}
\end{equation} L \begin{equation}
\frac{\partial L^2}{\partial u}-\frac{d}{dx}\frac{\partial L^2}{\partial u'}=2L(\frac{\partial L}{\partial u}-\frac{d}{dx}\frac{\partial L}{\partial u'})=2L(0)=0?? \tag{2}
\end{equation} (1) (2)","['calculus', 'multivariable-calculus', 'calculus-of-variations']"
46,Multivariate function to univariate function,Multivariate function to univariate function,,"I'm currently trying to understand the proof of the directional derivative of a multivariate function $f(x,y)$ at the point $(x_0,y_0)$ along the vector $\vec u \langle a,b \rangle $ . $$D_\vec u f(x_0,y_0) = \lim_{h\to0} \dfrac{f(x_0 + ah, y_0 + bh) - f(x_0, y_0)}{h}$$ Then, it says let's put: $$ g(h) = f(x_0 + ah, y_0 + bh) $$ How can we change a function of 2 variables to a function of only one variable, which is a totally different one. Is it really valid? How is this process called, I couldn't find the exact name to search for more details. Thank you.","I'm currently trying to understand the proof of the directional derivative of a multivariate function at the point along the vector . Then, it says let's put: How can we change a function of 2 variables to a function of only one variable, which is a totally different one. Is it really valid? How is this process called, I couldn't find the exact name to search for more details. Thank you.","f(x,y) (x_0,y_0) \vec u \langle a,b \rangle  D_\vec u f(x_0,y_0) = \lim_{h\to0} \dfrac{f(x_0 + ah, y_0 + bh) - f(x_0, y_0)}{h}  g(h) = f(x_0 + ah, y_0 + bh) ","['calculus', 'multivariable-calculus']"
47,2D limit with exponentials,2D limit with exponentials,,"I want to show that $$\lim_{(a,b)\to (0,0)^+} \frac{ 1-e^{-(a+b)} }{ (1-e^{-a})(1-e^{-b}) } - \frac 1a - \frac 1b =0 $$ where the + means that the limit is taken only with respect to paths in the nonnegative quarter .(i.e, a,b are non-negative)","I want to show that where the + means that the limit is taken only with respect to paths in the nonnegative quarter .(i.e, a,b are non-negative)","\lim_{(a,b)\to (0,0)^+} \frac{ 1-e^{-(a+b)} }{ (1-e^{-a})(1-e^{-b}) } - \frac 1a - \frac 1b =0 ","['real-analysis', 'multivariable-calculus', 'exponential-function']"
48,"Show that $f(x,y)$=$\sqrt[3]{\lvert x^2-(y+1)^2 \lvert}\sin(\lvert x+y+1 \lvert))$ is differentiable at $(0,-1)$, $(1,0)$ and $(-1,0)$","Show that = is differentiable at ,  and","f(x,y) \sqrt[3]{\lvert x^2-(y+1)^2 \lvert}\sin(\lvert x+y+1 \lvert)) (0,-1) (1,0) (-1,0)","I want to show that this function is differentiable in these points. At $(0,-1)$ i use the definition to check the differentiability: At $(0,-1)$ i have that $f(0,-1)=0$ , $f_y(0,-1)=0$ and $f_x(0,-1)=0$ . To prove differentiability i need to check if the limit $$\lim_{(x,y)\to (0,-1)}\frac{f(x,y)-f(0,-1)-f_x(0,-1)(x-0)-f_y(0,-1)(y+1)}{\sqrt{(x-0)^2+(y+1)^2}}$$ equals to zero. This limit becomes $$\lim_{(x,y)\to (0,-1)}\frac{\sqrt[3]{\lvert x^2-(y+1)^2 \lvert}\sin(\lvert x+y+1 \lvert))}{\sqrt{x^2+(y+1)^2}}$$ I am not sure how to proceed. Thanks for the help in advance!!","I want to show that this function is differentiable in these points. At i use the definition to check the differentiability: At i have that , and . To prove differentiability i need to check if the limit equals to zero. This limit becomes I am not sure how to proceed. Thanks for the help in advance!!","(0,-1) (0,-1) f(0,-1)=0 f_y(0,-1)=0 f_x(0,-1)=0 \lim_{(x,y)\to (0,-1)}\frac{f(x,y)-f(0,-1)-f_x(0,-1)(x-0)-f_y(0,-1)(y+1)}{\sqrt{(x-0)^2+(y+1)^2}} \lim_{(x,y)\to (0,-1)}\frac{\sqrt[3]{\lvert x^2-(y+1)^2 \lvert}\sin(\lvert x+y+1 \lvert))}{\sqrt{x^2+(y+1)^2}}","['calculus', 'real-analysis', 'limits', 'multivariable-calculus', 'derivatives']"
49,"How to ""split up"" triple integrals when changing order of integration?","How to ""split up"" triple integrals when changing order of integration?",,"I'm trying to set up six different triple integrals of a tetrahedron and am having difficulty changing the order of integration, particularly finding the bounds for dzdxdy/dzdydx and dxdydz/dxdzdy. I am given the vertices of the tetrahedron, (8,7,1), (7,4,1), (2,5,1), and (5,5,5), and I have found the equations of the planes (see image below). I was able to come up with the projection on the xy-plane (see image below), but don't know how to ""split it up"" so I can find the bounds/integrate. I know with certain problems you need to divide the shape into subsections so each part can be integrated separately and then add them, but I've scoured my book, notes, and the internet and am still not sure how to split this particular triangle up. Is there a method to it (e.g. equations you can solve for), or do you just split it up however you like so that integration is possible for each of the sections? I'd appreciate any help regarding how I can move forward with this problem specifically, or just general information about how to ""break up"" integrals that are 2D projections. Thanks!","I'm trying to set up six different triple integrals of a tetrahedron and am having difficulty changing the order of integration, particularly finding the bounds for dzdxdy/dzdydx and dxdydz/dxdzdy. I am given the vertices of the tetrahedron, (8,7,1), (7,4,1), (2,5,1), and (5,5,5), and I have found the equations of the planes (see image below). I was able to come up with the projection on the xy-plane (see image below), but don't know how to ""split it up"" so I can find the bounds/integrate. I know with certain problems you need to divide the shape into subsections so each part can be integrated separately and then add them, but I've scoured my book, notes, and the internet and am still not sure how to split this particular triangle up. Is there a method to it (e.g. equations you can solve for), or do you just split it up however you like so that integration is possible for each of the sections? I'd appreciate any help regarding how I can move forward with this problem specifically, or just general information about how to ""break up"" integrals that are 2D projections. Thanks!",,"['integration', 'multivariable-calculus']"
50,"Evaluate using polar coordinates: $\iint_Re^{-x^2-y^2}dA$, where $R$ is part of an annulus bounded by $y=|x|$","Evaluate using polar coordinates: , where  is part of an annulus bounded by",\iint_Re^{-x^2-y^2}dA R y=|x|,"$R$ is the portion of the annulus $\{(x,y):4\le\sqrt{x^2+y^2}\le9\}$ in the upper half plane, bounded by the graph $y=|x|$ . Evaluate the integral $$\iint_Re^{-x^2-y^2}dA$$ My Try: $$\sqrt{x^2+y^2}=4$$ $$x^2+y^2=16$$ $$\sqrt{x^2+y^2}=9$$ $$x^2+y^2=81$$ So, $4\le r\le9$ and $\dfrac{\pi}{4}\le\theta\le\dfrac{3\pi}{4}$ So, my integral will be $$\int_{\pi/4}^{3\pi/4}\int_{4}^{9}re^{-r^2}dr\ d\theta$$ Is my above integral correct?","is the portion of the annulus in the upper half plane, bounded by the graph . Evaluate the integral My Try: So, and So, my integral will be Is my above integral correct?","R \{(x,y):4\le\sqrt{x^2+y^2}\le9\} y=|x| \iint_Re^{-x^2-y^2}dA \sqrt{x^2+y^2}=4 x^2+y^2=16 \sqrt{x^2+y^2}=9 x^2+y^2=81 4\le r\le9 \dfrac{\pi}{4}\le\theta\le\dfrac{3\pi}{4} \int_{\pi/4}^{3\pi/4}\int_{4}^{9}re^{-r^2}dr\ d\theta","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
51,"Proving that $\frac{D(s_1 , s_2)}{D(a,b)} \frac{D(a,b)}{D(x,y)} = \frac{D(s_1 ,s_2)}{D(x,y)}$",Proving that,"\frac{D(s_1 , s_2)}{D(a,b)} \frac{D(a,b)}{D(x,y)} = \frac{D(s_1 ,s_2)}{D(x,y)}","Using Lagrangian discribtion for incompressible fluid . $$\frac{D(x,y)}{D(a,b)}=\begin{vmatrix} \frac{\partial x}{\partial a} & \frac{\partial y}{\partial a}  \\  \frac{\partial x}{\partial b} & \frac{\partial y}{\partial b}  \\  \end{vmatrix}=1 $$ Prove that : $$\frac{D(s_1 , s_2)}{D(a,b)} \frac{D(a,b)}{D(x,y)} = \frac{D(s_1 ,s_2)}{D(x,y)}$$ Which implies that $$\begin{vmatrix} \frac{\partial s_1}{\partial a} & \frac{\partial s_2}{\partial a}  \\  \frac{\partial s_1}{\partial b} & \frac{\partial s_2}{\partial b}  \\  \end{vmatrix} \begin{vmatrix} \frac{\partial a}{\partial x} & \frac{\partial b}{\partial x}  \\  \frac{\partial a}{\partial y} & \frac{\partial b}{\partial y}  \\  \end{vmatrix}=\begin{vmatrix} \frac{\partial s_1}{\partial x} & \frac{\partial s_2}{\partial x}  \\  \frac{\partial s_1}{\partial y} & \frac{\partial s_2}{\partial y}  \\  \end{vmatrix}$$ which implies that $$(\frac{\partial s_1}{\partial a} \frac{\partial s_2}{\partial b} -\frac{\partial s_2}{\partial a} \frac{\partial s_1}{\partial b})(\frac{\partial a}{\partial x} \frac{\partial b}{\partial y}-\frac{\partial b}{\partial x} \frac{\partial a}{\partial y})=2\frac{\partial s_1}{\partial x}\frac{\partial s_2}{\partial y}-\frac{\partial s_2}{\partial x}\frac{\partial s_1}{\partial y}$$ What is wrong ?  It is not supposed to get 2 . And the other question is if $$\frac{D(x,y)}{D(a,b)}=1$$ Then $$\frac{D(a,b)}{D(x,y)}=1$$",Using Lagrangian discribtion for incompressible fluid . Prove that : Which implies that which implies that What is wrong ?  It is not supposed to get 2 . And the other question is if Then,"\frac{D(x,y)}{D(a,b)}=\begin{vmatrix}
\frac{\partial x}{\partial a} & \frac{\partial y}{\partial a}  \\ 
\frac{\partial x}{\partial b} & \frac{\partial y}{\partial b}  \\ 
\end{vmatrix}=1  \frac{D(s_1 , s_2)}{D(a,b)} \frac{D(a,b)}{D(x,y)} = \frac{D(s_1 ,s_2)}{D(x,y)} \begin{vmatrix}
\frac{\partial s_1}{\partial a} & \frac{\partial s_2}{\partial a}  \\ 
\frac{\partial s_1}{\partial b} & \frac{\partial s_2}{\partial b}  \\ 
\end{vmatrix} \begin{vmatrix}
\frac{\partial a}{\partial x} & \frac{\partial b}{\partial x}  \\ 
\frac{\partial a}{\partial y} & \frac{\partial b}{\partial y}  \\ 
\end{vmatrix}=\begin{vmatrix}
\frac{\partial s_1}{\partial x} & \frac{\partial s_2}{\partial x}  \\ 
\frac{\partial s_1}{\partial y} & \frac{\partial s_2}{\partial y}  \\ 
\end{vmatrix} (\frac{\partial s_1}{\partial a} \frac{\partial s_2}{\partial b} -\frac{\partial s_2}{\partial a} \frac{\partial s_1}{\partial b})(\frac{\partial a}{\partial x} \frac{\partial b}{\partial y}-\frac{\partial b}{\partial x} \frac{\partial a}{\partial y})=2\frac{\partial s_1}{\partial x}\frac{\partial s_2}{\partial y}-\frac{\partial s_2}{\partial x}\frac{\partial s_1}{\partial y} \frac{D(x,y)}{D(a,b)}=1 \frac{D(a,b)}{D(x,y)}=1","['multivariable-calculus', 'chain-rule', 'fluid-dynamics']"
52,"$\min f(x)=\frac{1}{2}x^t Bx + c^t x$ subject to $Ax=b, x\ge 0$ then $f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda})$",subject to  then,"\min f(x)=\frac{1}{2}x^t Bx + c^t x Ax=b, x\ge 0 f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda})","$$\min f(x)=\frac{1}{2}x^t Bx + c^t x\\\mbox{s.t.} \\Ax=b\\ x\ge 0$$ Let $\overline{x}$ be a regular solution of the problem and $\overline{\lambda}$ the vector of Lagrange Multipliers associated to   the equality restrictions. Prove that $f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda})$ This problem is the same as $$\min f(x)=\frac{1}{2}x^t Bx + c^t x\\\mbox{s.t.} \\Ax-b\le 0\\ -x\le 0$$ I can't use advances method like KKT or whatever, so to solve problems with inequality constraints I must create a matrix with all the inequalities and analyze the intersection points. That is, the points $x$ such that $a_ix-b_i= 0$ and $a_jx-b_j= 0$ for some $i$ and $j$ or that $a_ix-b_i= 0$ and $-x\le 0$ . I must investigate if the points $x$ in the intersection are written like this: $\nabla f(x) = B_I\lambda$ , where $B$ is the matrix of all inequalities, and $B_I$ is the submatrix of $B$ that contains only the inequalities of the intersection. However I have some questions. The minimum point will always be in the intersections? If it's true, how do I arrive at the final result? If $\overline{x}$ is a point in the intersection of two inequalities and satisfy $\nabla f(\overline{x})=B_I\overline{\lambda}$ for negative $\lambda_i$ for all $i$ then how $f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda})$ ?","Let be a regular solution of the problem and the vector of Lagrange Multipliers associated to   the equality restrictions. Prove that This problem is the same as I can't use advances method like KKT or whatever, so to solve problems with inequality constraints I must create a matrix with all the inequalities and analyze the intersection points. That is, the points such that and for some and or that and . I must investigate if the points in the intersection are written like this: , where is the matrix of all inequalities, and is the submatrix of that contains only the inequalities of the intersection. However I have some questions. The minimum point will always be in the intersections? If it's true, how do I arrive at the final result? If is a point in the intersection of two inequalities and satisfy for negative for all then how ?",\min f(x)=\frac{1}{2}x^t Bx + c^t x\\\mbox{s.t.} \\Ax=b\\ x\ge 0 \overline{x} \overline{\lambda} f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda}) \min f(x)=\frac{1}{2}x^t Bx + c^t x\\\mbox{s.t.} \\Ax-b\le 0\\ -x\le 0 x a_ix-b_i= 0 a_jx-b_j= 0 i j a_ix-b_i= 0 -x\le 0 x \nabla f(x) = B_I\lambda B B_I B \overline{x} \nabla f(\overline{x})=B_I\overline{\lambda} \lambda_i i f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda}),"['linear-algebra', 'multivariable-calculus', 'optimization', 'linear-programming', 'lagrange-multiplier']"
53,"Proving that $\frac{D(a,b,c)}{D(x,y,z)}=1$",Proving that,"\frac{D(a,b,c)}{D(x,y,z)}=1","In deriving continuity equations using Lagrangian . We consider the element of fluid which occupied a rectangular parallelopiped having its centre at the point $(a,b,c)$ and its edges $\delta a$ , $\delta b$ , $\delta c $ parallel to the axes . At the time $t$ the same element for an oblique parallelepiped . The centre now has for its co-ordinates $x$ , $y$ , $z$ ; and the projections of the edges on the co-ordinate axes are respectively $$ \frac{\partial x}{\partial a} \delta a \ , \ \frac{\partial y}{\partial a} \delta a \ , \ \frac{\partial z}{\partial c} \delta a$$ $$\frac{\partial x}{\partial b} \delta b \ , \ \frac{\partial y}{\partial b} \delta b \ , \ \frac{\partial z}{\partial b} \delta b$$ $$\frac{\partial x}{\partial c} \delta c \ , \ \frac{\partial y}{\partial c} \delta c \ , \ \frac{\partial z}{\partial c} \delta c$$ How can i get these projections ? The volume of the parallelepiped is therefore $$\begin{vmatrix} \frac{\partial x}{\partial a} & \frac{\partial y}{\partial a} & \frac{\partial z}{\partial a} \\  \frac{\partial x}{\partial b} & \frac{\partial y}{\partial b} & \frac{\partial z}{\partial b} \\  \frac{\partial x}{\partial c} & \frac{\partial y}{\partial c} & \frac{\partial z}{\partial c} \end{vmatrix} \delta a \delta b \delta c$$ or as its often written $$\frac{D(x,y,z)}{D(a,b,c)} \delta a \delta b \delta c$$ since the fluid mass is unchanged and the fluid is incompressible we have $$\frac{D(x,y,z)}{D(a,b,c)} =1$$ Is there a way to prove that $$\frac{D(a,b,c)}{D(x,y,z)}= 1$$ without expanding the determinant ?","In deriving continuity equations using Lagrangian . We consider the element of fluid which occupied a rectangular parallelopiped having its centre at the point and its edges , , parallel to the axes . At the time the same element for an oblique parallelepiped . The centre now has for its co-ordinates , , ; and the projections of the edges on the co-ordinate axes are respectively How can i get these projections ? The volume of the parallelepiped is therefore or as its often written since the fluid mass is unchanged and the fluid is incompressible we have Is there a way to prove that without expanding the determinant ?","(a,b,c) \delta a \delta b \delta c  t x y z  \frac{\partial x}{\partial a} \delta a \ , \ \frac{\partial y}{\partial a} \delta a \ , \ \frac{\partial z}{\partial c} \delta a \frac{\partial x}{\partial b} \delta b \ , \ \frac{\partial y}{\partial b} \delta b \ , \ \frac{\partial z}{\partial b} \delta b \frac{\partial x}{\partial c} \delta c \ , \ \frac{\partial y}{\partial c} \delta c \ , \ \frac{\partial z}{\partial c} \delta c \begin{vmatrix}
\frac{\partial x}{\partial a} & \frac{\partial y}{\partial a} & \frac{\partial z}{\partial a} \\ 
\frac{\partial x}{\partial b} & \frac{\partial y}{\partial b} & \frac{\partial z}{\partial b} \\ 
\frac{\partial x}{\partial c} & \frac{\partial y}{\partial c} & \frac{\partial z}{\partial c}
\end{vmatrix} \delta a \delta b \delta c \frac{D(x,y,z)}{D(a,b,c)} \delta a \delta b \delta c \frac{D(x,y,z)}{D(a,b,c)} =1 \frac{D(a,b,c)}{D(x,y,z)}= 1","['multivariable-calculus', 'derivatives', 'vector-spaces', 'fluid-dynamics']"
54,Smoothness of a parametric curve,Smoothness of a parametric curve,,"Suppose that f(t) and g(t) are differentiable on [a, b]. What can be said about the smoothness of the curve parameterized by x = f(t) and y = g(t) on [a, b]? (Smooth here is used in the sense of having no corners or cusps.) A) It must be smooth B) It might or might not be smooth. C) It cannot be smooth. How is the answer B? I thought it would always be smooth when you differentiate a function. Can someone explain?","Suppose that f(t) and g(t) are differentiable on [a, b]. What can be said about the smoothness of the curve parameterized by x = f(t) and y = g(t) on [a, b]? (Smooth here is used in the sense of having no corners or cusps.) A) It must be smooth B) It might or might not be smooth. C) It cannot be smooth. How is the answer B? I thought it would always be smooth when you differentiate a function. Can someone explain?",,['multivariable-calculus']
55,"Integrating $\iint_{R} \frac{y}{x^2+y^2}\,dA$ where $R$ is bounded by $y=x,y=2x,x=2$",Integrating  where  is bounded by,"\iint_{R} \frac{y}{x^2+y^2}\,dA R y=x,y=2x,x=2","So I'm supposed to set an integral up for both orders of integration and evaluate using the ""nicer"" of the two $$\iint_{R} \frac{y}{x^2+y^2} \,dA$$ for $R$ bounded by $y=x, y = 2x$ , and $x=2$ . So what I have tried to do so far was figuring out the region which I'm integrating over which is So this is what my two integrals are: $$\int_{0}^{2}\int_{y=x}^{y=2x} \frac{y}{x^2+y^2} \,dy\, dx$$ $$\int_{0}^{2}\int_{x=\frac{y}{2}}^{x=y} \frac{y}{x^2+y^2}\, dx\, dy$$ I believe the easier integral would be the second one when you integrate with respect to $dy$ first, because since $y$ would be a constant,  you can factor out $x$ and integrate $\frac{1}{x^2+y^2}$ .   I was wondering if my thought process is correct, and if it does can you help me integrate this?","So I'm supposed to set an integral up for both orders of integration and evaluate using the ""nicer"" of the two for bounded by , and . So what I have tried to do so far was figuring out the region which I'm integrating over which is So this is what my two integrals are: I believe the easier integral would be the second one when you integrate with respect to first, because since would be a constant,  you can factor out and integrate .   I was wondering if my thought process is correct, and if it does can you help me integrate this?","\iint_{R} \frac{y}{x^2+y^2} \,dA R y=x, y = 2x x=2 \int_{0}^{2}\int_{y=x}^{y=2x} \frac{y}{x^2+y^2} \,dy\, dx \int_{0}^{2}\int_{x=\frac{y}{2}}^{x=y} \frac{y}{x^2+y^2}\, dx\, dy dy y x \frac{1}{x^2+y^2}","['integration', 'multivariable-calculus']"
56,Taking the gradient of $||\nabla f(x) - p||$ with respect to $x$ and $p$,Taking the gradient of  with respect to  and,||\nabla f(x) - p|| x p,"$$||\nabla f(x) - p||$$ I'm trying to take the gradients of this with respect to $x$ and $p$ . For $x$ , this is what I did: $$g(x) = ||\nabla f(x) - p || = \sqrt{(\frac{\partial f}{\partial x_1}-p_1)^2 + \cdots +(\frac{\partial f}{\partial x_n}-p_n)}\implies\\ \frac{\partial g}{\partial x_i} = \frac{2(\frac{\partial f}{\partial x_i}-p_i)\frac{\partial ^2 f}{\partial x_i^2}}{2\sqrt{(\frac{\partial f}{\partial x_1}-p_1)^2 + \cdots +(\frac{\partial f}{\partial x_n}-p_n)}}$$ however I don't think there's a closed form of writing $\nabla g$ . Am I right? For $p$ : $$h(p) = ||\nabla f(x) -p|| = \sqrt{(\frac{\partial f}{\partial x_1}-p_1)^2 + \cdots +(\frac{\partial f}{\partial x_n}-p_n)}\implies \\ \frac{\partial h}{\partial p_i} = \frac{2(\frac{\partial f}{\partial x_i}-p_i)(-1)}{2\sqrt{(\frac{\partial f}{\partial x_1}-p_1)^2 + \cdots +(\frac{\partial f}{\partial x_n}-p_n)}}$$ So $$\nabla h(p) = \frac{p-\nabla f(x)}{||\nabla f(x) -p||}$$ but the result in my book is just $p-\nabla f(x)$ . What am I doing wrong?","I'm trying to take the gradients of this with respect to and . For , this is what I did: however I don't think there's a closed form of writing . Am I right? For : So but the result in my book is just . What am I doing wrong?",||\nabla f(x) - p|| x p x g(x) = ||\nabla f(x) - p || = \sqrt{(\frac{\partial f}{\partial x_1}-p_1)^2 + \cdots +(\frac{\partial f}{\partial x_n}-p_n)}\implies\\ \frac{\partial g}{\partial x_i} = \frac{2(\frac{\partial f}{\partial x_i}-p_i)\frac{\partial ^2 f}{\partial x_i^2}}{2\sqrt{(\frac{\partial f}{\partial x_1}-p_1)^2 + \cdots +(\frac{\partial f}{\partial x_n}-p_n)}} \nabla g p h(p) = ||\nabla f(x) -p|| = \sqrt{(\frac{\partial f}{\partial x_1}-p_1)^2 + \cdots +(\frac{\partial f}{\partial x_n}-p_n)}\implies \\ \frac{\partial h}{\partial p_i} = \frac{2(\frac{\partial f}{\partial x_i}-p_i)(-1)}{2\sqrt{(\frac{\partial f}{\partial x_1}-p_1)^2 + \cdots +(\frac{\partial f}{\partial x_n}-p_n)}} \nabla h(p) = \frac{p-\nabla f(x)}{||\nabla f(x) -p||} p-\nabla f(x),"['calculus', 'linear-algebra', 'multivariable-calculus', 'derivatives']"
57,Determine if this two-variable piecewise function is continuous,Determine if this two-variable piecewise function is continuous,,"I have this piecewise function and I have to determine if it is continuous in $(0,0)$ $$f(x)=\begin{cases}\dfrac{\sqrt[]{|x|}\sin^2y}{x^2+y^2}&\text{if }(x,y)\neq (0,0)\\0&\text{if }(x,y)=(0,0)\end{cases}$$ So I have to prove if $$\lim_{(x,y)\to(0,0)}\dfrac{\sqrt[]{|x|}\sin^2y}{x^2+y^2}=0$$ Thus I used polar coordinates and got $$\lim_{\rho\to0}\dfrac{\sqrt[]{\rho|\cos\theta|}\sin^2(\rho\sin\theta)}{\rho^2}$$ I know that $\cos x\leq1, \sin x\leq1$ , then $$\frac{\sqrt[]{\rho|\cos\theta|}\sin^2(\rho\sin\theta)}{\rho^2}<\frac{\sqrt\rho\sin^2\rho}{\rho^2}=g(\rho)$$ $$\lim_{\rho\to0}g(\rho)=0\implies \lim_{(x,y)\to(0,0)}\dfrac{\sqrt[]{|x|}\sin^2y}{x^2+y^2}=0$$ However Wolfram Alpha says the limit does not exist. Why? I thought that this procedure of polar coordinates yields the final result. Could Wolfram Alpha also consider  complex $x,y$ ?","I have this piecewise function and I have to determine if it is continuous in So I have to prove if Thus I used polar coordinates and got I know that , then However Wolfram Alpha says the limit does not exist. Why? I thought that this procedure of polar coordinates yields the final result. Could Wolfram Alpha also consider  complex ?","(0,0) f(x)=\begin{cases}\dfrac{\sqrt[]{|x|}\sin^2y}{x^2+y^2}&\text{if }(x,y)\neq (0,0)\\0&\text{if }(x,y)=(0,0)\end{cases} \lim_{(x,y)\to(0,0)}\dfrac{\sqrt[]{|x|}\sin^2y}{x^2+y^2}=0 \lim_{\rho\to0}\dfrac{\sqrt[]{\rho|\cos\theta|}\sin^2(\rho\sin\theta)}{\rho^2} \cos x\leq1, \sin x\leq1 \frac{\sqrt[]{\rho|\cos\theta|}\sin^2(\rho\sin\theta)}{\rho^2}<\frac{\sqrt\rho\sin^2\rho}{\rho^2}=g(\rho) \lim_{\rho\to0}g(\rho)=0\implies \lim_{(x,y)\to(0,0)}\dfrac{\sqrt[]{|x|}\sin^2y}{x^2+y^2}=0 x,y","['limits', 'multivariable-calculus']"
58,"Given a norm, show a map is Affine","Given a norm, show a map is Affine",,"So I have this problem: Let $\|v\|_1=|x|+|y|$ be a norm in the plane, if $v=(x,y)$ . Now I'm asked to show that, if $T: \mathbb R^2 \rightarrow \mathbb R^2 $ satisfies $\|T(v)-T(u)\|_1=\|v-u\|_1$ , then $T$ is of the type $T(v)=p+A(v)$ with $A$ a linear map. I am not quite sure where to start, I have worked for some time with properties of norms, and I don't arrive anywhere.  It may be similar in some way to proving that a rigid motion satisfying that norm given is affine... but I can't find the way to prove this. Can you help me with this problem?","So I have this problem: Let be a norm in the plane, if . Now I'm asked to show that, if satisfies , then is of the type with a linear map. I am not quite sure where to start, I have worked for some time with properties of norms, and I don't arrive anywhere.  It may be similar in some way to proving that a rigid motion satisfying that norm given is affine... but I can't find the way to prove this. Can you help me with this problem?","\|v\|_1=|x|+|y| v=(x,y) T: \mathbb R^2 \rightarrow \mathbb R^2  \|T(v)-T(u)\|_1=\|v-u\|_1 T T(v)=p+A(v) A","['calculus', 'multivariable-calculus', 'normed-spaces', 'affine-geometry', 'rigid-transformation']"
59,Points of Confusion About Second-Order Taylor Formula of Taylor's Theorem For Many Variables,Points of Confusion About Second-Order Taylor Formula of Taylor's Theorem For Many Variables,,"My textbook has written the following for the second-order Taylor formula of Taylor's theorem for many variables: $$f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x_0}) + \sum_{i = 1}^n h_i \dfrac{\partial{f}}{\partial{x_i}}(\mathbf{x}_0) + \dfrac{1}{2} \sum_{i, j = 1}^n h_i h_j \dfrac{\partial^2 f}{\partial{x_i} \partial{x_j}} (\mathbf{x}_0) + R_2(\mathbf{x}_0, \mathbf{h}),$$ where $$R_2(\mathbf{x}_0, \mathbf{h}) = \sum_{i, j, k = 1}^n \int_0^1 \dfrac{(t - 1)^2}{2} \dfrac{\partial^3{f}}{\partial{x_i} \partial{x_j} \partial{x_k}} (\mathbf{x_0 + t \mathbf{h}})h_i h_j h_k \ dt.$$ The integrand is a continuous function of $t$ and is therefore bounded by a positive constant $C$ on a small neighbourhood of $\mathbf{x}_0$ (because it has to be close to its value at $\mathbf{x}_0$ ). Also note that $|h_i| \le ||\mathbf{h}||$ , for $||\mathbf{h}||$ small, and so $$|R_2(\mathbf{x}_0, \mathbf{h})| \le ||\mathbf{h}||^3C$$ I have a number of points of confusion: The integrand is a continuous function of $t$ and is therefore bounded by a positive constant $C$ on a small neighbourhood of $\mathbf{x}_0$ (because it has to be close to its value at $\mathbf{x}_0$ ). Can someone please explain this? Also note that $|h_i| \le ||\mathbf{h}||$ , for $||\mathbf{h}||$ small, ... Why is this the case? And why only for $||\mathbf{h}||$ small? $$|R_2(\mathbf{x}_0, \mathbf{h})| \le ||\mathbf{h}||^3C$$ I'm trying to figure out where this came from. On a previous page, the textbook states the following: $$| R_k(x_0, h) | = \left| \int_{x_0}^{x_0 + h} \dfrac{(x_0 + h - \tau)^k}{k!} f^{k + 1}(\tau) \ d \tau \right| \le \dfrac{|h|^{k + 1}}{k!} M$$ But, if we have $||\mathbf{h}||^3C$ , then, according to the above statement, we would need $k = 2$ ; but the $2!$ factor is missing, so it seems that this assumption might be incorrect? So where did this come from? I would greatly it if people could please take the time to clarify these points of confusion.","My textbook has written the following for the second-order Taylor formula of Taylor's theorem for many variables: where The integrand is a continuous function of and is therefore bounded by a positive constant on a small neighbourhood of (because it has to be close to its value at ). Also note that , for small, and so I have a number of points of confusion: The integrand is a continuous function of and is therefore bounded by a positive constant on a small neighbourhood of (because it has to be close to its value at ). Can someone please explain this? Also note that , for small, ... Why is this the case? And why only for small? I'm trying to figure out where this came from. On a previous page, the textbook states the following: But, if we have , then, according to the above statement, we would need ; but the factor is missing, so it seems that this assumption might be incorrect? So where did this come from? I would greatly it if people could please take the time to clarify these points of confusion.","f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x_0}) + \sum_{i = 1}^n h_i \dfrac{\partial{f}}{\partial{x_i}}(\mathbf{x}_0) + \dfrac{1}{2} \sum_{i, j = 1}^n h_i h_j \dfrac{\partial^2 f}{\partial{x_i} \partial{x_j}} (\mathbf{x}_0) + R_2(\mathbf{x}_0, \mathbf{h}), R_2(\mathbf{x}_0, \mathbf{h}) = \sum_{i, j, k = 1}^n \int_0^1 \dfrac{(t - 1)^2}{2} \dfrac{\partial^3{f}}{\partial{x_i} \partial{x_j} \partial{x_k}} (\mathbf{x_0 + t \mathbf{h}})h_i h_j h_k \ dt. t C \mathbf{x}_0 \mathbf{x}_0 |h_i| \le ||\mathbf{h}|| ||\mathbf{h}|| |R_2(\mathbf{x}_0, \mathbf{h})| \le ||\mathbf{h}||^3C t C \mathbf{x}_0 \mathbf{x}_0 |h_i| \le ||\mathbf{h}|| ||\mathbf{h}|| ||\mathbf{h}|| |R_2(\mathbf{x}_0, \mathbf{h})| \le ||\mathbf{h}||^3C | R_k(x_0, h) | = \left| \int_{x_0}^{x_0 + h} \dfrac{(x_0 + h - \tau)^k}{k!} f^{k + 1}(\tau) \ d \tau \right| \le \dfrac{|h|^{k + 1}}{k!} M ||\mathbf{h}||^3C k = 2 2!","['real-analysis', 'integration', 'multivariable-calculus', 'taylor-expansion', 'vector-analysis']"
60,"differentiate $g(f(x),x)$ with respect to $f(x)$",differentiate  with respect to,"g(f(x),x) f(x)","Suppose I have a function $g(y,x)$ which is differentiable with respect to both arguments. I know that $y=f(x)$ is bijective, and thus the inverse function exists $x = f^{-1}(y)$ . My question is when I calculate $$\frac{d g(y,x)}{d y}$$ should I care about $x$ as well because $x$ implicitly changes when $y$ changes? I know that when I calculate $$\frac{d g(y,x)}{d x} $$ I have to take into account the fact that $y=f(x)$ also changes in $x$","Suppose I have a function which is differentiable with respect to both arguments. I know that is bijective, and thus the inverse function exists . My question is when I calculate should I care about as well because implicitly changes when changes? I know that when I calculate I have to take into account the fact that also changes in","g(y,x) y=f(x) x = f^{-1}(y) \frac{d g(y,x)}{d y} x x y \frac{d g(y,x)}{d x}  y=f(x) x","['multivariable-calculus', 'notation', 'partial-derivative']"
61,Higher total derivatives (Frechét derivatives),Higher total derivatives (Frechét derivatives),,"This issue I just cannnot resolve, so I'd highly appreciate your help.  Let $a_1, ... , a_n \in \mathbb{R}^k$ with $k$ a natural positive number. If we consider the function $$ W: \mathbb{R}^k \to \mathbb{R}, x \to \sum_{i=1}^n {(x-a_i)}\cdot {(x-a_i)}$$ ( $\cdot$ being the standard inner product) then we can write $ W = \sum_{i=1}^n N \circ T_{a_i}$ , where $$N(x) := x \cdot x, T_{a_i}(x) := x - a_i$$ giving $$D_x N(h) = 2 (x \cdot h), D_x T_{a_i}(h) = h$$ With the linearity of the total differential and the chain rule ( $D_p (f\circ g) = D_{g(p)}f \circ D_p g$ ) we obtain $$D_xW(h) = \sum_{i=1}^n (D_x(N\circ T_{a_i}))(h) = \sum_{i=1}^n (D_{T_{a_i}(x)} N \circ D_x T_{a_i}) (h) = \sum_{i=1}^n D_{T_{a_i}(x)} N(h) =  \sum_{i=1}^n 2 (x-a_i) \cdot h$$ Now you can supposedly take the second derivative: $$D(D_xW(h))(g) = \sum_{i=1}^n 2D_x [(T_{a_i} (g)) \cdot h] = 2 \sum_{i=1}^n g \cdot h$$ I don't understand  1.  how the first step (equality) above follows and  2.  why this simply doesn't equal zero, since for a function $f:\mathbb{R}^k \to \mathbb{R}$ evaluating the total derivative $D_pf$ just gives $D_pf(x) \in \mathbb{R}$ for $p, x \in \mathbb{R}^k$ , so that $D(D_xW(h))(g)= (D_xW(h))'(g) = 0$ . Or if there is a better alternative to calculating the second derivative, please share! (For context this came up showing that $W$ has a minimum in $x_0 := \frac 1 n \sum_{i=1}^n a_i$ - supposedly $W$ arises naturally in the context of the method of least-squares.) I hope everything is clear and again, I appreciate your efforts. Edit: I hope this warrants the ""functional analysis""/""Frechét derivative"" tag, as (if I understand correctly from the wikipedia article) they are concerned with infinite dimensional vector spaces, but in the finite dimensional case like the one at hand it just seems to be the derivative that I've learned.","This issue I just cannnot resolve, so I'd highly appreciate your help.  Let with a natural positive number. If we consider the function ( being the standard inner product) then we can write , where giving With the linearity of the total differential and the chain rule ( ) we obtain Now you can supposedly take the second derivative: I don't understand  1.  how the first step (equality) above follows and  2.  why this simply doesn't equal zero, since for a function evaluating the total derivative just gives for , so that . Or if there is a better alternative to calculating the second derivative, please share! (For context this came up showing that has a minimum in - supposedly arises naturally in the context of the method of least-squares.) I hope everything is clear and again, I appreciate your efforts. Edit: I hope this warrants the ""functional analysis""/""Frechét derivative"" tag, as (if I understand correctly from the wikipedia article) they are concerned with infinite dimensional vector spaces, but in the finite dimensional case like the one at hand it just seems to be the derivative that I've learned.","a_1, ... , a_n \in \mathbb{R}^k k  W: \mathbb{R}^k \to \mathbb{R}, x \to \sum_{i=1}^n {(x-a_i)}\cdot {(x-a_i)} \cdot  W = \sum_{i=1}^n N \circ T_{a_i} N(x) := x \cdot x, T_{a_i}(x) := x - a_i D_x N(h) = 2 (x \cdot h), D_x T_{a_i}(h) = h D_p (f\circ g) = D_{g(p)}f \circ D_p g D_xW(h) = \sum_{i=1}^n (D_x(N\circ T_{a_i}))(h) = \sum_{i=1}^n (D_{T_{a_i}(x)} N \circ D_x T_{a_i}) (h) = \sum_{i=1}^n D_{T_{a_i}(x)} N(h) =  \sum_{i=1}^n 2 (x-a_i) \cdot h D(D_xW(h))(g) = \sum_{i=1}^n 2D_x [(T_{a_i} (g)) \cdot h] = 2 \sum_{i=1}^n g \cdot h f:\mathbb{R}^k \to \mathbb{R} D_pf D_pf(x) \in \mathbb{R} p, x \in \mathbb{R}^k D(D_xW(h))(g)= (D_xW(h))'(g) = 0 W x_0 := \frac 1 n \sum_{i=1}^n a_i W","['functional-analysis', 'multivariable-calculus', 'multilinear-algebra', 'chain-rule', 'frechet-derivative']"
62,Do a pair of orthogonal directions with slopes equal to zero imply $\nabla f = 0$?,Do a pair of orthogonal directions with slopes equal to zero imply ?,\nabla f = 0,"The definition given on wikipedia for a saddle point is is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are both zero (a critical point), but which is not a local extremum of the function. Looking at their picture ( https://en.wikipedia.org/wiki/Saddle_point#/media/File:Saddle_point.svg ) it's clear that $f_x(0,0) = 0$ and $f_y(0,0)=0$ .... just from taking a look. What isn't as clear is that $D_u f(0,0) = 0$ and that $D_v f(0,0)=0$ whenever $u\perp v$ (for unit $u$ and $v$ ). However if I turn the graph so that $\langle 1,0\rangle \mapsto u$ and $\langle 0,1\rangle \mapsto v$ then I would have $D_u f(0,0) = 0$ and that $D_v f(0,0)=0$ but then I don't see why $f_x(0,0)=0$ and $f_y(0,0)=0$ in that coordinate system. Put another way.....  Is it true that $\nabla f(x_0,y_0)=0$ if and only if $D_u f(x_0,y_0)= D_v f(x_0,y_0)=0$ for some (and hence any) pair of unit vectors $u,v$ such that $u\perp v$ ? If this is true then in the boxed sentence above I could write instead ""a point where $\nabla f=0$ but which ...""","The definition given on wikipedia for a saddle point is is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are both zero (a critical point), but which is not a local extremum of the function. Looking at their picture ( https://en.wikipedia.org/wiki/Saddle_point#/media/File:Saddle_point.svg ) it's clear that and .... just from taking a look. What isn't as clear is that and that whenever (for unit and ). However if I turn the graph so that and then I would have and that but then I don't see why and in that coordinate system. Put another way.....  Is it true that if and only if for some (and hence any) pair of unit vectors such that ? If this is true then in the boxed sentence above I could write instead ""a point where but which ...""","f_x(0,0) = 0 f_y(0,0)=0 D_u f(0,0) = 0 D_v f(0,0)=0 u\perp v u v \langle 1,0\rangle \mapsto u \langle 0,1\rangle \mapsto v D_u f(0,0) = 0 D_v f(0,0)=0 f_x(0,0)=0 f_y(0,0)=0 \nabla f(x_0,y_0)=0 D_u f(x_0,y_0)= D_v f(x_0,y_0)=0 u,v u\perp v \nabla f=0","['analysis', 'multivariable-calculus']"
63,"How to solve $v f^{'}_v + f = 0$, $f(0, y) = y^2$","How to solve ,","v f^{'}_v + f = 0 f(0, y) = y^2","So I was presented with this equation in my textbook, currently studying multi variable calculus: $$yf^{'}_x(x,y) - xf^{'}_y(x,y) = f(x,y)$$ Using the substitution: $$x^2 + y^2 = u$$ $$e^{{-x^2}{/2}} = v$$ I get the equation (which is correct): $$v f^{'}_v + f = 0$$ My next task is to find the solution, $f(x,y)$ that satisfies $f(0, y) = y^2$ . The correct answer should be $f(x,y) = (x^2 + y^2)e^{{-x^2}{/2}}$ . But here I am stuck, I have solved similar tasks but I don't know how to handle the $f$ . For example $v f^{'}_v = x$ or similar would be easy to solve.","So I was presented with this equation in my textbook, currently studying multi variable calculus: Using the substitution: I get the equation (which is correct): My next task is to find the solution, that satisfies . The correct answer should be . But here I am stuck, I have solved similar tasks but I don't know how to handle the . For example or similar would be easy to solve.","yf^{'}_x(x,y) - xf^{'}_y(x,y) = f(x,y) x^2 + y^2 = u e^{{-x^2}{/2}} = v v f^{'}_v + f = 0 f(x,y) f(0, y) = y^2 f(x,y) = (x^2 + y^2)e^{{-x^2}{/2}} f v f^{'}_v = x",['multivariable-calculus']
64,"For a positive function, can the length of gradient get arbitrarily close to 0?","For a positive function, can the length of gradient get arbitrarily close to 0?",,"Consider a differentiable function $f:R^d\rightarrow R_+$ . Let $a=\inf\limits_{x\in R^d} f(x)$ . It's evident that $a\ge0$ . Now if there exists $x\in R^d$ such that $f(x)=a$ , then $f'(x)=0$ . If there doesn't exist $x\in R^d$ such that $f(x)=a$ , can we prove that $\forall \epsilon>0, \exists x\in R^d$ such that $|f'(x)|<\epsilon$ ? Is there any relevant theorem?","Consider a differentiable function . Let . It's evident that . Now if there exists such that , then . If there doesn't exist such that , can we prove that such that ? Is there any relevant theorem?","f:R^d\rightarrow R_+ a=\inf\limits_{x\in R^d} f(x) a\ge0 x\in R^d f(x)=a f'(x)=0 x\in R^d f(x)=a \forall \epsilon>0, \exists x\in R^d |f'(x)|<\epsilon","['real-analysis', 'multivariable-calculus', 'supremum-and-infimum']"
65,What is the value of $\Delta(|u|^q)$?,What is the value of ?,\Delta(|u|^q),"Suppose $u$ is a vector in $\mathbb R^n$ and for $q>2$ , I know $\nabla (|u|^q)=q|u|^{q-2}u\nabla u$ , then what is $\Delta(|u|^q)$ ? Update: What I got is $|u|^{q-2}(\nabla \cdot u \nabla u + u \Delta u)$ .","Suppose is a vector in and for , I know , then what is ? Update: What I got is .",u \mathbb R^n q>2 \nabla (|u|^q)=q|u|^{q-2}u\nabla u \Delta(|u|^q) |u|^{q-2}(\nabla \cdot u \nabla u + u \Delta u),"['real-analysis', 'analysis', 'multivariable-calculus']"
66,$f$ is differentiable and satisfies $f(t\vec{x})=t^pf(\vec{x})$ prove $\vec{x} \cdot \nabla f(\vec{x}) = pf(\vec{x})$,is differentiable and satisfies  prove,f f(t\vec{x})=t^pf(\vec{x}) \vec{x} \cdot \nabla f(\vec{x}) = pf(\vec{x}),"let the three-variable function $f$ be differentiable and satisfy $f(t\vec{x})=t^pf(\vec{x})$ for all $,\vec{x} \in \Bbb{R^3}, t \in \Bbb{R}$ and where $p$ is a constant. How would you prove that $\vec{x} \cdot \nabla f(\vec{x}) = pf(\vec{x})$ I am very confused for this question and have no idea on how to use the information given to reach a conclusion.",let the three-variable function be differentiable and satisfy for all and where is a constant. How would you prove that I am very confused for this question and have no idea on how to use the information given to reach a conclusion.,"f f(t\vec{x})=t^pf(\vec{x}) ,\vec{x} \in \Bbb{R^3}, t \in \Bbb{R} p \vec{x} \cdot \nabla f(\vec{x}) = pf(\vec{x})","['calculus', 'multivariable-calculus', 'chain-rule']"
67,"Find the line through $P = (2,4,7) $ that intersects and is perpendicular to the line : $x = -1 + t, y = -2 + 3t, z = 4 $",Find the line through  that intersects and is perpendicular to the line :,"P = (2,4,7)  x = -1 + t, y = -2 + 3t, z = 4 ","Find the line through $P = (2,4,7) $ that intersects and is   perpendicular to the line : $x = -1 + t, y = -2 + 3t, z = 4 $ I set up a system of equations and obtained : $-1 + t = x_0 +2 \\ -2 + 3t = y_0 + 4 \\ 4 = z_0 + 7$ Solving for each and putting them I obtain the vector $ v_1 = <t -3, 3t-6, -3>$ I then set the direction vector as $ v_2 =<1,3,4>$ Since $v_1 \cdot v_2 = 0 $ means they are orthogonal I obtain : $<t -3, 3t-6, -3> \cdot <1,3,4> = 0  $ Multiplying out and solving for t I get $t = 10/21$ Would I now just substitute my given value for t back into $t_1$ ?",Find the line through that intersects and is   perpendicular to the line : I set up a system of equations and obtained : Solving for each and putting them I obtain the vector I then set the direction vector as Since means they are orthogonal I obtain : Multiplying out and solving for t I get Would I now just substitute my given value for t back into ?,"P = (2,4,7)  x = -1 + t, y = -2 + 3t, z = 4  -1 + t = x_0 +2 \\ -2 + 3t = y_0 + 4 \\ 4 = z_0 + 7  v_1 = <t -3, 3t-6, -3>  v_2 =<1,3,4> v_1 \cdot v_2 = 0  <t -3, 3t-6, -3> \cdot <1,3,4> = 0   t = 10/21 t_1","['geometry', 'multivariable-calculus', 'vectors']"
68,Volume of a section of a sphere,Volume of a section of a sphere,,"A section of a sphere is described by $0 ≤ 𝑟 ≤ 2$, $0 ≤ 𝜃 ≤ 90°$, $30° ≤ 𝜑 ≤ 90°$. Now, just by using simple symmetry, I can see that the volume of this section will be less than 1/8th of the volume of a sphere with radius 2. Volume of the section < (Volume of the sphere of radius $2$) $/$ $8$ Volume of a sphere of radius $2$ is $33.51$ according to $4/3 \pi r^3$. I solved it using differential method but my answer is more than $1/8$th of the volume. What is wrong here?","A section of a sphere is described by $0 ≤ 𝑟 ≤ 2$, $0 ≤ 𝜃 ≤ 90°$, $30° ≤ 𝜑 ≤ 90°$. Now, just by using simple symmetry, I can see that the volume of this section will be less than 1/8th of the volume of a sphere with radius 2. Volume of the section < (Volume of the sphere of radius $2$) $/$ $8$ Volume of a sphere of radius $2$ is $33.51$ according to $4/3 \pi r^3$. I solved it using differential method but my answer is more than $1/8$th of the volume. What is wrong here?",,"['multivariable-calculus', 'volume', 'spherical-coordinates']"
69,Prove that $t\mapsto \frac{d}{dt}\left[\rho(t)\right]^{-1}=-\rho(t)^{-1}\rho'(t)\rho(t)^{-1}$,Prove that,t\mapsto \frac{d}{dt}\left[\rho(t)\right]^{-1}=-\rho(t)^{-1}\rho'(t)\rho(t)^{-1},"Let $\rho:(a,b)\to \operatorname{ISO}\left(\Bbb{R}^n\right)$ be differentiable. I want to prove that  \begin{align}\frac{d}{dt}\left[\rho(t)\right]^{-1}=-\rho(t)^{-1}\rho'(t)\rho(t)^{-1}\end{align}  $\operatorname{ISO}\left(\Bbb{R}^n\right)$ is a space of continuous linear maps from $\Bbb{R}^n$ to $\Bbb{R}^n$. I have proven before that \begin{align}f:\operatorname{ISO}\left(\Bbb{R}^n\right)\to \operatorname{ISO}\left(\Bbb{R}^n\right)\end{align}  \begin{align}u\mapsto u^{-1}\end{align}is differentiable and \begin{align}f'(u)(h)=-u^{-1}h u^{-1}\end{align}   Along the line, I used Neumann's Lemma but I got stuck when proving this. Here is my work: I tried by using definition: Let $h\in(a,b),$ then \begin{align}\rho(t+h)-\rho(t)&=\rho(t+h)^{-1}-\rho(t)^{-1}\\&=\left[\rho(t+h)-\rho(t)+\rho(t)\right]^{-1}-\rho(t)^{-1}\\&=\left[\left[\left(\rho(t+h)-\rho(t)\right)\left(\rho(t)\right)+I\right]^{-1}-I\right]\rho(t)^{-1}\end{align} I'm thinking of applying Neumann's Lemma but I can't move on. I want someone to help see my fault. Otherwise, alternative methods will be highly regarded. Thanks!","Let $\rho:(a,b)\to \operatorname{ISO}\left(\Bbb{R}^n\right)$ be differentiable. I want to prove that  \begin{align}\frac{d}{dt}\left[\rho(t)\right]^{-1}=-\rho(t)^{-1}\rho'(t)\rho(t)^{-1}\end{align}  $\operatorname{ISO}\left(\Bbb{R}^n\right)$ is a space of continuous linear maps from $\Bbb{R}^n$ to $\Bbb{R}^n$. I have proven before that \begin{align}f:\operatorname{ISO}\left(\Bbb{R}^n\right)\to \operatorname{ISO}\left(\Bbb{R}^n\right)\end{align}  \begin{align}u\mapsto u^{-1}\end{align}is differentiable and \begin{align}f'(u)(h)=-u^{-1}h u^{-1}\end{align}   Along the line, I used Neumann's Lemma but I got stuck when proving this. Here is my work: I tried by using definition: Let $h\in(a,b),$ then \begin{align}\rho(t+h)-\rho(t)&=\rho(t+h)^{-1}-\rho(t)^{-1}\\&=\left[\rho(t+h)-\rho(t)+\rho(t)\right]^{-1}-\rho(t)^{-1}\\&=\left[\left[\left(\rho(t+h)-\rho(t)\right)\left(\rho(t)\right)+I\right]^{-1}-I\right]\rho(t)^{-1}\end{align} I'm thinking of applying Neumann's Lemma but I can't move on. I want someone to help see my fault. Otherwise, alternative methods will be highly regarded. Thanks!",,"['real-analysis', 'analysis', 'multivariable-calculus', 'convergence-divergence', 'uniform-convergence']"
70,"Partial derivatives of $f$ exist, but only $n-1$ of them are continuous, implies differentiability","Partial derivatives of  exist, but only  of them are continuous, implies differentiability",f n-1,"The problem I am working on is stated as follows. Let $A \subset R^2$ be open and $(a_1,a_2)\in A$. Suppose $f:A\to R$ satisfies that $\frac{\partial f}{\partial x_1}(a_1, a_2)$ exists,   and that $\frac{\partial f}{\partial x_2}$ is continuous in $A$. Prove that $f$ is differentiable at $(a_1, a_2)$. My professor gave the title as a remark to the proof that if all partials exist and are continuous in $A$ then $f$ is differentiable. I am having trouble proving it even for this specific case that I am working with. I used an argument involving the mean value theorem to show that  $|f(y)-f(x)-\sum_{i=1}^2\frac{\partial f}{\partial x_i}(x_1,x_2)*(y_i-x_i)|$ $\leq ||y-x||*[|\frac{\partial f}{\partial x_1}(u_1,y_2)-\frac{\partial f}{\partial x_1}(x_1,x_2)|+|\frac{\partial f}{\partial x_2}(x_1,u_2)-\frac{\partial f}{\partial x_2}(x_1,x_2)|]$ ,where $u_i$ is between $x_i$ and $y_i$. From here I can use the continuity of $\frac{\partial f}{\partial x_2}$ to show that the second term in the inequality is bounded by $\frac{\epsilon}{2}$ but I am unsure of how I can get a nice upper bound for the first term. I realize I skipped a lot of intermediate steps but I am not sure if this is even the correct way to proceed with this proof. If someone could help me with a hint or maybe some intuition on why this statement should be true I would really appreciate it.","The problem I am working on is stated as follows. Let $A \subset R^2$ be open and $(a_1,a_2)\in A$. Suppose $f:A\to R$ satisfies that $\frac{\partial f}{\partial x_1}(a_1, a_2)$ exists,   and that $\frac{\partial f}{\partial x_2}$ is continuous in $A$. Prove that $f$ is differentiable at $(a_1, a_2)$. My professor gave the title as a remark to the proof that if all partials exist and are continuous in $A$ then $f$ is differentiable. I am having trouble proving it even for this specific case that I am working with. I used an argument involving the mean value theorem to show that  $|f(y)-f(x)-\sum_{i=1}^2\frac{\partial f}{\partial x_i}(x_1,x_2)*(y_i-x_i)|$ $\leq ||y-x||*[|\frac{\partial f}{\partial x_1}(u_1,y_2)-\frac{\partial f}{\partial x_1}(x_1,x_2)|+|\frac{\partial f}{\partial x_2}(x_1,u_2)-\frac{\partial f}{\partial x_2}(x_1,x_2)|]$ ,where $u_i$ is between $x_i$ and $y_i$. From here I can use the continuity of $\frac{\partial f}{\partial x_2}$ to show that the second term in the inequality is bounded by $\frac{\epsilon}{2}$ but I am unsure of how I can get a nice upper bound for the first term. I realize I skipped a lot of intermediate steps but I am not sure if this is even the correct way to proceed with this proof. If someone could help me with a hint or maybe some intuition on why this statement should be true I would really appreciate it.",,"['real-analysis', 'multivariable-calculus']"
71,Any idea on how to find a upper bound for a limit in R^2?,Any idea on how to find a upper bound for a limit in R^2?,,"I'm working on this problem of continuity in $\mathbb{R}^2$ the statement is the following:  Prove by definition that  $$ \lim_{ (x,y) \to (0,0) } \frac{y^2}{3+\sin^2(x^2+y^2)+y^4}=0 $$ Given $\epsilon >0$, we have that  $$| \frac{y^2}{3+\sin^2(x^2+y^2)+y^4} | =  y^2 \frac{1}{3+\sin^2(x^2+y^2)+y^4}$$ If we know that  $ \frac{1}{3+\sin^2(x^2+y^2)+y^4} \leq  \frac{1}{\sin^2(x^2+y^2)} $ then we end with  $$ y^2 \frac{1}{3+\sin^2(x^2+y^2)+y^4} \leq y^2  \frac{1}{\sin^2(x^2+y^2)}$$ and even more,  $$y^2  \frac{1}{\sin^2(x^2+y^2)} \leq  \frac{x^2+y^2}{\sin^2(x^2+y^2)} $$ I don't see any way to get rid out the $\frac{1}{\sin^2(x^2+y^2)}$ to conclude the $\delta$ since I can't say that $\frac{1}{\sin^2(x^2+y^2)} \leq 1$ because is the contrary, in fact  $\frac{1}{\sin^2(x^2+y^2)} \geq 1$. Any ideas? Thanks in advance.","I'm working on this problem of continuity in $\mathbb{R}^2$ the statement is the following:  Prove by definition that  $$ \lim_{ (x,y) \to (0,0) } \frac{y^2}{3+\sin^2(x^2+y^2)+y^4}=0 $$ Given $\epsilon >0$, we have that  $$| \frac{y^2}{3+\sin^2(x^2+y^2)+y^4} | =  y^2 \frac{1}{3+\sin^2(x^2+y^2)+y^4}$$ If we know that  $ \frac{1}{3+\sin^2(x^2+y^2)+y^4} \leq  \frac{1}{\sin^2(x^2+y^2)} $ then we end with  $$ y^2 \frac{1}{3+\sin^2(x^2+y^2)+y^4} \leq y^2  \frac{1}{\sin^2(x^2+y^2)}$$ and even more,  $$y^2  \frac{1}{\sin^2(x^2+y^2)} \leq  \frac{x^2+y^2}{\sin^2(x^2+y^2)} $$ I don't see any way to get rid out the $\frac{1}{\sin^2(x^2+y^2)}$ to conclude the $\delta$ since I can't say that $\frac{1}{\sin^2(x^2+y^2)} \leq 1$ because is the contrary, in fact  $\frac{1}{\sin^2(x^2+y^2)} \geq 1$. Any ideas? Thanks in advance.",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus']"
72,"Prove that $\ell(\Bbb{R}^n,\ell(\Bbb{R}^m,\Bbb{R}^q))=\ell_b(\Bbb{R}^n\times\Bbb{R}^m,\Bbb{R}^q)$",Prove that,"\ell(\Bbb{R}^n,\ell(\Bbb{R}^m,\Bbb{R}^q))=\ell_b(\Bbb{R}^n\times\Bbb{R}^m,\Bbb{R}^q)","Prove that \begin{align}\ell(\Bbb{R}^n,\ell(\Bbb{R}^m,\Bbb{R}^q))=\ell_b(\Bbb{R}^n\times\Bbb{R}^m,\Bbb{R}^q)\end{align} where $\ell(\Bbb{R}^n,\ell(\Bbb{R}^m,\Bbb{R}^q))$ represents the space of continuous linear maps from $\Bbb{R}^n$ to $\ell(\Bbb{R}^m,\Bbb{R}^q)$ and $\ell_b(\Bbb{R}^n\times\Bbb{R}^m,\Bbb{R}^q),$ the space of bilinear maps from $\Bbb{R}^n\times\Bbb{R}^m$ to $\Bbb{R}^q.$ Also $\dim \Bbb{R}^n,\dim \Bbb{R}^m,\dim \Bbb{R}^q<\infty.$ How do I go about this? Any help?","Prove that \begin{align}\ell(\Bbb{R}^n,\ell(\Bbb{R}^m,\Bbb{R}^q))=\ell_b(\Bbb{R}^n\times\Bbb{R}^m,\Bbb{R}^q)\end{align} where $\ell(\Bbb{R}^n,\ell(\Bbb{R}^m,\Bbb{R}^q))$ represents the space of continuous linear maps from $\Bbb{R}^n$ to $\ell(\Bbb{R}^m,\Bbb{R}^q)$ and $\ell_b(\Bbb{R}^n\times\Bbb{R}^m,\Bbb{R}^q),$ the space of bilinear maps from $\Bbb{R}^n\times\Bbb{R}^m$ to $\Bbb{R}^q.$ Also $\dim \Bbb{R}^n,\dim \Bbb{R}^m,\dim \Bbb{R}^q<\infty.$ How do I go about this? Any help?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
73,"The functions $f$ and $g$ are not continuous and have directional derivative at $(0,0)$",The functions  and  are not continuous and have directional derivative at,"f g (0,0)","Let $$f:\Bbb{R}^2\to\Bbb{R}$$ $$(x,y)\mapsto f(x,y)=\begin{cases}y^2\log|x|& ,x\neq 0,\\0 &,x=0.\end{cases}$$ and  $$g:\Bbb{R}^2\to\Bbb{R}$$ $$(x,y)\mapsto f(x,y)=\begin{cases}\frac{x^2 y}{x^2+y^2}& ,(x,y)\neq (0,0),\\\;0 &,x=y=0.\end{cases}$$ I want to show that $f$ and $g$ are not continuous at $(0,0).$ $f$ and $g$ have directional derivatives at $(0,0).$ PART A: To show that $f$ and $g$ are not continuous, it suffices to use line, i.e. $y=\alpha x,\;\alpha\in \Bbb{R}$. So, I have $$f(x,y)=f(x,\alpha x)=\alpha^2 x^2\log|x|$$ which goes to $0$ as $x\to 0$ Also, $$g(x,y)=g(x,\alpha x)=\frac{\alpha x^3 }{x^2+\alpha^2 x^2}=\frac{\alpha x }{1+\alpha^2 \alpha^2}$$ which also goes to $0$ as $x\to 0.$ So, why I'm I not arriving at the desired result? Could my computation be wrong? PART B: Let $(x_0,y_0)\in \Bbb{R}^2.$ Then, for $x_0\neq 0,$ $$f((x_0,y_0)+(h_1,h_2))-f(x_0,y_0)$$ $$f(x_0+h_1,y_0+h_2)-f(x_0,y_0)$$ $$=(y_0+h_2)^2\log|x_0+h_1|-y^2_0 \log|x_0|.$$ I got stuck here. So, could anyone please, help-out? I would be glad for any help rendered.","Let $$f:\Bbb{R}^2\to\Bbb{R}$$ $$(x,y)\mapsto f(x,y)=\begin{cases}y^2\log|x|& ,x\neq 0,\\0 &,x=0.\end{cases}$$ and  $$g:\Bbb{R}^2\to\Bbb{R}$$ $$(x,y)\mapsto f(x,y)=\begin{cases}\frac{x^2 y}{x^2+y^2}& ,(x,y)\neq (0,0),\\\;0 &,x=y=0.\end{cases}$$ I want to show that $f$ and $g$ are not continuous at $(0,0).$ $f$ and $g$ have directional derivatives at $(0,0).$ PART A: To show that $f$ and $g$ are not continuous, it suffices to use line, i.e. $y=\alpha x,\;\alpha\in \Bbb{R}$. So, I have $$f(x,y)=f(x,\alpha x)=\alpha^2 x^2\log|x|$$ which goes to $0$ as $x\to 0$ Also, $$g(x,y)=g(x,\alpha x)=\frac{\alpha x^3 }{x^2+\alpha^2 x^2}=\frac{\alpha x }{1+\alpha^2 \alpha^2}$$ which also goes to $0$ as $x\to 0.$ So, why I'm I not arriving at the desired result? Could my computation be wrong? PART B: Let $(x_0,y_0)\in \Bbb{R}^2.$ Then, for $x_0\neq 0,$ $$f((x_0,y_0)+(h_1,h_2))-f(x_0,y_0)$$ $$f(x_0+h_1,y_0+h_2)-f(x_0,y_0)$$ $$=(y_0+h_2)^2\log|x_0+h_1|-y^2_0 \log|x_0|.$$ I got stuck here. So, could anyone please, help-out? I would be glad for any help rendered.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'continuity']"
74,"If $\Omega\subset\mathbb{R}^n$ is convex and $f$ is differentiable, then $f(x)-f(y)\geq f'(y)(x-y),\;\forall \;x,y\in \Omega$","If  is convex and  is differentiable, then","\Omega\subset\mathbb{R}^n f f(x)-f(y)\geq f'(y)(x-y),\;\forall \;x,y\in \Omega","Let $\Omega$ be a convex set in $\Bbb{R}^n$. We say that that $f:\Omega\to \Bbb{R}$ is convex if $$f(tx+(1-t)y)\leq tf(x)+(1-t)f(y),\;\forall\;0\leq t\leq 1,\;\&\;\forall\;x,y\in \Omega.$$ I want to show that if $f$ is convex and  differentiable on $\Omega,$ then $$f(x)-f(y)\geq f'(y)(x-y),\;\forall \;x,y\in \Omega.$$ I'm thinking of using Partial derivatives but don't know how to go about it. Please, can anyone help out?","Let $\Omega$ be a convex set in $\Bbb{R}^n$. We say that that $f:\Omega\to \Bbb{R}$ is convex if $$f(tx+(1-t)y)\leq tf(x)+(1-t)f(y),\;\forall\;0\leq t\leq 1,\;\&\;\forall\;x,y\in \Omega.$$ I want to show that if $f$ is convex and  differentiable on $\Omega,$ then $$f(x)-f(y)\geq f'(y)(x-y),\;\forall \;x,y\in \Omega.$$ I'm thinking of using Partial derivatives but don't know how to go about it. Please, can anyone help out?",,"['calculus', 'multivariable-calculus', 'derivatives', 'convex-analysis', 'convex-optimization']"
75,Finding center of a solid,Finding center of a solid,,"I am finding the center of mass of sphere $x^2+y^2+z^2=2z$ when $\delta(x,y,z)=\sqrt{x^2+y^2+z^2}$. I did the mass as follows (I hope it is right): $$M=\int_0^{2\pi}d\theta\int_0^{\pi/2}\int_0^{2\cos{\phi}}\rho^3\sin{\phi}\,d\rho \,d\phi=8\pi/5.$$ Now I did the $x_0$ of the center: $$x_0=\frac{1}{M} \int_0^{2\pi} \cos(\theta) \, d\theta \int_0^{\pi/2} \int_0^{2\cos\phi} \rho^4\sin^2 \phi \, d\rho \, d\phi=0.$$ Here, I suspect the rest for $y_0$ and $z_0$ is obvious. Indeed, I think that I would get $y_0=0, z_0=1$ after evaluating the corresponding integrals because the center of the sphere is $(0,0,1)$. What do you think?","I am finding the center of mass of sphere $x^2+y^2+z^2=2z$ when $\delta(x,y,z)=\sqrt{x^2+y^2+z^2}$. I did the mass as follows (I hope it is right): $$M=\int_0^{2\pi}d\theta\int_0^{\pi/2}\int_0^{2\cos{\phi}}\rho^3\sin{\phi}\,d\rho \,d\phi=8\pi/5.$$ Now I did the $x_0$ of the center: $$x_0=\frac{1}{M} \int_0^{2\pi} \cos(\theta) \, d\theta \int_0^{\pi/2} \int_0^{2\cos\phi} \rho^4\sin^2 \phi \, d\rho \, d\phi=0.$$ Here, I suspect the rest for $y_0$ and $z_0$ is obvious. Indeed, I think that I would get $y_0=0, z_0=1$ after evaluating the corresponding integrals because the center of the sphere is $(0,0,1)$. What do you think?",,['integration']
76,Why do we compute normal vector for computing projection?,Why do we compute normal vector for computing projection?,,"A First Course in Complex Analysis by Matthias Beck, Gerald Marchesi, Dennis Pixton, and Lucas Sabalka Exer 3.27 (Exer 3.27) Consider the plane $H$ determined by the equation $x + y -z = 0$. What is a unit normal vector to $H$? Compute the image of $X:=H\cap \mathbb S^{2}$ under the stereographic projection $\Phi$. - For 2,  I computed $X$ to be a 3D circle , parametrised here , and its image to be $Y:= \Phi(X) = \{|z-(1+i)|^2 = 3\}$, but now I ask: - For 1, What's the relevance of asking about the unit normal vector? I computed the unit normal vectors to be $[1,1,-1]\frac{\pm 1}{\sqrt{3}}$. I observe their terminal points to be on the unit sphere. Here are the parametrisations: $Y:= \Phi(X) = \{|z-(1+i)|^2 = 3\}$ is parametrised: $$\begin{bmatrix} y_1(t)\\  y_2(t)\\  y_3(t) \end{bmatrix} = \begin{bmatrix} \sqrt{3}\cos(t) + 1\\  \sqrt{3}\sin(t) + 1\\  0 \end{bmatrix} = \begin{bmatrix} 1\\  1\\  0 \end{bmatrix} + \begin{bmatrix} 1\\  0\\  0 \end{bmatrix}\sqrt{3}\cos(t)+   \begin{bmatrix} 0\\  1\\  0 \end{bmatrix}\sqrt{3}\sin(t)$$ $X$ is parametrised: $$\begin{bmatrix} x_1(t)\\  x_2(t)\\  x_3(t) \end{bmatrix} = \begin{bmatrix} \sqrt{\frac 2 3} \cos[t]\\  -\sqrt{\frac 2 4} \sin[t] - \sqrt{\frac 2 {12}} \cos[t]\\  -\sqrt{\frac 2 4} \sin[t] + \sqrt{\frac 2 {12}} \cos[t] \end{bmatrix} = \begin{bmatrix} \sqrt{\frac 1 3}\\  -\sqrt{\frac 1 {12}}\\  \sqrt{\frac 1 {12}} \end{bmatrix}\sqrt{2}\cos(t)+   \begin{bmatrix} 0\\  -\sqrt{\frac 1 {4}}\\  -\sqrt{\frac 1 {4}} \end{bmatrix}\sqrt{2}\sin(t)$$ $$H = \{x + y -z = 0\} = \{[1,1,-1] \cdot [x,y,z]=0\} = \{[1,1,-1]\frac{1}{\sqrt{3}} \cdot [x,y,z]=0\}$$ is parametrised: $$\begin{bmatrix} h_1(r,s)\\  h_2(r,s)\\  h_3(r,s) \end{bmatrix}=\begin{bmatrix} 1\\  0\\  1 \end{bmatrix}r +   \begin{bmatrix} 0\\  1\\  1 \end{bmatrix}s$$","A First Course in Complex Analysis by Matthias Beck, Gerald Marchesi, Dennis Pixton, and Lucas Sabalka Exer 3.27 (Exer 3.27) Consider the plane $H$ determined by the equation $x + y -z = 0$. What is a unit normal vector to $H$? Compute the image of $X:=H\cap \mathbb S^{2}$ under the stereographic projection $\Phi$. - For 2,  I computed $X$ to be a 3D circle , parametrised here , and its image to be $Y:= \Phi(X) = \{|z-(1+i)|^2 = 3\}$, but now I ask: - For 1, What's the relevance of asking about the unit normal vector? I computed the unit normal vectors to be $[1,1,-1]\frac{\pm 1}{\sqrt{3}}$. I observe their terminal points to be on the unit sphere. Here are the parametrisations: $Y:= \Phi(X) = \{|z-(1+i)|^2 = 3\}$ is parametrised: $$\begin{bmatrix} y_1(t)\\  y_2(t)\\  y_3(t) \end{bmatrix} = \begin{bmatrix} \sqrt{3}\cos(t) + 1\\  \sqrt{3}\sin(t) + 1\\  0 \end{bmatrix} = \begin{bmatrix} 1\\  1\\  0 \end{bmatrix} + \begin{bmatrix} 1\\  0\\  0 \end{bmatrix}\sqrt{3}\cos(t)+   \begin{bmatrix} 0\\  1\\  0 \end{bmatrix}\sqrt{3}\sin(t)$$ $X$ is parametrised: $$\begin{bmatrix} x_1(t)\\  x_2(t)\\  x_3(t) \end{bmatrix} = \begin{bmatrix} \sqrt{\frac 2 3} \cos[t]\\  -\sqrt{\frac 2 4} \sin[t] - \sqrt{\frac 2 {12}} \cos[t]\\  -\sqrt{\frac 2 4} \sin[t] + \sqrt{\frac 2 {12}} \cos[t] \end{bmatrix} = \begin{bmatrix} \sqrt{\frac 1 3}\\  -\sqrt{\frac 1 {12}}\\  \sqrt{\frac 1 {12}} \end{bmatrix}\sqrt{2}\cos(t)+   \begin{bmatrix} 0\\  -\sqrt{\frac 1 {4}}\\  -\sqrt{\frac 1 {4}} \end{bmatrix}\sqrt{2}\sin(t)$$ $$H = \{x + y -z = 0\} = \{[1,1,-1] \cdot [x,y,z]=0\} = \{[1,1,-1]\frac{1}{\sqrt{3}} \cdot [x,y,z]=0\}$$ is parametrised: $$\begin{bmatrix} h_1(r,s)\\  h_2(r,s)\\  h_3(r,s) \end{bmatrix}=\begin{bmatrix} 1\\  0\\  1 \end{bmatrix}r +   \begin{bmatrix} 0\\  1\\  1 \end{bmatrix}s$$",,"['linear-algebra', 'complex-analysis', 'geometry', 'analysis', 'multivariable-calculus']"
77,"Prove that $X\cdot (\nabla \times X)=0$, where $X \perp T_x$","Prove that , where",X\cdot (\nabla \times X)=0 X \perp T_x,"Consider a $C^1$ map $f:R^2\to R^3$ such that $df_x:R^2\to R^3$ has rank $2$ everywhere. Define the tangent plane $T_x$ as $df_x(R^2)\subset R^3$. Suppose a vector field $X$ in $R^3$ is orthogonal to $T_x$ for all $x$, i.e., $X(f(x))\cdot Y=0$ for all $x\in R^2$ and $Y\in T_x$. Show that $X\cdot (\nabla \times X)=0$ at all points $f(x)$. My thoughts: any vector field on $R^3$ can be written as $a(p,q,r)\partial_p+b(p,q,r)\partial q+c(p,q,r)\partial _r$ where $p,q,r$ are coordinates in $R^3$. So $$\nabla\times X=(\partial_qc\partial_r-\partial_rb\partial_q,\partial_ra\partial_p-\partial_pc\partial_r,\partial_pb\partial_q-\partial_qa\partial_p)$$ and $$X\cdot (\nabla \times X)=a\partial_p\partial_qc\partial_r-a\partial_p\partial_rb\partial_q+b\partial_q\partial_ra\partial_p-b\partial_q\partial_pc\partial_r+c\partial_r\partial_pb\partial_q-c\partial_r\partial_qa\partial _p$$ I need to show that the above expression applied to $f(x)$ is zero for all $x\in R^2$. Further, $$X(f(x))=(a\partial_pf(x),b\partial_qf(x),c\partial_rf(x))\\Y=(\partial_xf_1u+\partial_yf_1 v,\partial_xf_2u+\partial_yf_2 v,\partial_xf_3u+\partial_yf_3 v)$$ where $f=(f_1,f_2,f_3), u,v,\in R$. And we are given that $X(f(x))\cdot Y=0$. Even if I expand this, I don't know what to do next...","Consider a $C^1$ map $f:R^2\to R^3$ such that $df_x:R^2\to R^3$ has rank $2$ everywhere. Define the tangent plane $T_x$ as $df_x(R^2)\subset R^3$. Suppose a vector field $X$ in $R^3$ is orthogonal to $T_x$ for all $x$, i.e., $X(f(x))\cdot Y=0$ for all $x\in R^2$ and $Y\in T_x$. Show that $X\cdot (\nabla \times X)=0$ at all points $f(x)$. My thoughts: any vector field on $R^3$ can be written as $a(p,q,r)\partial_p+b(p,q,r)\partial q+c(p,q,r)\partial _r$ where $p,q,r$ are coordinates in $R^3$. So $$\nabla\times X=(\partial_qc\partial_r-\partial_rb\partial_q,\partial_ra\partial_p-\partial_pc\partial_r,\partial_pb\partial_q-\partial_qa\partial_p)$$ and $$X\cdot (\nabla \times X)=a\partial_p\partial_qc\partial_r-a\partial_p\partial_rb\partial_q+b\partial_q\partial_ra\partial_p-b\partial_q\partial_pc\partial_r+c\partial_r\partial_pb\partial_q-c\partial_r\partial_qa\partial _p$$ I need to show that the above expression applied to $f(x)$ is zero for all $x\in R^2$. Further, $$X(f(x))=(a\partial_pf(x),b\partial_qf(x),c\partial_rf(x))\\Y=(\partial_xf_1u+\partial_yf_1 v,\partial_xf_2u+\partial_yf_2 v,\partial_xf_3u+\partial_yf_3 v)$$ where $f=(f_1,f_2,f_3), u,v,\in R$. And we are given that $X(f(x))\cdot Y=0$. Even if I expand this, I don't know what to do next...",,"['calculus', 'real-analysis', 'multivariable-calculus', 'differential-geometry', 'differential-topology']"
78,"Calc 3 : ""Find the surface area of the part of the sphere $x^2+y^2+z^2= r^2$ where $2r/3<z$","Calc 3 : ""Find the surface area of the part of the sphere  where",x^2+y^2+z^2= r^2 2r/3<z,Ive been struggling with this problem. I understand that as a surface integral problem I need to parameterize in terms of spherical coordinates. What I don't know is how does $2r/3<z$ affect the bounds and how I am supposed to integrate this. Any tips would be greatly appreciated. Thanks!,Ive been struggling with this problem. I understand that as a surface integral problem I need to parameterize in terms of spherical coordinates. What I don't know is how does $2r/3<z$ affect the bounds and how I am supposed to integrate this. Any tips would be greatly appreciated. Thanks!,,"['integration', 'multivariable-calculus', 'spheres']"
79,find flux outward a sphere cutted with $y\le-4$,find flux outward a sphere cutted with,y\le-4,"$$D=\{x^2+y^2+z^2\le 25,y\le -4\}. \qquad F=\{z^2,y^2,x^2\}$$ In order to find the total flux going outward D I need to evaluate two fluxes(or maybe not ? ): the first one is the flux of the circular region surface $x^2+z^2\le 9$ (which is the intersection between the sphere and $y\le -4$) and the second one is the surface created by cutting the sphere I calculated the first flux by first parametrizing the surface and then by evaluating the double integral over if : $$\iint\limits_{S_1} F\,ds$$ So I have $r(\theta)=(3\cos\theta,-4,3\sin\theta)$ with $0\le \theta \le 2\pi$ and the normal vector is $n=(0,-1,0)$. and eventually the flux will be = $-144\pi$ The problems start when I try to calculate the flux of the second surface : So i have $r(\theta,\phi)=(p\sin\phi\sin\theta,p\cos\phi,p\sin\phi\cos\theta)$ with $0\le \theta \le 2\pi$,$\cos^{-1}{(-4/5)\le \phi \le \pi}$ The normal shoud be : $n = (-p^2\sin^2\phi\sin\theta,-p^2\sin\phi\cos\phi,-p^2\sin^2\phi\sin^2\theta)$ And when I multiply $F(r(\theta,\phi))n = -p^4\cos^2\theta(2\sin^4\phi\sin\theta+\sin\phi\cos\phi)$ Is that really What I should put in the integral? (in order to find tge flux of the second region) What I'd like to get is the total flux! What would you do to?","$$D=\{x^2+y^2+z^2\le 25,y\le -4\}. \qquad F=\{z^2,y^2,x^2\}$$ In order to find the total flux going outward D I need to evaluate two fluxes(or maybe not ? ): the first one is the flux of the circular region surface $x^2+z^2\le 9$ (which is the intersection between the sphere and $y\le -4$) and the second one is the surface created by cutting the sphere I calculated the first flux by first parametrizing the surface and then by evaluating the double integral over if : $$\iint\limits_{S_1} F\,ds$$ So I have $r(\theta)=(3\cos\theta,-4,3\sin\theta)$ with $0\le \theta \le 2\pi$ and the normal vector is $n=(0,-1,0)$. and eventually the flux will be = $-144\pi$ The problems start when I try to calculate the flux of the second surface : So i have $r(\theta,\phi)=(p\sin\phi\sin\theta,p\cos\phi,p\sin\phi\cos\theta)$ with $0\le \theta \le 2\pi$,$\cos^{-1}{(-4/5)\le \phi \le \pi}$ The normal shoud be : $n = (-p^2\sin^2\phi\sin\theta,-p^2\sin\phi\cos\phi,-p^2\sin^2\phi\sin^2\theta)$ And when I multiply $F(r(\theta,\phi))n = -p^4\cos^2\theta(2\sin^4\phi\sin\theta+\sin\phi\cos\phi)$ Is that really What I should put in the integral? (in order to find tge flux of the second region) What I'd like to get is the total flux! What would you do to?",,"['integration', 'multivariable-calculus', 'surfaces']"
80,why is the curl of a function (which looks like a vortex) zero?,why is the curl of a function (which looks like a vortex) zero?,,"Can anyone explain why the curl of this function zero ? Clearly it's rotating then how is it irrotational ? Someone on SE said Not to mistake Curl for rotation, they aren't exactly similar. Is this true ? $\vec{V}(x,y,z)=[\frac{−y}{x^2+y^2},\frac{x}{x^2+y^2},0]$ Image Source: WolframAlpha Edit : As pointed by Rahul the topic of irrotational vortices have been already discussed over here , that somehow clears my intuitive understanding of curl: rotating the particles about their axes and not just revolving around center (which Travis has already pointed out in his answer) of the vortex but this still doesn't clear up my doubt about why the intuitive understanding doesn't match the calculations. If the curl is present at the center (z-axis), why doesn't it show up in the result (which is zero)?","Can anyone explain why the curl of this function zero ? Clearly it's rotating then how is it irrotational ? Someone on SE said Not to mistake Curl for rotation, they aren't exactly similar. Is this true ? $\vec{V}(x,y,z)=[\frac{−y}{x^2+y^2},\frac{x}{x^2+y^2},0]$ Image Source: WolframAlpha Edit : As pointed by Rahul the topic of irrotational vortices have been already discussed over here , that somehow clears my intuitive understanding of curl: rotating the particles about their axes and not just revolving around center (which Travis has already pointed out in his answer) of the vortex but this still doesn't clear up my doubt about why the intuitive understanding doesn't match the calculations. If the curl is present at the center (z-axis), why doesn't it show up in the result (which is zero)?",,"['multivariable-calculus', 'intuition', 'curl']"
81,Relation between area and perimeter of an ellipse in terms of semi-major and semi-minor axes.,Relation between area and perimeter of an ellipse in terms of semi-major and semi-minor axes.,,"We know the perimeter of a circunference (in terms of its radius $r$), $2 \pi r$, can be obtained by differentiating its area (with respect to its radius), $\pi r^2$. This fact generalizes to higher dimensions. Using partial derivatives, is there something similar if we consider: i) an ellipse ($\frac{x^2}{a^2} + \frac{y^2}{b^2} =1$) instead of a circunference and ii) its semi-major and semi-minor axes $a$ and $b$ instead of the radius $r$? Does it generalize to higher dimensions?","We know the perimeter of a circunference (in terms of its radius $r$), $2 \pi r$, can be obtained by differentiating its area (with respect to its radius), $\pi r^2$. This fact generalizes to higher dimensions. Using partial derivatives, is there something similar if we consider: i) an ellipse ($\frac{x^2}{a^2} + \frac{y^2}{b^2} =1$) instead of a circunference and ii) its semi-major and semi-minor axes $a$ and $b$ instead of the radius $r$? Does it generalize to higher dimensions?",,"['geometry', 'multivariable-calculus', 'differential-geometry']"
82,Intuition behind directional derivative (geometrically)?,Intuition behind directional derivative (geometrically)?,,"The directional derivative in the direction <0, 1> is < ∂f/∂x, ∂f, ∂y> ⋅ <0, 1>. This makes perfect sense because this comes out to taking ∂f/∂x and multiplying it to 1, and adding it to (∂f/∂y)*(0), which is zero, coming to an end result of ∂f/∂x. I do not understand how this can be generalized. How can we say, for each and every possible function, that the rate of change at a given point in the direction  will be a(∂f/∂x) + b(∂f/ ∂y)? Someone explained the directional derivative as being the sum of the vector's component in the x direction multiplied by a pure step in the x direction, and the vectors component in the y direction multiplied by a pure step in the y direction. For example, how can we say that for every possible function, the directional derivative in the direction <1/sqrt(2), 1/sqrt(2)> is equal to (1/sqrt(2))∂f/∂x + (1/sqrt(2))∂f/∂y? I would think things would vary from function to function. What relationship am I missing here? Please help!","The directional derivative in the direction <0, 1> is < ∂f/∂x, ∂f, ∂y> ⋅ <0, 1>. This makes perfect sense because this comes out to taking ∂f/∂x and multiplying it to 1, and adding it to (∂f/∂y)*(0), which is zero, coming to an end result of ∂f/∂x. I do not understand how this can be generalized. How can we say, for each and every possible function, that the rate of change at a given point in the direction  will be a(∂f/∂x) + b(∂f/ ∂y)? Someone explained the directional derivative as being the sum of the vector's component in the x direction multiplied by a pure step in the x direction, and the vectors component in the y direction multiplied by a pure step in the y direction. For example, how can we say that for every possible function, the directional derivative in the direction <1/sqrt(2), 1/sqrt(2)> is equal to (1/sqrt(2))∂f/∂x + (1/sqrt(2))∂f/∂y? I would think things would vary from function to function. What relationship am I missing here? Please help!",,"['multivariable-calculus', 'vectors', '3d']"
83,"Find the second degree Taylor polynomial of $f(x,y)=e^{-x^2-y^2}cos(xy)$ at $x_0=0$, $y_0=0$.","Find the second degree Taylor polynomial of  at , .","f(x,y)=e^{-x^2-y^2}cos(xy) x_0=0 y_0=0","I would like to either confirm my solution is correct, or find the error in it. I used the following MATLAB code to try to check my answer, but the solution it gave differs from mine. f = exp(-(x^2+y^2)) cos(x y); taylor(f,[x,y],[0,0]) ans = x^4/2 + (x^2*y^2)/2 - x^2 + y^4/2 - y^2 + 1 My solution: The second degree Taylor polynomial at the point $(a,b)$ is given by $$p_2(x,y)=f(a,b)+Df(a,b)\left[\begin{array}{c} x-a\\y-b \end{array}\right]+\frac{1}{2}[x-a\mbox{ }y-b]Hf(a,b)\left[\begin{array}{c} x-a\\y-b \end{array}\right]$$ where $Df(a,b)$ is the Jacobian matrix and $Hf(a,b)$ is the Hessian matrix (i.e. first and second order partial derivatives, respectively). Thus, we begin by computing the partial derivatives: \begin{equation*} \begin{split} \frac{\partial f}{\partial x}(x,y)=-e^{-x^2-y^2}(y\mbox{ sin}(xy)+2x\mbox{ cos}(xy))\\ \frac{\partial f}{\partial y}(x,y)=-e^{-x^2-y^2}(x\mbox{ sin}(xy)+2y\mbox{ cos}(xy))\\ \frac{\partial^2 f}{\partial x^2}(x,y)=e^{-x^2-y^2}((4x^2-y^2-2)\mbox{cos}(xy)+4xy\mbox{ sin}(xy))\\ \frac{\partial^2 f}{\partial y^2}(x,y)=e^{-x^2-y^2}(4xy\mbox{ sin}(xy)-(x^2-4y^2+2)\mbox{cos}(xy))\\ \frac{\partial^2 f}{\partial x\partial y}(x,y)=\frac{\partial^2 f}{\partial y\partial x}(x,y)=e^{-x^2-y^2}((2x^2+2y^2-1)\mbox{sin}(xy)+3xy\mbox{ cos}(xy)) \end{split} \end{equation*} Then, at the point $(x_0,y_0)=(0,0)$, \begin{equation*} \begin{split} f(0,0)=1\\ \frac{\partial f}{\partial x}(0,0)=\frac{\partial f}{\partial y}(0,0)=0\\ \frac{\partial^2 f}{\partial x^2}(0,0)=\frac{\partial^2 f}{\partial y^2}(0,0)=-2\\ \frac{\partial^2 f}{\partial x\partial y}(0,0)=\frac{\partial^2 f}{\partial y\partial x}(0,0)=0 \end{split} \end{equation*} Therefore, the second degree Taylor polynomial at the point $(0,0)$ is \begin{equation*} \begin{split} p_2(x,y)=f(0,0)+Df(0,0)\left[\begin{array}{c} x-0\\y-0 \end{array}\right]+\frac{1}{2}[x-0\mbox{ }y-0]Hf(0,0)\left[ \begin{array}{c} x-0\\y-0 \end{array}\right]\\ =1+[0\mbox{ }0]\left[ \begin{array}{c} x\\y \end{array}\right]+\frac{1}{2}[x\mbox{ }y]\begin{bmatrix} -2&0\\0&-2 \end{bmatrix}\left[ \begin{array}{c} x\\y \end{array}\right]=1-x^2-y^2 \end{split} \end{equation*}","I would like to either confirm my solution is correct, or find the error in it. I used the following MATLAB code to try to check my answer, but the solution it gave differs from mine. f = exp(-(x^2+y^2)) cos(x y); taylor(f,[x,y],[0,0]) ans = x^4/2 + (x^2*y^2)/2 - x^2 + y^4/2 - y^2 + 1 My solution: The second degree Taylor polynomial at the point $(a,b)$ is given by $$p_2(x,y)=f(a,b)+Df(a,b)\left[\begin{array}{c} x-a\\y-b \end{array}\right]+\frac{1}{2}[x-a\mbox{ }y-b]Hf(a,b)\left[\begin{array}{c} x-a\\y-b \end{array}\right]$$ where $Df(a,b)$ is the Jacobian matrix and $Hf(a,b)$ is the Hessian matrix (i.e. first and second order partial derivatives, respectively). Thus, we begin by computing the partial derivatives: \begin{equation*} \begin{split} \frac{\partial f}{\partial x}(x,y)=-e^{-x^2-y^2}(y\mbox{ sin}(xy)+2x\mbox{ cos}(xy))\\ \frac{\partial f}{\partial y}(x,y)=-e^{-x^2-y^2}(x\mbox{ sin}(xy)+2y\mbox{ cos}(xy))\\ \frac{\partial^2 f}{\partial x^2}(x,y)=e^{-x^2-y^2}((4x^2-y^2-2)\mbox{cos}(xy)+4xy\mbox{ sin}(xy))\\ \frac{\partial^2 f}{\partial y^2}(x,y)=e^{-x^2-y^2}(4xy\mbox{ sin}(xy)-(x^2-4y^2+2)\mbox{cos}(xy))\\ \frac{\partial^2 f}{\partial x\partial y}(x,y)=\frac{\partial^2 f}{\partial y\partial x}(x,y)=e^{-x^2-y^2}((2x^2+2y^2-1)\mbox{sin}(xy)+3xy\mbox{ cos}(xy)) \end{split} \end{equation*} Then, at the point $(x_0,y_0)=(0,0)$, \begin{equation*} \begin{split} f(0,0)=1\\ \frac{\partial f}{\partial x}(0,0)=\frac{\partial f}{\partial y}(0,0)=0\\ \frac{\partial^2 f}{\partial x^2}(0,0)=\frac{\partial^2 f}{\partial y^2}(0,0)=-2\\ \frac{\partial^2 f}{\partial x\partial y}(0,0)=\frac{\partial^2 f}{\partial y\partial x}(0,0)=0 \end{split} \end{equation*} Therefore, the second degree Taylor polynomial at the point $(0,0)$ is \begin{equation*} \begin{split} p_2(x,y)=f(0,0)+Df(0,0)\left[\begin{array}{c} x-0\\y-0 \end{array}\right]+\frac{1}{2}[x-0\mbox{ }y-0]Hf(0,0)\left[ \begin{array}{c} x-0\\y-0 \end{array}\right]\\ =1+[0\mbox{ }0]\left[ \begin{array}{c} x\\y \end{array}\right]+\frac{1}{2}[x\mbox{ }y]\begin{bmatrix} -2&0\\0&-2 \end{bmatrix}\left[ \begin{array}{c} x\\y \end{array}\right]=1-x^2-y^2 \end{split} \end{equation*}",,"['multivariable-calculus', 'proof-verification', 'matlab']"
84,Derivative of a composition of differentiable function and a continously differentiable curve,Derivative of a composition of differentiable function and a continously differentiable curve,,"Let $f:\mathbb{R}^2\to\mathbb{R}$ be a differential function at the origin, and $\gamma_1,\gamma_2: (-1,1)\to \mathbb{R}^2$ continously differentiable curves at the origin, s.t. $\gamma_1(0)=\gamma_2(0)=(0,0)$, and $\forall t\in(-1,1) :\gamma_2(t)=\gamma_1(t)+(t^2,t^3)$. Show $\frac {d (f(\gamma_1(t))} {dt}\bigg\rvert_{t=0} = \frac {d (f(\gamma_2(t))} {dt} \bigg\rvert_{t=0}$. I tried applying the chain rule the following way: 1)$D_{f\circ\gamma_1(t)}(0)=D_f(0)D_{\gamma_1}(0)$. 2) $D_{f\circ\gamma_2(t)}(0)=D_f(0)D_{\gamma_2}(0)=D_f(0)D_{\gamma_1+(t^2,t^3)}(0)=^*D_f(0)(D_{\gamma_1}+(2t,3t^2)(0)=D_f(0)D_{\gamma_1}(0)$ (*)- From linearity of the derivative operation Is this use of the chain rule correct? And if so, where did I use continuity of the derivatives of $\gamma$?","Let $f:\mathbb{R}^2\to\mathbb{R}$ be a differential function at the origin, and $\gamma_1,\gamma_2: (-1,1)\to \mathbb{R}^2$ continously differentiable curves at the origin, s.t. $\gamma_1(0)=\gamma_2(0)=(0,0)$, and $\forall t\in(-1,1) :\gamma_2(t)=\gamma_1(t)+(t^2,t^3)$. Show $\frac {d (f(\gamma_1(t))} {dt}\bigg\rvert_{t=0} = \frac {d (f(\gamma_2(t))} {dt} \bigg\rvert_{t=0}$. I tried applying the chain rule the following way: 1)$D_{f\circ\gamma_1(t)}(0)=D_f(0)D_{\gamma_1}(0)$. 2) $D_{f\circ\gamma_2(t)}(0)=D_f(0)D_{\gamma_2}(0)=D_f(0)D_{\gamma_1+(t^2,t^3)}(0)=^*D_f(0)(D_{\gamma_1}+(2t,3t^2)(0)=D_f(0)D_{\gamma_1}(0)$ (*)- From linearity of the derivative operation Is this use of the chain rule correct? And if so, where did I use continuity of the derivatives of $\gamma$?",,"['multivariable-calculus', 'derivatives']"
85,Multivariable Taylor's formula and approximation (big O),Multivariable Taylor's formula and approximation (big O),,"I post here because I encounter a problem to understand something. To begin, let consider $f : \mathbb{R}^n \rightarrow \mathbb{R}$ a n times differentiable function. Then, we have, by the Taylor's formula : $$ \forall a, h \in \mathbb{R}^n \quad f(a+h)=f(a)+Df_a(h) + ... + \frac{1}{n!}D^nf_a(h^n) + o_o(||h||^n)$$ So, when $n=1$, it's easy to see that each term of the taylor expansion is ""bigger"" than the next one, i.e for $k \in \mathbb{N}$, we have : $D^kf_a(h^k)=f^{(k)}(a)h^k$ and then $o(h^k) = o(D^kf_a(h^k))$, but I've some trouble when $n>1$. Actually, for $n=2$ and the taylor formula at the order 2, we have : $D^2f_a((x_1, x_2), (x_1,x_2)) = \frac{\partial ^2 f}{\partial x_1^2}(a) x_1^2 + 2\frac{\partial ^2 f}{\partial x_1 \partial x_2}(a) x_1x_2 + \frac{\partial ^2 f}{\partial x_2^2}(a) x_2^2$ And I want to show that : $o(||(x_1,x_2)||^2) = o(||D^2f_a((x_1, x_2), (x_1,x_2))||)$. By considering (and utilizating the equivalence of the norm in finite dimension), if I consider the norm $||(x_1,x_2)|| = \max (|x_1|, |x_2|)$, then I have : $D^2f_a((x_1, x_2), (x_1,x_2)) = O(||(x_1,x_2)||^2)$, but I whould like to have $||(x_1,x_2)||^2 = O(||D^2f_a((x_1, x_2), (x_1,x_2))||)$. But, still with the norm max, if I consider $x_1x_2$, I should have : $x_1x_2$ which verify : $x_1x_2 \geq C||(x_1,x_2)^2||$, but if I consider $(x_1,x_2) = (e, e^2)$, for $e<<0$, I have : $e^3 \geq Ce^2$, which is not true. So, I'm kind of lost. And here, I've only considered $\mathbb{R}^n$, but what happen if I consider a vector space of infinite dimension ? (cause I could not use the equivalence of norm)","I post here because I encounter a problem to understand something. To begin, let consider $f : \mathbb{R}^n \rightarrow \mathbb{R}$ a n times differentiable function. Then, we have, by the Taylor's formula : $$ \forall a, h \in \mathbb{R}^n \quad f(a+h)=f(a)+Df_a(h) + ... + \frac{1}{n!}D^nf_a(h^n) + o_o(||h||^n)$$ So, when $n=1$, it's easy to see that each term of the taylor expansion is ""bigger"" than the next one, i.e for $k \in \mathbb{N}$, we have : $D^kf_a(h^k)=f^{(k)}(a)h^k$ and then $o(h^k) = o(D^kf_a(h^k))$, but I've some trouble when $n>1$. Actually, for $n=2$ and the taylor formula at the order 2, we have : $D^2f_a((x_1, x_2), (x_1,x_2)) = \frac{\partial ^2 f}{\partial x_1^2}(a) x_1^2 + 2\frac{\partial ^2 f}{\partial x_1 \partial x_2}(a) x_1x_2 + \frac{\partial ^2 f}{\partial x_2^2}(a) x_2^2$ And I want to show that : $o(||(x_1,x_2)||^2) = o(||D^2f_a((x_1, x_2), (x_1,x_2))||)$. By considering (and utilizating the equivalence of the norm in finite dimension), if I consider the norm $||(x_1,x_2)|| = \max (|x_1|, |x_2|)$, then I have : $D^2f_a((x_1, x_2), (x_1,x_2)) = O(||(x_1,x_2)||^2)$, but I whould like to have $||(x_1,x_2)||^2 = O(||D^2f_a((x_1, x_2), (x_1,x_2))||)$. But, still with the norm max, if I consider $x_1x_2$, I should have : $x_1x_2$ which verify : $x_1x_2 \geq C||(x_1,x_2)^2||$, but if I consider $(x_1,x_2) = (e, e^2)$, for $e<<0$, I have : $e^3 \geq Ce^2$, which is not true. So, I'm kind of lost. And here, I've only considered $\mathbb{R}^n$, but what happen if I consider a vector space of infinite dimension ? (cause I could not use the equivalence of norm)",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
86,"Generalized polar coordinates, how to switch form cartesian to polar","Generalized polar coordinates, how to switch form cartesian to polar",,"I was solving a problem: $$\left(\frac xa + \frac yb\right)^ 4=4xy,\quad a>0 , b>0 $$ Find the are bounded by curve. It is not hard to see that x and y must have the same sign, that function exist only in first and third quadrant, and that the function is symetrical to ( 0,0). So i want to transform this to polar coordinates. The regular transformation is $$ x=r\cos\phi \\ y=r\sin\phi $$  I have noticed that these are only used for circle with centre in (0,0). Whenever the function is  more complicated the transformation is different. Why ? The book solution says that we will moved generalized coordinates use moved generalized coordinates :  $$  x-x_0 = ar\beta \cos \alpha \phi\\ y-y_0 = br\beta \sin \alpha \phi $$  Then it says $$ x_0=y_0=0 $$ $$ \alpha=2, \beta=1 $$ It says we did this to make the curve equation simpler. I see why the first equation is true but the second with $\alpha$ and $\beta$, I don't understand. Why are $\alpha =2, \beta = 1$, and where does this generalized polar coordinates equation come from ? Also, how to think when switching to polar coordinates when dealing with ˝more complicated curves˝ ( for me they are complicated) Edit : also i have seen a different transformation used in this problem $$ x=ar\cos^ 2\phi \\ y=br\sin^ 2\phi $$ then this was transformed with double integral identity.","I was solving a problem: $$\left(\frac xa + \frac yb\right)^ 4=4xy,\quad a>0 , b>0 $$ Find the are bounded by curve. It is not hard to see that x and y must have the same sign, that function exist only in first and third quadrant, and that the function is symetrical to ( 0,0). So i want to transform this to polar coordinates. The regular transformation is $$ x=r\cos\phi \\ y=r\sin\phi $$  I have noticed that these are only used for circle with centre in (0,0). Whenever the function is  more complicated the transformation is different. Why ? The book solution says that we will moved generalized coordinates use moved generalized coordinates :  $$  x-x_0 = ar\beta \cos \alpha \phi\\ y-y_0 = br\beta \sin \alpha \phi $$  Then it says $$ x_0=y_0=0 $$ $$ \alpha=2, \beta=1 $$ It says we did this to make the curve equation simpler. I see why the first equation is true but the second with $\alpha$ and $\beta$, I don't understand. Why are $\alpha =2, \beta = 1$, and where does this generalized polar coordinates equation come from ? Also, how to think when switching to polar coordinates when dealing with ˝more complicated curves˝ ( for me they are complicated) Edit : also i have seen a different transformation used in this problem $$ x=ar\cos^ 2\phi \\ y=br\sin^ 2\phi $$ then this was transformed with double integral identity.",,"['calculus', 'multivariable-calculus', 'polar-coordinates', 'multiple-integral']"
87,Finding extremals which arise from a non-linear third order ODE,Finding extremals which arise from a non-linear third order ODE,,"I'm trying to determine the extremals of the functional $F(x,y,y',y'') = (y')^2 + (y'')^2$. I know that in this case the first integral satisfies $$F - y' \left( F_{y'} - \frac{\mathrm{d}}{\mathrm{d}x}F_{y''}\right)-y''F_{y''}=k$$ where $k$ is a constant. For the aforementioned functional, I can easily see that $F_{y'} = 2y'$ and that $F_{y''} = 2y''$, and furthermore $\frac{\mathrm{d}}{\mathrm{d}x}F_{y''}=2y'''$. If I put this into the equation satisfied by the extremal, we get $$(y')^2 + (y'')^2 -y'(2y' - 2y''') - y''(2y'')=k$$ and it is then possible to re-write this as $$(y')^2 + (y'')^2 = 2y'y''' - k$$ I am unsure what to do next. Is there a trick that I can use to solve this nonlinear third order ODE? I tried integrating both sides between 0 and 1 (as those are the two $x$-values that the boundary conditions specify) as I can see that the functional appears on the left hand side of the equation, but the exercise I am doing is asking for the extremals which implies that I need to find $y(x)$ rather than what the stationary value of the integral is.","I'm trying to determine the extremals of the functional $F(x,y,y',y'') = (y')^2 + (y'')^2$. I know that in this case the first integral satisfies $$F - y' \left( F_{y'} - \frac{\mathrm{d}}{\mathrm{d}x}F_{y''}\right)-y''F_{y''}=k$$ where $k$ is a constant. For the aforementioned functional, I can easily see that $F_{y'} = 2y'$ and that $F_{y''} = 2y''$, and furthermore $\frac{\mathrm{d}}{\mathrm{d}x}F_{y''}=2y'''$. If I put this into the equation satisfied by the extremal, we get $$(y')^2 + (y'')^2 -y'(2y' - 2y''') - y''(2y'')=k$$ and it is then possible to re-write this as $$(y')^2 + (y'')^2 = 2y'y''' - k$$ I am unsure what to do next. Is there a trick that I can use to solve this nonlinear third order ODE? I tried integrating both sides between 0 and 1 (as those are the two $x$-values that the boundary conditions specify) as I can see that the functional appears on the left hand side of the equation, but the exercise I am doing is asking for the extremals which implies that I need to find $y(x)$ rather than what the stationary value of the integral is.",,"['ordinary-differential-equations', 'multivariable-calculus']"
88,One Critical Value despite being on a compact set,One Critical Value despite being on a compact set,,"Let $\alpha > 1$ and $s >0$ find all the local extrema of $f: \mathbb R_{\geq 0} \times \mathbb R_{\geq 0}\to\mathbb R, f(x,y):=x^{\alpha}+y^{\alpha}$ under the constraint that $h(x,y):=x+y-s=0$ In the solutions we found that using lagrange multipliers $(\frac{s}{2},\frac{s}{2})$ is the only candidate, HOWEVER , defining $M:=\{(x,y)\in \mathbb R_{\geq 0}^{2}:x+y=s\}$, I would assume that $M$ is compact and therefore $f|_{M}$ needs to take on a minimum and a maximum, but I have only found one candidate, how can that be?","Let $\alpha > 1$ and $s >0$ find all the local extrema of $f: \mathbb R_{\geq 0} \times \mathbb R_{\geq 0}\to\mathbb R, f(x,y):=x^{\alpha}+y^{\alpha}$ under the constraint that $h(x,y):=x+y-s=0$ In the solutions we found that using lagrange multipliers $(\frac{s}{2},\frac{s}{2})$ is the only candidate, HOWEVER , defining $M:=\{(x,y)\in \mathbb R_{\geq 0}^{2}:x+y=s\}$, I would assume that $M$ is compact and therefore $f|_{M}$ needs to take on a minimum and a maximum, but I have only found one candidate, how can that be?",,"['real-analysis', 'multivariable-calculus', 'optimization', 'compactness']"
89,What does it mean for an integral to be stationary?,What does it mean for an integral to be stationary?,,I may have the wrong group.  I could not find calculus of variations and had to start somewhere. In the calculus of variations we start by finding the 0 points where the functions are at minimum or maximum. Is this the same as stationary that is referred to in standard textbooks ? i.e  where the integral = I[f] is stationary.   Can stationary be replaced with 0?   I get confused  when a new word is added to describe something that doesn't need a new word.  Or perhaps means something else.  If so can someone explain?,I may have the wrong group.  I could not find calculus of variations and had to start somewhere. In the calculus of variations we start by finding the 0 points where the functions are at minimum or maximum. Is this the same as stationary that is referred to in standard textbooks ? i.e  where the integral = I[f] is stationary.   Can stationary be replaced with 0?   I get confused  when a new word is added to describe something that doesn't need a new word.  Or perhaps means something else.  If so can someone explain?,,['multivariable-calculus']
90,Limit of a function at a point in the domain of the function and/or the closure of the domain.,Limit of a function at a point in the domain of the function and/or the closure of the domain.,,"I'm working through Vector Calculus, Linear Algebra and Differential Forms: A Unified Approach (5th ed, Hubbard) and on page 91 the author writes ""Limits like $\lim_{x\rightarrow x_0}f(x)$ can only be defined if you can approach $x_0$ by points where $f$ can be evaluated. Thus when $x_0$ is in the closure of the domain of $f$, it makes sense to ask whether the limit $\lim_{x\rightarrow x_0}f(x)$ exists. Of course, this includes the case when $x_0$ is in the domain of $f$; in that case for the limit to exist we must have $$\lim_{x\rightarrow x_0}f(x)=f(x_0).\hspace{5em}(1)$$ But the interesting case is when $x_0$ is in the closure of the domain but not in the domain. For example, does it make sense to speak of $$\lim_{x\rightarrow 0}(1+x)^{1/x}?\hspace{7em}(2)""$$ And on the next page where he defines the limit of a function, he gives the usual definition except that he allows $|x-x_0|=0$ (in his words this makes limits better behaved under composition). are my statements correct? If $x_0$ is in the closure of the domain and the domain, then since the author allows $|x-x_0|=0$ in the definition of the limit (and clearly $x_0-x_0=0$), we get the condition (1). If $x_0$ is in the closure of the domain but not the domain, it is in the boundary of the domain and $|x-x_0|>0$ for all $x$ , so for (2) it does make sense to speak of the limit since we can still approach $x_0$ from points in the domain and since $|x-x_0|>0$  we don't require (1) (or even for $f(x_0)$ to exist, which it cannot since $x_0$ is not in the domain).","I'm working through Vector Calculus, Linear Algebra and Differential Forms: A Unified Approach (5th ed, Hubbard) and on page 91 the author writes ""Limits like $\lim_{x\rightarrow x_0}f(x)$ can only be defined if you can approach $x_0$ by points where $f$ can be evaluated. Thus when $x_0$ is in the closure of the domain of $f$, it makes sense to ask whether the limit $\lim_{x\rightarrow x_0}f(x)$ exists. Of course, this includes the case when $x_0$ is in the domain of $f$; in that case for the limit to exist we must have $$\lim_{x\rightarrow x_0}f(x)=f(x_0).\hspace{5em}(1)$$ But the interesting case is when $x_0$ is in the closure of the domain but not in the domain. For example, does it make sense to speak of $$\lim_{x\rightarrow 0}(1+x)^{1/x}?\hspace{7em}(2)""$$ And on the next page where he defines the limit of a function, he gives the usual definition except that he allows $|x-x_0|=0$ (in his words this makes limits better behaved under composition). are my statements correct? If $x_0$ is in the closure of the domain and the domain, then since the author allows $|x-x_0|=0$ in the definition of the limit (and clearly $x_0-x_0=0$), we get the condition (1). If $x_0$ is in the closure of the domain but not the domain, it is in the boundary of the domain and $|x-x_0|>0$ for all $x$ , so for (2) it does make sense to speak of the limit since we can still approach $x_0$ from points in the domain and since $|x-x_0|>0$  we don't require (1) (or even for $f(x_0)$ to exist, which it cannot since $x_0$ is not in the domain).",,"['calculus', 'multivariable-calculus']"
91,Derivative of Softmax without cross entropy,Derivative of Softmax without cross entropy,,"There are several resources that show how to find the derivatives of the softmax + cross_entropy loss together. However, I want to derive the derivatives separately. For the purposes of this question, I will use a fixed input vector containing 4 values. Input vector $$\left [ x_{0}, \quad x_{1}, \quad x_{2}, \quad x_{3}\right ]$$ Softmax Function and Derivative My softmax function is defined as : $$\left [ \frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}\right ] $$ Since each element in the vector depends on all the values of the input vector, it makes sense that the gradients for each output element will contain some expression that contains all the input values. My jacobian is this: $$ \left[\begin{matrix}\frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{0}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{2}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{2}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\end{matrix}\right] $$ Each row contains the contribution from each output element. To calculate the 'final' derivative of each node , I sum up all the elements in each row, to get a vector which is the same size as my input vector. Due to numerical stability issues, summing up the values gives unstable results.  However, it is quite easy to reduce the sum of each row to this expression: Notice that except the first term (the only term that is positive) in each row, summing all the negative terms is equivalent to doing: $$\sum_{i}{} softmax_{x_0} * softmax_{x_i} $$ and the first term is just $$ softmax_{x_0} $$ Which means the derivative of softmax is : $$softmax - softmax^2$$ or $$softmax(1-softmax)$$ This seems correct, and Geoff Hinton's video (at time 4:07) has this same solution. This answer also seems to get to the same equation as me. Cross Entropy Loss and its derivative The cross entropy takes in as input the softmax vector and a 'target' probability distribution. $$\left [ t_{0}, \quad t_{1}, \quad t_{2}, \quad t_{3}\right ]$$ Let the softmax index at i be denoted as $s_i$  So the full softmax vector is : $$\left [ s_{0}, \quad s_{1}, \quad s_{2}, \quad s_{3}\right ]$$ Cross entropy function $$ - \sum_{i}^{classes} t_i log(s_i) $$ For our case it is $$ - t_{0} \log{\left (\frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{1} \log{\left (\frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{2} \log{\left (\frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{3} \log{\left (\frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} $$ Derivative of cross entropy Using the simple multiplication rule along with the log rule, the derivative of cross entropy is: $$ -\frac{t_i}{s_i} $$ Using chain rule to get derivative of softmax with cross entropy We can just multiply the cross entropy derivative (which calculates Loss with respect to softmax output) with the softmax derivative (which calculates Softmax with respect to input) to get: $$ -\frac{t_i}{s_i} * s_i(1-s_i) $$ Simplifying , it gives $$ -t_i *(1-s_i) $$ Analytically computing derivative of softmax with cross entropy This document derives the derivative of softmax with cross entropy and it gets: $$ s_i - t_i $$ Which is different from the one derived using chain rule. Implementation using numpy I thought perhaps both the derivatives would evaluate to the same result, and I had missed some simplification that could be applied using assumptions (e.g. probability distributions sum up to 1) This is the code to evaluate: x = np.array([-1.0, -1.0, 1.0])                 # unscaled logits, my x vector t = np.array([0.0,1.0,0.0])                     # target probability distribution   ## Function definitions  def softmax(v):     exps = np.exp(v)     sum  = np.sum(exps)     return exps/sum  def cross_entropy(inps,targets):     return np.sum(-targets*np.log(inps))  def cross_entropy_derivatives(inps,targets):     return -targets/inps  def softmax_derivatives(softmax):     return softmax  * (1-softmax)   soft = softmax(v)                               # [0.10650698, 0.10650698, 0.78698604]  cross_entropy(soft,t)                           # 2.2395447662218846  cross_der = cross_entropy_derivatives(soft,t)   # [-0.       , -9.3890561, -0.       ]  soft_der = softmax_derivatives(soft)            # [0.09516324, 0.09516324, 0.16763901]  ## Derivative using chain rule   cross_der * soft_der                            # [-0.        , -0.89349302, -0.        ]   ## Derivative using analytical derivation   soft - t                                        # [ 0.10650698, -0.89349302,  0.78698604] Notice the difference in values. My question, to clarify, is, what is the mistake that I am making. These two values should be quite similar.","There are several resources that show how to find the derivatives of the softmax + cross_entropy loss together. However, I want to derive the derivatives separately. For the purposes of this question, I will use a fixed input vector containing 4 values. Input vector $$\left [ x_{0}, \quad x_{1}, \quad x_{2}, \quad x_{3}\right ]$$ Softmax Function and Derivative My softmax function is defined as : $$\left [ \frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}\right ] $$ Since each element in the vector depends on all the values of the input vector, it makes sense that the gradients for each output element will contain some expression that contains all the input values. My jacobian is this: $$ \left[\begin{matrix}\frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{0}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{2}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{2}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\end{matrix}\right] $$ Each row contains the contribution from each output element. To calculate the 'final' derivative of each node , I sum up all the elements in each row, to get a vector which is the same size as my input vector. Due to numerical stability issues, summing up the values gives unstable results.  However, it is quite easy to reduce the sum of each row to this expression: Notice that except the first term (the only term that is positive) in each row, summing all the negative terms is equivalent to doing: $$\sum_{i}{} softmax_{x_0} * softmax_{x_i} $$ and the first term is just $$ softmax_{x_0} $$ Which means the derivative of softmax is : $$softmax - softmax^2$$ or $$softmax(1-softmax)$$ This seems correct, and Geoff Hinton's video (at time 4:07) has this same solution. This answer also seems to get to the same equation as me. Cross Entropy Loss and its derivative The cross entropy takes in as input the softmax vector and a 'target' probability distribution. $$\left [ t_{0}, \quad t_{1}, \quad t_{2}, \quad t_{3}\right ]$$ Let the softmax index at i be denoted as $s_i$  So the full softmax vector is : $$\left [ s_{0}, \quad s_{1}, \quad s_{2}, \quad s_{3}\right ]$$ Cross entropy function $$ - \sum_{i}^{classes} t_i log(s_i) $$ For our case it is $$ - t_{0} \log{\left (\frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{1} \log{\left (\frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{2} \log{\left (\frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{3} \log{\left (\frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} $$ Derivative of cross entropy Using the simple multiplication rule along with the log rule, the derivative of cross entropy is: $$ -\frac{t_i}{s_i} $$ Using chain rule to get derivative of softmax with cross entropy We can just multiply the cross entropy derivative (which calculates Loss with respect to softmax output) with the softmax derivative (which calculates Softmax with respect to input) to get: $$ -\frac{t_i}{s_i} * s_i(1-s_i) $$ Simplifying , it gives $$ -t_i *(1-s_i) $$ Analytically computing derivative of softmax with cross entropy This document derives the derivative of softmax with cross entropy and it gets: $$ s_i - t_i $$ Which is different from the one derived using chain rule. Implementation using numpy I thought perhaps both the derivatives would evaluate to the same result, and I had missed some simplification that could be applied using assumptions (e.g. probability distributions sum up to 1) This is the code to evaluate: x = np.array([-1.0, -1.0, 1.0])                 # unscaled logits, my x vector t = np.array([0.0,1.0,0.0])                     # target probability distribution   ## Function definitions  def softmax(v):     exps = np.exp(v)     sum  = np.sum(exps)     return exps/sum  def cross_entropy(inps,targets):     return np.sum(-targets*np.log(inps))  def cross_entropy_derivatives(inps,targets):     return -targets/inps  def softmax_derivatives(softmax):     return softmax  * (1-softmax)   soft = softmax(v)                               # [0.10650698, 0.10650698, 0.78698604]  cross_entropy(soft,t)                           # 2.2395447662218846  cross_der = cross_entropy_derivatives(soft,t)   # [-0.       , -9.3890561, -0.       ]  soft_der = softmax_derivatives(soft)            # [0.09516324, 0.09516324, 0.16763901]  ## Derivative using chain rule   cross_der * soft_der                            # [-0.        , -0.89349302, -0.        ]   ## Derivative using analytical derivation   soft - t                                        # [ 0.10650698, -0.89349302,  0.78698604] Notice the difference in values. My question, to clarify, is, what is the mistake that I am making. These two values should be quite similar.",,"['calculus', 'multivariable-calculus', 'neural-networks']"
92,How to solve a nonlinear ODE with multiple product of derivative terms?,How to solve a nonlinear ODE with multiple product of derivative terms?,,"I came across a highly nonlinear 4th order ODE, with multiple derivative product terms, after applying a transformation to an even more complex ODE. I have two queries, one rather specific and other more general. Is there a general solution to $A^2 \nu  G^{(4)}(\eta )-4 A \nu  G^{(3)}(\eta )-A G(\eta ) G^{(3)}(\eta )+A G'(\eta ) G''(\eta )+4 \nu  G''(\eta )+2 G(\eta ) G''(\eta )=0$ $A$ and $\nu$ are constants from the transformation which for our purposes is arbitrary. It does have a trivial solution $G(\eta) = \alpha \eta + \beta$, the linear function. I could not find any other solutions with all the usual methods. Can somebody suggest a different solution or a method to solve the equation? How do we approach such ODE's with multiple derivative product terms? i.e the kind of ODE's which are nonlinear but without any other functions appearing in it. I mean ODE's of the type $L\left(G^{(4)}(\eta),G^{(3)}(\eta),G^{(2)}(\eta),G^{(1)}(\eta),G(\eta)\right)=0$","I came across a highly nonlinear 4th order ODE, with multiple derivative product terms, after applying a transformation to an even more complex ODE. I have two queries, one rather specific and other more general. Is there a general solution to $A^2 \nu  G^{(4)}(\eta )-4 A \nu  G^{(3)}(\eta )-A G(\eta ) G^{(3)}(\eta )+A G'(\eta ) G''(\eta )+4 \nu  G''(\eta )+2 G(\eta ) G''(\eta )=0$ $A$ and $\nu$ are constants from the transformation which for our purposes is arbitrary. It does have a trivial solution $G(\eta) = \alpha \eta + \beta$, the linear function. I could not find any other solutions with all the usual methods. Can somebody suggest a different solution or a method to solve the equation? How do we approach such ODE's with multiple derivative product terms? i.e the kind of ODE's which are nonlinear but without any other functions appearing in it. I mean ODE's of the type $L\left(G^{(4)}(\eta),G^{(3)}(\eta),G^{(2)}(\eta),G^{(1)}(\eta),G(\eta)\right)=0$",,"['ordinary-differential-equations', 'analysis', 'multivariable-calculus', 'nonlinear-system']"
93,Differentiability of a determinant and its inverse,Differentiability of a determinant and its inverse,,"Show that $\det:\mathbb{R}^{n\times m}\rightarrow\mathbb{R}, M \mapsto \det(M)$ is differentiable (without having to calculate its derivative. Show that its inverse is differentiable as well, i.e. for $\text{GL}(n)=\{M\in\mathbb{R}^{n\times m}:\det(M)\neq 0\}$ we have $\text{GL}(n)\rightarrow \text{GL}(n), M\mapsto M^{-1}$ is differentiable. Am I correct here that the correct procedure is to express the function as a polynomial making use of the Leibniz formula, and then argue that this function is infinitely differentiable as a composition of differentiable functions? Or am I missing a step here, or is possibly expressing it as a polynomial not allowed (or rather I would have to prove its polynomial representation first)? This one I'm lost on and wouldn't know how to do. It probably involves Cramer's rule in some fashion and the Jacobian matrix and determinant, but I would be incapable of writing this up I think. Does anybody know where I could look at a proof of this?","Show that $\det:\mathbb{R}^{n\times m}\rightarrow\mathbb{R}, M \mapsto \det(M)$ is differentiable (without having to calculate its derivative. Show that its inverse is differentiable as well, i.e. for $\text{GL}(n)=\{M\in\mathbb{R}^{n\times m}:\det(M)\neq 0\}$ we have $\text{GL}(n)\rightarrow \text{GL}(n), M\mapsto M^{-1}$ is differentiable. Am I correct here that the correct procedure is to express the function as a polynomial making use of the Leibniz formula, and then argue that this function is infinitely differentiable as a composition of differentiable functions? Or am I missing a step here, or is possibly expressing it as a polynomial not allowed (or rather I would have to prove its polynomial representation first)? This one I'm lost on and wouldn't know how to do. It probably involves Cramer's rule in some fashion and the Jacobian matrix and determinant, but I would be incapable of writing this up I think. Does anybody know where I could look at a proof of this?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'proof-verification', 'jacobian']"
94,Computing the electric flux [duplicate],Computing the electric flux [duplicate],,"This question already has an answer here : Using Divergence Theorem (1 answer) Closed 5 years ago . Let $B_r=\{x\in \mathbb R^3 : |x|\le r\}$ and let $dS_r$ denote the area element on $\partial B_r$. Set $$E(x)=C\int_{\partial B_R}\nabla_x |x-y|^{-1} dS_y$$ Show that for $|x|< R$, $E$ is zero and for $r<R$, the flux $\int_{\partial B_r} E(x)\cdot \nu \ dS_x$ is zero. First of all, what does $\nabla_x$ stand for? The $\nabla$ without subscripts usually stands for the gradient, but I'm not sure about $\nabla_x$. For the flux integral, is it better to use the divergence theorem or use the definition? In the former case, how do I find the divergence (i.e., how to differentiate a line integral)?","This question already has an answer here : Using Divergence Theorem (1 answer) Closed 5 years ago . Let $B_r=\{x\in \mathbb R^3 : |x|\le r\}$ and let $dS_r$ denote the area element on $\partial B_r$. Set $$E(x)=C\int_{\partial B_R}\nabla_x |x-y|^{-1} dS_y$$ Show that for $|x|< R$, $E$ is zero and for $r<R$, the flux $\int_{\partial B_r} E(x)\cdot \nu \ dS_x$ is zero. First of all, what does $\nabla_x$ stand for? The $\nabla$ without subscripts usually stands for the gradient, but I'm not sure about $\nabla_x$. For the flux integral, is it better to use the divergence theorem or use the definition? In the former case, how do I find the divergence (i.e., how to differentiate a line integral)?",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
95,Find the stationary points of w = $ −3x^2 − 4xy − y^2− 12y + 16x $ which reside at 1st quadrant,Find the stationary points of w =  which reside at 1st quadrant, −3x^2 − 4xy − y^2− 12y + 16x ,"Find the stationary points of  w = $ −3x^2 − 4xy − y^2− 12y + 16x $ which reside at 1st quadrant. I did this problem with traditional method : like find $f_x$ = 0 $f_y$ = 0  and checking $f_{xx} f_{yy} - f_{xy}^2 $ which I got negative. I got the critical point as  = <-20,34>. Now how can I get a critical point on first quadrant? They've given the answer as <8/3,0>.","Find the stationary points of  w = $ −3x^2 − 4xy − y^2− 12y + 16x $ which reside at 1st quadrant. I did this problem with traditional method : like find $f_x$ = 0 $f_y$ = 0  and checking $f_{xx} f_{yy} - f_{xy}^2 $ which I got negative. I got the critical point as  = <-20,34>. Now how can I get a critical point on first quadrant? They've given the answer as <8/3,0>.",,['multivariable-calculus']
96,A line integral as the average of squares of distances,A line integral as the average of squares of distances,,"Let $A$ be a domain  in $\mathbb R^2$ whose boundary $\gamma $ is a smooth positively oriented curve and whose area is $|A|$. Find a function $F:\mathbb R^2\to \mathbb R$ such that $$\frac{1}{|A|}\int_\gamma  Fdx+Fdy$$ is the average value of the square of the distance from the origin to a point of $A$. I guess I should use Green's theorem at some point, but I don't know how exactly, and how to start. The distance from $(x,y)\in A$ to the origin is $\sqrt{x^2+y^2}$. So the average I believe is $\frac{\sum_{(x,y)\in A} x^2+y^2}{|A|}$? I don't know where to get from this.","Let $A$ be a domain  in $\mathbb R^2$ whose boundary $\gamma $ is a smooth positively oriented curve and whose area is $|A|$. Find a function $F:\mathbb R^2\to \mathbb R$ such that $$\frac{1}{|A|}\int_\gamma  Fdx+Fdy$$ is the average value of the square of the distance from the origin to a point of $A$. I guess I should use Green's theorem at some point, but I don't know how exactly, and how to start. The distance from $(x,y)\in A$ to the origin is $\sqrt{x^2+y^2}$. So the average I believe is $\frac{\sum_{(x,y)\in A} x^2+y^2}{|A|}$? I don't know where to get from this.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
97,Finding limit of norm of difference quotient,Finding limit of norm of difference quotient,,"Let $F:\mathbb{R}^2\to\mathbb{R}^2$ be continuously differentiable and suppose $F(1,1)=(1,0)$. Suppose you are given with the Jacobian matrix of $F$. How can we compute for $\displaystyle\lim_{(x,y)\to(1,1)}\dfrac{\|F(x,y)-F(1,1)\|}{\|(x,y)-(1,1)\|}$? Thanks","Let $F:\mathbb{R}^2\to\mathbb{R}^2$ be continuously differentiable and suppose $F(1,1)=(1,0)$. Suppose you are given with the Jacobian matrix of $F$. How can we compute for $\displaystyle\lim_{(x,y)\to(1,1)}\dfrac{\|F(x,y)-F(1,1)\|}{\|(x,y)-(1,1)\|}$? Thanks",,"['calculus', 'real-analysis', 'multivariable-calculus']"
98,"Which kind of functions $f(x, \ y)$ can be written as $g(x)\cdot h(y)$? [duplicate]",Which kind of functions  can be written as ? [duplicate],"f(x, \ y) g(x)\cdot h(y)","This question already has an answer here : Is that true that not every function $f(x,y)$ can be writen as $h(x) g(y)$? [closed] (1 answer) Closed 6 years ago . What are the properties of the function $f(x, y)$ that make it possible to be separated into a product of two one-variable functions, $g(x)$ and $h(y)$?","This question already has an answer here : Is that true that not every function $f(x,y)$ can be writen as $h(x) g(y)$? [closed] (1 answer) Closed 6 years ago . What are the properties of the function $f(x, y)$ that make it possible to be separated into a product of two one-variable functions, $g(x)$ and $h(y)$?",,"['calculus', 'multivariable-calculus', 'functions']"
99,"Using the Lagrange Multipliers Method to prove $\ |h(x,y)|\leq 1$",Using the Lagrange Multipliers Method to prove,"\ |h(x,y)|\leq 1","Using the Lagrange multipliers method, I have found that the  maximum and minimum values of the function    $$f(x,y)=xy$$   on the curve   $$x^2-yx+y^2=1$$ are   $$1 \ \text{at} \ (\pm1,\pm1) \ \ \ \text{and} \ \ -\frac{1}{3} \ \text{at} \ \big(\pm\frac{1}{\sqrt{3}},\mp\frac{1}{\sqrt{3}}\big) \ \ \text{respectively.}$$    Using this, prove that $$\Big|\frac{xy}{x^2-yx+y^2}\Big|\leq1 \ \ \ \forall(x,y)\neq0$$ I don't really know where to start. I thought of multiplying across as the inequality will be preserved, which yields $$|xy|\leq|x^2-yx+y^2|$$ then $$|xy|-|x^2-yx+y^2|\leq 0$$ At this point, I'm relying on my algebraic manipulation skills to yield something true. I'm not using the information above obtained by the Lagrange multipliers method.","Using the Lagrange multipliers method, I have found that the  maximum and minimum values of the function    $$f(x,y)=xy$$   on the curve   $$x^2-yx+y^2=1$$ are   $$1 \ \text{at} \ (\pm1,\pm1) \ \ \ \text{and} \ \ -\frac{1}{3} \ \text{at} \ \big(\pm\frac{1}{\sqrt{3}},\mp\frac{1}{\sqrt{3}}\big) \ \ \text{respectively.}$$    Using this, prove that $$\Big|\frac{xy}{x^2-yx+y^2}\Big|\leq1 \ \ \ \forall(x,y)\neq0$$ I don't really know where to start. I thought of multiplying across as the inequality will be preserved, which yields $$|xy|\leq|x^2-yx+y^2|$$ then $$|xy|-|x^2-yx+y^2|\leq 0$$ At this point, I'm relying on my algebraic manipulation skills to yield something true. I'm not using the information above obtained by the Lagrange multipliers method.",,"['multivariable-calculus', 'inequality']"
