,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is the series $\sum \sin^n(n)$ divergent?,Is the series  divergent?,\sum \sin^n(n),"I'm almost sure that the series $\sum \sin^n(n)$ is not convergent, but lack proof. Thank for any help.","I'm almost sure that the series $\sum \sin^n(n)$ is not convergent, but lack proof. Thank for any help.",,['real-analysis']
1,What does recursive cosine sequence converge to?,What does recursive cosine sequence converge to?,,"I have a sequence defined as follow: $a_0 = 1, a_n=\cos\left(a_{n-1}\right)$. I want to count $\lim_{n\rightarrow\infty} a_n$ - it definitely does have limit by looking at the graph, the first few numbers of the limit are 0.7390851, but I have no idea, if that number is related to some other real number ($\pi$ or something like that). The sequence is from this site , but they don't provide actual result for their own sequence.","I have a sequence defined as follow: $a_0 = 1, a_n=\cos\left(a_{n-1}\right)$. I want to count $\lim_{n\rightarrow\infty} a_n$ - it definitely does have limit by looking at the graph, the first few numbers of the limit are 0.7390851, but I have no idea, if that number is related to some other real number ($\pi$ or something like that). The sequence is from this site , but they don't provide actual result for their own sequence.",,['real-analysis']
2,"If $f(x)$ is smooth and odd, must $f(x)/x$ be smooth?","If  is smooth and odd, must  be smooth?",f(x) f(x)/x,"Let $f:\mathbb R\to\mathbb R$ be: smooth, i.e., infinitely differentiable, and odd, i.e., $f(x)=-f(-x)$ for all $x$ . Let $g:\mathbb R\to\mathbb R$ be defined as $$g(x):=\left\{  \matrix{f(x)/x, & x\neq 0 \\ \lim_{x\to 0} f(x)/x, & x=0} \right.$$ Must $g$ be smooth? In fact, is $g$ necessarily well defined at $x=0$ ? This is not a HW problem, by the way. I am have been working on a different problem and just realized I have been assuming the above fact without proof. I am aware of some theorems in complex analysis that make this true for complex-analytic functions, but I also know that real analysis isn't always so clean.","Let be: smooth, i.e., infinitely differentiable, and odd, i.e., for all . Let be defined as Must be smooth? In fact, is necessarily well defined at ? This is not a HW problem, by the way. I am have been working on a different problem and just realized I have been assuming the above fact without proof. I am aware of some theorems in complex analysis that make this true for complex-analytic functions, but I also know that real analysis isn't always so clean.","f:\mathbb R\to\mathbb R f(x)=-f(-x) x g:\mathbb R\to\mathbb R g(x):=\left\{ 
\matrix{f(x)/x, & x\neq 0 \\
\lim_{x\to 0} f(x)/x, & x=0}
\right. g g x=0","['real-analysis', 'smooth-functions']"
3,Issue with Spivak's Solution,Issue with Spivak's Solution,,Here was the problem: Here is the solution from his solutions book: This is barely a proof.  How can he just say let $f(c) = 0$? How do you prove that $f(c) =0$ and how do you prove that $f(d) = 0$? How can I use the IVT to prove that criterion? Thanks!,Here was the problem: Here is the solution from his solutions book: This is barely a proof.  How can he just say let $f(c) = 0$? How do you prove that $f(c) =0$ and how do you prove that $f(d) = 0$? How can I use the IVT to prove that criterion? Thanks!,,"['calculus', 'real-analysis', 'analysis', 'continuity']"
4,Negating the Definition of a Convergent Sequence to Find the Definition of a Divergent Sequence,Negating the Definition of a Convergent Sequence to Find the Definition of a Divergent Sequence,,"My task is to write a precise mathematical statement that ""the sequence $(a_n)$ does not converge to a number $\mathscr l$"" So, I have my definition of a convergent sequence: ""$\forall\varepsilon>0$ $\exists N\in\Bbb R$ such that $|x_n -\mathscr l|<\varepsilon$ $\forall n \in \Bbb N$ with $n>N$"" Would the correct negation of this be ""$\forall\varepsilon>0$ $\exists N\in\Bbb R$ such that $|x_n -\mathscr l|>\varepsilon$ $\forall n \in \Bbb N$ with $n>N$""? It doesn't seem that this is the answer as the next part of my task is to prove that a sequence is divergent using my formed proof, but it'd be difficult to do since it's a general proof of divergence and not just a proof that $(a_n)$ doesn't converge a specific number $\mathscr l$ Perhaps I should find a prove that $(a_n)$ tends to $\pm\infty$? This is more simple but it does not include monotone sequences such as $x_n:=(-1)^n$. Can someone assist me with this task? All comments and answers are appreciated.","My task is to write a precise mathematical statement that ""the sequence $(a_n)$ does not converge to a number $\mathscr l$"" So, I have my definition of a convergent sequence: ""$\forall\varepsilon>0$ $\exists N\in\Bbb R$ such that $|x_n -\mathscr l|<\varepsilon$ $\forall n \in \Bbb N$ with $n>N$"" Would the correct negation of this be ""$\forall\varepsilon>0$ $\exists N\in\Bbb R$ such that $|x_n -\mathscr l|>\varepsilon$ $\forall n \in \Bbb N$ with $n>N$""? It doesn't seem that this is the answer as the next part of my task is to prove that a sequence is divergent using my formed proof, but it'd be difficult to do since it's a general proof of divergence and not just a proof that $(a_n)$ doesn't converge a specific number $\mathscr l$ Perhaps I should find a prove that $(a_n)$ tends to $\pm\infty$? This is more simple but it does not include monotone sequences such as $x_n:=(-1)^n$. Can someone assist me with this task? All comments and answers are appreciated.",,['real-analysis']
5,Continuity of $L^1$ functions with respect to translation,Continuity of  functions with respect to translation,L^1,"Let $f\in L^1$, consider the map $t\mapsto f_t=f(x-t)$, then how can one show that $t\mapsto f_t$ is continuous? More explicitly one wants to show that $\lim_{h\to 0}|f_{t+h}-f_t|_{L^1}=0$. I tried to use approximation by $C_0(\mathbb{R})$ functions $g^n$ to approximate $f$ in $L^1$ norm. Then one has $\lim_{h\to 0}|g^n_{t+h}-g^n_t|_{L^1}=0$, but then I came across the problem: how can one show that the two limits can exchange so that one has $$\lim_{h\to 0}|f_{t+h}-f_t|_{L^1}=\lim_{h\to 0}\lim_{n\to\infty}|g^n_{t+h}-g^n_t|_{L^1}=\lim_{n\to\infty}\lim_{h\to 0}|g^n_{t+h}-g^n_t|_{L^1}=0.$$ Can someone help me with some conditions on which two limits can be exchanged, or do you have a better way of proving the continuity? Thank you!","Let $f\in L^1$, consider the map $t\mapsto f_t=f(x-t)$, then how can one show that $t\mapsto f_t$ is continuous? More explicitly one wants to show that $\lim_{h\to 0}|f_{t+h}-f_t|_{L^1}=0$. I tried to use approximation by $C_0(\mathbb{R})$ functions $g^n$ to approximate $f$ in $L^1$ norm. Then one has $\lim_{h\to 0}|g^n_{t+h}-g^n_t|_{L^1}=0$, but then I came across the problem: how can one show that the two limits can exchange so that one has $$\lim_{h\to 0}|f_{t+h}-f_t|_{L^1}=\lim_{h\to 0}\lim_{n\to\infty}|g^n_{t+h}-g^n_t|_{L^1}=\lim_{n\to\infty}\lim_{h\to 0}|g^n_{t+h}-g^n_t|_{L^1}=0.$$ Can someone help me with some conditions on which two limits can be exchanged, or do you have a better way of proving the continuity? Thank you!",,"['real-analysis', 'measure-theory', 'limits', 'continuity', 'lebesgue-integral']"
6,Proving that $x - \frac{x^3}{3!} < \sin x < x$ for all $x>0$,Proving that  for all,x - \frac{x^3}{3!} < \sin x < x x>0,Prove that $x - \frac{x^3}{3!} < \sin(x) < x$ for all $x>0$ This should be fairly straightforward but the proof seems to be alluding me. I want to show $x - \frac{x^3}{3!} < \sin(x) < x$ for all $x>0$.  I recognize this shouldn't be too difficult but perhaps finals have fried my brain.,Prove that $x - \frac{x^3}{3!} < \sin(x) < x$ for all $x>0$ This should be fairly straightforward but the proof seems to be alluding me. I want to show $x - \frac{x^3}{3!} < \sin(x) < x$ for all $x>0$.  I recognize this shouldn't be too difficult but perhaps finals have fried my brain.,,"['calculus', 'real-analysis', 'algebra-precalculus', 'trigonometry', 'inequality']"
7,a sequence $\{s_n\}$ with $\sum s_n$ convergent,a sequence  with  convergent,\{s_n\} \sum s_n,what would be $\{s_n\ge0\}$  such that  $$\sum_{n=1}^\infty s_n$$ converges but $$\lim_{n\to\infty}(n s_n) \neq 0$$,what would be $\{s_n\ge0\}$  such that  $$\sum_{n=1}^\infty s_n$$ converges but $$\lim_{n\to\infty}(n s_n) \neq 0$$,,"['real-analysis', 'sequences-and-series']"
8,Prove that: $\int_{0}^{1} \frac{x^{4}\log x}{x^2-1}\le \frac{1}{8}$,Prove that:,\int_{0}^{1} \frac{x^{4}\log x}{x^2-1}\le \frac{1}{8},"Here is another interesting integral inequality : $$\int_{0}^{1} \frac{x^{4}\log x}{x^2-1}\le \frac{1}{8}$$ According to W|A the difference between RS and LS is extremely small, namely 0.00241056. I don't know what would work here since the difference is so small.","Here is another interesting integral inequality : $$\int_{0}^{1} \frac{x^{4}\log x}{x^2-1}\le \frac{1}{8}$$ According to W|A the difference between RS and LS is extremely small, namely 0.00241056. I don't know what would work here since the difference is so small.",,"['real-analysis', 'integration', 'inequality']"
9,"Proof of a simple property of real, constant functions.","Proof of a simple property of real, constant functions.",,"I recently came across the following theorem: $$ \forall x_1, x_2 \in \mathbb{R},\textrm{function, } f: \mathbb{R} \rightarrow \mathbb{R}, x \mapsto y; \ |f(x_1) - f(x_2)| \leq (x_1-x_2)^2 \implies f \textrm{ is constant.}\ \mathbf{(1)} $$ I've been trying for some time, but the proof of $\mathbf{(1)}$ remains as elusive as ever. I've made two major attempts, the second of which I'll outline here. Though, I would be glad to detail the first as well if requested, I won't now since I think it's mostly wrong. But for the second, this is what I have so far: If, $\forall x_1,\ x_2,\ |f(x_1) - f(x_2)| \leq (x_1-x_2)^2$, then $f$ is continuous. This is so as $f$ is defined for all reals, $(\forall x \in \mathbb{R})\ f$ has finite limits, and each of those limits equals $f(x)$. Assume $f$ wasn't constant, then $\exists x_1,\ x_2 \ni x_1 \neq x_2 \implies |f(x_1)- f(x_2)| > 0$. Since $f$ is continuous, there exist an infinity of such pairs, $x_1$ and  $x_2$.  For all such $x_1$ and $x_2$, we may construct a set, $S$, consitsting of $f(x_1)$ and $f(x_2)$ (not as pairs); since f is defined for all $x,\ S$ is ""absolutely"" bounded and as such has a least upper bound and and greatest lower bound, which we will denote as $\alpha_1\ = f(a_1)$ and $\alpha_2 = f(a_2)$ respectively. To show $f$ is constant, it will suffice to show that $\alpha_1 = \alpha_2$. Does anyone see how the proof could be completed? Or even, do you think there might be a better approach? Thank you all in advance.","I recently came across the following theorem: $$ \forall x_1, x_2 \in \mathbb{R},\textrm{function, } f: \mathbb{R} \rightarrow \mathbb{R}, x \mapsto y; \ |f(x_1) - f(x_2)| \leq (x_1-x_2)^2 \implies f \textrm{ is constant.}\ \mathbf{(1)} $$ I've been trying for some time, but the proof of $\mathbf{(1)}$ remains as elusive as ever. I've made two major attempts, the second of which I'll outline here. Though, I would be glad to detail the first as well if requested, I won't now since I think it's mostly wrong. But for the second, this is what I have so far: If, $\forall x_1,\ x_2,\ |f(x_1) - f(x_2)| \leq (x_1-x_2)^2$, then $f$ is continuous. This is so as $f$ is defined for all reals, $(\forall x \in \mathbb{R})\ f$ has finite limits, and each of those limits equals $f(x)$. Assume $f$ wasn't constant, then $\exists x_1,\ x_2 \ni x_1 \neq x_2 \implies |f(x_1)- f(x_2)| > 0$. Since $f$ is continuous, there exist an infinity of such pairs, $x_1$ and  $x_2$.  For all such $x_1$ and $x_2$, we may construct a set, $S$, consitsting of $f(x_1)$ and $f(x_2)$ (not as pairs); since f is defined for all $x,\ S$ is ""absolutely"" bounded and as such has a least upper bound and and greatest lower bound, which we will denote as $\alpha_1\ = f(a_1)$ and $\alpha_2 = f(a_2)$ respectively. To show $f$ is constant, it will suffice to show that $\alpha_1 = \alpha_2$. Does anyone see how the proof could be completed? Or even, do you think there might be a better approach? Thank you all in advance.",,"['real-analysis', 'functions']"
10,Weak limit of an $L^1$ sequence,Weak limit of an  sequence,L^1,"We have functions $f_n\in L^1$ such that $\int f_ng$ has a limit for every $g\in L^\infty$. Does there exist a function $f\in L^1$ such that the limit equals $\int fg$? I think this is not true in general (really? - why?), then can this be true if we also know that $f_n$ belong to a certain subspace of $L^1$?","We have functions $f_n\in L^1$ such that $\int f_ng$ has a limit for every $g\in L^\infty$. Does there exist a function $f\in L^1$ such that the limit equals $\int fg$? I think this is not true in general (really? - why?), then can this be true if we also know that $f_n$ belong to a certain subspace of $L^1$?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'banach-spaces']"
11,The sum of infinitely many $c$s is $c$ implies $c = 0$.,The sum of infinitely many s is  implies .,c c c = 0,"This seems like an obvious claim, but I would like to be able to prove this rigorously. Suppose I have $c \in \mathbb{R}$ satisfying $$\lim_{n \to \infty}\sum_{i=1}^{n}c = c\text{.}$$ How does it follow that $c = 0$? What I think I shouldn't do : $$c\lim_{n \to \infty}n = c$$ because this gives $0 \cdot \infty = 0$ (is this true?). To understand the context of this question, I am working with a probability measure $\mathbb{P}$ and am trying to show that $\mathbb{P}\left(\emptyset\right) = 0$ using the definition (and in this case, $\mathbb{P}\left(\emptyset\right) = c$). Maybe infinite sums are defined differently in measure theory? I don't know.","This seems like an obvious claim, but I would like to be able to prove this rigorously. Suppose I have $c \in \mathbb{R}$ satisfying $$\lim_{n \to \infty}\sum_{i=1}^{n}c = c\text{.}$$ How does it follow that $c = 0$? What I think I shouldn't do : $$c\lim_{n \to \infty}n = c$$ because this gives $0 \cdot \infty = 0$ (is this true?). To understand the context of this question, I am working with a probability measure $\mathbb{P}$ and am trying to show that $\mathbb{P}\left(\emptyset\right) = 0$ using the definition (and in this case, $\mathbb{P}\left(\emptyset\right) = c$). Maybe infinite sums are defined differently in measure theory? I don't know.",,"['real-analysis', 'algebra-precalculus', 'probability-theory', 'measure-theory']"
12,Does there exist a continuous onto function from $\mathbb{R}-\mathbb{Q}$ to $\mathbb{Q}$?,Does there exist a continuous onto function from  to ?,\mathbb{R}-\mathbb{Q} \mathbb{Q},"Does there exist a continuous onto function from $\mathbb{R}-\mathbb{Q}$ to  $\mathbb{Q}$? (where domain is all irrational numbers) I found many answers for contradicting the fact that there doesnt exist a continuous function which maps rationals to irrationals and vice versa. But proving that thing was easier since our domain of definition of function was a connected set, we could use that connectedness or we could use the fact that rationals are countable and irrationals are uncountable. But in this case those properties are not useful. I somehow think that baire category theorem might be useful but I am not good at using it.","Does there exist a continuous onto function from $\mathbb{R}-\mathbb{Q}$ to  $\mathbb{Q}$? (where domain is all irrational numbers) I found many answers for contradicting the fact that there doesnt exist a continuous function which maps rationals to irrationals and vice versa. But proving that thing was easier since our domain of definition of function was a connected set, we could use that connectedness or we could use the fact that rationals are countable and irrationals are uncountable. But in this case those properties are not useful. I somehow think that baire category theorem might be useful but I am not good at using it.",,['real-analysis']
13,Average Frequency of 1's in an Infinite Binary Sequences Can be Anything Between 0 and 1,Average Frequency of 1's in an Infinite Binary Sequences Can be Anything Between 0 and 1,,"The following question came up when me and a friend of mine were discussing some basic things about probability: Let $p$ be a real number in $[0,1]$. Does there exist a sequence $(x_1, x_2, x_3, \ldots)$ with each $x_i$ being either $0$ or $1$, such that $$ \lim_{n\to \infty} \frac{f(n)}{n} =p $$ where $f(n)= x_1+x_2+\cdots+x_n$, that is, $f(n)$ is the number of times $1$ has appeared in the first $n$ slots. Motivation: Consider a coin which may or may not be fair, and say the probability of ""heads"" showing up is $p$. Suppose we want to have a machine which simulates this coin. That is, we want to have a machine which shows either ""H"" or ""T"" every second ('second' here is a unit of time) on its screen. If the machine properly simulates the coin, then we must have the average frequency of ""H"" occuring is $p$.","The following question came up when me and a friend of mine were discussing some basic things about probability: Let $p$ be a real number in $[0,1]$. Does there exist a sequence $(x_1, x_2, x_3, \ldots)$ with each $x_i$ being either $0$ or $1$, such that $$ \lim_{n\to \infty} \frac{f(n)}{n} =p $$ where $f(n)= x_1+x_2+\cdots+x_n$, that is, $f(n)$ is the number of times $1$ has appeared in the first $n$ slots. Motivation: Consider a coin which may or may not be fair, and say the probability of ""heads"" showing up is $p$. Suppose we want to have a machine which simulates this coin. That is, we want to have a machine which shows either ""H"" or ""T"" every second ('second' here is a unit of time) on its screen. If the machine properly simulates the coin, then we must have the average frequency of ""H"" occuring is $p$.",,"['real-analysis', 'probability', 'sequences-and-series', 'probability-theory']"
14,If monotone decreasing and $\int_0^\infty f(x)dx <\infty$ then $\lim\limits_{x\to\infty} xf(x)=0.$,If monotone decreasing and  then,\int_0^\infty f(x)dx <\infty \lim\limits_{x\to\infty} xf(x)=0.,Let $f:\mathbb{R}_+ \to \mathbb{R}_+$ be a monotone decreasing function defined on the positive real numbers with $$\int_0^\infty f(x)dx <\infty.$$ Show that $$\lim_{x\to\infty} xf(x)=0.$$ This is my proof: Suppose not. Then there is $\varepsilon$  such that for any $M>0$ there exists $x\geq M$ such that $xf(x)\geq \varepsilon$. So we can construct a sequence $(x_n)$ such that $x_n \to \infty $ and $x_n f(x_n ) \geq \varepsilon$. So $$\frac{\varepsilon}{x_n}\leq f(x_n) \implies \sum_{n\in\mathbb{N}}\frac{\varepsilon}{x_n} \leq  \sum_{n\in\mathbb{N}} f(x_n) \leq \int_0^1 f(x)dx.$$ So we get a contradiction.  I feal like I have the correct idea but some details are wrong.  Any help would be appreciated.,Let $f:\mathbb{R}_+ \to \mathbb{R}_+$ be a monotone decreasing function defined on the positive real numbers with $$\int_0^\infty f(x)dx <\infty.$$ Show that $$\lim_{x\to\infty} xf(x)=0.$$ This is my proof: Suppose not. Then there is $\varepsilon$  such that for any $M>0$ there exists $x\geq M$ such that $xf(x)\geq \varepsilon$. So we can construct a sequence $(x_n)$ such that $x_n \to \infty $ and $x_n f(x_n ) \geq \varepsilon$. So $$\frac{\varepsilon}{x_n}\leq f(x_n) \implies \sum_{n\in\mathbb{N}}\frac{\varepsilon}{x_n} \leq  \sum_{n\in\mathbb{N}} f(x_n) \leq \int_0^1 f(x)dx.$$ So we get a contradiction.  I feal like I have the correct idea but some details are wrong.  Any help would be appreciated.,,['real-analysis']
15,What is the norm measuring in function spaces,What is the norm measuring in function spaces,,"In spatial euclidean vector spaces norm is an intuitive concept: It measures the distance from the null vector and from other vectors. The generalization to function spaces is quite a mental leap (at least for me). My question: Is there some kind of intuition what ""norm"" or ""euclidean norm"" is even supposed to ""mean"" here? What is the null vector and how what does a ""distance"" between to functions reveal? Edit : Please also see this follow up question: Visualization of 2-dimensional function spaces","In spatial euclidean vector spaces norm is an intuitive concept: It measures the distance from the null vector and from other vectors. The generalization to function spaces is quite a mental leap (at least for me). My question: Is there some kind of intuition what ""norm"" or ""euclidean norm"" is even supposed to ""mean"" here? What is the null vector and how what does a ""distance"" between to functions reveal? Edit : Please also see this follow up question: Visualization of 2-dimensional function spaces",,"['real-analysis', 'measure-theory', 'intuition', 'vector-spaces']"
16,Evaluating $\lim_{n\rightarrow\infty} (\frac{(1+\frac{1}{n})^n}{e})^n$,Evaluating,\lim_{n\rightarrow\infty} (\frac{(1+\frac{1}{n})^n}{e})^n,"We know that $\lim_{n\rightarrow\infty}(1+\frac{1}{n})^n=e$ and so I thought the approach to evaluating the limit in the question would be to just use this fact and substitute it into the numerator. This approach would tell us the above limit evaluates to $1$ . However, that does not seem to be the correct limiting value. In fact, it evaluates to $\frac {1}{\sqrt{e}}$ . Why is this so?","We know that and so I thought the approach to evaluating the limit in the question would be to just use this fact and substitute it into the numerator. This approach would tell us the above limit evaluates to . However, that does not seem to be the correct limiting value. In fact, it evaluates to . Why is this so?",\lim_{n\rightarrow\infty}(1+\frac{1}{n})^n=e 1 \frac {1}{\sqrt{e}},"['real-analysis', 'limits']"
17,Does every continuous map from $\mathbb{Q}$ to $\mathbb{Q}$ extends continuously as a map from $\mathbb{R}$ to $\mathbb{R}$?,Does every continuous map from  to  extends continuously as a map from  to ?,\mathbb{Q} \mathbb{Q} \mathbb{R} \mathbb{R},"Given a continuous function $f:\mathbb{Q}\to\mathbb{Q}$ ,does there exist a continuous function  $g:\mathbb{R}\to\mathbb{R}$ such that   $g|_{\Bbb Q} = f$? What I have no Idea about how to attempt this Question! Any suggestion will be very helpful.","Given a continuous function $f:\mathbb{Q}\to\mathbb{Q}$ ,does there exist a continuous function  $g:\mathbb{R}\to\mathbb{R}$ such that   $g|_{\Bbb Q} = f$? What I have no Idea about how to attempt this Question! Any suggestion will be very helpful.",,[]
18,Proof that determinant is continuous using $\epsilon-\delta $ definition,Proof that determinant is continuous using  definition,\epsilon-\delta ,"I need to prove that the determinant $\det: M(n, \mathbb{R}) \rightarrow \mathbb{R}$ is a continuous function given the euclidean metric on the vector space of all $n \times n$ matrices over $\mathbb{R}$, i.e. $\Vert M \Vert = \sqrt{\sum_{i,j=1}^n M_{i,j}^2}$. So what I need to prove, I think, is that there exists a $\delta > 0$ such that if $\Vert M - M' \Vert < \delta$, for any $M' \in M(n,\mathbb{R})$ then it follows that for all $\epsilon > 0: |\det(M) - \det(M')|< \epsilon$. Unfortunately I have no idea how to derive the correct inequalities between the given euclidean metric and a determinant, since I'm kind of struggling with the permutation part in the definition of a determinant. Can anybody help, please?","I need to prove that the determinant $\det: M(n, \mathbb{R}) \rightarrow \mathbb{R}$ is a continuous function given the euclidean metric on the vector space of all $n \times n$ matrices over $\mathbb{R}$, i.e. $\Vert M \Vert = \sqrt{\sum_{i,j=1}^n M_{i,j}^2}$. So what I need to prove, I think, is that there exists a $\delta > 0$ such that if $\Vert M - M' \Vert < \delta$, for any $M' \in M(n,\mathbb{R})$ then it follows that for all $\epsilon > 0: |\det(M) - \det(M')|< \epsilon$. Unfortunately I have no idea how to derive the correct inequalities between the given euclidean metric and a determinant, since I'm kind of struggling with the permutation part in the definition of a determinant. Can anybody help, please?",,"['real-analysis', 'linear-algebra', 'matrices', 'continuity', 'determinant']"
19,What is the limit $\lim_{n\to\infty}\frac{1^n+2^n+3^n+\dots+n^n}{n^{n+1}}$?,What is the limit ?,\lim_{n\to\infty}\frac{1^n+2^n+3^n+\dots+n^n}{n^{n+1}},"I want to determine the limit given below: $$\lim_{n\to\infty}\frac{1^n+2^n+3^n+\ldots+n^n}{n^{n+1}}$$ I have tried to solve thise several times ,but with no results.I have tried using lema stolz cezaro and managed to find a general formula for the summation, but couldnt prove it $$\frac{1}{n}\sum_{k=0}^{n-1}\left(1-\frac{k}{n}\right)^n$$ Edit: Thank you for your responses,but i want to ask you if it is possible to solve the problem by not using integrals :D. NOTE Im in the eleventh grade and i had been given this problem by my math tutor ,so, as a consequence i dont know how to interpretate the great majority of your answers.Thanks for your time ,gents!","I want to determine the limit given below: $$\lim_{n\to\infty}\frac{1^n+2^n+3^n+\ldots+n^n}{n^{n+1}}$$ I have tried to solve thise several times ,but with no results.I have tried using lema stolz cezaro and managed to find a general formula for the summation, but couldnt prove it $$\frac{1}{n}\sum_{k=0}^{n-1}\left(1-\frac{k}{n}\right)^n$$ Edit: Thank you for your responses,but i want to ask you if it is possible to solve the problem by not using integrals :D. NOTE Im in the eleventh grade and i had been given this problem by my math tutor ,so, as a consequence i dont know how to interpretate the great majority of your answers.Thanks for your time ,gents!",,"['real-analysis', 'analysis', 'limits', 'limits-without-lhopital']"
20,Mean value theorem and the axiom of choice,Mean value theorem and the axiom of choice,,"There's this theorem in Spivak's book of Calculus : Theorem 7 Suppose that $f$ is continuous at $a$, and that $f'(x)$ exists for all $x$ in some interval containing $a$, except perhaps for $x=a$. Suppose, moreover, that $\lim_{x \to a} f'(x)$ exists. Then $f'(a)$ also exists, and    $$f'(a) = \lim_{x \to a} f'(x)$$ Proof By definition,   $$f'(a) = \lim_{h \to 0} \frac{f(a+h)-f(a)}{h}$$   For sufficiently small $h>0$ the function $f$ will be continuous on $[a,a+h]$ and differentiable on $(a,a+h)$ (a similar assertion holds for sufficiently small $h<0$). By the Mean Value Theorem there is a number $\alpha_h$ in $(a,a+h)$ such that   $$\frac{f(a+h)-f(a)}{h} = f'(\alpha_h)$$   Now $\alpha_h$ approaches $a$ as $h$ approaches $0$, because $\alpha_h$ is in $(a,a+h)$; since $\lim_{x \to a} f'(x)$ exists, it follows that   $$f'(a) = \lim_{h \to 0} \frac{f(a+h)-f(a)}{h} = \lim_{h \to 0} f'(\alpha_h) = \lim_{x \to a} f'(x)$$   (It is a good idea to supply a rigorous $\epsilon$-$\delta$ argument for this final step, which we have treated somewhat informally.) $\blacksquare$ Following the recommendation of supplying the details I had the question of $\alpha_h$ being a choice function. This is because for every $h$ there is the possibility of having a lot of points $c$ such that $f'(c)=\frac{f(a+h)-f(a)}{h}$. Then we choose one of them to have a function $\alpha_h$. I'm not sure though because I'm always struggling with the axiom of choice that I cannot distinguish if it's needed or not. Also maybe in this case what the author is saying is something different. Can you guys please help me?","There's this theorem in Spivak's book of Calculus : Theorem 7 Suppose that $f$ is continuous at $a$, and that $f'(x)$ exists for all $x$ in some interval containing $a$, except perhaps for $x=a$. Suppose, moreover, that $\lim_{x \to a} f'(x)$ exists. Then $f'(a)$ also exists, and    $$f'(a) = \lim_{x \to a} f'(x)$$ Proof By definition,   $$f'(a) = \lim_{h \to 0} \frac{f(a+h)-f(a)}{h}$$   For sufficiently small $h>0$ the function $f$ will be continuous on $[a,a+h]$ and differentiable on $(a,a+h)$ (a similar assertion holds for sufficiently small $h<0$). By the Mean Value Theorem there is a number $\alpha_h$ in $(a,a+h)$ such that   $$\frac{f(a+h)-f(a)}{h} = f'(\alpha_h)$$   Now $\alpha_h$ approaches $a$ as $h$ approaches $0$, because $\alpha_h$ is in $(a,a+h)$; since $\lim_{x \to a} f'(x)$ exists, it follows that   $$f'(a) = \lim_{h \to 0} \frac{f(a+h)-f(a)}{h} = \lim_{h \to 0} f'(\alpha_h) = \lim_{x \to a} f'(x)$$   (It is a good idea to supply a rigorous $\epsilon$-$\delta$ argument for this final step, which we have treated somewhat informally.) $\blacksquare$ Following the recommendation of supplying the details I had the question of $\alpha_h$ being a choice function. This is because for every $h$ there is the possibility of having a lot of points $c$ such that $f'(c)=\frac{f(a+h)-f(a)}{h}$. Then we choose one of them to have a function $\alpha_h$. I'm not sure though because I'm always struggling with the axiom of choice that I cannot distinguish if it's needed or not. Also maybe in this case what the author is saying is something different. Can you guys please help me?",,"['real-analysis', 'set-theory', 'axiom-of-choice']"
21,Compute the limit of $\sqrt{1-a}\sum\limits_{n=0}^{+\infty} a^{n^2}$ when $a\to1^-$,Compute the limit of  when,\sqrt{1-a}\sum\limits_{n=0}^{+\infty} a^{n^2} a\to1^-,"I need some suggestions, hints for the limit when $a \to 1^{-}$ of $$\sqrt{\,1 - a\,}\,\sum_{n = 0}^{\infty}a^{n^{2}}.$$","I need some suggestions, hints for the limit when $a \to 1^{-}$ of $$\sqrt{\,1 - a\,}\,\sum_{n = 0}^{\infty}a^{n^{2}}.$$",,"['calculus', 'real-analysis', 'limits']"
22,convergence of $\sum \limits_{n=1}^{\infty }\bigl\{ \frac {1\cdot3 \cdots (2n-1)} {2\cdot 4\cdots (2n)}\cdot \frac {4n+3} {2n+2}\bigr\} ^{2}$,convergence of,\sum \limits_{n=1}^{\infty }\bigl\{ \frac {1\cdot3 \cdots (2n-1)} {2\cdot 4\cdots (2n)}\cdot \frac {4n+3} {2n+2}\bigr\} ^{2},"I am investigating the convergence of $$\begin{split}\sum _{n=1}^{\infty }\left\{ \dfrac {1\cdot 3\cdots (2n-1)} {2\cdot 4\cdots (2n)}\cdot \dfrac {4n+3} {2n+2}\right\} ^{2} &= \sum _{n=1}^{\infty }\left\{ \dfrac {\prod _{t=1}^n (2t-1)} {\prod _{t=1}^n (2t)}\cdot \dfrac {4n+3} {2n+2}\right\} ^{2} \\ &=\sum _{n=1}^{\infty }\left\{ \prod _{t=1}^n\left( 1-\dfrac {1} {2t}\right) \dfrac {4n+3} {2n+2}\right\} ^{2} \end{split}$$ which after some manipulations I have reduced to $$\sum _{n=1}^{\infty }e^ \left\{ 2\ln \left(2 -\dfrac {1} {2n+2}\right) +2\cdot \sum _{t=1}^{n}\ln \left( 1-\dfrac {1}{2t}\right) \right\} $$ and from an alternative approach I was able to reduce it to $$\sum _{n=1}^{\infty } \dfrac{\left( 4n+3\right) ^{2}}{4\left(n+1\right)^{2}} \prod _{t=1}^n\left( 2+\dfrac{1}{2t^{2}}-\dfrac{2}{t}\right)$$  I am unsure how to proceed from here in either of the two cases. Any help would be much appreciated.","I am investigating the convergence of $$\begin{split}\sum _{n=1}^{\infty }\left\{ \dfrac {1\cdot 3\cdots (2n-1)} {2\cdot 4\cdots (2n)}\cdot \dfrac {4n+3} {2n+2}\right\} ^{2} &= \sum _{n=1}^{\infty }\left\{ \dfrac {\prod _{t=1}^n (2t-1)} {\prod _{t=1}^n (2t)}\cdot \dfrac {4n+3} {2n+2}\right\} ^{2} \\ &=\sum _{n=1}^{\infty }\left\{ \prod _{t=1}^n\left( 1-\dfrac {1} {2t}\right) \dfrac {4n+3} {2n+2}\right\} ^{2} \end{split}$$ which after some manipulations I have reduced to $$\sum _{n=1}^{\infty }e^ \left\{ 2\ln \left(2 -\dfrac {1} {2n+2}\right) +2\cdot \sum _{t=1}^{n}\ln \left( 1-\dfrac {1}{2t}\right) \right\} $$ and from an alternative approach I was able to reduce it to $$\sum _{n=1}^{\infty } \dfrac{\left( 4n+3\right) ^{2}}{4\left(n+1\right)^{2}} \prod _{t=1}^n\left( 2+\dfrac{1}{2t^{2}}-\dfrac{2}{t}\right)$$  I am unsure how to proceed from here in either of the two cases. Any help would be much appreciated.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
23,An application of Jensen's Inequality,An application of Jensen's Inequality,,"Given that $\{\phi_n\}$ is a sequence of non-negative numbers whose sum is $1$ and $\{\psi_n\}$ is a sequence of positive numbers, how can I show that  $$ \prod_{n=1}^{\infty}~\psi_n^{\phi_n}~\leq~\sum_{n=1}^{\infty}~\phi_n\psi_n~? $$ Thanks. PS: I'm not too sure about the title. Perhaps, someone could give it a better title?","Given that $\{\phi_n\}$ is a sequence of non-negative numbers whose sum is $1$ and $\{\psi_n\}$ is a sequence of positive numbers, how can I show that  $$ \prod_{n=1}^{\infty}~\psi_n^{\phi_n}~\leq~\sum_{n=1}^{\infty}~\phi_n\psi_n~? $$ Thanks. PS: I'm not too sure about the title. Perhaps, someone could give it a better title?",,"['real-analysis', 'inequality']"
24,Link between a Dense subset and a Continuous mapping,Link between a Dense subset and a Continuous mapping,,"arising out of comment made by Yuval Filmus in what is the cardinality of set of all smooth functions in $L^1$? I got this idea (forgive me for my ignorance for if it is nothing but an elementary definition/result in real analysis). The idea is like this Let $f:X\to Y$ is a mapping, where $X$ is a complete metric space (not sure if its strictly needed or whether a looser condition would do). If $f$ is a continuous mapping then $f$ is uniquely specified by a mapping $g:E\to Y$ where $E$ is a dense subset of $X$. What are the condition under which it is valid ? also  the validity of the converse statement.","arising out of comment made by Yuval Filmus in what is the cardinality of set of all smooth functions in $L^1$? I got this idea (forgive me for my ignorance for if it is nothing but an elementary definition/result in real analysis). The idea is like this Let $f:X\to Y$ is a mapping, where $X$ is a complete metric space (not sure if its strictly needed or whether a looser condition would do). If $f$ is a continuous mapping then $f$ is uniquely specified by a mapping $g:E\to Y$ where $E$ is a dense subset of $X$. What are the condition under which it is valid ? also  the validity of the converse statement.",,"['real-analysis', 'general-topology']"
25,Prove $\int_0^1\frac{\ln x\ln(1+x)}{1-x}\ dx=\zeta(3)-\frac32\ln2\zeta(2)$,Prove,\int_0^1\frac{\ln x\ln(1+x)}{1-x}\ dx=\zeta(3)-\frac32\ln2\zeta(2),How to prove without using Euler sums that $$I=\int_0^1\frac{\ln x\ln(1+x)}{1-x}\ dx=\zeta(3)-\frac32\ln2\zeta(2)$$ where $\zeta$ is the Riemann zeta function. We can relate this integral to some Euler sum as follows: \begin{align} I&=-\sum_{n=1}^\infty\frac{(-1)^n}{n}\int_0^1\frac{x^n\ln x}{1-x}\ dx\\ &=-\sum_{n=1}^\infty\frac{(-1)^n}{n}(H_n^{(2)}-\zeta(2))\\ &=-\sum_{n=1}^\infty\frac{(-1)^nH_n^{(2)}}{n}-\ln2\zeta(2) \end{align} Also the integral $I$ can be related to $\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^2}$ . So I am looking for a different way to evaluate $I$ besides using these two sums.,How to prove without using Euler sums that where is the Riemann zeta function. We can relate this integral to some Euler sum as follows: Also the integral can be related to . So I am looking for a different way to evaluate besides using these two sums.,"I=\int_0^1\frac{\ln x\ln(1+x)}{1-x}\ dx=\zeta(3)-\frac32\ln2\zeta(2) \zeta \begin{align}
I&=-\sum_{n=1}^\infty\frac{(-1)^n}{n}\int_0^1\frac{x^n\ln x}{1-x}\ dx\\
&=-\sum_{n=1}^\infty\frac{(-1)^n}{n}(H_n^{(2)}-\zeta(2))\\
&=-\sum_{n=1}^\infty\frac{(-1)^nH_n^{(2)}}{n}-\ln2\zeta(2)
\end{align} I \sum_{n=1}^\infty\frac{(-1)^nH_n}{n^2} I","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'riemann-zeta']"
26,Proving $\int_0^\pi \frac{\log(1+x\cos (y))}{\cos y}dy=\pi \arcsin x$,Proving,\int_0^\pi \frac{\log(1+x\cos (y))}{\cos y}dy=\pi \arcsin x,"How would I go about proving that, $$\int_0^\pi \frac{\log(1+x\cos (y))}{\cos (y)}\,dy=\pi \arcsin (x)$$ I have tried to do this by computing the integral directly, but it appears to be too difficult. Maybe there is a better approach to this that I do not know of.","How would I go about proving that, I have tried to do this by computing the integral directly, but it appears to be too difficult. Maybe there is a better approach to this that I do not know of.","\int_0^\pi \frac{\log(1+x\cos (y))}{\cos (y)}\,dy=\pi \arcsin (x)","['calculus', 'real-analysis', 'integration']"
27,Proof that $\frac{1 + \sqrt{5}}{2}$ is irrational.,Proof that  is irrational.,\frac{1 + \sqrt{5}}{2},"I'm trying to prove that $\frac{1 + \sqrt{5}}{2}$, the ""Golden ratio,"" is irrational. I have only been able to do so thus far by taking as given (which I haven't had any trouble proving) exercise 1.1. in Rudin: Lemma. (Taken as proved) If $r \neq 0$ is rational and $x$ is irrational, then $r + x$ is irrational and $rx$ is irrational. I'd be very interested if someone knows of an alternate approach. Since the proof I have written was far from elegant, I'd appreciate any critiques on it as well. Here's what I came up with: Proof. Let's first establish that $\sqrt{5}$ is irrational. Assume to the contrary that $\sqrt{5} \in \mathbb{Q}$. Then, $\exists m, n \in \mathbb{Z}, \left(\sqrt{5} = \frac{m}{n} \wedge n \neq 0\right)$. Without loss of generality, let $m$ and $n$ be coprime. Squaring both sides and algebraically rearranging this equation yields  \begin{align} 5n^2 = m^2, \end{align}  in which case $5 \mid m^2$. We can therefore deduce that $5 \mid m$. (Side note: the only explanation I was able to come up with, which I will leave as a conceptual argument for the moment, is the fundamental theorem of arithmetic: if we attempt to establish the contrapositive, that $5$ does not divide $m$ means that for all $p_i$ in the prime factorization of $m$, $p_i \neq 5$. In squaring $m$, we double the exponents, but $5$ is still not a factor, and because it's prime it can't be generated from any of the other factors. So, $5$ also doesn't divide $m^2$. This is far from as elegant as establishing, say, that if $m$ is odd, $m^2$ is also odd. If there is a better way to establish this fact, though, I'd be very interested in hearing it.) So, since $5 \mid m$, we can write $m = 5a$ for some $a \in \mathbb{Z}$. Substituting into our equation gives \begin{align} 5n^2 = (5a)^2 = 25a^2, \end{align} and simplifying gives  \begin{align} n^2 = 5a^2, \end{align} so $5 \mid n^2$ and thus $5 \mid n$, a contradiction, as we assumed $m$ and $n$ were coprime. Thus, $5$ is irrational. From here, since $1 \in \mathbb{Q} - \{0\}$, we can use the fact that $r + x$ is irrational with $r = 1$ and $x = \sqrt{5}$ to deduce that $1 + \sqrt{5}$ is irrational. Similarly, using the fact that $rx$ is irrational, we can set $x = 1 + \sqrt{5}$ and $r = \frac{1}{2}$ to deduce that $rx = \frac{1 + \sqrt{5}}{2}$ is irrational, which is our goal. How does this look? I'd be very interested in any critiques of this or alternate methods of proof. Thanks.","I'm trying to prove that $\frac{1 + \sqrt{5}}{2}$, the ""Golden ratio,"" is irrational. I have only been able to do so thus far by taking as given (which I haven't had any trouble proving) exercise 1.1. in Rudin: Lemma. (Taken as proved) If $r \neq 0$ is rational and $x$ is irrational, then $r + x$ is irrational and $rx$ is irrational. I'd be very interested if someone knows of an alternate approach. Since the proof I have written was far from elegant, I'd appreciate any critiques on it as well. Here's what I came up with: Proof. Let's first establish that $\sqrt{5}$ is irrational. Assume to the contrary that $\sqrt{5} \in \mathbb{Q}$. Then, $\exists m, n \in \mathbb{Z}, \left(\sqrt{5} = \frac{m}{n} \wedge n \neq 0\right)$. Without loss of generality, let $m$ and $n$ be coprime. Squaring both sides and algebraically rearranging this equation yields  \begin{align} 5n^2 = m^2, \end{align}  in which case $5 \mid m^2$. We can therefore deduce that $5 \mid m$. (Side note: the only explanation I was able to come up with, which I will leave as a conceptual argument for the moment, is the fundamental theorem of arithmetic: if we attempt to establish the contrapositive, that $5$ does not divide $m$ means that for all $p_i$ in the prime factorization of $m$, $p_i \neq 5$. In squaring $m$, we double the exponents, but $5$ is still not a factor, and because it's prime it can't be generated from any of the other factors. So, $5$ also doesn't divide $m^2$. This is far from as elegant as establishing, say, that if $m$ is odd, $m^2$ is also odd. If there is a better way to establish this fact, though, I'd be very interested in hearing it.) So, since $5 \mid m$, we can write $m = 5a$ for some $a \in \mathbb{Z}$. Substituting into our equation gives \begin{align} 5n^2 = (5a)^2 = 25a^2, \end{align} and simplifying gives  \begin{align} n^2 = 5a^2, \end{align} so $5 \mid n^2$ and thus $5 \mid n$, a contradiction, as we assumed $m$ and $n$ were coprime. Thus, $5$ is irrational. From here, since $1 \in \mathbb{Q} - \{0\}$, we can use the fact that $r + x$ is irrational with $r = 1$ and $x = \sqrt{5}$ to deduce that $1 + \sqrt{5}$ is irrational. Similarly, using the fact that $rx$ is irrational, we can set $x = 1 + \sqrt{5}$ and $r = \frac{1}{2}$ to deduce that $rx = \frac{1 + \sqrt{5}}{2}$ is irrational, which is our goal. How does this look? I'd be very interested in any critiques of this or alternate methods of proof. Thanks.",,['real-analysis']
28,"How to find: $~\min\limits_{f\in E}(\int_0^1f(x) \,dx)$",How to find:,"~\min\limits_{f\in E}(\int_0^1f(x) \,dx)","I came across to the following  problem: Let $E$ be the set of all continuous function $f:[0,1]\to \mathbb{R}$ such that $$f(x)+f(y)\ge |x-y|\qquad\forall\,x,y\in [0,1]$$ Then find $$\min_{f\in E}\left(\int_0^1f(x) dx\right)$$ My attempt: I took the double integral on both side which yields $$ 2\int_0^1f(x)dx =\int_0^1\int_0^1f(x) +f(y)dydx \ge \int_0^1\int_0^1|x-y|dxdy =\frac{1}{3} $$ Thus,  $$~\min\limits_{f\in E}(\int_0^1f(x) \,dx) \ge \frac{1}{6}$$ Unfortunately I don't know How to get the minimizer. Please give help me with a hint or an answer. Minimize $\min_{f\in E}\left(\int_0^1f(x) dx\right)$","I came across to the following  problem: Let $E$ be the set of all continuous function $f:[0,1]\to \mathbb{R}$ such that $$f(x)+f(y)\ge |x-y|\qquad\forall\,x,y\in [0,1]$$ Then find $$\min_{f\in E}\left(\int_0^1f(x) dx\right)$$ My attempt: I took the double integral on both side which yields $$ 2\int_0^1f(x)dx =\int_0^1\int_0^1f(x) +f(y)dydx \ge \int_0^1\int_0^1|x-y|dxdy =\frac{1}{3} $$ Thus,  $$~\min\limits_{f\in E}(\int_0^1f(x) \,dx) \ge \frac{1}{6}$$ Unfortunately I don't know How to get the minimizer. Please give help me with a hint or an answer. Minimize $\min_{f\in E}\left(\int_0^1f(x) dx\right)$",,"['calculus', 'real-analysis', 'optimization', 'contest-math']"
29,"In general, why is the product topology not equal to the box topology","In general, why is the product topology not equal to the box topology",,"I am trying to understand a counter-example showing that the box topology and product topology are not equal. Here it is: Let $\tau$ and $\tau'$ be the product and box topologies respectively. Let $X_i = \mathbb{R}\ \forall i$ and let $U_i = (-1,1)\ \forall i$. Then $U:= \prod_{i=1}^{\infty}U_i$ and $U \in \tau'$ but $U \notin \tau$. I don't understand why $U \notin \tau$. Any insight would be appreciated!","I am trying to understand a counter-example showing that the box topology and product topology are not equal. Here it is: Let $\tau$ and $\tau'$ be the product and box topologies respectively. Let $X_i = \mathbb{R}\ \forall i$ and let $U_i = (-1,1)\ \forall i$. Then $U:= \prod_{i=1}^{\infty}U_i$ and $U \in \tau'$ but $U \notin \tau$. I don't understand why $U \notin \tau$. Any insight would be appreciated!",,"['real-analysis', 'general-topology', 'product-space']"
30,Prove that an increasing and surjective function is continuous.,Prove that an increasing and surjective function is continuous.,,"If $f:[a,b]\rightarrow [f(a),f(b)]$ is increasing and surjective, prove that it is continuous. Fix $c \in (a,b)$. Take $\epsilon >0$. We then wish to find the set of $x$ such that $|f(x)-f(c)|<\epsilon$ by definition of continuity. That's as far as I got and I don't know how to proceed. Any suggestions/proofs?","If $f:[a,b]\rightarrow [f(a),f(b)]$ is increasing and surjective, prove that it is continuous. Fix $c \in (a,b)$. Take $\epsilon >0$. We then wish to find the set of $x$ such that $|f(x)-f(c)|<\epsilon$ by definition of continuity. That's as far as I got and I don't know how to proceed. Any suggestions/proofs?",,"['real-analysis', 'continuity']"
31,$L=\lim_{x\to\infty}(f(x)+f'(x))$ exists . Which of the following statements is\are correct?,exists . Which of the following statements is\are correct?,L=\lim_{x\to\infty}(f(x)+f'(x)),"Let $f$ be a continously differentiable function on $\mathbb R$.  Suppose that $$L=\lim_{x\to\infty}(f(x)+f'(x))$$ exists.  If $0<L<\infty$, then which of the following statements is\are correct? If $\lim_{x\to\infty} f'(x)$ exists, then it is $0$. If $\lim_{x\to\infty} f(x)$ exists, then it is $L$. If $\lim_{x\to\infty} f'(x)$ exists, then $\lim_{x\to\infty}f(x)=0$. If $\lim_{x\to\infty} f(x)$ exists, then $\lim_{x\to\infty}f'(x)=0$. My Guess I could not conclude the answer and prove that properly. But, I guess that it must be 1 and 2. help me.","Let $f$ be a continously differentiable function on $\mathbb R$.  Suppose that $$L=\lim_{x\to\infty}(f(x)+f'(x))$$ exists.  If $0<L<\infty$, then which of the following statements is\are correct? If $\lim_{x\to\infty} f'(x)$ exists, then it is $0$. If $\lim_{x\to\infty} f(x)$ exists, then it is $L$. If $\lim_{x\to\infty} f'(x)$ exists, then $\lim_{x\to\infty}f(x)=0$. If $\lim_{x\to\infty} f(x)$ exists, then $\lim_{x\to\infty}f'(x)=0$. My Guess I could not conclude the answer and prove that properly. But, I guess that it must be 1 and 2. help me.",,"['real-analysis', 'limits']"
32,How to convince a high school student that differentials don't work like fractions in general?,How to convince a high school student that differentials don't work like fractions in general?,,"It all started when I tried to convince a 10th grader that if $f$ is a function defined on $\mathbb{R}^n$ the differential is defined by: $\large \displaystyle df = \frac{\partial{f}}{\partial{x_1}}dx_1 + \frac{\partial{f}}{\partial{x_2}}dx_2 + \cdots  \frac{\partial{f}}{\partial{x_n}}dx_n$ and if $x_i = g_i(t)$ then: $\large\displaystyle \frac{df}{dt} = \frac{\partial{f}}{\partial{x_1}}\frac{dx_1}{dt} + \frac{\partial{f}}{\partial{x_2}}\frac{dx_2}{dt} + \cdots  \frac{\partial{f}}{\partial{x_n}}\frac{dx_n}{dt}$ As he's a 10th grader, he's supposed to think of $df$ as a small change in the value of $f$ caused by a small change in $(x_1,...,x_n)$. I have defined $df$ for a differentiable function $f: \mathbb{R} \to \mathbb{R}$ in the following naive but intuitive way and he has happily accepted this definition: $\large \displaystyle df = \lim_{\Delta{x} \to 0} \Delta{y}$ where $\large \Delta{y} = f'(x)\Delta{x} + \epsilon(\Delta{x})\Delta{x}$ and  $\large \epsilon(\Delta{x})$ is a function of $\large \Delta{x}$ that compensates the error for turning $\large f'(x) = \displaystyle \lim_{\Delta{x} \to 0}\frac{\Delta{y}}{\Delta{x}}$ into an equality and by definition we have $\large \displaystyle \lim_{\Delta{x} \to 0}\epsilon(\Delta{x}) = 0$ Using that definition, I convinced him why the differential of a multivariable function is generalized to higher dimensions that way. But I failed to convince him why it's not a good idea to cancel $\partial{x_i}$ in the denominator with $dx_i$ just like we're dealing with fractions. I'm also afraid of proving the chain rule for him by dividing $\Delta{t}$ and then letting $\Delta{t} \to 0$. I'm looking for an easy explanation, suitable for a high school student, that convinces him why differentials shouldn't be looked at as fractions contrary to what many students think in high school.","It all started when I tried to convince a 10th grader that if $f$ is a function defined on $\mathbb{R}^n$ the differential is defined by: $\large \displaystyle df = \frac{\partial{f}}{\partial{x_1}}dx_1 + \frac{\partial{f}}{\partial{x_2}}dx_2 + \cdots  \frac{\partial{f}}{\partial{x_n}}dx_n$ and if $x_i = g_i(t)$ then: $\large\displaystyle \frac{df}{dt} = \frac{\partial{f}}{\partial{x_1}}\frac{dx_1}{dt} + \frac{\partial{f}}{\partial{x_2}}\frac{dx_2}{dt} + \cdots  \frac{\partial{f}}{\partial{x_n}}\frac{dx_n}{dt}$ As he's a 10th grader, he's supposed to think of $df$ as a small change in the value of $f$ caused by a small change in $(x_1,...,x_n)$. I have defined $df$ for a differentiable function $f: \mathbb{R} \to \mathbb{R}$ in the following naive but intuitive way and he has happily accepted this definition: $\large \displaystyle df = \lim_{\Delta{x} \to 0} \Delta{y}$ where $\large \Delta{y} = f'(x)\Delta{x} + \epsilon(\Delta{x})\Delta{x}$ and  $\large \epsilon(\Delta{x})$ is a function of $\large \Delta{x}$ that compensates the error for turning $\large f'(x) = \displaystyle \lim_{\Delta{x} \to 0}\frac{\Delta{y}}{\Delta{x}}$ into an equality and by definition we have $\large \displaystyle \lim_{\Delta{x} \to 0}\epsilon(\Delta{x}) = 0$ Using that definition, I convinced him why the differential of a multivariable function is generalized to higher dimensions that way. But I failed to convince him why it's not a good idea to cancel $\partial{x_i}$ in the denominator with $dx_i$ just like we're dealing with fractions. I'm also afraid of proving the chain rule for him by dividing $\Delta{t}$ and then letting $\Delta{t} \to 0$. I'm looking for an easy explanation, suitable for a high school student, that convinces him why differentials shouldn't be looked at as fractions contrary to what many students think in high school.",,"['real-analysis', 'multivariable-calculus', 'differential-geometry', 'education']"
33,Convergence/Divergence of $\int_{0}^{1/e} \frac{\log \left(\frac{1}{x}\right)}{(\log^2 (x)-1)^{3/2}} \mathrm{dx}$,Convergence/Divergence of,\int_{0}^{1/e} \frac{\log \left(\frac{1}{x}\right)}{(\log^2 (x)-1)^{3/2}} \mathrm{dx},"Initially I wanted to compute  $$\int_{0}^{1/e} \frac{\log \left(\frac{1}{x}\right)}{(\log^2 (x)-1)^{3/2}} \mathrm{dx}$$ but it seems that Mathematica says that the integral diverges. I thought of some variable change, but I also wonder if there is something easy to prove it diverges. Any hint / suggestion here would be precious to me. Thanks!","Initially I wanted to compute  $$\int_{0}^{1/e} \frac{\log \left(\frac{1}{x}\right)}{(\log^2 (x)-1)^{3/2}} \mathrm{dx}$$ but it seems that Mathematica says that the integral diverges. I thought of some variable change, but I also wonder if there is something easy to prove it diverges. Any hint / suggestion here would be precious to me. Thanks!",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
34,What is the precise definition of 'uniformly differentiable'?,What is the precise definition of 'uniformly differentiable'?,,"Since I'm not familiar with manifold concept, let's restrict ourselves to functions with real domain. Let $A\subset \mathbb{R}$ and $f:A\rightarrow \mathbb{R}^k$. What is '$f$ is uniformly differentiable on $A$' referring to?","Since I'm not familiar with manifold concept, let's restrict ourselves to functions with real domain. Let $A\subset \mathbb{R}$ and $f:A\rightarrow \mathbb{R}^k$. What is '$f$ is uniformly differentiable on $A$' referring to?",,['real-analysis']
35,Probability Theory is Applied Measure Theory?,Probability Theory is Applied Measure Theory?,,"So I’ve taken a probability theory class and I’m studying for a measure theory class next quarter. While I always knew probability theory incorporated a lot of measure theory, it seems to me like it really is just applied measure theory. It incorporates some other notions like Bayesian probability results, but it is otherwise just renaming notions from measure theory and developing special cases of measures and $\sigma-$ algebras. Is this an unjustified understanding of probability theory?","So I’ve taken a probability theory class and I’m studying for a measure theory class next quarter. While I always knew probability theory incorporated a lot of measure theory, it seems to me like it really is just applied measure theory. It incorporates some other notions like Bayesian probability results, but it is otherwise just renaming notions from measure theory and developing special cases of measures and algebras. Is this an unjustified understanding of probability theory?",\sigma-,"['real-analysis', 'probability-theory', 'measure-theory']"
36,"If $ \lim_{x\to \infty} (f(x+1)-f(x))=1$, then $ \lim_{x\to \infty} \frac{f(x)}{x}=1$?","If , then ?", \lim_{x\to \infty} (f(x+1)-f(x))=1  \lim_{x\to \infty} \frac{f(x)}{x}=1,"Let $f:\mathbb{R} \to \mathbb{R}$ be a fuction such that $\displaystyle \lim_{x\to \infty} (f(x+1)-f(x))=1$ . Is it true then that $\displaystyle \lim_{x\to \infty} \frac{f(x)}{x}=1$ ? I think it is and here is how I went about it. Let $\varepsilon>0$ . Then there is some $\delta_\varepsilon>0$ such that $$1-\varepsilon < f(x)-f(x-1)<1+\varepsilon, \forall x>\delta_\varepsilon.$$ Thus, we may write the following inequalities for an $x> \delta_\epsilon$ : $$1-\varepsilon < f(x)-f(x-1)<1+\varepsilon\\ 1-\varepsilon < f(x-1)-f(x-2)<1+\varepsilon\\     \vdots\\ 1-\varepsilon < f(\delta_\varepsilon+1)-f(\delta_\varepsilon)<1+\varepsilon$$ and after we sum these up we get that $$(x-\delta_\epsilon)(1-\epsilon)<f(x)-f(\delta_\varepsilon)<(x-\delta_\varepsilon)(1+\varepsilon), \forall x>\delta_\varepsilon.$$ This implies that $$\frac{f(\delta_\varepsilon)+(x-\delta_\varepsilon)(1-\varepsilon)}{x}<\frac{f(x)}{x}<\frac{f(\delta_\varepsilon)+(x-\delta_\varepsilon)(1+\varepsilon)}{x}, \forall x>\delta_\varepsilon.$$ If we take $\displaystyle\limsup_{x\to\infty}$ , we get that $1-\varepsilon < \displaystyle\limsup_{x\to\infty} \frac{f(x)}{x}< 1+\varepsilon$ , $\forall \varepsilon > 0$ , so $\displaystyle\limsup_{x\to\infty} \frac{f(x)}{x}=1$ . In the same way we get that $\displaystyle\liminf_{x\to\infty} \frac{f(x)}{x}=1$ , so $\displaystyle\lim_{x\to\infty} \frac{f(x)}{x}=1$ as desired. Is this proof correct? I am a bit unsure that I am taking that $\limsup$ correctly, even though I can't see why it could be wrong.","Let be a fuction such that . Is it true then that ? I think it is and here is how I went about it. Let . Then there is some such that Thus, we may write the following inequalities for an : and after we sum these up we get that This implies that If we take , we get that , , so . In the same way we get that , so as desired. Is this proof correct? I am a bit unsure that I am taking that correctly, even though I can't see why it could be wrong.","f:\mathbb{R} \to \mathbb{R} \displaystyle \lim_{x\to \infty} (f(x+1)-f(x))=1 \displaystyle \lim_{x\to \infty} \frac{f(x)}{x}=1 \varepsilon>0 \delta_\varepsilon>0 1-\varepsilon < f(x)-f(x-1)<1+\varepsilon, \forall x>\delta_\varepsilon. x> \delta_\epsilon 1-\varepsilon < f(x)-f(x-1)<1+\varepsilon\\
1-\varepsilon < f(x-1)-f(x-2)<1+\varepsilon\\    
\vdots\\
1-\varepsilon < f(\delta_\varepsilon+1)-f(\delta_\varepsilon)<1+\varepsilon (x-\delta_\epsilon)(1-\epsilon)<f(x)-f(\delta_\varepsilon)<(x-\delta_\varepsilon)(1+\varepsilon), \forall x>\delta_\varepsilon. \frac{f(\delta_\varepsilon)+(x-\delta_\varepsilon)(1-\varepsilon)}{x}<\frac{f(x)}{x}<\frac{f(\delta_\varepsilon)+(x-\delta_\varepsilon)(1+\varepsilon)}{x}, \forall x>\delta_\varepsilon. \displaystyle\limsup_{x\to\infty} 1-\varepsilon < \displaystyle\limsup_{x\to\infty} \frac{f(x)}{x}< 1+\varepsilon \forall \varepsilon > 0 \displaystyle\limsup_{x\to\infty} \frac{f(x)}{x}=1 \displaystyle\liminf_{x\to\infty} \frac{f(x)}{x}=1 \displaystyle\lim_{x\to\infty} \frac{f(x)}{x}=1 \limsup","['real-analysis', 'limits', 'solution-verification', 'epsilon-delta', 'limsup-and-liminf']"
37,Evaluating a Lebesgue Integral,Evaluating a Lebesgue Integral,,"I have the following integral: $$\lim_{n \to \infty} \int_{0}^{1} \frac{n\sqrt{n}x}{1+n^2x^2} \, \mathrm{d}x $$ To use the dominated convergence theorem I know that limit of $f_n$ is $0$ and $|f_n|<\frac{n^{1/2}}{2}$ . However, I am having trouble to find a function that is greater than $\frac{n^{1/2}}{2}$ for all n. Can someone help? Thanks in advance.","I have the following integral: To use the dominated convergence theorem I know that limit of is and . However, I am having trouble to find a function that is greater than for all n. Can someone help? Thanks in advance.","\lim_{n \to \infty} \int_{0}^{1} \frac{n\sqrt{n}x}{1+n^2x^2} \, \mathrm{d}x
 f_n 0 |f_n|<\frac{n^{1/2}}{2} \frac{n^{1/2}}{2}","['real-analysis', 'integration', 'definite-integrals', 'lebesgue-integral']"
38,Solve $f (x + y) + f (y + z) + f (z + x) \ge 3f (x + 2y + 3z)$,Solve,f (x + y) + f (y + z) + f (z + x) \ge 3f (x + 2y + 3z),"Find all functions   $f : \mathbb{R} \to \mathbb{R}$ which satisfy : $f (x + y) + f (y + z) + f (z + x) ≥ 3f (x + 2y + 3z)$ for real $x,y,z$. Attempt at solution: I have tried plugging in $x = -y$ and $x = -z$. This does not seem to be getting me anywhere. Any help is appreciated.","Find all functions   $f : \mathbb{R} \to \mathbb{R}$ which satisfy : $f (x + y) + f (y + z) + f (z + x) ≥ 3f (x + 2y + 3z)$ for real $x,y,z$. Attempt at solution: I have tried plugging in $x = -y$ and $x = -z$. This does not seem to be getting me anywhere. Any help is appreciated.",,['real-analysis']
39,Prove that an open ball is also closed in an ultrametric space,Prove that an open ball is also closed in an ultrametric space,,"I am trying to prove that all open balls in an ultrametric space are also closed. I found this proof (link below - page 3, #2) but I still don't understand it. Can someone explain their logic? What exactly do they mean? http://assets.cambridge.org/97805211/92439/excerpt/9780521192439_excerpt.pdf","I am trying to prove that all open balls in an ultrametric space are also closed. I found this proof (link below - page 3, #2) but I still don't understand it. Can someone explain their logic? What exactly do they mean? http://assets.cambridge.org/97805211/92439/excerpt/9780521192439_excerpt.pdf",,['real-analysis']
40,"If $\phi \in C^1_c(\mathbb R)$ then $ \lim_n \int_\mathbb R \frac{\sin(nx)}{x}\phi(x)\,dx = \pi\phi(0)$.",If  then .,"\phi \in C^1_c(\mathbb R)  \lim_n \int_\mathbb R \frac{\sin(nx)}{x}\phi(x)\,dx = \pi\phi(0)","Let $\phi \in C^1_c(\mathbb R)$. Prove that $$ \lim_{n \to +\infty} \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx = \pi\phi(0). $$ Unfortunately, I didn't manage to give a complete proof. First of all, I fixed $\varepsilon>0$. Then there exists a $\delta >0$ s.t.  $$ \vert x \vert < \delta \Rightarrow \vert \phi(x)-\phi(0) \vert < \frac{\varepsilon}{\pi}. $$ Now, I would use the well-known fact that  $$ \int_\mathbb R \frac{\sin x}{x} \, dx = \pi. $$ On the other hand, by substitution rule, we have also $$ \int_\mathbb R \frac{\sin(nx)}{x} \, dx = \int_\mathbb R \frac{\sin x}{x} \, dx = \pi. $$ Indeed, I would like to estimate the quantity $$ \begin{split} & \left\vert \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx - \pi \phi(0) \right\vert = \\  & = \left\vert \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx - \phi(0)\int_\mathbb R \frac{\sin{(nx)}}{x}dx \right\vert \le \\  & \le \int_\mathbb R \left\vert \frac{\sin(nx)}{x}\right\vert \cdot \left\vert  \phi(x)-\phi(0) \right\vert dx \end{split} $$ but the problem is that $x \mapsto \frac{\sin(nx)}{x}$ is not absolutely integrable over $\mathbb R$. Another big problem is that I don't see how to use the hypothesis $\phi$ has compact support. I think that I should use dominated convergence theorem, but I've never done exercises about this theorem. Would you please help me? Thank you very much indeed.","Let $\phi \in C^1_c(\mathbb R)$. Prove that $$ \lim_{n \to +\infty} \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx = \pi\phi(0). $$ Unfortunately, I didn't manage to give a complete proof. First of all, I fixed $\varepsilon>0$. Then there exists a $\delta >0$ s.t.  $$ \vert x \vert < \delta \Rightarrow \vert \phi(x)-\phi(0) \vert < \frac{\varepsilon}{\pi}. $$ Now, I would use the well-known fact that  $$ \int_\mathbb R \frac{\sin x}{x} \, dx = \pi. $$ On the other hand, by substitution rule, we have also $$ \int_\mathbb R \frac{\sin(nx)}{x} \, dx = \int_\mathbb R \frac{\sin x}{x} \, dx = \pi. $$ Indeed, I would like to estimate the quantity $$ \begin{split} & \left\vert \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx - \pi \phi(0) \right\vert = \\  & = \left\vert \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx - \phi(0)\int_\mathbb R \frac{\sin{(nx)}}{x}dx \right\vert \le \\  & \le \int_\mathbb R \left\vert \frac{\sin(nx)}{x}\right\vert \cdot \left\vert  \phi(x)-\phi(0) \right\vert dx \end{split} $$ but the problem is that $x \mapsto \frac{\sin(nx)}{x}$ is not absolutely integrable over $\mathbb R$. Another big problem is that I don't see how to use the hypothesis $\phi$ has compact support. I think that I should use dominated convergence theorem, but I've never done exercises about this theorem. Would you please help me? Thank you very much indeed.",,"['real-analysis', 'limits']"
41,$\int_0^\infty ne^{-nx}\sin\left(\frac1{x}\right)\;dx\to ?$ as $n\to\infty$,as,\int_0^\infty ne^{-nx}\sin\left(\frac1{x}\right)\;dx\to ? n\to\infty,"I want to find limit of $\displaystyle\int_0^\infty ne^{-nx}\sin\left(\frac{1}{x}\right)\;dx$ as $n\to\infty$ if it exists or to prove that it doesn't exist. I see that $ne^{-nx}\sin\left(\frac{1}{x}\right)\to 0$ for all $x>0$ and that the convergence is uniform on $[a,\infty)$ for all $a>0$. That implies $\displaystyle\int_a^\infty ne^{-nx}\sin\left(\frac{1}{x}\right)\;dx\to 0$ as $n\to\infty$ for all $a>0$. Can anyone tell me what the next step is or if I'm on the wrong track? Thanks.","I want to find limit of $\displaystyle\int_0^\infty ne^{-nx}\sin\left(\frac{1}{x}\right)\;dx$ as $n\to\infty$ if it exists or to prove that it doesn't exist. I see that $ne^{-nx}\sin\left(\frac{1}{x}\right)\to 0$ for all $x>0$ and that the convergence is uniform on $[a,\infty)$ for all $a>0$. That implies $\displaystyle\int_a^\infty ne^{-nx}\sin\left(\frac{1}{x}\right)\;dx\to 0$ as $n\to\infty$ for all $a>0$. Can anyone tell me what the next step is or if I'm on the wrong track? Thanks.",,"['real-analysis', 'measure-theory']"
42,"Uniform continuity on (0,1) implies boundedness [closed]","Uniform continuity on (0,1) implies boundedness [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I need to prove that if $f: (0,1) \rightarrow \mathbb{R}$ is Uniformly continuous then it is  bounded. Thank you.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I need to prove that if $f: (0,1) \rightarrow \mathbb{R}$ is Uniformly continuous then it is  bounded. Thank you.",,['real-analysis']
43,Harmonic function composed with conformal map is harmonic (in $\mathbb{R}^n$),Harmonic function composed with conformal map is harmonic (in ),\mathbb{R}^n,"Here's the setup: Let $U,V$ open $\subset \mathbb{R}^n$, and let $u:V\rightarrow \mathbb{R}$ be harmonic, and $v:U\rightarrow V$ be conformal, i.e. $v$ is $C^1$ and the Jacobian $J_v(x)$ is a scalar multiple of an orthogonal transformation for all [; x\in U ;]. I'm trying to prove $u\circ v$ is harmonic. [I've seen this stated as a fact in a few places without reference, namely here: http://en.wikipedia.org/wiki/Conformal_map#Uses , but maybe my hypotheses are slightly different and this is not true at all] I've seen a proof that if $u$ is $C^2$ and $T$ is an orthogonal transformation then $\Delta (u \circ T) = \Delta(u) \circ T$. So I'm thinking that to show $u\circ v$ is harmonic, we can use the fact that $v$ acts locally as its Jacobian, which is an orthogonal transformation, and move the Laplacian onto $u$ and conclude $u\circ v$ is harmonic. However, I'm having trouble making this idea precise.  After glancing at my copy of baby Rudin, my hunch is to use the inverse function theorem or constant rank theorem, but I'm unsure how to apply those.  Any suggestions?","Here's the setup: Let $U,V$ open $\subset \mathbb{R}^n$, and let $u:V\rightarrow \mathbb{R}$ be harmonic, and $v:U\rightarrow V$ be conformal, i.e. $v$ is $C^1$ and the Jacobian $J_v(x)$ is a scalar multiple of an orthogonal transformation for all [; x\in U ;]. I'm trying to prove $u\circ v$ is harmonic. [I've seen this stated as a fact in a few places without reference, namely here: http://en.wikipedia.org/wiki/Conformal_map#Uses , but maybe my hypotheses are slightly different and this is not true at all] I've seen a proof that if $u$ is $C^2$ and $T$ is an orthogonal transformation then $\Delta (u \circ T) = \Delta(u) \circ T$. So I'm thinking that to show $u\circ v$ is harmonic, we can use the fact that $v$ acts locally as its Jacobian, which is an orthogonal transformation, and move the Laplacian onto $u$ and conclude $u\circ v$ is harmonic. However, I'm having trouble making this idea precise.  After glancing at my copy of baby Rudin, my hunch is to use the inverse function theorem or constant rank theorem, but I'm unsure how to apply those.  Any suggestions?",,['real-analysis']
44,Particularly nice bump function,Particularly nice bump function,,"Is there a smooth ( $C^\infty$ ) function $f: \mathbb{R} \to \left[ 0, 1 \right]$ such that: $f(x) = 1$ iff $x = 0$ , $f(x) = 0$ iff $\left\lvert x \right\rvert \geq 1$ , and $0 < f(x) < 1$ otherwise; Every derivative of $f$ is simultaneously $0$ at $x$ if and only if $x = 0$ or $\left\lvert x \right\rvert \geq 1$ ; $f(x) = f(-x)$ for all $x$ ; $f’(x) > 0$ for $-1 < x < 0$ and $f’(x) < 0$ for $0 < x < 1$ ; and $1 - f(x) = f(1 - x)$ for $0 \leq x \leq 1$ ? If not, for which subsets of the above five conditions are there satisfactory functions? I know that the standard bump function $f(x) = e^{1 + 1/(x^2 - 1)}$ satisfies all but the last one of my conditions; what other conditions could I exclude, or is there an extension of one of the standard examples to meet the last criterion as well?","Is there a smooth ( ) function such that: iff , iff , and otherwise; Every derivative of is simultaneously at if and only if or ; for all ; for and for ; and for ? If not, for which subsets of the above five conditions are there satisfactory functions? I know that the standard bump function satisfies all but the last one of my conditions; what other conditions could I exclude, or is there an extension of one of the standard examples to meet the last criterion as well?","C^\infty f: \mathbb{R} \to \left[ 0, 1 \right] f(x) = 1 x = 0 f(x) = 0 \left\lvert x \right\rvert \geq 1 0 < f(x) < 1 f 0 x x = 0 \left\lvert x \right\rvert \geq 1 f(x) = f(-x) x f’(x) > 0 -1 < x < 0 f’(x) < 0 0 < x < 1 1 - f(x) = f(1 - x) 0 \leq x \leq 1 f(x) = e^{1 + 1/(x^2 - 1)}","['real-analysis', 'smooth-functions']"
45,"What does ""finite but unbounded"" mean?","What does ""finite but unbounded"" mean?",,"In the following example, Gelbaum and Olmsted ( Counterexamples in Analysis , 1964, p.37 ) speak of a derivative being “finite but unbounded”. A function whose derivative is finite but unbounded on a closed interval. The function $$f(x)=\begin{cases}x^2\sin\dfrac1{x^2}&\text{if }x\neq 0\\ 0&\text{if }x= 0\end{cases}$$ has the derivative $$f'(x)=\begin{cases}2x\sin\dfrac1{x^2}-\dfrac2x\cos\dfrac1{x^2}&\text{if }x\neq 0\\ 0&\text{if }x= 0,\end{cases}$$ which is unbounded on $[-1,1]$ . What does this mean and how is it possible that a quantity is both finite and unbounded?","In the following example, Gelbaum and Olmsted ( Counterexamples in Analysis , 1964, p.37 ) speak of a derivative being “finite but unbounded”. A function whose derivative is finite but unbounded on a closed interval. The function has the derivative which is unbounded on . What does this mean and how is it possible that a quantity is both finite and unbounded?","f(x)=\begin{cases}x^2\sin\dfrac1{x^2}&\text{if }x\neq 0\\
0&\text{if }x= 0\end{cases} f'(x)=\begin{cases}2x\sin\dfrac1{x^2}-\dfrac2x\cos\dfrac1{x^2}&\text{if }x\neq 0\\
0&\text{if }x= 0,\end{cases} [-1,1]",[]
46,Product rule for gradient,Product rule for gradient,,"I have two functions scalar functions of vector $\textbf{x}$, $f(\textbf{x})$ and $g(\textbf{x})$. I know the gradient of $f(\textbf{x})$ (i.e. $\triangledown f(\textbf{x})$) and I want to find the gradient of $f(\textbf{x})g(\textbf{x})$. Can I use the product rule $$\triangledown f(\textbf{x})g(\textbf{x})=g(\textbf{x})\triangledown f(\textbf{x})+f(\textbf{x})\triangledown g(\textbf{x}).$$ I mean does the product rule of differentiation also apply to gradients? Thanks in advance.","I have two functions scalar functions of vector $\textbf{x}$, $f(\textbf{x})$ and $g(\textbf{x})$. I know the gradient of $f(\textbf{x})$ (i.e. $\triangledown f(\textbf{x})$) and I want to find the gradient of $f(\textbf{x})g(\textbf{x})$. Can I use the product rule $$\triangledown f(\textbf{x})g(\textbf{x})=g(\textbf{x})\triangledown f(\textbf{x})+f(\textbf{x})\triangledown g(\textbf{x}).$$ I mean does the product rule of differentiation also apply to gradients? Thanks in advance.",,"['calculus', 'real-analysis', 'vector-analysis']"
47,Additive but not $\sigma$-additive function,Additive but not -additive function,\sigma,"Give an example of a measure space $(\Omega, \mathit{F})$ and a function $\mu$ on $\mathit{F}$ that is additive but not $\sigma$ -additive, i.e. $\mu(\cup A_i)= \sum\mu(A_i)$ for a finite collection of disjoint $A_i$ but not for some infinite collections. I know a measure function defined on $\sigma$ -algebra is $\sigma$ -additive, but I struggle finding a function that would not be additive for infinite collections. Can someone give me an example and show me why?","Give an example of a measure space and a function on that is additive but not -additive, i.e. for a finite collection of disjoint but not for some infinite collections. I know a measure function defined on -algebra is -additive, but I struggle finding a function that would not be additive for infinite collections. Can someone give me an example and show me why?","(\Omega, \mathit{F}) \mu \mathit{F} \sigma \mu(\cup A_i)= \sum\mu(A_i) A_i \sigma \sigma","['real-analysis', 'measure-theory', 'probability-theory']"
48,What is the set $\{x\in\Bbb R\mid \forall q\in\Bbb Q: q^x\in\Bbb Q\}$?,What is the set ?,\{x\in\Bbb R\mid \forall q\in\Bbb Q: q^x\in\Bbb Q\},What is the set $\{x\in\Bbb R\mid \forall q\in\Bbb Q: q^x\in\Bbb Q\}$? Of course $\Bbb Z$ is a subset of this set. Are there any other? if not what is the proof? is there a good reference for it?,What is the set $\{x\in\Bbb R\mid \forall q\in\Bbb Q: q^x\in\Bbb Q\}$? Of course $\Bbb Z$ is a subset of this set. Are there any other? if not what is the proof? is there a good reference for it?,,"['real-analysis', 'analytic-number-theory']"
49,closed and open set - set $S$ is open if and only if its complement is closed?,closed and open set - set  is open if and only if its complement is closed?,S,"Let set $S$ be a set of real numbers. A point $p∈S$ is set to be interior point of $S$ provided that there exist a $δ>0$ such that $(p-δ,p+δ)⊆S$. The set $S$ is said to be an open set if every element of $S$ is an interior point. How can I prove that Set $S$ is open if and only if its complement is closed.","Let set $S$ be a set of real numbers. A point $p∈S$ is set to be interior point of $S$ provided that there exist a $δ>0$ such that $(p-δ,p+δ)⊆S$. The set $S$ is said to be an open set if every element of $S$ is an interior point. How can I prove that Set $S$ is open if and only if its complement is closed.",,['real-analysis']
50,Prove that $\lim_{x\to 2}x^2=4$ using $\epsilon-\delta$ definition.,Prove that  using  definition.,\lim_{x\to 2}x^2=4 \epsilon-\delta,"Prove that $\lim_{x\to 2}x^2=4$ using $\epsilon-\delta$ definition. By the mean of $\epsilon-\delta$ definition, $|x-2|\le \delta,|x+2|\le \delta+4$ then $|x-2||x+2|\le \delta(\delta+4),|x^2-4|\le \delta^2+4\delta$. Assign $\epsilon=\delta^2+4\delta,|x^2-4|\le\epsilon$. Q.E.D. Is this method correct? If yes, why I always see people do this by putting $\delta = \min(1,\epsilon/5)$...? Thanks.","Prove that $\lim_{x\to 2}x^2=4$ using $\epsilon-\delta$ definition. By the mean of $\epsilon-\delta$ definition, $|x-2|\le \delta,|x+2|\le \delta+4$ then $|x-2||x+2|\le \delta(\delta+4),|x^2-4|\le \delta^2+4\delta$. Assign $\epsilon=\delta^2+4\delta,|x^2-4|\le\epsilon$. Q.E.D. Is this method correct? If yes, why I always see people do this by putting $\delta = \min(1,\epsilon/5)$...? Thanks.",,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
51,Example of not being a sigma algebra as complement property does not hold,Example of not being a sigma algebra as complement property does not hold,,"I am working on a homework problem and am somewhat lost. I know that an answer will not be given on a silver platter and am fine with that - I need to know what I am missing in understanding so that I can solve the problem. I need an infinite collection of subsets of $\mathbb{R}$ that contains $\mathbb{R}$, is closed under the formation of countable unions and countable intersections, but is not a $\sigma$-algebra. So I immediately thought that the only requirement not mentioned to make it a $\sigma$-algebra is the closure under complementation. That is why I thought of maybe using $\mathcal{P}(\mathbb{R})-\{\varnothing\}$, the powerset 'minus' the null set.  Is this okay? Can you subtract 'nothing' like this? Otherwise I am quite lost and any direction would be greatly appreciated. Nate P.S> I could not find suitable suggestions to my question by looking around on the site.","I am working on a homework problem and am somewhat lost. I know that an answer will not be given on a silver platter and am fine with that - I need to know what I am missing in understanding so that I can solve the problem. I need an infinite collection of subsets of $\mathbb{R}$ that contains $\mathbb{R}$, is closed under the formation of countable unions and countable intersections, but is not a $\sigma$-algebra. So I immediately thought that the only requirement not mentioned to make it a $\sigma$-algebra is the closure under complementation. That is why I thought of maybe using $\mathcal{P}(\mathbb{R})-\{\varnothing\}$, the powerset 'minus' the null set.  Is this okay? Can you subtract 'nothing' like this? Otherwise I am quite lost and any direction would be greatly appreciated. Nate P.S> I could not find suitable suggestions to my question by looking around on the site.",,"['real-analysis', 'measure-theory']"
52,"Is this ""reverse the limit"" process right?","Is this ""reverse the limit"" process right?",,"Suppose you have two (nice enough) functions $f$ and $g$ and a constant $\lambda$ such that $$\lim_{x\to\infty}\frac{f(x)}{g(x)}=\lambda$$ Is it true that $$\lim_{x\to\infty}\frac{f^{-1}(x)}{g^{-1}(x/\lambda)}=1$$ The ""reasoning"" goes like this: $$\frac{f(x)}{g(x)}\approx\lambda$$ $$f(x)\approx\lambda g(x)$$ $$x\approx f^{-1}(\lambda g(x))$$ $$g^{-1}(x/\lambda)\approx f^{-1}(x)$$ $$\frac{f^{-1}(x)}{g^{-1}(x/\lambda)}\approx 1$$ all of this supposing there is no problem in $x\mapsto f^{-1}(x)$ and $x\mapsto g^{-1}(x/\lambda)$ I think assuming ""nice enough"" (continuity, inverse, ...) and being a bit more precise like $$f(x)=\lambda g(x)+o(g(x))$$ will prove the statement. What I'm more interested in is under what conditions does it remain true in discrete variable (and non-existing inverse function for $f$ or $g$ or both) Thanks!","Suppose you have two (nice enough) functions $f$ and $g$ and a constant $\lambda$ such that $$\lim_{x\to\infty}\frac{f(x)}{g(x)}=\lambda$$ Is it true that $$\lim_{x\to\infty}\frac{f^{-1}(x)}{g^{-1}(x/\lambda)}=1$$ The ""reasoning"" goes like this: $$\frac{f(x)}{g(x)}\approx\lambda$$ $$f(x)\approx\lambda g(x)$$ $$x\approx f^{-1}(\lambda g(x))$$ $$g^{-1}(x/\lambda)\approx f^{-1}(x)$$ $$\frac{f^{-1}(x)}{g^{-1}(x/\lambda)}\approx 1$$ all of this supposing there is no problem in $x\mapsto f^{-1}(x)$ and $x\mapsto g^{-1}(x/\lambda)$ I think assuming ""nice enough"" (continuity, inverse, ...) and being a bit more precise like $$f(x)=\lambda g(x)+o(g(x))$$ will prove the statement. What I'm more interested in is under what conditions does it remain true in discrete variable (and non-existing inverse function for $f$ or $g$ or both) Thanks!",,"['real-analysis', 'limits']"
53,The set of integers is not open or is open,The set of integers is not open or is open,,"Baby Rudin gives the example of the set of all integers being not open if it is a subset of $\mathbb{R}^2$. If we consider the set of integers in $\mathbb{R}$, is this set also not open? I can find a neighbourhood which will contain any point, $p$, however is it a requirement that a neighbourhood contains more than one point? I'm trying to understand this fully and have searched through the various posts that have a slight relation and can not find out specifically how these take interior and isolated points into account and how these relate to openess .","Baby Rudin gives the example of the set of all integers being not open if it is a subset of $\mathbb{R}^2$. If we consider the set of integers in $\mathbb{R}$, is this set also not open? I can find a neighbourhood which will contain any point, $p$, however is it a requirement that a neighbourhood contains more than one point? I'm trying to understand this fully and have searched through the various posts that have a slight relation and can not find out specifically how these take interior and isolated points into account and how these relate to openess .",,"['real-analysis', 'analysis']"
54,The space of functions that vanish at infinity is a Banach space,The space of functions that vanish at infinity is a Banach space,,"Prove that $C_0(X) = \{ f \in C(X) \mid \forall \varepsilon >0 \ \exists K \subset X \text{ compact such that } |f(x)| < \varepsilon \text{ for } x \notin K \}$ is a Banach space with the norm $\|f\|_{\infty}$. I'm trying to prove that $C_0(X)$ is closed subset of $C(X)$, therefore I suppose $f \in \overline{C_0(X)}$ so there exist a sequence $f_n \in C_0(X)$ such that $f_n \to f$. I am stuck here: why $f \in C_0(X)$?","Prove that $C_0(X) = \{ f \in C(X) \mid \forall \varepsilon >0 \ \exists K \subset X \text{ compact such that } |f(x)| < \varepsilon \text{ for } x \notin K \}$ is a Banach space with the norm $\|f\|_{\infty}$. I'm trying to prove that $C_0(X)$ is closed subset of $C(X)$, therefore I suppose $f \in \overline{C_0(X)}$ so there exist a sequence $f_n \in C_0(X)$ such that $f_n \to f$. I am stuck here: why $f \in C_0(X)$?",,"['real-analysis', 'banach-spaces']"
55,"Prove that if an infinite series converges, then the associative property holds","Prove that if an infinite series converges, then the associative property holds",,"I'm self-studying from the book Understanding Analysis by Stephen Abbott and have no idea how to do exercise 2.5.2 on page 57. The exercise is as follows: Prove that if an infinite series converges, then the associative property holds. Assume $a_1+a_2 + a_3+a_4 + a_5+\cdots$ converges to a limit $L$ (i.e., the sequence of partial sums $(s_n) \to L$). [This sentence is already confusing me; I don't understand why if $(a_n) \to L$, this implies that $(s_n) \to L$?] Show that any regrouping of the terms $$ (a_1 + a_2 + \cdots + a_{n_1}) + (a_{n_1+1} + \cdots + a_{n_2}) + (a_{n_2 + 1} + \cdots + a_{n_3}) + \cdots $$ leads to a series that also converges to $L$. Now, I'm aware that it is best to show what I've tried so far, but I have no idea how to get started. Any insight is much appreciated.","I'm self-studying from the book Understanding Analysis by Stephen Abbott and have no idea how to do exercise 2.5.2 on page 57. The exercise is as follows: Prove that if an infinite series converges, then the associative property holds. Assume $a_1+a_2 + a_3+a_4 + a_5+\cdots$ converges to a limit $L$ (i.e., the sequence of partial sums $(s_n) \to L$). [This sentence is already confusing me; I don't understand why if $(a_n) \to L$, this implies that $(s_n) \to L$?] Show that any regrouping of the terms $$ (a_1 + a_2 + \cdots + a_{n_1}) + (a_{n_1+1} + \cdots + a_{n_2}) + (a_{n_2 + 1} + \cdots + a_{n_3}) + \cdots $$ leads to a series that also converges to $L$. Now, I'm aware that it is best to show what I've tried so far, but I have no idea how to get started. Any insight is much appreciated.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
56,How prove $ \sqrt{2}+\sqrt{3}>\pi$?,How prove ?, \sqrt{2}+\sqrt{3}>\pi,How prove that $ \sqrt{2}+\sqrt{3}>\pi$? Maybe some easy way?,How prove that $ \sqrt{2}+\sqrt{3}>\pi$? Maybe some easy way?,,"['real-analysis', 'real-numbers']"
57,Polynomials and Derivatives,Polynomials and Derivatives,,"I am doing an exercise and came to a point where I'd need to know this: As a consequence of Rolle's theorem, the derivative of a function has a zero whenever our function has more than one zero. But can we say that if all the roots of a polynomial, $p(n)$, are real, all the roots of $p'(n)$ are real?","I am doing an exercise and came to a point where I'd need to know this: As a consequence of Rolle's theorem, the derivative of a function has a zero whenever our function has more than one zero. But can we say that if all the roots of a polynomial, $p(n)$, are real, all the roots of $p'(n)$ are real?",,"['calculus', 'real-analysis']"
58,Pointwise Convergence of $\sum \frac{\sin(\sqrt{n}x)}{n}$,Pointwise Convergence of,\sum \frac{\sin(\sqrt{n}x)}{n},"I am having trouble in proving the pointwise convergence of $$ g(x)=\sum_{n=1}^\infty \frac{\sin(\sqrt{n}x)}{n}$$ for all real numbers $x$ using elementary methods (e.g. integral test, Weierstrass M-test, Abel's Test, Dirichlet's Test, Comparison with Riemann sum, etc.). Can someone help me on this?","I am having trouble in proving the pointwise convergence of $$ g(x)=\sum_{n=1}^\infty \frac{\sin(\sqrt{n}x)}{n}$$ for all real numbers $x$ using elementary methods (e.g. integral test, Weierstrass M-test, Abel's Test, Dirichlet's Test, Comparison with Riemann sum, etc.). Can someone help me on this?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'trigonometric-series']"
59,Proving that $8^x+4^x\geq 5^x+6^x$ for $x\geq 0$.,Proving that  for .,8^x+4^x\geq 5^x+6^x x\geq 0,"I want to prove that $$8^x+4^x\geq 6^x+5^x$$ for all $x\geq 0$ . How can I do this? My attempt: I try by AM-GM: $$8^x+4^x\geq 2\sqrt{8^x4^x}=2(\sqrt{32})^x.$$ However, $\sqrt{32}\approx 5.5$ so I am not sure if $$2(\sqrt{32})^x\geq 5^x+6^x$$ is true. Also, I try to compute derivatives but this doesn't simplify the problem. What can I do?","I want to prove that for all . How can I do this? My attempt: I try by AM-GM: However, so I am not sure if is true. Also, I try to compute derivatives but this doesn't simplify the problem. What can I do?",8^x+4^x\geq 6^x+5^x x\geq 0 8^x+4^x\geq 2\sqrt{8^x4^x}=2(\sqrt{32})^x. \sqrt{32}\approx 5.5 2(\sqrt{32})^x\geq 5^x+6^x,"['real-analysis', 'inequality']"
60,"If $A$ is dense in $\Bbb Q$, then it must be dense in $\Bbb R$.","If  is dense in , then it must be dense in .",A \Bbb Q \Bbb R,"I have $A$ is a subset of $\mathbb{R}$ . If $A$ is dense in $\mathbb{Q}$ , then it must be dense in $\mathbb{R}$ . I am confused because $A$ is dense in $\mathbb{Q}$ . Does that imply that between any two rational numbers, there exists a real number? I understand for anything to be dense in $\Bbb R$ , there must exist something that lies between any two real numbers. However, how does knowing something is dense in $\mathbb{Q}$ prove that it must be dense in the reals? Any help is appreciated.","I have is a subset of . If is dense in , then it must be dense in . I am confused because is dense in . Does that imply that between any two rational numbers, there exists a real number? I understand for anything to be dense in , there must exist something that lies between any two real numbers. However, how does knowing something is dense in prove that it must be dense in the reals? Any help is appreciated.",A \mathbb{R} A \mathbb{Q} \mathbb{R} A \mathbb{Q} \Bbb R \mathbb{Q},"['real-analysis', 'real-numbers']"
61,Find $\lim_{n\to\infty} \cos(\frac{\pi}{4}) \cos(\frac{\pi}{8})\ldots \cos(\frac{\pi}{2^n}) $,Find,\lim_{n\to\infty} \cos(\frac{\pi}{4}) \cos(\frac{\pi}{8})\ldots \cos(\frac{\pi}{2^n}) ,I already know that $$ a_n = \cos\left(\frac{\pi}{2^{n+1}}\right) = \overbrace{\frac{\sqrt{2+\sqrt{2+\ldots + \sqrt{2}}}}{2}}^{n\text{ roots}}$$ Also I know that $$\lim_{n\to\infty}  2\cos\left(\frac{\pi}{2^n}\right) = 2 \text{ and if } a_n \xrightarrow {n\to\infty} a \text{ then } \sqrt[n]{a_1 a_2 \ldots a_n} \xrightarrow{n\to\infty} a $$ With that method I only got indeterminate form $$ \lim_{n\to\infty} \cos\left(\frac{\pi}{4}\right) \cos\left(\frac{\pi}{8}\right)\ldots \cos\left(\frac{\pi}{2^n}\right) = \Big(\frac{\sqrt[n]{a_1 a_2 \ldots a_n}}{2}\Big)^n = 1^\infty  $$ Anyone knows a working solution?,I already know that Also I know that With that method I only got indeterminate form Anyone knows a working solution?," a_n = \cos\left(\frac{\pi}{2^{n+1}}\right) = \overbrace{\frac{\sqrt{2+\sqrt{2+\ldots + \sqrt{2}}}}{2}}^{n\text{ roots}} \lim_{n\to\infty}  2\cos\left(\frac{\pi}{2^n}\right) = 2
\text{ and if } a_n \xrightarrow {n\to\infty} a \text{ then } \sqrt[n]{a_1 a_2 \ldots a_n} \xrightarrow{n\to\infty} a   \lim_{n\to\infty} \cos\left(\frac{\pi}{4}\right) \cos\left(\frac{\pi}{8}\right)\ldots \cos\left(\frac{\pi}{2^n}\right) = \Big(\frac{\sqrt[n]{a_1 a_2 \ldots a_n}}{2}\Big)^n = 1^\infty  ","['real-analysis', 'trigonometry']"
62,Existence of a sequence $\{\epsilon_n\}_{n\ge 1}$ such that $\sum\limits_{n=1}^{\infty}\frac{1}{n^{\varepsilon_n}} $ converges,Existence of a sequence  such that  converges,\{\epsilon_n\}_{n\ge 1} \sum\limits_{n=1}^{\infty}\frac{1}{n^{\varepsilon_n}} ,I am trying to find a sequence $\{\varepsilon_n\}_{n\ge 1}$  such that $$\lim_{n\to \infty}\varepsilon_n=1~~~~~~~~~~~~~\text{and}~~~~~~~~~\sum_{n=1}^{\infty}\frac{1}{n^{\varepsilon_n}} <\infty.$$ In case such sequence $\{\varepsilon_n\}_{n\ge 1}$ exists I would like to have an explicit example. Remark: This is an interesting problem since we know that for the case where  $\varepsilon_n = 1$ we have $$\sum_{n= 1}^{\infty}  \frac{1}{n^1}=\infty$$,I am trying to find a sequence $\{\varepsilon_n\}_{n\ge 1}$  such that $$\lim_{n\to \infty}\varepsilon_n=1~~~~~~~~~~~~~\text{and}~~~~~~~~~\sum_{n=1}^{\infty}\frac{1}{n^{\varepsilon_n}} <\infty.$$ In case such sequence $\{\varepsilon_n\}_{n\ge 1}$ exists I would like to have an explicit example. Remark: This is an interesting problem since we know that for the case where  $\varepsilon_n = 1$ we have $$\sum_{n= 1}^{\infty}  \frac{1}{n^1}=\infty$$,,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'harmonic-numbers']"
63,Prove the limit exists,Prove the limit exists,,"I was toying around with Abel summability when I stumbled upon a limit I could not prove existed. $$\lim_{x\to-1^+}\sum_{n=2}^\infty x^n\ln(n)\tag{$*$}$$ While it may not be clear that such a limit could converge , it may be helpful to note a similar example: $$\lim_{x\to-1^+}\sum_{n=1}^\infty nx^{n-1}=\lim_{x\to-1^+}\frac1{(1-x)^2}=\frac14$$ However, a lack of closed form of $(*)$ makes it difficult for me to show it converges.  WolframAlpha returns the series as a derivative of the Lerchphi function, though it doesn't seem quite helpful. By considering $$f_k(x)=\sum_{n=2}^k x^n\ln(n)$$ I find that $$f_{35}(-0.75)-0.25f'_{35}(-0.75)=0.225803586648$$ Which is a quick linear approximation of $f_{35}(x)$ centered at $x=-\frac34$.  This agrees with what I think to be the limit: $$\lim_{x\to-1^+}\sum_{n=2}^\infty x^n\ln(n)\stackrel?=\eta'(0)=\frac12\ln\left(\frac\pi2\right)=0.225791352645\tag{$**$}$$ Where $\eta(s)$ is the Dirichlet eta function . I also tried considering more elementary approaches to showing the limit exists, such as using $\ln(n+1)=\ln(n)+\mathcal O(n^{-1})$, however, I could not make use of it. Bonus points if you can prove $(**)$.","I was toying around with Abel summability when I stumbled upon a limit I could not prove existed. $$\lim_{x\to-1^+}\sum_{n=2}^\infty x^n\ln(n)\tag{$*$}$$ While it may not be clear that such a limit could converge , it may be helpful to note a similar example: $$\lim_{x\to-1^+}\sum_{n=1}^\infty nx^{n-1}=\lim_{x\to-1^+}\frac1{(1-x)^2}=\frac14$$ However, a lack of closed form of $(*)$ makes it difficult for me to show it converges.  WolframAlpha returns the series as a derivative of the Lerchphi function, though it doesn't seem quite helpful. By considering $$f_k(x)=\sum_{n=2}^k x^n\ln(n)$$ I find that $$f_{35}(-0.75)-0.25f'_{35}(-0.75)=0.225803586648$$ Which is a quick linear approximation of $f_{35}(x)$ centered at $x=-\frac34$.  This agrees with what I think to be the limit: $$\lim_{x\to-1^+}\sum_{n=2}^\infty x^n\ln(n)\stackrel?=\eta'(0)=\frac12\ln\left(\frac\pi2\right)=0.225791352645\tag{$**$}$$ Where $\eta(s)$ is the Dirichlet eta function . I also tried considering more elementary approaches to showing the limit exists, such as using $\ln(n+1)=\ln(n)+\mathcal O(n^{-1})$, however, I could not make use of it. Bonus points if you can prove $(**)$.",,"['real-analysis', 'sequences-and-series', 'limits']"
64,Foundation of ordering of real numbers,Foundation of ordering of real numbers,,"This might be a silly question, but what is the mathematical foundation for the ordering of the real numbers? How do we know that $1<2$ or $300<1000$... Are the real numbers simply defined as being ordered in this way by construction? Thanks for any contribution.","This might be a silly question, but what is the mathematical foundation for the ordering of the real numbers? How do we know that $1<2$ or $300<1000$... Are the real numbers simply defined as being ordered in this way by construction? Thanks for any contribution.",,"['real-analysis', 'order-theory']"
65,"For any given function $f\colon [0,1]\to\Bbb R$, what is $\int_0^1\frac{f(x)}{f(x)+f(1-x)}dx$?","For any given function , what is ?","f\colon [0,1]\to\Bbb R \int_0^1\frac{f(x)}{f(x)+f(1-x)}dx","I have a general function assuming the following integral does exist $$\int_0^1\frac{f(x)}{f(x)+f(1-x)}dx.$$ How do I solve it? I have tried to split it up from $0$ to $0.5$ and from $0.5$ to $1$, but I don't know what to do next. Thanks for help.","I have a general function assuming the following integral does exist $$\int_0^1\frac{f(x)}{f(x)+f(1-x)}dx.$$ How do I solve it? I have tried to split it up from $0$ to $0.5$ and from $0.5$ to $1$, but I don't know what to do next. Thanks for help.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'closed-form']"
66,Every matrix is a limit of a sequence of invertible matrices.,Every matrix is a limit of a sequence of invertible matrices.,,"I'm trying to prove that every matrix $n\times n$ is a limit of a sequence of invertible matrices $n\times n$. I know this intuitively, but I couldn't prove formally. I'm trying to prove this using the fact the set of the non-invertible matrices is closed. Am I on the right way? Thanks in advance","I'm trying to prove that every matrix $n\times n$ is a limit of a sequence of invertible matrices $n\times n$. I know this intuitively, but I couldn't prove formally. I'm trying to prove this using the fact the set of the non-invertible matrices is closed. Am I on the right way? Thanks in advance",,"['real-analysis', 'linear-algebra']"
67,"Prove that $f(x)\equiv0$ on $\left[0,1\right]$ if $f(0)=0$ and $|f'(x)|\le|f(x)|$",Prove that  on  if  and,"f(x)\equiv0 \left[0,1\right] f(0)=0 |f'(x)|\le|f(x)|","Let $f(x)$ differentiable on $\left[0,1\right]$ such that $f(0) = 0$.  Also, assuming that $\forall x\in \left[0,1\right]:\left|f'(x)\right| \le \left|f(x)\right|$. Prove that $f(x)\equiv 0$ What I did so far: Using LMVT: $$\frac{f(x)-f(0)}{x-0} = \frac{f(x)}{x} = f'(c)$$ where $c\in \left(0,x\right)$ $$\left|f('c)\right| = \left|\frac{f(x)}{x}\right|.$$ Since, $\left|x\right| \in (0,1)$ $$\left|\frac{f(x)}{x}\right| \ge \left|f(x)\right| \ge \left|f'(x)\right|$$ So, $$\left|f('c)\right| \ge \left|f'(x)\right|$$ But I'm not sure how is it helping me. What am I missing?","Let $f(x)$ differentiable on $\left[0,1\right]$ such that $f(0) = 0$.  Also, assuming that $\forall x\in \left[0,1\right]:\left|f'(x)\right| \le \left|f(x)\right|$. Prove that $f(x)\equiv 0$ What I did so far: Using LMVT: $$\frac{f(x)-f(0)}{x-0} = \frac{f(x)}{x} = f'(c)$$ where $c\in \left(0,x\right)$ $$\left|f('c)\right| = \left|\frac{f(x)}{x}\right|.$$ Since, $\left|x\right| \in (0,1)$ $$\left|\frac{f(x)}{x}\right| \ge \left|f(x)\right| \ge \left|f'(x)\right|$$ So, $$\left|f('c)\right| \ge \left|f'(x)\right|$$ But I'm not sure how is it helping me. What am I missing?",,"['calculus', 'real-analysis', 'functions', 'derivatives']"
68,Integral $\int_0^{\pi/4}\log \tan x \frac{\cos 2x}{1+\alpha^2\sin^2 2x}dx=-\frac{\pi}{4\alpha}\text{arcsinh}\alpha$,Integral,\int_0^{\pi/4}\log \tan x \frac{\cos 2x}{1+\alpha^2\sin^2 2x}dx=-\frac{\pi}{4\alpha}\text{arcsinh}\alpha,"Hi I am trying to prove this $$ I:=\int_0^{\pi/4}\log\left(\tan\left(x\right)\right)\, \frac{\cos\left(2x\right)}{1+\alpha^{2}\sin^{2}\left(2x\right)}\,{\rm d}x =-\,\frac{\pi}{4\alpha}\,\text{arcsinh}\left(\alpha\right),\qquad \alpha^2<1. $$ What an amazing result this is!  I tried to write $$ I=\int_0^{\pi/4} \log  \sin x\frac{\cos 2x}{1+\alpha^2\sin^2 2x}-\int_0^{\pi/4}\log \cos x \frac{\cos 2x}{1+\alpha^2\sin^2 2x}dx  $$ and played around enough here to realize it probably isn't the best idea. Now back to the original integral I, we can possibly change variables $y=\tan x$ and re-writing the original integral to obtain $$ \int_0^{\pi/4}\log \tan x \frac{\cos 2x}{1+{\alpha^2}\big(1-\cos^2 (2x)\big)}dx=\int_0^1 \log y \frac{1-y^2}{1+y^2}\frac{1}{1+{\alpha^2}\big(1-(\frac{1-y^2}{1+y^2})^2\big)}\frac{dy}{1+y^2}. $$ Simplifying this we have $$ I=\int_0^1\log y \frac{1-y^2}{1+y^2}\frac{(1+y^2)^2}{(1+y^2)^2+4\alpha^2y^2}\frac{dy}{1+y^2}=\int_0^1\log  y \frac{1-y^2}{(1+y^2)^2+4\alpha^2y^2}dy $$ Another change of variables $y=e^{-t}$ and we have $$ I=-\int_0^\infty \frac{t(1-e^{-2t})}{(1+e^{-2t})^2+4\alpha^2 e^{-2t}} e^{-t}dt $$ but this is where I am stuck...How can we calculate I?  Thanks.","Hi I am trying to prove this $$ I:=\int_0^{\pi/4}\log\left(\tan\left(x\right)\right)\, \frac{\cos\left(2x\right)}{1+\alpha^{2}\sin^{2}\left(2x\right)}\,{\rm d}x =-\,\frac{\pi}{4\alpha}\,\text{arcsinh}\left(\alpha\right),\qquad \alpha^2<1. $$ What an amazing result this is!  I tried to write $$ I=\int_0^{\pi/4} \log  \sin x\frac{\cos 2x}{1+\alpha^2\sin^2 2x}-\int_0^{\pi/4}\log \cos x \frac{\cos 2x}{1+\alpha^2\sin^2 2x}dx  $$ and played around enough here to realize it probably isn't the best idea. Now back to the original integral I, we can possibly change variables $y=\tan x$ and re-writing the original integral to obtain $$ \int_0^{\pi/4}\log \tan x \frac{\cos 2x}{1+{\alpha^2}\big(1-\cos^2 (2x)\big)}dx=\int_0^1 \log y \frac{1-y^2}{1+y^2}\frac{1}{1+{\alpha^2}\big(1-(\frac{1-y^2}{1+y^2})^2\big)}\frac{dy}{1+y^2}. $$ Simplifying this we have $$ I=\int_0^1\log y \frac{1-y^2}{1+y^2}\frac{(1+y^2)^2}{(1+y^2)^2+4\alpha^2y^2}\frac{dy}{1+y^2}=\int_0^1\log  y \frac{1-y^2}{(1+y^2)^2+4\alpha^2y^2}dy $$ Another change of variables $y=e^{-t}$ and we have $$ I=-\int_0^\infty \frac{t(1-e^{-2t})}{(1+e^{-2t})^2+4\alpha^2 e^{-2t}} e^{-t}dt $$ but this is where I am stuck...How can we calculate I?  Thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
69,Bounded sequence with divergent Cesaro means,Bounded sequence with divergent Cesaro means,,"Is there a bounded real-valued sequence with divergent Cesaro means (i.e. not Cesaro summable)? More specifically, is there a bounded sequence $\{w_k\}\in l^\infty$ such that $$\lim_{M\rightarrow\infty} \frac{\sum_{k=1}^M w_k}{M}$$ does not exist? I encountered this problem while studying the limit of average payoffs criterion for ranking payoff sequences in infinitely repeated games.","Is there a bounded real-valued sequence with divergent Cesaro means (i.e. not Cesaro summable)? More specifically, is there a bounded sequence $\{w_k\}\in l^\infty$ such that $$\lim_{M\rightarrow\infty} \frac{\sum_{k=1}^M w_k}{M}$$ does not exist? I encountered this problem while studying the limit of average payoffs criterion for ranking payoff sequences in infinitely repeated games.",,"['real-analysis', 'game-theory', 'cesaro-summable']"
70,Uniform Convergence of integrals,Uniform Convergence of integrals,,"If a sequence of functions $f_n$ are uniformly convergent in a given interval $[a,b]$ to a function $f$, are all riemann integrable, then the integral $$\int ^b_af_ndx\rightarrow\int^b_afdx$$ and $f$ is riemann integrable. but is the convergence uniform? More precisely, is it true that $\forall \epsilon >0, \exists N:\forall n>N$, $$\left| \int ^b_af_ndx-\int^b_afdx\right|<\epsilon$$ How would I prove it? Or is it obvious?","If a sequence of functions $f_n$ are uniformly convergent in a given interval $[a,b]$ to a function $f$, are all riemann integrable, then the integral $$\int ^b_af_ndx\rightarrow\int^b_afdx$$ and $f$ is riemann integrable. but is the convergence uniform? More precisely, is it true that $\forall \epsilon >0, \exists N:\forall n>N$, $$\left| \int ^b_af_ndx-\int^b_afdx\right|<\epsilon$$ How would I prove it? Or is it obvious?",,"['calculus', 'real-analysis', 'analysis']"
71,Difference between an injective f and monotonic f?,Difference between an injective f and monotonic f?,,What is the difference between an injective function and a monotonic function? An injection is a function where its values only can be occurred once ($f(a)=f(b) \Rightarrow  a=b$). This means that a function is either decreasing or increasing. Isn't this the same for a monotonic function?,What is the difference between an injective function and a monotonic function? An injection is a function where its values only can be occurred once ($f(a)=f(b) \Rightarrow  a=b$). This means that a function is either decreasing or increasing. Isn't this the same for a monotonic function?,,['real-analysis']
72,"Sum of a rearranged alternating harmonic series, with three positive terms followed by one negative term","Sum of a rearranged alternating harmonic series, with three positive terms followed by one negative term",,"The series is this: $$ 1 + 1/3 + 1/5 - 1/2 + 1/7 + 1/9 + 1/11 - 1/4 + 1/13 + 1/15 + 1/17 - 1/6 ...$$ The hint is to consider partial sums to $4n$ terms. I did that, and got the summation $$ \sum 1/(6x-5) + 1/(6x-3) + 1/(6x-1) - 1/(2x) $$ But then I got stuck. The terms don't seem to simplify to anything... Wolfram alpha gives the result of this summation as $\log(12)/2$, but I have no idea how to get there. Any help greatly appreciated.","The series is this: $$ 1 + 1/3 + 1/5 - 1/2 + 1/7 + 1/9 + 1/11 - 1/4 + 1/13 + 1/15 + 1/17 - 1/6 ...$$ The hint is to consider partial sums to $4n$ terms. I did that, and got the summation $$ \sum 1/(6x-5) + 1/(6x-3) + 1/(6x-1) - 1/(2x) $$ But then I got stuck. The terms don't seem to simplify to anything... Wolfram alpha gives the result of this summation as $\log(12)/2$, but I have no idea how to get there. Any help greatly appreciated.",,"['real-analysis', 'sequences-and-series', 'analysis']"
73,"If derivative of a function is the zero function in $\mathbb R^n$, then the function is constant when the domain is path-connected","If derivative of a function is the zero function in , then the function is constant when the domain is path-connected",\mathbb R^n,"Some definitions first. Let $A \subseteq \mathbb R^n$. Let $x,y \in A$. A path between $x$ and $y$ is a continuous function $f: [0,1] \rightarrow \mathbb{R}^n$ with $f(0) = x$ and $f(1) = y$.  The set $A$ is path-connected when for every $x, y \in A$, there exists a $C^1$ path between $x$ and $y$. Let $f: A \rightarrow \mathbb{R}^m$ be a function, with $A \subseteq \mathbb{R}^n$. Suppose that $f'(a) = 0$ for all $a \in A$. Now if $A$ is path-connected, then $f$ is constant. In a proof I saw of this theorem the property that every path between two points is $C^1$ is used. My question is: is this necessary? If so, I'd like to see a counterexample. In other words, I'm looking for a function $f: A \rightarrow \mathbb{R}^m$ with zero derivative everywhere, $A$ such that there is a path between any two points (but the path is not necessarily $C^1$) and $f$ is NOT constant.","Some definitions first. Let $A \subseteq \mathbb R^n$. Let $x,y \in A$. A path between $x$ and $y$ is a continuous function $f: [0,1] \rightarrow \mathbb{R}^n$ with $f(0) = x$ and $f(1) = y$.  The set $A$ is path-connected when for every $x, y \in A$, there exists a $C^1$ path between $x$ and $y$. Let $f: A \rightarrow \mathbb{R}^m$ be a function, with $A \subseteq \mathbb{R}^n$. Suppose that $f'(a) = 0$ for all $a \in A$. Now if $A$ is path-connected, then $f$ is constant. In a proof I saw of this theorem the property that every path between two points is $C^1$ is used. My question is: is this necessary? If so, I'd like to see a counterexample. In other words, I'm looking for a function $f: A \rightarrow \mathbb{R}^m$ with zero derivative everywhere, $A$ such that there is a path between any two points (but the path is not necessarily $C^1$) and $f$ is NOT constant.",,"['real-analysis', 'general-topology', 'multivariable-calculus']"
74,"Evaluate $\int_0^{\pi/2} \frac{\arctan{\left(\frac{2\sin{x}}{2\cos{x}-1}\right)}\sin{\left(\frac{x}{2}\right)}}{\sqrt{\cos{x}}} \, \mathrm{d}x$",Evaluate,"\int_0^{\pi/2} \frac{\arctan{\left(\frac{2\sin{x}}{2\cos{x}-1}\right)}\sin{\left(\frac{x}{2}\right)}}{\sqrt{\cos{x}}} \, \mathrm{d}x","Evaluate: $$\int_0^{\frac{\pi}{2}} \frac{\arctan{\left(\frac{2\sin{x}}{2\cos{x}-1}\right)}\sin{\left(\frac{x}{2}\right)}}{\sqrt{\cos{x}}} \, \mathrm{d}x$$ I believe there is a ""nice"" closed form solution but Wolfram is too weak.  These arctan integrals are so tricky!  I sense a substitution like $\sin{\frac{x}{2}}$ because of arctan argument and $\sqrt{\cos{x}}$ but I just cant get it.  Any ideas or tips please. Source: https://tieba.baidu.com/p/4794735082 (Exercise 3.1.22).","Evaluate: I believe there is a ""nice"" closed form solution but Wolfram is too weak.  These arctan integrals are so tricky!  I sense a substitution like because of arctan argument and but I just cant get it.  Any ideas or tips please. Source: https://tieba.baidu.com/p/4794735082 (Exercise 3.1.22).","\int_0^{\frac{\pi}{2}} \frac{\arctan{\left(\frac{2\sin{x}}{2\cos{x}-1}\right)}\sin{\left(\frac{x}{2}\right)}}{\sqrt{\cos{x}}} \, \mathrm{d}x \sin{\frac{x}{2}} \sqrt{\cos{x}}","['real-analysis', 'calculus']"
75,Asymptotic approximation regarding the Gamma function $\Gamma$.,Asymptotic approximation regarding the Gamma function .,\Gamma,On the wikipedia page for Gamma function I saw an interesting formula $$ \lim_{n\to \infty} \frac{\Gamma(n+\alpha)}{\Gamma(n)n^\alpha} = 1 $$ for all $\alpha\in\Bbb C$ . I couldn't find the source of this and searching here in MSE didn't provide the result I want. Could anyone show me how this formula is derived? I'm very inexperienced with properties/identities of $\Gamma$ so forgive me if this question is trivial.,On the wikipedia page for Gamma function I saw an interesting formula for all . I couldn't find the source of this and searching here in MSE didn't provide the result I want. Could anyone show me how this formula is derived? I'm very inexperienced with properties/identities of so forgive me if this question is trivial.,"
\lim_{n\to \infty} \frac{\Gamma(n+\alpha)}{\Gamma(n)n^\alpha} = 1
 \alpha\in\Bbb C \Gamma","['real-analysis', 'number-theory', 'special-functions', 'gamma-function']"
76,"Show that $\int_0^1 \prod_{n\geq 1} (1-x^n) \, dx = \frac{4\pi\sqrt{3}\sinh(\frac{\pi}{3}\sqrt{23})}{\sqrt{23}\cosh(\frac{\pi}{2}\sqrt{23})}$ [duplicate]",Show that  [duplicate],"\int_0^1 \prod_{n\geq 1} (1-x^n) \, dx = \frac{4\pi\sqrt{3}\sinh(\frac{\pi}{3}\sqrt{23})}{\sqrt{23}\cosh(\frac{\pi}{2}\sqrt{23})}","This question already has answers here : An integration of product $(1-x^n)$ (2 answers) Closed 5 years ago . $$\int_0^1 \prod_{n\geq 1} (1-x^n) \, dx = \frac{4\pi\sqrt{3}\sinh(\frac{\pi}{3}\sqrt{23})}{\sqrt{23}\cosh(\frac{\pi}{2}\sqrt{23})}$$ This monstrous expression is from Tolaso Network (tolaso.com.gr). I have no idea how to approach it - converting the product to a sum of logarithms does no good, and one cannot switch the order of product/integral either. The product in itself doesn't converge to anything nice either. I am interested in seeing the proof of the above identity, as well as an explanation of how exactly $\sqrt{23}$ becomes involved in such a deceptive integral. Both real and complex analytic solutions are welcome. A proof without the pentagonal number theorem would be nice as well, since that somewhat trivializes the problem.","This question already has answers here : An integration of product $(1-x^n)$ (2 answers) Closed 5 years ago . This monstrous expression is from Tolaso Network (tolaso.com.gr). I have no idea how to approach it - converting the product to a sum of logarithms does no good, and one cannot switch the order of product/integral either. The product in itself doesn't converge to anything nice either. I am interested in seeing the proof of the above identity, as well as an explanation of how exactly becomes involved in such a deceptive integral. Both real and complex analytic solutions are welcome. A proof without the pentagonal number theorem would be nice as well, since that somewhat trivializes the problem.","\int_0^1 \prod_{n\geq 1} (1-x^n) \, dx = \frac{4\pi\sqrt{3}\sinh(\frac{\pi}{3}\sqrt{23})}{\sqrt{23}\cosh(\frac{\pi}{2}\sqrt{23})} \sqrt{23}","['real-analysis', 'integration', 'sequences-and-series', 'definite-integrals', 'infinite-product']"
77,"A log arctan integral $\int_0^1 \log x \arctan^2 x \, dx$",A log arctan integral,"\int_0^1 \log x \arctan^2 x \, dx","Here is an integral that arose while solving another problem. Can we express $$\mathcal{J}=\int_0^1 \log x \arctan^2 x \, {\rm d}x$$ in terms of known mathematical constants or special functions? I had a couple of ideas for this one. For example begin by parts , that is: \begin{align*} \int_{0}^{1} \log x \arctan^2 x\, {\rm d}x &= \left [ \left ( x \log x - x \right )\arctan^2 x \right ]_0^1 - \int_{0}^{1}\left ( x \log x - x \right )\arctan^2 x \, {\rm d}x \\   &=-\frac{\pi^2}{16} - \int_{0}^{1}x \log x \arctan^2 x \, {\rm d}x + \int_{0}^{1}x \arctan^2 x \, {\rm d}x \\   &= - \frac{\pi^2}{16} - \int_{0}^{1}x \log x \arctan^2 x \, {\rm d}x -\frac{\pi}{4}+\frac{\pi^2}{16} +\frac{\log 256}{16} \end{align*} since the RHS integral is trivial. We can even find an elementary antiderivative. The problem is with the second. An idea that pumped to me while writing down my thoughts is that probably the easiest way of computing is by trying to evaluate the integral $$\int_{0}^{1} t^{s-1} \arctan^2 x \, {\rm d}x$$ Of course $s$ is imposed on restrictions that are yet unknown to me. Playing around I see that this method is fruitless since: $$\int_{0}^{1}x^{s-1} \arctan^2 x \, {\rm d}x = \left [ \frac{x^s}{s} \arctan^2 x  \right ]_0^1  - \frac{2}{s}\int_{0}^{1} \frac{x^{s-1} \arctan x}{x^2+1} \, {\rm d}x$$ and if someone tries to apply parts again at the second integral beginning with the rational function then he encounters hypergeometrics. I don't know any of that. So, any help is welcome!","Here is an integral that arose while solving another problem. Can we express $$\mathcal{J}=\int_0^1 \log x \arctan^2 x \, {\rm d}x$$ in terms of known mathematical constants or special functions? I had a couple of ideas for this one. For example begin by parts , that is: \begin{align*} \int_{0}^{1} \log x \arctan^2 x\, {\rm d}x &= \left [ \left ( x \log x - x \right )\arctan^2 x \right ]_0^1 - \int_{0}^{1}\left ( x \log x - x \right )\arctan^2 x \, {\rm d}x \\   &=-\frac{\pi^2}{16} - \int_{0}^{1}x \log x \arctan^2 x \, {\rm d}x + \int_{0}^{1}x \arctan^2 x \, {\rm d}x \\   &= - \frac{\pi^2}{16} - \int_{0}^{1}x \log x \arctan^2 x \, {\rm d}x -\frac{\pi}{4}+\frac{\pi^2}{16} +\frac{\log 256}{16} \end{align*} since the RHS integral is trivial. We can even find an elementary antiderivative. The problem is with the second. An idea that pumped to me while writing down my thoughts is that probably the easiest way of computing is by trying to evaluate the integral $$\int_{0}^{1} t^{s-1} \arctan^2 x \, {\rm d}x$$ Of course $s$ is imposed on restrictions that are yet unknown to me. Playing around I see that this method is fruitless since: $$\int_{0}^{1}x^{s-1} \arctan^2 x \, {\rm d}x = \left [ \frac{x^s}{s} \arctan^2 x  \right ]_0^1  - \frac{2}{s}\int_{0}^{1} \frac{x^{s-1} \arctan x}{x^2+1} \, {\rm d}x$$ and if someone tries to apply parts again at the second integral beginning with the rational function then he encounters hypergeometrics. I don't know any of that. So, any help is welcome!",,"['real-analysis', 'definite-integrals']"
78,"Is $(a,a]=\{\emptyset\}$?",Is ?,"(a,a]=\{\emptyset\}","Let $a \in \mathbb{R}$, and consider the half open interval $(a,a]$. Is it correct to write this half open interval as $(a,a]=\{\emptyset \}$? Or $(a,a]=\{a \}$?","Let $a \in \mathbb{R}$, and consider the half open interval $(a,a]$. Is it correct to write this half open interval as $(a,a]=\{\emptyset \}$? Or $(a,a]=\{a \}$?",,"['real-analysis', 'elementary-set-theory', 'convention']"
79,Fourier transform of a radial function,Fourier transform of a radial function,,"Consider a function $f \in L^2(\mathbb{R}^n)$ such that $f$ is radial. My question is, is the Fourier transform $\hat{f}(\xi)$ automatically radial (I can see it is even in each variable $x_i$), or we need some conditions on $f$? Thanks for your help.","Consider a function $f \in L^2(\mathbb{R}^n)$ such that $f$ is radial. My question is, is the Fourier transform $\hat{f}(\xi)$ automatically radial (I can see it is even in each variable $x_i$), or we need some conditions on $f$? Thanks for your help.",,"['real-analysis', 'functional-analysis', 'reference-request', 'fourier-analysis']"
80,What's equicontinuous? What's uniform equicontinuous? What's pointwise equicontinuous?,What's equicontinuous? What's uniform equicontinuous? What's pointwise equicontinuous?,,"I have some problem about my homework. Is the sequence of function $f_n:\mathbb R \to \Bbb R$ defined by     $$f_n(x) = cos(n+x) + log(1+\frac{1}{\sqrt{n+2}}sin^2(n^nx))$$equicountinuous? prove or disprove. But I have checked the textbook, which says A sequence of functions ($f_n$) in $C^0$ is equicontinuous if     $$\forall \epsilon >0, \exists\delta>0 $$ such that $$|s-t|<\delta, n\in \Bbb N \implies |f_n(s)-f_n(t)|<\epsilon$$For total clarity, the concept might better be labeled uniform equicontinuity, in contrast to pointwise equicontinuity, which requires     $$\forall\epsilon>0\forall x\in [a,b],\exists\delta>0 $$such that $$|x-t|<\delta,n\in\Bbb N \implies|f_n(x)-f_n(t)|<\epsilon$$ Now my question is, what the difference between pointwise-equicontinuous and uniform-equicontinuous? Can I have some examples? And which does it usually means when only mentioned equicontinuous?","I have some problem about my homework. Is the sequence of function $f_n:\mathbb R \to \Bbb R$ defined by     $$f_n(x) = cos(n+x) + log(1+\frac{1}{\sqrt{n+2}}sin^2(n^nx))$$equicountinuous? prove or disprove. But I have checked the textbook, which says A sequence of functions ($f_n$) in $C^0$ is equicontinuous if     $$\forall \epsilon >0, \exists\delta>0 $$ such that $$|s-t|<\delta, n\in \Bbb N \implies |f_n(s)-f_n(t)|<\epsilon$$For total clarity, the concept might better be labeled uniform equicontinuity, in contrast to pointwise equicontinuity, which requires     $$\forall\epsilon>0\forall x\in [a,b],\exists\delta>0 $$such that $$|x-t|<\delta,n\in\Bbb N \implies|f_n(x)-f_n(t)|<\epsilon$$ Now my question is, what the difference between pointwise-equicontinuous and uniform-equicontinuous? Can I have some examples? And which does it usually means when only mentioned equicontinuous?",,"['real-analysis', 'continuity']"
81,Finding $\lim_{x\to 0} \frac{(1+\tan x)^{\frac{1}{x}}-e}{x}$,Finding,\lim_{x\to 0} \frac{(1+\tan x)^{\frac{1}{x}}-e}{x},"How would I go about solving this following limit? $$\lim_{x\to 0} \frac{(1+\tan x)^{\frac{1}{x}}-e}{x}$$ My attempts: Direct substitution yields the limit to be undefined, also ruling out the possibility of using L'Hospital's Rule. I don't see any clever substitutions that can be made with this limit. Would squeeze theorem help here? Maybe using the trig. identities: $$-1 \le \cos x \le 1$$ and $$-1 \le \cos x \le 1$$ EDIT I attempted to break the limit down term by term. So, for the first one: $$y = \lim_{x\to 0} (1 + \tan x)^{1/x}$$ Taking the natural log of both sides: $$\ln y = \lim_{x\to 0} \frac{\ln(1+\tan x)}{x}$$ Direct sub. yields $0/0$. Using L'Hospital's rule: $$\ln y = \lim_{x\to 0} \frac{\frac{\sec^2{x}}{1+\tan x}}{1} = \frac{\sec^2{x}}{1+\tan x} = 1$$ Thus, $\ln y = 1$, so $y= e$ EDIT #2 Thanks to a random comment, it actually does help me: $$\lim_{x\to 0} \frac{(1+\tan x)^{\frac{1}{x}}-e}{x}$$ $$\lim_{x\to 0} \frac{e-e}{0} = \frac{0}{0}$$ Thus, we can use L'Hospitals here: $$\lim_{x\to 0} \frac{(\tan x+1)^{1/x} \left(\frac{\sec^2 x}{x(\tan(x)+1)}-\frac{\ln(\tan(x)+1))}{x^2}\right)}{1} = (\tan x+1)^{1/x} \left(\frac{\sec^2 x}{x(\tan(x)+1)}-\frac{\ln(\tan(x)+1))}{x^2}\right)$$ I haven't made any further progress, sadly. Any help would be appreciated.","How would I go about solving this following limit? $$\lim_{x\to 0} \frac{(1+\tan x)^{\frac{1}{x}}-e}{x}$$ My attempts: Direct substitution yields the limit to be undefined, also ruling out the possibility of using L'Hospital's Rule. I don't see any clever substitutions that can be made with this limit. Would squeeze theorem help here? Maybe using the trig. identities: $$-1 \le \cos x \le 1$$ and $$-1 \le \cos x \le 1$$ EDIT I attempted to break the limit down term by term. So, for the first one: $$y = \lim_{x\to 0} (1 + \tan x)^{1/x}$$ Taking the natural log of both sides: $$\ln y = \lim_{x\to 0} \frac{\ln(1+\tan x)}{x}$$ Direct sub. yields $0/0$. Using L'Hospital's rule: $$\ln y = \lim_{x\to 0} \frac{\frac{\sec^2{x}}{1+\tan x}}{1} = \frac{\sec^2{x}}{1+\tan x} = 1$$ Thus, $\ln y = 1$, so $y= e$ EDIT #2 Thanks to a random comment, it actually does help me: $$\lim_{x\to 0} \frac{(1+\tan x)^{\frac{1}{x}}-e}{x}$$ $$\lim_{x\to 0} \frac{e-e}{0} = \frac{0}{0}$$ Thus, we can use L'Hospitals here: $$\lim_{x\to 0} \frac{(\tan x+1)^{1/x} \left(\frac{\sec^2 x}{x(\tan(x)+1)}-\frac{\ln(\tan(x)+1))}{x^2}\right)}{1} = (\tan x+1)^{1/x} \left(\frac{\sec^2 x}{x(\tan(x)+1)}-\frac{\ln(\tan(x)+1))}{x^2}\right)$$ I haven't made any further progress, sadly. Any help would be appreciated.",,"['calculus', 'real-analysis', 'limits', 'trigonometry']"
82,"Proving that $x_n\to L$ implies $|x_n|\to |L|$, and what about the converse?","Proving that  implies , and what about the converse?",x_n\to L |x_n|\to |L|,"Problem 3. Show that for a sequence $(x_n)$ the following are true: (i) $\lim x_n=0$ if and only if $\lim |x_n|=0$. (ii) $\lim x_n=L$ implies $\lim |x_n|=|L|$. Is the converse true? Prove or give a counterexample. (i) is already done, easy. I'm halfway done with (ii), I split it into three cases: $L = 0$, $L > 0$ and $L < 0$. For $L = 0$ I just refer to part (i). For $L > 0$, we know that $|x_n - L| < \epsilon$ if lim $|x_n|$ = $|L|$, then we must have $||x_n| -|L|| < \epsilon$ but by the reverse triangle equality, $||x_n| - |L|| < |x_n - L|$ so clearly $||x_n|-|L|| < \epsilon$, thus lim $x_n = L$ $\implies$ lim $|x_n| = |L|$ for $L > 0$ for some reason I'm confused as to part three, $L<0$. I'm having trouble seeing the difference between $L>0$ and $L<0$, and i'm starting to think that splitting it up like that isn't necessary at all.","Problem 3. Show that for a sequence $(x_n)$ the following are true: (i) $\lim x_n=0$ if and only if $\lim |x_n|=0$. (ii) $\lim x_n=L$ implies $\lim |x_n|=|L|$. Is the converse true? Prove or give a counterexample. (i) is already done, easy. I'm halfway done with (ii), I split it into three cases: $L = 0$, $L > 0$ and $L < 0$. For $L = 0$ I just refer to part (i). For $L > 0$, we know that $|x_n - L| < \epsilon$ if lim $|x_n|$ = $|L|$, then we must have $||x_n| -|L|| < \epsilon$ but by the reverse triangle equality, $||x_n| - |L|| < |x_n - L|$ so clearly $||x_n|-|L|| < \epsilon$, thus lim $x_n = L$ $\implies$ lim $|x_n| = |L|$ for $L > 0$ for some reason I'm confused as to part three, $L<0$. I'm having trouble seeing the difference between $L>0$ and $L<0$, and i'm starting to think that splitting it up like that isn't necessary at all.",,"['calculus', 'real-analysis', 'absolute-value']"
83,"Show that if $f$ is integrable on $[a,b]$, then $|f|$ is also integrable.","Show that if  is integrable on , then  is also integrable.","f [a,b] |f|","The problem suggests doing it by showing that $U(P,|f|) - L(P,|f|) \le U(P,f)-L(P,f)$ for some partition $P$ .  I can get the other steps after that, but I've tried proving this inequality on my own and multiple tutors at my university couldn't figure it out using the material in my book.  Every time I've attempted it on my own, I've gotten the arrow in the opposite direction.  Doesn't $\displaystyle\sup_{i \in [x_{i-1},x_i]} |f(x)| \ge \displaystyle\sup_{i \in [x_{i-1},x_i]} f(x)$ hold? Note that my class is using a pretty simplified Analysis (question 5.5.2a) book that doesn't cover metric spaces or Lebesgue integrals.  Everything is Riemann and we show a function is Riemann integrable if and only if its upper and lower Darboux integrals are equal.","The problem suggests doing it by showing that for some partition .  I can get the other steps after that, but I've tried proving this inequality on my own and multiple tutors at my university couldn't figure it out using the material in my book.  Every time I've attempted it on my own, I've gotten the arrow in the opposite direction.  Doesn't hold? Note that my class is using a pretty simplified Analysis (question 5.5.2a) book that doesn't cover metric spaces or Lebesgue integrals.  Everything is Riemann and we show a function is Riemann integrable if and only if its upper and lower Darboux integrals are equal.","U(P,|f|) - L(P,|f|) \le U(P,f)-L(P,f) P \displaystyle\sup_{i \in [x_{i-1},x_i]} |f(x)| \ge \displaystyle\sup_{i \in [x_{i-1},x_i]} f(x)","['real-analysis', 'integration', 'absolute-value', 'riemann-integration']"
84,Evaluating: $\lim\limits_{x\to0}\left(\frac{\sin x}{x}\right)^{{6}/{x^{2}}}$,Evaluating:,\lim\limits_{x\to0}\left(\frac{\sin x}{x}\right)^{{6}/{x^{2}}},I am trying to evaluate the following but without result. $$\lim_{x\to0}\left(\frac{\sin x}{x}\right)^{{6}/{x^{2}}}$$ Can you please give me some hints? I have tried to put log to both sides but it hasn't lead me somewhere... Thanks a lot,I am trying to evaluate the following but without result. $$\lim_{x\to0}\left(\frac{\sin x}{x}\right)^{{6}/{x^{2}}}$$ Can you please give me some hints? I have tried to put log to both sides but it hasn't lead me somewhere... Thanks a lot,,"['calculus', 'real-analysis', 'analysis', 'limits']"
85,Prove that $f$ has finite number of roots,Prove that  has finite number of roots,f,"Let $f:[0,1]\to \mathbb{R}$ be a differentiable function. If there do not exist any $x\in[0,1]$ such that $f(x)=f'(x)=0$, prove that $f$ has only finite number of zeros in $[0,1]$. I'm not getting any idea how to proceed.","Let $f:[0,1]\to \mathbb{R}$ be a differentiable function. If there do not exist any $x\in[0,1]$ such that $f(x)=f'(x)=0$, prove that $f$ has only finite number of zeros in $[0,1]$. I'm not getting any idea how to proceed.",,"['real-analysis', 'functions', 'roots']"
86,"the value of $\lim\limits_{n\rightarrow\infty}n^2\left(\int_0^1\left(1+x^n\right)^\frac{1}{n} \, dx-1\right)$",the value of,"\lim\limits_{n\rightarrow\infty}n^2\left(\int_0^1\left(1+x^n\right)^\frac{1}{n} \, dx-1\right)","This is exercise from my lecturer, for IMC preparation. I haven't found any idea. Find the value of $$\lim_{n\rightarrow\infty}n^2\left(\int_0^1 \left(1+x^n\right)^\frac{1}{n} \, dx-1\right)$$ Thank you","This is exercise from my lecturer, for IMC preparation. I haven't found any idea. Find the value of $$\lim_{n\rightarrow\infty}n^2\left(\int_0^1 \left(1+x^n\right)^\frac{1}{n} \, dx-1\right)$$ Thank you",,"['calculus', 'real-analysis', 'limits', 'definite-integrals']"
87,Least norm in convex set in Banach space,Least norm in convex set in Banach space,,"The following statement is true for Hilbert spaces $H$: Every closed convex set ${\cal C} \subset H$ has a unique element $x$ such that for any $y \in C$, we have $|x| \leq |y|$. Is this statement still true for Banach spaces? If not, what is a counterexample?","The following statement is true for Hilbert spaces $H$: Every closed convex set ${\cal C} \subset H$ has a unique element $x$ such that for any $y \in C$, we have $|x| \leq |y|$. Is this statement still true for Banach spaces? If not, what is a counterexample?",,['real-analysis']
88,A Continuous Nowhere-Differentiable Function,A Continuous Nowhere-Differentiable Function,,"The book Understanding Analysis by Stephen Abbott asserts that $$ g(x)=\sum_{n=0}^{\infty}\frac{1}{2^n}h(2^nx), $$ where $h(x)=\left|x\right|$, $h:\left[-1,1\right]\to\mathbb{R}$, and $h(x+2)=h(x)$, is continuous on all of $\mathbb{R}$ but fails to be differentiable at any point. However, if I am not mistaken, can the $2^n$'s be cancelled out in $g$? I tried plotting this and could not obtain a nowhere-differentiable function.","The book Understanding Analysis by Stephen Abbott asserts that $$ g(x)=\sum_{n=0}^{\infty}\frac{1}{2^n}h(2^nx), $$ where $h(x)=\left|x\right|$, $h:\left[-1,1\right]\to\mathbb{R}$, and $h(x+2)=h(x)$, is continuous on all of $\mathbb{R}$ but fails to be differentiable at any point. However, if I am not mistaken, can the $2^n$'s be cancelled out in $g$? I tried plotting this and could not obtain a nowhere-differentiable function.",,['real-analysis']
89,"What is the cardinality of a set of all monotonic functions on a segment $[0,1]$?",What is the cardinality of a set of all monotonic functions on a segment ?,"[0,1]","What is the cardinality of a set of all real monotonic functions on a segment $[0,1]$? Does it really matter that functions are monotonic?","What is the cardinality of a set of all real monotonic functions on a segment $[0,1]$? Does it really matter that functions are monotonic?",,"['real-analysis', 'cardinals']"
90,Is this set of natural sequences countable?,Is this set of natural sequences countable?,,"Let $X$ be the set of all natural sequences such that the next term is at least twice as large as the previous one, that is $$X = \{x_n: x_n \in \mathbb{N} \text{ and } x_{n+1} \geq 2x_n \text{  }\forall n\}$$ Is this set countable? I think it isn't, and I would like some feedback on my proof below. Tentative proof : We will construct a surjective function $f: X \rightarrow \{0, 1\}^{\mathbb{N}}$ , where $\{0, 1\}^{\mathbb{N}}$ is the set of all binary sequences. For every sequence $x_n \in X$ , we map it into the sequence $f(x_n) = s_n = \mathbb{1}\{x_{n+1} > 2x_n\}$ . That is, $$ s_n =  \begin{cases} 1, & x_{n+1} > 2x_n \\ 0, & x_{n+1} = 2x_n \end{cases} $$ We now show that $f$ is a surjection: take any $s_n \in \{0, 1\}^{\mathbb{N}}$ , and consider the sequence defined as $x_1 = 1$ and $$ x_{n+1} =  \begin{cases} 2x_n +1 , & s_n = 1 \\ 2x_n, & s_n = 0 \end{cases} $$ By construction, $x_n \in X$ and $f(x_n) = s_n$ . So for any $s_n \in \{0, 1\}^{\mathbb{N}}$ there exists a $x_n \in X$ such that $f(x_n) = s_n$ . Therefore, $f$ is a surjection. We know that $|\{0, 1\}^{\mathbb{N}}| > |\mathbb{N}|$ (the set of all binary sequences is not countable); since we constructed a surjection from $\{0, 1\}^{\mathbb{N}}$ to $X$ , we have that $$|X| \geq |\{0, 1\}^{\mathbb{N}}| > |\mathbb{N}| $$ Therefore, $X$ is not countable. Is this proof correct?","Let be the set of all natural sequences such that the next term is at least twice as large as the previous one, that is Is this set countable? I think it isn't, and I would like some feedback on my proof below. Tentative proof : We will construct a surjective function , where is the set of all binary sequences. For every sequence , we map it into the sequence . That is, We now show that is a surjection: take any , and consider the sequence defined as and By construction, and . So for any there exists a such that . Therefore, is a surjection. We know that (the set of all binary sequences is not countable); since we constructed a surjection from to , we have that Therefore, is not countable. Is this proof correct?","X X = \{x_n: x_n \in \mathbb{N} \text{ and } x_{n+1} \geq 2x_n \text{  }\forall n\} f: X \rightarrow \{0, 1\}^{\mathbb{N}} \{0, 1\}^{\mathbb{N}} x_n \in X f(x_n) = s_n = \mathbb{1}\{x_{n+1} > 2x_n\}  s_n = 
\begin{cases}
1, & x_{n+1} > 2x_n \\
0, & x_{n+1} = 2x_n
\end{cases}
 f s_n \in \{0, 1\}^{\mathbb{N}} x_1 = 1  x_{n+1} = 
\begin{cases}
2x_n +1 , & s_n = 1 \\
2x_n, & s_n = 0
\end{cases}
 x_n \in X f(x_n) = s_n s_n \in \{0, 1\}^{\mathbb{N}} x_n \in X f(x_n) = s_n f |\{0, 1\}^{\mathbb{N}}| > |\mathbb{N}| \{0, 1\}^{\mathbb{N}} X |X| \geq |\{0, 1\}^{\mathbb{N}}| > |\mathbb{N}|  X","['real-analysis', 'sequences-and-series', 'elementary-set-theory']"
91,"Show that $\exists \delta > 0, \forall x \in ]0,\pi[, \exists n \in \Bbb N, |\sin(xk^n)|\ge \delta$.",Show that .,"\exists \delta > 0, \forall x \in ]0,\pi[, \exists n \in \Bbb N, |\sin(xk^n)|\ge \delta","Let $k \ge 2$ , $k \in \Bbb N$ . Show that $\exists \delta > 0, \forall x \in ]0,\pi[, \exists n \in \Bbb N, |\sin(xk^n)|\ge \delta$ . My intuition tells me that we can pick $\delta=1/2$ . I tried to study $u_n = \sin(xk^n)=U_{k^n}(\sin(x))=U_k(u_{n-1})$ . I thought of introducing a variant of Chebyshev polynomials $U_n(\sin(x))=\sin(nx)$ . But it didn't help much. Does someone have a hint? Thanks. (here $\Bbb N = \{0,1,2,\ldots\}$ .) I needed this result for solving this exercice (which is a oral exam that was posed during the competitive exams of Polytechnique, #1 engineering school in France). Show that for $f(z)=z^k$ with $k \ge 2$ $$ \exists \delta >0, \forall x,y \in \Bbb U, x \neq y \implies \exists n \in \Bbb N, |f^{(n)}(x)-f^{(n)}(y)| \ge \delta. $$ Suppose $x \neq 0$ and $y/x=e^{i \theta}$ , \begin{align} | f^{(n)}(x) - f^{(n)}(y) | &= |x^{k^n} - y^{k^n}| =  | 1- e^{i \theta k^n}|\\ &= | e^{-i \theta k^n/2} - e^{i \theta k^n/2}| = |2i \sin(\theta k^n/2)| = 2|\sin (\theta k^n/2)| \end{align} So $$ |f^{(n)}(x)-f^{(n)}(y)| \ge \delta \iff |\sin (\theta k^n/2)| \ge \delta/2 $$","Let , . Show that . My intuition tells me that we can pick . I tried to study . I thought of introducing a variant of Chebyshev polynomials . But it didn't help much. Does someone have a hint? Thanks. (here .) I needed this result for solving this exercice (which is a oral exam that was posed during the competitive exams of Polytechnique, #1 engineering school in France). Show that for with Suppose and , So","k \ge 2 k \in \Bbb N \exists \delta > 0, \forall x \in ]0,\pi[, \exists n \in \Bbb N, |\sin(xk^n)|\ge \delta \delta=1/2 u_n = \sin(xk^n)=U_{k^n}(\sin(x))=U_k(u_{n-1}) U_n(\sin(x))=\sin(nx) \Bbb N = \{0,1,2,\ldots\} f(z)=z^k k \ge 2 
\exists \delta >0, \forall x,y \in \Bbb U, x \neq y \implies \exists n \in \Bbb N, |f^{(n)}(x)-f^{(n)}(y)| \ge \delta.
 x \neq 0 y/x=e^{i \theta} \begin{align}
| f^{(n)}(x) - f^{(n)}(y) | &= |x^{k^n} - y^{k^n}|
=
 | 1- e^{i \theta k^n}|\\
&= | e^{-i \theta k^n/2} - e^{i \theta k^n/2}|
= |2i \sin(\theta k^n/2)| = 2|\sin (\theta k^n/2)|
\end{align} 
|f^{(n)}(x)-f^{(n)}(y)| \ge \delta \iff |\sin (\theta k^n/2)| \ge \delta/2
","['real-analysis', 'calculus', 'sequences-and-series']"
92,"Why is Axiom of Choice ""a convenient and safe labour-saving device""?","Why is Axiom of Choice ""a convenient and safe labour-saving device""?",,"In Terence Tao's Analysis , he states The axiom is almost universally accepted by mathematicians. One reason for this conﬁdence is a theorem due to the great logician Kurt Godel, who showed that a result proven using the axiom of choice will never contradict a result proven without the axiom of choice.   More precisely, Godel demonstrated that the axiom of choice is undecidable; it can neither be proved nor disproved from the other axioms of set theory, so long as those axioms are themselves consistent. Then he writes In practice, this means that any “real-life” application of analysis (more precisely, any application involving only “decidable” questions) which can be rigorously supported using the axiom of choice, can also be rigorously supported without the axiom of choice, though in many cases it would take a much more complicated and lengthier argument to do so if one were not allowed to use the axiom of choice. Thus one can view the axiom of choice as a convenient and safe labour-saving device in analysis. In ZF, if a proposition is ""decidable"", and if we prove the proposition in ZFC, then it is true in ZF. I understand it. If we prove sth is true in ZFC, then it is undecidable or true in ZF. but I think the hardest part is to demonstrate a proposition is  ""decidable"". How can we do that? Is there anyway to prove a proposition ""decidable""? So I think the axiom of choice is not safe. I don't understand Tao's words. Also, I don't understand why he asserts ""real-life"" application is always ""decidable"".","In Terence Tao's Analysis , he states The axiom is almost universally accepted by mathematicians. One reason for this conﬁdence is a theorem due to the great logician Kurt Godel, who showed that a result proven using the axiom of choice will never contradict a result proven without the axiom of choice.   More precisely, Godel demonstrated that the axiom of choice is undecidable; it can neither be proved nor disproved from the other axioms of set theory, so long as those axioms are themselves consistent. Then he writes In practice, this means that any “real-life” application of analysis (more precisely, any application involving only “decidable” questions) which can be rigorously supported using the axiom of choice, can also be rigorously supported without the axiom of choice, though in many cases it would take a much more complicated and lengthier argument to do so if one were not allowed to use the axiom of choice. Thus one can view the axiom of choice as a convenient and safe labour-saving device in analysis. In ZF, if a proposition is ""decidable"", and if we prove the proposition in ZFC, then it is true in ZF. I understand it. If we prove sth is true in ZFC, then it is undecidable or true in ZF. but I think the hardest part is to demonstrate a proposition is  ""decidable"". How can we do that? Is there anyway to prove a proposition ""decidable""? So I think the axiom of choice is not safe. I don't understand Tao's words. Also, I don't understand why he asserts ""real-life"" application is always ""decidable"".",,"['real-analysis', 'analysis', 'set-theory', 'axiom-of-choice', 'axioms']"
93,Lebesgue's Differentiation Theorem for Continuous Functions,Lebesgue's Differentiation Theorem for Continuous Functions,,"If $f:\mathbb{R}^n\to\mathbb{R}$ is continuous, does Lebesgue's differentiation theorem hold at all points? That is, does $$\lim_{r\to0}\frac{1}{|B(x,r)|}\int_{B(x,r)}f(y) \, dy=f(x)$$ $\textit{everywhere}$?","If $f:\mathbb{R}^n\to\mathbb{R}$ is continuous, does Lebesgue's differentiation theorem hold at all points? That is, does $$\lim_{r\to0}\frac{1}{|B(x,r)|}\int_{B(x,r)}f(y) \, dy=f(x)$$ $\textit{everywhere}$?",,['real-analysis']
94,"Is there a ""one-line"" proof of $x<y\Rightarrow x^n<y^n$ (for $n$ an odd natural number)?","Is there a ""one-line"" proof of  (for  an odd natural number)?",x<y\Rightarrow x^n<y^n n,"Given the usual field and ordering axioms for the real numbers, it isn't difficult to prove that $x<y$, without any further restrictions on the signs of $x$ and $y$, implies $x^n<y^n$, with $n$ an odd natural number. However, all the proofs I've seen and come up with myself seem to rely on distinguishing between the different possible combinations of signs of $x$ and $y$. I find such ""case-by-case"" proofs, especially of elementary facts, rather unappealing. Now, my question is: does there exist an elegant one-line proof of the fact that   $x<y\Rightarrow x^n<y^n$ (for $n$ an odd natural number) which doesn't take into account the signs of $x$ and $y$? Thanks for reading and for any comments!","Given the usual field and ordering axioms for the real numbers, it isn't difficult to prove that $x<y$, without any further restrictions on the signs of $x$ and $y$, implies $x^n<y^n$, with $n$ an odd natural number. However, all the proofs I've seen and come up with myself seem to rely on distinguishing between the different possible combinations of signs of $x$ and $y$. I find such ""case-by-case"" proofs, especially of elementary facts, rather unappealing. Now, my question is: does there exist an elegant one-line proof of the fact that   $x<y\Rightarrow x^n<y^n$ (for $n$ an odd natural number) which doesn't take into account the signs of $x$ and $y$? Thanks for reading and for any comments!",,"['real-analysis', 'inequality', 'real-numbers']"
95,How to prove that a smooth function is NOT analytic?,How to prove that a smooth function is NOT analytic?,,"For class we are exploring an example of a function that is smooth at $x=0$ but not analytic in any open interval centered at $0$. My question is, how can one prove that a function is not analytic? I am unaware of what tools are available to do so.","For class we are exploring an example of a function that is smooth at $x=0$ but not analytic in any open interval centered at $0$. My question is, how can one prove that a function is not analytic? I am unaware of what tools are available to do so.",,"['real-analysis', 'sequences-and-series']"
96,Series with $\zeta$,Series with,\zeta,How do I calculate the following series: $$ \zeta(2)+\zeta(3)+\zeta(4)+ \dots + \zeta(2013) + \zeta(2014) $$ All I know is that $\zeta(2)=\pi^2/6$ and $\zeta(4)=\pi^4/90$. But this is not enough to solve this problem. How do I do this?,How do I calculate the following series: $$ \zeta(2)+\zeta(3)+\zeta(4)+ \dots + \zeta(2013) + \zeta(2014) $$ All I know is that $\zeta(2)=\pi^2/6$ and $\zeta(4)=\pi^4/90$. But this is not enough to solve this problem. How do I do this?,,"['real-analysis', 'abstract-algebra', 'riemann-zeta']"
97,"For f continuous on $[0,1]$, show that there exist points $\alpha_k$ such that $\sum \limits_{k=1}^n \frac{1}{f'(\alpha_k)} = n $","For f continuous on , show that there exist points  such that","[0,1] \alpha_k \sum \limits_{k=1}^n \frac{1}{f'(\alpha_k)} = n ","Suppose that $f$ is continous on $[0,1]$ , differentiable on $(0,1)$ , and $f(0)=0$ and  $f(1)=1$.For every integer $n$ show  that there must exist $n$ distinct points $\alpha_1,\alpha_2\cdots,\alpha_n$ in that interval so that    $\sum \limits_{k=1}^n \frac{1}{f'(\alpha_k)} = n $ Is it theorem or i can show it with basic differentiation knowledge .Please give me hint .","Suppose that $f$ is continous on $[0,1]$ , differentiable on $(0,1)$ , and $f(0)=0$ and  $f(1)=1$.For every integer $n$ show  that there must exist $n$ distinct points $\alpha_1,\alpha_2\cdots,\alpha_n$ in that interval so that    $\sum \limits_{k=1}^n \frac{1}{f'(\alpha_k)} = n $ Is it theorem or i can show it with basic differentiation knowledge .Please give me hint .",,"['real-analysis', 'analysis', 'derivatives', 'continuity']"
98,Continuous bijection from $\mathbb{R}^n$ to $\mathbb{R}^m$,Continuous bijection from  to,\mathbb{R}^n \mathbb{R}^m,"Is there a continuous bijection from $\mathbb{R}^n$ to  $\mathbb{R}^m$, for $n \neq m$? Such a map would not be an open map, since $\mathbb{R}^n$ and  $\mathbb{R}^m$ are not homeomorphic.","Is there a continuous bijection from $\mathbb{R}^n$ to  $\mathbb{R}^m$, for $n \neq m$? Such a map would not be an open map, since $\mathbb{R}^n$ and  $\mathbb{R}^m$ are not homeomorphic.",,"['real-analysis', 'general-topology']"
99,"$\{ a + b\sqrt{2} \ : \ a, b \in \mathbb{Z} \}$ dense in $\mathbb{R}$? [duplicate]",dense in ? [duplicate],"\{ a + b\sqrt{2} \ : \ a, b \in \mathbb{Z} \} \mathbb{R}","This question already has answers here : Proving that $m+n\sqrt{2}$ is dense in $\mathbb R$ (6 answers) Closed 9 years ago . I'm guessing $\{ a + b\sqrt{2} \ : \ a, b \in \mathbb{Z} \}$ is dense in $\mathbb{R}$. I'm having a mental block. How do you show that? (This is motivated by a different hypothesis: if $f$ is continuous with two periods $T_1$, $T_2$, then $f$ is constant if $T_1/T_2$ is not rational.)","This question already has answers here : Proving that $m+n\sqrt{2}$ is dense in $\mathbb R$ (6 answers) Closed 9 years ago . I'm guessing $\{ a + b\sqrt{2} \ : \ a, b \in \mathbb{Z} \}$ is dense in $\mathbb{R}$. I'm having a mental block. How do you show that? (This is motivated by a different hypothesis: if $f$ is continuous with two periods $T_1$, $T_2$, then $f$ is constant if $T_1/T_2$ is not rational.)",,"['real-analysis', 'real-numbers']"
