,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Complex modulus of $\left|\frac{-3z+2i}{2iz+1}\right|$ given that $\left|z\right|=\frac{1}{\sqrt3}$,Complex modulus of  given that,\left|\frac{-3z+2i}{2iz+1}\right| \left|z\right|=\frac{1}{\sqrt3},"The following question was on my first year algebra exam way back in 1989. If $\left|z\right|=\frac{1}{\sqrt3}$, then find $\left|\frac{-3z+2i}{2iz+1}\right|$. I couldn't figure it out then, and 28 years on, I still can't. It was only worth 4 marks, so the solution must be simpler than all the things I have tried over the years to no avail.","The following question was on my first year algebra exam way back in 1989. If $\left|z\right|=\frac{1}{\sqrt3}$, then find $\left|\frac{-3z+2i}{2iz+1}\right|$. I couldn't figure it out then, and 28 years on, I still can't. It was only worth 4 marks, so the solution must be simpler than all the things I have tried over the years to no avail.",,"['complex-analysis', 'complex-numbers', 'mobius-transformation']"
1,What do polynomials look like in the complex plane?,What do polynomials look like in the complex plane?,,"I have a hard time visualizing the fundamental theorem of algebra, which says that any polynomial has at least one zero, superficially I know this is true as every polynomial must have either an imaginary zero or real zero, but how do I visualize this in the complex plane? For example if we have a real polynomial, we know that it is zero when it crosses the x axis this is because $y = 0$, however if $f(z) = 0$, then it must be the case that $f(z) = w = u+iv = 0+i0=0$ therefore every zero in $f(z)$ passes the origin? That does not make sense to me, what am I missing here?","I have a hard time visualizing the fundamental theorem of algebra, which says that any polynomial has at least one zero, superficially I know this is true as every polynomial must have either an imaginary zero or real zero, but how do I visualize this in the complex plane? For example if we have a real polynomial, we know that it is zero when it crosses the x axis this is because $y = 0$, however if $f(z) = 0$, then it must be the case that $f(z) = w = u+iv = 0+i0=0$ therefore every zero in $f(z)$ passes the origin? That does not make sense to me, what am I missing here?",,"['complex-analysis', 'graphing-functions']"
2,How is Cauchy's estimate derived?,How is Cauchy's estimate derived?,,"Cauchy's integral formula says $$ f^{(n)}(z)=\frac{n!}{2\pi i}\int_C\frac{f(\zeta)d\zeta}{(\zeta-z)^{n+1}}. $$ If we let $C$ be the circle of radius $r$, such that $|f(\zeta)|\leq M$ on $C$, then taking $z=a$, one obtains Cauchy's estimate that  $$ |f^{(n)}(a)|\leq Mn!r^{-n}. $$ How is this derived? I see instead $$ |f^{(n)}(a)|\leq\frac{n!}{2\pi}\int_C \frac{|f(\zeta)||d\zeta|}{|\zeta-a|^{n+1}}\leq Mn!\int_C\frac{|d\zeta|}{|\zeta-a|^{n+1}} $$ but I don't see how this eventually gets to Cauchy's estimate.","Cauchy's integral formula says $$ f^{(n)}(z)=\frac{n!}{2\pi i}\int_C\frac{f(\zeta)d\zeta}{(\zeta-z)^{n+1}}. $$ If we let $C$ be the circle of radius $r$, such that $|f(\zeta)|\leq M$ on $C$, then taking $z=a$, one obtains Cauchy's estimate that  $$ |f^{(n)}(a)|\leq Mn!r^{-n}. $$ How is this derived? I see instead $$ |f^{(n)}(a)|\leq\frac{n!}{2\pi}\int_C \frac{|f(\zeta)||d\zeta|}{|\zeta-a|^{n+1}}\leq Mn!\int_C\frac{|d\zeta|}{|\zeta-a|^{n+1}} $$ but I don't see how this eventually gets to Cauchy's estimate.",,['complex-analysis']
3,Prove: The positive integers cannot be partitioned into arithmetic sequences (using Complex Analysis),Prove: The positive integers cannot be partitioned into arithmetic sequences (using Complex Analysis),,"An arithmetic sequence of step $d$ is a set of the form: {$a, a+d, a+2d, a+3d, ...$} where $a, d$ are positive integers. Show that the positive integers cannot be partitioned into a finite number of arithmetic sequences s.t. each sequence has a distinct step $d$. (Except the trivial sequence $a=1, d=1$.) Now the book gives me a hint: Write $\sum_{n\in\mathbb{N}}z^n$ as a sum of terms of the type $\frac{z^a}{1-z^d}$. But it isn't clear that the power series can even be put such a form, and even if it can, why does that help me?","An arithmetic sequence of step $d$ is a set of the form: {$a, a+d, a+2d, a+3d, ...$} where $a, d$ are positive integers. Show that the positive integers cannot be partitioned into a finite number of arithmetic sequences s.t. each sequence has a distinct step $d$. (Except the trivial sequence $a=1, d=1$.) Now the book gives me a hint: Write $\sum_{n\in\mathbb{N}}z^n$ as a sum of terms of the type $\frac{z^a}{1-z^d}$. But it isn't clear that the power series can even be put such a form, and even if it can, why does that help me?",,"['number-theory', 'complex-analysis']"
4,Value of Summation of $\log(n)$,Value of Summation of,\log(n),"Context: I am learning Dijstra's Algorithm to find shortest path to any node, given the start node. Here, we can use Fibonnacci Heap as Priority Queue. Following is few lines of algorithm: For each vertex in PriorityQueue{     do_something() } If $V$ is the number of vertices, the subsequent lookup times in the priority queue will be: $$O(\log V), O(\log V-1), \ldots$$ Question: What would the value of $O(\log V) + O(\log V-1) + O(\log V-2) + .. + O(\log 1)$?","Context: I am learning Dijstra's Algorithm to find shortest path to any node, given the start node. Here, we can use Fibonnacci Heap as Priority Queue. Following is few lines of algorithm: For each vertex in PriorityQueue{     do_something() } If $V$ is the number of vertices, the subsequent lookup times in the priority queue will be: $$O(\log V), O(\log V-1), \ldots$$ Question: What would the value of $O(\log V) + O(\log V-1) + O(\log V-2) + .. + O(\log 1)$?",,"['complex-analysis', 'asymptotics', 'logarithms', 'summation']"
5,Does constant modulus on boundary of annulus imply constant function?,Does constant modulus on boundary of annulus imply constant function?,,"Suppose I have a function $f:\mathbb{C}\rightarrow \mathbb{C}$, holomorphic on some neighborhood of an annulus $r\le|z|\le R$, $r<R$. If, for $z\in\{|z|=r\text{ or }|z|=R\}$, $|f(z)|=C$ for some constant C, does it follow that $f(z)$ is a constant function?","Suppose I have a function $f:\mathbb{C}\rightarrow \mathbb{C}$, holomorphic on some neighborhood of an annulus $r\le|z|\le R$, $r<R$. If, for $z\in\{|z|=r\text{ or }|z|=R\}$, $|f(z)|=C$ for some constant C, does it follow that $f(z)$ is a constant function?",,['complex-analysis']
6,What is it that makes holomorphic functions so rigid?,What is it that makes holomorphic functions so rigid?,,"This semester I took a complex analysis class and, as far as I've seen, holomorphic functions on the complex plane have very ""powerful"" properties; For example, the identifying theorem, or the Casorati-Weierstrass theorem, Morera's Theorem, or even Gauss' mean value theorem. Thing is, most of the theorems have very ""light"" conditions but lead to ""heavy"" results. One comes to the conclusion that the holomorphy of a function is itself a very strong property. But what is it that makes it that way? In real analysis, one can say some things about differentiable functions, but not that much as one can say about complex-differentiable functions. EDIT: As stated in the answers, complex-differentiability implies complex-analyticity and that is a big thing indeed. Actually, the proof of this theorem uses Cauchy's integral formula and the uniform convergence of a series on a circle. I'm already familiar with this things though and I was kinda hoping for an answer orientated around the fact that the field $\mathbb{C}$ is algebraicly closed. Does this algebraic property of the plane play a crucial role in the implicitation ""differentiability $\rightarrow$ analyticity"" ?","This semester I took a complex analysis class and, as far as I've seen, holomorphic functions on the complex plane have very ""powerful"" properties; For example, the identifying theorem, or the Casorati-Weierstrass theorem, Morera's Theorem, or even Gauss' mean value theorem. Thing is, most of the theorems have very ""light"" conditions but lead to ""heavy"" results. One comes to the conclusion that the holomorphy of a function is itself a very strong property. But what is it that makes it that way? In real analysis, one can say some things about differentiable functions, but not that much as one can say about complex-differentiable functions. EDIT: As stated in the answers, complex-differentiability implies complex-analyticity and that is a big thing indeed. Actually, the proof of this theorem uses Cauchy's integral formula and the uniform convergence of a series on a circle. I'm already familiar with this things though and I was kinda hoping for an answer orientated around the fact that the field $\mathbb{C}$ is algebraicly closed. Does this algebraic property of the plane play a crucial role in the implicitation ""differentiability $\rightarrow$ analyticity"" ?",,"['complex-analysis', 'analysis', 'holomorphic-functions']"
7,Why is $\zeta(1+it) \neq 0$ equivalent to the prime number theorem?,Why is  equivalent to the prime number theorem?,\zeta(1+it) \neq 0,"Reading through Titchmarsh's book on the Riemann zeta function , chapter 3 discusses the Prime Number Theorem.  One way to prove this result is to check the zeta function has no zeros on the line $z = 1 + it,$ $$ \zeta(1 + it) \neq 0$$ Indeed the book has $3$ or $4$ proofs of this result.  Actually connecting it to the prime number theorem is another matter.  One version of the Prime Number Theoriem is: $$  \sum_{n \leq x} \Lambda (n) = x + o(x)$$ involving the van Mangoldt function, but why is this equivalent to the non-vanishing of the Riemann zeta function.  I think you can start from Perron's theorem $$ \frac{1}{2\pi i}\int_{1-iT}^{1+iT} \frac{\zeta'(w)}{\zeta(w)} \, \frac{x^w}{w}dw = \sum_{n \leq x} \Lambda (n) $$ and then I don't know how to proceed.","Reading through Titchmarsh's book on the Riemann zeta function , chapter 3 discusses the Prime Number Theorem.  One way to prove this result is to check the zeta function has no zeros on the line $z = 1 + it,$ $$ \zeta(1 + it) \neq 0$$ Indeed the book has $3$ or $4$ proofs of this result.  Actually connecting it to the prime number theorem is another matter.  One version of the Prime Number Theoriem is: $$  \sum_{n \leq x} \Lambda (n) = x + o(x)$$ involving the van Mangoldt function, but why is this equivalent to the non-vanishing of the Riemann zeta function.  I think you can start from Perron's theorem $$ \frac{1}{2\pi i}\int_{1-iT}^{1+iT} \frac{\zeta'(w)}{\zeta(w)} \, \frac{x^w}{w}dw = \sum_{n \leq x} \Lambda (n) $$ and then I don't know how to proceed.",,"['complex-analysis', 'number-theory', 'contour-integration', 'analytic-number-theory', 'riemann-zeta']"
8,Function's analytic continuation is its own derivative,Function's analytic continuation is its own derivative,,"This is the question we were asked at the university by our professor for complex analysis. Not as an exam, but as a challenge. I don't think he knew the answer himself. Find a nontrivial example of a function $f$ defined on a neighbourhood of $z\in \Bbb C$ and a path from $z$ to $z$ so that the analytic continuation of $f$ along the path is $f'$. It's easy to find trivial example: $e^x$. What I want is $f\ne f'$. A variation to this problem, but much easier, is to find $f$ whose analytic continuation is $f+a$, $af$, or $-f$ (for some $a\in \Bbb C$). They are a logarithm, a power and a square root. But the question in title I never knew how to tackle. Can anyone shed some light on it, please?","This is the question we were asked at the university by our professor for complex analysis. Not as an exam, but as a challenge. I don't think he knew the answer himself. Find a nontrivial example of a function $f$ defined on a neighbourhood of $z\in \Bbb C$ and a path from $z$ to $z$ so that the analytic continuation of $f$ along the path is $f'$. It's easy to find trivial example: $e^x$. What I want is $f\ne f'$. A variation to this problem, but much easier, is to find $f$ whose analytic continuation is $f+a$, $af$, or $-f$ (for some $a\in \Bbb C$). They are a logarithm, a power and a square root. But the question in title I never knew how to tackle. Can anyone shed some light on it, please?",,"['complex-analysis', 'derivatives']"
9,Finite Sum $\sum\limits_{k=0}^{n}\cos(kx)$,Finite Sum,\sum\limits_{k=0}^{n}\cos(kx),"I am being asked to prove that $$\sum\limits_{k=0}^{n}\cos(kx)=\frac{1}{2}+\frac{\sin(\frac{2n+1}{2}x)}{2\sin(x/2)}$$ I have some progress made, but I am stuck and could use some help. What I did: It holds that $$\sum\limits_{k=0}^{n}\cos(kx)=\sum\limits_{k=0}^{n}Re(\cos(kx))=\sum\limits_{k=0}^{n}Re(\cos(x)^{k})=Re(\sum\limits_{k=0}^{n}\cos(x)^{k})=Re\left(\cos(0)\cdot\frac{\cos(x)^{n}-1}{\cos(x)-1}\right)=Re\left(\frac{\cos(x)^{n}-1}{\cos(x)-1}\right) $$ For any $z_{1},z_{2}\in\mathbb{C}$ we have it that if $z_{1}=a+bi,z_{2}=c+di$ then $$\frac{z_{1}}{z_{2}}=\frac{z_{1}\overline{z2}}{|z_{2}|^{2}}=\frac{(a+bi)(c-di)}{|z_{2}|^{2}}=\frac{ac-bd+i(bc-ad)}{|z_{2}|^{2}}$$ hence $$Re\left(\frac{z_{1}}{z_{2}}\right)=\frac{Re(z_{1})Re(z_{2})-Im(z_{1})Im(z_{2})}{|z_{2}|^{2}}$$ Thus, $$Re\left(\frac{\cos(x)^{n}-1}{\cos(x)-1}\right)=\frac{(\cos(nx)-1)(\cos(x)-1)-\sin(nx)\sin(x)}{(\cos(x)-1)^{2}+\sin^{2}(x)}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{\cos^{2}(x)-2\cos(x)+1+\sin^{2}(x)}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{-2\cos(x)+2}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{-2(\cos(x)-1)}= \frac{=\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{-2(-2\cdot\sin^{2}(x/2))}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{4\sin^{2}(x/2)}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{4\sin^{2}(x/2)}=\frac{\cos(x(n+1))-\cos(nx)-\cos(x)+1}{4\sin^{2}(x/2)} $$ This is the part where I am stuck, I would appriciate any help or hint on how to continue. Edit: Given the corrections by André I get: $$(\cos(nx+x)-1)(\cos(x)-1)+\sin(nx+x)\sin(x)=\cos(nx+x)\cos(x)-\cos(nx)-\cos(x)+1+\sin(nx+x)\sin(x)$$ so $$\cos(nx+x)\cos(x)+\sin(nx+x)\sin(x)=\cos(xn+x-x)-\cos(nx)=0$$ Edit 2: I found anoter mistake in the above, I will try to correct Edit 3: When multiplying correctly the above it works out :-)","I am being asked to prove that I have some progress made, but I am stuck and could use some help. What I did: It holds that For any we have it that if then hence Thus, This is the part where I am stuck, I would appriciate any help or hint on how to continue. Edit: Given the corrections by André I get: so Edit 2: I found anoter mistake in the above, I will try to correct Edit 3: When multiplying correctly the above it works out :-)","\sum\limits_{k=0}^{n}\cos(kx)=\frac{1}{2}+\frac{\sin(\frac{2n+1}{2}x)}{2\sin(x/2)} \sum\limits_{k=0}^{n}\cos(kx)=\sum\limits_{k=0}^{n}Re(\cos(kx))=\sum\limits_{k=0}^{n}Re(\cos(x)^{k})=Re(\sum\limits_{k=0}^{n}\cos(x)^{k})=Re\left(\cos(0)\cdot\frac{\cos(x)^{n}-1}{\cos(x)-1}\right)=Re\left(\frac{\cos(x)^{n}-1}{\cos(x)-1}\right)
 z_{1},z_{2}\in\mathbb{C} z_{1}=a+bi,z_{2}=c+di \frac{z_{1}}{z_{2}}=\frac{z_{1}\overline{z2}}{|z_{2}|^{2}}=\frac{(a+bi)(c-di)}{|z_{2}|^{2}}=\frac{ac-bd+i(bc-ad)}{|z_{2}|^{2}} Re\left(\frac{z_{1}}{z_{2}}\right)=\frac{Re(z_{1})Re(z_{2})-Im(z_{1})Im(z_{2})}{|z_{2}|^{2}} Re\left(\frac{\cos(x)^{n}-1}{\cos(x)-1}\right)=\frac{(\cos(nx)-1)(\cos(x)-1)-\sin(nx)\sin(x)}{(\cos(x)-1)^{2}+\sin^{2}(x)}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{\cos^{2}(x)-2\cos(x)+1+\sin^{2}(x)}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{-2\cos(x)+2}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{-2(\cos(x)-1)}=
\frac{=\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{-2(-2\cdot\sin^{2}(x/2))}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{4\sin^{2}(x/2)}=\frac{\cos(nx)\cos(x)-\cos(nx)-\cos(x)+1-\sin(nx)\sin(x)}{4\sin^{2}(x/2)}=\frac{\cos(x(n+1))-\cos(nx)-\cos(x)+1}{4\sin^{2}(x/2)}
 (\cos(nx+x)-1)(\cos(x)-1)+\sin(nx+x)\sin(x)=\cos(nx+x)\cos(x)-\cos(nx)-\cos(x)+1+\sin(nx+x)\sin(x) \cos(nx+x)\cos(x)+\sin(nx+x)\sin(x)=\cos(xn+x-x)-\cos(nx)=0","['complex-analysis', 'trigonometry', 'complex-numbers', 'summation']"
10,Intuition behind the residue at infinity [duplicate],Intuition behind the residue at infinity [duplicate],,"This question already has an answer here : The residue at $\infty$ (1 answer) Closed 10 years ago . The residue at infinity is given by: $$\underset{z_0=\infty}{\operatorname{Res}}f(z)=\frac{1}{2\pi i}\int_{C_0} f(z)dz$$ Where $f$ is an analytic function except at finite number of singular points and $C_0$ is a closed countour so all singular points lie inside it. It can be proven that the residue at infinity can be computed calculating the residue at zero. $$\underset{z_0=\infty}{\operatorname{Res}}f(z)=\underset{z_0=0}{\operatorname{Res}}\frac{-1}{z^2}f\left(\frac{1}{z}\right)$$ The proof is just to expand $-\frac{1}{z^2}f\left(\frac{1}{z}\right)$ as a Laurent series and to see that the $1/z$ is the integral mentioned. I can see that we change $f(z)$ to $f(1/z)$ so the variable tends to infinity. But, is there any intutive reason of why we introduce the $-1/z^2$ factor?","This question already has an answer here : The residue at $\infty$ (1 answer) Closed 10 years ago . The residue at infinity is given by: $$\underset{z_0=\infty}{\operatorname{Res}}f(z)=\frac{1}{2\pi i}\int_{C_0} f(z)dz$$ Where $f$ is an analytic function except at finite number of singular points and $C_0$ is a closed countour so all singular points lie inside it. It can be proven that the residue at infinity can be computed calculating the residue at zero. $$\underset{z_0=\infty}{\operatorname{Res}}f(z)=\underset{z_0=0}{\operatorname{Res}}\frac{-1}{z^2}f\left(\frac{1}{z}\right)$$ The proof is just to expand $-\frac{1}{z^2}f\left(\frac{1}{z}\right)$ as a Laurent series and to see that the $1/z$ is the integral mentioned. I can see that we change $f(z)$ to $f(1/z)$ so the variable tends to infinity. But, is there any intutive reason of why we introduce the $-1/z^2$ factor?",,"['complex-analysis', 'intuition', 'residue-calculus']"
11,"If $f,g$ are both analytic and $f(z) = g(z)$ for uncountably many $z$, is it true that $f = g$?","If  are both analytic and  for uncountably many , is it true that ?","f,g f(z) = g(z) z f = g","If two analytical functions of $\mathbb{C}$  f and g are equal on an infinite number of input values, than they are equal. I can't seem to find a counterexample, but I haven't seen this anywhere except on the particular practice exam question I'm trying to solve. Edit:Uncountably infinite. Sorry about that.","If two analytical functions of $\mathbb{C}$  f and g are equal on an infinite number of input values, than they are equal. I can't seem to find a counterexample, but I haven't seen this anywhere except on the particular practice exam question I'm trying to solve. Edit:Uncountably infinite. Sorry about that.",,"['analysis', 'complex-analysis']"
12,half iterate of $x^2+c$,half iterate of,x^2+c,"I'm looking for literature on fractional iterates of $x^2+c$, where c>0.  For c=0, generating the half iterate is trivial. $$h(h(x))=x^2$$ $$h(x)=x^{\sqrt{2}}$$ The question is, for $c>0,$ and $x>1$, when is the half iterate of $x^2+c$ smaller than the half iterate of $x^2$?  We know that the full iterate is always larger, since $x^2+c>x^2$, for $c>0$, and $x>1$. Intuitively, one would think that the half iterate of $x^2+c$ would also always be larger, but I believe I have found some counter examples. In examining the parabolic case for $c=0.25$, I believe $x=800000000$ is a counter example.  $800000000^{\sqrt{2}} \approx 3898258249628$, but I calculate the half iterate of $f(x)=x^2+0.25$, $h_{x^2+0.25}(800000000) \approx 3898248180100$, which is smaller. For $c=0$, this is the equation for the superfunction which can be used to calculate fractional iterations.  $f(x)=x^2$, and $g(x) = f^{o x}$, $g(z) = 2^{2^z}$.  For $c=0.25$, this is the parabolic case, which has been studied a great deal in understanding the mandelbrot set, and the superfunction is entire, and I presume there is a uniqueness criteria.  For $c>0.25$, the problem becomes trickier because $x^2+c$ has complex fixed points, and I am also looking for any literature on unique solutions to calculating real valued fractional iterates for $c>0.25$. What I am also interested in is the abel function of $x^2$, which is $\text{abel}(z) = \log_2(\log_2(z))$.   I am interested in the abel function of $x^2$ composed with the superfunction of $x^2+c$. $$\theta(z)=\text{abel}_{x^2}(\text{superfunction}_{x^2+c}(z))-z$$ As real $z$ increases, if $\theta$ converges to a $1$-cyclic function, as opposed to a constant, then there are counter examples like the one I gave, and sometimes the superfunction is growing slower than $2^{2^z}$, and othertimes it is growing faster, with the two function intersecting each other an infinite number of times.  I'm also wondering if $\theta$ converge to an analytic function?  Any relevant links would be appreciated. - Sheldon","I'm looking for literature on fractional iterates of $x^2+c$, where c>0.  For c=0, generating the half iterate is trivial. $$h(h(x))=x^2$$ $$h(x)=x^{\sqrt{2}}$$ The question is, for $c>0,$ and $x>1$, when is the half iterate of $x^2+c$ smaller than the half iterate of $x^2$?  We know that the full iterate is always larger, since $x^2+c>x^2$, for $c>0$, and $x>1$. Intuitively, one would think that the half iterate of $x^2+c$ would also always be larger, but I believe I have found some counter examples. In examining the parabolic case for $c=0.25$, I believe $x=800000000$ is a counter example.  $800000000^{\sqrt{2}} \approx 3898258249628$, but I calculate the half iterate of $f(x)=x^2+0.25$, $h_{x^2+0.25}(800000000) \approx 3898248180100$, which is smaller. For $c=0$, this is the equation for the superfunction which can be used to calculate fractional iterations.  $f(x)=x^2$, and $g(x) = f^{o x}$, $g(z) = 2^{2^z}$.  For $c=0.25$, this is the parabolic case, which has been studied a great deal in understanding the mandelbrot set, and the superfunction is entire, and I presume there is a uniqueness criteria.  For $c>0.25$, the problem becomes trickier because $x^2+c$ has complex fixed points, and I am also looking for any literature on unique solutions to calculating real valued fractional iterates for $c>0.25$. What I am also interested in is the abel function of $x^2$, which is $\text{abel}(z) = \log_2(\log_2(z))$.   I am interested in the abel function of $x^2$ composed with the superfunction of $x^2+c$. $$\theta(z)=\text{abel}_{x^2}(\text{superfunction}_{x^2+c}(z))-z$$ As real $z$ increases, if $\theta$ converges to a $1$-cyclic function, as opposed to a constant, then there are counter examples like the one I gave, and sometimes the superfunction is growing slower than $2^{2^z}$, and othertimes it is growing faster, with the two function intersecting each other an infinite number of times.  I'm also wondering if $\theta$ converge to an analytic function?  Any relevant links would be appreciated. - Sheldon",,"['complex-analysis', 'function-and-relation-composition', 'tetration', 'complex-dynamics', 'fractional-iteration']"
13,"Show that holomorphic $f_1, . . . , f_n $ are constant if $\sum_{k=1}^n \left| f_k(z) \right|$ is constant.",Show that holomorphic  are constant if  is constant.,"f_1, . . . , f_n  \sum_{k=1}^n \left| f_k(z) \right|","While studying for an exam in complex analysis, I came across this problem. Unfortunately I was not able to solve it. Any help would be greatly appreciated. Let $U ⊂ \mathbb{C}$ be a domain and $f_1, . . . , f_n : U \rightarrow \mathbb{C}$ holomorphic functions, such that $\sum_{k=1}^n \left| f_k(z) \right|$ is constant on $U$. Show that $f_1, . . . , f_n$ are constant.","While studying for an exam in complex analysis, I came across this problem. Unfortunately I was not able to solve it. Any help would be greatly appreciated. Let $U ⊂ \mathbb{C}$ be a domain and $f_1, . . . , f_n : U \rightarrow \mathbb{C}$ holomorphic functions, such that $\sum_{k=1}^n \left| f_k(z) \right|$ is constant on $U$. Show that $f_1, . . . , f_n$ are constant.",,['complex-analysis']
14,"Complex Integration of $\int_0^\infty e^{-ax}\cos(bx)\,dx$",Complex Integration of,"\int_0^\infty e^{-ax}\cos(bx)\,dx","Out of Stein's book, we're asked to show find a formula for $$\int_0^\infty e^{-ax}\cos(bx)\,dx,\quad a>0.$$While this is very doable via integration by parts, I'm asked to use contour integration, where we're suggested to integrate over a sector with angle $\omega$ such that $\cos(\omega)=a/\sqrt{a^2+b^2}.$ I've attempted this multiple times, and I keep having trouble with some integrals. I've set the contour up so that on the first segment, it's on the real axis, so we have the integral $$\int_0^R e^{-az}\cos(bz)\,dz.$$ Then I parameterize the arc as $z(\theta)=Re^{i\theta}$ for $0\leq\theta\leq \omega$, so the second integral becomes $$\int_0^\omega e^{-a(Re^{i\theta})}\cos(b(Re^{i\theta}))\left(iRe^{i\theta}\right)\,d\theta.$$The final segment I parameterized as $z(t)=Re^{i\omega}(1-t)$ and set up the final integral as $$\int_0^1e^{-a(Re^{i\omega}(1-t))}\cos\big(b(Re^{i\omega}(1-t))\big)(-Re^{i\omega})\,dt.$$I've tried finding some way to bound one of the last two integrals so that I can show one of them goes to $0$ as $R\to\infty$, but I've not had any luck. Will someone make a suggestion if my approach and parameterizations are correct? Thanks! Update : My thoughts are really that the integral which goes to zero is the arc. I keep working it down in the following way; we know that it is  \begin{align} &\leq R\int_0^\omega\left|e^{-aR(\cos\theta+i\sin\theta)}\cdot\left(\frac{e^{ibRe^{i\theta}}+e^{-ibRe^{i\theta}}}{2}\right)\right|\,d\theta\\ &\leq\frac{R}{2}\int_0^\omega\left|e^{-aR\cos\theta}\cdot\left(e^{ibR(\cos\theta+i\sin\theta)}+e^{-bR(\cos\theta+i\sin\theta)}\right)\right|\,d\theta\\ &\leq\frac{R}{2}\int_0^\omega\left|e^{-aR\cos\theta-bR\sin\theta}\right|+\left|e^{-aR\cos\theta+bR\sin\theta}\right|\,d\theta. \end{align} At this point, it is easy to show that the first term tends to zero, since $(-aR\cos\theta)<0$ and $bR\sin\theta>0$ (since $b$ and $\sin\theta$ have the same sign). The second term, however is what causes me trouble. I just finished working it out again, and I get that it only goes to zero if $a^2>b^2$, which isn't necessary in the general formula when achieved by integration by parts. I am really at a loss... Added Solution : See the solution I've posted and please leave comments on your thoughts about it. Thanks!","Out of Stein's book, we're asked to show find a formula for $$\int_0^\infty e^{-ax}\cos(bx)\,dx,\quad a>0.$$While this is very doable via integration by parts, I'm asked to use contour integration, where we're suggested to integrate over a sector with angle $\omega$ such that $\cos(\omega)=a/\sqrt{a^2+b^2}.$ I've attempted this multiple times, and I keep having trouble with some integrals. I've set the contour up so that on the first segment, it's on the real axis, so we have the integral $$\int_0^R e^{-az}\cos(bz)\,dz.$$ Then I parameterize the arc as $z(\theta)=Re^{i\theta}$ for $0\leq\theta\leq \omega$, so the second integral becomes $$\int_0^\omega e^{-a(Re^{i\theta})}\cos(b(Re^{i\theta}))\left(iRe^{i\theta}\right)\,d\theta.$$The final segment I parameterized as $z(t)=Re^{i\omega}(1-t)$ and set up the final integral as $$\int_0^1e^{-a(Re^{i\omega}(1-t))}\cos\big(b(Re^{i\omega}(1-t))\big)(-Re^{i\omega})\,dt.$$I've tried finding some way to bound one of the last two integrals so that I can show one of them goes to $0$ as $R\to\infty$, but I've not had any luck. Will someone make a suggestion if my approach and parameterizations are correct? Thanks! Update : My thoughts are really that the integral which goes to zero is the arc. I keep working it down in the following way; we know that it is  \begin{align} &\leq R\int_0^\omega\left|e^{-aR(\cos\theta+i\sin\theta)}\cdot\left(\frac{e^{ibRe^{i\theta}}+e^{-ibRe^{i\theta}}}{2}\right)\right|\,d\theta\\ &\leq\frac{R}{2}\int_0^\omega\left|e^{-aR\cos\theta}\cdot\left(e^{ibR(\cos\theta+i\sin\theta)}+e^{-bR(\cos\theta+i\sin\theta)}\right)\right|\,d\theta\\ &\leq\frac{R}{2}\int_0^\omega\left|e^{-aR\cos\theta-bR\sin\theta}\right|+\left|e^{-aR\cos\theta+bR\sin\theta}\right|\,d\theta. \end{align} At this point, it is easy to show that the first term tends to zero, since $(-aR\cos\theta)<0$ and $bR\sin\theta>0$ (since $b$ and $\sin\theta$ have the same sign). The second term, however is what causes me trouble. I just finished working it out again, and I get that it only goes to zero if $a^2>b^2$, which isn't necessary in the general formula when achieved by integration by parts. I am really at a loss... Added Solution : See the solution I've posted and please leave comments on your thoughts about it. Thanks!",,['complex-analysis']
15,Cauchy's theorem: what about non-smooth homotopies?,Cauchy's theorem: what about non-smooth homotopies?,,"This morning I realized I have never understood a technical issue about Cauchy's theorem (homotopy form) of complex analysis. To illustrate, let me first give a definition. (In what follows $\Omega$ will always denote an open subset of the complex plane.) Definition Let $\gamma, \eta\colon [0, 1]_t \to \Omega$ be two piecewise smooth closed curves (or circuits ). We say that $\gamma, \eta$ are $\Omega$- homotopic if there exists a continuous mapping $H \colon [0, 1]_\lambda \times [0, 1]_t \to \Omega$ s.t. $H(0, t)=\gamma(t)$ and $H(1, t)=\eta(t), \quad \forall t \in [0, 1]$; $H(\lambda, 0)=H(\lambda, 1), \quad \forall \lambda \in [0, 1]$. Theorem (Cauchy) Let $f\colon \Omega \to \mathbb{C}$ be holomorphic. If $\gamma, \eta$ are $\Omega$-homotopic circuits, then $$\int_{\gamma} f(z)\, dz= \int_{\eta}f(z)\, dz.$$ Problem The function $H$ above is only continuous and need not be smooth. So for $0< \lambda < 1$, the closed curve $H(\lambda, \cdot)$ may be pretty much everything (a Peano curve, for example). Does this void the validity of theorem as it is stated above? How can integration be defined over such a pathological object? The proof of Cauchy's theorem that I have in mind goes as follows. To begin, one observes that for a sufficiently small value of $\lambda_1$, the circuits $\gamma=H(0, \cdot)$ and $H(\lambda_1, \cdot)$ are close toghether ; that is, they can be covered by a finite sequence of disks not leaving $\Omega$ like in the following figure: Since $f$ is locally exact, its integrals over every single disk depend only on the local primitive. Playing a bit with this, one arrives at $$\int_\gamma f(z)\, dz= \int_{H(\lambda_1, \cdot)} f(z)\, dz.$$ Then one repeats this process, yielding a $\lambda_2$ greater than $\lambda_1$ and such that $$\int_{H(\lambda_1,\cdot)} f(z)\, dz= \int_{H(\lambda_2, \cdot)} f(z)\, dz.$$ And so on. A compactness argument finally shows that this algorithm ends in a finite number of steps. Problem is : this proof assumes implicitly that $H(\lambda_1, \cdot), H(\lambda_2, \cdot) \ldots$ are piecewise smooth, to make sense of integrals $$\int_{H(\lambda_j, \cdot)}f(z)\, dz.$$  This, however, does not follow from the definition if $H$ is only assumed to be continuous. Therefore this proof works only for smooth $H$. Is this regularity condition necessary?","This morning I realized I have never understood a technical issue about Cauchy's theorem (homotopy form) of complex analysis. To illustrate, let me first give a definition. (In what follows $\Omega$ will always denote an open subset of the complex plane.) Definition Let $\gamma, \eta\colon [0, 1]_t \to \Omega$ be two piecewise smooth closed curves (or circuits ). We say that $\gamma, \eta$ are $\Omega$- homotopic if there exists a continuous mapping $H \colon [0, 1]_\lambda \times [0, 1]_t \to \Omega$ s.t. $H(0, t)=\gamma(t)$ and $H(1, t)=\eta(t), \quad \forall t \in [0, 1]$; $H(\lambda, 0)=H(\lambda, 1), \quad \forall \lambda \in [0, 1]$. Theorem (Cauchy) Let $f\colon \Omega \to \mathbb{C}$ be holomorphic. If $\gamma, \eta$ are $\Omega$-homotopic circuits, then $$\int_{\gamma} f(z)\, dz= \int_{\eta}f(z)\, dz.$$ Problem The function $H$ above is only continuous and need not be smooth. So for $0< \lambda < 1$, the closed curve $H(\lambda, \cdot)$ may be pretty much everything (a Peano curve, for example). Does this void the validity of theorem as it is stated above? How can integration be defined over such a pathological object? The proof of Cauchy's theorem that I have in mind goes as follows. To begin, one observes that for a sufficiently small value of $\lambda_1$, the circuits $\gamma=H(0, \cdot)$ and $H(\lambda_1, \cdot)$ are close toghether ; that is, they can be covered by a finite sequence of disks not leaving $\Omega$ like in the following figure: Since $f$ is locally exact, its integrals over every single disk depend only on the local primitive. Playing a bit with this, one arrives at $$\int_\gamma f(z)\, dz= \int_{H(\lambda_1, \cdot)} f(z)\, dz.$$ Then one repeats this process, yielding a $\lambda_2$ greater than $\lambda_1$ and such that $$\int_{H(\lambda_1,\cdot)} f(z)\, dz= \int_{H(\lambda_2, \cdot)} f(z)\, dz.$$ And so on. A compactness argument finally shows that this algorithm ends in a finite number of steps. Problem is : this proof assumes implicitly that $H(\lambda_1, \cdot), H(\lambda_2, \cdot) \ldots$ are piecewise smooth, to make sense of integrals $$\int_{H(\lambda_j, \cdot)}f(z)\, dz.$$  This, however, does not follow from the definition if $H$ is only assumed to be continuous. Therefore this proof works only for smooth $H$. Is this regularity condition necessary?",,['complex-analysis']
16,Proving the Maximum Modulus Principle using the Open Mapping Theorem,Proving the Maximum Modulus Principle using the Open Mapping Theorem,,"I was reading on Wikipedia that ""The maximum modulus principle can be viewed as a special case of the open mapping theorem, which states that a nonconstant holomorphic function maps open sets to open sets. If $|f|$ attains a local maximum at $a$, then the image of a sufficiently small open neighborhood of $a$ cannot be open. Therefore, $f$ is constant"". Could someone expand upon that? I don't follow why the image of a open neighborhood of a would not be open?","I was reading on Wikipedia that ""The maximum modulus principle can be viewed as a special case of the open mapping theorem, which states that a nonconstant holomorphic function maps open sets to open sets. If $|f|$ attains a local maximum at $a$, then the image of a sufficiently small open neighborhood of $a$ cannot be open. Therefore, $f$ is constant"". Could someone expand upon that? I don't follow why the image of a open neighborhood of a would not be open?",,['complex-analysis']
17,Why is complex analysis so nice? And how is it connected/motivating for algebraic topology? [duplicate],Why is complex analysis so nice? And how is it connected/motivating for algebraic topology? [duplicate],,"This question already has answers here : Why does being holomorphic imply so much about a function? (10 answers) Closed 2 years ago . This is very much a soft question, but after seeing Cauchy's integral formula in lecture today I was really struck by how neat complex analysis is. I don't understand how all of these amazing analytic properties (global extrapolations from local properties/holomorphic implies infinitely holomorphic) can come from just algebraically adjoining the square root of -1. When I asked my professor about this, he said it was a function of the complementary relationship between complex analysis and algebraic topology and didn't really expand on that. Even not knowing much algebraic topology, this connection does seem clear in some ways (the importance of simple connectedness for Cauchy's theorem and dealing with so many paths/using code words for homotopy). However, I am still not sure what it is about the complex plane that lends itself to this special link, especially when it comes to functions. $\mathbb{R}^{2}$ is topologically equivalent (maybe just point set topology questions?) from what I understand and it definitely isn't as nice. I would appreciate any sort of discussion or direction towards references (especially for someone that hasn't learned much topology formally-Hatcher is a difficult text for me to grapple with on my own) and I hope this is interesting to other people.","This question already has answers here : Why does being holomorphic imply so much about a function? (10 answers) Closed 2 years ago . This is very much a soft question, but after seeing Cauchy's integral formula in lecture today I was really struck by how neat complex analysis is. I don't understand how all of these amazing analytic properties (global extrapolations from local properties/holomorphic implies infinitely holomorphic) can come from just algebraically adjoining the square root of -1. When I asked my professor about this, he said it was a function of the complementary relationship between complex analysis and algebraic topology and didn't really expand on that. Even not knowing much algebraic topology, this connection does seem clear in some ways (the importance of simple connectedness for Cauchy's theorem and dealing with so many paths/using code words for homotopy). However, I am still not sure what it is about the complex plane that lends itself to this special link, especially when it comes to functions. $\mathbb{R}^{2}$ is topologically equivalent (maybe just point set topology questions?) from what I understand and it definitely isn't as nice. I would appreciate any sort of discussion or direction towards references (especially for someone that hasn't learned much topology formally-Hatcher is a difficult text for me to grapple with on my own) and I hope this is interesting to other people.",,"['complex-analysis', 'reference-request', 'soft-question']"
18,A Binomial Identity simplify,A Binomial Identity simplify,,"I want to symplify $$ \sum_{\ell=1}^{k} \frac{1}{\ell}\sum_{m=1}^{\min\{\ell,k-\ell\}}\binom{\ell}{m}\binom{k-\ell-1}{m-1}. $$",I want to symplify," \sum_{\ell=1}^{k} \frac{1}{\ell}\sum_{m=1}^{\min\{\ell,k-\ell\}}\binom{\ell}{m}\binom{k-\ell-1}{m-1}.
","['complex-analysis', 'binomial-coefficients', 'generating-functions', 'binomial-theorem']"
19,How to switch to a Laurent series' next convergence ring?,How to switch to a Laurent series' next convergence ring?,,"Given the Laurent series $\sum\limits_{k=-\infty}^\infty a_k^{(l)} z^k = f(z)|_{r_l<|z|<R_l}$ of a meromorphic function $f$ on $\mathbb C$ with convergence region $r_l< |z|< R_l$, one can use analytical continuation to obtain the values in the regions $r_m<|z|<R_m$ where either $r_m=R_l$ or $R_m=r_l$, i.e. switch between series converging in neighbouring rings the boundaries of which are touching isolated singularities. But how can the new coefficients $a_k^{(m)}$ be obtained from the old ones? As an example to what I mean, take the geometric series $$1+q+q^2+... = \sum_{k=0}^\infty q^k = \frac1{1-q}\Bigg|_{|q|<1}$$ which conveges for $|q|<1$, and its ""counterpart"" for $|q|>1$, $$\frac1{1-q}\Big|_{|q|>1} = \frac1q\frac{1}{\tfrac1q-1}\Bigg|_{\big|\tfrac1q\big|<1} = -\frac1q\sum_{k=0}^\infty \left(\frac1q\right)^k = -\sum_{k=-\infty}^1 q^k$$ So the question for that example would be, how to get from the $r_1=0\le|q|<1=R_1$ coefficients $$a_k^{(1)} = \begin{cases}1 & k\ge0 \\ 0 & k<0\end{cases}$$ to the $r_2=R_1=1<|q|<\infty=R_2$ ones $$a_k^{(2)} = \begin{cases}0 & k > 1 \\ -1 & k\le 1\end{cases}$$ without the trick I used, i.e. only from the coefficients (and convergence radii)?","Given the Laurent series $\sum\limits_{k=-\infty}^\infty a_k^{(l)} z^k = f(z)|_{r_l<|z|<R_l}$ of a meromorphic function $f$ on $\mathbb C$ with convergence region $r_l< |z|< R_l$, one can use analytical continuation to obtain the values in the regions $r_m<|z|<R_m$ where either $r_m=R_l$ or $R_m=r_l$, i.e. switch between series converging in neighbouring rings the boundaries of which are touching isolated singularities. But how can the new coefficients $a_k^{(m)}$ be obtained from the old ones? As an example to what I mean, take the geometric series $$1+q+q^2+... = \sum_{k=0}^\infty q^k = \frac1{1-q}\Bigg|_{|q|<1}$$ which conveges for $|q|<1$, and its ""counterpart"" for $|q|>1$, $$\frac1{1-q}\Big|_{|q|>1} = \frac1q\frac{1}{\tfrac1q-1}\Bigg|_{\big|\tfrac1q\big|<1} = -\frac1q\sum_{k=0}^\infty \left(\frac1q\right)^k = -\sum_{k=-\infty}^1 q^k$$ So the question for that example would be, how to get from the $r_1=0\le|q|<1=R_1$ coefficients $$a_k^{(1)} = \begin{cases}1 & k\ge0 \\ 0 & k<0\end{cases}$$ to the $r_2=R_1=1<|q|<\infty=R_2$ ones $$a_k^{(2)} = \begin{cases}0 & k > 1 \\ -1 & k\le 1\end{cases}$$ without the trick I used, i.e. only from the coefficients (and convergence radii)?",,"['complex-analysis', 'laurent-series']"
20,Is it Possible to Construct all Proofs in Complex Analysis using Brownian Motion?,Is it Possible to Construct all Proofs in Complex Analysis using Brownian Motion?,,"(First, I am very aware of the fact that Brownian motion is actually probably more difficult to understand than at least basic complex analysis, so the pedagogical merits of such an approach would be questionable for anyone besides a probabilist wanting to refresh or reshape their already existing complex analysis knowledge.) During my stochastic processes lecture, my professor said something to the effect that: ""Every statement in complex analysis can be proven using Brownian motion, in particular using the fact that the image of a Brownian path under a conformal map is Brownian motion up to a time change."" To what extent is this true? Even after he gave a proof of Liouville's Theorem using Brownian motion and promised to give a proof of the Riemann Mapping Theorem during the next lecture, I was still unconvinced. However, now the more that I think about it, it becomes more plausible -- Brownian motion is very closely related to the theory of harmonic functions, and analytic functions are just 2-dimensional harmonic functions satisfying the Cauchy-Riemann equations (is this correct?). Brownian motion has already been shown to have numerous applications to potential theory and PDEs (to the best of my knowledge), so is a similar formulation of complex analysis in terms of Brownian motion also theoretically possible? (even if not desirable?)","(First, I am very aware of the fact that Brownian motion is actually probably more difficult to understand than at least basic complex analysis, so the pedagogical merits of such an approach would be questionable for anyone besides a probabilist wanting to refresh or reshape their already existing complex analysis knowledge.) During my stochastic processes lecture, my professor said something to the effect that: ""Every statement in complex analysis can be proven using Brownian motion, in particular using the fact that the image of a Brownian path under a conformal map is Brownian motion up to a time change."" To what extent is this true? Even after he gave a proof of Liouville's Theorem using Brownian motion and promised to give a proof of the Riemann Mapping Theorem during the next lecture, I was still unconvinced. However, now the more that I think about it, it becomes more plausible -- Brownian motion is very closely related to the theory of harmonic functions, and analytic functions are just 2-dimensional harmonic functions satisfying the Cauchy-Riemann equations (is this correct?). Brownian motion has already been shown to have numerous applications to potential theory and PDEs (to the best of my knowledge), so is a similar formulation of complex analysis in terms of Brownian motion also theoretically possible? (even if not desirable?)",,"['complex-analysis', 'soft-question', 'stochastic-processes', 'brownian-motion', 'harmonic-functions']"
21,What differentiates algebraic geometry over $\mathbb{C}$ from complex analysis?,What differentiates algebraic geometry over  from complex analysis?,\mathbb{C},"I have just begun learning algebraic geometry, and there are a lot more connections to complex analysis than I expected. For example: $\mathbb{CP}^1$ is the Riemann sphere Elliptic curves are parametrized by Weierstrass's elliptic functions Riemann-Roch is a theorem in complex analysis and in algebraic geometry This surprised me a lot at first, but it does make some more sense when one thinks about it -- analytic (i.e. holomorphic) functions are in a crude sense just ""polynomials with arbitrarily large degree"", and analysis over $\mathbb{C}$ probably should have some more algebraic properties over $\mathbb{R}$ because complex numbers were first investigated due to the fact that they are algebraically closed. After all, polynomials are holomorphic, rational functions are meromorphic, and all smooth algebraic curves are (to the best of my knowledge) Riemann surfaces. This leads me to the question: What most importantly differentiates complex analysis from the study of smooth algebraic varieties over $\mathbb{C}$? To quote Wikipedia , ""Algebraic varieties are locally defined as the common zero sets of polynomials and since polynomials over the complex numbers are holomorphic functions, algebraic varieties over C can be interpreted as analytic spaces. Similarly, regular morphisms between varieties are interpreted as holomorphic mappings between analytic spaces. Somewhat surprisingly, it is often possible to go the other way, to interpret analytic objects in an algebraic way ."" So another way to phrase the question: What are the most important examples of when it is impossible to interpret analytic objects in an algebraic way? The only ones I can think of with my limited knowledge: It's not possible to ""homogenize"" an arbitrary analytic function, like it is with a polynomial, because the terms of an arbitrary analytic function have unbounded degree. Only smooth algebraic varieties correspond to objects of study of complex analysis. Related questions: Connection between algebraic geometry and complex analysis? Complex analysis book for Algebraic Geometers Complex Analysis and Algebra","I have just begun learning algebraic geometry, and there are a lot more connections to complex analysis than I expected. For example: $\mathbb{CP}^1$ is the Riemann sphere Elliptic curves are parametrized by Weierstrass's elliptic functions Riemann-Roch is a theorem in complex analysis and in algebraic geometry This surprised me a lot at first, but it does make some more sense when one thinks about it -- analytic (i.e. holomorphic) functions are in a crude sense just ""polynomials with arbitrarily large degree"", and analysis over $\mathbb{C}$ probably should have some more algebraic properties over $\mathbb{R}$ because complex numbers were first investigated due to the fact that they are algebraically closed. After all, polynomials are holomorphic, rational functions are meromorphic, and all smooth algebraic curves are (to the best of my knowledge) Riemann surfaces. This leads me to the question: What most importantly differentiates complex analysis from the study of smooth algebraic varieties over $\mathbb{C}$? To quote Wikipedia , ""Algebraic varieties are locally defined as the common zero sets of polynomials and since polynomials over the complex numbers are holomorphic functions, algebraic varieties over C can be interpreted as analytic spaces. Similarly, regular morphisms between varieties are interpreted as holomorphic mappings between analytic spaces. Somewhat surprisingly, it is often possible to go the other way, to interpret analytic objects in an algebraic way ."" So another way to phrase the question: What are the most important examples of when it is impossible to interpret analytic objects in an algebraic way? The only ones I can think of with my limited knowledge: It's not possible to ""homogenize"" an arbitrary analytic function, like it is with a polynomial, because the terms of an arbitrary analytic function have unbounded degree. Only smooth algebraic varieties correspond to objects of study of complex analysis. Related questions: Connection between algebraic geometry and complex analysis? Complex analysis book for Algebraic Geometers Complex Analysis and Algebra",,"['complex-analysis', 'algebraic-geometry', 'soft-question', 'complex-geometry']"
22,Factor $x^4+1$ over $\mathbb{R}$,Factor  over,x^4+1 \mathbb{R},"Factor $x^4+1$ over $\mathbb{R}$ Well, I read this question first wrongly, because the reader is about complex analysis, I did it for $\mathbb{C}$ first. I got. $x^4+1=(x-e^{\pi i/4 })(x-e^{3 \pi i/4})(x-e^{5\pi i/4})(x-e^{7\pi i/4})$. My teacher told me that there is very smart way to do this for $\mathbb{R}$ that we already learned. But I only can think of trial and error kind of methods.","Factor $x^4+1$ over $\mathbb{R}$ Well, I read this question first wrongly, because the reader is about complex analysis, I did it for $\mathbb{C}$ first. I got. $x^4+1=(x-e^{\pi i/4 })(x-e^{3 \pi i/4})(x-e^{5\pi i/4})(x-e^{7\pi i/4})$. My teacher told me that there is very smart way to do this for $\mathbb{R}$ that we already learned. But I only can think of trial and error kind of methods.",,"['complex-analysis', 'polynomials', 'factoring']"
23,When does a complex function have a square root?,When does a complex function have a square root?,,"I would like to show that there is a holomorphic $f$ on a neighborhood of zero such that $f(z)^2=1-\cos(z)$. In other words, I want to show that $1-\cos(z)$ has a complex square root. I know that this has something to do with whether one can define a branch of $\log(1-\cos(z))$ in a neighborhood of zero (since $z^{1/2}=e^{\frac12 (\log(1-\cos(z)))}$, but I thought this could not be possible because at zero, $1-\cos(z)=0$, and $\log(z)$ is not defined. Can someone help me out? In general I'm missing out on some subtlety about the complex logarithm.","I would like to show that there is a holomorphic $f$ on a neighborhood of zero such that $f(z)^2=1-\cos(z)$. In other words, I want to show that $1-\cos(z)$ has a complex square root. I know that this has something to do with whether one can define a branch of $\log(1-\cos(z))$ in a neighborhood of zero (since $z^{1/2}=e^{\frac12 (\log(1-\cos(z)))}$, but I thought this could not be possible because at zero, $1-\cos(z)=0$, and $\log(z)$ is not defined. Can someone help me out? In general I'm missing out on some subtlety about the complex logarithm.",,['complex-analysis']
24,Find a conformal map from semi-disc onto unit disc,Find a conformal map from semi-disc onto unit disc,,"This comes straight from Conway's Complex Analysis, VII.4, exercise 4. Find an analytic function $f$ which maps $G:=$ {${z: |z| < 1, Re(z) > 0}$} onto $B(0; 1)$ in a one-one fashion. $B(0;1)$ is the open unit disc. My first intuition was to use $z^2$, which does the job splendidly, except for the segment $(-1,0] \subset B(0;1)$.  Under $z^2$, the pre-image for this segment is the segment $[-i,i]$, which is not in $G$. My next thought is to modify $z^2$, something like $a(z-h)^2+k$.  I've yet to work out the details, but my gut tells me this isn't the right idea. I've been teaching myself conformal maps in preparation for a qualifying exam.  So, if there's a shockingly basic, obvious solution... please patronize me.","This comes straight from Conway's Complex Analysis, VII.4, exercise 4. Find an analytic function $f$ which maps $G:=$ {${z: |z| < 1, Re(z) > 0}$} onto $B(0; 1)$ in a one-one fashion. $B(0;1)$ is the open unit disc. My first intuition was to use $z^2$, which does the job splendidly, except for the segment $(-1,0] \subset B(0;1)$.  Under $z^2$, the pre-image for this segment is the segment $[-i,i]$, which is not in $G$. My next thought is to modify $z^2$, something like $a(z-h)^2+k$.  I've yet to work out the details, but my gut tells me this isn't the right idea. I've been teaching myself conformal maps in preparation for a qualifying exam.  So, if there's a shockingly basic, obvious solution... please patronize me.",,"['complex-analysis', 'conformal-geometry']"
25,A book for complex analysis,A book for complex analysis,,"I am currently learning complex analysis in my undergraduate studies and it's quite hard to understand for me. During my mathematical study, I learned that nothing is too hard to understand, it's just necessary to find the right source for me. When I read lecture notes or books, they feel mostly too ""dry"" for me and it's not so enjoyable for me to read. In the past I learned first with books like e.g. analysis I or II for dummies and afterwords I learned with proper mathematical books, which I then could enjoy and understand fully, and use as my main source. But now as I progress to more advanced subjects, I find it hard to find books that help me. I learn best with a visual approach: the more graphics the better. And it's always good when the author explains a lot and is not assuming too many things as trivia. I hope I made myself clear what kind of book I need and maybe someone can help me. PS: I'm not restricted to books; if you know some good lecture notes, you are more than welcome. My native language is German, so I'm fine with sources in English and German.","I am currently learning complex analysis in my undergraduate studies and it's quite hard to understand for me. During my mathematical study, I learned that nothing is too hard to understand, it's just necessary to find the right source for me. When I read lecture notes or books, they feel mostly too ""dry"" for me and it's not so enjoyable for me to read. In the past I learned first with books like e.g. analysis I or II for dummies and afterwords I learned with proper mathematical books, which I then could enjoy and understand fully, and use as my main source. But now as I progress to more advanced subjects, I find it hard to find books that help me. I learn best with a visual approach: the more graphics the better. And it's always good when the author explains a lot and is not assuming too many things as trivia. I hope I made myself clear what kind of book I need and maybe someone can help me. PS: I'm not restricted to books; if you know some good lecture notes, you are more than welcome. My native language is German, so I'm fine with sources in English and German.",,"['complex-analysis', 'reference-request', 'book-recommendation']"
26,I think I don't truly understand Cauchy's Integral theorem,I think I don't truly understand Cauchy's Integral theorem,,"Cauchy's theorem states that closed line integral of some holomorphic functions yields zero, in some good regions (i.e. simply connected domain). More explicitly, $$ \oint_\gamma f(z) d z=0. $$ Many textbooks use Goursat's theorem or Green's theorem to prove this. I understand both proofs and can write it myself. Example: in Stein and Shakarchi's Complex Analysis the proof that the integral over a triangle of a holomorphic function is zero is due to a limiting argument. While this proof is elegant and slick, it doesn't give me any intuition of why the integral is zero. Are there any intuitive interpretations of this theorem that might help my understanding? Something that might explain why these integrals turn out to be zero while others are not?","Cauchy's theorem states that closed line integral of some holomorphic functions yields zero, in some good regions (i.e. simply connected domain). More explicitly, Many textbooks use Goursat's theorem or Green's theorem to prove this. I understand both proofs and can write it myself. Example: in Stein and Shakarchi's Complex Analysis the proof that the integral over a triangle of a holomorphic function is zero is due to a limiting argument. While this proof is elegant and slick, it doesn't give me any intuition of why the integral is zero. Are there any intuitive interpretations of this theorem that might help my understanding? Something that might explain why these integrals turn out to be zero while others are not?","
\oint_\gamma f(z) d z=0.
","['complex-analysis', 'contour-integration', 'complex-integration']"
27,Proving the identity $\sum_{n=-\infty}^\infty e^{-\pi n^2x}=x^{-1/2}\sum_{n=-\infty}^\infty e^{-\pi n^2/x}.$,Proving the identity,\sum_{n=-\infty}^\infty e^{-\pi n^2x}=x^{-1/2}\sum_{n=-\infty}^\infty e^{-\pi n^2/x}.,"Can you help prove the functional equation: $$\sum_{n=-\infty}^\infty e^{-\pi n^2x}=x^{-1/2}\sum_{n=-\infty}^\infty e^{-\pi n^2/x}.$$ Specifically, I am looking for a solution using complex analysis, but I am interested in any solutions. Thanks!","Can you help prove the functional equation: $$\sum_{n=-\infty}^\infty e^{-\pi n^2x}=x^{-1/2}\sum_{n=-\infty}^\infty e^{-\pi n^2/x}.$$ Specifically, I am looking for a solution using complex analysis, but I am interested in any solutions. Thanks!",,"['complex-analysis', 'summation', 'special-functions', 'big-list']"
28,Complex analysis book with a view toward Riemann surfaces?,Complex analysis book with a view toward Riemann surfaces?,,"I am considering complex analysis as my next area of study.  There are already a few threads asking about complex analysis texts (see Complex Analysis Book and What is a good complex analysis textbook? ).  However, I'm looking for something a little more specific, if such a thing exists. Is there a nice, slow-paced introductory complex analysis text that features at least some (introductory) material on Riemann surfaces? A look through texts mentioned in the pages linked above did not yield any.  I am not big on analysis and tend to favor more algebraic, topological, and geometric-flavored areas of mathematics.  I am however trying to learn at least at a basic level the core disciplines of mathematics, and I feel I would be amiss if I did not study complex analysis.  For background: I have a basic knowledge of real analysis, algebra (group, ring, and field theory), linear algebra, and will have knowledge of topology. In addition to my above desire in a complex analysis text: is there one you would recommend for its view toward algebraic, topological, or geometric applications of complex analysis? Any online lecture notes (or inexpensive book) on Riemann surfaces that would be accessible after or along with an introductory look at complex analysis would be welcome as well. EDIT: After what has developed, I feel this question is now appropriate: Is there a complex analysis text that would be particularly recommended if one wishes to study Riemann surfaces?  What topics in particular is it important to develop a good grasp of?","I am considering complex analysis as my next area of study.  There are already a few threads asking about complex analysis texts (see Complex Analysis Book and What is a good complex analysis textbook? ).  However, I'm looking for something a little more specific, if such a thing exists. Is there a nice, slow-paced introductory complex analysis text that features at least some (introductory) material on Riemann surfaces? A look through texts mentioned in the pages linked above did not yield any.  I am not big on analysis and tend to favor more algebraic, topological, and geometric-flavored areas of mathematics.  I am however trying to learn at least at a basic level the core disciplines of mathematics, and I feel I would be amiss if I did not study complex analysis.  For background: I have a basic knowledge of real analysis, algebra (group, ring, and field theory), linear algebra, and will have knowledge of topology. In addition to my above desire in a complex analysis text: is there one you would recommend for its view toward algebraic, topological, or geometric applications of complex analysis? Any online lecture notes (or inexpensive book) on Riemann surfaces that would be accessible after or along with an introductory look at complex analysis would be welcome as well. EDIT: After what has developed, I feel this question is now appropriate: Is there a complex analysis text that would be particularly recommended if one wishes to study Riemann surfaces?  What topics in particular is it important to develop a good grasp of?",,"['complex-analysis', 'reference-request', 'riemann-surfaces']"
29,Holomorphic vs differentiable (in the real sense).,Holomorphic vs differentiable (in the real sense).,,"Why a holomorphic function is infinitely differentiable just because of satisfying the Cauchy Riemann equations, but on the other side, a two variable real function that is twice differentiable is not infinitely differentiable? I'm asking this for two reasons: 1) $\mathbb{R}^2$ is somehow an analog to the complex plane (they are isomorphic). 2) Holomorphic means that is differentiable in any direction of the complex plane, so why if a function is differentiable in any direction of the real plane then it is not infinitely differentiable?","Why a holomorphic function is infinitely differentiable just because of satisfying the Cauchy Riemann equations, but on the other side, a two variable real function that is twice differentiable is not infinitely differentiable? I'm asking this for two reasons: 1) $\mathbb{R}^2$ is somehow an analog to the complex plane (they are isomorphic). 2) Holomorphic means that is differentiable in any direction of the complex plane, so why if a function is differentiable in any direction of the real plane then it is not infinitely differentiable?",,['complex-analysis']
30,Can we prove that all equations can be solved via complex numbers?,Can we prove that all equations can be solved via complex numbers?,,"$x^2+1=0$ cannot be solved via real numbers. Because of this, we extend the real numbers to complex numbers.We can solve $x^2+1=0$  and $x^2+x+1=0$ equations after we define complex numbers. I wonder if we can solve all equations ( includes only the functions that are analytic.) via complex numbers or not?  If It is yes, how can we prove that claim? For example: Can $z^{100}-5z+2=e^{i.\operatorname{erf}(z)}$  be solved via complex numbers? where  $\operatorname{erf}(x) = \frac{2}{\sqrt{\pi}}\int_{0}^x e^{-t^2}\,\mathrm dt$ Note: This is just an example, I am not wondering the solution for a special example, I am wondering if a general proof is possible or not. Update: I mention the functions that are analytic. $\bar z$ or $\Re{(z)}$ are not analytic functions. Thanks for answers.","$x^2+1=0$ cannot be solved via real numbers. Because of this, we extend the real numbers to complex numbers.We can solve $x^2+1=0$  and $x^2+x+1=0$ equations after we define complex numbers. I wonder if we can solve all equations ( includes only the functions that are analytic.) via complex numbers or not?  If It is yes, how can we prove that claim? For example: Can $z^{100}-5z+2=e^{i.\operatorname{erf}(z)}$  be solved via complex numbers? where  $\operatorname{erf}(x) = \frac{2}{\sqrt{\pi}}\int_{0}^x e^{-t^2}\,\mathrm dt$ Note: This is just an example, I am not wondering the solution for a special example, I am wondering if a general proof is possible or not. Update: I mention the functions that are analytic. $\bar z$ or $\Re{(z)}$ are not analytic functions. Thanks for answers.",,['complex-analysis']
31,Fermat's last theorem for entire functions,Fermat's last theorem for entire functions,,"Let $f,g,h$ be entire functions, i.e., holomorphic functions on $\mathbb{C}$ . Suppose $f^n+g^n=h^n$ for some integer $n\geq2$ . What can we say about $f,g,h$ ? Clearly this is Fermat's last theorem for entire functions. I did a little search on the internet but, somewhat surprisingly, I found nothing relevant. Where can I find the answer? Thanks in advance. :) Edit: In particular, I would like to know why there are no nontrivial solutions for $n\geq4$ . Here a trivial solution is a solution of the form $f=ap,g=bp,h=cp$ where $a,b,c\in\mathbb{C}$ satisfy $a^n+b^n=c^n$ and $p$ is entire.","Let be entire functions, i.e., holomorphic functions on . Suppose for some integer . What can we say about ? Clearly this is Fermat's last theorem for entire functions. I did a little search on the internet but, somewhat surprisingly, I found nothing relevant. Where can I find the answer? Thanks in advance. :) Edit: In particular, I would like to know why there are no nontrivial solutions for . Here a trivial solution is a solution of the form where satisfy and is entire.","f,g,h \mathbb{C} f^n+g^n=h^n n\geq2 f,g,h n\geq4 f=ap,g=bp,h=cp a,b,c\in\mathbb{C} a^n+b^n=c^n p","['complex-analysis', 'number-theory', 'entire-functions']"
32,"If a holomorphic function $f$ has modulus $1$ on the unit circle, why does $f(z_0)=0$ for some $z_0$ in the disk?","If a holomorphic function  has modulus  on the unit circle, why does  for some  in the disk?",f 1 f(z_0)=0 z_0,"I don't understand the final step of an argument I read. Suppose $f$ is holomorphic in a neighborhood containing the closed unit disk, nonconstant, and $|f(z)|=1$ when $|z|=1$. There is some point $z_0$ in the unit disk such that $f(z_0)=0$. By the maximum modulus principle, it follows that $|f(z)|<1$ in the open unit disk. Since the closed disk is compact, $f$ obtains a minimum on the closed disk, necessarily on the interior in this situation. But why does that imply that $f(z_0)=0$ for some $z_0$? I'm aware of the minimum modulus principle, that the modulus of a holomorphic, nonconstant, nonzero function on a domain does not obtain a minimum in the domain. But I'm not sure if that applies here.","I don't understand the final step of an argument I read. Suppose $f$ is holomorphic in a neighborhood containing the closed unit disk, nonconstant, and $|f(z)|=1$ when $|z|=1$. There is some point $z_0$ in the unit disk such that $f(z_0)=0$. By the maximum modulus principle, it follows that $|f(z)|<1$ in the open unit disk. Since the closed disk is compact, $f$ obtains a minimum on the closed disk, necessarily on the interior in this situation. But why does that imply that $f(z_0)=0$ for some $z_0$? I'm aware of the minimum modulus principle, that the modulus of a holomorphic, nonconstant, nonzero function on a domain does not obtain a minimum in the domain. But I'm not sure if that applies here.",,['complex-analysis']
33,Prove $\left|1+z_1\right| +\left|1+z_2 \right| + \left|1+z_1z_2\right|\geq 2$,Prove,\left|1+z_1\right| +\left|1+z_2 \right| + \left|1+z_1z_2\right|\geq 2,"I am struggling with this problem Let $z_1,z_2\in\mathbb{C}$ prove that $$\left|1+z_1\right| +\left|1+z_2 \right| + \left|1+z_1z_2\right|\geq 2$$ I know a similar question has been solved: if $|z_i|=1$ prove $|z_1+1|+|z_2+1|+|z_1z_2+1|\ge 2$ . However, I don't have the $|z_1|=|z_2|=1$ condition, so I am not sure if the question is wrong or the last condition is implicit. This is the work I have done: \begin{align*}             |1+z_1|+|1+z_2|+|1+z_1 z_2| &= |1+z_1| + |1+z_2|+|-(1+z_1z_2)|\\                                 &\geq |1+z_1| + \left|(1+z_2)-(1+z_1z_2)\right|\\                                 &= |1+z_1|+ |z_2-z_1z_2|\\                                 &\geq |(1+z_1)+(z_2-z_1z_2)|   \end{align*} I'll appreciate your help. I am not sure what I am missing here.","I am struggling with this problem Let prove that I know a similar question has been solved: if $|z_i|=1$ prove $|z_1+1|+|z_2+1|+|z_1z_2+1|\ge 2$ . However, I don't have the condition, so I am not sure if the question is wrong or the last condition is implicit. This is the work I have done: I'll appreciate your help. I am not sure what I am missing here.","z_1,z_2\in\mathbb{C} \left|1+z_1\right| +\left|1+z_2 \right| + \left|1+z_1z_2\right|\geq 2 |z_1|=|z_2|=1 \begin{align*}
            |1+z_1|+|1+z_2|+|1+z_1 z_2| &= |1+z_1| + |1+z_2|+|-(1+z_1z_2)|\\
                                &\geq |1+z_1| + \left|(1+z_2)-(1+z_1z_2)\right|\\
                                &= |1+z_1|+ |z_2-z_1z_2|\\
                                &\geq |(1+z_1)+(z_2-z_1z_2)|
  \end{align*}","['complex-analysis', 'inequality', 'complex-numbers', 'cauchy-schwarz-inequality']"
34,The Schwarz Reflection Principle for a circle,The Schwarz Reflection Principle for a circle,,"I'm working on the following exercise (not homework) from Ahlfors' text: "" If $f(z)$ is analytic in $|z| \leq 1$ and satisfies $|f| = 1$ on $|z| = 1$, show that $f(z)$ is rational."" I already know about the reflection principle for the case of a half plane, so I tried using the ""Cayley transform"" $$T (\zeta)=\frac{\zeta-i}{\zeta+i}$$ Which maps the closed upper half plane onto the closed unit disk with $1$ removed. I defined $$g(\zeta)=(T^{-1} \circ f \circ T)(\zeta)=i\frac{1+f \left( \frac{\zeta-i}{\zeta+i} \right)}{1-f \left( \frac{\zeta-i}{\zeta+i} \right)},$$ And tried to apply the reflection principle in the book. $g$ is indeed analytic in the upper half plane, but for $\zeta \in \mathbb R$, I'm afraid that $g$ might get infinite (because on the boundary, $f$ takes values on the unit circle). If so, it will not be continuous and not even real, and the reflection principle is not applicable. Am I missing something here? After all Ahlfors does mention in the text a generalized reflection principle for arbitrary circles $C,C'$. Thanks","I'm working on the following exercise (not homework) from Ahlfors' text: "" If $f(z)$ is analytic in $|z| \leq 1$ and satisfies $|f| = 1$ on $|z| = 1$, show that $f(z)$ is rational."" I already know about the reflection principle for the case of a half plane, so I tried using the ""Cayley transform"" $$T (\zeta)=\frac{\zeta-i}{\zeta+i}$$ Which maps the closed upper half plane onto the closed unit disk with $1$ removed. I defined $$g(\zeta)=(T^{-1} \circ f \circ T)(\zeta)=i\frac{1+f \left( \frac{\zeta-i}{\zeta+i} \right)}{1-f \left( \frac{\zeta-i}{\zeta+i} \right)},$$ And tried to apply the reflection principle in the book. $g$ is indeed analytic in the upper half plane, but for $\zeta \in \mathbb R$, I'm afraid that $g$ might get infinite (because on the boundary, $f$ takes values on the unit circle). If so, it will not be continuous and not even real, and the reflection principle is not applicable. Am I missing something here? After all Ahlfors does mention in the text a generalized reflection principle for arbitrary circles $C,C'$. Thanks",,"['complex-analysis', 'analysis', 'reflection']"
35,Holomorphic function $\varphi$ with fixed point $z_0$ such that $\varphi'(z_o)=1$ is linear?,Holomorphic function  with fixed point  such that  is linear?,\varphi z_0 \varphi'(z_o)=1,"This is an exercise in complex analysis: Let $\Omega\subset{\Bbb C}$ be open and bounded, and $\varphi:\Omega\to\Omega$ a holomorphic function. Prove that if there exists a point $z_0\in\Omega$ such that   $$ \varphi(z_0)=z_0\qquad\text{and   }\qquad \varphi'(z_0)=1 $$   then $\varphi$ is linear. I'm trying work out the case $z_0=0$ first, in which $$ \varphi(z)=z+\sum_{n=2}^{\infty}a_nz^2. $$ It suffices to show that $a_n=0$ for all $n\geq 2$. If let $$ \varphi(z)=z+a_2z^2+O(z^3) $$ then  $$ \varphi^k(0)=z+ka_2z^2+O(z^3), $$ and $$ \varphi^k(0)=0,\quad (\varphi^k)'(0)=1. $$ If one can show that $\{ka_2\}_{k=1}^{\infty}$ is uniformly bounded, then one at least has $a_2=0$. But I don't know how to go on. Any idea?","This is an exercise in complex analysis: Let $\Omega\subset{\Bbb C}$ be open and bounded, and $\varphi:\Omega\to\Omega$ a holomorphic function. Prove that if there exists a point $z_0\in\Omega$ such that   $$ \varphi(z_0)=z_0\qquad\text{and   }\qquad \varphi'(z_0)=1 $$   then $\varphi$ is linear. I'm trying work out the case $z_0=0$ first, in which $$ \varphi(z)=z+\sum_{n=2}^{\infty}a_nz^2. $$ It suffices to show that $a_n=0$ for all $n\geq 2$. If let $$ \varphi(z)=z+a_2z^2+O(z^3) $$ then  $$ \varphi^k(0)=z+ka_2z^2+O(z^3), $$ and $$ \varphi^k(0)=0,\quad (\varphi^k)'(0)=1. $$ If one can show that $\{ka_2\}_{k=1}^{\infty}$ is uniformly bounded, then one at least has $a_2=0$. But I don't know how to go on. Any idea?",,[]
36,Pointwise convergence of sequences of holomorphic functions to holomorphic functions,Pointwise convergence of sequences of holomorphic functions to holomorphic functions,,"Let $(f_{n})_{n \in \mathbb{N}}$ be a sequence of holomorphic functions on the open unit disc $D$ in $\mathbb{C}$ , and suppose that this sequence converges pointwise to a function $f$ . By Osgood's theorem one can conclude then that there is an open and dense subset $V$ of the disc, so that the function $f$ is holomorphic there and that the convergence of the sequence is locally uniform on $V$ . If we further suppose that the limit function $f$ is also holomorphic on the entire disc $D$ , is it then possible to conclude that the sequence $(f_{n})_{n \in \mathbb{N}}$ converges locally uniformly to $f$ on $D$ ?","Let be a sequence of holomorphic functions on the open unit disc in , and suppose that this sequence converges pointwise to a function . By Osgood's theorem one can conclude then that there is an open and dense subset of the disc, so that the function is holomorphic there and that the convergence of the sequence is locally uniform on . If we further suppose that the limit function is also holomorphic on the entire disc , is it then possible to conclude that the sequence converges locally uniformly to on ?",(f_{n})_{n \in \mathbb{N}} D \mathbb{C} f V f V f D (f_{n})_{n \in \mathbb{N}} f D,['complex-analysis']
37,On the zeta sum $\sum_{n=1}^\infty[\zeta(5n)-1]$ and others,On the zeta sum  and others,\sum_{n=1}^\infty[\zeta(5n)-1],"For p = 2, we have, $\begin{align}&\sum_{n=1}^\infty[\zeta(pn)-1] = \frac{3}{4}\end{align}$ It seems there is a general form for odd p . For example, for p = 5, define $z_5 = e^{\pi i/5}$.  Then, $\begin{align} &5 \sum_{n=1}^\infty[\zeta(5n)-1] = 6+\gamma+z_5^{-1}\psi(z_5^{-1})+z_5\psi(z_5)+z_5^{-3}\psi(z_5^{-3})+z_5^{3}\psi(z_5^{3}) = 0.18976\dots \end{align}$ with the Euler-Mascheroni constant $\gamma$ and the digamma function $\psi(z)$. Anyone knows how to prove/disprove this? Also, how do we split $\psi(e^{\pi i/p})$ into its real and imaginary parts so as to express the above purely in real terms? More details in my blog .","For p = 2, we have, $\begin{align}&\sum_{n=1}^\infty[\zeta(pn)-1] = \frac{3}{4}\end{align}$ It seems there is a general form for odd p . For example, for p = 5, define $z_5 = e^{\pi i/5}$.  Then, $\begin{align} &5 \sum_{n=1}^\infty[\zeta(5n)-1] = 6+\gamma+z_5^{-1}\psi(z_5^{-1})+z_5\psi(z_5)+z_5^{-3}\psi(z_5^{-3})+z_5^{3}\psi(z_5^{3}) = 0.18976\dots \end{align}$ with the Euler-Mascheroni constant $\gamma$ and the digamma function $\psi(z)$. Anyone knows how to prove/disprove this? Also, how do we split $\psi(e^{\pi i/p})$ into its real and imaginary parts so as to express the above purely in real terms? More details in my blog .",,"['complex-analysis', 'special-functions', 'riemann-zeta', 'gamma-function']"
38,Prove that the composition of differentiable functions is differentiable.,Prove that the composition of differentiable functions is differentiable.,,"Prove that the composition of differentiable functions is differentiable. That is, if $f$ is differentiable at $z$, and if $g$ is differentiable at $f (z)$, then $g\circ f$ is differentiable at $z$. My attempt: I begin with $g(f(z+h))−g(f(z))=[g'(f(z))+\epsilon $]$[f(z+h)−f(z)]$ where  $\epsilon→0$ as $h → 0$ Does anyone could help me with this exercise?","Prove that the composition of differentiable functions is differentiable. That is, if $f$ is differentiable at $z$, and if $g$ is differentiable at $f (z)$, then $g\circ f$ is differentiable at $z$. My attempt: I begin with $g(f(z+h))−g(f(z))=[g'(f(z))+\epsilon $]$[f(z+h)−f(z)]$ where  $\epsilon→0$ as $h → 0$ Does anyone could help me with this exercise?",,['complex-analysis']
39,Why is every meromorphic function on $\hat{\mathbb{C}}$ a rational function?,Why is every meromorphic function on  a rational function?,\hat{\mathbb{C}},"I know that an analytic function on $\mathbb{C}$ with a nonessential singularity at $\infty$ is necessarily a polynomial. Now consider a meromorphic function $f$ on the extended complex plane $\hat{\mathbb{C}}$. I know that $f$ has only finitely many poles, say $z_1,\dots,z_n$ in $\mathbb{C}$. Suppose also that $f$ has a nonessential singularity at $\infty$. Then if $z_i$ have orders $n_i$, it follows that $\prod(z-z_i)^{n_i}f(z)$ is analytic on $\mathbb{C}$, and has a nonessential singularity at $\infty$, and is thus a polynomial, so $f$ is a rational function. But I'm curious, what if $f$ doesn't have a singularity at $\infty$, or in fact has an essential singularity at $\infty$ instead? Is $f$ still a rational function?","I know that an analytic function on $\mathbb{C}$ with a nonessential singularity at $\infty$ is necessarily a polynomial. Now consider a meromorphic function $f$ on the extended complex plane $\hat{\mathbb{C}}$. I know that $f$ has only finitely many poles, say $z_1,\dots,z_n$ in $\mathbb{C}$. Suppose also that $f$ has a nonessential singularity at $\infty$. Then if $z_i$ have orders $n_i$, it follows that $\prod(z-z_i)^{n_i}f(z)$ is analytic on $\mathbb{C}$, and has a nonessential singularity at $\infty$, and is thus a polynomial, so $f$ is a rational function. But I'm curious, what if $f$ doesn't have a singularity at $\infty$, or in fact has an essential singularity at $\infty$ instead? Is $f$ still a rational function?",,"['complex-analysis', 'rational-functions']"
40,Approximating roots of the truncated Taylor series of $\exp$ by values of the Lambert W function,Approximating roots of the truncated Taylor series of  by values of the Lambert W function,\exp,"If you map the nth roots of unity $z$ with the function $-W(-z/e)$ you get decent starting points for some root finding algorithm to the roots of the scaled truncated taylor series of $\exp$. Here W is the lambertW function, $e$ is $\exp(1)$ and 'scaled' in 'scaled truncated taylor series of exp' means the following: say if $$s_5(x) = 1+x+x^2/2+x^3/6+x^4/24+x^5/120$$ is the 'truncated taylor series of exp' of degree 5 then we will look at $s_5(5x)$ so we are looking at $s_n(nx)$  in general. Here is a plot for the case $n=33$ (it only works for uneven $n$). Using the lambert W function comes from formula (1.1) from paper 221 available from here . This formula is: $$e^{-nz}s_n(nz)=1-\frac{\sqrt{n}}{\tau_n\sqrt{2\pi}}\int_0^z(\zeta e^{1-\zeta})^n\textrm{d}\zeta,~~z\in \mathbb{C}$$ $-W(-z/e)$ is the inverse of $ze^{1-z}$. How to get a better map from the roots of unity to the roots of this polynomial? Alternatively, is there some infinite sum representation for the roots? There isn't much difference: ""applying"" LambertW to some start values is pretty much the same as an infinite series. Here is an octave script for such a plot as the one above (To use lambertw(), as in the script, install the 'specfun' package for octave - or use a more number/function theory centric system than octave).","If you map the nth roots of unity $z$ with the function $-W(-z/e)$ you get decent starting points for some root finding algorithm to the roots of the scaled truncated taylor series of $\exp$. Here W is the lambertW function, $e$ is $\exp(1)$ and 'scaled' in 'scaled truncated taylor series of exp' means the following: say if $$s_5(x) = 1+x+x^2/2+x^3/6+x^4/24+x^5/120$$ is the 'truncated taylor series of exp' of degree 5 then we will look at $s_5(5x)$ so we are looking at $s_n(nx)$  in general. Here is a plot for the case $n=33$ (it only works for uneven $n$). Using the lambert W function comes from formula (1.1) from paper 221 available from here . This formula is: $$e^{-nz}s_n(nz)=1-\frac{\sqrt{n}}{\tau_n\sqrt{2\pi}}\int_0^z(\zeta e^{1-\zeta})^n\textrm{d}\zeta,~~z\in \mathbb{C}$$ $-W(-z/e)$ is the inverse of $ze^{1-z}$. How to get a better map from the roots of unity to the roots of this polynomial? Alternatively, is there some infinite sum representation for the roots? There isn't much difference: ""applying"" LambertW to some start values is pretty much the same as an infinite series. Here is an octave script for such a plot as the one above (To use lambertw(), as in the script, install the 'specfun' package for octave - or use a more number/function theory centric system than octave).",,"['complex-analysis', 'reference-request']"
41,Analytic continuation of $\Phi(s)=\sum_{n \ge 1} e^{-n^s}$,Analytic continuation of,\Phi(s)=\sum_{n \ge 1} e^{-n^s},"(After 3 bounties I've also posted on mathoverflow ). While discussing theta functions , I thought: $\zeta(s)=\sum n^{-s}=1+2^{-s}+3^{-s}+ \cdot\cdot\cdot$ and $\Phi(s)=\sum e^{-n^s}=e^{-1}+e^{-2^s}+e^{-3^s}+\cdot\cdot\cdot $ What is the analytic continuation of $\Phi(s)?$ User @reuns had an insightful point that maybe, $\sum_n (e^{-n^{-s}}-1)=\sum_{k\ge 1} \frac{(-1)^k}{k!} \zeta(sk).$ If the sum were instead a product, then the analytic continuation would coincide with the analytic continuation of $\zeta(s).$","(After 3 bounties I've also posted on mathoverflow ). While discussing theta functions , I thought: and What is the analytic continuation of User @reuns had an insightful point that maybe, If the sum were instead a product, then the analytic continuation would coincide with the analytic continuation of",\zeta(s)=\sum n^{-s}=1+2^{-s}+3^{-s}+ \cdot\cdot\cdot \Phi(s)=\sum e^{-n^s}=e^{-1}+e^{-2^s}+e^{-3^s}+\cdot\cdot\cdot  \Phi(s)? \sum_n (e^{-n^{-s}}-1)=\sum_{k\ge 1} \frac{(-1)^k}{k!} \zeta(sk). \zeta(s).,"['complex-analysis', 'reference-request', 'soft-question', 'riemann-zeta', 'analytic-continuation']"
42,Solution set of $\cos(\cos(\cos(\cos(x)))) = \sin(\sin(\sin(\sin(x))))$,Solution set of,\cos(\cos(\cos(\cos(x)))) = \sin(\sin(\sin(\sin(x)))),"(I'm new here, so I hope this question hasn't come up before) A bit of motivation for the problem: It is well-known that the equations $\cos(x) = \sin(x)$, $\cos(\cos(x)) = \sin(\sin(x))$, and $\cos(\cos(\cos(x))) = \sin(\sin(\sin(x)))$ all have infinitely many solutions in $\mathbb{C}$ (the first and third have infinitely many solutions in $\mathbb{R}$ as well).  The proofs in these cases are elementary, but break down when applying it to further (lacking a better word) iterations.  Using the fourth to illustrate: Consider the two functions $\cos(\cos(\cos(\cos(z))))$ and $\sin(\sin(\sin(\sin(z))))$, and for convenience, let $H(z) = \cos(\cos(\cos(\cos(z)))) - \sin(\sin(\sin(\sin(z))))$. Also, let $V = \{z \in \mathbb{C} \, | \, H(z) = 0\}$. The original question (while not phrased in this manner) was: Find $V$. It is not difficult to verify that $\not\exists z\in V$ such that $\Im(z) = 0$.  To prove this, locate local extrema of the function $H(x)$ (where in an abuse of notation, I use $H(x)$ to denote the restriction of $H$ to the real numbers), and one will find that all (relative) maximum and minimum values of the function $H(x)$ are strictly positive.  In fact, one can prove that $H(x) \geq \frac{1}{10} > 0$, $\forall x \in \mathbb{R}$.  There is a sharper estimate, but this is sufficient for our purposes, and proves that there are no real solutions to the equation $H(x) = 0$. Having proved that there exist no real solutions, the question now becomes: Is it possible to analytically (i.e. without numerical methods) prove that $V$ is non-empty? My first instinct was to try to use the Argument Principle, as applied either to a ball of radius $n\in\mathbb{N}$ centered at $0$, or a rectangle of side-length $n$ centered at $0$, but I'm not sure if those integrals can be computed explicitly (even as contour integrals). Note:  It would be elementary to write an algorithm based on Newton's Method or some improvement thereof and attempt to find roots.  But stability is an issue if you are far from a root.","(I'm new here, so I hope this question hasn't come up before) A bit of motivation for the problem: It is well-known that the equations $\cos(x) = \sin(x)$, $\cos(\cos(x)) = \sin(\sin(x))$, and $\cos(\cos(\cos(x))) = \sin(\sin(\sin(x)))$ all have infinitely many solutions in $\mathbb{C}$ (the first and third have infinitely many solutions in $\mathbb{R}$ as well).  The proofs in these cases are elementary, but break down when applying it to further (lacking a better word) iterations.  Using the fourth to illustrate: Consider the two functions $\cos(\cos(\cos(\cos(z))))$ and $\sin(\sin(\sin(\sin(z))))$, and for convenience, let $H(z) = \cos(\cos(\cos(\cos(z)))) - \sin(\sin(\sin(\sin(z))))$. Also, let $V = \{z \in \mathbb{C} \, | \, H(z) = 0\}$. The original question (while not phrased in this manner) was: Find $V$. It is not difficult to verify that $\not\exists z\in V$ such that $\Im(z) = 0$.  To prove this, locate local extrema of the function $H(x)$ (where in an abuse of notation, I use $H(x)$ to denote the restriction of $H$ to the real numbers), and one will find that all (relative) maximum and minimum values of the function $H(x)$ are strictly positive.  In fact, one can prove that $H(x) \geq \frac{1}{10} > 0$, $\forall x \in \mathbb{R}$.  There is a sharper estimate, but this is sufficient for our purposes, and proves that there are no real solutions to the equation $H(x) = 0$. Having proved that there exist no real solutions, the question now becomes: Is it possible to analytically (i.e. without numerical methods) prove that $V$ is non-empty? My first instinct was to try to use the Argument Principle, as applied either to a ball of radius $n\in\mathbb{N}$ centered at $0$, or a rectangle of side-length $n$ centered at $0$, but I'm not sure if those integrals can be computed explicitly (even as contour integrals). Note:  It would be elementary to write an algorithm based on Newton's Method or some improvement thereof and attempt to find roots.  But stability is an issue if you are far from a root.",,"['complex-analysis', 'trigonometry']"
43,An outrageous way to derive a Laurent series: why does this work?,An outrageous way to derive a Laurent series: why does this work?,,"I had to compute a series expansion of $1/(e^{x}-1)$ about $x=0$, and in the course of its derivation, I made a couple of manipulations that are not allowed mathematically. Still, comparing the final result against Maple showed that it was right. The following is what I did: \begin{equation} \begin{split} \frac{1}{e^{x}-1} &= \frac{e^{-x}}{1-e^{-x}} = \sum_{n=1}^{\infty} e^{-nx} \\ &= \sum_{n=1}^{\infty} \sum_{k=-\infty}^{\infty}\frac{(-nx)^{k}}{\Gamma(k+1)}\\ &= \sum_{k=-\infty}^{\infty}(-1)^{k}\frac{x^{k}}{\Gamma(k+1)}\sum_{n=1}^{\infty} n^{k}\\ &= \sum_{k=-\infty}^{\infty}(-1)^{k}\frac{\zeta(-k)}{\Gamma(k+1)} x^{k}\\ &= \sum_{k=-1}^{\infty}(-1)^{k}\frac{\zeta(-k)}{\Gamma(k+1)} x^{k}\\ \end{split} \end{equation} I naively exchanged the order of summation to obtain the third line. Then, I pretended that the summation over $n$ converged (i.e., as if $k$ were always smaller than -1), and replaced it by the Riemann zeta function. Notice that whenever $1/\Gamma(k+1) = 0$ , the coefficient of $x^{k}$ vanishes except when $k=-1$, for which the poles of gamma and zeta functions cancel out and give a finite value. I suppose that there is a deeper reason that such unreliable steps led me to the correct result? My guess is that it has to do with the analytic structure of the summand as a function of $k$, but I haven't been able to figure out in detail why and when this works. I'll appreciate any insights on this.","I had to compute a series expansion of $1/(e^{x}-1)$ about $x=0$, and in the course of its derivation, I made a couple of manipulations that are not allowed mathematically. Still, comparing the final result against Maple showed that it was right. The following is what I did: \begin{equation} \begin{split} \frac{1}{e^{x}-1} &= \frac{e^{-x}}{1-e^{-x}} = \sum_{n=1}^{\infty} e^{-nx} \\ &= \sum_{n=1}^{\infty} \sum_{k=-\infty}^{\infty}\frac{(-nx)^{k}}{\Gamma(k+1)}\\ &= \sum_{k=-\infty}^{\infty}(-1)^{k}\frac{x^{k}}{\Gamma(k+1)}\sum_{n=1}^{\infty} n^{k}\\ &= \sum_{k=-\infty}^{\infty}(-1)^{k}\frac{\zeta(-k)}{\Gamma(k+1)} x^{k}\\ &= \sum_{k=-1}^{\infty}(-1)^{k}\frac{\zeta(-k)}{\Gamma(k+1)} x^{k}\\ \end{split} \end{equation} I naively exchanged the order of summation to obtain the third line. Then, I pretended that the summation over $n$ converged (i.e., as if $k$ were always smaller than -1), and replaced it by the Riemann zeta function. Notice that whenever $1/\Gamma(k+1) = 0$ , the coefficient of $x^{k}$ vanishes except when $k=-1$, for which the poles of gamma and zeta functions cancel out and give a finite value. I suppose that there is a deeper reason that such unreliable steps led me to the correct result? My guess is that it has to do with the analytic structure of the summand as a function of $k$, but I haven't been able to figure out in detail why and when this works. I'll appreciate any insights on this.",,"['complex-analysis', 'riemann-zeta', 'fake-proofs', 'laurent-series']"
44,"Power series which diverges precisely at the roots of unity, converges elsewhere","Power series which diverges precisely at the roots of unity, converges elsewhere",,"Is there a complex power series $\sum a_nz^n$ with radius of convergence $1$ which diverges at the roots of unity (e.g., $z=e^{2\pi i\theta}$ , $\theta \in \mathbb{Q}$ ) and converges elsewhere on the unit circle ( $z=e^{2\pi i\theta}$ , $\theta \in \mathbb{R} \setminus \mathbb{Q}$ )? I know $\sum \frac{z^n}{n}$ is a series with radius of convergence $1$ which converges everywhere on the unit circle except $1$ . Perhaps we can play around with this to get the desired result. I also know that $\sum \frac{z^{n!}}{n}$ diverges at the roots of unity, but I am not aware of a result that it converges at all other points on the unit circle. Note similar questions have been asked here before, but they do not directly answer the question posed above.","Is there a complex power series with radius of convergence which diverges at the roots of unity (e.g., , ) and converges elsewhere on the unit circle ( , )? I know is a series with radius of convergence which converges everywhere on the unit circle except . Perhaps we can play around with this to get the desired result. I also know that diverges at the roots of unity, but I am not aware of a result that it converges at all other points on the unit circle. Note similar questions have been asked here before, but they do not directly answer the question posed above.",\sum a_nz^n 1 z=e^{2\pi i\theta} \theta \in \mathbb{Q} z=e^{2\pi i\theta} \theta \in \mathbb{R} \setminus \mathbb{Q} \sum \frac{z^n}{n} 1 1 \sum \frac{z^{n!}}{n},"['complex-analysis', 'convergence-divergence', 'power-series', 'conditional-convergence']"
45,Proving that all entire and injective functions take the form $f = ax + b$?,Proving that all entire and injective functions take the form ?,f = ax + b,"Prove that all entire functions that are also injective take the form $f(z)=az+b$ with $a,b\in\Bbb C$ . Solution: We take $g : \Bbb C^* \to \Bbb C$ , $g( z) = f(1/z)$ , which is holomorphic everywhere except the origin. Now, we try to find out what type of singularity is the origin for $g$ . If the origin is a removable singularity for $g$ , then $g$ is bounded on a closed disk centred at the origin, which implies that $f$ is bounded outside a closed circle containing the origin. But $f$ is bounded on this closed circle, because $f$ is continuous, therefore, $f$ is bounded. Since $f$ is entire and bounded, by Liouville's Theorem, $f$ is constant. This contradicts the injectivity of $f$ . So the origin is not a removable singularity for $g$ . Suppose now that $0$ is an essential singularity for $g$ . Then, by Casorati-Weierstrass Theorem, if we chose a punctured disk centred at the origin $D^*$ , then $g ( D^*)$ is dense in $\Bbb C$ . This implies $f (\{ \lvert z\lvert > r\})$ is dense in $\Bbb C$ . But $f (\{ \lvert z\lvert < r\})$ is open because any holomorphic mapping is an open mapping. Then $f (\{ \lvert z\lvert > r\})\cap f (\{ \lvert z\lvert < r\})\ne \emptyset$ , which is again a contradiction with the injectivity of $f$ . Therefore $0$ is a pole for $g$ . Since the Laurent expansion is unique, and the principal part of $g$ is the same as the analytic part of $f$ , it follows that the analytic part of $f$ has finitely many terms, which implies that $f$ is a polynomial. Since $f$ is injective, the polynomial can have at most one root. Because $f$ is not constant, we conclude that the only expression of $f$ can be of the form $f ( z ) = az + b$ , where $a, b \in \Bbb C$ and $a \ne 0$ . Original text image I'm a little confused at both the overall logic in this proof. Are we simply using $g(z)$ to make conclusions about $f(z)$ , because $g(z)$ is the reciprocal of $f$ ? Is the proof assuming that $f$ is injective and entire (all the while knowing that it has some sort of singularity at $z = 0$ ), and then trying to reach contradictions in the essential singularity and removable singularity cases? Then, once it concludes that $z = 0$ is a pole singularity, it reaches the conclusion that $f$ must be of the form $f(z) = az + b$ ? Also, more specific questions about the different cases: Removable singularity case: Why is $f$ bounded on the closed circle if $f$ is continuous? Am I missing something simple? Essential singularity case: Why exactly is $f(\{|z| > r \} \cap f(\{|z|<r\}) \neq \emptyset$ ? $f(\{|z| > r \})$ is dense, but how does $f(\{|z|<r\}$ being open guarantee that their union is non-empty?","Prove that all entire functions that are also injective take the form with . Solution: We take , , which is holomorphic everywhere except the origin. Now, we try to find out what type of singularity is the origin for . If the origin is a removable singularity for , then is bounded on a closed disk centred at the origin, which implies that is bounded outside a closed circle containing the origin. But is bounded on this closed circle, because is continuous, therefore, is bounded. Since is entire and bounded, by Liouville's Theorem, is constant. This contradicts the injectivity of . So the origin is not a removable singularity for . Suppose now that is an essential singularity for . Then, by Casorati-Weierstrass Theorem, if we chose a punctured disk centred at the origin , then is dense in . This implies is dense in . But is open because any holomorphic mapping is an open mapping. Then , which is again a contradiction with the injectivity of . Therefore is a pole for . Since the Laurent expansion is unique, and the principal part of is the same as the analytic part of , it follows that the analytic part of has finitely many terms, which implies that is a polynomial. Since is injective, the polynomial can have at most one root. Because is not constant, we conclude that the only expression of can be of the form , where and . Original text image I'm a little confused at both the overall logic in this proof. Are we simply using to make conclusions about , because is the reciprocal of ? Is the proof assuming that is injective and entire (all the while knowing that it has some sort of singularity at ), and then trying to reach contradictions in the essential singularity and removable singularity cases? Then, once it concludes that is a pole singularity, it reaches the conclusion that must be of the form ? Also, more specific questions about the different cases: Removable singularity case: Why is bounded on the closed circle if is continuous? Am I missing something simple? Essential singularity case: Why exactly is ? is dense, but how does being open guarantee that their union is non-empty?","f(z)=az+b a,b\in\Bbb C g : \Bbb C^* \to \Bbb C g( z) = f(1/z) g g g f f f f f f f g 0 g D^* g ( D^*) \Bbb C f (\{ \lvert z\lvert > r\}) \Bbb C f (\{ \lvert z\lvert < r\}) f (\{ \lvert z\lvert > r\})\cap f (\{ \lvert z\lvert < r\})\ne \emptyset f 0 g g f f f f f f f ( z ) = az + b a, b \in \Bbb C a \ne 0 g(z) f(z) g(z) f f z = 0 z = 0 f f(z) = az + b f f f(\{|z| > r \} \cap f(\{|z|<r\}) \neq \emptyset f(\{|z| > r \}) f(\{|z|<r\}",['complex-analysis']
46,Why does a meromorphic function in the (extended) complex plane have finitely many poles?,Why does a meromorphic function in the (extended) complex plane have finitely many poles?,,Let $f$ be meromorphic in $\mathbb{C} \cup \{\infty\}$. Why must $f$ have only finitely many poles? Edit: Renamed question following the comments.,Let $f$ be meromorphic in $\mathbb{C} \cup \{\infty\}$. Why must $f$ have only finitely many poles? Edit: Renamed question following the comments.,,['complex-analysis']
47,"Let $f(z)$ be entire function. Show that if $f(z)$ is real when $|z| = 1$, then $f(z)$ must be a constant function using Maximum Modulus theorem","Let  be entire function. Show that if  is real when , then  must be a constant function using Maximum Modulus theorem",f(z) f(z) |z| = 1 f(z),"Let $f(z)$ be entire function. Consider the functions $e^{if(z)}$ and $e^{−if(z)}$ and applying the Maximum Modulus Theorem, show that if $f(z)$ is real when $|z| = 1$, then $f(z)$ must be a constant function. (We take $f(z)=u(z)+iv(z)$) I am confused as so far I have $|g(z)|=|e^{if(z)}|=|e^{-v(z)}|$ and then since $f(z)$ is real, $f(z)=u(z)$ and $v(z)=0$ so I assumed it would follow that $|g(z)|=|e^{v(z)}|=1$. Similarly, $|g(z)|=|e^{-if(z)}|=|e^{v(z)}|=1$. Using Liouville I assumed one could say that both $g(z)$ and $h(z)$ are bounded entire functions, they are constant and so it follows that $v(z)$ is constant, meaning that both its partial derivatives are equal to 0 and, due to Cauchy Riemann, both of the partial derivatives of $u(z)$ are equal to zero.  It would then follow that $f(z)$ is constant. I don't know how to go about the question using the Maximum Modulus Theorem, also I feel I am overlooking the importance of $|z|=1$ perhaps? Any help would be much appreciated!!","Let $f(z)$ be entire function. Consider the functions $e^{if(z)}$ and $e^{−if(z)}$ and applying the Maximum Modulus Theorem, show that if $f(z)$ is real when $|z| = 1$, then $f(z)$ must be a constant function. (We take $f(z)=u(z)+iv(z)$) I am confused as so far I have $|g(z)|=|e^{if(z)}|=|e^{-v(z)}|$ and then since $f(z)$ is real, $f(z)=u(z)$ and $v(z)=0$ so I assumed it would follow that $|g(z)|=|e^{v(z)}|=1$. Similarly, $|g(z)|=|e^{-if(z)}|=|e^{v(z)}|=1$. Using Liouville I assumed one could say that both $g(z)$ and $h(z)$ are bounded entire functions, they are constant and so it follows that $v(z)$ is constant, meaning that both its partial derivatives are equal to 0 and, due to Cauchy Riemann, both of the partial derivatives of $u(z)$ are equal to zero.  It would then follow that $f(z)$ is constant. I don't know how to go about the question using the Maximum Modulus Theorem, also I feel I am overlooking the importance of $|z|=1$ perhaps? Any help would be much appreciated!!",,['complex-analysis']
48,Residue of $z^2 e^{1/\sin z}$ at $z=\pi$,Residue of  at,z^2 e^{1/\sin z} z=\pi,"A while back I was working through many problems in Mathews and Walker's Mathematical Methods of Physics . In the appendix is this problem: A-6 . Find the residue of the function $z^2 e^{1/\sin z}$ at the isolated (essential) singularity $z=\pi$. It is not too hard to get this residue numerically (it is $-7.5764\cdots$).  I struggled for many pages to find an exact result.  Does an analytical expression for this residue exist and, if so, what is it?  (And briefly, how did you get it?)","A while back I was working through many problems in Mathews and Walker's Mathematical Methods of Physics . In the appendix is this problem: A-6 . Find the residue of the function $z^2 e^{1/\sin z}$ at the isolated (essential) singularity $z=\pi$. It is not too hard to get this residue numerically (it is $-7.5764\cdots$).  I struggled for many pages to find an exact result.  Does an analytical expression for this residue exist and, if so, what is it?  (And briefly, how did you get it?)",,"['complex-analysis', 'special-functions', 'physics']"
49,Is $1+2+3+4+\cdots=-\frac{1}{12}$ the unique ''value'' of this series?,Is  the unique ''value'' of this series?,1+2+3+4+\cdots=-\frac{1}{12},"I'm reading about zeta-function regularization in physics and I have some mathematical doubt.  I understand  that, since a sum of infinite terms is not well defined in a field, a series that is considered divergent in the  usual meaning  can have a ''value'', defined in some less conventional way. The zeta-function regularization is one of such ways that, as an example, assign the value $-\frac{1}{12}$ to the infinite series $S=1+2+3+4+...$ using the fact that the zeta-function $\zeta(s)$ is the analytic continuation of the series $\sum_{n=1}^\infty n^{-s}$, and $\zeta(-1)=-\frac{1}{12}$. But how we can be sure that there does not exists other possible ''regularizations'' that gives different values to the same series? And, if more than one value does exist, there is some  criterion to select between them?  Or the zeta-function regularization is preferred only for physical motivations (because the experiments confirm its values)? On the web I've found a lot of posts less or more reliable about this topic, someone knows a reference to a well defined axiomatic approach to the problem of the value of divergent series?","I'm reading about zeta-function regularization in physics and I have some mathematical doubt.  I understand  that, since a sum of infinite terms is not well defined in a field, a series that is considered divergent in the  usual meaning  can have a ''value'', defined in some less conventional way. The zeta-function regularization is one of such ways that, as an example, assign the value $-\frac{1}{12}$ to the infinite series $S=1+2+3+4+...$ using the fact that the zeta-function $\zeta(s)$ is the analytic continuation of the series $\sum_{n=1}^\infty n^{-s}$, and $\zeta(-1)=-\frac{1}{12}$. But how we can be sure that there does not exists other possible ''regularizations'' that gives different values to the same series? And, if more than one value does exist, there is some  criterion to select between them?  Or the zeta-function regularization is preferred only for physical motivations (because the experiments confirm its values)? On the web I've found a lot of posts less or more reliable about this topic, someone knows a reference to a well defined axiomatic approach to the problem of the value of divergent series?",,"['complex-analysis', 'mathematical-physics', 'divergent-series', 'regularization']"
50,complex conjugates of holomorphic functions,complex conjugates of holomorphic functions,,"I came across this question whilst doing some research into complex analysis, and I just can't see what to do! Let $f(z)$ be a holomorphic function on $\mathbb{C}$. Show that $\overline{f(\overline{z})}$ is holomorphic, whilst $f(\overline{z})$ is holomorphic if and only if $f(z)$ is constant. I know that holomorphic means that the function is differentiable everywhere, and I need to apply the Cauchy-Riemann equations somehow, but I'm not sure how to approach this.","I came across this question whilst doing some research into complex analysis, and I just can't see what to do! Let $f(z)$ be a holomorphic function on $\mathbb{C}$. Show that $\overline{f(\overline{z})}$ is holomorphic, whilst $f(\overline{z})$ is holomorphic if and only if $f(z)$ is constant. I know that holomorphic means that the function is differentiable everywhere, and I need to apply the Cauchy-Riemann equations somehow, but I'm not sure how to approach this.",,"['complex-analysis', 'analysis']"
51,"What is the geometric intuition for the $\bar \partial$-Poincare lemma, or for $\bar \partial$ more generally?","What is the geometric intuition for the -Poincare lemma, or for  more generally?",\bar \partial \bar \partial,"The one variable $\bar \partial$ -Poincare lemma is proven in Huybrechts and Forster  and so on in essentially the same way: one shows that for a local form $f d \bar z$ , with $$g(z) := \frac{1}{2\pi i} \int_{B_\varepsilon} \frac{f(w)}{w-z} dw \wedge d\bar w$$ we have $\bar \partial g = fd\bar z$ . I can follow the proof on a formal, line by line level, but the manipulations used to obtain the result really don't have any geometric meaning to me, partially because I really don't understand how I should think of $\bar \partial$ except as ""some operator with kernel the holomorphic functions, and which we really really want to be locally exact so that we can use sheaf cohomology.""  My questions here are: is there any intuitive reason the lemma would be true at all, and why should this integral, which suspiciously resembles that of the Cauchy integral formula, be the correct $g$ , and is there more geometric intuition for the $\bar \partial$ operator generally than 'it kills holomorphic functions'?","The one variable -Poincare lemma is proven in Huybrechts and Forster  and so on in essentially the same way: one shows that for a local form , with we have . I can follow the proof on a formal, line by line level, but the manipulations used to obtain the result really don't have any geometric meaning to me, partially because I really don't understand how I should think of except as ""some operator with kernel the holomorphic functions, and which we really really want to be locally exact so that we can use sheaf cohomology.""  My questions here are: is there any intuitive reason the lemma would be true at all, and why should this integral, which suspiciously resembles that of the Cauchy integral formula, be the correct , and is there more geometric intuition for the operator generally than 'it kills holomorphic functions'?",\bar \partial f d \bar z g(z) := \frac{1}{2\pi i} \int_{B_\varepsilon} \frac{f(w)}{w-z} dw \wedge d\bar w \bar \partial g = fd\bar z \bar \partial g \bar \partial,"['complex-analysis', 'homology-cohomology', 'intuition', 'complex-geometry', 'complex-integration']"
52,Convergence of the infinite product $\prod_{n = 1}^{\infty} \frac{z - \alpha_n}{z - \beta_n}$,Convergence of the infinite product,\prod_{n = 1}^{\infty} \frac{z - \alpha_n}{z - \beta_n},"I've been trying to solve this homework problem for a while but I can't seem to get any significant ideas about how to approach it, so I would really appreciate any hints that could help me solve it. The problem is exercise 8.14 from Steven Krantz and Robert Greene's book Function Theory of One Complex Variable. It goes as follows: Suppose that    $$\sum |\alpha_n - \beta_n| < \infty$$   Then determine the largest open set of $z$ for which   $$\prod_{n = 1}^{\infty} \frac{z - \alpha_n}{z - \beta_n}$$   converges normally. What I've tried so far is writing the factors as $$\frac{z - \alpha_n}{z - \beta_n} = 1 + \frac{z - \alpha_n}{z - \beta_n} - 1 = 1 + \frac{\beta_n - \alpha_n}{z - \beta_n}$$ so as to put the infinite product in the form $\displaystyle{\prod (1 + f_n(z))}$ to try to apply the basic convergence criteria I have available  which says that this product would converge normally if the series $$\sum |f_n(z)|$$  converges normally. Now I'm kind of stuck here because I think that maybe I would have to bound this sum with the sum $\sum |\alpha_n - \beta_n|$ but I'm not sure about how to proceed (assuming that this is the right way to follow). So I would really appreciate some hints that would get me in the right track to solve this problem. Thank you very much.","I've been trying to solve this homework problem for a while but I can't seem to get any significant ideas about how to approach it, so I would really appreciate any hints that could help me solve it. The problem is exercise 8.14 from Steven Krantz and Robert Greene's book Function Theory of One Complex Variable. It goes as follows: Suppose that    $$\sum |\alpha_n - \beta_n| < \infty$$   Then determine the largest open set of $z$ for which   $$\prod_{n = 1}^{\infty} \frac{z - \alpha_n}{z - \beta_n}$$   converges normally. What I've tried so far is writing the factors as $$\frac{z - \alpha_n}{z - \beta_n} = 1 + \frac{z - \alpha_n}{z - \beta_n} - 1 = 1 + \frac{\beta_n - \alpha_n}{z - \beta_n}$$ so as to put the infinite product in the form $\displaystyle{\prod (1 + f_n(z))}$ to try to apply the basic convergence criteria I have available  which says that this product would converge normally if the series $$\sum |f_n(z)|$$  converges normally. Now I'm kind of stuck here because I think that maybe I would have to bound this sum with the sum $\sum |\alpha_n - \beta_n|$ but I'm not sure about how to proceed (assuming that this is the right way to follow). So I would really appreciate some hints that would get me in the right track to solve this problem. Thank you very much.",,['complex-analysis']
53,Prove $\int_0^\infty\Big|\Gamma\Big(\frac13+\frac{i\alpha}{\pi}\Big)\Big|^6\alpha \sinh\alpha~d\alpha=\frac{\pi\sqrt{27}}{4}\Gamma^9\Big(\frac23\Big)$,Prove,\int_0^\infty\Big|\Gamma\Big(\frac13+\frac{i\alpha}{\pi}\Big)\Big|^6\alpha \sinh\alpha~d\alpha=\frac{\pi\sqrt{27}}{4}\Gamma^9\Big(\frac23\Big),"In the book Formfactors in integrable models of quantum field theory F.A. Smirnov gives the following integral (there is a typo in the book, kindly corrected by Mr. Smirnov) \begin{align} &\int_{-\infty}^{\infty}\prod_{j=1}^3\Gamma\Bigl(\frac 1 3 -\frac {\alpha-\beta_j}{2\pi i}\Bigr) \Gamma\Bigl(\frac 1 3 +\frac {\alpha-\beta_j}{2\pi i}\Bigr)(3\alpha-\sum\beta_m)e^{\frac 1 2(-\alpha+\sum\beta_m)}d\alpha\nonumber\\ &=\frac{(2\pi \Gamma(\frac 2 3))^2}{\Gamma(\frac 4 3)} \prod_{k<j}\Gamma\Bigl(\frac 2 3 -\frac {\beta_k-\beta_j}{2\pi i}\Bigr) \Gamma\Bigl(\frac 2 3 +\frac {\beta_k-\beta_j}{2\pi i}\Bigr) \sum e^{\beta_m}\,,\label{1}\tag{1} \end{align} where $|\text{Im}~\beta_j|<2\pi/3$ . The derivation uses some non standard techniques developed by the author with the regard to integrable models in quantum field theory. Q: Can anybody derive this integral directly? \eqref{1} looks like formula $(3.6.6)$ in the book ""Special functions"" by Andrews, Askey, and Roy \begin{align} \frac1{2\pi i}&\int\Gamma(a+s)\Gamma(b+s)\Gamma(c+s)\Gamma(a-s)\Gamma(b-s)\Gamma(c-s)\cos\pi s\,ds\\ &=\frac{\Gamma(a)\Gamma(b)\Gamma(c)\Gamma(a+b)\Gamma(b+c)\Gamma(c+a)}{2\Gamma(a+b+c)}\label{2}\tag{2} \end{align} where the contour of integration is along the imaginary axis suitably deformed so that increasing sequence of poles is separated from the decreasing sequence of poles. One can see that they both contain $3$ independent parameters. I tried to show that \eqref{1} is a consequence of \eqref{2} but so far couldn't.","In the book Formfactors in integrable models of quantum field theory F.A. Smirnov gives the following integral (there is a typo in the book, kindly corrected by Mr. Smirnov) where . The derivation uses some non standard techniques developed by the author with the regard to integrable models in quantum field theory. Q: Can anybody derive this integral directly? \eqref{1} looks like formula in the book ""Special functions"" by Andrews, Askey, and Roy where the contour of integration is along the imaginary axis suitably deformed so that increasing sequence of poles is separated from the decreasing sequence of poles. One can see that they both contain independent parameters. I tried to show that \eqref{1} is a consequence of \eqref{2} but so far couldn't.","\begin{align}
&\int_{-\infty}^{\infty}\prod_{j=1}^3\Gamma\Bigl(\frac 1 3 -\frac {\alpha-\beta_j}{2\pi i}\Bigr)
\Gamma\Bigl(\frac 1 3 +\frac {\alpha-\beta_j}{2\pi i}\Bigr)(3\alpha-\sum\beta_m)e^{\frac 1 2(-\alpha+\sum\beta_m)}d\alpha\nonumber\\
&=\frac{(2\pi \Gamma(\frac 2 3))^2}{\Gamma(\frac 4 3)}
\prod_{k<j}\Gamma\Bigl(\frac 2 3 -\frac {\beta_k-\beta_j}{2\pi i}\Bigr)
\Gamma\Bigl(\frac 2 3 +\frac {\beta_k-\beta_j}{2\pi i}\Bigr)
\sum e^{\beta_m}\,,\label{1}\tag{1}
\end{align} |\text{Im}~\beta_j|<2\pi/3 (3.6.6) \begin{align}
\frac1{2\pi i}&\int\Gamma(a+s)\Gamma(b+s)\Gamma(c+s)\Gamma(a-s)\Gamma(b-s)\Gamma(c-s)\cos\pi s\,ds\\
&=\frac{\Gamma(a)\Gamma(b)\Gamma(c)\Gamma(a+b)\Gamma(b+c)\Gamma(c+a)}{2\Gamma(a+b+c)}\label{2}\tag{2}
\end{align} 3","['complex-analysis', 'definite-integrals', 'special-functions', 'contour-integration', 'alternative-proof']"
54,Is every entire function is a sum of an entire function bounded on every horizontal strip and an entire function bounded on every vertical strip?,Is every entire function is a sum of an entire function bounded on every horizontal strip and an entire function bounded on every vertical strip?,,"Is it true that every entire function  is a sum of an entire function bounded on every horizontal strip (horizontal strip is a set of the form $H_y:=\{x+iy : x \in \mathbb R \}$ ) and an entire function bounded on every vertical strip (vertical strip is a set of the form $V_x:=\{x+iy:y\in \mathbb R \}$) ? I see no way of rigorously deciding it anyway. NOTE : By entire function , I mean any holomorphic function $f: \mathbb C \to \mathbb C$","Is it true that every entire function  is a sum of an entire function bounded on every horizontal strip (horizontal strip is a set of the form $H_y:=\{x+iy : x \in \mathbb R \}$ ) and an entire function bounded on every vertical strip (vertical strip is a set of the form $V_x:=\{x+iy:y\in \mathbb R \}$) ? I see no way of rigorously deciding it anyway. NOTE : By entire function , I mean any holomorphic function $f: \mathbb C \to \mathbb C$",,['complex-analysis']
55,What do $dz$ and $|dz|$ mean?,What do  and  mean?,dz |dz|,"I'm having a hard time understanding complex differentials. I know that when I have a field $\mathbb K$ and a $\mathbb K-$vector space $\mathbb K^n,$ then we define $dx_i\in \mathrm{Lin}(\mathbb K^n,\mathbb K)$ on the standard basis $\{e_i\}_{i=1}^n$ of $\mathbb K^n$ as follows: $$ dx_i(e_j)=\begin{cases}1 & \mbox{ for }j=i,\\ 0 & \mbox{ for }j \neq i.\end{cases}$$ So defined $dx_i$ form a basis of $\mathrm{Lin}(\mathbb K^n,\mathbb K)$ Now I have the symbol $dz$ for $z$ being a complex variable and I'm not sure I understand what it means. I know that this is supposed to be true and a definition of $dz:$ $$dz =d\:\mathrm{Re}(z)+id\:\mathrm{Im}(z).$$ I cannot fathom this definition though. What is the space in which the operations on the right-hand side are performed? $\mathrm{Re}(z)$ and $\mathrm{Im}(z)$ are real variables, right? So the space should be $\mathrm{Lin}(\mathbb R^2,\mathbb R).$ But this is an $\mathbb R -$space, not a $\mathbb C -$space so the multiplication by $i$ shouldn't be allowed. And then I see the symbol $|dz|$ and integrals are computed with it, like here , page 3. What does this symbol mean? Edit: I would like to improve the formulation of a part of my problem and post my newly found (thanks to the comments) answer to that part. Let's take the equality $$dz=dx+idy,$$ where $x=\mathrm{Re}(z)$ and $y=\mathrm{Im}(z).$ According to the definition in the first paragraph of this post, $dz$ is a $\mathbb C-$linear map, $dz:\mathbb C\to\mathbb C,$ and $dz=\operatorname{id}_{\mathbb C}.$ On the other hand, $dx$ and $dy$ are $\mathbb R-$linear maps, $dx,dy:\mathbb R^2\to \mathbb R$ given by $$ \begin{eqnarray} dx(e_1)=1,\\ dx(e_2)=0,\\ dy(e_1)=0,\\ dy(e_2)=1. \end{eqnarray} $$ I understand that I should carry out the identification: $$\mathbb R^2\ni e_1\mapsto 1\in\mathbb C,$$$$\mathbb R^2\ni e_2\mapsto i\in \mathbb C.$$ This gives me $$ \begin{eqnarray} dx(1)=1,\\ dx(i)=0,\\ dy(1)=0,\\ dy(i)=1. \end{eqnarray} $$ These are clearly not $\mathbb{C}-$linear maps. This was my problem. $dy$ is not a $\mathbb{C}-$linear map but just an $\mathbb{R}-$linear map from $\mathbb C$ into $\mathbb R.$ The set of all such linear maps is an $\mathbb R-$vector space, not a $\mathbb{C}-$vector space so there is no such thing as the product $i\cdot dy.$ However, after Pierre-Yves Gaillard's comments, I realized that I should also carry out another identification -- in the codomains of $dx$ and $dy:$ $$\mathbb R \ni 1 \mapsto 1\in \mathbb C,$$ that is consider the codomains of $dx$ and $dy$ to be the real axis of the complex plane. This doesn't make $dx$ and $dy$ $\mathbb C-$linear maps, but it does make them complex functions and so allows them to be multiplied by $i$. And indeed, now  $$dz=\operatorname{id}_{\mathbb C}=dx+idy.$$ I'm sorry about being so obtuse. I'm not sure this question has any value at all to the community, so perhaps I should remove this part? However, I still do not understand what the definition of $|dz|$ is in these terms.","I'm having a hard time understanding complex differentials. I know that when I have a field $\mathbb K$ and a $\mathbb K-$vector space $\mathbb K^n,$ then we define $dx_i\in \mathrm{Lin}(\mathbb K^n,\mathbb K)$ on the standard basis $\{e_i\}_{i=1}^n$ of $\mathbb K^n$ as follows: $$ dx_i(e_j)=\begin{cases}1 & \mbox{ for }j=i,\\ 0 & \mbox{ for }j \neq i.\end{cases}$$ So defined $dx_i$ form a basis of $\mathrm{Lin}(\mathbb K^n,\mathbb K)$ Now I have the symbol $dz$ for $z$ being a complex variable and I'm not sure I understand what it means. I know that this is supposed to be true and a definition of $dz:$ $$dz =d\:\mathrm{Re}(z)+id\:\mathrm{Im}(z).$$ I cannot fathom this definition though. What is the space in which the operations on the right-hand side are performed? $\mathrm{Re}(z)$ and $\mathrm{Im}(z)$ are real variables, right? So the space should be $\mathrm{Lin}(\mathbb R^2,\mathbb R).$ But this is an $\mathbb R -$space, not a $\mathbb C -$space so the multiplication by $i$ shouldn't be allowed. And then I see the symbol $|dz|$ and integrals are computed with it, like here , page 3. What does this symbol mean? Edit: I would like to improve the formulation of a part of my problem and post my newly found (thanks to the comments) answer to that part. Let's take the equality $$dz=dx+idy,$$ where $x=\mathrm{Re}(z)$ and $y=\mathrm{Im}(z).$ According to the definition in the first paragraph of this post, $dz$ is a $\mathbb C-$linear map, $dz:\mathbb C\to\mathbb C,$ and $dz=\operatorname{id}_{\mathbb C}.$ On the other hand, $dx$ and $dy$ are $\mathbb R-$linear maps, $dx,dy:\mathbb R^2\to \mathbb R$ given by $$ \begin{eqnarray} dx(e_1)=1,\\ dx(e_2)=0,\\ dy(e_1)=0,\\ dy(e_2)=1. \end{eqnarray} $$ I understand that I should carry out the identification: $$\mathbb R^2\ni e_1\mapsto 1\in\mathbb C,$$$$\mathbb R^2\ni e_2\mapsto i\in \mathbb C.$$ This gives me $$ \begin{eqnarray} dx(1)=1,\\ dx(i)=0,\\ dy(1)=0,\\ dy(i)=1. \end{eqnarray} $$ These are clearly not $\mathbb{C}-$linear maps. This was my problem. $dy$ is not a $\mathbb{C}-$linear map but just an $\mathbb{R}-$linear map from $\mathbb C$ into $\mathbb R.$ The set of all such linear maps is an $\mathbb R-$vector space, not a $\mathbb{C}-$vector space so there is no such thing as the product $i\cdot dy.$ However, after Pierre-Yves Gaillard's comments, I realized that I should also carry out another identification -- in the codomains of $dx$ and $dy:$ $$\mathbb R \ni 1 \mapsto 1\in \mathbb C,$$ that is consider the codomains of $dx$ and $dy$ to be the real axis of the complex plane. This doesn't make $dx$ and $dy$ $\mathbb C-$linear maps, but it does make them complex functions and so allows them to be multiplied by $i$. And indeed, now  $$dz=\operatorname{id}_{\mathbb C}=dx+idy.$$ I'm sorry about being so obtuse. I'm not sure this question has any value at all to the community, so perhaps I should remove this part? However, I still do not understand what the definition of $|dz|$ is in these terms.",,['complex-analysis']
56,What is $0^{i}$?,What is ?,0^{i},"$$\lim_{n\to 0} n^{i} = \lim_{n\to 0} e^{i\log(n)} $$ I know that $0^{0}$ is generally undefined, but can equal one in the context of the empty set mapping to itself only one time. I realize that in terms of the equation above, the limit does not exist, but can $0^{i}$ be interpreted in a way to assign it a value? For the curious, I ran in to this when trying to calculate the imaginary-derivative of $\sin(x)$.","$$\lim_{n\to 0} n^{i} = \lim_{n\to 0} e^{i\log(n)} $$ I know that $0^{0}$ is generally undefined, but can equal one in the context of the empty set mapping to itself only one time. I realize that in terms of the equation above, the limit does not exist, but can $0^{i}$ be interpreted in a way to assign it a value? For the curious, I ran in to this when trying to calculate the imaginary-derivative of $\sin(x)$.",,['complex-analysis']
57,Integrating $\int_0^\infty\frac{\log (1+z^2)}{e^z-1}dz$ using residue calculus.,Integrating  using residue calculus.,\int_0^\infty\frac{\log (1+z^2)}{e^z-1}dz,"I've been looking at how to integrate the following definite integral using the residue calculus, but can't seem to get my thoughts together. I know the $\log$ term is a multivalued function and the branch points are $i$, $-i$, and complex infinity, so I know that there must be branch cuts from $i$ and $-i$ stretching out to complex infinity (I think of this as being on a Riemann sphere with infinity at the ""furthest 'pole' on the sphere."" Because of this I know that I can't use an ordinary key-hole contour because this would intersect these branch cuts. There must be any number of contours I could attempt, so my question is are there any known useful contours for dealing with such integrals? Or maybe residue calculus is no help here? $$\int_0^\infty\frac{\log (1+z^2)}{e^z-1}dz.$$","I've been looking at how to integrate the following definite integral using the residue calculus, but can't seem to get my thoughts together. I know the $\log$ term is a multivalued function and the branch points are $i$, $-i$, and complex infinity, so I know that there must be branch cuts from $i$ and $-i$ stretching out to complex infinity (I think of this as being on a Riemann sphere with infinity at the ""furthest 'pole' on the sphere."" Because of this I know that I can't use an ordinary key-hole contour because this would intersect these branch cuts. There must be any number of contours I could attempt, so my question is are there any known useful contours for dealing with such integrals? Or maybe residue calculus is no help here? $$\int_0^\infty\frac{\log (1+z^2)}{e^z-1}dz.$$",,"['complex-analysis', 'definite-integrals', 'residue-calculus']"
58,Definite integral calculation with poles at $0$ and $\pm i\sqrt{3}$,Definite integral calculation with poles at  and,0 \pm i\sqrt{3},"$$\int_0^\infty \frac{\sin(2\pi x)}{x(x^2+3)} \, dx$$ I looked at $\frac{e^{2\pi i z}}{z^{3}+3z}$ , also calculated the residues, but they don't get me the right answer. I used that $\int_{-\infty}^\infty f(z)\,dz = 2\pi i (\sum \operatorname{Res} z_r) + \pi i \operatorname{Res}_0$ , but my answer turns out wrong when I check with wolframalpha. Residue for $0$ is $1$ , for $z=\sqrt{3}i$ it's $-\frac{e^{-2\pi}}{2}$ . . . In a worse attempt I forgot $2\pi$ and used $z$ only (i.e. $\frac{e^{iz}}{z^{3}+3z}$ ) and the result was a little closer, but missing a factor of 2 and and $i$ . Can anyone see the right way? Please do tell.","I looked at , also calculated the residues, but they don't get me the right answer. I used that , but my answer turns out wrong when I check with wolframalpha. Residue for is , for it's . . . In a worse attempt I forgot and used only (i.e. ) and the result was a little closer, but missing a factor of 2 and and . Can anyone see the right way? Please do tell.","\int_0^\infty \frac{\sin(2\pi x)}{x(x^2+3)} \, dx \frac{e^{2\pi i z}}{z^{3}+3z} \int_{-\infty}^\infty f(z)\,dz = 2\pi i (\sum \operatorname{Res} z_r) + \pi i \operatorname{Res}_0 0 1 z=\sqrt{3}i -\frac{e^{-2\pi}}{2} 2\pi z \frac{e^{iz}}{z^{3}+3z} i",['complex-analysis']
59,Geometrical Meaning of derivative of complex function,Geometrical Meaning of derivative of complex function,,"What's the geometrical meaning of $f'(z)$ in complex analysis, as we know in real analysis $f'(x)$ has meaning ie. Slope of curve or gives max/ min. But what does derivative $f'(z)$ has geometrical meaning in complex analysis","What's the geometrical meaning of $f'(z)$ in complex analysis, as we know in real analysis $f'(x)$ has meaning ie. Slope of curve or gives max/ min. But what does derivative $f'(z)$ has geometrical meaning in complex analysis",,"['complex-analysis', 'analysis']"
60,Physical interpretation of residues,Physical interpretation of residues,,"What is physical interpretation of residues of poles (of any order) of a complex function? Poles represents the points where a complex function cease to be analytic and residues are calculated to solve complex integration but I am curious about it's physical interpretation, if any?","What is physical interpretation of residues of poles (of any order) of a complex function? Poles represents the points where a complex function cease to be analytic and residues are calculated to solve complex integration but I am curious about it's physical interpretation, if any?",,['complex-analysis']
61,Why can we view $z$ and $\bar z$ as independent variables in complex analysis?,Why can we view  and  as independent variables in complex analysis?,z \bar z,"I am quite confused about how to understand $\frac{\partial f}{\partial z}f(z,\bar z).$ Do $z$ and $\bar z$ in $f(z,\bar z)$ act the same way as $x$ and $y$ in $f(x,y)$? If so, how can we prove this?","I am quite confused about how to understand $\frac{\partial f}{\partial z}f(z,\bar z).$ Do $z$ and $\bar z$ in $f(z,\bar z)$ act the same way as $x$ and $y$ in $f(x,y)$? If so, how can we prove this?",,['complex-analysis']
62,Approximating $1/z$ by polynomials,Approximating  by polynomials,1/z,"Let $C=\{\mathrm e^{\mathrm it}, 0\le t\le 3\pi/2\}$ and $f(z)=1/z$. By Runge's theorem, there is a sequence of polynomials $p_n(z)$ such that $$\lim_n p_n(z)=f(z)$$ uniformly on $C$.  Does anyone know such a sequence?","Let $C=\{\mathrm e^{\mathrm it}, 0\le t\le 3\pi/2\}$ and $f(z)=1/z$. By Runge's theorem, there is a sequence of polynomials $p_n(z)$ such that $$\lim_n p_n(z)=f(z)$$ uniformly on $C$.  Does anyone know such a sequence?",,"['complex-analysis', 'approximation']"
63,Why can't I combine complex powers,Why can't I combine complex powers,,I came across this 'paradox' - $$1=e^{2\pi i}\Rightarrow 1=(e^{2\pi i})^{2\pi i}=e^{2\pi i \cdot 2\pi i}=e^{-4\pi^2}$$ I realized the fallacy lies in the fact that in general $(x^y)^z\ne x^{yz}$. Why doesn't it work with complex numbers even though it is valid in real case? Is it related to the fact that logarithm of complex number is not unique?,I came across this 'paradox' - $$1=e^{2\pi i}\Rightarrow 1=(e^{2\pi i})^{2\pi i}=e^{2\pi i \cdot 2\pi i}=e^{-4\pi^2}$$ I realized the fallacy lies in the fact that in general $(x^y)^z\ne x^{yz}$. Why doesn't it work with complex numbers even though it is valid in real case? Is it related to the fact that logarithm of complex number is not unique?,,['complex-analysis']
64,A criterion for the existence of a holomorphic logarithm of a holomorphic function,A criterion for the existence of a holomorphic logarithm of a holomorphic function,,"Suppose $\Omega$ is a domain of the complex plane (i.e. an open and connected subset of the plane). Suppose $f$ is holomorphic on $\Omega$, and $f$ is not identically zero. Suppose $f$ has a holomorphic logarithm on $\Omega$, which means that there is a function $g$ holomorphic on $\Omega$ such that $e^g=f$. Then it is easy to show that $f$ has holomorphic $n$-th roots on $\Omega$ for each $n$, which means that for each integer $n$, there exist a function $g_n$ holomorphic on $\Omega$ such that $(g_n)^n = f$. Is the converse true? i.e. if $f$ has holomorphic $n$-th roots on $\Omega$ for all $n$, then can we find a function $g$ holomorphic on $\Omega$ such that $f=e^g$? A few remarks : One can prove that if $f$ has holomorphic $n$-th roots on $\Omega$ for all $n$, then $f$ does not vanish on $\Omega$. Therefore, we can define a holomorphic logarithm locally , but is it possible to find a global holomorphic logarithm? Furthermore, notice that $\Omega$ is not supposed simply connected, in which case the answer to my question is yes.","Suppose $\Omega$ is a domain of the complex plane (i.e. an open and connected subset of the plane). Suppose $f$ is holomorphic on $\Omega$, and $f$ is not identically zero. Suppose $f$ has a holomorphic logarithm on $\Omega$, which means that there is a function $g$ holomorphic on $\Omega$ such that $e^g=f$. Then it is easy to show that $f$ has holomorphic $n$-th roots on $\Omega$ for each $n$, which means that for each integer $n$, there exist a function $g_n$ holomorphic on $\Omega$ such that $(g_n)^n = f$. Is the converse true? i.e. if $f$ has holomorphic $n$-th roots on $\Omega$ for all $n$, then can we find a function $g$ holomorphic on $\Omega$ such that $f=e^g$? A few remarks : One can prove that if $f$ has holomorphic $n$-th roots on $\Omega$ for all $n$, then $f$ does not vanish on $\Omega$. Therefore, we can define a holomorphic logarithm locally , but is it possible to find a global holomorphic logarithm? Furthermore, notice that $\Omega$ is not supposed simply connected, in which case the answer to my question is yes.",,['complex-analysis']
65,Gamma function has no zeros,Gamma function has no zeros,,"I want to show that $\Gamma(z)$ has no zeros. My idea is to use the formula $$\Gamma(z)\Gamma(1-z)=\dfrac{\pi}{\sin(\pi z)}$$ which holds for all $z\in\mathbb{C}$. If $\Gamma(z)=0$, then the left-hand side is $0$ and the right-hand side isn't, so impossible. But I'm worried that it wouldn't work if $\Gamma(1-z)=\pm\infty$. How to resolve this case?","I want to show that $\Gamma(z)$ has no zeros. My idea is to use the formula $$\Gamma(z)\Gamma(1-z)=\dfrac{\pi}{\sin(\pi z)}$$ which holds for all $z\in\mathbb{C}$. If $\Gamma(z)=0$, then the left-hand side is $0$ and the right-hand side isn't, so impossible. But I'm worried that it wouldn't work if $\Gamma(1-z)=\pm\infty$. How to resolve this case?",,"['complex-analysis', 'special-functions', 'gamma-function']"
66,Describing Riemann Surfaces,Describing Riemann Surfaces,,"We've just learned about Riemann surfaces in my complex analysis class and to get a better understanding, I've been trying to find similar problems. I came across this: Describe the Riemann surfaces of the functions $f(z) = \sqrt{z(z^2 + 1)}$ and $g(z) = \sqrt{\frac{z^2 + 1}{z}}$. I've tried approaching it the same way that we did in class, by finding branch points and trying to determine the branch cut, but I'm having a hard time finding the branch cuts and explaining what the Riemann surface looks like.","We've just learned about Riemann surfaces in my complex analysis class and to get a better understanding, I've been trying to find similar problems. I came across this: Describe the Riemann surfaces of the functions $f(z) = \sqrt{z(z^2 + 1)}$ and $g(z) = \sqrt{\frac{z^2 + 1}{z}}$. I've tried approaching it the same way that we did in class, by finding branch points and trying to determine the branch cut, but I'm having a hard time finding the branch cuts and explaining what the Riemann surface looks like.",,"['complex-analysis', 'riemann-surfaces']"
67,Fourier series is to Fourier transform what Laurent series is to ...?,Fourier series is to Fourier transform what Laurent series is to ...?,,"Since the coefficients $$a_k = \frac1{2\pi i}\oint_C\frac{f(z)}{(z-c)^{k+1}}\,dz$$ for the Laurent series $$f(z)\Big|_{r\le|z|\le R} = \sum_{k=-\infty}^{\infty}a_k\cdot(z-c)^k $$ of a function $f\in\mathcal H(B(r,R))$ (i.e. a function that is holomorphic on the Annulus of radii $r\le R$), evaluated on a circle of radius $\rho\in[r,R]$, $$\tilde a_k := \rho^k a_k = \frac1{2\pi}\int\limits_{\phi=0}^{2\pi} f(c+\rho e^{i\phi}) e^{-ik\phi}\,d\phi \\\Rightarrow f(c+\rho e^{i\phi}) = \sum_{k=-\infty}^\infty \tilde a_k\,e^{ik\phi},$$ are (up to the factor $\rho^k$) equivalent to the Fourier coefficients of a function $$\tilde f(x):=f\Big(c+\rho e^{i\tfrac x{2\pi\rho}}\Big),\quad x\in[-\pi\rho,+\pi\rho],$$ I was wondering if there is a meaningful limit $\rho\to\infty$ for the Laurent series. For the Fourier series, the limit becomes the Fourier transform with continous ""coefficients"", $\tilde a_k\to \tilde a(k)\equiv \mathcal F\{\tilde f(x)\}(k)$, such that $$\tilde f(x) = \int_{-\infty}^\infty \tilde a(k) e^{ikx}\, dk, \\\tilde a(k) = \frac1{2\pi}\int_{-\infty}^\infty \tilde f(x) e^{-ikx}\, dx.$$ So in a similar way, this limit would turn the Laurent series into a ""Laurent transform"", $$f(z)\Big|_{r\le|z|} = \int_{-\infty}^\infty a(k)\cdot (z-c)^k, \\ a(k) = \lim_{R\to\infty} \frac1{2\pi}\oint\limits_{|z|=R}\frac{f(z)}{(z-c)^{k+1}}\,dz$$ (up to some factors that I probably forgot, and assuming that $f$ remains holomorphic for $R\to\infty$), describing $f(z)$ by its values at $\mathbb C$-infinity. So, does this make sense, has it been investigated upon before and/or any applications?","Since the coefficients $$a_k = \frac1{2\pi i}\oint_C\frac{f(z)}{(z-c)^{k+1}}\,dz$$ for the Laurent series $$f(z)\Big|_{r\le|z|\le R} = \sum_{k=-\infty}^{\infty}a_k\cdot(z-c)^k $$ of a function $f\in\mathcal H(B(r,R))$ (i.e. a function that is holomorphic on the Annulus of radii $r\le R$), evaluated on a circle of radius $\rho\in[r,R]$, $$\tilde a_k := \rho^k a_k = \frac1{2\pi}\int\limits_{\phi=0}^{2\pi} f(c+\rho e^{i\phi}) e^{-ik\phi}\,d\phi \\\Rightarrow f(c+\rho e^{i\phi}) = \sum_{k=-\infty}^\infty \tilde a_k\,e^{ik\phi},$$ are (up to the factor $\rho^k$) equivalent to the Fourier coefficients of a function $$\tilde f(x):=f\Big(c+\rho e^{i\tfrac x{2\pi\rho}}\Big),\quad x\in[-\pi\rho,+\pi\rho],$$ I was wondering if there is a meaningful limit $\rho\to\infty$ for the Laurent series. For the Fourier series, the limit becomes the Fourier transform with continous ""coefficients"", $\tilde a_k\to \tilde a(k)\equiv \mathcal F\{\tilde f(x)\}(k)$, such that $$\tilde f(x) = \int_{-\infty}^\infty \tilde a(k) e^{ikx}\, dk, \\\tilde a(k) = \frac1{2\pi}\int_{-\infty}^\infty \tilde f(x) e^{-ikx}\, dx.$$ So in a similar way, this limit would turn the Laurent series into a ""Laurent transform"", $$f(z)\Big|_{r\le|z|} = \int_{-\infty}^\infty a(k)\cdot (z-c)^k, \\ a(k) = \lim_{R\to\infty} \frac1{2\pi}\oint\limits_{|z|=R}\frac{f(z)}{(z-c)^{k+1}}\,dz$$ (up to some factors that I probably forgot, and assuming that $f$ remains holomorphic for $R\to\infty$), describing $f(z)$ by its values at $\mathbb C$-infinity. So, does this make sense, has it been investigated upon before and/or any applications?",,"['complex-analysis', 'fourier-analysis', 'integral-transforms', 'laurent-series']"
68,$|f(f(z))-z^2|$ must be large somewhere in the disc $\mathbb{D}$?,must be large somewhere in the disc ?,|f(f(z))-z^2| \mathbb{D},"I wish to prove the statement shown in the following block. I thought this may have appeared in the Math Stack before; sorry if I failed to find it (see ""Research"" below). The proposition seems to call for a proof via H.A. Schwarz Lemma, but I am interested in any proof of it. Let $\sup$ always mean the supremum in the unit open disc $\mathbb{D}$ . Let $f$ map $\mathbb{D}$ into $\mathbb{D}$ analytically, and fix the origin. Prove that $$\sup \left|f(f(z))-z^2\right| \,\,\geq\,\,\frac{1}{4}.$$ My Attempt. If it happens that $f$ is a rotation, $z \mapsto {e^{i\psi}}z,$ then the result follows from the choice $z=1/2 \in $ the disc: $$\left|f(f(\frac{1}{2}))-(\frac{1}{2})^2\right| \,\,\geq\,\,\left|f(f(\frac{1}{2}))\right|-\left|\frac{1}{4}\right|\,\,=\,\,\left|f(\frac{1}{2})\right|-\frac{1}{4}\,\,=\,\,\left|\frac{1}{2}\right|-\frac{1}{4}\,\,=\,\,\frac{1}{4},$$ since rotation means $|z|\,=\,|f(z)|$ for any $z$ in $\mathbb{D}$ . Therefore we assume $f$ is not a rotation. By Schwarz Lemma, we know $f'(0) \in \mathbb{D}$ , and we know $|f(z)|<|z|<1$ throughout the disc. ( Starting here I pursue an idea; I am not sure if it is helpful... ) Define the function $$\phi(z)\,\,=\,\,\frac{f(f(z))-z^2}{2},$$ and note that it also satisfies the hypotheses of the Schwarz Lemma. It is easy to check that $\phi$ is not a rotation when $f$ is not a rotation. So now our goal is to show $$\sup |\phi(z)| \,\,\geq\,\,\frac{1}{8}.$$ Remarks. That's what I have done. The derivative of $\phi$ is $\frac{1}{2}(f'(f(z))f'(z)-2z)$ , and using this we can know that $|\phi'(0)|<\frac{1}{2}.$ Of course we know $|\phi(z)|<|z|<1$ throughout the disc. Another idea is to pass to series expansions of $f$ and $\phi$ . Research. Approach Zero search results. Schwarz Lemma search results: https://math.stackexchange.com/search?page=11&tab=Relevance&q=schwarz%20lemma","I wish to prove the statement shown in the following block. I thought this may have appeared in the Math Stack before; sorry if I failed to find it (see ""Research"" below). The proposition seems to call for a proof via H.A. Schwarz Lemma, but I am interested in any proof of it. Let always mean the supremum in the unit open disc . Let map into analytically, and fix the origin. Prove that My Attempt. If it happens that is a rotation, then the result follows from the choice the disc: since rotation means for any in . Therefore we assume is not a rotation. By Schwarz Lemma, we know , and we know throughout the disc. ( Starting here I pursue an idea; I am not sure if it is helpful... ) Define the function and note that it also satisfies the hypotheses of the Schwarz Lemma. It is easy to check that is not a rotation when is not a rotation. So now our goal is to show Remarks. That's what I have done. The derivative of is , and using this we can know that Of course we know throughout the disc. Another idea is to pass to series expansions of and . Research. Approach Zero search results. Schwarz Lemma search results: https://math.stackexchange.com/search?page=11&tab=Relevance&q=schwarz%20lemma","\sup \mathbb{D} f \mathbb{D} \mathbb{D} \sup \left|f(f(z))-z^2\right| \,\,\geq\,\,\frac{1}{4}. f z \mapsto {e^{i\psi}}z, z=1/2 \in  \left|f(f(\frac{1}{2}))-(\frac{1}{2})^2\right| \,\,\geq\,\,\left|f(f(\frac{1}{2}))\right|-\left|\frac{1}{4}\right|\,\,=\,\,\left|f(\frac{1}{2})\right|-\frac{1}{4}\,\,=\,\,\left|\frac{1}{2}\right|-\frac{1}{4}\,\,=\,\,\frac{1}{4}, |z|\,=\,|f(z)| z \mathbb{D} f f'(0) \in \mathbb{D} |f(z)|<|z|<1 \phi(z)\,\,=\,\,\frac{f(f(z))-z^2}{2}, \phi f \sup |\phi(z)| \,\,\geq\,\,\frac{1}{8}. \phi \frac{1}{2}(f'(f(z))f'(z)-2z) |\phi'(0)|<\frac{1}{2}. |\phi(z)|<|z|<1 f \phi","['complex-analysis', 'inequality']"
69,What is the Riemann surface of $y=\sqrt{z+z^2+z^4+\cdots +z^{2^n}+\cdots}$?,What is the Riemann surface of ?,y=\sqrt{z+z^2+z^4+\cdots +z^{2^n}+\cdots},"The following appears as the second-to-last problem of Stewart's Complex Analysis : Describe the Riemann surface of the function $y=\sqrt{z+z^2+z^4+\cdots +z^{2^n}+\cdots}$. This problem intimidated me when I first saw it as an undergrad, as the series under the root isn't the expansion of any function I know (then or now). That renders the Riemann surface more subtle than usual, since any info about poles and zeroes must be found from this series alone. For instance, it has a zero at $z=0$ and therefore a branch point. But there should be other zeroes — indeed, infinitely many zeroes — since the number associated with any truncation of the series grows as $2^N$!. This suggests that the Riemann surface of this function must be exotic. I'll give a sketch of an argument below, taking the pedestrian approach of hunting poles and zeroes of the function. However, I'd be glad to see an elegant and advanced perspective of the problem, particularly if it uses some ideas which I wouldn't have encountered as an undergraduate. The more food for thought, the better! EDIT: As noted by O.L.'s answer below, the series above is the canonical example of a lacunary function with the unit circle as natural boundary. What that leaves open is the nature of the resulting Riemann surface, which would seem to hinge upon the quantity of the function's zeros within the unit circle. David Speyer has given an argument in his answer that this number is at least 10. Is there a more definitive result on the zeroes of this lacuuary function?","The following appears as the second-to-last problem of Stewart's Complex Analysis : Describe the Riemann surface of the function $y=\sqrt{z+z^2+z^4+\cdots +z^{2^n}+\cdots}$. This problem intimidated me when I first saw it as an undergrad, as the series under the root isn't the expansion of any function I know (then or now). That renders the Riemann surface more subtle than usual, since any info about poles and zeroes must be found from this series alone. For instance, it has a zero at $z=0$ and therefore a branch point. But there should be other zeroes — indeed, infinitely many zeroes — since the number associated with any truncation of the series grows as $2^N$!. This suggests that the Riemann surface of this function must be exotic. I'll give a sketch of an argument below, taking the pedestrian approach of hunting poles and zeroes of the function. However, I'd be glad to see an elegant and advanced perspective of the problem, particularly if it uses some ideas which I wouldn't have encountered as an undergraduate. The more food for thought, the better! EDIT: As noted by O.L.'s answer below, the series above is the canonical example of a lacunary function with the unit circle as natural boundary. What that leaves open is the nature of the resulting Riemann surface, which would seem to hinge upon the quantity of the function's zeros within the unit circle. David Speyer has given an argument in his answer that this number is at least 10. Is there a more definitive result on the zeroes of this lacuuary function?",,"['complex-analysis', 'power-series', 'riemann-surfaces', 'analyticity']"
70,"$f$ entire, $f$ satisfies $|f(x+iy)|\leq\frac{1}{|y|}$ for all $x,y\in\mathbb{R}$. Prove that $f\equiv 0$. [duplicate]","entire,  satisfies  for all . Prove that . [duplicate]","f f |f(x+iy)|\leq\frac{1}{|y|} x,y\in\mathbb{R} f\equiv 0","This question already has answers here : Application of Liouville's Theorem (4 answers) Closed 8 years ago . Let $f$ be an entire function. Suppose that $f$ satisfies   $$ |f(x+iy)|\leq\frac{1}{|y|}. $$   for all $x,y\in\mathbb{R}$. Prove that $f$ is identically zero. I'm having some trouble with this, but I'm probably just overthinking it. The first instinct as usual is to try and make something happen with Liouville's theorem (and then since $f$ tends to zero on the imaginary axis we can conclude that $f\equiv 0$), but since $f$ is potentially unbounded on the real axis it's inapplicable. I'm pretty sure it's still impossible for $|f|\to\infty$ as $x\to\pm\infty$ while the function is bounded everywhere else, as it seems there would be some topological obstruction, but I could be wrong. I've also thought of getting an estimate on the $|f'|$ as in the proof of Liouville's theorem, but it fails for the same reasons. Any hints or tips are greatly appreciated. Thank you","This question already has answers here : Application of Liouville's Theorem (4 answers) Closed 8 years ago . Let $f$ be an entire function. Suppose that $f$ satisfies   $$ |f(x+iy)|\leq\frac{1}{|y|}. $$   for all $x,y\in\mathbb{R}$. Prove that $f$ is identically zero. I'm having some trouble with this, but I'm probably just overthinking it. The first instinct as usual is to try and make something happen with Liouville's theorem (and then since $f$ tends to zero on the imaginary axis we can conclude that $f\equiv 0$), but since $f$ is potentially unbounded on the real axis it's inapplicable. I'm pretty sure it's still impossible for $|f|\to\infty$ as $x\to\pm\infty$ while the function is bounded everywhere else, as it seems there would be some topological obstruction, but I could be wrong. I've also thought of getting an estimate on the $|f'|$ as in the proof of Liouville's theorem, but it fails for the same reasons. Any hints or tips are greatly appreciated. Thank you",,['complex-analysis']
71,Show that if $|f(z)| \leq M |z|^n$ then $f$ is a polynomial max degree n,Show that if  then  is a polynomial max degree n,|f(z)| \leq M |z|^n f,"I can't prove this statement, can anybody show me how to prove it? $$f:\mathbb{C}\rightarrow \mathbb{C} \in \mathcal{O}(\mathbb{C}), \exists n\in \mathbb{N}, R >0 , M>0 : |f(z)| \le M|z|^{n} \ \ \forall |z|>R \Rightarrow \deg(f)\le n $$ To show is that if there exists such an $M$, that then $f$ is a polynomial of max degree $n$. I started like this: $$f(z) = \sum_{n=0}^{\infty} a_n (z-z_0)^n$$ So if I put this into the inequality: $$|f(z)| = \left| \sum_{n=0}^{\infty} a_n (z-z_0)^n \right| \le M |z|^n .$$","I can't prove this statement, can anybody show me how to prove it? $$f:\mathbb{C}\rightarrow \mathbb{C} \in \mathcal{O}(\mathbb{C}), \exists n\in \mathbb{N}, R >0 , M>0 : |f(z)| \le M|z|^{n} \ \ \forall |z|>R \Rightarrow \deg(f)\le n $$ To show is that if there exists such an $M$, that then $f$ is a polynomial of max degree $n$. I started like this: $$f(z) = \sum_{n=0}^{\infty} a_n (z-z_0)^n$$ So if I put this into the inequality: $$|f(z)| = \left| \sum_{n=0}^{\infty} a_n (z-z_0)^n \right| \le M |z|^n .$$",,['complex-analysis']
72,Singularities at infinity,Singularities at infinity,,"I'm a little confused on the concept of singularities at infinity. For example, take the function $f(z) = 1/z$. This has a removable singularity at infinity, since $f(1/z) = z$ is analytic at zero. However, the residue of $f(z)$ at infinity is by definition $$\text{Res } 1/z^2 f(z) = -1.$$ which is nonzero. This seems bizarre; for removable singularities in the regular complex plane have residue equal to zero. If we intuitively think of the definition of a residue to mean the coefficient of $z^{-1}$, then it also appears that we have a 'pole' of order at least one at infinity, despite the fact that it vanishes at infinity. Again, this is in contrast to points in the plane where functions always blow up at poles. I think this shows that Cauchy's integral theorem fails at infinity, so a function analytic in a neighborhood of infinity does not necessarily have a zero contour integral over a curve enclosing infinity. What other pathologies do singularities at infinity exhibit? Can anybody clarify just what is going on here and what to watch out for? Or provide a good reference?","I'm a little confused on the concept of singularities at infinity. For example, take the function $f(z) = 1/z$. This has a removable singularity at infinity, since $f(1/z) = z$ is analytic at zero. However, the residue of $f(z)$ at infinity is by definition $$\text{Res } 1/z^2 f(z) = -1.$$ which is nonzero. This seems bizarre; for removable singularities in the regular complex plane have residue equal to zero. If we intuitively think of the definition of a residue to mean the coefficient of $z^{-1}$, then it also appears that we have a 'pole' of order at least one at infinity, despite the fact that it vanishes at infinity. Again, this is in contrast to points in the plane where functions always blow up at poles. I think this shows that Cauchy's integral theorem fails at infinity, so a function analytic in a neighborhood of infinity does not necessarily have a zero contour integral over a curve enclosing infinity. What other pathologies do singularities at infinity exhibit? Can anybody clarify just what is going on here and what to watch out for? Or provide a good reference?",,['complex-analysis']
73,Riemann zeta function and the volume of the unit $n$-ball,Riemann zeta function and the volume of the unit -ball,n,"The volume of a unit $n$-dimensional ball (in Euclidean space) is $$V_n = \frac{\pi^{n/2}}{\frac{n}{2}\Gamma(\frac{n}{2})}$$ The completed Riemann zeta function, or Riemann xi function, is $$\xi(s) = (s-1) \frac{\frac{s}{2}\Gamma(\frac{s}{2})}{\pi^{s/2}} \zeta(s)$$ Save for the $(s-1)$, the extra factor is exactly the inverse of $V_s$. Is there any explanation for this, or is it just a funny coincidence?","The volume of a unit $n$-dimensional ball (in Euclidean space) is $$V_n = \frac{\pi^{n/2}}{\frac{n}{2}\Gamma(\frac{n}{2})}$$ The completed Riemann zeta function, or Riemann xi function, is $$\xi(s) = (s-1) \frac{\frac{s}{2}\Gamma(\frac{s}{2})}{\pi^{s/2}} \zeta(s)$$ Save for the $(s-1)$, the extra factor is exactly the inverse of $V_s$. Is there any explanation for this, or is it just a funny coincidence?",,"['complex-analysis', 'geometry', 'analytic-number-theory', 'riemann-zeta', 'spheres']"
74,Sources on Several Complex Variables,Sources on Several Complex Variables,,"I have searched the past entries about sources on SCV but couldn't find about this topic. If I am not careful enough, sorry for this! We are using Hörmander's book which is really hard to follow. What do you suggest? Besides textbooks, it's welcome if you suggest manuscripts, lecture notes etc. Thank you!","I have searched the past entries about sources on SCV but couldn't find about this topic. If I am not careful enough, sorry for this! We are using Hörmander's book which is really hard to follow. What do you suggest? Besides textbooks, it's welcome if you suggest manuscripts, lecture notes etc. Thank you!",,"['reference-request', 'complex-analysis', 'several-complex-variables']"
75,Is there a classification of isolated essential singularities?,Is there a classification of isolated essential singularities?,,"In the thread Why do we categorize all other (iso.) singularities as ""essential""? , here is one of the questions that was asked: Do we not care about essential singularities to classify them further? The accepted answer for this question addresses this as follows: So all essential singularities have some things in common, but on the other hand this should not lead us to believe that they are all the same. What they have in common is complicated behaviour, but they can be complicated in very different ways! Indeed, different transcendental entire functions (those that have an essential singularity at infinity; i.e. are not polynomials) can vary very much with respect to their behavior near infinity. Just for example, for some such functions, such as $z\mapsto e^z$, there exist curves tending to infinity on which the function is bounded, while for others this is not the case. I see that essential singularities can be very different, but perhaps there can still be a classification. Here is how I would state my question: If we say that functions having an isolated essential singularity at $0$ are equivalent when they differ by multiplication by a non-vanishing holomorphic function, is there a nice or fundamental set of representatives of this equivalence relation? More formally: Let $\mathcal{S}$ (for ""singularity"") be the set of germs of functions at $0$ that are holomorphic except for an isolated essential singularity at $0$; in other words, let $\mathcal{S}$ be the quotient of    $$\left\{(f,U)\,\middle\vert\, \begin{array}{c} U\subseteq\mathbb{C}\text{ is a neighborhood of }0,\\ f\colon U\to \widehat{\mathbb{C}}\text{ is holomorphic on }U\setminus\{0\},\\ f\text{ has an essential singularity at }0\end{array}\right\}$$   by the usual equivalence relation for germs. Let $\mathcal{I}$ (for ""invertible"") be the set of germs of holomorphic functions at $0$ that are non-vanishing in a neighborhood of $0$; thus, for any $[f]\in \mathcal{I}$, we also have $[1/f]\in\mathcal{I}$. Multiplication of germs at a point is well-defined, so we have a map $\mathcal{I}\times\mathcal{S}\to\mathcal{S}$; it is in fact a group action. Is there a nice or fundamental set of representatives for the orbits of this action? Very naively, I might guess that a set of representatives is given by $e^{1/f}$ for holomorphic functions $f$ vanishing at $0$, together with any function obtainable from these by repeated composition with $e^z$ (thus, for example, $e^{e^{1/z}}$), but I don't know how I would go about determining whether this is correct. P.S. I'm pretty weak with complex analysis; explanations assuming little background knowledge would be very welcome.","In the thread Why do we categorize all other (iso.) singularities as ""essential""? , here is one of the questions that was asked: Do we not care about essential singularities to classify them further? The accepted answer for this question addresses this as follows: So all essential singularities have some things in common, but on the other hand this should not lead us to believe that they are all the same. What they have in common is complicated behaviour, but they can be complicated in very different ways! Indeed, different transcendental entire functions (those that have an essential singularity at infinity; i.e. are not polynomials) can vary very much with respect to their behavior near infinity. Just for example, for some such functions, such as $z\mapsto e^z$, there exist curves tending to infinity on which the function is bounded, while for others this is not the case. I see that essential singularities can be very different, but perhaps there can still be a classification. Here is how I would state my question: If we say that functions having an isolated essential singularity at $0$ are equivalent when they differ by multiplication by a non-vanishing holomorphic function, is there a nice or fundamental set of representatives of this equivalence relation? More formally: Let $\mathcal{S}$ (for ""singularity"") be the set of germs of functions at $0$ that are holomorphic except for an isolated essential singularity at $0$; in other words, let $\mathcal{S}$ be the quotient of    $$\left\{(f,U)\,\middle\vert\, \begin{array}{c} U\subseteq\mathbb{C}\text{ is a neighborhood of }0,\\ f\colon U\to \widehat{\mathbb{C}}\text{ is holomorphic on }U\setminus\{0\},\\ f\text{ has an essential singularity at }0\end{array}\right\}$$   by the usual equivalence relation for germs. Let $\mathcal{I}$ (for ""invertible"") be the set of germs of holomorphic functions at $0$ that are non-vanishing in a neighborhood of $0$; thus, for any $[f]\in \mathcal{I}$, we also have $[1/f]\in\mathcal{I}$. Multiplication of germs at a point is well-defined, so we have a map $\mathcal{I}\times\mathcal{S}\to\mathcal{S}$; it is in fact a group action. Is there a nice or fundamental set of representatives for the orbits of this action? Very naively, I might guess that a set of representatives is given by $e^{1/f}$ for holomorphic functions $f$ vanishing at $0$, together with any function obtainable from these by repeated composition with $e^z$ (thus, for example, $e^{e^{1/z}}$), but I don't know how I would go about determining whether this is correct. P.S. I'm pretty weak with complex analysis; explanations assuming little background knowledge would be very welcome.",,"['complex-analysis', 'singularity-theory']"
76,Calculating a harmonic conjugate,Calculating a harmonic conjugate,,"Is the following reasoning correct? Determine a harmonic conjugate to the function \begin{equation} f(x,y)=2y^{3}-6x^{2}y+4x^{2}-7xy-4y^{2}+3x+4y-4 \end{equation} We first of all check if $f(x,y)$ is indeed a harmonic function. This amounts to show $f(x,y)$ satisfy the two-dimensional Laplace equation \begin{equation} \frac{\partial^{2 }f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}}=0 \tag{1} \end{equation} We have $\frac{\partial^{2}f}{\partial x^{2}}=8-12y$ and $\frac{\partial^{2} f}{\partial y^{2}}=12y-8$ . Thus, (1) is fulfilled, and so $f(x,y)$ is harmonic. Next, we seek to determine a harmonic conjugate to the given function. Let $u(x,y)=2y^{3}-6x^{2}y+4x^{2}-7xy-4y^{2}+3x+4y-4$ . \begin{equation*} u_{x}=v_{y} \iff -12xy+8x-7y+3=v_{y} \end{equation*} Integrate with respect to $y$ \begin{equation} v=-6xy^{2}+8xy-\frac{7}{2}y^{2}+3y+h(x) \tag{2} \end{equation} where $h(x)$ is a function of $x$ alone. To determine this, we use the second Cauchy-Riemann equation $v_{x}=-u_{y}$ \begin{align*} -u_{y}=v_{x} &\iff 6x^{2}+7x-6y^{2}+8y-4=h'(x)-6y^{2}+8y \\ &\iff h'(x)=6x^{2}+7x-4 \end{align*} Integrating with respect to $x$ we have \begin{equation} h(x)=2x^{3}+\frac{7}{2}x^{2}-4x+C \end{equation} where $C$ is an arbitrary constant. Therefore, if we let $C=0$ , then one harmonic conjugate of $u$ is given as: \begin{equation} v=2x^{3}+\frac{7}{2}x^{2}-6xy^{2}+8xy-4x-\frac{7}{2}y^{2}+3y \end{equation}","Is the following reasoning correct? Determine a harmonic conjugate to the function We first of all check if is indeed a harmonic function. This amounts to show satisfy the two-dimensional Laplace equation We have and . Thus, (1) is fulfilled, and so is harmonic. Next, we seek to determine a harmonic conjugate to the given function. Let . Integrate with respect to where is a function of alone. To determine this, we use the second Cauchy-Riemann equation Integrating with respect to we have where is an arbitrary constant. Therefore, if we let , then one harmonic conjugate of is given as:","\begin{equation} f(x,y)=2y^{3}-6x^{2}y+4x^{2}-7xy-4y^{2}+3x+4y-4 \end{equation} f(x,y) f(x,y) \begin{equation}
\frac{\partial^{2 }f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}}=0 \tag{1}
\end{equation} \frac{\partial^{2}f}{\partial x^{2}}=8-12y \frac{\partial^{2} f}{\partial y^{2}}=12y-8 f(x,y) u(x,y)=2y^{3}-6x^{2}y+4x^{2}-7xy-4y^{2}+3x+4y-4 \begin{equation*}
u_{x}=v_{y} \iff -12xy+8x-7y+3=v_{y}
\end{equation*} y \begin{equation}
v=-6xy^{2}+8xy-\frac{7}{2}y^{2}+3y+h(x) \tag{2}
\end{equation} h(x) x v_{x}=-u_{y} \begin{align*}
-u_{y}=v_{x} &\iff 6x^{2}+7x-6y^{2}+8y-4=h'(x)-6y^{2}+8y \\
&\iff h'(x)=6x^{2}+7x-4
\end{align*} x \begin{equation}
h(x)=2x^{3}+\frac{7}{2}x^{2}-4x+C
\end{equation} C C=0 u \begin{equation}
v=2x^{3}+\frac{7}{2}x^{2}-6xy^{2}+8xy-4x-\frac{7}{2}y^{2}+3y
\end{equation}","['complex-analysis', 'self-learning', 'harmonic-functions']"
77,Techniques to compute complex integrals over infinite contours,Techniques to compute complex integrals over infinite contours,,"In asymptotic analysis and analytic number theory, one often has to deal with complex integrals over infinite contours in the complex plane, and the required techniques to do so often go beyond the standard courses of complex analysis in one variable. In particular, I am interested in the following type of argument which I don´t fully understand and which I will try to illustrate by an example (taken from Paris and Kaminski, ""Asymptotics and Mellin-Barnes Integrals""): Consider the so called Cahen-Mellin integral $$e^{-z}=\frac{1}{2\pi i}\int_{(c)}\Gamma(s)z^{-s}ds,\ \arg(z)<\frac{\pi}{2}, z\neq 0,$$ which is an integral representation of the exponential function by taking the inverse Mellin transform of the Gamma function, where the integration contour $(c)$ stands for the vertical line $\{\Re(s)=c\}$ with some $c>0$. It can be shown that the integrand has ""the controlling behavior"" $|z|^{-\sigma}O(|t|^{\sigma-{1\over 2}}e^{t\arg(z)-{1\over 2}\pi|t|})$ as $|t|\to\infty$, where $s=\sigma + it$. Now, aside from the obvious way to show the validity of the above integral representation, Paris and Kaminski argue that because of the aforementioned exponential decay of the integrand we are allowed to move the contour of integration over the poles of the Gamma function and use the residues of the latter to obtain the exponential series. This is precisely the argument I want to understand, so I will try to break it down into few smaller questions: (Q1) How does the exponential decay of the integrand allow us to displace the contour of intagration over singularities? Is there a more general setup where the asymptotic behavior of the integrand allows for moving the contour of integration through and over singularities? (Q2) After the displacement of the integration contour (still a vertical line), what kind of a residue theorem allows for considering all of the infinitely many singularities of the gamma function? The version of the residue theorem I know uses bounded interior of a (simply) closed contour in the complex plane and only finitely many residues contained in there. Remark 1: In order to compute the above integral in a classical way, I would take a finite line segments of the vertical line, symmetric with respect to the positive real axis, i.e. $\{Re(s)=c, -r_n\leq\Im(s)\leq r_n\}$, construct circle segments with radii $r_n$ encompassing each of the poles, with $r_n\uparrow \infty$ suitably chosen such that no poles lie on the segment contours, and then show that the integrals over the half-circles tend to zero as $n\to\infty$, thus obtaining on the one hand the integral over the infinite vertical line and on the other hand the infinite sum of the residues. However, I haven´t really checked whether the exponential decay of the integrand would suffice for the half-circle integrals to vanish in the limit. Remark 2: I could imagine that there might be a version of the residue theorem suitably formulated for the Riemann sphere resp. $\bar{\mathbb{C}}:=\mathbb{C}\cup\{\infty\}$, where basically infinite contours from the complex plane correspond to closed ones on the sphere. (Q3) Where could one find a more systematic treatment of integrals over infinite contours in the complex plane, including contour shifting over poles, other types of contour modifications, usage of infinitely many residues, as well as other techniques for the exact computation of such integrals? I understand that such techniques are often to be applied ""individually"", thus such literature would ideally contain a few good examples. Thanks in advance for your attention and sorry if I appear to sound too confused :-), I am only trying to fill in certain ""gaps"" in my knowledge of complex analysis. PS: I am not interested in numerical computations or general asymptotic expansions for contour integrals (even though in the above example the residues ""expansion"" appears as a special case thereof).","In asymptotic analysis and analytic number theory, one often has to deal with complex integrals over infinite contours in the complex plane, and the required techniques to do so often go beyond the standard courses of complex analysis in one variable. In particular, I am interested in the following type of argument which I don´t fully understand and which I will try to illustrate by an example (taken from Paris and Kaminski, ""Asymptotics and Mellin-Barnes Integrals""): Consider the so called Cahen-Mellin integral $$e^{-z}=\frac{1}{2\pi i}\int_{(c)}\Gamma(s)z^{-s}ds,\ \arg(z)<\frac{\pi}{2}, z\neq 0,$$ which is an integral representation of the exponential function by taking the inverse Mellin transform of the Gamma function, where the integration contour $(c)$ stands for the vertical line $\{\Re(s)=c\}$ with some $c>0$. It can be shown that the integrand has ""the controlling behavior"" $|z|^{-\sigma}O(|t|^{\sigma-{1\over 2}}e^{t\arg(z)-{1\over 2}\pi|t|})$ as $|t|\to\infty$, where $s=\sigma + it$. Now, aside from the obvious way to show the validity of the above integral representation, Paris and Kaminski argue that because of the aforementioned exponential decay of the integrand we are allowed to move the contour of integration over the poles of the Gamma function and use the residues of the latter to obtain the exponential series. This is precisely the argument I want to understand, so I will try to break it down into few smaller questions: (Q1) How does the exponential decay of the integrand allow us to displace the contour of intagration over singularities? Is there a more general setup where the asymptotic behavior of the integrand allows for moving the contour of integration through and over singularities? (Q2) After the displacement of the integration contour (still a vertical line), what kind of a residue theorem allows for considering all of the infinitely many singularities of the gamma function? The version of the residue theorem I know uses bounded interior of a (simply) closed contour in the complex plane and only finitely many residues contained in there. Remark 1: In order to compute the above integral in a classical way, I would take a finite line segments of the vertical line, symmetric with respect to the positive real axis, i.e. $\{Re(s)=c, -r_n\leq\Im(s)\leq r_n\}$, construct circle segments with radii $r_n$ encompassing each of the poles, with $r_n\uparrow \infty$ suitably chosen such that no poles lie on the segment contours, and then show that the integrals over the half-circles tend to zero as $n\to\infty$, thus obtaining on the one hand the integral over the infinite vertical line and on the other hand the infinite sum of the residues. However, I haven´t really checked whether the exponential decay of the integrand would suffice for the half-circle integrals to vanish in the limit. Remark 2: I could imagine that there might be a version of the residue theorem suitably formulated for the Riemann sphere resp. $\bar{\mathbb{C}}:=\mathbb{C}\cup\{\infty\}$, where basically infinite contours from the complex plane correspond to closed ones on the sphere. (Q3) Where could one find a more systematic treatment of integrals over infinite contours in the complex plane, including contour shifting over poles, other types of contour modifications, usage of infinitely many residues, as well as other techniques for the exact computation of such integrals? I understand that such techniques are often to be applied ""individually"", thus such literature would ideally contain a few good examples. Thanks in advance for your attention and sorry if I appear to sound too confused :-), I am only trying to fill in certain ""gaps"" in my knowledge of complex analysis. PS: I am not interested in numerical computations or general asymptotic expansions for contour integrals (even though in the above example the residues ""expansion"" appears as a special case thereof).",,"['complex-analysis', 'reference-request']"
78,zeros of exponential polynomials,zeros of exponential polynomials,,Let $\exp[n;z]$ denote the $n$th Taylor polynomial for the exponential function. In the 1920's Szegő initiated the study of the asymptotic properties of the zeros (rescaled by dividing by $n$) of this family of polynomials and one consequence of his results is that they can approach arbitrarily closely to the imaginary axis.  This prompts the following question: Is it possible for $\exp[n;z]$ to have a root which lies precisely on the imaginary axis?,Let $\exp[n;z]$ denote the $n$th Taylor polynomial for the exponential function. In the 1920's Szegő initiated the study of the asymptotic properties of the zeros (rescaled by dividing by $n$) of this family of polynomials and one consequence of his results is that they can approach arbitrarily closely to the imaginary axis.  This prompts the following question: Is it possible for $\exp[n;z]$ to have a root which lies precisely on the imaginary axis?,,"['complex-analysis', 'polynomials']"
79,"Jordan Curve Theorem, Professor Tao's proof","Jordan Curve Theorem, Professor Tao's proof",,"Here is Professor Terry Tao's proof of the Jordan curve theorem using complex analysis, I more or less followed the proof until the following paragraph (see section 4). (Actually there is no need to read everything before the following paragraph to answer the question.) https://terrytao.wordpress.com/2016/10/02/math-246a-notes-3-cauchys-theorem-and-its-consequences/ There are two things which I do not understand: firstly, how do we know the boundary of $\Omega_\delta$ consists of one or more simple closed curves? How to rigorously demonstrate that its boundary cannot be some random collection of segments? Also, why does the fact that the union of squares is connected implies the boundary of $\Omega_\delta$ consists of exactly one simple closed curve. Also the sentence on why the boundary of $\Omega_\delta$ is simple got me confused.(last sentence of the paragraph above the picture) secondly, what's the point of covering $N_{\epsilon/10}(\gamma([a,b]))$ ? Why not just cover $\gamma([a,b])$ ? Any explanation would be immensely appreciated!! Here $W$ denotes the winding number. $N_{\epsilon/10}(\gamma([a,b]))$ is the set of points $z\in\mathbb{C}$ such that $\text{distance}(z,\gamma)<\epsilon/10.$","Here is Professor Terry Tao's proof of the Jordan curve theorem using complex analysis, I more or less followed the proof until the following paragraph (see section 4). (Actually there is no need to read everything before the following paragraph to answer the question.) https://terrytao.wordpress.com/2016/10/02/math-246a-notes-3-cauchys-theorem-and-its-consequences/ There are two things which I do not understand: firstly, how do we know the boundary of consists of one or more simple closed curves? How to rigorously demonstrate that its boundary cannot be some random collection of segments? Also, why does the fact that the union of squares is connected implies the boundary of consists of exactly one simple closed curve. Also the sentence on why the boundary of is simple got me confused.(last sentence of the paragraph above the picture) secondly, what's the point of covering ? Why not just cover ? Any explanation would be immensely appreciated!! Here denotes the winding number. is the set of points such that","\Omega_\delta \Omega_\delta \Omega_\delta N_{\epsilon/10}(\gamma([a,b])) \gamma([a,b]) W N_{\epsilon/10}(\gamma([a,b])) z\in\mathbb{C} \text{distance}(z,\gamma)<\epsilon/10.","['complex-analysis', 'algebraic-topology']"
80,Complex Analysis with differential forms,Complex Analysis with differential forms,,I'm studying a little of Complex Analysis and I have seen that I can use the integrals of complex functions as integrals of differential forms in $\mathbb{R}^n.$ For example: Cauchy Theorem for complex analytic functions is a consequence of the fact that a closed differential form on a simply connected set is exact. My question is : Are there other famous theorem of Complex Analysis that I can prove with the theory of differential forms in Euclidean space?,I'm studying a little of Complex Analysis and I have seen that I can use the integrals of complex functions as integrals of differential forms in For example: Cauchy Theorem for complex analytic functions is a consequence of the fact that a closed differential form on a simply connected set is exact. My question is : Are there other famous theorem of Complex Analysis that I can prove with the theory of differential forms in Euclidean space?,\mathbb{R}^n.,"['complex-analysis', 'reference-request', 'differential-forms', 'big-list', 'de-rham-cohomology']"
81,Solving $(z+1)^5 = z^5$,Solving,(z+1)^5 = z^5,"The question says to solve this equation: $(z+1)^5 = z^5$ I did. Just want to find out if I did it properly and if my run-around logic makes sense. First I begin my writing the equations as: $$ (z+1)^5 = z^5$$ $$ \mathbf{e}^{5 \mathbf{Log}(z+1)} = \mathbf{e}^{5 \mathbf{Log}(z)} $$ So $$  \mathbf{Log}(z+1) = \ln|z+1| + \mathbf{Arg}(z+1)i $$   $$  \mathbf{Log}(z) = \ln|z| + \mathbf{Arg}(z)i $$ Now, because the natural logarithm is one-to-one, I write: $$ \ln |z| = \ln |z+1| \Rightarrow |z| = |z+1|$$ So assign $ z = a +bi$ So that $|z| = \sqrt{a^2 +b^2} = |z+1| = \sqrt{(a+1)^2 +b^2} \Rightarrow a^2 +b^2 = (a+1)^2 +b^2 \Rightarrow a^2 = (a+1)^2 \Rightarrow a = -\frac12 $ So, $z = -\frac12 + bi$ and $z+1 = \frac12 + bi$ for some $b \in \mathbb R$ Now to find $b$ $$ \mathbf{Arg}(z+1) = \mathbf{Arg}(z)$$ $$ \tan^{-1} \frac{b}{\frac12} = \pi - \tan^{-1}\frac{b}{-\frac12}$$ I have a feeling this last part isn't quite right, so I just want to find out if I'm approaching this question properly? Ultimately, I get $ z = -\frac12$ which upon inspection...is wrong...","The question says to solve this equation: $(z+1)^5 = z^5$ I did. Just want to find out if I did it properly and if my run-around logic makes sense. First I begin my writing the equations as: $$ (z+1)^5 = z^5$$ $$ \mathbf{e}^{5 \mathbf{Log}(z+1)} = \mathbf{e}^{5 \mathbf{Log}(z)} $$ So $$  \mathbf{Log}(z+1) = \ln|z+1| + \mathbf{Arg}(z+1)i $$   $$  \mathbf{Log}(z) = \ln|z| + \mathbf{Arg}(z)i $$ Now, because the natural logarithm is one-to-one, I write: $$ \ln |z| = \ln |z+1| \Rightarrow |z| = |z+1|$$ So assign $ z = a +bi$ So that $|z| = \sqrt{a^2 +b^2} = |z+1| = \sqrt{(a+1)^2 +b^2} \Rightarrow a^2 +b^2 = (a+1)^2 +b^2 \Rightarrow a^2 = (a+1)^2 \Rightarrow a = -\frac12 $ So, $z = -\frac12 + bi$ and $z+1 = \frac12 + bi$ for some $b \in \mathbb R$ Now to find $b$ $$ \mathbf{Arg}(z+1) = \mathbf{Arg}(z)$$ $$ \tan^{-1} \frac{b}{\frac12} = \pi - \tan^{-1}\frac{b}{-\frac12}$$ I have a feeling this last part isn't quite right, so I just want to find out if I'm approaching this question properly? Ultimately, I get $ z = -\frac12$ which upon inspection...is wrong...",,"['complex-analysis', 'complex-numbers']"
82,Find all five solutions of the equation $z^5+z^4+z^3+z^2+z+1 = 0$,Find all five solutions of the equation,z^5+z^4+z^3+z^2+z+1 = 0,$z^5+z^4+z^3+z^2+z+1 = 0$ I can't figure this out can someone offer any suggestions? Factoring it into $(z+1)(z^4+z^2+1)$ didn't do anything but show -1 is one solution. I solved for all roots of $z^4 = -4$ but the structure for this example was more simple.,$z^5+z^4+z^3+z^2+z+1 = 0$ I can't figure this out can someone offer any suggestions? Factoring it into $(z+1)(z^4+z^2+1)$ didn't do anything but show -1 is one solution. I solved for all roots of $z^4 = -4$ but the structure for this example was more simple.,,"['complex-analysis', 'complex-numbers', 'roots', 'roots-of-unity']"
83,Singularity at infinity of a function entire,Singularity at infinity of a function entire,,"How to prove that every non-constant entire function $\,\,f:\mathbb{C}\rightarrow\mathbb{C}\,\,$ has a singularity at infinity? What type of singularity must this be?","How to prove that every non-constant entire function $\,\,f:\mathbb{C}\rightarrow\mathbb{C}\,\,$ has a singularity at infinity? What type of singularity must this be?",,['complex-analysis']
84,Why isn't $f(z)=\bar{z}$ complex differentiable,Why isn't  complex differentiable,f(z)=\bar{z},It is quite easy to see why $f(z)=\bar{z}$ isn't complex differentiable. $\frac{\partial{u}}{\partial{x}}=1\neq-1=\frac{\partial{v}}{\partial{y}}$ But I struggle to see why this is the case. Visualization of the conjugate function in my head is simply flipping the complex plane upside down. I don't see how this creates any discontinuities. Am I having the wrong visualization or have I misunderstood complex differentiability?,It is quite easy to see why $f(z)=\bar{z}$ isn't complex differentiable. $\frac{\partial{u}}{\partial{x}}=1\neq-1=\frac{\partial{v}}{\partial{y}}$ But I struggle to see why this is the case. Visualization of the conjugate function in my head is simply flipping the complex plane upside down. I don't see how this creates any discontinuities. Am I having the wrong visualization or have I misunderstood complex differentiability?,,"['complex-analysis', 'derivatives', 'partial-differential-equations', 'cauchy-riemann-equations']"
85,Does there exist a complex function which is differentiable at one point and nowhere else continuous?,Does there exist a complex function which is differentiable at one point and nowhere else continuous?,,"Let $f\colon\mathbb{C}\to\mathbb{C}$ . We know that if $f^{\prime}(a)$ exists for some $a\in\mathbb{C}$ then $f$ is continuous at $a$ . This is because, from the definition of the derivative, $$f(z)-f(a)=[f^{\prime}(a)+\varepsilon(z)](z-a)$$ for some $\varepsilon$ such that $\varepsilon(z)\to0$ as $z\to a$ , and hence $f(z)\to f(a)$ . I think we can interpret this geometrically by saying that, for $z$ close to $a$ , if we take the vector from $a$ to $z$ , rotate it counterclockwise about its tail by $\mathrm{Arg}\,{(f^{\prime}(a))}$ and scale its length by $\lvert f^{\prime}(a)\rvert$ , then we get approximately the vector from $f(a)$ to $f(z)$ (it's only approximate, because of the $\varepsilon$ error term, but for $z$ close to $a$ this $\varepsilon$ is small). However, consider the following heuristic argument: let $z_{1}$ and $z_{2}$ both be ""close to"" $a$ (and therefore close to each other). The above interpretation then suggests that the vector from $f(a)$ to $f(z_{1})$ should be approximately the same as the vector from $f(a)$ to $f(z_{2})$ , and therefore $f(z_{2})-f(z_{1})$ should be small in magnitude. This seems to suggest that $f$ should be continuous not just at $a$ , but also in some small interval around $a$ . This seems too good to be true. Hence my question: Does there exist a function $f\colon\mathbb{C}\to\mathbb{C}$ such that $f^{\prime}(a)$ exists at some point $a\in\mathbb{C}$ but $f$ is continuous nowhere else? Update, added later: A quick thanks to those who responded. I have since thought through this with a friend and we've found a problem with the heuristic argument: Properly interpreted, the heuristic argument is saying that if $z_{1}$ and $z_{2}$ are close to $a$ , then $f(z_{1})$ is close to $f(z_{2})$ . However, to have continuity at $z_{1}$ , say, we need to be able to make $f(z_{2})$ arbitrarily close to $f(z_{1})$ by choosing $z_{2}$ to be sufficiently close to $z_{1}$ --- therein lies the problem. Suppose $\lvert z_{3}-z_{1} \rvert < \lvert z_{2}-z_{1} \rvert$ , i.e., suppose $z_{3}$ is even closer to $z_{1}$ than $z_{2}$ is. Then, for $f$ to be continuous, we need to be able to ensure that $f(z_{3})$ is closer still to $f(z_{1})$ (than $f(z_{2})$ is); in general, we can't ensure this. If I get round to it, I might try to work through the details in a specific case and see if there are any other problems. If I get anywhere, I'll post the results as an additional answer.","Let . We know that if exists for some then is continuous at . This is because, from the definition of the derivative, for some such that as , and hence . I think we can interpret this geometrically by saying that, for close to , if we take the vector from to , rotate it counterclockwise about its tail by and scale its length by , then we get approximately the vector from to (it's only approximate, because of the error term, but for close to this is small). However, consider the following heuristic argument: let and both be ""close to"" (and therefore close to each other). The above interpretation then suggests that the vector from to should be approximately the same as the vector from to , and therefore should be small in magnitude. This seems to suggest that should be continuous not just at , but also in some small interval around . This seems too good to be true. Hence my question: Does there exist a function such that exists at some point but is continuous nowhere else? Update, added later: A quick thanks to those who responded. I have since thought through this with a friend and we've found a problem with the heuristic argument: Properly interpreted, the heuristic argument is saying that if and are close to , then is close to . However, to have continuity at , say, we need to be able to make arbitrarily close to by choosing to be sufficiently close to --- therein lies the problem. Suppose , i.e., suppose is even closer to than is. Then, for to be continuous, we need to be able to ensure that is closer still to (than is); in general, we can't ensure this. If I get round to it, I might try to work through the details in a specific case and see if there are any other problems. If I get anywhere, I'll post the results as an additional answer.","f\colon\mathbb{C}\to\mathbb{C} f^{\prime}(a) a\in\mathbb{C} f a f(z)-f(a)=[f^{\prime}(a)+\varepsilon(z)](z-a) \varepsilon \varepsilon(z)\to0 z\to a f(z)\to f(a) z a a z \mathrm{Arg}\,{(f^{\prime}(a))} \lvert f^{\prime}(a)\rvert f(a) f(z) \varepsilon z a \varepsilon z_{1} z_{2} a f(a) f(z_{1}) f(a) f(z_{2}) f(z_{2})-f(z_{1}) f a a f\colon\mathbb{C}\to\mathbb{C} f^{\prime}(a) a\in\mathbb{C} f z_{1} z_{2} a f(z_{1}) f(z_{2}) z_{1} f(z_{2}) f(z_{1}) z_{2} z_{1} \lvert z_{3}-z_{1} \rvert < \lvert z_{2}-z_{1} \rvert z_{3} z_{1} z_{2} f f(z_{3}) f(z_{1}) f(z_{2})","['complex-analysis', 'examples-counterexamples']"
86,How to find Laurent series Expansion,How to find Laurent series Expansion,,"$f(z)$ is defined like this: $$ f(z) = \frac{z}{(z-1)(z-3)} $$ I need to find a series for $f(z)$ that involves positive and negative powers of $(z-1)$, which converges to $f(z)$ when $0 \leq |z - 1| \leq 2$. What I understand from question is I must expand $f(z)$ Laurent series. $$ f(z) = \sum_{m=0}^{\infty}a_{m}(z-1)^{m} + \sum_{m=1}^{\infty}b_{m}(z-1)^{-m}$$ where, $$ a_{m} = \frac{1}{j2\pi}\oint_{C}\frac{f(z)}{(z-1)^{m+1}}dz $$ $$ b_{m} = \frac{1}{j2\pi}\oint_{C}\frac{f(z)}{(z-1)^{1-m}}dz $$ This is what theory tells me. But I apply partial fraction method to this function like this: $$ f(z) = \frac{z}{(z-1)(z-3)} = \frac{z^{-1}}{(1-z^{-1})(1-3z^{-1})} = \frac{-1/2}{(1-z^{-1})} + \frac{1/2}{(1-3z^{-1})} $$ And I know this series expansion from z-transform like this: $$ f(z) = -\frac{1}{2} \sum_{k=0}^{\infty}z^{-k} + \frac{1}{2} \sum_{k=0}^{\infty}3^{k}z^{-k}$$ I obtain a series expansion but it looks like Mclaurin series not a Laurent series. Here, my first question an expression may have different type of series expansion? And second, how to find a Laurent series for $ f(z) $","$f(z)$ is defined like this: $$ f(z) = \frac{z}{(z-1)(z-3)} $$ I need to find a series for $f(z)$ that involves positive and negative powers of $(z-1)$, which converges to $f(z)$ when $0 \leq |z - 1| \leq 2$. What I understand from question is I must expand $f(z)$ Laurent series. $$ f(z) = \sum_{m=0}^{\infty}a_{m}(z-1)^{m} + \sum_{m=1}^{\infty}b_{m}(z-1)^{-m}$$ where, $$ a_{m} = \frac{1}{j2\pi}\oint_{C}\frac{f(z)}{(z-1)^{m+1}}dz $$ $$ b_{m} = \frac{1}{j2\pi}\oint_{C}\frac{f(z)}{(z-1)^{1-m}}dz $$ This is what theory tells me. But I apply partial fraction method to this function like this: $$ f(z) = \frac{z}{(z-1)(z-3)} = \frac{z^{-1}}{(1-z^{-1})(1-3z^{-1})} = \frac{-1/2}{(1-z^{-1})} + \frac{1/2}{(1-3z^{-1})} $$ And I know this series expansion from z-transform like this: $$ f(z) = -\frac{1}{2} \sum_{k=0}^{\infty}z^{-k} + \frac{1}{2} \sum_{k=0}^{\infty}3^{k}z^{-k}$$ I obtain a series expansion but it looks like Mclaurin series not a Laurent series. Here, my first question an expression may have different type of series expansion? And second, how to find a Laurent series for $ f(z) $",,"['complex-analysis', 'power-series', 'laurent-series']"
87,Does there exist an holomorphic function such that $|f(z)|\geq \frac{1}{\sqrt{|z|}}$?,Does there exist an holomorphic function such that ?,|f(z)|\geq \frac{1}{\sqrt{|z|}},"I have some trouble solving this problem: Does there exist a holomorphic function $f$ on $\mathbb C\setminus \{0\}$ such that $$|f(z)|\geq \frac{1}{\sqrt{|z|}}$$ for all $z\in\mathbb C \setminus \{0\}$ ? I don't know where to start. My intuition is that you would get a problem with the singularity near $0$ , but I am not sure how to prove it. Any help would be appreciated! Thanks!","I have some trouble solving this problem: Does there exist a holomorphic function on such that for all ? I don't know where to start. My intuition is that you would get a problem with the singularity near , but I am not sure how to prove it. Any help would be appreciated! Thanks!",f \mathbb C\setminus \{0\} |f(z)|\geq \frac{1}{\sqrt{|z|}} z\in\mathbb C \setminus \{0\} 0,['complex-analysis']
88,Addition theorems for elliptic functions: is there a painless way?,Addition theorems for elliptic functions: is there a painless way?,,"The Weierstrass $\wp$ function satisfies the addition formula $$\wp(z+Z)+\wp(z)+\wp(Z) = \left(\frac{\wp'(z)-\wp'(Z)}{\wp(z)-\wp(Z)}\right)^2.$$ Of course, this is just the $x$-coordinate of the sum of the points $(\wp'(z), \wp(z))$ and $(\wp'(Z), \wp(Z))$ on the Weierstrass elliptic curve $y^2=4x^3-g_2x-g_3$. If one has an a priori knowledge of this fact, the computation of the addition formula is absolutely trivial. However, it seems that, in the literature, there is no ""illuminating"" proof from first principles. Perhaps my standards for ""illuminating"" are too high, but in comparison with proofs of addition formulas for trigonometric functions, the addition formula for $\wp$ is painstaking to establish. I would like to know how the formula above, or an equivalent formula (perhaps for the Weierstrass $\sigma$-functions?) may be deduced in the most direct possible manner. Of course, a proof such as one that involves the comparison of power series around the origin is very direct, but it requires a first-hand knowledge of the formula, so it's not quite what I'm looking for. All the best, and thank you!","The Weierstrass $\wp$ function satisfies the addition formula $$\wp(z+Z)+\wp(z)+\wp(Z) = \left(\frac{\wp'(z)-\wp'(Z)}{\wp(z)-\wp(Z)}\right)^2.$$ Of course, this is just the $x$-coordinate of the sum of the points $(\wp'(z), \wp(z))$ and $(\wp'(Z), \wp(Z))$ on the Weierstrass elliptic curve $y^2=4x^3-g_2x-g_3$. If one has an a priori knowledge of this fact, the computation of the addition formula is absolutely trivial. However, it seems that, in the literature, there is no ""illuminating"" proof from first principles. Perhaps my standards for ""illuminating"" are too high, but in comparison with proofs of addition formulas for trigonometric functions, the addition formula for $\wp$ is painstaking to establish. I would like to know how the formula above, or an equivalent formula (perhaps for the Weierstrass $\sigma$-functions?) may be deduced in the most direct possible manner. Of course, a proof such as one that involves the comparison of power series around the origin is very direct, but it requires a first-hand knowledge of the formula, so it's not quite what I'm looking for. All the best, and thank you!",,"['complex-analysis', 'special-functions', 'elliptic-functions']"
89,Infinite products - reference needed!,Infinite products - reference needed!,,"I am looking for a small treatment of basic theorems about infinite products ; surprisingly enough they are nowhere to be found after googling a little. The reason for this is that I am beginning to read Davenport's Multiplicative Number Theory, and the treatment of L-functions in there requires to understand convergence/absolute convergence of infinite products, which I know little about. Most importantly I'd like to know why $$ \prod (1+|a_n|) \to a < \infty    \quad \Longrightarrow \quad \prod (1+ a_n) \to b \neq 0. $$ I believe I'll need more properties of products later on, so just a proof of this would be appreciated but I'd also need the reference. Thanks in advance,","I am looking for a small treatment of basic theorems about infinite products ; surprisingly enough they are nowhere to be found after googling a little. The reason for this is that I am beginning to read Davenport's Multiplicative Number Theory, and the treatment of L-functions in there requires to understand convergence/absolute convergence of infinite products, which I know little about. Most importantly I'd like to know why $$ \prod (1+|a_n|) \to a < \infty    \quad \Longrightarrow \quad \prod (1+ a_n) \to b \neq 0. $$ I believe I'll need more properties of products later on, so just a proof of this would be appreciated but I'd also need the reference. Thanks in advance,",,"['complex-analysis', 'reference-request', 'products', 'infinite-product']"
90,Is a meromorphic function always a ratio of two holomorphic functions?,Is a meromorphic function always a ratio of two holomorphic functions?,,"Suppose $D$ is a region (connected open set) in complex plane, and $f$ is a meromorphic function on $D$. Question : Does there always exist two holomorphic function $g$ and $h$ such that $f=\frac{g}{h}$? When $D$ is the whole complex plane I know it is true thank to Weierstrass infinite multiply formula, and I don't know whether it is true for any region $D$. Any comments are welcome.","Suppose $D$ is a region (connected open set) in complex plane, and $f$ is a meromorphic function on $D$. Question : Does there always exist two holomorphic function $g$ and $h$ such that $f=\frac{g}{h}$? When $D$ is the whole complex plane I know it is true thank to Weierstrass infinite multiply formula, and I don't know whether it is true for any region $D$. Any comments are welcome.",,['complex-analysis']
91,A question on Hermitian metric on complex manifold.,A question on Hermitian metric on complex manifold.,,"We say that a Riemannian metric $g$ on a complex manifold $(X,I)$ is Hermitian if  $$ g(x,y)=g(Ix,Iy) $$ for any $x,y\in \Gamma(X,TX)$. Here we consider $X$ as a real even dimensional manifold with complex structure $I$. How can one show that $g$ is locally of the form  $$ g=\sum_{i,j}g_{i,\overline{j}}dz_{i} \otimes d\overline{z}_{j}  $$ where $z_1,\dots$ are local complex coordinate of $X$. I am confused with two definition of complex structure; one given by  $I\in \Gamma(X,End(TX))$ and the other given by local coordinate.","We say that a Riemannian metric $g$ on a complex manifold $(X,I)$ is Hermitian if  $$ g(x,y)=g(Ix,Iy) $$ for any $x,y\in \Gamma(X,TX)$. Here we consider $X$ as a real even dimensional manifold with complex structure $I$. How can one show that $g$ is locally of the form  $$ g=\sum_{i,j}g_{i,\overline{j}}dz_{i} \otimes d\overline{z}_{j}  $$ where $z_1,\dots$ are local complex coordinate of $X$. I am confused with two definition of complex structure; one given by  $I\in \Gamma(X,End(TX))$ and the other given by local coordinate.",,"['complex-analysis', 'complex-geometry']"
92,Plotting in the Complex Plane,Plotting in the Complex Plane,,"I just wonder how do you plot a function on the complex plane? For example,$$f(z)=\left|\dfrac{1}{z}\right|$$  What is the difference plotting this function in the complex plane or real plane?","I just wonder how do you plot a function on the complex plane? For example,$$f(z)=\left|\dfrac{1}{z}\right|$$  What is the difference plotting this function in the complex plane or real plane?",,"['complex-analysis', 'graphing-functions', 'coordinate-systems', 'polar-coordinates']"
93,Sequence of powers of Gaussian integers capturing all positive integers?,Sequence of powers of Gaussian integers capturing all positive integers?,,"Fix a complex number $z=x+iy$ where $x,y\in \mathbf{Z}$ Consider the sequence generated by the powers $$z^0, z^1, z^2, z^3,z^4 \ldots$$ The question is whether it is possible to capture any positive integer as either the real part or the imaginary part of this sequence of the powers of a fixed complex number. From some calculation and reasoning, it is very easy to see that in order to generate all positive integers, $x$ and $y$ have to be relatively prime; it appears that the sequence of real parts and the sequence of imaginary parts blows up very early and hence it would not be unwise to conjecture that such a complex number does not exist. Is there any way to get hold of this problem? Thanks in advance. My curiosity partly grew out of observations on various polynomial bijection questions e.g. , I saw on MathOverflow . I hope that at least the simple statement of the problem appeals to one's thoughts.","Fix a complex number where Consider the sequence generated by the powers The question is whether it is possible to capture any positive integer as either the real part or the imaginary part of this sequence of the powers of a fixed complex number. From some calculation and reasoning, it is very easy to see that in order to generate all positive integers, and have to be relatively prime; it appears that the sequence of real parts and the sequence of imaginary parts blows up very early and hence it would not be unwise to conjecture that such a complex number does not exist. Is there any way to get hold of this problem? Thanks in advance. My curiosity partly grew out of observations on various polynomial bijection questions e.g. , I saw on MathOverflow . I hope that at least the simple statement of the problem appeals to one's thoughts.","z=x+iy x,y\in \mathbf{Z} z^0, z^1, z^2, z^3,z^4 \ldots x y","['number-theory', 'complex-analysis', 'polynomials']"
94,"Show that $\left|\frac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < 1$ when $|\alpha|,|\beta| < 1$",Show that  when,"\left|\frac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < 1 |\alpha|,|\beta| < 1","This is the question I'm stumbling with: When $|\alpha| < 1$ and $|\beta| < 1$, show that: $$\left|\cfrac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < 1$$ The chapter that contains this question contains (among others) the triangle inequalities: $$\left||z_1| - |z_2|\right| \le |z_1 + z_2| \le |z_1| + |z_2| $$ I've tried to use the triangle inequalities to increase the dividend and/or decrease the divisor: $$\left|\cfrac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < \cfrac{|\alpha| +|\beta|}{\left|1-|\bar{\alpha}\beta|\right|}$$ But it's not clear if or why that would be smaller than one.  I've also tried to multiply the equation by the conjugated divisor $\cfrac{1-\alpha\bar{\beta}}{1-\alpha\bar{\beta}}$, which gives a real divisor, but the equation does not appear solvable. Any hint would be much appreciated.","This is the question I'm stumbling with: When $|\alpha| < 1$ and $|\beta| < 1$, show that: $$\left|\cfrac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < 1$$ The chapter that contains this question contains (among others) the triangle inequalities: $$\left||z_1| - |z_2|\right| \le |z_1 + z_2| \le |z_1| + |z_2| $$ I've tried to use the triangle inequalities to increase the dividend and/or decrease the divisor: $$\left|\cfrac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < \cfrac{|\alpha| +|\beta|}{\left|1-|\bar{\alpha}\beta|\right|}$$ But it's not clear if or why that would be smaller than one.  I've also tried to multiply the equation by the conjugated divisor $\cfrac{1-\alpha\bar{\beta}}{1-\alpha\bar{\beta}}$, which gives a real divisor, but the equation does not appear solvable. Any hint would be much appreciated.",,"['complex-analysis', 'inequality', 'complex-numbers']"
95,Understanding the Analytic Continuation of the Gamma Function,Understanding the Analytic Continuation of the Gamma Function,,"So my book proves the convergence of $\Gamma(z) = \int_0^{\infty}t^{z-1}e^{-t}dt$ in the right half plane $Re(z) > 0$, and then goes on to prove the initial recurrence relation $\Gamma(z+1)=z\Gamma(z)$ by applying integration by parts to $\Gamma(z+1)$: $$\int_0^{\infty}t^{z}e^{-t}dt = -t^ze^{-t}|_0^{\infty} + z\int_0^{\infty}t^{z-1}e^{-t}dt$$ The book explicitly states this equality to be true only in the right half plane, since otherwise $-t^ze^{-t}|_0^{\infty} = \infty$, instead of equaling zero.  With this initial recurrence relation we are 'supposably' able to analytically continue the Gamma function to $Re(z) > -1$ (not including the origin) by writing the relation in the form: $$\Gamma(z) = \frac{\Gamma(z+1)}{z}$$ What I don't understand is this relation is still only true in the right half plane, since otherwise $-t^ze^{-t}|_0^{\infty}\neq 0$.  I don't see what reason we have to believe that, for instance, $\Gamma(-\frac{1}{2}) = \frac{\Gamma(\frac{1}{2})}{-\frac{1}{2}}$. Furthermore $\int_0^{\infty}t^{z-1}e^{-t}dt$ is clearly not convergent in the left half plane, so I can't even imagine why it would be plausible to think that a recurrence relation directly based on it could possibly lead to a genuine analytic continuation of its domain.","So my book proves the convergence of $\Gamma(z) = \int_0^{\infty}t^{z-1}e^{-t}dt$ in the right half plane $Re(z) > 0$, and then goes on to prove the initial recurrence relation $\Gamma(z+1)=z\Gamma(z)$ by applying integration by parts to $\Gamma(z+1)$: $$\int_0^{\infty}t^{z}e^{-t}dt = -t^ze^{-t}|_0^{\infty} + z\int_0^{\infty}t^{z-1}e^{-t}dt$$ The book explicitly states this equality to be true only in the right half plane, since otherwise $-t^ze^{-t}|_0^{\infty} = \infty$, instead of equaling zero.  With this initial recurrence relation we are 'supposably' able to analytically continue the Gamma function to $Re(z) > -1$ (not including the origin) by writing the relation in the form: $$\Gamma(z) = \frac{\Gamma(z+1)}{z}$$ What I don't understand is this relation is still only true in the right half plane, since otherwise $-t^ze^{-t}|_0^{\infty}\neq 0$.  I don't see what reason we have to believe that, for instance, $\Gamma(-\frac{1}{2}) = \frac{\Gamma(\frac{1}{2})}{-\frac{1}{2}}$. Furthermore $\int_0^{\infty}t^{z-1}e^{-t}dt$ is clearly not convergent in the left half plane, so I can't even imagine why it would be plausible to think that a recurrence relation directly based on it could possibly lead to a genuine analytic continuation of its domain.",,"['complex-analysis', 'gamma-function']"
96,Complex analysis book recommendations with more exercise than Ahlfors' book.,Complex analysis book recommendations with more exercise than Ahlfors' book.,,"I am learning complex analysis and currently reading Ahlfors’ Complex Analysis. I feel like I need to do more exercises than the five problems provided in each section. Therefore, I am seeking recommendations for complex analysis books with more exercises than Ahlfors’, but still at the same level and cover all (or more) of the topics covered in Ahlfors' book. Additionally, I am seeking recommendations for only strictly pure mathematics books without any real-world applications. I will continue to read Ahlfors’ book alongside the recommended book because I think Ahlfors is still worth reading.","I am learning complex analysis and currently reading Ahlfors’ Complex Analysis. I feel like I need to do more exercises than the five problems provided in each section. Therefore, I am seeking recommendations for complex analysis books with more exercises than Ahlfors’, but still at the same level and cover all (or more) of the topics covered in Ahlfors' book. Additionally, I am seeking recommendations for only strictly pure mathematics books without any real-world applications. I will continue to read Ahlfors’ book alongside the recommended book because I think Ahlfors is still worth reading.",,"['complex-analysis', 'reference-request', 'problem-solving', 'book-recommendation']"
97,"Using complex analysis, show that $\int_0^\infty\frac{\arctan(x)}{1+x^2}\,dx=\frac{\pi^2}8$","Using complex analysis, show that","\int_0^\infty\frac{\arctan(x)}{1+x^2}\,dx=\frac{\pi^2}8","The result is of course confirmed by substituting $x\mapsto\tan(x)$ : $$I = \int_0^\infty \frac{\arctan(x)}{1+x^2} \, dx = \int_0^{\frac\pi2} x \, dx = \frac{\pi^2}8$$ Or integrating by parts: $$I = \lim_{x\to\infty} \arctan^2(x) - I \implies 2I = \left(\frac\pi2\right)^2 \implies I = \frac{\pi^2}8$$ Or splitting the integral at $x=1$ and substituting $x\mapsto\frac1x$ on the integral over $[1,\infty)$ : $$I= \int_0^1 \frac{\arctan(x) + \arctan\left(\frac1x\right)}{1 + x^2} \, dx = \frac\pi2 \int_0^1 \frac{dx}{1+x^2} = \frac{\pi^2}8$$ Or differentiating under the integral sign: $$I(a) = \int_0^\infty \frac{\arctan(ax)}{1+x^2} \, dx \implies I'(a) = \int_0^\infty \frac x{(1+x^2)(1+a^2x^2)} \, dx = \frac{\ln(a)}{a^2-1} \\ I(0) = 0 \implies I(1) = \int_0^1 \frac{\ln(x)}{x^2-1} \, dx = \frac{\pi^2}8$$ Or getting the same integral of $\frac{\ln(x)}{x^2-1}$ by converting $\arctan(x)$ to an integral representation and computing the resulting double integral (per @Dr.WolfgangHintze's suggestion) using the same substitution as in the third method above: $$\begin{align*} I &= \int_0^\infty \int_0^x \frac x{(1+x^2)(1+x^2y^2)} \, dy \, dx \\[1ex] &= \int_0^\infty \int_y^\infty \frac x{(1+x^2)(1+x^2y^2)} \, dx \, dy \\[1ex] &= \frac12 \int_0^\infty \frac{\ln\left(\frac{y^2+y^4}{1+y^4}\right)}{y^2-1} \, dy \\[1ex] &= \int_0^\infty \frac{\ln(y)}{y^2-1} \, dy + \frac12 \int_0^\infty \frac{\ln(1 + y^2)}{y^2-1} \, dy - \frac12 \int_0^\infty \frac{\ln(1+y^4)}{y^2-1} \, dy \\[1ex] &= (1 + 2 - 2) \int_0^1 \frac{\ln(y)}{y^2-1} \, dy = \frac{\pi^2}8 \end{align*}$$ I was wondering how, if at all possible, one might approach it with the residue theorem? I see that $z=\pm i$ are simple poles of $\frac1{1+z^2}$ , but they're also the branch points of $\arctan(z)$ , since $$\arctan(z) = -\frac i2 \log\left(\frac{i-z}{i+z}\right)$$ so I don't believe the theorem can be readily applied here. The integrand is odd so I don't think there's much to infer from symmetry. Maybe there's a way to massage the integrand to get closer to something with which we can use a contour integral.","The result is of course confirmed by substituting : Or integrating by parts: Or splitting the integral at and substituting on the integral over : Or differentiating under the integral sign: Or getting the same integral of by converting to an integral representation and computing the resulting double integral (per @Dr.WolfgangHintze's suggestion) using the same substitution as in the third method above: I was wondering how, if at all possible, one might approach it with the residue theorem? I see that are simple poles of , but they're also the branch points of , since so I don't believe the theorem can be readily applied here. The integrand is odd so I don't think there's much to infer from symmetry. Maybe there's a way to massage the integrand to get closer to something with which we can use a contour integral.","x\mapsto\tan(x) I = \int_0^\infty \frac{\arctan(x)}{1+x^2} \, dx = \int_0^{\frac\pi2} x \, dx = \frac{\pi^2}8 I = \lim_{x\to\infty} \arctan^2(x) - I \implies 2I = \left(\frac\pi2\right)^2 \implies I = \frac{\pi^2}8 x=1 x\mapsto\frac1x [1,\infty) I= \int_0^1 \frac{\arctan(x) + \arctan\left(\frac1x\right)}{1 + x^2} \, dx = \frac\pi2 \int_0^1 \frac{dx}{1+x^2} = \frac{\pi^2}8 I(a) = \int_0^\infty \frac{\arctan(ax)}{1+x^2} \, dx \implies I'(a) = \int_0^\infty \frac x{(1+x^2)(1+a^2x^2)} \, dx = \frac{\ln(a)}{a^2-1} \\ I(0) = 0 \implies I(1) = \int_0^1 \frac{\ln(x)}{x^2-1} \, dx = \frac{\pi^2}8 \frac{\ln(x)}{x^2-1} \arctan(x) \begin{align*}
I &= \int_0^\infty \int_0^x \frac x{(1+x^2)(1+x^2y^2)} \, dy \, dx \\[1ex]
&= \int_0^\infty \int_y^\infty \frac x{(1+x^2)(1+x^2y^2)} \, dx \, dy \\[1ex]
&= \frac12 \int_0^\infty \frac{\ln\left(\frac{y^2+y^4}{1+y^4}\right)}{y^2-1} \, dy \\[1ex]
&= \int_0^\infty \frac{\ln(y)}{y^2-1} \, dy + \frac12 \int_0^\infty \frac{\ln(1 + y^2)}{y^2-1} \, dy - \frac12 \int_0^\infty \frac{\ln(1+y^4)}{y^2-1} \, dy \\[1ex]
&= (1 + 2 - 2) \int_0^1 \frac{\ln(y)}{y^2-1} \, dy = \frac{\pi^2}8
\end{align*} z=\pm i \frac1{1+z^2} \arctan(z) \arctan(z) = -\frac i2 \log\left(\frac{i-z}{i+z}\right)","['complex-analysis', 'definite-integrals', 'residue-calculus']"
98,Maximum of $|(z-a_1)\cdots(z-a_n)|$ on the unit circle,Maximum of  on the unit circle,|(z-a_1)\cdots(z-a_n)|,"Let $a_1,\ldots,a_n$ be points on the unit circle. Let $P(z)=(z-a_1)\cdots(z-a_n)$. The maximum principle or Rouche's theorem can be used to show that there exists a point $b$ on the unit circle such that $|P(b)|\geq1$. Question: What is the maximum value $c$ such that there always exists a point $b$ on the unit circle such that $|P(b)|\geq c$, regardless of what $a_1,\ldots,a_n$ are?","Let $a_1,\ldots,a_n$ be points on the unit circle. Let $P(z)=(z-a_1)\cdots(z-a_n)$. The maximum principle or Rouche's theorem can be used to show that there exists a point $b$ on the unit circle such that $|P(b)|\geq1$. Question: What is the maximum value $c$ such that there always exists a point $b$ on the unit circle such that $|P(b)|\geq c$, regardless of what $a_1,\ldots,a_n$ are?",,"['complex-analysis', 'inequality', 'complex-numbers']"
99,Definite integral using the method of residues,Definite integral using the method of residues,,"I have the following integral to compute: $$ \int_{0}^{\infty}\frac{\log x}{1 + x^2}\text{d}x.$$  Following is my attempt: $$  \int_{0}^{\infty}\frac{\log x}{1 + x^2}\text{d}x  = \int_{0}^{1}\frac{\log x}{1 + x^2}\text{d}x +  \int_{1}^{\infty}\frac{\log x}{1 + x^2}\text{d}x .$$  But using the substitution $x=1/u$, we get:  $$\int_{0}^{1}\frac{\log x}{1 + x^2}\text{d}x= -\int_{\infty}^{1}\frac{1}{u^2}\cdot\frac{\log (1/u)}{1 + (1/u)^2}\text{d}u = -\int_{1}^{\infty}\frac{\log u}{1 + u^2}\text{d}u.$$  Hence $$ \int_{0}^{\infty}\frac{\log x}{1 + x^2}\text{d}x = -\int_{1}^{\infty}\frac{\log u}{1 + u^2}\text{d}u +  \int_{1}^{\infty}\frac{\log x}{1 + x^2}\text{d}x = 0$$ since $u$ is a dummy variable. What I'd like to do now is to compute the same integral using the method of residues(I have no experience with it) and I'd gladly appreciate any kind of help. Thanks.","I have the following integral to compute: $$ \int_{0}^{\infty}\frac{\log x}{1 + x^2}\text{d}x.$$  Following is my attempt: $$  \int_{0}^{\infty}\frac{\log x}{1 + x^2}\text{d}x  = \int_{0}^{1}\frac{\log x}{1 + x^2}\text{d}x +  \int_{1}^{\infty}\frac{\log x}{1 + x^2}\text{d}x .$$  But using the substitution $x=1/u$, we get:  $$\int_{0}^{1}\frac{\log x}{1 + x^2}\text{d}x= -\int_{\infty}^{1}\frac{1}{u^2}\cdot\frac{\log (1/u)}{1 + (1/u)^2}\text{d}u = -\int_{1}^{\infty}\frac{\log u}{1 + u^2}\text{d}u.$$  Hence $$ \int_{0}^{\infty}\frac{\log x}{1 + x^2}\text{d}x = -\int_{1}^{\infty}\frac{\log u}{1 + u^2}\text{d}u +  \int_{1}^{\infty}\frac{\log x}{1 + x^2}\text{d}x = 0$$ since $u$ is a dummy variable. What I'd like to do now is to compute the same integral using the method of residues(I have no experience with it) and I'd gladly appreciate any kind of help. Thanks.",,"['complex-analysis', 'definite-integrals']"
