,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,The difference between m and n in calculating a Fourier series,The difference between m and n in calculating a Fourier series,,"I am studying for an exam in Differential Equations, and one of the topics I should know about is Fourier series. Now, I am using Boyce 9e, and in there I found the general equation for a Fourier series: $$\frac{a_0}{2} + \sum_{m=1}^{\infty} (a_m cos\frac{m \pi x}{L} + b_m sin\frac{m \pi x}{L})$$ I also found the equations to calculate the coefficients in terms of n, where n is any real integer: $$a_n = \frac{1}{L} \int_{-L}^{L} f(x) cos\frac{n \pi x}{L}dx$$ $$b_n = \frac{1}{L} \int_{-L}^{L} f(x) sin\frac{n \pi x}{L}dx$$ I noticed that the coefficients are calculated in terms of n, but are used in the general equation in terms of m. I also noticed that at the end of some exercises in my book, they convert from n to m. So my question is: what is the difference between n and m, and why can't I calculate my coefficients in terms of m directly? Why do I have to calculate them in terms of n, and then convert them? I hope that some of you can help me out!","I am studying for an exam in Differential Equations, and one of the topics I should know about is Fourier series. Now, I am using Boyce 9e, and in there I found the general equation for a Fourier series: $$\frac{a_0}{2} + \sum_{m=1}^{\infty} (a_m cos\frac{m \pi x}{L} + b_m sin\frac{m \pi x}{L})$$ I also found the equations to calculate the coefficients in terms of n, where n is any real integer: $$a_n = \frac{1}{L} \int_{-L}^{L} f(x) cos\frac{n \pi x}{L}dx$$ $$b_n = \frac{1}{L} \int_{-L}^{L} f(x) sin\frac{n \pi x}{L}dx$$ I noticed that the coefficients are calculated in terms of n, but are used in the general equation in terms of m. I also noticed that at the end of some exercises in my book, they convert from n to m. So my question is: what is the difference between n and m, and why can't I calculate my coefficients in terms of m directly? Why do I have to calculate them in terms of n, and then convert them? I hope that some of you can help me out!",,"['ordinary-differential-equations', 'fourier-series']"
1,How to prove that the number $\pi$ is irrational through differential equations?,How to prove that the number  is irrational through differential equations?,\pi,How can it be proved that $\pi$ is an irrational number through differential equations?,How can it be proved that $\pi$ is an irrational number through differential equations?,,['ordinary-differential-equations']
2,"$dy/dx + 3y = 8$, $y(0) = 0$ : Initial value problem",",  : Initial value problem",dy/dx + 3y = 8 y(0) = 0,"$$\begin{cases} \frac{dy}{dx} + 3y = 8, \\ y(0) = 0. \end{cases}$$ So, I have been getting an answer of $3$ by integrating and getting $\ln(8-3y) = x$ and solving. But my book says the answer must be expressed as a function of $x$. I do not know what to do.","$$\begin{cases} \frac{dy}{dx} + 3y = 8, \\ y(0) = 0. \end{cases}$$ So, I have been getting an answer of $3$ by integrating and getting $\ln(8-3y) = x$ and solving. But my book says the answer must be expressed as a function of $x$. I do not know what to do.",,['ordinary-differential-equations']
3,"What does it mean mathematically to set some of the integration constants in the general solution to a linear differential equation, equal to zero?","What does it mean mathematically to set some of the integration constants in the general solution to a linear differential equation, equal to zero?",,"I'm trying to calculate the position of a particle in a quadrapole magnet depending on the entry position $x_0$ and the combined (constant) physical parameters $k$.  Given an equation $$x(t) =\frac{(\frac{x''(t)}{k})''}{k},$$ solving via assuming that $x(t) = e^{\lambda t}$ et,c... I arrive at the general solution $$x(t) = c_1\cos(\sqrt{k}\cdot t)+c_2\sin(\sqrt{k}\cdot t)+c_3e^{-\sqrt{k}\cdot t}+c_4e^{\sqrt{k}\cdot t}$$ with $c_1,c_2,c_3,c_4$ arbitrary constants. What would it mean mathematically if I were to set say $c_1,c_2,c_3 = 0$, assuming I don't have other constraints (in my example I would have additional $x(0) = x_0$, but as far as I can see that doesn't forbid it). Given that they are arbitrary, I can't see a problem with it. Of course, if you have additional starting conditions, you have to set the constants accordingly, but in my example $c_4 = x_0$ seems to do the job and leaves me with a much simpler solution. So why would I ever NOT eliminate every unnecessary term?","I'm trying to calculate the position of a particle in a quadrapole magnet depending on the entry position $x_0$ and the combined (constant) physical parameters $k$.  Given an equation $$x(t) =\frac{(\frac{x''(t)}{k})''}{k},$$ solving via assuming that $x(t) = e^{\lambda t}$ et,c... I arrive at the general solution $$x(t) = c_1\cos(\sqrt{k}\cdot t)+c_2\sin(\sqrt{k}\cdot t)+c_3e^{-\sqrt{k}\cdot t}+c_4e^{\sqrt{k}\cdot t}$$ with $c_1,c_2,c_3,c_4$ arbitrary constants. What would it mean mathematically if I were to set say $c_1,c_2,c_3 = 0$, assuming I don't have other constraints (in my example I would have additional $x(0) = x_0$, but as far as I can see that doesn't forbid it). Given that they are arbitrary, I can't see a problem with it. Of course, if you have additional starting conditions, you have to set the constants accordingly, but in my example $c_4 = x_0$ seems to do the job and leaves me with a much simpler solution. So why would I ever NOT eliminate every unnecessary term?",,"['calculus', 'ordinary-differential-equations', 'integration', 'physics']"
4,Help solving differential equation,Help solving differential equation,,"I want to solve the following differential equation: $y[t]$ : vertical position (height) of the object at time t $y_c$  : height of the ceiling $y_e$  : equilibrium point, the height at which the mass will stop at the end of its movement. $a[t]$ : acceleration at time t $t$    : time $k$    : spring coefficient $m$    : mass of the object $G$    : gravity $$\begin{align} &F = -ky[t] - mG \\ \Leftrightarrow &ma[t] = -ky[t] -mG  \\ \Leftrightarrow&my''[t] = -ky[t] -mG\\ \Leftrightarrow&y''[t] = -\dfrac{k}{m}y[t] -G \end{align}$$ subject to: $$y[0] = y_c\\y'[0] = 0$$ I'm not really sure how to do so. The equation is for modeling the movement of a falling object that is attached to a spring that is attached to the ceiling, thus gravity ($G$) is involved. appreciate your help: Appreciate your help :)","I want to solve the following differential equation: $y[t]$ : vertical position (height) of the object at time t $y_c$  : height of the ceiling $y_e$  : equilibrium point, the height at which the mass will stop at the end of its movement. $a[t]$ : acceleration at time t $t$    : time $k$    : spring coefficient $m$    : mass of the object $G$    : gravity $$\begin{align} &F = -ky[t] - mG \\ \Leftrightarrow &ma[t] = -ky[t] -mG  \\ \Leftrightarrow&my''[t] = -ky[t] -mG\\ \Leftrightarrow&y''[t] = -\dfrac{k}{m}y[t] -G \end{align}$$ subject to: $$y[0] = y_c\\y'[0] = 0$$ I'm not really sure how to do so. The equation is for modeling the movement of a falling object that is attached to a spring that is attached to the ceiling, thus gravity ($G$) is involved. appreciate your help: Appreciate your help :)",,"['ordinary-differential-equations', 'physics', 'mathematical-physics']"
5,Differential equation involving function defined in terms of an integral,Differential equation involving function defined in terms of an integral,,"Let $$F(y)=\int_0^\infty e^{-2x}\cos(2xy)\, dx$$ for $y\in \mathbb{R}$. Show that $F$ satisfies $F'(y)+2yF(y)=0$. I've shown that I can differentiate under the integral sign, and this gives $F'(y)=-2\int_0^\infty xe^{-2x}\sin(2xy)\,dx$, but how in the world is this equal to $-2yF(y)$? Is there some trick I'm missing here?","Let $$F(y)=\int_0^\infty e^{-2x}\cos(2xy)\, dx$$ for $y\in \mathbb{R}$. Show that $F$ satisfies $F'(y)+2yF(y)=0$. I've shown that I can differentiate under the integral sign, and this gives $F'(y)=-2\int_0^\infty xe^{-2x}\sin(2xy)\,dx$, but how in the world is this equal to $-2yF(y)$? Is there some trick I'm missing here?",,"['real-analysis', 'ordinary-differential-equations', 'integration']"
6,ODE theory. Need someone to jog my memory,ODE theory. Need someone to jog my memory,,"I have two questions regarding the same thing. Let's say I have a homogeneous ODE (is that what they are called?) $ay'' + by' + cy = 0$ The trick in this problem is to multiply both sides by $e^{rt}$ and do some trick to get the homogeneous solution which usually includes strictly exponential or sometimes a mixed with trigonometric functions. This is what most book does, but now that I had forgotten most of this stuff, could someone remind me why we ignore the trivial solution $y = 0$? Let's say the ODE $ay'' + by' + cy = 0$ has the homogeneous solution $y = Ae^{-r_1t} + Be^{r_2t}$ (Let $r_1, r_2 > 0$ and are roots of the characteristic equation). If it happens that $\lim_{x\to \infty} y = 1$. Why can't there be a solution? Is there a theorem that says this? I can't remember it out for the life of me. Thank you for reading","I have two questions regarding the same thing. Let's say I have a homogeneous ODE (is that what they are called?) $ay'' + by' + cy = 0$ The trick in this problem is to multiply both sides by $e^{rt}$ and do some trick to get the homogeneous solution which usually includes strictly exponential or sometimes a mixed with trigonometric functions. This is what most book does, but now that I had forgotten most of this stuff, could someone remind me why we ignore the trivial solution $y = 0$? Let's say the ODE $ay'' + by' + cy = 0$ has the homogeneous solution $y = Ae^{-r_1t} + Be^{r_2t}$ (Let $r_1, r_2 > 0$ and are roots of the characteristic equation). If it happens that $\lim_{x\to \infty} y = 1$. Why can't there be a solution? Is there a theorem that says this? I can't remember it out for the life of me. Thank you for reading",,['ordinary-differential-equations']
7,How to find Singular solutions of differential equation based physical model?,How to find Singular solutions of differential equation based physical model?,,"To get singular solutions, do we always need a guess or experiment? Can we get it from a relation of family of curves of general solution? For example, $(y')^2-xy'+y=0$ has the general solution $y=cx-c^2$. It has a singular solution of $y=x^2/4$, too. If you draw family of curves of general solution (a bunch of straight lines) as well as curve of singular solution (a parabola), you can find parabola is touching general family of curves with a pattern. Can that be a point to get singular solution? In general, is there a way to calculate singular solutions mathematically?","To get singular solutions, do we always need a guess or experiment? Can we get it from a relation of family of curves of general solution? For example, $(y')^2-xy'+y=0$ has the general solution $y=cx-c^2$. It has a singular solution of $y=x^2/4$, too. If you draw family of curves of general solution (a bunch of straight lines) as well as curve of singular solution (a parabola), you can find parabola is touching general family of curves with a pattern. Can that be a point to get singular solution? In general, is there a way to calculate singular solutions mathematically?",,"['calculus', 'ordinary-differential-equations', 'differential-geometry', 'singular-solution']"
8,circles and linear fractional transformations,circles and linear fractional transformations,,"I'm realizing how little (in some respects) I know about circles.  Here's something that emerged out of something I was fiddling with. My question is whether this is ""well known"" in the way that $229\times983=225107$ is ""well known"" (don't publish it unless you're publishing a table); or well known in the sense that every book includes it (for suitable values of ""every""); or well known in the sense that everybody knows it (for at least moderately reasonable values of ""everybody""). I'm looking at the circle $|z|=1$ in $\mathbb{C}$.  Let $$f(z)=\dfrac{-3z+1}{z-3}.\tag{This is $f$.}$$  This of course fixes $\pm 1$ and leaves the circle invariant, and maps $\pm i$ to $\dfrac{-3\pm4i}{5}$.  If we draw a circle through those two images of $\pm i$ meeting the unit circle at a right angle, it is centered at $-5/3$ and has radius $4/3$.  That circle meets the real axis at $-1/3$.  So look at the line $\operatorname{Re}=-1/3$.  Look at the point on that line where $\operatorname{Im} = y$.  Draw the line through that point and the aformentioned center $-5/3$.  That line crosses the circle twice.  It would seem that those two points are $f(z)$ and $f(-\bar z)$, where $z$ and $-\bar z$ are the two points on the unit circle with imaginary part $y$. This gives us a simple geometric picture of how $f$ behaves.  That allows us to use routine Euclidean geometry to show that $\theta\mapsto f(e^{i\theta})$ satisfies the differential equation $$ \left|\dfrac{dg}{d\theta}\right| = \text{constant}\cdot\operatorname{Re}\left(g-\left(-\dfrac 5 3\right)\right) $$ subject to the constraint that the values of $g$ are on the unit circle.  (The equation says the rate at which $g$ moves along the circle is proportional to a certain affine function of the real part.)","I'm realizing how little (in some respects) I know about circles.  Here's something that emerged out of something I was fiddling with. My question is whether this is ""well known"" in the way that $229\times983=225107$ is ""well known"" (don't publish it unless you're publishing a table); or well known in the sense that every book includes it (for suitable values of ""every""); or well known in the sense that everybody knows it (for at least moderately reasonable values of ""everybody""). I'm looking at the circle $|z|=1$ in $\mathbb{C}$.  Let $$f(z)=\dfrac{-3z+1}{z-3}.\tag{This is $f$.}$$  This of course fixes $\pm 1$ and leaves the circle invariant, and maps $\pm i$ to $\dfrac{-3\pm4i}{5}$.  If we draw a circle through those two images of $\pm i$ meeting the unit circle at a right angle, it is centered at $-5/3$ and has radius $4/3$.  That circle meets the real axis at $-1/3$.  So look at the line $\operatorname{Re}=-1/3$.  Look at the point on that line where $\operatorname{Im} = y$.  Draw the line through that point and the aformentioned center $-5/3$.  That line crosses the circle twice.  It would seem that those two points are $f(z)$ and $f(-\bar z)$, where $z$ and $-\bar z$ are the two points on the unit circle with imaginary part $y$. This gives us a simple geometric picture of how $f$ behaves.  That allows us to use routine Euclidean geometry to show that $\theta\mapsto f(e^{i\theta})$ satisfies the differential equation $$ \left|\dfrac{dg}{d\theta}\right| = \text{constant}\cdot\operatorname{Re}\left(g-\left(-\dfrac 5 3\right)\right) $$ subject to the constraint that the values of $g$ are on the unit circle.  (The equation says the rate at which $g$ moves along the circle is proportional to a certain affine function of the real part.)",,"['complex-analysis', 'ordinary-differential-equations', 'circles', 'conformal-geometry', 'linear-fractional-transformation']"
9,Squaring an arbitrary summation?,Squaring an arbitrary summation?,,"I'm trying to find a recurrence relation for the coefficients for the Maclaurin series for $\tan(x)$ by substituting $y=\sum_{k=0}^{\infty}C_{2k+1}x^{2k+1}$ into the differential equation $y'=1+y^2$. This is because $\tan(x)$ is the solution to the initial value problem for the aforementioned DE with the initial condition $y(0)=0$; this is also where the form $\sum_{k=0}^{\infty}C_{2k+1}x^{2k+1}$ comes from (the fact that $\tan(x)$ is an odd function and that $y(0)=0$ which implies $C_0=0$). But I have no clue how to work ""around"" the expression $y^2=\big(\sum_{n=1}^{\infty}C_{2k+1}x^{2k+1}\big)^2$. How can I find a recurrence relation with an infinite squared summation? Any help is appreciated, thank you.","I'm trying to find a recurrence relation for the coefficients for the Maclaurin series for $\tan(x)$ by substituting $y=\sum_{k=0}^{\infty}C_{2k+1}x^{2k+1}$ into the differential equation $y'=1+y^2$. This is because $\tan(x)$ is the solution to the initial value problem for the aforementioned DE with the initial condition $y(0)=0$; this is also where the form $\sum_{k=0}^{\infty}C_{2k+1}x^{2k+1}$ comes from (the fact that $\tan(x)$ is an odd function and that $y(0)=0$ which implies $C_0=0$). But I have no clue how to work ""around"" the expression $y^2=\big(\sum_{n=1}^{\infty}C_{2k+1}x^{2k+1}\big)^2$. How can I find a recurrence relation with an infinite squared summation? Any help is appreciated, thank you.",,"['sequences-and-series', 'ordinary-differential-equations', 'power-series']"
10,Solving an ordinary differential equation: $y'(t)+3y(t)=6t+5$,Solving an ordinary differential equation:,y'(t)+3y(t)=6t+5,"Prove if $y'(t)+3y(t)=6t+5$, $y(0)=3$, then  $y(t)=2e^{-3t}+2t+1$. I have no idea how to finish this problem.","Prove if $y'(t)+3y(t)=6t+5$, $y(0)=3$, then  $y(t)=2e^{-3t}+2t+1$. I have no idea how to finish this problem.",,['ordinary-differential-equations']
11,"`""Variation of Constant""` -method to solve linear DYs?","`""Variation of Constant""` -method to solve linear DYs?",,"My school instructs to use some method called ""variation of constant"" (first page here ) to solve linear DY more in my earlier question here . I think I solved the problem without the method just by the integrating factor -multiplication. Could someone explain why I get the same solution without using the method to the problem in my earlier question here ? In the foreign course book, the method is very fast covered on the page 636. It mixes up the integrating-factor -method and the variation -method making it for me very hard reading, cannot actually understand a word from it. Then in a rush-minute, it mentions some basic rule ""basic rule from analysis"" -- and well I think it has a mistake on page 637 with minus but well perhaps there is something better in English to explain the variation -method. I am not sure whether it is the variation-of-parameters -method or something else, I cannot yet understand why this method is actually needed in my earlier question (I got the solution by integrating-factor -method -- or so I think, I may be wrong. Please, correct me if I am wrong.). Perhaps Related Differential equation with a constant in it Substitution $x=\sinh(\theta)$ and $y=\cosh(\theta)$ to $(1+x^{2})y'-2xy=(1+x^{2})^{2}$?","My school instructs to use some method called ""variation of constant"" (first page here ) to solve linear DY more in my earlier question here . I think I solved the problem without the method just by the integrating factor -multiplication. Could someone explain why I get the same solution without using the method to the problem in my earlier question here ? In the foreign course book, the method is very fast covered on the page 636. It mixes up the integrating-factor -method and the variation -method making it for me very hard reading, cannot actually understand a word from it. Then in a rush-minute, it mentions some basic rule ""basic rule from analysis"" -- and well I think it has a mistake on page 637 with minus but well perhaps there is something better in English to explain the variation -method. I am not sure whether it is the variation-of-parameters -method or something else, I cannot yet understand why this method is actually needed in my earlier question (I got the solution by integrating-factor -method -- or so I think, I may be wrong. Please, correct me if I am wrong.). Perhaps Related Differential equation with a constant in it Substitution $x=\sinh(\theta)$ and $y=\cosh(\theta)$ to $(1+x^{2})y'-2xy=(1+x^{2})^{2}$?",,['ordinary-differential-equations']
12,"Given a function, how to obtain a differential equation?","Given a function, how to obtain a differential equation?",,"I could obtain a differential equation upon eliminating arbitrary constants from this equation $y = e^x(A \cos x + B \sin x)$. Here are the steps. $$ \frac{dy}{dx} = e^x(A \cos x + B \sin x) + e^x(-A \sin x + B \cos x) = y +e^x(-A \sin x + B \cos x)$$ $$ \frac{d^2 y}{dx^2} = \frac{dy}{dx} + e^x(-A \sin x + B \cos x) + e^x(-A \cos x - B \sin x) = \frac{dy}{dx}+\left(\frac{dy}{dx} - y\right) - y ,$$ or $$ \frac{d^2y}{dx^2} - 2\frac{dy}{dx} + 2y = 0 $$ which is the required differential equation. Now how do I obtain a differential equation for this one: $y = cx + c^2$?  I am assuming $c$ is some arbitrary constant. Here are the steps that I've tried.  $$ \frac{dy}{dx} = c$$ $$ \frac{d^2y}{dx^2} = 0 $$ and I am stuck. I am having trouble with this one too: $y = Ae^{3x} + Be^{2x}$ where A and B are constants.Let me edit this.For the above function differentiating w.r.t x first time gives $$ \frac{dy}{dx} = 3Ae^{3x} + 2Be^{2x}$$ Differentiating again w.r.t x gives $$ \frac{d^2y}{dx^2} = 9Ae^{3x} + 4Be^{2x}$$ What next? All Right, I've edited as per request. And also I corrected the pointed mistake. Sorry for the inconvenience.","I could obtain a differential equation upon eliminating arbitrary constants from this equation $y = e^x(A \cos x + B \sin x)$. Here are the steps. $$ \frac{dy}{dx} = e^x(A \cos x + B \sin x) + e^x(-A \sin x + B \cos x) = y +e^x(-A \sin x + B \cos x)$$ $$ \frac{d^2 y}{dx^2} = \frac{dy}{dx} + e^x(-A \sin x + B \cos x) + e^x(-A \cos x - B \sin x) = \frac{dy}{dx}+\left(\frac{dy}{dx} - y\right) - y ,$$ or $$ \frac{d^2y}{dx^2} - 2\frac{dy}{dx} + 2y = 0 $$ which is the required differential equation. Now how do I obtain a differential equation for this one: $y = cx + c^2$?  I am assuming $c$ is some arbitrary constant. Here are the steps that I've tried.  $$ \frac{dy}{dx} = c$$ $$ \frac{d^2y}{dx^2} = 0 $$ and I am stuck. I am having trouble with this one too: $y = Ae^{3x} + Be^{2x}$ where A and B are constants.Let me edit this.For the above function differentiating w.r.t x first time gives $$ \frac{dy}{dx} = 3Ae^{3x} + 2Be^{2x}$$ Differentiating again w.r.t x gives $$ \frac{d^2y}{dx^2} = 9Ae^{3x} + 4Be^{2x}$$ What next? All Right, I've edited as per request. And also I corrected the pointed mistake. Sorry for the inconvenience.",,['ordinary-differential-equations']
13,Prove that an autonomous ODE f(x)=x' has no nonconstant periodic solutions,Prove that an autonomous ODE f(x)=x' has no nonconstant periodic solutions,,Prove that an autonomous ODE $f(x)=x'$ has no nonconstant periodic solutions. I guess I could prove it by contradiction by saying $x(t+T) = x(t)$ implies $x(t) =$ constant.,Prove that an autonomous ODE $f(x)=x'$ has no nonconstant periodic solutions. I guess I could prove it by contradiction by saying $x(t+T) = x(t)$ implies $x(t) =$ constant.,,['ordinary-differential-equations']
14,Nonhomogeneous Linear O.D.E,Nonhomogeneous Linear O.D.E,,"I found this question and I cannot seem to answer it correctly and its kinda bothering. I am not seeing what I am not getting right with this particular problem. I took the same route as the OP and found the individual particular solutions of the RHS and added them together as a linear combination but to my surprise, get something totally different. Can someone look at this and let me know what I may be doing wrong. Original question is linked here: Solving Diff. Eq. $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~             {\textbf{Method of Undetermined Coefficients}}$ ${\bf{SOLUTION:}}$ $y(x)=y_{h}~+~y_{p}$ ${\text{Differential Equation:}}~~~~~~~~~~~~~~~~~$  $y^{(5)}+2y^{(3)}+y'=2x+\sin(x)+\cos(x)$. ${\text{Homogeneous Case:}}~~~~~~~~~~~~~~~~~~~~$  $y^{(5)}+2y^{(3)}+y'=0$. ${\text{Characteristic Polynomial:}}~~~~~~~~~$ $r^5+2r^3+r=0$. ${\text{Solved Roots of polynomial:}}~~~~~~$  $\bigg[\{r\rightarrow 0\},\; \{r\rightarrow -i\},\; \{r\rightarrow -i\},\; \{r\rightarrow i\},\; \{r\rightarrow i\}\bigg]$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ $${\text{General Form of the Homogeneous Solution}}$$ $$ y_{h}(x)=C_{1}e^{r_{1}x}+e^{ax}\Big(C_{2}\cos(bx)+C_{3}\sin(bx)+C_{4}x\cos(bx)+C_{5}x\sin(bx) \Big) $$ $${\text{Homogeneous Solution to the Differential Equation}}$$ $$y_{h}(x)=C_{1}+C_{2}\cos(x)-C_{3}\sin(x)+C_{4}x\cos(x)-C_{5}\sin(x);~~\Big(~\because \sin(-x)=-\sin(x)~\Big).$$ Now we shall seek a particular solution. ${\text{Non-Homogeneous Case:}}~~~~~~~~~~~~~~~~$  $y^{(5)}+2y^{(3)}+y'=2x$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ $$ f(x)=2x $$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$Let, $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \begin{array}{llll} y_{p}(x)=Ax+b \\ y_{p}'(x)=A \\ y_{p}''(x)=0 \\ y_{p}^{(3)}(x)=0 \\ y_{p}^{(4)}(x)=0 \\ y_{p}^{(5)}(x)=0 \end{array}$ Substituting derivatives into differential equation: $(0)+2(0)+(A)=2x$. After equating the undetermined coefficient ${\underline{A}}$ we get: $ \begin{array}{l} A=~0 \end{array} $ Making our particular solution to become, $$ y_{p}(x)=0. $$ (2)$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~y^{(5)}+2y^{(3)}+y'=\sin(x)$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ $$ f(x)=\sin(x) $$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$Let, $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \begin{array}{llll} y_{p}(x)=A\sin(x)+B\cos(x) \\ y_{p}'(x)=A\cos(x)-B\sin(x) \\ y_{p}''(x)=-A\sin(x)-B\cos(x) \\ y_{p}^{(3)}(x)=-A\cos(x)+B\sin(x) \\ y_{p}^{(4)}(x)=A\sin(x)+B\cos(x) \\ y_{p}^{(5)}(x)=A\cos(x)-B\sin(x) \end{array}$ Substituting derivatives into differential equation: $A\cos(x)-B\sin(x)+2\Big(-A\cos(x)+B\sin(x)\Big)+\Big(A\cos(x)-B\sin(x)\Big)=\sin(x)$. After equating the undetermined coefficients ${\underline{A}}$ and ${\underline{B}}$ we get: $ \begin{array}{l} A=~0 \\ 0\cdot B=1~~ ????~~ {\text{Huh}} \end{array} $ Making our particular solution to become, $$ y_{p}(x)=0~~?? $$ I guessed it will be the same situation for the $\cos(x)$ on the RHS when finding the particular solution, though I could be missing an important fact. Thanks.","I found this question and I cannot seem to answer it correctly and its kinda bothering. I am not seeing what I am not getting right with this particular problem. I took the same route as the OP and found the individual particular solutions of the RHS and added them together as a linear combination but to my surprise, get something totally different. Can someone look at this and let me know what I may be doing wrong. Original question is linked here: Solving Diff. Eq. $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~             {\textbf{Method of Undetermined Coefficients}}$ ${\bf{SOLUTION:}}$ $y(x)=y_{h}~+~y_{p}$ ${\text{Differential Equation:}}~~~~~~~~~~~~~~~~~$  $y^{(5)}+2y^{(3)}+y'=2x+\sin(x)+\cos(x)$. ${\text{Homogeneous Case:}}~~~~~~~~~~~~~~~~~~~~$  $y^{(5)}+2y^{(3)}+y'=0$. ${\text{Characteristic Polynomial:}}~~~~~~~~~$ $r^5+2r^3+r=0$. ${\text{Solved Roots of polynomial:}}~~~~~~$  $\bigg[\{r\rightarrow 0\},\; \{r\rightarrow -i\},\; \{r\rightarrow -i\},\; \{r\rightarrow i\},\; \{r\rightarrow i\}\bigg]$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ $${\text{General Form of the Homogeneous Solution}}$$ $$ y_{h}(x)=C_{1}e^{r_{1}x}+e^{ax}\Big(C_{2}\cos(bx)+C_{3}\sin(bx)+C_{4}x\cos(bx)+C_{5}x\sin(bx) \Big) $$ $${\text{Homogeneous Solution to the Differential Equation}}$$ $$y_{h}(x)=C_{1}+C_{2}\cos(x)-C_{3}\sin(x)+C_{4}x\cos(x)-C_{5}\sin(x);~~\Big(~\because \sin(-x)=-\sin(x)~\Big).$$ Now we shall seek a particular solution. ${\text{Non-Homogeneous Case:}}~~~~~~~~~~~~~~~~$  $y^{(5)}+2y^{(3)}+y'=2x$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ $$ f(x)=2x $$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$Let, $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \begin{array}{llll} y_{p}(x)=Ax+b \\ y_{p}'(x)=A \\ y_{p}''(x)=0 \\ y_{p}^{(3)}(x)=0 \\ y_{p}^{(4)}(x)=0 \\ y_{p}^{(5)}(x)=0 \end{array}$ Substituting derivatives into differential equation: $(0)+2(0)+(A)=2x$. After equating the undetermined coefficient ${\underline{A}}$ we get: $ \begin{array}{l} A=~0 \end{array} $ Making our particular solution to become, $$ y_{p}(x)=0. $$ (2)$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~y^{(5)}+2y^{(3)}+y'=\sin(x)$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ $$ f(x)=\sin(x) $$ $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$Let, $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \begin{array}{llll} y_{p}(x)=A\sin(x)+B\cos(x) \\ y_{p}'(x)=A\cos(x)-B\sin(x) \\ y_{p}''(x)=-A\sin(x)-B\cos(x) \\ y_{p}^{(3)}(x)=-A\cos(x)+B\sin(x) \\ y_{p}^{(4)}(x)=A\sin(x)+B\cos(x) \\ y_{p}^{(5)}(x)=A\cos(x)-B\sin(x) \end{array}$ Substituting derivatives into differential equation: $A\cos(x)-B\sin(x)+2\Big(-A\cos(x)+B\sin(x)\Big)+\Big(A\cos(x)-B\sin(x)\Big)=\sin(x)$. After equating the undetermined coefficients ${\underline{A}}$ and ${\underline{B}}$ we get: $ \begin{array}{l} A=~0 \\ 0\cdot B=1~~ ????~~ {\text{Huh}} \end{array} $ Making our particular solution to become, $$ y_{p}(x)=0~~?? $$ I guessed it will be the same situation for the $\cos(x)$ on the RHS when finding the particular solution, though I could be missing an important fact. Thanks.",,['ordinary-differential-equations']
15,system of second order differential equations,system of second order differential equations,,"How to solve the system of two second order differential equations?: $$x''(t)=f(x',x,y',y,t),\qquad y''(t)=g(x',x,y',y,t)$$ Is there any numerical method, algorithm?","How to solve the system of two second order differential equations?: $$x''(t)=f(x',x,y',y,t),\qquad y''(t)=g(x',x,y',y,t)$$ Is there any numerical method, algorithm?",,['ordinary-differential-equations']
16,Particular case of a differential equation,Particular case of a differential equation,,What is the best strategy to solve a differential equation of the following form? $$\frac{d f}{d x}(x) = g(x)[k(x) - f(x)]$$,What is the best strategy to solve a differential equation of the following form? $$\frac{d f}{d x}(x) = g(x)[k(x) - f(x)]$$,,"['integration', 'ordinary-differential-equations']"
17,how to solve this nonlinear partial differential equation?,how to solve this nonlinear partial differential equation?,,"How to solve this nonlinear partial differential equation? $$\displaystyle\frac{\partial^2}{\partial x^2} f(x,t) +b \frac{\partial^2}{\partial x^2} f(x,t) \cdot \frac{\partial^2}{\partial t^2} f(x,t) + a = 0,$$ where $a,b$ are constant.","How to solve this nonlinear partial differential equation? $$\displaystyle\frac{\partial^2}{\partial x^2} f(x,t) +b \frac{\partial^2}{\partial x^2} f(x,t) \cdot \frac{\partial^2}{\partial t^2} f(x,t) + a = 0,$$ where $a,b$ are constant.",,[]
18,ODE book for a computer science researcher (Birkoff/Rota vs Arnold),ODE book for a computer science researcher (Birkoff/Rota vs Arnold),,"I've have been looking for a book on ODEs and have narrowed it down to 2 candidates: Birkoff and Rota's 'ordinary differential equations' or Arnold's 'ordinary differential equations: 3rd edition'. From reading online and skimming the texts, it is my understanding that B&R's book teaches a traditional course on ODEs, full of all the tricks and numerical methods, whereas Arnold's book takes a less traditional geometric approach focusing less on tricks/numerical methods. What I am unsure of is whether having a deep theoretical understanding of ODEs is more important when applying differential equations, or if learning a more traditional tricks/numerical approach is better. Currently I'm leaning more towards the side of Arnold's approach, as I can't help but feel that if differential equations were needed there would exist many programs/packages for numerical procedures making them less important for me to learn. However, other comments on questions such as this make me think going the traditional route would be the safer option. I would appreciate any suggestions as to which approach would be better in my circumstances. edit: To clarify my intentions for reading this book , I will be immediately using it as a prerequisite it to the theoretical book John Lee's Smooth Manifolds after which point I will be reading a book on dynamical systems (which will obviously also use ODE's). However, as mentioned in the question title, my primary interest is within computer science so when choosing these texts I like to consider which may be best applicable (as most texts seem to provide adequate prerequisites for later texts).","I've have been looking for a book on ODEs and have narrowed it down to 2 candidates: Birkoff and Rota's 'ordinary differential equations' or Arnold's 'ordinary differential equations: 3rd edition'. From reading online and skimming the texts, it is my understanding that B&R's book teaches a traditional course on ODEs, full of all the tricks and numerical methods, whereas Arnold's book takes a less traditional geometric approach focusing less on tricks/numerical methods. What I am unsure of is whether having a deep theoretical understanding of ODEs is more important when applying differential equations, or if learning a more traditional tricks/numerical approach is better. Currently I'm leaning more towards the side of Arnold's approach, as I can't help but feel that if differential equations were needed there would exist many programs/packages for numerical procedures making them less important for me to learn. However, other comments on questions such as this make me think going the traditional route would be the safer option. I would appreciate any suggestions as to which approach would be better in my circumstances. edit: To clarify my intentions for reading this book , I will be immediately using it as a prerequisite it to the theoretical book John Lee's Smooth Manifolds after which point I will be reading a book on dynamical systems (which will obviously also use ODE's). However, as mentioned in the question title, my primary interest is within computer science so when choosing these texts I like to consider which may be best applicable (as most texts seem to provide adequate prerequisites for later texts).",,"['ordinary-differential-equations', 'reference-request', 'soft-question', 'computer-science', 'book-recommendation']"
19,"How to solve ODE, $\frac{dv}{dt} = \frac{-(v^2-4900)}{500}$","How to solve ODE,",\frac{dv}{dt} = \frac{-(v^2-4900)}{500},"Solve the following ODE, expressing v in terms of t. It is given that when t = 0, v = 0; $$\frac{dv}{dt} = \frac{-(v^2-4900)}{500}$$ I have tried manipulating the fraction, which resulted in $$t = -500∫\frac{1}{(v-70)(v+70)}dv$$ However, if I try to solve this integral, it would result in $$t =500(\frac{1}{140}\ln \left|v-70\right|-\frac{1}{140}\ln \left|v+70\right|+C)$$ This would make it difficult to isolate v, which is needed as the second part of the question requires me to find $$\lim_{t \to ∞} v$$ Any help would be appreciated","Solve the following ODE, expressing v in terms of t. It is given that when t = 0, v = 0; I have tried manipulating the fraction, which resulted in However, if I try to solve this integral, it would result in This would make it difficult to isolate v, which is needed as the second part of the question requires me to find Any help would be appreciated",\frac{dv}{dt} = \frac{-(v^2-4900)}{500} t = -500∫\frac{1}{(v-70)(v+70)}dv t =500(\frac{1}{140}\ln \left|v-70\right|-\frac{1}{140}\ln \left|v+70\right|+C) \lim_{t \to ∞} v,"['calculus', 'integration', 'ordinary-differential-equations', 'limits']"
20,Power series solution to $x^2y''+y'+y=0$ around $x=0$,Power series solution to  around,x^2y''+y'+y=0 x=0,If an ODE of the form $$y''+p(x)y'+q(x)y=0$$ has $p(x)$ and $q(x)$ that are analytic at $x_0$ then we can suppose a power series solution as such: $$y(x)=\sum_{n\geq 0} a_n(x-x_0)^n$$ If instead they have a first order pole at $x_0$ we have to suppose this other one: $$y(x)=\sum_{n\geq 0} a_n(x-x_0)^{n+r}$$ But what about a power series solution with coefficients that have higher order poles at $x_0$ like in this particular case where there is a $2^{\text{nd}}$ order pole at $x=0$ ?: $$x^2y''+y'+y=0$$ Is there any possible power series solution at $x=0$ ?,If an ODE of the form has and that are analytic at then we can suppose a power series solution as such: If instead they have a first order pole at we have to suppose this other one: But what about a power series solution with coefficients that have higher order poles at like in this particular case where there is a order pole at ?: Is there any possible power series solution at ?,y''+p(x)y'+q(x)y=0 p(x) q(x) x_0 y(x)=\sum_{n\geq 0} a_n(x-x_0)^n x_0 y(x)=\sum_{n\geq 0} a_n(x-x_0)^{n+r} x_0 2^{\text{nd}} x=0 x^2y''+y'+y=0 x=0,"['ordinary-differential-equations', 'power-series', 'frobenius-method']"
21,Showing the uniqueness of a solution to an ordinary differential equation [closed],Showing the uniqueness of a solution to an ordinary differential equation [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question $$ \dot{y}(x)+y(x)=u(x) $$ $$ y_c(x) = ce^{-x} + \int_{0}^{x} u(t) e^{(t-x)} dt $$ Show that if u is periodic with a period of T, then there exists exactly one solution to the differential equation with the period T. Where here $y_c(0) = c$ holds. The use of the second equation is hinted. I am stuck trying to demonstrate the proposed property. Could someone provide a hint on how to proceed? Your help will be greatly appreciated.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question Show that if u is periodic with a period of T, then there exists exactly one solution to the differential equation with the period T. Where here holds. The use of the second equation is hinted. I am stuck trying to demonstrate the proposed property. Could someone provide a hint on how to proceed? Your help will be greatly appreciated.", \dot{y}(x)+y(x)=u(x)   y_c(x) = ce^{-x} + \int_{0}^{x} u(t) e^{(t-x)} dt  y_c(0) = c,"['ordinary-differential-equations', 'periodic-functions']"
22,Stability of the null solution of a differential equation,Stability of the null solution of a differential equation,,"Is there a differential equation: $$\ddot x(t)=f(t, x(t)),\ f\in C^1(\mathbb{R}^2), \forall t\in \mathbb{R}\ f(t,0)=0;$$ that for any solution x(t) the following is true: $$\forall \epsilon>0\ \exists \delta>0\ (|x(0)|<\delta \implies \forall t\ge0\ |x(t)|<\epsilon)\ ?  $$ I guess that there is no such an equation. I have the following intuition. Let's look at the equation thinking that f is a force and x(t) is a coordinate of a particle that moves under that force. If we provide sufficiently enough initial speed $$\dot x(0)$$ to the particle, which is not limited, the particle will reach any high coordinate no matter where it was at the initial moment of time, because force f does not depend on the speed of the particle.","Is there a differential equation: that for any solution x(t) the following is true: I guess that there is no such an equation. I have the following intuition. Let's look at the equation thinking that f is a force and x(t) is a coordinate of a particle that moves under that force. If we provide sufficiently enough initial speed to the particle, which is not limited, the particle will reach any high coordinate no matter where it was at the initial moment of time, because force f does not depend on the speed of the particle.","\ddot x(t)=f(t, x(t)),\ f\in C^1(\mathbb{R}^2), \forall t\in \mathbb{R}\ f(t,0)=0; \forall \epsilon>0\ \exists \delta>0\ (|x(0)|<\delta \implies \forall t\ge0\ |x(t)|<\epsilon)\ ?   \dot x(0)","['ordinary-differential-equations', 'stability-theory']"
23,Differential equation with memory,Differential equation with memory,,"I am trying to integrate this equation: $$ \frac{dz}{dt} = \alpha \cdot (s(t)-z(t-1)) $$ I came up with this equation to model a ""leaky integrator"" system where s(t) are some samples from the environment and z(t) is a ""decision variable"". Thus this system can continuously decide between 2 alternatives based on the sign of z(t). However, I'm not sure how to integrate the equation and solve for z(t), mainly because of the term z(t-1) which Wolfram Alpha seems to struggle with... Any tips?","I am trying to integrate this equation: I came up with this equation to model a ""leaky integrator"" system where s(t) are some samples from the environment and z(t) is a ""decision variable"". Thus this system can continuously decide between 2 alternatives based on the sign of z(t). However, I'm not sure how to integrate the equation and solve for z(t), mainly because of the term z(t-1) which Wolfram Alpha seems to struggle with... Any tips?","
\frac{dz}{dt} = \alpha \cdot (s(t)-z(t-1))
","['ordinary-differential-equations', 'delay-differential-equations']"
24,Uniqueness of initial value problem to ODE,Uniqueness of initial value problem to ODE,,"I was presented with an i.v.p.: $$y'=\frac{1+y^2}{1+x^2},y(0)=1$$ Using separation of variables I obtained $$\arctan(y)=\arctan(x)+c$$ Substituting x=0 and y=1 gives $c=\frac{\pi}{4}$ but if I solve for y and get $$y=\tan(\arctan(x)+c)$$ then plugging in those values reduces to $$1=\tan(c)$$ which has a solution $c=\frac{\pi}{4}+k\pi$ . Which approach is correct? Does this i.v.p. have only one solution or infinitely many of them",I was presented with an i.v.p.: Using separation of variables I obtained Substituting x=0 and y=1 gives but if I solve for y and get then plugging in those values reduces to which has a solution . Which approach is correct? Does this i.v.p. have only one solution or infinitely many of them,"y'=\frac{1+y^2}{1+x^2},y(0)=1 \arctan(y)=\arctan(x)+c c=\frac{\pi}{4} y=\tan(\arctan(x)+c) 1=\tan(c) c=\frac{\pi}{4}+k\pi","['ordinary-differential-equations', 'initial-value-problems']"
25,how to obtain the value of $\frac{dy}{dx}$ at $x=1$ here...,how to obtain the value of  at  here...,\frac{dy}{dx} x=1,Consider the following values of X and Y I know here we are required to find the value of $\frac{dy}{dx}$ using numerical methods.. But I am not getting how to use here... Actaully I am not getting the question also...what $x$ and $y$ values are given here.. Please hint me how to deal with this problem.. Thanks in advance...,Consider the following values of X and Y I know here we are required to find the value of using numerical methods.. But I am not getting how to use here... Actaully I am not getting the question also...what and values are given here.. Please hint me how to deal with this problem.. Thanks in advance...,\frac{dy}{dx} x y,"['ordinary-differential-equations', 'numerical-methods']"
26,how to solve ode: $x''(t)-k^2\cdot x(-t)=0$,how to solve ode:,x''(t)-k^2\cdot x(-t)=0,"In this ordinary differential equation $$x''(t)-k^2\cdot x(-t)=0$$ both $x(t)$ and $x(-t)$ are included. I wonder whether this still is an ode and how to solve it. I tried to use this property: $$x(t)=x_{odd}(t)+x_{even}(t)=\frac{x(t)-x(-t)}{2}+\frac{x(t)+x(-t)}{2}$$ to substitute $x(-t)$ but failed to get rid of "" $-t$ "". Now I don't know how to continue the process. Any suggestions are appreciated. Thanks.","In this ordinary differential equation both and are included. I wonder whether this still is an ode and how to solve it. I tried to use this property: to substitute but failed to get rid of "" "". Now I don't know how to continue the process. Any suggestions are appreciated. Thanks.",x''(t)-k^2\cdot x(-t)=0 x(t) x(-t) x(t)=x_{odd}(t)+x_{even}(t)=\frac{x(t)-x(-t)}{2}+\frac{x(t)+x(-t)}{2} x(-t) -t,['ordinary-differential-equations']
27,A first-order linear differential equation,A first-order linear differential equation,,"I have a first-order linear ODE as follows: $\frac{dx}{dt}=-\alpha(x-x_0)-\beta(t)$ , and I am trying at a solution of this via integrating the factor method. Here's a try: $\frac{dx}{dt}=-\alpha x+\underbrace{\alpha x_0-\beta(t)}_{f(t)}$ Is it OK to assume the braced term as $f(t)$ if $x_0$ is constant? Thus: $\frac{dx}{dt}=-\alpha x+f(t)$ Next, the integrating factor is: $e^{\int\alpha dt}=e^{\alpha t}$ (only if $\alpha$ is a constant?) Also, can we take a constant as an I.F. , like $\alpha$ in this case? This can then lead to $\frac{d}{dt}\left[x e^{\alpha t} \right]=(\alpha x_0-\beta(t))e^{\alpha t}$ leading to $x(t)=e^{-\alpha t}\left[\int\limits_0^t (\alpha x_0-\beta(t'))e^{\alpha t'}dt' \right]$ Finally, can I write the above as follows? $x(t)=\int\limits_0^t e^{-\alpha(t-t')}\left[\alpha x_0-\beta(t') \right]dt'$ because the integration measure is over $t'$ that runs from $0$ to $t$ ?","I have a first-order linear ODE as follows: , and I am trying at a solution of this via integrating the factor method. Here's a try: Is it OK to assume the braced term as if is constant? Thus: Next, the integrating factor is: (only if is a constant?) Also, can we take a constant as an I.F. , like in this case? This can then lead to leading to Finally, can I write the above as follows? because the integration measure is over that runs from to ?",\frac{dx}{dt}=-\alpha(x-x_0)-\beta(t) \frac{dx}{dt}=-\alpha x+\underbrace{\alpha x_0-\beta(t)}_{f(t)} f(t) x_0 \frac{dx}{dt}=-\alpha x+f(t) e^{\int\alpha dt}=e^{\alpha t} \alpha \alpha \frac{d}{dt}\left[x e^{\alpha t} \right]=(\alpha x_0-\beta(t))e^{\alpha t} x(t)=e^{-\alpha t}\left[\int\limits_0^t (\alpha x_0-\beta(t'))e^{\alpha t'}dt' \right] x(t)=\int\limits_0^t e^{-\alpha(t-t')}\left[\alpha x_0-\beta(t') \right]dt' t' 0 t,['ordinary-differential-equations']
28,Use series method to obtain a general solution of the Cauchy-Euler Equation $x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0.$,Use series method to obtain a general solution of the Cauchy-Euler Equation,x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0.,"Use series method to obtain a general  solution of the Cauchy-Euler Equation $$x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0.$$ To be very honest, I didn't understand the language of the question for it says, ""series method"". I assumed that it intended to say, ""Find a power series solution of the Cauchy-Euler Equation $$x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0.$$ "" But this is again ambiguous, for ut specifies, no point(s) around which I am supposed to find a power series solution. I assumed that, they intended to find tge power series solution about the regular singular point $x=0.$ As, $x=0$ was a regular singular point, I tried applying the method of Frobenius to find a power series solution about $0$ . I proceeded as: Let $y=\sum_{n=0}^{\infty}c_nx^{n+r}$ be a solution to the equation where, $r$ is a constant that nay be determined with the assumption that $c_0\neq 0.$ This means, $y'=\sum_{n=0}^{\infty}(n+r)c_nx^{n+r-1}$ and $y''=\sum_{n=0}^{\infty}(n+r)(n+r-1)c_nx^{n+r-2}.$ Substituting these in the original equation we find, $$\sum_{n=0}^{\infty}\big [(n+r)(n+r-1)-(n+r)+\frac 34\big ]c_nx^{n+r}=0.$$ Hence, we obtain the recurrence, relation $ [(n+r)(n+r-1)-(n+r)+\frac 34\big ]c_n=0$ for all $n\geq 0.$ If $n=0$ then, $(4r^2-8r+3)c_0=0$ and as, $c_0\neq 0$ so, $4r^2-8r+3=0$ is actually the indicial equation. We find, $r_1=\frac 32,r_2=\frac 12.$ Now, if $r=r_1$ then, from the recurrence relation we have(under the value of $r=r_1$ ) $$c_n(n+n^2)=0$$ for all $n\geq 1.$ But here is the problem, as this means $c_n=0,\forall n\geq 1.$ I dont know, where I went wrong. Any help will be greatly appreciated. I feel that the problem is not in work, really. The thing is, the particular integral of this ODE will simply contain just two terms, but then what is meant by the series method?","Use series method to obtain a general  solution of the Cauchy-Euler Equation To be very honest, I didn't understand the language of the question for it says, ""series method"". I assumed that it intended to say, ""Find a power series solution of the Cauchy-Euler Equation "" But this is again ambiguous, for ut specifies, no point(s) around which I am supposed to find a power series solution. I assumed that, they intended to find tge power series solution about the regular singular point As, was a regular singular point, I tried applying the method of Frobenius to find a power series solution about . I proceeded as: Let be a solution to the equation where, is a constant that nay be determined with the assumption that This means, and Substituting these in the original equation we find, Hence, we obtain the recurrence, relation for all If then, and as, so, is actually the indicial equation. We find, Now, if then, from the recurrence relation we have(under the value of ) for all But here is the problem, as this means I dont know, where I went wrong. Any help will be greatly appreciated. I feel that the problem is not in work, really. The thing is, the particular integral of this ODE will simply contain just two terms, but then what is meant by the series method?","x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0. x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0. x=0. x=0 0 y=\sum_{n=0}^{\infty}c_nx^{n+r} r c_0\neq 0. y'=\sum_{n=0}^{\infty}(n+r)c_nx^{n+r-1} y''=\sum_{n=0}^{\infty}(n+r)(n+r-1)c_nx^{n+r-2}. \sum_{n=0}^{\infty}\big [(n+r)(n+r-1)-(n+r)+\frac 34\big ]c_nx^{n+r}=0.  [(n+r)(n+r-1)-(n+r)+\frac 34\big ]c_n=0 n\geq 0. n=0 (4r^2-8r+3)c_0=0 c_0\neq 0 4r^2-8r+3=0 r_1=\frac 32,r_2=\frac 12. r=r_1 r=r_1 c_n(n+n^2)=0 n\geq 1. c_n=0,\forall n\geq 1.",['ordinary-differential-equations']
29,Hint for showing that the equilibrium of a nonlinear system is a center,Hint for showing that the equilibrium of a nonlinear system is a center,,"Consider the second-order nonlinear dynamical system \begin{align*} \dot{x}&=x^3-y\\ \dot{y}&=x-x^2y \end{align*} The (0,0) equilibrium is obviously a center but I cannot find a way to prove this. I tried using some reversibility argument but this does not seem to work. Certainly the invariance of the equations under the transformation $\bar{x}=-x$ , $\bar{y}=-y$ , $\bar{t}=t$ is obvious but is this sufficient? EDIT: My original claim (which was based on the streamplot diagram) was wrong. As pointed by @Artem the equilibrium is an unstable focus .","Consider the second-order nonlinear dynamical system The (0,0) equilibrium is obviously a center but I cannot find a way to prove this. I tried using some reversibility argument but this does not seem to work. Certainly the invariance of the equations under the transformation , , is obvious but is this sufficient? EDIT: My original claim (which was based on the streamplot diagram) was wrong. As pointed by @Artem the equilibrium is an unstable focus .","\begin{align*}
\dot{x}&=x^3-y\\
\dot{y}&=x-x^2y
\end{align*} \bar{x}=-x \bar{y}=-y \bar{t}=t","['ordinary-differential-equations', 'dynamical-systems', 'stability-theory']"
30,"Confusion with the equation $\int_0^{\infty}f(x)\sin(\alpha x)\,dx=-\int_0^{\infty}xf(x)\cos(\alpha x)\,dx$",Confusion with the equation,"\int_0^{\infty}f(x)\sin(\alpha x)\,dx=-\int_0^{\infty}xf(x)\cos(\alpha x)\,dx","I’m given the question: If: $\displaystyle \int_0^{\infty}f(x)\sin(\alpha x)\,dx=-\displaystyle \int_0^{\infty}xf(x)\cos(\alpha x)\,dx$ and $f(1)=1$ , then find $f$ . My attempt: To me, if we add $\int_0^{\infty}xf(x)\cos(\alpha x)\,dx$ to both sides then we get: $\displaystyle \int_0^{\infty}(f(x)\sin(\alpha x)+xf(x)\cos(\alpha x ))\,dx=0$ Now the above integral is the Fourier integral of $0$ . Therefore: $f(x)=\dfrac{1}{\pi}\displaystyle \int_{-\infty}^{\infty}0\cdot\sin(\alpha  x)\,d\alpha=0$ which is not in line with original assumptions of the question. Where am I wrong?","I’m given the question: If: and , then find . My attempt: To me, if we add to both sides then we get: Now the above integral is the Fourier integral of . Therefore: which is not in line with original assumptions of the question. Where am I wrong?","\displaystyle \int_0^{\infty}f(x)\sin(\alpha x)\,dx=-\displaystyle \int_0^{\infty}xf(x)\cos(\alpha x)\,dx f(1)=1 f \int_0^{\infty}xf(x)\cos(\alpha x)\,dx \displaystyle \int_0^{\infty}(f(x)\sin(\alpha x)+xf(x)\cos(\alpha x ))\,dx=0 0 f(x)=\dfrac{1}{\pi}\displaystyle \int_{-\infty}^{\infty}0\cdot\sin(\alpha  x)\,d\alpha=0","['real-analysis', 'integration', 'ordinary-differential-equations', 'fourier-transform']"
31,Nonhomogeneous 2nd Order Differential Equation with variable coefficients,Nonhomogeneous 2nd Order Differential Equation with variable coefficients,,"I'm having trouble with solving this nonhomogeneous 2nd ODE $$x^{2}\frac{\mathrm{d}^{2}y }{\mathrm{d} x^{2}}+x\frac{\mathrm{d}y }{\mathrm{d} x}-y=x$$ In order to get the general solution I rewrote the equation like this $$x^{2}\frac{\mathrm{d}^{2}y }{\mathrm{d} x^{2}}+x\frac{\mathrm{d}y }{\mathrm{d} x}-y=0$$ Then I assumed that $y=x^{\alpha}$ , where $\alpha$ is a constant. When I plugged this into the 2nd equation I got $$\alpha (\alpha -1)x^{2}x^{\alpha-2}+\alpha x^{\alpha-1}x-x^{\alpha}=0$$ The general solution was $$y=C_{1}x+C_{2}x^{-1}$$ and then I got confused as to what to do next. Any suggestions?","I'm having trouble with solving this nonhomogeneous 2nd ODE In order to get the general solution I rewrote the equation like this Then I assumed that , where is a constant. When I plugged this into the 2nd equation I got The general solution was and then I got confused as to what to do next. Any suggestions?",x^{2}\frac{\mathrm{d}^{2}y }{\mathrm{d} x^{2}}+x\frac{\mathrm{d}y }{\mathrm{d} x}-y=x x^{2}\frac{\mathrm{d}^{2}y }{\mathrm{d} x^{2}}+x\frac{\mathrm{d}y }{\mathrm{d} x}-y=0 y=x^{\alpha} \alpha \alpha (\alpha -1)x^{2}x^{\alpha-2}+\alpha x^{\alpha-1}x-x^{\alpha}=0 y=C_{1}x+C_{2}x^{-1},['ordinary-differential-equations']
32,Is the relation $e^{At} = Pe^{Jt} P^{-1}$ valid for a Jordan form?,Is the relation  valid for a Jordan form?,e^{At} = Pe^{Jt} P^{-1},"Imagine you have a system of equations of the form: $$ \mathbf {\dot{x}} = A \mathbf{x} $$ And $A$ cannot be diagonalized, so you find it's Jordan form: $$ A = P J P^{-1} \\ \; \\ J  = \begin{pmatrix}  \lambda_1 & 1 & 0 & \dots  & 0 \\ 0 & \lambda_2 & 1 & \dots & 0 \\ 0 & 0 & \lambda_3 & \dots & 0 \\ \vdots &\vdots & \vdots &  \ddots & \vdots \\ 0 & 0 & 0&  \dots & \lambda_n \\ \end{pmatrix} $$ The solution of the differential equation is: $$ \mathbf{x}(t) = e^{At}\mathbf{x_0} $$ My question is: If the matrix $A$ was diagonalizable, then: $$ e^{At} = P \begin{pmatrix}  e^{\lambda_1t} & 0 & 0 & \dots  & 0 \\ 0 & e^{\lambda_2 t} & 0 & \dots & 0 \\ 0 & 0 & e^{\lambda_3 t} & \dots & 0 \\ \vdots &\vdots & \vdots &  \ddots & \vdots \\ 0 & 0 & 0&  \dots & e^{\lambda_n t} \\ \end{pmatrix} P^{-1} $$ because: $$ e^{At} = \sum_{n = 0}^{\infty} \frac{(At)^n}{n!} = \sum_{n = 0}^{\infty} \frac{(PDP^{-1})^n}{n!}t^n = P \left ( \sum_{n = 0}^{\infty} \frac{D^n}{n!}t^n \right )P^{-1} = P e^{Dt} P^{-1} $$ but, is this last relation: $$ \sum_{n = 0}^{\infty} \frac{(PJP^{-1})^n}{n!}t^n = P \left ( \sum_{n = 0}^{\infty} \frac{J^n}{n!}t^n \right )P^{-1} = P e^{Jt} P^{-1} $$ still apply for a Jordan form? I know that when you sum all the terms involved in $e^{Jt}$ , polynomials of $t$ will appear apart from the exponential related to the eigenvalue: $e^{\lambda_k t}$ . But, is this still valid?","Imagine you have a system of equations of the form: And cannot be diagonalized, so you find it's Jordan form: The solution of the differential equation is: My question is: If the matrix was diagonalizable, then: because: but, is this last relation: still apply for a Jordan form? I know that when you sum all the terms involved in , polynomials of will appear apart from the exponential related to the eigenvalue: . But, is this still valid?","
\mathbf {\dot{x}} = A \mathbf{x}
 A 
A = P J P^{-1} \\ \; \\
J  = \begin{pmatrix} 
\lambda_1 & 1 & 0 & \dots  & 0 \\
0 & \lambda_2 & 1 & \dots & 0 \\
0 & 0 & \lambda_3 & \dots & 0 \\
\vdots &\vdots & \vdots &  \ddots & \vdots \\
0 & 0 & 0&  \dots & \lambda_n \\
\end{pmatrix}
 
\mathbf{x}(t) = e^{At}\mathbf{x_0}
 A 
e^{At} = P \begin{pmatrix} 
e^{\lambda_1t} & 0 & 0 & \dots  & 0 \\
0 & e^{\lambda_2 t} & 0 & \dots & 0 \\
0 & 0 & e^{\lambda_3 t} & \dots & 0 \\
\vdots &\vdots & \vdots &  \ddots & \vdots \\
0 & 0 & 0&  \dots & e^{\lambda_n t} \\
\end{pmatrix} P^{-1}
 
e^{At} = \sum_{n = 0}^{\infty} \frac{(At)^n}{n!} = \sum_{n = 0}^{\infty} \frac{(PDP^{-1})^n}{n!}t^n = P \left ( \sum_{n = 0}^{\infty} \frac{D^n}{n!}t^n \right )P^{-1} = P e^{Dt} P^{-1}
 
\sum_{n = 0}^{\infty} \frac{(PJP^{-1})^n}{n!}t^n = P \left ( \sum_{n = 0}^{\infty} \frac{J^n}{n!}t^n \right )P^{-1} = P e^{Jt} P^{-1}
 e^{Jt} t e^{\lambda_k t}","['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
33,Exponential stability of a time varying system,Exponential stability of a time varying system,,"I want to show that the system below is exponentially stable and I want to estimate its region of attraction \begin{align} \dot x_1 &= -x_1+x_2+(x_1^2+x_2^2)\sin(t) \\ \dot x_2 &= -x_1-x_2+(x_1^2+x_2^2)\cos(t) \end{align} I tried using a Lyapunov function $V(x_1,x_2)=\frac 12 (x_1^2+x_2^2)$ and I have found \begin{align} \dot V(x_1,x_2) &= -x_1^2+x_1x_2+x_1(x_1^2+x_2^2)\sin(t)-x_2x_1-x_2^2+x_2(x_1^2+x_2^2)\cos(t) \\ &\le -x_1^2-x_2^2+x_1(x_1^2+x_2^2)+x_2(x_1^2+x_2^2) \end{align} And when $x_1^2+x_2^2 \le 1$ we have \begin{align} \dot V &\le -x_1^2-x_2^2+x_1+x_2 \end{align} so that $\dot V \le 0$ for $\lvert x_1 \rvert \ge 1$ and $\lvert x_2 \rvert \ge 1$ . Hence, we can only have the 2 conditions when $\dot V = 0$ and $x_1^2 + x_2^2 = 1$ . I suppose the region of attraction should be the unit circle. I'm not sure if I need to find a Lyapunov function with a negative definite derivative to ensure exponential stability, as this condition is more closely related to asymptotic stability, isn't it? EDIT: The equilibrium is $(0,0)$ , I linearised the system and got a Jacobian with a negative real part for the eigenvalues. Therefore, I can use the Lyapunov equation $A^TP+PA=-Q$ with A equal to the Jacobian and $Q=I$ and get a matrix $P$ so that $V=x^TPx$ and $\dot V = -x^TQx$ . However, I'm not sure if this gives the exponential stability?","I want to show that the system below is exponentially stable and I want to estimate its region of attraction I tried using a Lyapunov function and I have found And when we have so that for and . Hence, we can only have the 2 conditions when and . I suppose the region of attraction should be the unit circle. I'm not sure if I need to find a Lyapunov function with a negative definite derivative to ensure exponential stability, as this condition is more closely related to asymptotic stability, isn't it? EDIT: The equilibrium is , I linearised the system and got a Jacobian with a negative real part for the eigenvalues. Therefore, I can use the Lyapunov equation with A equal to the Jacobian and and get a matrix so that and . However, I'm not sure if this gives the exponential stability?","\begin{align}
\dot x_1 &= -x_1+x_2+(x_1^2+x_2^2)\sin(t) \\
\dot x_2 &= -x_1-x_2+(x_1^2+x_2^2)\cos(t)
\end{align} V(x_1,x_2)=\frac 12 (x_1^2+x_2^2) \begin{align}
\dot V(x_1,x_2) &= -x_1^2+x_1x_2+x_1(x_1^2+x_2^2)\sin(t)-x_2x_1-x_2^2+x_2(x_1^2+x_2^2)\cos(t) \\
&\le -x_1^2-x_2^2+x_1(x_1^2+x_2^2)+x_2(x_1^2+x_2^2)
\end{align} x_1^2+x_2^2 \le 1 \begin{align}
\dot V &\le -x_1^2-x_2^2+x_1+x_2
\end{align} \dot V \le 0 \lvert x_1 \rvert \ge 1 \lvert x_2 \rvert \ge 1 \dot V = 0 x_1^2 + x_2^2 = 1 (0,0) A^TP+PA=-Q Q=I P V=x^TPx \dot V = -x^TQx","['ordinary-differential-equations', 'nonlinear-system', 'stability-theory', 'lyapunov-functions']"
34,Are $x_1=t\cos(t)$ and $x_2=t\sin(t)$ are solutions to the ODE $x''=q(t)x$?,Are  and  are solutions to the ODE ?,x_1=t\cos(t) x_2=t\sin(t) x''=q(t)x,"I have an ODE of the form $x''=q(t)x$ and need to determine if there exists any continuous $q(t)$ so that $x_1=t\cos(t)$ and $x_2=t\sin(t)$ are linear independednt solutions of this ODE in $(-\pi,\pi)$ . My attempt : I calculated $x_1''(t) = -2\sin(t) - t\cos(t)$ and $x_2''(t) = 2\cos(t) - t\sin(t)$ , so once $q(t) = \frac{-2\sin(t) - t\cos(t)}{t\cos(t)}$ and in the second case $q(t) = \frac{2\cos(t) - t\sin(t)}{t\sin(t)}$ . I don't see how this two can be the same, so my answer would be no. I wanted to check with you if that's correct. Also what got me confused is that i read that linear combinations of solutions of ODE-s are also solutions, and I have proved that $x_1=\cos(t)$ and $x_2=\sin(t)$ are actually solutions of our ODE, so I don't know if that implies anything for our new possible solutions now. Would be glad for some help.","I have an ODE of the form and need to determine if there exists any continuous so that and are linear independednt solutions of this ODE in . My attempt : I calculated and , so once and in the second case . I don't see how this two can be the same, so my answer would be no. I wanted to check with you if that's correct. Also what got me confused is that i read that linear combinations of solutions of ODE-s are also solutions, and I have proved that and are actually solutions of our ODE, so I don't know if that implies anything for our new possible solutions now. Would be glad for some help.","x''=q(t)x q(t) x_1=t\cos(t) x_2=t\sin(t) (-\pi,\pi) x_1''(t) = -2\sin(t) - t\cos(t) x_2''(t) = 2\cos(t) - t\sin(t) q(t) = \frac{-2\sin(t) - t\cos(t)}{t\cos(t)} q(t) = \frac{2\cos(t) - t\sin(t)}{t\sin(t)} x_1=\cos(t) x_2=\sin(t)","['ordinary-differential-equations', 'trigonometry', 'solution-verification']"
35,Is the solution to this 1st order matrix ODE uniquely determined?,Is the solution to this 1st order matrix ODE uniquely determined?,,"I have a matrix function $e(t)$ (i.e. for each $t$ , $e(t)$ is a matrix) and the ODE $$ e'(t)=e(t)^{-T}g(t) $$ where $-T$ denotes the inverse transpose, and $g(t)$ is some fixed matrix function. My question is if given the initial condition $e(0)$ , whether $e(t)$ is determined uniquely? The answer is probably yes -- usually one could go so far as to integrate each side using the initial condition and then iterate the expression to obtain an 'explicit' expression for $e(t)$ (the path ordered exponential for example), but I am abit put-off by the appearance of the inverse $e^{-1}$ , hence my slight apprehension.","I have a matrix function (i.e. for each , is a matrix) and the ODE where denotes the inverse transpose, and is some fixed matrix function. My question is if given the initial condition , whether is determined uniquely? The answer is probably yes -- usually one could go so far as to integrate each side using the initial condition and then iterate the expression to obtain an 'explicit' expression for (the path ordered exponential for example), but I am abit put-off by the appearance of the inverse , hence my slight apprehension.","e(t) t e(t) 
e'(t)=e(t)^{-T}g(t)
 -T g(t) e(0) e(t) e(t) e^{-1}","['matrices', 'ordinary-differential-equations', 'matrix-equations']"
36,Solution of non-homogeneous second-order differential equation with time dependent coefficients [closed],Solution of non-homogeneous second-order differential equation with time dependent coefficients [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I am having difficulty understanding how to solve this differential equation: $$ \ddot x+\frac{\dot a(t)}{a(t)}\dot x+b^{2}(t)x=-\frac{1}{a(t)}k$$ where $a(t)$ and $b(t)$ are time-dependent coefficients and $k$ is a constant. Could you give me some advice? I think I have to use Wronskian but I can't solve it anyway. I hope I have expressed myself clearly and thank you in advance for your help.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I am having difficulty understanding how to solve this differential equation: where and are time-dependent coefficients and is a constant. Could you give me some advice? I think I have to use Wronskian but I can't solve it anyway. I hope I have expressed myself clearly and thank you in advance for your help.", \ddot x+\frac{\dot a(t)}{a(t)}\dot x+b^{2}(t)x=-\frac{1}{a(t)}k a(t) b(t) k,['ordinary-differential-equations']
37,Ordinary DEQ - Frobenius Method,Ordinary DEQ - Frobenius Method,,"I have a DEQ: $$y''-\frac{6}{x}y'+\frac{12+x^2}{x^2}y=0$$ Then in series form: $$\sum_{m=0}^{\infty}(m+r)(m+r-1)a_mx^{m+r-2}-6\sum_{m=0}^{\infty}(m+r)a_mx^{m+r-2}+12\sum_{m=0}^{\infty}a_mx^{m+r-2}+\sum_{m=0}^{\infty}a_mx^{m+r}$$ The lowerst power is obviously $m+r-2$ , and we get the indicial equation $r(r-1)-6r=0 \rightarrow r_1=0, r_1=7$ Next, we equate the powers, since the first 3 are the same, we change the index on the 4th series: $$s_4=\sum_{m=2}^{\infty}a_{m-2}x^{m+r-2}$$ Rearranging the series: $$((m+r)(m+r-1)-6(m+r)+12)a_m+a_{m-2}=0$$ Let $r=0$ , i get: $$a_m=-\frac{a_{m-2}}{m(m-7)+12}$$ Then $$a_2=-\frac{a_0}{30}, a_4=-\frac{a_2}{56}=-\frac{a_0}{1680}.....$$ now that just doesn't seem very likely,,i triple-checked my solution, and don't know where i might have made a mistake, can someone have a look?","I have a DEQ: Then in series form: The lowerst power is obviously , and we get the indicial equation Next, we equate the powers, since the first 3 are the same, we change the index on the 4th series: Rearranging the series: Let , i get: Then now that just doesn't seem very likely,,i triple-checked my solution, and don't know where i might have made a mistake, can someone have a look?","y''-\frac{6}{x}y'+\frac{12+x^2}{x^2}y=0 \sum_{m=0}^{\infty}(m+r)(m+r-1)a_mx^{m+r-2}-6\sum_{m=0}^{\infty}(m+r)a_mx^{m+r-2}+12\sum_{m=0}^{\infty}a_mx^{m+r-2}+\sum_{m=0}^{\infty}a_mx^{m+r} m+r-2 r(r-1)-6r=0 \rightarrow r_1=0, r_1=7 s_4=\sum_{m=2}^{\infty}a_{m-2}x^{m+r-2} ((m+r)(m+r-1)-6(m+r)+12)a_m+a_{m-2}=0 r=0 a_m=-\frac{a_{m-2}}{m(m-7)+12} a_2=-\frac{a_0}{30}, a_4=-\frac{a_2}{56}=-\frac{a_0}{1680}.....","['ordinary-differential-equations', 'solution-verification', 'frobenius-method']"
38,How to find the particular solution to this second degree differential equation $y'' - 6y' + 9y = 2xe^{2x}$?,How to find the particular solution to this second degree differential equation ?,y'' - 6y' + 9y = 2xe^{2x},I have this solution: $$y'' - 6y' + 9y = 2xe^{2x}$$ The general solution to it is $$y(x) = C_1e^{3x} + C_2xe^{3x}$$ But I cannot figure out how to find the particular solution. This is what I did. I imposed this as particular solution: $$y_0(x) = e^{2x}.(ax+b)$$ and now I have to find $a$ and $b$ . So I take the first and second derivative of $y_0(x)$ and I replace it in original equation and now what I have is $$x(a-2)+b-2a=0$$ and even now I can not find the solution to $a$ and $b$ . Could someone kindly help me find the solution to this problem?,I have this solution: The general solution to it is But I cannot figure out how to find the particular solution. This is what I did. I imposed this as particular solution: and now I have to find and . So I take the first and second derivative of and I replace it in original equation and now what I have is and even now I can not find the solution to and . Could someone kindly help me find the solution to this problem?,y'' - 6y' + 9y = 2xe^{2x} y(x) = C_1e^{3x} + C_2xe^{3x} y_0(x) = e^{2x}.(ax+b) a b y_0(x) x(a-2)+b-2a=0 a b,"['ordinary-differential-equations', 'derivatives']"
39,Solution to Kepler ODE,Solution to Kepler ODE,,"I'm working on Problem 8.20 from Teschl's ODEs book which asks to solve the ODE $$\frac{1}{r^2} \frac{dr}{d\varphi} = \sqrt{\frac{2 (H - \frac{1}{r})}{J_0^2} - \frac{1}{r^2}}$$ (where $H, J_0$ are constant with respect to $\varphi$ ). The hint is to substitute $\rho = r^{-1}$ and I did that and got $$\frac{d\rho}{d\varphi} = - \sqrt{\frac{2 (H - \rho)}{J_0^2} - \rho^2}$$ This is separable, but I'm stuck trying to integrate the reciprocal of the righthand side (I tried a trigonometric substitution and it didn't seem to help). The claimed answer is $$r (\varphi) = \frac{J_0^2}{1 - \sqrt{1 + H J_0^2} \cos (\varphi - \varphi_0)}$$","I'm working on Problem 8.20 from Teschl's ODEs book which asks to solve the ODE (where are constant with respect to ). The hint is to substitute and I did that and got This is separable, but I'm stuck trying to integrate the reciprocal of the righthand side (I tried a trigonometric substitution and it didn't seem to help). The claimed answer is","\frac{1}{r^2} \frac{dr}{d\varphi} = \sqrt{\frac{2 (H - \frac{1}{r})}{J_0^2} - \frac{1}{r^2}} H, J_0 \varphi \rho = r^{-1} \frac{d\rho}{d\varphi} = - \sqrt{\frac{2 (H - \rho)}{J_0^2} - \rho^2} r (\varphi) = \frac{J_0^2}{1 - \sqrt{1 + H J_0^2} \cos (\varphi - \varphi_0)}","['calculus', 'ordinary-differential-equations']"
40,Polar Coordinates ODE,Polar Coordinates ODE,,"I have the following ODE: $$\dot x=y+ax(x^2+y^2), \dot y=-x+ay(x^2+y^2)$$ and I want to prove that the system is equivalent with the following $$\dot r=ar^3, \dot \theta=-1$$ I start by taking polar coordinates but when I make the change I get $$ r \dot r=ax^4+2ax^2y^2+ay^4$$ Any suggestions?",I have the following ODE: and I want to prove that the system is equivalent with the following I start by taking polar coordinates but when I make the change I get Any suggestions?,"\dot x=y+ax(x^2+y^2), \dot y=-x+ay(x^2+y^2) \dot r=ar^3, \dot \theta=-1  r \dot r=ax^4+2ax^2y^2+ay^4","['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'polar-coordinates']"
41,Relation between speed and potential,Relation between speed and potential,,"I'm studying newtonian dynamical systems and they can be described by the differential equation $$1)\space  m\ddot{x} = F(x)$$ supposing $F$ sufficiently regular we could define the potential $V$ as its primitive so $$2)\space F(x) = -{dV\over dx}$$ We can also define the energy of the system $E$ as $$3)\space E = \frac{1}{2}mv^2 + V(x)$$ as a results we have $$4) \space v = \sqrt{\frac{2}{m}(E-V)}$$ What confuses me is that $(1)$ could also be described by the pair of equations $$\dot{x} = v,\space \space \dot{v} = \frac{F(x)}{m}$$ for the sake of simplicity take $m=1$ and by the second equation above I think that $$v(x) = -\int_{x_0}^{x}F(\zeta)d\zeta = V(x_0)-V(x)$$ So how $4)$ and the last equation could be the same thing? I was thinking about that and the conclusion I came is that I could not write the last relation, can't I? Every clarification is welcome, because in this moment I have a lot of confusion in my head. Thanks!","I'm studying newtonian dynamical systems and they can be described by the differential equation supposing sufficiently regular we could define the potential as its primitive so We can also define the energy of the system as as a results we have What confuses me is that could also be described by the pair of equations for the sake of simplicity take and by the second equation above I think that So how and the last equation could be the same thing? I was thinking about that and the conclusion I came is that I could not write the last relation, can't I? Every clarification is welcome, because in this moment I have a lot of confusion in my head. Thanks!","1)\space  m\ddot{x} = F(x) F V 2)\space F(x) = -{dV\over dx} E 3)\space E = \frac{1}{2}mv^2 + V(x) 4) \space v = \sqrt{\frac{2}{m}(E-V)} (1) \dot{x} = v,\space \space \dot{v} = \frac{F(x)}{m} m=1 v(x) = -\int_{x_0}^{x}F(\zeta)d\zeta = V(x_0)-V(x) 4)","['ordinary-differential-equations', 'dynamical-systems']"
42,How to numerically calculate a eigenvalue problem?,How to numerically calculate a eigenvalue problem?,,"Suppose we have a eigenvalue problem: \begin{array}{c} y^{\prime \prime}+\lambda y=0,0<x<l \\ y(0)=0,  y(l)=0 \end{array} and we know the eigenvalue is $\lambda =\frac{n^{2}\pi ^{2}  }{l^{2}}  $ , and eigenfunction $y\left ( x \right ) =C\sin \frac{n\pi x}{l}  $ . But if I want to calculate the eigenvalue numerically, I think first I should choose basis for the funtion, and then represent the linear operator in ODE as a matrix. My question is what basis should I pick for this problem? Can I choose basis such as $\left \{1,x,x^{2} , x^{3},... \right \}$ ? The eigenvalue for this problem is also determined by the boundary conditons, but how does the boundary conditons affect the matrix? I also found people with related  question:( Solve the eigenvalue problem $y''=\lambda y$ numerically ), and he numerically calculate the problem using finite difference. It seems that his method do not need to find basis. Is this a trick to solve this problem or still related to some kind of basis I am not aware of?","Suppose we have a eigenvalue problem: and we know the eigenvalue is , and eigenfunction . But if I want to calculate the eigenvalue numerically, I think first I should choose basis for the funtion, and then represent the linear operator in ODE as a matrix. My question is what basis should I pick for this problem? Can I choose basis such as ? The eigenvalue for this problem is also determined by the boundary conditons, but how does the boundary conditons affect the matrix? I also found people with related  question:( Solve the eigenvalue problem $y''=\lambda y$ numerically ), and he numerically calculate the problem using finite difference. It seems that his method do not need to find basis. Is this a trick to solve this problem or still related to some kind of basis I am not aware of?","\begin{array}{c}
y^{\prime \prime}+\lambda y=0,0<x<l \\
y(0)=0,  y(l)=0
\end{array} \lambda =\frac{n^{2}\pi ^{2}  }{l^{2}}   y\left ( x \right ) =C\sin \frac{n\pi x}{l}   \left \{1,x,x^{2} , x^{3},... \right \}","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'numerical-methods', 'boundary-value-problem']"
43,Differential equation $y''+(y')^2+1=0$,Differential equation,y''+(y')^2+1=0,"I’m trying to solve this equation but at the end I’m stuck and can’t reach the answer. I use the substitutions $u=y'$ and $y''=du/dx$ : $$du/dx+u^2+1=0, \\ -du/(u^2+1)=dx, \\ -\arctan(u)=x+c$$ Here I don’t know how to go on. The answer should be $$y=\ln|\cos(c_1-x)|+c_2$$",I’m trying to solve this equation but at the end I’m stuck and can’t reach the answer. I use the substitutions and : Here I don’t know how to go on. The answer should be,"u=y' y''=du/dx du/dx+u^2+1=0, \\ -du/(u^2+1)=dx, \\ -\arctan(u)=x+c y=\ln|\cos(c_1-x)|+c_2",['ordinary-differential-equations']
44,Using Wronskian to solve nonhomegeneous ODE,Using Wronskian to solve nonhomegeneous ODE,,"I have the given ODE: $$y''+2y'+2y=e^{-x}\sin x$$ This has the homogeneous solution $y_h=C_1\cos(i-1)x+C_2\sin(-i-1)x$ . The particular solution, in the form $y_p=uy_1+vy_2$ , we seek the Ansatz: $y_p=uy_1+vy_2=e^{-x}(\sin x+\cos x)$ .  So $y_1=e^{-x}\sin x$ and $y_2=e^{-x}\cos x$ Then we aim to solve for $u$ and $v$ by  use of the variation of parameters formula: $$u'y_1+v'y_2=0$$ $$u'y_1'+v'y_2'=f(x)$$ where $f(x)=e^{-x}\sin x$ . So here I should  use the Wronskian to facilitate the process. The Wronskian is naturally dependent on $y_1$ and $y_2$ and are $y_1=e^{-x}\sin x$ and $y_2=e^{-x}\cos x$ . So the Wronskian would be \begin{equation} \text{Det}\begin{vmatrix} e^{-x}\sin x & e^{-x}\cos x\\ e^{-x}\cos x-e^{-x}\sin x & -e^{-x}\cos x-e^{-x}\sin x \end{vmatrix} \end{equation} My calculation gives: $Det=-e^{-2x}\cos2x$ So how is this useful to solve the ODE, when I could just use the formula for variation of parameters? Thanks","I have the given ODE: This has the homogeneous solution . The particular solution, in the form , we seek the Ansatz: .  So and Then we aim to solve for and by  use of the variation of parameters formula: where . So here I should  use the Wronskian to facilitate the process. The Wronskian is naturally dependent on and and are and . So the Wronskian would be My calculation gives: So how is this useful to solve the ODE, when I could just use the formula for variation of parameters? Thanks","y''+2y'+2y=e^{-x}\sin x y_h=C_1\cos(i-1)x+C_2\sin(-i-1)x y_p=uy_1+vy_2 y_p=uy_1+vy_2=e^{-x}(\sin x+\cos x) y_1=e^{-x}\sin x y_2=e^{-x}\cos x u v u'y_1+v'y_2=0 u'y_1'+v'y_2'=f(x) f(x)=e^{-x}\sin x y_1 y_2 y_1=e^{-x}\sin x y_2=e^{-x}\cos x \begin{equation}
\text{Det}\begin{vmatrix}
e^{-x}\sin x & e^{-x}\cos x\\
e^{-x}\cos x-e^{-x}\sin x & -e^{-x}\cos x-e^{-x}\sin x
\end{vmatrix}
\end{equation} Det=-e^{-2x}\cos2x","['ordinary-differential-equations', 'wronskian']"
45,Solve a non-exact ODE by a different method,Solve a non-exact ODE by a different method,,"The following equation $$(x^3+2xy)dx-x^2dy=0$$ is not exact, since $$\frac{\partial M}{\partial y}=2x\ne\frac{\partial N}{\partial x}=-2x$$ I wanted to try the following then, $$-x^2dy=-(x^3+2xy)dx$$ $$\frac{dy}{dx}=\frac{x^3+2xy}{x^2}$$ $$dy=\frac{x^3+2xy}{x^2}dx$$ $$\int dy=\int xdx+ \int \frac{2y}{x}dx$$ But the last integral, according to what I suspect does not make any sense for finding a solution to the original problem. That would mean that the answer by this approach is: $$y=\frac{x^2}{2}+2y\ln x$$ However, this is not correct. Any ideas how to solve this? Thanks","The following equation is not exact, since I wanted to try the following then, But the last integral, according to what I suspect does not make any sense for finding a solution to the original problem. That would mean that the answer by this approach is: However, this is not correct. Any ideas how to solve this? Thanks",(x^3+2xy)dx-x^2dy=0 \frac{\partial M}{\partial y}=2x\ne\frac{\partial N}{\partial x}=-2x -x^2dy=-(x^3+2xy)dx \frac{dy}{dx}=\frac{x^3+2xy}{x^2} dy=\frac{x^3+2xy}{x^2}dx \int dy=\int xdx+ \int \frac{2y}{x}dx y=\frac{x^2}{2}+2y\ln x,['ordinary-differential-equations']
46,Formulae for projectile motion with resistance proportional to velocity,Formulae for projectile motion with resistance proportional to velocity,,"What are the formulae for resisted projectile motion in which the resistance is proportional to the velocity? I have a problem where my answers don't match up with the textbook answers and I need to verify with formulae. I need the velocities and positions with respect to time for both components, for future readers as well. The projectile is launched from the ground, at an angle $\theta$ and velocity $u$ :","What are the formulae for resisted projectile motion in which the resistance is proportional to the velocity? I have a problem where my answers don't match up with the textbook answers and I need to verify with formulae. I need the velocities and positions with respect to time for both components, for future readers as well. The projectile is launched from the ground, at an angle and velocity :",\theta u,"['integration', 'ordinary-differential-equations', 'projectile-motion']"
47,How to linearize this bernoulii ODE?,How to linearize this bernoulii ODE?,,I have a Bernoulli ODE problem: $$y'-xy=xy^{\frac{3}{2}}.$$ To solve it I will linearize the ODE. Let $z=y^{1-\frac{3}{2}}=y^{-\frac{1}{2}}$ then $y=z^{\frac{1}{2}}$ and $y'=\frac{1}{2} z^{-\frac{1}{2}} z'$ . Substitute in the ODE we have \begin{align} 			& \frac{1}{2} z^{-\frac{1}{2}} z' - x z^{\frac{1}{2}} =x \left( z^{\frac{1}{2}}\right) ^{\frac{3}{2}}\nonumber\\ 			\iff & \frac{1}{2} z' - x z =x  z^{\frac{3}{8}} \nonumber\\ 			\iff &  z' - 2 x z =2x  z^{\frac{3}{8}}.\label{p1ba} 		\end{align} But in the last ODE I can't get the linear ODE. What should be letting $z(x)$ such that the ODE can linear?,I have a Bernoulli ODE problem: To solve it I will linearize the ODE. Let then and . Substitute in the ODE we have But in the last ODE I can't get the linear ODE. What should be letting such that the ODE can linear?,"y'-xy=xy^{\frac{3}{2}}. z=y^{1-\frac{3}{2}}=y^{-\frac{1}{2}} y=z^{\frac{1}{2}} y'=\frac{1}{2} z^{-\frac{1}{2}} z' \begin{align}
			& \frac{1}{2} z^{-\frac{1}{2}} z' - x z^{\frac{1}{2}} =x \left( z^{\frac{1}{2}}\right) ^{\frac{3}{2}}\nonumber\\
			\iff & \frac{1}{2} z' - x z =x  z^{\frac{3}{8}} \nonumber\\
			\iff &  z' - 2 x z =2x  z^{\frac{3}{8}}.\label{p1ba}
		\end{align} z(x)",['ordinary-differential-equations']
48,Does the solution depend on initial conditions?,Does the solution depend on initial conditions?,,"I have started to learn numerical analysis of differential equations and after a bit information arose the question. Let's assume we have the IVP for the first-order ODE. We solved it and got some solution which satisfies the problem and the theorem of existance and uniqueness. If i change initial conditions, some problems will lose the property to be satisfied to the theorem of existing and uniqueness (well-posedness would be broken). So, the question is: If i write in my essay the next sentence, ""as we can see, the ODE should not depend on initial conditions"", would it be correct? Any explanations are welcomed","I have started to learn numerical analysis of differential equations and after a bit information arose the question. Let's assume we have the IVP for the first-order ODE. We solved it and got some solution which satisfies the problem and the theorem of existance and uniqueness. If i change initial conditions, some problems will lose the property to be satisfied to the theorem of existing and uniqueness (well-posedness would be broken). So, the question is: If i write in my essay the next sentence, ""as we can see, the ODE should not depend on initial conditions"", would it be correct? Any explanations are welcomed",,"['ordinary-differential-equations', 'numerical-methods', 'initial-value-problems']"
49,The line $y=x$ is an invariant set ODE,The line  is an invariant set ODE,y=x,"Let the system \begin{align*}     \dot{x} &= y^3 -4x\\     \dot{y} &= y^3-y-3x \end{align*} (a) Prove that the line $y=x$ is an invariant set. (b) Prove that $|x(t) - y(t)| \to 0$ when $t \to \infty$ , for all other trajectories. I have some questions of this exercise. For (a) I proposed the function $V(x,y)=x-y$ , then I calculated $\dot{V}(x,y)$ and evaluate the function in the points $y=x$ . The result I got was that $$\dot{V}(x,y)|_{x=y}=0$$ but then I do not know how to conclude. Is this right? For (b) I do not know well how to proceed. Any hint?","Let the system (a) Prove that the line is an invariant set. (b) Prove that when , for all other trajectories. I have some questions of this exercise. For (a) I proposed the function , then I calculated and evaluate the function in the points . The result I got was that but then I do not know how to conclude. Is this right? For (b) I do not know well how to proceed. Any hint?","\begin{align*}
    \dot{x} &= y^3 -4x\\
    \dot{y} &= y^3-y-3x
\end{align*} y=x |x(t) - y(t)| \to 0 t \to \infty V(x,y)=x-y \dot{V}(x,y) y=x \dot{V}(x,y)|_{x=y}=0","['ordinary-differential-equations', 'set-invariance']"
50,Solving $y' = \frac{x^2 + 2xy - y^2 - 2}{x^2 - 2xy - y^2 + 2}$ without tricks,Solving  without tricks,y' = \frac{x^2 + 2xy - y^2 - 2}{x^2 - 2xy - y^2 + 2},"I would like to solve the equation: $y' = \frac{y^2 - 2xy - x^2 + 2}{y^2 + 2xy  - x^2 - 2} \tag{1}$ From that, we have: $y'(y^2 + 2xy  - x^2 - 2) = y^2 - 2xy - x^2 + 2 \implies $ $\frac{(2x + 2yy')(x+y) - (x^2 + y^2 + 2)(1 + y')}{(x+y)^2} = 0 \implies $ $(\frac{x^2 + y^2 + 2}{x + y})' = 0 \implies $ $\frac{x^2 + y^2 + 2}{x + y} = C$ Where the last part agrees with the solution in the book. The ""trick"" is that I first looked at the solution in the book, differentiated it, and then applied here the process in reverse, so it looks like a solution. I was obviously not happy with that approach, so I looked online for a better solution. Unfortunately, I could not see a generic pattern in neither of the two existing solutions [solution 1 , solution 2] I found. Does there exists a more standard solution to the equation? Are the two existing solutions common, and are not considered as ""tricks""? Why would they be less of a trick than the one I described above? Thanks!","I would like to solve the equation: From that, we have: Where the last part agrees with the solution in the book. The ""trick"" is that I first looked at the solution in the book, differentiated it, and then applied here the process in reverse, so it looks like a solution. I was obviously not happy with that approach, so I looked online for a better solution. Unfortunately, I could not see a generic pattern in neither of the two existing solutions [solution 1 , solution 2] I found. Does there exists a more standard solution to the equation? Are the two existing solutions common, and are not considered as ""tricks""? Why would they be less of a trick than the one I described above? Thanks!",y' = \frac{y^2 - 2xy - x^2 + 2}{y^2 + 2xy  - x^2 - 2} \tag{1} y'(y^2 + 2xy  - x^2 - 2) = y^2 - 2xy - x^2 + 2 \implies  \frac{(2x + 2yy')(x+y) - (x^2 + y^2 + 2)(1 + y')}{(x+y)^2} = 0 \implies  (\frac{x^2 + y^2 + 2}{x + y})' = 0 \implies  \frac{x^2 + y^2 + 2}{x + y} = C,['ordinary-differential-equations']
51,Rewriting equation for solving differential equation,Rewriting equation for solving differential equation,,"When going through a solution concerning a differential equation we have the following expression: $(1-\frac{x^2}{y^2})\frac{dy}{dx} + \frac{2x}{y} = 0$ and then its said to note that: $ \frac{d}{dx}(\frac{x^2}{y}+y)=\frac{2x}{y}-\frac{x^2}{y^2}\frac{dy}{dx} + \frac{dy}{dx} = (1-\frac{x^2}{y^2})\frac{dy}{dx} + \frac{2x}{y}$ so that we later can use the expression to the left, but I don't really understand how the $\frac{dy}{dx}$ is used here, could someone explain how ? Specifically what is going on with the $-\frac{x^2}{y^2}\frac{dy}{dx}+ \frac{dy}{dx}$ terms in the middle part. Thanks","When going through a solution concerning a differential equation we have the following expression: and then its said to note that: so that we later can use the expression to the left, but I don't really understand how the is used here, could someone explain how ? Specifically what is going on with the terms in the middle part. Thanks",(1-\frac{x^2}{y^2})\frac{dy}{dx} + \frac{2x}{y} = 0  \frac{d}{dx}(\frac{x^2}{y}+y)=\frac{2x}{y}-\frac{x^2}{y^2}\frac{dy}{dx} + \frac{dy}{dx} = (1-\frac{x^2}{y^2})\frac{dy}{dx} + \frac{2x}{y} \frac{dy}{dx} -\frac{x^2}{y^2}\frac{dy}{dx}+ \frac{dy}{dx},"['ordinary-differential-equations', 'derivatives']"
52,How can I solve a differential equation of the form $c_1 \ddot{x}^2 + c_2 \ddot{x} + c_3 \dot{x}^2-c_4 =0$?,How can I solve a differential equation of the form ?,c_1 \ddot{x}^2 + c_2 \ddot{x} + c_3 \dot{x}^2-c_4 =0,"The solution to it will give us the eqs. of motion of a car moved by a constant force due to combustion of fuel at the engine that produces the main torque at the wheels and that is ""fighting"" against friction and drag. The differential equation comes from the 2nd law of Newton and the torque equations. In brief, we'd have \begin{align*} \sum F &= m(t) \ddot{x}\\ m(t)\ddot{x} &= F_{fuel} -F_f -F_d = F_{fuel} -\mu m(t)g-\kappa \dot{x}^2 \end{align*} Since there are two unknown functions we want to find out, $m(t)$ and $x(t)$ , $\dot{x}(t)$ , or $\ddot{x}(t)$ , we'll have to find a relation between $m(t)$ and any of these 3. We'll do it with the torque eqs.. We'll consider that the torque in one of the 4 wheels of the car is generated thanks to $\frac{1}{4}F_f$ and $F_{fuel}$ : \begin{align*} \sum \tau = r \sin{\theta}\sum F &= I\ddot{\varphi}\\ r(F_{fuel}+\frac{1}{4}F_f) &= \frac{1}{2}m_w r^2 \ddot{\varphi}\\ F_{fuel}+\frac{1}{4}\mu m(t)g &= \frac{1}{2}m_w \ddot{x}\\ m(t) &= \frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel} \end{align*} Now we substitute this expression in the Newton's equation and get the differential equation from the title for $c_1 \equiv \frac{2 m_w}{\mu g}$ , $c_2 \equiv 2m_w -\frac{4}{\mu g} F_{fuel}$ , $c_3 \equiv \kappa$ , and $c_4 \equiv 5 F_{fuel}$ : \begin{align*} \Bigg[\frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel}\Bigg]\ddot{x} &= F_{fuel} -\mu g \Bigg[\frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel}\Bigg]-\kappa \dot{x}^2\\ \frac{2 m_w}{\mu g} \ddot{x}^2 -\frac{4}{\mu g}F_{fuel}\ddot{x} &= F_{fuel} -2 m_w\ddot{x} + 4F_{fuel}-\kappa \dot{x}^2\\ \frac{2 m_w}{\mu g} \ddot{x}^2 + (2m_w -\frac{4}{\mu g} F_{fuel})\ddot{x} + \kappa \dot{x}^2 -5 F_{fuel}&= 0 \end{align*} I don't have the enough knowledge to approach this diff. eq. yet, and I don't even know if it is solvable analitically.","The solution to it will give us the eqs. of motion of a car moved by a constant force due to combustion of fuel at the engine that produces the main torque at the wheels and that is ""fighting"" against friction and drag. The differential equation comes from the 2nd law of Newton and the torque equations. In brief, we'd have Since there are two unknown functions we want to find out, and , , or , we'll have to find a relation between and any of these 3. We'll do it with the torque eqs.. We'll consider that the torque in one of the 4 wheels of the car is generated thanks to and : Now we substitute this expression in the Newton's equation and get the differential equation from the title for , , , and : I don't have the enough knowledge to approach this diff. eq. yet, and I don't even know if it is solvable analitically.","\begin{align*}
\sum F &= m(t) \ddot{x}\\
m(t)\ddot{x} &= F_{fuel} -F_f -F_d = F_{fuel} -\mu m(t)g-\kappa \dot{x}^2
\end{align*} m(t) x(t) \dot{x}(t) \ddot{x}(t) m(t) \frac{1}{4}F_f F_{fuel} \begin{align*}
\sum \tau = r \sin{\theta}\sum F &= I\ddot{\varphi}\\
r(F_{fuel}+\frac{1}{4}F_f) &= \frac{1}{2}m_w r^2 \ddot{\varphi}\\
F_{fuel}+\frac{1}{4}\mu m(t)g &= \frac{1}{2}m_w \ddot{x}\\
m(t) &= \frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel}
\end{align*} c_1 \equiv \frac{2 m_w}{\mu g} c_2 \equiv 2m_w -\frac{4}{\mu g} F_{fuel} c_3 \equiv \kappa c_4 \equiv 5 F_{fuel} \begin{align*}
\Bigg[\frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel}\Bigg]\ddot{x} &= F_{fuel} -\mu g \Bigg[\frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel}\Bigg]-\kappa \dot{x}^2\\
\frac{2 m_w}{\mu g} \ddot{x}^2 -\frac{4}{\mu g}F_{fuel}\ddot{x} &= F_{fuel} -2 m_w\ddot{x} + 4F_{fuel}-\kappa \dot{x}^2\\
\frac{2 m_w}{\mu g} \ddot{x}^2 + (2m_w -\frac{4}{\mu g} F_{fuel})\ddot{x} + \kappa \dot{x}^2 -5 F_{fuel}&= 0
\end{align*}","['calculus', 'ordinary-differential-equations', 'physics']"
53,"Does there exist two functions $f, g\in C^1(I)$ for which $W(f, g) (x) >0$ for some $x$ and $W(f, g) (x) <0$ for some $x$?",Does there exist two functions  for which  for some  and  for some ?,"f, g\in C^1(I) W(f, g) (x) >0 x W(f, g) (x) <0 x","$f, g\in C^1(I) $ where $I$ is an open interval and $f, g$ both are real valued. Let $W(f,g)(x) =\begin{vmatrix}f(x) &g(x) \\f'(x)&g'(x)\end{vmatrix}$ denote the Wronskian of $f, g$ at $x\in I$ $\textbf{Question}$ Does there exists such $f, g$ for which $W(f, g) (x) >0$ for some $x$ and $W(f, g) (x) <0$ for some $x$ ? $W(f, g) (x) \neq 0$ for some $x\in I$ implies $\{f, g\}$ linearly independent. If two functions are solutions of a differential equation $y""+p(x) y'+q(x) y=0$ on $I$ where $p, q\in C(I) $ then by Abel's identity we have $$W(f, g) (x) =W(f, g) (x_o) e^{-\int_{x_0}^{x} p(t) dt}$$ Then $W(f, g) (x_0) \neq 0$ for some $x_0\in I$ implies $W(f, g) \neq 0$ on $I$ Moreover $W(f,g)$ different from zero with the same sign at every point ${\displaystyle x} \in {\displaystyle I}$ Hence we have to find two functions $f, g$ with the properties: $f, g$ must have to be linearly independent. $f, g\in C^1(I) $ $f, g$ can't be the solution of $2$ nd order homogenous linear ODE. $W(f, g) $ attains both positive and negative values on $I$ . Let $0\in I$ be an open interval and $f, g\in C^1(I) $ defined by $f(x) =x^2$ and $g(x) =x|x| $ Then $f,g$ satisfy $1, 2,3 $ but not $4$ as $W(f, g) (x) =0$ on $I$ .",where is an open interval and both are real valued. Let denote the Wronskian of at Does there exists such for which for some and for some ? for some implies linearly independent. If two functions are solutions of a differential equation on where then by Abel's identity we have Then for some implies on Moreover different from zero with the same sign at every point Hence we have to find two functions with the properties: must have to be linearly independent. can't be the solution of nd order homogenous linear ODE. attains both positive and negative values on . Let be an open interval and defined by and Then satisfy but not as on .,"f, g\in C^1(I)  I f, g W(f,g)(x) =\begin{vmatrix}f(x) &g(x) \\f'(x)&g'(x)\end{vmatrix} f, g x\in I \textbf{Question} f, g W(f, g) (x) >0 x W(f, g) (x) <0 x W(f, g) (x) \neq 0 x\in I \{f, g\} y""+p(x) y'+q(x) y=0 I p, q\in C(I)  W(f, g) (x) =W(f, g) (x_o) e^{-\int_{x_0}^{x} p(t) dt} W(f, g) (x_0) \neq 0 x_0\in I W(f, g) \neq 0 I W(f,g) {\displaystyle x} \in {\displaystyle I} f, g f, g f, g\in C^1(I)  f, g 2 W(f, g)  I 0\in I f, g\in C^1(I)  f(x) =x^2 g(x) =x|x|  f,g 1, 2,3  4 W(f, g) (x) =0 I","['ordinary-differential-equations', 'analysis', 'independence', 'applications', 'wronskian']"
54,How to solve such fraction differential equation?,How to solve such fraction differential equation?,,"Here's my first-order differential equation: $$ (x^3 - 2xy^2)dx + 3yx^2dy = xdy - ydx $$ I've tried to make it fraction, but it isn't separable differential equation, also it isn't differential equation in total differentials, so after it I lose any clue for answer.","Here's my first-order differential equation: I've tried to make it fraction, but it isn't separable differential equation, also it isn't differential equation in total differentials, so after it I lose any clue for answer.","
(x^3 - 2xy^2)dx + 3yx^2dy = xdy - ydx
",['ordinary-differential-equations']
55,Basis of solutions of $x^{2}y''-4xy'+6y=0$?,Basis of solutions of ?,x^{2}y''-4xy'+6y=0,"What is the basis of solution space spanned by the solutions of the homogeneous ODE given by $$x^{2}y''-4xy'+6y=0$$ My work: The solution is of the type $x^{m}$ . By solving the auxiliary equation for this ODE, which is $m(m-1)-4m+6=0$ implies $m=2,m=3$ . Hence, the two solutions are $x^{2}$ and $x^{3}$ and thus the basis is { $x^{2}$ , $x^{3}$ }. But according to the solution I have the basis is { $x^{2}$ , $x^{3}$ , $x^{2}|x|$ }, but I actually want to know why $x^{2}|x|$ is explicitly specified as a solution?","What is the basis of solution space spanned by the solutions of the homogeneous ODE given by My work: The solution is of the type . By solving the auxiliary equation for this ODE, which is implies . Hence, the two solutions are and and thus the basis is { , }. But according to the solution I have the basis is { , , }, but I actually want to know why is explicitly specified as a solution?","x^{2}y''-4xy'+6y=0 x^{m} m(m-1)-4m+6=0 m=2,m=3 x^{2} x^{3} x^{2} x^{3} x^{2} x^{3} x^{2}|x| x^{2}|x|","['ordinary-differential-equations', 'homogeneous-equation']"
56,fourth-order finite difference for $(a(x)u'(x))'$,fourth-order finite difference for,(a(x)u'(x))',"Previously I asked here about constructing a symmetric matrix for doing finite difference for $(a(x)u'(x))'$ where the (diffusion) coefficient $a(x)$ is spatially varying. The answer provided there works for getting a second order accurate method. What about getting a fourth-order accurate method? If I follow the same idea and apply the following fourth-order accurate formula for first derivative $ u'_i = \dfrac{u_{i-1} - 8u_{i-1/2} + 8u_{i+1/2} - u_{i+1}}{6\Delta x}$ (1) in succession, then I end up getting a formula for $(a u')_i'$ which involves $u_{i-2}, u_{i-3/2}, u_{i-1}, u_{i-1/2}, u_i, u_{i+1/2}, u_{i+1}, u_{i+3/2}, u_2$ . It involves the mid point values $u_{i+n/2}$ and it depends on nine neighbouring values of $u_i$ , which doesn't sound right for fourth order accurate scheme. For the special case of $a(x)=1$ it also doesn't reduce to the formula $ u'' = \dfrac{-u_{i-2} + 16u_{i-1} - 30u_i + 16u_{i+1} - u_{i+2}}{12\Delta x^2}$ .   (2) So what's wrong with applying (1) in succession? What's the correct approach to get a finite difference formula for $(au')'$ with fourth order accuracy? Edit 1: After reading this post I managed to derive a symmetric four-order centered difference scheme for $[a(x)u'(x)]'$ by applying $ u'(x) = \dfrac{u_{i-3/2} - 27u_{i-1/2} + 27u_{i+1/2} - u_{i+3/2}}{24\Delta x} + \mathcal{O}(\Delta x^4)$ twice in succession. The resulting formula for $[a(x)u'(x)]'$ is $ (au')'_i = \dfrac{1}{576\Delta x^2} \left( a_{i-\frac{3}{2}}u_{i-3} - 27(a_{i-\frac{3}{2}} + a_{i-\frac{1}{2}})u_{i-2} + (27a_{i-\frac{3}{2}} + 729a_{i-\frac{1}{2}}+27a_{i+\frac{1}{2}})u_{i-1} - (a_{i-\frac{3}{2}} + 729a_{i-\frac{1}{2}} + 729a_{i+\frac{1}{2}} + a_{i+\frac{3}{2}})u_i + (27a_{i-\frac{1}{2}} + 729a_{i+\frac{1}{2}}+27a_{i+\frac{3}{2}})u_{i+1} - 27(a_{i+\frac{1}{2}} + a_{i+\frac{3}{2}})u_{i+2} + a_{i+\frac{3}{2}}u_{i+3} \right)$ which involves the mid point values of $a(x)$ and has seven stencils. Is there a formula that involves even less computations / stencils(e.g. five stencils)?","Previously I asked here about constructing a symmetric matrix for doing finite difference for where the (diffusion) coefficient is spatially varying. The answer provided there works for getting a second order accurate method. What about getting a fourth-order accurate method? If I follow the same idea and apply the following fourth-order accurate formula for first derivative (1) in succession, then I end up getting a formula for which involves . It involves the mid point values and it depends on nine neighbouring values of , which doesn't sound right for fourth order accurate scheme. For the special case of it also doesn't reduce to the formula .   (2) So what's wrong with applying (1) in succession? What's the correct approach to get a finite difference formula for with fourth order accuracy? Edit 1: After reading this post I managed to derive a symmetric four-order centered difference scheme for by applying twice in succession. The resulting formula for is which involves the mid point values of and has seven stencils. Is there a formula that involves even less computations / stencils(e.g. five stencils)?","(a(x)u'(x))' a(x)  u'_i = \dfrac{u_{i-1} - 8u_{i-1/2} + 8u_{i+1/2} - u_{i+1}}{6\Delta x} (a u')_i' u_{i-2}, u_{i-3/2}, u_{i-1}, u_{i-1/2}, u_i, u_{i+1/2}, u_{i+1}, u_{i+3/2}, u_2 u_{i+n/2} u_i a(x)=1  u'' = \dfrac{-u_{i-2} + 16u_{i-1} - 30u_i + 16u_{i+1} - u_{i+2}}{12\Delta x^2} (au')' [a(x)u'(x)]'  u'(x) = \dfrac{u_{i-3/2} - 27u_{i-1/2} + 27u_{i+1/2} - u_{i+3/2}}{24\Delta x} + \mathcal{O}(\Delta x^4) [a(x)u'(x)]'  (au')'_i = \dfrac{1}{576\Delta x^2} \left( a_{i-\frac{3}{2}}u_{i-3}
- 27(a_{i-\frac{3}{2}} + a_{i-\frac{1}{2}})u_{i-2}
+ (27a_{i-\frac{3}{2}} + 729a_{i-\frac{1}{2}}+27a_{i+\frac{1}{2}})u_{i-1}
- (a_{i-\frac{3}{2}} + 729a_{i-\frac{1}{2}} + 729a_{i+\frac{1}{2}} + a_{i+\frac{3}{2}})u_i
+ (27a_{i-\frac{1}{2}} + 729a_{i+\frac{1}{2}}+27a_{i+\frac{3}{2}})u_{i+1}
- 27(a_{i+\frac{1}{2}} + a_{i+\frac{3}{2}})u_{i+2}
+ a_{i+\frac{3}{2}}u_{i+3} \right) a(x)","['ordinary-differential-equations', 'numerical-methods', 'finite-differences', 'numerical-calculus', 'finite-difference-methods']"
57,"Spectrum of the operator on $L^2[0,1]$",Spectrum of the operator on,"L^2[0,1]","Consider the operator T on $L^2[0,1]$ , given by $T(f(x)) = \int_{1-x}^1 f(y)dy$ . I want to find the spectrum of this operator. I know the only possible candidates are 0 and non-zero Eigen values of T, since it is a compact operator. Since it is infinite dimensional $0$ has to be in spectrum. Now I've to check only non-zero eigen values $\lambda$ of T, but $T(f(x))= \lambda f(x)$ gives the following ODE, $f'(x) = -\frac{1}{\lambda}f(1-x)$ . Please help me how to proceed from here.","Consider the operator T on , given by . I want to find the spectrum of this operator. I know the only possible candidates are 0 and non-zero Eigen values of T, since it is a compact operator. Since it is infinite dimensional has to be in spectrum. Now I've to check only non-zero eigen values of T, but gives the following ODE, . Please help me how to proceed from here.","L^2[0,1] T(f(x)) = \int_{1-x}^1 f(y)dy 0 \lambda T(f(x))= \lambda f(x) f'(x) = -\frac{1}{\lambda}f(1-x)","['functional-analysis', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations']"
58,How to show that the solutions to one ODE give the solutions to an other?,How to show that the solutions to one ODE give the solutions to an other?,,"While working on a differential geometry problem I have come across a second order ODE that I am trying solve: \begin{equation}1+(h')^2-hh''=2ch(1+(h')^2)^{3/2}\end{equation} It turns out that is equation can't really be solved analytically but I am told that the following first order ODE gives all the solutions to the second order ODE: \begin{equation}h^2\pm\frac{h}{c\sqrt{1+(h')^2}}=\pm b^2\end{equation} where b is some real constant. The problem is that this equation can't be solved analytically either. I must show that this first order ODE does indeed give all the solutions to the second order equation. So far I haven't been successful in doing so but I will list what I have done (my attempts have been a little desperate and I haven't been very confident with any of them): I've isolated for h' in the first order ODE, calculated h'' and plugged this in to both sides of the second order ODE to see if both sides are equal. This was done using math software (maple) but it didn't seem like the sides were equal. I've assumed that h must satisfy some other condition such that the second order ODE is simplified in a way that gives us the first order equation. For instance, I've assumed h must also satisfy $h''=k^2h$ in which case this second order ODE becomes a first order one. I did this because I figured that as long as I get two linearly independent solutions to the second order ODE, I have spanned the entire solution space. If my first order equation above can get me these two solutions I have unlocked all the solutions to the second order equation. Is there a general way that I can show that these two equations share solutions? What would you do from here? What can I try in order to show that the solutions to the first order equation give me the solutions to the second order equation? PS: I am also quite confused about how a first order equation can have several linearly independent solutions. I don't see how it can. Am I expected to construct another linearly independent solution from the one that I get?","While working on a differential geometry problem I have come across a second order ODE that I am trying solve: It turns out that is equation can't really be solved analytically but I am told that the following first order ODE gives all the solutions to the second order ODE: where b is some real constant. The problem is that this equation can't be solved analytically either. I must show that this first order ODE does indeed give all the solutions to the second order equation. So far I haven't been successful in doing so but I will list what I have done (my attempts have been a little desperate and I haven't been very confident with any of them): I've isolated for h' in the first order ODE, calculated h'' and plugged this in to both sides of the second order ODE to see if both sides are equal. This was done using math software (maple) but it didn't seem like the sides were equal. I've assumed that h must satisfy some other condition such that the second order ODE is simplified in a way that gives us the first order equation. For instance, I've assumed h must also satisfy in which case this second order ODE becomes a first order one. I did this because I figured that as long as I get two linearly independent solutions to the second order ODE, I have spanned the entire solution space. If my first order equation above can get me these two solutions I have unlocked all the solutions to the second order equation. Is there a general way that I can show that these two equations share solutions? What would you do from here? What can I try in order to show that the solutions to the first order equation give me the solutions to the second order equation? PS: I am also quite confused about how a first order equation can have several linearly independent solutions. I don't see how it can. Am I expected to construct another linearly independent solution from the one that I get?",\begin{equation}1+(h')^2-hh''=2ch(1+(h')^2)^{3/2}\end{equation} \begin{equation}h^2\pm\frac{h}{c\sqrt{1+(h')^2}}=\pm b^2\end{equation} h''=k^2h,['ordinary-differential-equations']
59,Can systems of linear ODE's be solved when their matrix is not diagonalizable?,Can systems of linear ODE's be solved when their matrix is not diagonalizable?,,I'm working through my ODE homework right now and I've run into a repeated issue of ODE systems not being diagonalizable. I am not aware of any other methods to solve systems and my lecture notes do not have any comments on unsolvable systems. Do I need to approach the problem in a different way or is there simply not solutions to some systems? One problem I'm working with is $$x' =\pmatrix{1&1&1 \\ 2&1&-1 \\ -3&2&4}x$$ I can't find any generalized eigenvectors to work with and I'm stuck on how to solve it. I ask that you don't solve the problem and just nudge me in the right direction.,I'm working through my ODE homework right now and I've run into a repeated issue of ODE systems not being diagonalizable. I am not aware of any other methods to solve systems and my lecture notes do not have any comments on unsolvable systems. Do I need to approach the problem in a different way or is there simply not solutions to some systems? One problem I'm working with is I can't find any generalized eigenvectors to work with and I'm stuck on how to solve it. I ask that you don't solve the problem and just nudge me in the right direction.,x' =\pmatrix{1&1&1 \\ 2&1&-1 \\ -3&2&4}x,"['matrices', 'ordinary-differential-equations', 'matrix-exponential']"
60,Integrating factor of $x^2ydx-(x^3+y^3)dy=0$,Integrating factor of,x^2ydx-(x^3+y^3)dy=0,"$x^2ydx-(x^3+y^3)dy=0$ I have to find an integrating factor $\mu$ . Denote $P=x^2y,Q=-(x^3+y^3)$ The correct answer for getting an integrating factor is $\mu=e^{\int\frac{P_y-Q_x}{\color{red}{P}}dy}$ or $\mu=e^{\int\frac{P_y-Q_x}{\color{red}{-P}}dy}$ ? In my post Solve $ye^ydx+(1+xe^y)dy=0$ , I got a comment that the correct answer is $\mu=e^{\int\frac{P_y-Q_x}{\color{red}{P}}dy}.$ Then , $\int\frac{x^2+3x^2}{x^2y}dy=4ln|y| \implies \mu=e^{ln|y|}=y^4$ , But the correct integrating factor is $y^{-4}$ . How is it possible ? maybe $\mu=e^{\int\frac{P_y-Q_x}{\color{red}{-P}}dy}$ ? Thanks !","I have to find an integrating factor . Denote The correct answer for getting an integrating factor is or ? In my post Solve $ye^ydx+(1+xe^y)dy=0$ , I got a comment that the correct answer is Then , , But the correct integrating factor is . How is it possible ? maybe ? Thanks !","x^2ydx-(x^3+y^3)dy=0 \mu P=x^2y,Q=-(x^3+y^3) \mu=e^{\int\frac{P_y-Q_x}{\color{red}{P}}dy} \mu=e^{\int\frac{P_y-Q_x}{\color{red}{-P}}dy} \mu=e^{\int\frac{P_y-Q_x}{\color{red}{P}}dy}. \int\frac{x^2+3x^2}{x^2y}dy=4ln|y| \implies \mu=e^{ln|y|}=y^4 y^{-4} \mu=e^{\int\frac{P_y-Q_x}{\color{red}{-P}}dy}",['ordinary-differential-equations']
61,Is the function $f(x)=\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}e^{-\frac{x^2}{1-x^2}}$ a bump-function $\in C_c^\infty$? Diff. Eq.?,Is the function  a bump-function ? Diff. Eq.?,f(x)=\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}e^{-\frac{x^2}{1-x^2}} \in C_c^\infty,"Is the function $f(x)=\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}e^{-\frac{x^2}{1-x^2}}$ a bump-function $\in C_c^\infty$ ? Which autonomous differential equation it fulfill? (note it is not defined piece-wise) I was viewing this question on MSE and there was given here a pretty clever answer $q(x) = 1-\sqrt{x^2}+\sqrt{\left(1-\sqrt{x^2}\right)^2}$ for a compact-supported function that is not defined piece-wise, and I was trying to us it method to built a proper bump function in $C_c^\infty$ following the definition of Wikipedia (a compact-supported smooth function ) that is also not defined piece-wise. After playing a while with it on Wolfram-Alpha and Desmos , I believe that: $$f(x)=\textstyle{\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}}{\Large e}{\begin{array}\displaystyle{ -\frac{x^2}{1-x^2}} \\ \\ \\ \end{array}}$$ it is actually a bump function $\in C_c^\infty$ since their derivatives will be polynomials multiplied again by $f(x)$ , which will keep their functions smooth and keeping smoothness at the boundaries of their non-zero values $\partial x = [-1,\,1]$ , but my math skills aren´t enough to prove it, so far, I was able only to check the first 6th derivatives and it looks fine when plotted, without having discontinuities (I am worried of the hidden $\text{abs}(1-x^2)$ function which derivatives becomes Dirac's Delta functions $\delta(x \pm 1)$ ), neither changing their starting and ending values from zero ( example ). Since the function $f(x)=0,\,|x|>1$ as intended, I think it is already a smooth function on these points so it stills being a bump-function $\in C_c^\infty$ (if I am right, the only possible analytic function of compact-support is the zero function, so I believe that piecewise zero sections are smooth given they are analytic), and within $|x|<1$ it still being smooth since the term $e^{-\frac{x^2}{1-x^2}}$ will dominate any polynomial, but in the points $x=\{-1,\,1\}$ I don't know if will dominate any derivative of the Dirac's delta function. Also I would like to know which autonomous differential equation $f'(x) = G(f(x))$ with $G(x)$ at least $C^1$ almost-everywhere will have $f(x)$ as solution, maybe a non linear ODE, or a Delay Differential Equation (DDE) as it was shown here for another bump-function (so far is the only example I know): there is found that a bump-function which fulfill $\varphi'(t)=2\left(\varphi(2t+1)-\varphi(2t-1)\right)$ when the solution is only non-zero inside $[-1;\,1]$ , but unfortunately there is no closed-form for this function, but its Fourier Transform is show to be $\hat{\varphi}(\omega) = \prod\limits_{k=0}^{\infty}\frac{\sin\left(\frac{\pi\omega}{2^k}\right)}{\frac{\pi\omega}{2^k}}$ .... actually my main objective is to find a finite-duration solution to a differential equation that fulfill what is said in this paper Finite Time Differential Equations ... I have the Motivation for this questions explained in this another question . So summarizing: Is $f(x) \in C_c^\infty$ ? Which autonomous differential equation fulfill $f(x)$ as solution? Otherwise, proving it cannot be described through an autonomous diff. eq. is also welcome. My attempts so far... I believe that the diff. eq. found by Wolfram-Alpha is wrong: $$f'(x)\left(1-x^2\right)^2+f(x)(4x-2x^3)=0,\qquad f(0)=1$$ But I have had a lot of struggle dealing with the signs, so first, to see if I am not making mistakes, I will list a few things I am using for which I am not sure if they are formally right. First, to show explicitly where I have risks for discontinuities, I have made transparent that $\sqrt{(1-x^2)^2} \equiv |1-x^2|$ an absolute value function, so $f(x)$ acquire the form $f(x) = e^{-\frac{-x^2}{1-x^2}}\frac{(1-x^2+|1-x^2|)}{2}$ Second, for keeping tractable the sign issues and maintain unchanged the solutions of the diff. equation, I have found I have to keep $\frac{1}{\text{sgn}(a)} \neq \text{sgn}(a)$ , even when both plotted look the same (honestly for easy-hand-calculation I was using that a lot before). When working with the function $s(x) = \log\left(1-x^2+|1-x^2|\right)$ , I will have that $s'(x) = -\frac{2x}{|1-x^2|}$ , but when integrating $-\frac{2x}{|1-x^2|}$ on Wolfram-Alpha it shows a completely different thing, and even so, when derivating the W-A result it didn´t shows to be $s'(x)$ (even the results has Dirac's delta functions involved). So hereinafter, I will be using $ \int -\frac{2x}{|1-x^2|} dx = \log\left(1-x^2+|1-x^2|\right) + \mathcal{c}$ Also, noting that $1-x^2+|1-x^2| \equiv |1-x^2|\left(1+\text{sgn}(1-x^2)\right)$ In all the manipulations I am ignoring possible problems because of dividing by $0$ at the points $x=\{-1,\,1\}$ (they will arise on the topic later). Using these things, I believe that ""a true"" diff. equation for $f(x)$ (but not-autonomous ) is: $$f'(x)\left(1-x^2\right)^2+2x(1+|1-x^2|)f(x) = 0\qquad \text{Eq. 1}$$ From here, this is equivalent to: $$\frac{f'(x)}{f(x)}=\frac{-2x(1+|1-x^2|)}{\left(1-x^2\right)^2}\qquad \text{Eq. 2}$$ Since $f'(x)/f(x) = \frac{d}{dx}\left(\log(f(x))\right)$ , I tried to integrate the left-hand-side of Eq. 2 on Wolfram-Alpha but it was unable to find and antiderivative, but noting that: $$\frac{-2x(1+|1-x^2|)}{\left(1-x^2\right)^2} = \frac{-2x}{\left(1-x^2\right)^2}+\frac{-2x|1-x^2|}{\left(1-x^2\right)^2} =\frac{-2x}{\left(1-x^2\right)^2}+\frac{-2x}{|1-x^2|}$$ integrating both fractions and using point (3) I will have that: $$\Rightarrow \log(f(x)) = \frac{-x^2}{(1-x^2)}+\log(1-x^2+|1-x^2|)+\mathcal{c} $$ From where it can be seen that by applying both side the $\exp()$ function I will recover $f(x)$ for $c=-\log(2)$ , so I believe Eq. 1 is right. As @blamethelag reccomend on the answer I tried to find a recurrence relation for Eq. 2 using the General Leibniz formula : $$\left(fg\right)^{(n)} = \sum\limits_{k=0}^{n} {n \choose k}f^{(k)}g^{(n-k)} $$ But I wasn´t able to find and useful formula (I get stuck). But using this formula jointly with the Faà di Bruno's formula for a function composed with an exponential: $$\left(e^{g}\right)^{(n)} = e^{g}B_n(g',g'',\cdots, g^{(n)})$$ where $B_n()$ is the  nth complete exponential Bell polynomial . With these, I tried to expand the nth derivative of f(x): $$\begin{array}{r c l} \frac{d^n}{dx^n} f(x) & = & \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n {n \choose k} \frac{d^k}{dx^k}\left(e^{-\frac{x^2}{1-x^2}} \right)\frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\ & = & \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n {n \choose k} e^{-\frac{x^2}{1-x^2}}B_k\left(\frac{d}{dx}\left(\frac{-x^2}{1-x^2}\right),\frac{d^2}{dx^2}\left(\frac{-x^2}{1-x^2}\right),\cdots,\frac{d^k}{dx^k}\left(\frac{-x^2}{1-x^2}\right) \right) \frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\ & = & e^{-\frac{x^2}{1-x^2}} \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n \underbrace{{n \choose k} B_k\left(\frac{d}{dx}\left(\frac{-x^2}{1-x^2}\right),\frac{d^2}{dx^2}\left(\frac{-x^2}{1-x^2}\right),\cdots,\frac{d^k}{dx^k}\left(\frac{-x^2}{1-x^2}\right) \right)}_{\mathbb{P}_{n,k}(x)} \frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\ & = & e^{-\frac{x^2}{1-x^2}} \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n \mathbb{P}_{n,k}(x) \left( \frac{d^m}{dx^m}\left(1\right)+\frac{d^m}{dx^m}\left(-x^2\right)+\frac{d^m}{dx^m}\left(|1-x^2|\right) \right) \qquad \text{Eq.3} \end{array}$$ Here, I believe that the term $e^{\frac{-x^2}{1-x^2}}$ is going to ""dominate"" every possible polynomial $\mathbb{P}_{n,k}(x)$ since it already done it for the classical example of a bump function $\in C_c^\infty$ given by $r(x) = \begin{cases} e^{\frac{-x^2}{1-x^2}},\,|x|\leq 1\\ 0,\,\text{otherwise}\end{cases}$ But I am worried about the other part of the equation, since $\frac{d^m}{dx^m}\left(|1-x^2|\right)$ will make appear Dirac's Delta functions and derivatives of it (the following derivatives were solved by wolfram-alpha, by hand I have differences so maybe I am doing something wrong): $$\begin{array}{l c l} m = 0 & \rightarrow & |1-x^2| \\ m = 1 & \rightarrow & -2x\,\text{sgn}(1-x^2)\\ m = 2 & \rightarrow & -2\,\text{sgn}(1-x^2)+4\delta(x+1)+4\delta(x-1) \\ m = 3 & \rightarrow & 4\delta(x+1)+4\delta(x-1)+4\delta'(x+1)+4\delta'(x-1)\\ m = p\geq 4 & \rightarrow & 4 \left(\frac{d^{p-3}}{dx^{p-3}}+\frac{d^{p-2}}{dx^{p-2}}\right)\Big(\delta(x+1)+\delta(x-1)\Big)\\ \end{array}$$ And I don´t know if these Dirac's delta functions and its derivatives are actually helping by ""killing"" things outside $x=\{-1,\,1\}$ , or instead are  ruining the smoothness of $r(x)$ on the points $x=\{-1,\,1\}$ , discarding the hypothesis of the smoothness of $f(x)$ (and it is not seen on the plots because happens only in two zero-measure points $x=\{-1,\,1\}$ ). This is why I worried about the smoothness, since for every other points I think is granted by $r(x)$ . At least for the case $m=3$ , where the terms $\delta()$ and $\delta'()$ , Wolfram-Alpha is able to take the limits $x \to 1$ giving zero value, and it match with their right and left side limits $x \to 1^{\pm}$ in Wolfram-Alpha here and here . By expanding the terms of Eq. 3 you will have things of the form: $$e^{-x^2/(1-x^2)}\mathbb{P}(x)\delta^{(m)}(x\pm 1)$$ where since $w(x)=e^{-x^2/(1-x^2)}$ will ""win"" any polynomial on $x \to \pm 1$ , I will have that $w(x)\mathbb{P}(x)=0,\,x\to\pm 1$ , so by calling $u(x \pm 1) =  w(x \pm 1)\mathbb{P}(x\pm 1)$ and $z = x \pm 1$ , the ""problematic"" terms will look like $u(z)\delta^{(m)}(z)$ with $u(0) = 0$ . Now, for $m=3$ , the terms with issues are of the form $u(z)\delta(z)$ and $u(z)\delta'(z)$ using the properties of the Dirac's delta function shown on the Spanish version page of Wikipedia : $f(x)\delta'(x)=-f'(x)\delta(x)$ $x^n\delta(x) = 0,\,\forall n>0,\,x\in\mathbb{R}$ $h(x)\delta(x-a)=h(a)\delta(x-a)$ $h(x)\delta'(x-a)=h(a)\delta'(x-a)-h'(a)\delta(x-a)$ I believe it could be seen that the terms will vanish since it will behave as terms of the form $\{g(z)\to 0\}\cdot \delta(z)$ : this because of the property $z^n\delta(z)=0$ , and in this case $g(z)\to 0$ even faster than every possible $z^n$ , and if some terms $g(z)\delta(z)\delta(z)\cdots\delta(z)$ arises, they will also become zero since every left-side multiplication will become the term zero for the remaining terms. Unfortunately, I don´t know how to extend this for $m \geq 4$ since I don't know how to work with higher derivatives of the Dirac's delta function, which at least on Wikipedia are reviewed through Distribution Theory scope, for which I am completely ignorant. But, if the first property of Wikipedia is right, I believe that every product by the derivatives of the Dirac's delta function could be manipulated into standard  Dirac's delta functions, as example: $$\begin{array}{r c l} u(x)\delta'(x) & = & -u'(x)\delta(x) \iff \delta'(x) = -\frac{u'(x)}{u(x)}\delta(x) \\ \Rightarrow u(x)\delta''(x) & = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\delta(x)\right)  \\ & = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\right)\delta(x)-u'(x)\delta'(x) \\ & = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\right)\delta(x)+u'(x)\frac{u'(x)}{u(x)}\delta(x) \end{array}$$ So noting that $\frac{u^{(m)}(x)}{u(x)}\equiv \mathbb{P}(x)$ some polynomial on the variable $x$ , all the terms of these derivatives will be of the form $w(x)\mathbb{P(x)}\delta(x)$ for other polynomials $\mathbb{P}(x)$ (I am abusing of the notation), and the same procedure could be extended by construction to higher derivatives of $u(x)\delta^{(m)}(x)$ , so If this is right, the function $f(x)$ is keeping is smoothness on the points $x =\{-1,\,1\}$ , meaning $f(x)\in C_c^\infty$ inherited by the function $\exp\left(\frac{-x^2}{1-x^2}\right)$ . But since I cannot formally proving it better as I already explained (which is more an intuitive prove than a proper one - there are too many ""I believe""), I hope someone could confirm this through some theorem or valid method. 2nd thing added If the triangular function is defined as: $$\Lambda(x) = \frac{1}{2}\left(|1+x|+|1-x|-2|x| \right)$$ It looks like for bump functions $b(x)$ defined piecewise in $x \in [-1,\,1]$ , their domain could be extended by using $g(x) = b(x)\Lambda(x^n)$ with integer $n \geq 2$ . Above I have used $\Lambda(x^2) = 1-x^2+|1-x^2|$ . Also positive powers of $\Lambda^m(x^n), m\in\mathbb{Z}^+$ will work. 3rd added later - discussion about the definition of f(t) Due to enriching comments,answers, and explanations by chat, I have a better idea of what the issues are with the proposed function. Since I would like to focus the answers with the problem with the derivatives of the Dirac's Delta function $\delta^{(m)}()$ , in this section I do a brief review of the problem with the definition of the function on the points $x=\{-1,\,1\}$ . From what I have seen here , commonly bump functions are defines in open intervals as the example: $$q(x) = \begin{cases} e^{-\frac{x^2}{1-x^2}}, & |x|<1 \\ 0, & |x| \geq 1 \end{cases}$$ where in the points of the ""edges"" $\partial x = \{-1,\,1\}$ , since the function is matching the zero constant, to keep smoothness it also has to happen that $\lim\limits_{x \to \partial x^{\pm}} \frac{d^n}{dx^n}q(x) = 0,\,\forall n \leq 0,\,n \in \mathbb{z}$ , in other words, all its right and left side derivatives at the edges must match and been equal to $0$ . So far so good, but, seeing the Wikipedia page for Compact support it is said that for a function defined in an open interval $(-1,\,1)$ , its support it still being $[-1,\,1]$ , so I think is like being ""cheating"": the function domain have two points where is undefined by definition, so is discontinuous there (I think), instead of being defined on $[-1,\,1]$ and having the issue of being undefined because of dividing by zero on the exponent. However, about this forbidden division (which is mentioned by @blamethelag), there is an issue: following Wolfram-Alpha , the limit of $q(x)$ at the edges does not exist, and neither are equal their left and right side limits (solved because of the open interval definition I believe), but the same analysis for the function $f(x)$ shows that actually it is not only having identical right and left hand sides limits (which is the standard way of extending a function), following Wolfram-Alpha the limits at exactly the edges exists and are also zero, so If W-A is right, actually the function $f(x)$ is fulfilling the continuity definition $\lim\limits_{x \to c} f(x) = f(c)$ , so if this is right, the function $f(t)$ should be properly defined as a function. But since it must also fulfill the existence of all its derivatives to be a smooth function, here is where the derivatives of the Dirac's Delta function $\delta^{(m)}()$ could be doing a mess, and its where I am worried about. Also another discussion was an opinion received in other question here : @CalvinKhor has correctly noted that the function $f(x)$ is still defined piecewise since the absolute value function by definition is defined piecewise. There is no way to refute this, but I believe that in the spirit of the question it is still a function not defined piecewise because of the following: as @blamethelag noted, if I work with the piecewise section within $[-1,\,1]$ , the differential equation for the function $f(x)$ will be defined by: $$\frac{f'(x)}{f(x)}=\frac{-2x}{(1-x^2)^2}$$ , which solution will be behaving very different from being zero outside the interval $[-1,\,1]$ . But differently from it, using the definition of $f(x)$ presented here I was able to found the differential equation: $$\frac{f'(x)}{f(x)}=\frac{-2x}{(1-x^2)^2}+\frac{-2x}{|1-x^2|}$$ which, If I am right , it will be describing a function that actually behaves as the zero function outside the interval $[-1,\,1]$ . Note aside If I have a function $y(x) = e^{p(x)}(1-x^2+|1-x^2|)$ , using point (3) its trivial differential equation will be given by: $$\frac{y'(x)}{y(x)} = \frac{d}{dx}p(x)-\frac{2x}{|1-x^2|}$$ So I was trying to found some $p(x)$ that fulfills: $$y(x)\left(\frac{d}{dx}p(x)-\frac{2x}{|1-x^2|}\right) = 2y(2x+1)-2y(2x-1)$$ for matching the solution with an already known result, this unsuccessfully, but maybe someone else have an idea of how to made the matching... Or maybe found some $p(x)$ so the differential equation take an autonomous form $f'(x) = G(f(x))$ with $G(x)$ at least $C^1$ almost-everywhere . English language is not native for me, so probably this have a lot of mistakes: my apologies in advance","Is the function a bump-function ? Which autonomous differential equation it fulfill? (note it is not defined piece-wise) I was viewing this question on MSE and there was given here a pretty clever answer for a compact-supported function that is not defined piece-wise, and I was trying to us it method to built a proper bump function in following the definition of Wikipedia (a compact-supported smooth function ) that is also not defined piece-wise. After playing a while with it on Wolfram-Alpha and Desmos , I believe that: it is actually a bump function since their derivatives will be polynomials multiplied again by , which will keep their functions smooth and keeping smoothness at the boundaries of their non-zero values , but my math skills aren´t enough to prove it, so far, I was able only to check the first 6th derivatives and it looks fine when plotted, without having discontinuities (I am worried of the hidden function which derivatives becomes Dirac's Delta functions ), neither changing their starting and ending values from zero ( example ). Since the function as intended, I think it is already a smooth function on these points so it stills being a bump-function (if I am right, the only possible analytic function of compact-support is the zero function, so I believe that piecewise zero sections are smooth given they are analytic), and within it still being smooth since the term will dominate any polynomial, but in the points I don't know if will dominate any derivative of the Dirac's delta function. Also I would like to know which autonomous differential equation with at least almost-everywhere will have as solution, maybe a non linear ODE, or a Delay Differential Equation (DDE) as it was shown here for another bump-function (so far is the only example I know): there is found that a bump-function which fulfill when the solution is only non-zero inside , but unfortunately there is no closed-form for this function, but its Fourier Transform is show to be .... actually my main objective is to find a finite-duration solution to a differential equation that fulfill what is said in this paper Finite Time Differential Equations ... I have the Motivation for this questions explained in this another question . So summarizing: Is ? Which autonomous differential equation fulfill as solution? Otherwise, proving it cannot be described through an autonomous diff. eq. is also welcome. My attempts so far... I believe that the diff. eq. found by Wolfram-Alpha is wrong: But I have had a lot of struggle dealing with the signs, so first, to see if I am not making mistakes, I will list a few things I am using for which I am not sure if they are formally right. First, to show explicitly where I have risks for discontinuities, I have made transparent that an absolute value function, so acquire the form Second, for keeping tractable the sign issues and maintain unchanged the solutions of the diff. equation, I have found I have to keep , even when both plotted look the same (honestly for easy-hand-calculation I was using that a lot before). When working with the function , I will have that , but when integrating on Wolfram-Alpha it shows a completely different thing, and even so, when derivating the W-A result it didn´t shows to be (even the results has Dirac's delta functions involved). So hereinafter, I will be using Also, noting that In all the manipulations I am ignoring possible problems because of dividing by at the points (they will arise on the topic later). Using these things, I believe that ""a true"" diff. equation for (but not-autonomous ) is: From here, this is equivalent to: Since , I tried to integrate the left-hand-side of Eq. 2 on Wolfram-Alpha but it was unable to find and antiderivative, but noting that: integrating both fractions and using point (3) I will have that: From where it can be seen that by applying both side the function I will recover for , so I believe Eq. 1 is right. As @blamethelag reccomend on the answer I tried to find a recurrence relation for Eq. 2 using the General Leibniz formula : But I wasn´t able to find and useful formula (I get stuck). But using this formula jointly with the Faà di Bruno's formula for a function composed with an exponential: where is the  nth complete exponential Bell polynomial . With these, I tried to expand the nth derivative of f(x): Here, I believe that the term is going to ""dominate"" every possible polynomial since it already done it for the classical example of a bump function given by But I am worried about the other part of the equation, since will make appear Dirac's Delta functions and derivatives of it (the following derivatives were solved by wolfram-alpha, by hand I have differences so maybe I am doing something wrong): And I don´t know if these Dirac's delta functions and its derivatives are actually helping by ""killing"" things outside , or instead are  ruining the smoothness of on the points , discarding the hypothesis of the smoothness of (and it is not seen on the plots because happens only in two zero-measure points ). This is why I worried about the smoothness, since for every other points I think is granted by . At least for the case , where the terms and , Wolfram-Alpha is able to take the limits giving zero value, and it match with their right and left side limits in Wolfram-Alpha here and here . By expanding the terms of Eq. 3 you will have things of the form: where since will ""win"" any polynomial on , I will have that , so by calling and , the ""problematic"" terms will look like with . Now, for , the terms with issues are of the form and using the properties of the Dirac's delta function shown on the Spanish version page of Wikipedia : I believe it could be seen that the terms will vanish since it will behave as terms of the form : this because of the property , and in this case even faster than every possible , and if some terms arises, they will also become zero since every left-side multiplication will become the term zero for the remaining terms. Unfortunately, I don´t know how to extend this for since I don't know how to work with higher derivatives of the Dirac's delta function, which at least on Wikipedia are reviewed through Distribution Theory scope, for which I am completely ignorant. But, if the first property of Wikipedia is right, I believe that every product by the derivatives of the Dirac's delta function could be manipulated into standard  Dirac's delta functions, as example: So noting that some polynomial on the variable , all the terms of these derivatives will be of the form for other polynomials (I am abusing of the notation), and the same procedure could be extended by construction to higher derivatives of , so If this is right, the function is keeping is smoothness on the points , meaning inherited by the function . But since I cannot formally proving it better as I already explained (which is more an intuitive prove than a proper one - there are too many ""I believe""), I hope someone could confirm this through some theorem or valid method. 2nd thing added If the triangular function is defined as: It looks like for bump functions defined piecewise in , their domain could be extended by using with integer . Above I have used . Also positive powers of will work. 3rd added later - discussion about the definition of f(t) Due to enriching comments,answers, and explanations by chat, I have a better idea of what the issues are with the proposed function. Since I would like to focus the answers with the problem with the derivatives of the Dirac's Delta function , in this section I do a brief review of the problem with the definition of the function on the points . From what I have seen here , commonly bump functions are defines in open intervals as the example: where in the points of the ""edges"" , since the function is matching the zero constant, to keep smoothness it also has to happen that , in other words, all its right and left side derivatives at the edges must match and been equal to . So far so good, but, seeing the Wikipedia page for Compact support it is said that for a function defined in an open interval , its support it still being , so I think is like being ""cheating"": the function domain have two points where is undefined by definition, so is discontinuous there (I think), instead of being defined on and having the issue of being undefined because of dividing by zero on the exponent. However, about this forbidden division (which is mentioned by @blamethelag), there is an issue: following Wolfram-Alpha , the limit of at the edges does not exist, and neither are equal their left and right side limits (solved because of the open interval definition I believe), but the same analysis for the function shows that actually it is not only having identical right and left hand sides limits (which is the standard way of extending a function), following Wolfram-Alpha the limits at exactly the edges exists and are also zero, so If W-A is right, actually the function is fulfilling the continuity definition , so if this is right, the function should be properly defined as a function. But since it must also fulfill the existence of all its derivatives to be a smooth function, here is where the derivatives of the Dirac's Delta function could be doing a mess, and its where I am worried about. Also another discussion was an opinion received in other question here : @CalvinKhor has correctly noted that the function is still defined piecewise since the absolute value function by definition is defined piecewise. There is no way to refute this, but I believe that in the spirit of the question it is still a function not defined piecewise because of the following: as @blamethelag noted, if I work with the piecewise section within , the differential equation for the function will be defined by: , which solution will be behaving very different from being zero outside the interval . But differently from it, using the definition of presented here I was able to found the differential equation: which, If I am right , it will be describing a function that actually behaves as the zero function outside the interval . Note aside If I have a function , using point (3) its trivial differential equation will be given by: So I was trying to found some that fulfills: for matching the solution with an already known result, this unsuccessfully, but maybe someone else have an idea of how to made the matching... Or maybe found some so the differential equation take an autonomous form with at least almost-everywhere . English language is not native for me, so probably this have a lot of mistakes: my apologies in advance","f(x)=\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}e^{-\frac{x^2}{1-x^2}} \in C_c^\infty q(x) = 1-\sqrt{x^2}+\sqrt{\left(1-\sqrt{x^2}\right)^2} C_c^\infty f(x)=\textstyle{\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}}{\Large e}{\begin{array}\displaystyle{ -\frac{x^2}{1-x^2}} \\ \\ \\ \end{array}} \in C_c^\infty f(x) \partial x = [-1,\,1] \text{abs}(1-x^2) \delta(x \pm 1) f(x)=0,\,|x|>1 \in C_c^\infty |x|<1 e^{-\frac{x^2}{1-x^2}} x=\{-1,\,1\} f'(x) = G(f(x)) G(x) C^1 f(x) \varphi'(t)=2\left(\varphi(2t+1)-\varphi(2t-1)\right) [-1;\,1] \hat{\varphi}(\omega) = \prod\limits_{k=0}^{\infty}\frac{\sin\left(\frac{\pi\omega}{2^k}\right)}{\frac{\pi\omega}{2^k}} f(x) \in C_c^\infty f(x) f'(x)\left(1-x^2\right)^2+f(x)(4x-2x^3)=0,\qquad f(0)=1 \sqrt{(1-x^2)^2} \equiv |1-x^2| f(x) f(x) = e^{-\frac{-x^2}{1-x^2}}\frac{(1-x^2+|1-x^2|)}{2} \frac{1}{\text{sgn}(a)} \neq \text{sgn}(a) s(x) = \log\left(1-x^2+|1-x^2|\right) s'(x) = -\frac{2x}{|1-x^2|} -\frac{2x}{|1-x^2|} s'(x)  \int -\frac{2x}{|1-x^2|} dx = \log\left(1-x^2+|1-x^2|\right) + \mathcal{c} 1-x^2+|1-x^2| \equiv |1-x^2|\left(1+\text{sgn}(1-x^2)\right) 0 x=\{-1,\,1\} f(x) f'(x)\left(1-x^2\right)^2+2x(1+|1-x^2|)f(x) = 0\qquad \text{Eq. 1} \frac{f'(x)}{f(x)}=\frac{-2x(1+|1-x^2|)}{\left(1-x^2\right)^2}\qquad \text{Eq. 2} f'(x)/f(x) = \frac{d}{dx}\left(\log(f(x))\right) \frac{-2x(1+|1-x^2|)}{\left(1-x^2\right)^2} = \frac{-2x}{\left(1-x^2\right)^2}+\frac{-2x|1-x^2|}{\left(1-x^2\right)^2} =\frac{-2x}{\left(1-x^2\right)^2}+\frac{-2x}{|1-x^2|} \Rightarrow \log(f(x)) = \frac{-x^2}{(1-x^2)}+\log(1-x^2+|1-x^2|)+\mathcal{c}  \exp() f(x) c=-\log(2) \left(fg\right)^{(n)} = \sum\limits_{k=0}^{n} {n \choose k}f^{(k)}g^{(n-k)}  \left(e^{g}\right)^{(n)} = e^{g}B_n(g',g'',\cdots, g^{(n)}) B_n() \begin{array}{r c l}
\frac{d^n}{dx^n} f(x) & = & \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n {n \choose k} \frac{d^k}{dx^k}\left(e^{-\frac{x^2}{1-x^2}} \right)\frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\
& = & \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n {n \choose k} e^{-\frac{x^2}{1-x^2}}B_k\left(\frac{d}{dx}\left(\frac{-x^2}{1-x^2}\right),\frac{d^2}{dx^2}\left(\frac{-x^2}{1-x^2}\right),\cdots,\frac{d^k}{dx^k}\left(\frac{-x^2}{1-x^2}\right) \right) \frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\
& = & e^{-\frac{x^2}{1-x^2}} \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n \underbrace{{n \choose k} B_k\left(\frac{d}{dx}\left(\frac{-x^2}{1-x^2}\right),\frac{d^2}{dx^2}\left(\frac{-x^2}{1-x^2}\right),\cdots,\frac{d^k}{dx^k}\left(\frac{-x^2}{1-x^2}\right) \right)}_{\mathbb{P}_{n,k}(x)} \frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\
& = & e^{-\frac{x^2}{1-x^2}} \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n \mathbb{P}_{n,k}(x) \left( \frac{d^m}{dx^m}\left(1\right)+\frac{d^m}{dx^m}\left(-x^2\right)+\frac{d^m}{dx^m}\left(|1-x^2|\right) \right) \qquad \text{Eq.3}
\end{array} e^{\frac{-x^2}{1-x^2}} \mathbb{P}_{n,k}(x) \in C_c^\infty r(x) = \begin{cases} e^{\frac{-x^2}{1-x^2}},\,|x|\leq 1\\ 0,\,\text{otherwise}\end{cases} \frac{d^m}{dx^m}\left(|1-x^2|\right) \begin{array}{l c l}
m = 0 & \rightarrow & |1-x^2| \\
m = 1 & \rightarrow & -2x\,\text{sgn}(1-x^2)\\
m = 2 & \rightarrow & -2\,\text{sgn}(1-x^2)+4\delta(x+1)+4\delta(x-1) \\
m = 3 & \rightarrow & 4\delta(x+1)+4\delta(x-1)+4\delta'(x+1)+4\delta'(x-1)\\
m = p\geq 4 & \rightarrow & 4 \left(\frac{d^{p-3}}{dx^{p-3}}+\frac{d^{p-2}}{dx^{p-2}}\right)\Big(\delta(x+1)+\delta(x-1)\Big)\\
\end{array} x=\{-1,\,1\} r(x) x=\{-1,\,1\} f(x) x=\{-1,\,1\} r(x) m=3 \delta() \delta'() x \to 1 x \to 1^{\pm} e^{-x^2/(1-x^2)}\mathbb{P}(x)\delta^{(m)}(x\pm 1) w(x)=e^{-x^2/(1-x^2)} x \to \pm 1 w(x)\mathbb{P}(x)=0,\,x\to\pm 1 u(x \pm 1) =  w(x \pm 1)\mathbb{P}(x\pm 1) z = x \pm 1 u(z)\delta^{(m)}(z) u(0) = 0 m=3 u(z)\delta(z) u(z)\delta'(z) f(x)\delta'(x)=-f'(x)\delta(x) x^n\delta(x) = 0,\,\forall n>0,\,x\in\mathbb{R} h(x)\delta(x-a)=h(a)\delta(x-a) h(x)\delta'(x-a)=h(a)\delta'(x-a)-h'(a)\delta(x-a) \{g(z)\to 0\}\cdot \delta(z) z^n\delta(z)=0 g(z)\to 0 z^n g(z)\delta(z)\delta(z)\cdots\delta(z) m \geq 4 \begin{array}{r c l}
u(x)\delta'(x) & = & -u'(x)\delta(x) \iff \delta'(x) = -\frac{u'(x)}{u(x)}\delta(x) \\
\Rightarrow u(x)\delta''(x) & = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\delta(x)\right)  \\
& = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\right)\delta(x)-u'(x)\delta'(x) \\
& = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\right)\delta(x)+u'(x)\frac{u'(x)}{u(x)}\delta(x)
\end{array} \frac{u^{(m)}(x)}{u(x)}\equiv \mathbb{P}(x) x w(x)\mathbb{P(x)}\delta(x) \mathbb{P}(x) u(x)\delta^{(m)}(x) f(x) x =\{-1,\,1\} f(x)\in C_c^\infty \exp\left(\frac{-x^2}{1-x^2}\right) \Lambda(x) = \frac{1}{2}\left(|1+x|+|1-x|-2|x| \right) b(x) x \in [-1,\,1] g(x) = b(x)\Lambda(x^n) n \geq 2 \Lambda(x^2) = 1-x^2+|1-x^2| \Lambda^m(x^n), m\in\mathbb{Z}^+ \delta^{(m)}() x=\{-1,\,1\} q(x) = \begin{cases} e^{-\frac{x^2}{1-x^2}}, & |x|<1 \\ 0, & |x| \geq 1 \end{cases} \partial x = \{-1,\,1\} \lim\limits_{x \to \partial x^{\pm}} \frac{d^n}{dx^n}q(x) = 0,\,\forall n \leq 0,\,n \in \mathbb{z} 0 (-1,\,1) [-1,\,1] [-1,\,1] q(x) f(x) f(x) \lim\limits_{x \to c} f(x) = f(c) f(t) \delta^{(m)}() f(x) [-1,\,1] f(x) \frac{f'(x)}{f(x)}=\frac{-2x}{(1-x^2)^2} [-1,\,1] f(x) \frac{f'(x)}{f(x)}=\frac{-2x}{(1-x^2)^2}+\frac{-2x}{|1-x^2|} [-1,\,1] y(x) = e^{p(x)}(1-x^2+|1-x^2|) \frac{y'(x)}{y(x)} = \frac{d}{dx}p(x)-\frac{2x}{|1-x^2|} p(x) y(x)\left(\frac{d}{dx}p(x)-\frac{2x}{|1-x^2|}\right) = 2y(2x+1)-2y(2x-1) p(x) f'(x) = G(f(x)) G(x) C^1","['ordinary-differential-equations', 'solution-verification', 'special-functions', 'smooth-functions', 'finite-duration']"
62,Find power series solution for $y''-xy=0$ close to $x=0$.,Find power series solution for  close to .,y''-xy=0 x=0,"Find power series solution for $y''-xy=0$ close to $x=0$ . $\sum_{n=2}^\infty n(n-1)a_nx^{n-2}-x\sum_{n=0}^\infty a_nx^n=0$ Then, $a_{n+2}(n+2)(n+1)-a_{n-1}=0 \implies a_{n+2}(n^2+3n+2)-a_{n-1}=0 \implies a_{n+3}=a_n\cdot \frac{1}{(n+3)(n+2)}$ I got two problems: $(1)$ The answer is $a_{n+3}=(n+3)(n+2)a_n$ , where am I wrong ? $(2)$ How can I find the general power series solution ?","Find power series solution for close to . Then, I got two problems: The answer is , where am I wrong ? How can I find the general power series solution ?",y''-xy=0 x=0 \sum_{n=2}^\infty n(n-1)a_nx^{n-2}-x\sum_{n=0}^\infty a_nx^n=0 a_{n+2}(n+2)(n+1)-a_{n-1}=0 \implies a_{n+2}(n^2+3n+2)-a_{n-1}=0 \implies a_{n+3}=a_n\cdot \frac{1}{(n+3)(n+2)} (1) a_{n+3}=(n+3)(n+2)a_n (2),['ordinary-differential-equations']
63,How many solutions does the ODE have?,How many solutions does the ODE have?,,The question : Given ODE : $$ \begin{cases} y'-a^2(y')^3-\frac{\sin(x)}{x+y}=0 \\ y(0)=1 \end{cases} $$ Write how many solutions does the system have for $a=0$ and $a \ne 0$ . My try : for $a=0$ we have a unique solution by Picard's theorem. In the case where $a \ne 0$ I'm stuck and can't understand...,The question : Given ODE : Write how many solutions does the system have for and . My try : for we have a unique solution by Picard's theorem. In the case where I'm stuck and can't understand...,"
\begin{cases}
y'-a^2(y')^3-\frac{\sin(x)}{x+y}=0 \\
y(0)=1
\end{cases}
 a=0 a \ne 0 a=0 a \ne 0",['ordinary-differential-equations']
64,Arriving at a particular solution of the ODE $y''-2y'-2y=\sin x$,Arriving at a particular solution of the ODE,y''-2y'-2y=\sin x,"If we have an ODE: $(D^2-2D-2)y=\sin x$ then it may be observed that $y=\frac 1{13}(2\cos x -3\sin x)$ is a particular solution of the ODE. It can be found using undetermined coefficients method. I'm trying to find the particular solution using operator's method. I did the following: Let $a:=1+\sqrt 3, b:=1-\sqrt 3$ so that $D^2-2D-2=(D-a)(D-b)$ . \begin{align*} y&=\frac 1{D-a}.\frac 1{D-b}\sin x\\&=\frac 1{2(D-a)}\Re\left[\frac 1{iD-ib}.(e^{ix}-e^{-ix})\right]\\&=\frac 1{2(D-a)}.\Re\left[\frac 1{-1-ib}e^{ix}+\frac 1{-1+ib}e^{-ix}\right]\\&=\frac 1{2(D-a)}\Re\left[ -2 \Re\left(\frac{e^{ix}}{1+ib}\right) \right]\\&=\frac {-2}{1+b^2}.\frac 1{2(D-a)}(\cos x+b\sin x). \end{align*} etc. Continuation is possible but this is getting tedious. I saw the following solution then and in the solution, I didn't understand the red highlighted parts: \begin{align*} y&=\frac 1{\color{red}{D^2}-2D-2} \sin x\\&=\frac 1{\color {red}{-1}-2D-2}\sin x\\& =\frac 1{-2D-3}\sin x=-\frac 1{2D+3}\sin x\\&=-\frac {2D-3}{4\color{red}{D^2}-9}\sin x=-\frac {2D-3}{4(\color{red}{-1})-9}\sin x=\frac 1{13} (2D-3)\sin x. \end{align*} I tried to prove the red highlighted part as follows: Suppose that in general we have $p(D)y=\sin x$ , where $p(D)$ is a polynomial operator in $D$ . If $p$ has only even degree terms then: $D^2\sin x=(-1)\sin x,..., (D^2)^k\sin x=(-1)^k\sin x$ . So $p(D)\sin x=p(-1)\sin x\implies \frac {\sin x}{p(D)}=\frac {\sin x}{p(-1)}$ , assuming $p(-1)\ne 0$ . It follows that $y=\color{blue}{\frac{\sin x}{p(-1)}}$ . Can anyone please help me to establish the result similar to the one highlighted in blue, in case $p$ happens to contain odd powered terms along with the even powered ones? Thanks.","If we have an ODE: then it may be observed that is a particular solution of the ODE. It can be found using undetermined coefficients method. I'm trying to find the particular solution using operator's method. I did the following: Let so that . etc. Continuation is possible but this is getting tedious. I saw the following solution then and in the solution, I didn't understand the red highlighted parts: I tried to prove the red highlighted part as follows: Suppose that in general we have , where is a polynomial operator in . If has only even degree terms then: . So , assuming . It follows that . Can anyone please help me to establish the result similar to the one highlighted in blue, in case happens to contain odd powered terms along with the even powered ones? Thanks.","(D^2-2D-2)y=\sin x y=\frac 1{13}(2\cos x -3\sin x) a:=1+\sqrt 3, b:=1-\sqrt 3 D^2-2D-2=(D-a)(D-b) \begin{align*}
y&=\frac 1{D-a}.\frac 1{D-b}\sin x\\&=\frac 1{2(D-a)}\Re\left[\frac 1{iD-ib}.(e^{ix}-e^{-ix})\right]\\&=\frac 1{2(D-a)}.\Re\left[\frac 1{-1-ib}e^{ix}+\frac 1{-1+ib}e^{-ix}\right]\\&=\frac 1{2(D-a)}\Re\left[ -2 \Re\left(\frac{e^{ix}}{1+ib}\right) \right]\\&=\frac {-2}{1+b^2}.\frac 1{2(D-a)}(\cos x+b\sin x).
\end{align*} \begin{align*}
y&=\frac 1{\color{red}{D^2}-2D-2} \sin x\\&=\frac 1{\color {red}{-1}-2D-2}\sin x\\& =\frac 1{-2D-3}\sin x=-\frac 1{2D+3}\sin x\\&=-\frac {2D-3}{4\color{red}{D^2}-9}\sin x=-\frac {2D-3}{4(\color{red}{-1})-9}\sin x=\frac 1{13} (2D-3)\sin x.
\end{align*} p(D)y=\sin x p(D) D p D^2\sin x=(-1)\sin x,..., (D^2)^k\sin x=(-1)^k\sin x p(D)\sin x=p(-1)\sin x\implies \frac {\sin x}{p(D)}=\frac {\sin x}{p(-1)} p(-1)\ne 0 y=\color{blue}{\frac{\sin x}{p(-1)}} p","['real-analysis', 'calculus', 'ordinary-differential-equations']"
65,4th order linear ordinary differential equation,4th order linear ordinary differential equation,,"While I was solving an integral using Feynman Integration, I came across the following differential equation: $$y’’’’-y’’+y=0$$ I tried substituting $y$ with an exponential function which failed. Can someone else show me how to solve it?","While I was solving an integral using Feynman Integration, I came across the following differential equation: I tried substituting with an exponential function which failed. Can someone else show me how to solve it?",y’’’’-y’’+y=0 y,['ordinary-differential-equations']
66,Solve the heat equation using a transform method,Solve the heat equation using a transform method,,"I need to solve $k\frac{\partial^2U}{\partial x^2}=\frac{\partial U}{\partial t}$ subject to \begin{equation} U(0,t)=1, t>0 \\ U(x,0)=e^{-x},x>0 \end{equation} I tried using the Laplace transform with respect to $t$ since the function is defined for $t>0$ and we have the initial condition $U(x,0)=e^{-x}, x>0$ . My problem is that I'm left with the complicated function: \begin{equation} c_{1}\cos\left(\sqrt{\frac{s}{k}}x\right) +c_{2}\sin\left(\sqrt{\frac{s}{k}}x\right)+ \frac{k}{s-k}e^{-x} \end{equation} which I don't know how to invert. I was wondering if there's any other way to approach this problem (can you apply some other transform? Is there anything I'm not seeing?). I need some guidance urgently since I'm preparing for a final exam. Thank you so much for your help!",I need to solve subject to I tried using the Laplace transform with respect to since the function is defined for and we have the initial condition . My problem is that I'm left with the complicated function: which I don't know how to invert. I was wondering if there's any other way to approach this problem (can you apply some other transform? Is there anything I'm not seeing?). I need some guidance urgently since I'm preparing for a final exam. Thank you so much for your help!,"k\frac{\partial^2U}{\partial x^2}=\frac{\partial U}{\partial t} \begin{equation}
U(0,t)=1, t>0 \\ U(x,0)=e^{-x},x>0
\end{equation} t t>0 U(x,0)=e^{-x}, x>0 \begin{equation}
c_{1}\cos\left(\sqrt{\frac{s}{k}}x\right) +c_{2}\sin\left(\sqrt{\frac{s}{k}}x\right)+ \frac{k}{s-k}e^{-x}
\end{equation}","['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis']"
67,Computing Wronskian when functions are not given [closed],Computing Wronskian when functions are not given [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question If $y_1,y_2$ are solutions of $y''+\sin(t)y'+e^ty=0$ , and $y_1(0)=2,y_1'(0)=-3, y_2(0)=3, y_2'(0)=4$ , compute $W[y_1,y_2](-5\pi)$ . I don't know how to calculate this wronskian without finding $y_1,y_2$ explicitly. Any idea?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question If are solutions of , and , compute . I don't know how to calculate this wronskian without finding explicitly. Any idea?","y_1,y_2 y''+\sin(t)y'+e^ty=0 y_1(0)=2,y_1'(0)=-3, y_2(0)=3, y_2'(0)=4 W[y_1,y_2](-5\pi) y_1,y_2","['ordinary-differential-equations', 'wronskian']"
68,Solution to a system of ODE:s,Solution to a system of ODE:s,,"Consider a system of linear ODE:s: $$x'(t)=Ax(t)+C$$ Here $A$ and $C$ are coefficient matrices. If $C=0$ , the solution is $x(t)=\exp(Ax(t))x(0)$ , where $\exp$ represents the matrix exponential. Can we write a similar solution for a general $C$ ?","Consider a system of linear ODE:s: Here and are coefficient matrices. If , the solution is , where represents the matrix exponential. Can we write a similar solution for a general ?",x'(t)=Ax(t)+C A C C=0 x(t)=\exp(Ax(t))x(0) \exp C,"['ordinary-differential-equations', 'systems-of-equations']"
69,Reduce to first order and solve $yy'' = 3y'^2$,Reduce to first order and solve,yy'' = 3y'^2,"Reduce to first order and solve $yy'' = 3y'^2$ Dividing both sides by $y$ and re-arranging $y'' - 3y' = 0$ . This is clearly an homogenous equation, a solution might be $e^{3x}$ to test this: $y' = 3e^{3x}, y'' = 9e^{3x} \implies 9e^x-9e^x = 0$ Substituting $y = ue^{3x}, y' = u'e^{3x}+3ue^{3x}, y'' = u''e^{3x}+3u'e^{3x}+3u'e^{3x}+9ue^{3x}$ Plugging these in $$(u''e^{3x}+3u'e^{3x}+3u'e^{3x}+9ue^{3x})-3\cdot (u'e^{3x}+3ue^{3x})=0$$ $$\implies u''e^{3x}+3u'e^{3x}=0$$ Substituting $v = u'$ $$\implies v'e^{3x}+3ve^{3x}=0$$ $$=\int\frac{dv}{v} = 3\int dx$$ However the solution to this exercise is $(c_1x+c_2)^{-\frac{1}{2}}$ , how did they get this?","Reduce to first order and solve Dividing both sides by and re-arranging . This is clearly an homogenous equation, a solution might be to test this: Substituting Plugging these in Substituting However the solution to this exercise is , how did they get this?","yy'' = 3y'^2 y y'' - 3y' = 0 e^{3x} y' = 3e^{3x}, y'' = 9e^{3x} \implies 9e^x-9e^x = 0 y = ue^{3x}, y' = u'e^{3x}+3ue^{3x}, y'' = u''e^{3x}+3u'e^{3x}+3u'e^{3x}+9ue^{3x} (u''e^{3x}+3u'e^{3x}+3u'e^{3x}+9ue^{3x})-3\cdot (u'e^{3x}+3ue^{3x})=0 \implies u''e^{3x}+3u'e^{3x}=0 v = u' \implies v'e^{3x}+3ve^{3x}=0 =\int\frac{dv}{v} = 3\int dx (c_1x+c_2)^{-\frac{1}{2}}","['ordinary-differential-equations', 'multivariable-calculus']"
70,can global stability be inferred by the unique existence of stable equilibrium?,can global stability be inferred by the unique existence of stable equilibrium?,,"For N-dimensional dynamical systems, if we know there only exist one stable equilibrium (but whether it is global or local is not yet proved), can we infer, the stable equilibrium is globally stable? Thank you in advance for any response!","For N-dimensional dynamical systems, if we know there only exist one stable equilibrium (but whether it is global or local is not yet proved), can we infer, the stable equilibrium is globally stable? Thank you in advance for any response!",,"['ordinary-differential-equations', 'dynamical-systems']"
71,"Solving $(x^2 +1)y''+2xy=0, y(0)=1, y'(0)=1$ by power series",Solving  by power series,"(x^2 +1)y''+2xy=0, y(0)=1, y'(0)=1","I have to resolve this differential equation: \begin{eqnarray*} (x^2 +1)y''+2xy=0, \hspace{1cm}y(0)=1, y'(0)=1 \end{eqnarray*} by power series. So I know that: \begin{eqnarray*} y&=&\sum_{n=0}^{\infty}c_{n}x^{n}\\ y'&=&\sum_{n=1}^{\infty}nc_{n}x^{n-1}\\ y''&=& \sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2} \end{eqnarray*} Then \begin{eqnarray*} (x^2 +1)\sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2}+2x\sum_{n=0}^{\infty}c_{n}x^{n}&=&0\\  \sum_{n=2}^{\infty}n(n-1)c_{n}x^{n}+\sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2}+\sum_{n=0}^{\infty}2c_{n}x^{n+1}&=&0\\ \end{eqnarray*} So I do the substitution $k=n-2$ and $k=n+1$ and I have \begin{eqnarray*}  \sum_{k=2}^{\infty}k(k-1)c_{k}x^{k}+\sum_{k=0}^{\infty}(k+2)(k+1)c_{k+2}x^{k}+\sum_{k=1}^{\infty}2c_{k-1}x^{k}&=&0\\ 2c_{2}+6c_{3}x+2c_{0}x+\sum_{k=2}^{\infty}[k(k-1)c_{k}+(k+2)(k+1)c_{k+2}+2c_{k-1}]x^{k}&=&0 \end{eqnarray*} But in general: \begin{eqnarray*}  k(k-1)c_{k}+(k+2)(k+1)c_{k+2}+2c_{k-1}=0 \end{eqnarray*} And I don't know how can I continue, Cause I can't find $c_{1}$ and in general I don't know how can I write $c_{k}$ . I'm stuck. Do you know how can I continue? Can you give some hint to continue? Thank you.","I have to resolve this differential equation: by power series. So I know that: Then So I do the substitution and and I have But in general: And I don't know how can I continue, Cause I can't find and in general I don't know how can I write . I'm stuck. Do you know how can I continue? Can you give some hint to continue? Thank you.","\begin{eqnarray*}
(x^2 +1)y''+2xy=0, \hspace{1cm}y(0)=1, y'(0)=1
\end{eqnarray*} \begin{eqnarray*}
y&=&\sum_{n=0}^{\infty}c_{n}x^{n}\\
y'&=&\sum_{n=1}^{\infty}nc_{n}x^{n-1}\\
y''&=& \sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2}
\end{eqnarray*} \begin{eqnarray*}
(x^2 +1)\sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2}+2x\sum_{n=0}^{\infty}c_{n}x^{n}&=&0\\
 \sum_{n=2}^{\infty}n(n-1)c_{n}x^{n}+\sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2}+\sum_{n=0}^{\infty}2c_{n}x^{n+1}&=&0\\
\end{eqnarray*} k=n-2 k=n+1 \begin{eqnarray*}
 \sum_{k=2}^{\infty}k(k-1)c_{k}x^{k}+\sum_{k=0}^{\infty}(k+2)(k+1)c_{k+2}x^{k}+\sum_{k=1}^{\infty}2c_{k-1}x^{k}&=&0\\
2c_{2}+6c_{3}x+2c_{0}x+\sum_{k=2}^{\infty}[k(k-1)c_{k}+(k+2)(k+1)c_{k+2}+2c_{k-1}]x^{k}&=&0
\end{eqnarray*} \begin{eqnarray*}
 k(k-1)c_{k}+(k+2)(k+1)c_{k+2}+2c_{k-1}=0
\end{eqnarray*} c_{1} c_{k}","['ordinary-differential-equations', 'power-series']"
72,"$y' = 3y^{\frac{2}{3}}$ non unique solution at $(1,1)$?",non unique solution at ?,"y' = 3y^{\frac{2}{3}} (1,1)","When considering the differential equation $y' = 3y^{\frac{2}{3}}$ , I can find solutions $y = (t-a)^3, t > a$ and $y= (t-b)^3, t < b$ . Also $y = 0$ is a solution. I am suppose to show in reference to these two solutions that there are infinite number of solutions to this differential equation satisfying $y(1) = 1$ , and why this does not contradict existence and uniqueness. Problem is, for $t > 0, y > 0$ $f$ and $f_y$ are continuous, so in the domain $y > 0$ the solution should be unique. Could someone explain my flaw in logic>","When considering the differential equation , I can find solutions and . Also is a solution. I am suppose to show in reference to these two solutions that there are infinite number of solutions to this differential equation satisfying , and why this does not contradict existence and uniqueness. Problem is, for and are continuous, so in the domain the solution should be unique. Could someone explain my flaw in logic>","y' = 3y^{\frac{2}{3}} y = (t-a)^3, t > a y= (t-b)^3, t < b y = 0 y(1) = 1 t > 0, y > 0 f f_y y > 0","['ordinary-differential-equations', 'initial-value-problems']"
73,"Why do particular solutions of ordinary, non-homogeneous differential equations not contains arbitrary constants, but homogeneous solutions do?","Why do particular solutions of ordinary, non-homogeneous differential equations not contains arbitrary constants, but homogeneous solutions do?",,"I've been solving ordinary linear differential equations for a while. First order and second order mostly, utilizing a slew of methods and theorems to find particular and complementary solutions. I still don't quite understand the intuition behind the fact that when I find a solution to a non-homogeneous equation it doesn't contain arbitrary constants like what happens when I find a general solution to the associated homogeneous equation. In the latter case, I can show that every solution to the homogeneous equation is a linear combination of n linearly independent solutions to the same equation, where n is the order of the (differential) equation. One of such linear combinations is a particular solution to the homogeneous equation. What is it about adding an input term that makes it so that the same result (n l.i. solutions form a basis for all other solutions) doesn't apply to particular solutions to the non-homogeneous equation? In other words, why do particular solutions of non-homogeneous equations not contain arbitrary constants?","I've been solving ordinary linear differential equations for a while. First order and second order mostly, utilizing a slew of methods and theorems to find particular and complementary solutions. I still don't quite understand the intuition behind the fact that when I find a solution to a non-homogeneous equation it doesn't contain arbitrary constants like what happens when I find a general solution to the associated homogeneous equation. In the latter case, I can show that every solution to the homogeneous equation is a linear combination of n linearly independent solutions to the same equation, where n is the order of the (differential) equation. One of such linear combinations is a particular solution to the homogeneous equation. What is it about adding an input term that makes it so that the same result (n l.i. solutions form a basis for all other solutions) doesn't apply to particular solutions to the non-homogeneous equation? In other words, why do particular solutions of non-homogeneous equations not contain arbitrary constants?",,['ordinary-differential-equations']
74,Why using $p=y'$ doesn't lead to answer in $y^2(1+y'^2)=4$?,Why using  doesn't lead to answer in ?,p=y' y^2(1+y'^2)=4,"I have the ODE $y^2(1+y'^2)=4$ to solve this I used the substitution $y'=p$ $$y^2(1+p^2)=4$$ $$2y(1+p^2)dy+2py^2dp=0$$ $$(p^2+1)dy+py\;dp=0$$ $$\frac{dy}y+\frac{p}{p^2+1}dp=0$$ $$\ln|y|+\frac12\ln|p^2+1|=\ln|c|$$ $$y\sqrt{p^2+1}=c$$ Using $p^2+1=\frac4{y^2}$ , I get $2=c$ ! I can't find my mistake.","I have the ODE to solve this I used the substitution Using , I get ! I can't find my mistake.",y^2(1+y'^2)=4 y'=p y^2(1+p^2)=4 2y(1+p^2)dy+2py^2dp=0 (p^2+1)dy+py\;dp=0 \frac{dy}y+\frac{p}{p^2+1}dp=0 \ln|y|+\frac12\ln|p^2+1|=\ln|c| y\sqrt{p^2+1}=c p^2+1=\frac4{y^2} 2=c,['ordinary-differential-equations']
75,Solving exact second order differential equation,Solving exact second order differential equation,,"I just started reading about second order differential equations and I have issue with exact equations. In the book I'm reading from, says, The equation $p(x)y''+q(x)y'+r(x)y=s(x)$ is exact if we have $p''-q'+r=0$ . These type of equations can be solve with the following method, $$\frac d{dx}(py'+(q-p)y)=s(x)\xrightarrow{\int} \text{ first order  linear differential equation} \Rightarrow \ldots$$ I tried to find elementary proofs for why exact equations can be solve as mentioned in the book but some of them was too advanced for me like Wikipedia and I found a similar question on this site here which states that If an equation $P(x)y''+Q(x)y'+R(x)y=0$ can be written in the form: $$[P(x)y']'+[f(x)y]'=0$$ then the equation is said to be exact. And it cause more confusion for me because in the book says RHS is a function $s(x)$ but here RHS is zero. Can you please prove the statement in the book?","I just started reading about second order differential equations and I have issue with exact equations. In the book I'm reading from, says, The equation is exact if we have . These type of equations can be solve with the following method, I tried to find elementary proofs for why exact equations can be solve as mentioned in the book but some of them was too advanced for me like Wikipedia and I found a similar question on this site here which states that If an equation can be written in the form: then the equation is said to be exact. And it cause more confusion for me because in the book says RHS is a function but here RHS is zero. Can you please prove the statement in the book?","p(x)y''+q(x)y'+r(x)y=s(x) p''-q'+r=0 \frac d{dx}(py'+(q-p)y)=s(x)\xrightarrow{\int} \text{ first order
 linear differential equation} \Rightarrow \ldots P(x)y''+Q(x)y'+R(x)y=0 [P(x)y']'+[f(x)y]'=0 s(x)",['ordinary-differential-equations']
76,Differential equation with derivatives on both sides,Differential equation with derivatives on both sides,,"I was doing some differential equations while following lessons online and was just wondering if what I found written there is correct. Given differential equation: $y''=xy'+y'+1$ we're trying to make both left and right side derivatives of some functions like: $f'(x)=g'(x) \implies f(x)=g(x)+c$ . What my tutor did is: $(y')'=(xy)'+1$ $(y')'=(xy+x)'$ $y'=xy+x+c_1$ etc. I am curious if this is wrong or I haven't understood this so far. If $y=y(x)$ then $(xy)'$ should be $(xy)'=y+xy'$ , right? How do you get from $(xy)'$ following: $xy'+y'$ ?","I was doing some differential equations while following lessons online and was just wondering if what I found written there is correct. Given differential equation: we're trying to make both left and right side derivatives of some functions like: . What my tutor did is: etc. I am curious if this is wrong or I haven't understood this so far. If then should be , right? How do you get from following: ?",y''=xy'+y'+1 f'(x)=g'(x) \implies f(x)=g(x)+c (y')'=(xy)'+1 (y')'=(xy+x)' y'=xy+x+c_1 y=y(x) (xy)' (xy)'=y+xy' (xy)' xy'+y',['ordinary-differential-equations']
77,Existence of solution for linear system of ODE,Existence of solution for linear system of ODE,,"Consider linear system of ODE given by \begin{eqnarray} u'_i(t)&=&\sum\limits_{i=1}^{n}a_{ij}(t)u_j+b_i(t) &\quad i=1,2,\ldots,n\\ u_i(0)&=&u_i^0 &\quad i=1,2,\ldots,n \end{eqnarray} Suppose $a_{ij},b_i \in L^{\infty}(\mathbb{R}^+)$ and $u_i^0\in \mathbb{R}$ for $i=1,2,\ldots,n,$ then do we have the existence of solution? ?If so how to prove it? In other words, can we relax continuity assumption  on $a_{ij},b_i$ in the existence proof?","Consider linear system of ODE given by Suppose and for then do we have the existence of solution? ?If so how to prove it? In other words, can we relax continuity assumption  on in the existence proof?","\begin{eqnarray}
u'_i(t)&=&\sum\limits_{i=1}^{n}a_{ij}(t)u_j+b_i(t) &\quad i=1,2,\ldots,n\\
u_i(0)&=&u_i^0 &\quad i=1,2,\ldots,n
\end{eqnarray} a_{ij},b_i \in L^{\infty}(\mathbb{R}^+) u_i^0\in \mathbb{R} i=1,2,\ldots,n, a_{ij},b_i","['ordinary-differential-equations', 'analysis']"
78,Source for Peano article written in symbolic logic,Source for Peano article written in symbolic logic,,"L.E.J. Brouwer states in his 1912 address titled ""Intuitionism and Formalism"" that Peano published one of his most important discoveries concerning the existence of integrals of real differential equations in the Mathematische Annalen in the language of symbolic logic; the result was that it could only be read by a few of the initiated and that it did not become generally available until one of these had translated the article into German. I can only find this version but this is a French translation. Does anyone know where I can find Peano's original article written ""in the language of symbolic logic"" referenced by Brouwer?","L.E.J. Brouwer states in his 1912 address titled ""Intuitionism and Formalism"" that Peano published one of his most important discoveries concerning the existence of integrals of real differential equations in the Mathematische Annalen in the language of symbolic logic; the result was that it could only be read by a few of the initiated and that it did not become generally available until one of these had translated the article into German. I can only find this version but this is a French translation. Does anyone know where I can find Peano's original article written ""in the language of symbolic logic"" referenced by Brouwer?",,"['ordinary-differential-equations', 'logic', 'reference-request', 'math-history', 'online-resources']"
79,Solve $e^x \cdot 2y' = y^2 + y'^2$,Solve,e^x \cdot 2y' = y^2 + y'^2,"Solve $$e^x \cdot 2y' = y^2 + y'^2.$$ My Attempt Let $p = y'$ then $2e^x \cdot p=y^2+p^2$ . No differentiate both sides to get: $$2e^x \cdot p + 2e^x \frac{dp}{dx} = 2yp + 2p\frac{dp}{dx}$$ $$\frac{dp}{dx}(2e^x-2p) = 2yp-2e^xp \rightarrow \frac{dp}{dx}(e^x-p) = yp-e^xp$$ I can not easily identify this form so I make the substitution $y = ux$ and $y' = u'x + u$ $$(u'x+u)(e^x-ux) = yux-e^xux$$ Later manipulation does not really get me to an easier form, where should I go from here?","Solve My Attempt Let then . No differentiate both sides to get: I can not easily identify this form so I make the substitution and Later manipulation does not really get me to an easier form, where should I go from here?",e^x \cdot 2y' = y^2 + y'^2. p = y' 2e^x \cdot p=y^2+p^2 2e^x \cdot p + 2e^x \frac{dp}{dx} = 2yp + 2p\frac{dp}{dx} \frac{dp}{dx}(2e^x-2p) = 2yp-2e^xp \rightarrow \frac{dp}{dx}(e^x-p) = yp-e^xp y = ux y' = u'x + u (u'x+u)(e^x-ux) = yux-e^xux,['ordinary-differential-equations']
80,Solving ODE-systems with Matrix exponential is wrong?,Solving ODE-systems with Matrix exponential is wrong?,,"Originally I've learned that the solution of a systems of coupled ODE: $$\underbrace{\left[\begin{array}{cc}{y_1}'(x)\\ \vdots \\{y_n}'(x)\end{array}\right]}_{y'(x)}=  \underbrace{\left[\begin{array}{cccc}&a_{1\,1} &\cdots &a_{1\,n} \\ &\vdots \quad &&\vdots \\ &a_{n\,1}&\cdots&a_{n\,n}\end{array}\right]}_{A}\, \underbrace{\left[\begin{array}{cc}{y_1}(x)\\ \vdots \\{y_n}(x)\end{array}\right]}_{y(x)}$$ is determined by: $$y(x) = \exp(A\,x)\,C$$ where $C$ is a vector with constants $\left[\begin{array}{cc}C_1\\ \vdots \\C_n\end{array}\right]$ and $\exp(A\,x)$ the matrix exponential, that can be at best calculated by: $$\exp(A\,x) = V^{-1}\,\exp(\Lambda\,x)\,V$$ where $V$ is a vector full of Eigenvectors: $\left[\begin{array}{cc}v_1&\cdots&v_n\end{array}\right]$ and $\Lambda$ a matrix full of Eigenvalues on its main diagonal: $\left[\begin{array}{ccc}\lambda_1&\\ &\ddots\\&&v_n\end{array}\right]$ Now apparently this leads to another solution compared to: $$y(x) = c_1\,v_1\,\exp(\lambda_1\,x)+\cdots+c_n\,v_n\,\exp(\lambda_n\,x)$$ Even if one told me both solutions were to solve a system of ODE For example consider the system: $$\left(\begin{array}{cc}{y_1}'(x) \\ {y_2}'(x)\end{array}\right) = \left(\begin{array}{cc} 4 & 10\\ 8 & 2 \end{array}\right)\,\left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right)$$ with Eigenvalues $\lambda_1 = 12, \lambda_2 = -6$ and Eigenvectors: $v_1 = \left(\begin{array}{cc}1 \\ 8/10\end{array}\right), v_2 = \left(\begin{array}{cc}1 \\ -1\end{array}\right)$ According to the second plain solution process I'd obtain: $$\left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right) = \left(\begin{array}{cc}1 \\ 8/10\end{array}\right)\,\exp(12\,x)+\left(\begin{array}{cc}1 \\ -1\end{array}\right)\,\exp(-6\,x)$$ However the matrix exponential spits: $$\left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right) = C\,\left(\begin{array}{cc} \frac{4\,{\mathrm{e}}^{-6\,x}}{9}+\frac{5\,{\mathrm{e}}^{12\,x}}{9} & \frac{5\,{\mathrm{e}}^{12\,x}}{9}-\frac{5\,{\mathrm{e}}^{-6\,x}}{9}\\ \frac{4\,{\mathrm{e}}^{12\,x}}{9}-\frac{4\,{\mathrm{e}}^{-6\,x}}{9} & \frac{5\,{\mathrm{e}}^{-6\,x}}{9}+\frac{4\,{\mathrm{e}}^{12\,x}}{9} \end{array}\right)\,$$ Probably those two are inconvenient, because the constants are set differently. In fact the second approach is independent of constants somehow. So how's that all in relation with each other?","Originally I've learned that the solution of a systems of coupled ODE: is determined by: where is a vector with constants and the matrix exponential, that can be at best calculated by: where is a vector full of Eigenvectors: and a matrix full of Eigenvalues on its main diagonal: Now apparently this leads to another solution compared to: Even if one told me both solutions were to solve a system of ODE For example consider the system: with Eigenvalues and Eigenvectors: According to the second plain solution process I'd obtain: However the matrix exponential spits: Probably those two are inconvenient, because the constants are set differently. In fact the second approach is independent of constants somehow. So how's that all in relation with each other?","\underbrace{\left[\begin{array}{cc}{y_1}'(x)\\ \vdots \\{y_n}'(x)\end{array}\right]}_{y'(x)}= 
\underbrace{\left[\begin{array}{cccc}&a_{1\,1} &\cdots &a_{1\,n}
\\ &\vdots \quad &&\vdots \\
&a_{n\,1}&\cdots&a_{n\,n}\end{array}\right]}_{A}\,
\underbrace{\left[\begin{array}{cc}{y_1}(x)\\ \vdots \\{y_n}(x)\end{array}\right]}_{y(x)} y(x) = \exp(A\,x)\,C C \left[\begin{array}{cc}C_1\\ \vdots \\C_n\end{array}\right] \exp(A\,x) \exp(A\,x) = V^{-1}\,\exp(\Lambda\,x)\,V V \left[\begin{array}{cc}v_1&\cdots&v_n\end{array}\right] \Lambda \left[\begin{array}{ccc}\lambda_1&\\ &\ddots\\&&v_n\end{array}\right] y(x) = c_1\,v_1\,\exp(\lambda_1\,x)+\cdots+c_n\,v_n\,\exp(\lambda_n\,x) \left(\begin{array}{cc}{y_1}'(x) \\ {y_2}'(x)\end{array}\right) = \left(\begin{array}{cc} 4 & 10\\ 8 & 2 \end{array}\right)\,\left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right) \lambda_1 = 12, \lambda_2 = -6 v_1 = \left(\begin{array}{cc}1 \\ 8/10\end{array}\right), v_2 = \left(\begin{array}{cc}1 \\ -1\end{array}\right) \left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right) = \left(\begin{array}{cc}1 \\ 8/10\end{array}\right)\,\exp(12\,x)+\left(\begin{array}{cc}1 \\ -1\end{array}\right)\,\exp(-6\,x) \left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right) = C\,\left(\begin{array}{cc} \frac{4\,{\mathrm{e}}^{-6\,x}}{9}+\frac{5\,{\mathrm{e}}^{12\,x}}{9} & \frac{5\,{\mathrm{e}}^{12\,x}}{9}-\frac{5\,{\mathrm{e}}^{-6\,x}}{9}\\ \frac{4\,{\mathrm{e}}^{12\,x}}{9}-\frac{4\,{\mathrm{e}}^{-6\,x}}{9} & \frac{5\,{\mathrm{e}}^{-6\,x}}{9}+\frac{4\,{\mathrm{e}}^{12\,x}}{9} \end{array}\right)\,","['ordinary-differential-equations', 'systems-of-equations']"
81,How to use polar coordinate in ODE?,How to use polar coordinate in ODE?,,"I don't understand how to use polar coordinate. \begin{cases} \frac{dx(t)}{dt}=2x-y \\ \frac{dy(t)}{dt}=5x-2y  \\ \end{cases} $$ \frac{d}{dt} \left( \begin{array}{c} x \\ y \end{array} \right) = \begin{pmatrix} 2 & -1\\ 5 & -2 \end{pmatrix} \left( \begin{array}{c} x \\ y \end{array} \right)$$ let $A = \begin{bmatrix} 2 & -1\\ 5 & -2 \end{bmatrix}$ , we have $\det(A) = 1, \operatorname{tr}(A) = 0, P(\lambda) = \lambda^2+1 = 0 \Rightarrow \lambda_{1,2}=\pm i $ \begin{cases} 2x-y-ix = 0 \\ 5x-2y-iy = 0  \\ \end{cases} x = 1, y = 0 $\vec v_1 = (x,y) = (x, x(2-i)) = x(1,(2-i)) = \binom{1}{2-i} $ $E_{\lambda1} = \binom{1}{0}+i \binom{0}{-1} = \begin{pmatrix} 1 & 0\\ 2 & -1 \end{pmatrix} = P $ which give us $P^{-1}= \begin{pmatrix} -1 & 0\\ -2 & 1 \end{pmatrix}$ and $J = \begin{pmatrix} 0 & 1\\ -1 & 0 \end{pmatrix}$ $X(t)= e^{0t} \begin{pmatrix} \cos(t) & \sin(t)\\ -\sin(t) & \cos(t) \end{pmatrix}\begin{pmatrix} x_1\\ x_2 \end{pmatrix}$ $Y(t) = \begin{pmatrix} 1 & 0\\ 2 & -1 \end{pmatrix}\begin{pmatrix} \cos(t) & \sin(t)\\ -\sin(t) & \cos(t) \end{pmatrix}\begin{pmatrix} x_1\\ x_2 \end{pmatrix}$ It's a center stable So the solution is pretty simple but i want to try to use the polar coordinate and i can't do it $r^2= x^2 + y^2$ and $x = r\cos(\theta)$ , $y = r \sin(\theta)$ $2rr'= 2xx'+ 2yy' = 2x(2x-y) + 2y(5x-2y) = 4x^2 +8xy -4y^2 $ $\theta ' = \frac{y(2x-y)+x(5x-2y)}{x^2+y^2} = \frac{-y^2+5x^2}{x^2+y^2}$ maybe if the exercise doesnt ask polar coordinate, we don't do it ? i've check this : Polar coordinates differential equation but i still can't do it","I don't understand how to use polar coordinate. let , we have x = 1, y = 0 which give us and It's a center stable So the solution is pretty simple but i want to try to use the polar coordinate and i can't do it and , maybe if the exercise doesnt ask polar coordinate, we don't do it ? i've check this : Polar coordinates differential equation but i still can't do it","\begin{cases} \frac{dx(t)}{dt}=2x-y \\ \frac{dy(t)}{dt}=5x-2y  \\ \end{cases}  \frac{d}{dt} \left( \begin{array}{c} x \\ y \end{array} \right) = \begin{pmatrix} 2 & -1\\ 5 & -2 \end{pmatrix} \left( \begin{array}{c} x \\ y \end{array} \right) A = \begin{bmatrix} 2 & -1\\ 5 & -2 \end{bmatrix} \det(A) = 1, \operatorname{tr}(A) = 0, P(\lambda) = \lambda^2+1 = 0 \Rightarrow \lambda_{1,2}=\pm i  \begin{cases} 2x-y-ix = 0 \\ 5x-2y-iy = 0  \\ \end{cases} \vec v_1 = (x,y) = (x, x(2-i)) = x(1,(2-i)) = \binom{1}{2-i}  E_{\lambda1} = \binom{1}{0}+i \binom{0}{-1} = \begin{pmatrix} 1 & 0\\ 2 & -1 \end{pmatrix} = P  P^{-1}= \begin{pmatrix} -1 & 0\\ -2 & 1 \end{pmatrix} J = \begin{pmatrix} 0 & 1\\ -1 & 0 \end{pmatrix} X(t)= e^{0t} \begin{pmatrix} \cos(t) & \sin(t)\\ -\sin(t) & \cos(t) \end{pmatrix}\begin{pmatrix} x_1\\ x_2 \end{pmatrix} Y(t) = \begin{pmatrix} 1 & 0\\ 2 & -1 \end{pmatrix}\begin{pmatrix} \cos(t) & \sin(t)\\ -\sin(t) & \cos(t) \end{pmatrix}\begin{pmatrix} x_1\\ x_2 \end{pmatrix} r^2= x^2 + y^2 x = r\cos(\theta) y = r \sin(\theta) 2rr'= 2xx'+ 2yy' = 2x(2x-y) + 2y(5x-2y) = 4x^2 +8xy -4y^2  \theta ' = \frac{y(2x-y)+x(5x-2y)}{x^2+y^2} = \frac{-y^2+5x^2}{x^2+y^2}","['ordinary-differential-equations', 'trigonometry', 'dynamical-systems']"
82,Solve Non-Linear First-Order ODE for Parabola Focus point,Solve Non-Linear First-Order ODE for Parabola Focus point,,"I'd like to know how to solve this Non-Linear First-Order ODE , step-by-step: $$ \frac{2y'}{{y'}^2-1} = \frac{x}{y-D} $$ where $y=f(x)$ and $D$ is a constant. Wolfram Alpha gives solution, but hides step-by-step guide: https://www.wolframalpha.com/input/?i=2*f%27%28x%29%2F%281-%28f%27%28x%29%29%5E2%29%3Dx%2F%28D-f%28x%29%29 It also states that it's a D'Alembert's equation , but when I tried to research how to solve it, Google was overwhelmed with the D'Alembert's wave equation instead. Long story: I was trying to solve, which function reflects incoming parallel rays of light to a single point. I know it is a parabola, but wanted to prove it myself. The train of thoughts I had: Out of geometric reasoning and from the reflection of rays one can deduce the followings (see image ): $\frac{x}{y-D}=\tan(\pi-2\alpha)=-\tan(2\alpha)=-\frac{2\tan(\alpha)}{1-\tan^2(\alpha)}=\frac{2y'}{{y'}^2-1}$ And this is where I'm stuck. If anyone has a better method to prove this property, all suggestion is welcome.","I'd like to know how to solve this Non-Linear First-Order ODE , step-by-step: where and is a constant. Wolfram Alpha gives solution, but hides step-by-step guide: https://www.wolframalpha.com/input/?i=2*f%27%28x%29%2F%281-%28f%27%28x%29%29%5E2%29%3Dx%2F%28D-f%28x%29%29 It also states that it's a D'Alembert's equation , but when I tried to research how to solve it, Google was overwhelmed with the D'Alembert's wave equation instead. Long story: I was trying to solve, which function reflects incoming parallel rays of light to a single point. I know it is a parabola, but wanted to prove it myself. The train of thoughts I had: Out of geometric reasoning and from the reflection of rays one can deduce the followings (see image ): And this is where I'm stuck. If anyone has a better method to prove this property, all suggestion is welcome.","
\frac{2y'}{{y'}^2-1} = \frac{x}{y-D}
 y=f(x) D \frac{x}{y-D}=\tan(\pi-2\alpha)=-\tan(2\alpha)=-\frac{2\tan(\alpha)}{1-\tan^2(\alpha)}=\frac{2y'}{{y'}^2-1}","['ordinary-differential-equations', 'physics']"
83,A non-standard ordinary differential equation: $\frac{d\left(xf\left(x\right)\right)}{dx}f(x)=\text{const}$,A non-standard ordinary differential equation:,\frac{d\left(xf\left(x\right)\right)}{dx}f(x)=\text{const},How can I solve this ODE? $$\frac{d(xf(x))}{dx}f(x)=\text{const}.$$ It is clear that $f\left(x\right)=\text{const}$ is a solution but is it the only one? I don't need a formal proof just a good argument. A formal proof is also good though.,How can I solve this ODE? It is clear that is a solution but is it the only one? I don't need a formal proof just a good argument. A formal proof is also good though.,\frac{d(xf(x))}{dx}f(x)=\text{const}. f\left(x\right)=\text{const},"['calculus', 'ordinary-differential-equations']"
84,Explicit solution to ODE $rg''(r)= \gamma g(r)^2$.,Explicit solution to ODE .,rg''(r)= \gamma g(r)^2,"While working on a project, I have come across a function $g\in C^\infty((0,\infty),\mathbb{R})$ satisfying the ODE $$ r g''(r) = \gamma g(r)^2,\quad r > 0,$$ where $\gamma$ is some positive constant. I know also that $g > 0$ , $g$ must be decreasing, $\lim_{r\to 0} g(r) < \infty$ is a constant I know, and $g(r) = o(r^{-1/2})$ for large $r$ . I believe these constraints should determine $g$ uniquely, and the ODE looks (to my untrained eye) simple enough that I'm hoping there is an explicit solution, but I couldn't find one (other than $\frac{2}{\gamma r}$ , which doesn't fit my constraints). As I don't have a particularly rich background in differential equations, I would be grateful for an insight of any kind. After a bit of research, I found that this equation falls into the class of Emden-Fowler equations ( $n=-1,m=2$ in the notation of the link), but I couldn't find explicit solutions for this particular case. Another thing I noted was that defining $f\colon \mathbb{R}^3\setminus\{0\}\to \mathbb{R}$ by $f(x) = \frac{g(|x|)}{|x|}$ for $x\in \mathbb{R}^3\setminus\{0\}$ yields a positive function $f\in C^\infty(\mathbb{R}^3\setminus \{0\})$ with $$\Delta f = \gamma f^2,$$ since the Laplacian in three dimensions, when applied to spherically symmetric functions, can be written as $\Delta_\text{radial} = \frac 1r \frac{\partial^2}{\partial r^2} (r \, \cdot)$ . Edit: Thanks to @Jacob Manaker and @Eli, whose answers are below, I am now quite convinced that no explicit general solution to the above equation exists (what a shame).","While working on a project, I have come across a function satisfying the ODE where is some positive constant. I know also that , must be decreasing, is a constant I know, and for large . I believe these constraints should determine uniquely, and the ODE looks (to my untrained eye) simple enough that I'm hoping there is an explicit solution, but I couldn't find one (other than , which doesn't fit my constraints). As I don't have a particularly rich background in differential equations, I would be grateful for an insight of any kind. After a bit of research, I found that this equation falls into the class of Emden-Fowler equations ( in the notation of the link), but I couldn't find explicit solutions for this particular case. Another thing I noted was that defining by for yields a positive function with since the Laplacian in three dimensions, when applied to spherically symmetric functions, can be written as . Edit: Thanks to @Jacob Manaker and @Eli, whose answers are below, I am now quite convinced that no explicit general solution to the above equation exists (what a shame).","g\in C^\infty((0,\infty),\mathbb{R})  r g''(r) = \gamma g(r)^2,\quad r > 0, \gamma g > 0 g \lim_{r\to 0} g(r) < \infty g(r) = o(r^{-1/2}) r g \frac{2}{\gamma r} n=-1,m=2 f\colon \mathbb{R}^3\setminus\{0\}\to \mathbb{R} f(x) = \frac{g(|x|)}{|x|} x\in \mathbb{R}^3\setminus\{0\} f\in C^\infty(\mathbb{R}^3\setminus \{0\}) \Delta f = \gamma f^2, \Delta_\text{radial} = \frac 1r \frac{\partial^2}{\partial r^2} (r \, \cdot)","['real-analysis', 'ordinary-differential-equations', 'analysis']"
85,Why is it true that $\int _0^x\left(F_y\left(s\right)-G_y\left(3s\right)\right)ds=F\left(x\right)-\frac{1}{3}G\left(x\right)$?,Why is it true that ?,\int _0^x\left(F_y\left(s\right)-G_y\left(3s\right)\right)ds=F\left(x\right)-\frac{1}{3}G\left(x\right),"Note that the functions F and G only have one variable ( $F,\:G:\:\mathbb{R}\:\rightarrow \:\mathbb{R}$ ), and that we initially got $$u\left(x,y\right)=F\left(x+y\right)+G\left(3x-y\right)$$ $$\frac{\partial u}{\partial y}\left(x,y\right)=\frac{\partial \:F}{\partial \:y}\left(x+y\right)-\frac{\partial \:G}{\partial \:y}\left(3x-y\right)$$ $$\frac{\partial u}{\partial y}\left(x,0\right)=\frac{\partial \:F}{\partial \:y}\left(x\right)-\frac{\partial \:G}{\partial \:y}\left(3x\right)$$ Where $\frac{\partial u}{\partial y}\left(x,0\right)=0$ by our initial conditions given So we have $$\frac{\partial \:\:F}{\partial \:\:y}\left(x\right)-\frac{\partial \:\:G}{\partial \:\:y}\left(3x\right)=0$$ And we now take this integral which gives us $$\int _0^x\left(\frac{\partial \:F}{\partial \:y}\left(s\right)-\frac{\partial \:G}{\partial \:y}\left(3s\right)\right)ds=\int _0^x0\:ds$$ Which essentially gives us $$F\left(x\right)-\frac{1}{3}G\left(x\right)=C$$ I don't understand why we are getting this result if we calculate the integral, and also why the right side is equal to a constant C (it should be 0 as it is a bounded integral on 0) The other conditions which might be useful are: $u\left(x,0\right)=3x^2$ and $\frac{\partial ^2ũ}{\partial \xi \partial \eta }=0$ for $\xi =x+y,\:\eta =3x-y$ and $ũ\left(\xi ,\eta \right)=F\left(\xi \right)+G\left(\eta \right)$","Note that the functions F and G only have one variable ( ), and that we initially got Where by our initial conditions given So we have And we now take this integral which gives us Which essentially gives us I don't understand why we are getting this result if we calculate the integral, and also why the right side is equal to a constant C (it should be 0 as it is a bounded integral on 0) The other conditions which might be useful are: and for and","F,\:G:\:\mathbb{R}\:\rightarrow \:\mathbb{R} u\left(x,y\right)=F\left(x+y\right)+G\left(3x-y\right) \frac{\partial u}{\partial y}\left(x,y\right)=\frac{\partial \:F}{\partial \:y}\left(x+y\right)-\frac{\partial \:G}{\partial \:y}\left(3x-y\right) \frac{\partial u}{\partial y}\left(x,0\right)=\frac{\partial \:F}{\partial \:y}\left(x\right)-\frac{\partial \:G}{\partial \:y}\left(3x\right) \frac{\partial u}{\partial y}\left(x,0\right)=0 \frac{\partial \:\:F}{\partial \:\:y}\left(x\right)-\frac{\partial \:\:G}{\partial \:\:y}\left(3x\right)=0 \int _0^x\left(\frac{\partial \:F}{\partial \:y}\left(s\right)-\frac{\partial \:G}{\partial \:y}\left(3s\right)\right)ds=\int _0^x0\:ds F\left(x\right)-\frac{1}{3}G\left(x\right)=C u\left(x,0\right)=3x^2 \frac{\partial ^2ũ}{\partial \xi \partial \eta }=0 \xi =x+y,\:\eta =3x-y ũ\left(\xi ,\eta \right)=F\left(\xi \right)+G\left(\eta \right)","['integration', 'ordinary-differential-equations', 'partial-differential-equations']"
86,"Solving $y'' - 2y' + y = \delta(t-2)$ for y(0) = 0, y'(0) = 0 by using Laplace Transforms. Need help finishing the problem.","Solving  for y(0) = 0, y'(0) = 0 by using Laplace Transforms. Need help finishing the problem.",y'' - 2y' + y = \delta(t-2),"So I'm working on this question and I've taken the Laplcace transforms for everything and separated it as $Y(s) = F(s)G(s)$ , and then I've put it into the convolution equation to get $y(t)$ . My problem is, I don't understand how to fully/properly integrate this when I have the $\delta(T-2)$ inside the integral. Can anyone explain what to do? I'm adding a picture of what I have until now that I am sure is correct so that there's somewhat of an explanation of my train of thought? (the u and v' at the bottom were if I was going to solve with integration by parts, which I also am not sure of how to use here). But yeah, please help and tell me where I'm going wrong...","So I'm working on this question and I've taken the Laplcace transforms for everything and separated it as , and then I've put it into the convolution equation to get . My problem is, I don't understand how to fully/properly integrate this when I have the inside the integral. Can anyone explain what to do? I'm adding a picture of what I have until now that I am sure is correct so that there's somewhat of an explanation of my train of thought? (the u and v' at the bottom were if I was going to solve with integration by parts, which I also am not sure of how to use here). But yeah, please help and tell me where I'm going wrong...",Y(s) = F(s)G(s) y(t) \delta(T-2),"['integration', 'ordinary-differential-equations', 'laplace-transform', 'convolution', 'dirac-delta']"
87,blowup in finite time of $\frac{dx}{dt}=1+x^{10}$,blowup in finite time of,\frac{dx}{dt}=1+x^{10},"I feel like this is extremely silly question. It is a question from Strogatz' Nonlinear Dynamics and Chaos I want to show that $\frac{dx}{dt}=1+x^{10}$ blows up in finite based on the fact that $\frac{dx}{dt}=1+x^2$ blows up in finite time. Well clearly, $1+x^2 < 1+x^{10}$ but then I realized that this is only true for $x > 1$ (in absolute value) So what if $x<1$ ? Is there a nice and elegant way to do this problem without explicitly solving for a solution?","I feel like this is extremely silly question. It is a question from Strogatz' Nonlinear Dynamics and Chaos I want to show that blows up in finite based on the fact that blows up in finite time. Well clearly, but then I realized that this is only true for (in absolute value) So what if ? Is there a nice and elegant way to do this problem without explicitly solving for a solution?",\frac{dx}{dt}=1+x^{10} \frac{dx}{dt}=1+x^2 1+x^2 < 1+x^{10} x > 1 x<1,['ordinary-differential-equations']
88,"For a differential equation, prove that there exists an integrating factor dependent only on $x$","For a differential equation, prove that there exists an integrating factor dependent only on",x,"Consider the differential equation $M(x, y)dx + N(x,y)dy = 0$ . Prove that there exists an integrating factor that is dependent on $x$ if and only if $\frac {M_y - N_x}{N} = f(x)$ , a function of $x$ only. I need to prove that in such a case, the integrating factor is: $$I(x) = e^ { \int {f(x)dx}}.$$ I tried to approach this problem by rewriting $M(x, y)dx + N(x,y)dy = 0$ as $M(x, y) + N(x,y) \frac {dy}{dx} = 0.$ Assuming the differential equation is exact, there exists a potential function $φ$ such that $ \frac {∂φ}{∂x} + \frac{∂φ}{∂y} \frac{dy}{dx} = 0.$ But this implies that $\frac {∂φ}{∂x} = 0$ . So $φ(x, y)$ is a function of $y$ only. So we can deduce that $φ(x, y)$ is a function of $x$ only and $φ(x, y) = c$ , where $c$ is a constant. This is where I'm stuck and I'm not sure how to proceed with this proof. Any guidance is greatly appreciated!","Consider the differential equation . Prove that there exists an integrating factor that is dependent on if and only if , a function of only. I need to prove that in such a case, the integrating factor is: I tried to approach this problem by rewriting as Assuming the differential equation is exact, there exists a potential function such that But this implies that . So is a function of only. So we can deduce that is a function of only and , where is a constant. This is where I'm stuck and I'm not sure how to proceed with this proof. Any guidance is greatly appreciated!","M(x, y)dx + N(x,y)dy = 0 x \frac {M_y - N_x}{N} = f(x) x I(x) = e^ { \int {f(x)dx}}. M(x, y)dx + N(x,y)dy = 0 M(x, y) + N(x,y) \frac {dy}{dx} = 0. φ  \frac {∂φ}{∂x} + \frac{∂φ}{∂y} \frac{dy}{dx} = 0. \frac {∂φ}{∂x} = 0 φ(x, y) y φ(x, y) x φ(x, y) = c c",['ordinary-differential-equations']
89,Find all $f(x)$ so that $[y(x)]^2\cos x + y(x)f(x)y'(x)=0$ is exact,Find all  so that  is exact,f(x) [y(x)]^2\cos x + y(x)f(x)y'(x)=0,"Finde all functions $f(x)$ , so that the ODE is exact... $$[y(x)]^2\cos x + y(x)f(x)y'(x)=0$$ First I rewrote the ODE slightly... \begin{align*}     &[y(x)]^2\cos x + y(x)f(x)y'(x)=0 \\     \Longrightarrow \quad & [y(x)]^2\cos x + y(x)f(x)\frac{dy}{dx}=0 \\     \Longrightarrow \quad & ([y(x)]^2\cos x)dx + y(x)f(x)dy=0 \\ \end{align*} Then I identified $P$ and $Q$ ... \begin{align*}     &P_x(x,y) = [y(x)]^2\cos x\\     &Q_y(x,y) = y(x)f(x) \\      \\     \Longrightarrow \quad &P_{x,y}(x,y) = 2y(x)\cos x \\     \Longrightarrow \quad &Q_{x,y}(x,y) = y'(x)f(x) + y(x)f'(x) \\ \end{align*} Since $P_{x,y}$ must be equal to $Q_{x,y}$ for the ODE to be exact, I put them equal to find out $f(x)$ but following equations doesn't look solveable... \begin{align*}     y'(x)f(x) + y(x)f'(x) \overset{!}{=}2\cos(x) y(x) \end{align*} Any suggestions where I went wrong?","Finde all functions , so that the ODE is exact... First I rewrote the ODE slightly... Then I identified and ... Since must be equal to for the ODE to be exact, I put them equal to find out but following equations doesn't look solveable... Any suggestions where I went wrong?","f(x) [y(x)]^2\cos x + y(x)f(x)y'(x)=0 \begin{align*}
    &[y(x)]^2\cos x + y(x)f(x)y'(x)=0 \\
    \Longrightarrow \quad & [y(x)]^2\cos x + y(x)f(x)\frac{dy}{dx}=0 \\
    \Longrightarrow \quad & ([y(x)]^2\cos x)dx + y(x)f(x)dy=0 \\
\end{align*} P Q \begin{align*}
    &P_x(x,y) = [y(x)]^2\cos x\\
    &Q_y(x,y) = y(x)f(x) \\ 
    \\
    \Longrightarrow \quad &P_{x,y}(x,y) = 2y(x)\cos x \\
    \Longrightarrow \quad &Q_{x,y}(x,y) = y'(x)f(x) + y(x)f'(x) \\
\end{align*} P_{x,y} Q_{x,y} f(x) \begin{align*}
    y'(x)f(x) + y(x)f'(x) \overset{!}{=}2\cos(x) y(x)
\end{align*}","['integration', 'ordinary-differential-equations']"
90,Find the indicial equation of $x^{3}y''+(\cos 2x-1)y'+2xy=0$,Find the indicial equation of,x^{3}y''+(\cos 2x-1)y'+2xy=0,Find the indicial equation of $x^{3}y''+(\cos 2x-1)y'+2xy=0$ (where $y''$ stands for the second derivative) The reference answer is $m(m-1)-2m+2=0$ I really don't know how to ... please help. thanks in advance!!,Find the indicial equation of (where stands for the second derivative) The reference answer is I really don't know how to ... please help. thanks in advance!!,x^{3}y''+(\cos 2x-1)y'+2xy=0 y'' m(m-1)-2m+2=0,['ordinary-differential-equations']
91,How to solve the coupled differential equations with 3 variables?,How to solve the coupled differential equations with 3 variables?,,"I have the following set of differential equations, \begin{aligned} & x' - \alpha(yz' - zy') =  -\beta(yP_z -zP_y) \\ & y' - \alpha(zx' - xz') =  -\beta(zP_x -xP_z) \\ & z' - \alpha(xy' - yx') =  -\beta(xP_y -yP_x) \end{aligned} I know how to implement runga kutta 4th order or Euler or Heun's method...what I can't figure out is how to update the $x'$ or $y'$ or $z'$ term while I am solving it. Any help or direction towards possible resources would be appreciated.","I have the following set of differential equations, I know how to implement runga kutta 4th order or Euler or Heun's method...what I can't figure out is how to update the or or term while I am solving it. Any help or direction towards possible resources would be appreciated.","\begin{aligned}
& x' - \alpha(yz' - zy') =  -\beta(yP_z -zP_y) \\
& y' - \alpha(zx' - xz') =  -\beta(zP_x -xP_z) \\
& z' - \alpha(xy' - yx') =  -\beta(xP_y -yP_x)
\end{aligned} x' y' z'","['ordinary-differential-equations', 'numerical-methods', 'systems-of-equations']"
92,Differential equation with the Substitution method,Differential equation with the Substitution method,,"I'm trying to solve this equation below : $$ y' = \sin\left(\frac{y}{x}\right) + \frac{y}{x}  \tag 1 $$ The first step was to to substitute $\frac{y}{x}$ with $u$ => $ u = \frac{y}{x}$ => $y' = u'x + u \tag 2$ Then with $(1) = (2)$ ,  I got at the end $$\dfrac{\ln\left(\cos\left(y\right)+1\right)-\ln\left(1-\cos\left(y\right)\right)}{2} = \ln|x| + C$$ And after more simplification : $$\frac{\cos(y)}{1-\cos(y)} = 2Cx   $$ How can I get the general function from this equation ?","I'm trying to solve this equation below : The first step was to to substitute with => => Then with ,  I got at the end And after more simplification : How can I get the general function from this equation ?", y' = \sin\left(\frac{y}{x}\right) + \frac{y}{x}  \tag 1  \frac{y}{x} u  u = \frac{y}{x} y' = u'x + u \tag 2 (1) = (2) \dfrac{\ln\left(\cos\left(y\right)+1\right)-\ln\left(1-\cos\left(y\right)\right)}{2} = \ln|x| + C \frac{\cos(y)}{1-\cos(y)} = 2Cx   ,"['ordinary-differential-equations', 'integro-differential-equations']"
93,Solve the differential equation: $\frac{dy}{dx}=1 + a\frac{y}{x}$,Solve the differential equation:,\frac{dy}{dx}=1 + a\frac{y}{x},What are the steps to get to $y(x)$ ? $$\frac{dy}{dx}=1 + a\frac{y}{x}$$,What are the steps to get to ?,y(x) \frac{dy}{dx}=1 + a\frac{y}{x},['ordinary-differential-equations']
94,using Liouville-Ostrogradsky to solve differential equation,using Liouville-Ostrogradsky to solve differential equation,,"I have a differential equation: $$y''+p_1\left(x\right)y'+p_0\left(x\right)y=0$$ for $x>0$ and it is given that $p_0,p_1$ are continuous for any $x\in \mathbb{R}$ . I am questioned if $y_1(x)=x$ and $y_2(x)=\ln(x)$ can be the solutions of this equation. Answer: They are linear independent, therefore using Liouville-Ostrogradsky: $$W(y_1,y_2)(x)=ce^{-\int \:p_1\left(x\right)dx}$$ we get: $$W(y_1,y_2)(x)=\left|\begin{pmatrix}x&\ln\left(x\right)\\ 1&\frac{1}{x}\end{pmatrix}\right|=\ln\left(x\right)-1$$ so, we can choose c=1 for comfort and get: $$\ln\left(x\right)-1=e^{-\int p_1\left(x\right)dx}$$ therefore: $$\int p_1\left(x\right)dx=\frac{1}{\ln\left(\ln\left(x\right)-1\right)}$$ Therefore, after deriving both sides: $$p_1\left(x\right)=\frac{1}{x\cdot \left(\ln\left(x\right)-1\right)\cdot \ln^2\left(\ln\left(x\right)-1\right)}$$ Now the thing that bothers me, that is was said about $p_1$ that it is continuous for $x\in \mathbb{R}, x>0$ , and here the denominator is defined only for any $x>e$ , so I don't know if the authors of the question meant that it should be continuous for any $x>e$ and then we can continue and find $p_0$ or it contradicts the given statement and therefore those two functions cannot be a solution for the equation. Am I right that those are the two options of answers for this diff. equation ? If it is about $x>e$ , then we can choose $$y=x+\ln(x)$$ , calculate $y'$ and $y''$ , put it all together in the first formula: $y''+p_1\left(x\right)y'+p_0\left(x\right)y=0$ and extract $p_0$ . Otherwise if the question means that $p_1(x)$ has to be continuous on $x>0$ , then $p_1(x)$ doesn't satisfies that because it is defined only for $x>e$ , is this correct ?","I have a differential equation: for and it is given that are continuous for any . I am questioned if and can be the solutions of this equation. Answer: They are linear independent, therefore using Liouville-Ostrogradsky: we get: so, we can choose c=1 for comfort and get: therefore: Therefore, after deriving both sides: Now the thing that bothers me, that is was said about that it is continuous for , and here the denominator is defined only for any , so I don't know if the authors of the question meant that it should be continuous for any and then we can continue and find or it contradicts the given statement and therefore those two functions cannot be a solution for the equation. Am I right that those are the two options of answers for this diff. equation ? If it is about , then we can choose , calculate and , put it all together in the first formula: and extract . Otherwise if the question means that has to be continuous on , then doesn't satisfies that because it is defined only for , is this correct ?","y''+p_1\left(x\right)y'+p_0\left(x\right)y=0 x>0 p_0,p_1 x\in \mathbb{R} y_1(x)=x y_2(x)=\ln(x) W(y_1,y_2)(x)=ce^{-\int \:p_1\left(x\right)dx} W(y_1,y_2)(x)=\left|\begin{pmatrix}x&\ln\left(x\right)\\ 1&\frac{1}{x}\end{pmatrix}\right|=\ln\left(x\right)-1 \ln\left(x\right)-1=e^{-\int p_1\left(x\right)dx} \int p_1\left(x\right)dx=\frac{1}{\ln\left(\ln\left(x\right)-1\right)} p_1\left(x\right)=\frac{1}{x\cdot \left(\ln\left(x\right)-1\right)\cdot \ln^2\left(\ln\left(x\right)-1\right)} p_1 x\in \mathbb{R}, x>0 x>e x>e p_0 x>e y=x+\ln(x) y' y'' y''+p_1\left(x\right)y'+p_0\left(x\right)y=0 p_0 p_1(x) x>0 p_1(x) x>e","['ordinary-differential-equations', 'functions', 'derivatives', 'continuity', 'wronskian']"
95,Integrating Factor of $(x\ln(y) + xy)\mathrm{d}x + (y\ln(x) + xy)\mathrm{d}y$,Integrating Factor of,(x\ln(y) + xy)\mathrm{d}x + (y\ln(x) + xy)\mathrm{d}y,"Last day I try to prove that the equation $$ (x\ln(y) + xy)\mathrm{d}x + (y\ln(x) + xy)\mathrm{d}y =0 $$ is not exact, really easy task, but I want to go further, I tried to find a integrating factor such that the equation become exact, but is so strange, I try every method that I know, but I get nothing, later I catch some hope with the factor $R = \ln\big(\ln(x+y)\big)$ , but then nothing again. There's some method to find this integrating factor? Do you know any? Thank you so much!","Last day I try to prove that the equation is not exact, really easy task, but I want to go further, I tried to find a integrating factor such that the equation become exact, but is so strange, I try every method that I know, but I get nothing, later I catch some hope with the factor , but then nothing again. There's some method to find this integrating factor? Do you know any? Thank you so much!","
(x\ln(y) + xy)\mathrm{d}x + (y\ln(x) + xy)\mathrm{d}y =0
 R = \ln\big(\ln(x+y)\big)","['ordinary-differential-equations', 'derivatives', 'logarithms', 'integrating-factor']"
96,Differential equation with integrating factor,Differential equation with integrating factor,,Hey I am supposed to solve the following differential equation: $(1-x^{2}y)dx+x^{2}(y-x)dy=0$ I found integrating factor: $\varphi (x)=-\frac{2}{x}$ So I multiply my original equation and I got: $\left ( \frac{1}{x^{2}} -y\right )dx+\left ( y-x \right )dy=0$ But then I try to integrate it and I got stuck. The answer should be: $y^{2}-2xy-\frac{2}{x}=C$ Can somebody help me? Thanks,Hey I am supposed to solve the following differential equation: I found integrating factor: So I multiply my original equation and I got: But then I try to integrate it and I got stuck. The answer should be: Can somebody help me? Thanks,(1-x^{2}y)dx+x^{2}(y-x)dy=0 \varphi (x)=-\frac{2}{x} \left ( \frac{1}{x^{2}} -y\right )dx+\left ( y-x \right )dy=0 y^{2}-2xy-\frac{2}{x}=C,['ordinary-differential-equations']
97,How do I find the general solution to $m\frac{dv}{dt} = mg- kv^2$?,How do I find the general solution to ?,m\frac{dv}{dt} = mg- kv^2,"The velocity $v$ of a freefalling skydiver is modeled by the differential equation $$ m\frac{dv}{dt} = mg - kv^2,$$ where $m$ is the mass of the skydiver, $g$ is the gravitational constant, and $k$ is the drag coefficient determined by the position of the diver during the dive. Find the general solution of the differential equation. So is it my job to solve for velocity here ( $v$ )? or am I missing something?","The velocity of a freefalling skydiver is modeled by the differential equation where is the mass of the skydiver, is the gravitational constant, and is the drag coefficient determined by the position of the diver during the dive. Find the general solution of the differential equation. So is it my job to solve for velocity here ( )? or am I missing something?","v  m\frac{dv}{dt} = mg - kv^2, m g k v","['calculus', 'ordinary-differential-equations', 'physics', 'mathematical-physics', 'differential']"
98,Applying Picard–Lindelöf theorem for a second order ODE,Applying Picard–Lindelöf theorem for a second order ODE,,"I'm trying to find the solution to the following IVP and prove its uniqueness by applying the Picard–Lindelöf theorem. $$ y''+x^2y'+xy=0,\\  y(0)=y'(0)=0 $$ I've used the theorem in a few problems I've solved but the second order here confuses me. I know two things: To apply the theorem , my equation has to be of the following form : $g'(x)=f(x,g)$ By setting $y'(x)=g(x)$ I can turn this into a first order system : $$y'(x)=g(x)\\g'(x)+x^2g(x)+xy=0$$ Now I can write the new equation like this: $$g'(x)=-x^2g(x)-xy\ \implies g'(x)=f(x,g,y)$$ Because of the ' $y$ ' term , this is not the form I'm looking for. Or is it? I know that $y$ is a function of $x$ , so is it correct to write the following? $$g'(x)=f(x,g,y(x))=f(x,g)$$ Normally I would go on and prove that f is continuous and Lipschitz in $x,g$ . What do I do in this case ? Do I have to show that f is continuous and Lipschitz in $y$ as well?","I'm trying to find the solution to the following IVP and prove its uniqueness by applying the Picard–Lindelöf theorem. I've used the theorem in a few problems I've solved but the second order here confuses me. I know two things: To apply the theorem , my equation has to be of the following form : By setting I can turn this into a first order system : Now I can write the new equation like this: Because of the ' ' term , this is not the form I'm looking for. Or is it? I know that is a function of , so is it correct to write the following? Normally I would go on and prove that f is continuous and Lipschitz in . What do I do in this case ? Do I have to show that f is continuous and Lipschitz in as well?","
y''+x^2y'+xy=0,\\ 
y(0)=y'(0)=0
 g'(x)=f(x,g) y'(x)=g(x) y'(x)=g(x)\\g'(x)+x^2g(x)+xy=0 g'(x)=-x^2g(x)-xy\ \implies g'(x)=f(x,g,y) y y x g'(x)=f(x,g,y(x))=f(x,g) x,g y","['ordinary-differential-equations', 'lipschitz-functions', 'initial-value-problems']"
99,How to find the initial value of a solution to a differential equation in order to comply with certain limits,How to find the initial value of a solution to a differential equation in order to comply with certain limits,,"For the IVP, $y'+\frac{2x^2-4xy-y^2}{3x^2}=0, x>0, y(1)=y_0$ I got this, once all the calculations have been done: $$y'=\frac{y_0^2(-2x^2+4x+1)+4y_0(x^2+x+1)-2(x^2+4x-2)}{(2+y_0-x(y_0-1))^2}$$ I am confident with this calculation, and I also verified with Wolfram Alpha. Then the question asked there is exactly one value of $y_0$ such that the IVP satisfied $\lim_{x\to 0}y'(x)\neq 1$ , while $\lim_ {x\to0}y'(x)=1$ for all other values of $y_0$ . What is this value of $y_0$ corresponding to the different limits? So I took the limit $$\lim_{x\to0}y'(x)=\frac{y_0^2+4y_0+4}{(2+y_0)^2}=1$$ So no matter what value of $y_0$ I have, the limit will always be 1. Where did I misunderstand or did wrong? Any help will be great, stuck about 2 days...","For the IVP, I got this, once all the calculations have been done: I am confident with this calculation, and I also verified with Wolfram Alpha. Then the question asked there is exactly one value of such that the IVP satisfied , while for all other values of . What is this value of corresponding to the different limits? So I took the limit So no matter what value of I have, the limit will always be 1. Where did I misunderstand or did wrong? Any help will be great, stuck about 2 days...","y'+\frac{2x^2-4xy-y^2}{3x^2}=0, x>0, y(1)=y_0 y'=\frac{y_0^2(-2x^2+4x+1)+4y_0(x^2+x+1)-2(x^2+4x-2)}{(2+y_0-x(y_0-1))^2} y_0 \lim_{x\to 0}y'(x)\neq 1 \lim_ {x\to0}y'(x)=1 y_0 y_0 \lim_{x\to0}y'(x)=\frac{y_0^2+4y_0+4}{(2+y_0)^2}=1 y_0","['ordinary-differential-equations', 'limits', 'initial-value-problems']"
