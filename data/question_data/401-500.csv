,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why does Wolframalpha think that this sum converges?,Why does Wolframalpha think that this sum converges?,,"Looking at the sum: $$\sum_{n=1}^\infty\tan\left(\frac\pi{2^n}\right)$$ I'd say that it does not converge, because for $n=1$ the tangent $\tan\left(\frac\pi 2\right)$ should be undefined. But Wolframlpha thinks that the sum converges somewhere around $1.63312×10^{16}$ . What am I missing?","Looking at the sum: $$\sum_{n=1}^\infty\tan\left(\frac\pi{2^n}\right)$$ I'd say that it does not converge, because for $n=1$ the tangent $\tan\left(\frac\pi 2\right)$ should be undefined. But Wolframlpha thinks that the sum converges somewhere around $1.63312×10^{16}$ . What am I missing?",,"['real-analysis', 'convergence-divergence', 'summation']"
1,Evaluate:: $ 2 \sum_{n=1}^\infty \frac{(-1)^{n+1}}{n+1}\left( 1 + \frac12 +\cdots + \frac 1n\right) $,Evaluate::, 2 \sum_{n=1}^\infty \frac{(-1)^{n+1}}{n+1}\left( 1 + \frac12 +\cdots + \frac 1n\right) ,"How to evaluate the series: $$  2 \sum_{n=1}^\infty \frac{(-1)^{n+1}}{n+1}\left( 1 + \frac12 + \cdots + \frac 1n\right) $$ According to Mathematica, this converges to $ (\log 2)^2 $.","How to evaluate the series: $$  2 \sum_{n=1}^\infty \frac{(-1)^{n+1}}{n+1}\left( 1 + \frac12 + \cdots + \frac 1n\right) $$ According to Mathematica, this converges to $ (\log 2)^2 $.",,"['real-analysis', 'sequences-and-series']"
2,How to prove that the derivative of Heaviside's unit step function is the Dirac delta?,How to prove that the derivative of Heaviside's unit step function is the Dirac delta?,,"Here is a problem from Griffith's book Introduction to E&M . Let $\theta(x)$ be the step function $$\theta(x) = \begin{cases} 0, & x \le 0, \\ 1, & x \gt 0.  \end{cases} $$ The question is how to prove $\frac{d\theta}{dx} = \delta(x)$ . I think since the function is discontinuous at $x = 0$ , there is no definition of $\frac{d\theta}{dx}$ at the point $x = 0$ at all. Thus, how could we show the equivalence of a equation if the left part of equation is not defined at the point $x = 0$ ?","Here is a problem from Griffith's book Introduction to E&M . Let be the step function The question is how to prove . I think since the function is discontinuous at , there is no definition of at the point at all. Thus, how could we show the equivalence of a equation if the left part of equation is not defined at the point ?","\theta(x) \theta(x) = \begin{cases}
0, & x \le 0, \\
1, & x \gt 0. 
\end{cases}
 \frac{d\theta}{dx} = \delta(x) x = 0 \frac{d\theta}{dx} x = 0 x = 0","['real-analysis', 'distribution-theory']"
3,Isometries of $\mathbb{R}^n$,Isometries of,\mathbb{R}^n,"Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ be such that $\left\| f(x)-f(y)\right\| =\left\| x-y\right\|$.  Is $f$ necessarily surjective? If this is so, you can prove (Mazur-Ulam Theorem) that $f$ is affine, and hence you could classify all isometries of $\mathbb{R}^n$.  However, at the moment, I can't think of any good ideas to prove that $f$ is surjective.  For that matter, is it even the case that $f$ must be surjective? Any ideas would be most welcomed. Thanks much!","Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ be such that $\left\| f(x)-f(y)\right\| =\left\| x-y\right\|$.  Is $f$ necessarily surjective? If this is so, you can prove (Mazur-Ulam Theorem) that $f$ is affine, and hence you could classify all isometries of $\mathbb{R}^n$.  However, at the moment, I can't think of any good ideas to prove that $f$ is surjective.  For that matter, is it even the case that $f$ must be surjective? Any ideas would be most welcomed. Thanks much!",,['real-analysis']
4,When has one sufficiently mastered an area of mathematics?,When has one sufficiently mastered an area of mathematics?,,"This is a rather soft question regarding the mastery of various mathematical subjects, such as undergraduate subjects. In particular, say, when has one mastered undergraduate analysis? Is it realistic to expect some individual to be able to prove every theorem and do every exercise in Baby Rudin? What about analysis not covered in Baby Rudin? Say, should one also be able to prove compactness is equivalent to sequential compactness, and sequential compactness is equivalent to closed and boundedness in $\mathbb{R}^n$? Should one be able to prove every major statement about completeness? Continuity? Integration? Or is it enough to be able to read and understand any proof in a particular area? When can one feel satisfied with what they know about a particular area of basic mathematics? Can the seasoned mathematician prove these theorems by heart without much thought? If so, is this due to brute memorization, or simply mathematical maturity? In short, what should one reasonably expect an undergraduate to know about any particular subject after completing a course in said subject? I ask specifically about undergraduate subjects, since these are often the areas expected to be mastered by the time one begins graduate school and beyond. This question applies to all areas of mathematics, graduate and undergraduate. Interested to hear different interpretations of the words sufficiently and mastered .","This is a rather soft question regarding the mastery of various mathematical subjects, such as undergraduate subjects. In particular, say, when has one mastered undergraduate analysis? Is it realistic to expect some individual to be able to prove every theorem and do every exercise in Baby Rudin? What about analysis not covered in Baby Rudin? Say, should one also be able to prove compactness is equivalent to sequential compactness, and sequential compactness is equivalent to closed and boundedness in $\mathbb{R}^n$? Should one be able to prove every major statement about completeness? Continuity? Integration? Or is it enough to be able to read and understand any proof in a particular area? When can one feel satisfied with what they know about a particular area of basic mathematics? Can the seasoned mathematician prove these theorems by heart without much thought? If so, is this due to brute memorization, or simply mathematical maturity? In short, what should one reasonably expect an undergraduate to know about any particular subject after completing a course in said subject? I ask specifically about undergraduate subjects, since these are often the areas expected to be mastered by the time one begins graduate school and beyond. This question applies to all areas of mathematics, graduate and undergraduate. Interested to hear different interpretations of the words sufficiently and mastered .",,"['real-analysis', 'analysis', 'soft-question', 'proof-writing']"
5,Why does separation of variable gives the general solution to a PDE,Why does separation of variable gives the general solution to a PDE,,"I was reading about physics and came across the method of using separation of variables to solve specific PDEs, but I can't figure out why the specific solutions give rise to the general solution (the book didn't give any explanation for all these). The specific example in the book was the Laplace Equation in $2$ variables: $$\frac {\partial^2 V}{\partial x^2}+\frac {\partial^2 V}{\partial y^2}=0$$ For the above example, separation of variable is essentially solving for the eigen-vectors of the operator $\frac {\partial^2 }{\partial x^2}$ and $\frac {\partial^2 }{\partial y^2}$ , which are Hermitian and commutes with each other. I know that in the finite dimensional case, such operators are simultaneously diagonalizable, then solving for the eigen-vectors will give all the solution, but I'm not sure does this work for infinite dimension. I'm also not sure does this approach works in the general case, for other PDEs that can be solved by separation of variable. All the other post I find on here are all explaining how or when separation of variable work, instead of why such techniques will give the general solutions. Another side question is: What kind of classes will cover these topics? The only undergraduate class that seems relevant at my university is Linear Analysis, which doesn't cover this. The graduate PDE sequence have graduate Real Analysis sequence as pre-requisite, which I don't think I'll be able to take soon.","I was reading about physics and came across the method of using separation of variables to solve specific PDEs, but I can't figure out why the specific solutions give rise to the general solution (the book didn't give any explanation for all these). The specific example in the book was the Laplace Equation in variables: For the above example, separation of variable is essentially solving for the eigen-vectors of the operator and , which are Hermitian and commutes with each other. I know that in the finite dimensional case, such operators are simultaneously diagonalizable, then solving for the eigen-vectors will give all the solution, but I'm not sure does this work for infinite dimension. I'm also not sure does this approach works in the general case, for other PDEs that can be solved by separation of variable. All the other post I find on here are all explaining how or when separation of variable work, instead of why such techniques will give the general solutions. Another side question is: What kind of classes will cover these topics? The only undergraduate class that seems relevant at my university is Linear Analysis, which doesn't cover this. The graduate PDE sequence have graduate Real Analysis sequence as pre-requisite, which I don't think I'll be able to take soon.",2 \frac {\partial^2 V}{\partial x^2}+\frac {\partial^2 V}{\partial y^2}=0 \frac {\partial^2 }{\partial x^2} \frac {\partial^2 }{\partial y^2},"['real-analysis', 'linear-algebra', 'functional-analysis', 'partial-differential-equations']"
6,Is the IVT equivalent to completeness?,Is the IVT equivalent to completeness?,,"Obviously we can use the completeness of the real numbers (least upper bound axiom, or one of the equivalent principles) to prove the IVT. Can we go in the opposite direction? This isn't a homework problem or something. I'm just wondering. If the answer is ""yes"", then I'm not really asking for much explanation. A reference, or a place to look if I'm stuck, will do.","Obviously we can use the completeness of the real numbers (least upper bound axiom, or one of the equivalent principles) to prove the IVT. Can we go in the opposite direction? This isn't a homework problem or something. I'm just wondering. If the answer is ""yes"", then I'm not really asking for much explanation. A reference, or a place to look if I'm stuck, will do.",,['real-analysis']
7,"If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$","If  converges, then so does",\sum_{n=1}^{\infty} a_n^{2} \sum_{n=1}^{\infty} \frac {a_n}{n},"Let $a_1,a_2,a_3,\ldots$ be reals. Prove that if $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ . For this I have shown the case for when $ a_n^{2} \le\frac {|a_n|}{n}$ $\Rightarrow$ $ |a_n|\le\frac {1}{n}$ $\Rightarrow$ $\frac {|a_n|}{n} \le \frac{1}{n^{2}}$ and we know that $\sum_{n=1}^{\infty} \frac {1}{n^{2}}$ converges and hence $\sum_{n=1}^{\infty}\frac {a_n}{n}$ converges by the comparison test. Now considering $ a_n^{2} \ge\frac {|a_n|}{n}$ $\Rightarrow$ $\frac {|a_n|}{n} \le a_n^{2}$ $\rightarrow$ combining the two cases for any n we have: $\frac {|a_n|}{n}\le\frac{1}{n^{2}}+a_n^{2}$ Hence using the comparion test again we know that $\sum_{n=1}^{\infty} a_n^{2}$ converges and $\sum_{n=1}^{\infty} \frac {1}{n^{2}}$ converges hence the sum converges so we can conclude that $\sum_{n=1}^{\infty} \frac {a_n}{n}$ is absoluetly convergent $\Rightarrow$ convergent.  Not to sure if this is correct, any help would be much appreciated, many thanks.","Let be reals. Prove that if converges, then so does . For this I have shown the case for when and we know that converges and hence converges by the comparison test. Now considering combining the two cases for any n we have: Hence using the comparion test again we know that converges and converges hence the sum converges so we can conclude that is absoluetly convergent convergent.  Not to sure if this is correct, any help would be much appreciated, many thanks.","a_1,a_2,a_3,\ldots \sum_{n=1}^{\infty} a_n^{2} \sum_{n=1}^{\infty} \frac {a_n}{n}  a_n^{2} \le\frac {|a_n|}{n} \Rightarrow  |a_n|\le\frac {1}{n} \Rightarrow \frac {|a_n|}{n} \le \frac{1}{n^{2}} \sum_{n=1}^{\infty} \frac {1}{n^{2}} \sum_{n=1}^{\infty}\frac {a_n}{n}  a_n^{2} \ge\frac {|a_n|}{n} \Rightarrow \frac {|a_n|}{n} \le a_n^{2} \rightarrow \frac {|a_n|}{n}\le\frac{1}{n^{2}}+a_n^{2} \sum_{n=1}^{\infty} a_n^{2} \sum_{n=1}^{\infty} \frac {1}{n^{2}} \sum_{n=1}^{\infty} \frac {a_n}{n} \Rightarrow","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
8,Absolutely continuous functions,Absolutely continuous functions,,"This might probably be classed as a soft question. But I would be very interested to know the motivation behind the definition of an absolutely continuous function. To state ""A real valued function $ f $ on $ [a,b] $ is absolutely continuous on said interval if $\forall $$\epsilon >0 \ $, $\exists\delta>0\ $  such that  $$ \sum^n_{k=1}|f(b_k) -f(a_k)|< \epsilon $$ for every $n$ disjoint subintervals $ \ (a_k,b_k) $ of $ \ [a,b] $, $k=1,\ldots,n$, such that $ \sum^n_{k=1}|b_k -a_k|< \delta $. Why the use of the disjoint sub-intervals? What purpose do they serve? Somehow the definition didn't seem natural to me. What I mean is just as the notion of uniform continuity is motivated by the definition of continuity itself or as the concept of compactness serves to generalise the notion of finiteness, how can one look at Absolute Continuity in this respect?","This might probably be classed as a soft question. But I would be very interested to know the motivation behind the definition of an absolutely continuous function. To state ""A real valued function $ f $ on $ [a,b] $ is absolutely continuous on said interval if $\forall $$\epsilon >0 \ $, $\exists\delta>0\ $  such that  $$ \sum^n_{k=1}|f(b_k) -f(a_k)|< \epsilon $$ for every $n$ disjoint subintervals $ \ (a_k,b_k) $ of $ \ [a,b] $, $k=1,\ldots,n$, such that $ \sum^n_{k=1}|b_k -a_k|< \delta $. Why the use of the disjoint sub-intervals? What purpose do they serve? Somehow the definition didn't seem natural to me. What I mean is just as the notion of uniform continuity is motivated by the definition of continuity itself or as the concept of compactness serves to generalise the notion of finiteness, how can one look at Absolute Continuity in this respect?",,['real-analysis']
9,A convex function is differentiable at all but countably many points,A convex function is differentiable at all but countably many points,,"Let $f:\Bbb R\to\Bbb R$ be a convex function. Then $f$ is differentiable at all but countably many points. It is clear that a convex function can be non-differentiable at countably many points, for example $f(x)=\int\lfloor x\rfloor\,dx$. I just made this theorem up, but my heuristic for why it should be true is that the only possible non-differentiable singularities I can imagine in a convex function are corners, and these involve a jump discontinuity in the derivative, so since the derivative is increasing (where it is defined), you get an inequality like $f'(y)-f'(x)\ge \sum_{t\in(x,y)}\omega_t(f')$, where $\omega_t(f')$ is the oscillation at $t$ (limit from right minus limit from left) and the sum is over all real numbers between $x$ and $y$. Since the sum is convergent (assuming that $x\le y$ are points such that $f$ is differentiable at $x$ and $y$ so that this makes sense), there can only be countably many values in the sum which are non-zero, and at all other points the oscillation is zero and so the derivative exists. Thus there are only countably many non-differentiable points in the interval $(x,y)$, so as long as there is a suitable sequence $(x_n)\to-\infty$, $(y_n)\to\infty$ of differentiable points, the total number of non-differentiable points is a countable union of countable sets, which is countable. Furthermore, I would conjecture that the set of non-differentiable points has empty interior-of-closure, i.e. you can't make a function that is non-differentiable at the rational numbers, but as the above discussion shows there are still a lot of holes in the proof (and I'm making a lot of unjustified assumptions regarding the derivative already being somewhat well-defined). Does anyone know how to approach such a statement?","Let $f:\Bbb R\to\Bbb R$ be a convex function. Then $f$ is differentiable at all but countably many points. It is clear that a convex function can be non-differentiable at countably many points, for example $f(x)=\int\lfloor x\rfloor\,dx$. I just made this theorem up, but my heuristic for why it should be true is that the only possible non-differentiable singularities I can imagine in a convex function are corners, and these involve a jump discontinuity in the derivative, so since the derivative is increasing (where it is defined), you get an inequality like $f'(y)-f'(x)\ge \sum_{t\in(x,y)}\omega_t(f')$, where $\omega_t(f')$ is the oscillation at $t$ (limit from right minus limit from left) and the sum is over all real numbers between $x$ and $y$. Since the sum is convergent (assuming that $x\le y$ are points such that $f$ is differentiable at $x$ and $y$ so that this makes sense), there can only be countably many values in the sum which are non-zero, and at all other points the oscillation is zero and so the derivative exists. Thus there are only countably many non-differentiable points in the interval $(x,y)$, so as long as there is a suitable sequence $(x_n)\to-\infty$, $(y_n)\to\infty$ of differentiable points, the total number of non-differentiable points is a countable union of countable sets, which is countable. Furthermore, I would conjecture that the set of non-differentiable points has empty interior-of-closure, i.e. you can't make a function that is non-differentiable at the rational numbers, but as the above discussion shows there are still a lot of holes in the proof (and I'm making a lot of unjustified assumptions regarding the derivative already being somewhat well-defined). Does anyone know how to approach such a statement?",,"['real-analysis', 'derivatives', 'convex-analysis']"
10,Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$.,Prove that .,f'(a)=\lim_{x\rightarrow a}f'(x),"Let $f$ be a real-valued function continuous on $[a,b]$ and differentiable on $(a,b)$. Suppose that $\lim_{x\rightarrow a}f'(x)$ exists. Then, prove that $f$ is differentiable at $a$ and $f'(a)=\lim_{x\rightarrow a}f'(x)$. It seems like an easy example, but a little bit tricky. I'm not sure which theorems should be used in here. ============================================================== Using @David Mitra's advice and @Pete L. Clark's notes I tried to solve this proof. I want to know my proof is correct or not. By MVT, for $h>0$ and $c_h \in (a,a+h)$ $$\frac{f(a+h)-f(a)}{h}=f'(c_h)$$ and $\lim_{h \rightarrow 0^+}c_h=a$. Then $$\lim_{h \rightarrow 0^+}\frac{f(a+h)-f(a)}{h}=\lim_{h \rightarrow 0^+}f'(c_h)=\lim_{h \rightarrow 0^+}f'(a)$$ But that's enough? I think I should show something more, but don't know what it is.","Let $f$ be a real-valued function continuous on $[a,b]$ and differentiable on $(a,b)$. Suppose that $\lim_{x\rightarrow a}f'(x)$ exists. Then, prove that $f$ is differentiable at $a$ and $f'(a)=\lim_{x\rightarrow a}f'(x)$. It seems like an easy example, but a little bit tricky. I'm not sure which theorems should be used in here. ============================================================== Using @David Mitra's advice and @Pete L. Clark's notes I tried to solve this proof. I want to know my proof is correct or not. By MVT, for $h>0$ and $c_h \in (a,a+h)$ $$\frac{f(a+h)-f(a)}{h}=f'(c_h)$$ and $\lim_{h \rightarrow 0^+}c_h=a$. Then $$\lim_{h \rightarrow 0^+}\frac{f(a+h)-f(a)}{h}=\lim_{h \rightarrow 0^+}f'(c_h)=\lim_{h \rightarrow 0^+}f'(a)$$ But that's enough? I think I should show something more, but don't know what it is.",,"['real-analysis', 'limits', 'derivatives', 'continuity']"
11,Limit of $L^p$ norm when $p\to0$,Limit of  norm when,L^p p\to0,"Let ($\Omega$, $\cal{F}$, $\mu$) be a probability space and $f\in L^1(\Omega)$. Prove that $$\displaystyle\lim_{p\to 0} \left[ \int_{\Omega}|f|^pd\mu \right]^{\frac{1}{p}}=\exp \left[ \int_{\Omega}\log|f| d\mu  \right],$$ where $\exp[-\infty]=0$. To simplify the problem, we may assume $\log|f|\in L^1(\Omega).$","Let ($\Omega$, $\cal{F}$, $\mu$) be a probability space and $f\in L^1(\Omega)$. Prove that $$\displaystyle\lim_{p\to 0} \left[ \int_{\Omega}|f|^pd\mu \right]^{\frac{1}{p}}=\exp \left[ \int_{\Omega}\log|f| d\mu  \right],$$ where $\exp[-\infty]=0$. To simplify the problem, we may assume $\log|f|\in L^1(\Omega).$",,"['real-analysis', 'measure-theory']"
12,Show $(1+\frac{1}{3}-\frac{1}{5}-\frac{1}{7}+\frac{1}{9}+\frac{1}{11}-\cdots)^2 = 1+\frac{1}{9}+\frac{1}{25}+\frac{1}{49} + \cdots$,Show,(1+\frac{1}{3}-\frac{1}{5}-\frac{1}{7}+\frac{1}{9}+\frac{1}{11}-\cdots)^2 = 1+\frac{1}{9}+\frac{1}{25}+\frac{1}{49} + \cdots,"Last month I was calculating $\displaystyle \int_0^\infty \frac{1}{1+x^4}\, dx$ when I stumbled on the surprising identity: $$\sum_{n=0}^\infty (-1)^n\left(\frac{1}{4n+1} +\frac{1}{4n+3}\right) = \frac{\pi}{\sqrt8}$$ and I knew $$\sum_{n=0}^\infty \frac{1}{(2n+1)^2} = \frac{\pi^2}{8}$$ So if I could find a proof that $$\left(\sum_{n=0}^\infty (-1)^n\left(\frac{1}{4n+1} +\frac{1}{4n+3}\right)\right)^2 = \sum_{n=0}^\infty \frac{1}{(2n+1)^2}$$ then this could be a new proof that $\zeta(2)=\frac{\pi^2}{6}$. I've thought over this for almost a month and I'm no closer on showing this identity. Note: Article on the multiplication of conditionally convergent series: http://www.jstor.org/stable/2369519","Last month I was calculating $\displaystyle \int_0^\infty \frac{1}{1+x^4}\, dx$ when I stumbled on the surprising identity: $$\sum_{n=0}^\infty (-1)^n\left(\frac{1}{4n+1} +\frac{1}{4n+3}\right) = \frac{\pi}{\sqrt8}$$ and I knew $$\sum_{n=0}^\infty \frac{1}{(2n+1)^2} = \frac{\pi^2}{8}$$ So if I could find a proof that $$\left(\sum_{n=0}^\infty (-1)^n\left(\frac{1}{4n+1} +\frac{1}{4n+3}\right)\right)^2 = \sum_{n=0}^\infty \frac{1}{(2n+1)^2}$$ then this could be a new proof that $\zeta(2)=\frac{\pi^2}{6}$. I've thought over this for almost a month and I'm no closer on showing this identity. Note: Article on the multiplication of conditionally convergent series: http://www.jstor.org/stable/2369519",,"['real-analysis', 'sequences-and-series', 'number-theory']"
13,"Is $ \pi $ definable in $(\Bbb R,0,1,+,×, <,\exp) $?",Is  definable in ?," \pi  (\Bbb R,0,1,+,×, <,\exp) ","Is there a first-order formula $\phi(x) $ with exactly one free variable $ x $ in the language of ordered fields together with the unary function symbol $ \exp $ such that in the standard interpretation of this language in $\Bbb R $ (where $ \exp $ is interpreted as the exponential function $ x \mapsto e^x $), $\phi (x) $ holds iff $ x=\pi $? EDIT: As Levon pointed out, a negative answer to this problem would imply that $π$ and $e$ (and $e^e$, $(2e)^{3e^2}$, and so on) are algebraically independent over $\Bbb Q$, which is an unsolved problem. So, if you think that a definition of $\pi$ is impossible, I would be pleased if you could show something like, that it is possible to reduce $\phi$ to a formula which contains no terms involving bound variables inside exponential functions, which would reduce the problem more or less to a question on algebraical independece. However because there are such intricate connections between exponential and trigonometric functions, I don't think that $\pi$ should be undefinable.","Is there a first-order formula $\phi(x) $ with exactly one free variable $ x $ in the language of ordered fields together with the unary function symbol $ \exp $ such that in the standard interpretation of this language in $\Bbb R $ (where $ \exp $ is interpreted as the exponential function $ x \mapsto e^x $), $\phi (x) $ holds iff $ x=\pi $? EDIT: As Levon pointed out, a negative answer to this problem would imply that $π$ and $e$ (and $e^e$, $(2e)^{3e^2}$, and so on) are algebraically independent over $\Bbb Q$, which is an unsolved problem. So, if you think that a definition of $\pi$ is impossible, I would be pleased if you could show something like, that it is possible to reduce $\phi$ to a formula which contains no terms involving bound variables inside exponential functions, which would reduce the problem more or less to a question on algebraical independece. However because there are such intricate connections between exponential and trigonometric functions, I don't think that $\pi$ should be undefinable.",,"['real-analysis', 'model-theory', 'first-order-logic']"
14,Prove $\frac{x^2}{y}+\frac{y^2}{z}+\frac{z^2}{x}\ge 4+(x-y)^2$ for positives $4 \le x + y + z \le 5$,Prove  for positives,\frac{x^2}{y}+\frac{y^2}{z}+\frac{z^2}{x}\ge 4+(x-y)^2 4 \le x + y + z \le 5,"Let $x,y,z>0$ , and such $$4\le x+y+z\le 5.$$ Show that $$\dfrac{x^2}{y}+\dfrac{y^2}{z}+\dfrac{z^2}{x}\ge 4+(x-y)^2.$$ It seems that the condition $\dfrac{x^2}{y}+\dfrac{y^2}{z}+\dfrac{z^2}{x}\ge 4+(x-y)^2$ is maybe old, and this condition is strange? http://www.artofproblemsolving.com/Forum/viewtopic.php?f=52&t=586357 . I want to use $$\dfrac{x^2}{y}=2x-y+\dfrac{(x-y)^2}{y}.$$ If we let $$x\to x'r,y\to y'r,z\to z'r,$$ then $$x'+y'+z'=4, r\in [1,\dfrac{5}{4}]$$ But I can't prove that either, because I felt I can't use the condition. Thank you.","Let , and such Show that It seems that the condition is maybe old, and this condition is strange? http://www.artofproblemsolving.com/Forum/viewtopic.php?f=52&t=586357 . I want to use If we let then But I can't prove that either, because I felt I can't use the condition. Thank you.","x,y,z>0 4\le x+y+z\le 5. \dfrac{x^2}{y}+\dfrac{y^2}{z}+\dfrac{z^2}{x}\ge 4+(x-y)^2. \dfrac{x^2}{y}+\dfrac{y^2}{z}+\dfrac{z^2}{x}\ge 4+(x-y)^2 \dfrac{x^2}{y}=2x-y+\dfrac{(x-y)^2}{y}. x\to x'r,y\to y'r,z\to z'r, x'+y'+z'=4, r\in [1,\dfrac{5}{4}]","['real-analysis', 'algebra-precalculus', 'inequality']"
15,Why are norms continuous? [closed],Why are norms continuous? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Describe why norms are continuous function by mathematical symbols.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Describe why norms are continuous function by mathematical symbols.",,"['real-analysis', 'functional-analysis', 'continuity']"
16,Proving $ f(x) = x^2 $ is not uniformly continuous on the real line,Proving  is not uniformly continuous on the real line, f(x) = x^2 ,"This is  homework problem and the very premise has me stumped. It's in a text on PDE. The exercise says to show that $ f(x) = x^2 $ is not uniformly continuous on the real line. But every definition I know says that it is a continuous function, and unless you attach some special condition, like restricting the interval or making it a periodic function (perhaps saying $f(x-2) = f(x)$ or some such) it's by definition continuous. There's always a derivative since $f'(x) = 2x$. The preceding chapter is about Drichelet and the like, as an extension of Fourier series, so I am guessing that a Fourier expansion does something here but every proof of the proposition seems to have nothing to do with Fourier series in the slightest. So I am pretty lost here. This whole question seems utterly nonsensical.","This is  homework problem and the very premise has me stumped. It's in a text on PDE. The exercise says to show that $ f(x) = x^2 $ is not uniformly continuous on the real line. But every definition I know says that it is a continuous function, and unless you attach some special condition, like restricting the interval or making it a periodic function (perhaps saying $f(x-2) = f(x)$ or some such) it's by definition continuous. There's always a derivative since $f'(x) = 2x$. The preceding chapter is about Drichelet and the like, as an extension of Fourier series, so I am guessing that a Fourier expansion does something here but every proof of the proposition seems to have nothing to do with Fourier series in the slightest. So I am pretty lost here. This whole question seems utterly nonsensical.",,['real-analysis']
17,Why are continuous functions not dense in $L^\infty$?,Why are continuous functions not dense in ?,L^\infty,Why are the continuous functions not dense in $L^\infty$? I mean both concretely (i.e. a counter example) and intuitively why is this the case.,Why are the continuous functions not dense in $L^\infty$? I mean both concretely (i.e. a counter example) and intuitively why is this the case.,,"['real-analysis', 'measure-theory', 'probability-theory']"
18,Is Dirichlet function Riemann integrable?,Is Dirichlet function Riemann integrable?,,"""Dirichlet function"" is meant to be the characteristic function of rational numbers on $[a,b]\subset\mathbb{R}$. On one hand, a function on $[a,b]$ is Riemann integrable if and only if it is bounded and continuous almost everywhere, which the Dirichlet function satisfies. On the other hand, the upper integral of Dirichlet function is $b-a$, while the lower integral is $0$. They don't match, so that the function is not Riemann integrable. I feel confused about which explanation I should choose...","""Dirichlet function"" is meant to be the characteristic function of rational numbers on $[a,b]\subset\mathbb{R}$. On one hand, a function on $[a,b]$ is Riemann integrable if and only if it is bounded and continuous almost everywhere, which the Dirichlet function satisfies. On the other hand, the upper integral of Dirichlet function is $b-a$, while the lower integral is $0$. They don't match, so that the function is not Riemann integrable. I feel confused about which explanation I should choose...",,"['calculus', 'real-analysis', 'measure-theory', 'lebesgue-integral', 'riemann-integration']"
19,"A continuous, injective function $f: \mathbb{R} \to \mathbb{R}$ is either strictly increasing or strictly decreasing.","A continuous, injective function  is either strictly increasing or strictly decreasing.",f: \mathbb{R} \to \mathbb{R},"I would like to prove the statement in the title. Proof: We prove that if $f$ is not strictly decreasing, then it must be strictly increasing. So suppose $x < y$ . And that's pretty much how far I got.  Help will be appreciated.","I would like to prove the statement in the title. Proof: We prove that if is not strictly decreasing, then it must be strictly increasing. So suppose . And that's pretty much how far I got.  Help will be appreciated.",f x < y,"['calculus', 'real-analysis']"
20,Prove that the inverse image of an open set is open,Prove that the inverse image of an open set is open,,"Let $ X \subset \mathbb{R}$ be a non-empty, open set and let $f: X \rightarrow \mathbb{R}$ be a continuous function. Show that the inverse image of an open set is open under f, i.e. show: If $M \subset \mathbb{R}$ is open, then $f^{-1}(M)$ is open as well. I think that this should basically follow from the definitions, but I'm still having some troubles with the proof. Suppose $M \subset \mathbb{R}$ is open, then it follows that $\forall z \in M, \exists r > 0: K_r(z) \subseteq M$ Since f is continuous it follows for each $x_0 \in X$ that for each $\epsilon > 0 ,\exists \delta > 0 : |f(x)-f(x_0)|< \epsilon, \forall x \in X: |x-x_0|<\delta$ Now I need to show that $f^{-1}(M)$ is open, so that for each $z \in M, \exists r>0: K_r(f^{-1}(z_0)) \subseteq f^{-1}(M)$ I'm not sure how to prove this. I guess since $z = f(x)$ and since M is open we have that $\exists r>0:\{z:|z-z_0|<r\}\subseteq M$. And since f is continuous: $|f(x)-f(x_0)| = |z-z_0|< \epsilon$ for all $|x-x_0|<\delta$, but since $|x-x_0|=|f^{-1}(z)-f^{-1}(z_0)|<\delta$ it somehow follows from that that $f^{-1}(M)$ is open?! Which is just horrible and probably completely wrong. But I'm really confused from all the definitions right now and need some help please.","Let $ X \subset \mathbb{R}$ be a non-empty, open set and let $f: X \rightarrow \mathbb{R}$ be a continuous function. Show that the inverse image of an open set is open under f, i.e. show: If $M \subset \mathbb{R}$ is open, then $f^{-1}(M)$ is open as well. I think that this should basically follow from the definitions, but I'm still having some troubles with the proof. Suppose $M \subset \mathbb{R}$ is open, then it follows that $\forall z \in M, \exists r > 0: K_r(z) \subseteq M$ Since f is continuous it follows for each $x_0 \in X$ that for each $\epsilon > 0 ,\exists \delta > 0 : |f(x)-f(x_0)|< \epsilon, \forall x \in X: |x-x_0|<\delta$ Now I need to show that $f^{-1}(M)$ is open, so that for each $z \in M, \exists r>0: K_r(f^{-1}(z_0)) \subseteq f^{-1}(M)$ I'm not sure how to prove this. I guess since $z = f(x)$ and since M is open we have that $\exists r>0:\{z:|z-z_0|<r\}\subseteq M$. And since f is continuous: $|f(x)-f(x_0)| = |z-z_0|< \epsilon$ for all $|x-x_0|<\delta$, but since $|x-x_0|=|f^{-1}(z)-f^{-1}(z_0)|<\delta$ it somehow follows from that that $f^{-1}(M)$ is open?! Which is just horrible and probably completely wrong. But I'm really confused from all the definitions right now and need some help please.",,"['real-analysis', 'continuity']"
21,Evaluating $\int_0^1 \frac{\log x \log \left(1-x^4 \right)}{1+x^2}dx$,Evaluating,\int_0^1 \frac{\log x \log \left(1-x^4 \right)}{1+x^2}dx,"I am trying to prove that \begin{equation} \int_{0}^{1}\frac{\log\left(x\right) \log\left(\,{1 - x^{4}}\,\right)}{1 + x^{2}} \,\mathrm{d}x = \frac{\pi^{3}}{16} - 3\mathrm{G}\log\left(2\right) \tag{1} \end{equation} where $\mathrm{G}$ is Catalan's Constant . I was able to express it in terms of Euler Sums but it does not seem to be of any use. \begin{align} &\int_{0}^{1}\frac{\log\left(x\right) \log\left(\,{1 - x^{4}}\,\right)}{1 + x^{2}} \,\mathrm{d}x \\[3mm] = &\ \frac{1}{16}\sum_{n = 1}^{\infty} \frac{\psi_{1}\left(1/4 + n\right) - \psi_{1}\left(3/4 + n\right)}{n} \tag{2} \end{align} Here $\psi_{n}\left(z\right)$ denotes the polygamma function . Can you help me solve this problem $?$ .",I am trying to prove that where is Catalan's Constant . I was able to express it in terms of Euler Sums but it does not seem to be of any use. Here denotes the polygamma function . Can you help me solve this problem .,"\begin{equation}
\int_{0}^{1}\frac{\log\left(x\right)
\log\left(\,{1 - x^{4}}\,\right)}{1 + x^{2}}
\,\mathrm{d}x = \frac{\pi^{3}}{16} - 3\mathrm{G}\log\left(2\right)
\tag{1}
\end{equation} \mathrm{G} \begin{align}
&\int_{0}^{1}\frac{\log\left(x\right)
\log\left(\,{1 - x^{4}}\,\right)}{1 + x^{2}}
\,\mathrm{d}x
\\[3mm] = &\
\frac{1}{16}\sum_{n = 1}^{\infty}
\frac{\psi_{1}\left(1/4 + n\right) -
\psi_{1}\left(3/4 + n\right)}{n} \tag{2}
\end{align} \psi_{n}\left(z\right) ?","['real-analysis', 'integration', 'definite-integrals', 'closed-form']"
22,Prove that this function is bounded,Prove that this function is bounded,,"This is an exercise from Problems from the Book by Andreescu and Dospinescu.  When it was posted on AoPS a year ago I spent several hours trying to solve it, but to no avail, so I am hoping someone here can enlighten me. Problem: Prove that the function $f : [0, 1) \to \mathbb{R}$ defined by $\displaystyle f(x) = \log_2 (1 - x) + x + x^2 + x^4 + x^8 + ...$ is bounded. A preliminary observation is that $f$ satisfies $f(x^2) = f(x) + \log_2 (1 + x) - x$.  I played around with using this functional equation for awhile, but couldn't quite make it work.","This is an exercise from Problems from the Book by Andreescu and Dospinescu.  When it was posted on AoPS a year ago I spent several hours trying to solve it, but to no avail, so I am hoping someone here can enlighten me. Problem: Prove that the function $f : [0, 1) \to \mathbb{R}$ defined by $\displaystyle f(x) = \log_2 (1 - x) + x + x^2 + x^4 + x^8 + ...$ is bounded. A preliminary observation is that $f$ satisfies $f(x^2) = f(x) + \log_2 (1 + x) - x$.  I played around with using this functional equation for awhile, but couldn't quite make it work.",,"['real-analysis', 'functional-equations', 'asymptotics']"
23,Understanding the definition of Cauchy sequence,Understanding the definition of Cauchy sequence,,"My question is related with the definition of Cauchy sequence As we know that a sequence $(x_n)$ of real numbers is called Cauchy, if for every positive real number ε, there is a positive integer $N \in \mathbb{N}$ such that for all natural numbers $m, n > N$ $\mid x_m -x_n\mid < \epsilon$ My questions are 1  : What is the significance of choosing $m, n > N$  ? 2:  How to choose $N$? 3: Can we see it geometrically? I would be pleased if someone can make me understand through examples. Thanks","My question is related with the definition of Cauchy sequence As we know that a sequence $(x_n)$ of real numbers is called Cauchy, if for every positive real number ε, there is a positive integer $N \in \mathbb{N}$ such that for all natural numbers $m, n > N$ $\mid x_m -x_n\mid < \epsilon$ My questions are 1  : What is the significance of choosing $m, n > N$  ? 2:  How to choose $N$? 3: Can we see it geometrically? I would be pleased if someone can make me understand through examples. Thanks",,"['real-analysis', 'metric-spaces']"
24,Can we construct a function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that it has intermediate value property and discontinuous everywhere?,Can we construct a function  such that it has intermediate value property and discontinuous everywhere?,f:\mathbb{R} \rightarrow \mathbb{R},"Can we construct a function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that it has intermediate value property and discontinuous everywhere? I think it is probable because we can consider  $$  y =  \begin{cases} \sin \left( \frac{1}{x} \right), & \text{if } x \neq 0, \\ 0, & \text{if } x=0. \end{cases} $$ This function has intermediate value property but is discontinuous on $x=0$. Inspired by this example, let $r_n$ denote the rational number,and define  $$  y =  \begin{cases} \sum_{n=1}^{\infty} \frac{1}{2^n} \left| \sin \left( \frac{1}{x-r_n} \right) \right|, & \text{if } x \notin \mathbb{Q}, \\ 0, & \mbox{if }x \in \mathbb{Q}. \end{cases} $$ It is easy to see this function is discontinuons if $x$ is not a rational number. But I can't verify its intermediate value property.","Can we construct a function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that it has intermediate value property and discontinuous everywhere? I think it is probable because we can consider  $$  y =  \begin{cases} \sin \left( \frac{1}{x} \right), & \text{if } x \neq 0, \\ 0, & \text{if } x=0. \end{cases} $$ This function has intermediate value property but is discontinuous on $x=0$. Inspired by this example, let $r_n$ denote the rational number,and define  $$  y =  \begin{cases} \sum_{n=1}^{\infty} \frac{1}{2^n} \left| \sin \left( \frac{1}{x-r_n} \right) \right|, & \text{if } x \notin \mathbb{Q}, \\ 0, & \mbox{if }x \in \mathbb{Q}. \end{cases} $$ It is easy to see this function is discontinuons if $x$ is not a rational number. But I can't verify its intermediate value property.",,"['real-analysis', 'analysis', 'examples-counterexamples']"
25,"Continuous functions on $[0,1]$ is dense in $L^p[0,1]$ for $1\leq p< \infty$",Continuous functions on  is dense in  for,"[0,1] L^p[0,1] 1\leq p< \infty","I tried to show that the continuous functions on $[0,1]$ are dense in $L^p[0,1]$ for $ 1 \leq p< \infty $ by using Lusin's theorem. I proceeded as follows.. By using Lusin's theorem, for any $f \in L^p[0,1]$, for any given $ \epsilon $ $ > $ 0, there exists a closed set $ F_\epsilon $ such that $ m([0,1]- F_\epsilon) < \epsilon$ and $f$ restricted to $F_\epsilon$ is continuous. Using Tietze's extension theorem, extend $f$ to a continuous function $g$ on $[0,1]$. We claim that $\Vert f-g\Vert_p $ is sufficiently small. $$ \Vert f-g\Vert_p ^p = \displaystyle \int_{[0,1]-F_\epsilon} |f(x)-g(x)|^p dx  $$ $$  \leq \displaystyle \int_{[0,1]-F_\epsilon} 2^p (|f(x)|^p + |g(x)|^p) dx  $$  now using properties of $L^p$ functions, we can make first part of our integral sufficiently small. furthermore, since $g$ is conti on $[0,1]$, $g$ has an upper bound $M$, so that second part of integration also become sufficiently small. I thought I solved problem, but there was a serious problem.. our choice of g is dependent of $\epsilon$ , so constant $M$ is actually dependent of $\epsilon$, so it is not guaranteed that second part of integration becomes 0 as $\epsilon $ tends to 0. I think if our choice of extension can be chosen further specifically, for example, by imposing $g \leq f$ such kind of argument would work. Can anyone help to complete my proof here?","I tried to show that the continuous functions on $[0,1]$ are dense in $L^p[0,1]$ for $ 1 \leq p< \infty $ by using Lusin's theorem. I proceeded as follows.. By using Lusin's theorem, for any $f \in L^p[0,1]$, for any given $ \epsilon $ $ > $ 0, there exists a closed set $ F_\epsilon $ such that $ m([0,1]- F_\epsilon) < \epsilon$ and $f$ restricted to $F_\epsilon$ is continuous. Using Tietze's extension theorem, extend $f$ to a continuous function $g$ on $[0,1]$. We claim that $\Vert f-g\Vert_p $ is sufficiently small. $$ \Vert f-g\Vert_p ^p = \displaystyle \int_{[0,1]-F_\epsilon} |f(x)-g(x)|^p dx  $$ $$  \leq \displaystyle \int_{[0,1]-F_\epsilon} 2^p (|f(x)|^p + |g(x)|^p) dx  $$  now using properties of $L^p$ functions, we can make first part of our integral sufficiently small. furthermore, since $g$ is conti on $[0,1]$, $g$ has an upper bound $M$, so that second part of integration also become sufficiently small. I thought I solved problem, but there was a serious problem.. our choice of g is dependent of $\epsilon$ , so constant $M$ is actually dependent of $\epsilon$, so it is not guaranteed that second part of integration becomes 0 as $\epsilon $ tends to 0. I think if our choice of extension can be chosen further specifically, for example, by imposing $g \leq f$ such kind of argument would work. Can anyone help to complete my proof here?",,['real-analysis']
26,Historical Mistake of Assuming Measurability,Historical Mistake of Assuming Measurability,,"I am recently reading about Fourier transforms and convolutions. It was a surprise to me that it takes quite several paragraphs to prove the measurability of innocent looking $f(x-y)$ (reference: proof that $\hat{f}(x,y)=f(x-y)$ is measurable if $f$ is measurable, Stein & Shakarchi Prop 3.9 ) . It makes me wonder : Does it ever happen  in history that mathematicians publish wrong results because they assumed measurability of some (innocent looking) sets or functions ?","I am recently reading about Fourier transforms and convolutions. It was a surprise to me that it takes quite several paragraphs to prove the measurability of innocent looking (reference: proof that $\hat{f}(x,y)=f(x-y)$ is measurable if $f$ is measurable, Stein & Shakarchi Prop 3.9 ) . It makes me wonder : Does it ever happen  in history that mathematicians publish wrong results because they assumed measurability of some (innocent looking) sets or functions ?",f(x-y),"['real-analysis', 'analysis', 'measure-theory', 'math-history', 'fake-proofs']"
27,Does complex dynamics offer any insights into real dynamics?,Does complex dynamics offer any insights into real dynamics?,,"One of the most fascinating things about complex analysis is that it provides insights into real analysis. Here are two pictures from Needham's book Visual Complex Analysis (p. 65): Picture 1 (real). Consider the following two functions. Both have the radius of convergence $R = 1$. But while it's clear what is the problem with [a] (its behavior at $x =\pm 1$), it's not clear what's stopping [b] from having a larger radius of convergence. Picture 2 (complex). The mystery is revealed when we realize that from the point of view of the complex analysis, both [a] and [b] are slices of the same function and the radius of convergence is just the distance to the nearest pole. Is there something similar (not necessarily with pictures) regarding complex dynamics? Does complex dynamics offer any insights into real dynamics? Edit : by ""real dynamics"" I mean dynamics of real maps.","One of the most fascinating things about complex analysis is that it provides insights into real analysis. Here are two pictures from Needham's book Visual Complex Analysis (p. 65): Picture 1 (real). Consider the following two functions. Both have the radius of convergence $R = 1$. But while it's clear what is the problem with [a] (its behavior at $x =\pm 1$), it's not clear what's stopping [b] from having a larger radius of convergence. Picture 2 (complex). The mystery is revealed when we realize that from the point of view of the complex analysis, both [a] and [b] are slices of the same function and the radius of convergence is just the distance to the nearest pole. Is there something similar (not necessarily with pictures) regarding complex dynamics? Does complex dynamics offer any insights into real dynamics? Edit : by ""real dynamics"" I mean dynamics of real maps.",,"['real-analysis', 'complex-analysis', 'dynamical-systems', 'applications', 'complex-dynamics']"
28,Cauchy in measure implies convergent in measure.,Cauchy in measure implies convergent in measure.,,"A sequence $\{f_n\}$ of measurable functions is said to be a Cauchy sequence in measure if, given $ϵ > 0$, there is an $N$ such that for all $m, n ≥ N$ we have $m\{x \in E  :  |f_n(x) − f_m(x)| \ge ϵ\} < ϵ$. Show that if $\{f_n\}$ is Cauchy in measure, then there exists a measurable function $f$ to which the sequence $\{f_n\}$ converges in measure. Idea:  I need to show that $m\{x \in E : |f(x) − f_n(x)| \ge ϵ\} \rightarrow 0$ for some measurable function $f$.  I was thinking that there exists $f(x) = \lim_{k\to \infty} f_{n_k}(x)$ where $f_{n_k}$ is a subsequence. But I am unclear as to where to proceed.","A sequence $\{f_n\}$ of measurable functions is said to be a Cauchy sequence in measure if, given $ϵ > 0$, there is an $N$ such that for all $m, n ≥ N$ we have $m\{x \in E  :  |f_n(x) − f_m(x)| \ge ϵ\} < ϵ$. Show that if $\{f_n\}$ is Cauchy in measure, then there exists a measurable function $f$ to which the sequence $\{f_n\}$ converges in measure. Idea:  I need to show that $m\{x \in E : |f(x) − f_n(x)| \ge ϵ\} \rightarrow 0$ for some measurable function $f$.  I was thinking that there exists $f(x) = \lim_{k\to \infty} f_{n_k}(x)$ where $f_{n_k}$ is a subsequence. But I am unclear as to where to proceed.",,"['real-analysis', 'measure-theory', 'convergence-divergence']"
29,dual of $H^1_0$: $H^{-1}$ or $H_0^1$?,dual of :  or ?,H^1_0 H^{-1} H_0^1,"I have a problem related to dual of Sobolev space $H^1_0$. By definition, the dual of $H^1_0$ is $H^{-1}$, which contains $L^2$ as a subspace. However, from Riesz representation theorem, dual of a Hilbert space is itself (in the sense of isomorphism). It seems that this implies dual of $H^1_0$ is $H^1_0$ itself. From the result, I think the latter argument is not right. But where's the mistake? This problem may related to the question about Hilbert space and its dual A paradox on Hilbert spaces and their duals However I couldn't understand the answers well after reading several times... Thanks so much! Thanks a lot for all the kind solutions! The following are my conclusions: Dual space strongly depends on action we take. For example, in our case, if we take $H^1$ inner product, the dual space is $H_0^1$ itself; if taking $L^2$ inner product, the dual space is larger than $L^2$; and taking all possible dual actions, the dual is by definition $H^{-1}$. As dual of one space, there are certainly identifications between these spaces. Notice that identification between infinite dimensional spaces can be bijective and ISOMETRIC! Indeed there is a simple example: sequence {1,2,3...} and {2,4,6...}. Just taking metric of the latter to be half. Another basic example is simple elliptic equation in $H_0^1$: $$ \int_\Omega \nabla u\cdot \nabla v+\int_\Omega uv=\langle f,v\rangle , \forall v\in H_0^1(\Omega) $$ where $f\in H^{-1}$. This gives isometric identification of $H^{-1}$ and $H_0^1$. Brezis's book reminds that we can't take two different inner products to give both Riesz representation at the same time. The example there is very instructive.","I have a problem related to dual of Sobolev space $H^1_0$. By definition, the dual of $H^1_0$ is $H^{-1}$, which contains $L^2$ as a subspace. However, from Riesz representation theorem, dual of a Hilbert space is itself (in the sense of isomorphism). It seems that this implies dual of $H^1_0$ is $H^1_0$ itself. From the result, I think the latter argument is not right. But where's the mistake? This problem may related to the question about Hilbert space and its dual A paradox on Hilbert spaces and their duals However I couldn't understand the answers well after reading several times... Thanks so much! Thanks a lot for all the kind solutions! The following are my conclusions: Dual space strongly depends on action we take. For example, in our case, if we take $H^1$ inner product, the dual space is $H_0^1$ itself; if taking $L^2$ inner product, the dual space is larger than $L^2$; and taking all possible dual actions, the dual is by definition $H^{-1}$. As dual of one space, there are certainly identifications between these spaces. Notice that identification between infinite dimensional spaces can be bijective and ISOMETRIC! Indeed there is a simple example: sequence {1,2,3...} and {2,4,6...}. Just taking metric of the latter to be half. Another basic example is simple elliptic equation in $H_0^1$: $$ \int_\Omega \nabla u\cdot \nabla v+\int_\Omega uv=\langle f,v\rangle , \forall v\in H_0^1(\Omega) $$ where $f\in H^{-1}$. This gives isometric identification of $H^{-1}$ and $H_0^1$. Brezis's book reminds that we can't take two different inner products to give both Riesz representation at the same time. The example there is very instructive.",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
30,"Uniform convergence of derivatives, Tao 14.2.7.","Uniform convergence of derivatives, Tao 14.2.7.",,"This is ex. 14.2.7. from Terence Tao's Analysis II book. Let $I:=[a,b]$ be an interval and $f_n:I \rightarrow \mathbb R$ differentiable functions with $f_n'$ converges uniform to a function $g:I \rightarrow \mathbb R$. Suppose $\exists x_0 \in I: \lim \limits_{n \rightarrow \infty} f_n(x_0) = L \in \mathbb R$. Then the $f_n$ converge uniformly to a differentiable function $f:I \rightarrow \mathbb R$ with $f' = g$. We are not given that the $f_n'$ are continuous but he gives the hint that $$ d_{\infty}(f_n',f_m') \leq \epsilon \Rightarrow |(f_n(x)-f_m(x))-(f_n(x_0)-f_m(x_0))| \leq \epsilon |x-x_0| $$ This can be shown by the mean value theorem. My question is : How does this help me to prove the theorem ?","This is ex. 14.2.7. from Terence Tao's Analysis II book. Let $I:=[a,b]$ be an interval and $f_n:I \rightarrow \mathbb R$ differentiable functions with $f_n'$ converges uniform to a function $g:I \rightarrow \mathbb R$. Suppose $\exists x_0 \in I: \lim \limits_{n \rightarrow \infty} f_n(x_0) = L \in \mathbb R$. Then the $f_n$ converge uniformly to a differentiable function $f:I \rightarrow \mathbb R$ with $f' = g$. We are not given that the $f_n'$ are continuous but he gives the hint that $$ d_{\infty}(f_n',f_m') \leq \epsilon \Rightarrow |(f_n(x)-f_m(x))-(f_n(x_0)-f_m(x_0))| \leq \epsilon |x-x_0| $$ This can be shown by the mean value theorem. My question is : How does this help me to prove the theorem ?",,['real-analysis']
31,Geometric intuition for $\pi /4 = 1 - 1/3 + 1/5 - \cdots$?,Geometric intuition for ?,\pi /4 = 1 - 1/3 + 1/5 - \cdots,"Following reading this great post : Interesting and unexpected applications of $\pi$ , Vadim's answer reminded me of something an analysis professor had told me when I was an undergrad - that no one had ever given him a satisfactory intuitive explaination for why this series definition should be true (the implied assumption being that it is so simple, there should be some way to look at this to make it intuitive). Now it follows simply from putting $x=1$ in the series expansion $$ \tan^{-1}(x) = x - \frac{x^3}{3} + \frac{x^5}{5} - \cdots $$ but I don't see anything geometrically intuitive about this formula either!","Following reading this great post : Interesting and unexpected applications of $\pi$ , Vadim's answer reminded me of something an analysis professor had told me when I was an undergrad - that no one had ever given him a satisfactory intuitive explaination for why this series definition should be true (the implied assumption being that it is so simple, there should be some way to look at this to make it intuitive). Now it follows simply from putting $x=1$ in the series expansion $$ \tan^{-1}(x) = x - \frac{x^3}{3} + \frac{x^5}{5} - \cdots $$ but I don't see anything geometrically intuitive about this formula either!",,"['real-analysis', 'sequences-and-series', 'number-theory', 'intuition', 'pi']"
32,"$\lim_{n\to\infty}\frac{n -\big\lfloor\frac{n}{2}\big\rfloor+\big\lfloor\frac{n}{3}\big\rfloor-\dots}{n}$, a Brilliant problem",", a Brilliant problem",\lim_{n\to\infty}\frac{n -\big\lfloor\frac{n}{2}\big\rfloor+\big\lfloor\frac{n}{3}\big\rfloor-\dots}{n},"I encounter a question when visiting Brilliant : Find $\space\space\space\space\lim_{n\to\infty}s_n$ $=\lim_{n\to\infty}\frac{n - \big \lfloor \frac{n}{2} \big \rfloor+ \big \lfloor \frac{n}{3} \big \rfloor - \big \lfloor \frac{n}{4} \big \rfloor + \dots}{n}$ $=\lim_{n\to\infty}\frac{\sum_{k=1}^n(-1)^{k+1}\lfloor\frac{n}{k}\rfloor}{n}$ The answer in the website above doesn't really satisfies me, as the answer does not tell how the sequence converge and I doesn't understand how we can take subsequence $n_k=k!$ to solve the problem. I had some idea but doesn't seems to work: 1) It is easy to show $$s_n=\frac{\sum_{k=1}^{\big\lceil\frac{n}{2}\big\rceil}(\big\lfloor\frac{n}{2k-1}\big\rfloor-\big\lfloor\frac{n}{2k}\big\rfloor)}{n}$$ 2) On the other hand, $$s_n\approx\sum_{k=1}^n(-1)^{k+1}\frac{1}{k}\to\ln2$$ So I am wondering how $s_n\approx$ the alternate hamonic series $$\forall(n,k\in\mathbb N:n\ge k),\space\space\frac{n}{k}\in\Bigg[\bigg\lfloor\frac{n}{k}\bigg\rfloor,\bigg\lfloor\frac{n}{k}\bigg\rfloor+\bigg(\frac{k-1}{k}\bigg)\Bigg]$$ I tried to look at the graph , the sequence $s_n$ is very likely to converge to $\ln 2$, and the alternating harmonic series seems to be bounded by the graph of $s_n$ at most of the time. 3) Also I observed that $s_8=\frac{8-4+2-2+1-1+1-1}{8}=\frac{1}{2}$, the terms cancelled nicely, but I am afraid that the anlalogue is not generaly true for all $s_{2^k}$. 4) I have tried to use Stolz–Cesàro Theorem , but doesn't seems useful neither. 5) I know that $\forall x,y\in\mathbb R:x+y\in\mathbb Z, \lfloor x\rfloor+\lceil y\rceil=x+y$, which maybe is useful since we may thus write $s_n$ in a more beautiful manner? 6) If there is no $(-1)^{k+1}$, I think we can treat $s_n$ as a Riemann sum, but well, ... , seems useless. 7) I have tried to think about how many terms of summand of $ns_n$ is integer. 8) I have tried to think $\big\lfloor\frac{n}{k}\big\rfloor$ as the number of positive integer multiple of $k$ that $\lt n$, and I then considered sets of number that is counted and uncounted respectively, but well, the question doesn't seems that easy. Does this help? (1) (2) (3) Any help will be appreciate. Thank you! Remarks: I was wondering is there a deep subject studying this (if so references please). Can this (or variants) be represented as a simpler function?","I encounter a question when visiting Brilliant : Find $\space\space\space\space\lim_{n\to\infty}s_n$ $=\lim_{n\to\infty}\frac{n - \big \lfloor \frac{n}{2} \big \rfloor+ \big \lfloor \frac{n}{3} \big \rfloor - \big \lfloor \frac{n}{4} \big \rfloor + \dots}{n}$ $=\lim_{n\to\infty}\frac{\sum_{k=1}^n(-1)^{k+1}\lfloor\frac{n}{k}\rfloor}{n}$ The answer in the website above doesn't really satisfies me, as the answer does not tell how the sequence converge and I doesn't understand how we can take subsequence $n_k=k!$ to solve the problem. I had some idea but doesn't seems to work: 1) It is easy to show $$s_n=\frac{\sum_{k=1}^{\big\lceil\frac{n}{2}\big\rceil}(\big\lfloor\frac{n}{2k-1}\big\rfloor-\big\lfloor\frac{n}{2k}\big\rfloor)}{n}$$ 2) On the other hand, $$s_n\approx\sum_{k=1}^n(-1)^{k+1}\frac{1}{k}\to\ln2$$ So I am wondering how $s_n\approx$ the alternate hamonic series $$\forall(n,k\in\mathbb N:n\ge k),\space\space\frac{n}{k}\in\Bigg[\bigg\lfloor\frac{n}{k}\bigg\rfloor,\bigg\lfloor\frac{n}{k}\bigg\rfloor+\bigg(\frac{k-1}{k}\bigg)\Bigg]$$ I tried to look at the graph , the sequence $s_n$ is very likely to converge to $\ln 2$, and the alternating harmonic series seems to be bounded by the graph of $s_n$ at most of the time. 3) Also I observed that $s_8=\frac{8-4+2-2+1-1+1-1}{8}=\frac{1}{2}$, the terms cancelled nicely, but I am afraid that the anlalogue is not generaly true for all $s_{2^k}$. 4) I have tried to use Stolz–Cesàro Theorem , but doesn't seems useful neither. 5) I know that $\forall x,y\in\mathbb R:x+y\in\mathbb Z, \lfloor x\rfloor+\lceil y\rceil=x+y$, which maybe is useful since we may thus write $s_n$ in a more beautiful manner? 6) If there is no $(-1)^{k+1}$, I think we can treat $s_n$ as a Riemann sum, but well, ... , seems useless. 7) I have tried to think about how many terms of summand of $ns_n$ is integer. 8) I have tried to think $\big\lfloor\frac{n}{k}\big\rfloor$ as the number of positive integer multiple of $k$ that $\lt n$, and I then considered sets of number that is counted and uncounted respectively, but well, the question doesn't seems that easy. Does this help? (1) (2) (3) Any help will be appreciate. Thank you! Remarks: I was wondering is there a deep subject studying this (if so references please). Can this (or variants) be represented as a simpler function?",,"['real-analysis', 'sequences-and-series', 'limits', 'ceiling-and-floor-functions']"
33,Classifying the compact subsets of $L^p$,Classifying the compact subsets of,L^p,"Some of my favorite theorems in analysis are those which classify the (pre-)compact subsets of a particular space.  For example: The Heine-Borel Theorem classifies the compact subsets of $\mathbb{R}^n$. The Arzela-Ascoli Theorem classifies the compact subsets of $C(X,Y)$, where (usually) $X$ is compact and $Y$ is metric. Montel's Theorem classifies the compact subsets of $\text{Hol}(U)$. Can we give a similar description of the compact subsets of $L^p$? Notes: I realize that I'm being a little vague in two senses. First, $L^p$ of what?  Frankly, I don't know.  I would be very interested in seeing a theorem about $L^p[0,1]$, or more generally about $L^p(X)$ where $X$ is locally compact Hausdorff.  I just want to know what's out there. Second, what kind of ""classification"" am I looking for?  Well, hopefully one that is similar in spirit to the above three examples, and in some sense specific to $L^p$.  For instance, saying that a set is compact if and only if it is complete and totally bounded does not really count (since that is true in any metric space).","Some of my favorite theorems in analysis are those which classify the (pre-)compact subsets of a particular space.  For example: The Heine-Borel Theorem classifies the compact subsets of $\mathbb{R}^n$. The Arzela-Ascoli Theorem classifies the compact subsets of $C(X,Y)$, where (usually) $X$ is compact and $Y$ is metric. Montel's Theorem classifies the compact subsets of $\text{Hol}(U)$. Can we give a similar description of the compact subsets of $L^p$? Notes: I realize that I'm being a little vague in two senses. First, $L^p$ of what?  Frankly, I don't know.  I would be very interested in seeing a theorem about $L^p[0,1]$, or more generally about $L^p(X)$ where $X$ is locally compact Hausdorff.  I just want to know what's out there. Second, what kind of ""classification"" am I looking for?  Well, hopefully one that is similar in spirit to the above three examples, and in some sense specific to $L^p$.  For instance, saying that a set is compact if and only if it is complete and totally bounded does not really count (since that is true in any metric space).",,"['real-analysis', 'analysis', 'reference-request', 'functional-analysis']"
34,Expected absolute difference between two iid variables,Expected absolute difference between two iid variables,,"Suppose $X$ and $Y$ are iid random variables taking values in $[0,1]$, and let $\alpha > 0$. What is the maximum possible value of $\mathbb{E}|X-Y|^\alpha$? I have already asked this question for $\alpha = 1$ here : one can show that $\mathbb{E}|X-Y| \leq 1/2$ by integrating directly, and using some clever calculations. Basically, one has the useful identity $|X-Y| = \max{X,Y} - \min{X,Y}$, which allows a direct calculation. There is an easier argument to show $\mathbb{E}|X - Y|^2 \leq 1/2$. In both cases, the maximum is attained when the distribution is Bernoulli 1/2, i.e. $\mathbb{P}(X = 0) = \mathbb{P}(X = 1) = 1/2$. I suspect that this solution achieves the maximum for all $\alpha$ (it is always 1/2), but I have no ideas about how to try and prove this. Edit 1: @Shalop points out an easy proof for $\alpha > 1$, using the case $\alpha = 1$. Since $|x-y|^\alpha \leq |x-y|$ when $\alpha > 1$ and $x,y \in [0,1]$, $E|X-Y|^\alpha \leq E|X-Y| \leq 1/2$. So it only remains to deal with the case when $\alpha \in (0,1)$.","Suppose $X$ and $Y$ are iid random variables taking values in $[0,1]$, and let $\alpha > 0$. What is the maximum possible value of $\mathbb{E}|X-Y|^\alpha$? I have already asked this question for $\alpha = 1$ here : one can show that $\mathbb{E}|X-Y| \leq 1/2$ by integrating directly, and using some clever calculations. Basically, one has the useful identity $|X-Y| = \max{X,Y} - \min{X,Y}$, which allows a direct calculation. There is an easier argument to show $\mathbb{E}|X - Y|^2 \leq 1/2$. In both cases, the maximum is attained when the distribution is Bernoulli 1/2, i.e. $\mathbb{P}(X = 0) = \mathbb{P}(X = 1) = 1/2$. I suspect that this solution achieves the maximum for all $\alpha$ (it is always 1/2), but I have no ideas about how to try and prove this. Edit 1: @Shalop points out an easy proof for $\alpha > 1$, using the case $\alpha = 1$. Since $|x-y|^\alpha \leq |x-y|$ when $\alpha > 1$ and $x,y \in [0,1]$, $E|X-Y|^\alpha \leq E|X-Y| \leq 1/2$. So it only remains to deal with the case when $\alpha \in (0,1)$.",,"['real-analysis', 'probability', 'random-variables']"
35,Prove: bounded derivative if and only if uniform continuity,Prove: bounded derivative if and only if uniform continuity,,"The definition of uniform continuity of a real-valued function states: A function $f\colon A\mapsto\mathbb{R}$ is uniformly continuous on $A$ iff for every $\varepsilon \gt 0$ there exists a $\delta \gt 0$ such that for every $x$ and $y$ in $A$, whenever $y \in \left(x-\delta,x+\delta\right)$, it is the case that $f\left(y\right) \in \left(f\left(x\right)-\varepsilon,f\left(x\right)+\varepsilon\right)$. Basically how my book distinguishes this from point-wise continuity is that there exists a single $\delta$ that works for every point in the domain, so once we find that $\delta$, we know it works everywhere. On the other hand, point-wise continuity says that given a $c\in A$, there exists a $\delta$ such that the function is continuous at $c$, but all these $\delta$s may be different, perhaps depending on $c$, and we might not be able to find just one $\delta$ that works for all $c$s everywhere. I interpret this definition a completely different way, and I want to see if my conjecture is correct. I think that functions which have bounded derivatives are uniformly continuous. That is, if the ""steepness"" and ""shallowness"" of a function is limited to a certain minimum and maximum, then the there's a sufficiently small enough $\delta$ that we can use, particularly at the steepest part of the function (say at $x_0$), such that the output stays within the $\varepsilon$-neighborhood of $f\left(x_0\right)$. Is that correct? How can I prove/disprove it? The converse states that the derivative of a uniformly continuous function is bounded. Is that also true?","The definition of uniform continuity of a real-valued function states: A function $f\colon A\mapsto\mathbb{R}$ is uniformly continuous on $A$ iff for every $\varepsilon \gt 0$ there exists a $\delta \gt 0$ such that for every $x$ and $y$ in $A$, whenever $y \in \left(x-\delta,x+\delta\right)$, it is the case that $f\left(y\right) \in \left(f\left(x\right)-\varepsilon,f\left(x\right)+\varepsilon\right)$. Basically how my book distinguishes this from point-wise continuity is that there exists a single $\delta$ that works for every point in the domain, so once we find that $\delta$, we know it works everywhere. On the other hand, point-wise continuity says that given a $c\in A$, there exists a $\delta$ such that the function is continuous at $c$, but all these $\delta$s may be different, perhaps depending on $c$, and we might not be able to find just one $\delta$ that works for all $c$s everywhere. I interpret this definition a completely different way, and I want to see if my conjecture is correct. I think that functions which have bounded derivatives are uniformly continuous. That is, if the ""steepness"" and ""shallowness"" of a function is limited to a certain minimum and maximum, then the there's a sufficiently small enough $\delta$ that we can use, particularly at the steepest part of the function (say at $x_0$), such that the output stays within the $\varepsilon$-neighborhood of $f\left(x_0\right)$. Is that correct? How can I prove/disprove it? The converse states that the derivative of a uniformly continuous function is bounded. Is that also true?",,"['calculus', 'real-analysis', 'continuity']"
36,Prove that a set $E$ is closed iff it's complement $E^{c}$ is open,Prove that a set  is closed iff it's complement  is open,E E^{c},"I was wondering if this proof was right. $\Leftarrow$ Suppose $E$ is closed. Then choose $x\in E^{c}$, then $x\notin E$, and so $x$ is not a limit point of $E$. Hence there exists a neighborhood $N$ of $x$ such that $E \cap N$ is empty, such that $N \subset E^{c}$. Thus $x$ is an interior point of $E^{c}$ and $E^{c}$ is open $\Rightarrow$ Then suppose $E^{c}$ is open. Let $x$ be a limit point of $E$, then every neighborhood of $x$ contains a point of $E$, so that $x$ is not an interior point of $E^{c}$. Since $E^{c}$ is open, this means that $ x \in E$, Therefore $E$ is closed.","I was wondering if this proof was right. $\Leftarrow$ Suppose $E$ is closed. Then choose $x\in E^{c}$, then $x\notin E$, and so $x$ is not a limit point of $E$. Hence there exists a neighborhood $N$ of $x$ such that $E \cap N$ is empty, such that $N \subset E^{c}$. Thus $x$ is an interior point of $E^{c}$ and $E^{c}$ is open $\Rightarrow$ Then suppose $E^{c}$ is open. Let $x$ be a limit point of $E$, then every neighborhood of $x$ contains a point of $E$, so that $x$ is not an interior point of $E^{c}$. Since $E^{c}$ is open, this means that $ x \in E$, Therefore $E$ is closed.",,"['real-analysis', 'general-topology', 'proof-writing']"
37,"Closure of $\left\{\frac{\tan{n}}{n} | \, n \in \mathbb{N}\right\}$",Closure of,"\left\{\frac{\tan{n}}{n} | \, n \in \mathbb{N}\right\}","I tried searching mathematical literature but I was unable to come across anything apart from the non-convergence of the sequence. Simply put: Define $\displaystyle A = \left.\left\{\frac{\tan\left(n\right)}{n}\,\right\vert\, n \in \mathbb{N}\right\}.\quad$ Is $\overline{A} = \mathbb{R}$ ?.",I tried searching mathematical literature but I was unable to come across anything apart from the non-convergence of the sequence. Simply put: Define Is ?.,"\displaystyle A =
\left.\left\{\frac{\tan\left(n\right)}{n}\,\right\vert\,
n \in \mathbb{N}\right\}.\quad \overline{A} = \mathbb{R}","['real-analysis', 'general-topology']"
38,Why do introductory real analysis courses teach bottom up?,Why do introductory real analysis courses teach bottom up?,,"A big part of introductory real analysis courses is getting intuition for the $\epsilon-\delta\,$ proofs. For example, these types of proofs come up a lot when studying differentiation, continuity, and integration. Only later is the notion of open and closed sets introduced. Why not just introduce continuity in terms of open sets first (E.g. it would be a better visual representation)? It seems that the $\epsilon-\delta$ definition would be more understandable if a student is first exposed to the open set characterization.","A big part of introductory real analysis courses is getting intuition for the $\epsilon-\delta\,$ proofs. For example, these types of proofs come up a lot when studying differentiation, continuity, and integration. Only later is the notion of open and closed sets introduced. Why not just introduce continuity in terms of open sets first (E.g. it would be a better visual representation)? It seems that the $\epsilon-\delta$ definition would be more understandable if a student is first exposed to the open set characterization.",,"['real-analysis', 'general-topology', 'soft-question', 'education']"
39,Integral $I:=\int_0^1 \frac{\log^2 x}{x^2-x+1}\mathrm dx=\frac{10\pi^3}{81 \sqrt 3}$,Integral,I:=\int_0^1 \frac{\log^2 x}{x^2-x+1}\mathrm dx=\frac{10\pi^3}{81 \sqrt 3},"Hi how can we prove this integral below? $$ I:=\int_0^1 \frac{\log^2 x}{x^2-x+1}\mathrm dx=\frac{10\pi^3}{81 \sqrt 3} $$ I tried to use $$ I=\int_0^1 \frac{\log^2x}{1-x(1-x)}\mathrm dx $$ and now tried changing variables to $y=x(1-x)$ in order to write $$ I\propto \int_0^1 \sum_{n=0}^\infty y^n  $$ however I do not know how to manipulate the $\log^2 x$ term when doing this procedure when doing this substitution.  If we can do this the integral would be trivial from here. Complex methods are okay also, if you want to use this method we have complex roots at $x=(-1)^{1/3}$.  But what contour can we use suitable for the $\log^2x $ term? Thanks","Hi how can we prove this integral below? $$ I:=\int_0^1 \frac{\log^2 x}{x^2-x+1}\mathrm dx=\frac{10\pi^3}{81 \sqrt 3} $$ I tried to use $$ I=\int_0^1 \frac{\log^2x}{1-x(1-x)}\mathrm dx $$ and now tried changing variables to $y=x(1-x)$ in order to write $$ I\propto \int_0^1 \sum_{n=0}^\infty y^n  $$ however I do not know how to manipulate the $\log^2 x$ term when doing this procedure when doing this substitution.  If we can do this the integral would be trivial from here. Complex methods are okay also, if you want to use this method we have complex roots at $x=(-1)^{1/3}$.  But what contour can we use suitable for the $\log^2x $ term? Thanks",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
40,What is the difference between Cauchy and convergent sequence?,What is the difference between Cauchy and convergent sequence?,,I am really confused. I will appreciate if somebody can help me to define the difference between Cauchy and convergent sequence. Many thanks.,I am really confused. I will appreciate if somebody can help me to define the difference between Cauchy and convergent sequence. Many thanks.,,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'cauchy-sequences']"
41,Why is continuity characterized with open sets?,Why is continuity characterized with open sets?,,"Why is the topological definition of continuous in terms of open sets? I think my main complaint might be that the notion of open set seems too flexible/general and considers too many things that don't seem the right notion of ""closeness"". Conceptually, people explain ""continuous"" as: Nearby points map to nearby points. But we can easily construct sets for which *all their points are not “nearby” but they are still open . A simple example in metric spaces: the union of two open balls. The sets are still open but the points in one ball vs the other are not nearby. However the topological definition is in terms of open sets so it would consider maps balls like this from $X$ to $Y$ while that doesn't seem right to me. Is there something that I am missing? I guess I find it better to have a notion that captures the idea of “balls of radius epsilon in Y” to “balls of radius delta in X” a better notion of continuous. Another issue I find with this is that I find this in conflict even with the traditional epsilon-delta definition. The way I see it is that the topological definition should be more general (and abstract) and should encompass the metric space definition as a special case. Which to me it’s not clear it does because there is this union of disjoint open sets issue , that seem get included in the topological definition but for me they shouldn’t. This point seems important. Why were open sets chosen as the correct notion? A better definition for me would be (instead of open sets) to be in terms of “balls of radius epsilon in Y” to “balls of radius delta in X” in some topological way to define this. I have of course read the descriptions of open sets in wikiepdia but that doesn't seem to really clarify things. I know that open sets are the set of points under some topology that are ""close"". i.e. we only need sets to classify what points are considered ""close"". Which seems to me the main motivation why open sets were chosen, but the fact that disjoint open balls pass the test and are considered ""close by"" particularly disturbs me for some reason. Why is this specific complaint OK to ignore? What justifies not being worried about it? Another reason I find it weird to use open sets is because for me open sets (since I am most familiar with the definition of open sets in metric spaces), are a type of set where everything is an interior point . It's a type of set that: for all points we can always find a perturbation such that the point remains in the set (thus there is a neighbourhood that contains it in E). I find this problematic since it doesn't seem the right notion of ""nearby"" (at least to me); the reasons I prefer the definition to be restricted to only single open balls or sets that have no weird gaps (continuous sets? for some definition of that). This interior point issue doesn't seem to be what continuity (or limits actually) encompass conceptually. Continuity/limits seem to be a property about getting closer and closer (at least conceptually) or approaching. Therefore, for me it would be better to define it in terms of sets that reflected this idea of closeness. Something like neighbourhoods or (open) balls like in the traditional way of defining balls $B_{\delta}(p) = { x \in X | d(x,p) < \delta}$. Since this seems to be a clear notion of ""nearby"". Why are these ideas not preferred? What is wrong with it?","Why is the topological definition of continuous in terms of open sets? I think my main complaint might be that the notion of open set seems too flexible/general and considers too many things that don't seem the right notion of ""closeness"". Conceptually, people explain ""continuous"" as: Nearby points map to nearby points. But we can easily construct sets for which *all their points are not “nearby” but they are still open . A simple example in metric spaces: the union of two open balls. The sets are still open but the points in one ball vs the other are not nearby. However the topological definition is in terms of open sets so it would consider maps balls like this from $X$ to $Y$ while that doesn't seem right to me. Is there something that I am missing? I guess I find it better to have a notion that captures the idea of “balls of radius epsilon in Y” to “balls of radius delta in X” a better notion of continuous. Another issue I find with this is that I find this in conflict even with the traditional epsilon-delta definition. The way I see it is that the topological definition should be more general (and abstract) and should encompass the metric space definition as a special case. Which to me it’s not clear it does because there is this union of disjoint open sets issue , that seem get included in the topological definition but for me they shouldn’t. This point seems important. Why were open sets chosen as the correct notion? A better definition for me would be (instead of open sets) to be in terms of “balls of radius epsilon in Y” to “balls of radius delta in X” in some topological way to define this. I have of course read the descriptions of open sets in wikiepdia but that doesn't seem to really clarify things. I know that open sets are the set of points under some topology that are ""close"". i.e. we only need sets to classify what points are considered ""close"". Which seems to me the main motivation why open sets were chosen, but the fact that disjoint open balls pass the test and are considered ""close by"" particularly disturbs me for some reason. Why is this specific complaint OK to ignore? What justifies not being worried about it? Another reason I find it weird to use open sets is because for me open sets (since I am most familiar with the definition of open sets in metric spaces), are a type of set where everything is an interior point . It's a type of set that: for all points we can always find a perturbation such that the point remains in the set (thus there is a neighbourhood that contains it in E). I find this problematic since it doesn't seem the right notion of ""nearby"" (at least to me); the reasons I prefer the definition to be restricted to only single open balls or sets that have no weird gaps (continuous sets? for some definition of that). This interior point issue doesn't seem to be what continuity (or limits actually) encompass conceptually. Continuity/limits seem to be a property about getting closer and closer (at least conceptually) or approaching. Therefore, for me it would be better to define it in terms of sets that reflected this idea of closeness. Something like neighbourhoods or (open) balls like in the traditional way of defining balls $B_{\delta}(p) = { x \in X | d(x,p) < \delta}$. Since this seems to be a clear notion of ""nearby"". Why are these ideas not preferred? What is wrong with it?",,"['real-analysis', 'general-topology', 'metric-spaces', 'continuity', 'definition']"
42,Infinite Series $\sum_{n=1}^\infty\frac{H_n}{n^22^n}$,Infinite Series,\sum_{n=1}^\infty\frac{H_n}{n^22^n},How can I prove that  $$\sum_{n=1}^{\infty}\frac{H_n}{n^2 2^n}=\zeta(3)-\frac{1}{2}\log(2)\zeta(2).$$ Can anyone help me please?,How can I prove that  $$\sum_{n=1}^{\infty}\frac{H_n}{n^2 2^n}=\zeta(3)-\frac{1}{2}\log(2)\zeta(2).$$ Can anyone help me please?,,"['real-analysis', 'sequences-and-series']"
43,Showing the inequality $|\alpha + \beta|^p \leq 2^{p-1}(|\alpha|^p + |\beta|^p)$,Showing the inequality,|\alpha + \beta|^p \leq 2^{p-1}(|\alpha|^p + |\beta|^p),"I have a small question that I think is very basic but I am unsure how to tackle since my background in computing inequalities is embarrassingly weak - I would like to show that, for a real number $p \geq 1$ and complex numbers $\alpha, \beta$ , I have \begin{equation} |\alpha + \beta|^p \leq 2^{p-1}(|\alpha|^p + |\beta|^p) \end{equation} I thought it would be best to rewrite this as \begin{equation} \left|\frac{\alpha + \beta}{2}\right|^p \leq \frac{|\alpha|^p + |\beta|^p}{2} \end{equation} but then I am unsure what to do next - is this a sensible start anyways ? Any help would be great ! (P.S. this is not a homework question - I am currently trying to brush up my knowledge of $L^p$ spaces, and this inequality came up as a statement. I thought it might be worthwhile to  make sure I can fill in the gaps to improve my skills in computing inequalities.)","I have a small question that I think is very basic but I am unsure how to tackle since my background in computing inequalities is embarrassingly weak - I would like to show that, for a real number and complex numbers , I have I thought it would be best to rewrite this as but then I am unsure what to do next - is this a sensible start anyways ? Any help would be great ! (P.S. this is not a homework question - I am currently trying to brush up my knowledge of spaces, and this inequality came up as a statement. I thought it might be worthwhile to  make sure I can fill in the gaps to improve my skills in computing inequalities.)","p \geq 1 \alpha, \beta \begin{equation}
|\alpha + \beta|^p \leq 2^{p-1}(|\alpha|^p + |\beta|^p)
\end{equation} \begin{equation}
\left|\frac{\alpha + \beta}{2}\right|^p \leq \frac{|\alpha|^p + |\beta|^p}{2}
\end{equation} L^p","['calculus', 'real-analysis', 'functional-analysis', 'inequality']"
44,Can a continuous function from the reals to the reals assume each value an even number of times?,Can a continuous function from the reals to the reals assume each value an even number of times?,,"Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous. Is it possible for $f$ to assume each value in its range an even number of times? To clarify, some values might be taken 0 times, some 2, some 4, etc., but always an even (and therefore finite) number. I don't require that there be a value that is assumed any particular number of times.  For example, the function might be surjective, or never take on any value exactly twice. This question is possibly the same as A continuous function cannot take every value an exact even number of times? . That question may instead have meant, ""If $n$ is even, is it possible for $f$ to assume every value in its range exactly $n$ times?""  In any event, it has not been answered.","Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous. Is it possible for $f$ to assume each value in its range an even number of times? To clarify, some values might be taken 0 times, some 2, some 4, etc., but always an even (and therefore finite) number. I don't require that there be a value that is assumed any particular number of times.  For example, the function might be surjective, or never take on any value exactly twice. This question is possibly the same as A continuous function cannot take every value an exact even number of times? . That question may instead have meant, ""If $n$ is even, is it possible for $f$ to assume every value in its range exactly $n$ times?""  In any event, it has not been answered.",,"['calculus', 'real-analysis', 'continuity']"
45,A question on Terence Tao's representation of Peano Axioms,A question on Terence Tao's representation of Peano Axioms,,"While reading Terence Tao's book on Analysis I had some questions regarding the implication  of the Peano Axioms. After writing the following four axioms (which I will write without changing their original numbering), Axiom 2.1 $0$ is a natural number. Axiom 2.2 If $n$ is a natural number then $n{+\!+}$ is also a natural number. (Here $n{+\!+}$ denotes the successor of $n$ and previously in the book the notational implication has been bijected to the familiar $1, 2\ldots$ ). Axiom 2.3 $0$ is not the successor of natural number; i.e. we have $n{+\!+}\neq 0$ for every natural number $n$ . Axiom 2.4 Different natural numbers must have different successors; i.e., if $n, m$ are natural numbers and $n\neq m$ , then $n{+\!+}\neq m{+\!+}$ . Now let me quote the portion of the text from which my question arises, ""As one can see from this proposition, it now looks like we can keep all of the natural numbers distinct from each other. There is however still one more problem: while the axioms (particularly Axioms $2.1$ and $2.2$ ) allow us to confirm that $0,1,2,3,\ldots$ are distinct elements of $\mathbb{N}$ , there is the problem that there may be other   "" $\color{red}{\text{rogue}}$ "" elements in our number system which are not of this form:... ...What we want is some axiom which says that the only numbers in $\mathbb{N}$ are those which can be obtained from $0$ and the increment operation - in order to exclude elements such as $0.5$ . But it is difficult to quantify what we mean by ""can be obtained from""   without already using the natural numbers, which we are trying to define. Fortunately, there is an ingenious solution to try to capture this fact: Axiom 2.5 Let $P(n)$ be any property pertaining to a natural number $n$ . Suppose that $P(0)$ is true, and suppose that whenever $P(n)$ is true, $P(n{+\!+})$ is also     true. Then $P(n)$ is true for every natural number $n$ . The informal intuition behind this axiom is the following. Suppose $P(n)$ is such that $P(0)$ is true, and such that whenever $P(n)$ is true, then $P(n{+\!+})$ is true. Then since $P(0)$ is true, $P(0{+\!+}) = P(1)$ is true. Since $P(1)$ is true, $P(1{+\!+}) = P(2)$ is   true. Repeating this indefinitely, we see that $P(0), P(1), P(2), P(3),$ etc. are all true- however this line of reasoning will never let us conclude that $P(0.5)$ , for instance, is true. $\color{blue}{\text{Thus Axiom 2.5 should not hold for number systems which contain}}$ "" $\color{blue}{\text {unnecessary}}$ "" $\color{blue}{\text {elements such as $0.5$.}}$ My questions are precisely regarding the colored statements. How can we assume the existence of "" $\color{red}{\text{rogue}}$ "" elements in our number system? To be precise, how can one who doesn't know anything about the number system and the numbers try to convince himself of the existence of such elements? Which $P(n)$ only holds for the natural numbers? In other words, how the addition of the fifth axiom resolves the problem of the rogue elements ? Added:- After reading the various answers below, I think that I should elaborate why the answers below doesn't completely satisfy me. What KSmarts said in response to my first question seems circular to me. To quote from his answer, Suppose that in addition to the expected natural numbers $0,1,2,$ etc $\color{green}{\color{green}{\text{there are numbers}}\  a,b, \color{green}{\text{and}}\ c  \ \color{green}{\text{so that}}\ a{+\!+}=b, b{+\!+}=c,\  \color{green}{\text{and}}\ c{+\!+}=a}$ . This does not contradict any of the first four axioms, so it is a valid construction under those axioms. However, it does not correspond to what we expect of the natural numbers, so there is another axiom to restrict this construction. The colored portion is the reason for which I think that the answer is circular. Precisely my question is that how do we convince someone the existence of those $a,b,c$ 's who doesn't know anything about the numbers? Hurkyl raised a good point which tries to resolve the above question, For your first question, it's not that we assume that rogue elements exist, it's that we cannot assume they do not exist. If we wish that they do not exist, we must choose a definition that allows us to prove it. ""...it's that we cannot assume they do not exist."" Exactly! But in my view we can choose a definition to ""prove"" that such rogue elements don't $\color{darkviolet}{\text{only when}}$ we know the properties of those elements. For then we can ""construct"" the definition in a way which excludes the possibility of such rogue elements. So, How can we know the properties that only belongs to the rogue elements, unless we have a proper idea of the nature of those rogue elements? And it's precisely in the light of this question that my second question naturally arises. If we know the properties of the rogue elements, then we can define the property $P$ to be the ""complement"" of the properties which satisfy question my last question.","While reading Terence Tao's book on Analysis I had some questions regarding the implication  of the Peano Axioms. After writing the following four axioms (which I will write without changing their original numbering), Axiom 2.1 is a natural number. Axiom 2.2 If is a natural number then is also a natural number. (Here denotes the successor of and previously in the book the notational implication has been bijected to the familiar ). Axiom 2.3 is not the successor of natural number; i.e. we have for every natural number . Axiom 2.4 Different natural numbers must have different successors; i.e., if are natural numbers and , then . Now let me quote the portion of the text from which my question arises, ""As one can see from this proposition, it now looks like we can keep all of the natural numbers distinct from each other. There is however still one more problem: while the axioms (particularly Axioms and ) allow us to confirm that are distinct elements of , there is the problem that there may be other   "" "" elements in our number system which are not of this form:... ...What we want is some axiom which says that the only numbers in are those which can be obtained from and the increment operation - in order to exclude elements such as . But it is difficult to quantify what we mean by ""can be obtained from""   without already using the natural numbers, which we are trying to define. Fortunately, there is an ingenious solution to try to capture this fact: Axiom 2.5 Let be any property pertaining to a natural number . Suppose that is true, and suppose that whenever is true, is also     true. Then is true for every natural number . The informal intuition behind this axiom is the following. Suppose is such that is true, and such that whenever is true, then is true. Then since is true, is true. Since is true, is   true. Repeating this indefinitely, we see that etc. are all true- however this line of reasoning will never let us conclude that , for instance, is true. "" "" My questions are precisely regarding the colored statements. How can we assume the existence of "" "" elements in our number system? To be precise, how can one who doesn't know anything about the number system and the numbers try to convince himself of the existence of such elements? Which only holds for the natural numbers? In other words, how the addition of the fifth axiom resolves the problem of the rogue elements ? Added:- After reading the various answers below, I think that I should elaborate why the answers below doesn't completely satisfy me. What KSmarts said in response to my first question seems circular to me. To quote from his answer, Suppose that in addition to the expected natural numbers etc . This does not contradict any of the first four axioms, so it is a valid construction under those axioms. However, it does not correspond to what we expect of the natural numbers, so there is another axiom to restrict this construction. The colored portion is the reason for which I think that the answer is circular. Precisely my question is that how do we convince someone the existence of those 's who doesn't know anything about the numbers? Hurkyl raised a good point which tries to resolve the above question, For your first question, it's not that we assume that rogue elements exist, it's that we cannot assume they do not exist. If we wish that they do not exist, we must choose a definition that allows us to prove it. ""...it's that we cannot assume they do not exist."" Exactly! But in my view we can choose a definition to ""prove"" that such rogue elements don't we know the properties of those elements. For then we can ""construct"" the definition in a way which excludes the possibility of such rogue elements. So, How can we know the properties that only belongs to the rogue elements, unless we have a proper idea of the nature of those rogue elements? And it's precisely in the light of this question that my second question naturally arises. If we know the properties of the rogue elements, then we can define the property to be the ""complement"" of the properties which satisfy question my last question.","0 n n{+\!+} n{+\!+} n 1, 2\ldots 0 n{+\!+}\neq 0 n n, m n\neq m n{+\!+}\neq m{+\!+} 2.1 2.2 0,1,2,3,\ldots \mathbb{N} \color{red}{\text{rogue}} \mathbb{N} 0 0.5 P(n) n P(0) P(n) P(n{+\!+}) P(n) n P(n) P(0) P(n) P(n{+\!+}) P(0) P(0{+\!+}) = P(1) P(1) P(1{+\!+}) = P(2) P(0), P(1), P(2), P(3), P(0.5) \color{blue}{\text{Thus Axiom 2.5 should not hold for number systems which contain}} \color{blue}{\text {unnecessary}} \color{blue}{\text {elements such as 0.5.}} \color{red}{\text{rogue}} P(n) 0,1,2, \color{green}{\color{green}{\text{there are numbers}}\  a,b, \color{green}{\text{and}}\ c  \ \color{green}{\text{so that}}\ a{+\!+}=b, b{+\!+}=c,\  \color{green}{\text{and}}\ c{+\!+}=a} a,b,c \color{darkviolet}{\text{only when}} P",['real-analysis']
46,The set of functions which map convergent series to convergent series,The set of functions which map convergent series to convergent series,,"Suppose $f$ is some real function with the above property, i.e. if $\sum\limits_{n = 0}^\infty {x_n}$ converges, then $\sum\limits_{n = 0}^\infty {f(x_n)}$ also converges. My question is: can anything interesting be said regarding the behavior of such a function close to $0$, other than the fact that $f(0)=0$?","Suppose $f$ is some real function with the above property, i.e. if $\sum\limits_{n = 0}^\infty {x_n}$ converges, then $\sum\limits_{n = 0}^\infty {f(x_n)}$ also converges. My question is: can anything interesting be said regarding the behavior of such a function close to $0$, other than the fact that $f(0)=0$?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
47,"Compute $\int_0^\pi\frac{\cos nx}{a^2-2ab\cos x+b^2}\, dx$",Compute,"\int_0^\pi\frac{\cos nx}{a^2-2ab\cos x+b^2}\, dx","How to compute the following integral   \begin{equation} \int_0^\pi\frac{\cos nx}{a^2-2ab\cos x+b^2}\, dx \end{equation} I have been given two integral questions by my teacher. I cannot answer this one. I have also searched the similar question here but it looks like nothing is similar so I think this is not a duplicate. I could compute the integral if  \begin{equation} \int_0^\pi\frac{dx}{a^2-2ab\cos x+b^2} \end{equation} The $\cos nx$ part makes the integral is really difficult. I want to use the result to compute this integral (the real question given by my teacher) \begin{equation} \int_0^\pi\frac{x^2\cos nx}{a^2-2ab\cos x+b^2}\, dx \end{equation} My question is how to compute the first integral (in the grey-shaded part) preferably with elementary ways (high school methods) ?","How to compute the following integral   \begin{equation} \int_0^\pi\frac{\cos nx}{a^2-2ab\cos x+b^2}\, dx \end{equation} I have been given two integral questions by my teacher. I cannot answer this one. I have also searched the similar question here but it looks like nothing is similar so I think this is not a duplicate. I could compute the integral if  \begin{equation} \int_0^\pi\frac{dx}{a^2-2ab\cos x+b^2} \end{equation} The $\cos nx$ part makes the integral is really difficult. I want to use the result to compute this integral (the real question given by my teacher) \begin{equation} \int_0^\pi\frac{x^2\cos nx}{a^2-2ab\cos x+b^2}\, dx \end{equation} My question is how to compute the first integral (in the grey-shaded part) preferably with elementary ways (high school methods) ?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
48,Prove convergence of the sequence $(z_1+z_2+\cdots + z_n)/n$ of Cesaro means [duplicate],Prove convergence of the sequence  of Cesaro means [duplicate],(z_1+z_2+\cdots + z_n)/n,"This question already has answers here : On Cesàro convergence: If $x_n \to x$ then $z_n = \frac{x_1 + \dots +x_n}{n} \to x$ (3 answers) Closed 6 years ago . Prove that if $\lim_{n \to \infty}z_{n}=A$ then:  $$\lim_{n \to \infty}\frac{z_{1}+z_{2}+\cdots + z_{n}}{n}=A$$ I was thinking spliting it in: $$(z_{1}+z_{2}+\cdots+z_{N-1})+(z_{N}+z_{N+1}+\cdots+z_{n})$$ where $N$ is value of $n$ for which $|A-z_{n}|<\epsilon$ then taking the limit of this sum devided by $n$ , and noting that the second sum is as close as you wish to $nA$ while the first is as close as you wish to $0$. Not sure if this helps....","This question already has answers here : On Cesàro convergence: If $x_n \to x$ then $z_n = \frac{x_1 + \dots +x_n}{n} \to x$ (3 answers) Closed 6 years ago . Prove that if $\lim_{n \to \infty}z_{n}=A$ then:  $$\lim_{n \to \infty}\frac{z_{1}+z_{2}+\cdots + z_{n}}{n}=A$$ I was thinking spliting it in: $$(z_{1}+z_{2}+\cdots+z_{N-1})+(z_{N}+z_{N+1}+\cdots+z_{n})$$ where $N$ is value of $n$ for which $|A-z_{n}|<\epsilon$ then taking the limit of this sum devided by $n$ , and noting that the second sum is as close as you wish to $nA$ while the first is as close as you wish to $0$. Not sure if this helps....",,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'cesaro-summable']"
49,"$p_n(x)=p_{n-1}(x)+p_{n-1}^{\prime}(x)$, then all the roots of $p_k(x)$ are real",", then all the roots of  are real",p_n(x)=p_{n-1}(x)+p_{n-1}^{\prime}(x) p_k(x),"$p_0(x)=a_mx^m+a_{m-1}x^{m-1}+\dotsb+a_1x+a_0(a_m,\dotsc,a_1,a_0\in\Bbb R)$ is a polynomial,  and $$p_n(x)=p_{n-1}(x)+p_{n-1}^{\prime}(x),\qquad n=1,2,\dotsc$$ then, there exist $N\in\Bbb N$, such that $\forall n \geq N$, all the roots of $p_n(x)$ are real I think, let $f_n(x)=e^xp_n(x)$, then $$f_n(x)=\frac {d(f_{n-1}(x))}{dx} $$ we get that $$f_n(x)=\frac{d^n\left( e^xp_0(x)\right)}{dx^n}$$ so, If there exists a $p_N(x)$ such that $p_N(x)$ has  $m$ real roots, then $\forall n\geq N$,  $p_n(x)$ has  $m$ real roots. I'd be grateful for any help","$p_0(x)=a_mx^m+a_{m-1}x^{m-1}+\dotsb+a_1x+a_0(a_m,\dotsc,a_1,a_0\in\Bbb R)$ is a polynomial,  and $$p_n(x)=p_{n-1}(x)+p_{n-1}^{\prime}(x),\qquad n=1,2,\dotsc$$ then, there exist $N\in\Bbb N$, such that $\forall n \geq N$, all the roots of $p_n(x)$ are real I think, let $f_n(x)=e^xp_n(x)$, then $$f_n(x)=\frac {d(f_{n-1}(x))}{dx} $$ we get that $$f_n(x)=\frac{d^n\left( e^xp_0(x)\right)}{dx^n}$$ so, If there exists a $p_N(x)$ such that $p_N(x)$ has  $m$ real roots, then $\forall n\geq N$,  $p_n(x)$ has  $m$ real roots. I'd be grateful for any help",,"['real-analysis', 'polynomials', 'roots', 'real-numbers']"
50,"Justifying the ""Physicist's method"" for ODEs using differential forms","Justifying the ""Physicist's method"" for ODEs using differential forms",,"I need some help in untangling and solving the following exercise: Let the curve $c:[a,b] \to \mathbb{R}^2, t \mapsto (t, y(t))$ be a   solution for the ODE $$ y'(x) = f(x, y(x)). $$ Justify the   ""Physicist's method"" (no offense intended) of rearranging the equation $\frac{dy}{dx} = f(x,y)$ through formal multiplication of $dx$ to $$ dy = f(x,y)dx, $$ by showing that both differential forms agree in   every point $c(t)$ on the tangent space. As far as I understand the situation, we are considering the two differential forms $$\begin{equation} dy: \mathbb{R}^2 \to {\bigwedge}^1(T_p\mathbb{R}^2)\\ f(x,y)dx: \mathbb{R}^2 \to {\bigwedge}^1(T_p\mathbb{R}^2)\\ \end{equation} $$ Here $dy$ is a constant differential form, in the sense that $dy(p)(x,y) = y$ independent of $p=(v,w) \in \mathbb{R}^2$ . However, in general $f(x,y)dx$ is not a constant differential form. Now, if we choose a point on the curve $c$ , say $p = c(t_0) \in c([a,b])$ then we have $$ f(p)dx = f(c(t_0))dx = y'(t_0)dx, $$ since $c$ is a solution to the ODE given above. I don't know how to proceed from this point. How can we argue that this equals $dy$ ?","I need some help in untangling and solving the following exercise: Let the curve be a   solution for the ODE Justify the   ""Physicist's method"" (no offense intended) of rearranging the equation through formal multiplication of to by showing that both differential forms agree in   every point on the tangent space. As far as I understand the situation, we are considering the two differential forms Here is a constant differential form, in the sense that independent of . However, in general is not a constant differential form. Now, if we choose a point on the curve , say then we have since is a solution to the ODE given above. I don't know how to proceed from this point. How can we argue that this equals ?","c:[a,b] \to \mathbb{R}^2, t \mapsto (t, y(t))  y'(x) = f(x, y(x)).  \frac{dy}{dx} = f(x,y) dx 
dy = f(x,y)dx,  c(t) \begin{equation}
dy: \mathbb{R}^2 \to {\bigwedge}^1(T_p\mathbb{R}^2)\\
f(x,y)dx: \mathbb{R}^2 \to {\bigwedge}^1(T_p\mathbb{R}^2)\\
\end{equation}
 dy dy(p)(x,y) = y p=(v,w) \in \mathbb{R}^2 f(x,y)dx c p = c(t_0) \in c([a,b]) 
f(p)dx = f(c(t_0))dx = y'(t_0)dx,
 c dy","['real-analysis', 'ordinary-differential-equations', 'differential-geometry', 'differential-forms']"
51,"If $f$ is a smooth real valued function on real line such that $f'(0)=1$ and $|f^{(n)} (x)|$ is uniformly bounded by $1$ , then $f(x)=\sin x$?","If  is a smooth real valued function on real line such that  and  is uniformly bounded by  , then ?",f f'(0)=1 |f^{(n)} (x)| 1 f(x)=\sin x,"Let $f : \mathbb R \to \mathbb R$ be a smooth ( infinitely differentiable everywhere ) function such that $f '(0)=1$ and $|f^{(n)} (x)| \le 1 , \forall x \in \mathbb R , \forall n \ge 0$ ( as usual denoting $f^{(0)}(x):=f(x)$) ; then is it true that $f(x)=\sin x , \forall x \in \mathbb R$ ?","Let $f : \mathbb R \to \mathbb R$ be a smooth ( infinitely differentiable everywhere ) function such that $f '(0)=1$ and $|f^{(n)} (x)| \le 1 , \forall x \in \mathbb R , \forall n \ge 0$ ( as usual denoting $f^{(0)}(x):=f(x)$) ; then is it true that $f(x)=\sin x , \forall x \in \mathbb R$ ?",,"['calculus', 'real-analysis']"
52,Do Integrals over Fractals Exist?,Do Integrals over Fractals Exist?,,"Given, for example, a line integral like  $$ \int_\gamma f \; ds $$ with $f$ not further defined, yet. What happens, if the contour $\gamma$ happens to be a fractal curve? Since all fractal curves (to my knowledge) are not differentiable and have an infinite length, we should run into some trouble. Further questions of interest are: What happens if the integral is taken over a Julia Set? What happens in the case of a space-filling curve? Do we get a surface integral then? I'd be glad, if anybody could wrap her/his 2.79-Hausdorff-dimensional brain surface around it!","Given, for example, a line integral like  $$ \int_\gamma f \; ds $$ with $f$ not further defined, yet. What happens, if the contour $\gamma$ happens to be a fractal curve? Since all fractal curves (to my knowledge) are not differentiable and have an infinite length, we should run into some trouble. Further questions of interest are: What happens if the integral is taken over a Julia Set? What happens in the case of a space-filling curve? Do we get a surface integral then? I'd be glad, if anybody could wrap her/his 2.79-Hausdorff-dimensional brain surface around it!",,"['real-analysis', 'integration', 'fractals']"
53,The gradient as a row versus column vector,The gradient as a row versus column vector,,"Kaplan's Advanced Calculus defines the gradient of a function $f : \mathbb{R^n} \to \mathbb{R}$ as the $1 \times n$ row vector whose entries respectively contain the $n$ partial derivatives of $f$ . By this definition then, the gradient is just the Jacobian matrix of the transformation. We also know that using the Riesz representation theorem, assuming $f$ is differentiable at the point $x$ , we can define the gradient as the unique vector $\nabla f$ such that $$ df(x)(h) = \langle h, \nabla f(x) \rangle, \quad h \in \mathbb{R}^n $$ Assuming we ignore the distinction between row vectors and column vectors, the former definition follows easily from the latter. But, row vectors and column vectors are not the same things. So, I have the following questions: Is the distinction here between row/column vectors important? If (1) is true, then how can we know from the second defintion that the vector in question is a row vector and not a column vector?","Kaplan's Advanced Calculus defines the gradient of a function as the row vector whose entries respectively contain the partial derivatives of . By this definition then, the gradient is just the Jacobian matrix of the transformation. We also know that using the Riesz representation theorem, assuming is differentiable at the point , we can define the gradient as the unique vector such that Assuming we ignore the distinction between row vectors and column vectors, the former definition follows easily from the latter. But, row vectors and column vectors are not the same things. So, I have the following questions: Is the distinction here between row/column vectors important? If (1) is true, then how can we know from the second defintion that the vector in question is a row vector and not a column vector?","f : \mathbb{R^n} \to \mathbb{R} 1 \times n n f f x \nabla f  df(x)(h) = \langle h, \nabla f(x) \rangle, \quad h \in \mathbb{R}^n ","['real-analysis', 'multivariable-calculus', 'scalar-fields']"
54,On Lipschitz condition and absolute continuity,On Lipschitz condition and absolute continuity,,"A function $f(x)$ on $[0,1]$ is said to satisfy a Lipschitz condition if there exists a constant $M$, such that $$|f(x)-f(y)|\leqslant M|x-y| ~\forall~x,y\in[0,1]. $$ I want to show the following: If $f$ is Lipschitz, then (i) $f$ is absolutely continuous (ii) $f$ is of bounded variation. (iii) $f'(x)$ exists almost everywhere on $[0,1]$ and $f(x)=\int_0^x f'(t)\,\mathrm{d}t$, provided that $f(0)=0$. Attempt: (i) Let $\varepsilon \gt 0$. Choose $\delta=\varepsilon /M$. Then for any collection $\{[x_i,y_i]\}$ of non-overlapping intervals with total length $\sum |x_i-y_i|\lt \delta$, we have $$\sum|f(x_i)-f(y_i)|\lt M\sum|x_i-y_i|\lt \varepsilon.$$ Hence $f$ is absolutely continuous. (ii) For this, can I simply use the fact that an absolutely continuous functions is of bounded variation or I would have to prove it formally? (iii) I also know that if $f$ absolutely continuous $\implies$ $f$ is of bounded variation $\implies f'(x)$ exists a.e. and $$ f(x)=f(0)+\int_0^x f'(t)\,\mathrm{d}t. $$ Thus, $f(x)=\int_0^x f'(t)\,\mathrm{d}t \text{ if } f(0)=0.$ Can I use these known facts to prove (ii) and (iii)? I am also asked to find a function $h(x)$ on $[0,1]$ such that $$\tag{​*​}\int_0^1 h'(x)\text{d}x\lt h(1)-h(0).$$ This is  what I did:  Let $h$ be the Cantor function is monotone increasing of $[0,1]$. $h(0)=1,~h(1)=1$. $h'=0,~\forall~x$ not in the Cantor set. Then the inequality holds. Are there other examples where (*) holds?","A function $f(x)$ on $[0,1]$ is said to satisfy a Lipschitz condition if there exists a constant $M$, such that $$|f(x)-f(y)|\leqslant M|x-y| ~\forall~x,y\in[0,1]. $$ I want to show the following: If $f$ is Lipschitz, then (i) $f$ is absolutely continuous (ii) $f$ is of bounded variation. (iii) $f'(x)$ exists almost everywhere on $[0,1]$ and $f(x)=\int_0^x f'(t)\,\mathrm{d}t$, provided that $f(0)=0$. Attempt: (i) Let $\varepsilon \gt 0$. Choose $\delta=\varepsilon /M$. Then for any collection $\{[x_i,y_i]\}$ of non-overlapping intervals with total length $\sum |x_i-y_i|\lt \delta$, we have $$\sum|f(x_i)-f(y_i)|\lt M\sum|x_i-y_i|\lt \varepsilon.$$ Hence $f$ is absolutely continuous. (ii) For this, can I simply use the fact that an absolutely continuous functions is of bounded variation or I would have to prove it formally? (iii) I also know that if $f$ absolutely continuous $\implies$ $f$ is of bounded variation $\implies f'(x)$ exists a.e. and $$ f(x)=f(0)+\int_0^x f'(t)\,\mathrm{d}t. $$ Thus, $f(x)=\int_0^x f'(t)\,\mathrm{d}t \text{ if } f(0)=0.$ Can I use these known facts to prove (ii) and (iii)? I am also asked to find a function $h(x)$ on $[0,1]$ such that $$\tag{​*​}\int_0^1 h'(x)\text{d}x\lt h(1)-h(0).$$ This is  what I did:  Let $h$ be the Cantor function is monotone increasing of $[0,1]$. $h(0)=1,~h(1)=1$. $h'=0,~\forall~x$ not in the Cantor set. Then the inequality holds. Are there other examples where (*) holds?",,"['real-analysis', 'measure-theory']"
55,Proof of separability of $L^p$ spaces,Proof of separability of  spaces,L^p,"The following is a proof in Brezis book. It shows the separability of $L^{p}$ spaces: I have a  few questions regarding the proof: It says 'it is easy to construct a function $f_{2} \in \varepsilon...$ "" and it also says  "" it suffices to split $R$ into small cubes...'. Would it work to choose $f_{2}$ in the following way: Assume we split $R$ as suggested. Let $R_{i}$ denote each small cube of $R$ , consider $f_{2_{i}} := C_{i}\chi_{R_{i}}$ where $C_{i}$ is a constant chosen from $[0, \delta - (\text{sup} f|_{R_{i}} - \text{inf} f|_{R_{i}})$ , then let $f_{2}(x) := \sum_{i}f_{2_{i}}(x)$ . It would then follow that $\Vert f_{1} - f_{2} \Vert_{\infty} < \epsilon$ . Is this fine? Can anyone see how the inequality $\Vert f_{1} -f_{2} \Vert_{p} \leq \Vert f_{1}-f_{2} \Vert_{\infty}|R|^{\frac{1}{p}}$ is obtained? Where exactly is the separability of $\Omega = \mathbb{R}^{N}$ used? Note that $\chi$ denotes the characteristic function. Thanks a lot for any assistance. Let me know if something is unclear.","The following is a proof in Brezis book. It shows the separability of spaces: I have a  few questions regarding the proof: It says 'it is easy to construct a function "" and it also says  "" it suffices to split into small cubes...'. Would it work to choose in the following way: Assume we split as suggested. Let denote each small cube of , consider where is a constant chosen from , then let . It would then follow that . Is this fine? Can anyone see how the inequality is obtained? Where exactly is the separability of used? Note that denotes the characteristic function. Thanks a lot for any assistance. Let me know if something is unclear.","L^{p} f_{2} \in \varepsilon... R f_{2} R R_{i} R f_{2_{i}} := C_{i}\chi_{R_{i}} C_{i} [0, \delta - (\text{sup} f|_{R_{i}} - \text{inf} f|_{R_{i}}) f_{2}(x) := \sum_{i}f_{2_{i}}(x) \Vert f_{1} - f_{2} \Vert_{\infty} < \epsilon \Vert f_{1} -f_{2} \Vert_{p} \leq \Vert f_{1}-f_{2} \Vert_{\infty}|R|^{\frac{1}{p}} \Omega = \mathbb{R}^{N} \chi","['real-analysis', 'functional-analysis', 'proof-verification', 'normed-spaces', 'lp-spaces']"
56,"Are sines of primes dense in $[-1,1]?$",Are sines of primes dense in,"[-1,1]?","Let $P$ be the set of all prime numbers. Is $\sin(P)$ dense is $[-1,1]?$ How could we approach such a problem?","Let $P$ be the set of all prime numbers. Is $\sin(P)$ dense is $[-1,1]?$ How could we approach such a problem?",,['real-analysis']
57,Assume $f\left({\frac{f(x)}{x}}\right)= f(x)$. Show that $ f$ continuous,Assume . Show that  continuous,f\left({\frac{f(x)}{x}}\right)= f(x)  f,"Let $f : (0, \infty) \to (0, \infty)$ be a function that has primitives (that is, there is $F$ so that $F' = f$) and satisfies the relation $$f\left(\displaystyle{\frac{f(x)}{x}}\right)= f(x) , \forall x \in (0, \infty)$$ Prove that $f$ is continuous. I tried to obtain something with repeated substitutions and use the fact that if $f$ has primitives then I can use the intermediate value theorem, but I didn't manage to solve it. Remark this question is found in a Romanian mathematics magazine called ""Gazeta matematica"".","Let $f : (0, \infty) \to (0, \infty)$ be a function that has primitives (that is, there is $F$ so that $F' = f$) and satisfies the relation $$f\left(\displaystyle{\frac{f(x)}{x}}\right)= f(x) , \forall x \in (0, \infty)$$ Prove that $f$ is continuous. I tried to obtain something with repeated substitutions and use the fact that if $f$ has primitives then I can use the intermediate value theorem, but I didn't manage to solve it. Remark this question is found in a Romanian mathematics magazine called ""Gazeta matematica"".",,"['real-analysis', 'integration', 'continuity']"
58,How to show that the set of points of continuity is a $G_{\delta}$,How to show that the set of points of continuity is a,G_{\delta},"I am trying to solve this exercise from Royden's 3rd edition. The question is as follows: Let $f$ be a real-valued function defined for all real numbers. Show that the set of points at which $f$ is continuous is a $G_{\delta}$ . Let $$A_n = \{y : \text{there is a }~\delta_y \gt 0 : |f(s)-f(t)|\lt 1/n ~ \text{whenever}~ s,t \in (y-\delta, y+\delta)\}\;.$$ Then by the definition of open sets, $A_n$ is open. To complete the proof, I need help in showing that $f$ is continuous at say $x$ if and only if $x\in \cap A_n$ . If $f$ is continuous at $x$ , there is a $\delta \gt 0$ such that $|f(x) - f(a)| \lt 1/n$ whenever, $x\in (a-\delta, a+\delta)$ , so $x \in A_n$ , so it must be in $\cap A_n$ . Thanks.","I am trying to solve this exercise from Royden's 3rd edition. The question is as follows: Let be a real-valued function defined for all real numbers. Show that the set of points at which is continuous is a . Let Then by the definition of open sets, is open. To complete the proof, I need help in showing that is continuous at say if and only if . If is continuous at , there is a such that whenever, , so , so it must be in . Thanks.","f f G_{\delta} A_n = \{y : \text{there is a }~\delta_y \gt 0 : |f(s)-f(t)|\lt 1/n ~ \text{whenever}~ s,t \in (y-\delta, y+\delta)\}\;. A_n f x x\in \cap A_n f x \delta \gt 0 |f(x) - f(a)| \lt 1/n x\in (a-\delta, a+\delta) x \in A_n \cap A_n",['real-analysis']
59,A question about Existence of a Continuous function.,A question about Existence of a Continuous function.,,"Let $f$ be a continuous function on the interval $[1,2]$ . It follows from Stone-Weierstrass theorem that if $\displaystyle \int_1^2x^nf(x) \, dx=0$ for integers $n=0,1,2,\ldots$ , then $f$ must be identically zero. My question is, Does there exist a non-zero  continuous function $f$ on the interval $[1,2]$ , and a positive constant $M$ such that $\displaystyle \left|\int_1^2x^nf(x)\,dx\right|\leq M$ for all integers $n=0,1,2,\ldots$ ? If such a function exists, it must be an oscillating function which attains both positive and negative signs. I really appreciate for any answers, comments or suggestions.","Let be a continuous function on the interval . It follows from Stone-Weierstrass theorem that if for integers , then must be identically zero. My question is, Does there exist a non-zero  continuous function on the interval , and a positive constant such that for all integers ? If such a function exists, it must be an oscillating function which attains both positive and negative signs. I really appreciate for any answers, comments or suggestions.","f [1,2] \displaystyle \int_1^2x^nf(x) \, dx=0 n=0,1,2,\ldots f f [1,2] M \displaystyle \left|\int_1^2x^nf(x)\,dx\right|\leq M n=0,1,2,\ldots",['real-analysis']
60,Is there a bijective seacucumber?,Is there a bijective seacucumber?,,"A friend defined a seacucumber as a continuous function $f:\mathbb{C}\to\mathbb{C}$ such that $f(z+1)+f(z+i)+f(z-1)+f(z-i)=0$ for all $z\in\mathbb{C}$ . He wanted to know if there exists a bijective seacucumber. An example of a non-trivial seacucumber is $f(x+yi):=e^{i\pi(x+y)/2}$ . Assume there exists a bijective seacucumber $f$ . If we observe $f^{-1}(D)$ for some large enough disc $D$ , then this is bounded, so it is contained in a large enough disk $E$ . Then in any direction we can get outside $E$ and find $4$ points very close together where the function values add up to $0$ . As $f$ is continuous and bijective, any path through these $4$ points must go all the way around $D$ in order for this sum to be $0$ . If you draw some of these paths, you quickly notice that $f$ must have some very spirally properties. You see what we described as tentacles if you let a path pass through multiple such sets of $4$ points all the way around $E$ . Also, if you go further from the origin, you can find more of these sets of $4$ points, which means the amount of tentacles should keep increasing. This is all very heuristic unfortunately, and we could not come to a contradiction or come up with an example. We also tried to look whether a function from $\mathbb{Z}+i\mathbb{Z}$ to $\mathbb{C}$ satisfying the seacucumber equation can even be injective, but we expect this is possible. I would love to see how far we can come with this problem. Any progress would be greatly appreciated.","A friend defined a seacucumber as a continuous function such that for all . He wanted to know if there exists a bijective seacucumber. An example of a non-trivial seacucumber is . Assume there exists a bijective seacucumber . If we observe for some large enough disc , then this is bounded, so it is contained in a large enough disk . Then in any direction we can get outside and find points very close together where the function values add up to . As is continuous and bijective, any path through these points must go all the way around in order for this sum to be . If you draw some of these paths, you quickly notice that must have some very spirally properties. You see what we described as tentacles if you let a path pass through multiple such sets of points all the way around . Also, if you go further from the origin, you can find more of these sets of points, which means the amount of tentacles should keep increasing. This is all very heuristic unfortunately, and we could not come to a contradiction or come up with an example. We also tried to look whether a function from to satisfying the seacucumber equation can even be injective, but we expect this is possible. I would love to see how far we can come with this problem. Any progress would be greatly appreciated.",f:\mathbb{C}\to\mathbb{C} f(z+1)+f(z+i)+f(z-1)+f(z-i)=0 z\in\mathbb{C} f(x+yi):=e^{i\pi(x+y)/2} f f^{-1}(D) D E E 4 0 f 4 D 0 f 4 E 4 \mathbb{Z}+i\mathbb{Z} \mathbb{C},"['real-analysis', 'continuity', 'examples-counterexamples', 'functional-equations']"
61,What's the most efficient way to mow a lawn?,What's the most efficient way to mow a lawn?,,"For $S\subseteq\Bbb R^2$ and $x\in\Bbb R$, define $E_x(S)=\{y\in\Bbb R^2:d(y,S)<x\}$. ($E_x(S)$ represents the expansion of $S$ by $x$.) Given a path $\gamma:[0,1]\to\Bbb R^2$, denote its length as $L(\gamma)=\int_\gamma|dx|\in[0,\infty]$ (for non-rectifiable paths, $L(\gamma)=\infty$). Assume now that $S$ is connected and bounded, and let $$\lambda_\epsilon=\inf\Big\{L(\gamma):E_1(S)\subseteq \overline{E_1(\gamma)}\subseteq \overline{E_{1+\epsilon}(S)}\Big\}.$$ This is our best path for a lawnmower with cutting radius $1$ to mow the lawn $E_1(S)$ with an overspill of at most $\epsilon$. The questions are: Does $\lambda_\epsilon$ exist for every $\epsilon>0$? (I think I can prove this.) Does $\lambda_0$ exist? (Not sure about this.) Is $\lambda_\epsilon$ a minimum in either case, i.e. is there an acceptable path whose length is $\lambda_\epsilon$? Are there any algorithms for finding paths whose lengths are within $\delta$ of $\lambda_\epsilon$? (I'm not sure how to make this precise.) This is what I'm thinking about every time I mow my lawn, and I bet I'm not alone. Edit: A proof for the first question: Let $\epsilon>0$. Since $S$ is bounded, so is $E_{\epsilon/2}(S)$, and it is also totally bounded. Therefore, there is a finite cover $(B(\epsilon/2,x_k))_{k=1}^n$ of $E_{\epsilon/2}(S)$ by balls of radius $\epsilon/2$. Since $E_{\epsilon/2}(S)$ is a union of open connected sets, which are connected together by $S$, it is also connected. Now any open connected set is path-connected by polygonal paths, so there is a polygonal path from $x_i$ to $x_{i+1}$ for every $1\le i<n$. The concatenation of these is a polygonal path $\gamma$ which passes through each $x_i$ and remains in $\bigcup_{k=1}^nB(\epsilon/2,x_k)\subseteq E_{\epsilon}(S)$. Then $E_1(\gamma)\subseteq E_{1+\epsilon}(S)$, and any point in $E_1(S)$ is within $1-\epsilon/2$ of a point in $E_{\epsilon/2}(S)$, which is within $\epsilon/2$ of some $x_k\in\gamma$, so $E_1(S)\subseteq E_1(\gamma)$. Finally, since $\gamma$ is polygonal, it is rectifiable, so $\lambda_\epsilon\le L(\gamma)$ and $\lambda_\epsilon$ exists. If you consider the linear paths from $x_i$ to $x_j$ when the balls $B(\epsilon/2,x_i)$ and $B(\epsilon/2,x_j)$ are not disjoint, you get a graph on $n$ vertices, and the problem of finding the shortest path on this graph that visits all the vertices is exactly the Traveling Salesman Problem. Thus this does not bode well for efficient algorithms to solve this problem.","For $S\subseteq\Bbb R^2$ and $x\in\Bbb R$, define $E_x(S)=\{y\in\Bbb R^2:d(y,S)<x\}$. ($E_x(S)$ represents the expansion of $S$ by $x$.) Given a path $\gamma:[0,1]\to\Bbb R^2$, denote its length as $L(\gamma)=\int_\gamma|dx|\in[0,\infty]$ (for non-rectifiable paths, $L(\gamma)=\infty$). Assume now that $S$ is connected and bounded, and let $$\lambda_\epsilon=\inf\Big\{L(\gamma):E_1(S)\subseteq \overline{E_1(\gamma)}\subseteq \overline{E_{1+\epsilon}(S)}\Big\}.$$ This is our best path for a lawnmower with cutting radius $1$ to mow the lawn $E_1(S)$ with an overspill of at most $\epsilon$. The questions are: Does $\lambda_\epsilon$ exist for every $\epsilon>0$? (I think I can prove this.) Does $\lambda_0$ exist? (Not sure about this.) Is $\lambda_\epsilon$ a minimum in either case, i.e. is there an acceptable path whose length is $\lambda_\epsilon$? Are there any algorithms for finding paths whose lengths are within $\delta$ of $\lambda_\epsilon$? (I'm not sure how to make this precise.) This is what I'm thinking about every time I mow my lawn, and I bet I'm not alone. Edit: A proof for the first question: Let $\epsilon>0$. Since $S$ is bounded, so is $E_{\epsilon/2}(S)$, and it is also totally bounded. Therefore, there is a finite cover $(B(\epsilon/2,x_k))_{k=1}^n$ of $E_{\epsilon/2}(S)$ by balls of radius $\epsilon/2$. Since $E_{\epsilon/2}(S)$ is a union of open connected sets, which are connected together by $S$, it is also connected. Now any open connected set is path-connected by polygonal paths, so there is a polygonal path from $x_i$ to $x_{i+1}$ for every $1\le i<n$. The concatenation of these is a polygonal path $\gamma$ which passes through each $x_i$ and remains in $\bigcup_{k=1}^nB(\epsilon/2,x_k)\subseteq E_{\epsilon}(S)$. Then $E_1(\gamma)\subseteq E_{1+\epsilon}(S)$, and any point in $E_1(S)$ is within $1-\epsilon/2$ of a point in $E_{\epsilon/2}(S)$, which is within $\epsilon/2$ of some $x_k\in\gamma$, so $E_1(S)\subseteq E_1(\gamma)$. Finally, since $\gamma$ is polygonal, it is rectifiable, so $\lambda_\epsilon\le L(\gamma)$ and $\lambda_\epsilon$ exists. If you consider the linear paths from $x_i$ to $x_j$ when the balls $B(\epsilon/2,x_i)$ and $B(\epsilon/2,x_j)$ are not disjoint, you get a graph on $n$ vertices, and the problem of finding the shortest path on this graph that visits all the vertices is exactly the Traveling Salesman Problem. Thus this does not bode well for efficient algorithms to solve this problem.",,"['real-analysis', 'general-topology', 'numerical-methods', 'asymptotics']"
62,Examples of transcendental functions giving almost integers,Examples of transcendental functions giving almost integers,,"Informally speaking, an ""almost integer"" is a real number very close to an integer. There are some known ways to construct such examples in a systematic way. One is through the use of certain algebraic numbers called Pisot numbers. These numbers $\alpha$ have the property that their powers can get arbitrarly close to integers, that is: $\lim_{n \to \infty} \alpha - [\alpha^n] = 0$ where $[ .]$ is the nearest integer function. A well-known example is given by the golden ratio $\varphi = \frac{1 + \sqrt{5}}{2}$ , whose powers are increasingly close to integers: $\varphi^{19} = 9349.000107...$ $\varphi^{25} = 167761.00000596...$ Another example comes from numbers of the form $e^{\pi\sqrt{n}}$ . A well-known example is Ramanujan's constant: $e^{\pi\sqrt{163}} = 262537412640768743.99999999999925007...$ There's another interesting way to generate almost integers by using the numbers $e$ and $\pi$ . By using the identity $$\sum_{n=-\infty}^\infty e^{-\pi n^2x}=x^{-1/2}\sum_{n=-\infty}^\infty e^{-\pi n^2/x}.$$ we can derive the approximate identity $$ (*)  \sum_{k=0}^{n-1}{e^{-\frac{k^2\pi}{n}}}\approx\frac{1+\sqrt{n}}{2}$$ which provides a way to construct almost integers with increasing precision: $ e^{-\frac{\pi}{9}} + e^{-4\frac{\pi}{9}} + e^{-9\frac{\pi}{9}} + e^{-16\frac{\pi}{9}} + e^{-25\frac{\pi}{9}} + e^{-36\frac{\pi}{9}} + e^{-49\frac{\pi}{9}} + e^{-64\frac{\pi}{9}} = 1.0000000000010504... $ $\sum_{k=1}^{24} e^{-k^2\frac{\pi}{25}} = 2.000000000000000000000000000000000310793...$ $\sum_{k=1}^{48} e^{-k^2\frac{\pi}{49}} = 3.000000000000000000000000000000000000000000000000000000000000000000838654...$ So, the question is: is there another way to generate almost integers -with or without increasing precision- by using transcendental functions, as in the previous example? (Note that there's a trivial way to do this: By taking a convergent series $\sum_{k = 1 }^\infty x_k$ and its limit $L$ , the number $1/L\sum_{k = 1 }^n x_k$ will be an almost integer, namely close to $1$ , but I'm looking for  a an example like identity (*), or for a different, non trivial one). So, I am looking for an example that may be of the form $\sum_{k = 1 }^n f(x_k)$ , where $f(x)$ is a transcendental function of $x$ , that is able to generate a set of different almost integers (zero excluded).","Informally speaking, an ""almost integer"" is a real number very close to an integer. There are some known ways to construct such examples in a systematic way. One is through the use of certain algebraic numbers called Pisot numbers. These numbers have the property that their powers can get arbitrarly close to integers, that is: where is the nearest integer function. A well-known example is given by the golden ratio , whose powers are increasingly close to integers: Another example comes from numbers of the form . A well-known example is Ramanujan's constant: There's another interesting way to generate almost integers by using the numbers and . By using the identity we can derive the approximate identity which provides a way to construct almost integers with increasing precision: So, the question is: is there another way to generate almost integers -with or without increasing precision- by using transcendental functions, as in the previous example? (Note that there's a trivial way to do this: By taking a convergent series and its limit , the number will be an almost integer, namely close to , but I'm looking for  a an example like identity (*), or for a different, non trivial one). So, I am looking for an example that may be of the form , where is a transcendental function of , that is able to generate a set of different almost integers (zero excluded).",\alpha \lim_{n \to \infty} \alpha - [\alpha^n] = 0 [ .] \varphi = \frac{1 + \sqrt{5}}{2} \varphi^{19} = 9349.000107... \varphi^{25} = 167761.00000596... e^{\pi\sqrt{n}} e^{\pi\sqrt{163}} = 262537412640768743.99999999999925007... e \pi \sum_{n=-\infty}^\infty e^{-\pi n^2x}=x^{-1/2}\sum_{n=-\infty}^\infty e^{-\pi n^2/x}.  (*)  \sum_{k=0}^{n-1}{e^{-\frac{k^2\pi}{n}}}\approx\frac{1+\sqrt{n}}{2}  e^{-\frac{\pi}{9}} + e^{-4\frac{\pi}{9}} + e^{-9\frac{\pi}{9}} + e^{-16\frac{\pi}{9}} + e^{-25\frac{\pi}{9}} + e^{-36\frac{\pi}{9}} + e^{-49\frac{\pi}{9}} + e^{-64\frac{\pi}{9}} = 1.0000000000010504...  \sum_{k=1}^{24} e^{-k^2\frac{\pi}{25}} = 2.000000000000000000000000000000000310793... \sum_{k=1}^{48} e^{-k^2\frac{\pi}{49}} = 3.000000000000000000000000000000000000000000000000000000000000000000838654... \sum_{k = 1 }^\infty x_k L 1/L\sum_{k = 1 }^n x_k 1 \sum_{k = 1 }^n f(x_k) f(x) x,"['real-analysis', 'numerical-methods']"
63,Geometric representation of Euler-Maclaurin Summation Formula,Geometric representation of Euler-Maclaurin Summation Formula,,"In Tom Apostol's expository article (here's a free link) , upon seeing the figure below ( or this from the Wolfram project ) I was expecting more diagrams to come to continue the error decomposition of the shaded regions in the shape of ""curved triangular pieces"". For each horizontal unit interval, it seems that the hypotenuse of an implicit triangle (within shaded region) is the linear correction. Then, there's a ""cap"" that fills in the gap between the hypotenuse and the actual curve. In this figure, each cap is attached to the implicit hypotenuse from below due to the curve being convex. It was a disappointment not seeing further decomposition of the ""cap"" into regions described by parabolic, cubic, then quartic curves etc. Somehow none of the textbooks and other materials I've read have such diagrams for higher orders. Question Statement What is the correct analysis to demonstrate the 'geometry' of the successive orders of the Euler-Maclaurin formula? Is there any existing source with such diagrams (visualization) similar to the above regarding higher order? General Remarks I'm actually not sure whether my proposal is a valid idea (that such a demonstration is possible). Below is a more detailed description (repeating things said above) of what I mean by the tentative decomposition of pieces as the terms of Euler-Maclaurin formula. Basically it is approximating the definite integral $F(b)-F(a)$ where $b>a$ , unfolding in the direction ""opposite"" to Euler-Maclaurin formula itself. $$ \int_{a}^b f(x) \,\mathrm{d}x \approx F_0 + F_1 + F_2 + \cdots $$ The approximation is done by columns of unit width (summation of $f(k)$ , discretely sampled points) plus corrections to get close to the curve (via derivatives of end points). $$ \int_{a}^b f(x) \,\mathrm{d}x \approx \overbrace{ \underbrace{ \sum_{k = a}^b f(k) }_{ \textbf{unit columns} }  - \frac{f(a)+f(b)}2 }^{ \textbf{unit columns centered} } - \underbrace{ \frac{f'(b) - f'(a)}{12} - 0 }_{ \textstyle\genfrac{}{}{0pt}{}{\textbf{net result of} }{ \textbf{triangular & parabolic cap} } } - \overbrace{ \frac{f'''(b) - f'''(a)}{720} - 0}^{ \textbf{cubic & guartic cap} }  - \cdots$$ The correct analysis would have to account for both the emergence of Bernoulli numbers and the individual orders of correction ... and the remainder if possible. Elaboration on the Ideas For simplicity, consider $(a,b) \in \mathbb{N}^2$ and let the summation be in unit steps as usual, then pretend that things will scale nicely. The zero-th order approximation for the integral is the columns of unit width $$F_0 = \sum_{k=a}^b f(k)$$ Note that each column of height $f(k)$ is left-aligned. For example, the first column for $k = a$ occupies $x \in [a, a+1]$ . This means the last column at $k = b$ is entirely outside of the range of integration. The $2$ nd term (still zero-th order) shifts all the columns by $\frac12$ to make them centered. That is, each column of height $f(k)$ now resides at $x \in [a-\frac12, a+\frac12]$ . The last column is now half-outside, and so is the first column. Thus we remove half of each of them. $$ F_1 = -\frac{f(a) + f(b)}2$$ The $3$ rd term ( $1$ st order, linear) correction is supposed to shave off a triangular top from each column (note that the outermost are half-columns at $k = b$ and $k = a$ ). The height of each triangle is $f'(k)$ and the width is unity. $\color{brown}{\textit{Somehow}}$ there should be a telescoping of terms (combine or cancellation) with the $3$ rd order correction and we end up with $$F_2 = - \frac16 \frac{f'(b) - f'(a)}2$$ The $4$ th term ( $2$ nd order, parabolic) correction is supposed to be something involving both $f''(k)$ and $f'(k)$ . It should be a parabolic piece on top of the triangular (linear) correction of the previous order, similar in spirit ** to what one sees in the quadrature of parabola . Note that there's a fixed ratio of $\frac13$ (or $\frac23$ ) between the parabolic area and the rectangle it spans. $\color{brown}{\textit{Somehow}}$ there should be telescoping with the $2$ nd as well as the $4$ th order so that $$F_3 = 0~.$$ **Here one uses parabolic pieces to fill the gaps for one iteration then uses cubic pieces for the next iteration and so on, while in ancient quadrature one keeps using triangular pieces that are linear. The $5$ th term ( $3$ rd order, cubic) is supposed to some kind of lune (moon crest) on top of the parabolic correction, and $\color{brown}{\textit{somehow}}$ after cancellation it will end up being $$F_4 = -\frac{f'''(b) - f'''(a)}{720}$$ So on and so forth .... In the end, one shall keep the summation $F_0$ on one side of the equation by itself, moving the terms $F_1$ , $F_2$ , $F_3$ etc to the side of the integral, to have the Euler-Maclaurin summation formula. $$F_0 \approx \int_{a}^b f(x) \,\mathrm{d}x - F_1 - F_2 - \cdots$$","In Tom Apostol's expository article (here's a free link) , upon seeing the figure below ( or this from the Wolfram project ) I was expecting more diagrams to come to continue the error decomposition of the shaded regions in the shape of ""curved triangular pieces"". For each horizontal unit interval, it seems that the hypotenuse of an implicit triangle (within shaded region) is the linear correction. Then, there's a ""cap"" that fills in the gap between the hypotenuse and the actual curve. In this figure, each cap is attached to the implicit hypotenuse from below due to the curve being convex. It was a disappointment not seeing further decomposition of the ""cap"" into regions described by parabolic, cubic, then quartic curves etc. Somehow none of the textbooks and other materials I've read have such diagrams for higher orders. Question Statement What is the correct analysis to demonstrate the 'geometry' of the successive orders of the Euler-Maclaurin formula? Is there any existing source with such diagrams (visualization) similar to the above regarding higher order? General Remarks I'm actually not sure whether my proposal is a valid idea (that such a demonstration is possible). Below is a more detailed description (repeating things said above) of what I mean by the tentative decomposition of pieces as the terms of Euler-Maclaurin formula. Basically it is approximating the definite integral where , unfolding in the direction ""opposite"" to Euler-Maclaurin formula itself. The approximation is done by columns of unit width (summation of , discretely sampled points) plus corrections to get close to the curve (via derivatives of end points). The correct analysis would have to account for both the emergence of Bernoulli numbers and the individual orders of correction ... and the remainder if possible. Elaboration on the Ideas For simplicity, consider and let the summation be in unit steps as usual, then pretend that things will scale nicely. The zero-th order approximation for the integral is the columns of unit width Note that each column of height is left-aligned. For example, the first column for occupies . This means the last column at is entirely outside of the range of integration. The nd term (still zero-th order) shifts all the columns by to make them centered. That is, each column of height now resides at . The last column is now half-outside, and so is the first column. Thus we remove half of each of them. The rd term ( st order, linear) correction is supposed to shave off a triangular top from each column (note that the outermost are half-columns at and ). The height of each triangle is and the width is unity. there should be a telescoping of terms (combine or cancellation) with the rd order correction and we end up with The th term ( nd order, parabolic) correction is supposed to be something involving both and . It should be a parabolic piece on top of the triangular (linear) correction of the previous order, similar in spirit ** to what one sees in the quadrature of parabola . Note that there's a fixed ratio of (or ) between the parabolic area and the rectangle it spans. there should be telescoping with the nd as well as the th order so that **Here one uses parabolic pieces to fill the gaps for one iteration then uses cubic pieces for the next iteration and so on, while in ancient quadrature one keeps using triangular pieces that are linear. The th term ( rd order, cubic) is supposed to some kind of lune (moon crest) on top of the parabolic correction, and after cancellation it will end up being So on and so forth .... In the end, one shall keep the summation on one side of the equation by itself, moving the terms , , etc to the side of the integral, to have the Euler-Maclaurin summation formula.","F(b)-F(a) b>a 
\int_{a}^b f(x) \,\mathrm{d}x \approx F_0 + F_1 + F_2 + \cdots
 f(k) 
\int_{a}^b f(x) \,\mathrm{d}x \approx \overbrace{ \underbrace{ \sum_{k = a}^b f(k) }_{ \textbf{unit columns} }  - \frac{f(a)+f(b)}2 }^{ \textbf{unit columns centered} } - \underbrace{ \frac{f'(b) - f'(a)}{12} - 0 }_{ \textstyle\genfrac{}{}{0pt}{}{\textbf{net result of} }{ \textbf{triangular & parabolic cap} } } - \overbrace{ \frac{f'''(b) - f'''(a)}{720} - 0}^{ \textbf{cubic & guartic cap} }  - \cdots (a,b) \in \mathbb{N}^2 F_0 = \sum_{k=a}^b f(k) f(k) k = a x \in [a, a+1] k = b 2 \frac12 f(k) x \in [a-\frac12, a+\frac12]  F_1 = -\frac{f(a) + f(b)}2 3 1 k = b k = a f'(k) \color{brown}{\textit{Somehow}} 3 F_2 = - \frac16 \frac{f'(b) - f'(a)}2 4 2 f''(k) f'(k) \frac13 \frac23 \color{brown}{\textit{Somehow}} 2 4 F_3 = 0~. 5 3 \color{brown}{\textit{somehow}} F_4 = -\frac{f'''(b) - f'''(a)}{720} F_0 F_1 F_2 F_3 F_0 \approx \int_{a}^b f(x) \,\mathrm{d}x - F_1 - F_2 - \cdots","['real-analysis', 'integration', 'summation', 'taylor-expansion', 'visualization']"
64,Witt's proof of Gelfand-Mazur / Ostrowski's theorem,Witt's proof of Gelfand-Mazur / Ostrowski's theorem,,"Now asked on MathOverflow . Background : It seems that, after his groundbreaking work on quadratic forms and inventing Witt vectors, Ernst Witt developed the hobby of giving extremely short proofs to famous theorems. E.g. his collected works contain a one-page proof of the prime number theorem. What interests me in this question is his six-line proof of the Gelfand-Mazur Theorem (""The only $\Bbb C$ -Banach algebra $K$ which is a skew field is $\Bbb C$ itself"") resp. its historic predecessor and now corollary, Ostrowski's Theorem (""The only complete Archimedean fields are $\Bbb R$ and $\Bbb C$ ""; not to be confused with the more famous Ostrowski's theorem which classifies the valuations on $\Bbb Q$ ). Witt's article is ""Über einen Satz von Ostrowski"", Arch.  Math. 3 (1952), p. 334, reprinted on p. 404 of his Collected Papers. Unfortunately, I do not have access to either source right now. The best I could find is this (p. 245) English translation of its decisive three sentences. I understand the first sentence, which says that w.l.o.g. we can assume $\dim_{\Bbb R}K>2$ and hence $K^\times$ simply connected. I also understand the third sentence which says that there cannot be an isomorphism between the additive group of $K$ and the multiplicative group $K^\times$ (obviously, as we are in characteristic $0$ ; there is a misprint in the translation, since of course it's the element $-1$ which is of order 2 in $K^\times$ , and IIRC that's what Witt writes in the original). But the second sentence The differential equation $x^{-1}dx = y$ then [i.e. assuming $K\setminus \lbrace 0\rbrace$ simply connected] engenders a global isomorphism between the multiplicative group $(x\neq 0$ ) and the additive group $(y)$ . seems to hide some details. I guess the idea is that by simply connectedness, path-integrating $f(x) = x^{-1}$ from the startpoint $1$ to any $x\neq 0$ gives a well defined ""logarithm"" function $F(x)$ which has the property $F(ab) = F(a) + F(b)$ and is bijective. (Which then, as said, I understand gives a contradiction and shows no such $K$ with $\Bbb R$ -dimension $>2$ exists.) Question 1: How to understand the highlighted sentence? In particular, is my interpretation correct and if yes, how exactly to show such an $F$ is a group isomorphism $K^\times \simeq (K,+)$ ? Question 2: Does this prove Gelfand-Mazur, as the source of the above translation seems to imply, or merely Ostrowski's theorem, as Witt's own title claims? If it only proves Ostrowski's theorem, is there a way to upgrade this to a full proof of Gelfand-Mazur? Note 1: As said I cannot check it right now, but I think in the original article Witt actually writes the differential equation $x^{-1}dx = dy$ which makes more sense to me. Note 2: I am aware of the standard calculus proof that $F(x) = \int_1^x \frac{dt}{t}$ satisfies $F(ab) = F(a) +F(b)$ (actually, teaching that in my calculus class last week reminded me of this question), but it seems Witt is assuming a generalisation of this and maybe more differential calculus to a possibly infinite dimensional $\Bbb R$ resp. $\Bbb C$ -vector space, which is a bit outside of my comfort zone.","Now asked on MathOverflow . Background : It seems that, after his groundbreaking work on quadratic forms and inventing Witt vectors, Ernst Witt developed the hobby of giving extremely short proofs to famous theorems. E.g. his collected works contain a one-page proof of the prime number theorem. What interests me in this question is his six-line proof of the Gelfand-Mazur Theorem (""The only -Banach algebra which is a skew field is itself"") resp. its historic predecessor and now corollary, Ostrowski's Theorem (""The only complete Archimedean fields are and ""; not to be confused with the more famous Ostrowski's theorem which classifies the valuations on ). Witt's article is ""Über einen Satz von Ostrowski"", Arch.  Math. 3 (1952), p. 334, reprinted on p. 404 of his Collected Papers. Unfortunately, I do not have access to either source right now. The best I could find is this (p. 245) English translation of its decisive three sentences. I understand the first sentence, which says that w.l.o.g. we can assume and hence simply connected. I also understand the third sentence which says that there cannot be an isomorphism between the additive group of and the multiplicative group (obviously, as we are in characteristic ; there is a misprint in the translation, since of course it's the element which is of order 2 in , and IIRC that's what Witt writes in the original). But the second sentence The differential equation then [i.e. assuming simply connected] engenders a global isomorphism between the multiplicative group ) and the additive group . seems to hide some details. I guess the idea is that by simply connectedness, path-integrating from the startpoint to any gives a well defined ""logarithm"" function which has the property and is bijective. (Which then, as said, I understand gives a contradiction and shows no such with -dimension exists.) Question 1: How to understand the highlighted sentence? In particular, is my interpretation correct and if yes, how exactly to show such an is a group isomorphism ? Question 2: Does this prove Gelfand-Mazur, as the source of the above translation seems to imply, or merely Ostrowski's theorem, as Witt's own title claims? If it only proves Ostrowski's theorem, is there a way to upgrade this to a full proof of Gelfand-Mazur? Note 1: As said I cannot check it right now, but I think in the original article Witt actually writes the differential equation which makes more sense to me. Note 2: I am aware of the standard calculus proof that satisfies (actually, teaching that in my calculus class last week reminded me of this question), but it seems Witt is assuming a generalisation of this and maybe more differential calculus to a possibly infinite dimensional resp. -vector space, which is a bit outside of my comfort zone.","\Bbb C K \Bbb C \Bbb R \Bbb C \Bbb Q \dim_{\Bbb R}K>2 K^\times K K^\times 0 -1 K^\times x^{-1}dx = y K\setminus \lbrace 0\rbrace (x\neq 0 (y) f(x) = x^{-1} 1 x\neq 0 F(x) F(ab) = F(a) + F(b) K \Bbb R >2 F K^\times \simeq (K,+) x^{-1}dx = dy F(x) = \int_1^x \frac{dt}{t} F(ab) = F(a) +F(b) \Bbb R \Bbb C","['real-analysis', 'complex-analysis', 'functional-analysis', 'ordinary-differential-equations', 'banach-algebras']"
65,"Why does a polynomial with real, simple roots change its sign between its roots?","Why does a polynomial with real, simple roots change its sign between its roots?",,"In the mathematics book I have, there is a sub-chapter called ""Practical procedure to resolve inequalities"" that states: Given a polynomial $P(x)$ that has real, simple roots, and finding the solutions to the equation $P(x) = 0$ , afterwards sorting the solutions $x_1, x_2, ..., x_n$ , then the sign of $P$ over an interval $(x_i, x_{i + 1})$ is the opposite of its neighboring intervals $(x_{i - 1}, x_i)$ and $(x_{i + 1}, x_{i + 2})$ . I've plotted functions of the form $$a\prod_{i = 1}^{n}(x - a_i), \space a, a_1, a_2, ..., a_n \in [0, \infty), \space a_i \ne a_j \space \forall i, j \in \{1, 2, ..., n\} $$ What's an intuitive way of thinking about this and why it happens?","In the mathematics book I have, there is a sub-chapter called ""Practical procedure to resolve inequalities"" that states: Given a polynomial that has real, simple roots, and finding the solutions to the equation , afterwards sorting the solutions , then the sign of over an interval is the opposite of its neighboring intervals and . I've plotted functions of the form What's an intuitive way of thinking about this and why it happens?","P(x) P(x) = 0 x_1, x_2, ..., x_n P (x_i, x_{i + 1}) (x_{i - 1}, x_i) (x_{i + 1}, x_{i + 2}) a\prod_{i = 1}^{n}(x - a_i), \space a, a_1, a_2, ..., a_n \in [0, \infty), \space a_i \ne a_j \space \forall i, j \in \{1, 2, ..., n\} ","['real-analysis', 'calculus', 'polynomials', 'real-numbers']"
66,How to show that $\sqrt{2}+\sqrt{3}$ is algebraic?,How to show that  is algebraic?,\sqrt{2}+\sqrt{3},"In Abbot's Understanding Analysis I am asked to show that $\sqrt{2}+\sqrt{3}$ is an algebraic number. I have shown that those two are algebraic separately (that was simple), but I can't figure out what to do to show that their sum is algebraic, too. For example, I tried $(\sqrt{2}+\sqrt{3})^{2}=5+\sqrt{24}$ and then I tried to think of a polynomial of form $ax^2-bx^{0}=0$ e.g. $c(\sqrt{2}+\sqrt{3})^{2}-c(5+\sqrt{24})=0$ that would work, but couldn't find the $c$ value that would make $b=c(5+\sqrt{24})$ integer, but couldn't find one. Maybe I can somehow conjecture that the sum of two algebraic numbers must be algebraic, too, but I was wondering if there's a way to find a polynomial to show this. Thanks!","In Abbot's Understanding Analysis I am asked to show that $\sqrt{2}+\sqrt{3}$ is an algebraic number. I have shown that those two are algebraic separately (that was simple), but I can't figure out what to do to show that their sum is algebraic, too. For example, I tried $(\sqrt{2}+\sqrt{3})^{2}=5+\sqrt{24}$ and then I tried to think of a polynomial of form $ax^2-bx^{0}=0$ e.g. $c(\sqrt{2}+\sqrt{3})^{2}-c(5+\sqrt{24})=0$ that would work, but couldn't find the $c$ value that would make $b=c(5+\sqrt{24})$ integer, but couldn't find one. Maybe I can somehow conjecture that the sum of two algebraic numbers must be algebraic, too, but I was wondering if there's a way to find a polynomial to show this. Thanks!",,"['real-analysis', 'polynomials', 'galois-theory']"
67,How to evaluate $\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta) d\theta$?,How to evaluate ?,\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta) d\theta,"For $\alpha \in \mathbb{R}$, define $\displaystyle I(\alpha):=\int_{0}^{2\pi}e^{\alpha \cos \theta}\cos(\alpha \sin \theta)\; d\theta$. Calculate $I(0)$. Hence evaluate $\displaystyle\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta)\; d\theta$. Hint: To evaluate the integral that expresses $\displaystyle\frac{dI}{d\alpha}$, consider $\displaystyle\frac{\partial}{\partial \theta}(e^{\alpha \cos \theta}\sin(\alpha \sin \theta))$. How do I do this question? I think this might have something to do with the Fundamental Theorem of Calculus, but I'm not sure. I computed $\displaystyle I(0)=\int_{0}^{2\pi} d\theta=2 \pi$, and $\displaystyle I(1)=\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta) d\theta$. Following the hint I get $$\begin{align} \frac{\partial}{\partial \theta}(e^{\alpha \cos \theta}\sin(\alpha \sin \theta)) & =\alpha e^{\alpha \cos \theta} \sin (\alpha \sin \theta) + e^{\alpha \cos \theta}\cos(\alpha \sin \theta) \alpha \cos \theta \\ & =  \alpha e^{\alpha \cos \theta} \sin (\alpha \sin \theta) + \frac{dI}{d \alpha} \cos \theta. \\ \end{align}$$ Is this correct so far? The answers in the question referred as a duplicate does not help. I'm in a course dealing with real values, not complex.","For $\alpha \in \mathbb{R}$, define $\displaystyle I(\alpha):=\int_{0}^{2\pi}e^{\alpha \cos \theta}\cos(\alpha \sin \theta)\; d\theta$. Calculate $I(0)$. Hence evaluate $\displaystyle\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta)\; d\theta$. Hint: To evaluate the integral that expresses $\displaystyle\frac{dI}{d\alpha}$, consider $\displaystyle\frac{\partial}{\partial \theta}(e^{\alpha \cos \theta}\sin(\alpha \sin \theta))$. How do I do this question? I think this might have something to do with the Fundamental Theorem of Calculus, but I'm not sure. I computed $\displaystyle I(0)=\int_{0}^{2\pi} d\theta=2 \pi$, and $\displaystyle I(1)=\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta) d\theta$. Following the hint I get $$\begin{align} \frac{\partial}{\partial \theta}(e^{\alpha \cos \theta}\sin(\alpha \sin \theta)) & =\alpha e^{\alpha \cos \theta} \sin (\alpha \sin \theta) + e^{\alpha \cos \theta}\cos(\alpha \sin \theta) \alpha \cos \theta \\ & =  \alpha e^{\alpha \cos \theta} \sin (\alpha \sin \theta) + \frac{dI}{d \alpha} \cos \theta. \\ \end{align}$$ Is this correct so far? The answers in the question referred as a duplicate does not help. I'm in a course dealing with real values, not complex.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
68,"Calculate in closed form $\int_0^1 \int_0^1 \frac{dx\,dy}{1-xy(1-x)(1-y)}$",Calculate in closed form,"\int_0^1 \int_0^1 \frac{dx\,dy}{1-xy(1-x)(1-y)}","Can we possibly compute the following integral in terms of known constants? $$\int_0^1 \int_0^1 \frac{dx\,dy}{1-xy(1-x)(1-y)}$$ Some progress was already done here http://integralsandseries.prophpbb.com/topic279.html but still we have a hypergeometric function. What's your thoughts on it? UPDATE: The question was also posted on mathoverflow here https://mathoverflow.net/questions/206253/calculate-in-closed-form-int-01-int-01-fracdx-dy1-xy1-x1-y#","Can we possibly compute the following integral in terms of known constants? $$\int_0^1 \int_0^1 \frac{dx\,dy}{1-xy(1-x)(1-y)}$$ Some progress was already done here http://integralsandseries.prophpbb.com/topic279.html but still we have a hypergeometric function. What's your thoughts on it? UPDATE: The question was also posted on mathoverflow here https://mathoverflow.net/questions/206253/calculate-in-closed-form-int-01-int-01-fracdx-dy1-xy1-x1-y#",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
69,How do you show that $e^{-1/x^2}$ is differentiable at $0$?,How do you show that  is differentiable at ?,e^{-1/x^2} 0,Define the real valued function $f(x)$ by $f(x) = e^{-1/x^2}$ if $x \ne 0$ and $0$ if $x = 0$ . How do you show that $e^{-1/x^2}$ is differentiable at $0$ ? How do you show that this function is infinitely differentiable?,Define the real valued function by if and if . How do you show that is differentiable at ? How do you show that this function is infinitely differentiable?,f(x) f(x) = e^{-1/x^2} x \ne 0 0 x = 0 e^{-1/x^2} 0,"['calculus', 'real-analysis']"
70,Showing that $\lim\limits_{n\to\infty}\sum^n_{k=1}\frac{1}{k}-\ln(n)=0.5772\ldots$ [duplicate],Showing that  [duplicate],\lim\limits_{n\to\infty}\sum^n_{k=1}\frac{1}{k}-\ln(n)=0.5772\ldots,This question already has answers here : Is there another proof for Euler–Mascheroni Constant? (4 answers) Closed 3 years ago . How to show that  $$\lim_{n\to\infty}\left[\sum^n_{k=1}\frac{1}{k}-\ln(n)\right]=0.5772\ldots$$ No clue at all. Need help!  Appreciated!,This question already has answers here : Is there another proof for Euler–Mascheroni Constant? (4 answers) Closed 3 years ago . How to show that  $$\lim_{n\to\infty}\left[\sum^n_{k=1}\frac{1}{k}-\ln(n)\right]=0.5772\ldots$$ No clue at all. Need help!  Appreciated!,,"['real-analysis', 'calculus']"
71,Roots of Legendre Polynomial,Roots of Legendre Polynomial,,"I was wondering if the following properties of the Legendre polynomials are true in general. They hold for the first ten or fifteen polynomials. Are the roots always simple (i.e., multiplicity $1$)? Except for low-degree cases, the roots can't be calculated exactly, only approximated (unlike Chebyshev polynomials). Are roots of the entire family of Legendre Polynomials dense in the interval $[0,1]$ (i.e., it's not possible to find a subinterval, no matter how small, that doesn't contain at least one root of one polynomial)? If anyone knows of an article/text that proves any of the above, please let me know.  The definition of these polynomials can be found on Wikipedia .","I was wondering if the following properties of the Legendre polynomials are true in general. They hold for the first ten or fifteen polynomials. Are the roots always simple (i.e., multiplicity $1$)? Except for low-degree cases, the roots can't be calculated exactly, only approximated (unlike Chebyshev polynomials). Are roots of the entire family of Legendre Polynomials dense in the interval $[0,1]$ (i.e., it's not possible to find a subinterval, no matter how small, that doesn't contain at least one root of one polynomial)? If anyone knows of an article/text that proves any of the above, please let me know.  The definition of these polynomials can be found on Wikipedia .",,"['real-analysis', 'roots', 'special-functions', 'orthogonal-polynomials', 'legendre-polynomials']"
72,A simple way to obtain $\prod_{p\in\mathbb{P}}\frac{1}{1-p^{-s}}=\sum_{n=1}^{\infty}\frac{1}{n^s}$.,A simple way to obtain .,\prod_{p\in\mathbb{P}}\frac{1}{1-p^{-s}}=\sum_{n=1}^{\infty}\frac{1}{n^s},"Let $ p_1<p_2 <\cdots <p_k < \cdots $ the increasing list in set $\mathbb{P}$ of all prime numbers . By sum of infinite geometric series we have $\sum_{k=0}^\infty r^k = \frac{1}{1-r}$ , for $0<r<1$ . For all $s>1$ and $r=\frac{1}{p_k^{s}}$ we have $$ \begin{array}{cccccc} \dfrac{1}{1-p_{1}^{-s}} & = & 1+\dfrac{1}{(p_1^s)^1}+\dfrac{1}{(p_1^s)^2}+\dfrac{1}{(p_1^s)^3}+ & \!\!\cdots\!\! & +\dfrac{1}{(p_1^{s})^{\alpha_1}}+ & \cdots \\ \dfrac{1}{1-p_{2}^{-s}} & = & 1+\dfrac{1}{(p_2^s)^1}+\dfrac{1}{(p_2^s)^2}+\dfrac{1}{(p_2^s)^3}+ & \!\!\cdots\!\! & +\dfrac{1}{(p_2^s)^{\alpha_2}}+ & \cdots \\ \dfrac{1}{1-p_{3}^{-s}} & = & 1+\dfrac{1}{(p_3^s)^1}+\dfrac{1}{(p_3^s)^2}+\dfrac{1}{(p_3^s)^3}+ & \!\!\cdots\!\! & +\dfrac{1}{(p_3^s)^{\alpha_3}}+ & \cdots \\ \vdots  & \vdots &  \vdots & \vdots & \vdots &\vdots \\ \\ \vdots  & \vdots &  \vdots & \vdots & \vdots & \vdots \\ \dfrac{1}{1-p_{k}^{-s}} & = & 1+\dfrac{1}{(p_k^s)^1}+\dfrac{1}{(p_k^s)^2}+\dfrac{1}{(p_k^s)^3}+ & \!\!\cdots\!\! & +\dfrac{1}{(p_k^s)^{\alpha_k}}+ & \cdots \\ \vdots  & \vdots &  \vdots & \vdots & \vdots & \vdots \\ \end{array} $$ And the Fundamental Theorem of Arithmetic tells us that every integer $ n> 1$ can be decomposed uniquely as a product $$  n= p_{i_1}^{\alpha_{i_1}}p_{i_2}^{\alpha_{i_2}}\cdots p_{i_k}^{\alpha_{i_k}} $$ of powers of prime numbers $p_{i_1}< p_{i_2}< \cdots < p_{i_k}$ for integers $\alpha_{i_1},\alpha_{i_2},\ldots,\alpha_{i_k}\geq 1$ . Since $  n^s= (p_{i_1}^s)^{\alpha_{i_1}}(p_{i_2}^{s})^{\alpha_{i_2}}\cdots (p_{i_k}^s)^{\alpha_{i_k}}$ and  using brute force with I can prove that $$ \prod_{p\in\mathbb{P}}\frac{1}{1-p^{-s}}=\sum_{n=1}^\infty \frac{1}{n^s} $$ But I would like to know if there is a simple and elegant way to achieve this result is up through the above list.","Let the increasing list in set of all prime numbers . By sum of infinite geometric series we have , for . For all and we have And the Fundamental Theorem of Arithmetic tells us that every integer can be decomposed uniquely as a product of powers of prime numbers for integers . Since and  using brute force with I can prove that But I would like to know if there is a simple and elegant way to achieve this result is up through the above list."," p_1<p_2 <\cdots <p_k < \cdots  \mathbb{P} \sum_{k=0}^\infty r^k = \frac{1}{1-r} 0<r<1 s>1 r=\frac{1}{p_k^{s}} 
\begin{array}{cccccc}
\dfrac{1}{1-p_{1}^{-s}}
&
=
&
1+\dfrac{1}{(p_1^s)^1}+\dfrac{1}{(p_1^s)^2}+\dfrac{1}{(p_1^s)^3}+
&
\!\!\cdots\!\!
&
+\dfrac{1}{(p_1^{s})^{\alpha_1}}+
&
\cdots
\\
\dfrac{1}{1-p_{2}^{-s}}
&
=
&
1+\dfrac{1}{(p_2^s)^1}+\dfrac{1}{(p_2^s)^2}+\dfrac{1}{(p_2^s)^3}+
&
\!\!\cdots\!\!
&
+\dfrac{1}{(p_2^s)^{\alpha_2}}+
&
\cdots
\\
\dfrac{1}{1-p_{3}^{-s}}
&
=
&
1+\dfrac{1}{(p_3^s)^1}+\dfrac{1}{(p_3^s)^2}+\dfrac{1}{(p_3^s)^3}+
&
\!\!\cdots\!\!
&
+\dfrac{1}{(p_3^s)^{\alpha_3}}+
&
\cdots
\\
\vdots 
&
\vdots
& 
\vdots
&
\vdots
&
\vdots
&\vdots
\\
\\
\vdots 
&
\vdots
& 
\vdots
&
\vdots
&
\vdots
&
\vdots
\\
\dfrac{1}{1-p_{k}^{-s}}
&
=
&
1+\dfrac{1}{(p_k^s)^1}+\dfrac{1}{(p_k^s)^2}+\dfrac{1}{(p_k^s)^3}+
&
\!\!\cdots\!\!
&
+\dfrac{1}{(p_k^s)^{\alpha_k}}+
&
\cdots
\\
\vdots 
&
\vdots
& 
\vdots
&
\vdots
&
\vdots
&
\vdots
\\
\end{array}
  n> 1 
 n= p_{i_1}^{\alpha_{i_1}}p_{i_2}^{\alpha_{i_2}}\cdots p_{i_k}^{\alpha_{i_k}}
 p_{i_1}< p_{i_2}< \cdots < p_{i_k} \alpha_{i_1},\alpha_{i_2},\ldots,\alpha_{i_k}\geq 1 
 n^s= (p_{i_1}^s)^{\alpha_{i_1}}(p_{i_2}^{s})^{\alpha_{i_2}}\cdots (p_{i_k}^s)^{\alpha_{i_k}} 
\prod_{p\in\mathbb{P}}\frac{1}{1-p^{-s}}=\sum_{n=1}^\infty \frac{1}{n^s}
","['real-analysis', 'number-theory', 'probability-theory', 'prime-numbers', 'independence']"
73,Infinite Cartesian product of countable sets is uncountable,Infinite Cartesian product of countable sets is uncountable,,"Let $\{E_n\}_{n\in\mathbb{N}}$ be a sequence of countable sets and let $S=E_1\times\cdots\times E_n\times\cdots $. Show $S$ is uncountable. Prove that the same statement holds if each $E_n=\{0,1\}$. By the definition of Cartesian product of sets, $$\displaystyle S=\Pi_{n\in\mathbb{N}} \{f\colon\mathbb{N}\rightarrow\cup_{n\in\mathbb{N}}E_n\mid\forall n, f(n)\in E_n\}$$ If $E_n=\{ 0,1\}$, then $$\displaystyle S_{01}=\Pi_{n\in\mathbb{N}}\{0,1\}=E^{\mathbb{N}}$$, where $E=\{0,1\}$. By a theorem, $\cup_{n\in\mathbb{N}} E_n$ is countable since the sequence is countable. I'm not sure how to go on from here to show $S$ is uncountable. Can we say anything about the function $f$ that maps a countable $\mathbb{N}$ to another countable union of sequence of countable sets?","Let $\{E_n\}_{n\in\mathbb{N}}$ be a sequence of countable sets and let $S=E_1\times\cdots\times E_n\times\cdots $. Show $S$ is uncountable. Prove that the same statement holds if each $E_n=\{0,1\}$. By the definition of Cartesian product of sets, $$\displaystyle S=\Pi_{n\in\mathbb{N}} \{f\colon\mathbb{N}\rightarrow\cup_{n\in\mathbb{N}}E_n\mid\forall n, f(n)\in E_n\}$$ If $E_n=\{ 0,1\}$, then $$\displaystyle S_{01}=\Pi_{n\in\mathbb{N}}\{0,1\}=E^{\mathbb{N}}$$, where $E=\{0,1\}$. By a theorem, $\cup_{n\in\mathbb{N}} E_n$ is countable since the sequence is countable. I'm not sure how to go on from here to show $S$ is uncountable. Can we say anything about the function $f$ that maps a countable $\mathbb{N}$ to another countable union of sequence of countable sets?",,"['real-analysis', 'general-topology', 'elementary-set-theory']"
74,The epsilon-delta definition of continuity,The epsilon-delta definition of continuity,,As we know the epsilon-delta definition of continuity is: For given $$\varepsilon > 0\  \exists \delta > 0\ \text{s.t. } 0 < |x - x_0| < \delta \implies |f(x) - f(x_0)| < \varepsilon $$ My question: Why wouldn't this work if the implication would be: For given $$\varepsilon > 0\ \exists \delta > 0\ \text{s.t. } |f(x) - f(x_0)| < \varepsilon  \implies  0 < |x - x_0| < \delta ?$$,As we know the epsilon-delta definition of continuity is: For given $$\varepsilon > 0\  \exists \delta > 0\ \text{s.t. } 0 < |x - x_0| < \delta \implies |f(x) - f(x_0)| < \varepsilon $$ My question: Why wouldn't this work if the implication would be: For given $$\varepsilon > 0\ \exists \delta > 0\ \text{s.t. } |f(x) - f(x_0)| < \varepsilon  \implies  0 < |x - x_0| < \delta ?$$,,"['real-analysis', 'epsilon-delta']"
75,"Does there exist a continuous function from [0,1] to R that has uncountably many local maxima?","Does there exist a continuous function from [0,1] to R that has uncountably many local maxima?",,"Does there exist a continuous function from $[0,1]$ to $R$ that has uncountably many strict local maxima?","Does there exist a continuous function from $[0,1]$ to $R$ that has uncountably many strict local maxima?",,"['real-analysis', 'functions']"
76,Extension of real analytic function to a complex analytic function,Extension of real analytic function to a complex analytic function,,"I just learned that real analytic functions (by real analytic, I mean functions $f: \mathbb{R} \to \mathbb{R}$ which admit a local Taylor series expansion around any point $p \in \mathbb{R}$) cannot be extended to complex entire function always. I believe functions with this extension property are called real entire functions in some books, and a function which is real analytic, but not real entire is $f(x) = \frac{1}{1 + x^2}$. Clearly, with this example, the problem with any extension happens around $\pm i$. See also this MSE post. My question is, do real analytic functions admit extensions to a complex analytic function even locally? That is, given a real analytic function $f : \mathbb{R} \to \mathbb{R}$, can we find a complex analytic function $g : \Omega \to C$, such that $g|_\mathbb{R} = f$, and $\Omega$ contains a strip $\mathbb{R} \times (-\varepsilon, \varepsilon)$ around the real axis?","I just learned that real analytic functions (by real analytic, I mean functions $f: \mathbb{R} \to \mathbb{R}$ which admit a local Taylor series expansion around any point $p \in \mathbb{R}$) cannot be extended to complex entire function always. I believe functions with this extension property are called real entire functions in some books, and a function which is real analytic, but not real entire is $f(x) = \frac{1}{1 + x^2}$. Clearly, with this example, the problem with any extension happens around $\pm i$. See also this MSE post. My question is, do real analytic functions admit extensions to a complex analytic function even locally? That is, given a real analytic function $f : \mathbb{R} \to \mathbb{R}$, can we find a complex analytic function $g : \Omega \to C$, such that $g|_\mathbb{R} = f$, and $\Omega$ contains a strip $\mathbb{R} \times (-\varepsilon, \varepsilon)$ around the real axis?",,"['real-analysis', 'complex-analysis', 'reference-request', 'analytic-functions']"
77,Integral $\int_0^\infty \log(1+x^2)\frac{\cosh{\frac{\pi x}{2}}}{\sinh^2{\frac{\pi x}{2}}}\mathrm dx=2-\frac{4}{\pi}$,Integral,\int_0^\infty \log(1+x^2)\frac{\cosh{\frac{\pi x}{2}}}{\sinh^2{\frac{\pi x}{2}}}\mathrm dx=2-\frac{4}{\pi},"Hi I am trying to show$$ I:=\int_0^\infty \log(1+x^2)\frac{\cosh{\frac{\pi x}{2}}}{\sinh^2{\frac{\pi x}{2}}}\mathrm dx=2-\frac{4}{\pi}. $$ Thank you. What a desirable thing to want to prove!  It is a work of art this one.  I wish to prove this in as many ways as we can find. Note I tried writing $$ I=\int_0^\infty \log(1+x^2)\coth \frac{\pi x}{2} \sinh^{-2} \frac{\pi x}{2}\mathrm dx $$ but this didn't help me much.  We can also try introducing a parameter as follows $$ I(\alpha)=\int_0^\infty \log(1+x^2)\frac{\cosh{\frac{\alpha \pi x}{2}}}{\sinh^2{\frac{\pi x}{2}}}\mathrm dx, $$ But this is where I got stuck.  How can we calculate I?  Thanks.","Hi I am trying to show$$ I:=\int_0^\infty \log(1+x^2)\frac{\cosh{\frac{\pi x}{2}}}{\sinh^2{\frac{\pi x}{2}}}\mathrm dx=2-\frac{4}{\pi}. $$ Thank you. What a desirable thing to want to prove!  It is a work of art this one.  I wish to prove this in as many ways as we can find. Note I tried writing $$ I=\int_0^\infty \log(1+x^2)\coth \frac{\pi x}{2} \sinh^{-2} \frac{\pi x}{2}\mathrm dx $$ but this didn't help me much.  We can also try introducing a parameter as follows $$ I(\alpha)=\int_0^\infty \log(1+x^2)\frac{\cosh{\frac{\alpha \pi x}{2}}}{\sinh^2{\frac{\pi x}{2}}}\mathrm dx, $$ But this is where I got stuck.  How can we calculate I?  Thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
78,Is there a limit of $\cos(n!)$?,Is there a limit of ?,\cos(n!),I encountered a problem today to prove that $(X_n)$ with $X_n = \cos(n!)$ does not have a limit (when $n$ approaches infinity). I have no idea how to do it formally. Could someone help? The simpler the proof (by that I mean less complex theorems are used) the better. Thanks,I encountered a problem today to prove that $(X_n)$ with $X_n = \cos(n!)$ does not have a limit (when $n$ approaches infinity). I have no idea how to do it formally. Could someone help? The simpler the proof (by that I mean less complex theorems are used) the better. Thanks,,"['real-analysis', 'limits', 'analysis', 'trigonometry', 'factorial']"
79,"Is it possible to ""sort"" a continuous function?","Is it possible to ""sort"" a continuous function?",,"I was motivated for this question while seeking for a new sorting algorithm. Suppose a continuous function $f : [a, b] \to \mathbb{R}$ is given. I wanted to define the sorted version $g$ of $f$ , which shall satisfy the following properties: $g$ monotone increases $g(a)$ is the global minimum of $f$ $g(b)$ is the global maximum of $f$ And I eventually came up with the following equation: $$ \int_a^x g = \int_{\{y : f(y) \leq g(x)\}} f $$ And I tried to construct $g$ from $f$ explicitly. Simple argument: Since $\{y : f(y) \leq g(x)\}$ is a closed subset of the compact set $[a, b]$ , it is the union of some finitely many closed intervals (and possibly some isolated points), say $[a_1(x) = a, b_1(x)], [a_1(x), b_1(x)], \cdots, [a_n(x), b_n(x) = b]$ . That gives the following equation: $$ \int_a^x g = \sum_{k=1}^n\int_{a_k(x)}^{b_k(x)} f $$ By taking the derivative of both sides of the equation above, I get this: $$ g(x) = \sum_{k=1}^n (f(b_k(x)) \cdot b_k'(x) - f(a_k(x)) \cdot a_k'(x)) $$ By definition of $a_k$ and $b_k$ , $f \circ a_k = f \circ b_k = g$ , and thus: $$ g(x) = \sum_{k=1}^n (g(x) \cdot b_k'(x) - g(x) \cdot a_k'(x)) = \sum_{k=1}^n g(x) (b_k'(x) - a_k'(x)) = g(x) \sum_{k=1}^n (b_k'(x) - a_k'(x)) $$ ...which doesn't give any information of $g$ . Does this mean this equation is under-determined? Does it even make sense to ""sort"" a function?","I was motivated for this question while seeking for a new sorting algorithm. Suppose a continuous function is given. I wanted to define the sorted version of , which shall satisfy the following properties: monotone increases is the global minimum of is the global maximum of And I eventually came up with the following equation: And I tried to construct from explicitly. Simple argument: Since is a closed subset of the compact set , it is the union of some finitely many closed intervals (and possibly some isolated points), say . That gives the following equation: By taking the derivative of both sides of the equation above, I get this: By definition of and , , and thus: ...which doesn't give any information of . Does this mean this equation is under-determined? Does it even make sense to ""sort"" a function?","f : [a, b] \to \mathbb{R} g f g g(a) f g(b) f 
\int_a^x g = \int_{\{y : f(y) \leq g(x)\}} f
 g f \{y : f(y) \leq g(x)\} [a, b] [a_1(x) = a, b_1(x)], [a_1(x), b_1(x)], \cdots, [a_n(x), b_n(x) = b] 
\int_a^x g = \sum_{k=1}^n\int_{a_k(x)}^{b_k(x)} f
 
g(x) = \sum_{k=1}^n (f(b_k(x)) \cdot b_k'(x) - f(a_k(x)) \cdot a_k'(x))
 a_k b_k f \circ a_k = f \circ b_k = g 
g(x) = \sum_{k=1}^n (g(x) \cdot b_k'(x) - g(x) \cdot a_k'(x)) = \sum_{k=1}^n g(x) (b_k'(x) - a_k'(x)) = g(x) \sum_{k=1}^n (b_k'(x) - a_k'(x))
 g","['real-analysis', 'calculus', 'sorting']"
80,Iterated integral question,Iterated integral question,,"Show $$\lim_{n \to\infty} \int_0^1 \cdots  \int_0^1  \int_0^1 \frac{ x_1^2 + \cdots +  x_n^2}{x_1 + \cdots + x_n} \, dx_1 \cdots dx_n = \frac 2 3.$$ Not sure how to start off this iterated integral question, any help would be appreciated.","Show $$\lim_{n \to\infty} \int_0^1 \cdots  \int_0^1  \int_0^1 \frac{ x_1^2 + \cdots +  x_n^2}{x_1 + \cdots + x_n} \, dx_1 \cdots dx_n = \frac 2 3.$$ Not sure how to start off this iterated integral question, any help would be appreciated.",,"['calculus', 'real-analysis', 'probability', 'integration', 'sequences-and-series']"
81,To what extent is the taylor polynomial the best polynomial approximation?,To what extent is the taylor polynomial the best polynomial approximation?,,"Given a function $f\in\mathscr C^n([a,b])$ and a point $x_0\in [a,b]$, to what extent is the n-th taylor polynomial $T_n(x,x_0)=\sum_{k=0}^n\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k$ the best polynomial approximation of $f$ in $[a,b]$ ? This may seem to be dumb question, but is there a metric $\rho$ on $ C^n([a,b])$ so that $\rho(T_n(x,x_0),f)=\min\{\rho(p,f)\mid \text{p is a polynomial function} \}$ ?  Thank you","Given a function $f\in\mathscr C^n([a,b])$ and a point $x_0\in [a,b]$, to what extent is the n-th taylor polynomial $T_n(x,x_0)=\sum_{k=0}^n\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k$ the best polynomial approximation of $f$ in $[a,b]$ ? This may seem to be dumb question, but is there a metric $\rho$ on $ C^n([a,b])$ so that $\rho(T_n(x,x_0),f)=\min\{\rho(p,f)\mid \text{p is a polynomial function} \}$ ?  Thank you",,"['real-analysis', 'analysis', 'functional-analysis']"
82,Number of $\sigma$  -Algebra on the finite set,Number of   -Algebra on the finite set,\sigma,Let $X$ is a nonempty set with $m$ members . How many $\sigma$ -algebra can we make on this set?,Let $X$ is a nonempty set with $m$ members . How many $\sigma$ -algebra can we make on this set?,,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
83,If $f:\mathbb{R}\to\mathbb{R}$ is a left continuous function can the set of discontinuous points of $f$ have positive Lebesgue measure?,If  is a left continuous function can the set of discontinuous points of  have positive Lebesgue measure?,f:\mathbb{R}\to\mathbb{R} f,"If $f:\mathbb{R}\to\mathbb{R}$ is a left continuous function can the set of discontinuous points of $f$ have positive Lebesgue measure? I wondered this today, but made little progress. Thank you.","If $f:\mathbb{R}\to\mathbb{R}$ is a left continuous function can the set of discontinuous points of $f$ have positive Lebesgue measure? I wondered this today, but made little progress. Thank you.",,['real-analysis']
84,A sub-additivity inequality,A sub-additivity inequality,,"In trying to understand a result of D. Rider (Trans. AMS, 1973) I've got stuck on a lemma that he uses. At one point he makes a step without comment or explanation, but I can't see why it works. Here is a paraphrase of what I think he is claiming. Let $d_1,\dots, d_n$ be complex numbers of modulus $1$; let $b_1,\dots, b_n$ be complex numbers of modulus $\leq 1$. (In the intended application the numbers $b_1,\dots, b_n$ are actually the diagonal entries of a unitary $n\times n$ matrix, but I strongly suspect that is not used in the argument.) Then Rider seems to assert, without further comment, that $$ \left\vert\sum_{j=1}^n (d_jb_j-1)\right\vert^{1/2} \leq \left\vert\sum_{j=1}^n (d_j-1)\right\vert^{1/2} +  \left\vert\sum_{j=1}^n (b_j-1)\right\vert^{1/2} $$ Probably I am just being dense, and have failed to spot a routine estimate that does the job. If anyone could get me started on the right track that would be most helpful, as this is starting to get very irritating, and is not even the main part of Rider's argument. Update Firstly, thanks to everyone who answered, but in particular to Will Jagy for various off-MSE exchanges, and to George Lowther for his elegant argument (the key part that I had failed to think of, was the use of $\rm Re$ and its linearity rather than $\vert\cdot\vert$ and its sub-additivity, allowing one to boost up the observation made by Gerry Myerson). In case it's of interest, here are some more details on how the question I asked corresponds to Lemma 5.1 in Rider's paper. The paper is dealing with central trigonometric polynomials on compact groups, which means: linear combinations of traces of irreducible representations. Paraphrased lightly, and with a fairly trivial reduction step thrown in, the aforementioned lemma says the following: Let $\phi: G \to U(n)$ be an (irreducible) unitary representation of a (compact) group $G$. If $$\vert n^{-1}\operatorname{\rm Tr}\phi(g_i)-z_i\vert \leq \delta_i \qquad(i=1,\dots, p)$$   where $\vert z_i\vert=1$ for all $i$, then   $$ \left\vert n^{-1}\operatorname{Tr}\phi(g_1\dotsb g_n) - \prod_{i=1}^p z_i \right\vert \leq \left(\sum_{i=1}^p \delta_i^{1/2}\right)^2 $$ The proof given in the paper goes as follows: reduce to the case $p=2$ (if this can be done then induction will do the rest); then observe that WLOG $\phi(g_1)$ is a diagonal unitary matrix, w.r.t. an appropriate basis. If we let $d_1,\dots, d_n$ be the diagonal entries of $\phi(g_1)$ and $b_1,\dots, b_n$ be those of $\phi(g_2)$, then the trace of $\phi(g_1g_2)$ is just $\sum_{i=1}^n d_ib_i$, and so we have to prove Claim. If $|z_1|=|z_2|=1$, $\left\vert\sum_{i=1}^n (d_i - z_1)\right\vert\leq n\delta_1$, and    $\left\vert\sum_{i=1}^n (b_i - z_2)\right\vert\leq n\delta_2$, then   $$\left\vert \sum_{i=1}^n (d_ib_i - z_1z_2) \right\vert \leq n(\delta_1^{1/2}+\delta_2^{1/2})^2. $$ This is where Rider is content to say ""done""; and it is hopefully clear that this is equivalent to my original question.","In trying to understand a result of D. Rider (Trans. AMS, 1973) I've got stuck on a lemma that he uses. At one point he makes a step without comment or explanation, but I can't see why it works. Here is a paraphrase of what I think he is claiming. Let $d_1,\dots, d_n$ be complex numbers of modulus $1$; let $b_1,\dots, b_n$ be complex numbers of modulus $\leq 1$. (In the intended application the numbers $b_1,\dots, b_n$ are actually the diagonal entries of a unitary $n\times n$ matrix, but I strongly suspect that is not used in the argument.) Then Rider seems to assert, without further comment, that $$ \left\vert\sum_{j=1}^n (d_jb_j-1)\right\vert^{1/2} \leq \left\vert\sum_{j=1}^n (d_j-1)\right\vert^{1/2} +  \left\vert\sum_{j=1}^n (b_j-1)\right\vert^{1/2} $$ Probably I am just being dense, and have failed to spot a routine estimate that does the job. If anyone could get me started on the right track that would be most helpful, as this is starting to get very irritating, and is not even the main part of Rider's argument. Update Firstly, thanks to everyone who answered, but in particular to Will Jagy for various off-MSE exchanges, and to George Lowther for his elegant argument (the key part that I had failed to think of, was the use of $\rm Re$ and its linearity rather than $\vert\cdot\vert$ and its sub-additivity, allowing one to boost up the observation made by Gerry Myerson). In case it's of interest, here are some more details on how the question I asked corresponds to Lemma 5.1 in Rider's paper. The paper is dealing with central trigonometric polynomials on compact groups, which means: linear combinations of traces of irreducible representations. Paraphrased lightly, and with a fairly trivial reduction step thrown in, the aforementioned lemma says the following: Let $\phi: G \to U(n)$ be an (irreducible) unitary representation of a (compact) group $G$. If $$\vert n^{-1}\operatorname{\rm Tr}\phi(g_i)-z_i\vert \leq \delta_i \qquad(i=1,\dots, p)$$   where $\vert z_i\vert=1$ for all $i$, then   $$ \left\vert n^{-1}\operatorname{Tr}\phi(g_1\dotsb g_n) - \prod_{i=1}^p z_i \right\vert \leq \left(\sum_{i=1}^p \delta_i^{1/2}\right)^2 $$ The proof given in the paper goes as follows: reduce to the case $p=2$ (if this can be done then induction will do the rest); then observe that WLOG $\phi(g_1)$ is a diagonal unitary matrix, w.r.t. an appropriate basis. If we let $d_1,\dots, d_n$ be the diagonal entries of $\phi(g_1)$ and $b_1,\dots, b_n$ be those of $\phi(g_2)$, then the trace of $\phi(g_1g_2)$ is just $\sum_{i=1}^n d_ib_i$, and so we have to prove Claim. If $|z_1|=|z_2|=1$, $\left\vert\sum_{i=1}^n (d_i - z_1)\right\vert\leq n\delta_1$, and    $\left\vert\sum_{i=1}^n (b_i - z_2)\right\vert\leq n\delta_2$, then   $$\left\vert \sum_{i=1}^n (d_ib_i - z_1z_2) \right\vert \leq n(\delta_1^{1/2}+\delta_2^{1/2})^2. $$ This is where Rider is content to say ""done""; and it is hopefully clear that this is equivalent to my original question.",,['real-analysis']
85,How to evaluate the limit of multifactorial $\lim_{n\to 0} \sqrt[n]{n!!!!\cdots !}$,How to evaluate the limit of multifactorial,\lim_{n\to 0} \sqrt[n]{n!!!!\cdots !},"It is well known that $\displaystyle \lim_{n\to \infty}\sqrt[n]{n!}=\infty$ , however, if we let $n\to 0$ we have a different result with a beautiful combination of $e$ and $\gamma$ , that is $$\lim_{n\to 0}\sqrt[n]{n!}= e^{-\gamma}\tag{1}\label{1}$$ To prove result \eqref{1}, we observe that the limit attains the form of $1^{\infty}$ so we can write it as $$\lim_{n\to 0} \exp\left(\frac{\ln\Gamma(n+1)}{n}\right)\underbrace{=}_{\text{L'Hopital's rule}}\lim_{n\to 0} e^{\Gamma'(n+1)}=e^{\psi_0(1)}= e^{-\gamma}$$ Now I wish to know the limit of the following  multifactorial form  for $k\in\mathbb {Z^+}$ $$\lim_{n\to 0}\sqrt[n]{n\smash[b]{\underbrace{!! !!\cdots !}_{k}}}={?}\tag{2}\label{2}\\$$ For $k=1$ we are done above and for $k=2$ we get the limit $\sqrt{2} e^{-\frac{\gamma}{2}}$ . To prove this we use the double factorial argument ( see equation (5) ) \begin{align}\lim_{n\to 0}\sqrt[n]{n!!}&=\lim_{n\to 0} \left(2^{\frac{n}{2}+\frac{1-\cos(\pi n)}{4}}\pi^{\frac{\cos(\pi n)-1}{4}}\Gamma\left(1+\frac{n}{2}\right)\right)^{\frac{1}{n}}\\&=\sqrt{2}\lim_{n\to 0} \sqrt[n]{\Gamma\left(1+\frac{n}{2}\right)}\\&=\sqrt 2\exp\lim_{n\to 0}\left(2^{-1} \Gamma\left(1+\frac{n}{2}\right)\psi_0\left(1+\frac{n}{2}\right)\right)\tag{L'Hopital's rule}\\&=\sqrt{2}e^{-\frac{\gamma}{2}}\end{align} since for $k=1,2$ we have evaluated the limit. How to evaluate the limit of equation \eqref{2} for all $k>2$ ?","It is well known that , however, if we let we have a different result with a beautiful combination of and , that is To prove result \eqref{1}, we observe that the limit attains the form of so we can write it as Now I wish to know the limit of the following  multifactorial form  for For we are done above and for we get the limit . To prove this we use the double factorial argument ( see equation (5) ) since for we have evaluated the limit. How to evaluate the limit of equation \eqref{2} for all ?","\displaystyle \lim_{n\to \infty}\sqrt[n]{n!}=\infty n\to 0 e \gamma \lim_{n\to 0}\sqrt[n]{n!}= e^{-\gamma}\tag{1}\label{1} 1^{\infty} \lim_{n\to 0} \exp\left(\frac{\ln\Gamma(n+1)}{n}\right)\underbrace{=}_{\text{L'Hopital's rule}}\lim_{n\to 0} e^{\Gamma'(n+1)}=e^{\psi_0(1)}= e^{-\gamma} k\in\mathbb {Z^+} \lim_{n\to 0}\sqrt[n]{n\smash[b]{\underbrace{!! !!\cdots !}_{k}}}={?}\tag{2}\label{2}\\ k=1 k=2 \sqrt{2} e^{-\frac{\gamma}{2}} \begin{align}\lim_{n\to 0}\sqrt[n]{n!!}&=\lim_{n\to 0} \left(2^{\frac{n}{2}+\frac{1-\cos(\pi n)}{4}}\pi^{\frac{\cos(\pi n)-1}{4}}\Gamma\left(1+\frac{n}{2}\right)\right)^{\frac{1}{n}}\\&=\sqrt{2}\lim_{n\to 0} \sqrt[n]{\Gamma\left(1+\frac{n}{2}\right)}\\&=\sqrt 2\exp\lim_{n\to 0}\left(2^{-1} \Gamma\left(1+\frac{n}{2}\right)\psi_0\left(1+\frac{n}{2}\right)\right)\tag{L'Hopital's rule}\\&=\sqrt{2}e^{-\frac{\gamma}{2}}\end{align} k=1,2 k>2","['real-analysis', 'sequences-and-series', 'limits', 'factorial', 'gamma-function']"
86,"If $p\in\Bbb Z[X]$ show that: $ \max\limits_{x\in [0,1]}\left|p(x) \right| > \frac{1}{e^{n}}. $",If  show that:,"p\in\Bbb Z[X]  \max\limits_{x\in [0,1]}\left|p(x) \right| > \frac{1}{e^{n}}. ","This is problem 10 from the International Mathematical Competition for University Students of 2015, from day 2, in Bulgaria. I think it is an interesting problem! Let $n$ be a positive integer, and $p(x)$ be a polynomial of degree $n$ with integer coefficients. Prove that   $$ \max_{x\in [0,1]}\left|p(x) \right| > \frac{1}{e^{n}}. $$ Proposed by Géza Kós, Eötvös University, Budapest.","This is problem 10 from the International Mathematical Competition for University Students of 2015, from day 2, in Bulgaria. I think it is an interesting problem! Let $n$ be a positive integer, and $p(x)$ be a polynomial of degree $n$ with integer coefficients. Prove that   $$ \max_{x\in [0,1]}\left|p(x) \right| > \frac{1}{e^{n}}. $$ Proposed by Géza Kós, Eötvös University, Budapest.",,"['calculus', 'real-analysis', 'number-theory', 'analysis', 'contest-math']"
87,Does there exist a space filling curve which sends every convex set to a convex set?,Does there exist a space filling curve which sends every convex set to a convex set?,,"Does there exist a surjective continuous function $f:[0,1]\to [0,1]^2$ which maps every convex set to a convex set? Such a function could be considered an especially ""regular"" sort of space-filling curve.  There are of course many well-known examples of continuous surjections $[0,1]\to[0,1]^2$ such as the Peano curve but none of them seem to map convex sets to convex sets.","Does there exist a surjective continuous function which maps every convex set to a convex set? Such a function could be considered an especially ""regular"" sort of space-filling curve.  There are of course many well-known examples of continuous surjections such as the Peano curve but none of them seem to map convex sets to convex sets.","f:[0,1]\to [0,1]^2 [0,1]\to[0,1]^2","['real-analysis', 'general-topology']"
88,Prove a strong inequality $\sum_{k=1}^n\frac{k}{a_1+a_2+\cdots+a_k}\le\left(2-\frac{7\ln 2}{8\ln n}\right)\sum_{k=1}^n\frac 1{a_k}$,Prove a strong inequality,\sum_{k=1}^n\frac{k}{a_1+a_2+\cdots+a_k}\le\left(2-\frac{7\ln 2}{8\ln n}\right)\sum_{k=1}^n\frac 1{a_k},"For $a_i>0$ ( $i=1,2,\dots,n$ ), $n\ge 3$ , prove that $$\sum_{k=1}^n\frac{k}{a_1+a_2+\cdots+a_k}\le\left(2\color{red}{-\frac{7\ln 2}{8\ln n}}\right)\sum_{k=1}^n\frac 1{a_k}.$$ The case without $\color{red}{-\dfrac{7\ln 2}{8\ln n}}$ could be shown here . I have no idea how the $\color{red}{\text{red}}$ term comes from. Note: This question should not be closed although there was a duplicated question $4$ years ago (see here ). Duplicate of unanswered question suggests that if there is no accepted answer in the old question, the new question can stay open in the hope of attracting an answer. The question comes from the Chinese Mathematical Olympiad training team and there is no answer provided. Source: See Q.25 here (one of the official accounts that provides Chinese MO questions on January $23^{\rm rd}$ , $2018$ ) It has also appeared here (A blog from the person who set this question on December $17^{\rm th}$ , $2013$ ).","For ( ), , prove that The case without could be shown here . I have no idea how the term comes from. Note: This question should not be closed although there was a duplicated question years ago (see here ). Duplicate of unanswered question suggests that if there is no accepted answer in the old question, the new question can stay open in the hope of attracting an answer. The question comes from the Chinese Mathematical Olympiad training team and there is no answer provided. Source: See Q.25 here (one of the official accounts that provides Chinese MO questions on January , ) It has also appeared here (A blog from the person who set this question on December , ).","a_i>0 i=1,2,\dots,n n\ge 3 \sum_{k=1}^n\frac{k}{a_1+a_2+\cdots+a_k}\le\left(2\color{red}{-\frac{7\ln 2}{8\ln n}}\right)\sum_{k=1}^n\frac 1{a_k}. \color{red}{-\dfrac{7\ln 2}{8\ln n}} \color{red}{\text{red}} 4 23^{\rm rd} 2018 17^{\rm th} 2013","['real-analysis', 'inequality', 'summation', 'logarithms', 'contest-math']"
89,Is this sequence bounded or unbounded?,Is this sequence bounded or unbounded?,,"Let $\{ a_n \}_{n=0}^\infty$ be the sequence given by $a_0 = 3$ and $$a_{n+1} = a_n - \frac{1}{a_n}, \quad n\ge 0.$$ (you can easily check that the sequence is well defined). Question : Is this sequence bounded or not? I was only able to show that there are infinitely many $n \geq 0$ such that $a_n > 0$ and infinitely many $n \geq 0$ such that $a_n < 0$ in this way: suppose for example that we have eventually $a_n > 0$ . But then $a_n$ is eventually strictly decreasing and so, using the fact that eventually $a_n > 0$ together with the monotone convergence theorem for sequences we get that $a_n$ is convergent, which is absurd because this would imply, together with the fact that $a_{n+1} a_n = a_n^2 - 1$ for all $n$ , that it's limit $L \in \mathbb{R}$ satisfies the equation $L^2 = L^2 - 1$ . Using the same reasoning you can show that $a_n < 0$ eventually leads to a contradiction. I think the boundedness question is much harder to understand because this sequence behaves in a very chaotic way.","Let be the sequence given by and (you can easily check that the sequence is well defined). Question : Is this sequence bounded or not? I was only able to show that there are infinitely many such that and infinitely many such that in this way: suppose for example that we have eventually . But then is eventually strictly decreasing and so, using the fact that eventually together with the monotone convergence theorem for sequences we get that is convergent, which is absurd because this would imply, together with the fact that for all , that it's limit satisfies the equation . Using the same reasoning you can show that eventually leads to a contradiction. I think the boundedness question is much harder to understand because this sequence behaves in a very chaotic way.","\{ a_n \}_{n=0}^\infty a_0 = 3 a_{n+1} = a_n - \frac{1}{a_n}, \quad n\ge 0. n \geq 0 a_n > 0 n \geq 0 a_n < 0 a_n > 0 a_n a_n > 0 a_n a_{n+1} a_n = a_n^2 - 1 n L \in \mathbb{R} L^2 = L^2 - 1 a_n < 0","['real-analysis', 'calculus', 'sequences-and-series', 'real-numbers']"
90,The Modulus of all the roots of a Polynomial are equal to $1$,The Modulus of all the roots of a Polynomial are equal to,1,"Suppose the real number $\lambda \in (0,1)$, and let $n$ be a positive integer. Prove that all roots of the polynomial $$f\left ( x \right )=\sum_{k=0}^{n}\binom{n}{k}\lambda^{k\left ( n-k \right )}x^{k}$$ have modulus equal to $1.$ The Putnam problem 2014 B4 is similar:  Show that for each positive integer $n,$ all the roots of the polynomial $\sum_{k=0}^n 2^{k(n-k)}x^k$ are real numbers.","Suppose the real number $\lambda \in (0,1)$, and let $n$ be a positive integer. Prove that all roots of the polynomial $$f\left ( x \right )=\sum_{k=0}^{n}\binom{n}{k}\lambda^{k\left ( n-k \right )}x^{k}$$ have modulus equal to $1.$ The Putnam problem 2014 B4 is similar:  Show that for each positive integer $n,$ all the roots of the polynomial $\sum_{k=0}^n 2^{k(n-k)}x^k$ are real numbers.",,"['calculus', 'real-analysis', 'polynomials', 'roots']"
91,Towards a new proof of infinitude of primes ( with possible unified application to other primes of special forms whose Infinitude is unknown): [closed],Towards a new proof of infinitude of primes ( with possible unified application to other primes of special forms whose Infinitude is unknown): [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . The community reviewed whether to reopen this question 12 months ago and left it closed: Original close reason(s) were not resolved Improve this question I'm trying to analyse the primes with the following point of view Consider the following partial sum : $$S(p)=\sum_{n=2}^p\sin^2\left(\frac{π\Gamma(n)}{2n}\right)$$ The summand is zero for non-primes greater than 5 , and finite and non-decreasing for primes (see Connes paper on Wilson's theorem ) I treated this sum with Finite version Abel-Plana Summation Formula (APSF) (as in Olver's book ""Asymptotics and special functions"" ) \begin{align}f(x) = {} & \sin^2\left(\frac{π\Gamma(x)}{2x}\right)\\ \sum_{k=2}^p f(k)= {} & \frac{f(2) +f(p)}2 + \int_2^p f(x) \, dx \\ & {}+ i\int_0^∞\frac{f(2+iy) − f(2−iy)}{e^{2πy }− 1} \, dy +i \int_0^∞\frac{f(p-iy) − f(p+iy)}{e^{2πy }− 1} \, dy \end{align} Here the first integral $\int_2^p f(x) \,dx$ is okay (highly oscillatory but we can do something: at least numerically) ( numerical analysts are welcome to provide  graphs of this for large ( at least $10^4$ ) $p$ . I am unable to do so in Mathematica.) See on computational science SE $$i \int_0^∞\frac{f(p-iy) − f(p+iy)}{e^{2πy }− 1} \,dy$$ This integral is very tricky, I tried to get growth condition , upper and lower bounds on it but in vain. Then, I tried to attach a weight such that: $$F(z) = \omega(z)\sin^2\left(\frac{π\Gamma(z)}{2z}\right)$$ Here, $\omega(z)$ is a weight we have to construct . The following three condition should hold for $\omega(z)$ : $$\omega(z)>\frac{1}{z},\ \forall z\in\mathbf{R}$$ ( More generally this condition is added for divergence of $\int_0^\infty F(x)dx$ So , $\omega(z)$ can even be complex valued for real domain as long as the given integral is divergent ) $$\lim_{ y→∞}|F(x ± iy)|e^{−2πy }= 0$$ $$\int_0^\infty |F(x + iy) − F(x − iy)|e^{−2πy} \,dy<+\infty$$ for every $x≥1$ and tends to zero as $x\to\infty$ . This is to eliminate the tricky 2nd integral in the formula. I can't find a weight that satisfies this; nor do I know if it is even possible to find one (!?) (1) Is there any other way to eliminate this second integral as $p\rightarrow\infty$ ? (2) Is there a better summation technique to analyse this (type of) problem? (3) Can we twist (the frequency part of)/change $F(x)$ to make the second integral more sane? (i.e. to make the magnitude of $f$ 's imaginary part satisfy condition 3) Note : I know these type of trig primality-tests are not practical but this one interests me so .....( interest is due to the fact that $\Gamma$ is nicely analytic). Also this could provide a new insights to deal with primes ( if it's workable at all). If argument can't work please explain why(?). UPDATE : Instead of finding the weight ; I considered $\sin^2$ term as function of some other function such that: Construct a generalized function such that: $$ F_*(z, s) = \dfrac{\phi(\sin^2[π\Gamma(z)/(2z)])}{z^s} $$ (1) $\phi(x) =0$ if $x$ is zero ; and 'suitably' finite otherwise (Here , 'suitably' means a value which guarantees the expected divergence of sum (very close to 1 or greater than or equal to 1) ) (2) condition (3) holds for such function A very 'close' example : $$ F(z, s) = \dfrac{\sinh(\sin^2[π\Gamma(z)/(2z)])}{z^s} $$ Let us restrict $s\in[0,1]$ Hence , Now , $$ I(x,s) =\int_0^\infty\mathrm dy \frac{F(x + \mathrm iy, s) − F(x −\mathrm iy, s)}{\mathrm e^{2πy}-1}, $$ Questions remain: Can we get 'sharp' numerical asymptotic of $I(x)$ as as $x\rightarrow \infty$ ? Also, can we get quantitative upper and lower bound estimations on the functional ? Also, other possible candidates for $\phi$ ? And henceforth the above analogous analysis as above ? See: https://mathoverflow.net/q/354962/145581 Related Question : Infinitude of primes using series introduced in Connes' paper on Wilson's theorem Possible Unified Applications: We can apply it to other primes of special forms whose Infinitude is unknown. (as Γ is nicely analytic). $$S_2(p)=\sum_{n=2}^p\sin^2\left(\frac{π\Gamma(n)}{2n}\right)\sin^2\left(\frac{π\Gamma(n+2)}{2(n+2)}\right)$$ For more details see : On a growth condition satisfied by given functional : Also in this post there is list of values of functional for small X's . Any Comments from numerical methods- experts are welcome (numerical analysis of first and second integral for  large (at least $10^2$ ) values of respective variable ): See related numerical-method based question  : Comparison of integrals with a function (at least numerically): One sentence summary question: How to get hold of second integral ( growth conditions, roots and other properties ) so that we can use it for our purpose? Or How to get rid of second integral using a weight or composite function method described above ? I modified second integral to various forms to make it workable with given conditions . But if you have a version that works with given conditions as mentioned please add and explain . As one can see I have various doubt about this approach. But I need some expert comments with technical details why this approach is less likely workable.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . The community reviewed whether to reopen this question 12 months ago and left it closed: Original close reason(s) were not resolved Improve this question I'm trying to analyse the primes with the following point of view Consider the following partial sum : The summand is zero for non-primes greater than 5 , and finite and non-decreasing for primes (see Connes paper on Wilson's theorem ) I treated this sum with Finite version Abel-Plana Summation Formula (APSF) (as in Olver's book ""Asymptotics and special functions"" ) Here the first integral is okay (highly oscillatory but we can do something: at least numerically) ( numerical analysts are welcome to provide  graphs of this for large ( at least ) . I am unable to do so in Mathematica.) See on computational science SE This integral is very tricky, I tried to get growth condition , upper and lower bounds on it but in vain. Then, I tried to attach a weight such that: Here, is a weight we have to construct . The following three condition should hold for : ( More generally this condition is added for divergence of So , can even be complex valued for real domain as long as the given integral is divergent ) for every and tends to zero as . This is to eliminate the tricky 2nd integral in the formula. I can't find a weight that satisfies this; nor do I know if it is even possible to find one (!?) (1) Is there any other way to eliminate this second integral as ? (2) Is there a better summation technique to analyse this (type of) problem? (3) Can we twist (the frequency part of)/change to make the second integral more sane? (i.e. to make the magnitude of 's imaginary part satisfy condition 3) Note : I know these type of trig primality-tests are not practical but this one interests me so .....( interest is due to the fact that is nicely analytic). Also this could provide a new insights to deal with primes ( if it's workable at all). If argument can't work please explain why(?). UPDATE : Instead of finding the weight ; I considered term as function of some other function such that: Construct a generalized function such that: (1) if is zero ; and 'suitably' finite otherwise (Here , 'suitably' means a value which guarantees the expected divergence of sum (very close to 1 or greater than or equal to 1) ) (2) condition (3) holds for such function A very 'close' example : Let us restrict Hence , Now , Questions remain: Can we get 'sharp' numerical asymptotic of as as ? Also, can we get quantitative upper and lower bound estimations on the functional ? Also, other possible candidates for ? And henceforth the above analogous analysis as above ? See: https://mathoverflow.net/q/354962/145581 Related Question : Infinitude of primes using series introduced in Connes' paper on Wilson's theorem Possible Unified Applications: We can apply it to other primes of special forms whose Infinitude is unknown. (as Γ is nicely analytic). For more details see : On a growth condition satisfied by given functional : Also in this post there is list of values of functional for small X's . Any Comments from numerical methods- experts are welcome (numerical analysis of first and second integral for  large (at least ) values of respective variable ): See related numerical-method based question  : Comparison of integrals with a function (at least numerically): One sentence summary question: How to get hold of second integral ( growth conditions, roots and other properties ) so that we can use it for our purpose? Or How to get rid of second integral using a weight or composite function method described above ? I modified second integral to various forms to make it workable with given conditions . But if you have a version that works with given conditions as mentioned please add and explain . As one can see I have various doubt about this approach. But I need some expert comments with technical details why this approach is less likely workable.","S(p)=\sum_{n=2}^p\sin^2\left(\frac{π\Gamma(n)}{2n}\right) \begin{align}f(x) = {} & \sin^2\left(\frac{π\Gamma(x)}{2x}\right)\\
\sum_{k=2}^p f(k)= {} & \frac{f(2) +f(p)}2 + \int_2^p f(x) \, dx \\ & {}+ i\int_0^∞\frac{f(2+iy) − f(2−iy)}{e^{2πy }− 1} \, dy +i \int_0^∞\frac{f(p-iy) − f(p+iy)}{e^{2πy }− 1} \, dy
\end{align} \int_2^p f(x) \,dx 10^4 p i \int_0^∞\frac{f(p-iy) − f(p+iy)}{e^{2πy }− 1} \,dy F(z) = \omega(z)\sin^2\left(\frac{π\Gamma(z)}{2z}\right) \omega(z) \omega(z) \omega(z)>\frac{1}{z},\ \forall z\in\mathbf{R} \int_0^\infty F(x)dx \omega(z) \lim_{ y→∞}|F(x ± iy)|e^{−2πy }= 0 \int_0^\infty |F(x + iy) − F(x − iy)|e^{−2πy} \,dy<+\infty x≥1 x\to\infty p\rightarrow\infty F(x) f \Gamma \sin^2  F_*(z, s) = \dfrac{\phi(\sin^2[π\Gamma(z)/(2z)])}{z^s}  \phi(x) =0 x  F(z, s) = \dfrac{\sinh(\sin^2[π\Gamma(z)/(2z)])}{z^s}  s\in[0,1] 
I(x,s) =\int_0^\infty\mathrm dy \frac{F(x + \mathrm iy, s) − F(x −\mathrm iy, s)}{\mathrm e^{2πy}-1},
 I(x) x\rightarrow \infty \phi S_2(p)=\sum_{n=2}^p\sin^2\left(\frac{π\Gamma(n)}{2n}\right)\sin^2\left(\frac{π\Gamma(n+2)}{2(n+2)}\right) 10^2","['real-analysis', 'complex-analysis', 'prime-numbers', 'analytic-number-theory', 'summation-method']"
92,Proof Verification - Every sequence in $\Bbb R$ contains a monotone sub-sequence,Proof Verification - Every sequence in  contains a monotone sub-sequence,\Bbb R,"Came across the following exercise in Bartle 's Elements of Real Analysis . This is the solution I came up with. Would be grateful if someone could verify it for me and maybe suggest better/alternate solutions. I also looked up these related questions - (1) , (2) , (3) - but was not happy with proofs given there. I seem to need some help understanding these. Any such help is appreciated. Show that every sequence in $\Bbb R$ either has a monotone increasing sub-sequence or a monotone decreasing sub-sequence. Let $(x_n)$ be a sequence in $\Bbb R$. Suppose $(x_n)$ is not bounded. Without loss of generality we may assume that $(x_n)$ is not bounded above. Therefore given any real number there is a member of the sequence which is greater. Let $x_{n_1}$ be any member of the sequence. There is $x_{n_2} \gt \sup\{x_1, x_2, ..., x_{n_1} \}$. For $i \gt 1$ let $x_{n_i} = \{x_1, x_2, ..., x_{n_{i - 1}}\}$ then $(x_{n_k})$ forms a monotone subsequence of $(x_n)$. Now suppose instead that $(x_n)$ is bounded. By the Bolzano-Weierstrass Theorem there is a subsequence $(y_n)$ of $(x_n)$ which converges to a limit $y$. Without loss of generality there are infinitely many distinct values in $(y_n)$ that are unequal to $y$. Let $y_{k1}$ be the first such element. Let $y_{k2}$ be any element in $\{ y' \in (y_n) \ \ | \ \ |y' - y | \lt |y - y_{k1}|  \}$. For $i \gt 1$ let $y_{ki} \in \{ y' \in (y_n)  \ \ | \ \ |y' - y | \lt |y - y_{k \ i - 1}|  \}$. Such $y_{ki}$ exists for every $i \in  \Bbb N$ since $ \lim (y_n) = y $. Now let $(y_{kn})$ be the sub-sequence of $(y_n)$ thus formed. At least one of the two following sets must contain infinitely many elements. $\{ y \in (y_{kn}) \ \ | \ \ y \gt x\}$ $\{ y \in (y_{kn}) \ \ | \ \ y \lt x\}$ The one which does forms a monotone subsequence.","Came across the following exercise in Bartle 's Elements of Real Analysis . This is the solution I came up with. Would be grateful if someone could verify it for me and maybe suggest better/alternate solutions. I also looked up these related questions - (1) , (2) , (3) - but was not happy with proofs given there. I seem to need some help understanding these. Any such help is appreciated. Show that every sequence in $\Bbb R$ either has a monotone increasing sub-sequence or a monotone decreasing sub-sequence. Let $(x_n)$ be a sequence in $\Bbb R$. Suppose $(x_n)$ is not bounded. Without loss of generality we may assume that $(x_n)$ is not bounded above. Therefore given any real number there is a member of the sequence which is greater. Let $x_{n_1}$ be any member of the sequence. There is $x_{n_2} \gt \sup\{x_1, x_2, ..., x_{n_1} \}$. For $i \gt 1$ let $x_{n_i} = \{x_1, x_2, ..., x_{n_{i - 1}}\}$ then $(x_{n_k})$ forms a monotone subsequence of $(x_n)$. Now suppose instead that $(x_n)$ is bounded. By the Bolzano-Weierstrass Theorem there is a subsequence $(y_n)$ of $(x_n)$ which converges to a limit $y$. Without loss of generality there are infinitely many distinct values in $(y_n)$ that are unequal to $y$. Let $y_{k1}$ be the first such element. Let $y_{k2}$ be any element in $\{ y' \in (y_n) \ \ | \ \ |y' - y | \lt |y - y_{k1}|  \}$. For $i \gt 1$ let $y_{ki} \in \{ y' \in (y_n)  \ \ | \ \ |y' - y | \lt |y - y_{k \ i - 1}|  \}$. Such $y_{ki}$ exists for every $i \in  \Bbb N$ since $ \lim (y_n) = y $. Now let $(y_{kn})$ be the sub-sequence of $(y_n)$ thus formed. At least one of the two following sets must contain infinitely many elements. $\{ y \in (y_{kn}) \ \ | \ \ y \gt x\}$ $\{ y \in (y_{kn}) \ \ | \ \ y \lt x\}$ The one which does forms a monotone subsequence.",,"['real-analysis', 'sequences-and-series', 'proof-verification', 'self-learning']"
93,Can we use symmetry rules in improper integrals?,Can we use symmetry rules in improper integrals?,,"I wish to evaluate the integral $$I=\int^{\infty}_{-\infty}xe^{-x^2}dx$$ Can I simply note that that $f(x)=xe^{-x^2}$ is an odd function and say $I=0$? The only reason I have doubts is because of assuming the two infinities have the same length. However, when I hear people say, ""...the integral of an odd function vanishes on $\mathbb{R}$,"" it tempts me to accept the symmetry argument. The actual answer via limits is $0$.","I wish to evaluate the integral $$I=\int^{\infty}_{-\infty}xe^{-x^2}dx$$ Can I simply note that that $f(x)=xe^{-x^2}$ is an odd function and say $I=0$? The only reason I have doubts is because of assuming the two infinities have the same length. However, when I hear people say, ""...the integral of an odd function vanishes on $\mathbb{R}$,"" it tempts me to accept the symmetry argument. The actual answer via limits is $0$.",,"['real-analysis', 'integration', 'improper-integrals', 'even-and-odd-functions']"
94,Confusion about Tao's construction of reals,Confusion about Tao's construction of reals,,"Background: I am currently studying real analysis using Tao's Analysis Volume One and so far I am really enjoying myself though I seem to have run into some confusion regarding professor Tao's construction of the reals using rationals. The following is the definition of reals that he provides in the text: $\DeclareMathOperator*{\LIM}{LIM}$ Definition 5.3.1 (Real numbers). A real number is defined to be an object of the form $\LIM\limits_{n → ∞} a_n$, where $(a_n)_{n = 1}^∞$ is a Cauchy sequence of rational numbers. Two real numbers $\LIM\limits_{n → ∞} a_n$ and $\LIM\limits_{n→∞} b_n$ are said to be equal iff $(a_n)_{n = 1}^∞$ and $(b_n)_{n = 1}^∞$ are equivalent Cauchy sequences. The set of all real numbers is denoted $\mathbb{R}$. Problem: While snooping around the internet I have found that a real number is in fact an equivalence class of sequences of rationals whose corresponding terms can be arbitrary close to each other i.e $(a_n)_{n=0}^{\infty}$ and $(b_n)_{n=0}^{\infty}$ are equivalent if and only if  $$\forall ε>0, \ \exists N \in \mathbb{N} \ \text{such that} \ \forall n \ge N, \ |a_n-b_n|\leq ε.$$ But Tao's definition seems to suggest that real numbers are limits of said sequences so what are they ?","Background: I am currently studying real analysis using Tao's Analysis Volume One and so far I am really enjoying myself though I seem to have run into some confusion regarding professor Tao's construction of the reals using rationals. The following is the definition of reals that he provides in the text: $\DeclareMathOperator*{\LIM}{LIM}$ Definition 5.3.1 (Real numbers). A real number is defined to be an object of the form $\LIM\limits_{n → ∞} a_n$, where $(a_n)_{n = 1}^∞$ is a Cauchy sequence of rational numbers. Two real numbers $\LIM\limits_{n → ∞} a_n$ and $\LIM\limits_{n→∞} b_n$ are said to be equal iff $(a_n)_{n = 1}^∞$ and $(b_n)_{n = 1}^∞$ are equivalent Cauchy sequences. The set of all real numbers is denoted $\mathbb{R}$. Problem: While snooping around the internet I have found that a real number is in fact an equivalence class of sequences of rationals whose corresponding terms can be arbitrary close to each other i.e $(a_n)_{n=0}^{\infty}$ and $(b_n)_{n=0}^{\infty}$ are equivalent if and only if  $$\forall ε>0, \ \exists N \in \mathbb{N} \ \text{such that} \ \forall n \ge N, \ |a_n-b_n|\leq ε.$$ But Tao's definition seems to suggest that real numbers are limits of said sequences so what are they ?",,"['real-analysis', 'definition', 'real-numbers']"
95,Examples of Axiom of Choice used in introductory-level undergradute math,Examples of Axiom of Choice used in introductory-level undergradute math,,All of the applications of AoC I've encountered have been in upper level undergraduate or graduate math courses. Are there any basic results from courses like Calc I-III which (unbeknownst to students) rely on AoC?,All of the applications of AoC I've encountered have been in upper level undergraduate or graduate math courses. Are there any basic results from courses like Calc I-III which (unbeknownst to students) rely on AoC?,,"['calculus', 'real-analysis', 'education', 'axiom-of-choice']"
96,Baby Rudin vs. Abbott,Baby Rudin vs. Abbott,,"I am considering Stephen Abbott's Understanding Analysis and Walter Rudin's Principles of Mathematical Analysis . I am looking for a comparison between the two that addresses both of the following matters, Is one of the two substantially more mathematically rigorous than the other? Does one include substantially more challenging problems? Which provides a better introduction to Real Analysis? Follow-up question: Would someone who has worked through Abbott's book be at a disadvantage compared to someone who has completed Rudin's text?","I am considering Stephen Abbott's Understanding Analysis and Walter Rudin's Principles of Mathematical Analysis . I am looking for a comparison between the two that addresses both of the following matters, Is one of the two substantially more mathematically rigorous than the other? Does one include substantially more challenging problems? Which provides a better introduction to Real Analysis? Follow-up question: Would someone who has worked through Abbott's book be at a disadvantage compared to someone who has completed Rudin's text?",,"['real-analysis', 'soft-question']"
97,How to prove that $\mathbb{Q}$ ( the rationals) is a countable set,How to prove that  ( the rationals) is a countable set,\mathbb{Q},"I want to prove that $\mathbb{Q}$ is countable. So basically, I could find a bijection from $\mathbb{Q}$ to $\mathbb{N}$. But I have also recently proved that $\mathbb{Z}$ is countable, so is it equivalent to find a bijection from $\mathbb{Q}$ to $\mathbb{Z}$?","I want to prove that $\mathbb{Q}$ is countable. So basically, I could find a bijection from $\mathbb{Q}$ to $\mathbb{N}$. But I have also recently proved that $\mathbb{Z}$ is countable, so is it equivalent to find a bijection from $\mathbb{Q}$ to $\mathbb{Z}$?",,"['real-analysis', 'elementary-set-theory']"
98,All real functions are continuous,All real functions are continuous,,"I've heard that within the field of intuitionistic mathematics, all real functions are continuous (i.e. there are no discontinuous functions). Is there a good book where I can find a proof of this theorem?","I've heard that within the field of intuitionistic mathematics, all real functions are continuous (i.e. there are no discontinuous functions). Is there a good book where I can find a proof of this theorem?",,"['real-analysis', 'logic', 'reference-request', 'intuitionistic-logic']"
99,Proof of $\int_0^\infty \frac{x^{\alpha}dx}{1+2x\cos\beta +x^{2}}=\frac{\pi\sin (\alpha\beta)}{\sin (\alpha\pi)\sin \beta }$,Proof of,\int_0^\infty \frac{x^{\alpha}dx}{1+2x\cos\beta +x^{2}}=\frac{\pi\sin (\alpha\beta)}{\sin (\alpha\pi)\sin \beta },I found a nice formula of the following integral here $$\int_0^\infty \frac{x^{\alpha}dx}{1+2x\cos\beta +x^{2}}=\frac{\pi\sin (\alpha\beta)}{\sin (\alpha\pi)\sin \beta }$$ It states there that it can be proved by using contours method which I do not understand. It seems that the RHS is Euler's reflection formula for the gamma function but I am not so sure. Could anyone here please help me how to obtain it preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.,I found a nice formula of the following integral here $$\int_0^\infty \frac{x^{\alpha}dx}{1+2x\cos\beta +x^{2}}=\frac{\pi\sin (\alpha\beta)}{\sin (\alpha\pi)\sin \beta }$$ It states there that it can be proved by using contours method which I do not understand. It seems that the RHS is Euler's reflection formula for the gamma function but I am not so sure. Could anyone here please help me how to obtain it preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.,,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
