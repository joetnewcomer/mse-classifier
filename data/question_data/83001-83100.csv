,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,The largest eigenvalue of a singular random matrix,The largest eigenvalue of a singular random matrix,,"Let $X$ be a $m \times n$ random matrix whose entries are i.i.d. random variables with mean $0$ and finite variance $\sigma^2$ . Let $$ Y_n =\frac{1}{n} X X^T.$$ Then according to the Marchenko-Pastur theorem, as $m, n \rightarrow \infty$ and $m/n \rightarrow c \in (0,\infty)$ the largest eigenvalue is bounded by $$ \lambda_{\max} =\sigma^2 \left (1+ \sqrt{\frac{m}{n}} \right )^2. $$ Now let's assume $n$ remains fixed while $m \rightarrow \infty $ . Can one formally show that the largest eigenvalue is not bounded and that its rate of increase is related to $m$ ?","Let be a random matrix whose entries are i.i.d. random variables with mean and finite variance . Let Then according to the Marchenko-Pastur theorem, as and the largest eigenvalue is bounded by Now let's assume remains fixed while . Can one formally show that the largest eigenvalue is not bounded and that its rate of increase is related to ?","X m \times n 0 \sigma^2  Y_n =\frac{1}{n} X X^T. m, n \rightarrow \infty m/n \rightarrow c \in (0,\infty)  \lambda_{\max} =\sigma^2 \left (1+ \sqrt{\frac{m}{n}} \right )^2.  n m \rightarrow \infty  m","['matrices', 'eigenvalues-eigenvectors', 'random-matrices', 'spectral-radius']"
1,Does this type of matrix with the same diagonals have a name?,Does this type of matrix with the same diagonals have a name?,,"Given real numbers $x_1, x_2, x_3, \dots, x_n$ , we define the real symmetric matrix $$ \begin{pmatrix}  \color{red}{x_1} &    \color{red}{x_1} & \color{red}{x_1} & \cdots & \color{red}{x_1} \\  \color{red}{x_1} &    \color{blue}{x_2} & \color{blue}{x_2} & \color{blue}{\cdots} & \color{blue}{x_2} \\ \color{red}{x_1} &    \color{blue}{x_2} & \color{green}{x_3} & \cdots & \color{green}{x_3} \\ \color{red}{\vdots} & \color{blue}{\vdots} &  \color{green}{\vdots} & \ddots & \\  \color{red}{x_1} & \color{blue}{x_2} & \color{green}{x_3} & \cdots & x_n   \end{pmatrix} $$ So this matrix has all diagonals of the form $x_1, x_2, x_3, \dots, x_n$ . Does this matrix have a name? If not, I am curious if anything can be said about its eigenvalues and eigenvectors.","Given real numbers , we define the real symmetric matrix So this matrix has all diagonals of the form . Does this matrix have a name? If not, I am curious if anything can be said about its eigenvalues and eigenvectors.","x_1, x_2, x_3, \dots, x_n 
\begin{pmatrix} 
\color{red}{x_1} &    \color{red}{x_1} & \color{red}{x_1} & \cdots & \color{red}{x_1} \\ 
\color{red}{x_1} &    \color{blue}{x_2} & \color{blue}{x_2} & \color{blue}{\cdots} & \color{blue}{x_2} \\
\color{red}{x_1} &    \color{blue}{x_2} & \color{green}{x_3} & \cdots & \color{green}{x_3} \\
\color{red}{\vdots} & \color{blue}{\vdots} &  \color{green}{\vdots} & \ddots & \\ 
\color{red}{x_1} & \color{blue}{x_2} & \color{green}{x_3} & \cdots & x_n  
\end{pmatrix}
 x_1, x_2, x_3, \dots, x_n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'terminology']"
2,How can I maximize an absorbing state outcome in a Markov Chain?,How can I maximize an absorbing state outcome in a Markov Chain?,,"I am working on Markov Chain for a computer science project. The problem is based on the gunshot in ""The Good The Bad and the Ugly"". Three shooters have a certain chance to hit their target and a certain chance to choose one of the other two shooters. So the Good has a chance g to hit his target, The Bad has a chance b and The Ugly a chance u . I want to show that each shooter should target the best gunslinger when they shoot to maximize their likelihood to survive. For this I introduce gu , the probability for G to choose U, bu the probability for B to choose U and ub the probability for U to choose B. I've built my Markov chain, I implemented my transition matrix, identified the fundamental matrix F and the Absorbtion matrix A. So I just have to look at the first column of A to know that to go from the initial state (GBU everyone is alive and G starts shooting ) to the absorbing states G,B,U I have a probability of respectively A11, A21 and A31. So my question is, how can I find a way to maximize those last three probabilities. I've considered setting g , b and u and find optimal values of gu , bu and ub but I don't see how I could do that. Should I consider A as a function of gu , bu and ub and find the critical points such that ∇A=0 ? But considering that A is a very long expression it seems very unpractical. https://i.sstatic.net/TXR6f.jpg (The Markov chain and the transition matrix). Thanks in advance for your help. Small update: the question accepted answered in a mathematical point of view to the question, however I am still looking for an approach using Markov chain and the absorbing matrix! So feel free to comment!","I am working on Markov Chain for a computer science project. The problem is based on the gunshot in ""The Good The Bad and the Ugly"". Three shooters have a certain chance to hit their target and a certain chance to choose one of the other two shooters. So the Good has a chance g to hit his target, The Bad has a chance b and The Ugly a chance u . I want to show that each shooter should target the best gunslinger when they shoot to maximize their likelihood to survive. For this I introduce gu , the probability for G to choose U, bu the probability for B to choose U and ub the probability for U to choose B. I've built my Markov chain, I implemented my transition matrix, identified the fundamental matrix F and the Absorbtion matrix A. So I just have to look at the first column of A to know that to go from the initial state (GBU everyone is alive and G starts shooting ) to the absorbing states G,B,U I have a probability of respectively A11, A21 and A31. So my question is, how can I find a way to maximize those last three probabilities. I've considered setting g , b and u and find optimal values of gu , bu and ub but I don't see how I could do that. Should I consider A as a function of gu , bu and ub and find the critical points such that ∇A=0 ? But considering that A is a very long expression it seems very unpractical. https://i.sstatic.net/TXR6f.jpg (The Markov chain and the transition matrix). Thanks in advance for your help. Small update: the question accepted answered in a mathematical point of view to the question, however I am still looking for an approach using Markov chain and the absorbing matrix! So feel free to comment!",,"['probability', 'matrices', 'markov-chains', 'markov-process']"
3,"Is the type $(1,1)$ Kronecker delta tensor, $\delta_a^{\,\,b}$ equal to the trace of the identity matrix or always $1$ when $a=b$ and zero otherwise?","Is the type  Kronecker delta tensor,  equal to the trace of the identity matrix or always  when  and zero otherwise?","(1,1) \delta_a^{\,\,b} 1 a=b","I'll ask this question using very simple examples working in flat cartesian space (just $2$ spatial dimensions). I'll be using the Einstein summation convention throughout this question, but since I'm very new to this I will explicitly write the summation symbol at times. According to this article on raising and lowering indices on Wikipedia the identity matrix , can be represented as a Kronecker delta metric tensor (of type $(0,2)$ ), $$\delta_{ij}=\begin{pmatrix}1&0\\0&1\end{pmatrix}=\begin{cases}1 & \text{if} \, i=j \\ 0 &\text{if}\, i\ne j\end{cases}\tag{A}$$ and its' inverse of type $(2,0)$ , $$\delta^{ij}=\left({\delta_{ij}}\right)^{-1}=\begin{pmatrix}1&0\\0&1\end{pmatrix}=\begin{cases}1 & \text{if} \, i=j \\ 0 &\text{if}\, i\ne j\end{cases}\tag{B}$$ are just the $2$ d identity matrices. But how does one interpret a type $(1,1)$ Kronecker tensor metric, $\delta_a^{\,\,b}$ ? Here are some examples to put this question into context. Suppose we have a matrix $A_{ij}=\begin{pmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{pmatrix}$ . Some of the following are correct expressions for $A_{ij}$ : $$\delta_{ik}A^{k}_{\,\,j}\tag{1}$$ $$\delta_{ik}\delta_{j\ell}A^{k\ell}\tag{2}$$ $$\delta_{k\ell}\delta^{k\ell}A^{ij}\tag{3}$$ $(1)$ is correct (equal to $A_{ij}$ ) as the dummy index $k$ in the matrix is 'lowered' using the metric. Then according to the definition, $(\mathrm{A})$ , $\delta_{ii}=1$ and $k$ is summed upon. $(2)$ is also correct as $$\delta_{ik}\delta_{j\ell}A^{k\ell}=\delta_{ik}A^k_{\,\,j}=A_{ij}$$ $(3)$ is not correct as $$\delta_{k\ell}\delta^{k\ell}A^{ij}=\delta_k^{\,\,k}A^{ij}=\sum_{k=1}^2 \delta_k^{\,\,k}A^{ij}=2A^{ij}\ne A_{ij}\tag{C}$$ In the first equality of $(\mathrm{C})$ , I think of this as 'raising' the second index of the first Kronecker metric in $\delta_k^{\,\,k}\delta^{kk}$ . Since for a non-zero contribution $\ell=k$ , which means $\delta^{kk}=1$ , which is fine as this is what equation $(\mathrm{A})$ is telling me to do. So from this it seems like the $\delta_k^{\,\,k}$ is the trace of the matrix given in eqn $(\mathrm{A})$ (or $(\mathrm{B})$ ). But I have another $2$ examples that seem to contradict this, suppose we have two column vectors, $U^i$ and $V^i$ along with their respective row vectors, $$U^i=\begin{pmatrix}u_1\\u_2\end{pmatrix}\,\,\text{so that}\,\,\,\,U_i=\delta_{ij}U^j=\begin{pmatrix}u_1&u_2\end{pmatrix}$$ $$V^i=\begin{pmatrix}v_1\\v_2\end{pmatrix}\,\,\text{so that}\,\,\,\,V_i=\delta_{ij}V^j=\begin{pmatrix}v_1&v_2\end{pmatrix}$$ Now suppose we want to compute the inner product, $U\cdot V$ . There are obviously many ways of doing this, and one could simply write $U\cdot V=U_iV^i$ , but I want to purposely use the Kronecker delta metric to make a point. Here are some possible expressions for the inner product, $U\cdot V$ : $$\delta_{ij}U^iV^j\tag{4}$$ $$V^j\delta_{j\ell}U^{\ell}\tag{5}$$ $$U^aV_b\delta^b_{\,\,a}\tag{6}$$ $$U_iV^a\delta_a^{\,\,b}\delta_b^{\,\,i}\tag{7}$$ $(4)$ is correct as $\delta_{ij}U^iV^j=U_i\delta_{ij}V^j$ , and the way I've understood this is that the $j$ index on the $V^j$ has been 'lowered' using the metric and the only non-zero contribution is when $i=j$ , so writing these steps out explicitly, $$U_i\delta_{ij}V^j=U_i\delta_{ii}V^i=U_iV^i$$ since $\delta_{ii}=1$ according to the prescription in $(\mathrm{A})$ . $(5)$ is also correct for the same reasons as $(4)$ . Now for eqn, $(6)$ this is where the problem starts for me, since I am not sure what $\delta^b_{\,\,a}$ actually means, I can only guess that $\delta^b_{\,\,a}$ is non-zero only when $a=b$ , so from this I conclude that $$U^aV_b\delta^b_{\,\,a}=U^aV_a\delta^a_{\,\,a}\stackrel{\color{red}{?}}{=}2U^aV_a\ne U^aV_a\tag{D}$$ For $(\mathrm{D})$ , since we are working in $2$ d flat Cartesian space, this $\delta_{\,\,a}^a=\sum_{a=1}^2\delta_{\,\,a}^a=2$ is the trace or sum of the diagonal elements of $(\mathrm{A})$ , (as $a$ is a dummy index and hence summed over). In a similar way, I think eqn. $(7)$ should be $$U_iV^a\delta_a^{\,\,b}\delta_b^{\,\,i}=U_iV^a\delta_a^{\,\,i}\delta_i^{\,\,i}$$ $$\stackrel{\color{red}{?}}{=}2U_iV^a\delta_a^{\,\,i}\stackrel{\color{red}{?}}{=}2U_iV^i\delta_i^{\,\,i}\stackrel{\color{red}{?}}{=}4U_iV^i\ne U_iV^i\tag{E}$$ In the first equality of $(\mathrm{E})$ there is a contraction of the $b$ index, where it get sets equal to $i$ since this is the only way to get a non-zero contribution out of the expression, I interpret this as the trace and that is where the factor of $2$ comes from in the second equality (marked with a red question mark above it). I have then done exactly the same thing for the third equality and by my logic there is another trace so there should be another factor of $2$ which is in the fourth equality (these equalities are marked with red question marks as I'm not sure if these statements are true). Now here is the problem, eqns $(4-7)$ are all correct expressions for the inner product, $U\cdot V$ . So, the factors of $2$ I have introduced should not be there. But the question is, why are these manipulations in $(6)$ and $(7)$ wrong, when eqn. $(\mathrm{C})$ used the trace as $\delta_k^{\,\,k}=2$ ?","I'll ask this question using very simple examples working in flat cartesian space (just spatial dimensions). I'll be using the Einstein summation convention throughout this question, but since I'm very new to this I will explicitly write the summation symbol at times. According to this article on raising and lowering indices on Wikipedia the identity matrix , can be represented as a Kronecker delta metric tensor (of type ), and its' inverse of type , are just the d identity matrices. But how does one interpret a type Kronecker tensor metric, ? Here are some examples to put this question into context. Suppose we have a matrix . Some of the following are correct expressions for : is correct (equal to ) as the dummy index in the matrix is 'lowered' using the metric. Then according to the definition, , and is summed upon. is also correct as is not correct as In the first equality of , I think of this as 'raising' the second index of the first Kronecker metric in . Since for a non-zero contribution , which means , which is fine as this is what equation is telling me to do. So from this it seems like the is the trace of the matrix given in eqn (or ). But I have another examples that seem to contradict this, suppose we have two column vectors, and along with their respective row vectors, Now suppose we want to compute the inner product, . There are obviously many ways of doing this, and one could simply write , but I want to purposely use the Kronecker delta metric to make a point. Here are some possible expressions for the inner product, : is correct as , and the way I've understood this is that the index on the has been 'lowered' using the metric and the only non-zero contribution is when , so writing these steps out explicitly, since according to the prescription in . is also correct for the same reasons as . Now for eqn, this is where the problem starts for me, since I am not sure what actually means, I can only guess that is non-zero only when , so from this I conclude that For , since we are working in d flat Cartesian space, this is the trace or sum of the diagonal elements of , (as is a dummy index and hence summed over). In a similar way, I think eqn. should be In the first equality of there is a contraction of the index, where it get sets equal to since this is the only way to get a non-zero contribution out of the expression, I interpret this as the trace and that is where the factor of comes from in the second equality (marked with a red question mark above it). I have then done exactly the same thing for the third equality and by my logic there is another trace so there should be another factor of which is in the fourth equality (these equalities are marked with red question marks as I'm not sure if these statements are true). Now here is the problem, eqns are all correct expressions for the inner product, . So, the factors of I have introduced should not be there. But the question is, why are these manipulations in and wrong, when eqn. used the trace as ?","2 (0,2) \delta_{ij}=\begin{pmatrix}1&0\\0&1\end{pmatrix}=\begin{cases}1 & \text{if} \, i=j \\ 0 &\text{if}\, i\ne j\end{cases}\tag{A} (2,0) \delta^{ij}=\left({\delta_{ij}}\right)^{-1}=\begin{pmatrix}1&0\\0&1\end{pmatrix}=\begin{cases}1 & \text{if} \, i=j \\ 0 &\text{if}\, i\ne j\end{cases}\tag{B} 2 (1,1) \delta_a^{\,\,b} A_{ij}=\begin{pmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{pmatrix} A_{ij} \delta_{ik}A^{k}_{\,\,j}\tag{1} \delta_{ik}\delta_{j\ell}A^{k\ell}\tag{2} \delta_{k\ell}\delta^{k\ell}A^{ij}\tag{3} (1) A_{ij} k (\mathrm{A}) \delta_{ii}=1 k (2) \delta_{ik}\delta_{j\ell}A^{k\ell}=\delta_{ik}A^k_{\,\,j}=A_{ij} (3) \delta_{k\ell}\delta^{k\ell}A^{ij}=\delta_k^{\,\,k}A^{ij}=\sum_{k=1}^2 \delta_k^{\,\,k}A^{ij}=2A^{ij}\ne A_{ij}\tag{C} (\mathrm{C}) \delta_k^{\,\,k}\delta^{kk} \ell=k \delta^{kk}=1 (\mathrm{A}) \delta_k^{\,\,k} (\mathrm{A}) (\mathrm{B}) 2 U^i V^i U^i=\begin{pmatrix}u_1\\u_2\end{pmatrix}\,\,\text{so that}\,\,\,\,U_i=\delta_{ij}U^j=\begin{pmatrix}u_1&u_2\end{pmatrix} V^i=\begin{pmatrix}v_1\\v_2\end{pmatrix}\,\,\text{so that}\,\,\,\,V_i=\delta_{ij}V^j=\begin{pmatrix}v_1&v_2\end{pmatrix} U\cdot V U\cdot V=U_iV^i U\cdot V \delta_{ij}U^iV^j\tag{4} V^j\delta_{j\ell}U^{\ell}\tag{5} U^aV_b\delta^b_{\,\,a}\tag{6} U_iV^a\delta_a^{\,\,b}\delta_b^{\,\,i}\tag{7} (4) \delta_{ij}U^iV^j=U_i\delta_{ij}V^j j V^j i=j U_i\delta_{ij}V^j=U_i\delta_{ii}V^i=U_iV^i \delta_{ii}=1 (\mathrm{A}) (5) (4) (6) \delta^b_{\,\,a} \delta^b_{\,\,a} a=b U^aV_b\delta^b_{\,\,a}=U^aV_a\delta^a_{\,\,a}\stackrel{\color{red}{?}}{=}2U^aV_a\ne U^aV_a\tag{D} (\mathrm{D}) 2 \delta_{\,\,a}^a=\sum_{a=1}^2\delta_{\,\,a}^a=2 (\mathrm{A}) a (7) U_iV^a\delta_a^{\,\,b}\delta_b^{\,\,i}=U_iV^a\delta_a^{\,\,i}\delta_i^{\,\,i} \stackrel{\color{red}{?}}{=}2U_iV^a\delta_a^{\,\,i}\stackrel{\color{red}{?}}{=}2U_iV^i\delta_i^{\,\,i}\stackrel{\color{red}{?}}{=}4U_iV^i\ne U_iV^i\tag{E} (\mathrm{E}) b i 2 2 (4-7) U\cdot V 2 (6) (7) (\mathrm{C}) \delta_k^{\,\,k}=2","['matrices', 'inner-products', 'tensors', 'kronecker-delta']"
4,Understanding Representation theory and group actions,Understanding Representation theory and group actions,,"This might be a silly question, but I need help understanding an example of a representation of a group ,the following example is from Yvette Kosmann-Scwartbach's book called Groups and Symmetries: ''Let $t\in S_3 $ be the transposition $123\to 132$ and $c$ the cyclic permutation $123\to 231$ that generates $S_3$ .We set $j=e^{2i\pi/3}$ ,so that $j^2+j+1=0$ .We can represent $S_3$ on $\mathbb{C}^2 $ by defining $ρ(e)=I_2$ , $ρ(t)=\begin{pmatrix} 0 & 1 \\ 1 & 0 \\ \end{pmatrix}$ and $ρ(c)=\begin{pmatrix} j & 0 \\ 0 & j^2 \\ \end{pmatrix}$ ''. My understanding is that $ρ_g , (ρ_g=ρ(g))$ is defined by the action of $S_3$ on $\mathbb{C^2}$ and so for $ρ(e)=I_2$ we get $ρ(e)=[a_{e(1)},a_{e(2)}]$ ,where $e$ the ''neutral'' permutation and $a=\{a_1,a_2\}=\{(1,0),(0,1)\}$ a basis of $\mathbb{C^2}$ and thus $ρ(e)=[a_1,a_2]=I_2 $ ,since $e:123 \to 123$ . That kind of logic doens't seem to work on the rest.What am I missing and how does the author introduce $j$ on the matrices? Thank you in advance !","This might be a silly question, but I need help understanding an example of a representation of a group ,the following example is from Yvette Kosmann-Scwartbach's book called Groups and Symmetries: ''Let be the transposition and the cyclic permutation that generates .We set ,so that .We can represent on by defining , and ''. My understanding is that is defined by the action of on and so for we get ,where the ''neutral'' permutation and a basis of and thus ,since . That kind of logic doens't seem to work on the rest.What am I missing and how does the author introduce on the matrices? Thank you in advance !","t\in S_3  123\to 132 c 123\to 231 S_3 j=e^{2i\pi/3} j^2+j+1=0 S_3 \mathbb{C}^2  ρ(e)=I_2 ρ(t)=\begin{pmatrix}
0 & 1 \\
1 & 0 \\
\end{pmatrix} ρ(c)=\begin{pmatrix}
j & 0 \\
0 & j^2 \\
\end{pmatrix} ρ_g , (ρ_g=ρ(g)) S_3 \mathbb{C^2} ρ(e)=I_2 ρ(e)=[a_{e(1)},a_{e(2)}] e a=\{a_1,a_2\}=\{(1,0),(0,1)\} \mathbb{C^2} ρ(e)=[a_1,a_2]=I_2  e:123 \to 123 j","['matrices', 'group-theory', 'representation-theory', 'group-actions']"
5,"Showing that $H_n G_n \overset{d}{\rightarrow} \mathcal{N}(0,I)$ if $H_n H_n' = I$ and $G_n \overset{d}{\rightarrow} \mathcal{N}(0,I)$",Showing that  if  and,"H_n G_n \overset{d}{\rightarrow} \mathcal{N}(0,I) H_n H_n' = I G_n \overset{d}{\rightarrow} \mathcal{N}(0,I)","Suppose $A_n,B_n$ are invertible symmetric square matrices. Define $C_n := A_nB_nA_n$ so $C_n$ is symmetric and invertible as well. Define $C_n^{-1/2}$ to be such that $C_n^{-1} = C_n^{-1/2} C_n^{-1/2}$ , where $C_n^{-1/2}$ is symmetric (note that this is always possible by diagonalizing $C_n^{-1}$ since it is symmetric). Define $H_n:=C_n^{-1/2} A_n B_n^{1/2}$ , noting that $H_n H_n' = C_n^{-1/2} A_n B_n^{1/2} B_n^{-1/2}A_nC_n^{-1/2} = C_n^{-1/2} C_n C_n^{-1/2} = I$ , where $I$ is the identity matrix. Suppose $G_n \overset{d}{\rightarrow} \mathcal{N}(0,I),$ $\textbf{How do I show that $H_n G_n \overset{d}{\rightarrow} \mathcal{N}(0,I)$?}$","Suppose are invertible symmetric square matrices. Define so is symmetric and invertible as well. Define to be such that , where is symmetric (note that this is always possible by diagonalizing since it is symmetric). Define , noting that , where is the identity matrix. Suppose","A_n,B_n C_n := A_nB_nA_n C_n C_n^{-1/2} C_n^{-1} = C_n^{-1/2} C_n^{-1/2} C_n^{-1/2} C_n^{-1} H_n:=C_n^{-1/2} A_n B_n^{1/2} H_n H_n' = C_n^{-1/2} A_n B_n^{1/2} B_n^{-1/2}A_nC_n^{-1/2} = C_n^{-1/2} C_n C_n^{-1/2} = I I G_n \overset{d}{\rightarrow} \mathcal{N}(0,I), \textbf{How do I show that H_n G_n \overset{d}{\rightarrow} \mathcal{N}(0,I)?}","['matrices', 'probability-theory', 'symmetric-matrices']"
6,"Given the real numbers $x_1,y_1,z_1,x_2,y_2,z_2,x_3,y_3$ and $z_3$, prove this determinant equality: [closed]","Given the real numbers  and , prove this determinant equality: [closed]","x_1,y_1,z_1,x_2,y_2,z_2,x_3,y_3 z_3","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question $$(x_1+y_1+z_1)\begin{vmatrix}x_1 & y_1 & z_1 \newline x_2 & y_2 &z_2 \newline x_3 & y_3 & z_3\end{vmatrix} =  \begin{vmatrix} 1 & 1 &1 \newline \begin{vmatrix} y_1 & z_1 \newline y_2 & z_2 \end{vmatrix} & -\begin{vmatrix} x_1 & z_1 \newline x_2 & z_2 \end{vmatrix} & \begin{vmatrix} x_1 & y_1 \newline x_2 & y_2 \end{vmatrix} \newline \begin{vmatrix} y_1 & z_1 \newline y_3 & z_3 \end{vmatrix} & -\begin{vmatrix} x_1 & z_1 \newline x_3 & z_3 \end{vmatrix} & \begin{vmatrix} x_1 & y_1 \newline x_3 & y_3 \end{vmatrix} \end{vmatrix}$$ I found this equality playing with barycentric coordinates. It took me very long to figure that it is the case and I wonder if someone can come up with a simple proof for it. I did expand it all with wolframalpha and concluded it was true, I just can't figure the logic behind it","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I found this equality playing with barycentric coordinates. It took me very long to figure that it is the case and I wonder if someone can come up with a simple proof for it. I did expand it all with wolframalpha and concluded it was true, I just can't figure the logic behind it","(x_1+y_1+z_1)\begin{vmatrix}x_1 & y_1 & z_1 \newline x_2 & y_2 &z_2 \newline x_3 & y_3 & z_3\end{vmatrix} =  \begin{vmatrix} 1 & 1 &1 \newline \begin{vmatrix}
y_1 & z_1 \newline y_2 & z_2
\end{vmatrix} & -\begin{vmatrix}
x_1 & z_1 \newline x_2 & z_2
\end{vmatrix} & \begin{vmatrix}
x_1 & y_1 \newline x_2 & y_2
\end{vmatrix} \newline \begin{vmatrix}
y_1 & z_1 \newline y_3 & z_3
\end{vmatrix} & -\begin{vmatrix}
x_1 & z_1 \newline x_3 & z_3
\end{vmatrix} & \begin{vmatrix}
x_1 & y_1 \newline x_3 & y_3
\end{vmatrix} \end{vmatrix}","['linear-algebra', 'matrices', 'algebra-precalculus', 'determinant']"
7,Why is the volume of a parallelogram $P$ formed from the unit square $I^2$ with an integer matrix $A$ equal to the number of integer points in $P$?,Why is the volume of a parallelogram  formed from the unit square  with an integer matrix  equal to the number of integer points in ?,P I^2 A P,"I am aware of this existing post: Number of fixed points of a hyperbolic toral automorphism and its accepted answer: https://math.stackexchange.com/a/3371094/820472 . Unfortunately the last two proposed methods do not seem obvious to me and seem a bit disconnected ( note: what is and isn't connected is very subjective; here by connected I am referring to something that a first year undergraduate student would get after basic linear algebra). And then, while I like the method (Pick's formula) behind the first proposed solution it seems to assume a priori a relationship between the determinant of $A - I$ and the number of integer lattice points in a parallelogram. Question: My question is: Given an integer matrix $A$ such that $A - I$ is invertible, how do you justify that the number of integer points in a parallelogram spanned by $(A - I)(0, 1)$ and $(A - I)(1, 0)$ is equal to $\mathrm{det}(A - I)$ . Bonus question: Do we need that $A$ is an integer matrix? If it is the case, what do we know about a general real matrix $A$ ? Like a lower/upper bound for such integer points? Thanks for the help!","I am aware of this existing post: Number of fixed points of a hyperbolic toral automorphism and its accepted answer: https://math.stackexchange.com/a/3371094/820472 . Unfortunately the last two proposed methods do not seem obvious to me and seem a bit disconnected ( note: what is and isn't connected is very subjective; here by connected I am referring to something that a first year undergraduate student would get after basic linear algebra). And then, while I like the method (Pick's formula) behind the first proposed solution it seems to assume a priori a relationship between the determinant of and the number of integer lattice points in a parallelogram. Question: My question is: Given an integer matrix such that is invertible, how do you justify that the number of integer points in a parallelogram spanned by and is equal to . Bonus question: Do we need that is an integer matrix? If it is the case, what do we know about a general real matrix ? Like a lower/upper bound for such integer points? Thanks for the help!","A - I A A - I (A - I)(0, 1) (A - I)(1, 0) \mathrm{det}(A - I) A A","['linear-algebra', 'matrices', 'determinant', 'quotient-spaces', 'integer-lattices']"
8,Taylor Expansion of a function that maps matrices to scalars,Taylor Expansion of a function that maps matrices to scalars,,Consider first-order taylor approximations For $f: \mathbb{R} \to \mathbb{R}$ we have $f(\tilde x) \approx f(x) + (\tilde x - x)f'(x)$ For $f: \mathbb{R}^{n} \to \mathbb{R}$ we have $f(\mathbf{\tilde{x}}) \approx f(\mathbf {x}) + (\mathbf{\tilde{x}} - \mathbf{x})^T\nabla_xf(\mathbf{x})$ What shall be the corresponding expansion for a $f: \mathbb{R}^{n\times m} \to \mathbb{R}$ ? $f(\mathbf{\tilde{X}}) \approx f(\mathbf {X}) + ?$,Consider first-order taylor approximations For we have For we have What shall be the corresponding expansion for a ?,"f: \mathbb{R} \to \mathbb{R} f(\tilde x) \approx f(x) + (\tilde x - x)f'(x) f: \mathbb{R}^{n} \to \mathbb{R} f(\mathbf{\tilde{x}}) \approx f(\mathbf
{x}) + (\mathbf{\tilde{x}} - \mathbf{x})^T\nabla_xf(\mathbf{x}) f: \mathbb{R}^{n\times m} \to \mathbb{R} f(\mathbf{\tilde{X}}) \approx f(\mathbf
{X}) + ?","['calculus', 'linear-algebra', 'matrices', 'derivatives', 'taylor-expansion']"
9,"Calculus of Variation, Need help finding Euler Lagrange Equation for Klein Gordon [closed]","Calculus of Variation, Need help finding Euler Lagrange Equation for Klein Gordon [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I have been given the following Question: Find the Euler-Lagrange equation for the variational problem with the fundamental Integral: $$ \int_G\left[\frac{1}{2}\left(A_{\alpha \beta}\frac{\partial u}{\partial t_{\alpha}}\frac{\partial u}{\partial t_{\beta}}\right)+\frac{1}{2}m^2u^2\right]dt_1...dt_4 $$ where $m$ is a constant and $$ A=\begin{bmatrix}-1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{bmatrix}\\ \alpha,\beta=1,2,3,4 $$ I am not sure where to start here. Can anyone help me please.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I have been given the following Question: Find the Euler-Lagrange equation for the variational problem with the fundamental Integral: where is a constant and I am not sure where to start here. Can anyone help me please.","
\int_G\left[\frac{1}{2}\left(A_{\alpha \beta}\frac{\partial u}{\partial t_{\alpha}}\frac{\partial u}{\partial t_{\beta}}\right)+\frac{1}{2}m^2u^2\right]dt_1...dt_4
 m 
A=\begin{bmatrix}-1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{bmatrix}\\
\alpha,\beta=1,2,3,4
","['matrices', 'calculus-of-variations', 'euler-lagrange-equation']"
10,For which $B$ is $BA$ positive definite?,For which  is  positive definite?,B BA,"Suppose $A$ is an $m\times n$ real-valued full-rank matrix. What is a nice way to characterize the set of real-valued $n\times m$ matrices $B$ such that $BA$ is positive definite? For $n=1$ , this is the set of vectors in the same half-plane as $A$ (viewed as column vector).","Suppose is an real-valued full-rank matrix. What is a nice way to characterize the set of real-valued matrices such that is positive definite? For , this is the set of vectors in the same half-plane as (viewed as column vector).",A m\times n n\times m B BA n=1 A,"['linear-algebra', 'matrices', 'positive-definite']"
11,Estimate the limit or bounds of the smallest eigenvalue of a symmetric Toeplitz matrix,Estimate the limit or bounds of the smallest eigenvalue of a symmetric Toeplitz matrix,,"I have a symmetric matrix $K\in \mathbb{R}^{(2N+1)\times(2N+1)}$ , with $i,j=-N,\dots,N$ , $$ K_{ij}=-\frac{N}{2\pi[{(i-j)}^2-1/4]} $$ Because the matrix is strictly diagonally dominant with positive diagonals, all eigenvalues are positive. I want to know if the limit of the smallest eigenvalue $$\lim\limits_{N\to\infty}\lambda_0$$ exists. Numerical tests suggest that $\lambda_0\approx0.58$ increases very slowly with $N$ . How can I prove that the limit exists?","I have a symmetric matrix , with , Because the matrix is strictly diagonally dominant with positive diagonals, all eigenvalues are positive. I want to know if the limit of the smallest eigenvalue exists. Numerical tests suggest that increases very slowly with . How can I prove that the limit exists?","K\in \mathbb{R}^{(2N+1)\times(2N+1)} i,j=-N,\dots,N  K_{ij}=-\frac{N}{2\pi[{(i-j)}^2-1/4]}  \lim\limits_{N\to\infty}\lambda_0 \lambda_0\approx0.58 N","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'toeplitz-matrices']"
12,Express Two way ANOVA with interaction as matrix form,Express Two way ANOVA with interaction as matrix form,,"Consider a two-way ANOVA design with interaction: $y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}$ , $i=1,...,I$ , $j=1,...,J$ , $k=1,...,n$ , where $\epsilon_{ijk} \sim N(0,\sigma^2)$ are independent. I am trying to find out appropriate expressions for $A_I$ and $A_M$ from $$ y^T(I - n^{-1}J)y = y^T(I-H)y + y^T A_I y + y^T A_M y $$ ,where $y^TA_My$ is the sum of squares due to the main effect, $H$ is a hat matrix of $X$ , and $J$ is a matrix of one. I figured out that above equation can be expressed as SSTotal = SSError + SSA + SSB + SSInteraction where $SSTotal = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y})^2$ , $SSError = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y}_{ij})^2$ , $SSA = \sum_i \sum_j \sum_k (\bar{y}_{i} - \bar{y})^2$ , $SSB = \sum_i \sum_j \sum_k (\bar{y}_{j} - \bar{y})^2$ , $SSInteraction = \sum_i \sum_j \sum_k (\bar{y}_{ij} - \bar{y}_i - \bar{y}_j + \bar{y})^2$ and $\bar{y} = \sum_i \sum_j \sum_k \frac{y_{ijk}}{nIJ}$ , $\bar{y}_i = \sum_j \sum_k \frac{y_{ijk}}{nJ}$ , $\bar{y}_j = \sum_i \sum_k \frac{y_{ijk}}{nI}$ , $\bar{y}_{ij} = \sum_k \frac{y_{ijk}}{n}$ . My two questions: How could I show that cross terms are zero from $$ SSTotal = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y})^2 = \sum_i \sum_j \sum_k \Big((y_{ijk} - \bar{y}_{ij} )+(\bar{y}_i - \bar{y}) + (\bar{y}_j - \bar{y}) + (\bar{y}_{ij} - \bar{y}_i - \bar{y}_j + \bar{y}) \Big)^2 $$ I only show the $\sum_i \sum_j \sum_k (\bar{y}_i - \bar{y})(\bar{y}_j - \bar{y})=0$ . How could I express the above equation by using matrix?","Consider a two-way ANOVA design with interaction: , , , , where are independent. I am trying to find out appropriate expressions for and from ,where is the sum of squares due to the main effect, is a hat matrix of , and is a matrix of one. I figured out that above equation can be expressed as SSTotal = SSError + SSA + SSB + SSInteraction where , , , , and , , , . My two questions: How could I show that cross terms are zero from I only show the . How could I express the above equation by using matrix?","y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk} i=1,...,I j=1,...,J k=1,...,n \epsilon_{ijk} \sim N(0,\sigma^2) A_I A_M 
y^T(I - n^{-1}J)y = y^T(I-H)y + y^T A_I y + y^T A_M y
 y^TA_My H X J SSTotal = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y})^2 SSError = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y}_{ij})^2 SSA = \sum_i \sum_j \sum_k (\bar{y}_{i} - \bar{y})^2 SSB = \sum_i \sum_j \sum_k (\bar{y}_{j} - \bar{y})^2 SSInteraction = \sum_i \sum_j \sum_k (\bar{y}_{ij} - \bar{y}_i - \bar{y}_j + \bar{y})^2 \bar{y} = \sum_i \sum_j \sum_k \frac{y_{ijk}}{nIJ} \bar{y}_i = \sum_j \sum_k \frac{y_{ijk}}{nJ} \bar{y}_j = \sum_i \sum_k \frac{y_{ijk}}{nI} \bar{y}_{ij} = \sum_k \frac{y_{ijk}}{n} 
SSTotal = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y})^2 = \sum_i \sum_j \sum_k \Big((y_{ijk} - \bar{y}_{ij} )+(\bar{y}_i - \bar{y}) + (\bar{y}_j - \bar{y}) + (\bar{y}_{ij} - \bar{y}_i - \bar{y}_j + \bar{y}) \Big)^2
 \sum_i \sum_j \sum_k (\bar{y}_i - \bar{y})(\bar{y}_j - \bar{y})=0","['matrices', 'statistics', 'sums-of-squares', 'anova']"
13,Find the multiplicity of eigenvalue $2a$,Find the multiplicity of eigenvalue,2a,"Let $a,b$ be real numbers with $0<2a<b$ . Let $K_n$ denote the commutation matrix of order $n$ , and let $A$ be the $n^2\times n^2$ matrix defined by $$A:=a[vec(I_n)vec(I_n)'+K_n-2diag(K_n)+I_{n^2}]+bdiag(K_n)$$ where $vec$ denotes the vectorization operator , and $diag(C)$ denotes the diagonal matrix having the same diagonal as $C$ . Finally let $M$ be an $n\times n$ idempotent matrix with rank $r<n$ , and let $B$ be the matrix defined by $$B:=[M\otimes M]A[M \otimes M]$$ where $\otimes$ denotes the kronecker product . I am trying to find the multiplicity of eigenvalue $2a$ as a function of $M$ . Also I would like to show that $2a$ is the smallest nonzero eigenvalue of $B$ . After some work I found that the eigenvalues of $A$ are as follows : $b+na  \, \text{   with multiplicity } 1 \, \text{and eigenvector} \sum_{i=1}^n (e_i\otimes e_i),$ $b  \text{   with multiplicty } n-1 \, \text{and eigenvectors} \, \{(e_1\otimes e_1)-(e_i\otimes e_i), i=2,\dots, n\},$ $2a  \, \text{   with multiplicty } n(n-1)/2 \, \text{and eigenvectors} \, \{(e_i\otimes e_j)+(e_j\otimes e_i), i\neq j\},$ $0  \, \text{   with multiplicty } n(n-1)/2 \, \text{and eigenvectors} \, \{(e_i\otimes e_j)-(e_j\otimes e_i), i \neq j\},$ where $e_i$ denotes the $n\times 1$ vector with $1$ in position $i$ and zeros elsewhere.  I believe the multiplicity of $2a$ corresponds to the dimension of the $Col(M\otimes M) \cap Col(R)$ , where $R$ denotes the $n^2\times n(n-1)/2$ matrix having columns $(e_i\otimes e_j)+(e_j\otimes e_i)$ for $i\neq j$ , and $Col$ denotes the column space. Any ideas how to proceed? Thanks a lot for your help.","Let be real numbers with . Let denote the commutation matrix of order , and let be the matrix defined by where denotes the vectorization operator , and denotes the diagonal matrix having the same diagonal as . Finally let be an idempotent matrix with rank , and let be the matrix defined by where denotes the kronecker product . I am trying to find the multiplicity of eigenvalue as a function of . Also I would like to show that is the smallest nonzero eigenvalue of . After some work I found that the eigenvalues of are as follows : where denotes the vector with in position and zeros elsewhere.  I believe the multiplicity of corresponds to the dimension of the , where denotes the matrix having columns for , and denotes the column space. Any ideas how to proceed? Thanks a lot for your help.","a,b 0<2a<b K_n n A n^2\times n^2 A:=a[vec(I_n)vec(I_n)'+K_n-2diag(K_n)+I_{n^2}]+bdiag(K_n) vec diag(C) C M n\times n r<n B B:=[M\otimes M]A[M \otimes M] \otimes 2a M 2a B A b+na  \, \text{   with multiplicity } 1 \, \text{and eigenvector} \sum_{i=1}^n (e_i\otimes e_i), b  \text{   with multiplicty } n-1 \, \text{and eigenvectors} \, \{(e_1\otimes e_1)-(e_i\otimes e_i), i=2,\dots, n\}, 2a  \, \text{   with multiplicty } n(n-1)/2 \, \text{and eigenvectors} \, \{(e_i\otimes e_j)+(e_j\otimes e_i), i\neq j\}, 0  \, \text{   with multiplicty } n(n-1)/2 \, \text{and eigenvectors} \, \{(e_i\otimes e_j)-(e_j\otimes e_i), i \neq j\}, e_i n\times 1 1 i 2a Col(M\otimes M) \cap Col(R) R n^2\times n(n-1)/2 (e_i\otimes e_j)+(e_j\otimes e_i) i\neq j Col","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'kronecker-product']"
14,"General inverse for a symmetric matrix of the form $\mathbf A_n = \left(a_{\min(i,j)}b_{\max(i,j)}\right)$",General inverse for a symmetric matrix of the form,"\mathbf A_n = \left(a_{\min(i,j)}b_{\max(i,j)}\right)","I am looking for a general inverse formula for a symmetric matrix of the form $$ \mathbf A_n = \left(a_{\min(i,j)}b_{\max(i,j)}\right) = \pmatrix{ a_1b_1 & a_1b_2 & a_1b_3 & \cdots & a_1b_n \\ a_1b_2 & a_2b_2 & a_2b_3 & \cdots & a_2b_n \\ \vdots &    & \ddots  &        & \vdots \\ a_1b_n & a_2b_n &   &        & a_nb_n} $$ where $a_i,b_i $ are real numbers.  Empirically I can see that $ \mathbf A_n^{-1} $ is tridiagonal so I suspect it has previously been studied.  For example we have $$ \mathbf A_5 = \left( \begin{array}{ccccc}  \frac{a_2}{a_1 a_2 b_1-a_1^2 b_2} & \frac{1}{a_1 b_2-a_2 b_1} & 0 & 0 & 0 \\  \frac{1}{a_1 b_2-a_2 b_1} & \frac{a_1 b_3-a_3 b_1}{\left(a_2 b_1-a_1 b_2\right) \left(a_2 b_3-a_3 b_2\right)} & \frac{1}{a_2 b_3-a_3 b_2} & 0 & 0 \\  0 & \frac{1}{a_2 b_3-a_3 b_2} & \frac{a_2 b_4-a_4 b_2}{\left(a_3 b_2-a_2 b_3\right) \left(a_3 b_4-a_4 b_3\right)} & \frac{1}{a_3 b_4-a_4 b_3} & 0 \\  0 & 0 & \frac{1}{a_3 b_4-a_4 b_3} & \frac{a_3 b_5-a_5 b_3}{\left(a_4 b_3-a_3 b_4\right) \left(a_4 b_5-a_5 b_4\right)} & \frac{1}{a_4 b_5-a_5 b_4} \\  0 & 0 & 0 & \frac{1}{a_4 b_5-a_5 b_4} & \frac{b_4}{a_5 b_4 b_5-a_4 b_5^2} \\ \end{array} \right) $$ which seems to be based on a bunch of 2x2 determinants... Any help appreciated, thank you! p.","I am looking for a general inverse formula for a symmetric matrix of the form where are real numbers.  Empirically I can see that is tridiagonal so I suspect it has previously been studied.  For example we have which seems to be based on a bunch of 2x2 determinants... Any help appreciated, thank you! p."," \mathbf A_n = \left(a_{\min(i,j)}b_{\max(i,j)}\right) = \pmatrix{ a_1b_1 & a_1b_2 & a_1b_3 & \cdots & a_1b_n \\
a_1b_2 & a_2b_2 & a_2b_3 & \cdots & a_2b_n \\
\vdots &    & \ddots  &        & \vdots \\
a_1b_n & a_2b_n &   &        & a_nb_n}  a_i,b_i   \mathbf A_n^{-1}   \mathbf A_5 =
\left(
\begin{array}{ccccc}
 \frac{a_2}{a_1 a_2 b_1-a_1^2 b_2} & \frac{1}{a_1 b_2-a_2 b_1} & 0 & 0 & 0 \\
 \frac{1}{a_1 b_2-a_2 b_1} & \frac{a_1 b_3-a_3 b_1}{\left(a_2 b_1-a_1 b_2\right) \left(a_2 b_3-a_3 b_2\right)} & \frac{1}{a_2 b_3-a_3 b_2} & 0 & 0 \\
 0 & \frac{1}{a_2 b_3-a_3 b_2} & \frac{a_2 b_4-a_4 b_2}{\left(a_3 b_2-a_2 b_3\right) \left(a_3 b_4-a_4 b_3\right)} & \frac{1}{a_3 b_4-a_4 b_3} & 0 \\
 0 & 0 & \frac{1}{a_3 b_4-a_4 b_3} & \frac{a_3 b_5-a_5 b_3}{\left(a_4 b_3-a_3 b_4\right) \left(a_4 b_5-a_5 b_4\right)} & \frac{1}{a_4 b_5-a_5 b_4} \\
 0 & 0 & 0 & \frac{1}{a_4 b_5-a_5 b_4} & \frac{b_4}{a_5 b_4 b_5-a_4 b_5^2} \\
\end{array}
\right)
","['matrices', 'determinant', 'inverse']"
15,How to solve the $p$th power of matrix $C$?,How to solve the th power of matrix ?,p C,"if $p\equiv1(mod3)$ , $C=\left[   \begin{array}{ccc}     0 & 0 & 1 \\     1 & 0 & -3 \\     0 & 1 & -3 \\   \end{array} \right]\in M_3(\mathbb{F}_p).$ How to calculate $C^p$ ? I have tried some conventional methods, such as splitting the matrix, binomial theorem, etc. But got no result. Do you have other good solutions? Thanks for your answer.","if , How to calculate ? I have tried some conventional methods, such as splitting the matrix, binomial theorem, etc. But got no result. Do you have other good solutions? Thanks for your answer.","p\equiv1(mod3) C=\left[
  \begin{array}{ccc}
    0 & 0 & 1 \\
    1 & 0 & -3 \\
    0 & 1 & -3 \\
  \end{array}
\right]\in M_3(\mathbb{F}_p). C^p","['matrices', 'number-theory', 'finite-fields', 'minimal-polynomials']"
16,"Show that if $A$ and $B$ commute, then $B$ commutes with $e^A$","Show that if  and  commute, then  commutes with",A B B e^A,"Show that if $A$ and $B$ commute, then $B$ commutes with $e^A$ For the first one I have $$e^A = I + A + \frac{A^2}{2!} + \frac{A^3}{3!} +\dots$$ I believe I can pull out $$I + A + \frac{A^2}{2!} + \frac{A^3}{3!} +\dots$$ and multiply by $B$ on either side but not sure the validity of this proving this","Show that if and commute, then commutes with For the first one I have I believe I can pull out and multiply by on either side but not sure the validity of this proving this",A B B e^A e^A = I + A + \frac{A^2}{2!} + \frac{A^3}{3!} +\dots I + A + \frac{A^2}{2!} + \frac{A^3}{3!} +\dots B,"['linear-algebra', 'matrices', 'solution-verification', 'examples-counterexamples', 'exponentiation']"
17,Is it possible to have n matrices that generate a linearly independent vectors for the same input?,Is it possible to have n matrices that generate a linearly independent vectors for the same input?,,"The question I'm trying to solve is finding when there are $n$ $n \times n$ matrices $A_1, \dots, A_n$ such that for all $v \neq 0$ , $A_1v, \dots, A_nv$ are linearly independent. I think it's possible for $n = 1$ and all even $n$ , but I'm not sure how to find an explicit construction. So far, I've only shown it's impossible for odd $n > 3$ : WLOG we can assume $A_1 = I$ and then some other matrix has an eigenvector because $n$ is odd, which means plugging that eigenvector into all the matrices will not generate linearly independent vector. I think this also proves that no matrix can have an eigenvector, but I'm not 100% sure. Context: My friend asked me this question a couple of days ago. They're only in a linear algebra class, but I or any of my friends haven't been able to solve this with what we know from analysis/algebra so I thought to ask here. Thank you in advance!","The question I'm trying to solve is finding when there are matrices such that for all , are linearly independent. I think it's possible for and all even , but I'm not sure how to find an explicit construction. So far, I've only shown it's impossible for odd : WLOG we can assume and then some other matrix has an eigenvector because is odd, which means plugging that eigenvector into all the matrices will not generate linearly independent vector. I think this also proves that no matrix can have an eigenvector, but I'm not 100% sure. Context: My friend asked me this question a couple of days ago. They're only in a linear algebra class, but I or any of my friends haven't been able to solve this with what we know from analysis/algebra so I thought to ask here. Thank you in advance!","n n \times n A_1, \dots, A_n v \neq 0 A_1v, \dots, A_nv n = 1 n n > 3 A_1 = I n","['linear-algebra', 'matrices']"
18,The Lovasz-Stein theorem,The Lovasz-Stein theorem,,"Given a family $\mathcal{F}$ of subsets of some finite set $X$ , its cover number of $\mathcal{F}$ , $\text{Cov}(\mathcal{F})$ , is the minimum number of members of $\mathcal{F}$ whose union covers all points of $X$ . Theorem 2.16. If each member of $\mathcal{F}$ has at most $a$ elements, and each point $x\in X$ belongs to at least $v$ of the sets in $\mathcal{F}$ , then $$\text{Cov}(\mathcal{F})\leq  \dfrac{|\mathcal{F}|}{v}(1+\ln a).$$ Proof. Let $|X|=N, |\mathcal{F}|=M$ and consider the $N\times M$ $0$ - $1$ matrix $A=(a_{x,i})$ , where $a_{x,i}=1$ iff $x\in X$ belongs to the $i$ -th member of $\mathcal{F}$ . By our assumption, each row of $A$ has at least $v$ ones and each column at most $a$ ones. By double couting, we have that $Nv\leq Ma$ , or equivalently, $$\frac{M}{v}\geq \frac{N}{a} \quad \quad (*).$$ Our goal is to show that $A$ must contain an $N\times K $ submatrix $C$ with no all- $0$ rows and such that $$K\leq (M/v)(1+\ln a).$$ We describe a constructive procedure for producing the desired submatrix $C$ . Let $A_a=A$ and define $A'_a$ to be any maximal set of columns from $A_a$ whose supports are pairwise disjoint and whose columns each have $a$ ones. Let $K_a=|A'_a|$ . Discard from $A_a$ the columns of $A'_a$ and any row with a one in $A'_a$ . We are left with a $k_a\times (M-K_a)$ matrix $A_{a-1}$ , where $k_a=N-aK_a$ . Clearly, the columns of $A_{a-1}$ have at most $a-1$ ones (indeed, otherwise such a column could be added to the previously discarded set, contradicting its maximality). We continue by doing to $A_{a-1}$ what we did to $A_a$ . That is we define $A'_{a-1}$ to be any maximal set of columns from $A_{a-1}$ , whose supports are pairwise disjoint and whose columns each have $a-1$ ones. Let $K_{a-1}=|A'_{a-1}|$ , then discard from $A_{a-1}$ the columns of $A'_{a-1}$ and any row with a one in $A'_{a-1}$ getting a $k_{a-1}\times (M-K_a-K_{a-1})$ matrix $A_{a-2}$ , where $k_{a-1}=N-aK_a-(a-1)K_{a-1}$ . The process will terminate after at most $a$ steps. The union of the columns of the discarded sets form the desired submatrix $C$ with $K=\sum \limits_{i=1}^a K_i$ . The first step of the algorithm gives $k_a=N-aK_a$ , which we rewrite, setting $k_{a+1}=N$ , as $$K_a=\frac{k_{a+1}-k_a}{a}.$$ Analogously, $$K_i=\frac{k_{i+1}-k_i}{i} \  \text{for} \ i=1,\dots,a.$$ Now we derive an upper bound for $k_i$ by counting the number of ones in $A_{i-1}$ in two ways: every row of $A_{i-1}$ contains at least $v$ ones, and every column at most $i-1$ ones, thus $$vk_i\leq (i-1)(M-K_a-\dots-K_{i+1})\leq (i-1)M,$$ or equivalently $$k_i\leq \frac{(i-1)M}{v}.$$ So, $$K=\sum\limits_{i=1}^a K_i=\sum\limits_{i=1}^a \frac{k_{i+1}-k_i}{i}=$$ $$=\frac{k_{a+1}}{a}+\frac{k_{a}}{a(a-1)}+\frac{k_{a-1}}{(a-1)(a-2)}+\dots+\frac{k_{2}}{2\cdot 1}-k_1\leq $$ $$\leq \frac{N}{a}+\frac{M}{v}(\frac{1}{a}+\dots+\frac{1}{2})\leq \frac{N}{a}+\frac{M}{v}\ln a.$$ The last inequality here follows because $1+\frac{1}{2}+\dots+\frac{1}{n}$ is the $n$ -th harmonic number which is known to lie between $\ln n$ and $\ln n+1$ . Together with $(*)$ , this yields $K\leq (M/v)(1+\ln a)$ , as desired. This proof is copied from the book of Stasys Jukna ""Extremal combinatorics"" and I would like to clarify some moments of this proof. My questions: I am a bit confused with the procedure and how it works. In the first step we need to find any maximal set of columns which have pairwise disjoint supports and each of them have $a$ ones. What if there no columns with $a$ ones? Intuitively I can understand why this process terminates after at most $a$ steps but can anyone explain it in a rigorous way, please? How we take the union of the columns of the discarded sets if those columns have different size? If we take $C$ to be the union of the columns of the discarded sets. why any row of $C$ has $1$ among its entries? I would be very thankful if someone can explain those questions! Thank you so much for your attention!","Given a family of subsets of some finite set , its cover number of , , is the minimum number of members of whose union covers all points of . Theorem 2.16. If each member of has at most elements, and each point belongs to at least of the sets in , then Proof. Let and consider the - matrix , where iff belongs to the -th member of . By our assumption, each row of has at least ones and each column at most ones. By double couting, we have that , or equivalently, Our goal is to show that must contain an submatrix with no all- rows and such that We describe a constructive procedure for producing the desired submatrix . Let and define to be any maximal set of columns from whose supports are pairwise disjoint and whose columns each have ones. Let . Discard from the columns of and any row with a one in . We are left with a matrix , where . Clearly, the columns of have at most ones (indeed, otherwise such a column could be added to the previously discarded set, contradicting its maximality). We continue by doing to what we did to . That is we define to be any maximal set of columns from , whose supports are pairwise disjoint and whose columns each have ones. Let , then discard from the columns of and any row with a one in getting a matrix , where . The process will terminate after at most steps. The union of the columns of the discarded sets form the desired submatrix with . The first step of the algorithm gives , which we rewrite, setting , as Analogously, Now we derive an upper bound for by counting the number of ones in in two ways: every row of contains at least ones, and every column at most ones, thus or equivalently So, The last inequality here follows because is the -th harmonic number which is known to lie between and . Together with , this yields , as desired. This proof is copied from the book of Stasys Jukna ""Extremal combinatorics"" and I would like to clarify some moments of this proof. My questions: I am a bit confused with the procedure and how it works. In the first step we need to find any maximal set of columns which have pairwise disjoint supports and each of them have ones. What if there no columns with ones? Intuitively I can understand why this process terminates after at most steps but can anyone explain it in a rigorous way, please? How we take the union of the columns of the discarded sets if those columns have different size? If we take to be the union of the columns of the discarded sets. why any row of has among its entries? I would be very thankful if someone can explain those questions! Thank you so much for your attention!","\mathcal{F} X \mathcal{F} \text{Cov}(\mathcal{F}) \mathcal{F} X \mathcal{F} a x\in X v \mathcal{F} \text{Cov}(\mathcal{F})\leq
 \dfrac{|\mathcal{F}|}{v}(1+\ln a). |X|=N, |\mathcal{F}|=M N\times M 0 1 A=(a_{x,i}) a_{x,i}=1 x\in X i \mathcal{F} A v a Nv\leq Ma \frac{M}{v}\geq \frac{N}{a} \quad \quad (*). A N\times K  C 0 K\leq (M/v)(1+\ln a). C A_a=A A'_a A_a a K_a=|A'_a| A_a A'_a A'_a k_a\times (M-K_a) A_{a-1} k_a=N-aK_a A_{a-1} a-1 A_{a-1} A_a A'_{a-1} A_{a-1} a-1 K_{a-1}=|A'_{a-1}| A_{a-1} A'_{a-1} A'_{a-1} k_{a-1}\times (M-K_a-K_{a-1}) A_{a-2} k_{a-1}=N-aK_a-(a-1)K_{a-1} a C K=\sum \limits_{i=1}^a K_i k_a=N-aK_a k_{a+1}=N K_a=\frac{k_{a+1}-k_a}{a}. K_i=\frac{k_{i+1}-k_i}{i} \  \text{for} \ i=1,\dots,a. k_i A_{i-1} A_{i-1} v i-1 vk_i\leq (i-1)(M-K_a-\dots-K_{i+1})\leq (i-1)M, k_i\leq \frac{(i-1)M}{v}. K=\sum\limits_{i=1}^a K_i=\sum\limits_{i=1}^a \frac{k_{i+1}-k_i}{i}= =\frac{k_{a+1}}{a}+\frac{k_{a}}{a(a-1)}+\frac{k_{a-1}}{(a-1)(a-2)}+\dots+\frac{k_{2}}{2\cdot 1}-k_1\leq  \leq \frac{N}{a}+\frac{M}{v}(\frac{1}{a}+\dots+\frac{1}{2})\leq \frac{N}{a}+\frac{M}{v}\ln a. 1+\frac{1}{2}+\dots+\frac{1}{n} n \ln n \ln n+1 (*) K\leq (M/v)(1+\ln a) a a a C C 1","['combinatorics', 'matrices', 'discrete-mathematics', 'extremal-combinatorics']"
19,Property of unitary matrices,Property of unitary matrices,,"This article - https://doi.org/10.1016/0370-2693(88)91216-6 - states that for a $3 \times 3$ unitary matrix $A$ , we have $|A_{33}| = |(A_{11}A_{22}-A_{12} A_{21})|$ , where $A_{ij}$ stands for the element in row $i$ and column $j$ of matrix $A$ and |.| stands for the modulus. Why is this true? I tried expanding the square of the right-hand side of the equality but I got a term with $\Re(A_{11}A_{22}A_{12}^*A_{21}^*)$ that I don't think can easily be related to $|A_{33}|$ so this may not be the correct procedure.","This article - https://doi.org/10.1016/0370-2693(88)91216-6 - states that for a unitary matrix , we have , where stands for the element in row and column of matrix and |.| stands for the modulus. Why is this true? I tried expanding the square of the right-hand side of the equality but I got a term with that I don't think can easily be related to so this may not be the correct procedure.",3 \times 3 A |A_{33}| = |(A_{11}A_{22}-A_{12} A_{21})| A_{ij} i j A \Re(A_{11}A_{22}A_{12}^*A_{21}^*) |A_{33}|,"['matrices', 'unitary-matrices']"
20,Permutation Matrix Measure,Permutation Matrix Measure,,"I’m trying to make a neutal network with dynamic architecture, meaning that the weight matrices will be added/removed dynamically when they are simple permutation matrices and thus don’t actually do any computation. However, I’ve run into an issue, I need a differentiable way to determine ”how close” the weight matrices are to being a permutation matrix. This measure will then be added to the loss function as a regularization term. I know how to find the closest permutation matrix by making a cost matrix as one of the answers from here: Nearest signed permutation matrix to a given matrix $A$ However, if I use the norm of the cost matrix as a measure of ”permutivity” the simple 0 matrix will always give a lower measure (loss) than an actuall permutation matrix. Whenever I try to find or come up with a measure I always run into this same problem, there’s always some other matrix which gives a lower score than a permutation matrix. Do any of you guys know of a ”permutation matrix measure” pr do you have any brilliant ideas on how to make one? Keep in mind that it has to be differentiable to be usefull,so no max functions or anything like that. Update: I have found one measure that does what I need, but it’s too computationally heavy to use. I’ll write it here anyway, since it gives insight to what I’m looking for and might spawn new ideas: $ \text{”permutivity”} = \prod_{i = 1}^{n} \sum_{j = 1}^{m}\sum_{k = 1}^{m} (a_{j,k}-{p_{i}}_{j,k})^2 $ Where $p_i$ is the ”i:th” permutation matrix of size MxM. So basically the product of the L2 distance of all the different permutation matrices. As you can imagine, this is not practical for several reasons, first of all, the number of permutation matrices are m! and for neural networks m is usually fairly large. Secondly, the product would explode causing numerical overflows when trying to calculate it. The second issue can be remedied using some sort of ”cap” function around the sums, but I can’t see any simple way to fix the first problem :(","I’m trying to make a neutal network with dynamic architecture, meaning that the weight matrices will be added/removed dynamically when they are simple permutation matrices and thus don’t actually do any computation. However, I’ve run into an issue, I need a differentiable way to determine ”how close” the weight matrices are to being a permutation matrix. This measure will then be added to the loss function as a regularization term. I know how to find the closest permutation matrix by making a cost matrix as one of the answers from here: Nearest signed permutation matrix to a given matrix $A$ However, if I use the norm of the cost matrix as a measure of ”permutivity” the simple 0 matrix will always give a lower measure (loss) than an actuall permutation matrix. Whenever I try to find or come up with a measure I always run into this same problem, there’s always some other matrix which gives a lower score than a permutation matrix. Do any of you guys know of a ”permutation matrix measure” pr do you have any brilliant ideas on how to make one? Keep in mind that it has to be differentiable to be usefull,so no max functions or anything like that. Update: I have found one measure that does what I need, but it’s too computationally heavy to use. I’ll write it here anyway, since it gives insight to what I’m looking for and might spawn new ideas: Where is the ”i:th” permutation matrix of size MxM. So basically the product of the L2 distance of all the different permutation matrices. As you can imagine, this is not practical for several reasons, first of all, the number of permutation matrices are m! and for neural networks m is usually fairly large. Secondly, the product would explode causing numerical overflows when trying to calculate it. The second issue can be remedied using some sort of ”cap” function around the sums, but I can’t see any simple way to fix the first problem :("," \text{”permutivity”} = \prod_{i = 1}^{n} \sum_{j = 1}^{m}\sum_{k = 1}^{m} (a_{j,k}-{p_{i}}_{j,k})^2  p_i","['linear-algebra', 'matrices', 'optimization', 'permutation-matrices']"
21,Matrix inverse step in SVD & ridge regression,Matrix inverse step in SVD & ridge regression,,"When we do OLS of $y$ on $X$ , with $X$ being a n x p input matrix, the OLS $\beta$ is $(X^TX)^{-1}X^TY$ , and the Ridge regression beta is $(X^TX+\lambda I)^{-1}X^TY$ . Also, the singular value decomposition of X is $UDV^T$ where $U$ and $V$ are orthogonal matrices and $D$ being a diagonal matrix. In equation 3.47 of Elements of Statistical Learning the author states $$ \begin{aligned} X\beta^{ridge} &= X(X^TX+\lambda I)^{-1}X^Ty\\ &= UD(D^2+\lambda I)^{-1}DU^Ty \end{aligned} $$ which seems to suggest that $X^TX = D^2$ . But to arrive at that we first have $$ \begin{aligned} X^TX &=VDU^TUDV^T\\ &= VD^2V^T \end{aligned} $$ Now, I know $VV^T = I$ by property of orthogonal/orthonormal matrix. But there's a $D^2$ between them and I know matrix multiplications are not commutative. So how do we get to $VD^2V^T = D^2$ ?","When we do OLS of on , with being a n x p input matrix, the OLS is , and the Ridge regression beta is . Also, the singular value decomposition of X is where and are orthogonal matrices and being a diagonal matrix. In equation 3.47 of Elements of Statistical Learning the author states which seems to suggest that . But to arrive at that we first have Now, I know by property of orthogonal/orthonormal matrix. But there's a between them and I know matrix multiplications are not commutative. So how do we get to ?","y X X \beta (X^TX)^{-1}X^TY (X^TX+\lambda I)^{-1}X^TY UDV^T U V D 
\begin{aligned}
X\beta^{ridge} &= X(X^TX+\lambda I)^{-1}X^Ty\\
&= UD(D^2+\lambda I)^{-1}DU^Ty
\end{aligned}
 X^TX = D^2 
\begin{aligned}
X^TX &=VDU^TUDV^T\\
&= VD^2V^T
\end{aligned}
 VV^T = I D^2 VD^2V^T = D^2","['least-squares', 'matrices', 'linear-algebra', 'svd']"
22,Sum of matrix inverses,Sum of matrix inverses,,"I am trying to compute the following finite sum: $$\sum_{i=1}^n\left(A+\lambda_iB\right)^{-1}$$ In this sum, $A$ and $B$ are are positive-definite matrices (so they are inversible). The $\lambda_i$ are positive numbers so the sums $\left(A+\lambda_iB\right)$ are also positive-definite (and inversible). Is there a way to factor or rewrite this sum in a way where we can only compute $A^{-1}$ and $B^{-1}$ once instead of having to inverse so many matrices? I tried to use Woodbury's identity but I couldn't simplify the sum much. I could also rewrite the sum as: $$\sum_{i=1}^n B^{-1}\left(B^{-1}+\lambda_iA^{-1}\right)^{-1}A^{-1}$$ but it doesn't really simplify the problem. Edit: To simplify the problem we can instead compute the following sum: $$S(\lambda) = \sum_{i=1}^n\left(I+\lambda_iC\right)^{-1}$$ where $I$ is the identity matrix and $C$ is another positive-definite matrix.","I am trying to compute the following finite sum: In this sum, and are are positive-definite matrices (so they are inversible). The are positive numbers so the sums are also positive-definite (and inversible). Is there a way to factor or rewrite this sum in a way where we can only compute and once instead of having to inverse so many matrices? I tried to use Woodbury's identity but I couldn't simplify the sum much. I could also rewrite the sum as: but it doesn't really simplify the problem. Edit: To simplify the problem we can instead compute the following sum: where is the identity matrix and is another positive-definite matrix.",\sum_{i=1}^n\left(A+\lambda_iB\right)^{-1} A B \lambda_i \left(A+\lambda_iB\right) A^{-1} B^{-1} \sum_{i=1}^n B^{-1}\left(B^{-1}+\lambda_iA^{-1}\right)^{-1}A^{-1} S(\lambda) = \sum_{i=1}^n\left(I+\lambda_iC\right)^{-1} I C,"['matrices', 'matrix-calculus', 'positive-definite', 'symmetric-matrices']"
23,"Let $M$ and $N$ be two $3\times 3$ matrices such that $MN=NM$, Further if $M\neq N^2$ and $M^2=N^4$ then","Let  and  be two  matrices such that , Further if  and  then",M N 3\times 3 MN=NM M\neq N^2 M^2=N^4,"Let $M$ and $N$ be two $3\times 3$ matrices such that $MN=NM$ , Further if $M\neq N^2$ and $M^2=N^4$ then (A) Determinant of $(M^2+MN^2)$ is $0$ (B) There is a non-zero $3\times 3$ matrix $U$ such that $(M^2+MN^2)U$ is the zero matrix (C) Determinant of $(M^2+MN^2) \geq1$ (D) For a $3\times 3$ Matrix $U$ , if $(M^2+MN^2)U$ equals the zero matrix then $U$ is the zero matrix. My Thinking: $(M-N^2)(M+N^2)=M^2-N^4+MN^2-N^2M$ $\implies$ $(M-N^2)(M+N^2)=MNN-NNM=NMN-NMN=0$ (Here zero denotes Null matrix). Case $(1)$ $M+N^2 \neq0$ and $M-N^2= 0$ (Here zero is Null matrix) But it is given that $M\neq N^2$ So Case $1$ Can't be true. Case $(2)$ $M-N^2\neq 0$ and $M+N^2\neq 0$ But we know that $|M-N^2||M+N^2|=0$ $\implies$ $|M+N^2||M-N^2|= 0$ So Subcase $(1)$ of case $(2)$ $|M+N^2|=0$ and $|M-N^2|\neq 0$ Subcase $(2)$ of case $(2)$ $|M+N^2|\neq 0$ and $|M-N^2|=0$ But when $|M+N^2|\neq0$ then we can take inverse of this. $\implies$ $0=(M+N^2)(M-N^2)$ $\implies$ $0=(M+N^2)^{-1}(M+N^2)(M-N^2)$ $\implies$ $0= M-N^2$ i.e. subcase $(2)$ is false. $\implies$ Result of case $(2)$ is $|M+N^2|=0$ Case $(3)$ $M+N^2=0$ and $M-N^2\neq 0$ Result of case $(3)$ is $|M+N^2|=0$ If above is  true then Matrix $(M+N^2)=0$ From All three cases $|M+N^2|=0$ . So Option $(A)$ is True. My Doubt is option (B) I think this statement should be false because $(M^2+MN^2)U$ need not be zero always. This question was asked in Indian Exam JEE Advanced Paper $1$ of Year $2014$ https://www.jeeadv.ac.in/archive.html Linked Question Finding the value of the determinant $|M^2+MN^2|$ and determining whether $U$ is a zero matrix or not","Let and be two matrices such that , Further if and then (A) Determinant of is (B) There is a non-zero matrix such that is the zero matrix (C) Determinant of (D) For a Matrix , if equals the zero matrix then is the zero matrix. My Thinking: (Here zero denotes Null matrix). Case and (Here zero is Null matrix) But it is given that So Case Can't be true. Case and But we know that So Subcase of case and Subcase of case and But when then we can take inverse of this. i.e. subcase is false. Result of case is Case and Result of case is If above is  true then Matrix From All three cases . So Option is True. My Doubt is option (B) I think this statement should be false because need not be zero always. This question was asked in Indian Exam JEE Advanced Paper of Year https://www.jeeadv.ac.in/archive.html Linked Question Finding the value of the determinant $|M^2+MN^2|$ and determining whether $U$ is a zero matrix or not",M N 3\times 3 MN=NM M\neq N^2 M^2=N^4 (M^2+MN^2) 0 3\times 3 U (M^2+MN^2)U (M^2+MN^2) \geq1 3\times 3 U (M^2+MN^2)U U (M-N^2)(M+N^2)=M^2-N^4+MN^2-N^2M \implies (M-N^2)(M+N^2)=MNN-NNM=NMN-NMN=0 (1) M+N^2 \neq0 M-N^2= 0 M\neq N^2 1 (2) M-N^2\neq 0 M+N^2\neq 0 |M-N^2||M+N^2|=0 \implies |M+N^2||M-N^2|= 0 (1) (2) |M+N^2|=0 |M-N^2|\neq 0 (2) (2) |M+N^2|\neq 0 |M-N^2|=0 |M+N^2|\neq0 \implies 0=(M+N^2)(M-N^2) \implies 0=(M+N^2)^{-1}(M+N^2)(M-N^2) \implies 0= M-N^2 (2) \implies (2) |M+N^2|=0 (3) M+N^2=0 M-N^2\neq 0 (3) |M+N^2|=0 (M+N^2)=0 |M+N^2|=0 (A) (M^2+MN^2)U 1 2014,"['linear-algebra', 'matrices', 'determinant', 'jordan-normal-form']"
24,"If $B-A=ww^{\top}$ for symmetric and orthogonal matrices $A$ and $B$, how to show that $w$ has two nonzero entries?","If  for symmetric and orthogonal matrices  and , how to show that  has two nonzero entries?",B-A=ww^{\top} A B w,"Suppose that $A$ and $B$ are binary square matrices of same dimension and that $A=A^{\top}=A^{-1}$ (same for $B$ ), i.e. they are symmetric and orthogonal. In addition, suppose $\text{rank}(B-A)=1$ so that $B-A=uv^{\top}$ for nonzero vectors $u$ and $v$ . I want to show that $B-A$ has the form $ww^{\top}$ , where $w$ is a column vector with only two nonzero entries whose values are $\pm 1$ . Since $B-A$ is symmetric (can be easily verified), then $B-A=ww^{\top}$ for some nonzero vector $w$ . I know this part follows because the row space and column space of $B-A$ are the same, but I didn't think yet how to prove it rigorously. Since $w$ is nonzero, some entry of $w$ , say $w_i$ , is not zero. Now, let $e_i$ be a column vector such that the $i$ -th entry is $1$ and is the only nonzero entry. Then: $$ e_i^{\top}(B-A)e_i=e_i^{\top}(ww^{\top})e_i=(e_i^{\top}w)(w^{\top}e_i)=w_i^2, $$ where $w_i$ is the $i$ -th nonzero entry of $w$ . Since the entries of $B-A$ are in $\{-1,0,1\}$ because $A$ and $B$ are binary and $w_i \neq 0$ , then $w_i^2=1$ , which implies $w_i=\pm 1$ . With the above reasoning, I know that the nonzero entries of $w$ are either $-1$ or $1$ . Now, I just need to figure out how to prove that there are only two of these nonzero entries. If I could show that $w^{\top}w=2$ , I'd know that there are only two nonzero entries, although I'd not be able to tell their sign. But it'd be a start. I don't expect to get a complete solution from you (although this'd be nice). I just need some insight from more experienced people in linear algebra to go forward. Would you be able to give me some directions to follow? Maybe some properties of $B-A$ that I'm not seeing?","Suppose that and are binary square matrices of same dimension and that (same for ), i.e. they are symmetric and orthogonal. In addition, suppose so that for nonzero vectors and . I want to show that has the form , where is a column vector with only two nonzero entries whose values are . Since is symmetric (can be easily verified), then for some nonzero vector . I know this part follows because the row space and column space of are the same, but I didn't think yet how to prove it rigorously. Since is nonzero, some entry of , say , is not zero. Now, let be a column vector such that the -th entry is and is the only nonzero entry. Then: where is the -th nonzero entry of . Since the entries of are in because and are binary and , then , which implies . With the above reasoning, I know that the nonzero entries of are either or . Now, I just need to figure out how to prove that there are only two of these nonzero entries. If I could show that , I'd know that there are only two nonzero entries, although I'd not be able to tell their sign. But it'd be a start. I don't expect to get a complete solution from you (although this'd be nice). I just need some insight from more experienced people in linear algebra to go forward. Would you be able to give me some directions to follow? Maybe some properties of that I'm not seeing?","A B A=A^{\top}=A^{-1} B \text{rank}(B-A)=1 B-A=uv^{\top} u v B-A ww^{\top} w \pm 1 B-A B-A=ww^{\top} w B-A w w w_i e_i i 1 
e_i^{\top}(B-A)e_i=e_i^{\top}(ww^{\top})e_i=(e_i^{\top}w)(w^{\top}e_i)=w_i^2,
 w_i i w B-A \{-1,0,1\} A B w_i \neq 0 w_i^2=1 w_i=\pm 1 w -1 1 w^{\top}w=2 B-A","['linear-algebra', 'matrices', 'symmetric-matrices', 'orthogonal-matrices']"
25,Why does $\det A$ change sign when any $2$ columns of $A$ are interchanged?,Why does  change sign when any  columns of  are interchanged?,\det A 2 A,"I have tried to reason, using multilinear forms, the well-known fact that the determinant of a matrix $[A]$ changes its sign if any two columns of $[A]$ are interchanged. I am not confident if my reason is correct however, and would appreciate a nod or a refute. Thanks. (Disclaimer: I am confident that if my reasoning is correct, then it is unlikely that I am presenting anything novel here. I haven't however found the below-described argument -- using multilinear forms -- over math.stackexchange.com in response to similar queries, and therefore thought of seeking a review.) Here goes the reasoning: The definition of the determinant ( $det$ ) of a linear operator on a finite-dimensional vector space ( $\S 53$ of Finite-Dimensional Vector Spaces , $2^{\text{nd}}$ Ed., by Paul R. Halmos) implies the following. If $A$ is any linear operator on any $n$ -dimensional vector space $\mathcal V$ , if $\{x_1, \cdots, x_n\}$ is any basis in $\mathcal V$ , and if $w$ is any alternating $n$ -linear form on $\mathcal V$ , then $$\tag{1}\det \ A = \frac{w(Ax_1, \cdots, Ax_n)}{w(x_1, \cdots, x_n)}.$$ Under the hypothesis of (1), suppose some matrix $[A]$ together with $\{x_1, \cdots, x_n\}$ defines $A$ . Suppose $[A_1]$ is the matrix obtained by interchanging two columns, of $[A]$ , having any indices $h, k \ (h \neq k, 1 \leq h \leq n, 1 \leq k \leq n)$ . Let $A_1$ be the operator defined by $[A_1]$ together with $\{x_1, \cdots, x_n\}$ . It follows from the definition of a matrix ( $\S$ 37 of Finite-Dimensional Vector Spaces , $2^{\text{nd}}$ Ed., by Paul R. Halmos) that $Ax_h = A_1 x_k$ , $Ax_k = A_1 x_h$ , and that $Ax_i = A_1 x_i$ for $i = 1, \cdots, n$ except when $i$ equals $h$ or $k$ . This finding together with the skew-symmetric nature of $w$ ( $\S$ 30 of Finite-Dimensional Vector Spaces , $2^{\text{nd}}$ Ed., by Paul R. Halmos) implies that $$w(A_1 x_1, \cdots, A_1 x_n)= -w(A x_1, \cdots, A x_n).$$ Accordingly, we have $$ \det \ A_1  = \frac{w(A_1x_1, \cdots, A_1x_n)}{w(x_1, \cdots, x_n)} = \frac{-w(Ax_1, \cdots, Ax_n)}{w(x_1, \cdots, x_n)} = -\det \ A.$$ It is clear therefore that $\det\ [A_1] = -\det\ [A]$ .","I have tried to reason, using multilinear forms, the well-known fact that the determinant of a matrix changes its sign if any two columns of are interchanged. I am not confident if my reason is correct however, and would appreciate a nod or a refute. Thanks. (Disclaimer: I am confident that if my reasoning is correct, then it is unlikely that I am presenting anything novel here. I haven't however found the below-described argument -- using multilinear forms -- over math.stackexchange.com in response to similar queries, and therefore thought of seeking a review.) Here goes the reasoning: The definition of the determinant ( ) of a linear operator on a finite-dimensional vector space ( of Finite-Dimensional Vector Spaces , Ed., by Paul R. Halmos) implies the following. If is any linear operator on any -dimensional vector space , if is any basis in , and if is any alternating -linear form on , then Under the hypothesis of (1), suppose some matrix together with defines . Suppose is the matrix obtained by interchanging two columns, of , having any indices . Let be the operator defined by together with . It follows from the definition of a matrix ( 37 of Finite-Dimensional Vector Spaces , Ed., by Paul R. Halmos) that , , and that for except when equals or . This finding together with the skew-symmetric nature of ( 30 of Finite-Dimensional Vector Spaces , Ed., by Paul R. Halmos) implies that Accordingly, we have It is clear therefore that .","[A] [A] det \S 53 2^{\text{nd}} A n \mathcal V \{x_1, \cdots, x_n\} \mathcal V w n \mathcal V \tag{1}\det \ A = \frac{w(Ax_1, \cdots, Ax_n)}{w(x_1, \cdots, x_n)}. [A] \{x_1, \cdots, x_n\} A [A_1] [A] h, k \ (h \neq k, 1 \leq h \leq n, 1 \leq k \leq n) A_1 [A_1] \{x_1, \cdots, x_n\} \S 2^{\text{nd}} Ax_h = A_1 x_k Ax_k = A_1 x_h Ax_i = A_1 x_i i = 1, \cdots, n i h k w \S 2^{\text{nd}} w(A_1 x_1, \cdots, A_1 x_n)= -w(A x_1, \cdots, A x_n).  \det \ A_1  = \frac{w(A_1x_1, \cdots, A_1x_n)}{w(x_1, \cdots, x_n)} = \frac{-w(Ax_1, \cdots, Ax_n)}{w(x_1, \cdots, x_n)} = -\det \ A. \det\ [A_1] = -\det\ [A]","['linear-algebra', 'matrices', 'linear-transformations', 'determinant', 'multilinear-algebra']"
26,"Matrix A and B (both mxn) have the same four spaces, proof this: (Is my reasoning ok?)","Matrix A and B (both mxn) have the same four spaces, proof this: (Is my reasoning ok?)",,"Suppose the m by n matrices A and B have the same four subspaces. If they are both in row reduced echelon form, prove that F = G: $$A= \begin{bmatrix} I & F \\ 0 & 0 \\ \end{bmatrix} $$ $$ B=\begin{bmatrix} I & G \\ 0 & 0 \\ \end{bmatrix} $$ My reasoning: Because we know they have the same Nullspace, then \begin{array}{l} \text{if $Ax=0$ and $Bx=0$ then } \text{$Ax-Bx=0$} \text{ then $(A-B)x=0$ because x $\ne$ 0} \end{array} $$ \begin{bmatrix} 0 & F-G \\ 0 & 0 \\ \end{bmatrix}=0 $$ So, $F-G=0$ then $F=G$ ¿What do you think? ¿Is this ok? ¿How would you do this? Thank you.","Suppose the m by n matrices A and B have the same four subspaces. If they are both in row reduced echelon form, prove that F = G: My reasoning: Because we know they have the same Nullspace, then So, then ¿What do you think? ¿Is this ok? ¿How would you do this? Thank you.","A=
\begin{bmatrix}
I & F \\
0 & 0 \\
\end{bmatrix}
 
B=\begin{bmatrix}
I & G \\
0 & 0 \\
\end{bmatrix}
 \begin{array}{l}
\text{if Ax=0 and Bx=0 then }
\text{Ax-Bx=0}
\text{ then (A-B)x=0 because x \ne 0}
\end{array} 
\begin{bmatrix}
0 & F-G \\
0 & 0 \\
\end{bmatrix}=0
 F-G=0 F=G","['linear-algebra', 'matrices', 'solution-verification']"
27,"How to prove that a certain block matrix is positive semi definite, which depends on a undetermined submatrix","How to prove that a certain block matrix is positive semi definite, which depends on a undetermined submatrix",,"How should I proof the following matrix $$M = \begin{pmatrix} Z-A^TZA & -A^TZB\\ -B^TZA & -B^TZB \end{pmatrix},$$ to be positive semidefinite? The matrices $A\in \mathbb{R}^{n\times n}$ and $ B\in \mathbb{R}^{n\times m} $ are known. The matrix $Z\in R^{n\times n}$ is unknown and the actual goal is to construct the matrix $Z$ such that $M$ is positive semi-definite. I have tried the following things: constructing $Z$ as a diagonal matrix and by using a symbolic programming library (sympy) I found the expressions for the eigenvalues of $M$ . Here I found out that for this specific $Z$ , the eigenvalues will be indefinite (larger or smaller than zero) using theories off Schur Complement : Wikipedia Link . For example, in order for the third option of the link to be true; $Z-A^TZA \succ 0$ and $ -B^TZB-(-B^TZA)^T(Z-A^TZA)^{-1}(-A^TZB) \succeq 0$ , $(-B^TZA)^T= -A^TZB$ -> $Z$ should be symmetric. I think I could write this all out to be a larger Linear Matrix Inequality (LMI) and try to solve using python for example. However, I'm not sure if that is true and if this is the correct approach as this would take some time to implement this in a python code. Do you have any suggestions or feedback for a different approach? (My mathematical knowledge is not so great as I come from a mechanical/control engineering background. Furthermore, this is my first question on a stack website. I apologize for any trivial mistakes I made) EDIT: Extra info regarding the matrices $A$ , $B$ ; they are so-called state-space matrices which are part of a Linear Time-Invariant(LTI) dynamical model Wikipedia . The standard form is $x_{k+1}=Ax_k + Bu$ where $x \in \mathbb{R}^n$ represents the state of the system and $u \in \mathbb{R}^m$ is the input to this dynamical model. The matrix $A$ is the mapping of the current state of the system to the next state (in time) and $B$ represents the mapping of the input to the next state (in time). These state-space matrices ( $A, B$ ) are thus known as they represent the dynamics of a system. However, they have no special mathematical properties.","How should I proof the following matrix to be positive semidefinite? The matrices and are known. The matrix is unknown and the actual goal is to construct the matrix such that is positive semi-definite. I have tried the following things: constructing as a diagonal matrix and by using a symbolic programming library (sympy) I found the expressions for the eigenvalues of . Here I found out that for this specific , the eigenvalues will be indefinite (larger or smaller than zero) using theories off Schur Complement : Wikipedia Link . For example, in order for the third option of the link to be true; and , -> should be symmetric. I think I could write this all out to be a larger Linear Matrix Inequality (LMI) and try to solve using python for example. However, I'm not sure if that is true and if this is the correct approach as this would take some time to implement this in a python code. Do you have any suggestions or feedback for a different approach? (My mathematical knowledge is not so great as I come from a mechanical/control engineering background. Furthermore, this is my first question on a stack website. I apologize for any trivial mistakes I made) EDIT: Extra info regarding the matrices , ; they are so-called state-space matrices which are part of a Linear Time-Invariant(LTI) dynamical model Wikipedia . The standard form is where represents the state of the system and is the input to this dynamical model. The matrix is the mapping of the current state of the system to the next state (in time) and represents the mapping of the input to the next state (in time). These state-space matrices ( ) are thus known as they represent the dynamics of a system. However, they have no special mathematical properties.","M = \begin{pmatrix}
Z-A^TZA & -A^TZB\\
-B^TZA & -B^TZB
\end{pmatrix}, A\in \mathbb{R}^{n\times n}  B\in \mathbb{R}^{n\times m}  Z\in R^{n\times n} Z M Z M Z Z-A^TZA \succ 0  -B^TZB-(-B^TZA)^T(Z-A^TZA)^{-1}(-A^TZB) \succeq 0 (-B^TZA)^T= -A^TZB Z A B x_{k+1}=Ax_k + Bu x \in \mathbb{R}^n u \in \mathbb{R}^m A B A, B","['linear-algebra', 'matrices', 'positive-semidefinite', 'linear-matrix-inequality', 'schur-complement']"
28,A problem about characteristic polynomial,A problem about characteristic polynomial,,"I would like you to help me with a problem. If we consider a matrix $$A = \begin{bmatrix}        4 & 2 &-1 & 6\\        3 & 0 & 2 & 4\\       -1 & 0 & 3 & 0\\        0 & 0 & 5 & 0       \end{bmatrix}$$ the problem says that the characteristic polynomial is $P_A(\lambda) = (2-\lambda)(4-\lambda)(-1-\lambda)(5-\lambda)$ . However, when we calculate directly $$P_A(\lambda) = \det(A - \lambda I),$$ where $I$ is the identity metrix, we arrive at $$P_A(\lambda) = \lambda^4-7\lambda^3+5\lambda^2+52\lambda+40.$$ Hence, I have the following question. Is it possible to find another equivalent $A$ matrix using some property that preserves the characteristic polynomial and arrives at the proposed result? What am I doing wrong?","I would like you to help me with a problem. If we consider a matrix the problem says that the characteristic polynomial is . However, when we calculate directly where is the identity metrix, we arrive at Hence, I have the following question. Is it possible to find another equivalent matrix using some property that preserves the characteristic polynomial and arrives at the proposed result? What am I doing wrong?","A = \begin{bmatrix}
       4 & 2 &-1 & 6\\
       3 & 0 & 2 & 4\\
      -1 & 0 & 3 & 0\\
       0 & 0 & 5 & 0
      \end{bmatrix} P_A(\lambda) = (2-\lambda)(4-\lambda)(-1-\lambda)(5-\lambda) P_A(\lambda) = \det(A - \lambda I), I P_A(\lambda) = \lambda^4-7\lambda^3+5\lambda^2+52\lambda+40. A","['linear-algebra', 'matrices', 'characteristic-polynomial']"
29,Compatibility of Frobenius norm and $1$-norm.,Compatibility of Frobenius norm and -norm.,1,"I am trying to prove or disprove the compatibility (over $\mathbf{R}^n$ ) of the Frobenius norm of square matrices and the $1$ -norm for vectors. That is the norms $$\|A\| := \sqrt{\sum_{j=1}^n\sum_{i=1}^n a_{j,i}^2}$$ and $$\|\mathbf{x}\|_1 := \sum_{i=1}^n |x_i|$$ respectively. When I restrict my view to the case of $n=2$ , if $$A = \begin{bmatrix} a & b \\c& d \end{bmatrix}$$ and $\mathbf{x} = \begin{bmatrix} e \\ f \end{bmatrix}$ then I would essentially need to show that $$|ea+fb|+|ec+fd|\leq \sqrt{a^2 +b^2 +c^2+d^2} (|e|+|f|).$$ But this seems true and increasing the dimension would just make it more true. Any guidance on proving or a counterexample for $$\|A \mathbf{x}\|_1 \leq \|A\| \|\mathbf{x}\|_1$$ would be much appreciated.","I am trying to prove or disprove the compatibility (over ) of the Frobenius norm of square matrices and the -norm for vectors. That is the norms and respectively. When I restrict my view to the case of , if and then I would essentially need to show that But this seems true and increasing the dimension would just make it more true. Any guidance on proving or a counterexample for would be much appreciated.","\mathbf{R}^n 1 \|A\| := \sqrt{\sum_{j=1}^n\sum_{i=1}^n a_{j,i}^2} \|\mathbf{x}\|_1 := \sum_{i=1}^n |x_i| n=2 A = \begin{bmatrix} a & b \\c& d \end{bmatrix} \mathbf{x} = \begin{bmatrix} e \\ f \end{bmatrix} |ea+fb|+|ec+fd|\leq \sqrt{a^2 +b^2 +c^2+d^2} (|e|+|f|). \|A \mathbf{x}\|_1 \leq \|A\| \|\mathbf{x}\|_1","['linear-algebra', 'matrices', 'inequality', 'normed-spaces']"
30,Finding a specific basis to represent $S_3$,Finding a specific basis to represent,S_3,"I've found all the $3\times 3$ matrix representations of the permutation representation of $S_3$ with respect to the standard basis $\{e_1,e_2,e_3\}$ , but I'm then tasked with finding a different basis such that all the matrices are of the form: $$\begin{pmatrix}1&0&0\\0&*&*\\0&*&*\end{pmatrix}$$ Given my current understanding of the problem, this seems impossible, as if $\{b_1,b_2,b_3\}$ is our different basis, then we need $\sigma b_1=e_1$ for all $\sigma \in S_3$ , which is just not possible. Clearly I don't understand something, and would appreciate help in realising exactly what it is that I have misunderstood.","I've found all the matrix representations of the permutation representation of with respect to the standard basis , but I'm then tasked with finding a different basis such that all the matrices are of the form: Given my current understanding of the problem, this seems impossible, as if is our different basis, then we need for all , which is just not possible. Clearly I don't understand something, and would appreciate help in realising exactly what it is that I have misunderstood.","3\times 3 S_3 \{e_1,e_2,e_3\} \begin{pmatrix}1&0&0\\0&*&*\\0&*&*\end{pmatrix} \{b_1,b_2,b_3\} \sigma b_1=e_1 \sigma \in S_3","['linear-algebra', 'matrices', 'representation-theory', 'symmetric-groups']"
31,How can we apply this simple eigenvector expression 'repeatedly'?,How can we apply this simple eigenvector expression 'repeatedly'?,,"Let $A,B$ be linear operators on a complex vector space $V$ and suppose $$ABu = (\alpha + 2)Bu$$ where $u \in V$ is an eigenvector of $A$ with eigenvalue $\alpha$ and $\alpha \in \mathbb{C}$ . We can interpret this as either $Bu = 0$ or $Bu$ is an eigenvector of $A$ with eigenvalue $\alpha+2$ . I'm reading a proof which proves this equation above about the operators $A,B$ as a lemma and then claims that 'by applying the lemma repeatedly' we get the formula $$AB^ku = (\alpha+2k)B^ku$$ I have tried many things but cannot see how to apply it repeatedly. The lemma seems to a statement about $ABu$ , sticking many more $B$ 's seems like it would prevent us from using it. The context: $\pi$ is a representation of the Lie algebra $sl(2,\mathbb{C})$ acting on $V$ . Above I was letting $A = \pi(H)$ and $B = \pi(X)$ where $$H = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} \quad \quad X = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}$$ Calculation shows $[X,H] = HX - XH = 2X$ , and because $\pi$ is a Lie algebra homomorphism, we have $\pi([H,X]) = [\pi(H),\pi(X)] = 2\pi(X)$ . Playing with this equation is what proves the lemma above. Maybe playing with this repeatedly is what is needed.","Let be linear operators on a complex vector space and suppose where is an eigenvector of with eigenvalue and . We can interpret this as either or is an eigenvector of with eigenvalue . I'm reading a proof which proves this equation above about the operators as a lemma and then claims that 'by applying the lemma repeatedly' we get the formula I have tried many things but cannot see how to apply it repeatedly. The lemma seems to a statement about , sticking many more 's seems like it would prevent us from using it. The context: is a representation of the Lie algebra acting on . Above I was letting and where Calculation shows , and because is a Lie algebra homomorphism, we have . Playing with this equation is what proves the lemma above. Maybe playing with this repeatedly is what is needed.","A,B V ABu = (\alpha + 2)Bu u \in V A \alpha \alpha \in \mathbb{C} Bu = 0 Bu A \alpha+2 A,B AB^ku = (\alpha+2k)B^ku ABu B \pi sl(2,\mathbb{C}) V A = \pi(H) B = \pi(X) H = \begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix} \quad \quad X = \begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix} [X,H] = HX - XH = 2X \pi \pi([H,X]) = [\pi(H),\pi(X)] = 2\pi(X)","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'lie-groups', 'lie-algebras']"
32,Eigenvector matrix times diagonal matrix equals original matrix times eigenvector matrix?,Eigenvector matrix times diagonal matrix equals original matrix times eigenvector matrix?,,"Suppose we have a $n\times n$ real symmetric matrix $A$ , and $V=[v_1\ v_2\cdots v_n]$ whose columns are the eigenvectors corresponding to the $n$ eigenvalues $\lambda_1,\ldots,\lambda_n$ . Let $D$ be a diagonal matrix ${\rm diag}[\lambda_1\cdots\lambda_n]$ , where $\lambda_i$ are the eigenvalues of $A$ for $i=1,\ldots,n$ . How do we prove that $A𝑉 = 𝑉D$ ? edit: I realized what I said below is incorrect because multiplying $𝑉D$ does not give a diagonal matrix. But I am still confused as how I would know that $A𝑉 = 𝑉D$ when not given any numbers. I was trying to start this proof with expanding $𝑉D$ and I got another diagonal matrix with the diagonal entries being $v_1\lambda_1,\ldots,v_n\lambda_n$ and all other entries being $0$ . I am unsure how this could be equivalent to A𝑉 because wouldn't this resultant matrix not be a diagonal matrix? Or how would we know without knowing what the entries of A are?","Suppose we have a real symmetric matrix , and whose columns are the eigenvectors corresponding to the eigenvalues . Let be a diagonal matrix , where are the eigenvalues of for . How do we prove that ? edit: I realized what I said below is incorrect because multiplying does not give a diagonal matrix. But I am still confused as how I would know that when not given any numbers. I was trying to start this proof with expanding and I got another diagonal matrix with the diagonal entries being and all other entries being . I am unsure how this could be equivalent to A𝑉 because wouldn't this resultant matrix not be a diagonal matrix? Or how would we know without knowing what the entries of A are?","n\times n A V=[v_1\ v_2\cdots v_n] n \lambda_1,\ldots,\lambda_n D {\rm diag}[\lambda_1\cdots\lambda_n] \lambda_i A i=1,\ldots,n A𝑉 = 𝑉D 𝑉D A𝑉 = 𝑉D 𝑉D v_1\lambda_1,\ldots,v_n\lambda_n 0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
33,How to show that a quaternionic matrix is not a product of two involutions?,How to show that a quaternionic matrix is not a product of two involutions?,,"Problem. Let $$A=\begin{pmatrix}i&0\\0&1\end{pmatrix}$$ be a matrix over the real quaternion ring $\mathbb{H}.$ Prove that it is similar to its inverse, but is not a product of two involutions. Can it be a product of commutators of involutions? A square matrix $A$ over a ring is called similar to $B$ if there exists an invertible matrix $P$ such that $A=PBP^{-1}.$ A square matrix $T$ over a ring is called an involution if $T^2=I$ is the identity matrix. A square matrix $A$ over a ring is called a commutator of involution if there exist involutions $T, S$ such that $A=TST^{-1}S^{-1}.$ I have shown $$A^{-1}=\begin{pmatrix}-i&0\\0&1\end{pmatrix},$$ and $$A=\begin{pmatrix}ij&0\\0&1\end{pmatrix}A^{-1}\begin{pmatrix}ij&0\\0&1\end{pmatrix}^{-1}.$$ But I don't answer this remaining. I don't know why it is not a product of two involutions. If this is true, then it can be a product of commutators of involutions, can't it? Thanks for all your support.","Problem. Let be a matrix over the real quaternion ring Prove that it is similar to its inverse, but is not a product of two involutions. Can it be a product of commutators of involutions? A square matrix over a ring is called similar to if there exists an invertible matrix such that A square matrix over a ring is called an involution if is the identity matrix. A square matrix over a ring is called a commutator of involution if there exist involutions such that I have shown and But I don't answer this remaining. I don't know why it is not a product of two involutions. If this is true, then it can be a product of commutators of involutions, can't it? Thanks for all your support.","A=\begin{pmatrix}i&0\\0&1\end{pmatrix} \mathbb{H}. A B P A=PBP^{-1}. T T^2=I A T, S A=TST^{-1}S^{-1}. A^{-1}=\begin{pmatrix}-i&0\\0&1\end{pmatrix}, A=\begin{pmatrix}ij&0\\0&1\end{pmatrix}A^{-1}\begin{pmatrix}ij&0\\0&1\end{pmatrix}^{-1}.","['linear-algebra', 'abstract-algebra', 'matrices', 'matrix-decomposition']"
34,Number of operations and cost of matrix multiplication,Number of operations and cost of matrix multiplication,,"Let's suppose we have a $2 \times 3$ matrix $A$ and a $3 \times 4$ matrix $B$ . If we multiply these two matrices, $A B$ , we'll have $ 3$ multiplication and $2$ addition for each entry in the resultant $2\times 4$ matrix, which will make the total of $24$ multiplications and $16 $ additions , which will make it $40$ operations needed for matrix multiplication. Thus, the cost of matrix multiplication should be $40$ as there are $40$ operations done. However, I notice that addition is not included wherever I read about it. Please explain to me where I am wrong.","Let's suppose we have a matrix and a matrix . If we multiply these two matrices, , we'll have multiplication and addition for each entry in the resultant matrix, which will make the total of multiplications and additions , which will make it operations needed for matrix multiplication. Thus, the cost of matrix multiplication should be as there are operations done. However, I notice that addition is not included wherever I read about it. Please explain to me where I am wrong.",2 \times 3 A 3 \times 4 B A B  3 2 2\times 4 24 16  40 40 40,"['matrices', 'numerical-linear-algebra']"
35,All conjugacy classes of $\operatorname{SL}_2 \mathbb{F}_p$,All conjugacy classes of,\operatorname{SL}_2 \mathbb{F}_p,"For an odd prime $p$ , is the number of the conjugacy classes of $\operatorname{SL}_2 \mathbb{F}_p$ p+4 ? I showed a partial result: Let $A$ be a matrix. Consider its characteristic polynomial $p$ . Then over $\mathbb{F}_{p^2}$ , $p$ has roots. Write it $\xi, \zeta$ . Then since the determinant is $1$ , we have that $\zeta = \xi ^{-1}$ . And since the trace is in $\mathbb{F}_p$ , we have that $\xi + \xi ^{-1} \in \mathbb{F}_p$ . Thus $(\xi^{p-1} - 1)(\xi ^{p+1} - 1)=0$ The case that $\xi = \pm 1$ . In this case we have that $A = \pm \left( \begin{matrix}1 & * \\ 0 & 1 \end{matrix} \right)$ . (6 matrices are in this case.) The case that $\xi \not= \pm 1$ and $\xi^{p-1} = 1$ . In this case $\xi \in \mathbb{F}_p$ . So $A$ is conjugate to $\left( \begin{matrix}\xi & * \\ 0 & \xi \end{matrix} \right)$ ...? ( $(p-3) / 2$ matrices are in this case?) The case that $\xi \not= \pm 1$ and $\xi^{p+1} = 1$ . ...? From them how can I show the result?","For an odd prime , is the number of the conjugacy classes of p+4 ? I showed a partial result: Let be a matrix. Consider its characteristic polynomial . Then over , has roots. Write it . Then since the determinant is , we have that . And since the trace is in , we have that . Thus The case that . In this case we have that . (6 matrices are in this case.) The case that and . In this case . So is conjugate to ...? ( matrices are in this case?) The case that and . ...? From them how can I show the result?","p \operatorname{SL}_2 \mathbb{F}_p A p \mathbb{F}_{p^2} p \xi, \zeta 1 \zeta = \xi ^{-1} \mathbb{F}_p \xi + \xi ^{-1} \in \mathbb{F}_p (\xi^{p-1} - 1)(\xi ^{p+1} - 1)=0 \xi = \pm 1 A = \pm \left( \begin{matrix}1 & * \\ 0 & 1 \end{matrix} \right) \xi \not= \pm 1 \xi^{p-1} = 1 \xi \in \mathbb{F}_p A \left( \begin{matrix}\xi & * \\ 0 & \xi \end{matrix} \right) (p-3) / 2 \xi \not= \pm 1 \xi^{p+1} = 1","['linear-algebra', 'abstract-algebra', 'matrices']"
36,Counting words of length $n$ from $k$-sized alphabet with no substring of $k$ consecutive distinct letters,Counting words of length  from -sized alphabet with no substring of  consecutive distinct letters,n k k,"How many words of length $n$ are there, if we have an alphabet of $k$ distinct letters, but the words cannot contain any substring that is made of $k$ consecutive distinct letters, i.e, no $k$ -length substring that consists of the entire alphabet? Other than that, no restrictions apply: any amount of distinct letters may be used throughout the entire word, and any letter can be used as many times as we like, as long as every substring inside the length $n$ word complies with the rules above. There is the obvious case of $k=2$ which results in $2$ words, for every $n$ , because you can only start with either letter, and they alternate. For the larger case, I have come up with a recursive formula: $$C(n,k)=f(0,0,k)$$ $$f(i,d,k)=\begin{cases}\displaystyle(k-d) \cdot f(i+1,d+1,k) + \sum_{c=1}^df(i+1,c,k) & i<n,d<k\\ 1 &i=n,d<k\\ 0 &i>n\\ 0 &d\geq k \end{cases}$$ With $d$ being the current length of consecutive distinct letters, and $i$ the current word length. At every step, we can either use a letter other than the previous $d$ letters, in which case the length is increased by one, and the chain-length $d$ of distinct consecutive letters is also increased by one. In this case, there are $(k-d)$ such letters we can use at the stage, each subsequently resulting in the same contribution. Or, we can use a letter already in the last $d$ letters. In this case, the position of the letter matters. If we use the last of the $d$ letters, a whole new chain begins. If instead we use the second last $d$ letter, then a chain of length $2$ of consecutive distinct letters begins. The $3$ rd last would result in a chain of length $3$ and so on. I was wondering if there is a another way to count the amount of such words, perhaps using matrices or combinatorics?","How many words of length are there, if we have an alphabet of distinct letters, but the words cannot contain any substring that is made of consecutive distinct letters, i.e, no -length substring that consists of the entire alphabet? Other than that, no restrictions apply: any amount of distinct letters may be used throughout the entire word, and any letter can be used as many times as we like, as long as every substring inside the length word complies with the rules above. There is the obvious case of which results in words, for every , because you can only start with either letter, and they alternate. For the larger case, I have come up with a recursive formula: With being the current length of consecutive distinct letters, and the current word length. At every step, we can either use a letter other than the previous letters, in which case the length is increased by one, and the chain-length of distinct consecutive letters is also increased by one. In this case, there are such letters we can use at the stage, each subsequently resulting in the same contribution. Or, we can use a letter already in the last letters. In this case, the position of the letter matters. If we use the last of the letters, a whole new chain begins. If instead we use the second last letter, then a chain of length of consecutive distinct letters begins. The rd last would result in a chain of length and so on. I was wondering if there is a another way to count the amount of such words, perhaps using matrices or combinatorics?","n k k k n k=2 2 n C(n,k)=f(0,0,k) f(i,d,k)=\begin{cases}\displaystyle(k-d) \cdot f(i+1,d+1,k) + \sum_{c=1}^df(i+1,c,k) & i<n,d<k\\ 1 &i=n,d<k\\ 0 &i>n\\ 0 &d\geq k
\end{cases} d i d d (k-d) d d d 2 3 3","['combinatorics', 'matrices', 'discrete-mathematics', 'recurrence-relations', 'combinatorics-on-words']"
37,"Derivative of the $p$-Schatten norm of a symmetric matrix, raised to the $p$th power.","Derivative of the -Schatten norm of a symmetric matrix, raised to the th power.",p p,"Given a symmetric matrix $S$ , I would like to calculate the derivative of the $p$ -Schatten norm of $S$ raised to the $p$ th power i.e. $\frac{\partial\|S\|_p^p}{\partial S}$ where $\|S\|_p$ is the $p$ -Schatten norm of $S$ .","Given a symmetric matrix , I would like to calculate the derivative of the -Schatten norm of raised to the th power i.e. where is the -Schatten norm of .",S p S p \frac{\partial\|S\|_p^p}{\partial S} \|S\|_p p S,"['linear-algebra', 'matrices', 'matrix-calculus']"
38,Equivalence classes of solutions to a matrix equation,Equivalence classes of solutions to a matrix equation,,"I am wondering whether the following problem can be mathematically formalized an solved. Namely, I want to quantify how many solutions there are to the equation $$U_1 \cdot U_2 \cdot U_3 \cdot U_4 = U_4 \cdot U_3 \cdot U_2 \cdot U_1$$ where $\{U_a\}_{a=1}^4$ are unitary matrices of arbitrary (but the same) dimensions. For example, using the Pauli matrices, $U_1 = U_2 =\sigma_x$ with $U_3 = U_4 = \sigma_z $ would be a simple solution with $2\times 2$ matrices. Clearly, there are infinitely many solutions, but one can organize them into enumerable equivalence classes. For example, replacing $U_a\mapsto U_a e^{\mathrm{i} \phi_a}$ maps one solution to another equivalent solution. Similarly, if $V$ is a unitary matrix of the same dimension, then $U_a \mapsto V\cdot U_a \cdot V^{-1}$ (with the same $V$ for all $a\in\{1,2,3,4\}$ ) gives another equivalent solution. However, not all solutions of the same dimension are equivalent; for example, the solution $U_1 = U_2 = \sigma_x$ and $U_3 = U_4 = \sigma_z$ cannot be related to the solution $U_1 = U_3 = \sigma_x$ and $U_2 = U_4 = \sigma_z$ using the listed equivalence transformations. Finally, one can introduce the notion of ""reducible"" solutions, i.e. ones that are equivalent to $U_a$ 's with a block-diagonal structure; and similarly, there are presumably ""irreducible"" higher-dimensional solutions. Clearly, the knowledge of irreducible solutions is sufficient for the identification of all possible solutions. The two example solutions listed above are both irreducible in this sense. Thus, trying to be more precise now, I am wondering how to identify all equivalence classes of irreducible solutions to the equation at the beginning. Remark: The fact that I chose to describe this problem using group-theoretical terminology is not an accident. The formulated problem should indeed be equivalent to looking for irreducible representations of the following free group with a constraint, $$\Gamma = \left<a,b,c,d\,|\, a.b.c.d.a^{-1}.b^{-1}.c^{-1}.d^{-1}=1\right>.$$ This group corresponds to the maximal torsion-free subgroup of the Fuchsian symmetry group of the $\{8,8\}$ -tessellation of the hyperbolic plane. Perhaps this offers some of you helpful geometric insights that I overlooked.","I am wondering whether the following problem can be mathematically formalized an solved. Namely, I want to quantify how many solutions there are to the equation where are unitary matrices of arbitrary (but the same) dimensions. For example, using the Pauli matrices, with would be a simple solution with matrices. Clearly, there are infinitely many solutions, but one can organize them into enumerable equivalence classes. For example, replacing maps one solution to another equivalent solution. Similarly, if is a unitary matrix of the same dimension, then (with the same for all ) gives another equivalent solution. However, not all solutions of the same dimension are equivalent; for example, the solution and cannot be related to the solution and using the listed equivalence transformations. Finally, one can introduce the notion of ""reducible"" solutions, i.e. ones that are equivalent to 's with a block-diagonal structure; and similarly, there are presumably ""irreducible"" higher-dimensional solutions. Clearly, the knowledge of irreducible solutions is sufficient for the identification of all possible solutions. The two example solutions listed above are both irreducible in this sense. Thus, trying to be more precise now, I am wondering how to identify all equivalence classes of irreducible solutions to the equation at the beginning. Remark: The fact that I chose to describe this problem using group-theoretical terminology is not an accident. The formulated problem should indeed be equivalent to looking for irreducible representations of the following free group with a constraint, This group corresponds to the maximal torsion-free subgroup of the Fuchsian symmetry group of the -tessellation of the hyperbolic plane. Perhaps this offers some of you helpful geometric insights that I overlooked.","U_1 \cdot U_2 \cdot U_3 \cdot U_4 = U_4 \cdot U_3 \cdot U_2 \cdot U_1 \{U_a\}_{a=1}^4 U_1 = U_2 =\sigma_x U_3 = U_4 = \sigma_z  2\times 2 U_a\mapsto U_a e^{\mathrm{i} \phi_a} V U_a \mapsto V\cdot U_a \cdot V^{-1} V a\in\{1,2,3,4\} U_1 = U_2 = \sigma_x U_3 = U_4 = \sigma_z U_1 = U_3 = \sigma_x U_2 = U_4 = \sigma_z U_a \Gamma = \left<a,b,c,d\,|\, a.b.c.d.a^{-1}.b^{-1}.c^{-1}.d^{-1}=1\right>. \{8,8\}","['matrices', 'group-theory', 'representation-theory', 'equivalence-relations', 'hyperbolic-geometry']"
39,Why do $ \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$ and $ \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}$ generate a free semigroup?,Why do  and  generate a free semigroup?, \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}  \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix},"The title, basically (we do not include inverses, but I assume it would still hold if we did? But we would generate the free group and not the semigroup). We need to show that we cannot get the same matrix by two different sequences of multiplying these two matrices together. I tried finding the general form but couldn't really. Hints preferred - please don't spoil it unless necessary. Thanks a lot in advance!","The title, basically (we do not include inverses, but I assume it would still hold if we did? But we would generate the free group and not the semigroup). We need to show that we cannot get the same matrix by two different sequences of multiplying these two matrices together. I tried finding the general form but couldn't really. Hints preferred - please don't spoil it unless necessary. Thanks a lot in advance!",,"['abstract-algebra', 'matrices', 'semigroups']"
40,Finding a $3 \times 3$ matrix that only has 1 non-trivial invariant subspace of $\mathbb{R}^3$.,Finding a  matrix that only has 1 non-trivial invariant subspace of .,3 \times 3 \mathbb{R}^3,"Does such a matrix exist, and if so how can I find it? My intuition was the matrix $$M = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix}$$ as the only eigenspace is $\text{span} \{\mathbf{e}_1\}$ , but I'd also need to prove that there's no other 2-dimensional invariant subspace. If we let $\mathbf{u}$ and $\mathbf{w}$ be arbitrary linearly independent nonzero vectors in $\mathbb{R}$ , we want $M\mathbf{u} = \alpha \mathbf{u} + \beta \mathbf{w}$ and $M\mathbf{w} = \gamma \mathbf{u} + \delta \mathbf{w}$ . We then get the system of equations $$\begin{align*} u_1 + u_2 &= \alpha u_1 + \beta w_1 \\ u_2 + u_3 &= \alpha u_2 + \beta w_3 \\ u_3 &= \alpha u_3 + \beta w_3 \\ w_1 + w_2 &= \gamma u_1 + \delta w_1 \\ w_2 + w_3 &= \gamma u_2 + \delta w_3 \\ w_3 &= \gamma u_3 + \delta w_3 \end{align*}$$ I'm sure we could solve this by turning it into a matrix and RREF and finding values of the constants to make it nonzero, but I don't feel like this is the way. With some research I found the theorem: Let $T: V \to V$ be a linear operator on a finite dimensional vector space. If $T$ is diagonalizable, and $W$ is a $T$ -invariant subspace of $W$ , then the restriction of $T$ to $W$ , $T_W$ , is also diagonalizable. Does this mean, because the only eigenspace is the above and is one-dimensional, any 2-dimensional subspace cannot be invariant because the eigenspaces of $T_W$ must be eigenspaces of $T$ ? Also, does this imply that any $n \times n$ Jordan matrix only has a single non-trivial invariant subspace? And does this all generalise to $\mathbb{C}$ as well? Thanks for any help!","Does such a matrix exist, and if so how can I find it? My intuition was the matrix as the only eigenspace is , but I'd also need to prove that there's no other 2-dimensional invariant subspace. If we let and be arbitrary linearly independent nonzero vectors in , we want and . We then get the system of equations I'm sure we could solve this by turning it into a matrix and RREF and finding values of the constants to make it nonzero, but I don't feel like this is the way. With some research I found the theorem: Let be a linear operator on a finite dimensional vector space. If is diagonalizable, and is a -invariant subspace of , then the restriction of to , , is also diagonalizable. Does this mean, because the only eigenspace is the above and is one-dimensional, any 2-dimensional subspace cannot be invariant because the eigenspaces of must be eigenspaces of ? Also, does this imply that any Jordan matrix only has a single non-trivial invariant subspace? And does this all generalise to as well? Thanks for any help!",M = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix} \text{span} \{\mathbf{e}_1\} \mathbf{u} \mathbf{w} \mathbb{R} M\mathbf{u} = \alpha \mathbf{u} + \beta \mathbf{w} M\mathbf{w} = \gamma \mathbf{u} + \delta \mathbf{w} \begin{align*} u_1 + u_2 &= \alpha u_1 + \beta w_1 \\ u_2 + u_3 &= \alpha u_2 + \beta w_3 \\ u_3 &= \alpha u_3 + \beta w_3 \\ w_1 + w_2 &= \gamma u_1 + \delta w_1 \\ w_2 + w_3 &= \gamma u_2 + \delta w_3 \\ w_3 &= \gamma u_3 + \delta w_3 \end{align*} T: V \to V T W T W T W T_W T_W T n \times n \mathbb{C},"['linear-algebra', 'matrices', 'invariant-subspace']"
41,Understanding the proof of the Full SVD from the Economy SVD,Understanding the proof of the Full SVD from the Economy SVD,,"$\textbf{The Economy SVD}$ : Let $r:=rank(A)\leq \min(m,n)$ where $A\in\mathbb{R^{m\times n}}$ . We know that the number of singular values of $A$ or the cardinality of the set $\Sigma(A):=\{\sigma_{i}|i=1,2,\cdots,m\}$ is $r$ . Considering $AA^{T}$ , we know that it is symmetric and thus by the Jordan-Schur's decomposition, we may write: $$ AA^{T}=QDQ^{T} $$ Here $D$ which is a diagonal matrix, houses the eigenvalues of $AA^{T}$ which are $r$ in total since $rank(A)=rank(AA^{T})$ . Now this means that if we consider $D_{r}$ which is an $r\times r$ diagonal matrix that removes the $0$ 's from the diagonal entries and only contains that $r$ eigenvalues $\Sigma_{r}^{2}$ , as well as $Q_{r}$ excluding the zero orthonormal vectors we can write : $$ AA^{T}Q_{r}\Sigma_{r}^{-1}=Q_{r}\Sigma_{r} $$ Now let $U_{r}:=Q_{r}$ , $S_{r}:=\Sigma_{r}$ , and $V_{r}:=A^{T}Q_{r}\Sigma_{r}^{-1}=A^{T}U_{r}S_{r}^{-1}$ , we may write: $$ AV_{r}=U_{r}S_{r} $$ where $U_{r}^{T}U_{r}=I_{r}$ and it can also be shown that $V_{r}^{T}V_{r}=I_{r}$ which lets us to rewrite the equation above as: $$ A=U_{r}S_{r}V_{r}^{T} $$ Request : I hope someone can explain the beginning of the proof as I didn't understand it. The proof below essentially derives the full scale SVD from the economy SVD $\textbf{Theorem (Full Scale SVD)}$ : Consider the matrix $A\in\mathbb{R}^{m\times n}$ with $\operatorname{rank}(A)=r\leq p=\min(m,n)$ . We claim that there exist two orthogonal matrices $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$ and a diagonal matrix $S\in\mathbb{R}^{m\times n}$ , such that : $$ AV=US\iff A=USV^{T}\iff S=U^{T}AV $$ where $S$ has the singular values of $A$ on its diagonal $\operatorname{diag}(S):=\begin{bmatrix}\sigma_{1}&\sigma_{2}&\cdots&\sigma_{p}\end{bmatrix}$ . Furthermore, $\sigma_{1},\sigma_{2},\cdots,\sigma_{r}>0$ and the remaining $p-r$ singular values $\sigma_{r+1}=\sigma_{r+2}=\cdots=\sigma_{p}=0$ . The columns of $U$ and the columns of $V$ are called the left-singular vectors and right-singular vectors of $A$ , respectively . $\textbf{Proof}$ . Construct $V:=[V_{r}\;\;V_{n-r}]$ and $U:=[U_{r}\;\;U_{n-r}]$ , by adding orthogonal vectors so that $V$ forms a basis for $\mathbb{R}^{n}$ and $U$ forms a basis for $\mathbb{R}^{m}$ . Recall that $Av_i \in \operatorname{Im}(A) = \{y \in \Bbb C^n \mid y = Ax; x \in \Bbb C^n\}$ with $Av_i = \sigma_i u_i$ for $i = 1,\dots,r$ and $\sigma_i > 0$ . Moreover, $u_i = \frac 1{\sigma_i } Av_i$ form a basis for $\operatorname{Im}(A)$ for $i = 1,\dots, r$ . Thus, $v_{r+1},\dots,v_n \in \ker(A)$ i.e. $AV_{n-r} = 0$ , since otherwise $\operatorname{rank}(A) > r$ (proof by contradiction) The proof then goes on...",": Let where . We know that the number of singular values of or the cardinality of the set is . Considering , we know that it is symmetric and thus by the Jordan-Schur's decomposition, we may write: Here which is a diagonal matrix, houses the eigenvalues of which are in total since . Now this means that if we consider which is an diagonal matrix that removes the 's from the diagonal entries and only contains that eigenvalues , as well as excluding the zero orthonormal vectors we can write : Now let , , and , we may write: where and it can also be shown that which lets us to rewrite the equation above as: Request : I hope someone can explain the beginning of the proof as I didn't understand it. The proof below essentially derives the full scale SVD from the economy SVD : Consider the matrix with . We claim that there exist two orthogonal matrices and and a diagonal matrix , such that : where has the singular values of on its diagonal . Furthermore, and the remaining singular values . The columns of and the columns of are called the left-singular vectors and right-singular vectors of , respectively . . Construct and , by adding orthogonal vectors so that forms a basis for and forms a basis for . Recall that with for and . Moreover, form a basis for for . Thus, i.e. , since otherwise (proof by contradiction) The proof then goes on...","\textbf{The Economy SVD} r:=rank(A)\leq \min(m,n) A\in\mathbb{R^{m\times n}} A \Sigma(A):=\{\sigma_{i}|i=1,2,\cdots,m\} r AA^{T} 
AA^{T}=QDQ^{T}
 D AA^{T} r rank(A)=rank(AA^{T}) D_{r} r\times r 0 r \Sigma_{r}^{2} Q_{r} 
AA^{T}Q_{r}\Sigma_{r}^{-1}=Q_{r}\Sigma_{r}
 U_{r}:=Q_{r} S_{r}:=\Sigma_{r} V_{r}:=A^{T}Q_{r}\Sigma_{r}^{-1}=A^{T}U_{r}S_{r}^{-1} 
AV_{r}=U_{r}S_{r}
 U_{r}^{T}U_{r}=I_{r} V_{r}^{T}V_{r}=I_{r} 
A=U_{r}S_{r}V_{r}^{T}
 \textbf{Theorem (Full Scale SVD)} A\in\mathbb{R}^{m\times n} \operatorname{rank}(A)=r\leq p=\min(m,n) U\in\mathbb{R}^{m\times m} V\in\mathbb{R}^{n\times n} S\in\mathbb{R}^{m\times n} 
AV=US\iff A=USV^{T}\iff S=U^{T}AV
 S A \operatorname{diag}(S):=\begin{bmatrix}\sigma_{1}&\sigma_{2}&\cdots&\sigma_{p}\end{bmatrix} \sigma_{1},\sigma_{2},\cdots,\sigma_{r}>0 p-r \sigma_{r+1}=\sigma_{r+2}=\cdots=\sigma_{p}=0 U V A \textbf{Proof} V:=[V_{r}\;\;V_{n-r}] U:=[U_{r}\;\;U_{n-r}] V \mathbb{R}^{n} U \mathbb{R}^{m} Av_i \in \operatorname{Im}(A) = \{y \in \Bbb C^n \mid y = Ax; x \in \Bbb C^n\} Av_i = \sigma_i u_i i = 1,\dots,r \sigma_i > 0 u_i = \frac 1{\sigma_i } Av_i \operatorname{Im}(A) i = 1,\dots, r v_{r+1},\dots,v_n \in \ker(A) AV_{n-r} = 0 \operatorname{rank}(A) > r","['linear-algebra', 'matrices', 'proof-explanation', 'svd']"
42,How to prove a block matrix is positive definite?,How to prove a block matrix is positive definite?,,"Suppose there is a Hermitian matrix $W\in\mathbb{C}^{mn\times mn}$ with size $mn\times mn$ . Denote $W_{ij}$ as the submatrix with size $n\times n$ in the following position: \begin{equation} W=\begin{bmatrix} W_{11}& W_{12} &\cdots &W_{1m}\\ W_{21} &W_{22} &\cdots &W_{2m}\\ \vdots &\vdots &\ddots &\vdots \\ W_{m1}& W_{m2}& \cdots &W_{mm}\\ \end{bmatrix} \end{equation} $W_{ij}$ is known to be Hermitian and positive semi-definite. What kind of condition can guarantee that $W$ is positive definite? Do we have some results like if $W_{ii}\succ \sum_{j\neq i}W_{ij} $ for each $i$ , then $W$ is positive definite?","Suppose there is a Hermitian matrix with size . Denote as the submatrix with size in the following position: is known to be Hermitian and positive semi-definite. What kind of condition can guarantee that is positive definite? Do we have some results like if for each , then is positive definite?","W\in\mathbb{C}^{mn\times mn} mn\times mn W_{ij} n\times n \begin{equation}
W=\begin{bmatrix}
W_{11}& W_{12} &\cdots &W_{1m}\\
W_{21} &W_{22} &\cdots &W_{2m}\\
\vdots &\vdots &\ddots &\vdots \\
W_{m1}& W_{m2}& \cdots &W_{mm}\\
\end{bmatrix}
\end{equation} W_{ij} W W_{ii}\succ \sum_{j\neq i}W_{ij}  i W","['matrices', 'positive-definite', 'block-matrices', 'matrix-analysis']"
43,"$A \in GL_n (\mathbb C)$ and $N$ nilpotent matrix, A and N commute. Show there exists B such as $B^{2} = A + N$","and  nilpotent matrix, A and N commute. Show there exists B such as",A \in GL_n (\mathbb C) N B^{2} = A + N,"Let $n \in \mathbb N^{*}$ , $A \in GL_n (\mathbb C)$ and $N \in M_n (\mathbb C)$ with $N$ Nilpotent such as $AN=NA$ . Prove that there exists $B \in M_n (\mathbb C) $ such as $B^{2} = A + N$ Since $A$ and $N$ are triangularisable  and are commuting matrices, $A$ and $N$ are simultaneously triangularisable. There exists $P \in GL_n (\mathbb C)$ such as $A=P\left(     \begin{array}{ccccc}     \lambda_1                                    \\       & \ddots             &   & \huge*\\       &               & \ddots                \\       & \huge0 &   & \lambda_n      \end{array}     \right)P^{-1}\:\:\:$ with $(\lambda_1,\dots,\lambda_n)$ the eigenvalues of $A$ . $A$ is invertible thus $(\lambda_1,\dots,\lambda_n) \ne(0,\dots,0)$ and $N=P\left(     \begin{array}{ccccc}     0                                    \\       & \ddots             &   & \huge{*'}\\       &               & \ddots                \\       & \huge0 &   & 0     \end{array}     \right)P^{-1}$ Let's set $T=\left(     \begin{array}{ccccc}     \lambda_1                                    \\       & \ddots             &   & \huge*\\       &               & \ddots                \\       & \huge0 &   & \lambda_n      \end{array}     \right)+\left(     \begin{array}{ccccc}     0                                    \\       & \ddots             &   & \huge{*'}\\       &               & \ddots                \\       & \huge0 &   & 0     \end{array}     \right)=\left(     \begin{array}{ccccc}     \lambda_1                                    \\       & \ddots             &   & \huge*''\\       &               & \ddots                \\       & \huge0 &   & \lambda_n      \end{array}     \right)$ Let's define $ L \in M_n (\mathbb C)$ by $\forall (i,j) \in [\![1;n]\!]^{2}, L_{i,j}=\left\{     \begin{array}{lll}         0 & \mbox{if } i>j \\         \alpha_i & \mbox{if } i=j \\         \frac{1}{\alpha_i+\alpha_j} \times(T_i,j - \sum_{k=i+1}^{j-1} L_{i,k} L_{k,j}) &\mbox{else}     \end{array} \right.$ , with $(\alpha_1,\dots,\alpha_n) \:\:$ square roots of $(\lambda_1,\dots,\lambda_n)$ . The diagonal of L is well defined because for all $i$ , $\alpha_i \ne 0$ since $\lambda_i \ne 0$ . The upper part of L is defined step by step, the diagonal then the first subdiagonal, then the second subdiagonal, and so on; (for $j-i$ going from $0$ to $n-1$ , with $(i,j) \in [\![1;n]\!]^{2}$ and $j\geq i$ ). $L$ is an upper triangular matrix so $L^{2}$ is also an upper triangular matrix. Let $(i,j) \in [\![1;n]\!]^{2}$ and $j> i$ $L^{2}_{i,i}= \alpha_i^{2} = \lambda_i =T_{i,i}\\ L^{2}_{i,j}= \sum_{k=0}^{n}L_{i,k}L_{k,j}=\sum_{k=i}^{j}L_{i,k}L_{k,j}= L_{i,i}L_{i,j}+ \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j} +  L_{i,j}L_{j,j} = L_{i,j} ( \alpha_i+\alpha_j) + \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j}=\frac{1}{\alpha_i+\alpha_j} \times(T_i,j - \sum_{k=i+1}^{j-1} L_{i,k} L_{k,j}) \times ( \alpha_i+\alpha_j) + \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j}=T_{i,j}$ ie $L^{2}=T$ let's pose $B=PLP^{-1}$ we have $ B^{2}= PL^{2}P^{-1}= PTP^{-1}= A+N$ Several questions: Is the definition of $L$ correct? Is there another method, which does not use the matrix L ?","Let , and with Nilpotent such as . Prove that there exists such as Since and are triangularisable  and are commuting matrices, and are simultaneously triangularisable. There exists such as with the eigenvalues of . is invertible thus and Let's set Let's define by , with square roots of . The diagonal of L is well defined because for all , since . The upper part of L is defined step by step, the diagonal then the first subdiagonal, then the second subdiagonal, and so on; (for going from to , with and ). is an upper triangular matrix so is also an upper triangular matrix. Let and ie let's pose we have Several questions: Is the definition of correct? Is there another method, which does not use the matrix L ?","n \in \mathbb N^{*} A \in GL_n (\mathbb C) N \in M_n (\mathbb C) N AN=NA B \in M_n (\mathbb C)  B^{2} = A + N A N A N P \in GL_n (\mathbb C) A=P\left(
    \begin{array}{ccccc}
    \lambda_1                                    \\
      & \ddots             &   & \huge*\\
      &               & \ddots                \\
      & \huge0 &   & \lambda_n 
    \end{array}
    \right)P^{-1}\:\:\: (\lambda_1,\dots,\lambda_n) A A (\lambda_1,\dots,\lambda_n) \ne(0,\dots,0) N=P\left(
    \begin{array}{ccccc}
    0                                    \\
      & \ddots             &   & \huge{*'}\\
      &               & \ddots                \\
      & \huge0 &   & 0
    \end{array}
    \right)P^{-1} T=\left(
    \begin{array}{ccccc}
    \lambda_1                                    \\
      & \ddots             &   & \huge*\\
      &               & \ddots                \\
      & \huge0 &   & \lambda_n 
    \end{array}
    \right)+\left(
    \begin{array}{ccccc}
    0                                    \\
      & \ddots             &   & \huge{*'}\\
      &               & \ddots                \\
      & \huge0 &   & 0
    \end{array}
    \right)=\left(
    \begin{array}{ccccc}
    \lambda_1                                    \\
      & \ddots             &   & \huge*''\\
      &               & \ddots                \\
      & \huge0 &   & \lambda_n 
    \end{array}
    \right)  L \in M_n (\mathbb C) \forall (i,j) \in [\![1;n]\!]^{2}, L_{i,j}=\left\{
    \begin{array}{lll}
        0 & \mbox{if } i>j \\
        \alpha_i & \mbox{if } i=j \\
        \frac{1}{\alpha_i+\alpha_j} \times(T_i,j - \sum_{k=i+1}^{j-1} L_{i,k} L_{k,j}) &\mbox{else}
    \end{array}
\right. (\alpha_1,\dots,\alpha_n) \:\: (\lambda_1,\dots,\lambda_n) i \alpha_i \ne 0 \lambda_i \ne 0 j-i 0 n-1 (i,j) \in [\![1;n]\!]^{2} j\geq i L L^{2} (i,j) \in [\![1;n]\!]^{2} j> i L^{2}_{i,i}= \alpha_i^{2} = \lambda_i =T_{i,i}\\
L^{2}_{i,j}= \sum_{k=0}^{n}L_{i,k}L_{k,j}=\sum_{k=i}^{j}L_{i,k}L_{k,j}= L_{i,i}L_{i,j}+ \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j} +  L_{i,j}L_{j,j} = L_{i,j} ( \alpha_i+\alpha_j) + \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j}=\frac{1}{\alpha_i+\alpha_j} \times(T_i,j - \sum_{k=i+1}^{j-1} L_{i,k} L_{k,j}) \times ( \alpha_i+\alpha_j) + \sum_{k=i+1}^{j-1}L_{i,k}L_{k,j}=T_{i,j} L^{2}=T B=PLP^{-1}  B^{2}= PL^{2}P^{-1}= PTP^{-1}= A+N L","['linear-algebra', 'matrices', 'nilpotence']"
44,Proving there is no matrix in $\mathbb{F}_2^{2\times2}$ that commutes with every invertible matrix,Proving there is no matrix in  that commutes with every invertible matrix,\mathbb{F}_2^{2\times2},"Consider $\mathbb{F}_2^{2\times2}$ , the $2\times2$ -matrices over the finite field $\mathbb{F}_2$ . It seems to me (by trial and intuition, if I'm being honest), that there should be no matrix (besides $\mathbf{1}, \mathbf{0}$ ) that would commute with every invertible matrix in $\mathbb{F}_2^{2\times2}$ . Note that I am not requiring that this matrix commute with all other matrices in $\mathbb{F}_2^{2\times2}$ , only with the invertible ones. For one, we do know that the group of invertible matrices in $\mathbb{F}_2^{2\times2}$ has trivial center, so I would only need to check singular matrices. I tried to prove this by brute-force calculation, but since that is rather tedious, I would be interested to know a more analytical approach to this problem (or if I'm mistaken entirely).","Consider , the -matrices over the finite field . It seems to me (by trial and intuition, if I'm being honest), that there should be no matrix (besides ) that would commute with every invertible matrix in . Note that I am not requiring that this matrix commute with all other matrices in , only with the invertible ones. For one, we do know that the group of invertible matrices in has trivial center, so I would only need to check singular matrices. I tried to prove this by brute-force calculation, but since that is rather tedious, I would be interested to know a more analytical approach to this problem (or if I'm mistaken entirely).","\mathbb{F}_2^{2\times2} 2\times2 \mathbb{F}_2 \mathbf{1}, \mathbf{0} \mathbb{F}_2^{2\times2} \mathbb{F}_2^{2\times2} \mathbb{F}_2^{2\times2}","['linear-algebra', 'matrices']"
45,Finding $|A|^2+|B|^2$ if $A=\text{adj} (B)-B^T$ and $B=\text{adj} (A)-A^T$,Finding  if  and,|A|^2+|B|^2 A=\text{adj} (B)-B^T B=\text{adj} (A)-A^T,"Consider two $3\times3$ matrices $A$ and $B$ satisfying $A=\text{adj} (B)-B^T$ and $B=\text{adj} (A)-A^T$ (where $C^T$ denotes transpose of matrix $C$ ). If $A$ is a non-singular matrix, then find $AB$ and $|A|^2+|B|^2$ (where $|C|$ denotes determinant of matrix $C$ ). Also, find $|\text{adj} (2B^{-1})|$ . Given, $A=\text{adj} (B)-B^T$ . Taking transpose on both sides, I get $A^T=(\text{adj} B)^T-B$ . Putting $B=\text{adj} (A)-A^T$ in it, I get, $A^T=(\text{adj}B)^T-\text{adj(A)}+A^T\implies\text{adj} A=(\text{adj} B)^T$ . Taking determinant on both sides, I get $|A|^2=|B|^2$ (because $|\text{adj} C|=|C|^{n-1}$ where $n$ is the order of $C$ and $|C|=|C^T|)$ . Also, since $A$ is non-singular, if I divide $B=\text{adj} (A)-A^T$ by $|A|$ , I get $\dfrac B{|A|} = A^{-1} - \dfrac{A^T}{|A|} \implies  \dfrac{B + A^T}{|A|} = A^{-1}$ . Taking determinant on both sides, I get $\dfrac{|B+A^T|}{|A|^3} = \dfrac1{|A|} \implies |B + A^T| = |A|^2$ (because $|kC| = k^n|C|$ , where $k$ is any constant (here $\frac1{|A|}$ ) and $n$ is order of matrix $C$ . Also, $|C^{-1}|=\frac1{|C|}$ ). Not able to proceed next. EDIT: After @Upayan De's answer, $|B|=8$ . Now, $\text{adj}(kC)=k^{n-1}\text{adj}C$ , where $k$ is a constant and $n$ is the order of matrix $C$ . So, $\text{adj}(2B^{-1})=4\text{adj}(B^{-1})$ . So, $|\text{adj}(2B^{-1})|=|4\text{adj}(B^{-1})|=64|\text{adj}(B^{-1})|$ . Now, $|\text{adj}(B^{-1})|=(|\text{adj}B|)^{-1}=(|B|^2)^{-1}=1/64$ . So, $|\text{adj}(2B^{-1})|=1$","Consider two matrices and satisfying and (where denotes transpose of matrix ). If is a non-singular matrix, then find and (where denotes determinant of matrix ). Also, find . Given, . Taking transpose on both sides, I get . Putting in it, I get, . Taking determinant on both sides, I get (because where is the order of and . Also, since is non-singular, if I divide by , I get . Taking determinant on both sides, I get (because , where is any constant (here ) and is order of matrix . Also, ). Not able to proceed next. EDIT: After @Upayan De's answer, . Now, , where is a constant and is the order of matrix . So, . So, . Now, . So,",3\times3 A B A=\text{adj} (B)-B^T B=\text{adj} (A)-A^T C^T C A AB |A|^2+|B|^2 |C| C |\text{adj} (2B^{-1})| A=\text{adj} (B)-B^T A^T=(\text{adj} B)^T-B B=\text{adj} (A)-A^T A^T=(\text{adj}B)^T-\text{adj(A)}+A^T\implies\text{adj} A=(\text{adj} B)^T |A|^2=|B|^2 |\text{adj} C|=|C|^{n-1} n C |C|=|C^T|) A B=\text{adj} (A)-A^T |A| \dfrac B{|A|} = A^{-1} - \dfrac{A^T}{|A|} \implies  \dfrac{B + A^T}{|A|} = A^{-1} \dfrac{|B+A^T|}{|A|^3} = \dfrac1{|A|} \implies |B + A^T| = |A|^2 |kC| = k^n|C| k \frac1{|A|} n C |C^{-1}|=\frac1{|C|} |B|=8 \text{adj}(kC)=k^{n-1}\text{adj}C k n C \text{adj}(2B^{-1})=4\text{adj}(B^{-1}) |\text{adj}(2B^{-1})|=|4\text{adj}(B^{-1})|=64|\text{adj}(B^{-1})| |\text{adj}(B^{-1})|=(|\text{adj}B|)^{-1}=(|B|^2)^{-1}=1/64 |\text{adj}(2B^{-1})|=1,"['matrices', 'determinant']"
46,"If $A$ and $B$ are positive-definite matrices with $A>B$, must $A^k > B^k$ for $k>0$? [closed]","If  and  are positive-definite matrices with , must  for ? [closed]",A B A>B A^k > B^k k>0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Assume $A$ and $B$ are positive-definite matrices. If $A>B$ , can we conclude that $A^k > B^k$ for any positive scalar $k$ ? Note that $A>B$ means $A-B$ is a positive-definite matrix, not an element-wise comparison. I've tried using Cayley-Hamilton theorem but could not get anywhere with it so far.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Assume and are positive-definite matrices. If , can we conclude that for any positive scalar ? Note that means is a positive-definite matrix, not an element-wise comparison. I've tried using Cayley-Hamilton theorem but could not get anywhere with it so far.",A B A>B A^k > B^k k A>B A-B,"['linear-algebra', 'matrices']"
47,$D \overline{D}= \mu$ for a complex matrix $D$ and a scalar $\mu$ implies that $\mu$ is real,for a complex matrix  and a scalar  implies that  is real,D \overline{D}= \mu D \mu \mu,"The following question arose while reading Kevin Buzzard's notes on ""Forms of reductive algebraic groups"", available here . Let $D \in GL(n,\mathbb{C})$ be a matrix such that $D \overline{D} = \mu$ for a scalar $\mu \in \mathbb{C}$ .  Here the right hand side denotes the scalar matrix obtained by multiplying the identity by $\mu$ .  Buzzard claims that $\mu$ must be real.  His reasoning is that $\overline{\mu} = \mu$ , but I don't see why this is true.  If you apply complex conjugation to both sides of $D \overline{D} = \mu$ , you get $\overline{D} D = \overline{\mu}$ , but I don't see why we must have $D \overline{D} = \overline{D} D$ .","The following question arose while reading Kevin Buzzard's notes on ""Forms of reductive algebraic groups"", available here . Let be a matrix such that for a scalar .  Here the right hand side denotes the scalar matrix obtained by multiplying the identity by .  Buzzard claims that must be real.  His reasoning is that , but I don't see why this is true.  If you apply complex conjugation to both sides of , you get , but I don't see why we must have .","D \in GL(n,\mathbb{C}) D \overline{D} = \mu \mu \in \mathbb{C} \mu \mu \overline{\mu} = \mu D \overline{D} = \mu \overline{D} D = \overline{\mu} D \overline{D} = \overline{D} D","['linear-algebra', 'matrices']"
48,Solve $AX=B+AXC$ for the missing matrix $X$,Solve  for the missing matrix,AX=B+AXC X,"imgur link in case I mess up the formatting Solve the following matrix equation for $X$ $$AX=B+AXC$$ where non-symmetric matrix $A$ is $2 \times 2$ , matrix $B$ is $2 \times 3$ , and non-invertible matrix $C$ is $3 \times 3$ . $$\begin{bmatrix} 2&3\\1&2 \end{bmatrix}X=\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}+\begin{bmatrix}2&3\\1&2\end{bmatrix}X\begin{bmatrix}0&1&1\\0&0&1\\0&0&0\end{bmatrix}$$ This one has me stumped. The only way I managed to find an answer for $X$ was by doing this: $$\begin{align}X&=A^{-1}(B+AXC)\\\\ X&=A^{-1}B+XC\\\\ X-XC&=A^{-1}B\\\\ X(I-C)&=A^{-1}B\\\\ X&=A^{-1}B(I-C)^{-1}\end{align}$$ obviously, by plugging in this answer for $X$ into the original question, the answer is false. its probably because by doing $X(I-C)$ , you create an identity matrix that doesn't have a clear dimension. Your help would be much appreciated. EDIT: turns out my solution was correct. I must have made a miscalculation somewhere along the way. Thanks everyone!","imgur link in case I mess up the formatting Solve the following matrix equation for where non-symmetric matrix is , matrix is , and non-invertible matrix is . This one has me stumped. The only way I managed to find an answer for was by doing this: obviously, by plugging in this answer for into the original question, the answer is false. its probably because by doing , you create an identity matrix that doesn't have a clear dimension. Your help would be much appreciated. EDIT: turns out my solution was correct. I must have made a miscalculation somewhere along the way. Thanks everyone!","X AX=B+AXC A 2 \times 2 B 2 \times 3 C 3 \times 3 \begin{bmatrix} 2&3\\1&2 \end{bmatrix}X=\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}+\begin{bmatrix}2&3\\1&2\end{bmatrix}X\begin{bmatrix}0&1&1\\0&0&1\\0&0&0\end{bmatrix} X \begin{align}X&=A^{-1}(B+AXC)\\\\
X&=A^{-1}B+XC\\\\
X-XC&=A^{-1}B\\\\
X(I-C)&=A^{-1}B\\\\
X&=A^{-1}B(I-C)^{-1}\end{align} X X(I-C)","['linear-algebra', 'matrices', 'matrix-equations']"
49,About condition number,About condition number,,"I have the following exercise: Relate the 2-norm condition of $X\in \Bbb R^{m\times n}\ (m\geq n)$ to the 2-norm condition of the matrices: $$B=\begin{equation} \begin{bmatrix} I_m & X\\ 0 & I_n \end{bmatrix} \end{equation}$$ and $$C=\begin{equation} \begin{bmatrix} X\\ I_n \end{bmatrix} \end{equation} $$ My attemp: We know that the condition number of a matrix for the 2-norm is given by the following expression: $$\kappa_2(A)=||A||_2 ||A^{-1}||_2$$ Also I have a property that says if $B$ is a submatrix of $A$ then $$||B||_2\leq ||A||_2$$ So using this I have: $$\kappa_2(B)=||B||_2 ||B^{-1}||_2\geq ||X||_2||X^{-1}||_2=\kappa_2(X)$$ But I am not sure that it is necessarily true that if $X$ is a submatrix of $B$ then necessarily $ X^{-1} $ is a submatrix of $B^{-1}$ If this works then for the $ C $ it would be worth the same, but doing numerical experiments this fails, numerically I get: $$\kappa_2(C)\leq \kappa_2(X) \leq \kappa_2(B)$$ I appreciate any help you can give me, regards Edit: I have seen that $X$ is not square so the inverse does not exist and therefore kappa will not exist either. For this, in Golub's book it is said that it can also be calculated as the maximum singular value between the minimum singular value, that is: $$\kappa_2(A)=\dfrac{\sigma_{max}}{\sigma_{min}}$$ And in this case, as the matrix $X$ is given, for any matrix the 2-norm condition can be obtained, applying the singular value decomposition theorem","I have the following exercise: Relate the 2-norm condition of to the 2-norm condition of the matrices: and My attemp: We know that the condition number of a matrix for the 2-norm is given by the following expression: Also I have a property that says if is a submatrix of then So using this I have: But I am not sure that it is necessarily true that if is a submatrix of then necessarily is a submatrix of If this works then for the it would be worth the same, but doing numerical experiments this fails, numerically I get: I appreciate any help you can give me, regards Edit: I have seen that is not square so the inverse does not exist and therefore kappa will not exist either. For this, in Golub's book it is said that it can also be calculated as the maximum singular value between the minimum singular value, that is: And in this case, as the matrix is given, for any matrix the 2-norm condition can be obtained, applying the singular value decomposition theorem","X\in \Bbb R^{m\times n}\ (m\geq n) B=\begin{equation}
\begin{bmatrix}
I_m & X\\
0 & I_n
\end{bmatrix}
\end{equation} C=\begin{equation}
\begin{bmatrix}
X\\
I_n
\end{bmatrix}
\end{equation}  \kappa_2(A)=||A||_2 ||A^{-1}||_2 B A ||B||_2\leq ||A||_2 \kappa_2(B)=||B||_2 ||B^{-1}||_2\geq ||X||_2||X^{-1}||_2=\kappa_2(X) X B  X^{-1}  B^{-1}  C  \kappa_2(C)\leq \kappa_2(X) \leq \kappa_2(B) X \kappa_2(A)=\dfrac{\sigma_{max}}{\sigma_{min}} X","['matrices', 'svd', 'matrix-norms', 'condition-number', 'spectral-norm']"
50,What does this matrix operator norm mean?,What does this matrix operator norm mean?,,"I am trying to understand what this matrix operator norm means and what it does to matrix $A$ . $$ {{\left\| A \right\|}_{1,\,\infty }} := {\max_{{{\left\| x \right\|}_{\infty }}=1}}{{\left\| Ax \right\|}_{1}} $$ Can somebody help with the explanation and maybe an example?",I am trying to understand what this matrix operator norm means and what it does to matrix . Can somebody help with the explanation and maybe an example?,"A  {{\left\| A \right\|}_{1,\,\infty }} := {\max_{{{\left\| x \right\|}_{\infty }}=1}}{{\left\| Ax \right\|}_{1}} ","['matrices', 'matrix-norms']"
51,Solving $Ax= b$ for $A$,Solving  for,Ax= b A,"I tried to solve this problem for A using the linearity of matrices, but I'm going nowhere. Can someone help-me with this question? Solve or give me a hint? Thanks! \begin{matrix}  A \begin{bmatrix} -1 \\ -2\\ -1\\ \end{bmatrix} = \begin{bmatrix} -3 \\ -3 \\ -6\\ \end{bmatrix},\\ \\ \end{matrix} \begin{matrix}  A \begin{bmatrix} -1 \\ 0\\ 7\\ \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \\ 0\\ \end{bmatrix},\\ \\ \end{matrix} \begin{matrix}  A \begin{bmatrix} -1 \\ -4\\ 2\\ \end{bmatrix} = \begin{bmatrix} -2 \\ -1 \\ -2\\ \end{bmatrix},\\ \\ \end{matrix}","I tried to solve this problem for A using the linearity of matrices, but I'm going nowhere. Can someone help-me with this question? Solve or give me a hint? Thanks!","\begin{matrix}
 A \begin{bmatrix}
-1 \\
-2\\
-1\\
\end{bmatrix} = \begin{bmatrix}
-3 \\
-3 \\
-6\\
\end{bmatrix},\\ \\
\end{matrix} \begin{matrix}
 A \begin{bmatrix}
-1 \\
0\\
7\\
\end{bmatrix} = \begin{bmatrix}
2 \\
0 \\
0\\
\end{bmatrix},\\ \\
\end{matrix} \begin{matrix}
 A \begin{bmatrix}
-1 \\
-4\\
2\\
\end{bmatrix} = \begin{bmatrix}
-2 \\
-1 \\
-2\\
\end{bmatrix},\\ \\
\end{matrix}","['linear-algebra', 'matrices', 'vectors']"
52,Why do the signs of $\sin\theta$ change in 3d rotation matrices about the different coordinate axes?,Why do the signs of  change in 3d rotation matrices about the different coordinate axes?,\sin\theta,"Using matrices for 3-dimensional rotations, I'm confused on why the signs change $\sin(\theta)$ to $-\sin(\theta)$ when rotating around the $z$ -axis. What is the cause of this? Rotate X and Y around the Z-axis $$\begin{bmatrix} \cos\theta & -\sin\theta & 0\\ \sin\theta & \phantom{-}\cos\theta & 0\\ 0 & 0 & 1 \end{bmatrix} $$ Rotate Y and Z around the X-axis $$\begin{bmatrix} 1 & 0 & 0\\ 0 & \cos\theta & -\sin\theta\\ 0 & \sin\theta & \phantom{-}\cos\theta \end{bmatrix} $$ Rotate X and Z around the Y-axis $$\begin{bmatrix} \phantom{-}\cos\theta & 0 & \color{red}{\sin\theta}\\ 0 & 1 & 0\\ \color{red}{-\sin\theta} & 0 & \cos\theta \end{bmatrix} $$","Using matrices for 3-dimensional rotations, I'm confused on why the signs change to when rotating around the -axis. What is the cause of this? Rotate X and Y around the Z-axis Rotate Y and Z around the X-axis Rotate X and Z around the Y-axis","\sin(\theta) -\sin(\theta) z \begin{bmatrix}
\cos\theta & -\sin\theta & 0\\
\sin\theta & \phantom{-}\cos\theta & 0\\
0 & 0 & 1
\end{bmatrix}
 \begin{bmatrix}
1 & 0 & 0\\
0 & \cos\theta & -\sin\theta\\
0 & \sin\theta & \phantom{-}\cos\theta
\end{bmatrix}
 \begin{bmatrix}
\phantom{-}\cos\theta & 0 & \color{red}{\sin\theta}\\
0 & 1 & 0\\
\color{red}{-\sin\theta} & 0 & \cos\theta
\end{bmatrix}
","['matrices', 'rotations', 'quaternions']"
53,"Separating scaling, rotation and sheer coefficients correctly in this case","Separating scaling, rotation and sheer coefficients correctly in this case",,"This question follows-on from Correct the Fourier transform of a shear-distorted image? ($x_{new} = x + \alpha y$) . I have a group of distorted $x, y$ points and have found a matrix $A$ that corrects their positions: $$     \begin{bmatrix}     x' \\     y' \\     \end{bmatrix} =      \begin{bmatrix}     a_{xx} & a_{xy}  \\     a_{yx} & a_{yy} \\     \end{bmatrix}      \begin{bmatrix}     x \\     y \\     \end{bmatrix} $$ I now want to break this up into three separate terms, one for pure scaling or magnification $M$ , one for pure rotation $R$ , and one for shear $S$ . I think they will look like this: $$M = \begin{bmatrix}     m_x & 0\\     0 & m_y \\     \end{bmatrix} $$ $$R = \begin{bmatrix}     \cos \theta & -\sin \theta \\     \sin \theta & \cos \theta \\     \end{bmatrix}  $$ $$S = \begin{bmatrix}     1 & s_{xy}  \\     s_{yx} & 1 \\     \end{bmatrix}  $$ I see there are answers to Separating Out Parts of a Matrix (Translation, Rotation, Scaling) Is it true that any matrix can be decomposed into product of rotation,reflection,shear,scaling and projection matrices? and I recognize that those answers may apply to my problem, but they are beyond me; I need a simpler, more applied answer, or at least assistance getting there. Q1: I can imagine setting $A$ equal to the product, but I'm worried if the order matters. Will this order work? Would any order work equally as well? $$A = R \ S \ M $$ Q2: I now have five parameters instead of four. Since I've separated out rotation, should I now use only one shear, i.e. I have to choose only one of $s_xy$ or $s_yx$ and set the other to zero? Or just use $s_{xy} = s_{yx} = s$ ? $$S = \begin{bmatrix}     1 & s  \\     s & 1 \\     \end{bmatrix}  $$","This question follows-on from Correct the Fourier transform of a shear-distorted image? ($x_{new} = x + \alpha y$) . I have a group of distorted points and have found a matrix that corrects their positions: I now want to break this up into three separate terms, one for pure scaling or magnification , one for pure rotation , and one for shear . I think they will look like this: I see there are answers to Separating Out Parts of a Matrix (Translation, Rotation, Scaling) Is it true that any matrix can be decomposed into product of rotation,reflection,shear,scaling and projection matrices? and I recognize that those answers may apply to my problem, but they are beyond me; I need a simpler, more applied answer, or at least assistance getting there. Q1: I can imagine setting equal to the product, but I'm worried if the order matters. Will this order work? Would any order work equally as well? Q2: I now have five parameters instead of four. Since I've separated out rotation, should I now use only one shear, i.e. I have to choose only one of or and set the other to zero? Or just use ?","x, y A 
    \begin{bmatrix}
    x' \\
    y' \\
    \end{bmatrix} = 
    \begin{bmatrix}
    a_{xx} & a_{xy}  \\
    a_{yx} & a_{yy} \\
    \end{bmatrix} 
    \begin{bmatrix}
    x \\
    y \\
    \end{bmatrix}
 M R S M = \begin{bmatrix}
    m_x & 0\\
    0 & m_y \\
    \end{bmatrix}
 R = \begin{bmatrix}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta \\
    \end{bmatrix} 
 S = \begin{bmatrix}
    1 & s_{xy}  \\
    s_{yx} & 1 \\
    \end{bmatrix} 
 A A = R \ S \ M  s_xy s_yx s_{xy} = s_{yx} = s S = \begin{bmatrix}
    1 & s  \\
    s & 1 \\
    \end{bmatrix} 
","['matrices', 'linear-transformations']"
54,Transform the squared error loss function to matrix form,Transform the squared error loss function to matrix form,,"I have n data, $x_i$ are the inputs, and $y_i$ are the labels. $x_i$ is $d$ -dimension, and $y_i$ is $p$ -dimension. We have a prediction function that is $f(x) = W^Tx$ . $W$ is a matrix of dimension $d \times p$ . The squared error loss function of the prediction function is: $$   J(W) = \sum_{i=1}^n||y_i - W^Tx_i||_2^2 $$ I would like to write $J(W)$ in matrix form. How should I convert it to an equation involving $Y$ ( $n \times p$ dimension), $W$ ( $d \times p$ dimension) and $X$ ( $n \times d$ dimension). Hence the loss function will be in matrix form instead of a sum?","I have n data, are the inputs, and are the labels. is -dimension, and is -dimension. We have a prediction function that is . is a matrix of dimension . The squared error loss function of the prediction function is: I would like to write in matrix form. How should I convert it to an equation involving ( dimension), ( dimension) and ( dimension). Hence the loss function will be in matrix form instead of a sum?","x_i y_i x_i d y_i p f(x) = W^Tx W d \times p 
  J(W) = \sum_{i=1}^n||y_i - W^Tx_i||_2^2
 J(W) Y n \times p W d \times p X n \times d","['linear-algebra', 'matrices']"
55,Question about a statement involving system of linear equations and rank of matrix.,Question about a statement involving system of linear equations and rank of matrix.,,"the statement is: let $A\in \mathbb{R}^{m\times n}$ ,   if for every $c\in \mathbb{R}^m$ there exists a solution for $Ax=c$ , then $\operatorname{rank}(A)=m$ . Now I can understand that this is a true statement, since if $\operatorname{rank}(A)<m$ we can get no solution for the equation. What I wanted to ask if the given information was for every $c\in \mathbb{R}^m$ there exists a unique solution, could I infer that the matrix is square? ( $m = n$ ). This is coming from the idea that if $Ax=c\:$ has a unique solution, then $\operatorname{rank}(A) = n$ , so if it has a solution for every $c$ that means $\operatorname{rank}(A) = m $ and so $m=n$ . I would appreciate any feedback and would love to know if there's more interesting stuff to infer from the given information.","the statement is: let ,   if for every there exists a solution for , then . Now I can understand that this is a true statement, since if we can get no solution for the equation. What I wanted to ask if the given information was for every there exists a unique solution, could I infer that the matrix is square? ( ). This is coming from the idea that if has a unique solution, then , so if it has a solution for every that means and so . I would appreciate any feedback and would love to know if there's more interesting stuff to infer from the given information.",A\in \mathbb{R}^{m\times n} c\in \mathbb{R}^m Ax=c \operatorname{rank}(A)=m \operatorname{rank}(A)<m c\in \mathbb{R}^m m = n Ax=c\: \operatorname{rank}(A) = n c \operatorname{rank}(A) = m  m=n,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-rank']"
56,Maximize quadratic function over unit sphere,Maximize quadratic function over unit sphere,,"In lecture, we proved that, given a symmetric matrix $A$ , the $$\max_{\|x\|_2 = 1} x^T A x$$ is the largest eigenvalue $\lambda_{\max}$ of matrix $A$ : we diagonalize the matrix $A$ and show that for every unit vector x the inequality $x^T A x \leq \lambda_{\max}$ holds. I understand every part of the computation. What I do not understand is why it suffices to consider unit vectors for $x$ ? If the inequality is true for every $x$ in the unit sphere and we find one for which the inequality is an equality, then of course we have found a maximum. But in the unit sphere do not only lie the unit vectors.","In lecture, we proved that, given a symmetric matrix , the is the largest eigenvalue of matrix : we diagonalize the matrix and show that for every unit vector x the inequality holds. I understand every part of the computation. What I do not understand is why it suffices to consider unit vectors for ? If the inequality is true for every in the unit sphere and we find one for which the inequality is an equality, then of course we have found a maximum. But in the unit sphere do not only lie the unit vectors.",A \max_{\|x\|_2 = 1} x^T A x \lambda_{\max} A A x^T A x \leq \lambda_{\max} x x,"['linear-algebra', 'matrices', 'optimization', 'qcqp']"
57,"Problems relates to minimal polynomial and $AB, BA$",Problems relates to minimal polynomial and,"AB, BA","Let $A$ and $B$ be complex $3\times 3$ matrices. The minimal polynomial of $A$ is $x^3-1$ and the minimal polynomial of $B$ is $(x-1)^3$ . At this time show that $AB\neq BA$ I know that the characteristic and minimal polynomials of $AB$ and $BA$ are the same at least one of $A$ , $B$ is invertible. But in general, $AB$ and $BA$ do not have the same minimal polynomial. Am I right? How do we answer this question? Any hints would be appreciated.","Let and be complex matrices. The minimal polynomial of is and the minimal polynomial of is . At this time show that I know that the characteristic and minimal polynomials of and are the same at least one of , is invertible. But in general, and do not have the same minimal polynomial. Am I right? How do we answer this question? Any hints would be appreciated.",A B 3\times 3 A x^3-1 B (x-1)^3 AB\neq BA AB BA A B AB BA,"['linear-algebra', 'matrices', 'minimal-polynomials']"
58,Why does adding an extra column to a matrix not change the RREF of the first columns?,Why does adding an extra column to a matrix not change the RREF of the first columns?,,"This resource (2.2.7) points out that deleting a column from a matrix would not change the RREF of the initial undeleted columns. For example, the RREF form of this $3\times3$ matrix is the $3\times3$ identity matrix. \begin{bmatrix}1&1&1\\1&0&1\\1&2&3\end{bmatrix} The RREF form of this matrix (the above with third column deleted) would be the $3\times 3$ identity matrix with the third column deleted, i.e. the RREF form of the first two colums are preserved. \begin{bmatrix}1&1\\1&0\\1&2\end{bmatrix} Is there a way to geometrically, or numerically indicate why the RREF form of individual columns are not changed by adding/deleting other columns? Intuitively, I believe this has something to do with the fact that adding another column to a matrix doesn't change the linear independence of the other columns with respect to each other. Furthermore, how does this connect to deleting or adding rows? Would messing with rows change the RREF form of columns?","This resource (2.2.7) points out that deleting a column from a matrix would not change the RREF of the initial undeleted columns. For example, the RREF form of this matrix is the identity matrix. The RREF form of this matrix (the above with third column deleted) would be the identity matrix with the third column deleted, i.e. the RREF form of the first two colums are preserved. Is there a way to geometrically, or numerically indicate why the RREF form of individual columns are not changed by adding/deleting other columns? Intuitively, I believe this has something to do with the fact that adding another column to a matrix doesn't change the linear independence of the other columns with respect to each other. Furthermore, how does this connect to deleting or adding rows? Would messing with rows change the RREF form of columns?",3\times3 3\times3 \begin{bmatrix}1&1&1\\1&0&1\\1&2&3\end{bmatrix} 3\times 3 \begin{bmatrix}1&1\\1&0\\1&2\end{bmatrix},"['linear-algebra', 'matrices']"
59,Find function f(z) for a matrix,Find function f(z) for a matrix,,"Let $z \in \mathbb{C}$ . Let $ f(z)=  \begin{cases}     1,& \text{if } |z|<\frac{1}{2}\\     0, &             \text{if} |z-2| <\frac{1}{2} \\ 0, & \text{if} |z-3| <\frac{1}{2} \end{cases}$ Let \begin{equation*} A =  \begin{pmatrix} 4 & 1 & 0 & -4 & 0 & 0\\ 0 & 4 & -1 & 0 & -4 & 2\\ 0 & 0 & 6 & 0 & 0 & -6\\ 2 & 0 & 0 & -2 & 1 & 0\\ 0 & 2 & -1 & 0 & -2 & 2\\ 0 & 0 & 3 & 0 & 0 & -3\\ \end{pmatrix}\end{equation*} Find $f(A)$ . Attempt: We know that $\mathbb{C}^6$ is a Hilbert space. and $B(\mathbb{C}^6) \simeq \mathbb{M_6}$ . So we can consider $A$ as a function in $B(\mathbb{C}^6)$ And we can write $Ae_j= \sum_{n=1}^6a_n e_n$ , where $(e_n)$ is the orthonormal basis for $\mathbb{C}^6$ . Also I know that the spectrum $\sigma(A)= \{\text{eigenvalues of} A \}$ From here I'm not sure how to proceed. I'm trying to solve this using Banach Algebra point of view. But any other suggestions will be much appreciated too! EDIT: My apologies, there was a typo in the original question. Now corrected Thank you!","Let . Let Let Find . Attempt: We know that is a Hilbert space. and . So we can consider as a function in And we can write , where is the orthonormal basis for . Also I know that the spectrum From here I'm not sure how to proceed. I'm trying to solve this using Banach Algebra point of view. But any other suggestions will be much appreciated too! EDIT: My apologies, there was a typo in the original question. Now corrected Thank you!","z \in \mathbb{C}  f(z)= 
\begin{cases}
    1,& \text{if } |z|<\frac{1}{2}\\
    0, &             \text{if} |z-2| <\frac{1}{2} \\
0, & \text{if} |z-3| <\frac{1}{2}
\end{cases} \begin{equation*}
A = 
\begin{pmatrix}
4 & 1 & 0 & -4 & 0 & 0\\
0 & 4 & -1 & 0 & -4 & 2\\
0 & 0 & 6 & 0 & 0 & -6\\
2 & 0 & 0 & -2 & 1 & 0\\
0 & 2 & -1 & 0 & -2 & 2\\
0 & 0 & 3 & 0 & 0 & -3\\
\end{pmatrix}\end{equation*} f(A) \mathbb{C}^6 B(\mathbb{C}^6) \simeq \mathbb{M_6} A B(\mathbb{C}^6) Ae_j= \sum_{n=1}^6a_n e_n (e_n) \mathbb{C}^6 \sigma(A)= \{\text{eigenvalues of} A \}","['linear-algebra', 'matrices', 'functional-analysis', 'banach-algebras', 'matrix-exponential']"
60,Eigenvectors of a normal operator are an orthogonal basis,Eigenvectors of a normal operator are an orthogonal basis,,"I am trying to prove the following. Let $T\colon\mathbb C^n\to\mathbb C^n$ be a normal operator. Then there is a basis for $\mathbb C^n$ consisting of orthogonal eigenvectors of $T$ . I am allowed to use the following fact which I've already established: $$T\boldsymbol x=\mu\boldsymbol x\iff T^*\boldsymbol x=\overline\mu\boldsymbol x.$$ I've managed to produce a proof using Schur's theorem (i.e., $T$ is unitarily similar to a diagonal matrix), but this feels like too much ""heavy machinery"" to tackle this proof. Is there another more direct way to go about it?","I am trying to prove the following. Let be a normal operator. Then there is a basis for consisting of orthogonal eigenvectors of . I am allowed to use the following fact which I've already established: I've managed to produce a proof using Schur's theorem (i.e., is unitarily similar to a diagonal matrix), but this feels like too much ""heavy machinery"" to tackle this proof. Is there another more direct way to go about it?",T\colon\mathbb C^n\to\mathbb C^n \mathbb C^n T T\boldsymbol x=\mu\boldsymbol x\iff T^*\boldsymbol x=\overline\mu\boldsymbol x. T,"['linear-algebra', 'matrices', 'functional-analysis']"
61,Diagonalizing a matrix where the pairwise difference of eigenvalues are units,Diagonalizing a matrix where the pairwise difference of eigenvalues are units,,"Let $R$ be a commutative ring (with $1$ ), and $M$ be an $n\times n$ matrix over $R$ whose characteristic polynomial is $$\det(tI - M) = (t-\alpha_1)\cdots(t-\alpha_n)$$ with $\alpha_i \in R$ for all $i$ . Assume that $\alpha_i - \alpha_j \in R^\times$ whenever $i\ne j$ . Is it true that there is some invertible $P \in GL_n(R)$ with $$M = P\begin{pmatrix}\alpha_1 & & & \\ & \alpha_2 & & \\ & & \ddots & \\ & & & \alpha_n\end{pmatrix}P^{-1}?$$ This is true if $R$ is a field. The proof here shows that we must have $R^n = \oplus_{i=1}^n \ker(M - \alpha_i I_n)$ , at least in the case when $R$ is a local ring (but also it seems to work for general $R$ ). But would it follow that we can write $M$ in the form above? It is not clear to me that each $\ker(M - \alpha_i I_n)$ is free (if $R$ is not local), so I am not sure if we can necessarily find a basis of $R^n$ consisting of eigenvectors of $M$ . If the above is false, might there be some conditions on $R$ that would make it true, e.g. if we assume $R$ is an algebra over a field?","Let be a commutative ring (with ), and be an matrix over whose characteristic polynomial is with for all . Assume that whenever . Is it true that there is some invertible with This is true if is a field. The proof here shows that we must have , at least in the case when is a local ring (but also it seems to work for general ). But would it follow that we can write in the form above? It is not clear to me that each is free (if is not local), so I am not sure if we can necessarily find a basis of consisting of eigenvectors of . If the above is false, might there be some conditions on that would make it true, e.g. if we assume is an algebra over a field?",R 1 M n\times n R \det(tI - M) = (t-\alpha_1)\cdots(t-\alpha_n) \alpha_i \in R i \alpha_i - \alpha_j \in R^\times i\ne j P \in GL_n(R) M = P\begin{pmatrix}\alpha_1 & & & \\ & \alpha_2 & & \\ & & \ddots & \\ & & & \alpha_n\end{pmatrix}P^{-1}? R R^n = \oplus_{i=1}^n \ker(M - \alpha_i I_n) R R M \ker(M - \alpha_i I_n) R R^n M R R,"['linear-algebra', 'matrices', 'commutative-algebra', 'diagonalization']"
62,Prove that $A^T \cdot A$ equal $I$ for an orthonormal matrix $A$ directly using matrix multiplication,Prove that  equal  for an orthonormal matrix  directly using matrix multiplication,A^T \cdot A I A,"Apparently it is always true that $A \cdot A^T=A^T \cdot A=I$ for an orthonormal matrix $A$ . I know this can be proven using theorems regarding the rank and invertibility of matrices, but I would like to show it directly with matrix multiplication. Suppose that $A =  \begin{bmatrix} u_1 &v _1 \\ u_2 &v_2 \\ \end{bmatrix} $ is an orthonormal matrix. Then we know that $\begin{bmatrix} u_1\\u_2 \end{bmatrix}^T \begin{bmatrix} u_1\\u_2 \end{bmatrix}=1$ and $\begin{bmatrix} v_1\\v_2 \end{bmatrix}^T \begin{bmatrix} v_1\\v_2 \end{bmatrix}=1$ , and also that $\begin{bmatrix} u_1\\u_2 \end{bmatrix}^T \begin{bmatrix} v_1\\v_2 \end{bmatrix}=0$ However, we must have that $A \cdot A^T = \begin{bmatrix} u_1^2+v_1^2 & u_1u_2+v_1v_2 \\ u_1u_2+v_1v_2  & u_2^2+v_2^2 \\ \end{bmatrix}  $ , and I do not see why $u_1^2+v_1^2 =1$ or why $u_1u_2+v_1v_2=0.$ It seems like $A \cdot A^T = I$ is not necessarily true. Where is my mistake?","Apparently it is always true that for an orthonormal matrix . I know this can be proven using theorems regarding the rank and invertibility of matrices, but I would like to show it directly with matrix multiplication. Suppose that is an orthonormal matrix. Then we know that and , and also that However, we must have that , and I do not see why or why It seems like is not necessarily true. Where is my mistake?","A \cdot A^T=A^T \cdot A=I A A = 
\begin{bmatrix}
u_1 &v _1 \\
u_2 &v_2 \\
\end{bmatrix}
 \begin{bmatrix} u_1\\u_2 \end{bmatrix}^T \begin{bmatrix} u_1\\u_2 \end{bmatrix}=1 \begin{bmatrix} v_1\\v_2 \end{bmatrix}^T \begin{bmatrix} v_1\\v_2 \end{bmatrix}=1 \begin{bmatrix} u_1\\u_2 \end{bmatrix}^T \begin{bmatrix} v_1\\v_2 \end{bmatrix}=0 A \cdot A^T = \begin{bmatrix}
u_1^2+v_1^2 & u_1u_2+v_1v_2 \\
u_1u_2+v_1v_2  & u_2^2+v_2^2 \\
\end{bmatrix} 
 u_1^2+v_1^2 =1 u_1u_2+v_1v_2=0. A \cdot A^T = I","['linear-algebra', 'matrices', 'orthogonality', 'orthonormal', 'orthogonal-matrices']"
63,Matrix equation $X+AX-XA^2=0$ has only zero solution,Matrix equation  has only zero solution,X+AX-XA^2=0,"Let $A$ be a $n\times n$ complex matrix, with even numbers as eigenvalues. Show that the matrix equation $X+AX-XA^2=0$ has only zero solution. My attempt is just consider an eigenvector $v$ of $A$ , then $Xv+AXv-4m^2Xv=0.$ Then what to do? Or just $v'(X+AX-XA^2)v=0$ . But $A$ is not symmetric, how to proceed?","Let be a complex matrix, with even numbers as eigenvalues. Show that the matrix equation has only zero solution. My attempt is just consider an eigenvector of , then Then what to do? Or just . But is not symmetric, how to proceed?",A n\times n X+AX-XA^2=0 v A Xv+AXv-4m^2Xv=0. v'(X+AX-XA^2)v=0 A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations']"
64,What is the Kronecker Product of two vectors?,What is the Kronecker Product of two vectors?,,In my numerical methods course we got a homework problem that has a definition of a function $\phi(x) = vec(M) - x \otimes x $ where $x\otimes x$ is the kronecker product of an n-vector and $ M $ is an $n\times n$ Matrix that is vectorized (flattened) in column-major by the $vec()$ operator. I got confused as I thought the kronecker product would produce an $ n\times n$ matrix. But according to the instructor it's a vector? How would I compute the kronecker product of two vectors? I thought it would be the entries of the first vector times the second vector appended in a matrix. Thanks for anyone who can share some enlightenment. I tried to find some on Wikipedia but the examples there confirm my confusion. I looked at this already: Kronecker product and outer product confusion Thank you!,In my numerical methods course we got a homework problem that has a definition of a function where is the kronecker product of an n-vector and is an Matrix that is vectorized (flattened) in column-major by the operator. I got confused as I thought the kronecker product would produce an matrix. But according to the instructor it's a vector? How would I compute the kronecker product of two vectors? I thought it would be the entries of the first vector times the second vector appended in a matrix. Thanks for anyone who can share some enlightenment. I tried to find some on Wikipedia but the examples there confirm my confusion. I looked at this already: Kronecker product and outer product confusion Thank you!,\phi(x) = vec(M) - x \otimes x  x\otimes x  M  n\times n vec()  n\times n,"['linear-algebra', 'matrices', 'kronecker-product']"
65,Why is this diagonal matrix not possible over the reals,Why is this diagonal matrix not possible over the reals,,"Let $\alpha = \begin{pmatrix} 7 &3  &-4 \\   -2&-1  &2 \\   6&2  &-3  \end{pmatrix}$ over the reals. Show that there does not exist a invertible real matrix $\beta$ , so that $\delta = \beta ^{-1} \alpha \beta$ is a diagonal matrix My ""proof"" the characterisitc polynomial is $-\lambda ^3 + 3\lambda ^2 -\lambda + 3=-(\lambda - 3)(\lambda ^2 +1)$ . The solutions would then be $3,i,-i$ . Now, we have that $\delta = \beta ^{-1} \alpha \beta$ where the diagonal values of $\delta$ would then be eigenvalues of $\alpha$ . if both $\alpha , \beta$ are real $3 \times 3$ -matrices then $\delta$ must also be a real $3 \times 3$ -matrix. But that is not possible since the eigenvalues of $\alpha$ are $3,i,-i$ and thus $\beta$ must be complex..... Now I am stuck since I would think I would have to show that the diagonal of $\delta$ must be the eigenvalues of $\alpha$ . Is this a generel fact or should this be proven? If so - how?","Let over the reals. Show that there does not exist a invertible real matrix , so that is a diagonal matrix My ""proof"" the characterisitc polynomial is . The solutions would then be . Now, we have that where the diagonal values of would then be eigenvalues of . if both are real -matrices then must also be a real -matrix. But that is not possible since the eigenvalues of are and thus must be complex..... Now I am stuck since I would think I would have to show that the diagonal of must be the eigenvalues of . Is this a generel fact or should this be proven? If so - how?","\alpha = \begin{pmatrix}
7 &3  &-4 \\ 
 -2&-1  &2 \\ 
 6&2  &-3 
\end{pmatrix} \beta \delta = \beta ^{-1} \alpha \beta -\lambda ^3 + 3\lambda ^2 -\lambda + 3=-(\lambda - 3)(\lambda ^2 +1) 3,i,-i \delta = \beta ^{-1} \alpha \beta \delta \alpha \alpha , \beta 3 \times 3 \delta 3 \times 3 \alpha 3,i,-i \beta \delta \alpha",['linear-algebra']
66,An infinite hermitian matrix with each submatrix non singular,An infinite hermitian matrix with each submatrix non singular,,"Let $A$ be an hermitian infinite matrix with the property that $\forall n \in \mathbb{N}: A_{n \times n}$ , the submatrix of order $n$ (rows and columns from $1$ to $n$ ), is non-singular. My question: is it true that $A$ is non-singular ? Thanks.","Let be an hermitian infinite matrix with the property that , the submatrix of order (rows and columns from to ), is non-singular. My question: is it true that is non-singular ? Thanks.",A \forall n \in \mathbb{N}: A_{n \times n} n 1 n A,"['matrices', 'hermitian-matrices']"
67,derivative for the following matrix [duplicate],derivative for the following matrix [duplicate],,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . I have a question about the following derivative. Let us have $X\in\mathbb{R}^{m\times n}, z\in\mathbb{R}^{n}$ and I would like to find the derivative $$\frac{\partial (z^{T}X^{T}Xz)}{\partial z}. $$ Any idea? It gives me a hard time. Thank you.",This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . I have a question about the following derivative. Let us have and I would like to find the derivative Any idea? It gives me a hard time. Thank you.,"X\in\mathbb{R}^{m\times n}, z\in\mathbb{R}^{n} \frac{\partial (z^{T}X^{T}Xz)}{\partial z}. ","['calculus', 'linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives']"
68,find a matrix $A_{n \times n}$ which satisfy $A^{n}=0$ and $A^{n-1}\ne0$,find a matrix  which satisfy  and,A_{n \times n} A^{n}=0 A^{n-1}\ne0,"I asking a question for my brother. He just started his algebra course not long ago and he got a question he is stuck on (he just started the course so all his knoweldge is based on matrices and their properties). I need to find a Matrix $A_{2x2} \not=0$ which satisfies $A^2=0$ . Also, I need to find a Matrix $A_{3x3}$ which satisfies $A^3=0$ and $A^2\not=0$ . In the end I need to generalize the problem to $A_{n \times n}$ (find a matrix $A_{n \times n}$ which satisfy $A^{n}=0$ and $A^{n-1}\ne0$ ). In the first question I wrote $A$ as \begin{bmatrix} a & b \\ c & d \end{bmatrix} after multiplying $A$ with itself I found out that every matrix $2x2$ in the form of \begin{bmatrix} x & y \\ -\frac{x^2}{y} & -x \end{bmatrix} when multiplied with itself equal zero. In the second question I tried the same thing but it got to long to soon so I figured I am doing something wrong. also, the the equations I get when I am trying to find conditions for $A$ are to messy to deal with (9 in total). I did manage to find some matrices that satisfies $A^3=0$ such as \begin{bmatrix} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix} but I still wish to know how should I find the conditions for it to happen. About the third question, I guess I need to finish the second one to even begin thinking about the solution but I still can't see where should I start.","I asking a question for my brother. He just started his algebra course not long ago and he got a question he is stuck on (he just started the course so all his knoweldge is based on matrices and their properties). I need to find a Matrix which satisfies . Also, I need to find a Matrix which satisfies and . In the end I need to generalize the problem to (find a matrix which satisfy and ). In the first question I wrote as after multiplying with itself I found out that every matrix in the form of when multiplied with itself equal zero. In the second question I tried the same thing but it got to long to soon so I figured I am doing something wrong. also, the the equations I get when I am trying to find conditions for are to messy to deal with (9 in total). I did manage to find some matrices that satisfies such as but I still wish to know how should I find the conditions for it to happen. About the third question, I guess I need to finish the second one to even begin thinking about the solution but I still can't see where should I start.",A_{2x2} \not=0 A^2=0 A_{3x3} A^3=0 A^2\not=0 A_{n \times n} A_{n \times n} A^{n}=0 A^{n-1}\ne0 A \begin{bmatrix} a & b \\ c & d \end{bmatrix} A 2x2 \begin{bmatrix} x & y \\ -\frac{x^2}{y} & -x \end{bmatrix} A A^3=0 \begin{bmatrix} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix},"['linear-algebra', 'matrices']"
69,Can a given symmetric matrix be written as a linear combination of the identity and a rank-$1$ matrix?,Can a given symmetric matrix be written as a linear combination of the identity and a rank- matrix?,1,"Given an invertible symmetric matrix $M$ , is there a way to determine whether it can be written as $$M = a v v^T + b \mathrm{I}$$ for some scalars $a,b \neq 0$ and a vector $v$ ? I'm given matrix $M$ and want to find $a, b$ and $v$ .","Given an invertible symmetric matrix , is there a way to determine whether it can be written as for some scalars and a vector ? I'm given matrix and want to find and .","M M = a v v^T + b \mathrm{I} a,b \neq 0 v M a, b v","['linear-algebra', 'matrices', 'matrix-decomposition', 'matrix-rank', 'symmetric-matrices']"
70,Prove that there is a matrix $B$ so that $\det(M + tB) \neq 0$ [closed],Prove that there is a matrix  so that  [closed],B \det(M + tB) \neq 0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $M$ denote an $n\times n$ matrix with entries in a field $\mathbb{F}$ . Prove that there is a $n\times n$ matrix $B$ with entries in $\mathbb{F}$ so that $\det(M + tB)\neq 0$ for every non-zero $t\in\mathbb{F}$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let denote an matrix with entries in a field . Prove that there is a matrix with entries in so that for every non-zero .",M n\times n \mathbb{F} n\times n B \mathbb{F} \det(M + tB)\neq 0 t\in\mathbb{F},"['linear-algebra', 'matrices', 'determinant']"
71,How to prove this inequality on trace of inverse of a positive definite matrix?,How to prove this inequality on trace of inverse of a positive definite matrix?,,"I was stuck for a long time to prove the following trace inequality \begin{equation}\label{ine1} {\text {tr}} ({\bf A}_1^{-1}) \leq {\text {tr}} ({\bf A}^{-1}),  \tag{1} \end{equation} where ${\bf A} \in {\mathbb C}^{K \times K}$ is a positive definite matrix and ${\bf A}_1$ consists of the diagonal elements of ${\bf A}$ , i.e., ${\bf A}_1 = {\text {diag}} ( {\text {diag}} ({\bf A}))$ . I have run a lot of simulation using Matlab and this inequality always holds. (1) in the simple case with $K=2$ can be easily proven. However, I can not prove it for the more general $K>2$ case. Some of my efforts are as follows. Denote ${\bf A}_1 = {\text {diag}} \{ a_1, \cdots, a_K \}$ and the eigenvalues of ${\bf A}$ by $ \{ \lambda_1, \cdots, \lambda_K \}$ . Then, it is obvious that \begin{equation}\label{e1} \sum_{k=1}^K a_k = \sum_{k=1}^K \lambda_k. \tag{2} \end{equation} In addition, using Hadamard's inequality, we have \begin{equation}\label{ine2} \prod_{k=1}^K a_k \geq \prod_{k=1}^K \lambda_k. \tag{3} \end{equation} Since (1) is equivalent to \begin{equation} \sum_{k=1}^K \frac{1}{a_k} \leq \sum_{k=1}^K \frac{1}{\lambda_k}, \tag{4} \end{equation} I tried to prove (4) using (2), (3), and the relationship between different means (Harmonic mean, etc.). But they didn't work. Anyone providing the proof or relevant hints will be much appreciated. Thank you.","I was stuck for a long time to prove the following trace inequality where is a positive definite matrix and consists of the diagonal elements of , i.e., . I have run a lot of simulation using Matlab and this inequality always holds. (1) in the simple case with can be easily proven. However, I can not prove it for the more general case. Some of my efforts are as follows. Denote and the eigenvalues of by . Then, it is obvious that In addition, using Hadamard's inequality, we have Since (1) is equivalent to I tried to prove (4) using (2), (3), and the relationship between different means (Harmonic mean, etc.). But they didn't work. Anyone providing the proof or relevant hints will be much appreciated. Thank you.","\begin{equation}\label{ine1}
{\text {tr}} ({\bf A}_1^{-1}) \leq {\text {tr}} ({\bf A}^{-1}),  \tag{1}
\end{equation} {\bf A} \in {\mathbb C}^{K \times K} {\bf A}_1 {\bf A} {\bf A}_1 = {\text {diag}} ( {\text {diag}} ({\bf A})) K=2 K>2 {\bf A}_1 = {\text {diag}} \{ a_1, \cdots, a_K \} {\bf A}  \{ \lambda_1, \cdots, \lambda_K \} \begin{equation}\label{e1}
\sum_{k=1}^K a_k = \sum_{k=1}^K \lambda_k. \tag{2}
\end{equation} \begin{equation}\label{ine2}
\prod_{k=1}^K a_k \geq \prod_{k=1}^K \lambda_k. \tag{3}
\end{equation} \begin{equation}
\sum_{k=1}^K \frac{1}{a_k} \leq \sum_{k=1}^K \frac{1}{\lambda_k}, \tag{4}
\end{equation}","['matrices', 'inequality', 'inverse', 'trace', 'positive-definite']"
72,Determinant of Kronecker Product,Determinant of Kronecker Product,,"Problem. Let $A$ be an $m\times n$ matrix and $B$ be an $n\times m$ matrix. Show that $$\det(A\otimes B)=\det(B\otimes A).$$ This formula apparently holds if $A$ and $B$ are square matrices, but here this assumption fails in general. I am considering the general definition of determinant: $$\det(M)=\sum_{\sigma\in S_n}{(-1)^{\rm sgn(\sigma)}a_{1\sigma(1)}\cdots a_{n\sigma(n)}}.$$ We have $$\det(B\otimes A)=\det((B\otimes A)^T)=\det(B^T\otimes A^T).$$ The shape of $B^T$ is the same as $A$ and the shape of $A^T$ is the same as $B$ . Intuitively, the summands in the two determinants are exactly the same, but this argument is vague and complicated. It would be better if anyone has some easier proofs.","Problem. Let be an matrix and be an matrix. Show that This formula apparently holds if and are square matrices, but here this assumption fails in general. I am considering the general definition of determinant: We have The shape of is the same as and the shape of is the same as . Intuitively, the summands in the two determinants are exactly the same, but this argument is vague and complicated. It would be better if anyone has some easier proofs.",A m\times n B n\times m \det(A\otimes B)=\det(B\otimes A). A B \det(M)=\sum_{\sigma\in S_n}{(-1)^{\rm sgn(\sigma)}a_{1\sigma(1)}\cdots a_{n\sigma(n)}}. \det(B\otimes A)=\det((B\otimes A)^T)=\det(B^T\otimes A^T). B^T A A^T B,"['matrices', 'determinant', 'kronecker-product']"
73,Condition number of a positive definite matrix,Condition number of a positive definite matrix,,I want to prove that the condition number of a positive definite matrix $S$ is $$ k_2(S)= \frac{\max \lambda_i}{\min \lambda_i} $$ where $k_2$ is condition number for spectral norm. Can someone please help me with this?,I want to prove that the condition number of a positive definite matrix is where is condition number for spectral norm. Can someone please help me with this?,S  k_2(S)= \frac{\max \lambda_i}{\min \lambda_i}  k_2,"['linear-algebra', 'matrices', 'positive-definite', 'condition-number', 'spectral-norm']"
74,"$A$ is nilpotent, then $I+\lambda A$ is invertible for any $\lambda \in \mathbb{R}$","is nilpotent, then  is invertible for any",A I+\lambda A \lambda \in \mathbb{R},"I need help in this problem Let $A$ is a square real matrix such that $A^{n}=0$ for some positive integer $n .$ Such a matrix is called nilpotent. Show that if $A$ is nilpotent, then $I+\lambda A$ is invertible for any $\lambda \in \mathbb{R}$ I know that for $\lambda = 1$ it is true by $$\left(A+I\right)\left(I-A+A^2-...+(-1)^n A^{n-1}\right) = I +(-1)^{n-1} A^n = I$$ Thus proving that $A+I$ is invertible for any nilpotent $A$ . But I don't know if this is true for any $\lambda$ .","I need help in this problem Let is a square real matrix such that for some positive integer Such a matrix is called nilpotent. Show that if is nilpotent, then is invertible for any I know that for it is true by Thus proving that is invertible for any nilpotent . But I don't know if this is true for any .",A A^{n}=0 n . A I+\lambda A \lambda \in \mathbb{R} \lambda = 1 \left(A+I\right)\left(I-A+A^2-...+(-1)^n A^{n-1}\right) = I +(-1)^{n-1} A^n = I A+I A \lambda,"['linear-algebra', 'matrices', 'inverse', 'nilpotence']"
75,Generalized Vandermonde-Matrix,Generalized Vandermonde-Matrix,,"Let $z_1, \ldots, z_n \in \mathbb{C}\setminus\left\lbrace 0 \right\rbrace$ be distinct complex numbers, $\lambda_1 < \lambda_2 < \ldots < \lambda_n$ positive integers and define $$A = \left( z_i^{\lambda_k}\right)_{i,j = 1,\ldots, n} =  \begin{pmatrix} z_1^{\lambda_1} & z_1^{\lambda_2} & \cdots & z_1^{\lambda_n} \\ z_2^{\lambda_1} & z_2^{\lambda_2} & \cdots & z_2^{\lambda_n} \\ \vdots  & \vdots  & \ddots & \vdots  \\ z_n^{\lambda_1} & z_n^{\lambda_2} & \cdots & z_n^{\lambda_n}  \end{pmatrix}.$$ Is it true that $A$ is invertible? I found this related question but it deals with finite fields where in my case the underlying field is the complex plane. If $\lambda_k = k -1 $ for $k = 1, \ldots, n$ , then $A$ is the well-known Vandermonde-Matrix, so in this case the answer is positive. I have tried to calculate the determinant of $A$ analogously to how it's done if $A$ is the Vandermonde-Matrix but without success. Does anyone have a reference or a proof which answers this question? Thanks in advance...","Let be distinct complex numbers, positive integers and define Is it true that is invertible? I found this related question but it deals with finite fields where in my case the underlying field is the complex plane. If for , then is the well-known Vandermonde-Matrix, so in this case the answer is positive. I have tried to calculate the determinant of analogously to how it's done if is the Vandermonde-Matrix but without success. Does anyone have a reference or a proof which answers this question? Thanks in advance...","z_1, \ldots, z_n \in \mathbb{C}\setminus\left\lbrace 0 \right\rbrace \lambda_1 < \lambda_2 < \ldots < \lambda_n A = \left( z_i^{\lambda_k}\right)_{i,j = 1,\ldots, n} = 
\begin{pmatrix}
z_1^{\lambda_1} & z_1^{\lambda_2} & \cdots & z_1^{\lambda_n} \\
z_2^{\lambda_1} & z_2^{\lambda_2} & \cdots & z_2^{\lambda_n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
z_n^{\lambda_1} & z_n^{\lambda_2} & \cdots & z_n^{\lambda_n} 
\end{pmatrix}. A \lambda_k = k -1  k = 1, \ldots, n A A A","['matrices', 'complex-numbers', 'determinant']"
76,Proof by mathematical induction for matrices,Proof by mathematical induction for matrices,,"Let $N=((n_{ij}))$ be a $n\times n$ matrix with entries $n_{ij}= 1$ for all $1\le i, j\le n$ . $(i)$ Show that $N^2 = nN$ . I would like to use mathematical induction for the proof. For my base case, I let the square matrices to be $2 \times 2$ , and then I assume it is true for all square $n \times n$ matrices. However, when I am going to prove it is also true for all square matrices $(n+1)\times (n+1)$ , I was stuck in presenting my proof, like how can I present my proof without drawing completely the square matrices $(n+1) \times (n+1)$ out?","Let be a matrix with entries for all . Show that . I would like to use mathematical induction for the proof. For my base case, I let the square matrices to be , and then I assume it is true for all square matrices. However, when I am going to prove it is also true for all square matrices , I was stuck in presenting my proof, like how can I present my proof without drawing completely the square matrices out?","N=((n_{ij})) n\times n n_{ij}= 1 1\le i, j\le n (i) N^2 = nN 2 \times 2 n \times n (n+1)\times (n+1) (n+1) \times (n+1)","['linear-algebra', 'matrices', 'proof-writing', 'induction']"
77,Eigenvalues and matrix kronecker product,Eigenvalues and matrix kronecker product,,"I'm not able to understand why this equivalences are true for the kronecker product of a matrix and why a the eigenvalues of a kronecker product of two matrixes are the product of their eigenvalues. The book goes like this: For $A_{nxn}$ matrix with (possibly nondistinct) eigenvalues ( $\lambda_{1},\lambda_{2},...\lambda_{n}$ ) and $B_{pxp}$ with eigenvalues ( $\mu_{1},\mu_{2},...\mu_{p}$ ) then (np) eigenvalues of $A\otimes B$ are given by $\mu_{j}\lambda_{i}$ for i= 1,2,....n and j=1,2,3....p. To see this write $A$ and $B$ in their jordan form: $A=M_{A}J_{A}M^{-1}_{A}$ $B=M_{B}J_{B}M^{-1}_{B}$ Then $M_{A}\otimes M_{B}$ has inverse given by $M^{-1}_{A}\otimes M^{-1}_{A}$ Moreover, we know that the eigenvalues of $A\otimes B$ are the same as the eigenvalues of: $(M_{A}\otimes M_{B})(A\otimes B)(M^{-1}_{A}\otimes M^{-1}_{A})= (M_{A}AM^{-1}_{A})\otimes (M_{B}BM^{-1}_{B})=J_{A}\otimes J_{B}$ Can someone explain to me why: $(M_{A}\otimes M_{B})(A\otimes B)(M^{-1}_{A}\otimes M^{-1}_{A})= (M_{A}AM^{-1}_{A})\otimes (M_{B}BM^{-1}_{B})$ is true? Thanks in advance.","I'm not able to understand why this equivalences are true for the kronecker product of a matrix and why a the eigenvalues of a kronecker product of two matrixes are the product of their eigenvalues. The book goes like this: For matrix with (possibly nondistinct) eigenvalues ( ) and with eigenvalues ( ) then (np) eigenvalues of are given by for i= 1,2,....n and j=1,2,3....p. To see this write and in their jordan form: Then has inverse given by Moreover, we know that the eigenvalues of are the same as the eigenvalues of: Can someone explain to me why: is true? Thanks in advance.","A_{nxn} \lambda_{1},\lambda_{2},...\lambda_{n} B_{pxp} \mu_{1},\mu_{2},...\mu_{p} A\otimes B \mu_{j}\lambda_{i} A B A=M_{A}J_{A}M^{-1}_{A} B=M_{B}J_{B}M^{-1}_{B} M_{A}\otimes M_{B} M^{-1}_{A}\otimes M^{-1}_{A} A\otimes B (M_{A}\otimes M_{B})(A\otimes B)(M^{-1}_{A}\otimes M^{-1}_{A})= (M_{A}AM^{-1}_{A})\otimes (M_{B}BM^{-1}_{B})=J_{A}\otimes J_{B} (M_{A}\otimes M_{B})(A\otimes B)(M^{-1}_{A}\otimes M^{-1}_{A})= (M_{A}AM^{-1}_{A})\otimes (M_{B}BM^{-1}_{B})","['matrices', 'matrix-decomposition', 'kronecker-product']"
78,"generators of $SL(2,\mathbf{Q}_p)$",generators of,"SL(2,\mathbf{Q}_p)","I want to ask how to prove the normal subgroup generated by matrices $(1,𝑒;0,1)$ and $(1,0;𝑒,1)$ for $𝑒$ small is still the whole of $SL(2,\mathbf{Q}_p)$ ? This is mentioned in the answer of this question . Thanks!",I want to ask how to prove the normal subgroup generated by matrices and for small is still the whole of ? This is mentioned in the answer of this question . Thanks!,"(1,𝑒;0,1) (1,0;𝑒,1) 𝑒 SL(2,\mathbf{Q}_p)","['abstract-algebra', 'matrices']"
79,"Differences between matrices, bivectors and rank-2 tensors","Differences between matrices, bivectors and rank-2 tensors",,"I recently came across bivectors while looking into spacetime algebra, but couldn't understand their differences from the matrices, and from rank-2 tensors. While looking into bivectors, I found that they also follow the same distributivity laws as that of matrices. While being on the same question, I know that tensors are defined in vector spaces, but is it that matrices are defined on some other space, which makes it different from a rank-2 tensor? And if these three are completely different from one another, then why do we represent one with the other?","I recently came across bivectors while looking into spacetime algebra, but couldn't understand their differences from the matrices, and from rank-2 tensors. While looking into bivectors, I found that they also follow the same distributivity laws as that of matrices. While being on the same question, I know that tensors are defined in vector spaces, but is it that matrices are defined on some other space, which makes it different from a rank-2 tensor? And if these three are completely different from one another, then why do we represent one with the other?",,"['matrices', 'tensors']"
80,Find the minimum positive integer $n$ and the matrix $A^{2020}$,Find the minimum positive integer  and the matrix,n A^{2020},Let $A=a\begin{pmatrix} 1 & -1\\ 1 & 1 \end{pmatrix}$ $(a>0)$ and $I=\begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix}$ satisfy $A^4+I=\begin{pmatrix} 0 & 0\\ 0 & 0 \end{pmatrix}$ Find $a$ For this I can do. I saw $a=\frac{1}{\sqrt{2}}$ Find the minimum positive integer $n$ such that $A^n\begin{pmatrix} 0\\ 1 \end{pmatrix}= \begin{pmatrix} 1\\ 0 \end{pmatrix}$ Find $A^{2020}$ So please help to tell me about this 2 and 3. Give me some hints or ideas!!,Let and satisfy Find For this I can do. I saw Find the minimum positive integer such that Find So please help to tell me about this 2 and 3. Give me some hints or ideas!!,"A=a\begin{pmatrix} 1 & -1\\
1 & 1 \end{pmatrix} (a>0) I=\begin{pmatrix} 1 & 0\\
0 & 1 \end{pmatrix} A^4+I=\begin{pmatrix} 0 & 0\\
0 & 0 \end{pmatrix} a a=\frac{1}{\sqrt{2}} n A^n\begin{pmatrix} 0\\ 1 \end{pmatrix}= \begin{pmatrix} 1\\ 0 \end{pmatrix} A^{2020}",[]
81,Elementary block matrix operations to block-triangularize block tridiagonal matrix,Elementary block matrix operations to block-triangularize block tridiagonal matrix,,"For some flow modeling purpose, the system $(S)$ defined as $MX=L$ is solved, where $M$ is a tridiagonal block matrix defined as $$ M= \begin{pmatrix}B_1& C_1 &0 &0  \\ A_2 & B_2 &  C_2 & 0 \\ 0& A_3 & B_3 & C_3 \\ 0 & 0 & A_4 & B_4\end{pmatrix} $$ and $L$ is a block vector defined as $$ L = \begin{pmatrix} L_1 \\ L_2 \\ L_3 \\ L_4 \end{pmatrix}$$ For some physical constraints, I need to get rid of the block $A_4$ and have the last line as $$\begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix}$$ where $X$ is the resultant block after transforming $M$ . Note that $A_i$ blocks are not invertible. Here are two ideas I thought about using my humble matrices background, but I'm not sure they're right ( I'm assuming the same properties of simple matrices are available for block matrices, which is not always true.. ) : Change the third column $Col_3$ as follows : $Col_3 <- Col_3 - A_4B_4^{-1}Col_4 $ . The results retrieved were wrong... Multiplying $M$ on the left by the matrix $$ \begin{pmatrix}I& 0 &0 &0  \\ 0 & I &  0 & 0 \\ 0& 0 & I & 0 \\ Z_1 & Z_2 & Z_3 & I\end{pmatrix} $$ and try to find $Z_i$ as a function of the blocks $A_i, B_i$ and $C_i$ so I'd have the last line in a $\begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix}$ format. The computation was a bit complicated and didn't give good results. How can I transform M to have its last line in a $\begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix}$ format and have the same solution as the $(S)$ system ? Thanks!","For some flow modeling purpose, the system defined as is solved, where is a tridiagonal block matrix defined as and is a block vector defined as For some physical constraints, I need to get rid of the block and have the last line as where is the resultant block after transforming . Note that blocks are not invertible. Here are two ideas I thought about using my humble matrices background, but I'm not sure they're right ( I'm assuming the same properties of simple matrices are available for block matrices, which is not always true.. ) : Change the third column as follows : . The results retrieved were wrong... Multiplying on the left by the matrix and try to find as a function of the blocks and so I'd have the last line in a format. The computation was a bit complicated and didn't give good results. How can I transform M to have its last line in a format and have the same solution as the system ? Thanks!","(S) MX=L M  M= \begin{pmatrix}B_1& C_1 &0 &0  \\ A_2 & B_2 &  C_2 & 0 \\ 0& A_3 & B_3 & C_3 \\ 0 & 0 & A_4 & B_4\end{pmatrix}  L  L = \begin{pmatrix} L_1 \\ L_2 \\ L_3 \\ L_4 \end{pmatrix} A_4 \begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix} X M A_i Col_3 Col_3 <- Col_3 - A_4B_4^{-1}Col_4  M  \begin{pmatrix}I& 0 &0 &0  \\ 0 & I &  0 & 0 \\ 0& 0 & I & 0 \\ Z_1 & Z_2 & Z_3 & I\end{pmatrix}  Z_i A_i, B_i C_i \begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix} \begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix} (S)","['matrices', 'systems-of-equations', 'block-matrices', 'tridiagonal-matrices']"
82,Matrix-vector multiplication/cross product problem,Matrix-vector multiplication/cross product problem,,"How can I generally solve equations of the form $\mathbf{A} \mathbf{w} = \begin{pmatrix} x \\ y \\ z \end{pmatrix} \times \mathbf{w}$ for the matrix $\mathbf{A},$ where $\mathbf{w}$ can be any vector? I recognize that you could just set $\mathbf{w}$ to a vector with simple values, such as $\begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}$ , but doing so still isn't helpful. Also, $x,$ $y,$ and $z$ are entirely independent variables.","How can I generally solve equations of the form for the matrix where can be any vector? I recognize that you could just set to a vector with simple values, such as , but doing so still isn't helpful. Also, and are entirely independent variables.","\mathbf{A} \mathbf{w} =
\begin{pmatrix} x \\ y \\ z \end{pmatrix}
\times \mathbf{w} \mathbf{A}, \mathbf{w} \mathbf{w} \begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix} x, y, z","['matrices', 'algebra-precalculus', 'vectors', 'cross-product']"
83,Deletion of given matrix entry via matrix product,Deletion of given matrix entry via matrix product,,"Consider a matrix $A \in \mathbb{R}^{n \times n}$ . Is it possible to define an operator $\phi(i,j)$ which, when applied to matrix $A$ , will delete the $ij$ -th element, so that $[ \phi(i,j) A ]_{i,j} = 0$ ? If so, is it possible to define this operator in terms of the standard matrix product or the Kronecker product? If not, is there a proof that such an operator does not exist?","Consider a matrix . Is it possible to define an operator which, when applied to matrix , will delete the -th element, so that ? If so, is it possible to define this operator in terms of the standard matrix product or the Kronecker product? If not, is there a proof that such an operator does not exist?","A \in \mathbb{R}^{n \times n} \phi(i,j) A ij [ \phi(i,j) A ]_{i,j} = 0","['linear-algebra', 'matrices', 'linear-transformations']"
84,eigenvectors of an eigenvector matrix,eigenvectors of an eigenvector matrix,,"Suppose I have a square matrix $A\in \mathbb{R}^{d\times d}$ , with eigenvectors $v_1,v_2,\ldots,v_n$ . Suppose I construct a new matrix $V = [v_1\ v_2\ \cdots\ v_n]$ . Can anything be said about the eigenvalues or eigenvectors of this new matrix $V$ . Do, $A$ and $V$ have same eigenvalues? PS: I've not assumed A to be symmetric and $d$ can be greater than $n$ . But if it helps derive something, please feel free to assume so or any other assumptions required. PPS: I know that if $V$ is full rank, it diagonalizes $A$ . I'm just wondering if there is any other relation. Edit 1 : Adding some special cases, that can be considered $A$ is real-symmetric, so that $v_1, v_2, \ldots$ are orthogonal and $d=n$ . $A$ is normal (above holds).","Suppose I have a square matrix , with eigenvectors . Suppose I construct a new matrix . Can anything be said about the eigenvalues or eigenvectors of this new matrix . Do, and have same eigenvalues? PS: I've not assumed A to be symmetric and can be greater than . But if it helps derive something, please feel free to assume so or any other assumptions required. PPS: I know that if is full rank, it diagonalizes . I'm just wondering if there is any other relation. Edit 1 : Adding some special cases, that can be considered is real-symmetric, so that are orthogonal and . is normal (above holds).","A\in \mathbb{R}^{d\times d} v_1,v_2,\ldots,v_n V = [v_1\ v_2\ \cdots\ v_n] V A V d n V A A v_1, v_2, \ldots d=n A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
85,Trace of map & matrices,Trace of map & matrices,,"Let $1\leq m,n\in \mathbb{N}$ and let $\mathbb{K}$ be a field. For $a\in M_m(\mathbb{K})$ we consider the map $\mu_a$ that is defined by $$\mu_a: \mathbb{K}^{m\times n}\rightarrow \mathbb{K}^{m\times n}, \ c\mapsto ac$$ I want to show that $trace(\mu_a)=n\cdot trace(a)$ . I have done the following: Let $\lambda$ be the eigenvalues of $\mu_a$ then we have that $\mu_a(c)=\lambda c$ . From $\mu_a(c)=\lambda c$ we get $ac=\lambda c$ . So if $\lambda$ is an eigenvalue of $\mu_a$ , tthere is a non-zero $c\in\mathbb{K}^{m\times n}$ with $\mu_a(c)=\lambda c$ . The columns of $c$ are all eigenvectors of $a$ with eigenvalue $\lambda$ . The matrix $c$ has $n$ columns. So for each eigenvalue $\lambda$ of $a$ there are $n$ eigenvectors, so the multiplicity of $\lambda$ is $n$ . The trace of a matrix is the sum of teh eigenvalues considering the multiplicity. Since each eigenvalue of $\mu_a$ has a multiplicity of $n$ , it follows that $\text{trace}(\mu_a)=\sum_i n\cdot \lambda_i=n\cdot \sum_i\lambda $ . Since $\lambda_i$ is the eigenvalue of $a$ , it follows that $\text{trace}(a)=\sum_i\lambda_i$ . Therefore we get $\text{trace}(\mu_a)=n\cdot \text{trace}(a)$ . Is everything correct?","Let and let be a field. For we consider the map that is defined by I want to show that . I have done the following: Let be the eigenvalues of then we have that . From we get . So if is an eigenvalue of , tthere is a non-zero with . The columns of are all eigenvectors of with eigenvalue . The matrix has columns. So for each eigenvalue of there are eigenvectors, so the multiplicity of is . The trace of a matrix is the sum of teh eigenvalues considering the multiplicity. Since each eigenvalue of has a multiplicity of , it follows that . Since is the eigenvalue of , it follows that . Therefore we get . Is everything correct?","1\leq m,n\in \mathbb{N} \mathbb{K} a\in M_m(\mathbb{K}) \mu_a \mu_a: \mathbb{K}^{m\times n}\rightarrow \mathbb{K}^{m\times n}, \ c\mapsto ac trace(\mu_a)=n\cdot trace(a) \lambda \mu_a \mu_a(c)=\lambda c \mu_a(c)=\lambda c ac=\lambda c \lambda \mu_a c\in\mathbb{K}^{m\times n} \mu_a(c)=\lambda c c a \lambda c n \lambda a n \lambda n \mu_a n \text{trace}(\mu_a)=\sum_i n\cdot \lambda_i=n\cdot \sum_i\lambda  \lambda_i a \text{trace}(a)=\sum_i\lambda_i \text{trace}(\mu_a)=n\cdot \text{trace}(a)","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
86,Prove that $|\mathbf{A}+\mathbf{B}|=\mathbf{0}$ As per following condition,Prove that  As per following condition,|\mathbf{A}+\mathbf{B}|=\mathbf{0},"If $\mathbf{A}$ and $\mathbf{B}$ are real orthogonal matrices of the same order and $|\mathbf{B}|+|\mathbf{A}|=\mathbf{0}$ Prove that $|\mathbf{A}+\mathbf{B}|=\mathbf{0}$ My Approach:- $|\mathrm{A}|+|\mathrm{B}|=0$ $\Rightarrow|A|=-|B|$ $|\mathrm{A}| \cdot|\mathrm{B}|=-1 \quad\left[\because|\mathrm{B}|=\left|B^{-1}\right| \text {as they are orthogonal }\right]$ Let, $C=A\left(A^{T}+B^{T}\right) B$ $\Rightarrow|C|=\left|A A^{T} B+A B^{T} B\right|=|B+A| \ldots \ldots \ldots$ (i) And $|\mathrm{C}|=|\mathrm{A}|\left|A^{T}+B^{T}\right||B|=-\left|A^{T}+B^{T}\right|$ $\Rightarrow-\left|(A+B)^{T}\right|=-|A+B| \ldots \ldots \ldots \ldots$ (ii) $|A+B|=-|A+B|$ $\Rightarrow 2|A+B|=0$ $\boxed{\Rightarrow|A+B|=0}$ I am looking for another short approach.Any alternate solution would be greatly appreciated","If and are real orthogonal matrices of the same order and Prove that My Approach:- Let, (i) And (ii) I am looking for another short approach.Any alternate solution would be greatly appreciated",\mathbf{A} \mathbf{B} |\mathbf{B}|+|\mathbf{A}|=\mathbf{0} |\mathbf{A}+\mathbf{B}|=\mathbf{0} |\mathrm{A}|+|\mathrm{B}|=0 \Rightarrow|A|=-|B| |\mathrm{A}| \cdot|\mathrm{B}|=-1 \quad\left[\because|\mathrm{B}|=\left|B^{-1}\right| \text {as they are orthogonal }\right] C=A\left(A^{T}+B^{T}\right) B \Rightarrow|C|=\left|A A^{T} B+A B^{T} B\right|=|B+A| \ldots \ldots \ldots |\mathrm{C}|=|\mathrm{A}|\left|A^{T}+B^{T}\right||B|=-\left|A^{T}+B^{T}\right| \Rightarrow-\left|(A+B)^{T}\right|=-|A+B| \ldots \ldots \ldots \ldots |A+B|=-|A+B| \Rightarrow 2|A+B|=0 \boxed{\Rightarrow|A+B|=0},['linear-algebra']
87,Operator norm of a matrix in terms of its coefficients,Operator norm of a matrix in terms of its coefficients,,"Let $M:\mathbb{C}\to \mathbb{C}$ be a matrix and equip $\mathbb{C}$ with the norm $$\|x\|_\infty=\max_{1\le j\le n}|x_j|.$$ If the operator norm is given by $$\|M\|=\sup_{\|x\|=1}|Mx|,$$ is it possible to compute the operator norm exactly in terms of the matrix entries? Since $(Mx)_i=\sum_{j=1}^nm_{ij}x_j$ , we have $$\|M\|=\sup_{\|x\|_\infty=1}\max_{1\le i\le n}\bigg|\sum_{j=1}^nm_{ij}x_j\bigg|.$$ From here, it is clear how one might bound this norm, but it is not clear to me how to compute it exactly without knowledge of the matrix.","Let be a matrix and equip with the norm If the operator norm is given by is it possible to compute the operator norm exactly in terms of the matrix entries? Since , we have From here, it is clear how one might bound this norm, but it is not clear to me how to compute it exactly without knowledge of the matrix.","M:\mathbb{C}\to \mathbb{C} \mathbb{C} \|x\|_\infty=\max_{1\le j\le n}|x_j|. \|M\|=\sup_{\|x\|=1}|Mx|, (Mx)_i=\sum_{j=1}^nm_{ij}x_j \|M\|=\sup_{\|x\|_\infty=1}\max_{1\le i\le n}\bigg|\sum_{j=1}^nm_{ij}x_j\bigg|.","['matrices', 'functional-analysis', 'matrix-norms']"
88,"If $U \leq \mathbb{R}^4$, $\dim(U) =3$ and $\langle(0,0,0,1)\rangle \cap U = \{0\}$ then $U = \langle(1,0,0,0), (0,1,0,0), (0,0,1,0)\rangle$","If ,  and  then","U \leq \mathbb{R}^4 \dim(U) =3 \langle(0,0,0,1)\rangle \cap U = \{0\} U = \langle(1,0,0,0), (0,1,0,0), (0,0,1,0)\rangle","Suppose $U \leq \mathbb{R}^4$ , $\operatorname{dim}(U)=3$ and $\langle(0,0,0,1)\rangle \cap U =  \{0\}$ . Is it then true that $U = \langle(1,0,0,0), (0,1,0,0), (0,0,1,0)\rangle$ ? I say yes. Here's my reasoning: Put the three basis vectors of $U$ into RRE form. That is consider the matrix with the three basis vectors as rows. The only RRE form that doesn't contain $(0,0,0,1)$ is the matrix $\begin{bmatrix}1&0 &0 & 0\\0&1&0 &0 \\ 0 &0 &1 &0\end{bmatrix}$ . This proves the claim. Is this correct?","Suppose , and . Is it then true that ? I say yes. Here's my reasoning: Put the three basis vectors of into RRE form. That is consider the matrix with the three basis vectors as rows. The only RRE form that doesn't contain is the matrix . This proves the claim. Is this correct?","U \leq \mathbb{R}^4 \operatorname{dim}(U)=3 \langle(0,0,0,1)\rangle \cap U =  \{0\} U = \langle(1,0,0,0), (0,1,0,0), (0,0,1,0)\rangle U (0,0,0,1) \begin{bmatrix}1&0 &0 & 0\\0&1&0 &0 \\ 0 &0 &1 &0\end{bmatrix}",['linear-algebra']
89,$p$-Norm of Block Diagonal Matrix,-Norm of Block Diagonal Matrix,p,"Let $A\in \mathbb{K}^{r\times r}$ , $B\in \mathbb{K}^{(n-r)\times (n-r)}$ and $C = \mathbb{K}^{n\times n}$ such that \begin{equation} C = \begin{pmatrix} A & 0_{r\times (n-r)}\\ 0_{(n-r)\times r} & B\\ \end{pmatrix}. \end{equation} I want to prove $||C||_p = \max\{||A||_p, ||B||_p\}$ for all induced $p$ -matrix norms. I already showed the direction "" $\ge$ "" by writing \begin{equation} ||C||_p = \max_{||x||_p = 1} ||Cx||_p \ge \max_{\substack{||x||_p = 1\\ x_j=0\ \forall\ j>r}} ||Cx||_p = \max_{||x||_p = 1} ||Ax||_p = ||A||_p \end{equation} and the same for $B$ , thus $||C||_p \ge \max\{||A||_p, ||B||_p\}$ . However, I do not see an easy approach to prove the direction "" $\le$ "". Any help is appreciated.","Let , and such that I want to prove for all induced -matrix norms. I already showed the direction "" "" by writing and the same for , thus . However, I do not see an easy approach to prove the direction "" "". Any help is appreciated.","A\in \mathbb{K}^{r\times r} B\in \mathbb{K}^{(n-r)\times (n-r)} C = \mathbb{K}^{n\times n} \begin{equation}
C =
\begin{pmatrix}
A & 0_{r\times (n-r)}\\
0_{(n-r)\times r} & B\\
\end{pmatrix}.
\end{equation} ||C||_p = \max\{||A||_p, ||B||_p\} p \ge \begin{equation}
||C||_p = \max_{||x||_p = 1} ||Cx||_p \ge \max_{\substack{||x||_p = 1\\ x_j=0\ \forall\ j>r}} ||Cx||_p = \max_{||x||_p = 1} ||Ax||_p = ||A||_p
\end{equation} B ||C||_p \ge \max\{||A||_p, ||B||_p\} \le","['linear-algebra', 'matrices', 'normed-spaces', 'block-matrices']"
90,Symmetric Matrix over a finite field of Characteristic 2,Symmetric Matrix over a finite field of Characteristic 2,,"Let $M$ be a $n$ by $n$ symmetric matrix over a finite field of Characteristic 2. Suppose that the entries in the diagonal of $M$ are all zero, and $n$ is an odd number. I found that the rank of $M$ is at most $n-1$ . Is my observation true? How do we prove it? Thanks","Let be a by symmetric matrix over a finite field of Characteristic 2. Suppose that the entries in the diagonal of are all zero, and is an odd number. I found that the rank of is at most . Is my observation true? How do we prove it? Thanks",M n n M n M n-1,"['linear-algebra', 'matrices', 'finite-fields', 'matrix-rank']"
91,Matrix of paths from graph $G_1$ to graph $G_2$ to graph $G_3$,Matrix of paths from graph  to graph  to graph,G_1 G_2 G_3,"If $A$ is the adjacency matrix of graph $G$ , then it is a well know property that $A^n$ is a matrix where the element $(i, j)$ gives the number of walks of length $n$ from vertex $i$ to vertex $j$ . The matrix of paths is different and there are closed forms for paths of length 2 and 3 ( https://mathworld.wolfram.com/GraphPath.html ). For example, the matrix of paths of length 3 is given by: $$ P_3 = A^3 - \text{diag}(A^2) \cdot A - A \cdot \text{diag}(A^2) + A \times A^T - \text{diag}(A^3) $$ ("" $\cdot$ "" is normal matrix multiplication, "" $\times$ "" is element-wise multiplication and ""diag $(A)$ "" has the same principal diagonal as A with the remaining elements set to zero) I am interested in the matrix of paths of length 3, for 3 (potentially different) graphs. The interpretation of the product of two different adjacency matrices $A \cdot B$ is ( from this post ): ""Cell $(i,j)$ in $A \cdot B$ contains the number of walks from $i$ to $j$ where the   first step is in $A$ , but the second step is in $B$ "" Given three undirected graphs $G_1$ , $G_2$ and $G_3$ with (symmetric since undirected) adjacency matrices $A$ , $B$ and $C$ , what I need is an expression for the matrix of 3-paths from $G_1$ to $G_2$ to $G_3$ . My attempt at this is: $$ A \cdot B \cdot C + (A \cdot B \cdot C)^T - C \cdot \text{diag}(A \cdot B) - \text{diag}(A \cdot B) \cdot C - \text{diag}(A \cdot B \cdot C) $$ However, this is just based on observations and tweaking. It works on a non-trivial example, but I do not know if it is correct. What I'd like is: To confirm this is correct, or if it is not, find the correct expression. Have a sketch/intuition of a proof","If is the adjacency matrix of graph , then it is a well know property that is a matrix where the element gives the number of walks of length from vertex to vertex . The matrix of paths is different and there are closed forms for paths of length 2 and 3 ( https://mathworld.wolfram.com/GraphPath.html ). For example, the matrix of paths of length 3 is given by: ("" "" is normal matrix multiplication, "" "" is element-wise multiplication and ""diag "" has the same principal diagonal as A with the remaining elements set to zero) I am interested in the matrix of paths of length 3, for 3 (potentially different) graphs. The interpretation of the product of two different adjacency matrices is ( from this post ): ""Cell in contains the number of walks from to where the   first step is in , but the second step is in "" Given three undirected graphs , and with (symmetric since undirected) adjacency matrices , and , what I need is an expression for the matrix of 3-paths from to to . My attempt at this is: However, this is just based on observations and tweaking. It works on a non-trivial example, but I do not know if it is correct. What I'd like is: To confirm this is correct, or if it is not, find the correct expression. Have a sketch/intuition of a proof","A G A^n (i, j) n i j 
P_3 = A^3 - \text{diag}(A^2) \cdot A - A \cdot \text{diag}(A^2) + A \times A^T - \text{diag}(A^3)
 \cdot \times (A) A \cdot B (i,j) A \cdot B i j A B G_1 G_2 G_3 A B C G_1 G_2 G_3 
A \cdot B \cdot C + (A \cdot B \cdot C)^T - C \cdot \text{diag}(A \cdot B) - \text{diag}(A \cdot B) \cdot C - \text{diag}(A \cdot B \cdot C)
","['combinatorics', 'matrices', 'discrete-mathematics', 'graph-theory', 'adjacency-matrix']"
92,Generalised Fibonacci Sequence & Linear Algebra,Generalised Fibonacci Sequence & Linear Algebra,,"Consider a generalised fibonacci $G$ sequence $1, 1, 1, 3, 5, 9, 17...$ that's created by summing the last 3 entries in the sequence together: $G_0 = 1, G_1 = 1, G_2 = 1$ and $G_{n+1} = G_n + G_{n-1} + G_{n-2}$ for $n \ge 2$ . 1) Find a $3 \times3$ matrix M such that, for any $k \ge 2$ , $$\begin{pmatrix} G_{k+1} \\ G_k \\ G_{k-1} \end{pmatrix} = M \begin{pmatrix} G_{k} \\ G_{k-1} \\ G_{k-2} \end{pmatrix}$$ I figured out that $M = \begin{pmatrix} 1 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}$ from the given equation. 2) Find a numerical value for $G_{25}$ With a bit of thinking, I came up with the equation $$\begin{pmatrix} G_{k+1} \\ G_k \\ G_{k-1} \end{pmatrix} = M^k \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$$ Hence $G_{25}$ would be obtained by the first element from $M^{24}$ times [1, 1, 1]. Using a calculator, I got $3311233$ , which apparently is wrong. Was my equation above wrong? How could I approach this? Find limit of $\frac{\ln G_n}{n}$ to 10 decimal places as $n$ goes to $\infty$ . I have no idea how to approach this. I thought of using eigenvalues and eigenvectors, but since the M I found only gives me one real value of 1.83929... I don't quite see how that's going to be useful. Any help would be really appreciated.","Consider a generalised fibonacci sequence that's created by summing the last 3 entries in the sequence together: and for . 1) Find a matrix M such that, for any , I figured out that from the given equation. 2) Find a numerical value for With a bit of thinking, I came up with the equation Hence would be obtained by the first element from times [1, 1, 1]. Using a calculator, I got , which apparently is wrong. Was my equation above wrong? How could I approach this? Find limit of to 10 decimal places as goes to . I have no idea how to approach this. I thought of using eigenvalues and eigenvectors, but since the M I found only gives me one real value of 1.83929... I don't quite see how that's going to be useful. Any help would be really appreciated.","G 1, 1, 1, 3, 5, 9, 17... G_0 = 1, G_1 = 1, G_2 = 1 G_{n+1} = G_n + G_{n-1} + G_{n-2} n \ge 2 3 \times3 k \ge 2 \begin{pmatrix} G_{k+1} \\ G_k \\ G_{k-1} \end{pmatrix} = M \begin{pmatrix} G_{k} \\ G_{k-1} \\ G_{k-2} \end{pmatrix} M = \begin{pmatrix} 1 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix} G_{25} \begin{pmatrix} G_{k+1} \\ G_k \\ G_{k-1} \end{pmatrix} = M^k \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} G_{25} M^{24} 3311233 \frac{\ln G_n}{n} n \infty","['linear-algebra', 'matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'fibonacci-numbers']"
93,Effect of adding a matrix to both numerator and denominator of a ratio between determinants of two matrices,Effect of adding a matrix to both numerator and denominator of a ratio between determinants of two matrices,,"Assume matrix $A$ is symmetric and positive definite, and matrices $B$ and $C$ are symmetric and positive semi-definite. Originally I have ratio between determinants $$\frac{\det(A+B)}{\det(A)}$$ which is obviously greater than or equal to 1. How would this ratio change (increase or decrease) when I add another matrix $C$ inside the determinant on both numerator and determinator, as follows? $$\frac{\det(A+B+C)}{\det(A+C)}$$ My intuition is that $$\frac{\det(A+B+C)}{\det(A+C)} \leq \frac{\det(A+B)}{\det(A)}$$ but I haven't been able to prove this. Any insight on this is appreciated!","Assume matrix is symmetric and positive definite, and matrices and are symmetric and positive semi-definite. Originally I have ratio between determinants which is obviously greater than or equal to 1. How would this ratio change (increase or decrease) when I add another matrix inside the determinant on both numerator and determinator, as follows? My intuition is that but I haven't been able to prove this. Any insight on this is appreciated!",A B C \frac{\det(A+B)}{\det(A)} C \frac{\det(A+B+C)}{\det(A+C)} \frac{\det(A+B+C)}{\det(A+C)} \leq \frac{\det(A+B)}{\det(A)},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
94,n-th power of a matrix using the division of polynomials.,n-th power of a matrix using the division of polynomials.,,"Consider the matrix $$ A=\begin{pmatrix} 0 & 0 & 0\\ -2 & 1 & -1\\ 2 & 0 & 2 \end{pmatrix} $$ Calculate $A^3-3A^2+2A$ . What is the remainder of the division of the polynomial $X^n$ by the polynomial $X^3-3X^2+2X$ . Calculate $A^n$ for every natural number $n$ . I was solving the following problem and I was stuck in it. For part 1) the answer was the zero matrix. In part 2) I use the usual division and i get the following $$ X^n=X^{n-3}(X^3-3X^2+2X)+3X^{n-1}-2X^{n-2}. $$ When I pass to part 3) and using part 1) and 2), we obtain $$ A^n=3A^{n-1}-2A^{n-2}. $$ Using the fact that $A^3-3A^2+2A=O_{3\times 3}$ . but if I use this answer for calculating $A^2$ the answer is not correct, so I think $A^n$ obtained is not correct. Now, one can use the diagonalization of the matrix $A$ and obtain $$ A^n=\begin{pmatrix} 0 & 0 & 0\\ -2^n & 1 & 1-2^n\\ 2^n & 0 & 2^n \end{pmatrix} $$ Can you help me in proving part 2 (if not correct) and part 3 without using the diagonalization method.","Consider the matrix Calculate . What is the remainder of the division of the polynomial by the polynomial . Calculate for every natural number . I was solving the following problem and I was stuck in it. For part 1) the answer was the zero matrix. In part 2) I use the usual division and i get the following When I pass to part 3) and using part 1) and 2), we obtain Using the fact that . but if I use this answer for calculating the answer is not correct, so I think obtained is not correct. Now, one can use the diagonalization of the matrix and obtain Can you help me in proving part 2 (if not correct) and part 3 without using the diagonalization method.","
A=\begin{pmatrix}
0 & 0 & 0\\
-2 & 1 & -1\\
2 & 0 & 2
\end{pmatrix}
 A^3-3A^2+2A X^n X^3-3X^2+2X A^n n 
X^n=X^{n-3}(X^3-3X^2+2X)+3X^{n-1}-2X^{n-2}.
 
A^n=3A^{n-1}-2A^{n-2}.
 A^3-3A^2+2A=O_{3\times 3} A^2 A^n A 
A^n=\begin{pmatrix}
0 & 0 & 0\\
-2^n & 1 & 1-2^n\\
2^n & 0 & 2^n
\end{pmatrix}
","['linear-algebra', 'matrices', 'polynomials', 'matrix-calculus']"
95,"When does a matrix have an ""invariant quadratic form""?","When does a matrix have an ""invariant quadratic form""?",,"Yesterday I computed that the matrix $$ A = \begin{pmatrix} 2&1\\1&1\end{pmatrix}$$ satisfies $q(m,n) = q \left((m,n)A\right)$ for the quadratic form $$q(m,n) = m^2 - mn - n^2.$$ E.g., $-1 = q(1,1) = q(3,2) = q(8,5) =\ \ldots\ $ which is quite satisfying. On the other hand, the matrix $$B = \begin{pmatrix} 1&1\\1&0 \end{pmatrix}$$ fixes no such quadratic form, although it does preserve $(m,n)\mapsto (q(m,n))^2$ (since $B^2 = A$ this is maybe unsurprising). My question. Is it known when a square matrix with integer entries preserves a non-trivial quadratic form? Moreover, when does such a quadratic form have integer coefficients? It seems easy to verify for individual examples, but is there a general theory?","Yesterday I computed that the matrix satisfies for the quadratic form E.g., which is quite satisfying. On the other hand, the matrix fixes no such quadratic form, although it does preserve (since this is maybe unsurprising). My question. Is it known when a square matrix with integer entries preserves a non-trivial quadratic form? Moreover, when does such a quadratic form have integer coefficients? It seems easy to verify for individual examples, but is there a general theory?","
A = \begin{pmatrix} 2&1\\1&1\end{pmatrix} q(m,n) = q \left((m,n)A\right) q(m,n) = m^2 - mn - n^2. -1 = q(1,1) = q(3,2) = q(8,5) =\ \ldots\  B = \begin{pmatrix} 1&1\\1&0 \end{pmatrix} (m,n)\mapsto (q(m,n))^2 B^2 = A","['matrices', 'quadratic-forms', 'integer-lattices']"
96,Extreme points in the intersection of hyperplanes and hypercube,Extreme points in the intersection of hyperplanes and hypercube,,"Let $A$ be any $c \times n$ matrix such that $c < n$ and let $b$ be any $c\times 1$ vector. It is also known that $A$ and $b$ are element-wise positive. Consider the set defined as \begin{align} \mathcal{S}=\{x~|~Ax\leq b~,~x\in[0,1]^n\} \end{align} where $[0,1]^n$ is the standard hypercube. If $\mathcal{S}$ is non-empty with at least two points, it is easy to see that $\mathcal{S}$ is a closed convex set. I am interested in the extreme points of $\mathcal{S}$ . Extreme points are the points which can never be contained inside a non-degenerate line in $\mathcal{S}$ (can be thought of as corners). Another way to say is, they can never be written as convex-combinations of two other points in $\mathcal{S}$ . Is the following statement true? Question: if $e$ is an extreme point of $\mathcal{S}$ , the number of components in vector $e$ such that $0<e_i<1$ is upper-bounded by $c$ .","Let be any matrix such that and let be any vector. It is also known that and are element-wise positive. Consider the set defined as where is the standard hypercube. If is non-empty with at least two points, it is easy to see that is a closed convex set. I am interested in the extreme points of . Extreme points are the points which can never be contained inside a non-degenerate line in (can be thought of as corners). Another way to say is, they can never be written as convex-combinations of two other points in . Is the following statement true? Question: if is an extreme point of , the number of components in vector such that is upper-bounded by .","A c \times n c < n b c\times 1 A b \begin{align}
\mathcal{S}=\{x~|~Ax\leq b~,~x\in[0,1]^n\}
\end{align} [0,1]^n \mathcal{S} \mathcal{S} \mathcal{S} \mathcal{S} \mathcal{S} e \mathcal{S} e 0<e_i<1 c","['linear-algebra', 'matrices', 'convex-analysis', 'convex-optimization', 'linear-programming']"
97,Diagonalize matrix multiplication,Diagonalize matrix multiplication,,"Let $V$ be the space of $2\times 2$ matrices with complex coefficients. Let $A \in V$ and let $L_A:V \to V$ , defined by $L_A(X)=A\cdot X$ . I am trying to solve the exercise (10) from this book : find a basis in $V$ such that the $4x4$ matrix of $L_A$ is block diagonal i.e. is of the form $$\left(\begin{array}{cc} A & 0 \\ 0 & B\end{array}\right).$$ With $A$ and $B$ $2\times 2$ matrices. The linear map $L_A$ is diagonalisable as a map from $\mathbb{C}^4 \to \mathbb{C}^4$ , but I'm not sure how to obtain the required form and besides, the eigenvalues look rather ugly.","Let be the space of matrices with complex coefficients. Let and let , defined by . I am trying to solve the exercise (10) from this book : find a basis in such that the matrix of is block diagonal i.e. is of the form With and matrices. The linear map is diagonalisable as a map from , but I'm not sure how to obtain the required form and besides, the eigenvalues look rather ugly.","V 2\times 2 A \in V L_A:V \to V L_A(X)=A\cdot X V 4x4 L_A \left(\begin{array}{cc}
A & 0 \\ 0 & B\end{array}\right). A B 2\times 2 L_A \mathbb{C}^4 \to \mathbb{C}^4","['linear-algebra', 'matrices', 'linear-transformations', 'tensors', 'tensor-rank']"
98,"If $A$ and $B$ be two non-singular matrices of order $3$ such that $2A+3BB^{T}=I$ and $B^{-1}=A^{T}$, then determine the value of the following:","If  and  be two non-singular matrices of order  such that  and , then determine the value of the following:",A B 3 2A+3BB^{T}=I B^{-1}=A^{T},"If $A$ and $B$ be two non-singular matrices of order $3$ such that $2A+3BB^{T}=I$ and $B^{-1}=A^{T}$ , then determine the value of the following: (i) $\det\left(B^{T}-2B+3B^{3}+3BA\right)$ (ii) $\det\left(A^{-1}-3B^{3}\right)$ (iii) $\text{trace}\left(A^{-1}+I-AB-3B^{3}\right)$ My Attempt: Given $2A+3BB^{T}=I$ Taking transpose $2A^{T}+3BB^{T}=I$ So on subtracting we get $A=A^{T}$ i.e. $A$ is symmetric Given that, $B^{-1}=A^{T}=A$ and thus $AB=BA=I$ Now, all I need to get the answers to the above questions is to somehow prove that $B$ is also symmetric i.e. $B^{T}=B$ $AB=I$ $B^TA^T=I$ $B^TB^{-1}=I$ $B^T=B$ Given that $2A+3BB^T=I$ $2B^{-1}+3B^2=I$ $B-3B^3=2I$ Thus, $B^T-2B+3B^3+3BA=B-2B+3B^3+3I=3B^3-B+3I=-2I+3I=I$ Also, $A^{-1}+I-AB-3B^3=A^{-1}-3B^3=B-3B^3=2I$","If and be two non-singular matrices of order such that and , then determine the value of the following: (i) (ii) (iii) My Attempt: Given Taking transpose So on subtracting we get i.e. is symmetric Given that, and thus Now, all I need to get the answers to the above questions is to somehow prove that is also symmetric i.e. Given that Thus, Also,",A B 3 2A+3BB^{T}=I B^{-1}=A^{T} \det\left(B^{T}-2B+3B^{3}+3BA\right) \det\left(A^{-1}-3B^{3}\right) \text{trace}\left(A^{-1}+I-AB-3B^{3}\right) 2A+3BB^{T}=I 2A^{T}+3BB^{T}=I A=A^{T} A B^{-1}=A^{T}=A AB=BA=I B B^{T}=B AB=I B^TA^T=I B^TB^{-1}=I B^T=B 2A+3BB^T=I 2B^{-1}+3B^2=I B-3B^3=2I B^T-2B+3B^3+3BA=B-2B+3B^3+3I=3B^3-B+3I=-2I+3I=I A^{-1}+I-AB-3B^3=A^{-1}-3B^3=B-3B^3=2I,"['linear-algebra', 'matrices', 'matrix-rank', 'symmetric-matrices', 'orthogonal-matrices']"
99,SVD (or polar decomposition) of a matrix plus a constant,SVD (or polar decomposition) of a matrix plus a constant,,"Say I have a matrix $\hat M$ with a singular value decomposition (SVD) $\hat M=\hat U\hat D\hat V ^\dagger$ . Given this SVD, is there a simple way to get the SVD of $(\hat{M}-z\hat{1})$ , for $z\in\mathbb{C}$ some constant (here $\hat 1$ denotes the identity matrix)? Equivalently, if I have a polar decomposition $\hat{M}=\hat U\hat P$ , is there a simple way to get the polar decomposition of $(\hat{M}-z\hat{1})$ ? I'm actually only interested in getting the product $\hat{U}\hat{V}^\dagger$ in the case of the SVD, or the unitary part $\hat{U}$ in the polar decomposition, so if there's a method that doesn't get me the singular values $\hat{D}$ or positive-definite part $\hat P$ , that's fine too.","Say I have a matrix with a singular value decomposition (SVD) . Given this SVD, is there a simple way to get the SVD of , for some constant (here denotes the identity matrix)? Equivalently, if I have a polar decomposition , is there a simple way to get the polar decomposition of ? I'm actually only interested in getting the product in the case of the SVD, or the unitary part in the polar decomposition, so if there's a method that doesn't get me the singular values or positive-definite part , that's fine too.",\hat M \hat M=\hat U\hat D\hat V ^\dagger (\hat{M}-z\hat{1}) z\in\mathbb{C} \hat 1 \hat{M}=\hat U\hat P (\hat{M}-z\hat{1}) \hat{U}\hat{V}^\dagger \hat{U} \hat{D} \hat P,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd']"
