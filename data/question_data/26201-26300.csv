,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that the cyclic shift operator is unitary,Show that the cyclic shift operator is unitary,,Show that the cyclic shift operator is unitary and determine its diagonalization: $$A=\begin{bmatrix}        0&1     \\[0.3em]       &0&1 \\[0.3em]         & & \ddots \\ &&&.&1\\ 1&&&&0      \end{bmatrix}.$$,Show that the cyclic shift operator is unitary and determine its diagonalization: $$A=\begin{bmatrix}        0&1     \\[0.3em]       &0&1 \\[0.3em]         & & \ddots \\ &&&.&1\\ 1&&&&0      \end{bmatrix}.$$,,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'permutation-matrices', 'circulant-matrices']"
1,Is there a faster way to do this? Find an orthogonal matrix $P$ and a diagonal matrix $D$ such that $A=PDP^T$,Is there a faster way to do this? Find an orthogonal matrix  and a diagonal matrix  such that,P D A=PDP^T,"Let $A$=\begin{pmatrix} 0 & 1 & 1\\  1 & 0 & 1\\  1 & 1 & 0 \end{pmatrix} Find an orthogonal matrix $P$ and a diagonal matrix $D$ such that $A=PDP^T$. (Hint: The eigenvalues of $A$ are all integers. I managed to do this by finding the eigenvalues through solving the characteristic equation, and then finding the corresponding eigenspace. After that I went ahead to use the Grand-Schdmit process, and got an orthnormal basis. My solution was mighty complicated, involving 1.5 pages of workings. Is there a faster way to do this? Question is worth 5 marks.","Let $A$=\begin{pmatrix} 0 & 1 & 1\\  1 & 0 & 1\\  1 & 1 & 0 \end{pmatrix} Find an orthogonal matrix $P$ and a diagonal matrix $D$ such that $A=PDP^T$. (Hint: The eigenvalues of $A$ are all integers. I managed to do this by finding the eigenvalues through solving the characteristic equation, and then finding the corresponding eigenspace. After that I went ahead to use the Grand-Schdmit process, and got an orthnormal basis. My solution was mighty complicated, involving 1.5 pages of workings. Is there a faster way to do this? Question is worth 5 marks.",,['linear-algebra']
2,Degrees of Freedom of a Unitary Matrix Proof,Degrees of Freedom of a Unitary Matrix Proof,,"My book states that a 3$\times$3 unitary matrix has 9 degrees of freedom, but I have trouble seeing why that is true. So I want to try and show that an $n\times n$ matrix has $n^2$ degrees of freedom. Base case $n=1$: This is just a matrix with one entry which has length 1. It is clear that any entry with length 1 will do. So I have $1 = 1^2$ degree of freedom $n \implies n+1$: Suppose we have an $n \times n$ unitary matrix and it has $n^2$ degrees of freedom. I want to now show that a $(n+1) \times (n+1)$ unitary matrix has $(n+1)^2$ degrees of freedom. If I add a $(n+1)$th row and column to the $n \times n$ unitary matrix, I have added $(n+1) + n = 2n + 1$ entries. For every entry $a_{ij}$ I have added, I need to ensure that each $a_{ij}^2 = 0$ in order for the matrix to be unitary. Since there are numerous ways to choose $a_{ij}$ such that $a_{ij}^2$ = 0, I have added $2n+1$ degrees of freedom. So the total degrees of freedom is $n^2 + 2n + 1 = (n+1)^2$. Can I get some feedback on this proof? Thanks in advance! Note It seems like my approach above is at a dead end.","My book states that a 3$\times$3 unitary matrix has 9 degrees of freedom, but I have trouble seeing why that is true. So I want to try and show that an $n\times n$ matrix has $n^2$ degrees of freedom. Base case $n=1$: This is just a matrix with one entry which has length 1. It is clear that any entry with length 1 will do. So I have $1 = 1^2$ degree of freedom $n \implies n+1$: Suppose we have an $n \times n$ unitary matrix and it has $n^2$ degrees of freedom. I want to now show that a $(n+1) \times (n+1)$ unitary matrix has $(n+1)^2$ degrees of freedom. If I add a $(n+1)$th row and column to the $n \times n$ unitary matrix, I have added $(n+1) + n = 2n + 1$ entries. For every entry $a_{ij}$ I have added, I need to ensure that each $a_{ij}^2 = 0$ in order for the matrix to be unitary. Since there are numerous ways to choose $a_{ij}$ such that $a_{ij}^2$ = 0, I have added $2n+1$ degrees of freedom. So the total degrees of freedom is $n^2 + 2n + 1 = (n+1)^2$. Can I get some feedback on this proof? Thanks in advance! Note It seems like my approach above is at a dead end.",,"['linear-algebra', 'induction']"
3,What would be complexity of computing $3^{n^n}$?,What would be complexity of computing ?,3^{n^n},"Just curious, what would be the computational complexity of computing $3^{n^n}$? I am not sure what it would be like.","Just curious, what would be the computational complexity of computing $3^{n^n}$? I am not sure what it would be like.",,"['linear-algebra', 'number-theory', 'computational-complexity']"
4,Spectrum of a block matrix,Spectrum of a block matrix,,"Let $A$ be real , symmetric of order $n$ with $A > 0$ and the eigenvalues of $A$ are $a_1 \ge a_2 \ge \cdots \ge a_n > 0$. Let $B$ be a real matrix of order $n\times m$ ($m\le n$), $\operatorname{rank}B=m$ and whose singular values are $b_1 \ge \cdots \ge b_m > 0$. If \[X = \begin{pmatrix} A   &  B \\         B^t &  0\end{pmatrix}\] then show that the spectrum of $X$ is a subset of $P \cup Q$ where $$P =\left [ \frac 12\left(a_n-\sqrt{a_n^2+4b_1^2}\right) , \frac 12\left(a_1-\sqrt{a_1^2+4b_m^2}\right)\right]$$ and $$       Q = \left[ a_n , \frac 12 \left(a_1+\sqrt{a_1^2+4b_1^2}\right)\right].$$ Thanks for any help.","Let $A$ be real , symmetric of order $n$ with $A > 0$ and the eigenvalues of $A$ are $a_1 \ge a_2 \ge \cdots \ge a_n > 0$. Let $B$ be a real matrix of order $n\times m$ ($m\le n$), $\operatorname{rank}B=m$ and whose singular values are $b_1 \ge \cdots \ge b_m > 0$. If \[X = \begin{pmatrix} A   &  B \\         B^t &  0\end{pmatrix}\] then show that the spectrum of $X$ is a subset of $P \cup Q$ where $$P =\left [ \frac 12\left(a_n-\sqrt{a_n^2+4b_1^2}\right) , \frac 12\left(a_1-\sqrt{a_1^2+4b_m^2}\right)\right]$$ and $$       Q = \left[ a_n , \frac 12 \left(a_1+\sqrt{a_1^2+4b_1^2}\right)\right].$$ Thanks for any help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'singular-values']"
5,set of all trace $1$ matrices are connected?,set of all trace  matrices are connected?,1,"The heading is the question and here are my two approach, I want to know are they correct or not, if not I need to know the answer: 1) They are path connected as $\gamma(t)=At+(1-t)B, t\in [0,1]$  where $A$ and $B$ are trace one matrices. But I am not sure all matrices in this path are trace $1$? 2) as trace equals $1$, so considering diagonal entries I get a hyperplane $H$ with $x_{11}+\dots+x_{nn}=1$ and remaining other $n^2-n$ entries I can send to $\mathbb{R}^{n^2-n}$ and thus they are homeomorphic to $H\times \mathbb{R}^{n^2-n}$ as this is a product of two connected topological spaces, it is connected. So trace 1 matrices are connected. Well, here I have considered a matrix is just a point in $\mathbb{R}^{n^2}$ Thank you.","The heading is the question and here are my two approach, I want to know are they correct or not, if not I need to know the answer: 1) They are path connected as $\gamma(t)=At+(1-t)B, t\in [0,1]$  where $A$ and $B$ are trace one matrices. But I am not sure all matrices in this path are trace $1$? 2) as trace equals $1$, so considering diagonal entries I get a hyperplane $H$ with $x_{11}+\dots+x_{nn}=1$ and remaining other $n^2-n$ entries I can send to $\mathbb{R}^{n^2-n}$ and thus they are homeomorphic to $H\times \mathbb{R}^{n^2-n}$ as this is a product of two connected topological spaces, it is connected. So trace 1 matrices are connected. Well, here I have considered a matrix is just a point in $\mathbb{R}^{n^2}$ Thank you.",,"['linear-algebra', 'general-topology']"
6,Find all real numbers $t$ such that the quadratic form $f$ is positive definite.,Find all real numbers  such that the quadratic form  is positive definite.,t f,"Where $$f(x_1,x_2,x_3)=2x_1^2+x_2^2+3x_3^2+2tx_1x_2+2x_1x_3$$. This is a problem in my Matrix Analysis homework. Below is my effort. Let $x=(x_1,x_2,x_3)^T$, then we have $$f=x^*Sx$$, in which $$S=\left(\begin{matrix}2&t&1\\t&1&0\\1&0&3\end{matrix}\right)$$. $f$ is positive definite is equivalent to $S$ is positive definite which is equivalent to all the eigenvalues of $S$ is positive. The characteristic polynomial of $S$ is: $$\begin{align}|\lambda I-S|&=-\lambda^3+6\lambda^2+(3t^2-10)\lambda+(-3t^2+5)\\&=(-3+3\lambda)t^2+(-\lambda^3+6\lambda^2-10\lambda+5)\end{align}$$. Now the only problem left is that how do I find all the possible real values of $t$ that makes this polynomial of $\lambda$ only has positive roots?","Where $$f(x_1,x_2,x_3)=2x_1^2+x_2^2+3x_3^2+2tx_1x_2+2x_1x_3$$. This is a problem in my Matrix Analysis homework. Below is my effort. Let $x=(x_1,x_2,x_3)^T$, then we have $$f=x^*Sx$$, in which $$S=\left(\begin{matrix}2&t&1\\t&1&0\\1&0&3\end{matrix}\right)$$. $f$ is positive definite is equivalent to $S$ is positive definite which is equivalent to all the eigenvalues of $S$ is positive. The characteristic polynomial of $S$ is: $$\begin{align}|\lambda I-S|&=-\lambda^3+6\lambda^2+(3t^2-10)\lambda+(-3t^2+5)\\&=(-3+3\lambda)t^2+(-\lambda^3+6\lambda^2-10\lambda+5)\end{align}$$. Now the only problem left is that how do I find all the possible real values of $t$ that makes this polynomial of $\lambda$ only has positive roots?",,['linear-algebra']
7,Can a transformation matrix be expressed in terms of the vector to be transformed?,Can a transformation matrix be expressed in terms of the vector to be transformed?,,"I'm currently learning linear algebra with my friend via an online course, and we have a disagreement that we would like settled. Upon learning that vectors can be projected onto lines by a simple function, and that this function is a linear transformation, I recommended that we find a way to calculate the transformation matrix for the function. This function was characterized by a vector $\vec{v}$ pointing along the line to be projected onto.  The non-matrix form of the function we were given was this: $$proj_{\vec{v}}(\vec{x}) = (\vec{x} \cdot \hat{u})\hat{u}$$ where $\hat{u}$ is the normalized form of $\vec{v}$ calculated by ($\frac{1}{||\vec{v}||}\vec{v}$). When trying to construct this as a matrix, my friend came up with this (this is just for $\mathbb{R}^{3}$, but you get the idea): $$proj_{\vec{v}}(\vec{x}) = \left[\begin{array}{ccc} \vec{x} \cdot \hat{u} & 0 & 0 \\ 0 & \vec{x} \cdot \hat{u} & 0 \\ 0 & 0 & \vec{x} \cdot \hat{u} \end{array}\right] \cdot \hat{u}$$ I see multiple problems with this.  First of all, transformation matrices cannot be expressed in terms of the vector to be transformed, can they?  To my understanding, they are supposed to contain constant values that are the same no matter what vector is being transformed.  Second of all, linear transformations multiply $\vec{x}$ by the transformation matrix, not any other vector, right? I'm not particularly interested in the actual transformation matrix for this problem - I plan to get some practice with what i've learned by calculating that :) - I'd just like confirmation as to whether or not the things my friend has done are valid.  Thank you!","I'm currently learning linear algebra with my friend via an online course, and we have a disagreement that we would like settled. Upon learning that vectors can be projected onto lines by a simple function, and that this function is a linear transformation, I recommended that we find a way to calculate the transformation matrix for the function. This function was characterized by a vector $\vec{v}$ pointing along the line to be projected onto.  The non-matrix form of the function we were given was this: $$proj_{\vec{v}}(\vec{x}) = (\vec{x} \cdot \hat{u})\hat{u}$$ where $\hat{u}$ is the normalized form of $\vec{v}$ calculated by ($\frac{1}{||\vec{v}||}\vec{v}$). When trying to construct this as a matrix, my friend came up with this (this is just for $\mathbb{R}^{3}$, but you get the idea): $$proj_{\vec{v}}(\vec{x}) = \left[\begin{array}{ccc} \vec{x} \cdot \hat{u} & 0 & 0 \\ 0 & \vec{x} \cdot \hat{u} & 0 \\ 0 & 0 & \vec{x} \cdot \hat{u} \end{array}\right] \cdot \hat{u}$$ I see multiple problems with this.  First of all, transformation matrices cannot be expressed in terms of the vector to be transformed, can they?  To my understanding, they are supposed to contain constant values that are the same no matter what vector is being transformed.  Second of all, linear transformations multiply $\vec{x}$ by the transformation matrix, not any other vector, right? I'm not particularly interested in the actual transformation matrix for this problem - I plan to get some practice with what i've learned by calculating that :) - I'd just like confirmation as to whether or not the things my friend has done are valid.  Thank you!",,"['linear-algebra', 'matrices', 'functions', 'transformation']"
8,Proof that Gauss-Jordan elimination works,Proof that Gauss-Jordan elimination works,,"Gauss-Jordan elimination is a technique that can be used to calculate the inverse of matrices (if they are invertible).  It can also be used to solve simultaneous linear equations. However, after a few google searches, I have failed to find a proof that this algorithm works for all $n \times n$, invertible matrices.  How would you prove that the technique of using Gauss-Jordan elimination to calculate matrices will work for all invertible matrices of finite dimensions (we allow swapping of two rows)? Induction on $n$ is a possible idea: the base case is very clear, but how would you prove the inductive step? We are not trying to show that an answer generated using Gauss-Jordan will be correct.  We are trying to show that Gauss-Jordan can apply to all invertible matrices. Note: I realize that there is a similar question here, but this question is distinct in that it asks for a proof for invertible matrices.","Gauss-Jordan elimination is a technique that can be used to calculate the inverse of matrices (if they are invertible).  It can also be used to solve simultaneous linear equations. However, after a few google searches, I have failed to find a proof that this algorithm works for all $n \times n$, invertible matrices.  How would you prove that the technique of using Gauss-Jordan elimination to calculate matrices will work for all invertible matrices of finite dimensions (we allow swapping of two rows)? Induction on $n$ is a possible idea: the base case is very clear, but how would you prove the inductive step? We are not trying to show that an answer generated using Gauss-Jordan will be correct.  We are trying to show that Gauss-Jordan can apply to all invertible matrices. Note: I realize that there is a similar question here, but this question is distinct in that it asks for a proof for invertible matrices.",,"['linear-algebra', 'matrices', 'inverse', 'gaussian-elimination']"
9,About the uniqueness of rank-1 decomposition of a positive-definite Hermitian matrix,About the uniqueness of rank-1 decomposition of a positive-definite Hermitian matrix,,"Suppose T is positive-definite Hermitian matrix and I know that it can be expressed by eigen-decomposition as the following sum of rank-1 matrices:$ \textbf{T}= \sum    \lambda _{k}   \textbf{u}_{k}  \textbf{u}_{k}^{H} $where $\textbf{u}_{k} $ are orthogonal to each other. But my question is: is this rank-1 decomposition unique? For example, can T be also written in other forms, say:$\textbf{T}= \sum    \gamma _{k}   \textbf{v}_{k}  \textbf{v}_{k}^{H} $, only in this case $\textbf{v}_{k} $ do not necessarily need to be orthogonal vectors. If so, is there any relationship between $\textbf{u}_{k} $ and $\textbf{v}_{k} $? Thanks.","Suppose T is positive-definite Hermitian matrix and I know that it can be expressed by eigen-decomposition as the following sum of rank-1 matrices:$ \textbf{T}= \sum    \lambda _{k}   \textbf{u}_{k}  \textbf{u}_{k}^{H} $where $\textbf{u}_{k} $ are orthogonal to each other. But my question is: is this rank-1 decomposition unique? For example, can T be also written in other forms, say:$\textbf{T}= \sum    \gamma _{k}   \textbf{v}_{k}  \textbf{v}_{k}^{H} $, only in this case $\textbf{v}_{k} $ do not necessarily need to be orthogonal vectors. If so, is there any relationship between $\textbf{u}_{k} $ and $\textbf{v}_{k} $? Thanks.",,['linear-algebra']
10,How to get Point between two points at any specific distance?,How to get Point between two points at any specific distance?,,"I have two points,  approximately we take values for that: Point $A = (50, 150)$; Point $B = (150, 50)$; So the distance should be calculated here, $\text{distance} = \sqrt{(B_x - A_x)(B_x - A_x) + (B_y - A_y)(B_y - A_y)}$; Now I want any one poins which is far from Second point B at specific distance (Example, 10). B(x,y)            /           /          C(x,y)         /        /       /      /     /      /   A(x,y) Point c on Line Segment and its specific distance from point B(Ex, 10).. Which formula would be better to calculate C point here ? Please help me about that.","I have two points,  approximately we take values for that: Point $A = (50, 150)$; Point $B = (150, 50)$; So the distance should be calculated here, $\text{distance} = \sqrt{(B_x - A_x)(B_x - A_x) + (B_y - A_y)(B_y - A_y)}$; Now I want any one poins which is far from Second point B at specific distance (Example, 10). B(x,y)            /           /          C(x,y)         /        /       /      /     /      /   A(x,y) Point c on Line Segment and its specific distance from point B(Ex, 10).. Which formula would be better to calculate C point here ? Please help me about that.",,"['linear-algebra', 'geometry']"
11,What is good about simple Lie algebras?,What is good about simple Lie algebras?,,"Recently I've been reading Naive Lie Theory by John Stillwell. In the book our aim usually concerns finding whether Lie algebras or Lie groups are simple. I wonder what beautiful properties does a simple Lie algebra have? Well, I've only learned linear algebra, mathematical analysis and a bit of abstract algebra, so I might expect a more amateur answer~ I think there may be other readers of the book sharing the same question. And a good answer may help us gain more motivation to learn about Lie theory~ Much thanks!!!","Recently I've been reading Naive Lie Theory by John Stillwell. In the book our aim usually concerns finding whether Lie algebras or Lie groups are simple. I wonder what beautiful properties does a simple Lie algebra have? Well, I've only learned linear algebra, mathematical analysis and a bit of abstract algebra, so I might expect a more amateur answer~ I think there may be other readers of the book sharing the same question. And a good answer may help us gain more motivation to learn about Lie theory~ Much thanks!!!",,"['linear-algebra', 'soft-question', 'lie-algebras']"
12,How to find a vector space V and decompositions $V=A\oplus B = C\oplus D$ with $A$ isomorphic to $C$ but $B$ is not isomorphic to $D?$,How to find a vector space V and decompositions  with  isomorphic to  but  is not isomorphic to,V=A\oplus B = C\oplus D A C B D?,"I've tried to solve the following question (Exercise 10, page 107 from Roman's book: Advanced Linear Algebra), but I wasn't able to solve it. Find a vector space V and decompositions $V=A\oplus B = C\oplus D$ with $A$ isomorphic to $C$ but $B$ is not isomorphic to $D$.","I've tried to solve the following question (Exercise 10, page 107 from Roman's book: Advanced Linear Algebra), but I wasn't able to solve it. Find a vector space V and decompositions $V=A\oplus B = C\oplus D$ with $A$ isomorphic to $C$ but $B$ is not isomorphic to $D$.",,[]
13,"R is a PID , M a free R module, R-basis","R is a PID , M a free R module, R-basis",,"We are a group of Finnish second semester undergrad students who are trying to solve old exercises from this linear algebra class for the benefit of our whole semester and even all prospective students (aka TeXing the solutions for future generations and putting them on our Dropbox). So far so good, this one exercise really leaves us clueless (probably due to non of us quite understanding how to handle modules) and we hope somebody who knows ""non linear"" Algebra can teach us by solving this with explainations so we don't lose more time just on this one exercise ( we spent heartfelt 2-3 hours already trying to figure something, but we haven't got anything substantial which isn't just a mere guess) : This is the ""evil"" exercise: 7 a) Let $R$  be a PID, $M$ a free $R$ module and let $S$ be a $R$-submodule with $\{0\} \ne S \subset M$. Let $(x_{1},\dots,x_{n})$ be a $R$-basis of $M$ and let $t_{1},\ldots,t_{m}\in R$, with $t_{1}|\cdots|t_{m} \ne 0$ so that $(t_{1}x_{1},\cdots,t_{m}x_{m})$ is a $R$-basis of $S$. If we now let  $$F=\bigl\{f_{1}(y_{1})+\cdots+f_{k}(y_{k});k\ge 1, f_{1},...,f_{k}\in \mathrm{Hom}(M,R), \ y_{1},\ldots,y_{k}\in S\bigr\}.$$ Then we can show that $F=t_{1}R$ b) Let more concretely be $R=\mathbf{Z}$, $M= \mathbf{Z}^{3}$, $S= \mathbf{Z}a_{1}+\mathbf{Z}a_{2}+\mathbf{Z}a_{3}$.  Putting $a_{1}=(1,2,3)$, $a_{2}=(4,5,6)$, $a_{3}=(7,8,9)$. Compute $t_{1}>0, \ldots , t_{m} > 0$ in (a) c) Show that $M/S$ is isomorphic to $(\mathbf{Z}/3\mathbf{Z}) \times \mathbf{Z}$ When we asked our prof where he takes the exercises from, he told us he takes them from a book called ""Algebra"" by Artin. We couldn't find it in the book itself, so excuse us for not giving a direct reference. Our professor puts $30$% of pure Algebra stuff in the exercises and it is obvious that they are his favourite exercises (so if you want a good grade you need to know this material). We sincerely hope that a seasoned algebraist takes pity/empathy on us and excuse ourselves for the English mistakes and our mathematical incapability.","We are a group of Finnish second semester undergrad students who are trying to solve old exercises from this linear algebra class for the benefit of our whole semester and even all prospective students (aka TeXing the solutions for future generations and putting them on our Dropbox). So far so good, this one exercise really leaves us clueless (probably due to non of us quite understanding how to handle modules) and we hope somebody who knows ""non linear"" Algebra can teach us by solving this with explainations so we don't lose more time just on this one exercise ( we spent heartfelt 2-3 hours already trying to figure something, but we haven't got anything substantial which isn't just a mere guess) : This is the ""evil"" exercise: 7 a) Let $R$  be a PID, $M$ a free $R$ module and let $S$ be a $R$-submodule with $\{0\} \ne S \subset M$. Let $(x_{1},\dots,x_{n})$ be a $R$-basis of $M$ and let $t_{1},\ldots,t_{m}\in R$, with $t_{1}|\cdots|t_{m} \ne 0$ so that $(t_{1}x_{1},\cdots,t_{m}x_{m})$ is a $R$-basis of $S$. If we now let  $$F=\bigl\{f_{1}(y_{1})+\cdots+f_{k}(y_{k});k\ge 1, f_{1},...,f_{k}\in \mathrm{Hom}(M,R), \ y_{1},\ldots,y_{k}\in S\bigr\}.$$ Then we can show that $F=t_{1}R$ b) Let more concretely be $R=\mathbf{Z}$, $M= \mathbf{Z}^{3}$, $S= \mathbf{Z}a_{1}+\mathbf{Z}a_{2}+\mathbf{Z}a_{3}$.  Putting $a_{1}=(1,2,3)$, $a_{2}=(4,5,6)$, $a_{3}=(7,8,9)$. Compute $t_{1}>0, \ldots , t_{m} > 0$ in (a) c) Show that $M/S$ is isomorphic to $(\mathbf{Z}/3\mathbf{Z}) \times \mathbf{Z}$ When we asked our prof where he takes the exercises from, he told us he takes them from a book called ""Algebra"" by Artin. We couldn't find it in the book itself, so excuse us for not giving a direct reference. Our professor puts $30$% of pure Algebra stuff in the exercises and it is obvious that they are his favourite exercises (so if you want a good grade you need to know this material). We sincerely hope that a seasoned algebraist takes pity/empathy on us and excuse ourselves for the English mistakes and our mathematical incapability.",,"['linear-algebra', 'ring-theory']"
14,Methodology to solve a Math IQ puzzle?,Methodology to solve a Math IQ puzzle?,,Note :  Not looking for the solution - just help on how to solve. Here is the math puzzle: Is there a mathematical model/method that I could employ to solve this? Right now my only answer is to use Excel and trial-and-error my way to the solution.,Note :  Not looking for the solution - just help on how to solve. Here is the math puzzle: Is there a mathematical model/method that I could employ to solve this? Right now my only answer is to use Excel and trial-and-error my way to the solution.,,"['linear-algebra', 'puzzle']"
15,How do you use the Gram-Schmidt process to generate an orthonormal basis of $\mathbb{R}^3$?,How do you use the Gram-Schmidt process to generate an orthonormal basis of ?,\mathbb{R}^3,"These vectors form a basis on $\mathbb R^3$: $$\begin{bmatrix}1\\0\\-1\\\end{bmatrix},\begin{bmatrix}2\\-1\\0\\\end{bmatrix} ,\begin{bmatrix}1\\2\\1\\\end{bmatrix}$$ Can someone show how to use the Gram-Schmidt process to generate an orthonormal basis of $\mathbb R^3$?","These vectors form a basis on $\mathbb R^3$: $$\begin{bmatrix}1\\0\\-1\\\end{bmatrix},\begin{bmatrix}2\\-1\\0\\\end{bmatrix} ,\begin{bmatrix}1\\2\\1\\\end{bmatrix}$$ Can someone show how to use the Gram-Schmidt process to generate an orthonormal basis of $\mathbb R^3$?",,['linear-algebra']
16,Geometric Interpretation of Solutions to Linear Systems,Geometric Interpretation of Solutions to Linear Systems,,"I've been reading Linear Algebra by Jacob for self study and I'm wondering about the geometric interpretation of a unique solution to systems of equations in $\mathbb R^3$. For example, the homogeneous system $$  \begin{eqnarray*} x+5y-2z &=&0, \\ x-3y+z &=& 0, \\  x+5y-z &=& 0  \end{eqnarray*} $$ has the unique solution $x=y=z=0$. From Calc III, I would think of this as being 3 planes and the various $(x,y,z)$ values of the line (vector) that forms the intersection as the solution to the system. But the solution here and in many other systems in $\mathbb R^3$ is a point and that doesn't seem to be possible. Am I wrong in trying to understand this geometrically or is there something I'm missing? Sorry if this is hard to read, I haven't learning latex yet and thanks to everyone who responded to my question about self study books for real analysis.","I've been reading Linear Algebra by Jacob for self study and I'm wondering about the geometric interpretation of a unique solution to systems of equations in $\mathbb R^3$. For example, the homogeneous system $$  \begin{eqnarray*} x+5y-2z &=&0, \\ x-3y+z &=& 0, \\  x+5y-z &=& 0  \end{eqnarray*} $$ has the unique solution $x=y=z=0$. From Calc III, I would think of this as being 3 planes and the various $(x,y,z)$ values of the line (vector) that forms the intersection as the solution to the system. But the solution here and in many other systems in $\mathbb R^3$ is a point and that doesn't seem to be possible. Am I wrong in trying to understand this geometrically or is there something I'm missing? Sorry if this is hard to read, I haven't learning latex yet and thanks to everyone who responded to my question about self study books for real analysis.",,['linear-algebra']
17,How to prove two equations in linear algebra,How to prove two equations in linear algebra,,"Given the following definition: How to proof these two equations? and PS: Actually, there are two proofs preceding the two(I have no problem with the following two), they are: Maybe they are hints on solving the latter two. I encounter this problem here(section 2.1 about page 8~page 9)","Given the following definition: How to proof these two equations? and PS: Actually, there are two proofs preceding the two(I have no problem with the following two), they are: Maybe they are hints on solving the latter two. I encounter this problem here(section 2.1 about page 8~page 9)",,['linear-algebra']
18,Partial differential equation & Lax-Milgram Theory?,Partial differential equation & Lax-Milgram Theory?,,"Consider the elliptic problem $Lu = \exp(x)$ on $[0,1]$ with $Lu = -\frac{d^2u}{dx^2} + \frac{du}{dx}$ and boundary conditions $u(0) = 5$, $\frac{du}{dx}(1) + u(1) = 2$. Answer the following questions. Is $L$ a self-adjoint operator? Show that $(u, u_x) = \frac{1}{2}(u(1)^2 - u(0)^2)$ Put the system in the form $a(w,u)=F(w)$ and give both $a(w, u)$ and $F(w)$ such that we successfully can show coercivity of the bilinear form. Show that the bilinear form is coercive. Let a linear function space in $C^2[0, 1]$ be spanned by $\{Q_1(x), Q_2(x), ..., Q_N(x)\}$ with $Q_i(0) = 0; i = 1, ...,N$. Give the linear system that arises from the Galerkin projection of the above problem on this space.","Consider the elliptic problem $Lu = \exp(x)$ on $[0,1]$ with $Lu = -\frac{d^2u}{dx^2} + \frac{du}{dx}$ and boundary conditions $u(0) = 5$, $\frac{du}{dx}(1) + u(1) = 2$. Answer the following questions. Is $L$ a self-adjoint operator? Show that $(u, u_x) = \frac{1}{2}(u(1)^2 - u(0)^2)$ Put the system in the form $a(w,u)=F(w)$ and give both $a(w, u)$ and $F(w)$ such that we successfully can show coercivity of the bilinear form. Show that the bilinear form is coercive. Let a linear function space in $C^2[0, 1]$ be spanned by $\{Q_1(x), Q_2(x), ..., Q_N(x)\}$ with $Q_i(0) = 0; i = 1, ...,N$. Give the linear system that arises from the Galerkin projection of the above problem on this space.",,"['linear-algebra', 'partial-differential-equations']"
19,Can the following probabilistic argument about eigenvalues be made rigorous?,Can the following probabilistic argument about eigenvalues be made rigorous?,,"Consider the following $n \times n$ matrix $$ \left( \begin{matrix} 1/2 & 1/2 & 0 & 0 & 0  & 0 \\ 1/2 & 0 & 1/2 & 0 & 0 & 0 \\ 0 & 1/2 & 0 & 1/2 & 0 & 0 \\ \vdots & \ & \ddots &  & \ddots & \\ 0 & \cdots & 0 & 1/2 & 0 & 1/2 \\ 0 & \cdots & 0 & 0 & 1/2 & 1/2 \end{matrix} \right) $$ It has its largest eigenvalue at $1$, and the gap between $1$ and the second largest eigenvalue is on the order of $1/n^2$. The other day, someone offered me the following heuristic argument for why, for large $n$ , this gap cannot be larger than $c/n^2$ for some constant $c>0$: This is the transition matrix of a Markov chain on $n$ nodes which, except at the  endpoints, moves left and right with equal probability. Its stationary distribution is uniform. Now the second eigenvalue measures the time until you get close to the stationary distribution, and observe that by the central limit theorem, as $n$ gets large, a particle which begins at the midpoint will take on the order of $n^2$ steps until it puts a constant probability on order of $n$ nodes. Consequently, the gap cannot be larger than on the order of $1/n^2$. Intuitively, this makes sense. I would like to ask: Question: Is there a way to make the above argument rigorous? One difficulty is that the smallest eigenvalue also affects mixing time, but we can perhaps get rid of this difficulty by taking a convex combination with the identity. A more serious problem seems to be the boundary effects, but perhaps one can argue that they ""go away"" as $n \rightarrow \infty$. I'm not sure how to argue any of this, and I'd be very interested in seeing a formalization of this argument.","Consider the following $n \times n$ matrix $$ \left( \begin{matrix} 1/2 & 1/2 & 0 & 0 & 0  & 0 \\ 1/2 & 0 & 1/2 & 0 & 0 & 0 \\ 0 & 1/2 & 0 & 1/2 & 0 & 0 \\ \vdots & \ & \ddots &  & \ddots & \\ 0 & \cdots & 0 & 1/2 & 0 & 1/2 \\ 0 & \cdots & 0 & 0 & 1/2 & 1/2 \end{matrix} \right) $$ It has its largest eigenvalue at $1$, and the gap between $1$ and the second largest eigenvalue is on the order of $1/n^2$. The other day, someone offered me the following heuristic argument for why, for large $n$ , this gap cannot be larger than $c/n^2$ for some constant $c>0$: This is the transition matrix of a Markov chain on $n$ nodes which, except at the  endpoints, moves left and right with equal probability. Its stationary distribution is uniform. Now the second eigenvalue measures the time until you get close to the stationary distribution, and observe that by the central limit theorem, as $n$ gets large, a particle which begins at the midpoint will take on the order of $n^2$ steps until it puts a constant probability on order of $n$ nodes. Consequently, the gap cannot be larger than on the order of $1/n^2$. Intuitively, this makes sense. I would like to ask: Question: Is there a way to make the above argument rigorous? One difficulty is that the smallest eigenvalue also affects mixing time, but we can perhaps get rid of this difficulty by taking a convex combination with the identity. A more serious problem seems to be the boundary effects, but perhaps one can argue that they ""go away"" as $n \rightarrow \infty$. I'm not sure how to argue any of this, and I'd be very interested in seeing a formalization of this argument.",,"['linear-algebra', 'probability-theory', 'stochastic-processes']"
20,Polynomials and Linear Operators,Polynomials and Linear Operators,,"Let $p$, $q$, and $r$ be polynomials such that $p(x) = q(x)r(x)$, and let $T$ be a linear operator on a vector space $V$. Is there a simple way to show that $p(T) = q(T)r(T)$ ?","Let $p$, $q$, and $r$ be polynomials such that $p(x) = q(x)r(x)$, and let $T$ be a linear operator on a vector space $V$. Is there a simple way to show that $p(T) = q(T)r(T)$ ?",,['linear-algebra']
21,Bézout-like identities for linear operators,Bézout-like identities for linear operators,,"As usual, when I pose a question here the answer I receive generates more questions. Today I posed myself a problem originating from this answer by Joel Cohen. Let $V$ be a finite dimensional vector space over an arbitrary field. Let us agree to say that the linear operators $A, B$ verify a Bézout-like identity if there exist linear operators $X, Y$ such that    $$I=XA+YB,$$   where $I$ denotes the identity mapping. Problem : Find necessary and sufficient conditions for $A$ and $B$ to verify a Bézout-like identity. I believe the answer lies somewhere around $\ker(A), \ker(B)$. For example, if $A$ and $B$ are associated to the block $n \times n$ matrices $$A \equiv \begin{bmatrix} \mathbf{0} & \mathbf{0} \\ \mathbf{0} & P_{k \times k} \end{bmatrix}, \quad B \equiv \begin{bmatrix} Q_{h \times h} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix}$$ with nonsingular $P, Q$, then we can take $$X= \begin{bmatrix}  \mathbf{0} & \mathbf{0} \\ \mathbf{0} & P^{-1} \end{bmatrix}, \quad Y=\begin{bmatrix} Q^{-1} & \mathbf{0} \\ \mathbf{0} &  \mathbf{0} \end{bmatrix}$$ which yield a Bézout-like identity if and only if $k +h=n$. Moreover, if $k+h < n$, then we can be sure that no Bézout-like identity is possible. This could suggest that the sought condition is $$\ker(A) \oplus \ker(B)=V.$$","As usual, when I pose a question here the answer I receive generates more questions. Today I posed myself a problem originating from this answer by Joel Cohen. Let $V$ be a finite dimensional vector space over an arbitrary field. Let us agree to say that the linear operators $A, B$ verify a Bézout-like identity if there exist linear operators $X, Y$ such that    $$I=XA+YB,$$   where $I$ denotes the identity mapping. Problem : Find necessary and sufficient conditions for $A$ and $B$ to verify a Bézout-like identity. I believe the answer lies somewhere around $\ker(A), \ker(B)$. For example, if $A$ and $B$ are associated to the block $n \times n$ matrices $$A \equiv \begin{bmatrix} \mathbf{0} & \mathbf{0} \\ \mathbf{0} & P_{k \times k} \end{bmatrix}, \quad B \equiv \begin{bmatrix} Q_{h \times h} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix}$$ with nonsingular $P, Q$, then we can take $$X= \begin{bmatrix}  \mathbf{0} & \mathbf{0} \\ \mathbf{0} & P^{-1} \end{bmatrix}, \quad Y=\begin{bmatrix} Q^{-1} & \mathbf{0} \\ \mathbf{0} &  \mathbf{0} \end{bmatrix}$$ which yield a Bézout-like identity if and only if $k +h=n$. Moreover, if $k+h < n$, then we can be sure that no Bézout-like identity is possible. This could suggest that the sought condition is $$\ker(A) \oplus \ker(B)=V.$$",,"['linear-algebra', 'matrices', 'block-matrices']"
22,Permuting the rows of an invertible matrix to make 2 specific submatrices invertible,Permuting the rows of an invertible matrix to make 2 specific submatrices invertible,,"This question is similar to a previous one: Gauss Elimination with constraints Given an $n \times n$ matrix $M$ and a number $1 \leq m \leq n-1$ , we partition is as a block matrix: $$M = \begin{bmatrix} A & B \\ C & D \end{bmatrix}$$ where $A$ is an $m \times m$ matrix and $D$ is an $(n-m) \times (n-m)$ matrix. We then say that $M$ is $m$ -good if both $A$ and $D$ are invertible. Given any invertible matrix $M \in GL_n(\mathbb{F})$ and a number $1 \leq m \leq n-1$ , is it always possible to permute the rows of $M$ to make it $m$ -good ? Note: I only care about the case $\mathbb{F}=\mathbb{Z}_p$ , but I asked the question more generally because my feeling is that it doesn't matter what the field is.","This question is similar to a previous one: Gauss Elimination with constraints Given an matrix and a number , we partition is as a block matrix: where is an matrix and is an matrix. We then say that is -good if both and are invertible. Given any invertible matrix and a number , is it always possible to permute the rows of to make it -good ? Note: I only care about the case , but I asked the question more generally because my feeling is that it doesn't matter what the field is.",n \times n M 1 \leq m \leq n-1 M = \begin{bmatrix} A & B \\ C & D \end{bmatrix} A m \times m D (n-m) \times (n-m) M m A D M \in GL_n(\mathbb{F}) 1 \leq m \leq n-1 M m \mathbb{F}=\mathbb{Z}_p,"['linear-algebra', 'matrices', 'permutations', 'block-matrices']"
23,Orthogonal Matrices and Symplectic Matrices and Preservation of Forms,Orthogonal Matrices and Symplectic Matrices and Preservation of Forms,,"I would like to know the properties of orthogonal matrices and symplectic     matrices in terms of the forms they preserve. Could someone please add and/or     correct, maybe give some refs/examples? AFAIK, given a quadratic form q     on a vector space V over a field F, there is an associated orthogonal     group O(2n) ,a subgroup of GL(n,F),  which      preserve q; if F is the reals O(2n) preserves q= inner-product and norm (since     in R, the norm is induced by the inner-product).  Symplectic matrices only preserve symplectic forms, i.e., bilinear,antisymmetric,non-degenerate forms. Are there relations between these groups; do they overlap, intersect, etc? I am interested mostly in the case where the field is Z/2. Thanks","I would like to know the properties of orthogonal matrices and symplectic     matrices in terms of the forms they preserve. Could someone please add and/or     correct, maybe give some refs/examples? AFAIK, given a quadratic form q     on a vector space V over a field F, there is an associated orthogonal     group O(2n) ,a subgroup of GL(n,F),  which      preserve q; if F is the reals O(2n) preserves q= inner-product and norm (since     in R, the norm is induced by the inner-product).  Symplectic matrices only preserve symplectic forms, i.e., bilinear,antisymmetric,non-degenerate forms. Are there relations between these groups; do they overlap, intersect, etc? I am interested mostly in the case where the field is Z/2. Thanks",,"['linear-algebra', 'geometry']"
24,Diagonal of an inverse of a sparse matrix,Diagonal of an inverse of a sparse matrix,,"Inverse of a sparse matrix could be dense, but what if I'm only interested in the main diagonal of the result? Is there a method that is more efficient than computing the full inverse?","Inverse of a sparse matrix could be dense, but what if I'm only interested in the main diagonal of the result? Is there a method that is more efficient than computing the full inverse?",,"['linear-algebra', 'numerical-methods']"
25,"$ABACA = 0 \Longrightarrow BAC = 0$ if $A,B,C \ge 0$ are symmetric.",if  are symmetric.,"ABACA = 0 \Longrightarrow BAC = 0 A,B,C \ge 0","Problem. $A, B, C$ are $n \times n$ symmetric positively semi-definite matrices. Prove that $ABACA = 0 \Longrightarrow BAC = 0$ if $A,B,C \ge 0$ are symmetric. My attemp (there's mistake in it). We have $A B A C A = 0$ . Put $x = A B A$ . Then $$ x^{\top} C x = (A B A)^{\top} C A B A = (A B A C A) B A = 0.$$ If follows from $x^{\top} C x = 0$ and $C \ge 0$ that $C x =0$ . Indeed, there exists symmetric positively semi-definite matrix $\sqrt{C}$ . We have $0 = x^{\top} C x = (\sqrt{C}x)^{\top}(\sqrt{C}x)$ . It looks like (but may be no, because $x$ is not vector) $\sqrt{C}x = 0$ and hence $C x = \sqrt{C}(\sqrt{C}x) = 0$ . Thus $Cx = C A B A = 0$ . Put $y = A C$ . Then $y^{\top} B y = (A C)^{\top} B A C = (C A B A) C = 0$ . It follows (if I was right above) from $y^{\top} B y = 0$ and $B \ge 0$ that $B y =0$ , i.e. $B A C = 0$ , q.e.d. Additional information. I'm not sure that my attempt is close to truth, I want to find concise solution and anyway my attempt is far from being concise.","Problem. are symmetric positively semi-definite matrices. Prove that if are symmetric. My attemp (there's mistake in it). We have . Put . Then If follows from and that . Indeed, there exists symmetric positively semi-definite matrix . We have . It looks like (but may be no, because is not vector) and hence . Thus . Put . Then . It follows (if I was right above) from and that , i.e. , q.e.d. Additional information. I'm not sure that my attempt is close to truth, I want to find concise solution and anyway my attempt is far from being concise.","A, B, C n \times n ABACA = 0 \Longrightarrow BAC = 0 A,B,C \ge 0 A B A C A = 0 x = A B A  x^{\top} C x = (A B A)^{\top} C A B A = (A B A C A) B A = 0. x^{\top} C x = 0 C \ge 0 C x =0 \sqrt{C} 0 = x^{\top} C x = (\sqrt{C}x)^{\top}(\sqrt{C}x) x \sqrt{C}x = 0 C x = \sqrt{C}(\sqrt{C}x) = 0 Cx = C A B A = 0 y = A C y^{\top} B y = (A C)^{\top} B A C = (C A B A) C = 0 y^{\top} B y = 0 B \ge 0 B y =0 B A C = 0","['linear-algebra', 'matrices', 'vector-spaces', 'matrix-equations', 'matrix-calculus']"
26,Existence of canonical form for cubic and quartic form?,Existence of canonical form for cubic and quartic form?,,"I am a post graduate student who is currently studying some optimization for quadratic form. From the class lecture, I know that we can always turn any quadratic functions into theirs corresponding canonical form using some changes of variable. For example: $f\left( {x,y} \right) = x \times y$ can be rewritten as $f\left( {X,Y} \right) = \frac{1}{4}{X^2} - \frac{1}{4}{Y^2}$ through the following change of variable $\left\{ {\begin{array}{*{20}{c}} {X = x + y}\\ {Y = x - y} \end{array}} \right.$ . Out of pure curiosity, my question is: Does "" canonical cubic form "" and "" canonical quartic form "" for function of the form $f\left( {x,y,z} \right) = xyz$ and $g\left( {x,y,z,t} \right) = xyzt$ respectively even existed ? If these canonical form existed then what would be a systematic way to find them ? Thank you for your enthusiasm !","I am a post graduate student who is currently studying some optimization for quadratic form. From the class lecture, I know that we can always turn any quadratic functions into theirs corresponding canonical form using some changes of variable. For example: can be rewritten as through the following change of variable . Out of pure curiosity, my question is: Does "" canonical cubic form "" and "" canonical quartic form "" for function of the form and respectively even existed ? If these canonical form existed then what would be a systematic way to find them ? Thank you for your enthusiasm !","f\left( {x,y} \right) = x \times y f\left( {X,Y} \right) = \frac{1}{4}{X^2} - \frac{1}{4}{Y^2} \left\{ {\begin{array}{*{20}{c}}
{X = x + y}\\
{Y = x - y}
\end{array}} \right. f\left( {x,y,z} \right) = xyz g\left( {x,y,z,t} \right) = xyzt","['linear-algebra', 'algebraic-geometry', 'transformation', 'quadratic-forms', 'canonical-transformation']"
27,"In what sense are similar matrices ""the same,"" and how can this be generalized?","In what sense are similar matrices ""the same,"" and how can this be generalized?",,"I sort of intuitively see why we care about similar matrices, i.e., when $A=S^{-1}BS$ for some invertible matrix $S$ . But I want to make this intuition more precise and abstract. Matrices: First of all, as mentioned here , Because matrices are similar if and only if they represent the same linear operator with respect to (possibly) different bases, similar matrices share all properties of their shared underlying operator. This is followed by a long list of shared properties. However, I feel this doesn't give the full story. For example, we also care about when two different operators are similar, in which case (conversely), they can be represented by the same matrix with appropriate bases. In this case, the long list of properties is still shared by both operators. How can one precisely say what type of properties are shared by similar operators, and what's the most abstract way to understand this? Generalizations: If $A, B, S$ are elements of a group $G$ , then $A = S^{-1}BS$ is described by saying $A$ is the conjugation of $B$ by $S$ . For any $S \in G$ , conjugation by $S$ is an endomorphism of $G$ , and hence preserves all group-theoretic properties of any element. Matrix similarity is an extension of this idea, where we conjugate elements of an algebra $L(V)$ (operators) with the group of units in the algebra $GL(V)$ (invertible matrices). This type of conjugation then provides an algebra endomorphism, so we should expect the properties of some $T \in L(V)$ as an element of the algebra $L(V)$ to be preserved by conjugation—but some of the properties in the list linked above (e.g., determinant) are specific to $L(V)$ and cannot be generalized to an arbitrary algebra. The most general framework I can think of is as follows: We have some group $G$ which acts on some structure $X$ from the left and right. Thus conjugation makes sense. Can we say anything about what properties of an arbitrary $x \in X$ are preserved under conjugation, in a way that includes matrix similarity as a special case?","I sort of intuitively see why we care about similar matrices, i.e., when for some invertible matrix . But I want to make this intuition more precise and abstract. Matrices: First of all, as mentioned here , Because matrices are similar if and only if they represent the same linear operator with respect to (possibly) different bases, similar matrices share all properties of their shared underlying operator. This is followed by a long list of shared properties. However, I feel this doesn't give the full story. For example, we also care about when two different operators are similar, in which case (conversely), they can be represented by the same matrix with appropriate bases. In this case, the long list of properties is still shared by both operators. How can one precisely say what type of properties are shared by similar operators, and what's the most abstract way to understand this? Generalizations: If are elements of a group , then is described by saying is the conjugation of by . For any , conjugation by is an endomorphism of , and hence preserves all group-theoretic properties of any element. Matrix similarity is an extension of this idea, where we conjugate elements of an algebra (operators) with the group of units in the algebra (invertible matrices). This type of conjugation then provides an algebra endomorphism, so we should expect the properties of some as an element of the algebra to be preserved by conjugation—but some of the properties in the list linked above (e.g., determinant) are specific to and cannot be generalized to an arbitrary algebra. The most general framework I can think of is as follows: We have some group which acts on some structure from the left and right. Thus conjugation makes sense. Can we say anything about what properties of an arbitrary are preserved under conjugation, in a way that includes matrix similarity as a special case?","A=S^{-1}BS S A, B, S G A = S^{-1}BS A B S S \in G S G L(V) GL(V) T \in L(V) L(V) L(V) G X x \in X","['linear-algebra', 'abstract-algebra', 'group-actions', 'matrix-decomposition', 'similar-matrices']"
28,"does there exist $T\in GL(n,\mathbb{C})$ so that $\sigma(g) = T^{-1} \rho(g)T$ for all $g\in G$?",does there exist  so that  for all ?,"T\in GL(n,\mathbb{C}) \sigma(g) = T^{-1} \rho(g)T g\in G","Suppose that $\rho$ and $\sigma$ are degree $n$ irreducible representations of a group $G$ over $\mathbb{C}$ and that for every $g\in G,$ there is a matrix $T_g\in GL(n,\mathbb{C})$ depending on $g$ so that $\sigma(g) = T_g^{-1} \rho(g) T_g$ . Does there exist $T\in GL(n,\mathbb{C})$ so that $\sigma(g) = T^{-1} \rho(g)T$ for all $g\in G$ ? I think the answer is no, but I'm not sure how to come up with a counterexample. The question basically asks if $\sigma$ and $\rho$ are necessarily equivalent representations. As a first step, it seems reasonable to find a specific pair of degree n irreducible representations satisfying the constraints in the question, but I'm not sure how to do so. One irreducible representation that could be worth considering is the sign representation of $S_n$ .","Suppose that and are degree irreducible representations of a group over and that for every there is a matrix depending on so that . Does there exist so that for all ? I think the answer is no, but I'm not sure how to come up with a counterexample. The question basically asks if and are necessarily equivalent representations. As a first step, it seems reasonable to find a specific pair of degree n irreducible representations satisfying the constraints in the question, but I'm not sure how to do so. One irreducible representation that could be worth considering is the sign representation of .","\rho \sigma n G \mathbb{C} g\in G, T_g\in GL(n,\mathbb{C}) g \sigma(g) = T_g^{-1} \rho(g) T_g T\in GL(n,\mathbb{C}) \sigma(g) = T^{-1} \rho(g)T g\in G \sigma \rho S_n","['linear-algebra', 'abstract-algebra', 'group-theory', 'representation-theory', 'irreducible-representation']"
29,Trace inequality: $\mathrm{Tr}(|\rho^{1-t}x\rho^t|)\leq \mathrm{Tr}(|\rho^{1-t}y\rho^t|)$,Trace inequality:,\mathrm{Tr}(|\rho^{1-t}x\rho^t|)\leq \mathrm{Tr}(|\rho^{1-t}y\rho^t|),"I've been researching some operator spaces and have stumbled upon the following problem. Suppose $x,y\in\mathcal{B}(\mathcal{H})$ (for $\mathcal{H}$ separable) such that $0\leq x\leq y\leq 1$ . Further let $t\in[0,1]$ and $\rho$ be a density matrix (i.e. $\rho\geq 0$ , $\mathrm{Tr}(\rho)=1$ ). Is it true that $$ \mathrm{Tr}\big(\vert \rho^{1-t}x\rho^t\vert\big)\leq \mathrm{Tr}\big(\vert \rho^{1-t}y\rho^t\vert\big) \leq 1? $$ Obviously this holds for $t=\frac{1}{2}$ , but I'm not really sure how to approach it for other values. I've tried playing around with the polar decomposition but haven't gotten anywhere. I'm convinced that the inequality is actually true at least in the case of $2\times 2$ matrices as I've run a randomized computer search and have not found any counterexamples, but even there trying to prove it directly by writing out all the matrices is very ugly. EDIT: I've verified the inequality numerically on matrices of sizes up to $5\times 5$ with $10\,000$ random samples per size (hopefully the code is correct, generating random unitaries is a bit tricky). It seems my code was indeed incorrect.","I've been researching some operator spaces and have stumbled upon the following problem. Suppose (for separable) such that . Further let and be a density matrix (i.e. , ). Is it true that Obviously this holds for , but I'm not really sure how to approach it for other values. I've tried playing around with the polar decomposition but haven't gotten anywhere. I'm convinced that the inequality is actually true at least in the case of matrices as I've run a randomized computer search and have not found any counterexamples, but even there trying to prove it directly by writing out all the matrices is very ugly. EDIT: I've verified the inequality numerically on matrices of sizes up to with random samples per size (hopefully the code is correct, generating random unitaries is a bit tricky). It seems my code was indeed incorrect.","x,y\in\mathcal{B}(\mathcal{H}) \mathcal{H} 0\leq x\leq y\leq 1 t\in[0,1] \rho \rho\geq 0 \mathrm{Tr}(\rho)=1 
\mathrm{Tr}\big(\vert \rho^{1-t}x\rho^t\vert\big)\leq \mathrm{Tr}\big(\vert \rho^{1-t}y\rho^t\vert\big) \leq 1?
 t=\frac{1}{2} 2\times 2 5\times 5 10\,000","['linear-algebra', 'matrices', 'inequality', 'operator-algebras', 'trace']"
30,Product of matrices associated to bilinear forms,Product of matrices associated to bilinear forms,,"Let $V$ be a real vector space of finite dimension where $f,g: V \times V \to \mathbb{R}$ are two symmetric positive semidefinite bilinear forms. For a given basis $B$ of $V$ , there are two symmetric positive semidefinite matrices $F$ and $G$ that represent $f$ and $g$ , respectively. The matrix product $H=FG$ represents a positive semidefinite bilinear form $h$ on $V$ . I think the definition of $h$ does not depend on the basis $B$ and therefore neither on the associated matrices. How do I define the operation $(f,g)\mapsto h$ without making reference to associated matrices?","Let be a real vector space of finite dimension where are two symmetric positive semidefinite bilinear forms. For a given basis of , there are two symmetric positive semidefinite matrices and that represent and , respectively. The matrix product represents a positive semidefinite bilinear form on . I think the definition of does not depend on the basis and therefore neither on the associated matrices. How do I define the operation without making reference to associated matrices?","V f,g: V \times V \to \mathbb{R} B V F G f g H=FG h V h B (f,g)\mapsto h","['linear-algebra', 'matrices', 'linear-transformations']"
31,When is an invertible symmetric logic matrix unimodular?,When is an invertible symmetric logic matrix unimodular?,,"I am currently interested in invertible symmetric logical matrices, or, $(0,1)$ -matrices, i.e., $n \times n$ matrices whose entries are either $0$ or $1$ (integers). I noticed that many invertible symmetric logical matrices that arise naturally from other fields of mathematics have determinant $\pm 1$ , i.e., they are unimodular matrices. Question: when is an invertible symmetric logic matrix unimodular? Equivalent Question: can we find a property P such that ""P + invertible + symmetric + Logical Matrix"" implies ""determinant = $\pm 1$ ? Surprisingly, I find this question quite hard. For example, one of the property $P$ is the Lemma 2.1 in this article , which is very restrictive. I am wondering whether there are less restrictive conditions. There are many researches concerning logical matrices , and many concerns unimodular matrices . However, there are very few results on there intersections.","I am currently interested in invertible symmetric logical matrices, or, -matrices, i.e., matrices whose entries are either or (integers). I noticed that many invertible symmetric logical matrices that arise naturally from other fields of mathematics have determinant , i.e., they are unimodular matrices. Question: when is an invertible symmetric logic matrix unimodular? Equivalent Question: can we find a property P such that ""P + invertible + symmetric + Logical Matrix"" implies ""determinant = ? Surprisingly, I find this question quite hard. For example, one of the property is the Lemma 2.1 in this article , which is very restrictive. I am wondering whether there are less restrictive conditions. There are many researches concerning logical matrices , and many concerns unimodular matrices . However, there are very few results on there intersections.","(0,1) n \times n 0 1 \pm 1 \pm 1 P","['linear-algebra', 'abstract-algebra', 'matrices', 'symmetric-matrices', 'unimodular-matrices']"
32,Intermediate level proof based math book.,Intermediate level proof based math book.,,"I am studying linear algebra. I find elementary level linear algebra; i.e, concepts like vector spaces,basis, span dimension, solving determinants, finding Eigenvalues ,using Cayley Hamilton theorem, etc very easy. I am reading Sheldon Axler's Linear Algebra done right. I have completed exercises upto Chapter 3 exercise D. But I find the exercises from the next section onwards, which are proof based and focus on abstract vector spaces very hard. I am unable to solve it without help. Part of the reason is I have never encountered this sort of mathematics before. I would appreciate if anyone can recommend books which introduce  proof based mathematics and abstract mathematical concepts, the books need not be based on linear algebra only, they can be based on other topics also.","I am studying linear algebra. I find elementary level linear algebra; i.e, concepts like vector spaces,basis, span dimension, solving determinants, finding Eigenvalues ,using Cayley Hamilton theorem, etc very easy. I am reading Sheldon Axler's Linear Algebra done right. I have completed exercises upto Chapter 3 exercise D. But I find the exercises from the next section onwards, which are proof based and focus on abstract vector spaces very hard. I am unable to solve it without help. Part of the reason is I have never encountered this sort of mathematics before. I would appreciate if anyone can recommend books which introduce  proof based mathematics and abstract mathematical concepts, the books need not be based on linear algebra only, they can be based on other topics also.",,['linear-algebra']
33,"Generalized notion of perpendicularity, (not orthogonal)","Generalized notion of perpendicularity, (not orthogonal)",,"In 3 dimensions, we might call 2 planes perpendicular iff their normals are orthogonal. But this does not coincide with the definition of orthogonal subspaces - the dot product of any pair of vectors is $0$ . Is there a notion of perpendicularity any text introduces, generalising in $\mathbb R^n$ the perpendicular planes in $\mathbb R^3$ ? For example - I was thinking if we had two subspaces be ""perpendicular"" iff they are not parallel and are the sides of some $n$ -cube.","In 3 dimensions, we might call 2 planes perpendicular iff their normals are orthogonal. But this does not coincide with the definition of orthogonal subspaces - the dot product of any pair of vectors is . Is there a notion of perpendicularity any text introduces, generalising in the perpendicular planes in ? For example - I was thinking if we had two subspaces be ""perpendicular"" iff they are not parallel and are the sides of some -cube.",0 \mathbb R^n \mathbb R^3 n,"['linear-algebra', 'orthogonality', 'affine-geometry']"
34,Find the polynomial whose zeros are reciprocal of each other,Find the polynomial whose zeros are reciprocal of each other,,"I have been trying this sum taking the zeros as $\alpha$ and $\alpha^{-1}$ , using the formula $x^2-(\alpha+\beta)x+\alpha \beta$ . However I am stuck in this step: $$ \alpha x^2-(\alpha^2+1)x+\alpha=0. $$ This could have been the answer, but $\alpha$ is not defined in the question and hence cannot be a part of the answer. I have searched the internet for this question but nowhere the answer seems to be available. The source of the question is unknown, I found this question in a really old school question paper and that school is not in existence anymore.","I have been trying this sum taking the zeros as and , using the formula . However I am stuck in this step: This could have been the answer, but is not defined in the question and hence cannot be a part of the answer. I have searched the internet for this question but nowhere the answer seems to be available. The source of the question is unknown, I found this question in a really old school question paper and that school is not in existence anymore.","\alpha \alpha^{-1} x^2-(\alpha+\beta)x+\alpha \beta 
\alpha x^2-(\alpha^2+1)x+\alpha=0.
 \alpha","['linear-algebra', 'polynomials', 'roots']"
35,Knowing inverse matrix based upon a picture?,Knowing inverse matrix based upon a picture?,,"I know how to calculate an inverse matrix (by creating the augmented matrix and Gaussian elimination to get the identity matrix) and I know how to do it for a general $2 \times 2$ matrix by taking two different cases when the determinant is zero or not. And, unless I'm mistaken, I also understand how a matrix is a linear transformation - basically a function - that can be understood as transforming the standard basis vectors. That is, I can see a simple $2 \times 2$ matrix and know what it does geometrically. Is there a way to tell the inverse of a generic $2 \times 2$ matrix my looking at the matrix itself? I looked at Finding inverse of a matrix geometrically but I didn't get it (There were no pictures to help me gain geometric insight). I think that I could do it for some, like the inverse of a matrix that does a stretch would be a matrix that does a compression, or a matrix that rotates counterclockwise would be a matrix that rotates clockwise -- but is there an ""instant"" way of knowing what the inverse matrix is based upon a picture?","I know how to calculate an inverse matrix (by creating the augmented matrix and Gaussian elimination to get the identity matrix) and I know how to do it for a general matrix by taking two different cases when the determinant is zero or not. And, unless I'm mistaken, I also understand how a matrix is a linear transformation - basically a function - that can be understood as transforming the standard basis vectors. That is, I can see a simple matrix and know what it does geometrically. Is there a way to tell the inverse of a generic matrix my looking at the matrix itself? I looked at Finding inverse of a matrix geometrically but I didn't get it (There were no pictures to help me gain geometric insight). I think that I could do it for some, like the inverse of a matrix that does a stretch would be a matrix that does a compression, or a matrix that rotates counterclockwise would be a matrix that rotates clockwise -- but is there an ""instant"" way of knowing what the inverse matrix is based upon a picture?",2 \times 2 2 \times 2 2 \times 2,"['linear-algebra', 'matrices', 'linear-transformations', 'inverse', 'geometric-interpretation']"
36,$\bigcap\limits_{\varphi\in E^*}\ker(\varphi)$ and the Axiom of Choise,and the Axiom of Choise,\bigcap\limits_{\varphi\in E^*}\ker(\varphi),"Context. Give a nonzero $K$ -vector space $E$ , it is known that $\displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=0$ under AC. It is also known that, without AC, there are models of ZF in which some non zero vector space do not have nonzero linear forms. For such an $E$ , $\displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=E.$ So my question is: Question. Are there models of ZF for which the dimension of $\displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)$ can have any prescribed cardinality $\kappa$ ? (the model may depend on $\kappa$ , even if it would be supernice to have a single model which works for any cardinality). Or, do we have necessarily the alternative $\displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=0$ or $E$ ? I am also interested in the weaker question: can $\displaystyle\bigcap_{\varphi\in E^*}\ker(\varphi)$ be a nonzero proper subspace of $E$ ? Side remark. There is no precise reason why I am interested in this, just plain curiosity. These questions popped out because I realized that I needed the existence of some  complement of a given line of $E$ to prove that the intersection above is zero.","Context. Give a nonzero -vector space , it is known that under AC. It is also known that, without AC, there are models of ZF in which some non zero vector space do not have nonzero linear forms. For such an , So my question is: Question. Are there models of ZF for which the dimension of can have any prescribed cardinality ? (the model may depend on , even if it would be supernice to have a single model which works for any cardinality). Or, do we have necessarily the alternative or ? I am also interested in the weaker question: can be a nonzero proper subspace of ? Side remark. There is no precise reason why I am interested in this, just plain curiosity. These questions popped out because I realized that I needed the existence of some  complement of a given line of to prove that the intersection above is zero.",K E \displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=0 E \displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=E. \displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi) \kappa \kappa \displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=0 E \displaystyle\bigcap_{\varphi\in E^*}\ker(\varphi) E E,"['linear-algebra', 'set-theory', 'axiom-of-choice']"
37,"Consider the natural numbers x, y, z that simultaneously satisfy the conditions:","Consider the natural numbers x, y, z that simultaneously satisfy the conditions:",,"Consider the natural numbers $x, y, z$ that simultaneously satisfy the conditions: i) $x,y,z \in \{2000, 2001, 2002, \ldots, 2025\}$ : ii) $|y-z| \le 2$ iii) $\sqrt{1+x\sqrt{yz+1}}=2023$ Show that one of the numbers $x, y, z$ is equal to $2023$ . MY IDEAS Let $x \le y \le z$ $z-y$ must be $\le 2$ then $z-y$ can be $0, 1$ or $2$ . If $z-y=0$ , then $z=y$ The equality will become $\sqrt{1+x\sqrt{{z}^{2}+1}}=2023$ ${z}^{2}+1$ is irrational and will make all the equation irrational. But 2023 isn't irrational, which makes this case impossible. I tried doing the same for $z-y=1$ and $z-y=2$ but got nowhere. Like I don't know what to do. Hope one of you can help me. Any ideas are welcome! Thank you!","Consider the natural numbers that simultaneously satisfy the conditions: i) : ii) iii) Show that one of the numbers is equal to . MY IDEAS Let must be then can be or . If , then The equality will become is irrational and will make all the equation irrational. But 2023 isn't irrational, which makes this case impossible. I tried doing the same for and but got nowhere. Like I don't know what to do. Hope one of you can help me. Any ideas are welcome! Thank you!","x, y, z x,y,z \in \{2000, 2001, 2002, \ldots, 2025\} |y-z| \le 2 \sqrt{1+x\sqrt{yz+1}}=2023 x, y, z 2023 x \le y \le z z-y \le 2 z-y 0, 1 2 z-y=0 z=y \sqrt{1+x\sqrt{{z}^{2}+1}}=2023 {z}^{2}+1 z-y=1 z-y=2","['calculus', 'linear-algebra']"
38,The Definition of Orthogonal Complement,The Definition of Orthogonal Complement,,"In Linear Algebra Done Right by Axler, the author defines the orthogonal complement as follows: If $U$ is a subset of a vector space $V$ , then the orthogonal complement of $U$ , denoted by $U^\perp$ , is the set of all vectors in $V$ that are orthogonal to every vector in $U$ : $$U^{\perp}=\{v\in V : \langle v, u\rangle = 0 \quad\forall u\in U\}$$ Then, he says ""for example, if $U$ is a line in $\mathbb{R}^3$ , then $U^\perp$ is the plane containing the origin that is perpendicular to $U$ "". Below this comment, there is a theorem that says that for any subset $U$ of $V$ , $U\cap U^\perp \subset \{0\}$ . I have some questions and confusions. Is it common to define the orthogonal complement for any subset of $V$ instead of a subspace of $V$ ? From the example he mentioned, if $U$ is a line not going through the origin and $U^\perp$ is the plane containing the origin that is perpendicular to $U$ , I would expect that there would be an intersection of $U$ and $U^\perp$ which is not the origin. But the theorem implies this cannot happen. Graphically, how could one picture $U$ and $U^\perp$ such that $U\cap U^\perp=\emptyset$ ? All of these questions will be answered if we define the orthogonal complement only for a subspace, not for any set. I wonder what his intension was to define it for an arbitrary subset.","In Linear Algebra Done Right by Axler, the author defines the orthogonal complement as follows: If is a subset of a vector space , then the orthogonal complement of , denoted by , is the set of all vectors in that are orthogonal to every vector in : Then, he says ""for example, if is a line in , then is the plane containing the origin that is perpendicular to "". Below this comment, there is a theorem that says that for any subset of , . I have some questions and confusions. Is it common to define the orthogonal complement for any subset of instead of a subspace of ? From the example he mentioned, if is a line not going through the origin and is the plane containing the origin that is perpendicular to , I would expect that there would be an intersection of and which is not the origin. But the theorem implies this cannot happen. Graphically, how could one picture and such that ? All of these questions will be answered if we define the orthogonal complement only for a subspace, not for any set. I wonder what his intension was to define it for an arbitrary subset.","U V U U^\perp V U U^{\perp}=\{v\in V : \langle v, u\rangle = 0 \quad\forall u\in U\} U \mathbb{R}^3 U^\perp U U V U\cap U^\perp \subset \{0\} V V U U^\perp U U U^\perp U U^\perp U\cap U^\perp=\emptyset",['linear-algebra']
39,Proof for $X^3=I$ $\implies$ $X$ is diagonalizable.,Proof for    is diagonalizable.,X^3=I \implies X,"Let $X$ be a $3\times 3$ complex matrix, and suppose $X^3=I.$ Then, show that $X$ is diagonalizable. I searched the solution, and this seems to work, but there is a part I don't understand. Proof Let me use this proposition : Let $A$ be $3\times 3$ matrix. If a polynomial $P(x)$ satisfies $P(A)=O$ , then $P(x)$ is devided by minimal polynomial of $A$ ; $\varphi_A(x)$ . Now, for $P(x):=x^3-1=(x-1)(x-\omega)(x-\omega^2)$ , I have $P(X)=X^3-I=O,$ so by the proposition, $P(x)$ is devided by $\varphi_X(x).$ Thus, $\varphi_X(x)$ doesn't have repeated roots, and therefore $X$ is diagonalizable. I don't understand the part "" Thus, $\varphi_X(x)$ doesn't have repeated roots."" What I know is that $P(x)$ can be devided by $\varphi_X(x)$ , so I have $P(x)=\varphi_X(x)\cdot B(x)$ for some polynomial $B(x)$ . Why this leads the fact that $\varphi_X(x)$ doesn't have repeated roots ? Postscript I reffered to Ben's answer and Stephen's answer in $A^3 = I$. Find the possible Jordan Forms???","Let be a complex matrix, and suppose Then, show that is diagonalizable. I searched the solution, and this seems to work, but there is a part I don't understand. Proof Let me use this proposition : Let be matrix. If a polynomial satisfies , then is devided by minimal polynomial of ; . Now, for , I have so by the proposition, is devided by Thus, doesn't have repeated roots, and therefore is diagonalizable. I don't understand the part "" Thus, doesn't have repeated roots."" What I know is that can be devided by , so I have for some polynomial . Why this leads the fact that doesn't have repeated roots ? Postscript I reffered to Ben's answer and Stephen's answer in $A^3 = I$. Find the possible Jordan Forms???","X 3\times 3 X^3=I. X A 3\times 3 P(x) P(A)=O P(x) A \varphi_A(x) P(x):=x^3-1=(x-1)(x-\omega)(x-\omega^2) P(X)=X^3-I=O, P(x) \varphi_X(x). \varphi_X(x) X \varphi_X(x) P(x) \varphi_X(x) P(x)=\varphi_X(x)\cdot B(x) B(x) \varphi_X(x)","['linear-algebra', 'matrices', 'diagonalization', 'minimal-polynomials']"
40,Minimizing $\frac{\operatorname{Tr}(H^2)^2}{\operatorname{Tr}(H^3)\operatorname{Tr}H}$,Minimizing,\frac{\operatorname{Tr}(H^2)^2}{\operatorname{Tr}(H^3)\operatorname{Tr}H},Suppose $H$ is a diagonal positive definite $d\times d$ matrix with $\operatorname{Tr}(H)=1$ . I'm interested in $H$ which minimizes the following: $$J=\frac{\operatorname{Tr}(H^2)^2}{\operatorname{Tr}(H^3)\operatorname{Tr}H}$$ I can use numerical optimizer to solve it and get something like this on diagonal for $d=40$ . Largest value is $0.239376$ and remaining mass is split equally among remaining dimensions. What is the solution in the limit of $d\to \infty$ ? Motivation: minimizing $J$ gives shape of quadratic which is hardest to minimize with a single step of gradient descent.,Suppose is a diagonal positive definite matrix with . I'm interested in which minimizes the following: I can use numerical optimizer to solve it and get something like this on diagonal for . Largest value is and remaining mass is split equally among remaining dimensions. What is the solution in the limit of ? Motivation: minimizing gives shape of quadratic which is hardest to minimize with a single step of gradient descent.,H d\times d \operatorname{Tr}(H)=1 H J=\frac{\operatorname{Tr}(H^2)^2}{\operatorname{Tr}(H^3)\operatorname{Tr}H} d=40 0.239376 d\to \infty J,"['linear-algebra', 'optimization']"
41,Prove that $\det\begin{pmatrix}A&B\\-B&A\end{pmatrix}$ is a sum of squares of polynomials,Prove that  is a sum of squares of polynomials,\det\begin{pmatrix}A&B\\-B&A\end{pmatrix},"As discussed for example in this question , given any pair of real squared matrices $A,B$ we have the identity $$|\det(A+iB)|^2 = \det\begin{pmatrix}A&B\\ -B&A\end{pmatrix}.$$ In particular, this means that $\det\begin{pmatrix}A&B\\ -B&A\end{pmatrix}$ must be writable as the sum of squares of two polynomials in the elements of $A,B$ . Namely, it equals the sum of the squares of real and imaginary parts of $\det(A+iB)$ , which are polynomials in the elements of $A$ and $B$ . While this is clear from the above identity using complex numbers, is there a direct way to see that this is true reasoning only on the structure of this matrix, without passing through complex numbers? On the face of it, it looks like there should be a reasoning similar to what is done to show that the determinant of skew-symmetric matrices can be written as the square of the Pfaffian , but I'm not sure how to make this into a precise argument. This also relates to Calculating the determinant gives $(a^2+b^2+c^2+d^2)^2$? and Prove that $\left|\begin{smallmatrix}a&-b&-c&-d\\b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{smallmatrix}\right|=(a^2+b^2+c^2+d^2)^2$ , where the structure of the matrices considered in those questions is such that $\det(A+iB)$ is either purely real or purely imaginary. In fact, using the expressions in those posts as starting point and some guess work, it seems we have, at least when $A,B$ are $2\times2$ , the decomposition $$\det\begin{pmatrix}A&B\\-B&A\end{pmatrix} = (\det(A)-\det(B))^2 + (a_{11} b_{22} - a_{12} b_{21} + a_{22}b_{11} - a_{21}b_{12})^2.$$ The second term can also probably be cast in a more neat form.","As discussed for example in this question , given any pair of real squared matrices we have the identity In particular, this means that must be writable as the sum of squares of two polynomials in the elements of . Namely, it equals the sum of the squares of real and imaginary parts of , which are polynomials in the elements of and . While this is clear from the above identity using complex numbers, is there a direct way to see that this is true reasoning only on the structure of this matrix, without passing through complex numbers? On the face of it, it looks like there should be a reasoning similar to what is done to show that the determinant of skew-symmetric matrices can be written as the square of the Pfaffian , but I'm not sure how to make this into a precise argument. This also relates to Calculating the determinant gives $(a^2+b^2+c^2+d^2)^2$? and Prove that $\left|\begin{smallmatrix}a&-b&-c&-d\\b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{smallmatrix}\right|=(a^2+b^2+c^2+d^2)^2$ , where the structure of the matrices considered in those questions is such that is either purely real or purely imaginary. In fact, using the expressions in those posts as starting point and some guess work, it seems we have, at least when are , the decomposition The second term can also probably be cast in a more neat form.","A,B |\det(A+iB)|^2 = \det\begin{pmatrix}A&B\\ -B&A\end{pmatrix}. \det\begin{pmatrix}A&B\\ -B&A\end{pmatrix} A,B \det(A+iB) A B \det(A+iB) A,B 2\times2 \det\begin{pmatrix}A&B\\-B&A\end{pmatrix}
= (\det(A)-\det(B))^2 + (a_{11} b_{22} - a_{12} b_{21} + a_{22}b_{11} - a_{21}b_{12})^2.","['linear-algebra', 'matrices', 'determinant', 'block-matrices', 'pfaffian']"
42,Convex optimization using constraint projection matrices,Convex optimization using constraint projection matrices,,"I have a convex optimization of the form $$ \min_x \frac{1}{2} x^TAx-x^Tb \\ \text{s.t.}\ (I-P)x=0 $$ where $A$ is a $n$ by $n$ positive definite matrix, and $P$ is a $n$ by $n$ projection matrix (it has $p$ eigenvalues equal to zero, and $n-p$ eigenvalues equal to one) Intuitively, it seems like I can separate the subspaces by rewriting it to: $$ \min_x \frac{1}{2} x^TP^TAPx+\frac{\gamma}{2} x^T(I-P)^T(I-P)x-x^TP^Tb $$ And since $P$ is a projection matrix, it is symmetric and idempotent, it can be simplified to: $$ \min_x \frac{1}{2} x^T(PAP+\gamma(I-P))x-x^TPb $$ This results in an unconstrained optimization, and the result is obtained by solving: $$(PAP+\gamma(I-P))x=Pb$$ This formulation is particularly handy because the matrix $(PAP+\gamma(I-P))$ is still positive definite for any positive $\gamma$ value (which is in fact the eigenvalues of the constraint subspace), so it can be solved numerically using the conjugate gradient method. In practice, it works very well, but to be honest, I don't know if there is a flaw in my intuition, and I haven't found any resources on the subject. Is there a known method that I can use to demonstrate my intuition is correct?","I have a convex optimization of the form where is a by positive definite matrix, and is a by projection matrix (it has eigenvalues equal to zero, and eigenvalues equal to one) Intuitively, it seems like I can separate the subspaces by rewriting it to: And since is a projection matrix, it is symmetric and idempotent, it can be simplified to: This results in an unconstrained optimization, and the result is obtained by solving: This formulation is particularly handy because the matrix is still positive definite for any positive value (which is in fact the eigenvalues of the constraint subspace), so it can be solved numerically using the conjugate gradient method. In practice, it works very well, but to be honest, I don't know if there is a flaw in my intuition, and I haven't found any resources on the subject. Is there a known method that I can use to demonstrate my intuition is correct?","
\min_x \frac{1}{2} x^TAx-x^Tb \\
\text{s.t.}\ (I-P)x=0
 A n n P n n p n-p 
\min_x \frac{1}{2} x^TP^TAPx+\frac{\gamma}{2} x^T(I-P)^T(I-P)x-x^TP^Tb
 P 
\min_x \frac{1}{2} x^T(PAP+\gamma(I-P))x-x^TPb
 (PAP+\gamma(I-P))x=Pb (PAP+\gamma(I-P)) \gamma","['linear-algebra', 'convex-optimization', 'numerical-optimization', 'conjugate-gradient']"
43,Prove that the curve given by $f(t)=\cos(t) u_1 + \sin(t)u_2$ is an ellipse.,Prove that the curve given by  is an ellipse.,f(t)=\cos(t) u_1 + \sin(t)u_2,"Let $u_1$ , $u_2 \in \mathbb{R}^2$ be two linearly independent vectors. Prove that the curve given by $f(t)=\cos(t) u_1 + \sin(t)u_2$ is an ellipse. My attempt Let $u_1=(a_1,a_2)$ , $u_2=(b_1,b_2)$ , $x=a_1 \cos(t)+b_1 \sin(t)$ , $y=a_2 \cos(t)+b_2 \sin(t)$ Then: $x^2=a_1^2 \cos ^2(t) + a_1 b_1 \sin(2t) + b_1^2 \sin^2(t)$ $y^2=a_2^2 \cos ^2(t) + a_2 b_2 \sin(2t) + b_2^2 \sin^2(t)$ I'm trying to figure out what would be the two constants $\alpha, \beta$ such that $\dfrac{x^2}{\alpha^2}+ \dfrac{y^2}{\beta^2}=1$ , but I'm not sure what to do next. I'm aware that the fact that $u_1$ and $u_2$ are linearly independent implies that $f(t) \neq 0$ ...","Let , be two linearly independent vectors. Prove that the curve given by is an ellipse. My attempt Let , , , Then: I'm trying to figure out what would be the two constants such that , but I'm not sure what to do next. I'm aware that the fact that and are linearly independent implies that ...","u_1 u_2 \in \mathbb{R}^2 f(t)=\cos(t) u_1 + \sin(t)u_2 u_1=(a_1,a_2) u_2=(b_1,b_2) x=a_1 \cos(t)+b_1 \sin(t) y=a_2 \cos(t)+b_2 \sin(t) x^2=a_1^2 \cos ^2(t) + a_1 b_1 \sin(2t) + b_1^2 \sin^2(t) y^2=a_2^2 \cos ^2(t) + a_2 b_2 \sin(2t) + b_2^2 \sin^2(t) \alpha, \beta \dfrac{x^2}{\alpha^2}+ \dfrac{y^2}{\beta^2}=1 u_1 u_2 f(t) \neq 0","['linear-algebra', 'multivariable-calculus', 'vector-spaces', 'analytic-geometry']"
44,Connection between exponents of a root system and solutions to linear systems over finite fields,Connection between exponents of a root system and solutions to linear systems over finite fields,,"Let $h_1, \ldots, h_r$ be linear forms in variables $x_1, \ldots, x_n$ with integer coefficients. Let $\mathbb F_q$ denote the finite field with $q = p^e$ elements. I am asked to prove that except in a finite number of characteristics $p$ , the number of vectors $v \in \mathbb F_q^n$ such that $h_i(v) = 0$ for all $i$ is given for all $q$ by a polynomial $\chi(q)$ in $q$ with integer coefficients, and that $(-1)^n \chi(-1)$ is equal to the number of connected regions into which $\mathbb R^n$ is separated by the removal of all the hyperplanes $h_i = 0$ . My confusion is with this part (see next paragraph for more context; see below for question). This is an exercise from a set of notes on Lie algebras. The upshot of this result is meant to be the following: if we let the $h_i$ be the root hyperplanes of a finite root system, then $\chi(q)$ has integer roots $e_1, \ldots, e_n$ ; call these the exponents of the root system. The hyperplanes $h_i$ divide $\mathbb R^n$ into Weyl chambers, so in fact the order of the Weyl group is $$(-1)^n \chi(-1) = (-1)^n \prod_{i=1}^n (-1-e_i) = \prod_{i=1}^n (1+e_i)$$ My question is the following: is it not the case that a linear system over $\mathbb F_q$ will always have $q^k$ elements where $k$ is the dimension of the null space of coefficient matrix of the $h_i$ ? For all but finitely many characteristics, we can perform row reduction over the rationals and all of the values by which we multiply will be elements of $\mathbb F_q$ (e.g. if we multiply by, say, $\frac3{10}$ , $\frac{5}{22}$ , and $\frac{2}{15}$ in the row reduction process, then any $p$ larger than $11$ will do). So the rank, and hence the nullity, is some fixed constant for all but finitely many characteristics. This would imply that $\chi(q) = q^k$ , but then $(-1)^n \chi(-1) \in \{\pm 1\}$ , which (except possibly the $h_i$ are all zero) is not the number of connected regions of $\mathbb R^n \setminus \{h_i\}$ . Am I missing something here? I haven't been able to find any reference on the exponents of a root system as defined above (I've found plenty on the exponents of a Weyl/Coxeter group which satisfy the product relation above); perhaps there is some modification to the definition of exponents that makes this work out. Note : this is homework; just a mild push in the right direction would be ideal.","Let be linear forms in variables with integer coefficients. Let denote the finite field with elements. I am asked to prove that except in a finite number of characteristics , the number of vectors such that for all is given for all by a polynomial in with integer coefficients, and that is equal to the number of connected regions into which is separated by the removal of all the hyperplanes . My confusion is with this part (see next paragraph for more context; see below for question). This is an exercise from a set of notes on Lie algebras. The upshot of this result is meant to be the following: if we let the be the root hyperplanes of a finite root system, then has integer roots ; call these the exponents of the root system. The hyperplanes divide into Weyl chambers, so in fact the order of the Weyl group is My question is the following: is it not the case that a linear system over will always have elements where is the dimension of the null space of coefficient matrix of the ? For all but finitely many characteristics, we can perform row reduction over the rationals and all of the values by which we multiply will be elements of (e.g. if we multiply by, say, , , and in the row reduction process, then any larger than will do). So the rank, and hence the nullity, is some fixed constant for all but finitely many characteristics. This would imply that , but then , which (except possibly the are all zero) is not the number of connected regions of . Am I missing something here? I haven't been able to find any reference on the exponents of a root system as defined above (I've found plenty on the exponents of a Weyl/Coxeter group which satisfy the product relation above); perhaps there is some modification to the definition of exponents that makes this work out. Note : this is homework; just a mild push in the right direction would be ideal.","h_1, \ldots, h_r x_1, \ldots, x_n \mathbb F_q q = p^e p v \in \mathbb F_q^n h_i(v) = 0 i q \chi(q) q (-1)^n \chi(-1) \mathbb R^n h_i = 0 h_i \chi(q) e_1, \ldots, e_n h_i \mathbb R^n (-1)^n \chi(-1) = (-1)^n \prod_{i=1}^n (-1-e_i) = \prod_{i=1}^n (1+e_i) \mathbb F_q q^k k h_i \mathbb F_q \frac3{10} \frac{5}{22} \frac{2}{15} p 11 \chi(q) = q^k (-1)^n \chi(-1) \in \{\pm 1\} h_i \mathbb R^n \setminus \{h_i\}","['linear-algebra', 'lie-algebras', 'finite-fields', 'root-systems', 'weyl-group']"
45,"Prove that if $A$ has integer entries, $|\det A| = 1$, then the polyhedron formed by $A$ does not contain integer points.","Prove that if  has integer entries, , then the polyhedron formed by  does not contain integer points.",A |\det A| = 1 A,"Given $n$ vectors $v_1 , v_2 , v_3 , \dots, v_n \in \mathbb{R}^n$ with integer entries in each vector. Prove that if $|\det(v_1 , v_2 , \dots , v_n)| = 1$ , then the polyhedron $Ov_1v_2\dots v_n$ does not have any points with integer coordinates inside it, except the vertices. For example, My idea Use determinant to prove that $$\det A =  \text{Volume of the paralelipiped spanned by }A $$ and because $\det A$ has integer value, we have $\min|\det A| = 1$ . Then conclude that the parallelepiped spanned by $A$ is the smallest. So, if $A$ has any other integer points inside it, it will contain another smaller parallelepiped, which is conflicted with the previous statement. But here we have to consider the polyhedron, which is something I'm stuck with.","Given vectors with integer entries in each vector. Prove that if , then the polyhedron does not have any points with integer coordinates inside it, except the vertices. For example, My idea Use determinant to prove that and because has integer value, we have . Then conclude that the parallelepiped spanned by is the smallest. So, if has any other integer points inside it, it will contain another smaller parallelepiped, which is conflicted with the previous statement. But here we have to consider the polyhedron, which is something I'm stuck with.","n v_1 , v_2 , v_3 , \dots, v_n \in \mathbb{R}^n |\det(v_1 , v_2 , \dots , v_n)| = 1 Ov_1v_2\dots v_n \det A =  \text{Volume of the paralelipiped spanned by }A  \det A \min|\det A| = 1 A A","['linear-algebra', 'matrices', 'determinant', 'volume', 'integer-lattices']"
46,"When will ""permuted vectors"" be linearly independent?","When will ""permuted vectors"" be linearly independent?",,"Let $n\geq2$ be a natural number and let $x_1,\ldots,x_n$ be $n$ real numbers.  Is there a general sufficient condition to guarantee that the set of $n$ ""cyclicly permuted"" vectors $\left\{(x_1,x_2,\ldots,x_n), (x_n, x_1,\ldots,x_{n-1}), \ldots, (x_2,x_3,\ldots,x_1)\right\}$ is linearly independent? When $n=2$ it is sufficient and necessary that $(x_1+x_2)(x_1-x_2)\ne0$ .  When $n=3$ , the determinant of the matrix whose rows are those vectors is $(x_1+x_2+x_3)\left(\frac{{(x_1-x_2)}^2+{(x_1-x_3)}^2+{(x_2-x_3)}^2}{2}\right)$ , so the necessary and sufficient condition is that the sum of those $3$ numbers is not zero, and they are not all the same number. In general, the determinant of the matrix is $$ \prod_{k=0}^{n-1}\left(\sum_{\ell=0}^{n-1}e^{\frac{2\pi ik\ell}{n}}\cdot x_{\ell+1}\right). $$ But I am unable to deduce some intuitive condition for that to be non-zero. Since I think that determinant is a product of discrete Fourier transforms, I also tag this as related to Fourier transform. If this is inappropriate, I will remove that tag. P.S. The determinant can be found as Lemma 5.26 in Washington's Introduction to cyclotomic fields. Any help is greatly appreciated.","Let be a natural number and let be real numbers.  Is there a general sufficient condition to guarantee that the set of ""cyclicly permuted"" vectors is linearly independent? When it is sufficient and necessary that .  When , the determinant of the matrix whose rows are those vectors is , so the necessary and sufficient condition is that the sum of those numbers is not zero, and they are not all the same number. In general, the determinant of the matrix is But I am unable to deduce some intuitive condition for that to be non-zero. Since I think that determinant is a product of discrete Fourier transforms, I also tag this as related to Fourier transform. If this is inappropriate, I will remove that tag. P.S. The determinant can be found as Lemma 5.26 in Washington's Introduction to cyclotomic fields. Any help is greatly appreciated.","n\geq2 x_1,\ldots,x_n n n \left\{(x_1,x_2,\ldots,x_n), (x_n, x_1,\ldots,x_{n-1}), \ldots, (x_2,x_3,\ldots,x_1)\right\} n=2 (x_1+x_2)(x_1-x_2)\ne0 n=3 (x_1+x_2+x_3)\left(\frac{{(x_1-x_2)}^2+{(x_1-x_3)}^2+{(x_2-x_3)}^2}{2}\right) 3 
\prod_{k=0}^{n-1}\left(\sum_{\ell=0}^{n-1}e^{\frac{2\pi ik\ell}{n}}\cdot x_{\ell+1}\right).
","['linear-algebra', 'determinant', 'fourier-transform', 'linear-independence']"
47,An example when $(U^\perp)^\perp = U$ but $V\ne U\oplus U^\perp$?,An example when  but ?,(U^\perp)^\perp = U V\ne U\oplus U^\perp,"Let's talk of inner product spaces. I can show that if $V = U\oplus U^\perp$ , then $(U^\perp)^\perp = U$ holds. Then I wonder about the converse. I was able to dig up a post on SE where an example was given, showing that $U$ being closed is not enough for $V = U\oplus U^\perp$ . I know that if $V$ is Hilbert, then $U$ being closed implies $(U^\perp)^\perp = U$ . But the example's $V$ is not, rendering my search still open since there $U^\perp = \{0\}$ and $U\subsetneq V$ . But I am not able to find one, or disprove this. Hence I seek help! I'd appreciate if what you give is accessible with my current background. My background: I am taking a course on linear algebra, and the professor deals only with finite dimensional spaces. But whenever a chance comes up, I try to generalise the results and see what fails in infinite dimensions. So far, I have seen some counterexamples via $\ell^2$ .","Let's talk of inner product spaces. I can show that if , then holds. Then I wonder about the converse. I was able to dig up a post on SE where an example was given, showing that being closed is not enough for . I know that if is Hilbert, then being closed implies . But the example's is not, rendering my search still open since there and . But I am not able to find one, or disprove this. Hence I seek help! I'd appreciate if what you give is accessible with my current background. My background: I am taking a course on linear algebra, and the professor deals only with finite dimensional spaces. But whenever a chance comes up, I try to generalise the results and see what fails in infinite dimensions. So far, I have seen some counterexamples via .",V = U\oplus U^\perp (U^\perp)^\perp = U U V = U\oplus U^\perp V U (U^\perp)^\perp = U V U^\perp = \{0\} U\subsetneq V \ell^2,"['linear-algebra', 'functional-analysis', 'inner-products', 'orthogonality', 'direct-sum']"
48,"Linear algebra done right, Exercise 16, section 2.A","Linear algebra done right, Exercise 16, section 2.A",,"This question asks to prove that the real vector space of all continuous real-valued functions on the interval $[0,1]$ is infinite-dimensional. In an earlier question it was proved that a vector space $V$ is infinite-dimensional if and only if there exists a sequence of vectors $\{v_j\}$ such that $$v_1, v_2, \ldots, v_m$$ is linearly independent for every positive integer $m$ . I worked with the question and got this proof. Consider a sequence of such functions $\{f_j\}$ where $f_k(x) = x^k$ for all $k \in \mathbb N$ . It's easy to see that $$f_1, f_2, \ldots, f_m$$ is linearly independent for all positive integer $m$ . Thus this vector space is infinite-dimensional. However, when I checked the solution on this website, https://linearalgebras.com/2a.html , they constructed the function as $$f_k(x) = \begin{cases}x-\frac 1 k, & x\geq \frac 1 k; \\ 0, & \text{otherwise}. \end{cases}$$ I want to ask if there are any flaws in my proof.","This question asks to prove that the real vector space of all continuous real-valued functions on the interval is infinite-dimensional. In an earlier question it was proved that a vector space is infinite-dimensional if and only if there exists a sequence of vectors such that is linearly independent for every positive integer . I worked with the question and got this proof. Consider a sequence of such functions where for all . It's easy to see that is linearly independent for all positive integer . Thus this vector space is infinite-dimensional. However, when I checked the solution on this website, https://linearalgebras.com/2a.html , they constructed the function as I want to ask if there are any flaws in my proof.","[0,1] V \{v_j\} v_1, v_2, \ldots, v_m m \{f_j\} f_k(x) = x^k k \in \mathbb N f_1, f_2, \ldots, f_m m f_k(x) = \begin{cases}x-\frac 1 k, & x\geq \frac 1 k; \\ 0, & \text{otherwise}. \end{cases}","['linear-algebra', 'vector-spaces', 'solution-verification']"
49,Only commutators solve this functional equation,Only commutators solve this functional equation,,"I need to prove that the only linear transformations $f\colon  M_{n\times n}\rightarrow M_{n\times n}$ that solve the functional equation in $\mathcal{L}(M_{n\times n})$ $$f(XY)=f(X)Y+Xf(Y)$$ are the commutators. That is, I have to show that for every linear solution $f$ there exists a matrix $A$ such that $f(X)=[A,X]$ for all $X$ . Bearing in mind that every commutator satisfies the equation, I have tried to solve this studying the sets $V_A=\{X\in M_{n \times n}\colon f(X)=[A,X]\}$ , where $A\in \ker f$ , which happen to be subalgebras that are stable under $f$ and include inverses. Specifically, I have been looking for a maximality argument that leads me to conclude that some of these $V_A$ must be all of $M_{n\times n}$ . So far, I have not been able to exploit the finite dimension of $M_{n\times n}$ . This problem appears as an exercise at the end of an introductory chapter on matrices and finite dimensional vector spaces in Katsumi Nomizu's Fundamentals of Linear Algebra , so little else beyond the rank-nullity theorem is assumed.","I need to prove that the only linear transformations that solve the functional equation in are the commutators. That is, I have to show that for every linear solution there exists a matrix such that for all . Bearing in mind that every commutator satisfies the equation, I have tried to solve this studying the sets , where , which happen to be subalgebras that are stable under and include inverses. Specifically, I have been looking for a maximality argument that leads me to conclude that some of these must be all of . So far, I have not been able to exploit the finite dimension of . This problem appears as an exercise at the end of an introductory chapter on matrices and finite dimensional vector spaces in Katsumi Nomizu's Fundamentals of Linear Algebra , so little else beyond the rank-nullity theorem is assumed.","f\colon  M_{n\times n}\rightarrow M_{n\times n} \mathcal{L}(M_{n\times n}) f(XY)=f(X)Y+Xf(Y) f A f(X)=[A,X] X V_A=\{X\in M_{n \times n}\colon f(X)=[A,X]\} A\in \ker f f V_A M_{n\times n} M_{n\times n}","['linear-algebra', 'functional-equations']"
50,Can we always let two uncommute hermitian matrices $A$ and $B$ commute by enlarge the dimension?,Can we always let two uncommute hermitian matrices  and  commute by enlarge the dimension?,A B,"For example, we have two hermitian matrices $$A=\left( \begin{matrix} 	1&		0\\ 	0&		-1\\ \end{matrix} \right) ,B=\left( \begin{matrix} 	0&		1\\ 	1&		0\\ \end{matrix} \right) .$$ And for this special case, I can find another two hermitian matrices by placing $A$ and $B$ as the principal submatrix commute as follows: $$\tilde{A}=\left( \begin{matrix} 	1&		0&		-1\\ 	0&		-1&		1\\ 	-1&		1&		0\\ \end{matrix} \right) ,\tilde{B}=\left( \begin{matrix} 	0&		1&		1\\ 	1&		0&		1\\ 	1&		1&		0\\ \end{matrix} \right) .$$ I wonder if there exists general theorems guarantee that we can always find two enlarged commuting hermitian matrix like this?","For example, we have two hermitian matrices And for this special case, I can find another two hermitian matrices by placing and as the principal submatrix commute as follows: I wonder if there exists general theorems guarantee that we can always find two enlarged commuting hermitian matrix like this?","A=\left( \begin{matrix}
	1&		0\\
	0&		-1\\
\end{matrix} \right) ,B=\left( \begin{matrix}
	0&		1\\
	1&		0\\
\end{matrix} \right) . A B \tilde{A}=\left( \begin{matrix}
	1&		0&		-1\\
	0&		-1&		1\\
	-1&		1&		0\\
\end{matrix} \right) ,\tilde{B}=\left( \begin{matrix}
	0&		1&		1\\
	1&		0&		1\\
	1&		1&		0\\
\end{matrix} \right) .","['linear-algebra', 'matrices', 'hermitian-matrices']"
51,"Is there a very small gap or no gap in this proof? (""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler.)","Is there a very small gap or no gap in this proof? (""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler.)",,"I am reading ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. Let $V$ be a vector space. Let $V'$ be the dual space of $V$ . Let $W$ be a vector space. Let $W'$ be the dual space of $W$ . Definition: If $T\in\mathcal{L}(V,W)$ , then the dual map of $T$ is the linear map $T'\in\mathcal{L}(W',V')$ defined by $T'(\varphi)=\varphi\circ T$ for $\varphi\in W'$ . Definition: For $U\subset V$ . The annihilator of $U$ , denoted $U^0$ , is defined by $$U^0=\{\varphi\in V' : \varphi(u)=0\text{ for all }u\in U\}.$$ The next result is from ""Linear Algebra Done Right 3rd Edition"". (on p.107) 3.108 $T$ surjective is equivalent to $T'$ injective Suppose $V$ and $W$ are finite-dimensional and $T\in\mathcal{L}(V,W)$ . Then $T$ is surjective if and only if $T'$ is injective. Proof The map $T\in\mathcal{L}(V,W)$ is surjective if and only if $\operatorname{range} T=W$ , which happens if and only if $(\operatorname{range} T)^0=\{0\}$ , which happens if and only if $\operatorname{null} T'=\{0\}$ [by 3.107(a)], which happens if and only if $T'$ is injective. 3.107(a) is the following equation: If $T\in\mathcal{L}(V,W)$ , $$\operatorname{null} T'=(\operatorname{range} T)^0.$$ The author is very kind to the readers and, usually, there is no gaps in his proofs in this book. But I felt a small gap in the proof of 3.108 above. I felt the following fact is not so obvious. Fact 1: If $(\operatorname{range} T)^0=\{0\}$ , then $\operatorname{range} T=W$ . Proof: Assume that $(\operatorname{range} T)^0=\{0\}$ but $\operatorname{range} T\neq W$ . Then, $\dim \operatorname{range} T < \dim W$ . Let $v_1,\dots,v_k$ be a basis of $\operatorname{range} T$ . Let $v_1,\dots,v_k,\dots,v_l$ be a basis of $W$ ( $k<l$ ). Let $\varphi\in W'$ be a linear functional such that $\varphi(v_i)=0$ for all $i\in\{1,\dots,l-1\}$ and $\varphi(v_l)=1$ . Then, $\varphi\neq 0$ and $\varphi(v)=0$ for any $v\in\operatorname{range} T$ . So, $0\neq\varphi\in(\operatorname{range} T)^0$ . This is a contradiction. Does the above fact immediately follow from some famous result? By the way, the author commented about 3.107(a) as follows: The proof of part (a) of the result below does not use the hypothesis that $V$ and $W$ are finite-dimensional. And in 3.108, the author assumed that $V$ and $W$ are finite-dimensional. So, we need to use the assumption that $V$ and $W$ are finite-dimensional in the proof of 3.108. I guess we need to use the assumption that $W$ is finite-dimensional to prove Fact 1 above. I guess we don't need the assumption that $V$ is finite-dimensional to prove  3.108.","I am reading ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. Let be a vector space. Let be the dual space of . Let be a vector space. Let be the dual space of . Definition: If , then the dual map of is the linear map defined by for . Definition: For . The annihilator of , denoted , is defined by The next result is from ""Linear Algebra Done Right 3rd Edition"". (on p.107) 3.108 surjective is equivalent to injective Suppose and are finite-dimensional and . Then is surjective if and only if is injective. Proof The map is surjective if and only if , which happens if and only if , which happens if and only if [by 3.107(a)], which happens if and only if is injective. 3.107(a) is the following equation: If , The author is very kind to the readers and, usually, there is no gaps in his proofs in this book. But I felt a small gap in the proof of 3.108 above. I felt the following fact is not so obvious. Fact 1: If , then . Proof: Assume that but . Then, . Let be a basis of . Let be a basis of ( ). Let be a linear functional such that for all and . Then, and for any . So, . This is a contradiction. Does the above fact immediately follow from some famous result? By the way, the author commented about 3.107(a) as follows: The proof of part (a) of the result below does not use the hypothesis that and are finite-dimensional. And in 3.108, the author assumed that and are finite-dimensional. So, we need to use the assumption that and are finite-dimensional in the proof of 3.108. I guess we need to use the assumption that is finite-dimensional to prove Fact 1 above. I guess we don't need the assumption that is finite-dimensional to prove  3.108.","V V' V W W' W T\in\mathcal{L}(V,W) T T'\in\mathcal{L}(W',V') T'(\varphi)=\varphi\circ T \varphi\in W' U\subset V U U^0 U^0=\{\varphi\in V' : \varphi(u)=0\text{ for all }u\in U\}. T T' V W T\in\mathcal{L}(V,W) T T' T\in\mathcal{L}(V,W) \operatorname{range} T=W (\operatorname{range} T)^0=\{0\} \operatorname{null} T'=\{0\} T' T\in\mathcal{L}(V,W) \operatorname{null} T'=(\operatorname{range} T)^0. (\operatorname{range} T)^0=\{0\} \operatorname{range} T=W (\operatorname{range} T)^0=\{0\} \operatorname{range} T\neq W \dim \operatorname{range} T < \dim W v_1,\dots,v_k \operatorname{range} T v_1,\dots,v_k,\dots,v_l W k<l \varphi\in W' \varphi(v_i)=0 i\in\{1,\dots,l-1\} \varphi(v_l)=1 \varphi\neq 0 \varphi(v)=0 v\in\operatorname{range} T 0\neq\varphi\in(\operatorname{range} T)^0 V W V W V W W V","['linear-algebra', 'solution-verification', 'dual-spaces']"
52,Computing the smallest eigenvalue of a positive definite matrix $\bf{A}$ without using $\bf{A^{-1}}$,Computing the smallest eigenvalue of a positive definite matrix  without using,\bf{A} \bf{A^{-1}},"I want to numerically compute the smallest eigenvalue of an $n \times n$ positive definite matrix $\bf{A}$ . Why I want to avoid working with $\bf{A^{-1}}$ I know that I can apply power iteration to $\bf{A^{-1}}$ , but I wish to avoid working with this inverse matrix. In my application, $\bf{A}$ is implicitly expressed as $\bf{A} = \bf{K'K}$ with $\bf{K} = \bf{L^{-1}R}$ , where $\bf{L}$ is an $m \times m$ lower triangular band matrix; $\bf{R}$ is an $m \times n$ band matrix of full column rank $(m > n)$ . While $\bf{L}$ and $\bf{R}$ are sparse, $\bf{K}$ and $\bf{A}$ are fully dense. Given that both $m$ and $n$ can be large, I want to explicitly form neither $\bf{K}$ nor $\bf{A}$ . A bad luck is that $m \neq n$ , so that $\bf{R}$ is not square and there is no convenient factor form as $\bf{A^{-1}} = R^{-1}LL'R^{-1\prime}$ . What I have tried With $\bf{A}$ structured as above, it is computationally efficient to compute $\bf{Av}$ for any vector $\bf{v}$ using sparse linear algebra routines. It is easy to compute $\bf{A}$ 's largest eigenvalue $\mu$ using power iteration: $\bf{v_0} = (\frac{1}{n}, \ldots, \frac{1}{n})$ ; $\bf{u_0} = \bf{Av_0}$ ; $\lambda_0 = \bf{v_0'u_0}$ ; for $k = 1, 2, \ldots$ till convergence of $\{\lambda_k\}$ $\bf{v_k} = \bf{u_{k - 1}} / \|\bf{u_{k - 1}}\|$ ; $\bf{u_k} = \bf{Av_k}$ ; $\lambda_k = \bf{v_k'u_k}$ ; return $\lambda_k$ . To find the smallest eigenvalue, I apply this algorithm to $\bf{B} = \bf{A - \mu\bf{I}}$ , inspired by this thread: https://math.stackexchange.com/a/271876/407465 . Here is an R program to implement this method. PowerIter <- function (A, mu = 0) {   n <- nrow(A)   ## spectral shift   A <- A - diag(mu, n)   ## power iteration   v.old <- rep.int(1 / n, n)   u.old <- A %*% v.old   d.old <- sum(v.old, u.old)   k <- 0L   repeat {     v.new <- u.old * (1 / sqrt(sum(u.old ^ 2)))     u.new <- A %*% v.new     d.new <- sum(v.new * u.new)     if (abs(d.new - d.old) < abs(d.old) * 1e-8) break  ## test relative error     d.old <- d.new     u.old <- u.new     k <- k + 1L   }   cat(""convergence after"", k, ""iterations.\n"")   d.new } What goes wrong? I found that this suggested algorithm (via spectral shift) does not seem numerically stable. I composed two toy examples, one being successful, the other being problematic. A successful example \begin{equation} \bf{A} = \begin{pmatrix} 2 & -1 & 0\\ -1 & 2 & -1\\ 0 & -1 & 2 \end{pmatrix} \end{equation} With $|\bf{A} - \lambda\bf{I}| = (2 - \lambda)[(2 - \lambda)^2 - 2]$ , the three eigenvalues are $2 + \sqrt{2} \approx 3.4142136$ , $2$ and $2 - \sqrt{2} \approx 0.5857864$ . The algorithm is a success for this matrix. Here is the output of an R session. A <- matrix(c(2, -1, 0, -1, 2, -1, 0, -1, 2), nrow = 3)  d.max <- PowerIter(A) ## convergence after 7 iterations. ## [1] 3.414214  d.min <- PowerIter(A, d.max) + d.max ## convergence after 1 iterations. ## [1] 0.5857864 A problematic example Now let's construct $\bf{A} = \bf{Q'DQ}$ , where $\bf{D} = \textrm{diag}(d_1, d_2, d_3)$ are eigenvalues and $\bf{Q}$ is a rotation matrix \begin{equation} \bf{Q} = \begin{pmatrix} 0.36 & 0.48 & -0.8\\ -0.8 & 0.6 & 0\\ 0.48 & 0.64 & 0.6 \end{pmatrix} \end{equation} I fixed $d_1 = 1$ , $d_2 = 0.01$ and tried three choices for $d_3$ : $10^{-3}$ , $10^{-5}$ and $10^{-7}$ . Here is the output of an R session. test <- function (d) {   Q <- matrix(c(0.36, -0.8, 0.48, 0.48, 0.6, 0.64, -0.8, 0, 0.6), nrow = 3)   A <- crossprod(Q, d * Q)   d.max <- PowerIter(A)   d.min <- PowerIter(A, d.max) + d.max   c(min = d.min, max = d.max) }  test(c(1, 1e-2, 1e-3)) ## convergence after 3 iterations. ## convergence after 298 iterations. ##         min         max  ## 0.001000543 1.000000000   test(c(1, 1e-2, 1e-5)) ## convergence after 3 iterations. ## convergence after 279 iterations. ##         min         max  ## 1.04883e-05 1.00000e+00   test(c(1, 1e-2, 1e-7)) ## convergence after 3 iterations. ## convergence after 279 iterations. ##          min          max  ## 5.860836e-07 1.000000e+00 Computation of the smallest eigenvalue is slow and becomes increasingly inaccurate as $\bf{A}$ gets less well conditioned (but it is still far from being ill-conditioned!). My questions Is there a mathematical justification for such observation? How can I modify this algorithm for better numerical stability? If the algorithm can not be improved, can someone suggest another efficient algorithm? If no better algorithm exists, any lower bound for the smallest eigenvalue? Gershgorin Circle Theorem is not good as it is too loose a bound.","I want to numerically compute the smallest eigenvalue of an positive definite matrix . Why I want to avoid working with I know that I can apply power iteration to , but I wish to avoid working with this inverse matrix. In my application, is implicitly expressed as with , where is an lower triangular band matrix; is an band matrix of full column rank . While and are sparse, and are fully dense. Given that both and can be large, I want to explicitly form neither nor . A bad luck is that , so that is not square and there is no convenient factor form as . What I have tried With structured as above, it is computationally efficient to compute for any vector using sparse linear algebra routines. It is easy to compute 's largest eigenvalue using power iteration: ; ; ; for till convergence of ; ; ; return . To find the smallest eigenvalue, I apply this algorithm to , inspired by this thread: https://math.stackexchange.com/a/271876/407465 . Here is an R program to implement this method. PowerIter <- function (A, mu = 0) {   n <- nrow(A)   ## spectral shift   A <- A - diag(mu, n)   ## power iteration   v.old <- rep.int(1 / n, n)   u.old <- A %*% v.old   d.old <- sum(v.old, u.old)   k <- 0L   repeat {     v.new <- u.old * (1 / sqrt(sum(u.old ^ 2)))     u.new <- A %*% v.new     d.new <- sum(v.new * u.new)     if (abs(d.new - d.old) < abs(d.old) * 1e-8) break  ## test relative error     d.old <- d.new     u.old <- u.new     k <- k + 1L   }   cat(""convergence after"", k, ""iterations.\n"")   d.new } What goes wrong? I found that this suggested algorithm (via spectral shift) does not seem numerically stable. I composed two toy examples, one being successful, the other being problematic. A successful example With , the three eigenvalues are , and . The algorithm is a success for this matrix. Here is the output of an R session. A <- matrix(c(2, -1, 0, -1, 2, -1, 0, -1, 2), nrow = 3)  d.max <- PowerIter(A) ## convergence after 7 iterations. ## [1] 3.414214  d.min <- PowerIter(A, d.max) + d.max ## convergence after 1 iterations. ## [1] 0.5857864 A problematic example Now let's construct , where are eigenvalues and is a rotation matrix I fixed , and tried three choices for : , and . Here is the output of an R session. test <- function (d) {   Q <- matrix(c(0.36, -0.8, 0.48, 0.48, 0.6, 0.64, -0.8, 0, 0.6), nrow = 3)   A <- crossprod(Q, d * Q)   d.max <- PowerIter(A)   d.min <- PowerIter(A, d.max) + d.max   c(min = d.min, max = d.max) }  test(c(1, 1e-2, 1e-3)) ## convergence after 3 iterations. ## convergence after 298 iterations. ##         min         max  ## 0.001000543 1.000000000   test(c(1, 1e-2, 1e-5)) ## convergence after 3 iterations. ## convergence after 279 iterations. ##         min         max  ## 1.04883e-05 1.00000e+00   test(c(1, 1e-2, 1e-7)) ## convergence after 3 iterations. ## convergence after 279 iterations. ##          min          max  ## 5.860836e-07 1.000000e+00 Computation of the smallest eigenvalue is slow and becomes increasingly inaccurate as gets less well conditioned (but it is still far from being ill-conditioned!). My questions Is there a mathematical justification for such observation? How can I modify this algorithm for better numerical stability? If the algorithm can not be improved, can someone suggest another efficient algorithm? If no better algorithm exists, any lower bound for the smallest eigenvalue? Gershgorin Circle Theorem is not good as it is too loose a bound.","n \times n \bf{A} \bf{A^{-1}} \bf{A^{-1}} \bf{A} \bf{A} = \bf{K'K} \bf{K} = \bf{L^{-1}R} \bf{L} m \times m \bf{R} m \times n (m > n) \bf{L} \bf{R} \bf{K} \bf{A} m n \bf{K} \bf{A} m \neq n \bf{R} \bf{A^{-1}} = R^{-1}LL'R^{-1\prime} \bf{A} \bf{Av} \bf{v} \bf{A} \mu \bf{v_0} = (\frac{1}{n}, \ldots, \frac{1}{n}) \bf{u_0} = \bf{Av_0} \lambda_0 = \bf{v_0'u_0} k = 1, 2, \ldots \{\lambda_k\} \bf{v_k} = \bf{u_{k - 1}} / \|\bf{u_{k - 1}}\| \bf{u_k} = \bf{Av_k} \lambda_k = \bf{v_k'u_k} \lambda_k \bf{B} = \bf{A - \mu\bf{I}} \begin{equation}
\bf{A} = \begin{pmatrix}
2 & -1 & 0\\
-1 & 2 & -1\\
0 & -1 & 2
\end{pmatrix}
\end{equation} |\bf{A} - \lambda\bf{I}| = (2 - \lambda)[(2 - \lambda)^2 - 2] 2 + \sqrt{2} \approx 3.4142136 2 2 - \sqrt{2} \approx 0.5857864 \bf{A} = \bf{Q'DQ} \bf{D} = \textrm{diag}(d_1, d_2, d_3) \bf{Q} \begin{equation}
\bf{Q} = \begin{pmatrix}
0.36 & 0.48 & -0.8\\
-0.8 & 0.6 & 0\\
0.48 & 0.64 & 0.6
\end{pmatrix}
\end{equation} d_1 = 1 d_2 = 0.01 d_3 10^{-3} 10^{-5} 10^{-7} \bf{A}","['linear-algebra', 'eigenvalues-eigenvectors', 'numerical-methods', 'numerical-linear-algebra']"
53,"Highest weight of standard exterior power representation of $\mathfrak sl(n,\mathbb C)$",Highest weight of standard exterior power representation of,"\mathfrak sl(n,\mathbb C)","Consider the standard representation of $\mathfrak sl(n,\mathbb C)$ on $\mathbb{C}^n$ . Let $1 \leq i < n$ . Consider the quotient representation of $\mathfrak sl(n,\mathbb C)$ on the exterior power $\bigwedge^i(\mathbb{C}^n)$ . Consider the Cartan subalgebra $\mathfrak h$ of $\mathfrak sl(n,\mathbb C)$ consisting of diagonal, traceless matrices. Denote by $L_j$ the linear form $\mathfrak h \rightarrow \mathbb{C}$ that picks out the $j$ -th diagonal entry. Fulton and Harris claim that $\bigwedge^i(\mathbb{C}^n)$ has heighest weight $L_1 + ...+ L_i$ . I am struggling to verify that claim. I am even unable to show that $L_1 + ...+ L_i$ is a weight. Any suggestions?","Consider the standard representation of on . Let . Consider the quotient representation of on the exterior power . Consider the Cartan subalgebra of consisting of diagonal, traceless matrices. Denote by the linear form that picks out the -th diagonal entry. Fulton and Harris claim that has heighest weight . I am struggling to verify that claim. I am even unable to show that is a weight. Any suggestions?","\mathfrak sl(n,\mathbb C) \mathbb{C}^n 1 \leq i < n \mathfrak sl(n,\mathbb C) \bigwedge^i(\mathbb{C}^n) \mathfrak h \mathfrak sl(n,\mathbb C) L_j \mathfrak h \rightarrow \mathbb{C} j \bigwedge^i(\mathbb{C}^n) L_1 + ...+ L_i L_1 + ...+ L_i","['linear-algebra', 'abstract-algebra', 'representation-theory', 'lie-algebras', 'exterior-algebra']"
54,Can I Find the Eigenvalues of a Matrix this Way?,Can I Find the Eigenvalues of a Matrix this Way?,,"I have a matrix $A = xx^T - yy^T,$ where both $x$ and $y$ are linearly independent $n$ -column vectors, $n\geq 2$ . To find the eigenvalues, I reasoned this way: Since $x$ and $y$ are linearly independent vectors (given), then $rank(A) = 2.$ So, we have two non-zero eigenvalues and $(n-2)$ eigenvalues, each with a value of zero. Since both $x$ and $y$ are linearly independent, they form a basis for $V = span(x,y).$ Therefore, $Ax = (x.x)x - (x.y)y$ and $Ay = (x.y)y - (y.y)y$ The matrix $A$ relative to $V$ is: $A = \begin{bmatrix}x.x&x.y\\-x.y&-y.y\end{bmatrix}.$ Now, $Av =$ $\lambda$$v$ Therefore, $(A - \lambda I_{2})v = 0$ , where $A$ has been restricted to a $2 \times 2$ matrix. When we solve this, we get the $\lambda$ 's. Is this correct so far? Thanks.","I have a matrix where both and are linearly independent -column vectors, . To find the eigenvalues, I reasoned this way: Since and are linearly independent vectors (given), then So, we have two non-zero eigenvalues and eigenvalues, each with a value of zero. Since both and are linearly independent, they form a basis for Therefore, and The matrix relative to is: Now, Therefore, , where has been restricted to a matrix. When we solve this, we get the 's. Is this correct so far? Thanks.","A = xx^T - yy^T, x y n n\geq 2 x y rank(A) = 2. (n-2) x y V = span(x,y). Ax = (x.x)x - (x.y)y Ay = (x.y)y - (y.y)y A V A = \begin{bmatrix}x.x&x.y\\-x.y&-y.y\end{bmatrix}. Av = \lambdav (A - \lambda I_{2})v = 0 A 2 \times 2 \lambda","['linear-algebra', 'matrices', 'solution-verification', 'eigenvalues-eigenvectors']"
55,Proving $\bigwedge^k(U\cap V) = \left(\bigwedge^kU\right) \bigcap \left(\bigwedge^k V\right)$ via the universal mapping property,Proving  via the universal mapping property,\bigwedge^k(U\cap V) = \left(\bigwedge^kU\right) \bigcap \left(\bigwedge^k V\right),"Let $V$ be a (finite dimensional) vector space, its exterior algebra of order $k$ is the vector space $\bigwedge^k V$ consisting of the formal sums of terms of the form $v_1 \wedge v_2 \wedge \dots \wedge v_k$ , where each $v_i \in V$ (with a few additional properties regarding $\wedge$ ). There are quite a few ways to define $\bigwedge^k V$ rigorously, one of them is via the universal mapping property: There exists a vector space $L$ and an alternating multilinear map $M\colon V^k \to L$ with the universal mapping property in the sense that for any alternating multilinear map $M'\colon V^k \to L'$ , there exists a unique linear map $T\colon L\to L'$ such that $M' = T\circ M$ . This space $L$ is unique up to isomorphism and we define $\bigwedge^k V := L$ and write $v_1 \wedge v_2 \wedge \dots \wedge v_k:= M(v_1,v_2,\dots,v_k)$ . From the above construction, all the usual properties of $\bigwedge^k V$ follow, e.g. if $\{ e_1, \dots , e_n \}$ is a basis for $V$ , then $\{ e_{i_1} \wedge \dots \wedge e_{i_k} \}_{i_1<\dots<i_k }$ is a basis for $\bigwedge^k V$ . If $U$ is a subspace of $V$ , then $\bigwedge^k U$ can be canonically identified with a subspace of $\bigwedge^k V$ via the universal property (the inclusion map $i\colon U^k\to V^k$ composes with $M$ is an alternating multilinear map from $U^k$ into $\bigwedge^k V$ ). Now, let $W$ be a vector space and $U,V$ be subspaces of $W$ . I believe it is true that $\bigwedge^k(U\cap V) = \left(\bigwedge^kU\right) \bigcap \left(\bigwedge^k V\right)$ , which should be straight forward to prove  by fixing a common basis for $U,V$ in $W$ . However, I want to know if it can be demonstrated from the perspective of category theory using the universal mapping property. It seems that this would require some characterization of $U\cap V$ in terms of morphisms but my working knowledge of techniques  from category theory has mostly faded away at this point (not that there was much of it from the beginning). Any help is highly appreciated, especially if it's accompanied by a diagram :-)","Let be a (finite dimensional) vector space, its exterior algebra of order is the vector space consisting of the formal sums of terms of the form , where each (with a few additional properties regarding ). There are quite a few ways to define rigorously, one of them is via the universal mapping property: There exists a vector space and an alternating multilinear map with the universal mapping property in the sense that for any alternating multilinear map , there exists a unique linear map such that . This space is unique up to isomorphism and we define and write . From the above construction, all the usual properties of follow, e.g. if is a basis for , then is a basis for . If is a subspace of , then can be canonically identified with a subspace of via the universal property (the inclusion map composes with is an alternating multilinear map from into ). Now, let be a vector space and be subspaces of . I believe it is true that , which should be straight forward to prove  by fixing a common basis for in . However, I want to know if it can be demonstrated from the perspective of category theory using the universal mapping property. It seems that this would require some characterization of in terms of morphisms but my working knowledge of techniques  from category theory has mostly faded away at this point (not that there was much of it from the beginning). Any help is highly appreciated, especially if it's accompanied by a diagram :-)","V k \bigwedge^k V v_1 \wedge v_2 \wedge \dots \wedge v_k v_i \in V \wedge \bigwedge^k V L M\colon V^k \to L M'\colon V^k \to L' T\colon L\to L' M' = T\circ M L \bigwedge^k V := L v_1 \wedge v_2 \wedge \dots \wedge v_k:= M(v_1,v_2,\dots,v_k) \bigwedge^k V \{ e_1, \dots , e_n \} V \{ e_{i_1} \wedge \dots \wedge e_{i_k} \}_{i_1<\dots<i_k } \bigwedge^k V U V \bigwedge^k U \bigwedge^k V i\colon U^k\to V^k M U^k \bigwedge^k V W U,V W \bigwedge^k(U\cap V) = \left(\bigwedge^kU\right) \bigcap \left(\bigwedge^k V\right) U,V W U\cap V","['linear-algebra', 'category-theory', 'multilinear-algebra', 'exterior-algebra', 'universal-property']"
56,Trace thought of as a non-degenerate symmetric bilinear form over $End_{\Bbb{R}}V$,Trace thought of as a non-degenerate symmetric bilinear form over,End_{\Bbb{R}}V,"Let $V$ be a real finite-dimensional vector space of dimension $n$ . Let $End_{\Bbb{R}}V$ be the real vector space of linear mappings from $V$ to $V$ . $End_{\Bbb{R}}V$ has dimension $n^2$ . The trace is the unique (up to scale) $Tr\in(End_{\Bbb{R}}V)^{*}$ such that $Tr(A\circ B)=Tr(B\circ A)$ for every $A,B\in End_{\Bbb{R}}V$ . Normally we take the one that verifies $Tr(Id)=n$ , where $Id$ is the identity endomorphism of $V$ . This defines a non-degenerate symmetric bilinear form: $B_{Tr}:End_{\Bbb{R}}V\times End_{\Bbb{R}}V\to \Bbb{R}$ , $B_{Tr}(A,B):=Tr(A\circ B)$ . It is a well known result that non-degenerate symmetric bilinear forms over a real finite-dimensional vector space are classified by the index and the signature , so I wonder: Is there a formula to calculate the index and the signture of $B_{Tr}$ (maybe in terms of the dimension)? In other words, how do I find the corresponding $\pm1s$ ? There are $n^2$ of them. Who is an orthonormal basis for $(End_{\Bbb{R}}V,B_{Tr})$ ? In order to solve this, does it help the fact that the orthogonal complement of the symmetric endomorphisms are the alternating ones and viceversa? For the answerers: I have no knowledge about algebraic geometry. If you are able to connect your answer with ideas of the field of differential geometry it will be appreciated (I'm teaching myself it and that's how I came up with this question). Though coordinates help to understand things, finish with free-coordinate arguments will be appreciated too. Still any help will be welcome. Thanks in advance.","Let be a real finite-dimensional vector space of dimension . Let be the real vector space of linear mappings from to . has dimension . The trace is the unique (up to scale) such that for every . Normally we take the one that verifies , where is the identity endomorphism of . This defines a non-degenerate symmetric bilinear form: , . It is a well known result that non-degenerate symmetric bilinear forms over a real finite-dimensional vector space are classified by the index and the signature , so I wonder: Is there a formula to calculate the index and the signture of (maybe in terms of the dimension)? In other words, how do I find the corresponding ? There are of them. Who is an orthonormal basis for ? In order to solve this, does it help the fact that the orthogonal complement of the symmetric endomorphisms are the alternating ones and viceversa? For the answerers: I have no knowledge about algebraic geometry. If you are able to connect your answer with ideas of the field of differential geometry it will be appreciated (I'm teaching myself it and that's how I came up with this question). Though coordinates help to understand things, finish with free-coordinate arguments will be appreciated too. Still any help will be welcome. Thanks in advance.","V n End_{\Bbb{R}}V V V End_{\Bbb{R}}V n^2 Tr\in(End_{\Bbb{R}}V)^{*} Tr(A\circ B)=Tr(B\circ A) A,B\in End_{\Bbb{R}}V Tr(Id)=n Id V B_{Tr}:End_{\Bbb{R}}V\times End_{\Bbb{R}}V\to \Bbb{R} B_{Tr}(A,B):=Tr(A\circ B) B_{Tr} \pm1s n^2 (End_{\Bbb{R}}V,B_{Tr})","['linear-algebra', 'linear-transformations', 'trace', 'bilinear-form']"
57,What is the name for the unreachable part of a function's codomain?,What is the name for the unreachable part of a function's codomain?,,"A function associates each element from a ""domain"" set/space with an element from a ""codomain"" set/space. It does not have to associate all the elements in the codomain. If it does use them all, it is ""surjective"". I would like to know, what is the name for the set/space of elements in the codomain that are not used by a non-surjective function? I am particularly interested in the context of linear mappings. The space I would like the name of is somewhat like the opposite of the nullspace.","A function associates each element from a ""domain"" set/space with an element from a ""codomain"" set/space. It does not have to associate all the elements in the codomain. If it does use them all, it is ""surjective"". I would like to know, what is the name for the set/space of elements in the codomain that are not used by a non-surjective function? I am particularly interested in the context of linear mappings. The space I would like the name of is somewhat like the opposite of the nullspace.",,"['linear-algebra', 'functions', 'notation', 'linear-transformations', 'terminology']"
58,Cyclic Vector and Diagonalizable operator,Cyclic Vector and Diagonalizable operator,,"I've been working on this exercise: Prove that a diagonalizable operator $T$ on an $n$ -dimensional vector space has a cyclic vector iff it has $n$ -distinct eigenvalues. I tried to do the following: Let $v$ be the cyclic vector of $T$ . Then $C=\{v, Tv, \dots, T^{n-1}v\}$ is basis of $V$ and since $T$ is diagonalizable there is a basis of eigenvectors, say $B=\{b_1, \dots, b_n\}$ . Now, you can write $v$ in terms of basis $B$ . And hence the other vectors of basis $C$ , that is $Tv, T^2v, \dots, T^{n-1}v$ . In the end, I believe that the result will follow of the linear independence of $\{v, Tv, \dots, T^{n-1}v\}$ , however I couldn't show how. It seems to me that contradiction could be used, assuming that there are distinct eigenvalues ​​and somehow have that the set $\{v, Tv, \dots, T^{n-1}v\}$ will not be linearly independent. Is my idea correct? Any tips to continue? If there is a simple way to solve, I would be grateful for the help.","I've been working on this exercise: Prove that a diagonalizable operator on an -dimensional vector space has a cyclic vector iff it has -distinct eigenvalues. I tried to do the following: Let be the cyclic vector of . Then is basis of and since is diagonalizable there is a basis of eigenvectors, say . Now, you can write in terms of basis . And hence the other vectors of basis , that is . In the end, I believe that the result will follow of the linear independence of , however I couldn't show how. It seems to me that contradiction could be used, assuming that there are distinct eigenvalues ​​and somehow have that the set will not be linearly independent. Is my idea correct? Any tips to continue? If there is a simple way to solve, I would be grateful for the help.","T n n v T C=\{v, Tv, \dots, T^{n-1}v\} V T B=\{b_1, \dots, b_n\} v B C Tv, T^2v, \dots, T^{n-1}v \{v, Tv, \dots, T^{n-1}v\} \{v, Tv, \dots, T^{n-1}v\}","['linear-algebra', 'eigenvalues-eigenvectors']"
59,Invariant functions for irreducible representations of $\mathrm{SU}(2)$.,Invariant functions for irreducible representations of .,\mathrm{SU}(2),"The orbits of $\mathrm{SU}(2)$ acting irreducibly on $\mathbb{C}^2$ are three-spheres centered around the origin. In other words, an orbit is uniquely specified by the Euclidean norm in $\mathbb{C}^2$ . For the irreducible representation $\rho_n$ of $\mathrm{SU}(2)$ on $\mathbb{C}^n$ ( $n\geq 2$ ), the orbits will generally be three-dimensional so I expect there are $N=2n-3$ functions $f_1,\dots,f_{N}\colon\mathbb{C}^n\to \mathbb{R}$ which are invariant under $\rho_n$ and the orbits of $\rho_n$ are equal to $\{x\in \mathbb{C}^n\mid f_i(x)=c_i\}$ for some $c_1,\dots,c_N\in \mathbb{R}$ . My question is if what I have said is correct and if so, what else can be said about the functions $f_i$ ?","The orbits of acting irreducibly on are three-spheres centered around the origin. In other words, an orbit is uniquely specified by the Euclidean norm in . For the irreducible representation of on ( ), the orbits will generally be three-dimensional so I expect there are functions which are invariant under and the orbits of are equal to for some . My question is if what I have said is correct and if so, what else can be said about the functions ?","\mathrm{SU}(2) \mathbb{C}^2 \mathbb{C}^2 \rho_n \mathrm{SU}(2) \mathbb{C}^n n\geq 2 N=2n-3 f_1,\dots,f_{N}\colon\mathbb{C}^n\to \mathbb{R} \rho_n \rho_n \{x\in \mathbb{C}^n\mid f_i(x)=c_i\} c_1,\dots,c_N\in \mathbb{R} f_i","['linear-algebra', 'algebraic-geometry', 'representation-theory', 'algebraic-groups', 'invariant-theory']"
60,Basis Extension Theorem,Basis Extension Theorem,,"I know that using basis extension theorem we can extend a set of LI vector to the basis of vector space, but how we actually do it? I have only theoretically used this theorem in proof, how to actually extend a given set of LI vectors of a vector space to its basis? Thanks.","I know that using basis extension theorem we can extend a set of LI vector to the basis of vector space, but how we actually do it? I have only theoretically used this theorem in proof, how to actually extend a given set of LI vectors of a vector space to its basis? Thanks.",,"['linear-algebra', 'vector-spaces']"
61,Properties of the Riesz projector,Properties of the Riesz projector,,"We have a hilbert space $X$ , a continuous and linear function $A:X\to X$ and an eigenvalue $\lambda_0$ of $A$ . Furthermore, we have an arbitrary positive oriented closed jordan curve $\Gamma$ in $\mathbb{C}$ . Let the operator $A-\lambda\, \text{Id}$ be continuously invertible inside the curve for all $\lambda$ except at $\lambda=\lambda_0$ and $\textbf{assume}$ that $\lambda_0$ lies in the inside of $\Gamma$ . Then, we want to show that the operator $P_{\lambda_0} :X\to X$ defined by $$ P_{\lambda_0} := -\frac{1}{2\pi i} \int_\Gamma (A-\lambda \, \text{Id})^{-1}\, d\lambda $$ is continuous and linear with closed range (range means $P_{\lambda_0}(X)$ ) and independent of the choice of $\Gamma$ (if it fulfills the same properties). We should also show, that it is a projection and that the eigenspace of $\lambda_0$ is in the range of $P_{\lambda_0}$ .","We have a hilbert space , a continuous and linear function and an eigenvalue of . Furthermore, we have an arbitrary positive oriented closed jordan curve in . Let the operator be continuously invertible inside the curve for all except at and that lies in the inside of . Then, we want to show that the operator defined by is continuous and linear with closed range (range means ) and independent of the choice of (if it fulfills the same properties). We should also show, that it is a projection and that the eigenspace of is in the range of .","X A:X\to X \lambda_0 A \Gamma \mathbb{C} A-\lambda\, \text{Id} \lambda \lambda=\lambda_0 \textbf{assume} \lambda_0 \Gamma P_{\lambda_0} :X\to X 
P_{\lambda_0} := -\frac{1}{2\pi i} \int_\Gamma (A-\lambda \, \text{Id})^{-1}\, d\lambda
 P_{\lambda_0}(X) \Gamma \lambda_0 P_{\lambda_0}","['linear-algebra', 'complex-analysis', 'hilbert-spaces']"
62,What is the best book to learn probability theory through a linear algebra perspective?,What is the best book to learn probability theory through a linear algebra perspective?,,"I have heard from my linear algebra professor in undergraduate studies that probability theory can be examined using linear algebra. As a math student who enjoys linear algebra, does anyone have a good textbook that uses linear algebra to examine probability theory?","I have heard from my linear algebra professor in undergraduate studies that probability theory can be examined using linear algebra. As a math student who enjoys linear algebra, does anyone have a good textbook that uses linear algebra to examine probability theory?",,"['linear-algebra', 'probability', 'matrices', 'probability-theory', 'reference-request']"
63,"Why is $\operatorname{Hom}_\Bbb{Z}(A,A)$ a ring?",Why is  a ring?,"\operatorname{Hom}_\Bbb{Z}(A,A)","I am trying to prove and understand why the homomorphisms from an abelian group A to itself $\operatorname{Hom}_\Bbb{Z}(A,A)$ is a ring. Is my reasoning correct in the following proof? Show the homomorphisms from an abelian group A to itself $\text{Hom}_{\mathbb{Z}}(A,A)$ form a ring. For two homomorphisms $\delta_1\ \colon V\to V$ and $\delta_2\ \colon V\to V$ $\in \text{Hom}_{\mathbb{Z}}(A,A)$ define their additive and multiplicative binary operataions as $[\delta_1+\delta_2](x)=\delta_1(x)+\delta_2(x)$ and $\delta_1 * \delta_2=\delta_1 \circ \delta_2$ respectively. Additive Identity This would be the zero map so fix $\delta_1\in \text{Hom}_\mathbb{Z}(A,A)$ then $[\delta_1 + 0]=\delta_1$ $[0+\delta_1]=\delta_1$ Additive Associativity Addition of functions is associative. Inverse Suppose we have a homomorphism $\delta$ in $\text{Hom}_{\mathbb{Z}}(A,A)$ . Define another homomorphism from A to itself as $\delta^{-1}(v):=(\delta(v))^{-1}$ which exists because the image of A under $\delta$ is a subgroup of A (well known theorem of Homomorphisms). Now we must show that the composition of these functions yields the zero function which is the identity of the Abelian group. Let $v$ be an arbitrary element in $A$ . Then $\delta(v)+\delta^{-1}(v)=\delta(v)+\delta(v^{-1})=\delta(v+v^{-1})=\delta(0)=0$ with the last equlaity following from the fact that $\delta$ is a group homomorphism. Multiplicative Associativity Composition of functions is associative. Multiplicative Identity This would be the identity homomorphism so fix $\delta_1 \in \text{Hom}_{\mathbb{Z}}(A,A)$ then $(\delta_1 + 1)=(\delta_1 \circ 1)=\delta_1$ . Similarly, $(1 + \delta_1)=(1 \circ \delta_1)=\delta_1$ Multiplicative Distributive Laws Fix three homomorphisms $\delta_1, \delta_2, \delta_3$ . $\delta_1 * (\delta_2 + \delta_3) (x)= \delta_1 \circ (\delta_2 + \delta_3) (x) =\delta_1((\delta_2 + \delta_3)(x))=\delta_1(\delta_2(x) + \delta_3(x))= \delta_1(\delta_2(x)) + \delta_1(\delta_3(x))=(\delta_1*\delta_2 + \delta_1*\delta_3)$ Using the fact that $\delta_1$ is a homomorphism.","I am trying to prove and understand why the homomorphisms from an abelian group A to itself is a ring. Is my reasoning correct in the following proof? Show the homomorphisms from an abelian group A to itself form a ring. For two homomorphisms and define their additive and multiplicative binary operataions as and respectively. Additive Identity This would be the zero map so fix then Additive Associativity Addition of functions is associative. Inverse Suppose we have a homomorphism in . Define another homomorphism from A to itself as which exists because the image of A under is a subgroup of A (well known theorem of Homomorphisms). Now we must show that the composition of these functions yields the zero function which is the identity of the Abelian group. Let be an arbitrary element in . Then with the last equlaity following from the fact that is a group homomorphism. Multiplicative Associativity Composition of functions is associative. Multiplicative Identity This would be the identity homomorphism so fix then . Similarly, Multiplicative Distributive Laws Fix three homomorphisms . Using the fact that is a homomorphism.","\operatorname{Hom}_\Bbb{Z}(A,A) \text{Hom}_{\mathbb{Z}}(A,A) \delta_1\ \colon V\to V \delta_2\ \colon V\to V \in \text{Hom}_{\mathbb{Z}}(A,A) [\delta_1+\delta_2](x)=\delta_1(x)+\delta_2(x) \delta_1 * \delta_2=\delta_1 \circ \delta_2 \delta_1\in \text{Hom}_\mathbb{Z}(A,A) [\delta_1 + 0]=\delta_1 [0+\delta_1]=\delta_1 \delta \text{Hom}_{\mathbb{Z}}(A,A) \delta^{-1}(v):=(\delta(v))^{-1} \delta v A \delta(v)+\delta^{-1}(v)=\delta(v)+\delta(v^{-1})=\delta(v+v^{-1})=\delta(0)=0 \delta \delta_1 \in \text{Hom}_{\mathbb{Z}}(A,A) (\delta_1 + 1)=(\delta_1 \circ 1)=\delta_1 (1 + \delta_1)=(1 \circ \delta_1)=\delta_1 \delta_1, \delta_2, \delta_3 \delta_1 * (\delta_2 + \delta_3) (x)= \delta_1 \circ (\delta_2 + \delta_3) (x) =\delta_1((\delta_2 + \delta_3)(x))=\delta_1(\delta_2(x) + \delta_3(x))=
\delta_1(\delta_2(x)) + \delta_1(\delta_3(x))=(\delta_1*\delta_2 + \delta_1*\delta_3) \delta_1","['linear-algebra', 'abstract-algebra', 'group-theory', 'ring-theory', 'category-theory']"
64,What are the eigenvalues of $(a_m+a_n)^2$?,What are the eigenvalues of ?,(a_m+a_n)^2,"Let $a_m\in \mathbb{R}$ and consider the matrix defined by $A_{mn} = (a_m+a_n)^2$ . Using python and inputting a random vector $a = (a_n:a_n\in \mathbb{R})$ of various length and various values, the eigenvalues of $A$ always seem to consist of 1 negative value and 2 positive values. All the other eigenvalues are $0$ . Based on this, it seems that there should be a formula which determines the 3 nontrivial eigenvalues of $A$ (and possibly all the eigenvectors of $A$ ), but I can't think of a rigorous proof. EDIT : If $a_n$ had some ""symmetry"" so that $\sum a_n =\sum a_n^3=0$ , then it's easy to see that $a_n$ is an eigenvector and that $a_n^2 \pm c$ are eigenvectors where $$ c^2 = \frac{1}{N}\sum a_n^4 $$ However, the statement seems to be more general. Also, I'm not sure why all the other eigenvalues are 0.","Let and consider the matrix defined by . Using python and inputting a random vector of various length and various values, the eigenvalues of always seem to consist of 1 negative value and 2 positive values. All the other eigenvalues are . Based on this, it seems that there should be a formula which determines the 3 nontrivial eigenvalues of (and possibly all the eigenvectors of ), but I can't think of a rigorous proof. EDIT : If had some ""symmetry"" so that , then it's easy to see that is an eigenvector and that are eigenvectors where However, the statement seems to be more general. Also, I'm not sure why all the other eigenvalues are 0.","a_m\in \mathbb{R} A_{mn} = (a_m+a_n)^2 a = (a_n:a_n\in \mathbb{R}) A 0 A A a_n \sum a_n =\sum a_n^3=0 a_n a_n^2 \pm c 
c^2 = \frac{1}{N}\sum a_n^4
","['linear-algebra', 'diagonalization']"
65,Some questions about determinants,Some questions about determinants,,"I'm studying some Linear Algebra and determinants are being a real struggle for me. I can't understand them at all, but my biggest problem is with all the equivalent definitions and why they are in fact equivalent. Given $V, W$ vector spaces, we define the tensor product $\otimes$ of $f \colon V^k \to \mathbb{R}$ and $g \colon W^l \to \mathbb{R}$ as $(f \otimes g )(v_1, \dots, v_{k+l})$ $= f(v_1, \dots, v_k)g(v_{k+1}, \dots, v_{k+l})$ . Also, we define $V \otimes W = \langle v \otimes w \colon v \in V, w \in W \rangle$ (note that the tensor product of vectors is defined, since $V = V^{**}$ , which is a space of functions). Given $V$ a vector space, we define $X = V^{\otimes k} = V \otimes V \otimes \dots \otimes V$ ( $k$ times) and $Y =$ $\langle v_1 \otimes \dots \otimes v_k \colon \exists i,j \textrm{ } v_i = v_j \rangle$ (i.e. the set generated by the tensor product of $k$ vectors in which at least two are equal). We define $\wedge^k V = X/Y$ (call it exterior $k-$ power of $V$ ) and denote $\overline{u_1 \otimes \dots \otimes u_k} = u_1 \wedge \dots \wedge u_k$ . Given those important definitions of tensor product and exterior power of a space, let's introduce the definitions of determinant. Given $V$ a vector space with $\dim V = n$ and $T\colon V \to V$ a linear transformation, note that $\dim \wedge^n V = 1$ . Hence, the linear transformation $\wedge^n T \colon \wedge^n V \to \wedge^n V$ must be of the form $\wedge^n T(v) = cv$ for some constant $c$ . This constant is called the determinant of $T$ and denoted $c = \det T$ . The first thing that bothers me here is the assumption that $\wedge^n T(v) = cv$ for some constant $c$ , why is that? It seems trivial but I'm not getting it. The usual way we see determinants (in high school) is by beeing introduced to determinants of matrices, not of linear operators, but we can translate this definition to matrix just by saying that the determinant of $T$ is the determinant of it's matrix in some basis. My first question is: why is this definition independent of the chosen basis? Given a matrix $M = [m_{ij}]_{n \times n}$ we define its determinant $$\det T = \sum_{\sigma \in S_n} \textrm{sgn}(\sigma) m_{i\sigma(1)}\cdots m_{i\sigma(n)}.$$ Now given both definitions, why are those equivalent? Thanks in advance and if there is anything I can do to make this question better, let me know please!","I'm studying some Linear Algebra and determinants are being a real struggle for me. I can't understand them at all, but my biggest problem is with all the equivalent definitions and why they are in fact equivalent. Given vector spaces, we define the tensor product of and as . Also, we define (note that the tensor product of vectors is defined, since , which is a space of functions). Given a vector space, we define ( times) and (i.e. the set generated by the tensor product of vectors in which at least two are equal). We define (call it exterior power of ) and denote . Given those important definitions of tensor product and exterior power of a space, let's introduce the definitions of determinant. Given a vector space with and a linear transformation, note that . Hence, the linear transformation must be of the form for some constant . This constant is called the determinant of and denoted . The first thing that bothers me here is the assumption that for some constant , why is that? It seems trivial but I'm not getting it. The usual way we see determinants (in high school) is by beeing introduced to determinants of matrices, not of linear operators, but we can translate this definition to matrix just by saying that the determinant of is the determinant of it's matrix in some basis. My first question is: why is this definition independent of the chosen basis? Given a matrix we define its determinant Now given both definitions, why are those equivalent? Thanks in advance and if there is anything I can do to make this question better, let me know please!","V, W \otimes f \colon V^k \to \mathbb{R} g \colon W^l \to \mathbb{R} (f \otimes g )(v_1, \dots, v_{k+l}) = f(v_1, \dots, v_k)g(v_{k+1}, \dots, v_{k+l}) V \otimes W = \langle v \otimes w \colon v \in V, w \in W \rangle V = V^{**} V X = V^{\otimes k} = V \otimes V \otimes \dots \otimes V k Y = \langle v_1 \otimes \dots \otimes v_k \colon \exists i,j \textrm{ } v_i = v_j \rangle k \wedge^k V = X/Y k- V \overline{u_1 \otimes \dots \otimes u_k} = u_1 \wedge \dots \wedge u_k V \dim V = n T\colon V \to V \dim \wedge^n V = 1 \wedge^n T \colon \wedge^n V \to \wedge^n V \wedge^n T(v) = cv c T c = \det T \wedge^n T(v) = cv c T M = [m_{ij}]_{n \times n} \det T = \sum_{\sigma \in S_n} \textrm{sgn}(\sigma) m_{i\sigma(1)}\cdots m_{i\sigma(n)}.","['linear-algebra', 'determinant', 'tensor-products', 'exterior-algebra']"
66,basis for null space of matrix in a certain field,basis for null space of matrix in a certain field,,"Let $G=(V,E)$ be a connected graph. Let $T$ be a spanning tree of $G$ . Fix a $2$ -element field $\mathbb{F}$ (the elements are $\{0,1\}$ ). Let $S$ be an incidence matrix of $G$ in $\mathbb{F}$ . Find a basis $A$ for the null space of $S$ so that for each edge $e\in E\backslash E(T),$ there is exactly one vector $w \in A$ for which $\mathbb{1}_w(e) = 1$ (here $\mathbb{1}_w(e)$ is the function that is $1$ if the entry corresponding to edge $e$ of vector $w$ is $1$ and $0$ otherwise). What are the sets of edges of $G$ corresponding to elements of $A$ ? I know that the null space of $S$ has dimension $|E | - |E(T)|.$ However, I'm not sure how to find a basis for the null space. It might be useful to figure out what the left null space and row space of $S$ are, but again I'm not sure how to do that. I also think that a submatrix $F$ of $B$ 's columns has linearly dependent columns iff $F$ contains a cycle. I'm not sure if an inductive argument might be useful, seeing as induction seems to be used quite often for proofs in combinatorics.","Let be a connected graph. Let be a spanning tree of . Fix a -element field (the elements are ). Let be an incidence matrix of in . Find a basis for the null space of so that for each edge there is exactly one vector for which (here is the function that is if the entry corresponding to edge of vector is and otherwise). What are the sets of edges of corresponding to elements of ? I know that the null space of has dimension However, I'm not sure how to find a basis for the null space. It might be useful to figure out what the left null space and row space of are, but again I'm not sure how to do that. I also think that a submatrix of 's columns has linearly dependent columns iff contains a cycle. I'm not sure if an inductive argument might be useful, seeing as induction seems to be used quite often for proofs in combinatorics.","G=(V,E) T G 2 \mathbb{F} \{0,1\} S G \mathbb{F} A S e\in E\backslash E(T), w \in A \mathbb{1}_w(e) = 1 \mathbb{1}_w(e) 1 e w 1 0 G A S |E | - |E(T)|. S F B F","['linear-algebra', 'matrices', 'graph-theory', 'induction', 'algebraic-graph-theory']"
67,What is the number of ring isomorphisms from $\mathbb{Z}^n$ to $\mathbb{Z}^n$.,What is the number of ring isomorphisms from  to .,\mathbb{Z}^n \mathbb{Z}^n,"Yesterday, I faced the problem : what is the number of ring isomorphisms from $\mathbb{Z}^2$ to $\mathbb{Z}^2$ . And I got $6$ . After then, I found that if $n=3$ , the number is $3! \times 29$ . Suddenly, I wondered what is the number of isomorphisms from $\mathbb{Z}^n$ to $\mathbb{Z}^n$ . I have tried to solve it but failed. What's more, I couldn't find any answers about that on google. Is it easy to solve? I tried to solve the problem by using recursive relation. But it looks difficult. One of my friends gave me a suggestion to find the number of $n \times n$ invertible matrices with components $1$ or $0$ . At first, we thought that it is exactly $(2^n-1)(2^n-2) \dots (2^n-2^{n-1})$ . But this equation doesn't work because the construction of the equation discards some cases. For example, $n=3$ , the equation discards $ (\begin{smallmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \end{smallmatrix} )$ . The equation only works on a finite field. I found that it is a lower bound and upper bound $((n+1)!)^2 2^\frac{n(n+1)}{2}$ by a recursive relation. But I don't know until now what is the exact number of that. + Due to Perter Franek, I solved it. The answer is $n!$ . Idempotent elements should go idempotent elements and if $\phi$ is an isomorhpism, then $\phi(0)$ implies $0$ . The candidates of $e_i=(\delta_{ij})_j$ are each or sum of $\{e_i\}$ 's. But if some $e_i$ goes to sum of $e_i's$ , $0=\phi(e_i e_j)= \phi(e_i) \phi(e_j) \not =0$ for some $i \not = j$ .","Yesterday, I faced the problem : what is the number of ring isomorphisms from to . And I got . After then, I found that if , the number is . Suddenly, I wondered what is the number of isomorphisms from to . I have tried to solve it but failed. What's more, I couldn't find any answers about that on google. Is it easy to solve? I tried to solve the problem by using recursive relation. But it looks difficult. One of my friends gave me a suggestion to find the number of invertible matrices with components or . At first, we thought that it is exactly . But this equation doesn't work because the construction of the equation discards some cases. For example, , the equation discards . The equation only works on a finite field. I found that it is a lower bound and upper bound by a recursive relation. But I don't know until now what is the exact number of that. + Due to Perter Franek, I solved it. The answer is . Idempotent elements should go idempotent elements and if is an isomorhpism, then implies . The candidates of are each or sum of 's. But if some goes to sum of , for some .",\mathbb{Z}^2 \mathbb{Z}^2 6 n=3 3! \times 29 \mathbb{Z}^n \mathbb{Z}^n n \times n 1 0 (2^n-1)(2^n-2) \dots (2^n-2^{n-1}) n=3  (\begin{smallmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \end{smallmatrix} ) ((n+1)!)^2 2^\frac{n(n+1)}{2} n! \phi \phi(0) 0 e_i=(\delta_{ij})_j \{e_i\} e_i e_i's 0=\phi(e_i e_j)= \phi(e_i) \phi(e_j) \not =0 i \not = j,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'matrices']"
68,Solving a (fun!) coequalizer problem for $\mathrm{SL}_n(\mathbb{R})\rightarrow\mathrm{SL}_n(\mathbb{C})$ in $\mathbf{Grp}$,Solving a (fun!) coequalizer problem for  in,\mathrm{SL}_n(\mathbb{R})\rightarrow\mathrm{SL}_n(\mathbb{C}) \mathbf{Grp},"First off, the problem posed below is mostly arbitrary; it's just for my own education. (And maybe for yours, as well.) It's fairly clear to me what the (co)equalizers of abelian groups in $\mathbf{Grp}$ are, but it's less clear what those mean for non-abelian groups. So, I came up with a problem that seems non-trivial and interesting. I'm trying to coequalize $f,g:\mathrm{SL}_n(\mathbb{R})\rightrightarrows\mathrm{SL}_n(\mathbb{C})$ , where $f(A)=A$ $g(A)=(A^*)^{-1}$ (Both purposely not surjective.) To solve this, we need to find ""the best"" $l:\mathrm{SL}_n(\mathbb{C})\rightarrow L$ . For now, I'll settle for any $L$ that isn't $\{0\}$ . The images of both $f$ and $g$ are $\mathrm{SL}_n(\mathbb{R})\subset\mathrm{SL}_n(\mathbb{C})$ , so to start with I'll just look at that part of the domain of $l$ . $l(A^*)=l(A^{-1})$ , based on $f$ and $g$ . (Again, just on $\mathrm{SL}_n(\mathbb{R})$ for now.) $l(AA^*)=l(A^*A)=e_L$ , following from the statement above, and $l$ being a homomorphism. Since $AA^*$ and $A^*A$ are positive-definite Hermitian (PDH), and PDH have Cholesky decompositions resembling $AA^*$ , we can more generally say that $l(B)=e_L$ when $B$ is PDH. (Extending $l$ to $\mathrm{SL}_n(\mathbb{C})$ .) This also means that $l(D)=e_L$ when $D$ is diagonal with positive entries. For any $A\in\mathrm{SL}_n(\mathbb{C})$ , we can create an SVD $A=U\Sigma V^*$ , with unitary $U$ and $V$ , and $U,\Sigma,V\in\mathrm{SL}_n(\mathbb{C})$ . Since $l(\Sigma)=e_L$ , $l(A)=l(UV^*)$ . ( $UV^*$ should be unique, since $A$ is of full rank.) If $A$ is unitary, it can be diagonalized as $A=VDV^*$ for unitary $V$ and diagonal $D$ . Importantly , $D$ should only be in the kernel of $l$ if it only has positive (real) values, which is only true for $I$ . So it seems like $L$ is (at most) isomorphic to $\mathrm{SU}(n)$ , with $l(A)$ taking $A$ to an equivalence class based on its rotation action after removing any distortion it makes. Does that sound accurate and/or reasonable? (For example, maybe a matrix with a non-real determinant can sneak in when removing $\Sigma$ , thereby breaking $\mathrm{SL}_n(\mathbb{C})$ .) I spent several hours going through this, and I changed my conclusion about 5 times. The last few times were while proofreading. Whether or not my answer above is correct, I'd appreciate any pointers regarding shortcuts I could have taken, etc.","First off, the problem posed below is mostly arbitrary; it's just for my own education. (And maybe for yours, as well.) It's fairly clear to me what the (co)equalizers of abelian groups in are, but it's less clear what those mean for non-abelian groups. So, I came up with a problem that seems non-trivial and interesting. I'm trying to coequalize , where (Both purposely not surjective.) To solve this, we need to find ""the best"" . For now, I'll settle for any that isn't . The images of both and are , so to start with I'll just look at that part of the domain of . , based on and . (Again, just on for now.) , following from the statement above, and being a homomorphism. Since and are positive-definite Hermitian (PDH), and PDH have Cholesky decompositions resembling , we can more generally say that when is PDH. (Extending to .) This also means that when is diagonal with positive entries. For any , we can create an SVD , with unitary and , and . Since , . ( should be unique, since is of full rank.) If is unitary, it can be diagonalized as for unitary and diagonal . Importantly , should only be in the kernel of if it only has positive (real) values, which is only true for . So it seems like is (at most) isomorphic to , with taking to an equivalence class based on its rotation action after removing any distortion it makes. Does that sound accurate and/or reasonable? (For example, maybe a matrix with a non-real determinant can sneak in when removing , thereby breaking .) I spent several hours going through this, and I changed my conclusion about 5 times. The last few times were while proofreading. Whether or not my answer above is correct, I'd appreciate any pointers regarding shortcuts I could have taken, etc.","\mathbf{Grp} f,g:\mathrm{SL}_n(\mathbb{R})\rightrightarrows\mathrm{SL}_n(\mathbb{C}) f(A)=A g(A)=(A^*)^{-1} l:\mathrm{SL}_n(\mathbb{C})\rightarrow L L \{0\} f g \mathrm{SL}_n(\mathbb{R})\subset\mathrm{SL}_n(\mathbb{C}) l l(A^*)=l(A^{-1}) f g \mathrm{SL}_n(\mathbb{R}) l(AA^*)=l(A^*A)=e_L l AA^* A^*A AA^* l(B)=e_L B l \mathrm{SL}_n(\mathbb{C}) l(D)=e_L D A\in\mathrm{SL}_n(\mathbb{C}) A=U\Sigma V^* U V U,\Sigma,V\in\mathrm{SL}_n(\mathbb{C}) l(\Sigma)=e_L l(A)=l(UV^*) UV^* A A A=VDV^* V D D l I L \mathrm{SU}(n) l(A) A \Sigma \mathrm{SL}_n(\mathbb{C})","['linear-algebra', 'group-theory', 'category-theory', 'linear-transformations', 'limits-colimits']"
69,Is the polar decomposition of a product the product of polar decompositions?,Is the polar decomposition of a product the product of polar decompositions?,,"Given the polar decompositions $A=O_1P_1$ , $B=O_2P_2$ , and $AB=O_3P_3$ with $O_i$ being orthogonal matrices and $P_i$ being symmetric positive definite matrices, is there a nice way to write $O_3$ in terms of $O_1$ and $O_2$ (as well as write $P_3$ in terms of $P_1$ and $P_2$ )?","Given the polar decompositions , , and with being orthogonal matrices and being symmetric positive definite matrices, is there a nice way to write in terms of and (as well as write in terms of and )?",A=O_1P_1 B=O_2P_2 AB=O_3P_3 O_i P_i O_3 O_1 O_2 P_3 P_1 P_2,"['linear-algebra', 'matrices', 'matrix-decomposition']"
70,Is there a specific symbol for denoting a linear subspace like $\subseteq$ for denoting a subset?,Is there a specific symbol for denoting a linear subspace like  for denoting a subset?,\subseteq,"I was under the impression that this was a proper notation, but I was just corrected on this but did not get a proper explanation of what it should be. I get that it is definitely not the same, but what is the right symbol then? So I am specifically asking for a linear subspace. Or does this not make any difference?","I was under the impression that this was a proper notation, but I was just corrected on this but did not get a proper explanation of what it should be. I get that it is definitely not the same, but what is the right symbol then? So I am specifically asking for a linear subspace. Or does this not make any difference?",,"['linear-algebra', 'notation']"
71,Trouble understanding Weyl's unitary trick,Trouble understanding Weyl's unitary trick,,"Given a representation of a finite group: $\rho : G \to \text{GL}(V)$ , there exists a unitary representation $\tau$ which is isomorphic to $\rho$ . I came across Weyl's trick, where you redefine the inner product as follows $$\langle v,w \rangle = \frac1{|G|}\sum_{g \in G} \langle \rho(g)v, \rho(g)w \rangle_0,$$ where $\langle\ ,\rangle_0$ is the usual inner product. I see the idea of this trick since this new inner product is preserved under $\rho$ i.e. $\langle v,w \rangle = \langle \rho(g')v,\rho(g')w \rangle$ for all $g'\in G$ . I am not familiar with the idea of changing the definition of the inner product to achieve unitarity. As I understand Weyl's trick, the matrices in the set $\{\rho(g):g\in G\}$ and $\{\tau(g):g\in G\}$ are actually identical but the inner product definition is what makes the $\tau(g)$ matrices unitary? Is there a way to translate this back to the usual picture where I use the standard inner product (and therefore a fixed definition of unitarity) and the matrices of the different representations are not identical? Sorry if there is some vagueness/incorrectness in the question - I am new to group theory.","Given a representation of a finite group: , there exists a unitary representation which is isomorphic to . I came across Weyl's trick, where you redefine the inner product as follows where is the usual inner product. I see the idea of this trick since this new inner product is preserved under i.e. for all . I am not familiar with the idea of changing the definition of the inner product to achieve unitarity. As I understand Weyl's trick, the matrices in the set and are actually identical but the inner product definition is what makes the matrices unitary? Is there a way to translate this back to the usual picture where I use the standard inner product (and therefore a fixed definition of unitarity) and the matrices of the different representations are not identical? Sorry if there is some vagueness/incorrectness in the question - I am new to group theory.","\rho : G \to \text{GL}(V) \tau \rho \langle v,w \rangle = \frac1{|G|}\sum_{g \in G} \langle \rho(g)v, \rho(g)w \rangle_0, \langle\ ,\rangle_0 \rho \langle v,w \rangle = \langle \rho(g')v,\rho(g')w \rangle g'\in G \{\rho(g):g\in G\} \{\tau(g):g\in G\} \tau(g)","['linear-algebra', 'group-theory', 'representation-theory', 'unitary-matrices']"
72,Show that a real symmetric matrix is always diagonalizable,Show that a real symmetric matrix is always diagonalizable,,"Let $A \in \Bbb R^{n \times n}$ be a symmetric matrix and let $\lambda \in \Bbb R$ be an eigenvalue of $A$ . Prove that the geometric multiplicity $g(\lambda)$ of $A$ equals its algebraic multiplicity $a(\lambda)$ . We know that if $A$ is diagonalizable then $g(\lambda)=a(\lambda)$ . So all we have to show is that $A$ is diagonalizable. I found a proof by contradiction. Assuming $A$ is not diagonalizable we have $$(A- \lambda_i I)^2 v=0, \  (A- \lambda_i I) v \neq 0,$$ where $\lambda_i$ is some repeated eigenvalue. Then $$0=v^{\dagger}(A-\lambda_i I)^2v=v^{\dagger}(A-\lambda_i I)(A-\lambda_i I) \neq 0$$ which is a contradiction (where $\dagger$ stands for conjugate transpose). OK but isn't there a better proof? I see it could be approached by the Spectral theorem or Gram Schmidt Prove that real symmetric matrix is diagonalizable . A hint for how to do so would be appreciated.",Let be a symmetric matrix and let be an eigenvalue of . Prove that the geometric multiplicity of equals its algebraic multiplicity . We know that if is diagonalizable then . So all we have to show is that is diagonalizable. I found a proof by contradiction. Assuming is not diagonalizable we have where is some repeated eigenvalue. Then which is a contradiction (where stands for conjugate transpose). OK but isn't there a better proof? I see it could be approached by the Spectral theorem or Gram Schmidt Prove that real symmetric matrix is diagonalizable . A hint for how to do so would be appreciated.,"A \in \Bbb R^{n \times n} \lambda \in \Bbb R A g(\lambda) A a(\lambda) A g(\lambda)=a(\lambda) A A (A- \lambda_i I)^2 v=0, \  (A- \lambda_i I) v \neq 0, \lambda_i 0=v^{\dagger}(A-\lambda_i I)^2v=v^{\dagger}(A-\lambda_i I)(A-\lambda_i I) \neq 0 \dagger","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'symmetric-matrices']"
73,What is the range of $\vec{z}^{ \mathrm{ T } }A\vec{z} $?,What is the range of ?,\vec{z}^{ \mathrm{ T } }A\vec{z} ,"Let A be a 3 by 3 matrix $$\begin{pmatrix} 1 & -2  & -1\\ -2 & 1 & 1 \\ -1 & 1 & 4 \end{pmatrix}$$ Then we have a real-number vector $\vec{ z }= \left(   \begin{array}{c}     z_1 \\     z_2 \\     z_3   \end{array} \right)$ such that $$\vec{z}^{ \mathrm{ T } }\vec{z} = 1$$ $$z_1+z_2+z_3=1$$ What is the range of $\vec{z}^{ \mathrm{ T } }A\vec{z} $ ? I have found that $A$ 's eigenvalues are -1,2, and 5 and eigenvectors are $\left(   \begin{array}{c}     1 \\     1 \\     0   \end{array} \right)$$\left(   \begin{array}{c}     1 \\     -1 \\     1   \end{array} \right)$$\left(   \begin{array}{c}     -1 \\     1 \\     2   \end{array} \right)$ for each. Can anyone help me?","Let A be a 3 by 3 matrix Then we have a real-number vector such that What is the range of ? I have found that 's eigenvalues are -1,2, and 5 and eigenvectors are for each. Can anyone help me?","\begin{pmatrix}
1 & -2  & -1\\
-2 & 1 & 1 \\
-1 & 1 & 4
\end{pmatrix} \vec{ z }= \left(
  \begin{array}{c}
    z_1 \\
    z_2 \\
    z_3
  \end{array}
\right) \vec{z}^{ \mathrm{ T } }\vec{z} = 1 z_1+z_2+z_3=1 \vec{z}^{ \mathrm{ T } }A\vec{z}  A \left(
  \begin{array}{c}
    1 \\
    1 \\
    0
  \end{array}
\right)\left(
  \begin{array}{c}
    1 \\
    -1 \\
    1
  \end{array}
\right)\left(
  \begin{array}{c}
    -1 \\
    1 \\
    2
  \end{array}
\right)","['linear-algebra', 'optimization', 'non-convex-optimization', 'qcqp']"
74,Linear function preserving the Gram determinant,Linear function preserving the Gram determinant,,"In Euclidean space $X$ the Gram's  determinant of a system of vectors $x_1,...,x_k\in X$ is called the determinant of $k\times k$ matrix $ [\langle x_i,x_j \rangle]$ : $ G(x_1,..,x_k)=\det[\langle x_i,x_j \rangle]. $ In $n$ dimensional Euclidean space $X$ , let $f: X\rightarrow X$ be a linear mapping and let $k\in \{2,...,n-1\}$ be a fixed number. I wish to prove not using exterior algebra that if $ G(f(x_1),...,f(x_k))=G(x_1,...,x_k) $ for each $x_1,...,x_k\in X$ , then $f$ is orthogonal mapping. Thanks","In Euclidean space the Gram's  determinant of a system of vectors is called the determinant of matrix : In dimensional Euclidean space , let be a linear mapping and let be a fixed number. I wish to prove not using exterior algebra that if for each , then is orthogonal mapping. Thanks","X x_1,...,x_k\in X k\times k  [\langle x_i,x_j \rangle] 
G(x_1,..,x_k)=\det[\langle x_i,x_j \rangle].
 n X f: X\rightarrow X k\in \{2,...,n-1\} 
G(f(x_1),...,f(x_k))=G(x_1,...,x_k)
 x_1,...,x_k\in X f",['linear-algebra']
75,"Find the smallest eigenvalue of $G=[ \exp(-(x_i-x_j )^2]_{i,j}$ for ${\bf x}=[x_1,\dots,x_n]$",Find the smallest eigenvalue of  for,"G=[ \exp(-(x_i-x_j )^2]_{i,j} {\bf x}=[x_1,\dots,x_n]","Consider a sequence $\{x_1,...,x_n \}$ such that $b=\max_i  |x_i|$ and $d_{\min}=\min_{ij: i \neq j} |x_i-x_j|$ .  We assume that $b<\infty$ and $d_{\min}>0$ . Can we find a non-trivial lower bound on the smallest eigenvalue of $$G=[ \exp(-(x_i-x_j )^2)]_{i=1..n,j=1..n}$$ We want this lower bound to depend on some property of this sequence. I was thinking of writing it as \begin{align} u^T G u =\sum_i \sum_j  u_i u_j   \exp(-(x_i-x_j )^2) \end{align} and showing a lower bound that holds for all $(u_i,u_j)$ . We have the following bounds on each entry $$\exp(-d_{\min}^2) \ge \exp(-(x_i-x_j )^2) \ge  \exp(-4 b^2).$$ However, I don't know how to combine these two steps. Note that we know that $G$ is positive definite. This follows since $\exp(-t^2)$ is a positive definite kernel.","Consider a sequence such that and .  We assume that and . Can we find a non-trivial lower bound on the smallest eigenvalue of We want this lower bound to depend on some property of this sequence. I was thinking of writing it as and showing a lower bound that holds for all . We have the following bounds on each entry However, I don't know how to combine these two steps. Note that we know that is positive definite. This follows since is a positive definite kernel.","\{x_1,...,x_n \} b=\max_i  |x_i| d_{\min}=\min_{ij: i \neq j} |x_i-x_j| b<\infty d_{\min}>0 G=[ \exp(-(x_i-x_j )^2)]_{i=1..n,j=1..n} \begin{align}
u^T G u =\sum_i \sum_j  u_i u_j   \exp(-(x_i-x_j )^2)
\end{align} (u_i,u_j) \exp(-d_{\min}^2) \ge \exp(-(x_i-x_j )^2) \ge  \exp(-4 b^2). G \exp(-t^2)","['linear-algebra', 'eigenvalues-eigenvectors', 'positive-definite']"
76,Let $\lambda$ be a real eigenvalue of matrix $AB$. Prove that $|\lambda| > 1$.,Let  be a real eigenvalue of matrix . Prove that .,\lambda AB |\lambda| > 1,"Let $A$ and $B$ be real symmetric matrices with all eigenvalues strictly greater than 1. Let $\lambda$ be a real eigenvalue of matrix $AB$ . Prove that $|\lambda| > 1$ . My solution: Let $a$ and $b$ be eigenvalues of $A$ and $B$ corresponding the eigenvectors $y$ and $x$ , respectively. Looking at the following dot product: $$\langle ABx,y \rangle = \langle Bx,A^Ty \rangle=\langle Bx,Ay \rangle = \langle bx,ay \rangle=ab\langle x,y \rangle=\langle abx,y \rangle$$ we get $$(AB)x=(ab)x$$ Therefore, $\lambda := ab$ is an eigenvalue of $AB$ . Since $a>1$ and $b>1$ , it follows that $\lambda > 1$ However, it doesn't seem ok as the problem was actually asking to prove $|\lambda|>1$ . Indeed, $\lambda > 1 \implies |\lambda|>1$ , but then the problem wouldn't write $|\lambda|$ in my opinion. The given solution: The transforms given by $A$ and $B$ strictly increase the length of every nonzero vector, this can be seen easily on a basis where the matrix is diagonal with entries greater than $1$ in the diagonal. Hence their product $AB$ also strictly increases the length of any nonzero vector, and therefore its real eigenvalues are all greater than $1$ or less than $-1$ . Any help is appreciated.","Let and be real symmetric matrices with all eigenvalues strictly greater than 1. Let be a real eigenvalue of matrix . Prove that . My solution: Let and be eigenvalues of and corresponding the eigenvectors and , respectively. Looking at the following dot product: we get Therefore, is an eigenvalue of . Since and , it follows that However, it doesn't seem ok as the problem was actually asking to prove . Indeed, , but then the problem wouldn't write in my opinion. The given solution: The transforms given by and strictly increase the length of every nonzero vector, this can be seen easily on a basis where the matrix is diagonal with entries greater than in the diagonal. Hence their product also strictly increases the length of any nonzero vector, and therefore its real eigenvalues are all greater than or less than . Any help is appreciated.","A B \lambda AB |\lambda| > 1 a b A B y x \langle ABx,y \rangle = \langle Bx,A^Ty \rangle=\langle Bx,Ay \rangle = \langle bx,ay \rangle=ab\langle x,y \rangle=\langle abx,y \rangle (AB)x=(ab)x \lambda := ab AB a>1 b>1 \lambda > 1 |\lambda|>1 \lambda > 1 \implies |\lambda|>1 |\lambda| A B 1 AB 1 -1","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
77,Is this Linear Algebra Approach for Tangent Plane of an Ellipsoid Legal?,Is this Linear Algebra Approach for Tangent Plane of an Ellipsoid Legal?,,"In my vectorial calculus course, I was basically asked for finding a tangent plane for an ellipsoid parallel to a given plane by means of the gradient. Exactly the question was: Find the values of m for which the plane $x - 2y -2z  + m = 0$ is tangent to the ellipsoid $x^2 +4y^2 +16z^2 = 144$ I solve it using the gradient, but I thought it could be a more interesting way to solve it. Firstly, I suppose that if we were making the exact same problem with a sphere of radius 12, we could take advantage of its symmetry… Modify the scale of the coordinates to transform the ellipsoid into a sphere, by making the substitutions $x = x'$ , $ y = \frac{1}{2} y'$ , $z = \frac{1}{4}z'$ . Hence I am working in a kind of $\rm I\!R^{'3}$ instead of $\rm I\!R^3$ Apply the same exact substitution to the plane, in order to bring it to my new space, as well. Use the normalised vector normal to the plane, and find the scalar $\alpha$ for which the vector line intersects the surface of the sphere. Let's call it $v_1^{'}$ . Technically speaking, that vector represents the point in which the the plane in $\rm I\!R^{'3}$ is tangent to the sphere. Apply a linear transformation T to $v_1^{'}$ , so that $T:\rm I\!R^{'3} \rightarrow  \rm I\!R^3$ whose associate matrix is \begin{bmatrix} 1 & 0 & 0\\ 0 & 2 & 0\\ 0 & 0 & 4 \end{bmatrix} In order to bring my $v_1^{'}$ to the original space. Once I have my new vector $v_1$ , It also touches the ellipsoid at it's surface. But is clear that is not normal to my original plane. However, as it is the point of hypothetical contact of the plane with the ellipsoid, I just calculate the norm of the projection of $v_1$ on the my normal vector $n$ , and used it as the distance required for my plane to move in order to be tangent to the ellipsoid. Using the distance between planes equation, I found my final m and finished the exercise. However, I'm not sure if it's completely legal. And if it was, is there any other application of this technique?, we can use if for more complex problems?","In my vectorial calculus course, I was basically asked for finding a tangent plane for an ellipsoid parallel to a given plane by means of the gradient. Exactly the question was: Find the values of m for which the plane is tangent to the ellipsoid I solve it using the gradient, but I thought it could be a more interesting way to solve it. Firstly, I suppose that if we were making the exact same problem with a sphere of radius 12, we could take advantage of its symmetry… Modify the scale of the coordinates to transform the ellipsoid into a sphere, by making the substitutions , , . Hence I am working in a kind of instead of Apply the same exact substitution to the plane, in order to bring it to my new space, as well. Use the normalised vector normal to the plane, and find the scalar for which the vector line intersects the surface of the sphere. Let's call it . Technically speaking, that vector represents the point in which the the plane in is tangent to the sphere. Apply a linear transformation T to , so that whose associate matrix is In order to bring my to the original space. Once I have my new vector , It also touches the ellipsoid at it's surface. But is clear that is not normal to my original plane. However, as it is the point of hypothetical contact of the plane with the ellipsoid, I just calculate the norm of the projection of on the my normal vector , and used it as the distance required for my plane to move in order to be tangent to the ellipsoid. Using the distance between planes equation, I found my final m and finished the exercise. However, I'm not sure if it's completely legal. And if it was, is there any other application of this technique?, we can use if for more complex problems?","x - 2y -2z  + m = 0 x^2 +4y^2 +16z^2 = 144 x = x'  y = \frac{1}{2} y' z = \frac{1}{4}z' \rm I\!R^{'3} \rm I\!R^3 \alpha v_1^{'} \rm I\!R^{'3} v_1^{'} T:\rm I\!R^{'3} \rightarrow  \rm I\!R^3 \begin{bmatrix}
1 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 4
\end{bmatrix} v_1^{'} v_1 v_1 n","['linear-algebra', 'multivariable-calculus', 'vectors', 'vector-analysis']"
78,Regular Matrix Definition,Regular Matrix Definition,,"I found a definition of Regular Matrices that is, A regular matrix $A$ is a square matrix and there are some n ( $\geq$ 1) such that all the entries of $A^n$ are positive. I would like to know is this a correct definition? According to the definition, the following matrix is regular but how to prove it? Any hints $\\ $ \begin{pmatrix} 1 & 4& 3\\ 2& 0& 1\\ 4& 3& 2\\ \end{pmatrix}","I found a definition of Regular Matrices that is, A regular matrix is a square matrix and there are some n ( 1) such that all the entries of are positive. I would like to know is this a correct definition? According to the definition, the following matrix is regular but how to prove it? Any hints","A \geq A^n \\
 \begin{pmatrix}
1 & 4& 3\\
2& 0& 1\\
4& 3& 2\\
\end{pmatrix}","['linear-algebra', 'matrices']"
79,"If A positive definite then $\det(A) \leq \prod_{i=1}^n a_{i,i}$",If A positive definite then,"\det(A) \leq \prod_{i=1}^n a_{i,i}","If $A=(a_{ij})_{ij=1,...n}\in \mathbb{C}^{n \times n}$ positive definite $ \Rightarrow \det(A) \leq \prod_{i=1}^n a_{i,i}$ I've argumented with the Cholesky-decomposition (which exists because A is pos. def.) that $A = CC^*$ with $C$ being an lower triangular matrix and $C^*$ being an upper triangular matrix. Then $C$ and $C^*$ have the form... $C =\left( \begin{array}{rrrr} x_{1,1} & 0 & 0 & 0 \\ x_{1,2} & x_{2,2} & 0 & 0 \\ ... & ... & ... & 0 \\ x_{1,n} & x_{2,n} & ... & x_{n,n} \\ \end{array}\right) $ and $C^*=\left( \begin{array}{rrrr} x_{1,1} & x_{1,2} & ... & x_{1,n} \\ 0 & x_{2,2} & ... & x_{2,n} \\ 0 & 0 & ... & ... \\ 0 & 0 & 0 & x_{n,n} \\ \end{array}\right) $ then $CC^* = \left( \begin{array}{rrrr} x_{1,1}^2 &  &  & * \\  & x_{2,2}^2 +x_{1,2}^2 &  &  \\  &  & ... &  \\ * &  &  & x_{n,n}^2 + x_{1,n}^2+x_{2,n}^2+...+x_{n-1,n}^2\\ \end{array}\right) $ Obviously, $\prod_{i=1}^n a_{i,i} = x_{1,1}^2*(x_{2,2}^2+x_{1,2}^2)*...*(x_{n,n}^2+x_{1,n}^2+...+x_{n-1,n}^2)$ And $\det(A) = \det(CC^*) = \det(C) * \det(C^*) = \det(C^*)*\det(C^*) = x_{1,1}^2*x_{2,2}^2 *...*x_{n,n}^2$ If what I've proven so far is correct, I'm very happy. But something bothers me. In $\mathbb{R}$ it is clear that $\det(A) \leq \prod_{i=1}^n a_{i,i}$ because the other sums are positive real numbers. Since I'm in the complex realm this formular doesn't hold true, does it? I cannot define $""\leq""$ with complex numbers since it's not an order relation. There is no guarantee that the other sums besides $x_{1,1}^2*x_{2,2}^2 *...*x_{n,n}^2$ are greater than 0 or even a real number, because $(a+bi)^2 = (a^2-b^2)+i(2ab)$ is still a complex number in most cases. Can someone check my findings and elaborate? Thanks in advance.","If positive definite I've argumented with the Cholesky-decomposition (which exists because A is pos. def.) that with being an lower triangular matrix and being an upper triangular matrix. Then and have the form... and then Obviously, And If what I've proven so far is correct, I'm very happy. But something bothers me. In it is clear that because the other sums are positive real numbers. Since I'm in the complex realm this formular doesn't hold true, does it? I cannot define with complex numbers since it's not an order relation. There is no guarantee that the other sums besides are greater than 0 or even a real number, because is still a complex number in most cases. Can someone check my findings and elaborate? Thanks in advance.","A=(a_{ij})_{ij=1,...n}\in \mathbb{C}^{n \times n}  \Rightarrow \det(A) \leq \prod_{i=1}^n a_{i,i} A = CC^* C C^* C C^* C =\left( \begin{array}{rrrr}
x_{1,1} & 0 & 0 & 0 \\
x_{1,2} & x_{2,2} & 0 & 0 \\
... & ... & ... & 0 \\
x_{1,n} & x_{2,n} & ... & x_{n,n} \\
\end{array}\right)  C^*=\left( \begin{array}{rrrr}
x_{1,1} & x_{1,2} & ... & x_{1,n} \\
0 & x_{2,2} & ... & x_{2,n} \\
0 & 0 & ... & ... \\
0 & 0 & 0 & x_{n,n} \\
\end{array}\right)  CC^* = \left( \begin{array}{rrrr}
x_{1,1}^2 &  &  & * \\
 & x_{2,2}^2 +x_{1,2}^2 &  &  \\
 &  & ... &  \\
* &  &  & x_{n,n}^2 + x_{1,n}^2+x_{2,n}^2+...+x_{n-1,n}^2\\
\end{array}\right)  \prod_{i=1}^n a_{i,i} = x_{1,1}^2*(x_{2,2}^2+x_{1,2}^2)*...*(x_{n,n}^2+x_{1,n}^2+...+x_{n-1,n}^2) \det(A) = \det(CC^*) = \det(C) * \det(C^*) = \det(C^*)*\det(C^*) = x_{1,1}^2*x_{2,2}^2 *...*x_{n,n}^2 \mathbb{R} \det(A) \leq \prod_{i=1}^n a_{i,i} ""\leq"" x_{1,1}^2*x_{2,2}^2 *...*x_{n,n}^2 (a+bi)^2 = (a^2-b^2)+i(2ab)","['linear-algebra', 'matrices', 'complex-numbers', 'positive-definite']"
80,"$\mathbf{A}^T = p(\mathbf{A})$, prove that $\mathbf{A}$ is invertible",", prove that  is invertible",\mathbf{A}^T = p(\mathbf{A}) \mathbf{A},"Let $\mathbf{A}$ be a square matrix defined over a field $\mathbb{R}$ . It is known that $\mathbf{A}^\text{T} = p(\mathbf{A})$ , where $p(\mathbf{A})$ is a polynomial with a constant coefficient $a_0 \neq 0$ . Prove that $\mathbf{A}$ is invertible. Is it true that for every operator $\mathbb{\phi}: \mathbb{R}^n\rightarrow\mathbb{R}^n$ there exists some polynomial $p(x)$ and the basis for which the matrix $\phi$ satisfies the condition of $\mathbf{A}^\text{T} = p(\mathbf{A})$ ? Solution I. By the definition of $\mathbf{A}$ : $$\mathbf{A}^t = P(\mathbf{A}) = a_n \cdot \mathbf{A}^n + \cdots + a_1 \cdot \mathbf{A} + a_0 \cdot \mathbf{I}$$ Consider $$\mathbf{A}^t \cdot \mathbf{A} = a_n \cdot \mathbf{A}^n \cdot \mathbf{A} + \cdots + a_1 \cdot \mathbf{A} \cdot \mathbf{A} + a_0 \cdot \mathbf{I} \cdot \mathbf{A} \\ = a_n \cdot \mathbf{A}^{n + 1} + \cdots + a_1 \cdot \mathbf{A}^2 + a_0 \cdot \mathbf{A}$$ $$\mathbf{A} \cdot \mathbf{A}^t = a_n \cdot \mathbf{A} \cdot \mathbf{A}^n + \cdots + a_1 \cdot \mathbf{A} \cdot \mathbf{A} + a_0 \cdot \mathbf{A} \cdot I \\ = a_n \cdot \mathbf{A}^{n + 1} + \cdots + a_1 \cdot \mathbf{A}^2 + a_0 \cdot \mathbf{A}$$ hence, $\mathbf{A}^t \cdot \mathbf{A} = \mathbf{A} \cdot \mathbf{A}^t$ . Therefore, the matrix $\mathbf{A}$ is normal, which assumes $\text{ker}(\mathbf{A}) = \text{ker}(\mathbf{A^t})$ . Assume that $\mathbf{A}$ is not invertible, then its kernel is non-trivial. Take non-zero $v$ from $\text{ker}(\mathbf{A})$ : $$\mathbf{A}^t \cdot v = a_n \cdot \mathbf{A}^n \cdot v + \cdots + a_1 \cdot \mathbf{A} \cdot v + a_0 \cdot \mathbf{I} \cdot v \\ \mathbf{A}^t \cdot v = 0 + \cdots + 0 + a_0 \cdot v = a_0 \cdot v$$ As per definition, $a_0 \neq 0$ , so $a_0 \cdot v \neq 0$ , and it follows that $\mathbf{A}^t \cdot v \neq 0$ . However, the condition $\text{ker}(\mathbf{A})=\text{ker}(\mathbf{A^t})$ implies that $\mathbf{A}^t \cdot v = 0$ . We have a contradiction, hence $\mathbf{A}$ is invertible. II. It was proven that $\mathbf{A}$ is normal if its transpose can be represented by a polynomial. But normality is independent of the basis chosen, so the second statement would mean that every operator is normal, which is not the case. Any mistakes, improvements?","Let be a square matrix defined over a field . It is known that , where is a polynomial with a constant coefficient . Prove that is invertible. Is it true that for every operator there exists some polynomial and the basis for which the matrix satisfies the condition of ? Solution I. By the definition of : Consider hence, . Therefore, the matrix is normal, which assumes . Assume that is not invertible, then its kernel is non-trivial. Take non-zero from : As per definition, , so , and it follows that . However, the condition implies that . We have a contradiction, hence is invertible. II. It was proven that is normal if its transpose can be represented by a polynomial. But normality is independent of the basis chosen, so the second statement would mean that every operator is normal, which is not the case. Any mistakes, improvements?",\mathbf{A} \mathbb{R} \mathbf{A}^\text{T} = p(\mathbf{A}) p(\mathbf{A}) a_0 \neq 0 \mathbf{A} \mathbb{\phi}: \mathbb{R}^n\rightarrow\mathbb{R}^n p(x) \phi \mathbf{A}^\text{T} = p(\mathbf{A}) \mathbf{A} \mathbf{A}^t = P(\mathbf{A}) = a_n \cdot \mathbf{A}^n + \cdots + a_1 \cdot \mathbf{A} + a_0 \cdot \mathbf{I} \mathbf{A}^t \cdot \mathbf{A} = a_n \cdot \mathbf{A}^n \cdot \mathbf{A} + \cdots + a_1 \cdot \mathbf{A} \cdot \mathbf{A} + a_0 \cdot \mathbf{I} \cdot \mathbf{A} \\ = a_n \cdot \mathbf{A}^{n + 1} + \cdots + a_1 \cdot \mathbf{A}^2 + a_0 \cdot \mathbf{A} \mathbf{A} \cdot \mathbf{A}^t = a_n \cdot \mathbf{A} \cdot \mathbf{A}^n + \cdots + a_1 \cdot \mathbf{A} \cdot \mathbf{A} + a_0 \cdot \mathbf{A} \cdot I \\ = a_n \cdot \mathbf{A}^{n + 1} + \cdots + a_1 \cdot \mathbf{A}^2 + a_0 \cdot \mathbf{A} \mathbf{A}^t \cdot \mathbf{A} = \mathbf{A} \cdot \mathbf{A}^t \mathbf{A} \text{ker}(\mathbf{A}) = \text{ker}(\mathbf{A^t}) \mathbf{A} v \text{ker}(\mathbf{A}) \mathbf{A}^t \cdot v = a_n \cdot \mathbf{A}^n \cdot v + \cdots + a_1 \cdot \mathbf{A} \cdot v + a_0 \cdot \mathbf{I} \cdot v \\ \mathbf{A}^t \cdot v = 0 + \cdots + 0 + a_0 \cdot v = a_0 \cdot v a_0 \neq 0 a_0 \cdot v \neq 0 \mathbf{A}^t \cdot v \neq 0 \text{ker}(\mathbf{A})=\text{ker}(\mathbf{A^t}) \mathbf{A}^t \cdot v = 0 \mathbf{A} \mathbf{A},"['linear-algebra', 'matrices', 'solution-verification']"
81,Another Hadamard matrix of order 4?,Another Hadamard matrix of order 4?,,"Wikipedia states that there is, up to equivalence, a unique Hadamard matrix of order 4, namely $$ \def\p{\phantom+} \begin{pmatrix} \p1&\p1&\p1&\p1 \\ \p1&-1&\p1&-1 \\ \p1&\p1&-1&-1 \\ \p1&-1&-1&\p1 \end{pmatrix}.$$ As equialence operations are allowed negating some rows/columns or interchanging some rows/columns. But isn't the following another Hadamard matrix of order 4 that cannot be obtained in this way? $$ \begin{pmatrix} -1&\p1&\p1&\p1 \\ \p1&-1&\p1&\p1 \\ \p1&\p1&-1&\p1 \\ \p1&\p1&\p1&-1 \\ \end{pmatrix}.$$","Wikipedia states that there is, up to equivalence, a unique Hadamard matrix of order 4, namely As equialence operations are allowed negating some rows/columns or interchanging some rows/columns. But isn't the following another Hadamard matrix of order 4 that cannot be obtained in this way?","
\def\p{\phantom+}
\begin{pmatrix}
\p1&\p1&\p1&\p1 \\
\p1&-1&\p1&-1 \\
\p1&\p1&-1&-1 \\
\p1&-1&-1&\p1
\end{pmatrix}. 
\begin{pmatrix}
-1&\p1&\p1&\p1 \\
\p1&-1&\p1&\p1 \\
\p1&\p1&-1&\p1 \\
\p1&\p1&\p1&-1 \\
\end{pmatrix}.","['linear-algebra', 'combinatorics', 'matrices', 'equivalence-relations', 'hadamard-matrices']"
82,How to think about the isomorphism $V/\ker T\cong\operatorname{im}T$,How to think about the isomorphism,V/\ker T\cong\operatorname{im}T,"My question is in the context of linear algebra (of a linear map $T$ between two FDVS $V$ and $W$ ), though I’ve heard it is a more general result in group theory (First Isomorphism Theorem). I’ve seen some answers about intuition for this theorem, but they are in the context of group theory and more general/abstract. Do the following observations related to this theorem hold? Am I thinking about this result correctly? I would also appreciate any general comments about this result (whether in linear algebra or in general). (Visualizing movement between points and between affine subsets) A movement between two points in the image corresponds to a movement between two affine subsets of the domain that are parallel to the kernel (and vice versa). (See image below (I drew this, so not sure if accurate).) Specifically, if we move from $T(x)\in\operatorname{im}T$ to $T(x’)\in\operatorname{im}T$ then we move from $x+\ker T$ to $x’+\ker T$ in the domain (and vice versa). In fact, we can be more general: we can use any element in the fiber of $T(x)$ to represent $x + \ker T$ , and any element in the fiber of $T(x’)$ to represent $x’ + \ker T$ . (Application to systems of linear equations) Let $A$ be the matrix of $T$ . The solution set of $Ax = 0$ is $\ker T$ . More generally, consider $Ax = b$ . If this system is consistent, i.e. if $b$ is in the image of $T$ , then its solution set is given by $v + \ker T$ , where $v$ is any solution $Av = b$ . Edit: Here are a few more observations (some overlap a bit with what I’ve already said); they may or may not be accurate: (Removal of useless/trivial solutions) The map $T$ from $V$ to $W$ (or to im $T$ ) may not be injective, but if we think about the map $\tilde{T}$ from $V/\text{ker $T$}$ to im $T$ that sends $v + \text{ker $T$}$ to $v$ , we get a bijection. We’ve sort of “removed duplicates” and achieved a one-to-one-correspondence. Also, we've collapsed the kernel into a single point (i.e. 0), so all the extra trivial solutions have been removed; now, only 0 gets mapped to 0. (Partitioning the domain into copies of the kernel) The quotient space $V/\text{ker $T$}$ is the set of all affine subsets parallel to the kernel. So intuitively the isomorphism tells us that if we chop up the domain into copies of the kernel then we get something isomorphic to the image. (Number of remaining directions required to fill up the domain) $V/\text{ker $T$}$ has dimension $\dim V - \dim \ker T$ , so it sort of tells us how many more directions we need to fill up the space. E.g. in $\mathbb{R}^3$ , if the kernel has dimension 2, then $V/\text{ker $T$}$ has dimension 1; there is “one more direction” to go. Geometrically, sweeping a plane through a line (that's not in the plane) gives us all of 3D space. (The image is isomorphic to the subspace perpendicular to the kernel) The orthogonal complement $U^\perp$ of a subspace $U$ has dimension $V - U$ . Setting $U = \text{ker $T$}$ tells us that $(\ker T)^\perp$ has the same dimension as im $T$ , i.e. they are isomorphic. Thinking in $\mathbb{R}^3$ again, if the kernel is a plane, then im $T$ and $(\ker T)^\perp$ are lines, so they are isomorphic. Thinking of $V/\ker T$ as chopping up $\mathbb{R}^3$ into a line of parallel planes also shows how $V/\ker T$ is like a line as well.","My question is in the context of linear algebra (of a linear map between two FDVS and ), though I’ve heard it is a more general result in group theory (First Isomorphism Theorem). I’ve seen some answers about intuition for this theorem, but they are in the context of group theory and more general/abstract. Do the following observations related to this theorem hold? Am I thinking about this result correctly? I would also appreciate any general comments about this result (whether in linear algebra or in general). (Visualizing movement between points and between affine subsets) A movement between two points in the image corresponds to a movement between two affine subsets of the domain that are parallel to the kernel (and vice versa). (See image below (I drew this, so not sure if accurate).) Specifically, if we move from to then we move from to in the domain (and vice versa). In fact, we can be more general: we can use any element in the fiber of to represent , and any element in the fiber of to represent . (Application to systems of linear equations) Let be the matrix of . The solution set of is . More generally, consider . If this system is consistent, i.e. if is in the image of , then its solution set is given by , where is any solution . Edit: Here are a few more observations (some overlap a bit with what I’ve already said); they may or may not be accurate: (Removal of useless/trivial solutions) The map from to (or to im ) may not be injective, but if we think about the map from to im that sends to , we get a bijection. We’ve sort of “removed duplicates” and achieved a one-to-one-correspondence. Also, we've collapsed the kernel into a single point (i.e. 0), so all the extra trivial solutions have been removed; now, only 0 gets mapped to 0. (Partitioning the domain into copies of the kernel) The quotient space is the set of all affine subsets parallel to the kernel. So intuitively the isomorphism tells us that if we chop up the domain into copies of the kernel then we get something isomorphic to the image. (Number of remaining directions required to fill up the domain) has dimension , so it sort of tells us how many more directions we need to fill up the space. E.g. in , if the kernel has dimension 2, then has dimension 1; there is “one more direction” to go. Geometrically, sweeping a plane through a line (that's not in the plane) gives us all of 3D space. (The image is isomorphic to the subspace perpendicular to the kernel) The orthogonal complement of a subspace has dimension . Setting tells us that has the same dimension as im , i.e. they are isomorphic. Thinking in again, if the kernel is a plane, then im and are lines, so they are isomorphic. Thinking of as chopping up into a line of parallel planes also shows how is like a line as well.",T V W T(x)\in\operatorname{im}T T(x’)\in\operatorname{im}T x+\ker T x’+\ker T T(x) x + \ker T T(x’) x’ + \ker T A T Ax = 0 \ker T Ax = b b T v + \ker T v Av = b T V W T \tilde{T} V/\text{ker T} T v + \text{ker T} v V/\text{ker T} V/\text{ker T} \dim V - \dim \ker T \mathbb{R}^3 V/\text{ker T} U^\perp U V - U U = \text{ker T} (\ker T)^\perp T \mathbb{R}^3 T (\ker T)^\perp V/\ker T \mathbb{R}^3 V/\ker T,"['linear-algebra', 'abstract-algebra', 'group-theory', 'linear-transformations']"
83,How do I make the following question precise?(TIFR GS) [duplicate],How do I make the following question precise?(TIFR GS) [duplicate],,"This question already has an answer here : Can we say that there exist an integer $n$ such $A+nB$ invertible? (1 answer) Closed 4 years ago . This is a question from TIFR GS $2011$ .But I think this is a stupid question or the question is not precise.This question asks if given $A,B$ matrices of order $3\times 3$ ,and $A$ invertible,there exists any $n\in \mathbb Z$ such that $A+nB$ is invertible.My simple answer would be yes,because $n=0\in \mathbb Z$ and $A+nB=A+0B=A$ which is of course invertible as $A$ is invertible. Can someone help me to figure out if this question is stupid or the question should be made precise.If it the latter case,then one may write the precise version.I am also looking for a proof of this problem with linear transformations. Also can someone suggest similar or more general results related to this problem?","This question already has an answer here : Can we say that there exist an integer $n$ such $A+nB$ invertible? (1 answer) Closed 4 years ago . This is a question from TIFR GS .But I think this is a stupid question or the question is not precise.This question asks if given matrices of order ,and invertible,there exists any such that is invertible.My simple answer would be yes,because and which is of course invertible as is invertible. Can someone help me to figure out if this question is stupid or the question should be made precise.If it the latter case,then one may write the precise version.I am also looking for a proof of this problem with linear transformations. Also can someone suggest similar or more general results related to this problem?","2011 A,B 3\times 3 A n\in \mathbb Z A+nB n=0\in \mathbb Z A+nB=A+0B=A A","['linear-algebra', 'matrices', 'soft-question', 'linear-transformations']"
84,Proving that similarity transformation of state-space preserves the Euclidean norm,Proving that similarity transformation of state-space preserves the Euclidean norm,,"I am aware that the state space realization of a dynamical system is not unique. So if we have a dynamical system: $\dot{x} = Ax + Bu$ $y = Cx$ Then we can write it as $\begin{bmatrix} \dot{x}\\y \end{bmatrix} = G \begin{bmatrix}x\\u\end{bmatrix} $ where G is $G = \begin{bmatrix} A \ \ \ \ | & B\\  \hline C \ \ \ \ | & 0 \end{bmatrix} $ but one can also formulate it as $G = \begin{bmatrix} TAT^{-1} \ \ | & TB\\  \hline CT^{-1} \ \  \ \ \ | & 0 \end{bmatrix}$ for any invertible matrix $T$ . I need to show that these two realizations of $G$ have the same Euclidean norm. I have found a video showing how similarity transformations preserve the trace and the determinant, but they did just an example problem, without doing the proof. Edit: It has been given that $G \in \mathcal{RH}_2$ , where $\mathcal{RH}_2$ is a set of all real rational, strictly proper and stable transfer matrices. It is a Hilbert Space with the inner product: $\langle F,G \rangle = \sup_{\sigma > 0} \left\{ \frac{1}{2\pi}\int_{-\infty}^{\infty} trace \left\{ F^* (\sigma+j\omega) G(\sigma+j\omega) \right\} d\omega \right\}$ and the corresponding norm given by ${{\left\Vert F \right\Vert}_{2}}^2 = \frac{1}{2\pi} \int_{-\infty}^{\infty} trace \left\{  F^*(j\omega)F(j\omega) \right\}d\omega$","I am aware that the state space realization of a dynamical system is not unique. So if we have a dynamical system: Then we can write it as where G is but one can also formulate it as for any invertible matrix . I need to show that these two realizations of have the same Euclidean norm. I have found a video showing how similarity transformations preserve the trace and the determinant, but they did just an example problem, without doing the proof. Edit: It has been given that , where is a set of all real rational, strictly proper and stable transfer matrices. It is a Hilbert Space with the inner product: and the corresponding norm given by","\dot{x} = Ax + Bu y = Cx \begin{bmatrix}
\dot{x}\\y
\end{bmatrix}
= G
\begin{bmatrix}x\\u\end{bmatrix}
 G = \begin{bmatrix}
A \ \ \ \ | & B\\ 
\hline
C \ \ \ \ | & 0
\end{bmatrix}
 G = \begin{bmatrix}
TAT^{-1} \ \ | & TB\\ 
\hline
CT^{-1} \ \  \ \ \ | & 0
\end{bmatrix} T G G \in \mathcal{RH}_2 \mathcal{RH}_2 \langle F,G \rangle = \sup_{\sigma > 0} \left\{ \frac{1}{2\pi}\int_{-\infty}^{\infty} trace \left\{ F^* (\sigma+j\omega) G(\sigma+j\omega) \right\} d\omega \right\} {{\left\Vert F \right\Vert}_{2}}^2 = \frac{1}{2\pi} \int_{-\infty}^{\infty} trace \left\{ 
F^*(j\omega)F(j\omega) \right\}d\omega","['linear-algebra', 'dynamical-systems']"
85,Can eigenvectors be found without finding eigenvalues?,Can eigenvectors be found without finding eigenvalues?,,"Given a matrix $A$ and the set of all of its eigenvectors, it is possible to find all of the matrix’s eigenvalues by solving $A\vec v = \lambda\vec v$ .  Given the set of all eigenvalues, it is possible to find the corresponding eigenspaces by finding $\mathrm N(A-\lambda I)$ . It is also possible to find all of the eigenvalues independently of the eigenvectors by finding the zeroes of the characteristic polynomial $\lvert A – \lambda I\rvert$ . This makes me wonder, is it possible to find all of the eigenvectors or eigenspaces independently of the eigenvalues? If so, how? My linear algebra instructor’s assistant directed me to this video, which I found unhelpful.","Given a matrix and the set of all of its eigenvectors, it is possible to find all of the matrix’s eigenvalues by solving .  Given the set of all eigenvalues, it is possible to find the corresponding eigenspaces by finding . It is also possible to find all of the eigenvalues independently of the eigenvectors by finding the zeroes of the characteristic polynomial . This makes me wonder, is it possible to find all of the eigenvectors or eigenspaces independently of the eigenvalues? If so, how? My linear algebra instructor’s assistant directed me to this video, which I found unhelpful.",A A\vec v = \lambda\vec v \mathrm N(A-\lambda I) \lvert A – \lambda I\rvert,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
86,"If the columns of $A$ generate the subgroup $G$ of $X$, then $\vert\text{det}\;A\vert = X:G$.","If the columns of  generate the subgroup  of , then .",A G X \vert\text{det}\;A\vert = X:G,"I was recently reading a paper by Cantor and Mills , when I came across the following lemma. Let $m$ and $t$ be positive integers. Let $X$ be the additive group of all $m$ -dimensional column vectors with integer elements, let $Y$ be a finite set of t-dimensional column vectors with integer elements, and let c be the cardinality of $Y$ . Suppose that $A$ is an $m$ by $m$ matrix of integers and that $B$ is an $m$ by $t$ matrix of integers. If, for $x\in X$ and $y \in Y$ , the column vector $Ax + By$ determines $x$ and $y$ uniquely, then $\vert\text{det}\;A\vert \geq c$ . Proof. Let $G$ be the subgroup of $X$ generated by the columns of $A$ . Thus $G$ is the set of all vectors $Ax$ with $x \in X$ . By hypothesis, the column vectors of the form $Ax + By$ , with $x \in X$ and $y \in Y$ , are all distinct. Therefore as $y$ ranges over the $c$ elements of $Y$ , By ranges over $c$ distinct cosets of $G$ in $X$ . Hence the index $X:G$ of $G$ in $X$ is at least $c$ . On the other hand $X: G$ is equal to the absolute value of the determinant of $A$ . Thus, $$\vert\text{det}\;A\vert = X:G \geq c$$ and the proof is complete. I don't understand how the authors get $\vert\det A\vert = X:G$ and I wasn't able to prove it myself. Here's what I tried: We want to find the number of sets $S$ of $m$ -dimensional column vectors with integer entries such that for all $c,c' \in S$ , there exists $x,x' \in X$ that satisfy $Ax + c = Ax' + c'$ . Note that $$Ax + c = Ax' + c' \implies x + \frac{\text{adj}(A) \cdot c}{\det A}= x' +  \frac{\text{adj}(A) \cdot c'}{\det A},$$ thus there exists a solution $x,x' \in X$ if each entry of $\text{adj}(A) \cdot c$ and $\text{adj}(A) \cdot c'$ is the same modulo $\det A$ . At first, I thought that we could get $\det A$ sets just by assigning one set to each integer from $1, \dots, \det A$ . However, not every entry of $\text{adj}(A) \cdot c$ can take on all values $1, \dots, \det A$ , and there are plenty of examples of $c,c'$ for which are congruent modulo $\det A$ in one entry but not in another. I can't seem to make any progress beyond this. I've looked for a proof of the above statement to no avail. Could anyone explain how to prove this statement? Thanks in advance.","I was recently reading a paper by Cantor and Mills , when I came across the following lemma. Let and be positive integers. Let be the additive group of all -dimensional column vectors with integer elements, let be a finite set of t-dimensional column vectors with integer elements, and let c be the cardinality of . Suppose that is an by matrix of integers and that is an by matrix of integers. If, for and , the column vector determines and uniquely, then . Proof. Let be the subgroup of generated by the columns of . Thus is the set of all vectors with . By hypothesis, the column vectors of the form , with and , are all distinct. Therefore as ranges over the elements of , By ranges over distinct cosets of in . Hence the index of in is at least . On the other hand is equal to the absolute value of the determinant of . Thus, and the proof is complete. I don't understand how the authors get and I wasn't able to prove it myself. Here's what I tried: We want to find the number of sets of -dimensional column vectors with integer entries such that for all , there exists that satisfy . Note that thus there exists a solution if each entry of and is the same modulo . At first, I thought that we could get sets just by assigning one set to each integer from . However, not every entry of can take on all values , and there are plenty of examples of for which are congruent modulo in one entry but not in another. I can't seem to make any progress beyond this. I've looked for a proof of the above statement to no avail. Could anyone explain how to prove this statement? Thanks in advance.","m t X m Y Y A m m B m t x\in X y \in Y Ax + By x y \vert\text{det}\;A\vert \geq c G X A G Ax x \in X Ax + By x \in X y \in Y y c Y c G X X:G G X c X: G A \vert\text{det}\;A\vert = X:G \geq c \vert\det A\vert = X:G S m c,c' \in S x,x' \in X Ax + c = Ax' + c' Ax + c = Ax' + c' \implies x + \frac{\text{adj}(A) \cdot c}{\det A}= x' +  \frac{\text{adj}(A) \cdot c'}{\det A}, x,x' \in X \text{adj}(A) \cdot c \text{adj}(A) \cdot c' \det A \det A 1, \dots, \det A \text{adj}(A) \cdot c 1, \dots, \det A c,c' \det A","['linear-algebra', 'abstract-algebra', 'group-theory']"
87,A weird matrix property,A weird matrix property,,"I encountered the following weird matrix property. Consider any general matrix $M_{n\times n}$ with the property that the sum of each column vanishes, that is \begin{align} \sum^n_{j} M_{ji} =0 \end{align} Denoting $M_{(1)}$ : the matrix obtained from $M$ by removing the first column and row, $M_{(2)}$ : the matrix obtained from $M$ by removing the second column and row, $M_{(1,2)}$ : the matrix obtained from $M$ by removing the first and second column and row, $u_{(k)}=(1,\dots,1)$ : the $(k)$ -row vector with all elements being $1$ , $C_{(n-1)\times (n-2)}=\begin{pmatrix} 0 \dots  0  \\ 1_{(n-2)\times (n-2)}   \end{pmatrix} $ , where $ 1_{(n-2)\times (n-2)}$ is the identity matrix Define $p_1$ and $p_2$ as $$ p_1 = u_{(n-1)} \cdot\big(M_{(1)}\big)^{-1}\cdot\begin{pmatrix} 1  \\ 0 \\ \vdots\\0   \end{pmatrix}_{ (n-1)\times1}, \quad  p_2 = u_{(n-1)} \cdot\big(M_{(2)}\big)^{-1}\cdot\begin{pmatrix} 1  \\ 0 \\ \vdots\\0   \end{pmatrix}_{ (n-1)\times1}. $$ Prove that all the elements of the row vector $$ u_{(n-1)} \cdot \left(p_2\big(M_{(1)}\big)^{-1}+p_1\big(M_{(2)}\big)^{-1}\right)\cdot C - (p_1+p_2)u_{(n-2)}\cdot \big(M_{(1,2)}\big)^{-1}   $$ are identical. This property comes from some intuition of the problem that I have been playing with. I have also tested it by evaluating it with a large set of matrix $M$ satisfying the first requirement. ( I thank user1551 for spotting an important typo, corrected now!) I have tried writing the inverse using minors but does not seem to help as it is not easy to implement the requirement that $\sum_{j} M_{ji}=0$ .  Any comment/suggestion is greatly appreciated. Answers are of course the best! Thank you so much!","I encountered the following weird matrix property. Consider any general matrix with the property that the sum of each column vanishes, that is Denoting : the matrix obtained from by removing the first column and row, : the matrix obtained from by removing the second column and row, : the matrix obtained from by removing the first and second column and row, : the -row vector with all elements being , , where is the identity matrix Define and as Prove that all the elements of the row vector are identical. This property comes from some intuition of the problem that I have been playing with. I have also tested it by evaluating it with a large set of matrix satisfying the first requirement. ( I thank user1551 for spotting an important typo, corrected now!) I have tried writing the inverse using minors but does not seem to help as it is not easy to implement the requirement that .  Any comment/suggestion is greatly appreciated. Answers are of course the best! Thank you so much!","M_{n\times n} \begin{align} \sum^n_{j} M_{ji} =0 \end{align} M_{(1)} M M_{(2)} M M_{(1,2)} M u_{(k)}=(1,\dots,1) (k) 1 C_{(n-1)\times (n-2)}=\begin{pmatrix} 0 \dots  0  \\ 1_{(n-2)\times (n-2)}
  \end{pmatrix}   1_{(n-2)\times (n-2)} p_1 p_2  p_1 = u_{(n-1)} \cdot\big(M_{(1)}\big)^{-1}\cdot\begin{pmatrix} 1  \\ 0 \\ \vdots\\0
  \end{pmatrix}_{ (n-1)\times1}, \quad  p_2 = u_{(n-1)} \cdot\big(M_{(2)}\big)^{-1}\cdot\begin{pmatrix} 1  \\ 0 \\ \vdots\\0
  \end{pmatrix}_{ (n-1)\times1}.   u_{(n-1)} \cdot \left(p_2\big(M_{(1)}\big)^{-1}+p_1\big(M_{(2)}\big)^{-1}\right)\cdot C - (p_1+p_2)u_{(n-2)}\cdot \big(M_{(1,2)}\big)^{-1}    M \sum_{j} M_{ji}=0","['linear-algebra', 'matrices']"
88,Unitary matrix as a product of real orthogonal and complex symmetric,Unitary matrix as a product of real orthogonal and complex symmetric,,Show that any unitary matrix $U$ can be written as a product of real orthogonal matrix and complex symmetric matrix. Hint: For any unitary matrix $A$ and for any $n\in \mathbb{N}$ there is unitary matrix $B$ such that $B^n=A$ . (I proved it here ) My attempt: Since $U$ is unitary then $U^TU$ is also unitary matrix then by hint one can find unitary matrix $X$ such that $X^2=U^TU$ . Then I stucked and don't know how to proceed. Would be thankful if someone can show how to solve this problem.,Show that any unitary matrix can be written as a product of real orthogonal matrix and complex symmetric matrix. Hint: For any unitary matrix and for any there is unitary matrix such that . (I proved it here ) My attempt: Since is unitary then is also unitary matrix then by hint one can find unitary matrix such that . Then I stucked and don't know how to proceed. Would be thankful if someone can show how to solve this problem.,U A n\in \mathbb{N} B B^n=A U U^TU X X^2=U^TU,"['linear-algebra', 'matrices', 'unitary-matrices']"
89,Proving $x^TAx \geq 0$ for a positive definite $n \times n$ matrix A.,Proving  for a positive definite  matrix A.,x^TAx \geq 0 n \times n,"I am trying to prove that $x^TAx \geq 0$ for a positive definite $n \times n$ matrix $\mathbf{A}$ and $x \in \mathbb{R}^n$ . I have already proven that for any eigenvalue $y$ of $\mathbf{A}$ , $ y^TAy > 0$ . I have also shown that that $\mathbf{A}$ is diagonalizable (using the fact that positive - definite matrices are symmetric) so we can find a basis of eigenvalues of $\mathbf{A}$ for the vector space $\mathbb{R}^n$ , and therefore $x$ can be written as a linear combination of eigenvalues of $\mathbf{A}$ , i.e: $$x = a_1y_1 + … + a_ny_n$$ where $y_i, 1 \leq I \leq n$ are eigenvalues and the $a_i$ are scalars. I am just not sure how to complete the final step of showing $x^TAx \geq 0$ . Thank you in advance!","I am trying to prove that for a positive definite matrix and . I have already proven that for any eigenvalue of , . I have also shown that that is diagonalizable (using the fact that positive - definite matrices are symmetric) so we can find a basis of eigenvalues of for the vector space , and therefore can be written as a linear combination of eigenvalues of , i.e: where are eigenvalues and the are scalars. I am just not sure how to complete the final step of showing . Thank you in advance!","x^TAx \geq 0 n \times n \mathbf{A} x \in \mathbb{R}^n y \mathbf{A}  y^TAy > 0 \mathbf{A} \mathbf{A} \mathbb{R}^n x \mathbf{A} x = a_1y_1 + … + a_ny_n y_i, 1 \leq I \leq n a_i x^TAx \geq 0",['linear-algebra']
90,Definition of a Frenet curve,Definition of a Frenet curve,,"Recently I followed a course in Differential Geometry. The book we use is Differential Geometry: Curves - Surfaces - Manifolds by Wolfgang Kühnel. My question is about the definition of a Frenet curve. 2.4. Definition (Frenet curve). Let $c(s)$ be a regular curve in $\mathbb{R}^n$ which is parametrized by arc length and $n$ -times continuously differentiable. Then $c$ is called a Frenet curve , if at every point the vectors $c',c'',\dots,c^{(n-1)}$ are linearly independent. The Frenet $n$ -frame $e_1,e_2,\dots,e_n$ is then uniquely determined by the following conditions: (i) $e_1,\dots,e_n$ are orthonormal and positively oriented. (ii) For every $k=1,\dots,n-1$ one has $\text{Span}(e_1,\dots,e_k)=\text{Span}(c',\dots,c^{(k)})$ . (iii) $\langle c^{(k)},e_k\rangle>0$ for $k=1,\dots,n-1$ . My thought: In this definition, I think (i) and (ii) are enough to uniquely determined $e_1,\dots,e_n$ . Indeed, one obtains $e_1,\dots,e_{n-1}$ from $c',\dots,c^{(n-1)}$ by means of the Gram-Schmidt orthogonalization procedure, and the missing $e_n$ is then uniquely determined by condition (i). Why condition (iii) is necessary? Any help would be appreciated.","Recently I followed a course in Differential Geometry. The book we use is Differential Geometry: Curves - Surfaces - Manifolds by Wolfgang Kühnel. My question is about the definition of a Frenet curve. 2.4. Definition (Frenet curve). Let be a regular curve in which is parametrized by arc length and -times continuously differentiable. Then is called a Frenet curve , if at every point the vectors are linearly independent. The Frenet -frame is then uniquely determined by the following conditions: (i) are orthonormal and positively oriented. (ii) For every one has . (iii) for . My thought: In this definition, I think (i) and (ii) are enough to uniquely determined . Indeed, one obtains from by means of the Gram-Schmidt orthogonalization procedure, and the missing is then uniquely determined by condition (i). Why condition (iii) is necessary? Any help would be appreciated.","c(s) \mathbb{R}^n n c c',c'',\dots,c^{(n-1)} n e_1,e_2,\dots,e_n e_1,\dots,e_n k=1,\dots,n-1 \text{Span}(e_1,\dots,e_k)=\text{Span}(c',\dots,c^{(k)}) \langle c^{(k)},e_k\rangle>0 k=1,\dots,n-1 e_1,\dots,e_n e_1,\dots,e_{n-1} c',\dots,c^{(n-1)} e_n","['linear-algebra', 'differential-geometry']"
91,Closed but not topological complementary spaces,Closed but not topological complementary spaces,,"I’ve shown that if $U$ and $V$ are topologically  complementary then they are closed on a normed space. Also, I’ve shown that if $X$ is a Banach space and $U,V$ are closed complementary subspaces then they are topologically complementary. My question is what if $X$ is not Banach for second? I could not find a counter example for complementary subspaces that are closed but not topological. My definition for topologic complements on a normed space : $U,V \subset X$ are complementary subspaces. $\forall x \in X, \quad x=u_x+v_x$ where $u_x \in U, v_x \in V$ If the mappings $P_U(x)=u_x, P_V(x)=v_x$ are continuous then $U,V$ are topologically complementary. Thanks in advance","I’ve shown that if and are topologically  complementary then they are closed on a normed space. Also, I’ve shown that if is a Banach space and are closed complementary subspaces then they are topologically complementary. My question is what if is not Banach for second? I could not find a counter example for complementary subspaces that are closed but not topological. My definition for topologic complements on a normed space : are complementary subspaces. where If the mappings are continuous then are topologically complementary. Thanks in advance","U V X U,V X U,V \subset X \forall x \in X, \quad x=u_x+v_x u_x \in U, v_x \in V P_U(x)=u_x, P_V(x)=v_x U,V","['linear-algebra', 'functional-analysis', 'projection']"
92,"When $A$ is $3\times3$, prove that $A^2=0$ iff $A$ has rank less than or equal one and trace zero","When  is , prove that  iff  has rank less than or equal one and trace zero",A 3\times3 A^2=0 A,"Let $A \in M_3$ . Prove that $A^2=0\Leftrightarrow \operatorname{tr}(A)=0,\operatorname{rank}(A)\le 1$ I can easily prove $\operatorname{tr}(A)=0,\,\operatorname{rank}(A)\le 1 \Rightarrow A^2=0$ since $\operatorname{rank}(A)\le 1 \Rightarrow A^2=\operatorname{tr}(A)A$ . For $A^2=0 \Rightarrow \operatorname{tr}(A)=0, \operatorname{rank}(A)\le 1$ , I prove $\operatorname{tr}(A)=0$ by proving eigenvalues of a nilpotent matrix are zeros, but for $\operatorname{rank}\le 1$ I have no idea. Can anyone give me a hint?","Let . Prove that I can easily prove since . For , I prove by proving eigenvalues of a nilpotent matrix are zeros, but for I have no idea. Can anyone give me a hint?","A \in M_3 A^2=0\Leftrightarrow \operatorname{tr}(A)=0,\operatorname{rank}(A)\le 1 \operatorname{tr}(A)=0,\,\operatorname{rank}(A)\le 1 \Rightarrow A^2=0 \operatorname{rank}(A)\le 1 \Rightarrow A^2=\operatorname{tr}(A)A A^2=0 \Rightarrow \operatorname{tr}(A)=0, \operatorname{rank}(A)\le 1 \operatorname{tr}(A)=0 \operatorname{rank}\le 1","['linear-algebra', 'matrix-rank', 'trace']"
93,What are eigenvectors all about? [duplicate],What are eigenvectors all about? [duplicate],,"This question already has answers here : What is the importance of eigenvalues/eigenvectors? (11 answers) Closed 4 years ago . It's awkward, but to be completely honest: I never understood what eigenvectors are all about. The only reasons I can think of why they could be interesting are: They are calculated to diagonalize a matrix (which is useful, because one can calculate faster with diagonal matrices?). If one has a linear transformation that represents a rotation, an eigenvector would be the rotation axis. Everybody says that they are used ""everywhere"". But I find all of these reasons unsatisfying. The concept of an eigenvector just seems unnatural to me. I associate this concept with a lot of unmotivated calculations. I would love to hear an honest, down-to-earth explanation of why eigenvectors are interesting. Note: I am not asking what eigenvectors are, my question is purely about why a pure mathematician (not interested in numerical calculations) should care.","This question already has answers here : What is the importance of eigenvalues/eigenvectors? (11 answers) Closed 4 years ago . It's awkward, but to be completely honest: I never understood what eigenvectors are all about. The only reasons I can think of why they could be interesting are: They are calculated to diagonalize a matrix (which is useful, because one can calculate faster with diagonal matrices?). If one has a linear transformation that represents a rotation, an eigenvector would be the rotation axis. Everybody says that they are used ""everywhere"". But I find all of these reasons unsatisfying. The concept of an eigenvector just seems unnatural to me. I associate this concept with a lot of unmotivated calculations. I would love to hear an honest, down-to-earth explanation of why eigenvectors are interesting. Note: I am not asking what eigenvectors are, my question is purely about why a pure mathematician (not interested in numerical calculations) should care.",,"['linear-algebra', 'matrices']"
94,Differentiate expressions involving symmetric matrix $\mathbf{D}=\mathrm{diag}(\tau)\Omega\mathrm{diag}(\tau)$ with respect to element of $\tau$,Differentiate expressions involving symmetric matrix  with respect to element of,\mathbf{D}=\mathrm{diag}(\tau)\Omega\mathrm{diag}(\tau) \tau,"I have a square $q$ by $q$ symmetric matrix $\mathbf{D}=\operatorname{diag}(\tau)\Omega\operatorname{diag}(\tau)$ where $\Omega$ is a square $q$ by $q$ matrix, and $\tau$ is a vector of length $q$ . Basically $\mathbf{D}$ is a covariance matrix that I am decomposing into a correlation matrix $\Omega$ and a scale vector $\tau$ . I need to differentiate various functions that contain $\mathbf{D}$ with respect to $\tau_g$ , for $g$ in $1, \ldots, q$ .  Specifically I need to find: $$\frac{d(\log(|\mathbf{D}|)}{d\tau_g} $$ and $$\frac{d(\mathbf{b}^T\mathbf{D}^{-1}\mathbf{b})}{d\tau_g} $$ I've been following the matrix cook book ( https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) but it mentions under the section for derivatives of matrices, that many of the derivaties don't apply in general for matrices with structure (e.g symmetric matrices such as $\mathbf{D}$ ). Based on that, how would I go about finding the above two derivatives with respect to $\tau_g$ of an expression involving $\mathbf{D}$ ?  Any pointers / tips / useful identities would be appreciated.","I have a square by symmetric matrix where is a square by matrix, and is a vector of length . Basically is a covariance matrix that I am decomposing into a correlation matrix and a scale vector . I need to differentiate various functions that contain with respect to , for in .  Specifically I need to find: and I've been following the matrix cook book ( https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) but it mentions under the section for derivatives of matrices, that many of the derivaties don't apply in general for matrices with structure (e.g symmetric matrices such as ). Based on that, how would I go about finding the above two derivatives with respect to of an expression involving ?  Any pointers / tips / useful identities would be appreciated.","q q \mathbf{D}=\operatorname{diag}(\tau)\Omega\operatorname{diag}(\tau) \Omega q q \tau q \mathbf{D} \Omega \tau \mathbf{D} \tau_g g 1, \ldots, q \frac{d(\log(|\mathbf{D}|)}{d\tau_g}  \frac{d(\mathbf{b}^T\mathbf{D}^{-1}\mathbf{b})}{d\tau_g}  \mathbf{D} \tau_g \mathbf{D}","['linear-algebra', 'multivariable-calculus', 'derivatives', 'symmetric-matrices']"
95,Special linear matrices with minimal norm,Special linear matrices with minimal norm,,If $A$ is matrix in $\mathcal M_n(\Bbb R)$ we define the norm $$\Vert A\Vert = \sqrt{\operatorname{Tr}(A^TA)}$$ Now I'm wondering is it true that over the set of special linear matrices $M$ i.e. such that $\det(M)=1$ the matrices with minimal norm are the special orthogonal matrices? In other words if $A$ such that $\det(A)=1$ and $\Vert A\Vert\le \Vert M\Vert$ for all special linear matrix $M$ then $A$ is  special orthogonal i.e. $A^TA=I_n$ . I searched in internet for a proof  but I didn't find any thing.,If is matrix in we define the norm Now I'm wondering is it true that over the set of special linear matrices i.e. such that the matrices with minimal norm are the special orthogonal matrices? In other words if such that and for all special linear matrix then is  special orthogonal i.e. . I searched in internet for a proof  but I didn't find any thing.,A \mathcal M_n(\Bbb R) \Vert A\Vert = \sqrt{\operatorname{Tr}(A^TA)} M \det(M)=1 A \det(A)=1 \Vert A\Vert\le \Vert M\Vert M A A^TA=I_n,"['linear-algebra', 'matrices', 'optimization', 'orthogonal-matrices']"
96,Understanding algorithm for computing eigenvectors of 3x3 symmetric matrices,Understanding algorithm for computing eigenvectors of 3x3 symmetric matrices,,"As part of a project I am doing at work I have to code an eigensolver for symmetric 3x3 matrices. For a number of reasons, I cannot use a library for this task and have to implement the code myself. I have found this excellent paper that contains a suitable algorithm (the non-iterative one) that I can use and found it fairly tractable (my background is in Computer Science, so not an expert in linear algebra). The part where I am getting stuck is where they present an algorithm to compute the second eigenvector $\mathbf{E}$ (corresponding to the second largest eigenvalue) on page 15. They start by creating a right-handed orthonormal basis $\{\mathbf{U},\mathbf{V},\mathbf{W}\}$ containing the already computed eigenvector $\mathbf{W}$ and state that $\mathbf{E}$ has to be a circular combination of $\mathbf{U}$ and $\mathbf{V}$ . So far, so good. The next paragraph (bottom of page 15) is where they lose me. I cannot seem to understand the motivation for what they do next. It also seems to me that they are trying to multiply a 3x2 matrix with a 3x1 vector which isn't possible. Any help in clarifying this would be greatly appreciated. I could just implement the algorithm blindly, but I really don't like the idea of doing so!","As part of a project I am doing at work I have to code an eigensolver for symmetric 3x3 matrices. For a number of reasons, I cannot use a library for this task and have to implement the code myself. I have found this excellent paper that contains a suitable algorithm (the non-iterative one) that I can use and found it fairly tractable (my background is in Computer Science, so not an expert in linear algebra). The part where I am getting stuck is where they present an algorithm to compute the second eigenvector (corresponding to the second largest eigenvalue) on page 15. They start by creating a right-handed orthonormal basis containing the already computed eigenvector and state that has to be a circular combination of and . So far, so good. The next paragraph (bottom of page 15) is where they lose me. I cannot seem to understand the motivation for what they do next. It also seems to me that they are trying to multiply a 3x2 matrix with a 3x1 vector which isn't possible. Any help in clarifying this would be greatly appreciated. I could just implement the algorithm blindly, but I really don't like the idea of doing so!","\mathbf{E} \{\mathbf{U},\mathbf{V},\mathbf{W}\} \mathbf{W} \mathbf{E} \mathbf{U} \mathbf{V}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'algorithms', 'symmetric-matrices']"
97,How to get eigenvectors from eigenvalues,How to get eigenvectors from eigenvalues,,"I'm not seeing where the eigenvectors are in terms of eigenvalues in the paper that came out recently named ""Eigenvectors from Eigenvalues"" by the neutrino physicists and Terence Tao: https://arxiv.org/pdf/1908.03795.pdf I'm guessing that with as fundamentally groundbreaking as this paper is, some people on here have read it.  Please let me know if you know where in the paper the eigenvectors are in terms of eigenvalues. Edit:  To be more specific, I saw the norm squared part, but how do you narrow it down to the actual value of each element of each eigenvector. For any reals, you can just act the transformation on the at most the finite $2^n$ possibilities (unless it has infinite dimensions as can be the case in particle physics), where n is the dimension of the vector. But, what about when the values that the norm square is being taken of are complex? Also, if you can address infinite dimensions, that would be appreciated.  Meaning, is there a way to get the elements without testing each combination?","I'm not seeing where the eigenvectors are in terms of eigenvalues in the paper that came out recently named ""Eigenvectors from Eigenvalues"" by the neutrino physicists and Terence Tao: https://arxiv.org/pdf/1908.03795.pdf I'm guessing that with as fundamentally groundbreaking as this paper is, some people on here have read it.  Please let me know if you know where in the paper the eigenvectors are in terms of eigenvalues. Edit:  To be more specific, I saw the norm squared part, but how do you narrow it down to the actual value of each element of each eigenvector. For any reals, you can just act the transformation on the at most the finite possibilities (unless it has infinite dimensions as can be the case in particle physics), where n is the dimension of the vector. But, what about when the values that the norm square is being taken of are complex? Also, if you can address infinite dimensions, that would be appreciated.  Meaning, is there a way to get the elements without testing each combination?",2^n,"['linear-algebra', 'eigenvalues-eigenvectors']"
98,Why every $(n-2)\times(n-2)$-submatrix of this $(n-2)\times n$ matrix has full rank?,Why every -submatrix of this  matrix has full rank?,(n-2)\times(n-2) (n-2)\times n,"Let $A$ be an $(n-2)\times n$ matrix such that for every $i=1,2,...,n-2$ the $i$ th row consists of $1, -2, 1$ at $i$ th, $(i+1)$ st, and $(i+2)$ nd column, respectively, and all other entries are $0$ . Then $n$ -term arithmetic progressions are some integer solutions of $Ax=0$ . It is said that every $(n-2)\times(n-2)$ submatrix of $A$ has full rank. I am wondering why? I have a feeling that after crossing out two columns except the first or the last one, the remaining square matrix is upper triangular with non-zero diagonal entries. But is there a way to prove it?","Let be an matrix such that for every the th row consists of at th, st, and nd column, respectively, and all other entries are . Then -term arithmetic progressions are some integer solutions of . It is said that every submatrix of has full rank. I am wondering why? I have a feeling that after crossing out two columns except the first or the last one, the remaining square matrix is upper triangular with non-zero diagonal entries. But is there a way to prove it?","A (n-2)\times n i=1,2,...,n-2 i 1, -2, 1 i (i+1) (i+2) 0 n Ax=0 (n-2)\times(n-2) A","['linear-algebra', 'matrices', 'number-theory', 'integers']"
99,Adding and Subtracting Vector Subspaces,Adding and Subtracting Vector Subspaces,,"Let $U$ and $W$ be two vector subspaces of some other arbitrary vector space. We define $U - W = \left\{ \vec{u} - \vec{w} \, \big| \, \vec{u} \in U, \vec{w} \in W \right\}$ . We similarly define $U + W = \left\{ \vec{u} + \vec{w} \, \big| \, \vec{u} \in U, \vec{w} \in W \right\}$ . My question is: Why is $U - U = \left\{ 0 \right\}$ false ? From my understanding, subtracting something from itself always results in zero, no? Similarly, why is $U - W = U + W$ true ? Thanks for the clarification!","Let and be two vector subspaces of some other arbitrary vector space. We define . We similarly define . My question is: Why is false ? From my understanding, subtracting something from itself always results in zero, no? Similarly, why is true ? Thanks for the clarification!","U W U - W = \left\{ \vec{u} - \vec{w} \, \big| \, \vec{u} \in U, \vec{w} \in W \right\} U + W = \left\{ \vec{u} + \vec{w} \, \big| \, \vec{u} \in U, \vec{w} \in W \right\} U - U = \left\{ 0 \right\} U - W = U + W","['linear-algebra', 'vector-spaces']"
