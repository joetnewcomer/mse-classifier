,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Deriving the Covariance of Multivariate Gaussian,Deriving the Covariance of Multivariate Gaussian,,"I've been having trouble figuring out how to directly prove that the covariance of the multivariate Gaussian distribution $p(x) = \dfrac{1}{{{(2\pi)}^{\frac{n}{2}}}|\Sigma|^{\frac{1}{2}}}\exp\{{-\dfrac{1}{2}(x - \mu)^T \Sigma^{-1}(x-\mu)}\}$ where $x \in \mathbb{R}^n$ and $\Sigma$ is positive definite, is actually $\Sigma$. That is to say, given $\Sigma$ and the above density function, I want to prove that the covariance is $\Sigma$. I've been able to prove that the mean is $\mu$ by diagonalizing the matrix $\Sigma$ and integrating, but I'm struggling to do it for the proof that the covariance is $\Sigma$. Can anyone help?","I've been having trouble figuring out how to directly prove that the covariance of the multivariate Gaussian distribution $p(x) = \dfrac{1}{{{(2\pi)}^{\frac{n}{2}}}|\Sigma|^{\frac{1}{2}}}\exp\{{-\dfrac{1}{2}(x - \mu)^T \Sigma^{-1}(x-\mu)}\}$ where $x \in \mathbb{R}^n$ and $\Sigma$ is positive definite, is actually $\Sigma$. That is to say, given $\Sigma$ and the above density function, I want to prove that the covariance is $\Sigma$. I've been able to prove that the mean is $\mu$ by diagonalizing the matrix $\Sigma$ and integrating, but I'm struggling to do it for the proof that the covariance is $\Sigma$. Can anyone help?",,"['probability', 'probability-theory', 'probability-distributions']"
1,Probability of one Poisson variable being greater than another,Probability of one Poisson variable being greater than another,,"Given two Poisson distributions with different λ values, if each were to produce a single random variable, is there closed-form expression for calculating the probability of one random variable being greater than the other?","Given two Poisson distributions with different λ values, if each were to produce a single random variable, is there closed-form expression for calculating the probability of one random variable being greater than the other?",,"['probability', 'poisson-distribution']"
2,Expected number of rolls to get all colors on 6-sided die colored in with 3 colors,Expected number of rolls to get all colors on 6-sided die colored in with 3 colors,,"If I have a die that has 3 red sides, 2 blue sides, and 1 green side, how many rolls do I expect until every color has appeared at least once? I have run some tests and I’m getting numbers around 7.31, but clearly I’m looking for a mathematical solution. Thanks in advance.","If I have a die that has 3 red sides, 2 blue sides, and 1 green side, how many rolls do I expect until every color has appeared at least once? I have run some tests and I’m getting numbers around 7.31, but clearly I’m looking for a mathematical solution. Thanks in advance.",,['probability']
3,"If a random variable is independent from the two components of a random vector, are the random vector and the random variable independent?","If a random variable is independent from the two components of a random vector, are the random vector and the random variable independent?",,"in my probability class I was asked this seemingly very tricky question dealing with random variables and vectors: Let $ X,Y,Z $ be random variables with PDFs (continuous) such that we know that Z and X are independent and the variables Z and Y are independent. We are asked to prove or disprove (give a counterexample) that the random vector $ (X,Y) $ and $ Z $ are independent. I have tried to prove it just with the basic identities and definitions but got nothing so maybe it is false and we must give a counterexample? I do not even know how to deal with this, so I really need the help. Thanks all helpers.","in my probability class I was asked this seemingly very tricky question dealing with random variables and vectors: Let $ X,Y,Z $ be random variables with PDFs (continuous) such that we know that Z and X are independent and the variables Z and Y are independent. We are asked to prove or disprove (give a counterexample) that the random vector $ (X,Y) $ and $ Z $ are independent. I have tried to prove it just with the basic identities and definitions but got nothing so maybe it is false and we must give a counterexample? I do not even know how to deal with this, so I really need the help. Thanks all helpers.",,"['probability', 'random-variables', 'independence']"
4,Probability and exit polls,Probability and exit polls,,"I have a very simple probability question that I for some reason just can not solve. Question: Consider an election with two candidates, Candidate A and Candidate B. Every voter is invited to participate in an exit poll, where they are asked whom they voted for; some accept and some refuse. For a randomly selected voter, let A be the event that they voted for A, and W the event that they are willing to participate in the exit poll. Suppose that $P(W \mid A)=0,7$ but $P(W \mid A^C)=0,3$. In the exit poll, 60% of the respondents say they voted for A (assuming they are all honest), suggesting a comfortable victory for A. Find $P(A)$. Okay first we notice that $A,A^c$ obviously is a partition so we can use the total low of probability. getting \begin{align*} P(W) & = P(W\mid A) \cdot P(A)+P(W \mid A^c) \cdot P(A^c)\\      & = P(W\mid A) \cdot P(A)+P(W \mid A^c)\cdot (1-P(A))\\      & =0,7 \cdot P(A)+0,3\cdot (1-P(A)) \end{align*} Thus all I need is to find $P(W)$ and then solve for $P(A)$. However I have issue with finding $P(W)$. Any help?","I have a very simple probability question that I for some reason just can not solve. Question: Consider an election with two candidates, Candidate A and Candidate B. Every voter is invited to participate in an exit poll, where they are asked whom they voted for; some accept and some refuse. For a randomly selected voter, let A be the event that they voted for A, and W the event that they are willing to participate in the exit poll. Suppose that $P(W \mid A)=0,7$ but $P(W \mid A^C)=0,3$. In the exit poll, 60% of the respondents say they voted for A (assuming they are all honest), suggesting a comfortable victory for A. Find $P(A)$. Okay first we notice that $A,A^c$ obviously is a partition so we can use the total low of probability. getting \begin{align*} P(W) & = P(W\mid A) \cdot P(A)+P(W \mid A^c) \cdot P(A^c)\\      & = P(W\mid A) \cdot P(A)+P(W \mid A^c)\cdot (1-P(A))\\      & =0,7 \cdot P(A)+0,3\cdot (1-P(A)) \end{align*} Thus all I need is to find $P(W)$ and then solve for $P(A)$. However I have issue with finding $P(W)$. Any help?",,['probability']
5,"Help with understanding point from Nassim Taleb's book ""Dynamic Hedging""","Help with understanding point from Nassim Taleb's book ""Dynamic Hedging""",,"This is an excerpt from Nassim Taleb's book ""Dynamic Hedging"" (a book on option trading strategies) page vii Most examples in this book are presented as generic situations. The   volatility will be defined as 15.7% (to make one standard deviation   equal to 1% daily move). I'm trying to understand where the numbers 15.7% and 1% came from. How were they derived or estimated? Edit: Adding more context, here's a screen cap of the actual text:","This is an excerpt from Nassim Taleb's book ""Dynamic Hedging"" (a book on option trading strategies) page vii Most examples in this book are presented as generic situations. The   volatility will be defined as 15.7% (to make one standard deviation   equal to 1% daily move). I'm trying to understand where the numbers 15.7% and 1% came from. How were they derived or estimated? Edit: Adding more context, here's a screen cap of the actual text:",,"['probability', 'probability-distributions', 'standard-deviation']"
6,The chance of the word to be of all different letters?,The chance of the word to be of all different letters?,,"A four letter word is written down by taking letters from the word KANYAKUMARI.What is the chance of the word to be of all different letters? Since KANYAKUMARI has K,K,A,A,A,N,Y,U,M,R,I letters in which there are 2 K's and 3 A's.The chance that the four letter word to be of all different letters$=\frac{m}{n}$,where $m=\binom{8}{4}$,because there are 8 different letters K,A,N,Y,U,M,R,I.and $n=\binom{11}{4}$.So probability$=\frac{7}{33}$,but the correct answer is $\frac{840}{1109}$.I dont know where i have made the mistake.Please help me.","A four letter word is written down by taking letters from the word KANYAKUMARI.What is the chance of the word to be of all different letters? Since KANYAKUMARI has K,K,A,A,A,N,Y,U,M,R,I letters in which there are 2 K's and 3 A's.The chance that the four letter word to be of all different letters$=\frac{m}{n}$,where $m=\binom{8}{4}$,because there are 8 different letters K,A,N,Y,U,M,R,I.and $n=\binom{11}{4}$.So probability$=\frac{7}{33}$,but the correct answer is $\frac{840}{1109}$.I dont know where i have made the mistake.Please help me.",,"['probability', 'combinatorics']"
7,Why this probability was calculated using Binomial Distribution?,Why this probability was calculated using Binomial Distribution?,,"The following is an exercise in this book ( Discrete-Event System Simulation - Fourth Edition ). Exercise 5.3 A recent survey indicated that 82% of single women aged 25 years old will be married in their lifetime. Using the binomial distribution, find the probability that two or three women in a sample of twenty will never be married. Solution (From the book's solution manual) Let X be defined as the number of women in the sample never married P(2 ≤ X ≤ 3) = p(2) + p(3) = $ \binom{20}{2} (.18)^2 (.82)^{18} + \binom{20}{3} (.18)^3 (.82)^{17} $ = .173 + .228 = .401 My Question If I understand it correctly, the binomial distribution is a discrete probability distribution of a number of successes in a sequence of n independent yes/no experiments. But choosing 2 (or 3) women from a 20-women sample is not independent experiments, because choosing the first woman will affect the probability for the coming experiments. Why the binomial distribution was used here ?","The following is an exercise in this book ( Discrete-Event System Simulation - Fourth Edition ). Exercise 5.3 A recent survey indicated that 82% of single women aged 25 years old will be married in their lifetime. Using the binomial distribution, find the probability that two or three women in a sample of twenty will never be married. Solution (From the book's solution manual) Let X be defined as the number of women in the sample never married P(2 ≤ X ≤ 3) = p(2) + p(3) = = .173 + .228 = .401 My Question If I understand it correctly, the binomial distribution is a discrete probability distribution of a number of successes in a sequence of n independent yes/no experiments. But choosing 2 (or 3) women from a 20-women sample is not independent experiments, because choosing the first woman will affect the probability for the coming experiments. Why the binomial distribution was used here ?", \binom{20}{2} (.18)^2 (.82)^{18} + \binom{20}{3} (.18)^3 (.82)^{17} ,"['probability', 'probability-distributions', 'binomial-distribution']"
8,Probability of A given B given C,Probability of A given B given C,,"How would I find P ((A|B)|C)? Do I substitute the formula for P (A|B), P=(A ∩ B))/(P (B)), and then redo the function to get (P (A ∩ B ∩ C))/(P (C)^2)? I heard that P=(A|B ∩ C) can be used, but why would that be equivalent, unless you assume that C is independent of A?  Given a three-circle Venn diagram, the initial sample size would be too restricted.","How would I find P ((A|B)|C)? Do I substitute the formula for P (A|B), P=(A ∩ B))/(P (B)), and then redo the function to get (P (A ∩ B ∩ C))/(P (C)^2)? I heard that P=(A|B ∩ C) can be used, but why would that be equivalent, unless you assume that C is independent of A?  Given a three-circle Venn diagram, the initial sample size would be too restricted.",,['probability']
9,How to calculate the expected maximum tree size in a pseudoforest,How to calculate the expected maximum tree size in a pseudoforest,,"I would like to calculate the expected maximum tree size in a randomly generated pseudoforest of $N$ labelled nodes where self-loops are not permitted. Empty and single-node trees are also not permitted. For example, if we have $4$ labelled nodes, we can generate $3$ pseudoforests with a largest tree size of $2$, and $78$ pseudoforests with a maximum tree size of $4$. There are a total of $(n-1)^n$ possible pseudoforests, thus for $N = 4$ there are $81$. The expected maximum tree size for $N = 4$ would therefore be: $$ E(x) = \sum_{i=1}^{n}i\cdot p(i) = 2 \cdot \frac{3}{81} + 4\cdot\frac{78}{81} = 3.925... $$ Some observations: There will never be a pseudoforest where the maximum tree size is $n-1$. The number of pseudoforests of $N$ nodes containing only one connected tree (therefore maximum tree size of $N$) can be calculated using sequences $A000435$ or $A001864 / n$. For $N = 4$, this gives us $78$, ie. when $i = n$ in the summation. The minimum tree size is $2$ if $N$ is even, and $3$ if $N$ is odd. The sum of the numerators of $p(i)$ is equal to $(n-1)^n$ When $N = 5$, the summation is: $$ 3 \cdot \frac{80}{1024} + 5\cdot\frac{944}{1024} = 4.84375 $$ When $N = 6$, the summation is: $$ 2 \cdot \frac{15}{15625} + 3\cdot\frac{640}{15625} + 4\cdot\frac{1170}{15625} + 6\cdot\frac{13800}{15625} = 5.72352 $$ How can I calculate the numerators of $p(i)$ when $i < n$?","I would like to calculate the expected maximum tree size in a randomly generated pseudoforest of $N$ labelled nodes where self-loops are not permitted. Empty and single-node trees are also not permitted. For example, if we have $4$ labelled nodes, we can generate $3$ pseudoforests with a largest tree size of $2$, and $78$ pseudoforests with a maximum tree size of $4$. There are a total of $(n-1)^n$ possible pseudoforests, thus for $N = 4$ there are $81$. The expected maximum tree size for $N = 4$ would therefore be: $$ E(x) = \sum_{i=1}^{n}i\cdot p(i) = 2 \cdot \frac{3}{81} + 4\cdot\frac{78}{81} = 3.925... $$ Some observations: There will never be a pseudoforest where the maximum tree size is $n-1$. The number of pseudoforests of $N$ nodes containing only one connected tree (therefore maximum tree size of $N$) can be calculated using sequences $A000435$ or $A001864 / n$. For $N = 4$, this gives us $78$, ie. when $i = n$ in the summation. The minimum tree size is $2$ if $N$ is even, and $3$ if $N$ is odd. The sum of the numerators of $p(i)$ is equal to $(n-1)^n$ When $N = 5$, the summation is: $$ 3 \cdot \frac{80}{1024} + 5\cdot\frac{944}{1024} = 4.84375 $$ When $N = 6$, the summation is: $$ 2 \cdot \frac{15}{15625} + 3\cdot\frac{640}{15625} + 4\cdot\frac{1170}{15625} + 6\cdot\frac{13800}{15625} = 5.72352 $$ How can I calculate the numerators of $p(i)$ when $i < n$?",,"['probability', 'sequences-and-series', 'graph-theory', 'trees']"
10,Probability of exactly one empty box when n balls are randomly placed in n boxes. [duplicate],Probability of exactly one empty box when n balls are randomly placed in n boxes. [duplicate],,"This question already has answers here : Probability: $n$ balls into $n$ holes with exactly one hole remaining empty [duplicate] (4 answers) Closed 9 years ago . Each of $n$ balls is independently placed into one of $n$ boxes, with all boxes equally likely. What is the probability that exactly one box is empty? (Introduction to Probability, Blitzstein and Nwang, p.36). The number of possible permutations with replacement is $n^n$ In order to have one empty box, we need a different box having $2$ balls in it. We have $\dbinom{n}{1}$ choices for the empty box, $\dbinom{n-1}{1}$ choices left for the box with $2$ balls, and $(n-2)!$ permutations to assign the remaining balls to the remaining boxes. Result: $$\frac{\dbinom{n}{1} \dbinom{n-1}{1} (n-2)!}{n^n}$$ Is this correct?","This question already has answers here : Probability: $n$ balls into $n$ holes with exactly one hole remaining empty [duplicate] (4 answers) Closed 9 years ago . Each of $n$ balls is independently placed into one of $n$ boxes, with all boxes equally likely. What is the probability that exactly one box is empty? (Introduction to Probability, Blitzstein and Nwang, p.36). The number of possible permutations with replacement is $n^n$ In order to have one empty box, we need a different box having $2$ balls in it. We have $\dbinom{n}{1}$ choices for the empty box, $\dbinom{n-1}{1}$ choices left for the box with $2$ balls, and $(n-2)!$ permutations to assign the remaining balls to the remaining boxes. Result: $$\frac{\dbinom{n}{1} \dbinom{n-1}{1} (n-2)!}{n^n}$$ Is this correct?",,"['probability', 'combinatorics']"
11,Probability of passing this multiple choice exam [duplicate],Probability of passing this multiple choice exam [duplicate],,"This question already has answers here : Probability in multiple choice exams (2 answers) Closed 9 years ago . A multiple choice exam has 175 questions. Each question has 4 possible answers. Only 1 answer out of the 4 possible answers is correct. The pass rate for the exam is 70% (123 questions must be answered correctly). We know for a fact that 100 questions were answered correctly. Questions: What is the probability of passing the exam, if one were to guess on the remaining 75 questions? That is, pick at random one of the 4 answers for each of the 75 questions.","This question already has answers here : Probability in multiple choice exams (2 answers) Closed 9 years ago . A multiple choice exam has 175 questions. Each question has 4 possible answers. Only 1 answer out of the 4 possible answers is correct. The pass rate for the exam is 70% (123 questions must be answered correctly). We know for a fact that 100 questions were answered correctly. Questions: What is the probability of passing the exam, if one were to guess on the remaining 75 questions? That is, pick at random one of the 4 answers for each of the 75 questions.",,['probability']
12,Cashier has no change... catalan numbers.. probability question,Cashier has no change... catalan numbers.. probability question,,"I think this question uses catalan numbers.. but I don't know exactly how to answer it... its not homework or anything but I need to understand how to do it.. I tried drawing up likes for each 5r dolla bill the cashier has and down lines for each one he gave as change.. attempting to create ""mountains"" but the possibilities become very large extremely quickly which is giving me trouble creating an equation.. anyhow here is the question... 75 people have 5 dollar bills and 25 people have 10 dollar bills, and they are in line at a ticket counter which has no money and which charges 5 dollars for admission. If a 10 dollar bill is presented and there is no change, the line stops. What is the probability that the ticket seller always (after the first person in line, that is) has at least one 5 dollar bill for change? As above, what is the probability the ticket seller was always able to make change. What is the probability that the ticket seller was always able to make change if he is now allowed to use his own, single 5 dollar bill, if needed?","I think this question uses catalan numbers.. but I don't know exactly how to answer it... its not homework or anything but I need to understand how to do it.. I tried drawing up likes for each 5r dolla bill the cashier has and down lines for each one he gave as change.. attempting to create ""mountains"" but the possibilities become very large extremely quickly which is giving me trouble creating an equation.. anyhow here is the question... 75 people have 5 dollar bills and 25 people have 10 dollar bills, and they are in line at a ticket counter which has no money and which charges 5 dollars for admission. If a 10 dollar bill is presented and there is no change, the line stops. What is the probability that the ticket seller always (after the first person in line, that is) has at least one 5 dollar bill for change? As above, what is the probability the ticket seller was always able to make change. What is the probability that the ticket seller was always able to make change if he is now allowed to use his own, single 5 dollar bill, if needed?",,"['probability', 'catalan-numbers']"
13,correspondence between balls in compartments and integer vectors,correspondence between balls in compartments and integer vectors,,"I'm doing a self-review of probability and working through Ross' Introduction to Probability. The question is (Ross, ch2 number 51): suppose $n$ balls are randomly distributed into $N$ compartments.  Find the probability that $m$ balls fall in the first compartment, assuming all $N^n$ arrangements are equally likely. I solved this two ways -- it's $$\frac{ \displaystyle{n \choose m} (N-1)^{n-m}}{ N^n }.$$  Easy enough.  You can also think of this as a bernoulli trial w/ prob $$p = \frac{1}{N}$$ so that $$P(k=m) = \displaystyle{n \choose m} p^m (1-p)^{n-m} = \displaystyle{ n \choose m } \frac{1}{N}^m \left(\frac{N-1}{N}\right)^{n-m}$$ and those are equal.  So my answer is probably right. My question is Ross also mentions a formula for the number of nonnegative integer solutions to  $$x_1 + x_2 + \ldots + x_r = n;$$ there are  $$\displaystyle{n+r-1 \choose r-1}$$ solutions.  So I feel like the answer to this problem should also be the number of solutions to  $$x_2 + \ldots + x_N = n - m$$  divided by the number of solutions to  $$x_1 + \ldots + x_N = n,$$ ie $$ \frac{\displaystyle{ n - m + (N-1) - 1 \choose (N-1) - 1}}{\displaystyle{n + N - 1 \choose N - 1}}  = \frac{\displaystyle{ n - m + N-2 \choose N-2}}{\displaystyle{n + N - 1 \choose N - 1}}   $$ but it's not -- I couldn't make this factory correctly, so I tried it with a random choice of numbers for $n,N,m$ and it's definitely not equal.  My question is why not?  There should be a correspondence between balls in compartments and the number of nonnegative vectors $x_1 + \ldots + x_N = n$, right?","I'm doing a self-review of probability and working through Ross' Introduction to Probability. The question is (Ross, ch2 number 51): suppose $n$ balls are randomly distributed into $N$ compartments.  Find the probability that $m$ balls fall in the first compartment, assuming all $N^n$ arrangements are equally likely. I solved this two ways -- it's $$\frac{ \displaystyle{n \choose m} (N-1)^{n-m}}{ N^n }.$$  Easy enough.  You can also think of this as a bernoulli trial w/ prob $$p = \frac{1}{N}$$ so that $$P(k=m) = \displaystyle{n \choose m} p^m (1-p)^{n-m} = \displaystyle{ n \choose m } \frac{1}{N}^m \left(\frac{N-1}{N}\right)^{n-m}$$ and those are equal.  So my answer is probably right. My question is Ross also mentions a formula for the number of nonnegative integer solutions to  $$x_1 + x_2 + \ldots + x_r = n;$$ there are  $$\displaystyle{n+r-1 \choose r-1}$$ solutions.  So I feel like the answer to this problem should also be the number of solutions to  $$x_2 + \ldots + x_N = n - m$$  divided by the number of solutions to  $$x_1 + \ldots + x_N = n,$$ ie $$ \frac{\displaystyle{ n - m + (N-1) - 1 \choose (N-1) - 1}}{\displaystyle{n + N - 1 \choose N - 1}}  = \frac{\displaystyle{ n - m + N-2 \choose N-2}}{\displaystyle{n + N - 1 \choose N - 1}}   $$ but it's not -- I couldn't make this factory correctly, so I tried it with a random choice of numbers for $n,N,m$ and it's definitely not equal.  My question is why not?  There should be a correspondence between balls in compartments and the number of nonnegative vectors $x_1 + \ldots + x_N = n$, right?",,['probability']
14,Probability Puzzle: Mutating Loaded Die,Probability Puzzle: Mutating Loaded Die,,"Take an (initially) fair six-sided die (i.e. $P(x)=\frac{1}{6}$ for $x=1,…,6$) and roll it repeatedly. After each roll, the die becomes loaded for the next roll depending on the number $y$ that was just rolled according to the following system: $$P(y)=\frac{1}{y}$$ $$P(x)=\frac{1 - P(y)}{5} \text{, for } x \ne y$$ i.e. the probability that you roll that number again in the next roll is $\frac{1}{y}$ and the remaining numbers are of equal probability. What is the probability that you roll a $6$ on your $n$th roll? NB: This is not a homework or contest question, just an idea I had on a boring bus ride. Bonus points for calculating the probability of rolling the number $x$ on the $n$th roll.","Take an (initially) fair six-sided die (i.e. $P(x)=\frac{1}{6}$ for $x=1,…,6$) and roll it repeatedly. After each roll, the die becomes loaded for the next roll depending on the number $y$ that was just rolled according to the following system: $$P(y)=\frac{1}{y}$$ $$P(x)=\frac{1 - P(y)}{5} \text{, for } x \ne y$$ i.e. the probability that you roll that number again in the next roll is $\frac{1}{y}$ and the remaining numbers are of equal probability. What is the probability that you roll a $6$ on your $n$th roll? NB: This is not a homework or contest question, just an idea I had on a boring bus ride. Bonus points for calculating the probability of rolling the number $x$ on the $n$th roll.",,['probability']
15,Multiplication of odds vs. multiplication of probabilities,Multiplication of odds vs. multiplication of probabilities,,"I always believed that probabilities could be multiplied, until I encountered a statement in Machine Learning by Peter Flach about odds: ""Bayes’ rule tells us that we should simply multiply them: 1:6 times 4:1 is 4:6 , corresponding to a spam probability of $0.4$ ."" But converting to probabilities, $1:6 = 1/7$ and $4:1 = 4/5$ . Multiplying the probabilities we get $4/35$ ~ $= 0.11$ Does Bayes' Theorem work differently for odds and probabilities? What seems to be the problem?","I always believed that probabilities could be multiplied, until I encountered a statement in Machine Learning by Peter Flach about odds: ""Bayes’ rule tells us that we should simply multiply them: 1:6 times 4:1 is 4:6 , corresponding to a spam probability of ."" But converting to probabilities, and . Multiplying the probabilities we get ~ Does Bayes' Theorem work differently for odds and probabilities? What seems to be the problem?",0.4 1:6 = 1/7 4:1 = 4/5 4/35 = 0.11,"['probability', 'statistics', 'bayes-theorem']"
16,Rolling a dice until we have $5$ and an even number.,Rolling a dice until we have  and an even number.,5,"We roll a dice, until we have both: a five and some even number. Let X be the expected number of rolls.  Find expected value of X and Var(X). So I don't know how to begin. I think there should be something like $X= X_1 + X_2$ And then I would use geometric distribution. But how to divide X into easier events?  I thought about situations like rolling dice until we have both even and odd number. It's easy, because probability of obtaing an odd number is the same as obtaining even number. The same situation - if we roll until we get $5$ and, let me say, $3$. There is no problem since getting $5$ has the same probability as getting $3$.  But getting a $5$ and getting an even number have different proabilities, so it's not so easy... Can somebody help?","We roll a dice, until we have both: a five and some even number. Let X be the expected number of rolls.  Find expected value of X and Var(X). So I don't know how to begin. I think there should be something like $X= X_1 + X_2$ And then I would use geometric distribution. But how to divide X into easier events?  I thought about situations like rolling dice until we have both even and odd number. It's easy, because probability of obtaing an odd number is the same as obtaining even number. The same situation - if we roll until we get $5$ and, let me say, $3$. There is no problem since getting $5$ has the same probability as getting $3$.  But getting a $5$ and getting an even number have different proabilities, so it's not so easy... Can somebody help?",,['probability']
17,Probability of 100 coin tosses,Probability of 100 coin tosses,,"I was wondering if I could get some help on this problem. Suppose that a fair coin is tossed 100 times. Find the probability of observing at least 60 heads. Thanks! Note: on the study guide I did ""1-binomcdf(100,0.50,59)"" and still got it wrong Also, this table is included. This is what it looks like. It seems I got it right, but my professor does not approve of my methods. Note the crossed out ""-1/2"" and ""OK"" EDIT: Okay, so the answer is 0.0287.  I got that by... 1-P(X* < 9.5/5) =1-P(X* < 1.9) Then look at the table to find 1.9.  =1-(0.9713) =0.0287","I was wondering if I could get some help on this problem. Suppose that a fair coin is tossed 100 times. Find the probability of observing at least 60 heads. Thanks! Note: on the study guide I did ""1-binomcdf(100,0.50,59)"" and still got it wrong Also, this table is included. This is what it looks like. It seems I got it right, but my professor does not approve of my methods. Note the crossed out ""-1/2"" and ""OK"" EDIT: Okay, so the answer is 0.0287.  I got that by... 1-P(X* < 9.5/5) =1-P(X* < 1.9) Then look at the table to find 1.9.  =1-(0.9713) =0.0287",,['probability']
18,expectation equations,expectation equations,,"I am just trying to understand the following three equations. $\phi(x)$ denotes the standard Gaussian cumulative distribution function and $X$~$N(\mu,\sigma^2)$ (1) $\mathbb{E}[e^{tX}f(X)]=e^{\mu t+\frac{\sigma^2 t^2}{2}}\mathbb{E}f(X+t\sigma^2)$ for all real $t$ and suitable $f$ (2) For any nice function $f$, $\mathbb{E}[f(X)(X-\mu)]=\sigma^2\mathbb{E}[f'(X)]$ (3) $\mathbb{E}\phi(aX+b)=\phi(\frac{a\mu+b}{\sqrt{1+a^2\sigma^2}})$ My approach to see the equality: In (1) I just used the definition of $\mathbb{E}$, therefore $\mathbb{E}[e^{tX}f(X)]=\int_{-\infty}^{\infty}e^{tx}f(x)p(x)dx$ where $p(x)$ is the probability density function, $p(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$. The first term on the right hand side $e^{\mu t+\frac{\sigma^2 t^2}{2}}$ is the moment generating function and the result of $\int_{-\infty}^{\infty}e^{tx}p(x)dx$. How can I derive the second term on the right hand side? I do not know how to handle $f(x)$ in thee integral equation. In (2) I want to prove the quation $\int_{-\infty}^{\infty}(x-\mu)f(x)p(x)dx=\sigma^2\int_{-\infty}^{\infty}f'(x)p(x)dx$. Even if I simplify the LHS I do not see the relation. In (3) the LH states $\int_{-\infty}^{\infty}\phi(ax+b)p(x)dx=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}(\int_{-\infty}^{ax+b}e^{-\frac{z^2}{2}}dz)p(x)dx=?$ Maybe a coordinate tranform helps out, $ax+b=c$","I am just trying to understand the following three equations. $\phi(x)$ denotes the standard Gaussian cumulative distribution function and $X$~$N(\mu,\sigma^2)$ (1) $\mathbb{E}[e^{tX}f(X)]=e^{\mu t+\frac{\sigma^2 t^2}{2}}\mathbb{E}f(X+t\sigma^2)$ for all real $t$ and suitable $f$ (2) For any nice function $f$, $\mathbb{E}[f(X)(X-\mu)]=\sigma^2\mathbb{E}[f'(X)]$ (3) $\mathbb{E}\phi(aX+b)=\phi(\frac{a\mu+b}{\sqrt{1+a^2\sigma^2}})$ My approach to see the equality: In (1) I just used the definition of $\mathbb{E}$, therefore $\mathbb{E}[e^{tX}f(X)]=\int_{-\infty}^{\infty}e^{tx}f(x)p(x)dx$ where $p(x)$ is the probability density function, $p(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$. The first term on the right hand side $e^{\mu t+\frac{\sigma^2 t^2}{2}}$ is the moment generating function and the result of $\int_{-\infty}^{\infty}e^{tx}p(x)dx$. How can I derive the second term on the right hand side? I do not know how to handle $f(x)$ in thee integral equation. In (2) I want to prove the quation $\int_{-\infty}^{\infty}(x-\mu)f(x)p(x)dx=\sigma^2\int_{-\infty}^{\infty}f'(x)p(x)dx$. Even if I simplify the LHS I do not see the relation. In (3) the LH states $\int_{-\infty}^{\infty}\phi(ax+b)p(x)dx=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}(\int_{-\infty}^{ax+b}e^{-\frac{z^2}{2}}dz)p(x)dx=?$ Maybe a coordinate tranform helps out, $ax+b=c$",,"['probability', 'normal-distribution', 'expectation']"
19,"Combinatorics problem on $20$ people, $12$ months, and distinguishable groups","Combinatorics problem on  people,  months, and distinguishable groups",20 12,"Given $20$ people, what is the probability that, among the $12$ months   in the year, there are $4$ months containing exactly $2$ birthdays and   $4$ months containing exactly $3$ birthdays? I am less concerned about knowing the answer than making sure I have thought about the problem correctly. The general problem seems to be finding the distinct integer-valued vectors that satisfy $$x_1 + x_2 + ... x_{12} = 20\ \ \ \ \ \ \ \ \ x_i \ge 0, i = 1, ..., 12$$ It can be proven that there are $\binom{20+12-1}{12-1}$ such vectors. I am presuming this to be the sample space.  (Please correct me if I'm wrong) $4\cdot 2 + 4\cdot 3 + 4 \cdot 0= 20$, so there are $3$ distinct groups of $4$ months. Therefore we have $\binom{12}{4\ 4\ 4}$ possible ways to get the desired distribution of months. The probability I get is $$\frac{\binom{12}{4\ 4\ 4}}{\binom{20+12-1}{12-1}} $$. The answer in the back of the textbook is $1.0604 \times 10^{-3}$ so my answer is off. I just want to know where I went wrong.","Given $20$ people, what is the probability that, among the $12$ months   in the year, there are $4$ months containing exactly $2$ birthdays and   $4$ months containing exactly $3$ birthdays? I am less concerned about knowing the answer than making sure I have thought about the problem correctly. The general problem seems to be finding the distinct integer-valued vectors that satisfy $$x_1 + x_2 + ... x_{12} = 20\ \ \ \ \ \ \ \ \ x_i \ge 0, i = 1, ..., 12$$ It can be proven that there are $\binom{20+12-1}{12-1}$ such vectors. I am presuming this to be the sample space.  (Please correct me if I'm wrong) $4\cdot 2 + 4\cdot 3 + 4 \cdot 0= 20$, so there are $3$ distinct groups of $4$ months. Therefore we have $\binom{12}{4\ 4\ 4}$ possible ways to get the desired distribution of months. The probability I get is $$\frac{\binom{12}{4\ 4\ 4}}{\binom{20+12-1}{12-1}} $$. The answer in the back of the textbook is $1.0604 \times 10^{-3}$ so my answer is off. I just want to know where I went wrong.",,"['probability', 'combinatorics']"
20,Prove the Multiplication Rule (Conditional Form) with more than two events.,Prove the Multiplication Rule (Conditional Form) with more than two events.,,"Prove the Multiplication Rule (Conditional Form) with more than two events. For events $A_1, A_2,\ldots, A_n$ prove that $$ P(A_1 \cap A_2 \cap\ldots\cap A_n)= P(A_1)\ P(A_2|A_1)\ P(A_3|A_1 \cap A_2)\ \ldots\ P(A_n|A_1 \cap A_2 \cap ...  \cap\ A_{n-1}). $$ My first attempt was to try induction and I do get through the first two induction steps but I am not getting the answer when trying to prove for all $n$ Any help would be highly appreciated Thanks","Prove the Multiplication Rule (Conditional Form) with more than two events. For events $A_1, A_2,\ldots, A_n$ prove that $$ P(A_1 \cap A_2 \cap\ldots\cap A_n)= P(A_1)\ P(A_2|A_1)\ P(A_3|A_1 \cap A_2)\ \ldots\ P(A_n|A_1 \cap A_2 \cap ...  \cap\ A_{n-1}). $$ My first attempt was to try induction and I do get through the first two induction steps but I am not getting the answer when trying to prove for all $n$ Any help would be highly appreciated Thanks",,['probability']
21,What to bid for this treasure chest? (puzzle),What to bid for this treasure chest? (puzzle),,"Suppose you are given the opportunity to bid for a treasure chest, which you know to be priced anywhere between 0-1000 dollars inclusive). Treasure price is uniformly distributed. If you bid equal to or above the price, you win the treasure chest (at the cost of your bid). If you bid below the price, you do not earn the treasure chest and get your money back. Now, also suppose you have a friend who is willing to buy the treasure chest from you for 1.5 times the price of the treasure chest (should you obtain the chest). What should your bid be? I don't know how to approach the problem. How can I calculate expected value?","Suppose you are given the opportunity to bid for a treasure chest, which you know to be priced anywhere between 0-1000 dollars inclusive). Treasure price is uniformly distributed. If you bid equal to or above the price, you win the treasure chest (at the cost of your bid). If you bid below the price, you do not earn the treasure chest and get your money back. Now, also suppose you have a friend who is willing to buy the treasure chest from you for 1.5 times the price of the treasure chest (should you obtain the chest). What should your bid be? I don't know how to approach the problem. How can I calculate expected value?",,"['probability', 'puzzle']"
22,Probability of an even number,Probability of an even number,,"I have been having problems solving the following problem. Two real numbers $x$ and $y$ are chosen uniformly at random in the interval $(0,1)$. What is the probability that the closest integer to $x/y$ is even? I wrote a computer program to estimate the answer but I can't see how to solve it without one.","I have been having problems solving the following problem. Two real numbers $x$ and $y$ are chosen uniformly at random in the interval $(0,1)$. What is the probability that the closest integer to $x/y$ is even? I wrote a computer program to estimate the answer but I can't see how to solve it without one.",,[]
23,how to prove $\sum _{|k|\lt\sqrt m}\binom{2m}{m+k}\ge2^{2m-1}$,how to prove,\sum _{|k|\lt\sqrt m}\binom{2m}{m+k}\ge2^{2m-1},"how to prove $$\sum _{|k|\lt\sqrt m}\binom{2m}{m+k}\ge2^{2m-1},\forall m\ge1$$ Thanks in advance .","how to prove $$\sum _{|k|\lt\sqrt m}\binom{2m}{m+k}\ge2^{2m-1},\forall m\ge1$$ Thanks in advance .",,"['probability', 'combinatorics', 'contest-math']"
24,How come i can't compute the expected value using the MGF for uniform distribution?,How come i can't compute the expected value using the MGF for uniform distribution?,,"I thought that $M^r(0) = E[X^r]$, but for a uniform distribution the MGF is $\dfrac{e^{bt}-e^{at}}{(b-a)t}$, so there already is a singularity at $t=0$. So it would seem $M'(0) \neq E[X]$ Why is that?","I thought that $M^r(0) = E[X^r]$, but for a uniform distribution the MGF is $\dfrac{e^{bt}-e^{at}}{(b-a)t}$, so there already is a singularity at $t=0$. So it would seem $M'(0) \neq E[X]$ Why is that?",,"['probability', 'statistics']"
25,Probabilty of picking an irrational number,Probabilty of picking an irrational number,,I've started to learn some probabilty and it made think about this question: let us assume we randomize virtually any number between 0 and 1. What is the probability for this number to be irrational?,I've started to learn some probabilty and it made think about this question: let us assume we randomize virtually any number between 0 and 1. What is the probability for this number to be irrational?,,['probability']
26,probabilty of random points on perimeter containing center,probabilty of random points on perimeter containing center,,"related question: probablity of random pick up three points inside a regular triangle which form a triangle and contain the center What is the probability that a (possibly degenerate) triangle made by three randonly chosen points on the perimeter of an n-gon contains the centre of an n-gon? For a square, there is a $\frac{1}{16}$ chance that the points are in configuration a, $\frac{3}{16} $ for configuraion b, and $\frac{3}8$ for c and d. The probility that the points contain the center is $0$ for a and c, $\frac{1}3$ for b (since the center is contained iff one point is on each side of the line TF1 and arbitrarily taking the square to have unit sides yields $2\int_0^1 a-a^2 \mathrm{d} a=\frac{1}3$) and $\frac{1}2$ for d (center contained iff B1 is the opposite side of the line through D1H1 to C1, $\int_0^1 a \mathrm{d}a=\frac{1}2$).Therefore, if I have somehow not made an error, the probability is $\frac{1}4$. [edited] The limiting case of a circle is $\frac{2}{\tau}\int_0^{\frac{\tau}2}\frac{a}{\tau}\mathrm{d}a=\frac{1}4$ (using $\tau=2\pi$ just to be controversial)","related question: probablity of random pick up three points inside a regular triangle which form a triangle and contain the center What is the probability that a (possibly degenerate) triangle made by three randonly chosen points on the perimeter of an n-gon contains the centre of an n-gon? For a square, there is a $\frac{1}{16}$ chance that the points are in configuration a, $\frac{3}{16} $ for configuraion b, and $\frac{3}8$ for c and d. The probility that the points contain the center is $0$ for a and c, $\frac{1}3$ for b (since the center is contained iff one point is on each side of the line TF1 and arbitrarily taking the square to have unit sides yields $2\int_0^1 a-a^2 \mathrm{d} a=\frac{1}3$) and $\frac{1}2$ for d (center contained iff B1 is the opposite side of the line through D1H1 to C1, $\int_0^1 a \mathrm{d}a=\frac{1}2$).Therefore, if I have somehow not made an error, the probability is $\frac{1}4$. [edited] The limiting case of a circle is $\frac{2}{\tau}\int_0^{\frac{\tau}2}\frac{a}{\tau}\mathrm{d}a=\frac{1}4$ (using $\tau=2\pi$ just to be controversial)",,"['probability', 'geometric-probability']"
27,Number Theory and Probability Question,Number Theory and Probability Question,,Compute the probability that a randomly chosen positive divisor of $10^{99}$ is an integer multiple of $10^{88}$,Compute the probability that a randomly chosen positive divisor of $10^{99}$ is an integer multiple of $10^{88}$,,"['probability', 'elementary-number-theory']"
28,Bayesian Network for dummies,Bayesian Network for dummies,,I know the Bayes Theorem but I've never heard nor used Bayesian Networks. Now I'm told to use Bayesian networks to estimate a dysfunction probability in a noisy signal with Matlab Can someone please post links or simple straightforwarding guides on how to do that? Theory or how to use matlab's toolboxes are equally useful unless they are too generic,I know the Bayes Theorem but I've never heard nor used Bayesian Networks. Now I'm told to use Bayesian networks to estimate a dysfunction probability in a noisy signal with Matlab Can someone please post links or simple straightforwarding guides on how to do that? Theory or how to use matlab's toolboxes are equally useful unless they are too generic,,"['probability', 'probability-theory', 'matlab', 'bayesian-network']"
29,How are TV Texas holdem poker percentages worked out,How are TV Texas holdem poker percentages worked out,,"I watch with some mild interest on TV the poker games when I have little better to do at night. And I notice these guys playing Texas Holdem Poker. My question is, the TV has beside each players name a set of cards, beside this they have percentages, these percentages change as the game evolves. How are these percentages worked out. If I choose to play online. Given I cant see other peoples cards, and without hacking or cheating in other ways. How close can I get to these percentages if I had software to read only my cards and the up cards on the table as the game progresses. Remember, no cheating or hacking.","I watch with some mild interest on TV the poker games when I have little better to do at night. And I notice these guys playing Texas Holdem Poker. My question is, the TV has beside each players name a set of cards, beside this they have percentages, these percentages change as the game evolves. How are these percentages worked out. If I choose to play online. Given I cant see other peoples cards, and without hacking or cheating in other ways. How close can I get to these percentages if I had software to read only my cards and the up cards on the table as the game progresses. Remember, no cheating or hacking.",,"['probability', 'card-games']"
30,Is a dot product between two independent multivariate Gaussian random variables also Gaussian random variable?,Is a dot product between two independent multivariate Gaussian random variables also Gaussian random variable?,,"Let $x, z \sim N(0,I_p)$ be two independent multivariate Gaussian random variables. The question is whether the dot product $x'z$ is a Gaussian distributed variable. My guess is that it is not. However, I cannot find what is wrong with the following argument. Consider the joint distribution of $(x'z, z)$. We can write $p(x'z,z) = p(x'z|z)p(z)$. Since conditionally $x'z|z$ is a Gaussian and $z$ is Gaussian, the product of two Gaussian densities is a density of a multivariate Gaussian variable. Therefore $(x'z, z)$ are jointly Gaussian, which implies that marginally $x'z$ is also a Gaussian variable.","Let $x, z \sim N(0,I_p)$ be two independent multivariate Gaussian random variables. The question is whether the dot product $x'z$ is a Gaussian distributed variable. My guess is that it is not. However, I cannot find what is wrong with the following argument. Consider the joint distribution of $(x'z, z)$. We can write $p(x'z,z) = p(x'z|z)p(z)$. Since conditionally $x'z|z$ is a Gaussian and $z$ is Gaussian, the product of two Gaussian densities is a density of a multivariate Gaussian variable. Therefore $(x'z, z)$ are jointly Gaussian, which implies that marginally $x'z$ is also a Gaussian variable.",,"['probability', 'normal-distribution']"
31,Inequality on balls/bins with nested logs,Inequality on balls/bins with nested logs,,"Let $k = \lceil \frac{3 \ln n}{\ln \ln n}\rceil$. How does one show that $$ \left(\frac{e}{k}\right)^k \frac{1}{1-\frac{e}{k}} \le n^{-2} ? $$ This is from p. 44 of Motwani and Raghavan, Randomized Algorithms, where they're talking about ball/bin probabilities. The $\left(\frac{e}{x}\right)^x$ is motivated but the $k$ just comes out of nowhere. Ignoring that for the moment, I don't see the derivation of the inequality; even assuming the the 3 is really an approximation for $e$, and throwing out the $\log\log n$... and the geometric series, I get (letting $k^{*}= e \log n$): $$ \left(\frac{e}{k^{*}}\right)^{k^{*}}\approx n^{-\log\log n} \le n^{-2} $$ for $n \ge e^{e^2}$. But I changed things quite a bit, and this is quite a large constant ($\approx 3^{27}$). So two questions: how does one derive the full inequality? why that particular $k$?","Let $k = \lceil \frac{3 \ln n}{\ln \ln n}\rceil$. How does one show that $$ \left(\frac{e}{k}\right)^k \frac{1}{1-\frac{e}{k}} \le n^{-2} ? $$ This is from p. 44 of Motwani and Raghavan, Randomized Algorithms, where they're talking about ball/bin probabilities. The $\left(\frac{e}{x}\right)^x$ is motivated but the $k$ just comes out of nowhere. Ignoring that for the moment, I don't see the derivation of the inequality; even assuming the the 3 is really an approximation for $e$, and throwing out the $\log\log n$... and the geometric series, I get (letting $k^{*}= e \log n$): $$ \left(\frac{e}{k^{*}}\right)^{k^{*}}\approx n^{-\log\log n} \le n^{-2} $$ for $n \ge e^{e^2}$. But I changed things quite a bit, and this is quite a large constant ($\approx 3^{27}$). So two questions: how does one derive the full inequality? why that particular $k$?",,"['probability', 'inequality', 'asymptotics']"
32,Effects of condensing a random variable to only 2 possible values,Effects of condensing a random variable to only 2 possible values,,"$X$ is a random variable, which is not constant. $E[X]=0$. $E[X^4] \leq 2(E[X^2])^2$. Let $Y$ be given by: $P(Y=E[X|X \geq 0]) = P(X \geq 0)$ and $P(Y=E[X|X \lt 0]) = P(X \lt 0)$. Do we necessarily have $E[Y^4] \leq 2(E[Y^2])^2$?","$X$ is a random variable, which is not constant. $E[X]=0$. $E[X^4] \leq 2(E[X^2])^2$. Let $Y$ be given by: $P(Y=E[X|X \geq 0]) = P(X \geq 0)$ and $P(Y=E[X|X \lt 0]) = P(X \lt 0)$. Do we necessarily have $E[Y^4] \leq 2(E[Y^2])^2$?",,['probability']
33,Explanation of numeric experiment that approximates e?,Explanation of numeric experiment that approximates e?,,"Recently I found this post on Reddit. It describes the following algorithm to find e: Here is an example of e turning up   unexpectedly. Select a random number   between 0 and 1. Now select another   and add it to the first. Keep doing   this, piling on random numbers. How   many random numbers, on average, do   you need to make the total greater   than 1? The answer is e. This means that you need on average ~2.7 random real numbers to make the sum greater than 1. However, a random number between 0 and 1 would on average be equal to 0.5. So intuitively I would think that, on average, only 2 random numbers would be required to have a sum > 1. So where did I go wrong in my thinking? Update I just figured it out myself: You need at least two numbers to have a sum > 1, but often you'll need three, sometimes you'll need four, sometimes five, etc... So it only natural that the average required numbers is above 2. Thanks for the replies!","Recently I found this post on Reddit. It describes the following algorithm to find e: Here is an example of e turning up   unexpectedly. Select a random number   between 0 and 1. Now select another   and add it to the first. Keep doing   this, piling on random numbers. How   many random numbers, on average, do   you need to make the total greater   than 1? The answer is e. This means that you need on average ~2.7 random real numbers to make the sum greater than 1. However, a random number between 0 and 1 would on average be equal to 0.5. So intuitively I would think that, on average, only 2 random numbers would be required to have a sum > 1. So where did I go wrong in my thinking? Update I just figured it out myself: You need at least two numbers to have a sum > 1, but often you'll need three, sometimes you'll need four, sometimes five, etc... So it only natural that the average required numbers is above 2. Thanks for the replies!",,"['probability', 'stochastic-processes']"
34,Inclusion-Exclusion confusion: Meeting exactly one friend in lunch time during a semester,Inclusion-Exclusion confusion: Meeting exactly one friend in lunch time during a semester,,"From Trotter's Combinatorics Textbook, I was working on this problem from his Inclusion-Exclusion Chapter: A graduate student eats lunch in the campus food court every Tuesday over the course of a $15$ week semester. [They are] joined each week by some subset of a group of six friends from across campus. Over the course of a semester, [they] ate lunch with each friend 11 times, each pair 9 times, and each triple 6 times. [They] ate lunch with each group of four friends 4 times and each group of five friends 4 times. All seven of them ate lunch together only once that semester. Did the graduate student ever eat lunch alone? If so, how many times? The solution mainly involves making your sets $A_{i}$ as the set of weeks he ate with friend $i$ , and doing inclusion-exclusion from there. (See Chapter 7 Solutions, Question 9: https://trotter.math.gatech.edu/math-3012/toppage.html ) Now I tried to answer my own follow-up question of how many days he eat with exactly 1 friend. Since I know that $|A_{1} \cup \dots \cup A_{6}| = 14$ , I could just try to do: $|A_{1} \cup \dots \cup A_{6}| - |(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})|$ Now the formula for $|(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})|$ I couldn't find online or in the textbooks, so I tried to just count it out myself and noticed a pattern that I think generalizes: If I add up all the sets that are intersections of 2 friends, I've overcounted the sets that are intersections of 3 friends ${3 \choose 2} = 3$ times, so I have to subtract those sets and their quantities 2 times for them to be counted exactly once! Then for the sets 4-friend-intersections, I first counted them ${4 \choose 2}$ times, then subtracted them $2 \cdot {4 \choose 3}$ times, hence they've been counted $ 6 - 8 = -2$ times, so I have to add those sets and their quantities 3 times. You continue this logic and you get the coefficients should be +1, -2, +3, -4, +5. In particular I mean that: $$ |(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})| = \sum|(A_{i} \cap A_{j})| - 2 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k})| + 3 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l})| - 4 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l} \cap A_{m})| + 5 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l} \cap A_{m} \cap A_{n})|$$ But when I try plugging in the numbers I get -16: ${6 \choose 2}9 - 2 {6 \choose 3} 6 + 3  {6 \choose 4}  4 - 4  {6 \choose 5}  4 + 5  {6 \choose 6}  1 = -16$ which is weird since I use this same method for this problem ( Combinations' Problem ) and I get 7 and hence 13 - 7 = 6 times they ate with exactly 1 friend alone, which is correct. Am I missing something here? I've been think about this problem for a while now but am really stuck. Kindly please help here.","From Trotter's Combinatorics Textbook, I was working on this problem from his Inclusion-Exclusion Chapter: A graduate student eats lunch in the campus food court every Tuesday over the course of a week semester. [They are] joined each week by some subset of a group of six friends from across campus. Over the course of a semester, [they] ate lunch with each friend 11 times, each pair 9 times, and each triple 6 times. [They] ate lunch with each group of four friends 4 times and each group of five friends 4 times. All seven of them ate lunch together only once that semester. Did the graduate student ever eat lunch alone? If so, how many times? The solution mainly involves making your sets as the set of weeks he ate with friend , and doing inclusion-exclusion from there. (See Chapter 7 Solutions, Question 9: https://trotter.math.gatech.edu/math-3012/toppage.html ) Now I tried to answer my own follow-up question of how many days he eat with exactly 1 friend. Since I know that , I could just try to do: Now the formula for I couldn't find online or in the textbooks, so I tried to just count it out myself and noticed a pattern that I think generalizes: If I add up all the sets that are intersections of 2 friends, I've overcounted the sets that are intersections of 3 friends times, so I have to subtract those sets and their quantities 2 times for them to be counted exactly once! Then for the sets 4-friend-intersections, I first counted them times, then subtracted them times, hence they've been counted times, so I have to add those sets and their quantities 3 times. You continue this logic and you get the coefficients should be +1, -2, +3, -4, +5. In particular I mean that: But when I try plugging in the numbers I get -16: which is weird since I use this same method for this problem ( Combinations' Problem ) and I get 7 and hence 13 - 7 = 6 times they ate with exactly 1 friend alone, which is correct. Am I missing something here? I've been think about this problem for a while now but am really stuck. Kindly please help here.",15 A_{i} i |A_{1} \cup \dots \cup A_{6}| = 14 |A_{1} \cup \dots \cup A_{6}| - |(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})| |(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})| {3 \choose 2} = 3 {4 \choose 2} 2 \cdot {4 \choose 3}  6 - 8 = -2  |(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})| = \sum|(A_{i} \cap A_{j})| - 2 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k})| + 3 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l})| - 4 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l} \cap A_{m})| + 5 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l} \cap A_{m} \cap A_{n})| {6 \choose 2}9 - 2 {6 \choose 3} 6 + 3  {6 \choose 4}  4 - 4  {6 \choose 5}  4 + 5  {6 \choose 6}  1 = -16,"['probability', 'combinatorics', 'discrete-mathematics', 'inclusion-exclusion']"
35,Probability that the coefficients of a quadratic equation with real roots form a triangle,Probability that the coefficients of a quadratic equation with real roots form a triangle,,"Question : What is the probability that the coefficients of a quadratic equation form the sides of triangle given that it has real roots? Assume that the coefficients are uniformly distributed and positive. Note that it is sufficient to assume that the coefficients are uniformly distributed in $(0,1)$ since we can always divide by the largest coefficient to scale all the coefficients to $(0,1)$ . Experimental data : A simulation with $10^{10}$ trails gives the probability as $0.182185$ . This could be a coincidence but this value which agrees with $\displaystyle \frac{\log \pi}{2\pi}$ to five decimal places. Julia code : using Random  step = target = 10^7 count = count_q = qt = 0  while 1 > 0     count += 1     random_numbers = rand(3)     a = random_numbers[1]     b = random_numbers[2]     c = random_numbers[3]          if b^2 >= 4*a*c         count_q += 1         if (a + b >= c) && (b + c >= a) && (c + a >= b)             qt += 1         end     end          if count_q >= target         println(count,"" "",count_q/step,"" "",qt,"" "",qt/count_q)         target += step     end end","Question : What is the probability that the coefficients of a quadratic equation form the sides of triangle given that it has real roots? Assume that the coefficients are uniformly distributed and positive. Note that it is sufficient to assume that the coefficients are uniformly distributed in since we can always divide by the largest coefficient to scale all the coefficients to . Experimental data : A simulation with trails gives the probability as . This could be a coincidence but this value which agrees with to five decimal places. Julia code : using Random  step = target = 10^7 count = count_q = qt = 0  while 1 > 0     count += 1     random_numbers = rand(3)     a = random_numbers[1]     b = random_numbers[2]     c = random_numbers[3]          if b^2 >= 4*a*c         count_q += 1         if (a + b >= c) && (b + c >= a) && (c + a >= b)             qt += 1         end     end          if count_q >= target         println(count,"" "",count_q/step,"" "",qt,"" "",qt/count_q)         target += step     end end","(0,1) (0,1) 10^{10} 0.182185 \displaystyle \frac{\log \pi}{2\pi}","['probability', 'integration', 'geometry', 'algebra-precalculus', 'geometric-probability']"
36,"Counterfeit coin, conditional probability","Counterfeit coin, conditional probability",,"I am doing Problem AT9 [[ Harvard-MIT Math Tournament February 27, 1999 ]] here: https://hmmt-archive.s3.amazonaws.com/tournaments/1999/feb/adv/solutions.pdf As part of his effort to take over the world, Edward starts producing his own currency. As part of an effort to stop Edward, Alex works in the mint and produces 1 counterfeit coin for every 99 real ones. Alex isn’t very good at this, so none of the counterfeit coins are the right weight. Since the mint is not perfect, each coin is weighed before leaving. If the coin is not the right weight, then it is sent to a lab for testing. The scale is accurate 95% of the time, 5% of all the coins minted are sent to the lab, and the lab’s test is accurate 90% of the time. If the lab says a coin is counterfeit, what is the probability that it really is? I'm confused by the part that says: The scale is accurate 95% of the time, 5% of all the coins minted are sent to the lab If the scale is accurate 95% of the time, shouldn't the percentage of coins minted sent to the lab be $$(.01)(.95) + (.99)(.05) = .059?$$ And not 5% as asserted?","I am doing Problem AT9 [[ Harvard-MIT Math Tournament February 27, 1999 ]] here: https://hmmt-archive.s3.amazonaws.com/tournaments/1999/feb/adv/solutions.pdf As part of his effort to take over the world, Edward starts producing his own currency. As part of an effort to stop Edward, Alex works in the mint and produces 1 counterfeit coin for every 99 real ones. Alex isn’t very good at this, so none of the counterfeit coins are the right weight. Since the mint is not perfect, each coin is weighed before leaving. If the coin is not the right weight, then it is sent to a lab for testing. The scale is accurate 95% of the time, 5% of all the coins minted are sent to the lab, and the lab’s test is accurate 90% of the time. If the lab says a coin is counterfeit, what is the probability that it really is? I'm confused by the part that says: The scale is accurate 95% of the time, 5% of all the coins minted are sent to the lab If the scale is accurate 95% of the time, shouldn't the percentage of coins minted sent to the lab be And not 5% as asserted?",(.01)(.95) + (.99)(.05) = .059?,"['probability', 'algebra-precalculus', 'contest-math', 'conditional-probability']"
37,Modeling a probabilistic game with changing probabilities,Modeling a probabilistic game with changing probabilities,,"I am trying to model a simple game (a more complex problem I am working on reduces to it). You roll an $n$ -sided dice labeled 1 to $n$ . (This can be a normal 6-sided dice if you'd like.) If the dice shows 1 , you draw a card from a deck of $m$ cards labeled 1 to $m$ . (This can be a normal 52-card deck if you'd like.) You record which card you drew, replace it in the deck, and shuffle. This process repeats until you draw a card you have previously seen. The question: what is the distribution of the expected length of this game, in terms of the number of dice rolls? I am able to model the individual sub-games without an issue: Game 1 , the dice game, is geometrically distributed with $p=1/n$ . On average, we expect to see a 1 every $n$ throws. Game 2 , the card game, is a bit more complex, but we can model it like this: On our $(i+1)$ -th draw, we have already seen $i$ cards, all of which are unique (if not, the game would already be over). Therefore, the probability the given card $i+1$ matches one of the previous cards is $\frac{i}{m}$ for a deck of size $m$ . Factoring in the probability of not having already won, the distribution of winning the game on draw $i$ is $$ P(i) = \frac{m!/(m-i)!}{m^i} \cdot \frac{i}{m} $$ (The numerator of the left fraction is just the number of permutations of $i$ items from $m$ items.) I can't find a closed form for the expected value of this PDF, but it matches simulations given a value of $m$ . For example, consider the standard case of $n=6$ and $m=52$ . We expect a 1 on the dice every 6 rolls. We expect to draw a duplicate card after 9.7 draws, on average. (Solved numerically in Mathematica and confirmed in simulation.) So, we expect the game to last $\approx 58$ rolls on average, since we can multiply expectations of independent events. But what is the actual distribution of game lengths? Here's some simulation data. I play the game 100,000 times: The simulation code looks like this: def play_game():     cards_seen = set()     for i in it.count(1):         dice = random.randint(1, 6)         if dice != 1:             continue          card = random.randint(1, 52)         if card in cards_seen:             return i, 1 + len(cards_seen)          cards_seen.add(card) Statistics from the simulation: Mean number of turns: 58.19 ...median: 54 Mean number of cards drawn: 9.69 ...median: 9 Implied number of turns/per card (i.e., dice rolls 1): 6.005 This looks very familiar (i.e., vaguely negative-binomial), but the changing probability of pulling a matching card makes such an analysis tricky. I know the combination of distributions can be computed via convolution , but I'm not clear how I would apply that here, since these two distributions are not really independent — whether we play the card subgame is directly dependent on the results of the dice subgame. I believe this game can be modeled as a Markov chain, but I'm not sure if that helps with the analysis. We have dice states $D_k$ and card states $C_k$ . We start at state $D_1$ and move to state $C_1$ with probability $1/6$ , or remain at state $D_1$ with probability $5/6$ . At some state $C_k$ , we end the game with probability $P(k)$ from above, and move to state $D_{k+1}$ with probability $1-P(k)$ . Now the question reduces to finding the distribution of the number of steps in this chain.","I am trying to model a simple game (a more complex problem I am working on reduces to it). You roll an -sided dice labeled 1 to . (This can be a normal 6-sided dice if you'd like.) If the dice shows 1 , you draw a card from a deck of cards labeled 1 to . (This can be a normal 52-card deck if you'd like.) You record which card you drew, replace it in the deck, and shuffle. This process repeats until you draw a card you have previously seen. The question: what is the distribution of the expected length of this game, in terms of the number of dice rolls? I am able to model the individual sub-games without an issue: Game 1 , the dice game, is geometrically distributed with . On average, we expect to see a 1 every throws. Game 2 , the card game, is a bit more complex, but we can model it like this: On our -th draw, we have already seen cards, all of which are unique (if not, the game would already be over). Therefore, the probability the given card matches one of the previous cards is for a deck of size . Factoring in the probability of not having already won, the distribution of winning the game on draw is (The numerator of the left fraction is just the number of permutations of items from items.) I can't find a closed form for the expected value of this PDF, but it matches simulations given a value of . For example, consider the standard case of and . We expect a 1 on the dice every 6 rolls. We expect to draw a duplicate card after 9.7 draws, on average. (Solved numerically in Mathematica and confirmed in simulation.) So, we expect the game to last rolls on average, since we can multiply expectations of independent events. But what is the actual distribution of game lengths? Here's some simulation data. I play the game 100,000 times: The simulation code looks like this: def play_game():     cards_seen = set()     for i in it.count(1):         dice = random.randint(1, 6)         if dice != 1:             continue          card = random.randint(1, 52)         if card in cards_seen:             return i, 1 + len(cards_seen)          cards_seen.add(card) Statistics from the simulation: Mean number of turns: 58.19 ...median: 54 Mean number of cards drawn: 9.69 ...median: 9 Implied number of turns/per card (i.e., dice rolls 1): 6.005 This looks very familiar (i.e., vaguely negative-binomial), but the changing probability of pulling a matching card makes such an analysis tricky. I know the combination of distributions can be computed via convolution , but I'm not clear how I would apply that here, since these two distributions are not really independent — whether we play the card subgame is directly dependent on the results of the dice subgame. I believe this game can be modeled as a Markov chain, but I'm not sure if that helps with the analysis. We have dice states and card states . We start at state and move to state with probability , or remain at state with probability . At some state , we end the game with probability from above, and move to state with probability . Now the question reduces to finding the distribution of the number of steps in this chain.","n n m m p=1/n n (i+1) i i+1 \frac{i}{m} m i 
P(i) = \frac{m!/(m-i)!}{m^i} \cdot \frac{i}{m}
 i m m n=6 m=52 \approx 58 D_k C_k D_1 C_1 1/6 D_1 5/6 C_k P(k) D_{k+1} 1-P(k)","['probability', 'probability-distributions', 'markov-chains', 'card-games']"
38,How do I find the second order moment with a MGF?,How do I find the second order moment with a MGF?,,"Let $X$ be a random variable such that $M_X(t)=e^t M_X(-t)$ . Find $E(X)$ and $E\left(X^2\right)$ . I know the general procedure that to find the $n$ th moment, the $n$ th order derivative needs to be taken and then t must be set to 0. $M_X(t)^{\prime}=e^t M_X(-t)-e^t M_X^{\prime}(-t)$ The solution is $E[X] = \frac{1}{2}$ Question 1: For the second order moment: \begin{aligned} &M_X^{(2)}(t)=e^t\left(M_X^{(2)}(-t)-2 M_X^{(1)}(-t)+M_X(-t)\right) \\ &M_X^{(2)}(0)=M_X^{(2)}(0)-2 M_X^{(1)}(0)+M_X(0) \end{aligned} If I differentiate two times, It seems like there is no expression for the second derivative. Does this mean that $E[X^2]$ is undefined?","Let be a random variable such that . Find and . I know the general procedure that to find the th moment, the th order derivative needs to be taken and then t must be set to 0. The solution is Question 1: For the second order moment: If I differentiate two times, It seems like there is no expression for the second derivative. Does this mean that is undefined?","X M_X(t)=e^t M_X(-t) E(X) E\left(X^2\right) n n M_X(t)^{\prime}=e^t M_X(-t)-e^t M_X^{\prime}(-t) E[X] = \frac{1}{2} \begin{aligned}
&M_X^{(2)}(t)=e^t\left(M_X^{(2)}(-t)-2 M_X^{(1)}(-t)+M_X(-t)\right) \\
&M_X^{(2)}(0)=M_X^{(2)}(0)-2 M_X^{(1)}(0)+M_X(0)
\end{aligned} E[X^2]","['probability', 'moment-generating-functions']"
39,"What is the probability that a 2D symmetric random walk will reach a point $(x,y)$ before returning to the origin?",What is the probability that a 2D symmetric random walk will reach a point  before returning to the origin?,"(x,y)","Let $p_n(\vec{x})$ be the probability that a symmetric random walk (SRW) on $\mathbb{Z}^n$ starting at the origin reaches the point $\vec{x}\in\mathbb{Z}^n-\{0\}$ before returning to the origin. What is $p_2(\vec{x})$ ? Judging from this post , I fear the answer might be too complicated, but it's worth a shot. I've already been able to calculate $p_1(x)=\frac{1}{2|x|}$ as follows: Let $p_n(\vec{s},\vec{x})$ be the probability that a SRW on $\mathbb{Z}^n$ starting at $\vec{s}$ reaches the point $\vec{x}\in\mathbb{Z}^n-\{0\}$ before the origin. Lemma . $$p_n(\vec{s},\vec{x})+p_n(\vec{x}-\vec{s},\vec{x})=1\quad\text{for}\quad n=1,2\tag{1}$$ Proof . Firstly, a SRW for $n=1,2$ will reach each point with probability $1$ . Hence, we may assume WLOG that a SRW will either reach $0$ first, or $\vec{x}$ first. If $W$ is a walk, then $W+\vec{s}$ reaches $\vec{x}$ before $0$ iff $-W+\vec{x}-\vec{s}$ reaches $0$ before $\vec{x}$ . Hence, $p_n(\vec{s},\vec{x})=1-p_n(\vec{x}-\vec{s},\vec{x})$ . Rearranging gives the desired equality $(1)$ . \begin{multline}\shoveright\square \end{multline} Theorem. $$p_1(x)=\frac{1}{2|x|}\tag{2}$$ Proof. Suppose $0<s<x$ . Then a SRW that reaches $x$ before $0$ will also reach $s$ before $0$ . Then, $$p_1(s)p_1(s,x)=p_1(x)\tag{3}$$ Then we may write $(1)$ as $$\frac{1}{p_1(s)}+\frac{1}{p_1(x-s)}=\frac{1}{p_1(x)}\tag{4}$$ Setting $s=1$ and using induction gives $$\frac{1}{p(x)}=\frac{x}{p_1(1)}\tag{5}$$ We can quickly check that $p_1(1)=\frac{1}{2}$ : either the first step in the walk is to $1$ , or the first step is to $-1$ , in which case the walk will have to return to the origin before reaching $1$ . Hence, $p_1(x)=\frac{1}{2x}$ . By symmetry, $p_1(-x)=p_1(x)$ , so we arrive at $(2)$ . \begin{multline}\shoveright\square \end{multline} I've tried ways of extending this method to $n=2$ , but to no avail. The biggest problem is that there doesn't seem to be a nice analogue of $(3)$ for $n=2$ because a walk that reaches $\vec{x}$ does not necessarily pass through any other specific point. According to Wikipedia, $(2)$ can be derived from the fact that a SRW on $\mathbb{Z}^n$ is a martingale, but I know nothing of martingales so I do not know how to use this information to find a formula for $n=2$ . EDIT: I should have looked a little more closely at the post I linked. There is a link in the comments leading to this page , which shows that if we define $R_{n,m}$ to be the resistance between the origin and the point $(n,m)$ on $\mathbb{Z}^2$ where every edge has unit resistance, then $$R_{1,0} = \frac{1}{2}\\ R_{m,m}=\frac{2}{\pi}\sum_{k=1}^m\frac{1}{2k-1}\quad\quad m>0\\ 4R_{n,m} -R_{n-1,m}-R_{n+1,m}-R_{n,m-1}-R_{n,m+1}=0\quad\quad (n,m)\neq (0,0)$$ Using symmetry accross the $x$ -axis, $y$ -axis, and diagonals $y=\pm x$ , the above is enough to determine $R_{n,m}$ for every $n,m$ . Then, $$\boxed{p_2((n,m))=\frac{1}{4R_{n,m}}}$$ As a sanity check, the asymptotic from Yuval Peres's answer gives $$p_2((m,m))=\frac{\pi}{8}\left(\sum_{k=1}^m\frac{1}{2k-1}\right)^{-1}\sim \left(\frac{4}{\pi}\log(m)+\frac{4\gamma}{\pi}+\frac{8}{\pi}\log 2\right)^{-1}\quad\quad m>0$$ which is a tight approximation easily derivable from the fact that $H_{m-1}-\gamma =\psi(m) \sim \log m$ , and $\sum_{k=1}^m\frac{1}{2k-1}=H_{2m-1}-\frac{1}{2}H_{m-1}$ .","Let be the probability that a symmetric random walk (SRW) on starting at the origin reaches the point before returning to the origin. What is ? Judging from this post , I fear the answer might be too complicated, but it's worth a shot. I've already been able to calculate as follows: Let be the probability that a SRW on starting at reaches the point before the origin. Lemma . Proof . Firstly, a SRW for will reach each point with probability . Hence, we may assume WLOG that a SRW will either reach first, or first. If is a walk, then reaches before iff reaches before . Hence, . Rearranging gives the desired equality . Theorem. Proof. Suppose . Then a SRW that reaches before will also reach before . Then, Then we may write as Setting and using induction gives We can quickly check that : either the first step in the walk is to , or the first step is to , in which case the walk will have to return to the origin before reaching . Hence, . By symmetry, , so we arrive at . I've tried ways of extending this method to , but to no avail. The biggest problem is that there doesn't seem to be a nice analogue of for because a walk that reaches does not necessarily pass through any other specific point. According to Wikipedia, can be derived from the fact that a SRW on is a martingale, but I know nothing of martingales so I do not know how to use this information to find a formula for . EDIT: I should have looked a little more closely at the post I linked. There is a link in the comments leading to this page , which shows that if we define to be the resistance between the origin and the point on where every edge has unit resistance, then Using symmetry accross the -axis, -axis, and diagonals , the above is enough to determine for every . Then, As a sanity check, the asymptotic from Yuval Peres's answer gives which is a tight approximation easily derivable from the fact that , and .","p_n(\vec{x}) \mathbb{Z}^n \vec{x}\in\mathbb{Z}^n-\{0\} p_2(\vec{x}) p_1(x)=\frac{1}{2|x|} p_n(\vec{s},\vec{x}) \mathbb{Z}^n \vec{s} \vec{x}\in\mathbb{Z}^n-\{0\} p_n(\vec{s},\vec{x})+p_n(\vec{x}-\vec{s},\vec{x})=1\quad\text{for}\quad n=1,2\tag{1} n=1,2 1 0 \vec{x} W W+\vec{s} \vec{x} 0 -W+\vec{x}-\vec{s} 0 \vec{x} p_n(\vec{s},\vec{x})=1-p_n(\vec{x}-\vec{s},\vec{x}) (1) \begin{multline}\shoveright\square
\end{multline} p_1(x)=\frac{1}{2|x|}\tag{2} 0<s<x x 0 s 0 p_1(s)p_1(s,x)=p_1(x)\tag{3} (1) \frac{1}{p_1(s)}+\frac{1}{p_1(x-s)}=\frac{1}{p_1(x)}\tag{4} s=1 \frac{1}{p(x)}=\frac{x}{p_1(1)}\tag{5} p_1(1)=\frac{1}{2} 1 -1 1 p_1(x)=\frac{1}{2x} p_1(-x)=p_1(x) (2) \begin{multline}\shoveright\square
\end{multline} n=2 (3) n=2 \vec{x} (2) \mathbb{Z}^n n=2 R_{n,m} (n,m) \mathbb{Z}^2 R_{1,0} = \frac{1}{2}\\ R_{m,m}=\frac{2}{\pi}\sum_{k=1}^m\frac{1}{2k-1}\quad\quad m>0\\ 4R_{n,m} -R_{n-1,m}-R_{n+1,m}-R_{n,m-1}-R_{n,m+1}=0\quad\quad (n,m)\neq (0,0) x y y=\pm x R_{n,m} n,m \boxed{p_2((n,m))=\frac{1}{4R_{n,m}}} p_2((m,m))=\frac{\pi}{8}\left(\sum_{k=1}^m\frac{1}{2k-1}\right)^{-1}\sim \left(\frac{4}{\pi}\log(m)+\frac{4\gamma}{\pi}+\frac{8}{\pi}\log 2\right)^{-1}\quad\quad m>0 H_{m-1}-\gamma =\psi(m) \sim \log m \sum_{k=1}^m\frac{1}{2k-1}=H_{2m-1}-\frac{1}{2}H_{m-1}","['probability', 'stochastic-processes', 'martingales', 'random-walk']"
40,What's the probability of me being obliterated if Thanos snaps his fingers twice?,What's the probability of me being obliterated if Thanos snaps his fingers twice?,,"Thanos snaps his fingers and half the Earth's population disappear. He snaps again and half of the remaining half disappear. Now, what is the probability of any given person (for example, myself) disappearing after 2 snaps? My simplistic reasoning was that, since after 2 snaps 75% of the population are gone, it means that any one person (e.g. me) has a 75% chance of having been eliminated as well. My friend's asking for a formula, i.e. mathematical proof of this calculation and the best I can come up with is: event (snap): 50% chance of being obliterated event (snap): 50% chance of having survived the first snap and another 50% chance of perishing so 50% x 50% = 25% Sum of two events = 50% + 25% = 75% Could someone please verify if my logic is sound and what's the correct math behind it. Thanks so much!","Thanos snaps his fingers and half the Earth's population disappear. He snaps again and half of the remaining half disappear. Now, what is the probability of any given person (for example, myself) disappearing after 2 snaps? My simplistic reasoning was that, since after 2 snaps 75% of the population are gone, it means that any one person (e.g. me) has a 75% chance of having been eliminated as well. My friend's asking for a formula, i.e. mathematical proof of this calculation and the best I can come up with is: event (snap): 50% chance of being obliterated event (snap): 50% chance of having survived the first snap and another 50% chance of perishing so 50% x 50% = 25% Sum of two events = 50% + 25% = 75% Could someone please verify if my logic is sound and what's the correct math behind it. Thanks so much!",,['probability']
41,Maximizing probability of picking the dime,Maximizing probability of picking the dime,,"A money pouch contains a certain number of cents and only one dime. You and your friend are playing a game: They alternate turns and pick one coin at a time, which they put in their pockets. Whoever picks the dime wins. You are trying to decide whether it is better to play first or second: Initially you think that if you pick the dime in your first draw, you immediately win, while if you opt to play second, your probability to pick the dime increases. Which of the two options is more favorable for you? We assume that you can’t see the coins before taking them out of the pouch and also that you can’t tell from the size which coin is which. My thought: Let's say we have $n$ coins in total. If I play first, I have probability of picking the dime: $\frac {1}{n}+\frac {1}{n-2}+\frac {1}{n-4}...$ while if I play second, I have $\frac {1}{n-1}+\frac {1}{n-3}+\frac {1}{n-5}...$ . Clearly the second option gives a larger number. Is this correct? Thank you.","A money pouch contains a certain number of cents and only one dime. You and your friend are playing a game: They alternate turns and pick one coin at a time, which they put in their pockets. Whoever picks the dime wins. You are trying to decide whether it is better to play first or second: Initially you think that if you pick the dime in your first draw, you immediately win, while if you opt to play second, your probability to pick the dime increases. Which of the two options is more favorable for you? We assume that you can’t see the coins before taking them out of the pouch and also that you can’t tell from the size which coin is which. My thought: Let's say we have coins in total. If I play first, I have probability of picking the dime: while if I play second, I have . Clearly the second option gives a larger number. Is this correct? Thank you.",n \frac {1}{n}+\frac {1}{n-2}+\frac {1}{n-4}... \frac {1}{n-1}+\frac {1}{n-3}+\frac {1}{n-5}...,['probability']
42,Toss a coin until you get $n$ heads in a row. Expected number of tails tossed?,Toss a coin until you get  heads in a row. Expected number of tails tossed?,n,"You toss a fair coin until you get $n$ heads in a row. What is the expected number of tails you tossed? It can be well shown that the expected number of tosses required to get $n$ heads in a row is $2^{n+1}-2$ So naturally half of this is the expected number of tails? More formally: Let $H_n$ and $T_n$ respectively denote the number of heads and tails you have tossed in total up to time $n$ . $X_n : = H_n - T_n$ is a martinagle. As $X_{n+1} = X_n + C_{n+1}$ where $C_n = \{+1 \text{ if you tossed a head and } -1 \text{ if you tossed a tail on coin } n+1 \}$ And $\mathbb{E}[C_n] = 0 $ for all $n >=1$ Notice $X_0$ is $0$ and the stopping time $\tau$ to yield $n$ consecutive heads is a.s finite (by borell cantelli ) and in $\mathcal{L}_1$ Ergo by OST: $\mathbb{E}[H_{\tau} - T_{\tau}] = \mathbb{E}[X_{\tau}] = \mathbb{E}[X_0] = 0 $ And so $ \star \mathbb{E}[H_{\tau} ] = \mathbb{E}[T_{\tau} ]  $ We know from before that $\mathbb{E}[H_{\tau} + T_{\tau}] = 2^{n+1} -2 $ And so using $\star$ $\mathbb{E}[T_{\tau}] = \frac{1}{2} (2^{n+1} -2 ) = 2^n -1$ Having gone through all this I feel like I used an AK-47 to swat a fly. And that all the martingale theory was not needed. Is there a simply more concise way to prove that once you know the expected number of tosses for something, the expected number of tails will simply be half that?","You toss a fair coin until you get heads in a row. What is the expected number of tails you tossed? It can be well shown that the expected number of tosses required to get heads in a row is So naturally half of this is the expected number of tails? More formally: Let and respectively denote the number of heads and tails you have tossed in total up to time . is a martinagle. As where And for all Notice is and the stopping time to yield consecutive heads is a.s finite (by borell cantelli ) and in Ergo by OST: And so We know from before that And so using Having gone through all this I feel like I used an AK-47 to swat a fly. And that all the martingale theory was not needed. Is there a simply more concise way to prove that once you know the expected number of tosses for something, the expected number of tails will simply be half that?",n n 2^{n+1}-2 H_n T_n n X_n : = H_n - T_n X_{n+1} = X_n + C_{n+1} C_n = \{+1 \text{ if you tossed a head and } -1 \text{ if you tossed a tail on coin } n+1 \} \mathbb{E}[C_n] = 0  n >=1 X_0 0 \tau n \mathcal{L}_1 \mathbb{E}[H_{\tau} - T_{\tau}] = \mathbb{E}[X_{\tau}] = \mathbb{E}[X_0] = 0   \star \mathbb{E}[H_{\tau} ] = \mathbb{E}[T_{\tau} ]   \mathbb{E}[H_{\tau} + T_{\tau}] = 2^{n+1} -2  \star \mathbb{E}[T_{\tau}] = \frac{1}{2} (2^{n+1} -2 ) = 2^n -1,"['probability', 'expected-value', 'stopping-times']"
43,Expected number of tosses for a biased coin with Markov chain,Expected number of tosses for a biased coin with Markov chain,,"You have a biased coin, where the probability of flipping a head is 0.70. You flip once, and the coin comes up tails. What is the expected number of flips from that point (so counting that as flip #0) until the number of heads flipped in total equals the number of tails? My approach is wrong but I don't understand why!!! $x=0.7(1)+0.3(1+x)≈1.43$ . with 0.5 we get a head and we are done in 1 flip. with 0.5 we get an additional tail and so we will have made 1 flip but then we will need to repeat X. Could you please explain clearly Markov chains. I am not very good at it.","You have a biased coin, where the probability of flipping a head is 0.70. You flip once, and the coin comes up tails. What is the expected number of flips from that point (so counting that as flip #0) until the number of heads flipped in total equals the number of tails? My approach is wrong but I don't understand why!!! . with 0.5 we get a head and we are done in 1 flip. with 0.5 we get an additional tail and so we will have made 1 flip but then we will need to repeat X. Could you please explain clearly Markov chains. I am not very good at it.",x=0.7(1)+0.3(1+x)≈1.43,"['probability', 'recurrence-relations', 'markov-chains']"
44,Using the law of large numbers to calculate an integral,Using the law of large numbers to calculate an integral,,"Let $f:[0,1]\to\mathbb{R}$ be continuous. Prove $$ \lim_{n\to\infty}\int_0^1\int_0^1\cdots\int_0^1f\left((x_1x_2\cdots x_n)^{1/n}\right)dx_1dx_2\cdots dx_n = f\left(\frac1e\right) $$ My idea so far is to use uniform distributions to calculate this. Let $X_1,X_2,\dots$ be independent Uniform $(0,1)$ variables. My idea is to somehow use that this integral is equal to $$ \lim_{n\to\infty}\mathbb{E}\left((X_1X_2\cdots X_n)^{1/n}\right). $$ I think I can somehow use the fact that $\frac1n\sum_{i=1}^n\log X_i\overset{a.s.}{\to}\mathbb{E}\log X_i=-1$ . I think I have some of the ideas, but my question is how can I actually put together these ideas in a rigorous way prove this? Do I need the full power of the SLLN, or would I somehow be able to just use the WLLN? I also think maybe I will have to use the dominated convergence theorem, but I'm not sure. I'm struggling with self doubt on this question. Any help appreciated!","Let be continuous. Prove My idea so far is to use uniform distributions to calculate this. Let be independent Uniform variables. My idea is to somehow use that this integral is equal to I think I can somehow use the fact that . I think I have some of the ideas, but my question is how can I actually put together these ideas in a rigorous way prove this? Do I need the full power of the SLLN, or would I somehow be able to just use the WLLN? I also think maybe I will have to use the dominated convergence theorem, but I'm not sure. I'm struggling with self doubt on this question. Any help appreciated!","f:[0,1]\to\mathbb{R} 
\lim_{n\to\infty}\int_0^1\int_0^1\cdots\int_0^1f\left((x_1x_2\cdots x_n)^{1/n}\right)dx_1dx_2\cdots dx_n = f\left(\frac1e\right)
 X_1,X_2,\dots (0,1) 
\lim_{n\to\infty}\mathbb{E}\left((X_1X_2\cdots X_n)^{1/n}\right).
 \frac1n\sum_{i=1}^n\log X_i\overset{a.s.}{\to}\mathbb{E}\log X_i=-1",['probability']
45,Can two independent events be disjoint?,Can two independent events be disjoint?,,"If events $A$ and $B$ both have positive probabilities, and if they are disjoint, they surely cannot be independent since: $$\text{disjoint:}\quad P(A \text{ intersection } B) = 0 \iff P(A \text{ union }B) = P(A) + P(B),$$ $$\text{independent:}\quad\quad\quad\quad P(A \text{ intersection } B) = P(A)P(B).$$ so if P(A intersection B) is 0, then $P(A)P(B)$ should be 0 too, but since they're both above 0, then this is false. However I am not sure if that is the case the other way around, I cannot put my head around the question if two independent events can be disjoint. Can anyone help? Thanks in advance.","If events and both have positive probabilities, and if they are disjoint, they surely cannot be independent since: so if P(A intersection B) is 0, then should be 0 too, but since they're both above 0, then this is false. However I am not sure if that is the case the other way around, I cannot put my head around the question if two independent events can be disjoint. Can anyone help? Thanks in advance.","A B \text{disjoint:}\quad P(A \text{ intersection } B) = 0 \iff P(A \text{ union }B) = P(A) + P(B), \text{independent:}\quad\quad\quad\quad P(A \text{ intersection } B) = P(A)P(B). P(A)P(B)","['probability', 'independence']"
46,Distribution of first exit time of Brownian motion,Distribution of first exit time of Brownian motion,,"Let $B_t$ be standard one dimensional Brownian motion and $\tau = \inf\{s : B_s \notin (a,b) \}$ where $a<0<b$ are real numbers. What is the distribution of $\tau$ ? I know that for hitting times $\tau_a = \inf \{s : B_s =a \}$ the distribution can be calculated with the reflection principle. And clearly $ \tau = \tau_a \wedge \tau_b$ . So how can I continue?",Let be standard one dimensional Brownian motion and where are real numbers. What is the distribution of ? I know that for hitting times the distribution can be calculated with the reflection principle. And clearly . So how can I continue?,"B_t \tau = \inf\{s : B_s \notin (a,b) \} a<0<b \tau \tau_a = \inf \{s : B_s =a \}  \tau = \tau_a \wedge \tau_b","['probability', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
47,rolling dice - win if the sum of rolls is exactly $n$,rolling dice - win if the sum of rolls is exactly,n,"This question was asked during my interview: Suppose you have a fair dice (6 faces as usual). You can pick a positive integer $n$ . Then you can repeatedly roll a dice until the sum of the rolls exceeds or equals to $n$ . If the sum is exactly $n$ , it is a win. Otherwise, you lose. Find $n$ that maximizes your chance of winning. Let $P(n)$ be probability of win when the value is $n$ . Then, $$P(n) = \sum_{i=1}^6 \frac{1}{6}P(n-i)$$ However, solving this recurrence seems complicated and tedious. Is there an easier way to solve this?","This question was asked during my interview: Suppose you have a fair dice (6 faces as usual). You can pick a positive integer . Then you can repeatedly roll a dice until the sum of the rolls exceeds or equals to . If the sum is exactly , it is a win. Otherwise, you lose. Find that maximizes your chance of winning. Let be probability of win when the value is . Then, However, solving this recurrence seems complicated and tedious. Is there an easier way to solve this?",n n n n P(n) n P(n) = \sum_{i=1}^6 \frac{1}{6}P(n-i),['probability']
48,What is the probability that 2 integers have a greatest common factor of 2?,What is the probability that 2 integers have a greatest common factor of 2?,,"If we pick any two positive integers at random, what is the probability that their greatest common factor is 2? I have been wondering about this problem for a while and done some work on it. I started by thinking of a reasonable upper or lower bound. My upper bound was simply $.25$ as that is the probability of picking two even numbers (i.e. having a factor of 2) but if I was to pick (8,12) their gcf is 4. Then I thought about subtracting off the probability that the two numbers chosen had a common factor of 4 and then doing that for 6, 8, 10 and so on which got me to the sum of $1/4 -1/16 - 1/36 - 1/64 -...$ $= 1/2 - \frac{\pi^2}{24}$ I did some experimentation on smaller cases and I do not think my work was right, it should be closer to about $.158$ Also I think my method removed too much like when I subtracted off numbers that had 4 as a factor and numbers that had 6 as a factor I subtracted 12 twice. What should I do to adjust my work or get back on track? I was considering inclusion-exclusion but that seems too complicated for this problem. Thank you for any help","If we pick any two positive integers at random, what is the probability that their greatest common factor is 2? I have been wondering about this problem for a while and done some work on it. I started by thinking of a reasonable upper or lower bound. My upper bound was simply as that is the probability of picking two even numbers (i.e. having a factor of 2) but if I was to pick (8,12) their gcf is 4. Then I thought about subtracting off the probability that the two numbers chosen had a common factor of 4 and then doing that for 6, 8, 10 and so on which got me to the sum of I did some experimentation on smaller cases and I do not think my work was right, it should be closer to about Also I think my method removed too much like when I subtracted off numbers that had 4 as a factor and numbers that had 6 as a factor I subtracted 12 twice. What should I do to adjust my work or get back on track? I was considering inclusion-exclusion but that seems too complicated for this problem. Thank you for any help",.25 1/4 -1/16 - 1/36 - 1/64 -... = 1/2 - \frac{\pi^2}{24} .158,"['probability', 'number-theory', 'coprime']"
49,an interesting game,an interesting game,,"Alice, Bob and Cindy are playing a game of a circle. Firstly, Alice starts by drawing a point around the circle. Subsequently, being aware of Alice's decision Bob makes his move. Finally, Cindy puts a point around the circle being aware of Alice's and Bob's decisions. After all players fix their positions a point X drawn around the circle randomly. The winner of the game is the one whose position is the closest to the point X. Question: How should Bob make his choice in order to maximize the probability of winning?","Alice, Bob and Cindy are playing a game of a circle. Firstly, Alice starts by drawing a point around the circle. Subsequently, being aware of Alice's decision Bob makes his move. Finally, Cindy puts a point around the circle being aware of Alice's and Bob's decisions. After all players fix their positions a point X drawn around the circle randomly. The winner of the game is the one whose position is the closest to the point X. Question: How should Bob make his choice in order to maximize the probability of winning?",,"['probability', 'random']"
50,Expected value of outer product of multivariate normal random vector with itself,Expected value of outer product of multivariate normal random vector with itself,,"Let's say I have a random vector $\boldsymbol{t}$ that is distributed according to a multivariate normal distribution: $$ \boldsymbol{t} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Psi}) $$ I now want to find the expected value of the outer product of this random vector: $$ \mathbb{E}\left[\boldsymbol{t}\boldsymbol{t}^\intercal\right] $$ Is there a closed-form solution to this problem? In my studies, I have stumbled across the Wishart distribution . Might this be a way to tackle this problem?","Let's say I have a random vector that is distributed according to a multivariate normal distribution: I now want to find the expected value of the outer product of this random vector: Is there a closed-form solution to this problem? In my studies, I have stumbled across the Wishart distribution . Might this be a way to tackle this problem?","\boldsymbol{t} 
\boldsymbol{t} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Psi})
 
\mathbb{E}\left[\boldsymbol{t}\boldsymbol{t}^\intercal\right]
","['probability', 'random-variables', 'normal-distribution', 'expected-value', 'outer-product']"
51,Concentration of norm,Concentration of norm,,"Let $(X_1,...,X_n)\in \mathbb{R}^n$ be a random vector with independent sub-gaussian coordinates $X_i$ that satisfy $\mathbb{E}X_i^2=1$ . Then $$||||X||_2-\sqrt{n}||_{\psi_2}\leq CK^2$$ , Where $K=max||X_i||_{\psi_2}$ . In the book ""High dimensional probability"", it is claimed that we can assume $K\geq 1$ and $C$ is a universal constant. But it is not clear why we can do so. Because the above expression is not homogeneous. I tried to change variables but if I change $X_i$ then the property of unit variance no longer holds.","Let be a random vector with independent sub-gaussian coordinates that satisfy . Then , Where . In the book ""High dimensional probability"", it is claimed that we can assume and is a universal constant. But it is not clear why we can do so. Because the above expression is not homogeneous. I tried to change variables but if I change then the property of unit variance no longer holds.","(X_1,...,X_n)\in \mathbb{R}^n X_i \mathbb{E}X_i^2=1 ||||X||_2-\sqrt{n}||_{\psi_2}\leq CK^2 K=max||X_i||_{\psi_2} K\geq 1 C X_i","['probability', 'random-variables', 'normal-distribution']"
52,Probabilty of having exactly one sibling,Probabilty of having exactly one sibling,,"I'm stuck on the following probability problem: parents keep having children until they have one girl, at which point they stop; and babies are girls with probability 0.49. If we select a child uniformly at random (from the entire population of children) what's the probability he or she has exactly one sibling? If a couple has only $2$ children it means that they had first a boy and then a girl. So why the probability asked is not $0.51 \cdot 0.49$? What I am missing? It seems that the correct answer is $2 \cdot 0.51 \cdot (0.49)^2$, but I don't know how to get this result. Thanks for your help.","I'm stuck on the following probability problem: parents keep having children until they have one girl, at which point they stop; and babies are girls with probability 0.49. If we select a child uniformly at random (from the entire population of children) what's the probability he or she has exactly one sibling? If a couple has only $2$ children it means that they had first a boy and then a girl. So why the probability asked is not $0.51 \cdot 0.49$? What I am missing? It seems that the correct answer is $2 \cdot 0.51 \cdot (0.49)^2$, but I don't know how to get this result. Thanks for your help.",,['probability']
53,Covariance matrix of uniform distribution over the Sierpinski triangle,Covariance matrix of uniform distribution over the Sierpinski triangle,,"Let $(X_1, X_2)$ be uniform over the unit Sierpinski triangle (represented in Cartesian coordinates). What is its covariance matrix? This is a question I saw in a jobs ad . I would love some leads on solving it.","Let $(X_1, X_2)$ be uniform over the unit Sierpinski triangle (represented in Cartesian coordinates). What is its covariance matrix? This is a question I saw in a jobs ad . I would love some leads on solving it.",,"['probability', 'recreational-mathematics', 'covariance', 'fractals', 'fractal-analysis']"
54,How good can you approximate a continuous distribution by replacing trailing digits with uniformly random digits?,How good can you approximate a continuous distribution by replacing trailing digits with uniformly random digits?,,Let's say I'm in the business of sampling from the standard normal distribution. And let's ignore floating point issues and other numerical inaccuracies for the moment. Say I want to be lazy and instead of drawing an exact sample from the normal distribution I only get the first 3 digits beyond the decimal point right. After that I just add uniformly random digits. It seems to me that the random number I'm sampling in this fashion will still be fairly close to normally distributed. But in what sense? What kind of formal statement could one make here?,Let's say I'm in the business of sampling from the standard normal distribution. And let's ignore floating point issues and other numerical inaccuracies for the moment. Say I want to be lazy and instead of drawing an exact sample from the normal distribution I only get the first 3 digits beyond the decimal point right. After that I just add uniformly random digits. It seems to me that the random number I'm sampling in this fashion will still be fairly close to normally distributed. But in what sense? What kind of formal statement could one make here?,,"['probability', 'probability-distributions']"
55,If $\operatorname{var}(X) =0$ then $p( X = E (X)) = 1.$,If  then,\operatorname{var}(X) =0 p( X = E (X)) = 1.,Let $X$ be a random variable. Then $\operatorname{var}(X) =0$ implies $p( X = E (X)) = 1.$ Actually i need both directions. But i was able to show that  $p( X = E (X)) = 1$ implies $\operatorname{var}(X) =0.$ So all i need is the other direction. I think we have to use the definition of the variance and set the $\operatorname{var}(X) = 0.$ But why is $p(X = E(X)) = 1$?,Let $X$ be a random variable. Then $\operatorname{var}(X) =0$ implies $p( X = E (X)) = 1.$ Actually i need both directions. But i was able to show that  $p( X = E (X)) = 1$ implies $\operatorname{var}(X) =0.$ So all i need is the other direction. I think we have to use the definition of the variance and set the $\operatorname{var}(X) = 0.$ But why is $p(X = E(X)) = 1$?,,"['probability', 'probability-theory', 'variance']"
56,Who has to buy the beer?,Who has to buy the beer?,,"There are $115$ members in a team in which everyone has a smart phone. No member of the team wishes to buy beer for every other member of the team. But one Friday, after the work hours, everyone expresses the desire to have beer. The team leader proposes a game, which everyone accepts to play. The game is that one team member would send a message "" Beer "" to one of the other team members, who has to forward it to some one other than the person from whom he/she received the message. At the end of the game whoever is the last person to be with the message has to buy beer for everyone else. Someone arbitrarily sets a time-limit for the game. The team leader starts the game by sending the message to someone of his/her team. Assuming that anyone in the team could send message to anyone without any difficulty, what is the probability that at the end of the game, when $2009$ messages have been passed, it's the team-leader who is found to be the one with the last message, and hence, the one to buy beer for everyone else? So, there are $115$ vertices and there is an edge from one to every other vertex. The graph is, thus, $114-$ regular and simple. So, the problem is actually this: Can there be a path starting from a vertex $u$ and after having traversed $2009$ edges, such that one can not leave a vertex by the same edge one came from, culminating back to $u$? Since, the path $u-u$ must contain some cycle, which can be of length $3$ to $115$,the question is does there exist a non-negative solution of: $2x_1+3x_2+...+115x_{114}=2009$ such that such a $u-u$ path exists? For instance, $x_1=1000$ and $x_2=3$ can not be a solution as it would imply that one took the edge $v-v$, $v\in V(G)$, consecutively which is not allowed. How do we solve this?","There are $115$ members in a team in which everyone has a smart phone. No member of the team wishes to buy beer for every other member of the team. But one Friday, after the work hours, everyone expresses the desire to have beer. The team leader proposes a game, which everyone accepts to play. The game is that one team member would send a message "" Beer "" to one of the other team members, who has to forward it to some one other than the person from whom he/she received the message. At the end of the game whoever is the last person to be with the message has to buy beer for everyone else. Someone arbitrarily sets a time-limit for the game. The team leader starts the game by sending the message to someone of his/her team. Assuming that anyone in the team could send message to anyone without any difficulty, what is the probability that at the end of the game, when $2009$ messages have been passed, it's the team-leader who is found to be the one with the last message, and hence, the one to buy beer for everyone else? So, there are $115$ vertices and there is an edge from one to every other vertex. The graph is, thus, $114-$ regular and simple. So, the problem is actually this: Can there be a path starting from a vertex $u$ and after having traversed $2009$ edges, such that one can not leave a vertex by the same edge one came from, culminating back to $u$? Since, the path $u-u$ must contain some cycle, which can be of length $3$ to $115$,the question is does there exist a non-negative solution of: $2x_1+3x_2+...+115x_{114}=2009$ such that such a $u-u$ path exists? For instance, $x_1=1000$ and $x_2=3$ can not be a solution as it would imply that one took the edge $v-v$, $v\in V(G)$, consecutively which is not allowed. How do we solve this?",,"['probability', 'discrete-mathematics', 'graph-theory']"
57,Contraction Property of Conditional Expectation,Contraction Property of Conditional Expectation,,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space with $\Omega$ finite. From Jensen's inequality $$\big|\; \mathbb{E}[X|\mathcal{G}] \;\big|^{p} \;\leq\; \mathbb{E}\big[\, |X|^{p}\,|\,\mathcal{G}\,\big] \quad\mathbb{P}\text{-a.s.}$$ for $p\geq 1$, $\mathcal{G}\subset\mathcal{F}$ and $X:\Omega\rightarrow\mathbb{R}$. By taking expectations, this directly gives the famous contraction property $$\big\|\; \mathbb{E}[X|\mathcal{G}] \;\big\|_{p} \;\leq\; \|\; X \;\|_{p}$$ for $p\geq 1$, $\mathcal{G}\subset\mathcal{F}$ and $X:\Omega\rightarrow\mathbb{R}$, where $\|X\|_{p}:=\mathbb{E}\big[\,|X|^{p}\,\big]$. Is it possible to generalize this result to arbitrary norms $\|\cdot\|$ on $\mathbb{R}^{d}$, where $d=|\Omega|$? Can somebody contruct a counter example? Thanks!","Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space with $\Omega$ finite. From Jensen's inequality $$\big|\; \mathbb{E}[X|\mathcal{G}] \;\big|^{p} \;\leq\; \mathbb{E}\big[\, |X|^{p}\,|\,\mathcal{G}\,\big] \quad\mathbb{P}\text{-a.s.}$$ for $p\geq 1$, $\mathcal{G}\subset\mathcal{F}$ and $X:\Omega\rightarrow\mathbb{R}$. By taking expectations, this directly gives the famous contraction property $$\big\|\; \mathbb{E}[X|\mathcal{G}] \;\big\|_{p} \;\leq\; \|\; X \;\|_{p}$$ for $p\geq 1$, $\mathcal{G}\subset\mathcal{F}$ and $X:\Omega\rightarrow\mathbb{R}$, where $\|X\|_{p}:=\mathbb{E}\big[\,|X|^{p}\,\big]$. Is it possible to generalize this result to arbitrary norms $\|\cdot\|$ on $\mathbb{R}^{d}$, where $d=|\Omega|$? Can somebody contruct a counter example? Thanks!",,"['probability', 'lp-spaces', 'conditional-expectation']"
58,expected distance btw random point inside the circle and its centre,expected distance btw random point inside the circle and its centre,,"Let us randomly choose a point inside the circle of radius $1$. Find the expected distance between the point and the centre of the circle. I approached it in a such way:  no matter where we choose a point, we can draw a radius from the centre such that this point will be on it. Thus, this problem is about finding the expected point randomly on the interval $(0,1)$, which is $0.5$, obviously. But I really can't figure out how to show that the expected point on this interval is $0.5$. 😤","Let us randomly choose a point inside the circle of radius $1$. Find the expected distance between the point and the centre of the circle. I approached it in a such way:  no matter where we choose a point, we can draw a radius from the centre such that this point will be on it. Thus, this problem is about finding the expected point randomly on the interval $(0,1)$, which is $0.5$, obviously. But I really can't figure out how to show that the expected point on this interval is $0.5$. 😤",,"['probability', 'statistics']"
59,What is Moment Generating Function simply explained?,What is Moment Generating Function simply explained?,,"I'm taking Econometrics this Trimester and there a quite few things I don't get yet, and one of them is MGF. I have tried Wikipedia etc. but seems to be written in a bit more advanced language. Any good explanation or link? Thanks","I'm taking Econometrics this Trimester and there a quite few things I don't get yet, and one of them is MGF. I have tried Wikipedia etc. but seems to be written in a bit more advanced language. Any good explanation or link? Thanks",,"['probability', 'statistics', 'moment-generating-functions']"
60,Is there a fundamental reason to expect $e$ to appear in this probability question?,Is there a fundamental reason to expect  to appear in this probability question?,e,"I came across the following question, whose answer is $e$. I was sort of amazed, since I didn't see a reason why $e$ should be making an appearance. So, phrasing the main question in the title another way: should it have been possible to predict that at least my answer should involve $e$ in some nontrivial way? Question: Suppose you draw random variables $x_i \in [0,1]$, where the $x_i$ are uniformly distributed. If $x_i > x_{i-1}$, we draw again, and otherwise, we stop. What's the expected number of draws before we stop? Solution: The probability that we take $n$ draws is given by $\frac{n-1}{n!}$ since there are $n!$ ways to order $x_1,\ldots, x_n$, and exactly $n-1$ choices for the placement of $x_{n-1}$ in the ordering $x_1 < x_1 < \cdots < x_{\widehat{n-1}} < x_n$. That is, for it to take precisely $n$ draws, we need $x_1 < x_2, x_2 < x_3, \ldots, x_{n-2} < x_{n-1}$ but then $x_n < x_{n-1}$. Thus, the expected value is given by $$ E(\text{number of draws}) = \sum_{n = 2}^\infty \frac{1}{(n-2)!}  = \fbox{e} $$ P.S. It's also possible I simply made a mistake, and if that's the case please point it out and I can edit or delete the question accordingly.","I came across the following question, whose answer is $e$. I was sort of amazed, since I didn't see a reason why $e$ should be making an appearance. So, phrasing the main question in the title another way: should it have been possible to predict that at least my answer should involve $e$ in some nontrivial way? Question: Suppose you draw random variables $x_i \in [0,1]$, where the $x_i$ are uniformly distributed. If $x_i > x_{i-1}$, we draw again, and otherwise, we stop. What's the expected number of draws before we stop? Solution: The probability that we take $n$ draws is given by $\frac{n-1}{n!}$ since there are $n!$ ways to order $x_1,\ldots, x_n$, and exactly $n-1$ choices for the placement of $x_{n-1}$ in the ordering $x_1 < x_1 < \cdots < x_{\widehat{n-1}} < x_n$. That is, for it to take precisely $n$ draws, we need $x_1 < x_2, x_2 < x_3, \ldots, x_{n-2} < x_{n-1}$ but then $x_n < x_{n-1}$. Thus, the expected value is given by $$ E(\text{number of draws}) = \sum_{n = 2}^\infty \frac{1}{(n-2)!}  = \fbox{e} $$ P.S. It's also possible I simply made a mistake, and if that's the case please point it out and I can edit or delete the question accordingly.",,"['probability', 'probability-distributions', 'uniform-distribution']"
61,"""X is distributed as ..."" notation","""X is distributed as ..."" notation",,"A question about notation: We sometimes use $\sim$ to denote ""distributed as"" e.g. if $X$ is Gaussian we write $X \sim N(\mu, \sigma^2)$. Is it acceptable to use the ""~"" notation for an arbitrary distribution? e.g. can we write $$X \sim \begin{cases} \frac{3}{2}x^2, & x \in [-1,1] \\0 & \text{otherwise} \end{cases}$$ If not, is there another way to write ""$X$ is distributed as ...""? i.e. without the more verbose ""Let $p_X(x)$ be the pdf of $X$. Then $p_X(x)=$ ...","A question about notation: We sometimes use $\sim$ to denote ""distributed as"" e.g. if $X$ is Gaussian we write $X \sim N(\mu, \sigma^2)$. Is it acceptable to use the ""~"" notation for an arbitrary distribution? e.g. can we write $$X \sim \begin{cases} \frac{3}{2}x^2, & x \in [-1,1] \\0 & \text{otherwise} \end{cases}$$ If not, is there another way to write ""$X$ is distributed as ...""? i.e. without the more verbose ""Let $p_X(x)$ be the pdf of $X$. Then $p_X(x)=$ ...",,"['probability', 'probability-distributions', 'notation']"
62,"What is the probability of selecting 0 from the [0, 1] set?","What is the probability of selecting 0 from the [0, 1] set?",,I'm a high school student so don't get mad at me for asking this question. So if $p(i)$ equals to probability of getting $i$ as outcome then we have: 1 - For every $i$: $p(i)=p$ 2 - $lim_{n\to\infty} (np)=1$ So p is exactly 0. Because if p is some real number greater than zero then $pn$ can become infinitely large. But it contradicts with the fact that $lim_{n\to\infty}(pn)=1$! because $0.n=0$ not $1$! Thanks in advance :).,I'm a high school student so don't get mad at me for asking this question. So if $p(i)$ equals to probability of getting $i$ as outcome then we have: 1 - For every $i$: $p(i)=p$ 2 - $lim_{n\to\infty} (np)=1$ So p is exactly 0. Because if p is some real number greater than zero then $pn$ can become infinitely large. But it contradicts with the fact that $lim_{n\to\infty}(pn)=1$! because $0.n=0$ not $1$! Thanks in advance :).,,"['probability', 'limits']"
63,The measure-theoretical definition of a bootstrap sample,The measure-theoretical definition of a bootstrap sample,,"I’m currently learning the bootstrap method, and I have two questions to ask about the definition of a bootstrap sample. Let $ (\Omega,\mathscr{S},\mathsf{P}) $ be a probability space. Let $ X_{1},\ldots,X_{n} $ be i.i.d. random variables on $ (\Omega,\mathscr{S},\mathsf{P}) $, with their common c.d.f. denoted by $ F $. Let $ \hat{F} $ denote the empirical c.d.f. of $ X_{1},\ldots,X_{n} $, i.e., $$ \forall x \in \mathbf{R}: \qquad \hat{F}(x) = \frac{1}{n} \sum_{i = 1}^{n} \chi_{(- \infty,x]} \circ X_{i}. $$ Clearly, $ \hat{F}(x) $ is a random variable on $ (\Omega,\mathscr{S},\mathsf{P}) $ for each $ x \in \mathbf{R} $, and for each $ \omega \in \Omega $, the function $$ \left\{ \begin{matrix} \mathbf{R} & \to & [0,1] \\ x & \mapsto & \left[ \hat{F}(x) \right] \! (\omega)\end{matrix} \right\} $$ is the c.d.f. of some discrete random variable. Question 1: What does it mean to say that $ (X_{1}^{*},\ldots,X_{n}^{*}) $ is a bootstrap sample drawn from $ \hat{F} $? As mentioned, $ \hat{F}(x) $ is not a number but a random variable for each $ x \in \mathbf{R} $. I require an answer to this question strictly in terms of measure theory. Question 2: What probability space are $ X_{1}^{*},\ldots,X_{n}^{*} $ defined on? Is it still $ (\Omega,\mathscr{S},\mathsf{P}) $? Thanks!","I’m currently learning the bootstrap method, and I have two questions to ask about the definition of a bootstrap sample. Let $ (\Omega,\mathscr{S},\mathsf{P}) $ be a probability space. Let $ X_{1},\ldots,X_{n} $ be i.i.d. random variables on $ (\Omega,\mathscr{S},\mathsf{P}) $, with their common c.d.f. denoted by $ F $. Let $ \hat{F} $ denote the empirical c.d.f. of $ X_{1},\ldots,X_{n} $, i.e., $$ \forall x \in \mathbf{R}: \qquad \hat{F}(x) = \frac{1}{n} \sum_{i = 1}^{n} \chi_{(- \infty,x]} \circ X_{i}. $$ Clearly, $ \hat{F}(x) $ is a random variable on $ (\Omega,\mathscr{S},\mathsf{P}) $ for each $ x \in \mathbf{R} $, and for each $ \omega \in \Omega $, the function $$ \left\{ \begin{matrix} \mathbf{R} & \to & [0,1] \\ x & \mapsto & \left[ \hat{F}(x) \right] \! (\omega)\end{matrix} \right\} $$ is the c.d.f. of some discrete random variable. Question 1: What does it mean to say that $ (X_{1}^{*},\ldots,X_{n}^{*}) $ is a bootstrap sample drawn from $ \hat{F} $? As mentioned, $ \hat{F}(x) $ is not a number but a random variable for each $ x \in \mathbf{R} $. I require an answer to this question strictly in terms of measure theory. Question 2: What probability space are $ X_{1}^{*},\ldots,X_{n}^{*} $ defined on? Is it still $ (\Omega,\mathscr{S},\mathsf{P}) $? Thanks!",,"['probability', 'probability-theory', 'statistics', 'random-variables', 'bootstrap-sampling']"
64,"Expected value question regarding bijections of $\{1,2,\ldots,n\}$",Expected value question regarding bijections of,"\{1,2,\ldots,n\}","Context: this is just a problem I thought of for fun. Let $\Sigma_n$ be the symmetric group on $\{1,2,\ldots n\}$. We define a random variable $X$ on $\Sigma_n$ by: $$X(f)=\mbox{max}\{k\mid\forall j\leq k, f(j)\geq j\}.$$ In other words, $X(f)$ is the largest $k$ such that, in the graph of $f$, the points $(1,f(1))$, $(2,f(2))$, $\cdots$, $(k,f(k))$ all lie on or above the diagonal. My question is: What is $E[X]$? Is there even a ""nice"" way of finding or expressing it? If not, is there a way of estimating $E[X]$?","Context: this is just a problem I thought of for fun. Let $\Sigma_n$ be the symmetric group on $\{1,2,\ldots n\}$. We define a random variable $X$ on $\Sigma_n$ by: $$X(f)=\mbox{max}\{k\mid\forall j\leq k, f(j)\geq j\}.$$ In other words, $X(f)$ is the largest $k$ such that, in the graph of $f$, the points $(1,f(1))$, $(2,f(2))$, $\cdots$, $(k,f(k))$ all lie on or above the diagonal. My question is: What is $E[X]$? Is there even a ""nice"" way of finding or expressing it? If not, is there a way of estimating $E[X]$?",,"['probability', 'combinatorics']"
65,Distribution of interarrival times in a Poisson process,Distribution of interarrival times in a Poisson process,,"I am new to Statistics. I am studying Poisson process, I have certain questions to ask. A process of arrival times in continuous time is called a Poisson process of rate $\lambda$ if the following two conditions hold: The number of arrivals in an interval of length $t$ is $\text{Pois}(\lambda t)$ random variable. The number of arrivals that occur in disjoint time intervals are independent of each other. Let $X_1$ denote the time of first arrival in a Poisson process of rate $\lambda$. Let $X_2$ denote the time elapsed between the first arrival and the second arrival. We can find the distribution of $X_1$ as follows: $$\mathbb{P}(X_1>t)=\mathbb{P}\left(\text{No arrivals in }[0,t]\right)=\mathrm{e}^{-\lambda t}$$ Thus $\mathbb{P}(X_1\le t)=1-\mathrm{e}^{-\lambda t}$, and hence $X_1\sim\text{Expo}(\lambda)$. Suppose we want to find the conditional distribution of $X_2$ given $X_1$. I found the following discussion in my textbook. $\begin{equation}\begin{split}\mathbb{P}(X_2>t\mid X_1=s) & = \mathbb{P}\left(\text{No arrivals in }(s,s+t] \mid \text{Exactly one arrival in [0,s]} \right) \\ & =\mathbb{P}\left(\text{No arrivals in }(s,s+t]\right)\\ &=\mathrm{e}^{-\lambda t}\end{split}\end{equation}$. Thus, $X_1$ and $X_2$ are independent, and $X_2\sim\text{Expo}(\lambda)$. However, I have the following questions regarding the above discussion. Since $X_1$ is a continuous random variable, $\mathbb{P}(X_1=k)=0$ for every $k\in\mathbb{R}$. Thus, $\mathbb{P}(X_1=s)=0$. In other words, we are conditioning on an event with zero probability. But when I studied conditional probability, conditioning on events with zero probability was not defined. So in this case, is conditioning on an event with zero probability valid? Second, assuming that conditioning on $X_1=s$ is valid, what we have found is the conditional distribution of $X_2$ given $X_1=s$. In other words, the conditional distribution of $X_2$ given $X_1$ is $\text{Expo}(\lambda)$, not the distribution of $X_2$ itself. But the author claims that $X_2\sim\text{Expo}(\lambda)$. Why is this true?","I am new to Statistics. I am studying Poisson process, I have certain questions to ask. A process of arrival times in continuous time is called a Poisson process of rate $\lambda$ if the following two conditions hold: The number of arrivals in an interval of length $t$ is $\text{Pois}(\lambda t)$ random variable. The number of arrivals that occur in disjoint time intervals are independent of each other. Let $X_1$ denote the time of first arrival in a Poisson process of rate $\lambda$. Let $X_2$ denote the time elapsed between the first arrival and the second arrival. We can find the distribution of $X_1$ as follows: $$\mathbb{P}(X_1>t)=\mathbb{P}\left(\text{No arrivals in }[0,t]\right)=\mathrm{e}^{-\lambda t}$$ Thus $\mathbb{P}(X_1\le t)=1-\mathrm{e}^{-\lambda t}$, and hence $X_1\sim\text{Expo}(\lambda)$. Suppose we want to find the conditional distribution of $X_2$ given $X_1$. I found the following discussion in my textbook. $\begin{equation}\begin{split}\mathbb{P}(X_2>t\mid X_1=s) & = \mathbb{P}\left(\text{No arrivals in }(s,s+t] \mid \text{Exactly one arrival in [0,s]} \right) \\ & =\mathbb{P}\left(\text{No arrivals in }(s,s+t]\right)\\ &=\mathrm{e}^{-\lambda t}\end{split}\end{equation}$. Thus, $X_1$ and $X_2$ are independent, and $X_2\sim\text{Expo}(\lambda)$. However, I have the following questions regarding the above discussion. Since $X_1$ is a continuous random variable, $\mathbb{P}(X_1=k)=0$ for every $k\in\mathbb{R}$. Thus, $\mathbb{P}(X_1=s)=0$. In other words, we are conditioning on an event with zero probability. But when I studied conditional probability, conditioning on events with zero probability was not defined. So in this case, is conditioning on an event with zero probability valid? Second, assuming that conditioning on $X_1=s$ is valid, what we have found is the conditional distribution of $X_2$ given $X_1=s$. In other words, the conditional distribution of $X_2$ given $X_1$ is $\text{Expo}(\lambda)$, not the distribution of $X_2$ itself. But the author claims that $X_2\sim\text{Expo}(\lambda)$. Why is this true?",,"['probability', 'probability-theory', 'probability-distributions', 'poisson-process']"
66,"Probability that $5$ numbers chosen randomly in the interval $[0, 1]$ all lie in the middle half of the interval.",Probability that  numbers chosen randomly in the interval  all lie in the middle half of the interval.,"5 [0, 1]","My attempt: Have $5$ random variables uniform on the interval $[0, 1]$, with cdf $F(x) = x$. The middle half of the interval is $[\frac{1}{4}, \frac{3}{4}]$. The probability that one of the random variables lies in this interval is $$ p = F\Big(\frac{3}{4}\Big) - F\Big(\frac{1}{4}\Big) = \frac{3}{4} - \frac{1}{4} = \frac{1}{2} .$$ The probability that all five random variables lie in this interval is then $$ p^5 = \Big(\frac{1}{2}\Big)^5 = \frac{1}{32} .$$ There was no solution for this exercise in my textbook and I couldn't find the answer anywhere online. Hoping somebody can verify if this is the correct answer.","My attempt: Have $5$ random variables uniform on the interval $[0, 1]$, with cdf $F(x) = x$. The middle half of the interval is $[\frac{1}{4}, \frac{3}{4}]$. The probability that one of the random variables lies in this interval is $$ p = F\Big(\frac{3}{4}\Big) - F\Big(\frac{1}{4}\Big) = \frac{3}{4} - \frac{1}{4} = \frac{1}{2} .$$ The probability that all five random variables lie in this interval is then $$ p^5 = \Big(\frac{1}{2}\Big)^5 = \frac{1}{32} .$$ There was no solution for this exercise in my textbook and I couldn't find the answer anywhere online. Hoping somebody can verify if this is the correct answer.",,"['probability', 'order-statistics']"
67,Probability of choosing same color ball 2 times in a row,Probability of choosing same color ball 2 times in a row,,"My son's teacher and I are having a debate about the correct answer to a question.  I have an engineer at hand and he has a mathematician so we both feel well supported.  We've also both researched the internet and found answers that we feel support our answers.  Since his answer came from this website, I decided to ask this same website.  Here is the question from my son's 6th grade quiz: ""A box contains three balls of different colors. The colors are red, white and blue. What is the probability of choosing the same color ball 2 times in a row?"" Your choices are: A. 2/3             B. 1/9             C. 1/3            D. 2/27 My son answered B. 1/9.  I concurred and so did my husband, the engineer.  The test answer sheet also said 1/9.  His teacher says it would be 1/3 and he emailed me an explanation from this website about 4 red and 6 white balls (link: Probability of first and second drawn balls of the same color, without replacement ). Honestly, I don't see it.  I think that explanation would support 1/9. We've spent quite a bit of time on this so we really have tried to reconcile on this.  Could you help us with this simpler sample set of only 3 balls, one of each color?  Note, we have agreed that we can assume that the first ball was replaced since there is no zero% probability in the available answers. We think that the first pick would 1/3 for any color and the second pick (with the ball replaced in the set) would be 1/3 again.  1/3 x 1/3 = 1/9 He says that the first pick would be a 3/3 (100%) chance of picking any color since the color is not specified, second pick 1/3 so 1/1 x 1/3 = 1/3 I'd really appreciate your help. Thank you.","My son's teacher and I are having a debate about the correct answer to a question.  I have an engineer at hand and he has a mathematician so we both feel well supported.  We've also both researched the internet and found answers that we feel support our answers.  Since his answer came from this website, I decided to ask this same website.  Here is the question from my son's 6th grade quiz: ""A box contains three balls of different colors. The colors are red, white and blue. What is the probability of choosing the same color ball 2 times in a row?"" Your choices are: A. 2/3             B. 1/9             C. 1/3            D. 2/27 My son answered B. 1/9.  I concurred and so did my husband, the engineer.  The test answer sheet also said 1/9.  His teacher says it would be 1/3 and he emailed me an explanation from this website about 4 red and 6 white balls (link: Probability of first and second drawn balls of the same color, without replacement ). Honestly, I don't see it.  I think that explanation would support 1/9. We've spent quite a bit of time on this so we really have tried to reconcile on this.  Could you help us with this simpler sample set of only 3 balls, one of each color?  Note, we have agreed that we can assume that the first ball was replaced since there is no zero% probability in the available answers. We think that the first pick would 1/3 for any color and the second pick (with the ball replaced in the set) would be 1/3 again.  1/3 x 1/3 = 1/9 He says that the first pick would be a 3/3 (100%) chance of picking any color since the color is not specified, second pick 1/3 so 1/1 x 1/3 = 1/3 I'd really appreciate your help. Thank you.",,['probability']
68,The PDF of the area of a random triangle in a square,The PDF of the area of a random triangle in a square,,"Three points are chosen uniformly and independently at random on the unit rectangle $(0,1)\times(0,1)$. What is the probability distribution (in the form of a cumulative distribution function and/or a probability density function supported on $[0,1/2]$) of the area of the triangle whose vertices are so chosen?  What are the moments of this distribution? The area might be expressed as the norm of the sum of three three-dimensional vector cross products,  as $$\frac12\|p_1\times p_2+p_2\times p_3+p_3\times p_1\|.$$ Closely related: Probability of lies a point in a random triangle . See also http://mathworld.wolfram.com/SquareTrianglePicking.html and http://www.diva-portal.org/smash/get/diva2:644463/FULLTEXT01.pdf","Three points are chosen uniformly and independently at random on the unit rectangle $(0,1)\times(0,1)$. What is the probability distribution (in the form of a cumulative distribution function and/or a probability density function supported on $[0,1/2]$) of the area of the triangle whose vertices are so chosen?  What are the moments of this distribution? The area might be expressed as the norm of the sum of three three-dimensional vector cross products,  as $$\frac12\|p_1\times p_2+p_2\times p_3+p_3\times p_1\|.$$ Closely related: Probability of lies a point in a random triangle . See also http://mathworld.wolfram.com/SquareTrianglePicking.html and http://www.diva-portal.org/smash/get/diva2:644463/FULLTEXT01.pdf",,"['probability', 'geometry']"
69,What is the probability that $7$ cards are chosen and no suit is missing?,What is the probability that  cards are chosen and no suit is missing?,7,"Cards are drawn one by one from a regular deck ($13$ cards for each of the   $4$ suits). If $7$ cards are drawn, what is the probability that no suit will be   missing? Ok, so I tried the approach where I choose the $1$ suit out of $4$ and then I don't know what do next. I dont know how am I supposed to arrange the cards in such a random manner, and I found the total which is obvious, $52$ choose $7$.","Cards are drawn one by one from a regular deck ($13$ cards for each of the   $4$ suits). If $7$ cards are drawn, what is the probability that no suit will be   missing? Ok, so I tried the approach where I choose the $1$ suit out of $4$ and then I don't know what do next. I dont know how am I supposed to arrange the cards in such a random manner, and I found the total which is obvious, $52$ choose $7$.",,['probability']
70,Mario Party 3 Mini-game Probability Question,Mario Party 3 Mini-game Probability Question,,"I have a question about a mini-game in Mario Party 3. I have extracted the mathematical information from the game below. Setup : Four players $A,B,C$, and $D$ line up in some order. There are $12$ cards which are face-down on a table in front of the first person in line. The cards are as follows: $6$ blue cards, $4$ green cards, and $2$ gray cards. Gameplay : The players take turns, based on their order in line, flipping over a card. If the card is blue, the player is safe and goes to the back of the line and the card is discarded. If the card is green, the player is out, removed, and the green card is discarded. If the card is gray, the player remains in the game and the order of the remaining players (including the player that drew the gray card) in the line is randomly shuffled; the gray card is discarded. The game ends when there is only one person left. My Question : Let's say the initial order of the line-up is $A, B, C, D$. I was wondering if there was a way to determine which player has the best, second best, third best, and worst chance of winning the game. I'm not sure if this is possible since the two gray cards shuffle the line-up when they are flipped over. I would also be interested in solutions of the simplified version of the game with $10$ cards ($6$ blue and $4$ green) in which the gray cards have been removed. Thanks!","I have a question about a mini-game in Mario Party 3. I have extracted the mathematical information from the game below. Setup : Four players $A,B,C$, and $D$ line up in some order. There are $12$ cards which are face-down on a table in front of the first person in line. The cards are as follows: $6$ blue cards, $4$ green cards, and $2$ gray cards. Gameplay : The players take turns, based on their order in line, flipping over a card. If the card is blue, the player is safe and goes to the back of the line and the card is discarded. If the card is green, the player is out, removed, and the green card is discarded. If the card is gray, the player remains in the game and the order of the remaining players (including the player that drew the gray card) in the line is randomly shuffled; the gray card is discarded. The game ends when there is only one person left. My Question : Let's say the initial order of the line-up is $A, B, C, D$. I was wondering if there was a way to determine which player has the best, second best, third best, and worst chance of winning the game. I'm not sure if this is possible since the two gray cards shuffle the line-up when they are flipped over. I would also be interested in solutions of the simplified version of the game with $10$ cards ($6$ blue and $4$ green) in which the gray cards have been removed. Thanks!",,"['probability', 'recreational-mathematics']"
71,How many times $n$ must you play a game in which you have a $1/N$ chance of winning to have a better than 50% chance of winning at least once,How many times  must you play a game in which you have a  chance of winning to have a better than 50% chance of winning at least once,n 1/N,"I am having difficulty approaching the above problem, and would like a hint. I tried doing an inclusion exclusion argument: Let $A_{i}$ be winning the game on the i'th try, then by inclusion exclusion we have, for winning at least 1 game in n tries: $$P\left(\bigcup_{i=1}^{n}A_i\right)=\frac{n}{N}-{n\choose 2}\frac{1}{N^2}+{n\choose 3}\frac{1}{N^2}+.....\pm \frac{1}{N^{n}} $$ unless I am missing something, I don't see how I could evaluate numerically the probability without knowing the value of $N$ edit: We have spent a fair amount of time on stirling's formula, so maybe this would be a way to at least make sense of the choose function portion?","I am having difficulty approaching the above problem, and would like a hint. I tried doing an inclusion exclusion argument: Let $A_{i}$ be winning the game on the i'th try, then by inclusion exclusion we have, for winning at least 1 game in n tries: $$P\left(\bigcup_{i=1}^{n}A_i\right)=\frac{n}{N}-{n\choose 2}\frac{1}{N^2}+{n\choose 3}\frac{1}{N^2}+.....\pm \frac{1}{N^{n}} $$ unless I am missing something, I don't see how I could evaluate numerically the probability without knowing the value of $N$ edit: We have spent a fair amount of time on stirling's formula, so maybe this would be a way to at least make sense of the choose function portion?",,['probability']
72,Probability of drawing cards in ascending order,Probability of drawing cards in ascending order,,Given 200 cards where each card has a unique number from 1 to 200. We randomly pick 30 cards (the order we pick them matters). What is the probability the unique numbers of the cards we pick are in ascending order?,Given 200 cards where each card has a unique number from 1 to 200. We randomly pick 30 cards (the order we pick them matters). What is the probability the unique numbers of the cards we pick are in ascending order?,,['probability']
73,"If $X_n$ converges in distribution to $X$, is it true that $\alpha_n X_n$ converges to $ \alpha X$ as well?","If  converges in distribution to , is it true that  converges to  as well?",X_n X \alpha_n X_n  \alpha X,Suppose we have a sequence of non-negative random variables $(X_n)_n$ converging weakly to the random variable $X$. Let also $(\alpha_n)$ be a sequence of positive numbers converging to $\alpha>0$. I'm stuck in proving or disproving that $\alpha_k X_n$ converge in distribution to $\alpha X$. Any suggestion?,Suppose we have a sequence of non-negative random variables $(X_n)_n$ converging weakly to the random variable $X$. Let also $(\alpha_n)$ be a sequence of positive numbers converging to $\alpha>0$. I'm stuck in proving or disproving that $\alpha_k X_n$ converge in distribution to $\alpha X$. Any suggestion?,,"['probability', 'probability-distributions', 'convergence-divergence', 'probability-limit-theorems']"
74,Expected value of probability of intersection.,Expected value of probability of intersection.,,"let's assume we choose at random two subsets $A$ and $B$ of a finite set $X$ (i.e. $|X|=n$). By randomness I mean that $Pr[x\in A] = \frac1{n}$ for each $x\in X$, the same for $B$. What will be the average size of their intersection? More formally, I would like to compute: $$E[|A \cap B|\ |\ |A|, |B|]$$ I tried to write it as a sum of expected values with additional condition about the size of $A\cap B$, but it leads me to a result which is not necessarily less than $1$, so either I've made a mistake or my whole reasoning has been wrong. Is there any tricky computation of this expression?","let's assume we choose at random two subsets $A$ and $B$ of a finite set $X$ (i.e. $|X|=n$). By randomness I mean that $Pr[x\in A] = \frac1{n}$ for each $x\in X$, the same for $B$. What will be the average size of their intersection? More formally, I would like to compute: $$E[|A \cap B|\ |\ |A|, |B|]$$ I tried to write it as a sum of expected values with additional condition about the size of $A\cap B$, but it leads me to a result which is not necessarily less than $1$, so either I've made a mistake or my whole reasoning has been wrong. Is there any tricky computation of this expression?",,"['probability', 'conditional-expectation']"
75,"Application of Stein's Lemma to Calculate Moments of Normal (0,1)","Application of Stein's Lemma to Calculate Moments of Normal (0,1)",,"Say we have $X \sim N(0,1)$. I was wondering how we can use Stein's Lemma to derive the moments of the r.v. $X$ by calculating the first few moments. How would you summarize it in the form $EX^n$ if the moments we calculated end up being quite different in form?  Or are they? So what I've tried to do so far is that I used the formula: $E\bigl(g(X)(X-\mu)\bigr)=\sigma^2 E\bigl(g'(X)\bigr)$ And from there, I tried to see if I can make any substitutions but I don't think you can.  Furthermore, I really don't see how you can even calculate moments from this thing.  From class, I think you have to use Taylor expansion but I really don't see how you can get to that here either.  I really need some help on this.","Say we have $X \sim N(0,1)$. I was wondering how we can use Stein's Lemma to derive the moments of the r.v. $X$ by calculating the first few moments. How would you summarize it in the form $EX^n$ if the moments we calculated end up being quite different in form?  Or are they? So what I've tried to do so far is that I used the formula: $E\bigl(g(X)(X-\mu)\bigr)=\sigma^2 E\bigl(g'(X)\bigr)$ And from there, I tried to see if I can make any substitutions but I don't think you can.  Furthermore, I really don't see how you can even calculate moments from this thing.  From class, I think you have to use Taylor expansion but I really don't see how you can get to that here either.  I really need some help on this.",,"['probability', 'probability-theory', 'normal-distribution']"
76,Empirical Kullback-Leibler divergence of two time series,Empirical Kullback-Leibler divergence of two time series,,"I have an two vectors (time series) with the same length (1200 elements) $x$ and $y$. Further both time series are stationary. I don't know the theoretical distribution of $x$ and $y$. I would like to calculate relative entropy of these r.v.-s. I think I have to use an empirical distribution function. Have you any ideas how to calculate Kullback-Leibler divergence of two time series, with different distribution? Thanks a lot in advance!","I have an two vectors (time series) with the same length (1200 elements) $x$ and $y$. Further both time series are stationary. I don't know the theoretical distribution of $x$ and $y$. I would like to calculate relative entropy of these r.v.-s. I think I have to use an empirical distribution function. Have you any ideas how to calculate Kullback-Leibler divergence of two time series, with different distribution? Thanks a lot in advance!",,"['probability', 'statistics', 'statistical-inference', 'information-theory', 'time-series']"
77,"Coin toss problem, get exactly 2 heads in 5 tosses","Coin toss problem, get exactly 2 heads in 5 tosses",,"Suppose we toss a fair coin until we get exactly 2 heads. What is   the probability that exactly 5 tosses are required? My try:  We have to make sure that the first 4 tosses does not have 2 heads and the last toss must be a head. That is, the first 4 tosses need to contain 1 head and 3 tails. The probability of this event is $\frac{4}{2^4}=1/4$. Then the probability of 5th toss is head is $1/2$. Hence, in the end the answer is $\frac{1}{4}\cdot\frac{1}{2}=\frac{1}{8}$. Am I correct?","Suppose we toss a fair coin until we get exactly 2 heads. What is   the probability that exactly 5 tosses are required? My try:  We have to make sure that the first 4 tosses does not have 2 heads and the last toss must be a head. That is, the first 4 tosses need to contain 1 head and 3 tails. The probability of this event is $\frac{4}{2^4}=1/4$. Then the probability of 5th toss is head is $1/2$. Hence, in the end the answer is $\frac{1}{4}\cdot\frac{1}{2}=\frac{1}{8}$. Am I correct?",,['probability']
78,Is conditional entropy ever taken to be a random variable?,Is conditional entropy ever taken to be a random variable?,,"In probability theory, the conditional expectation $E(X|Y)$ and variance $V(X|Y)$ er usually taken to be random variables, st. the value of $E(X|Y)$ depends on what value $Y$ ends up taking. I've just started learning information theory, but I get the impression that conditionals are usually ""averaged out"", st. $H(X|Y)$ really means $EH(X|Y)$. I suppose because it just turns out more practical that way. Is that a correct distinction to make? Are there ever examples in information theory where the conditional entropy (or divergence etc.) is taken to be a random variable and not ""averaged out""?","In probability theory, the conditional expectation $E(X|Y)$ and variance $V(X|Y)$ er usually taken to be random variables, st. the value of $E(X|Y)$ depends on what value $Y$ ends up taking. I've just started learning information theory, but I get the impression that conditionals are usually ""averaged out"", st. $H(X|Y)$ really means $EH(X|Y)$. I suppose because it just turns out more practical that way. Is that a correct distinction to make? Are there ever examples in information theory where the conditional entropy (or divergence etc.) is taken to be a random variable and not ""averaged out""?",,"['probability', 'probability-theory', 'information-theory', 'conditional-expectation', 'entropy']"
79,"Intuition of joint density of min(X,Y) and max(X,Y)","Intuition of joint density of min(X,Y) and max(X,Y)",,"The problem is to find the joint density of $U = min(X,Y)$ and $V=max(X,Y)$ when both are exponential random variables. The solution to it is: I can finish it after the first step but I don't understand the intuition behind $$P(U \le u, V \le v) = P(V\le v)-P(U>u, V\le v) $$ I understand that it's because $P(A\cap B) = P(B) -  P(A^c\cap B)$ but what would make you think that it's a good idea to start off by rewriting it like this?",The problem is to find the joint density of and when both are exponential random variables. The solution to it is: I can finish it after the first step but I don't understand the intuition behind I understand that it's because but what would make you think that it's a good idea to start off by rewriting it like this?,"U = min(X,Y) V=max(X,Y) P(U \le u, V \le v) = P(V\le v)-P(U>u, V\le v)  P(A\cap B) = P(B) -  P(A^c\cap B)","['probability', 'probability-distributions']"
80,Comparing the probabilities of rolling a $12$ or two consecutive $7$'s first with a pair of dice,Comparing the probabilities of rolling a  or two consecutive 's first with a pair of dice,12 7,A pair of dice is rolled repeatedly which event is more likely to occur first? Event $A$ both dice shows 6's. Event $B$ two consecutive rolls give a sum of 7 each. By not solving I think it's A since it's more likely to happen since your just going to roll it once rather then event B. Event A So I know that probability of getting a 6 in a dice is $\frac{1}{6}$ since it's two so it will be $\frac{1}{36}$ Event B sum of 7 each roll of two dice so $\frac{6}{36}$  or $\frac{1}{6}$ but it's twice thrown so $\frac{1}{36}$ WAIT it's EQUAL?! I think I'm wrong here somewhere.,A pair of dice is rolled repeatedly which event is more likely to occur first? Event $A$ both dice shows 6's. Event $B$ two consecutive rolls give a sum of 7 each. By not solving I think it's A since it's more likely to happen since your just going to roll it once rather then event B. Event A So I know that probability of getting a 6 in a dice is $\frac{1}{6}$ since it's two so it will be $\frac{1}{36}$ Event B sum of 7 each roll of two dice so $\frac{6}{36}$  or $\frac{1}{6}$ but it's twice thrown so $\frac{1}{36}$ WAIT it's EQUAL?! I think I'm wrong here somewhere.,,['probability']
81,Expected rolls to get 3 of any number,Expected rolls to get 3 of any number,,"Suppose I am rolling a die repeatedly, and I keep a tally of how many times each number has come up. As soon as a number has come up 3 times, the game is over. It does not need to be 3 times in a row - the tally just needs to reach 3. What is the expected number of rolls in a given game? From simulation, I get an answer of approximately 7.29, but I'm trying to figure out how to solve it exactly. I'm having trouble even beginning to frame this, so any help would be appreciated.","Suppose I am rolling a die repeatedly, and I keep a tally of how many times each number has come up. As soon as a number has come up 3 times, the game is over. It does not need to be 3 times in a row - the tally just needs to reach 3. What is the expected number of rolls in a given game? From simulation, I get an answer of approximately 7.29, but I'm trying to figure out how to solve it exactly. I'm having trouble even beginning to frame this, so any help would be appreciated.",,['probability']
82,Probability of rolling a sum of N with up to infinite rolls of a die,Probability of rolling a sum of N with up to infinite rolls of a die,,"I'm trying to figure out if there is some (relatively simple) formula for calculating the probability of rolling a sum of N with as many rolls as needed with a single regular six-sided die. For example: $N=1$ is $0.16666 = 1/6 = 1/6$ (1) $N=2$ is $0.19444 = 7/36 = 1/6 + 1/36$ (2 or 1,1) $N=3$ is $0.22685 = 49/216 = 1/6 + 2/36 + 1/216$ (3 or 1,2/2,1 or 1,1,1) $N=4$ is $0.26466 = 343/1296 = 1/6 + 3/36 + 3/216 + 1/1296$ (4 or 2,1/1,2/2,2 or 1,1,2/1,2,1/2,1,1 or 1,1,1,1) ... Does this series converge to essentially one and is there some good formula? It seems like pascal's triangle is involved somehow (at least for n=1-6, but I'm not sure how (if even possible) to convert it into a formula. Any help/advice is appreciated.","I'm trying to figure out if there is some (relatively simple) formula for calculating the probability of rolling a sum of N with as many rolls as needed with a single regular six-sided die. For example: $N=1$ is $0.16666 = 1/6 = 1/6$ (1) $N=2$ is $0.19444 = 7/36 = 1/6 + 1/36$ (2 or 1,1) $N=3$ is $0.22685 = 49/216 = 1/6 + 2/36 + 1/216$ (3 or 1,2/2,1 or 1,1,1) $N=4$ is $0.26466 = 343/1296 = 1/6 + 3/36 + 3/216 + 1/1296$ (4 or 2,1/1,2/2,2 or 1,1,2/1,2,1/2,1,1 or 1,1,1,1) ... Does this series converge to essentially one and is there some good formula? It seems like pascal's triangle is involved somehow (at least for n=1-6, but I'm not sure how (if even possible) to convert it into a formula. Any help/advice is appreciated.",,"['probability', 'dice']"
83,Intersection of Random Subsets,Intersection of Random Subsets,,"Consider $n$ independently drawn $q$-subsets $e_1,...,e_n$ from a finite set $P$. What is the probability that the intersection of the $n$ subsets is non-empty, in terms of $n$, $q$, and $|P|$? $$\mathbb{P}\left[\bigcap_{i=1}^n e_i \neq \emptyset \right] = \,\,?$$ EDIT: I was asked to give my thoughts on the problem. It comes up from a bit of research I am doing on edge colorings of graphs with no bichromatic cycles. I am not sure how to attack this type of problem. I should have stated: I am not necessarily looking for an answer. Any insight or suggestions on how to proceed would be welcome.","Consider $n$ independently drawn $q$-subsets $e_1,...,e_n$ from a finite set $P$. What is the probability that the intersection of the $n$ subsets is non-empty, in terms of $n$, $q$, and $|P|$? $$\mathbb{P}\left[\bigcap_{i=1}^n e_i \neq \emptyset \right] = \,\,?$$ EDIT: I was asked to give my thoughts on the problem. It comes up from a bit of research I am doing on edge colorings of graphs with no bichromatic cycles. I am not sure how to attack this type of problem. I should have stated: I am not necessarily looking for an answer. Any insight or suggestions on how to proceed would be welcome.",,"['probability', 'combinatorics', 'discrete-mathematics']"
84,Binomial Distribution: Finding the number of trials given probability and successess,Binomial Distribution: Finding the number of trials given probability and successess,,"Say we want to find the number of trials needed to be 90% sure that we will have at least two or more success, given the probability of a success is say, 50%. This question is easy when you want to find the number of trials for at least one success, but anything more than one and it gets complicated. My initial inclination was to find the number of trials needed to get at least one, then multiply it by two, however that isn't the correct answer. Is there anyway to do this by hand>?","Say we want to find the number of trials needed to be 90% sure that we will have at least two or more success, given the probability of a success is say, 50%. This question is easy when you want to find the number of trials for at least one success, but anything more than one and it gets complicated. My initial inclination was to find the number of trials needed to get at least one, then multiply it by two, however that isn't the correct answer. Is there anyway to do this by hand>?",,['probability']
85,Best strategy for rolling $20$-sided and $10$-sided dice,Best strategy for rolling -sided and -sided dice,20 10,"There is a $20$ -sided (face value of $1$ - $20$ ) die and a $10$ -sided (face value of $1$ - $10$ ) dice. $A$ and $B$ roll the $20$ and $10$ -sided dice, respectively. Both of them can roll their dice twice. They may choose to stop after the first roll or may continue to roll for the second time. They will compare the face value of the last rolls. If $A$ gets a bigger (and not equal) number, $A$ wins. Otherwise, $B$ wins. What's the best strategy for $A$ ?  What's $A$ 's winning probability? I know this problem can be solved using the indifference equations, which have been described in details in this paper by Ferguson and Ferguson. However, this approach is complicated and it’s easy to make mistakes for this specific problem. Are there any other more intuitive methods? Note: $A$ and $B$ roll simultaneouly. They don't know each other's number until the end when they compare them with one another.","There is a -sided (face value of - ) die and a -sided (face value of - ) dice. and roll the and -sided dice, respectively. Both of them can roll their dice twice. They may choose to stop after the first roll or may continue to roll for the second time. They will compare the face value of the last rolls. If gets a bigger (and not equal) number, wins. Otherwise, wins. What's the best strategy for ?  What's 's winning probability? I know this problem can be solved using the indifference equations, which have been described in details in this paper by Ferguson and Ferguson. However, this approach is complicated and it’s easy to make mistakes for this specific problem. Are there any other more intuitive methods? Note: and roll simultaneouly. They don't know each other's number until the end when they compare them with one another.",20 1 20 10 1 10 A B 20 10 A A B A A A B,"['probability', 'game-theory', 'dice']"
86,Flip 98 fair coins and 1 HH coin and 1 TT coin,Flip 98 fair coins and 1 HH coin and 1 TT coin,,"Flip $98$ fair coins and $1 \ HH$ coin and $1 \ TT$ coin. Given that you see an $H$ , what is the probability that it was the $HH$ coin. Applying Bayes Theorem, : $$P(HH|H) = \frac{P(H|HH) * P(HH)}{P(H)}$$ $P(H|HH) = 1$ $P(HH) = \frac{1}{100}$ $P(H) = \frac{100}{200} = \frac{1}{2}$ So I get $P(HH|H) = \frac{1}{50}$ 1) Is this the correct answer? 2) What's wrong with the 'intuitive' answer? I.e. You see $1\ H$ , so we only have $99$ possibilities remaining.  Of these $99$ , only $1$ of them is $HH => 1/99$","Flip fair coins and coin and coin. Given that you see an , what is the probability that it was the coin. Applying Bayes Theorem, : So I get 1) Is this the correct answer? 2) What's wrong with the 'intuitive' answer? I.e. You see , so we only have possibilities remaining.  Of these , only of them is",98 1 \ HH 1 \ TT H HH P(HH|H) = \frac{P(H|HH) * P(HH)}{P(H)} P(H|HH) = 1 P(HH) = \frac{1}{100} P(H) = \frac{100}{200} = \frac{1}{2} P(HH|H) = \frac{1}{50} 1\ H 99 99 1 HH => 1/99,"['probability', 'bayes-theorem']"
87,"the distribution of distance between two random points from $U(0,1)^3$",the distribution of distance between two random points from,"U(0,1)^3","Suppose $x_1$ and $x_2$ are two uniformly distributed points from unit cube $(0,1)^3$, what's the distribution of the distance between $x_1$ and $x_2$? I did a quick simulation and find that this distribution's kurtosis is 2.5, smaller than 3. can anyone help me with a closed form pdf? thanks.","Suppose $x_1$ and $x_2$ are two uniformly distributed points from unit cube $(0,1)^3$, what's the distribution of the distance between $x_1$ and $x_2$? I did a quick simulation and find that this distribution's kurtosis is 2.5, smaller than 3. can anyone help me with a closed form pdf? thanks.",,"['probability', 'probability-distributions']"
88,How can it be meaningful to add a discrete random variable to a continuous random variable while they are functions over different sample spaces?,How can it be meaningful to add a discrete random variable to a continuous random variable while they are functions over different sample spaces?,,"It seems that one usually use the set of all possible values of $X$ as the sample space of random variable $X$. (Therefore discrete random variables have countable sample space, continuous random variables have uncountable sample space.) However, I don't think this is right. Since random variables are defined as measurable functions over sample space, the above assumption would make sum/product of a discrete random variable and a continuous  random variable meaningless (because they are function over different spaces.) Well, this post says that Every uncountable standard Borel space is isomorphic to [0,1] with the Borel σ-algebra. Moreover, every non-atomic probability measure on a standard Borel space is equivalent to Lebesgue-measure on [0,1]. However, this doesn't solve all the problems. First, that claim is true only for uncountable sample space, so the problem of adding of a discrete RV and a continuous RV is still unsolved. Second, even with two continuous RV, there are still problems if we consider expectation (integration). Although the quoted claim says that ""every non-atomic probability measure on a standard Borel space is equivalent to Lebesgue-measure on [0,1]."" , however, two different measures cannot be transformed to Lebesgue measure on [0,1] with same isomorphism. Therefore two RVs $X$ and $Y$ may have different measures on [0,1], which will make $\mathbb{E}XY$ meaningless. This really confused me.... Can people really base all different random variables onto one same sample space with one same measure?","It seems that one usually use the set of all possible values of $X$ as the sample space of random variable $X$. (Therefore discrete random variables have countable sample space, continuous random variables have uncountable sample space.) However, I don't think this is right. Since random variables are defined as measurable functions over sample space, the above assumption would make sum/product of a discrete random variable and a continuous  random variable meaningless (because they are function over different spaces.) Well, this post says that Every uncountable standard Borel space is isomorphic to [0,1] with the Borel σ-algebra. Moreover, every non-atomic probability measure on a standard Borel space is equivalent to Lebesgue-measure on [0,1]. However, this doesn't solve all the problems. First, that claim is true only for uncountable sample space, so the problem of adding of a discrete RV and a continuous RV is still unsolved. Second, even with two continuous RV, there are still problems if we consider expectation (integration). Although the quoted claim says that ""every non-atomic probability measure on a standard Borel space is equivalent to Lebesgue-measure on [0,1]."" , however, two different measures cannot be transformed to Lebesgue measure on [0,1] with same isomorphism. Therefore two RVs $X$ and $Y$ may have different measures on [0,1], which will make $\mathbb{E}XY$ meaningless. This really confused me.... Can people really base all different random variables onto one same sample space with one same measure?",,"['probability', 'probability-theory']"
89,Application of Kolmogorov three series theorem in Homework Problem,Application of Kolmogorov three series theorem in Homework Problem,,"I have a homework problem which I encounter some difficulty in. I sincerely hope you all can assist me in attempting the following question: Given a sequence of i.i.d random variables $(X_n)_{n \in \mathbb{N}}$ with the condition that $E[|X_1|^{\alpha}]<\infty$ for $0<\alpha<1$. Here $E$ refers to the expectation. I am therefore tasked to apply the Kolmogorov's three series theorem to prove that ${{X_1+X_2+\cdots+X_n}\over {n^{{1} \over {\alpha}}}}\rightarrow0$ almost surely. I am being advised to apply Borel Cantelli Lemma to do some truncation to the summation in the question. However, I have no idea how to begin with. Kindly advise me on the details as I am new to modern Probability Theory. Thank you in advance for your suggestions.","I have a homework problem which I encounter some difficulty in. I sincerely hope you all can assist me in attempting the following question: Given a sequence of i.i.d random variables $(X_n)_{n \in \mathbb{N}}$ with the condition that $E[|X_1|^{\alpha}]<\infty$ for $0<\alpha<1$. Here $E$ refers to the expectation. I am therefore tasked to apply the Kolmogorov's three series theorem to prove that ${{X_1+X_2+\cdots+X_n}\over {n^{{1} \over {\alpha}}}}\rightarrow0$ almost surely. I am being advised to apply Borel Cantelli Lemma to do some truncation to the summation in the question. However, I have no idea how to begin with. Kindly advise me on the details as I am new to modern Probability Theory. Thank you in advance for your suggestions.",,['probability']
90,Is a Bernoulli process a Markov chain?,Is a Bernoulli process a Markov chain?,,"For a Bernoulli process, the outcome of a future trial is independent of the outcome of past trials. I.e., the future behaviour of a Bernoulli process is independent of its past, i.e. a Bernoulli process posesses the Markov property. So every Bernoulli trial is a Markov chain, right?","For a Bernoulli process, the outcome of a future trial is independent of the outcome of past trials. I.e., the future behaviour of a Bernoulli process is independent of its past, i.e. a Bernoulli process posesses the Markov property. So every Bernoulli trial is a Markov chain, right?",,"['probability', 'stochastic-processes', 'markov-chains']"
91,"German tank problem, simple derivation [duplicate]","German tank problem, simple derivation [duplicate]",,"This question already has answers here : Why does this expected value simplify as shown? (4 answers) Closed 10 years ago . I was reading the recent question on the German tank problem , and had trouble with one of the derivations in this section . $$\sum_{m=k}^N m \frac{\binom{m-1}{k-1}}{\binom N k} = \frac{k(N+1)}{k+1}$$ I understand where the left-hand side comes from, in the context of the problem. I'm just having trouble arriving at the right-hand side. \begin{align*} \sum_{m=k}^N m \frac{\binom{m-1}{k-1}}{\binom N k} &= \sum_{m=k}^N m \frac{(m-1)!}{(k-1)!(m-k)!} \frac{k! (N-k)!}{N!}\\ &= k \sum_{m=k}^N \frac{m!}{(m-k)!} \frac{(N-k)!}{N!} \end{align*} This looks promising, since I have found the $k$ that appears in the right-hand side, but I don't know what to do with the sum.","This question already has answers here : Why does this expected value simplify as shown? (4 answers) Closed 10 years ago . I was reading the recent question on the German tank problem , and had trouble with one of the derivations in this section . $$\sum_{m=k}^N m \frac{\binom{m-1}{k-1}}{\binom N k} = \frac{k(N+1)}{k+1}$$ I understand where the left-hand side comes from, in the context of the problem. I'm just having trouble arriving at the right-hand side. \begin{align*} \sum_{m=k}^N m \frac{\binom{m-1}{k-1}}{\binom N k} &= \sum_{m=k}^N m \frac{(m-1)!}{(k-1)!(m-k)!} \frac{k! (N-k)!}{N!}\\ &= k \sum_{m=k}^N \frac{m!}{(m-k)!} \frac{(N-k)!}{N!} \end{align*} This looks promising, since I have found the $k$ that appears in the right-hand side, but I don't know what to do with the sum.",,"['probability', 'binomial-coefficients', 'factorial']"
92,Poisson Distribution and Median,Poisson Distribution and Median,,"The number of automobile accidents at the corner of Wall and Street is assumed to have Poisson distribution with a mean of five per week.  Let A denote the number of automobile accidents that will occur next week.  Find (a) $\Pr[A<3]$ (b)  The median of A (c) $\sigma_A$ Since this is a Poisson distribution, the probability function is: $\Pr(A=k)=\cfrac{\lambda^k}{k!} e^{-\lambda}\tag{1}$ where $\lambda=5$ because the rate of occurrence is 5 accidents per week.  Therefore: (a) $\Pr[A<3]=e^{-5}(5^0 + 5 ^1 + 5^2/2!)=0.247$ (c) $\sigma_A=\sqrt{\lambda}=\sqrt{5}$ Although I wasn't sure how to calculate (b).  Equation (1) is valid for $k>0$ so how do you find the median for a probability distribution which can take an infinite number of k values? Thanks in advance.","The number of automobile accidents at the corner of Wall and Street is assumed to have Poisson distribution with a mean of five per week.  Let A denote the number of automobile accidents that will occur next week.  Find (a) (b)  The median of A (c) Since this is a Poisson distribution, the probability function is: where because the rate of occurrence is 5 accidents per week.  Therefore: (a) (c) Although I wasn't sure how to calculate (b).  Equation (1) is valid for so how do you find the median for a probability distribution which can take an infinite number of k values? Thanks in advance.",\Pr[A<3] \sigma_A \Pr(A=k)=\cfrac{\lambda^k}{k!} e^{-\lambda}\tag{1} \lambda=5 \Pr[A<3]=e^{-5}(5^0 + 5 ^1 + 5^2/2!)=0.247 \sigma_A=\sqrt{\lambda}=\sqrt{5} k>0,"['probability', 'statistics']"
93,Expected number of coin tosses before $k$ heads,Expected number of coin tosses before  heads,k,"If the probability of getting a head is $p$, how do you compute the expected number of coin tosses to get $k$ heads? I thought this might be the mean of the negative binomial distribution but this gives me $pk/(1-p)$ which is $k$ for $p=1/2$ which can't be right.","If the probability of getting a head is $p$, how do you compute the expected number of coin tosses to get $k$ heads? I thought this might be the mean of the negative binomial distribution but this gives me $pk/(1-p)$ which is $k$ for $p=1/2$ which can't be right.",,['probability']
94,Binomial probability with summation [duplicate],Binomial probability with summation [duplicate],,"This question already has answers here : Prove that $\sum_{k=0}^{m}\frac{\binom{m}{k}}{\binom{n}{k}}=\frac{n+1}{n+1-m}$ (4 answers) Closed 4 years ago . Show that $$\sum_{k=0}^{m} \frac{m!(n-k)!}{n!(m-k)!} = \frac{n+1}{n-m+1}$$ Attempt: It becomes: $$\sum_{k=0}^{m } \frac{\binom{m}{k}}{\binom{n}{k}}$$ Telescoping, pairing, binomial theorem don't seem to work Possibly a combinatoric proof? Considering the cards of numbers 1, 2, 3, 4, ... , m, .... , n on the table.  We must pick k cards from the n on the table. The probability that we pick these k cards from the first m is $$P(k) = \frac{\binom{m}{k}}{\binom{n}{k}}$$ So summing these probabilities will give us the LHS. Now is there a nice way of doing this probability a different way to yield the RHS?","This question already has answers here : Prove that $\sum_{k=0}^{m}\frac{\binom{m}{k}}{\binom{n}{k}}=\frac{n+1}{n+1-m}$ (4 answers) Closed 4 years ago . Show that $$\sum_{k=0}^{m} \frac{m!(n-k)!}{n!(m-k)!} = \frac{n+1}{n-m+1}$$ Attempt: It becomes: $$\sum_{k=0}^{m } \frac{\binom{m}{k}}{\binom{n}{k}}$$ Telescoping, pairing, binomial theorem don't seem to work Possibly a combinatoric proof? Considering the cards of numbers 1, 2, 3, 4, ... , m, .... , n on the table.  We must pick k cards from the n on the table. The probability that we pick these k cards from the first m is $$P(k) = \frac{\binom{m}{k}}{\binom{n}{k}}$$ So summing these probabilities will give us the LHS. Now is there a nice way of doing this probability a different way to yield the RHS?",,"['probability', 'combinatorics', 'binomial-coefficients', 'summation']"
95,Confusion about Banach Matchbox problem,Confusion about Banach Matchbox problem,,"While trying to solve Banach matchbox problem, I am getting a wrong answer. I dont understand what mistake I made. Please help me understand. The problem statement is presented below (Source: Here ) Suppose a mathematician carries two matchboxes at all times: one in his left pocket and one in his right. Each time he needs a match, he is equally likely to take it from either pocket. Suppose he reaches into his pocket and discovers that the box picked is empty. If it is assumed that each of the matchboxes originally contained $N$ matches, what is the probability that there are exactly $k$ matches in the other box? My solution goes like this. Lets say pocket $1$ becomes empty. Now, we want to find the probability that pocket $2$ contains $k$ matches (or $n-k$ matches have been removed from it. I also note that wikipedia solution does not consider the $1^{st}$ equality -- maybe thats where i am wrong?). Let $p = P[k\ \text{matches left in pocket}\ 2\ |\ \text{pocket 1 found empty}]$ = $\frac{P[k\ \text{matches left in pocket}\ 2\ \text{and pocket 1 found empty}]}{\sum_{i=0}^{n}P[i\ \text{matches left in pocket}\ 2\ \text{and pocket 1 found empty}]}$ = $\frac{\binom{2n-k}{n} \cdot \frac{1}{2^{2n-k}}} {\sum_{i=0}^{n}\binom{2n-i}{n} \cdot \frac{1}{2^{2n-i}}}$ In my $2^{nd}$ equality, I have written the probability of removing all matches from pocket $1$ and $n-k$ from pocket $2$ using Bernoulli trials with probability $\frac{1}{2}$. The denominator is a running sum over a similar quantity. Now, my answer to the original problem is $2p$ (the role of pockets could be switched). I am unable to see whats wrong with my approach. Please explain. Thanks","While trying to solve Banach matchbox problem, I am getting a wrong answer. I dont understand what mistake I made. Please help me understand. The problem statement is presented below (Source: Here ) Suppose a mathematician carries two matchboxes at all times: one in his left pocket and one in his right. Each time he needs a match, he is equally likely to take it from either pocket. Suppose he reaches into his pocket and discovers that the box picked is empty. If it is assumed that each of the matchboxes originally contained $N$ matches, what is the probability that there are exactly $k$ matches in the other box? My solution goes like this. Lets say pocket $1$ becomes empty. Now, we want to find the probability that pocket $2$ contains $k$ matches (or $n-k$ matches have been removed from it. I also note that wikipedia solution does not consider the $1^{st}$ equality -- maybe thats where i am wrong?). Let $p = P[k\ \text{matches left in pocket}\ 2\ |\ \text{pocket 1 found empty}]$ = $\frac{P[k\ \text{matches left in pocket}\ 2\ \text{and pocket 1 found empty}]}{\sum_{i=0}^{n}P[i\ \text{matches left in pocket}\ 2\ \text{and pocket 1 found empty}]}$ = $\frac{\binom{2n-k}{n} \cdot \frac{1}{2^{2n-k}}} {\sum_{i=0}^{n}\binom{2n-i}{n} \cdot \frac{1}{2^{2n-i}}}$ In my $2^{nd}$ equality, I have written the probability of removing all matches from pocket $1$ and $n-k$ from pocket $2$ using Bernoulli trials with probability $\frac{1}{2}$. The denominator is a running sum over a similar quantity. Now, my answer to the original problem is $2p$ (the role of pockets could be switched). I am unable to see whats wrong with my approach. Please explain. Thanks",,"['probability', 'probability-theory', 'probability-distributions', 'intuition']"
96,The prime numbers do not satisfies Benford's law,The prime numbers do not satisfies Benford's law,,"A set of numbers is said to satisfy Benford's law if the leading digit d (d ∈ {1, ..., 9}) occurs with probability, $$ P(d)=\log_{10}(d+1)-\log_{10}d,$$ how do you prove that the prime numbers do not satisfies Benford's law?","A set of numbers is said to satisfy Benford's law if the leading digit d (d ∈ {1, ..., 9}) occurs with probability, $$ P(d)=\log_{10}(d+1)-\log_{10}d,$$ how do you prove that the prime numbers do not satisfies Benford's law?",,['probability']
97,Probability that distance between $X$ and $Y$ is $>$ $L/3$,Probability that distance between  and  is,X Y > L/3,"Two points are selected randomly on a line of length $L$, so as to be on opposite sides of the midpoint of the line[In other words, the two points $X$ and $Y$ are independent random variables with a uniform distribution over $(0, L/2)$ and $(L/2, L)$ respectively. Find the probability that the distance between the two points is greater than $L/3$. Attempt: We have $f_X(x) = f_Y(y) = 2/L => f(x,y) = 4/L^2 $ The probability I want is $P(Y-X > L/3) = P(Y> L/3 + X)$. So I need to evaluate: $$ \iint_{(x,y): y > L/3 + x} f(x,y)\,dy\,dx.$$ I said the limits on $y$ were from $L/3 + x$ to $L$ and $x$ from $0$ to $L/2$, so I evaluate $$\int_0^{L/2} \int_{L/3 +x}^{L} \frac{4}{L^2}\,dy\,dx$$ which gives $5/6$, but that is incorrect. Where did I go wrong?","Two points are selected randomly on a line of length $L$, so as to be on opposite sides of the midpoint of the line[In other words, the two points $X$ and $Y$ are independent random variables with a uniform distribution over $(0, L/2)$ and $(L/2, L)$ respectively. Find the probability that the distance between the two points is greater than $L/3$. Attempt: We have $f_X(x) = f_Y(y) = 2/L => f(x,y) = 4/L^2 $ The probability I want is $P(Y-X > L/3) = P(Y> L/3 + X)$. So I need to evaluate: $$ \iint_{(x,y): y > L/3 + x} f(x,y)\,dy\,dx.$$ I said the limits on $y$ were from $L/3 + x$ to $L$ and $x$ from $0$ to $L/2$, so I evaluate $$\int_0^{L/2} \int_{L/3 +x}^{L} \frac{4}{L^2}\,dy\,dx$$ which gives $5/6$, but that is incorrect. Where did I go wrong?",,['probability']
98,Classic $2n$ people around a table problem,Classic  people around a table problem,2n,"A total of $2n$ people, consisting of $n$ married couples, are randomly seated (all possible orderings being equally likely) at a round table. Let $C_i$ denote the event that the members of couple $i$ are seated next to each other, $i = 1,...,n$ a) Find $P(C_i)$ Attempt: So this is the prob that one particular couple sit next to each other. There are $2n$ seats for the first person. Given that the table is round, there are 2 seats for the wife/husband, (either to the left or right) and $(2n-2)!$ rearrangements of the remaining people.  Since each of the orderings are equally likely, $|s| = (2n)!$ and the prob we want is $(2n) \cdot 2 \cdot (2n-2)! / (2n)! = 2/(2n-1) $ Is it a good argument? b) For $j \neq i,\, \text{find} \, P(C_i|C_j)$ Attempt: This is the prob that given some couple $j$ already sitting next to one another, what is the probability that some other couple $i$ are sitting next to each other. By definition, this is equal to $P(C_i, C_j)/P(C_j)$. I already computed the denominator in a), so I need only worry about the numerator.  For the numerator: If couple $i$ and $j$ are to sit next to each other, there are $(2n)$ places for the first person and $2$ choices for the next person. For the other couple, I am not really sure what to say since if one member of couple $j$ sits next to a member of couple $i$, then there is only one place for the other member of couple $j$. But, if the couple $j$ do not sit any where near couple $i$ then there is more than one place for the other member. It seems reasonable to compute therefore, $$P(C_i,C_j) = P(C_i,C_j| \text{one member of j next to i})P(\text{one member of j next to i}) + P(C_i,C_j|\text{member of j not next to i})P(\text{member of j not next to i})$$ Does this make sense and is my approach good or not? c) Approximate the probability, for $n$ large, that there are no married couples who are seated next to each other. Attempt: $P(\text{no married couples next to each other}) = 1-P(\text{at least one couple sit next to each other})$I know the approximation will be Poisson since n is large, but I am not sure where to go from here. Thanks!","A total of $2n$ people, consisting of $n$ married couples, are randomly seated (all possible orderings being equally likely) at a round table. Let $C_i$ denote the event that the members of couple $i$ are seated next to each other, $i = 1,...,n$ a) Find $P(C_i)$ Attempt: So this is the prob that one particular couple sit next to each other. There are $2n$ seats for the first person. Given that the table is round, there are 2 seats for the wife/husband, (either to the left or right) and $(2n-2)!$ rearrangements of the remaining people.  Since each of the orderings are equally likely, $|s| = (2n)!$ and the prob we want is $(2n) \cdot 2 \cdot (2n-2)! / (2n)! = 2/(2n-1) $ Is it a good argument? b) For $j \neq i,\, \text{find} \, P(C_i|C_j)$ Attempt: This is the prob that given some couple $j$ already sitting next to one another, what is the probability that some other couple $i$ are sitting next to each other. By definition, this is equal to $P(C_i, C_j)/P(C_j)$. I already computed the denominator in a), so I need only worry about the numerator.  For the numerator: If couple $i$ and $j$ are to sit next to each other, there are $(2n)$ places for the first person and $2$ choices for the next person. For the other couple, I am not really sure what to say since if one member of couple $j$ sits next to a member of couple $i$, then there is only one place for the other member of couple $j$. But, if the couple $j$ do not sit any where near couple $i$ then there is more than one place for the other member. It seems reasonable to compute therefore, $$P(C_i,C_j) = P(C_i,C_j| \text{one member of j next to i})P(\text{one member of j next to i}) + P(C_i,C_j|\text{member of j not next to i})P(\text{member of j not next to i})$$ Does this make sense and is my approach good or not? c) Approximate the probability, for $n$ large, that there are no married couples who are seated next to each other. Attempt: $P(\text{no married couples next to each other}) = 1-P(\text{at least one couple sit next to each other})$I know the approximation will be Poisson since n is large, but I am not sure where to go from here. Thanks!",,['probability']
99,Convergence in distribution (weak convergence),Convergence in distribution (weak convergence),,"Let $X_n$ and $X$ be random variables taking values in the metric space $(S,d)$. The sequence $(X_n)_n$ is convergent to $X$ in distribution (or weakly) if $E[f(X_n)] \to E[f(X)]$ for all $f:S\to R$ continuous and bounded. I read somewhere that it's equivalent to consider only uniformly continuous and bounded $f$. Could you give me a proof of this fact?","Let $X_n$ and $X$ be random variables taking values in the metric space $(S,d)$. The sequence $(X_n)_n$ is convergent to $X$ in distribution (or weakly) if $E[f(X_n)] \to E[f(X)]$ for all $f:S\to R$ continuous and bounded. I read somewhere that it's equivalent to consider only uniformly continuous and bounded $f$. Could you give me a proof of this fact?",,"['probability', 'functional-analysis', 'probability-theory', 'convergence-divergence']"
