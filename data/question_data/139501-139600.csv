,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Factorize the differential operator $[D^2-(x^2+1)]$ into two first order linear operators,Factorize the differential operator  into two first order linear operators,[D^2-(x^2+1)],I am Trying to factorize the differential operator $[D^2-(x^2+1)]$ into two first order linear operators. But I have not been able to. I am trying to split it into $[D+A][D-B]f=[D^2-(x^2+1)]f$. But at the end I get a really ugly differential equation that I cannot solve (It should be an easy one). Is there any hint? Thanks,I am Trying to factorize the differential operator $[D^2-(x^2+1)]$ into two first order linear operators. But I have not been able to. I am trying to split it into $[D+A][D-B]f=[D^2-(x^2+1)]f$. But at the end I get a really ugly differential equation that I cannot solve (It should be an easy one). Is there any hint? Thanks,,['ordinary-differential-equations']
1,Find the solution of the given initial value problem $y^{(4)}-4y'''+4y''=0$,Find the solution of the given initial value problem,y^{(4)}-4y'''+4y''=0,"I can't really figure this out but this is what I've done some far: $y^{(4)}-4y'''+4y''=0$ where $y(1)=9+e^2$ $y'(1)=7+2e^3$ $y''(1)=4e^2$ and $y'''(1)=8e^2$ I found the characteristic equation $r^4-4r^3+4r^2=0$ and it factored to $r^2(r-2)^2=0$ So $y=c_1+c_2+c_3e^{2t}+c_4te^{2t}$ and then found the derivatives and plugged in the initial values to get $9+e^2=c_1+c_2+c_3e^2+c_4e^2$ $7+2e^2=2c_3e^2+3c_4e^2$ $4e^2=4c_3e^2+8c_4e^2$ $8e^2=8c_3e^2+20c_4e^2$ But when I solve, I get $c_3=1$ and $c_4=0$ but thats not right. What am I doing wrong?","I can't really figure this out but this is what I've done some far: $y^{(4)}-4y'''+4y''=0$ where $y(1)=9+e^2$ $y'(1)=7+2e^3$ $y''(1)=4e^2$ and $y'''(1)=8e^2$ I found the characteristic equation $r^4-4r^3+4r^2=0$ and it factored to $r^2(r-2)^2=0$ So $y=c_1+c_2+c_3e^{2t}+c_4te^{2t}$ and then found the derivatives and plugged in the initial values to get $9+e^2=c_1+c_2+c_3e^2+c_4e^2$ $7+2e^2=2c_3e^2+3c_4e^2$ $4e^2=4c_3e^2+8c_4e^2$ $8e^2=8c_3e^2+20c_4e^2$ But when I solve, I get $c_3=1$ and $c_4=0$ but thats not right. What am I doing wrong?",,['ordinary-differential-equations']
2,Determine the eigenvalues (and corresponding eigenfunctions) if phi satisfies...,Determine the eigenvalues (and corresponding eigenfunctions) if phi satisfies...,,I am stuck on how to approach this question: Solution: Any help is appreciated. Leo,I am stuck on how to approach this question: Solution: Any help is appreciated. Leo,,['ordinary-differential-equations']
3,meaning of $X'=AX$,meaning of,X'=AX,i have been trying to learn differential equations and I saw various types and methods to solve those types of equations. My question is that $X'=AX$ represents linear DE but I am just seeing linear transformation when I am seeing $AX$. What does this represent ?,i have been trying to learn differential equations and I saw various types and methods to solve those types of equations. My question is that $X'=AX$ represents linear DE but I am just seeing linear transformation when I am seeing $AX$. What does this represent ?,,"['linear-algebra', 'ordinary-differential-equations']"
4,Solving Differential equation with laplace transformation,Solving Differential equation with laplace transformation,,"Solve the differential equation with the laplace transformation.   $$y''-4y'+9y=9\quad,\quad  y(0)=0 \quad ,\quad y'(0)=-8$$ I will solve this question to this state, but I cannot continue. \begin{align} s^2 y(s)-4sy(s)+9y(s)=\frac{9}{s}  \end{align} \begin{align} y(s)&=\frac{9}{s}(\frac{1}{s^2}-4s+9) \\ &=\frac{1}{s}-\frac{s-4}{(s-2)^2}+5 \\ &=\frac{1}{s}-\frac{s-2}{(s-2)^2}-5 + \frac{2}{(s-2)^2}+5 \end{align} I did this so far, but I cannot transform last term.","Solve the differential equation with the laplace transformation.   $$y''-4y'+9y=9\quad,\quad  y(0)=0 \quad ,\quad y'(0)=-8$$ I will solve this question to this state, but I cannot continue. \begin{align} s^2 y(s)-4sy(s)+9y(s)=\frac{9}{s}  \end{align} \begin{align} y(s)&=\frac{9}{s}(\frac{1}{s^2}-4s+9) \\ &=\frac{1}{s}-\frac{s-4}{(s-2)^2}+5 \\ &=\frac{1}{s}-\frac{s-2}{(s-2)^2}-5 + \frac{2}{(s-2)^2}+5 \end{align} I did this so far, but I cannot transform last term.",,"['ordinary-differential-equations', 'laplace-transform']"
5,Use Method of Undetermined Coefficients to find general solution,Use Method of Undetermined Coefficients to find general solution,,The problem is $u''-w_{o}^{2}u=\cos(w_{o}t)$. The only thing I am having trouble with is what to have $u(t)$ to be. I tried a linear combination of sin and cos but that didn't work so I'm having a hard time especially since you have to have this to find the particular solution.,The problem is $u''-w_{o}^{2}u=\cos(w_{o}t)$. The only thing I am having trouble with is what to have $u(t)$ to be. I tried a linear combination of sin and cos but that didn't work so I'm having a hard time especially since you have to have this to find the particular solution.,,['ordinary-differential-equations']
6,Orthogonal trajectories for a given family of curves,Orthogonal trajectories for a given family of curves,,"Find the orthogonal trajectories for the given family of curves: $$y^2=Cx^3-2$$ Derivative with respect to x, and finding the value of C: $$2yy' = C3x^2 $$ $$C=\frac{y^2+2}{x^3}$$ Replacing C, and solving for y': $$2yy' = \frac{3y^2+6}{x}$$ $$y'=\frac{3y^2+6}{2xy}$$ Finding the orthogonal and separating terms: $$y' = -\frac{2xy}{3y^2+6}$$ $$\frac{(3y^2+6)}{y}y'=-2x$$ $$\frac{(3y^2+6)}{y}dy=-2xdx$$ Integrating: $$\int{\frac{(3y^2+6)}{y}dy}=\int{-2xdx}$$ $$\frac{3y^2}{2}+6log(y)=-x^2+C$$ $$C=\frac{3y^2}{2}+6log(y)+x^2$$ Was I close? We weren't given solutions, so I'd appreciate a check on my answer. :)","Find the orthogonal trajectories for the given family of curves: $$y^2=Cx^3-2$$ Derivative with respect to x, and finding the value of C: $$2yy' = C3x^2 $$ $$C=\frac{y^2+2}{x^3}$$ Replacing C, and solving for y': $$2yy' = \frac{3y^2+6}{x}$$ $$y'=\frac{3y^2+6}{2xy}$$ Finding the orthogonal and separating terms: $$y' = -\frac{2xy}{3y^2+6}$$ $$\frac{(3y^2+6)}{y}y'=-2x$$ $$\frac{(3y^2+6)}{y}dy=-2xdx$$ Integrating: $$\int{\frac{(3y^2+6)}{y}dy}=\int{-2xdx}$$ $$\frac{3y^2}{2}+6log(y)=-x^2+C$$ $$C=\frac{3y^2}{2}+6log(y)+x^2$$ Was I close? We weren't given solutions, so I'd appreciate a check on my answer. :)",,"['calculus', 'ordinary-differential-equations']"
7,Legendre's equation polynomial solution,Legendre's equation polynomial solution,,"This is a problem on analytic solutions of ordinarry differential equations. Any help will be greatly appreciated. Please, try to be as specific as possible as I don't handle this material very well yet. $$\displaystyle \frac{d}{dz} \left ((1-z^2) \frac{dw}{dz} \right ) + \lambda w = 0$$ What would be a condition of $\lambda$ such that the equation has a polynomial solution?","This is a problem on analytic solutions of ordinarry differential equations. Any help will be greatly appreciated. Please, try to be as specific as possible as I don't handle this material very well yet. $$\displaystyle \frac{d}{dz} \left ((1-z^2) \frac{dw}{dz} \right ) + \lambda w = 0$$ What would be a condition of $\lambda$ such that the equation has a polynomial solution?",,['analysis']
8,Rearranging a logarithmic equation,Rearranging a logarithmic equation,,I'm building a web app that displays the frequency of a sound. I have an equation that returns a pixel that a particular frequency should be mapped at. However I would like to reverse the equation so that it returns the frequency that a specific pixel relates to. I haven't really worked with logarithms much before and so I am having difficulty with it. The pixel to frequency equation is: $$    p = \frac{\frac{\log f}{\log 10} - \min }{r\cdot w}. $$ Any help would be greatly appreciated.,I'm building a web app that displays the frequency of a sound. I have an equation that returns a pixel that a particular frequency should be mapped at. However I would like to reverse the equation so that it returns the frequency that a specific pixel relates to. I haven't really worked with logarithms much before and so I am having difficulty with it. The pixel to frequency equation is: $$    p = \frac{\frac{\log f}{\log 10} - \min }{r\cdot w}. $$ Any help would be greatly appreciated.,,"['ordinary-differential-equations', 'logarithms']"
9,absolute stability / inequality,absolute stability / inequality,,"i want to find the amountof $\theta \in[0,1]$ where it is absolute stable whith $y'=\lambda y$ ,$\lambda \in \mathbb C$ or $\lambda \in \mathbb R$ for $$u_{j+1}=u_j+h[\theta f(t_j,u_j)+(1-\theta )f(t_{j+1},u_{j+1})]$$ i got $$u_{j+1}=u_j+h[\theta \lambda u_j+(1-\theta)\lambda u_{j+1}]$$ $$u_{j+1}=u_j(1+h\theta \lambda)+u_{j+1}h(\lambda -\theta \lambda)$$ $$u_{j+1}=u_j \dfrac{(1+h\theta \lambda)}{(1+h\lambda(\theta -1)}$$right? for the absolute stability it needs to be $$\dfrac{|1+h\theta \lambda|}{|1+h\lambda (\theta -1)|}<1$$ $$|1+h\theta \lambda|<|1+h\lambda (\theta -1)|$$ so how can i calculate the inequality for the right $\theta$ and it must be $Re(\lambda)<0$. I think $\theta \in [0,1/2)$ is right, but cant proof it","i want to find the amountof $\theta \in[0,1]$ where it is absolute stable whith $y'=\lambda y$ ,$\lambda \in \mathbb C$ or $\lambda \in \mathbb R$ for $$u_{j+1}=u_j+h[\theta f(t_j,u_j)+(1-\theta )f(t_{j+1},u_{j+1})]$$ i got $$u_{j+1}=u_j+h[\theta \lambda u_j+(1-\theta)\lambda u_{j+1}]$$ $$u_{j+1}=u_j(1+h\theta \lambda)+u_{j+1}h(\lambda -\theta \lambda)$$ $$u_{j+1}=u_j \dfrac{(1+h\theta \lambda)}{(1+h\lambda(\theta -1)}$$right? for the absolute stability it needs to be $$\dfrac{|1+h\theta \lambda|}{|1+h\lambda (\theta -1)|}<1$$ $$|1+h\theta \lambda|<|1+h\lambda (\theta -1)|$$ so how can i calculate the inequality for the right $\theta$ and it must be $Re(\lambda)<0$. I think $\theta \in [0,1/2)$ is right, but cant proof it",,"['ordinary-differential-equations', 'numerical-methods']"
10,What is the use of differentiation?,What is the use of differentiation?,,I tried to know from lot of books. But they always give only the formulas for differentiation. I want to know why do we differentiate the expressions?,I tried to know from lot of books. But they always give only the formulas for differentiation. I want to know why do we differentiate the expressions?,,['ordinary-differential-equations']
11,second order differential equation possible solution,second order differential equation possible solution,,I have a second order differential equation: $$y''=-a^2y(t)$$ I search some possible solution such as: $$y(t)=\sin(at)$$ or $$y(t)=\cos(at)$$ Is $y(t)= 0 $  also a possible solution or not?,I have a second order differential equation: $$y''=-a^2y(t)$$ I search some possible solution such as: $$y(t)=\sin(at)$$ or $$y(t)=\cos(at)$$ Is $y(t)= 0 $  also a possible solution or not?,,['ordinary-differential-equations']
12,Cannot understand this part from the textbook,Cannot understand this part from the textbook,,"I'm reading the book Ordinary Differential Equations and Dynamical Systems by Gerald Teschl. Or rather, I am reading the online edition: http://www.mat.univie.ac.at/~gerald/ftp/book-ode/index.html On page 15 it says: More generally, consider the differential equation $$ \dot{x} =  f\left(\frac{ax + bt + c}{\alpha x + \beta t + \gamma}\right). $$ Two   cases can occur. If $a \beta - \alpha b = 0$, our differential   equation is of the form $$\dot{x} = \widetilde{f}(ax +bt) $$ I understand the rest of the page. But this step makes no sense to me. Would someone care to explain to me what the author might mean? Thanks in advance","I'm reading the book Ordinary Differential Equations and Dynamical Systems by Gerald Teschl. Or rather, I am reading the online edition: http://www.mat.univie.ac.at/~gerald/ftp/book-ode/index.html On page 15 it says: More generally, consider the differential equation $$ \dot{x} =  f\left(\frac{ax + bt + c}{\alpha x + \beta t + \gamma}\right). $$ Two   cases can occur. If $a \beta - \alpha b = 0$, our differential   equation is of the form $$\dot{x} = \widetilde{f}(ax +bt) $$ I understand the rest of the page. But this step makes no sense to me. Would someone care to explain to me what the author might mean? Thanks in advance",,"['ordinary-differential-equations', 'dynamical-systems']"
13,The solution of the differential equation $\frac{\mathrm{d}y}{\mathrm{d}x}=2xy^2$,The solution of the differential equation,\frac{\mathrm{d}y}{\mathrm{d}x}=2xy^2,"Question from pg 32 of Barron's AP Calculus The solution of the differential equation $\frac{\mathrm{d}y}{\mathrm{d}x}=2xy^2$ for which $y = -1$ when $x = 1$ is (A) $y = -\frac{1}{x^2}$ for $x \neq 0$ (B) $y = -\frac{1}{x^2}$ for $x > 0$ I think A is a possible solution; however the answer given is This function is discontinuous at $x=0$ . Since the particular solution much be differentiable in an interval containing the inital value $x = 1$ , the domain is $x > 0$ . Giving (B) as the correct answer. Even from the answer given in the book, should not (A) be an acceptiable answer as well because its domain contains the domain of (B)?","Question from pg 32 of Barron's AP Calculus The solution of the differential equation for which when is (A) for (B) for I think A is a possible solution; however the answer given is This function is discontinuous at . Since the particular solution much be differentiable in an interval containing the inital value , the domain is . Giving (B) as the correct answer. Even from the answer given in the book, should not (A) be an acceptiable answer as well because its domain contains the domain of (B)?",\frac{\mathrm{d}y}{\mathrm{d}x}=2xy^2 y = -1 x = 1 y = -\frac{1}{x^2} x \neq 0 y = -\frac{1}{x^2} x > 0 x=0 x = 1 x > 0,"['calculus', 'ordinary-differential-equations']"
14,Solving Linear ODE,Solving Linear ODE,,"Can someone help me prove that given $$    y'' + \frac{2}{x}y' + \lambda y = 0 $$ and boundary conditions $$   {\lim_{x \to 0} xy = 0 , \hspace{5mm} y(1) =0 } $$ that $$     y(x) = \frac{1}{x}\left[ A \sin{(\sqrt{\lambda}x)} + B \cos{(\sqrt\lambda x)} \right] $$ When trying to solve it the usual way I get $$ m^2 + \frac{2}{x}m + \lambda = 0 $$ $$ m = -\frac{1}{x} \pm j\sqrt{\lambda - \frac{1}{x^2}}$$ which obviously won't me to the solution because I have the $\sqrt{\lambda - \frac{1}{x^2}}$ term instead of a $\sqrt{\lambda x}$ term.","Can someone help me prove that given $$    y'' + \frac{2}{x}y' + \lambda y = 0 $$ and boundary conditions $$   {\lim_{x \to 0} xy = 0 , \hspace{5mm} y(1) =0 } $$ that $$     y(x) = \frac{1}{x}\left[ A \sin{(\sqrt{\lambda}x)} + B \cos{(\sqrt\lambda x)} \right] $$ When trying to solve it the usual way I get $$ m^2 + \frac{2}{x}m + \lambda = 0 $$ $$ m = -\frac{1}{x} \pm j\sqrt{\lambda - \frac{1}{x^2}}$$ which obviously won't me to the solution because I have the $\sqrt{\lambda - \frac{1}{x^2}}$ term instead of a $\sqrt{\lambda x}$ term.",,['ordinary-differential-equations']
15,how to solve this nonlinear equation system by changing this system to linear system,how to solve this nonlinear equation system by changing this system to linear system,,consider following nonlinear equation system how solve it? $$x'=|y|$$ $$y'=x$$ and whats the matrix that associated to this system,consider following nonlinear equation system how solve it? $$x'=|y|$$ $$y'=x$$ and whats the matrix that associated to this system,,"['ordinary-differential-equations', 'dynamical-systems']"
16,1st-order linear ODE with tridiagonal matrix. Efficient solutions?,1st-order linear ODE with tridiagonal matrix. Efficient solutions?,,"I have a 1st-rder linear ODE system where the system is characterized by $A$. Given an initial state $x_0$, I want the state at some later time $t$, efficiently. $A$ happens to be a symmetric tridiagonal matrix where the coefficients are the same in each diagonal line. $$ A = \begin{pmatrix}  a & b & 0 & 0 & ... \\ b & a & b & 0 & ... \\ 0 & b & a & b & ... \\ 0 & 0 & b & a & ... \\ \vdots &&\ddots&\ddots&\ddots\end{pmatrix} $$ I need to solve this for many different $A$'s and also many different $x_0$'s. The general solution to that is of course $$x_t = e^{A t}x_0$$ One way to do this is to eigen-decompose $A$ into $A = F D F^\top$. Then the solution becomes $$x_t = (F e^{D t} F^\top) x_0$$ Which is a little better because now the multiplication with $x_0$ is $O(n^2)$, but the eigen-decomposition is $O(n^3)$ still. But this doesn't take advantage of the constant values of $a$ and $b$ or the symmetric tridiagonal nature of $A$. So it seems like I should be able to do better.","I have a 1st-rder linear ODE system where the system is characterized by $A$. Given an initial state $x_0$, I want the state at some later time $t$, efficiently. $A$ happens to be a symmetric tridiagonal matrix where the coefficients are the same in each diagonal line. $$ A = \begin{pmatrix}  a & b & 0 & 0 & ... \\ b & a & b & 0 & ... \\ 0 & b & a & b & ... \\ 0 & 0 & b & a & ... \\ \vdots &&\ddots&\ddots&\ddots\end{pmatrix} $$ I need to solve this for many different $A$'s and also many different $x_0$'s. The general solution to that is of course $$x_t = e^{A t}x_0$$ One way to do this is to eigen-decompose $A$ into $A = F D F^\top$. Then the solution becomes $$x_t = (F e^{D t} F^\top) x_0$$ Which is a little better because now the multiplication with $x_0$ is $O(n^2)$, but the eigen-decomposition is $O(n^3)$ still. But this doesn't take advantage of the constant values of $a$ and $b$ or the symmetric tridiagonal nature of $A$. So it seems like I should be able to do better.",,"['linear-algebra', 'ordinary-differential-equations', 'numerical-linear-algebra']"
17,Solving a system of differential equations,Solving a system of differential equations,,"I would like to get some help by solving the following problem: $$  p'_1= \frac 1 x p_1 - p_2 + x$$ $$ p'_2=\frac 1{x^2}p_1+\frac 2 x p_2 - x^2 $$ with initial conditions $$p_1(1)=p_2(1)=0, x \gt0 $$ EDIT: If I use Wolframalpha, I get Where $u$ and $v$ are obviously $p_1$ and $p_2$. Can anybody explain whats going on?","I would like to get some help by solving the following problem: $$  p'_1= \frac 1 x p_1 - p_2 + x$$ $$ p'_2=\frac 1{x^2}p_1+\frac 2 x p_2 - x^2 $$ with initial conditions $$p_1(1)=p_2(1)=0, x \gt0 $$ EDIT: If I use Wolframalpha, I get Where $u$ and $v$ are obviously $p_1$ and $p_2$. Can anybody explain whats going on?",,['ordinary-differential-equations']
18,estimate of solution of initial value problem,estimate of solution of initial value problem,,Let $ \phi : \mathbb R \to \mathbb R $ a continuous function. Prove that the initial value problem $$ y' =y + \phi (x) e^{ -y^{2}} ; \quad y(0)=1$$ has a unique solution on the real line. (ii) If $ y$ is the unique solution prove that $$ | y(x) -e^{x} | \leq ( e^x -1) \max_{ 0 \leq t \leq x} | \phi (t) | \quad \forall x>0 .$$ I have prove that there is a unique solution but I can't prove the inequality in (ii). Any help? Thanks in advance!,Let $ \phi : \mathbb R \to \mathbb R $ a continuous function. Prove that the initial value problem $$ y' =y + \phi (x) e^{ -y^{2}} ; \quad y(0)=1$$ has a unique solution on the real line. (ii) If $ y$ is the unique solution prove that $$ | y(x) -e^{x} | \leq ( e^x -1) \max_{ 0 \leq t \leq x} | \phi (t) | \quad \forall x>0 .$$ I have prove that there is a unique solution but I can't prove the inequality in (ii). Any help? Thanks in advance!,,['ordinary-differential-equations']
19,Physical experiments for an introductory ordinary differential equations course,Physical experiments for an introductory ordinary differential equations course,,I am looking for simple experiments that students can perform as projects for an ODE course. For example experiments related to Newton's law of cooling or dynamics of a pendulum. What other topics would you suggest? Do you know of a text with such a perspective?,I am looking for simple experiments that students can perform as projects for an ODE course. For example experiments related to Newton's law of cooling or dynamics of a pendulum. What other topics would you suggest? Do you know of a text with such a perspective?,,['ordinary-differential-equations']
20,Why does $y'' + y = 0$ contain essential singularity at $z =\infty$?,Why does  contain essential singularity at ?,y'' + y = 0 z =\infty,"$y'' + y = 0$ has sine and cosine solutions. It has no regular singularities, although it has an essential singularity at $z \to \infty$ because if you let  $$w = \frac{1}{z}$$  $$\frac{dy}{dz} = \frac{dy}{dw}\frac{dw}{dz} = -w^2\frac{dy}{dw}$$ $$\frac{d^2y}{dz^2} = -\frac{d}{dw} \left(w^2\frac{dy}{dw}\right) \frac{dw}{dz} = w^3\left(2\frac{dy}{dw} + w\frac{d^2y}{dw^2}\right)$$ Now I evaluate the behavior $w \to 0$; $z \to \infty$. Plugging these new differentials into the previous differential equation I get $$w^4\frac{d^2y}{dw^2} + 2w^3\frac{dy}{dw} + y = 0$$ $$\frac{d^2y}{dw^2} + \frac{2}{w}\frac{dy}{dw} + \frac{1}{w^4}y = 0$$ This contains an essential singularity at $z =\infty$. I'm not quite sure I understand why. Is it because sinusoids are not analytic at infinity? They contain no poles do they? Edit: I don't need a formal explanation, maybe something intuitive will be more helpful.","$y'' + y = 0$ has sine and cosine solutions. It has no regular singularities, although it has an essential singularity at $z \to \infty$ because if you let  $$w = \frac{1}{z}$$  $$\frac{dy}{dz} = \frac{dy}{dw}\frac{dw}{dz} = -w^2\frac{dy}{dw}$$ $$\frac{d^2y}{dz^2} = -\frac{d}{dw} \left(w^2\frac{dy}{dw}\right) \frac{dw}{dz} = w^3\left(2\frac{dy}{dw} + w\frac{d^2y}{dw^2}\right)$$ Now I evaluate the behavior $w \to 0$; $z \to \infty$. Plugging these new differentials into the previous differential equation I get $$w^4\frac{d^2y}{dw^2} + 2w^3\frac{dy}{dw} + y = 0$$ $$\frac{d^2y}{dw^2} + \frac{2}{w}\frac{dy}{dw} + \frac{1}{w^4}y = 0$$ This contains an essential singularity at $z =\infty$. I'm not quite sure I understand why. Is it because sinusoids are not analytic at infinity? They contain no poles do they? Edit: I don't need a formal explanation, maybe something intuitive will be more helpful.",,['ordinary-differential-equations']
21,How to prove existence of periodic solutions of ordinary differential equations?,How to prove existence of periodic solutions of ordinary differential equations?,,"I have a non linear first order ordinary differential equation with periodic coefficients. I am trying to prove that the periodic solution of the differential equation exists. I am giving you an example of the problem I am having: $$\large\frac{dx}{dt} = \mu - d\cdot x$$ where I assume that $\mu$ and $d$ are periodic in time and have the same period. Now, I have to prove that the solution of the differential equation i.e., $x$ is also periodic in time with the same period. Is there any particular method I should apply? I need help badly. Your help/suggestion will be greatly appreciated.","I have a non linear first order ordinary differential equation with periodic coefficients. I am trying to prove that the periodic solution of the differential equation exists. I am giving you an example of the problem I am having: $$\large\frac{dx}{dt} = \mu - d\cdot x$$ where I assume that $\mu$ and $d$ are periodic in time and have the same period. Now, I have to prove that the solution of the differential equation i.e., $x$ is also periodic in time with the same period. Is there any particular method I should apply? I need help badly. Your help/suggestion will be greatly appreciated.",,['ordinary-differential-equations']
22,Solving an inhomogenous parameter dependent ODE,Solving an inhomogenous parameter dependent ODE,,"I was trying to solve the ODE \begin{equation} \ddot{r} r = \alpha(\dot{r}^2-1) \end{equation} where $\alpha$ is an arbitrary constant. There are some simple cases when $\alpha = -1 $ then you can use separation of variables to find the solution. For the initial condition when $r'(0)=1$ it also simplifies, $r(t)= t + r(0)$. Not sure how take on the general case. Any help is welcome!","I was trying to solve the ODE \begin{equation} \ddot{r} r = \alpha(\dot{r}^2-1) \end{equation} where $\alpha$ is an arbitrary constant. There are some simple cases when $\alpha = -1 $ then you can use separation of variables to find the solution. For the initial condition when $r'(0)=1$ it also simplifies, $r(t)= t + r(0)$. Not sure how take on the general case. Any help is welcome!",,['ordinary-differential-equations']
23,Stability of a system that has (Jacobian-like) matrix with eigenvalue of less than 1 that has $x$ as non-eigenvector,Stability of a system that has (Jacobian-like) matrix with eigenvalue of less than 1 that has  as non-eigenvector,x,"This is about general equilibrium: Suppose that $x(t)$ represents outputs of all sectors and parts of the   whole economy - represented as matrix. How outputs evolve to $x(t+1)$   is determined by the matrix $A$ - $A$ representes how previous outputs   are used as inputs to produce new ouputs - so $A$ can be said as table   of outputs produced from inputs.  $$x(t+1) = A \cdot x(t)$$ and it continues on to say that if the matrix $A$ has (absolute value of) all eigenvalues less than 1, the system is stable, while if not, is unstable. The question  is, I do get that when when $x$ is the eigenvector of $A$, it becomes unstable, but what if it's not - then can we still say that the system is unstable? (so what I am saying is, let's assume that the system will never have $x$ as eigenvector of $A$. Then what happens?) If so, can anyone show the proof of it? (The linked question seems to assume that $x$ is an eigenvector of $A$ - I do not assume that at here.)","This is about general equilibrium: Suppose that $x(t)$ represents outputs of all sectors and parts of the   whole economy - represented as matrix. How outputs evolve to $x(t+1)$   is determined by the matrix $A$ - $A$ representes how previous outputs   are used as inputs to produce new ouputs - so $A$ can be said as table   of outputs produced from inputs.  $$x(t+1) = A \cdot x(t)$$ and it continues on to say that if the matrix $A$ has (absolute value of) all eigenvalues less than 1, the system is stable, while if not, is unstable. The question  is, I do get that when when $x$ is the eigenvector of $A$, it becomes unstable, but what if it's not - then can we still say that the system is unstable? (so what I am saying is, let's assume that the system will never have $x$ as eigenvector of $A$. Then what happens?) If so, can anyone show the proof of it? (The linked question seems to assume that $x$ is an eigenvector of $A$ - I do not assume that at here.)",,"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'economics']"
24,"Solution of the problem $y’(t)=f(t)y(t), \; y(0)=1$ where $f:\mathbb{R}\to\mathbb{R}$ is continuous. (CSIR JUNE 2012)",Solution of the problem  where  is continuous. (CSIR JUNE 2012),"y’(t)=f(t)y(t), \; y(0)=1 f:\mathbb{R}\to\mathbb{R}","Consider the initial value problem  $$y’(t)=f(t)y(t), \;y(0)=1$$ where $f:\mathbb{R}\to\mathbb{R}$ is continuous. Then this initial value problem has: Infinitely many solutions for some $f$. A unique solution in $\mathbb{R}$. No solution in $\mathbb{R}$ for some$ f$. A solution in an interval containing $0$, but not on $\mathbb{R}$ for some $f$. Can anyone help me finding which of the options are correct?  Thanks.","Consider the initial value problem  $$y’(t)=f(t)y(t), \;y(0)=1$$ where $f:\mathbb{R}\to\mathbb{R}$ is continuous. Then this initial value problem has: Infinitely many solutions for some $f$. A unique solution in $\mathbb{R}$. No solution in $\mathbb{R}$ for some$ f$. A solution in an interval containing $0$, but not on $\mathbb{R}$ for some $f$. Can anyone help me finding which of the options are correct?  Thanks.",,['ordinary-differential-equations']
25,Show that the series representation of the Bessel function works,Show that the series representation of the Bessel function works,,"For the following series representation of the Bessel function: $$w = J_n = \sum_{k=0}^{\infty} \frac{(-1)^k z^{n+2k}}{k!(n+k)!2^{n+2k}}.$$ I want to show that w is indeed the Bessel function, such that $w'' + \frac{1}{z}w' + (1-\frac{n^2}{z^2})w = 0$, with the series definition. I got the following first and second derivatives: $$w' = \sum_{k=1}^{\infty} \frac{(-1)^k (n+2k) z^{n+2k-1}}{k!(n+k)!2^{n+2k}},$$ and $$w'' = \sum_{k=1}^{\infty} \frac{(-1)^k (n+2k) (n+2k-1) z^{n+2k-2}}{k!(n+k)!2^{n+2k}}.$$ I tried summing the equations by brute force, which got me as far as $$w'' + \frac{1}{z}w' + (1-\frac{n^2}{z^2})w = \sum_{k=1}^{\infty} \frac{(-1)^k (4nk+4k^2) z^{n+2k-2}+(-1)^k z^{n+2k}}{k!(n+k)!2^{n+2k}} + (1-\frac{n^2}{z^2})(\frac{z^n}{n!2^n}),$$ but that doesn't seem to equal zero. Any ideas much appreciated.","For the following series representation of the Bessel function: $$w = J_n = \sum_{k=0}^{\infty} \frac{(-1)^k z^{n+2k}}{k!(n+k)!2^{n+2k}}.$$ I want to show that w is indeed the Bessel function, such that $w'' + \frac{1}{z}w' + (1-\frac{n^2}{z^2})w = 0$, with the series definition. I got the following first and second derivatives: $$w' = \sum_{k=1}^{\infty} \frac{(-1)^k (n+2k) z^{n+2k-1}}{k!(n+k)!2^{n+2k}},$$ and $$w'' = \sum_{k=1}^{\infty} \frac{(-1)^k (n+2k) (n+2k-1) z^{n+2k-2}}{k!(n+k)!2^{n+2k}}.$$ I tried summing the equations by brute force, which got me as far as $$w'' + \frac{1}{z}w' + (1-\frac{n^2}{z^2})w = \sum_{k=1}^{\infty} \frac{(-1)^k (4nk+4k^2) z^{n+2k-2}+(-1)^k z^{n+2k}}{k!(n+k)!2^{n+2k}} + (1-\frac{n^2}{z^2})(\frac{z^n}{n!2^n}),$$ but that doesn't seem to equal zero. Any ideas much appreciated.",,"['sequences-and-series', 'ordinary-differential-equations', 'special-functions', 'power-series']"
26,Basic differential equation proof,Basic differential equation proof,,"Show that if $u(t)$ solves $\dot{u} = Au$, then $v(t) = u(-t)$, solves   $\dot{v} = Bv$, where $B = -A$. Similarly, show that if u(t) solves $\dot{u} = Au$, then $v(t) =  u(2t)$ solves $\dot{v} = Bv$, where $B = 2A$. I don't know how to formally prove this. It seems obvious because since the solution $v(t) = u(-t)$ is the same as $u(t)$ but it has a scalar which is negative and a scalar can be taken out of the solution and placed outside which follows that we have $-A$. The same goes with the second part of the question.","Show that if $u(t)$ solves $\dot{u} = Au$, then $v(t) = u(-t)$, solves   $\dot{v} = Bv$, where $B = -A$. Similarly, show that if u(t) solves $\dot{u} = Au$, then $v(t) =  u(2t)$ solves $\dot{v} = Bv$, where $B = 2A$. I don't know how to formally prove this. It seems obvious because since the solution $v(t) = u(-t)$ is the same as $u(t)$ but it has a scalar which is negative and a scalar can be taken out of the solution and placed outside which follows that we have $-A$. The same goes with the second part of the question.",,['ordinary-differential-equations']
27,Solve $xy'(\cos(2x^3)\cos(x)-\sin(2x^3)\sin(x)) -y(6x^3+x)\sin(2x^3+x)+(12x^3+2x)\sin(2x^3+x)=0$,Solve,xy'(\cos(2x^3)\cos(x)-\sin(2x^3)\sin(x)) -y(6x^3+x)\sin(2x^3+x)+(12x^3+2x)\sin(2x^3+x)=0,"I need to solve this differential equation. What I'm looking for is a way to simplify this equation. Can anybody give me hints/tricks to understand the following equation better: \begin{align*} xy'(\cos(2x^3)\cos(x)-\sin(2x^3)&\sin(x))-y(6x^3+x)\sin(2x^3+x)\\&+(12x^3+2x)\sin(2x^3+x)=0 \end{align*} Ultimately I want to separate variables and integrate to solve for $y(x)$ Thanks for your help :) Edit: Please If you can help me solve this differential equation, any help would be appreciated :) thanks","I need to solve this differential equation. What I'm looking for is a way to simplify this equation. Can anybody give me hints/tricks to understand the following equation better: Ultimately I want to separate variables and integrate to solve for Thanks for your help :) Edit: Please If you can help me solve this differential equation, any help would be appreciated :) thanks","\begin{align*}
xy'(\cos(2x^3)\cos(x)-\sin(2x^3)&\sin(x))-y(6x^3+x)\sin(2x^3+x)\\&+(12x^3+2x)\sin(2x^3+x)=0
\end{align*} y(x)",['ordinary-differential-equations']
28,Finding general solution to system of ODEs using complex eigenvalues,Finding general solution to system of ODEs using complex eigenvalues,,Use the eigenvalue method to find the general solution to the initial value problem: $x_1' = 3x_1-5x_2$ $x_2' = 5x_1+3x_2$ $x_1(0) = 1$ and $x_2(0) = 4$ I found complex eigenvalues $\lambda=3-5i$ and $\lambda = 3+5i$ which have corresponding eigenvectors $\left[ \begin{array}{cccc} 1\\i \end{array} \right]$ and $\left[ \begin{array}{cccc} 1\\-i \end{array} \right]$.  Now I'm not sure how I can write the general solution.  Does it involve both eigenvectors?,Use the eigenvalue method to find the general solution to the initial value problem: $x_1' = 3x_1-5x_2$ $x_2' = 5x_1+3x_2$ $x_1(0) = 1$ and $x_2(0) = 4$ I found complex eigenvalues $\lambda=3-5i$ and $\lambda = 3+5i$ which have corresponding eigenvectors $\left[ \begin{array}{cccc} 1\\i \end{array} \right]$ and $\left[ \begin{array}{cccc} 1\\-i \end{array} \right]$.  Now I'm not sure how I can write the general solution.  Does it involve both eigenvectors?,,['ordinary-differential-equations']
29,A differential equation of Buckling Rod.,A differential equation of Buckling Rod.,,"I tried to solve a differential equation, but unfortunately got stuck at some point. The problem is to solve the differentail equation of hard clamped on both ends rod.  And the force compresses the rod at both ends.  the solution $v(x)$ is the value of bending I need. I assuming, that the differential equation of buckling rod is  $$ EI_{x}v''''+Pv''=0$$ where $P$ is a force. and $EI_x$ is inflexibility. Then I find the solution for the diffierential equation: $$v(x) = \frac{c_1\cos(x\sqrt{P})+c_2\sin(x\sqrt{P})}{P}+c_4 x+c_3$$ The boundary conditions: $$v(0)=v(l)=0=v'(0)=v'(l)$$ gives the trivial solution for $c_{1},c_{2},c_{3},c_{4}$ but I need non-trivial ones. Could you please help me to find the mistake or explain what's wrong in my equation?","I tried to solve a differential equation, but unfortunately got stuck at some point. The problem is to solve the differentail equation of hard clamped on both ends rod.  And the force compresses the rod at both ends.  the solution $v(x)$ is the value of bending I need. I assuming, that the differential equation of buckling rod is  $$ EI_{x}v''''+Pv''=0$$ where $P$ is a force. and $EI_x$ is inflexibility. Then I find the solution for the diffierential equation: $$v(x) = \frac{c_1\cos(x\sqrt{P})+c_2\sin(x\sqrt{P})}{P}+c_4 x+c_3$$ The boundary conditions: $$v(0)=v(l)=0=v'(0)=v'(l)$$ gives the trivial solution for $c_{1},c_{2},c_{3},c_{4}$ but I need non-trivial ones. Could you please help me to find the mistake or explain what's wrong in my equation?",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'classical-mechanics', 'mathematical-physics']"
30,Solution simple ODE,Solution simple ODE,,I'm desperately trying to find the solution of this simple ODE: $$\frac{dx}{dt}= C +\frac{x-a_1}{b_1} + \frac{x-a_2}{b_2} $$ Where C is a constant. Someone has a clue? Thanks for the feedback already: Ok some more info: I think I can solve this by substituting $x$ by $e^{t}$. In that case I get: $$ e^t = C+\frac{e^t-a_1}{b_1} + \frac{e^t-a_2}{b_2}$$ But now I'm stuck. Does it mean x is just: $$ x (1-1/b_1 -1/b_2)= (C-a_1/b_a -a_2/b_2)  $$ But then it is no longer depending on t... Iḿ doing something wrong here,I'm desperately trying to find the solution of this simple ODE: $$\frac{dx}{dt}= C +\frac{x-a_1}{b_1} + \frac{x-a_2}{b_2} $$ Where C is a constant. Someone has a clue? Thanks for the feedback already: Ok some more info: I think I can solve this by substituting $x$ by $e^{t}$. In that case I get: $$ e^t = C+\frac{e^t-a_1}{b_1} + \frac{e^t-a_2}{b_2}$$ But now I'm stuck. Does it mean x is just: $$ x (1-1/b_1 -1/b_2)= (C-a_1/b_a -a_2/b_2)  $$ But then it is no longer depending on t... Iḿ doing something wrong here,,['ordinary-differential-equations']
31,Finding the general solution of a sixth degree differential equation,Finding the general solution of a sixth degree differential equation,,Find a differential equation whose solutions are $y_1 = e^{2x} + e^{-4x}\sin(3x)$ and $y_2 =  e^{-2x} + 5e^{2x}$. Am I supposed to assume that $y_1$ and $y_2$ can take the forms: $y_1 = Ae^{2x} + e^{-4x}[C\cos(3x)+D\sin(3x)]$ $y_2 = Ee^{-2x} + Fe^{2x}$ I'm not sure where to go from here.  Any advice? Thank you!,Find a differential equation whose solutions are $y_1 = e^{2x} + e^{-4x}\sin(3x)$ and $y_2 =  e^{-2x} + 5e^{2x}$. Am I supposed to assume that $y_1$ and $y_2$ can take the forms: $y_1 = Ae^{2x} + e^{-4x}[C\cos(3x)+D\sin(3x)]$ $y_2 = Ee^{-2x} + Fe^{2x}$ I'm not sure where to go from here.  Any advice? Thank you!,,['ordinary-differential-equations']
32,Find two other linearly independent solutions to the second order differential equation,Find two other linearly independent solutions to the second order differential equation,,"Fnd a general solution to the diﬀerential equation $y'' - y' - 2y = 0$. Then, use the two solutions you found to write two other linearly independent solutions to the problem. Write a second general solution using your new linearly independent solutions. We have the characteristic equation: $r^2 - r - 2 = 0$ $(r-2)(r+1) = 0$ Thus, we have real, distinct roots $r_1 = 2$ and $r_2 = -1$.  Our independent solutions are then $c_1e^{r_1x}$ and $c_2e^{r_2x}$ and so our general solution is $y(x) = c_1e^{r_1x} + c_2e^{r_2x}$. Now, I'm just unsure how I would go about writing two other linearly independent solutions and the second general solution.  Can I just substitute two random values for $c_1$ and $c_2$? Thank you!","Fnd a general solution to the diﬀerential equation $y'' - y' - 2y = 0$. Then, use the two solutions you found to write two other linearly independent solutions to the problem. Write a second general solution using your new linearly independent solutions. We have the characteristic equation: $r^2 - r - 2 = 0$ $(r-2)(r+1) = 0$ Thus, we have real, distinct roots $r_1 = 2$ and $r_2 = -1$.  Our independent solutions are then $c_1e^{r_1x}$ and $c_2e^{r_2x}$ and so our general solution is $y(x) = c_1e^{r_1x} + c_2e^{r_2x}$. Now, I'm just unsure how I would go about writing two other linearly independent solutions and the second general solution.  Can I just substitute two random values for $c_1$ and $c_2$? Thank you!",,['ordinary-differential-equations']
33,Direction Field for System of Equations,Direction Field for System of Equations,,"I have a question that asks to draw the direction field for the set of linear systems. $$\begin{align} \frac{d\,x}{dt} &= -x + y + 1 \\ \frac{d\,y}{dt}&=x+y+3\end{align}$$ My attempt: What I did first was set the system in the form of $d(Q)/dt = KQ + b$, then I found the critical points which was at $[2,1]$ and then I chose an arbitrary point in the plane, lets pick $[1,2]$, and set it as my Q and then solved and found the vector $[1,3]$ to be associated to that point, but this is where I have a problem. How will this vector be plotted? I am on point $[1,2]$ and the tangent vector is $[1,3]$ but how can I draw this? Will it go up 1 unit and right 3?","I have a question that asks to draw the direction field for the set of linear systems. $$\begin{align} \frac{d\,x}{dt} &= -x + y + 1 \\ \frac{d\,y}{dt}&=x+y+3\end{align}$$ My attempt: What I did first was set the system in the form of $d(Q)/dt = KQ + b$, then I found the critical points which was at $[2,1]$ and then I chose an arbitrary point in the plane, lets pick $[1,2]$, and set it as my Q and then solved and found the vector $[1,3]$ to be associated to that point, but this is where I have a problem. How will this vector be plotted? I am on point $[1,2]$ and the tangent vector is $[1,3]$ but how can I draw this? Will it go up 1 unit and right 3?",,['ordinary-differential-equations']
34,How to solve that ODE?,How to solve that ODE?,,"I'm am trying to solve the following differentia equation $$ u'(x)=\frac{u(x)}{\sqrt{u^2(x)-\alpha^2}}, $$ where $\alpha$ is some non-zero real number. Any ideas ?","I'm am trying to solve the following differentia equation $$ u'(x)=\frac{u(x)}{\sqrt{u^2(x)-\alpha^2}}, $$ where $\alpha$ is some non-zero real number. Any ideas ?",,"['real-analysis', 'ordinary-differential-equations']"
35,Solving probability problem using differential equation,Solving probability problem using differential equation,,"I have a line that has initially length one. A process reduces the length of the line. During each step of the process 2 points $x_1$ and $x_2$ on the remaining line are selected. The part of the line between the left side and the maximum of $x_1$ and $x_2$ is cut off. What is the expected number of times $E(x)$ the process should be repeated such that a fraction x of the initial line has disappeared. I would like to solve this problem using a differential equation. I did allready some work on this problem, but I am not sure of the correct DE to solve the problem. More precisely I would like to know if the correct DE to solve this problem is $$E(x) = 1-x^2 + \int_0^x 2y+2yE(\frac{x-y}{1-y}) dy$$ or $$E(x) = 1-x^2 + \int_0^x 1+2yE(\frac{x-y}{1-y}) dy$$ As you see my problem is that I don't know whether the first term of the integrand should be 1 or 2y. Some additional info to make it easier to understand the DE's: $1-x^2$ = probability 1 step is enough to make disappear more than x $E(\frac{x-y}{1-y})$ = expected number of times the process should be repeated such that a fraction x  has disappeared, starting from length y < x $2y$ = probability distribution function for the maximum of x_1 and x_2 with 0<=y<=1","I have a line that has initially length one. A process reduces the length of the line. During each step of the process 2 points $x_1$ and $x_2$ on the remaining line are selected. The part of the line between the left side and the maximum of $x_1$ and $x_2$ is cut off. What is the expected number of times $E(x)$ the process should be repeated such that a fraction x of the initial line has disappeared. I would like to solve this problem using a differential equation. I did allready some work on this problem, but I am not sure of the correct DE to solve the problem. More precisely I would like to know if the correct DE to solve this problem is $$E(x) = 1-x^2 + \int_0^x 2y+2yE(\frac{x-y}{1-y}) dy$$ or $$E(x) = 1-x^2 + \int_0^x 1+2yE(\frac{x-y}{1-y}) dy$$ As you see my problem is that I don't know whether the first term of the integrand should be 1 or 2y. Some additional info to make it easier to understand the DE's: $1-x^2$ = probability 1 step is enough to make disappear more than x $E(\frac{x-y}{1-y})$ = expected number of times the process should be repeated such that a fraction x  has disappeared, starting from length y < x $2y$ = probability distribution function for the maximum of x_1 and x_2 with 0<=y<=1",,"['probability', 'ordinary-differential-equations']"
36,Solving $xdy+(y-xe^x)dx=0$ from GRE 8767.,Solving  from GRE 8767.,xdy+(y-xe^x)dx=0,"I'm a little confused about GRE 8767 problem #40. Let $y=f(x)$ be a solution of the differential equation $xdy+(y-xe^x)dx=0$ such that $y=0$ when $x=1$. What is the value of $f(2)$? I see that this is an exact differential since $g(x,y)=xy+e^x-xe^x+C$ is such that $g_x=y-xe^x$ and $g_y=x$. How can I use this to find $f(2)$? I tried writing $$ g(2,f(2))=2f(2)+e^2-2e^2+C=\cdots $$ but I'm not sure what follows. Thanks. The final answer is $e^2/2$ by the way.","I'm a little confused about GRE 8767 problem #40. Let $y=f(x)$ be a solution of the differential equation $xdy+(y-xe^x)dx=0$ such that $y=0$ when $x=1$. What is the value of $f(2)$? I see that this is an exact differential since $g(x,y)=xy+e^x-xe^x+C$ is such that $g_x=y-xe^x$ and $g_y=x$. How can I use this to find $f(2)$? I tried writing $$ g(2,f(2))=2f(2)+e^2-2e^2+C=\cdots $$ but I'm not sure what follows. Thanks. The final answer is $e^2/2$ by the way.",,"['ordinary-differential-equations', 'gre-exam']"
37,Is this a non-linear differential equation? And is there a solution?,Is this a non-linear differential equation? And is there a solution?,,"I have the following two equations, and I'm not sure I'm analyzing them properly. $$ I(t)=CV'(t)\\ V(t)I(t) = P + R_{C}I^{2}(t)\\ \text{Substitute:}\\ CV(t)V'(t) = P + R_{C}C^{2}(V'(t))^{2}\\ $$ If I'm right, this gives me a non-linear differential equation, which puts me well past my mathematical comfort zone. Is this, in fact, a non-linear differential equation? If not, can anyone help alleviate my misunderstanding? If it IS non-linear, does anyone recognize this as an equation that has been/can be solved? Or am I out of luck as far as an exact solution goes?","I have the following two equations, and I'm not sure I'm analyzing them properly. $$ I(t)=CV'(t)\\ V(t)I(t) = P + R_{C}I^{2}(t)\\ \text{Substitute:}\\ CV(t)V'(t) = P + R_{C}C^{2}(V'(t))^{2}\\ $$ If I'm right, this gives me a non-linear differential equation, which puts me well past my mathematical comfort zone. Is this, in fact, a non-linear differential equation? If not, can anyone help alleviate my misunderstanding? If it IS non-linear, does anyone recognize this as an equation that has been/can be solved? Or am I out of luck as far as an exact solution goes?",,['ordinary-differential-equations']
38,Find a continuous solution of the initial-value problem,Find a continuous solution of the initial-value problem,,"This question is from DE book by Braun(Pg no 10, Q no 17), Find a continuous solution of the initial-value problem $y'+y= g(t), y(0)= 0$ where $g(t)=\begin{cases}2, &0 \leq t\leq 1, \\0, &t > 1\end{cases} $ since the intial condition is given at (0,0), therefore we consider $g(t)=2$ and so solving $y'+y=2$ gives integrating factor $\mu(t)=Ce^t \Rightarrow y=2+Ce^{-t} \Rightarrow C=-2 \Rightarrow y=2(1-e^{-t})$, the answer given in the text it this $y(t) = \begin{cases} 2(1-e^{-t}), &0\leq t\leq 1\\ 2(e-1)e^{-t},  &t > 1 \end{cases} $ even if we consider $g(t)=0$ we get $\ln|y|=-t+C$ and we cannot proceed further since there is no initial condition, $y(1)=?$ my question is how to solve for $y$ at $t>1$ and also, do we find left and right limit at $t=1$ to prove that the answer is a continuous function?","This question is from DE book by Braun(Pg no 10, Q no 17), Find a continuous solution of the initial-value problem $y'+y= g(t), y(0)= 0$ where $g(t)=\begin{cases}2, &0 \leq t\leq 1, \\0, &t > 1\end{cases} $ since the intial condition is given at (0,0), therefore we consider $g(t)=2$ and so solving $y'+y=2$ gives integrating factor $\mu(t)=Ce^t \Rightarrow y=2+Ce^{-t} \Rightarrow C=-2 \Rightarrow y=2(1-e^{-t})$, the answer given in the text it this $y(t) = \begin{cases} 2(1-e^{-t}), &0\leq t\leq 1\\ 2(e-1)e^{-t},  &t > 1 \end{cases} $ even if we consider $g(t)=0$ we get $\ln|y|=-t+C$ and we cannot proceed further since there is no initial condition, $y(1)=?$ my question is how to solve for $y$ at $t>1$ and also, do we find left and right limit at $t=1$ to prove that the answer is a continuous function?",,['ordinary-differential-equations']
39,Help solving differential equation: $y' = x\sqrt{4+y^{2}}/{y(9+x^{2})}$,Help solving differential equation:,y' = x\sqrt{4+y^{2}}/{y(9+x^{2})},"I do not have an idea where to start to solve the following differential equation, so every tip is welcome. $$y' = \frac{x\sqrt{4+y^{2}}}{y(9+x^{2})} $$","I do not have an idea where to start to solve the following differential equation, so every tip is welcome. $$y' = \frac{x\sqrt{4+y^{2}}}{y(9+x^{2})} $$",,['ordinary-differential-equations']
40,Integrating Factor: How to solve it,Integrating Factor: How to solve it,,"Verify that: $$\frac12(Mx+Ny)d(\ln(xy))+\frac12(Mx-Ny)d(\ln(x/y))=Mdx+Ndy$$ Hence show that, if the de $Mdx+Ndy=0$ is homogenous, then $Mx+Ny$ is an integrating factor unless $Mx+Ny=0$ Note: Verification is trivial, hence nothing much to be done there, but I couldnt solve the second part of the question ""Hence..."" so for the completeness of the problem I added it. Further on, isnt the statement "" $Mdx+Ndy=0$ is homogenous "" superfluous as RHS is already zero, so why add the word homogenous. Perhaps I am being pedantic? And lastly I would like to have some hints in solving the INTEGRATING Factor part. EDIT: My approach I approached like this: I multiplied the function $Mx+Ny$ to both sides of the equation $Mdx+Ndy=0$ and tried to show, that $d(u(x,y))=0$ but I couldnt prove it. Soham","Verify that: $$\frac12(Mx+Ny)d(\ln(xy))+\frac12(Mx-Ny)d(\ln(x/y))=Mdx+Ndy$$ Hence show that, if the de $Mdx+Ndy=0$ is homogenous, then $Mx+Ny$ is an integrating factor unless $Mx+Ny=0$ Note: Verification is trivial, hence nothing much to be done there, but I couldnt solve the second part of the question ""Hence..."" so for the completeness of the problem I added it. Further on, isnt the statement "" $Mdx+Ndy=0$ is homogenous "" superfluous as RHS is already zero, so why add the word homogenous. Perhaps I am being pedantic? And lastly I would like to have some hints in solving the INTEGRATING Factor part. EDIT: My approach I approached like this: I multiplied the function $Mx+Ny$ to both sides of the equation $Mdx+Ndy=0$ and tried to show, that $d(u(x,y))=0$ but I couldnt prove it. Soham",,"['calculus', 'ordinary-differential-equations']"
41,How do I solve a certain characteristic system?,How do I solve a certain characteristic system?,,"I am studying PDEs and have the following (seemingly simple) problem: Find a surface that passes through the curve $$x^2+y^2=z=1$$ and is orthogonal to the family of surfaces $$z(x+y)=c(3z+1)\qquad(c\in\Bbb R)$$ After writing down the orthogonality condition (assuming my calculations are correct $(*)$), this yields the following equation: $$u(3u+1)(u_x+u_y)-x-y=0$$ We usually solve such equations by using the method of characteristics, which tells us (using assumption $(*)$ again) to solve the following characteristic system: $$\begin{align}\dot x=&u(3u+1)\\\dot y=&u(3u+1)\\\dot u =&x+y\end{align}$$ Differentiating the last equation of this system with respect to $t$ gives us $\ddot u=\dot x+\dot y$, which using the first two equations gives us $$\ddot u = 2u(3u+1)$$ After staring at this equation for some time, I decided to ask Wolfram|Alpha . The result seems pretty ugly, so the following questions arise: Did I make a mistake/am I missing something? Is my approach correct? How do I proceed? Thanks.","I am studying PDEs and have the following (seemingly simple) problem: Find a surface that passes through the curve $$x^2+y^2=z=1$$ and is orthogonal to the family of surfaces $$z(x+y)=c(3z+1)\qquad(c\in\Bbb R)$$ After writing down the orthogonality condition (assuming my calculations are correct $(*)$), this yields the following equation: $$u(3u+1)(u_x+u_y)-x-y=0$$ We usually solve such equations by using the method of characteristics, which tells us (using assumption $(*)$ again) to solve the following characteristic system: $$\begin{align}\dot x=&u(3u+1)\\\dot y=&u(3u+1)\\\dot u =&x+y\end{align}$$ Differentiating the last equation of this system with respect to $t$ gives us $\ddot u=\dot x+\dot y$, which using the first two equations gives us $$\ddot u = 2u(3u+1)$$ After staring at this equation for some time, I decided to ask Wolfram|Alpha . The result seems pretty ugly, so the following questions arise: Did I make a mistake/am I missing something? Is my approach correct? How do I proceed? Thanks.",,"['ordinary-differential-equations', 'partial-differential-equations']"
42,A differential equation introduced from a physics problem,A differential equation introduced from a physics problem,,"Try to solve the equation \[ c_1 \sqrt{f(x)} + c_2f'(x) = c_3 \sqrt{f(x)} f''(x) \] holds for all $x \ge 0$. There might be another condition: $f(0) = 0$. It is introduced from a high school physics exam problem on $s, v, a$. The answer to the problem makes a hypothesis that the motion is uniformly accelerated motion and checks and says that it is true. It is equivalent to only check when $f(x) = (\alpha x + \beta)^2$ where $\alpha, \beta \ge 0$, then the equation becomes \[ c_1 (\alpha x + \beta) + 2c_2 \alpha (\alpha x + \beta) = 2c_3 \alpha (\alpha x + \beta) \] and find a solution with $\alpha, \beta \ge 0$, saying proved . I don't think it's a rigorous proof. I wonder whether the equation can be solved rigorously? Thanks for help.","Try to solve the equation \[ c_1 \sqrt{f(x)} + c_2f'(x) = c_3 \sqrt{f(x)} f''(x) \] holds for all $x \ge 0$. There might be another condition: $f(0) = 0$. It is introduced from a high school physics exam problem on $s, v, a$. The answer to the problem makes a hypothesis that the motion is uniformly accelerated motion and checks and says that it is true. It is equivalent to only check when $f(x) = (\alpha x + \beta)^2$ where $\alpha, \beta \ge 0$, then the equation becomes \[ c_1 (\alpha x + \beta) + 2c_2 \alpha (\alpha x + \beta) = 2c_3 \alpha (\alpha x + \beta) \] and find a solution with $\alpha, \beta \ge 0$, saying proved . I don't think it's a rigorous proof. I wonder whether the equation can be solved rigorously? Thanks for help.",,"['calculus', 'ordinary-differential-equations']"
43,How do I numerically calculate a function from its gradient?,How do I numerically calculate a function from its gradient?,,"I know the gradient of a function t on a cartesian grid: $\vec{g}(xi,yj,zk)=\nabla t(xi,yj,zk)$. I know t for the center pillar: $\ t(xc,yc,zk)$. For each node in the cartesian grid I want to calculate the timeshift: $\ ts(xi,yj,zk) = t(xi,yj,zk) - t(xc,yc,zk)$ According to the gradient theorem:  $\ ts(xi,yj,zk) = \int_{(xc,yc,zk)}^{(xi,yj,zk)} \vec{g}(\vec{r})d\vec{r}$ But how do I quck and runtime effective code this (in Matlab)? I want to avoid summing over n contributions for every node in the grid; O(n^4) :-(. Thanks in advance for any answers!","I know the gradient of a function t on a cartesian grid: $\vec{g}(xi,yj,zk)=\nabla t(xi,yj,zk)$. I know t for the center pillar: $\ t(xc,yc,zk)$. For each node in the cartesian grid I want to calculate the timeshift: $\ ts(xi,yj,zk) = t(xi,yj,zk) - t(xc,yc,zk)$ According to the gradient theorem:  $\ ts(xi,yj,zk) = \int_{(xc,yc,zk)}^{(xi,yj,zk)} \vec{g}(\vec{r})d\vec{r}$ But how do I quck and runtime effective code this (in Matlab)? I want to avoid summing over n contributions for every node in the grid; O(n^4) :-(. Thanks in advance for any answers!",,"['integration', 'ordinary-differential-equations', 'numerical-methods']"
44,Show velocity of a particle during its flight at time $t$,Show velocity of a particle during its flight at time,t,"I'm completely stuck, I think I have to use Newton's second law but I have no idea where to start, any help would be appreciated! At time $t=0$ a particle of unit mass is projected vertically upward with velocity $v_0$ against gravity, and the resistance of the air to the particle's motion is $k$ times its velocity. Show that during its flight the velocity $v$ of the particle at time $t$ is: $$v = \left(v_{0} + \frac{g}{k}\right)  e^{-kt} - \frac{g}{k}$$ Deduce that the particle reaches its greatest height when $$t = \frac{1}{k} \ln\left({1+\frac{kv_{0}}{g}}\right)$$ and that the height reached is $$ \frac{v_{0}}{k} - \frac{g}{k^2} \ln{\left(1 + \frac{kv_{0}}{g}\right)}$$ Thanks!","I'm completely stuck, I think I have to use Newton's second law but I have no idea where to start, any help would be appreciated! At time $t=0$ a particle of unit mass is projected vertically upward with velocity $v_0$ against gravity, and the resistance of the air to the particle's motion is $k$ times its velocity. Show that during its flight the velocity $v$ of the particle at time $t$ is: $$v = \left(v_{0} + \frac{g}{k}\right)  e^{-kt} - \frac{g}{k}$$ Deduce that the particle reaches its greatest height when $$t = \frac{1}{k} \ln\left({1+\frac{kv_{0}}{g}}\right)$$ and that the height reached is $$ \frac{v_{0}}{k} - \frac{g}{k^2} \ln{\left(1 + \frac{kv_{0}}{g}\right)}$$ Thanks!",,"['ordinary-differential-equations', 'integration', 'classical-mechanics']"
45,1st order ODE problem (forward euler),1st order ODE problem (forward euler),,"Let ${du\over dt} = u, \ u(0)=1$. Let the step size be $\tau = 1/3$. Let $0=t_0<t_1=\frac{1}{3}<t_2 =\frac{2}{3}<t_3=1$. Given $U^0=u(0) =1$, we find $U^{n+1}$ $U^{n+1}=U^n+\tau f(t,U^N) \\ U^1=U^0+\tau f(t_0,U^0) \\ U^1 =1+\frac{1}{3}\cdot 1$ I tried to understand but Im still stuck, how does $f(t_0,U^0)=1$? I know $f(t_0,U^0)=f(0,1)$, but how does $f=(0,1)=1$?","Let ${du\over dt} = u, \ u(0)=1$. Let the step size be $\tau = 1/3$. Let $0=t_0<t_1=\frac{1}{3}<t_2 =\frac{2}{3}<t_3=1$. Given $U^0=u(0) =1$, we find $U^{n+1}$ $U^{n+1}=U^n+\tau f(t,U^N) \\ U^1=U^0+\tau f(t_0,U^0) \\ U^1 =1+\frac{1}{3}\cdot 1$ I tried to understand but Im still stuck, how does $f(t_0,U^0)=1$? I know $f(t_0,U^0)=f(0,1)$, but how does $f=(0,1)=1$?",,"['ordinary-differential-equations', 'numerical-methods']"
46,2nd order ODE to 1st order ODE/Forward euler method,2nd order ODE to 1st order ODE/Forward euler method,,"I have a $2^\text{nd}$ ODE: $$ \begin{cases}{d^2u \over dt^2} =5tu+\sin \left({du\over dt}\right)\\[5 pt] u(0)=1\\[5 pt]  {du\over dt}(0)=0 \end{cases} $$ I was reading my notes and it asked to write the $2^\text{nd}$ order ODE as a system of $1^\text{st}$ order ODEs. And then to construct a forward euler discretisation of the ODE with step size $\tau =1/2$ and interval $[0,2]$. What was done in the notes was: $$\begin{align} \text{Let }&v={du \over dt}\\ &{dv \over dt}={d^2u \over dt^2}\\  \implies &{dv \over dt}=5tu+\sin v, \ v(0)=0. \end{align} $$ I understood the above, but I'm not sure what was done after that. Could someone explain to me what was done below? Let $$ w= \left(     \begin{matrix}       u \\       v     \end{matrix}   \right)\\  \text{then } {dw \over dt}=f(t,w), \;\;\;\;\;\; w(0)=w_0 \\ \text{where } f(t,w)=\left(     \begin{matrix}       v \\       5tu+\sin v     \end{matrix}   \right) \text{ and } w_0=\left(     \begin{matrix}       1 \\       0     \end{matrix}   \right)$$ Continuing on from there, how does the following work? In particular how does $$ f(t_0, W^0)= \left(     \begin{matrix}       V^0 \\       5\cdot 0 \cdot U^0 + \sin V^0      \end{matrix}   \right) = \left(     \begin{matrix}       0 \\       0     \end{matrix}   \right)$$ Forward euler for the $1^\text{st}$ order system: Given $W^0=w_0$, find $W^{n+1}$ such that $$W^{n+1}=W^n+\tau f(t_n,w)$$ $$n=0 \implies $W^0= \left(     \begin{array}{c}       1 \\       0     \end{array}   \right) \\f(t_0, W^0)= \left(     \begin{array}{c}       V^0 \\       5\cdot 0 \cdot U^0 + \sin V^0      \end{array}   \right) = \left(     \begin{array}{c}       0 \\       0     \end{array}   \right) \\ \implies W^1 = W^0 +\tau \left(     \begin{array}{c}       0 \\       0     \end{array}   \right) \implies W^1 = W^0 $$","I have a $2^\text{nd}$ ODE: $$ \begin{cases}{d^2u \over dt^2} =5tu+\sin \left({du\over dt}\right)\\[5 pt] u(0)=1\\[5 pt]  {du\over dt}(0)=0 \end{cases} $$ I was reading my notes and it asked to write the $2^\text{nd}$ order ODE as a system of $1^\text{st}$ order ODEs. And then to construct a forward euler discretisation of the ODE with step size $\tau =1/2$ and interval $[0,2]$. What was done in the notes was: $$\begin{align} \text{Let }&v={du \over dt}\\ &{dv \over dt}={d^2u \over dt^2}\\  \implies &{dv \over dt}=5tu+\sin v, \ v(0)=0. \end{align} $$ I understood the above, but I'm not sure what was done after that. Could someone explain to me what was done below? Let $$ w= \left(     \begin{matrix}       u \\       v     \end{matrix}   \right)\\  \text{then } {dw \over dt}=f(t,w), \;\;\;\;\;\; w(0)=w_0 \\ \text{where } f(t,w)=\left(     \begin{matrix}       v \\       5tu+\sin v     \end{matrix}   \right) \text{ and } w_0=\left(     \begin{matrix}       1 \\       0     \end{matrix}   \right)$$ Continuing on from there, how does the following work? In particular how does $$ f(t_0, W^0)= \left(     \begin{matrix}       V^0 \\       5\cdot 0 \cdot U^0 + \sin V^0      \end{matrix}   \right) = \left(     \begin{matrix}       0 \\       0     \end{matrix}   \right)$$ Forward euler for the $1^\text{st}$ order system: Given $W^0=w_0$, find $W^{n+1}$ such that $$W^{n+1}=W^n+\tau f(t_n,w)$$ $$n=0 \implies $W^0= \left(     \begin{array}{c}       1 \\       0     \end{array}   \right) \\f(t_0, W^0)= \left(     \begin{array}{c}       V^0 \\       5\cdot 0 \cdot U^0 + \sin V^0      \end{array}   \right) = \left(     \begin{array}{c}       0 \\       0     \end{array}   \right) \\ \implies W^1 = W^0 +\tau \left(     \begin{array}{c}       0 \\       0     \end{array}   \right) \implies W^1 = W^0 $$",,"['ordinary-differential-equations', 'numerical-methods']"
47,differential equation solution,differential equation solution,,"I have  one problem in this task and please help me solve it. The problem is a first order non linear equation:    $$xy \frac{dy}{dx}=1-x^2.$$ Here I have  moved $y$  in one side and $x$ to the other, so I have $$y\frac{dy}{dx}=\frac{1}{x}-x.$$ Here, I integrate both  sides. On the right, I get: $$\ln(x)-\frac{x^2}{2}+c$$ but what about left part? $\displaystyle \int y \frac{dy}{dx}$ How do I evaluate that? I have tried to take $y$ as a function of $x$, for example $y=kx$, $\frac{dy}{dx}=k$ and   so  $y \frac{dy}{dx}=kxk=k^2x$ ; if  we integrate we get $k^2x^2/2=y^2/2$ so does it mean that $\displaystyle \int y \frac{dy}{dx} =\frac{y^2}{2}$ ? thanks","I have  one problem in this task and please help me solve it. The problem is a first order non linear equation:    $$xy \frac{dy}{dx}=1-x^2.$$ Here I have  moved $y$  in one side and $x$ to the other, so I have $$y\frac{dy}{dx}=\frac{1}{x}-x.$$ Here, I integrate both  sides. On the right, I get: $$\ln(x)-\frac{x^2}{2}+c$$ but what about left part? $\displaystyle \int y \frac{dy}{dx}$ How do I evaluate that? I have tried to take $y$ as a function of $x$, for example $y=kx$, $\frac{dy}{dx}=k$ and   so  $y \frac{dy}{dx}=kxk=k^2x$ ; if  we integrate we get $k^2x^2/2=y^2/2$ so does it mean that $\displaystyle \int y \frac{dy}{dx} =\frac{y^2}{2}$ ? thanks",,"['calculus', 'ordinary-differential-equations']"
48,Newtonian potential of a rotationally-invariant function,Newtonian potential of a rotationally-invariant function,,"Lately I read up in the wikipedia article about the Newtonian potential , that for any compactly supported continuous function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ that is rotationally invariant (for simplicity I assume $d>2$) outside of the support of $f$ whe have the equality $$f*\Gamma(x) =\lambda \Gamma(x),\quad \lambda=\int_{\mathbb{R}^d} f(y)\,dy.$$ where $$\Gamma(x) = \frac{1}{d(2-d)\omega_d} | x | ^{2-d} $$ and $\omega_d$ is the volume of the unit $d$-ball. I like the statement and tried to show it but had no success, therefore I looked up the mentioned references in the wikipedia article, but in both books I could not find the theorem. Can anyone provide a proof or better a reference containing a basic proof?","Lately I read up in the wikipedia article about the Newtonian potential , that for any compactly supported continuous function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ that is rotationally invariant (for simplicity I assume $d>2$) outside of the support of $f$ whe have the equality $$f*\Gamma(x) =\lambda \Gamma(x),\quad \lambda=\int_{\mathbb{R}^d} f(y)\,dy.$$ where $$\Gamma(x) = \frac{1}{d(2-d)\omega_d} | x | ^{2-d} $$ and $\omega_d$ is the volume of the unit $d$-ball. I like the statement and tried to show it but had no success, therefore I looked up the mentioned references in the wikipedia article, but in both books I could not find the theorem. Can anyone provide a proof or better a reference containing a basic proof?",,"['analysis', 'reference-request', 'ordinary-differential-equations', 'convolution', 'orthogonal-polynomials']"
49,Where is the mistake?,Where is the mistake?,,"Consider the IVP  $$x'(t) = 1 + (x(t))^2,\quad x(0) = 0.$$  Backward Euler method is applied to this problem the numerical solution $x_1$ at time $t_1 = h$.Then I found  $$x_1= \frac{1+ (1- 4h^2)^{1/2}}{2h}$$ but solution say $$X_1=\frac{2h}{1 +(1-4h^2)^{1/2}}$$ Where is the mistake? $$\begin{align*} X_{n+1}= X_n +hX'_{n+1} &\implies X_{n+1}= X_n +h( 1 +(X_{n+1})^2)\\ &\implies x(h)= X(0)+h(1+X(h)^2)\\ &\implies hx(h)^2 -X(h)+h\\ &\implies  X_1= \frac{1+(1-4hh)^{1/2}}{ 2h} &\text{(by root finding formula)} \end{align*}$$","Consider the IVP  $$x'(t) = 1 + (x(t))^2,\quad x(0) = 0.$$  Backward Euler method is applied to this problem the numerical solution $x_1$ at time $t_1 = h$.Then I found  $$x_1= \frac{1+ (1- 4h^2)^{1/2}}{2h}$$ but solution say $$X_1=\frac{2h}{1 +(1-4h^2)^{1/2}}$$ Where is the mistake? $$\begin{align*} X_{n+1}= X_n +hX'_{n+1} &\implies X_{n+1}= X_n +h( 1 +(X_{n+1})^2)\\ &\implies x(h)= X(0)+h(1+X(h)^2)\\ &\implies hx(h)^2 -X(h)+h\\ &\implies  X_1= \frac{1+(1-4hh)^{1/2}}{ 2h} &\text{(by root finding formula)} \end{align*}$$",,['ordinary-differential-equations']
50,Approach to Analytically Solving Nonlinear Differential,Approach to Analytically Solving Nonlinear Differential,,"This first order nonlinear differential was posted on a science forum and I am very interested in it.  I would like to know what are the most appropriate steps taken to test for the possibility of an analytical solution, as well as what might be the appropriate steps to solve it analytically. $\dot{x} = - \alpha {x}^{1/2} \arctan{(kx)}$ for $x>0$ and $t \in [0,T]$, where $T < \infty$ and $k > 0$ I'm also kind of curious where it might have come from, which I hadn't asked the original poster and so I do not know!  I'm not looking for a lesson, or a complete solution, just an outline if possible.  The only book I have on differentials does not cover nonlinear and so I'm not even sure if this is a particular case of some subset of study.","This first order nonlinear differential was posted on a science forum and I am very interested in it.  I would like to know what are the most appropriate steps taken to test for the possibility of an analytical solution, as well as what might be the appropriate steps to solve it analytically. $\dot{x} = - \alpha {x}^{1/2} \arctan{(kx)}$ for $x>0$ and $t \in [0,T]$, where $T < \infty$ and $k > 0$ I'm also kind of curious where it might have come from, which I hadn't asked the original poster and so I do not know!  I'm not looking for a lesson, or a complete solution, just an outline if possible.  The only book I have on differentials does not cover nonlinear and so I'm not even sure if this is a particular case of some subset of study.",,['ordinary-differential-equations']
51,Question about non-homogeneous and homogeneous linear D.E.,Question about non-homogeneous and homogeneous linear D.E.,,Suppose $y_1$ is a solution of a non-homogeneous linear D.E. and $y_2$ is a solution to a homogeneous equation. Which of the following is/are true? a. $-y_1$ is a solution to the non-homogeneous equation. b. $-y_2$ is a solution to the homogeneous equation. c. $y_1-y_2$ is a solution of the non-homogeneous equation. d. $y_2-y_1$ is a solution of the homogeneous equation.,Suppose $y_1$ is a solution of a non-homogeneous linear D.E. and $y_2$ is a solution to a homogeneous equation. Which of the following is/are true? a. $-y_1$ is a solution to the non-homogeneous equation. b. $-y_2$ is a solution to the homogeneous equation. c. $y_1-y_2$ is a solution of the non-homogeneous equation. d. $y_2-y_1$ is a solution of the homogeneous equation.,,['ordinary-differential-equations']
52,Differential equation for distribution,Differential equation for distribution,,Consider a distribution $T \in D'(\mathbb{R})$ such as (E) : $T' + gT = 0$ with $g \in D(\mathbb{R})$. Could you prove that $T$ is a strong solution of (E) ? I know that we must use the fondamental theorem of calculus with distribution but i don't know how to conclude... Thanks for answers,Consider a distribution $T \in D'(\mathbb{R})$ such as (E) : $T' + gT = 0$ with $g \in D(\mathbb{R})$. Could you prove that $T$ is a strong solution of (E) ? I know that we must use the fondamental theorem of calculus with distribution but i don't know how to conclude... Thanks for answers,,"['ordinary-differential-equations', 'distribution-theory']"
53,Existence and uniqueness theorems for ODE. Log-Lipschitz regularity.,Existence and uniqueness theorems for ODE. Log-Lipschitz regularity.,,"Let $\mathbb{X}$ be a linear space with a complete metric $d:\mathbb{X}\times\mathbb{X}\to [0,+\infty)$. Let's $B[x_o,b]$ is a compact ball of radius $b$ centered at $x_o$. THEOREM:If $F:[t_o-a,t_o+a]\times B[x_o,b]\subset\mathbb{R}\times\mathbb{X}\to \mathbb{X}$ a limited application, continuous and continuous Lipschiz in $B[x_o,b]$ (note that if $\mathbb {X}$ have finite demension the condition is limited to be redundant).Then there exists a unique solution   $$ \varphi : [t_o-\alpha,t_o+\alpha]\to B[x_o,b] $$   to the problem of Cauchy   $$ x'(t)=F(x,t)\quad x(t_o)=x_o $$   where $\alpha=\min\{a,b\backslash M\}$ and $M=\sup\{|F(t,x)| : (t,x)\in [t_o-a,t_o+a]\times B[x_o,b] \}$. DEFINITION:We say that $F$  is $\gamma$-log-Lipschitz in $B[x_o,b]$ if there exist $\gamma \ge 0$, $L>0$ and $C>0$ such that $$ \|F(x,t)- F(y,t) \| \le C{\bigg(\log\frac{L}{\|x-y\|}\bigg)^{\gamma}}\|x-y\|, $$  for all $ x,y \in B[x_o,b]$ and all $t\in [t_o-a,t_o+a]$. QUESTION 1. There is a version of this theorem for  Log-Lipschitz fields?We may waive the conditions of compactness of the ball and the range in this case? QUESTION 2. There are other more unusual versions of this theorem where the field $ F $ satisfas  $$ \|F(x,t)- F(y,t) \| \le |\Psi(x,y)|\cdot\|x-y\|, $$  for some function $ \Psi :\mathbb{X}\times\mathbb{X}\to\mathbb{R}$?We may waive the conditions of compactness of the ball and the range in this case? Thank you.","Let $\mathbb{X}$ be a linear space with a complete metric $d:\mathbb{X}\times\mathbb{X}\to [0,+\infty)$. Let's $B[x_o,b]$ is a compact ball of radius $b$ centered at $x_o$. THEOREM:If $F:[t_o-a,t_o+a]\times B[x_o,b]\subset\mathbb{R}\times\mathbb{X}\to \mathbb{X}$ a limited application, continuous and continuous Lipschiz in $B[x_o,b]$ (note that if $\mathbb {X}$ have finite demension the condition is limited to be redundant).Then there exists a unique solution   $$ \varphi : [t_o-\alpha,t_o+\alpha]\to B[x_o,b] $$   to the problem of Cauchy   $$ x'(t)=F(x,t)\quad x(t_o)=x_o $$   where $\alpha=\min\{a,b\backslash M\}$ and $M=\sup\{|F(t,x)| : (t,x)\in [t_o-a,t_o+a]\times B[x_o,b] \}$. DEFINITION:We say that $F$  is $\gamma$-log-Lipschitz in $B[x_o,b]$ if there exist $\gamma \ge 0$, $L>0$ and $C>0$ such that $$ \|F(x,t)- F(y,t) \| \le C{\bigg(\log\frac{L}{\|x-y\|}\bigg)^{\gamma}}\|x-y\|, $$  for all $ x,y \in B[x_o,b]$ and all $t\in [t_o-a,t_o+a]$. QUESTION 1. There is a version of this theorem for  Log-Lipschitz fields?We may waive the conditions of compactness of the ball and the range in this case? QUESTION 2. There are other more unusual versions of this theorem where the field $ F $ satisfas  $$ \|F(x,t)- F(y,t) \| \le |\Psi(x,y)|\cdot\|x-y\|, $$  for some function $ \Psi :\mathbb{X}\times\mathbb{X}\to\mathbb{R}$?We may waive the conditions of compactness of the ball and the range in this case? Thank you.",,"['analysis', 'functional-analysis', 'ordinary-differential-equations']"
54,General solution to a differential equation,General solution to a differential equation,,"Is a differential equation still having a general solution even if the differential equation have a singular solution? for example: \begin{aligned}  \frac{dy}{dx} = x y^{1/2} \end{aligned} The Solution: \begin{aligned}  y= \left(\frac {1}{4}x^2+c \right)^2 \end{aligned} But also this singular solution (there is not a constant to obtain it from the above, but is a solution) \begin{aligned}  y= 0 \end{aligned} Is this function correct named as a general solution?: \begin{aligned}  y= \left(\frac {1}{4}x^2+c \right)^2 \end{aligned}","Is a differential equation still having a general solution even if the differential equation have a singular solution? for example: The Solution: But also this singular solution (there is not a constant to obtain it from the above, but is a solution) Is this function correct named as a general solution?:","\begin{aligned} 
\frac{dy}{dx} = x y^{1/2}
\end{aligned} \begin{aligned} 
y= \left(\frac {1}{4}x^2+c \right)^2
\end{aligned} \begin{aligned} 
y= 0
\end{aligned} \begin{aligned} 
y= \left(\frac {1}{4}x^2+c \right)^2
\end{aligned}","['ordinary-differential-equations', 'singular-solution']"
55,Simultaneous Differential Equations,Simultaneous Differential Equations,,"How can we solve the simultaneous equations: $$\frac{d}{dt}\left[\frac{1}{\sqrt{1-x^2-y^2}}\frac{\dot x}{\sqrt{\dot x^2+\dot y^2}}\right]=\frac{x\sqrt{\dot x^2+\dot y^2}}{(1-x^2-y^2)^\frac{3}{2}}$$ $$\frac{d}{dt}\left[\frac{1}{\sqrt{1-x^2-y^2}}\frac{\dot y}{\sqrt{\dot x^2+\dot y^2}}\right]=\frac{y\sqrt{\dot x^2+\dot y^2}}{(1-x^2-y^2)^\frac{3}{2}}$$ I am hoping that the solution is $y=x$, fingers-crossed.","How can we solve the simultaneous equations: $$\frac{d}{dt}\left[\frac{1}{\sqrt{1-x^2-y^2}}\frac{\dot x}{\sqrt{\dot x^2+\dot y^2}}\right]=\frac{x\sqrt{\dot x^2+\dot y^2}}{(1-x^2-y^2)^\frac{3}{2}}$$ $$\frac{d}{dt}\left[\frac{1}{\sqrt{1-x^2-y^2}}\frac{\dot y}{\sqrt{\dot x^2+\dot y^2}}\right]=\frac{y\sqrt{\dot x^2+\dot y^2}}{(1-x^2-y^2)^\frac{3}{2}}$$ I am hoping that the solution is $y=x$, fingers-crossed.",,"['ordinary-differential-equations', 'integration']"
56,Sifting out Solutions to Differential Equations,Sifting out Solutions to Differential Equations,,"I am wondering, is there a general method one uses to seek methods of solutions without much strenuous work and calculations, and possibly a way to get non-trivial solutions for PDE's. A simple counter example would be: Suppose we are given: $\dfrac{\partial u}{\partial x}-2u = 0$ And are asked to find solutions that would satisfy this equation. Well a sort of trivial solution would be to say, suppose we let $u=e^{2x}$. This would imply from the equation that, $$2e^{2x}-2e^{2x}=0.$$ Now we all know that this would not be the case for much tougher equations to begin with, especially non-linear equations. What I want to find out is, is there a systematic manner of sifting out the other solutions that are more non-trivial without a great deal of work. Maybe like some common trick or something. It is sort of like for ODE's of certain type, we can expect an exponential form of a solution.","I am wondering, is there a general method one uses to seek methods of solutions without much strenuous work and calculations, and possibly a way to get non-trivial solutions for PDE's. A simple counter example would be: Suppose we are given: $\dfrac{\partial u}{\partial x}-2u = 0$ And are asked to find solutions that would satisfy this equation. Well a sort of trivial solution would be to say, suppose we let $u=e^{2x}$. This would imply from the equation that, $$2e^{2x}-2e^{2x}=0.$$ Now we all know that this would not be the case for much tougher equations to begin with, especially non-linear equations. What I want to find out is, is there a systematic manner of sifting out the other solutions that are more non-trivial without a great deal of work. Maybe like some common trick or something. It is sort of like for ODE's of certain type, we can expect an exponential form of a solution.",,"['ordinary-differential-equations', 'partial-differential-equations']"
57,Solving a differential equation by power series.,Solving a differential equation by power series.,,"Here's a question I'm struggling with and I'd like help on it. Please, this is not a homework problem. I want to find the power series solutions about the origin of two linearly independent solutions of $$w''-zw=0.$$ Also, how do I show that these solutions are analytic? thanks.","Here's a question I'm struggling with and I'd like help on it. Please, this is not a homework problem. I want to find the power series solutions about the origin of two linearly independent solutions of $$w''-zw=0.$$ Also, how do I show that these solutions are analytic? thanks.",,"['complex-analysis', 'ordinary-differential-equations']"
58,Periodic Solutions for a System,Periodic Solutions for a System,,"I'm currently brushing up on my ODE theory by reading through some texts. I was told that the system $$x'_1=x_2\hspace{5mm}x'_2=-x_1+(1-x_1^2-x_2^2)x_2\hspace{5mm}\Big('=\frac{d}{dt}\Big)$$ has a rather interesting property: Apparently, all periodic solutions of this system are of the form $\varphi=(\varphi_1,\varphi_2)$, where $\varphi_1(t)=\sin(t+c)$, $\varphi_2(t)=\cos(t+c)$, and $c$ is an arbitrary constant. (This is excluding the trivial periodic solution $\varphi=0$ of course.) Is there an easy proof to see why this is true? It looks like a really interesting result that comes out of nowhere (or in my opinion at least), and I'm hoping that the reasoning behind it could help me understand periodic solutions better.","I'm currently brushing up on my ODE theory by reading through some texts. I was told that the system $$x'_1=x_2\hspace{5mm}x'_2=-x_1+(1-x_1^2-x_2^2)x_2\hspace{5mm}\Big('=\frac{d}{dt}\Big)$$ has a rather interesting property: Apparently, all periodic solutions of this system are of the form $\varphi=(\varphi_1,\varphi_2)$, where $\varphi_1(t)=\sin(t+c)$, $\varphi_2(t)=\cos(t+c)$, and $c$ is an arbitrary constant. (This is excluding the trivial periodic solution $\varphi=0$ of course.) Is there an easy proof to see why this is true? It looks like a really interesting result that comes out of nowhere (or in my opinion at least), and I'm hoping that the reasoning behind it could help me understand periodic solutions better.",,['ordinary-differential-equations']
59,IVP: error in initial conditions,IVP: error in initial conditions,,"For an IVP $y' = f(x,y)$, with initial condition $y(x_0) = \alpha$, can something be said about how large the error at $x_1$, $x_1 > x_0$, is going to be if instead we had started with the initial condition $y(x_0) = \alpha + h$ for a small $h$? Of course, I'm assuming both solutions guaranteed by the Existence-Uniqueness theorem extend to $x_1$.","For an IVP $y' = f(x,y)$, with initial condition $y(x_0) = \alpha$, can something be said about how large the error at $x_1$, $x_1 > x_0$, is going to be if instead we had started with the initial condition $y(x_0) = \alpha + h$ for a small $h$? Of course, I'm assuming both solutions guaranteed by the Existence-Uniqueness theorem extend to $x_1$.",,"['ordinary-differential-equations', 'numerical-methods']"
60,"ODE help. $u''(x) +(-2 -x^2 )u = 0 ,\quad 0<x<1,\quad u'(0)=u'(1) = 0$",ODE help.,"u''(x) +(-2 -x^2 )u = 0 ,\quad 0<x<1,\quad u'(0)=u'(1) = 0","$u''(x) +(-2 -x^2 )u = 0 ,\quad 0<x<1,\quad u'(0)=u'(1) = 0.$ I'm not sure where to even start. What method?","$u''(x) +(-2 -x^2 )u = 0 ,\quad 0<x<1,\quad u'(0)=u'(1) = 0.$ I'm not sure where to even start. What method?",,['ordinary-differential-equations']
61,Solution for Logistic Growth Equation with Linearly Growing Carrying Capacity,Solution for Logistic Growth Equation with Linearly Growing Carrying Capacity,,"Does someone have a solution for the following equation where the carrying capacity varies linearly with time (and only time): $$\frac{\mathrm dN}{\mathrm dt}= rN\left(1-\frac{N}{K}\right)$$ $K = m \cdot t + b$ where $m$ and $b$ are constant. I'm looking for a solution  that models population growth with a linearly increasing upper limit: I'm looking for an equation that is in the form of $N$ as a function of $t$ where $r$, $m$, and $b$ are constants. Sorry, the last math class I took was multivariate calculus in college, so I'm not even sure this is a valid question.","Does someone have a solution for the following equation where the carrying capacity varies linearly with time (and only time): $$\frac{\mathrm dN}{\mathrm dt}= rN\left(1-\frac{N}{K}\right)$$ $K = m \cdot t + b$ where $m$ and $b$ are constant. I'm looking for a solution  that models population growth with a linearly increasing upper limit: I'm looking for an equation that is in the form of $N$ as a function of $t$ where $r$, $m$, and $b$ are constants. Sorry, the last math class I took was multivariate calculus in college, so I'm not even sure this is a valid question.",,['ordinary-differential-equations']
62,Green's identities for bilaplacian,Green's identities for bilaplacian,,Derive the Green's identities in local and integral form for the bilaplacian. Thank you for any help.,Derive the Green's identities in local and integral form for the bilaplacian. Thank you for any help.,,['ordinary-differential-equations']
63,Differential operator acting on the eigenfunctions of a commuting operator,Differential operator acting on the eigenfunctions of a commuting operator,,"I am reading an article at the moment and there is one step that I am having trouble understanding. The article proves that if $P$ and $Q$ are commuting differential operators there is a non-trivial polynomial $f(s,t)$ such that $f(P,Q)=0$. In order to to this it considers the eigenvalue problem $Py=Ey$ for any number $E$. This problem has $n$ ($n$ being the order of $P$) linearly  independent solutions by basic existence and uniqueness theorem. These span a space, $V_E$. The article takes as a basis of this space the solutions $y_i$ with $\frac{d^j y_i}{dx^j}(0)=\delta_{ij}$. If $Q$ commutes with $P$ then $Q$ maps $V_E$ into itself. Now the article claims, and this is the step I do not understand, that the matrix elements will be polynomials in $E$. Anyone who can explain this will get my gratitude. Edit: The article I am reading is ""Methods of algebraic geometry in the theory of non-linear equations"" by Krichever in Russian Math Surveys 32, 1977 . The operators the article studies are ordinary differential operators acting on $C^\infty(\mathbb{R},\mathbb{C})$. Operators are assumed to have constant leading coefficient. (So $P=\sum_{i=0}^n a_i x^i$ where $a_n$ is a non-zero constant.) The theorem is Theorem 2.1 on page 9 of the pdf. It is specifically the second sentence of the proof that I have problems with. I know that there actually is such a polynomial as claimed in the article as I know algebraic proofs of this fact. I am however trying to understand the analytic proof given by Krichever.","I am reading an article at the moment and there is one step that I am having trouble understanding. The article proves that if $P$ and $Q$ are commuting differential operators there is a non-trivial polynomial $f(s,t)$ such that $f(P,Q)=0$. In order to to this it considers the eigenvalue problem $Py=Ey$ for any number $E$. This problem has $n$ ($n$ being the order of $P$) linearly  independent solutions by basic existence and uniqueness theorem. These span a space, $V_E$. The article takes as a basis of this space the solutions $y_i$ with $\frac{d^j y_i}{dx^j}(0)=\delta_{ij}$. If $Q$ commutes with $P$ then $Q$ maps $V_E$ into itself. Now the article claims, and this is the step I do not understand, that the matrix elements will be polynomials in $E$. Anyone who can explain this will get my gratitude. Edit: The article I am reading is ""Methods of algebraic geometry in the theory of non-linear equations"" by Krichever in Russian Math Surveys 32, 1977 . The operators the article studies are ordinary differential operators acting on $C^\infty(\mathbb{R},\mathbb{C})$. Operators are assumed to have constant leading coefficient. (So $P=\sum_{i=0}^n a_i x^i$ where $a_n$ is a non-zero constant.) The theorem is Theorem 2.1 on page 9 of the pdf. It is specifically the second sentence of the proof that I have problems with. I know that there actually is such a polynomial as claimed in the article as I know algebraic proofs of this fact. I am however trying to understand the analytic proof given by Krichever.",,['ordinary-differential-equations']
64,Simple Differential Solution,Simple Differential Solution,,"Doing this question for revision Find solutions to: $\ x(dy/dx)=x^2e^{-x} + y$ satisfying $\ y(1) = 0$ I've divided through by$\ x$ and rearranged to get $\ (dy/dx)-y/x=xe^{-x}$ Then I used $\ -1/x$ as an integrating factor getting $\ e^{∫-1/x}= 1/x$ which gives me $\ y/x= ∫e^{-x}$ $\ y/x = -e^{-x} + c$ $\ y = -xe^{-x} + cx$ Plugging in initial values I get $\ 0=-e^{-1} + c$ and thus $\ c=e^{-1} $ so Finally I have: $\ y = -xe^{-x} + xe^{-1}$ Is my working correct, I have no answers for the paper I'm getting this question from.","Doing this question for revision Find solutions to: $\ x(dy/dx)=x^2e^{-x} + y$ satisfying $\ y(1) = 0$ I've divided through by$\ x$ and rearranged to get $\ (dy/dx)-y/x=xe^{-x}$ Then I used $\ -1/x$ as an integrating factor getting $\ e^{∫-1/x}= 1/x$ which gives me $\ y/x= ∫e^{-x}$ $\ y/x = -e^{-x} + c$ $\ y = -xe^{-x} + cx$ Plugging in initial values I get $\ 0=-e^{-1} + c$ and thus $\ c=e^{-1} $ so Finally I have: $\ y = -xe^{-x} + xe^{-1}$ Is my working correct, I have no answers for the paper I'm getting this question from.",,['ordinary-differential-equations']
65,The Spiral Behavior of Second Order Equations,The Spiral Behavior of Second Order Equations,,"$$\frac{\mathrm d^2y}{\mathrm dt^2} + p\frac{\mathrm dy}{\mathrm dt} + qy = 0$$ If the eigenvalues are complex, what conditions on $p$ and $q$ guarantee that solutions spiral around the origin in a clockwise direction ? I found the eigenvalues to be: $\lambda = -\frac{p}{2} \pm \frac{\sqrt{p^2 - 4q}}{2}$ and not sure how we can answer this. All I note that if $p > 0$ and $p^2 - 4q < 0$, then we have a spiral sink. How does it help me if I can get conditions for spiral sink or source and match their directions (counterclockwise or clockwise) when the problem indicates we are able to do this without the use of technology sketching a phase portrait?","$$\frac{\mathrm d^2y}{\mathrm dt^2} + p\frac{\mathrm dy}{\mathrm dt} + qy = 0$$ If the eigenvalues are complex, what conditions on $p$ and $q$ guarantee that solutions spiral around the origin in a clockwise direction ? I found the eigenvalues to be: $\lambda = -\frac{p}{2} \pm \frac{\sqrt{p^2 - 4q}}{2}$ and not sure how we can answer this. All I note that if $p > 0$ and $p^2 - 4q < 0$, then we have a spiral sink. How does it help me if I can get conditions for spiral sink or source and match their directions (counterclockwise or clockwise) when the problem indicates we are able to do this without the use of technology sketching a phase portrait?",,['ordinary-differential-equations']
66,"birth-death models, equilibria and stability","birth-death models, equilibria and stability",,"For the following two differential equations which model birth rate in relation to population density $\dfrac {dN}{dt} = bN^2 - aN$, $\dfrac {dN}{dt} = bN^2 \left(1 - \dfrac N K \right) - aN$ where $a$ and $b$ are positive constants I need to locate the equilibria of $N$, determine stability, and sketch solution curves for ""various starting values"" $N_0$. I have no idea how to go about this.","For the following two differential equations which model birth rate in relation to population density $\dfrac {dN}{dt} = bN^2 - aN$, $\dfrac {dN}{dt} = bN^2 \left(1 - \dfrac N K \right) - aN$ where $a$ and $b$ are positive constants I need to locate the equilibria of $N$, determine stability, and sketch solution curves for ""various starting values"" $N_0$. I have no idea how to go about this.",,['ordinary-differential-equations']
67,How do I interpret an integral where the lower limit of integration is missing?,How do I interpret an integral where the lower limit of integration is missing?,,"I'm reading An Introduction to Ordinary Differential Equations by Agarwal and O'Regan.  On page 28, I have the expression $$y\left(x\right)=c\exp\left(-\int^x{p\left(t\right)\, dt} \right)$$ which is equation 5.4. My problem is the missing lower limit of integration.  I don't understand what the notation means.  The context of the equation is solving the homogenous equation $$y^\prime+p\left(x\right)y=0.$$ This leads to $$\frac{y^\prime}{y}+p\left(x\right)=0.$$ The text says that by integrating both sides, we get the expression that is puzzling me.  Any advice on how to interpret this notation would be appreciated.","I'm reading An Introduction to Ordinary Differential Equations by Agarwal and O'Regan.  On page 28, I have the expression $$y\left(x\right)=c\exp\left(-\int^x{p\left(t\right)\, dt} \right)$$ which is equation 5.4. My problem is the missing lower limit of integration.  I don't understand what the notation means.  The context of the equation is solving the homogenous equation $$y^\prime+p\left(x\right)y=0.$$ This leads to $$\frac{y^\prime}{y}+p\left(x\right)=0.$$ The text says that by integrating both sides, we get the expression that is puzzling me.  Any advice on how to interpret this notation would be appreciated.",,"['real-analysis', 'ordinary-differential-equations']"
68,solving an initial value ODE with a twist,solving an initial value ODE with a twist,,"The goal of this problem is to solve the initial value problem $$y' = -f(x)y;\quad  y(0) = 1;$$ where $$f(x)= \begin{cases} 1,&\text{if }x\leq 2\\ \frac{3}{2},& \text {if } x>2. \end{cases} $$ Since $f$ is discontinuous, it is necessary to solve the above ODE separately in each of the intervals where $f$ is continuous. (a) Determine the intervals where $f$ is continuous. (b) Solve the equation in each of these intervals. Note that each of the solutions obtained will have a different constant of integration. (c) Match the solutions at the points where $f$ is discontinuous, in order to make the solution $y$ continuous on $\Bbb R$. Note that in this case it is impossible to make $y'$ continuous at the points where $f$ is discontinuous. I just don't understand what part (c) is asking, if anyone could help me with the concepts I would really appreciate it.","The goal of this problem is to solve the initial value problem $$y' = -f(x)y;\quad  y(0) = 1;$$ where $$f(x)= \begin{cases} 1,&\text{if }x\leq 2\\ \frac{3}{2},& \text {if } x>2. \end{cases} $$ Since $f$ is discontinuous, it is necessary to solve the above ODE separately in each of the intervals where $f$ is continuous. (a) Determine the intervals where $f$ is continuous. (b) Solve the equation in each of these intervals. Note that each of the solutions obtained will have a different constant of integration. (c) Match the solutions at the points where $f$ is discontinuous, in order to make the solution $y$ continuous on $\Bbb R$. Note that in this case it is impossible to make $y'$ continuous at the points where $f$ is discontinuous. I just don't understand what part (c) is asking, if anyone could help me with the concepts I would really appreciate it.",,[]
69,Using differential equations to graph velocity over time of a falling object subject to wind resistance,Using differential equations to graph velocity over time of a falling object subject to wind resistance,,"Wind resistance -- upwards acceleration, typically varies either linearly or quadratically by the current velocity. There is a constant downward acceleration due to gravity. How can we model the velocity over time of a falling object, subject only to wind resistance and downwards gravity? I don't have much experience with differential equations, but I do know that this answer necessarily involves it, so could you possibly explain every step? Thank you.","Wind resistance -- upwards acceleration, typically varies either linearly or quadratically by the current velocity. There is a constant downward acceleration due to gravity. How can we model the velocity over time of a falling object, subject only to wind resistance and downwards gravity? I don't have much experience with differential equations, but I do know that this answer necessarily involves it, so could you possibly explain every step? Thank you.",,"['calculus', 'physics', 'ordinary-differential-equations']"
70,How to solve a spring-mass differential equation with impulse at $t=\pi$ without Laplace transform?,How to solve a spring-mass differential equation with impulse at  without Laplace transform?,t=\pi,"Consider the differential equation $$x''+x=F_0\delta(t-\pi)$$ with $x(0)=0$ and $x'(0)=1$ . I can solve this using Laplace transform. The result I find is $$x(t)=\begin{cases}\sin{(t)}\ \ \ \ \ &t\leq \pi \\ \sin{(t)}(1-F_0)\ \ \ \ \ &t\geq \pi\end{cases}$$ My question is about how to solve this differential equation without Laplace transform. Let's think about the system as an undamped spring-mass system. The input to the system is an impulse of magnitude $F_0$ at time $\pi$ . For $t<\pi$ we have a simple homogeneous equation and the solution is $$x(t)=\sin{(t)}$$ Note that $x(\pi)=0$ and $x'(\pi)=-1$ . At $t=\pi$ the impulse changes the velocity to $F_0$ . To solve the system for $t\geq\pi$ I once again solved $$x''+x=0$$ with the initial conditions $$x(\pi)=0$$ $$x'(\pi)=F_0$$ Since the solution is $$x(t)=c_1\cos{(t)}+c_2\sin{(t)}$$ and $$x'(t)=-c_1\sin{(t)}+c_2\cos{(t)}$$ the constants come out to $c_1=0$ and $c_2=-F_0$ . The solution is thus $$x(t)=-F_0\sin{(t)}$$ Why do I have to, apparently, add in the solution obtained for $t<\pi$ ? I think the mistake I am making is related to the initial conditions when I solved for $t>\pi$ but I am not sure.","Consider the differential equation with and . I can solve this using Laplace transform. The result I find is My question is about how to solve this differential equation without Laplace transform. Let's think about the system as an undamped spring-mass system. The input to the system is an impulse of magnitude at time . For we have a simple homogeneous equation and the solution is Note that and . At the impulse changes the velocity to . To solve the system for I once again solved with the initial conditions Since the solution is and the constants come out to and . The solution is thus Why do I have to, apparently, add in the solution obtained for ? I think the mistake I am making is related to the initial conditions when I solved for but I am not sure.",x''+x=F_0\delta(t-\pi) x(0)=0 x'(0)=1 x(t)=\begin{cases}\sin{(t)}\ \ \ \ \ &t\leq \pi \\ \sin{(t)}(1-F_0)\ \ \ \ \ &t\geq \pi\end{cases} F_0 \pi t<\pi x(t)=\sin{(t)} x(\pi)=0 x'(\pi)=-1 t=\pi F_0 t\geq\pi x''+x=0 x(\pi)=0 x'(\pi)=F_0 x(t)=c_1\cos{(t)}+c_2\sin{(t)} x'(t)=-c_1\sin{(t)}+c_2\cos{(t)} c_1=0 c_2=-F_0 x(t)=-F_0\sin{(t)} t<\pi t>\pi,['ordinary-differential-equations']
71,Solving $y''(x)-2xy'(x)+y(x) = 0$ using power series,Solving  using power series,y''(x)-2xy'(x)+y(x) = 0,"I'm solving $y''(x) -2xy'(x) +y(x) = 0$ using the power series ansatz $y(x) = \sum_{n=0}^{\infty}a_n x^n$ . Plugging in I get: \begin{equation} \bigg(\sum_{n=0}^{\infty}n(n-1)a_nx^{n-2}\bigg)-2x\bigg(\sum_{n=0}^{\infty}na_nx^{n-1}\bigg)+\sum_{n=0}^{\infty}a_n x^n = 0  \end{equation} \begin{equation} \sum_{n=0}^{\infty}x^n \big((n+2)(n+1)a_{n+2} -2na_n+a_n\big) = 0  \end{equation} \begin{equation} (n+2)(n+1)a_{n+2} -2na_n+a_n = 0  \end{equation} \begin{equation} a_{n+2} = \frac{2n-1}{(n+2)(n+1)}a_n \end{equation} Now we set $a_0$ and $a_1$ as parameters, to observe a pattern: \begin{align} n\qquad & \\\\ 0\qquad & a_2 = \frac{-1}{2 \cdot 1} a_0 &= \frac{a_0}{2!}\prod_{l=0}^{0}(4l-1) \\\\ 1\qquad & a_3 = \frac{1}{3\cdot 2} a_1 &= \frac{a_1}{3!}\prod_{l=0}^{0}(4l+1) \\\\ 2\qquad & a_4 = \frac{3}{4\cdot 3} a_2 = \frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{4!} \prod_{l=0}^{1}(4l-1) \\\\ 3\qquad & a_5 = \frac{5}{5\cdot 4} a_3 = \frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{5!}\prod_{l=0}^{1}(4l+1) \\\\ 4 \qquad & a_6 = \frac{7}{6\cdot 5} a_4 = \frac{7}{6\cdot 5}\frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{6!} \prod_{l=0}^{2}(4l-1)\\\\ 5 \qquad & a_7 = \frac{9}{7\cdot 6} a_5 = \frac{9}{7\cdot 6}\frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{7!}\prod_{l=0}^{2}(4l+1) \\\\ 6 \qquad & a_8 = \frac{11}{8\cdot 7} a_6 = \frac{11}{8\cdot 7}\frac{7}{6\cdot 5}\frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{8!} \prod_{l=0}^{3}(4l-1)\\\\ 7 \qquad & a_9 = \frac{13}{9\cdot 8} a_7 = \frac{13}{9 \cdot 8}\frac{9}{7\cdot 6}\frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{9!}\prod_{l=0}^{3}(4l+1) \\\\ \cdots \end{align} So I now only need to plug these into $y(x)$ . For that I split the sum into odd and even parts: \begin{align} y(x) &= \sum_{n=0}^{\infty}a_n x^n \\\\&= \bigg( \sum_{k=0}^{\infty}a_{2k} x^{2k} \bigg) + \bigg( \sum_{k=0}^{\infty}a_{2k+1} x^{2k+1} \bigg) \\\\&= \bigg(a_0 + \sum_{k=1}^{\infty}a_{2k} x^{2k} \bigg) + \bigg(a_1x + \sum_{k=1}^{\infty}a_{2k+1} x^{2k+1} \bigg) \\\\&= \bigg(a_0 + \sum_{k=1}^{\infty} x^{2k} \frac{a_0}{(k+2)!}\prod_{l=0}^{k}(4l-1)  \bigg) + \bigg(a_1x + \sum_{k=1}^{\infty} x^{2k+1} \frac{a_1}{(k+2)!} \prod_{l=0}^{k}(4l+1) \bigg) \\\\&= a_0 \bigg(1 + \sum_{k=1}^{\infty} \frac{x^{2k}}{(k+2)!}\prod_{l=0}^{k}(4l-1)  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty} \frac{x^{2k+1}}{(k+2)!} \prod_{l=0}^{k}(4l+1) \bigg)  \end{align} But now I tried to check the solution by plugging it back in the equation. But first I made following abbreviations: \begin{equation} \alpha_k = \frac{1}{(2+k)!}\prod_{l=0}^{k}(4l-1) \qquad\qquad \beta_k = \frac{1}{(2+k)!}\prod_{l=0}^{k}(4l+1) \end{equation} So we now have: \begin{align} y(x) &= a_0 \bigg(1 + \sum_{k=1}^{\infty} x^{2k} \alpha_k  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty}x^{2k+1} \beta_k \bigg) \\\\ y'(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 2k x^{2k-1} \alpha_k  \bigg) + a_1 \bigg(1 + \sum_{k=1}^{\infty}(2k+1)x^{2k} \beta_k \bigg) \\\\ y''(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 2k(2k-1) x^{2k-2} \alpha_k  \bigg) + a_1 \bigg(\sum_{k=1}^{\infty}(2k+1)2kx^{2k-1} \beta_k \bigg)  \end{align} I started with following term: \begin{align} 2xy'(x)-y(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 4kx^{2k} \alpha_k  \bigg) + a_1 \bigg(2x + \sum_{k=1}^{\infty}2(2k+1)x^{2k+1} \beta_k \bigg) \\\\ & \quad- a_0 \bigg(1+\sum_{k=1}^{\infty} x^{2k} \alpha_k  \bigg) - a_1 \bigg(x + \sum_{k=1}^{\infty}x^{2k+1} \beta_k \bigg) \\\\ &= a_0 \bigg(-1 + \sum_{k=1}^{\infty} x^{2k}(4k-1) \alpha_k  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty}(4k+1)x^{2k+1}\beta_k\bigg)  \end{align} But this obviously doesn't equal $y''(x)$ . Where did I go wrong?","I'm solving using the power series ansatz . Plugging in I get: Now we set and as parameters, to observe a pattern: So I now only need to plug these into . For that I split the sum into odd and even parts: But now I tried to check the solution by plugging it back in the equation. But first I made following abbreviations: So we now have: I started with following term: But this obviously doesn't equal . Where did I go wrong?","y''(x) -2xy'(x) +y(x) = 0 y(x) = \sum_{n=0}^{\infty}a_n x^n \begin{equation}
\bigg(\sum_{n=0}^{\infty}n(n-1)a_nx^{n-2}\bigg)-2x\bigg(\sum_{n=0}^{\infty}na_nx^{n-1}\bigg)+\sum_{n=0}^{\infty}a_n x^n = 0 
\end{equation} \begin{equation}
\sum_{n=0}^{\infty}x^n \big((n+2)(n+1)a_{n+2} -2na_n+a_n\big) = 0 
\end{equation} \begin{equation}
(n+2)(n+1)a_{n+2} -2na_n+a_n = 0 
\end{equation} \begin{equation}
a_{n+2} = \frac{2n-1}{(n+2)(n+1)}a_n
\end{equation} a_0 a_1 \begin{align}
n\qquad & \\\\
0\qquad & a_2 = \frac{-1}{2 \cdot 1} a_0 &= \frac{a_0}{2!}\prod_{l=0}^{0}(4l-1) \\\\
1\qquad & a_3 = \frac{1}{3\cdot 2} a_1 &= \frac{a_1}{3!}\prod_{l=0}^{0}(4l+1) \\\\
2\qquad & a_4 = \frac{3}{4\cdot 3} a_2 = \frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{4!} \prod_{l=0}^{1}(4l-1) \\\\
3\qquad & a_5 = \frac{5}{5\cdot 4} a_3 = \frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{5!}\prod_{l=0}^{1}(4l+1) \\\\
4 \qquad & a_6 = \frac{7}{6\cdot 5} a_4 = \frac{7}{6\cdot 5}\frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{6!} \prod_{l=0}^{2}(4l-1)\\\\
5 \qquad & a_7 = \frac{9}{7\cdot 6} a_5 = \frac{9}{7\cdot 6}\frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{7!}\prod_{l=0}^{2}(4l+1) \\\\
6 \qquad & a_8 = \frac{11}{8\cdot 7} a_6 = \frac{11}{8\cdot 7}\frac{7}{6\cdot 5}\frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{8!} \prod_{l=0}^{3}(4l-1)\\\\
7 \qquad & a_9 = \frac{13}{9\cdot 8} a_7 = \frac{13}{9 \cdot 8}\frac{9}{7\cdot 6}\frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{9!}\prod_{l=0}^{3}(4l+1) \\\\
\cdots
\end{align} y(x) \begin{align}
y(x) &= \sum_{n=0}^{\infty}a_n x^n \\\\&= \bigg( \sum_{k=0}^{\infty}a_{2k} x^{2k} \bigg) + \bigg( \sum_{k=0}^{\infty}a_{2k+1} x^{2k+1} \bigg) \\\\&= \bigg(a_0 + \sum_{k=1}^{\infty}a_{2k} x^{2k} \bigg) + \bigg(a_1x + \sum_{k=1}^{\infty}a_{2k+1} x^{2k+1} \bigg) \\\\&= \bigg(a_0 + \sum_{k=1}^{\infty} x^{2k} \frac{a_0}{(k+2)!}\prod_{l=0}^{k}(4l-1)  \bigg) + \bigg(a_1x + \sum_{k=1}^{\infty} x^{2k+1} \frac{a_1}{(k+2)!} \prod_{l=0}^{k}(4l+1) \bigg) \\\\&= a_0 \bigg(1 + \sum_{k=1}^{\infty} \frac{x^{2k}}{(k+2)!}\prod_{l=0}^{k}(4l-1)  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty} \frac{x^{2k+1}}{(k+2)!} \prod_{l=0}^{k}(4l+1) \bigg) 
\end{align} \begin{equation}
\alpha_k = \frac{1}{(2+k)!}\prod_{l=0}^{k}(4l-1) \qquad\qquad \beta_k = \frac{1}{(2+k)!}\prod_{l=0}^{k}(4l+1)
\end{equation} \begin{align}
y(x) &= a_0 \bigg(1 + \sum_{k=1}^{\infty} x^{2k} \alpha_k  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty}x^{2k+1} \beta_k \bigg) \\\\
y'(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 2k x^{2k-1} \alpha_k  \bigg) + a_1 \bigg(1 + \sum_{k=1}^{\infty}(2k+1)x^{2k} \beta_k \bigg) \\\\
y''(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 2k(2k-1) x^{2k-2} \alpha_k  \bigg) + a_1 \bigg(\sum_{k=1}^{\infty}(2k+1)2kx^{2k-1} \beta_k \bigg) 
\end{align} \begin{align}
2xy'(x)-y(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 4kx^{2k} \alpha_k  \bigg) + a_1 \bigg(2x + \sum_{k=1}^{\infty}2(2k+1)x^{2k+1} \beta_k \bigg) \\\\ & \quad- a_0 \bigg(1+\sum_{k=1}^{\infty} x^{2k} \alpha_k  \bigg) - a_1 \bigg(x + \sum_{k=1}^{\infty}x^{2k+1} \beta_k \bigg) \\\\ &= a_0 \bigg(-1 + \sum_{k=1}^{\infty} x^{2k}(4k-1) \alpha_k  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty}(4k+1)x^{2k+1}\beta_k\bigg) 
\end{align} y''(x)","['ordinary-differential-equations', 'power-series']"
72,Closed Formula for maximal interval of existence of autonomous ODE,Closed Formula for maximal interval of existence of autonomous ODE,,"I just stumbled across the following theorem ( https://jordanbell.info/LaTeX/mathematics/domainODE/domainODE.pdf ) and was wondering about its validity and proof, as the author does not provide a reference. And it does seem like a very strong and useful result. Assume we have an autonomous ODE: \begin{equation*} \frac{dx}{dt} = f(x),\: x(0) = x_0 \end{equation*} for a continuous, positive function $f:[x_0, \infty) \to [0, \infty)$ . (I assume that maybe local Lipschitz continuity was forgotten as an assumption here). Then, the endpoint $T$ maximal interval of existence $(-\infty, T)$ of the solution $x(t)$ is given by the formula: \begin{equation*} T:=\int_{x_0}^{\infty} \frac{dx}{f(x)} \end{equation*} So I have three questions: Is this result correct ? Can it be extended to higher dimensional cases, i.e. $f: \mathbb{R}^n \to \mathbb{R}^n$ ? I would guess that the result holds analogously for strictly negative $f$ . Now for example, if $f$ is positive for $x \in (0, x_1)$ and negative for $x \in (x_1, x_2)$ , is it possible to extend a (maybe weaker) form of this result to functions $f$ switching signs ?","I just stumbled across the following theorem ( https://jordanbell.info/LaTeX/mathematics/domainODE/domainODE.pdf ) and was wondering about its validity and proof, as the author does not provide a reference. And it does seem like a very strong and useful result. Assume we have an autonomous ODE: for a continuous, positive function . (I assume that maybe local Lipschitz continuity was forgotten as an assumption here). Then, the endpoint maximal interval of existence of the solution is given by the formula: So I have three questions: Is this result correct ? Can it be extended to higher dimensional cases, i.e. ? I would guess that the result holds analogously for strictly negative . Now for example, if is positive for and negative for , is it possible to extend a (maybe weaker) form of this result to functions switching signs ?","\begin{equation*}
\frac{dx}{dt} = f(x),\: x(0) = x_0
\end{equation*} f:[x_0, \infty) \to [0, \infty) T (-\infty, T) x(t) \begin{equation*}
T:=\int_{x_0}^{\infty} \frac{dx}{f(x)}
\end{equation*} f: \mathbb{R}^n \to \mathbb{R}^n f f x \in (0, x_1) x \in (x_1, x_2) f","['ordinary-differential-equations', 'analysis']"
73,Construction of general solutions from a particular solution. Ermakov equation.,Construction of general solutions from a particular solution. Ermakov equation.,,"My question is in particular about the Ermakov equation, but in general it is about the construction of general solutions to ordinary differential equations. This is the first time I see such construction, so apologies if it is too trivial. The Ermakov equation $$ \frac{d^2y}{dx^2} + f(x) y = a y^{-3}, $$ where f(x) is a function of the independent variable $x$ and $a$ is a constant, has as a general solution $$ C_1 y^2 = a w^2 + w^2\left(C_2 + C_1\int\frac{dx}{w^2} \right)^2, $$ for $C_1,C_2$ constants. The function $w=w(x)$ is taken as any solution of the homogeneous equation $$ \frac{d^2w}{dx^2} + f(x) w = 0. $$ The question: Why any solution $w(x)$ can yield a general solution for $y$ ? I am not sure this is obvious, or perhaps I misunderstood something. For instance, I can find, say, $w$ real and thus $y$ will be real. However, it may as well be the case that the most general solution for $w$ is complex, and then my previous real solution would generate a ""restricted"" solution for $y$ .","My question is in particular about the Ermakov equation, but in general it is about the construction of general solutions to ordinary differential equations. This is the first time I see such construction, so apologies if it is too trivial. The Ermakov equation where f(x) is a function of the independent variable and is a constant, has as a general solution for constants. The function is taken as any solution of the homogeneous equation The question: Why any solution can yield a general solution for ? I am not sure this is obvious, or perhaps I misunderstood something. For instance, I can find, say, real and thus will be real. However, it may as well be the case that the most general solution for is complex, and then my previous real solution would generate a ""restricted"" solution for .","
\frac{d^2y}{dx^2} + f(x) y = a y^{-3},
 x a 
C_1 y^2 = a w^2 + w^2\left(C_2 + C_1\int\frac{dx}{w^2} \right)^2,
 C_1,C_2 w=w(x) 
\frac{d^2w}{dx^2} + f(x) w = 0.
 w(x) y w y w y",['ordinary-differential-equations']
74,Laplace Transform Differential equations,Laplace Transform Differential equations,,"Given Equation is $(D^4-a^4)y = x^4$ Solving the auxiliary equation gives $D = a,-a,+ai,-ai$ So the homogeneous solution is $y_h = c_1e^{ax} + {c_2e^{-ax}} + c_3sin(ax) + c_4 cos (ax) $ Taking the Laplace transform $ Y(s) = \frac{24}{s^5(s^4-a^4)}$ Using the incomplete partial fraction method is a lengthy process as $s^5$ is in the denominator. Finding the particular integral using inverse differential operator seems to be the easier method. Is there a quicker way to solve this equation using the Laplace transform method?? If we use convolution, since we already know the homogeneous solution, does that help us in some way??","Given Equation is Solving the auxiliary equation gives So the homogeneous solution is Taking the Laplace transform Using the incomplete partial fraction method is a lengthy process as is in the denominator. Finding the particular integral using inverse differential operator seems to be the easier method. Is there a quicker way to solve this equation using the Laplace transform method?? If we use convolution, since we already know the homogeneous solution, does that help us in some way??","(D^4-a^4)y = x^4 D = a,-a,+ai,-ai y_h = c_1e^{ax} + {c_2e^{-ax}} + c_3sin(ax) + c_4 cos (ax)   Y(s) = \frac{24}{s^5(s^4-a^4)} s^5","['ordinary-differential-equations', 'laplace-transform']"
75,Applying linearity of a differential operator to solve ODE,Applying linearity of a differential operator to solve ODE,,"I'm attempting to solve the following ODE: $$xy''+y'-y=0$$ According to Frobenius' theorem, there exists a solution to this ODE in the form of a series given by the linear combination of two solutions such that: $y=C_1y_1+C_2y_2$ , where: $$y_1=\displaystyle \sum_{n=0}^\infty a_nx^n$$ $$y_2=y_1log|x|+\displaystyle \sum_{n=0}^\infty b_nx^n$$ My issue is not how to solve the ODE, I've already managed to do it, but rather how to find the second solution, $y_2$ , in a more elegant way instead of deriving the expression twice, substituting the series in the equation and identifying a pattern for the general term of the sum. In order to do this, following my professor's advice, I have defined the differential operator: $$L=x\frac{d^2}{dx^2}+\frac{d}{dx}-1$$ where, of course, $L[y_1]=L[y_2]=0$ since they are both solutions to my equation. Applying this operator on the second solution, I get: $$L[y_2]=L[y_1]log|x|+y_1L[log|x|]+L[u_2]=0$$ Since the first term is null, then i end up with: $$y_1L[log|x|]+L[u_2]=0$$ and from here I get: $$-y_1+L[u_2]=0$$ although, according to the calculations I did earlier, before using operators, what I should really obtain is: $$2y_1'+L[u_2]=0$$ Why is this? I'm assuming it is because I'm considering my operator is linear when it really isn't, although I don't quite understand why it wouldn't be, since it fullfils the basic properties of linearity.","I'm attempting to solve the following ODE: According to Frobenius' theorem, there exists a solution to this ODE in the form of a series given by the linear combination of two solutions such that: , where: My issue is not how to solve the ODE, I've already managed to do it, but rather how to find the second solution, , in a more elegant way instead of deriving the expression twice, substituting the series in the equation and identifying a pattern for the general term of the sum. In order to do this, following my professor's advice, I have defined the differential operator: where, of course, since they are both solutions to my equation. Applying this operator on the second solution, I get: Since the first term is null, then i end up with: and from here I get: although, according to the calculations I did earlier, before using operators, what I should really obtain is: Why is this? I'm assuming it is because I'm considering my operator is linear when it really isn't, although I don't quite understand why it wouldn't be, since it fullfils the basic properties of linearity.",xy''+y'-y=0 y=C_1y_1+C_2y_2 y_1=\displaystyle \sum_{n=0}^\infty a_nx^n y_2=y_1log|x|+\displaystyle \sum_{n=0}^\infty b_nx^n y_2 L=x\frac{d^2}{dx^2}+\frac{d}{dx}-1 L[y_1]=L[y_2]=0 L[y_2]=L[y_1]log|x|+y_1L[log|x|]+L[u_2]=0 y_1L[log|x|]+L[u_2]=0 -y_1+L[u_2]=0 2y_1'+L[u_2]=0,"['sequences-and-series', 'ordinary-differential-equations', 'operator-theory', 'taylor-expansion']"
76,Writing $\exp{tA}$ as finite sum,Writing  as finite sum,\exp{tA},"Let $A \in \mathcal{M}_{n\times n} (\mathbb{R})$ such that $A^2=\alpha A$ for some $\alpha \neq 0$ . Under this assumption, we have by induction that $A^{n}=\alpha^{n-1}A$ ; $n \geq2$ ; then: $$\exp(tA)=I+ tA+\frac{t^2A^2}{2!}+ \dots \ =I+At+\frac{A\alpha t^2}{2!} + \frac{A\alpha^2 t^3}{3!} + \dots = \\ = I+ \frac{1}{\alpha}A(1+ \alpha t+ \frac{\alpha^2 t^2}{2!} +\dots)-  \frac{A}{\alpha}=I+\frac{1}{\alpha}A\exp({\alpha t})- \frac{A}{\alpha}=I+A(\frac{\exp(\alpha t)-1}{\alpha}) $$ Are these steps correct?","Let such that for some . Under this assumption, we have by induction that ; ; then: Are these steps correct?","A \in \mathcal{M}_{n\times n} (\mathbb{R}) A^2=\alpha A \alpha \neq 0 A^{n}=\alpha^{n-1}A n \geq2 \exp(tA)=I+ tA+\frac{t^2A^2}{2!}+ \dots \ =I+At+\frac{A\alpha t^2}{2!} + \frac{A\alpha^2 t^3}{3!} + \dots = \\ = I+ \frac{1}{\alpha}A(1+ \alpha t+ \frac{\alpha^2 t^2}{2!} +\dots)-  \frac{A}{\alpha}=I+\frac{1}{\alpha}A\exp({\alpha t})- \frac{A}{\alpha}=I+A(\frac{\exp(\alpha t)-1}{\alpha})
","['ordinary-differential-equations', 'matrix-exponential']"
77,"How to solve the eigenvalue problem $y'' + \lambda y=0$, $y'(-L) = y'(L)=0$","How to solve the eigenvalue problem ,",y'' + \lambda y=0 y'(-L) = y'(L)=0,"How do I solve the eigenvalue problem $y'' + \lambda y=0$ , $y'(-L) = y'(L)=0$ ? My attempt: Case 1: $\lambda=0$ . If $\lambda=0$ , then the BVP becomes $y''(x)=0$ , $y'(-L)=y'(L)=0$ . The general solution to this differential equation is $y(x)=Ax+B$ . Taking the derivative gives $y'(x)=A$ , and the boundary condition implies that $A=0$ , but $B$ can be arbitrary. This means that $\mu_0=0$ must be an eigenvalue and the function $$y_0(x)=B,\quad B\text{ is a constant}$$ is its corresponding eigenfunction. Case 2: $\lambda<0$ . Let $\lambda = -p^2<0$ . The BVP becomes $y''(x)-p^2y(x)=0$ , $y'(-L)=y'(L)=0$ . The solution to this DE is $y(x)=A\cosh(px)+B\sinh(px)$ . Taking the derivative gives $y'(x)=Ap\sinh(px)+Bp\cosh(px)$ . Plugging in $-L$ and $L$ gives \begin{align*}     A\cosh(pL) - B\sinh(pL) &= 0\\     A\cosh(pL) + B\sinh(pL) &= 0 \end{align*} Solving for $A$ and $B$ gives $A=B=0$ , which is a trivial solution. Case 3: $\lambda>0$ . Let $\lambda = p^2 > 0$ . The BVP becomes $y''(x)+p^2y(x)=0$ , $y'(-L)=y'(L)=0$ . The solution to this DE is $y(x)=A\cos(px)+B\sin(px)$ . Taking the derivative gives $y'(x)=-Ap\sin(px) + Bp\cos(px)$ , and applying the boundary condition, \begin{alignat}{3}     & y'(-a) && = Ap\sin(pL) + Bp\cos(pL) = 0 \quad&& \Longrightarrow\quad A\sin(pL) + B\cos(pL) = 0 \label{eq:X2}\\     & y'(a) && = -Ap\sin(pL) + Bp\cos(pL) = 0 \quad&& \Longrightarrow\quad A\sin(pL) - B\cos(pL) = 0 \label{eq:Y2} \end{alignat} Adding the two equations, we obtain $2A\sin(pL)=0$ . This implies that either $A=0$ or $\sin(pL)=0$ . Subtracting the two equations gives $2B\cos(pL)=0$ . This implies that either $B=0$ or $\cos(pL)=0$ . To summarize, we have $$A=0,\;\text{or}\;\sin(pL)=0,\quad\text{and}\quad B=0\;\text{or}\;\cos(pL)=0.$$ Together, they form four cases: $A=B=0$ , $A=\cos(pL)=0$ , and $B=\sin(pL)=0$ , and the final one gives $\cos(pL)=\sin(pL)=0$ , which is not possible. The second, and third case gives $$p_1 = \frac{n\pi}{L}\quad\text{and}\quad p_2 = \frac{(n-\frac{1}{2})\pi}{L},\quad n =1, 2, 3,\ldots,$$ respectively. Therefore, there are only two non-trivial families of solutions in this case: $$\cos\left(\frac{n\pi}{L}x\right)\quad\text{and}\quad\sin\left(\frac{(n-\frac{1}{2})\pi}{L}x\right),\quad n=1, 2, 3,\ldots$$ and thus the corresponding eigenfunctions are $$y_n(x)=a_n\cos\left(\sqrt{\lambda_{n, 1}}x\right)+b_n\sin\left(\sqrt{\lambda_{n, 2}}x\right)$$ where $$\lambda_{n, 1}=\left(\frac{n\pi}{L}\right)^2,\quad \lambda_{n, 2}=\left(\frac{(n-\frac{1}{2})\pi}{L}\right)^2$$ Question: Is my solution correct? Should there be two families of eigenvalues like case 3?","How do I solve the eigenvalue problem , ? My attempt: Case 1: . If , then the BVP becomes , . The general solution to this differential equation is . Taking the derivative gives , and the boundary condition implies that , but can be arbitrary. This means that must be an eigenvalue and the function is its corresponding eigenfunction. Case 2: . Let . The BVP becomes , . The solution to this DE is . Taking the derivative gives . Plugging in and gives Solving for and gives , which is a trivial solution. Case 3: . Let . The BVP becomes , . The solution to this DE is . Taking the derivative gives , and applying the boundary condition, Adding the two equations, we obtain . This implies that either or . Subtracting the two equations gives . This implies that either or . To summarize, we have Together, they form four cases: , , and , and the final one gives , which is not possible. The second, and third case gives respectively. Therefore, there are only two non-trivial families of solutions in this case: and thus the corresponding eigenfunctions are where Question: Is my solution correct? Should there be two families of eigenvalues like case 3?","y'' + \lambda y=0 y'(-L) = y'(L)=0 \lambda=0 \lambda=0 y''(x)=0 y'(-L)=y'(L)=0 y(x)=Ax+B y'(x)=A A=0 B \mu_0=0 y_0(x)=B,\quad B\text{ is a constant} \lambda<0 \lambda = -p^2<0 y''(x)-p^2y(x)=0 y'(-L)=y'(L)=0 y(x)=A\cosh(px)+B\sinh(px) y'(x)=Ap\sinh(px)+Bp\cosh(px) -L L \begin{align*}
    A\cosh(pL) - B\sinh(pL) &= 0\\
    A\cosh(pL) + B\sinh(pL) &= 0
\end{align*} A B A=B=0 \lambda>0 \lambda = p^2 > 0 y''(x)+p^2y(x)=0 y'(-L)=y'(L)=0 y(x)=A\cos(px)+B\sin(px) y'(x)=-Ap\sin(px) + Bp\cos(px) \begin{alignat}{3}
    & y'(-a) && = Ap\sin(pL) + Bp\cos(pL) = 0 \quad&& \Longrightarrow\quad A\sin(pL) + B\cos(pL) = 0 \label{eq:X2}\\
    & y'(a) && = -Ap\sin(pL) + Bp\cos(pL) = 0 \quad&& \Longrightarrow\quad A\sin(pL) - B\cos(pL) = 0 \label{eq:Y2}
\end{alignat} 2A\sin(pL)=0 A=0 \sin(pL)=0 2B\cos(pL)=0 B=0 \cos(pL)=0 A=0,\;\text{or}\;\sin(pL)=0,\quad\text{and}\quad B=0\;\text{or}\;\cos(pL)=0. A=B=0 A=\cos(pL)=0 B=\sin(pL)=0 \cos(pL)=\sin(pL)=0 p_1 = \frac{n\pi}{L}\quad\text{and}\quad p_2 = \frac{(n-\frac{1}{2})\pi}{L},\quad n =1, 2, 3,\ldots, \cos\left(\frac{n\pi}{L}x\right)\quad\text{and}\quad\sin\left(\frac{(n-\frac{1}{2})\pi}{L}x\right),\quad n=1, 2, 3,\ldots y_n(x)=a_n\cos\left(\sqrt{\lambda_{n, 1}}x\right)+b_n\sin\left(\sqrt{\lambda_{n, 2}}x\right) \lambda_{n, 1}=\left(\frac{n\pi}{L}\right)^2,\quad \lambda_{n, 2}=\left(\frac{(n-\frac{1}{2})\pi}{L}\right)^2","['ordinary-differential-equations', 'partial-differential-equations']"
78,Finding the analytical solution of a first order system with pure time delay,Finding the analytical solution of a first order system with pure time delay,,"I have a simple system and I am searching for the solution for f(t): $$\frac{\partial f(t)}{\partial t} = c_1 \left( f(t) + g(t) + c_2 \right)$$ . It turns out that, in this system $g$ can be related to $f$ by a pure time delay: $$g(t) = f(t-a)u(t-a)$$ where $u(t)$ is the Heaviside function. Applying the Laplace transform to the equation above and isolating $F(s)$ leads to: $$F(s) = \frac{f(0) + c_1c_2}{s - c_1(1 - e^{-as})}$$ . Since there is a complex exponential at the denominator I cannot use the inverse Laplace transform nor break this into partial fractions. How can one find the solution of such an equation? If I cannot directly resolve the equation, can I infer a given solution (from the numerical integration of this system) and identify the coefficients? If the analytical solution does not exists, is there a proof that explains why?","I have a simple system and I am searching for the solution for f(t): . It turns out that, in this system can be related to by a pure time delay: where is the Heaviside function. Applying the Laplace transform to the equation above and isolating leads to: . Since there is a complex exponential at the denominator I cannot use the inverse Laplace transform nor break this into partial fractions. How can one find the solution of such an equation? If I cannot directly resolve the equation, can I infer a given solution (from the numerical integration of this system) and identify the coefficients? If the analytical solution does not exists, is there a proof that explains why?",\frac{\partial f(t)}{\partial t} = c_1 \left( f(t) + g(t) + c_2 \right) g f g(t) = f(t-a)u(t-a) u(t) F(s) F(s) = \frac{f(0) + c_1c_2}{s - c_1(1 - e^{-as})},"['ordinary-differential-equations', 'inverse-laplace', 'delay-differential-equations']"
79,Proving solutions of $y''+p(x)y'+q(x)y=0$ to be linearly independent,Proving solutions of  to be linearly independent,y''+p(x)y'+q(x)y=0,"When studying Elementary Differential Equations by William, I found trouble understanding Theorem 5.1.5 It says the two solutions are linearly independent iff their Wronskian is never zero, but I think they can still be linearly independent even if the Wronskian is zero for some $x$ . In the proof, when $W(x_0)=0$ , Theorem 5.1.4 is used to show $W\equiv 0$ . This theorem is the Abel's identity . It seems flawless, until I saw this answer . So $p(x)$ must be continuous on $(a,b)$ , but it is not as long as $W(x_0)=0$ , so we shouldn't use Abel's identity. This is because $$     y_1''+p(x)y_1'+q(x)y_1,\quad     y_2''+p(x)y_2'+q(x)y_2 $$ $$     y_1''y_2+p(x)y_1'y_2+q(x)y_1y_2,\quad     y_2''y_1+p(x)y_2'y_1+q(x)y_2y_1 $$ $$ p(x) = \frac{y_1''y_2-y_2''y_1}{y_2'y_1-y_1'y_2} = \frac{y_1''y_2-y_2''y_1}{W[y_1,y_2](x)}, $$ $p(x)$ is undefined at $x_0\in(a,b)$ . Also, I noticed all this by considering an example: for two solutions $y_1=1+x$ and $y_2=1+x^2$ , $W(x) = x^2+2x-1$ , on interval $(0,\infty)$ . So although $W(\sqrt{2}-1)=0$ , I think the two functions are still linearly independent on the interval, at least according to the definitions here . But using Theorem 5.1.5 they should be linearly dependent, because $W$ is zero for a point in the interval? Now it really confuses me despite thinking about it for hours. Which part did I miss? I am sorry if the question is too dumb, still not accustomed to the linearly independence of functions.","When studying Elementary Differential Equations by William, I found trouble understanding Theorem 5.1.5 It says the two solutions are linearly independent iff their Wronskian is never zero, but I think they can still be linearly independent even if the Wronskian is zero for some . In the proof, when , Theorem 5.1.4 is used to show . This theorem is the Abel's identity . It seems flawless, until I saw this answer . So must be continuous on , but it is not as long as , so we shouldn't use Abel's identity. This is because is undefined at . Also, I noticed all this by considering an example: for two solutions and , , on interval . So although , I think the two functions are still linearly independent on the interval, at least according to the definitions here . But using Theorem 5.1.5 they should be linearly dependent, because is zero for a point in the interval? Now it really confuses me despite thinking about it for hours. Which part did I miss? I am sorry if the question is too dumb, still not accustomed to the linearly independence of functions.","x W(x_0)=0 W\equiv 0 p(x) (a,b) W(x_0)=0 
    y_1''+p(x)y_1'+q(x)y_1,\quad
    y_2''+p(x)y_2'+q(x)y_2
 
    y_1''y_2+p(x)y_1'y_2+q(x)y_1y_2,\quad
    y_2''y_1+p(x)y_2'y_1+q(x)y_2y_1
 
p(x) = \frac{y_1''y_2-y_2''y_1}{y_2'y_1-y_1'y_2} = \frac{y_1''y_2-y_2''y_1}{W[y_1,y_2](x)},
 p(x) x_0\in(a,b) y_1=1+x y_2=1+x^2 W(x) = x^2+2x-1 (0,\infty) W(\sqrt{2}-1)=0 W","['ordinary-differential-equations', 'wronskian']"
80,Solving a coupled system of ordinary differential equations,Solving a coupled system of ordinary differential equations,,I want to find two functions $f_1(t)$ and $f_2(t)$ such that $$ \log f_1=A_{11} \log\left(-\dot{f_1}+a_1 f_1\right) +A_{12} \log\left(-\dot{f_2}+a_2 f_2\right) \\ \log f_2=A_{21} \log\left(-\dot{f_1}+a_1 f_1\right) +A_{22} \log\left(-\dot{f_2}+a_2 f_2\right) $$ where $A_{ij}$ and $a_i$ are known constants. The one-dimensional version of this system is $$ \log f=A \log\left(-\dot{f}+a f\right) $$ and its solution is $$ f=\left(\frac{1-e^{a\frac{A-1}{A}t}}{a} \right)^{\frac{A}{A-1}} $$ I'm hoping that a similar solution exists for the two-dimensional case but all my guesses so far have been wrong.,I want to find two functions and such that where and are known constants. The one-dimensional version of this system is and its solution is I'm hoping that a similar solution exists for the two-dimensional case but all my guesses so far have been wrong.,"f_1(t) f_2(t) 
\log f_1=A_{11} \log\left(-\dot{f_1}+a_1 f_1\right) +A_{12} \log\left(-\dot{f_2}+a_2 f_2\right) \\
\log f_2=A_{21} \log\left(-\dot{f_1}+a_1 f_1\right) +A_{22} \log\left(-\dot{f_2}+a_2 f_2\right)
 A_{ij} a_i 
\log f=A \log\left(-\dot{f}+a f\right)
 
f=\left(\frac{1-e^{a\frac{A-1}{A}t}}{a} \right)^{\frac{A}{A-1}}
","['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'nonlinear-system']"
81,Solving $y(2x+y)dx+(3x^2+4xy-y)dy=0$,Solving,y(2x+y)dx+(3x^2+4xy-y)dy=0,"I am trying to solve the following ODE, using substitution method. $y(2x+y)dx+(3x^2+4xy-y)dy=0$ Substitution Method is used when you have an ODE of the form $p(x,y)\ dx + q(x,y) \ dy = 0$ where $\frac{\partial p}{\partial y}\ne \frac{\partial q}{\partial x}$ , so the equation is not exact. But it might be exact when we multiply it by an integrating factor $u(x)=e^{\int f(x) dx}$ , where $f(x)=\frac{1}{q}(\frac{\partial p}{\partial y}-\frac{\partial q}{\partial x})$ . Here is what I have so far, $p(x,y)=y(2x+y)$ , $q(x,y)=3x^2+4xy-y$ $2x+2y=\frac{\partial p}{\partial y}\ne \frac{\partial q}{\partial x}=6x+4y$ So it is not exact. The point I am stuck at is that $\frac{1}{q}(\frac{\partial p}{\partial y}-\frac{\partial q}{\partial x})=\frac{1}{3x^2+4xy-y}(-4x-2y)$ , shouldn't $f$ be a function in the variable $x$ only? Edit: After some research, I found out that there are 3 cases: 1- $u$ could be a function in $x$ only. 2- $u$ could be a function in $y$ only. 3- $u$ could be a function in both $x$ and $y$ , and in this case it will be a PDE instead. Now, I was given this question in an intro to ODEs class, so I am assuming that there is a way I could use to turn my equation into an ODE? or is there a mistake in my solution in the first place?","I am trying to solve the following ODE, using substitution method. Substitution Method is used when you have an ODE of the form where , so the equation is not exact. But it might be exact when we multiply it by an integrating factor , where . Here is what I have so far, , So it is not exact. The point I am stuck at is that , shouldn't be a function in the variable only? Edit: After some research, I found out that there are 3 cases: 1- could be a function in only. 2- could be a function in only. 3- could be a function in both and , and in this case it will be a PDE instead. Now, I was given this question in an intro to ODEs class, so I am assuming that there is a way I could use to turn my equation into an ODE? or is there a mistake in my solution in the first place?","y(2x+y)dx+(3x^2+4xy-y)dy=0 p(x,y)\ dx + q(x,y) \ dy = 0 \frac{\partial p}{\partial y}\ne \frac{\partial q}{\partial x} u(x)=e^{\int f(x) dx} f(x)=\frac{1}{q}(\frac{\partial p}{\partial y}-\frac{\partial q}{\partial x}) p(x,y)=y(2x+y) q(x,y)=3x^2+4xy-y 2x+2y=\frac{\partial p}{\partial y}\ne \frac{\partial q}{\partial x}=6x+4y \frac{1}{q}(\frac{\partial p}{\partial y}-\frac{\partial q}{\partial x})=\frac{1}{3x^2+4xy-y}(-4x-2y) f x u x u y u x y",['ordinary-differential-equations']
82,Solve the second-order ordinary differential equation $4y''+\frac1{y^3}=0$,Solve the second-order ordinary differential equation,4y''+\frac1{y^3}=0,"$$4y''+\frac1{y^3}=0$$ I managed to turn this into a first order equation by multiplying by $y'$ , then integrating by $x$ : $4y''y'+y'\frac1{y^3}=0\\2y'+\frac{-1}{2y^2}=E$ but when I tried to simplify, I encountered an unfamiliar expression: $y'={2Ey^2+1\over4y^2}\\{dy\over dx}{4y^2\over Ey^2+1}=1\\\int{4y^2\over Ey^2+1}dy=\int1dx$ How can I integrate this expression by y? Is there anything I could've done differently to solve the equation?","I managed to turn this into a first order equation by multiplying by , then integrating by : but when I tried to simplify, I encountered an unfamiliar expression: How can I integrate this expression by y? Is there anything I could've done differently to solve the equation?",4y''+\frac1{y^3}=0 y' x 4y''y'+y'\frac1{y^3}=0\\2y'+\frac{-1}{2y^2}=E y'={2Ey^2+1\over4y^2}\\{dy\over dx}{4y^2\over Ey^2+1}=1\\\int{4y^2\over Ey^2+1}dy=\int1dx,['ordinary-differential-equations']
83,Solve PDE with condition,Solve PDE with condition,,"\begin{equation}     \begin{cases}         \sqrt{x}u_x - \sqrt{y}u_y = u^2 \\         u_{x=y} = \phi (y)     \end{cases} \end{equation} I want to change u(x,y) to v $(\tau, s )$ : \begin{equation}     \begin{cases}         \frac{dx}{ds} = \sqrt{x} , x(s=0) = \tau \implies x = (\frac{s+c}{2})^2\tau^2 \\         \frac{dy}{ds} = -\sqrt{y} , y(s=0) = \tau \implies y = (\frac{-s+c}{2})^2\tau^2 \\         \tau = \frac{\sqrt{x} + \sqrt{y}}{c} \\         s^2 = 2\frac{x+y}{\tau^2} - c^2 = 2\frac{c^2(x+y)}{(\sqrt{x} + \sqrt{y})^2} - c^2 = (c\frac{\sqrt{x}-\sqrt{y}}{\sqrt{x} + \sqrt{y}})^2 \\         s = c\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}     \end{cases} \end{equation} So we change the condition to : \begin{equation}     \begin{cases}         \frac{dv}{ds} = v^2 \\         v_{s=0} = \phi(\tau^2) \\     \end{cases} \end{equation} We find the solution : \begin{equation}     \begin{cases}         v = \frac{-1}{s+c} \\         v_{s=0} = \phi(\tau^2) \\         \implies v = \frac{-\phi(\tau^2)}{s\phi(\tau^2)+c} \\         Check : \frac{dv}{ds} = \frac{\phi^2(\tau^2)}{(s\phi(\tau^2)+c)^2} = v^2 \\     \end{cases} \end{equation} Change $v(\tau, s)$ to $u(x,y)$ : \begin{equation}     u(x,y) = \frac{-\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2})}{c\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + c} = \frac{-\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2})}{\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1} \\  \end{equation} Choose simple case c = 1 \begin{equation}     \begin{cases}         \frac{du}{dx} = -\frac{(\sqrt{x} + \sqrt{y})sgn(\sqrt{x}-\sqrt{y}) - |\sqrt{x} - \sqrt{y}|}{2\sqrt{x}(\sqrt{x} + \sqrt{y})^2(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \\          \frac{du}{dy} = -\frac{-(\sqrt{x} + \sqrt{y})sgn(\sqrt{x}-\sqrt{y}) - |\sqrt{x} - \sqrt{y}|}{2\sqrt{y}(\sqrt{x} + \sqrt{y})^2(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \\         \sqrt{x}\frac{du}{dx} - \sqrt{y}\frac{du}{dy} = -\frac{sgn(\sqrt{x}-\sqrt{y}) }{(\sqrt{x} + \sqrt{y})(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \neq u^2     \end{cases} \end{equation} Where did i do wrong ?","I want to change u(x,y) to v : So we change the condition to : We find the solution : Change to : Choose simple case c = 1 Where did i do wrong ?","\begin{equation}
    \begin{cases}
        \sqrt{x}u_x - \sqrt{y}u_y = u^2 \\
        u_{x=y} = \phi (y)
    \end{cases}
\end{equation} (\tau, s ) \begin{equation}
    \begin{cases}
        \frac{dx}{ds} = \sqrt{x} , x(s=0) = \tau \implies x = (\frac{s+c}{2})^2\tau^2 \\
        \frac{dy}{ds} = -\sqrt{y} , y(s=0) = \tau \implies y = (\frac{-s+c}{2})^2\tau^2 \\
        \tau = \frac{\sqrt{x} + \sqrt{y}}{c} \\
        s^2 = 2\frac{x+y}{\tau^2} - c^2 = 2\frac{c^2(x+y)}{(\sqrt{x} + \sqrt{y})^2} - c^2 = (c\frac{\sqrt{x}-\sqrt{y}}{\sqrt{x} + \sqrt{y}})^2 \\
        s = c\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}
    \end{cases}
\end{equation} \begin{equation}
    \begin{cases}
        \frac{dv}{ds} = v^2 \\
        v_{s=0} = \phi(\tau^2) \\
    \end{cases}
\end{equation} \begin{equation}
    \begin{cases}
        v = \frac{-1}{s+c} \\
        v_{s=0} = \phi(\tau^2) \\
        \implies v = \frac{-\phi(\tau^2)}{s\phi(\tau^2)+c} \\
        Check : \frac{dv}{ds} = \frac{\phi^2(\tau^2)}{(s\phi(\tau^2)+c)^2} = v^2 \\
    \end{cases}
\end{equation} v(\tau, s) u(x,y) \begin{equation}
    u(x,y) = \frac{-\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2})}{c\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + c} = \frac{-\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2})}{\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1} \\ 
\end{equation} \begin{equation}
    \begin{cases}
        \frac{du}{dx} = -\frac{(\sqrt{x} + \sqrt{y})sgn(\sqrt{x}-\sqrt{y}) - |\sqrt{x} - \sqrt{y}|}{2\sqrt{x}(\sqrt{x} + \sqrt{y})^2(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \\ 
        \frac{du}{dy} = -\frac{-(\sqrt{x} + \sqrt{y})sgn(\sqrt{x}-\sqrt{y}) - |\sqrt{x} - \sqrt{y}|}{2\sqrt{y}(\sqrt{x} + \sqrt{y})^2(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \\
        \sqrt{x}\frac{du}{dx} - \sqrt{y}\frac{du}{dy} = -\frac{sgn(\sqrt{x}-\sqrt{y}) }{(\sqrt{x} + \sqrt{y})(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \neq u^2
    \end{cases}
\end{equation}","['ordinary-differential-equations', 'partial-differential-equations', 'change-of-variable']"
84,How am I factorizing the $\frac{d}{dx}$ operator while trying to solve for the equation of a catenoid bubble?,How am I factorizing the  operator while trying to solve for the equation of a catenoid bubble?,\frac{d}{dx},"I've been working on my Math IA for a while, its a project part of the IB course which requires me to write a ~20 page math paper investigating and exploring something within maths. I chose to write my paper on using the Euler Lagrange equation to solve for the function which minimizes the surface area of a bubble between two rings, aka a catenoid. I did all the math for the paper a while ago, and I'm revisiting it to write the introduction and conclusion. While rereading the math, something didn't make sense to me. My final answer was correct, but a step didn't make sense or just didn't seem right. A section of my working out (which I took from a couple sources) starts off with this equation, from which we have to solve for $f$ . $$f'\sqrt{1+(f')^2} - f'\frac{d}{dx} \left( \frac{ff'}{\sqrt{1+(f')^2}} \right)=0$$ and from here we can replace the first $f'$ with $\frac{d}{dx}f$ , giving us: $$\frac{d}{dx}f\sqrt{1+(f')^2} - f'\frac{d}{dx} \left( \frac{ff'}{\sqrt{1+(f')^2}} \right)=0$$ and then for some reason, I factorise the operator $\frac{d}{dx}$ to get: $$\frac{d}{dx} \left[ f\sqrt{1+(f')^2} - \left( \frac{f(f')^2}{\sqrt{1+(f')^2}} \right) \right]=0$$ and from there I just integrate, solve for $f'$ , and solve the separable diff equation to solve for $f$ in terms of $x$ ( $\cosh$ ). The step I am confused about is when I factorize out the $\frac{d}{dx}$ , since its an operator. The math works because I got the correct answer, but why does it work, to me that shouldn't work but for some reason it does. Is there a reason for why this works? If so what is it.","I've been working on my Math IA for a while, its a project part of the IB course which requires me to write a ~20 page math paper investigating and exploring something within maths. I chose to write my paper on using the Euler Lagrange equation to solve for the function which minimizes the surface area of a bubble between two rings, aka a catenoid. I did all the math for the paper a while ago, and I'm revisiting it to write the introduction and conclusion. While rereading the math, something didn't make sense to me. My final answer was correct, but a step didn't make sense or just didn't seem right. A section of my working out (which I took from a couple sources) starts off with this equation, from which we have to solve for . and from here we can replace the first with , giving us: and then for some reason, I factorise the operator to get: and from there I just integrate, solve for , and solve the separable diff equation to solve for in terms of ( ). The step I am confused about is when I factorize out the , since its an operator. The math works because I got the correct answer, but why does it work, to me that shouldn't work but for some reason it does. Is there a reason for why this works? If so what is it.",f f'\sqrt{1+(f')^2} - f'\frac{d}{dx} \left( \frac{ff'}{\sqrt{1+(f')^2}} \right)=0 f' \frac{d}{dx}f \frac{d}{dx}f\sqrt{1+(f')^2} - f'\frac{d}{dx} \left( \frac{ff'}{\sqrt{1+(f')^2}} \right)=0 \frac{d}{dx} \frac{d}{dx} \left[ f\sqrt{1+(f')^2} - \left( \frac{f(f')^2}{\sqrt{1+(f')^2}} \right) \right]=0 f' f x \cosh \frac{d}{dx},"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives', 'euler-lagrange-equation']"
85,Proving that a standard state-space model is linear,Proving that a standard state-space model is linear,,"I recently learned about the standard continuous-time state-space model ( wiki ): $$\begin{align} \dot{x}(t) &= Ax(t) + Bu(t) \\ y(t) &= Cx(t) + Du(t). \end{align}$$ It's known that these systems are LTI (linear time-invariant), but that's not entirely obvious to me. Take the linearity of some system $G$ , for example, which stipulates among other things that $$\alpha u(t) \rightarrow G \rightarrow \alpha y(t).$$ I don't see how this is implied by the CT SSM above. Am I making a super simple mistake in my analysis below? Let $C=1, D=0$ for simplicity so that $y(t) = x(t)$ . This reduces the problem to solving the first differential equation, whose solution is the sum of the homogeneous solution and some particular solution, which comes out via variation of parameters to $$y(t) = x_0 e^{At} + \int K e^{A(t-\tau)} \, Bu(\tau) \, d\tau.$$ From this, it doesn't seem like $y(t)$ should always be linear w.r.t. $u(t)$ . What am I missing / what have I got wrong? Why then are these systems said to be linear?","I recently learned about the standard continuous-time state-space model ( wiki ): It's known that these systems are LTI (linear time-invariant), but that's not entirely obvious to me. Take the linearity of some system , for example, which stipulates among other things that I don't see how this is implied by the CT SSM above. Am I making a super simple mistake in my analysis below? Let for simplicity so that . This reduces the problem to solving the first differential equation, whose solution is the sum of the homogeneous solution and some particular solution, which comes out via variation of parameters to From this, it doesn't seem like should always be linear w.r.t. . What am I missing / what have I got wrong? Why then are these systems said to be linear?","\begin{align}
\dot{x}(t) &= Ax(t) + Bu(t) \\
y(t) &= Cx(t) + Du(t).
\end{align} G \alpha u(t) \rightarrow G \rightarrow \alpha y(t). C=1, D=0 y(t) = x(t) y(t) = x_0 e^{At} + \int K e^{A(t-\tau)} \, Bu(\tau) \, d\tau. y(t) u(t)","['ordinary-differential-equations', 'signal-processing']"
86,Assigning canonical form to the differential equation using characteristics,Assigning canonical form to the differential equation using characteristics,,"The problem statement is to simplify it, i.e. give it a canonical form to this differential equation using the characteristics to achieve that. However, even after many hints and trials on similar problems, I am still unable to proceed further with this one. $$u_{xx}-2u_{xy}+u_{yy}+9u_x+9u_y-9u=0.$$ Here is all the additional hints that I've been given by my professor: We need to write the characteristic equation $A dy = (B \pm \sqrt{B^2-AC}) dx$ , integrate to solve it, write parameters $\lambda$ and $\mu$ , $y=(1\pm ...)x+\text{const}$ and \begin{cases} \lambda= y-(1+\cdots)x\\\mu=y-(1-...)x\end{cases} Then we basically what we are left to do is to turn $u(x,y)$ into $u(\lambda(x,y), \mu(x,y))$ by calculating $\frac{\partial u}{\partial x}=\frac{\partial u}{\partial \lambda}\cdot\frac{\partial \lambda}{\partial x}+\frac{\partial u}{\partial \mu}\cdot\frac{\partial \mu}{\partial x}$ and etc. Despite being provided all the hints, I am still struggling to connect the dots between individual hints (maybe because of some persisent unidentified error in my calculations). I would appreciate if someone could provide a solution to this problem.","The problem statement is to simplify it, i.e. give it a canonical form to this differential equation using the characteristics to achieve that. However, even after many hints and trials on similar problems, I am still unable to proceed further with this one. Here is all the additional hints that I've been given by my professor: We need to write the characteristic equation , integrate to solve it, write parameters and , and Then we basically what we are left to do is to turn into by calculating and etc. Despite being provided all the hints, I am still struggling to connect the dots between individual hints (maybe because of some persisent unidentified error in my calculations). I would appreciate if someone could provide a solution to this problem.","u_{xx}-2u_{xy}+u_{yy}+9u_x+9u_y-9u=0. A dy = (B \pm \sqrt{B^2-AC}) dx \lambda \mu y=(1\pm ...)x+\text{const} \begin{cases} \lambda= y-(1+\cdots)x\\\mu=y-(1-...)x\end{cases} u(x,y) u(\lambda(x,y), \mu(x,y)) \frac{\partial u}{\partial x}=\frac{\partial u}{\partial \lambda}\cdot\frac{\partial \lambda}{\partial x}+\frac{\partial u}{\partial \mu}\cdot\frac{\partial \mu}{\partial x}","['ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'characteristics']"
87,Second order nonlinear ODE with a squared first derivative,Second order nonlinear ODE with a squared first derivative,,"In the context of an HJB equation for a control problem that I'm trying to solve, I have encountered the following ODE $$  4 \gamma  \zeta ^2 \beta  g(x) = \zeta ^2 \left(2 \gamma  \sigma ^2 x^2 g''(x)+1\right)-2 \zeta  g'(x) \left(\rho +2 \gamma  \zeta  x \left(\mu -\rho -\sigma ^2\right)\right)+\rho ^2 g'(x)^2\ . $$ where Greek letters are constants. Setting each of these equal to unity simplifies the above to $$  4 g(x)=2 x^2 g''(x)+g'(x)^2+2 (2 x-1) g'(x)+1\ , $$ This latter ODE in fact admits the non-trivial solution $g(x) = x - 2 x^2$ . Is there any way to apply this knowledge to full problem?","In the context of an HJB equation for a control problem that I'm trying to solve, I have encountered the following ODE where Greek letters are constants. Setting each of these equal to unity simplifies the above to This latter ODE in fact admits the non-trivial solution . Is there any way to apply this knowledge to full problem?","
 4 \gamma  \zeta ^2 \beta  g(x) = \zeta ^2 \left(2 \gamma  \sigma ^2 x^2 g''(x)+1\right)-2 \zeta  g'(x) \left(\rho +2 \gamma  \zeta  x \left(\mu -\rho -\sigma ^2\right)\right)+\rho ^2 g'(x)^2\ .
 
 4 g(x)=2 x^2 g''(x)+g'(x)^2+2 (2 x-1) g'(x)+1\ ,
 g(x) = x - 2 x^2","['ordinary-differential-equations', 'reference-request']"
88,Determine stability of non-hyperbolic stationary point,Determine stability of non-hyperbolic stationary point,,"Given the system $$\begin{align*} \dot{x_1} &= x_2+x_1^2-x_1^3 \\ \dot{x_2} &= -x_2+\mu x_1^2 \end{align*} $$ determine the stability of the stationary point in the origin for $\mu = \{-1,0, 1\}$ . Attempt First step of determining stability is to find the Jacobian-matrix: $$\mathbf{D}\mathbf{f}(x,y) = \begin{bmatrix} 2x_1-3x_1^2 & 1 \\ 2\mu x & -1 \end{bmatrix} $$ Next, we determine the eigenvalues of the Jacobian-matrix evaluated in the origin. $$\mathbf{D}\mathbf{f}(0,0) = \begin{bmatrix} 0 & 1 \\ 0 & -1 \end{bmatrix} \Rightarrow \text{eigenvalues}: \lambda=\{0,-1\}$$ To me, it seems the eigenvalues in the origin are independent of $\mu$ . Furthermore, the origin is non-hyperbolic because one of the eigenvalues are $0$ . Question How do I determine the stability of the origin why one of the eigenvalues are $0$ , and how can I see the dependence of $\mu$ on the stability. Is it possible to solve this problem via heuristics instead of by computation?","Given the system determine the stability of the stationary point in the origin for . Attempt First step of determining stability is to find the Jacobian-matrix: Next, we determine the eigenvalues of the Jacobian-matrix evaluated in the origin. To me, it seems the eigenvalues in the origin are independent of . Furthermore, the origin is non-hyperbolic because one of the eigenvalues are . Question How do I determine the stability of the origin why one of the eigenvalues are , and how can I see the dependence of on the stability. Is it possible to solve this problem via heuristics instead of by computation?","\begin{align*} \dot{x_1} &= x_2+x_1^2-x_1^3 \\
\dot{x_2} &= -x_2+\mu x_1^2
\end{align*}  \mu = \{-1,0, 1\} \mathbf{D}\mathbf{f}(x,y) = \begin{bmatrix} 2x_1-3x_1^2 & 1 \\
2\mu x & -1 \end{bmatrix}  \mathbf{D}\mathbf{f}(0,0) = \begin{bmatrix} 0 & 1 \\
0 & -1 \end{bmatrix} \Rightarrow \text{eigenvalues}: \lambda=\{0,-1\} \mu 0 0 \mu","['ordinary-differential-equations', 'jacobian', 'stability-theory', 'stationary-point']"
89,Brezis' exercise 8.24.2: do we need the assumption that $k$ is sufficiently large?,Brezis' exercise 8.24.2: do we need the assumption that  is sufficiently large?,k,"Let $I$ be the open interval $(0, 1)$ . I'm trying to solve a problem in Brezis' Functional Analysis , i.e., Exercise 8.24 Prove that for every $\varepsilon>0$ there exists a constant $C_{\varepsilon}$ such that $$ |u(1)|^2 \leq \varepsilon\left\|u^{\prime}\right\|_{L^2}^2+C_{\varepsilon}\|u\|_{L^2}^2 \quad \forall u \in H^1(I) . $$ Prove that if the constant $k>0$ is sufficiently large, then for every $f \in L^2(I)$ there exists a unique $u \in H^2(I)$ satisfying $$ (1) \quad \begin{cases} -u^{\prime \prime}+k u=f \quad \text{on} \quad I, \\ u^{\prime}(0)=0 \quad \text{and} \quad u^{\prime}(1)=u(1). \end{cases} $$ What is the weak formulation of problem (1)? What is the associated minimization problem? In below attempt of (2.), I don't need that $k$ is sufficiently large. I only need $k \ge 0$ . Could you verify if my attempt contains some subtle mistakes? Let $K := \{v \in H^1(I) : v'(0)=0, v'(1)=v(1)\}$ . Then $K$ is a closed subspace of $H^1(I)$ . If $u$ is a classical solution to $(1)$ , then $$ \int_I [-u''v + kuv] = \int_I fv, \quad \forall v \in K, $$ which (by integration by parts) is equivalent to $$ (2) \quad  u(1) v(1) + \int_I [u'v' + k uv] = \int_I fv, \quad \forall v \in K. $$ Then $(2)$ is the weak forlulation of $(1)$ . We define a symmetric bilinear form $a$ on $K$ by $$ a(u, v) := u(1) v(1) + \int_I [u'v' + k uv]. $$ It follows from (1.) that $a$ is continuous. Because $k$ is non-negative, $a$ is coercive. We define $\varphi \in (H^1(I))^*$ by $$ \varphi (v) = \int_I fv, \quad \forall v \in K. $$ By Lax-Milgram theorem, $(2)$ has a unique solution $u \in K$ . The associated minimization is $$ u= \operatorname{argmin}_{v \in K} \left \{ \frac{1}{2} a(v, v) -  \varphi (v)\right \}. $$ Notice that $(2)$ implies $$ \int_I u'v' = -\int_I (ku-f)v, \quad \forall v \in C^\infty_c (I), $$ which implies $u \in H^2(I)$ with $u''=ku-f$ .","Let be the open interval . I'm trying to solve a problem in Brezis' Functional Analysis , i.e., Exercise 8.24 Prove that for every there exists a constant such that Prove that if the constant is sufficiently large, then for every there exists a unique satisfying What is the weak formulation of problem (1)? What is the associated minimization problem? In below attempt of (2.), I don't need that is sufficiently large. I only need . Could you verify if my attempt contains some subtle mistakes? Let . Then is a closed subspace of . If is a classical solution to , then which (by integration by parts) is equivalent to Then is the weak forlulation of . We define a symmetric bilinear form on by It follows from (1.) that is continuous. Because is non-negative, is coercive. We define by By Lax-Milgram theorem, has a unique solution . The associated minimization is Notice that implies which implies with .","I (0, 1) \varepsilon>0 C_{\varepsilon} 
|u(1)|^2 \leq \varepsilon\left\|u^{\prime}\right\|_{L^2}^2+C_{\varepsilon}\|u\|_{L^2}^2 \quad \forall u \in H^1(I) .
 k>0 f \in L^2(I) u \in H^2(I) 
(1) \quad
\begin{cases}
-u^{\prime \prime}+k u=f \quad \text{on} \quad I, \\
u^{\prime}(0)=0 \quad \text{and} \quad u^{\prime}(1)=u(1).
\end{cases}
 k k \ge 0 K := \{v \in H^1(I) : v'(0)=0, v'(1)=v(1)\} K H^1(I) u (1) 
\int_I [-u''v + kuv] = \int_I fv,
\quad \forall v \in K,
 
(2) \quad 
u(1) v(1) + \int_I [u'v' + k uv] = \int_I fv,
\quad \forall v \in K.
 (2) (1) a K 
a(u, v) := u(1) v(1) + \int_I [u'v' + k uv].
 a k a \varphi \in (H^1(I))^* 
\varphi (v) = \int_I fv,
\quad \forall v \in K.
 (2) u \in K 
u= \operatorname{argmin}_{v \in K} \left \{ \frac{1}{2} a(v, v) -  \varphi (v)\right \}.
 (2) 
\int_I u'v' = -\int_I (ku-f)v,
\quad \forall v \in C^\infty_c (I),
 u \in H^2(I) u''=ku-f","['functional-analysis', 'ordinary-differential-equations', 'sobolev-spaces']"
90,Solving a second order differential equation using the method of undetermined coefficients,Solving a second order differential equation using the method of undetermined coefficients,,"Problem: Find the general solution of the following differential equation. $$ y'' + 2y' + 4y = 13 \cos(4x) $$ Answer: So the first step is to find the complementary solution, $y_c$ . \begin{align*} m^2 +2m + 4 &= 0 \\ m &= \dfrac{ -2 \pm \sqrt{ 4 - 4(1)(4)} }{ 2(1)} \\ m &= \dfrac{ -2 \pm \sqrt{ -12 } }{ 2 } \\ m &= -1 \pm \sqrt{3} i \\ \end{align*} Now we have: $$ y_c = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) )$$ . Now we need to find $y_p$ . \begin{align*} y_p &= A \cos(4x) + B \sin(4x) \\ y'_p &= - 4A \sin(4x) + 4B \cos(4x) \\ y''_p &= - 16A \cos(4x) - 16B \sin(4x) \\ \end{align*} Let $LHS = y'' + 2y' + 4y $ . We have: \begin{align*} LHS &= - 16A \cos(4x) - 16B \sin(4x) + \\     &2 ( - 4A \sin(4x) + 4B \cos(4x)  ) 	+ 4( A \cos(4x) + B \sin(4x) ) \\ % LHS &= 	- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + \\     & 8B \cos(4x)	+ 4A \cos(4x) + 4B \sin(4x) \\ \end{align*} \begin{align*} - 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + 8B \cos(4x) + \\ 	& 4A \cos(4x) + 4B \sin(4x) \\     &= 13 \cos(4x) \\ \end{align*} \begin{align*} - 12A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + 8B \cos(4x) 		+ 4B \sin(4x) &= 13 \cos(4x) \\ % - 12A \cos(4x) - 12B \sin(4x) - 8A \sin(4x) + 8B \cos(4x) &= 13 \cos(4x) \end{align*} Now we set up the following system of linear equations: \begin{align*} -12A + 8B &= 13 \\ -12B - 8A &= 0 \\ \end{align*} We rewrite as: \begin{align*} -12A + 8B &= 13 \\ 2A + 3B &= 0 \\ \end{align*} Now we solve the system of equations: \begin{align*} A &= \left( - \dfrac{ 3 }{2} \right) B \\ -12 \left( \dfrac{-3B}{2} \right) + 8B &= 13 \\ 18B + 8B &= 13 \\ B &= 2 \\ A &= -3 \\ \end{align*} Hence our answer is: $$ y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) +   + 48 \cos(4x) - 32 \sin(4x)   $$ However, the book's answer is: $$ y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) + 		\left( \dfrac{1}{2}\right) \sin(4x) - \dfrac{3 }{4} - \cos(4x) $$ Where did I go wrong? Based upon the comments made by user170231 I have updated my answer. However, it is still wrong. Here is the updated answer: So the first step is to find the complementary solution, $y_c$ . \begin{align*} m^2 +2m + 4 &= 0 \\ m &= \dfrac{ -2 \pm \sqrt{ 4 - 4(1)(4)} }{ 2(1)} \\ m &= \dfrac{ -2 \pm \sqrt{ -12 } }{ 2 } \\ m &= -1 \pm \sqrt{3} i \\ \end{align*} Now we have: $$ y_c = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) )$$ . Now we need to find $y_p$ . \begin{align*} y_p &= A \cos(4x) + B \sin(4x) \\ y'_p &= - 4A \sin(4x) + 4B \cos(4x) \\ y''_p &= - 16A \cos(4x) - 16B \sin(4x) \\ \end{align*} Let $LHS = y'' + 2y' + 4y $ . We have: \begin{align*} LHS &= - 16A \cos(4x) - 16B \sin(4x) + 2 ( - 4A \sin(4x) + 4B \cos(4x)  ) \\ 	&+ 4( A \cos(4x) + B \sin(4x) ) \\ % LHS &= 	- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) \\    &+ 8B \cos(4x) \\ 	& + 4A \cos(4x) + 4B \sin(4x) \\ \end{align*} \begin{align*} - 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) \\    & + 8B \cos(4x) \\ 	&+ 4A \cos(4x) + 4B \sin(4x) \\  &= 13 \cos(4x) \\ % - 12A \cos(4x) - 16B \sin(4x) \\     &- 8A \sin(4x) + 8B \cos(4x) \\ 		+ 4B \sin(4x) &= 13 \cos(4x) \\ \end{align*} \begin{align*} - 12A \cos(4x) - 12B \sin(4x) - 8A \sin(4x) \\  &+ 8B \cos(4x) \\ &= 13 \cos(4x) \end{align*} Now we set up the following system of linear equations: \begin{align*} -12A + 8B &= 13 \\ -12B - 8A &= 0 \\ \end{align*} We rewrite as: \begin{align*} -12A + 8B &= 13 \\ 2A + 3B &= 0 \\ \end{align*} Now we solve the system of equations: \begin{align*} A &= \left( - \dfrac{ 3 }{2} \right) B \\ -12 \left( \dfrac{-3B}{2} \right) + 8B &= 13 \\ 18B + 8B &= 13 \\ 26B &= 13 \\ B &= \dfrac{1}{2} \\ A &= \left( \dfrac{-3}{2}\right) \left( \dfrac{1}{2} \right) \\ A &= - \dfrac{ 3 }{4 } \end{align*} Hence our answer is: $$ y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) )   - \dfrac{ 3 }{4 } \cos(4x)  + \dfrac{1}{2} \sin(4x)   $$ However, the book's answer is: $$ y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) + 		\left( \dfrac{1}{2}\right) \sin(4x) - \dfrac{3 }{4} - \cos(4x) $$ Where did I go wrong?","Problem: Find the general solution of the following differential equation. Answer: So the first step is to find the complementary solution, . Now we have: . Now we need to find . Let . We have: Now we set up the following system of linear equations: We rewrite as: Now we solve the system of equations: Hence our answer is: However, the book's answer is: Where did I go wrong? Based upon the comments made by user170231 I have updated my answer. However, it is still wrong. Here is the updated answer: So the first step is to find the complementary solution, . Now we have: . Now we need to find . Let . We have: Now we set up the following system of linear equations: We rewrite as: Now we solve the system of equations: Hence our answer is: However, the book's answer is: Where did I go wrong?"," y'' + 2y' + 4y = 13 \cos(4x)  y_c \begin{align*}
m^2 +2m + 4 &= 0 \\
m &= \dfrac{ -2 \pm \sqrt{ 4 - 4(1)(4)} }{ 2(1)} \\
m &= \dfrac{ -2 \pm \sqrt{ -12 } }{ 2 } \\
m &= -1 \pm \sqrt{3} i \\
\end{align*}  y_c = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) y_p \begin{align*}
y_p &= A \cos(4x) + B \sin(4x) \\
y'_p &= - 4A \sin(4x) + 4B \cos(4x) \\
y''_p &= - 16A \cos(4x) - 16B \sin(4x) \\
\end{align*} LHS = y'' + 2y' + 4y  \begin{align*}
LHS &= - 16A \cos(4x) - 16B \sin(4x) + \\
    &2 ( - 4A \sin(4x) + 4B \cos(4x)  )
	+ 4( A \cos(4x) + B \sin(4x) ) \\
%
LHS &=
	- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + \\
    & 8B \cos(4x)	+ 4A \cos(4x) + 4B \sin(4x) \\
\end{align*} \begin{align*}
- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + 8B \cos(4x) + \\
	& 4A \cos(4x) + 4B \sin(4x) \\
    &= 13 \cos(4x) \\
\end{align*} \begin{align*}
- 12A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + 8B \cos(4x)
		+ 4B \sin(4x) &= 13 \cos(4x) \\
%
- 12A \cos(4x) - 12B \sin(4x) - 8A \sin(4x) + 8B \cos(4x) &= 13 \cos(4x)
\end{align*} \begin{align*}
-12A + 8B &= 13 \\
-12B - 8A &= 0 \\
\end{align*} \begin{align*}
-12A + 8B &= 13 \\
2A + 3B &= 0 \\
\end{align*} \begin{align*}
A &= \left( - \dfrac{ 3 }{2} \right) B \\
-12 \left( \dfrac{-3B}{2} \right) + 8B &= 13 \\
18B + 8B &= 13 \\
B &= 2 \\
A &= -3 \\
\end{align*}  y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) +
  + 48 \cos(4x) - 32 \sin(4x)     y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) +
		\left( \dfrac{1}{2}\right) \sin(4x) - \dfrac{3 }{4} - \cos(4x)  y_c \begin{align*}
m^2 +2m + 4 &= 0 \\
m &= \dfrac{ -2 \pm \sqrt{ 4 - 4(1)(4)} }{ 2(1)} \\
m &= \dfrac{ -2 \pm \sqrt{ -12 } }{ 2 } \\
m &= -1 \pm \sqrt{3} i \\
\end{align*}  y_c = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) y_p \begin{align*}
y_p &= A \cos(4x) + B \sin(4x) \\
y'_p &= - 4A \sin(4x) + 4B \cos(4x) \\
y''_p &= - 16A \cos(4x) - 16B \sin(4x) \\
\end{align*} LHS = y'' + 2y' + 4y  \begin{align*}
LHS &=
- 16A \cos(4x) - 16B \sin(4x) + 2 ( - 4A \sin(4x) + 4B \cos(4x)  ) \\
	&+ 4( A \cos(4x) + B \sin(4x) ) \\
%
LHS &=
	- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) \\
   &+ 8B \cos(4x) \\
	& + 4A \cos(4x) + 4B \sin(4x) \\
\end{align*} \begin{align*}
- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) \\
   & + 8B \cos(4x) \\
	&+ 4A \cos(4x) + 4B \sin(4x) \\
 &= 13 \cos(4x) \\
%
- 12A \cos(4x) - 16B \sin(4x) \\
    &- 8A \sin(4x) + 8B \cos(4x) \\
		+ 4B \sin(4x) &= 13 \cos(4x) \\
\end{align*} \begin{align*}
- 12A \cos(4x) - 12B \sin(4x) - 8A \sin(4x) \\
 &+ 8B \cos(4x) \\
&= 13 \cos(4x)
\end{align*} \begin{align*}
-12A + 8B &= 13 \\
-12B - 8A &= 0 \\
\end{align*} \begin{align*}
-12A + 8B &= 13 \\
2A + 3B &= 0 \\
\end{align*} \begin{align*}
A &= \left( - \dfrac{ 3 }{2} \right) B \\
-12 \left( \dfrac{-3B}{2} \right) + 8B &= 13 \\
18B + 8B &= 13 \\
26B &= 13 \\
B &= \dfrac{1}{2} \\
A &= \left( \dfrac{-3}{2}\right) \left( \dfrac{1}{2} \right) \\
A &= - \dfrac{ 3 }{4 }
\end{align*}  y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) )
  - \dfrac{ 3 }{4 } \cos(4x)  + \dfrac{1}{2} \sin(4x)     y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) +
		\left( \dfrac{1}{2}\right) \sin(4x) - \dfrac{3 }{4} - \cos(4x) ",['ordinary-differential-equations']
91,Why is this the general solution of this DE?,Why is this the general solution of this DE?,,"I am reading a device physics text (Sze Physics of Semiconductor Devices , 3e, Chapter 2.4.3) and the author makes the claim that the solution $y(x)$ to a simple linear DE of the form $$y' +P(x)y = Q(x)$$ along with the boundary condition at $x = 0$ which I will just call $y(0)$ is $$y(x) = \frac{\int_0^{x}Q\exp \left(\int_0^{x''} P \,dx' \right) \,dx'' + y(0)}{\exp \left(\int_0^x P \,dx' \right)}.$$ My differential equations knowledge is weak as it's been a long time. I have some hazy sense that I should be looking for an integrating factor or something of the sort, but I would greatly appreciate if someone could step through the argument leading to this solution. Thank you!","I am reading a device physics text (Sze Physics of Semiconductor Devices , 3e, Chapter 2.4.3) and the author makes the claim that the solution to a simple linear DE of the form along with the boundary condition at which I will just call is My differential equations knowledge is weak as it's been a long time. I have some hazy sense that I should be looking for an integrating factor or something of the sort, but I would greatly appreciate if someone could step through the argument leading to this solution. Thank you!","y(x) y' +P(x)y = Q(x) x = 0 y(0) y(x) = \frac{\int_0^{x}Q\exp \left(\int_0^{x''} P \,dx' \right) \,dx'' + y(0)}{\exp \left(\int_0^x P \,dx' \right)}.","['integration', 'ordinary-differential-equations', 'integrating-factor']"
92,How to slightly modify my ODE system in order to capture bump in the data?,How to slightly modify my ODE system in order to capture bump in the data?,,"I have the following two sets of data, which show the dependencies of two quantities, namely, $S$ and $B$ , on time ( $0$ h, $3$ h, $6$ h, $9$ h, $15$ h, $18$ h, $21$ h, and $24$ h): Sdata = {{0, 9.74},{3, 4.92},{6, 8.29},{9, 5.54},{15, 2.08},{18, 1.38},{21, 1.99},{24, 0.893}};  Bdata = {{0, 0.915094},{3, 0.736097},{6, 0.793694},{9, 0.833664},{15, 1},{18, 0.99578},{21, 0.897964},{24, 0.214499}}; I have modeled the dynamics of the above system by a coupled ODE system: $$\frac{dS(t)}{dt} = - \frac{a}{1 + B(t)} S(t),$$ $$\frac{dB(t)}{dt} = \frac{c}{1 + S(t)} B(t) - d B^2(t) \Big( \frac{1 - B(t)}{B(t)} \Big)^n,$$ where $a$ , $c$ , $d$ , and $n$ are constants to be determined from the data. Using Mathematica for fitting, we obtain: Sdata = {{0, 9.74}, {3, 4.92}, {6, 8.29}, {9, 5.54}, {15, 2.08}, {18,  1.38}, {21, 1.99}, {24, 0.893}}; Bdata = {{0, 0.915094}, {3, 0.736097}, {6, 0.793694}, {9,  0.833664}, {15, 1}, {18, 0.99578}, {21, 0.897964}, {24, 0.214499}}; order = 1; interpolatedData = {intS,  intB} = {Interpolation[Sdata, InterpolationOrder -> order],  Interpolation[Bdata, InterpolationOrder -> order]}; sys = {S'[t] == -a/(1 + B[t]) S[t],  B'[t] == c B[t]/(1 + S[t]) - d B[t]^2 ((1 - B[t])/B[t])^n}; squareDiffs = MapApply[(#1 - #2)^2 &, sys]; withInt[t_] = squareDiffs /. {S -> intS, B -> intB}; totalSquaredError = Total@Flatten[withInt /@ (Range[0, 24, 3])]; forMin = Join[{totalSquaredError}, restrictions]; restrictions = Thread[{a, c, d, n} > 0]; {resid, bestFitParams} =  NMinimize[forMin, {a, c, d, n}, Method -> ""RandomSearch""] init = {S[0] == Sdata[[1, 2]], B[0] == Bdata[[1, 2]]}; new = Join[sys /. bestFitParams, init]; {sSol[t_], bSol[t_]} = NDSolveValue[new, {S[t], B[t]}, {t, 0, 24}];  lp = ListPlot[Bdata]; p = Plot[bSol[t], {t, 0, 24}, PlotRange -> All]; Show[lp, p] lpp = ListPlot[Sdata]; pp = Plot[sSol[t], {t, 0, 24}, PlotRange -> All]; Show[lpp, pp] See here the fitted curves. (Blue points show $S(t)$ data and red points show $B(t)$ data.) The fitted curve to the red points, i.e., $B(t)$ , cannot capture the bump in the data around $t = 15$ . I appreciate any insight or hint for how changing my ODE system slightly in order to capture this bump before falling down of the values. EDIT The datasets with standard deviations: SdatawithSD = {{0, Around[9.74, 2.89]}, {3, Around[4.92, 1.65]}, {6,  Around[8.29, 4.04]}, {9, Around[5.54, 2.45]}, {15,  Around[2.08, 1.91]}, {18, Around[1.38, 0.962]}, {21,  Around[1.99, 2.41]}, {24, Around[0.893, 0.359]}};   BdatawithSD = {{0, Around[0.915094, 0.1]}, {3,  Around[0.736097, 0.091668]}, {6, Around[0.793694, 0.082575]}, {9,  Around[0.833664, 0.070242]}, {15, Around[1, 0.002851]}, {18,  Around[0.99578, 0.015591]}, {21, Around[0.897964, 0.04783]}, {24,  Around[0.214499, 0.01231]}};","I have the following two sets of data, which show the dependencies of two quantities, namely, and , on time ( h, h, h, h, h, h, h, and h): Sdata = {{0, 9.74},{3, 4.92},{6, 8.29},{9, 5.54},{15, 2.08},{18, 1.38},{21, 1.99},{24, 0.893}};  Bdata = {{0, 0.915094},{3, 0.736097},{6, 0.793694},{9, 0.833664},{15, 1},{18, 0.99578},{21, 0.897964},{24, 0.214499}}; I have modeled the dynamics of the above system by a coupled ODE system: where , , , and are constants to be determined from the data. Using Mathematica for fitting, we obtain: Sdata = {{0, 9.74}, {3, 4.92}, {6, 8.29}, {9, 5.54}, {15, 2.08}, {18,  1.38}, {21, 1.99}, {24, 0.893}}; Bdata = {{0, 0.915094}, {3, 0.736097}, {6, 0.793694}, {9,  0.833664}, {15, 1}, {18, 0.99578}, {21, 0.897964}, {24, 0.214499}}; order = 1; interpolatedData = {intS,  intB} = {Interpolation[Sdata, InterpolationOrder -> order],  Interpolation[Bdata, InterpolationOrder -> order]}; sys = {S'[t] == -a/(1 + B[t]) S[t],  B'[t] == c B[t]/(1 + S[t]) - d B[t]^2 ((1 - B[t])/B[t])^n}; squareDiffs = MapApply[(#1 - #2)^2 &, sys]; withInt[t_] = squareDiffs /. {S -> intS, B -> intB}; totalSquaredError = Total@Flatten[withInt /@ (Range[0, 24, 3])]; forMin = Join[{totalSquaredError}, restrictions]; restrictions = Thread[{a, c, d, n} > 0]; {resid, bestFitParams} =  NMinimize[forMin, {a, c, d, n}, Method -> ""RandomSearch""] init = {S[0] == Sdata[[1, 2]], B[0] == Bdata[[1, 2]]}; new = Join[sys /. bestFitParams, init]; {sSol[t_], bSol[t_]} = NDSolveValue[new, {S[t], B[t]}, {t, 0, 24}];  lp = ListPlot[Bdata]; p = Plot[bSol[t], {t, 0, 24}, PlotRange -> All]; Show[lp, p] lpp = ListPlot[Sdata]; pp = Plot[sSol[t], {t, 0, 24}, PlotRange -> All]; Show[lpp, pp] See here the fitted curves. (Blue points show data and red points show data.) The fitted curve to the red points, i.e., , cannot capture the bump in the data around . I appreciate any insight or hint for how changing my ODE system slightly in order to capture this bump before falling down of the values. EDIT The datasets with standard deviations: SdatawithSD = {{0, Around[9.74, 2.89]}, {3, Around[4.92, 1.65]}, {6,  Around[8.29, 4.04]}, {9, Around[5.54, 2.45]}, {15,  Around[2.08, 1.91]}, {18, Around[1.38, 0.962]}, {21,  Around[1.99, 2.41]}, {24, Around[0.893, 0.359]}};   BdatawithSD = {{0, Around[0.915094, 0.1]}, {3,  Around[0.736097, 0.091668]}, {6, Around[0.793694, 0.082575]}, {9,  Around[0.833664, 0.070242]}, {15, Around[1, 0.002851]}, {18,  Around[0.99578, 0.015591]}, {21, Around[0.897964, 0.04783]}, {24,  Around[0.214499, 0.01231]}};","S B 0 3 6 9 15 18 21 24 \frac{dS(t)}{dt} = - \frac{a}{1 + B(t)} S(t), \frac{dB(t)}{dt} = \frac{c}{1 + S(t)} B(t) - d B^2(t) \Big( \frac{1 - B(t)}{B(t)} \Big)^n, a c d n S(t) B(t) B(t) t = 15",['ordinary-differential-equations']
93,"Prove that $f'(x)$ is constant from $2(f'(x))^2 + f(x)f''(x)=c$, c constant?","Prove that  is constant from , c constant?",f'(x) 2(f'(x))^2 + f(x)f''(x)=c,"I was fiddling around with derivatives of functions and messed up a question so badly that I was left with the above expression and the knowledge that $f(0)=0$ . To proceed, I assumed $f'(x)=\text{constant}$ for all $x$ and that gave me the answer to the problem. I also know from solving it with another method that there really exists only 1 function $f(x)$ . The problem is, if a non-constant $f'(x)$ could be found, there would exist another solution to the problem, which I know to not be the case, so $f'(x)$ should be constant, and provable from the above equation and knowing $f(x)=0$ . If so, how do we go about proving it? For the curious, the original question stated that, for a continuous and differentiable $g(x)$ , define $f(x)$ to be $f(x)=g''(x)$ and $g(x)=kf^3(x)$ for a constant $k$ , and $g(0)=g'(0)=0$ . You arrive at my equation by taking the derivative of both sides of the equation twice and simplifying it by $f(x)$ (we are given that $f(x)$ is not $0$ for all $x$ ).","I was fiddling around with derivatives of functions and messed up a question so badly that I was left with the above expression and the knowledge that . To proceed, I assumed for all and that gave me the answer to the problem. I also know from solving it with another method that there really exists only 1 function . The problem is, if a non-constant could be found, there would exist another solution to the problem, which I know to not be the case, so should be constant, and provable from the above equation and knowing . If so, how do we go about proving it? For the curious, the original question stated that, for a continuous and differentiable , define to be and for a constant , and . You arrive at my equation by taking the derivative of both sides of the equation twice and simplifying it by (we are given that is not for all ).",f(0)=0 f'(x)=\text{constant} x f(x) f'(x) f'(x) f(x)=0 g(x) f(x) f(x)=g''(x) g(x)=kf^3(x) k g(0)=g'(0)=0 f(x) f(x) 0 x,"['ordinary-differential-equations', 'derivatives']"
94,Books on mathematical frameworks?,Books on mathematical frameworks?,,"I am a first year student of applied mathematics. I currently find myself always trying to explain any phenomenon with mathematical models- the few ones (models) that I encountered till now. I always try to fit phenomenas to models like a data scientist does when he fits data to models. Recently I found this book: Robert B. Banks Growth and Diffusion Phenomena: Mathematical Frameworks and Applications This book is different from other applied mathematics books in the sense that it focuses on a particular model unlike others that try to model the whole world in a single book. It particularly focuses on growth and diffusion phenomena. Which is , to me , great because this book studies these two models in great detail rather than just stating many models on the surface level. I also found some book that is specific to some single phenomenon like this book on Hysteresis Mathematical Models of Hysteresis and their Applications Can someone please recommend to me some books like these that describes a certain phenomena or discusses a certain mathematical framework to study in detail and apply some phenomena? Any book that discusses and models in detail some phenomena: could be growth,could be decline, could be (what happens in the limiting case)- type of thing,  could be something else, could be anything...","I am a first year student of applied mathematics. I currently find myself always trying to explain any phenomenon with mathematical models- the few ones (models) that I encountered till now. I always try to fit phenomenas to models like a data scientist does when he fits data to models. Recently I found this book: Robert B. Banks Growth and Diffusion Phenomena: Mathematical Frameworks and Applications This book is different from other applied mathematics books in the sense that it focuses on a particular model unlike others that try to model the whole world in a single book. It particularly focuses on growth and diffusion phenomena. Which is , to me , great because this book studies these two models in great detail rather than just stating many models on the surface level. I also found some book that is specific to some single phenomenon like this book on Hysteresis Mathematical Models of Hysteresis and their Applications Can someone please recommend to me some books like these that describes a certain phenomena or discusses a certain mathematical framework to study in detail and apply some phenomena? Any book that discusses and models in detail some phenomena: could be growth,could be decline, could be (what happens in the limiting case)- type of thing,  could be something else, could be anything...",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'stochastic-processes', 'mathematical-modeling']"
95,How do you solve the ODE $x'+2tx=2t^3$,How do you solve the ODE,x'+2tx=2t^3,"I do not get their explanation here at all. In class, we derived the following formula for the method of integrating  factors for solving first order inhomogeneous linear ODEs of the form $$y'(x)+p(x)y(x)=q(x).$$ $$y(x)=\dfrac{1}{u(x)}\left[\int u(x)q(x)dx+C\right],$$ where the integrating factor is given as $$u(x)=e^{\int p(x)dx}.$$ Following their method, I indeed got the same integrating factor $u(t)=e^{t^2}$ , but with the formula above, how then am I supposed to arrive at their conclusion? Integration by parts on the $u(t)q(t)$ part seems to get me nowhere as (without using the error function), $e^{t^2}$ cannot be differentated away or integrated. Could someone perhaps help fill in the missing lines of working or explain the intermediary steps?","I do not get their explanation here at all. In class, we derived the following formula for the method of integrating  factors for solving first order inhomogeneous linear ODEs of the form where the integrating factor is given as Following their method, I indeed got the same integrating factor , but with the formula above, how then am I supposed to arrive at their conclusion? Integration by parts on the part seems to get me nowhere as (without using the error function), cannot be differentated away or integrated. Could someone perhaps help fill in the missing lines of working or explain the intermediary steps?","y'(x)+p(x)y(x)=q(x). y(x)=\dfrac{1}{u(x)}\left[\int u(x)q(x)dx+C\right], u(x)=e^{\int p(x)dx}. u(t)=e^{t^2} u(t)q(t) e^{t^2}",['ordinary-differential-equations']
96,Generating a group of transformations from vector fields,Generating a group of transformations from vector fields,,"Consider i) Find the vector fields $V_1, V_2, V_3$ which generate the following smooth one-parameter groups of transformations of $\mathbb{R}$ : $$ x \mapsto \psi_1^s x=x+s, \quad x \mapsto \psi_2^s x=e^s x, \quad x \mapsto \psi_3^s x=\frac{x}{1-s x} . $$ ii) Deduce that these vector fields generate a group of transformations of the form $$ x \mapsto \frac{a x+b}{c x+d}, \quad a d-b c=1 . $$ I don't understand what is meant by a group of transformations generated by the vector fields. I don't understand how we generate elements from these. Could someone elaborate on this?",Consider i) Find the vector fields which generate the following smooth one-parameter groups of transformations of : ii) Deduce that these vector fields generate a group of transformations of the form I don't understand what is meant by a group of transformations generated by the vector fields. I don't understand how we generate elements from these. Could someone elaborate on this?,"V_1, V_2, V_3 \mathbb{R} 
x \mapsto \psi_1^s x=x+s, \quad x \mapsto \psi_2^s x=e^s x, \quad x \mapsto \psi_3^s x=\frac{x}{1-s x} .
 
x \mapsto \frac{a x+b}{c x+d}, \quad a d-b c=1 .
","['ordinary-differential-equations', 'differential-geometry', 'lie-groups', 'dynamical-systems', 'lie-algebras']"
97,Where did the $i$ go in the second order differential equation solution?,Where did the  go in the second order differential equation solution?,i,"Let $a\ddot{y}+b\dot{y}+cy=0$ such that $b^2-4ac<0$ , we then have a solution of the form $y=e^{\alpha t}(A\cos(\beta t)+B\sin(\beta t))$ Where $\alpha$ is the real part, and $\beta$ the absolute value of the imaginary part of the solutions of the following polynomial equation: $ax^2+bx+c=0$ However, when I try to prove this general solution, I find $y=Ae^{(\alpha+i\beta)t}+Be^{(\alpha-i\beta)t}$ $y=e^{\alpha t}(Ae^{i\beta t}+Be^{-i\beta t})$ $y=e^{\alpha t}(A(\cos(\beta t)+i\sin(\beta t))+B(\cos(-\beta t)+i\sin(-\beta t)))$ $y=e^{\alpha t}((A+B)\cos(\beta t)+i(A-B)\sin(\beta t))$ Could anyone please explain to me how we get rid of the $i$ to find the general solution?","Let such that , we then have a solution of the form Where is the real part, and the absolute value of the imaginary part of the solutions of the following polynomial equation: However, when I try to prove this general solution, I find Could anyone please explain to me how we get rid of the to find the general solution?",a\ddot{y}+b\dot{y}+cy=0 b^2-4ac<0 y=e^{\alpha t}(A\cos(\beta t)+B\sin(\beta t)) \alpha \beta ax^2+bx+c=0 y=Ae^{(\alpha+i\beta)t}+Be^{(\alpha-i\beta)t} y=e^{\alpha t}(Ae^{i\beta t}+Be^{-i\beta t}) y=e^{\alpha t}(A(\cos(\beta t)+i\sin(\beta t))+B(\cos(-\beta t)+i\sin(-\beta t))) y=e^{\alpha t}((A+B)\cos(\beta t)+i(A-B)\sin(\beta t)) i,"['ordinary-differential-equations', 'complex-numbers']"
98,Integral relating gamma function's linear subspaces,Integral relating gamma function's linear subspaces,,"I'm trying to solve the equation: $$\int_0^\infty \frac{\cos(s\ln(t)+\frac{π(4n+1)}{4})}{\sqrt{t}e^t}\mathrm{d}t=0$$ For the variable $s\in\mathbb{R}$ , where $n\in\mathbb{Z}$ is any nonzero integer. This is related to a more complicated integral equation: $$\int_0^\infty \left(\frac{\cos(s\ln(t))-\sin(s\ln(t))}{\sqrt{t}e^t}\right)\mathrm{d}t=0$$ This equation involves certain linear subspaces of the complete gamma function. More specifically, the subspace of all the vomplef numbers of the form: $a(1+i)$ where $a$ is any real number. Thanks for the help.","I'm trying to solve the equation: For the variable , where is any nonzero integer. This is related to a more complicated integral equation: This equation involves certain linear subspaces of the complete gamma function. More specifically, the subspace of all the vomplef numbers of the form: where is any real number. Thanks for the help.",\int_0^\infty \frac{\cos(s\ln(t)+\frac{π(4n+1)}{4})}{\sqrt{t}e^t}\mathrm{d}t=0 s\in\mathbb{R} n\in\mathbb{Z} \int_0^\infty \left(\frac{\cos(s\ln(t))-\sin(s\ln(t))}{\sqrt{t}e^t}\right)\mathrm{d}t=0 a(1+i) a,"['ordinary-differential-equations', 'indefinite-integrals', 'gamma-function']"
99,"How to get $\frac{d}{dt}(D_yx(t,y))=[D_xf(t,x(t,y))](D_yx(t,y)$?",How to get ?,"\frac{d}{dt}(D_yx(t,y))=[D_xf(t,x(t,y))](D_yx(t,y)","Consider the nonlinear DE $x'=f(t,x)$ where $f\in C^1$ . For a fixed $\tau$ let $x(t,y)$ be the solution of the IVP $$ x'=f(t,x), x(\tau)=y $$ How to get the equation of variation for the $n\times n$ Jacobian matrix $D_yx$ is $$ \frac{d}{dt}(D_yx(t,y))=[D_xf(t,x(t,y))](D_yx(t,y)) $$ And thus $$ \frac{d}{dt}(det(D_yx(t,y)))=tr([D_xf(t,x(t,y))])det((D_yx(t,y))) $$ I am confused about the derivative. If we take derivative of $D_y x$ w.r.t. $t$ , why we have $D_x f$ ? Is that we exchange the derivative? $$ \frac{d}{dt}(D_yx(t,y))=D_y[\frac{d}{dt}(x(t,y))]=? $$","Consider the nonlinear DE where . For a fixed let be the solution of the IVP How to get the equation of variation for the Jacobian matrix is And thus I am confused about the derivative. If we take derivative of w.r.t. , why we have ? Is that we exchange the derivative?","x'=f(t,x) f\in C^1 \tau x(t,y) 
x'=f(t,x), x(\tau)=y
 n\times n D_yx 
\frac{d}{dt}(D_yx(t,y))=[D_xf(t,x(t,y))](D_yx(t,y))
 
\frac{d}{dt}(det(D_yx(t,y)))=tr([D_xf(t,x(t,y))])det((D_yx(t,y)))
 D_y x t D_x f 
\frac{d}{dt}(D_yx(t,y))=D_y[\frac{d}{dt}(x(t,y))]=?
","['ordinary-differential-equations', 'analysis']"
