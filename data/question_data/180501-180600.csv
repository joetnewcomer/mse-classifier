,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Mulltivariable limit quesion and my attempt,Mulltivariable limit quesion and my attempt,,"Given  $$ F(x,y) = \left\{ \begin{array}{ll} 0 & (x,y)=(2y,y) \\ \exp \biggl( \frac{|x-2y|}{x^2 -4xy +4y^2} \biggr) & (x, y) \ne (2y, y) \end{array} \right. $$ Task is to examine whether function is continuous at origin and my attempt is as follows:","Given  $$ F(x,y) = \left\{ \begin{array}{ll} 0 & (x,y)=(2y,y) \\ \exp \biggl( \frac{|x-2y|}{x^2 -4xy +4y^2} \biggr) & (x, y) \ne (2y, y) \end{array} \right. $$ Task is to examine whether function is continuous at origin and my attempt is as follows:",,"['limits', 'multivariable-calculus', 'continuity']"
1,Show that f solves the so called wave equation,Show that f solves the so called wave equation,,"Task $\text{Let } \; c \in \mathbb{R} \; \text{ be a given parameter, with } \; c > 0$ $\text{ Show that } \; f: (\mathbb{R}^3 \setminus \{ \vec{0} \}) \times \mathbb{R} \to \mathbb{R} \; \text{ with:}$ $$ f(x,y,z,t) :=  \frac {\cos(\|(x,y,z)\|_2 -ct)}{\|(x,y,z)\|_2} $$ $\text{ solves the so called }$ wave equation : $$ \frac {\partial^²f}{\partial x^2}(x,y,z,t) + \frac {\partial^²f}{\partial y^2}(x,y,z,t) + \frac {\partial^²f}{\partial z^2}(x,y,z,t) - \frac {1}{c^2} \frac {\partial^²f}{\partial t^2}(x,y,z,t) = 0 $$ $\text{for all } \; (x,y,z,t) \in (\mathbb{R}^3 \setminus \{ \vec{0} \}) \times \mathbb{R}$ This task seems to require more thinking and knowledge than actual calculations, since calculating all those by hand and adding them up will only lead to a giant unsolvable equation IMHO. There is a extra hint given to support this: You may use following rule: $ \text{Let } \; g \in \mathscr{C}^2(\mathbb{R}), r: \mathbb{R}^3  \setminus \{ \vec{0} \} \to \mathbb{R} \text{ with } r(\vec{x}) := \|\vec{x}\|_2 \; \text { and } \; \vec{x} \in \mathbb{R}^3  \setminus \{ \vec{0} \} \; \text{ then: }$ $$ \frac {\partial^2 (g \circ r)}{\partial x_i^2}(\vec{x}) = \frac {g'(r(\vec{x}))}{r(\vec{x})} + \left( g''(r(\vec{x})) - \frac {g'(r(\vec{x}))}{r(\vec{x})} \right) \frac {x_i^2}{r^2(\vec{x})} \quad \text {for $\quad$ i = 1,2,3} $$ My Efforts I tried to simply calculate the partial derivates for x,y,z and t and put them into the equation, but even with the use of the hint I ended up with a giant mess. The partial derivates for t should be fairly easy: $$ \frac {\partial^2 f}{\partial t^2} = -\frac {c^2\cos(\|(x,y,z)\|_2-ct)}{\|(x,y,z)\|_2} $$ But I struggle to make use of the hint, after a couple of tries I came up with this: $$ \text{Let } \; g(x) = \frac{\cos(x-ct)}{x} \; \text{ and } \; \vec{x} = (x,y,z) $$ $$ => (g \circ r)(\vec{x}) = f(\vec{x},t) = f(x,y,z,t) $$ $$ => \frac {\partial^2 f}{\partial x_i^2} (\vec{x},t) = \frac {\partial^2 (g \circ r)}{\partial x_i^2} (\vec{x})$$ $$ = -\frac{\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_2} + \left( \frac{2\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + (2 - \|\vec{x}\|_4) \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_6}  +  \frac{\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_2} \right)  *  \frac{x_i^2}{\|\vec{x}\|_4} $$ After tons of simplifications (since the derivates for the $x_i$'s are almost the same) e.g.: $$ \|\vec{x}\|_2 = \|(x,y,z)\|_2 = \left( \sqrt{x^2+y^2+z^2} \right)^2 = x^2 + y^2 + z^2 $$ $$ ... \frac{x^2}{\|\vec{x}\|_4} + ... \frac{y^2}{\|\vec{x}\|_4} + ... \frac{z^2}{\|\vec{x}\|_4} = \frac{\|\vec{x}\|_2}{\|\vec{x}\|_4} = \frac{1}{\|\vec{x}\|_2} $$ I came up with this equation: $$ -3 * \left( \frac{\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_2} \right)  +  \frac{2\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + (2 - \|\vec{x}\|_4) \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_8}  +  \frac{\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_4}   +  \frac{\cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_2} = 0 $$ The solution I can think of right now would be to set $\|\vec{x}\|_2$ = $1$: $$ -3 * ( \sin(1 - ct) + \cos(1 - ct) ) + 2 \sin(1 - ct) + \cos(1 - ct) + \sin(1 - ct) + \cos(1 - ct)  + \cos(1 - ct) $$ $$ = -3\sin(1 - ct) -3\cos(1 - ct) + 3 \sin(1 - ct) + 3\cos(1 - ct) = 0 $$ But that seems more like a stupid hack instead of a solution, since this does not solve the equation for every $(x,y,z) \in \mathbb{R}^3$ but only for those where $\|\vec{x}\|_2$ = $1$. But it solves the equation for every $t \in \mathbb{R}$ Question What is that fact, relation or obvious simplification I'm missing here? Since this is a quite unusual amount of work that needs to be put into calculating those per hand.","Task $\text{Let } \; c \in \mathbb{R} \; \text{ be a given parameter, with } \; c > 0$ $\text{ Show that } \; f: (\mathbb{R}^3 \setminus \{ \vec{0} \}) \times \mathbb{R} \to \mathbb{R} \; \text{ with:}$ $$ f(x,y,z,t) :=  \frac {\cos(\|(x,y,z)\|_2 -ct)}{\|(x,y,z)\|_2} $$ $\text{ solves the so called }$ wave equation : $$ \frac {\partial^²f}{\partial x^2}(x,y,z,t) + \frac {\partial^²f}{\partial y^2}(x,y,z,t) + \frac {\partial^²f}{\partial z^2}(x,y,z,t) - \frac {1}{c^2} \frac {\partial^²f}{\partial t^2}(x,y,z,t) = 0 $$ $\text{for all } \; (x,y,z,t) \in (\mathbb{R}^3 \setminus \{ \vec{0} \}) \times \mathbb{R}$ This task seems to require more thinking and knowledge than actual calculations, since calculating all those by hand and adding them up will only lead to a giant unsolvable equation IMHO. There is a extra hint given to support this: You may use following rule: $ \text{Let } \; g \in \mathscr{C}^2(\mathbb{R}), r: \mathbb{R}^3  \setminus \{ \vec{0} \} \to \mathbb{R} \text{ with } r(\vec{x}) := \|\vec{x}\|_2 \; \text { and } \; \vec{x} \in \mathbb{R}^3  \setminus \{ \vec{0} \} \; \text{ then: }$ $$ \frac {\partial^2 (g \circ r)}{\partial x_i^2}(\vec{x}) = \frac {g'(r(\vec{x}))}{r(\vec{x})} + \left( g''(r(\vec{x})) - \frac {g'(r(\vec{x}))}{r(\vec{x})} \right) \frac {x_i^2}{r^2(\vec{x})} \quad \text {for $\quad$ i = 1,2,3} $$ My Efforts I tried to simply calculate the partial derivates for x,y,z and t and put them into the equation, but even with the use of the hint I ended up with a giant mess. The partial derivates for t should be fairly easy: $$ \frac {\partial^2 f}{\partial t^2} = -\frac {c^2\cos(\|(x,y,z)\|_2-ct)}{\|(x,y,z)\|_2} $$ But I struggle to make use of the hint, after a couple of tries I came up with this: $$ \text{Let } \; g(x) = \frac{\cos(x-ct)}{x} \; \text{ and } \; \vec{x} = (x,y,z) $$ $$ => (g \circ r)(\vec{x}) = f(\vec{x},t) = f(x,y,z,t) $$ $$ => \frac {\partial^2 f}{\partial x_i^2} (\vec{x},t) = \frac {\partial^2 (g \circ r)}{\partial x_i^2} (\vec{x})$$ $$ = -\frac{\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_2} + \left( \frac{2\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + (2 - \|\vec{x}\|_4) \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_6}  +  \frac{\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_2} \right)  *  \frac{x_i^2}{\|\vec{x}\|_4} $$ After tons of simplifications (since the derivates for the $x_i$'s are almost the same) e.g.: $$ \|\vec{x}\|_2 = \|(x,y,z)\|_2 = \left( \sqrt{x^2+y^2+z^2} \right)^2 = x^2 + y^2 + z^2 $$ $$ ... \frac{x^2}{\|\vec{x}\|_4} + ... \frac{y^2}{\|\vec{x}\|_4} + ... \frac{z^2}{\|\vec{x}\|_4} = \frac{\|\vec{x}\|_2}{\|\vec{x}\|_4} = \frac{1}{\|\vec{x}\|_2} $$ I came up with this equation: $$ -3 * \left( \frac{\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_2} \right)  +  \frac{2\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + (2 - \|\vec{x}\|_4) \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_8}  +  \frac{\|\vec{x}\|_2 \sin(\|\vec{x}\|_2 - ct) + \cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_4}   +  \frac{\cos(\|\vec{x}\|_2 - ct)}{\|\vec{x}\|_2} = 0 $$ The solution I can think of right now would be to set $\|\vec{x}\|_2$ = $1$: $$ -3 * ( \sin(1 - ct) + \cos(1 - ct) ) + 2 \sin(1 - ct) + \cos(1 - ct) + \sin(1 - ct) + \cos(1 - ct)  + \cos(1 - ct) $$ $$ = -3\sin(1 - ct) -3\cos(1 - ct) + 3 \sin(1 - ct) + 3\cos(1 - ct) = 0 $$ But that seems more like a stupid hack instead of a solution, since this does not solve the equation for every $(x,y,z) \in \mathbb{R}^3$ but only for those where $\|\vec{x}\|_2$ = $1$. But it solves the equation for every $t \in \mathbb{R}$ Question What is that fact, relation or obvious simplification I'm missing here? Since this is a quite unusual amount of work that needs to be put into calculating those per hand.",,"['multivariable-calculus', 'trigonometry', 'partial-differential-equations', 'partial-derivative']"
2,Calculating marginal probability density when multivariate pdf's support is $0<y<2$ and $y<x<3$,Calculating marginal probability density when multivariate pdf's support is  and,0<y<2 y<x<3,"Suppose that multivariate pdf $f(x,y)$'s support is in $0<y<2$ and $y<x<3$. I now want to calculate marginal probability density function $f_X(x)$ and $f_Y(y)$. But arranging terms only get me to $0<y<x<2$. How do I apply calculus here?","Suppose that multivariate pdf $f(x,y)$'s support is in $0<y<2$ and $y<x<3$. I now want to calculate marginal probability density function $f_X(x)$ and $f_Y(y)$. But arranging terms only get me to $0<y<x<2$. How do I apply calculus here?",,"['probability', 'multivariable-calculus']"
3,"calculating partial derivatives at $(0,0)$",calculating partial derivatives at,"(0,0)","Let $f:\mathbb R^2 \to \mathbb R$ given by := $$f(x,y) = \begin{cases} 0 & \text{, if xy=0  } \\ 1 & \text{, if xy $\neq$ 0} \end{cases}$$ I've to show that $\partial_1 f(0,0)=0=\partial_2 f(0,0)$. Also show that $f$ is not continuous at $0$.. I don't know how to calculate partial derivatives in this case.please if anyone can explain it to me...","Let $f:\mathbb R^2 \to \mathbb R$ given by := $$f(x,y) = \begin{cases} 0 & \text{, if xy=0  } \\ 1 & \text{, if xy $\neq$ 0} \end{cases}$$ I've to show that $\partial_1 f(0,0)=0=\partial_2 f(0,0)$. Also show that $f$ is not continuous at $0$.. I don't know how to calculate partial derivatives in this case.please if anyone can explain it to me...",,['multivariable-calculus']
4,Lagrange multipliers and calculus,Lagrange multipliers and calculus,,"I'm trying to solve this problema but I don't know what else to do: Let A be a nonzero symmetric 3x3 matrix. Thus, its entries satisfy $a(ij) = a(ji)$. Consider the function $f(x) = (1/2)(Ax)\cdot x$. What are the maxima and minima for $f$ on $B = {(a, b, c) | a^2 + b^2 +c^2 \leq 1}$? I have learned to find the gradient of both of these and use lagrange multipliers. I find out that $\nabla f=A \cdot x$, and I define $g(x)=\| x \|^2$ so $\nabla g= 2 x $ where x is a vector in $\mathbb{R^3}$.","I'm trying to solve this problema but I don't know what else to do: Let A be a nonzero symmetric 3x3 matrix. Thus, its entries satisfy $a(ij) = a(ji)$. Consider the function $f(x) = (1/2)(Ax)\cdot x$. What are the maxima and minima for $f$ on $B = {(a, b, c) | a^2 + b^2 +c^2 \leq 1}$? I have learned to find the gradient of both of these and use lagrange multipliers. I find out that $\nabla f=A \cdot x$, and I define $g(x)=\| x \|^2$ so $\nabla g= 2 x $ where x is a vector in $\mathbb{R^3}$.",,"['multivariable-calculus', 'lagrange-multiplier']"
5,"How do I solve the limit $\lim_{(x,y)\to(0,0)} \frac{1-\cos(x^2+y^2)}{x^2y^2(x^2+y^2)}$?",How do I solve the limit ?,"\lim_{(x,y)\to(0,0)} \frac{1-\cos(x^2+y^2)}{x^2y^2(x^2+y^2)}","I've just started solving 2-variable limits and I'm stuck at one of the examples: $$\lim_{(x,y)\to(0,0)} \frac{1-\cos(x^2+y^2)}{x^2y^2(x^2+y^2)}$$ How do I approach limits like that? I've been thinking on it for quite a long time and I don't know what could I possibly do. Intuition (which may be wrong) tells me that this limit exists and is equal to $0$ because of the nominator, but as I said - I don't seem to find any way to solve it.","I've just started solving 2-variable limits and I'm stuck at one of the examples: $$\lim_{(x,y)\to(0,0)} \frac{1-\cos(x^2+y^2)}{x^2y^2(x^2+y^2)}$$ How do I approach limits like that? I've been thinking on it for quite a long time and I don't know what could I possibly do. Intuition (which may be wrong) tells me that this limit exists and is equal to $0$ because of the nominator, but as I said - I don't seem to find any way to solve it.",,"['limits', 'multivariable-calculus']"
6,Expressing a unit tangent vector in terms of r(t),Expressing a unit tangent vector in terms of r(t),,"Is there a simple way to express $N(t)$, the unit normal vector of a vector curve, in terms of $r(t)$?  I know that $T(t)$=$\frac{r'(t)}{||r'(t)}||$ and that $N(t)$=$\frac{T'(t)}{||T'(t)||}$.  Is it possible to simplify the definition of $N(t)$, or is the simplest version [$\frac{r'(t)}{||r'(t)||}$]'? Why is $N(t)$ not defined as just $\frac{r''(t)}{||r''(t)||}$?","Is there a simple way to express $N(t)$, the unit normal vector of a vector curve, in terms of $r(t)$?  I know that $T(t)$=$\frac{r'(t)}{||r'(t)}||$ and that $N(t)$=$\frac{T'(t)}{||T'(t)||}$.  Is it possible to simplify the definition of $N(t)$, or is the simplest version [$\frac{r'(t)}{||r'(t)||}$]'? Why is $N(t)$ not defined as just $\frac{r''(t)}{||r''(t)||}$?",,"['multivariable-calculus', 'derivatives', 'vectors']"
7,Volume of sphere with triple integral,Volume of sphere with triple integral,,"Using the same notations as in this picture : The element of volume is: $r^2 \sin(\theta) \, dr \, d\theta \, d\phi$ If I try to create the volume visually, I begin with integrating $r$ between $0$ and $R$ to get the radius. Now I can either: integrate $\theta$ between $0$ and $\pi$ to have the area of half a circle, and then integrate $\phi$ between $0$ and $2\pi$ to have the full sphere volume integrate $\theta$ between $0$ and $2\pi$ to have the area of a circle, and then integrate $\phi$ between $0$ and $\pi$ to have the full sphere volume Of course the second method won't work because $$\int_0^{2\pi} \sin(\theta)d\theta = 0.$$ But I don't understand why it works visually but it fails when I write it down. I'm not sure there is something to understand but it still disturbs me...","Using the same notations as in this picture : The element of volume is: $r^2 \sin(\theta) \, dr \, d\theta \, d\phi$ If I try to create the volume visually, I begin with integrating $r$ between $0$ and $R$ to get the radius. Now I can either: integrate $\theta$ between $0$ and $\pi$ to have the area of half a circle, and then integrate $\phi$ between $0$ and $2\pi$ to have the full sphere volume integrate $\theta$ between $0$ and $2\pi$ to have the area of a circle, and then integrate $\phi$ between $0$ and $\pi$ to have the full sphere volume Of course the second method won't work because $$\int_0^{2\pi} \sin(\theta)d\theta = 0.$$ But I don't understand why it works visually but it fails when I write it down. I'm not sure there is something to understand but it still disturbs me...",,"['integration', 'multivariable-calculus', 'volume', 'spherical-coordinates']"
8,Express partial derivatives of second order (and the Laplacian) in polar coordinates [duplicate],Express partial derivatives of second order (and the Laplacian) in polar coordinates [duplicate],,"This question already has an answer here : Computing second partial derivative with polar coordinates (1 answer) Closed 6 years ago . $z=f(x,y)$ where $x=rcosθ$ and $y=rsinθ$ Find $ \frac{\partial z}{\partial x}$ and $ \frac{\partial^2 z}{\partial x^2}$ I'm having big troubles with using chain rule, in particularly the second derivative. Spent 2 hours on this already. What I have... $ \frac{\partial z}{\partial x}=\frac{\partial r}{\partial x}\frac{\partial z}{\partial r}+\frac{\partial z}{\partial θ}\frac{\partial θ}{\partial x}$. So I'm guessing that $\frac{\partial z}{\partial r}$ and $\frac{\partial z}{\partial θ}$ are unkown? And I will need to use them to differentiate again. Any help is really welcome. This is simply only part of a bigger question. Full Question:","This question already has an answer here : Computing second partial derivative with polar coordinates (1 answer) Closed 6 years ago . $z=f(x,y)$ where $x=rcosθ$ and $y=rsinθ$ Find $ \frac{\partial z}{\partial x}$ and $ \frac{\partial^2 z}{\partial x^2}$ I'm having big troubles with using chain rule, in particularly the second derivative. Spent 2 hours on this already. What I have... $ \frac{\partial z}{\partial x}=\frac{\partial r}{\partial x}\frac{\partial z}{\partial r}+\frac{\partial z}{\partial θ}\frac{\partial θ}{\partial x}$. So I'm guessing that $\frac{\partial z}{\partial r}$ and $\frac{\partial z}{\partial θ}$ are unkown? And I will need to use them to differentiate again. Any help is really welcome. This is simply only part of a bigger question. Full Question:",,"['multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'polar-coordinates', 'laplacian']"
9,"Proof $\phi(r\cos\theta,r\sin\theta)=\theta$ is not Lipschitz.",Proof  is not Lipschitz.,"\phi(r\cos\theta,r\sin\theta)=\theta","Taking $S=\lbrace(x,y)\in\mathbf{R}^2:1<x^2+y^2<9\rbrace\backslash ( [0,\infty)\times\lbrace 0\rbrace),$ and defining $\phi:S\to\mathbf{R}$ as $\phi(r\cos\theta,r\sin\theta)=\theta$ for $1<r<3$ and $0<\theta<2\pi,$ how would I go about showing that $\phi$ is not Lipschitz? I have been given a comment that $\phi$ is a $C^\infty$-function and any partial derivative of $\phi$ will be bounded, but I'm not sure how to proceed. Please can someone explain or suggest a method of attack to this? I know I must show that there is no $K\in\mathbf{R}$ such that $|\phi(v)-\phi(u)|\leq K\|v-u\|$ for all $v,u\in S,$ but I don't know which theorems are useful here, and the result is not intuitive to me. Even just a name of a theorem to help would be great.","Taking $S=\lbrace(x,y)\in\mathbf{R}^2:1<x^2+y^2<9\rbrace\backslash ( [0,\infty)\times\lbrace 0\rbrace),$ and defining $\phi:S\to\mathbf{R}$ as $\phi(r\cos\theta,r\sin\theta)=\theta$ for $1<r<3$ and $0<\theta<2\pi,$ how would I go about showing that $\phi$ is not Lipschitz? I have been given a comment that $\phi$ is a $C^\infty$-function and any partial derivative of $\phi$ will be bounded, but I'm not sure how to proceed. Please can someone explain or suggest a method of attack to this? I know I must show that there is no $K\in\mathbf{R}$ such that $|\phi(v)-\phi(u)|\leq K\|v-u\|$ for all $v,u\in S,$ but I don't know which theorems are useful here, and the result is not intuitive to me. Even just a name of a theorem to help would be great.",,"['real-analysis', 'multivariable-calculus']"
10,Are the plane and the line parallel?,Are the plane and the line parallel?,,"Problem In the picture above are given the coordinates of the points $O(0,0,0)$, $A(6,0,0)$, $C(0,12,0)$, $D(0,0,5)$, $K(0,6,5)$, $L(6,12,4)$, $M(6,8,0)$, $N(0,8,0)$. It seems as if line $KL$ is parallel to plane $DEM$. Show if they are indeed parallel or not. My attack I set up an equation for plane $DEM$: $5y+8z=40$. Then, I set up a vector equation for line $KL$: $\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}0\\6\\5\end{pmatrix}+\lambda\begin{pmatrix}6\\6\\-1\end{pmatrix}$. I plugged in $y=6+6\lambda$ and $z=5-\lambda$ into the equation for plane $DEM$, and ended up with $\lambda=-\frac{15}{11}$. This means they intersect at a point with $\lambda=-\frac{15}{11}$. But , my book says they are parallel. Am I wrong or is my book wrong? Thanks for the help. @Minibill, how do you explain the visible intersection in this plot of the line and the plane?","Problem In the picture above are given the coordinates of the points $O(0,0,0)$, $A(6,0,0)$, $C(0,12,0)$, $D(0,0,5)$, $K(0,6,5)$, $L(6,12,4)$, $M(6,8,0)$, $N(0,8,0)$. It seems as if line $KL$ is parallel to plane $DEM$. Show if they are indeed parallel or not. My attack I set up an equation for plane $DEM$: $5y+8z=40$. Then, I set up a vector equation for line $KL$: $\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}0\\6\\5\end{pmatrix}+\lambda\begin{pmatrix}6\\6\\-1\end{pmatrix}$. I plugged in $y=6+6\lambda$ and $z=5-\lambda$ into the equation for plane $DEM$, and ended up with $\lambda=-\frac{15}{11}$. This means they intersect at a point with $\lambda=-\frac{15}{11}$. But , my book says they are parallel. Am I wrong or is my book wrong? Thanks for the help. @Minibill, how do you explain the visible intersection in this plot of the line and the plane?",,['multivariable-calculus']
11,"If $S \in L(X,Y)$ and lim$_{r \to 0}\frac{\|Sr\|}{\|r\|}=0$,then $S=0$.","If  and lim,then .","S \in L(X,Y) _{r \to 0}\frac{\|Sr\|}{\|r\|}=0 S=0","Here is a lemma whose proof is as under: If $S \in L(X,Y)$ and lim$_{r \to 0}\frac{\|Sr\|}{\|r\|}=0$,then $S=0$. Proof: The condition lim$_{r \to 0}\Big(\frac{\|Sr\|}{\|r\|}\Big)=0$means that for each $\epsilon \gt 0$ there is a $\delta \gt 0$ such that                   $\Big(\frac{\|Sr\|}{\|r\|}\Big)\leq \epsilon $ whenever $0\lt \|r\| \lt \delta$ Let $\text{u}\in X$ be a non-zero vector.Choose a non-zero $t\in \mathbb R$ so that $\|t\text{u}\|\lt \delta $ .Then            $\Big(\frac{\|S(t\text{u})\|}{\|t\text{u}\|}\Big)= \Big(\frac{\|S\text{u}\|}{\|\text{u}\|}\Big)\leq \epsilon $ and therefore $\|S\text{u}\|\leq\epsilon \|\text{u}\|.$ This is true for any $\epsilon \gt 0.$Hence $S\text{u}=0$ for all $u \in X$ This means that $S=0$ $---------------------------------------$ I can't understand the step why did we take a vector $\text{u}\in X$ and then introduce $t$ in proof?  Please help....","Here is a lemma whose proof is as under: If $S \in L(X,Y)$ and lim$_{r \to 0}\frac{\|Sr\|}{\|r\|}=0$,then $S=0$. Proof: The condition lim$_{r \to 0}\Big(\frac{\|Sr\|}{\|r\|}\Big)=0$means that for each $\epsilon \gt 0$ there is a $\delta \gt 0$ such that                   $\Big(\frac{\|Sr\|}{\|r\|}\Big)\leq \epsilon $ whenever $0\lt \|r\| \lt \delta$ Let $\text{u}\in X$ be a non-zero vector.Choose a non-zero $t\in \mathbb R$ so that $\|t\text{u}\|\lt \delta $ .Then            $\Big(\frac{\|S(t\text{u})\|}{\|t\text{u}\|}\Big)= \Big(\frac{\|S\text{u}\|}{\|\text{u}\|}\Big)\leq \epsilon $ and therefore $\|S\text{u}\|\leq\epsilon \|\text{u}\|.$ This is true for any $\epsilon \gt 0.$Hence $S\text{u}=0$ for all $u \in X$ This means that $S=0$ $---------------------------------------$ I can't understand the step why did we take a vector $\text{u}\in X$ and then introduce $t$ in proof?  Please help....",,['multivariable-calculus']
12,Parallel transport along a cardioid,Parallel transport along a cardioid,,"I want to calculate an explicit example of a vector parallel transported along a cardioid to see what happens. Maybe someone could help me with that since no author of any book or pdf on the topic is capable of showing a single numerical example. So we need a vector field on a manifold (which is the cardioid itself) $X=\frac{dx^{i}}{dt}\frac{\partial}{\partial x^{i}}$ and a curve $x^{i}=x^{i}(t)$. My problem is, I'm not sure how to make up a curve + vector field on a manifold. Let's take the parametrization of the cardioid in Cartesian coordinates as $x(t)=a(1+2\cos t + \cos 2t)$ $y(t)=a(2\sin t + \sin 2t)$ (I think this could be written in polar coordinates which would make more sense, but I'm not sure what happens there) So I think this should be the curve on which the vector is transported. Now I'm not sure how to make up the vector field. For the vector field I also need a function $f$, but what function? A vector function? For example could I just take $f=r(\phi, \rho)= (\rho \cos \phi, \rho \sin \phi)$ (polar coordinates) and then $X=\frac{dx^{i}}{dt}\frac{\partial}{\partial x^{i}}= \frac{dx(t)}{dt}\frac{\partial r(\phi, \rho)}{\partial \phi}+\frac{dy(t)}{dt}\frac{\partial r(\phi,\rho)}{\partial \rho}$ ? I think this looks right since the $\frac{\partial}{\partial x^{i}}$ span the tangent space. Now how exactly does the condition for parallel transport in coordinates for this looks like?  The general formula is $\frac{\partial X^{\mu}}{dt}+ \Gamma^{\mu}_{v\lambda} \frac{\partial x^{v}(c(t))}{dt}X^{\lambda}=0$ (I know how to calculate the Levi-Civita connection with the metric,but I'm not sure about the rest)","I want to calculate an explicit example of a vector parallel transported along a cardioid to see what happens. Maybe someone could help me with that since no author of any book or pdf on the topic is capable of showing a single numerical example. So we need a vector field on a manifold (which is the cardioid itself) $X=\frac{dx^{i}}{dt}\frac{\partial}{\partial x^{i}}$ and a curve $x^{i}=x^{i}(t)$. My problem is, I'm not sure how to make up a curve + vector field on a manifold. Let's take the parametrization of the cardioid in Cartesian coordinates as $x(t)=a(1+2\cos t + \cos 2t)$ $y(t)=a(2\sin t + \sin 2t)$ (I think this could be written in polar coordinates which would make more sense, but I'm not sure what happens there) So I think this should be the curve on which the vector is transported. Now I'm not sure how to make up the vector field. For the vector field I also need a function $f$, but what function? A vector function? For example could I just take $f=r(\phi, \rho)= (\rho \cos \phi, \rho \sin \phi)$ (polar coordinates) and then $X=\frac{dx^{i}}{dt}\frac{\partial}{\partial x^{i}}= \frac{dx(t)}{dt}\frac{\partial r(\phi, \rho)}{\partial \phi}+\frac{dy(t)}{dt}\frac{\partial r(\phi,\rho)}{\partial \rho}$ ? I think this looks right since the $\frac{\partial}{\partial x^{i}}$ span the tangent space. Now how exactly does the condition for parallel transport in coordinates for this looks like?  The general formula is $\frac{\partial X^{\mu}}{dt}+ \Gamma^{\mu}_{v\lambda} \frac{\partial x^{v}(c(t))}{dt}X^{\lambda}=0$ (I know how to calculate the Levi-Civita connection with the metric,but I'm not sure about the rest)",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
13,How to prove this assertion about $\mathbb{R}^k$?,How to prove this assertion about ?,\mathbb{R}^k,"Suppose $k \geq 3$, $x$, $y \in \mathbb{R}^k$, $|x-y| = d > 0$, and $r > 0$. Then how to prove the following assertions? (a) If $2r > d$, then there are infinitely many $z \in \mathbb{R}^k$ such that $$ | z - x | = | z -y | = r.$$ (b) If $2r = d$, then there is exactly one such $z \in \mathbb{R}^k$ for which $$ | z - x | = | z -y | = r.$$ (c) If $2r < d$, then there is no $z \in \mathbb{R}^k$ such that $$ | z - x | = | z -y | = r.$$ How must these statements be modified if $k$ is $2$ or $1$?","Suppose $k \geq 3$, $x$, $y \in \mathbb{R}^k$, $|x-y| = d > 0$, and $r > 0$. Then how to prove the following assertions? (a) If $2r > d$, then there are infinitely many $z \in \mathbb{R}^k$ such that $$ | z - x | = | z -y | = r.$$ (b) If $2r = d$, then there is exactly one such $z \in \mathbb{R}^k$ for which $$ | z - x | = | z -y | = r.$$ (c) If $2r < d$, then there is no $z \in \mathbb{R}^k$ such that $$ | z - x | = | z -y | = r.$$ How must these statements be modified if $k$ is $2$ or $1$?",,"['calculus', 'algebra-precalculus', 'analysis', 'multivariable-calculus', 'euclidean-geometry']"
14,Jacobian matrix of the inverse of a bijective function,Jacobian matrix of the inverse of a bijective function,,"Let $f:\mathbb{C}^n\rightarrow\mathbb{C}^n$ be a function such that $f=f(f_1,\ldots,f_n)$ and $f_i=f_i(x_1,\ldots,x_n)$. Also, $f$ is bijective and its Jacobian matrix exists. Does$f^{-1}\,$Jacobian matrix exist?","Let $f:\mathbb{C}^n\rightarrow\mathbb{C}^n$ be a function such that $f=f(f_1,\ldots,f_n)$ and $f_i=f_i(x_1,\ldots,x_n)$. Also, $f$ is bijective and its Jacobian matrix exists. Does$f^{-1}\,$Jacobian matrix exist?",,"['calculus', 'multivariable-calculus', 'derivatives']"
15,Why don't we go beyond the Hessian in multivariate optimization?,Why don't we go beyond the Hessian in multivariate optimization?,,"In univariate optimization, we perform the first derivative test to identify stationary points and the second derivative test to classify the stationary points as minima, maxima and inconclusive. When both $f'(x)$ and $f''(x)$ are zero, we go for higher-order derivative test to find the nature of the stationary point. However, in case of multivariate optimization we don't go beyond the Hessian second derivative test . Why?","In univariate optimization, we perform the first derivative test to identify stationary points and the second derivative test to classify the stationary points as minima, maxima and inconclusive. When both $f'(x)$ and $f''(x)$ are zero, we go for higher-order derivative test to find the nature of the stationary point. However, in case of multivariate optimization we don't go beyond the Hessian second derivative test . Why?",,"['multivariable-calculus', 'optimization', 'nonlinear-optimization']"
16,A Lagrange Multiplier Problem : How to deal with this case when $b< 8$,A Lagrange Multiplier Problem : How to deal with this case when,b< 8,"I was trying to solve the following problem of several variables calculus given in my class.I am stuck in a particular case of the problem.Please help me to solve the problem.Thnx in advance. Find the shortest distance from $(0,b)$ to the parabola $x^2-16y=0$ using Langrange multiplier method. What I done so far: To minimise $f(x,y)=x^2+(y-b)^2$ subject to $g(x,y)=0$ where $g(x,y)=x^2-16y$ I formed $F(x,y)= x^2+(y-b)^2+\lambda (x^2-16y)$ and thus solving for stationary points using $$F_x=2x(\lambda+1)=0,F_y=2(y-b)-16\lambda=0,x^2-16y=0$$ I got $(0,0)$ is a staionary point and when $\lambda=1$ staionary points are $(\pm4\sqrt{b-8},b-8)$ provided $b\ge8$ In this case ($b\ge8$) minimum value of $f$ can easily checked by putting those points in $f(x,y)$ My problem is that what will happen if $b< 8$ ?How should I deal with this case? I am totally stuck here. Please help me. Thnx again.","I was trying to solve the following problem of several variables calculus given in my class.I am stuck in a particular case of the problem.Please help me to solve the problem.Thnx in advance. Find the shortest distance from $(0,b)$ to the parabola $x^2-16y=0$ using Langrange multiplier method. What I done so far: To minimise $f(x,y)=x^2+(y-b)^2$ subject to $g(x,y)=0$ where $g(x,y)=x^2-16y$ I formed $F(x,y)= x^2+(y-b)^2+\lambda (x^2-16y)$ and thus solving for stationary points using $$F_x=2x(\lambda+1)=0,F_y=2(y-b)-16\lambda=0,x^2-16y=0$$ I got $(0,0)$ is a staionary point and when $\lambda=1$ staionary points are $(\pm4\sqrt{b-8},b-8)$ provided $b\ge8$ In this case ($b\ge8$) minimum value of $f$ can easily checked by putting those points in $f(x,y)$ My problem is that what will happen if $b< 8$ ?How should I deal with this case? I am totally stuck here. Please help me. Thnx again.",,"['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
17,divergence theorem for surface of cylinder,divergence theorem for surface of cylinder,,"I need to use the divergence theorem to evaluate the surface integral $$I = \int \int F\cdot n \, dS$$ where $F= x^3 i +y^3 j +z^3 k$ and $S$ is the surface of the cylinder $x^2+y^2 =4$ between $-1<z<1$ I know that first i take the divergence of $F$ which is $3r^2$. Then in the coordinates $$\int\int\int 3r^{3/2}\,dz\,dr\,d\theta$$ I am not sure what limits are though. $\theta$ would be $0$ to $2\pi$. I think $z$ would just be $-1$ to $1$. Am I on the right track?","I need to use the divergence theorem to evaluate the surface integral $$I = \int \int F\cdot n \, dS$$ where $F= x^3 i +y^3 j +z^3 k$ and $S$ is the surface of the cylinder $x^2+y^2 =4$ between $-1<z<1$ I know that first i take the divergence of $F$ which is $3r^2$. Then in the coordinates $$\int\int\int 3r^{3/2}\,dz\,dr\,d\theta$$ I am not sure what limits are though. $\theta$ would be $0$ to $2\pi$. I think $z$ would just be $-1$ to $1$. Am I on the right track?",,['multivariable-calculus']
18,"Partial derivative of the composition $z(x,y)=u(x,-y)$",Partial derivative of the composition,"z(x,y)=u(x,-y)","I need to show this to use it in another problem: Lets say you have $u: \mathbb{R}^2 \rightarrow \mathbb{R}=u(x,y)$. Then you make a new function $z(x,y)=u(x,-y)$ I need to show that $\frac{\partial z}{\partial y}(a,b)=-\frac{\partial u}{\partial y}(a,-b)$. Is this true? How can I show it? I mean that I evaluate it in the points $(a,b)$ and $(a,-b)$.","I need to show this to use it in another problem: Lets say you have $u: \mathbb{R}^2 \rightarrow \mathbb{R}=u(x,y)$. Then you make a new function $z(x,y)=u(x,-y)$ I need to show that $\frac{\partial z}{\partial y}(a,b)=-\frac{\partial u}{\partial y}(a,-b)$. Is this true? How can I show it? I mean that I evaluate it in the points $(a,b)$ and $(a,-b)$.",,"['multivariable-calculus', 'partial-derivative']"
19,why does directional derivative move fastest along the gradient?,why does directional derivative move fastest along the gradient?,,"i have just started learning multi-variable calculus , i learned that directional derivative moves fastest along the gradient . i am not able to digest it well as for the 2-D curves that i studied a curve moves fastest along the tangent and slowest along the normal but over here it moves fastest along the gradient and slowest along the tangent plane . i am not able to understand why ?  thanks for your help","i have just started learning multi-variable calculus , i learned that directional derivative moves fastest along the gradient . i am not able to digest it well as for the 2-D curves that i studied a curve moves fastest along the tangent and slowest along the normal but over here it moves fastest along the gradient and slowest along the tangent plane . i am not able to understand why ?  thanks for your help",,['multivariable-calculus']
20,Question from Munkres Analysis on Manifolds Inverse Function Theorem Section,Question from Munkres Analysis on Manifolds Inverse Function Theorem Section,,"This is the first exercise in the section on the Inverse Function Theorem (section 8). Let $f:\Bbb R^2\to \Bbb R^2$ be defined by the equation $f(x,y)=(x^2-y^2,2xy)$. a) Show that it is one to one on the set containing all $(x,y)$ such that $x>0$. b) What is the image of the function on that set? c) If $g$ is the inverse function, find $Dg(0,1)!$ My question is mainly about part c.  Here is what I think needs to be done. Use the fact that $g(f(x,y))=(x,y)$ and differentiate to get $D(g(0,1))\cdot Df(\frac{1}{\sqrt2},\frac{1}{\sqrt2})=I$ Then multiply by the inverse matrix of $Df(\frac{1}{\sqrt2},\frac{1}{\sqrt2})$ to get $Dg(0,1)$.  Is this right?","This is the first exercise in the section on the Inverse Function Theorem (section 8). Let $f:\Bbb R^2\to \Bbb R^2$ be defined by the equation $f(x,y)=(x^2-y^2,2xy)$. a) Show that it is one to one on the set containing all $(x,y)$ such that $x>0$. b) What is the image of the function on that set? c) If $g$ is the inverse function, find $Dg(0,1)!$ My question is mainly about part c.  Here is what I think needs to be done. Use the fact that $g(f(x,y))=(x,y)$ and differentiate to get $D(g(0,1))\cdot Df(\frac{1}{\sqrt2},\frac{1}{\sqrt2})=I$ Then multiply by the inverse matrix of $Df(\frac{1}{\sqrt2},\frac{1}{\sqrt2})$ to get $Dg(0,1)$.  Is this right?",,['multivariable-calculus']
21,Domain and double integral,Domain and double integral,,"Let $$D = \{(x,y)\in R^2 : 0<x<y<2x,x^2+y^2>4,xy<4\}$$ and $f : D \rightarrow R$ the continus and bounded function defined by $f(x,y)=xy$ I'm stucked to find some bounds for $\iint_D f(x,y) \,dx\,dy$ In the book I read they say : Let define $\phi1,\phi2 : [2/\sqrt{5},2]\rightarrow R$ : $$ \phi1(x)=\left\{ 	\begin{array}{ll} 		\sqrt{4-x^2}  & \mbox{if } x \in [2/\sqrt{5},\sqrt{2}] \\ 		x & \mbox{if } x \in [\sqrt{2},2] 	\end{array} \right.$$ $$ \phi2(x)=\left\{ 	\begin{array}{ll} 		2x  & \mbox{if } x \in [2/\sqrt{5},\sqrt{2}] \\ 		4/x & \mbox{if } x \in [\sqrt{2},2] 	\end{array} \right.$$ and so $$ D=\{(x,y)\in R^2:2/\sqrt{5}<x<2,\phi1(x)<y<\phi2(x)\} $$ And then it's easy to integrate. My question : How did they found this ? I really don't understand","Let $$D = \{(x,y)\in R^2 : 0<x<y<2x,x^2+y^2>4,xy<4\}$$ and $f : D \rightarrow R$ the continus and bounded function defined by $f(x,y)=xy$ I'm stucked to find some bounds for $\iint_D f(x,y) \,dx\,dy$ In the book I read they say : Let define $\phi1,\phi2 : [2/\sqrt{5},2]\rightarrow R$ : $$ \phi1(x)=\left\{ 	\begin{array}{ll} 		\sqrt{4-x^2}  & \mbox{if } x \in [2/\sqrt{5},\sqrt{2}] \\ 		x & \mbox{if } x \in [\sqrt{2},2] 	\end{array} \right.$$ $$ \phi2(x)=\left\{ 	\begin{array}{ll} 		2x  & \mbox{if } x \in [2/\sqrt{5},\sqrt{2}] \\ 		4/x & \mbox{if } x \in [\sqrt{2},2] 	\end{array} \right.$$ and so $$ D=\{(x,y)\in R^2:2/\sqrt{5}<x<2,\phi1(x)<y<\phi2(x)\} $$ And then it's easy to integrate. My question : How did they found this ? I really don't understand",,['multivariable-calculus']
22,Uniqueness of the gradient vector,Uniqueness of the gradient vector,,"It says we can define gradient as the unique vector $\nabla f$ such that $Df(x)(v)=\langle \nabla f(x),v \rangle$ I don't understand how uniqueness is coming. I can prove uniqueness if it was given $dim(Hom(E,W))$ is finite where $f:E\subset V \to W$ and differentiable on open subset $E$ of $V$.","It says we can define gradient as the unique vector $\nabla f$ such that $Df(x)(v)=\langle \nabla f(x),v \rangle$ I don't understand how uniqueness is coming. I can prove uniqueness if it was given $dim(Hom(E,W))$ is finite where $f:E\subset V \to W$ and differentiable on open subset $E$ of $V$.",,['multivariable-calculus']
23,Question on Partial Derviatives,Question on Partial Derviatives,,"For function $f(x,y) = x^2 y$ The partial derivatives for $x$ is $2.x.y $. I'm new to such math equation and i'm learning them now.  May i know why is it so? Thanks!","For function $f(x,y) = x^2 y$ The partial derivatives for $x$ is $2.x.y $. I'm new to such math equation and i'm learning them now.  May i know why is it so? Thanks!",,"['multivariable-calculus', 'functions', 'derivatives']"
24,Gradient of modulus of vector.,Gradient of modulus of vector.,,"I came across this in my lecture notes: This is using index notation, non-bold r is the modulus of r , and the partials are with respect to the components of r . I understand most of the steps, but I don't understand how they get from $$\partial_i \sqrt{r_j r_j}$$ to $$\frac{(\partial_i r_j)r_j}{\sqrt{r_j r_j}}$$ Can anybody help?","I came across this in my lecture notes: This is using index notation, non-bold r is the modulus of r , and the partials are with respect to the components of r . I understand most of the steps, but I don't understand how they get from $$\partial_i \sqrt{r_j r_j}$$ to $$\frac{(\partial_i r_j)r_j}{\sqrt{r_j r_j}}$$ Can anybody help?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
25,Is it true that $d\textbf{S} = dy dz\textbf{ i }+ dx dz\textbf{ j }+ dx dy\textbf{ k }$,Is it true that,d\textbf{S} = dy dz\textbf{ i }+ dx dz\textbf{ j }+ dx dy\textbf{ k },"I came up with this in my mind, Just wondering if it is true I am thinking about it too, will post my observations, if any","I came up with this in my mind, Just wondering if it is true I am thinking about it too, will post my observations, if any",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'surfaces']"
26,Derivative of matrix vector dot product with respect to matrix,Derivative of matrix vector dot product with respect to matrix,,"Given the function $$f(N) = x_1^T M x_2 $$ where $x_1 = Nv_1 $ $x_2 = Nv_2 $ $x_1, x_2, v_1, v_2$ are vectors with dimension $n \times 1$ $M$ and $N$ are matrices with dimension $n \times n$ what's the derivative of $f(N)$ with respect to $N$?","Given the function $$f(N) = x_1^T M x_2 $$ where $x_1 = Nv_1 $ $x_2 = Nv_2 $ $x_1, x_2, v_1, v_2$ are vectors with dimension $n \times 1$ $M$ and $N$ are matrices with dimension $n \times n$ what's the derivative of $f(N)$ with respect to $N$?",,"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'scalar-fields']"
27,Constants in multivariable integration,Constants in multivariable integration,,"I'm reviewing multivariable integrals, and the constants are confusing me. If I have: $$ f(x, y) = \int \frac{\partial f(x,y)}{\partial x}  dx $$ $$ f(x, y) = \int 2xy \,dx $$ factor out $y$ which is treated as a constant. $$ f(x, y) = y \int 2x \,dx $$ But, I expected $$ f(x, y) = 2y \int x \,dx $$ Since $2$ is also a constant. The question is, why factor out $y$ and not $2$ as well. What am I missing?","I'm reviewing multivariable integrals, and the constants are confusing me. If I have: $$ f(x, y) = \int \frac{\partial f(x,y)}{\partial x}  dx $$ $$ f(x, y) = \int 2xy \,dx $$ factor out $y$ which is treated as a constant. $$ f(x, y) = y \int 2x \,dx $$ But, I expected $$ f(x, y) = 2y \int x \,dx $$ Since $2$ is also a constant. The question is, why factor out $y$ and not $2$ as well. What am I missing?",,['multivariable-calculus']
28,On the Continuity of the Jacobian of a diffeomorphism,On the Continuity of the Jacobian of a diffeomorphism,,"Let $\phi:U\longrightarrow V$ be a diffeomorphism between the open sets $U, V\subseteq \mathbb R^n$. Provided $J\phi(x)\neq 0$ for all $x\in U$ we have a map $$J\phi:U\longrightarrow GL_n(\mathbb R), x\mapsto J\phi(x).$$ Is this map continuous? Above $J\phi(x)$ is the Jacobian of $\phi$.","Let $\phi:U\longrightarrow V$ be a diffeomorphism between the open sets $U, V\subseteq \mathbb R^n$. Provided $J\phi(x)\neq 0$ for all $x\in U$ we have a map $$J\phi:U\longrightarrow GL_n(\mathbb R), x\mapsto J\phi(x).$$ Is this map continuous? Above $J\phi(x)$ is the Jacobian of $\phi$.",,"['real-analysis', 'multivariable-calculus', 'vector-analysis']"
29,Finding the partial derivatives of this function,Finding the partial derivatives of this function,,"Let $g \in C^1(\mathbb R)$ be a real valued function and $f$ defined by $$f(u,v,w) = \int_{u}^{v} g(w^2+\sqrt{s})\,\,ds$$ where $u,v,w \in \mathbb R$ and $u,v>0$. Find all partial derivatives. I'm not sure how to attempt this problem. I assume if it were a function of two variables, say something like $$f(x,w) = \int_{0}^{x} g(w^2+\sqrt{s})\,\,ds$$ then for example the partial derivative with respect to $x$ would just be $g(w^2 + \sqrt{x})$ (is that true?). Anyhow, some hint or strategy would be very welcomed.","Let $g \in C^1(\mathbb R)$ be a real valued function and $f$ defined by $$f(u,v,w) = \int_{u}^{v} g(w^2+\sqrt{s})\,\,ds$$ where $u,v,w \in \mathbb R$ and $u,v>0$. Find all partial derivatives. I'm not sure how to attempt this problem. I assume if it were a function of two variables, say something like $$f(x,w) = \int_{0}^{x} g(w^2+\sqrt{s})\,\,ds$$ then for example the partial derivative with respect to $x$ would just be $g(w^2 + \sqrt{x})$ (is that true?). Anyhow, some hint or strategy would be very welcomed.",,"['multivariable-calculus', 'partial-derivative']"
30,Explanation of Lagrange Equation with Chain Rule,Explanation of Lagrange Equation with Chain Rule,,"I am just reading through some lecture notes explaining the Lagrange Equation, and I am a bit confused with some chain rule stuff, I get to the part with: $$\frac{\partial F}{\partial y} = \frac{d}{dx}(\frac{\partial F}{\partial y'})$$ Here is where I am confused, it says, expanding the total derivative on the right using the chain rule: $$\frac{\partial F}{\partial y} = \frac{\partial }{\partial x} (\frac{\partial F}{\partial y'}) + \frac{\partial }{\partial y} (\frac{\partial F}{\partial y'})y' + \frac{\partial }{\partial y'} (\frac{\partial F}{\partial y'})y''   $$ Could someone give me a 'dummies' explanation of the intermediate step here(perhaps with diagram would help)? Just a bit confused. Many Thanks","I am just reading through some lecture notes explaining the Lagrange Equation, and I am a bit confused with some chain rule stuff, I get to the part with: $$\frac{\partial F}{\partial y} = \frac{d}{dx}(\frac{\partial F}{\partial y'})$$ Here is where I am confused, it says, expanding the total derivative on the right using the chain rule: $$\frac{\partial F}{\partial y} = \frac{\partial }{\partial x} (\frac{\partial F}{\partial y'}) + \frac{\partial }{\partial y} (\frac{\partial F}{\partial y'})y' + \frac{\partial }{\partial y'} (\frac{\partial F}{\partial y'})y''   $$ Could someone give me a 'dummies' explanation of the intermediate step here(perhaps with diagram would help)? Just a bit confused. Many Thanks",,"['multivariable-calculus', 'euler-lagrange-equation']"
31,Multiplying partial derivatives by normal derivatives?,Multiplying partial derivatives by normal derivatives?,,"I'm taking a course on multivariable calculus. The professor wrote the following: $f=uv, u=u(t), v=v(t)$ $\frac{d(uv)}{dt} = f_u\frac{du}{dt} + f_v\frac{dv}{dt}=v\frac{du}{dt}+u\frac{dv}{dt}$ Here's the little I understand: $f$ is a function of $u, v$ . $u$ and $v$ are functions of $t$ . The derivative of $f$ with respect to $t$ , which is written as $\frac{d(uv)}{dt}$ , is the sum of the partial of $f$ with respect to $u$ times the derivative of $u$ with respect to $t$ and the partial of $f$ with respect to $v$ times the derivative of $v$ times $t$ . Why is this true, and what is going on in the third part of the equation (after the second equals sign)? I guess the middle part could be written as $\frac{\partial f}{\partial u}\frac{du}{dt} + \frac{\partial f}{\partial v}\frac{dv}{dt}$ , but how dows that simplify into $v\frac{du}{dt}+u\frac{dv}{dt}$ ?","I'm taking a course on multivariable calculus. The professor wrote the following: Here's the little I understand: is a function of . and are functions of . The derivative of with respect to , which is written as , is the sum of the partial of with respect to times the derivative of with respect to and the partial of with respect to times the derivative of times . Why is this true, and what is going on in the third part of the equation (after the second equals sign)? I guess the middle part could be written as , but how dows that simplify into ?","f=uv, u=u(t), v=v(t) \frac{d(uv)}{dt} = f_u\frac{du}{dt} + f_v\frac{dv}{dt}=v\frac{du}{dt}+u\frac{dv}{dt} f u, v u v t f t \frac{d(uv)}{dt} f u u t f v v t \frac{\partial f}{\partial u}\frac{du}{dt} + \frac{\partial f}{\partial v}\frac{dv}{dt} v\frac{du}{dt}+u\frac{dv}{dt}","['multivariable-calculus', 'partial-derivative']"
32,Derivation of Mahalanobis Distance,Derivation of Mahalanobis Distance,,"I was recently reading up on the Mahalanobis Distance , and understood how it generalizes distance measures for multivariate data such as the Euclidean Distance. However, what got me wondering was how does one derive the formula constructively? I understand how this general form applies in particular cases, but is there a way to construct this general formula from 'first principles'? Particularly, could someone explain what is the significance of the inverted covariance matrix? (especially in the non-trivial case when it is not a diagonal matrix)","I was recently reading up on the Mahalanobis Distance , and understood how it generalizes distance measures for multivariate data such as the Euclidean Distance. However, what got me wondering was how does one derive the formula constructively? I understand how this general form applies in particular cases, but is there a way to construct this general formula from 'first principles'? Particularly, could someone explain what is the significance of the inverted covariance matrix? (especially in the non-trivial case when it is not a diagonal matrix)",,"['linear-algebra', 'multivariable-calculus']"
33,Evaluate a Double Integral,Evaluate a Double Integral,,"Evaluate $$\int_0^1\int_0^{\sqrt{1-x^2}} e^{-(x^2+y^2)}\,dy\,dx$$ Sorry if the formatting is off Is there a way to evaluate without using polar coordinates or is that the only way to integrate this? Any help is greatly appreciated","Evaluate $$\int_0^1\int_0^{\sqrt{1-x^2}} e^{-(x^2+y^2)}\,dy\,dx$$ Sorry if the formatting is off Is there a way to evaluate without using polar coordinates or is that the only way to integrate this? Any help is greatly appreciated",,['multivariable-calculus']
34,How do I set up a triple integral with vague parameters in the z coordinate direction?,How do I set up a triple integral with vague parameters in the z coordinate direction?,,"The problem asks to find the volume of the solid bounded by $z = 16xy,~ z \geq 0,~ 0 \leq x \leq 5,~ 0 \leq y \leq 4$. But I am having trouble setting up the integral for the $z$ parameters. Any help setting up the problem would be greatly appreciated!","The problem asks to find the volume of the solid bounded by $z = 16xy,~ z \geq 0,~ 0 \leq x \leq 5,~ 0 \leq y \leq 4$. But I am having trouble setting up the integral for the $z$ parameters. Any help setting up the problem would be greatly appreciated!",,"['integration', 'multivariable-calculus']"
35,"Where is my mistake in finding $\int_D e^\frac{x-y}{x+y}\,dx\,dy$?",Where is my mistake in finding ?,"\int_D e^\frac{x-y}{x+y}\,dx\,dy","I'm trying to compute $$\int_D \! \exp\left(\frac{x-y}{x+y}\right) \, \mathrm{d}x \, \mathrm{d}y,$$ where $D$ is the region $0 \leq x \leq 1$, $0 \leq y \leq 1-x$, a triangle in the first quadrant. I tried to solve it using variable change, $t=x-y, s=x+y$, we can see that $0 \leq t \leq 1$ and $ 0\leq s \leq 1$ as those are the exreme values for $t,s$. The jacobian of this transform is $\frac{1}{2}$ So overall the integral i calculated is $$\frac{1}{2} \int_{0}^{1} \int_{0}^{1} \! e^{t/s} \, \mathrm{d}t \, \mathrm{d}s=\frac{1}{2}\int_{0}^{1} \! se^{t/s} \Bigg|_{t=0}^{t=1} \, \mathrm{d}s=\frac{1}{2} \int_{0}^{1} \! s(e^{1/s}-1) \, \mathrm{d}s$$ I don't know how to find the antiderivative of this function, but even if i did, this integral does not converge. Where is my mistake? because I know for a fact the correct answer is $\frac{e-1}{4}$.","I'm trying to compute $$\int_D \! \exp\left(\frac{x-y}{x+y}\right) \, \mathrm{d}x \, \mathrm{d}y,$$ where $D$ is the region $0 \leq x \leq 1$, $0 \leq y \leq 1-x$, a triangle in the first quadrant. I tried to solve it using variable change, $t=x-y, s=x+y$, we can see that $0 \leq t \leq 1$ and $ 0\leq s \leq 1$ as those are the exreme values for $t,s$. The jacobian of this transform is $\frac{1}{2}$ So overall the integral i calculated is $$\frac{1}{2} \int_{0}^{1} \int_{0}^{1} \! e^{t/s} \, \mathrm{d}t \, \mathrm{d}s=\frac{1}{2}\int_{0}^{1} \! se^{t/s} \Bigg|_{t=0}^{t=1} \, \mathrm{d}s=\frac{1}{2} \int_{0}^{1} \! s(e^{1/s}-1) \, \mathrm{d}s$$ I don't know how to find the antiderivative of this function, but even if i did, this integral does not converge. Where is my mistake? because I know for a fact the correct answer is $\frac{e-1}{4}$.",,"['integration', 'multivariable-calculus', 'multiple-integral']"
36,"Calculate $\int_D x^3y\ dx\,dy$",Calculate,"\int_D x^3y\ dx\,dy","Let $D$ the bounded region by the $y$-axis  and the parable $x= -4y^2 + 3$. How can I calculate the integral $$\int_D x^3y\ dx\,dy$$ I am stuck with this problem some help to solve this please.","Let $D$ the bounded region by the $y$-axis  and the parable $x= -4y^2 + 3$. How can I calculate the integral $$\int_D x^3y\ dx\,dy$$ I am stuck with this problem some help to solve this please.",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
37,About the order of integration for double integrals,About the order of integration for double integrals,,"I have to compute $\int\int (2x-y) \,dx \, dy $ on the domain $\{ (x,y) \in R^2 : 1\leq x\leq 4, 0\leq y\leq \sqrt{x} \}$ So mi first try is to do: $\int_0^{\sqrt{x}}\int_{1}^{4} (2x-y)\, dx \, dy = 15 \sqrt{x}-\frac{3}{2}x$ But if i do this like: $\int_1^4 \int_0^{\sqrt{x}} (2x-y) \,dy\, dx = \frac{421}{20}$ In the first case the result still depends on $x$, but in the second case i get a value. I know i did not changed the order of integration, but i just intepreted in diferent ways what my homework ask. Im a bit confused here, my question is: what is the correct order of integration?","I have to compute $\int\int (2x-y) \,dx \, dy $ on the domain $\{ (x,y) \in R^2 : 1\leq x\leq 4, 0\leq y\leq \sqrt{x} \}$ So mi first try is to do: $\int_0^{\sqrt{x}}\int_{1}^{4} (2x-y)\, dx \, dy = 15 \sqrt{x}-\frac{3}{2}x$ But if i do this like: $\int_1^4 \int_0^{\sqrt{x}} (2x-y) \,dy\, dx = \frac{421}{20}$ In the first case the result still depends on $x$, but in the second case i get a value. I know i did not changed the order of integration, but i just intepreted in diferent ways what my homework ask. Im a bit confused here, my question is: what is the correct order of integration?",,"['integration', 'multivariable-calculus']"
38,Interesting line integral using green's theorem,Interesting line integral using green's theorem,,"Find $$\int_C \frac{2x^3+2xy^2-2y}{ x^2+y^2} \, dx+\frac{2y^3+2x^2y+2x}{ x^2+y^2} \, dy$$ Where $C$ is any simple closed loop which contains the origin. What I figured out I cannot use the direct version of Green's theorem. I know that there is another version of greens theorem which is as follows : $$\oint_{C+S} P \,dx + Q\,dy  = \iint_D \left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y} \right)dA,$$ where $C$ and $S$ are oriented opposite to each other and $D$ is the region enclosed between the two loops. Using that I found \begin{align*} \int_{C+S} \left( 2x-\frac{2y}{ x^2+y^2}\right) \, dx+\left(2y+\frac{2x}{ x^2+y^2} \right) \, dy &= \iint_D \left[\left(2+\frac{4xy}{(x^2+y^2)^2} \right)-\left(2-\frac{4xy}{(x^2+y^2)^2} \right) \right]dA \\ &=\iint_D \frac{8xy}{(x^2+y^2)^2} dA \end{align*} The region inside any closed loop can be defined in polar coordinates as $$\{(r, \theta)\in D \mid 0\leq r\leq r(\theta), \, 0\leq \theta \leq 2\pi \}$$ In polar coordinates the integral becomes $$ =\iint_D \frac{8xy}{(x^2+y^2)^2} dA = \int_0^{2\pi}\!\!\int_0^{r(\theta)} \frac{4\sin2\theta}{r} dr\,d\theta $$ Oops, I am again stuck; it's not zero.","Find $$\int_C \frac{2x^3+2xy^2-2y}{ x^2+y^2} \, dx+\frac{2y^3+2x^2y+2x}{ x^2+y^2} \, dy$$ Where $C$ is any simple closed loop which contains the origin. What I figured out I cannot use the direct version of Green's theorem. I know that there is another version of greens theorem which is as follows : $$\oint_{C+S} P \,dx + Q\,dy  = \iint_D \left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y} \right)dA,$$ where $C$ and $S$ are oriented opposite to each other and $D$ is the region enclosed between the two loops. Using that I found \begin{align*} \int_{C+S} \left( 2x-\frac{2y}{ x^2+y^2}\right) \, dx+\left(2y+\frac{2x}{ x^2+y^2} \right) \, dy &= \iint_D \left[\left(2+\frac{4xy}{(x^2+y^2)^2} \right)-\left(2-\frac{4xy}{(x^2+y^2)^2} \right) \right]dA \\ &=\iint_D \frac{8xy}{(x^2+y^2)^2} dA \end{align*} The region inside any closed loop can be defined in polar coordinates as $$\{(r, \theta)\in D \mid 0\leq r\leq r(\theta), \, 0\leq \theta \leq 2\pi \}$$ In polar coordinates the integral becomes $$ =\iint_D \frac{8xy}{(x^2+y^2)^2} dA = \int_0^{2\pi}\!\!\int_0^{r(\theta)} \frac{4\sin2\theta}{r} dr\,d\theta $$ Oops, I am again stuck; it's not zero.",,"['calculus', 'integration', 'multivariable-calculus']"
39,"In regards to lagrange multipliers, Confusion about derivation.","In regards to lagrange multipliers, Confusion about derivation.",,"In my calculus III textbook, the following sentence is causing trouble for me and preventing me from understanding the theory behind Lagrange multipliers. ""Since the gradient vector for a given function is orthogonal to its level curves at any given point, for a level curve of $f$ to be tangent to the constraint curve $g(x,y) = 0$, the gradients of $f$ and $g$ must be parallel"" There are bits and pieces I understand, but I'm missing the holistic picture that will put my mind at ease. I'm quite certain that I understand that for a given curve $f(x,y)$, its gradient will be tangent to the level surface $f(x,y,z) = k$ because its directional derivative will be $0$. Specifically, I'm hung up on the idea that they must be parallel I cannot directly see how the case where they are anti-parallel isn't possible. Furthermore, I'm not sure why the constraint curve $g(x,y)$ is set to $0$ in this explanation. If someone could explain in detail the ideas behind this sentence, I would appreciate it.","In my calculus III textbook, the following sentence is causing trouble for me and preventing me from understanding the theory behind Lagrange multipliers. ""Since the gradient vector for a given function is orthogonal to its level curves at any given point, for a level curve of $f$ to be tangent to the constraint curve $g(x,y) = 0$, the gradients of $f$ and $g$ must be parallel"" There are bits and pieces I understand, but I'm missing the holistic picture that will put my mind at ease. I'm quite certain that I understand that for a given curve $f(x,y)$, its gradient will be tangent to the level surface $f(x,y,z) = k$ because its directional derivative will be $0$. Specifically, I'm hung up on the idea that they must be parallel I cannot directly see how the case where they are anti-parallel isn't possible. Furthermore, I'm not sure why the constraint curve $g(x,y)$ is set to $0$ in this explanation. If someone could explain in detail the ideas behind this sentence, I would appreciate it.",,"['multivariable-calculus', 'intuition', 'lagrange-multiplier']"
40,Jacobian of a transformation in cylindrical coordinates,Jacobian of a transformation in cylindrical coordinates,,"In an area called transformation optics, they transform Maxwell equations from one space coordinate system to another, and then somehow obtain the properties of background material $(\epsilon , \mu)$ in the first coordinate system, and this way find the required $(\epsilon , \mu)$ to direct EM waves in arbitrary directions. From this paper , we have a transformation defined in cylindrical coordinates as : $$\rho'=R_1+\frac{R_2-R_1}{R_2}\rho$$ $$\phi'=\phi$$ $$z'=z$$ This transformation maps the circular region $0\leq\rho \leq R$ to the circular annular region $R_1\leq \rho' \leq R_2$. In the paper, the Jacobi matrix for this transformation is written as (equation 14): $$ A=\begin{pmatrix} (R_2-R_1)/R_2 & 0 & 0\\  0 & \rho'/\rho & 0\\  0& 0 & 1 \end{pmatrix}$$ We know that A is defined as $$ A=\begin{pmatrix} \frac{\partial x'}{\partial x} & \frac{\partial x'}{\partial y} & \frac{\partial x'}{\partial z}\\  \frac{\partial y'}{\partial x} & \frac{\partial y'}{\partial y} & \frac{\partial y'}{\partial z}\\ \frac{\partial z'}{\partial x} & \frac{\partial z'}{\partial y} & \frac{\partial z'}{\partial z} \end{pmatrix}$$ Why the element $A_{22}=\rho'/\rho$ instead of $1$?  This result is obtained here , equation (11), using more accurate mathematical notation. In another transformation, defined as: $$\rho'=\rho$$ $$\phi'=\frac{a}{2l}(z+l)+\phi$$ $$z'=z$$ The matrix $A$ is obtained as: $$ A=\begin{pmatrix} 1 & 0 & 0\\  0 & 1& \frac{a}{2l}\rho'\\  0& 0 & 1 \end{pmatrix}$$ In this example, why the $A_{23}$ element is not $\frac{a}{2l}$? Note: I think it has something to do with metric tensor, etc. that I don't understand.","In an area called transformation optics, they transform Maxwell equations from one space coordinate system to another, and then somehow obtain the properties of background material $(\epsilon , \mu)$ in the first coordinate system, and this way find the required $(\epsilon , \mu)$ to direct EM waves in arbitrary directions. From this paper , we have a transformation defined in cylindrical coordinates as : $$\rho'=R_1+\frac{R_2-R_1}{R_2}\rho$$ $$\phi'=\phi$$ $$z'=z$$ This transformation maps the circular region $0\leq\rho \leq R$ to the circular annular region $R_1\leq \rho' \leq R_2$. In the paper, the Jacobi matrix for this transformation is written as (equation 14): $$ A=\begin{pmatrix} (R_2-R_1)/R_2 & 0 & 0\\  0 & \rho'/\rho & 0\\  0& 0 & 1 \end{pmatrix}$$ We know that A is defined as $$ A=\begin{pmatrix} \frac{\partial x'}{\partial x} & \frac{\partial x'}{\partial y} & \frac{\partial x'}{\partial z}\\  \frac{\partial y'}{\partial x} & \frac{\partial y'}{\partial y} & \frac{\partial y'}{\partial z}\\ \frac{\partial z'}{\partial x} & \frac{\partial z'}{\partial y} & \frac{\partial z'}{\partial z} \end{pmatrix}$$ Why the element $A_{22}=\rho'/\rho$ instead of $1$?  This result is obtained here , equation (11), using more accurate mathematical notation. In another transformation, defined as: $$\rho'=\rho$$ $$\phi'=\frac{a}{2l}(z+l)+\phi$$ $$z'=z$$ The matrix $A$ is obtained as: $$ A=\begin{pmatrix} 1 & 0 & 0\\  0 & 1& \frac{a}{2l}\rho'\\  0& 0 & 1 \end{pmatrix}$$ In this example, why the $A_{23}$ element is not $\frac{a}{2l}$? Note: I think it has something to do with metric tensor, etc. that I don't understand.",,"['calculus', 'multivariable-calculus', 'transformation']"
41,Flux of a vector field.,Flux of a vector field.,,"In $\mathbb{R}^3 \setminus \{0\}$, it's given the vector field $$\vec{E}(\vec{r}) = g(r) \vec{r}$$ where $g$ is some function of class $\mathcal{C}^\infty$ defined in $[0, + \infty[$, $\vec{r} = (x,y,z)$ and $r = \sqrt{x^2 + y^2 + z^2}$. Also, $\mathrm{div} \hspace{.5pt}\vec{E}(\vec{r}) = r g'(r) + 3g(r)$. I have to compute the flux of $\vec{E}$ through the sphere $S_a^2$. My 1st attempt : Denoting by $B_a$ the ball centered in the origin with radius $a$, and using the divergence theorem, I have: $$\int_{S_a^2} \vec{E} \cdot \vec{N} \hspace{.5pt}\mathrm{d}S = \int_{B_a} r g'(r) + 3g(r) \hspace{.5pt} \mathrm{d}V$$ Using spherical coordinates, I would calculate: $$\int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{a} \left(r g'(r) + 3g(r) \hspace{.5pt}\right)r^2 \sin{\varphi} \mathrm{d}r  \mathrm{d}\varphi  \mathrm{d}\theta = 4\pi \int_{0}^{a} \left(r g'(r) + 3g(r) \hspace{.5pt}\right)r^2 \mathrm{d}r$$ My 2nd attempt : By definition. $$\int_{S_a^2} \vec{E} \cdot \vec{N} \hspace{.5pt}\mathrm{d}S = \int_{[0,2\pi] \times [0, \pi]} \vec{E}(X(\theta, \varphi))\cdot (-a^2 \sin \varphi \cos \varphi, -a^2 \sin^2 \varphi \sin \theta, -a^2 \sin^2 \varphi \cos \theta) \mathrm{d}\theta \mathrm{d}\varphi$$ where $X$ is a parametrization of the sphere using spherical coordinates. However, I'm having trouble computing $\vec{E}(X(\theta,\varphi))$. Does not make any sense to me. I would make $(x,y,z) = (r \sin \varphi \cos \theta, \cdots )$ or $(x,y,z) = (a \sin \varphi \cos \theta, \cdots )$? In the first case, $r$ is a variable but the integral is only on $\theta$ and $\varphi$. There is no way to give an answer without it being in terms of integrals of $g$, is it? If someone know an easy way to solve this, it is also welcome. Thank you.","In $\mathbb{R}^3 \setminus \{0\}$, it's given the vector field $$\vec{E}(\vec{r}) = g(r) \vec{r}$$ where $g$ is some function of class $\mathcal{C}^\infty$ defined in $[0, + \infty[$, $\vec{r} = (x,y,z)$ and $r = \sqrt{x^2 + y^2 + z^2}$. Also, $\mathrm{div} \hspace{.5pt}\vec{E}(\vec{r}) = r g'(r) + 3g(r)$. I have to compute the flux of $\vec{E}$ through the sphere $S_a^2$. My 1st attempt : Denoting by $B_a$ the ball centered in the origin with radius $a$, and using the divergence theorem, I have: $$\int_{S_a^2} \vec{E} \cdot \vec{N} \hspace{.5pt}\mathrm{d}S = \int_{B_a} r g'(r) + 3g(r) \hspace{.5pt} \mathrm{d}V$$ Using spherical coordinates, I would calculate: $$\int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{a} \left(r g'(r) + 3g(r) \hspace{.5pt}\right)r^2 \sin{\varphi} \mathrm{d}r  \mathrm{d}\varphi  \mathrm{d}\theta = 4\pi \int_{0}^{a} \left(r g'(r) + 3g(r) \hspace{.5pt}\right)r^2 \mathrm{d}r$$ My 2nd attempt : By definition. $$\int_{S_a^2} \vec{E} \cdot \vec{N} \hspace{.5pt}\mathrm{d}S = \int_{[0,2\pi] \times [0, \pi]} \vec{E}(X(\theta, \varphi))\cdot (-a^2 \sin \varphi \cos \varphi, -a^2 \sin^2 \varphi \sin \theta, -a^2 \sin^2 \varphi \cos \theta) \mathrm{d}\theta \mathrm{d}\varphi$$ where $X$ is a parametrization of the sphere using spherical coordinates. However, I'm having trouble computing $\vec{E}(X(\theta,\varphi))$. Does not make any sense to me. I would make $(x,y,z) = (r \sin \varphi \cos \theta, \cdots )$ or $(x,y,z) = (a \sin \varphi \cos \theta, \cdots )$? In the first case, $r$ is a variable but the integral is only on $\theta$ and $\varphi$. There is no way to give an answer without it being in terms of integrals of $g$, is it? If someone know an easy way to solve this, it is also welcome. Thank you.",,"['calculus', 'integration', 'multivariable-calculus']"
42,Find the differential of $f(A)=det(A^{-1}-A)$ where $A$ is invertible.,Find the differential of  where  is invertible.,f(A)=det(A^{-1}-A) A,"The question is if $A$ is an invertible matrix with real entries of size $n$. Is $f(A)=det(A^{-1}-A)$ differentiable? and what is the differential. I think I managed to show it's differentiable. the determinant is a polynomial. If we derive it by $A_{ij}$ we will get either a polynomial or a constant, at any rate, it will be continuous,  and so all the partial derivatives are continuous, and that implies that $f$ is differentiable. But what is the differential?","The question is if $A$ is an invertible matrix with real entries of size $n$. Is $f(A)=det(A^{-1}-A)$ differentiable? and what is the differential. I think I managed to show it's differentiable. the determinant is a polynomial. If we derive it by $A_{ij}$ we will get either a polynomial or a constant, at any rate, it will be continuous,  and so all the partial derivatives are continuous, and that implies that $f$ is differentiable. But what is the differential?",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives', 'determinant']"
43,This double integral,This double integral,,$$  \int_0^1\int_0^1x^3y^2\sqrt{1+x^2+y^2}\hspace{1mm}dxdy$$ We have to compute this up to 4 decimal places,$$  \int_0^1\int_0^1x^3y^2\sqrt{1+x^2+y^2}\hspace{1mm}dxdy$$ We have to compute this up to 4 decimal places,,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
44,Gradient descent with adaptive learning ratio.,Gradient descent with adaptive learning ratio.,,"I have a neural network, trained with SGD (stochastic gradient descent) with learning ratio $\alpha$. Each iteration I try to recalculate the weights with a rule: $$\Delta \vec{w} = -\alpha \frac{\partial E}{\partial \vec{w}}$$ I know the next parameters: $E$ (softmax or MSE error, averaged by all samples of iteration), $\vec{w}$ (current weights), $\frac{\partial E}{\partial \vec{w}}$ (error gradient, averaged by all samples of iteration). Very often $\lvert\Delta \vec{w}\rvert \ll E$: the weights change too slow to learn anything. Sometimes $\lvert\Delta \vec{w}\rvert \gg E$: the weights change a too fast, that also not good. To avoid it, each time I have to manually select learning ratio for each layer of network. It is a real waste of time. Cannot you advice me a good method to select $\alpha$'s automatically?","I have a neural network, trained with SGD (stochastic gradient descent) with learning ratio $\alpha$. Each iteration I try to recalculate the weights with a rule: $$\Delta \vec{w} = -\alpha \frac{\partial E}{\partial \vec{w}}$$ I know the next parameters: $E$ (softmax or MSE error, averaged by all samples of iteration), $\vec{w}$ (current weights), $\frac{\partial E}{\partial \vec{w}}$ (error gradient, averaged by all samples of iteration). Very often $\lvert\Delta \vec{w}\rvert \ll E$: the weights change too slow to learn anything. Sometimes $\lvert\Delta \vec{w}\rvert \gg E$: the weights change a too fast, that also not good. To avoid it, each time I have to manually select learning ratio for each layer of network. It is a real waste of time. Cannot you advice me a good method to select $\alpha$'s automatically?",,"['multivariable-calculus', 'optimization', 'machine-learning', 'neural-networks']"
45,Find volume of sphere $ x^2+y^2+z^2=9$ bounded by planes $z=0$ and $z=2$ using double integral,Find volume of sphere  bounded by planes  and  using double integral, x^2+y^2+z^2=9 z=0 z=2,"Find volume of sphere $x^2+y^2+z^2=9 $ bounded by planes $z=0$ and $z=2$ using double integral I tried to take the total volume of the bigger hemisphere but i get zero, i managed to take the volume of the smaller hemisphere only.","Find volume of sphere $x^2+y^2+z^2=9 $ bounded by planes $z=0$ and $z=2$ using double integral I tried to take the total volume of the bigger hemisphere but i get zero, i managed to take the volume of the smaller hemisphere only.",,"['integration', 'multivariable-calculus']"
46,Hessian of a square root of a quadratic form,Hessian of a square root of a quadratic form,,"What is the Hessian matrix of the square root of a quadratic form: $\left(w^T H w\right)^{0.5}$? Got the gradient, $0.5  \left(w^T H w\right)^{-0.5}   ( 2   H   w)$, which gives numerically correct results, but I fail with classical differentiation rules in calculating the Hessian.","What is the Hessian matrix of the square root of a quadratic form: $\left(w^T H w\right)^{0.5}$? Got the gradient, $0.5  \left(w^T H w\right)^{-0.5}   ( 2   H   w)$, which gives numerically correct results, but I fail with classical differentiation rules in calculating the Hessian.",,"['linear-algebra', 'multivariable-calculus', 'partial-derivative']"
47,local invertibility does not imply global invertibility,local invertibility does not imply global invertibility,,"What is an example of a smooth function with continuous derivatives, that is  locally invertible but not globally, and the reason for that is not injectivity. My first idea was $f:\mathbb{R}^{2}\to \mathbb{R}^{2}$ defined by $f(x,y)=(e^{x}\cos y, e^{x}\sin y)$ everything above is satisfied except that the function is injective... What example one can take? Thanks","What is an example of a smooth function with continuous derivatives, that is  locally invertible but not globally, and the reason for that is not injectivity. My first idea was $f:\mathbb{R}^{2}\to \mathbb{R}^{2}$ defined by $f(x,y)=(e^{x}\cos y, e^{x}\sin y)$ everything above is satisfied except that the function is injective... What example one can take? Thanks",,"['calculus', 'multivariable-calculus', 'inverse']"
48,Direction of gradient from level surface?,Direction of gradient from level surface?,,"In the diagram below, we see a level surface with a gradient. As a consequence of the multivariable chain rule, the gradient is normal to the surface. That's clear to me. Why is the gradient pointing outward rather than into the sphere? I understand that if it were, it would be a negative gradient, but that's a consequence, not an explanation. Why does the gradient have to point away from a level set rather than into it?","In the diagram below, we see a level surface with a gradient. As a consequence of the multivariable chain rule, the gradient is normal to the surface. That's clear to me. Why is the gradient pointing outward rather than into the sphere? I understand that if it were, it would be a negative gradient, but that's a consequence, not an explanation. Why does the gradient have to point away from a level set rather than into it?",,"['multivariable-calculus', 'derivatives']"
49,"Partial derivative of $g(x,y)=f(h(x,y),l(x,y))$",Partial derivative of,"g(x,y)=f(h(x,y),l(x,y))","Let $f,h,l: \mathbb{R}^2 \to \mathbb{R}$ be derivable functions. If $g(x,y)=f(h(x,y),l(x,y))$, is the following formula true? $$\frac{∂g}{∂x}(x,y)= \left(\frac{∂f}{∂x}(h(x,y),l(x,y)) +\frac{∂f}{∂y}(h(x,y),l(x,y))\right)\left( \frac{∂h}{∂x}(x,y)+\frac{∂l}{∂x}(x,y)\right) $$ It looks close to the chain rule $\frac{∂f}{∂x} \cdot \frac{∂x}{∂t}$ Also does this notation $\frac{∂f}{∂x}(h(x,y),l(x,y)) $ make sense as a derivative of $f(x,y)$ by the variable $x$ and after the derivation, substituting $h,l$ in the place of $x,y$ ?","Let $f,h,l: \mathbb{R}^2 \to \mathbb{R}$ be derivable functions. If $g(x,y)=f(h(x,y),l(x,y))$, is the following formula true? $$\frac{∂g}{∂x}(x,y)= \left(\frac{∂f}{∂x}(h(x,y),l(x,y)) +\frac{∂f}{∂y}(h(x,y),l(x,y))\right)\left( \frac{∂h}{∂x}(x,y)+\frac{∂l}{∂x}(x,y)\right) $$ It looks close to the chain rule $\frac{∂f}{∂x} \cdot \frac{∂x}{∂t}$ Also does this notation $\frac{∂f}{∂x}(h(x,y),l(x,y)) $ make sense as a derivative of $f(x,y)$ by the variable $x$ and after the derivation, substituting $h,l$ in the place of $x,y$ ?",,"['multivariable-calculus', 'partial-derivative']"
50,"How to integrate $\int_{0}^{1} \int_{0}^{\pi} \int_{0}^{\pi} r^2 \sin\theta \sqrt{1 - r^2\cos^2\theta - r^2\sin^2\theta} \,d\phi\, d\theta \,dr$",How to integrate,"\int_{0}^{1} \int_{0}^{\pi} \int_{0}^{\pi} r^2 \sin\theta \sqrt{1 - r^2\cos^2\theta - r^2\sin^2\theta} \,d\phi\, d\theta \,dr","Find the center of mass of the hemispherical region $W$ defined by the inequalities $x^2 + y^2 + z^2 \leq 1$ and $z \geq 0$ with unity density. By symmetry we know the $x$ and $y$ coordinates will be $0$ so we just need the $z$ coordinate. The volume of a hemisphere is just $\frac{2}{3} \pi$ from the formula for the volume of a sphere, so we just need to compute $\iiint_{W}^{} z \, dz \, dy \, dz$. This formula along with using the same limits for the integration of the volume of a hemisphere, we have: $\int_{0}^{1} \int_{0}^{\pi} \int_{0}^{\pi} r^2 \sin\theta z \, d\phi \, d\theta \, dr$. $z = \sqrt{1 - x^2 - y^2} = \sqrt{1 - r^2\cos^2\theta - r^2\sin^2\theta}$, so we substitute that in and get: $\int_{0}^{1} \int_{0}^{\pi} \int_{0}^{\pi} r^2 \sin\theta \sqrt{1 - r^2\cos^2\theta - r^2\sin^2\theta} \, d\phi \, d\theta \, dr$. Is there an easy way to integrate this?","Find the center of mass of the hemispherical region $W$ defined by the inequalities $x^2 + y^2 + z^2 \leq 1$ and $z \geq 0$ with unity density. By symmetry we know the $x$ and $y$ coordinates will be $0$ so we just need the $z$ coordinate. The volume of a hemisphere is just $\frac{2}{3} \pi$ from the formula for the volume of a sphere, so we just need to compute $\iiint_{W}^{} z \, dz \, dy \, dz$. This formula along with using the same limits for the integration of the volume of a hemisphere, we have: $\int_{0}^{1} \int_{0}^{\pi} \int_{0}^{\pi} r^2 \sin\theta z \, d\phi \, d\theta \, dr$. $z = \sqrt{1 - x^2 - y^2} = \sqrt{1 - r^2\cos^2\theta - r^2\sin^2\theta}$, so we substitute that in and get: $\int_{0}^{1} \int_{0}^{\pi} \int_{0}^{\pi} r^2 \sin\theta \sqrt{1 - r^2\cos^2\theta - r^2\sin^2\theta} \, d\phi \, d\theta \, dr$. Is there an easy way to integrate this?",,['integration']
51,"Triple integral and region of integration: $f(x,y,z)=z^2$",Triple integral and region of integration:,"f(x,y,z)=z^2","Calculate $\iiint_s f(x,y,z) \;dxdydz$ for $f(x,y,z) = z^2$ and $S$ the region bounded by $z=0$, $x^2 + z = 1$ and $y^2 + z=1$ I've already plotted the region but I am really having difficult to find the integration region. I tried the following: 1)$0\leq z \leq 1$, $-sqrt{1-z} \leq y \leq \sqrt{1-z}$ but I couldnt find the x variation (same for $x$ instead of $y$) But maybe there is an easy one that I can't see I would be glad if someone could explain how to write, step by step, a region. Thanks!","Calculate $\iiint_s f(x,y,z) \;dxdydz$ for $f(x,y,z) = z^2$ and $S$ the region bounded by $z=0$, $x^2 + z = 1$ and $y^2 + z=1$ I've already plotted the region but I am really having difficult to find the integration region. I tried the following: 1)$0\leq z \leq 1$, $-sqrt{1-z} \leq y \leq \sqrt{1-z}$ but I couldnt find the x variation (same for $x$ instead of $y$) But maybe there is an easy one that I can't see I would be glad if someone could explain how to write, step by step, a region. Thanks!",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
52,How to find the part of a plane lying inside a cylinder?,How to find the part of a plane lying inside a cylinder?,,"I have the plane $x + y + z = 1$ and the cylinder $x^2 + y^2 = 4,$ and need to find the part of the plane which is inside the cylinder; I have the feeling it's going to be an ellipse. I tried doing $$ x^2 + y^2 = 4x + 4y + 4z,$$ but this is a paraboloid, so it can't be what i'm looking for. What am I doing wrong and how can I find the part of the plane inside the cylinder?","I have the plane $x + y + z = 1$ and the cylinder $x^2 + y^2 = 4,$ and need to find the part of the plane which is inside the cylinder; I have the feeling it's going to be an ellipse. I tried doing $$ x^2 + y^2 = 4x + 4y + 4z,$$ but this is a paraboloid, so it can't be what i'm looking for. What am I doing wrong and how can I find the part of the plane inside the cylinder?",,['multivariable-calculus']
53,A result that follows from Stokes' theorem-- Important?,A result that follows from Stokes' theorem-- Important?,,"From Stokes theorem, it is easy to prove the folowing proposition: $\int\int_\vec{S}\vec{F}d\vec{S}=0$ if $\vec{F}=curl$ $\vec{G}$ for vector fields $\vec{F}$ and $\vec{G}$ and a closed parametrized surface $\vec{S}$. Are there any important results that can be derived from this proposition?","From Stokes theorem, it is easy to prove the folowing proposition: $\int\int_\vec{S}\vec{F}d\vec{S}=0$ if $\vec{F}=curl$ $\vec{G}$ for vector fields $\vec{F}$ and $\vec{G}$ and a closed parametrized surface $\vec{S}$. Are there any important results that can be derived from this proposition?",,"['multivariable-calculus', 'soft-question', 'vector-fields']"
54,How can I know if a vector $\mathbf{a}$ is tangent to a surface $S$?,How can I know if a vector  is tangent to a surface ?,\mathbf{a} S,"I am given a surface $S$ parametrized by $\mathbf{r} (u, v) = x(u, v) \mathbf{i} + y(u, v) \mathbf{j} + z(u, v) \mathbf{k}$ and a vector $\mathbf{a}.$ How can I know if the vector is tangent to the surface?","I am given a surface $S$ parametrized by $\mathbf{r} (u, v) = x(u, v) \mathbf{i} + y(u, v) \mathbf{j} + z(u, v) \mathbf{k}$ and a vector $\mathbf{a}.$ How can I know if the vector is tangent to the surface?",,['multivariable-calculus']
55,Find max/min of $e^{2x}\left(x+y^{2}+2y\right)$,Find max/min of,e^{2x}\left(x+y^{2}+2y\right),"$$e^{2x}\left(x+y^{2}+2y\right)$$ FOC: $$\begin{cases} 2e^{2x}\left(x+y^{2}+2y\right)+e^{2x}=0\\ e^{2x}\left(2y+2\right)=0 \end{cases}\rightarrow\begin{cases} x=\frac{1}{2}\\ y=-1 \end{cases}$$ SOC: $$\begin{pmatrix}4e^{2x}\left(x+y^{2}+2y\right)+2e^{2x} & 2e^{2x}\left(2y+2\right)\\ 2e^{2x}\left(2y+2\right) & 2e^{2x} \end{pmatrix}\rightarrow\begin{pmatrix}0 & 0\\ 0 & 2e \end{pmatrix}$$ As both leading principal minors are equal to 0, Hessian is positive semidefinite but not positive definite, i.e. the function is convex but not strictly. So I cannot rule out the possibility that the function has local minimum at this point. WolframAlpha tells that the point, in fact, is local minimum. How can I get such result?","$$e^{2x}\left(x+y^{2}+2y\right)$$ FOC: $$\begin{cases} 2e^{2x}\left(x+y^{2}+2y\right)+e^{2x}=0\\ e^{2x}\left(2y+2\right)=0 \end{cases}\rightarrow\begin{cases} x=\frac{1}{2}\\ y=-1 \end{cases}$$ SOC: $$\begin{pmatrix}4e^{2x}\left(x+y^{2}+2y\right)+2e^{2x} & 2e^{2x}\left(2y+2\right)\\ 2e^{2x}\left(2y+2\right) & 2e^{2x} \end{pmatrix}\rightarrow\begin{pmatrix}0 & 0\\ 0 & 2e \end{pmatrix}$$ As both leading principal minors are equal to 0, Hessian is positive semidefinite but not positive definite, i.e. the function is convex but not strictly. So I cannot rule out the possibility that the function has local minimum at this point. WolframAlpha tells that the point, in fact, is local minimum. How can I get such result?",,"['multivariable-calculus', 'optimization']"
56,Double integral definition,Double integral definition,,"I don't understand the definition of double integral. For instance in the functions with single variable the definite integral was defined as Riemannian sum as: $$\lim_{n\to\infty}\sum_{k=1}^{n} f(c_k)\delta x_k$$ In which I could assume $f(c_k)$ as the height and $\delta x_k$ as the width of the rectangles we're going to calculate the sum of but for the double integration in the region $R$ we've got the definition: $$\lim_{\delta A\to\infty}\sum_{k=1}^{n}f(x_k,y_k)\delta A_k $$ Now I can assume the $\delta A$ to be the surface of small rectangles which cover the region $R$.Then what $f(x_k,y_k)$ supposed to be? I'm trying to learn by analogy that $f(c_k)$ was assumed to be the height but now what is the $f(x_k,y_k)$ supposed to be in the double integral definition? Thanks in advance P.S: Notice that I'm trying to find out what $f(x_k,y_k)$ tries to represent when we're trying to find the surface of region $R$ not the volume !","I don't understand the definition of double integral. For instance in the functions with single variable the definite integral was defined as Riemannian sum as: $$\lim_{n\to\infty}\sum_{k=1}^{n} f(c_k)\delta x_k$$ In which I could assume $f(c_k)$ as the height and $\delta x_k$ as the width of the rectangles we're going to calculate the sum of but for the double integration in the region $R$ we've got the definition: $$\lim_{\delta A\to\infty}\sum_{k=1}^{n}f(x_k,y_k)\delta A_k $$ Now I can assume the $\delta A$ to be the surface of small rectangles which cover the region $R$.Then what $f(x_k,y_k)$ supposed to be? I'm trying to learn by analogy that $f(c_k)$ was assumed to be the height but now what is the $f(x_k,y_k)$ supposed to be in the double integral definition? Thanks in advance P.S: Notice that I'm trying to find out what $f(x_k,y_k)$ tries to represent when we're trying to find the surface of region $R$ not the volume !",,"['integration', 'multivariable-calculus', 'definite-integrals']"
57,"What is the domain of $Z=\sin(\ln(x\,\arccos{y}))$?",What is the domain of ?,"Z=\sin(\ln(x\,\arccos{y}))","What is the domain of $Z=\sin(\ln(x\,\arccos{y}))$? I see that is should be $-\dfrac{\pi}{2}\leq \ln(x\,\arccos{y}) \leq \dfrac{\pi}{2}$ and then $e^{-\dfrac{\pi}{2}} \leq x*\arccos(y) \leq e^{\dfrac{\pi}{2}}$ and now I'm stuck.","What is the domain of $Z=\sin(\ln(x\,\arccos{y}))$? I see that is should be $-\dfrac{\pi}{2}\leq \ln(x\,\arccos{y}) \leq \dfrac{\pi}{2}$ and then $e^{-\dfrac{\pi}{2}} \leq x*\arccos(y) \leq e^{\dfrac{\pi}{2}}$ and now I'm stuck.",,"['calculus', 'multivariable-calculus', 'inequality', 'graphing-functions']"
58,Three linearly independent vector fields,Three linearly independent vector fields,,"How can one find three linearly independent vector fields on $S^1\times S^2$? I know that $S^1\times S^2 \cong SO_3( \mathbb{R})$, i.e. the set of orthogonal $3 \times 3$ matrices with determinant $1$, which is a Lie group and thus parallelizable. I am, however, interested in an explicit form for three vector fields.","How can one find three linearly independent vector fields on $S^1\times S^2$? I know that $S^1\times S^2 \cong SO_3( \mathbb{R})$, i.e. the set of orthogonal $3 \times 3$ matrices with determinant $1$, which is a Lie group and thus parallelizable. I am, however, interested in an explicit form for three vector fields.",,"['multivariable-calculus', 'differential-topology', 'lie-groups', 'vector-analysis']"
59,Problem calculating line integral,Problem calculating line integral,,"I have $\gamma=[0,1]\to\mathbb{R}^3$ defined by $\gamma(t)=(\cos(2\pi t), \sin (2\pi t), t^2-t)\;\forall t\in[0,1]$  and I'm asked to calculate $\displaystyle\int_{\gamma}\displaystyle\frac{2xy\mathrm{dx}-(x^2+z^2)\mathrm{dy}+2yz\mathrm{dz}}{(x^2+z^2)^2}$. I have made an attempt to solve it but it seems that it leads to an integral hard to calculate (and very messy). I thought that it would be a good idea make the following change of variables: $$x=\cos(2\pi t)\\y=\sin(2\pi t)\\z=t^2-t$$. Then I'd have $$\mathrm{dx} = -2\pi\sin(2\pi t) \mathrm{dt} \\ \mathrm{dy =-2\pi\cos(2\pi t)\mathrm{dt}} \\ \mathrm{dz} = 2t-1\;\mathrm{dt}$$. Now, making the substitution returns a long integral: $2xy\mathrm{dx}-(x^2+z^2)\mathrm{dy}+2yz\mathrm{dz} = [2\cos(2\pi t)\sin(2\pi t)(-2\pi\sin(2\pi t))dt]-(\cos^2(2\pi t) +t^4-2t^3+t^2)+2[\sin(2\pi t)(t^2-t)(2t-1)\mathrm{dt}]=-4\pi\sin^2(2\pi t)\cos(2\pi t)-\cos^2(2\pi t) -t^4+2t^3-t^2+4t^3\sin(2\pi t)-6t^3\sin(2\pi t)+2t\sin(2\pi t)\;\mathrm{dt} = [\sin(2\pi t)][-4\pi\sin(2\pi t)\cos(2\pi t)+\sin(2\pi t)+4t^3-6t^3+2t]+t^2(-t^2+2t-1)-1.$ And $(x^2+z^2)^2= (\cos^2 (2\pi t)+t^2-2t^3+t^2)^2$ Which means I should calculate... $$\int_0^1 \frac{[\sin(2\pi t)][-4\pi\sin(2\pi t)\cos(2\pi t)+\sin(2\pi t)+4t^3-6t^3+2t]+t^2(-t^2+2t-1)-1}{(\cos^2 (2\pi t)+t^2-2t^3+t^2)^2}dt$$. Is that right?. How badly did I messed up?","I have $\gamma=[0,1]\to\mathbb{R}^3$ defined by $\gamma(t)=(\cos(2\pi t), \sin (2\pi t), t^2-t)\;\forall t\in[0,1]$  and I'm asked to calculate $\displaystyle\int_{\gamma}\displaystyle\frac{2xy\mathrm{dx}-(x^2+z^2)\mathrm{dy}+2yz\mathrm{dz}}{(x^2+z^2)^2}$. I have made an attempt to solve it but it seems that it leads to an integral hard to calculate (and very messy). I thought that it would be a good idea make the following change of variables: $$x=\cos(2\pi t)\\y=\sin(2\pi t)\\z=t^2-t$$. Then I'd have $$\mathrm{dx} = -2\pi\sin(2\pi t) \mathrm{dt} \\ \mathrm{dy =-2\pi\cos(2\pi t)\mathrm{dt}} \\ \mathrm{dz} = 2t-1\;\mathrm{dt}$$. Now, making the substitution returns a long integral: $2xy\mathrm{dx}-(x^2+z^2)\mathrm{dy}+2yz\mathrm{dz} = [2\cos(2\pi t)\sin(2\pi t)(-2\pi\sin(2\pi t))dt]-(\cos^2(2\pi t) +t^4-2t^3+t^2)+2[\sin(2\pi t)(t^2-t)(2t-1)\mathrm{dt}]=-4\pi\sin^2(2\pi t)\cos(2\pi t)-\cos^2(2\pi t) -t^4+2t^3-t^2+4t^3\sin(2\pi t)-6t^3\sin(2\pi t)+2t\sin(2\pi t)\;\mathrm{dt} = [\sin(2\pi t)][-4\pi\sin(2\pi t)\cos(2\pi t)+\sin(2\pi t)+4t^3-6t^3+2t]+t^2(-t^2+2t-1)-1.$ And $(x^2+z^2)^2= (\cos^2 (2\pi t)+t^2-2t^3+t^2)^2$ Which means I should calculate... $$\int_0^1 \frac{[\sin(2\pi t)][-4\pi\sin(2\pi t)\cos(2\pi t)+\sin(2\pi t)+4t^3-6t^3+2t]+t^2(-t^2+2t-1)-1}{(\cos^2 (2\pi t)+t^2-2t^3+t^2)^2}dt$$. Is that right?. How badly did I messed up?",,"['integration', 'multivariable-calculus', 'definite-integrals']"
60,Chain Rule to Compute Second Derivative,Chain Rule to Compute Second Derivative,,"I was going through Marsden's book, Elementary Classical Analysis , and came across the following exercise in Chapter 6. It reads as follows: If $f: A \subset \mathbb{R}^n \to \mathbb{R}^m$ and $g: B \subset \mathbb{R}^m \to \mathbb{R}^p$, show that   \begin{align*} D^2(g \circ f(x_0))(x, y) &= D^2(g(x_0)) (Df(x_0) \cdot x, Df(x_0) \cdot y) \\ &+\; Dg(f(x_0)) \cdot D^2f(x_0)(x, y). \end{align*} I found this question concerning the same exercise, but my problem was not answered here. I know that I am supposed to apply the chain rule twice to compute this result. What I do not understand is why there is an addition involved in the result to begin with. How is the use of the product rule justified here? If I apply the chain rule once, I get $$ D(g \circ f(x_0)) = Dg(f(x_0))) \circ Df(x_0),$$ where $Df : A \to L(\mathbb{R}^n, \mathbb{R}^m)$ and $Dg : B \to L(\mathbb{R}^m, \mathbb{R}^p)$. Clearly this is the composition of two linear transformations. But neither the product rule (introduced in the text to differentiate $gf$, where $f : A \subset \mathbb{R}^n \to \mathbb{R}^m$ and $g: A \to \mathbb{R}$) nor the chain rule applies here. I know that we can view this equation in terms of matrix multiplication for suitably-chosen bases. But how can I differentiate the composition of linear transformations as written above? Can I view the composition of these linear operations as a bilinear form, and apply the generalized product rule to differentiate this bilinear form?","I was going through Marsden's book, Elementary Classical Analysis , and came across the following exercise in Chapter 6. It reads as follows: If $f: A \subset \mathbb{R}^n \to \mathbb{R}^m$ and $g: B \subset \mathbb{R}^m \to \mathbb{R}^p$, show that   \begin{align*} D^2(g \circ f(x_0))(x, y) &= D^2(g(x_0)) (Df(x_0) \cdot x, Df(x_0) \cdot y) \\ &+\; Dg(f(x_0)) \cdot D^2f(x_0)(x, y). \end{align*} I found this question concerning the same exercise, but my problem was not answered here. I know that I am supposed to apply the chain rule twice to compute this result. What I do not understand is why there is an addition involved in the result to begin with. How is the use of the product rule justified here? If I apply the chain rule once, I get $$ D(g \circ f(x_0)) = Dg(f(x_0))) \circ Df(x_0),$$ where $Df : A \to L(\mathbb{R}^n, \mathbb{R}^m)$ and $Dg : B \to L(\mathbb{R}^m, \mathbb{R}^p)$. Clearly this is the composition of two linear transformations. But neither the product rule (introduced in the text to differentiate $gf$, where $f : A \subset \mathbb{R}^n \to \mathbb{R}^m$ and $g: A \to \mathbb{R}$) nor the chain rule applies here. I know that we can view this equation in terms of matrix multiplication for suitably-chosen bases. But how can I differentiate the composition of linear transformations as written above? Can I view the composition of these linear operations as a bilinear form, and apply the generalized product rule to differentiate this bilinear form?",,"['real-analysis', 'multivariable-calculus', 'multilinear-algebra']"
61,"What is the limit of this function as $(x,y)$ approaches $(0,0)$?",What is the limit of this function as  approaches ?,"(x,y) (0,0)","Let the function $f \colon (\mathbf{R}^2 \setminus \{(x,y) \in \mathbf{R}^2 \colon x+y = 0 \}) \to \mathbf{R}$ be defined as follows:  $$ f(x,y) \colon= \frac{xy}{x+y}$$ if $(x,y) \in \mathbf{R}^2$ and  $x+y \ne 0$. Then what is the value of $$\lim_{(x,y) \to (0,0)} f(x,y)?$$ Using the iterated limits, we obtain  $$\lim_{x\to 0} (\lim_{y\to 0} f(x,y) ) = \lim_{x \to 0} (\lim_{y\to 0} \frac{xy}{x+y} ) = \lim_{x\to 0} 0 = 0,$$ and also  $$\lim_{y\to 0} (\lim_{x\to 0} f(x,y) ) = \lim_{y \to 0} (\lim_{x\to 0} \frac{xy}{x+y} ) = \lim_{y\to 0} 0 = 0.$$ However, the equality of these two iterated limits is only necessary, and not sufficient, for the existence of my desired limit. What is the limit as $(x,y)$ approaches $(0,0)$ if we define $f$ as follows?  $$ f(x,y) \colon= \frac{xy}{x+y}$$ if $x+y \ne 0$, and  $$ f(x,y) \colon= 0$$ if $x+y=0$.","Let the function $f \colon (\mathbf{R}^2 \setminus \{(x,y) \in \mathbf{R}^2 \colon x+y = 0 \}) \to \mathbf{R}$ be defined as follows:  $$ f(x,y) \colon= \frac{xy}{x+y}$$ if $(x,y) \in \mathbf{R}^2$ and  $x+y \ne 0$. Then what is the value of $$\lim_{(x,y) \to (0,0)} f(x,y)?$$ Using the iterated limits, we obtain  $$\lim_{x\to 0} (\lim_{y\to 0} f(x,y) ) = \lim_{x \to 0} (\lim_{y\to 0} \frac{xy}{x+y} ) = \lim_{x\to 0} 0 = 0,$$ and also  $$\lim_{y\to 0} (\lim_{x\to 0} f(x,y) ) = \lim_{y \to 0} (\lim_{x\to 0} \frac{xy}{x+y} ) = \lim_{y\to 0} 0 = 0.$$ However, the equality of these two iterated limits is only necessary, and not sufficient, for the existence of my desired limit. What is the limit as $(x,y)$ approaches $(0,0)$ if we define $f$ as follows?  $$ f(x,y) \colon= \frac{xy}{x+y}$$ if $x+y \ne 0$, and  $$ f(x,y) \colon= 0$$ if $x+y=0$.",,"['calculus', 'real-analysis', 'analysis', 'limits', 'multivariable-calculus']"
62,Proving a vector identity,Proving a vector identity,,"Let $\vec{a} , \vec{b} , \vec{c} $ three nonzero, non parallel vectors in $\mathbb{R}^3 $ for which $ (\vec{a} \times \vec{b} ) \times \vec{c} =\vec{0} $ . Prove that $\vec{a}\cdot \vec{c} = \vec{b}\cdot \vec{c} $ . My attempt: When writing $\vec{a}=(a_1 ,a_2 , a_3 ) $, etc... , and calculating the vector product, I get that the following system must hold: $ b_1 (c_3 a_3 +c_2 a_2 ) = a_1 (c_2 b_2 +c_3 b_3 ) $ $ b_2 (c_3 a_3 +c_1 a_1 ) = a_2 (c_3 b_3 +c_1 b_1 ) $ $ b_3 (c_1 a_1 +c_2 a_2 ) = a_3 (c_1 b_1 +c_2 b_2 ) $ I know that after multiplying the first equality by $c_1$ , the second one by $c_2 $ , the third by $c_3 $ , and summing them all up , I get the same left and right hand sides, but I have no idea about what it gives me... Will you please help me ? Thanks in advance","Let $\vec{a} , \vec{b} , \vec{c} $ three nonzero, non parallel vectors in $\mathbb{R}^3 $ for which $ (\vec{a} \times \vec{b} ) \times \vec{c} =\vec{0} $ . Prove that $\vec{a}\cdot \vec{c} = \vec{b}\cdot \vec{c} $ . My attempt: When writing $\vec{a}=(a_1 ,a_2 , a_3 ) $, etc... , and calculating the vector product, I get that the following system must hold: $ b_1 (c_3 a_3 +c_2 a_2 ) = a_1 (c_2 b_2 +c_3 b_3 ) $ $ b_2 (c_3 a_3 +c_1 a_1 ) = a_2 (c_3 b_3 +c_1 b_1 ) $ $ b_3 (c_1 a_1 +c_2 a_2 ) = a_3 (c_1 b_1 +c_2 b_2 ) $ I know that after multiplying the first equality by $c_1$ , the second one by $c_2 $ , the third by $c_3 $ , and summing them all up , I get the same left and right hand sides, but I have no idea about what it gives me... Will you please help me ? Thanks in advance",,"['multivariable-calculus', 'vectors']"
63,Double integral region,Double integral region,,"I have made an attempt on a problem from an old exam. I'm not sure if my method is correct or not, as it differs from the teacher's solution and I'm unsure of the theory. Does my solution lack any important","I have made an attempt on a problem from an old exam. I'm not sure if my method is correct or not, as it differs from the teacher's solution and I'm unsure of the theory. Does my solution lack any important",,['multivariable-calculus']
64,Verify Stokes's Formula for...,Verify Stokes's Formula for...,,"Verify Stokes's Formula for $\textbf{F}(x,y,z)=(3y,-xz,yz^2)$, where $S$ is the surface of the paraboloid $2z=x^2+y^2$ bounded by the plane $z=2$. So I need to compute the integral using the formula $\iint_S \text{curl}~\textbf{F} \dot ~\eta ~d\sigma$ and the formula $\int_\Gamma \textbf{F}~\dot~\textbf{T}~ds$ and get the same answer. I have a formula that says $$\int_\Gamma \textbf{F} \cdot \textbf{T}~ds = \int_a^b \textbf{F}(\gamma(t)) \cdot \gamma'(t)dt,$$ so I thought that the line that I am integrating across is $4=x^2+y^2$, so I said $\gamma(t)=(2\text{cos}(t),2\text{sin}(t),2)$, with $0 \leq t \leq 2\pi$. From here it's a pretty straight forward computation, but I'm not sure I set it up correctly. Does this look right? Now for the first integral, I have $$\iint\limits_S \text{curl}~\textbf{F} \cdot \eta ~d\sigma = \iint\limits_R \text{curl}~\textbf{F}(\textbf{r}(u,v)) \cdot \frac{\partial \textbf{r}}{\partial u}\times\frac{\partial \textbf{r}}{\partial v} du \, dv.$$ However, I can't figure out what $\textbf{r}(u,v)$ should be. I tried using polar coordinates but it was pretty ugly. Any tips?","Verify Stokes's Formula for $\textbf{F}(x,y,z)=(3y,-xz,yz^2)$, where $S$ is the surface of the paraboloid $2z=x^2+y^2$ bounded by the plane $z=2$. So I need to compute the integral using the formula $\iint_S \text{curl}~\textbf{F} \dot ~\eta ~d\sigma$ and the formula $\int_\Gamma \textbf{F}~\dot~\textbf{T}~ds$ and get the same answer. I have a formula that says $$\int_\Gamma \textbf{F} \cdot \textbf{T}~ds = \int_a^b \textbf{F}(\gamma(t)) \cdot \gamma'(t)dt,$$ so I thought that the line that I am integrating across is $4=x^2+y^2$, so I said $\gamma(t)=(2\text{cos}(t),2\text{sin}(t),2)$, with $0 \leq t \leq 2\pi$. From here it's a pretty straight forward computation, but I'm not sure I set it up correctly. Does this look right? Now for the first integral, I have $$\iint\limits_S \text{curl}~\textbf{F} \cdot \eta ~d\sigma = \iint\limits_R \text{curl}~\textbf{F}(\textbf{r}(u,v)) \cdot \frac{\partial \textbf{r}}{\partial u}\times\frac{\partial \textbf{r}}{\partial v} du \, dv.$$ However, I can't figure out what $\textbf{r}(u,v)$ should be. I tried using polar coordinates but it was pretty ugly. Any tips?",,['multivariable-calculus']
65,Checking a solution of a P.D.E.,Checking a solution of a P.D.E.,,"I have the following P.D.E.: $$-yu_x + xu_y = 0 \quad\text{where } u(0, y) = f(y).$$ I derived a solution as follows: \begin{align}   -yu_x + xu_y ={}& 0 \\   \iff{}& \nabla u(x,y) \cdot \langle -y, x\rangle = 0 \\   \implies{}& \frac{\Bbb dy}{\Bbb dx} = \frac{-x}{y} \\   \iff{}& y\,\Bbb dy = -x\,\Bbb dx \\   \iff{}& \int y\,\Bbb dy = \int -x\,\Bbb dx \\   \iff{}& y^2 = -x^2 + c_2 \\   \iff{}& y^2 + x^2 = c_2. \end{align} Since $u(x,y)$ is constant along the O.D.E. ${\Bbb dy}/{\Bbb dx}$ , we have: \begin{align} u(x,y) &= c_1 \\        &= f(c_2) \quad\text{for some function $f$} \\        &= f(y^2+x^2). \end{align} I want to check that this satisfies the P.D.E. Specifically, any function $f$ should satisfy the P.D.E. My calculus is a bit rusty, and I am not exactly sure how to do this. Here is my reasoning: Since $u(x,y) = -yu_x + xu_y = 0$ we have to substitute in for $f$ which yields $$u(x, y) = -yf_x2x + xf_y2y = 2xyf_y - 2xyf_x.$$ We need the above to equal $0$ . The $2xy$ and $-2xy$ give me evidence that it should cancel, and that my calculus is off $\ldots$ What is not clear to me is that, we don’t know what $f$ is, hence I can’t find out what $f_x$ or $f_y$ are. Though, the problem does look symmetrical, and I could see a potential solution involving polar coordinates, but I’m not quite sure how to solve it in this way either. How can I verify that the above is indeed equal to $0$ and hence satisfies the P.D.E.? Thanks for all the help!","I have the following P.D.E.: I derived a solution as follows: Since is constant along the O.D.E. , we have: I want to check that this satisfies the P.D.E. Specifically, any function should satisfy the P.D.E. My calculus is a bit rusty, and I am not exactly sure how to do this. Here is my reasoning: Since we have to substitute in for which yields We need the above to equal . The and give me evidence that it should cancel, and that my calculus is off What is not clear to me is that, we don’t know what is, hence I can’t find out what or are. Though, the problem does look symmetrical, and I could see a potential solution involving polar coordinates, but I’m not quite sure how to solve it in this way either. How can I verify that the above is indeed equal to and hence satisfies the P.D.E.? Thanks for all the help!","-yu_x + xu_y = 0 \quad\text{where } u(0, y) = f(y). \begin{align}
  -yu_x + xu_y ={}& 0 \\
  \iff{}& \nabla u(x,y) \cdot \langle -y, x\rangle = 0 \\
  \implies{}& \frac{\Bbb dy}{\Bbb dx} = \frac{-x}{y} \\
  \iff{}& y\,\Bbb dy = -x\,\Bbb dx \\
  \iff{}& \int y\,\Bbb dy = \int -x\,\Bbb dx \\
  \iff{}& y^2 = -x^2 + c_2 \\
  \iff{}& y^2 + x^2 = c_2.
\end{align} u(x,y) {\Bbb dy}/{\Bbb dx} \begin{align}
u(x,y) &= c_1 \\
       &= f(c_2) \quad\text{for some function f} \\
       &= f(y^2+x^2).
\end{align} f u(x,y) = -yu_x + xu_y = 0 f u(x, y) = -yf_x2x + xf_y2y = 2xyf_y - 2xyf_x. 0 2xy -2xy \ldots f f_x f_y 0","['multivariable-calculus', 'partial-differential-equations', 'parametric']"
66,Iterating the chain rule in multiple variables,Iterating the chain rule in multiple variables,,"$$f:\mathbb{R^3}\rightarrow\mathbb{R},\quad g:\mathbb{R^2}\rightarrow\mathbb{R},\quad h:\mathbb{R}\rightarrow\mathbb{R}$$ $f,g,h$ are differentiable along their domain. I'm asked to find the total derivative of the form: $$H(x,y,z):=g(f(x,y,h(x)),\ g(z,y))$$ I started, but then realized something must be wrong: $$DH(x,y,z)\ \ ``=""\ Dg(f(x,y,h(x)),\ g(z,y))\cdot Df(x,y,h(x))\cdot \ Dg(z,y)\cdot Dh(x)$$ These linear mappings cant be composed as I have them written.","$$f:\mathbb{R^3}\rightarrow\mathbb{R},\quad g:\mathbb{R^2}\rightarrow\mathbb{R},\quad h:\mathbb{R}\rightarrow\mathbb{R}$$ $f,g,h$ are differentiable along their domain. I'm asked to find the total derivative of the form: $$H(x,y,z):=g(f(x,y,h(x)),\ g(z,y))$$ I started, but then realized something must be wrong: $$DH(x,y,z)\ \ ``=""\ Dg(f(x,y,h(x)),\ g(z,y))\cdot Df(x,y,h(x))\cdot \ Dg(z,y)\cdot Dh(x)$$ These linear mappings cant be composed as I have them written.",,"['calculus', 'algebra-precalculus', 'multivariable-calculus']"
67,Volume integral help,Volume integral help,,"I have a volume integral to compute with the following bounded volume $V\in \mathbb{R}^3$ $$ \frac{x^2+y^2}{4}+z^2\leq 1 \;\;,\;\; \frac{1}{2} \sqrt{x^2+y^2}\leq z\;,\;\; z\geq 0$$ I hadn't a clue how to do it until my lecturer said to use spherical coordinates $$\widetilde{r}(r,\theta,\phi)=\begin{pmatrix} 2rsin{\phi}cos{\theta} \\ 2rsin{\phi}sin{\theta} \\ rcos{\phi} \end{pmatrix}\;\;\;\;\;\; \phi:\left[0,\frac{\pi}{4}\right], \theta:\left[0,2\pi\right], r:[0,1]$$ As the cone cuts out the ellipsoid at $\frac{\pi}{4}$. However I'm having a hard time trying to show that the cone cuts out the ellipsoid at $\frac{\pi}{4}$ mathematically though. Also I don't see why $r$ is only from $[0,1]$ rather than $[0,2]$ as the largest value of $r$ is $2$ and occurs when the ellipsoid and cone intersect. Any help or hints? I've shown that it cuts the ellipsoid at $z= \frac{1}{\sqrt{2}}$ and therefore $x^2+y^2=2$ on that plane, however I can't seem to find the angle. EDIT: Managed to find a way to compute the volume, answer is below.","I have a volume integral to compute with the following bounded volume $V\in \mathbb{R}^3$ $$ \frac{x^2+y^2}{4}+z^2\leq 1 \;\;,\;\; \frac{1}{2} \sqrt{x^2+y^2}\leq z\;,\;\; z\geq 0$$ I hadn't a clue how to do it until my lecturer said to use spherical coordinates $$\widetilde{r}(r,\theta,\phi)=\begin{pmatrix} 2rsin{\phi}cos{\theta} \\ 2rsin{\phi}sin{\theta} \\ rcos{\phi} \end{pmatrix}\;\;\;\;\;\; \phi:\left[0,\frac{\pi}{4}\right], \theta:\left[0,2\pi\right], r:[0,1]$$ As the cone cuts out the ellipsoid at $\frac{\pi}{4}$. However I'm having a hard time trying to show that the cone cuts out the ellipsoid at $\frac{\pi}{4}$ mathematically though. Also I don't see why $r$ is only from $[0,1]$ rather than $[0,2]$ as the largest value of $r$ is $2$ and occurs when the ellipsoid and cone intersect. Any help or hints? I've shown that it cuts the ellipsoid at $z= \frac{1}{\sqrt{2}}$ and therefore $x^2+y^2=2$ on that plane, however I can't seem to find the angle. EDIT: Managed to find a way to compute the volume, answer is below.",,['multivariable-calculus']
68,Verify integral over a surface,Verify integral over a surface,,"Show $\int \int_{S} (x^2 +y^2) d\sigma = \frac{9\pi}{4}$ where $S = \left\{(x,y,z) : x>0, y>0, 3>z>0, z^2 =3(x^2+y^2)\right\}$. We have the formula $\int \int_{S} f(x,y,z) = \int \int_{D} f(x, y, g(x,y)) \sqrt{(\frac{\partial z }{\partial x})^2 + (\frac{\partial z }{\partial y})^2+1}dA$. I figured I could rewrite the constraint on $z$ as $z = \sqrt{3(x^2+y^2)}$ which gives $ \frac{\partial z }{\partial x}=\frac{\sqrt{3}x}{\sqrt{(x^2+y^2)}}$ and $ \frac{\partial z }{\partial y}=\frac{\sqrt{3}y}{\sqrt{(x^2+y^2)}}$. Plugging this in to the formula gives $\int \int 2(x^2+y^2)$ after you simplify. I thought both integrals went from $0$ to $\sqrt{3}$, but this clearly doesn't give the correct answer. Where am I messing up?","Show $\int \int_{S} (x^2 +y^2) d\sigma = \frac{9\pi}{4}$ where $S = \left\{(x,y,z) : x>0, y>0, 3>z>0, z^2 =3(x^2+y^2)\right\}$. We have the formula $\int \int_{S} f(x,y,z) = \int \int_{D} f(x, y, g(x,y)) \sqrt{(\frac{\partial z }{\partial x})^2 + (\frac{\partial z }{\partial y})^2+1}dA$. I figured I could rewrite the constraint on $z$ as $z = \sqrt{3(x^2+y^2)}$ which gives $ \frac{\partial z }{\partial x}=\frac{\sqrt{3}x}{\sqrt{(x^2+y^2)}}$ and $ \frac{\partial z }{\partial y}=\frac{\sqrt{3}y}{\sqrt{(x^2+y^2)}}$. Plugging this in to the formula gives $\int \int 2(x^2+y^2)$ after you simplify. I thought both integrals went from $0$ to $\sqrt{3}$, but this clearly doesn't give the correct answer. Where am I messing up?",,['multivariable-calculus']
69,Double integral and polar coordinates,Double integral and polar coordinates,,"Please, help me solve this double integral $$\int^{2\pi}_0d\varphi\int^{2}_1\frac{1}{\sqrt{\rho^3\cos^3\varphi+\rho^3\sin^3\varphi}}\rho\,d\rho$$ I really don't know how to figure out and carry of $\cos^3\varphi$ and $\sin^3\varphi$ into $d\varphi$","Please, help me solve this double integral $$\int^{2\pi}_0d\varphi\int^{2}_1\frac{1}{\sqrt{\rho^3\cos^3\varphi+\rho^3\sin^3\varphi}}\rho\,d\rho$$ I really don't know how to figure out and carry of $\cos^3\varphi$ and $\sin^3\varphi$ into $d\varphi$",,"['integration', 'analysis', 'multivariable-calculus', 'definite-integrals']"
70,Proving $\nabla_A tr(ABA^T C) = CAB + C^T A B^T$ [duplicate],Proving  [duplicate],\nabla_A tr(ABA^T C) = CAB + C^T A B^T,"This question already has answers here : How is $\nabla_X \operatorname{Tr}\left\{ X^T A X B\right\} = AXB + A^TXB^T$? (4 answers) Closed 3 years ago . The above equation appears without proof on page 9 (equation 3) of Andrew Ng's notes on Machine Learning I have tried various approaches to prove this to no avail. From the notes it seems that it should be provable from first principles. I tried using the ordinary chain rule on an element by element basis, but this quickly gets unwieldy. Any hint on the approach to resolve this would help a lot.","This question already has answers here : How is $\nabla_X \operatorname{Tr}\left\{ X^T A X B\right\} = AXB + A^TXB^T$? (4 answers) Closed 3 years ago . The above equation appears without proof on page 9 (equation 3) of Andrew Ng's notes on Machine Learning I have tried various approaches to prove this to no avail. From the notes it seems that it should be provable from first principles. I tried using the ordinary chain rule on an element by element basis, but this quickly gets unwieldy. Any hint on the approach to resolve this would help a lot.",,"['linear-algebra', 'multivariable-calculus', 'trace']"
71,Finding max/min of multivariable function,Finding max/min of multivariable function,,"The following function $f(x,y) = 3xy + \frac{6}{1 + x^2 + y^2 }$ within $\frac{1}{3} \leq x^2 +y^2 \leq 4$ I do partial differention $\frac{\partial z}{\partial x} = 3y - \frac{12x}{1 + x^2 + y^2}$ $\frac{\partial z}{\partial y} = 3x - \frac{12y}{1 + x^2+ y^2 }$ I try to simplify and get an expression of just one variable setting $\frac{\partial z}{\partial x} - \frac{\partial z}{\partial x} = 0$ getting that $x = y$ $3x - \frac{12x}{1 + x^2+x^2 }$ solving for $x = 0$, $x_1 = 0, x_2 = \frac{1}{\sqrt{2}}, x_3 = \frac{-1}{\sqrt{2}}$ which gives me $f(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}) = 3\frac{1}{\sqrt{2}}\frac{1}{\sqrt{2}} + \frac{6}{1 + \left(\frac{1}{\sqrt{2}}\right)^2 + \left(\frac{1}{\sqrt{2}}\right)^2 } = \frac{9}{2}$ second part of the problem is to try the outer and inner boundaries. I set for inner boundary: $x = \frac{1}{\sqrt{3}}\cos{t}$ , $y = \frac{1}{\sqrt{3}}\sin{t}$ and outer $x = 2\cos{t}$ , $y = 2sin{t}$ I then create a new function: $h(t) = f(x, y) = f(2\cos{t}, 2\sin{t})$ differentiate and get $\sin{t} = \cos{t}$ for $ x = 0$ $f(2\frac{1}{\sqrt{2}},2\frac{1}{\sqrt{2}}) = 3\frac{2}{\sqrt{2}}\frac{2}{\sqrt{2}} + \frac{6}{1 + \left(\frac{2}{\sqrt{2}}\right)^2 + \left(\frac{2}{\sqrt{2}}\right)^2 } = \frac{36}{5}$ which actually yields the correct maximum. for the inner boundary I get $f(\frac{1}{\sqrt{3}}\frac{1}{\sqrt{2}},\frac{1}{\sqrt{3}}\frac{1}{\sqrt{2}}) = 5$ but neither 9/2 or 5 gets the correct minimum! I can't see what i'm doing wrong.","The following function $f(x,y) = 3xy + \frac{6}{1 + x^2 + y^2 }$ within $\frac{1}{3} \leq x^2 +y^2 \leq 4$ I do partial differention $\frac{\partial z}{\partial x} = 3y - \frac{12x}{1 + x^2 + y^2}$ $\frac{\partial z}{\partial y} = 3x - \frac{12y}{1 + x^2+ y^2 }$ I try to simplify and get an expression of just one variable setting $\frac{\partial z}{\partial x} - \frac{\partial z}{\partial x} = 0$ getting that $x = y$ $3x - \frac{12x}{1 + x^2+x^2 }$ solving for $x = 0$, $x_1 = 0, x_2 = \frac{1}{\sqrt{2}}, x_3 = \frac{-1}{\sqrt{2}}$ which gives me $f(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}) = 3\frac{1}{\sqrt{2}}\frac{1}{\sqrt{2}} + \frac{6}{1 + \left(\frac{1}{\sqrt{2}}\right)^2 + \left(\frac{1}{\sqrt{2}}\right)^2 } = \frac{9}{2}$ second part of the problem is to try the outer and inner boundaries. I set for inner boundary: $x = \frac{1}{\sqrt{3}}\cos{t}$ , $y = \frac{1}{\sqrt{3}}\sin{t}$ and outer $x = 2\cos{t}$ , $y = 2sin{t}$ I then create a new function: $h(t) = f(x, y) = f(2\cos{t}, 2\sin{t})$ differentiate and get $\sin{t} = \cos{t}$ for $ x = 0$ $f(2\frac{1}{\sqrt{2}},2\frac{1}{\sqrt{2}}) = 3\frac{2}{\sqrt{2}}\frac{2}{\sqrt{2}} + \frac{6}{1 + \left(\frac{2}{\sqrt{2}}\right)^2 + \left(\frac{2}{\sqrt{2}}\right)^2 } = \frac{36}{5}$ which actually yields the correct maximum. for the inner boundary I get $f(\frac{1}{\sqrt{3}}\frac{1}{\sqrt{2}},\frac{1}{\sqrt{3}}\frac{1}{\sqrt{2}}) = 5$ but neither 9/2 or 5 gets the correct minimum! I can't see what i'm doing wrong.",,"['calculus', 'multivariable-calculus']"
72,Double Integral with abstract functions of 2 variables,Double Integral with abstract functions of 2 variables,,"I am required to prove something, and so far I have come to set up an integral $$\int_0^l{\int_0^T{u(x,t)\, \frac{d}{dt}u(x,t) dt }dx}.$$ I was just wondering how to think about these integrals and approach them because they seem to be very confusing when they are not given functions. Thanks in advance! (The outer integral is an integral over the length, not from 0 to 1)","I am required to prove something, and so far I have come to set up an integral $$\int_0^l{\int_0^T{u(x,t)\, \frac{d}{dt}u(x,t) dt }dx}.$$ I was just wondering how to think about these integrals and approach them because they seem to be very confusing when they are not given functions. Thanks in advance! (The outer integral is an integral over the length, not from 0 to 1)",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
73,Optimization with a constrained function,Optimization with a constrained function,,"Okay so I understand how to find points of extrema when for example, We have $3x^2 + 2y^2 + 6z^2$ subject to the constaint $x+y+z=1$. I followed the method of the Lagrange multiplier and resulted in the point $(\frac13, \frac12, \frac16)$. I know this point is correct. However, I don't know how to prove that it's a minimum value. Any ideas?","Okay so I understand how to find points of extrema when for example, We have $3x^2 + 2y^2 + 6z^2$ subject to the constaint $x+y+z=1$. I followed the method of the Lagrange multiplier and resulted in the point $(\frac13, \frac12, \frac16)$. I know this point is correct. However, I don't know how to prove that it's a minimum value. Any ideas?",,"['calculus', 'multivariable-calculus', 'optimization']"
74,"Function of two variables, derivation","Function of two variables, derivation",,"If $t = \frac{1}{2}((y_{1})^{2} + (y_{2})^{2})$ and if it written that $u$ and $v$ are functions depending on $t$, does that mean that $(y_{1})^{2}$ and $(y_{2})^{2}$ must be ""parts "" of $u$, i. e. we can choose, for example that $u=e^{(y_{1})^{2} + (y_{2})^{2}}$ and not $u=e^{(y_{1})^{2}}$. Then, if we choose $u=e^{(y_{1})^{2} + (y_{2})^{2}}$, and we have $v=\frac{-uu'}{2tu'-u}$,how to calculate $u'$? Thank you!","If $t = \frac{1}{2}((y_{1})^{2} + (y_{2})^{2})$ and if it written that $u$ and $v$ are functions depending on $t$, does that mean that $(y_{1})^{2}$ and $(y_{2})^{2}$ must be ""parts "" of $u$, i. e. we can choose, for example that $u=e^{(y_{1})^{2} + (y_{2})^{2}}$ and not $u=e^{(y_{1})^{2}}$. Then, if we choose $u=e^{(y_{1})^{2} + (y_{2})^{2}}$, and we have $v=\frac{-uu'}{2tu'-u}$,how to calculate $u'$? Thank you!",,"['multivariable-calculus', 'differential-geometry']"
75,Surface integrals help?,Surface integrals help?,,"I'm having trouble understanding visually how a surface integral works/calculates. For a standard double integral, function $f(x,y)$ and a rectangular region $U=[a,b]\times[c,d]$ then the double integral: $$\iint_{U}f(x,y)dxdy=\int_{a}^{b}dx\int_{c}^{d}dy f(x,y)$$ simply calculates the volume underneath the graph $f$. My problem now is when you aren't integrating over a flat region of space but a curved surface $S\subseteq\mathbb{R}^3$. Now I can't visualize this as a volume below the graph $f$ as there is another restraint. However someone explained surface integrals in this way to me. Imagine I am calculating the mass of a sheet of metal represented as a surface $S\subseteq\mathbb{R}^3$, with a function $f(x,y,z)$ defining its density at any point. In this case the mass will be obtained by integrating the density-function over the surface. i.e I am still finding the surface area of the surface but multiplying that area by its value of $f(x,y,z)$ at every point. So if I want to find the surface area itself, I use $f(x,y,z)=1$. This would be consistent with my description of volume in the standard flat space integral as I'm simply multiplying the area $(U)$ by the height at that individual point $(f(x,y))$. Now I'm wondering whether for the curved space model whether the volume is actually the space between the two graphs if plotted on the same axis. So my question is this. Do my ideas make any sense? Also you have any ideas on how I can visualize these integrals please share them? It really helps my problem solving when I can physically visualize the problem in my head. Thanks","I'm having trouble understanding visually how a surface integral works/calculates. For a standard double integral, function $f(x,y)$ and a rectangular region $U=[a,b]\times[c,d]$ then the double integral: $$\iint_{U}f(x,y)dxdy=\int_{a}^{b}dx\int_{c}^{d}dy f(x,y)$$ simply calculates the volume underneath the graph $f$. My problem now is when you aren't integrating over a flat region of space but a curved surface $S\subseteq\mathbb{R}^3$. Now I can't visualize this as a volume below the graph $f$ as there is another restraint. However someone explained surface integrals in this way to me. Imagine I am calculating the mass of a sheet of metal represented as a surface $S\subseteq\mathbb{R}^3$, with a function $f(x,y,z)$ defining its density at any point. In this case the mass will be obtained by integrating the density-function over the surface. i.e I am still finding the surface area of the surface but multiplying that area by its value of $f(x,y,z)$ at every point. So if I want to find the surface area itself, I use $f(x,y,z)=1$. This would be consistent with my description of volume in the standard flat space integral as I'm simply multiplying the area $(U)$ by the height at that individual point $(f(x,y))$. Now I'm wondering whether for the curved space model whether the volume is actually the space between the two graphs if plotted on the same axis. So my question is this. Do my ideas make any sense? Also you have any ideas on how I can visualize these integrals please share them? It really helps my problem solving when I can physically visualize the problem in my head. Thanks",,"['integration', 'multivariable-calculus']"
76,Differentiate vector norm by matrix,Differentiate vector norm by matrix,,"I've been trying to perform the following differentiation of a neural network: $$\frac{\delta||h(XW)\alpha-y||^2}{\delta W} = \frac{\delta}{\delta W}\sum_i(h(XW)_i\alpha-y_i)^2$$ Where $X$ and $W$ and matrices, $\alpha$ and $y$ are vectors, and $h$ is a point wise applied function. I've been reading the Wikipedia article on Matrix calculus and ""The Matrix Cookbook"" all day, but I can't seem to get things to work. I think it should probably be $$2(h(XW)\alpha-y)\frac{\delta}{\delta W}(h(XW)\alpha)$$ But I certainly get stuck at the $h$ function, which I guess you could say is from matrix to matrix. Any hints would be appreciated. Update: I think this derivation is correct: $$\frac{\delta||h(XW)\alpha-y||^2}{\delta W} = \frac{\delta}{\delta W}(h(XW)\alpha-y)^T(h(XW)\alpha-y) = 2(h(XW)\alpha-y)^T\frac{\delta}{\delta W}(h(XW)\alpha) = 2(h(XW)\alpha-y)^TX^Th'(XW)\alpha$$ This was derived by derivating over each element of $W$ using traces. Update 2: I found this great presentation of the topic: Schonemann_Trace_Derivatives_Presentation.pdf which I recommend very much. I've reformulated by defining $H=h(XW)$, $H'=h'(XW)$, $E = HA-Y$. Hence the problem has the pretty solution $$\frac{\delta}{\delta W}||E||_F^2 = \frac{\delta}{\delta W}Tr(EE^T) = 2X^T(H' \odot EA^T)$$ Where $\odot$ is the point wise product. By working more with the trace manipulations you can also get a formula for the generalized problem $h(h(...)W_2)W_1)W_0$.","I've been trying to perform the following differentiation of a neural network: $$\frac{\delta||h(XW)\alpha-y||^2}{\delta W} = \frac{\delta}{\delta W}\sum_i(h(XW)_i\alpha-y_i)^2$$ Where $X$ and $W$ and matrices, $\alpha$ and $y$ are vectors, and $h$ is a point wise applied function. I've been reading the Wikipedia article on Matrix calculus and ""The Matrix Cookbook"" all day, but I can't seem to get things to work. I think it should probably be $$2(h(XW)\alpha-y)\frac{\delta}{\delta W}(h(XW)\alpha)$$ But I certainly get stuck at the $h$ function, which I guess you could say is from matrix to matrix. Any hints would be appreciated. Update: I think this derivation is correct: $$\frac{\delta||h(XW)\alpha-y||^2}{\delta W} = \frac{\delta}{\delta W}(h(XW)\alpha-y)^T(h(XW)\alpha-y) = 2(h(XW)\alpha-y)^T\frac{\delta}{\delta W}(h(XW)\alpha) = 2(h(XW)\alpha-y)^TX^Th'(XW)\alpha$$ This was derived by derivating over each element of $W$ using traces. Update 2: I found this great presentation of the topic: Schonemann_Trace_Derivatives_Presentation.pdf which I recommend very much. I've reformulated by defining $H=h(XW)$, $H'=h'(XW)$, $E = HA-Y$. Hence the problem has the pretty solution $$\frac{\delta}{\delta W}||E||_F^2 = \frac{\delta}{\delta W}Tr(EE^T) = 2X^T(H' \odot EA^T)$$ Where $\odot$ is the point wise product. By working more with the trace manipulations you can also get a formula for the generalized problem $h(h(...)W_2)W_1)W_0$.",,"['calculus', 'matrices', 'multivariable-calculus', 'matrix-calculus']"
77,Weak Laplacian of $\|x\|^\alpha$,Weak Laplacian of,\|x\|^\alpha,"Let $\alpha> 0$ and consider the function $\|\mathbf x\|^\alpha = (x^2 + y^2)^{\frac{\alpha}{2}}$ defined on $\mathbb R^2$. I want to compute the Laplacian $\Delta (\|\mathbf x\|^\alpha)$ in the sense of distributions. If $\alpha \geq 2$ the function is differentiable and we get $\Delta (\|\mathbf x\|^\alpha) = \alpha^2 \|\mathbf x\|^{\alpha -2}$. Does this formula hold in general? From the fact that $\|x\|^{\alpha}$ is smooth outside the origin the above formula should hold on $\mathbb R^2 \setminus \{0\}$, but what about the singularity at $0$?","Let $\alpha> 0$ and consider the function $\|\mathbf x\|^\alpha = (x^2 + y^2)^{\frac{\alpha}{2}}$ defined on $\mathbb R^2$. I want to compute the Laplacian $\Delta (\|\mathbf x\|^\alpha)$ in the sense of distributions. If $\alpha \geq 2$ the function is differentiable and we get $\Delta (\|\mathbf x\|^\alpha) = \alpha^2 \|\mathbf x\|^{\alpha -2}$. Does this formula hold in general? From the fact that $\|x\|^{\alpha}$ is smooth outside the origin the above formula should hold on $\mathbb R^2 \setminus \{0\}$, but what about the singularity at $0$?",,"['multivariable-calculus', 'partial-differential-equations', 'distribution-theory', 'weak-derivatives']"
78,Calculate the integral of a 2 form,Calculate the integral of a 2 form,,"I am trying to compute the integral $$ \int\int_{S}\frac{1}{x}dy\wedge dz+\frac{1}{y}dz\wedge dx+\frac{1}{z}dx\wedge dy $$ over an ellipsoid given by $$ \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1. $$ The way I tried to do it was by using the divergence theorem. For $F:=<\frac{1}{x},\frac{1}{y},\frac{1}{z}>$, $$ \int\int_{S}\frac{1}{x}dy\wedge dz+\frac{1}{y}dz\wedge dx+\frac{1}{z}dx\wedge dy=\int\int_{S}F\cdot dS=\int\int\int_{D}\nabla\cdot F dV. $$ Then $\nabla\cdot F=-(\frac{1}{x^2}+\frac{1}{y^2}+\frac{1}{z^2})$ so $$ \int\int\int_{D}\nabla\cdot F dV=-\int\int\int_{D}(\frac{1}{x^2}+\frac{1}{y^2}+\frac{1}{z^2})dV. $$ Finally I changed to spherical coordinates by substituting $x=a\rho\cos\theta\sin\phi$,$y=b\rho\sin\theta\sin\phi$,$z=c\rho\cos\phi$ to get $$ -\int\int\int_{D}(\frac{1}{x^2}+\frac{1}{y^2}+\frac{1}{z^2})dV\\ =-abc\int_{0}^{\pi}\int_{0}^{2\pi}\int_{0}^{1}(\frac{1}{a^2\rho^2\cos^2\theta\sin^2\phi}+\frac{1}{b^2\rho^2\sin^2\theta\sin^2\phi}+\frac{1}{c^2\rho^2\cos^2\phi})\rho^2\sin\phi d\rho d\theta d\phi\\ =-abc\int_{0}^{\pi}\int_{0}^{2\pi}\int_{0}^{1}(\frac{1}{a^2\cos^2\theta\sin\phi}+\frac{1}{b^2\sin^2\theta\sin\phi}+\frac{\sin\phi}{c^2\cos^2\phi}) d\rho d\theta d\phi. $$ I plugged the last integral into Maple to find that the integral was divergent. Did I do something wrong or is that what I am supposed to get? If its correct, how can the integral over a bounded surface be infinite... Sorry my multivariable is a little rusty.","I am trying to compute the integral $$ \int\int_{S}\frac{1}{x}dy\wedge dz+\frac{1}{y}dz\wedge dx+\frac{1}{z}dx\wedge dy $$ over an ellipsoid given by $$ \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1. $$ The way I tried to do it was by using the divergence theorem. For $F:=<\frac{1}{x},\frac{1}{y},\frac{1}{z}>$, $$ \int\int_{S}\frac{1}{x}dy\wedge dz+\frac{1}{y}dz\wedge dx+\frac{1}{z}dx\wedge dy=\int\int_{S}F\cdot dS=\int\int\int_{D}\nabla\cdot F dV. $$ Then $\nabla\cdot F=-(\frac{1}{x^2}+\frac{1}{y^2}+\frac{1}{z^2})$ so $$ \int\int\int_{D}\nabla\cdot F dV=-\int\int\int_{D}(\frac{1}{x^2}+\frac{1}{y^2}+\frac{1}{z^2})dV. $$ Finally I changed to spherical coordinates by substituting $x=a\rho\cos\theta\sin\phi$,$y=b\rho\sin\theta\sin\phi$,$z=c\rho\cos\phi$ to get $$ -\int\int\int_{D}(\frac{1}{x^2}+\frac{1}{y^2}+\frac{1}{z^2})dV\\ =-abc\int_{0}^{\pi}\int_{0}^{2\pi}\int_{0}^{1}(\frac{1}{a^2\rho^2\cos^2\theta\sin^2\phi}+\frac{1}{b^2\rho^2\sin^2\theta\sin^2\phi}+\frac{1}{c^2\rho^2\cos^2\phi})\rho^2\sin\phi d\rho d\theta d\phi\\ =-abc\int_{0}^{\pi}\int_{0}^{2\pi}\int_{0}^{1}(\frac{1}{a^2\cos^2\theta\sin\phi}+\frac{1}{b^2\sin^2\theta\sin\phi}+\frac{\sin\phi}{c^2\cos^2\phi}) d\rho d\theta d\phi. $$ I plugged the last integral into Maple to find that the integral was divergent. Did I do something wrong or is that what I am supposed to get? If its correct, how can the integral over a bounded surface be infinite... Sorry my multivariable is a little rusty.",,"['multivariable-calculus', 'differential-geometry']"
79,Derivative of a Vector with respect to its norm (special relativity),Derivative of a Vector with respect to its norm (special relativity),,"I came across an equation (related to special relativity) that requires me to to take a derivative of a vector with respect to to it's own norm. In a bit more detail, what I mean is, let: $$\vec T(\tau) = \{T_1(\tau), T_2(\tau), \dotsb, T_t(\tau)\}$$ $$\vec X(\tau) = \{X_1(\tau), X_2(\tau), \dotsb, X_x(\tau)\}$$ With $\vec T$ and $\vec X$ having dimensions $t$ and $x$ respectively and both being vector functions of $\tau$. Also let: $$\tau^2 = w^2 - s^2 = \vec T \cdot \vec T - \vec X \cdot \vec X$$ Where: $$w = \lVert \vec T \rVert \qquad s = \lVert \vec X \rVert$$ Knowing that: $$d\tau^2 = dw^2 - ds^2= d\vec T \cdot d\vec T - d\vec X \cdot d\vec X$$ How then, can take the derivative $\frac {d\vec T}{dw}$? It may be useful to know that $d\vec X = [\beta]\cdot d\vec T$, where $[\beta]$ is a constant matrix of dimension $x\times t$. Thanks in advance.","I came across an equation (related to special relativity) that requires me to to take a derivative of a vector with respect to to it's own norm. In a bit more detail, what I mean is, let: $$\vec T(\tau) = \{T_1(\tau), T_2(\tau), \dotsb, T_t(\tau)\}$$ $$\vec X(\tau) = \{X_1(\tau), X_2(\tau), \dotsb, X_x(\tau)\}$$ With $\vec T$ and $\vec X$ having dimensions $t$ and $x$ respectively and both being vector functions of $\tau$. Also let: $$\tau^2 = w^2 - s^2 = \vec T \cdot \vec T - \vec X \cdot \vec X$$ Where: $$w = \lVert \vec T \rVert \qquad s = \lVert \vec X \rVert$$ Knowing that: $$d\tau^2 = dw^2 - ds^2= d\vec T \cdot d\vec T - d\vec X \cdot d\vec X$$ How then, can take the derivative $\frac {d\vec T}{dw}$? It may be useful to know that $d\vec X = [\beta]\cdot d\vec T$, where $[\beta]$ is a constant matrix of dimension $x\times t$. Thanks in advance.",,"['calculus', 'multivariable-calculus', 'normed-spaces', 'vectors']"
80,Volume with triple integrals,Volume with triple integrals,,Calculate integral $$\iiint_V \frac{e^{-x^2-y^2-z^2}}{\sqrt{x^2+y^2+z^2}} dV$$ Where $V\subset\mathbb{R}^3$ is the exterior of a origocentered sphere with radius of 2 \begin{align*} V=&\iiint_V \frac{e^{-x^2-y^2-z^2}}{\sqrt{x^2+y^2+z^2}} dV \\ =& \int_0^{2\pi}\int_0^{\pi}\int_0^2 \frac{e^{-r^2}}{r}\rho^2\sin\phi dr d\phi d \theta \\ =& 2\pi \Biggl[-\cos\phi\Biggr]_0^{\pi} \int_0^2 re^{-r^2} dr \\ =& 2\pi (\underbrace{-\cos\pi}_{=1} +\underbrace{\cos0}_{=1}) \int_0^2 re^{-r^2} dr \\ =& 4\pi  \Biggl[\frac{-e^{-r^2}}{2}\Biggr]_0^2 \\ =& 2\pi  \left( -e^{-2^2}+1\right) \\ =& 2\pi -2\pi e^{-4} \end{align*},Calculate integral $$\iiint_V \frac{e^{-x^2-y^2-z^2}}{\sqrt{x^2+y^2+z^2}} dV$$ Where $V\subset\mathbb{R}^3$ is the exterior of a origocentered sphere with radius of 2 \begin{align*} V=&\iiint_V \frac{e^{-x^2-y^2-z^2}}{\sqrt{x^2+y^2+z^2}} dV \\ =& \int_0^{2\pi}\int_0^{\pi}\int_0^2 \frac{e^{-r^2}}{r}\rho^2\sin\phi dr d\phi d \theta \\ =& 2\pi \Biggl[-\cos\phi\Biggr]_0^{\pi} \int_0^2 re^{-r^2} dr \\ =& 2\pi (\underbrace{-\cos\pi}_{=1} +\underbrace{\cos0}_{=1}) \int_0^2 re^{-r^2} dr \\ =& 4\pi  \Biggl[\frac{-e^{-r^2}}{2}\Biggr]_0^2 \\ =& 2\pi  \left( -e^{-2^2}+1\right) \\ =& 2\pi -2\pi e^{-4} \end{align*},,"['calculus', 'multivariable-calculus']"
81,If the partial derivatives are continuous then the tangent plane exists.,If the partial derivatives are continuous then the tangent plane exists.,,"Background: I am studying Calculus from Stewart which is not rigorous. So, Stewart does not talk about this problem. Also, multivariable calculus is dealt mostly in context of three dimensions which is alright for me. And, I would like the answer in the context of 3 dimensions. Definitions: Differentiable: A function is differentiable at a point iff the tangents with respect to every direction (a,b) lies on one plane. This definition is just an intuition. I may be wrong and it may not be equivalent to the standard definition. But the problem still stands even if the two definitions are not equivalent. Motivation: Problem: If the partial derivatives are continuous at a point, then the function is differentiable at the point. This problem that if the partial derivatives are continuous at a point, then all the tangents lie on a single plane has been bugging me for a while. Sidenote: I saw in one forum post that existence of both partial derivatives at the point, and continuity of one is enough for the above problem to be true. Trials and thoughts: I don't seem to have a mental picture of of how this theorem is true and how the argument works. I have thought about proving the inverse: If all the tangents lie on a plane, then one of the partial derivatives might not be continuous. But, I haven;t succeeded. All I have done is set up the problem using the respective definitions. First I found the equation of the plane which contains the the two partial tangents and the point. Then I found the slope of the line which lies on the plane and is directed in a particular direction (a,b). Then, I tried to show that this ratio is equal to the directional derivative in the direction (a,b). The proof of this will illuminate why this theorem works the way it works. Thanks for the answers.","Background: I am studying Calculus from Stewart which is not rigorous. So, Stewart does not talk about this problem. Also, multivariable calculus is dealt mostly in context of three dimensions which is alright for me. And, I would like the answer in the context of 3 dimensions. Definitions: Differentiable: A function is differentiable at a point iff the tangents with respect to every direction (a,b) lies on one plane. This definition is just an intuition. I may be wrong and it may not be equivalent to the standard definition. But the problem still stands even if the two definitions are not equivalent. Motivation: Problem: If the partial derivatives are continuous at a point, then the function is differentiable at the point. This problem that if the partial derivatives are continuous at a point, then all the tangents lie on a single plane has been bugging me for a while. Sidenote: I saw in one forum post that existence of both partial derivatives at the point, and continuity of one is enough for the above problem to be true. Trials and thoughts: I don't seem to have a mental picture of of how this theorem is true and how the argument works. I have thought about proving the inverse: If all the tangents lie on a plane, then one of the partial derivatives might not be continuous. But, I haven;t succeeded. All I have done is set up the problem using the respective definitions. First I found the equation of the plane which contains the the two partial tangents and the point. Then I found the slope of the line which lies on the plane and is directed in a particular direction (a,b). Then, I tried to show that this ratio is equal to the directional derivative in the direction (a,b). The proof of this will illuminate why this theorem works the way it works. Thanks for the answers.",,"['real-analysis', 'multivariable-calculus']"
82,Tricky limit involving sine,Tricky limit involving sine,,"I'm trying to evaluate $$\text{lim}_{(x,y) \rightarrow (0,0)} \frac{x^4 + \text{sin}^2(y^2)}{x^4+y^4}.$$ I'm pretty sure that the limit exists and is $1$; at the very least, you get that if you approach $(0,0)$ along the lines $x=0$ and $y=0$ and $x=y$.  But I can't seem to figure out how to to show that the limit is $1$.  Thanks!","I'm trying to evaluate $$\text{lim}_{(x,y) \rightarrow (0,0)} \frac{x^4 + \text{sin}^2(y^2)}{x^4+y^4}.$$ I'm pretty sure that the limit exists and is $1$; at the very least, you get that if you approach $(0,0)$ along the lines $x=0$ and $y=0$ and $x=y$.  But I can't seem to figure out how to to show that the limit is $1$.  Thanks!",,"['limits', 'multivariable-calculus']"
83,Boundary integral of a harmonic function around a pole,Boundary integral of a harmonic function around a pole,,"I have a radial harmonic function $h:\mathbb R^N\backslash\{0\}\to\mathbb R$ which has a pole of order $m$ in 0, and I would like to compute $$ \frac{1}{\sigma_N}\int_{\partial B_r(x_0)}h(x)d\sigma(x), $$ where $\sigma$ is the surface measure and $\sigma_N$ the surface area of the ball $B_r(x_0)=\{x\in\mathbb R^N\ |\ ||x-x_0||<r\}$. I known that the above integral equals $h(x_0)$ if $\overline{B_r(x_0)}$ does not contain 0 (since then its just the mean value property). Moreover, in dimension $N=2$, I could show using complex analysis, that the above integral is constant if $0\in B_r(x_0)$. But what about higher dimensions? I read a paper where this constant fact is used without comment in higher dimensions, so I guess its right, but I cannot see the clue behind it. Thx in advance!","I have a radial harmonic function $h:\mathbb R^N\backslash\{0\}\to\mathbb R$ which has a pole of order $m$ in 0, and I would like to compute $$ \frac{1}{\sigma_N}\int_{\partial B_r(x_0)}h(x)d\sigma(x), $$ where $\sigma$ is the surface measure and $\sigma_N$ the surface area of the ball $B_r(x_0)=\{x\in\mathbb R^N\ |\ ||x-x_0||<r\}$. I known that the above integral equals $h(x_0)$ if $\overline{B_r(x_0)}$ does not contain 0 (since then its just the mean value property). Moreover, in dimension $N=2$, I could show using complex analysis, that the above integral is constant if $0\in B_r(x_0)$. But what about higher dimensions? I read a paper where this constant fact is used without comment in higher dimensions, so I guess its right, but I cannot see the clue behind it. Thx in advance!",,"['integration', 'multivariable-calculus', 'harmonic-functions']"
84,"How to evaluate the double integral $\int _{0}^{1}\!\int _{{x}^{2}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dy}\,{dx}$?",How to evaluate the double integral ?,"\int _{0}^{1}\!\int _{{x}^{2}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dy}\,{dx}","This is an exam problem that should be solvable in less than 30 minutes: $$\int _{0}^{1}\!\int _{{x}^{2}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dy}\,{dx}$$ I have tried switching the order of integration and the the boundaries like so: $$\int _{0}^{1}\!\int _{\sqrt {y}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dx}\,{dy}$$ But I always end up with having to evaluate something of the form: $$\int \!\sin \left( {y}^{3}\right) {dy}$$ Which even using software looks like a difficult one to evaluate and gives an absurdly long answer. Any pointers would help, I don't necessarily need all of the steps but if you can it would be very helpful. Many thanks!","This is an exam problem that should be solvable in less than 30 minutes: $$\int _{0}^{1}\!\int _{{x}^{2}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dy}\,{dx}$$ I have tried switching the order of integration and the the boundaries like so: $$\int _{0}^{1}\!\int _{\sqrt {y}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dx}\,{dy}$$ But I always end up with having to evaluate something of the form: $$\int \!\sin \left( {y}^{3}\right) {dy}$$ Which even using software looks like a difficult one to evaluate and gives an absurdly long answer. Any pointers would help, I don't necessarily need all of the steps but if you can it would be very helpful. Many thanks!",,['multivariable-calculus']
85,Global maxima of a function subject to a constraint.,Global maxima of a function subject to a constraint.,,"I am trying to prove that the global maxima of $f(x_1,x_2,...,x_n)=(x_1x_2···x_n)^2$, subject to $\lVert(x_1,x_2,...,x_n)\rVert_2=r$ is  $(r^2/n)^n$ I know I have to find the critical points of the Lagrange function, that is $$F(x_1,x_2,...,x_n,\lambda)=(x_1x_2···x_n)^2+\lambda(\sqrt{\sum_{i=1}^nx_i^2}-r)$$ In order to do that, I've built the system in which all partial derivatives are null, so I have: $$ D_1F(x_1,x_2,...,x_n,\lambda)=2(x_1x_2···x_n)(x_2x_3···x_n)+\frac{\lambda x_1}{\sqrt{\sum_{i=1}^nx_i^2}}=0 $$ $$ ... $$ $$ D_jF(x_1,x_2,...,x_n,\lambda)=2(x_1x_2···x_n)(x_1x_2···x_{j-1}x_{j+1}···x_n)+\frac{\lambda x_j}{\sqrt{\sum_{i=1}^nx_i^2}}=0 $$ $$ ... $$ $$ D_nF(x_1,x_2,...,x_n,\lambda)=2(x_1x_2···x_n)(x_1x_2···x_{n-1})+\frac{\lambda x_n}{\sqrt{\sum_{i=1}^nx_i^2}}=0 $$ $$ D_{n+1}F(x_1,x_2,...,x_n,\lambda)=\sqrt{\sum_{i=1}^n}x_i^2-r=0 $$ I think, because I have tried to solve reduced forms of this system in a mathematical software, that the solutions are like the following: $$ (x_1,x_2,...,x_n)=(\pm\frac{r}{\sqrt{n}},\pm\frac{r}{\sqrt{n}},...\pm\frac{r}{\sqrt{n}}) $$ where all the $x_i$ take the positive and negative sign, so there are $2^n$ solutions. However, I didn't manage to solve the system. I have tried to replace the summation with $r$ in all equations, to sum all the equations, to reduce dividing by $x_1···x_n$... but I didn't get to any interesting end. Could you help me, please, giving me any hint about how can I solve this system? Thank you very much.","I am trying to prove that the global maxima of $f(x_1,x_2,...,x_n)=(x_1x_2···x_n)^2$, subject to $\lVert(x_1,x_2,...,x_n)\rVert_2=r$ is  $(r^2/n)^n$ I know I have to find the critical points of the Lagrange function, that is $$F(x_1,x_2,...,x_n,\lambda)=(x_1x_2···x_n)^2+\lambda(\sqrt{\sum_{i=1}^nx_i^2}-r)$$ In order to do that, I've built the system in which all partial derivatives are null, so I have: $$ D_1F(x_1,x_2,...,x_n,\lambda)=2(x_1x_2···x_n)(x_2x_3···x_n)+\frac{\lambda x_1}{\sqrt{\sum_{i=1}^nx_i^2}}=0 $$ $$ ... $$ $$ D_jF(x_1,x_2,...,x_n,\lambda)=2(x_1x_2···x_n)(x_1x_2···x_{j-1}x_{j+1}···x_n)+\frac{\lambda x_j}{\sqrt{\sum_{i=1}^nx_i^2}}=0 $$ $$ ... $$ $$ D_nF(x_1,x_2,...,x_n,\lambda)=2(x_1x_2···x_n)(x_1x_2···x_{n-1})+\frac{\lambda x_n}{\sqrt{\sum_{i=1}^nx_i^2}}=0 $$ $$ D_{n+1}F(x_1,x_2,...,x_n,\lambda)=\sqrt{\sum_{i=1}^n}x_i^2-r=0 $$ I think, because I have tried to solve reduced forms of this system in a mathematical software, that the solutions are like the following: $$ (x_1,x_2,...,x_n)=(\pm\frac{r}{\sqrt{n}},\pm\frac{r}{\sqrt{n}},...\pm\frac{r}{\sqrt{n}}) $$ where all the $x_i$ take the positive and negative sign, so there are $2^n$ solutions. However, I didn't manage to solve the system. I have tried to replace the summation with $r$ in all equations, to sum all the equations, to reduce dividing by $x_1···x_n$... but I didn't get to any interesting end. Could you help me, please, giving me any hint about how can I solve this system? Thank you very much.",,['multivariable-calculus']
86,Flux and Gauss theorem,Flux and Gauss theorem,,"I have a problem; There seems to be something wrong with my understanding of gauss theorem. Let's say $F = [y ; x^2y; y^2z]$. I want to calculate the flux of $F$ going out of $$D = \{1 \le z \le 2 - x^2 - y^2\}$$ Method number 1 LEt's use Gauss theorem. So $\nabla \cdot F = x^2 + y^2$, and $$\int_{D} x^2 + y^2 = \int_{A} \int_1^{2-x^2-y^2} x^2 + y^2 dz = $$ $$\int_{A} (x^2 + y^2)(1 - x^2 - y^2) dxdy= \int_0^1\int_0^{2\pi} \rho ^3(1-\rho^2)) d\rho d\theta = \frac{\pi}{6}$$ Method number 2 LEt's do the explicit calculations. So let's write $D$ as $r_{up} = [x ; y ; 2 - x^2 - y^2]$ (for the upper part) and $r_{down} = [x ; y; 1]$ This leads to $$n_{up} = r_x \wedge r_y = [2x ; 2y ; 1]$$ $$n_{down} = r_x \wedge r_y = [0; 0; -1]$$ (I didn't normalize the normal because in the integral I'm going to write directly $dxdy$ without the norm of $n$) Then $$\int_{D} F \cdot n_{up} = \int_{D} 2xy + 2x^2y^2 + y^2z = \frac{\pi}{8}$$ and $$\int_{A} F \cdot n_{down} = \int_{A} -y^2 = -\frac{\pi}{4}$$ (where A is the unitary circle) So the two results are different. Why is this so? Thank you in advance EDIT I edited $n_{down}$, which I belive must be pointing out of $D$ so I changed it in $(0, 0, -1)$. This does not change the fact that the twto methods yields different results, though","I have a problem; There seems to be something wrong with my understanding of gauss theorem. Let's say $F = [y ; x^2y; y^2z]$. I want to calculate the flux of $F$ going out of $$D = \{1 \le z \le 2 - x^2 - y^2\}$$ Method number 1 LEt's use Gauss theorem. So $\nabla \cdot F = x^2 + y^2$, and $$\int_{D} x^2 + y^2 = \int_{A} \int_1^{2-x^2-y^2} x^2 + y^2 dz = $$ $$\int_{A} (x^2 + y^2)(1 - x^2 - y^2) dxdy= \int_0^1\int_0^{2\pi} \rho ^3(1-\rho^2)) d\rho d\theta = \frac{\pi}{6}$$ Method number 2 LEt's do the explicit calculations. So let's write $D$ as $r_{up} = [x ; y ; 2 - x^2 - y^2]$ (for the upper part) and $r_{down} = [x ; y; 1]$ This leads to $$n_{up} = r_x \wedge r_y = [2x ; 2y ; 1]$$ $$n_{down} = r_x \wedge r_y = [0; 0; -1]$$ (I didn't normalize the normal because in the integral I'm going to write directly $dxdy$ without the norm of $n$) Then $$\int_{D} F \cdot n_{up} = \int_{D} 2xy + 2x^2y^2 + y^2z = \frac{\pi}{8}$$ and $$\int_{A} F \cdot n_{down} = \int_{A} -y^2 = -\frac{\pi}{4}$$ (where A is the unitary circle) So the two results are different. Why is this so? Thank you in advance EDIT I edited $n_{down}$, which I belive must be pointing out of $D$ so I changed it in $(0, 0, -1)$. This does not change the fact that the twto methods yields different results, though",,"['calculus', 'multivariable-calculus', 'definite-integrals', 'surfaces']"
87,Bilinear map on the set of finite sequences,Bilinear map on the set of finite sequences,,"Let $X = \{ x = (x_n)_{n=1} ^ {\infty}\subset \mathbb{R} \ \ | \ \ \exists N \in \mathbb{N} : \forall n>N : x_n=0 \}$ Let the norm on $X$ be $||x|| = \sum _{n=1} ^{\infty} |x_n|$ (which is fine, because the sequences have only finitely many nonzero terms). Given $a = (a_n) _{n=1} ^{\infty}$ we define a bilinear map: $B : X \times X \ni (x,y) \rightarrow \sum_{n=1} ^{\infty} a_nx_ny_n \in \mathbb{R}$ Prove that $B(x, \cdot)$ is continuous for any $x \in X$. What is the necessary and sufficient condition for $B$ to be continuous? For $u,w \in X :||u - w|| < \delta$ we have: $\sum _{n=1} ^{\infty} |u_n-w_n|< \delta$ $|B(x, u) - B(x, w)| = |\sum _1 ^{\infty} a_nx_n(u_n-w_n)| \le \sum _1 ^{\infty} |a_nx_n(u_n-w_n)| \le \max \{a_nx_n \ | n \in \mathbb{N} \} \cdot \delta < \epsilon$ if for any given $\epsilon$ we take $\delta = \frac{\epsilon}{\max...}$ Isn't this approach a bit naive. It seems to me that since there are only finitely many nonzero terms in every sequence in $X$ we can choose such a maximum product $a_nx_n$. Could you tell me if I'm right and help me with the rest? Thank you!","Let $X = \{ x = (x_n)_{n=1} ^ {\infty}\subset \mathbb{R} \ \ | \ \ \exists N \in \mathbb{N} : \forall n>N : x_n=0 \}$ Let the norm on $X$ be $||x|| = \sum _{n=1} ^{\infty} |x_n|$ (which is fine, because the sequences have only finitely many nonzero terms). Given $a = (a_n) _{n=1} ^{\infty}$ we define a bilinear map: $B : X \times X \ni (x,y) \rightarrow \sum_{n=1} ^{\infty} a_nx_ny_n \in \mathbb{R}$ Prove that $B(x, \cdot)$ is continuous for any $x \in X$. What is the necessary and sufficient condition for $B$ to be continuous? For $u,w \in X :||u - w|| < \delta$ we have: $\sum _{n=1} ^{\infty} |u_n-w_n|< \delta$ $|B(x, u) - B(x, w)| = |\sum _1 ^{\infty} a_nx_n(u_n-w_n)| \le \sum _1 ^{\infty} |a_nx_n(u_n-w_n)| \le \max \{a_nx_n \ | n \in \mathbb{N} \} \cdot \delta < \epsilon$ if for any given $\epsilon$ we take $\delta = \frac{\epsilon}{\max...}$ Isn't this approach a bit naive. It seems to me that since there are only finitely many nonzero terms in every sequence in $X$ we can choose such a maximum product $a_nx_n$. Could you tell me if I'm right and help me with the rest? Thank you!",,"['real-analysis', 'multivariable-calculus', 'continuity']"
88,"How to find $\int_{S^2}f \cdot n \ \text{d}S$ if $f(x,y,z):=(x^3,y^3,z^3)^T$",How to find  if,"\int_{S^2}f \cdot n \ \text{d}S f(x,y,z):=(x^3,y^3,z^3)^T","With $\mathbb{S}^2$ being the unit sphere, how to find $$\int\limits_{\mathbb{S}^2} \vec{f} \cdot \vec{n} \ \text{d}S$$ if $\vec{f}(x,y,z):=(x^3,y^3,z^3)^T$? Apparently, we need to use Gauss. With $B$ being the unit ball we get: $$\int\limits_{\mathbb{S}^2} \vec{f} \cdot \vec{n} \ \text{d}S = \int\limits_B \operatorname{div} \vec{f} \ \text{d}V$$ with $\operatorname{div}f=3x^2+3y^2+3z^2$. Yet I'm not quite sure how to move further. I've tried the transformation theorem: $$\int_{\phi(K)} f = \int_{K} f(\phi(x)) |\det \phi'(x)| \ \text{d}x$$ with $\phi(r,\alpha,\beta):=(r \cos\alpha, r \sin\alpha \cos\beta, r \sin\alpha \sin\beta)$ and got stuck right away.","With $\mathbb{S}^2$ being the unit sphere, how to find $$\int\limits_{\mathbb{S}^2} \vec{f} \cdot \vec{n} \ \text{d}S$$ if $\vec{f}(x,y,z):=(x^3,y^3,z^3)^T$? Apparently, we need to use Gauss. With $B$ being the unit ball we get: $$\int\limits_{\mathbb{S}^2} \vec{f} \cdot \vec{n} \ \text{d}S = \int\limits_B \operatorname{div} \vec{f} \ \text{d}V$$ with $\operatorname{div}f=3x^2+3y^2+3z^2$. Yet I'm not quite sure how to move further. I've tried the transformation theorem: $$\int_{\phi(K)} f = \int_{K} f(\phi(x)) |\det \phi'(x)| \ \text{d}x$$ with $\phi(r,\alpha,\beta):=(r \cos\alpha, r \sin\alpha \cos\beta, r \sin\alpha \sin\beta)$ and got stuck right away.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'vector-analysis']"
89,Find the differential of an n-variable function,Find the differential of an n-variable function,,"The problem goes like this: If $f:\mathbb{R}^n\to\mathbb{R}, f(x)=\arctan||x||^4$, prove that $Df(x)(x)=\displaystyle\frac{4||x||^4}{1+||x||^8}$ Now, I've calculated each of the partial differentials (if that's the right word) and applied that $1\times n$ matrix to a vector $(x_1, ... ,x_n)$ and I get this: $4(\displaystyle\frac{x_1^4}{1+x_1^8}+...+\frac{x_n^4}{1+x_n^8})$ Now, the similarity between those terms and the final solution is obvious but I just can't seem to get the sum to become the above. Am I going about this the wrong way or am I just missing something? ($||\cdot||$ is the Euclidian norm)","The problem goes like this: If $f:\mathbb{R}^n\to\mathbb{R}, f(x)=\arctan||x||^4$, prove that $Df(x)(x)=\displaystyle\frac{4||x||^4}{1+||x||^8}$ Now, I've calculated each of the partial differentials (if that's the right word) and applied that $1\times n$ matrix to a vector $(x_1, ... ,x_n)$ and I get this: $4(\displaystyle\frac{x_1^4}{1+x_1^8}+...+\frac{x_n^4}{1+x_n^8})$ Now, the similarity between those terms and the final solution is obvious but I just can't seem to get the sum to become the above. Am I going about this the wrong way or am I just missing something? ($||\cdot||$ is the Euclidian norm)",,['multivariable-calculus']
90,Multivariate differentiability verification,Multivariate differentiability verification,,"I have tried an attempt on the following: Let $F:U\in \Bbb{R}^n\to \Bbb{R}$ and $f:\Bbb{R}\to \Bbb{R}$ where $f$ is an even function. Now $F(\mathbf{x})=f(|\mathbf{x}|)$, where $|\ . |$ is the Euclidean norm. Given $f$ is $C^r$, I have to show that $F$ is $C^r$. My attempt was to argument by induction. I showed that $F$ is $C^1$, basically using univariate chain rule on $\frac{\partial f\left(\sqrt{x_1^2+\dots+x_n^2}\right)}{\partial x_1}$ and arguing that it would be continuous using the fact that $f$ is $C^r$. Since the argument works for all $x_i$, it establishes first-differntiability for $F$ and continuity of the operator $DF(\mathbf{x})$. Then I moved on to show $F$ is $C^r$ assuming $F$ is $C^{r-1}$ and $f$ is $C^r$. Now what I did was partially differentiate: $$ {\partial \over \partial x_2} \left[\frac{\partial ^{k-1}f}{\partial x_1^{k-1}}\right] = \underbrace{{\partial \over \partial |\mathbf{x}|} \left[\frac{\partial ^{k-1}f}{\partial x_1^{k-1}}\right]}_{(1)} . \underbrace{{\partial |\mathbf{x}| \over \partial x_2}}_{(2)} $$ My argument runs as follows: this partial derivative is continuous since $f$ is $C^r$ and hence $(1)$ is continuous. Continuity of $(2)$ is establishd in a similar manner, since $\sqrt{\mathbf{x}}$ is $C^r$ if $\mathbf{x} \neq \mathbf{0}$. Then the product would be continuous. I am uneasy on multivariate arguments and dont know if my approach is completely off track. I didnt use the even function property, so I am sure I might me missing something. Any hints and references would be welcome. Thanks!","I have tried an attempt on the following: Let $F:U\in \Bbb{R}^n\to \Bbb{R}$ and $f:\Bbb{R}\to \Bbb{R}$ where $f$ is an even function. Now $F(\mathbf{x})=f(|\mathbf{x}|)$, where $|\ . |$ is the Euclidean norm. Given $f$ is $C^r$, I have to show that $F$ is $C^r$. My attempt was to argument by induction. I showed that $F$ is $C^1$, basically using univariate chain rule on $\frac{\partial f\left(\sqrt{x_1^2+\dots+x_n^2}\right)}{\partial x_1}$ and arguing that it would be continuous using the fact that $f$ is $C^r$. Since the argument works for all $x_i$, it establishes first-differntiability for $F$ and continuity of the operator $DF(\mathbf{x})$. Then I moved on to show $F$ is $C^r$ assuming $F$ is $C^{r-1}$ and $f$ is $C^r$. Now what I did was partially differentiate: $$ {\partial \over \partial x_2} \left[\frac{\partial ^{k-1}f}{\partial x_1^{k-1}}\right] = \underbrace{{\partial \over \partial |\mathbf{x}|} \left[\frac{\partial ^{k-1}f}{\partial x_1^{k-1}}\right]}_{(1)} . \underbrace{{\partial |\mathbf{x}| \over \partial x_2}}_{(2)} $$ My argument runs as follows: this partial derivative is continuous since $f$ is $C^r$ and hence $(1)$ is continuous. Continuity of $(2)$ is establishd in a similar manner, since $\sqrt{\mathbf{x}}$ is $C^r$ if $\mathbf{x} \neq \mathbf{0}$. Then the product would be continuous. I am uneasy on multivariate arguments and dont know if my approach is completely off track. I didnt use the even function property, so I am sure I might me missing something. Any hints and references would be welcome. Thanks!",,"['calculus', 'multivariable-calculus']"
91,Polar Integral Confusion,Polar Integral Confusion,,"Yet again, a friend of mine asked for help with a polar integral, we both got the same answer, the book again gave a different answer. Question Use a polar integral to find the area inside the circle $r=4 \sin \theta$ and outside $r=2$. Proposed Solution We see that the two circles intersect at $ \pm \frac{\pi}{6}$. Because of the symmetry of the region, we may write $$2 \int_{0}^{\frac{\pi}{6}} \int_{2}^{4 \sin \theta} r  \, \,dr d\theta $$ and equivalently $$2 \int_{0}^{\frac{\pi}{6}} 8 \sin^2 \theta - 2 \, \,d\theta$$ and finally $$-4 \int_{0}^{\frac{\pi}{6}} 2\cos 2\theta -1 \, \,d\theta$$  Which is $\frac{2}{3} ( \pi - 3\sqrt{3})$. The book gives an answer of $\frac{2}{3} ( 2\pi - 3\sqrt{3})$, the same as evaluating the integral from $0$ to $\frac{\pi}{3}$. Which answer is correct?","Yet again, a friend of mine asked for help with a polar integral, we both got the same answer, the book again gave a different answer. Question Use a polar integral to find the area inside the circle $r=4 \sin \theta$ and outside $r=2$. Proposed Solution We see that the two circles intersect at $ \pm \frac{\pi}{6}$. Because of the symmetry of the region, we may write $$2 \int_{0}^{\frac{\pi}{6}} \int_{2}^{4 \sin \theta} r  \, \,dr d\theta $$ and equivalently $$2 \int_{0}^{\frac{\pi}{6}} 8 \sin^2 \theta - 2 \, \,d\theta$$ and finally $$-4 \int_{0}^{\frac{\pi}{6}} 2\cos 2\theta -1 \, \,d\theta$$  Which is $\frac{2}{3} ( \pi - 3\sqrt{3})$. The book gives an answer of $\frac{2}{3} ( 2\pi - 3\sqrt{3})$, the same as evaluating the integral from $0$ to $\frac{\pi}{3}$. Which answer is correct?",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
92,Double Integral Confusion,Double Integral Confusion,,"A buddy was asking me for help with one of his MV Calc problems, and I ended up getting the same answer as him so I figured I'd ask it here... Question Find $$\iint_{R} (x-1) \, dA$$ where $R$ is the region enclosed by $y=x$ and $y=x^3$ in the first quadrant. So naturally I told him to compute $$\int_{0}^{1} \int_{x^3}^{x} (x-1) \, dy \, dx$$ as $x$ varies from $0$ to $1$ and $y$ from $x^3$ to $x$. Using this integral, we got $\frac{-7}{60}$. His textbook gives an answer of $\frac{-1}{2}$.","A buddy was asking me for help with one of his MV Calc problems, and I ended up getting the same answer as him so I figured I'd ask it here... Question Find $$\iint_{R} (x-1) \, dA$$ where $R$ is the region enclosed by $y=x$ and $y=x^3$ in the first quadrant. So naturally I told him to compute $$\int_{0}^{1} \int_{x^3}^{x} (x-1) \, dy \, dx$$ as $x$ varies from $0$ to $1$ and $y$ from $x^3$ to $x$. Using this integral, we got $\frac{-7}{60}$. His textbook gives an answer of $\frac{-1}{2}$.",,"['calculus', 'integration', 'multivariable-calculus']"
93,Proving Borsuk-Ulam with Stokes,Proving Borsuk-Ulam with Stokes,,"What is the easiest way to deduce the Borsuk-Ulam theorem in the case $n=2$ by using integration on manifolds and Stokes theorem? So I want to prove the following: Given a map $f\colon S^2\rightarrow \mathbb{R}^2$, show that there is $x$, such thath $f(x)=f(-x)$. My idea is: Suppose there is no such $x$ define $$g\colon S^2\rightarrow S^1,x\mapsto\frac{f(x)-f(-x)}{|f(x)-f(-x)|}.$$ Now $g$ preserves antipodal points. I think one can get a contradiction somehow by looking at the 2-Form $dg_1\wedge dg_2$ and invoking Stokes theorem?","What is the easiest way to deduce the Borsuk-Ulam theorem in the case $n=2$ by using integration on manifolds and Stokes theorem? So I want to prove the following: Given a map $f\colon S^2\rightarrow \mathbb{R}^2$, show that there is $x$, such thath $f(x)=f(-x)$. My idea is: Suppose there is no such $x$ define $$g\colon S^2\rightarrow S^1,x\mapsto\frac{f(x)-f(-x)}{|f(x)-f(-x)|}.$$ Now $g$ preserves antipodal points. I think one can get a contradiction somehow by looking at the 2-Form $dg_1\wedge dg_2$ and invoking Stokes theorem?",,"['multivariable-calculus', 'differential-geometry', 'differential-topology', 'differential-forms']"
94,Finding a volume,Finding a volume,,"Find the volume of $D\{(x,y,z)\in \mathbb{R}^3:\frac{x^2}{a^2} +\frac{y^2}{b^2}\leq z\leq 1 \}$ It looks like (1) I believe this could be solve with a double integral an considering the function $f(x,y) = \displaystyle\frac{x^2}{a^2} + \displaystyle\frac{y^2}{b^2}$ as the height and then substracting this from the volume of the rectangle that contains the volume. Then I should integrate between $0\leq \displaystyle\frac{x^2}{a^2}+\displaystyle\frac{y^2}{b^2}\leq 1$ (2) Another way may be defining g(x,y) = 1- f(x,y), I believe this should give the same answer as (1). I'm not sure wheter I should use polar coordinates or spherical coordinates instead -or a similar change, since the symmetry the the volume isn't exactly spheric-. But after the changes $x=r\cos\theta$ and $y=r\sin\theta$ doesn't seem wasy to determine the limits of $r$ and $\theta$ because would lead to the inequality $0\leq b^2r^2\cos^2\theta+a^2r^2\sin^2\theta\leq a^2b^2$ and I would conclude -probably wrong- $0\leq r \leq \left(\displaystyle\frac{cos^2\theta}{a^2} +\displaystyle\frac{\sin^2\theta}{b^2} \right)^{-1}$ which seems odd and also would give me some troubles if I try to compute the integral with that limit -because I'd be integrating $\displaystyle\int\int \displaystyle\frac{r^3\cos^2\theta}{a^2}+\displaystyle\frac{r^3\sin^2\theta}{b^2}\;drd\theta$-. Probably my approach is wrong and I don't get how to do this, but there is another way or it must be done as I said?.","Find the volume of $D\{(x,y,z)\in \mathbb{R}^3:\frac{x^2}{a^2} +\frac{y^2}{b^2}\leq z\leq 1 \}$ It looks like (1) I believe this could be solve with a double integral an considering the function $f(x,y) = \displaystyle\frac{x^2}{a^2} + \displaystyle\frac{y^2}{b^2}$ as the height and then substracting this from the volume of the rectangle that contains the volume. Then I should integrate between $0\leq \displaystyle\frac{x^2}{a^2}+\displaystyle\frac{y^2}{b^2}\leq 1$ (2) Another way may be defining g(x,y) = 1- f(x,y), I believe this should give the same answer as (1). I'm not sure wheter I should use polar coordinates or spherical coordinates instead -or a similar change, since the symmetry the the volume isn't exactly spheric-. But after the changes $x=r\cos\theta$ and $y=r\sin\theta$ doesn't seem wasy to determine the limits of $r$ and $\theta$ because would lead to the inequality $0\leq b^2r^2\cos^2\theta+a^2r^2\sin^2\theta\leq a^2b^2$ and I would conclude -probably wrong- $0\leq r \leq \left(\displaystyle\frac{cos^2\theta}{a^2} +\displaystyle\frac{\sin^2\theta}{b^2} \right)^{-1}$ which seems odd and also would give me some troubles if I try to compute the integral with that limit -because I'd be integrating $\displaystyle\int\int \displaystyle\frac{r^3\cos^2\theta}{a^2}+\displaystyle\frac{r^3\sin^2\theta}{b^2}\;drd\theta$-. Probably my approach is wrong and I don't get how to do this, but there is another way or it must be done as I said?.",,"['integration', 'multivariable-calculus', 'definite-integrals', 'volume']"
95,Why is this true: $\nabla \cdot (S\cdot \vec v )=S:(\nabla \otimes \vec v)+\vec v \cdot (\nabla \cdot S)$?,Why is this true: ?,\nabla \cdot (S\cdot \vec v )=S:(\nabla \otimes \vec v)+\vec v \cdot (\nabla \cdot S),I am doing fluid mechanics and I don't understand a particular step that is being used. It is the following step which I don't understand: $\nabla \cdot (S\cdot \vec  v )=S:(\nabla \otimes \vec v)+\vec v \cdot (\nabla \cdot S)$ (In the above: $S$ is a matrix;  $:$ denotes a double contraction) Can someone please provide me some background/ intuition why this is a logical step?,I am doing fluid mechanics and I don't understand a particular step that is being used. It is the following step which I don't understand: $\nabla \cdot (S\cdot \vec  v )=S:(\nabla \otimes \vec v)+\vec v \cdot (\nabla \cdot S)$ (In the above: $S$ is a matrix;  $:$ denotes a double contraction) Can someone please provide me some background/ intuition why this is a logical step?,,"['multivariable-calculus', 'vector-analysis', 'tensors', 'fluid-dynamics']"
96,Conditions for global invertibility of a function,Conditions for global invertibility of a function,,"Let $U$ be open and convex, $f:U \rightarrow \Bbb R^n$ continuously differentiable, where $\|Df(x)-Id\| < 1$, for all $x \in U$. Then, $f$ is injective on $U$ (thus, $f:U \rightarrow f(U)$ is globally invertible.) Attempt at a solution: Let $g(x) = f(x) - x$ and choose $a,b \in U$ where $a \neq b$. \begin{align} ||g(b)-g(a)|| &=||f(b)-b- \left(f(a)-a) \right)|| \\  &= \left|\left|\int_{0}^{1} \left[\frac {d}{dt}\left(\left(1-t\right)a + tb-f\left(\left(1-t\right)a+tb\right)\right) dt \right]\right|\right| \\  &\le\left|\left|\int_{0}^{1} \left[\left(b-a\right)-Df\left(\left(1-t\right)a + tb\right)\left(b-a\right) dt \right]\right|\right| \\ &\le\left|\left|b-a\right|\right|\int_{0}^{1} \left|\left|Id-Df\left(\left(1-t\right)a + tb\right)\right|\right| dt \lt\left|\left|b-a\right|\right|\int_{0}^{1}dt \end{align} Thus, $$||g(b)-g(a)||\lt\left|\left|b-a\right|\right|$$ It is at this point I do not come any further.","Let $U$ be open and convex, $f:U \rightarrow \Bbb R^n$ continuously differentiable, where $\|Df(x)-Id\| < 1$, for all $x \in U$. Then, $f$ is injective on $U$ (thus, $f:U \rightarrow f(U)$ is globally invertible.) Attempt at a solution: Let $g(x) = f(x) - x$ and choose $a,b \in U$ where $a \neq b$. \begin{align} ||g(b)-g(a)|| &=||f(b)-b- \left(f(a)-a) \right)|| \\  &= \left|\left|\int_{0}^{1} \left[\frac {d}{dt}\left(\left(1-t\right)a + tb-f\left(\left(1-t\right)a+tb\right)\right) dt \right]\right|\right| \\  &\le\left|\left|\int_{0}^{1} \left[\left(b-a\right)-Df\left(\left(1-t\right)a + tb\right)\left(b-a\right) dt \right]\right|\right| \\ &\le\left|\left|b-a\right|\right|\int_{0}^{1} \left|\left|Id-Df\left(\left(1-t\right)a + tb\right)\right|\right| dt \lt\left|\left|b-a\right|\right|\int_{0}^{1}dt \end{align} Thus, $$||g(b)-g(a)||\lt\left|\left|b-a\right|\right|$$ It is at this point I do not come any further.",,['multivariable-calculus']
97,How does the Jacobian relate 3D to 2D?,How does the Jacobian relate 3D to 2D?,,"May be it is simple, but I'm on Google for hours without finding a clue. I'm reading an article in computer vision where the optical flow equation is $\nabla I\cdot v + {dI \over dt} = 0 $ and for the 3D version it is $\nabla  I\cdot [J_\pi V]+{dI \over dt}=0$ I understand most of the paper but this sentence ""where the associated 3D displacement of a point is related to the motion of its projection by the $2 \times 3$  Jacobian matrix $J_\pi={\partial p \over \partial P}$ "" $P$ is a 3D point and $p$ is a 2D one, $v$ is 2d vector and $V$ is a 3D one, $\pi$ is the projection matrix (I think), I don't understand this Jacobian part, thanks in advance for any help or guidance. EDIT: Link to the original article has been added.","May be it is simple, but I'm on Google for hours without finding a clue. I'm reading an article in computer vision where the optical flow equation is $\nabla I\cdot v + {dI \over dt} = 0 $ and for the 3D version it is $\nabla  I\cdot [J_\pi V]+{dI \over dt}=0$ I understand most of the paper but this sentence ""where the associated 3D displacement of a point is related to the motion of its projection by the $2 \times 3$  Jacobian matrix $J_\pi={\partial p \over \partial P}$ "" $P$ is a 3D point and $p$ is a 2D one, $v$ is 2d vector and $V$ is a 3D one, $\pi$ is the projection matrix (I think), I don't understand this Jacobian part, thanks in advance for any help or guidance. EDIT: Link to the original article has been added.",,['multivariable-calculus']
98,"Is $\iint\limits_{D}\frac{1}{(x^{2}+y^{2})^{5}+5}dx\,dy$ convergent in $D=[0,+\infty)\times[0,+\infty)$",Is  convergent in,"\iint\limits_{D}\frac{1}{(x^{2}+y^{2})^{5}+5}dx\,dy D=[0,+\infty)\times[0,+\infty)","Can you tell me whether my approach is correct. We first switch to polar coordinates and we get the following integral: $\int\limits_{0}^{\frac{\pi}{2}}\int\limits_{0}^{\infty}\frac{r}{r^{10}+5}d\phi dr=\int\limits _{0}^{\frac{\pi}{2}}d\phi\int\limits _{0}^{\infty}\frac{r}{r^{10}+5}dr=\frac{\pi}{2}\int\limits _{0}^{\infty}\frac{r}{r^{10}+5}dr$ From now on we solve it as it were an improper integral of single variable. We know that $\int\limits _{1}^{+\infty}\frac{1}{r^{\alpha}}$ is convergent $\iff\alpha\ge2$. We use the following criteria. Let $F(x)\ge 0,\,G(x)\ge 0$ be such that $\lim\limits_{x\to\infty}\frac{F(x)}{G(x)}=\lambda,\lambda\in\mathbb{{R}}\cup\{+\infty\}$. If $\lambda$ is real and if $\int\limits_{a}^{+\infty}G(x)dx$ is convergent $\implies \int\limits _{a}^{+\infty}F(x)dx$ is also convergent. We take $G(r)=\frac{1}{r^{9}}$ and compute $\lim\limits_{r\to\infty}\frac{F(r)}{G(r)}=\frac{r^{10}}{r^{10}+5}=1$, hence the integral is convergent.","Can you tell me whether my approach is correct. We first switch to polar coordinates and we get the following integral: $\int\limits_{0}^{\frac{\pi}{2}}\int\limits_{0}^{\infty}\frac{r}{r^{10}+5}d\phi dr=\int\limits _{0}^{\frac{\pi}{2}}d\phi\int\limits _{0}^{\infty}\frac{r}{r^{10}+5}dr=\frac{\pi}{2}\int\limits _{0}^{\infty}\frac{r}{r^{10}+5}dr$ From now on we solve it as it were an improper integral of single variable. We know that $\int\limits _{1}^{+\infty}\frac{1}{r^{\alpha}}$ is convergent $\iff\alpha\ge2$. We use the following criteria. Let $F(x)\ge 0,\,G(x)\ge 0$ be such that $\lim\limits_{x\to\infty}\frac{F(x)}{G(x)}=\lambda,\lambda\in\mathbb{{R}}\cup\{+\infty\}$. If $\lambda$ is real and if $\int\limits_{a}^{+\infty}G(x)dx$ is convergent $\implies \int\limits _{a}^{+\infty}F(x)dx$ is also convergent. We take $G(r)=\frac{1}{r^{9}}$ and compute $\lim\limits_{r\to\infty}\frac{F(r)}{G(r)}=\frac{r^{10}}{r^{10}+5}=1$, hence the integral is convergent.",,"['multivariable-calculus', 'improper-integrals']"
99,Derivative of area of ellipse with respect to axis,Derivative of area of ellipse with respect to axis,,"Suppose $A(a,b)$ defines the area of an ellipse with axes $a,b$. We know that $A(a,b)=\pi ab$, and so $\partial_bA(a,b) = \pi a$. But suppose I parameterize the ellipse in polar coordinates as $r(\theta) = \sqrt{a^2\cos^2 \theta + b^2\sin^2 \theta}$. Then the area is given by $$\int_0^{2\pi}\frac12 r^2 d\theta$$ and so $$\partial_bA(a,b)=\partial_b \int_0^{2\pi}\frac12 a^2\cos^2 \theta + b^2 \sin^2 \theta\,d\theta\\=\int_0^{2\pi}\partial_b \frac12( a^2\cos^2 \theta + b^2 \sin^2 \theta) \, d\theta=\pi b.$$ What am I doing wrong?","Suppose $A(a,b)$ defines the area of an ellipse with axes $a,b$. We know that $A(a,b)=\pi ab$, and so $\partial_bA(a,b) = \pi a$. But suppose I parameterize the ellipse in polar coordinates as $r(\theta) = \sqrt{a^2\cos^2 \theta + b^2\sin^2 \theta}$. Then the area is given by $$\int_0^{2\pi}\frac12 r^2 d\theta$$ and so $$\partial_bA(a,b)=\partial_b \int_0^{2\pi}\frac12 a^2\cos^2 \theta + b^2 \sin^2 \theta\,d\theta\\=\int_0^{2\pi}\partial_b \frac12( a^2\cos^2 \theta + b^2 \sin^2 \theta) \, d\theta=\pi b.$$ What am I doing wrong?",,"['integration', 'multivariable-calculus']"
